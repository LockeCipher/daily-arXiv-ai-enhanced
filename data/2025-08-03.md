<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 28]
- [cs.RO](#cs.RO) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Robust and Efficient 3D Gaussian Splatting for Urban Scene Reconstruction](https://arxiv.org/abs/2507.23006)
*Zhensheng Yuan,Haozhi Huang,Zhen Xiong,Di Wang,Guanghua Yang*

Main category: cs.CV

TL;DR: 提出了一种快速重建和实时渲染城市规模场景的框架，优化训练效率并保持多视角捕获的外观变化鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决城市规模场景重建中训练效率低、外观不一致的问题。

Method: 采用场景分区并行训练，基于可见性的图像选择策略，可控的LOD策略调节高斯密度，外观变换模块处理不一致性，增强模块提升重建质量。

Result: 实验表明该方法高效重建城市规模场景，在效率和质量上优于先前方法。

Conclusion: 该方法为城市规模场景重建提供了高效且高质量的解决方案。

Abstract: We present a framework that enables fast reconstruction and real-time rendering of urban-scale scenes while maintaining robustness against appearance variations across multi-view captures. Our approach begins with scene partitioning for parallel training, employing a visibility-based image selection strategy to optimize training efficiency. A controllable level-of-detail (LOD) strategy explicitly regulates Gaussian density under a user-defined budget, enabling efficient training and rendering while maintaining high visual fidelity. The appearance transformation module mitigates the negative effects of appearance inconsistencies across images while enabling flexible adjustments. Additionally, we utilize enhancement modules, such as depth regularization, scale regularization, and antialiasing, to improve reconstruction fidelity. Experimental results demonstrate that our method effectively reconstructs urban-scale scenes and outperforms previous approaches in both efficiency and quality. The source code is available at: https://yzslab.github.io/REUrbanGS.

</details>


### [2] [Recovering Diagnostic Value: Super-Resolution-Aided Echocardiographic Classification in Resource-Constrained Imaging](https://arxiv.org/abs/2507.23027)
*Krishan Agyakari Raja Babu,Om Prabhu,Annu,Mohanasankar Sivaprakasam*

Main category: cs.CV

TL;DR: 论文探讨了深度学习超分辨率技术（SR）在提升低质量超声心动图分类准确性中的应用，特别是在资源有限的环境中。


<details>
  <summary>Details</summary>
Motivation: 在资源有限的环境中，低质量的超声心动图限制了诊断模型的效能，而超分辨率技术在此领域的应用尚未充分探索。

Method: 使用CAMUS数据集，按图像质量分层，评估了两种SR模型（SRGAN和SRResNet）在两种临床任务中的表现。

Result: SRResNet在提升图像质量和分类准确性方面表现显著，且计算高效。

Conclusion: 超分辨率技术能有效恢复低质量超声心动图的诊断价值，适用于资源有限环境中的AI辅助诊断。

Abstract: Automated cardiac interpretation in resource-constrained settings (RCS) is often hindered by poor-quality echocardiographic imaging, limiting the effectiveness of downstream diagnostic models. While super-resolution (SR) techniques have shown promise in enhancing magnetic resonance imaging (MRI) and computed tomography (CT) scans, their application to echocardiography-a widely accessible but noise-prone modality-remains underexplored. In this work, we investigate the potential of deep learning-based SR to improve classification accuracy on low-quality 2D echocardiograms. Using the publicly available CAMUS dataset, we stratify samples by image quality and evaluate two clinically relevant tasks of varying complexity: a relatively simple Two-Chamber vs. Four-Chamber (2CH vs. 4CH) view classification and a more complex End-Diastole vs. End-Systole (ED vs. ES) phase classification. We apply two widely used SR models-Super-Resolution Generative Adversarial Network (SRGAN) and Super-Resolution Residual Network (SRResNet), to enhance poor-quality images and observe significant gains in performance metric-particularly with SRResNet, which also offers computational efficiency. Our findings demonstrate that SR can effectively recover diagnostic value in degraded echo scans, making it a viable tool for AI-assisted care in RCS, achieving more with less.

</details>


### [3] [Adaptive Time-step Training for Enhancing Spike-Based Neural Radiance Fields](https://arxiv.org/abs/2507.23033)
*Ranxi Lin,Canming Yao,Jiayi Li,Weihang Liu,Xin Lou,Pingqiang Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种基于脉冲神经网络的动态时间步长训练策略（PATA），用于优化NeRF模型的资源消耗，同时保持渲染质量。


<details>
  <summary>Details</summary>
Motivation: NeRF模型在训练和推理中依赖密集点采样，导致高计算资源消耗，限制了其在边缘计算等资源受限场景的应用。

Method: 提出PATA策略，结合脉冲神经网络（SNNs）的动态时间步长调整，自动平衡渲染质量和时间步长。

Result: 实验显示，PATA在保持渲染质量的同时，减少推理时间步长64%和运行功耗61.55%。

Conclusion: PATA为资源受限场景提供了一种高效的NeRF解决方案。

Abstract: Neural Radiance Fields (NeRF)-based models have achieved remarkable success in 3D reconstruction and rendering tasks. However, during both training and inference, these models rely heavily on dense point sampling along rays from multiple viewpoints, resulting in a surge in floating-point operations and severely limiting their use in resource-constrained scenarios like edge computing. Spiking Neural Networks (SNNs), which communicate via binary spikes over discrete time steps, offer a promising alternative due to their energy-efficient nature. Given the inherent variability in scene scale and texture complexity in neural rendering and the prevailing practice of training separate models per scene, we propose a spike-based NeRF framework with a dynamic time step training strategy, termed Pretrain-Adaptive Time-step Adjustment (PATA). This approach automatically explores the trade-off between rendering quality and time step length during training. Consequently, it enables scene-adaptive inference with variable time steps and reduces the additional consumption of computational resources in the inference process. Anchoring to the established Instant-NGP architecture, we evaluate our method across diverse datasets. The experimental results show that PATA can preserve rendering fidelity while reducing inference time steps by 64\% and running power by 61.55\%.

</details>


### [4] [Reference-Guided Diffusion Inpainting For Multimodal Counterfactual Generation](https://arxiv.org/abs/2507.23058)
*Alexandru Buburuzan*

Main category: cs.CV

TL;DR: 论文提出了两种新型合成数据生成方法MObI和AnydoorMed，分别用于自动驾驶和医学影像分析，通过扩散模型实现高真实感和可控性。


<details>
  <summary>Details</summary>
Motivation: 由于真实数据采集成本高且复杂，合成数据方法在安全关键应用中日益重要，但需要高真实感和可控性。

Method: MObI利用扩散模型进行多模态对象修复，支持相机和激光雷达数据；AnydoorMed专注于医学影像的参考引导修复。

Result: 两种方法均能生成高度真实、可控的多模态合成数据，支持复杂的反事实场景构建。

Conclusion: 研究表明，基于扩散模型的参考引导修复方法可广泛应用于不同感知模态，为下一代高真实感合成数据系统铺平道路。

Abstract: Safety-critical applications, such as autonomous driving and medical image analysis, require extensive multimodal data for rigorous testing. Synthetic data methods are gaining prominence due to the cost and complexity of gathering real-world data, but they demand a high degree of realism and controllability to be useful. This work introduces two novel methods for synthetic data generation in autonomous driving and medical image analysis, namely MObI and AnydoorMed, respectively. MObI is a first-of-its-kind framework for Multimodal Object Inpainting that leverages a diffusion model to produce realistic and controllable object inpaintings across perceptual modalities, demonstrated simultaneously for camera and lidar. Given a single reference RGB image, MObI enables seamless object insertion into existing multimodal scenes at a specified 3D location, guided by a bounding box, while maintaining semantic consistency and multimodal coherence. Unlike traditional inpainting methods that rely solely on edit masks, this approach uses 3D bounding box conditioning to ensure accurate spatial positioning and realistic scaling. AnydoorMed extends this paradigm to the medical imaging domain, focusing on reference-guided inpainting for mammography scans. It leverages a diffusion-based model to inpaint anomalies with impressive detail preservation, maintaining the reference anomaly's structural integrity while semantically blending it with the surrounding tissue. Together, these methods demonstrate that foundation models for reference-guided inpainting in natural images can be readily adapted to diverse perceptual modalities, paving the way for the next generation of systems capable of constructing highly realistic, controllable and multimodal counterfactual scenarios.

</details>


### [5] [X-NeMo: Expressive Neural Motion Reenactment via Disentangled Latent Attention](https://arxiv.org/abs/2507.23143)
*Xiaochen Zhao,Hongyi Xu,Guoxian Song,You Xie,Chenxu Zhang,Xiu Li,Linjie Luo,Jinli Suo,Yebin Liu*

Main category: cs.CV

TL;DR: X-NeMo是一种基于扩散的零样本肖像动画方法，通过驱动视频的面部动作生成静态肖像的动画，解决了身份泄漏和表情捕捉问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法中的身份泄漏问题以及难以捕捉细微和极端表情的挑战。

Method: 提出一种端到端训练框架，从驱动图像中提取1D身份无关的潜在运动描述符，并通过交叉注意力控制图像生成。

Result: X-NeMo在实验中表现优于现有方法，生成高表现力且身份相似的动画。

Conclusion: X-NeMo通过创新的运动描述符和训练框架，显著提升了肖像动画的质量和身份保持能力。

Abstract: We propose X-NeMo, a novel zero-shot diffusion-based portrait animation pipeline that animates a static portrait using facial movements from a driving video of a different individual. Our work first identifies the root causes of the key issues in prior approaches, such as identity leakage and difficulty in capturing subtle and extreme expressions. To address these challenges, we introduce a fully end-to-end training framework that distills a 1D identity-agnostic latent motion descriptor from driving image, effectively controlling motion through cross-attention during image generation. Our implicit motion descriptor captures expressive facial motion in fine detail, learned end-to-end from a diverse video dataset without reliance on pretrained motion detectors. We further enhance expressiveness and disentangle motion latents from identity cues by supervising their learning with a dual GAN decoder, alongside spatial and color augmentations. By embedding the driving motion into a 1D latent vector and controlling motion via cross-attention rather than additive spatial guidance, our design eliminates the transmission of spatial-aligned structural clues from the driving condition to the diffusion backbone, substantially mitigating identity leakage. Extensive experiments demonstrate that X-NeMo surpasses state-of-the-art baselines, producing highly expressive animations with superior identity resemblance. Our code and models are available for research.

</details>


### [6] [Neural Multi-View Self-Calibrated Photometric Stereo without Photometric Stereo Cues](https://arxiv.org/abs/2507.23162)
*Xu Cao,Takafumi Taketomi*

Main category: cs.CV

TL;DR: 提出了一种神经逆向渲染方法，从多视角图像中联合重建几何、空间变化反射率和光照条件。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要光照校准或中间线索（如每视角法线图），而本文方法直接从原始图像中联合优化所有场景参数。

Method: 使用神经隐式场表示几何和反射率，结合阴影感知体积渲染。空间网络预测有符号距离和反射率潜在码，反射率网络基于潜在码和角度编码的方向估计反射率。

Result: 在形状和光照估计精度上优于现有方法，适用于视角未对齐的多光源图像，并能处理复杂几何和反射率物体。

Conclusion: 该方法无需中间线索，直接从图像中联合优化场景参数，表现出优越性能。

Abstract: We propose a neural inverse rendering approach that jointly reconstructs geometry, spatially varying reflectance, and lighting conditions from multi-view images captured under varying directional lighting. Unlike prior multi-view photometric stereo methods that require light calibration or intermediate cues such as per-view normal maps, our method jointly optimizes all scene parameters from raw images in a single stage. We represent both geometry and reflectance as neural implicit fields and apply shadow-aware volume rendering. A spatial network first predicts the signed distance and a reflectance latent code for each scene point. A reflectance network then estimates reflectance values conditioned on the latent code and angularly encoded surface normal, view, and light directions. The proposed method outperforms state-of-the-art normal-guided approaches in shape and lighting estimation accuracy, generalizes to view-unaligned multi-light images, and handles objects with challenging geometry and reflectance.

</details>


### [7] [Single Image Rain Streak Removal Using Harris Corner Loss and R-CBAM Network](https://arxiv.org/abs/2507.23185)
*Jongwook Si,Sungyoung Kim*

Main category: cs.CV

TL;DR: 提出了一种结合Corner Loss和R-CBAM模块的图像去雨方法，显著提升了去雨效果。


<details>
  <summary>Details</summary>
Motivation: 单图像去雨问题不仅需要抑制噪声，还需保留细节和视觉质量。

Method: 引入Corner Loss防止边界和细节丢失，并使用R-CBAM模块动态调整特征重要性。

Result: 在Rain100L和Rain100H数据集上分别达到33.29 dB和26.16 dB的PSNR。

Conclusion: 该方法在去雨任务中表现优于现有方法。

Abstract: The problem of single-image rain streak removal goes beyond simple noise suppression, requiring the simultaneous preservation of fine structural details and overall visual quality. In this study, we propose a novel image restoration network that effectively constrains the restoration process by introducing a Corner Loss, which prevents the loss of object boundaries and detailed texture information during restoration. Furthermore, we propose a Residual Convolutional Block Attention Module (R-CBAM) Block into the encoder and decoder to dynamically adjust the importance of features in both spatial and channel dimensions, enabling the network to focus more effectively on regions heavily affected by rain streaks. Quantitative evaluations conducted on the Rain100L and Rain100H datasets demonstrate that the proposed method significantly outperforms previous approaches, achieving a PSNR of 33.29 dB on Rain100L and 26.16 dB on Rain100H.

</details>


### [8] [Adversarial-Guided Diffusion for Multimodal LLM Attacks](https://arxiv.org/abs/2507.23202)
*Chengwei Xia,Fan Ma,Ruijie Quan,Kun Zhan,Yi Yang*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的对抗攻击方法（AGD），通过将目标语义注入反向扩散的噪声中，有效欺骗多模态大语言模型（MLLMs），同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 传统对抗攻击方法通常在高频段嵌入扰动，容易被防御措施（如低通滤波）抑制。AGD旨在通过全频谱噪声嵌入对抗信号，提高攻击的鲁棒性。

Method: 采用对抗引导扩散（AGD）方法，将目标语义注入反向扩散的噪声中，利用扩散模型的全频谱特性，使对抗信号不易被防御措施抑制。

Result: 实验表明，AGD在攻击性能和对抗防御鲁棒性上优于现有方法。

Conclusion: AGD通过全频谱噪声嵌入对抗信号，显著提升了对抗攻击的鲁棒性和有效性。

Abstract: This paper addresses the challenge of generating adversarial image using a diffusion model to deceive multimodal large language models (MLLMs) into generating the targeted responses, while avoiding significant distortion of the clean image. To address the above challenges, we propose an adversarial-guided diffusion (AGD) approach for adversarial attack MLLMs. We introduce adversarial-guided noise to ensure attack efficacy. A key observation in our design is that, unlike most traditional adversarial attacks which embed high-frequency perturbations directly into the clean image, AGD injects target semantics into the noise component of the reverse diffusion. Since the added noise in a diffusion model spans the entire frequency spectrum, the adversarial signal embedded within it also inherits this full-spectrum property. Importantly, during reverse diffusion, the adversarial image is formed as a linear combination of the clean image and the noise. Thus, when applying defenses such as a simple low-pass filtering, which act independently on each component, the adversarial image within the noise component is less likely to be suppressed, as it is not confined to the high-frequency band. This makes AGD inherently robust to variety defenses. Extensive experiments demonstrate that our AGD outperforms state-of-the-art methods in attack performance as well as in model robustness to some defenses.

</details>


### [9] [PixNerd: Pixel Neural Field Diffusion](https://arxiv.org/abs/2507.23268)
*Shuai Wang,Ziteng Gao,Chenhui Zhu,Weilin Huang,Limin Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为PixelNerd的单尺度、单阶段、高效的端到端解决方案，通过神经场建模补丁解码，避免了传统两阶段训练中的累积误差和解码伪影。


<details>
  <summary>Details</summary>
Motivation: 解决传统扩散变换器中因预训练变分自编码器（VAE）压缩潜在空间引入的累积误差和解码伪影问题。

Method: 提出PixelNerd框架，利用神经场表示直接建模像素空间中的补丁解码，无需复杂级联流程或VAE。

Result: 在ImageNet 256×256和512×512上分别达到2.15和2.84 FID，同时在文本到图像任务中表现优异。

Conclusion: PixelNerd提供了一种高效且性能优越的替代方案，避免了传统方法的复杂性。

Abstract: The current success of diffusion transformers heavily depends on the compressed latent space shaped by the pre-trained variational autoencoder(VAE). However, this two-stage training paradigm inevitably introduces accumulated errors and decoding artifacts. To address the aforementioned problems, researchers return to pixel space at the cost of complicated cascade pipelines and increased token complexity. In contrast to their efforts, we propose to model the patch-wise decoding with neural field and present a single-scale, single-stage, efficient, end-to-end solution, coined as pixel neural field diffusion~(PixelNerd). Thanks to the efficient neural field representation in PixNerd, we directly achieved 2.15 FID on ImageNet $256\times256$ and 2.84 FID on ImageNet $512\times512$ without any complex cascade pipeline or VAE. We also extend our PixNerd framework to text-to-image applications. Our PixNerd-XXL/16 achieved a competitive 0.73 overall score on the GenEval benchmark and 80.9 overall score on the DPG benchmark.

</details>


### [10] [iLRM: An Iterative Large 3D Reconstruction Model](https://arxiv.org/abs/2507.23277)
*Gyeongjin Kang,Seungtae Nam,Xiangyu Sun,Sameh Khamis,Abdelrahman Mohamed,Eunbyung Park*

Main category: cs.CV

TL;DR: iLRM提出了一种迭代式大规模3D重建模型，通过解耦场景表示、分解多视图交互和注入高分辨率信息，显著提升了重建质量和速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的方法因全注意力机制导致计算成本过高，难以扩展。iLRM旨在解决这一问题，实现高效、可扩展的3D重建。

Method: iLRM采用迭代细化机制，解耦场景表示，分解多视图交互为两阶段注意力，并在每层注入高分辨率信息。

Result: 在RE10K和DL3DV数据集上，iLRM在重建质量和速度上均优于现有方法，且具有更高的可扩展性。

Conclusion: iLRM通过创新的迭代机制和注意力优化，显著提升了3D重建的效率和性能。

Abstract: Feed-forward 3D modeling has emerged as a promising approach for rapid and high-quality 3D reconstruction. In particular, directly generating explicit 3D representations, such as 3D Gaussian splatting, has attracted significant attention due to its fast and high-quality rendering, as well as numerous applications. However, many state-of-the-art methods, primarily based on transformer architectures, suffer from severe scalability issues because they rely on full attention across image tokens from multiple input views, resulting in prohibitive computational costs as the number of views or image resolution increases. Toward a scalable and efficient feed-forward 3D reconstruction, we introduce an iterative Large 3D Reconstruction Model (iLRM) that generates 3D Gaussian representations through an iterative refinement mechanism, guided by three core principles: (1) decoupling the scene representation from input-view images to enable compact 3D representations; (2) decomposing fully-attentional multi-view interactions into a two-stage attention scheme to reduce computational costs; and (3) injecting high-resolution information at every layer to achieve high-fidelity reconstruction. Experimental results on widely used datasets, such as RE10K and DL3DV, demonstrate that iLRM outperforms existing methods in both reconstruction quality and speed. Notably, iLRM exhibits superior scalability, delivering significantly higher reconstruction quality under comparable computational cost by efficiently leveraging a larger number of input views.

</details>


### [11] [UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing](https://arxiv.org/abs/2507.23278)
*Hao Tang,Chenwei Xie,Xiaoyi Bao,Tingyu Weng,Pandeng Li,Yun Zheng,Liwei Wang*

Main category: cs.CV

TL;DR: UniLIP扩展了CLIP的功能，支持重建、生成和编辑，通过两阶段训练和自蒸馏策略保持原有理解能力，并在生成和编辑任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP统一方法需要额外模块支持重建和生成任务，导致性能不一致或理解能力下降。UniLIP旨在解决这一问题。

Method: 采用两阶段训练和自蒸馏策略，提出双条件架构连接MLLM和扩散变换器，利用可学习查询和多模态隐藏状态。

Result: 在生成任务中，GenEval和WISE得分分别为0.87和0.53；在编辑任务中，ImgEdit得分为3.62，均超越同类模型。

Conclusion: UniLIP成功扩展了CLIP的应用范围，在理解和生成/编辑任务中均表现优异。

Abstract: In this paper, we propose UniLIP, which extends CLIP to reconstruction, generation and editing, thereby building a unified tokenizer upon its exceptional comprehension capabilities. Previous CLIP-based unified methods often require additional diffusion decoders or quantization to support reconstruction and generation tasks, leading to inconsistent reconstruction or degradation of original comprehension performance.In contrast, we introduce a two-stage training scheme and a self-distillation strategy that progressively integrates reconstruction capabilities into CLIP, allowing it to maintain original comprehension performance while achieving effective image reconstruction. Furthermore, we propose a dual-condition architecture to connect the MLLM and diffusion transformer, using both learnable queries and the last layer multimodal hidden states as joint conditions. This method not only enables the utilization of the MLLM's strong reasoning capabilities in generation tasks, but also maximizes the exploitation of the rich information in UniLIP features during editing tasks. In text-to-image generation tasks, UniLIP obtains scores of 0.87 and 0.53 on GenEval and WISE benchmark respectively, surpassing all previous unified models of similar scale. In image editing, UniLIP also achieves a score of 3.62 on the ImgEdit Benchmark, surpassing recent state-of-the-art models such as BAGEL and UniWorld-V1. UniLIP effectively expand the application scope of CLIP, enabling continuous CLIP features to not only serve as the optimal choice for understanding tasks but also achieve highly competitive performance in generation and editing tasks.

</details>


### [12] [Training-free Geometric Image Editing on Diffusion Models](https://arxiv.org/abs/2507.23300)
*Hanshen Zhu,Zhen Zhu,Kaile Zhang,Yiming Gong,Yuliang Liu,Xiang Bai*

Main category: cs.CV

TL;DR: 提出了一种解耦的图像几何编辑流程，通过分离对象变换、源区域修复和目标区域细化，提升了编辑效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散方法在复杂或大规模变换时难以保持场景一致性的问题。

Method: 采用训练无关的扩散方法FreeFine，分别处理对象变换、源区域修复和目标区域细化。

Result: 在GeoBench基准测试中，FreeFine在图像保真度和编辑精度上优于现有方法。

Conclusion: 解耦的编辑流程和FreeFine方法显著提升了复杂几何编辑的效果。

Abstract: We tackle the task of geometric image editing, where an object within an image is repositioned, reoriented, or reshaped while preserving overall scene coherence. Previous diffusion-based editing methods often attempt to handle all relevant subtasks in a single step, proving difficult when transformations become large or structurally complex. We address this by proposing a decoupled pipeline that separates object transformation, source region inpainting, and target region refinement. Both inpainting and refinement are implemented using a training-free diffusion approach, FreeFine. In experiments on our new GeoBench benchmark, which contains both 2D and 3D editing scenarios, FreeFine outperforms state-of-the-art alternatives in image fidelity, and edit precision, especially under demanding transformations. Code and benchmark are available at: https://github.com/CIawevy/FreeFine

</details>


### [13] [MagicRoad: Semantic-Aware 3D Road Surface Reconstruction via Obstacle Inpainting](https://arxiv.org/abs/2507.23340)
*Xingyue Peng,Yuandong Lyu,Lang Zhang,Jian Zhu,Songtao Wang,Jiaxin Deng,Songxin Lu,Weiliang Ma,Dangen She,Peng Jia,XianPeng Lang*

Main category: cs.CV

TL;DR: 提出了一种鲁棒的路面重建框架，结合遮挡感知的2D高斯面元和语义引导的颜色增强，以恢复干净、一致的路面。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态遮挡、视觉杂乱和光照天气变化下表现不佳，需要一种更鲁棒的解决方案。

Method: 采用平面适应的高斯表示进行大规模建模，结合分割引导的视频修复和语义感知的HSV空间颜色校正。

Result: 在城市场景数据集上，该方法显著优于现有方法，实现了视觉一致和几何精确的重建。

Conclusion: 该框架在复杂真实条件下表现出色，为自动驾驶提供了可靠的路面重建支持。

Abstract: Road surface reconstruction is essential for autonomous driving, supporting centimeter-accurate lane perception and high-definition mapping in complex urban environments.While recent methods based on mesh rendering or 3D Gaussian splatting (3DGS) achieve promising results under clean and static conditions, they remain vulnerable to occlusions from dynamic agents, visual clutter from static obstacles, and appearance degradation caused by lighting and weather changes. We present a robust reconstruction framework that integrates occlusion-aware 2D Gaussian surfels with semantic-guided color enhancement to recover clean, consistent road surfaces. Our method leverages a planar-adapted Gaussian representation for efficient large-scale modeling, employs segmentation-guided video inpainting to remove both dynamic and static foreground objects, and enhances color coherence via semantic-aware correction in HSV space. Extensive experiments on urban-scale datasets demonstrate that our framework produces visually coherent and geometrically faithful reconstructions, significantly outperforming prior methods under real-world conditions.

</details>


### [14] [IN45023 Neural Network Design Patterns in Computer Vision Seminar Report, Summer 2025](https://arxiv.org/abs/2507.23357)
*Radu-Andrei Bourceanu,Neil De La Fuente,Jan Grimm,Andrei Jardan,Andriy Manucharyan,Cornelius Weiss,Roman Pflugfelder*

Main category: cs.CV

TL;DR: 分析六篇关键论文，探讨计算机视觉设计模式的演变，涵盖ResNet、ViT、GANs、LDMs、DINO和MAE等技术。


<details>
  <summary>Details</summary>
Motivation: 研究计算机视觉领域的关键设计模式及其演变，以推动图像识别、生成模型和自监督学习的发展。

Method: 通过分析六篇代表性论文，总结其架构、训练方法和创新点。

Result: ResNet解决梯度消失问题，ViT引入注意力机制，GANs和LDMs提升生成质量，DINO和MAE优化自监督学习。

Conclusion: 计算机视觉设计模式不断创新，从残差连接到注意力机制，再到自监督学习，推动了领域进步。

Abstract: This report analyzes the evolution of key design patterns in computer vision by examining six influential papers. The analy- sis begins with foundational architectures for image recognition. We review ResNet, which introduced residual connections to overcome the vanishing gradient problem and enable effective training of significantly deeper convolutional networks. Subsequently, we examine the Vision Transformer (ViT), which established a new paradigm by applying the Transformer ar- chitecture to sequences of image patches, demonstrating the efficacy of attention-based models for large-scale image recogni- tion. Building on these visual representation backbones, we investigate generative models. Generative Adversarial Networks (GANs) are analyzed for their novel adversarial training process, which challenges a generator against a discriminator to learn complex data distributions. Then, Latent Diffusion Models (LDMs) are covered, which improve upon prior generative methods by performing a sequential denoising process in a perceptually compressed latent space. LDMs achieve high-fidelity synthesis with greater computational efficiency, representing the current state-of-the-art for image generation. Finally, we explore self-supervised learning techniques that reduce dependency on labeled data. DINO is a self-distillation framework in which a student network learns to match the output of a momentum-updated teacher, yielding features with strong k-NN classification performance. We conclude with Masked Autoencoders (MAE), which utilize an asymmetric encoder-decoder design to reconstruct heavily masked inputs, providing a highly scalable and effective method for pre-training large-scale vision models.

</details>


### [15] [UniEmo: Unifying Emotional Understanding and Generation with Learnable Expert Queries](https://arxiv.org/abs/2507.23372)
*Yijie Zhu,Lingsen Zhang,Zitong Yu,Rui Shao,Tao Tan,Liqiang Nie*

Main category: cs.CV

TL;DR: UniEmo是一个统一框架，将情感理解和生成任务整合，通过分层情感理解链和多尺度特征提取实现互补增强。


<details>
  <summary>Details</summary>
Motivation: 情感理解和生成任务通常被分开处理，但它们本质互补，可以相互增强。

Method: 提出分层情感理解链和可学习专家查询，结合扩散模型生成情感图像，并引入情感相关系数和条件损失。

Result: UniEmo在情感理解和生成任务中显著优于现有方法。

Conclusion: 联合训练和生成驱动的双重反馈过程提升了模型的理解能力。

Abstract: Emotional understanding and generation are often treated as separate tasks, yet they are inherently complementary and can mutually enhance each other. In this paper, we propose the UniEmo, a unified framework that seamlessly integrates these two tasks. The key challenge lies in the abstract nature of emotions, necessitating the extraction of visual representations beneficial for both tasks. To address this, we propose a hierarchical emotional understanding chain with learnable expert queries that progressively extracts multi-scale emotional features, thereby serving as a foundational step for unification. Simultaneously, we fuse these expert queries and emotional representations to guide the diffusion model in generating emotion-evoking images. To enhance the diversity and fidelity of the generated emotional images, we further introduce the emotional correlation coefficient and emotional condition loss into the fusion process. This step facilitates fusion and alignment for emotional generation guided by the understanding. In turn, we demonstrate that joint training allows the generation component to provide implicit feedback to the understanding part. Furthermore, we propose a novel data filtering algorithm to select high-quality and diverse emotional images generated by the well-trained model, which explicitly feedback into the understanding part. Together, these generation-driven dual feedback processes enhance the model's understanding capacity. Extensive experiments show that UniEmo significantly outperforms state-of-the-art methods in both emotional understanding and generation tasks. The code for the proposed method is available at https://github.com/JiuTian-VL/UniEmo.

</details>


### [16] [NeRF Is a Valuable Assistant for 3D Gaussian Splatting](https://arxiv.org/abs/2507.23374)
*Shuangkang Fang,I-Chao Shen,Takeo Igarashi,Yufeng Wang,ZeSheng Wang,Yi Yang,Wenrui Ding,Shuchang Zhou*

Main category: cs.CV

TL;DR: NeRF-GS结合NeRF和3DGS的优势，通过联合优化提升3D场景表示性能。


<details>
  <summary>Details</summary>
Motivation: 解决3DGS对高斯初始化敏感、空间感知有限及高斯间关联弱的问题。

Method: 通过共享3D空间信息对齐NeRF和3DGS的空间特征，优化残差向量增强3DGS个性化能力。

Result: 在基准数据集上表现优于现有方法，达到SOTA。

Conclusion: NeRF和3DGS是互补的，为混合方法提供了新思路。

Abstract: We introduce NeRF-GS, a novel framework that jointly optimizes Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework leverages the inherent continuous spatial representation of NeRF to mitigate several limitations of 3DGS, including sensitivity to Gaussian initialization, limited spatial awareness, and weak inter-Gaussian correlations, thereby enhancing its performance. In NeRF-GS, we revisit the design of 3DGS and progressively align its spatial features with NeRF, enabling both representations to be optimized within the same scene through shared 3D spatial information. We further address the formal distinctions between the two approaches by optimizing residual vectors for both implicit features and Gaussian positions to enhance the personalized capabilities of 3DGS. Experimental results on benchmark datasets show that NeRF-GS surpasses existing methods and achieves state-of-the-art performance. This outcome confirms that NeRF and 3DGS are complementary rather than competing, offering new insights into hybrid approaches that combine 3DGS and NeRF for efficient 3D scene representation.

</details>


### [17] [Out-of-Distribution Detection in Medical Imaging via Diffusion Trajectories](https://arxiv.org/abs/2507.23411)
*Lemar Abdi,Francisco Caetano,Amaan Valiuddin,Christiaan Viviers,Hamdi Joudeh,Fons van der Sommen*

Main category: cs.CV

TL;DR: 提出了一种基于Stein分数的去噪扩散模型（SBDDM）的无监督OOD检测方法，通过扩散轨迹曲率实现高效异常检测。


<details>
  <summary>Details</summary>
Motivation: 解决现有生成方法在计算成本高、不可靠及需重新训练等问题，提升异常检测的效率和鲁棒性。

Method: 利用SBDDM的前向扩散轨迹，通过Stein分数捕获曲率，仅需五步扩散即可完成异常评分。

Result: 在Near-OOD和Far-OOD检测中分别相对提升10.43%和18.10%，计算成本显著降低。

Conclusion: SBDDM为实时可靠的计算机辅助诊断提供了实用工具。

Abstract: In medical imaging, unsupervised out-of-distribution (OOD) detection offers an attractive approach for identifying pathological cases with extremely low incidence rates. In contrast to supervised methods, OOD-based approaches function without labels and are inherently robust to data imbalances. Current generative approaches often rely on likelihood estimation or reconstruction error, but these methods can be computationally expensive, unreliable, and require retraining if the inlier data changes. These limitations hinder their ability to distinguish nominal from anomalous inputs efficiently, consistently, and robustly. We propose a reconstruction-free OOD detection method that leverages the forward diffusion trajectories of a Stein score-based denoising diffusion model (SBDDM). By capturing trajectory curvature via the estimated Stein score, our approach enables accurate anomaly scoring with only five diffusion steps. A single SBDDM pre-trained on a large, semantically aligned medical dataset generalizes effectively across multiple Near-OOD and Far-OOD benchmarks, achieving state-of-the-art performance while drastically reducing computational cost during inference. Compared to existing methods, SBDDM achieves a relative improvement of up to 10.43% and 18.10% for Near-OOD and Far-OOD detection, making it a practical building block for real-time, reliable computer-aided diagnosis.

</details>


### [18] [Stable-Sim2Real: Exploring Simulation of Real-Captured 3D Data with Two-Stage Depth Diffusion](https://arxiv.org/abs/2507.23483)
*Mutian Xu,Chongjie Ye,Haolin Liu,Yushuang Wu,Jiahao Chang,Xiaoguang Han*

Main category: cs.CV

TL;DR: 提出了一种名为Stable-Sim2Real的两阶段深度扩散模型，用于数据驱动的3D数据模拟，显著提升了真实世界3D视觉任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D数据模拟方法难以完全捕捉真实数据复杂性的问题。

Method: 采用两阶段深度扩散模型：第一阶段生成粗略深度残差，第二阶段通过调整扩散损失优化局部区域。

Result: 实验表明，使用该方法生成的3D模拟数据显著提升了真实任务的性能，且与真实数据高度相似。

Conclusion: Stable-Sim2Real为3D数据模拟提供了一种高效的新方法，具有实际应用潜力。

Abstract: 3D data simulation aims to bridge the gap between simulated and real-captured 3D data, which is a fundamental problem for real-world 3D visual tasks. Most 3D data simulation methods inject predefined physical priors but struggle to capture the full complexity of real data. An optimal approach involves learning an implicit mapping from synthetic to realistic data in a data-driven manner, but progress in this solution has met stagnation in recent studies. This work explores a new solution path of data-driven 3D simulation, called Stable-Sim2Real, based on a novel two-stage depth diffusion model. The initial stage finetunes Stable-Diffusion to generate the residual between the real and synthetic paired depth, producing a stable but coarse depth, where some local regions may deviate from realistic patterns. To enhance this, both the synthetic and initial output depth are fed into a second-stage diffusion, where diffusion loss is adjusted to prioritize these distinct areas identified by a 3D discriminator. We provide a new benchmark scheme to evaluate 3D data simulation methods. Extensive experiments show that training the network with the 3D simulated data derived from our method significantly enhances performance in real-world 3D visual tasks. Moreover, the evaluation demonstrates the high similarity between our 3D simulated data and real-captured patterns. Project page: https://mutianxu.github.io/stable-sim2real/.

</details>


### [19] [Online Estimation of Table-Top Grown Strawberry Mass in Field Conditions with Occlusions](https://arxiv.org/abs/2507.23487)
*Jinshan Zhen,Yuanyue Ge,Tianxiao Zhu,Hui Zhao,Ya Xiong*

Main category: cs.CV

TL;DR: 本文提出了一种基于RGB-D传感和深度学习的草莓质量估计方法，通过实例分割、遮挡修复和角度校正实现实时在线估计，误差率低。


<details>
  <summary>Details</summary>
Motivation: 解决田间种植草莓因遮挡和姿态变化导致的质量估计难题。

Method: 结合YOLOv8-Seg实例分割、CycleGAN遮挡修复和倾斜角度校正，通过多项式回归模型映射几何特征到质量。

Result: 孤立草莓和遮挡情况下的平均质量估计误差分别为8.11%和10.47%，CycleGAN在遮挡修复上优于LaMa模型。

Conclusion: 该方法为复杂遮挡条件下的自动化收获和产量监测提供了可靠解决方案。

Abstract: Accurate mass estimation of table-top grown strawberries under field conditions remains challenging due to frequent occlusions and pose variations. This study proposes a vision-based pipeline integrating RGB-D sensing and deep learning to enable non-destructive, real-time and online mass estimation. The method employed YOLOv8-Seg for instance segmentation, Cycle-consistent generative adversarial network (CycleGAN) for occluded region completion, and tilt-angle correction to refine frontal projection area calculations. A polynomial regression model then mapped the geometric features to mass. Experiments demonstrated mean mass estimation errors of 8.11% for isolated strawberries and 10.47% for occluded cases. CycleGAN outperformed large mask inpainting (LaMa) model in occlusion recovery, achieving superior pixel area ratios (PAR) (mean: 0.978 vs. 1.112) and higher intersection over union (IoU) scores (92.3% vs. 47.7% in the [0.9-1] range). This approach addresses critical limitations of traditional methods, offering a robust solution for automated harvesting and yield monitoring with complex occlusion patterns.

</details>


### [20] [Gaussian Splatting Feature Fields for Privacy-Preserving Visual Localization](https://arxiv.org/abs/2507.23569)
*Maxime Pietrantoni,Gabriela Csurka,Torsten Sattler*

Main category: cs.CV

TL;DR: 论文提出了一种基于3D高斯泼溅（3DGS）的视觉定位方法GSFFs，结合显式几何模型与隐式特征场，实现了高精度且保护隐私的定位。


<details>
  <summary>Details</summary>
Motivation: 视觉定位需要精确估计相机位姿，同时保护隐私。传统方法在隐私保护和性能上存在不足，因此需要一种新的表示方法。

Method: 提出GSFFs，结合3DGS的几何信息与隐式特征场，通过对比学习对齐3D特征场与2D特征编码器，并利用聚类正则化特征学习。

Result: 在多个真实数据集上，隐私和非隐私保护定位管道均达到最先进性能。

Conclusion: GSFFs通过结合显式几何与隐式特征，实现了高精度且隐私保护的视觉定位。

Abstract: Visual localization is the task of estimating a camera pose in a known environment. In this paper, we utilize 3D Gaussian Splatting (3DGS)-based representations for accurate and privacy-preserving visual localization. We propose Gaussian Splatting Feature Fields (GSFFs), a scene representation for visual localization that combines an explicit geometry model (3DGS) with an implicit feature field. We leverage the dense geometric information and differentiable rasterization algorithm from 3DGS to learn robust feature representations grounded in 3D. In particular, we align a 3D scale-aware feature field and a 2D feature encoder in a common embedding space through a contrastive framework. Using a 3D structure-informed clustering procedure, we further regularize the representation learning and seamlessly convert the features to segmentations, which can be used for privacy-preserving visual localization. Pose refinement, which involves aligning either feature maps or segmentations from a query image with those rendered from the GSFFs scene representation, is used to achieve localization. The resulting privacy- and non-privacy-preserving localization pipelines, evaluated on multiple real-world datasets, show state-of-the-art performances.

</details>


### [21] [DivControl: Knowledge Diversion for Controllable Image Generation](https://arxiv.org/abs/2507.23620)
*Yucheng Xie,Fu Feng,Ruixiao Shi,Jing Wang,Yong Rui,Xin Geng*

Main category: cs.CV

TL;DR: DivControl提出了一种可分解的预训练框架，通过知识分流和动态门控实现统一可控生成和高效适应，显著降低训练成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在图像到图像生成中要么需要为每个条件训练单独模型，要么依赖纠缠表示的统一架构，导致泛化能力差和适应成本高。

Method: DivControl通过SVD分解ControlNet为基本组件，利用知识分流和动态门控实现条件无关的learngenes和条件特定的tailors，并通过表示对齐损失提升条件保真度。

Result: DivControl在可控性上达到SOTA，训练成本降低36.4倍，同时在未见条件上表现出强大的零样本和少样本性能。

Conclusion: DivControl展示了卓越的可扩展性、模块化和迁移能力，为可控生成提供了高效解决方案。

Abstract: Diffusion models have advanced from text-to-image (T2I) to image-to-image (I2I) generation by incorporating structured inputs such as depth maps, enabling fine-grained spatial control. However, existing methods either train separate models for each condition or rely on unified architectures with entangled representations, resulting in poor generalization and high adaptation costs for novel conditions. To this end, we propose DivControl, a decomposable pretraining framework for unified controllable generation and efficient adaptation. DivControl factorizes ControlNet via SVD into basic components-pairs of singular vectors-which are disentangled into condition-agnostic learngenes and condition-specific tailors through knowledge diversion during multi-condition training. Knowledge diversion is implemented via a dynamic gate that performs soft routing over tailors based on the semantics of condition instructions, enabling zero-shot generalization and parameter-efficient adaptation to novel conditions. To further improve condition fidelity and training efficiency, we introduce a representation alignment loss that aligns condition embeddings with early diffusion features. Extensive experiments demonstrate that DivControl achieves state-of-the-art controllability with 36.4$\times$ less training cost, while simultaneously improving average performance on basic conditions. It also delivers strong zero-shot and few-shot performance on unseen conditions, demonstrating superior scalability, modularity, and transferability.

</details>


### [22] [Adaptively Distilled ControlNet: Accelerated Training and Superior Sampling for Medical Image Synthesis](https://arxiv.org/abs/2507.23652)
*Kunpeng Qiu,Zhiying Zhou,Yongxin Guo*

Main category: cs.CV

TL;DR: 提出了一种任务无关的框架Adaptively Distilled ControlNet，通过双模型蒸馏加速训练和优化，解决了医学图像标注中的隐私和标注效率问题。


<details>
  <summary>Details</summary>
Motivation: 医学图像标注受隐私和人工标注限制，影响分割模型的性能和泛化能力。现有mask-controllable扩散模型在合成图像时难以实现精确的病灶-掩模对齐。

Method: 采用双模型蒸馏框架：教师模型基于掩模-图像对训练，通过参数空间中的噪声对齐和病灶-背景比的自适应正则化指导学生模型。采样时仅使用学生模型，保护隐私。

Result: 在两个医学数据集上表现优异：TransUNet在KiTS19上mDice/mIoU提升2.4%/4.2%，SANet在Polyps上提升2.6%/3.5%。

Conclusion: Adaptively Distilled ControlNet在医学图像生成中表现出高效性和优越性，同时保护隐私。

Abstract: Medical image annotation is constrained by privacy concerns and labor-intensive labeling, significantly limiting the performance and generalization of segmentation models. While mask-controllable diffusion models excel in synthesis, they struggle with precise lesion-mask alignment. We propose \textbf{Adaptively Distilled ControlNet}, a task-agnostic framework that accelerates training and optimization through dual-model distillation. Specifically, during training, a teacher model, conditioned on mask-image pairs, regularizes a mask-only student model via predicted noise alignment in parameter space, further enhanced by adaptive regularization based on lesion-background ratios. During sampling, only the student model is used, enabling privacy-preserving medical image generation. Comprehensive evaluations on two distinct medical datasets demonstrate state-of-the-art performance: TransUNet improves mDice/mIoU by 2.4%/4.2% on KiTS19, while SANet achieves 2.6%/3.5% gains on Polyps, highlighting its effectiveness and superiority. Code is available at GitHub.

</details>


### [23] [I2V-GS: Infrastructure-to-Vehicle View Transformation with Gaussian Splatting for Autonomous Driving Data Generation](https://arxiv.org/abs/2507.23683)
*Jialei Chen,Wuhao Xu,Sipeng He,Baoru Huang,Dongchun Ren*

Main category: cs.CV

TL;DR: I2V-GS是一种通过高斯散射将基础设施视图转换为车辆视图的新方法，用于生成自动驾驶数据，显著提升了合成质量。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶数据主要依赖车辆采集，成本高且效率低，而通过合成真实世界图像的数据是一种潜在解决方案。

Method: 采用自适应深度扭曲生成密集训练视图，使用级联策略扩展视图范围，并通过跨视图信息进行置信度引导优化。

Result: I2V-GS在车辆视图下的合成质量显著优于StreetGaussian，NTA-Iou、NTL-Iou和FID分别提升45.7%、34.2%和14.9%。

Conclusion: I2V-GS是首个实现基础设施-车辆视图转换生成自动驾驶数据集的框架，展示了高效的数据合成潜力。

Abstract: Vast and high-quality data are essential for end-to-end autonomous driving systems. However, current driving data is mainly collected by vehicles, which is expensive and inefficient. A potential solution lies in synthesizing data from real-world images. Recent advancements in 3D reconstruction demonstrate photorealistic novel view synthesis, highlighting the potential of generating driving data from images captured on the road. This paper introduces a novel method, I2V-GS, to transfer the Infrastructure view To the Vehicle view with Gaussian Splatting. Reconstruction from sparse infrastructure viewpoints and rendering under large view transformations is a challenging problem. We adopt the adaptive depth warp to generate dense training views. To further expand the range of views, we employ a cascade strategy to inpaint warped images, which also ensures inpainting content is consistent across views. To further ensure the reliability of the diffusion model, we utilize the cross-view information to perform a confidenceguided optimization. Moreover, we introduce RoadSight, a multi-modality, multi-view dataset from real scenarios in infrastructure views. To our knowledge, I2V-GS is the first framework to generate autonomous driving datasets with infrastructure-vehicle view transformation. Experimental results demonstrate that I2V-GS significantly improves synthesis quality under vehicle view, outperforming StreetGaussian in NTA-Iou, NTL-Iou, and FID by 45.7%, 34.2%, and 14.9%, respectively.

</details>


### [24] [UniLDiff: Unlocking the Power of Diffusion Priors for All-in-One Image Restoration](https://arxiv.org/abs/2507.23685)
*Zihan Cheng,Liangtai Zhou,Dian Chen,Ni Tang,Xiaotong Luo,Yanyun Qu*

Main category: cs.CV

TL;DR: 提出了一种基于潜在扩散模型（LDM）的统一图像修复框架，通过Degradation-Aware Feature Fusion（DAFF）模块和Detail-Aware Expert Module（DAEM）模块，实现了对多样化退化的自适应处理和细节恢复。


<details>
  <summary>Details</summary>
Motivation: 解决All-in-One Image Restoration（AiOIR）中的核心挑战，利用扩散模型的强大生成能力处理多样化退化问题。

Method: 采用潜在扩散模型（LDM），设计DAFF模块自适应处理退化类型，DAEM模块增强细节恢复。

Result: 在多任务和混合退化设置下，实验表明该方法性能达到最先进水平。

Conclusion: 扩散先验在统一图像修复中具有实际潜力，代码将开源。

Abstract: All-in-One Image Restoration (AiOIR) has emerged as a promising yet challenging research direction. To address its core challenges, we propose a novel unified image restoration framework based on latent diffusion models (LDMs). Our approach structurally integrates low-quality visual priors into the diffusion process, unlocking the powerful generative capacity of diffusion models for diverse degradations. Specifically, we design a Degradation-Aware Feature Fusion (DAFF) module to enable adaptive handling of diverse degradation types. Furthermore, to mitigate detail loss caused by the high compression and iterative sampling of LDMs, we design a Detail-Aware Expert Module (DAEM) in the decoder to enhance texture and fine-structure recovery. Extensive experiments across multi-task and mixed degradation settings demonstrate that our method consistently achieves state-of-the-art performance, highlighting the practical potential of diffusion priors for unified image restoration. Our code will be released.

</details>


### [25] [Enhanced Velocity Field Modeling for Gaussian Video Reconstruction](https://arxiv.org/abs/2507.23704)
*Zhenyang Li,Xiaoyang Bai,Tongchen Zhang,Pengfei Shen,Weiwei Xu,Yifan Peng*

Main category: cs.CV

TL;DR: 论文提出了一种名为FlowGaussian-VR的流增强速度场建模方案，用于解决复杂运动和尺度变化视频中的3D高斯重建问题，显著提升了视觉质量和动态内容处理能力。


<details>
  <summary>Details</summary>
Motivation: 高保真3D视频重建对VR/AR中的动态场景实时渲染至关重要，但现有方法在复杂运动和尺度变化下表现不佳，导致视觉质量下降。

Method: FlowGaussian-VR包含两个核心组件：基于光流优化的速度场渲染（VFR）流程和流辅助自适应密度化（FAD）策略，用于动态区域的高斯调整。

Result: 在多个真实数据集上的实验表明，该方法显著提升了视觉质量（PSNR增益超过2.5 dB），减少了动态纹理的模糊，并实现了高斯轨迹的规范化跟踪。

Conclusion: FlowGaussian-VR通过流增强的速度场建模，有效解决了复杂动态场景中的重建问题，为VR/AR应用提供了高质量的3D视频重建方案。

Abstract: High-fidelity 3D video reconstruction is essential for enabling real-time rendering of dynamic scenes with realistic motion in virtual and augmented reality (VR/AR). The deformation field paradigm of 3D Gaussian splatting has achieved near-photorealistic results in video reconstruction due to the great representation capability of deep deformation networks. However, in videos with complex motion and significant scale variations, deformation networks often overfit to irregular Gaussian trajectories, leading to suboptimal visual quality. Moreover, the gradient-based densification strategy designed for static scene reconstruction proves inadequate to address the absence of dynamic content. In light of these challenges, we propose a flow-empowered velocity field modeling scheme tailored for Gaussian video reconstruction, dubbed FlowGaussian-VR. It consists of two core components: a velocity field rendering (VFR) pipeline which enables optical flow-based optimization, and a flow-assisted adaptive densification (FAD) strategy that adjusts the number and size of Gaussians in dynamic regions. We validate our model's effectiveness on multi-view dynamic reconstruction and novel view synthesis with multiple real-world datasets containing challenging motion scenarios, demonstrating not only notable visual improvements (over 2.5 dB gain in PSNR) and less blurry artifacts in dynamic textures, but also regularized and trackable per-Gaussian trajectories.

</details>


### [26] [SeqAffordSplat: Scene-level Sequential Affordance Reasoning on 3D Gaussian Splatting](https://arxiv.org/abs/2507.23772)
*Di Li,Jie Feng,Jiahao Chen,Weisheng Dong,Guanbin Li,Yuhui Zheng,Mingtao Feng,Guangming Shi*

Main category: cs.CV

TL;DR: 论文提出了一种新的任务Sequential 3D Gaussian Affordance Reasoning，并建立了SeqAffordSplat基准和SeqSplatNet框架，用于解决复杂场景中的长时程3D功能区域推理问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯泼溅的方法局限于单对象、单步交互，无法满足复杂现实任务的需求。

Method: 提出SeqSplatNet框架，结合大语言模型和条件解码器，生成序列化的3D功能区域掩码；引入预训练策略和特征注入机制。

Result: 在SeqAffordSplat基准上取得了最优性能，实现了从单步交互到复杂场景级任务的提升。

Conclusion: SeqSplatNet有效推动了3D功能区域推理从单步交互向复杂长时程任务的发展。

Abstract: 3D affordance reasoning, the task of associating human instructions with the functional regions of 3D objects, is a critical capability for embodied agents. Current methods based on 3D Gaussian Splatting (3DGS) are fundamentally limited to single-object, single-step interactions, a paradigm that falls short of addressing the long-horizon, multi-object tasks required for complex real-world applications. To bridge this gap, we introduce the novel task of Sequential 3D Gaussian Affordance Reasoning and establish SeqAffordSplat, a large-scale benchmark featuring 1800+ scenes to support research on long-horizon affordance understanding in complex 3DGS environments. We then propose SeqSplatNet, an end-to-end framework that directly maps an instruction to a sequence of 3D affordance masks. SeqSplatNet employs a large language model that autoregressively generates text interleaved with special segmentation tokens, guiding a conditional decoder to produce the corresponding 3D mask. To handle complex scene geometry, we introduce a pre-training strategy, Conditional Geometric Reconstruction, where the model learns to reconstruct complete affordance region masks from known geometric observations, thereby building a robust geometric prior. Furthermore, to resolve semantic ambiguities, we design a feature injection mechanism that lifts rich semantic features from 2D Vision Foundation Models (VFM) and fuses them into the 3D decoder at multiple scales. Extensive experiments demonstrate that our method sets a new state-of-the-art on our challenging benchmark, effectively advancing affordance reasoning from single-step interactions to complex, sequential tasks at the scene level.

</details>


### [27] [SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions](https://arxiv.org/abs/2507.23784)
*Jessica Bader,Leander Girrbach,Stephan Alaniz,Zeynep Akata*

Main category: cs.CV

TL;DR: 论文提出了SUB基准，用于评估概念瓶颈模型（CBMs）在分布变化下识别概念的可靠性，并开发了Tied Diffusion Guidance（TDG）方法精确生成图像。


<details>
  <summary>Details</summary>
Motivation: 概念瓶颈模型（CBMs）在透明AI应用中表现优异，但在分布变化下识别概念的可靠性不足，需要更严格的评估方法。

Method: 通过SUB基准（基于CUB数据集的38,400张合成图像）和TDG方法（共享噪声的双并行去噪过程）精确控制生成图像。

Result: SUB基准和TDG方法成功生成具有特定概念的图像，为CBMs的鲁棒性评估提供了工具。

Conclusion: SUB和TDG为概念模型的鲁棒性评估提供了新方法，推动了更可靠透明AI的发展。

Abstract: Concept Bottleneck Models (CBMs) and other concept-based interpretable models show great promise for making AI applications more transparent, which is essential in fields like medicine. Despite their success, we demonstrate that CBMs struggle to reliably identify the correct concepts under distribution shifts. To assess the robustness of CBMs to concept variations, we introduce SUB: a fine-grained image and concept benchmark containing 38,400 synthetic images based on the CUB dataset. To create SUB, we select a CUB subset of 33 bird classes and 45 concepts to generate images which substitute a specific concept, such as wing color or belly pattern. We introduce a novel Tied Diffusion Guidance (TDG) method to precisely control generated images, where noise sharing for two parallel denoising processes ensures that both the correct bird class and the correct attribute are generated. This novel benchmark enables rigorous evaluation of CBMs and similar interpretable models, contributing to the development of more robust methods. Our code is available at https://github.com/ExplainableML/sub and the dataset at http://huggingface.co/datasets/Jessica-bader/SUB.

</details>


### [28] [Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis](https://arxiv.org/abs/2507.23785)
*Bowen Zhang,Sicheng Xu,Chuxin Wang,Jiaolong Yang,Feng Zhao,Dong Chen,Baining Guo*

Main category: cs.CV

TL;DR: 提出了一种从单视频输入生成高质量动态3D内容的新框架，通过直接编码高斯点云及其时间变化，解决了4D扩散建模的挑战。


<details>
  <summary>Details</summary>
Motivation: 直接4D扩散建模因数据构建成本高和高维表示困难而极具挑战性，需要一种高效的方法来生成动态3D内容。

Method: 引入Direct 4DMesh-to-GS Variation Field VAE，直接编码高斯点云及其时间变化，并训练一个基于输入视频和高斯点云的时间感知扩散变换器模型。

Result: 在Objaverse数据集上训练的模型表现出优于现有方法的生成质量，并能泛化到真实视频输入。

Conclusion: 该方法为生成高质量动态3D内容提供了新途径，展示了在合成数据训练下的泛化能力。

Abstract: In this paper, we present a novel framework for video-to-4D generation that creates high-quality dynamic 3D content from single video inputs. Direct 4D diffusion modeling is extremely challenging due to costly data construction and the high-dimensional nature of jointly representing 3D shape, appearance, and motion. We address these challenges by introducing a Direct 4DMesh-to-GS Variation Field VAE that directly encodes canonical Gaussian Splats (GS) and their temporal variations from 3D animation data without per-instance fitting, and compresses high-dimensional animations into a compact latent space. Building upon this efficient representation, we train a Gaussian Variation Field diffusion model with temporal-aware Diffusion Transformer conditioned on input videos and canonical GS. Trained on carefully-curated animatable 3D objects from the Objaverse dataset, our model demonstrates superior generation quality compared to existing methods. It also exhibits remarkable generalization to in-the-wild video inputs despite being trained exclusively on synthetic data, paving the way for generating high-quality animated 3D content. Project page: https://gvfdiffusion.github.io/.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [29] [GSFusion:Globally Optimized LiDAR-Inertial-Visual Mapping for Gaussian Splatting](https://arxiv.org/abs/2507.23273)
*Jaeseok Park,Chanoh Park,Minsu Kim,Soohwan Kim*

Main category: cs.RO

TL;DR: GSFusion是一种在线LiDAR-惯性-视觉映射系统，通过全局位姿图优化中的surfel-to-surfel约束实现高精度地图一致性，解决了3D高斯泼溅（3DGS）与LiDAR结合时的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统基于相机传感器（包括RGB-D）的3DGS方法存在计算负载高、在纹理或光照差的环境中失效以及操作范围短等局限性。LiDAR虽为替代方案，但与3DGS结合时面临全局对齐精度和稀疏数据导致优化时间长的挑战。

Method: 提出GSFusion系统，采用surfel-to-surfel约束实现全局位姿图优化，并通过像素感知的高斯初始化策略和边界sigmoid约束处理稀疏数据。

Result: 在公开和自有数据集上的实验表明，GSFusion在渲染质量和地图构建效率上优于现有3DGS SLAM系统。

Conclusion: GSFusion通过创新的约束和初始化策略，有效解决了LiDAR与3DGS结合的挑战，提升了系统的性能和效率。

Abstract: While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic mapping, conventional approaches based on camera sensor, even RGB-D, suffer from fundamental limitations such as high computational load, failure in environments with poor texture or illumination, and short operational ranges. LiDAR emerges as a robust alternative, but its integration with 3DGS introduces new challenges, such as the need for exceptional global alignment for photorealistic quality and prolonged optimization times caused by sparse data. To address these challenges, we propose GSFusion, an online LiDAR-Inertial-Visual mapping system that ensures high-precision map consistency through a surfel-to-surfel constraint in the global pose-graph optimization. To handle sparse data, our system employs a pixel-aware Gaussian initialization strategy for efficient representation and a bounded sigmoid constraint to prevent uncontrolled Gaussian growth. Experiments on public and our datasets demonstrate our system outperforms existing 3DGS SLAM systems in terms of rendering quality and map-building efficiency.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [30] [Towards High-Resolution Alignment and Super-Resolution of Multi-Sensor Satellite Imagery](https://arxiv.org/abs/2507.23150)
*Philip Wootaek Shin,Vishal Gaur,Rahul Ramachandran,Manil Maskey,Jack Sampson,Vijaykrishnan Narayanan,Sujit Roy*

Main category: eess.IV

TL;DR: 该论文提出了一种初步框架，用于对齐和协调30米分辨率的HLS影像与10米分辨率的HLS影像，以改善超分辨率Landsat影像的质量。


<details>
  <summary>Details</summary>
Motivation: 高分辨率卫星影像对地理空间分析至关重要，但不同卫星传感器的空间分辨率差异给数据融合和下游应用带来挑战。现有超分辨率方法依赖人工降尺度图像，不适用于具有不同光谱和时间特性的异质卫星传感器。

Method: 开发了一个框架，以HLS10为参考，对齐和协调HLS30影像，旨在弥合传感器之间的分辨率差距。

Result: 定量和定性评估证明了该方法的有效性，显示了其在增强卫星遥感应用中的潜力。

Conclusion: 该研究为异质卫星影像超分辨率的可行性提供了见解，并指出了未来发展的关键考虑因素。

Abstract: High-resolution satellite imagery is essential for geospatial analysis, yet differences in spatial resolution across satellite sensors present challenges for data fusion and downstream applications. Super-resolution techniques can help bridge this gap, but existing methods rely on artificially downscaled images rather than real sensor data and are not well suited for heterogeneous satellite sensors with differing spectral, temporal characteristics. In this work, we develop a preliminary framework to align and Harmonized Landsat Sentinel 30m(HLS 30) imagery using Harmonized Landsat Sentinel 10m(HLS10) as a reference from the HLS dataset. Our approach aims to bridge the resolution gap between these sensors and improve the quality of super-resolved Landsat imagery. Quantitative and qualitative evaluations demonstrate the effectiveness of our method, showing its potential for enhancing satellite-based sensing applications. This study provides insights into the feasibility of heterogeneous satellite image super-resolution and highlights key considerations for future advancements in the field.

</details>


### [31] [LesionGen: A Concept-Guided Diffusion Model for Dermatology Image Synthesis](https://arxiv.org/abs/2507.23001)
*Jamil Fayyad,Nourhan Bayasi,Ziyang Yu,Homayoun Najjaran*

Main category: eess.IV

TL;DR: LesionGen是一个基于文本到图像扩散概率模型的皮肤病图像合成框架，通过高质量图像-描述对生成多样且真实的皮肤病图像，解决了数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 皮肤病分类需要大量多样且标注良好的数据集，但隐私问题、标注成本高和人口代表性不足限制了数据可用性。

Method: LesionGen利用专家标注和伪生成的概念丰富描述，微调预训练的扩散模型，生成基于皮肤病学描述的图像。

Result: 仅使用合成数据训练的模型分类准确率接近真实数据训练的模型，且在表现最差的子组中有显著提升。

Conclusion: LesionGen为皮肤病图像合成提供了有效解决方案，缓解了数据稀缺问题。

Abstract: Deep learning models for skin disease classification require large, diverse, and well-annotated datasets. However, such resources are often limited due to privacy concerns, high annotation costs, and insufficient demographic representation. While text-to-image diffusion probabilistic models (T2I-DPMs) offer promise for medical data synthesis, their use in dermatology remains underexplored, largely due to the scarcity of rich textual descriptions in existing skin image datasets. In this work, we introduce LesionGen, a clinically informed T2I-DPM framework for dermatology image synthesis. Unlike prior methods that rely on simplistic disease labels, LesionGen is trained on structured, concept-rich dermatological captions derived from expert annotations and pseudo-generated, concept-guided reports. By fine-tuning a pretrained diffusion model on these high-quality image-caption pairs, we enable the generation of realistic and diverse skin lesion images conditioned on meaningful dermatological descriptions. Our results demonstrate that models trained solely on our synthetic dataset achieve classification accuracy comparable to those trained on real images, with notable gains in worst-case subgroup performance. Code and data are available here.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [32] [Investigating the Invertibility of Multimodal Latent Spaces: Limitations of Optimization-Based Methods](https://arxiv.org/abs/2507.23010)
*Siwoo Park*

Main category: cs.LG

TL;DR: 论文探讨了多模态潜在空间在任务特定AI模型中的逆向能力及其广泛用途，发现其逆向映射在语义和感知上缺乏一致性。


<details>
  <summary>Details</summary>
Motivation: 研究多模态潜在空间在逆向任务中的表现，填补现有模型在逆向映射能力上的研究空白。

Method: 提出基于优化的框架，在文本-图像和文本-音频模态中双向推断输入特征。

Result: 实验表明，优化虽能实现目标对齐，但逆向映射的感知质量混乱且语义不可解释。

Conclusion: 多模态潜在空间缺乏稳健逆向映射的结构，需进一步研究开发语义丰富且可逆的潜在空间。

Abstract: This paper investigates the inverse capabilities and broader utility of multimodal latent spaces within task-specific AI (Artificial Intelligence) models. While these models excel at their designed forward tasks (e.g., text-to-image generation, audio-to-text transcription), their potential for inverse mappings remains largely unexplored. We propose an optimization-based framework to infer input characteristics from desired outputs, applying it bidirectionally across Text-Image (BLIP, Flux.1-dev) and Text-Audio (Whisper-Large-V3, Chatterbox-TTS) modalities.   Our central hypothesis posits that while optimization can guide models towards inverse tasks, their multimodal latent spaces will not consistently support semantically meaningful and perceptually coherent inverse mappings. Experimental results consistently validate this hypothesis. We demonstrate that while optimization can force models to produce outputs that align textually with targets (e.g., a text-to-image model generating an image that an image captioning model describes correctly, or an ASR model transcribing optimized audio accurately), the perceptual quality of these inversions is chaotic and incoherent. Furthermore, when attempting to infer the original semantic input from generative models, the reconstructed latent space embeddings frequently lack semantic interpretability, aligning with nonsensical vocabulary tokens.   These findings highlight a critical limitation. multimodal latent spaces, primarily optimized for specific forward tasks, do not inherently possess the structure required for robust and interpretable inverse mappings. Our work underscores the need for further research into developing truly semantically rich and invertible multimodal latent spaces.

</details>


### [33] [DepMicroDiff: Diffusion-Based Dependency-Aware Multimodal Imputation for Microbiome Data](https://arxiv.org/abs/2507.23676)
*Rabeya Tus Sadia,Qiang Cheng*

Main category: cs.LG

TL;DR: DepMicroDiff是一种结合扩散模型和依赖感知Transformer的新框架，用于微生物组数据插补，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 微生物组数据的稀疏性和噪声对准确插补构成挑战，现有方法未能充分捕捉微生物间的复杂依赖关系和上下文元数据。

Method: 结合扩散生成模型和依赖感知Transformer（DAT），利用VAE预训练和LLM编码的元数据进行条件化。

Result: 在TCGA数据集上表现优异，Pearson相关系数达0.712，余弦相似度达0.812，RMSE和MAE更低。

Conclusion: DepMicroDiff在微生物组数据插补中表现出鲁棒性和泛化能力。

Abstract: Microbiome data analysis is essential for understanding host health and disease, yet its inherent sparsity and noise pose major challenges for accurate imputation, hindering downstream tasks such as biomarker discovery. Existing imputation methods, including recent diffusion-based models, often fail to capture the complex interdependencies between microbial taxa and overlook contextual metadata that can inform imputation. We introduce DepMicroDiff, a novel framework that combines diffusion-based generative modeling with a Dependency-Aware Transformer (DAT) to explicitly capture both mutual pairwise dependencies and autoregressive relationships. DepMicroDiff is further enhanced by VAE-based pretraining across diverse cancer datasets and conditioning on patient metadata encoded via a large language model (LLM). Experiments on TCGA microbiome datasets show that DepMicroDiff substantially outperforms state-of-the-art baselines, achieving higher Pearson correlation (up to 0.712), cosine similarity (up to 0.812), and lower RMSE and MAE across multiple cancer types, demonstrating its robustness and generalizability for microbiome imputation.

</details>
