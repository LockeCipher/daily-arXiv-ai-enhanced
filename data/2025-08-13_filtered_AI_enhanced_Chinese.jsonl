{"id": "2508.08384", "pdf": "https://arxiv.org/pdf/2508.08384", "abs": "https://arxiv.org/abs/2508.08384", "authors": ["Mutian Tong", "Rundi Wu", "Changxi Zheng"], "title": "Spatiotemporally Consistent Indoor Lighting Estimation with Diffusion Priors", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "11 pages. Accepted by SIGGRAPH 2025 as Conference Paper", "summary": "Indoor lighting estimation from a single image or video remains a challenge due to its highly ill-posed nature, especially when the lighting condition of the scene varies spatially and temporally. We propose a method that estimates from an input video a continuous light field describing the spatiotemporally varying lighting of the scene. We leverage 2D diffusion priors for optimizing such light field represented as a MLP. To enable zero-shot generalization to in-the-wild scenes, we fine-tune a pre-trained image diffusion model to predict lighting at multiple locations by jointly inpainting multiple chrome balls as light probes. We evaluate our method on indoor lighting estimation from a single image or video and show superior performance over compared baselines. Most importantly, we highlight results on spatiotemporally consistent lighting estimation from in-the-wild videos, which is rarely demonstrated in previous works.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ece\u89c6\u9891\u4e2d\u4f30\u8ba1\u8fde\u7eed\u5149\u573a\u7684\u65b9\u6cd5\uff0c\u5229\u75282D\u6269\u6563\u5148\u9a8c\u4f18\u5316MLP\u8868\u793a\u7684\u5149\u573a\uff0c\u5e76\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\u3002", "motivation": "\u5ba4\u5185\u5149\u7167\u4f30\u8ba1\u56e0\u5176\u9ad8\u5ea6\u4e0d\u9002\u5b9a\u6027\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5f53\u573a\u666f\u5149\u7167\u6761\u4ef6\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0a\u53d8\u5316\u65f6\u3002", "method": "\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u7684\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0c\u8054\u5408\u4fee\u590d\u591a\u4e2a\u94ec\u7403\u4f5c\u4e3a\u5149\u63a2\u9488\uff0c\u9884\u6d4b\u591a\u4f4d\u7f6e\u7684\u5149\u7167\u3002", "result": "\u5728\u5355\u56fe\u50cf\u6216\u89c6\u9891\u7684\u5ba4\u5185\u5149\u7167\u4f30\u8ba1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u771f\u5b9e\u89c6\u9891\u4e2d\u5b9e\u73b0\u4e86\u65f6\u7a7a\u4e00\u81f4\u7684\u5149\u7167\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u4ee5\u5f80\u5de5\u4f5c\u4e2d\u96be\u4ee5\u5b9e\u73b0\u7684\u65f6\u7a7a\u4e00\u81f4\u5149\u7167\u4f30\u8ba1\u95ee\u9898\u3002"}}
{"id": "2508.08542", "pdf": "https://arxiv.org/pdf/2508.08542", "abs": "https://arxiv.org/abs/2508.08542", "authors": ["Dasith de Silva Edirimuni", "Xuequan Lu", "Ajmal Saeed Mian", "Lei Wei", "Gang Li", "Scott Schaefer", "Ying He"], "title": "Hybrid Long and Short Range Flows for Point Cloud Filtering", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Point cloud capture processes are error-prone and introduce noisy artifacts that necessitate filtering/denoising. Recent filtering methods often suffer from point clustering or noise retaining issues. In this paper, we propose Hybrid Point Cloud Filtering ($\\textbf{HybridPF}$) that considers both short-range and long-range filtering trajectories when removing noise. It is well established that short range scores, given by $\\nabla_{x}\\log p(x_t)$, may provide the necessary displacements to move noisy points to the underlying clean surface. By contrast, long range velocity flows approximate constant displacements directed from a high noise variant patch $x_0$ towards the corresponding clean surface $x_1$. Here, noisy patches $x_t$ are viewed as intermediate states between the high noise variant and the clean patches. Our intuition is that long range information from velocity flow models can guide the short range scores to align more closely with the clean points. In turn, score models generally provide a quicker convergence to the clean surface. Specifically, we devise two parallel modules, the ShortModule and LongModule, each consisting of an Encoder-Decoder pair to respectively account for short-range scores and long-range flows. We find that short-range scores, guided by long-range features, yield filtered point clouds with good point distributions and convergence near the clean surface. We design a joint loss function to simultaneously train the ShortModule and LongModule, in an end-to-end manner. Finally, we identify a key weakness in current displacement based methods, limitations on the decoder architecture, and propose a dynamic graph convolutional decoder to improve the inference process. Comprehensive experiments demonstrate that our HybridPF achieves state-of-the-art results while enabling faster inference speed.", "AI": {"tldr": "HybridPF\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u77ed\u7a0b\u548c\u957f\u7a0b\u6ee4\u6ce2\u8f68\u8ff9\u7684\u70b9\u4e91\u53bb\u566a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e76\u884c\u6a21\u5757\u548c\u52a8\u6001\u56fe\u5377\u79ef\u89e3\u7801\u5668\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6ee4\u6ce2\u65b9\u6cd5\u5b58\u5728\u70b9\u805a\u7c7b\u6216\u566a\u58f0\u6b8b\u7559\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u53bb\u566a\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1ShortModule\u548cLongModule\u5e76\u884c\u6a21\u5757\uff0c\u7ed3\u5408\u77ed\u7a0b\u5206\u6570\u548c\u957f\u7a0b\u6d41\u4fe1\u606f\uff0c\u5e76\u91c7\u7528\u52a8\u6001\u56fe\u5377\u79ef\u89e3\u7801\u5668\u3002", "result": "HybridPF\u5728\u53bb\u566a\u6548\u679c\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u8fbe\u5230\u6700\u4f18\u3002", "conclusion": "HybridPF\u901a\u8fc7\u7ed3\u5408\u77ed\u7a0b\u548c\u957f\u7a0b\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u70b9\u4e91\u53bb\u566a\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2508.08826", "pdf": "https://arxiv.org/pdf/2508.08826", "abs": "https://arxiv.org/abs/2508.08826", "authors": ["Meng Gai", "Guoping Wang", "Sheng Li"], "title": "Geometry-Aware Global Feature Aggregation for Real-Time Indirect Illumination", "categories": ["cs.GR", "cs.AI"], "comment": "10 pages", "summary": "Real-time rendering with global illumination is crucial to afford the user realistic experience in virtual environments. We present a learning-based estimator to predict diffuse indirect illumination in screen space, which then is combined with direct illumination to synthesize globally-illuminated high dynamic range (HDR) results. Our approach tackles the challenges of capturing long-range/long-distance indirect illumination when employing neural networks and is generalized to handle complex lighting and scenarios.   From the neural network thinking of the solver to the rendering equation, we present a novel network architecture to predict indirect illumination. Our network is equipped with a modified attention mechanism that aggregates global information guided by spacial geometry features, as well as a monochromatic design that encodes each color channel individually.   We conducted extensive evaluations, and the experimental results demonstrate our superiority over previous learning-based techniques. Our approach excels at handling complex lighting such as varying-colored lighting and environment lighting. It can successfully capture distant indirect illumination and simulates the interreflections between textured surfaces well (i.e., color bleeding effects); it can also effectively handle new scenes that are not present in the training dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u5c4f\u5e55\u7a7a\u95f4\u6f2b\u53cd\u5c04\u95f4\u63a5\u5149\u7167\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7ed3\u5408\u76f4\u63a5\u5149\u7167\u751f\u6210\u5168\u5c40\u5149\u7167\u7684\u9ad8\u52a8\u6001\u8303\u56f4\u7ed3\u679c\u3002", "motivation": "\u5b9e\u65f6\u6e32\u67d3\u4e2d\u5168\u5c40\u5149\u7167\u5bf9\u7528\u6237\u4f53\u9a8c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u957f\u8ddd\u79bb\u95f4\u63a5\u5149\u7167\u3002", "method": "\u91c7\u7528\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7ed3\u5408\u6539\u8fdb\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u5355\u8272\u8bbe\u8ba1\uff0c\u9884\u6d4b\u95f4\u63a5\u5149\u7167\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u5b66\u4e60\u6280\u672f\uff0c\u80fd\u6709\u6548\u5904\u7406\u590d\u6742\u5149\u7167\u548c\u65b0\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5168\u5c40\u5149\u7167\u6e32\u67d3\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2508.08831", "pdf": "https://arxiv.org/pdf/2508.08831", "abs": "https://arxiv.org/abs/2508.08831", "authors": ["Bo-Hsun Chen", "Nevindu M. Batagoda", "Dan Negrut"], "title": "DiffPhysCam: Differentiable Physics-Based Camera Simulation for Inverse Rendering and Embodied AI", "categories": ["cs.GR", "cs.CV", "cs.RO"], "comment": "19 pages, 17 figures, and 4 tables", "summary": "We introduce DiffPhysCam, a differentiable camera simulator designed to support robotics and embodied AI applications by enabling gradient-based optimization in visual perception pipelines. Generating synthetic images that closely mimic those from real cameras is essential for training visual models and enabling end-to-end visuomotor learning. Moreover, differentiable rendering allows inverse reconstruction of real-world scenes as digital twins, facilitating simulation-based robotics training. However, existing virtual cameras offer limited control over intrinsic settings, poorly capture optical artifacts, and lack tunable calibration parameters -- hindering sim-to-real transfer. DiffPhysCam addresses these limitations through a multi-stage pipeline that provides fine-grained control over camera settings, models key optical effects such as defocus blur, and supports calibration with real-world data. It enables both forward rendering for image synthesis and inverse rendering for 3D scene reconstruction, including mesh and material texture optimization. We show that DiffPhysCam enhances robotic perception performance in synthetic image tasks. As an illustrative example, we create a digital twin of a real-world scene using inverse rendering, simulate it in a multi-physics environment, and demonstrate navigation of an autonomous ground vehicle using images generated by DiffPhysCam.", "AI": {"tldr": "DiffPhysCam\u662f\u4e00\u79cd\u53ef\u5fae\u5206\u76f8\u673a\u6a21\u62df\u5668\uff0c\u652f\u6301\u673a\u5668\u4eba\u5b66\u548c\u5177\u8eabAI\u5e94\u7528\uff0c\u901a\u8fc7\u68af\u5ea6\u4f18\u5316\u63d0\u5347\u89c6\u89c9\u611f\u77e5\u7ba1\u9053\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u865a\u62df\u76f8\u673a\u5728\u63a7\u5236\u76f8\u673a\u8bbe\u7f6e\u3001\u6a21\u62df\u5149\u5b66\u4f2a\u5f71\u548c\u53ef\u8c03\u6821\u51c6\u53c2\u6570\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u4ece\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u3002DiffPhysCam\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "DiffPhysCam\u91c7\u7528\u591a\u9636\u6bb5\u7ba1\u9053\uff0c\u63d0\u4f9b\u5bf9\u76f8\u673a\u8bbe\u7f6e\u7684\u7cbe\u7ec6\u63a7\u5236\uff0c\u6a21\u62df\u5173\u952e\u5149\u5b66\u6548\u679c\uff08\u5982\u6563\u7126\u6a21\u7cca\uff09\uff0c\u5e76\u652f\u6301\u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u7684\u6821\u51c6\u3002", "result": "DiffPhysCam\u5728\u5408\u6210\u56fe\u50cf\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u673a\u5668\u4eba\u611f\u77e5\u6027\u80fd\uff0c\u5e76\u6210\u529f\u7528\u4e8e\u9006\u5411\u6e32\u67d3\u521b\u5efa\u6570\u5b57\u5b6a\u751f\u573a\u666f\u3002", "conclusion": "DiffPhysCam\u901a\u8fc7\u53ef\u5fae\u5206\u6e32\u67d3\u548c\u7cbe\u7ec6\u63a7\u5236\uff0c\u4e3a\u673a\u5668\u4eba\u89c6\u89c9\u548c\u5177\u8eabAI\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2508.08487", "pdf": "https://arxiv.org/pdf/2508.08487", "abs": "https://arxiv.org/abs/2508.08487", "authors": ["Qian Wang", "Ziqi Huang", "Ruoxi Jia", "Paul Debevec", "Ning Yu"], "title": "MAViS: A Multi-Agent Framework for Long-Sequence Video Storytelling", "categories": ["cs.CV", "cs.AI", "cs.MA"], "comment": "Video Generation Agent", "summary": "Despite recent advances, long-sequence video generation frameworks still suffer from significant limitations: poor assistive capability, suboptimal visual quality, and limited expressiveness. To mitigate these limitations, we propose MAViS, an end-to-end multi-agent collaborative framework for long-sequence video storytelling. MAViS orchestrates specialized agents across multiple stages, including script writing, shot designing, character modeling, keyframe generation, video animation, and audio generation. In each stage, agents operate under the 3E Principle -- Explore, Examine, and Enhance -- to ensure the completeness of intermediate outputs. Considering the capability limitations of current generative models, we propose the Script Writing Guidelines to optimize compatibility between scripts and generative tools. Experimental results demonstrate that MAViS achieves state-of-the-art performance in assistive capability, visual quality, and video expressiveness. Its modular framework further enables scalability with diverse generative models and tools. With just a brief user prompt, MAViS is capable of producing high-quality, expressive long-sequence video storytelling, enriching inspirations and creativity for users. To the best of our knowledge, MAViS is the only framework that provides multimodal design output -- videos with narratives and background music.", "AI": {"tldr": "MAViS\u662f\u4e00\u4e2a\u591a\u4ee3\u7406\u534f\u4f5c\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u957f\u5e8f\u5217\u89c6\u9891\u6545\u4e8b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6846\u67b6\u5728\u8f85\u52a9\u80fd\u529b\u3001\u89c6\u89c9\u8d28\u91cf\u548c\u8868\u73b0\u529b\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u957f\u5e8f\u5217\u89c6\u9891\u751f\u6210\u6846\u67b6\u5b58\u5728\u8f85\u52a9\u80fd\u529b\u5dee\u3001\u89c6\u89c9\u8d28\u91cf\u4e0d\u4f73\u548c\u8868\u73b0\u529b\u6709\u9650\u7684\u95ee\u9898\uff0cMAViS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "MAViS\u901a\u8fc7\u591a\u9636\u6bb5\u4ee3\u7406\u534f\u4f5c\uff08\u5982\u811a\u672c\u7f16\u5199\u3001\u955c\u5934\u8bbe\u8ba1\u3001\u89d2\u8272\u5efa\u6a21\u7b49\uff09\uff0c\u9075\u5faa3E\u539f\u5219\uff08\u63a2\u7d22\u3001\u68c0\u67e5\u3001\u589e\u5f3a\uff09\uff0c\u5e76\u4f18\u5316\u811a\u672c\u4e0e\u751f\u6210\u5de5\u5177\u7684\u517c\u5bb9\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMAViS\u5728\u8f85\u52a9\u80fd\u529b\u3001\u89c6\u89c9\u8d28\u91cf\u548c\u8868\u73b0\u529b\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u80fd\u751f\u6210\u591a\u6a21\u6001\u8bbe\u8ba1\u8f93\u51fa\uff08\u89c6\u9891\u3001\u53d9\u4e8b\u548c\u80cc\u666f\u97f3\u4e50\uff09\u3002", "conclusion": "MAViS\u662f\u9996\u4e2a\u63d0\u4f9b\u591a\u6a21\u6001\u8bbe\u8ba1\u8f93\u51fa\u7684\u6846\u67b6\uff0c\u4ec5\u9700\u7b80\u77ed\u7528\u6237\u63d0\u793a\u5373\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u957f\u5e8f\u5217\u89c6\u9891\u6545\u4e8b\u3002"}}
{"id": "2508.08498", "pdf": "https://arxiv.org/pdf/2508.08498", "abs": "https://arxiv.org/abs/2508.08498", "authors": ["Aneel Damaraju", "Dean Hazineh", "Todd Zickler"], "title": "CObL: Toward Zero-Shot Ordinal Layering without User Prompting", "categories": ["cs.CV"], "comment": "ICCV 2025: Project page with demo, datasets, and code:   https://vision.seas.harvard.edu/cobl/", "summary": "Vision benefits from grouping pixels into objects and understanding their spatial relationships, both laterally and in depth. We capture this with a scene representation comprising an occlusion-ordered stack of \"object layers,\" each containing an isolated and amodally-completed object. To infer this representation from an image, we introduce a diffusion-based architecture named Concurrent Object Layers (CObL). CObL generates a stack of object layers in parallel, using Stable Diffusion as a prior for natural objects and inference-time guidance to ensure the inferred layers composite back to the input image. We train CObL using a few thousand synthetically-generated images of multi-object tabletop scenes, and we find that it zero-shot generalizes to photographs of real-world tabletops with varying numbers of novel objects. In contrast to recent models for amodal object completion, CObL reconstructs multiple occluded objects without user prompting and without knowing the number of objects beforehand. Unlike previous models for unsupervised object-centric representation learning, CObL is not limited to the world it was trained in.", "AI": {"tldr": "CObL\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u67b6\u6784\uff0c\u901a\u8fc7\u5e76\u884c\u751f\u6210\u5bf9\u8c61\u5c42\u5806\u6808\uff0c\u5229\u7528Stable Diffusion\u4f5c\u4e3a\u5148\u9a8c\uff0c\u65e0\u9700\u7528\u6237\u63d0\u793a\u5373\u53ef\u91cd\u5efa\u591a\u4e2a\u906e\u6321\u5bf9\u8c61\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u771f\u5b9e\u4e16\u754c\u573a\u666f\u3002", "motivation": "\u89c6\u89c9\u4efb\u52a1\u9700\u8981\u5c06\u50cf\u7d20\u5206\u7ec4\u4e3a\u5bf9\u8c61\u5e76\u7406\u89e3\u5176\u7a7a\u95f4\u5173\u7cfb\uff08\u5305\u62ec\u906e\u6321\u5173\u7cfb\uff09\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u65e0\u9700\u7528\u6237\u63d0\u793a\u6216\u9884\u5148\u77e5\u9053\u5bf9\u8c61\u6570\u91cf\u7684\u60c5\u51b5\u4e0b\u96be\u4ee5\u5b8c\u6210\u591a\u5bf9\u8c61\u91cd\u5efa\uff0c\u4e14\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51faCObL\u67b6\u6784\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u5e76\u884c\u751f\u6210\u5bf9\u8c61\u5c42\u5806\u6808\uff0c\u901a\u8fc7Stable Diffusion\u4f5c\u4e3a\u5148\u9a8c\uff0c\u5e76\u4f7f\u7528\u63a8\u7406\u65f6\u6307\u5bfc\u786e\u4fdd\u5c42\u5806\u6808\u80fd\u5408\u6210\u56de\u8f93\u5165\u56fe\u50cf\u3002\u8bad\u7ec3\u6570\u636e\u4e3a\u5408\u6210\u7684\u591a\u5bf9\u8c61\u684c\u9762\u573a\u666f\u56fe\u50cf\u3002", "result": "CObL\u65e0\u9700\u7528\u6237\u63d0\u793a\u5373\u53ef\u91cd\u5efa\u591a\u4e2a\u906e\u6321\u5bf9\u8c61\uff0c\u5e76\u80fd\u96f6\u6837\u672c\u6cdb\u5316\u5230\u771f\u5b9e\u4e16\u754c\u684c\u9762\u573a\u666f\u7684\u65b0\u5bf9\u8c61\u3002", "conclusion": "CObL\u5728\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u548c\u7528\u6237\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u591a\u5bf9\u8c61\u91cd\u5efa\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.09131", "pdf": "https://arxiv.org/pdf/2508.09131", "abs": "https://arxiv.org/abs/2508.09131", "authors": ["Zixin Yin", "Xili Dai", "Ling-Hao Chen", "Deyu Zhou", "Jianan Wang", "Duomin Wang", "Gang Yu", "Lionel M. Ni", "Heung-Yeung Shum"], "title": "Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Text-guided color editing in images and videos is a fundamental yet unsolved problem, requiring fine-grained manipulation of color attributes, including albedo, light source color, and ambient lighting, while preserving physical consistency in geometry, material properties, and light-matter interactions. Existing training-free methods offer broad applicability across editing tasks but struggle with precise color control and often introduce visual inconsistency in both edited and non-edited regions. In this work, we present ColorCtrl, a training-free color editing method that leverages the attention mechanisms of modern Multi-Modal Diffusion Transformers (MM-DiT). By disentangling structure and color through targeted manipulation of attention maps and value tokens, our method enables accurate and consistent color editing, along with word-level control of attribute intensity. Our method modifies only the intended regions specified by the prompt, leaving unrelated areas untouched. Extensive experiments on both SD3 and FLUX.1-dev demonstrate that ColorCtrl outperforms existing training-free approaches and achieves state-of-the-art performances in both edit quality and consistency. Furthermore, our method surpasses strong commercial models such as FLUX.1 Kontext Max and GPT-4o Image Generation in terms of consistency. When extended to video models like CogVideoX, our approach exhibits greater advantages, particularly in maintaining temporal coherence and editing stability. Finally, our method also generalizes to instruction-based editing diffusion models such as Step1X-Edit and FLUX.1 Kontext dev, further demonstrating its versatility.", "AI": {"tldr": "ColorCtrl\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6587\u672c\u5f15\u5bfc\u989c\u8272\u7f16\u8f91\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u7cbe\u786e\u4e14\u4e00\u81f4\u7684\u989c\u8272\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7cbe\u786e\u989c\u8272\u63a7\u5236\u548c\u89c6\u89c9\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u89e3\u8026\u7ed3\u6784\u548c\u989c\u8272\uff0c\u5e76\u9488\u5bf9\u6027\u64cd\u4f5c\u6ce8\u610f\u529b\u56fe\u548c\u503c\u6807\u8bb0\uff0c\u5b9e\u73b0\u7cbe\u786e\u7f16\u8f91\u3002", "result": "\u5728SD3\u548cFLUX.1-dev\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u89c6\u9891\u6a21\u578b\u4e2d\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "conclusion": "ColorCtrl\u5728\u7f16\u8f91\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u9002\u7528\u4e8e\u591a\u79cd\u6269\u6563\u6a21\u578b\u3002"}}
{"id": "2508.08518", "pdf": "https://arxiv.org/pdf/2508.08518", "abs": "https://arxiv.org/abs/2508.08518", "authors": ["Ilerioluwakiiye Abolade", "Emmanuel Idoko", "Solomon Odelola", "Promise Omoigui", "Adetola Adebanwo", "Aondana Iorumbur", "Udunna Anazodo", "Alessandro Crimi", "Raymond Confidence"], "title": "SharpXR: Structure-Aware Denoising for Pediatric Chest X-Rays", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted at MICCAI 2025 MIRASOL Workshop, 10 pages, 5 figures", "summary": "Pediatric chest X-ray imaging is essential for early diagnosis, particularly in low-resource settings where advanced imaging modalities are often inaccessible. Low-dose protocols reduce radiation exposure in children but introduce substantial noise that can obscure critical anatomical details. Conventional denoising methods often degrade fine details, compromising diagnostic accuracy. In this paper, we present SharpXR, a structure-aware dual-decoder U-Net designed to denoise low-dose pediatric X-rays while preserving diagnostically relevant features. SharpXR combines a Laplacian-guided edge-preserving decoder with a learnable fusion module that adaptively balances noise suppression and structural detail retention. To address the scarcity of paired training data, we simulate realistic Poisson-Gaussian noise on the Pediatric Pneumonia Chest X-ray dataset. SharpXR outperforms state-of-the-art baselines across all evaluation metrics while maintaining computational efficiency suitable for resource-constrained settings. SharpXR-denoised images improved downstream pneumonia classification accuracy from 88.8% to 92.5%, underscoring its diagnostic value in low-resource pediatric care.", "AI": {"tldr": "SharpXR\u662f\u4e00\u79cd\u7ed3\u6784\u611f\u77e5\u7684\u53cc\u89e3\u7801\u5668U-Net\uff0c\u7528\u4e8e\u53bb\u566a\u4f4e\u5242\u91cf\u513f\u79d1X\u5c04\u7ebf\uff0c\u540c\u65f6\u4fdd\u7559\u8bca\u65ad\u76f8\u5173\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u80ba\u708e\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u5728\u8d44\u6e90\u532e\u4e4f\u5730\u533a\uff0c\u4f4e\u5242\u91cf\u513f\u79d1X\u5c04\u7ebf\u6210\u50cf\u56e0\u566a\u58f0\u95ee\u9898\u5f71\u54cd\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u4f20\u7edf\u53bb\u566a\u65b9\u6cd5\u4f1a\u7834\u574f\u7ec6\u8282\u3002", "method": "SharpXR\u7ed3\u5408\u62c9\u666e\u62c9\u65af\u5f15\u5bfc\u7684\u8fb9\u7f18\u4fdd\u7559\u89e3\u7801\u5668\u548c\u53ef\u5b66\u4e60\u878d\u5408\u6a21\u5757\uff0c\u81ea\u9002\u5e94\u5e73\u8861\u566a\u58f0\u6291\u5236\u4e0e\u7ed3\u6784\u4fdd\u7559\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u566a\u58f0\u751f\u6210\u8bad\u7ec3\u6570\u636e\u3002", "result": "SharpXR\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80ba\u708e\u5206\u7c7b\u51c6\u786e\u7387\u4ece88.8%\u63d0\u5347\u81f392.5%\u3002", "conclusion": "SharpXR\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u8bca\u65ad\u4ef7\u503c\uff0c\u9002\u5408\u513f\u79d1\u4f4e\u5242\u91cfX\u5c04\u7ebf\u53bb\u566a\u3002"}}
{"id": "2508.08556", "pdf": "https://arxiv.org/pdf/2508.08556", "abs": "https://arxiv.org/abs/2508.08556", "authors": ["Yunqi Miao", "Zhiyu Qu", "Mingqi Gao", "Changrui Chen", "Jifei Song", "Jungong Han", "Jiankang Deng"], "title": "Unlocking the Potential of Diffusion Priors in Blind Face Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Although diffusion prior is rising as a powerful solution for blind face restoration (BFR), the inherent gap between the vanilla diffusion model and BFR settings hinders its seamless adaptation. The gap mainly stems from the discrepancy between 1) high-quality (HQ) and low-quality (LQ) images and 2) synthesized and real-world images. The vanilla diffusion model is trained on images with no or less degradations, whereas BFR handles moderately to severely degraded images. Additionally, LQ images used for training are synthesized by a naive degradation model with limited degradation patterns, which fails to simulate complex and unknown degradations in real-world scenarios. In this work, we use a unified network FLIPNET that switches between two modes to resolve specific gaps. In Restoration mode, the model gradually integrates BFR-oriented features and face embeddings from LQ images to achieve authentic and faithful face restoration. In Degradation mode, the model synthesizes real-world like degraded images based on the knowledge learned from real-world degradation datasets. Extensive evaluations on benchmark datasets show that our model 1) outperforms previous diffusion prior based BFR methods in terms of authenticity and fidelity, and 2) outperforms the naive degradation model in modeling the real-world degradations.", "AI": {"tldr": "FLIPNET\u901a\u8fc7\u5207\u6362\u6062\u590d\u548c\u9000\u5316\u6a21\u5f0f\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u76f2\u4eba\u8138\u6062\u590d\u4e2d\u7684\u9002\u5e94\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u771f\u5b9e\u6027\u548c\u4fdd\u771f\u5ea6\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u76f2\u4eba\u8138\u6062\u590d\u4e2d\u5b58\u5728\u9002\u5e94\u6027\u95ee\u9898\uff0c\u4e3b\u8981\u6e90\u4e8e\u9ad8\u8d28\u91cf\u4e0e\u4f4e\u8d28\u91cf\u56fe\u50cf\u3001\u5408\u6210\u4e0e\u771f\u5b9e\u56fe\u50cf\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "method": "\u4f7f\u7528FLIPNET\u7f51\u7edc\uff0c\u5207\u6362\u6062\u590d\u6a21\u5f0f\u548c\u9000\u5316\u6a21\u5f0f\uff0c\u5206\u522b\u5904\u7406\u56fe\u50cf\u6062\u590d\u548c\u6a21\u62df\u771f\u5b9e\u9000\u5316\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u6269\u6563\u6a21\u578b\u548c\u9000\u5316\u6a21\u578b\u3002", "conclusion": "FLIPNET\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u76f2\u4eba\u8138\u6062\u590d\u4e2d\u7684\u9002\u5e94\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2508.08588", "pdf": "https://arxiv.org/pdf/2508.08588", "abs": "https://arxiv.org/abs/2508.08588", "authors": ["Jingyun Liang", "Jingkai Zhou", "Shikai Li", "Chenjie Cao", "Lei Sun", "Yichen Qian", "Weihua Chen", "Fan Wang"], "title": "RealisMotion: Decomposed Human Motion Control and Video Generation in the World Space", "categories": ["cs.CV", "eess.IV"], "comment": "Project page: https://jingyunliang.github.io/RealisMotion", "summary": "Generating human videos with realistic and controllable motions is a challenging task. While existing methods can generate visually compelling videos, they lack separate control over four key video elements: foreground subject, background video, human trajectory and action patterns. In this paper, we propose a decomposed human motion control and video generation framework that explicitly decouples motion from appearance, subject from background, and action from trajectory, enabling flexible mix-and-match composition of these elements. Concretely, we first build a ground-aware 3D world coordinate system and perform motion editing directly in the 3D space. Trajectory control is implemented by unprojecting edited 2D trajectories into 3D with focal-length calibration and coordinate transformation, followed by speed alignment and orientation adjustment; actions are supplied by a motion bank or generated via text-to-motion methods. Then, based on modern text-to-video diffusion transformer models, we inject the subject as tokens for full attention, concatenate the background along the channel dimension, and add motion (trajectory and action) control signals by addition. Such a design opens up the possibility for us to generate realistic videos of anyone doing anything anywhere. Extensive experiments on benchmark datasets and real-world cases demonstrate that our method achieves state-of-the-art performance on both element-wise controllability and overall video quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u89e3\u7684\u4eba\u4f53\u8fd0\u52a8\u63a7\u5236\u548c\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u8fd0\u52a8\u4e0e\u5916\u89c2\u3001\u4e3b\u4f53\u4e0e\u80cc\u666f\u3001\u52a8\u4f5c\u4e0e\u8f68\u8ff9\uff0c\u5b9e\u73b0\u7075\u6d3b\u7ec4\u5408\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u72ec\u7acb\u63a7\u5236\u89c6\u9891\u7684\u56db\u4e2a\u5173\u952e\u5143\u7d20\uff08\u524d\u666f\u4e3b\u4f53\u3001\u80cc\u666f\u89c6\u9891\u3001\u4eba\u4f53\u8f68\u8ff9\u548c\u52a8\u4f5c\u6a21\u5f0f\uff09\uff0c\u9650\u5236\u4e86\u89c6\u9891\u751f\u6210\u7684\u7075\u6d3b\u6027\u3002", "method": "\u6784\u5efa\u5730\u9762\u611f\u77e5\u76843D\u4e16\u754c\u5750\u6807\u7cfb\uff0c\u57283D\u7a7a\u95f4\u4e2d\u8fdb\u884c\u8fd0\u52a8\u7f16\u8f91\uff1b\u901a\u8fc72D\u8f68\u8ff9\u53cd\u6295\u5f71\u52303D\u5b9e\u73b0\u8f68\u8ff9\u63a7\u5236\uff0c\u52a8\u4f5c\u7531\u8fd0\u52a8\u5e93\u6216\u6587\u672c\u751f\u6210\uff1b\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u6a21\u578b\uff0c\u6ce8\u5165\u4e3b\u4f53\u3001\u80cc\u666f\u548c\u8fd0\u52a8\u63a7\u5236\u4fe1\u53f7\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u548c\u5b9e\u9645\u6848\u4f8b\u4e2d\uff0c\u65b9\u6cd5\u5728\u5143\u7d20\u53ef\u63a7\u6027\u548c\u89c6\u9891\u8d28\u91cf\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5bf9\u89c6\u9891\u5143\u7d20\u7684\u7075\u6d3b\u63a7\u5236\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.08601", "pdf": "https://arxiv.org/pdf/2508.08601", "abs": "https://arxiv.org/abs/2508.08601", "authors": ["Yan Team"], "title": "Yan: Foundational Interactive Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present Yan, a foundational framework for interactive video generation, covering the entire pipeline from simulation and generation to editing. Specifically, Yan comprises three core modules. AAA-level Simulation: We design a highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based shift-window denoising inference process, achieving real-time 1080P/60FPS interactive simulation. Multi-Modal Generation: We introduce a hierarchical autoregressive caption method that injects game-specific knowledge into open-domain multi-modal video diffusion models (VDMs), then transforming the VDM into a frame-wise, action-controllable, real-time infinite interactive video generator. Notably, when the textual and visual prompts are sourced from different domains, the model demonstrates strong generalization, allowing it to blend and compose the style and mechanics across domains flexibly according to user prompts. Multi-Granularity Editing: We propose a hybrid model that explicitly disentangles interactive mechanics simulation from visual rendering, enabling multi-granularity video content editing during interaction through text. Collectively, Yan offers an integration of these modules, pushing interactive video generation beyond isolated capabilities toward a comprehensive AI-driven interactive creation paradigm, paving the way for the next generation of creative tools, media, and entertainment. The project page is: https://greatx3.github.io/Yan/.", "AI": {"tldr": "Yan\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u89c6\u9891\u751f\u6210\u7684\u57fa\u7840\u6846\u67b6\uff0c\u6db5\u76d6\u4ece\u6a21\u62df\u3001\u751f\u6210\u5230\u7f16\u8f91\u7684\u5b8c\u6574\u6d41\u7a0b\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1aAAA\u7ea7\u6a21\u62df\u3001\u591a\u6a21\u6001\u751f\u6210\u548c\u591a\u7c92\u5ea6\u7f16\u8f91\u3002", "motivation": "\u63a8\u52a8\u4ea4\u4e92\u5f0f\u89c6\u9891\u751f\u6210\u4ece\u5b64\u7acb\u529f\u80fd\u5411\u5168\u9762\u7684AI\u9a71\u52a8\u521b\u4f5c\u8303\u5f0f\u53d1\u5c55\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u521b\u610f\u5de5\u5177\u3001\u5a92\u4f53\u548c\u5a31\u4e50\u94fa\u5e73\u9053\u8def\u3002", "method": "1. AAA\u7ea7\u6a21\u62df\uff1a\u91c7\u7528\u9ad8\u538b\u7f29\u3001\u4f4e\u5ef6\u8fdf\u76843D-VAE\u548cKV\u7f13\u5b58\u7a97\u53e3\u53bb\u566a\u63a8\u7406\uff1b2. \u591a\u6a21\u6001\u751f\u6210\uff1a\u7ed3\u5408\u5206\u5c42\u81ea\u56de\u5f52\u6807\u6ce8\u548c\u5f00\u653e\u57df\u89c6\u9891\u6269\u6563\u6a21\u578b\uff1b3. \u591a\u7c92\u5ea6\u7f16\u8f91\uff1a\u901a\u8fc7\u6df7\u5408\u6a21\u578b\u89e3\u8026\u4ea4\u4e92\u673a\u5236\u4e0e\u89c6\u89c9\u6e32\u67d3\u3002", "result": "\u5b9e\u73b0\u5b9e\u65f61080P/60FPS\u4ea4\u4e92\u6a21\u62df\uff0c\u652f\u6301\u8de8\u57df\u98ce\u683c\u548c\u673a\u5236\u7075\u6d3b\u7ec4\u5408\uff0c\u5e76\u80fd\u901a\u8fc7\u6587\u672c\u8fdb\u884c\u591a\u7c92\u5ea6\u7f16\u8f91\u3002", "conclusion": "Yan\u901a\u8fc7\u6574\u5408\u4e09\u5927\u6a21\u5757\uff0c\u4e3a\u4ea4\u4e92\u5f0f\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.08605", "pdf": "https://arxiv.org/pdf/2508.08605", "abs": "https://arxiv.org/abs/2508.08605", "authors": ["Honglei Xu", "Zhilu Zhang", "Junjie Fan", "Xiaohe Wu", "Wangmeng Zuo"], "title": "SelfHVD: Self-Supervised Handheld Video Deblurring for Mobile Phones", "categories": ["cs.CV"], "comment": null, "summary": "Shooting video with a handheld mobile phone, the most common photographic device, often results in blurry frames due to shaking hands and other instability factors. Although previous video deblurring methods have achieved impressive progress, they still struggle to perform satisfactorily on real-world handheld video due to the blur domain gap between training and testing data. To address the issue, we propose a self-supervised method for handheld video deblurring, which is driven by sharp clues in the video. First, to train the deblurring model, we extract the sharp clues from the video and take them as misalignment labels of neighboring blurry frames. Second, to improve the model's ability, we propose a novel Self-Enhanced Video Deblurring (SEVD) method to create higher-quality paired video data. Third, we propose a Self-Constrained Spatial Consistency Maintenance (SCSCM) method to regularize the model, preventing position shifts between the output and input frames. Moreover, we construct a synthetic and a real-world handheld video dataset for handheld video deblurring. Extensive experiments on these two and other common real-world datasets demonstrate that our method significantly outperforms existing self-supervised ones. The code and datasets are publicly available at https://github.com/cshonglei/SelfHVD.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u76d1\u7763\u7684\u624b\u6301\u89c6\u9891\u53bb\u6a21\u7cca\u65b9\u6cd5\uff0c\u5229\u7528\u89c6\u9891\u4e2d\u7684\u6e05\u6670\u7ebf\u7d22\u4f5c\u4e3a\u8bad\u7ec3\u6807\u7b7e\uff0c\u5e76\u901a\u8fc7\u81ea\u589e\u5f3a\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u7ea6\u675f\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u624b\u6301\u8bbe\u5907\u62cd\u6444\u89c6\u9891\u6a21\u7cca\u95ee\u9898\uff0c\u5c24\u5176\u662f\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u6570\u636e\u4e4b\u95f4\u7684\u6a21\u7cca\u57df\u5dee\u8ddd\u3002", "method": "1. \u63d0\u53d6\u89c6\u9891\u4e2d\u7684\u6e05\u6670\u7ebf\u7d22\u4f5c\u4e3a\u8bad\u7ec3\u6807\u7b7e\uff1b2. \u63d0\u51fa\u81ea\u589e\u5f3a\u89c6\u9891\u53bb\u6a21\u7cca\uff08SEVD\uff09\u65b9\u6cd5\u751f\u6210\u9ad8\u8d28\u91cf\u914d\u5bf9\u6570\u636e\uff1b3. \u63d0\u51fa\u81ea\u7ea6\u675f\u7a7a\u95f4\u4e00\u81f4\u6027\u7ef4\u62a4\uff08SCSCM\uff09\u65b9\u6cd5\u9632\u6b62\u8f93\u51fa\u5e27\u4f4d\u7f6e\u504f\u79fb\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u624b\u6301\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u81ea\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u76d1\u7763\u548c\u589e\u5f3a\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u624b\u6301\u89c6\u9891\u53bb\u6a21\u7cca\u6027\u80fd\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.08701", "pdf": "https://arxiv.org/pdf/2508.08701", "abs": "https://arxiv.org/abs/2508.08701", "authors": ["Ouyang Xu", "Baoming Zhang", "Ruiyu Mao", "Yunhui Guo"], "title": "SafeFix: Targeted Model Repair via Controlled Image Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Deep learning models for visual recognition often exhibit systematic errors due to underrepresented semantic subpopulations. Although existing debugging frameworks can pinpoint these failures by identifying key failure attributes, repairing the model effectively remains difficult. Current solutions often rely on manually designed prompts to generate synthetic training images -- an approach prone to distribution shift and semantic errors. To overcome these challenges, we introduce a model repair module that builds on an interpretable failure attribution pipeline. Our approach uses a conditional text-to-image model to generate semantically faithful and targeted images for failure cases. To preserve the quality and relevance of the generated samples, we further employ a large vision-language model (LVLM) to filter the outputs, enforcing alignment with the original data distribution and maintaining semantic consistency. By retraining vision models with this rare-case-augmented synthetic dataset, we significantly reduce errors associated with rare cases. Our experiments demonstrate that this targeted repair strategy improves model robustness without introducing new bugs. Code is available at https://github.com/oxu2/SafeFix", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6761\u4ef6\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u548c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u9488\u5bf9\u6027\u4fee\u590d\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u89c6\u89c9\u8bc6\u522b\u6a21\u578b\u5728\u7f55\u89c1\u5b50\u7fa4\u4f53\u4e0a\u7684\u9519\u8bef\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u89c6\u89c9\u8bc6\u522b\u4e2d\u56e0\u8bed\u4e49\u5b50\u7fa4\u4f53\u4ee3\u8868\u6027\u4e0d\u8db3\u800c\u5bfc\u81f4\u7684\u7cfb\u7edf\u6027\u9519\u8bef\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u4fee\u590d\u8fd9\u4e9b\u9519\u8bef\u3002", "method": "\u7ed3\u5408\u53ef\u89e3\u91ca\u7684\u5931\u8d25\u5f52\u56e0\u6d41\u7a0b\uff0c\u4f7f\u7528\u6761\u4ef6\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u751f\u6210\u8bed\u4e49\u51c6\u786e\u7684\u4fee\u590d\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fc7\u6ee4\u8f93\u51fa\u4ee5\u786e\u4fdd\u6570\u636e\u5206\u5e03\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u578b\u5728\u7f55\u89c1\u60c5\u51b5\u4e0b\u7684\u9519\u8bef\uff0c\u63d0\u5347\u4e86\u9c81\u68d2\u6027\u4e14\u672a\u5f15\u5165\u65b0\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u7684\u9488\u5bf9\u6027\u4fee\u590d\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u8bc6\u522b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u7f55\u89c1\u5b50\u7fa4\u4f53\u4e0a\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2508.08783", "pdf": "https://arxiv.org/pdf/2508.08783", "abs": "https://arxiv.org/abs/2508.08783", "authors": ["Tianyu Xiong", "Dayi Tan", "Wei Tian"], "title": "DiffPose-Animal: A Language-Conditioned Diffusion Framework for Animal Pose Estimation", "categories": ["cs.CV"], "comment": "13pages,2figures", "summary": "Animal pose estimation is a fundamental task in computer vision, with growing importance in ecological monitoring, behavioral analysis, and intelligent livestock management. Compared to human pose estimation, animal pose estimation is more challenging due to high interspecies morphological diversity, complex body structures, and limited annotated data. In this work, we introduce DiffPose-Animal, a novel diffusion-based framework for top-down animal pose estimation. Unlike traditional heatmap regression methods, DiffPose-Animal reformulates pose estimation as a denoising process under the generative framework of diffusion models. To enhance semantic guidance during keypoint generation, we leverage large language models (LLMs) to extract both global anatomical priors and local keypoint-wise semantics based on species-specific prompts. These textual priors are encoded and fused with image features via cross-attention modules to provide biologically meaningful constraints throughout the denoising process. Additionally, a diffusion-based keypoint decoder is designed to progressively refine pose predictions, improving robustness to occlusion and annotation sparsity. Extensive experiments on public animal pose datasets demonstrate the effectiveness and generalization capability of our method, especially under challenging scenarios with diverse species, cluttered backgrounds, and incomplete keypoints.", "AI": {"tldr": "DiffPose-Animal\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u52a8\u7269\u59ff\u6001\u4f30\u8ba1\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u89e3\u5256\u5b66\u5148\u9a8c\u77e5\u8bc6\uff0c\u901a\u8fc7\u53bb\u566a\u8fc7\u7a0b\u9010\u6b65\u4f18\u5316\u59ff\u6001\u9884\u6d4b\u3002", "motivation": "\u52a8\u7269\u59ff\u6001\u4f30\u8ba1\u56e0\u7269\u79cd\u5f62\u6001\u591a\u6837\u6027\u548c\u6570\u636e\u6807\u6ce8\u6709\u9650\u800c\u66f4\u5177\u6311\u6218\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u3002", "method": "\u63d0\u51faDiffPose-Animal\u6846\u67b6\uff0c\u5c06\u59ff\u6001\u4f30\u8ba1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6269\u6563\u6a21\u578b\u4e2d\u7684\u53bb\u566a\u8fc7\u7a0b\uff0c\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u7684\u89e3\u5256\u5b66\u5148\u9a8c\u77e5\u8bc6\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u7269\u79cd\u591a\u6837\u6027\u548c\u906e\u6321\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DiffPose-Animal\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u6a21\u578b\u548c\u8bed\u4e49\u5148\u9a8c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u7269\u59ff\u6001\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.08794", "pdf": "https://arxiv.org/pdf/2508.08794", "abs": "https://arxiv.org/abs/2508.08794", "authors": ["Yingxue Pang", "Shijie Zhao", "Mengxi Guo", "Junlin Li", "Li Zhang"], "title": "Region-Adaptive Video Sharpening via Rate-Perception Optimization", "categories": ["cs.CV"], "comment": null, "summary": "Sharpening is a widely adopted video enhancement technique. However, uniform sharpening intensity ignores texture variations, degrading video quality. Sharpening also increases bitrate, and there's a lack of techniques to optimally allocate these additional bits across diverse regions. Thus, this paper proposes RPO-AdaSharp, an end-to-end region-adaptive video sharpening model for both perceptual enhancement and bitrate savings. We use the coding tree unit (CTU) partition mask as prior information to guide and constrain the allocation of increased bits. Experiments on benchmarks demonstrate the effectiveness of the proposed model qualitatively and quantitatively.", "AI": {"tldr": "RPO-AdaSharp\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u533a\u57df\u81ea\u9002\u5e94\u89c6\u9891\u9510\u5316\u6a21\u578b\uff0c\u65e8\u5728\u4f18\u5316\u9510\u5316\u6548\u679c\u5e76\u8282\u7701\u6bd4\u7279\u7387\u3002", "motivation": "\u4f20\u7edf\u5747\u5300\u9510\u5316\u5f3a\u5ea6\u5ffd\u7565\u4e86\u7eb9\u7406\u53d8\u5316\uff0c\u5bfc\u81f4\u89c6\u9891\u8d28\u91cf\u4e0b\u964d\uff0c\u540c\u65f6\u589e\u52a0\u4e86\u6bd4\u7279\u7387\uff0c\u7f3a\u4e4f\u9488\u5bf9\u4e0d\u540c\u533a\u57df\u4f18\u5316\u6bd4\u7279\u5206\u914d\u7684\u6280\u672f\u3002", "method": "\u5229\u7528\u7f16\u7801\u6811\u5355\u5143\uff08CTU\uff09\u5206\u533a\u63a9\u7801\u4f5c\u4e3a\u5148\u9a8c\u4fe1\u606f\uff0c\u5f15\u5bfc\u548c\u7ea6\u675f\u589e\u52a0\u7684\u6bd4\u7279\u5206\u914d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u8d28\u91cf\u548c\u6bd4\u7279\u7387\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "RPO-AdaSharp\u6709\u6548\u89e3\u51b3\u4e86\u9510\u5316\u5e26\u6765\u7684\u6bd4\u7279\u7387\u589e\u52a0\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u89c6\u9891\u8d28\u91cf\u3002"}}
{"id": "2508.08798", "pdf": "https://arxiv.org/pdf/2508.08798", "abs": "https://arxiv.org/abs/2508.08798", "authors": ["Yao Lu", "Jiawei Li", "Ming Jiang"], "title": "MonoPartNeRF:Human Reconstruction from Monocular Video via Part-Based Neural Radiance Fields", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, Neural Radiance Fields (NeRF) have achieved remarkable progress in dynamic human reconstruction and rendering. Part-based rendering paradigms, guided by human segmentation, allow for flexible parameter allocation based on structural complexity, thereby enhancing representational efficiency. However, existing methods still struggle with complex pose variations, often producing unnatural transitions at part boundaries and failing to reconstruct occluded regions accurately in monocular settings. We propose MonoPartNeRF, a novel framework for monocular dynamic human rendering that ensures smooth transitions and robust occlusion recovery. First, we build a bidirectional deformation model that combines rigid and non-rigid transformations to establish a continuous, reversible mapping between observation and canonical spaces. Sampling points are projected into a parameterized surface-time space (u, v, t) to better capture non-rigid motion. A consistency loss further suppresses deformation-induced artifacts and discontinuities. We introduce a part-based pose embedding mechanism that decomposes global pose vectors into local joint embeddings based on body regions. This is combined with keyframe pose retrieval and interpolation, along three orthogonal directions, to guide pose-aware feature sampling. A learnable appearance code is integrated via attention to model dynamic texture changes effectively. Experiments on the ZJU-MoCap and MonoCap datasets demonstrate that our method significantly outperforms prior approaches under complex pose and occlusion conditions, achieving superior joint alignment, texture fidelity, and structural continuity.", "AI": {"tldr": "MonoPartNeRF\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5355\u76ee\u52a8\u6001\u4eba\u4f53\u6e32\u67d3\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5411\u53d8\u5f62\u6a21\u578b\u548c\u90e8\u5206\u59ff\u6001\u5d4c\u5165\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u59ff\u6001\u53d8\u5316\u548c\u906e\u6321\u6062\u590d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u59ff\u6001\u53d8\u5316\u548c\u5355\u76ee\u8bbe\u7f6e\u4e0b\u7684\u906e\u6321\u6062\u590d\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u8fb9\u754c\u8fc7\u6e21\u4e0d\u81ea\u7136\u548c\u906e\u6321\u533a\u57df\u91cd\u5efa\u4e0d\u51c6\u786e\u3002", "method": "\u63d0\u51fa\u53cc\u5411\u53d8\u5f62\u6a21\u578b\u7ed3\u5408\u521a\u6027\u548c\u975e\u521a\u6027\u53d8\u6362\uff0c\u5f15\u5165\u53c2\u6570\u5316\u8868\u9762-\u65f6\u95f4\u7a7a\u95f4\u91c7\u6837\u70b9\uff0c\u7ed3\u5408\u90e8\u5206\u59ff\u6001\u5d4c\u5165\u673a\u5236\u548c\u5173\u952e\u5e27\u59ff\u6001\u63d2\u503c\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5efa\u6a21\u52a8\u6001\u7eb9\u7406\u53d8\u5316\u3002", "result": "\u5728ZJU-MoCap\u548cMonoCap\u6570\u636e\u96c6\u4e0a\uff0cMonoPartNeRF\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u590d\u6742\u59ff\u6001\u548c\u906e\u6321\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u66f4\u597d\u7684\u5173\u8282\u5bf9\u9f50\u3001\u7eb9\u7406\u4fdd\u771f\u5ea6\u548c\u7ed3\u6784\u8fde\u7eed\u6027\u3002", "conclusion": "MonoPartNeRF\u901a\u8fc7\u521b\u65b0\u7684\u53d8\u5f62\u548c\u59ff\u6001\u5efa\u6a21\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5355\u76ee\u52a8\u6001\u4eba\u4f53\u6e32\u67d3\u7684\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.08812", "pdf": "https://arxiv.org/pdf/2508.08812", "abs": "https://arxiv.org/abs/2508.08812", "authors": ["Yuqi Peng", "Lingtao Zheng", "Yufeng Yang", "Yi Huang", "Mingfu Yan", "Jianzhuang Liu", "Shifeng Chen"], "title": "TARA: Token-Aware LoRA for Composable Personalization in Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Personalized text-to-image generation aims to synthesize novel images of a specific subject or style using only a few reference images. Recent methods based on Low-Rank Adaptation (LoRA) enable efficient single-concept customization by injecting lightweight, concept-specific adapters into pre-trained diffusion models. However, combining multiple LoRA modules for multi-concept generation often leads to identity missing and visual feature leakage. In this work, we identify two key issues behind these failures: (1) token-wise interference among different LoRA modules, and (2) spatial misalignment between the attention map of a rare token and its corresponding concept-specific region. To address these issues, we propose Token-Aware LoRA (TARA), which introduces a token mask to explicitly constrain each module to focus on its associated rare token to avoid interference, and a training objective that encourages the spatial attention of a rare token to align with its concept region. Our method enables training-free multi-concept composition by directly injecting multiple independently trained TARA modules at inference time. Experimental results demonstrate that TARA enables efficient multi-concept inference and effectively preserving the visual identity of each concept by avoiding mutual interference between LoRA modules. The code and models are available at https://github.com/YuqiPeng77/TARA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faToken-Aware LoRA (TARA)\u65b9\u6cd5\uff0c\u89e3\u51b3\u591a\u6982\u5ff5LoRA\u6a21\u5757\u7ec4\u5408\u65f6\u7684\u8eab\u4efd\u7f3a\u5931\u548c\u7279\u5f81\u6cc4\u6f0f\u95ee\u9898\uff0c\u901a\u8fc7\u4ee4\u724c\u63a9\u7801\u548c\u7a7a\u95f4\u5bf9\u9f50\u5b9e\u73b0\u9ad8\u6548\u591a\u6982\u5ff5\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLoRA\u7684\u65b9\u6cd5\u5728\u591a\u6982\u5ff5\u751f\u6210\u65f6\u5b58\u5728\u8eab\u4efd\u7f3a\u5931\u548c\u7279\u5f81\u6cc4\u6f0f\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51faTARA\u65b9\u6cd5\uff0c\u5f15\u5165\u4ee4\u724c\u63a9\u7801\u7ea6\u675f\u6a21\u5757\u4e13\u6ce8\u4e8e\u5173\u8054\u4ee4\u724c\uff0c\u5e76\u901a\u8fc7\u8bad\u7ec3\u76ee\u6807\u5b9e\u73b0\u7a7a\u95f4\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTARA\u80fd\u9ad8\u6548\u7ec4\u5408\u591a\u6982\u5ff5\u5e76\u907f\u514d\u6a21\u5757\u95f4\u5e72\u6270\uff0c\u4fdd\u7559\u5404\u6982\u5ff5\u7684\u89c6\u89c9\u8eab\u4efd\u3002", "conclusion": "TARA\u901a\u8fc7\u4ee4\u724c\u611f\u77e5\u548c\u7a7a\u95f4\u5bf9\u9f50\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6982\u5ff5LoRA\u7ec4\u5408\u7684\u95ee\u9898\u3002"}}
{"id": "2508.08867", "pdf": "https://arxiv.org/pdf/2508.08867", "abs": "https://arxiv.org/abs/2508.08867", "authors": ["Lin Zeng", "Boming Zhao", "Jiarui Hu", "Xujie Shen", "Ziqiang Dang", "Hujun Bao", "Zhaopeng Cui"], "title": "GaussianUpdate: Continual 3D Gaussian Splatting Update for Changing Environments", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "Novel view synthesis with neural models has advanced rapidly in recent years, yet adapting these models to scene changes remains an open problem. Existing methods are either labor-intensive, requiring extensive model retraining, or fail to capture detailed types of changes over time. In this paper, we present GaussianUpdate, a novel approach that combines 3D Gaussian representation with continual learning to address these challenges. Our method effectively updates the Gaussian radiance fields with current data while preserving information from past scenes. Unlike existing methods, GaussianUpdate explicitly models different types of changes through a novel multi-stage update strategy. Additionally, we introduce a visibility-aware continual learning approach with generative replay, enabling self-aware updating without the need to store images. The experiments on the benchmark dataset demonstrate our method achieves superior and real-time rendering with the capability of visualizing changes over different times", "AI": {"tldr": "GaussianUpdate\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u54083D\u9ad8\u65af\u8868\u793a\u548c\u6301\u7eed\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u573a\u666f\u7684\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u80fd\u591f\u5b9e\u65f6\u66f4\u65b0\u5e76\u4fdd\u7559\u5386\u53f2\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u5927\u91cf\u91cd\u65b0\u8bad\u7ec3\uff0c\u8981\u4e48\u65e0\u6cd5\u6355\u6349\u573a\u666f\u968f\u65f6\u95f4\u53d8\u5316\u7684\u7ec6\u8282\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u52a8\u6001\u573a\u666f\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u91c7\u75283D\u9ad8\u65af\u8868\u793a\u4e0e\u6301\u7eed\u5b66\u4e60\u7ed3\u5408\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u66f4\u65b0\u7b56\u7565\u5efa\u6a21\u4e0d\u540c\u7c7b\u578b\u7684\u53d8\u5316\uff0c\u5e76\u5f15\u5165\u53ef\u89c1\u6027\u611f\u77e5\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6e32\u67d3\uff0c\u5e76\u80fd\u591f\u53ef\u89c6\u5316\u4e0d\u540c\u65f6\u95f4\u70b9\u7684\u573a\u666f\u53d8\u5316\u3002", "conclusion": "GaussianUpdate\u5728\u52a8\u6001\u573a\u666f\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5b9e\u65f6\u6e32\u67d3\u548c\u573a\u666f\u53d8\u5316\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.08939", "pdf": "https://arxiv.org/pdf/2508.08939", "abs": "https://arxiv.org/abs/2508.08939", "authors": ["Eduarda Caldeira", "Fadi Boutros", "Naser Damer"], "title": "MADPromptS: Unlocking Zero-Shot Morphing Attack Detection with Multiple Prompt Aggregation", "categories": ["cs.CV"], "comment": "Accepted at ACM Multimedia Workshops", "summary": "Face Morphing Attack Detection (MAD) is a critical challenge in face recognition security, where attackers can fool systems by interpolating the identity information of two or more individuals into a single face image, resulting in samples that can be verified as belonging to multiple identities by face recognition systems. While multimodal foundation models (FMs) like CLIP offer strong zero-shot capabilities by jointly modeling images and text, most prior works on FMs for biometric recognition have relied on fine-tuning for specific downstream tasks, neglecting their potential for direct, generalizable deployment. This work explores a pure zero-shot approach to MAD by leveraging CLIP without any additional training or fine-tuning, focusing instead on the design and aggregation of multiple textual prompts per class. By aggregating the embeddings of diverse prompts, we better align the model's internal representations with the MAD task, capturing richer and more varied cues indicative of bona-fide or attack samples. Our results show that prompt aggregation substantially improves zero-shot detection performance, demonstrating the effectiveness of exploiting foundation models' built-in multimodal knowledge through efficient prompt engineering.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCLIP\u7684\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u4eba\u8138\u878d\u5408\u653b\u51fb\uff08MAD\uff09\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5e76\u805a\u5408\u591a\u4e2a\u6587\u672c\u63d0\u793a\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u4eba\u8138\u878d\u5408\u653b\u51fb\uff08MAD\uff09\u5bf9\u5b89\u5168\u6784\u6210\u5a01\u80c1\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u5fae\u8c03\uff0c\u5ffd\u89c6\u4e86\u57fa\u7840\u6a21\u578b\uff08\u5982CLIP\uff09\u7684\u76f4\u63a5\u6cdb\u5316\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u96f6\u6837\u672c\u65b9\u6cd5\u3002", "method": "\u5229\u7528CLIP\u6a21\u578b\uff0c\u8bbe\u8ba1\u5e76\u805a\u5408\u591a\u4e2a\u6587\u672c\u63d0\u793a\uff0c\u901a\u8fc7\u5d4c\u5165\u5bf9\u9f50\u63d0\u5347\u6a21\u578b\u5bf9MAD\u4efb\u52a1\u7684\u8868\u5f81\u80fd\u529b\uff0c\u65e0\u9700\u5fae\u8c03\u6216\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u793a\u805a\u5408\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u68c0\u6d4b\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u901a\u8fc7\u9ad8\u6548\u63d0\u793a\u5de5\u7a0b\u5229\u7528\u57fa\u7840\u6a21\u578b\u5185\u7f6e\u591a\u6a21\u6001\u77e5\u8bc6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u7eaf\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u672c\u6587\u5c55\u793a\u4e86CLIP\u5728MAD\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65e0\u9700\u5fae\u8c03\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.08978", "pdf": "https://arxiv.org/pdf/2508.08978", "abs": "https://arxiv.org/abs/2508.08978", "authors": ["Zhentao Fan", "Zongzuo Wang", "Weiwei Zhang"], "title": "TaoCache: Structure-Maintained Video Generation Acceleration", "categories": ["cs.CV"], "comment": null, "summary": "Existing cache-based acceleration methods for video diffusion models primarily skip early or mid denoising steps, which often leads to structural discrepancies relative to full-timestep generation and can hinder instruction following and character consistency. We present TaoCache, a training-free, plug-and-play caching strategy that, instead of residual-based caching, adopts a fixed-point perspective to predict the model's noise output and is specifically effective in late denoising stages. By calibrating cosine similarities and norm ratios of consecutive noise deltas, TaoCache preserves high-resolution structure while enabling aggressive skipping. The approach is orthogonal to complementary accelerations such as Pyramid Attention Broadcast (PAB) and TeaCache, and it integrates seamlessly into DiT-based frameworks. Across Latte-1, OpenSora-Plan v110, and Wan2.1, TaoCache attains substantially higher visual quality (LPIPS, SSIM, PSNR) than prior caching methods under the same speedups.", "AI": {"tldr": "TaoCache\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u7f13\u5b58\u7b56\u7565\uff0c\u901a\u8fc7\u56fa\u5b9a\u70b9\u89c6\u89d2\u9884\u6d4b\u566a\u58f0\u8f93\u51fa\uff0c\u6709\u6548\u63d0\u5347\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7f13\u5b58\u52a0\u901f\u65b9\u6cd5\u5728\u8df3\u8fc7\u65e9\u671f\u6216\u4e2d\u671f\u53bb\u566a\u6b65\u9aa4\u65f6\u4f1a\u5bfc\u81f4\u7ed3\u6784\u5dee\u5f02\uff0c\u5f71\u54cd\u6307\u4ee4\u8ddf\u968f\u548c\u89d2\u8272\u4e00\u81f4\u6027\u3002", "method": "\u91c7\u7528\u56fa\u5b9a\u70b9\u89c6\u89d2\u9884\u6d4b\u566a\u58f0\u8f93\u51fa\uff0c\u6821\u51c6\u4f59\u5f26\u76f8\u4f3c\u5ea6\u548c\u566a\u58f0\u589e\u91cf\u8303\u6570\u6bd4\uff0c\u4fdd\u7559\u9ad8\u5206\u8fa8\u7387\u7ed3\u6784\u3002", "result": "\u5728\u76f8\u540c\u52a0\u901f\u6761\u4ef6\u4e0b\uff0cTaoCache\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7f13\u5b58\u65b9\u6cd5\uff08LPIPS\u3001SSIM\u3001PSNR\u6307\u6807\uff09\u3002", "conclusion": "TaoCache\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u517c\u5bb9\u6027\u5f3a\u7684\u7f13\u5b58\u7b56\u7565\uff0c\u9002\u7528\u4e8eDiT\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u89c6\u89c9\u8d28\u91cf\u3002"}}
{"id": "2508.08987", "pdf": "https://arxiv.org/pdf/2508.08987", "abs": "https://arxiv.org/abs/2508.08987", "authors": ["Ding Xia", "Naoto Inoue", "Qianru Qiu", "Kotaro Kikuchi"], "title": "ColorGPT: Leveraging Large Language Models for Multimodal Color Recommendation", "categories": ["cs.CV", "cs.HC"], "comment": "Accepted to ICDAR2025", "summary": "Colors play a crucial role in the design of vector graphic documents by enhancing visual appeal, facilitating communication, improving usability, and ensuring accessibility. In this context, color recommendation involves suggesting appropriate colors to complete or refine a design when one or more colors are missing or require alteration. Traditional methods often struggled with these challenges due to the complex nature of color design and the limited data availability. In this study, we explored the use of pretrained Large Language Models (LLMs) and their commonsense reasoning capabilities for color recommendation, raising the question: Can pretrained LLMs serve as superior designers for color recommendation tasks? To investigate this, we developed a robust, rigorously validated pipeline, ColorGPT, that was built by systematically testing multiple color representations and applying effective prompt engineering techniques. Our approach primarily targeted color palette completion by recommending colors based on a set of given colors and accompanying context. Moreover, our method can be extended to full palette generation, producing an entire color palette corresponding to a provided textual description. Experimental results demonstrated that our LLM-based pipeline outperformed existing methods in terms of color suggestion accuracy and the distribution of colors in the color palette completion task. For the full palette generation task, our approach also yielded improvements in color diversity and similarity compared to current techniques.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u989c\u8272\u63a8\u8350\u7684\u53ef\u80fd\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aColorGPT\u7684\u7ba1\u9053\uff0c\u5728\u989c\u8272\u8c03\u8272\u677f\u5b8c\u6210\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u989c\u8272\u63a8\u8350\u65b9\u6cd5\u56e0\u8bbe\u8ba1\u590d\u6742\u6027\u548c\u6570\u636e\u6709\u9650\u6027\u5b58\u5728\u6311\u6218\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22LLMs\u662f\u5426\u80fd\u6210\u4e3a\u66f4\u4f18\u7684\u989c\u8272\u63a8\u8350\u5de5\u5177\u3002", "method": "\u5f00\u53d1\u4e86ColorGPT\u7ba1\u9053\uff0c\u901a\u8fc7\u6d4b\u8bd5\u591a\u79cd\u989c\u8272\u8868\u793a\u548c\u63d0\u793a\u5de5\u7a0b\u6280\u672f\uff0c\u4e13\u6ce8\u4e8e\u989c\u8272\u8c03\u8272\u677f\u5b8c\u6210\u548c\u751f\u6210\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cColorGPT\u5728\u989c\u8272\u5efa\u8bae\u51c6\u786e\u6027\u548c\u8c03\u8272\u677f\u5206\u5e03\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u989c\u8272\u591a\u6837\u6027\u548c\u76f8\u4f3c\u6027\u4e0a\u4e5f\u6709\u63d0\u5347\u3002", "conclusion": "\u9884\u8bad\u7ec3LLMs\u5728\u989c\u8272\u63a8\u8350\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0cColorGPT\u4e3a\u8bbe\u8ba1\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u6548\u5de5\u5177\u3002"}}
{"id": "2508.09009", "pdf": "https://arxiv.org/pdf/2508.09009", "abs": "https://arxiv.org/abs/2508.09009", "authors": ["Luyang Cao", "Han Xu", "Jian Zhang", "Lei Qi", "Jiayi Ma", "Yinghuan Shi", "Yang Gao"], "title": "Towards Perfection: Building Inter-component Mutual Correction for Retinex-based Low-light Image Enhancement", "categories": ["cs.CV"], "comment": "This article has been accepted by ACMMM 2025", "summary": "In low-light image enhancement, Retinex-based deep learning methods have garnered significant attention due to their exceptional interpretability. These methods decompose images into mutually independent illumination and reflectance components, allows each component to be enhanced separately. In fact, achieving perfect decomposition of illumination and reflectance components proves to be quite challenging, with some residuals still existing after decomposition. In this paper, we formally name these residuals as inter-component residuals (ICR), which has been largely underestimated by previous methods. In our investigation, ICR not only affects the accuracy of the decomposition but also causes enhanced components to deviate from the ideal outcome, ultimately reducing the final synthesized image quality. To address this issue, we propose a novel Inter-correction Retinex model (IRetinex) to alleviate ICR during the decomposition and enhancement stage. In the decomposition stage, we leverage inter-component residual reduction module to reduce the feature similarity between illumination and reflectance components. In the enhancement stage, we utilize the feature similarity between the two components to detect and mitigate the impact of ICR within each enhancement unit. Extensive experiments on three low-light benchmark datasets demonstrated that by reducing ICR, our method outperforms state-of-the-art approaches both qualitatively and quantitatively.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Inter-correction Retinex\u6a21\u578b\uff08IRetinex\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4e2d\u5206\u89e3\u9636\u6bb5\u7684\u4e92\u6210\u5206\u6b8b\u5dee\uff08ICR\uff09\u95ee\u9898\uff0c\u901a\u8fc7\u51cf\u5c11ICR\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u5728\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4e2d\uff0cRetinex\u65b9\u6cd5\u5206\u89e3\u56fe\u50cf\u4e3a\u5149\u7167\u548c\u53cd\u5c04\u6210\u5206\u65f6\u5b58\u5728\u4e92\u6210\u5206\u6b8b\u5dee\uff08ICR\uff09\uff0c\u5f71\u54cd\u5206\u89e3\u7cbe\u5ea6\u548c\u6700\u7ec8\u56fe\u50cf\u8d28\u91cf\u3002", "method": "\u63d0\u51faIRetinex\u6a21\u578b\uff0c\u5728\u5206\u89e3\u9636\u6bb5\u51cf\u5c11\u5149\u7167\u548c\u53cd\u5c04\u6210\u5206\u7684\u7279\u5f81\u76f8\u4f3c\u6027\uff0c\u5728\u589e\u5f3a\u9636\u6bb5\u5229\u7528\u7279\u5f81\u76f8\u4f3c\u6027\u68c0\u6d4b\u548c\u51cf\u8f7bICR\u7684\u5f71\u54cd\u3002", "result": "\u5728\u4e09\u4e2a\u4f4e\u5149\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u901a\u8fc7\u51cf\u5c11ICR\uff0cIRetinex\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u7684\u6548\u679c\u3002"}}
{"id": "2508.09061", "pdf": "https://arxiv.org/pdf/2508.09061", "abs": "https://arxiv.org/abs/2508.09061", "authors": ["Fuhao Chang", "Shuxin Li", "Yabei Li", "Lei He"], "title": "VLM-3D:End-to-End Vision-Language Models for Open-World 3D Perception", "categories": ["cs.CV"], "comment": null, "summary": "Open-set perception in complex traffic environments poses a critical challenge for autonomous driving systems, particularly in identifying previously unseen object categories, which is vital for ensuring safety. Visual Language Models (VLMs), with their rich world knowledge and strong semantic reasoning capabilities, offer new possibilities for addressing this task. However, existing approaches typically leverage VLMs to extract visual features and couple them with traditional object detectors, resulting in multi-stage error propagation that hinders perception accuracy. To overcome this limitation, we propose VLM-3D, the first end-to-end framework that enables VLMs to perform 3D geometric perception in autonomous driving scenarios. VLM-3D incorporates Low-Rank Adaptation (LoRA) to efficiently adapt VLMs to driving tasks with minimal computational overhead, and introduces a joint semantic-geometric loss design: token-level semantic loss is applied during early training to ensure stable convergence, while 3D IoU loss is introduced in later stages to refine the accuracy of 3D bounding box predictions. Evaluations on the nuScenes dataset demonstrate that the proposed joint semantic-geometric loss in VLM-3D leads to a 12.8% improvement in perception accuracy, fully validating the effectiveness and advancement of our method.", "AI": {"tldr": "VLM-3D\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u8fdb\u884c3D\u51e0\u4f55\u611f\u77e5\uff0c\u901a\u8fc7\u8054\u5408\u8bed\u4e49-\u51e0\u4f55\u635f\u5931\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u611f\u77e5\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5f00\u653e\u96c6\u611f\u77e5\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u8bc6\u522b\u672a\u89c1\u7269\u4f53\u7c7b\u522b\uff0c\u786e\u4fdd\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51faVLM-3D\u6846\u67b6\uff0c\u7ed3\u5408\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u548c\u8054\u5408\u8bed\u4e49-\u51e0\u4f55\u635f\u5931\u8bbe\u8ba1\uff08\u65e9\u671f\u8bed\u4e49\u635f\u5931\uff0c\u540e\u671f3D IoU\u635f\u5931\uff09\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0c\u611f\u77e5\u7cbe\u5ea6\u63d0\u534712.8%\u3002", "conclusion": "VLM-3D\u901a\u8fc7\u7aef\u5230\u7aef\u8bbe\u8ba1\u548c\u8054\u5408\u635f\u5931\uff0c\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5f00\u653e\u96c6\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2508.09136", "pdf": "https://arxiv.org/pdf/2508.09136", "abs": "https://arxiv.org/abs/2508.09136", "authors": ["Ya Zou", "Jingfeng Yao", "Siyuan Yu", "Shuai Zhang", "Wenyu Liu", "Xinggang Wang"], "title": "Turbo-VAED: Fast and Stable Transfer of Video-VAEs to Mobile Devices", "categories": ["cs.CV"], "comment": null, "summary": "There is a growing demand for deploying large generative AI models on mobile devices. For recent popular video generative models, however, the Variational AutoEncoder (VAE) represents one of the major computational bottlenecks. Both large parameter sizes and mismatched kernels cause out-of-memory errors or extremely slow inference on mobile devices. To address this, we propose a low-cost solution that efficiently transfers widely used video VAEs to mobile devices. (1) We analyze redundancy in existing VAE architectures and get empirical design insights. By integrating 3D depthwise separable convolutions into our model, we significantly reduce the number of parameters. (2) We observe that the upsampling techniques in mainstream video VAEs are poorly suited to mobile hardware and form the main bottleneck. In response, we propose a decoupled 3D pixel shuffle scheme that slashes end-to-end delay. Building upon these, we develop a universal mobile-oriented VAE decoder, Turbo-VAED. (3) We propose an efficient VAE decoder training method. Since only the decoder is used during deployment, we distill it to Turbo-VAED instead of retraining the full VAE, enabling fast mobile adaptation with minimal performance loss. To our knowledge, our method enables real-time 720p video VAE decoding on mobile devices for the first time. This approach is widely applicable to most video VAEs. When integrated into four representative models, with training cost as low as $95, it accelerates original VAEs by up to 84.5x at 720p resolution on GPUs, uses as low as 17.5% of original parameter count, and retains 96.9% of the original reconstruction quality. Compared to mobile-optimized VAEs, Turbo-VAED achieves a 2.9x speedup in FPS and better reconstruction quality on the iPhone 16 Pro. The code and models will soon be available at https://github.com/hustvl/Turbo-VAED.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTurbo-VAED\u7684\u4f4e\u6210\u672c\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u5c06\u89c6\u9891\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u9ad8\u6548\u90e8\u7f72\u5230\u79fb\u52a8\u8bbe\u5907\u4e0a\uff0c\u663e\u8457\u51cf\u5c11\u53c2\u6570\u6570\u91cf\u548c\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "motivation": "\u79fb\u52a8\u8bbe\u5907\u4e0a\u90e8\u7f72\u5927\u578b\u751f\u6210AI\u6a21\u578b\u7684\u9700\u6c42\u589e\u957f\uff0c\u4f46\u73b0\u6709\u89c6\u9891VAE\u5b58\u5728\u53c2\u6570\u8fc7\u5927\u548c\u5185\u6838\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5bfc\u81f4\u5185\u5b58\u4e0d\u8db3\u6216\u63a8\u7406\u901f\u5ea6\u6781\u6162\u3002", "method": "1. \u5206\u6790\u73b0\u6709VAE\u67b6\u6784\u5197\u4f59\uff0c\u91c7\u75283D\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u51cf\u5c11\u53c2\u6570\uff1b2. \u63d0\u51fa\u89e3\u8026\u76843D\u50cf\u7d20\u6d17\u724c\u65b9\u6848\u4f18\u5316\u4e0a\u91c7\u6837\uff1b3. \u8bbe\u8ba1\u9ad8\u6548\u89e3\u7801\u5668\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ec5\u84b8\u998f\u89e3\u7801\u5668\u4ee5\u51cf\u5c11\u6027\u80fd\u635f\u5931\u3002", "result": "Turbo-VAED\u5728720p\u5206\u8fa8\u7387\u4e0b\u5c06\u539f\u59cbVAE\u52a0\u901f84.5\u500d\uff0c\u53c2\u6570\u51cf\u5c11\u81f317.5%\uff0c\u91cd\u5efa\u8d28\u91cf\u4fdd\u755996.9%\uff0c\u5728iPhone 16 Pro\u4e0aFPS\u63d0\u53472.9\u500d\u3002", "conclusion": "Turbo-VAED\u9996\u6b21\u5b9e\u73b0\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u65f6720p\u89c6\u9891VAE\u89e3\u7801\uff0c\u9002\u7528\u4e8e\u5927\u591a\u6570\u89c6\u9891VAE\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.09068", "pdf": "https://arxiv.org/pdf/2508.09068", "abs": "https://arxiv.org/abs/2508.09068", "authors": ["Conall Daly", "Anil Kokaram"], "title": "A new dataset and comparison for multi-camera frame synthesis", "categories": ["eess.IV", "cs.CV"], "comment": "SPIE2025 - Applications of Digital Image Processing XLVIII accepted   manuscript", "summary": "Many methods exist for frame synthesis in image sequences but can be broadly categorised into frame interpolation and view synthesis techniques. Fundamentally, both frame interpolation and view synthesis tackle the same task, interpolating a frame given surrounding frames in time or space. However, most frame interpolation datasets focus on temporal aspects with single cameras moving through time and space, while view synthesis datasets are typically biased toward stereoscopic depth estimation use cases. This makes direct comparison between view synthesis and frame interpolation methods challenging. In this paper, we develop a novel multi-camera dataset using a custom-built dense linear camera array to enable fair comparison between these approaches. We evaluate classical and deep learning frame interpolators against a view synthesis method (3D Gaussian Splatting) for the task of view in-betweening. Our results reveal that deep learning methods do not significantly outperform classical methods on real image data, with 3D Gaussian Splatting actually underperforming frame interpolators by as much as 3.5 dB PSNR. However, in synthetic scenes, the situation reverses -- 3D Gaussian Splatting outperforms frame interpolation algorithms by almost 5 dB PSNR at a 95% confidence level.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u76f8\u673a\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u516c\u5e73\u6bd4\u8f83\u5e27\u63d2\u503c\u548c\u89c6\u56fe\u5408\u6210\u65b9\u6cd5\uff0c\u53d1\u73b0\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u548c\u7ecf\u5178\u65b9\u6cd5\u5728\u771f\u5b9e\u56fe\u50cf\u6570\u636e\u4e0a\u8868\u73b0\u76f8\u8fd1\uff0c\u800c3D\u9ad8\u65af\u6cfc\u6e85\u5728\u5408\u6210\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u5e27\u63d2\u503c\u548c\u89c6\u56fe\u5408\u6210\u65b9\u6cd5\u7684\u6570\u636e\u96c6\u5b58\u5728\u504f\u5dee\uff0c\u96be\u4ee5\u76f4\u63a5\u6bd4\u8f83\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65b0\u7684\u6570\u636e\u96c6\u4ee5\u5b9e\u73b0\u516c\u5e73\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528\u81ea\u5b9a\u4e49\u5bc6\u96c6\u7ebf\u6027\u76f8\u673a\u9635\u5217\u6784\u5efa\u591a\u76f8\u673a\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u7ecf\u5178\u548c\u6df1\u5ea6\u5b66\u4e60\u5e27\u63d2\u503c\u65b9\u6cd5\u4e0e3D\u9ad8\u65af\u6cfc\u6e85\u5728\u89c6\u56fe\u63d2\u503c\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5728\u771f\u5b9e\u56fe\u50cf\u6570\u636e\u4e2d\uff0c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u672a\u663e\u8457\u4f18\u4e8e\u7ecf\u5178\u65b9\u6cd5\uff0c3D\u9ad8\u65af\u6cfc\u6e85\u8868\u73b0\u8f83\u5dee\uff1b\u4f46\u5728\u5408\u6210\u573a\u666f\u4e2d\uff0c3D\u9ad8\u65af\u6cfc\u6e85\u663e\u8457\u4f18\u4e8e\u5e27\u63d2\u503c\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4e0d\u540c\u65b9\u6cd5\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u516c\u5e73\u6bd4\u8f83\u7684\u57fa\u7840\u3002"}}
{"id": "2508.09078", "pdf": "https://arxiv.org/pdf/2508.09078", "abs": "https://arxiv.org/abs/2508.09078", "authors": ["Conall Daly", "Darren Ramsook", "Anil Kokaram"], "title": "Efficient motion-based metrics for video frame interpolation", "categories": ["eess.IV", "cs.CV"], "comment": "SPIE2025 - Applications of Digital Image Processing XLVIII accepted   manuscript", "summary": "Video frame interpolation (VFI) offers a way to generate intermediate frames between consecutive frames of a video sequence. Although the development of advanced frame interpolation algorithms has received increased attention in recent years, assessing the perceptual quality of interpolated content remains an ongoing area of research. In this paper, we investigate simple ways to process motion fields, with the purposes of using them as video quality metric for evaluating frame interpolation algorithms. We evaluate these quality metrics using the BVI-VFI dataset which contains perceptual scores measured for interpolated sequences. From our investigation we propose a motion metric based on measuring the divergence of motion fields. This metric correlates reasonably with these perceptual scores (PLCC=0.51) and is more computationally efficient (x2.7 speedup) compared to FloLPIPS (a well known motion-based metric). We then use our new proposed metrics to evaluate a range of state of the art frame interpolation metrics and find our metrics tend to favour more perceptual pleasing interpolated frames that may not score highly in terms of PSNR or SSIM.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u7b80\u5355\u5904\u7406\u8fd0\u52a8\u573a\u6765\u8bc4\u4f30\u89c6\u9891\u5e27\u63d2\u503c\u7b97\u6cd5\u7684\u611f\u77e5\u8d28\u91cf\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fd0\u52a8\u573a\u53d1\u6563\u6027\u7684\u65b0\u6307\u6807\uff0c\u8be5\u6307\u6807\u4e0e\u611f\u77e5\u8bc4\u5206\u76f8\u5173\u6027\u8f83\u597d\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u9891\u5e27\u63d2\u503c\u7b97\u6cd5\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u8bc4\u4f30\u63d2\u503c\u5185\u5bb9\u7684\u611f\u77e5\u8d28\u91cf\u4ecd\u662f\u4e00\u4e2a\u7814\u7a76\u70ed\u70b9\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u7b80\u5355\u6709\u6548\u7684\u8fd0\u52a8\u573a\u5904\u7406\u65b9\u6cd5\uff0c\u4ee5\u4f5c\u4e3a\u8bc4\u4f30\u63d2\u503c\u7b97\u6cd5\u7684\u8d28\u91cf\u6307\u6807\u3002", "method": "\u901a\u8fc7\u5904\u7406\u8fd0\u52a8\u573a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fd0\u52a8\u573a\u53d1\u6563\u6027\u7684\u8d28\u91cf\u6307\u6807\uff0c\u5e76\u4f7f\u7528BVI-VFI\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u65b0\u6307\u6807\u4e0e\u611f\u77e5\u8bc4\u5206\u7684\u76f8\u5173\u6027\u8f83\u597d\uff08PLCC=0.51\uff09\uff0c\u8ba1\u7b97\u6548\u7387\u6bd4FloLPIPS\u9ad82.7\u500d\u3002", "conclusion": "\u65b0\u6307\u6807\u66f4\u503e\u5411\u4e8e\u611f\u77e5\u8d28\u91cf\u8f83\u597d\u7684\u63d2\u503c\u5e27\uff0c\u800c\u975e\u4f20\u7edfPSNR\u6216SSIM\u8bc4\u5206\u9ad8\u7684\u5e27\u3002"}}
