<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 11]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [ARGS: Advanced Regularization on Aligning Gaussians over the Surface](https://arxiv.org/abs/2508.21344)
*Jeong Uk Lee,Sung Hee Choi*

Main category: cs.GR

TL;DR: 该论文在SuGaR基础上提出两种正则化策略：有效秩正则化防止高斯过度各向异性，以及神经SDF正则化提供全局表面先验，从而提升3D高斯泼溅的网格重建质量和视觉一致性。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅(3DGS)方法在重建高质量3D网格和视觉效果方面仍有不足，SuGaR等模型在视觉保真度和场景一致性方面还有改进空间。

Method: 1. 有效秩正则化：鼓励高斯保持更平衡的"圆盘状"形状而非"针状"形状；2. 神经SDF正则化：集成带Eikonal损失的SDF作为连续全局表面先验，指导高斯与底层几何更好对齐。

Result: 提出的两种互补正则化策略改善了单个高斯基元的保真度及其集体表面行为，能够从3DGS数据生成更准确和一致的视觉效果。

Conclusion: 通过形状正则化和全局表面先验的结合，该方法显著提升了3D高斯泼溅的表面重建质量，为高质量3D网格重建提供了有效解决方案。

Abstract: Reconstructing high-quality 3D meshes and visuals from 3D Gaussian Splatting(3DGS) still remains a central challenge in computer graphics. Although existing models such as SuGaR offer effective solutions for rendering, there is is still room to improve improve both visual fidelity and scene consistency. This work builds upon SuGaR by introducing two complementary regularization strategies that address common limitations in both the shape of individual Gaussians and the coherence of the overall surface. The first strategy introduces an effective rank regularization, motivated by recent studies on Gaussian primitive structures. This regularization discourages extreme anisotropy-specifically, "needle-like" shapes-by favoring more balanced, "disk-like" forms that are better suited for stable surface reconstruction. The second strategy integrates a neural Signed Distance Function (SDF) into the optimization process. The SDF is regularized with an Eikonal loss to maintain proper distance properties and provides a continuous global surface prior, guiding Gaussians toward better alignment with the underlying geometry. These two regularizations aim to improve both the fidelity of individual Gaussian primitives and their collective surface behavior. The final model can make more accurate and coherent visuals from 3DGS data.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [Q-Align: Alleviating Attention Leakage in Zero-Shot Appearance Transfer via Query-Query Alignment](https://arxiv.org/abs/2508.21090)
*Namu Kim,Wonbin Kweon,Minsoo Kim,Hwanjo Yu*

Main category: cs.CV

TL;DR: Q-Align通过Query-Query对齐解决大模型零样本外观迁移中的注意力泄漏问题，在保持结构的同时提升外观保真度


<details>
  <summary>Details</summary>
Motivation: 解决大规模图像生成模型在零样本外观迁移中出现的注意力泄漏问题，该问题源于Query-Key对齐导致的语义映射错误

Method: 提出Q-Align方法，包含三个核心贡献：1) Query-Query对齐实现精细空间语义映射；2) Key-Value重排增强特征对应；3) 使用重排后的键值进行注意力精化保持语义一致性

Result: 通过大量实验验证，Q-Align在外观保真度方面优于最先进方法，同时保持竞争力的结构保持能力

Conclusion: Q-Align有效解决了注意力泄漏问题，为零样本外观迁移提供了更精确的语义对齐方法

Abstract: We observe that zero-shot appearance transfer with large-scale image generation models faces a significant challenge: Attention Leakage. This challenge arises when the semantic mapping between two images is captured by the Query-Key alignment. To tackle this issue, we introduce Q-Align, utilizing Query-Query alignment to mitigate attention leakage and improve the semantic alignment in zero-shot appearance transfer. Q-Align incorporates three core contributions: (1) Query-Query alignment, facilitating the sophisticated spatial semantic mapping between two images; (2) Key-Value rearrangement, enhancing feature correspondence through realignment; and (3) Attention refinement using rearranged keys and values to maintain semantic consistency. We validate the effectiveness of Q-Align through extensive experiments and analysis, and Q-Align outperforms state-of-the-art methods in appearance fidelity while maintaining competitive structure preservation.

</details>


### [3] [ERTACache: Error Rectification and Timesteps Adjustment for Efficient Diffusion](https://arxiv.org/abs/2508.21091)
*Xurui Peng,Hong Liu,Chenqian Yan,Rui Ma,Fangmin Chen,Xing Wang,Zhihua Wu,Songwei Liu,Mingbao Lin*

Main category: cs.CV

TL;DR: ERTACache是一个扩散模型加速框架，通过智能缓存和误差校正实现2倍推理加速，同时保持或提升视觉质量


<details>
  <summary>Details</summary>
Motivation: 扩散模型由于迭代推理过程导致计算开销巨大，简单的特征缓存会导致明显的质量下降，需要解决缓存引起的累积误差问题

Method: 提出ERTACache框架：1）离线残差分析识别可重用步骤；2）动态调整积分间隔；3）闭式残差线性化模型近似缓存误差；联合校正特征偏移误差和步长放大误差

Result: 在标准图像和视频生成基准测试中实现2倍推理加速，视觉质量保持甚至提升；在Wan2.1视频扩散模型上实现2倍加速且VBench指标下降极小

Conclusion: ERTACache通过系统性的误差分析和校正机制，在保持扩散模型质量的同时显著提升推理效率，为实际应用提供了有效的加速解决方案

Abstract: Diffusion models suffer from substantial computational overhead due to their inherently iterative inference process. While feature caching offers a promising acceleration strategy by reusing intermediate outputs across timesteps, naive reuse often incurs noticeable quality degradation. In this work, we formally analyze the cumulative error introduced by caching and decompose it into two principal components: feature shift error, caused by inaccuracies in cached outputs, and step amplification error, which arises from error propagation under fixed timestep schedules. To address these issues, we propose ERTACache, a principled caching framework that jointly rectifies both error types. Our method employs an offline residual profiling stage to identify reusable steps, dynamically adjusts integration intervals via a trajectory-aware correction coefficient, and analytically approximates cache-induced errors through a closed-form residual linearization model. Together, these components enable accurate and efficient sampling under aggressive cache reuse. Extensive experiments across standard image and video generation benchmarks show that ERTACache achieves up to 2x inference speedup while consistently preserving or even improving visual quality. Notably, on the state-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x acceleration with minimal VBench degradation, effectively maintaining baseline fidelity while significantly improving efficiency. The code is available at https://github.com/bytedance/ERTACache.

</details>


### [4] [Safe-Control: A Safety Patch for Mitigating Unsafe Content in Text-to-Image Generation Models](https://arxiv.org/abs/2508.21099)
*Xiangtao Meng,Yingkai Dong,Ning Yu,Li Wang,Zheng Li,Shanqing Guo*

Main category: cs.CV

TL;DR: Safe-Control是一种即插即用的安全补丁，通过数据驱动策略和安全感知条件，在T2I模型中注入安全控制信号，有效减少不安全内容生成，同时保持良性图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有的T2I模型安全机制在分布偏移下容易被规避，或需要大量模型特定调整，无法有效应对滥用风险。

Method: 使用数据驱动策略和安全感知条件，以补丁形式向锁定的T2I模型注入安全控制信号，支持灵活构建和合并多种安全补丁。

Result: 在6个不同T2I模型上评估显示，Safe-Control将不安全内容生成概率降至7%，显著优于其他7种最先进的安全机制（约20%），同时保持良性图像质量和文本对齐。

Conclusion: Safe-Control提供了一种有效、灵活且兼容性强的T2I模型安全防护方案，能够适应不断变化的安全需求。

Abstract: Despite the advancements in Text-to-Image (T2I) generation models, their potential for misuse or even abuse raises serious safety concerns. Model developers have made tremendous efforts to introduce safety mechanisms that can address these concerns in T2I models. However, the existing safety mechanisms, whether external or internal, either remain susceptible to evasion under distribution shifts or require extensive model-specific adjustments. To address these limitations, we introduce Safe-Control, an innovative plug-and-play safety patch designed to mitigate unsafe content generation in T2I models. Using data-driven strategies and safety-aware conditions, Safe-Control injects safety control signals into the locked T2I model, acting as an update in a patch-like manner. Model developers can also construct various safety patches to meet the evolving safety requirements, which can be flexibly merged into a single, unified patch. Its plug-and-play design further ensures adaptability, making it compatible with other T2I models of similar denoising architecture. We conduct extensive evaluations on six diverse and public T2I models. Empirical results highlight that Safe-Control is effective in reducing unsafe content generation across six diverse T2I models with similar generative architectures, yet it successfully maintains the quality and text alignment of benign images. Compared to seven state-of-the-art safety mechanisms, including both external and internal defenses, Safe-Control significantly outperforms all baselines in reducing unsafe content generation. For example, it reduces the probability of unsafe content generation to 7%, compared to approximately 20% for most baseline methods, under both unsafe prompts and the latest adversarial attacks.

</details>


### [5] [Reverse Imaging for Wide-spectrum Generalization of Cardiac MRI Segmentation](https://arxiv.org/abs/2508.21254)
*Yidong Zhao,Peter Kellman,Hui Xue,Tongyun Yang,Yi Zhang,Yuchi Han,Orlando Simonetti,Qian Tao*

Main category: cs.CV

TL;DR: Reverse Imaging是一种基于物理原理的心脏MRI数据增强方法，通过反演推断底层自旋属性来解决不同成像序列间的泛化问题，显著提升分割模型在不同对比度图像上的准确性。


<details>
  <summary>Details</summary>
Motivation: 预训练的心脏MRI分割模型难以泛化到不同成像序列，因为图像对比度差异巨大。虽然成像协议不同，但所有图像都由相同的自旋属性（质子密度、T1、T2值）控制，这为解决泛化问题提供了物理基础。

Method: 提出Reverse Imaging方法：1）通过求解非线性逆问题从观测图像反演推断底层自旋属性；2）使用扩散模型学习自旋属性的先验分布；3）基于估计的自旋属性合成任意新序列的图像，实现数据增强和域适应。

Result: 该方法能够从MR图像中获得有意义的自旋属性估计，作为可解释的潜在变量，实现高度灵活的图像合成。实验表明Reverse Imaging能够在差异巨大的图像对比度和成像协议上实现高度准确的分割。

Conclusion: Reverse Imaging通过物理驱动的数据增强方法，从根本上解决了心脏MRI分割的泛化问题，实现了跨不同成像序列的宽谱泛化能力，为医学图像分析提供了新的解决方案。

Abstract: Pretrained segmentation models for cardiac magnetic resonance imaging (MRI) struggle to generalize across different imaging sequences due to significant variations in image contrast. These variations arise from changes in imaging protocols, yet the same fundamental spin properties, including proton density, T1, and T2 values, govern all acquired images. With this core principle, we introduce Reverse Imaging, a novel physics-driven method for cardiac MRI data augmentation and domain adaptation to fundamentally solve the generalization problem. Our method reversely infers the underlying spin properties from observed cardiac MRI images, by solving ill-posed nonlinear inverse problems regularized by the prior distribution of spin properties. We acquire this "spin prior" by learning a generative diffusion model from the multiparametric SAturation-recovery single-SHot acquisition sequence (mSASHA) dataset, which offers joint cardiac T1 and T2 maps. Our method enables approximate but meaningful spin-property estimates from MR images, which provide an interpretable "latent variable" that lead to highly flexible image synthesis of arbitrary novel sequences. We show that Reverse Imaging enables highly accurate segmentation across vastly different image contrasts and imaging protocols, realizing wide-spectrum generalization of cardiac MRI segmentation.

</details>


### [6] [Print2Volume: Generating Synthetic OCT-based 3D Fingerprint Volume from 2D Fingerprint Image](https://arxiv.org/abs/2508.21371)
*Qingran Miao,Haixia Wang,Haohao Sun,Yilong Zhang*

Main category: cs.CV

TL;DR: 提出了Print2Volume框架，从2D指纹图像生成逼真的合成OCT 3D指纹数据，解决了OCT数据稀缺问题，显著提升了指纹识别性能


<details>
  <summary>Details</summary>
Motivation: OCT技术可获取高分辨率3D指纹数据，但数据采集成本高、耗时长，导致大规模公开数据集稀缺，阻碍了深度学习算法的发展

Method: 三阶段框架：1) 2D风格转换模块将二值指纹转为灰度图像；2) 3D结构扩展网络将2D图像外推为3D解剖体积；3) 基于3D GAN的OCT真实感细化器添加纹理和噪声

Result: 生成了42万个合成样本，通过在合成数据上预训练并在小规模真实数据上微调，将EER从15.62%降至2.50%

Conclusion: Print2Volume框架有效解决了OCT指纹数据稀缺问题，生成的合成数据质量高，显著提升了生物识别性能

Abstract: Optical Coherence Tomography (OCT) enables the acquisition of high-resolution, three-dimensional fingerprint data, capturing rich subsurface structures for robust biometric recognition. However, the high cost and time-consuming nature of OCT data acquisition have led to a scarcity of large-scale public datasets, significantly hindering the development of advanced algorithms, particularly data-hungry deep learning models. To address this critical bottleneck, this paper introduces Print2Volume, a novel framework for generating realistic, synthetic OCT-based 3D fingerprints from 2D fingerprint image. Our framework operates in three sequential stages: (1) a 2D style transfer module that converts a binary fingerprint into a grayscale images mimicking the style of a Z-direction mean-projected OCT scan; (2) a 3D Structure Expansion Network that extrapolates the 2D im-age into a plausible 3D anatomical volume; and (3) an OCT Realism Refiner, based on a 3D GAN, that renders the structural volume with authentic textures, speckle noise, and other imaging characteristics. Using Print2Volume, we generated a large-scale synthetic dataset of 420,000 samples. Quantitative experiments demonstrate the high quality of our synthetic data and its significant impact on recognition performance. By pre-training a recognition model on our synthetic data and fine-tuning it on a small real-world dataset, we achieved a remarkable reduction in the Equal Error Rate (EER) from 15.62% to 2.50% on the ZJUT-EIFD benchmark, proving the effectiveness of our approach in overcoming data scarcity.

</details>


### [7] [Scale-GS: Efficient Scalable Gaussian Splatting via Redundancy-filtering Training on Streaming Content](https://arxiv.org/abs/2508.21444)
*Jiayu Yang,Weijian Su,Songqian Zhang,Yuqi Han,Jinli Suo,Qiang Zhang*

Main category: cs.CV

TL;DR: M框架通过分层高斯球体组织和混合变形生成策略，显著降低了动态3D高斯溅射的训练时间和数据量，同时保持高质量渲染效果


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射在动态场景中存在数据量大和训练时间长的问题，需要一种高效的流式训练框架来解决这些限制

Method: 使用基于锚点的分层高斯球体结构，粗粒度高斯表示场景结构，细粒度高斯负责细节渲染；采用混合变形和生成策略建模运动；双向自适应掩码机制提升训练效率

Result: 在广泛实验中，M框架实现了卓越的视觉质量，同时显著减少了与最先进方法相比的训练时间

Conclusion: 提出的M框架通过创新的分层组织和运动建模策略，成功解决了动态3D高斯溅射的效率和可扩展性问题

Abstract: 3D Gaussian Splatting (3DGS) enables high-fidelity real-time rendering, a key requirement for immersive applications. However, the extension of 3DGS to dynamic scenes remains limitations on the substantial data volume of dense Gaussians and the prolonged training time required for each frame. This paper presents \M, a scalable Gaussian Splatting framework designed for efficient training in streaming tasks. Specifically, Gaussian spheres are hierarchically organized by scale within an anchor-based structure. Coarser-level Gaussians represent the low-resolution structure of the scene, while finer-level Gaussians, responsible for detailed high-fidelity rendering, are selectively activated by the coarser-level Gaussians. To further reduce computational overhead, we introduce a hybrid deformation and spawning strategy that models motion of inter-frame through Gaussian deformation and triggers Gaussian spawning to characterize wide-range motion. Additionally, a bidirectional adaptive masking mechanism enhances training efficiency by removing static regions and prioritizing informative viewpoints. Extensive experiments demonstrate that \M~ achieves superior visual quality while significantly reducing training time compared to state-of-the-art methods.

</details>


### [8] [Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation](https://arxiv.org/abs/2508.21529)
*Ronan Docherty,Antonis Vamvakeros,Samuel J. Cooper*

Main category: cs.CV

TL;DR: 这篇论文提出了一种卷积神经网络来重新采样基础模型的低分辨率特征向量，以解决微观图像分析中的细细特征表达和大图像处理问题，并在多种微观图分割任务中展示了良好效果。


<details>
  <summary>Details</summary>
Motivation: 基础模型通常使用片段基础的特征描述符，在微观图像中无法有效表达细微特征，且面对大尺寸图像时计算效率低。需要一种方法来提高特征的空间分辨率以满足微观图像分析的需求。

Method: 训练一个卷积神经网络，以输入图像为参考，对低分辨率（大片段尺寸）的基础模型特征进行重新采样。该网络在各种微观图像分割任务中不需要重新训练即可直接应用。

Result: 重新采样后的深度特征能够区分难以分割的相位，如细线裂缝。交互式分割使用这些特征能够以更少的标签和更快的速度生成高质量的分割结果，效果远超传统卷积网络的训练或微调。

Conclusion: 通过卷积网络重新采样基础模型特征的方法，有效解决了微观图像分析中的细微特征表达和大图像处理挑战，为材料和生物微观图像分割提供了高效的解决方案。

Abstract: Feature foundation models - usually vision transformers - offer rich semantic descriptors of images, useful for downstream tasks such as (interactive) segmentation and object detection. For computational efficiency these descriptors are often patch-based, and so struggle to represent the fine features often present in micrographs; they also struggle with the large image sizes present in materials and biological image analysis. In this work, we train a convolutional neural network to upsample low-resolution (i.e, large patch size) foundation model features with reference to the input image. We apply this upsampler network (without any further training) to efficiently featurise and then segment a variety of microscopy images, including plant cells, a lithium-ion battery cathode and organic crystals. The richness of these upsampled features admits separation of hard to segment phases, like hairline cracks. We demonstrate that interactive segmentation with these deep features produces high-quality segmentations far faster and with far fewer labels than training or finetuning a more traditional convolutional network.

</details>


### [9] [Complete Gaussian Splats from a Single Image with Denoising Diffusion Models](https://arxiv.org/abs/2508.21542)
*Ziwei Liao,Mohamed Sayed,Steven L. Waslander,Sara Vicente,Daniyar Turmukhambetov,Michael Firman*

Main category: cs.CV

TL;DR: 提出了一种基于潜在扩散模型的单图像3D场景重建方法，使用高斯泼溅技术补全遮挡和未观测区域，通过变分自重构器和扩散模型实现自监督学习。


<details>
  <summary>Details</summary>
Motivation: 传统高斯泼溅方法需要密集观测且无法重建遮挡区域，回归方法只能预测单一模式导致模糊和不真实。需要一种能够从单张图像生成完整3D场景并处理遮挡歧义的方法。

Method: 使用变分自重构器从2D图像自监督学习潜在空间，然后在潜在空间上训练扩散模型，以生成基于单张输入图像的3D高斯泼溅表示分布。

Result: 方法能够生成忠实重建和多样化样本，成功补全遮挡表面，实现高质量的360度渲染。

Conclusion: 提出的生成式方法能够有效解决单图像3D重建中的遮挡补全问题，相比传统回归方法产生更真实和多样化的结果。

Abstract: Gaussian splatting typically requires dense observations of the scene and can fail to reconstruct occluded and unobserved areas. We propose a latent diffusion model to reconstruct a complete 3D scene with Gaussian splats, including the occluded parts, from only a single image during inference. Completing the unobserved surfaces of a scene is challenging due to the ambiguity of the plausible surfaces. Conventional methods use a regression-based formulation to predict a single "mode" for occluded and out-of-frustum surfaces, leading to blurriness, implausibility, and failure to capture multiple possible explanations. Thus, they often address this problem partially, focusing either on objects isolated from the background, reconstructing only visible surfaces, or failing to extrapolate far from the input views. In contrast, we propose a generative formulation to learn a distribution of 3D representations of Gaussian splats conditioned on a single input image. To address the lack of ground-truth training data, we propose a Variational AutoReconstructor to learn a latent space only from 2D images in a self-supervised manner, over which a diffusion model is trained. Our method generates faithful reconstructions and diverse samples with the ability to complete the occluded surfaces for high-quality 360-degree renderings.

</details>


### [10] [FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA](https://arxiv.org/abs/2508.21712)
*Alvaro Patricio,Atabak Dehban,Rodrigo Ventura*

Main category: cs.CV

TL;DR: FLORA是一个轻量级的合成数据生成管道，使用LoRA微调Flux 1.1扩散模型，大幅降低计算需求，在消费级GPU上就能生成高质量合成数据用于目标检测任务。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的合成数据生成方法需要企业级GPU和大量合成图像，计算资源需求过高，限制了实际应用。

Method: 使用Flux 1.1 Dev扩散模型，仅通过低秩适应（LoRA）进行微调，构建轻量级合成数据生成管道。

Result: 在7个不同目标检测数据集上验证，仅用500张合成图像就能超越ODGEN基线使用5000张图像的性能，mAP@.50:.95提升高达21.3%。

Conclusion: FLORA证明了质量与效率并重的方法比暴力生成更有效，仅用10%的数据和少量计算成本就能达到最先进性能，使合成数据生成更加实用和可及。

Abstract: Recent advances in diffusion-based generative models have demonstrated significant potential in augmenting scarce datasets for object detection tasks. Nevertheless, most recent models rely on resource-intensive full fine-tuning of large-scale diffusion models, requiring enterprise-grade GPUs (e.g., NVIDIA V100) and thousands of synthetic images. To address these limitations, we propose Flux LoRA Augmentation (FLORA), a lightweight synthetic data generation pipeline. Our approach uses the Flux 1.1 Dev diffusion model, fine-tuned exclusively through Low-Rank Adaptation (LoRA). This dramatically reduces computational requirements, enabling synthetic dataset generation with a consumer-grade GPU (e.g., NVIDIA RTX 4090). We empirically evaluate our approach on seven diverse object detection datasets. Our results demonstrate that training object detectors with just 500 synthetic images generated by our approach yields superior detection performance compared to models trained on 5000 synthetic images from the ODGEN baseline, achieving improvements of up to 21.3% in mAP@.50:.95. This work demonstrates that it is possible to surpass state-of-the-art performance with far greater efficiency, as FLORA achieves superior results using only 10% of the data and a fraction of the computational cost. This work demonstrates that a quality and efficiency-focused approach is more effective than brute-force generation, making advanced synthetic data creation more practical and accessible for real-world scenarios.

</details>


### [11] [CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models](https://arxiv.org/abs/2508.21732)
*João Valente,Atabak Dehban,Rodrigo Ventura*

Main category: cs.CV

TL;DR: CAD2DMD-SET是一个合成数据生成工具，通过3D CAD模型和高级渲染技术生成数字测量设备(DMD)的多样化合成数据集，用于微调大型视觉语言模型(LVLMs)，显著提升了模型在真实世界复杂场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型在数字测量设备读数等简单场景中表现不佳，特别是在头戴式摄像头和增强现实应用中常见的杂乱、遮挡、极端视角和运动模糊等真实世界条件下。

Method: 利用3D CAD模型、高级渲染和高保真图像合成技术开发CAD2DMD-SET工具，生成带有VQA标注的合成DMD数据集，并创建包含1000张标注真实图像的DMDBench验证集。

Result: 对三种最先进的LVLM进行基准测试，使用CAD2DMD-SET生成的数据集微调LoRA后，InternVL模型的ANLS分数提升了200%，且在其他任务上没有性能下降。

Conclusion: CAD2DMD-SET训练数据集显著提高了LVLM在挑战性条件下的鲁棒性和性能，该工具将作为开源发布，允许社区添加不同的测量设备并生成自己的数据集。

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities across various multimodal tasks. They continue, however, to struggle with trivial scenarios such as reading values from Digital Measurement Devices (DMDs), particularly in real-world conditions involving clutter, occlusions, extreme viewpoints, and motion blur; common in head-mounted cameras and Augmented Reality (AR) applications. Motivated by these limitations, this work introduces CAD2DMD-SET, a synthetic data generation tool designed to support visual question answering (VQA) tasks involving DMDs. By leveraging 3D CAD models, advanced rendering, and high-fidelity image composition, our tool produces diverse, VQA-labelled synthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we present DMDBench, a curated validation set of 1,000 annotated real-world images designed to evaluate model performance under practical constraints. Benchmarking three state-of-the-art LVLMs using Average Normalised Levenshtein Similarity (ANLS) and further fine-tuning LoRA's of these models with CAD2DMD-SET's generated dataset yielded substantial improvements, with InternVL showcasing a score increase of 200% without degrading on other tasks. This demonstrates that the CAD2DMD-SET training dataset substantially improves the robustness and performance of LVLMs when operating under the previously stated challenging conditions. The CAD2DMD-SET tool is expected to be released as open-source once the final version of this manuscript is prepared, allowing the community to add different measurement devices and generate their own datasets.

</details>


### [12] [The Demon is in Ambiguity: Revisiting Situation Recognition with Single Positive Multi-Label Learning](https://arxiv.org/abs/2508.21816)
*Yiming Lin,Yuchen Niu,Shang Wang,Kaizhu Huang,Qiufeng Wang,Xiao-Bo Jin*

Main category: cs.CV

TL;DR: 本文揭示了场景识别中动词分类本质上是多标签问题，提出了单正例多标签学习框架和GE-VerbMLP模型，在保持传统指标竞争力的同时实现了3%以上的MAP提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法将动词分类视为单标签问题，但视觉事件识别存在固有歧义性，同一图像可能被多个动词类别合理描述，需要重新审视这一设定。

Method: 将动词分类重新定义为单正例多标签学习(SPMLL)问题，提出Graph Enhanced Verb Multilayer Perceptron (GE-VerbMLP)模型，结合图神经网络捕捉标签相关性和对抗训练优化决策边界。

Result: 在真实数据集上的大量实验表明，该方法在保持传统top-1和top-5准确率竞争力的同时，实现了超过3%的MAP提升。

Conclusion: 动词分类本质上是多标签问题，SPMLL框架和GE-VerbMLP模型能有效处理标注不完整问题，为场景识别任务提供了更合理的评估和解决方案。

Abstract: Context recognition (SR) is a fundamental task in computer vision that aims to extract structured semantic summaries from images by identifying key events and their associated entities. Specifically, given an input image, the model must first classify the main visual events (verb classification), then identify the participating entities and their semantic roles (semantic role labeling), and finally localize these entities in the image (semantic role localization). Existing methods treat verb classification as a single-label problem, but we show through a comprehensive analysis that this formulation fails to address the inherent ambiguity in visual event recognition, as multiple verb categories may reasonably describe the same image. This paper makes three key contributions: First, we reveal through empirical analysis that verb classification is inherently a multi-label problem due to the ubiquitous semantic overlap between verb categories. Second, given the impracticality of fully annotating large-scale datasets with multiple labels, we propose to reformulate verb classification as a single positive multi-label learning (SPMLL) problem - a novel perspective in SR research. Third, we design a comprehensive multi-label evaluation benchmark for SR that is carefully designed to fairly evaluate model performance in a multi-label setting. To address the challenges of SPMLL, we futher develop the Graph Enhanced Verb Multilayer Perceptron (GE-VerbMLP), which combines graph neural networks to capture label correlations and adversarial training to optimize decision boundaries. Extensive experiments on real-world datasets show that our approach achieves more than 3\% MAP improvement while remaining competitive on traditional top-1 and top-5 accuracy metrics.

</details>
