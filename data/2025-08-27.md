<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 3]
- [cs.CV](#cs.CV) [Total: 11]
- [quant-ph](#quant-ph) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Real-time 3D Visualization of Radiance Fields on Light Field Displays](https://arxiv.org/abs/2508.18540)
*Jonghyun Kim,Cheng Sun,Michael Stengel,Matthew Chan,Andrew Russell,Jaehyun Jung,Wil Braithwaite,Shalini De Mello,David Luebke*

Main category: cs.GR

TL;DR: 提出了一种统一高效框架，用于光场显示器上的实时辐射场渲染，支持多种辐射场表示，实现45视角下200+FPS的实时交互


<details>
  <summary>Details</summary>
Motivation: 辐射场技术虽然能够实现高质量3D场景重建，但将其与光场显示器集成面临巨大计算挑战，因为光场显示器需要从多个视角进行高分辨率渲染，而辐射场渲染本身计算密集

Method: 基于单次平面扫描策略和共享非方向性组件缓存的统一架构，支持NeRFs、3D高斯泼溅和稀疏体素等多种辐射场表示，无需重新训练即可泛化到不同场景格式

Result: 在Looking Glass显示器上实现实时交互应用，45个视角512p分辨率下达到200+FPS，相比独立渲染每个视角实现最高22倍加速，同时保持图像质量

Conclusion: 该框架成功解决了辐射场在光场显示器上的实时渲染问题，为沉浸式3D交互提供了高效解决方案，具有广泛的适用性和优异的性能表现

Abstract: Radiance fields have revolutionized photo-realistic 3D scene visualization by enabling high-fidelity reconstruction of complex environments, making them an ideal match for light field displays. However, integrating these technologies presents significant computational challenges, as light field displays require multiple high-resolution renderings from slightly shifted viewpoints, while radiance fields rely on computationally intensive volume rendering. In this paper, we propose a unified and efficient framework for real-time radiance field rendering on light field displays. Our method supports a wide range of radiance field representations, including NeRFs, 3D Gaussian Splatting, and Sparse Voxels, within a shared architecture based on a single-pass plane sweeping strategy and caching of shared, non-directional components. The framework generalizes across different scene formats without retraining, and avoids redundant computation across views. We further demonstrate a real-time interactive application on a Looking Glass display, achieving 200+ FPS at 512p across 45 views, enabling seamless, immersive 3D interaction. On standard benchmarks, our method achieves up to 22x speedup compared to independently rendering each view, while preserving image quality.

</details>


### [2] [SemLayoutDiff: Semantic Layout Generation with Diffusion Model for Indoor Scene Synthesis](https://arxiv.org/abs/2508.18597)
*Xiaohao Sun,Divyam Goel,Angle X. Chang*

Main category: cs.GR

TL;DR: SemLayoutDiff是一个统一的3D室内场景生成模型，通过结合语义地图和扩散模型，能够根据房间掩码约束生成多样化的家具布局。


<details>
  <summary>Details</summary>
Motivation: 现有的3D场景生成方法无法有效考虑建筑约束条件（如门窗位置），导致生成的家具布局可能不实用或被阻挡。

Method: 提出结合自上而下语义地图和对象属性的场景表示，使用分类扩散模型生成语义地图，再通过交叉注意力网络预测家具位置，同时考虑门窗等建筑元素。

Result: 在3D-FRONT数据集上的实验表明，该方法能生成空间连贯、真实且多样的场景，优于先前方法。

Conclusion: SemLayoutDiff通过显式考虑建筑约束条件，实现了更实用和合理的3D室内场景合成。

Abstract: We present SemLayoutDiff, a unified model for synthesizing diverse 3D indoor scenes across multiple room types. The model introduces a scene layout representation combining a top-down semantic map and attributes for each object. Unlike prior approaches, which cannot condition on architectural constraints, SemLayoutDiff employs a categorical diffusion model capable of conditioning scene synthesis explicitly on room masks. It first generates a coherent semantic map, followed by a cross-attention-based network to predict furniture placements that respect the synthesized layout. Our method also accounts for architectural elements such as doors and windows, ensuring that generated furniture arrangements remain practical and unobstructed. Experiments on the 3D-FRONT dataset show that SemLayoutDiff produces spatially coherent, realistic, and varied scenes, outperforming previous methods.

</details>


### [3] [A Bag of Tricks for Efficient Implicit Neural Point Clouds](https://arxiv.org/abs/2508.19140)
*Florian Hahlbohm,Linus Franke,Leon Overkämping,Paula Wespe,Susana Castillo,Martin Eisemann,Marcus Magnor*

Main category: cs.GR

TL;DR: 速度优化隐式神经点云(INPC)表示，通过改进光栅化器、样本采集和预训练等技术，实现训练速度提升25%，渲染速度提升2倍，VRAM使用量减少20%，同时保持图像质量


<details>
  <summary>Details</summary>
Motivation: INPC结合了神经场的表达力和点基渲染的效率，但在实际应用中遇到渲染速度慢的问题，影响了其实用性

Method: 采用三个主要优化技术：1)改进的光栅化器实现 2)更有效的样本采集技术 3)为洞洞填充的卷积神经网络添加预训练。还将点模型化为小高斯分布来提升外推视角的质量

Result: 优化后的INPC流水线实现了训练速度提升25%，渲染速度提升2倍，VRAM使用量减少20%，同时进一步提升了图像质量

Conclusion: 通过系统性的性能优化方案，INPC在保持高图像质量的同时大幅提升了训练和渲染效率，为实际应用提供了可行性。这些优化技术具有广泛的适用性

Abstract: Implicit Neural Point Cloud (INPC) is a recent hybrid representation that combines the expressiveness of neural fields with the efficiency of point-based rendering, achieving state-of-the-art image quality in novel view synthesis. However, as with other high-quality approaches that query neural networks during rendering, the practical usability of INPC is limited by comparatively slow rendering. In this work, we present a collection of optimizations that significantly improve both the training and inference performance of INPC without sacrificing visual fidelity. The most significant modifications are an improved rasterizer implementation, more effective sampling techniques, and the incorporation of pre-training for the convolutional neural network used for hole-filling. Furthermore, we demonstrate that points can be modeled as small Gaussians during inference to further improve quality in extrapolated, e.g., close-up views of the scene. We design our implementations to be broadly applicable beyond INPC and systematically evaluate each modification in a series of experiments. Our optimized INPC pipeline achieves up to 25% faster training, 2x faster rendering, and 20% reduced VRAM usage paired with slight image quality improvements.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [4] [FastAvatar: Instant 3D Gaussian Splatting for Faces from Single Unconstrained Poses](https://arxiv.org/abs/2508.18389)
*Hao Liang,Zhixuan Ge,Ashish Tiwari,Soumendu Majee,G. M. Dilshan Godaliyadda,Ashok Veeraraghavan,Guha Balakrishnan*

Main category: cs.CV

TL;DR: FastAvatar是一个快速前馈框架，能够从任意姿态的单张人脸图像在10毫秒内生成3D高斯泼溅模型，实现姿态不变的身份保持和实时身份插值。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸3D重建方法要么速度慢（需要逐人脸优化），要么重建质量不佳。需要一种既能保持高质量重建又能实现实时推理的解决方案。

Method: 使用编码器-解码器神经网络设计，先构建3DGS人脸模板模型，然后将输入图像编码为身份特定的姿态不变潜在嵌入，解码预测模板高斯参数残差。

Result: 在重建质量上显著优于现有前馈方法（如GAGAvatar），速度比逐人脸优化方法快1000倍，支持实时身份插值和属性编辑。

Conclusion: FastAvatar结合了优秀的重建质量和速度，扩展了3D高斯泼溅在消费级和交互式系统中照片级真实感头像应用的范围。

Abstract: We present FastAvatar, a pose-invariant, feed-forward framework that can generate a 3D Gaussian Splatting (3DGS) model from a single face image from an arbitrary pose in near-instant time (<10ms). FastAvatar uses a novel encoder-decoder neural network design to achieve both fast fitting and identity preservation regardless of input pose. First, FastAvatar constructs a 3DGS face ``template'' model from a training dataset of faces with multi-view captures. Second, FastAvatar encodes the input face image into an identity-specific and pose-invariant latent embedding, and decodes this embedding to predict residuals to the structural and appearance parameters of each Gaussian in the template 3DGS model. By only inferring residuals in a feed-forward fashion, model inference is fast and robust. FastAvatar significantly outperforms existing feed-forward face 3DGS methods (e.g., GAGAvatar) in reconstruction quality, and runs 1000x faster than per-face optimization methods (e.g., FlashAvatar, GaussianAvatars and GASP). In addition, FastAvatar's novel latent space design supports real-time identity interpolation and attribute editing which is not possible with any existing feed-forward 3DGS face generation framework. FastAvatar's combination of excellent reconstruction quality and speed expands the scope of 3DGS for photorealistic avatar applications in consumer and interactive systems.

</details>


### [5] [LPLC: A Dataset for License Plate Legibility Classification](https://arxiv.org/abs/2508.18425)
*Lucas Wojcik,Gabriel E. Lima,Valfride Nascimento,Eduil Nascimento Jr.,Rayson Laroca,David Menotti*

Main category: cs.CV

TL;DR: 本文提出了一个新的车牌可读性分类数据集LPLC，包含10,210张车辆图像和12,687个标注车牌，用于判断车牌是否需要超分辨率处理或是否完全不可读。基准测试显示现有模型在此任务上表现不佳（F1分数低于80%），凸显了该任务的挑战性。


<details>
  <summary>Details</summary>
Motivation: 自动车牌识别系统在处理低质量车牌时面临挑战，现有超分辨率方法无法根本解决问题。需要选择性对需要增强可读性的车牌进行预处理，但缺乏专门的数据集支持相关研究。

Method: 构建包含10,210张车辆图像和12,687个标注车牌的新数据集，采用细粒度标注策略（包括遮挡情况、4个可读性等级和字符标签）。使用ViT、ResNet和YOLO三种图像识别网络作为基准模型进行分类任务。

Result: 所有三个基准模型的整体F1分数都低于80%，表明该任务具有相当难度。超分辨率和车牌识别方法的分析结果进一步验证了任务的挑战性。

Conclusion: 车牌可读性分类是一个具有挑战性的任务，需要进一步研究。提出的LPLC数据集为相关研究提供了重要资源，有助于推动选择性图像预处理方法的发展。

Abstract: Automatic License Plate Recognition (ALPR) faces a major challenge when dealing with illegible license plates (LPs). While reconstruction methods such as super-resolution (SR) have emerged, the core issue of recognizing these low-quality LPs remains unresolved. To optimize model performance and computational efficiency, image pre-processing should be applied selectively to cases that require enhanced legibility. To support research in this area, we introduce a novel dataset comprising 10,210 images of vehicles with 12,687 annotated LPs for legibility classification (the LPLC dataset). The images span a wide range of vehicle types, lighting conditions, and camera/image quality levels. We adopt a fine-grained annotation strategy that includes vehicle- and LP-level occlusions, four legibility categories (perfect, good, poor, and illegible), and character labels for three categories (excluding illegible LPs). As a benchmark, we propose a classification task using three image recognition networks to determine whether an LP image is good enough, requires super-resolution, or is completely unrecoverable. The overall F1 score, which remained below 80% for all three baseline models (ViT, ResNet, and YOLO), together with the analyses of SR and LP recognition methods, highlights the difficulty of the task and reinforces the need for further research. The proposed dataset is publicly available at https://github.com/lmlwojcik/lplc-dataset.

</details>


### [6] [Wan-S2V: Audio-Driven Cinematic Video Generation](https://arxiv.org/abs/2508.18621)
*Xin Gao,Li Hu,Siqi Hu,Mingyang Huang,Chaonan Ji,Dechao Meng,Jinwei Qi,Penchong Qiao,Zhen Shen,Yafei Song,Ke Sun,Linrui Tian,Guangyuan Wang,Qi Wang,Zhongjian Wang,Jiayu Xiao,Sheng Xu,Bang Zhang,Peng Zhang,Xindi Zhang,Zhe Zhang,Jingren Zhou,Lian Zhuo*

Main category: cs.CV

TL;DR: Wan-S2V是一个基于Wan构建的音频驱动角色动画模型，在电影级动画制作中显著优于现有方法，特别是在角色互动、身体动作和镜头运动等复杂场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的音频驱动角色动画方法主要适用于语音和歌唱场景，但在需要复杂角色互动、细腻身体动作和动态镜头工作的影视制作中表现不足，无法满足电影级动画的需求。

Method: 基于Wan框架构建Wan-S2V音频驱动模型，通过增强表现力和保真度来适应电影级动画制作需求，支持长视频生成和精确的口型同步编辑。

Result: 在Hunyuan-Avatar和Omnihuman等先进模型的基准测试中，Wan-S2V方法始终显著优于现有解决方案，在电影级动画场景中展现出卓越的性能。

Conclusion: Wan-S2V成功解决了电影级角色动画的长久挑战，为复杂影视制作提供了高质量的音频驱动动画解决方案，具有广泛的应用潜力。

Abstract: Current state-of-the-art (SOTA) methods for audio-driven character animation demonstrate promising performance for scenarios primarily involving speech and singing. However, they often fall short in more complex film and television productions, which demand sophisticated elements such as nuanced character interactions, realistic body movements, and dynamic camera work. To address this long-standing challenge of achieving film-level character animation, we propose an audio-driven model, which we refere to as Wan-S2V, built upon Wan. Our model achieves significantly enhanced expressiveness and fidelity in cinematic contexts compared to existing approaches. We conducted extensive experiments, benchmarking our method against cutting-edge models such as Hunyuan-Avatar and Omnihuman. The experimental results consistently demonstrate that our approach significantly outperforms these existing solutions. Additionally, we explore the versatility of our method through its applications in long-form video generation and precise video lip-sync editing.

</details>


### [7] [ROSE: Remove Objects with Side Effects in Videos](https://arxiv.org/abs/2508.18633)
*Chenxuan Miao,Yutong Feng,Jianshu Zeng,Zixiang Gao,Hantang Liu,Yunfeng Yan,Donglian Qi,Xi Chen,Bin Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: ROSE是一个视频对象移除框架，专门处理对象移除后的副作用（阴影、反射、光线、半透明和镜像效果），通过合成数据和扩散变换器实现高质量的视频修复。


<details>
  <summary>Details</summary>
Motivation: 现有视频对象移除方法在处理对象副作用（如阴影、反射等）时表现不佳，主要原因是缺乏配对的监督数据来训练模型识别和消除这些副作用。

Method: 使用3D渲染引擎生成大规模合成配对数据集，构建基于扩散变换器的视频修复模型，通过参考整个视频进行基于参考的擦除，并引入额外监督来显式预测受副作用影响的区域。

Result: ROSE在提出的ROSE-Bench基准测试中表现出色，相比现有视频对象擦除模型具有优越性能，并能很好地泛化到真实世界视频场景。

Conclusion: 该框架通过系统性处理对象副作用和合成数据生成，显著提升了视频对象移除的质量，为处理复杂视觉副作用提供了有效解决方案。

Abstract: Video object removal has achieved advanced performance due to the recent success of video generative models. However, when addressing the side effects of objects, e.g., their shadows and reflections, existing works struggle to eliminate these effects for the scarcity of paired video data as supervision. This paper presents ROSE, termed Remove Objects with Side Effects, a framework that systematically studies the object's effects on environment, which can be categorized into five common cases: shadows, reflections, light, translucency and mirror. Given the challenges of curating paired videos exhibiting the aforementioned effects, we leverage a 3D rendering engine for synthetic data generation. We carefully construct a fully-automatic pipeline for data preparation, which simulates a large-scale paired dataset with diverse scenes, objects, shooting angles, and camera trajectories. ROSE is implemented as an video inpainting model built on diffusion transformer. To localize all object-correlated areas, the entire video is fed into the model for reference-based erasing. Moreover, additional supervision is introduced to explicitly predict the areas affected by side effects, which can be revealed through the differential mask between the paired videos. To fully investigate the model performance on various side effect removal, we presents a new benchmark, dubbed ROSE-Bench, incorporating both common scenarios and the five special side effects for comprehensive evaluation. Experimental results demonstrate that ROSE achieves superior performance compared to existing video object erasing models and generalizes well to real-world video scenarios. The project page is https://rose2025-inpaint.github.io/.

</details>


### [8] [SFormer: SNR-guided Transformer for Underwater Image Enhancement from the Frequency Domain](https://arxiv.org/abs/2508.18664)
*Xin Tian,Yingtie Lei,Xiujun Zhang,Zimeng Li,Chi-Man Pun,Xuhang Chen*

Main category: cs.CV

TL;DR: 提出了SFormer方法，在频域使用SNR先验，结合傅里叶注意力SNR先验变换器和频率自适应变换器，显著提升水下图像增强效果


<details>
  <summary>Details</summary>
Motivation: 现有基于SNR先验的水下图像增强方法在空间域存在两个局限：无法有效分离跨通道干扰，以及在放大信息结构同时抑制噪声方面帮助有限

Method: 提出频域SNR先验，将特征分解为幅度和相位谱；设计傅里叶注意力SNR先验变换器(FAST)和频率自适应变换器(FAT)；采用U型架构整合常规RGB流和SNR引导分支

Result: 在UIEB、EUVP和LSUI数据集的4800对图像上训练，PSNR提升3.1dB，SSIM提升0.08，成功恢复水下场景的颜色、纹理和对比度

Conclusion: 频域SNR先验比空间域方法更有效，SFormer方法在定量和定性评估上都优于现有最新方法

Abstract: Recent learning-based underwater image enhancement (UIE) methods have advanced by incorporating physical priors into deep neural networks, particularly using the signal-to-noise ratio (SNR) prior to reduce wavelength-dependent attenuation. However, spatial domain SNR priors have two limitations: (i) they cannot effectively separate cross-channel interference, and (ii) they provide limited help in amplifying informative structures while suppressing noise. To overcome these, we propose using the SNR prior in the frequency domain, decomposing features into amplitude and phase spectra for better channel modulation. We introduce the Fourier Attention SNR-prior Transformer (FAST), combining spectral interactions with SNR cues to highlight key spectral components. Additionally, the Frequency Adaptive Transformer (FAT) bottleneck merges low- and high-frequency branches using a gated attention mechanism to enhance perceptual quality. Embedded in a unified U-shaped architecture, these modules integrate a conventional RGB stream with an SNR-guided branch, forming SFormer. Trained on 4,800 paired images from UIEB, EUVP, and LSUI, SFormer surpasses recent methods with a 3.1 dB gain in PSNR and 0.08 in SSIM, successfully restoring colors, textures, and contrast in underwater scenes.

</details>


### [9] [ColorGS: High-fidelity Surgical Scene Reconstruction with Colored Gaussian Splatting](https://arxiv.org/abs/2508.18696)
*Qun Ji,Peng Li,Mingqiang Wei*

Main category: cs.CV

TL;DR: ColorGS是一个用于内窥镜视频中可变形组织重建的新框架，通过自适应颜色编码和增强变形建模，在保持实时渲染效率的同时实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉细微颜色变化和建模全局变形方面存在局限，3D高斯泼溅的固定颜色分配和线性变形建模无法处理复杂纹理和一致全局变形。

Method: 提出Colored Gaussian Primitives使用动态锚点和可学习颜色参数自适应编码空间变化纹理；设计Enhanced Deformation Model结合时间感知高斯基函数和可学习时间无关变形。

Result: 在DaVinci机器人手术视频和基准数据集上，PSNR达到39.85（比之前方法高1.5），SSIM达到97.25%，同时保持实时渲染效率。

Conclusion: ColorGS通过平衡高保真度和计算实用性，推进了手术场景重建技术，对术中指导和AR/VR应用具有重要意义。

Abstract: High-fidelity reconstruction of deformable tissues from endoscopic videos remains challenging due to the limitations of existing methods in capturing subtle color variations and modeling global deformations. While 3D Gaussian Splatting (3DGS) enables efficient dynamic reconstruction, its fixed per-Gaussian color assignment struggles with intricate textures, and linear deformation modeling fails to model consistent global deformation. To address these issues, we propose ColorGS, a novel framework that integrates spatially adaptive color encoding and enhanced deformation modeling for surgical scene reconstruction. First, we introduce Colored Gaussian Primitives, which employ dynamic anchors with learnable color parameters to adaptively encode spatially varying textures, significantly improving color expressiveness under complex lighting and tissue similarity. Second, we design an Enhanced Deformation Model (EDM) that combines time-aware Gaussian basis functions with learnable time-independent deformations, enabling precise capture of both localized tissue deformations and global motion consistency caused by surgical interactions. Extensive experiments on DaVinci robotic surgery videos and benchmark datasets (EndoNeRF, StereoMIS) demonstrate that ColorGS achieves state-of-the-art performance, attaining a PSNR of 39.85 (1.5 higher than prior 3DGS-based methods) and superior SSIM (97.25\%) while maintaining real-time rendering efficiency. Our work advances surgical scene reconstruction by balancing high fidelity with computational practicality, critical for intraoperative guidance and AR/VR applications.

</details>


### [10] [PseudoMapTrainer: Learning Online Mapping without HD Maps](https://arxiv.org/abs/2508.18788)
*Christian Löwens,Thorben Funke,Jingchao Xie,Alexandru Paul Condurache*

Main category: cs.CV

TL;DR: PseudoMapTrainer是一种无需地面真实高精地图的在线地图构建方法，通过多视角图像和预训练分割网络生成伪标签进行训练


<details>
  <summary>Details</summary>
Motivation: 现有在线地图构建方法依赖昂贵且地理多样性不足的高精地图标注数据，限制了模型的泛化能力

Method: 使用高斯溅射从多相机图像重建道路表面，结合预训练2D分割网络生成伪标签；提出掩码感知分配算法和损失函数处理部分掩码的伪标签

Result: 首次实现了无需地面真实地图的在线地图模型训练，并能利用大规模无标注众包数据进行半监督预训练

Conclusion: 该方法为在线地图构建提供了更经济、可扩展的解决方案，代码已开源

Abstract: Online mapping models show remarkable results in predicting vectorized maps from multi-view camera images only. However, all existing approaches still rely on ground-truth high-definition maps during training, which are expensive to obtain and often not geographically diverse enough for reliable generalization. In this work, we propose PseudoMapTrainer, a novel approach to online mapping that uses pseudo-labels generated from unlabeled sensor data. We derive those pseudo-labels by reconstructing the road surface from multi-camera imagery using Gaussian splatting and semantics of a pre-trained 2D segmentation network. In addition, we introduce a mask-aware assignment algorithm and loss function to handle partially masked pseudo-labels, allowing for the first time the training of online mapping models without any ground-truth maps. Furthermore, our pseudo-labels can be effectively used to pre-train an online model in a semi-supervised manner to leverage large-scale unlabeled crowdsourced data. The code is available at github.com/boschresearch/PseudoMapTrainer.

</details>


### [11] [Harnessing Meta-Learning for Controllable Full-Frame Video Stabilization](https://arxiv.org/abs/2508.18859)
*Muhammad Kashif Ali,Eun Woo Im,Dongjin Kim,Tae Hyun Kim,Vivek Gupta,Haonan Luo,Tianrui Li*

Main category: cs.CV

TL;DR: 这篇论文提出了一种基于快速适配的视频稳傳方法，通过在测试时适配模型来改善像素级合成视频稳傳的性能。该方法利用低级视觉线索和专门的冗勒定位模块，在保持全帧输出的同时显著提升稳定性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 像素级合成视频稳傳方法虽能生成全帧输出，但因为视频运动和内容的多样性，固定参数的模型很难实现稳健的通用性。需要一种能够根据每个输入视频进行快速适配的方法来提升稳定性和质量。

Method: 提出了一种新题的快速适配方法，在推理时利用低级视觉线索来改善输出的稳定性和视觉质量。还提出了冗勒定位模块和针对性适配策略，将适配重点放在高冗勒段落上，以最少的适配步骤实现最大化稳定性。

Result: 通过单次适配就能获得显著的性能提升。在多样化的实际数据集上进行了广泛实验，证明了该方法的多用途性。该方法一贯地改善了各种全帧合成模型的性能，包括定性和定量指标，以及下游应用的结果。

Conclusion: 该方法能够让现代稳傳器超越长期以来的SOTA方法，同时保持现代方法的全帧特性，并提供类似于经典方法的用户控制机制。

Abstract: Video stabilization remains a fundamental problem in computer vision, particularly pixel-level synthesis solutions for video stabilization, which synthesize full-frame outputs, add to the complexity of this task. These methods aim to enhance stability while synthesizing full-frame videos, but the inherent diversity in motion profiles and visual content present in each video sequence makes robust generalization with fixed parameters difficult. To address this, we present a novel method that improves pixel-level synthesis video stabilization methods by rapidly adapting models to each input video at test time. The proposed approach takes advantage of low-level visual cues available during inference to improve both the stability and visual quality of the output. Notably, the proposed rapid adaptation achieves significant performance gains even with a single adaptation pass. We further propose a jerk localization module and a targeted adaptation strategy, which focuses the adaptation on high-jerk segments for maximizing stability with fewer adaptation steps. The proposed methodology enables modern stabilizers to overcome the longstanding SOTA approaches while maintaining the full frame nature of the modern methods, while offering users with control mechanisms akin to classical approaches. Extensive experiments on diverse real-world datasets demonstrate the versatility of the proposed method. Our approach consistently improves the performance of various full-frame synthesis models in both qualitative and quantitative terms, including results on downstream applications.

</details>


### [12] [Can we make NeRF-based visual localization privacy-preserving?](https://arxiv.org/abs/2508.18971)
*Maxime Pietrantoni,Martin Humenberger,Torsten Sattler,Gabriela Csurka*

Main category: cs.CV

TL;DR: 本文提出了一种隐私保护的神经分割场(ppNeSF)方法，用于解决基于NeRF的视觉定位中的隐私泄露问题，并通过新的评估协议验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 基于NeRF的视觉定位方法虽然能提供高质量的新视角合成，但会无意中编码精细的场景细节，在云服务部署时存在隐私泄露风险。

Method: 提出ppNeSF方法，使用分割监督而非RGB图像训练NeRF变体，分割标签通过自监督学习获得，既能保持3D区分性又能模糊可识别细节。

Result: ppNeSF的分割空间可用于精确的视觉定位，取得了最先进的结果，同时有效保护隐私。

Conclusion: 该方法成功解决了NeRF-based视觉定位中的隐私问题，为隐私保护的3D场景表示提供了有效解决方案。

Abstract: Visual localization (VL) is the task of estimating the camera pose in a known scene. VL methods, a.o., can be distinguished based on how they represent the scene, e.g., explicitly through a (sparse) point cloud or a collection of images or implicitly through the weights of a neural network. Recently, NeRF-based methods have become popular for VL. While NeRFs offer high-quality novel view synthesis, they inadvertently encode fine scene details, raising privacy concerns when deployed in cloud-based localization services as sensitive information could be recovered. In this paper, we tackle this challenge on two ends. We first propose a new protocol to assess privacy-preservation of NeRF-based representations. We show that NeRFs trained with photometric losses store fine-grained details in their geometry representations, making them vulnerable to privacy attacks, even if the head that predicts colors is removed. Second, we propose ppNeSF (Privacy-Preserving Neural Segmentation Field), a NeRF variant trained with segmentation supervision instead of RGB images. These segmentation labels are learned in a self-supervised manner, ensuring they are coarse enough to obscure identifiable scene details while remaining discriminativeness in 3D. The segmentation space of ppNeSF can be used for accurate visual localization, yielding state-of-the-art results.

</details>


### [13] [Style4D-Bench: A Benchmark Suite for 4D Stylization](https://arxiv.org/abs/2508.19243)
*Beiqi Chen,Shuai Shao,Haitang Feng,Jianhuang Lai,Jianlou Si,Guangcong Wang*

Main category: cs.CV

TL;DR: 提出了首个4D风格化基准Style4D-Bench，包含评估协议、强基线方法和高质量4D场景数据集，并开发了基于4D高斯溅射的Style4D框架实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 4D风格化是一个新兴领域，缺乏标准化的评估基准和方法，需要建立统一的评估标准和推动该领域发展

Method: 基于4D高斯溅射构建Style4D框架，包含基础4DGS场景表示、风格高斯表示（轻量级MLP）和整体几何保持风格迁移模块，通过对比一致性学习和结构内容保持增强时空一致性

Result: 在Style4D-Bench上进行了广泛实验，Style4D实现了最先进的4D风格化性能，生成细粒度风格细节，具有稳定的时间动态和一致的多视角渲染

Conclusion: Style4D-Bench将成为动态3D场景风格化渲染研究的重要资源，有助于基准测试和推动该领域发展

Abstract: We introduce Style4D-Bench, the first benchmark suite specifically designed for 4D stylization, with the goal of standardizing evaluation and facilitating progress in this emerging area. Style4D-Bench comprises: 1) a comprehensive evaluation protocol measuring spatial fidelity, temporal coherence, and multi-view consistency through both perceptual and quantitative metrics, 2) a strong baseline that make an initial attempt for 4D stylization, and 3) a curated collection of high-resolution dynamic 4D scenes with diverse motions and complex backgrounds. To establish a strong baseline, we present Style4D, a novel framework built upon 4D Gaussian Splatting. It consists of three key components: a basic 4DGS scene representation to capture reliable geometry, a Style Gaussian Representation that leverages lightweight per-Gaussian MLPs for temporally and spatially aware appearance control, and a Holistic Geometry-Preserved Style Transfer module designed to enhance spatio-temporal consistency via contrastive coherence learning and structural content preservation. Extensive experiments on Style4D-Bench demonstrate that Style4D achieves state-of-the-art performance in 4D stylization, producing fine-grained stylistic details with stable temporal dynamics and consistent multi-view rendering. We expect Style4D-Bench to become a valuable resource for benchmarking and advancing research in stylized rendering of dynamic 3D scenes. Project page: https://becky-catherine.github.io/Style4D . Code: https://github.com/Becky-catherine/Style4D-Bench .

</details>


### [14] [VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space](https://arxiv.org/abs/2508.19247)
*Lin Li,Zehuan Huang,Haoran Feng,Gengxiong Zhuang,Rui Chen,Chunchao Guo,Lu Sheng*

Main category: cs.CV

TL;DR: VoxHammer是一种无需训练的3D局部编辑方法，通过在3D潜在空间中保留未编辑区域的特征，实现精确且连贯的编辑效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过编辑多视角图像再重建3D模型，难以精确保持未编辑区域的完整性和整体一致性。

Method: 首先预测3D模型的反转轨迹获取潜在表示和键值令牌，然后在去噪编辑阶段用反转特征替换保留区域的特征，确保一致性。

Result: 在构建的Edit3D-Bench数据集上实验表明，VoxHammer在保持区域3D一致性和整体质量方面显著优于现有方法。

Conclusion: 该方法为合成高质量编辑配对数据提供了可能，为上下文3D生成奠定了数据基础。

Abstract: 3D local editing of specified regions is crucial for game industry and robot interaction. Recent methods typically edit rendered multi-view images and then reconstruct 3D models, but they face challenges in precisely preserving unedited regions and overall coherence. Inspired by structured 3D generative models, we propose VoxHammer, a novel training-free approach that performs precise and coherent editing in 3D latent space. Given a 3D model, VoxHammer first predicts its inversion trajectory and obtains its inverted latents and key-value tokens at each timestep. Subsequently, in the denoising and editing phase, we replace the denoising features of preserved regions with the corresponding inverted latents and cached key-value tokens. By retaining these contextual features, this approach ensures consistent reconstruction of preserved areas and coherent integration of edited parts. To evaluate the consistency of preserved regions, we constructed Edit3D-Bench, a human-annotated dataset comprising hundreds of samples, each with carefully labeled 3D editing regions. Experiments demonstrate that VoxHammer significantly outperforms existing methods in terms of both 3D consistency of preserved regions and overall quality. Our method holds promise for synthesizing high-quality edited paired data, thereby laying the data foundation for in-context 3D generation. See our project page at https://huanngzh.github.io/VoxHammer-Page/.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [15] [Quantum-Circuit-Based Visual Fractal Image Generation in Qiskit and Analytics](https://arxiv.org/abs/2508.18835)
*Hillol Biswas*

Main category: quant-ph

TL;DR: 本文探讨了量子计算在分形图像生成中的应用，特别是Julia集合的生成，利用量子叠加、随机性和纠缠等特性来操纵生成的数据集模式。


<details>
  <summary>Details</summary>
Motivation: 自然界具有量子特性，而分形在微观和宏观可观测实体或现象中展现出迷人的自相似性。量子系统中的概率密度或波函数在不同能量或长度尺度上可能呈现重复的干涉模式。研究旨在探索量子计算如何应用于分形图像生成这一新兴领域。

Method: 采用量子电路构建方法生成Julia集合数据集，利用量子叠加、随机性和纠缠等量子力学基本原理来操纵生成的模式。

Result: 提出了使用量子电路进行分形Julia图像生成的新方法，为量子生成艺术在各个生态系统中的定制化应用开辟了独特的研究方向。

Conclusion: 量子计算在分形图像生成领域具有巨大潜力，特别是在量子生成艺术方面，可以基于量子艺术主题创建令人兴奋的景观，这代表了一个独特的未来研究方向。

Abstract: As nature is ascribed as quantum, the fractals also pose some intriguing appearance which is found in many micro and macro observable entities or phenomena. Fractals show self-similarity across sizes; structures that resemble the entire are revealed when zoomed in. In Quantum systems, the probability density or wavefunction may exhibit recurring interference patterns at various energy or length scales. Fractals are produced by basic iterative rules (such as Mandelbrot or Julia sets), and they provide limitless complexity. Despite its simplicity, the Schr\"odinger equation in quantum mechanics produces incredibly intricate patterns of interference and entanglement, particularly in chaotic quantum systems. Quantum computing, the root where lies to the using the principles of quantum-mechanical phenomenon, when applied in fractal image generation, what outcomes are expected? The paper outlines the generation of a Julia set dataset using an approach coupled with building quantum circuit, highlighting the concepts of superposition, randomness, and entanglement as foundational elements to manipulate the generated dataset patterns. As Quantum computing is finding many application areas, the possibility of using quantum circuits for fractal Julia image generation posits a unique direction of future research where it can be applied to quantum generative arts across various ecosystems with a customised approach, such as producing an exciting landscape based on a quantum art theme.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [16] [RDDM: Practicing RAW Domain Diffusion Model for Real-world Image Restoration](https://arxiv.org/abs/2508.19154)
*Yan Chen,Yi Wen,Wei Li,Junchao Liu,Yong Guo,Jie Hu,Xinghao Chen*

Main category: eess.IV

TL;DR: RDDM是一种直接在RAW域进行图像恢复的端到端扩散模型，通过RAW域VAE和可微分后色调处理模块，解决了传统sRGB域扩散模型的高保真度与真实感生成之间的困境。


<details>
  <summary>Details</summary>
Motivation: 现有的sRGB域扩散模型处理有损的sRGB输入，忽略了传感器RAW图像在许多场景中的可访问性（如边缘设备中的图像和视频捕获），导致性能不佳。RDDM旨在通过直接在RAW域恢复图像来绕过这一限制。

Method: 1) 提出RAW域VAE（RVAE）学习最优潜在表示；2) 设计可微分后色调处理（PTP）模块实现RAW和sRGB空间的联合优化；3) 开发可扩展的退化管道从现有sRGB数据集合成RAW LQ-HQ对；4) 设计可配置多拜耳（CMB）LoRA模块处理不同的RAW模式。

Result: 大量实验证明RDDM优于最先进的sRGB扩散方法，以更少的伪影产生更高保真度的结果。

Conclusion: RDDM通过直接在RAW域操作，成功解决了传统方法的高保真度与真实感之间的权衡问题，为图像恢复提供了新的有效途径。

Abstract: We present the RAW domain diffusion model (RDDM), an end-to-end diffusion model that restores photo-realistic images directly from the sensor RAW data. While recent sRGB-domain diffusion methods achieve impressive results, they are caught in a dilemma between high fidelity and realistic generation. As these models process lossy sRGB inputs and neglect the accessibility of the sensor RAW images in many scenarios, e.g., in image and video capturing in edge devices, resulting in sub-optimal performance. RDDM bypasses this limitation by directly restoring images in the RAW domain, replacing the conventional two-stage image signal processing (ISP) + IR pipeline. However, a simple adaptation of pre-trained diffusion models to the RAW domain confronts the out-of-distribution (OOD) issues. To this end, we propose: (1) a RAW-domain VAE (RVAE) learning optimal latent representations, (2) a differentiable Post Tone Processing (PTP) module enabling joint RAW and sRGB space optimization. To compensate for the deficiency in the dataset, we develop a scalable degradation pipeline synthesizing RAW LQ-HQ pairs from existing sRGB datasets for large-scale training. Furthermore, we devise a configurable multi-bayer (CMB) LoRA module handling diverse RAW patterns such as RGGB, BGGR, etc. Extensive experiments demonstrate RDDM's superiority over state-of-the-art sRGB diffusion methods, yielding higher fidelity results with fewer artifacts.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [17] [The Mind's Eye: A Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation](https://arxiv.org/abs/2508.18569)
*Girish A. Koushik,Fatemeh Nazarieh,Katherine Birch,Shenbin Qian,Diptesh Kanojia*

Main category: cs.CL

TL;DR: 这是一个自我评估的视觉隐喻生成框架，通过源-目标-含义映射和轻量级强化学习来提高隐喻对齐效果。


<details>
  <summary>Details</summary>
Motivation: 视觉隐喻生成需要合并语言理解和视觉一致性，目前的方法在隐喻对齐方面遇到挑战。

Method: 提出两种方法：训练免费流水线（显式分解提示为源-目标-含义映射）和训练基于流水线（使用自我评估奖励进行轻量级强化学习）。

Result: 在测试集上，训练免费方法在分解、CLIP和含义对齐指标上超越GPT-4o和Imagen，训练基于方法也表现优异。用户研究显示用户更偏好GPT-4o，但开源方法在抽象隐喻上更优。

Conclusion: 结构化提示和轻量级RL能够在计算资源有限的情况下有效实现隐喻对齐，与人类偏好的差距主要来自美学质量和采样设置。

Abstract: Visual metaphor generation is a challenging task that aims to generate an image given an input text metaphor. Inherently, it needs language understanding to bind a source concept with a target concept, in a way that preserves meaning while ensuring visual coherence. We propose a self-evaluating visual metaphor generation framework that focuses on metaphor alignment. Our self-evaluation approach combines existing metrics with our newly proposed metaphor decomposition score and a meaning alignment (MA) metric. Within this setup, we explore two novel approaches: a training-free pipeline that explicitly decomposes prompts into source-target-meaning (S-T-M) mapping for image synthesis, and a complementary training-based pipeline that improves alignment using our proposed self-evaluation reward schema, without any large-scale retraining. On the held-out test set, the training-free approach surpasses strong closed baselines (GPT-4o, Imagen) on decomposition, CLIP, and MA scores, with the training-based approach close behind. We evaluate our framework output using a user-facing study, and observed that participants preferred GPT-4o overall, while our training-free pipeline led open-source methods and edged Imagen on abstract metaphors. Our analyses show S-T-M prompting helps longer or more abstract metaphors, with closed models excelling on short, concrete cases; we also observe sensitivity to sampler settings. Overall, structured prompting and lightweight RL perform metaphor alignment well under modest compute, and remaining gaps to human preference appear driven by aesthetics and sampling.

</details>
