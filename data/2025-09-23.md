<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 4]
- [cs.CV](#cs.CV) [Total: 59]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Neural Atlas Graphs for Dynamic Scene Decomposition and Editing](https://arxiv.org/abs/2509.16336)
*Jan Philipp Schneider,Pratik Singh Bisht,Ilya Chugunov,Andreas Kolb,Michael Moeller,Felix Heide*

Main category: cs.GR

TL;DR: 本文提出神经图谱图（NAGs），一种混合高分辨率场景表示方法，结合了神经图谱的2D编辑能力和场景图的3D空间关系建模，在动态场景表示和编辑方面取得突破。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态场景表示上面临编辑性和场景复杂性之间的权衡：神经图谱支持2D编辑但无法处理复杂遮挡，场景图能处理3D空间关系但难以进行视图一致的编辑。

Method: NAGs将场景图中的每个节点表示为视图依赖的神经图谱，既支持2D外观编辑，又能保持3D元素的排序和定位。该方法在测试时进行拟合。

Result: 在Waymo开放数据集上PSNR提升5dB达到SOTA，在DAVIS视频数据集上相比现有方法PSNR提升超过7dB，支持高分辨率环境编辑和反事实驾驶场景创建。

Conclusion: NAGs成功解决了动态场景表示中编辑性与复杂性的平衡问题，在自动驾驶和视频编辑领域具有广泛应用前景。

Abstract: Learning editable high-resolution scene representations for dynamic scenes is an open problem with applications across the domains from autonomous driving to creative editing - the most successful approaches today make a trade-off between editability and supporting scene complexity: neural atlases represent dynamic scenes as two deforming image layers, foreground and background, which are editable in 2D, but break down when multiple objects occlude and interact. In contrast, scene graph models make use of annotated data such as masks and bounding boxes from autonomous-driving datasets to capture complex 3D spatial relationships, but their implicit volumetric node representations are challenging to edit view-consistently. We propose Neural Atlas Graphs (NAGs), a hybrid high-resolution scene representation, where every graph node is a view-dependent neural atlas, facilitating both 2D appearance editing and 3D ordering and positioning of scene elements. Fit at test-time, NAGs achieve state-of-the-art quantitative results on the Waymo Open Dataset - by 5 dB PSNR increase compared to existing methods - and make environmental editing possible in high resolution and visual quality - creating counterfactual driving scenarios with new backgrounds and edited vehicle appearance. We find that the method also generalizes beyond driving scenes and compares favorably - by more than 7 dB in PSNR - to recent matting and video editing baselines on the DAVIS video dataset with a diverse set of human and animal-centric scenes.

</details>


### [2] [PhysHDR: When Lighting Meets Materials and Scene Geometry in HDR Reconstruction](https://arxiv.org/abs/2509.16869)
*Hrishav Bakul Barua,Kalin Stefanov,Ganesh Krishnasamy,KokSheik Wong,Abhinav Dhall*

Main category: cs.GR

TL;DR: PhysHDR是一个基于潜在扩散模型的HDR图像重建方法，通过结合光照、深度信息和材料属性来提升重建质量


<details>
  <summary>Details</summary>
Motivation: 现有的LDR到HDR转换方法缺乏对光照、照明和场景几何的显式建模，且未考虑不同材料（如镜面反射和漫反射表面）与光照的交互差异，限制了HDR图像的重建质量

Method: 提出PhysHDR模型，使用潜在扩散生成框架，在去噪过程中结合光照和深度信息进行条件控制，并引入新颖的损失函数来融入场景表面的材料属性

Result: 实验结果表明PhysHDR在HDR图像重建方面优于多个最新的最先进方法

Conclusion: PhysHDR通过显式建模光照、深度和材料属性，有效提升了HDR图像重建的质量，证明了物理感知方法在计算视觉任务中的重要性

Abstract: Low Dynamic Range (LDR) to High Dynamic Range (HDR) image translation is a fundamental task in many computational vision problems. Numerous data-driven methods have been proposed to address this problem; however, they lack explicit modeling of illumination, lighting, and scene geometry in images. This limits the quality of the reconstructed HDR images. Since lighting and shadows interact differently with different materials, (e.g., specular surfaces such as glass and metal, and lambertian or diffuse surfaces such as wood and stone), modeling material-specific properties (e.g., specular and diffuse reflectance) has the potential to improve the quality of HDR image reconstruction. This paper presents PhysHDR, a simple yet powerful latent diffusion-based generative model for HDR image reconstruction. The denoising process is conditioned on lighting and depth information and guided by a novel loss to incorporate material properties of surfaces in the scene. The experimental results establish the efficacy of PhysHDR in comparison to a number of recent state-of-the-art methods.

</details>


### [3] [High Resolution UDF Meshing via Iterative Networks](https://arxiv.org/abs/2509.17212)
*Federico Stella,Nicolas Talabot,Hieu Le,Pascal Fua*

Main category: cs.GR

TL;DR: 提出了一种迭代神经网络方法，用于从无符号距离场（UDF）中提取更准确和完整的网格表面，特别是在高分辨率下处理噪声和模糊区域。


<details>
  <summary>Details</summary>
Motivation: 当前UDF三角化方法在单个体素内操作，不考虑邻域信息，导致在UDF模糊或噪声区域出现表面缺失和孔洞。需要一种能够整合邻域信息的方法来改进表面恢复。

Method: 开发了一种迭代神经网络，通过多次传递和空间传播信息来逐步改进表面提取。该方法整合新检测的表面、距离值和梯度，在多次迭代中纠正错误并稳定提取过程。

Result: 在多种3D模型上的实验表明，该方法比现有方法产生更准确和完整的网格，特别是在复杂几何形状和高分辨率情况下表现优异。

Conclusion: 提出的迭代方法能够有效解决UDF表面提取中的挑战，特别是在高分辨率下传统方法失败的情况下，实现了更好的表面恢复效果。

Abstract: Unsigned Distance Fields (UDFs) are a natural implicit representation for open surfaces but, unlike Signed Distance Fields (SDFs), are challenging to triangulate into explicit meshes. This is especially true at high resolutions where neural UDFs exhibit higher noise levels, which makes it hard to capture fine details. Most current techniques perform within single voxels without reference to their neighborhood, resulting in missing surface and holes where the UDF is ambiguous or noisy. We show that this can be remedied by performing several passes and by reasoning on previously extracted surface elements to incorporate neighborhood information. Our key contribution is an iterative neural network that does this and progressively improves surface recovery within each voxel by spatially propagating information from increasingly distant neighbors. Unlike single-pass methods, our approach integrates newly detected surfaces, distance values, and gradients across multiple iterations, effectively correcting errors and stabilizing extraction in challenging regions. Experiments on diverse 3D models demonstrate that our method produces significantly more accurate and complete meshes than existing approaches, particularly for complex geometries, enabling UDF surface extraction at higher resolutions where traditional methods fail.

</details>


### [4] [VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video Diffusion Models](https://arxiv.org/abs/2509.17985)
*Geonung Kim,Janghyeok Han,Sunghyun Cho*

Main category: cs.GR

TL;DR: VideoFrom3D是一个从粗糙几何、相机轨迹和参考图像合成高质量3D场景视频的新框架，通过结合图像和视频扩散模型的优势解决复杂场景生成难题


<details>
  <summary>Details</summary>
Motivation: 简化3D图形设计工作流程，实现灵活的设计探索和快速交付物生产，解决现有视频扩散模型在复杂场景中难以同时保证视觉质量、运动和时间一致性的问题

Method: 采用双模块框架：Sparse Anchor-view Generation (SAG)模块使用图像扩散模型生成高质量、跨视角一致的锚点视图；Geometry-guided Generative Inbetweening (GGI)模块基于锚点视图使用视频扩散模型忠实插值中间帧，增强流程相机控制和结构引导

Result: 综合实验表明该方法在多样化和挑战性场景下能生成高质量、风格一致的场景视频，优于简单和扩展的基线方法

Conclusion: VideoFrom3D框架无需3D场景模型和自然图像的配对数据集，成功实现了从粗糙几何到高质量3D场景视频的有效合成

Abstract: In this paper, we propose VideoFrom3D, a novel framework for synthesizing high-quality 3D scene videos from coarse geometry, a camera trajectory, and a reference image. Our approach streamlines the 3D graphic design workflow, enabling flexible design exploration and rapid production of deliverables. A straightforward approach to synthesizing a video from coarse geometry might condition a video diffusion model on geometric structure. However, existing video diffusion models struggle to generate high-fidelity results for complex scenes due to the difficulty of jointly modeling visual quality, motion, and temporal consistency. To address this, we propose a generative framework that leverages the complementary strengths of image and video diffusion models. Specifically, our framework consists of a Sparse Anchor-view Generation (SAG) and a Geometry-guided Generative Inbetweening (GGI) module. The SAG module generates high-quality, cross-view consistent anchor views using an image diffusion model, aided by Sparse Appearance-guided Sampling. Building on these anchor views, GGI module faithfully interpolates intermediate frames using a video diffusion model, enhanced by flow-based camera control and structural guidance. Notably, both modules operate without any paired dataset of 3D scene models and natural images, which is extremely difficult to obtain. Comprehensive experiments show that our method produces high-quality, style-consistent scene videos under diverse and challenging scenarios, outperforming simple and extended baselines.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [5] [From Canopy to Ground via ForestGen3D: Learning Cross-Domain Generation of 3D Forest Structure from Aerial-to-Terrestrial LiDAR](https://arxiv.org/abs/2509.16346)
*Juan Castorena,E. Louise Loudermilk,Scott Pokswinski,Rodman Linn*

Main category: cs.CV

TL;DR: ForestGen3D是一个生成式建模框架，仅使用航空LiDAR输入就能合成高保真度的3D森林结构，通过条件去噪扩散概率模型重建被遮挡的冠层下细节。


<details>
  <summary>Details</summary>
Motivation: 生态系统中的3D结构对生态过程至关重要，但大规模测量成本高昂且不可行。需要准确表征3D植被结构来预测野火、干旱等影响。

Method: 基于条件去噪扩散概率模型，使用配准的ALS/TLS数据训练，引入基于ALS观测凸包的几何包含先验来确保生态合理性。

Result: 在混合针叶林生态系统中，ForestGen3D在树木、样地和景观尺度上产生与TLS参考高度匹配的高保真重建，在几何相似性和生物物理指标方面表现优异。

Conclusion: ForestGen3D成为在仅使用ALS环境中进行生态建模、野火模拟和结构燃料表征的可扩展工具。

Abstract: The 3D structure of living and non-living components in ecosystems plays a critical role in determining ecological processes and feedbacks from both natural and human-driven disturbances. Anticipating the effects of wildfire, drought, disease, or atmospheric deposition depends on accurate characterization of 3D vegetation structure, yet widespread measurement remains prohibitively expensive and often infeasible. We introduce ForestGen3D, a novel generative modeling framework that synthesizes high-fidelity 3D forest structure using only aerial LiDAR (ALS) inputs. ForestGen3D is based on conditional denoising diffusion probabilistic models (DDPMs) trained on co-registered ALS/TLS (terrestrial LiDAR) data. The model learns to generate TLS-like 3D point clouds conditioned on sparse ALS observations, effectively reconstructing occluded sub-canopy detail at scale. To ensure ecological plausibility, we introduce a geometric containment prior based on the convex hull of ALS observations and provide theoretical and empirical guarantees that generated structures remain spatially consistent. We evaluate ForestGen3D at tree, plot, and landscape scales using real-world data from mixed conifer ecosystems, and show that it produces high-fidelity reconstructions that closely match TLS references in terms of geometric similarity and biophysical metrics, such as tree height, DBH, crown diameter and crown volume. Additionally, we demonstrate that the containment property can serve as a practical proxy for generation quality in settings where TLS ground truth is unavailable. Our results position ForestGen3D as a scalable tool for ecological modeling, wildfire simulation, and structural fuel characterization in ALS-only environments.

</details>


### [6] [Introducing Resizable Region Packing Problem in Image Generation, with a Heuristic Solution](https://arxiv.org/abs/2509.16363)
*Hrishikesh Sharma*

Main category: cs.CV

TL;DR: 本文介绍了一种新的图像数据生成问题——可调整锚定区域打包问题(RARP)，该问题源于经典的装箱问题，并提出了一个通用的启发式算法来解决这个问题。


<details>
  <summary>Details</summary>
Motivation: 传统图像数据生成方法存在优化问题，需要将相关对象以合适的大小放置在场景画布的有意义位置。本文旨在解决合成图像数据生成中的区域打包问题。

Method: 提出了一种新颖的启发式算法，采用贪心方法迭代打包区域对，同时遵守优化约束。该算法能够处理任意数量、任意形状、任意位置的区域打包到图像画布中。

Result: 通过实现该算法生成了大规模合成异常检测数据集，视觉检查和解决方案正确性验证证明了算法的有效性。

Conclusion: 随着深度学习中生成建模的兴起和合成数据生成成为主流，新引入的RARP问题预计将在图像科学社区中得到重视。

Abstract: The problem of image data generation in computer vision has traditionally been a harder problem to solve, than discriminative problems. Such data generation entails placing relevant objects of appropriate sizes each, at meaningful location in a scene canvas. There have been two classes of popular approaches to such generation: graphics based, and generative models-based. Optimization problems are known to lurk in the background for both these classes of approaches. In this paper, we introduce a novel, practically useful manifestation of the classical Bin Packing problem in the context of generation of synthetic image data. We conjecture that the newly introduced problem, Resizable Anchored Region Packing(RARP) Problem, is NP-hard, and provide detailed arguments about our conjecture. As a first solution, we present a novel heuristic algorithm that is generic enough and therefore scales and packs arbitrary number of arbitrary-shaped regions at arbitrary locations, into an image canvas. The algorithm follows greedy approach to iteratively pack region pairs in a careful way, while obeying the optimization constraints. The algorithm is validated by an implementation that was used to generate a large-scale synthetic anomaly detection dataset, with highly varying degree of bin packing parameters per image sample i.e. RARP instance. Visual inspection of such data and checking of the correctness of each solution proves the effectiveness of our algorithm. With generative modeling being on rise in deep learning, and synthetic data generation poised to become mainstream, we expect that the newly introduced problem will be valued in the imaging scientific community.

</details>


### [7] [StereoAdapter: Adapting Stereo Depth Estimation to Underwater Scenes](https://arxiv.org/abs/2509.16415)
*Zhengri Wu,Yiran Wang,Yu Wen,Zeyu Zhang,Biao Wu,Hao Tang*

Main category: cs.CV

TL;DR: StereoAdapter是一个参数高效的自监督框架，用于水下立体深度估计，通过LoRA适配的单目基础编码器和循环立体细化模块，在模拟和真实世界基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 水下立体深度估计为机器人任务提供准确的3D几何信息，但现有方法面临两个关键挑战：参数高效地适应大型视觉基础编码器到水下领域，以及紧密融合全局一致但尺度模糊的单目先验与局部度量但光度脆弱的立体对应关系。

Method: 提出StereoAdapter框架，集成LoRA适配的单目基础编码器和循环立体细化模块，引入动态LoRA适配进行高效秩选择，并在合成UW-StereoDepth-40K数据集上进行预训练以增强鲁棒性。

Result: 在模拟和真实世界基准测试中，TartanAir上提升6.11%，SQUID上提升5.12%，BlueROV2机器人实际部署进一步证明了方法的鲁棒性。

Conclusion: StereoAdapter通过参数高效的自监督框架成功解决了水下立体深度估计的关键挑战，在多个基准测试中表现出优越性能。

Abstract: Underwater stereo depth estimation provides accurate 3D geometry for robotics tasks such as navigation, inspection, and mapping, offering metric depth from low-cost passive cameras while avoiding the scale ambiguity of monocular methods. However, existing approaches face two critical challenges: (i) parameter-efficiently adapting large vision foundation encoders to the underwater domain without extensive labeled data, and (ii) tightly fusing globally coherent but scale-ambiguous monocular priors with locally metric yet photometrically fragile stereo correspondences. To address these challenges, we propose StereoAdapter, a parameter-efficient self-supervised framework that integrates a LoRA-adapted monocular foundation encoder with a recurrent stereo refinement module. We further introduce dynamic LoRA adaptation for efficient rank selection and pre-training on the synthetic UW-StereoDepth-40K dataset to enhance robustness under diverse underwater conditions. Comprehensive evaluations on both simulated and real-world benchmarks show improvements of 6.11% on TartanAir and 5.12% on SQUID compared to state-of-the-art methods, while real-world deployment with the BlueROV2 robot further demonstrates the consistent robustness of our approach. Code: https://github.com/AIGeeksGroup/StereoAdapter. Website: https://aigeeksgroup.github.io/StereoAdapter.

</details>


### [8] [Octree Latent Diffusion for Semantic 3D Scene Generation and Completion](https://arxiv.org/abs/2509.16483)
*Xujia Zhang,Brendan Crowe,Christoffer Heckman*

Main category: cs.CV

TL;DR: 本文提出了一种统一的3D语义场景完成、扩展和生成框架Octree Latent Semantic Diffusion，能够在室内外场景中工作，通过双八叉图潜在表示实现高效处理。


<details>
  <summary>Details</summary>
Motivation: 现有的3D场景处理方法通常将完成、扩展和生成问题解耦处理，且多为领域特定模型（如室内vs室外）。为了统一这些技术并提供跨领域兼容性，需要开发一个通用框架。

Method: 采用双八叉图潜在表示，将合成过程分为两个阶段：结构扩散（预测二进制分割信号构建粗粒度八叉树）和潜在语义扩散（生成语义嵌入，通过图VAE解码为体素级语义标签）。推理时使用潜在修复或外推技术进行条件生成。

Result: 实验表明该方法能够从单次LiDAR扫描中实现高质量结构重建、连贯语义和鲁棒完成，并在分布外LiDAR数据上表现出零样本泛化能力。

Conclusion: 基于双八叉图潜在空间的生成式完成方法为真实世界机器人感知任务提供了一种实用且可扩展的替代方案，优于传统的基于回归的流程。

Abstract: The completion, extension, and generation of 3D semantic scenes are an interrelated set of capabilities that are useful for robotic navigation and exploration. Existing approaches seek to decouple these problems and solve them oneoff. Additionally, these approaches are often domain-specific, requiring separate models for different data distributions, e.g. indoor vs. outdoor scenes. To unify these techniques and provide cross-domain compatibility, we develop a single framework that can perform scene completion, extension, and generation in both indoor and outdoor scenes, which we term Octree Latent Semantic Diffusion. Our approach operates directly on an efficient dual octree graph latent representation: a hierarchical, sparse, and memory-efficient occupancy structure. This technique disentangles synthesis into two stages: (i) structure diffusion, which predicts binary split signals to construct a coarse occupancy octree, and (ii) latent semantic diffusion, which generates semantic embeddings decoded by a graph VAE into voxellevel semantic labels. To perform semantic scene completion or extension, our model leverages inference-time latent inpainting, or outpainting respectively. These inference-time methods use partial LiDAR scans or maps to condition generation, without the need for retraining or finetuning. We demonstrate highquality structure, coherent semantics, and robust completion from single LiDAR scans, as well as zero-shot generalization to out-of-distribution LiDAR data. These results indicate that completion-through-generation in a dual octree graph latent space is a practical and scalable alternative to regression-based pipelines for real-world robotic perception tasks.

</details>


### [9] [RLGF: Reinforcement Learning with Geometric Feedback for Autonomous Driving Video Generation](https://arxiv.org/abs/2509.16500)
*Tianyi Yan,Wencheng Han,Xia Zhou,Xueyang Zhang,Kun Zhan,Cheng-zhong Xu,Jianbing Shen*

Main category: cs.CV

TL;DR: 本文提出RLGF方法，通过强化学习结合几何反馈来改进视频扩散模型，解决合成数据在自动驾驶系统中存在的几何失真问题，显著提升3D目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的视频生成模型虽然视觉逼真，但存在细微的几何失真，限制了其在自动驾驶感知任务中的实用性。作者发现使用合成数据与真实数据在3D目标检测性能上存在显著差距。

Method: 引入强化学习与几何反馈（RLGF）方法，包含两个核心组件：高效的潜在空间窗口优化技术和分层几何奖励系统，通过专门的潜在空间感知模型提供多层次的几何对齐奖励。

Result: 在nuScenes数据集上应用RLGF后，几何误差显著降低（VP误差减少21%，深度误差减少57%），3D目标检测mAP提升12.7%，大幅缩小了与真实数据性能的差距。

Conclusion: RLGF提供了一种即插即用的解决方案，能够生成几何上准确可靠的合成视频，为自动驾驶开发提供高质量的数据支持。

Abstract: Synthetic data is crucial for advancing autonomous driving (AD) systems, yet current state-of-the-art video generation models, despite their visual realism, suffer from subtle geometric distortions that limit their utility for downstream perception tasks. We identify and quantify this critical issue, demonstrating a significant performance gap in 3D object detection when using synthetic versus real data. To address this, we introduce Reinforcement Learning with Geometric Feedback (RLGF), RLGF uniquely refines video diffusion models by incorporating rewards from specialized latent-space AD perception models. Its core components include an efficient Latent-Space Windowing Optimization technique for targeted feedback during diffusion, and a Hierarchical Geometric Reward (HGR) system providing multi-level rewards for point-line-plane alignment, and scene occupancy coherence. To quantify these distortions, we propose GeoScores. Applied to models like DiVE on nuScenes, RLGF substantially reduces geometric errors (e.g., VP error by 21\%, Depth error by 57\%) and dramatically improves 3D object detection mAP by 12.7\%, narrowing the gap to real-data performance. RLGF offers a plug-and-play solution for generating geometrically sound and reliable synthetic videos for AD development.

</details>


### [10] [Preconditioned Deformation Grids](https://arxiv.org/abs/2509.18097)
*Julian Kaltheuner,Alexander Oebel,Hannah Droege,Patrick Stotko,Reinhard Klein*

Main category: cs.CV

TL;DR: 提出Preconditioned Deformation Grids方法，直接从无序点云序列估计连贯变形场，无需显式对应关系，在长序列重建中优于现有技术


<details>
  <summary>Details</summary>
Motivation: 现有方法需要多个正则化项或大量训练数据，导致重建精度妥协、过度平滑或对未见物体和运动泛化能力差

Method: 使用多分辨率体素网格捕捉不同空间尺度的整体运动，结合基于网格的Sobolev预处理和Chamfer损失，添加弱等距损失确保时间一致性

Result: 广泛评估表明该方法在长序列上取得优于最先进技术的结果，特别是在变形精度和一致性方面表现优异

Conclusion: 该方法通过灵活的变形表示和有效的优化策略，成功解决了动态表面重建中的关键挑战，为点云序列处理提供了新思路

Abstract: Dynamic surface reconstruction of objects from point cloud sequences is a challenging field in computer graphics. Existing approaches either require multiple regularization terms or extensive training data which, however, lead to compromises in reconstruction accuracy as well as over-smoothing or poor generalization to unseen objects and motions. To address these lim- itations, we introduce Preconditioned Deformation Grids, a novel technique for estimating coherent deformation fields directly from unstructured point cloud sequences without requiring or forming explicit correspondences. Key to our approach is the use of multi-resolution voxel grids that capture the overall motion at varying spatial scales, enabling a more flexible deformation representation. In conjunction with incorporating grid-based Sobolev preconditioning into gradient-based optimization, we show that applying a Chamfer loss between the input point clouds as well as to an evolving template mesh is sufficient to obtain accurate deformations. To ensure temporal consistency along the object surface, we include a weak isometry loss on mesh edges which complements the main objective without constraining deformation fidelity. Extensive evaluations demonstrate that our method achieves superior results, particularly for long sequences, compared to state-of-the-art techniques.

</details>


### [11] [OS-DiffVSR: Towards One-step Latent Diffusion Model for High-detailed Real-world Video Super-Resolution](https://arxiv.org/abs/2509.16507)
*Hanting Li,Huaao Tang,Jianhong Han,Tianxiong Zhou,Jiulong Cui,Haizhen Xie,Yan Chen,Jie Hu*

Main category: cs.CV

TL;DR: 本文提出了一种名为OS-DiffVSR的单步扩散模型，用于真实世界视频超分辨率任务，能够在仅需一步扩散采样的情况下，实现比需要多步采样的现有方法更好的视频质量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的视频超分辨率方法在视频质量和推理效率之间存在权衡，多步扩散过程导致推理效率低下。本文旨在解决这一效率与质量的平衡问题。

Method: 提出了相邻帧对抗训练范式来提升合成视频质量，并设计了多帧融合机制来保持帧间时间一致性并减少视频闪烁。

Result: 在多个流行VSR基准测试上的广泛实验表明，OS-DiffVSR能够实现比需要数十步采样的现有扩散基VSR方法更好的质量。

Conclusion: OS-DiffVSR成功地解决了扩散基VSR方法中效率与质量的权衡问题，通过单步扩散实现了高质量的视频超分辨率重建。

Abstract: Recently, latent diffusion models has demonstrated promising performance in real-world video super-resolution (VSR) task, which can reconstruct high-quality videos from distorted low-resolution input through multiple diffusion steps. Compared to image super-resolution (ISR), VSR methods needs to process each frame in a video, which poses challenges to its inference efficiency. However, video quality and inference efficiency have always been a trade-off for the diffusion-based VSR methods. In this work, we propose One-Step Diffusion model for real-world Video Super-Resolution, namely OS-DiffVSR. Specifically, we devise a novel adjacent frame adversarial training paradigm, which can significantly improve the quality of synthetic videos. Besides, we devise a multi-frame fusion mechanism to maintain inter-frame temporal consistency and reduce the flicker in video. Extensive experiments on several popular VSR benchmarks demonstrate that OS-DiffVSR can even achieve better quality than existing diffusion-based VSR methods that require dozens of sampling steps.

</details>


### [12] [Efficient Rectified Flow for Image Fusion](https://arxiv.org/abs/2509.16549)
*Zirui Wang,Jiayi Zhang,Tianwei Guan,Yuhan Zhou,Xingyuan Li,Minjing Dong,Jinyuan Liu*

Main category: cs.CV

TL;DR: RFfusion是一种基于Rectified Flow的高效一步扩散模型，用于图像融合任务，通过优化采样路径和引入任务特定的VAE架构，在保持高质量融合结果的同时显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的图像融合方法存在计算复杂和推理时间长的问题，限制了其实际应用。本文旨在解决这些效率问题。

Method: 提出RFfusion方法：1）将Rectified Flow引入图像融合任务，实现一步采样；2）设计任务特定的VAE架构，在潜在空间进行融合操作；3）采用两阶段训练策略解决VAE重建目标与图像融合需求之间的差异。

Result: 实验表明，该方法在推理速度和融合质量方面均优于现有最先进方法，实现了高效高质量的图像融合。

Conclusion: RFfusion通过Rectified Flow和任务特定VAE架构的协同设计，成功解决了扩散模型在图像融合中的效率瓶颈，为实际应用提供了可行的解决方案。

Abstract: Image fusion is a fundamental and important task in computer vision, aiming to combine complementary information from different modalities to fuse images. In recent years, diffusion models have made significant developments in the field of image fusion. However, diffusion models often require complex computations and redundant inference time, which reduces the applicability of these methods. To address this issue, we propose RFfusion, an efficient one-step diffusion model for image fusion based on Rectified Flow. We incorporate Rectified Flow into the image fusion task to straighten the sampling path in the diffusion model, achieving one-step sampling without the need for additional training, while still maintaining high-quality fusion results. Furthermore, we propose a task-specific variational autoencoder (VAE) architecture tailored for image fusion, where the fusion operation is embedded within the latent space to further reduce computational complexity. To address the inherent discrepancy between conventional reconstruction-oriented VAE objectives and the requirements of image fusion, we introduce a two-stage training strategy. This approach facilitates the effective learning and integration of complementary information from multi-modal source images, thereby enabling the model to retain fine-grained structural details while significantly enhancing inference efficiency. Extensive experiments demonstrate that our method outperforms other state-of-the-art methods in terms of both inference speed and fusion quality. Code is available at https://github.com/zirui0625/RFfusion.

</details>


### [13] [ST-GS: Vision-Based 3D Semantic Occupancy Prediction with Spatial-Temporal Gaussian Splatting](https://arxiv.org/abs/2509.16552)
*Xiaoyang Yan,Muleilan Pei,Shaojie Shen*

Main category: cs.CV

TL;DR: 提出了一种新颖的时空高斯溅射（ST-GS）框架，通过双模式注意力机制和几何感知时间融合方案，增强基于高斯的方法在3D占用预测中的空间交互和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的3D语义高斯方法在视觉中心自动驾驶中存在多视图空间交互不足和多帧时间一致性有限的问题，需要改进空间和时间建模能力。

Method: 开发了指导信息空间聚合策略（双模式注意力机制）来加强高斯表示的空间交互，并引入几何感知时间融合方案来利用历史上下文改善场景完成的时间连续性。

Result: 在nuScenes占用预测基准上的大量实验表明，该方法不仅达到了最先进的性能，而且相比现有基于高斯的方法具有明显更好的时间一致性。

Conclusion: ST-GS框架有效解决了高斯方法在空间交互和时间一致性方面的限制，为视觉中心自动驾驶提供了更全面的场景理解能力。

Abstract: 3D occupancy prediction is critical for comprehensive scene understanding in vision-centric autonomous driving. Recent advances have explored utilizing 3D semantic Gaussians to model occupancy while reducing computational overhead, but they remain constrained by insufficient multi-view spatial interaction and limited multi-frame temporal consistency. To overcome these issues, in this paper, we propose a novel Spatial-Temporal Gaussian Splatting (ST-GS) framework to enhance both spatial and temporal modeling in existing Gaussian-based pipelines. Specifically, we develop a guidance-informed spatial aggregation strategy within a dual-mode attention mechanism to strengthen spatial interaction in Gaussian representations. Furthermore, we introduce a geometry-aware temporal fusion scheme that effectively leverages historical context to improve temporal continuity in scene completion. Extensive experiments on the large-scale nuScenes occupancy prediction benchmark showcase that our proposed approach not only achieves state-of-the-art performance but also delivers markedly better temporal consistency compared to existing Gaussian-based methods.

</details>


### [14] [V-CECE: Visual Counterfactual Explanations via Conceptual Edits](https://arxiv.org/abs/2509.16567)
*Nikolaos Spanos,Maria Lymperaiou,Giorgos Filandrianos,Konstantinos Thomas,Athanasios Voulodimos,Giorgos Stamou*

Main category: cs.CV

TL;DR: 提出了一种无需训练的即插即用黑盒反事实生成框架，利用预训练图像编辑扩散模型，通过理论保证的最优编辑步骤生成人类级别的反事实解释。


<details>
  <summary>Details</summary>
Motivation: 现有的黑盒反事实生成框架忽视了编辑的语义内容，且过度依赖训练过程来指导生成。

Method: 使用预训练图像编辑扩散模型，基于理论保证的逐步编辑策略，在无需访问分类器内部结构的情况下生成可解释的反事实。

Result: 通过CNN、ViT和LVLM分类器的实验，展示了人类推理与神经网络行为之间的解释差距，并通过全面的人类评估验证了框架的有效性。

Conclusion: 该框架能够生成高质量的人类级别反事实解释，为理解神经网络决策提供了更透明的工具。

Abstract: Recent black-box counterfactual generation frameworks fail to take into account the semantic content of the proposed edits, while relying heavily on training to guide the generation process. We propose a novel, plug-and-play black-box counterfactual generation framework, which suggests step-by-step edits based on theoretical guarantees of optimal edits to produce human-level counterfactual explanations with zero training. Our framework utilizes a pre-trained image editing diffusion model, and operates without access to the internals of the classifier, leading to an explainable counterfactual generation process. Throughout our experimentation, we showcase the explanatory gap between human reasoning and neural model behavior by utilizing both Convolutional Neural Network (CNN), Vision Transformer (ViT) and Large Vision Language Model (LVLM) classifiers, substantiated through a comprehensive human evaluation.

</details>


### [15] [A Novel Metric for Detecting Memorization in Generative Models for Brain MRI Synthesis](https://arxiv.org/abs/2509.16582)
*Antonio Scardace,Lemuel Puglisi,Francesco Guarnera,Sebastiano Battiato,Daniele Ravì*

Main category: cs.CV

TL;DR: DeepSSIM是一种用于量化生成模型中记忆化现象的自监督度量方法，通过将图像投影到学习嵌入空间并强制嵌入之间的余弦相似度与图像空间的真实SSIM分数匹配，有效检测医学图像生成模型中的训练数据泄露风险。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型在医学影像中具有巨大潜力，但存在记忆敏感训练数据的风险，可能导致患者信息泄露。现有方法难以在大规模生成样本中有效检测记忆化现象。

Method: 提出DeepSSIM自监督度量方法：训练模型将图像投影到学习嵌入空间，使嵌入间的余弦相似度与图像空间的SSIM分数匹配；采用结构保持增强技术捕获领域特定的解剖特征，无需精确空间对齐即可可靠估计相似度。

Result: 在脑部MRI合成数据的案例研究中，DeepSSIM相比最先进的记忆化度量方法表现优异，F1分数平均提升+52.03%。实验使用2,195个MRI扫描数据（来自IXI和CoRR数据集）。

Conclusion: DeepSSIM为生成模型中的记忆化检测提供了有效的量化工具，显著优于现有方法，有助于降低医学图像生成中的隐私泄露风险。

Abstract: Deep generative models have emerged as a transformative tool in medical imaging, offering substantial potential for synthetic data generation. However, recent empirical studies highlight a critical vulnerability: these models can memorize sensitive training data, posing significant risks of unauthorized patient information disclosure. Detecting memorization in generative models remains particularly challenging, necessitating scalable methods capable of identifying training data leakage across large sets of generated samples. In this work, we propose DeepSSIM, a novel self-supervised metric for quantifying memorization in generative models. DeepSSIM is trained to: i) project images into a learned embedding space and ii) force the cosine similarity between embeddings to match the ground-truth SSIM (Structural Similarity Index) scores computed in the image space. To capture domain-specific anatomical features, training incorporates structure-preserving augmentations, allowing DeepSSIM to estimate similarity reliably without requiring precise spatial alignment. We evaluate DeepSSIM in a case study involving synthetic brain MRI data generated by a Latent Diffusion Model (LDM) trained under memorization-prone conditions, using 2,195 MRI scans from two publicly available datasets (IXI and CoRR). Compared to state-of-the-art memorization metrics, DeepSSIM achieves superior performance, improving F1 scores by an average of +52.03% over the best existing method. Code and data of our approach are publicly available at the following link: https://github.com/brAIn-science/DeepSSIM.

</details>


### [16] [FakeChain: Exposing Shallow Cues in Multi-Step Deepfake Detection](https://arxiv.org/abs/2509.16602)
*Minji Heo,Simon S. Woo*

Main category: cs.CV

TL;DR: FakeChain是一个用于检测多步混合深度伪造的大规模基准测试，揭示了检测器主要依赖最后一步操作的痕迹而非累积痕迹，导致泛化能力受限。


<details>
  <summary>Details</summary>
Motivation: 现有的深度伪造检测模型主要针对单步伪造训练，而现实中越来越多的深度伪造是通过多种方法顺序合成的混合伪造，这给检测带来了新的技术挑战。

Method: 构建了包含1步、2步和3步伪造的FakeChain基准测试，使用5种最先进的生成器进行合成，分析检测性能和频谱特性。

Result: 检测性能高度依赖于最终操作类型，当与训练分布不同时F1分数最多下降58.83%，表明检测器主要依赖最后阶段痕迹而非累积痕迹。

Conclusion: 检测模型需要显式考虑操作历史和序列，FakeChain等基准测试对于反映现实世界中日益复杂的合成场景至关重要。

Abstract: Multi-step or hybrid deepfakes, created by sequentially applying different deepfake creation methods such as Face-Swapping, GAN-based generation, and Diffusion methods, can pose an emerging and unforseen technical challenge for detection models trained on single-step forgeries. While prior studies have mainly focused on detecting isolated single manipulation, little is known about the detection model behavior under such compositional, hybrid, and complex manipulation pipelines. In this work, we introduce \textbf{FakeChain}, a large-scale benchmark comprising 1-, 2-, and 3-Step forgeries synthesized using five state-of-the-art representative generators. Using this approach, we analyze detection performance and spectral properties across hybrid manipulation at different step, along with varying generator combinations and quality settings. Surprisingly, our findings reveal that detection performance highly depends on the final manipulation type, with F1-score dropping by up to \textbf{58.83\%} when it differs from training distribution. This clearly demonstrates that detectors rely on last-stage artifacts rather than cumulative manipulation traces, limiting generalization. Such findings highlight the need for detection models to explicitly consider manipulation history and sequences. Our results highlight the importance of benchmarks such as FakeChain, reflecting growing synthesis complexity and diversity in real-world scenarios. Our sample code is available here\footnote{https://github.com/minjihh/FakeChain}.

</details>


### [17] [Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation](https://arxiv.org/abs/2509.16630)
*Yue Ma,Zexuan Yan,Hongyu Liu,Hongfa Wang,Heng Pan,Yingqing He,Junkun Yuan,Ailing Zeng,Chengfei Cai,Heung-Yeung Shum,Zhifeng Li,Wei Liu,Linfeng Zhang,Qifeng Chen*

Main category: cs.CV

TL;DR: Follow-Your-Emoji-Faster是一个基于扩散模型的高效肖像动画框架，通过面部关键点驱动，解决了身份保持、表情准确迁移和长期时间一致性的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决肖像动画中身份保持、表情准确迁移和长期时间一致性的核心挑战，同时确保生成效率。

Method: 增强Stable Diffusion，包含表情感知关键点作为显式运动信号和细粒度面部损失；提出渐进生成策略和泰勒插值缓存实现2.6倍无损加速。

Result: 在EmojiBench++基准测试中表现出优越的动画质量和可控性，支持真实人脸、卡通、雕塑和动物等多种肖像类型。

Conclusion: 该方法高效生成高质量动画结果，具有用户友好性和可访问性，代码和数据集将公开。

Abstract: We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework for freestyle portrait animation driven by facial landmarks. The main challenges in this task are preserving the identity of the reference portrait, accurately transferring target expressions, and maintaining long-term temporal consistency while ensuring generation efficiency. To address identity preservation and accurate expression retargeting, we enhance Stable Diffusion with two key components: a expression-aware landmarks as explicit motion signals, which improve motion alignment, support exaggerated expressions, and reduce identity leakage; and a fine-grained facial loss that leverages both expression and facial masks to better capture subtle expressions and faithfully preserve the reference appearance. With these components, our model supports controllable and expressive animation across diverse portrait types, including real faces, cartoons, sculptures, and animals. However, diffusion-based frameworks typically struggle to efficiently generate long-term stable animation results, which remains a core challenge in this task. To address this, we propose a progressive generation strategy for stable long-term animation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless acceleration. These two strategies ensure that our method produces high-quality results efficiently, making it user-friendly and accessible. Finally, we introduce EmojiBench++, a more comprehensive benchmark comprising diverse portraits, driving videos, and landmark sequences. Extensive evaluations on EmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior performance in both animation quality and controllability. The code, training dataset and benchmark will be found in https://follow-your-emoji.github.io/.

</details>


### [18] [DA-Font: Few-Shot Font Generation via Dual-Attention Hybrid Integration](https://arxiv.org/abs/2509.16632)
*Weiran Chen,Guiqian Zhu,Ying Li,Yi Ji,Chunping Liu*

Main category: cs.CV

TL;DR: DA-Font是一个少样本字体生成框架，通过双注意力混合模块解决现有方法存在的笔画错误、伪影和模糊等问题，在保持字符形状准确性和风格纹理方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 少样本字体生成可以显著降低字体设计的人工成本，但现有方法由于字体风格的多样性和复杂性，生成结果常存在可见缺陷，如笔画错误、伪影和模糊。

Method: 提出DA-Font框架，集成双注意力混合模块（DAHM），包含组件注意力块和关系注意力块，并设计了角点一致性损失和弹性网格特征损失来改善几何对齐。

Result: 大量实验表明，DA-Font在不同字体风格和字符上优于现有最先进方法，在结构完整性和局部保真度方面表现出色。

Conclusion: DA-Font通过双注意力机制和新的损失函数设计，有效提升了少样本字体生成的质量，解决了现有方法的缺陷问题。

Abstract: Few-shot font generation aims to create new fonts with a limited number of glyph references. It can be used to significantly reduce the labor cost of manual font design. However, due to the variety and complexity of font styles, the results generated by existing methods often suffer from visible defects, such as stroke errors, artifacts and blurriness. To address these issues, we propose DA-Font, a novel framework which integrates a Dual-Attention Hybrid Module (DAHM). Specifically, we introduce two synergistic attention blocks: the component attention block that leverages component information from content images to guide the style transfer process, and the relation attention block that further refines spatial relationships through interacting the content feature with both original and stylized component-wise representations. These two blocks collaborate to preserve accurate character shapes and stylistic textures. Moreover, we also design a corner consistency loss and an elastic mesh feature loss to better improve geometric alignment. Extensive experiments show that our DA-Font outperforms the state-of-the-art methods across diverse font styles and characters, demonstrating its effectiveness in enhancing structural integrity and local fidelity. The source code can be found at \href{https://github.com/wrchen2001/DA-Font}{\textit{https://github.com/wrchen2001/DA-Font}}.

</details>


### [19] [InstanceAssemble: Layout-Aware Image Generation via Instance Assembling Attention](https://arxiv.org/abs/2509.16691)
*Qiang Xiang,Shuang Sun,Binglei Li,Dejia Song,Huaxia Li,Nemo Chen,Xu Tang,Yao Hu,Junping Zhang*

Main category: cs.CV

TL;DR: 提出InstanceAssemble方法，通过实例组装注意力机制实现布局到图像的精准控制，支持边界框定位和多模态内容控制，在复杂布局条件下达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 当前布局到图像生成方法性能仍不理想，需要更精确的位置控制和内容控制能力

Method: 基于DiT架构，通过实例组装注意力机制整合布局条件，使用轻量级LoRA模块实现灵活适配，支持边界框定位和文本/视觉内容控制

Result: 在提出的Denselayout基准测试中达到最优性能，与多种风格LoRA模块兼容性好

Conclusion: InstanceAssemble方法在布局控制精度和生成质量方面显著优于现有方法，为可控图像生成提供了有效解决方案

Abstract: Diffusion models have demonstrated remarkable capabilities in generating high-quality images. Recent advancements in Layout-to-Image (L2I) generation have leveraged positional conditions and textual descriptions to facilitate precise and controllable image synthesis. Despite overall progress, current L2I methods still exhibit suboptimal performance. Therefore, we propose InstanceAssemble, a novel architecture that incorporates layout conditions via instance-assembling attention, enabling position control with bounding boxes (bbox) and multimodal content control including texts and additional visual content. Our method achieves flexible adaption to existing DiT-based T2I models through light-weighted LoRA modules. Additionally, we propose a Layout-to-Image benchmark, Denselayout, a comprehensive benchmark for layout-to-image generation, containing 5k images with 90k instances in total. We further introduce Layout Grounding Score (LGS), an interpretable evaluation metric to more precisely assess the accuracy of L2I generation. Experiments demonstrate that our InstanceAssemble method achieves state-of-the-art performance under complex layout conditions, while exhibiting strong compatibility with diverse style LoRA modules.

</details>


### [20] [Animalbooth: multimodal feature enhancement for animal subject personalization](https://arxiv.org/abs/2509.16702)
*Chen Liu,Haitao Wu,Kafeng Wang,Xiaowang Zhang*

Main category: cs.CV

TL;DR: AnimalBooth是一个用于个性化动物图像生成的框架，通过Animal Net和自适应注意力模块增强身份保持，并引入频率控制特征集成模块来指导扩散过程，从全局结构到细节纹理逐步生成。


<details>
  <summary>Details</summary>
Motivation: 个性化动物图像生成面临外观线索丰富和形态变化大的挑战，现有方法存在跨域特征错位问题，导致身份漂移。

Method: 提出AnimalBooth框架，包含Animal Net和自适应注意力模块来减轻跨域对齐错误，并引入基于离散余弦变换的频率控制特征集成模块，在潜在空间进行滤波以指导扩散过程。

Result: 在多个基准测试上，AnimalBooth始终优于强基线方法，显著提高了身份保真度和感知质量。

Conclusion: 该方法有效解决了动物图像生成中的身份保持问题，并通过构建AnimalBench高分辨率数据集推动了该领域的研究。

Abstract: Personalized animal image generation is challenging due to rich appearance cues and large morphological variability. Existing approaches often exhibit feature misalignment across domains, which leads to identity drift. We present AnimalBooth, a framework that strengthens identity preservation with an Animal Net and an adaptive attention module, mitigating cross domain alignment errors. We further introduce a frequency controlled feature integration module that applies Discrete Cosine Transform filtering in the latent space to guide the diffusion process, enabling a coarse to fine progression from global structure to detailed texture. To advance research in this area, we curate AnimalBench, a high resolution dataset for animal personalization. Extensive experiments show that AnimalBooth consistently outperforms strong baselines on multiple benchmarks and improves both identity fidelity and perceptual quality.

</details>


### [21] [HyPlaneHead: Rethinking Tri-plane-like Representations in Full-Head Image Synthesis](https://arxiv.org/abs/2509.16748)
*Heyuan Li,Kenkun Liu,Lingteng Qiu,Qi Zuo,Keru Zheng,Zilong Dong,Xiaoguang Han*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的混合平面（hy-plane）表示方法，结合了平面和球形平面的优势，解决了传统tri-plane表示中的特征纠缠、特征图利用效率低和特征穿透等问题，在头部图像合成任务中达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的tri-plane表示在3D感知GAN中存在特征纠缠导致镜像伪影的问题，而球形tri-plane虽然缓解了特征纠缠但存在特征图利用不均和细节生成困难的问题。此外，两种方法都存在特征穿透问题，限制了其性能潜力。

Method: 提出了混合平面表示，结合平面和球形平面的优势；引入近等面积扭曲策略替代传统的theta-phi扭曲，最大化方形特征图的有效利用；生成器合成单通道统一特征图而非多通道分离特征图，消除特征穿透问题。

Result: HyPlaneHead方法在完整头部图像合成任务中实现了最先进的性能，有效解决了传统方法存在的各种问题。

Conclusion: 通过系统分析tri-plane表示的问题并提出创新解决方案，hy-plane表示方法显著提升了3D感知GAN的性能，为相关领域的研究提供了新的思路。

Abstract: Tri-plane-like representations have been widely adopted in 3D-aware GANs for head image synthesis and other 3D object/scene modeling tasks due to their efficiency. However, querying features via Cartesian coordinate projection often leads to feature entanglement, which results in mirroring artifacts. A recent work, SphereHead, attempted to address this issue by introducing spherical tri-planes based on a spherical coordinate system. While it successfully mitigates feature entanglement, SphereHead suffers from uneven mapping between the square feature maps and the spherical planes, leading to inefficient feature map utilization during rendering and difficulties in generating fine image details. Moreover, both tri-plane and spherical tri-plane representations share a subtle yet persistent issue: feature penetration across convolutional channels can cause interference between planes, particularly when one plane dominates the others. These challenges collectively prevent tri-plane-based methods from reaching their full potential. In this paper, we systematically analyze these problems for the first time and propose innovative solutions to address them. Specifically, we introduce a novel hybrid-plane (hy-plane for short) representation that combines the strengths of both planar and spherical planes while avoiding their respective drawbacks. We further enhance the spherical plane by replacing the conventional theta-phi warping with a novel near-equal-area warping strategy, which maximizes the effective utilization of the square feature map. In addition, our generator synthesizes a single-channel unified feature map instead of multiple feature maps in separate channels, thereby effectively eliminating feature penetration. With a series of technical improvements, our hy-plane representation enables our method, HyPlaneHead, to achieve state-of-the-art performance in full-head image synthesis.

</details>


### [22] [DiffEye: Diffusion-Based Continuous Eye-Tracking Data Generation Conditioned on Natural Images](https://arxiv.org/abs/2509.16767)
*Ozgur Kara,Harris Nisar,James M. Rehg*

Main category: cs.CV

TL;DR: DiffEye是一个基于扩散模型的训练框架，用于生成连续且多样化的眼动轨迹，解决了现有方法无法捕捉人类视觉注意力多样性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有眼动预测模型通常基于离散的注视点序列，丢弃了原始轨迹中的丰富信息，且无法反映人类观察同一图像时的变异性。大多数方法预测固定长度的单一扫描路径，与真实视觉注意力的多样性和随机性相冲突。

Method: 提出DiffEye扩散模型框架，使用视觉刺激作为条件输入，引入对应位置嵌入(CPE)组件将空间注视信息与基于图像块的语义特征对齐。利用原始眼动轨迹而非扫描路径进行训练。

Result: DiffEye在扫描路径生成方面达到最先进性能，首次实现了连续眼动轨迹的生成。即使在小数据集上训练，也能产生高质量、真实的眼动模式。生成的轨迹可转换为扫描路径和显著性图，更准确地反映人类视觉注意力分布。

Conclusion: DiffEye是首个在自然图像上使用扩散模型并充分利用原始眼动数据丰富性的方法，成功解决了眼动预测中的多样性和连续性问题。

Abstract: Numerous models have been developed for scanpath and saliency prediction, which are typically trained on scanpaths, which model eye movement as a sequence of discrete fixation points connected by saccades, while the rich information contained in the raw trajectories is often discarded. Moreover, most existing approaches fail to capture the variability observed among human subjects viewing the same image. They generally predict a single scanpath of fixed, pre-defined length, which conflicts with the inherent diversity and stochastic nature of real-world visual attention. To address these challenges, we propose DiffEye, a diffusion-based training framework designed to model continuous and diverse eye movement trajectories during free viewing of natural images. Our method builds on a diffusion model conditioned on visual stimuli and introduces a novel component, namely Corresponding Positional Embedding (CPE), which aligns spatial gaze information with the patch-based semantic features of the visual input. By leveraging raw eye-tracking trajectories rather than relying on scanpaths, DiffEye captures the inherent variability in human gaze behavior and generates high-quality, realistic eye movement patterns, despite being trained on a comparatively small dataset. The generated trajectories can also be converted into scanpaths and saliency maps, resulting in outputs that more accurately reflect the distribution of human visual attention. DiffEye is the first method to tackle this task on natural images using a diffusion model while fully leveraging the richness of raw eye-tracking data. Our extensive evaluation shows that DiffEye not only achieves state-of-the-art performance in scanpath generation but also enables, for the first time, the generation of continuous eye movement trajectories. Project webpage: https://diff-eye.github.io/

</details>


### [23] [MedGS: Gaussian Splatting for Multi-Modal 3D Medical Imaging](https://arxiv.org/abs/2509.16806)
*Kacper Marzol,Ignacy Kolton,Weronika Smolak-Dyżewska,Joanna Kaleta,Marcin Mazur,Przemysław Spurek*

Main category: cs.CV

TL;DR: MedGS是一个半监督神经隐式表面重建框架，使用高斯泼溅(GS)插值机制处理多模态3D医学影像数据，实现高效的帧间插值和高质量表面重建。


<details>
  <summary>Details</summary>
Motivation: 传统方法在医学影像表面重建和帧间插值中面临图像噪声和帧间信息不完整的限制，需要更鲁棒和高效的解决方案。

Method: 将医学影像数据表示为3D空间中连续的2D帧，使用基于高斯的分布建模，采用高斯泼溅插值机制进行半监督学习。

Result: MedGS比传统神经隐式方法训练更高效，GS显式表示增强了噪声鲁棒性，支持灵活编辑，并能精确建模复杂解剖结构且减少伪影。

Conclusion: MedGS框架非常适合医学成像中可扩展和实用的应用，为多模态3D医学影像处理提供了有效的解决方案。

Abstract: Multi-modal three-dimensional (3D) medical imaging data, derived from ultrasound, magnetic resonance imaging (MRI), and potentially computed tomography (CT), provide a widely adopted approach for non-invasive anatomical visualization. Accurate modeling, registration, and visualization in this setting depend on surface reconstruction and frame-to-frame interpolation. Traditional methods often face limitations due to image noise and incomplete information between frames. To address these challenges, we present MedGS, a semi-supervised neural implicit surface reconstruction framework that employs a Gaussian Splatting (GS)-based interpolation mechanism. In this framework, medical imaging data are represented as consecutive two-dimensional (2D) frames embedded in 3D space and modeled using Gaussian-based distributions. This representation enables robust frame interpolation and high-fidelity surface reconstruction across imaging modalities. As a result, MedGS offers more efficient training than traditional neural implicit methods. Its explicit GS-based representation enhances noise robustness, allows flexible editing, and supports precise modeling of complex anatomical structures with fewer artifacts. These features make MedGS highly suitable for scalable and practical applications in medical imaging.

</details>


### [24] [ISCS: Parameter-Guided Channel Ordering and Grouping for Learned Image Compression](https://arxiv.org/abs/2509.16853)
*Jinhao Wang,Cihan Ruan,Nam Ling,Wei Wang,Wei Jiang*

Main category: cs.CV

TL;DR: 提出了一种基于预训练VAE-LIC模型的通用、数据集无关的方法，利用权重方差、偏置大小和成对相关性等内在参数统计量来识别和组织重要通道，发现了不变显著通道空间（ISCS）结构，并通过确定性通道排序和分组策略实现切片并行解码，提高编码效率。


<details>
  <summary>Details</summary>
Motivation: 现有学习图像压缩研究中，只有少量潜在通道对重建至关重要，而许多其他通道携带有限信息。利用这种不平衡性可以提高编码和计算效率，但现有方法依赖昂贵的数据集特定消融测试，且通常孤立分析通道而忽略其相互依赖性。

Method: 利用预训练VAE-LIC模型的权重方差、偏置大小和成对相关性等内在参数统计量来估计通道重要性，识别出不变显著通道空间（ISCS），其中显著核心通道捕获主导结构，显著辅助通道提供补充细节，并基于此提出确定性通道排序和分组策略。

Result: 在多个LIC架构上的实验表明，该方法有效降低了比特率和计算量，同时保持了重建质量，为现有学习压缩框架提供了实用且模块化的增强。

Conclusion: 提出的方法能够通用地识别和组织LIC模型中的重要通道，通过ISCS结构和切片并行解码策略，实现了编码效率和计算效率的提升，为学习图像压缩提供了有效的优化方案。

Abstract: Prior studies in learned image compression (LIC) consistently show that only a small subset of latent channels is critical for reconstruction, while many others carry limited information. Exploiting this imbalance could improve both coding and computational efficiency, yet existing approaches often rely on costly, dataset-specific ablation tests and typically analyze channels in isolation, ignoring their interdependencies.   We propose a generalizable, dataset-agnostic method to identify and organize important channels in pretrained VAE-based LIC models. Instead of brute-force empirical evaluations, our approach leverages intrinsic parameter statistics-weight variances, bias magnitudes, and pairwise correlations-to estimate channel importance. This analysis reveals a consistent organizational structure, termed the Invariant Salient Channel Space (ISCS), where Salient-Core channels capture dominant structures and Salient-Auxiliary channels provide complementary details. Building on ISCS, we introduce a deterministic channel ordering and grouping strategy that enables slice-parallel decoding, reduces redundancy, and improves bitrate efficiency.   Experiments across multiple LIC architectures demonstrate that our method effectively reduces bitrate and computation while maintaining reconstruction quality, providing a practical and modular enhancement to existing learned compression frameworks.

</details>


### [25] [ConfidentSplat: Confidence-Weighted Depth Fusion for Accurate 3D Gaussian Splatting SLAM](https://arxiv.org/abs/2509.16863)
*Amanuel T. Dufera,Yuan-Li Cai*

Main category: cs.CV

TL;DR: ConfidentSplat是一个基于3D高斯泼溅的RGB-only SLAM系统，通过置信度加权融合机制解决现有方法中深度估计不可靠导致的几何不准确问题。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-only 3DGS SLAM方法由于深度估计不可靠存在几何不准确性，需要一种能够自适应融合多视角几何和单目先验的可靠深度估计方法。

Method: 采用置信度加权融合机制，结合多视角几何和Omnidata ViT单目先验，基于多视角几何一致性生成可靠性估计，指导可变形3DGS地图的优化，并集成DROID-SLAM风格的前端和后端优化。

Result: 在TUM-RGBD、ScanNet等标准基准和移动数据集上验证，相比基线方法在重建精度（L1深度误差）和新视角合成质量（PSNR、SSIM、LPIPS）方面有显著提升。

Conclusion: ConfidentSplat证明了基于置信度的传感器融合方法在推进密集视觉SLAM技术发展中的有效性。

Abstract: We introduce ConfidentSplat, a novel 3D Gaussian Splatting (3DGS)-based SLAM system for robust, highfidelity RGB-only reconstruction. Addressing geometric inaccuracies in existing RGB-only 3DGS SLAM methods that stem from unreliable depth estimation, ConfidentSplat incorporates a core innovation: a confidence-weighted fusion mechanism. This mechanism adaptively integrates depth cues from multiview geometry with learned monocular priors (Omnidata ViT), dynamically weighting their contributions based on explicit reliability estimates-derived predominantly from multi-view geometric consistency-to generate high-fidelity proxy depth for map supervision. The resulting proxy depth guides the optimization of a deformable 3DGS map, which efficiently adapts online to maintain global consistency following pose updates from a DROID-SLAM-inspired frontend and backend optimizations (loop closure, global bundle adjustment). Extensive validation on standard benchmarks (TUM-RGBD, ScanNet) and diverse custom mobile datasets demonstrates significant improvements in reconstruction accuracy (L1 depth error) and novel view synthesis fidelity (PSNR, SSIM, LPIPS) over baselines, particularly in challenging conditions. ConfidentSplat underscores the efficacy of principled, confidence-aware sensor fusion for advancing state-of-the-art dense visual SLAM.

</details>


### [26] [$\mathtt{M^3VIR}$: A Large-Scale Multi-Modality Multi-View Synthesized Benchmark Dataset for Image Restoration and Content Creation](https://arxiv.org/abs/2509.16873)
*Yuanzhi Li,Lebin Zhou,Nam Ling,Zhenghao Chen,Wei Wang,Wei Jiang*

Main category: cs.CV

TL;DR: 该论文提出了M³VIR数据集，这是一个大规模多模态多视图数据集，专门用于解决游戏内容生成和恢复任务中现有数据集的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集通常局限于特定领域或依赖人工降质，无法准确捕捉游戏内容的独特特征，且缺乏可控视频生成的基准。

Method: 使用Unreal Engine 5渲染80个场景的高保真游戏内容，提供真实的LR-HR配对和多视图帧，包括M³VIR_MR用于超分辨率、新视图合成等任务，以及M³VIR_MS用于可控视频生成研究。

Result: 建立了多个最先进SR和NVS方法的性能基准，为可控视频生成领域提供了首个基准数据集。

Conclusion: M³VIR数据集的发布将促进AI驱动的恢复、压缩和可控内容生成研究，为下一代云游戏和娱乐应用提供支持。

Abstract: The gaming and entertainment industry is rapidly evolving, driven by immersive experiences and the integration of generative AI (GAI) technologies. Training such models effectively requires large-scale datasets that capture the diversity and context of gaming environments. However, existing datasets are often limited to specific domains or rely on artificial degradations, which do not accurately capture the unique characteristics of gaming content. Moreover, benchmarks for controllable video generation remain absent.   To address these limitations, we introduce $\mathtt{M^3VIR}$, a large-scale, multi-modal, multi-view dataset specifically designed to overcome the shortcomings of current resources. Unlike existing datasets, $\mathtt{M^3VIR}$ provides diverse, high-fidelity gaming content rendered with Unreal Engine 5, offering authentic ground-truth LR-HR paired and multi-view frames across 80 scenes in 8 categories. It includes $\mathtt{M^3VIR\_MR}$ for super-resolution (SR), novel view synthesis (NVS), and combined NVS+SR tasks, and $\mathtt{M^3VIR\_{MS}}$, the first multi-style, object-level ground-truth set enabling research on controlled video generation. Additionally, we benchmark several state-of-the-art SR and NVS methods to establish performance baselines. While no existing approaches directly handle controlled video generation, $\mathtt{M^3VIR}$ provides a benchmark for advancing this area. By releasing the dataset, we aim to facilitate research in AI-powered restoration, compression, and controllable content generation for next-generation cloud gaming and entertainment.

</details>


### [27] [PRISM: Precision-Recall Informed Data-Free Knowledge Distillation via Generative Diffusion](https://arxiv.org/abs/2509.16897)
*Xuewan He,Jielei Wang,Zihan Cheng,Yuchen Su,Shiyue Huang,Guoming Lu*

Main category: cs.CV

TL;DR: PRISM是一种基于精度-召回率的数据自由知识蒸馏方法，通过能量引导分布对齐和多样化提示工程来解决大规模图像合成中的模式崩溃问题，提升知识迁移效果。


<details>
  <summary>Details</summary>
Motivation: 现有数据自由知识蒸馏方法在小规模图像上表现良好，但在合成大规模图像时会出现模式崩溃，导致知识迁移受限。直接使用现成扩散模型生成数据集面临精度-召回率挑战：1）确保合成数据与真实分布对齐；2）确保覆盖真实分布流形。

Method: 提出PRISM方法，包含两个核心组件：1）能量引导分布对齐，避免生成分布外样本；2）多样化提示工程，增强对真实分布流形的覆盖。

Result: 在多个大规模图像数据集上的实验表明PRISM具有优越性，使用PRISM训练的模型展现出强大的领域泛化能力。

Conclusion: PRISM通过解决精度-召回率挑战，有效提升了数据自由知识蒸馏在大规模图像上的性能，为无真实数据情况下的知识迁移提供了有效解决方案。

Abstract: Data-free knowledge distillation (DFKD) transfers knowledge from a teacher to a student without access to the real in-distribution (ID) data. While existing methods perform well on small-scale images, they suffer from mode collapse when synthesizing large-scale images, resulting in limited knowledge transfer. Recently, leveraging advanced generative models to synthesize photorealistic images has emerged as a promising alternative. Nevertheless, directly using off-the-shelf diffusion to generate datasets faces the precision-recall challenges: 1) ensuring synthetic data aligns with the real distribution, and 2) ensuring coverage of the real ID manifold. In response, we propose PRISM, a precision-recall informed synthesis method. Specifically, we introduce Energy-guided Distribution Alignment to avoid the generation of out-of-distribution samples, and design the Diversified Prompt Engineering to enhance coverage of the real ID manifold. Extensive experiments on various large-scale image datasets demonstrate the superiority of PRISM. Moreover, we demonstrate that models trained with PRISM exhibit strong domain generalization.

</details>


### [28] [Parameter-efficient fine-tuning (PEFT) of Vision Foundation Models for Atypical Mitotic Figure Classification](https://arxiv.org/abs/2509.16935)
*Lavish Ramchandani,Gunjan Deotale,Dev Kumar Das*

Main category: cs.CV

TL;DR: 该研究使用大型视觉基础模型（Virchow、Virchow2、UNI）结合LoRA参数高效微调技术，在MIDOG 2025挑战赛中实现了88.37%的平衡准确率，展示了基础模型在非典型有丝分裂分类中的潜力。


<details>
  <summary>Details</summary>
Motivation: 非典型有丝分裂（AMFs）的检测面临形态特征细微、类别不平衡和病理学家间观察差异等挑战，需要开发更准确可靠的自动分类方法。

Method: 采用大型视觉基础模型（Virchow、Virchow2、UNI）结合LoRA参数高效微调技术，通过不同LoRA秩和数据集划分策略进行系统实验，并使用三折交叉验证集成方法。

Result: 最佳方法（Virchow+LoRA秩8+三折集成）在初步测试集上达到88.37%的平衡准确率，在挑战赛中排名并列第9位。

Conclusion: 基础模型结合高效适应策略在非典型有丝分裂分类中具有良好前景，但需要在特异性和领域泛化方面进一步改进。

Abstract: Atypical mitotic figures (AMFs) are rare abnormal cell divisions associated with tumor aggressiveness and poor prognosis. Their detection remains a significant challenge due to subtle morphological cues, class imbalance, and inter-observer variability among pathologists. The MIDOG 2025 challenge introduced a dedicated track for atypical mitosis classification, enabling systematic evaluation of deep learning methods. In this study, we investigated the use of large vision foundation models, including Virchow, Virchow2, and UNI, with Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning. We conducted extensive experiments with different LoRA ranks, as well as random and group-based data splits, to analyze robustness under varied conditions. Our best approach, Virchow with LoRA rank 8 and ensemble of three-fold cross-validation, achieved a balanced accuracy of 88.37% on the preliminary test set, ranking joint 9th in the challenge leaderboard. These results highlight the promise of foundation models with efficient adaptation strategies for the classification of atypical mitosis, while underscoring the need for improvements in specificity and domain generalization.

</details>


### [29] [Prototype-Based Pseudo-Label Denoising for Source-Free Domain Adaptation in Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2509.16942)
*Bin Wang,Fei Deng,Zeyu Chen,Zhicheng Yu,Yiguang Liu*

Main category: cs.CV

TL;DR: ProSFDA是一个原型引导的无源域自适应框架，用于遥感图像语义分割，通过原型加权伪标签和原型对比策略解决伪标签噪声问题


<details>
  <summary>Details</summary>
Motivation: 解决无源域自适应中由于目标域缺乏真实标签导致的伪标签噪声问题，这些噪声阻碍了有效缓解域偏移

Method: 采用原型加权伪标签进行可靠的自训练，并引入原型对比策略促进同类特征的聚合，从而学习具有判别性的目标域表示

Result: 大量实验表明该方法显著优于现有方法

Conclusion: ProSFDA框架通过原型引导机制有效解决了SFDA中的伪标签噪声问题，提升了域自适应性能

Abstract: Source-Free Domain Adaptation (SFDA) enables domain adaptation for semantic segmentation of Remote Sensing Images (RSIs) using only a well-trained source model and unlabeled target domain data. However, the lack of ground-truth labels in the target domain often leads to the generation of noisy pseudo-labels. Such noise impedes the effective mitigation of domain shift (DS). To address this challenge, we propose ProSFDA, a prototype-guided SFDA framework. It employs prototype-weighted pseudo-labels to facilitate reliable self-training (ST) under pseudo-labels noise. We, in addition, introduce a prototype-contrast strategy that encourages the aggregation of features belonging to the same class, enabling the model to learn discriminative target domain representations without relying on ground-truth supervision. Extensive experiments show that our approach substantially outperforms existing methods.

</details>


### [30] [Catching the Details: Self-Distilled RoI Predictors for Fine-Grained MLLM Perception](https://arxiv.org/abs/2509.16944)
*Yuheng Shi,Xiaohuan Pei,Minjing Dong,Chang Xu*

Main category: cs.CV

TL;DR: 本文提出了一种高效、无需标注的自蒸馏区域提议网络（SD-RPN），解决了多模态大语言模型（MLLMs）在高分辨率图像处理中的计算效率与准确性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型需要高分辨率视觉信息进行细粒度感知，但处理整个高分辨率图像计算成本过高。现有的基于感兴趣区域（RoI）的方法存在训练依赖大规模标注数据或训练免费方法计算效率低且准确性不足的问题。

Method: 提出SD-RPN，通过将MLLM中间层的噪声注意力图转换为高质量伪RoI标签，训练轻量级区域提议网络。该网络使用MLLM中间层特征在单次前向传播中预测RoI，将RoI识别与自回归生成解耦。

Result: 在LLaVA-1.5架构上验证，仅使用少量（如10K）问答对训练，在TextVQA、DocVQA和V-Star等未见基准上实现了超过10%的绝对准确率提升。

Conclusion: 该方法为增强MLLMs的细粒度感知提供了实用且可扩展的解决方案，无需昂贵的监督或完整模型微调。

Abstract: Multimodal Large Language Models (MLLMs) require high-resolution visual information to perform fine-grained perception, yet processing entire high-resolution images is computationally prohibitive. While recent methods leverage a Region-of-Interest (RoI) mechanism to focus on salient areas, they typically present a difficult trade-off: training-based approaches depend on large-scale annotated datasets, while training-free methods that utilize the model's internal attention are computationally inefficient and less accurate, requiring either multi-pass prefill stages or reliance on the slow auto-regressive decoding process. In this paper, we propose an efficient, annotation-free Self-Distilled Region Proposal Network (SD-RPN) that resolves this trade-off. The SD-RPN is built around a pipeline that transforms the noisy attention maps from the MLLM's middle layers into high-quality pseudo-RoI labels by explicitly denoising the signal and resolving ambiguity. We use these labels to train a lightweight Region Proposal Network (RPN) that learns a more precise localization. This RPN is also highly efficient, predicting the RoI in a single forward pass using features from the MLLM's middle layers, decoupling RoI identification from the auto-regressive generation and avoiding costly multi-pass operations.To validate our approach, we integrate the framework into the LLaVA-1.5 architecture. Despite being trained on only a few (e.g. 10K) question-answer pairs, our method demonstrates exceptional data efficiency and generalization, achieving over a 10% absolute accuracy improvement on unseen benchmarks, including TextVQA, DocVQA, and V-Star. Our work presents a practical and scalable solution for enhancing the fine-grained perception of MLLMs without requiring costly supervision or full model fine-tuning. Code is available at https://github.com/YuHengsss/SD-RPN.

</details>


### [31] [VidCLearn: A Continual Learning Approach for Text-to-Video Generation](https://arxiv.org/abs/2509.16956)
*Luca Zanchetta,Lorenzo Papa,Luca Maiano,Irene Amerini*

Main category: cs.CV

TL;DR: VidCLearn是一个用于基于扩散的文本到视频生成的持续学习框架，通过师生架构和生成重放来解决现有模型难以融入新数据的问题。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频生成模型依赖静态知识，难以在不重新训练的情况下融入新数据，限制了模型的适应性和扩展性。

Method: 采用师生架构，学生模型通过新文本-视频对增量更新，教师模型通过生成重放保留已学知识；引入时间一致性损失增强运动平滑性，视频检索模块在推理时提供结构指导。

Result: 实验结果表明VidCLearn在视觉质量、语义对齐和时间一致性方面优于基线方法。

Conclusion: VidCLearn框架在保持满意生成性能的同时，比现有模型计算效率更高，解决了文本到视频生成中的持续学习挑战。

Abstract: Text-to-video generation is an emerging field in generative AI, enabling the creation of realistic, semantically accurate videos from text prompts. While current models achieve impressive visual quality and alignment with input text, they typically rely on static knowledge, making it difficult to incorporate new data without retraining from scratch. To address this limitation, we propose VidCLearn, a continual learning framework for diffusion-based text-to-video generation. VidCLearn features a student-teacher architecture where the student model is incrementally updated with new text-video pairs, and the teacher model helps preserve previously learned knowledge through generative replay. Additionally, we introduce a novel temporal consistency loss to enhance motion smoothness and a video retrieval module to provide structural guidance at inference. Our architecture is also designed to be more computationally efficient than existing models while retaining satisfactory generation performance. Experimental results show VidCLearn's superiority over baseline methods in terms of visual quality, semantic alignment, and temporal coherence.

</details>


### [32] [Penalizing Boundary Activation for Object Completeness in Diffusion Models](https://arxiv.org/abs/2509.16968)
*Haoyang Xu,Tianhao Zhao,Sibei Yang,Yutian Li*

Main category: cs.CV

TL;DR: 本文分析了扩散模型中物体显示不完整的问题，发现RandomCrop数据增强是主要原因，并提出了一种无需训练的训练时解决方案来改善物体完整性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在文本到图像生成中表现出色，但普遍存在物体显示不完整的问题，这影响了模型在下游应用中的性能。

Method: 提出了一种训练免费的解决方案，在早期去噪步骤中对图像边界的激活值进行惩罚，该方法可轻松应用于预训练的Stable Diffusion模型，计算开销极小。

Result: 大量实验证明该方法有效，显著提高了物体完整性和图像质量。

Conclusion: 通过分析发现RandomCrop是导致物体不完整的主要因素，提出的边界激活惩罚方法能有效解决这一问题，且具有实用性和高效性。

Abstract: Diffusion models have emerged as a powerful technique for text-to-image (T2I) generation, creating high-quality, diverse images across various domains. However, a common limitation in these models is the incomplete display of objects, where fragments or missing parts undermine the model's performance in downstream applications. In this study, we conduct an in-depth analysis of the incompleteness issue and reveal that the primary factor behind incomplete object generation is the usage of RandomCrop during model training. This widely used data augmentation method, though enhances model generalization ability, disrupts object continuity during training. To address this, we propose a training-free solution that penalizes activation values at image boundaries during the early denoising steps. Our method is easily applicable to pre-trained Stable Diffusion models with minimal modifications and negligible computational overhead. Extensive experiments demonstrate the effectiveness of our method, showing substantial improvements in object integrity and image quality.

</details>


### [33] [VCE: Safe Autoregressive Image Generation via Visual Contrast Exploitation](https://arxiv.org/abs/2509.16986)
*Feng Han,Chao Gong,Zhipeng Wei,Jingjing Chen,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本文提出了Visual Contrast Exploitation (VCE)框架，用于保护自回归图像生成模型免受不安全概念（如NSFW内容、侵权风格）的影响，填补了现有概念擦除方法主要针对扩散模型的空白。


<details>
  <summary>Details</summary>
Motivation: 自回归图像生成模型（如GPT-4o、LlamaGen）能够生成逼真图像，但也可能产生NSFW内容或侵权风格，引发版权和伦理担忧。现有概念擦除方法主要针对扩散模型，不适用于逐token生成的自回归模型。

Method: VCE框架包含：(1)创新的对比图像对构建范式，精确解耦不安全概念与其相关语义；(2)基于DPO的精细化训练方法，增强模型识别和利用图像对中视觉对比特征的能力，实现精确概念擦除。

Result: 在三个挑战性任务（艺术家风格擦除、显式内容擦除、对象移除）上的综合实验表明，该方法能有效保护模型，在擦除不安全概念的同时保持无关安全概念的完整性，达到最先进水平。

Conclusion: VCE框架成功解决了自回归图像生成模型的安全保护问题，为模型的安全部署提供了有效解决方案，代码和模型已开源。

Abstract: Recently, autoregressive image generation models have wowed audiences with their remarkable capability in creating surprisingly realistic images. Models such as GPT-4o and LlamaGen can not only produce images that faithfully mimic renowned artistic styles like Ghibli, Van Gogh, or Picasso, but also potentially generate Not-Safe-For-Work (NSFW) content, raising significant concerns regarding copyright infringement and ethical use. Despite these concerns, methods to safeguard autoregressive text-to-image models remain underexplored. Previous concept erasure methods, primarily designed for diffusion models that operate in denoising latent space, are not directly applicable to autoregressive models that generate images token by token. To address this critical gap, we propose Visual Contrast Exploitation (VCE), a novel framework comprising: (1) an innovative contrastive image pair construction paradigm that precisely decouples unsafe concepts from their associated content semantics, and (2) a sophisticated DPO-based training approach that enhances the model's ability to identify and leverage visual contrastive features from image pairs, enabling precise concept erasure. Our comprehensive experiments across three challenging tasks-artist style erasure, explicit content erasure, and object removal-demonstrate that our method effectively secures the model, achieving state-of-the-art results while erasing unsafe concepts and maintaining the integrity of unrelated safe concepts. The code and models are available at https://github.com/Maplebb/VCE.

</details>


### [34] [When Color-Space Decoupling Meets Diffusion for Adverse-Weather Image Restoration](https://arxiv.org/abs/2509.17024)
*Wenxuan Fang,Jili Fan,Chao Wang,Xiantao Hu,Jiangwei Weng,Ying Tai,Jian Yang,Jun Li*

Main category: cs.CV

TL;DR: LCDiff是一个新颖的恶劣天气图像恢复框架，通过亮度-色度分解网络和亮度引导扩散模型，无需显式退化提示即可有效处理各种天气退化，并在新构建的DriveWeather数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统任务特定方法难以泛化到未见或复杂退化类型，而基于提示学习的方法过度依赖视觉语言模型的退化估计能力，导致恢复结果不一致。需要一种不依赖显式退化提示的鲁棒恢复方法。

Method: 提出LCDiff框架：1）亮度-色度分解网络（LCDN）在YCbCr色彩空间中分别处理退化相关亮度和退化不变色度；2）亮度引导扩散模型（LGDM）利用亮度信息作为引导条件，结合动态时间步长损失优化去噪网络。

Result: 在DriveWeather数据集上的大量实验表明，该方法超越了现有最先进方法，为恶劣天气图像恢复设立了新的基准。

Conclusion: LCDiff通过亮度-色度分解和亮度引导扩散的有效结合，实现了不依赖显式退化提示的高质量图像恢复，为解决恶劣天气图像恢复问题提供了新的解决方案。

Abstract: Adverse Weather Image Restoration (AWIR) is a highly challenging task due to the unpredictable and dynamic nature of weather-related degradations. Traditional task-specific methods often fail to generalize to unseen or complex degradation types, while recent prompt-learning approaches depend heavily on the degradation estimation capabilities of vision-language models, resulting in inconsistent restorations. In this paper, we propose \textbf{LCDiff}, a novel framework comprising two key components: \textit{Lumina-Chroma Decomposition Network} (LCDN) and \textit{Lumina-Guided Diffusion Model} (LGDM). LCDN processes degraded images in the YCbCr color space, separately handling degradation-related luminance and degradation-invariant chrominance components. This decomposition effectively mitigates weather-induced degradation while preserving color fidelity. To further enhance restoration quality, LGDM leverages degradation-related luminance information as a guiding condition, eliminating the need for explicit degradation prompts. Additionally, LGDM incorporates a \textit{Dynamic Time Step Loss} to optimize the denoising network, ensuring a balanced recovery of both low- and high-frequency features in the image. Finally, we present DriveWeather, a comprehensive all-weather driving dataset designed to enable robust evaluation. Extensive experiments demonstrate that our approach surpasses state-of-the-art methods, setting a new benchmark in AWIR. The dataset and code are available at: https://github.com/fiwy0527/LCDiff.

</details>


### [35] [Efficient 3D Scene Reconstruction and Simulation from Sparse Endoscopic Views](https://arxiv.org/abs/2509.17027)
*Zhenya Yang*

Main category: cs.CV

TL;DR: 提出基于高斯泼溅的框架，从内窥镜数据直接重建交互式手术场景，通过虚拟相机正则化和深度正则化解决视角多样性不足导致的过拟合问题，并采用稀疏控制节点材料点方法实现快速变形模拟。


<details>
  <summary>Details</summary>
Motivation: 传统手术模拟构建方法繁琐耗时且难以扩展，导致细节差、模拟不真实。需要一种能直接从内窥镜数据高效重建高质量交互式手术场景的方法。

Method: 使用高斯泼溅表示法重建手术场景，引入虚拟相机正则化方法采样虚拟视角优化几何精度，结合深度正则化，并采用稀疏控制节点材料点方法实现实时物理变形模拟。

Result: 在代表性手术数据上的实验表明，该方法能在几分钟内重建手术场景，并能实时产生物理合理的变形效果。

Conclusion: 该方法成功解决了内窥镜数据视角受限导致的几何精度问题，实现了高效、高质量的手术场景重建和实时交互模拟。

Abstract: Surgical simulation is essential for medical training, enabling practitioners to develop crucial skills in a risk-free environment while improving patient safety and surgical outcomes. However, conventional methods for building simulation environments are cumbersome, time-consuming, and difficult to scale, often resulting in poor details and unrealistic simulations. In this paper, we propose a Gaussian Splatting-based framework to directly reconstruct interactive surgical scenes from endoscopic data while ensuring efficiency, rendering quality, and realism. A key challenge in this data-driven simulation paradigm is the restricted movement of endoscopic cameras, which limits viewpoint diversity. As a result, the Gaussian Splatting representation overfits specific perspectives, leading to reduced geometric accuracy. To address this issue, we introduce a novel virtual camera-based regularization method that adaptively samples virtual viewpoints around the scene and incorporates them into the optimization process to mitigate overfitting. An effective depth-based regularization is applied to both real and virtual views to further refine the scene geometry. To enable fast deformation simulation, we propose a sparse control node-based Material Point Method, which integrates physical properties into the reconstructed scene while significantly reducing computational costs. Experimental results on representative surgical data demonstrate that our method can efficiently reconstruct and simulate surgical scenes from sparse endoscopic views. Notably, our method takes only a few minutes to reconstruct the surgical scene and is able to produce physically plausible deformations in real-time with user-defined interactions.

</details>


### [36] [HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis](https://arxiv.org/abs/2509.17083)
*Zipeng Wang,Dan Xu*

Main category: cs.CV

TL;DR: HyRF提出了一种结合显式高斯和神经场的混合辐射场表示方法，通过分解场景为紧凑的高斯集合和网格神经场，在保持实时渲染的同时将模型大小减少20倍以上。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅(3DGS)虽然能实现实时高质量新视角合成，但存在显著内存开销问题。现有的神经场压缩方法难以捕捉高斯属性的高频空间变化，导致细节重建质量下降。

Method: HyRF将场景分解为：(1)存储关键高频参数的紧凑显式高斯集合；(2)预测剩余属性的网格神经场。采用解耦神经场架构分别建模几何和视图相关颜色，并提出混合渲染方案结合高斯泼溅和神经场预测的背景。

Result: 实验表明HyRF在保持实时性能的同时，相比3DGS将模型大小减少超过20倍，并达到最先进的渲染质量。

Conclusion: HyRF成功结合了显式高斯和神经场的优势，解决了3DGS的内存开销问题，同时保持了高质量的渲染效果和实时性能。

Abstract: Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative to NeRF-based approaches, enabling real-time, high-quality novel view synthesis through explicit, optimizable 3D Gaussians. However, 3DGS suffers from significant memory overhead due to its reliance on per-Gaussian parameters to model view-dependent effects and anisotropic shapes. While recent works propose compressing 3DGS with neural fields, these methods struggle to capture high-frequency spatial variations in Gaussian properties, leading to degraded reconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a novel scene representation that combines the strengths of explicit Gaussians and neural fields. HyRF decomposes the scene into (1) a compact set of explicit Gaussians storing only critical high-frequency parameters and (2) grid-based neural fields that predict remaining properties. To enhance representational capacity, we introduce a decoupled neural field architecture, separately modeling geometry (scale, opacity, rotation) and view-dependent color. Additionally, we propose a hybrid rendering scheme that composites Gaussian splatting with a neural field-predicted background, addressing limitations in distant scene representation. Experiments demonstrate that HyRF achieves state-of-the-art rendering quality while reducing model size by over 20 times compared to 3DGS and maintaining real-time performance. Our project page is available at https://wzpscott.github.io/hyrf/.

</details>


### [37] [SynergyNet: Fusing Generative Priors and State-Space Models for Facial Beauty Prediction](https://arxiv.org/abs/2509.17172)
*Djamel Eddine Boukhari*

Main category: cs.CV

TL;DR: 本文提出了Mamba-Diffusion Network (MD-Net)，一种双流架构，结合预训练扩散模型的U-Net编码器和Vision Mamba，用于面部美感预测，在SCUT-FBP5500基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN或ViT的模型存在架构偏差：CNN擅长局部特征提取但难以处理长距离依赖，ViT能建模全局关系但计算成本高。需要解决这种权衡问题。

Method: MD-Net采用双流架构：第一流使用预训练潜在扩散模型的冻结U-Net编码器提供细粒度美学质量；第二流使用Vision Mamba高效捕获全局面部结构。通过交叉注意力机制整合互补表示。

Result: 在SCUT-FBP5500基准测试中，MD-Net达到Pearson相关系数0.9235，创造了新的最先进水平。

Conclusion: MD-Net展示了融合生成和序列建模范式的混合架构在复杂视觉评估任务中的巨大潜力。

Abstract: The automated prediction of facial beauty is a benchmark task in affective computing that requires a sophisticated understanding of both local aesthetic details (e.g., skin texture) and global facial harmony (e.g., symmetry, proportions). Existing models, based on either Convolutional Neural Networks (CNNs) or Vision Transformers (ViTs), exhibit inherent architectural biases that limit their performance; CNNs excel at local feature extraction but struggle with long-range dependencies, while ViTs model global relationships at a significant computational cost. This paper introduces the \textbf{Mamba-Diffusion Network (MD-Net)}, a novel dual-stream architecture that resolves this trade-off by delegating specialized roles to state-of-the-art models. The first stream leverages a frozen U-Net encoder from a pre-trained latent diffusion model, providing a powerful generative prior for fine-grained aesthetic qualities. The second stream employs a Vision Mamba (Vim), a modern state-space model, to efficiently capture global facial structure with linear-time complexity. By synergistically integrating these complementary representations through a cross-attention mechanism, MD-Net creates a holistic and nuanced feature space for prediction. Evaluated on the SCUT-FBP5500 benchmark, MD-Net sets a new state-of-the-art, achieving a Pearson Correlation of \textbf{0.9235} and demonstrating the significant potential of hybrid architectures that fuse generative and sequential modeling paradigms for complex visual assessment tasks.

</details>


### [38] [Echo-Path: Pathology-Conditioned Echo Video Generation](https://arxiv.org/abs/2509.17190)
*Kabir Hamzah Muhammad,Marawan Elbatel,Yi Qin,Xiaomeng Li*

Main category: cs.CV

TL;DR: Echo-Path是一个创新的生成框架，能够根据特定心脏病理条件生成超声心动图视频，专注于房间隔缺损（ASD）和肺动脉高压（PAH）两种疾病，通过病理条件机制控制疾病特异性结构模式，提高自动化诊断模型的性能。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死亡原因，但某些病理的超声心动图数据稀缺，限制了稳健自动化诊断模型的发展。需要生成合成数据来解决数据不足问题。

Method: 在先进的回声视频生成器中引入病理条件机制，使模型能够学习和控制心脏中疾病特异性结构和运动模式，生成具有目标异常的逼真超声视频序列。

Result: 合成视频实现了低分布距离，表明高视觉保真度。生成的回声显示出合理的病理标记。使用合成数据训练的分类器在真实数据上泛化良好，将ASD和PAH的诊断准确率分别提高了7%和8%。

Conclusion: Echo-Path框架能够有效生成高质量的病理特异性超声心动图视频，解决了数据稀缺问题，显著提升了心脏疾病的自动化诊断性能。

Abstract: Cardiovascular diseases (CVDs) remain the leading cause of mortality globally, and echocardiography is critical for diagnosis of both common and congenital cardiac conditions. However, echocardiographic data for certain pathologies are scarce, hindering the development of robust automated diagnosis models. In this work, we propose Echo-Path, a novel generative framework to produce echocardiogram videos conditioned on specific cardiac pathologies. Echo-Path can synthesize realistic ultrasound video sequences that exhibit targeted abnormalities, focusing here on atrial septal defect (ASD) and pulmonary arterial hypertension (PAH). Our approach introduces a pathology-conditioning mechanism into a state-of-the-art echo video generator, allowing the model to learn and control disease-specific structural and motion patterns in the heart. Quantitative evaluation demonstrates that the synthetic videos achieve low distribution distances, indicating high visual fidelity. Clinically, the generated echoes exhibit plausible pathology markers. Furthermore, classifiers trained on our synthetic data generalize well to real data and, when used to augment real training sets, it improves downstream diagnosis of ASD and PAH by 7\% and 8\% respectively. Code, weights and dataset are available here https://github.com/Marshall-mk/EchoPathv1

</details>


### [39] [Point-RTD: Replaced Token Denoising for Pretraining Transformer Models on Point Clouds](https://arxiv.org/abs/2509.17207)
*Gunner Stone,Youngsook Choi,Alireza Tavakkoli,Ankita Shukla*

Main category: cs.CV

TL;DR: Point-RTD是一种新的3D点云预训练策略，通过替换令牌去噪的破坏-重建框架提高令牌鲁棒性，相比传统掩码重建方法在性能和效率上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统的基于掩码的重建方法在隐藏数据段后进行预测，这种方法在3D点云任务中可能不够有效。作者希望通过破坏-重建框架来更有效地学习结构先验知识。

Method: 提出Point-RTD预训练策略，采用判别器-生成器架构进行去噪处理。该方法破坏点云令牌，然后通过重建过程来去噪，而不是简单地掩码和预测。

Result: 在ShapeNet数据集上，Point-RTD相比PointMAE重建误差降低93%以上，测试集上的Chamfer距离降低14倍以上。在ShapeNet、ModelNet10和ModelNet40基准测试中收敛更快，分类准确率更高。

Conclusion: Point-RTD在3D点云任务的预训练中明显优于基线Point-MAE框架，在性能和效率方面都有显著提升。

Abstract: Pre-training strategies play a critical role in advancing the performance of transformer-based models for 3D point cloud tasks. In this paper, we introduce Point-RTD (Replaced Token Denoising), a novel pretraining strategy designed to improve token robustness through a corruption-reconstruction framework. Unlike traditional mask-based reconstruction tasks that hide data segments for later prediction, Point-RTD corrupts point cloud tokens and leverages a discriminator-generator architecture for denoising. This shift enables more effective learning of structural priors and significantly enhances model performance and efficiency. On the ShapeNet dataset, Point-RTD reduces reconstruction error by over 93% compared to PointMAE, and achieves more than 14x lower Chamfer Distance on the test set. Our method also converges faster and yields higher classification accuracy on ShapeNet, ModelNet10, and ModelNet40 benchmarks, clearly outperforming the baseline Point-MAE framework in every case.

</details>


### [40] [DT-NeRF: A Diffusion and Transformer-Based Optimization Approach for Neural Radiance Fields in 3D Reconstruction](https://arxiv.org/abs/2509.17232)
*Bo Liu,Runlong Li,Li Zhou,Yan Zhou*

Main category: cs.CV

TL;DR: 本文提出DT-NeRF方法，结合扩散模型和Transformer来提升3D场景重建的细节恢复和多视角一致性，在稀疏视角下有效恢复细节并保持几何精度。


<details>
  <summary>Details</summary>
Motivation: 传统NeRF方法在稀疏视角下细节恢复不足，多视角一致性较差，需要结合先进生成模型来提升3D重建质量。

Method: DT-NeRF将扩散模型与Transformer结合，通过扩散模型优化神经辐射场，利用Transformer保持多视角一致性。

Result: 在Matterport3D和ShapeNet数据集上，DT-NeRF在PSNR、SSIM、Chamfer Distance和Fidelity等指标上显著优于传统NeRF和其他先进方法。消融实验证实扩散和Transformer模块的关键作用。

Conclusion: DT-NeRF展示了模块间的协同效应，为3D场景重建提供了高效准确解决方案。未来可进一步优化模型，探索更先进的生成模型和网络架构以提升大规模动态场景性能。

Abstract: This paper proposes a Diffusion Model-Optimized Neural Radiance Field (DT-NeRF) method, aimed at enhancing detail recovery and multi-view consistency in 3D scene reconstruction. By combining diffusion models with Transformers, DT-NeRF effectively restores details under sparse viewpoints and maintains high accuracy in complex geometric scenes. Experimental results demonstrate that DT-NeRF significantly outperforms traditional NeRF and other state-of-the-art methods on the Matterport3D and ShapeNet datasets, particularly in metrics such as PSNR, SSIM, Chamfer Distance, and Fidelity. Ablation experiments further confirm the critical role of the diffusion and Transformer modules in the model's performance, with the removal of either module leading to a decline in performance. The design of DT-NeRF showcases the synergistic effect between modules, providing an efficient and accurate solution for 3D scene reconstruction. Future research may focus on further optimizing the model, exploring more advanced generative models and network architectures to enhance its performance in large-scale dynamic scenes.

</details>


### [41] [SPFSplatV2: Efficient Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views](https://arxiv.org/abs/2509.17246)
*Ranran Huang,Krystian Mikolajczyk*

Main category: cs.CV

TL;DR: SPFSplatV2是一个无需真实相机位姿的3D高斯泼溅框架，可从稀疏多视角图像高效重建3D场景，在训练和推理过程中都不需要真实位姿信息。


<details>
  <summary>Details</summary>
Motivation: 传统3D重建方法依赖真实相机位姿，限制了在无位姿数据场景下的应用。本文旨在开发一个不依赖真实位姿的端到端框架，提高方法的可扩展性。

Method: 使用共享特征提取主干网络，在规范空间中同时预测3D高斯基元和相机位姿；引入掩码注意力机制估计目标位姿，通过重投影损失增强几何约束；框架兼容不同重建架构。

Result: 在域内和域外新视角合成任务中达到最先进性能，即使在极端视角变化和有限图像重叠情况下也表现优异，超越了依赖几何监督的相对位姿估计方法。

Conclusion: 该方法消除了对真实位姿的依赖，为利用更大更多样化数据集提供了可扩展性，展示了无位姿3D重建的潜力。

Abstract: We introduce SPFSplatV2, an efficient feed-forward framework for 3D Gaussian splatting from sparse multi-view images, requiring no ground-truth poses during training and inference. It employs a shared feature extraction backbone, enabling simultaneous prediction of 3D Gaussian primitives and camera poses in a canonical space from unposed inputs. A masked attention mechanism is introduced to efficiently estimate target poses during training, while a reprojection loss enforces pixel-aligned Gaussian primitives, providing stronger geometric constraints. We further demonstrate the compatibility of our training framework with different reconstruction architectures, resulting in two model variants. Remarkably, despite the absence of pose supervision, our method achieves state-of-the-art performance in both in-domain and out-of-domain novel view synthesis, even under extreme viewpoint changes and limited image overlap, and surpasses recent methods that rely on geometric supervision for relative pose estimation. By eliminating dependence on ground-truth poses, our method offers the scalability to leverage larger and more diverse datasets. Code and pretrained models will be available on our project page: https://ranrhuang.github.io/spfsplatv2/.

</details>


### [42] [SmokeSeer: 3D Gaussian Splatting for Smoke Removal and Scene Reconstruction](https://arxiv.org/abs/2509.17329)
*Neham Jain,Andrew Jong,Sebastian Scherer,Ioannis Gkioulekas*

Main category: cs.CV

TL;DR: SmokeSeer是一种从视频中同时进行3D场景重建和烟雾去除的方法，利用热成像和RGB图像，通过3D高斯泼溅技术将场景分解为烟雾和非烟雾成分。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的烟雾会严重降低图像质量并阻碍可见性。现有方法要么依赖容易产生幻觉的数据驱动先验，要么仅限于静态低密度烟雾。

Method: 基于3D高斯泼溅技术，融合热成像和RGB图像信息，利用热成像中散射减少的特性透过烟雾观察场景，并将场景明确分解为烟雾和非烟雾成分。

Result: 该方法能够处理广泛的烟雾密度范围，并能适应随时间变化的烟雾。在合成数据和真实世界多视角烟雾数据集上进行了验证。

Conclusion: SmokeSeer提供了一种有效的同时进行3D重建和烟雾去除的解决方案，代码和数据已开源。

Abstract: Smoke in real-world scenes can severely degrade the quality of images and hamper visibility. Recent methods for image restoration either rely on data-driven priors that are susceptible to hallucinations, or are limited to static low-density smoke. We introduce SmokeSeer, a method for simultaneous 3D scene reconstruction and smoke removal from a video capturing multiple views of a scene. Our method uses thermal and RGB images, leveraging the fact that the reduced scattering in thermal images enables us to see through the smoke. We build upon 3D Gaussian splatting to fuse information from the two image modalities, and decompose the scene explicitly into smoke and non-smoke components. Unlike prior approaches, SmokeSeer handles a broad range of smoke densities and can adapt to temporally varying smoke. We validate our approach on synthetic data and introduce a real-world multi-view smoke dataset with RGB and thermal images. We provide open-source code and data at the project website.

</details>


### [43] [Diff-GNSS: Diffusion-based Pseudorange Error Estimation](https://arxiv.org/abs/2509.17397)
*Jiaqi Zhu,Shouyi Lu,Ziyao Li,Guirong Zhuo,Lu Xiong*

Main category: cs.CV

TL;DR: Diff-GNSS是一个基于条件扩散模型的GNSS伪距误差估计框架，通过粗到精的估计方法解决复杂误差分布问题，显著提升城市定位精度


<details>
  <summary>Details</summary>
Motivation: GNSS在城市环境中因多径和非视距接收导致测量误差大，现有学习方法受限于复杂误差分布，需要更有效的误差建模方法

Method: 采用Mamba模块进行粗估计，条件去噪扩散层进行精估计，利用GNSS测量质量特征作为条件引导反向去噪过程，并集成逐卫星不确定性建模

Result: 在公开和自收集数据集上的实验表明，Diff-GNSS在多个指标上一致优于最先进的基线方法

Conclusion: 这是扩散模型在伪距误差估计中的首次应用，提出的扩散精化模块即插即用，可显著提高现有网络的估计精度

Abstract: Global Navigation Satellite Systems (GNSS) are vital for reliable urban positioning. However, multipath and non-line-of-sight reception often introduce large measurement errors that degrade accuracy. Learning-based methods for predicting and compensating pseudorange errors have gained traction, but their performance is limited by complex error distributions. To address this challenge, we propose Diff-GNSS, a coarse-to-fine GNSS measurement (pseudorange) error estimation framework that leverages a conditional diffusion model to capture such complex distributions. Firstly, a Mamba-based module performs coarse estimation to provide an initial prediction with appropriate scale and trend. Then, a conditional denoising diffusion layer refines the estimate, enabling fine-grained modeling of pseudorange errors. To suppress uncontrolled generative diversity and achieve controllable synthesis, three key features related to GNSS measurement quality are used as conditions to precisely guide the reverse denoising process. We further incorporate per-satellite uncertainty modeling within the diffusion stage to assess the reliability of the predicted errors. We have collected and publicly released a real-world dataset covering various scenes. Experiments on public and self-collected datasets show that DiffGNSS consistently outperforms state-of-the-art baselines across multiple metrics. To the best of our knowledge, this is the first application of diffusion models to pseudorange error estimation. The proposed diffusion-based refinement module is plug-and-play and can be readily integrated into existing networks to markedly improve estimation accuracy.

</details>


### [44] [EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian Splats from a Mobile Device](https://arxiv.org/abs/2509.17430)
*Gunjan Chhablani,Xiaomeng Ye,Muhammad Zubair Irshad,Zsolt Kira*

Main category: cs.CV

TL;DR: EmbodiedSplat提出了一种通过3D高斯泼溅技术重建真实部署环境来微调策略的新方法，显著提高了模拟到现实的迁移性能。


<details>
  <summary>Details</summary>
Motivation: 当前Embodied AI领域主要依赖仿真训练，但合成环境缺乏真实感，而高保真实世界重建成本高昂，导致模拟到现实迁移成为主要挑战。

Method: 利用iPhone捕获部署场景，通过3D高斯泼溅技术重建网格，在Habitat-Sim模拟器中创建近似真实条件的训练环境，并对训练策略、预训练数据集和网格重建技术进行全面分析。

Result: 实验显示，使用EmbodiedSplat微调的智能体在真实世界图像导航任务上比HM3D和HSSD预训练的基线方法分别提高了20%和40%的成功率，模拟与真实相关性达到0.87-0.97。

Conclusion: 该方法能够以最小成本有效适应不同环境，显著提升策略在真实世界中的性能表现。

Abstract: The field of Embodied AI predominantly relies on simulation for training and evaluation, often using either fully synthetic environments that lack photorealism or high-fidelity real-world reconstructions captured with expensive hardware. As a result, sim-to-real transfer remains a major challenge. In this paper, we introduce EmbodiedSplat, a novel approach that personalizes policy training by efficiently capturing the deployment environment and fine-tuning policies within the reconstructed scenes. Our method leverages 3D Gaussian Splatting (GS) and the Habitat-Sim simulator to bridge the gap between realistic scene capture and effective training environments. Using iPhone-captured deployment scenes, we reconstruct meshes via GS, enabling training in settings that closely approximate real-world conditions. We conduct a comprehensive analysis of training strategies, pre-training datasets, and mesh reconstruction techniques, evaluating their impact on sim-to-real predictivity in real-world scenarios. Experimental results demonstrate that agents fine-tuned with EmbodiedSplat outperform both zero-shot baselines pre-trained on large-scale real-world datasets (HM3D) and synthetically generated datasets (HSSD), achieving absolute success rate improvements of 20\% and 40\% on real-world Image Navigation task. Moreover, our approach yields a high sim-vs-real correlation (0.87--0.97) for the reconstructed meshes, underscoring its effectiveness in adapting policies to diverse environments with minimal effort. Project page: https://gchhablani.github.io/embodied-splat

</details>


### [45] [CARINOX: Inference-time Scaling with Category-Aware Reward-based Initial Noise Optimization and Exploration](https://arxiv.org/abs/2509.17458)
*Seyed Amir Kasaei,Ali Aghayari,Arash Marioriyad,Niki Sepasian,Shayan Baghayi Nejad,MohammadAmin Fazli,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: CARINOX是一个统一的框架，结合噪声优化和探索，通过基于人类判断相关性的奖励选择程序，显著提升文本到图像扩散模型的组合对齐能力。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像扩散模型在复杂对象关系、属性和空间排列描述上的组合对齐失败问题，克服现有优化和探索方法的局限性。

Method: 结合噪声优化和探索，采用基于人类判断相关性的原则性奖励选择程序，在互补基准上进行评估。

Result: 在T2I-CompBench++上平均对齐分数提升16%，在HRS基准上提升11%，在所有主要类别上一致优于最先进方法，同时保持图像质量和多样性。

Conclusion: CARINOX通过统一优化和探索策略，结合原则性奖励选择，有效解决了扩散模型的组合对齐挑战，显著提升了性能。

Abstract: Text-to-image diffusion models, such as Stable Diffusion, can produce high-quality and diverse images but often fail to achieve compositional alignment, particularly when prompts describe complex object relationships, attributes, or spatial arrangements. Recent inference-time approaches address this by optimizing or exploring the initial noise under the guidance of reward functions that score text-image alignment without requiring model fine-tuning. While promising, each strategy has intrinsic limitations when used alone: optimization can stall due to poor initialization or unfavorable search trajectories, whereas exploration may require a prohibitively large number of samples to locate a satisfactory output. Our analysis further shows that neither single reward metrics nor ad-hoc combinations reliably capture all aspects of compositionality, leading to weak or inconsistent guidance. To overcome these challenges, we present Category-Aware Reward-based Initial Noise Optimization and Exploration (CARINOX), a unified framework that combines noise optimization and exploration with a principled reward selection procedure grounded in correlation with human judgments. Evaluations on two complementary benchmarks covering diverse compositional challenges show that CARINOX raises average alignment scores by +16% on T2I-CompBench++ and +11% on the HRS benchmark, consistently outperforming state-of-the-art optimization and exploration-based methods across all major categories, while preserving image quality and diversity. The project page is available at https://amirkasaei.com/carinox/{this URL}.

</details>


### [46] [Stable Video-Driven Portraits](https://arxiv.org/abs/2509.17476)
*Mallikarjun B. R.,Fei Yin,Vikram Voleti,Nikita Drobyshev,Maksim Lapin,Aaryaman Vasishta,Varun Jampani*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的新框架，利用驱动视频中掩码面部区域（眼睛、鼻子、嘴巴）作为强运动控制信号，实现高质量、可控的肖像动画生成


<details>
  <summary>Details</summary>
Motivation: 解决现有肖像动画方法在表达能力、时间一致性和对未见身份或大姿态变化的泛化能力方面的局限性，特别是扩散模型在弱控制信号和架构限制下的约束

Method: 采用跨身份监督训练防止外观泄漏，引入最小新参数的新架构以利用预训练扩散模型的强先验，使用时空注意力机制捕获细微运动，利用历史帧确保连续性，提出信号融合策略平衡运动保真度和身份保持

Result: 实现了优越的时间一致性和准确的表情控制，能够生成高质量、可控的肖像动画，适用于实际应用

Conclusion: 该框架通过强运动控制信号、高效架构设计和时空一致性机制，显著提升了肖像动画的质量和实用性

Abstract: Portrait animation aims to generate photo-realistic videos from a single source image by reenacting the expression and pose from a driving video. While early methods relied on 3D morphable models or feature warping techniques, they often suffered from limited expressivity, temporal inconsistency, and poor generalization to unseen identities or large pose variations. Recent advances using diffusion models have demonstrated improved quality but remain constrained by weak control signals and architectural limitations. In this work, we propose a novel diffusion based framework that leverages masked facial regions specifically the eyes, nose, and mouth from the driving video as strong motion control cues. To enable robust training without appearance leakage, we adopt cross identity supervision. To leverage the strong prior from the pretrained diffusion model, our novel architecture introduces minimal new parameters that converge faster and help in better generalization. We introduce spatial temporal attention mechanisms that allow inter frame and intra frame interactions, effectively capturing subtle motions and reducing temporal artifacts. Our model uses history frames to ensure continuity across segments. At inference, we propose a novel signal fusion strategy that balances motion fidelity with identity preservation. Our approach achieves superior temporal consistency and accurate expression control, enabling high-quality, controllable portrait animation suitable for real-world applications.

</details>


### [47] [4D-MoDe: Towards Editable and Scalable Volumetric Streaming via Motion-Decoupled 4D Gaussian Compression](https://arxiv.org/abs/2509.17506)
*Houqiang Zhong,Zihan Zheng,Qiang Hu,Yuan Tian,Ning Cao,Lan Xu,Xiaoyun Zhang,Zhengxue Cheng,Li Song,Wenjun Zhang*

Main category: cs.CV

TL;DR: 4D-MoDe是一个运动解耦的4D高斯压缩框架，用于可扩展和可编辑的体视频流传输，通过分层表示和前瞻性运动分解显著降低存储成本。


<details>
  <summary>Details</summary>
Motivation: 体视频在沉浸式远程呈现和AR/VR中至关重要，但现有表示方法存在数据量大、运动复杂和可编辑性有限的问题，难以实现高质量动态体内容的大规模传输。

Method: 采用分层表示分离静态背景和动态前景，使用前瞻性运动分解策略；多分辨率运动估计网格和轻量级共享MLP捕捉连续运动轨迹；动态高斯补偿机制建模新内容；自适应分组方案平衡时间一致性和压缩效率；基于熵感知的训练管道联合优化运动场和高斯参数。

Result: 在多个数据集上的实验表明，4D-MoDe以低一个数量级的存储成本（低至11.4 KB/帧）实现了具有竞争力的重建质量，同时支持背景替换和仅前景流传输等实际应用。

Conclusion: 4D-MoDe框架为体视频流传输提供了一种高效、可扩展且可编辑的解决方案，显著降低了存储需求并保持了高质量的重建效果。

Abstract: Volumetric video has emerged as a key medium for immersive telepresence and augmented/virtual reality, enabling six-degrees-of-freedom (6DoF) navigation and realistic spatial interactions. However, delivering high-quality dynamic volumetric content at scale remains challenging due to massive data volume, complex motion, and limited editability of existing representations. In this paper, we present 4D-MoDe, a motion-decoupled 4D Gaussian compression framework designed for scalable and editable volumetric video streaming. Our method introduces a layered representation that explicitly separates static backgrounds from dynamic foregrounds using a lookahead-based motion decomposition strategy, significantly reducing temporal redundancy and enabling selective background/foreground streaming. To capture continuous motion trajectories, we employ a multi-resolution motion estimation grid and a lightweight shared MLP, complemented by a dynamic Gaussian compensation mechanism to model emergent content. An adaptive grouping scheme dynamically inserts background keyframes to balance temporal consistency and compression efficiency. Furthermore, an entropy-aware training pipeline jointly optimizes the motion fields and Gaussian parameters under a rate-distortion (RD) objective, while employing range-based and KD-tree compression to minimize storage overhead. Extensive experiments on multiple datasets demonstrate that 4D-MoDe consistently achieves competitive reconstruction quality with an order of magnitude lower storage cost (e.g., as low as \textbf{11.4} KB/frame) compared to state-of-the-art methods, while supporting practical applications such as background replacement and foreground-only streaming.

</details>


### [48] [PRNU-Bench: A Novel Benchmark and Model for PRNU-Based Camera Identification](https://arxiv.org/abs/2509.17581)
*Florinel Alin Croitoru,Vlad Hondru,Radu Tudor Ionescu*

Main category: cs.CV

TL;DR: 提出了一个用于相机识别的PRNU基准测试，包含13K张照片和120+相机，并提出了一种基于Hadamard乘积的混合架构模型，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的相机识别方法在真实场景下性能有限，需要开发能够在不同场景下进行"野外"评估的基准测试和更有效的PRNU估计方法。

Method: 使用混合架构：去噪自编码器估计PRNU信号，卷积网络进行1:N验证。创新地采用Hadamard乘积作为参考和查询PRNU信号的输入，替代传统的对比学习方法。

Result: 该方法在提出的基准测试上取得了显著优于基于去噪自编码器和对比学习的最先进模型的结果。

Conclusion: 提出的基准测试和混合架构模型为相机识别提供了有效的解决方案，特别是在真实场景下的性能表现优异。

Abstract: We propose a novel benchmark for camera identification via Photo Response Non-Uniformity (PRNU) estimation. The benchmark comprises 13K photos taken with 120+ cameras, where the training and test photos are taken in different scenarios, enabling ``in-the-wild'' evaluation. In addition, we propose a novel PRNU-based camera identification model that employs a hybrid architecture, comprising a denoising autoencoder to estimate the PRNU signal and a convolutional network that can perform 1:N verification of camera devices. Instead of using a conventional approach based on contrastive learning, our method takes the Hadamard product between reference and query PRNU signals as input. This novel design leads to significantly better results compared with state-of-the-art models based on denoising autoencoders and contrastive learning. We release our dataset and code at: https://github.com/CroitoruAlin/PRNU-Bench.

</details>


### [49] [SISMA: Semantic Face Image Synthesis with Mamba](https://arxiv.org/abs/2509.17651)
*Filippo Botti,Alex Ergasti,Tomaso Fontanini,Claudio Ferrari,Massimo Bertozzi,Andrea Prati*

Main category: cs.CV

TL;DR: 提出基于Mamba的新型架构SISMA，用于语义图像合成，在降低计算需求的同时生成高质量人脸图像


<details>
  <summary>Details</summary>
Motivation: 扩散模型在语义图像合成中很受欢迎，但其训练和推理计算成本高，注意力层的二次复杂度导致计算需求大

Method: 基于最近提出的Mamba架构开发SISMA，通过语义掩码控制图像形状生成高质量样本，降低计算需求

Result: 在CelebAMask-HQ数据集上的实验表明，SISMA不仅获得更好的FID分数，而且运行速度比最先进架构快三倍

Conclusion: SISMA是transformer-based模型的一个可行、轻量级替代方案

Abstract: Diffusion Models have become very popular for Semantic Image Synthesis (SIS) of human faces. Nevertheless, their training and inference is computationally expensive and their computational requirements are high due to the quadratic complexity of attention layers. In this paper, we propose a novel architecture called SISMA, based on the recently proposed Mamba. SISMA generates high quality samples by controlling their shape using a semantic mask at a reduced computational demand. We validated our approach through comprehensive experiments with CelebAMask-HQ, revealing that our architecture not only achieves a better FID score yet also operates at three times the speed of state-of-the-art architectures. This indicates that the proposed design is a viable, lightweight substitute to transformer-based models.

</details>


### [50] [Clothing agnostic Pre-inpainting Virtual Try-ON](https://arxiv.org/abs/2509.17654)
*Sehyun Kim,Hye Jun Lee,Jiwoo Lee,Taemin Lee*

Main category: cs.CV

TL;DR: CaP-VTON是一种基于扩散模型的虚拟试穿方法，通过多类别掩码和皮肤修复技术解决了现有方法在服装合成中的轮廓保持和皮肤修复问题，显著提升了短袖合成的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的Leffa方法在基于扩散模型的虚拟试穿中存在底部检测不准确和服装轮廓残留的问题，特别是在长袖转短袖/无袖时皮肤修复效果不佳。

Method: CaP-VTON整合了基于Dress Code的多类别掩码和基于Stable Diffusion的皮肤修复技术，特别引入了生成皮肤模块，考虑人体姿势和颜色进行高质量皮肤修复。

Result: CaP-VTON在短袖合成准确率上达到92.5%，比Leffa提升了15.4%，在视觉评估中能一致性地复现参考服装的风格和形状。

Conclusion: 该方法具有模型无关性，可应用于各种基于扩散的虚拟试穿系统，对电商、定制造型和虚拟形象创建等高精度虚拟试穿应用有重要贡献。

Abstract: With the development of deep learning technology, virtual try-on technology has become an important application value in the fields of e-commerce, fashion, and entertainment. The recently proposed Leffa has improved the texture distortion problem of diffu-sion-based models, but there are limitations in that the bottom detection inaccuracy and the existing clothing silhouette remain in the synthesis results. To solve this problem, this study proposes CaP-VTON (Clothing agnostic Pre-inpainting Virtual Try-ON). CaP-VTON has improved the naturalness and consistency of whole-body clothing syn-thesis by integrating multi-category masking based on Dress Code and skin inpainting based on Stable Diffusion. In particular, a generate skin module was introduced to solve the skin restoration problem that occurs when long-sleeved images are converted into short-sleeved or sleeveless ones, and high-quality restoration was implemented consider-ing the human body posture and color. As a result, CaP-VTON recorded 92.5\%, which is 15.4\% better than Leffa in short-sleeved synthesis accuracy, and showed the performance of consistently reproducing the style and shape of reference clothing in visual evaluation. These structures maintain model-agnostic properties and are applicable to various diffu-sion-based virtual inspection systems, and can contribute to applications that require high-precision virtual wearing, such as e-commerce, custom styling, and avatar creation.

</details>


### [51] [Multi-Agent Amodal Completion: Direct Synthesis with Fine-Grained Semantic Guidance](https://arxiv.org/abs/2509.17757)
*Hongxing Fan,Lipeng Wang,Haohua Chen,Zehuan Huang,Jiangtao Wu,Lu Sheng*

Main category: cs.CV

TL;DR: 提出基于协作多智能体推理的框架，通过多智能体协作分析遮挡关系和边界扩展，结合细粒度语义指导，实现高质量的无模态图像补全。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在数据需求、泛化能力和渐进式管道中的误差累积问题，提升遮挡物体不可见部分的生成质量。

Method: 使用多智能体协作推理框架，分析遮挡关系并确定边界扩展，结合扩散变换器的注意力图生成分层RGBA输出，无需额外分割。

Result: 在广泛评估中，该框架实现了最先进的视觉质量，能够准确合成物体并防止不需要元素的再生。

Conclusion: 该协作多智能体推理框架有效解决了无模态补全中的关键挑战，为图像编辑和增强现实应用提供了高质量的解决方案。

Abstract: Amodal completion, generating invisible parts of occluded objects, is vital for applications like image editing and AR. Prior methods face challenges with data needs, generalization, or error accumulation in progressive pipelines. We propose a Collaborative Multi-Agent Reasoning Framework based on upfront collaborative reasoning to overcome these issues. Our framework uses multiple agents to collaboratively analyze occlusion relationships and determine necessary boundary expansion, yielding a precise mask for inpainting. Concurrently, an agent generates fine-grained textual descriptions, enabling Fine-Grained Semantic Guidance. This ensures accurate object synthesis and prevents the regeneration of occluders or other unwanted elements, especially within large inpainting areas. Furthermore, our method directly produces layered RGBA outputs guided by visible masks and attention maps from a Diffusion Transformer, eliminating extra segmentation. Extensive evaluations demonstrate our framework achieves state-of-the-art visual quality.

</details>


### [52] [I2VWM: Robust Watermarking for Image to Video Generation](https://arxiv.org/abs/2509.17773)
*Guanjie Wang,Zehua Ma,Han Fang,Weiming Zhang*

Main category: cs.CV

TL;DR: 本文提出了I2VWM框架，通过鲁棒扩散距离概念解决图像到视频生成中的跨模态水印追踪问题，在训练时使用视频模拟噪声层，推理时采用光流对齐模块，显著提升了水印的时序鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着图像引导视频生成技术的快速发展，其在错误信息和欺诈中的潜在滥用风险日益凸显，现有单模态水印方法无法有效追踪I2V生成视频中的源图像，亟需跨模态水印解决方案。

Method: 提出鲁棒扩散距离概念衡量水印信号在生成视频中的时序持续性，构建I2VWM框架：训练阶段使用视频模拟噪声层，推理阶段采用光流对齐模块实现跨模态水印追踪。

Result: 在开源和商业I2V模型上的实验表明，I2VWM在保持不可感知性的同时显著提升了水印鲁棒性，为生成视频时代的跨模态水印建立了新范式。

Conclusion: I2VWM框架成功解决了I2V场景下的源图像追踪难题，通过创新的鲁棒扩散距离和跨模态设计，为生成视频水印技术提供了有效解决方案。

Abstract: The rapid progress of image-guided video generation (I2V) has raised concerns about its potential misuse in misinformation and fraud, underscoring the urgent need for effective digital watermarking. While existing watermarking methods demonstrate robustness within a single modality, they fail to trace source images in I2V settings. To address this gap, we introduce the concept of Robust Diffusion Distance, which measures the temporal persistence of watermark signals in generated videos. Building on this, we propose I2VWM, a cross-modal watermarking framework designed to enhance watermark robustness across time. I2VWM leverages a video-simulation noise layer during training and employs an optical-flow-based alignment module during inference. Experiments on both open-source and commercial I2V models demonstrate that I2VWM significantly improves robustness while maintaining imperceptibility, establishing a new paradigm for cross-modal watermarking in the era of generative video. \href{https://github.com/MrCrims/I2VWM-Robust-Watermarking-for-Image-to-Video-Generation}{Code Released.}

</details>


### [53] [Accurate and Efficient Low-Rank Model Merging in Core Space](https://arxiv.org/abs/2509.17786)
*Aniello Panariello,Daniel Marczak,Simone Magistri,Angelo Porrello,Bartłomiej Twardowski,Andrew D. Bagdanov,Simone Calderara,Joost van de Weijer*

Main category: cs.CV

TL;DR: 提出了Core Space框架，用于在共同对齐基中合并LoRA适配模型，保持低秩适配效率的同时显著提高跨任务准确性


<details>
  <summary>Details</summary>
Motivation: 现有LoRA模型合并方法通常需要合并全尺寸权重矩阵，牺牲了参数高效适配的效率优势

Method: Core Space合并框架，通过投影到核心空间实现LoRA适配模型的合并，确保信息无损且计算高效

Result: 在视觉和语言任务上达到最先进结果，同时仅使用少量计算资源，显著优于现有合并技术

Conclusion: Core Space框架成功解决了LoRA模型合并的效率与准确性权衡问题，为参数高效适配提供了有效的合并解决方案

Abstract: In this paper, we address the challenges associated with merging low-rank adaptations of large neural networks. With the rise of parameter-efficient adaptation techniques, such as Low-Rank Adaptation (LoRA), model fine-tuning has become more accessible. While fine-tuning models with LoRA is highly efficient, existing merging methods often sacrifice this efficiency by merging fully-sized weight matrices. We propose the Core Space merging framework, which enables the merging of LoRA-adapted models within a common alignment basis, thereby preserving the efficiency of low-rank adaptation while substantially improving accuracy across tasks. We further provide a formal proof that projection into Core Space ensures no loss of information and provide a complexity analysis showing the efficiency gains. Extensive empirical results demonstrate that Core Space significantly improves existing merging techniques and achieves state-of-the-art results on both vision and language tasks while utilizing a fraction of the computational resources. Codebase is available at https://github.com/apanariello4/core-space-merging.

</details>


### [54] [From Restoration to Reconstruction: Rethinking 3D Gaussian Splatting for Underwater Scenes](https://arxiv.org/abs/2509.17789)
*Guoxi Huang,Haoran Wang,Zipeng Qi,Wenjun Lu,David Bull,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: R-Splatting是一个统一框架，将水下图像恢复与3D高斯泼溅结合，通过多视图增强和不确定性感知优化，提升水下3D重建的渲染质量和几何精度。


<details>
  <summary>Details</summary>
Motivation: 水下图像退化对3D重建构成重大挑战，简化的物理模型在复杂场景中往往失效。需要一种能够同时改善渲染质量和几何保真度的方法。

Method: 集成多种水下图像恢复模型生成增强视图；使用轻量级光照生成器采样潜在编码；提出不确定性感知不透明度优化，将不透明度建模为随机函数以正则化训练。

Result: 在Seathru-NeRF和新BlueCoral3D数据集上的实验表明，R-Splatting在渲染质量和几何精度方面均优于强基线方法。

Conclusion: 该方法成功地将水下图像恢复与3D高斯泼溅相结合，通过统一框架有效解决了水下复杂场景的3D重建问题。

Abstract: Underwater image degradation poses significant challenges for 3D reconstruction, where simplified physical models often fail in complex scenes. We propose \textbf{R-Splatting}, a unified framework that bridges underwater image restoration (UIR) with 3D Gaussian Splatting (3DGS) to improve both rendering quality and geometric fidelity. Our method integrates multiple enhanced views produced by diverse UIR models into a single reconstruction pipeline. During inference, a lightweight illumination generator samples latent codes to support diverse yet coherent renderings, while a contrastive loss ensures disentangled and stable illumination representations. Furthermore, we propose \textit{Uncertainty-Aware Opacity Optimization (UAOO)}, which models opacity as a stochastic function to regularize training. This suppresses abrupt gradient responses triggered by illumination variation and mitigates overfitting to noisy or view-specific artifacts. Experiments on Seathru-NeRF and our new BlueCoral3D dataset demonstrate that R-Splatting outperforms strong baselines in both rendering quality and geometric accuracy.

</details>


### [55] [Degradation-Aware All-in-One Image Restoration via Latent Prior Encoding](https://arxiv.org/abs/2509.17792)
*S M A Sharif,Abdur Rehman,Fayaz Ali Dharejo,Radu Timofte,Rizwan Ali Naqvi*

Main category: cs.CV

TL;DR: 本文提出了一种基于学习潜在先验推理的全能图像修复方法，通过自适应特征选择、空间定位和退化语义三个结构化推理步骤，无需外部文本提示或手工架构先验，在多种退化任务中实现优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界图像常受多种空间多样化退化影响，现有全能修复方法依赖外部文本提示或手工架构先验，这些离散假设限制了方法对未知或混合退化的泛化能力。

Method: 将全能修复重新定义为学习潜在先验推理，从输入中自动推断退化感知表示，设计轻量级解码模块实现自适应特征选择、空间定位和退化语义的结构化推理。

Result: 在六种常见退化任务、五种复合设置和未见退化上的实验表明，该方法平均PSNR提升1.68 dB，同时效率提高三倍，优于现有最优方法。

Conclusion: 通过学习潜在先验推理的结构化范式，该方法在无需显式任务提示的情况下实现了对多样化图像退化的有效修复，展现了强大的泛化能力和计算效率。

Abstract: Real-world images often suffer from spatially diverse degradations such as haze, rain, snow, and low-light, significantly impacting visual quality and downstream vision tasks. Existing all-in-one restoration (AIR) approaches either depend on external text prompts or embed hand-crafted architectural priors (e.g., frequency heuristics); both impose discrete, brittle assumptions that weaken generalization to unseen or mixed degradations. To address this limitation, we propose to reframe AIR as learned latent prior inference, where degradation-aware representations are automatically inferred from the input without explicit task cues. Based on latent priors, we formulate AIR as a structured reasoning paradigm: (1) which features to route (adaptive feature selection), (2) where to restore (spatial localization), and (3) what to restore (degradation semantics). We design a lightweight decoding module that efficiently leverages these latent encoded cues for spatially-adaptive restoration. Extensive experiments across six common degradation tasks, five compound settings, and previously unseen degradations demonstrate that our method outperforms state-of-the-art (SOTA) approaches, achieving an average PSNR improvement of 1.68 dB while being three times more efficient.

</details>


### [56] [ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment](https://arxiv.org/abs/2509.17818)
*Yiyang Chen,Xuanhua He,Xiujun Ma,Yue Ma*

Main category: cs.CV

TL;DR: ContextFlow是一个无需训练的基于DiT的视频对象编辑框架，通过高阶Rectified Flow求解器和自适应上下文增强机制，解决了现有方法在保真度和时间一致性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的无需训练视频对象编辑方法存在两个主要限制：一阶求解器导致的反演不准确，以及粗糙的"硬"特征替换引起的上下文冲突。这些问题在Diffusion Transformers中更为严重，因为先前的层选择启发式方法不适用。

Method: ContextFlow采用高阶Rectified Flow求解器建立稳健的编辑基础，核心是自适应上下文增强机制，通过并行重建和编辑路径的Key-Value对连接来丰富自注意力上下文，而非替换特征。同时基于Guidance Responsiveness Metric进行数据驱动分析，识别任务特定的关键层。

Result: 大量实验表明，ContextFlow显著优于现有的无需训练方法，甚至超越了几种最先进的基于训练的方法，能够产生时间一致、高保真的结果。

Conclusion: ContextFlow通过创新的自适应上下文增强和系统性的层选择策略，为基于DiT的视频对象编辑提供了有效的解决方案，在保持时间一致性和保真度方面表现出色。

Abstract: Training-free video object editing aims to achieve precise object-level manipulation, including object insertion, swapping, and deletion. However, it faces significant challenges in maintaining fidelity and temporal consistency. Existing methods, often designed for U-Net architectures, suffer from two primary limitations: inaccurate inversion due to first-order solvers, and contextual conflicts caused by crude "hard" feature replacement. These issues are more challenging in Diffusion Transformers (DiTs), where the unsuitability of prior layer-selection heuristics makes effective guidance challenging. To address these limitations, we introduce ContextFlow, a novel training-free framework for DiT-based video object editing. In detail, we first employ a high-order Rectified Flow solver to establish a robust editing foundation. The core of our framework is Adaptive Context Enrichment (for specifying what to edit), a mechanism that addresses contextual conflicts. Instead of replacing features, it enriches the self-attention context by concatenating Key-Value pairs from parallel reconstruction and editing paths, empowering the model to dynamically fuse information. Additionally, to determine where to apply this enrichment (for specifying where to edit), we propose a systematic, data-driven analysis to identify task-specific vital layers. Based on a novel Guidance Responsiveness Metric, our method pinpoints the most influential DiT blocks for different tasks (e.g., insertion, swapping), enabling targeted and highly effective guidance. Extensive experiments show that ContextFlow significantly outperforms existing training-free methods and even surpasses several state-of-the-art training-based approaches, delivering temporally coherent, high-fidelity results.

</details>


### [57] [Semantic and Visual Crop-Guided Diffusion Models for Heterogeneous Tissue Synthesis in Histopathology](https://arxiv.org/abs/2509.17847)
*Saghir Alfasly,Wataru Uegami,MD Enamul Hoq,Ghazal Alabtah,H. R. Tizhoosh*

Main category: cs.CV

TL;DR: 提出了一种用于组织病理学图像合成的潜在扩散模型，通过双条件方法结合语义分割图和特定组织视觉裁剪，能够生成具有精确区域注释的高保真异质性组织图像。


<details>
  <summary>Details</summary>
Motivation: 解决组织病理学中合成数据生成的独特挑战：保持组织异质性、捕捉细微形态特征，并扩展到未标注数据集。现有方法依赖文本提示或抽象视觉嵌入，难以保留关键形态细节。

Method: 使用潜在扩散模型，采用新颖的双条件方法：结合语义分割图和来自相应语义区域的原始组织裁剪。对于未标注数据，引入自监督扩展，使用基础模型嵌入将全切片图像聚类为100种组织类型，自动生成伪语义图进行训练。

Result: 在Camelyon16数据集上，提示引导合成将Frechet距离从430.1降低到72.0（减少6倍），在Panda和TCGA数据集上FD降低2-3倍。仅使用合成数据训练的DeepLabv3+模型在Camelyon16和Panda上分别达到0.71和0.95的IoU，接近真实数据基线（0.72和0.96）。

Conclusion: 该框架通过扩展到11,765个TCGA全切片图像而无需手动标注，为生成多样化、标注的组织病理学数据提供了实用解决方案，解决了计算病理学中的关键瓶颈。

Abstract: Synthetic data generation in histopathology faces unique challenges: preserving tissue heterogeneity, capturing subtle morphological features, and scaling to unannotated datasets. We present a latent diffusion model that generates realistic heterogeneous histopathology images through a novel dual-conditioning approach combining semantic segmentation maps with tissue-specific visual crops. Unlike existing methods that rely on text prompts or abstract visual embeddings, our approach preserves critical morphological details by directly incorporating raw tissue crops from corresponding semantic regions. For annotated datasets (i.e., Camelyon16, Panda), we extract patches ensuring 20-80% tissue heterogeneity. For unannotated data (i.e., TCGA), we introduce a self-supervised extension that clusters whole-slide images into 100 tissue types using foundation model embeddings, automatically generating pseudo-semantic maps for training. Our method synthesizes high-fidelity images with precise region-wise annotations, achieving superior performance on downstream segmentation tasks. When evaluated on annotated datasets, models trained on our synthetic data show competitive performance to those trained on real data, demonstrating the utility of controlled heterogeneous tissue generation. In quantitative evaluation, prompt-guided synthesis reduces Frechet Distance by up to 6X on Camelyon16 (from 430.1 to 72.0) and yields 2-3x lower FD across Panda and TCGA. Downstream DeepLabv3+ models trained solely on synthetic data attain test IoU of 0.71 and 0.95 on Camelyon16 and Panda, within 1-2% of real-data baselines (0.72 and 0.96). By scaling to 11,765 TCGA whole-slide images without manual annotations, our framework offers a practical solution for an urgent need for generating diverse, annotated histopathology data, addressing a critical bottleneck in computational pathology.

</details>


### [58] [ProDyG: Progressive Dynamic Scene Reconstruction via Gaussian Splatting from Monocular Videos](https://arxiv.org/abs/2509.17864)
*Shi Chen,Erik Sandström,Sandro Lombardi,Siyuan Li,Martin R. Oswald*

Main category: cs.CV

TL;DR: 提出了一种在线动态场景重建方法，通过SLAM系统中分离静态和动态部分，使用运动掩码策略进行鲁棒姿态跟踪，并利用运动支架图进行动态部分重建。


<details>
  <summary>Details</summary>
Motivation: 现有SLAM方法通常仅移除动态部分或需要RGB-D输入，离线方法无法扩展到长视频序列，而基于transformer的前馈方法缺乏全局一致性和外观细节。

Method: 在SLAM系统中分离静态和动态部分，采用新颖的运动掩码策略进行鲁棒姿态跟踪，利用渐进式适应的运动支架图重建动态部分。

Result: 方法能够生成与离线方法相竞争的新视角渲染结果，并在跟踪性能上与最先进的动态SLAM方法相当。

Conclusion: 该方法实现了真正实用的动态3D重建，具备在线操作、全局姿态和地图一致性、详细外观建模能力，并能灵活处理RGB和RGB-D输入。

Abstract: Achieving truly practical dynamic 3D reconstruction requires online operation, global pose and map consistency, detailed appearance modeling, and the flexibility to handle both RGB and RGB-D inputs. However, existing SLAM methods typically merely remove the dynamic parts or require RGB-D input, while offline methods are not scalable to long video sequences, and current transformer-based feedforward methods lack global consistency and appearance details. To this end, we achieve online dynamic scene reconstruction by disentangling the static and dynamic parts within a SLAM system. The poses are tracked robustly with a novel motion masking strategy, and dynamic parts are reconstructed leveraging a progressive adaptation of a Motion Scaffolds graph. Our method yields novel view renderings competitive to offline methods and achieves on-par tracking with state-of-the-art dynamic SLAM methods.

</details>


### [59] [DragOSM: Extract Building Roofs and Footprints from Aerial Images by Aligning Historical Labels](https://arxiv.org/abs/2509.17951)
*Kai Li,Xingxing Weng,Yupeng Deng,Yu Meng,Chao Pang,Gui-Song Xia,Xiangyu Zhao*

Main category: cs.CV

TL;DR: 本文提出DragOSM方法，通过引入对齐标记概念来校正OpenStreetMap历史标签与遥感图像中建筑物屋顶和足迹的位置偏差，解决了倾斜图像中建筑物标注不准确的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于分割的方法在倾斜遥感图像中表现不佳，因为屋顶和足迹存在显著位移，且立面像素与屋顶边界融合。虽然可以利用OpenStreetMap等历史标注，但这些标注存在位置偏差且通常只有单一标注（屋顶或足迹），无法准确描述建筑物结构。

Method: 提出DragOSM模型，将标签对齐建模为交互式去噪过程，将位置偏差建模为高斯分布。训练时通过随机高斯扰动模拟错位来学习校正误差，推理时迭代优化输入标签的位置。

Result: 在新建的ReBO数据集（包含179,265个建筑物，5,473张图像）上的实验证明了DragOSM的有效性。

Conclusion: DragOSM能够有效校正OpenStreetMap历史标签与遥感图像的位置偏差，为大规模城市分析提供了可靠的建筑物标注解决方案。

Abstract: Extracting polygonal roofs and footprints from remote sensing images is critical for large-scale urban analysis. Most existing methods rely on segmentation-based models that assume clear semantic boundaries of roofs, but these approaches struggle in off- nadir images, where the roof and footprint are significantly displaced, and facade pixels are fused with the roof boundary. With the increasing availability of open vector map annotations, e.g., OpenStreetMap, utilizing historical labels for off-nadir image annotation has become viable because remote sensing images are georeferenced once captured. However, these historical labels commonly suffer from significant positional discrepancies with new images and only have one annotation (roof or footprint), which fails to describe the correct structures of a building. To address these discrepancies, we first introduce a concept of an alignment token, which encodes the correction vector to guide the label correction. Based on this concept, we then propose Drag OpenStreetMap Labels (DragOSM), a novel model designed to align dislocated historical labels with roofs and footprints. Specifically, DragOSM formulates the label alignment as an interactive denoising process, modeling the positional discrepancy as a Gaussian distribution. During training, it learns to correct these errors by simulating misalignment with random Gaussian perturbations; during inference, it iteratively refines the positions of input labels. To validate our method, we further present a new dataset, Repairing Buildings in OSM (ReBO), comprising 179,265 buildings with both OpenStreetMap and manually corrected annotations across 5,473 images from 41 cities. Experimental results on ReBO demonstrate the effectiveness of DragOSM. Code, dataset, and trained models are publicly available at https://github.com/likaiucas/DragOSM.git.

</details>


### [60] [StableGuard: Towards Unified Copyright Protection and Tamper Localization in Latent Diffusion Models](https://arxiv.org/abs/2509.17993)
*Haoxin Yang,Bangzhen Liu,Xuemiao Xu,Cheng Xu,Yuyang Yu,Zikai Huang,Yi Wang,Shengfeng He*

Main category: cs.CV

TL;DR: StableGuard是一个新颖的框架，通过在扩散生成过程中无缝集成二进制水印，为潜在扩散模型提供端到端的版权保护和篡改定位解决方案。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的进步增强了AI生成内容的真实感，但也引发了滥用担忧，需要强大的版权保护和篡改定位能力。现有方法依赖后处理，导致应用不便和取证可靠性受损。

Method: 开发了多路复用水印VAE（MPW-VAE），通过轻量级潜在残差适配器生成配对的水印和无水印图像；提出专家混合引导取证网络（MoE-GFN），动态整合整体水印模式、局部篡改痕迹和频域线索；两者通过自监督端到端方式联合优化。

Result: 大量实验表明，StableGuard在图像保真度、水印验证和篡改定位方面始终优于最先进的方法。

Conclusion: StableGuard通过端到端设计成功解决了扩散模型中的版权保护和篡改定位问题，实现了水印嵌入和取证准确性之间的协同优化。

Abstract: The advancement of diffusion models has enhanced the realism of AI-generated content but also raised concerns about misuse, necessitating robust copyright protection and tampering localization. Although recent methods have made progress toward unified solutions, their reliance on post hoc processing introduces considerable application inconvenience and compromises forensic reliability. We propose StableGuard, a novel framework that seamlessly integrates a binary watermark into the diffusion generation process, ensuring copyright protection and tampering localization in Latent Diffusion Models through an end-to-end design. We develop a Multiplexing Watermark VAE (MPW-VAE) by equipping a pretrained Variational Autoencoder (VAE) with a lightweight latent residual-based adapter, enabling the generation of paired watermarked and watermark-free images. These pairs, fused via random masks, create a diverse dataset for training a tampering-agnostic forensic network. To further enhance forensic synergy, we introduce a Mixture-of-Experts Guided Forensic Network (MoE-GFN) that dynamically integrates holistic watermark patterns, local tampering traces, and frequency-domain cues for precise watermark verification and tampered region detection. The MPW-VAE and MoE-GFN are jointly optimized in a self-supervised, end-to-end manner, fostering a reciprocal training between watermark embedding and forensic accuracy. Extensive experiments demonstrate that StableGuard consistently outperforms state-of-the-art methods in image fidelity, watermark verification, and tampering localization.

</details>


### [61] [GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction](https://arxiv.org/abs/2509.18090)
*Jiahe Li,Jiawei Zhang,Youmin Zhang,Xiao Bai,Jin Zheng,Xiaohan Yu,Lin Gu*

Main category: cs.CV

TL;DR: GeoSVR是一个基于稀疏体素的显式框架，用于实现准确、详细和完整的表面重建，解决了高斯泼溅方法的表示瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于高斯泼溅的主流方法存在表示瓶颈限制，而稀疏体素在保持覆盖完整性和几何清晰度方面具有优势，但面临缺乏场景约束和局部表面细化的挑战。

Method: 提出体素不确定性深度约束来最大化单目深度线索效果，同时引入体素导向的不确定性避免质量下降；设计稀疏体素表面正则化来增强微小体素的几何一致性，促进基于体素的锐利准确表面形成。

Result: 在多种挑战性场景下的广泛实验表明，该方法在几何精度、细节保持和重建完整性方面优于现有方法，同时保持高效率。

Conclusion: GeoSVR通过创新的体素不确定性深度约束和稀疏体素表面正则化，成功实现了准确、详细和完整的表面重建，为基于稀疏体素的表面重建提供了有效解决方案。

Abstract: Reconstructing accurate surfaces with radiance fields has achieved remarkable progress in recent years. However, prevailing approaches, primarily based on Gaussian Splatting, are increasingly constrained by representational bottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based framework that explores and extends the under-investigated potential of sparse voxels for achieving accurate, detailed, and complete surface reconstruction. As strengths, sparse voxels support preserving the coverage completeness and geometric clarity, while corresponding challenges also arise from absent scene constraints and locality in surface refinement. To ensure correct scene convergence, we first propose a Voxel-Uncertainty Depth Constraint that maximizes the effect of monocular depth cues while presenting a voxel-oriented uncertainty to avoid quality degradation, enabling effective and robust scene constraints yet preserving highly accurate geometries. Subsequently, Sparse Voxel Surface Regularization is designed to enhance geometric consistency for tiny voxels and facilitate the voxel-based formation of sharp and accurate surfaces. Extensive experiments demonstrate our superior performance compared to existing methods across diverse challenging scenarios, excelling in geometric accuracy, detail preservation, and reconstruction completeness while maintaining high efficiency. Code is available at https://github.com/Fictionarry/GeoSVR.

</details>


### [62] [ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image Generation](https://arxiv.org/abs/2509.18092)
*Guocheng Gordon Qian,Daniil Ostashev,Egor Nemchinov,Avihay Assouline,Sergey Tulyakov,Kuan-Chieh Jackson Wang,Kfir Aberman*

Main category: cs.CV

TL;DR: 提出了一种新的属性特定图像提示范式，通过使用不同的参考图像集来分别控制人类外观的各个属性（如发型、服装、身份），实现了对多视觉因素的可组合和分离控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法在个性化文本到图像合成中主要关注身份保持，但缺乏模块化能力，无法提供对特定视觉属性的分离控制。

Method: 将输入编码为属性特定标记，注入预训练的文本到图像扩散模型；构建跨参考训练数据集，提出多属性跨参考训练策略，鼓励模型从不对齐的属性输入中生成忠实输出。

Result: 大量实验表明，该方法在准确遵循视觉和文本提示方面达到了最先进的性能。

Conclusion: 该框架通过将视觉提示与文本驱动生成相结合，为更可配置的人类图像合成铺平了道路。

Abstract: Generating high-fidelity images of humans with fine-grained control over attributes such as hairstyle and clothing remains a core challenge in personalized text-to-image synthesis. While prior methods emphasize identity preservation from a reference image, they lack modularity and fail to provide disentangled control over specific visual attributes. We introduce a new paradigm for attribute-specific image prompting, in which distinct sets of reference images are used to guide the generation of individual aspects of human appearance, such as hair, clothing, and identity. Our method encodes these inputs into attribute-specific tokens, which are injected into a pre-trained text-to-image diffusion model. This enables compositional and disentangled control over multiple visual factors, even across multiple people within a single image. To promote natural composition and robust disentanglement, we curate a cross-reference training dataset featuring subjects in diverse poses and expressions, and propose a multi-attribute cross-reference training strategy that encourages the model to generate faithful outputs from misaligned attribute inputs while adhering to both identity and textual conditioning. Extensive experiments show that our method achieves state-of-the-art performance in accurately following both visual and textual prompts. Our framework paves the way for more configurable human image synthesis by combining visual prompting with text-driven generation. Webpage is available at: https://snap-research.github.io/composeme/.

</details>


### [63] [Seg4Diff: Unveiling Open-Vocabulary Segmentation in Text-to-Image Diffusion Transformers](https://arxiv.org/abs/2509.18096)
*Chaehyun Kim,Heeseong Shin,Eunbeen Hong,Heeji Yoon,Anurag Arnab,Paul Hongsuck Seo,Sunghwan Hong,Seungryong Kim*

Main category: cs.CV

TL;DR: 本文提出了Seg4Diff框架，用于分析多模态扩散变换器（MM-DiT）中的注意力结构，发现特定层能自然产生高质量语义分割掩码，并通过轻量级微调提升分割和生成性能。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态扩散变换器通过联合自注意力实现了丰富的跨模态对齐，但对注意力图如何具体贡献于图像生成的理解仍然有限。

Method: 引入Seg4Diff框架系统分析MM-DiT的注意力结构，识别出语义接地专家层，并应用轻量级微调方案增强语义分组能力。

Result: 发现语义分组是扩散变换器的涌现特性，可以通过选择性放大来同时提升分割性能和生成图像保真度。

Conclusion: 该研究为统一视觉感知和生成的模型铺平了道路，展示了语义分组能力在扩散模型中的重要作用。

Abstract: Text-to-image diffusion models excel at translating language prompts into photorealistic images by implicitly grounding textual concepts through their cross-modal attention mechanisms. Recent multi-modal diffusion transformers extend this by introducing joint self-attention over concatenated image and text tokens, enabling richer and more scalable cross-modal alignment. However, a detailed understanding of how and where these attention maps contribute to image generation remains limited. In this paper, we introduce Seg4Diff (Segmentation for Diffusion), a systematic framework for analyzing the attention structures of MM-DiT, with a focus on how specific layers propagate semantic information from text to image. Through comprehensive analysis, we identify a semantic grounding expert layer, a specific MM-DiT block that consistently aligns text tokens with spatially coherent image regions, naturally producing high-quality semantic segmentation masks. We further demonstrate that applying a lightweight fine-tuning scheme with mask-annotated image data enhances the semantic grouping capabilities of these layers and thereby improves both segmentation performance and generated image fidelity. Our findings demonstrate that semantic grouping is an emergent property of diffusion transformers and can be selectively amplified to advance both segmentation and generation performance, paving the way for unified models that bridge visual perception and generation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [64] [Automated Coral Spawn Monitoring for Reef Restoration: The Coral Spawn and Larvae Imaging Camera System (CSLICS)](https://arxiv.org/abs/2509.17299)
*Dorian Tsai,Christopher A. Brunner,Riki Lamont,F. Mikaela Nordborg,Andrea Severati,Java Terry,Karen Jackel,Matthew Dunbabin,Tobias Fischer,Scarlett Raine*

Main category: cs.RO

TL;DR: 提出了一种名为CSLICS的低成本模块化相机系统，用于自动检测、分类和计数珊瑚产卵，以解决珊瑚礁恢复中人工计数效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 当前珊瑚养殖中的人工产卵计数方法劳动强度大，是珊瑚生产流程中的关键瓶颈，需要自动化解决方案来提高效率和扩大珊瑚礁恢复规模。

Method: 使用低成本模块化相机和基于人机协同标注训练的目标检测器，开发了珊瑚产卵和幼虫成像相机系统（CSLICS），用于幼虫饲养池中的自动产卵计数。

Result: 实验结果显示，表面产卵检测F1分数达82.4%，水下产卵检测F1分数为65.3%，相比人工方法每次产卵事件可节省5720小时劳动时间。

Conclusion: CSLICS系统能够准确测量受精成功率和水下产卵计数，增强了珊瑚养殖过程，有助于扩大珊瑚礁恢复规模以应对气候变化威胁。

Abstract: Coral aquaculture for reef restoration requires accurate and continuous spawn counting for resource distribution and larval health monitoring, but current methods are labor-intensive and represent a critical bottleneck in the coral production pipeline. We propose the Coral Spawn and Larvae Imaging Camera System (CSLICS), which uses low cost modular cameras and object detectors trained using human-in-the-loop labeling approaches for automated spawn counting in larval rearing tanks. This paper details the system engineering, dataset collection, and computer vision techniques to detect, classify and count coral spawn. Experimental results from mass spawning events demonstrate an F1 score of 82.4\% for surface spawn detection at different embryogenesis stages, 65.3\% F1 score for sub-surface spawn detection, and a saving of 5,720 hours of labor per spawning event compared to manual sampling methods at the same frequency. Comparison of manual counts with CSLICS monitoring during a mass coral spawning event on the Great Barrier Reef demonstrates CSLICS' accurate measurement of fertilization success and sub-surface spawn counts. These findings enhance the coral aquaculture process and enable upscaling of coral reef restoration efforts to address climate change threats facing ecosystems like the Great Barrier Reef.

</details>


### [65] [ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion](https://arxiv.org/abs/2509.17941)
*Zichao Hu,Chen Tang,Michael J. Munje,Yifeng Zhu,Alex Liu,Shuijing Liu,Garrett Warnell,Peter Stone,Joydeep Biswas*

Main category: cs.RO

TL;DR: ComposableNav是一个基于扩散模型的机器人导航框架，能够将复杂指令分解为独立的运动基元，通过并行组合来满足训练中未见过的规范组合。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在动态环境中导航时面临的组合性挑战：指令可能包含多个规范，随着机器人技能集的扩展，规范组合数量呈指数级增长。

Method: 采用两阶段训练：1）监督预训练学习基础扩散模型用于动态导航；2）强化学习微调将基础模型塑造成不同的运动基元。部署时并行组合基元来满足新规范组合。

Result: 通过仿真和真实世界实验证明，ComposableNav能够生成满足多样且未见过的规范组合的轨迹，显著优于非组合性VLM策略和成本图组合基线。

Conclusion: ComposableNav通过组合性方法有效解决了机器人导航中的指令遵循问题，展示了处理复杂规范组合的能力。

Abstract: This paper considers the problem of enabling robots to navigate dynamic environments while following instructions. The challenge lies in the combinatorial nature of instruction specifications: each instruction can include multiple specifications, and the number of possible specification combinations grows exponentially as the robot's skill set expands. For example, "overtake the pedestrian while staying on the right side of the road" consists of two specifications: "overtake the pedestrian" and "walk on the right side of the road." To tackle this challenge, we propose ComposableNav, based on the intuition that following an instruction involves independently satisfying its constituent specifications, each corresponding to a distinct motion primitive. Using diffusion models, ComposableNav learns each primitive separately, then composes them in parallel at deployment time to satisfy novel combinations of specifications unseen in training. Additionally, to avoid the onerous need for demonstrations of individual motion primitives, we propose a two-stage training procedure: (1) supervised pre-training to learn a base diffusion model for dynamic navigation, and (2) reinforcement learning fine-tuning that molds the base model into different motion primitives. Through simulation and real-world experiments, we show that ComposableNav enables robots to follow instructions by generating trajectories that satisfy diverse and unseen combinations of specifications, significantly outperforming both non-compositional VLM-based policies and costmap composing baselines. Videos and additional materials can be found on the project page: https://amrl.cs.utexas.edu/ComposableNav/

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [66] [VAInpaint: Zero-Shot Video-Audio inpainting framework with LLMs-driven Module](https://arxiv.org/abs/2509.17022)
*Kam Man Wu,Zeyue Tian,Liya Ji,Qifeng Chen*

Main category: cs.MM

TL;DR: VAInpaint是一个新颖的音频视频修复管道，通过结合分割模型、LLM分析和文本驱动的音频分离模型，实现精确移除视频中物体及其对应音频。


<details>
  <summary>Details</summary>
Motivation: 精确移除视频中的物体及其对应音频而不影响场景其他部分是一个重要挑战，现有方法难以同时处理音频和视频的修复。

Method: 使用分割模型生成掩码指导视频修复，LLM分析全局场景和区域描述，文本驱动的音频分离模型基于定制数据集进行微调。

Result: 实验表明该方法在音频和视频修复方面达到了与当前基准相当的性能。

Conclusion: VAInpaint提供了一种有效的混合音频视觉内容修复解决方案，在保持场景完整性的同时实现精确的对象移除。

Abstract: Video and audio inpainting for mixed audio-visual content has become a crucial task in multimedia editing recently. However, precisely removing an object and its corresponding audio from a video without affecting the rest of the scene remains a significant challenge. To address this, we propose VAInpaint, a novel pipeline that first utilizes a segmentation model to generate masks and guide a video inpainting model in removing objects. At the same time, an LLM then analyzes the scene globally, while a region-specific model provides localized descriptions. Both the overall and regional descriptions will be inputted into an LLM, which will refine the content and turn it into text queries for our text-driven audio separation model. Our audio separation model is fine-tuned on a customized dataset comprising segmented MUSIC instrument images and VGGSound backgrounds to enhance its generalization performance. Experiments show that our method achieves performance comparable to current benchmarks in both audio and video inpainting.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [67] [TASO: Task-Aligned Sparse Optimization for Parameter-Efficient Model Adaptation](https://arxiv.org/abs/2509.17688)
*Daiye Miao,Yufang Liu,Jie Wang,Changzhi Sun,Yunke Zhang,Demei Yan,Shaokang Dong,Qi Zhang,Yuanbin Wu*

Main category: cs.CL

TL;DR: TASO是一种基于预训练模型权重重要性信息来减少LoRA冗余的方法，通过识别任务特定的核心区域来确定LoRA模块的稀疏结构，在保持性能的同时显著减少可训练参数数量。


<details>
  <summary>Details</summary>
Motivation: LoRA方法虽然简单有效，但存在显著的参数冗余问题，这不仅增加了可训练参数数量，还阻碍了微调效果。识别和消除LoRA中的冗余参数是一个具有挑战性的问题。

Method: TASO利用预训练模型权重的重要性信息，估计下游任务的参数重要性，基于重要性分数分布识别任务特定核心区域，并使用这些位置信息确定LoRA模块的稀疏结构，在微调前实现冗余消除。

Result: 实验结果表明，在参数预算与rank=1的LoRA相当的情况下，TASO在多个任务上持续优于标准LoRA，实现了强大的微调性能，同时有效消除了冗余参数。

Conclusion: TASO提供了一种新颖的任务对齐视角来减少LoRA冗余，显著减少了任务适应所需的可训练参数数量，为参数高效微调提供了有效解决方案。

Abstract: LoRA has become one of the most widely used parameter-efficient fine-tuning methods due to its simplicity and effectiveness. However, numerous studies have shown that LoRA often introduces substantial parameter redundancy, which not only increases the number of trainable parameters but also hinders the effectiveness of fine-tuning. Since identifying redundant parameters in LoRA is inherently difficult, how to eliminate them efficiently and accurately remains a challenging problem. In this paper, we propose TASO, a redundancy reduction method that leverages importance information from the pretrained model's weights to mitigate LoRA redundancy. Specifically, we estimate parameter importance on downstream tasks and identify task-specific core regions based on the distribution of importance scores. The location information of these core regions is then used to determine the sparse structure of LoRA modules, enabling redundancy removal before fine-tuning. Our approach significantly reduces the number of trainable parameters required for task adaptation, while providing a novel task-aligned perspective for LoRA redundancy reduction. Experimental results demonstrate that, with a parameter budget comparable to LoRA with rank $r = 1$, TASO consistently outperforms standard LoRA across multiple tasks, achieving strong fine-tuning performance while effectively eliminating redundant parameters.

</details>


### [68] [Qwen3-Omni Technical Report](https://arxiv.org/abs/2509.17765)
*Jin Xu,Zhifang Guo,Hangrui Hu,Yunfei Chu,Xiong Wang,Jinzheng He,Yuxuan Wang,Xian Shi,Ting He,Xinfa Zhu,Yuanjun Lv,Yongqi Wang,Dake Guo,He Wang,Linhan Ma,Pei Zhang,Xinyu Zhang,Hongkun Hao,Zishan Guo,Baosong Yang,Bin Zhang,Ziyang Ma,Xipin Wei,Shuai Bai,Keqin Chen,Xuejing Liu,Peng Wang,Mingkun Yang,Dayiheng Liu,Xingzhang Ren,Bo Zheng,Rui Men,Fan Zhou,Bowen Yu,Jianxin Yang,Le Yu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: Qwen3-Omni是首个在文本、图像、音频和视频多模态任务中保持最先进性能的统一模型，特别在音频任务上表现突出，在36个基准测试中取得开源SOTA。


<details>
  <summary>Details</summary>
Motivation: 开发一个统一的多模态模型，避免传统多模态模型在单一模态性能上的退化，实现跨模态的无缝感知和生成。

Method: 采用Thinker-Talker MoE架构，统一处理文本、图像、音频和视频；使用多码本方案预测离散语音编解码器；引入Thinking模型增强多模态推理能力。

Result: 在音频和音视频基准测试中，Qwen3-Omni在32个开源基准和22个总体基准上达到SOTA，超越Gemini-2.5-Pro等闭源模型；理论端到端首包延迟仅234ms。

Conclusion: Qwen3-Omni成功实现了多模态统一建模，在保持各模态性能的同时显著提升了音频处理能力，为多模态AI发展提供了重要突破。

Abstract: We present Qwen3-Omni, a single multimodal model that, for the first time, maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages, and speech generation in 10 languages. To reduce first-packet latency in streaming synthesis, Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms. To further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.

</details>
