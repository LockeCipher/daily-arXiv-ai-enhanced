<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 27]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.CR](#cs.CR) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory](https://arxiv.org/abs/2507.01110)
*Felix Windisch,Lukas Radl,Thomas Köhler,Michael Steiner,Dieter Schmalstieg,Markus Steinberger*

Main category: cs.GR

TL;DR: 提出了一种无需分区的单GPU训练和渲染超大规模高斯场景的框架，支持多尺度重建和交互式可视化。


<details>
  <summary>Details</summary>
Motivation: 解决高斯泼溅技术在大规模场景中因分区导致的边界伪影、训练复杂性和GPU内存限制问题。

Method: 采用层次化高斯与顺序点树结合的混合数据结构，动态流式传输相关高斯，并利用轻量级缓存和视图调度系统。

Result: 实现了无缝多尺度重建和复杂场景的交互式可视化，从空中俯瞰到地面细节。

Conclusion: 该方法在单消费级GPU上成功训练和渲染超大规模高斯场景，解决了现有技术的局限性。

Abstract: Gaussian Splatting has emerged as a high-performance technique for novel view synthesis, enabling real-time rendering and high-quality reconstruction of small scenes. However, scaling to larger environments has so far relied on partitioning the scene into chunks -- a strategy that introduces artifacts at chunk boundaries, complicates training across varying scales, and is poorly suited to unstructured scenarios such as city-scale flyovers combined with street-level views. Moreover, rendering remains fundamentally limited by GPU memory, as all visible chunks must reside in VRAM simultaneously. We introduce A LoD of Gaussians, a framework for training and rendering ultra-large-scale Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our method stores the full scene out-of-core (e.g., in CPU memory) and trains a Level-of-Detail (LoD) representation directly, dynamically streaming only the relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with Sequential Point Trees enables efficient, view-dependent LoD selection, while a lightweight caching and view scheduling system exploits temporal coherence to support real-time streaming and rendering. Together, these innovations enable seamless multi-scale reconstruction and interactive visualization of complex scenes -- from broad aerial views to fine-grained ground-level details.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [Geometry-aware 4D Video Generation for Robot Manipulation](https://arxiv.org/abs/2507.01099)
*Zeyi Liu,Shuang Li,Eric Cousineau,Siyuan Feng,Benjamin Burchfiel,Shuran Song*

Main category: cs.CV

TL;DR: 提出了一种4D视频生成模型，通过跨视角点图对齐监督，实现多视角3D一致性，提升机器人对复杂环境的动态预测能力。


<details>
  <summary>Details</summary>
Motivation: 增强机器人在复杂环境中规划和交互的能力，解决现有视频生成模型在多视角几何一致性和时间连贯性上的不足。

Method: 提出4D视频生成模型，利用跨视角点图对齐监督训练，学习共享3D场景表示，无需相机姿态输入即可预测新视角的未来视频序列。

Result: 在模拟和真实机器人数据集上，模型生成的视频在视觉稳定性和空间对齐性上优于现有基线方法。

Conclusion: 该方法支持机器人操作和泛化到新视角，通过预测的4D视频可恢复机器人末端执行器轨迹。

Abstract: Understanding and predicting the dynamics of the physical world can enhance a robot's ability to plan and interact effectively in complex environments. While recent video generation models have shown strong potential in modeling dynamic scenes, generating videos that are both temporally coherent and geometrically consistent across camera views remains a significant challenge. To address this, we propose a 4D video generation model that enforces multi-view 3D consistency of videos by supervising the model with cross-view pointmap alignment during training. This geometric supervision enables the model to learn a shared 3D representation of the scene, allowing it to predict future video sequences from novel viewpoints based solely on the given RGB-D observations, without requiring camera poses as inputs. Compared to existing baselines, our method produces more visually stable and spatially aligned predictions across multiple simulated and real-world robotic datasets. We further show that the predicted 4D videos can be used to recover robot end-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting robust robot manipulation and generalization to novel camera viewpoints.

</details>


### [3] [AIGVE-MACS: Unified Multi-Aspect Commenting and Scoring Model for AI-Generated Video Evaluation](https://arxiv.org/abs/2507.01255)
*Xiao Liu,Jiawei Zhang*

Main category: cs.CV

TL;DR: 提出AIGVE-MACS模型，用于AI生成视频的多方面评估，提供分数和语言反馈，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成视频评估指标缺乏解释性，难以与人类评价对齐。

Method: 结合视觉语言模型、加权损失和动态帧采样策略，构建AIGVE-BENCH 2基准。

Result: 在评分相关性和评论质量上达到最优，视频生成质量提升53.5%。

Conclusion: AIGVE-MACS为AI生成视频评估提供了全面、人类对齐的新范式。

Abstract: The rapid advancement of AI-generated video models has created a pressing need for robust and interpretable evaluation frameworks. Existing metrics are limited to producing numerical scores without explanatory comments, resulting in low interpretability and human evaluation alignment. To address those challenges, we introduce AIGVE-MACS, a unified model for AI-Generated Video Evaluation(AIGVE), which can provide not only numerical scores but also multi-aspect language comment feedback in evaluating these generated videos. Central to our approach is AIGVE-BENCH 2, a large-scale benchmark comprising 2,500 AI-generated videos and 22,500 human-annotated detailed comments and numerical scores across nine critical evaluation aspects. Leveraging AIGVE-BENCH 2, AIGVE-MACS incorporates recent Vision-Language Models with a novel token-wise weighted loss and a dynamic frame sampling strategy to better align with human evaluators. Comprehensive experiments across supervised and zero-shot benchmarks demonstrate that AIGVE-MACS achieves state-of-the-art performance in both scoring correlation and comment quality, significantly outperforming prior baselines including GPT-4o and VideoScore. In addition, we further showcase a multi-agent refinement framework where feedback from AIGVE-MACS drives iterative improvements in video generation, leading to 53.5% quality enhancement. This work establishes a new paradigm for comprehensive, human-aligned evaluation of AI-generated videos. We release the AIGVE-BENCH 2 and AIGVE-MACS at https://huggingface.co/xiaoliux/AIGVE-MACS.

</details>


### [4] [DiffusionLight-Turbo: Accelerated Light Probes for Free via Single-Pass Chrome Ball Inpainting](https://arxiv.org/abs/2507.01305)
*Worameth Chinchuthakun,Pakkapon Phongthawee,Amit Raj,Varun Jampani,Pramook Khungurn,Supasorn Suwajanakorn*

Main category: cs.CV

TL;DR: 提出了一种通过将任务重新定义为镀铬球修复问题，从单张低动态范围（LDR）图像估计光照的简单有效技术，利用预训练的扩散模型Stable Diffusion XL，并通过迭代修复生成高质量结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖有限的HDR全景数据集，泛化能力不足，而扩散模型在生成HDR格式的镀铬球时存在内容不一致或错误的问题。

Method: 提出DiffusionLight，通过迭代修复计算多个输出的中值镀铬球作为稳定的低频光照先验，并训练Exposure LoRA生成多曝光LDR图像以合并为HDR光探针。进一步提出DiffusionLight-Turbo，通过Turbo LoRA直接预测迭代过程的平均镀铬球，显著加速推理。

Result: 实验结果表明，该方法在多样化场景中生成逼真的光照估计，并在野外场景中表现出优越的泛化能力。

Conclusion: DiffusionLight及其加速版本DiffusionLight-Turbo通过创新的迭代修复和LoRA技术，有效解决了光照估计任务中的挑战，同时显著提升了效率。

Abstract: We introduce a simple yet effective technique for estimating lighting from a single low-dynamic-range (LDR) image by reframing the task as a chrome ball inpainting problem. This approach leverages a pre-trained diffusion model, Stable Diffusion XL, to overcome the generalization failures of existing methods that rely on limited HDR panorama datasets. While conceptually simple, the task remains challenging because diffusion models often insert incorrect or inconsistent content and cannot readily generate chrome balls in HDR format. Our analysis reveals that the inpainting process is highly sensitive to the initial noise in the diffusion process, occasionally resulting in unrealistic outputs. To address this, we first introduce DiffusionLight, which uses iterative inpainting to compute a median chrome ball from multiple outputs to serve as a stable, low-frequency lighting prior that guides the generation of a high-quality final result. To generate high-dynamic-range (HDR) light probes, an Exposure LoRA is fine-tuned to create LDR images at multiple exposure values, which are then merged. While effective, DiffusionLight is time-intensive, requiring approximately 30 minutes per estimation. To reduce this overhead, we introduce DiffusionLight-Turbo, which reduces the runtime to about 30 seconds with minimal quality loss. This 60x speedup is achieved by training a Turbo LoRA to directly predict the averaged chrome balls from the iterative process. Inference is further streamlined into a single denoising pass using a LoRA swapping technique. Experimental results that show our method produces convincing light estimates across diverse settings and demonstrates superior generalization to in-the-wild scenarios. Our code is available at https://diffusionlight.github.io/turbo

</details>


### [5] [Frequency Domain-Based Diffusion Model for Unpaired Image Dehazing](https://arxiv.org/abs/2507.01275)
*Chengxu Liu,Lu Qi,Jinshan Pan,Xueming Qian,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 提出了一种基于频域的扩散模型（OURS），用于无配对图像去雾，通过振幅残差编码器和相位校正模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于对比学习的方法引入了与雾无关的内容信息，且忽略了频域中雾相关的特性，如振幅谱中的退化表现。

Method: 提出频域扩散模型，利用振幅残差编码器（ARE）补偿振幅差距，并通过相位校正模块（PCM）消除伪影。

Result: 在合成和真实数据集上，OURS优于其他最先进方法。

Conclusion: 频域扩散模型结合振幅和相位优化，显著提升了无配对图像去雾的效果。

Abstract: Unpaired image dehazing has attracted increasing attention due to its flexible data requirements during model training. Dominant methods based on contrastive learning not only introduce haze-unrelated content information, but also ignore haze-specific properties in the frequency domain (\ie,~haze-related degradation is mainly manifested in the amplitude spectrum). To address these issues, we propose a novel frequency domain-based diffusion model, named \ours, for fully exploiting the beneficial knowledge in unpaired clear data. In particular, inspired by the strong generative ability shown by Diffusion Models (DMs), we tackle the dehazing task from the perspective of frequency domain reconstruction and perform the DMs to yield the amplitude spectrum consistent with the distribution of clear images. To implement it, we propose an Amplitude Residual Encoder (ARE) to extract the amplitude residuals, which effectively compensates for the amplitude gap from the hazy to clear domains, as well as provide supervision for the DMs training. In addition, we propose a Phase Correction Module (PCM) to eliminate artifacts by further refining the phase spectrum during dehazing with a simple attention mechanism. Experimental results demonstrate that our \ours outperforms other state-of-the-art methods on both synthetic and real-world datasets.

</details>


### [6] [Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation](https://arxiv.org/abs/2507.01631)
*Camille Billouard,Dawa Derksen,Alexandre Constantin,Bruno Vallet*

Main category: cs.CV

TL;DR: Snake-NeRF是一种用于大规模场景3D重建的框架，通过分块处理和优化采样策略，解决了传统NeRF方法内存占用高的问题。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF方法因内存限制仅适用于小场景，而大规模场景需要分块处理，但传统分块方法可能导致边缘重建错误。

Method: 提出Snake-NeRF框架，采用外存方法避免同时加载所有数据和网络，将场景划分为无重叠的3D块，并引入2×2块级进策略和分段采样器。

Result: 实验表明，Snake-NeRF能在单GPU上线性时间处理大规模卫星图像，且不损失重建质量。

Conclusion: Snake-NeRF成功扩展了NeRF的应用范围，适用于大规模场景的3D重建。

Abstract: Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D reconstruction from multiview satellite imagery. However, state-of-the-art NeRF methods are typically constrained to small scenes due to the memory footprint during training, which we study in this paper. Previous work on large-scale NeRFs palliate this by dividing the scene into NeRFs. This paper introduces Snake-NeRF, a framework that scales to large scenes. Our out-of-core method eliminates the need to load all images and networks simultaneously, and operates on a single device. We achieve this by dividing the region of interest into NeRFs that 3D tile without overlap. Importantly, we crop the images with overlap to ensure each NeRFs is trained with all the necessary pixels. We introduce a novel $2\times 2$ 3D tile progression strategy and segmented sampler, which together prevent 3D reconstruction errors along the tile edges. Our experiments conclude that large satellite images can effectively be processed with linear time complexity, on a single GPU, and without compromise in quality.

</details>


### [7] [Learning Camera-Agnostic White-Balance Preferences](https://arxiv.org/abs/2507.01342)
*Luxi Zhao,Mahmoud Afifi,Michael S. Brown*

Main category: cs.CV

TL;DR: 该论文提出了一种轻量级方法，通过学习后光照估计映射，将中性白平衡校正转换为美学偏好的校正，实现跨相机美学一致性。


<details>
  <summary>Details</summary>
Motivation: 商业自动白平衡（AWB）系统通常追求美学偏好而非准确中性校正，且现有学习型方法难以跨相机传感器泛化。

Method: 提出一种后光照估计映射，将中性AWB校正转换为相机无关空间的美学偏好校正，模型仅含约500参数。

Result: 在771张智能手机图像数据集上，该方法达到最优性能，计算时间仅0.024毫秒。

Conclusion: 该方法在保持轻量级的同时，实现了跨相机美学一致性，兼容现有技术且计算开销低。

Abstract: The image signal processor (ISP) pipeline in modern cameras consists of several modules that transform raw sensor data into visually pleasing images in a display color space. Among these, the auto white balance (AWB) module is essential for compensating for scene illumination. However, commercial AWB systems often strive to compute aesthetic white-balance preferences rather than accurate neutral color correction. While learning-based methods have improved AWB accuracy, they typically struggle to generalize across different camera sensors -- an issue for smartphones with multiple cameras. Recent work has explored cross-camera AWB, but most methods remain focused on achieving neutral white balance. In contrast, this paper is the first to address aesthetic consistency by learning a post-illuminant-estimation mapping that transforms neutral illuminant corrections into aesthetically preferred corrections in a camera-agnostic space. Once trained, our mapping can be applied after any neutral AWB module to enable consistent and stylized color rendering across unseen cameras. Our proposed model is lightweight -- containing only $\sim$500 parameters -- and runs in just 0.024 milliseconds on a typical flagship mobile CPU. Evaluated on a dataset of 771 smartphone images from three different cameras, our method achieves state-of-the-art performance while remaining fully compatible with existing cross-camera AWB techniques, introducing minimal computational and memory overhead.

</details>


### [8] [3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation](https://arxiv.org/abs/2507.01367)
*Tianrui Lou,Xiaojun Jia,Siyuan Liang,Jiawei Liang,Ming Zhang,Yanjun Xiao,Xiaochun Cao*

Main category: cs.CV

TL;DR: 该论文提出了一种基于3D高斯溅射（3DGS）的物理攻击框架PGA，用于解决现有伪装攻击方法在多视角和复杂环境中的鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 现有物理攻击方法依赖目标对象的网格先验和模拟环境，耗时长且与现实差异大，导致对抗效果和鲁棒性不足。

Method: PGA利用3DGS实现快速精确的重建和逼真渲染，通过防止高斯间的相互遮挡和自遮挡，结合min-max优化调整背景，提升多视角鲁棒性。

Result: 大量实验验证了PGA的有效性和优越性。

Conclusion: PGA显著提升了物理攻击在多视角和复杂环境中的对抗效果和鲁棒性。

Abstract: Physical adversarial attack methods expose the vulnerabilities of deep neural networks and pose a significant threat to safety-critical scenarios such as autonomous driving. Camouflage-based physical attack is a more promising approach compared to the patch-based attack, offering stronger adversarial effectiveness in complex physical environments. However, most prior work relies on mesh priors of the target object and virtual environments constructed by simulators, which are time-consuming to obtain and inevitably differ from the real world. Moreover, due to the limitations of the backgrounds in training images, previous methods often fail to produce multi-view robust adversarial camouflage and tend to fall into sub-optimal solutions. Due to these reasons, prior work lacks adversarial effectiveness and robustness across diverse viewpoints and physical environments. We propose a physical attack framework based on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and precise reconstruction with few images, along with photo-realistic rendering capabilities. Our framework further enhances cross-view robustness and adversarial effectiveness by preventing mutual and self-occlusion among Gaussians and employing a min-max optimization approach that adjusts the imaging background of each viewpoint, helping the algorithm filter out non-robust adversarial features. Extensive experiments validate the effectiveness and superiority of PGA. Our code is available at:https://github.com/TRLou/PGA.

</details>


### [9] [Coherent Online Road Topology Estimation and Reasoning with Standard-Definition Maps](https://arxiv.org/abs/2507.01397)
*Khanh Son Pham,Christian Witte,Jens Behley,Johannes Betz,Cyrill Stachniss*

Main category: cs.CV

TL;DR: 提出了一种基于标准地图（SD）的自动驾驶高精地图（HD）预测方法，通过结合先验信息和去噪技术提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中高精地图在线构建的挑战，尤其是道路拓扑的高复杂性建模问题。

Method: 利用先验地图信息（SD地图），提出了一种网络架构，结合混合车道段编码和去噪技术，并引入时间一致性。

Result: 实验表明，该方法显著优于现有方法，验证了建模方案的有效性。

Conclusion: 通过结合先验信息和时间一致性，实现了更优的高精地图预测性能。

Abstract: Most autonomous cars rely on the availability of high-definition (HD) maps. Current research aims to address this constraint by directly predicting HD map elements from onboard sensors and reasoning about the relationships between the predicted map and traffic elements. Despite recent advancements, the coherent online construction of HD maps remains a challenging endeavor, as it necessitates modeling the high complexity of road topologies in a unified and consistent manner. To address this challenge, we propose a coherent approach to predict lane segments and their corresponding topology, as well as road boundaries, all by leveraging prior map information represented by commonly available standard-definition (SD) maps. We propose a network architecture, which leverages hybrid lane segment encodings comprising prior information and denoising techniques to enhance training stability and performance. Furthermore, we facilitate past frames for temporal consistency. Our experimental evaluation demonstrates that our approach outperforms previous methods by a large margin, highlighting the benefits of our modeling scheme.

</details>


### [10] [DocShaDiffusion: Diffusion Model in Latent Space for Document Image Shadow Removal](https://arxiv.org/abs/2507.01422)
*Wenjie Liu,Bingshu Wang,Ze Wang,C. L. Philip Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为DocShaDiffusion的潜在空间扩散模型，用于文档图像阴影去除，并设计了阴影软掩模生成模块（SSGM）和阴影掩模引导扩散模块（SMGDM）以解决彩色阴影问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅处理恒定颜色背景的阴影，而忽略了彩色阴影，因此需要一种更高效的方法来去除文档图像中的彩色阴影。

Method: 1. 设计潜在空间扩散模型DocShaDiffusion；2. 提出SSGM生成阴影掩模并添加噪声；3. 提出SMGDM引导扩散和去噪过程；4. 引入阴影鲁棒感知特征损失；5. 构建合成数据集SDCSRD。

Result: 在三个公开数据集上的实验验证了该方法的优越性。

Conclusion: DocShaDiffusion在文档图像阴影去除任务中表现优异，代码和数据集将公开。

Abstract: Document shadow removal is a crucial task in the field of document image enhancement. However, existing methods tend to remove shadows with constant color background and ignore color shadows. In this paper, we first design a diffusion model in latent space for document image shadow removal, called DocShaDiffusion. It translates shadow images from pixel space to latent space, enabling the model to more easily capture essential features. To address the issue of color shadows, we design a shadow soft-mask generation module (SSGM). It is able to produce accurate shadow mask and add noise into shadow regions specially. Guided by the shadow mask, a shadow mask-aware guided diffusion module (SMGDM) is proposed to remove shadows from document images by supervising the diffusion and denoising process. We also propose a shadow-robust perceptual feature loss to preserve details and structures in document images. Moreover, we develop a large-scale synthetic document color shadow removal dataset (SDCSRD). It simulates the distribution of realistic color shadows and provides powerful supports for the training of models. Experiments on three public datasets validate the proposed method's superiority over state-of-the-art. Our code and dataset will be publicly available.

</details>


### [11] [DiffMark: Diffusion-based Robust Watermark Against Deepfakes](https://arxiv.org/abs/2507.01428)
*Chen Sun,Haiyang Sun,Zhiqing Guo,Yunfeng Diao,Liejun Wang,Dan Ma,Gaobo Yang,Keqin Li*

Main category: cs.CV

TL;DR: DiffMark是一种基于扩散模型的鲁棒水印框架，通过改进训练和采样方案，结合面部图像和水印条件生成水印图像，并利用交叉信息融合模块和对抗性指导增强水印对Deepfake操作的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法对Deepfake操作的鲁棒性不足，扩散模型在图像生成中的优异表现提供了改进水印技术的机会。

Method: 通过修改扩散模型的训练和采样方案，结合面部图像和水印条件，使用交叉信息融合模块和对抗性指导生成水印图像。

Result: 实验证明DiffMark在典型Deepfake操作中表现有效。

Conclusion: DiffMark通过扩散模型和对抗性指导实现了对Deepfake操作的鲁棒水印生成。

Abstract: Deepfakes pose significant security and privacy threats through malicious facial manipulations. While robust watermarking can aid in authenticity verification and source tracking, existing methods often lack the sufficient robustness against Deepfake manipulations. Diffusion models have demonstrated remarkable performance in image generation, enabling the seamless fusion of watermark with image during generation. In this study, we propose a novel robust watermarking framework based on diffusion model, called DiffMark. By modifying the training and sampling scheme, we take the facial image and watermark as conditions to guide the diffusion model to progressively denoise and generate corresponding watermarked image. In the construction of facial condition, we weight the facial image by a timestep-dependent factor that gradually reduces the guidance intensity with the decrease of noise, thus better adapting to the sampling process of diffusion model. To achieve the fusion of watermark condition, we introduce a cross information fusion (CIF) module that leverages a learnable embedding table to adaptively extract watermark features and integrates them with image features via cross-attention. To enhance the robustness of the watermark against Deepfake manipulations, we integrate a frozen autoencoder during training phase to simulate Deepfake manipulations. Additionally, we introduce Deepfake-resistant guidance that employs specific Deepfake model to adversarially guide the diffusion sampling process to generate more robust watermarked images. Experimental results demonstrate the effectiveness of the proposed DiffMark on typical Deepfakes. Our code will be available at https://github.com/vpsg-research/DiffMark.

</details>


### [12] [Representation Entanglement for Generation:Training Diffusion Transformers Is Much Easier Than You Think](https://arxiv.org/abs/2507.01467)
*Ge Wu,Shen Zhang,Ruijing Shi,Shanghua Gao,Zhenyuan Chen,Lei Wang,Zhaowei Chen,Hongcheng Gao,Yao Tang,Jian Yang,Ming-Ming Cheng,Xiang Li*

Main category: cs.CV

TL;DR: REG通过将低层图像潜在表示与预训练模型的高层类别标记纠缠，显著提升了扩散模型的生成质量和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如REPA）在去噪推理过程中未能充分利用判别性表示，限制了生成模型的潜力。

Method: 提出REG方法，将图像潜在表示与预训练模型的单一高层类别标记纠缠，实现图像和类别的联合生成。

Result: 在ImageNet 256×256上，REG显著加速训练（63倍和23倍），并在更短训练时间内超越现有方法。

Conclusion: REG通过语义引导生成，高效且轻量地提升了扩散模型的性能。

Abstract: REPA and its variants effectively mitigate training challenges in diffusion models by incorporating external visual representations from pretrained models, through alignment between the noisy hidden projections of denoising networks and foundational clean image representations. We argue that the external alignment, which is absent during the entire denoising inference process, falls short of fully harnessing the potential of discriminative representations. In this work, we propose a straightforward method called Representation Entanglement for Generation (REG), which entangles low-level image latents with a single high-level class token from pretrained foundation models for denoising. REG acquires the capability to produce coherent image-class pairs directly from pure noise, substantially improving both generation quality and training efficiency. This is accomplished with negligible additional inference overhead, requiring only one single additional token for denoising (<0.5\% increase in FLOPs and latency). The inference process concurrently reconstructs both image latents and their corresponding global semantics, where the acquired semantic knowledge actively guides and enhances the image generation process. On ImageNet 256$\times$256, SiT-XL/2 + REG demonstrates remarkable convergence acceleration, achieving $\textbf{63}\times$ and $\textbf{23}\times$ faster training than SiT-XL/2 and SiT-XL/2 + REPA, respectively. More impressively, SiT-L/2 + REG trained for merely 400K iterations outperforms SiT-XL/2 + REPA trained for 4M iterations ($\textbf{10}\times$ longer). Code is available at: https://github.com/Martinser/REG.

</details>


### [13] [A Gift from the Integration of Discriminative and Diffusion-based Generative Learning: Boundary Refinement Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2507.01573)
*Hao Wang,Keyan Hu,Xin Guo,Haifeng Li,Chao Tao*

Main category: cs.CV

TL;DR: 论文提出了一种结合判别式学习和扩散生成学习的框架IDGBR，用于优化遥感图像语义分割的边界精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖判别式学习，擅长捕捉低频特征但忽视高频边界细节，而扩散生成模型擅长生成高频细节但低频语义推断不足。

Method: 结合判别式模型生成粗分割图，并通过扩散去噪过程细化边界。

Result: 在五个遥感数据集上验证了IDGBR能一致优化不同判别式架构的粗分割结果。

Conclusion: IDGBR框架有效整合了判别式和生成式学习的优势，显著提升了边界分割精度。

Abstract: Remote sensing semantic segmentation must address both what the ground objects are within an image and where they are located. Consequently, segmentation models must ensure not only the semantic correctness of large-scale patches (low-frequency information) but also the precise localization of boundaries between patches (high-frequency information). However, most existing approaches rely heavily on discriminative learning, which excels at capturing low-frequency features, while overlooking its inherent limitations in learning high-frequency features for semantic segmentation. Recent studies have revealed that diffusion generative models excel at generating high-frequency details. Our theoretical analysis confirms that the diffusion denoising process significantly enhances the model's ability to learn high-frequency features; however, we also observe that these models exhibit insufficient semantic inference for low-frequency features when guided solely by the original image. Therefore, we integrate the strengths of both discriminative and generative learning, proposing the Integration of Discriminative and diffusion-based Generative learning for Boundary Refinement (IDGBR) framework. The framework first generates a coarse segmentation map using a discriminative backbone model. This map and the original image are fed into a conditioning guidance network to jointly learn a guidance representation subsequently leveraged by an iterative denoising diffusion process refining the coarse segmentation. Extensive experiments across five remote sensing semantic segmentation datasets (binary and multi-class segmentation) confirm our framework's capability of consistent boundary refinement for coarse results from diverse discriminative architectures. The source code will be available at https://github.com/KeyanHu-git/IDGBR.

</details>


### [14] [SketchColour: Channel Concat Guided DiT-based Sketch-to-Colour Pipeline for 2D Animation](https://arxiv.org/abs/2507.01586)
*Bryan Constantine Sadihin,Michael Hua Wang,Shei Pern Chua,Hang Su*

Main category: cs.CV

TL;DR: SketchColour是一种基于扩散变换器（DiT）的2D动画草图着色方法，通过轻量级适配器和LoRA微调，显著减少参数和GPU内存使用，并在SAKUGA数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统2D动画制作需要大量手工绘制和着色，耗时耗力。本文旨在通过自动化技术提高效率。

Method: 采用扩散变换器（DiT）替代传统U-Net去噪器，结合轻量级通道连接适配器和LoRA微调，避免参数膨胀。

Result: 在SAKUGA数据集上表现优于现有方法，训练数据仅为竞争模型的一半，且生成动画时间一致、伪影少。

Conclusion: SketchColour提供了一种高效、低成本的2D动画着色解决方案，显著提升生产效率。

Abstract: The production of high-quality 2D animation is highly labor-intensive process, as animators are currently required to draw and color a large number of frames by hand. We present SketchColour, the first sketch-to-colour pipeline for 2D animation built on a diffusion transformer (DiT) backbone. By replacing the conventional U-Net denoiser with a DiT-style architecture and injecting sketch information via lightweight channel-concatenation adapters accompanied with LoRA finetuning, our method natively integrates conditioning without the parameter and memory bloat of a duplicated ControlNet, greatly reducing parameter count and GPU memory usage. Evaluated on the SAKUGA dataset, SketchColour outperforms previous state-of-the-art video colourization methods across all metrics, despite using only half the training data of competing models. Our approach produces temporally coherent animations with minimal artifacts such as colour bleeding or object deformation. Our code is available at: https://bconstantine.github.io/SketchColour .

</details>


### [15] [Towards Controllable Real Image Denoising with Camera Parameters](https://arxiv.org/abs/2507.01587)
*Youngjin Oh,Junhyeong Kwon,Keuntek Lee,Nam Ik Cho*

Main category: cs.CV

TL;DR: 提出了一种基于相机参数的可控去噪框架，通过ISO、快门速度和F值调整去噪强度。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法缺乏根据噪声水平、相机设置和用户偏好调整去噪强度的灵活性。

Method: 将ISO、快门速度和F值转换为向量，用于控制去噪网络的性能。

Result: 实验表明，该方法为去噪网络增加了可控性并提升了性能。

Conclusion: 该框架成功实现了基于相机参数的自适应去噪，代码已开源。

Abstract: Recent deep learning-based image denoising methods have shown impressive performance; however, many lack the flexibility to adjust the denoising strength based on the noise levels, camera settings, and user preferences. In this paper, we introduce a new controllable denoising framework that adaptively removes noise from images by utilizing information from camera parameters. Specifically, we focus on ISO, shutter speed, and F-number, which are closely related to noise levels. We convert these selected parameters into a vector to control and enhance the performance of the denoising network. Experimental results show that our method seamlessly adds controllability to standard denoising neural networks and improves their performance. Code is available at https://github.com/OBAKSA/CPADNet.

</details>


### [16] [DepthSync: Diffusion Guidance-Based Depth Synchronization for Scale- and Geometry-Consistent Video Depth Estimation](https://arxiv.org/abs/2507.01603)
*Yue-Jiang Dong,Wang Zhao,Jiale Xu,Ying Shan,Song-Hai Zhang*

Main category: cs.CV

TL;DR: DepthSync提出了一种无需训练的框架，通过扩散引导实现长视频深度预测的尺度和几何一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理长视频时存在尺度不一致和几何预测不一致的问题，主要由于滑动窗口分割和仅依赖2D扩散先验。

Method: 引入尺度引导和几何引导，分别同步窗口间的深度尺度和窗口内的几何对齐，协同指导去噪过程。

Result: 实验表明，DepthSync在多数据集上显著提升了深度预测的尺度和几何一致性，尤其适用于长视频。

Conclusion: DepthSync通过尺度和几何引导，有效解决了长视频深度预测的一致性问题，无需额外训练。

Abstract: Diffusion-based video depth estimation methods have achieved remarkable success with strong generalization ability. However, predicting depth for long videos remains challenging. Existing methods typically split videos into overlapping sliding windows, leading to accumulated scale discrepancies across different windows, particularly as the number of windows increases. Additionally, these methods rely solely on 2D diffusion priors, overlooking the inherent 3D geometric structure of video depths, which results in geometrically inconsistent predictions. In this paper, we propose DepthSync, a novel, training-free framework using diffusion guidance to achieve scale- and geometry-consistent depth predictions for long videos. Specifically, we introduce scale guidance to synchronize the depth scale across windows and geometry guidance to enforce geometric alignment within windows based on the inherent 3D constraints in video depths. These two terms work synergistically, steering the denoising process toward consistent depth predictions. Experiments on various datasets validate the effectiveness of our method in producing depth estimates with improved scale and geometry consistency, particularly for long videos.

</details>


### [17] [Autoregressive Image Generation with Linear Complexity: A Spatial-Aware Decay Perspective](https://arxiv.org/abs/2507.01652)
*Yuxin Mao,Zhen Qin,Jinxing Zhou,Hui Deng,Xuyang Shen,Bin Fan,Jing Zhang,Yiran Zhong,Yuchao Dai*

Main category: cs.CV

TL;DR: LASADGen提出了一种新型线性注意力机制LASAD，通过保留2D空间关系，解决了传统线性注意力在图像生成中长距离依赖不足的问题，实现了高效且高质量的图像生成。


<details>
  <summary>Details</summary>
Motivation: 传统自回归模型依赖Transformer架构，计算复杂度高且内存开销大。线性注意力虽能降低计算负担，但在图像生成中因无法捕捉长距离依赖而质量下降。

Method: 提出LASAD机制，基于真实2D空间位置计算位置依赖的衰减因子，保留空间关系。基于此构建LASADGen模型，实现线性复杂度的选择性空间上下文注意力。

Result: 在ImageNet上，LASADGen在图像生成质量和计算效率上达到最优水平。

Conclusion: LASADGen通过结合线性注意力的高效性和空间理解能力，为高质量图像生成提供了新思路。

Abstract: Autoregressive (AR) models have garnered significant attention in image generation for their ability to effectively capture both local and global structures within visual data. However, prevalent AR models predominantly rely on the transformer architectures, which are beset by quadratic computational complexity concerning input sequence length and substantial memory overhead due to the necessity of maintaining key-value caches. Although linear attention mechanisms have successfully reduced this burden in language models, our initial experiments reveal that they significantly degrade image generation quality because of their inability to capture critical long-range dependencies in visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a novel attention mechanism that explicitly preserves genuine 2D spatial relationships within the flattened image sequences by computing position-dependent decay factors based on true 2D spatial location rather than 1D sequence positions. Based on this mechanism, we present LASADGen, an autoregressive image generator that enables selective attention to relevant spatial contexts with linear complexity. Experiments on ImageNet show LASADGen achieves state-of-the-art image generation performance and computational efficiency, bridging the gap between linear attention's efficiency and spatial understanding needed for high-quality generation.

</details>


### [18] [RobuSTereo: Robust Zero-Shot Stereo Matching under Adverse Weather](https://arxiv.org/abs/2507.01653)
*Yuran Wang,Yingping Liang,Yutao Hu,Ying Fu*

Main category: cs.CV

TL;DR: RobuSTereo框架通过扩散模拟和稳健特征编码，提升立体匹配模型在恶劣天气下的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有立体匹配模型在恶劣天气下表现不佳，主要因数据稀缺和特征提取困难。

Method: 提出扩散模拟生成合成数据，并设计稳健特征编码器（ConvNet+去噪Transformer）。

Result: 实验表明，RobuSTereo显著提升了模型在恶劣天气下的鲁棒性和泛化能力。

Conclusion: RobuSTereo有效解决了数据稀缺和特征提取问题，提升了立体匹配模型在恶劣天气下的性能。

Abstract: Learning-based stereo matching models struggle in adverse weather conditions due to the scarcity of corresponding training data and the challenges in extracting discriminative features from degraded images. These limitations significantly hinder zero-shot generalization to out-of-distribution weather conditions. In this paper, we propose \textbf{RobuSTereo}, a novel framework that enhances the zero-shot generalization of stereo matching models under adverse weather by addressing both data scarcity and feature extraction challenges. First, we introduce a diffusion-based simulation pipeline with a stereo consistency module, which generates high-quality stereo data tailored for adverse conditions. By training stereo matching models on our synthetic datasets, we reduce the domain gap between clean and degraded images, significantly improving the models' robustness to unseen weather conditions. The stereo consistency module ensures structural alignment across synthesized image pairs, preserving geometric integrity and enhancing depth estimation accuracy. Second, we design a robust feature encoder that combines a specialized ConvNet with a denoising transformer to extract stable and reliable features from degraded images. The ConvNet captures fine-grained local structures, while the denoising transformer refines global representations, effectively mitigating the impact of noise, low visibility, and weather-induced distortions. This enables more accurate disparity estimation even under challenging visual conditions. Extensive experiments demonstrate that \textbf{RobuSTereo} significantly improves the robustness and generalization of stereo matching models across diverse adverse weather scenarios.

</details>


### [19] [Using Wavelet Domain Fingerprints to Improve Source Camera Identification](https://arxiv.org/abs/2507.01712)
*Xinle Tian,Matthew Nunes,Emiko Dupont,Shaunagh Downing,Freddie Lichtenstein,Matt Burns*

Main category: cs.CV

TL;DR: 提出了一种基于小波域的指纹提取方法，简化了SPN提取和比较流程，提高了检测精度和处理速度。


<details>
  <summary>Details</summary>
Motivation: 传统小波去噪方法在提取传感器模式噪声（SPN）时需要图像重建步骤，效率较低。

Method: 提出小波域指纹概念，直接在频域进行指纹提取和比较，省去反演步骤。

Result: 实验表明，该方法在真实数据集上检测精度更高，处理速度显著提升。

Conclusion: 小波域指纹方法优化了SPN提取流程，具有更高的效率和准确性。

Abstract: Camera fingerprint detection plays a crucial role in source identification and image forensics, with wavelet denoising approaches proving to be particularly effective in extracting sensor pattern noise (SPN). In this article, we propose a modification to wavelet-based SPN extraction. Rather than constructing the fingerprint as an image, we introduce the notion of a wavelet domain fingerprint. This avoids the final inversion step of the denoising algorithm and allows fingerprint comparisons to be made directly in the wavelet domain. As such, our modification streamlines the extraction and comparison process. Experimental results on real-world datasets demonstrate that our method not only achieves higher detection accuracy but can also significantly improve processing speed.

</details>


### [20] [Calibrated Self-supervised Vision Transformers Improve Intracranial Arterial Calcification Segmentation from Clinical CT Head Scans](https://arxiv.org/abs/2507.01744)
*Benjamin Jin,Grant Mair,Joanna M. Wardlaw,Maria del C. Valdés Hernández*

Main category: cs.CV

TL;DR: 该论文研究了在3D医学图像分割中使用Vision Transformers（ViTs）的潜力，特别是针对颅内动脉钙化（IAC）的自动量化。通过自监督的MAE框架预训练ViTs，并在IST-3临床试验数据上进行微调，取得了优于传统监督方法的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管ViTs在自然图像领域表现优异，但在3D医学图像分割中应用较少。IAC是一种与中风和痴呆相关的生物标志物，自动量化IAC有助于大规模风险评估。

Method: 采用MAE框架对ViTs进行自监督预训练，并在IST-3数据集上进行微调，重点研究了ViTs在IAC分割中的关键因素（如patch大小和上采样方法）。

Result: 1）自监督ViT比监督nnU-Net基线高3.2 Dice分数；2）小patch尺寸对IAC分割至关重要，插值上采样优于转置卷积；3）ViT对高切片厚度更具鲁棒性，临床风险分类提升46%。

Conclusion: 研究表明，自监督ViTs在3D医学图像分割中具有显著优势，尤其是对IAC的自动量化，为临床风险评估提供了新工具。

Abstract: Vision Transformers (ViTs) have gained significant popularity in the natural image domain but have been less successful in 3D medical image segmentation. Nevertheless, 3D ViTs are particularly interesting for large medical imaging volumes due to their efficient self-supervised training within the masked autoencoder (MAE) framework, which enables the use of imaging data without the need for expensive manual annotations. intracranial arterial calcification (IAC) is an imaging biomarker visible on routinely acquired CT scans linked to neurovascular diseases such as stroke and dementia, and automated IAC quantification could enable their large-scale risk assessment. We pre-train ViTs with MAE and fine-tune them for IAC segmentation for the first time. To develop our models, we use highly heterogeneous data from a large clinical trial, the third International Stroke Trial (IST-3). We evaluate key aspects of MAE pre-trained ViTs in IAC segmentation, and analyse the clinical implications. We show: 1) our calibrated self-supervised ViT beats a strong supervised nnU-Net baseline by 3.2 Dice points, 2) low patch sizes are crucial for ViTs for IAC segmentation and interpolation upsampling with regular convolutions is preferable to transposed convolutions for ViT-based models, and 3) our ViTs increase robustness to higher slice thicknesses and improve risk group classification in a clinical scenario by 46%. Our code is available online.

</details>


### [21] [Rethinking Discrete Tokens: Treating Them as Conditions for Continuous Autoregressive Image Synthesis](https://arxiv.org/abs/2507.01756)
*Peng Zheng,Junke Wang,Yi Chang,Yizhou Yu,Rui Ma,Zuxuan Wu*

Main category: cs.CV

TL;DR: 论文提出DisCon框架，通过将离散标记作为条件信号而非生成目标，解决了连续标记建模的优化挑战，同时避免了量化带来的信息损失，显著提升了图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前基于自回归的视觉生成模型因量化过程导致信息损失，影响图像保真度。连续标记虽能避免量化问题，但因其高维无界特性，增加了密度估计难度和生成异常的风险。

Method: 提出DisCon框架，将离散标记作为条件信号，建模连续表示的条件概率，避免直接建模连续标记的挑战。

Result: 在ImageNet 256×256生成任务中，DisCon的gFID得分为1.38，显著优于现有自回归方法。

Conclusion: DisCon通过结合离散和连续标记的优势，有效提升了图像生成的质量和效率。

Abstract: Recent advances in large language models (LLMs) have spurred interests in encoding images as discrete tokens and leveraging autoregressive (AR) frameworks for visual generation. However, the quantization process in AR-based visual generation models inherently introduces information loss that degrades image fidelity. To mitigate this limitation, recent studies have explored to autoregressively predict continuous tokens. Unlike discrete tokens that reside in a structured and bounded space, continuous representations exist in an unbounded, high-dimensional space, making density estimation more challenging and increasing the risk of generating out-of-distribution artifacts. Based on the above findings, this work introduces DisCon (Discrete-Conditioned Continuous Autoregressive Model), a novel framework that reinterprets discrete tokens as conditional signals rather than generation targets. By modeling the conditional probability of continuous representations conditioned on discrete tokens, DisCon circumvents the optimization challenges of continuous token modeling while avoiding the information loss caused by quantization. DisCon achieves a gFID score of 1.38 on ImageNet 256$\times$256 generation, outperforming state-of-the-art autoregressive approaches by a clear margin.

</details>


### [22] [FreeLoRA: Enabling Training-Free LoRA Fusion for Autoregressive Multi-Subject Personalization](https://arxiv.org/abs/2507.01792)
*Peng Zheng,Ye Wang,Rui Ma,Zuxuan Wu*

Main category: cs.CV

TL;DR: FreeLoRA提出了一种无需训练的框架，通过融合多个特定主题的LoRA模块实现多主题个性化图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多主题个性化生成时需复杂调整或联合优化，FreeLoRA旨在简化这一过程。

Method: 采用Full Token Tuning策略训练每个主题的LoRA模块，并通过Subject-Aware Inference在推理时激活对应模块。

Result: 实验表明FreeLoRA在主题保真度和提示一致性上表现优异。

Conclusion: FreeLoRA为多主题个性化生成提供了一种简单高效的解决方案。

Abstract: Subject-driven image generation plays a crucial role in applications such as virtual try-on and poster design. Existing approaches typically fine-tune pretrained generative models or apply LoRA-based adaptations for individual subjects. However, these methods struggle with multi-subject personalization, as combining independently adapted modules often requires complex re-tuning or joint optimization. We present FreeLoRA, a simple and generalizable framework that enables training-free fusion of subject-specific LoRA modules for multi-subject personalization. Each LoRA module is adapted on a few images of a specific subject using a Full Token Tuning strategy, where it is applied across all tokens in the prompt to encourage weakly supervised token-content alignment. At inference, we adopt Subject-Aware Inference, activating each module only on its corresponding subject tokens. This enables training-free fusion of multiple personalized subjects within a single image, while mitigating overfitting and mutual interference between subjects. Extensive experiments show that FreeLoRA achieves strong performance in both subject fidelity and prompt consistency.

</details>


### [23] [MobileIE: An Extremely Lightweight and Effective ConvNet for Real-Time Image Enhancement on Mobile Devices](https://arxiv.org/abs/2507.01838)
*Hailong Yan,Ao Li,Xiangtao Zhang,Zhe Liu,Zenglin Shi,Ce Zhu,Le Zhang*

Main category: cs.CV

TL;DR: 提出了一种极轻量级的CNN框架，用于移动设备上的实时图像增强，通过重参数化和增量权重优化策略实现高效性。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在资源受限平台上部署的高计算和内存需求问题。

Method: 结合重参数化和增量权重优化策略，引入特征自变换模块和分层双路径注意力机制，并使用局部方差加权损失优化性能。

Result: 首次实现高达1,100 FPS的实时图像增强推理，并在多个任务中取得速度和性能的最佳平衡。

Conclusion: 该框架为移动设备上的实时图像增强提供了高效解决方案，代码已开源。

Abstract: Recent advancements in deep neural networks have driven significant progress in image enhancement (IE). However, deploying deep learning models on resource-constrained platforms, such as mobile devices, remains challenging due to high computation and memory demands. To address these challenges and facilitate real-time IE on mobile, we introduce an extremely lightweight Convolutional Neural Network (CNN) framework with around 4K parameters. Our approach integrates reparameterization with an Incremental Weight Optimization strategy to ensure efficiency. Additionally, we enhance performance with a Feature Self-Transform module and a Hierarchical Dual-Path Attention mechanism, optimized with a Local Variance-Weighted loss. With this efficient framework, we are the first to achieve real-time IE inference at up to 1,100 frames per second (FPS) while delivering competitive image quality, achieving the best trade-off between speed and performance across multiple IE tasks. The code will be available at https://github.com/AVC2-UESTC/MobileIE.git.

</details>


### [24] [Reasoning to Edit: Hypothetical Instruction-Based Image Editing with Visual Reasoning](https://arxiv.org/abs/2507.01908)
*Qingdong He,Xueqin Chen,Chaoyi Wang,Yanjie Pan,Xiaobin Hu,Zhenye Gan,Yabiao Wang,Chengjie Wang,Xiangtai Li,Jiangning Zhang*

Main category: cs.CV

TL;DR: 论文提出Reason50K数据集和ReasonBrain框架，用于处理复杂的隐含假设指令图像编辑任务，通过多模态大语言模型和扩散模型实现推理和编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理复杂的隐含假设指令，且缺乏支持推理的训练和评估数据集。

Method: 提出Reason50K数据集和ReasonBrain框架，结合MLLM和扩散模型，并引入FRCE模块和CME增强器。

Result: ReasonBrain在推理场景中表现优于现有方法，并具有零样本泛化能力。

Conclusion: Reason50K和ReasonBrain为复杂指令图像编辑提供了有效解决方案，数据集和代码将公开。

Abstract: Instruction-based image editing (IIE) has advanced rapidly with the success of diffusion models. However, existing efforts primarily focus on simple and explicit instructions to execute editing operations such as adding, deleting, moving, or swapping objects. They struggle to handle more complex implicit hypothetical instructions that require deeper reasoning to infer plausible visual changes and user intent. Additionally, current datasets provide limited support for training and evaluating reasoning-aware editing capabilities. Architecturally, these methods also lack mechanisms for fine-grained detail extraction that support such reasoning. To address these limitations, we propose Reason50K, a large-scale dataset specifically curated for training and evaluating hypothetical instruction reasoning image editing, along with ReasonBrain, a novel framework designed to reason over and execute implicit hypothetical instructions across diverse scenarios. Reason50K includes over 50K samples spanning four key reasoning scenarios: Physical, Temporal, Causal, and Story reasoning. ReasonBrain leverages Multimodal Large Language Models (MLLMs) for editing guidance generation and a diffusion model for image synthesis, incorporating a Fine-grained Reasoning Cue Extraction (FRCE) module to capture detailed visual and textual semantics essential for supporting instruction reasoning. To mitigate the semantic loss, we further introduce a Cross-Modal Enhancer (CME) that enables rich interactions between the fine-grained cues and MLLM-derived features. Extensive experiments demonstrate that ReasonBrain consistently outperforms state-of-the-art baselines on reasoning scenarios while exhibiting strong zero-shot generalization to conventional IIE tasks. Our dataset and code will be released publicly.

</details>


### [25] [LongAnimation: Long Animation Generation with Dynamic Global-Local Memory](https://arxiv.org/abs/2507.01945)
*Nan Chen,Mengqi Huang,Yihao Meng,Zhendong Mao*

Main category: cs.CV

TL;DR: 提出了一种名为LongAnimation的框架，用于解决长动画着色中的颜色一致性问题，结合动态全局-局部记忆模块和颜色一致性奖励。


<details>
  <summary>Details</summary>
Motivation: 长动画着色在动画产业中成本高昂，现有方法仅适用于短片段且忽视全局信息，导致长期颜色不一致。

Method: 提出LongAnimation框架，包括SketchDiT、动态全局-局部记忆模块（DGLM）和颜色一致性奖励，动态提取全局特征并融合局部信息。

Result: 在短片段（14帧）和长片段（平均500帧）动画上验证了LongAnimation在保持颜色一致性方面的有效性。

Conclusion: LongAnimation通过动态全局-局部范式显著提升了长动画着色的颜色一致性，适用于开放领域任务。

Abstract: Animation colorization is a crucial part of real animation industry production. Long animation colorization has high labor costs. Therefore, automated long animation colorization based on the video generation model has significant research value. Existing studies are limited to short-term colorization. These studies adopt a local paradigm, fusing overlapping features to achieve smooth transitions between local segments. However, the local paradigm neglects global information, failing to maintain long-term color consistency. In this study, we argue that ideal long-term color consistency can be achieved through a dynamic global-local paradigm, i.e., dynamically extracting global color-consistent features relevant to the current generation. Specifically, we propose LongAnimation, a novel framework, which mainly includes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color Consistency Reward. The SketchDiT captures hybrid reference features to support the DGLM module. The DGLM module employs a long video understanding model to dynamically compress global historical features and adaptively fuse them with the current generation features. To refine the color consistency, we introduce a Color Consistency Reward. During inference, we propose a color consistency fusion to smooth the video segment transition. Extensive experiments on both short-term (14 frames) and long-term (average 500 frames) animations show the effectiveness of LongAnimation in maintaining short-term and long-term color consistency for open-domain animation colorization task. The code can be found at https://cn-makers.github.io/long_animation_web/.

</details>


### [26] [FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model](https://arxiv.org/abs/2507.01953)
*Yukang Cao,Chenyang Si,Jinghao Wang,Ziwei Liu*

Main category: cs.CV

TL;DR: FreeMorph是一种无需调优的图像变形方法，适用于不同语义或布局的输入，通过创新设计解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预训练扩散模型的微调，受限于时间和语义/布局差异，FreeMorph旨在无需实例训练即可实现高质量图像变形。

Method: 1) 提出基于引导的球形插值设计，通过修改自注意力模块解决身份丢失问题；2) 引入步长导向的变分趋势，混合输入图像的自注意力模块以实现可控过渡。

Result: FreeMorph性能优于现有方法，速度快10~50倍，成为图像变形的新标杆。

Conclusion: FreeMorph通过创新设计解决了调优方法的局限性，实现了高效、高质量的图像变形。

Abstract: We present FreeMorph, the first tuning-free method for image morphing that accommodates inputs with different semantics or layouts. Unlike existing methods that rely on finetuning pre-trained diffusion models and are limited by time constraints and semantic/layout discrepancies, FreeMorph delivers high-fidelity image morphing without requiring per-instance training. Despite their efficiency and potential, tuning-free methods face challenges in maintaining high-quality results due to the non-linear nature of the multi-step denoising process and biases inherited from the pre-trained diffusion model. In this paper, we introduce FreeMorph to address these challenges by integrating two key innovations. 1) We first propose a guidance-aware spherical interpolation design that incorporates explicit guidance from the input images by modifying the self-attention modules, thereby addressing identity loss and ensuring directional transitions throughout the generated sequence. 2) We further introduce a step-oriented variation trend that blends self-attention modules derived from each input image to achieve controlled and consistent transitions that respect both inputs. Our extensive evaluations demonstrate that FreeMorph outperforms existing methods, being 10x ~ 50x faster and establishing a new state-of-the-art for image morphing.

</details>


### [27] [How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks](https://arxiv.org/abs/2507.01955)
*Rahul Ramachandran,Ali Garjani,Roman Bachmann,Andrei Atanov,Oğuzhan Fatih Kar,Amir Zamir*

Main category: cs.CV

TL;DR: 论文评估了多模态基础模型在标准计算机视觉任务上的表现，发现它们虽不及专业模型，但作为通用模型表现尚可，且在语义任务上优于几何任务。GPT-4o在非推理模型中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探讨多模态基础模型在视觉理解方面的实际能力，并解决其无法直接表达复杂视觉任务（如分割或3D几何）的挑战。

Method: 通过提示链将标准视觉任务转化为文本可提示和API兼容的任务，建立标准化评估框架。

Result: 模型在语义任务上表现优于几何任务，GPT-4o在非推理模型中表现最佳，推理模型在几何任务上有改进。

Conclusion: 多模态基础模型在视觉任务上表现尚可，但仍有改进空间，尤其是在几何任务和减少提示敏感性方面。

Abstract: Multimodal foundation models, such as GPT-4o, have recently made remarkable progress, but it is not clear where exactly these models stand in terms of understanding vision. In this paper, we benchmark the performance of popular multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0 Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision tasks (semantic segmentation, object detection, image classification, depth and surface normal prediction) using established datasets (e.g., COCO, ImageNet and its variants, etc).   The main challenges to performing this are: 1) most models are trained to output text and cannot natively express versatile domains, such as segments or 3D geometry, and 2) many leading models are proprietary and accessible only at an API level, i.e., there is no weight access to adapt them. We address these challenges by translating standard vision tasks into equivalent text-promptable and API-compatible tasks via prompt chaining to create a standardized benchmarking framework.   We observe that 1) the models are not close to the state-of-the-art specialist models at any task. However, 2) they are respectable generalists; this is remarkable as they are presumably trained on primarily image-text-based tasks. 3) They perform semantic tasks notably better than geometric ones. 4) While the prompt-chaining techniques affect performance, better models exhibit less sensitivity to prompt variations. 5) GPT-4o performs the best among non-reasoning models, securing the top position in 4 out of 6 tasks, 6) reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a preliminary analysis of models with native image generation, like the latest GPT-4o, shows they exhibit quirks like hallucinations and spatial misalignments.

</details>


### [28] [Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation](https://arxiv.org/abs/2507.01957)
*Zhuoyang Zhang,Luke J. Huang,Chengyue Wu,Shang Yang,Kelly Peng,Yao Lu,Song Han*

Main category: cs.CV

TL;DR: Locality-aware Parallel Decoding (LPD) 通过灵活并行自回归建模和局部感知生成顺序，显著加速自回归图像生成，减少生成步骤并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 传统自回归图像生成依赖逐块预测，导致高延迟。现有方法尝试多块预测并行化，但效果有限。

Method: 引入灵活并行自回归建模（支持任意生成顺序和并行度）和局部感知生成顺序（最小化组内依赖，最大化上下文支持）。

Result: 在 ImageNet 类条件生成中，生成步骤从 256 减少到 20（256×256 分辨率），1024 减少到 48（512×512 分辨率），延迟降低至少 3.4 倍。

Conclusion: LPD 在保持生成质量的同时，显著提升了并行化和生成效率。

Abstract: We present Locality-aware Parallel Decoding (LPD) to accelerate autoregressive image generation. Traditional autoregressive image generation relies on next-patch prediction, a memory-bound process that leads to high latency. Existing works have tried to parallelize next-patch prediction by shifting to multi-patch prediction to accelerate the process, but only achieved limited parallelization. To achieve high parallelization while maintaining generation quality, we introduce two key techniques: (1) Flexible Parallelized Autoregressive Modeling, a novel architecture that enables arbitrary generation ordering and degrees of parallelization. It uses learnable position query tokens to guide generation at target positions while ensuring mutual visibility among concurrently generated tokens for consistent parallel decoding. (2) Locality-aware Generation Ordering, a novel schedule that forms groups to minimize intra-group dependencies and maximize contextual support, enhancing generation quality. With these designs, we reduce the generation steps from 256 to 20 (256$\times$256 res.) and 1024 to 48 (512$\times$512 res.) without compromising quality on the ImageNet class-conditional generation, and achieving at least 3.4$\times$ lower latency than previous parallelized autoregressive models.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [29] [Prompt Mechanisms in Medical Imaging: A Comprehensive Survey](https://arxiv.org/abs/2507.01055)
*Hao Yang,Xinlong Liang,Zhang Li,Yue Sun,Zheyu Hu,Xinghe Xie,Behdad Dashtbozorg,Jincheng Huang,Shiwei Zhu,Luyi Han,Jiong Zhang,Shanshan Wang,Ritse Mann,Qifeng Yu,Tao Tan*

Main category: eess.IV

TL;DR: 本文综述了提示工程在医学影像中的应用，探讨了其如何通过多模态提示提升深度学习模型的性能、适应性和可解释性，同时指出了优化设计和临床部署的挑战。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学影像中潜力巨大，但面临数据稀缺、分布偏移和任务泛化等挑战。提示方法为模型提供了灵活、领域特定的指导，有望解决这些问题。

Method: 系统分析了文本指令、视觉提示和可学习嵌入等多模态提示方法，及其在图像生成、分割和分类等任务中的应用。

Result: 提示机制显著提升了任务准确性、鲁棒性和数据效率，减少了手动特征工程需求，并增强了模型可解释性。

Conclusion: 尽管进展显著，提示设计优化、数据异质性和临床部署的扩展性仍是挑战。未来方向包括多模态提示和临床整合，以推动医学诊断和个性化治疗的革命。

Abstract: Deep learning offers transformative potential in medical imaging, yet its clinical adoption is frequently hampered by challenges such as data scarcity, distribution shifts, and the need for robust task generalization. Prompt-based methodologies have emerged as a pivotal strategy to guide deep learning models, providing flexible, domain-specific adaptations that significantly enhance model performance and adaptability without extensive retraining. This systematic review critically examines the burgeoning landscape of prompt engineering in medical imaging. We dissect diverse prompt modalities, including textual instructions, visual prompts, and learnable embeddings, and analyze their integration for core tasks such as image generation, segmentation, and classification. Our synthesis reveals how these mechanisms improve task-specific outcomes by enhancing accuracy, robustness, and data efficiency and reducing reliance on manual feature engineering while fostering greater model interpretability by making the model's guidance explicit. Despite substantial advancements, we identify persistent challenges, particularly in prompt design optimization, data heterogeneity, and ensuring scalability for clinical deployment. Finally, this review outlines promising future trajectories, including advanced multimodal prompting and robust clinical integration, underscoring the critical role of prompt-driven AI in accelerating the revolution of diagnostics and personalized treatment planning in medicine.

</details>


### [30] [BronchoGAN: Anatomically consistent and domain-agnostic image-to-image translation for video bronchoscopy](https://arxiv.org/abs/2507.01387)
*Ahmad Soliman,Ron Keuth,Marian Himstedt*

Main category: eess.IV

TL;DR: BronchoGAN利用解剖学约束和中间深度图像表示，实现跨域支气管镜图像合成，提升生成图像的解剖学准确性和数据集规模。


<details>
  <summary>Details</summary>
Motivation: 支气管镜图像数据有限，跨域图像转换对临床训练深度学习模型至关重要。

Method: 结合条件GAN和解剖学约束（支气管孔匹配），利用基础模型生成深度图像作为中间表示，减少对单一数据集的依赖。

Result: 实验显示，BronchoGAN能成功将虚拟支气管镜等图像转换为逼真的人体气道图像，解剖学结构（如支气管孔）保留良好，FID、SSIM和Dice系数显著提升。

Conclusion: BronchoGAN通过解剖学约束和深度表示，填补了公共支气管镜图像数据的空白，为大规模数据集生成提供了解决方案。

Abstract: The limited availability of bronchoscopy images makes image synthesis particularly interesting for training deep learning models. Robust image translation across different domains -- virtual bronchoscopy, phantom as well as in-vivo and ex-vivo image data -- is pivotal for clinical applications. This paper proposes BronchoGAN introducing anatomical constraints for image-to-image translation being integrated into a conditional GAN. In particular, we force bronchial orifices to match across input and output images. We further propose to use foundation model-generated depth images as intermediate representation ensuring robustness across a variety of input domains establishing models with substantially less reliance on individual training datasets. Moreover our intermediate depth image representation allows to easily construct paired image data for training. Our experiments showed that input images from different domains (e.g. virtual bronchoscopy, phantoms) can be successfully translated to images mimicking realistic human airway appearance. We demonstrated that anatomical settings (i.e. bronchial orifices) can be robustly preserved with our approach which is shown qualitatively and quantitatively by means of improved FID, SSIM and dice coefficients scores. Our anatomical constraints enabled an improvement in the Dice coefficient of up to 0.43 for synthetic images. Through foundation models for intermediate depth representations, bronchial orifice segmentation integrated as anatomical constraints into conditional GANs we are able to robustly translate images from different bronchoscopy input domains. BronchoGAN allows to incorporate public CT scan data (virtual bronchoscopy) in order to generate large-scale bronchoscopy image datasets with realistic appearance. BronchoGAN enables to bridge the gap of missing public bronchoscopy images.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [31] [Empowering Manufacturers with Privacy-Preserving AI Tools: A Case Study in Privacy-Preserving Machine Learning to Solve Real-World Problems](https://arxiv.org/abs/2507.01808)
*Xiaoyu Ji,Jessica Shorland,Joshua Shank,Pascal Delpe-Brice,Latanya Sweeney,Jan Allebach,Ali Shakouri*

Main category: cs.CR

TL;DR: 论文提出了一种隐私保护平台，帮助中小型制造商安全共享数据，研究者开发工具解决实际问题，并通过平台回传工具供他人使用。以食品晶体质量控制为例，开发了自动分析工具。


<details>
  <summary>Details</summary>
Motivation: 中小型制造商因竞争和隐私问题不愿共享数据，但需要创新工具解决实际问题。

Method: 提出隐私保护平台，安全共享数据；开发自动晶体分析工具，结合机器学习模型。

Result: 开发了快速准确的晶体分析工具，并通过平台部署到工厂。

Conclusion: 隐私保护平台和工具成功解决了制造商的实际问题，未来可进一步扩展。

Abstract: Small- and medium-sized manufacturers need innovative data tools but, because of competition and privacy concerns, often do not want to share their proprietary data with researchers who might be interested in helping. This paper introduces a privacy-preserving platform by which manufacturers may safely share their data with researchers through secure methods, so that those researchers then create innovative tools to solve the manufacturers' real-world problems, and then provide tools that execute solutions back onto the platform for others to use with privacy and confidentiality guarantees. We illustrate this problem through a particular use case which addresses an important problem in the large-scale manufacturing of food crystals, which is that quality control relies on image analysis tools. Previous to our research, food crystals in the images were manually counted, which required substantial and time-consuming human efforts, but we have developed and deployed a crystal analysis tool which makes this process both more rapid and accurate. The tool enables automatic characterization of the crystal size distribution and numbers from microscope images while the natural imperfections from the sample preparation are automatically removed; a machine learning model to count high resolution translucent crystals and agglomeration of crystals was also developed to aid in these efforts. The resulting algorithm was then packaged for real-world use on the factory floor via a web-based app secured through the originating privacy-preserving platform, allowing manufacturers to use it while keeping their proprietary data secure. After demonstrating this full process, future directions are also explored.

</details>
