{"id": "2507.13388", "pdf": "https://arxiv.org/pdf/2507.13388", "abs": "https://arxiv.org/abs/2507.13388", "authors": ["Zhen-Qi Chen", "Yuan-Fu Yang"], "title": "DLSF: Dual-Layer Synergistic Fusion for High-Fidelity Image Syn-thesis", "categories": ["cs.GR"], "comment": null, "summary": "With the rapid advancement of diffusion-based generative models, Stable Diffusion (SD) has emerged as a state-of-the-art framework for high-fidelity im-age synthesis. However, existing SD models suffer from suboptimal feature aggregation, leading to in-complete semantic alignment and loss of fine-grained details, especially in highly textured and complex scenes. To address these limitations, we propose a novel dual-latent integration framework that en-hances feature interactions between the base latent and refined latent representations. Our approach em-ploys a feature concatenation strategy followed by an adaptive fusion module, which can be instantiated as either (i) an Adaptive Global Fusion (AGF) for hier-archical feature harmonization, or (ii) a Dynamic Spatial Fusion (DSF) for spatially-aware refinement. This design enables more effective cross-latent com-munication, preserving both global coherence and local texture fidelity. Our GitHub page: https://anonymous.4open.science/r/MVA2025-22 .", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u6f5c\u5728\u96c6\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5f81\u62fc\u63a5\u548c\u81ea\u9002\u5e94\u878d\u5408\u6a21\u5757\u6539\u8fdbStable Diffusion\u6a21\u578b\u7684\u7279\u5f81\u805a\u5408\u95ee\u9898\u3002", "motivation": "\u73b0\u6709SD\u6a21\u578b\u5728\u7279\u5f81\u805a\u5408\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u8bed\u4e49\u5bf9\u9f50\u4e0d\u5b8c\u6574\u548c\u7ec6\u8282\u4e22\u5931\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u573a\u666f\u4e2d\u3002", "method": "\u91c7\u7528\u53cc\u6f5c\u5728\u96c6\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u7279\u5f81\u62fc\u63a5\u548c\u81ea\u9002\u5e94\u878d\u5408\u6a21\u5757\uff08AGF\u6216DSF\uff09\uff0c\u4ee5\u589e\u5f3a\u6f5c\u5728\u8868\u793a\u95f4\u7684\u7279\u5f81\u4ea4\u4e92\u3002", "result": "\u65b0\u6846\u67b6\u63d0\u5347\u4e86\u5168\u5c40\u4e00\u81f4\u6027\u548c\u5c40\u90e8\u7eb9\u7406\u4fdd\u771f\u5ea6\uff0c\u6539\u8fdb\u4e86\u590d\u6742\u573a\u666f\u4e0b\u7684\u751f\u6210\u6548\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86SD\u6a21\u578b\u7684\u7279\u5f81\u805a\u5408\u95ee\u9898\uff0c\u4e3a\u9ad8\u4fdd\u771f\u56fe\u50cf\u5408\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.13586", "pdf": "https://arxiv.org/pdf/2507.13586", "abs": "https://arxiv.org/abs/2507.13586", "authors": ["Kaiyuan Tang", "Kuangshi Ai", "Jun Han", "Chaoli Wang"], "title": "TexGS-VolVis: Expressive Scene Editing for Volume Visualization via Textured Gaussian Splatting", "categories": ["cs.GR", "cs.CL", "cs.CV"], "comment": "Accepted by IEEE VIS 2025", "summary": "Advancements in volume visualization (VolVis) focus on extracting insights from 3D volumetric data by generating visually compelling renderings that reveal complex internal structures. Existing VolVis approaches have explored non-photorealistic rendering techniques to enhance the clarity, expressiveness, and informativeness of visual communication. While effective, these methods often rely on complex predefined rules and are limited to transferring a single style, restricting their flexibility. To overcome these limitations, we advocate the representation of VolVis scenes using differentiable Gaussian primitives combined with pretrained large models to enable arbitrary style transfer and real-time rendering. However, conventional 3D Gaussian primitives tightly couple geometry and appearance, leading to suboptimal stylization results. To address this, we introduce TexGS-VolVis, a textured Gaussian splatting framework for VolVis. TexGS-VolVis employs 2D Gaussian primitives, extending each Gaussian with additional texture and shading attributes, resulting in higher-quality, geometry-consistent stylization and enhanced lighting control during inference. Despite these improvements, achieving flexible and controllable scene editing remains challenging. To further enhance stylization, we develop image- and text-driven non-photorealistic scene editing tailored for TexGS-VolVis and 2D-lift-3D segmentation to enable partial editing with fine-grained control. We evaluate TexGS-VolVis both qualitatively and quantitatively across various volume rendering scenes, demonstrating its superiority over existing methods in terms of efficiency, visual quality, and editing flexibility.", "AI": {"tldr": "TexGS-VolVis\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7eb9\u7406\u9ad8\u65af\u6e85\u5c04\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u54082D\u9ad8\u65af\u57fa\u5143\u548c\u9884\u8bad\u7ec3\u5927\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u51e0\u4f55\u4e00\u81f4\u7684\u98ce\u683c\u5316\u548c\u5b9e\u65f6\u6e32\u67d3\u3002", "motivation": "\u73b0\u6709\u4f53\u79ef\u53ef\u89c6\u5316\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u9884\u5b9a\u4e49\u89c4\u5219\u4e14\u98ce\u683c\u5355\u4e00\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u8868\u73b0\u529b\u3002", "method": "\u91c7\u7528\u7eb9\u7406\u9ad8\u65af\u6e85\u5c04\u6846\u67b6\uff08TexGS-VolVis\uff09\uff0c\u6269\u5c55\u9ad8\u65af\u57fa\u5143\u4ee5\u5305\u542b\u7eb9\u7406\u548c\u5149\u7167\u5c5e\u6027\uff0c\u5e76\u7ed3\u5408\u56fe\u50cf\u548c\u6587\u672c\u9a71\u52a8\u7f16\u8f91\u3002", "result": "\u5728\u591a\u79cd\u4f53\u79ef\u6e32\u67d3\u573a\u666f\u4e2d\uff0cTexGS-VolVis\u5728\u6548\u7387\u3001\u89c6\u89c9\u8d28\u91cf\u548c\u7f16\u8f91\u7075\u6d3b\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TexGS-VolVis\u4e3a\u4f53\u79ef\u53ef\u89c6\u5316\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u5b9e\u65f6\u6e32\u67d3\u548c\u7cbe\u7ec6\u7f16\u8f91\u3002"}}
{"id": "2507.13360", "pdf": "https://arxiv.org/pdf/2507.13360", "abs": "https://arxiv.org/abs/2507.13360", "authors": ["Le-Anh Tran", "Chung Nguyen Tran", "Ngoc-Luu Nguyen", "Nhan Cach Dang", "Jordi Carrabina", "David Castells-Rufas", "Minh Son Nguyen"], "title": "Low-Light Enhancement via Encoder-Decoder Network with Illumination Guidance", "categories": ["cs.CV"], "comment": "6 pages, 3 figures, ICCCE 2025", "summary": "This paper introduces a novel deep learning framework for low-light image enhancement, named the Encoder-Decoder Network with Illumination Guidance (EDNIG). Building upon the U-Net architecture, EDNIG integrates an illumination map, derived from Bright Channel Prior (BCP), as a guidance input. This illumination guidance helps the network focus on underexposed regions, effectively steering the enhancement process. To further improve the model's representational power, a Spatial Pyramid Pooling (SPP) module is incorporated to extract multi-scale contextual features, enabling better handling of diverse lighting conditions. Additionally, the Swish activation function is employed to ensure smoother gradient propagation during training. EDNIG is optimized within a Generative Adversarial Network (GAN) framework using a composite loss function that combines adversarial loss, pixel-wise mean squared error (MSE), and perceptual loss. Experimental results show that EDNIG achieves competitive performance compared to state-of-the-art methods in quantitative metrics and visual quality, while maintaining lower model complexity, demonstrating its suitability for real-world applications. The source code for this work is available at https://github.com/tranleanh/ednig.", "AI": {"tldr": "EDNIG\u662f\u4e00\u79cd\u57fa\u4e8eU-Net\u67b6\u6784\u7684\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4eae\u5ea6\u56fe\u5f15\u5bfc\u548cSPP\u6a21\u5757\u63d0\u5347\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u6548\u679c\uff0c\u7ed3\u5408GAN\u6846\u67b6\u4f18\u5316\uff0c\u6027\u80fd\u4f18\u8d8a\u4e14\u6a21\u578b\u590d\u6742\u5ea6\u4f4e\u3002", "motivation": "\u89e3\u51b3\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u4eae\u5ea6\u5f15\u5bfc\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u3002", "method": "\u57fa\u4e8eU-Net\u67b6\u6784\uff0c\u5f15\u5165\u4eae\u5ea6\u56fe\uff08BCP\uff09\u548cSPP\u6a21\u5757\uff0c\u4f7f\u7528Swish\u6fc0\u6d3b\u51fd\u6570\uff0c\u5e76\u5728GAN\u6846\u67b6\u4e0b\u4f18\u5316\u3002", "result": "\u5728\u5b9a\u91cf\u6307\u6807\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6a21\u578b\u590d\u6742\u5ea6\u8f83\u4f4e\u3002", "conclusion": "EDNIG\u9002\u5408\u5b9e\u9645\u5e94\u7528\uff0c\u6027\u80fd\u4f18\u8d8a\u4e14\u9ad8\u6548\u3002"}}
{"id": "2507.13371", "pdf": "https://arxiv.org/pdf/2507.13371", "abs": "https://arxiv.org/abs/2507.13371", "authors": ["Yeming Cai", "Yang Wang", "Zhenglin Li"], "title": "Transformer-Based Framework for Motion Capture Denoising and Anomaly Detection in Medical Rehabilitation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper proposes an end-to-end deep learning framework integrating optical motion capture with a Transformer-based model to enhance medical rehabilitation. It tackles data noise and missing data caused by occlusion and environmental factors, while detecting abnormal movements in real time to ensure patient safety. Utilizing temporal sequence modeling, our framework denoises and completes motion capture data, improving robustness. Evaluations on stroke and orthopedic rehabilitation datasets show superior performance in data reconstruction and anomaly detection, providing a scalable, cost-effective solution for remote rehabilitation with reduced on-site supervision.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u5149\u5b66\u52a8\u4f5c\u6355\u6349\u4e0eTransformer\u6a21\u578b\uff0c\u7528\u4e8e\u533b\u7597\u5eb7\u590d\uff0c\u89e3\u51b3\u6570\u636e\u566a\u58f0\u548c\u7f3a\u5931\u95ee\u9898\uff0c\u5e76\u5b9e\u65f6\u68c0\u6d4b\u5f02\u5e38\u52a8\u4f5c\u3002", "motivation": "\u533b\u7597\u5eb7\u590d\u4e2d\uff0c\u5149\u5b66\u52a8\u4f5c\u6355\u6349\u5e38\u56e0\u906e\u6321\u548c\u73af\u5883\u56e0\u7d20\u5bfc\u81f4\u6570\u636e\u566a\u58f0\u548c\u7f3a\u5931\uff0c\u5f71\u54cd\u5eb7\u590d\u6548\u679c\u548c\u60a3\u8005\u5b89\u5168\u3002", "method": "\u91c7\u7528Transformer\u6a21\u578b\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\uff0c\u5bf9\u52a8\u4f5c\u6355\u6349\u6570\u636e\u53bb\u566a\u548c\u8865\u5168\uff0c\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5728\u5352\u4e2d\u548c\u9aa8\u79d1\u5eb7\u590d\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6570\u636e\u91cd\u5efa\u548c\u5f02\u5e38\u68c0\u6d4b\u6548\u679c\u663e\u8457\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8fdc\u7a0b\u5eb7\u590d\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u73b0\u573a\u76d1\u7763\u9700\u6c42\u3002"}}
{"id": "2507.13374", "pdf": "https://arxiv.org/pdf/2507.13374", "abs": "https://arxiv.org/abs/2507.13374", "authors": ["Kevin Dela Rosa"], "title": "Smart Routing for Multimodal Video Retrieval: When to Search What", "categories": ["cs.CV", "cs.AI", "cs.IR"], "comment": "Accepted to ICCV 2025 Multimodal Representation and Retrieval   Workshop", "summary": "We introduce ModaRoute, an LLM-based intelligent routing system that dynamically selects optimal modalities for multimodal video retrieval. While dense text captions can achieve 75.9% Recall@5, they require expensive offline processing and miss critical visual information present in 34% of clips with scene text not captured by ASR. By analyzing query intent and predicting information needs, ModaRoute reduces computational overhead by 41% while achieving 60.9% Recall@5. Our approach uses GPT-4.1 to route queries across ASR (speech), OCR (text), and visual indices, averaging 1.78 modalities per query versus exhaustive 3.0 modality search. Evaluation on 1.8M video clips demonstrates that intelligent routing provides a practical solution for scaling multimodal retrieval systems, reducing infrastructure costs while maintaining competitive effectiveness for real-world deployment.", "AI": {"tldr": "ModaRoute\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u6a21\u6001\u89c6\u9891\u68c0\u7d22\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6700\u4f18\u6a21\u6001\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u7684\u68c0\u7d22\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u5bc6\u96c6\u6587\u672c\u63cf\u8ff0\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u53ef\u80fd\u9057\u6f0f\u5173\u952e\u89c6\u89c9\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528GPT-4.1\u5206\u6790\u67e5\u8be2\u610f\u56fe\u5e76\u9884\u6d4b\u4fe1\u606f\u9700\u6c42\uff0c\u52a8\u6001\u9009\u62e9ASR\u3001OCR\u548c\u89c6\u89c9\u7d22\u5f15\u4e2d\u7684\u6700\u4f18\u6a21\u6001\u3002", "result": "\u5728180\u4e07\u89c6\u9891\u7247\u6bb5\u4e0a\u6d4b\u8bd5\uff0c\u8ba1\u7b97\u5f00\u9500\u51cf\u5c1141%\uff0cRecall@5\u8fbe\u523060.9%\uff0c\u5e73\u5747\u6bcf\u67e5\u8be2\u4f7f\u75281.78\u79cd\u6a21\u6001\u3002", "conclusion": "\u667a\u80fd\u8def\u7531\u4e3a\u591a\u6a21\u6001\u68c0\u7d22\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u964d\u4f4e\u4e86\u6210\u672c\u5e76\u4fdd\u6301\u4e86\u7ade\u4e89\u529b\u3002"}}
{"id": "2507.13401", "pdf": "https://arxiv.org/pdf/2507.13401", "abs": "https://arxiv.org/abs/2507.13401", "authors": ["Shreya Kadambi", "Risheek Garrepalli", "Shubhankar Borse", "Munawar Hyatt", "Fatih Porikli"], "title": "MADI: Masking-Augmented Diffusion with Inference-Time Scaling for Visual Editing", "categories": ["cs.CV", "cs.LG"], "comment": "26 pages", "summary": "Despite the remarkable success of diffusion models in text-to-image generation, their effectiveness in grounded visual editing and compositional control remains challenging. Motivated by advances in self-supervised learning and in-context generative modeling, we propose a series of simple yet powerful design choices that significantly enhance diffusion model capacity for structured, controllable generation and editing. We introduce Masking-Augmented Diffusion with Inference-Time Scaling (MADI), a framework that improves the editability, compositionality and controllability of diffusion models through two core innovations. First, we introduce Masking-Augmented gaussian Diffusion (MAgD), a novel training strategy with dual corruption process which combines standard denoising score matching and masked reconstruction by masking noisy input from forward process. MAgD encourages the model to learn discriminative and compositional visual representations, thus enabling localized and structure-aware editing. Second, we introduce an inference-time capacity scaling mechanism based on Pause Tokens, which act as special placeholders inserted into the prompt for increasing computational capacity at inference time. Our findings show that adopting expressive and dense prompts during training further enhances performance, particularly for MAgD. Together, these contributions in MADI substantially enhance the editability of diffusion models, paving the way toward their integration into more general-purpose, in-context generative diffusion architectures.", "AI": {"tldr": "MADI\u6846\u67b6\u901a\u8fc7Masking-Augmented gaussian Diffusion\u548cPause Tokens\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u53ef\u7f16\u8f91\u6027\u548c\u53ef\u63a7\u6027\u3002", "motivation": "\u5c3d\u7ba1\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7ed3\u6784\u5316\u7f16\u8f91\u548c\u7ec4\u5408\u63a7\u5236\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51faMADI\u6846\u67b6\uff0c\u5305\u62ecMAgD\u8bad\u7ec3\u7b56\u7565\uff08\u7ed3\u5408\u53bb\u566a\u548c\u63a9\u7801\u91cd\u5efa\uff09\u548cPause Tokens\u673a\u5236\uff08\u63a8\u7406\u65f6\u589e\u52a0\u8ba1\u7b97\u80fd\u529b\uff09\u3002", "result": "MADI\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u7f16\u8f91\u6027\u548c\u7ec4\u5408\u6027\uff0c\u652f\u6301\u5c40\u90e8\u5316\u548c\u7ed3\u6784\u611f\u77e5\u7684\u7f16\u8f91\u3002", "conclusion": "MADI\u4e3a\u6269\u6563\u6a21\u578b\u5728\u901a\u7528\u4e0a\u4e0b\u6587\u751f\u6210\u67b6\u6784\u4e2d\u7684\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.13404", "pdf": "https://arxiv.org/pdf/2507.13404", "abs": "https://arxiv.org/abs/2507.13404", "authors": ["Delin An", "Pan Du", "Jian-Xun Wang", "Chaoli Wang"], "title": "AortaDiff: Volume-Guided Conditional Diffusion Models for Multi-Branch Aortic Surface Generation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate 3D aortic construction is crucial for clinical diagnosis, preoperative planning, and computational fluid dynamics (CFD) simulations, as it enables the estimation of critical hemodynamic parameters such as blood flow velocity, pressure distribution, and wall shear stress. Existing construction methods often rely on large annotated training datasets and extensive manual intervention. While the resulting meshes can serve for visualization purposes, they struggle to produce geometrically consistent, well-constructed surfaces suitable for downstream CFD analysis. To address these challenges, we introduce AortaDiff, a diffusion-based framework that generates smooth aortic surfaces directly from CT/MRI volumes. AortaDiff first employs a volume-guided conditional diffusion model (CDM) to iteratively generate aortic centerlines conditioned on volumetric medical images. Each centerline point is then automatically used as a prompt to extract the corresponding vessel contour, ensuring accurate boundary delineation. Finally, the extracted contours are fitted into a smooth 3D surface, yielding a continuous, CFD-compatible mesh representation. AortaDiff offers distinct advantages over existing methods, including an end-to-end workflow, minimal dependency on large labeled datasets, and the ability to generate CFD-compatible aorta meshes with high geometric fidelity. Experimental results demonstrate that AortaDiff performs effectively even with limited training data, successfully constructing both normal and pathologically altered aorta meshes, including cases with aneurysms or coarctation. This capability enables the generation of high-quality visualizations and positions AortaDiff as a practical solution for cardiovascular research.", "AI": {"tldr": "AortaDiff\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u6846\u67b6\uff0c\u76f4\u63a5\u4eceCT/MRI\u4f53\u79ef\u751f\u6210\u5e73\u6ed1\u7684\u4e3b\u52a8\u8109\u8868\u9762\uff0c\u9002\u7528\u4e8eCFD\u5206\u6790\uff0c\u51cf\u5c11\u5bf9\u5927\u578b\u6807\u6ce8\u6570\u636e\u96c6\u7684\u4f9d\u8d56\u3002", "motivation": "\u51c6\u786e\u76843D\u4e3b\u52a8\u8109\u6784\u5efa\u5bf9\u4e34\u5e8a\u8bca\u65ad\u548cCFD\u6a21\u62df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u624b\u52a8\u5e72\u9884\uff0c\u96be\u4ee5\u751f\u6210\u51e0\u4f55\u4e00\u81f4\u7684\u8868\u9762\u3002", "method": "AortaDiff\u901a\u8fc7\u4f53\u79ef\u5f15\u5bfc\u7684\u6761\u4ef6\u6269\u6563\u6a21\u578b\u751f\u6210\u4e3b\u52a8\u8109\u4e2d\u5fc3\u7ebf\uff0c\u5e76\u81ea\u52a8\u63d0\u53d6\u8840\u7ba1\u8f6e\u5ed3\uff0c\u6700\u7ec8\u62df\u5408\u4e3a\u5e73\u6ed1\u76843D\u8868\u9762\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAortaDiff\u5728\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\u4ecd\u80fd\u6709\u6548\u6784\u5efa\u6b63\u5e38\u548c\u75c5\u7406\u4e3b\u52a8\u8109\u7f51\u683c\uff0c\u5305\u62ec\u52a8\u8109\u7624\u6216\u72ed\u7a84\u75c5\u4f8b\u3002", "conclusion": "AortaDiff\u63d0\u4f9b\u7aef\u5230\u7aef\u5de5\u4f5c\u6d41\u7a0b\uff0c\u751f\u6210\u9ad8\u51e0\u4f55\u4fdd\u771f\u5ea6\u7684CFD\u517c\u5bb9\u7f51\u683c\uff0c\u662f\u5fc3\u8840\u7ba1\u7814\u7a76\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13428", "pdf": "https://arxiv.org/pdf/2507.13428", "abs": "https://arxiv.org/abs/2507.13428", "authors": ["Jing Gu", "Xian Liu", "Yu Zeng", "Ashwin Nagarajan", "Fangrui Zhu", "Daniel Hong", "Yue Fan", "Qianqi Yan", "Kaiwen Zhou", "Ming-Yu Liu", "Xin Eric Wang"], "title": "\"PhyWorldBench\": A Comprehensive Evaluation of Physical Realism in Text-to-Video Models", "categories": ["cs.CV", "cs.AI"], "comment": "31 pages, 21 figures", "summary": "Video generation models have achieved remarkable progress in creating high-quality, photorealistic content. However, their ability to accurately simulate physical phenomena remains a critical and unresolved challenge. This paper presents PhyWorldBench, a comprehensive benchmark designed to evaluate video generation models based on their adherence to the laws of physics. The benchmark covers multiple levels of physical phenomena, ranging from fundamental principles like object motion and energy conservation to more complex scenarios involving rigid body interactions and human or animal motion. Additionally, we introduce a novel \"\"Anti-Physics\"\" category, where prompts intentionally violate real-world physics, enabling the assessment of whether models can follow such instructions while maintaining logical consistency. Besides large-scale human evaluation, we also design a simple yet effective method that could utilize current MLLM to evaluate the physics realism in a zero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation models, including five open-source and five proprietary models, with a detailed comparison and analysis. we identify pivotal challenges models face in adhering to real-world physics. Through systematic testing of their outputs across 1,050 curated prompts-spanning fundamental, composite, and anti-physics scenarios-we identify pivotal challenges these models face in adhering to real-world physics. We then rigorously examine their performance on diverse physical phenomena with varying prompt types, deriving targeted recommendations for crafting prompts that enhance fidelity to physical principles.", "AI": {"tldr": "PhyWorldBench\u662f\u4e00\u4e2a\u8bc4\u4f30\u89c6\u9891\u751f\u6210\u6a21\u578b\u7269\u7406\u6a21\u62df\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u6db5\u76d6\u4ece\u57fa\u7840\u7269\u7406\u73b0\u8c61\u5230\u590d\u6742\u573a\u666f\uff0c\u5e76\u5f15\u5165\u201c\u53cd\u7269\u7406\u201d\u7c7b\u522b\u3002\u901a\u8fc7\u4eba\u7c7b\u8bc4\u4f30\u548cMLLM\u65b9\u6cd5\u6d4b\u8bd5\u4e8612\u4e2a\u6a21\u578b\uff0c\u53d1\u73b0\u5176\u5728\u7269\u7406\u4e00\u81f4\u6027\u4e0a\u7684\u6311\u6218\u3002", "motivation": "\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u7269\u7406\u73b0\u8c61\u6a21\u62df\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u7cfb\u7edf\u8bc4\u4f30\u5176\u7269\u7406\u4e00\u81f4\u6027\u3002", "method": "\u8bbe\u8ba1PhyWorldBench\u57fa\u51c6\uff0c\u5305\u542b\u591a\u7ea7\u7269\u7406\u73b0\u8c61\u548c\u201c\u53cd\u7269\u7406\u201d\u7c7b\u522b\uff0c\u7ed3\u5408\u4eba\u7c7b\u8bc4\u4f30\u548cMLLM\u65b9\u6cd5\u6d4b\u8bd5\u6a21\u578b\u3002", "result": "\u6d4b\u8bd512\u4e2a\u6a21\u578b\uff0c\u53d1\u73b0\u5176\u5728\u7269\u7406\u4e00\u81f4\u6027\u4e0a\u7684\u663e\u8457\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4f18\u5316\u63d0\u793a\u7684\u5efa\u8bae\u3002", "conclusion": "PhyWorldBench\u63ed\u793a\u4e86\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u7269\u7406\u6a21\u62df\u4e0a\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.13530", "pdf": "https://arxiv.org/pdf/2507.13530", "abs": "https://arxiv.org/abs/2507.13530", "authors": ["Lukas Baumg\u00e4rtner", "Ronny Bergmann", "Roland Herzog", "Stephan Schmidt", "Manuel Wei\u00df"], "title": "Total Generalized Variation of the Normal Vector Field and Applications to Mesh Denoising", "categories": ["cs.CV", "math.DG", "math.OC"], "comment": null, "summary": "We propose a novel formulation for the second-order total generalized variation (TGV) of the normal vector on an oriented, triangular mesh embedded in $\\mathbb{R}^3$. The normal vector is considered as a manifold-valued function, taking values on the unit sphere. Our formulation extends previous discrete TGV models for piecewise constant scalar data that utilize a Raviart-Thomas function space. To exctend this formulation to the manifold setting, a tailor-made tangential Raviart-Thomas type finite element space is constructed in this work. The new regularizer is compared to existing methods in mesh denoising experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e8c\u9636\u603b\u5e7f\u4e49\u53d8\u5206\uff08TGV\uff09\u516c\u5f0f\uff0c\u7528\u4e8e\u5904\u7406\u5d4c\u5165\u5728\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u7684\u4e09\u89d2\u7f51\u683c\u4e0a\u7684\u6cd5\u5411\u91cf\uff0c\u5e76\u5c06\u5176\u4e0e\u73b0\u6709\u65b9\u6cd5\u5728\u7f51\u683c\u53bb\u566a\u5b9e\u9a8c\u4e2d\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "motivation": "\u6269\u5c55\u79bb\u6563TGV\u6a21\u578b\u4ee5\u5904\u7406\u6d41\u5f62\u503c\u51fd\u6570\uff08\u5982\u5355\u4f4d\u7403\u9762\u4e0a\u7684\u6cd5\u5411\u91cf\uff09\uff0c\u5e76\u6784\u5efa\u9002\u5408\u7684\u6709\u9650\u5143\u7a7a\u95f4\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5b9a\u5236\u7684\u5207\u5411Raviart-Thomas\u578b\u6709\u9650\u5143\u7a7a\u95f4\uff0c\u7528\u4e8e\u6d41\u5f62\u8bbe\u7f6e\u4e0b\u7684TGV\u516c\u5f0f\u3002", "result": "\u65b0\u6b63\u5219\u5316\u5668\u5728\u7f51\u683c\u53bb\u566a\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6cd5\u5411\u91cf\u5904\u7406\u4e0a\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u7f51\u683c\u53bb\u566a\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.13546", "pdf": "https://arxiv.org/pdf/2507.13546", "abs": "https://arxiv.org/abs/2507.13546", "authors": ["Dmitrii Mikhailov", "Aleksey Letunovskiy", "Maria Kovaleva", "Vladimir Arkhipkin", "Vladimir Korviakov", "Vladimir Polovnikov", "Viacheslav Vasilev", "Evelina Sidorova", "Denis Dimitrov"], "title": "$\\nabla$NABLA: Neighborhood Adaptive Block-Level Attention", "categories": ["cs.CV"], "comment": null, "summary": "Recent progress in transformer-based architectures has demonstrated remarkable success in video generation tasks. However, the quadratic complexity of full attention mechanisms remains a critical bottleneck, particularly for high-resolution and long-duration video sequences. In this paper, we propose NABLA, a novel Neighborhood Adaptive Block-Level Attention mechanism that dynamically adapts to sparsity patterns in video diffusion transformers (DiTs). By leveraging block-wise attention with adaptive sparsity-driven threshold, NABLA reduces computational overhead while preserving generative quality. Our method does not require custom low-level operator design and can be seamlessly integrated with PyTorch's Flex Attention operator. Experiments demonstrate that NABLA achieves up to 2.7x faster training and inference compared to baseline almost without compromising quantitative metrics (CLIP score, VBench score, human evaluation score) and visual quality drop. The code and model weights are available here: https://github.com/gen-ai-team/Wan2.1-NABLA", "AI": {"tldr": "NABLA\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u90bb\u57df\u81ea\u9002\u5e94\u5757\u7ea7\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5168\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u662f\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u74f6\u9888\uff0c\u5c24\u5176\u662f\u9ad8\u5206\u8fa8\u7387\u548c\u957f\u89c6\u9891\u5e8f\u5217\u3002", "method": "\u91c7\u7528\u5757\u7ea7\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u7a00\u758f\u9608\u503c\uff0c\u52a8\u6001\u9002\u5e94\u89c6\u9891\u6269\u6563\u53d8\u6362\u5668\u4e2d\u7684\u7a00\u758f\u6a21\u5f0f\u3002", "result": "NABLA\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u6bd4\u57fa\u7ebf\u5feb2.7\u500d\uff0c\u4e14\u51e0\u4e4e\u4e0d\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u548c\u91cf\u5316\u6307\u6807\u3002", "conclusion": "NABLA\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u5b9a\u5236\u4f4e\u5c42\u64cd\u4f5c\u8bbe\u8ba1\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u9002\u7528\u4e8e\u89c6\u9891\u751f\u6210\u4efb\u52a1\u3002"}}
{"id": "2507.13568", "pdf": "https://arxiv.org/pdf/2507.13568", "abs": "https://arxiv.org/abs/2507.13568", "authors": ["Kaihong Wang", "Donghyun Kim", "Margrit Betke"], "title": "LoRA-Loop: Closing the Synthetic Replay Cycle for Continual VLM Learning", "categories": ["cs.CV"], "comment": null, "summary": "Continual learning for vision-language models has achieved remarkable performance through synthetic replay, where samples are generated using Stable Diffusion to regularize during finetuning and retain knowledge. However, real-world downstream applications often exhibit domain-specific nuances and fine-grained semantics not captured by generators, causing synthetic-replay methods to produce misaligned samples that misguide finetuning and undermine retention of prior knowledge. In this work, we propose a LoRA-enhanced synthetic-replay framework that injects task-specific low-rank adapters into a frozen Stable Diffusion model, efficiently capturing each new task's unique visual and semantic patterns. Specifically, we introduce a two-stage, confidence-based sample selection: we first rank real task data by post-finetuning VLM confidence to focus LoRA finetuning on the most representative examples, then generate synthetic samples and again select them by confidence for distillation. Our approach integrates seamlessly with existing replay pipelines-simply swap in the adapted generator to boost replay fidelity. Extensive experiments on the Multi-domain Task Incremental Learning (MTIL) benchmark show that our method outperforms previous synthetic-replay techniques, achieving an optimal balance among plasticity, stability, and zero-shot capability. These results demonstrate the effectiveness of generator adaptation via LoRA for robust continual learning in VLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLoRA\u589e\u5f3a\u7684\u5408\u6210\u91cd\u653e\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u7684\u4f4e\u79e9\u9002\u914d\u5668\u6539\u8fdbStable Diffusion\u6a21\u578b\uff0c\u63d0\u5347\u6301\u7eed\u5b66\u4e60\u4e2d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5408\u6210\u91cd\u653e\u65b9\u6cd5\u751f\u6210\u7684\u6837\u672c\u53ef\u80fd\u56e0\u672a\u6355\u6349\u9886\u57df\u7279\u5b9a\u7ec6\u8282\u800c\u8bef\u5bfc\u5fae\u8c03\uff0c\u5bfc\u81f4\u77e5\u8bc6\u9057\u5fd8\u3002", "method": "\u91c7\u7528LoRA\u589e\u5f3a\u7684\u5408\u6210\u91cd\u653e\u6846\u67b6\uff0c\u7ed3\u5408\u4e24\u9636\u6bb5\u7f6e\u4fe1\u5ea6\u6837\u672c\u9009\u62e9\uff0c\u4f18\u5316\u751f\u6210\u6837\u672c\u7684\u4ee3\u8868\u6027\u3002", "result": "\u5728MTIL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u8861\u4e86\u53ef\u5851\u6027\u3001\u7a33\u5b9a\u6027\u548c\u96f6\u6837\u672c\u80fd\u529b\u3002", "conclusion": "LoRA\u9002\u914d\u751f\u6210\u5668\u80fd\u6709\u6548\u63d0\u5347\u6301\u7eed\u5b66\u4e60\u4e2d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.13599", "pdf": "https://arxiv.org/pdf/2507.13599", "abs": "https://arxiv.org/abs/2507.13599", "authors": ["Chengxu Liu", "Lu Qi", "Jinshan Pan", "Xueming Qian", "Ming-Hsuan Yang"], "title": "Learning Deblurring Texture Prior from Unpaired Data with Diffusion Model", "categories": ["cs.CV"], "comment": "Accepted by ICCV2025", "summary": "Since acquiring large amounts of realistic blurry-sharp image pairs is difficult and expensive, learning blind image deblurring from unpaired data is a more practical and promising solution. Unfortunately, dominant approaches rely heavily on adversarial learning to bridge the gap from blurry domains to sharp domains, ignoring the complex and unpredictable nature of real-world blur patterns. In this paper, we propose a novel diffusion model (DM)-based framework, dubbed \\ours, for image deblurring by learning spatially varying texture prior from unpaired data. In particular, \\ours performs DM to generate the prior knowledge that aids in recovering the textures of blurry images. To implement this, we propose a Texture Prior Encoder (TPE) that introduces a memory mechanism to represent the image textures and provides supervision for DM training. To fully exploit the generated texture priors, we present the Texture Transfer Transformer layer (TTformer), in which a novel Filter-Modulated Multi-head Self-Attention (FM-MSA) efficiently removes spatially varying blurring through adaptive filtering. Furthermore, we implement a wavelet-based adversarial loss to preserve high-frequency texture details. Extensive evaluations show that \\ours provides a promising unsupervised deblurring solution and outperforms SOTA methods in widely-used benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff08\\ours\uff09\uff0c\u901a\u8fc7\u4ece\u975e\u914d\u5bf9\u6570\u636e\u4e2d\u5b66\u4e60\u7a7a\u95f4\u53d8\u5316\u7684\u7eb9\u7406\u5148\u9a8c\uff0c\u5b9e\u73b0\u56fe\u50cf\u53bb\u6a21\u7cca\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u7eb9\u7406\u5148\u9a8c\u7f16\u7801\u5668\uff08TPE\uff09\u548c\u7eb9\u7406\u8f6c\u79fb\u53d8\u6362\u5c42\uff08TTformer\uff09\u9ad8\u6548\u53bb\u9664\u7a7a\u95f4\u53d8\u5316\u7684\u6a21\u7cca\uff0c\u5e76\u901a\u8fc7\u5c0f\u6ce2\u5bf9\u6297\u635f\u5931\u4fdd\u7559\u9ad8\u9891\u7eb9\u7406\u7ec6\u8282\u3002", "motivation": "\u7531\u4e8e\u83b7\u53d6\u5927\u91cf\u771f\u5b9e\u7684\u6a21\u7cca-\u6e05\u6670\u56fe\u50cf\u5bf9\u56f0\u96be\u4e14\u6602\u8d35\uff0c\u4ece\u975e\u914d\u5bf9\u6570\u636e\u4e2d\u5b66\u4e60\u76f2\u56fe\u50cf\u53bb\u6a21\u7cca\u66f4\u5177\u5b9e\u7528\u6027\u548c\u524d\u666f\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5bf9\u6297\u5b66\u4e60\uff0c\u5ffd\u7565\u4e86\u771f\u5b9e\u4e16\u754c\u6a21\u7cca\u6a21\u5f0f\u7684\u590d\u6742\u6027\u548c\u4e0d\u53ef\u9884\u6d4b\u6027\u3002", "method": "\u63d0\u51fa\u6269\u6563\u6a21\u578b\u6846\u67b6\uff0c\u5305\u62ec\u7eb9\u7406\u5148\u9a8c\u7f16\u7801\u5668\uff08TPE\uff09\u548c\u7eb9\u7406\u8f6c\u79fb\u53d8\u6362\u5c42\uff08TTformer\uff09\uff0c\u5229\u7528FM-MSA\u81ea\u9002\u5e94\u6ee4\u6ce2\u53bb\u9664\u7a7a\u95f4\u53d8\u5316\u6a21\u7cca\uff0c\u5e76\u91c7\u7528\u5c0f\u6ce2\u5bf9\u6297\u635f\u5931\u4fdd\u7559\u9ad8\u9891\u7ec6\u8282\u3002", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\\ours \u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65e0\u76d1\u7763\u53bb\u6a21\u7cca\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u771f\u5b9e\u4e16\u754c\u7684\u590d\u6742\u6a21\u7cca\u6a21\u5f0f\u3002"}}
{"id": "2507.13607", "pdf": "https://arxiv.org/pdf/2507.13607", "abs": "https://arxiv.org/abs/2507.13607", "authors": ["Kento Kawai", "Takeru Oba", "Kyotaro Tokoro", "Kazutoshi Akita", "Norimichi Ukita"], "title": "Efficient Burst Super-Resolution with One-step Diffusion", "categories": ["cs.CV"], "comment": "NTIRE2025", "summary": "While burst Low-Resolution (LR) images are useful for improving their Super Resolution (SR) image compared to a single LR image, prior burst SR methods are trained in a deterministic manner, which produces a blurry SR image. Since such blurry images are perceptually degraded, we aim to reconstruct sharp and high-fidelity SR images by a diffusion model. Our method improves the efficiency of the diffusion model with a stochastic sampler with a high-order ODE as well as one-step diffusion using knowledge distillation. Our experimental results demonstrate that our method can reduce the runtime to 1.6 % of its baseline while maintaining the SR quality measured based on image distortion and perceptual quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7206\u53d1\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u901a\u8fc7\u968f\u673a\u91c7\u6837\u5668\u548c\u77e5\u8bc6\u84b8\u998f\u63d0\u9ad8\u6548\u7387\uff0c\u663e\u8457\u51cf\u5c11\u8fd0\u884c\u65f6\u95f4\u5e76\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7206\u53d1\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u751f\u6210\u7684\u56fe\u50cf\u6a21\u7cca\u4e14\u611f\u77e5\u8d28\u91cf\u5dee\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6269\u6563\u6a21\u578b\u91cd\u5efa\u6e05\u6670\u4e14\u9ad8\u4fdd\u771f\u7684\u8d85\u5206\u8fa8\u7387\u56fe\u50cf\u3002", "method": "\u4f7f\u7528\u968f\u673a\u91c7\u6837\u5668\u7ed3\u5408\u9ad8\u9636ODE\u548c\u77e5\u8bc6\u84b8\u998f\u7684\u4e00\u6b65\u6269\u6563\u65b9\u6cd5\uff0c\u63d0\u9ad8\u6269\u6563\u6a21\u578b\u7684\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06\u8fd0\u884c\u65f6\u95f4\u51cf\u5c11\u81f3\u57fa\u7ebf\u76841.6%\uff0c\u540c\u65f6\u4fdd\u6301\u57fa\u4e8e\u56fe\u50cf\u5931\u771f\u548c\u611f\u77e5\u8d28\u91cf\u7684\u8d85\u5206\u8fa8\u7387\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u4e3a\u7206\u53d1\u8d85\u5206\u8fa8\u7387\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13648", "pdf": "https://arxiv.org/pdf/2507.13648", "abs": "https://arxiv.org/abs/2507.13648", "authors": ["Seungjun Moon", "Sangjoon Yu", "Gyeong-Moon Park"], "title": "EPSilon: Efficient Point Sampling for Lightening of Hybrid-based 3D Avatar Generation", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement of neural radiance fields (NeRF) has paved the way to generate animatable human avatars from a monocular video. However, the sole usage of NeRF suffers from a lack of details, which results in the emergence of hybrid representation that utilizes SMPL-based mesh together with NeRF representation. While hybrid-based models show photo-realistic human avatar generation qualities, they suffer from extremely slow inference due to their deformation scheme: to be aligned with the mesh, hybrid-based models use the deformation based on SMPL skinning weights, which needs high computational costs on each sampled point. We observe that since most of the sampled points are located in empty space, they do not affect the generation quality but result in inference latency with deformation. In light of this observation, we propose EPSilon, a hybrid-based 3D avatar generation scheme with novel efficient point sampling strategies that boost both training and inference. In EPSilon, we propose two methods to omit empty points at rendering; empty ray omission (ERO) and empty interval omission (EIO). In ERO, we wipe out rays that progress through the empty space. Then, EIO narrows down the sampling interval on the ray, which wipes out the region not occupied by either clothes or mesh. The delicate sampling scheme of EPSilon enables not only great computational cost reduction during deformation but also the designation of the important regions to be sampled, which enables a single-stage NeRF structure without hierarchical sampling. Compared to existing methods, EPSilon maintains the generation quality while using only 3.9% of sampled points and achieves around 20 times faster inference, together with 4 times faster training convergence. We provide video results on https://github.com/seungjun-moon/epsilon.", "AI": {"tldr": "EPSilon\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6df7\u54083D\u5934\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a7a\u70b9\u91c7\u6837\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eNeRF\u548cSMPL\u7684\u6df7\u5408\u65b9\u6cd5\u56e0\u53d8\u5f62\u8ba1\u7b97\u6210\u672c\u9ad8\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u6162\uff0cEPSilon\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u91c7\u6837\u7b56\u7565\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u7a7a\u70b9\u91c7\u6837\u7b56\u7565\uff1a\u7a7a\u5c04\u7ebf\u5ffd\u7565\uff08ERO\uff09\u548c\u7a7a\u533a\u95f4\u5ffd\u7565\uff08EIO\uff09\uff0c\u4ee5\u51cf\u5c11\u65e0\u6548\u8ba1\u7b97\u3002", "result": "EPSilon\u4ec5\u97003.9%\u7684\u91c7\u6837\u70b9\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u7ea620\u500d\uff0c\u8bad\u7ec3\u6536\u655b\u901f\u5ea6\u63d0\u53474\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "EPSilon\u901a\u8fc7\u9ad8\u6548\u91c7\u6837\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u6df7\u54083D\u5934\u50cf\u751f\u6210\u7684\u6548\u7387\uff0c\u4e3a\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2507.13663", "pdf": "https://arxiv.org/pdf/2507.13663", "abs": "https://arxiv.org/abs/2507.13663", "authors": ["Xingyu Jiang", "Ning Gao", "Hongkun Dou", "Xiuhui Zhang", "Xiaoqing Zhong", "Yue Deng", "Hongjue Li"], "title": "Global Modeling Matters: A Fast, Lightweight and Effective Baseline for Efficient Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Natural image quality is often degraded by adverse weather conditions, significantly impairing the performance of downstream tasks. Image restoration has emerged as a core solution to this challenge and has been widely discussed in the literature. Although recent transformer-based approaches have made remarkable progress in image restoration, their increasing system complexity poses significant challenges for real-time processing, particularly in real-world deployment scenarios. To this end, most existing methods attempt to simplify the self-attention mechanism, such as by channel self-attention or state space model. However, these methods primarily focus on network architecture while neglecting the inherent characteristics of image restoration itself. In this context, we explore a pyramid Wavelet-Fourier iterative pipeline to demonstrate the potential of Wavelet-Fourier processing for image restoration. Inspired by the above findings, we propose a novel and efficient restoration baseline, named Pyramid Wavelet-Fourier Network (PW-FNet). Specifically, PW-FNet features two key design principles: 1) at the inter-block level, integrates a pyramid wavelet-based multi-input multi-output structure to achieve multi-scale and multi-frequency bands decomposition; and 2) at the intra-block level, incorporates Fourier transforms as an efficient alternative to self-attention mechanisms, effectively reducing computational complexity while preserving global modeling capability. Extensive experiments on tasks such as image deraining, raindrop removal, image super-resolution, motion deblurring, image dehazing, image desnowing and underwater/low-light enhancement demonstrate that PW-FNet not only surpasses state-of-the-art methods in restoration quality but also achieves superior efficiency, with significantly reduced parameter size, computational cost and inference time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91d1\u5b57\u5854\u5c0f\u6ce2-\u5085\u91cc\u53f6\u8fed\u4ee3\u7ba1\u9053\u7684\u9ad8\u6548\u56fe\u50cf\u6062\u590d\u57fa\u7ebfPW-FNet\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u591a\u9891\u5e26\u5206\u89e3\u548c\u5085\u91cc\u53f6\u53d8\u6362\u66ff\u4ee3\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u6062\u590d\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u81ea\u7136\u56fe\u50cf\u8d28\u91cf\u5e38\u56e0\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u964d\uff0c\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u867d\u6709\u6548\u4f46\u590d\u6742\u5ea6\u9ad8\uff0c\u96be\u4ee5\u5b9e\u65f6\u5904\u7406\u3002", "method": "PW-FNet\u91c7\u7528\u91d1\u5b57\u5854\u5c0f\u6ce2\u591a\u8f93\u5165\u591a\u8f93\u51fa\u7ed3\u6784\u5b9e\u73b0\u591a\u5c3a\u5ea6\u5206\u89e3\uff0c\u5e76\u5728\u5757\u5185\u7528\u5085\u91cc\u53f6\u53d8\u6362\u66ff\u4ee3\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u5728\u591a\u79cd\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\uff0cPW-FNet\u5728\u6062\u590d\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u5747\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u53c2\u6570\u89c4\u6a21\u548c\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "PW-FNet\u5c55\u793a\u4e86\u5c0f\u6ce2-\u5085\u91cc\u53f6\u5904\u7406\u5728\u56fe\u50cf\u6062\u590d\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u9ad8\u6548\u6062\u590d\u63d0\u4f9b\u4e86\u65b0\u57fa\u7ebf\u3002"}}
{"id": "2507.13693", "pdf": "https://arxiv.org/pdf/2507.13693", "abs": "https://arxiv.org/abs/2507.13693", "authors": ["Hongyi Liu", "Haifeng Wang"], "title": "Gaussian kernel-based motion measurement", "categories": ["cs.CV"], "comment": null, "summary": "The growing demand for structural health monitoring has driven increasing interest in high-precision motion measurement, as structural information derived from extracted motions can effectively reflect the current condition of the structure. Among various motion measurement techniques, vision-based methods stand out due to their low cost, easy installation, and large-scale measurement. However, when it comes to sub-pixel-level motion measurement, current vision-based methods either lack sufficient accuracy or require extensive manual parameter tuning (e.g., pyramid layers, target pixels, and filter parameters) to reach good precision. To address this issue, we developed a novel Gaussian kernel-based motion measurement method, which can extract the motion between different frames via tracking the location of Gaussian kernels. The motion consistency, which fits practical structural conditions, and a super-resolution constraint, are introduced to increase accuracy and robustness of our method. Numerical and experimental validations show that it can consistently reach high accuracy without customized parameter setup for different test samples.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6838\u7684\u8fd0\u52a8\u6d4b\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u53c2\u5373\u53ef\u5b9e\u73b0\u4e9a\u50cf\u7d20\u7ea7\u7cbe\u5ea6\u3002", "motivation": "\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\u9700\u6c42\u589e\u957f\uff0c\u73b0\u6709\u89c6\u89c9\u65b9\u6cd5\u5728\u4e9a\u50cf\u7d20\u7ea7\u8fd0\u52a8\u6d4b\u91cf\u4e2d\u7cbe\u5ea6\u4e0d\u8db3\u6216\u9700\u5927\u91cf\u624b\u52a8\u8c03\u53c2\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u9ad8\u65af\u6838\u7684\u8fd0\u52a8\u6d4b\u91cf\u65b9\u6cd5\uff0c\u5f15\u5165\u8fd0\u52a8\u4e00\u81f4\u6027\u548c\u8d85\u5206\u8fa8\u7387\u7ea6\u675f\u4ee5\u63d0\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "result": "\u6570\u503c\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u5b9a\u5236\u53c2\u6570\u5373\u53ef\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9ad8\u7cbe\u5ea6\u7684\u8fd0\u52a8\u6d4b\u91cf\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13708", "pdf": "https://arxiv.org/pdf/2507.13708", "abs": "https://arxiv.org/abs/2507.13708", "authors": ["Sofia Jamil", "Bollampalli Areen Reddy", "Raghvendra Kumar", "Sriparna Saha", "Koustava Goswami", "K. J. Joseph"], "title": "PoemTale Diffusion: Minimising Information Loss in Poem to Image Generation with Multi-Stage Prompt Refinement", "categories": ["cs.CV"], "comment": "ECAI 2025", "summary": "Recent advancements in text-to-image diffusion models have achieved remarkable success in generating realistic and diverse visual content. A critical factor in this process is the model's ability to accurately interpret textual prompts. However, these models often struggle with creative expressions, particularly those involving complex, abstract, or highly descriptive language. In this work, we introduce a novel training-free approach tailored to improve image generation for a unique form of creative language: poetic verse, which frequently features layered, abstract, and dual meanings. Our proposed PoemTale Diffusion approach aims to minimise the information that is lost during poetic text-to-image conversion by integrating a multi stage prompt refinement loop into Language Models to enhance the interpretability of poetic texts. To support this, we adapt existing state-of-the-art diffusion models by modifying their self-attention mechanisms with a consistent self-attention technique to generate multiple consistent images, which are then collectively used to convey the poem's meaning. Moreover, to encourage research in the field of poetry, we introduce the P4I (PoemForImage) dataset, consisting of 1111 poems sourced from multiple online and offline resources. We engaged a panel of poetry experts for qualitative assessments. The results from both human and quantitative evaluations validate the efficacy of our method and contribute a novel perspective to poem-to-image generation with enhanced information capture in the generated images.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u8bad\u7ec3\u65b9\u6cd5PoemTale Diffusion\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u63d0\u793a\u4f18\u5316\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6539\u8fdb\uff0c\u63d0\u5347\u8bd7\u6b4c\u6587\u672c\u5230\u56fe\u50cf\u7684\u751f\u6210\u6548\u679c\uff0c\u5e76\u53d1\u5e03\u4e86P4I\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u3001\u62bd\u8c61\u7684\u8bd7\u6b4c\u8bed\u8a00\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u4fe1\u606f\u4e22\u5931\u4e25\u91cd\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u63d0\u793a\u4f18\u5316\u5faa\u73af\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6539\u8fdb\uff0c\u751f\u6210\u4e00\u81f4\u7684\u591a\u5e45\u56fe\u50cf\u4ee5\u4f20\u8fbe\u8bd7\u6b4c\u542b\u4e49\u3002", "result": "\u901a\u8fc7\u4eba\u7c7b\u548c\u5b9a\u91cf\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u751f\u6210\u56fe\u50cf\u80fd\u66f4\u597d\u5730\u6355\u6349\u8bd7\u6b4c\u4fe1\u606f\u3002", "conclusion": "PoemTale Diffusion\u4e3a\u8bd7\u6b4c\u5230\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u63a8\u52a8\u4e86\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2507.13739", "pdf": "https://arxiv.org/pdf/2507.13739", "abs": "https://arxiv.org/abs/2507.13739", "authors": ["Junsu Kim", "Yunhoe Ku", "Seungryul Baek"], "title": "Can Synthetic Images Conquer Forgetting? Beyond Unexplored Doubts in Few-Shot Class-Incremental Learning", "categories": ["cs.CV", "cs.AI"], "comment": "6th CLVISION ICCV Workshop accepted", "summary": "Few-shot class-incremental learning (FSCIL) is challenging due to extremely limited training data; while aiming to reduce catastrophic forgetting and learn new information. We propose Diffusion-FSCIL, a novel approach that employs a text-to-image diffusion model as a frozen backbone. Our conjecture is that FSCIL can be tackled using a large generative model's capabilities benefiting from 1) generation ability via large-scale pre-training; 2) multi-scale representation; 3) representational flexibility through the text encoder. To maximize the representation capability, we propose to extract multiple complementary diffusion features to play roles as latent replay with slight support from feature distillation for preventing generative biases. Our framework realizes efficiency through 1) using a frozen backbone; 2) minimal trainable components; 3) batch processing of multiple feature extractions. Extensive experiments on CUB-200, \\emph{mini}ImageNet, and CIFAR-100 show that Diffusion-FSCIL surpasses state-of-the-art methods, preserving performance on previously learned classes and adapting effectively to new ones.", "AI": {"tldr": "Diffusion-FSCIL\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u89e3\u51b3\u5c11\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u8f7b\u91cf\u7ea7\u84b8\u998f\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5c11\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\u4e2d\u6570\u636e\u4e0d\u8db3\u548c\u707e\u96be\u6027\u9057\u5fd8\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u51bb\u7ed3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u4e3b\u5e72\uff0c\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\u5e76\u8f85\u4ee5\u8f7b\u91cf\u7ea7\u84b8\u998f\u3002", "result": "\u5728CUB-200\u3001miniImageNet\u548cCIFAR-100\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u4fdd\u6301\u65e7\u7c7b\u6027\u80fd\u5e76\u9002\u5e94\u65b0\u7c7b\u3002", "conclusion": "Diffusion-FSCIL\u901a\u8fc7\u751f\u6210\u6a21\u578b\u7684\u80fd\u529b\u548c\u9ad8\u6548\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5c11\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\u95ee\u9898\u3002"}}
{"id": "2507.13753", "pdf": "https://arxiv.org/pdf/2507.13753", "abs": "https://arxiv.org/abs/2507.13753", "authors": ["Tongtong Su", "Chengyu Wang", "Bingyan Liu", "Jun Huang", "Dongming Lu"], "title": "Encapsulated Composition of Text-to-Image and Text-to-Video Models for High-Quality Video Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, large text-to-video (T2V) synthesis models have garnered considerable attention for their abilities to generate videos from textual descriptions. However, achieving both high imaging quality and effective motion representation remains a significant challenge for these T2V models. Existing approaches often adapt pre-trained text-to-image (T2I) models to refine video frames, leading to issues such as flickering and artifacts due to inconsistencies across frames. In this paper, we introduce EVS, a training-free Encapsulated Video Synthesizer that composes T2I and T2V models to enhance both visual fidelity and motion smoothness of generated videos. Our approach utilizes a well-trained diffusion-based T2I model to refine low-quality video frames by treating them as out-of-distribution samples, effectively optimizing them with noising and denoising steps. Meanwhile, we employ T2V backbones to ensure consistent motion dynamics. By encapsulating the T2V temporal-only prior into the T2I generation process, EVS successfully leverages the strengths of both types of models, resulting in videos of improved imaging and motion quality. Experimental results validate the effectiveness of our approach compared to previous approaches. Our composition process also leads to a significant improvement of 1.6x-4.5x speedup in inference time. Source codes: https://github.com/Tonniia/EVS.", "AI": {"tldr": "EVS\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5c01\u88c5\u89c6\u9891\u5408\u6210\u5668\uff0c\u7ed3\u5408T2I\u548cT2V\u6a21\u578b\uff0c\u63d0\u5347\u751f\u6210\u89c6\u9891\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u8fd0\u52a8\u5e73\u6ed1\u6027\u3002", "motivation": "\u73b0\u6709T2V\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u65f6\u5b58\u5728\u753b\u9762\u95ea\u70c1\u548c\u4f2a\u5f71\u95ee\u9898\uff0cEVS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684T2I\u6a21\u578b\u4f18\u5316\u4f4e\u8d28\u91cf\u89c6\u9891\u5e27\uff0c\u540c\u65f6\u7ed3\u5408T2V\u6a21\u578b\u4fdd\u8bc1\u8fd0\u52a8\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eEVS\u5728\u89c6\u9891\u8d28\u91cf\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "EVS\u901a\u8fc7\u7ed3\u5408T2I\u548cT2V\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u548c\u9ad8\u6548\u63a8\u7406\u3002"}}
{"id": "2507.13769", "pdf": "https://arxiv.org/pdf/2507.13769", "abs": "https://arxiv.org/abs/2507.13769", "authors": ["Mingyang Yu", "Zhijian Wu", "Dingjiang Huang"], "title": "Learning Spectral Diffusion Prior for Hyperspectral Image Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Hyperspectral image (HSI) reconstruction aims to recover 3D HSI from its degraded 2D measurements. Recently great progress has been made in deep learning-based methods, however, these methods often struggle to accurately capture high-frequency details of the HSI. To address this issue, this paper proposes a Spectral Diffusion Prior (SDP) that is implicitly learned from hyperspectral images using a diffusion model. Leveraging the powerful ability of the diffusion model to reconstruct details, this learned prior can significantly improve the performance when injected into the HSI model. To further improve the effectiveness of the learned prior, we also propose the Spectral Prior Injector Module (SPIM) to dynamically guide the model to recover the HSI details. We evaluate our method on two representative HSI methods: MST and BISRNet. Experimental results show that our method outperforms existing networks by about 0.5 dB, effectively improving the performance of HSI reconstruction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5149\u8c31\u6269\u6563\u5148\u9a8c\uff08SDP\uff09\u548c\u5149\u8c31\u5148\u9a8c\u6ce8\u5165\u6a21\u5757\uff08SPIM\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\uff08HSI\uff09\u91cd\u5efa\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u6355\u6349HSI\u7684\u9ad8\u9891\u7ec6\u8282\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u5148\u9a8c\u77e5\u8bc6\u6765\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "method": "\u901a\u8fc7\u6269\u6563\u6a21\u578b\u9690\u5f0f\u5b66\u4e60HSI\u7684\u5149\u8c31\u6269\u6563\u5148\u9a8c\uff08SDP\uff09\uff0c\u5e76\u7ed3\u5408\u52a8\u6001\u6307\u5bfc\u7684\u5149\u8c31\u5148\u9a8c\u6ce8\u5165\u6a21\u5757\uff08SPIM\uff09\u6765\u6062\u590d\u7ec6\u8282\u3002", "result": "\u5728MST\u548cBISRNet\u4e24\u79cd\u4ee3\u8868\u6027HSI\u65b9\u6cd5\u4e0a\uff0c\u6027\u80fd\u63d0\u5347\u7ea60.5 dB\u3002", "conclusion": "SDP\u548cSPIM\u7684\u7ed3\u5408\u6709\u6548\u63d0\u5347\u4e86HSI\u91cd\u5efa\u7684\u7ec6\u8282\u6062\u590d\u80fd\u529b\u3002"}}
{"id": "2507.13789", "pdf": "https://arxiv.org/pdf/2507.13789", "abs": "https://arxiv.org/abs/2507.13789", "authors": ["Kyriakos Flouris", "Moritz Halter", "Yolanne Y. R. Lee", "Samuel Castonguay", "Luuk Jacobs", "Pietro Dirix", "Jonathan Nestmann", "Sebastian Kozerke", "Ender Konukoglu"], "title": "Localized FNO for Spatiotemporal Hemodynamic Upsampling in Aneurysm MRI", "categories": ["cs.CV", "cs.AI", "physics.comp-ph"], "comment": null, "summary": "Hemodynamic analysis is essential for predicting aneurysm rupture and guiding treatment. While magnetic resonance flow imaging enables time-resolved volumetric blood velocity measurements, its low spatiotemporal resolution and signal-to-noise ratio limit its diagnostic utility. To address this, we propose the Localized Fourier Neural Operator (LoFNO), a novel 3D architecture that enhances both spatial and temporal resolution with the ability to predict wall shear stress (WSS) directly from clinical imaging data. LoFNO integrates Laplacian eigenvectors as geometric priors for improved structural awareness on irregular, unseen geometries and employs an Enhanced Deep Super-Resolution Network (EDSR) layer for robust upsampling. By combining geometric priors with neural operator frameworks, LoFNO de-noises and spatiotemporally upsamples flow data, achieving superior velocity and WSS predictions compared to interpolation and alternative deep learning methods, enabling more precise cerebrovascular diagnostics.", "AI": {"tldr": "LoFNO\u662f\u4e00\u79cd\u65b0\u578b3D\u67b6\u6784\uff0c\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u5148\u9a8c\u548c\u795e\u7ecf\u7b97\u5b50\u6846\u67b6\uff0c\u63d0\u5347\u8840\u6d41\u6570\u636e\u7684\u65f6\u7a7a\u5206\u8fa8\u7387\uff0c\u76f4\u63a5\u9884\u6d4b\u58c1\u9762\u526a\u5207\u5e94\u529b\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u78c1\u5171\u632f\u8840\u6d41\u6210\u50cf\u7684\u4f4e\u65f6\u7a7a\u5206\u8fa8\u7387\u548c\u4fe1\u566a\u6bd4\u9650\u5236\u4e86\u5176\u8bca\u65ad\u6548\u679c\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u63d0\u5347\u8840\u6d41\u52a8\u529b\u5b66\u5206\u6790\u7684\u7cbe\u5ea6\u3002", "method": "\u63d0\u51faLoFNO\u67b6\u6784\uff0c\u6574\u5408\u62c9\u666e\u62c9\u65af\u7279\u5f81\u5411\u91cf\u4f5c\u4e3a\u51e0\u4f55\u5148\u9a8c\uff0c\u7ed3\u5408EDSR\u5c42\u8fdb\u884c\u9c81\u68d2\u4e0a\u91c7\u6837\uff0c\u5b9e\u73b0\u53bb\u566a\u548c\u65f6\u7a7a\u5206\u8fa8\u7387\u63d0\u5347\u3002", "result": "LoFNO\u5728\u901f\u5ea6\u548c\u58c1\u9762\u526a\u5207\u5e94\u529b\u9884\u6d4b\u4e0a\u4f18\u4e8e\u63d2\u503c\u548c\u5176\u4ed6\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "LoFNO\u4e3a\u8111\u8840\u7ba1\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u5de5\u5177\uff0c\u63d0\u5347\u4e86\u8840\u6d41\u52a8\u529b\u5b66\u5206\u6790\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.13797", "pdf": "https://arxiv.org/pdf/2507.13797", "abs": "https://arxiv.org/abs/2507.13797", "authors": ["Huu-Phu Do", "Yu-Wei Chen", "Yi-Cheng Liao", "Chi-Wei Hsiao", "Han-Yang Wang", "Wei-Chen Chiu", "Ching-Chun Huang"], "title": "DynFaceRestore: Balancing Fidelity and Quality in Diffusion-Guided Blind Face Restoration with Dynamic Blur-Level Mapping and Guidance", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Blind Face Restoration aims to recover high-fidelity, detail-rich facial images from unknown degraded inputs, presenting significant challenges in preserving both identity and detail. Pre-trained diffusion models have been increasingly used as image priors to generate fine details. Still, existing methods often use fixed diffusion sampling timesteps and a global guidance scale, assuming uniform degradation. This limitation and potentially imperfect degradation kernel estimation frequently lead to under- or over-diffusion, resulting in an imbalance between fidelity and quality. We propose DynFaceRestore, a novel blind face restoration approach that learns to map any blindly degraded input to Gaussian blurry images. By leveraging these blurry images and their respective Gaussian kernels, we dynamically select the starting timesteps for each blurry image and apply closed-form guidance during the diffusion sampling process to maintain fidelity. Additionally, we introduce a dynamic guidance scaling adjuster that modulates the guidance strength across local regions, enhancing detail generation in complex areas while preserving structural fidelity in contours. This strategy effectively balances the trade-off between fidelity and quality. DynFaceRestore achieves state-of-the-art performance in both quantitative and qualitative evaluations, demonstrating robustness and effectiveness in blind face restoration.", "AI": {"tldr": "DynFaceRestore\u662f\u4e00\u79cd\u65b0\u9896\u7684\u76f2\u8138\u6062\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6269\u6563\u91c7\u6837\u8d77\u59cb\u65f6\u95f4\u6b65\u548c\u5c40\u90e8\u8c03\u6574\u5f15\u5bfc\u5f3a\u5ea6\uff0c\u5e73\u8861\u4e86\u4fdd\u771f\u5ea6\u548c\u7ec6\u8282\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u76f2\u8138\u6062\u590d\u4e2d\u56e0\u56fa\u5b9a\u6269\u6563\u91c7\u6837\u65f6\u95f4\u6b65\u548c\u5168\u5c40\u5f15\u5bfc\u5c3a\u5ea6\u5bfc\u81f4\u4fdd\u771f\u5ea6\u4e0e\u8d28\u91cf\u4e0d\u5e73\u8861\u3002", "method": "\u5229\u7528\u6a21\u7cca\u56fe\u50cf\u548c\u9ad8\u65af\u6838\u52a8\u6001\u9009\u62e9\u8d77\u59cb\u65f6\u95f4\u6b65\uff0c\u5f15\u5165\u52a8\u6001\u5f15\u5bfc\u7f29\u653e\u8c03\u8282\u5668\u5c40\u90e8\u8c03\u6574\u5f15\u5bfc\u5f3a\u5ea6\u3002", "result": "\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "DynFaceRestore\u901a\u8fc7\u52a8\u6001\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u76f2\u8138\u6062\u590d\u4e2d\u7684\u4fdd\u771f\u5ea6\u4e0e\u8d28\u91cf\u5e73\u8861\u95ee\u9898\u3002"}}
{"id": "2507.13803", "pdf": "https://arxiv.org/pdf/2507.13803", "abs": "https://arxiv.org/abs/2507.13803", "authors": ["Weiqi Yang", "Xu Zhou", "Jingfu Guan", "Hao Du", "Tianyu Bai"], "title": "GRAM-MAMBA: Holistic Feature Alignment for Wireless Perception with Adaptive Low-Rank Compensation", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal fusion is crucial for Internet of Things (IoT) perception, widely deployed in smart homes, intelligent transport, industrial automation, and healthcare. However, existing systems often face challenges: high model complexity hinders deployment in resource-constrained environments, unidirectional modal alignment neglects inter-modal relationships, and robustness suffers when sensor data is missing. These issues impede efficient and robust multimodal perception in real-world IoT settings. To overcome these limitations, we propose GRAM-MAMBA. This framework utilizes the linear-complexity Mamba model for efficient sensor time-series processing, combined with an optimized GRAM matrix strategy for pairwise alignment among modalities, addressing the shortcomings of traditional single-modality alignment. Inspired by Low-Rank Adaptation (LoRA), we introduce an adaptive low-rank layer compensation strategy to handle missing modalities post-training. This strategy freezes the pre-trained model core and irrelevant adaptive layers, fine-tuning only those related to available modalities and the fusion process. Extensive experiments validate GRAM-MAMBA's effectiveness. On the SPAWC2021 indoor positioning dataset, the pre-trained model shows lower error than baselines; adapting to missing modalities yields a 24.5% performance boost by training less than 0.2% of parameters. On the USC-HAD human activity recognition dataset, it achieves 93.55% F1 and 93.81% Overall Accuracy (OA), outperforming prior work; the update strategy increases F1 by 23% while training less than 0.3% of parameters. These results highlight GRAM-MAMBA's potential for achieving efficient and robust multimodal perception in resource-constrained environments.", "AI": {"tldr": "GRAM-MAMBA\u6846\u67b6\u901a\u8fc7\u7ebf\u6027\u590d\u6742\u5ea6\u7684Mamba\u6a21\u578b\u548c\u4f18\u5316\u7684GRAM\u77e9\u9635\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u878d\u5408\u4e2d\u7684\u6548\u7387\u3001\u5bf9\u9f50\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u878d\u5408\u7cfb\u7edf\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u56f0\u96be\uff0c\u6a21\u6001\u5bf9\u9f50\u4e0d\u5145\u5206\uff0c\u4e14\u5bf9\u7f3a\u5931\u6570\u636e\u9c81\u68d2\u6027\u5dee\uff0c\u9700\u8981\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408Mamba\u6a21\u578b\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u4f7f\u7528GRAM\u77e9\u9635\u4f18\u5316\u6a21\u6001\u5bf9\u9f50\uff0c\u5e76\u5f15\u5165\u4f4e\u79e9\u81ea\u9002\u5e94\u5c42\u8865\u507f\u7f3a\u5931\u6a21\u6001\u3002", "result": "\u5728SPAWC2021\u548cUSC-HAD\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5b9a\u4f4d\u8bef\u5dee\u66f4\u4f4e\uff0c\u6d3b\u52a8\u8bc6\u522b\u51c6\u786e\u7387\u66f4\u9ad8\uff0c\u4e14\u53c2\u6570\u8bad\u7ec3\u91cf\u6781\u5c11\u3002", "conclusion": "GRAM-MAMBA\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u591a\u6a21\u6001\u611f\u77e5\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.13852", "pdf": "https://arxiv.org/pdf/2507.13852", "abs": "https://arxiv.org/abs/2507.13852", "authors": ["Luigi Russo", "Francesco Mauro", "Babak Memar", "Alessandro Sebastianelli", "Silvia Liberata Ullo", "Paolo Gamba"], "title": "A Quantum-assisted Attention U-Net for Building Segmentation over Tunis using Sentinel-1 Data", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted at IEEE Joint Urban Remote Sensing Event (JURSE) 2025", "summary": "Building segmentation in urban areas is essential in fields such as urban planning, disaster response, and population mapping. Yet accurately segmenting buildings in dense urban regions presents challenges due to the large size and high resolution of satellite images. This study investigates the use of a Quanvolutional pre-processing to enhance the capability of the Attention U-Net model in the building segmentation. Specifically, this paper focuses on the urban landscape of Tunis, utilizing Sentinel-1 Synthetic Aperture Radar (SAR) imagery. In this work, Quanvolution was used to extract more informative feature maps that capture essential structural details in radar imagery, proving beneficial for accurate building segmentation. Preliminary results indicate that proposed methodology achieves comparable test accuracy to the standard Attention U-Net model while significantly reducing network parameters. This result aligns with findings from previous works, confirming that Quanvolution not only maintains model accuracy but also increases computational efficiency. These promising outcomes highlight the potential of quantum-assisted Deep Learning frameworks for large-scale building segmentation in urban environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u91cf\u5b50\u5377\u79ef\u9884\u5904\u7406\u589e\u5f3aAttention U-Net\u6a21\u578b\u5728\u57ce\u5e02\u5efa\u7b51\u5206\u5272\u4e2d\u7684\u80fd\u529b\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u57ce\u5e02\u5efa\u7b51\u5206\u5272\u5728\u89c4\u5212\u3001\u707e\u5bb3\u54cd\u5e94\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\u7684\u5904\u7406\u5b58\u5728\u6311\u6218\u3002", "method": "\u91c7\u7528\u91cf\u5b50\u5377\u79ef\u9884\u5904\u7406\u63d0\u53d6SAR\u56fe\u50cf\u7279\u5f81\uff0c\u7ed3\u5408Attention U-Net\u6a21\u578b\u8fdb\u884c\u5206\u5272\u3002", "result": "\u65b9\u6cd5\u5728\u6d4b\u8bd5\u7cbe\u5ea6\u4e0a\u4e0e\u6807\u51c6\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u7f51\u7edc\u53c2\u6570\u3002", "conclusion": "\u91cf\u5b50\u8f85\u52a9\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u5728\u5927\u89c4\u6a21\u57ce\u5e02\u5efa\u7b51\u5206\u5272\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.13891", "pdf": "https://arxiv.org/pdf/2507.13891", "abs": "https://arxiv.org/abs/2507.13891", "authors": ["Yu Wei", "Jiahui Zhang", "Xiaoqin Zhang", "Ling Shao", "Shijian Lu"], "title": "PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations", "categories": ["cs.CV"], "comment": "Accepted by ICCV2025", "summary": "COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing attention due to its remarkable performance in reconstructing high-quality 3D scenes from unposed images or videos. However, it often struggles to handle scenes with complex camera trajectories as featured by drastic rotation and translation across adjacent camera views, leading to degraded estimation of camera poses and further local minima in joint optimization of camera poses and 3D-GS. We propose PCR-GS, an innovative COLMAP-free 3DGS technique that achieves superior 3D scene modeling and camera pose estimation via camera pose co-regularization. PCR-GS achieves regularization from two perspectives. The first is feature reprojection regularization which extracts view-robust DINO features from adjacent camera views and aligns their semantic information for camera pose regularization. The second is wavelet-based frequency regularization which exploits discrepancy in high-frequency details to further optimize the rotation matrix in camera poses. Extensive experiments over multiple real-world scenes show that the proposed PCR-GS achieves superior pose-free 3D-GS scene modeling under dramatic changes of camera trajectories.", "AI": {"tldr": "PCR-GS\u662f\u4e00\u79cd\u65e0\u9700COLMAP\u76843D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u901a\u8fc7\u76f8\u673a\u59ff\u6001\u5171\u6b63\u5219\u5316\u89e3\u51b3\u4e86\u590d\u6742\u76f8\u673a\u8f68\u8ff9\u4e0b\u7684\u573a\u666f\u5efa\u6a21\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u76f8\u673a\u8f68\u8ff9\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u548c3D\u573a\u666f\u5efa\u6a21\u7684\u8054\u5408\u4f18\u5316\u9677\u5165\u5c40\u90e8\u6700\u5c0f\u503c\u3002", "method": "PCR-GS\u901a\u8fc7\u7279\u5f81\u91cd\u6295\u5f71\u6b63\u5219\u5316\u548c\u57fa\u4e8e\u5c0f\u6ce2\u7684\u9ad8\u9891\u6b63\u5219\u5316\uff0c\u4f18\u5316\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPCR-GS\u5728\u5267\u70c8\u53d8\u5316\u7684\u76f8\u673a\u8f68\u8ff9\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u9ad8\u8d28\u91cf\u76843D\u573a\u666f\u5efa\u6a21\u3002", "conclusion": "PCR-GS\u901a\u8fc7\u53cc\u91cd\u6b63\u5219\u5316\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u573a\u666f\u4e0b\u76843D\u5efa\u6a21\u6027\u80fd\u3002"}}
{"id": "2507.13929", "pdf": "https://arxiv.org/pdf/2507.13929", "abs": "https://arxiv.org/abs/2507.13929", "authors": ["Hsiang-Hui Hung", "Huu-Phu Do", "Yung-Hui Li", "Ching-Chun Huang"], "title": "TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted by MM 2024", "summary": "We present TimeNeRF, a generalizable neural rendering approach for rendering novel views at arbitrary viewpoints and at arbitrary times, even with few input views. For real-world applications, it is expensive to collect multiple views and inefficient to re-optimize for unseen scenes. Moreover, as the digital realm, particularly the metaverse, strives for increasingly immersive experiences, the ability to model 3D environments that naturally transition between day and night becomes paramount. While current techniques based on Neural Radiance Fields (NeRF) have shown remarkable proficiency in synthesizing novel views, the exploration of NeRF's potential for temporal 3D scene modeling remains limited, with no dedicated datasets available for this purpose. To this end, our approach harnesses the strengths of multi-view stereo, neural radiance fields, and disentanglement strategies across diverse datasets. This equips our model with the capability for generalizability in a few-shot setting, allows us to construct an implicit content radiance field for scene representation, and further enables the building of neural radiance fields at any arbitrary time. Finally, we synthesize novel views of that time via volume rendering. Experiments show that TimeNeRF can render novel views in a few-shot setting without per-scene optimization. Most notably, it excels in creating realistic novel views that transition smoothly across different times, adeptly capturing intricate natural scene changes from dawn to dusk.", "AI": {"tldr": "TimeNeRF\u662f\u4e00\u79cd\u901a\u7528\u7684\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4efb\u610f\u89c6\u89d2\u548c\u65f6\u95f4\u6e32\u67d3\u65b0\u89c6\u56fe\uff0c\u5373\u4f7f\u8f93\u5165\u89c6\u56fe\u8f83\u5c11\u3002\u5b83\u7ed3\u5408\u591a\u89c6\u89d2\u7acb\u4f53\u89c6\u89c9\u3001\u795e\u7ecf\u8f90\u5c04\u573a\u548c\u89e3\u7f20\u7b56\u7565\uff0c\u5b9e\u73b0\u5c11\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u80fd\u6784\u5efa\u4efb\u610f\u65f6\u95f4\u7684\u795e\u7ecf\u8f90\u5c04\u573a\u3002\u5b9e\u9a8c\u8868\u660e\uff0cTimeNeRF\u65e0\u9700\u9010\u573a\u666f\u4f18\u5316\u5373\u53ef\u751f\u6210\u65b0\u89c6\u56fe\uff0c\u5e76\u80fd\u5e73\u6ed1\u8fc7\u6e21\u4e0d\u540c\u65f6\u95f4\u7684\u573a\u666f\u53d8\u5316\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u591a\u89c6\u56fe\u91c7\u96c6\u6210\u672c\u9ad8\u4e14\u9010\u573a\u666f\u4f18\u5316\u6548\u7387\u4f4e\u3002\u968f\u7740\u5143\u5b87\u5b99\u5bf9\u6c89\u6d78\u5f0f\u4f53\u9a8c\u7684\u9700\u6c42\u589e\u52a0\uff0c\u5efa\u6a21\u81ea\u7136\u8fc7\u6e21\u663c\u591c\u76843D\u573a\u666f\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709NeRF\u6280\u672f\u5728\u65f6\u95f43D\u573a\u666f\u5efa\u6a21\u65b9\u9762\u6f5c\u529b\u672a\u5145\u5206\u63a2\u7d22\uff0c\u7f3a\u4e4f\u4e13\u7528\u6570\u636e\u96c6\u3002", "method": "\u7ed3\u5408\u591a\u89c6\u89d2\u7acb\u4f53\u89c6\u89c9\u3001\u795e\u7ecf\u8f90\u5c04\u573a\u548c\u89e3\u7f20\u7b56\u7565\uff0c\u6784\u5efa\u9690\u5f0f\u5185\u5bb9\u8f90\u5c04\u573a\u8868\u793a\u573a\u666f\uff0c\u5e76\u652f\u6301\u4efb\u610f\u65f6\u95f4\u7684\u795e\u7ecf\u8f90\u5c04\u573a\u6784\u5efa\u3002\u901a\u8fc7\u4f53\u6e32\u67d3\u5408\u6210\u65b0\u89c6\u56fe\u3002", "result": "TimeNeRF\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u65e0\u9700\u9010\u573a\u666f\u4f18\u5316\u5373\u53ef\u6e32\u67d3\u65b0\u89c6\u56fe\uff0c\u5e76\u80fd\u5e73\u6ed1\u8fc7\u6e21\u4e0d\u540c\u65f6\u95f4\u7684\u573a\u666f\u53d8\u5316\uff0c\u5982\u4ece\u9ece\u660e\u5230\u9ec4\u660f\u7684\u81ea\u7136\u573a\u666f\u53d8\u5316\u3002", "conclusion": "TimeNeRF\u5c55\u793a\u4e86\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u751f\u6210\u4efb\u610f\u65f6\u95f4\u65b0\u89c6\u56fe\u7684\u80fd\u529b\uff0c\u4e3a\u65f6\u95f4\u52a8\u60013D\u573a\u666f\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13934", "pdf": "https://arxiv.org/pdf/2507.13934", "abs": "https://arxiv.org/abs/2507.13934", "authors": ["Marzieh Gheisari", "Auguste Genovesio"], "title": "DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization", "categories": ["cs.CV"], "comment": null, "summary": "Unsupervised disentanglement of static appearance and dynamic motion in video remains a fundamental challenge, often hindered by information leakage and blurry reconstructions in existing VAE- and GAN-based approaches. We introduce DiViD, the first end-to-end video diffusion framework for explicit static-dynamic factorization. DiViD's sequence encoder extracts a global static token from the first frame and per-frame dynamic tokens, explicitly removing static content from the motion code. Its conditional DDPM decoder incorporates three key inductive biases: a shared-noise schedule for temporal consistency, a time-varying KL-based bottleneck that tightens at early timesteps (compressing static information) and relaxes later (enriching dynamics), and cross-attention that routes the global static token to all frames while keeping dynamic tokens frame-specific. An orthogonality regularizer further prevents residual static-dynamic leakage. We evaluate DiViD on real-world benchmarks using swap-based accuracy and cross-leakage metrics. DiViD outperforms state-of-the-art sequential disentanglement methods: it achieves the highest swap-based joint accuracy, preserves static fidelity while improving dynamic transfer, and reduces average cross-leakage.", "AI": {"tldr": "DiViD\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u89c6\u9891\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u663e\u5f0f\u5206\u79bb\u9759\u6001\u5916\u89c2\u548c\u52a8\u6001\u8fd0\u52a8\uff0c\u901a\u8fc7\u5168\u5c40\u9759\u6001\u4ee4\u724c\u548c\u5e27\u7ea7\u52a8\u6001\u4ee4\u724c\u5b9e\u73b0\uff0c\u5e76\u5f15\u5165\u591a\u79cd\u5f52\u7eb3\u504f\u7f6e\u548c\u6b63\u5219\u5316\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eVAE\u548cGAN\u7684\u65b9\u6cd5\u5728\u89c6\u9891\u4e2d\u5206\u79bb\u9759\u6001\u548c\u52a8\u6001\u4fe1\u606f\u65f6\u5b58\u5728\u4fe1\u606f\u6cc4\u6f0f\u548c\u6a21\u7cca\u91cd\u5efa\u95ee\u9898\uff0cDiViD\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "DiViD\u4f7f\u7528\u5e8f\u5217\u7f16\u7801\u5668\u63d0\u53d6\u5168\u5c40\u9759\u6001\u4ee4\u724c\u548c\u5e27\u7ea7\u52a8\u6001\u4ee4\u724c\uff0c\u5e76\u91c7\u7528\u6761\u4ef6DDPM\u89e3\u7801\u5668\uff0c\u7ed3\u5408\u5171\u4eab\u566a\u58f0\u8ba1\u5212\u3001\u65f6\u53d8KL\u74f6\u9888\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u7b49\u673a\u5236\u3002", "result": "DiViD\u5728\u771f\u5b9e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u4ea4\u6362\u8054\u5408\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u9759\u6001\u4fdd\u771f\u5ea6\u548c\u52a8\u6001\u4f20\u9012\u80fd\u529b\u3002", "conclusion": "DiViD\u901a\u8fc7\u663e\u5f0f\u5206\u79bb\u9759\u6001\u548c\u52a8\u6001\u4fe1\u606f\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4fe1\u606f\u6cc4\u6f0f\uff0c\u4e3a\u89c6\u9891\u5206\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.12898", "pdf": "https://arxiv.org/pdf/2507.12898", "abs": "https://arxiv.org/abs/2507.12898", "authors": ["Yao Feng", "Hengkai Tan", "Xinyi Mao", "Guodong Liu", "Shuhe Huang", "Chendong Xiang", "Hang Su", "Jun Zhu"], "title": "Generalist Bimanual Manipulation via Foundation Video Diffusion Models", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "comment": null, "summary": "Bimanual robotic manipulation, which involves the coordinated control of two robotic arms, is foundational for solving challenging tasks. Despite recent progress in general-purpose manipulation, data scarcity and embodiment heterogeneity remain serious obstacles to further scaling up in bimanual settings. In this paper, we introduce VIdeo Diffusion for Action Reasoning (VIDAR), a two-stage framework that leverages large-scale, diffusion-based video pre-training and a novel masked inverse dynamics model for action prediction. We pre-train the video diffusion model on 750K multi-view videos from three real-world bimanual robot platforms, utilizing a unified observation space that encodes robot, camera, task, and scene contexts. Our masked inverse dynamics model learns masks to extract action-relevant information from generated trajectories without requiring pixel-level labels, and the masks can effectively generalize to unseen backgrounds. Our experiments demonstrate that with only 20 minutes of human demonstrations on an unseen robot platform (only 1% of typical data requirements), VIDAR generalizes to unseen tasks and backgrounds with strong semantic understanding, surpassing state-of-the-art methods. Our findings highlight the potential of video foundation models, coupled with masked action prediction, to enable scalable and generalizable robotic manipulation in diverse real-world settings.", "AI": {"tldr": "VIDAR\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u89c6\u9891\u9884\u8bad\u7ec3\u548c\u63a9\u7801\u9006\u52a8\u529b\u5b66\u6a21\u578b\u89e3\u51b3\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u5f02\u6784\u6027\u95ee\u9898\u3002", "motivation": "\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u5f02\u6784\u6027\u9650\u5236\u4e86\u5176\u6269\u5c55\u80fd\u529b\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u6269\u6563\u6a21\u578b\u89c6\u9891\u9884\u8bad\u7ec3\u548c\u63a9\u7801\u9006\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5229\u7528\u591a\u89c6\u89d2\u89c6\u9891\u6570\u636e\u548c\u65e0\u50cf\u7d20\u6807\u7b7e\u7684\u52a8\u4f5c\u9884\u6d4b\u3002", "result": "\u4ec5\u970020\u5206\u949f\u4eba\u7c7b\u6f14\u793a\uff0cVIDAR\u5728\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u548c\u80cc\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u89c6\u9891\u57fa\u7840\u6a21\u578b\u4e0e\u63a9\u7801\u52a8\u4f5c\u9884\u6d4b\u7ed3\u5408\uff0c\u6709\u671b\u5b9e\u73b0\u53ef\u6269\u5c55\u548c\u901a\u7528\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u3002"}}
{"id": "2507.13366", "pdf": "https://arxiv.org/pdf/2507.13366", "abs": "https://arxiv.org/abs/2507.13366", "authors": ["Baoshen Guo", "Zhiqing Hong", "Junyi Li", "Shenhao Wang", "Jinhua Zhao"], "title": "Leveraging the Spatial Hierarchy: Coarse-to-fine Trajectory Generation via Cascaded Hybrid Diffusion", "categories": ["cs.SI", "cs.CV"], "comment": null, "summary": "Urban mobility data has significant connections with economic growth and plays an essential role in various smart-city applications. However, due to privacy concerns and substantial data collection costs, fine-grained human mobility trajectories are difficult to become publicly available on a large scale. A promising solution to address this issue is trajectory synthesizing. However, existing works often ignore the inherent structural complexity of trajectories, unable to handle complicated high-dimensional distributions and generate realistic fine-grained trajectories. In this paper, we propose Cardiff, a coarse-to-fine Cascaded hybrid diffusion-based trajectory synthesizing framework for fine-grained and privacy-preserving mobility generation. By leveraging the hierarchical nature of urban mobility, Cardiff decomposes the generation process into two distinct levels, i.e., discrete road segment-level and continuous fine-grained GPS-level: (i) In the segment-level, to reduce computational costs and redundancy in raw trajectories, we first encode the discrete road segments into low-dimensional latent embeddings and design a diffusion transformer-based latent denoising network for segment-level trajectory synthesis. (ii) Taking the first stage of generation as conditions, we then design a fine-grained GPS-level conditional denoising network with a noise augmentation mechanism to achieve robust and high-fidelity generation. Additionally, the Cardiff framework not only progressively generates high-fidelity trajectories through cascaded denoising but also flexibly enables a tunable balance between privacy preservation and utility. Experimental results on three large real-world trajectory datasets demonstrate that our method outperforms state-of-the-art baselines in various metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCardiff\u7684\u5206\u5c42\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u7ec6\u7c92\u5ea6\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u8f68\u8ff9\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u9ad8\u7ef4\u590d\u6742\u5206\u5e03\u548c\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u7531\u4e8e\u9690\u79c1\u95ee\u9898\u548c\u6570\u636e\u6536\u96c6\u6210\u672c\uff0c\u7ec6\u7c92\u5ea6\u8f68\u8ff9\u6570\u636e\u96be\u4ee5\u5927\u89c4\u6a21\u516c\u5f00\u3002\u73b0\u6709\u8f68\u8ff9\u5408\u6210\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u9ad8\u7ef4\u590d\u6742\u5206\u5e03\uff0c\u751f\u6210\u4e0d\u771f\u5b9e\u3002", "method": "Cardiff\u91c7\u7528\u5206\u5c42\u751f\u6210\u65b9\u6cd5\uff1a\u5148\u901a\u8fc7\u6269\u6563\u53d8\u6362\u5668\u5408\u6210\u79bb\u6563\u8def\u6bb5\u7ea7\u8f68\u8ff9\uff0c\u518d\u57fa\u4e8e\u6761\u4ef6\u53bb\u566a\u7f51\u7edc\u751f\u6210\u7ec6\u7c92\u5ea6GPS\u7ea7\u8f68\u8ff9\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cCardiff\u5728\u591a\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Cardiff\u901a\u8fc7\u5206\u5c42\u751f\u6210\u548c\u9690\u79c1\u4fdd\u62a4\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u8f68\u8ff9\u5408\u6210\uff0c\u5e73\u8861\u4e86\u9690\u79c1\u4e0e\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.13782", "pdf": "https://arxiv.org/pdf/2507.13782", "abs": "https://arxiv.org/abs/2507.13782", "authors": ["Malo Gicquel", "Ruoyi Zhao", "Anika Wuestefeld", "Nicola Spotorno", "Olof Strandberg", "Kalle \u00c5str\u00f6m", "Yu Xiao", "Laura EM Wisse", "Danielle van Westen", "Rik Ossenkoppele", "Niklas Mattsson-Carlgren", "David Berron", "Oskar Hansson", "Gabrielle Flood", "Jacob Vogel"], "title": "Converting T1-weighted MRI from 3T to 7T quality using deep learning", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Ultra-high resolution 7 tesla (7T) magnetic resonance imaging (MRI) provides detailed anatomical views, offering better signal-to-noise ratio, resolution and tissue contrast than 3T MRI, though at the cost of accessibility. We present an advanced deep learning model for synthesizing 7T brain MRI from 3T brain MRI. Paired 7T and 3T T1-weighted images were acquired from 172 participants (124 cognitively unimpaired, 48 impaired) from the Swedish BioFINDER-2 study. To synthesize 7T MRI from 3T images, we trained two models: a specialized U-Net, and a U-Net integrated with a generative adversarial network (GAN U-Net). Our models outperformed two additional state-of-the-art 3T-to-7T models in image-based evaluation metrics. Four blinded MRI professionals judged our synthetic 7T images as comparable in detail to real 7T images, and superior in subjective visual quality to 7T images, apparently due to the reduction of artifacts. Importantly, automated segmentations of the amygdalae of synthetic GAN U-Net 7T images were more similar to manually segmented amygdalae (n=20), than automated segmentations from the 3T images that were used to synthesize the 7T images. Finally, synthetic 7T images showed similar performance to real 3T images in downstream prediction of cognitive status using MRI derivatives (n=3,168). In all, we show that synthetic T1-weighted brain images approaching 7T quality can be generated from 3T images, which may improve image quality and segmentation, without compromising performance in downstream tasks. Future directions, possible clinical use cases, and limitations are discussed.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u4ece3T MRI\u5408\u62107T MRI\uff0c\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5408\u6210\u56fe\u50cf\u5728\u7ec6\u8282\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u63a5\u8fd1\u771f\u5b9e7T\u56fe\u50cf\u3002", "motivation": "7T MRI\u63d0\u4f9b\u66f4\u9ad8\u7684\u5206\u8fa8\u7387\u548c\u7ec4\u7ec7\u5bf9\u6bd4\u5ea6\uff0c\u4f46\u666e\u53ca\u6027\u8f83\u4f4e\uff0c\u56e0\u6b64\u5e0c\u671b\u901a\u8fc73T MRI\u5408\u62107T\u56fe\u50cf\u4ee5\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002", "method": "\u4f7f\u7528U-Net\u548cGAN U-Net\u6a21\u578b\uff0c\u57fa\u4e8e172\u540d\u53c2\u4e0e\u8005\u7684\u914d\u5bf93T\u548c7T T1\u52a0\u6743\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5408\u62107T\u56fe\u50cf\u5728\u7ec6\u8282\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u4f18\u4e8e\u771f\u5b9e7T\u56fe\u50cf\uff0c\u4e14\u81ea\u52a8\u5206\u5272\u7ed3\u679c\u66f4\u63a5\u8fd1\u624b\u52a8\u5206\u5272\u3002", "conclusion": "\u5408\u62107T\u56fe\u50cf\u53ef\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548c\u5206\u5272\u6548\u679c\uff0c\u4e14\u4e0d\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u5177\u6709\u6f5c\u5728\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.13915", "pdf": "https://arxiv.org/pdf/2507.13915", "abs": "https://arxiv.org/abs/2507.13915", "authors": ["Huu-Phu Do", "Po-Chih Hu", "Hao-Chien Hsueh", "Che-Kai Liu", "Vu-Hoang Tran", "Ching-Chun Huang"], "title": "Blind Super Resolution with Reference Images and Implicit Degradation Representation", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by ACCV 2024", "summary": "Previous studies in blind super-resolution (BSR) have primarily concentrated on estimating degradation kernels directly from low-resolution (LR) inputs to enhance super-resolution. However, these degradation kernels, which model the transition from a high-resolution (HR) image to its LR version, should account for not only the degradation process but also the downscaling factor. Applying the same degradation kernel across varying super-resolution scales may be impractical. Our research acknowledges degradation kernels and scaling factors as pivotal elements for the BSR task and introduces a novel strategy that utilizes HR images as references to establish scale-aware degradation kernels. By employing content-irrelevant HR reference images alongside the target LR image, our model adaptively discerns the degradation process. It is then applied to generate additional LR-HR pairs through down-sampling the HR reference images, which are keys to improving the SR performance. Our reference-based training procedure is applicable to proficiently trained blind SR models and zero-shot blind SR methods, consistently outperforming previous methods in both scenarios. This dual consideration of blur kernels and scaling factors, coupled with the use of a reference image, contributes to the effectiveness of our approach in blind super-resolution tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53c2\u8003\u56fe\u50cf\u7684\u76f2\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u9ad8\u5206\u8fa8\u7387\u53c2\u8003\u56fe\u50cf\u751f\u6210\u5c3a\u5ea6\u611f\u77e5\u7684\u9000\u5316\u6838\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d85\u5206\u8fa8\u7387\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u76f2\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u76f4\u63a5\u4f30\u8ba1\u9000\u5316\u6838\uff0c\u4f46\u5ffd\u7565\u4e86\u4e0d\u540c\u8d85\u5206\u8fa8\u7387\u5c3a\u5ea6\u4e0b\u9000\u5316\u6838\u7684\u5dee\u5f02\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u9ad8\u5206\u8fa8\u7387\u53c2\u8003\u56fe\u50cf\uff0c\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528\u5185\u5bb9\u65e0\u5173\u7684\u9ad8\u5206\u8fa8\u7387\u53c2\u8003\u56fe\u50cf\u4e0e\u76ee\u6807\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u81ea\u9002\u5e94\u5730\u8bc6\u522b\u9000\u5316\u8fc7\u7a0b\uff0c\u5e76\u751f\u6210\u989d\u5916\u7684\u4f4e-\u9ad8\u5206\u8fa8\u7387\u5bf9\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u597d\u7684\u76f2\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u548c\u96f6\u6837\u672c\u76f2\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u4e2d\u5747\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408\u6a21\u7cca\u6838\u548c\u5c3a\u5ea6\u56e0\u5b50\uff0c\u5e76\u5229\u7528\u53c2\u8003\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76f2\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u7684\u6548\u679c\u3002"}}
