<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 23]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.RO](#cs.RO) [Total: 1]
- [math.NA](#math.NA) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation](https://arxiv.org/abs/2508.11203)
*Seungmi Lee,Kwan Yun,Junyong Noh*

Main category: cs.GR

TL;DR: StyleMM是一个基于文本描述的3D可变形模型（3DMM）风格化框架，通过扩散模型生成风格化图像并微调预训练网络，实现3D风格转换。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D风格转换中难以保持面部属性（如身份、对齐和表情）的一致性，StyleMM旨在解决这一问题。

Method: 结合预训练的网格变形网络和纹理生成器，利用扩散模型生成风格化图像作为目标，通过图像训练实现3D风格转换。

Result: StyleMM在身份多样性和风格化能力上优于现有方法，支持对形状、表情和纹理参数的显式控制。

Conclusion: StyleMM提供了一种高效的3D风格转换方法，适用于生成风格化且可动画的3D人脸模型。

Abstract: We introduce StyleMM, a novel framework that can construct a stylized 3D Morphable Model (3DMM) based on user-defined text descriptions specifying a target style. Building upon a pre-trained mesh deformation network and a texture generator for original 3DMM-based realistic human faces, our approach fine-tunes these models using stylized facial images generated via text-guided image-to-image (i2i) translation with a diffusion model, which serve as stylization targets for the rendered mesh. To prevent undesired changes in identity, facial alignment, or expressions during i2i translation, we introduce a stylization method that explicitly preserves the facial attributes of the source image. By maintaining these critical attributes during image stylization, the proposed approach ensures consistent 3D style transfer across the 3DMM parameter space through image-based training. Once trained, StyleMM enables feed-forward generation of stylized face meshes with explicit control over shape, expression, and texture parameters, producing meshes with consistent vertex connectivity and animatability. Quantitative and qualitative evaluations demonstrate that our approach outperforms state-of-the-art methods in terms of identity-level facial diversity and stylization capability. The code and videos are available at [kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).

</details>


### [2] [SPG: Style-Prompting Guidance for Style-Specific Content Creation](https://arxiv.org/abs/2508.11476)
*Qian Liang,Zichong Chen,Yang Zhou,Hui Huang*

Main category: cs.GR

TL;DR: 提出了一种名为Style-Prompting Guidance (SPG)的新采样策略，用于风格特定的图像生成，结合Classifier-Free Guidance (CFG)实现语义保真和风格一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的文本到图像扩散模型在生成与文本提示对齐的图像方面表现优异，但控制输出图像的视觉风格仍具挑战性。

Method: SPG构建风格噪声向量，并利用其与无条件噪声的方向偏差引导扩散过程朝向目标风格分布。

Result: 实验表明，SPG在风格生成方面优于现有方法，且兼容ControlNet和IPAdapter等可控框架。

Conclusion: SPG是一种简单、鲁棒且广泛适用的方法，能够有效实现风格一致的图像生成。

Abstract: Although recent text-to-image (T2I) diffusion models excel at aligning generated images with textual prompts, controlling the visual style of the output remains a challenging task. In this work, we propose Style-Prompting Guidance (SPG), a novel sampling strategy for style-specific image generation. SPG constructs a style noise vector and leverages its directional deviation from unconditional noise to guide the diffusion process toward the target style distribution. By integrating SPG with Classifier-Free Guidance (CFG), our method achieves both semantic fidelity and style consistency. SPG is simple, robust, and compatible with controllable frameworks like ControlNet and IPAdapter, making it practical and widely applicable. Extensive experiments demonstrate the effectiveness and generality of our approach compared to state-of-the-art methods. Code is available at https://github.com/Rumbling281441/SPG.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [3] [VSF: Simple, Efficient, and Effective Negative Guidance in Few-Step Image Generation Models By \underline{V}alue \underline{S}ign \underline{F}lip](https://arxiv.org/abs/2508.10931)
*Wenqi Guo,Shan Du*

Main category: cs.CV

TL;DR: VSF是一种简单高效的方法，通过翻转负提示的注意力值符号，动态抑制不想要的内容，适用于少步扩散和流匹配图像生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法如CFG、NASA和NAG在负提示引导方面存在不足，VSF旨在提供更高效的解决方案。

Method: VSF通过翻转负提示的注意力值符号，动态抑制不想要的内容，计算开销小，适用于多种架构。

Result: 实验表明，VSF在少步模型中显著优于现有方法，且在非少步模型中也能与CFG竞争，同时保持图像质量。

Conclusion: VSF是一种高效且通用的负提示引导方法，适用于多种生成任务。

Abstract: We introduce Value Sign Flip (VSF), a simple and efficient method for incorporating negative prompt guidance in few-step diffusion and flow-matching image generation models. Unlike existing approaches such as classifier-free guidance (CFG), NASA, and NAG, VSF dynamically suppresses undesired content by flipping the sign of attention values from negative prompts. Our method requires only small computational overhead and integrates effectively with MMDiT-style architectures such as Stable Diffusion 3.5 Turbo, as well as cross-attention-based models like Wan. We validate VSF on challenging datasets with complex prompt pairs and demonstrate superior performance in both static image and video generation tasks. Experimental results show that VSF significantly improves negative prompt adherence compared to prior methods in few-step models, and even CFG in non-few-step models, while maintaining competitive image quality. Code and ComfyUI node are available in https://github.com/weathon/VSF/tree/main.

</details>


### [4] [HQ-OV3D: A High Box Quality Open-World 3D Detection Framework based on Diffision Model](https://arxiv.org/abs/2508.10935)
*Qi Liu,Yabei Li,Hongsong Wang,Lei He*

Main category: cs.CV

TL;DR: 提出HQ-OV3D框架，通过高质量伪标签生成和优化，提升开放词汇3D检测的几何精度。


<details>
  <summary>Details</summary>
Motivation: 传统封闭集3D检测无法满足开放世界应用需求，现有开放词汇方法忽视几何质量。

Method: 提出IMCV提案生成器和ACA去噪器，利用跨模态几何一致性和标注类几何先验优化伪标签。

Result: 在新型类上mAP提升7.37%，伪标签质量显著优于现有方法。

Conclusion: HQ-OV3D可作为独立检测器或插件伪标签生成器，提升开放词汇3D检测性能。

Abstract: Traditional closed-set 3D detection frameworks fail to meet the demands of open-world applications like autonomous driving. Existing open-vocabulary 3D detection methods typically adopt a two-stage pipeline consisting of pseudo-label generation followed by semantic alignment. While vision-language models (VLMs) recently have dramatically improved the semantic accuracy of pseudo-labels, their geometric quality, particularly bounding box precision, remains commonly neglected.To address this issue, we propose a High Box Quality Open-Vocabulary 3D Detection (HQ-OV3D) framework, dedicated to generate and refine high-quality pseudo-labels for open-vocabulary classes. The framework comprises two key components: an Intra-Modality Cross-Validated (IMCV) Proposal Generator that utilizes cross-modality geometric consistency to generate high-quality initial 3D proposals, and an Annotated-Class Assisted (ACA) Denoiser that progressively refines 3D proposals by leveraging geometric priors from annotated categories through a DDIM-based denoising mechanism.Compared to the state-of-the-art method, training with pseudo-labels generated by our approach achieves a 7.37% improvement in mAP on novel classes, demonstrating the superior quality of the pseudo-labels produced by our framework. HQ-OV3D can serve not only as a strong standalone open-vocabulary 3D detector but also as a plug-in high-quality pseudo-label generator for existing open-vocabulary detection or annotation pipelines.

</details>


### [5] [Vision-Only Gaussian Splatting for Collaborative Semantic Occupancy Prediction](https://arxiv.org/abs/2508.10936)
*Cheng Chen,Hao Huang,Saurabh Bagchi*

Main category: cs.CV

TL;DR: 提出了一种基于稀疏3D语义高斯泼溅的协作3D语义占据预测方法，通过共享和融合高斯基元，降低了通信成本并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉方法在协作感知中因密集3D体素或2D平面特征导致的高通信成本或依赖深度估计的问题。

Method: 利用稀疏3D语义高斯泼溅，通过共享和融合高斯基元，实现跨代理融合、几何与语义联合编码及稀疏对象中心消息传递。

Result: 在mIoU和IoU指标上分别比单代理和基线协作方法提升8.42/3.28和5.11/22.41分，通信量减少至34.6%时仍保持性能优势。

Conclusion: 该方法在协作感知中显著提升了性能并降低了通信成本，适用于有限通信预算的场景。

Abstract: Collaborative perception enables connected vehicles to share information, overcoming occlusions and extending the limited sensing range inherent in single-agent (non-collaborative) systems. Existing vision-only methods for 3D semantic occupancy prediction commonly rely on dense 3D voxels, which incur high communication costs, or 2D planar features, which require accurate depth estimation or additional supervision, limiting their applicability to collaborative scenarios. To address these challenges, we propose the first approach leveraging sparse 3D semantic Gaussian splatting for collaborative 3D semantic occupancy prediction. By sharing and fusing intermediate Gaussian primitives, our method provides three benefits: a neighborhood-based cross-agent fusion that removes duplicates and suppresses noisy or inconsistent Gaussians; a joint encoding of geometry and semantics in each primitive, which reduces reliance on depth supervision and allows simple rigid alignment; and sparse, object-centric messages that preserve structural information while reducing communication volume. Extensive experiments demonstrate that our approach outperforms single-agent perception and baseline collaborative methods by +8.42 and +3.28 points in mIoU, and +5.11 and +22.41 points in IoU, respectively. When further reducing the number of transmitted Gaussians, our method still achieves a +1.9 improvement in mIoU, using only 34.6% communication volume, highlighting robust performance under limited communication budgets.

</details>


### [6] [Personalized Face Super-Resolution with Identity Decoupling and Fitting](https://arxiv.org/abs/2508.10937)
*Jiarui Yang,Hang Guo,Wen Huang,Tao Dai,Shutao Xia*

Main category: cs.CV

TL;DR: 提出了一种新的面部超分辨率方法IDFSR，通过身份解耦和拟合，在极端退化场景下提升身份一致性和图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在极端退化（如8倍以上缩放）下容易生成缺乏真实身份约束的幻觉面部，IDFSR旨在解决这一问题。

Method: 1）掩蔽低分辨率图像中的面部区域；2）对齐参考图像以提供风格指导；3）利用真实图像的ID嵌入进行细粒度建模。

Result: IDFSR在极端退化条件下显著优于现有方法，尤其在身份一致性上表现优异。

Conclusion: IDFSR通过身份解耦和拟合，有效提升了极端退化场景下的面部超分辨率性能。

Abstract: In recent years, face super-resolution (FSR) methods have achieved remarkable progress, generally maintaining high image fidelity and identity (ID) consistency under standard settings. However, in extreme degradation scenarios (e.g., scale $> 8\times$), critical attributes and ID information are often severely lost in the input image, making it difficult for conventional models to reconstruct realistic and ID-consistent faces. Existing methods tend to generate hallucinated faces under such conditions, producing restored images lacking authentic ID constraints. To address this challenge, we propose a novel FSR method with Identity Decoupling and Fitting (IDFSR), designed to enhance ID restoration under large scaling factors while mitigating hallucination effects. Our approach involves three key designs: 1) \textbf{Masking} the facial region in the low-resolution (LR) image to eliminate unreliable ID cues; 2) \textbf{Warping} a reference image to align with the LR input, providing style guidance; 3) Leveraging \textbf{ID embeddings} extracted from ground truth (GT) images for fine-grained ID modeling and personalized adaptation. We first pretrain a diffusion-based model to explicitly decouple style and ID by forcing it to reconstruct masked LR face regions using both style and identity embeddings. Subsequently, we freeze most network parameters and perform lightweight fine-tuning of the ID embedding using a small set of target ID images. This embedding encodes fine-grained facial attributes and precise ID information, significantly improving both ID consistency and perceptual quality. Extensive quantitative evaluations and visual comparisons demonstrate that the proposed IDFSR substantially outperforms existing approaches under extreme degradation, particularly achieving superior performance on ID consistency.

</details>


### [7] [EVCtrl: Efficient Control Adapter for Visual Generation](https://arxiv.org/abs/2508.10963)
*Zixiang Yang,Yue Ma,Yinhan Zhang,Shanhui Mo,Dongrui Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: EVCtrl是一种轻量级控制适配器，通过时空双缓存策略减少冗余计算，显著提升生成效率，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决ControlNet在视频生成中因冗余计算导致的高延迟问题。

Method: 提出时空双缓存策略：空间上分区处理控制信号，时间上选择性跳过冗余去噪步骤。

Result: 在多个模型上实现2倍以上的速度提升，生成质量几乎无损失。

Conclusion: EVCtrl是一种高效、无需训练的解决方案，适用于图像和视频的精确控制生成。

Abstract: Visual generation includes both image and video generation, training probabilistic models to create coherent, diverse, and semantically faithful content from scratch. While early research focused on unconditional sampling, practitioners now demand controllable generation that allows precise specification of layout, pose, motion, or style. While ControlNet grants precise spatial-temporal control, its auxiliary branch markedly increases latency and introduces redundant computation in both uncontrolled regions and denoising steps, especially for video. To address this problem, we introduce EVCtrl, a lightweight, plug-and-play control adapter that slashes overhead without retraining the model. Specifically, we propose a spatio-temporal dual caching strategy for sparse control information. For spatial redundancy, we first profile how each layer of DiT-ControlNet responds to fine-grained control, then partition the network into global and local functional zones. A locality-aware cache focuses computation on the local zones that truly need the control signal, skipping the bulk of redundant computation in global regions. For temporal redundancy, we selectively omit unnecessary denoising steps to improve efficiency. Extensive experiments on CogVideo-Controlnet, Wan2.1-Controlnet, and Flux demonstrate that our method is effective in image and video control generation without the need for training. For example, it achieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and Wan2.1-Controlnet, respectively, with almost no degradation in generation quality.Codes are available in the supplementary materials.

</details>


### [8] [HierOctFusion: Multi-scale Octree-based 3D Shape Generation via Part-Whole-Hierarchy Message Passing](https://arxiv.org/abs/2508.11106)
*Xinjie Gao,Bi'an Du,Wei Hu*

Main category: cs.CV

TL;DR: HierOctFusion提出了一种基于八叉树的分层扩散模型，通过多尺度特征交互和语义部分信息注入，提升了3D内容生成的精细度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法将3D对象视为整体，忽略了语义部分层次结构，且高分辨率建模计算成本高。HierOctFusion旨在通过分层生成和部分感知建模解决这些问题。

Method: 采用多尺度八叉树扩散模型，引入跨注意力机制注入部分级信息，并构建带有部分类别标注的3D数据集。

Result: 实验表明，HierOctFusion在形状质量和效率上优于现有方法。

Conclusion: HierOctFusion通过分层和部分感知建模，显著提升了3D内容生成的性能。

Abstract: 3D content generation remains a fundamental yet challenging task due to the inherent structural complexity of 3D data. While recent octree-based diffusion models offer a promising balance between efficiency and quality through hierarchical generation, they often overlook two key insights: 1) existing methods typically model 3D objects as holistic entities, ignoring their semantic part hierarchies and limiting generalization; and 2) holistic high-resolution modeling is computationally expensive, whereas real-world objects are inherently sparse and hierarchical, making them well-suited for layered generation. Motivated by these observations, we propose HierOctFusion, a part-aware multi-scale octree diffusion model that enhances hierarchical feature interaction for generating fine-grained and sparse object structures. Furthermore, we introduce a cross-attention conditioning mechanism that injects part-level information into the generation process, enabling semantic features to propagate effectively across hierarchical levels from parts to the whole. Additionally, we construct a 3D dataset with part category annotations using a pre-trained segmentation model to facilitate training and evaluation. Experiments demonstrate that HierOctFusion achieves superior shape quality and efficiency compared to prior methods.

</details>


### [9] [Residual-based Efficient Bidirectional Diffusion Model for Image Dehazing and Haze Generation](https://arxiv.org/abs/2508.11134)
*Bing Liu,Le Wang,Hao Liu,Mingming Liu*

Main category: cs.CV

TL;DR: 提出了一种基于残差的高效双向扩散模型（RBDM），用于实现有雾和无雾图像之间的双向转换。


<details>
  <summary>Details</summary>
Motivation: 现有深度去雾方法仅关注去雾，缺乏有雾和无雾图像之间的双向转换能力。

Method: 设计双马尔可夫链以平滑转换残差，通过扰动图像并预测噪声学习条件分布，引入统一评分函数以降低计算成本。

Result: 在合成和真实数据集上表现优于或至少与现有方法相当，仅需15步采样。

Conclusion: RBDM成功实现了有雾和无雾图像之间的双向转换，且高效低耗。

Abstract: Current deep dehazing methods only focus on removing haze from hazy images, lacking the capability to translate between hazy and haze-free images. To address this issue, we propose a residual-based efficient bidirectional diffusion model (RBDM) that can model the conditional distributions for both dehazing and haze generation. Firstly, we devise dual Markov chains that can effectively shift the residuals and facilitate bidirectional smooth transitions between them. Secondly, the RBDM perturbs the hazy and haze-free images at individual timesteps and predicts the noise in the perturbed data to simultaneously learn the conditional distributions. Finally, to enhance performance on relatively small datasets and reduce computational costs, our method introduces a unified score function learned on image patches instead of entire images. Our RBDM successfully implements size-agnostic bidirectional transitions between haze-free and hazy images with only 15 sampling steps. Extensive experiments demonstrate that the proposed method achieves superior or at least comparable performance to state-of-the-art methods on both synthetic and real-world datasets.

</details>


### [10] [LEARN: A Story-Driven Layout-to-Image Generation Framework for STEM Instruction](https://arxiv.org/abs/2508.11153)
*Maoquan Zhang,Bisser Raytchev,Xiujuan Sun*

Main category: cs.CV

TL;DR: LEARN是一个布局感知的扩散框架，用于生成与STEM教育教学对齐的插图，通过布局条件生成和视觉语义训练，支持中高层次的推理。


<details>
  <summary>Details</summary>
Motivation: 解决STEM教育中抽象科学概念的视觉表达问题，减少认知负荷，提升学习效果。

Method: 利用BookCover数据集，结合布局条件生成、对比视觉语义训练和提示调制。

Result: 生成连贯的视觉序列，支持Bloom分类法的中高层次推理，减少认知负荷。

Conclusion: LEARN为教育生成AI提供了新方向，未来可结合多模态系统和知识图谱。

Abstract: LEARN is a layout-aware diffusion framework designed to generate pedagogically aligned illustrations for STEM education. It leverages a curated BookCover dataset that provides narrative layouts and structured visual cues, enabling the model to depict abstract and sequential scientific concepts with strong semantic alignment. Through layout-conditioned generation, contrastive visual-semantic training, and prompt modulation, LEARN produces coherent visual sequences that support mid-to-high-level reasoning in line with Bloom's taxonomy while reducing extraneous cognitive load as emphasized by Cognitive Load Theory. By fostering spatially organized and story-driven narratives, the framework counters fragmented attention often induced by short-form media and promotes sustained conceptual focus. Beyond static diagrams, LEARN demonstrates potential for integration with multimodal systems and curriculum-linked knowledge graphs to create adaptive, exploratory educational content. As the first generative approach to unify layout-based storytelling, semantic structure learning, and cognitive scaffolding, LEARN represents a novel direction for generative AI in education. The code and dataset will be released to facilitate future research and practical deployment.

</details>


### [11] [Semi-supervised Image Dehazing via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models](https://arxiv.org/abs/2508.11165)
*Bing Liu,Le Wang,Mingming Liu,Hao Liu,Rui Yao,Yong Zhou,Peng Liu,Tongqiang Xia*

Main category: cs.CV

TL;DR: 提出了一种基于EM-B3DM的半监督图像去雾方法，通过两阶段学习方案解决了真实世界雾霾图像处理的难题。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理真实世界的厚雾场景，主要原因是缺乏真实配对数据和鲁棒先验。

Method: 采用EM算法和双向布朗桥扩散模型（EM-B3DM），分两阶段学习：第一阶段用EM算法解耦联合分布，第二阶段利用预训练模型和大规模未配对数据提升性能。还引入RDC块增强细节。

Result: 在合成和真实数据集上，EM-B3DM表现优于或至少与最先进方法相当。

Conclusion: EM-B3DM是一种高效的去雾方法，尤其适用于缺乏配对数据的场景。

Abstract: Existing dehazing methods deal with real-world haze images with difficulty, especially scenes with thick haze. One of the main reasons is the lack of real-world paired data and robust priors. To avoid the costly collection of paired hazy and clear images, we propose an efficient semi-supervised image dehazing method via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models (EM-B3DM) with a two-stage learning scheme. In the first stage, we employ the EM algorithm to decouple the joint distribution of paired hazy and clear images into two conditional distributions, which are then modeled using a unified Brownian Bridge diffusion model to directly capture the structural and content-related correlations between hazy and clear images. In the second stage, we leverage the pre-trained model and large-scale unpaired hazy and clear images to further improve the performance of image dehazing. Additionally, we introduce a detail-enhanced Residual Difference Convolution block (RDC) to capture gradient-level information, significantly enhancing the model's representation capability. Extensive experiments demonstrate that our EM-B3DM achieves superior or at least comparable performance to state-of-the-art methods on both synthetic and real-world datasets.

</details>


### [12] [Versatile Video Tokenization with Generative 2D Gaussian Splatting](https://arxiv.org/abs/2508.11183)
*Zhenghao Chen,Zicong Chen,Lei Liu,Yiming Wu,Dong Xu*

Main category: cs.CV

TL;DR: GVT是一种基于生成2D高斯分布的视频标记化方法，通过空间自适应和时间分离策略提升视频处理性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频标记化方法在空间和时间维度上存在冗余和适应性不足的问题。

Method: 提出GVT，结合2D高斯分布和STGE机制，通过GSP策略分离静态和动态内容。

Result: GVT在视频重建、动作识别和压缩任务中表现优异。

Conclusion: GVT通过生成式高斯分布和时空分离策略，显著提升了视频处理的效率和性能。

Abstract: Video tokenization procedure is critical for a wide range of video processing tasks. Most existing approaches directly transform video into fixed-grid and patch-wise tokens, which exhibit limited versatility. Spatially, uniformly allocating a fixed number of tokens often leads to over-encoding in low-information regions. Temporally, reducing redundancy remains challenging without explicitly distinguishing between static and dynamic content. In this work, we propose the Gaussian Video Transformer (GVT), a versatile video tokenizer built upon a generative 2D Gaussian Splatting (2DGS) strategy. We first extract latent rigid features from a video clip and represent them with a set of 2D Gaussians generated by our proposed Spatio-Temporal Gaussian Embedding (STGE) mechanism in a feed-forward manner. Such generative 2D Gaussians not only enhance spatial adaptability by assigning higher (resp., lower) rendering weights to regions with higher (resp., lower) information content during rasterization, but also improve generalization by avoiding per-video optimization.To enhance the temporal versatility, we introduce a Gaussian Set Partitioning (GSP) strategy that separates the 2D Gaussians into static and dynamic sets, which explicitly model static content shared across different time-steps and dynamic content specific to each time-step, enabling a compact representation.We primarily evaluate GVT on the video reconstruction, while also assessing its performance on action recognition and compression using the UCF101, Kinetics, and DAVIS datasets. Extensive experiments demonstrate that GVT achieves a state-of-the-art video reconstruction quality, outperforms the baseline MAGVIT-v2 in action recognition, and delivers comparable compression performance.

</details>


### [13] [UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning](https://arxiv.org/abs/2508.11196)
*Jiajin Guan,Haibo Mei,Bonan Zhang,Dan Liu,Yuanshuang Fu,Yue Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级视觉语言模型UAV-VL-R1，专为无人机航拍图像设计，通过混合训练方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 通用视觉语言模型在无人机航拍图像任务中表现不佳，因其高分辨率、复杂空间语义和实时性要求。

Method: 采用监督微调（SFT）和多阶段强化学习（RL）的混合方法，结合GRPO算法提升结构化推理能力。

Result: UAV-VL-R1在零样本任务中比基线模型准确率提升48.17%，且内存占用低，适合实时部署。

Conclusion: UAV-VL-R1在航拍图像任务中表现出色，GRPO算法弥补了SFT在数学任务中的局限性。

Abstract: Recent advances in vision-language models (VLMs) have demonstrated strong generalization in natural image tasks. However, their performance often degrades on unmanned aerial vehicle (UAV)-based aerial imagery, which features high resolution, complex spatial semantics, and strict real-time constraints. These challenges limit the applicability of general-purpose VLMs to structured aerial reasoning tasks. To address these challenges, we propose UAV-VL-R1, a lightweight VLM explicitly designed for aerial visual reasoning. It is trained using a hybrid method that combines supervised fine-tuning (SFT) and multi-stage reinforcement learning (RL). We leverage the group relative policy optimization (GRPO) algorithm to promote structured and interpretable reasoning through rule-guided rewards and intra-group policy alignment. To support model training and evaluation, we introduce a high-resolution visual question answering dataset named HRVQA-VL, which consists of 50,019 annotated samples covering eight UAV-relevant reasoning tasks, including object counting, transportation recognition, and spatial scene inference. Experimental results show that UAV-VL-R1 achieves a 48.17% higher zero-shot accuracy than the Qwen2-VL-2B-Instruct baseline and even outperforms its 72B-scale variant, which is 36x larger, on multiple tasks. Ablation studies reveal that while SFT improves semantic alignment, it may reduce reasoning diversity in mathematical tasks. GRPO-based RL compensates for this limitation by enhancing logical flexibility and the robustness of inference. Additionally, UAV-VL-R1 requires only 3.9GB of memory under FP16 inference and can be quantized to 2.5GB with INT8, supporting real-time deployment on resource-constrained UAV platforms.

</details>


### [14] [TimeMachine: Fine-Grained Facial Age Editing with Identity Preservation](https://arxiv.org/abs/2508.11284)
*Yilin Mi,Qixin Yan,Zheng-Peng Duan,Chunle Guo,Hubery Yin,Hao Liu,Chen Li,Chongyi Li*

Main category: cs.CV

TL;DR: TimeMachine是一种基于扩散模型的框架，通过注入高精度年龄信息实现细粒度年龄编辑，同时保持身份特征不变。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在面部图像编辑中难以实现细粒度年龄编辑且保持身份特征不变，因此提出TimeMachine解决这一问题。

Method: 采用多交叉注意力模块分离年龄和身份特征，并引入Age Classifier Guidance模块在潜在空间预测年龄，同时构建HFFA数据集支持训练。

Result: 实验表明TimeMachine在细粒度年龄编辑和身份一致性保持方面达到最先进性能。

Conclusion: TimeMachine通过创新的模块设计和高质量数据集，成功实现了高精度的年龄编辑，同时保持了身份特征。

Abstract: With the advancement of generative models, facial image editing has made significant progress. However, achieving fine-grained age editing while preserving personal identity remains a challenging task.In this paper, we propose TimeMachine, a novel diffusion-based framework that achieves accurate age editing while keeping identity features unchanged. To enable fine-grained age editing, we inject high-precision age information into the multi-cross attention module, which explicitly separates age-related and identity-related features. This design facilitates more accurate disentanglement of age attributes, thereby allowing precise and controllable manipulation of facial aging.Furthermore, we propose an Age Classifier Guidance (ACG) module that predicts age directly in the latent space, instead of performing denoising image reconstruction during training. By employing a lightweight module to incorporate age constraints, this design enhances age editing accuracy by modest increasing training cost. Additionally, to address the lack of large-scale, high-quality facial age datasets, we construct a HFFA dataset (High-quality Fine-grained Facial-Age dataset) which contains one million high-resolution images labeled with identity and facial attributes. Experimental results demonstrate that TimeMachine achieves state-of-the-art performance in fine-grained age editing while preserving identity consistency.

</details>


### [15] [Denoise-then-Retrieve: Text-Conditioned Video Denoising for Video Moment Retrieval](https://arxiv.org/abs/2508.11313)
*Weijia Liu,Jiuxin Cao,Bo Miao,Zhiheng Fu,Xuelin Zhu,Jiawei Ge,Bo Liu,Mehwish Nasim,Ajmal Mian*

Main category: cs.CV

TL;DR: 提出了一种去噪后检索范式（DRNet），通过过滤文本无关视频片段提升视频时刻检索（VMR）性能。


<details>
  <summary>Details</summary>
Motivation: 现有VMR方法编码所有视频片段（包括无关内容），破坏多模态对齐并阻碍优化。

Method: 引入DRNet，包含文本条件去噪（TCD）和文本重构反馈（TRF）模块，动态识别噪声片段并净化多模态表示。

Result: 在Charades-STA和QVHighlights数据集上超越现有方法。

Conclusion: 去噪后检索范式可提升VMR性能，并兼容现有模型。

Abstract: Current text-driven Video Moment Retrieval (VMR) methods encode all video clips, including irrelevant ones, disrupting multimodal alignment and hindering optimization. To this end, we propose a denoise-then-retrieve paradigm that explicitly filters text-irrelevant clips from videos and then retrieves the target moment using purified multimodal representations. Following this paradigm, we introduce the Denoise-then-Retrieve Network (DRNet), comprising Text-Conditioned Denoising (TCD) and Text-Reconstruction Feedback (TRF) modules. TCD integrates cross-attention and structured state space blocks to dynamically identify noisy clips and produce a noise mask to purify multimodal video representations. TRF further distills a single query embedding from purified video representations and aligns it with the text embedding, serving as auxiliary supervision for denoising during training. Finally, we perform conditional retrieval using text embeddings on purified video representations for accurate VMR. Experiments on Charades-STA and QVHighlights demonstrate that our approach surpasses state-of-the-art methods on all metrics. Furthermore, our denoise-then-retrieve paradigm is adaptable and can be seamlessly integrated into advanced VMR models to boost performance.

</details>


### [16] [Noise Matters: Optimizing Matching Noise for Diffusion Classifiers](https://arxiv.org/abs/2508.11330)
*Yanghao Wang,Long Chen*

Main category: cs.CV

TL;DR: 论文提出NoOp方法，通过优化噪声解决扩散分类器（DC）的噪声不稳定性问题，提升分类速度和性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散分类器（DC）因噪声不稳定性需大量噪声采样，导致分类速度慢。研究发现存在“好噪声”可缓解此问题。

Method: 提出NoOp方法，通过频率匹配和空间匹配原则优化噪声：1）优化数据集特定噪声；2）训练Meta-Network生成图像特定噪声偏移。

Result: 在多个数据集上的实验验证了NoOp的有效性，显著提升了分类性能。

Conclusion: NoOp通过优化噪声解决了DC的噪声不稳定性问题，为高效稳定的图像分类提供了新思路。

Abstract: Although today's pretrained discriminative vision-language models (e.g., CLIP) have demonstrated strong perception abilities, such as zero-shot image classification, they also suffer from the bag-of-words problem and spurious bias. To mitigate these problems, some pioneering studies leverage powerful generative models (e.g., pretrained diffusion models) to realize generalizable image classification, dubbed Diffusion Classifier (DC). Specifically, by randomly sampling a Gaussian noise, DC utilizes the differences of denoising effects with different category conditions to classify categories. Unfortunately, an inherent and notorious weakness of existing DCs is noise instability: different random sampled noises lead to significant performance changes. To achieve stable classification performance, existing DCs always ensemble the results of hundreds of sampled noises, which significantly reduces the classification speed. To this end, we firstly explore the role of noise in DC, and conclude that: there are some ``good noises'' that can relieve the instability. Meanwhile, we argue that these good noises should meet two principles: Frequency Matching and Spatial Matching. Regarding both principles, we propose a novel Noise Optimization method to learn matching (i.e., good) noise for DCs: NoOp. For frequency matching, NoOp first optimizes a dataset-specific noise: Given a dataset and a timestep t, optimize one randomly initialized parameterized noise. For Spatial Matching, NoOp trains a Meta-Network that adopts an image as input and outputs image-specific noise offset. The sum of optimized noise and noise offset will be used in DC to replace random noise. Extensive ablations on various datasets demonstrated the effectiveness of NoOp.

</details>


### [17] [GANDiff FR: Hybrid GAN Diffusion Synthesis for Causal Bias Attribution in Face Recognition](https://arxiv.org/abs/2508.11334)
*Md Asgor Hossain Reaj,Rajan Das Gupta,Md Yeasin Rahat,Nafiz Fahad,Md Jawadul Hasan,Tze Hui Liew*

Main category: cs.CV

TL;DR: GANDiff FR是一个合成框架，通过精确控制人口统计和环境因素来测量、解释和减少偏见，结合StyleGAN3和扩散模型实现细粒度属性控制，并验证了合成数据的真实性和偏见驱动因素。


<details>
  <summary>Details</summary>
Motivation: 旨在提供一个可重复、可量化的方法来测量和减少人脸识别系统中的偏见，同时满足监管要求（如欧盟AI法案）。

Method: 结合StyleGAN3（身份保留生成）和扩散模型（属性控制），生成10,000张人口平衡的人脸数据，并通过自动检测和人工评审验证真实性。

Result: AdaFace将组间TPR差异减少了60%，光照因素占剩余偏见的42%。合成数据在跨数据集评估中表现出强泛化能力（r=0.85）。

Conclusion: GANDiff FR为公平性审计提供了一个可重复、符合监管标准的框架，尽管计算开销较高，但能生成更多属性条件变体。

Abstract: We introduce GANDiff FR, the first synthetic framework that precisely controls demographic and environmental factors to measure, explain, and reduce bias with reproducible rigor. GANDiff FR unifies StyleGAN3-based identity-preserving generation with diffusion-based attribute control, enabling fine-grained manipulation of pose around 30 degrees, illumination (four directions), and expression (five levels) under ceteris paribus conditions. We synthesize 10,000 demographically balanced faces across five cohorts validated for realism via automated detection (98.2%) and human review (89%) to isolate and quantify bias drivers. Benchmarking ArcFace, CosFace, and AdaFace under matched operating points shows AdaFace reduces inter-group TPR disparity by 60% (2.5% vs. 6.3%), with illumination accounting for 42% of residual bias. Cross-dataset evaluation on RFW, BUPT, and CASIA WebFace confirms strong synthetic-to-real transfer (r 0.85). Despite around 20% computational overhead relative to pure GANs, GANDiff FR yields three times more attribute-conditioned variants, establishing a reproducible, regulation-aligned (EU AI Act) standard for fairness auditing. Code and data are released to support transparent, scalable bias evaluation.

</details>


### [18] [RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator](https://arxiv.org/abs/2508.11409)
*Zhiming Liu,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: RMFAT是一种轻量级循环框架，用于高效恢复受大气湍流影响的视频，显著降低计算负担并提升实时性能。


<details>
  <summary>Details</summary>
Motivation: 大气湍流导致视频质量下降，现有方法计算成本高，难以实时部署。

Method: 采用轻量级循环框架，仅需两帧输入，结合多尺度特征编码和解码以及时间扭曲模块。

Result: 在清晰度恢复和推理速度上优于现有方法（SSIM提升9%，运行时间减少四倍）。

Conclusion: RMFAT适用于实时大气湍流抑制任务，性能显著提升。

Abstract: Atmospheric turbulence severely degrades video quality by introducing distortions such as geometric warping, blur, and temporal flickering, posing significant challenges to both visual clarity and temporal consistency. Current state-of-the-art methods are based on transformer and 3D architectures and require multi-frame input, but their large computational cost and memory usage limit real-time deployment, especially in resource-constrained scenarios. In this work, we propose RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator, designed for efficient and temporally consistent video restoration under AT conditions. RMFAT adopts a lightweight recurrent framework that restores each frame using only two inputs at a time, significantly reducing temporal window size and computational burden. It further integrates multi-scale feature encoding and decoding with temporal warping modules at both encoder and decoder stages to enhance spatial detail and temporal coherence. Extensive experiments on synthetic and real-world atmospheric turbulence datasets demonstrate that RMFAT not only outperforms existing methods in terms of clarity restoration (with nearly a 9\% improvement in SSIM) but also achieves significantly improved inference speed (more than a fourfold reduction in runtime), making it particularly suitable for real-time atmospheric turbulence suppression tasks.

</details>


### [19] [Remove360: Benchmarking Residuals After Object Removal in 3D Gaussian Splatting](https://arxiv.org/abs/2508.11431)
*Simona Kocour,Assia Benbihi,Torsten Sattler*

Main category: cs.CV

TL;DR: 论文提出了一个评估框架和数据集Remove360，用于衡量3D高斯泼溅中物体移除后遗留的语义痕迹（语义残留），揭示了当前技术在真实场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究物体移除后语义信息的残留问题对隐私保护的3D重建和可编辑场景表示至关重要。

Method: 通过创建Remove360数据集和评估框架，分析物体移除前后的RGB图像和对象级掩码，评估语义残留情况。

Result: 实验表明，当前方法在视觉几何缺失的情况下仍能保留语义信息，但存在显著局限性。

Conclusion: 当前3D物体移除技术需改进以应对真实世界的复杂性，Remove360为未来研究提供了基准。

Abstract: Understanding what semantic information persists after object removal is critical for privacy-preserving 3D reconstruction and editable scene representations. In this work, we introduce a novel benchmark and evaluation framework to measure semantic residuals, the unintended semantic traces left behind, after object removal in 3D Gaussian Splatting. We conduct experiments across a diverse set of indoor and outdoor scenes, showing that current methods can preserve semantic information despite the absence of visual geometry. We also release Remove360, a dataset of pre/post-removal RGB images and object-level masks captured in real-world environments. While prior datasets have focused on isolated object instances, Remove360 covers a broader and more complex range of indoor and outdoor scenes, enabling evaluation of object removal in the context of full-scene representations. Given ground truth images of a scene before and after object removal, we assess whether we can truly eliminate semantic presence, and if downstream models can still infer what was removed. Our findings reveal critical limitations in current 3D object removal techniques and underscore the need for more robust solutions capable of handling real-world complexity. The evaluation framework is available at github.com/spatial-intelligence-ai/Remove360.git. Data are available at huggingface.co/datasets/simkoc/Remove360.

</details>


### [20] [MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation](https://arxiv.org/abs/2508.11433)
*Qian Liang,Yujia Wu,Kuncheng Li,Jiwei Wei,Shiyuan He,Jinyu Guo,Ning Xie*

Main category: cs.CV

TL;DR: MM-R1框架通过跨模态思维链推理策略，解锁统一MLLMs在个性化图像生成中的潜力，实现零样本高保真生成。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs方法通常针对特定主题，需大量数据微调，限制了可扩展性。

Method: 采用跨模态思维链推理（X-CoT）和分组奖励近端策略优化（GRPO），结合视觉推理与生成过程。

Result: 实验表明MM-R1能零样本生成高保真且文本对齐的个性化图像。

Conclusion: MM-R1为统一MLLMs的个性化图像生成提供了高效、可扩展的解决方案。

Abstract: Multimodal Large Language Models (MLLMs) with unified architectures excel across a wide range of vision-language tasks, yet aligning them with personalized image generation remains a significant challenge. Existing methods for MLLMs are frequently subject-specific, demanding a data-intensive fine-tuning process for every new subject, which limits their scalability. In this paper, we introduce MM-R1, a framework that integrates a cross-modal Chain-of-Thought (X-CoT) reasoning strategy to unlock the inherent potential of unified MLLMs for personalized image generation. Specifically, we structure personalization as an integrated visual reasoning and generation process: (1) grounding subject concepts by interpreting and understanding user-provided images and contextual cues, and (2) generating personalized images conditioned on both the extracted subject representations and user prompts. To further enhance the reasoning capability, we adopt Grouped Reward Proximal Policy Optimization (GRPO) to explicitly align the generation. Experiments demonstrate that MM-R1 unleashes the personalization capability of unified MLLMs to generate images with high subject fidelity and strong text alignment in a zero-shot manner.

</details>


### [21] [CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models](https://arxiv.org/abs/2508.11484)
*Xiaoxue Wu,Bingjie Gao,Yu Qiao,Yaohui Wang,Xinyuan Chen*

Main category: cs.CV

TL;DR: CineTrans是一个用于生成具有电影风格过渡的多镜头视频的新框架，通过构建数据集和注意力机制实现稳定过渡。


<details>
  <summary>Details</summary>
Motivation: 尽管视频合成技术取得了显著进展，但多镜头视频生成的研究仍处于初级阶段，现有模型的过渡能力有限且不稳定。

Method: 构建多镜头视频文本数据集Cine250K，利用扩散模型的注意力图设计基于掩码的控制机制，实现任意位置的过渡。

Result: CineTrans生成的视频具有电影编辑风格，避免了不稳定过渡或简单拼接，并在实验中显著优于现有基线。

Conclusion: CineTrans通过创新的数据集和控制机制，显著提升了多镜头视频生成的质量和稳定性。

Abstract: Despite significant advances in video synthesis, research into multi-shot video generation remains in its infancy. Even with scaled-up models and massive datasets, the shot transition capabilities remain rudimentary and unstable, largely confining generated videos to single-shot sequences. In this work, we introduce CineTrans, a novel framework for generating coherent multi-shot videos with cinematic, film-style transitions. To facilitate insights into the film editing style, we construct a multi-shot video-text dataset Cine250K with detailed shot annotations. Furthermore, our analysis of existing video diffusion models uncovers a correspondence between attention maps in the diffusion model and shot boundaries, which we leverage to design a mask-based control mechanism that enables transitions at arbitrary positions and transfers effectively in a training-free setting. After fine-tuning on our dataset with the mask mechanism, CineTrans produces cinematic multi-shot sequences while adhering to the film editing style, avoiding unstable transitions or naive concatenations. Finally, we propose specialized evaluation metrics for transition control, temporal consistency and overall quality, and demonstrate through extensive experiments that CineTrans significantly outperforms existing baselines across all criteria.

</details>


### [22] [Reinforcing Video Reasoning Segmentation to Think Before It Segments](https://arxiv.org/abs/2508.11538)
*Sitong Gong,Lu Zhang,Yunzhi Zhuge,Xu Jia,Pingping Zhang,Huchuan Lu*

Main category: cs.CV

TL;DR: Veason-R1是一种专用于视频推理分割（VRS）的LVLM，通过GRPO和CoT初始化训练，显著提升了时空推理能力，并在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在推理时缺乏可解释性且性能不佳，主要由于时空推理能力不足。

Method: 采用GRPO和CoT初始化训练模型，结合高质量CoT数据，优化推理链和奖励机制。

Result: 在ReVOS和ReasonVOS等基准测试中表现优异，性能提升显著（如+1.3 J&F和+10.0 J&F），且抗幻觉能力增强（+8.8 R）。

Conclusion: Veason-R1通过结构化推理和优化方法，显著提升了VRS任务的性能，具有广泛的应用潜力。

Abstract: Video reasoning segmentation (VRS) endeavors to delineate referred objects in videos guided by implicit instructions that encapsulate human intent and temporal logic. Previous approaches leverage large vision language models (LVLMs) to encode object semantics into <SEG> tokens for mask prediction. However, this paradigm suffers from limited interpretability during inference and suboptimal performance due to inadequate spatiotemporal reasoning. Drawing inspiration from seminal breakthroughs in reinforcement learning, we introduce Veason-R1, a specialized LVLM for VRS that emphasizes structured reasoning in segmentation. Veason-R1 is trained through Group Relative Policy Optimization (GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, we curate high-quality CoT training data to instill structured reasoning trajectories, bridging video-level semantics and frame-level spatial grounding, yielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPO fine-tuning encourages efficient exploration of the reasoning space by optimizing reasoning chains. To this end, we incorporate a holistic reward mechanism that synergistically enhances spatial alignment and temporal consistency, bolstering keyframe localization and fine-grained grounding. Comprehensive empirical evaluations demonstrate that Veason-R1 achieves state-of-the-art performance on multiple benchmarks, surpassing prior art by significant margins (e.g., +1.3 J &F in ReVOS and +10.0 J &F in ReasonVOS), while exhibiting robustness to hallucinations (+8.8 R). Our code and model weights will be available at Veason-R1.

</details>


### [23] [Training-Free Anomaly Generation via Dual-Attention Enhancement in Diffusion Model](https://arxiv.org/abs/2508.11550)
*Zuo Zuo,Jiahao Dong,Yanyun Qu,Zongze Wu*

Main category: cs.CV

TL;DR: 提出了一种基于Stable Diffusion的训练免费异常生成框架AAG，通过改进交叉注意力和自注意力机制生成高质量异常图像，提升下游异常检测任务性能。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测中数据稀缺问题突出，现有异常生成方法缺乏逼真性或需额外训练数据。

Method: AAG框架利用Stable Diffusion，结合Cross-Attention Enhancement（CAE）和Self-Attention Enhancement（SAE）机制，通过文本提示和掩码生成逼真异常图像。

Result: 在MVTec AD和VisA数据集上验证了AAG的有效性，生成的异常图像显著提升了异常检测任务性能。

Conclusion: AAG为工业异常检测提供了一种高效、无需训练的异常生成解决方案，具有广泛的应用潜力。

Abstract: Industrial anomaly detection (AD) plays a significant role in manufacturing where a long-standing challenge is data scarcity. A growing body of works have emerged to address insufficient anomaly data via anomaly generation. However, these anomaly generation methods suffer from lack of fidelity or need to be trained with extra data. To this end, we propose a training-free anomaly generation framework dubbed AAG, which is based on Stable Diffusion (SD)'s strong generation ability for effective anomaly image generation. Given a normal image, mask and a simple text prompt, AAG can generate realistic and natural anomalies in the specific regions and simultaneously keep contents in other regions unchanged. In particular, we propose Cross-Attention Enhancement (CAE) to re-engineer the cross-attention mechanism within Stable Diffusion based on the given mask. CAE increases the similarity between visual tokens in specific regions and text embeddings, which guides these generated visual tokens in accordance with the text description. Besides, generated anomalies need to be more natural and plausible with object in given image. We propose Self-Attention Enhancement (SAE) which improves similarity between each normal visual token and anomaly visual tokens. SAE ensures that generated anomalies are coherent with original pattern. Extensive experiments on MVTec AD and VisA datasets demonstrate effectiveness of AAG in anomaly generation and its utility. Furthermore, anomaly images generated by AAG can bolster performance of various downstream anomaly inspection tasks.

</details>


### [24] [CoreEditor: Consistent 3D Editing via Correspondence-constrained Diffusion](https://arxiv.org/abs/2508.11603)
*Zhe Zhu,Honghua Chen,Peng Li,Mingqiang Wei*

Main category: cs.CV

TL;DR: CoreEditor通过引入对应约束注意力机制和语义相似性，解决了多视图编辑中的一致性问题，并提供了选择性编辑流程，显著提升了3D编辑质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多视图信息交换中缺乏显式控制，导致编辑不一致和细节模糊。

Method: 提出CoreEditor框架，采用对应约束注意力机制和语义相似性建模，设计选择性编辑流程。

Result: 实验表明，CoreEditor能生成高质量、3D一致的编辑结果，细节更清晰，优于现有方法。

Conclusion: CoreEditor通过多视图一致性控制和用户交互设计，显著提升了文本驱动的3D编辑效果。

Abstract: Text-driven 3D editing seeks to modify 3D scenes according to textual descriptions, and most existing approaches tackle this by adapting pre-trained 2D image editors to multi-view inputs. However, without explicit control over multi-view information exchange, they often fail to maintain cross-view consistency, leading to insufficient edits and blurry details. We introduce CoreEditor, a novel framework for consistent text-to-3D editing. The key innovation is a correspondence-constrained attention mechanism that enforces precise interactions between pixels expected to remain consistent throughout the diffusion denoising process. Beyond relying solely on geometric alignment, we further incorporate semantic similarity estimated during denoising, enabling more reliable correspondence modeling and robust multi-view editing. In addition, we design a selective editing pipeline that allows users to choose preferred results from multiple candidates, offering greater flexibility and user control. Extensive experiments show that CoreEditor produces high-quality, 3D-consistent edits with sharper details, significantly outperforming prior methods.

</details>


### [25] [LoRAtorio: An intrinsic approach to LoRA Skill Composition](https://arxiv.org/abs/2508.11624)
*Niki Foteinopoulou,Ignas Budvytis,Stephan Liwicki*

Main category: cs.CV

TL;DR: LoRAtorio是一个无需训练的多LoRA组合框架，通过利用模型内在行为解决现有方法在多LoRA组合中的局限性，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多LoRA组合中表现不佳，尤其是在开放环境下，无法预先知道所需技能的数量和性质。

Method: 在潜在空间中划分空间块，计算每个块的预测噪声与基础模型的余弦相似度，构建空间感知权重矩阵，加权聚合LoRA输出。

Result: LoRAtorio在ClipScore上提升1.3%，在GPT-4V评估中胜率72.43%，且适用于多种潜在扩散模型。

Conclusion: LoRAtorio通过动态模块选择和加权聚合，在多LoRA组合中实现了最先进的性能。

Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted technique in text-to-image diffusion models, enabling the personalisation of visual concepts such as characters, styles, and objects. However, existing approaches struggle to effectively compose multiple LoRA adapters, particularly in open-ended settings where the number and nature of required skills are not known in advance. In this work, we present LoRAtorio, a novel train-free framework for multi-LoRA composition that leverages intrinsic model behaviour. Our method is motivated by two key observations: (1) LoRA adapters trained on narrow domains produce denoised outputs that diverge from the base model, and (2) when operating out-of-distribution, LoRA outputs show behaviour closer to the base model than when conditioned in distribution. The balance between these two observations allows for exceptional performance in the single LoRA scenario, which nevertheless deteriorates when multiple LoRAs are loaded. Our method operates in the latent space by dividing it into spatial patches and computing cosine similarity between each patch's predicted noise and that of the base model. These similarities are used to construct a spatially-aware weight matrix, which guides a weighted aggregation of LoRA outputs. To address domain drift, we further propose a modification to classifier-free guidance that incorporates the base model's unconditional score into the composition. We extend this formulation to a dynamic module selection setting, enabling inference-time selection of relevant LoRA adapters from a large pool. LoRAtorio achieves state-of-the-art performance, showing up to a 1.3% improvement in ClipScore and a 72.43% win rate in GPT-4V pairwise evaluations, and generalises effectively to multiple latent diffusion models.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [26] [Efficient Image-to-Image Schrödinger Bridge for CT Field of View Extension](https://arxiv.org/abs/2508.11211)
*Zhenhao Li,Long Yang,Xiaojie Yin,Haijun Yu,Jiazhou Wang,Hongbin Han,Weigang Hu,Yixing Huang*

Main category: eess.IV

TL;DR: 提出了一种基于Schrödinger Bridge扩散模型（I²SB）的高效CT视野扩展框架，显著提升了重建速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统CT扫描在视野受限时会产生截断数据，导致重建不完整和伪影，现有方法难以解决。扩散模型虽先进但计算成本高且推理慢。

Method: 采用I²SB扩散模型，直接学习有限视野与扩展视野图像的随机映射，避免传统扩散模型从纯噪声合成的低效问题。

Result: I²SB在模拟和真实数据上均表现优异，RMSE分别为49.8HU和152.0HU，推理速度达0.19秒/切片，比现有方法快700倍以上。

Conclusion: I²SB在精度和效率上的优势使其适合实时或临床部署，解决了传统扩散模型的计算瓶颈。

Abstract: Computed tomography (CT) is a cornerstone imaging modality for non-invasive, high-resolution visualization of internal anatomical structures. However, when the scanned object exceeds the scanner's field of view (FOV), projection data are truncated, resulting in incomplete reconstructions and pronounced artifacts near FOV boundaries. Conventional reconstruction algorithms struggle to recover accurate anatomy from such data, limiting clinical reliability. Deep learning approaches have been explored for FOV extension, with diffusion generative models representing the latest advances in image synthesis. Yet, conventional diffusion models are computationally demanding and slow at inference due to their iterative sampling process. To address these limitations, we propose an efficient CT FOV extension framework based on the image-to-image Schr\"odinger Bridge (I$^2$SB) diffusion model. Unlike traditional diffusion models that synthesize images from pure Gaussian noise, I$^2$SB learns a direct stochastic mapping between paired limited-FOV and extended-FOV images. This direct correspondence yields a more interpretable and traceable generative process, enhancing anatomical consistency and structural fidelity in reconstructions. I$^2$SB achieves superior quantitative performance, with root-mean-square error (RMSE) values of 49.8\,HU on simulated noisy data and 152.0HU on real data, outperforming state-of-the-art diffusion models such as conditional denoising diffusion probabilistic models (cDDPM) and patch-based diffusion methods. Moreover, its one-step inference enables reconstruction in just 0.19s per 2D slice, representing over a 700-fold speedup compared to cDDPM (135s) and surpassing diffusionGAN (0.58s), the second fastest. This combination of accuracy and efficiency makes I$^2$SB highly suitable for real-time or clinical deployment.

</details>


### [27] [AnatoMaskGAN: GNN-Driven Slice Feature Fusion and Noise Augmentation for Medical Semantic Image Synthesis](https://arxiv.org/abs/2508.11375)
*Zonglin Wu,Yule Xue,Qianxiang Hu,Yaoyao Feng,Yuqi Ma,Shanxiong Chen*

Main category: eess.IV

TL;DR: AnatoMaskGAN通过嵌入切片相关空间特征和引入多样化的图像增强策略，提升了复杂医学图像的生成质量和空间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有GAN方法在复杂医学扫描中缺乏空间一致性和多样性，限制了数据增强和分析的效果。

Method: 提出AnatoMaskGAN框架，包括GNN切片特征融合模块、三维空间噪声注入策略和灰度纹理分类器。

Result: 在L2R-OASIS和L2R-Abdomen CT数据集上，PSNR和SSIM显著提升，优于现有方法。

Conclusion: AnatoMaskGAN的核心设计均对提升重建精度和感知质量有显著贡献。

Abstract: Medical semantic-mask synthesis boosts data augmentation and analysis, yet most GAN-based approaches still produce one-to-one images and lack spatial consistency in complex scans. To address this, we propose AnatoMaskGAN, a novel synthesis framework that embeds slice-related spatial features to precisely aggregate inter-slice contextual dependencies, introduces diverse image-augmentation strategies, and optimizes deep feature learning to improve performance on complex medical images. Specifically, we design a GNN-based strongly correlated slice-feature fusion module to model spatial relationships between slices and integrate contextual information from neighboring slices, thereby capturing anatomical details more comprehensively; we introduce a three-dimensional spatial noise-injection strategy that weights and fuses spatial features with noise to enhance modeling of structural diversity; and we incorporate a grayscale-texture classifier to optimize grayscale distribution and texture representation during generation. Extensive experiments on the public L2R-OASIS and L2R-Abdomen CT datasets show that AnatoMaskGAN raises PSNR on L2R-OASIS to 26.50 dB (0.43 dB higher than the current state of the art) and achieves an SSIM of 0.8602 on L2R-Abdomen CT--a 0.48 percentage-point gain over the best model, demonstrating its superiority in reconstruction accuracy and perceptual quality. Ablation studies that successively remove the slice-feature fusion module, spatial 3D noise-injection strategy, and grayscale-texture classifier reveal that each component contributes significantly to PSNR, SSIM, and LPIPS, further confirming the independent value of each core design in enhancing reconstruction accuracy and perceptual quality.

</details>


### [28] [LKFMixer: Exploring Large Kernel Feature For Efficient Image Super-Resolution](https://arxiv.org/abs/2508.11391)
*Yinggan Tang,Quanwei Hu*

Main category: eess.IV

TL;DR: LKFMixer是一种纯卷积神经网络模型，通过大卷积核模拟自注意力的非局部特征捕捉能力，解决了Transformer计算量大的问题，并在图像超分辨率任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 自注意力在Transformer中的成功表明非局部信息对图像超分辨率很重要，但其计算量大，难以实现轻量化模型。

Method: 提出LKFMixer模型，使用31x31大卷积核模拟自注意力，通过坐标分解减少参数和计算量，并设计了空间特征调制块（SFMB）和特征选择块（FSB）增强特征处理。

Result: LKFMixer在超分辨率性能和重建质量上优于其他SOTA方法，例如在Manga109数据集上比SwinIR-light PSNR提升0.6dB，推理速度快5倍。

Conclusion: LKFMixer通过大卷积核和优化模块设计，有效平衡了非局部特征捕捉和计算效率，为轻量化超分辨率模型提供了新思路。

Abstract: The success of self-attention (SA) in Transformer demonstrates the importance of non-local information to image super-resolution (SR), but the huge computing power required makes it difficult to implement lightweight models. To solve this problem, we propose a pure convolutional neural network (CNN) model, LKFMixer, which utilizes large convolutional kernel to simulate the ability of self-attention to capture non-local features. Specifically, we increase the kernel size to 31 to obtain the larger receptive field as possible, and reduce the parameters and computations by coordinate decomposition. Meanwhile, a spatial feature modulation block (SFMB) is designed to enhance the focus of feature information on both spatial and channel dimension. In addition, by introducing feature selection block (FSB), the model can adaptively adjust the weights between local features and non-local features. Extensive experiments show that the proposed LKFMixer family outperform other state-of-the-art (SOTA) methods in terms of SR performance and reconstruction quality. In particular, compared with SwinIR-light on Manga109 dataset, LKFMixer-L achieves 0.6dB PSNR improvement at $\times$4 scale, while the inference speed is $\times$5 times faster. The code is available at https://github.com/Supereeeee/LKFMixer.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [29] [GenFlowRL: Shaping Rewards with Generative Object-Centric Flow in Visual Reinforcement Learning](https://arxiv.org/abs/2508.11049)
*Kelin Yu,Sheng Zhang,Harshit Soora,Furong Huang,Heng Huang,Pratap Tokekar,Ruohan Gao*

Main category: cs.RO

TL;DR: GenFlowRL通过从多样化的跨体现数据集中提取生成的流来塑造奖励，从而学习通用且鲁棒的策略。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型依赖生成数据质量且缺乏环境反馈，难以处理精细操作；视频强化学习受限于视频生成的不确定性和大规模数据集收集的挑战。

Method: 提出GenFlowRL，利用从跨体现数据集训练的生成流提取低维、以对象为中心的特征，塑造奖励以学习策略。

Result: 在10个操作任务的仿真和真实跨体现评估中，GenFlowRL表现优异，能有效利用生成的以对象为中心的流特征。

Conclusion: GenFlowRL通过生成流提取的特征，在多样化和挑战性场景中实现了卓越性能。

Abstract: Recent advances have shown that video generation models can enhance robot learning by deriving effective robot actions through inverse dynamics. However, these methods heavily depend on the quality of generated data and struggle with fine-grained manipulation due to the lack of environment feedback. While video-based reinforcement learning improves policy robustness, it remains constrained by the uncertainty of video generation and the challenges of collecting large-scale robot datasets for training diffusion models. To address these limitations, we propose GenFlowRL, which derives shaped rewards from generated flow trained from diverse cross-embodiment datasets. This enables learning generalizable and robust policies from diverse demonstrations using low-dimensional, object-centric features. Experiments on 10 manipulation tasks, both in simulation and real-world cross-embodiment evaluations, demonstrate that GenFlowRL effectively leverages manipulation features extracted from generated object-centric flow, consistently achieving superior performance across diverse and challenging scenarios. Our Project Page: https://colinyu1.github.io/genflowrl

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [30] [Fluid Dynamics and Domain Reconstruction from Noisy Flow Images Using Physics-Informed Neural Networks and Quasi-Conformal Mapping](https://arxiv.org/abs/2508.11216)
*Han Zhang,Xue-Cheng Tai,Jean-Michel Morel,Raymond H. Chan*

Main category: math.NA

TL;DR: 该论文提出了一种基于物理约束的优化方法，用于去噪血流图像，通过交替求解流体和几何子问题，实现了高质量的图像重建。


<details>
  <summary>Details</summary>
Motivation: 血流成像在医学诊断和治疗规划中至关重要，但高质量图像获取仍具挑战性。本文旨在解决因短采集时间或设备误差导致的图像噪声问题。

Method: 将问题建模为优化问题，分解为流体子问题（使用物理信息神经网络重建速度场）和几何子问题（优化准共形映射推断流动区域），交替求解。

Result: 实验验证了方法在合成和真实血流数据上的有效性和鲁棒性，并通过消融研究评估了关键超参数的影响。

Conclusion: 该方法能够高质量重建血流图像，为医学应用提供了可靠工具。

Abstract: Blood flow imaging provides important information for hemodynamic behavior within the vascular system and plays an essential role in medical diagnosis and treatment planning. However, obtaining high-quality flow images remains a significant challenge. In this work, we address the problem of denoising flow images that may suffer from artifacts due to short acquisition times or device-induced errors. We formulate this task as an optimization problem, where the objective is to minimize the discrepancy between the modeled velocity field, constrained to satisfy the Navier-Stokes equations, and the observed noisy velocity data. To solve this problem, we decompose it into two subproblems: a fluid subproblem and a geometry subproblem. The fluid subproblem leverages a Physics-Informed Neural Network to reconstruct the velocity field from noisy observations, assuming a fixed domain. The geometry subproblem aims to infer the underlying flow region by optimizing a quasi-conformal mapping that deforms a reference domain. These two subproblems are solved in an alternating Gauss-Seidel fashion, iteratively refining both the velocity field and the domain. Upon convergence, the framework yields a high-quality reconstruction of the flow image. We validate the proposed method through experiments on synthetic flow data in a converging channel geometry under varying levels of Gaussian noise, and on real-like flow data in an aortic geometry with signal-dependent noise. The results demonstrate the effectiveness and robustness of the approach. Additionally, ablation studies are conducted to assess the influence of key hyperparameters.

</details>
