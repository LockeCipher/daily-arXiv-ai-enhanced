{"id": "2508.00428", "pdf": "https://arxiv.org/pdf/2508.00428", "abs": "https://arxiv.org/abs/2508.00428", "authors": ["Nan Xiang", "Tianyi Liang", "Haiwen Huang", "Shiqi Jiang", "Hao Huang", "Yifei Huang", "Liangyu Chen", "Changbo Wang", "Chenhui Li"], "title": "Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D Generation", "categories": ["cs.GR", "cs.HC"], "comment": "IEEE VIS VAST 2025 ACM 2012 CCS - Human-centered computing,   Visualization, Visualization design and evaluation methods", "summary": "Text-to-3D (T23D) generation has transformed digital content creation, yet remains bottlenecked by blind trial-and-error prompting processes that yield unpredictable results. While visual prompt engineering has advanced in text-to-image domains, its application to 3D generation presents unique challenges requiring multi-view consistency evaluation and spatial understanding. We present Sel3DCraft, a visual prompt engineering system for T23D that transforms unstructured exploration into a guided visual process. Our approach introduces three key innovations: a dual-branch structure combining retrieval and generation for diverse candidate exploration; a multi-view hybrid scoring approach that leverages MLLMs with innovative high-level metrics to assess 3D models with human-expert consistency; and a prompt-driven visual analytics suite that enables intuitive defect identification and refinement. Extensive testing and user studies demonstrate that Sel3DCraft surpasses other T23D systems in supporting creativity for designers.", "AI": {"tldr": "Sel3DCraft\u662f\u4e00\u4e2a\u7528\u4e8e\u6587\u672c\u52303D\u751f\u6210\u7684\u89c6\u89c9\u63d0\u793a\u5de5\u7a0b\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u89c6\u56fe\u8bc4\u5206\u548c\u89c6\u89c9\u5206\u6790\u63d0\u53473D\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u6587\u672c\u52303D\u751f\u6210\u4e2d\u76f2\u76ee\u8bd5\u9519\u63d0\u793a\u5bfc\u81f4\u7ed3\u679c\u4e0d\u53ef\u9884\u6d4b\u7684\u95ee\u9898\uff0c\u63d0\u5347\u751f\u6210\u8fc7\u7a0b\u7684\u6548\u7387\u548c\u53ef\u63a7\u6027\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u7ed3\u6784\uff08\u68c0\u7d22\u4e0e\u751f\u6210\u7ed3\u5408\uff09\u3001\u591a\u89c6\u56fe\u6df7\u5408\u8bc4\u5206\u65b9\u6cd5\uff08\u7ed3\u5408MLLMs\u548c\u9ad8\u5c42\u6b21\u6307\u6807\uff09\u4ee5\u53ca\u89c6\u89c9\u5206\u6790\u5de5\u5177\u3002", "result": "Sel3DCraft\u5728\u652f\u6301\u8bbe\u8ba1\u5e08\u521b\u9020\u529b\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u6587\u672c\u52303D\u751f\u6210\u7cfb\u7edf\u3002", "conclusion": "Sel3DCraft\u901a\u8fc7\u89c6\u89c9\u63d0\u793a\u5de5\u7a0b\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u52303D\u751f\u6210\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002"}}
{"id": "2508.00782", "pdf": "https://arxiv.org/pdf/2508.00782", "abs": "https://arxiv.org/abs/2508.00782", "authors": ["Kien T. Pham", "Yingqing He", "Yazhou Xing", "Qifeng Chen", "Long Chen"], "title": "SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.MM", "cs.SD", "eess.AS"], "comment": "The 33rd ACM Multimedia Conference (MM '25)", "summary": "Audio-driven video generation aims to synthesize realistic videos that align with input audio recordings, akin to the human ability to visualize scenes from auditory input. However, existing approaches predominantly focus on exploring semantic information, such as the classes of sounding sources present in the audio, limiting their ability to generate videos with accurate content and spatial composition. In contrast, we humans can not only naturally identify the semantic categories of sounding sources but also determine their deeply encoded spatial attributes, including locations and movement directions. This useful information can be elucidated by considering specific spatial indicators derived from the inherent physical properties of sound, such as loudness or frequency. As prior methods largely ignore this factor, we present SpA2V, the first framework explicitly exploits these spatial auditory cues from audios to generate videos with high semantic and spatial correspondence. SpA2V decomposes the generation process into two stages: 1) Audio-guided Video Planning: We meticulously adapt a state-of-the-art MLLM for a novel task of harnessing spatial and semantic cues from input audio to construct Video Scene Layouts (VSLs). This serves as an intermediate representation to bridge the gap between the audio and video modalities. 2) Layout-grounded Video Generation: We develop an efficient and effective approach to seamlessly integrate VSLs as conditional guidance into pre-trained diffusion models, enabling VSL-grounded video generation in a training-free manner. Extensive experiments demonstrate that SpA2V excels in generating realistic videos with semantic and spatial alignment to the input audios.", "AI": {"tldr": "SpA2V\u662f\u4e00\u4e2a\u5229\u7528\u97f3\u9891\u4e2d\u7684\u7a7a\u95f4\u548c\u8bed\u4e49\u7ebf\u7d22\u751f\u6210\u89c6\u9891\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u8bed\u4e49\u548c\u7a7a\u95f4\u5bf9\u5e94\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8bed\u4e49\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u97f3\u9891\u4e2d\u7684\u7a7a\u95f4\u5c5e\u6027\uff08\u5982\u4f4d\u7f6e\u548c\u8fd0\u52a8\u65b9\u5411\uff09\uff0c\u800c\u4eba\u7c7b\u53ef\u4ee5\u81ea\u7136\u8bc6\u522b\u8fd9\u4e9b\u4fe1\u606f\u3002SpA2V\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "1\uff09\u97f3\u9891\u5f15\u5bfc\u7684\u89c6\u9891\u89c4\u5212\uff1a\u5229\u7528MLLM\u4ece\u97f3\u9891\u4e2d\u63d0\u53d6\u7a7a\u95f4\u548c\u8bed\u4e49\u7ebf\u7d22\uff0c\u6784\u5efa\u89c6\u9891\u573a\u666f\u5e03\u5c40\uff08VSL\uff09\u30022\uff09\u57fa\u4e8e\u5e03\u5c40\u7684\u89c6\u9891\u751f\u6210\uff1a\u5c06VSL\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u8bad\u7ec3\u81ea\u7531\u7684\u89c6\u9891\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSpA2V\u80fd\u591f\u751f\u6210\u4e0e\u8f93\u5165\u97f3\u9891\u5728\u8bed\u4e49\u548c\u7a7a\u95f4\u4e0a\u9ad8\u5ea6\u5bf9\u9f50\u7684\u771f\u5b9e\u89c6\u9891\u3002", "conclusion": "SpA2V\u901a\u8fc7\u5229\u7528\u97f3\u9891\u4e2d\u7684\u7a7a\u95f4\u7ebf\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u771f\u5b9e\u6027\u3002"}}
{"id": "2508.00144", "pdf": "https://arxiv.org/pdf/2508.00144", "abs": "https://arxiv.org/abs/2508.00144", "authors": ["Akshat Rakheja", "Aarsh Ashdhir", "Aryan Bhattacharjee", "Vanshika Sharma"], "title": "World Consistency Score: A Unified Metric for Video Generation Quality", "categories": ["cs.CV"], "comment": "27 pages, 1 figure", "summary": "We introduce World Consistency Score (WCS), a novel unified evaluation metric for generative video models that emphasizes internal world consistency of the generated videos. WCS integrates four interpretable sub-components - object permanence, relation stability, causal compliance, and flicker penalty - each measuring a distinct aspect of temporal and physical coherence in a video. These submetrics are combined via a learned weighted formula to produce a single consistency score that aligns with human judgments. We detail the motivation for WCS in the context of existing video evaluation metrics, formalize each submetric and how it is computed with open-source tools (trackers, action recognizers, CLIP embeddings, optical flow), and describe how the weights of the WCS combination are trained using human preference data. We also outline an experimental validation blueprint: using benchmarks like VBench-2.0, EvalCrafter, and LOVE to test WCS's correlation with human evaluations, performing sensitivity analyses, and comparing WCS against established metrics (FVD, CLIPScore, VBench, FVMD). The proposed WCS offers a comprehensive and interpretable framework for evaluating video generation models on their ability to maintain a coherent \"world\" over time, addressing gaps left by prior metrics focused only on visual fidelity or prompt alignment.", "AI": {"tldr": "World Consistency Score (WCS) \u662f\u4e00\u79cd\u65b0\u7684\u751f\u6210\u89c6\u9891\u6a21\u578b\u8bc4\u4f30\u6307\u6807\uff0c\u5f3a\u8c03\u89c6\u9891\u7684\u5185\u90e8\u4e16\u754c\u4e00\u81f4\u6027\uff0c\u5305\u542b\u56db\u4e2a\u53ef\u89e3\u91ca\u7684\u5b50\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u5b66\u4e60\u6743\u91cd\u516c\u5f0f\u7efc\u5408\u8bc4\u5206\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u8bc4\u4f30\u6307\u6807\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u4fdd\u771f\u5ea6\u6216\u63d0\u793a\u5bf9\u9f50\uff0c\u5ffd\u7565\u4e86\u65f6\u95f4\u4e0e\u7269\u7406\u4e00\u81f4\u6027\uff0cWCS\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "WCS\u6574\u5408\u4e86\u56db\u4e2a\u5b50\u6307\u6807\uff08\u7269\u4f53\u6301\u4e45\u6027\u3001\u5173\u7cfb\u7a33\u5b9a\u6027\u3001\u56e0\u679c\u5408\u89c4\u6027\u548c\u95ea\u70c1\u60e9\u7f5a\uff09\uff0c\u4f7f\u7528\u5f00\u6e90\u5de5\u5177\u8ba1\u7b97\uff0c\u5e76\u901a\u8fc7\u4eba\u7c7b\u504f\u597d\u6570\u636e\u8bad\u7ec3\u6743\u91cd\u3002", "result": "WCS\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff08\u5982VBench-2.0\u7b49\u57fa\u51c6\u6d4b\u8bd5\uff09\u4e0e\u4eba\u7c7b\u8bc4\u4ef7\u76f8\u5173\u6027\u9ad8\uff0c\u4f18\u4e8e\u73b0\u6709\u6307\u6807\uff08\u5982FVD\u3001CLIPScore\u7b49\uff09\u3002", "conclusion": "WCS\u4e3a\u8bc4\u4f30\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u4e16\u754c\u4e00\u81f4\u6027\u63d0\u4f9b\u4e86\u5168\u9762\u4e14\u53ef\u89e3\u91ca\u7684\u6846\u67b6\u3002"}}
{"id": "2508.00248", "pdf": "https://arxiv.org/pdf/2508.00248", "abs": "https://arxiv.org/abs/2508.00248", "authors": ["Chenggang Guo", "Hao Xu", "XianMing Wan"], "title": "Guided Depth Map Super-Resolution via Multi-Scale Fusion U-shaped Mamba Network", "categories": ["cs.CV"], "comment": null, "summary": "Depth map super-resolution technology aims to improve the spatial resolution of low-resolution depth maps and effectively restore high-frequency detail information. Traditional convolutional neural network has limitations in dealing with long-range dependencies and are unable to fully model the global contextual information in depth maps. Although transformer can model global dependencies, its computational complexity and memory consumption are quadratic, which significantly limits its ability to process high-resolution depth maps. In this paper, we propose a multi-scale fusion U-shaped Mamba (MSF-UM) model, a novel guided depth map super-resolution framework. The core innovation of this model is to integrate Mamba's efficient state-space modeling capabilities into a multi-scale U-shaped fusion structure guided by a color image. The structure combining the residual dense channel attention block and the Mamba state space module is designed, which combines the local feature extraction capability of the convolutional layer with the modeling advantage of the state space model for long-distance dependencies. At the same time, the model adopts a multi-scale cross-modal fusion strategy to make full use of the high-frequency texture information from the color image to guide the super-resolution process of the depth map. Compared with existing mainstream methods, the proposed MSF-UM significantly reduces the number of model parameters while achieving better reconstruction accuracy. Extensive experiments on multiple publicly available datasets validate the effectiveness of the model, especially showing excellent generalization ability in the task of large-scale depth map super-resolution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c3a\u5ea6\u878d\u5408U\u5f62Mamba\u6a21\u578b\uff08MSF-UM\uff09\uff0c\u7528\u4e8e\u6df1\u5ea6\u56fe\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\uff0c\u7ed3\u5408\u4e86\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u548c\u957f\u8ddd\u79bb\u4f9d\u8d56\u5efa\u6a21\u7684\u4f18\u52bf\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u53c2\u6570\u6570\u91cf\u5e76\u63d0\u9ad8\u4e86\u91cd\u5efa\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u5904\u7406\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800cTransformer\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u6d88\u8017\u8f83\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5728\u9ad8\u5206\u8fa8\u7387\u6df1\u5ea6\u56fe\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86\u7ed3\u5408\u6b8b\u5dee\u5bc6\u96c6\u901a\u9053\u6ce8\u610f\u529b\u5757\u548cMamba\u72b6\u6001\u7a7a\u95f4\u6a21\u5757\u7684\u591a\u5c3a\u5ea6U\u5f62\u878d\u5408\u7ed3\u6784\uff0c\u5229\u7528\u5f69\u8272\u56fe\u50cf\u7684\u9ad8\u9891\u7eb9\u7406\u4fe1\u606f\u6307\u5bfc\u6df1\u5ea6\u56fe\u8d85\u5206\u8fa8\u7387\u8fc7\u7a0b\u3002", "result": "\u6a21\u578b\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u53c2\u6570\u6570\u91cf\u5e76\u63d0\u9ad8\u4e86\u91cd\u5efa\u7cbe\u5ea6\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6df1\u5ea6\u56fe\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u79c0\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MSF-UM\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u548c\u957f\u8ddd\u79bb\u4f9d\u8d56\u5efa\u6a21\uff0c\u4ee5\u53ca\u591a\u5c3a\u5ea6\u8de8\u6a21\u6001\u878d\u5408\u7b56\u7565\uff0c\u4e3a\u6df1\u5ea6\u56fe\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00259", "pdf": "https://arxiv.org/pdf/2508.00259", "abs": "https://arxiv.org/abs/2508.00259", "authors": ["Wentao Sun", "Hanqing Xu", "Quanyun Wu", "Dedong Zhang", "Yiping Chen", "Lingfei Ma", "John S. Zelek", "Jonathan Li"], "title": "PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian Splatting", "categories": ["cs.CV"], "comment": "22 pages, 9 figures", "summary": "We introduce PointGauss, a novel point cloud-guided framework for real-time multi-object segmentation in Gaussian Splatting representations. Unlike existing methods that suffer from prolonged initialization and limited multi-view consistency, our approach achieves efficient 3D segmentation by directly parsing Gaussian primitives through a point cloud segmentation-driven pipeline. The key innovation lies in two aspects: (1) a point cloud-based Gaussian primitive decoder that generates 3D instance masks within 1 minute, and (2) a GPU-accelerated 2D mask rendering system that ensures multi-view consistency. Extensive experiments demonstrate significant improvements over previous state-of-the-art methods, achieving performance gains of 1.89 to 31.78% in multi-view mIoU, while maintaining superior computational efficiency. To address the limitations of current benchmarks (single-object focus, inconsistent 3D evaluation, small scale, and partial coverage), we present DesktopObjects-360, a novel comprehensive dataset for 3D segmentation in radiance fields, featuring: (1) complex multi-object scenes, (2) globally consistent 2D annotations, (3) large-scale training data (over 27 thousand 2D masks), (4) full 360{\\deg} coverage, and (5) 3D evaluation masks.", "AI": {"tldr": "PointGauss\u662f\u4e00\u79cd\u57fa\u4e8e\u70b9\u4e91\u7684\u5b9e\u65f6\u591a\u76ee\u6807\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u65af\u6e85\u5c04\u8868\u793a\u5b9e\u73b0\u9ad8\u65483D\u5206\u5272\uff0c\u663e\u8457\u63d0\u5347\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u521d\u59cb\u5316\u65f6\u95f4\u957f\u548c\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0cPointGauss\u65e8\u5728\u901a\u8fc7\u70b9\u4e91\u5206\u5272\u9a71\u52a8\u6d41\u7a0b\u76f4\u63a5\u89e3\u6790\u9ad8\u65af\u57fa\u5143\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u70b9\u4e91\u9ad8\u65af\u57fa\u5143\u89e3\u7801\u5668\u548cGPU\u52a0\u901f\u76842D\u63a9\u7801\u6e32\u67d3\u7cfb\u7edf\uff0c\u5206\u522b\u5b9e\u73b0\u5feb\u901f3D\u5b9e\u4f8b\u63a9\u7801\u751f\u6210\u548c\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u4fdd\u8bc1\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5728\u591a\u89c6\u56femIoU\u4e0a\u6027\u80fd\u63d0\u53471.89%\u81f331.78%\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u8ba1\u7b97\u3002", "conclusion": "PointGauss\u57283D\u5206\u5272\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5f15\u5165\u65b0\u6570\u636e\u96c6DesktopObjects-360\u4ee5\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.00272", "pdf": "https://arxiv.org/pdf/2508.00272", "abs": "https://arxiv.org/abs/2508.00272", "authors": ["Wenyue Chong"], "title": "Towards Robust Semantic Correspondence: A Benchmark and Insights", "categories": ["cs.CV"], "comment": null, "summary": "Semantic correspondence aims to identify semantically meaningful relationships between different images and is a fundamental challenge in computer vision. It forms the foundation for numerous tasks such as 3D reconstruction, object tracking, and image editing. With the progress of large-scale vision models, semantic correspondence has achieved remarkable performance in controlled and high-quality conditions. However, the robustness of semantic correspondence in challenging scenarios is much less investigated. In this work, we establish a novel benchmark for evaluating semantic correspondence in adverse conditions. The benchmark dataset comprises 14 distinct challenging scenarios that reflect commonly encountered imaging issues, including geometric distortion, image blurring, digital artifacts, and environmental occlusion. Through extensive evaluations, we provide several key insights into the robustness of semantic correspondence approaches: (1) All existing methods suffer from noticeable performance drops under adverse conditions; (2) Using large-scale vision models can enhance overall robustness, but fine-tuning on these models leads to a decline in relative robustness; (3) The DINO model outperforms the Stable Diffusion in relative robustness, and their fusion achieves better absolute robustness; Moreover, We evaluate common robustness enhancement strategies for semantic correspondence and find that general data augmentations are ineffective, highlighting the need for task-specific designs. These results are consistent across both our dataset and real-world benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u8bed\u4e49\u5bf9\u5e94\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u9c81\u68d2\u6027\u7684\u65b0\u57fa\u51c6\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u6311\u6218\u6027\u573a\u666f\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u5927\u6a21\u578b\u80fd\u63d0\u5347\u9c81\u68d2\u6027\u4f46\u5fae\u8c03\u4f1a\u964d\u4f4e\u76f8\u5bf9\u9c81\u68d2\u6027\uff0cDINO\u6a21\u578b\u4f18\u4e8eStable Diffusion\uff0c\u4e14\u901a\u7528\u6570\u636e\u589e\u5f3a\u65e0\u6548\u3002", "motivation": "\u8bed\u4e49\u5bf9\u5e94\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u57fa\u7840\u4efb\u52a1\uff0c\u4f46\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u7814\u7a76\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u65b0\u57fa\u51c6\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "method": "\u6784\u5efa\u5305\u542b14\u79cd\u6311\u6218\u6027\u573a\u666f\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u73b0\u6709\u65b9\u6cd5\u548c\u5927\u6a21\u578b\uff08\u5982DINO\u548cStable Diffusion\uff09\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5206\u6790\u5e38\u89c1\u589e\u5f3a\u7b56\u7565\u7684\u6548\u679c\u3002", "result": "\u73b0\u6709\u65b9\u6cd5\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u5927\u6a21\u578b\u63d0\u5347\u9c81\u68d2\u6027\u4f46\u5fae\u8c03\u964d\u4f4e\u76f8\u5bf9\u9c81\u68d2\u6027\uff0cDINO\u4f18\u4e8eStable Diffusion\uff0c\u901a\u7528\u6570\u636e\u589e\u5f3a\u65e0\u6548\u3002", "conclusion": "\u8bed\u4e49\u5bf9\u5e94\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u9700\u4efb\u52a1\u7279\u5b9a\u8bbe\u8ba1\uff0c\u5927\u6a21\u578b\u548c\u6a21\u578b\u878d\u5408\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u9700\u907f\u514d\u5fae\u8c03\u3002"}}
{"id": "2508.00289", "pdf": "https://arxiv.org/pdf/2508.00289", "abs": "https://arxiv.org/abs/2508.00289", "authors": ["Christian Simon", "Masato Ishii", "Akio Hayakawa", "Zhi Zhong", "Shusuke Takahashi", "Takashi Shibuya", "Yuki Mitsufuji"], "title": "TITAN-Guide: Taming Inference-Time AligNment for Guided Text-to-Video Diffusion Models", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "In the recent development of conditional diffusion models still require heavy supervised fine-tuning for performing control on a category of tasks. Training-free conditioning via guidance with off-the-shelf models is a favorable alternative to avoid further fine-tuning on the base model. However, the existing training-free guidance frameworks either have heavy memory requirements or offer sub-optimal control due to rough estimation. These shortcomings limit the applicability to control diffusion models that require intense computation, such as Text-to-Video (T2V) diffusion models. In this work, we propose Taming Inference Time Alignment for Guided Text-to-Video Diffusion Model, so-called TITAN-Guide, which overcomes memory space issues, and provides more optimal control in the guidance process compared to the counterparts. In particular, we develop an efficient method for optimizing diffusion latents without backpropagation from a discriminative guiding model. In particular, we study forward gradient descents for guided diffusion tasks with various options on directional directives. In our experiments, we demonstrate the effectiveness of our approach in efficiently managing memory during latent optimization, while previous methods fall short. Our proposed approach not only minimizes memory requirements but also significantly enhances T2V performance across a range of diffusion guidance benchmarks. Code, models, and demo are available at https://titanguide.github.io.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTITAN-Guide\u7684\u65e0\u8bad\u7ec3\u6307\u5bfc\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u63a8\u7406\u65f6\u95f4\u5bf9\u9f50\uff0c\u89e3\u51b3\u4e86\u5185\u5b58\u9700\u6c42\u548c\u63a7\u5236\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65e0\u8bad\u7ec3\u6307\u5bfc\u6846\u67b6\u5b58\u5728\u5185\u5b58\u9700\u6c42\u9ad8\u6216\u63a7\u5236\u6548\u679c\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5728\u8ba1\u7b97\u5bc6\u96c6\u578b\u4efb\u52a1\uff08\u5982\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\uff09\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65e0\u9700\u53cd\u5411\u4f20\u64ad\u7684\u9ad8\u6548\u6269\u6563\u6f5c\u5728\u4f18\u5316\u65b9\u6cd5\uff0c\u7814\u7a76\u4e86\u524d\u5411\u68af\u5ea6\u4e0b\u964d\u53ca\u5176\u65b9\u5411\u6027\u6307\u5bfc\u9009\u9879\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5185\u5b58\u7ba1\u7406\u548c\u6f5c\u5728\u4f18\u5316\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "TITAN-Guide\u4e0d\u4ec5\u51cf\u5c11\u4e86\u5185\u5b58\u9700\u6c42\uff0c\u8fd8\u5728\u591a\u4e2a\u6269\u6563\u6307\u5bfc\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.00298", "pdf": "https://arxiv.org/pdf/2508.00298", "abs": "https://arxiv.org/abs/2508.00298", "authors": ["Jin Lyu", "Liang An", "Li Lin", "Pujin Cheng", "Yebin Liu", "Xiaoying Tang"], "title": "AniMer+: Unified Pose and Shape Estimation Across Mammalia and Aves via Family-Aware Transformer", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2412.00837", "summary": "In the era of foundation models, achieving a unified understanding of different dynamic objects through a single network has the potential to empower stronger spatial intelligence. Moreover, accurate estimation of animal pose and shape across diverse species is essential for quantitative analysis in biological research. However, this topic remains underexplored due to the limited network capacity of previous methods and the scarcity of comprehensive multi-species datasets. To address these limitations, we introduce AniMer+, an extended version of our scalable AniMer framework. In this paper, we focus on a unified approach for reconstructing mammals (mammalia) and birds (aves). A key innovation of AniMer+ is its high-capacity, family-aware Vision Transformer (ViT) incorporating a Mixture-of-Experts (MoE) design. Its architecture partitions network layers into taxa-specific components (for mammalia and aves) and taxa-shared components, enabling efficient learning of both distinct and common anatomical features within a single model. To overcome the critical shortage of 3D training data, especially for birds, we introduce a diffusion-based conditional image generation pipeline. This pipeline produces two large-scale synthetic datasets: CtrlAni3D for quadrupeds and CtrlAVES3D for birds. To note, CtrlAVES3D is the first large-scale, 3D-annotated dataset for birds, which is crucial for resolving single-view depth ambiguities. Trained on an aggregated collection of 41.3k mammalian and 12.4k avian images (combining real and synthetic data), our method demonstrates superior performance over existing approaches across a wide range of benchmarks, including the challenging out-of-domain Animal Kingdom dataset. Ablation studies confirm the effectiveness of both our novel network architecture and the generated synthetic datasets in enhancing real-world application performance.", "AI": {"tldr": "AniMer+\u662f\u4e00\u4e2a\u6269\u5c55\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u91cd\u5efa\u54fa\u4e73\u52a8\u7269\u548c\u9e1f\u7c7b\u7684\u59ff\u6001\u4e0e\u5f62\u72b6\uff0c\u901a\u8fc7\u9ad8\u5bb9\u91cf\u7684ViT\u548cMoE\u8bbe\u8ba1\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\uff0c\u5e76\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u5408\u6210\u6570\u636e\u4ee5\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u5728\u57fa\u7840\u6a21\u578b\u65f6\u4ee3\uff0c\u901a\u8fc7\u5355\u4e00\u7f51\u7edc\u7edf\u4e00\u7406\u89e3\u4e0d\u540c\u52a8\u6001\u5bf9\u8c61\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u540c\u65f6\u751f\u7269\u7814\u7a76\u4e2d\u8de8\u7269\u79cd\u7684\u59ff\u6001\u4e0e\u5f62\u72b6\u51c6\u786e\u4f30\u8ba1\u4e5f\u81f3\u5173\u91cd\u8981\u3002", "method": "AniMer+\u91c7\u7528\u9ad8\u5bb9\u91cf\u3001\u5bb6\u65cf\u611f\u77e5\u7684ViT\u7ed3\u5408MoE\u8bbe\u8ba1\uff0c\u5c06\u7f51\u7edc\u5c42\u5206\u4e3a\u7269\u79cd\u7279\u5b9a\u548c\u5171\u4eab\u90e8\u5206\uff0c\u5e76\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u5408\u6210\u6570\u636e\u96c6CtrlAni3D\u548cCtrlAVES3D\u3002", "result": "\u572841.3k\u54fa\u4e73\u52a8\u7269\u548c12.4k\u9e1f\u7c7b\u56fe\u50cf\uff08\u542b\u5408\u6210\u6570\u636e\uff09\u4e0a\u8bad\u7ec3\u540e\uff0cAniMer+\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5305\u62ec\u6311\u6218\u6027\u7684Animal Kingdom\u6570\u636e\u96c6\u3002", "conclusion": "AniMer+\u7684\u7f51\u7edc\u67b6\u6784\u548c\u5408\u6210\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u9645\u5e94\u7528\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u7f51\u7edc\u5bb9\u91cf\u4e0d\u8db3\u7684\u95ee\u9898\u3002"}}
{"id": "2508.00299", "pdf": "https://arxiv.org/pdf/2508.00299", "abs": "https://arxiv.org/abs/2508.00299", "authors": ["Danzhen Fu", "Jiagao Hu", "Daiguo Zhou", "Fei Wang", "Zepeng Wang", "Wenhua Liao"], "title": "Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via Motion Sequence", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "ICCV 2025 Workshop (HiGen)", "summary": "Pedestrian detection models in autonomous driving systems often lack robustness due to insufficient representation of dangerous pedestrian scenarios in training datasets. To address this limitation, we present a novel framework for controllable pedestrian video editing in multi-view driving scenarios by integrating video inpainting and human motion control techniques. Our approach begins by identifying pedestrian regions of interest across multiple camera views, expanding detection bounding boxes with a fixed ratio, and resizing and stitching these regions into a unified canvas while preserving cross-view spatial relationships. A binary mask is then applied to designate the editable area, within which pedestrian editing is guided by pose sequence control conditions. This enables flexible editing functionalities, including pedestrian insertion, replacement, and removal. Extensive experiments demonstrate that our framework achieves high-quality pedestrian editing with strong visual realism, spatiotemporal coherence, and cross-view consistency. These results establish the proposed method as a robust and versatile solution for multi-view pedestrian video generation, with broad potential for applications in data augmentation and scenario simulation in autonomous driving.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u89c6\u89d2\u884c\u4eba\u89c6\u9891\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u9891\u4fee\u590d\u548c\u52a8\u4f5c\u63a7\u5236\u6280\u672f\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u884c\u4eba\u68c0\u6d4b\u6a21\u578b\u56e0\u8bad\u7ec3\u6570\u636e\u7f3a\u4e4f\u5371\u9669\u573a\u666f\u800c\u9c81\u68d2\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u8bc6\u522b\u591a\u89c6\u89d2\u884c\u4eba\u533a\u57df\uff0c\u6269\u5c55\u68c0\u6d4b\u6846\u5e76\u62fc\u63a5\u4e3a\u7edf\u4e00\u753b\u5e03\uff0c\u901a\u8fc7\u59ff\u6001\u5e8f\u5217\u63a7\u5236\u6761\u4ef6\u8fdb\u884c\u884c\u4eba\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u9ad8\u8d28\u91cf\u5b8c\u6210\u884c\u4eba\u7f16\u8f91\uff0c\u5177\u6709\u89c6\u89c9\u771f\u5b9e\u611f\u3001\u65f6\u7a7a\u4e00\u81f4\u6027\u548c\u8de8\u89c6\u89d2\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u89c6\u89d2\u884c\u4eba\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u6570\u636e\u589e\u5f3a\u548c\u573a\u666f\u6a21\u62df\u3002"}}
{"id": "2508.00308", "pdf": "https://arxiv.org/pdf/2508.00308", "abs": "https://arxiv.org/abs/2508.00308", "authors": ["Chunyan She", "Fujun Han", "Chengyu Fang", "Shukai Duan", "Lidan Wang"], "title": "Exploring Fourier Prior and Event Collaboration for Low-Light Image Enhancement", "categories": ["cs.CV"], "comment": "Accepted by ACM MM 2025", "summary": "The event camera, benefiting from its high dynamic range and low latency, provides performance gain for low-light image enhancement. Unlike frame-based cameras, it records intensity changes with extremely high temporal resolution, capturing sufficient structure information. Currently, existing event-based methods feed a frame and events directly into a single model without fully exploiting modality-specific advantages, which limits their performance. Therefore, by analyzing the role of each sensing modality, the enhancement pipeline is decoupled into two stages: visibility restoration and structure refinement. In the first stage, we design a visibility restoration network with amplitude-phase entanglement by rethinking the relationship between amplitude and phase components in Fourier space. In the second stage, a fusion strategy with dynamic alignment is proposed to mitigate the spatial mismatch caused by the temporal resolution discrepancy between two sensing modalities, aiming to refine the structure information of the image enhanced by the visibility restoration network. In addition, we utilize spatial-frequency interpolation to simulate negative samples with diverse illumination, noise and artifact degradations, thereby developing a contrastive loss that encourages the model to learn discriminative representations. Experiments demonstrate that the proposed method outperforms state-of-the-art models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\uff0c\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u4f4e\u5ef6\u8fdf\u7279\u6027\uff0c\u5206\u522b\u8fdb\u884c\u53ef\u89c1\u6027\u6062\u590d\u548c\u7ed3\u6784\u7ec6\u5316\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u548c\u5e27\u76f8\u673a\u7684\u6a21\u6001\u4f18\u52bf\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "\u5206\u4e24\u9636\u6bb5\uff1a1) \u57fa\u4e8e\u5085\u91cc\u53f6\u7a7a\u95f4\u5e45\u5ea6-\u76f8\u4f4d\u7ea0\u7f20\u7684\u53ef\u89c1\u6027\u6062\u590d\u7f51\u7edc\uff1b2) \u52a8\u6001\u5bf9\u9f50\u878d\u5408\u7b56\u7565\u7ec6\u5316\u7ed3\u6784\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u5206\u9636\u6bb5\u5904\u7406\u548c\u52a8\u6001\u5bf9\u9f50\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u6027\u80fd\u3002"}}
{"id": "2508.00312", "pdf": "https://arxiv.org/pdf/2508.00312", "abs": "https://arxiv.org/abs/2508.00312", "authors": ["Suhang Cai", "Xiaohao Peng", "Chong Wang", "Xiaojie Cai", "Jiangbo Qian"], "title": "GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video anomaly detection (VAD) plays a critical role in public safety applications such as intelligent surveillance. However, the rarity, unpredictability, and high annotation cost of real-world anomalies make it difficult to scale VAD datasets, which limits the performance and generalization ability of existing models. To address this challenge, we propose a generative video-enhanced weakly-supervised video anomaly detection (GV-VAD) framework that leverages text-conditioned video generation models to produce semantically controllable and physically plausible synthetic videos. These virtual videos are used to augment training data at low cost. In addition, a synthetic sample loss scaling strategy is utilized to control the influence of generated synthetic samples for efficient training. The experiments show that the proposed framework outperforms state-of-the-art methods on UCF-Crime datasets. The code is available at https://github.com/Sumutan/GV-VAD.git.", "AI": {"tldr": "\u63d0\u51faGV-VAD\u6846\u67b6\uff0c\u5229\u7528\u6587\u672c\u6761\u4ef6\u89c6\u9891\u751f\u6210\u6a21\u578b\u751f\u6210\u53ef\u63a7\u7684\u5408\u6210\u89c6\u9891\uff0c\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u5347\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u771f\u5b9e\u5f02\u5e38\u6570\u636e\u7a00\u7f3a\u3001\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u6587\u672c\u6761\u4ef6\u89c6\u9891\u751f\u6210\u6a21\u578b\u751f\u6210\u5408\u6210\u89c6\u9891\uff0c\u5e76\u91c7\u7528\u5408\u6210\u6837\u672c\u635f\u5931\u7f29\u653e\u7b56\u7565\u4f18\u5316\u8bad\u7ec3\u3002", "result": "\u5728UCF-Crime\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GV-VAD\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.00319", "pdf": "https://arxiv.org/pdf/2508.00319", "abs": "https://arxiv.org/abs/2508.00319", "authors": ["Sunghyun Park", "Seokeon Choi", "Hyoungwoo Park", "Sungrack Yun"], "title": "Steering Guidance for Personalized Text-to-Image Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": "ICCV 2025", "summary": "Personalizing text-to-image diffusion models is crucial for adapting the pre-trained models to specific target concepts, enabling diverse image generation. However, fine-tuning with few images introduces an inherent trade-off between aligning with the target distribution (e.g., subject fidelity) and preserving the broad knowledge of the original model (e.g., text editability). Existing sampling guidance methods, such as classifier-free guidance (CFG) and autoguidance (AG), fail to effectively guide the output toward well-balanced space: CFG restricts the adaptation to the target distribution, while AG compromises text alignment. To address these limitations, we propose personalization guidance, a simple yet effective method leveraging an unlearned weak model conditioned on a null text prompt. Moreover, our method dynamically controls the extent of unlearning in a weak model through weight interpolation between pre-trained and fine-tuned models during inference. Unlike existing guidance methods, which depend solely on guidance scales, our method explicitly steers the outputs toward a balanced latent space without additional computational overhead. Experimental results demonstrate that our proposed guidance can improve text alignment and target distribution fidelity, integrating seamlessly with various fine-tuning strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e2a\u6027\u5316\u5f15\u5bfc\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f31\u6a21\u578b\u548c\u6743\u91cd\u63d2\u503c\u52a8\u6001\u63a7\u5236\u8f93\u51fa\uff0c\u5e73\u8861\u76ee\u6807\u5206\u5e03\u548c\u6587\u672c\u5bf9\u9f50\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5f15\u5bfc\u65b9\u6cd5\uff08\u5982CFG\u548cAG\uff09\u5728\u4e2a\u6027\u5316\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u65e0\u6cd5\u5e73\u8861\u76ee\u6807\u5206\u5e03\u548c\u6587\u672c\u5bf9\u9f50\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u672a\u5b66\u4e60\u7684\u5f31\u6a21\u578b\u548c\u7a7a\u6587\u672c\u63d0\u793a\uff0c\u52a8\u6001\u63a7\u5236\u6743\u91cd\u63d2\u503c\u4ee5\u5e73\u8861\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u63d0\u5347\u6587\u672c\u5bf9\u9f50\u548c\u76ee\u6807\u5206\u5e03\u4fdd\u771f\u5ea6\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e2a\u6027\u5316\u5f15\u5bfc\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5fae\u8c03\u7b56\u7565\u3002"}}
{"id": "2508.00330", "pdf": "https://arxiv.org/pdf/2508.00330", "abs": "https://arxiv.org/abs/2508.00330", "authors": ["Lilika Makabe", "Hiroaki Santo", "Fumio Okura", "Michael S. Brown", "Yasuyuki Matsushita"], "title": "Spectral Sensitivity Estimation with an Uncalibrated Diffraction Grating", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces a practical and accurate calibration method for camera spectral sensitivity using a diffraction grating. Accurate calibration of camera spectral sensitivity is crucial for various computer vision tasks, including color correction, illumination estimation, and material analysis. Unlike existing approaches that require specialized narrow-band filters or reference targets with known spectral reflectances, our method only requires an uncalibrated diffraction grating sheet, readily available off-the-shelf. By capturing images of the direct illumination and its diffracted pattern through the grating sheet, our method estimates both the camera spectral sensitivity and the diffraction grating parameters in a closed-form manner. Experiments on synthetic and real-world data demonstrate that our method outperforms conventional reference target-based methods, underscoring its effectiveness and practicality.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u884d\u5c04\u5149\u6805\u7684\u76f8\u673a\u5149\u8c31\u7075\u654f\u5ea6\u6821\u51c6\u65b9\u6cd5\uff0c\u65e0\u9700\u4e13\u7528\u8bbe\u5907\uff0c\u4ec5\u9700\u666e\u901a\u5149\u6805\u7247\u5373\u53ef\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6821\u51c6\u3002", "motivation": "\u76f8\u673a\u5149\u8c31\u7075\u654f\u5ea6\u6821\u51c6\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u8272\u5f69\u6821\u6b63\u3001\u5149\u7167\u4f30\u8ba1\u548c\u6750\u8d28\u5206\u6790\uff09\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e13\u7528\u8bbe\u5907\u6216\u5df2\u77e5\u5149\u8c31\u53cd\u5c04\u7387\u7684\u76ee\u6807\uff0c\u9650\u5236\u4e86\u5b9e\u7528\u6027\u3002", "method": "\u901a\u8fc7\u6355\u83b7\u76f4\u63a5\u5149\u7167\u53ca\u5176\u901a\u8fc7\u5149\u6805\u7247\u7684\u884d\u5c04\u56fe\u6848\u56fe\u50cf\uff0c\u4ee5\u95ed\u5408\u5f62\u5f0f\u540c\u65f6\u4f30\u8ba1\u76f8\u673a\u5149\u8c31\u7075\u654f\u5ea6\u548c\u5149\u6805\u53c2\u6570\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u53c2\u8003\u76ee\u6807\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u5b9e\u7528\uff0c\u4e3a\u76f8\u673a\u5149\u8c31\u7075\u654f\u5ea6\u6821\u51c6\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u4fbf\u6377\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00366", "pdf": "https://arxiv.org/pdf/2508.00366", "abs": "https://arxiv.org/abs/2508.00366", "authors": ["Liang Han", "Xu Zhang", "Haichuan Song", "Kanle Shi", "Yu-Shen Liu", "Zhizhong Han"], "title": "SparseRecon: Neural Implicit Surface Reconstruction from Sparse Views with Feature and Depth Consistencies", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Surface reconstruction from sparse views aims to reconstruct a 3D shape or scene from few RGB images. The latest methods are either generalization-based or overfitting-based. However, the generalization-based methods do not generalize well on views that were unseen during training, while the reconstruction quality of overfitting-based methods is still limited by the limited geometry clues. To address this issue, we propose SparseRecon, a novel neural implicit reconstruction method for sparse views with volume rendering-based feature consistency and uncertainty-guided depth constraint. Firstly, we introduce a feature consistency loss across views to constrain the neural implicit field. This design alleviates the ambiguity caused by insufficient consistency information of views and ensures completeness and smoothness in the reconstruction results. Secondly, we employ an uncertainty-guided depth constraint to back up the feature consistency loss in areas with occlusion and insignificant features, which recovers geometry details for better reconstruction quality. Experimental results demonstrate that our method outperforms the state-of-the-art methods, which can produce high-quality geometry with sparse-view input, especially in the scenarios with small overlapping views. Project page: https://hanl2010.github.io/SparseRecon/.", "AI": {"tldr": "SparseRecon\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7a00\u758f\u89c6\u56fe\u795e\u7ecf\u9690\u5f0f\u91cd\u5efa\u65b9\u6cd5\uff0c\u7ed3\u5408\u4f53\u79ef\u6e32\u67d3\u7279\u5f81\u4e00\u81f4\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u6df1\u5ea6\u7ea6\u675f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u672a\u89c1\u89c6\u56fe\u4e0a\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u91cd\u5efa\u8d28\u91cf\u53d7\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u7a00\u758f\u89c6\u56fe\u91cd\u5efa\u7684\u73b0\u6709\u65b9\u6cd5\uff08\u6cdb\u5316\u578b\u548c\u8fc7\u62df\u5408\u578b\uff09\u5206\u522b\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u548c\u91cd\u5efa\u8d28\u91cf\u53d7\u9650\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "SparseRecon\u901a\u8fc7\u4f53\u79ef\u6e32\u67d3\u7279\u5f81\u4e00\u81f4\u6027\u635f\u5931\u548c\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u6df1\u5ea6\u7ea6\u675f\uff0c\u4f18\u5316\u795e\u7ecf\u9690\u5f0f\u573a\uff0c\u63d0\u5347\u7a00\u758f\u89c6\u56fe\u4e0b\u7684\u91cd\u5efa\u8d28\u91cf\u548c\u51e0\u4f55\u7ec6\u8282\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSparseRecon\u5728\u7a00\u758f\u89c6\u56fe\u8f93\u5165\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u89c6\u56fe\u91cd\u53e0\u8f83\u5c11\u7684\u573a\u666f\u4e2d\uff0c\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u51e0\u4f55\u91cd\u5efa\u3002", "conclusion": "SparseRecon\u901a\u8fc7\u7279\u5f81\u4e00\u81f4\u6027\u548c\u6df1\u5ea6\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u89c6\u56fe\u91cd\u5efa\u7684\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.00397", "pdf": "https://arxiv.org/pdf/2508.00397", "abs": "https://arxiv.org/abs/2508.00397", "authors": ["Xi Xue", "Kunio Suzuki", "Nabarun Goswami", "Takuya Shintate"], "title": "Video Forgery Detection with Optical Flow Residuals and Spatial-Temporal Consistency", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement of diffusion-based video generation models has led to increasingly realistic synthetic content, presenting new challenges for video forgery detection. Existing methods often struggle to capture fine-grained temporal inconsistencies, particularly in AI-generated videos with high visual fidelity and coherent motion. In this work, we propose a detection framework that leverages spatial-temporal consistency by combining RGB appearance features with optical flow residuals. The model adopts a dual-branch architecture, where one branch analyzes RGB frames to detect appearance-level artifacts, while the other processes flow residuals to reveal subtle motion anomalies caused by imperfect temporal synthesis. By integrating these complementary features, the proposed method effectively detects a wide range of forged videos. Extensive experiments on text-to-video and image-to-video tasks across ten diverse generative models demonstrate the robustness and strong generalization ability of the proposed approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRGB\u5916\u89c2\u7279\u5f81\u548c\u5149\u6d41\u6b8b\u5dee\u7684\u53cc\u5206\u652f\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4bAI\u751f\u6210\u89c6\u9891\u4e2d\u7684\u4f2a\u9020\u5185\u5bb9\u3002", "motivation": "\u968f\u7740\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u89c6\u9891\u8d8a\u6765\u8d8a\u903c\u771f\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u7ec6\u5fae\u7684\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u67b6\u6784\uff0c\u5206\u522b\u5206\u6790RGB\u5e27\u548c\u5149\u6d41\u6b8b\u5dee\uff0c\u4ee5\u68c0\u6d4b\u5916\u89c2\u548c\u8fd0\u52a8\u5f02\u5e38\u3002", "result": "\u5728\u591a\u79cd\u751f\u6210\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u9c81\u68d2\u6027\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u7a7a\u95f4\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u7279\u5f81\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u591a\u79cd\u4f2a\u9020\u89c6\u9891\u3002"}}
{"id": "2508.00406", "pdf": "https://arxiv.org/pdf/2508.00406", "abs": "https://arxiv.org/abs/2508.00406", "authors": ["Tao Wu", "Jingyuan Ye", "Ying Fu"], "title": "PMR: Physical Model-Driven Multi-Stage Restoration of Turbulent Dynamic Videos", "categories": ["cs.CV"], "comment": null, "summary": "Geometric distortions and blurring caused by atmospheric turbulence degrade the quality of long-range dynamic scene videos. Existing methods struggle with restoring edge details and eliminating mixed distortions, especially under conditions of strong turbulence and complex dynamics. To address these challenges, we introduce a Dynamic Efficiency Index ($DEI$), which combines turbulence intensity, optical flow, and proportions of dynamic regions to accurately quantify video dynamic intensity under varying turbulence conditions and provide a high-dynamic turbulence training dataset. Additionally, we propose a Physical Model-Driven Multi-Stage Video Restoration ($PMR$) framework that consists of three stages: \\textbf{de-tilting} for geometric stabilization, \\textbf{motion segmentation enhancement} for dynamic region refinement, and \\textbf{de-blurring} for quality restoration. $PMR$ employs lightweight backbones and stage-wise joint training to ensure both efficiency and high restoration quality. Experimental results demonstrate that the proposed method effectively suppresses motion trailing artifacts, restores edge details and exhibits strong generalization capability, especially in real-world scenarios characterized by high-turbulence and complex dynamics. We will make the code and datasets openly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6548\u7387\u6307\u6570\uff08DEI\uff09\u548c\u591a\u9636\u6bb5\u89c6\u9891\u6062\u590d\u6846\u67b6\uff08PMR\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u6c14\u6e4d\u6d41\u5bfc\u81f4\u7684\u89c6\u9891\u5931\u771f\u548c\u6a21\u7cca\u95ee\u9898\u3002", "motivation": "\u5927\u6c14\u6e4d\u6d41\u5bfc\u81f4\u7684\u957f\u8ddd\u79bb\u52a8\u6001\u573a\u666f\u89c6\u9891\u8d28\u91cf\u4e0b\u964d\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6062\u590d\u8fb9\u7f18\u7ec6\u8282\u548c\u6d88\u9664\u6df7\u5408\u5931\u771f\uff0c\u5c24\u5176\u662f\u5728\u5f3a\u6e4d\u6d41\u548c\u590d\u6742\u52a8\u6001\u6761\u4ef6\u4e0b\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u6548\u7387\u6307\u6570\uff08DEI\uff09\u91cf\u5316\u89c6\u9891\u52a8\u6001\u5f3a\u5ea6\uff0c\u5e76\u8bbe\u8ba1\u7269\u7406\u6a21\u578b\u9a71\u52a8\u7684\u591a\u9636\u6bb5\u89c6\u9891\u6062\u590d\u6846\u67b6\uff08PMR\uff09\uff0c\u5305\u62ec\u53bb\u503e\u659c\u3001\u8fd0\u52a8\u5206\u5272\u589e\u5f3a\u548c\u53bb\u6a21\u7cca\u4e09\u4e2a\u9636\u6bb5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u6291\u5236\u8fd0\u52a8\u62d6\u5c3e\u4f2a\u5f71\uff0c\u6062\u590d\u8fb9\u7f18\u7ec6\u8282\uff0c\u5e76\u5728\u9ad8\u6e4d\u6d41\u548c\u590d\u6742\u52a8\u6001\u7684\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u52a8\u6001\u6e4d\u6d41\u6761\u4ef6\u4e0b\u7684\u89c6\u9891\u6062\u590d\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u3002"}}
{"id": "2508.00412", "pdf": "https://arxiv.org/pdf/2508.00412", "abs": "https://arxiv.org/abs/2508.00412", "authors": ["Hanqi Chen", "Xu Zhang", "Xiaoliu Guan", "Lielin Jiang", "Guanzhong Wang", "Zeyu Chen", "Yi Liu"], "title": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable generative capabilities, particularly benefiting from Transformer architectures that enhance visual and artistic fidelity. However, their inherently sequential denoising process results in high inference latency, limiting their deployment in real-time scenarios. Existing training-free acceleration approaches typically reuse intermediate features at fixed timesteps or layers, overlooking the evolving semantic focus across denoising stages and Transformer blocks.To address this, we propose Sortblock, a training-free inference acceleration framework that dynamically caches block-wise features based on their similarity across adjacent timesteps. By ranking the evolution of residuals, Sortblock adaptively determines a recomputation ratio, selectively skipping redundant computations while preserving generation quality. Furthermore, we incorporate a lightweight linear prediction mechanism to reduce accumulated errors in skipped blocks.Extensive experiments across various tasks and DiT architectures demonstrate that Sortblock achieves over 2$\\times$ inference speedup with minimal degradation in output quality, offering an effective and generalizable solution for accelerating diffusion-based generative models.", "AI": {"tldr": "Sortblock\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u63a8\u7406\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u7f13\u5b58\u5757\u7ea7\u7279\u5f81\u548c\u9009\u62e9\u6027\u8df3\u8fc7\u5197\u4f59\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u5347DiTs\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "DiTs\u7684\u5e8f\u5217\u53bb\u566a\u8fc7\u7a0b\u5bfc\u81f4\u9ad8\u63a8\u7406\u5ef6\u8fdf\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u5e94\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u53bb\u566a\u9636\u6bb5\u548cTransformer\u5757\u7684\u8bed\u4e49\u53d8\u5316\u3002", "method": "\u63d0\u51faSortblock\u6846\u67b6\uff0c\u52a8\u6001\u7f13\u5b58\u5757\u7ea7\u7279\u5f81\uff0c\u57fa\u4e8e\u76f8\u90bb\u65f6\u95f4\u6b65\u7684\u76f8\u4f3c\u6027\u6392\u5e8f\uff0c\u81ea\u9002\u5e94\u786e\u5b9a\u91cd\u8ba1\u7b97\u6bd4\u4f8b\uff0c\u5e76\u7ed3\u5408\u7ebf\u6027\u9884\u6d4b\u51cf\u5c11\u8bef\u5dee\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSortblock\u5728\u591a\u79cd\u4efb\u52a1\u548cDiT\u67b6\u6784\u4e2d\u5b9e\u73b0\u8d85\u8fc72\u500d\u7684\u63a8\u7406\u52a0\u901f\uff0c\u4e14\u8f93\u51fa\u8d28\u91cf\u51e0\u4e4e\u65e0\u635f\u3002", "conclusion": "Sortblock\u4e3a\u6269\u6563\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u52a0\u901f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00413", "pdf": "https://arxiv.org/pdf/2508.00413", "abs": "https://arxiv.org/abs/2508.00413", "authors": ["Junyu Chen", "Dongyun Zou", "Wenkun He", "Junsong Chen", "Enze Xie", "Song Han", "Han Cai"], "title": "DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured Latent Space", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV 2025", "summary": "We present DC-AE 1.5, a new family of deep compression autoencoders for high-resolution diffusion models. Increasing the autoencoder's latent channel number is a highly effective approach for improving its reconstruction quality. However, it results in slow convergence for diffusion models, leading to poorer generation quality despite better reconstruction quality. This issue limits the quality upper bound of latent diffusion models and hinders the employment of autoencoders with higher spatial compression ratios. We introduce two key innovations to address this challenge: i) Structured Latent Space, a training-based approach to impose a desired channel-wise structure on the latent space with front latent channels capturing object structures and latter latent channels capturing image details; ii) Augmented Diffusion Training, an augmented diffusion training strategy with additional diffusion training objectives on object latent channels to accelerate convergence. With these techniques, DC-AE 1.5 delivers faster convergence and better diffusion scaling results than DC-AE. On ImageNet 512x512, DC-AE-1.5-f64c128 delivers better image generation quality than DC-AE-f32c32 while being 4x faster. Code: https://github.com/dc-ai-projects/DC-Gen.", "AI": {"tldr": "DC-AE 1.5\u662f\u4e00\u79cd\u65b0\u578b\u6df1\u5ea6\u538b\u7f29\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\u548c\u589e\u5f3a\u6269\u6563\u8bad\u7ec3\u89e3\u51b3\u4e86\u9ad8\u5206\u8fa8\u7387\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6536\u655b\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u589e\u52a0\u81ea\u7f16\u7801\u5668\u7684\u6f5c\u5728\u901a\u9053\u6570\u53ef\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\uff0c\u4f46\u4f1a\u5bfc\u81f4\u6269\u6563\u6a21\u578b\u6536\u655b\u7f13\u6162\uff0c\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\u548c\u589e\u5f3a\u6269\u6563\u8bad\u7ec3\u7b56\u7565\uff0c\u524d\u8005\u5bf9\u6f5c\u5728\u7a7a\u95f4\u65bd\u52a0\u901a\u9053\u7ed3\u6784\uff0c\u540e\u8005\u901a\u8fc7\u989d\u5916\u6269\u6563\u76ee\u6807\u52a0\u901f\u6536\u655b\u3002", "result": "DC-AE 1.5\u5728ImageNet 512x512\u4e0a\u6bd4DC-AE\u751f\u6210\u8d28\u91cf\u66f4\u597d\u4e14\u5feb4\u500d\u3002", "conclusion": "DC-AE 1.5\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u89e3\u51b3\u4e86\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u8d28\u91cf\u4e0a\u9650\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u66f4\u9ad8\u7a7a\u95f4\u538b\u7f29\u6bd4\u7684\u81ea\u7f16\u7801\u5668\u3002"}}
{"id": "2508.00418", "pdf": "https://arxiv.org/pdf/2508.00418", "abs": "https://arxiv.org/abs/2508.00418", "authors": ["Sangwoo Youn", "Minji Lee", "Nokap Tony Park", "Yeonggyoo Jeon", "Taeyoung Na"], "title": "IN2OUT: Fine-Tuning Video Inpainting Model for Video Outpainting Using Hierarchical Discriminator", "categories": ["cs.CV", "eess.IV"], "comment": "ICIP 2025. Code: https://github.com/sang-w00/IN2OUT", "summary": "Video outpainting presents a unique challenge of extending the borders while maintaining consistency with the given content. In this paper, we suggest the use of video inpainting models that excel in object flow learning and reconstruction in outpainting rather than solely generating the background as in existing methods. However, directly applying or fine-tuning inpainting models to outpainting has shown to be ineffective, often leading to blurry results. Our extensive experiments on discriminator designs reveal that a critical component missing in the outpainting fine-tuning process is a discriminator capable of effectively assessing the perceptual quality of the extended areas. To tackle this limitation, we differentiate the objectives of adversarial training into global and local goals and introduce a hierarchical discriminator that meets both objectives. Additionally, we develop a specialized outpainting loss function that leverages both local and global features of the discriminator. Fine-tuning on this adversarial loss function enhances the generator's ability to produce both visually appealing and globally coherent outpainted scenes. Our proposed method outperforms state-of-the-art methods both quantitatively and qualitatively. Supplementary materials including the demo video and the code are available in SigPort.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u9891\u4fee\u590d\u6a21\u578b\u7684\u89c6\u9891\u5916\u7ed8\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u5224\u522b\u5668\u8bbe\u8ba1\u548c\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5916\u7ed8\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5916\u7ed8\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u80cc\u666f\u751f\u6210\uff0c\u800c\u5ffd\u7565\u4e86\u5bf9\u8c61\u6d41\u52a8\u548c\u91cd\u5efa\u7684\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u6548\u679c\u6a21\u7cca\u3002", "method": "\u91c7\u7528\u5206\u5c42\u5224\u522b\u5668\uff0c\u533a\u5206\u5168\u5c40\u548c\u5c40\u90e8\u5bf9\u6297\u8bad\u7ec3\u76ee\u6807\uff0c\u5e76\u8bbe\u8ba1\u4e13\u7528\u5916\u7ed8\u635f\u5931\u51fd\u6570\u3002", "result": "\u65b9\u6cd5\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u901a\u8fc7\u6539\u8fdb\u5224\u522b\u5668\u548c\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u5916\u7ed8\u3002"}}
{"id": "2508.00427", "pdf": "https://arxiv.org/pdf/2508.00427", "abs": "https://arxiv.org/abs/2508.00427", "authors": ["Seunggeun Chi", "Enna Sachdeva", "Pin-Hao Huang", "Kwonjoon Lee"], "title": "Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV 2025 (Highlight)", "summary": "Amodal completion, which is the process of inferring the full appearance of objects despite partial occlusions, is crucial for understanding complex human-object interactions (HOI) in computer vision and robotics. Existing methods, such as those that use pre-trained diffusion models, often struggle to generate plausible completions in dynamic scenarios because they have a limited understanding of HOI. To solve this problem, we've developed a new approach that uses physical prior knowledge along with a specialized multi-regional inpainting technique designed for HOI. By incorporating physical constraints from human topology and contact information, we define two distinct regions: the primary region, where occluded object parts are most likely to be, and the secondary region, where occlusions are less probable. Our multi-regional inpainting method uses customized denoising strategies across these regions within a diffusion model. This improves the accuracy and realism of the generated completions in both their shape and visual detail. Our experimental results show that our approach significantly outperforms existing methods in HOI scenarios, moving machine perception closer to a more human-like understanding of dynamic environments. We also show that our pipeline is robust even without ground-truth contact annotations, which broadens its applicability to tasks like 3D reconstruction and novel view/pose synthesis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\u548c\u591a\u533a\u57df\u4fee\u590d\u6280\u672f\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u573a\u666f\u4e2d\u7684\u4eba-\u7269\u4ea4\u4e92\uff08HOI\uff09\u906e\u6321\u8865\u5168\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u7ed3\u679c\u7684\u51c6\u786e\u6027\u548c\u771f\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff09\u5728\u52a8\u6001HOI\u573a\u666f\u4e2d\u96be\u4ee5\u751f\u6210\u5408\u7406\u7684\u906e\u6321\u8865\u5168\u7ed3\u679c\uff0c\u56e0\u5176\u5bf9HOI\u7684\u7406\u89e3\u6709\u9650\u3002", "method": "\u5229\u7528\u4eba\u7c7b\u62d3\u6251\u548c\u63a5\u89e6\u4fe1\u606f\u7684\u7269\u7406\u7ea6\u675f\uff0c\u5b9a\u4e49\u4e3b\u6b21\u533a\u57df\uff0c\u5e76\u5728\u6269\u6563\u6a21\u578b\u4e2d\u91c7\u7528\u5b9a\u5236\u5316\u7684\u53bb\u566a\u7b56\u7565\u8fdb\u884c\u591a\u533a\u57df\u4fee\u590d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728HOI\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u771f\u5b9e\u63a5\u89e6\u6807\u6ce8\u4ecd\u5177\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u673a\u5668\u5bf9\u52a8\u6001\u73af\u5883\u7684\u611f\u77e5\u80fd\u529b\uff0c\u9002\u7528\u4e8e3D\u91cd\u5efa\u548c\u65b0\u89c6\u89d2/\u59ff\u6001\u5408\u6210\u7b49\u4efb\u52a1\u3002"}}
{"id": "2508.00453", "pdf": "https://arxiv.org/pdf/2508.00453", "abs": "https://arxiv.org/abs/2508.00453", "authors": ["Baisong Li", "Xingwang Wang", "Haixiao Xu"], "title": "PIF-Net: Ill-Posed Prior Guided Multispectral and Hyperspectral Image Fusion via Invertible Mamba and Fusion-Aware LoRA", "categories": ["cs.CV"], "comment": null, "summary": "The goal of multispectral and hyperspectral image fusion (MHIF) is to generate high-quality images that simultaneously possess rich spectral information and fine spatial details. However, due to the inherent trade-off between spectral and spatial information and the limited availability of observations, this task is fundamentally ill-posed. Previous studies have not effectively addressed the ill-posed nature caused by data misalignment. To tackle this challenge, we propose a fusion framework named PIF-Net, which explicitly incorporates ill-posed priors to effectively fuse multispectral images and hyperspectral images. To balance global spectral modeling with computational efficiency, we design a method based on an invertible Mamba architecture that maintains information consistency during feature transformation and fusion, ensuring stable gradient flow and process reversibility. Furthermore, we introduce a novel fusion module called the Fusion-Aware Low-Rank Adaptation module, which dynamically calibrates spectral and spatial features while keeping the model lightweight. Extensive experiments on multiple benchmark datasets demonstrate that PIF-Net achieves significantly better image restoration performance than current state-of-the-art methods while maintaining model efficiency.", "AI": {"tldr": "PIF-Net\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u5149\u8c31\u548c\u9ad8\u5149\u8c31\u56fe\u50cf\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u4e0d\u9002\u5b9a\u5148\u9a8c\u548c\u53ef\u9006Mamba\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u5bf9\u9f50\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u591a\u5149\u8c31\u548c\u9ad8\u5149\u8c31\u56fe\u50cf\u878d\u5408\u4efb\u52a1\u7531\u4e8e\u5149\u8c31\u4e0e\u7a7a\u95f4\u4fe1\u606f\u7684\u56fa\u6709\u6743\u8861\u4ee5\u53ca\u89c2\u6d4b\u6570\u636e\u6709\u9650\uff0c\u5177\u6709\u4e0d\u9002\u5b9a\u6027\u3002\u73b0\u6709\u7814\u7a76\u672a\u80fd\u6709\u6548\u89e3\u51b3\u6570\u636e\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002", "method": "\u63d0\u51faPIF-Net\u6846\u67b6\uff0c\u7ed3\u5408\u4e0d\u9002\u5b9a\u5148\u9a8c\uff0c\u91c7\u7528\u53ef\u9006Mamba\u67b6\u6784\u5e73\u8861\u5168\u5c40\u5149\u8c31\u5efa\u6a21\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u6821\u51c6\u7684Fusion-Aware Low-Rank Adaptation\u6a21\u5757\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cPIF-Net\u7684\u56fe\u50cf\u6062\u590d\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u9ad8\u6548\u3002", "conclusion": "PIF-Net\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u548c\u6a21\u5757\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u5149\u8c31\u548c\u9ad8\u5149\u8c31\u56fe\u50cf\u878d\u5408\u4e2d\u7684\u4e0d\u9002\u5b9a\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u4e0e\u6548\u7387\u3002"}}
{"id": "2508.00471", "pdf": "https://arxiv.org/pdf/2508.00471", "abs": "https://arxiv.org/abs/2508.00471", "authors": ["Yiwen Wang", "Xinning Chai", "Yuhong Zhang", "Zhengxue Cheng", "Jun Zhao", "Rong Xie", "Li Song"], "title": "Semantic and Temporal Integration in Latent Diffusion Space for High-Fidelity Video Super-Resolution", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Recent advancements in video super-resolution (VSR) models have demonstrated impressive results in enhancing low-resolution videos. However, due to limitations in adequately controlling the generation process, achieving high fidelity alignment with the low-resolution input while maintaining temporal consistency across frames remains a significant challenge. In this work, we propose Semantic and Temporal Guided Video Super-Resolution (SeTe-VSR), a novel approach that incorporates both semantic and temporal-spatio guidance in the latent diffusion space to address these challenges. By incorporating high-level semantic information and integrating spatial and temporal information, our approach achieves a seamless balance between recovering intricate details and ensuring temporal coherence. Our method not only preserves high-reality visual content but also significantly enhances fidelity. Extensive experiments demonstrate that SeTe-VSR outperforms existing methods in terms of detail recovery and perceptual quality, highlighting its effectiveness for complex video super-resolution tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8bed\u4e49\u548c\u65f6\u7a7a\u5f15\u5bfc\u7684\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff08SeTe-VSR\uff09\uff0c\u901a\u8fc7\u6f5c\u5728\u6269\u6563\u7a7a\u95f4\u5b9e\u73b0\u7ec6\u8282\u6062\u590d\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5bf9\u9f50\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "method": "\u5f15\u5165\u8bed\u4e49\u548c\u65f6\u7a7a\u5f15\u5bfc\uff0c\u7ed3\u5408\u6f5c\u5728\u6269\u6563\u7a7a\u95f4\uff0c\u5e73\u8861\u7ec6\u8282\u6062\u590d\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "SeTe-VSR\u5728\u7ec6\u8282\u6062\u590d\u548c\u611f\u77e5\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SeTe-VSR\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u7684\u6311\u6218\u3002"}}
{"id": "2508.00477", "pdf": "https://arxiv.org/pdf/2508.00477", "abs": "https://arxiv.org/abs/2508.00477", "authors": ["Yuzhuo Chen", "Zehua Ma", "Jianhua Wang", "Kai Kang", "Shunyu Yao", "Weiming Zhang"], "title": "LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer", "categories": ["cs.CV"], "comment": "8 pages, 5 figures, 3 tables", "summary": "In controllable image synthesis, generating coherent and consistent images from multiple references with spatial layout awareness remains an open challenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework that, for the first time, extends single-reference diffusion models to multi-reference scenarios in a training-free manner. Built upon the MMDiT model, LAMIC introduces two plug-and-play attention mechanisms: 1) Group Isolation Attention (GIA) to enhance entity disentanglement; and 2) Region-Modulated Attention (RMA) to enable layout-aware generation. To comprehensively evaluate model capabilities, we further introduce three metrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout control; and 2) Background Similarity (BG-S) for measuring background consistency. Extensive experiments show that LAMIC achieves state-of-the-art performance across most major metrics: it consistently outperforms existing multi-reference baselines in ID-S, BG-S, IN-R and AVG scores across all settings, and achieves the best DPG in complex composition tasks. These results demonstrate LAMIC's superior abilities in identity keeping, background preservation, layout control, and prompt-following, all achieved without any training or fine-tuning, showcasing strong zero-shot generalization ability. By inheriting the strengths of advanced single-reference models and enabling seamless extension to multi-image scenarios, LAMIC establishes a new training-free paradigm for controllable multi-image composition. As foundation models continue to evolve, LAMIC's performance is expected to scale accordingly. Our implementation is available at: https://github.com/Suchenl/LAMIC.", "AI": {"tldr": "LAMIC\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u5e03\u5c40\u611f\u77e5\u591a\u56fe\u50cf\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u79cd\u6ce8\u610f\u529b\u673a\u5236\uff08GIA\u548cRMA\uff09\u5b9e\u73b0\u591a\u53c2\u8003\u573a\u666f\u4e0b\u7684\u56fe\u50cf\u751f\u6210\uff0c\u5e76\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u591a\u53c2\u8003\u56fe\u50cf\u5408\u6210\u4e2d\u4fdd\u6301\u7a7a\u95f4\u5e03\u5c40\u4e00\u81f4\u6027\u548c\u8fde\u8d2f\u6027\u7684\u6311\u6218\u3002", "method": "\u57fa\u4e8eMMDiT\u6a21\u578b\uff0c\u5f15\u5165Group Isolation Attention (GIA)\u548cRegion-Modulated Attention (RMA)\u4e24\u79cd\u6ce8\u610f\u529b\u673a\u5236\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5728ID-S\u3001BG-S\u3001IN-R\u548cAVG\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "LAMIC\u4e3a\u53ef\u63a7\u591a\u56fe\u50cf\u5408\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u9700\u8bad\u7ec3\u8303\u5f0f\uff0c\u5177\u6709\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.00548", "pdf": "https://arxiv.org/pdf/2508.00548", "abs": "https://arxiv.org/abs/2508.00548", "authors": ["Seunghyun Shin", "Dongmin Shin", "Jisu Shin", "Hae-Gon Jeon", "Joon-Young Lee"], "title": "Video Color Grading via Look-Up Table Generation", "categories": ["cs.CV"], "comment": "ICCV2025", "summary": "Different from color correction and transfer, color grading involves adjusting colors for artistic or storytelling purposes in a video, which is used to establish a specific look or mood. However, due to the complexity of the process and the need for specialized editing skills, video color grading remains primarily the domain of professional colorists. In this paper, we present a reference-based video color grading framework. Our key idea is explicitly generating a look-up table (LUT) for color attribute alignment between reference scenes and input video via a diffusion model. As a training objective, we enforce that high-level features of the reference scenes like look, mood, and emotion should be similar to that of the input video. Our LUT-based approach allows for color grading without any loss of structural details in the whole video frames as well as achieving fast inference. We further build a pipeline to incorporate a user-preference via text prompts for low-level feature enhancement such as contrast and brightness, etc. Experimental results, including extensive user studies, demonstrate the effectiveness of our approach for video color grading. Codes are publicly available at https://github.com/seunghyuns98/VideoColorGrading.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53c2\u8003\u7684\u89c6\u9891\u8272\u5f69\u5206\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u67e5\u627e\u8868\uff08LUT\uff09\u5b9e\u73b0\u8272\u5f69\u5c5e\u6027\u5bf9\u9f50\uff0c\u5e76\u7ed3\u5408\u7528\u6237\u504f\u597d\u6587\u672c\u63d0\u793a\u8fdb\u884c\u4f4e\u7ea7\u7279\u5f81\u589e\u5f3a\u3002", "motivation": "\u89c6\u9891\u8272\u5f69\u5206\u7ea7\u901a\u5e38\u9700\u8981\u4e13\u4e1a\u6280\u80fd\uff0c\u672c\u6587\u65e8\u5728\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u4f7f\u5176\u66f4\u6613\u4e8e\u975e\u4e13\u4e1a\u4eba\u58eb\u64cd\u4f5c\u3002", "method": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210LUT\u4ee5\u5bf9\u9f50\u53c2\u8003\u573a\u666f\u4e0e\u8f93\u5165\u89c6\u9891\u7684\u8272\u5f69\u5c5e\u6027\uff0c\u5e76\u7ed3\u5408\u6587\u672c\u63d0\u793a\u8c03\u6574\u5bf9\u6bd4\u5ea6\u548c\u4eae\u5ea6\u7b49\u4f4e\u7ea7\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u548c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5b9e\u73b0\u89c6\u9891\u8272\u5f69\u5206\u7ea7\uff0c\u4e14\u4e0d\u635f\u5931\u7ed3\u6784\u7ec6\u8282\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u89c6\u9891\u8272\u5f69\u5206\u7ea7\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.00552", "pdf": "https://arxiv.org/pdf/2508.00552", "abs": "https://arxiv.org/abs/2508.00552", "authors": ["Chihan Huang", "Belal Alsinglawi", "Islam Al-qudah"], "title": "DBLP: Noise Bridge Consistency Distillation For Efficient And Reliable Adversarial Purification", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in deep neural networks (DNNs) have led to remarkable success across a wide range of tasks. However, their susceptibility to adversarial perturbations remains a critical vulnerability. Existing diffusion-based adversarial purification methods often require intensive iterative denoising, severely limiting their practical deployment. In this paper, we propose Diffusion Bridge Distillation for Purification (DBLP), a novel and efficient diffusion-based framework for adversarial purification. Central to our approach is a new objective, noise bridge distillation, which constructs a principled alignment between the adversarial noise distribution and the clean data distribution within a latent consistency model (LCM). To further enhance semantic fidelity, we introduce adaptive semantic enhancement, which fuses multi-scale pyramid edge maps as conditioning input to guide the purification process. Extensive experiments across multiple datasets demonstrate that DBLP achieves state-of-the-art (SOTA) robust accuracy, superior image quality, and around 0.2s inference time, marking a significant step toward real-time adversarial purification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDBLP\u7684\u9ad8\u6548\u6269\u6563\u5bf9\u6297\u51c0\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u566a\u58f0\u6865\u84b8\u998f\u548c\u81ea\u9002\u5e94\u8bed\u4e49\u589e\u5f3a\u5b9e\u73b0\u5feb\u901f\u4e14\u9ad8\u8d28\u91cf\u7684\u51c0\u5316\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6613\u53d7\u5bf9\u6297\u6270\u52a8\u653b\u51fb\uff0c\u73b0\u6709\u6269\u6563\u51c0\u5316\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u566a\u58f0\u6865\u84b8\u998f\u76ee\u6807\uff0c\u6784\u5efa\u5bf9\u6297\u566a\u58f0\u4e0e\u5e72\u51c0\u6570\u636e\u7684\u5206\u5e03\u5bf9\u9f50\uff0c\u5e76\u7ed3\u5408\u591a\u5c3a\u5ea6\u8fb9\u7f18\u56fe\u589e\u5f3a\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u9c81\u68d2\u7cbe\u5ea6\u548c\u56fe\u50cf\u8d28\u91cf\uff0c\u63a8\u7406\u65f6\u95f4\u7ea60.2\u79d2\u3002", "conclusion": "DBLP\u4e3a\u5b9e\u65f6\u5bf9\u6297\u51c0\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00558", "pdf": "https://arxiv.org/pdf/2508.00558", "abs": "https://arxiv.org/abs/2508.00558", "authors": ["Jens U. Kreber", "Joerg Stueckler"], "title": "Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted for publication at the IEEE/CVF International Conference on   Computer Vision (ICCV), 2025", "summary": "Articulated objects are an important type of interactable objects in everyday environments. In this paper, we propose PhysNAP, a novel diffusion model-based approach for generating articulated objects that aligns them with partial point clouds and improves their physical plausibility. The model represents part shapes by signed distance functions (SDFs). We guide the reverse diffusion process using a point cloud alignment loss computed using the predicted SDFs. Additionally, we impose non-penetration and mobility constraints based on the part SDFs for guiding the model to generate more physically plausible objects. We also make our diffusion approach category-aware to further improve point cloud alignment if category information is available. We evaluate the generative ability and constraint consistency of samples generated with PhysNAP using the PartNet-Mobility dataset. We also compare it with an unguided baseline diffusion model and demonstrate that PhysNAP can improve constraint consistency and provides a tradeoff with generative ability.", "AI": {"tldr": "PhysNAP\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u4e0e\u90e8\u5206\u70b9\u4e91\u5bf9\u9f50\u4e14\u7269\u7406\u5408\u7406\u7684\u94f0\u63a5\u7269\u4f53\u3002", "motivation": "\u94f0\u63a5\u7269\u4f53\u662f\u65e5\u5e38\u73af\u5883\u4e2d\u91cd\u8981\u7684\u53ef\u4ea4\u4e92\u5bf9\u8c61\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u7269\u7406\u5408\u7406\u6027\u548c\u70b9\u4e91\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528SDF\u8868\u793a\u90e8\u5206\u5f62\u72b6\uff0c\u901a\u8fc7\u70b9\u4e91\u5bf9\u9f50\u635f\u5931\u548c\u975e\u7a7f\u900f\u3001\u79fb\u52a8\u6027\u7ea6\u675f\u5f15\u5bfc\u6269\u6563\u8fc7\u7a0b\uff0c\u5e76\u5f15\u5165\u7c7b\u522b\u4fe1\u606f\u4f18\u5316\u5bf9\u9f50\u3002", "result": "\u5728PartNet-Mobility\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cPhysNAP\u5728\u7ea6\u675f\u4e00\u81f4\u6027\u548c\u751f\u6210\u80fd\u529b\u4e0a\u4f18\u4e8e\u65e0\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u3002", "conclusion": "PhysNAP\u80fd\u6709\u6548\u63d0\u5347\u94f0\u63a5\u7269\u4f53\u751f\u6210\u7684\u7269\u7406\u5408\u7406\u6027\u548c\u70b9\u4e91\u5bf9\u9f50\u6548\u679c\uff0c\u540c\u65f6\u5e73\u8861\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2508.00590", "pdf": "https://arxiv.org/pdf/2508.00590", "abs": "https://arxiv.org/abs/2508.00590", "authors": ["Yihe Tian", "Kwan Man Cheng", "Zhengbo Zhang", "Tao Zhang", "Suju Li", "Dongmei Yan", "Bing Xu"], "title": "A Novel Modeling Framework and Data Product for Extended VIIRS-like Artificial Nighttime Light Image Reconstruction (1986-2024)", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Artificial Night-Time Light (NTL) remote sensing is a vital proxy for quantifying the intensity and spatial distribution of human activities. Although the NPP-VIIRS sensor provides high-quality NTL observations, its temporal coverage, which begins in 2012, restricts long-term time-series studies that extend to earlier periods. Despite the progress in extending VIIRS-like NTL time-series, current methods still suffer from two significant shortcomings: the underestimation of light intensity and the structural omission. To overcome these limitations, we propose a novel reconstruction framework consisting of a two-stage process: construction and refinement. The construction stage features a Hierarchical Fusion Decoder (HFD) designed to enhance the fidelity of the initial reconstruction. The refinement stage employs a Dual Feature Refiner (DFR), which leverages high-resolution impervious surface masks to guide and enhance fine-grained structural details. Based on this framework, we developed the Extended VIIRS-like Artificial Nighttime Light (EVAL) product for China, extending the standard data record backwards by 26 years to begin in 1986. Quantitative evaluation shows that EVAL significantly outperforms existing state-of-the-art products, boosting the $\\text{R}^2$ from 0.68 to 0.80 while lowering the RMSE from 1.27 to 0.99. Furthermore, EVAL exhibits excellent temporal consistency and maintains a high correlation with socioeconomic parameters, confirming its reliability for long-term analysis. The resulting EVAL dataset provides a valuable new resource for the research community and is publicly available at https://doi.org/10.11888/HumanNat.tpdc.302930.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591c\u95f4\u706f\u5149\uff08NTL\uff09\u91cd\u5efa\u6846\u67b6EVAL\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f4e\u4f30\u5149\u5f3a\u548c\u7ed3\u6784\u7f3a\u5931\u7684\u95ee\u9898\uff0c\u5c06VIIRS\u6570\u636e\u6269\u5c55\u52301986\u5e74\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4ea7\u54c1\u3002", "motivation": "\u73b0\u6709VIIRS\u6570\u636e\u59cb\u4e8e2012\u5e74\uff0c\u9650\u5236\u4e86\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u7814\u7a76\uff0c\u4e14\u5f53\u524d\u6269\u5c55\u65b9\u6cd5\u5b58\u5728\u5149\u5f3a\u4f4e\u4f30\u548c\u7ed3\u6784\u7f3a\u5931\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u91cd\u5efa\u6846\u67b6\uff1a\u6784\u5efa\u9636\u6bb5\u4f7f\u7528\u5206\u5c42\u878d\u5408\u89e3\u7801\u5668\uff08HFD\uff09\u63d0\u9ad8\u521d\u59cb\u91cd\u5efa\u4fdd\u771f\u5ea6\uff1b\u7ec6\u5316\u9636\u6bb5\u5229\u7528\u53cc\u7279\u5f81\u7ec6\u5316\u5668\uff08DFR\uff09\u7ed3\u5408\u9ad8\u5206\u8fa8\u7387\u4e0d\u900f\u6c34\u9762\u63a9\u819c\u589e\u5f3a\u7ec6\u8282\u3002", "result": "EVAL\u4ea7\u54c1\u5c06VIIRS\u6570\u636e\u6269\u5c55\u52301986\u5e74\uff0cR\u00b2\u4ece0.68\u63d0\u5347\u81f30.80\uff0cRMSE\u4ece1.27\u964d\u81f30.99\uff0c\u5177\u6709\u4f18\u5f02\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u793e\u4f1a\u7ecf\u6d4e\u53c2\u6570\u76f8\u5173\u6027\u3002", "conclusion": "EVAL\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u957f\u671f\u5206\u6790\u8d44\u6e90\uff0c\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2508.00591", "pdf": "https://arxiv.org/pdf/2508.00591", "abs": "https://arxiv.org/abs/2508.00591", "authors": ["Mingrui Liu", "Sixiao Zhang", "Cheng Long"], "title": "Wukong Framework for Not Safe For Work Detection in Text-to-Image systems", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": "Under review", "summary": "Text-to-Image (T2I) generation is a popular AI-generated content (AIGC) technology enabling diverse and creative image synthesis. However, some outputs may contain Not Safe For Work (NSFW) content (e.g., violence), violating community guidelines. Detecting NSFW content efficiently and accurately, known as external safeguarding, is essential. Existing external safeguards fall into two types: text filters, which analyze user prompts but overlook T2I model-specific variations and are prone to adversarial attacks; and image filters, which analyze final generated images but are computationally costly and introduce latency. Diffusion models, the foundation of modern T2I systems like Stable Diffusion, generate images through iterative denoising using a U-Net architecture with ResNet and Transformer blocks. We observe that: (1) early denoising steps define the semantic layout of the image, and (2) cross-attention layers in U-Net are crucial for aligning text and image regions. Based on these insights, we propose Wukong, a transformer-based NSFW detection framework that leverages intermediate outputs from early denoising steps and reuses U-Net's pre-trained cross-attention parameters. Wukong operates within the diffusion process, enabling early detection without waiting for full image generation. We also introduce a new dataset containing prompts, seeds, and image-specific NSFW labels, and evaluate Wukong on this and two public benchmarks. Results show that Wukong significantly outperforms text-based safeguards and achieves comparable accuracy of image filters, while offering much greater efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faWukong\uff0c\u4e00\u79cd\u57fa\u4e8eTransformer\u7684NSFW\u68c0\u6d4b\u6846\u67b6\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u65e9\u671f\u53bb\u566a\u6b65\u9aa4\u7684\u4e2d\u95f4\u8f93\u51fa\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u51c6\u786e\u7684NSFW\u5185\u5bb9\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u5916\u90e8\u5b89\u5168\u63aa\u65bd\uff08\u6587\u672c\u8fc7\u6ee4\u5668\u548c\u56fe\u50cf\u8fc7\u6ee4\u5668\uff09\u5b58\u5728\u4e0d\u8db3\uff1a\u6587\u672c\u8fc7\u6ee4\u5668\u6613\u53d7\u5bf9\u6297\u653b\u51fb\uff0c\u56fe\u50cf\u8fc7\u6ee4\u5668\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5ef6\u8fdf\u5927\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u51c6\u786e\u7684NSFW\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "Wukong\u5229\u7528\u6269\u6563\u6a21\u578b\u65e9\u671f\u53bb\u566a\u6b65\u9aa4\u7684\u4e2d\u95f4\u8f93\u51fa\uff0c\u5e76\u91cd\u7528U-Net\u9884\u8bad\u7ec3\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u53c2\u6570\uff0c\u5b9e\u73b0\u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u65e9\u671f\u68c0\u6d4bNSFW\u5185\u5bb9\u3002", "result": "Wukong\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u6587\u672c\u7684\u5b89\u5168\u63aa\u65bd\uff0c\u5e76\u4e0e\u56fe\u50cf\u8fc7\u6ee4\u5668\u51c6\u786e\u7387\u76f8\u5f53\uff0c\u540c\u65f6\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "Wukong\u4e3aT2I\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684NSFW\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002"}}
{"id": "2508.00599", "pdf": "https://arxiv.org/pdf/2508.00599", "abs": "https://arxiv.org/abs/2508.00599", "authors": ["Junzhe Lu", "Jing Lin", "Hongkun Dou", "Ailing Zeng", "Yue Deng", "Xian Liu", "Zhongang Cai", "Lei Yang", "Yulun Zhang", "Haoqian Wang", "Ziwei Liu"], "title": "DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior", "categories": ["cs.CV"], "comment": "ICCV 2025 (oral); Code released: https://github.com/moonbow721/DPoser", "summary": "We present DPoser-X, a diffusion-based prior model for 3D whole-body human poses. Building a versatile and robust full-body human pose prior remains challenging due to the inherent complexity of articulated human poses and the scarcity of high-quality whole-body pose datasets. To address these limitations, we introduce a Diffusion model as body Pose prior (DPoser) and extend it to DPoser-X for expressive whole-body human pose modeling. Our approach unifies various pose-centric tasks as inverse problems, solving them through variational diffusion sampling. To enhance performance on downstream applications, we introduce a novel truncated timestep scheduling method specifically designed for pose data characteristics. We also propose a masked training mechanism that effectively combines whole-body and part-specific datasets, enabling our model to capture interdependencies between body parts while avoiding overfitting to specific actions. Extensive experiments demonstrate DPoser-X's robustness and versatility across multiple benchmarks for body, hand, face, and full-body pose modeling. Our model consistently outperforms state-of-the-art alternatives, establishing a new benchmark for whole-body human pose prior modeling.", "AI": {"tldr": "DPoser-X\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u76843D\u5168\u8eab\u4eba\u4f53\u59ff\u6001\u5148\u9a8c\u6a21\u578b\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u53d8\u5206\u6269\u6563\u91c7\u6837\u89e3\u51b3\u59ff\u6001\u4efb\u52a1\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u4eba\u4f53\u59ff\u6001\u7684\u590d\u6742\u6027\u548c\u9ad8\u8d28\u91cf\u5168\u8eab\u59ff\u6001\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u6027\uff0c\u6784\u5efa\u4e00\u4e2a\u901a\u7528\u4e14\u9c81\u68d2\u7684\u5168\u8eab\u59ff\u6001\u5148\u9a8c\u6a21\u578b\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faDPoser-X\u6a21\u578b\uff0c\u91c7\u7528\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u59ff\u6001\u5148\u9a8c\uff0c\u5f15\u5165\u622a\u65ad\u65f6\u95f4\u6b65\u8c03\u5ea6\u548c\u63a9\u7801\u8bad\u7ec3\u673a\u5236\uff0c\u7ed3\u5408\u5168\u8eab\u548c\u5c40\u90e8\u6570\u636e\u96c6\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u5168\u8eab\u59ff\u6001\u5148\u9a8c\u5efa\u6a21\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\u3002", "conclusion": "DPoser-X\u5c55\u793a\u4e86\u5728\u5168\u8eab\u4eba\u4f53\u59ff\u6001\u5efa\u6a21\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u901a\u7528\u6027\uff0c\u4e3a\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00639", "pdf": "https://arxiv.org/pdf/2508.00639", "abs": "https://arxiv.org/abs/2508.00639", "authors": ["Luisa Gall\u00e9e", "Catharina Silvia Lisson", "Christoph Gerhard Lisson", "Daniela Drees", "Felix Weig", "Daniel Vogele", "Meinrad Beer", "Michael G\u00f6tz"], "title": "Minimum Data, Maximum Impact: 20 annotated samples for explainable lung nodule classification", "categories": ["cs.CV"], "comment": "Accepted at iMIMIC - Interpretability of Machine Intelligence in   Medical Image Computing workshop MICCAI 2025 Medical Image Computing and   Computer Assisted Intervention", "summary": "Classification models that provide human-interpretable explanations enhance clinicians' trust and usability in medical image diagnosis. One research focus is the integration and prediction of pathology-related visual attributes used by radiologists alongside the diagnosis, aligning AI decision-making with clinical reasoning. Radiologists use attributes like shape and texture as established diagnostic criteria and mirroring these in AI decision-making both enhances transparency and enables explicit validation of model outputs. However, the adoption of such models is limited by the scarcity of large-scale medical image datasets annotated with these attributes. To address this challenge, we propose synthesizing attribute-annotated data using a generative model. We enhance the Diffusion Model with attribute conditioning and train it using only 20 attribute-labeled lung nodule samples from the LIDC-IDRI dataset. Incorporating its generated images into the training of an explainable model boosts performance, increasing attribute prediction accuracy by 13.4% and target prediction accuracy by 1.8% compared to training with only the small real attribute-annotated dataset. This work highlights the potential of synthetic data to overcome dataset limitations, enhancing the applicability of explainable models in medical image analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u751f\u6210\u6a21\u578b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u5c5e\u6027\u6807\u6ce8\u6570\u636e\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u5347\u53ef\u89e3\u91ca\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u9700\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u4ee5\u589e\u5f3a\u4e34\u5e8a\u4fe1\u4efb\uff0c\u4f46\u5c5e\u6027\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u9650\u5236\u4e86\u6b64\u7c7b\u6a21\u578b\u7684\u5e94\u7528\u3002", "method": "\u6539\u8fdb\u6269\u6563\u6a21\u578b\u4ee5\u751f\u6210\u5c5e\u6027\u6807\u6ce8\u6570\u636e\uff0c\u4ec5\u9700\u5c11\u91cf\u771f\u5b9e\u6807\u6ce8\u6837\u672c\uff0820\u4e2a\u80ba\u7ed3\u8282\u6837\u672c\uff09\uff0c\u5e76\u5c06\u5176\u7528\u4e8e\u8bad\u7ec3\u53ef\u89e3\u91ca\u6a21\u578b\u3002", "result": "\u5408\u6210\u6570\u636e\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5c5e\u6027\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u9ad813.4%\uff0c\u76ee\u6807\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u9ad81.8%\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u53ef\u6709\u6548\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63a8\u52a8\u53ef\u89e3\u91ca\u6a21\u578b\u5728\u533b\u5b66\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2508.00698", "pdf": "https://arxiv.org/pdf/2508.00698", "abs": "https://arxiv.org/abs/2508.00698", "authors": ["Hongfei Zhang", "Kun Zhou", "Ruizheng Wu", "Jiangbo Lu"], "title": "Can Large Pretrained Depth Estimation Models Help With Image Dehazing?", "categories": ["cs.CV"], "comment": "Submitted to AAAI2026", "summary": "Image dehazing remains a challenging problem due to the spatially varying nature of haze in real-world scenes. While existing methods have demonstrated the promise of large-scale pretrained models for image dehazing, their architecture-specific designs hinder adaptability across diverse scenarios with different accuracy and efficiency requirements. In this work, we systematically investigate the generalization capability of pretrained depth representations-learned from millions of diverse images-for image dehazing. Our empirical analysis reveals that the learned deep depth features maintain remarkable consistency across varying haze levels. Building on this insight, we propose a plug-and-play RGB-D fusion module that seamlessly integrates with diverse dehazing architectures. Extensive experiments across multiple benchmarks validate both the effectiveness and broad applicability of our approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u6df1\u5ea6\u7279\u5f81\u7684\u56fe\u50cf\u53bb\u96fe\u65b9\u6cd5\uff0c\u901a\u8fc7RGB-D\u878d\u5408\u6a21\u5757\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u53bb\u96fe\u65b9\u6cd5\u56e0\u67b6\u6784\u7279\u5b9a\u8bbe\u8ba1\u96be\u4ee5\u9002\u5e94\u591a\u6837\u573a\u666f\uff0c\u9884\u8bad\u7ec3\u6df1\u5ea6\u7279\u5f81\u7684\u4e00\u81f4\u6027\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u7814\u7a76\u9884\u8bad\u7ec3\u6df1\u5ea6\u7279\u5f81\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u51fa\u53ef\u63d2\u62d4\u7684RGB-D\u878d\u5408\u6a21\u5757\uff0c\u9002\u914d\u591a\u79cd\u53bb\u96fe\u67b6\u6784\u3002", "result": "\u591a\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "\u9884\u8bad\u7ec3\u6df1\u5ea6\u7279\u5f81\u7ed3\u5408RGB-D\u6a21\u5757\u80fd\u663e\u8457\u63d0\u5347\u53bb\u96fe\u6027\u80fd\uff0c\u9002\u5e94\u4e0d\u540c\u573a\u666f\u9700\u6c42\u3002"}}
{"id": "2508.00701", "pdf": "https://arxiv.org/pdf/2508.00701", "abs": "https://arxiv.org/abs/2508.00701", "authors": ["Chende Zheng", "Ruiqi suo", "Chenhao Lin", "Zhengyu Zhao", "Le Yang", "Shuai Liu", "Minghui Yang", "Cong Wang", "Chao Shen"], "title": "D3: Training-Free AI-Generated Video Detection Using Second-Order Features", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 4 figures", "summary": "The evolution of video generation techniques, such as Sora, has made it increasingly easy to produce high-fidelity AI-generated videos, raising public concern over the dissemination of synthetic content. However, existing detection methodologies remain limited by their insufficient exploration of temporal artifacts in synthetic videos. To bridge this gap, we establish a theoretical framework through second-order dynamical analysis under Newtonian mechanics, subsequently extending the Second-order Central Difference features tailored for temporal artifact detection. Building on this theoretical foundation, we reveal a fundamental divergence in second-order feature distributions between real and AI-generated videos. Concretely, we propose Detection by Difference of Differences (D3), a novel training-free detection method that leverages the above second-order temporal discrepancies. We validate the superiority of our D3 on 4 open-source datasets (Gen-Video, VideoPhy, EvalCrafter, VidProM), 40 subsets in total. For example, on GenVideo, D3 outperforms the previous best method by 10.39% (absolute) mean Average Precision. Additional experiments on time cost and post-processing operations demonstrate D3's exceptional computational efficiency and strong robust performance. Our code is available at https://github.com/Zig-HS/D3.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8c\u9636\u52a8\u529b\u5b66\u5206\u6790\u7684\u89c6\u9891\u68c0\u6d4b\u65b9\u6cd5D3\uff0c\u7528\u4e8e\u533a\u5206\u771f\u5b9e\u89c6\u9891\u4e0eAI\u751f\u6210\u89c6\u9891\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5bf9\u5408\u6210\u89c6\u9891\u7684\u65f6\u95f4\u4f2a\u5f71\u63a2\u7d22\u4e0d\u8db3\uff0c\u5bfc\u81f4\u68c0\u6d4b\u6548\u679c\u6709\u9650\u3002", "method": "\u901a\u8fc7\u725b\u987f\u529b\u5b66\u4e0b\u7684\u4e8c\u9636\u52a8\u529b\u5b66\u5206\u6790\u5efa\u7acb\u7406\u8bba\u6846\u67b6\uff0c\u63d0\u51faSecond-order Central Difference\u7279\u5f81\uff0c\u5e76\u5f00\u53d1\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u68c0\u6d4b\u65b9\u6cd5D3\u3002", "result": "\u57284\u4e2a\u5f00\u6e90\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86D3\u7684\u4f18\u8d8a\u6027\uff0c\u4f8b\u5982\u5728GenVideo\u4e0a\u6bd4\u4e4b\u524d\u6700\u4f73\u65b9\u6cd5\u63d0\u5347\u4e8610.39%\u7684\u5e73\u5747\u7cbe\u5ea6\u3002", "conclusion": "D3\u5728\u8ba1\u7b97\u6548\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u5408\u6210\u89c6\u9891\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.00728", "pdf": "https://arxiv.org/pdf/2508.00728", "abs": "https://arxiv.org/abs/2508.00728", "authors": ["Guanning Zeng", "Xiang Zhang", "Zirui Wang", "Haiyang Xu", "Zeyuan Chen", "Bingnan Li", "Zhuowen Tu"], "title": "YOLO-Count: Differentiable Object Counting for Text-to-Image Generation", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "We propose YOLO-Count, a differentiable open-vocabulary object counting model that tackles both general counting challenges and enables precise quantity control for text-to-image (T2I) generation. A core contribution is the 'cardinality' map, a novel regression target that accounts for variations in object size and spatial distribution. Leveraging representation alignment and a hybrid strong-weak supervision scheme, YOLO-Count bridges the gap between open-vocabulary counting and T2I generation control. Its fully differentiable architecture facilitates gradient-based optimization, enabling accurate object count estimation and fine-grained guidance for generative models. Extensive experiments demonstrate that YOLO-Count achieves state-of-the-art counting accuracy while providing robust and effective quantity control for T2I systems.", "AI": {"tldr": "YOLO-Count\u662f\u4e00\u4e2a\u53ef\u5fae\u5206\u7684\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u8ba1\u6570\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u901a\u7528\u8ba1\u6570\u95ee\u9898\uff0c\u5e76\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u7cbe\u786e\u7684\u6570\u91cf\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u5f00\u653e\u8bcd\u6c47\u8ba1\u6570\u4e0e\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u63a7\u5236\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u5bf9\u8c61\u6570\u91cf\u4f30\u8ba1\u548c\u751f\u6210\u6307\u5bfc\u3002", "method": "\u63d0\u51fa'\u57fa\u6570'\u56fe\u4f5c\u4e3a\u56de\u5f52\u76ee\u6807\uff0c\u7ed3\u5408\u8868\u793a\u5bf9\u9f50\u548c\u6df7\u5408\u5f3a\u5f31\u76d1\u7763\u65b9\u6848\uff0c\u91c7\u7528\u5b8c\u5168\u53ef\u5fae\u5206\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660eYOLO-Count\u5728\u8ba1\u6570\u51c6\u786e\u6027\u548c\u751f\u6210\u63a7\u5236\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "YOLO-Count\u6210\u529f\u5b9e\u73b0\u4e86\u5f00\u653e\u8bcd\u6c47\u8ba1\u6570\u4e0e\u751f\u6210\u6a21\u578b\u7684\u7cbe\u786e\u6570\u91cf\u63a7\u5236\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.00750", "pdf": "https://arxiv.org/pdf/2508.00750", "abs": "https://arxiv.org/abs/2508.00750", "authors": ["Prerana Ramkumar"], "title": "SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Generative Adversarial Networks (GANs) have achieved realistic super-resolution (SR) of images however, they lack semantic consistency and per-pixel confidence, limiting their credibility in critical remote sensing applications such as disaster response, urban planning and agriculture. This paper introduces Semantic and Uncertainty-Aware ESRGAN (SU-ESRGAN), the first SR framework designed for satellite imagery to integrate the ESRGAN, segmentation loss via DeepLabv3 for class detail preservation and Monte Carlo dropout to produce pixel-wise uncertainty maps. The SU-ESRGAN produces results (PSNR, SSIM, LPIPS) comparable to the Baseline ESRGAN on aerial imagery. This novel model is valuable in satellite systems or UAVs that use wide field-of-view (FoV) cameras, trading off spatial resolution for coverage. The modular design allows integration in UAV data pipelines for on-board or post-processing SR to enhance imagery resulting due to motion blur, compression and sensor limitations. Further, the model is fine-tuned to evaluate its performance on cross domain applications. The tests are conducted on two drone based datasets which differ in altitude and imaging perspective. Performance evaluation of the fine-tuned models show a stronger adaptation to the Aerial Maritime Drone Dataset, whose imaging characteristics align with the training data, highlighting the importance of domain-aware training in SR-applications.", "AI": {"tldr": "SU-ESRGAN\u662f\u4e00\u79cd\u9488\u5bf9\u536b\u661f\u56fe\u50cf\u7684\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u7ed3\u5408\u4e86ESRGAN\u3001DeepLabv3\u5206\u5272\u635f\u5931\u548c\u8499\u7279\u5361\u6d1b\u4e22\u5f03\uff0c\u751f\u6210\u50cf\u7d20\u7ea7\u4e0d\u786e\u5b9a\u6027\u5730\u56fe\uff0c\u63d0\u5347\u4e86\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "GANs\u5728\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u50cf\u7d20\u7ea7\u7f6e\u4fe1\u5ea6\uff0c\u9650\u5236\u4e86\u5176\u5728\u9065\u611f\u5173\u952e\u5e94\u7528\u4e2d\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u63d0\u51faSU-ESRGAN\u6846\u67b6\uff0c\u6574\u5408ESRGAN\u3001DeepLabv3\u5206\u5272\u635f\u5931\u548c\u8499\u7279\u5361\u6d1b\u4e22\u5f03\uff0c\u751f\u6210\u4e0d\u786e\u5b9a\u6027\u5730\u56fe\u3002", "result": "\u5728PSNR\u3001SSIM\u548cLPIPS\u6307\u6807\u4e0a\u4e0e\u57fa\u7ebfESRGAN\u76f8\u5f53\uff0c\u9002\u7528\u4e8e\u65e0\u4eba\u673a\u548c\u536b\u661f\u7cfb\u7edf\u3002", "conclusion": "SU-ESRGAN\u5728\u8de8\u57df\u5e94\u7528\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5f3a\u8c03\u4e86\u9886\u57df\u611f\u77e5\u8bad\u7ec3\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.00766", "pdf": "https://arxiv.org/pdf/2508.00766", "abs": "https://arxiv.org/abs/2508.00766", "authors": ["Irene Iele", "Francesco Di Feola", "Valerio Guarrasi", "Paolo Soda"], "title": "Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Image-to-image translation has emerged as a powerful technique in medical imaging, enabling tasks such as image denoising and cross-modality conversion. However, it suffers from limitations in handling out-of-distribution samples without causing performance degradation. To address this limitation, we propose a novel Test-Time Adaptation (TTA) framework that dynamically adjusts the translation process based on the characteristics of each test sample. Our method introduces a Reconstruction Module to quantify the domain shift and a Dynamic Adaptation Block that selectively modifies the internal features of a pretrained translation model to mitigate the shift without compromising the performance on in-distribution samples that do not require adaptation. We evaluate our approach on two medical image-to-image translation tasks: low-dose CT denoising and T1 to T2 MRI translation, showing consistent improvements over both the baseline translation model without TTA and prior TTA methods. Our analysis highlights the limitations of the state-of-the-art that uniformly apply the adaptation to both out-of-distribution and in-distribution samples, demonstrating that dynamic, sample-specific adjustment offers a promising path to improve model resilience in real-world scenarios. The code is available at: https://github.com/cosbidev/Sample-Aware_TTA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\uff08TTA\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u7ffb\u8bd1\u4efb\u52a1\uff0c\u901a\u8fc7\u91cf\u5316\u57df\u504f\u79fb\u548c\u9009\u62e9\u6027\u8c03\u6574\u6a21\u578b\u7279\u5f81\uff0c\u63d0\u5347\u5bf9\u5206\u5e03\u5916\u6837\u672c\u7684\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u56fe\u50cf\u7ffb\u8bd1\u6280\u672f\u5728\u5904\u7406\u5206\u5e03\u5916\u6837\u672c\u65f6\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u91cd\u5efa\u6a21\u5757\u91cf\u5316\u57df\u504f\u79fb\uff0c\u5e76\u4f7f\u7528\u52a8\u6001\u9002\u5e94\u5757\u9009\u62e9\u6027\u8c03\u6574\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5185\u90e8\u7279\u5f81\u3002", "result": "\u5728\u4f4e\u5242\u91cfCT\u53bb\u566a\u548cT1\u5230T2 MRI\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u548c\u73b0\u6709TTA\u65b9\u6cd5\u3002", "conclusion": "\u52a8\u6001\u6837\u672c\u7279\u5f02\u6027\u8c03\u6574\u662f\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2508.00230", "pdf": "https://arxiv.org/pdf/2508.00230", "abs": "https://arxiv.org/abs/2508.00230", "authors": ["Paul Albert", "Frederic Z. Zhang", "Hemanth Saratchandran", "Anton van den Hengel", "Ehsan Abbasnejad"], "title": "Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "To appear in ICCV 2025", "summary": "Parameter-efficient fine-tuning (PEFT) has become a standard approach for adapting large pre-trained models. Amongst PEFT methods, low-rank adaptation (LoRA) has achieved notable success. However, recent studies have highlighted its limitations compared against full-rank alternatives, particularly when applied to multimodal and large language models. In this work, we present a quantitative comparison amongst full-rank and low-rank PEFT methods using a synthetic matrix approximation benchmark with controlled spectral properties. Our results confirm that LoRA struggles to approximate matrices with relatively flat spectrums or high frequency components -- signs of high effective ranks. To this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the Khatri-Rao product to produce weight updates, which, by construction, tends to produce matrix product with a high effective rank. We demonstrate performance gains with KRAdapter on vision-language models up to 1B parameters and on large language models up to 8B parameters, particularly on unseen common-sense reasoning tasks. In addition, KRAdapter maintains the memory and compute efficiency of LoRA, making it a practical and robust alternative to fine-tune billion-scale parameter models.", "AI": {"tldr": "KRAdapter\u662f\u4e00\u79cd\u65b0\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7Khatri-Rao\u79ef\u751f\u6210\u6743\u91cd\u66f4\u65b0\uff0c\u89e3\u51b3\u4e86LoRA\u5728\u8fd1\u4f3c\u9ad8\u6709\u6548\u79e9\u77e9\u9635\u65f6\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "LoRA\u5728\u591a\u6a21\u6001\u548c\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5c40\u9650\u6027\u4fc3\u4f7f\u7814\u7a76\u66f4\u9ad8\u6548\u7684PEFT\u65b9\u6cd5\u3002", "method": "\u63d0\u51faKRAdapter\uff0c\u5229\u7528Khatri-Rao\u79ef\u751f\u6210\u9ad8\u6709\u6548\u79e9\u7684\u6743\u91cd\u66f4\u65b0\uff0c\u5e76\u4e0eLoRA\u8fdb\u884c\u5b9a\u91cf\u6bd4\u8f83\u3002", "result": "KRAdapter\u57281B\u53c2\u6570\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c8B\u53c2\u6570\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u4e8eLoRA\uff0c\u5c24\u5176\u5728\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e2d\u3002", "conclusion": "KRAdapter\u5728\u4fdd\u6301LoRA\u9ad8\u6548\u6027\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u662f\u5fae\u8c03\u5927\u89c4\u6a21\u6a21\u578b\u7684\u5b9e\u7528\u9009\u62e9\u3002"}}
{"id": "2508.00250", "pdf": "https://arxiv.org/pdf/2508.00250", "abs": "https://arxiv.org/abs/2508.00250", "authors": ["Victor D. Martinez", "Vidya Manian", "Sudhir Malik"], "title": "Jet Image Generation in High Energy Physics Using Diffusion Models", "categories": ["hep-ph", "cs.AI", "cs.CV", "cs.LG"], "comment": "The paper is under review at IEEE Transactions in Nuclear Science", "summary": "This article presents, for the first time, the application of diffusion models for generating jet images corresponding to proton-proton collision events at the Large Hadron Collider (LHC). The kinematic variables of quark, gluon, W-boson, Z-boson, and top quark jets from the JetNet simulation dataset are mapped to two-dimensional image representations. Diffusion models are trained on these images to learn the spatial distribution of jet constituents. We compare the performance of score-based diffusion models and consistency models in accurately generating class-conditional jet images. Unlike approaches based on latent distributions, our method operates directly in image space. The fidelity of the generated images is evaluated using several metrics, including the Fr\\'echet Inception Distance (FID), which demonstrates that consistency models achieve higher fidelity and generation stability compared to score-based diffusion models. These advancements offer significant improvements in computational efficiency and generation accuracy, providing valuable tools for High Energy Physics (HEP) research.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5c06\u6269\u6563\u6a21\u578b\u5e94\u7528\u4e8e\u751f\u6210LHC\u8d28\u5b50-\u8d28\u5b50\u78b0\u649e\u4e8b\u4ef6\u7684\u55b7\u6ce8\u56fe\u50cf\uff0c\u6bd4\u8f83\u4e86\u57fa\u4e8e\u5206\u6570\u548c\u4e00\u81f4\u6027\u6a21\u578b\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u540e\u8005\u5728\u751f\u6210\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\u4e0a\u66f4\u4f18\u3002", "motivation": "\u63a2\u7d22\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u9ad8\u80fd\u7269\u7406\u55b7\u6ce8\u56fe\u50cf\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u6539\u8fdb\u8ba1\u7b97\u6548\u7387\u548c\u751f\u6210\u51c6\u786e\u6027\u3002", "method": "\u5c06\u55b7\u6ce8\u7684\u52a8\u529b\u5b66\u53d8\u91cf\u6620\u5c04\u4e3a\u4e8c\u7ef4\u56fe\u50cf\uff0c\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u5b66\u4e60\u55b7\u6ce8\u6210\u5206\u7684\u7a7a\u95f4\u5206\u5e03\uff0c\u6bd4\u8f83\u5206\u6570\u6269\u6563\u6a21\u578b\u548c\u4e00\u81f4\u6027\u6a21\u578b\u7684\u8868\u73b0\u3002", "result": "\u4e00\u81f4\u6027\u6a21\u578b\u5728\u751f\u6210\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u5206\u6570\u6269\u6563\u6a21\u578b\uff0cFID\u8bc4\u5206\u66f4\u9ad8\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u4e3a\u9ad8\u80fd\u7269\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u55b7\u6ce8\u56fe\u50cf\u751f\u6210\u5de5\u5177\uff0c\u4e00\u81f4\u6027\u6a21\u578b\u8868\u73b0\u66f4\u4f73\u3002"}}
{"id": "2508.00438", "pdf": "https://arxiv.org/pdf/2508.00438", "abs": "https://arxiv.org/abs/2508.00438", "authors": ["Sumin Seo", "In Kyu Lee", "Hyun-Woo Kim", "Jaesik Min", "Chung-Hwan Jung"], "title": "Diffusion-Based User-Guided Data Augmentation for Coronary Stenosis Detection", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted at MICCAI 2025. Dataset available at   https://github.com/medipixel/DiGDA", "summary": "Coronary stenosis is a major risk factor for ischemic heart events leading to increased mortality, and medical treatments for this condition require meticulous, labor-intensive analysis. Coronary angiography provides critical visual cues for assessing stenosis, supporting clinicians in making informed decisions for diagnosis and treatment. Recent advances in deep learning have shown great potential for automated localization and severity measurement of stenosis. In real-world scenarios, however, the success of these competent approaches is often hindered by challenges such as limited labeled data and class imbalance. In this study, we propose a novel data augmentation approach that uses an inpainting method based on a diffusion model to generate realistic lesions, allowing user-guided control of severity. Extensive evaluation on lesion detection and severity classification across various synthetic dataset sizes shows superior performance of our method on both a large-scale in-house dataset and a public coronary angiography dataset. Furthermore, our approach maintains high detection and classification performance even when trained with limited data, highlighting its clinical importance in improving the assessment of severity of stenosis and optimizing data utilization for more reliable decision support.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u903c\u771f\u7684\u51a0\u72b6\u52a8\u8109\u72ed\u7a84\u75c5\u53d8\uff0c\u4ee5\u89e3\u51b3\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u51a0\u72b6\u52a8\u8109\u72ed\u7a84\u662f\u7f3a\u8840\u6027\u5fc3\u810f\u4e8b\u4ef6\u7684\u4e3b\u8981\u98ce\u9669\u56e0\u7d20\uff0c\u4f20\u7edf\u533b\u7597\u5206\u6790\u8017\u65f6\u4e14\u52b3\u52a8\u5bc6\u96c6\uff0c\u6df1\u5ea6\u5b66\u4e60\u867d\u5177\u6f5c\u529b\u4f46\u53d7\u9650\u4e8e\u6570\u636e\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6269\u6563\u6a21\u578b\u7684\u4fee\u590d\u65b9\u6cd5\u751f\u6210\u7528\u6237\u53ef\u63a7\u5236\u4e25\u91cd\u7a0b\u5ea6\u7684\u75c5\u53d8\uff0c\u589e\u5f3a\u6570\u636e\u591a\u6837\u6027\u3002", "result": "\u5728\u5927\u89c4\u6a21\u5185\u90e8\u548c\u516c\u5171\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u75c5\u53d8\u68c0\u6d4b\u548c\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u6570\u636e\u6709\u9650\u65f6\u4ecd\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u72ed\u7a84\u4e25\u91cd\u7a0b\u5ea6\u8bc4\u4f30\u7684\u53ef\u9760\u6027\uff0c\u4f18\u5316\u4e86\u6570\u636e\u5229\u7528\uff0c\u5177\u6709\u91cd\u8981\u4e34\u5e8a\u610f\u4e49\u3002"}}
{"id": "2508.00669", "pdf": "https://arxiv.org/pdf/2508.00669", "abs": "https://arxiv.org/abs/2508.00669", "authors": ["Wenxuan Wang", "Zizhan Ma", "Meidan Ding", "Shiyi Zheng", "Shengyuan Liu", "Jie Liu", "Jiaming Ji", "Wenting Chen", "Xiang Li", "Linlin Shen", "Yixuan Yuan"], "title": "Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "The proliferation of Large Language Models (LLMs) in medicine has enabled impressive capabilities, yet a critical gap remains in their ability to perform systematic, transparent, and verifiable reasoning, a cornerstone of clinical practice. This has catalyzed a shift from single-step answer generation to the development of LLMs explicitly designed for medical reasoning. This paper provides the first systematic review of this emerging field. We propose a taxonomy of reasoning enhancement techniques, categorized into training-time strategies (e.g., supervised fine-tuning, reinforcement learning) and test-time mechanisms (e.g., prompt engineering, multi-agent systems). We analyze how these techniques are applied across different data modalities (text, image, code) and in key clinical applications such as diagnosis, education, and treatment planning. Furthermore, we survey the evolution of evaluation benchmarks from simple accuracy metrics to sophisticated assessments of reasoning quality and visual interpretability. Based on an analysis of 60 seminal studies from 2022-2025, we conclude by identifying critical challenges, including the faithfulness-plausibility gap and the need for native multimodal reasoning, and outlining future directions toward building efficient, robust, and sociotechnically responsible medical AI.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u533b\u5b66\u63a8\u7406\u9886\u57df\u7684\u53d1\u5c55\uff0c\u63d0\u51fa\u4e86\u589e\u5f3a\u63a8\u7406\u6280\u672f\u7684\u5206\u7c7b\u6cd5\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u6570\u636e\u6a21\u6001\u548c\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u6280\u672f\u5e94\u7528\uff0c\u5e76\u603b\u7ed3\u4e86\u5173\u952e\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u533b\u5b66\u5b9e\u8df5\u4e2d\u7cfb\u7edf\u3001\u900f\u660e\u548c\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u662f\u6838\u5fc3\uff0c\u4f46\u73b0\u6709LLMs\u5728\u6b64\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u7528\u4e8e\u533b\u5b66\u63a8\u7406\u7684LLMs\u3002", "method": "\u63d0\u51fa\u8bad\u7ec3\u65f6\u548c\u6d4b\u8bd5\u65f6\u7684\u63a8\u7406\u589e\u5f3a\u6280\u672f\u5206\u7c7b\u6cd5\uff0c\u5206\u679060\u9879\u5173\u952e\u7814\u7a76\uff082022-2025\u5e74\uff09\uff0c\u6db5\u76d6\u4e0d\u540c\u6570\u636e\u6a21\u6001\u548c\u4e34\u5e8a\u5e94\u7528\u3002", "result": "\u603b\u7ed3\u4e86\u4ece\u7b80\u5355\u51c6\u786e\u6027\u5230\u590d\u6742\u63a8\u7406\u8d28\u91cf\u8bc4\u4f30\u7684\u8bc4\u6d4b\u57fa\u51c6\u6f14\u53d8\uff0c\u5e76\u8bc6\u522b\u4e86\u5173\u952e\u6311\u6218\uff08\u5982\u5fe0\u5b9e\u6027\u4e0e\u5408\u7406\u6027\u5dee\u8ddd\uff09\u3002", "conclusion": "\u672a\u6765\u9700\u6784\u5efa\u9ad8\u6548\u3001\u7a33\u5065\u4e14\u793e\u4f1a\u6280\u672f\u8d23\u4efb\u5f3a\u7684\u533b\u5b66AI\uff0c\u91cd\u70b9\u5173\u6ce8\u539f\u751f\u591a\u6a21\u6001\u63a8\u7406\u7b49\u65b9\u5411\u3002"}}
{"id": "2508.00697", "pdf": "https://arxiv.org/pdf/2508.00697", "abs": "https://arxiv.org/abs/2508.00697", "authors": ["Yiming Wu", "Huan Wang", "Zhenghao Chen", "Jianxin Pang", "Dong Xu"], "title": "On-Device Diffusion Transformer Policy for Efficient Robot Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "ICCV 2025", "summary": "Diffusion Policies have significantly advanced robotic manipulation tasks via imitation learning, but their application on resource-constrained mobile platforms remains challenging due to computational inefficiency and extensive memory footprint. In this paper, we propose LightDP, a novel framework specifically designed to accelerate Diffusion Policies for real-time deployment on mobile devices. LightDP addresses the computational bottleneck through two core strategies: network compression of the denoising modules and reduction of the required sampling steps. We first conduct an extensive computational analysis on existing Diffusion Policy architectures, identifying the denoising network as the primary contributor to latency. To overcome performance degradation typically associated with conventional pruning methods, we introduce a unified pruning and retraining pipeline, optimizing the model's post-pruning recoverability explicitly. Furthermore, we combine pruning techniques with consistency distillation to effectively reduce sampling steps while maintaining action prediction accuracy. Experimental evaluations on the standard datasets, \\ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that LightDP achieves real-time action prediction on mobile devices with competitive performance, marking an important step toward practical deployment of diffusion-based policies in resource-limited environments. Extensive real-world experiments also show the proposed LightDP can achieve performance comparable to state-of-the-art Diffusion Policies.", "AI": {"tldr": "LightDP\u662f\u4e00\u4e2a\u4e13\u4e3a\u79fb\u52a8\u8bbe\u5907\u8bbe\u8ba1\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u538b\u7f29\u53bb\u566a\u6a21\u5757\u548c\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\uff0c\u52a0\u901fDiffusion Policies\u7684\u5b9e\u65f6\u90e8\u7f72\u3002", "motivation": "Diffusion Policies\u5728\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u5e73\u53f0\u4e0a\u5e94\u7528\u65f6\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u4f4e\u548c\u5185\u5b58\u5360\u7528\u5927\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7f51\u7edc\u538b\u7f29\u548c\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\u7684\u7b56\u7565\uff0c\u7ed3\u5408\u7edf\u4e00\u7684\u526a\u679d\u548c\u518d\u8bad\u7ec3\u6d41\u7a0b\u4ee5\u53ca\u4e00\u81f4\u6027\u84b8\u998f\u6280\u672f\u3002", "result": "\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\uff0cLightDP\u5b9e\u73b0\u4e86\u5b9e\u65f6\u52a8\u4f5c\u9884\u6d4b\uff0c\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7684Diffusion Policies\u76f8\u5f53\u3002", "conclusion": "LightDP\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u57fa\u4e8e\u6269\u6563\u7684\u7b56\u7565\u7684\u5b9e\u9645\u90e8\u7f72\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2508.00721", "pdf": "https://arxiv.org/pdf/2508.00721", "abs": "https://arxiv.org/abs/2508.00721", "authors": ["Yuxiang Wan", "Ryan Devera", "Wenjie Zhang", "Ju Sun"], "title": "FMPlug: Plug-In Foundation Flow-Matching Priors for Inverse Problems", "categories": ["eess.IV", "cs.CV", "cs.LG", "eess.SP"], "comment": null, "summary": "We present FMPlug, a novel plug-in framework that enhances foundation flow-matching (FM) priors for solving ill-posed inverse problems. Unlike traditional approaches that rely on domain-specific or untrained priors, FMPlug smartly leverages two simple but powerful insights: the similarity between observed and desired objects and the Gaussianity of generative flows. By introducing a time-adaptive warm-up strategy and sharp Gaussianity regularization, FMPlug unlocks the true potential of domain-agnostic foundation models. Our method beats state-of-the-art methods that use foundation FM priors by significant margins, on image super-resolution and Gaussian deblurring.", "AI": {"tldr": "FMPlug\u662f\u4e00\u79cd\u65b0\u578b\u63d2\u4ef6\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u89c2\u5bdf\u5bf9\u8c61\u4e0e\u76ee\u6807\u5bf9\u8c61\u7684\u76f8\u4f3c\u6027\u4ee5\u53ca\u751f\u6210\u6d41\u7684\u9ad8\u65af\u6027\uff0c\u589e\u5f3a\u57fa\u7840\u6d41\u5339\u914d\u5148\u9a8c\uff0c\u89e3\u51b3\u4e0d\u9002\u5b9a\u9006\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9886\u57df\u7279\u5b9a\u6216\u65e0\u8bad\u7ec3\u5148\u9a8c\uff0cFMPlug\u65e8\u5728\u901a\u8fc7\u901a\u7528\u57fa\u7840\u6a21\u578b\u66f4\u9ad8\u6548\u5730\u89e3\u51b3\u9006\u95ee\u9898\u3002", "method": "\u5f15\u5165\u65f6\u95f4\u81ea\u9002\u5e94\u9884\u70ed\u7b56\u7565\u548c\u9510\u5229\u9ad8\u65af\u6027\u6b63\u5219\u5316\uff0c\u4f18\u5316\u57fa\u7840\u6d41\u5339\u914d\u5148\u9a8c\u3002", "result": "\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u548c\u9ad8\u65af\u53bb\u6a21\u7cca\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FMPlug\u901a\u8fc7\u7b80\u5355\u800c\u5f3a\u5927\u7684\u6d1e\u5bdf\u529b\uff0c\u91ca\u653e\u4e86\u9886\u57df\u65e0\u5173\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u529b\u3002"}}
