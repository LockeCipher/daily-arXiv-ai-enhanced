<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 5]
- [cs.CV](#cs.CV) [Total: 18]
- [eess.IV](#eess.IV) [Total: 3]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Spatiotemporally Consistent Indoor Lighting Estimation with Diffusion Priors](https://arxiv.org/abs/2508.08384)
*Mutian Tong,Rundi Wu,Changxi Zheng*

Main category: cs.GR

TL;DR: 提出一种从视频中估计连续光场的方法，利用2D扩散先验优化MLP表示的光场，并在真实场景中实现零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 室内光照估计因其高度不适定性而具有挑战性，尤其是当场景光照条件在空间和时间上变化时。

Method: 通过微调预训练的图像扩散模型，联合修复多个铬球作为光探针，预测多位置的光照。

Result: 在单图像或视频的室内光照估计中表现优于基线方法，尤其在真实视频中实现了时空一致的光照估计。

Conclusion: 该方法在真实场景中表现出色，解决了以往工作中难以实现的时空一致光照估计问题。

Abstract: Indoor lighting estimation from a single image or video remains a challenge due to its highly ill-posed nature, especially when the lighting condition of the scene varies spatially and temporally. We propose a method that estimates from an input video a continuous light field describing the spatiotemporally varying lighting of the scene. We leverage 2D diffusion priors for optimizing such light field represented as a MLP. To enable zero-shot generalization to in-the-wild scenes, we fine-tune a pre-trained image diffusion model to predict lighting at multiple locations by jointly inpainting multiple chrome balls as light probes. We evaluate our method on indoor lighting estimation from a single image or video and show superior performance over compared baselines. Most importantly, we highlight results on spatiotemporally consistent lighting estimation from in-the-wild videos, which is rarely demonstrated in previous works.

</details>


### [2] [Hybrid Long and Short Range Flows for Point Cloud Filtering](https://arxiv.org/abs/2508.08542)
*Dasith de Silva Edirimuni,Xuequan Lu,Ajmal Saeed Mian,Lei Wei,Gang Li,Scott Schaefer,Ying He*

Main category: cs.GR

TL;DR: HybridPF提出了一种结合短程和长程滤波轨迹的点云去噪方法，通过并行模块和动态图卷积解码器优化性能。


<details>
  <summary>Details</summary>
Motivation: 现有滤波方法存在点聚类或噪声残留问题，需要一种更高效的去噪方法。

Method: 设计ShortModule和LongModule并行模块，结合短程分数和长程流信息，并采用动态图卷积解码器。

Result: HybridPF在去噪效果和推理速度上达到最优。

Conclusion: HybridPF通过结合短程和长程信息，显著提升了点云去噪的性能和效率。

Abstract: Point cloud capture processes are error-prone and introduce noisy artifacts that necessitate filtering/denoising. Recent filtering methods often suffer from point clustering or noise retaining issues. In this paper, we propose Hybrid Point Cloud Filtering ($\textbf{HybridPF}$) that considers both short-range and long-range filtering trajectories when removing noise. It is well established that short range scores, given by $\nabla_{x}\log p(x_t)$, may provide the necessary displacements to move noisy points to the underlying clean surface. By contrast, long range velocity flows approximate constant displacements directed from a high noise variant patch $x_0$ towards the corresponding clean surface $x_1$. Here, noisy patches $x_t$ are viewed as intermediate states between the high noise variant and the clean patches. Our intuition is that long range information from velocity flow models can guide the short range scores to align more closely with the clean points. In turn, score models generally provide a quicker convergence to the clean surface. Specifically, we devise two parallel modules, the ShortModule and LongModule, each consisting of an Encoder-Decoder pair to respectively account for short-range scores and long-range flows. We find that short-range scores, guided by long-range features, yield filtered point clouds with good point distributions and convergence near the clean surface. We design a joint loss function to simultaneously train the ShortModule and LongModule, in an end-to-end manner. Finally, we identify a key weakness in current displacement based methods, limitations on the decoder architecture, and propose a dynamic graph convolutional decoder to improve the inference process. Comprehensive experiments demonstrate that our HybridPF achieves state-of-the-art results while enabling faster inference speed.

</details>


### [3] [Geometry-Aware Global Feature Aggregation for Real-Time Indirect Illumination](https://arxiv.org/abs/2508.08826)
*Meng Gai,Guoping Wang,Sheng Li*

Main category: cs.GR

TL;DR: 提出了一种基于学习的屏幕空间漫反射间接光照估计方法，结合直接光照生成全局光照的高动态范围结果。


<details>
  <summary>Details</summary>
Motivation: 实时渲染中全局光照对用户体验至关重要，但传统方法难以捕捉长距离间接光照。

Method: 采用新型神经网络架构，结合改进的注意力机制和单色设计，预测间接光照。

Result: 实验表明，该方法优于现有学习技术，能有效处理复杂光照和新场景。

Conclusion: 该方法在全局光照渲染中表现出色，具有广泛适用性。

Abstract: Real-time rendering with global illumination is crucial to afford the user realistic experience in virtual environments. We present a learning-based estimator to predict diffuse indirect illumination in screen space, which then is combined with direct illumination to synthesize globally-illuminated high dynamic range (HDR) results. Our approach tackles the challenges of capturing long-range/long-distance indirect illumination when employing neural networks and is generalized to handle complex lighting and scenarios.   From the neural network thinking of the solver to the rendering equation, we present a novel network architecture to predict indirect illumination. Our network is equipped with a modified attention mechanism that aggregates global information guided by spacial geometry features, as well as a monochromatic design that encodes each color channel individually.   We conducted extensive evaluations, and the experimental results demonstrate our superiority over previous learning-based techniques. Our approach excels at handling complex lighting such as varying-colored lighting and environment lighting. It can successfully capture distant indirect illumination and simulates the interreflections between textured surfaces well (i.e., color bleeding effects); it can also effectively handle new scenes that are not present in the training dataset.

</details>


### [4] [DiffPhysCam: Differentiable Physics-Based Camera Simulation for Inverse Rendering and Embodied AI](https://arxiv.org/abs/2508.08831)
*Bo-Hsun Chen,Nevindu M. Batagoda,Dan Negrut*

Main category: cs.GR

TL;DR: DiffPhysCam是一种可微分相机模拟器，支持机器人学和具身AI应用，通过梯度优化提升视觉感知管道的性能。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟相机在控制相机设置、模拟光学伪影和可调校准参数方面存在不足，限制了从模拟到现实的迁移。DiffPhysCam旨在解决这些问题。

Method: DiffPhysCam采用多阶段管道，提供对相机设置的精细控制，模拟关键光学效果（如散焦模糊），并支持基于真实数据的校准。

Result: DiffPhysCam在合成图像任务中提升了机器人感知性能，并成功用于逆向渲染创建数字孪生场景。

Conclusion: DiffPhysCam通过可微分渲染和精细控制，为机器人视觉和具身AI提供了有效的工具。

Abstract: We introduce DiffPhysCam, a differentiable camera simulator designed to support robotics and embodied AI applications by enabling gradient-based optimization in visual perception pipelines. Generating synthetic images that closely mimic those from real cameras is essential for training visual models and enabling end-to-end visuomotor learning. Moreover, differentiable rendering allows inverse reconstruction of real-world scenes as digital twins, facilitating simulation-based robotics training. However, existing virtual cameras offer limited control over intrinsic settings, poorly capture optical artifacts, and lack tunable calibration parameters -- hindering sim-to-real transfer. DiffPhysCam addresses these limitations through a multi-stage pipeline that provides fine-grained control over camera settings, models key optical effects such as defocus blur, and supports calibration with real-world data. It enables both forward rendering for image synthesis and inverse rendering for 3D scene reconstruction, including mesh and material texture optimization. We show that DiffPhysCam enhances robotic perception performance in synthetic image tasks. As an illustrative example, we create a digital twin of a real-world scene using inverse rendering, simulate it in a multi-physics environment, and demonstrate navigation of an autonomous ground vehicle using images generated by DiffPhysCam.

</details>


### [5] [Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer](https://arxiv.org/abs/2508.09131)
*Zixin Yin,Xili Dai,Ling-Hao Chen,Deyu Zhou,Jianan Wang,Duomin Wang,Gang Yu,Lionel M. Ni,Heung-Yeung Shum*

Main category: cs.GR

TL;DR: ColorCtrl是一种无需训练的文本引导颜色编辑方法，利用多模态扩散变换器的注意力机制，实现精确且一致的颜色编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法在精确颜色控制和视觉一致性方面表现不佳，需要一种更有效的解决方案。

Method: 通过解耦结构和颜色，并针对性操作注意力图和值标记，实现精确编辑。

Result: 在SD3和FLUX.1-dev上表现优于现有方法，并在视频模型中保持时间一致性。

Conclusion: ColorCtrl在编辑质量和一致性上达到最先进水平，并适用于多种扩散模型。

Abstract: Text-guided color editing in images and videos is a fundamental yet unsolved problem, requiring fine-grained manipulation of color attributes, including albedo, light source color, and ambient lighting, while preserving physical consistency in geometry, material properties, and light-matter interactions. Existing training-free methods offer broad applicability across editing tasks but struggle with precise color control and often introduce visual inconsistency in both edited and non-edited regions. In this work, we present ColorCtrl, a training-free color editing method that leverages the attention mechanisms of modern Multi-Modal Diffusion Transformers (MM-DiT). By disentangling structure and color through targeted manipulation of attention maps and value tokens, our method enables accurate and consistent color editing, along with word-level control of attribute intensity. Our method modifies only the intended regions specified by the prompt, leaving unrelated areas untouched. Extensive experiments on both SD3 and FLUX.1-dev demonstrate that ColorCtrl outperforms existing training-free approaches and achieves state-of-the-art performances in both edit quality and consistency. Furthermore, our method surpasses strong commercial models such as FLUX.1 Kontext Max and GPT-4o Image Generation in terms of consistency. When extended to video models like CogVideoX, our approach exhibits greater advantages, particularly in maintaining temporal coherence and editing stability. Finally, our method also generalizes to instruction-based editing diffusion models such as Step1X-Edit and FLUX.1 Kontext dev, further demonstrating its versatility.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [6] [MAViS: A Multi-Agent Framework for Long-Sequence Video Storytelling](https://arxiv.org/abs/2508.08487)
*Qian Wang,Ziqi Huang,Ruoxi Jia,Paul Debevec,Ning Yu*

Main category: cs.CV

TL;DR: MAViS是一个多代理协作框架，用于生成长序列视频故事，解决了现有框架在辅助能力、视觉质量和表现力上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有长序列视频生成框架存在辅助能力差、视觉质量不佳和表现力有限的问题，MAViS旨在解决这些限制。

Method: MAViS通过多阶段代理协作（如脚本编写、镜头设计、角色建模等），遵循3E原则（探索、检查、增强），并优化脚本与生成工具的兼容性。

Result: 实验表明，MAViS在辅助能力、视觉质量和表现力上达到最先进水平，并能生成多模态设计输出（视频、叙事和背景音乐）。

Conclusion: MAViS是首个提供多模态设计输出的框架，仅需简短用户提示即可生成高质量长序列视频故事。

Abstract: Despite recent advances, long-sequence video generation frameworks still suffer from significant limitations: poor assistive capability, suboptimal visual quality, and limited expressiveness. To mitigate these limitations, we propose MAViS, an end-to-end multi-agent collaborative framework for long-sequence video storytelling. MAViS orchestrates specialized agents across multiple stages, including script writing, shot designing, character modeling, keyframe generation, video animation, and audio generation. In each stage, agents operate under the 3E Principle -- Explore, Examine, and Enhance -- to ensure the completeness of intermediate outputs. Considering the capability limitations of current generative models, we propose the Script Writing Guidelines to optimize compatibility between scripts and generative tools. Experimental results demonstrate that MAViS achieves state-of-the-art performance in assistive capability, visual quality, and video expressiveness. Its modular framework further enables scalability with diverse generative models and tools. With just a brief user prompt, MAViS is capable of producing high-quality, expressive long-sequence video storytelling, enriching inspirations and creativity for users. To the best of our knowledge, MAViS is the only framework that provides multimodal design output -- videos with narratives and background music.

</details>


### [7] [CObL: Toward Zero-Shot Ordinal Layering without User Prompting](https://arxiv.org/abs/2508.08498)
*Aneel Damaraju,Dean Hazineh,Todd Zickler*

Main category: cs.CV

TL;DR: CObL是一种基于扩散的架构，通过并行生成对象层堆栈，利用Stable Diffusion作为先验，无需用户提示即可重建多个遮挡对象，并能泛化到真实世界场景。


<details>
  <summary>Details</summary>
Motivation: 视觉任务需要将像素分组为对象并理解其空间关系（包括遮挡关系）。现有方法在无需用户提示或预先知道对象数量的情况下难以完成多对象重建，且泛化能力有限。

Method: 提出CObL架构，利用扩散模型并行生成对象层堆栈，通过Stable Diffusion作为先验，并使用推理时指导确保层堆栈能合成回输入图像。训练数据为合成的多对象桌面场景图像。

Result: CObL无需用户提示即可重建多个遮挡对象，并能零样本泛化到真实世界桌面场景的新对象。

Conclusion: CObL在无需先验知识和用户干预的情况下，实现了多对象重建和泛化能力，优于现有方法。

Abstract: Vision benefits from grouping pixels into objects and understanding their spatial relationships, both laterally and in depth. We capture this with a scene representation comprising an occlusion-ordered stack of "object layers," each containing an isolated and amodally-completed object. To infer this representation from an image, we introduce a diffusion-based architecture named Concurrent Object Layers (CObL). CObL generates a stack of object layers in parallel, using Stable Diffusion as a prior for natural objects and inference-time guidance to ensure the inferred layers composite back to the input image. We train CObL using a few thousand synthetically-generated images of multi-object tabletop scenes, and we find that it zero-shot generalizes to photographs of real-world tabletops with varying numbers of novel objects. In contrast to recent models for amodal object completion, CObL reconstructs multiple occluded objects without user prompting and without knowing the number of objects beforehand. Unlike previous models for unsupervised object-centric representation learning, CObL is not limited to the world it was trained in.

</details>


### [8] [Unlocking the Potential of Diffusion Priors in Blind Face Restoration](https://arxiv.org/abs/2508.08556)
*Yunqi Miao,Zhiyu Qu,Mingqi Gao,Changrui Chen,Jifei Song,Jungong Han,Jiankang Deng*

Main category: cs.CV

TL;DR: FLIPNET通过切换恢复和退化模式，解决了扩散模型在盲人脸恢复中的适应性问题，提升了真实性和保真度。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在盲人脸恢复中存在适应性问题，主要源于高质量与低质量图像、合成与真实图像之间的差异。

Method: 使用FLIPNET网络，切换恢复模式和退化模式，分别处理图像恢复和模拟真实退化。

Result: 在基准数据集上表现优异，优于现有扩散模型和退化模型。

Conclusion: FLIPNET有效解决了扩散模型在盲人脸恢复中的适应性问题，提升了性能。

Abstract: Although diffusion prior is rising as a powerful solution for blind face restoration (BFR), the inherent gap between the vanilla diffusion model and BFR settings hinders its seamless adaptation. The gap mainly stems from the discrepancy between 1) high-quality (HQ) and low-quality (LQ) images and 2) synthesized and real-world images. The vanilla diffusion model is trained on images with no or less degradations, whereas BFR handles moderately to severely degraded images. Additionally, LQ images used for training are synthesized by a naive degradation model with limited degradation patterns, which fails to simulate complex and unknown degradations in real-world scenarios. In this work, we use a unified network FLIPNET that switches between two modes to resolve specific gaps. In Restoration mode, the model gradually integrates BFR-oriented features and face embeddings from LQ images to achieve authentic and faithful face restoration. In Degradation mode, the model synthesizes real-world like degraded images based on the knowledge learned from real-world degradation datasets. Extensive evaluations on benchmark datasets show that our model 1) outperforms previous diffusion prior based BFR methods in terms of authenticity and fidelity, and 2) outperforms the naive degradation model in modeling the real-world degradations.

</details>


### [9] [RealisMotion: Decomposed Human Motion Control and Video Generation in the World Space](https://arxiv.org/abs/2508.08588)
*Jingyun Liang,Jingkai Zhou,Shikai Li,Chenjie Cao,Lei Sun,Yichen Qian,Weihua Chen,Fan Wang*

Main category: cs.CV

TL;DR: 提出了一种分解的人体运动控制和视频生成框架，通过分离运动与外观、主体与背景、动作与轨迹，实现灵活组合。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法独立控制视频的四个关键元素（前景主体、背景视频、人体轨迹和动作模式），限制了视频生成的灵活性。

Method: 构建地面感知的3D世界坐标系，在3D空间中进行运动编辑；通过2D轨迹反投影到3D实现轨迹控制，动作由运动库或文本生成；基于扩散变换模型，注入主体、背景和运动控制信号。

Result: 在基准数据集和实际案例中，方法在元素可控性和视频质量上达到最先进水平。

Conclusion: 该方法实现了对视频元素的灵活控制，生成高质量视频，具有广泛应用潜力。

Abstract: Generating human videos with realistic and controllable motions is a challenging task. While existing methods can generate visually compelling videos, they lack separate control over four key video elements: foreground subject, background video, human trajectory and action patterns. In this paper, we propose a decomposed human motion control and video generation framework that explicitly decouples motion from appearance, subject from background, and action from trajectory, enabling flexible mix-and-match composition of these elements. Concretely, we first build a ground-aware 3D world coordinate system and perform motion editing directly in the 3D space. Trajectory control is implemented by unprojecting edited 2D trajectories into 3D with focal-length calibration and coordinate transformation, followed by speed alignment and orientation adjustment; actions are supplied by a motion bank or generated via text-to-motion methods. Then, based on modern text-to-video diffusion transformer models, we inject the subject as tokens for full attention, concatenate the background along the channel dimension, and add motion (trajectory and action) control signals by addition. Such a design opens up the possibility for us to generate realistic videos of anyone doing anything anywhere. Extensive experiments on benchmark datasets and real-world cases demonstrate that our method achieves state-of-the-art performance on both element-wise controllability and overall video quality.

</details>


### [10] [Yan: Foundational Interactive Video Generation](https://arxiv.org/abs/2508.08601)
*Yan Team*

Main category: cs.CV

TL;DR: Yan是一个交互式视频生成的基础框架，涵盖从模拟、生成到编辑的完整流程，包含三个核心模块：AAA级模拟、多模态生成和多粒度编辑。


<details>
  <summary>Details</summary>
Motivation: 推动交互式视频生成从孤立功能向全面的AI驱动创作范式发展，为下一代创意工具、媒体和娱乐铺平道路。

Method: 1. AAA级模拟：采用高压缩、低延迟的3D-VAE和KV缓存窗口去噪推理；2. 多模态生成：结合分层自回归标注和开放域视频扩散模型；3. 多粒度编辑：通过混合模型解耦交互机制与视觉渲染。

Result: 实现实时1080P/60FPS交互模拟，支持跨域风格和机制灵活组合，并能通过文本进行多粒度编辑。

Conclusion: Yan通过整合三大模块，为交互式视频生成提供了全面的解决方案，具有广泛的应用潜力。

Abstract: We present Yan, a foundational framework for interactive video generation, covering the entire pipeline from simulation and generation to editing. Specifically, Yan comprises three core modules. AAA-level Simulation: We design a highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based shift-window denoising inference process, achieving real-time 1080P/60FPS interactive simulation. Multi-Modal Generation: We introduce a hierarchical autoregressive caption method that injects game-specific knowledge into open-domain multi-modal video diffusion models (VDMs), then transforming the VDM into a frame-wise, action-controllable, real-time infinite interactive video generator. Notably, when the textual and visual prompts are sourced from different domains, the model demonstrates strong generalization, allowing it to blend and compose the style and mechanics across domains flexibly according to user prompts. Multi-Granularity Editing: We propose a hybrid model that explicitly disentangles interactive mechanics simulation from visual rendering, enabling multi-granularity video content editing during interaction through text. Collectively, Yan offers an integration of these modules, pushing interactive video generation beyond isolated capabilities toward a comprehensive AI-driven interactive creation paradigm, paving the way for the next generation of creative tools, media, and entertainment. The project page is: https://greatx3.github.io/Yan/.

</details>


### [11] [SelfHVD: Self-Supervised Handheld Video Deblurring for Mobile Phones](https://arxiv.org/abs/2508.08605)
*Honglei Xu,Zhilu Zhang,Junjie Fan,Xiaohe Wu,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 提出一种自监督的手持视频去模糊方法，利用视频中的清晰线索作为训练标签，并通过自增强和空间一致性约束提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决手持设备拍摄视频模糊问题，尤其是训练与测试数据之间的模糊域差距。

Method: 1. 提取视频中的清晰线索作为训练标签；2. 提出自增强视频去模糊（SEVD）方法生成高质量配对数据；3. 提出自约束空间一致性维护（SCSCM）方法防止输出帧位置偏移。

Result: 在合成和真实手持视频数据集上显著优于现有自监督方法。

Conclusion: 该方法通过自监督和增强策略有效提升了手持视频去模糊性能，代码和数据集已开源。

Abstract: Shooting video with a handheld mobile phone, the most common photographic device, often results in blurry frames due to shaking hands and other instability factors. Although previous video deblurring methods have achieved impressive progress, they still struggle to perform satisfactorily on real-world handheld video due to the blur domain gap between training and testing data. To address the issue, we propose a self-supervised method for handheld video deblurring, which is driven by sharp clues in the video. First, to train the deblurring model, we extract the sharp clues from the video and take them as misalignment labels of neighboring blurry frames. Second, to improve the model's ability, we propose a novel Self-Enhanced Video Deblurring (SEVD) method to create higher-quality paired video data. Third, we propose a Self-Constrained Spatial Consistency Maintenance (SCSCM) method to regularize the model, preventing position shifts between the output and input frames. Moreover, we construct a synthetic and a real-world handheld video dataset for handheld video deblurring. Extensive experiments on these two and other common real-world datasets demonstrate that our method significantly outperforms existing self-supervised ones. The code and datasets are publicly available at https://github.com/cshonglei/SelfHVD.

</details>


### [12] [SafeFix: Targeted Model Repair via Controlled Image Generation](https://arxiv.org/abs/2508.08701)
*Ouyang Xu,Baoming Zhang,Ruiyu Mao,Yunhui Guo*

Main category: cs.CV

TL;DR: 论文提出了一种利用条件文本到图像模型和大型视觉语言模型生成针对性修复图像的方法，显著减少了视觉识别模型在罕见子群体上的错误。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在视觉识别中因语义子群体代表性不足而导致的系统性错误，现有方法难以有效修复这些错误。

Method: 结合可解释的失败归因流程，使用条件文本到图像模型生成语义准确的修复图像，并通过大型视觉语言模型过滤输出以确保数据分布和语义一致性。

Result: 实验表明，该方法显著减少了模型在罕见情况下的错误，提升了鲁棒性且未引入新问题。

Conclusion: 提出的针对性修复策略有效提升了视觉识别模型的性能，尤其在罕见子群体上表现突出。

Abstract: Deep learning models for visual recognition often exhibit systematic errors due to underrepresented semantic subpopulations. Although existing debugging frameworks can pinpoint these failures by identifying key failure attributes, repairing the model effectively remains difficult. Current solutions often rely on manually designed prompts to generate synthetic training images -- an approach prone to distribution shift and semantic errors. To overcome these challenges, we introduce a model repair module that builds on an interpretable failure attribution pipeline. Our approach uses a conditional text-to-image model to generate semantically faithful and targeted images for failure cases. To preserve the quality and relevance of the generated samples, we further employ a large vision-language model (LVLM) to filter the outputs, enforcing alignment with the original data distribution and maintaining semantic consistency. By retraining vision models with this rare-case-augmented synthetic dataset, we significantly reduce errors associated with rare cases. Our experiments demonstrate that this targeted repair strategy improves model robustness without introducing new bugs. Code is available at https://github.com/oxu2/SafeFix

</details>


### [13] [DiffPose-Animal: A Language-Conditioned Diffusion Framework for Animal Pose Estimation](https://arxiv.org/abs/2508.08783)
*Tianyu Xiong,Dayi Tan,Wei Tian*

Main category: cs.CV

TL;DR: DiffPose-Animal是一种基于扩散模型的动物姿态估计新方法，利用语言模型提取解剖学先验知识，通过去噪过程逐步优化姿态预测。


<details>
  <summary>Details</summary>
Motivation: 动物姿态估计因物种形态多样性和数据标注有限而更具挑战性，传统方法难以应对。

Method: 提出DiffPose-Animal框架，将姿态估计重新定义为扩散模型中的去噪过程，结合语言模型提取的解剖学先验知识。

Result: 在公开数据集上验证了方法的有效性，尤其在物种多样性和遮挡场景下表现优异。

Conclusion: DiffPose-Animal通过结合生成模型和语义先验，显著提升了动物姿态估计的鲁棒性和泛化能力。

Abstract: Animal pose estimation is a fundamental task in computer vision, with growing importance in ecological monitoring, behavioral analysis, and intelligent livestock management. Compared to human pose estimation, animal pose estimation is more challenging due to high interspecies morphological diversity, complex body structures, and limited annotated data. In this work, we introduce DiffPose-Animal, a novel diffusion-based framework for top-down animal pose estimation. Unlike traditional heatmap regression methods, DiffPose-Animal reformulates pose estimation as a denoising process under the generative framework of diffusion models. To enhance semantic guidance during keypoint generation, we leverage large language models (LLMs) to extract both global anatomical priors and local keypoint-wise semantics based on species-specific prompts. These textual priors are encoded and fused with image features via cross-attention modules to provide biologically meaningful constraints throughout the denoising process. Additionally, a diffusion-based keypoint decoder is designed to progressively refine pose predictions, improving robustness to occlusion and annotation sparsity. Extensive experiments on public animal pose datasets demonstrate the effectiveness and generalization capability of our method, especially under challenging scenarios with diverse species, cluttered backgrounds, and incomplete keypoints.

</details>


### [14] [Region-Adaptive Video Sharpening via Rate-Perception Optimization](https://arxiv.org/abs/2508.08794)
*Yingxue Pang,Shijie Zhao,Mengxi Guo,Junlin Li,Li Zhang*

Main category: cs.CV

TL;DR: RPO-AdaSharp是一种端到端的区域自适应视频锐化模型，旨在优化锐化效果并节省比特率。


<details>
  <summary>Details</summary>
Motivation: 传统均匀锐化强度忽略了纹理变化，导致视频质量下降，同时增加了比特率，缺乏针对不同区域优化比特分配的技术。

Method: 利用编码树单元（CTU）分区掩码作为先验信息，引导和约束增加的比特分配。

Result: 实验结果表明，该模型在质量和比特率方面均表现出色。

Conclusion: RPO-AdaSharp有效解决了锐化带来的比特率增加问题，同时提升了视频质量。

Abstract: Sharpening is a widely adopted video enhancement technique. However, uniform sharpening intensity ignores texture variations, degrading video quality. Sharpening also increases bitrate, and there's a lack of techniques to optimally allocate these additional bits across diverse regions. Thus, this paper proposes RPO-AdaSharp, an end-to-end region-adaptive video sharpening model for both perceptual enhancement and bitrate savings. We use the coding tree unit (CTU) partition mask as prior information to guide and constrain the allocation of increased bits. Experiments on benchmarks demonstrate the effectiveness of the proposed model qualitatively and quantitatively.

</details>


### [15] [MonoPartNeRF:Human Reconstruction from Monocular Video via Part-Based Neural Radiance Fields](https://arxiv.org/abs/2508.08798)
*Yao Lu,Jiawei Li,Ming Jiang*

Main category: cs.CV

TL;DR: MonoPartNeRF提出了一种新的单目动态人体渲染框架，通过双向变形模型和部分姿态嵌入机制，解决了复杂姿态变化和遮挡恢复问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂姿态变化和单目设置下的遮挡恢复方面表现不佳，导致边界过渡不自然和遮挡区域重建不准确。

Method: 提出双向变形模型结合刚性和非刚性变换，引入参数化表面-时间空间采样点，结合部分姿态嵌入机制和关键帧姿态插值，通过注意力建模动态纹理变化。

Result: 在ZJU-MoCap和MonoCap数据集上，MonoPartNeRF显著优于现有方法，在复杂姿态和遮挡条件下表现出更好的关节对齐、纹理保真度和结构连续性。

Conclusion: MonoPartNeRF通过创新的变形和姿态建模方法，有效提升了单目动态人体渲染的质量和鲁棒性。

Abstract: In recent years, Neural Radiance Fields (NeRF) have achieved remarkable progress in dynamic human reconstruction and rendering. Part-based rendering paradigms, guided by human segmentation, allow for flexible parameter allocation based on structural complexity, thereby enhancing representational efficiency. However, existing methods still struggle with complex pose variations, often producing unnatural transitions at part boundaries and failing to reconstruct occluded regions accurately in monocular settings. We propose MonoPartNeRF, a novel framework for monocular dynamic human rendering that ensures smooth transitions and robust occlusion recovery. First, we build a bidirectional deformation model that combines rigid and non-rigid transformations to establish a continuous, reversible mapping between observation and canonical spaces. Sampling points are projected into a parameterized surface-time space (u, v, t) to better capture non-rigid motion. A consistency loss further suppresses deformation-induced artifacts and discontinuities. We introduce a part-based pose embedding mechanism that decomposes global pose vectors into local joint embeddings based on body regions. This is combined with keyframe pose retrieval and interpolation, along three orthogonal directions, to guide pose-aware feature sampling. A learnable appearance code is integrated via attention to model dynamic texture changes effectively. Experiments on the ZJU-MoCap and MonoCap datasets demonstrate that our method significantly outperforms prior approaches under complex pose and occlusion conditions, achieving superior joint alignment, texture fidelity, and structural continuity.

</details>


### [16] [TARA: Token-Aware LoRA for Composable Personalization in Diffusion Models](https://arxiv.org/abs/2508.08812)
*Yuqi Peng,Lingtao Zheng,Yufeng Yang,Yi Huang,Mingfu Yan,Jianzhuang Liu,Shifeng Chen*

Main category: cs.CV

TL;DR: 论文提出Token-Aware LoRA (TARA)方法，解决多概念LoRA模块组合时的身份缺失和特征泄漏问题，通过令牌掩码和空间对齐实现高效多概念生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于LoRA的方法在多概念生成时存在身份缺失和特征泄漏问题，需要改进。

Method: 提出TARA方法，引入令牌掩码约束模块专注于关联令牌，并通过训练目标实现空间对齐。

Result: 实验表明TARA能高效组合多概念并避免模块间干扰，保留各概念的视觉身份。

Conclusion: TARA通过令牌感知和空间对齐，有效解决了多概念LoRA组合的问题。

Abstract: Personalized text-to-image generation aims to synthesize novel images of a specific subject or style using only a few reference images. Recent methods based on Low-Rank Adaptation (LoRA) enable efficient single-concept customization by injecting lightweight, concept-specific adapters into pre-trained diffusion models. However, combining multiple LoRA modules for multi-concept generation often leads to identity missing and visual feature leakage. In this work, we identify two key issues behind these failures: (1) token-wise interference among different LoRA modules, and (2) spatial misalignment between the attention map of a rare token and its corresponding concept-specific region. To address these issues, we propose Token-Aware LoRA (TARA), which introduces a token mask to explicitly constrain each module to focus on its associated rare token to avoid interference, and a training objective that encourages the spatial attention of a rare token to align with its concept region. Our method enables training-free multi-concept composition by directly injecting multiple independently trained TARA modules at inference time. Experimental results demonstrate that TARA enables efficient multi-concept inference and effectively preserving the visual identity of each concept by avoiding mutual interference between LoRA modules. The code and models are available at https://github.com/YuqiPeng77/TARA.

</details>


### [17] [GaussianUpdate: Continual 3D Gaussian Splatting Update for Changing Environments](https://arxiv.org/abs/2508.08867)
*Lin Zeng,Boming Zhao,Jiarui Hu,Xujie Shen,Ziqiang Dang,Hujun Bao,Zhaopeng Cui*

Main category: cs.CV

TL;DR: GaussianUpdate提出了一种结合3D高斯表示和持续学习的新方法，用于动态场景的新视角合成，能够实时更新并保留历史信息。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么需要大量重新训练，要么无法捕捉场景随时间变化的细节，因此需要一种更高效且精确的动态场景建模方法。

Method: 采用3D高斯表示与持续学习结合，通过多阶段更新策略建模不同类型的变化，并引入可见性感知的持续学习方法。

Result: 在基准数据集上实现了实时渲染，并能够可视化不同时间点的场景变化。

Conclusion: GaussianUpdate在动态场景建模中表现出色，为实时渲染和场景变化分析提供了有效解决方案。

Abstract: Novel view synthesis with neural models has advanced rapidly in recent years, yet adapting these models to scene changes remains an open problem. Existing methods are either labor-intensive, requiring extensive model retraining, or fail to capture detailed types of changes over time. In this paper, we present GaussianUpdate, a novel approach that combines 3D Gaussian representation with continual learning to address these challenges. Our method effectively updates the Gaussian radiance fields with current data while preserving information from past scenes. Unlike existing methods, GaussianUpdate explicitly models different types of changes through a novel multi-stage update strategy. Additionally, we introduce a visibility-aware continual learning approach with generative replay, enabling self-aware updating without the need to store images. The experiments on the benchmark dataset demonstrate our method achieves superior and real-time rendering with the capability of visualizing changes over different times

</details>


### [18] [MADPromptS: Unlocking Zero-Shot Morphing Attack Detection with Multiple Prompt Aggregation](https://arxiv.org/abs/2508.08939)
*Eduarda Caldeira,Fadi Boutros,Naser Damer*

Main category: cs.CV

TL;DR: 本文提出了一种基于CLIP的零样本方法，用于检测人脸融合攻击（MAD），通过设计并聚合多个文本提示，无需额外训练即可提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 人脸融合攻击（MAD）对安全构成威胁，现有方法多依赖微调，忽视了基础模型（如CLIP）的直接泛化能力。本文旨在探索零样本方法。

Method: 利用CLIP模型，设计并聚合多个文本提示，通过嵌入对齐提升模型对MAD任务的表征能力，无需微调或额外训练。

Result: 实验表明，提示聚合显著提升了零样本检测性能，验证了通过高效提示工程利用基础模型内置多模态知识的有效性。

Conclusion: 通过纯零样本方法，本文展示了CLIP在MAD任务中的潜力，为未来研究提供了无需微调的高效解决方案。

Abstract: Face Morphing Attack Detection (MAD) is a critical challenge in face recognition security, where attackers can fool systems by interpolating the identity information of two or more individuals into a single face image, resulting in samples that can be verified as belonging to multiple identities by face recognition systems. While multimodal foundation models (FMs) like CLIP offer strong zero-shot capabilities by jointly modeling images and text, most prior works on FMs for biometric recognition have relied on fine-tuning for specific downstream tasks, neglecting their potential for direct, generalizable deployment. This work explores a pure zero-shot approach to MAD by leveraging CLIP without any additional training or fine-tuning, focusing instead on the design and aggregation of multiple textual prompts per class. By aggregating the embeddings of diverse prompts, we better align the model's internal representations with the MAD task, capturing richer and more varied cues indicative of bona-fide or attack samples. Our results show that prompt aggregation substantially improves zero-shot detection performance, demonstrating the effectiveness of exploiting foundation models' built-in multimodal knowledge through efficient prompt engineering.

</details>


### [19] [TaoCache: Structure-Maintained Video Generation Acceleration](https://arxiv.org/abs/2508.08978)
*Zhentao Fan,Zongzuo Wang,Weiwei Zhang*

Main category: cs.CV

TL;DR: TaoCache是一种无需训练、即插即用的缓存策略，通过固定点视角预测噪声输出，有效提升视频扩散模型的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有缓存加速方法在跳过早期或中期去噪步骤时会导致结构差异，影响指令跟随和角色一致性。

Method: 采用固定点视角预测噪声输出，校准余弦相似度和噪声增量范数比，保留高分辨率结构。

Result: 在相同加速条件下，TaoCache在多个数据集上显著优于现有缓存方法（LPIPS、SSIM、PSNR指标）。

Conclusion: TaoCache是一种高效且兼容性强的缓存策略，适用于DiT框架，显著提升视觉质量。

Abstract: Existing cache-based acceleration methods for video diffusion models primarily skip early or mid denoising steps, which often leads to structural discrepancies relative to full-timestep generation and can hinder instruction following and character consistency. We present TaoCache, a training-free, plug-and-play caching strategy that, instead of residual-based caching, adopts a fixed-point perspective to predict the model's noise output and is specifically effective in late denoising stages. By calibrating cosine similarities and norm ratios of consecutive noise deltas, TaoCache preserves high-resolution structure while enabling aggressive skipping. The approach is orthogonal to complementary accelerations such as Pyramid Attention Broadcast (PAB) and TeaCache, and it integrates seamlessly into DiT-based frameworks. Across Latte-1, OpenSora-Plan v110, and Wan2.1, TaoCache attains substantially higher visual quality (LPIPS, SSIM, PSNR) than prior caching methods under the same speedups.

</details>


### [20] [ColorGPT: Leveraging Large Language Models for Multimodal Color Recommendation](https://arxiv.org/abs/2508.08987)
*Ding Xia,Naoto Inoue,Qianru Qiu,Kotaro Kikuchi*

Main category: cs.CV

TL;DR: 论文探讨了使用预训练大语言模型（LLMs）进行颜色推荐的可能性，提出了一种名为ColorGPT的管道，在颜色调色板完成和生成任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统颜色推荐方法因设计复杂性和数据有限性存在挑战，研究旨在探索LLMs是否能成为更优的颜色推荐工具。

Method: 开发了ColorGPT管道，通过测试多种颜色表示和提示工程技术，专注于颜色调色板完成和生成任务。

Result: 实验表明，ColorGPT在颜色建议准确性和调色板分布上优于现有方法，同时在颜色多样性和相似性上也有提升。

Conclusion: 预训练LLMs在颜色推荐任务中具有潜力，ColorGPT为设计领域提供了一种新的高效工具。

Abstract: Colors play a crucial role in the design of vector graphic documents by enhancing visual appeal, facilitating communication, improving usability, and ensuring accessibility. In this context, color recommendation involves suggesting appropriate colors to complete or refine a design when one or more colors are missing or require alteration. Traditional methods often struggled with these challenges due to the complex nature of color design and the limited data availability. In this study, we explored the use of pretrained Large Language Models (LLMs) and their commonsense reasoning capabilities for color recommendation, raising the question: Can pretrained LLMs serve as superior designers for color recommendation tasks? To investigate this, we developed a robust, rigorously validated pipeline, ColorGPT, that was built by systematically testing multiple color representations and applying effective prompt engineering techniques. Our approach primarily targeted color palette completion by recommending colors based on a set of given colors and accompanying context. Moreover, our method can be extended to full palette generation, producing an entire color palette corresponding to a provided textual description. Experimental results demonstrated that our LLM-based pipeline outperformed existing methods in terms of color suggestion accuracy and the distribution of colors in the color palette completion task. For the full palette generation task, our approach also yielded improvements in color diversity and similarity compared to current techniques.

</details>


### [21] [Towards Perfection: Building Inter-component Mutual Correction for Retinex-based Low-light Image Enhancement](https://arxiv.org/abs/2508.09009)
*Luyang Cao,Han Xu,Jian Zhang,Lei Qi,Jiayi Ma,Yinghuan Shi,Yang Gao*

Main category: cs.CV

TL;DR: 该论文提出了一种新的Inter-correction Retinex模型（IRetinex），用于解决低光图像增强中分解阶段的互成分残差（ICR）问题，通过减少ICR提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 在低光图像增强中，Retinex方法分解图像为光照和反射成分时存在互成分残差（ICR），影响分解精度和最终图像质量。

Method: 提出IRetinex模型，在分解阶段减少光照和反射成分的特征相似性，在增强阶段利用特征相似性检测和减轻ICR的影响。

Result: 在三个低光基准数据集上的实验表明，该方法在定性和定量上均优于现有技术。

Conclusion: 通过减少ICR，IRetinex显著提升了低光图像增强的效果。

Abstract: In low-light image enhancement, Retinex-based deep learning methods have garnered significant attention due to their exceptional interpretability. These methods decompose images into mutually independent illumination and reflectance components, allows each component to be enhanced separately. In fact, achieving perfect decomposition of illumination and reflectance components proves to be quite challenging, with some residuals still existing after decomposition. In this paper, we formally name these residuals as inter-component residuals (ICR), which has been largely underestimated by previous methods. In our investigation, ICR not only affects the accuracy of the decomposition but also causes enhanced components to deviate from the ideal outcome, ultimately reducing the final synthesized image quality. To address this issue, we propose a novel Inter-correction Retinex model (IRetinex) to alleviate ICR during the decomposition and enhancement stage. In the decomposition stage, we leverage inter-component residual reduction module to reduce the feature similarity between illumination and reflectance components. In the enhancement stage, we utilize the feature similarity between the two components to detect and mitigate the impact of ICR within each enhancement unit. Extensive experiments on three low-light benchmark datasets demonstrated that by reducing ICR, our method outperforms state-of-the-art approaches both qualitatively and quantitatively.

</details>


### [22] [VLM-3D:End-to-End Vision-Language Models for Open-World 3D Perception](https://arxiv.org/abs/2508.09061)
*Fuhao Chang,Shuxin Li,Yabei Li,Lei He*

Main category: cs.CV

TL;DR: VLM-3D是一个端到端框架，利用视觉语言模型（VLMs）在自动驾驶场景中进行3D几何感知，通过联合语义-几何损失设计显著提升感知精度。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中开放集感知的挑战，特别是识别未见物体类别，确保安全性。

Method: 提出VLM-3D框架，结合低秩适应（LoRA）和联合语义-几何损失设计（早期语义损失，后期3D IoU损失）。

Result: 在nuScenes数据集上，感知精度提升12.8%。

Conclusion: VLM-3D通过端到端设计和联合损失，有效提升了自动驾驶中的开放集感知能力。

Abstract: Open-set perception in complex traffic environments poses a critical challenge for autonomous driving systems, particularly in identifying previously unseen object categories, which is vital for ensuring safety. Visual Language Models (VLMs), with their rich world knowledge and strong semantic reasoning capabilities, offer new possibilities for addressing this task. However, existing approaches typically leverage VLMs to extract visual features and couple them with traditional object detectors, resulting in multi-stage error propagation that hinders perception accuracy. To overcome this limitation, we propose VLM-3D, the first end-to-end framework that enables VLMs to perform 3D geometric perception in autonomous driving scenarios. VLM-3D incorporates Low-Rank Adaptation (LoRA) to efficiently adapt VLMs to driving tasks with minimal computational overhead, and introduces a joint semantic-geometric loss design: token-level semantic loss is applied during early training to ensure stable convergence, while 3D IoU loss is introduced in later stages to refine the accuracy of 3D bounding box predictions. Evaluations on the nuScenes dataset demonstrate that the proposed joint semantic-geometric loss in VLM-3D leads to a 12.8% improvement in perception accuracy, fully validating the effectiveness and advancement of our method.

</details>


### [23] [Turbo-VAED: Fast and Stable Transfer of Video-VAEs to Mobile Devices](https://arxiv.org/abs/2508.09136)
*Ya Zou,Jingfeng Yao,Siyuan Yu,Shuai Zhang,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: 提出了一种名为Turbo-VAED的低成本解决方案，用于将视频变分自编码器（VAE）高效部署到移动设备上，显著减少参数数量和延迟，同时保持高质量重建。


<details>
  <summary>Details</summary>
Motivation: 移动设备上部署大型生成AI模型的需求增长，但现有视频VAE存在参数过大和内核不匹配问题，导致内存不足或推理速度极慢。

Method: 1. 分析现有VAE架构冗余，采用3D深度可分离卷积减少参数；2. 提出解耦的3D像素洗牌方案优化上采样；3. 设计高效解码器训练方法，仅蒸馏解码器以减少性能损失。

Result: Turbo-VAED在720p分辨率下将原始VAE加速84.5倍，参数减少至17.5%，重建质量保留96.9%，在iPhone 16 Pro上FPS提升2.9倍。

Conclusion: Turbo-VAED首次实现移动设备上实时720p视频VAE解码，适用于大多数视频VAE，具有广泛的应用潜力。

Abstract: There is a growing demand for deploying large generative AI models on mobile devices. For recent popular video generative models, however, the Variational AutoEncoder (VAE) represents one of the major computational bottlenecks. Both large parameter sizes and mismatched kernels cause out-of-memory errors or extremely slow inference on mobile devices. To address this, we propose a low-cost solution that efficiently transfers widely used video VAEs to mobile devices. (1) We analyze redundancy in existing VAE architectures and get empirical design insights. By integrating 3D depthwise separable convolutions into our model, we significantly reduce the number of parameters. (2) We observe that the upsampling techniques in mainstream video VAEs are poorly suited to mobile hardware and form the main bottleneck. In response, we propose a decoupled 3D pixel shuffle scheme that slashes end-to-end delay. Building upon these, we develop a universal mobile-oriented VAE decoder, Turbo-VAED. (3) We propose an efficient VAE decoder training method. Since only the decoder is used during deployment, we distill it to Turbo-VAED instead of retraining the full VAE, enabling fast mobile adaptation with minimal performance loss. To our knowledge, our method enables real-time 720p video VAE decoding on mobile devices for the first time. This approach is widely applicable to most video VAEs. When integrated into four representative models, with training cost as low as $95, it accelerates original VAEs by up to 84.5x at 720p resolution on GPUs, uses as low as 17.5% of original parameter count, and retains 96.9% of the original reconstruction quality. Compared to mobile-optimized VAEs, Turbo-VAED achieves a 2.9x speedup in FPS and better reconstruction quality on the iPhone 16 Pro. The code and models will soon be available at https://github.com/hustvl/Turbo-VAED.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [24] [SharpXR: Structure-Aware Denoising for Pediatric Chest X-Rays](https://arxiv.org/abs/2508.08518)
*Ilerioluwakiiye Abolade,Emmanuel Idoko,Solomon Odelola,Promise Omoigui,Adetola Adebanwo,Aondana Iorumbur,Udunna Anazodo,Alessandro Crimi,Raymond Confidence*

Main category: eess.IV

TL;DR: SharpXR是一种结构感知的双解码器U-Net，用于去噪低剂量儿科X射线，同时保留诊断相关特征，显著提升肺炎分类准确率。


<details>
  <summary>Details</summary>
Motivation: 在资源匮乏地区，低剂量儿科X射线成像因噪声问题影响诊断准确性，传统去噪方法会破坏细节。

Method: SharpXR结合拉普拉斯引导的边缘保留解码器和可学习融合模块，自适应平衡噪声抑制与结构保留，并通过模拟噪声生成训练数据。

Result: SharpXR在所有评估指标上优于现有方法，肺炎分类准确率从88.8%提升至92.5%。

Conclusion: SharpXR在资源受限环境中具有高效性和诊断价值，适合儿科低剂量X射线去噪。

Abstract: Pediatric chest X-ray imaging is essential for early diagnosis, particularly in low-resource settings where advanced imaging modalities are often inaccessible. Low-dose protocols reduce radiation exposure in children but introduce substantial noise that can obscure critical anatomical details. Conventional denoising methods often degrade fine details, compromising diagnostic accuracy. In this paper, we present SharpXR, a structure-aware dual-decoder U-Net designed to denoise low-dose pediatric X-rays while preserving diagnostically relevant features. SharpXR combines a Laplacian-guided edge-preserving decoder with a learnable fusion module that adaptively balances noise suppression and structural detail retention. To address the scarcity of paired training data, we simulate realistic Poisson-Gaussian noise on the Pediatric Pneumonia Chest X-ray dataset. SharpXR outperforms state-of-the-art baselines across all evaluation metrics while maintaining computational efficiency suitable for resource-constrained settings. SharpXR-denoised images improved downstream pneumonia classification accuracy from 88.8% to 92.5%, underscoring its diagnostic value in low-resource pediatric care.

</details>


### [25] [A new dataset and comparison for multi-camera frame synthesis](https://arxiv.org/abs/2508.09068)
*Conall Daly,Anil Kokaram*

Main category: eess.IV

TL;DR: 论文提出了一种新的多相机数据集，用于公平比较帧插值和视图合成方法，发现深度学习方法和经典方法在真实图像数据上表现相近，而3D高斯泼溅在合成场景中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有帧插值和视图合成方法的数据集存在偏差，难以直接比较，因此需要开发新的数据集以实现公平评估。

Method: 使用自定义密集线性相机阵列构建多相机数据集，并评估经典和深度学习帧插值方法与3D高斯泼溅在视图插值任务上的表现。

Result: 在真实图像数据中，深度学习方法未显著优于经典方法，3D高斯泼溅表现较差；但在合成场景中，3D高斯泼溅显著优于帧插值方法。

Conclusion: 研究揭示了不同方法在不同场景下的表现差异，为未来研究提供了公平比较的基础。

Abstract: Many methods exist for frame synthesis in image sequences but can be broadly categorised into frame interpolation and view synthesis techniques. Fundamentally, both frame interpolation and view synthesis tackle the same task, interpolating a frame given surrounding frames in time or space. However, most frame interpolation datasets focus on temporal aspects with single cameras moving through time and space, while view synthesis datasets are typically biased toward stereoscopic depth estimation use cases. This makes direct comparison between view synthesis and frame interpolation methods challenging. In this paper, we develop a novel multi-camera dataset using a custom-built dense linear camera array to enable fair comparison between these approaches. We evaluate classical and deep learning frame interpolators against a view synthesis method (3D Gaussian Splatting) for the task of view in-betweening. Our results reveal that deep learning methods do not significantly outperform classical methods on real image data, with 3D Gaussian Splatting actually underperforming frame interpolators by as much as 3.5 dB PSNR. However, in synthetic scenes, the situation reverses -- 3D Gaussian Splatting outperforms frame interpolation algorithms by almost 5 dB PSNR at a 95% confidence level.

</details>


### [26] [Efficient motion-based metrics for video frame interpolation](https://arxiv.org/abs/2508.09078)
*Conall Daly,Darren Ramsook,Anil Kokaram*

Main category: eess.IV

TL;DR: 论文研究了如何通过简单处理运动场来评估视频帧插值算法的感知质量，提出了一种基于运动场发散性的新指标，该指标与感知评分相关性较好且计算效率更高。


<details>
  <summary>Details</summary>
Motivation: 尽管视频帧插值算法发展迅速，但评估插值内容的感知质量仍是一个研究热点。本文旨在探索简单有效的运动场处理方法，以作为评估插值算法的质量指标。

Method: 通过处理运动场，提出了一种基于运动场发散性的质量指标，并使用BVI-VFI数据集进行验证。

Result: 新指标与感知评分的相关性较好（PLCC=0.51），计算效率比FloLPIPS高2.7倍。

Conclusion: 新指标更倾向于感知质量较好的插值帧，而非传统PSNR或SSIM评分高的帧。

Abstract: Video frame interpolation (VFI) offers a way to generate intermediate frames between consecutive frames of a video sequence. Although the development of advanced frame interpolation algorithms has received increased attention in recent years, assessing the perceptual quality of interpolated content remains an ongoing area of research. In this paper, we investigate simple ways to process motion fields, with the purposes of using them as video quality metric for evaluating frame interpolation algorithms. We evaluate these quality metrics using the BVI-VFI dataset which contains perceptual scores measured for interpolated sequences. From our investigation we propose a motion metric based on measuring the divergence of motion fields. This metric correlates reasonably with these perceptual scores (PLCC=0.51) and is more computationally efficient (x2.7 speedup) compared to FloLPIPS (a well known motion-based metric). We then use our new proposed metrics to evaluate a range of state of the art frame interpolation metrics and find our metrics tend to favour more perceptual pleasing interpolated frames that may not score highly in terms of PSNR or SSIM.

</details>
