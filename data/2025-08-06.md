<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 43]
- [cs.CR](#cs.CR) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven Talking Head Generation](https://arxiv.org/abs/2508.03457)
*Haotian Wang,Yuzhe Weng,Jun Du,Haoran Xu,Xiaoyan Wu,Shan He,Bing Yin,Cong Liu,Jianqing Gao,Qingfeng Liu*

Main category: cs.GR

TL;DR: READ是一个基于扩散-变换器的实时说话头部生成框架，通过压缩视频和语音潜在空间及异步噪声调度器，显著提升生成速度和质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在说话头部生成中取得了进展，但推理速度过慢限制了实际应用。

Method: 使用时间VAE压缩视频潜在空间，预训练SpeechAE生成语音潜在代码，设计A2V-DiT建模，并引入异步噪声调度器（ANS）。

Result: READ在质量和速度上优于现有方法，实现了长时间生成的稳定性。

Conclusion: READ在实时性和生成质量之间取得了平衡，为说话头部生成提供了高效解决方案。

Abstract: The introduction of diffusion models has brought significant advances to the field of audio-driven talking head generation. However, the extremely slow inference speed severely limits the practical implementation of diffusion-based talking head generation models. In this study, we propose READ, the first real-time diffusion-transformer-based talking head generation framework. Our approach first learns a spatiotemporal highly compressed video latent space via a temporal VAE, significantly reducing the token count to accelerate generation. To achieve better audio-visual alignment within this compressed latent space, a pre-trained Speech Autoencoder (SpeechAE) is proposed to generate temporally compressed speech latent codes corresponding to the video latent space. These latent representations are then modeled by a carefully designed Audio-to-Video Diffusion Transformer (A2V-DiT) backbone for efficient talking head synthesis. Furthermore, to ensure temporal consistency and accelerated inference in extended generation, we propose a novel asynchronous noise scheduler (ANS) for both the training and inference process of our framework. The ANS leverages asynchronous add-noise and asynchronous motion-guided generation in the latent space, ensuring consistency in generated video clips. Experimental results demonstrate that READ outperforms state-of-the-art methods by generating competitive talking head videos with significantly reduced runtime, achieving an optimal balance between quality and speed while maintaining robust metric stability in long-time generation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [Learning Latent Representations for Image Translation using Frequency Distributed CycleGAN](https://arxiv.org/abs/2508.03415)
*Shivangi Nigam,Adarsh Prasad Behera,Shekhar Verma,P. Nagabhushan*

Main category: cs.CV

TL;DR: Fd-CycleGAN是一种改进的图像到图像翻译框架，通过增强潜在表示学习和频率感知监督，提升了生成图像的质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 改进CycleGAN在低数据量情况下的表现，同时更好地捕捉局部和全局数据分布特征。

Method: 结合局部邻域编码（LNE）和频率感知监督，使用KL/JS散度和基于对数的相似性度量来量化真实与生成图像的分布对齐。

Result: 在多个数据集上表现优于基线CycleGAN和其他先进方法，生成图像具有更高的感知质量、更快的收敛速度和更好的模式多样性。

Conclusion: Fd-CycleGAN通过频率引导的潜在学习显著提升了图像翻译任务的泛化能力，适用于文档修复、艺术风格迁移和医学图像合成等领域。

Abstract: This paper presents Fd-CycleGAN, an image-to-image (I2I) translation framework that enhances latent representation learning to approximate real data distributions. Building upon the foundation of CycleGAN, our approach integrates Local Neighborhood Encoding (LNE) and frequency-aware supervision to capture fine-grained local pixel semantics while preserving structural coherence from the source domain. We employ distribution-based loss metrics, including KL/JS divergence and log-based similarity measures, to explicitly quantify the alignment between real and generated image distributions in both spatial and frequency domains. To validate the efficacy of Fd-CycleGAN, we conduct experiments on diverse datasets -- Horse2Zebra, Monet2Photo, and a synthetically augmented Strike-off dataset. Compared to baseline CycleGAN and other state-of-the-art methods, our approach demonstrates superior perceptual quality, faster convergence, and improved mode diversity, particularly in low-data regimes. By effectively capturing local and global distribution characteristics, Fd-CycleGAN achieves more visually coherent and semantically consistent translations. Our results suggest that frequency-guided latent learning significantly improves generalization in image translation tasks, with promising applications in document restoration, artistic style transfer, and medical image synthesis. We also provide comparative insights with diffusion-based generative models, highlighting the advantages of our lightweight adversarial approach in terms of training efficiency and qualitative output.

</details>


### [3] [DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a Stage-Wise Diffusion Transformer Framework](https://arxiv.org/abs/2508.02807)
*Tongchun Zuo,Zaiyu Huang,Shuliang Ning,Ente Lin,Chao Liang,Zerong Zheng,Jianwen Jiang,Yuan Zhang,Mingyuan Gao,Xin Dong*

Main category: cs.CV

TL;DR: DreamVVT是一个基于扩散变换器（DiTs）的两阶段框架，用于解决视频虚拟试穿（VVT）中的细粒度细节保留和时间一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖稀缺的配对数据集，且未能充分利用预训练模型和测试输入的先验知识，导致在无约束场景中难以保持细节和时间一致性。

Method: 第一阶段从输入视频中采样关键帧，结合视觉语言模型（VLM）生成高保真试穿图像；第二阶段利用骨架图和动态描述，通过增强的视频生成模型生成时间一致的视频。

Result: 实验表明，DreamVVT在细节保留和时间稳定性上优于现有方法。

Conclusion: DreamVVT通过两阶段设计和预训练模型的有效利用，显著提升了视频虚拟试穿的性能。

Abstract: Video virtual try-on (VVT) technology has garnered considerable academic interest owing to its promising applications in e-commerce advertising and entertainment. However, most existing end-to-end methods rely heavily on scarce paired garment-centric datasets and fail to effectively leverage priors of advanced visual models and test-time inputs, making it challenging to accurately preserve fine-grained garment details and maintain temporal consistency in unconstrained scenarios. To address these challenges, we propose DreamVVT, a carefully designed two-stage framework built upon Diffusion Transformers (DiTs), which is inherently capable of leveraging diverse unpaired human-centric data to enhance adaptability in real-world scenarios. To further leverage prior knowledge from pretrained models and test-time inputs, in the first stage, we sample representative frames from the input video and utilize a multi-frame try-on model integrated with a vision-language model (VLM), to synthesize high-fidelity and semantically consistent keyframe try-on images. These images serve as complementary appearance guidance for subsequent video generation. \textbf{In the second stage}, skeleton maps together with fine-grained motion and appearance descriptions are extracted from the input content, and these along with the keyframe try-on images are then fed into a pretrained video generation model enhanced with LoRA adapters. This ensures long-term temporal coherence for unseen regions and enables highly plausible dynamic motions. Extensive quantitative and qualitative experiments demonstrate that DreamVVT surpasses existing methods in preserving detailed garment content and temporal stability in real-world scenarios. Our project page https://virtu-lab.github.io/

</details>


### [4] [GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing](https://arxiv.org/abs/2508.02831)
*Mikołaj Zieliński,Krzysztof Byrski,Tomasz Szczepanik,Przemysław Spurek*

Main category: cs.CV

TL;DR: GENIE结合了NeRF的高保真渲染和GS的可编辑性，通过高斯特征嵌入和高效搜索实现实时编辑。


<details>
  <summary>Details</summary>
Motivation: 解决NeRF难以编辑和GS渲染质量不足的问题，结合两者优势。

Method: 使用高斯特征嵌入和RT-GPS搜索，结合多分辨率哈希网格。

Result: 实现了实时编辑和高保真渲染的平衡。

Conclusion: GENIE在可编辑性和渲染质量上取得了突破。

Abstract: Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have recently transformed 3D scene representation and rendering. NeRF achieves high-fidelity novel view synthesis by learning volumetric representations through neural networks, but its implicit encoding makes editing and physical interaction challenging. In contrast, GS represents scenes as explicit collections of Gaussian primitives, enabling real-time rendering, faster training, and more intuitive manipulation. This explicit structure has made GS particularly well-suited for interactive editing and integration with physics-based simulation. In this paper, we introduce GENIE (Gaussian Encoding for Neural Radiance Fields Interactive Editing), a hybrid model that combines the photorealistic rendering quality of NeRF with the editable and structured representation of GS. Instead of using spherical harmonics for appearance modeling, we assign each Gaussian a trainable feature embedding. These embeddings are used to condition a NeRF network based on the k nearest Gaussians to each query point. To make this conditioning efficient, we introduce Ray-Traced Gaussian Proximity Search (RT-GPS), a fast nearest Gaussian search based on a modified ray-tracing pipeline. We also integrate a multi-resolution hash grid to initialize and update Gaussian features. Together, these components enable real-time, locality-aware editing: as Gaussian primitives are repositioned or modified, their interpolated influence is immediately reflected in the rendered output. By combining the strengths of implicit and explicit representations, GENIE supports intuitive scene manipulation, dynamic interaction, and compatibility with physical simulation, bridging the gap between geometry-based editing and neural rendering. The code can be found under (https://github.com/MikolajZielinski/genie)

</details>


### [5] [RDDPM: Robust Denoising Diffusion Probabilistic Model for Unsupervised Anomaly Segmentation](https://arxiv.org/abs/2508.02903)
*Mehrdad Moradi,Kamran Paynabar*

Main category: cs.CV

TL;DR: 论文提出了一种鲁棒的降噪扩散模型，用于在仅有污染（混合正常和异常）未标记数据的情况下进行无监督异常分割。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型依赖正常数据进行训练，限制了其在实际场景中的应用。本文旨在解决这一问题。

Method: 通过将数据最大似然估计转化为非线性回归问题，提出鲁棒回归视角下的降噪扩散概率模型。

Result: 实验表明，该方法在污染数据下优于现有扩散模型，AUROC和AUPRC分别提升8.08%和10.37%。

Conclusion: 提出的鲁棒扩散模型框架灵活且高效，适用于实际场景中的异常分割任务。

Abstract: Recent advancements in diffusion models have demonstrated significant success in unsupervised anomaly segmentation. For anomaly segmentation, these models are first trained on normal data; then, an anomalous image is noised to an intermediate step, and the normal image is reconstructed through backward diffusion. Unlike traditional statistical methods, diffusion models do not rely on specific assumptions about the data or target anomalies, making them versatile for use across different domains. However, diffusion models typically assume access to normal data for training, limiting their applicability in realistic settings. In this paper, we propose novel robust denoising diffusion models for scenarios where only contaminated (i.e., a mix of normal and anomalous) unlabeled data is available. By casting maximum likelihood estimation of the data as a nonlinear regression problem, we reinterpret the denoising diffusion probabilistic model through a regression lens. Using robust regression, we derive a robust version of denoising diffusion probabilistic models. Our novel framework offers flexibility in constructing various robust diffusion models. Our experiments show that our approach outperforms current state of the art diffusion models, for unsupervised anomaly segmentation when only contaminated data is available. Our method outperforms existing diffusion-based approaches, achieving up to 8.08\% higher AUROC and 10.37\% higher AUPRC on MVTec datasets. The implementation code is available at: https://github.com/mehrdadmoradi124/RDDPM

</details>


### [6] [How Diffusion Prior Landscapes Shape the Posterior in Blind Deconvolution](https://arxiv.org/abs/2508.02923)
*Minh-Hai Nguyen,Edouard Pauwels,Pierre Weiss*

Main category: cs.CV

TL;DR: MAP估计在盲去卷积中倾向于模糊解，但扩散先验揭示了局部极小值对应自然图像，需良好初始化以解决盲去卷积问题。


<details>
  <summary>Details</summary>
Motivation: 研究MAP估计在盲去卷积中的局限性，探索扩散先验如何揭示更优解。

Method: 通过分析扩散先验的似然景观，发现模糊图像似然更高，局部极小值对应自然图像。理论分析盲去模糊后验，验证梯度下降可找到实际解。

Result: MAP估计倾向于产生模糊解，但局部极小值对应自然图像，需良好初始化以优化结果。

Conclusion: 扩散先验和局部初始化可克服MAP限制，为盲去卷积提供更优解。

Abstract: The Maximum A Posteriori (MAP) estimation is a widely used framework in blind deconvolution to recover sharp images from blurred observations. The estimated image and blur filter are defined as the maximizer of the posterior distribution. However, when paired with sparsity-promoting image priors, MAP estimation has been shown to favors blurry solutions, limiting its effectiveness. In this paper, we revisit this result using diffusion-based priors, a class of models that capture realistic image distributions. Through an empirical examination of the prior's likelihood landscape, we uncover two key properties: first, blurry images tend to have higher likelihoods; second, the landscape contains numerous local minimizers that correspond to natural images. Building on these insights, we provide a theoretical analysis of the blind deblurring posterior. This reveals that the MAP estimator tends to produce sharp filters (close to the Dirac delta function) and blurry solutions. However local minimizers of the posterior, which can be obtained with gradient descent, correspond to realistic, natural images, effectively solving the blind deconvolution problem. Our findings suggest that overcoming MAP's limitations requires good local initialization to local minima in the posterior landscape. We validate our analysis with numerical experiments, demonstrating the practical implications of our insights for designing improved priors and optimization techniques.

</details>


### [7] [X-Actor: Emotional and Expressive Long-Range Portrait Acting from Audio](https://arxiv.org/abs/2508.02944)
*Chenxu Zhang,Zenan Li,Hongyi Xu,You Xie,Xiaochen Zhao,Tianpei Gu,Guoxian Song,Xin Chen,Chao Liang,Jianwen Jiang,Linjie Luo*

Main category: cs.CV

TL;DR: X-Actor是一个音频驱动的肖像动画框架，通过单张参考图像和音频生成逼真、情感丰富的说话头部视频。其核心是两阶段解耦生成流程，结合自回归扩散模型和视频合成模块，实现长时情感连贯的动画效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注唇部同步和短时视觉保真度，而X-Actor旨在生成具有动态情感演变的演员级长时肖像表演。

Method: 采用两阶段解耦生成流程：1）音频条件自回归扩散模型预测面部运动潜在标记；2）扩散视频合成模块将其转化为高保真视频。

Result: 实验表明，X-Actor能生成超越标准说话头部动画的影院级表演，并在长时音频驱动情感肖像表演中达到最优效果。

Conclusion: X-Actor通过解耦设计和扩散训练范式，实现了长时情感连贯的高质量肖像动画，为音频驱动表演开辟了新方向。

Abstract: We present X-Actor, a novel audio-driven portrait animation framework that generates lifelike, emotionally expressive talking head videos from a single reference image and an input audio clip. Unlike prior methods that emphasize lip synchronization and short-range visual fidelity in constrained speaking scenarios, X-Actor enables actor-quality, long-form portrait performance capturing nuanced, dynamically evolving emotions that flow coherently with the rhythm and content of speech. Central to our approach is a two-stage decoupled generation pipeline: an audio-conditioned autoregressive diffusion model that predicts expressive yet identity-agnostic facial motion latent tokens within a long temporal context window, followed by a diffusion-based video synthesis module that translates these motions into high-fidelity video animations. By operating in a compact facial motion latent space decoupled from visual and identity cues, our autoregressive diffusion model effectively captures long-range correlations between audio and facial dynamics through a diffusion-forcing training paradigm, enabling infinite-length emotionally-rich motion prediction without error accumulation. Extensive experiments demonstrate that X-Actor produces compelling, cinematic-style performances that go beyond standard talking head animations and achieves state-of-the-art results in long-range, audio-driven emotional portrait acting.

</details>


### [8] [Towards Robust Image Denoising with Scale Equivariance](https://arxiv.org/abs/2508.02967)
*Dawei Zhang,Xiaojie Guo*

Main category: cs.CV

TL;DR: 论文提出了一种基于尺度等变性的盲去噪框架，通过HNM和IGM模块提升模型在空间非均匀噪声下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有去噪模型在空间非均匀噪声（OOD条件）下泛化能力不足，需要探索更鲁棒的方法。

Method: 提出了一种包含HNM（稳定特征分布）和IGM（信息调制）的尺度等变框架。

Result: 模型在合成和真实数据集上均优于现有方法，尤其在空间异质噪声下表现突出。

Conclusion: 尺度等变性是提升去噪模型OOD鲁棒性的有效归纳偏置，HNM和IGM模块显著提升了性能。

Abstract: Despite notable advances in image denoising, existing models often struggle to generalize beyond in-distribution noise patterns, particularly when confronted with out-of-distribution (OOD) conditions characterized by spatially variant noise. This generalization gap remains a fundamental yet underexplored challenge. In this work, we investigate \emph{scale equivariance} as a core inductive bias for improving OOD robustness. We argue that incorporating scale-equivariant structures enables models to better adapt from training on spatially uniform noise to inference on spatially non-uniform degradations. Building on this insight, we propose a robust blind denoising framework equipped with two key components: a Heterogeneous Normalization Module (HNM) and an Interactive Gating Module (IGM). HNM stabilizes feature distributions and dynamically corrects features under varying noise intensities, while IGM facilitates effective information modulation via gated interactions between signal and feature paths. Extensive evaluations demonstrate that our model consistently outperforms state-of-the-art methods on both synthetic and real-world benchmarks, especially under spatially heterogeneous noise. Code will be made publicly available.

</details>


### [9] [Diffusion Models with Adaptive Negative Sampling Without External Resources](https://arxiv.org/abs/2508.02973)
*Alakh Desai,Nuno Vasconcelos*

Main category: cs.CV

TL;DR: ANSWER是一种无需训练的采样方法，通过结合负提示和分类器自由引导（CFG），提高扩散模型生成图像与文本提示的匹配度。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成高质量图像时，对提示的遵循程度和质量存在显著差异。负提示被用于改善提示遵循性，但现有方法依赖外部资源且不完整。

Method: 提出ANSWER方法，利用扩散模型内部对否定的理解，结合CFG，无需显式负提示即可实现负条件采样。

Result: ANSWER在多个基准测试中优于基线方法，人类评估显示其受欢迎程度是其他方法的2倍。

Conclusion: ANSWER是一种高效、无需训练的技术，适用于支持CFG的任何扩散模型，显著提升了生成图像与提示的匹配度。

Abstract: Diffusion models (DMs) have demonstrated an unparalleled ability to create diverse and high-fidelity images from text prompts. However, they are also well-known to vary substantially regarding both prompt adherence and quality. Negative prompting was introduced to improve prompt compliance by specifying what an image must not contain. Previous works have shown the existence of an ideal negative prompt that can maximize the odds of the positive prompt. In this work, we explore relations between negative prompting and classifier-free guidance (CFG) to develop a sampling procedure, {\it Adaptive Negative Sampling Without External Resources} (ANSWER), that accounts for both positive and negative conditions from a single prompt. This leverages the internal understanding of negation by the diffusion model to increase the odds of generating images faithful to the prompt. ANSWER is a training-free technique, applicable to any model that supports CFG, and allows for negative grounding of image concepts without an explicit negative prompts, which are lossy and incomplete. Experiments show that adding ANSWER to existing DMs outperforms the baselines on multiple benchmarks and is preferred by humans 2x more over the other methods.

</details>


### [10] [Separating Shared and Domain-Specific LoRAs for Multi-Domain Learning](https://arxiv.org/abs/2508.02978)
*Yusaku Takama,Ning Ding,Tatsuya Yokota,Toru Tamaki*

Main category: cs.CV

TL;DR: 论文提出一种方法，确保共享和领域特定的LoRAs存在于不同的子空间中，以更好地捕获领域特定信息。


<details>
  <summary>Details</summary>
Motivation: 现有多领域学习架构中，共享和领域特定LoRA的结构是否有效捕获领域信息尚不明确。

Method: 提出一种方法，将共享和领域特定LoRAs分别置于预训练权重的列和左零子空间中。

Result: 在三个动作识别数据集（UCF101、Kinetics400和HMDB51）上验证了方法的有效性，并分析了LoRA权重的维度。

Conclusion: 该方法在某些情况下有效，并通过LoRA权重维度分析提供了进一步见解。

Abstract: Existing architectures of multi-domain learning have two types of adapters: shared LoRA for all domains and domain-specific LoRA for each particular domain. However, it remains unclear whether this structure effectively captures domain-specific information. In this paper, we propose a method that ensures that shared and domain-specific LoRAs exist in different subspaces; specifically, the column and left null subspaces of the pre-trained weights. We apply the proposed method to action recognition with three datasets (UCF101, Kinetics400, and HMDB51) and demonstrate its effectiveness in some cases along with the analysis of the dimensions of LoRA weights.

</details>


### [11] [Seeing It Before It Happens: In-Generation NSFW Detection for Diffusion-Based Text-to-Image Models](https://arxiv.org/abs/2508.03006)
*Fan Yang,Yihao Huang,Jiayi Zhu,Ling Shi,Geguang Pu,Jin Song Dong,Kailong Wang*

Main category: cs.CV

TL;DR: 提出了In-Generation Detection (IGD)方法，利用扩散模型生成过程中的预测噪声检测NSFW内容，准确率达91.32%。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成高质量图像的同时可能被滥用生成NSFW内容，现有方法主要关注生成前或生成后的检测，而生成过程中的检测尚未充分探索。

Method: 利用扩散过程中的预测噪声作为内部信号，区分NSFW与良性提示。

Result: 在七种NSFW类别上，IGD的平均检测准确率为91.32%，优于七种基线方法。

Conclusion: IGD是一种简单有效的方法，填补了扩散模型生成过程中NSFW检测的空白。

Abstract: Diffusion-based text-to-image (T2I) models enable high-quality image generation but also pose significant risks of misuse, particularly in producing not-safe-for-work (NSFW) content. While prior detection methods have focused on filtering prompts before generation or moderating images afterward, the in-generation phase of diffusion models remains largely unexplored for NSFW detection. In this paper, we introduce In-Generation Detection (IGD), a simple yet effective approach that leverages the predicted noise during the diffusion process as an internal signal to identify NSFW content. This approach is motivated by preliminary findings suggesting that the predicted noise may capture semantic cues that differentiate NSFW from benign prompts, even when the prompts are adversarially crafted. Experiments conducted on seven NSFW categories show that IGD achieves an average detection accuracy of 91.32% over naive and adversarial NSFW prompts, outperforming seven baseline methods.

</details>


### [12] [SA-3DGS: A Self-Adaptive Compression Method for 3D Gaussian Splatting](https://arxiv.org/abs/2508.03017)
*Liheng Zhang,Weihao Yu,Zubo Lu,Haozhi Gu,Jin Huang*

Main category: cs.CV

TL;DR: SA-3DGS通过重要性评分和聚类压缩，显著减少3D高斯模型的存储需求，同时保持渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以识别场景中真正不重要的高斯点，导致压缩和渲染性能下降。

Method: SA-3DGS通过学习重要性评分识别不重要高斯点，结合聚类和修复模块优化压缩。

Result: 实验显示，该方法在多个数据集上实现高达66倍压缩，且渲染质量不降反升。

Conclusion: SA-3DGS在压缩和泛化能力上表现优异，适用于其他基于剪枝的方法。

Abstract: Recent advancements in 3D Gaussian Splatting have enhanced efficient and high-quality novel view synthesis. However, representing scenes requires a large number of Gaussian points, leading to high storage demands and limiting practical deployment. The latest methods facilitate the compression of Gaussian models but struggle to identify truly insignificant Gaussian points in the scene, leading to a decline in subsequent Gaussian pruning, compression quality, and rendering performance. To address this issue, we propose SA-3DGS, a method that significantly reduces storage costs while maintaining rendering quality. SA-3DGS learns an importance score to automatically identify the least significant Gaussians in scene reconstruction, thereby enabling effective pruning and redundancy reduction. Next, the importance-aware clustering module compresses Gaussians attributes more accurately into the codebook, improving the codebook's expressive capability while reducing model size. Finally, the codebook repair module leverages contextual scene information to repair the codebook, thereby recovering the original Gaussian point attributes and mitigating the degradation in rendering quality caused by information loss. Experimental results on several benchmark datasets show that our method achieves up to 66x compression while maintaining or even improving rendering quality. The proposed Gaussian pruning approach is not only adaptable to but also improves other pruning-based methods (e.g., LightGaussian), showcasing excellent performance and strong generalization ability.

</details>


### [13] [MoCA: Identity-Preserving Text-to-Video Generation via Mixture of Cross Attention](https://arxiv.org/abs/2508.03034)
*Qi Xie,Yongjia Ma,Donglin Di,Xuehao Gao,Xun Yang*

Main category: cs.CV

TL;DR: MoCA是一种基于扩散变换器（DiT）的视频扩散模型，通过混合交叉注意力机制提升文本到视频（T2V）生成中的身份一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在细粒度面部动态捕捉和时序身份一致性方面表现不足，需要改进。

Method: 采用DiT框架，嵌入MoCA层，结合分层时序池化和时序感知交叉注意力专家，并引入潜在视频感知损失。

Result: 在CelebIPVid数据集上，MoCA在面部相似度上优于现有方法5%以上。

Conclusion: MoCA通过混合注意力机制和时序建模，显著提升了T2V生成中的身份一致性。

Abstract: Achieving ID-preserving text-to-video (T2V) generation remains challenging despite recent advances in diffusion-based models. Existing approaches often fail to capture fine-grained facial dynamics or maintain temporal identity coherence. To address these limitations, we propose MoCA, a novel Video Diffusion Model built on a Diffusion Transformer (DiT) backbone, incorporating a Mixture of Cross-Attention mechanism inspired by the Mixture-of-Experts paradigm. Our framework improves inter-frame identity consistency by embedding MoCA layers into each DiT block, where Hierarchical Temporal Pooling captures identity features over varying timescales, and Temporal-Aware Cross-Attention Experts dynamically model spatiotemporal relationships. We further incorporate a Latent Video Perceptual Loss to enhance identity coherence and fine-grained details across video frames. To train this model, we collect CelebIPVid, a dataset of 10,000 high-resolution videos from 1,000 diverse individuals, promoting cross-ethnicity generalization. Extensive experiments on CelebIPVid show that MoCA outperforms existing T2V methods by over 5% across Face similarity.

</details>


### [14] [Multi-human Interactive Talking Dataset](https://arxiv.org/abs/2508.03050)
*Zeyu Zhu,Weijia Wu,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 论文介绍了MIT数据集和CovOG模型，用于多人类对话视频生成，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单人类独白或孤立面部动画，缺乏对多人类交互的适用性。

Method: 开发自动管道收集和标注多人对话视频，构建MIT数据集；提出CovOG模型，整合多人类姿态编码器和交互音频驱动。

Result: MIT数据集包含12小时高清视频，标注精细；CovOG模型展示了生成多人类对话视频的可行性。

Conclusion: MIT数据集和CovOG模型为未来研究提供了有价值的基准，解决了多人类交互视频生成的挑战。

Abstract: Existing studies on talking video generation have predominantly focused on single-person monologues or isolated facial animations, limiting their applicability to realistic multi-human interactions. To bridge this gap, we introduce MIT, a large-scale dataset specifically designed for multi-human talking video generation. To this end, we develop an automatic pipeline that collects and annotates multi-person conversational videos. The resulting dataset comprises 12 hours of high-resolution footage, each featuring two to four speakers, with fine-grained annotations of body poses and speech interactions. It captures natural conversational dynamics in multi-speaker scenario, offering a rich resource for studying interactive visual behaviors. To demonstrate the potential of MIT, we furthur propose CovOG, a baseline model for this novel task. It integrates a Multi-Human Pose Encoder (MPE) to handle varying numbers of speakers by aggregating individual pose embeddings, and an Interactive Audio Driver (IAD) to modulate head dynamics based on speaker-specific audio features. Together, these components showcase the feasibility and challenges of generating realistic multi-human talking videos, establishing MIT as a valuable benchmark for future research. The code is avalibale at: https://github.com/showlab/Multi-human-Talking-Video-Dataset.

</details>


### [15] [RobustGS: Unified Boosting of Feedforward 3D Gaussian Splatting under Low-Quality Conditions](https://arxiv.org/abs/2508.03077)
*Anran Wu,Long Peng,Xin Di,Xueyuan Dai,Chen Wu,Yang Wang,Xueyang Fu,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 提出RobustGS模块，提升3D高斯溅射（3DGS）在恶劣成像条件下的鲁棒性，实现高质量3D重建。


<details>
  <summary>Details</summary>
Motivation: 现有前馈3DGS方法假设输入多视图图像干净高质量，但实际场景中图像常受噪声、低光或雨等影响，导致重建质量下降。

Method: 引入通用多视图特征增强模块RobustGS，包括广义退化学习器和语义感知状态空间模型，以增强退化感知和语义信息聚合。

Result: 实验表明，RobustGS能显著提升重建质量，适用于多种退化类型。

Conclusion: RobustGS模块可无缝集成现有方法，提升3D重建的鲁棒性和质量。

Abstract: Feedforward 3D Gaussian Splatting (3DGS) overcomes the limitations of optimization-based 3DGS by enabling fast and high-quality reconstruction without the need for per-scene optimization. However, existing feedforward approaches typically assume that input multi-view images are clean and high-quality. In real-world scenarios, images are often captured under challenging conditions such as noise, low light, or rain, resulting in inaccurate geometry and degraded 3D reconstruction. To address these challenges, we propose a general and efficient multi-view feature enhancement module, RobustGS, which substantially improves the robustness of feedforward 3DGS methods under various adverse imaging conditions, enabling high-quality 3D reconstruction. The RobustGS module can be seamlessly integrated into existing pretrained pipelines in a plug-and-play manner to enhance reconstruction robustness. Specifically, we introduce a novel component, Generalized Degradation Learner, designed to extract generic representations and distributions of multiple degradations from multi-view inputs, thereby enhancing degradation-awareness and improving the overall quality of 3D reconstruction. In addition, we propose a novel semantic-aware state-space model. It first leverages the extracted degradation representations to enhance corrupted inputs in the feature space. Then, it employs a semantic-aware strategy to aggregate semantically similar information across different views, enabling the extraction of fine-grained cross-view correspondences and further improving the quality of 3D representations. Extensive experiments demonstrate that our approach, when integrated into existing methods in a plug-and-play manner, consistently achieves state-of-the-art reconstruction quality across various types of degradations.

</details>


### [16] [H3R: Hybrid Multi-view Correspondence for Generalizable 3D Reconstruction](https://arxiv.org/abs/2508.03118)
*Heng Jia,Linchao Zhu,Na Zhao*

Main category: cs.CV

TL;DR: H3R是一种混合框架，结合体积潜在融合和基于注意力的特征聚合，解决了3D重建中的多视角对应建模问题，提高了泛化能力和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有的3D重建方法在几何精度和鲁棒性之间存在权衡，H3R旨在通过结合显式和隐式方法的优势来解决这一问题。

Method: H3R由两部分组成：一个通过极线约束增强几何一致性的潜在体积模块，以及一个利用Plücker坐标进行自适应对应优化的相机感知Transformer。

Result: H3R在多个基准测试中表现优异，PSNR分别提高了0.59 dB、1.06 dB和0.22 dB，且收敛速度快2倍。

Conclusion: H3R通过混合框架和空间对齐的基础模型，显著提升了3D重建的性能和泛化能力。

Abstract: Despite recent advances in feed-forward 3D Gaussian Splatting, generalizable 3D reconstruction remains challenging, particularly in multi-view correspondence modeling. Existing approaches face a fundamental trade-off: explicit methods achieve geometric precision but struggle with ambiguous regions, while implicit methods provide robustness but suffer from slow convergence. We present H3R, a hybrid framework that addresses this limitation by integrating volumetric latent fusion with attention-based feature aggregation. Our framework consists of two complementary components: an efficient latent volume that enforces geometric consistency through epipolar constraints, and a camera-aware Transformer that leverages Pl\"ucker coordinates for adaptive correspondence refinement. By integrating both paradigms, our approach enhances generalization while converging 2$\times$ faster than existing methods. Furthermore, we show that spatial-aligned foundation models (e.g., SD-VAE) substantially outperform semantic-aligned models (e.g., DINOv2), resolving the mismatch between semantic representations and spatial reconstruction requirements. Our method supports variable-number and high-resolution input views while demonstrating robust cross-dataset generalization. Extensive experiments show that our method achieves state-of-the-art performance across multiple benchmarks, with significant PSNR improvements of 0.59 dB, 1.06 dB, and 0.22 dB on the RealEstate10K, ACID, and DTU datasets, respectively. Code is available at https://github.com/JiaHeng-DLUT/H3R.

</details>


### [17] [UniEdit-I: Training-free Image Editing for Unified VLM via Iterative Understanding, Editing and Verifying](https://arxiv.org/abs/2508.03142)
*Chengyu Bai,Jintao Chen,Xiang Bai,Yilong Chen,Qi She,Ming Lu,Shanghang Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种无需训练的图像编辑框架UniEdit-I，通过理解、编辑和验证三个迭代步骤，为统一视觉语言模型（VLM）提供图像编辑能力。


<details>
  <summary>Details</summary>
Motivation: 当前统一VLM在图像生成方面表现优异，但缺乏图像编辑能力。本文旨在填补这一空白，提出一种无需额外训练的方法。

Method: 通过三个步骤实现图像编辑：1）理解步骤生成源提示和目标提示；2）编辑步骤引入时间自适应偏移进行连贯编辑；3）验证步骤检查对齐并提供反馈。

Result: 在BLIP3-o上实现，并在GEdit-Bench基准测试中达到SOTA性能。

Conclusion: UniEdit-I为统一VLM提供了高效的图像编辑能力，无需额外训练，具有广泛应用潜力。

Abstract: In recent years, unified vision-language models (VLMs) have rapidly advanced, effectively tackling both visual understanding and generation tasks within a single design. While many unified VLMs have explored various design choices, the recent hypothesis from OpenAI's GPT-4o suggests a promising generation pipeline: Understanding VLM->Visual Feature->Projector->Diffusion Model->Image. The understanding VLM is frozen, and only the generation-related modules are trained. This pipeline maintains the strong capability of understanding VLM while enabling the image generation ability of the unified VLM. Although this pipeline has shown very promising potential for the future development of unified VLM, how to easily enable image editing capability is still unexplored. In this paper, we introduce a novel training-free framework named UniEdit-I to enable the unified VLM with image editing capability via three iterative steps: understanding, editing, and verifying. 1. The understanding step analyzes the source image to create a source prompt through structured semantic analysis and makes minimal word replacements to form the target prompt based on the editing instruction. 2. The editing step introduces a time-adaptive offset, allowing for coherent editing from coarse to fine throughout the denoising process. 3. The verification step checks the alignment between the target prompt and the intermediate edited image, provides automatic consistency scores and corrective feedback, and determines whether to stop early or continue the editing loop. This understanding, editing, and verifying loop iterates until convergence, delivering high-fidelity editing in a training-free manner. We implemented our method based on the latest BLIP3-o and achieved state-of-the-art (SOTA) performance on the GEdit-Bench benchmark.

</details>


### [18] [SARD: Segmentation-Aware Anomaly Synthesis via Region-Constrained Diffusion with Discriminative Mask Guidance](https://arxiv.org/abs/2508.03143)
*Yanshu Wang,Xichen Xu,Xiaoning Lei,Guoyang Xie*

Main category: cs.CV

TL;DR: SARD是一种基于扩散的异常生成框架，通过区域约束扩散和判别性掩码引导，提升了异常合成的空间可控性和区域保真度。


<details>
  <summary>Details</summary>
Motivation: 现有扩散方法在空间可控性和区域保真度上表现不足，限制了工业异常检测系统的鲁棒性提升。

Method: 提出SARD框架，结合区域约束扩散（RCD）和判别性掩码引导（DMG），选择性更新异常区域并评估全局与局部保真度。

Result: 在MVTec-AD和BTAD数据集上，SARD在分割精度和视觉质量上超越现有方法，达到新SOTA。

Conclusion: SARD通过改进扩散过程的空间控制能力，显著提升了异常合成的质量和实用性。

Abstract: Synthesizing realistic and spatially precise anomalies is essential for enhancing the robustness of industrial anomaly detection systems. While recent diffusion-based methods have demonstrated strong capabilities in modeling complex defect patterns, they often struggle with spatial controllability and fail to maintain fine-grained regional fidelity. To overcome these limitations, we propose SARD (Segmentation-Aware anomaly synthesis via Region-constrained Diffusion with discriminative mask Guidance), a novel diffusion-based framework specifically designed for anomaly generation. Our approach introduces a Region-Constrained Diffusion (RCD) process that preserves the background by freezing it and selectively updating only the foreground anomaly regions during the reverse denoising phase, thereby effectively reducing background artifacts. Additionally, we incorporate a Discriminative Mask Guidance (DMG) module into the discriminator, enabling joint evaluation of both global realism and local anomaly fidelity, guided by pixel-level masks. Extensive experiments on the MVTec-AD and BTAD datasets show that SARD surpasses existing methods in segmentation accuracy and visual quality, setting a new state-of-the-art for pixel-level anomaly synthesis.

</details>


### [19] [Duplex-GS: Proxy-Guided Weighted Blending for Real-Time Order-Independent Gaussian Splatting](https://arxiv.org/abs/2508.03180)
*Weihang Liu,Yuke Li,Yuxuan Li,Jingyi Yu,Xin Lou*

Main category: cs.CV

TL;DR: Duplex-GS提出了一种双层次框架，结合代理高斯表示和顺序无关渲染技术，显著提升了3D高斯泼溅的渲染效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法依赖计算密集的顺序alpha混合操作，导致资源受限平台上的性能瓶颈。

Method: 通过代理高斯表示和单元搜索光栅化优化视图自适应基数排序的开销，结合顺序无关透明技术实现高效渲染。

Result: 实验表明，该方法在多种场景下表现稳健，速度提升1.5至4倍，基数排序开销减少52.2%至86.9%。

Conclusion: Duplex-GS验证了顺序无关透明技术在高斯泼溅中的优势，实现了高质量实时渲染。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated remarkable rendering fidelity and efficiency. However, these methods still rely on computationally expensive sequential alpha-blending operations, resulting in significant overhead, particularly on resource-constrained platforms. In this paper, we propose Duplex-GS, a dual-hierarchy framework that integrates proxy Gaussian representations with order-independent rendering techniques to achieve photorealistic results while sustaining real-time performance. To mitigate the overhead caused by view-adaptive radix sort, we introduce cell proxies for local Gaussians management and propose cell search rasterization for further acceleration. By seamlessly combining our framework with Order-Independent Transparency (OIT), we develop a physically inspired weighted sum rendering technique that simultaneously eliminates "popping" and "transparency" artifacts, yielding substantial improvements in both accuracy and efficiency. Extensive experiments on a variety of real-world datasets demonstrate the robustness of our method across diverse scenarios, including multi-scale training views and large-scale environments. Our results validate the advantages of the OIT rendering paradigm in Gaussian Splatting, achieving high-quality rendering with an impressive 1.5 to 4 speedup over existing OIT based Gaussian Splatting approaches and 52.2% to 86.9% reduction of the radix sort overhead without quality degradation.

</details>


### [20] [ActionSink: Toward Precise Robot Manipulation with Dynamic Integration of Action Flow](https://arxiv.org/abs/2508.03218)
*Shanshan Guo,Xiwen Liang,Junfan Lin,Yuzheng Zhuang,Liang Lin,Xiaodan Liang*

Main category: cs.CV

TL;DR: 本文提出了一种名为ActionSink的新型机器人操作框架，通过自监督方式将机器人动作重新定义为视频中的光流（称为“动作流”），并通过检索和整合动作流来提高动作估计的精度。


<details>
  <summary>Details</summary>
Motivation: 尽管通用大型预训练模型在高层次感知和规划方面取得了进展，但低层次动作估计的低精度仍然是机器人操作性能的关键限制因素。

Method: ActionSink包含两个主要模块：1）粗到细的动作流匹配器，通过迭代检索和去噪过程提高动作流精度；2）动态动作流整合器，利用工作记忆池动态管理历史动作流，并通过多层融合模块整合当前和历史的动作流。

Result: ActionSink在LIBERO基准测试中比之前的最先进方法提高了7.9%的成功率，在LIBERO-Long任务中获得了近8%的准确率提升。

Conclusion: ActionSink通过自监督的动作流和动态整合机制，显著提高了机器人操作中的动作估计精度，为学习型机器人操作提供了新方向。

Abstract: Language-instructed robot manipulation has garnered significant interest due to the potential of learning from collected data. While the challenges in high-level perception and planning are continually addressed along the progress of general large pre-trained models, the low precision of low-level action estimation has emerged as the key limiting factor in manipulation performance. To this end, this paper introduces a novel robot manipulation framework, i.e., ActionSink, to pave the way toward precise action estimations in the field of learning-based robot manipulation. As the name suggests, ActionSink reformulates the actions of robots as action-caused optical flows from videos, called "action flow", in a self-supervised manner, which are then used to be retrieved and integrated to enhance the action estimation. Specifically, ActionSink incorporates two primary modules. The first module is a coarse-to-fine action flow matcher, which continuously refines the accuracy of action flow via iterative retrieval and denoising process. The second module is a dynamic action flow integrator, which employs a working memory pool that dynamically and efficiently manages the historical action flows that should be used to integrate to enhance the current action estimation. In this module, a multi-layer fusion module is proposed to integrate direct estimation and action flows from both the current and the working memory, achieving highly accurate action estimation through a series of estimation-integration processes. Our ActionSink framework outperformed prior SOTA on the LIBERO benchmark by a 7.9\% success rate, and obtained nearly an 8\% accuracy gain on the challenging long-horizon visual task LIBERO-Long.

</details>


### [21] [Trace3D: Consistent Segmentation Lifting via Gaussian Instance Tracing](https://arxiv.org/abs/2508.03227)
*Hongyu Shen,Junfeng Ni,Yixin Chen,Weishuo Li,Mingtao Pei,Siyuan Huang*

Main category: cs.CV

TL;DR: 提出Gaussian Instance Tracing (GIT)方法，通过实例权重矩阵和自适应密度控制，解决2D到3D分割中的不一致性问题，提升分割边界质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在2D到3D分割中存在视角间掩码不一致和边界噪声问题，缺乏语义线索优化高斯模型。

Method: 引入GIT，扩展高斯表示，加入实例权重矩阵，利用高斯一致性修正2D分割不一致；提出自适应密度控制机制，优化高斯分布。

Result: 实验表明，GIT在在线和离线场景下均提升3D分割质量，支持分层分割、对象提取和场景编辑等应用。

Conclusion: GIT通过语义优化和密度控制，显著改善了2D到3D分割的边界一致性和清晰度。

Abstract: We address the challenge of lifting 2D visual segmentation to 3D in Gaussian Splatting. Existing methods often suffer from inconsistent 2D masks across viewpoints and produce noisy segmentation boundaries as they neglect these semantic cues to refine the learned Gaussians. To overcome this, we introduce Gaussian Instance Tracing (GIT), which augments the standard Gaussian representation with an instance weight matrix across input views. Leveraging the inherent consistency of Gaussians in 3D, we use this matrix to identify and correct 2D segmentation inconsistencies. Furthermore, since each Gaussian ideally corresponds to a single object, we propose a GIT-guided adaptive density control mechanism to split and prune ambiguous Gaussians during training, resulting in sharper and more coherent 2D and 3D segmentation boundaries. Experimental results show that our method extracts clean 3D assets and consistently improves 3D segmentation in both online (e.g., self-prompting) and offline (e.g., contrastive lifting) settings, enabling applications such as hierarchical segmentation, object extraction, and scene editing.

</details>


### [22] [FFHQ-Makeup: Paired Synthetic Makeup Dataset with Facial Consistency Across Multiple Styles](https://arxiv.org/abs/2508.03241)
*Xingchao Yang,Shiori Ueda,Yuantian Huang,Tomoya Akiyama,Takafumi Taketomi*

Main category: cs.CV

TL;DR: FFHQ-Makeup是一个高质量合成的化妆数据集，解决了现有数据集中配对图像不足和真实性问题，通过改进的化妆转移方法生成18K身份的90K配对图像。


<details>
  <summary>Details</summary>
Motivation: 现有化妆数据集在真实性和配对一致性上存在不足，难以满足美容相关任务的需求。

Method: 基于FFHQ数据集，采用改进的化妆转移方法，将真实化妆风格转移到18K身份上，每个身份配对5种化妆风格。

Result: 生成了90K高质量配对图像，保持了身份和表情的一致性。

Conclusion: FFHQ-Makeup填补了高质量配对化妆数据集的空白，为美容相关研究提供了宝贵资源。

Abstract: Paired bare-makeup facial images are essential for a wide range of beauty-related tasks, such as virtual try-on, facial privacy protection, and facial aesthetics analysis. However, collecting high-quality paired makeup datasets remains a significant challenge. Real-world data acquisition is constrained by the difficulty of collecting large-scale paired images, while existing synthetic approaches often suffer from limited realism or inconsistencies between bare and makeup images. Current synthetic methods typically fall into two categories: warping-based transformations, which often distort facial geometry and compromise the precision of makeup; and text-to-image generation, which tends to alter facial identity and expression, undermining consistency. In this work, we present FFHQ-Makeup, a high-quality synthetic makeup dataset that pairs each identity with multiple makeup styles while preserving facial consistency in both identity and expression. Built upon the diverse FFHQ dataset, our pipeline transfers real-world makeup styles from existing datasets onto 18K identities by introducing an improved makeup transfer method that disentangles identity and makeup. Each identity is paired with 5 different makeup styles, resulting in a total of 90K high-quality bare-makeup image pairs. To the best of our knowledge, this is the first work that focuses specifically on constructing a makeup dataset. We hope that FFHQ-Makeup fills the gap of lacking high-quality bare-makeup paired datasets and serves as a valuable resource for future research in beauty-related tasks.

</details>


### [23] [Ultralight Polarity-Split Neuromorphic SNN for Event-Stream Super-Resolution](https://arxiv.org/abs/2508.03244)
*Chuanzhi Xu,Haoxian Zhou,Langyi Chen,Yuk Ying Chung,Qiang Qu*

Main category: cs.CV

TL;DR: 提出了一种基于脉冲神经网络（SNN）的超轻量级事件超分辨率方法，适用于资源受限设备，通过双向前极性分割事件编码和可学习时空极性感知损失优化性能。


<details>
  <summary>Details</summary>
Motivation: 事件相机的高时间分辨率、低延迟和高动态范围优势受限于空间分辨率，影响细粒度感知任务。

Method: 采用SNN设计实时流式事件超分辨率方法，引入双向前极性分割事件编码和可学习时空极性感知损失（LearnSTPLoss）。

Result: 在多数据集上实现竞争性超分辨率性能，显著减少模型大小和推理时间。

Conclusion: 轻量级设计适合嵌入事件相机或作为下游视觉任务的高效预处理模块。

Abstract: Event cameras offer unparalleled advantages such as high temporal resolution, low latency, and high dynamic range. However, their limited spatial resolution poses challenges for fine-grained perception tasks. In this work, we propose an ultra-lightweight, stream-based event-to-event super-resolution method based on Spiking Neural Networks (SNNs), designed for real-time deployment on resource-constrained devices. To further reduce model size, we introduce a novel Dual-Forward Polarity-Split Event Encoding strategy that decouples positive and negative events into separate forward paths through a shared SNN. Furthermore, we propose a Learnable Spatio-temporal Polarity-aware Loss (LearnSTPLoss) that adaptively balances temporal, spatial, and polarity consistency using learnable uncertainty-based weights. Experimental results demonstrate that our method achieves competitive super-resolution performance on multiple datasets while significantly reducing model size and inference time. The lightweight design enables embedding the module into event cameras or using it as an efficient front-end preprocessing for downstream vision tasks.

</details>


### [24] [Robust Single-Stage Fully Sparse 3D Object Detection via Detachable Latent Diffusion](https://arxiv.org/abs/2508.03252)
*Wentao Qu,Guofeng Mei,Jing Wang,Yujiao Wu,Xiaoshui Huang,Liang Xiao*

Main category: cs.CV

TL;DR: RSDNet提出了一种基于DDPM的单阶段稀疏3D目标检测方法，通过轻量级去噪网络和可分离潜在框架提高效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多步迭代推理，效率低，且对多级扰动的鲁棒性不足。

Method: RSDNet采用轻量级去噪网络（如多级DAEs）在潜在特征空间学习去噪过程，并引入语义-几何条件指导以增强稀疏表示的边界感知。

Result: 在公开基准测试中，RSDNet表现优于现有方法，达到最先进的检测性能。

Conclusion: RSDNet通过单步推理和可分离潜在框架，显著提升了3D目标检测的效率和鲁棒性。

Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have shown success in robust 3D object detection tasks. Existing methods often rely on the score matching from 3D boxes or pre-trained diffusion priors. However, they typically require multi-step iterations in inference, which limits efficiency. To address this, we propose a \textbf{R}obust single-stage fully \textbf{S}parse 3D object \textbf{D}etection \textbf{Net}work with a Detachable Latent Framework (DLF) of DDPMs, named RSDNet. Specifically, RSDNet learns the denoising process in latent feature spaces through lightweight denoising networks like multi-level denoising autoencoders (DAEs). This enables RSDNet to effectively understand scene distributions under multi-level perturbations, achieving robust and reliable detection. Meanwhile, we reformulate the noising and denoising mechanisms of DDPMs, enabling DLF to construct multi-type and multi-level noise samples and targets, enhancing RSDNet robustness to multiple perturbations. Furthermore, a semantic-geometric conditional guidance is introduced to perceive the object boundaries and shapes, alleviating the center feature missing problem in sparse representations, enabling RSDNet to perform in a fully sparse detection pipeline. Moreover, the detachable denoising network design of DLF enables RSDNet to perform single-step detection in inference, further enhancing detection efficiency. Extensive experiments on public benchmarks show that RSDNet can outperform existing methods, achieving state-of-the-art detection.

</details>


### [25] [V.I.P. : Iterative Online Preference Distillation for Efficient Video Diffusion Models](https://arxiv.org/abs/2508.03254)
*Jisoo Kim,Wooseok Seo,Junwan Kim,Seungho Park,Sooyeon Park,Youngjae Yu*

Main category: cs.CV

TL;DR: 论文提出了一种结合DPO和SFT的蒸馏方法ReDPO，以及数据筛选框架V.I.P.，用于高效且高质量的视频生成。


<details>
  <summary>Details</summary>
Motivation: 解决现有蒸馏方法因监督微调（SFT）导致的模式崩溃问题，提升剪枝模型的性能。

Method: 结合DPO和SFT，利用DPO指导学生模型恢复目标属性，而非被动模仿教师模型；同时提出V.I.P.框架筛选高质量数据对。

Result: 在VideoCrafter2和AnimateDiff上分别减少36.2%和67.5%的参数，性能保持或超越完整模型。

Conclusion: ReDPO和V.I.P.框架有效提升了剪枝模型的视频生成效率和质量。

Abstract: With growing interest in deploying text-to-video (T2V) models in resource-constrained environments, reducing their high computational cost has become crucial, leading to extensive research on pruning and knowledge distillation methods while maintaining performance. However, existing distillation methods primarily rely on supervised fine-tuning (SFT), which often leads to mode collapse as pruned models with reduced capacity fail to directly match the teacher's outputs, ultimately resulting in degraded quality. To address this challenge, we propose an effective distillation method, ReDPO, that integrates DPO and SFT. Our approach leverages DPO to guide the student model to focus on recovering only the targeted properties, rather than passively imitating the teacher, while also utilizing SFT to enhance overall performance. We additionally propose V.I.P., a novel framework for filtering and curating high-quality pair datasets, along with a step-by-step online approach for calibrated training. We validate our method on two leading T2V models, VideoCrafter2 and AnimateDiff, achieving parameter reduction of 36.2% and 67.5% each, while maintaining or even surpassing the performance of full models. Further experiments demonstrate the effectiveness of both ReDPO and V.I.P. framework in enabling efficient and high-quality video generation. Our code and videos are available at https://jiiiisoo.github.io/VIP.github.io/.

</details>


### [26] [Zero Shot Domain Adaptive Semantic Segmentation by Synthetic Data Generation and Progressive Adaptation](https://arxiv.org/abs/2508.03300)
*Jun Luo,Zijing Zhao,Yang Liu*

Main category: cs.CV

TL;DR: SDGPA提出了一种零样本域适应语义分割方法，通过合成数据生成和渐进适应解决训练与测试数据分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 解决语义分割模型在训练与测试数据分布偏移时的性能限制，尤其是在零样本域适应场景下（无目标域图像，仅有文本描述）。

Method: 利用预训练文本到图像扩散模型生成目标风格图像，通过裁剪和编辑小区域提高空间精度，构建中间域并采用渐进适应策略。

Result: 实验表明，SDGPA在零样本语义分割中达到最先进性能。

Conclusion: SDGPA通过合成数据生成和渐进适应策略，有效解决了零样本域适应语义分割问题。

Abstract: Deep learning-based semantic segmentation models achieve impressive results yet remain limited in handling distribution shifts between training and test data. In this paper, we present SDGPA (Synthetic Data Generation and Progressive Adaptation), a novel method that tackles zero-shot domain adaptive semantic segmentation, in which no target images are available, but only a text description of the target domain's style is provided. To compensate for the lack of target domain training data, we utilize a pretrained off-the-shelf text-to-image diffusion model, which generates training images by transferring source domain images to target style. Directly editing source domain images introduces noise that harms segmentation because the layout of source images cannot be precisely maintained. To address inaccurate layouts in synthetic data, we propose a method that crops the source image, edits small patches individually, and then merges them back together, which helps improve spatial precision. Recognizing the large domain gap, SDGPA constructs an augmented intermediate domain, leveraging easier adaptation subtasks to enable more stable model adaptation to the target domain. Additionally, to mitigate the impact of noise in synthetic data, we design a progressive adaptation strategy, ensuring robust learning throughout the training process. Extensive experiments demonstrate that our method achieves state-of-the-art performance in zero-shot semantic segmentation. The code is available at https://github.com/ROUJINN/SDGPA

</details>


### [27] [Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding and Generation](https://arxiv.org/abs/2508.03320)
*Peiyu Wang,Yi Peng,Yimeng Gan,Liang Hu,Tianyidan Xie,Xiaokun Wang,Yichen Wei,Chuanxin Tang,Bo Zhu,Changshi Li,Hongyang Wei,Eric Li,Xuchen Song,Yang Liu,Yahui Zhou*

Main category: cs.CV

TL;DR: Skywork UniPic是一个15亿参数的自回归模型，统一了图像理解、文本到图像生成和图像编辑功能，性能优异且资源需求低。


<details>
  <summary>Details</summary>
Motivation: 旨在证明紧凑的多模态系统可以在商品硬件上实现最先进的性能，同时避免任务特定的适配器或模块间连接器的需求。

Method: 采用解耦编码策略、渐进式分辨率感知训练计划以及精心策划的大规模数据集和奖励模型。

Result: 在多个基准测试中表现优异，如GenEval得分0.86，DPG-Bench复杂生成记录85.5，并能在低GPU内存下生成高分辨率图像。

Conclusion: Skywork UniPic为可部署的高保真多模态AI提供了实用范例，代码和权重已公开。

Abstract: We introduce Skywork UniPic, a 1.5 billion-parameter autoregressive model that unifies image understanding, text-to-image generation, and image editing within a single architecture-eliminating the need for task-specific adapters or inter-module connectors-and demonstrate that compact multimodal systems can achieve state-of-the-art performance on commodity hardware. Skywork UniPic achieves a GenEval score of 0.86, surpassing most existing unified models; sets a new DPG-Bench complex-generation record of 85.5; attains 5.83 on GEditBench-EN and 3.49 on ImgEdit-Bench for image editing; and generates 1024 x 1024 images with under 15 GB of GPU memory (e.g., RTX 4090). (1) a decoupled encoding strategy that leverages a masked autoregressive encoder for synthesis and a SigLIP2 encoder for understanding, all feeding a shared autoregressive decoder; (2) a progressive, resolution-aware training schedule scaling from 256 x 256 to 1024 x 1024 while dynamically unfreezing parameters to balance capacity and stability; and (3) meticulously curated, 100 million-scale datasets augmented with task-specific reward models to refine generation and editing objectives. By demonstrating that high-fidelity multimodal integration need not incur prohibitive resource demands, Skywork UniPic establishes a practical paradigm for deployable, high-fidelity multimodal AI. Code and weights are publicly available at https://huggingface.co/Skywork/Skywork-UniPic-1.5B.

</details>


### [28] [Macro-from-Micro Planning for High-Quality and Parallelized Autoregressive Long Video Generation](https://arxiv.org/abs/2508.03334)
*Xunzhi Xiang,Yabo Chen,Guiyu Zhang,Zhongyu Wang,Zhe Gao,Quanming Xiang,Gonghu Shang,Junqi Liu,Haibin Huang,Yang Gao,Chi Zhang,Qi Fan,Xuelong Li*

Main category: cs.CV

TL;DR: 论文提出了一种名为MMPL的框架，通过分层规划解决自回归扩散模型在长视频生成中的时间漂移问题，实现了高效并行化生成。


<details>
  <summary>Details</summary>
Motivation: 自回归扩散模型在长视频生成中存在时间漂移和并行化困难的问题，限制了其应用。

Method: 提出MMPL框架，分Micro Planning和Macro Planning两阶段规划全局情节，并通过并行填充中间帧。

Result: 实验表明，该方法在质量和稳定性上优于现有长视频生成模型。

Conclusion: MMPL框架有效解决了长视频生成中的时间漂移问题，并实现了高效并行化。

Abstract: Current autoregressive diffusion models excel at video generation but are generally limited to short temporal durations. Our theoretical analysis indicates that the autoregressive modeling typically suffers from temporal drift caused by error accumulation and hinders parallelization in long video synthesis. To address these limitations, we propose a novel planning-then-populating framework centered on Macro-from-Micro Planning (MMPL) for long video generation. MMPL sketches a global storyline for the entire video through two hierarchical stages: Micro Planning and Macro Planning. Specifically, Micro Planning predicts a sparse set of future keyframes within each short video segment, offering motion and appearance priors to guide high-quality video segment generation. Macro Planning extends the in-segment keyframes planning across the entire video through an autoregressive chain of micro plans, ensuring long-term consistency across video segments. Subsequently, MMPL-based Content Populating generates all intermediate frames in parallel across segments, enabling efficient parallelization of autoregressive generation. The parallelization is further optimized by Adaptive Workload Scheduling for balanced GPU execution and accelerated autoregressive video generation. Extensive experiments confirm that our method outperforms existing long video generation models in quality and stability. Generated videos and comparison results are in our project page.

</details>


### [29] [Beyond Illumination: Fine-Grained Detail Preservation in Extreme Dark Image Restoration](https://arxiv.org/abs/2508.03336)
*Tongshun Zhang,Pingping Liu,Zixuan Zhong,Zijian Zhang,Qiuzhan Zhou*

Main category: cs.CV

TL;DR: 提出了一种双阶段方法，通过残差傅里叶引导模块（RFGM）和Mamba模块，有效恢复极暗图像中的细节。


<details>
  <summary>Details</summary>
Motivation: 现有方法在恢复暗图像细节时效果不佳，无法保留精细结构和锐利边缘，影响下游应用。

Method: 第一阶段使用RFGM在频域恢复全局光照；第二阶段采用Patch Mamba和Grad Mamba模块细化纹理结构。

Result: 实验表明，该方法显著提升了细节恢复性能，且模块轻量高效。

Conclusion: 该方法在恢复暗图像细节方面表现优异，并可无缝集成到现有框架中。

Abstract: Recovering fine-grained details in extremely dark images remains challenging due to severe structural information loss and noise corruption. Existing enhancement methods often fail to preserve intricate details and sharp edges, limiting their effectiveness in downstream applications like text and edge detection. To address these deficiencies, we propose an efficient dual-stage approach centered on detail recovery for dark images. In the first stage, we introduce a Residual Fourier-Guided Module (RFGM) that effectively restores global illumination in the frequency domain. RFGM captures inter-stage and inter-channel dependencies through residual connections, providing robust priors for high-fidelity frequency processing while mitigating error accumulation risks from unreliable priors. The second stage employs complementary Mamba modules specifically designed for textural structure refinement: (1) Patch Mamba operates on channel-concatenated non-downsampled patches, meticulously modeling pixel-level correlations to enhance fine-grained details without resolution loss. (2) Grad Mamba explicitly focuses on high-gradient regions, alleviating state decay in state space models and prioritizing reconstruction of sharp edges and boundaries. Extensive experiments on multiple benchmark datasets and downstream applications demonstrate that our method significantly improves detail recovery performance while maintaining efficiency. Crucially, the proposed modules are lightweight and can be seamlessly integrated into existing Fourier-based frameworks with minimal computational overhead. Code is available at https://github.com/bywlzts/RFGM.

</details>


### [30] [CIVQLLIE: Causal Intervention with Vector Quantization for Low-Light Image Enhancement](https://arxiv.org/abs/2508.03338)
*Tongshun Zhang,Pingping Liu,Zhe Zhang,Qiuzhan Zhou*

Main category: cs.CV

TL;DR: CIVQLLIE是一种基于离散表示学习和因果推理的低光图像增强框架，通过向量量化和多级因果干预解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 夜间场景图像可见性差，现有低光增强方法缺乏解释性或依赖不可靠的先验，物理方法则简化假设难以适应复杂场景。

Method: 利用向量量化将图像特征映射到离散视觉标记代码本，通过像素级和特征级因果干预校正分布偏移，并结合高频细节重建模块。

Result: CIVQLLIE通过可靠的先验和多级干预，显著提升了低光图像的增强效果。

Conclusion: 该框架结合离散表示学习和因果推理，为低光图像增强提供了更可靠和高效的解决方案。

Abstract: Images captured in nighttime scenes suffer from severely reduced visibility, hindering effective content perception. Current low-light image enhancement (LLIE) methods face significant challenges: data-driven end-to-end mapping networks lack interpretability or rely on unreliable prior guidance, struggling under extremely dark conditions, while physics-based methods depend on simplified assumptions that often fail in complex real-world scenarios. To address these limitations, we propose CIVQLLIE, a novel framework that leverages the power of discrete representation learning through causal reasoning. We achieve this through Vector Quantization (VQ), which maps continuous image features to a discrete codebook of visual tokens learned from large-scale high-quality images. This codebook serves as a reliable prior, encoding standardized brightness and color patterns that are independent of degradation. However, direct application of VQ to low-light images fails due to distribution shifts between degraded inputs and the learned codebook. Therefore, we propose a multi-level causal intervention approach to systematically correct these shifts. First, during encoding, our Pixel-level Causal Intervention (PCI) module intervenes to align low-level features with the brightness and color distributions expected by the codebook. Second, a Feature-aware Causal Intervention (FCI) mechanism with Low-frequency Selective Attention Gating (LSAG) identifies and enhances channels most affected by illumination degradation, facilitating accurate codebook token matching while enhancing the encoder's generalization performance through flexible feature-level intervention. Finally, during decoding, the High-frequency Detail Reconstruction Module (HDRM) leverages structural information preserved in the matched codebook representations to reconstruct fine details using deformable convolution techniques.

</details>


### [31] [Diffusion Once and Done: Degradation-Aware LoRA for Efficient All-in-One Image Restoration](https://arxiv.org/abs/2508.03373)
*Ni Tang,Xiaotong Luo,Zihan Cheng,Liangtai Zhou,Dongxiao Zhang,Yanyun Qu*

Main category: cs.CV

TL;DR: 提出了一种高效的图像修复方法DOD，通过单步采样实现高性能修复，解决了现有方法的高计算成本和适应性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的图像修复方法计算成本高且适应性有限，需要改进。

Method: 引入多退化特征调制和参数高效的低秩适应，结合高保真细节增强模块。

Result: 实验表明，DOD在视觉质量和推理效率上优于现有方法。

Conclusion: DOD是一种高效且适应性强的图像修复方法。

Abstract: Diffusion models have revealed powerful potential in all-in-one image restoration (AiOIR), which is talented in generating abundant texture details. The existing AiOIR methods either retrain a diffusion model or fine-tune the pretrained diffusion model with extra conditional guidance. However, they often suffer from high inference costs and limited adaptability to diverse degradation types. In this paper, we propose an efficient AiOIR method, Diffusion Once and Done (DOD), which aims to achieve superior restoration performance with only one-step sampling of Stable Diffusion (SD) models. Specifically, multi-degradation feature modulation is first introduced to capture different degradation prompts with a pretrained diffusion model. Then, parameter-efficient conditional low-rank adaptation integrates the prompts to enable the fine-tuning of the SD model for adapting to different degradation types. Besides, a high-fidelity detail enhancement module is integrated into the decoder of SD to improve structural and textural details. Experiments demonstrate that our method outperforms existing diffusion-based restoration approaches in both visual quality and inference efficiency.

</details>


### [32] [SCFlow: Implicitly Learning Style and Content Disentanglement with Flow Models](https://arxiv.org/abs/2508.03402)
*Pingchuan Ma,Xiaopei Yang,Yusong Li,Ming Gui,Felix Krause,Johannes Schusterbauer,Björn Ommer*

Main category: cs.CV

TL;DR: SCFlow提出了一种通过可逆合并风格与内容的方法，避免显式解耦的挑战，利用流匹配框架实现自然解耦。


<details>
  <summary>Details</summary>
Motivation: 现有方法在风格与内容解耦时面临语义重叠和主观性的挑战，SCFlow尝试通过可逆合并绕过显式解耦的困难。

Method: SCFlow采用流匹配框架，学习风格与内容的双向映射，通过合成数据集训练，无需显式监督。

Result: SCFlow在零样本设置下泛化至ImageNet-1k和WikiArt，性能竞争性，表明解耦可从可逆合并中自然涌现。

Conclusion: SCFlow通过可逆合并风格与内容，实现了自然解耦，为可控生成任务提供了新思路。

Abstract: Explicitly disentangling style and content in vision models remains challenging due to their semantic overlap and the subjectivity of human perception. Existing methods propose separation through generative or discriminative objectives, but they still face the inherent ambiguity of disentangling intertwined concepts. Instead, we ask: Can we bypass explicit disentanglement by learning to merge style and content invertibly, allowing separation to emerge naturally? We propose SCFlow, a flow-matching framework that learns bidirectional mappings between entangled and disentangled representations. Our approach is built upon three key insights: 1) Training solely to merge style and content, a well-defined task, enables invertible disentanglement without explicit supervision; 2) flow matching bridges on arbitrary distributions, avoiding the restrictive Gaussian priors of diffusion models and normalizing flows; and 3) a synthetic dataset of 510,000 samples (51 styles $\times$ 10,000 content samples) was curated to simulate disentanglement through systematic style-content pairing. Beyond controllable generation tasks, we demonstrate that SCFlow generalizes to ImageNet-1k and WikiArt in zero-shot settings and achieves competitive performance, highlighting that disentanglement naturally emerges from the invertible merging process.

</details>


### [33] [Spatial Imputation Drives Cross-Domain Alignment for EEG Classification](https://arxiv.org/abs/2508.03437)
*Hongjun Liu,Chao Yao,Yalan Zhang,Xiaokun wang,Xiaojuan Ban*

Main category: cs.CV

TL;DR: IMAC是一个自监督框架，通过通道依赖掩码和插值任务解决跨域EEG信号分类中的数据分布偏移问题，实现了空间时间序列对齐。


<details>
  <summary>Details</summary>
Motivation: EEG信号分类面临因电极配置、采集协议和硬件差异导致的数据分布偏移问题，需要一种鲁棒的方法进行跨域对齐。

Method: IMAC采用3D-to-2D位置统一映射标准化电极布局，提出通道依赖掩码和重建任务，模拟跨域变化，并通过解耦结构分别建模时空信息。

Result: 在10个公开EEG数据集上，IMAC在跨主体和跨中心验证中表现优异，分类准确率领先基线方法35%。

Conclusion: IMAC通过空间时间序列对齐和解耦结构，显著提升了EEG信号分类的鲁棒性和准确性。

Abstract: Electroencephalogram (EEG) signal classification faces significant challenges due to data distribution shifts caused by heterogeneous electrode configurations, acquisition protocols, and hardware discrepancies across domains. This paper introduces IMAC, a novel channel-dependent mask and imputation self-supervised framework that formulates the alignment of cross-domain EEG data shifts as a spatial time series imputation task. To address heterogeneous electrode configurations in cross-domain scenarios, IMAC first standardizes different electrode layouts using a 3D-to-2D positional unification mapping strategy, establishing unified spatial representations. Unlike previous mask-based self-supervised representation learning methods, IMAC introduces spatio-temporal signal alignment. This involves constructing a channel-dependent mask and reconstruction task framed as a low-to-high resolution EEG spatial imputation problem. Consequently, this approach simulates cross-domain variations such as channel omissions and temporal instabilities, thus enabling the model to leverage the proposed imputer for robust signal alignment during inference. Furthermore, IMAC incorporates a disentangled structure that separately models the temporal and spatial information of the EEG signals separately, reducing computational complexity while enhancing flexibility and adaptability. Comprehensive evaluations across 10 publicly available EEG datasets demonstrate IMAC's superior performance, achieving state-of-the-art classification accuracy in both cross-subject and cross-center validation scenarios. Notably, IMAC shows strong robustness under both simulated and real-world distribution shifts, surpassing baseline methods by up to $35$\% in integrity scores while maintaining consistent classification accuracy.

</details>


### [34] [RAAG: Ratio Aware Adaptive Guidance](https://arxiv.org/abs/2508.03442)
*Shangwen Zhu,Qianyu Peng,Yuting Hu,Zhantao Yang,Han Zhang,Zhao Pu,Ruili Feng,Fan Cheng*

Main category: cs.CV

TL;DR: 研究发现流式生成模型在早期采样阶段对引导尺度敏感，提出了一种自适应引导调度方法，显著提升采样速度与生成质量。


<details>
  <summary>Details</summary>
Motivation: 探索引导机制在不同采样阶段的作用，尤其是快速低步长模式下，以解决早期步骤对引导尺度的敏感性。

Method: 通过理论分析和实验验证，提出基于RATIO的自适应引导调度方法，自动调整早期步骤的引导强度。

Result: 实验证明该方法在图像和视频模型中可提升3倍采样速度，同时保持或改进生成质量。

Conclusion: 自适应引导调度是释放快速流式生成模型潜力的关键。

Abstract: Flow-based generative models have recently achieved remarkable progress in image and video synthesis, with classifier-free guidance (CFG) becoming the standard tool for high-fidelity, controllable generation. However, despite their practical success, little is known about how guidance interacts with different stages of the sampling process-especially in the fast, low-step regimes typical of modern flow-based pipelines. In this work, we uncover and analyze a fundamental instability: the earliest reverse steps are acutely sensitive to the guidance scale, owing to a pronounced spike in the relative strength (RATIO) of conditional to unconditional predictions. Through rigorous theoretical analysis and empirical validation, we show that this RATIO spike is intrinsic to the data distribution, independent of the model architecture, and causes exponential error amplification when paired with strong guidance. To address this, we propose a simple, theoretically grounded, RATIO-aware adaptive guidance schedule that automatically dampens the guidance scale at early steps based on the evolving RATIO, using a closed-form exponential decay. Our method is lightweight, requires no additional inference overhead, and is compatible with standard flow frameworks. Experiments across state-of-the-art image (SD3.5, Lumina) and video (WAN2.1) models demonstrate that our approach enables up to 3x faster sampling while maintaining or improving generation quality, robustness, and semantic alignment. Extensive ablation studies further confirm the generality and stability of our schedule across models, datasets, and hyperparameters. Our findings highlight the critical role of stepwise guidance adaptation in unlocking the full potential of fast flow-based generative models.

</details>


### [35] [CoPS: Conditional Prompt Synthesis for Zero-Shot Anomaly Detection](https://arxiv.org/abs/2508.03447)
*Qiyu Chen,Zhen Qu,Wei Luo,Haiming Yao,Yunkang Cao,Yuxin Jiang,Yinan Duan,Huiyuan Luo,Chengkan Lv,Zhengtao Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种名为CoPS的新框架，通过动态生成基于视觉特征的提示，提升了零样本异常检测的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在零样本异常检测中面临静态提示无法捕捉连续多样状态和固定标签信息稀疏的问题。

Method: CoPS通过提取正常和异常原型并注入提示中，同时利用变分自编码器融合多样类别信息。

Result: 在13个工业和医学数据集上，CoPS在分类和分割任务中AUROC提升了2.5%。

Conclusion: CoPS通过动态提示和语义融合显著提升了零样本异常检测的泛化能力。

Abstract: Recently, large pre-trained vision-language models have shown remarkable performance in zero-shot anomaly detection (ZSAD). With fine-tuning on a single auxiliary dataset, the model enables cross-category anomaly detection on diverse datasets covering industrial defects and medical lesions. Compared to manually designed prompts, prompt learning eliminates the need for expert knowledge and trial-and-error. However, it still faces the following challenges: (i) static learnable tokens struggle to capture the continuous and diverse patterns of normal and anomalous states, limiting generalization to unseen categories; (ii) fixed textual labels provide overly sparse category information, making the model prone to overfitting to a specific semantic subspace. To address these issues, we propose Conditional Prompt Synthesis (CoPS), a novel framework that synthesizes dynamic prompts conditioned on visual features to enhance ZSAD performance. Specifically, we extract representative normal and anomaly prototypes from fine-grained patch features and explicitly inject them into prompts, enabling adaptive state modeling. Given the sparsity of class labels, we leverage a variational autoencoder to model semantic image features and implicitly fuse varied class tokens into prompts. Additionally, integrated with our spatially-aware alignment mechanism, extensive experiments demonstrate that CoPS surpasses state-of-the-art methods by 2.5% AUROC in both classification and segmentation across 13 industrial and medical datasets. Code will be available at https://github.com/cqylunlun/CoPS.

</details>


### [36] [Video Demoireing using Focused-Defocused Dual-Camera System](https://arxiv.org/abs/2508.03449)
*Xuan Dong,Xiangyuan Sun,Xia Wang,Jian Song,Ya Li,Weixin Li*

Main category: cs.CV

TL;DR: 提出了一种基于双摄像头的去摩尔纹框架，通过同步拍摄聚焦和散焦视频，利用散焦视频指导聚焦视频的去摩尔纹处理，显著提升了效果。


<details>
  <summary>Details</summary>
Motivation: 现有去摩尔纹方法难以区分摩尔纹与真实纹理，且难以保持色调一致性和时间连贯性。

Method: 使用双摄像头同步拍摄聚焦和散焦视频，通过光流对齐和多尺度CNN结合多维训练损失进行去摩尔纹处理，最后使用联合双边滤波保持一致性。

Result: 实验表明，该方法显著优于现有图像和视频去摩尔纹方法。

Conclusion: 双摄像头框架有效解决了摩尔纹与真实纹理的区分问题，同时保持了色调和时间一致性。

Abstract: Moire patterns, unwanted color artifacts in images and videos, arise from the interference between spatially high-frequency scene contents and the spatial discrete sampling of digital cameras. Existing demoireing methods primarily rely on single-camera image/video processing, which faces two critical challenges: 1) distinguishing moire patterns from visually similar real textures, and 2) preserving tonal consistency and temporal coherence while removing moire artifacts. To address these issues, we propose a dual-camera framework that captures synchronized videos of the same scene: one in focus (retaining high-quality textures but may exhibit moire patterns) and one defocused (with significantly reduced moire patterns but blurred textures). We use the defocused video to help distinguish moire patterns from real texture, so as to guide the demoireing of the focused video. We propose a frame-wise demoireing pipeline, which begins with an optical flow based alignment step to address any discrepancies in displacement and occlusion between the focused and defocused frames. Then, we leverage the aligned defocused frame to guide the demoireing of the focused frame using a multi-scale CNN and a multi-dimensional training loss. To maintain tonal and temporal consistency, our final step involves a joint bilateral filter to leverage the demoireing result from the CNN as the guide to filter the input focused frame to obtain the final output. Experimental results demonstrate that our proposed framework largely outperforms state-of-the-art image and video demoireing methods.

</details>


### [37] [AVPDN: Learning Motion-Robust and Scale-Adaptive Representations for Video-Based Polyp Detection](https://arxiv.org/abs/2508.03458)
*Zilin Chen,Shengnan Lu*

Main category: cs.CV

TL;DR: AVPDN是一种用于结肠镜视频中多尺度息肉检测的鲁棒框架，通过AFIA和SACI模块提升特征表示和多尺度特征整合。


<details>
  <summary>Details</summary>
Motivation: 结肠镜视频的动态性和快速相机移动导致背景噪声和假阳性风险增加，需要一种更有效的息肉检测方法。

Method: AVPDN结合AFIA模块（三支架构增强特征表示）和SACI模块（多尺度特征整合），利用自注意力和扩张卷积。

Result: 在多个公开基准测试中表现优异，展示了方法的有效性和泛化能力。

Conclusion: AVPDN在视频息肉检测任务中具有竞争力，解决了动态视频中的噪声和假阳性问题。

Abstract: Accurate detection of polyps is of critical importance for the early and intermediate stages of colorectal cancer diagnosis. Compared to static images, dynamic colonoscopy videos provide more comprehensive visual information, which can facilitate the development of effective treatment plans. However, unlike fixed-camera recordings, colonoscopy videos often exhibit rapid camera movement, introducing substantial background noise that disrupts the structural integrity of the scene and increases the risk of false positives. To address these challenges, we propose the Adaptive Video Polyp Detection Network (AVPDN), a robust framework for multi-scale polyp detection in colonoscopy videos. AVPDN incorporates two key components: the Adaptive Feature Interaction and Augmentation (AFIA) module and the Scale-Aware Context Integration (SACI) module. The AFIA module adopts a triple-branch architecture to enhance feature representation. It employs dense self-attention for global context modeling, sparse self-attention to mitigate the influence of low query-key similarity in feature aggregation, and channel shuffle operations to facilitate inter-branch information exchange. In parallel, the SACI module is designed to strengthen multi-scale feature integration. It utilizes dilated convolutions with varying receptive fields to capture contextual information at multiple spatial scales, thereby improving the model's denoising capability. Experiments conducted on several challenging public benchmarks demonstrate the effectiveness and generalization ability of the proposed method, achieving competitive performance in video-based polyp detection tasks.

</details>


### [38] [When Cars Have Stereotypes: Auditing Demographic Bias in Objects from Text-to-Image Models](https://arxiv.org/abs/2508.03483)
*Dasol Choi Jihwan Lee,Minjae Lee,Minsuk Kahng*

Main category: cs.CV

TL;DR: 论文研究了文本到图像生成中的对象（如汽车）的隐性人口统计偏见，提出了SODA框架来系统测量这些偏见，发现模型生成的图像中存在与人口统计相关的视觉属性偏见。


<details>
  <summary>Details</summary>
Motivation: 探索文本到图像生成中对象的人口统计偏见，揭示模型如何反映和强化刻板印象。

Method: 引入SODA框架，通过比较中性提示和人口统计提示生成的图像（共2,700张），分析视觉属性的差异。

Result: 发现人口统计提示导致特定视觉属性（如颜色模式）的偏见，某些模型生成多样性较低，加剧了视觉差异。

Conclusion: SODA框架为揭示生成模型中的刻板印象提供了实用方法，是迈向更负责任AI开发的重要一步。

Abstract: While prior research on text-to-image generation has predominantly focused on biases in human depictions, we investigate a more subtle yet pervasive phenomenon: demographic bias in generated objects (e.g., cars). We introduce SODA (Stereotyped Object Diagnostic Audit), a novel framework for systematically measuring such biases. Our approach compares visual attributes of objects generated with demographic cues (e.g., "for young people'') to those from neutral prompts, across 2,700 images produced by three state-of-the-art models (GPT Image-1, Imagen 4, and Stable Diffusion) in five object categories. Through a comprehensive analysis, we uncover strong associations between specific demographic groups and visual attributes, such as recurring color patterns prompted by gender or ethnicity cues. These patterns reflect and reinforce not only well-known stereotypes but also more subtle and unintuitive biases. We also observe that some models generate less diverse outputs, which in turn amplifies the visual disparities compared to neutral prompts. Our proposed auditing framework offers a practical approach for testing, revealing how stereotypes still remain embedded in today's generative models. We see this as an essential step toward more systematic and responsible AI development.

</details>


### [39] [LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Text-to-Image Generation](https://arxiv.org/abs/2508.03485)
*Lianwei Yang,Haokun Lin,Tianchen Zhao,Yichen Wu,Hongyu Zhu,Ruiqi Xie,Zhenan Sun,Yu Wang,Qingyi Gu*

Main category: cs.CV

TL;DR: LRQ-DiT提出了一种高效的低比特后训练量化框架，通过Twin-Log量化和自适应旋转方案，解决了DiT模型在极端低比特量化中的权重分布和激活异常问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: DiT模型在文本到图像生成中表现出色，但其高计算成本和参数量在资源受限场景中难以应用。现有PTQ方法在极端低比特量化下性能下降严重。

Method: 提出LRQ-DiT框架，包括Twin-Log量化（TLQ）和自适应旋转方案（ARS），分别针对权重分布和激活异常进行优化。

Result: 在PixArt、FLUX等模型及多个数据集上验证，LRQ-DiT在低比特量化下保持了图像质量，优于现有PTQ基线。

Conclusion: LRQ-DiT为DiT模型的低比特量化提供了高效解决方案，显著减少了资源需求，同时保持了生成质量。

Abstract: Diffusion Transformers (DiTs) have achieved impressive performance in text-to-image generation. However, their high computational cost and large parameter sizes pose significant challenges for usage in resource-constrained scenarios. Post-training quantization (PTQ) is a promising solution to reduce memory usage and accelerate inference, but existing PTQ methods suffer from severe performance degradation under extreme low-bit settings. We identify two key obstacles to low-bit post-training quantization for DiT models: (1) model weights follow a Gaussian-like distribution with long tails, causing uniform quantization to poorly allocate intervals and leading to significant errors; (2) two types of activation outliers: (i) Mild Outliers with slightly elevated values, and (ii) Salient Outliers with large magnitudes concentrated in specific channels, which disrupt activation quantization. To address these issues, we propose LRQ-DiT, an efficient and accurate PTQ framework. We introduce Twin-Log Quantization (TLQ), a log-based method that aligns well with the weight distribution and reduces quantization errors. We also propose an Adaptive Rotation Scheme (ARS) that dynamically applies Hadamard or outlier-aware rotations based on activation fluctuation, effectively mitigating the impact of both types of outliers. We evaluate LRQ-DiT on PixArt and FLUX under various bit-width settings, and validate the performance on COCO, MJHQ, and sDCI datasets. LRQ-DiT achieves low-bit quantization of DiT models while preserving image quality, outperforming existing PTQ baselines.

</details>


### [40] [Quality-Aware Language-Conditioned Local Auto-Regressive Anomaly Synthesis and Detection](https://arxiv.org/abs/2508.03539)
*Long Qian,Bingke Zhu,Yingying Chen,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: ARAS是一种语言条件自回归异常合成方法，通过令牌锚定的潜在编辑精确注入局部缺陷，结合QARAD框架提升异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散和粗修复方法在结构缺陷、语义控制和生成效率上的不足。

Method: 采用语言条件自回归方法，结合硬门控自回归操作符和无训练上下文保留掩码采样核。

Result: 在MVTec AD、VisA和BTAD数据集上表现优于现有方法，合成速度提升5倍。

Conclusion: ARAS和QARAD框架显著提升了异常合成的质量和检测性能。

Abstract: Despite substantial progress in anomaly synthesis methods, existing diffusion-based and coarse inpainting pipelines commonly suffer from structural deficiencies such as micro-structural discontinuities, limited semantic controllability, and inefficient generation. To overcome these limitations, we introduce ARAS, a language-conditioned, auto-regressive anomaly synthesis approach that precisely injects local, text-specified defects into normal images via token-anchored latent editing. Leveraging a hard-gated auto-regressive operator and a training-free, context-preserving masked sampling kernel, ARAS significantly enhances defect realism, preserves fine-grained material textures, and provides continuous semantic control over synthesized anomalies. Integrated within our Quality-Aware Re-weighted Anomaly Detection (QARAD) framework, we further propose a dynamic weighting strategy that emphasizes high-quality synthetic samples by computing an image-text similarity score with a dual-encoder model. Extensive experiments across three benchmark datasets-MVTec AD, VisA, and BTAD, demonstrate that our QARAD outperforms SOTA methods in both image- and pixel-level anomaly detection tasks, achieving improved accuracy, robustness, and a 5 times synthesis speedup compared to diffusion-based alternatives. Our complete code and synthesized dataset will be publicly available.

</details>


### [41] [Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations and Sentences](https://arxiv.org/abs/2508.03542)
*Dmitrii Korzh,Dmitrii Tarasov,Artyom Iudin,Elvir Karimov,Matvey Skripkin,Nikita Kuzmin,Andrey Kuznetsov,Oleg Y. Rogov,Ivan Oseledets*

Main category: cs.CV

TL;DR: 论文提出了一种开源数据集和方法，用于将口语数学表达式转换为LaTeX，解决了现有方法的局限性，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决口语数学表达式转换为LaTeX的挑战，填补现有方法在数据集规模、多语言支持和应用场景上的不足。

Method: 基于ASR后校正模型和音频语言模型，结合开源数据集（66,000个标注样本）进行训练和测试。

Result: 在MathSpeech基准测试中CER为28%，在S2L-equations基准测试中显著优于MathSpeech模型（27% vs. 64%），并在S2L-sentences基准测试中CER为40%。

Conclusion: 该研究为多模态AI在数学内容识别领域的未来进展奠定了基础。

Abstract: Conversion of spoken mathematical expressions is a challenging task that involves transcribing speech into a strictly structured symbolic representation while addressing the ambiguity inherent in the pronunciation of equations. Although significant progress has been achieved in automatic speech recognition (ASR) and language models (LM), the problem of converting spoken mathematics into LaTeX remains underexplored. This task directly applies to educational and research domains, such as lecture transcription or note creation. Based on ASR post-correction, prior work requires 2 transcriptions, focuses only on isolated equations, has a limited test set, and provides neither training data nor multilingual coverage. To address these issues, we present the first fully open-source large-scale dataset, comprising over 66,000 human-annotated audio samples of mathematical equations and sentences in both English and Russian, drawn from diverse scientific domains. In addition to the ASR post-correction models and few-shot prompting, we apply audio language models, demonstrating comparable character error rate (CER) results on the MathSpeech benchmark (28% vs. 30%) for the equations conversion. In contrast, on the proposed S2L-equations benchmark, our models outperform the MathSpeech model by a substantial margin of more than 40 percentage points, even after accounting for LaTeX formatting artifacts (27% vs. 64%). We establish the first benchmark for mathematical sentence recognition (S2L-sentences) and achieve an equation CER of 40%. This work lays the groundwork for future advances in multimodal AI, with a particular focus on mathematical content recognition.

</details>


### [42] [Uni3R: Unified 3D Reconstruction and Semantic Understanding via Generalizable Gaussian Splatting from Unposed Multi-View Images](https://arxiv.org/abs/2508.03643)
*Xiangyu Sun,Haoyi jiang,Liu Liu,Seungtae Nam,Gyeongjin Kang,Xinjie wang,Wei Sui,Zhizhong Su,Wenyu Liu,Xinggang Wang,Eunbyung Park*

Main category: cs.CV

TL;DR: Uni3R是一个新的前馈框架，直接从无姿态多视图图像中联合重建带有开放词汇语义的统一3D场景表示，避免了传统方法的解耦和优化成本。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在3D场景重建中语义理解与重建解耦或需要昂贵优化的问题，提升可扩展性和泛化性。

Method: 利用跨视图Transformer整合多视图信息，回归带有语义特征场的3D高斯基元，实现统一表示。

Result: 在多个基准测试中达到新SOTA，如RE10K上25.07 PSNR和ScanNet上55.84 mIoU。

Conclusion: Uni3R为可泛化的统一3D场景重建和理解提供了新范式。

Abstract: Reconstructing and semantically interpreting 3D scenes from sparse 2D views remains a fundamental challenge in computer vision. Conventional methods often decouple semantic understanding from reconstruction or necessitate costly per-scene optimization, thereby restricting their scalability and generalizability. In this paper, we introduce Uni3R, a novel feed-forward framework that jointly reconstructs a unified 3D scene representation enriched with open-vocabulary semantics, directly from unposed multi-view images. Our approach leverages a Cross-View Transformer to robustly integrate information across arbitrary multi-view inputs, which then regresses a set of 3D Gaussian primitives endowed with semantic feature fields. This unified representation facilitates high-fidelity novel view synthesis, open-vocabulary 3D semantic segmentation, and depth prediction, all within a single, feed-forward pass. Extensive experiments demonstrate that Uni3R establishes a new state-of-the-art across multiple benchmarks, including 25.07 PSNR on RE10K and 55.84 mIoU on ScanNet. Our work signifies a novel paradigm towards generalizable, unified 3D scene reconstruction and understanding. The code is available at https://github.com/HorizonRobotics/Uni3R.

</details>


### [43] [LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences](https://arxiv.org/abs/2508.03692)
*Ao Liang,Youquan Liu,Yu Yang,Dongyue Lu,Linfeng Li,Lingdong Kong,Huaici Zhao,Wei Tsang Ooi*

Main category: cs.CV

TL;DR: LiDARCrafter是一个统一的4D LiDAR生成和编辑框架，通过自然语言输入生成动态场景，并在nuScenes数据集上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成世界模型多关注视频或占用网格，忽略了LiDAR特性，动态4D LiDAR建模在可控性、时间一致性和评估标准化方面存在挑战。

Method: LiDARCrafter将自然语言指令解析为场景图，通过三分支扩散网络生成对象结构、运动轨迹和几何形状，并利用自回归模块生成时间一致的4D LiDAR序列。

Result: 在nuScenes数据集上的实验表明，LiDARCrafter在保真度、可控性和时间一致性方面达到最佳性能。

Conclusion: LiDARCrafter为数据增强和仿真提供了新途径，代码和基准已开源。

Abstract: Generative world models have become essential data engines for autonomous driving, yet most existing efforts focus on videos or occupancy grids, overlooking the unique LiDAR properties. Extending LiDAR generation to dynamic 4D world modeling presents challenges in controllability, temporal coherence, and evaluation standardization. To this end, we present LiDARCrafter, a unified framework for 4D LiDAR generation and editing. Given free-form natural language inputs, we parse instructions into ego-centric scene graphs, which condition a tri-branch diffusion network to generate object structures, motion trajectories, and geometry. These structured conditions enable diverse and fine-grained scene editing. Additionally, an autoregressive module generates temporally coherent 4D LiDAR sequences with smooth transitions. To support standardized evaluation, we establish a comprehensive benchmark with diverse metrics spanning scene-, object-, and sequence-level aspects. Experiments on the nuScenes dataset using this benchmark demonstrate that LiDARCrafter achieves state-of-the-art performance in fidelity, controllability, and temporal consistency across all levels, paving the way for data augmentation and simulation. The code and benchmark are released to the community.

</details>


### [44] [LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation](https://arxiv.org/abs/2508.03694)
*Jianxiong Gao,Zhaoxi Chen,Xian Liu,Jianfeng Feng,Chenyang Si,Yanwei Fu,Yu Qiao,Ziwei Liu*

Main category: cs.CV

TL;DR: LongVie是一个端到端自回归框架，用于可控长视频生成，解决了现有方法在长视频中的时间不一致性和视觉退化问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在短片段生成中有效，但在长视频中面临时间不一致性和视觉退化等挑战。

Method: 提出LongVie框架，采用统一噪声初始化、全局控制信号归一化、多模态控制框架和退化感知训练策略。

Result: LongVie在长范围可控性、一致性和质量方面达到最先进性能。

Conclusion: LongVie通过创新设计解决了长视频生成的挑战，并展示了卓越的性能。

Abstract: Controllable ultra-long video generation is a fundamental yet challenging task. Although existing methods are effective for short clips, they struggle to scale due to issues such as temporal inconsistency and visual degradation. In this paper, we initially investigate and identify three key factors: separate noise initialization, independent control signal normalization, and the limitations of single-modality guidance. To address these issues, we propose LongVie, an end-to-end autoregressive framework for controllable long video generation. LongVie introduces two core designs to ensure temporal consistency: 1) a unified noise initialization strategy that maintains consistent generation across clips, and 2) global control signal normalization that enforces alignment in the control space throughout the entire video. To mitigate visual degradation, LongVie employs 3) a multi-modal control framework that integrates both dense (e.g., depth maps) and sparse (e.g., keypoints) control signals, complemented by 4) a degradation-aware training strategy that adaptively balances modality contributions over time to preserve visual quality. We also introduce LongVGenBench, a comprehensive benchmark consisting of 100 high-resolution videos spanning diverse real-world and synthetic environments, each lasting over one minute. Extensive experiments show that LongVie achieves state-of-the-art performance in long-range controllability, consistency, and quality.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [45] [BadBlocks: Low-Cost and Stealthy Backdoor Attacks Tailored for Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.03221)
*Yu Pan,Jiahao Chen,Lin Wang,Bingrong Dai,Yi Du*

Main category: cs.CR

TL;DR: 论文提出了一种新型轻量级后门攻击方法BadBlocks，能在扩散模型中高效注入后门并绕过现有防御框架。


<details>
  <summary>Details</summary>
Motivation: 扩散模型易受后门攻击，现有防御方法已能应对多数攻击，但缺乏对更轻量、隐蔽攻击的研究。

Method: 通过选择性污染UNet架构的特定模块，BadBlocks以低计算资源（30%计算量，20%GPU时间）实现后门注入。

Result: 实验显示BadBlocks攻击成功率高（ASR），感知质量损失低（FID评分），并能绕过先进防御框架。

Conclusion: BadBlocks显著降低了后门攻击门槛，攻击者可用消费级GPU对大规模扩散模型注入后门。

Abstract: In recent years,Diffusion models have achieved remarkable progress in the field of image generation.However,recent studies have shown that diffusion models are susceptible to backdoor attacks,in which attackers can manipulate the output by injecting covert triggers such as specific visual patterns or textual phrases into the training dataset.Fortunately,with the continuous advancement of defense techniques,defenders have become increasingly capable of identifying and mitigating most backdoor attacks using visual inspection and neural network-based detection methods.However,in this paper,we identify a novel type of backdoor threat that is more lightweight and covert than existing approaches,which we name BadBlocks,requires only about 30\% of the computational resources and 20\% GPU time typically needed by previous backdoor attacks,yet it successfully injects backdoors and evades the most advanced defense frameworks.BadBlocks enables attackers to selectively contaminate specific blocks within the UNet architecture of diffusion models while maintaining normal functionality in the remaining components.Experimental results demonstrate that BadBlocks achieves a high attack success rate (ASR) and low perceptual quality loss (as measured by FID Score),even under extremely constrained computational resources and GPU time.Moreover,BadBlocks is able to bypass existing defense frameworks,especially the attention-based backdoor detection method, highlighting it as a novel and noteworthy threat.Ablation studies further demonstrate that effective backdoor injection does not require fine-tuning the entire network and highlight the pivotal role of certain neural network layers in backdoor mapping.Overall,BadBlocks significantly reduces the barrier to conducting backdoor attacks in all aspects.It enables attackers to inject backdoors into large-scale diffusion models even using consumer-grade GPUs.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [46] [Investigation on deep learning-based galaxy image translation models](https://arxiv.org/abs/2508.03291)
*Hengxin Ruan,Qiufan Lin,Shupei Chen,Yang Wang,Wei Zhang*

Main category: astro-ph.IM

TL;DR: 研究了生成模型在星系图像翻译中保留高阶物理信息（如红移）的能力，发现现有模型在保留红移信息方面存在不足，但仍对某些应用有价值。


<details>
  <summary>Details</summary>
Motivation: 星系图像翻译在星系物理和宇宙学中很重要，但现有研究多关注像素和形态统计，缺乏对高阶物理信息保留的讨论。

Method: 测试了四种生成模型（Swin Transformer、SRGAN、胶囊网络、扩散模型）在SDSS和CFHTLS星系图像上的表现。

Result: 模型在保留红移信息方面表现不佳，但跨波段峰值通量包含有用信息，翻译图像仍可用于某些下游任务。

Conclusion: 研究为星系图像中复杂物理信息的体现提供了启示，并指导科学用途的图像翻译模型开发。

Abstract: Galaxy image translation is an important application in galaxy physics and cosmology. With deep learning-based generative models, image translation has been performed for image generation, data quality enhancement, information extraction, and generalized for other tasks such as deblending and anomaly detection. However, most endeavors on image translation primarily focus on the pixel-level and morphology-level statistics of galaxy images. There is a lack of discussion on the preservation of complex high-order galaxy physical information, which would be more challenging but crucial for studies that rely on high-fidelity image translation. Therefore, we investigated the effectiveness of generative models in preserving high-order physical information (represented by spectroscopic redshift) along with pixel-level and morphology-level information. We tested four representative models, i.e. a Swin Transformer, an SRGAN, a capsule network, and a diffusion model, using the SDSS and CFHTLS galaxy images. We found that these models show different levels of incapabilities in retaining redshift information, even if the global structures of galaxies and morphology-level statistics can be roughly reproduced. In particular, the cross-band peak fluxes of galaxies were found to contain meaningful redshift information, whereas they are subject to noticeable uncertainties in the translation of images, which may substantially be due to the nature of many-to-many mapping. Nonetheless, imperfect translated images may still contain a considerable amount of information and thus hold promise for downstream applications for which high image fidelity is not strongly required. Our work can facilitate further research on how complex physical information is manifested on galaxy images, and it provides implications on the development of image translation models for scientific use.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [47] [Nexus-INR: Diverse Knowledge-guided Arbitrary-Scale Multimodal Medical Image Super-Resolution](https://arxiv.org/abs/2508.03073)
*Bo Zhang,JianFei Huo,Zheng Zhang,Wufan Wang,Hui Gao,Xiangyang Gong,Wendong Wang*

Main category: eess.IV

TL;DR: Nexus-INR是一种基于多样化知识引导的任意分辨率超分辨率框架，通过双分支编码器、知识蒸馏模块和集成分割模块，提升了医学图像超分辨率和下游分割任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统CNN方法无法适应任意分辨率超分辨率（ARSR），而基于INR的方法虽解决了固定上采样问题，但在处理多模态图像时仍有局限。

Method: Nexus-INR包含双分支编码器（分离解剖结构和模态特征）、知识蒸馏模块（跨模态注意力引导重建）和集成分割模块（嵌入解剖语义）。

Result: 在BraTS2020数据集上，Nexus-INR在超分辨率和分割任务中均优于现有方法。

Conclusion: Nexus-INR通过多样化知识引导，实现了高质量的医学图像超分辨率和下游任务性能提升。

Abstract: Arbitrary-resolution super-resolution (ARSR) provides crucial flexibility for medical image analysis by adapting to diverse spatial resolutions. However, traditional CNN-based methods are inherently ill-suited for ARSR, as they are typically designed for fixed upsampling factors. While INR-based methods overcome this limitation, they still struggle to effectively process and leverage multi-modal images with varying resolutions and details. In this paper, we propose Nexus-INR, a Diverse Knowledge-guided ARSR framework, which employs varied information and downstream tasks to achieve high-quality, adaptive-resolution medical image super-resolution. Specifically, Nexus-INR contains three key components. A dual-branch encoder with an auxiliary classification task to effectively disentangle shared anatomical structures and modality-specific features; a knowledge distillation module using cross-modal attention that guides low-resolution modality reconstruction with high-resolution reference, enhanced by self-supervised consistency loss; an integrated segmentation module that embeds anatomical semantics to improve both reconstruction quality and downstream segmentation performance. Experiments on the BraTS2020 dataset for both super-resolution and downstream segmentation demonstrate that Nexus-INR outperforms state-of-the-art methods across various metrics.

</details>


### [48] [CADD: Context aware disease deviations via restoration of brain images using normative conditional diffusion models](https://arxiv.org/abs/2508.03594)
*Ana Lawry Aguila,Ayodeji Ijishakin,Juan Eugenio Iglesias,Tomomi Takenaga,Yukihiro Nomura,Takeharu Yoshikawa,Osamu Abe,Shouhei Hanaoka*

Main category: eess.IV

TL;DR: CADD是一种基于条件扩散模型的3D图像规范建模方法，通过结合临床信息和创新的修复策略，显著提升了异质性队列中神经异常的检测性能。


<details>
  <summary>Details</summary>
Motivation: 利用机器学习分析真实世界医学数据（如医院档案）有望革新脑部图像的疾病检测，但异质性队列中的病理检测具有挑战性。规范建模和扩散模型为这一问题提供了解决方案，但现有方法缺乏临床信息整合且修复效果不佳。

Method: 提出CADD，首个用于3D图像规范建模的条件扩散模型，采用创新的推理修复策略，平衡异常去除与保留个体特征。

Result: 在三个具有挑战性的数据集（包括低对比度、厚切片和运动伪影的临床扫描）上，CADD实现了神经异常检测的最先进性能。

Conclusion: CADD通过结合临床信息和优化修复策略，显著提升了异质性队列中的异常检测能力，为医学图像分析提供了新方向。

Abstract: Applying machine learning to real-world medical data, e.g. from hospital archives, has the potential to revolutionize disease detection in brain images. However, detecting pathology in such heterogeneous cohorts is a difficult challenge. Normative modeling, a form of unsupervised anomaly detection, offers a promising approach to studying such cohorts where the ``normal'' behavior is modeled and can be used at subject level to detect deviations relating to disease pathology. Diffusion models have emerged as powerful tools for anomaly detection due to their ability to capture complex data distributions and generate high-quality images. Their performance relies on image restoration; differences between the original and restored images highlight potential abnormalities. However, unlike normative models, these diffusion model approaches do not incorporate clinical information which provides important context to guide the disease detection process. Furthermore, standard approaches often poorly restore healthy regions, resulting in poor reconstructions and suboptimal detection performance. We present CADD, the first conditional diffusion model for normative modeling in 3D images. To guide the healthy restoration process, we propose a novel inference inpainting strategy which balances anomaly removal with retention of subject-specific features. Evaluated on three challenging datasets, including clinical scans, which may have lower contrast, thicker slices, and motion artifacts, CADD achieves state-of-the-art performance in detecting neurological abnormalities in heterogeneous cohorts.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [49] [DiWA: Diffusion Policy Adaptation with World Models](https://arxiv.org/abs/2508.03645)
*Akshay L Chandra,Iman Nematollahi,Chenguang Huang,Tim Welschehold,Wolfram Burgard,Abhinav Valada*

Main category: cs.RO

TL;DR: DiWA框架利用世界模型离线微调扩散策略，显著提升样本效率，减少实际交互需求。


<details>
  <summary>Details</summary>
Motivation: 扩散策略的强化学习微调存在奖励传播困难和实际交互需求高的问题。

Method: 提出DiWA框架，通过世界模型离线微调扩散策略，减少环境交互依赖。

Result: 在CALVIN基准测试中，DiWA仅需少量离线交互即提升八项任务性能。

Conclusion: DiWA首次实现基于离线世界模型的扩散策略微调，为机器人技能学习提供高效安全方案。

Abstract: Fine-tuning diffusion policies with reinforcement learning (RL) presents significant challenges. The long denoising sequence for each action prediction impedes effective reward propagation. Moreover, standard RL methods require millions of real-world interactions, posing a major bottleneck for practical fine-tuning. Although prior work frames the denoising process in diffusion policies as a Markov Decision Process to enable RL-based updates, its strong dependence on environment interaction remains highly inefficient. To bridge this gap, we introduce DiWA, a novel framework that leverages a world model for fine-tuning diffusion-based robotic skills entirely offline with reinforcement learning. Unlike model-free approaches that require millions of environment interactions to fine-tune a repertoire of robot skills, DiWA achieves effective adaptation using a world model trained once on a few hundred thousand offline play interactions. This results in dramatically improved sample efficiency, making the approach significantly more practical and safer for real-world robot learning. On the challenging CALVIN benchmark, DiWA improves performance across eight tasks using only offline adaptation, while requiring orders of magnitude fewer physical interactions than model-free baselines. To our knowledge, this is the first demonstration of fine-tuning diffusion policies for real-world robotic skills using an offline world model. We make the code publicly available at https://diwa.cs.uni-freiburg.de.

</details>
