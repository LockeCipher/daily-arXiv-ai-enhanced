<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 28]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [LighthouseGS: Indoor Structure-aware 3D Gaussian Splatting for Panorama-Style Mobile Captures](https://arxiv.org/abs/2507.06109)
*Seungoh Han,Jaehoon Jang,Hyunsu Kim,Jaeheung Surh,Junhyung Kwak,Hyowon Ha,Kyungdon Joo*

Main category: cs.GR

TL;DR: LighthouseGS提出了一种基于3D高斯泼溅的实时新视角合成框架，适用于手持设备拍摄的室内场景，解决了窄基线和旋转主导运动带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法需要高质量图像覆盖整个场景，限制了普通用户的可用性。LighthouseGS旨在通过简单的全景式手持拍摄实现高保真渲染。

Method: 利用粗略几何先验（如相机位姿和单目深度估计），提出平面支架组装初始化方法和稳定剪枝策略，并引入几何和光度校正。

Result: 在真实和合成室内场景测试中，LighthouseGS实现了优于现有方法的光照真实渲染。

Conclusion: LighthouseGS展示了全景视图合成和物体放置的潜力，为手持设备拍摄提供了实用解决方案。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time novel view synthesis (NVS) with impressive quality in indoor scenes. However, achieving high-fidelity rendering requires meticulously captured images covering the entire scene, limiting accessibility for general users. We aim to develop a practical 3DGS-based NVS framework using simple panorama-style motion with a handheld camera (e.g., mobile device). While convenient, this rotation-dominant motion and narrow baseline make accurate camera pose and 3D point estimation challenging, especially in textureless indoor scenes. To address these challenges, we propose LighthouseGS, a novel framework inspired by the lighthouse-like sweeping motion of panoramic views. LighthouseGS leverages rough geometric priors, such as mobile device camera poses and monocular depth estimation, and utilizes the planar structures often found in indoor environments. We present a new initialization method called plane scaffold assembly to generate consistent 3D points on these structures, followed by a stable pruning strategy to enhance geometry and optimization stability. Additionally, we introduce geometric and photometric corrections to resolve inconsistencies from motion drift and auto-exposure in mobile devices. Tested on collected real and synthetic indoor scenes, LighthouseGS delivers photorealistic rendering, surpassing state-of-the-art methods and demonstrating the potential for panoramic view synthesis and object placement.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [Structured Captions Improve Prompt Adherence in Text-to-Image Models (Re-LAION-Caption 19M)](https://arxiv.org/abs/2507.05300)
*Nicholas Merchant,Haitz Sáez de Ocáriz Borde,Andrei Cristian Popescu,Carlos Garcia Jurado Suarez*

Main category: cs.CV

TL;DR: 论文提出通过结构化标注提升生成模型的提示遵循能力，并发布了一个高质量数据集Re-LAION-Caption 19M。


<details>
  <summary>Details</summary>
Motivation: 生成式文本到图像模型在大规模数据集（如LAION-5B）上因标注噪声和无结构性问题导致提示遵循能力差，用户需依赖提示工程。

Method: 提出在训练中强制使用结构化标注（四部分模板：主体、场景、美学、相机细节），并基于Mistral 7B Instruct生成高质量数据集Re-LAION-Caption 19M。对PixArt-Σ和Stable Diffusion 2进行微调，对比结构化与非结构化标注的效果。

Result: 结构化标注版本在视觉问答（VQA）模型中表现出更高的文本-图像对齐分数。

Conclusion: 结构化标注能显著提升生成模型的可控性和对齐能力，数据集已公开。

Abstract: We argue that generative text-to-image models often struggle with prompt adherence due to the noisy and unstructured nature of large-scale datasets like LAION-5B. This forces users to rely heavily on prompt engineering to elicit desirable outputs. In this work, we propose that enforcing a consistent caption structure during training can significantly improve model controllability and alignment. We introduce Re-LAION-Caption 19M, a high-quality subset of Re-LAION-5B, comprising 19 million 1024x1024 images with captions generated by a Mistral 7B Instruct-based LLaVA-Next model. Each caption follows a four-part template: subject, setting, aesthetics, and camera details. We fine-tune PixArt-$\Sigma$ and Stable Diffusion 2 using both structured and randomly shuffled captions, and show that structured versions consistently yield higher text-image alignment scores using visual question answering (VQA) models. The dataset is publicly available at https://huggingface.co/datasets/supermodelresearch/Re-LAION-Caption19M.

</details>


### [3] [CorrDetail: Visual Detail Enhanced Self-Correction for Face Forgery Detection](https://arxiv.org/abs/2507.05302)
*Binjia Zhou,Hengrui Lou,Lizhe Chen,Haoyuan Li,Dawei Luo,Shuai Chen,Jie Lei,Zunlei Feng,Yijun Bei*

Main category: cs.CV

TL;DR: 提出了一种名为CorrDetail的视觉细节增强自校正框架，用于可解释的人脸伪造检测，解决了现有方法在伪造细节解释和幻觉问题上的不足。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成技术的快速发展，面部深度伪造的广泛出现对安全领域构成重大挑战，亟需有效的深度伪造检测方法。现有方法在伪造细节解释和幻觉问题上存在不足。

Method: 引入CorrDetail框架，通过错误引导的问题校正真实的伪造细节，并结合视觉细粒度细节增强模块提供更精确的视觉伪造细节。采用融合决策策略增强模型对极端样本的判别能力。

Result: 实验表明，CorrDetail在性能上优于最新方法，能准确识别伪造细节，并表现出强大的泛化能力。

Conclusion: CorrDetail为解决深度伪造检测中的解释性和幻觉问题提供了有效方案，具有显著的性能优势。

Abstract: With the swift progression of image generation technology, the widespread emergence of facial deepfakes poses significant challenges to the field of security, thus amplifying the urgent need for effective deepfake detection.Existing techniques for face forgery detection can broadly be categorized into two primary groups: visual-based methods and multimodal approaches. The former often lacks clear explanations for forgery details, while the latter, which merges visual and linguistic modalities, is more prone to the issue of hallucinations.To address these shortcomings, we introduce a visual detail enhanced self-correction framework, designated CorrDetail, for interpretable face forgery detection. CorrDetail is meticulously designed to rectify authentic forgery details when provided with error-guided questioning, with the aim of fostering the ability to uncover forgery details rather than yielding hallucinated responses. Additionally, to bolster the reliability of its findings, a visual fine-grained detail enhancement module is incorporated, supplying CorrDetail with more precise visual forgery details. Ultimately, a fusion decision strategy is devised to further augment the model's discriminative capacity in handling extreme samples, through the integration of visual information compensation and model bias reduction.Experimental results demonstrate that CorrDetail not only achieves state-of-the-art performance compared to the latest methodologies but also excels in accurately identifying forged details, all while exhibiting robust generalization capabilities.

</details>


### [4] [Enhancing Underwater Images Using Deep Learning with Subjective Image Quality Integration](https://arxiv.org/abs/2507.05393)
*Jose M. Montero,Jose-Luis Lisani*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的改进水下图像质量的方法，通过将人类主观评估融入训练过程，结合分类器和生成对抗网络（GANs）实现图像增强。


<details>
  <summary>Details</summary>
Motivation: 深度学习在图像处理领域的进展为水下图像质量提升提供了新思路，但现有方法往往忽视人类主观评估的重要性。

Method: 首先训练分类器网络区分高低质量图像，随后用GANs结合多种增强标准优化低质量图像。

Result: 模型在颜色保真度和图像清晰度等标准下显著提升了图像质量，定量和定性分析均验证了其有效性。

Conclusion: 结合人类主观评估的深度学习模型能有效提升水下图像质量，为未来研究提供了新方向。

Abstract: Recent advances in deep learning, particularly neural networks, have significantly impacted a wide range of fields, including the automatic enhancement of underwater images. This paper presents a deep learning-based approach to improving underwater image quality by integrating human subjective assessments into the training process. To this end, we utilize publicly available datasets containing underwater images labeled by experts as either high or low quality. Our method involves first training a classifier network to distinguish between high- and low-quality images. Subsequently, generative adversarial networks (GANs) are trained using various enhancement criteria to refine the low-quality images. The performance of the GAN models is evaluated using quantitative metrics such as PSNR, SSIM, and UIQM, as well as through qualitative analysis. Results demonstrate that the proposed model -- particularly when incorporating criteria such as color fidelity and image sharpness -- achieves substantial improvements in both perceived and measured image quality.

</details>


### [5] [Mastering Regional 3DGS: Locating, Initializing, and Editing with Diverse 2D Priors](https://arxiv.org/abs/2507.05426)
*Lanqing Guo,Yufei Wang,Hezhen Hu,Yan Zheng,Yeying Jin,Siyu Huang,Zhangyang Wang*

Main category: cs.CV

TL;DR: 提出了一种基于2D扩散编辑和逆渲染的3D场景局部编辑方法，解决了3D语义解析性能不足的问题，实现了高效且一致的编辑。


<details>
  <summary>Details</summary>
Motivation: 3D语义解析性能较差，限制了3D场景局部编辑的精确性和一致性。

Method: 利用2D扩散编辑识别修改区域，通过逆渲染进行3D定位，结合深度图初始化粗粒度3D高斯分布，迭代优化细节和纹理。

Result: 实验表明，该方法在性能上达到最优，且速度提升高达4倍。

Conclusion: 该方法为3D场景局部编辑提供了更高效和一致的解决方案。

Abstract: Many 3D scene editing tasks focus on modifying local regions rather than the entire scene, except for some global applications like style transfer, and in the context of 3D Gaussian Splatting (3DGS), where scenes are represented by a series of Gaussians, this structure allows for precise regional edits, offering enhanced control over specific areas of the scene; however, the challenge lies in the fact that 3D semantic parsing often underperforms compared to its 2D counterpart, making targeted manipulations within 3D spaces more difficult and limiting the fidelity of edits, which we address by leveraging 2D diffusion editing to accurately identify modification regions in each view, followed by inverse rendering for 3D localization, then refining the frontal view and initializing a coarse 3DGS with consistent views and approximate shapes derived from depth maps predicted by a 2D foundation model, thereby supporting an iterative, view-consistent editing process that gradually enhances structural details and textures to ensure coherence across perspectives. Experiments demonstrate that our method achieves state-of-the-art performance while delivering up to a $4\times$ speedup, providing a more efficient and effective approach to 3D scene local editing.

</details>


### [6] [Cloud Diffusion Part 1: Theory and Motivation](https://arxiv.org/abs/2507.05496)
*Andrew Randono*

Main category: cs.CV

TL;DR: 论文提出了一种基于尺度不变噪声的扩散模型（Cloud Diffusion Model），用于替代传统的白噪声扩散模型，以提高推理速度、高频细节和可控性。


<details>
  <summary>Details</summary>
Motivation: 自然图像的统计特性具有尺度不变性，而传统扩散模型使用的白噪声与这种特性不符。通过引入尺度不变噪声，可以更好地匹配自然图像的统计特性。

Method: 提出Cloud Diffusion Model，用尺度不变噪声替代白噪声，并在后续论文中构建和训练该模型。

Result: 预计新模型能实现更快的推理速度、更好的高频细节和更高的可控性。

Conclusion: Cloud Diffusion Model有望改进传统扩散模型的性能，后续研究将验证其效果。

Abstract: Diffusion models for image generation function by progressively adding noise to an image set and training a model to separate out the signal from the noise. The noise profile used by these models is white noise -- that is, noise based on independent normal distributions at each point whose mean and variance is independent of the scale. By contrast, most natural image sets exhibit a type of scale invariance in their low-order statistical properties characterized by a power-law scaling. Consequently, natural images are closer (in a quantifiable sense) to a different probability distribution that emphasizes large scale correlations and de-emphasizes small scale correlations. These scale invariant noise profiles can be incorporated into diffusion models in place of white noise to form what we will call a ``Cloud Diffusion Model". We argue that these models can lead to faster inference, improved high-frequency details, and greater controllability. In a follow-up paper, we will build and train a Cloud Diffusion Model that uses scale invariance at a fundamental level and compare it to classic, white noise diffusion models.

</details>


### [7] [LoomNet: Enhancing Multi-View Image Generation via Latent Space Weaving](https://arxiv.org/abs/2507.05499)
*Giulio Federico,Fabio Carrara,Claudio Gennaro,Giuseppe Amato,Marco Di Benedetto*

Main category: cs.CV

TL;DR: LoomNet是一种新型多视图扩散架构，通过并行应用同一扩散模型生成一致的多视图图像，显著提升3D网格质量。


<details>
  <summary>Details</summary>
Motivation: 解决单图像生成多视图图像时空间一致性不足的问题，以提升3D表面重建质量。

Method: 采用多视图扩散架构，通过共享潜在空间实现视图一致性；各视图生成假设并投影到正交平面，融合后处理为统一解释。

Result: LoomNet在15秒内生成16个高质量一致视图，图像质量和重建指标优于现有方法，并能生成多样化的新视图。

Conclusion: LoomNet通过共享潜在空间和多视图协作，显著提升了多视图图像生成的一致性和质量。

Abstract: Generating consistent multi-view images from a single image remains challenging. Lack of spatial consistency often degrades 3D mesh quality in surface reconstruction. To address this, we propose LoomNet, a novel multi-view diffusion architecture that produces coherent images by applying the same diffusion model multiple times in parallel to collaboratively build and leverage a shared latent space for view consistency. Each viewpoint-specific inference generates an encoding representing its own hypothesis of the novel view from a given camera pose, which is projected onto three orthogonal planes. For each plane, encodings from all views are fused into a single aggregated plane. These aggregated planes are then processed to propagate information and interpolate missing regions, combining the hypotheses into a unified, coherent interpretation. The final latent space is then used to render consistent multi-view images. LoomNet generates 16 high-quality and coherent views in just 15 seconds. In our experiments, LoomNet outperforms state-of-the-art methods on both image quality and reconstruction metrics, also showing creativity by producing diverse, plausible novel views from the same input.

</details>


### [8] [Simulating Refractive Distortions and Weather-Induced Artifacts for Resource-Constrained Autonomous Perception](https://arxiv.org/abs/2507.05536)
*Moseli Mots'oehli,Feimei Chen,Hok Wai Chan,Itumeleng Tlali,Thulani Babeli,Kyungim Baek,Huaijin Chen*

Main category: cs.CV

TL;DR: 提出了一种针对非洲复杂驾驶场景的低成本单目行车记录仪数据的增强方法，包括折射和天气模块，并提供了基准测试结果和工具包。


<details>
  <summary>Details</summary>
Motivation: 解决发展中国家（尤其是非洲）自动驾驶数据集稀缺的问题，以提升低资源环境下的感知能力。

Method: 开发了一个数据增强流水线，包括折射模块（模拟低质量镜头和空气湍流效应）和天气模块（添加雾和镜头光晕）。

Result: 提供了三种图像恢复模型的基准性能，并发布了增强数据集和工具包。

Conclusion: 该方法为非洲等资源匮乏地区的感知研究提供了低成本解决方案。

Abstract: The scarcity of autonomous vehicle datasets from developing regions, particularly across Africa's diverse urban, rural, and unpaved roads, remains a key obstacle to robust perception in low-resource settings. We present a procedural augmentation pipeline that enhances low-cost monocular dashcam footage with realistic refractive distortions and weather-induced artifacts tailored to challenging African driving scenarios. Our refractive module simulates optical effects from low-quality lenses and air turbulence, including lens distortion, Perlin noise, Thin-Plate Spline (TPS), and divergence-free (incompressible) warps. The weather module adds homogeneous fog, heterogeneous fog, and lens flare. To establish a benchmark, we provide baseline performance using three image restoration models. To support perception research in underrepresented African contexts, without costly data collection, labeling, or simulation, we release our distortion toolkit, augmented dataset splits, and benchmark results.

</details>


### [9] [Semi-Supervised Defect Detection via Conditional Diffusion and CLIP-Guided Noise Filtering](https://arxiv.org/abs/2507.05588)
*Shuai Li,Shihan Chen,Wanru Geng,Zhaohua Xu,Xiaolu Liu,Can Dong,Zhen Tian,Changlin Chen*

Main category: cs.CV

TL;DR: 论文提出了一种基于条件扩散的半监督缺陷检测框架（DSYM），通过两阶段协作训练和分阶段联合优化策略，显著提高了数据效率。


<details>
  <summary>Details</summary>
Motivation: 工业质量检测中，传统缺陷检测方法效率低、成本高且鲁棒性差，亟需高精度、低依赖标注的解决方案。

Method: 采用条件扩散模型生成多尺度伪缺陷样本，结合CLIP跨模态特征噪声过滤机制，利用标记数据初始训练后引入未标记数据。

Result: 在NEU-DET数据集上，使用与传统监督方法相同标记数据量时mAP@0.5达78.4%，仅需40%标记数据时达75.1%。

Conclusion: DSYM框架为工业质量检测提供了高精度、低标注依赖的缺陷检测方案，已开源。

Abstract: In the realm of industrial quality inspection, defect detection stands as a critical component, particularly in high-precision, safety-critical sectors such as automotive components aerospace, and medical devices. Traditional methods, reliant on manual inspection or early image processing algorithms, suffer from inefficiencies, high costs, and limited robustness. This paper introduces a semi-supervised defect detection framework based on conditional diffusion (DSYM), leveraging a two-stage collaborative training mechanism and a staged joint optimization strategy. The framework utilizes labeled data for initial training and subsequently incorporates unlabeled data through the generation of pseudo-labels. A conditional diffusion model synthesizes multi-scale pseudo-defect samples, while a CLIP cross-modal feature-based noise filtering mechanism mitigates label contamination. Experimental results on the NEU-DET dataset demonstrate a 78.4% mAP@0.5 with the same amount of labeled data as traditional supervised methods, and 75.1% mAP@0.5 with only 40% of the labeled data required by the original supervised model, showcasing significant advantages in data efficiency. This research provides a high-precision, low-labeling-dependent solution for defect detection in industrial quality inspection scenarios. The work of this article has been open-sourced at https://github.com/cLin-c/Semisupervised-DSYM.

</details>


### [10] [Rethinking Layered Graphic Design Generation with a Top-Down Approach](https://arxiv.org/abs/2507.05601)
*Jingye Chen,Zhaowen Wang,Nanxuan Zhao,Li Zhang,Difan Liu,Jimei Yang,Qifeng Chen*

Main category: cs.CV

TL;DR: Accordion是一个将AI生成的图形设计转换为可编辑分层设计的框架，同时通过用户提示优化无意义的AI生成文本。


<details>
  <summary>Details</summary>
Motivation: 非分层设计虽缺乏可编辑性，但仍能启发设计师的布局和文本风格选择。

Method: 基于视觉语言模型（VLM）的三阶段框架，结合SAM和元素移除模型，以全局参考图像指导分层分解。

Result: 在DesignIntention基准测试中表现优异，支持文本到模板、添加文本到背景等任务。

Conclusion: Accordion在生成可编辑分层设计和设计变体方面效果显著。

Abstract: Graphic design is crucial for conveying ideas and messages. Designers usually organize their work into objects, backgrounds, and vectorized text layers to simplify editing. However, this workflow demands considerable expertise. With the rise of GenAI methods, an endless supply of high-quality graphic designs in pixel format has become more accessible, though these designs often lack editability. Despite this, non-layered designs still inspire human designers, influencing their choices in layouts and text styles, ultimately guiding the creation of layered designs. Motivated by this observation, we propose Accordion, a graphic design generation framework taking the first attempt to convert AI-generated designs into editable layered designs, meanwhile refining nonsensical AI-generated text with meaningful alternatives guided by user prompts. It is built around a vision language model (VLM) playing distinct roles in three curated stages. For each stage, we design prompts to guide the VLM in executing different tasks. Distinct from existing bottom-up methods (e.g., COLE and Open-COLE) that gradually generate elements to create layered designs, our approach works in a top-down manner by using the visually harmonious reference image as global guidance to decompose each layer. Additionally, it leverages multiple vision experts such as SAM and element removal models to facilitate the creation of graphic layers. We train our method using the in-house graphic design dataset Design39K, augmented with AI-generated design images coupled with refined ground truth created by a customized inpainting model. Experimental results and user studies by designers show that Accordion generates favorable results on the DesignIntention benchmark, including tasks such as text-to-template, adding text to background, and text de-rendering, and also excels in creating design variations.

</details>


### [11] [Kernel Density Steering: Inference-Time Scaling via Mode Seeking for Image Restoration](https://arxiv.org/abs/2507.05604)
*Yuyang Hu,Kangfu Mei,Mojtaba Sahraee-Ardakan,Ulugbek S. Kamilov,Peyman Milanfar,Mauricio Delbracio*

Main category: cs.CV

TL;DR: 提出Kernel Density Steering（KDS）框架，通过局部模式搜索提升扩散模型在图像修复中的鲁棒性和保真度。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在图像修复中存在保真度不一致和伪影问题，需改进。

Method: 采用N粒子集合，通过核密度估计梯度引导样本向高密度区域移动，避免伪影。

Result: KDS显著提升了超分辨率和图像修复任务的定量和定性表现。

Conclusion: KDS是一种无需重新训练或外部验证的即插即用框架，可有效提升扩散模型性能。

Abstract: Diffusion models show promise for image restoration, but existing methods often struggle with inconsistent fidelity and undesirable artifacts. To address this, we introduce Kernel Density Steering (KDS), a novel inference-time framework promoting robust, high-fidelity outputs through explicit local mode-seeking. KDS employs an $N$-particle ensemble of diffusion samples, computing patch-wise kernel density estimation gradients from their collective outputs. These gradients steer patches in each particle towards shared, higher-density regions identified within the ensemble. This collective local mode-seeking mechanism, acting as "collective wisdom", steers samples away from spurious modes prone to artifacts, arising from independent sampling or model imperfections, and towards more robust, high-fidelity structures. This allows us to obtain better quality samples at the expense of higher compute by simultaneously sampling multiple particles. As a plug-and-play framework, KDS requires no retraining or external verifiers, seamlessly integrating with various diffusion samplers. Extensive numerical validations demonstrate KDS substantially improves both quantitative and qualitative performance on challenging real-world super-resolution and image inpainting tasks.

</details>


### [12] [AdaptaGen: Domain-Specific Image Generation through Hierarchical Semantic Optimization Framework](https://arxiv.org/abs/2507.05621)
*Suoxiang Zhang,Xiaxi Li,Hongrui Chang,Zhuoyan Hou,Guoxin Wu,Ronghua Ji*

Main category: cs.CV

TL;DR: 论文提出AdaptaGen框架，通过分层语义优化和跨模态适应机制，解决领域特定图像生成中的语义偏差和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在领域特定图像生成中，未能有效结合语义理解和视觉表示，且缺乏领域语义约束，导致生成结果出现幻觉和语义偏差。

Method: 提出AdaptaGen框架，包括矩阵提示优化、多视角理解、跨模态适应机制和两阶段标题语义转换。

Result: 实验证明AdaptaGen在40个类别上表现优异，仅需每类16张图像，显著提升图像质量、多样性和语义一致性。

Conclusion: AdaptaGen通过分层优化和跨模态适应，有效解决了领域特定图像生成的语义偏差问题，提升了生成效果。

Abstract: Domain-specific image generation aims to produce high-quality visual content for specialized fields while ensuring semantic accuracy and detail fidelity. However, existing methods exhibit two critical limitations: First, current approaches address prompt engineering and model adaptation separately, overlooking the inherent dependence between semantic understanding and visual representation in specialized domains. Second, these techniques inadequately incorporate domain-specific semantic constraints during content synthesis, resulting in generation outcomes that exhibit hallucinations and semantic deviations. To tackle these issues, we propose AdaptaGen, a hierarchical semantic optimization framework that integrates matrix-based prompt optimization with multi-perspective understanding, capturing comprehensive semantic relationships from both global and local perspectives. To mitigate hallucinations in specialized domains, we design a cross-modal adaptation mechanism, which, when combined with intelligent content synthesis, enables preserving core thematic elements while incorporating diverse details across images. Additionally, we introduce a two-phase caption semantic transformation during the generation phase. This approach maintains semantic coherence while enhancing visual diversity, ensuring the generated images adhere to domain-specific constraints. Experimental results confirm our approach's effectiveness, with our framework achieving superior performance across 40 categories from diverse datasets using only 16 images per category, demonstrating significant improvements in image quality, diversity, and semantic consistency.

</details>


### [13] [Knowledge-guided Complex Diffusion Model for PolSAR Image Classification in Contourlet Domain](https://arxiv.org/abs/2507.05666)
*Junfei Shi,Yu Cheng,Haiyan Jin,Junhuai Li,Zhaolin Xiao,Maoguo Gong,Weisi Lin*

Main category: cs.CV

TL;DR: 提出了一种基于Contourlet变换的结构知识引导的复数扩散模型，用于PolSAR图像分类，解决了传统实数扩散模型在捕捉复数相位信息和保留精细结构细节方面的不足。


<details>
  <summary>Details</summary>
Motivation: 传统实数扩散模型在PolSAR数据中难以捕捉复数相位信息且无法保留精细结构细节，因此需要一种新方法。

Method: 利用Contourlet变换分解数据为低频和高频子带，设计知识引导的复数扩散网络建模低频统计特性，并用高频结构信息指导扩散过程。

Result: 在三个真实PolSAR数据集上的实验表明，该方法在边缘细节保留和区域同质性方面优于现有方法。

Conclusion: 所提方法通过结合Contourlet变换和结构知识引导的复数扩散，显著提升了PolSAR图像分类的精度和细节保留能力。

Abstract: Diffusion models have demonstrated exceptional performance across various domains due to their ability to model and generate complicated data distributions. However, when applied to PolSAR data, traditional real-valued diffusion models face challenges in capturing complex-valued phase information.Moreover, these models often struggle to preserve fine structural details. To address these limitations, we leverage the Contourlet transform, which provides rich multiscale and multidirectional representations well-suited for PolSAR imagery. We propose a structural knowledge-guided complex diffusion model for PolSAR image classification in the Contourlet domain. Specifically, the complex Contourlet transform is first applied to decompose the data into low- and high-frequency subbands, enabling the extraction of statistical and boundary features. A knowledge-guided complex diffusion network is then designed to model the statistical properties of the low-frequency components. During the process, structural information from high-frequency coefficients is utilized to guide the diffusion process, improving edge preservation. Furthermore, multiscale and multidirectional high-frequency features are jointly learned to further boost classification accuracy. Experimental results on three real-world PolSAR datasets demonstrate that our approach surpasses state-of-the-art methods, particularly in preserving edge details and maintaining region homogeneity in complex terrain.

</details>


### [14] [Modeling and Reversing Brain Lesions Using Diffusion Models](https://arxiv.org/abs/2507.05670)
*Omar Zamzam,Haleh Akrami,Anand Joshi,Richard Leahy*

Main category: cs.CV

TL;DR: 该论文提出了一种基于扩散模型的框架，用于分析和逆转脑损伤过程，通过分割异常区域、估计并逆转组织变形，最终恢复健康脑组织的估计。


<details>
  <summary>Details</summary>
Motivation: 现有脑损伤分割方法未能区分受损和变形组织，限制了分析的准确性。

Method: 采用扩散模型框架，包括异常区域分割、组织变形逆转和核心损伤区域修复。

Result: 与传统方法相比，提高了损伤分割和表征的准确性。

Conclusion: 该框架为脑损伤分析提供了更强大的工具，适用于临床和研究。

Abstract: Brain lesions are abnormalities or injuries in brain tissue that are often detectable using magnetic resonance imaging (MRI), which reveals structural changes in the affected areas. This broad definition of brain lesions includes areas of the brain that are irreversibly damaged, as well as areas of brain tissue that are deformed as a result of lesion growth or swelling. Despite the importance of differentiating between damaged and deformed tissue, existing lesion segmentation methods overlook this distinction, labeling both of them as a single anomaly. In this work, we introduce a diffusion model-based framework for analyzing and reversing the brain lesion process. Our pipeline first segments abnormal regions in the brain, then estimates and reverses tissue deformations by restoring displaced tissue to its original position, isolating the core lesion area representing the initial damage. Finally, we inpaint the core lesion area to arrive at an estimation of the pre-lesion healthy brain. This proposed framework reverses a forward lesion growth process model that is well-established in biomechanical studies that model brain lesions. Our results demonstrate improved accuracy in lesion segmentation, characterization, and brain labeling compared to traditional methods, offering a robust tool for clinical and research applications in brain lesion analysis. Since pre-lesion healthy versions of abnormal brains are not available in any public dataset for validation of the reverse process, we simulate a forward model to synthesize multiple lesioned brain images.

</details>


### [15] [MedGen: Unlocking Medical Video Generation by Scaling Granularly-annotated Medical Videos](https://arxiv.org/abs/2507.05675)
*Rongsheng Wang,Junying Chen,Ke Ji,Zhenyang Cai,Shunian Chen,Yunjin Yang,Benyou Wang*

Main category: cs.CV

TL;DR: 介绍了首个大规模医学视频生成数据集MedVideoCap-55K及模型MedGen，解决了医学视频生成中数据不足和准确性低的问题。


<details>
  <summary>Details</summary>
Motivation: 医学视频生成在临床培训和教育中至关重要，但现有模型因缺乏高质量医学数据集而表现不佳。

Method: 构建了包含55,000个医学视频片段的MedVideoCap-55K数据集，并开发了MedGen模型。

Result: MedGen在视觉质量和医学准确性上表现优异，媲美商业系统。

Conclusion: MedVideoCap-55K和MedGen为医学视频生成研究提供了重要资源，推动了该领域的发展。

Abstract: Recent advances in video generation have shown remarkable progress in open-domain settings, yet medical video generation remains largely underexplored. Medical videos are critical for applications such as clinical training, education, and simulation, requiring not only high visual fidelity but also strict medical accuracy. However, current models often produce unrealistic or erroneous content when applied to medical prompts, largely due to the lack of large-scale, high-quality datasets tailored to the medical domain. To address this gap, we introduce MedVideoCap-55K, the first large-scale, diverse, and caption-rich dataset for medical video generation. It comprises over 55,000 curated clips spanning real-world medical scenarios, providing a strong foundation for training generalist medical video generation models. Built upon this dataset, we develop MedGen, which achieves leading performance among open-source models and rivals commercial systems across multiple benchmarks in both visual quality and medical accuracy. We hope our dataset and model can serve as a valuable resource and help catalyze further research in medical video generation. Our code and data is available at https://github.com/FreedomIntelligence/MedGen

</details>


### [16] [LiON-LoRA: Rethinking LoRA Fusion to Unify Controllable Spatial and Temporal Generation for Video Diffusion](https://arxiv.org/abs/2507.05678)
*Yisu Zhang,Chenjie Cao,Chaohui Yu,Jianke Zhu*

Main category: cs.CV

TL;DR: LiON-LoRA是一种改进的LoRA框架，通过线性可扩展性、正交性和范数一致性，实现对视频扩散模型中相机轨迹和物体运动的精确控制。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA方法在融合和扩展性上不稳定，难以同时精确控制相机轨迹和物体运动。

Method: 分析LoRA特征的正交性，强制层间范数一致性，引入可控令牌到扩散变换器中，并扩展至时间生成。

Result: LiON-LoRA在轨迹控制精度和运动强度调整上优于现有方法，泛化能力强且训练数据需求低。

Conclusion: LiON-LoRA通过创新设计解决了LoRA融合问题，实现了对视频生成的高效控制。

Abstract: Video Diffusion Models (VDMs) have demonstrated remarkable capabilities in synthesizing realistic videos by learning from large-scale data. Although vanilla Low-Rank Adaptation (LoRA) can learn specific spatial or temporal movement to driven VDMs with constrained data, achieving precise control over both camera trajectories and object motion remains challenging due to the unstable fusion and non-linear scalability. To address these issues, we propose LiON-LoRA, a novel framework that rethinks LoRA fusion through three core principles: Linear scalability, Orthogonality, and Norm consistency. First, we analyze the orthogonality of LoRA features in shallow VDM layers, enabling decoupled low-level controllability. Second, norm consistency is enforced across layers to stabilize fusion during complex camera motion combinations. Third, a controllable token is integrated into the diffusion transformer (DiT) to linearly adjust motion amplitudes for both cameras and objects with a modified self-attention mechanism to ensure decoupled control. Additionally, we extend LiON-LoRA to temporal generation by leveraging static-camera videos, unifying spatial and temporal controllability. Experiments demonstrate that LiON-LoRA outperforms state-of-the-art methods in trajectory control accuracy and motion strength adjustment, achieving superior generalization with minimal training data. Project Page: https://fuchengsu.github.io/lionlora.github.io/

</details>


### [17] [DreamArt: Generating Interactable Articulated Objects from a Single Image](https://arxiv.org/abs/2507.05763)
*Ruijie Lu,Yu Liu,Jiaxiang Tang,Junfeng Ni,Yuxiang Wang,Diwen Wan,Gang Zeng,Yixin Chen,Siyuan Huang*

Main category: cs.CV

TL;DR: DreamArt是一个从单视角图像生成高质量、可交互的铰接式物体的框架，通过三阶段流程实现：3D重建、关节先验学习和运动优化。


<details>
  <summary>Details</summary>
Motivation: 当前图像到3D的方法忽视部件分解和关节建模，而神经重建方法依赖密集多视角数据，限制了可扩展性。DreamArt旨在解决这些问题。

Method: 采用三阶段流程：1) 3D重建与部件分割；2) 基于视频扩散模型的关节先验学习；3) 关节运动优化与纹理细化。

Result: 实验表明，DreamArt能生成高质量铰接式物体，具有准确的部件形状、高保真外观和合理的关节运动。

Conclusion: DreamArt为铰接式物体生成提供了可扩展的解决方案，适用于Embodied AI和AR/VR应用。

Abstract: Generating articulated objects, such as laptops and microwaves, is a crucial yet challenging task with extensive applications in Embodied AI and AR/VR. Current image-to-3D methods primarily focus on surface geometry and texture, neglecting part decomposition and articulation modeling. Meanwhile, neural reconstruction approaches (e.g., NeRF or Gaussian Splatting) rely on dense multi-view or interaction data, limiting their scalability. In this paper, we introduce DreamArt, a novel framework for generating high-fidelity, interactable articulated assets from single-view images. DreamArt employs a three-stage pipeline: firstly, it reconstructs part-segmented and complete 3D object meshes through a combination of image-to-3D generation, mask-prompted 3D segmentation, and part amodal completion. Second, we fine-tune a video diffusion model to capture part-level articulation priors, leveraging movable part masks as prompt and amodal images to mitigate ambiguities caused by occlusion. Finally, DreamArt optimizes the articulation motion, represented by a dual quaternion, and conducts global texture refinement and repainting to ensure coherent, high-quality textures across all parts. Experimental results demonstrate that DreamArt effectively generates high-quality articulated objects, possessing accurate part shape, high appearance fidelity, and plausible articulation, thereby providing a scalable solution for articulated asset generation. Our project page is available at https://dream-art-0.github.io/DreamArt/.

</details>


### [18] [SPADE: Spatial-Aware Denoising Network for Open-vocabulary Panoptic Scene Graph Generation with Long- and Local-range Context Reasoning](https://arxiv.org/abs/2507.05798)
*Xin Hu,Ke Qin,Guiduo Duan,Ming Li,Yuan-Fang Li,Tao He*

Main category: cs.CV

TL;DR: SPADE框架通过空间感知去噪网络改进开放词汇PSG任务，结合反演引导校准和空间感知上下文推理，显著提升空间关系预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预训练视觉语言模型（VLMs），但忽视了其在空间关系推理中的固有局限，导致关系预测不理想。

Method: SPADE框架包括反演引导校准的UNet适应和空间感知上下文推理两步，利用轻量级LoRA微调和空间感知关系图变换器。

Result: 在PSG和Visual Genome数据集上，SPADE在封闭和开放场景中均优于现有方法，尤其在空间关系预测上表现突出。

Conclusion: SPADE通过空间感知设计有效解决了VLMs在空间关系推理中的不足，为开放词汇PSG任务提供了新思路。

Abstract: Panoptic Scene Graph Generation (PSG) integrates instance segmentation with relation understanding to capture pixel-level structural relationships in complex scenes. Although recent approaches leveraging pre-trained vision-language models (VLMs) have significantly improved performance in the open-vocabulary setting, they commonly ignore the inherent limitations of VLMs in spatial relation reasoning, such as difficulty in distinguishing object relative positions, which results in suboptimal relation prediction. Motivated by the denoising diffusion model's inversion process in preserving the spatial structure of input images, we propose SPADE (SPatial-Aware Denoising-nEtwork) framework -- a novel approach for open-vocabulary PSG. SPADE consists of two key steps: (1) inversion-guided calibration for the UNet adaptation, and (2) spatial-aware context reasoning. In the first step, we calibrate a general pre-trained teacher diffusion model into a PSG-specific denoising network with cross-attention maps derived during inversion through a lightweight LoRA-based fine-tuning strategy. In the second step, we develop a spatial-aware relation graph transformer that captures both local and long-range contextual information, facilitating the generation of high-quality relation queries. Extensive experiments on benchmark PSG and Visual Genome datasets demonstrate that SPADE outperforms state-of-the-art methods in both closed- and open-set scenarios, particularly for spatial relationship prediction.

</details>


### [19] [D-FCGS: Feedforward Compression of Dynamic Gaussian Splatting for Free-Viewpoint Videos](https://arxiv.org/abs/2507.05859)
*Wenkang Zhang,Yan Zhao,Qiang Wang,Li Song,Zhengxue Cheng*

Main category: cs.CV

TL;DR: 提出了一种名为D-FCGS的前馈压缩框架，用于动态高斯点云序列的高效压缩，无需逐场景优化，实现了40倍以上的压缩率。


<details>
  <summary>Details</summary>
Motivation: 自由视点视频（FVV）需要高效的动态3D表示压缩方法，但现有方法通常依赖优化编码，限制了通用性。

Method: 采用Group-of-Frames结构和I-P帧编码，通过稀疏控制点提取帧间运动，并利用双先验感知熵模型压缩运动张量。

Result: 实验表明，D-FCGS在保持视觉质量的同时，实现了40倍以上的压缩率，且处理时间少于2秒。

Conclusion: D-FCGS为动态3D高斯点云的前馈压缩提供了高效解决方案，推动了FVV的可扩展传输与存储。

Abstract: Free-viewpoint video (FVV) enables immersive 3D experiences, but efficient compression of dynamic 3D representations remains a major challenge. Recent advances in 3D Gaussian Splatting (3DGS) and its dynamic extensions have enabled high-fidelity scene modeling. However, existing methods often couple scene reconstruction with optimization-dependent coding, which limits generalizability. This paper presents Feedforward Compression of Dynamic Gaussian Splatting (D-FCGS), a novel feedforward framework for compressing temporally correlated Gaussian point cloud sequences. Our approach introduces a Group-of-Frames (GoF) structure with I-P frame coding, where inter-frame motions are extracted via sparse control points. The resulting motion tensors are compressed in a feedforward manner using a dual prior-aware entropy model that combines hyperprior and spatial-temporal priors for accurate rate estimation. For reconstruction, we perform control-point-guided motion compensation and employ a refinement network to enhance view-consistent fidelity. Trained on multi-view video-derived Gaussian frames, D-FCGS generalizes across scenes without per-scene optimization. Experiments show that it matches the rate-distortion performance of optimization-based methods, achieving over 40 times compression in under 2 seconds while preserving visual quality across viewpoints. This work advances feedforward compression for dynamic 3DGS, paving the way for scalable FVV transmission and storage in immersive applications.

</details>


### [20] [What You Have is What You Track: Adaptive and Robust Multimodal Tracking](https://arxiv.org/abs/2507.05899)
*Yuedong Tan,Jiawei Shao,Eduard Zamfir,Ruanjun Li,Zhaochong An,Chao Ma,Danda Paudel,Luc Van Gool,Radu Timofte,Zongwei Wu*

Main category: cs.CV

TL;DR: 本文研究了多模态数据在视觉跟踪中的重要性，特别是在数据不完整时现有跟踪器的性能下降问题，并提出了一种灵活的动态框架以提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态数据能提升视觉跟踪的鲁棒性，但传感器同步问题常导致数据缺失，现有跟踪器对此缺乏适应性。

Method: 提出了一种基于异构专家混合的动态计算单元激活机制，结合视频级掩码策略，确保时空一致性。

Result: 模型在9个基准测试中达到SOTA性能，适应不同缺失率和场景复杂度。

Conclusion: 该框架不仅解决了数据缺失问题，还提升了跟踪性能，代码和基准将公开。

Abstract: Multimodal data is known to be helpful for visual tracking by improving robustness to appearance variations. However, sensor synchronization challenges often compromise data availability, particularly in video settings where shortages can be temporal. Despite its importance, this area remains underexplored. In this paper, we present the first comprehensive study on tracker performance with temporally incomplete multimodal data. Unsurprisingly, under such a circumstance, existing trackers exhibit significant performance degradation, as their rigid architectures lack the adaptability needed to effectively handle missing modalities. To address these limitations, we propose a flexible framework for robust multimodal tracking. We venture that a tracker should dynamically activate computational units based on missing data rates. This is achieved through a novel Heterogeneous Mixture-of-Experts fusion mechanism with adaptive complexity, coupled with a video-level masking strategy that ensures both temporal consistency and spatial completeness which is critical for effective video tracking. Surprisingly, our model not only adapts to varying missing rates but also adjusts to scene complexity. Extensive experiments show that our model achieves SOTA performance across 9 benchmarks, excelling in both conventional complete and missing modality settings. The code and benchmark will be publicly available at https://github.com/supertyd/FlexTrack/tree/main.

</details>


### [21] [High-Fidelity and Generalizable Neural Surface Reconstruction with Sparse Feature Volumes](https://arxiv.org/abs/2507.05952)
*Aoxiang Fan,Corentin Dumery,Nicolas Talabot,Hieu Le,Pascal Fua*

Main category: cs.CV

TL;DR: 提出了一种稀疏表示方法，通过两阶段策略实现高效高分辨率神经表面重建，显著减少存储需求并提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决密集3D特征体积在高分辨率下内存效率低的问题，提升重建质量。

Method: 两阶段方法：1）预测体素占用率；2）仅在占用率高的体素中计算特征和体积渲染。开发了稀疏体积的高效采样、特征聚合和查询算法。

Result: 存储需求减少50倍以上，支持512^3分辨率重建，性能优于现有方法。

Conclusion: 稀疏表示方法显著提升了重建效率和分辨率，优于当前最先进技术。

Abstract: Generalizable neural surface reconstruction has become a compelling technique to reconstruct from few images without per-scene optimization, where dense 3D feature volume has proven effective as a global representation of scenes. However, the dense representation does not scale well to increasing voxel resolutions, severely limiting the reconstruction quality. We thus present a sparse representation method, that maximizes memory efficiency and enables significantly higher resolution reconstructions on standard hardware. We implement this through a two-stage approach: First training a network to predict voxel occupancies from posed images and associated depth maps, then computing features and performing volume rendering only in voxels with sufficiently high occupancy estimates. To support this sparse representation, we developed custom algorithms for efficient sampling, feature aggregation, and querying from sparse volumes-overcoming the dense-volume assumptions inherent in existing works. Experiments on public datasets demonstrate that our approach reduces storage requirements by more than 50 times without performance degradation, enabling reconstructions at $512^3$ resolution compared to the typical $128^3$ on similar hardware, and achieving superior reconstruction accuracy over current state-of-the-art methods.

</details>


### [22] [Tora2: Motion and Appearance Customized Diffusion Transformer for Multi-Entity Video Generation](https://arxiv.org/abs/2507.05963)
*Zhenghao Zhang,Junchao Liao,Xiangyu Meng,Long Qin,Weizhi Wang*

Main category: cs.CV

TL;DR: Tora2是Tora的增强版，通过解耦个性化提取器和门控自注意力机制，实现了多实体外观和运动的同步定制，显著提升了视频生成的性能。


<details>
  <summary>Details</summary>
Motivation: 改进现有扩散变换器模型在运动引导视频生成中的能力，特别是在多实体外观和运动定制方面的表现。

Method: 引入解耦个性化提取器生成个性化嵌入，设计门控自注意力机制整合轨迹、文本和视觉信息，并使用对比损失优化运动与个性化嵌入的映射。

Result: Tora2在多条件视频生成中表现优异，实现了多实体外观和运动的同步定制，性能优于现有方法。

Conclusion: Tora2在视频生成领域取得了重要进展，为多条件视频生成提供了更先进的运动控制能力。

Abstract: Recent advances in diffusion transformer models for motion-guided video generation, such as Tora, have shown significant progress. In this paper, we present Tora2, an enhanced version of Tora, which introduces several design improvements to expand its capabilities in both appearance and motion customization. Specifically, we introduce a decoupled personalization extractor that generates comprehensive personalization embeddings for multiple open-set entities, better preserving fine-grained visual details compared to previous methods. Building on this, we design a gated self-attention mechanism to integrate trajectory, textual description, and visual information for each entity. This innovation significantly reduces misalignment in multimodal conditioning during training. Moreover, we introduce a contrastive loss that jointly optimizes trajectory dynamics and entity consistency through explicit mapping between motion and personalization embeddings. Tora2 is, to our best knowledge, the first method to achieve simultaneous multi-entity customization of appearance and motion for video generation. Experimental results demonstrate that Tora2 achieves competitive performance with state-of-the-art customization methods while providing advanced motion control capabilities, which marks a critical advancement in multi-condition video generation. Project page: https://github.com/alibaba/Tora .

</details>


### [23] [T-LoRA: Single Image Diffusion Model Customization Without Overfitting](https://arxiv.org/abs/2507.05964)
*Vera Soboleva,Aibek Alanov,Andrey Kuznetsov,Konstantin Sobolev*

Main category: cs.CV

TL;DR: T-LoRA是一种针对扩散模型个性化设计的时序依赖低秩适应框架，通过动态调整秩约束更新和正交初始化，有效解决了单图像定制中的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在有限训练样本（尤其是单图像）下容易过拟合的问题，提升生成多样性和泛化能力。

Method: 提出T-LoRA框架，包括动态调整秩约束更新和正交初始化技术，针对不同扩散时间步进行敏感微调。

Result: T-LoRA在概念保真度和文本对齐性上优于标准LoRA及其他方法，适用于数据有限和资源受限场景。

Conclusion: T-LoRA为扩散模型个性化提供了高效解决方案，尤其在单图像定制中表现突出。

Abstract: While diffusion model fine-tuning offers a powerful approach for customizing pre-trained models to generate specific objects, it frequently suffers from overfitting when training samples are limited, compromising both generalization capability and output diversity. This paper tackles the challenging yet most impactful task of adapting a diffusion model using just a single concept image, as single-image customization holds the greatest practical potential. We introduce T-LoRA, a Timestep-Dependent Low-Rank Adaptation framework specifically designed for diffusion model personalization. In our work we show that higher diffusion timesteps are more prone to overfitting than lower ones, necessitating a timestep-sensitive fine-tuning strategy. T-LoRA incorporates two key innovations: (1) a dynamic fine-tuning strategy that adjusts rank-constrained updates based on diffusion timesteps, and (2) a weight parametrization technique that ensures independence between adapter components through orthogonal initialization. Extensive experiments show that T-LoRA and its individual components outperform standard LoRA and other diffusion model personalization techniques. They achieve a superior balance between concept fidelity and text alignment, highlighting the potential of T-LoRA in data-limited and resource-constrained scenarios. Code is available at https://github.com/ControlGenAI/T-LoRA.

</details>


### [24] [TextPixs: Glyph-Conditioned Diffusion with Character-Aware Attention and OCR-Guided Supervision](https://arxiv.org/abs/2507.06033)
*Syeda Anshrah Gillani,Mirza Samad Ahmed Baig,Osama Ahmed Khan,Shahid Munir Shah,Umema Mujeeb,Maheen Ali*

Main category: cs.CV

TL;DR: 本文提出了一种新框架GCDA，用于解决文本到图像扩散模型中无法生成可读文本的问题，通过字符感知注意力机制和OCR微调，显著提升了文本渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型无法生成可读且拼写正确的文本，限制了其在广告、学习和创意设计等实际应用中的潜力。

Method: GCDA框架包含双流文本编码器、字符感知注意力机制和OCR微调阶段，分别用于增强字符感知表示、避免失真和优化文本可读性。

Result: GCDA在MARIO-10M和T2I-CompBench数据集上取得了最佳性能，字符错误率（0.08）和词错误率（0.15）显著优于之前的方法，同时保持了高保真图像合成质量（FID: 14.3）。

Conclusion: GCDA通过字符感知设计和OCR微调，显著提升了文本到图像生成中的文本可读性，为实际应用提供了更强大的工具。

Abstract: The modern text-to-image diffusion models boom has opened a new era in digital content production as it has proven the previously unseen ability to produce photorealistic and stylistically diverse imagery based on the semantics of natural-language descriptions. However, the consistent disadvantage of these models is that they cannot generate readable, meaningful, and correctly spelled text in generated images, which significantly limits the use of practical purposes like advertising, learning, and creative design. This paper introduces a new framework, namely Glyph-Conditioned Diffusion with Character-Aware Attention (GCDA), using which a typical diffusion backbone is extended by three well-designed modules. To begin with, the model has a dual-stream text encoder that encodes both semantic contextual information and explicit glyph representations, resulting in a character-aware representation of the input text that is rich in nature. Second, an attention mechanism that is aware of the character is proposed with a new attention segregation loss that aims to limit the attention distribution of each character independently in order to avoid distortion artifacts. Lastly, GCDA has an OCR-in-the-loop fine-tuning phase, where a full text perceptual loss, directly optimises models to be legible and accurately spell. Large scale experiments to benchmark datasets, such as MARIO-10M and T2I-CompBench, reveal that GCDA sets a new state-of-the-art on all metrics, with better character based metrics on text rendering (Character Error Rate: 0.08 vs 0.21 for the previous best; Word Error Rate: 0.15 vs 0.25), human perception, and comparable image synthesis quality on high-fidelity (FID: 14.3).

</details>


### [25] [VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis](https://arxiv.org/abs/2507.06060)
*Alexandre Symeonidis-Herzig,Özge Mercanoğlu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: VisualSpeaker通过结合光真实感渲染和视觉语音识别，显著提升了3D面部动画的质量和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖网格域，无法充分利用2D计算机视觉和图形学的快速视觉创新，限制了3D面部动画的表现力。

Method: 提出VisualSpeaker，采用光真实感可微分渲染，并通过视觉语音识别监督训练，引入感知唇读损失函数。

Result: 在MEAD数据集上，Lip Vertex Error指标提升56.1%，同时保持网格驱动动画的可控性。

Conclusion: VisualSpeaker在提升动画质量的同时，支持精准口型，对消除手语动画中的歧义至关重要。

Abstract: Realistic, high-fidelity 3D facial animations are crucial for expressive avatar systems in human-computer interaction and accessibility. Although prior methods show promising quality, their reliance on the mesh domain limits their ability to fully leverage the rapid visual innovations seen in 2D computer vision and graphics. We propose VisualSpeaker, a novel method that bridges this gap using photorealistic differentiable rendering, supervised by visual speech recognition, for improved 3D facial animation. Our contribution is a perceptual lip-reading loss, derived by passing photorealistic 3D Gaussian Splatting avatar renders through a pre-trained Visual Automatic Speech Recognition model during training. Evaluation on the MEAD dataset demonstrates that VisualSpeaker improves both the standard Lip Vertex Error metric by 56.1% and the perceptual quality of the generated animations, while retaining the controllability of mesh-driven animation. This perceptual focus naturally supports accurate mouthings, essential cues that disambiguate similar manual signs in sign language avatars.

</details>


### [26] [ScoreAdv: Score-based Targeted Generation of Natural Adversarial Examples via Diffusion Models](https://arxiv.org/abs/2507.06078)
*Chihan Huang,Hao Tang*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Despite the success of deep learning across various domains, it remains vulnerable to adversarial attacks. Although many existing adversarial attack methods achieve high success rates, they typically rely on $\ell_{p}$-norm perturbation constraints, which do not align with human perceptual capabilities. Consequently, researchers have shifted their focus toward generating natural, unrestricted adversarial examples (UAEs). GAN-based approaches suffer from inherent limitations, such as poor image quality due to instability and mode collapse. Meanwhile, diffusion models have been employed for UAE generation, but they still rely on iterative PGD perturbation injection, without fully leveraging their central denoising capabilities. In this paper, we introduce a novel approach for generating UAEs based on diffusion models, named ScoreAdv. This method incorporates an interpretable adversarial guidance mechanism to gradually shift the sampling distribution towards the adversarial distribution, while using an interpretable saliency map to inject the visual information of a reference image into the generated samples. Notably, our method is capable of generating an unlimited number of natural adversarial examples and can attack not only classification models but also retrieval models. We conduct extensive experiments on ImageNet and CelebA datasets, validating the performance of ScoreAdv across ten target models in both black-box and white-box settings. Our results demonstrate that ScoreAdv achieves state-of-the-art attack success rates and image quality. Furthermore, the dynamic balance between denoising and adversarial perturbation enables ScoreAdv to remain robust even under defensive measures.

</details>


### [27] [Reflections Unlock: Geometry-Aware Reflection Disentanglement in 3D Gaussian Splatting for Photorealistic Scenes Rendering](https://arxiv.org/abs/2507.06103)
*Jiayi Song,Zihan Ye,Qingyuan Zhou,Weidong Yang,Ben Fei,Jingyi Xu,Ying He,Wanli Ouyang*

Main category: cs.CV

TL;DR: Ref-Unlock是一种基于3D高斯泼溅的几何感知反射建模框架，通过显式分离透射和反射分量，显著提升复杂反射场景的渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如NeRF和3DGS）常将反射误认为物理几何，导致重建质量下降，亟需一种能准确处理反射的通用解决方案。

Method: 采用双分支表示和高阶球谐函数捕捉高频反射细节，结合反射去除模块和几何感知平滑约束，增强几何一致性。

Result: Ref-Unlock在反射场景渲染中显著优于传统GS方法，与NeRF模型竞争，并支持灵活的反射编辑。

Conclusion: Ref-Unlock为反射场景的真实渲染提供了高效且通用的解决方案。

Abstract: Accurately rendering scenes with reflective surfaces remains a significant challenge in novel view synthesis, as existing methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) often misinterpret reflections as physical geometry, resulting in degraded reconstructions. Previous methods rely on incomplete and non-generalizable geometric constraints, leading to misalignment between the positions of Gaussian splats and the actual scene geometry. When dealing with real-world scenes containing complex geometry, the accumulation of Gaussians further exacerbates surface artifacts and results in blurred reconstructions. To address these limitations, in this work, we propose Ref-Unlock, a novel geometry-aware reflection modeling framework based on 3D Gaussian Splatting, which explicitly disentangles transmitted and reflected components to better capture complex reflections and enhance geometric consistency in real-world scenes. Our approach employs a dual-branch representation with high-order spherical harmonics to capture high-frequency reflective details, alongside a reflection removal module providing pseudo reflection-free supervision to guide clean decomposition. Additionally, we incorporate pseudo-depth maps and a geometry-aware bilateral smoothness constraint to enhance 3D geometric consistency and stability in decomposition. Extensive experiments demonstrate that Ref-Unlock significantly outperforms classical GS-based reflection methods and achieves competitive results with NeRF-based models, while enabling flexible vision foundation models (VFMs) driven reflection editing. Our method thus offers an efficient and generalizable solution for realistic rendering of reflective scenes. Our code is available at https://ref-unlock.github.io/.

</details>


### [28] [Omni-Video: Democratizing Unified Video Understanding and Generation](https://arxiv.org/abs/2507.06119)
*Zhiyu Tan,Hao Yang,Luozheng Qin,Jia Gong,Mengping Yang,Hao Li*

Main category: cs.CV

TL;DR: Omni-Video是一个统一的视频理解、生成和编辑框架，通过多模态大语言模型（MLLMs）生成视觉线索，并结合扩散解码器生成高质量视频。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型主要关注图像处理，缺乏统一的视频理解与生成模型，因此提出了Omni-Video框架。

Method: 1) 轻量级架构设计，将视觉头与MLLMs结合，适配器连接扩散解码器；2) 高效多阶段训练方案，利用有限数据和计算资源快速连接MLLMs与扩散解码器。

Result: 模型在视频生成、编辑和理解任务中表现出良好的泛化能力。

Conclusion: Omni-Video为统一视频建模提供了高效且有效的解决方案。

Abstract: Notable breakthroughs in unified understanding and generation modeling have led to remarkable advancements in image understanding, reasoning, production and editing, yet current foundational models predominantly focus on processing images, creating a gap in the development of unified models for video understanding and generation. This report presents Omni-Video, an efficient and effective unified framework for video understanding, generation, as well as instruction-based editing. Our key insight is to teach existing multimodal large language models (MLLMs) to produce continuous visual clues that are used as the input of diffusion decoders, which produce high-quality videos conditioned on these visual clues. To fully unlock the potential of our system for unified video modeling, we integrate several technical improvements: 1) a lightweight architectural design that respectively attaches a vision head on the top of MLLMs and a adapter before the input of diffusion decoders, the former produce visual tokens for the latter, which adapts these visual tokens to the conditional space of diffusion decoders; and 2) an efficient multi-stage training scheme that facilitates a fast connection between MLLMs and diffusion decoders with limited data and computational resources. We empirically demonstrate that our model exhibits satisfactory generalization abilities across video generation, editing and understanding tasks.

</details>


### [29] [Prompt-Free Conditional Diffusion for Multi-object Image Augmentation](https://arxiv.org/abs/2507.06146)
*Haoyu Wang,Lei Zhang,Wei Wei,Chen Ding,Yanning Zhang*

Main category: cs.CV

TL;DR: 提出了一种无提示条件扩散框架，通过局部-全局语义融合策略和LoRA知识注入，解决多目标图像生成中的类别偏差和多样性不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成多目标图像时，要么依赖文本条件导致偏差，要么依赖原始图像导致多样性不足。

Method: 采用局部-全局语义融合策略提取图像语义，通过LoRA注入知识，并设计基于计数的损失函数辅助训练。

Result: 实验表明，该方法优于现有基线，具有强下游任务增益和跨域泛化能力。

Conclusion: 提出的框架有效解决了多目标图像生成的偏差和多样性问题，提升了生成数据的实用性。

Abstract: Diffusion models has underpinned much recent advances of dataset augmentation in various computer vision tasks. However, when involving generating multi-object images as real scenarios, most existing methods either rely entirely on text condition, resulting in a deviation between the generated objects and the original data, or rely too much on the original images, resulting in a lack of diversity in the generated images, which is of limited help to downstream tasks. To mitigate both problems with one stone, we propose a prompt-free conditional diffusion framework for multi-object image augmentation. Specifically, we introduce a local-global semantic fusion strategy to extract semantics from images to replace text, and inject knowledge into the diffusion model through LoRA to alleviate the category deviation between the original model and the target dataset. In addition, we design a reward model based counting loss to assist the traditional reconstruction loss for model training. By constraining the object counts of each category instead of pixel-by-pixel constraints, bridging the quantity deviation between the generated data and the original data while improving the diversity of the generated data. Experimental results demonstrate the superiority of the proposed method over several representative state-of-the-art baselines and showcase strong downstream task gain and out-of-domain generalization capabilities. Code is available at \href{https://github.com/00why00/PFCD}{here}.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [30] [Feature-Based vs. GAN-Based Learning from Demonstrations: When and Why](https://arxiv.org/abs/2507.05906)
*Chenhao Li,Marco Hutter,Andreas Krause*

Main category: cs.LG

TL;DR: 本文比较了基于特征和基于GAN的演示学习方法，重点分析了奖励函数结构及其对策略学习的影响，提出了任务导向的方法选择框架。


<details>
  <summary>Details</summary>
Motivation: 探讨特征方法和GAN方法在演示学习中的优缺点，为任务需求提供方法选择的指导。

Method: 对比分析特征方法和GAN方法的奖励函数结构、训练稳定性及泛化能力。

Result: 特征方法提供密集可解释奖励但泛化能力有限，GAN方法灵活但训练不稳定。

Conclusion: 方法选择应基于任务需求（如保真度、多样性、可解释性、适应性），而非范式优劣。

Abstract: This survey provides a comparative analysis of feature-based and GAN-based approaches to learning from demonstrations, with a focus on the structure of reward functions and their implications for policy learning. Feature-based methods offer dense, interpretable rewards that excel at high-fidelity motion imitation, yet often require sophisticated representations of references and struggle with generalization in unstructured settings. GAN-based methods, in contrast, use implicit, distributional supervision that enables scalability and adaptation flexibility, but are prone to training instability and coarse reward signals. Recent advancements in both paradigms converge on the importance of structured motion representations, which enable smoother transitions, controllable synthesis, and improved task integration. We argue that the dichotomy between feature-based and GAN-based methods is increasingly nuanced: rather than one paradigm dominating the other, the choice should be guided by task-specific priorities such as fidelity, diversity, interpretability, and adaptability. This work outlines the algorithmic trade-offs and design considerations that underlie method selection, offering a framework for principled decision-making in learning from demonstrations.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [31] [NeoBabel: A Multilingual Open Tower for Visual Generation](https://arxiv.org/abs/2507.06137)
*Mohammad Mahdi Derakhshani,Dheeraj Varghese,Marzieh Fadaee,Cees G. M. Snoek*

Main category: cs.CL

TL;DR: NeoBabel是一种多语言图像生成框架，支持六种语言，通过多语言预训练和高分辨率指令调优，在性能和效率上达到新高度，同时保持强大的英语能力。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成系统以英语为中心，导致非英语用户面临障碍和数字不平等。翻译管道引入语义漂移和计算开销，NeoBabel旨在解决这些问题。

Method: 结合大规模多语言预训练和高分辨率指令调优，训练NeoBabel模型，并扩展两个多语言基准测试m-GenEval和m-DPG。

Result: NeoBabel在多语言基准测试中表现优异（m-GenEval: 0.75，m-DPG: 0.68），同时保持与英语模型的竞争力，且模型体积更小。

Conclusion: 多语言能力不是妥协，而是提升生成AI鲁棒性、效率和文化保真度的催化剂。NeoBabel为包容性AI研究提供了工具包和数据集。

Abstract: Text-to-image generation advancements have been predominantly English-centric, creating barriers for non-English speakers and perpetuating digital inequities. While existing systems rely on translation pipelines, these introduce semantic drift, computational overhead, and cultural misalignment. We introduce NeoBabel, a novel multilingual image generation framework that sets a new Pareto frontier in performance, efficiency and inclusivity, supporting six languages: English, Chinese, Dutch, French, Hindi, and Persian. The model is trained using a combination of large-scale multilingual pretraining and high-resolution instruction tuning. To evaluate its capabilities, we expand two English-only benchmarks to multilingual equivalents: m-GenEval and m-DPG. NeoBabel achieves state-of-the-art multilingual performance while retaining strong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG. Notably, it performs on par with leading models on English tasks while outperforming them by +0.11 and +0.09 on multilingual benchmarks, even though these models are built on multilingual base LLMs. This demonstrates the effectiveness of our targeted alignment training for preserving and extending crosslingual generalization. We further introduce two new metrics to rigorously assess multilingual alignment and robustness to code-mixed prompts. Notably, NeoBabel matches or exceeds English-only models while being 2-4x smaller. We release an open toolkit, including all code, model checkpoints, a curated dataset of 124M multilingual text-image pairs, and standardized multilingual evaluation protocols, to advance inclusive AI research. Our work demonstrates that multilingual capability is not a trade-off but a catalyst for improved robustness, efficiency, and cultural fidelity in generative AI.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [32] [PWD: Prior-Guided and Wavelet-Enhanced Diffusion Model for Limited-Angle CT](https://arxiv.org/abs/2507.05317)
*Yi Liu,Yiyang Wen,Zekun Zhou,Junqi Ma,Linghang Wang,Yucheng Yao,Liu Shi,Qiegen Liu*

Main category: eess.IV

TL;DR: 提出了一种基于先验信息嵌入和小波特征融合的快速采样扩散模型（PWD），用于有限角度CT重建，显著减少采样步骤并保持重建质量。


<details>
  <summary>Details</summary>
Motivation: 标准扩散模型在有限角度CT重建中计算开销大，而跳过采样策略会损失细节，需改进。

Method: PWD在训练阶段学习LACT图像与目标图像的分布对应关系，推理时利用LACT图像作为先验指导采样，并结合小波域多尺度特征融合。

Result: 在临床数据集上，PWD仅需50步采样，PSNR提升至少1.7 dB，SSIM提升10%。

Conclusion: PWD在高效采样的同时保持了重建质量，优于现有方法。

Abstract: Generative diffusion models have received increasing attention in medical imaging, particularly in limited-angle computed tomography (LACT). Standard diffusion models achieve high-quality image reconstruction but require a large number of sampling steps during inference, resulting in substantial computational overhead. Although skip-sampling strategies have been proposed to improve efficiency, they often lead to loss of fine structural details. To address this issue, we propose a prior information embedding and wavelet feature fusion fast sampling diffusion model for LACT reconstruction. The PWD enables efficient sampling while preserving reconstruction fidelity in LACT, and effectively mitigates the degradation typically introduced by skip-sampling. Specifically, during the training phase, PWD maps the distribution of LACT images to that of fully sampled target images, enabling the model to learn structural correspondences between them. During inference, the LACT image serves as an explicit prior to guide the sampling trajectory, allowing for high-quality reconstruction with significantly fewer steps. In addition, PWD performs multi-scale feature fusion in the wavelet domain, effectively enhancing the reconstruction of fine details by leveraging both low-frequency and high-frequency information. Quantitative and qualitative evaluations on clinical dental arch CBCT and periapical datasets demonstrate that PWD outperforms existing methods under the same sampling condition. Using only 50 sampling steps, PWD achieves at least 1.7 dB improvement in PSNR and 10% gain in SSIM.

</details>


### [33] [Self-supervised Deep Learning for Denoising in Ultrasound Microvascular Imaging](https://arxiv.org/abs/2507.05451)
*Lijie Huang,Jingyi Yin,Jingke Zhang,U-Wai Lok,Ryan M. DeRuiter,Jieyang Jin,Kate M. Knoll,Kendra E. Petersen,James D. Krier,Xiang-yang Zhu,Gina K. Hesley,Kathryn A. Robinson,Andrew J. Bentall,Thomas D. Atwell,Andrew D. Rule,Lilach O. Lerman,Shigao Chen,Chengwu Huang*

Main category: eess.IV

TL;DR: HA2HA是一种自监督去噪框架，专为超声微血管成像（UMI）设计，通过从互补角度子集构建训练对，显著提升图像质量。


<details>
  <summary>Details</summary>
Motivation: UMI在无对比剂或深部组织场景中常受低信噪比（SNR）限制，影响血管定量和疾病诊断。

Method: HA2HA利用互补角度的射频血流数据构建训练对，保持血管信号一致而噪声变化。

Result: 在多种数据集上验证，CNR和SNR提升超过15 dB，微血管血流可视化效果改善。

Conclusion: HA2HA为无标记、通用且临床适用的UMI解决方案，适用于无对比剂和对比增强场景。

Abstract: Ultrasound microvascular imaging (UMI) is often hindered by low signal-to-noise ratio (SNR), especially in contrast-free or deep tissue scenarios, which impairs subsequent vascular quantification and reliable disease diagnosis. To address this challenge, we propose Half-Angle-to-Half-Angle (HA2HA), a self-supervised denoising framework specifically designed for UMI. HA2HA constructs training pairs from complementary angular subsets of beamformed radio-frequency (RF) blood flow data, across which vascular signals remain consistent while noise varies. HA2HA was trained using in-vivo contrast-free pig kidney data and validated across diverse datasets, including contrast-free and contrast-enhanced data from pig kidneys, as well as human liver and kidney. An improvement exceeding 15 dB in both contrast-to-noise ratio (CNR) and SNR was observed, indicating a substantial enhancement in image quality. In addition to power Doppler imaging, denoising directly in the RF domain is also beneficial for other downstream processing such as color Doppler imaging (CDI). CDI results of human liver derived from the HA2HA-denoised signals exhibited improved microvascular flow visualization, with a suppressed noisy background. HA2HA offers a label-free, generalizable, and clinically applicable solution for robust vascular imaging in both contrast-free and contrast-enhanced UMI.

</details>


### [34] [Diffusion-Based Limited-Angle CT Reconstruction under Noisy Conditions](https://arxiv.org/abs/2507.05647)
*Jiaqi Guo,Santiago López-Tapia*

Main category: eess.IV

TL;DR: 论文提出了一种基于扩散模型的框架，用于解决有限角度计算机断层扫描（LACT）中的缺失角度投影问题，并通过噪声感知机制提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LACT因缺失角度投影导致重建图像存在严重伪影，现有方法多假设理想无噪声测量，未能解决实际噪声问题。

Method: 将LACT视为正弦图修复任务，采用基于MR-SDE的扩散模型完成缺失角度视图，并引入RNSD$^+$噪声感知机制。

Result: 实验表明，该方法在数据一致性和感知质量上优于基线模型，且对噪声强度和采集场景具有良好泛化性。

Conclusion: 提出的框架有效解决了LACT中的噪声问题，提升了重建的可靠性和鲁棒性。

Abstract: Limited-Angle Computed Tomography (LACT) is a challenging inverse problem where missing angular projections lead to incomplete sinograms and severe artifacts in the reconstructed images. While recent learning-based methods have demonstrated effectiveness, most of them assume ideal, noise-free measurements and fail to address the impact of measurement noise. To overcome this limitation, we treat LACT as a sinogram inpainting task and propose a diffusion-based framework that completes missing angular views using a Mean-Reverting Stochastic Differential Equation (MR-SDE) formulation. To improve robustness under realistic noise, we propose RNSD$^+$, a novel noise-aware rectification mechanism that explicitly models inference-time uncertainty, enabling reliable and robust reconstruction. Extensive experiments demonstrate that our method consistently surpasses baseline models in data consistency and perceptual quality, and generalizes well across varying noise intensity and acquisition scenarios.

</details>


### [35] [LangMamba: A Language-driven Mamba Framework for Low-dose CT Denoising with Vision-language Models](https://arxiv.org/abs/2507.06140)
*Zhihao Chen,Tao Chen,Chenhui Wang,Qi Gao,Huidong Xie,Chuang Niu,Ge Wang,Hongming Shan*

Main category: eess.IV

TL;DR: LangMamba是一个基于语言驱动的Mamba框架，用于低剂量CT（LDCT）去噪，通过结合视觉语言模型（VLM）的语义信息提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT（LDCT）图像质量较差，影响诊断准确性。现有深度学习方法忽略高层语义信息，而视觉语言模型（VLMs）能提供结构化语义指导，为LDCT重建提供新思路。

Method: LangMamba采用两阶段学习策略：1）预训练语言引导的自编码器（LangAE），将正常剂量CT（NDCT）映射到语义空间；2）结合SEED模块和LangDA损失，增强语义对齐和去噪效果。

Result: 实验表明，LangMamba在公开数据集上优于现有方法，显著提升细节保留和视觉保真度，且LangAE具有强泛化能力，LangDA损失增强可解释性。

Conclusion: 语言作为监督信号在LDCT去噪中具有潜力，LangMamba为图像重建提供了新思路，代码已开源。

Abstract: Low-dose computed tomography (LDCT) reduces radiation exposure but often degrades image quality, potentially compromising diagnostic accuracy. Existing deep learning-based denoising methods focus primarily on pixel-level mappings, overlooking the potential benefits of high-level semantic guidance. Recent advances in vision-language models (VLMs) suggest that language can serve as a powerful tool for capturing structured semantic information, offering new opportunities to improve LDCT reconstruction. In this paper, we introduce LangMamba, a Language-driven Mamba framework for LDCT denoising that leverages VLM-derived representations to enhance supervision from normal-dose CT (NDCT). LangMamba follows a two-stage learning strategy. First, we pre-train a Language-guided AutoEncoder (LangAE) that leverages frozen VLMs to map NDCT images into a semantic space enriched with anatomical information. Second, we synergize LangAE with two key components to guide LDCT denoising: Semantic-Enhanced Efficient Denoiser (SEED), which enhances NDCT-relevant local semantic while capturing global features with efficient Mamba mechanism, and Language-engaged Dual-space Alignment (LangDA) Loss, which ensures that denoised images align with NDCT in both perceptual and semantic spaces. Extensive experiments on two public datasets demonstrate that LangMamba outperforms conventional state-of-the-art methods, significantly improving detail preservation and visual fidelity. Remarkably, LangAE exhibits strong generalizability to unseen datasets, thereby reducing training costs. Furthermore, LangDA loss improves explainability by integrating language-guided insights into image reconstruction and offers a plug-and-play fashion. Our findings shed new light on the potential of language as a supervisory signal to advance LDCT denoising. The code is publicly available on https://github.com/hao1635/LangMamba.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [36] [3DGS_LSR:Large_Scale Relocation for Autonomous Driving Based on 3D Gaussian Splatting](https://arxiv.org/abs/2507.05661)
*Haitao Lu,Haijier Chen,Haoze Liu,Shoujian Zhang,Bo Xu,Ziao Liu*

Main category: cs.RO

TL;DR: 论文提出了一种基于3D高斯泼溅（3DGS）的大规模重定位框架3DGS-LSR，仅需单目RGB图像即可实现厘米级定位，适用于复杂城市环境中的自主机器人导航。


<details>
  <summary>Details</summary>
Motivation: 解决复杂城市环境中GNSS定位不可靠以及传统地图方法存储和计算效率低的问题。

Method: 结合多传感器数据构建高精度3DGS地图，使用SuperPoint和SuperGlue进行特征提取与匹配，并通过迭代优化策略逐步优化定位结果。

Result: 在KITTI数据集上，3DGS-LSR在城镇道路、林荫大道和交通密集高速公路上的平均定位精度分别为0.026m、0.029m和0.081m，显著优于其他方法。

Conclusion: 3DGS-LSR为自主机器人在GNSS失效的复杂城市环境中提供了可靠的定位能力。

Abstract: In autonomous robotic systems, precise localization is a prerequisite for safe navigation. However, in complex urban environments, GNSS positioning often suffers from signal occlusion and multipath effects, leading to unreliable absolute positioning. Traditional mapping approaches are constrained by storage requirements and computational inefficiency, limiting their applicability to resource-constrained robotic platforms. To address these challenges, we propose 3DGS-LSR: a large-scale relocalization framework leveraging 3D Gaussian Splatting (3DGS), enabling centimeter-level positioning using only a single monocular RGB image on the client side. We combine multi-sensor data to construct high-accuracy 3DGS maps in large outdoor scenes, while the robot-side localization requires just a standard camera input. Using SuperPoint and SuperGlue for feature extraction and matching, our core innovation is an iterative optimization strategy that refines localization results through step-by-step rendering, making it suitable for real-time autonomous navigation. Experimental validation on the KITTI dataset demonstrates our 3DGS-LSR achieves average positioning accuracies of 0.026m, 0.029m, and 0.081m in town roads, boulevard roads, and traffic-dense highways respectively, significantly outperforming other representative methods while requiring only monocular RGB input. This approach provides autonomous robots with reliable localization capabilities even in challenging urban environments where GNSS fails.

</details>
