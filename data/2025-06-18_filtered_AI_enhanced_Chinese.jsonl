{"id": "2506.13814", "pdf": "https://arxiv.org/pdf/2506.13814", "abs": "https://arxiv.org/abs/2506.13814", "authors": ["Lufei Liu", "Tor M. Aamodt"], "title": "ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering", "categories": ["cs.GR", "cs.LG", "eess.IV"], "comment": "Published at ICML 2025", "summary": "Graphics rendering applications increasingly leverage neural networks in tasks such as denoising, supersampling, and frame extrapolation to improve image quality while maintaining frame rates. The temporal coherence inherent in these tasks presents an opportunity to reuse intermediate results from previous frames and avoid redundant computations. Recent work has shown that caching intermediate features to be reused in subsequent inferences is an effective method to reduce latency in diffusion models. We extend this idea to real-time rendering and present ReFrame, which explores different caching policies to optimize trade-offs between quality and performance in rendering workloads. ReFrame can be applied to a variety of encoder-decoder style networks commonly found in rendering pipelines. Experimental results show that we achieve 1.4x speedup on average with negligible quality loss in three real-time rendering tasks. Code available: https://ubc-aamodt-group.github.io/reframe-layer-caching/", "AI": {"tldr": "ReFrame\u901a\u8fc7\u7f13\u5b58\u4e2d\u95f4\u7279\u5f81\u4f18\u5316\u5b9e\u65f6\u6e32\u67d3\u4efb\u52a1\uff0c\u5728\u8d28\u91cf\u635f\u5931\u53ef\u5ffd\u7565\u7684\u60c5\u51b5\u4e0b\u5e73\u5747\u63d0\u901f1.4\u500d\u3002", "motivation": "\u5229\u7528\u6e32\u67d3\u4efb\u52a1\u4e2d\u7684\u65f6\u95f4\u8fde\u8d2f\u6027\uff0c\u907f\u514d\u5197\u4f59\u8ba1\u7b97\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u6269\u5c55\u7f13\u5b58\u4e2d\u95f4\u7279\u5f81\u7684\u65b9\u6cd5\u81f3\u5b9e\u65f6\u6e32\u67d3\uff0c\u63a2\u7d22\u4e0d\u540c\u7f13\u5b58\u7b56\u7565\u4ee5\u5e73\u8861\u8d28\u91cf\u4e0e\u6027\u80fd\u3002", "result": "\u5728\u4e09\u79cd\u5b9e\u65f6\u6e32\u67d3\u4efb\u52a1\u4e2d\uff0c\u5e73\u5747\u63d0\u901f1.4\u500d\u4e14\u8d28\u91cf\u635f\u5931\u53ef\u5ffd\u7565\u3002", "conclusion": "ReFrame\u9002\u7528\u4e8e\u591a\u79cd\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\uff0c\u6709\u6548\u4f18\u5316\u6e32\u67d3\u6027\u80fd\u3002"}}
{"id": "2506.14315", "pdf": "https://arxiv.org/pdf/2506.14315", "abs": "https://arxiv.org/abs/2506.14315", "authors": ["Jinyan Yuan", "Bangbang Yang", "Keke Wang", "Panwang Pan", "Lin Ma", "Xuehai Zhang", "Xiao Liu", "Zhaopeng Cui", "Yuewen Ma"], "title": "ImmerseGen: Agent-Guided Immersive World Generation with Alpha-Textured Proxies", "categories": ["cs.GR", "cs.CV"], "comment": "Project webpage: https://immersegen.github.io", "summary": "Automatic creation of 3D scenes for immersive VR presence has been a significant research focus for decades. However, existing methods often rely on either high-poly mesh modeling with post-hoc simplification or massive 3D Gaussians, resulting in a complex pipeline or limited visual realism. In this paper, we demonstrate that such exhaustive modeling is unnecessary for achieving compelling immersive experience. We introduce ImmerseGen, a novel agent-guided framework for compact and photorealistic world modeling. ImmerseGen represents scenes as hierarchical compositions of lightweight geometric proxies, i.e., simplified terrain and billboard meshes, and generates photorealistic appearance by synthesizing RGBA textures onto these proxies. Specifically, we propose terrain-conditioned texturing for user-centric base world synthesis, and RGBA asset texturing for midground and foreground scenery.This reformulation offers several advantages: (i) it simplifies modeling by enabling agents to guide generative models in producing coherent textures that integrate seamlessly with the scene; (ii) it bypasses complex geometry creation and decimation by directly synthesizing photorealistic textures on proxies, preserving visual quality without degradation; (iii) it enables compact representations suitable for real-time rendering on mobile VR headsets. To automate scene creation from text prompts, we introduce VLM-based modeling agents enhanced with semantic grid-based analysis for improved spatial reasoning and accurate asset placement. ImmerseGen further enriches scenes with dynamic effects and ambient audio to support multisensory immersion. Experiments on scene generation and live VR showcases demonstrate that ImmerseGen achieves superior photorealism, spatial coherence and rendering efficiency compared to prior methods. Project webpage: https://immersegen.github.io.", "AI": {"tldr": "ImmerseGen\u662f\u4e00\u79cd\u65b0\u578b\u7684\u4ee3\u7406\u5f15\u5bfc\u6846\u67b6\uff0c\u7528\u4e8e\u7d27\u51d1\u4e14\u903c\u771f\u76843D\u573a\u666f\u5efa\u6a21\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u51e0\u4f55\u4ee3\u7406\u548cRGBA\u7eb9\u7406\u5408\u6210\uff0c\u7b80\u5316\u4e86\u4f20\u7edf\u590d\u6742\u5efa\u6a21\u6d41\u7a0b\u3002", "motivation": "\u73b0\u6709\u76843D\u573a\u666f\u5efa\u6a21\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u9ad8\u591a\u8fb9\u5f62\u7f51\u683c\u6216\u5927\u91cf3D\u9ad8\u65af\u5206\u5e03\uff0c\u5bfc\u81f4\u6d41\u7a0b\u590d\u6742\u6216\u89c6\u89c9\u6548\u679c\u53d7\u9650\u3002\u672c\u6587\u65e8\u5728\u8bc1\u660e\u65e0\u9700\u590d\u6742\u5efa\u6a21\u5373\u53ef\u5b9e\u73b0\u6c89\u6d78\u5f0f\u4f53\u9a8c\u3002", "method": "ImmerseGen\u901a\u8fc7\u5206\u5c42\u8f7b\u91cf\u7ea7\u51e0\u4f55\u4ee3\u7406\uff08\u5982\u7b80\u5316\u5730\u5f62\u548c\u5e7f\u544a\u724c\u7f51\u683c\uff09\u8868\u793a\u573a\u666f\uff0c\u5e76\u5408\u6210RGBA\u7eb9\u7406\u3002\u63d0\u51fa\u5730\u5f62\u6761\u4ef6\u7eb9\u7406\u548cRGBA\u8d44\u4ea7\u7eb9\u7406\u6280\u672f\uff0c\u7ed3\u5408VLM\u4ee3\u7406\u589e\u5f3a\u8bed\u4e49\u7f51\u683c\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cImmerseGen\u5728\u903c\u771f\u5ea6\u3001\u7a7a\u95f4\u4e00\u81f4\u6027\u548c\u6e32\u67d3\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u79fb\u52a8VR\u8bbe\u5907\u7684\u5b9e\u65f6\u6e32\u67d3\u3002", "conclusion": "ImmerseGen\u901a\u8fc7\u7b80\u5316\u5efa\u6a21\u6d41\u7a0b\u548c\u76f4\u63a5\u7eb9\u7406\u5408\u6210\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u903c\u771f\u76843D\u573a\u666f\u751f\u6210\uff0c\u4e3a\u6c89\u6d78\u5f0fVR\u4f53\u9a8c\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.13770", "pdf": "https://arxiv.org/pdf/2506.13770", "abs": "https://arxiv.org/abs/2506.13770", "authors": ["Shiwen Zhang", "Zhuowei Chen", "Lang Chen", "Yanze Wu"], "title": "CDST: Color Disentangled Style Transfer for Universal Style Reference Customization", "categories": ["cs.CV"], "comment": "codes and models will be released if the paper is accepted", "summary": "We introduce Color Disentangled Style Transfer (CDST), a novel and efficient two-stream style transfer training paradigm which completely isolates color from style and forces the style stream to be color-blinded. With one same model, CDST unlocks universal style transfer capabilities in a tuning-free manner during inference. Especially, the characteristics-preserved style transfer with style and content references is solved in the tuning-free way for the first time. CDST significantly improves the style similarity by multi-feature image embeddings compression and preserves strong editing capability via our new CDST style definition inspired by Diffusion UNet disentanglement law. By conducting thorough qualitative and quantitative experiments and human evaluations, we demonstrate that CDST achieves state-of-the-art results on various style transfer tasks.", "AI": {"tldr": "CDST\u662f\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u6d41\u98ce\u683c\u8fc1\u79fb\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u5b8c\u5168\u5206\u79bb\u989c\u8272\u4e0e\u98ce\u683c\uff0c\u5b9e\u73b0\u65e0\u9700\u8c03\u4f18\u7684\u901a\u7528\u98ce\u683c\u8fc1\u79fb\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u98ce\u683c\u8fc1\u79fb\u4e2d\u989c\u8272\u4e0e\u98ce\u683c\u6df7\u6dc6\u7684\u95ee\u9898\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u65e0\u9700\u8c03\u4f18\u7684\u7279\u5f81\u4fdd\u7559\u98ce\u683c\u8fc1\u79fb\u3002", "method": "\u91c7\u7528\u53cc\u6d41\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u591a\u7279\u5f81\u56fe\u50cf\u5d4c\u5165\u538b\u7f29\u548c\u57fa\u4e8eDiffusion UNet\u89e3\u8026\u5b9a\u5f8b\u7684\u65b0\u98ce\u683c\u5b9a\u4e49\u3002", "result": "\u5728\u591a\u79cd\u98ce\u683c\u8fc1\u79fb\u4efb\u52a1\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u548c\u4eba\u5de5\u8bc4\u4f30\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "CDST\u4e3a\u98ce\u683c\u8fc1\u79fb\u9886\u57df\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u663e\u8457\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.13780", "pdf": "https://arxiv.org/pdf/2506.13780", "abs": "https://arxiv.org/abs/2506.13780", "authors": ["Sedat Porikli", "Vedat Porikli"], "title": "Hidden Bias in the Machine: Stereotypes in Text-to-Image Models", "categories": ["cs.CV", "cs.AI", "cs.CY", "cs.LG"], "comment": "Equal contribution by both authors, Published at CVPR 2025 Workshop on Experimental Model Auditing via Controllable Synthesis (EMACS) and Workshop on Demographic Diversity in Computer Vision (DemoDiv)", "summary": "Text-to-Image (T2I) models have transformed visual content creation, producing highly realistic images from natural language prompts. However, concerns persist around their potential to replicate and magnify existing societal biases. To investigate these issues, we curated a diverse set of prompts spanning thematic categories such as occupations, traits, actions, ideologies, emotions, family roles, place descriptions, spirituality, and life events. For each of the 160 unique topics, we crafted multiple prompt variations to reflect a wide range of meanings and perspectives. Using Stable Diffusion 1.5 (UNet-based) and Flux-1 (DiT-based) models with original checkpoints, we generated over 16,000 images under consistent settings. Additionally, we collected 8,000 comparison images from Google Image Search. All outputs were filtered to exclude abstract, distorted, or nonsensical results. Our analysis reveals significant disparities in the representation of gender, race, age, somatotype, and other human-centric factors across generated images. These disparities often mirror and reinforce harmful stereotypes embedded in societal narratives. We discuss the implications of these findings and emphasize the need for more inclusive datasets and development practices to foster fairness in generative visual systems.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u5982\u4f55\u590d\u5236\u548c\u653e\u5927\u793e\u4f1a\u504f\u89c1\uff0c\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u7684\u63d0\u793a\u548c\u56fe\u50cf\u5206\u6790\u63ed\u793a\u4e86\u6027\u522b\u3001\u79cd\u65cf\u7b49\u65b9\u9762\u7684\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u8c03\u67e5T2I\u6a21\u578b\u5728\u751f\u6210\u56fe\u50cf\u65f6\u662f\u5426\u590d\u5236\u548c\u653e\u5927\u793e\u4f1a\u504f\u89c1\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u516c\u5e73\u7684\u89c6\u89c9\u751f\u6210\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528Stable Diffusion 1.5\u548cFlux-1\u6a21\u578b\u751f\u621016,000\u591a\u5f20\u56fe\u50cf\uff0c\u5e76\u6536\u96c68,000\u5f20Google\u56fe\u50cf\u8fdb\u884c\u6bd4\u8f83\uff0c\u5206\u6790\u4eba\u7c7b\u4e2d\u5fc3\u56e0\u7d20\u7684\u5dee\u5f02\u3002", "result": "\u53d1\u73b0\u751f\u6210\u7684\u56fe\u50cf\u5728\u6027\u522b\u3001\u79cd\u65cf\u7b49\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u5f3a\u5316\u4e86\u793e\u4f1a\u53d9\u4e8b\u4e2d\u7684\u6709\u5bb3\u523b\u677f\u5370\u8c61\u3002", "conclusion": "\u5f3a\u8c03\u9700\u8981\u66f4\u5305\u5bb9\u7684\u6570\u636e\u96c6\u548c\u5f00\u53d1\u5b9e\u8df5\uff0c\u4ee5\u51cf\u5c11\u751f\u6210\u89c6\u89c9\u7cfb\u7edf\u4e2d\u7684\u504f\u89c1\u3002"}}
{"id": "2506.13846", "pdf": "https://arxiv.org/pdf/2506.13846", "abs": "https://arxiv.org/abs/2506.13846", "authors": ["Runtao Liu", "Jiahao Zhan", "Yingqing He", "Chen Wei", "Alan Yuille", "Qifeng Chen"], "title": "Fake it till You Make it: Reward Modeling as Discriminative Prediction", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "An effective reward model plays a pivotal role in reinforcement learning for post-training enhancement of visual generative models. However, current approaches of reward modeling suffer from implementation complexity due to their reliance on extensive human-annotated preference data or meticulously engineered quality dimensions that are often incomplete and engineering-intensive. Inspired by adversarial training in generative adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward modeling framework that eliminates manual preference annotation and explicit quality dimension engineering. Our method trains the reward model through discrimination between a small set of representative, unpaired target samples(denoted as Preference Proxy Data) and model-generated ordinary outputs, requiring only a few hundred target samples. Comprehensive experiments demonstrate our GAN-RM's effectiveness across multiple key applications including test-time scaling implemented as Best-of-N sample filtering, post-training approaches like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGAN-RM\u7684\u9ad8\u6548\u5956\u52b1\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u907f\u514d\u4eba\u5de5\u6807\u6ce8\u548c\u663e\u5f0f\u8d28\u91cf\u7ef4\u5ea6\u8bbe\u8ba1\uff0c\u4ec5\u9700\u5c11\u91cf\u76ee\u6807\u6837\u672c\u5373\u53ef\u5b9e\u73b0\u6709\u6548\u7684\u5956\u52b1\u5efa\u6a21\u3002", "motivation": "\u5f53\u524d\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u6216\u590d\u6742\u8d28\u91cf\u7ef4\u5ea6\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u590d\u6742\u4e14\u4e0d\u5b8c\u6574\u3002\u672c\u6587\u65e8\u5728\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u5229\u7528GAN\u7684\u5bf9\u6297\u8bad\u7ec3\u601d\u60f3\uff0c\u901a\u8fc7\u533a\u5206\u5c11\u91cf\u76ee\u6807\u6837\u672c\u4e0e\u6a21\u578b\u751f\u6210\u6837\u672c\uff0c\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6216\u663e\u5f0f\u8d28\u91cf\u7ef4\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eGAN-RM\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u6709\u6548\uff0c\u5305\u62ec\u6d4b\u8bd5\u65f6\u7f29\u653e\u548c\u8bad\u7ec3\u540e\u4f18\u5316\u65b9\u6cd5\u3002", "conclusion": "GAN-RM\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u590d\u6742\u4eba\u5de5\u5e72\u9884\u7684\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u89c6\u89c9\u751f\u6210\u6a21\u578b\u589e\u5f3a\u4efb\u52a1\u3002"}}
{"id": "2506.14121", "pdf": "https://arxiv.org/pdf/2506.14121", "abs": "https://arxiv.org/abs/2506.14121", "authors": ["Siyu Xu", "Wenjie Li", "Guangwei Gao", "Jian Yang", "Guo-Jun Qi", "Chia-Wen Lin"], "title": "FADPNet: Frequency-Aware Dual-Path Network for Face Super-Resolution", "categories": ["cs.CV"], "comment": "12 pages, 11 figures, 6 tales", "summary": "Face super-resolution (FSR) under limited computational costs remains an open problem. Existing approaches typically treat all facial pixels equally, resulting in suboptimal allocation of computational resources and degraded FSR performance. CNN is relatively sensitive to high-frequency facial features, such as component contours and facial outlines. Meanwhile, Mamba excels at capturing low-frequency features like facial color and fine-grained texture, and does so with lower complexity than Transformers. Motivated by these observations, we propose FADPNet, a Frequency-Aware Dual-Path Network that decomposes facial features into low- and high-frequency components and processes them via dedicated branches. For low-frequency regions, we introduce a Mamba-based Low-Frequency Enhancement Block (LFEB), which combines state-space attention with squeeze-and-excitation operations to extract low-frequency global interactions and emphasize informative channels. For high-frequency regions, we design a CNN-based Deep Position-Aware Attention (DPA) module to enhance spatially-dependent structural details, complemented by a lightweight High-Frequency Refinement (HFR) module that further refines frequency-specific representations. Through the above designs, our method achieves an excellent balance between FSR quality and model efficiency, outperforming existing approaches.", "AI": {"tldr": "FADPNet\u662f\u4e00\u79cd\u9891\u7387\u611f\u77e5\u53cc\u8def\u5f84\u7f51\u7edc\uff0c\u901a\u8fc7\u5206\u79bb\u5904\u7406\u4f4e\u9891\u548c\u9ad8\u9891\u9762\u90e8\u7279\u5f81\uff0c\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\uff0c\u63d0\u5347\u9762\u90e8\u8d85\u5206\u8fa8\u7387\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5bf9\u6240\u6709\u9762\u90e8\u50cf\u7d20\u4e00\u89c6\u540c\u4ec1\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u4e0d\u4f18\u548c\u6027\u80fd\u4e0b\u964d\u3002CNN\u5bf9\u9ad8\u9891\u7279\u5f81\u654f\u611f\uff0cMamba\u64c5\u957f\u4f4e\u9891\u7279\u5f81\u4e14\u590d\u6742\u5ea6\u4f4e\uff0c\u56e0\u6b64\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "method": "\u63d0\u51faFADPNet\uff0c\u5206\u89e3\u7279\u5f81\u4e3a\u4f4e\u9891\u548c\u9ad8\u9891\uff0c\u5206\u522b\u7528Mamba\u548cCNN\u5904\u7406\u3002\u4f4e\u9891\u7528LFEB\u6a21\u5757\uff0c\u9ad8\u9891\u7528DPA\u548cHFR\u6a21\u5757\u3002", "result": "\u65b9\u6cd5\u5728\u8d85\u5206\u8fa8\u7387\u8d28\u91cf\u548c\u6a21\u578b\u6548\u7387\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FADPNet\u901a\u8fc7\u9891\u7387\u611f\u77e5\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9762\u90e8\u8d85\u5206\u8fa8\u7387\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2506.14130", "pdf": "https://arxiv.org/pdf/2506.14130", "abs": "https://arxiv.org/abs/2506.14130", "authors": ["Chunyu Cao", "Jintao Cheng", "Zeyu Chen", "Linfan Zhan", "Rui Fan", "Zhijian He", "Xiaoyu Tang"], "title": "KDMOS:Knowledge Distillation for Motion Segmentation", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Motion Object Segmentation (MOS) is crucial for autonomous driving, as it enhances localization, path planning, map construction, scene flow estimation, and future state prediction. While existing methods achieve strong performance, balancing accuracy and real-time inference remains a challenge. To address this, we propose a logits-based knowledge distillation framework for MOS, aiming to improve accuracy while maintaining real-time efficiency. Specifically, we adopt a Bird's Eye View (BEV) projection-based model as the student and a non-projection model as the teacher. To handle the severe imbalance between moving and non-moving classes, we decouple them and apply tailored distillation strategies, allowing the teacher model to better learn key motion-related features. This approach significantly reduces false positives and false negatives. Additionally, we introduce dynamic upsampling, optimize the network architecture, and achieve a 7.69% reduction in parameter count, mitigating overfitting. Our method achieves a notable IoU of 78.8% on the hidden test set of the SemanticKITTI-MOS dataset and delivers competitive results on the Apollo dataset. The KDMOS implementation is available at https://github.com/SCNU-RISLAB/KDMOS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8elogits\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff08KDMOS\uff09\uff0c\u7528\u4e8e\u8fd0\u52a8\u76ee\u6807\u5206\u5272\uff08MOS\uff09\uff0c\u901a\u8fc7BEV\u6295\u5f71\u6a21\u578b\uff08\u5b66\u751f\uff09\u548c\u975e\u6295\u5f71\u6a21\u578b\uff08\u6559\u5e08\uff09\u7684\u7ed3\u5408\uff0c\u63d0\u5347\u7cbe\u5ea6\u5e76\u4fdd\u6301\u5b9e\u65f6\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u4e14\u8fd0\u52a8\u4e0e\u975e\u8fd0\u52a8\u7c7b\u522b\u4e25\u91cd\u4e0d\u5e73\u8861\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u91c7\u7528BEV\u6295\u5f71\u6a21\u578b\uff08\u5b66\u751f\uff09\u548c\u975e\u6295\u5f71\u6a21\u578b\uff08\u6559\u5e08\uff09\uff0c\u901a\u8fc7\u89e3\u8026\u8fd0\u52a8\u4e0e\u975e\u8fd0\u52a8\u7c7b\u522b\u5e76\u5e94\u7528\u5b9a\u5236\u84b8\u998f\u7b56\u7565\uff0c\u4f18\u5316\u7f51\u7edc\u67b6\u6784\u5e76\u51cf\u5c11\u53c2\u6570\u3002", "result": "\u5728SemanticKITTI-MOS\u9690\u85cf\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523078.8%\u7684IoU\uff0c\u5e76\u5728Apollo\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "KDMOS\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86MOS\u4efb\u52a1\u7684\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u51cf\u5c11\u4e86\u8fc7\u62df\u5408\u3002"}}
{"id": "2506.14168", "pdf": "https://arxiv.org/pdf/2506.14168", "abs": "https://arxiv.org/abs/2506.14168", "authors": ["Hu Yu", "Biao Gong", "Hangjie Yuan", "DanDan Zheng", "Weilong Chai", "Jingdong Chen", "Kecheng Zheng", "Feng Zhao"], "title": "VideoMAR: Autoregressive Video Generatio with Continuous Tokens", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to NeurIPS 2025", "summary": "Masked-based autoregressive models have demonstrated promising image generation capability in continuous space. However, their potential for video generation remains under-explored. In this paper, we propose \\textbf{VideoMAR}, a concise and efficient decoder-only autoregressive image-to-video model with continuous tokens, composing temporal frame-by-frame and spatial masked generation. We first identify temporal causality and spatial bi-directionality as the first principle of video AR models, and propose the next-frame diffusion loss for the integration of mask and video generation. Besides, the huge cost and difficulty of long sequence autoregressive modeling is a basic but crucial issue. To this end, we propose the temporal short-to-long curriculum learning and spatial progressive resolution training, and employ progressive temperature strategy at inference time to mitigate the accumulation error. Furthermore, VideoMAR replicates several unique capacities of language models to video generation. It inherently bears high efficiency due to simultaneous temporal-wise KV cache and spatial-wise parallel generation, and presents the capacity of spatial and temporal extrapolation via 3D rotary embeddings. On the VBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos I2V) while requiring significantly fewer parameters ($9.3\\%$), training data ($0.5\\%$), and GPU resources ($0.2\\%$).", "AI": {"tldr": "VideoMAR\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u7801\u5668\u81ea\u56de\u5f52\u56fe\u50cf\u5230\u89c6\u9891\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u65f6\u95f4\u5e27\u751f\u6210\u548c\u7a7a\u95f4\u63a9\u7801\u751f\u6210\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u53c2\u6570\u3001\u8bad\u7ec3\u6570\u636e\u548cGPU\u8d44\u6e90\u9700\u6c42\u3002", "motivation": "\u63a2\u7d22\u81ea\u56de\u5f52\u6a21\u578b\u5728\u89c6\u9891\u751f\u6210\u4e2d\u7684\u6f5c\u529b\uff0c\u89e3\u51b3\u957f\u5e8f\u5217\u5efa\u6a21\u7684\u9ad8\u6210\u672c\u548c\u96be\u5ea6\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u65f6\u95f4\u56e0\u679c\u6027\u548c\u7a7a\u95f4\u53cc\u5411\u6027\u4f5c\u4e3a\u89c6\u9891\u81ea\u56de\u5f52\u6a21\u578b\u7684\u57fa\u672c\u539f\u5219\uff0c\u91c7\u7528\u4e0b\u4e00\u5e27\u6269\u6563\u635f\u5931\u3001\u65f6\u95f4\u77ed\u5230\u957f\u8bfe\u7a0b\u5b66\u4e60\u548c\u7a7a\u95f4\u6e10\u8fdb\u5206\u8fa8\u7387\u8bad\u7ec3\u3002", "result": "\u5728VBench-I2V\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVideoMAR\u8d85\u8d8a\u5148\u524d\u6700\u4f73\u6a21\u578b\uff08Cosmos I2V\uff09\uff0c\u53c2\u6570\u3001\u8bad\u7ec3\u6570\u636e\u548cGPU\u8d44\u6e90\u9700\u6c42\u5927\u5e45\u964d\u4f4e\u3002", "conclusion": "VideoMAR\u5c55\u793a\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u5728\u89c6\u9891\u751f\u6210\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u6f5c\u529b\uff0c\u540c\u65f6\u5177\u5907\u65f6\u7a7a\u5916\u63a8\u80fd\u529b\u3002"}}
{"id": "2506.14181", "pdf": "https://arxiv.org/pdf/2506.14181", "abs": "https://arxiv.org/abs/2506.14181", "authors": ["Yufei Li", "Jirui Wu", "Long Tian", "Liming Wang", "Xiaonan Liu", "Zijun Liu", "Xiyang Liu"], "title": "Meta-SurDiff: Classification Diffusion Model Optimized by Meta Learning is Reliable for Online Surgical Phase Recognition", "categories": ["cs.CV"], "comment": "15 pages, 5 figures", "summary": "Online surgical phase recognition has drawn great attention most recently due to its potential downstream applications closely related to human life and health. Despite deep models have made significant advances in capturing the discriminative long-term dependency of surgical videos to achieve improved recognition, they rarely account for exploring and modeling the uncertainty in surgical videos, which should be crucial for reliable online surgical phase recognition. We categorize the sources of uncertainty into two types, frame ambiguity in videos and unbalanced distribution among surgical phases, which are inevitable in surgical videos. To address this pivot issue, we introduce a meta-learning-optimized classification diffusion model (Meta-SurDiff), to take full advantage of the deep generative model and meta-learning in achieving precise frame-level distribution estimation for reliable online surgical phase recognition. For coarse recognition caused by ambiguous video frames, we employ a classification diffusion model to assess the confidence of recognition results at a finer-grained frame-level instance. For coarse recognition caused by unbalanced phase distribution, we use a meta-learning based objective to learn the diffusion model, thus enhancing the robustness of classification boundaries for different surgical phases.We establish effectiveness of Meta-SurDiff in online surgical phase recognition through extensive experiments on five widely used datasets using more than four practical metrics. The datasets include Cholec80, AutoLaparo, M2Cai16, OphNet, and NurViD, where OphNet comes from ophthalmic surgeries, NurViD is the daily care dataset, while the others come from laparoscopic surgeries. We will release the code upon acceptance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5143\u5b66\u4e60\u4f18\u5316\u7684\u5206\u7c7b\u6269\u6563\u6a21\u578b\uff08Meta-SurDiff\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u624b\u672f\u89c6\u9891\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4ee5\u5b9e\u73b0\u53ef\u9760\u7684\u5728\u7ebf\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u3002", "motivation": "\u624b\u672f\u89c6\u9891\u4e2d\u5b58\u5728\u4e0d\u53ef\u907f\u514d\u7684\u4e0d\u786e\u5b9a\u6027\uff08\u5982\u5e27\u6a21\u7cca\u548c\u9636\u6bb5\u5206\u5e03\u4e0d\u5e73\u8861\uff09\uff0c\u73b0\u6709\u6df1\u5ea6\u6a21\u578b\u672a\u5145\u5206\u5efa\u6a21\u8fd9\u4e9b\u4e0d\u786e\u5b9a\u6027\uff0c\u5f71\u54cd\u8bc6\u522b\u7684\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528\u5206\u7c7b\u6269\u6563\u6a21\u578b\u8bc4\u4f30\u6a21\u7cca\u5e27\u7684\u7f6e\u4fe1\u5ea6\uff0c\u5e76\u901a\u8fc7\u5143\u5b66\u4e60\u4f18\u5316\u6269\u6563\u6a21\u578b\u4ee5\u589e\u5f3a\u5206\u7c7b\u8fb9\u754c\u5bf9\u4e0d\u540c\u624b\u672f\u9636\u6bb5\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u4e94\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\uff08Cholec80\u3001AutoLaparo\u3001M2Cai16\u3001OphNet\u3001NurViD\uff09\u4e0a\u9a8c\u8bc1\u4e86Meta-SurDiff\u7684\u6709\u6548\u6027\u3002", "conclusion": "Meta-SurDiff\u901a\u8fc7\u5efa\u6a21\u4e0d\u786e\u5b9a\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u7ebf\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2506.14229", "pdf": "https://arxiv.org/pdf/2506.14229", "abs": "https://arxiv.org/abs/2506.14229", "authors": ["Changbai Li", "Haodong Zhu", "Hanlin Chen", "Juan Zhang", "Tongfei Chen", "Shuo Yang", "Shuwei Shao", "Wenhao Dong", "Baochang Zhang"], "title": "HRGS: Hierarchical Gaussian Splatting for Memory-Efficient High-Resolution 3D Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has made significant strides in real-time 3D scene reconstruction, but faces memory scalability issues in high-resolution scenarios. To address this, we propose Hierarchical Gaussian Splatting (HRGS), a memory-efficient framework with hierarchical block-level optimization. First, we generate a global, coarse Gaussian representation from low-resolution data. Then, we partition the scene into multiple blocks, refining each block with high-resolution data. The partitioning involves two steps: Gaussian partitioning, where irregular scenes are normalized into a bounded cubic space with a uniform grid for task distribution, and training data partitioning, where only relevant observations are retained for each block. By guiding block refinement with the coarse Gaussian prior, we ensure seamless Gaussian fusion across adjacent blocks. To reduce computational demands, we introduce Importance-Driven Gaussian Pruning (IDGP), which computes importance scores for each Gaussian and removes those with minimal contribution, speeding up convergence and reducing memory usage. Additionally, we incorporate normal priors from a pretrained model to enhance surface reconstruction quality. Our method enables high-quality, high-resolution 3D scene reconstruction even under memory constraints. Extensive experiments on three benchmarks show that HRGS achieves state-of-the-art performance in high-resolution novel view synthesis (NVS) and surface reconstruction tasks.", "AI": {"tldr": "HRGS\u63d0\u51fa\u4e86\u4e00\u79cd\u5185\u5b58\u9ad8\u6548\u7684\u5206\u5c42\u9ad8\u65af\u6e85\u5c04\u6846\u67b6\uff0c\u901a\u8fc7\u5757\u7ea7\u4f18\u5316\u548c\u91cd\u8981\u6027\u9a71\u52a8\u7684\u9ad8\u65af\u4fee\u526a\uff0c\u89e3\u51b3\u4e863DGS\u5728\u9ad8\u5206\u8fa8\u7387\u573a\u666f\u4e2d\u7684\u5185\u5b58\u6269\u5c55\u95ee\u9898\u3002", "motivation": "3D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u5728\u5b9e\u65f63D\u573a\u666f\u91cd\u5efa\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u9ad8\u5206\u8fa8\u7387\u573a\u666f\u4e0b\u9762\u4e34\u5185\u5b58\u6269\u5c55\u95ee\u9898\u3002", "method": "HRGS\u9996\u5148\u751f\u6210\u4e00\u4e2a\u5168\u5c40\u7684\u7c97\u7565\u9ad8\u65af\u8868\u793a\uff0c\u7136\u540e\u5c06\u573a\u666f\u5212\u5206\u4e3a\u591a\u4e2a\u5757\uff0c\u5e76\u7528\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u7ec6\u5316\u6bcf\u4e2a\u5757\u3002\u901a\u8fc7\u9ad8\u65af\u5206\u533a\u548c\u8bad\u7ec3\u6570\u636e\u5206\u533a\u4f18\u5316\u4efb\u52a1\u5206\u914d\uff0c\u5e76\u5f15\u5165\u91cd\u8981\u6027\u9a71\u52a8\u7684\u9ad8\u65af\u4fee\u526a\uff08IDGP\uff09\u51cf\u5c11\u8ba1\u7b97\u9700\u6c42\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHRGS\u5728\u9ad8\u5206\u8fa8\u7387\u65b0\u89c6\u89d2\u5408\u6210\uff08NVS\uff09\u548c\u8868\u9762\u91cd\u5efa\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "HRGS\u80fd\u591f\u5728\u5185\u5b58\u9650\u5236\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u9ad8\u5206\u8fa8\u7387\u76843D\u573a\u666f\u91cd\u5efa\u3002"}}
{"id": "2506.14322", "pdf": "https://arxiv.org/pdf/2506.14322", "abs": "https://arxiv.org/abs/2506.14322", "authors": ["Avigail Cohen Rimon", "Mirela Ben-Chen", "Or Litany"], "title": "FRIDU: Functional Map Refinement with Guided Image Diffusion", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to SGP 2025 (Symposium on Geometry Processing)", "summary": "We propose a novel approach for refining a given correspondence map between two shapes. A correspondence map represented as a functional map, namely a change of basis matrix, can be additionally treated as a 2D image. With this perspective, we train an image diffusion model directly in the space of functional maps, enabling it to generate accurate maps conditioned on an inaccurate initial map. The training is done purely in the functional space, and thus is highly efficient. At inference time, we use the pointwise map corresponding to the current functional map as guidance during the diffusion process. The guidance can additionally encourage different functional map objectives, such as orthogonality and commutativity with the Laplace-Beltrami operator. We show that our approach is competitive with state-of-the-art methods of map refinement and that guided diffusion models provide a promising pathway to functional map processing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u529f\u80fd\u6620\u5c04\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u76f4\u63a5\u751f\u6210\u51c6\u786e\u7684\u529f\u80fd\u6620\u5c04\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u5229\u7528\u70b9\u6620\u5c04\u4f5c\u4e3a\u6307\u5bfc\u3002", "motivation": "\u73b0\u6709\u529f\u80fd\u6620\u5c04\u4f18\u5316\u65b9\u6cd5\u6548\u7387\u4e0d\u9ad8\uff0c\u4e14\u96be\u4ee5\u6ee1\u8db3\u6b63\u4ea4\u6027\u548c\u4e0e\u62c9\u666e\u62c9\u65af-\u8d1d\u5c14\u7279\u62c9\u7c73\u7b97\u5b50\u7684\u4ea4\u6362\u6027\u7b49\u76ee\u6807\u3002", "method": "\u5c06\u529f\u80fd\u6620\u5c04\u89c6\u4e3a2D\u56fe\u50cf\uff0c\u8bad\u7ec3\u56fe\u50cf\u6269\u6563\u6a21\u578b\u76f4\u63a5\u5728\u529f\u80fd\u7a7a\u95f4\u751f\u6210\u51c6\u786e\u6620\u5c04\uff0c\u63a8\u7406\u65f6\u5229\u7528\u70b9\u6620\u5c04\u4f5c\u4e3a\u6307\u5bfc\u3002", "result": "\u65b9\u6cd5\u5728\u529f\u80fd\u6620\u5c04\u4f18\u5316\u4efb\u52a1\u4e2d\u4e0e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u7ade\u4e89\uff0c\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u529f\u80fd\u6620\u5c04\u5904\u7406\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u529f\u80fd\u6620\u5c04\u4f18\u5316\u65b9\u6cd5\u9ad8\u6548\u4e14\u7075\u6d3b\uff0c\u4e3a\u529f\u80fd\u6620\u5c04\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2506.14399", "pdf": "https://arxiv.org/pdf/2506.14399", "abs": "https://arxiv.org/abs/2506.14399", "authors": ["Tian Xia", "Fabio De Sousa Ribeiro", "Rajat R Rasal", "Avinash Kori", "Raghav Mehta", "Ben Glocker"], "title": "Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Counterfactual image generation aims to simulate realistic visual outcomes under specific causal interventions. Diffusion models have recently emerged as a powerful tool for this task, combining DDIM inversion with conditional generation via classifier-free guidance (CFG). However, standard CFG applies a single global weight across all conditioning variables, which can lead to poor identity preservation and spurious attribute changes - a phenomenon known as attribute amplification. To address this, we propose Decoupled Classifier-Free Guidance (DCFG), a flexible and model-agnostic framework that introduces group-wise conditioning control. DCFG builds on an attribute-split embedding strategy that disentangles semantic inputs, enabling selective guidance on user-defined attribute groups. For counterfactual generation, we partition attributes into intervened and invariant sets based on a causal graph and apply distinct guidance to each. Experiments on CelebA-HQ, MIMIC-CXR, and EMBED show that DCFG improves intervention fidelity, mitigates unintended changes, and enhances reversibility, enabling more faithful and interpretable counterfactual image generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u7684\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\uff08DCFG\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u53cd\u4e8b\u5b9e\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5c5e\u6027\u63a7\u5236\u548c\u5e72\u9884\u4fdd\u771f\u5ea6\u3002", "motivation": "\u6807\u51c6\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\uff08CFG\uff09\u5728\u5168\u5c40\u6743\u91cd\u4e0b\u53ef\u80fd\u5bfc\u81f4\u8eab\u4efd\u4fdd\u7559\u4e0d\u4f73\u548c\u865a\u5047\u5c5e\u6027\u53d8\u5316\uff08\u5c5e\u6027\u653e\u5927\u73b0\u8c61\uff09\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7075\u6d3b\u7684\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDCFG\u6846\u67b6\uff0c\u901a\u8fc7\u5c5e\u6027\u5206\u5272\u5d4c\u5165\u7b56\u7565\u89e3\u8026\u8bed\u4e49\u8f93\u5165\uff0c\u5bf9\u7528\u6237\u5b9a\u4e49\u7684\u5c5e\u6027\u7ec4\u8fdb\u884c\u9009\u62e9\u6027\u5f15\u5bfc\u3002\u57fa\u4e8e\u56e0\u679c\u56fe\u5c06\u5c5e\u6027\u5206\u4e3a\u5e72\u9884\u7ec4\u548c\u4e0d\u53d8\u7ec4\uff0c\u5e76\u5206\u522b\u5e94\u7528\u4e0d\u540c\u7684\u5f15\u5bfc\u3002", "result": "\u5728CelebA-HQ\u3001MIMIC-CXR\u548cEMBED\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDCFG\u63d0\u9ad8\u4e86\u5e72\u9884\u4fdd\u771f\u5ea6\uff0c\u51cf\u5c11\u4e86\u610f\u5916\u53d8\u5316\uff0c\u5e76\u589e\u5f3a\u4e86\u53ef\u9006\u6027\u3002", "conclusion": "DCFG\u80fd\u591f\u5b9e\u73b0\u66f4\u5fe0\u5b9e\u548c\u53ef\u89e3\u91ca\u7684\u53cd\u4e8b\u5b9e\u56fe\u50cf\u751f\u6210\u3002"}}
{"id": "2506.14404", "pdf": "https://arxiv.org/pdf/2506.14404", "abs": "https://arxiv.org/abs/2506.14404", "authors": ["Nikos Spyrou", "Athanasios Vlontzos", "Paraskevas Pegios", "Thomas Melistas", "Nefeli Gkouti", "Yannis Panagakis", "Giorgos Papanastasiou", "Sotirios A. Tsaftaris"], "title": "Causally Steered Diffusion for Automated Video Counterfactual Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Adapting text-to-image (T2I) latent diffusion models for video editing has shown strong visual fidelity and controllability, but challenges remain in maintaining causal relationships in video content. Edits affecting causally dependent attributes risk generating unrealistic or misleading outcomes if these relationships are ignored. In this work, we propose a causally faithful framework for counterfactual video generation, guided by a vision-language model (VLM). Our method is agnostic to the underlying video editing system and does not require access to its internal mechanisms or finetuning. Instead, we guide the generation by optimizing text prompts based on an assumed causal graph, addressing the challenge of latent space control in LDMs. We evaluate our approach using standard video quality metrics and counterfactual-specific criteria, such as causal effectiveness and minimality. Our results demonstrate that causally faithful video counterfactuals can be effectively generated within the learned distribution of LDMs through prompt-based causal steering. With its compatibility with any black-box video editing system, our method holds significant potential for generating realistic \"what-if\" video scenarios in diverse areas such as healthcare and digital media.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u5173\u7cfb\u7684\u89c6\u9891\u7f16\u8f91\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u751f\u6210\u5fe0\u5b9e\u4e8e\u56e0\u679c\u5173\u7cfb\u7684\u53cd\u4e8b\u5b9e\u89c6\u9891\uff0c\u65e0\u9700\u4fee\u6539\u5e95\u5c42\u7f16\u8f91\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6269\u6563\u6a21\u578b\u5728\u89c6\u9891\u7f16\u8f91\u4e2d\u96be\u4ee5\u4fdd\u6301\u56e0\u679c\u5173\u7cfb\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e0d\u771f\u5b9e\u6216\u8bef\u5bfc\u6027\u7ed3\u679c\u3002", "method": "\u901a\u8fc7\u4f18\u5316\u57fa\u4e8e\u56e0\u679c\u56fe\u7684\u6587\u672c\u63d0\u793a\uff0c\u5f15\u5bfc\u751f\u6210\u8fc7\u7a0b\uff0c\u4e0d\u4f9d\u8d56\u7f16\u8f91\u7cfb\u7edf\u7684\u5185\u90e8\u673a\u5236\u6216\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u5fe0\u5b9e\u4e8e\u56e0\u679c\u5173\u7cfb\u7684\u53cd\u4e8b\u5b9e\u89c6\u9891\uff0c\u5e76\u901a\u8fc7\u6807\u51c6\u89c6\u9891\u8d28\u91cf\u6307\u6807\u548c\u56e0\u679c\u6709\u6548\u6027\u7b49\u6807\u51c6\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u517c\u5bb9\u4efb\u4f55\u9ed1\u76d2\u89c6\u9891\u7f16\u8f91\u7cfb\u7edf\uff0c\u5728\u533b\u7597\u548c\u6570\u5b57\u5a92\u4f53\u7b49\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.14541", "pdf": "https://arxiv.org/pdf/2506.14541", "abs": "https://arxiv.org/abs/2506.14541", "authors": ["Rongchang Lu", "Tianduo Luo", "Yunzhi Zhang", "Conghan Yue", "Pei Yang", "Guibao Liu", "Changyang Gu"], "title": "Exploring Diffusion with Test-Time Training on Efficient Image Restoration", "categories": ["cs.CV"], "comment": "Submitted to The 8th Chinese Conference on Pattern Recognition and Computer Vision (2025). Contact to nomodeset@qq.com. Source code will open in 4 months", "summary": "Image restoration faces challenges including ineffective feature fusion, computational bottlenecks and inefficient diffusion processes. To address these, we propose DiffRWKVIR, a novel framework unifying Test-Time Training (TTT) with efficient diffusion. Our approach introduces three key innovations: (1) Omni-Scale 2D State Evolution extends RWKV's location-dependent parameterization to hierarchical multi-directional 2D scanning, enabling global contextual awareness with linear complexity O(L); (2) Chunk-Optimized Flash Processing accelerates intra-chunk parallelism by 3.2x via contiguous chunk processing (O(LCd) complexity), reducing sequential dependencies and computational overhead; (3) Prior-Guided Efficient Diffusion extracts a compact Image Prior Representation (IPR) in only 5-20 steps, proving 45% faster training/inference than DiffIR while solving computational inefficiency in denoising. Evaluated across super-resolution and inpainting benchmarks (Set5, Set14, BSD100, Urban100, Places365), DiffRWKVIR outperforms SwinIR, HAT, and MambaIR/v2 in PSNR, SSIM, LPIPS, and efficiency metrics. Our method establishes a new paradigm for adaptive, high-efficiency image restoration with optimized hardware utilization.", "AI": {"tldr": "DiffRWKVIR\u662f\u4e00\u79cd\u7ed3\u5408\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u548c\u9ad8\u6548\u6269\u6563\u7684\u65b0\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u56fe\u50cf\u6062\u590d\u4e2d\u7684\u7279\u5f81\u878d\u5408\u3001\u8ba1\u7b97\u74f6\u9888\u548c\u6269\u6563\u6548\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u4e09\u9879\u521b\u65b0\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u56fe\u50cf\u6062\u590d\u4e2d\u5b58\u5728\u7279\u5f81\u878d\u5408\u4e0d\u9ad8\u6548\u3001\u8ba1\u7b97\u74f6\u9888\u548c\u6269\u6563\u8fc7\u7a0b\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51faDiffRWKVIR\u6846\u67b6\uff0c\u5305\u542b\u4e09\u9879\u521b\u65b0\uff1aOmni-Scale 2D State Evolution\u5b9e\u73b0\u5168\u5c40\u4e0a\u4e0b\u6587\u611f\u77e5\uff1bChunk-Optimized Flash Processing\u52a0\u901f\u5e76\u884c\u5904\u7406\uff1bPrior-Guided Efficient Diffusion\u63d0\u53d6\u7d27\u51d1\u56fe\u50cf\u5148\u9a8c\u8868\u793a\u3002", "result": "\u5728\u8d85\u5206\u8fa8\u7387\u548c\u4fee\u590d\u4efb\u52a1\u4e2d\uff0cDiffRWKVIR\u5728PSNR\u3001SSIM\u3001LPIPS\u548c\u6548\u7387\u6307\u6807\u4e0a\u4f18\u4e8eSwinIR\u3001HAT\u548cMambaIR/v2\u3002", "conclusion": "DiffRWKVIR\u4e3a\u9ad8\u6548\u81ea\u9002\u5e94\u7684\u56fe\u50cf\u6062\u590d\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u4f18\u5316\u4e86\u786c\u4ef6\u5229\u7528\u7387\u3002"}}
{"id": "2506.14549", "pdf": "https://arxiv.org/pdf/2506.14549", "abs": "https://arxiv.org/abs/2506.14549", "authors": ["Yong Liu", "Wenpeng Xiao", "Qianqian Wang", "Junlin Chen", "Shiyin Wang", "Yitong Wang", "Xinglong Wu", "Yansong Tang"], "title": "DreamLight: Towards Harmonious and Consistent Image Relighting", "categories": ["cs.CV"], "comment": null, "summary": "We introduce a model named DreamLight for universal image relighting in this work, which can seamlessly composite subjects into a new background while maintaining aesthetic uniformity in terms of lighting and color tone. The background can be specified by natural images (image-based relighting) or generated from unlimited text prompts (text-based relighting). Existing studies primarily focus on image-based relighting, while with scant exploration into text-based scenarios. Some works employ intricate disentanglement pipeline designs relying on environment maps to provide relevant information, which grapples with the expensive data cost required for intrinsic decomposition and light source. Other methods take this task as an image translation problem and perform pixel-level transformation with autoencoder architecture. While these methods have achieved decent harmonization effects, they struggle to generate realistic and natural light interaction effects between the foreground and background. To alleviate these challenges, we reorganize the input data into a unified format and leverage the semantic prior provided by the pretrained diffusion model to facilitate the generation of natural results. Moreover, we propose a Position-Guided Light Adapter (PGLA) that condenses light information from different directions in the background into designed light query embeddings, and modulates the foreground with direction-biased masked attention. In addition, we present a post-processing module named Spectral Foreground Fixer (SFF) to adaptively reorganize different frequency components of subject and relighted background, which helps enhance the consistency of foreground appearance. Extensive comparisons and user study demonstrate that our DreamLight achieves remarkable relighting performance.", "AI": {"tldr": "DreamLight\u662f\u4e00\u79cd\u901a\u7528\u7684\u56fe\u50cf\u91cd\u5149\u7167\u6a21\u578b\uff0c\u652f\u6301\u57fa\u4e8e\u56fe\u50cf\u548c\u6587\u672c\u7684\u91cd\u5149\u7167\uff0c\u901a\u8fc7\u7edf\u4e00\u8f93\u5165\u683c\u5f0f\u548c\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u8bed\u4e49\u5148\u9a8c\u751f\u6210\u81ea\u7136\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u57fa\u4e8e\u56fe\u50cf\u7684\u91cd\u5149\u7167\uff0c\u4e14\u4f9d\u8d56\u590d\u6742\u7684\u73af\u5883\u6620\u5c04\u6216\u50cf\u7d20\u7ea7\u8f6c\u6362\uff0c\u96be\u4ee5\u5b9e\u73b0\u524d\u666f\u4e0e\u80cc\u666f\u7684\u81ea\u7136\u5149\u7167\u4ea4\u4e92\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u8f93\u5165\u683c\u5f0f\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u8bed\u4e49\u5148\u9a8c\uff0c\u63d0\u51fa\u4f4d\u7f6e\u5f15\u5bfc\u5149\u9002\u914d\u5668\uff08PGLA\uff09\u548c\u9891\u8c31\u524d\u666f\u4fee\u590d\u5668\uff08SFF\uff09\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u548c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cDreamLight\u5728\u91cd\u5149\u7167\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "DreamLight\u901a\u8fc7\u521b\u65b0\u7684\u6a21\u5757\u8bbe\u8ba1\u548c\u6570\u636e\u91cd\u7ec4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u91cd\u5149\u7167\u7684\u81ea\u7136\u6027\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2506.14560", "pdf": "https://arxiv.org/pdf/2506.14560", "abs": "https://arxiv.org/abs/2506.14560", "authors": ["David Butler", "Adrian Hilton", "Gustavo Carneiro"], "title": "Risk Estimation of Knee Osteoarthritis Progression via Predictive Multi-task Modelling from Efficient Diffusion Model using X-ray Images", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Medical imaging plays a crucial role in assessing knee osteoarthritis (OA) risk by enabling early detection and disease monitoring. Recent machine learning methods have improved risk estimation (i.e., predicting the likelihood of disease progression) and predictive modelling (i.e., the forecasting of future outcomes based on current data) using medical images, but clinical adoption remains limited due to their lack of interpretability. Existing approaches that generate future images for risk estimation are complex and impractical. Additionally, previous methods fail to localize anatomical knee landmarks, limiting interpretability. We address these gaps with a new interpretable machine learning method to estimate the risk of knee OA progression via multi-task predictive modelling that classifies future knee OA severity and predicts anatomical knee landmarks from efficiently generated high-quality future images. Such image generation is achieved by leveraging a diffusion model in a class-conditioned latent space to forecast disease progression, offering a visual representation of how particular health conditions may evolve. Applied to the Osteoarthritis Initiative dataset, our approach improves the state-of-the-art (SOTA) by 2\\%, achieving an AUC of 0.71 in predicting knee OA progression while offering ~9% faster inference time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u9884\u6d4b\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u672a\u6765\u56fe\u50cf\uff0c\u7528\u4e8e\u819d\u5173\u8282\u9aa8\u5173\u8282\u708e\uff08OA\uff09\u8fdb\u5c55\u98ce\u9669\u8bc4\u4f30\uff0c\u540c\u65f6\u5b9a\u4f4d\u89e3\u5256\u6807\u5fd7\u70b9\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u819d\u5173\u8282OA\u98ce\u9669\u8bc4\u4f30\u4e2d\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u4e14\u751f\u6210\u672a\u6765\u56fe\u50cf\u7684\u590d\u6742\u6027\u9ad8\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u91c7\u7528\u3002", "method": "\u91c7\u7528\u6269\u6563\u6a21\u578b\u5728\u7c7b\u522b\u6761\u4ef6\u6f5c\u5728\u7a7a\u95f4\u4e2d\u751f\u6210\u672a\u6765\u56fe\u50cf\uff0c\u7ed3\u5408\u591a\u4efb\u52a1\u9884\u6d4b\u6a21\u578b\u5206\u7c7b\u672a\u6765OA\u4e25\u91cd\u7a0b\u5ea6\u5e76\u9884\u6d4b\u89e3\u5256\u6807\u5fd7\u70b9\u3002", "result": "\u5728Osteoarthritis Initiative\u6570\u636e\u96c6\u4e0a\uff0cAUC\u63d0\u53472%\u81f30.71\uff0c\u63a8\u7406\u65f6\u95f4\u7f29\u77ed\u7ea69%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6548\u7387\uff0c\u6709\u671b\u63a8\u52a8\u4e34\u5e8a\u91c7\u7528\u3002"}}
{"id": "2506.14603", "pdf": "https://arxiv.org/pdf/2506.14603", "abs": "https://arxiv.org/abs/2506.14603", "authors": ["Amirmojtaba Sabour", "Sanja Fidler", "Karsten Kreis"], "title": "Align Your Flow: Scaling Continuous-Time Flow Map Distillation", "categories": ["cs.CV", "cs.LG"], "comment": "Project page: https://research.nvidia.com/labs/toronto-ai/AlignYourFlow/", "summary": "Diffusion- and flow-based models have emerged as state-of-the-art generative modeling approaches, but they require many sampling steps. Consistency models can distill these models into efficient one-step generators; however, unlike flow- and diffusion-based methods, their performance inevitably degrades when increasing the number of steps, which we show both analytically and empirically. Flow maps generalize these approaches by connecting any two noise levels in a single step and remain effective across all step counts. In this paper, we introduce two new continuous-time objectives for training flow maps, along with additional novel training techniques, generalizing existing consistency and flow matching objectives. We further demonstrate that autoguidance can improve performance, using a low-quality model for guidance during distillation, and an additional boost can be achieved by adversarial finetuning, with minimal loss in sample diversity. We extensively validate our flow map models, called Align Your Flow, on challenging image generation benchmarks and achieve state-of-the-art few-step generation performance on both ImageNet 64x64 and 512x512, using small and efficient neural networks. Finally, we show text-to-image flow map models that outperform all existing non-adversarially trained few-step samplers in text-conditioned synthesis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAlign Your Flow\u7684\u6d41\u6620\u5c04\u6a21\u578b\uff0c\u901a\u8fc7\u65b0\u7684\u8fde\u7eed\u65f6\u95f4\u76ee\u6807\u548c\u8bad\u7ec3\u6280\u672f\uff0c\u6539\u8fdb\u4e86\u751f\u6210\u6a21\u578b\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u5e76\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6210\u679c\u3002", "motivation": "\u6269\u6563\u548c\u6d41\u6a21\u578b\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u9700\u8981\u591a\u6b65\u91c7\u6837\u3002\u4e00\u81f4\u6027\u6a21\u578b\u867d\u7136\u53ef\u4ee5\u4e00\u6b65\u751f\u6210\uff0c\u4f46\u6027\u80fd\u968f\u6b65\u6570\u589e\u52a0\u800c\u4e0b\u964d\u3002\u6d41\u6620\u5c04\u901a\u8fc7\u8fde\u63a5\u4efb\u610f\u4e24\u4e2a\u566a\u58f0\u7ea7\u522b\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u8fde\u7eed\u65f6\u95f4\u76ee\u6807\u8bad\u7ec3\u6d41\u6620\u5c04\uff0c\u7ed3\u5408\u81ea\u5f15\u5bfc\u548c\u5bf9\u6297\u5fae\u8c03\u6280\u672f\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5728ImageNet 64x64\u548c512x512\u4e0a\u5b9e\u73b0\u4e86\u9886\u5148\u7684\u5c11\u6b65\u751f\u6210\u6027\u80fd\uff0c\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u975e\u5bf9\u6297\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "\u6d41\u6620\u5c04\u6a21\u578b\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.14605", "pdf": "https://arxiv.org/pdf/2506.14605", "abs": "https://arxiv.org/abs/2506.14605", "authors": ["Giacomo Meanti", "Thomas Ryckeboer", "Michael Arbel", "Julien Mairal"], "title": "Unsupervised Imaging Inverse Problems with Diffusion Distribution Matching", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "Code available at https://github.com/inria-thoth/ddm4ip", "summary": "This work addresses image restoration tasks through the lens of inverse problems using unpaired datasets. In contrast to traditional approaches -- which typically assume full knowledge of the forward model or access to paired degraded and ground-truth images -- the proposed method operates under minimal assumptions and relies only on small, unpaired datasets. This makes it particularly well-suited for real-world scenarios, where the forward model is often unknown or misspecified, and collecting paired data is costly or infeasible. The method leverages conditional flow matching to model the distribution of degraded observations, while simultaneously learning the forward model via a distribution-matching loss that arises naturally from the framework. Empirically, it outperforms both single-image blind and unsupervised approaches on deblurring and non-uniform point spread function (PSF) calibration tasks. It also matches state-of-the-art performance on blind super-resolution. We also showcase the effectiveness of our method with a proof of concept for lens calibration: a real-world application traditionally requiring time-consuming experiments and specialized equipment. In contrast, our approach achieves this with minimal data acquisition effort.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u914d\u5bf9\u6570\u636e\u96c6\u7684\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u6761\u4ef6\u6d41\u5339\u914d\u548c\u5206\u5e03\u5339\u914d\u635f\u5931\u5b66\u4e60\u524d\u5411\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u573a\u666f\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5b8c\u6574\u524d\u5411\u6a21\u578b\u6216\u914d\u5bf9\u6570\u636e\u7684\u9650\u5236\uff0c\u9002\u7528\u4e8e\u524d\u5411\u6a21\u578b\u672a\u77e5\u6216\u914d\u5bf9\u6570\u636e\u96be\u4ee5\u83b7\u53d6\u7684\u573a\u666f\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u6d41\u5339\u914d\u5efa\u6a21\u9000\u5316\u89c2\u6d4b\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u5206\u5e03\u5339\u914d\u635f\u5931\u5b66\u4e60\u524d\u5411\u6a21\u578b\u3002", "result": "\u5728\u53bb\u6a21\u7cca\u548c\u975e\u5747\u5300\u70b9\u6269\u6563\u51fd\u6570\u6821\u51c6\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5355\u56fe\u50cf\u76f2\u65b9\u6cd5\u548c\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u5728\u76f2\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u8fbe\u5230\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u6570\u636e\u83b7\u53d6\u6210\u672c\u4f4e\uff0c\u9002\u7528\u4e8e\u4f20\u7edf\u8017\u65f6\u5b9e\u9a8c\u7684\u5e94\u7528\uff08\u5982\u955c\u5934\u6821\u51c6\uff09\u3002"}}
{"id": "2506.14642", "pdf": "https://arxiv.org/pdf/2506.14642", "abs": "https://arxiv.org/abs/2506.14642", "authors": ["Yuke Xing", "Jiarui Wang", "Peizhi Niu", "Wenjie Huang", "Guangtao Zhai", "Yiling Xu"], "title": "3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D Gaussian-Splatting", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has emerged as a promising approach for novel view synthesis, offering real-time rendering with high visual fidelity. However, its substantial storage requirements present significant challenges for practical applications. While recent state-of-the-art (SOTA) 3DGS methods increasingly incorporate dedicated compression modules, there is a lack of a comprehensive framework to evaluate their perceptual impact. Therefore we present 3DGS-IEval-15K, the first large-scale image quality assessment (IQA) dataset specifically designed for compressed 3DGS representations. Our dataset encompasses 15,200 images rendered from 10 real-world scenes through 6 representative 3DGS algorithms at 20 strategically selected viewpoints, with different compression levels leading to various distortion effects. Through controlled subjective experiments, we collect human perception data from 60 viewers. We validate dataset quality through scene diversity and MOS distribution analysis, and establish a comprehensive benchmark with 30 representative IQA metrics covering diverse types. As the largest-scale 3DGS quality assessment dataset to date, our work provides a foundation for developing 3DGS specialized IQA metrics, and offers essential data for investigating view-dependent quality distribution patterns unique to 3DGS. The database is publicly available at https://github.com/YukeXing/3DGS-IEval-15K.", "AI": {"tldr": "3DGS-IEval-15K\u662f\u9996\u4e2a\u9488\u5bf9\u538b\u7f293D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u8868\u793a\u7684\u5927\u89c4\u6a21\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u5305\u542b15,200\u5f20\u56fe\u50cf\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e0d\u540c\u538b\u7f29\u7b97\u6cd5\u7684\u611f\u77e5\u5f71\u54cd\u3002", "motivation": "3DGS\u5728\u5b9e\u65f6\u6e32\u67d3\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u9ad8\u5b58\u50a8\u9700\u6c42\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002\u76ee\u524d\u7f3a\u4e4f\u8bc4\u4f30\u538b\u7f29\u7b97\u6cd5\u611f\u77e5\u5f71\u54cd\u7684\u7efc\u5408\u6846\u67b6\u3002", "method": "\u901a\u8fc76\u79cd\u4ee3\u8868\u60273DGS\u7b97\u6cd5\u572810\u4e2a\u771f\u5b9e\u573a\u666f\u4e2d\u6e32\u67d3\u56fe\u50cf\uff0c\u8986\u76d620\u4e2a\u89c6\u89d2\u548c\u4e0d\u540c\u538b\u7f29\u7ea7\u522b\uff0c\u6536\u96c660\u540d\u89c2\u4f17\u7684\u4e3b\u89c2\u8bc4\u4ef7\u6570\u636e\u3002", "result": "\u6570\u636e\u96c6\u901a\u8fc7\u573a\u666f\u591a\u6837\u6027\u548cMOS\u5206\u5e03\u5206\u6790\u9a8c\u8bc1\u8d28\u91cf\uff0c\u5e76\u5efa\u7acb\u4e8630\u79cdIQA\u6307\u6807\u7684\u57fa\u51c6\u3002", "conclusion": "3DGS-IEval-15K\u4e3a\u5f00\u53d13DGS\u4e13\u7528IQA\u6307\u6807\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5e76\u652f\u6301\u7814\u7a763DGS\u7279\u6709\u7684\u89c6\u89d2\u4f9d\u8d56\u8d28\u91cf\u5206\u5e03\u6a21\u5f0f\u3002"}}
{"id": "2506.14706", "pdf": "https://arxiv.org/pdf/2506.14706", "abs": "https://arxiv.org/abs/2506.14706", "authors": ["Ni Ou", "Zhuo Chen", "Xinru Zhang", "Junzheng Wang"], "title": "Iterative Camera-LiDAR Extrinsic Optimization via Surrogate Diffusion", "categories": ["cs.CV"], "comment": "7 pages, 4 figures, accepted by IROS 2025", "summary": "Cameras and LiDAR are essential sensors for autonomous vehicles. The fusion of camera and LiDAR data addresses the limitations of individual sensors but relies on precise extrinsic calibration. Recently, numerous end-to-end calibration methods have been proposed; however, most predict extrinsic parameters in a single step and lack iterative optimization capabilities. To address the increasing demand for higher accuracy, we propose a versatile iterative framework based on surrogate diffusion. This framework can enhance the performance of any calibration method without requiring architectural modifications. Specifically, the initial extrinsic parameters undergo iterative refinement through a denoising process, in which the original calibration method serves as a surrogate denoiser to estimate the final extrinsics at each step. For comparative analysis, we selected four state-of-the-art calibration methods as surrogate denoisers and compared the results of our diffusion process with those of two other iterative approaches. Extensive experiments demonstrate that when integrated with our diffusion model, all calibration methods achieve higher accuracy, improved robustness, and greater stability compared to other iterative techniques and their single-step counterparts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u66ff\u4ee3\u6269\u6563\u7684\u8fed\u4ee3\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u76f8\u673a\u548cLiDAR\u5916\u53c2\u6807\u5b9a\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u65e0\u9700\u4fee\u6539\u67b6\u6784\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u6807\u5b9a\u65b9\u6cd5\u591a\u4e3a\u5355\u6b65\u9884\u6d4b\uff0c\u7f3a\u4e4f\u8fed\u4ee3\u4f18\u5316\u80fd\u529b\uff0c\u96be\u4ee5\u6ee1\u8db3\u66f4\u9ad8\u7cbe\u5ea6\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u66ff\u4ee3\u6269\u6563\u6846\u67b6\u5bf9\u521d\u59cb\u5916\u53c2\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\uff0c\u539f\u59cb\u6807\u5b9a\u65b9\u6cd5\u4f5c\u4e3a\u66ff\u4ee3\u53bb\u566a\u5668\u9010\u6b65\u4f30\u8ba1\u6700\u7ec8\u5916\u53c2\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u56db\u79cd\u5148\u8fdb\u6807\u5b9a\u65b9\u6cd5\u7684\u7cbe\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6269\u6563\u6a21\u578b\u4e3a\u5916\u53c2\u6807\u5b9a\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u8fed\u4ee3\u4f18\u5316\u65b9\u6848\uff0c\u4f18\u4e8e\u5176\u4ed6\u8fed\u4ee3\u65b9\u6cd5\u548c\u5355\u6b65\u65b9\u6cd5\u3002"}}
{"id": "2506.14742", "pdf": "https://arxiv.org/pdf/2506.14742", "abs": "https://arxiv.org/abs/2506.14742", "authors": ["Ziqiao Peng", "Wentao Hu", "Junyuan Ma", "Xiangyu Zhu", "Xiaomei Zhang", "Hao Zhao", "Hui Tian", "Jun He", "Hongyan Liu", "Zhaoxin Fan"], "title": "SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads Synthesis Using Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Achieving high synchronization in the synthesis of realistic, speech-driven talking head videos presents a significant challenge. A lifelike talking head requires synchronized coordination of subject identity, lip movements, facial expressions, and head poses. The absence of these synchronizations is a fundamental flaw, leading to unrealistic results. To address the critical issue of synchronization, identified as the ''devil'' in creating realistic talking heads, we introduce SyncTalk++, which features a Dynamic Portrait Renderer with Gaussian Splatting to ensure consistent subject identity preservation and a Face-Sync Controller that aligns lip movements with speech while innovatively using a 3D facial blendshape model to reconstruct accurate facial expressions. To ensure natural head movements, we propose a Head-Sync Stabilizer, which optimizes head poses for greater stability. Additionally, SyncTalk++ enhances robustness to out-of-distribution (OOD) audio by incorporating an Expression Generator and a Torso Restorer, which generate speech-matched facial expressions and seamless torso regions. Our approach maintains consistency and continuity in visual details across frames and significantly improves rendering speed and quality, achieving up to 101 frames per second. Extensive experiments and user studies demonstrate that SyncTalk++ outperforms state-of-the-art methods in synchronization and realism. We recommend watching the supplementary video: https://ziqiaopeng.github.io/synctalk++.", "AI": {"tldr": "SyncTalk++\u901a\u8fc7\u52a8\u6001\u8096\u50cf\u6e32\u67d3\u5668\u3001\u9762\u90e8\u540c\u6b65\u63a7\u5236\u5668\u548c\u5934\u90e8\u540c\u6b65\u7a33\u5b9a\u5668\u89e3\u51b3\u4e86\u8bed\u97f3\u9a71\u52a8\u89c6\u9891\u4e2d\u7684\u540c\u6b65\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u771f\u5b9e\u611f\u548c\u6e32\u67d3\u901f\u5ea6\u3002", "motivation": "\u8bed\u97f3\u9a71\u52a8\u89c6\u9891\u4e2d\u8eab\u4efd\u3001\u5507\u52a8\u3001\u8868\u60c5\u548c\u5934\u90e8\u59ff\u52bf\u7684\u540c\u6b65\u662f\u5b9e\u73b0\u771f\u5b9e\u611f\u7684\u5173\u952e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u540c\u6b65\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u9ad8\u65af\u6563\u5c04\u7684\u52a8\u6001\u8096\u50cf\u6e32\u67d3\u5668\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u9762\u90e8\u540c\u6b65\u63a7\u5236\u5668\u5bf9\u9f50\u5507\u52a8\u548c\u8868\u60c5\uff0c\u5934\u90e8\u540c\u6b65\u7a33\u5b9a\u5668\u4f18\u5316\u5934\u90e8\u59ff\u52bf\uff0c\u5e76\u5f15\u5165\u8868\u60c5\u751f\u6210\u5668\u548c\u8eaf\u5e72\u4fee\u590d\u5668\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "SyncTalk++\u5728\u540c\u6b65\u6027\u548c\u771f\u5b9e\u611f\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6e32\u67d3\u901f\u5ea6\u8fbe101\u5e27/\u79d2\u3002", "conclusion": "SyncTalk++\u901a\u8fc7\u591a\u6a21\u5757\u534f\u540c\u89e3\u51b3\u4e86\u540c\u6b65\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u9a71\u52a8\u89c6\u9891\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2506.14753", "pdf": "https://arxiv.org/pdf/2506.14753", "abs": "https://arxiv.org/abs/2506.14753", "authors": ["Qinchan", "Li", "Kenneth Chen", "Changyue", "Su", "Wittawat Jitkrittum", "Qi Sun", "Patsorn Sangkloy"], "title": "Cost-Aware Routing for Efficient Text-To-Image Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Diffusion models are well known for their ability to generate a high-fidelity image for an input prompt through an iterative denoising process. Unfortunately, the high fidelity also comes at a high computational cost due the inherently sequential generative process. In this work, we seek to optimally balance quality and computational cost, and propose a framework to allow the amount of computation to vary for each prompt, depending on its complexity. Each prompt is automatically routed to the most appropriate text-to-image generation function, which may correspond to a distinct number of denoising steps of a diffusion model, or a disparate, independent text-to-image model. Unlike uniform cost reduction techniques (e.g., distillation, model quantization), our approach achieves the optimal trade-off by learning to reserve expensive choices (e.g., 100+ denoising steps) only for a few complex prompts, and employ more economical choices (e.g., small distilled model) for less sophisticated prompts. We empirically demonstrate on COCO and DiffusionDB that by learning to route to nine already-trained text-to-image models, our approach is able to deliver an average quality that is higher than that achievable by any of these models alone.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8def\u7531\u7684\u6846\u67b6\uff0c\u6839\u636e\u63d0\u793a\u7684\u590d\u6742\u5ea6\u52a8\u6001\u9009\u62e9\u6700\u9002\u5408\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u4ee5\u5e73\u8861\u8ba1\u7b97\u6210\u672c\u4e0e\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6839\u636e\u63d0\u793a\u590d\u6742\u5ea6\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u8d44\u6e90\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u8def\u7531\u673a\u5236\uff0c\u5c06\u63d0\u793a\u5206\u914d\u5230\u4e0d\u540c\u7684\u751f\u6210\u51fd\u6570\uff08\u5982\u4e0d\u540c\u6b65\u6570\u7684\u6269\u6563\u6a21\u578b\u6216\u5176\u4ed6\u72ec\u7acb\u6a21\u578b\uff09\uff0c\u4ee5\u4f18\u5316\u8ba1\u7b97\u4e0e\u8d28\u91cf\u7684\u5e73\u8861\u3002", "result": "\u5728COCO\u548cDiffusionDB\u6570\u636e\u96c6\u4e0a\uff0c\u8def\u7531\u5230\u4e5d\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u65b9\u6cd5\u5e73\u5747\u8d28\u91cf\u9ad8\u4e8e\u5355\u72ec\u4f7f\u7528\u4efb\u4e00\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5b66\u4e60\u52a8\u6001\u8def\u7531\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6210\u672c\u4e0e\u751f\u6210\u8d28\u91cf\u7684\u6700\u4f18\u6743\u8861\u3002"}}
{"id": "2506.14769", "pdf": "https://arxiv.org/pdf/2506.14769", "abs": "https://arxiv.org/abs/2506.14769", "authors": ["Jiahua Ma", "Yiran Qin", "Yixiong Li", "Xuanqi Liao", "Yulan Guo", "Ruimao Zhang"], "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal Diffusion", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating expert demonstrations through action diffusion. However, in practical applications, hardware limitations often degrade data quality, while real-time constraints restrict model inference to instantaneous state and scene observations. These limitations seriously reduce the efficacy of learning from expert demonstrations, resulting in failures in object localization, grasp planning, and long-horizon task execution. To address these challenges, we propose Causal Diffusion Policy (CDP), a novel transformer-based diffusion model that enhances action prediction by conditioning on historical action sequences, thereby enabling more coherent and context-aware visuomotor policy learning. To further mitigate the computational cost associated with autoregressive inference, a caching mechanism is also introduced to store attention key-value pairs from previous timesteps, substantially reducing redundant computations during execution. Extensive experiments in both simulated and real-world environments, spanning diverse 2D and 3D manipulation tasks, demonstrate that CDP uniquely leverages historical action sequences to achieve significantly higher accuracy than existing methods. Moreover, even when faced with degraded input observation quality, CDP maintains remarkable precision by reasoning through temporal continuity, which highlights its practical robustness for robotic control under realistic, imperfect conditions.", "AI": {"tldr": "CDP\u901a\u8fc7\u7ed3\u5408\u5386\u53f2\u52a8\u4f5c\u5e8f\u5217\u548c\u7f13\u5b58\u673a\u5236\uff0c\u63d0\u5347\u4e86\u6269\u6563\u7b56\u7565\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u786c\u4ef6\u9650\u5236\u548c\u5b9e\u65f6\u7ea6\u675f\u5bfc\u81f4\u4e13\u5bb6\u793a\u8303\u6570\u636e\u8d28\u91cf\u4e0b\u964d\uff0c\u5f71\u54cd\u673a\u5668\u4eba\u884c\u4e3a\u5b66\u4e60\u6548\u679c\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eTransformer\u7684\u56e0\u679c\u6269\u6563\u7b56\u7565\uff08CDP\uff09\uff0c\u5229\u7528\u5386\u53f2\u52a8\u4f5c\u5e8f\u5217\u548c\u7f13\u5b58\u673a\u5236\u4f18\u5316\u52a8\u4f5c\u9884\u6d4b\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\uff0cCDP\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u8f93\u5165\u8d28\u91cf\u4e0b\u964d\u65f6\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "CDP\u901a\u8fc7\u65f6\u95f4\u8fde\u7eed\u6027\u63a8\u7406\uff0c\u4e3a\u4e0d\u5b8c\u7f8e\u6761\u4ef6\u4e0b\u7684\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.14135", "pdf": "https://arxiv.org/pdf/2506.14135", "abs": "https://arxiv.org/abs/2506.14135", "authors": ["Ying Chai", "Litao Deng", "Ruizhi Shao", "Jiajun Zhang", "Liangjun Xing", "Hongwen Zhang", "Yebin Liu"], "title": "GAF: Gaussian Action Field as a Dvnamic World Model for Robotic Mlanipulation", "categories": ["cs.RO", "cs.CV"], "comment": "http://chaiying1.github.io/GAF.github.io/project_page/", "summary": "Accurate action inference is critical for vision-based robotic manipulation. Existing approaches typically follow either a Vision-to-Action (V-A) paradigm, predicting actions directly from visual inputs, or a Vision-to-3D-to-Action (V-3D-A) paradigm, leveraging intermediate 3D representations. However, these methods often struggle with action inaccuracies due to the complexity and dynamic nature of manipulation scenes. In this paper, we propose a V-4D-A framework that enables direct action reasoning from motion-aware 4D representations via a Gaussian Action Field (GAF). GAF extends 3D Gaussian Splatting (3DGS) by incorporating learnable motion attributes, allowing simultaneous modeling of dynamic scenes and manipulation actions. To learn time-varying scene geometry and action-aware robot motion, GAF supports three key query types: reconstruction of the current scene, prediction of future frames, and estimation of initial action via robot motion. Furthermore, the high-quality current and future frames generated by GAF facilitate manipulation action refinement through a GAF-guided diffusion model. Extensive experiments demonstrate significant improvements, with GAF achieving +11.5385 dB PSNR and -0.5574 LPIPS improvements in reconstruction quality, while boosting the average success rate in robotic manipulation tasks by 10.33% over state-of-the-art methods. Project page: http://chaiying1.github.io/GAF.github.io/project_page/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cdV-4D-A\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u65af\u52a8\u4f5c\u573a\uff08GAF\uff09\u76f4\u63a5\u4ece\u8fd0\u52a8\u611f\u77e5\u76844D\u8868\u793a\u4e2d\u8fdb\u884c\u52a8\u4f5c\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08V-A\u6216V-3D-A\uff09\u5728\u590d\u6742\u52a8\u6001\u573a\u666f\u4e2d\u52a8\u4f5c\u63a8\u7406\u4e0d\u51c6\u786e\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6846\u67b6\u3002", "method": "GAF\u6269\u5c55\u4e863D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\uff0c\u5f15\u5165\u53ef\u5b66\u4e60\u8fd0\u52a8\u5c5e\u6027\uff0c\u652f\u6301\u573a\u666f\u91cd\u5efa\u3001\u672a\u6765\u5e27\u9884\u6d4b\u548c\u521d\u59cb\u52a8\u4f5c\u4f30\u8ba1\uff0c\u5e76\u901a\u8fc7GAF\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\u4f18\u5316\u52a8\u4f5c\u3002", "result": "GAF\u5728\u91cd\u5efa\u8d28\u91cf\u4e0a\u63d0\u534711.5385 dB PSNR\u548c-0.5574 LPIPS\uff0c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad810.33%\u3002", "conclusion": "V-4D-A\u6846\u67b6\u901a\u8fc7GAF\u663e\u8457\u63d0\u5347\u4e86\u52a8\u4f5c\u63a8\u7406\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6027\u80fd\u3002"}}
{"id": "2506.14198", "pdf": "https://arxiv.org/pdf/2506.14198", "abs": "https://arxiv.org/abs/2506.14198", "authors": ["Jeremy A. Collins", "Lor\u00e1nd Cheng", "Kunal Aneja", "Albert Wilcox", "Benjamin Joffe", "Animesh Garg"], "title": "AMPLIFY: Actionless Motion Priors for Robot Learning from Videos", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Action-labeled data for robotics is scarce and expensive, limiting the generalization of learned policies. In contrast, vast amounts of action-free video data are readily available, but translating these observations into effective policies remains a challenge. We introduce AMPLIFY, a novel framework that leverages large-scale video data by encoding visual dynamics into compact, discrete motion tokens derived from keypoint trajectories. Our modular approach separates visual motion prediction from action inference, decoupling the challenges of learning what motion defines a task from how robots can perform it. We train a forward dynamics model on abundant action-free videos and an inverse dynamics model on a limited set of action-labeled examples, allowing for independent scaling. Extensive evaluations demonstrate that the learned dynamics are both accurate, achieving up to 3.7x better MSE and over 2.5x better pixel prediction accuracy compared to prior approaches, and broadly useful. In downstream policy learning, our dynamics predictions enable a 1.2-2.2x improvement in low-data regimes, a 1.4x average improvement by learning from action-free human videos, and the first generalization to LIBERO tasks from zero in-distribution action data. Beyond robotic control, we find the dynamics learned by AMPLIFY to be a versatile latent world model, enhancing video prediction quality. Our results present a novel paradigm leveraging heterogeneous data sources to build efficient, generalizable world models. More information can be found at https://amplify-robotics.github.io/.", "AI": {"tldr": "AMPLIFY\u6846\u67b6\u5229\u7528\u5927\u89c4\u6a21\u65e0\u52a8\u4f5c\u89c6\u9891\u6570\u636e\uff0c\u901a\u8fc7\u5173\u952e\u70b9\u8f68\u8ff9\u751f\u6210\u7d27\u51d1\u7684\u8fd0\u52a8\u6807\u8bb0\uff0c\u5206\u79bb\u89c6\u89c9\u8fd0\u52a8\u9884\u6d4b\u4e0e\u52a8\u4f5c\u63a8\u65ad\uff0c\u663e\u8457\u63d0\u5347\u4f4e\u6570\u636e\u91cf\u4e0b\u7684\u7b56\u7565\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u673a\u5668\u4eba\u52a8\u4f5c\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u4e14\u6602\u8d35\uff0c\u800c\u65e0\u52a8\u4f5c\u89c6\u9891\u6570\u636e\u4e30\u5bcc\u4f46\u96be\u4ee5\u8f6c\u5316\u4e3a\u6709\u6548\u7b56\u7565\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5229\u7528\u8fd9\u4e9b\u6570\u636e\u63d0\u5347\u7b56\u7565\u6cdb\u5316\u80fd\u529b\u3002", "method": "AMPLIFY\u901a\u8fc7\u5173\u952e\u70b9\u8f68\u8ff9\u751f\u6210\u8fd0\u52a8\u6807\u8bb0\uff0c\u5206\u79bb\u89c6\u89c9\u8fd0\u52a8\u9884\u6d4b\uff08\u4f7f\u7528\u5927\u91cf\u65e0\u52a8\u4f5c\u89c6\u9891\u8bad\u7ec3\u524d\u5411\u52a8\u529b\u5b66\u6a21\u578b\uff09\u4e0e\u52a8\u4f5c\u63a8\u65ad\uff08\u4f7f\u7528\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u8bad\u7ec3\u9006\u5411\u52a8\u529b\u5b66\u6a21\u578b\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cAMPLIFY\u5728MSE\u548c\u50cf\u7d20\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u5206\u522b\u63d0\u53473.7\u500d\u548c2.5\u500d\uff0c\u7b56\u7565\u5b66\u4e60\u5728\u4f4e\u6570\u636e\u91cf\u4e0b\u63d0\u53471.2-2.2\u500d\uff0c\u5e76\u80fd\u4ece\u65e0\u52a8\u4f5c\u4eba\u7c7b\u89c6\u9891\u4e2d\u5b66\u4e60\u3002", "conclusion": "AMPLIFY\u63d0\u4f9b\u4e86\u4e00\u79cd\u5229\u7528\u5f02\u6784\u6570\u636e\u6784\u5efa\u9ad8\u6548\u3001\u53ef\u6cdb\u5316\u4e16\u754c\u6a21\u578b\u7684\u65b0\u8303\u5f0f\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u63a7\u5236\u53ca\u5176\u4ed6\u9886\u57df\u3002"}}
{"id": "2506.14209", "pdf": "https://arxiv.org/pdf/2506.14209", "abs": "https://arxiv.org/abs/2506.14209", "authors": ["Pengwei Wang"], "title": "Latent Anomaly Detection: Masked VQ-GAN for Unsupervised Segmentation in Medical CBCT", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Advances in treatment technology now allow for the use of customizable 3D-printed hydrogel wound dressings for patients with osteoradionecrosis (ORN) of the jaw (ONJ). Meanwhile, deep learning has enabled precise segmentation of 3D medical images using tools like nnUNet.   However, the scarcity of labeled data in ONJ imaging makes supervised training impractical. This study aims to develop an unsupervised training approach for automatically identifying anomalies in imaging scans.   We propose a novel two-stage training pipeline. In the first stage, a VQ-GAN is trained to accurately reconstruct normal subjects. In the second stage, random cube masking and ONJ-specific masking are applied to train a new encoder capable of recovering the data.   The proposed method achieves successful segmentation on both simulated and real patient data.   This approach provides a fast initial segmentation solution, reducing the burden of manual labeling. Additionally, it has the potential to be directly used for 3D printing when combined with hand-tuned post-processing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u8bc6\u522bONJ\u5f71\u50cf\u4e2d\u7684\u5f02\u5e38\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u5b9e\u73b0\uff0c\u51cf\u5c11\u624b\u52a8\u6807\u6ce8\u8d1f\u62c5\u3002", "motivation": "\u7531\u4e8eONJ\u5f71\u50cf\u4e2d\u6807\u8bb0\u6570\u636e\u7684\u7a00\u7f3a\u6027\uff0c\u76d1\u7763\u8bad\u7ec3\u4e0d\u5207\u5b9e\u9645\uff0c\u9700\u8981\u5f00\u53d1\u65e0\u76d1\u7763\u65b9\u6cd5\u3002", "method": "\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1a1) \u4f7f\u7528VQ-GAN\u91cd\u5efa\u6b63\u5e38\u6837\u672c\uff1b2) \u5e94\u7528\u968f\u673a\u548cONJ\u7279\u5b9a\u63a9\u7801\u8bad\u7ec3\u65b0\u7f16\u7801\u5668\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u60a3\u8005\u6570\u636e\u4e0a\u6210\u529f\u5b9e\u73b0\u5206\u5272\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u5feb\u901f\u521d\u59cb\u5206\u5272\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u6709\u671b\u76f4\u63a5\u7528\u4e8e3D\u6253\u5370\u3002"}}
{"id": "2506.14303", "pdf": "https://arxiv.org/pdf/2506.14303", "abs": "https://arxiv.org/abs/2506.14303", "authors": ["Niran Nataraj", "Maina Sogabe", "Kenji Kawashima"], "title": "orGAN: A Synthetic Data Augmentation Pipeline for Simultaneous Generation of Surgical Images and Ground Truth Labels", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "24 pages, 7figures", "summary": "Deep learning in medical imaging faces obstacles: limited data diversity, ethical issues, high acquisition costs, and the need for precise annotations. Bleeding detection and localization during surgery is especially challenging due to the scarcity of high-quality datasets that reflect real surgical scenarios. We propose orGAN, a GAN-based system for generating high-fidelity, annotated surgical images of bleeding. By leveraging small \"mimicking organ\" datasets, synthetic models that replicate tissue properties and bleeding, our approach reduces ethical concerns and data-collection costs. orGAN builds on StyleGAN with Relational Positional Learning to simulate bleeding events realistically and mark bleeding coordinates. A LaMa-based inpainting module then restores clean, pre-bleed visuals, enabling precise pixel-level annotations. In evaluations, a balanced dataset of orGAN and mimicking-organ images achieved 90% detection accuracy in surgical settings and up to 99% frame-level accuracy. While our development data lack diverse organ morphologies and contain intraoperative artifacts, orGAN markedly advances ethical, efficient, and cost-effective creation of realistic annotated bleeding datasets, supporting broader integration of AI in surgical practice.", "AI": {"tldr": "orGAN\u662f\u4e00\u79cd\u57fa\u4e8eGAN\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u5e26\u6ce8\u91ca\u7684\u624b\u672f\u51fa\u8840\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u533b\u5b66\u5f71\u50cf\u4e2d\u6570\u636e\u591a\u6837\u6027\u4e0d\u8db3\u3001\u4f26\u7406\u95ee\u9898\u548c\u6210\u672c\u9ad8\u7684\u6311\u6218\u3002", "motivation": "\u624b\u672f\u4e2d\u51fa\u8840\u68c0\u6d4b\u548c\u5b9a\u4f4d\u56e0\u9ad8\u8d28\u91cf\u6570\u636e\u7a00\u7f3a\u800c\u56f0\u96be\uff0c\u4f20\u7edf\u65b9\u6cd5\u9762\u4e34\u4f26\u7406\u548c\u6210\u672c\u95ee\u9898\u3002", "method": "\u5229\u7528\u5c0f\u578b\u201c\u6a21\u62df\u5668\u5b98\u201d\u6570\u636e\u96c6\u548cStyleGAN\u7ed3\u5408\u5173\u7cfb\u4f4d\u7f6e\u5b66\u4e60\uff0c\u751f\u6210\u903c\u771f\u51fa\u8840\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7LaMa\u4fee\u590d\u6a21\u5757\u63d0\u4f9b\u7cbe\u786e\u6ce8\u91ca\u3002", "result": "\u8bc4\u4f30\u4e2d\uff0corGAN\u751f\u6210\u7684\u56fe\u50cf\u5728\u624b\u672f\u573a\u666f\u4e2d\u8fbe\u523090%\u68c0\u6d4b\u51c6\u786e\u7387\u548c99%\u5e27\u7ea7\u51c6\u786e\u7387\u3002", "conclusion": "orGAN\u663e\u8457\u63d0\u5347\u4e86\u4f26\u7406\u3001\u9ad8\u6548\u4e14\u4f4e\u6210\u672c\u751f\u6210\u771f\u5b9e\u51fa\u8840\u6570\u636e\u96c6\u7684\u80fd\u529b\uff0c\u652f\u6301AI\u5728\u624b\u672f\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2506.14381", "pdf": "https://arxiv.org/pdf/2506.14381", "abs": "https://arxiv.org/abs/2506.14381", "authors": ["Yuxuan Jiang", "Siyue Teng", "Qiang Zhu", "Chen Feng", "Chengxi Zeng", "Fan Zhang", "Shuyuan Zhu", "Bing Zeng", "David Bull"], "title": "Compressed Video Super-Resolution based on Hierarchical Encoding", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "This paper presents a general-purpose video super-resolution (VSR) method, dubbed VSR-HE, specifically designed to enhance the perceptual quality of compressed content. Targeting scenarios characterized by heavy compression, the method upscales low-resolution videos by a ratio of four, from 180p to 720p or from 270p to 1080p. VSR-HE adopts hierarchical encoding transformer blocks and has been sophisticatedly optimized to eliminate a wide range of compression artifacts commonly introduced by H.265/HEVC encoding across various quantization parameter (QP) levels. To ensure robustness and generalization, the model is trained and evaluated under diverse compression settings, allowing it to effectively restore fine-grained details and preserve visual fidelity. The proposed VSR-HE has been officially submitted to the ICME 2025 Grand Challenge on VSR for Video Conferencing (Team BVI-VSR), under both the Track 1 (General-Purpose Real-World Video Content) and Track 2 (Talking Head Videos).", "AI": {"tldr": "VSR-HE\u662f\u4e00\u79cd\u901a\u7528\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u63d0\u5347\u538b\u7f29\u5185\u5bb9\u7684\u611f\u77e5\u8d28\u91cf\uff0c\u901a\u8fc7\u5206\u5c42\u7f16\u7801\u53d8\u6362\u5757\u6d88\u9664\u538b\u7f29\u4f2a\u5f71\uff0c\u5e76\u5728\u591a\u79cd\u538b\u7f29\u8bbe\u7f6e\u4e0b\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "motivation": "\u9488\u5bf9\u9ad8\u538b\u7f29\u573a\u666f\uff0c\u63d0\u5347\u4f4e\u5206\u8fa8\u7387\u89c6\u9891\u7684\u611f\u77e5\u8d28\u91cf\uff0c\u6d88\u9664H.265/HEVC\u7f16\u7801\u5f15\u5165\u7684\u538b\u7f29\u4f2a\u5f71\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7f16\u7801\u53d8\u6362\u5757\uff0c\u5bf9\u4f4e\u5206\u8fa8\u7387\u89c6\u9891\u8fdb\u884c\u56db\u500d\u4e0a\u91c7\u6837\uff08\u5982180p\u5230720p\uff09\uff0c\u5e76\u5728\u591a\u79cd\u91cf\u5316\u53c2\u6570\u4e0b\u4f18\u5316\u3002", "result": "\u6a21\u578b\u80fd\u591f\u6709\u6548\u6062\u590d\u7ec6\u7c92\u5ea6\u7ec6\u8282\u5e76\u4fdd\u6301\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u9002\u7528\u4e8e\u901a\u7528\u89c6\u9891\u5185\u5bb9\u548c\u8bf4\u8bdd\u4eba\u89c6\u9891\u3002", "conclusion": "VSR-HE\u5728\u538b\u7f29\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5df2\u63d0\u4ea4\u81f3ICME 2025\u6311\u6218\u8d5b\u3002"}}
{"id": "2506.14390", "pdf": "https://arxiv.org/pdf/2506.14390", "abs": "https://arxiv.org/abs/2506.14390", "authors": ["Conrad Orglmeister", "Erik Bochinski", "Volker Eiselein", "Elvira Fleig"], "title": "Enclosing Prototypical Variational Autoencoder for Explainable Out-of-Distribution Detection", "categories": ["cs.LG", "cs.CV"], "comment": "This preprint has not undergone peer review or any post-submission improvements or corrections. The Version of Record of this contribution is published in Computer Safety, Reliability and Security - SAFECOMP 2024 Workshops - DECSoS, SASSUR, TOASTS, and WAISE, and is available online at https://doi.org/10.1007/978-3-031-68738-9_29", "summary": "Understanding the decision-making and trusting the reliability of Deep Machine Learning Models is crucial for adopting such methods to safety-relevant applications. We extend self-explainable Prototypical Variational models with autoencoder-based out-of-distribution (OOD) detection: A Variational Autoencoder is applied to learn a meaningful latent space which can be used for distance-based classification, likelihood estimation for OOD detection, and reconstruction. The In-Distribution (ID) region is defined by a Gaussian mixture distribution with learned prototypes representing the center of each mode. Furthermore, a novel restriction loss is introduced that promotes a compact ID region in the latent space without collapsing it into single points. The reconstructive capabilities of the Autoencoder ensure the explainability of the prototypes and the ID region of the classifier, further aiding the discrimination of OOD samples. Extensive evaluations on common OOD detection benchmarks as well as a large-scale dataset from a real-world railway application demonstrate the usefulness of the approach, outperforming previous methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u89e3\u91ca\u539f\u578b\u53d8\u5206\u6a21\u578b\u4e0e\u81ea\u7f16\u7801\u5668\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u6f5c\u5728\u7a7a\u95f4\uff0c\u7528\u4e8e\u5206\u7c7b\u3001OOD\u68c0\u6d4b\u548c\u91cd\u5efa\uff0c\u5e76\u5728\u771f\u5b9e\u94c1\u8def\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u6df1\u5ea6\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5b89\u5168\u76f8\u5173\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u68c0\u6d4b\u5206\u5e03\u5916\u6837\u672c\u5e76\u89e3\u91ca\u51b3\u7b56\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u6f5c\u5728\u7a7a\u95f4\uff0c\u5b9a\u4e49\u9ad8\u65af\u6df7\u5408\u5206\u5e03\u4f5c\u4e3a\u5206\u5e03\u5185\u533a\u57df\uff0c\u5e76\u5f15\u5165\u9650\u5236\u635f\u5931\u4ee5\u4fdd\u6301\u6f5c\u5728\u7a7a\u95f4\u7684\u7d27\u51d1\u6027\u3002", "result": "\u5728\u5e38\u89c1OOD\u68c0\u6d4b\u57fa\u51c6\u548c\u771f\u5b9e\u94c1\u8def\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u81ea\u89e3\u91ca\u6027\u548cOOD\u68c0\u6d4b\u80fd\u529b\uff0c\u4e3a\u5b89\u5168\u76f8\u5173\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
