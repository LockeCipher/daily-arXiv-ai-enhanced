{"id": "2506.19139", "pdf": "https://arxiv.org/pdf/2506.19139", "abs": "https://arxiv.org/abs/2506.19139", "authors": ["Lukas Radl", "Felix Windisch", "Thomas Deixelberger", "Jozef Hladky", "Michael Steiner", "Dieter Schmalstieg", "Markus Steinberger"], "title": "SOF: Sorted Opacity Fields for Fast Unbounded Surface Reconstruction", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Recent advances in 3D Gaussian representations have significantly improved the quality and efficiency of image-based scene reconstruction. Their explicit nature facilitates real-time rendering and fast optimization, yet extracting accurate surfaces - particularly in large-scale, unbounded environments - remains a difficult task. Many existing methods rely on approximate depth estimates and global sorting heuristics, which can introduce artifacts and limit the fidelity of the reconstructed mesh. In this paper, we present Sorted Opacity Fields (SOF), a method designed to recover detailed surfaces from 3D Gaussians with both speed and precision. Our approach improves upon prior work by introducing hierarchical resorting and a robust formulation of Gaussian depth, which better aligns with the level-set. To enhance mesh quality, we incorporate a level-set regularizer operating on the opacity field and introduce losses that encourage geometrically-consistent primitive shapes. In addition, we develop a parallelized Marching Tetrahedra algorithm tailored to our opacity formulation, reducing meshing time by up to an order of magnitude. As demonstrated by our quantitative evaluation, SOF achieves higher reconstruction accuracy while cutting total processing time by more than a factor of three. These results mark a step forward in turning efficient Gaussian-based rendering into equally efficient geometry extraction.", "AI": {"tldr": "SOF\u65b9\u6cd5\u901a\u8fc7\u5c42\u6b21\u5316\u91cd\u6392\u5e8f\u548c\u9c81\u68d2\u7684Gaussian\u6df1\u5ea6\u516c\u5f0f\uff0c\u4ece3D\u9ad8\u65af\u4e2d\u6062\u590d\u7cbe\u7ec6\u8868\u9762\uff0c\u63d0\u5347\u91cd\u5efa\u7cbe\u5ea6\u548c\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8fd1\u4f3c\u6df1\u5ea6\u4f30\u8ba1\u548c\u5168\u5c40\u6392\u5e8f\u542f\u53d1\u5f0f\uff0c\u5bfc\u81f4\u91cd\u5efa\u7f51\u683c\u7684\u4fdd\u771f\u5ea6\u53d7\u9650\uff0cSOF\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f15\u5165\u5c42\u6b21\u5316\u91cd\u6392\u5e8f\u3001\u9c81\u68d2\u7684Gaussian\u6df1\u5ea6\u516c\u5f0f\u3001\u6c34\u5e73\u96c6\u6b63\u5219\u5316\u5668\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u635f\u5931\uff0c\u5e76\u5f00\u53d1\u5e76\u884c\u5316\u7684Marching Tetrahedra\u7b97\u6cd5\u3002", "result": "SOF\u5728\u91cd\u5efa\u7cbe\u5ea6\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u603b\u5904\u7406\u65f6\u95f4\u51cf\u5c11\u8d85\u8fc7\u4e09\u500d\u3002", "conclusion": "SOF\u5c06\u9ad8\u6548\u7684\u9ad8\u65af\u6e32\u67d3\u8f6c\u5316\u4e3a\u9ad8\u6548\u7684\u51e0\u4f55\u63d0\u53d6\uff0c\u6807\u5fd7\u7740\u8be5\u9886\u57df\u7684\u8fdb\u6b65\u3002"}}
{"id": "2506.19278", "pdf": "https://arxiv.org/pdf/2506.19278", "abs": "https://arxiv.org/abs/2506.19278", "authors": ["Tianshan Zhang", "Hao Tang"], "title": "Style Transfer: A Decade Survey", "categories": ["cs.GR"], "comment": "32 pages", "summary": "The revolutionary advancement of Artificial Intelligence Generated Content (AIGC) has fundamentally transformed the landscape of visual content creation and artistic expression. While remarkable progress has been made in image generation and style transfer, the underlying mechanisms and aesthetic implications of these technologies remain insufficiently understood. This paper presents a comprehensive survey of AIGC technologies in visual arts, tracing their evolution from early algorithmic frameworks to contemporary deep generative models. We identify three pivotal paradigms: Variational Autoencoders (VAE), Generative Adversarial Networks (GANs), and Diffusion Models, and examine their roles in bridging the gap between human creativity and machine synthesis. To support our analysis, we systematically review over 500 research papers published in the past decade, spanning both foundational developments and state-of-the-art innovations. Furthermore, we propose a multidimensional evaluation framework that incorporates Technical Innovation, Artistic Merit, Visual Quality, Computational Efficiency, and Creative Potential. Our findings reveal both the transformative capacities and current limitations of AIGC systems, emphasizing their profound impact on the future of creative practices. Through this extensive synthesis, we offer a unified perspective on the convergence of artificial intelligence and artistic expression, while outlining key challenges and promising directions for future research in this rapidly evolving field.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86AIGC\u6280\u672f\u5728\u89c6\u89c9\u827a\u672f\u4e2d\u7684\u53d1\u5c55\uff0c\u5206\u6790\u4e86VAE\u3001GANs\u548cDiffusion Models\u4e09\u5927\u8303\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u7ef4\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5176\u6f5c\u529b\u4e0e\u5c40\u9650\u3002", "motivation": "\u63a2\u8ba8AIGC\u6280\u672f\u5728\u89c6\u89c9\u827a\u672f\u4e2d\u7684\u673a\u5236\u4e0e\u7f8e\u5b66\u5f71\u54cd\uff0c\u586b\u8865\u5f53\u524d\u7406\u89e3\u7684\u4e0d\u8db3\u3002", "method": "\u7cfb\u7edf\u56de\u987e500\u591a\u7bc7\u8bba\u6587\uff0c\u5206\u6790VAE\u3001GANs\u548cDiffusion Models\uff0c\u63d0\u51fa\u591a\u7ef4\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u63ed\u793a\u4e86AIGC\u7cfb\u7edf\u7684\u53d8\u9769\u80fd\u529b\u4e0e\u5c40\u9650\uff0c\u5f3a\u8c03\u5176\u5bf9\u672a\u6765\u521b\u610f\u5b9e\u8df5\u7684\u5f71\u54cd\u3002", "conclusion": "\u4e3a\u4eba\u5de5\u667a\u80fd\u4e0e\u827a\u672f\u8868\u8fbe\u7684\u878d\u5408\u63d0\u4f9b\u7edf\u4e00\u89c6\u89d2\uff0c\u5e76\u6307\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2506.19400", "pdf": "https://arxiv.org/pdf/2506.19400", "abs": "https://arxiv.org/abs/2506.19400", "authors": ["Liang Zhou", "Xinyi Gou", "Daniel Weiskopf"], "title": "Continuous Indexed Points for Multivariate Volume Visualization", "categories": ["cs.GR"], "comment": "Peer reviewed and accepted by Computational Visual Media", "summary": "We introduce continuous indexed points for improved multivariate volume visualization. Indexed points represent linear structures in parallel coordinates and can be used to encode local correlation of multivariate (including multifield, multifaceted, and multiattribute) volume data. First, we perform local linear fitting in the spatial neighborhood of each volume sample using principal component analysis, accelerated by hierarchical spatial data structures. This local linear information is then visualized as continuous indexed points in parallel coordinates: a density representation of indexed points in a continuous domain. With our new method, multivariate volume data can be analyzed using the eigenvector information from local spatial embeddings. We utilize both 1-flat and 2-flat indexed points, allowing us to identify correlations between two variables and even three variables, respectively. An interactive occlusion shading model facilitates good spatial perception of the volume rendering of volumetric correlation characteristics. Interactive exploration is supported by specifically designed multivariate transfer function widgets working in the image plane of parallel coordinates. We show that our generic technique works for multi-attribute datasets. The effectiveness and usefulness of our new method is demonstrated through a case study, an expert user study, and domain expert feedback.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fde\u7eed\u7d22\u5f15\u70b9\u7684\u591a\u53d8\u91cf\u4f53\u79ef\u53ef\u89c6\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u7ebf\u6027\u62df\u5408\u548c\u4e3b\u6210\u5206\u5206\u6790\uff0c\u5728\u5e73\u884c\u5750\u6807\u4e2d\u7f16\u7801\u591a\u53d8\u91cf\u6570\u636e\u7684\u5c40\u90e8\u76f8\u5173\u6027\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u63a2\u7d22\u3002", "motivation": "\u591a\u53d8\u91cf\u4f53\u79ef\u6570\u636e\u7684\u53ef\u89c6\u5316\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u6355\u6349\u5c40\u90e8\u76f8\u5173\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u4e3b\u6210\u5206\u5206\u6790\u8fdb\u884c\u5c40\u90e8\u7ebf\u6027\u62df\u5408\uff0c\u5e76\u901a\u8fc7\u5c42\u6b21\u7a7a\u95f4\u6570\u636e\u7ed3\u6784\u52a0\u901f\uff0c\u751f\u6210\u8fde\u7eed\u7d22\u5f15\u70b9\u5728\u5e73\u884c\u5750\u6807\u4e2d\u53ef\u89c6\u5316\u3002", "result": "\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5206\u6790\u591a\u53d8\u91cf\u6570\u636e\u7684\u5c40\u90e8\u76f8\u5173\u6027\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u63a2\u7d22\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u548c\u7528\u6237\u53cd\u9988\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u53d8\u91cf\u4f53\u79ef\u6570\u636e\u7684\u53ef\u89c6\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.19415", "pdf": "https://arxiv.org/pdf/2506.19415", "abs": "https://arxiv.org/abs/2506.19415", "authors": ["Jonathan Haberl", "Philipp Fleck", "Clemens Arth"], "title": "Virtual Memory for 3D Gaussian Splatting", "categories": ["cs.GR", "cs.CV", "cs.HC"], "comment": "Based on the Master Thesis from Jonathan Haberl from 2024, Submitted   to TVCG in Feb. 2025;", "summary": "3D Gaussian Splatting represents a breakthrough in the field of novel view synthesis. It establishes Gaussians as core rendering primitives for highly accurate real-world environment reconstruction. Recent advances have drastically increased the size of scenes that can be created. In this work, we present a method for rendering large and complex 3D Gaussian Splatting scenes using virtual memory. By leveraging well-established virtual memory and virtual texturing techniques, our approach efficiently identifies visible Gaussians and dynamically streams them to the GPU just in time for real-time rendering. Selecting only the necessary Gaussians for both storage and rendering results in reduced memory usage and effectively accelerates rendering, especially for highly complex scenes. Furthermore, we demonstrate how level of detail can be integrated into our proposed method to further enhance rendering speed for large-scale scenes. With an optimized implementation, we highlight key practical considerations and thoroughly evaluate the proposed technique and its impact on desktop and mobile devices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u865a\u62df\u5185\u5b58\u6280\u672f\u9ad8\u6548\u6e32\u67d3\u5927\u89c4\u6a213D\u9ad8\u65af\u6cfc\u6e85\u573a\u666f\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6d41\u5f0f\u4f20\u8f93\u53ef\u89c1\u9ad8\u65af\u5143\u7d20\uff0c\u51cf\u5c11\u5185\u5b58\u5360\u7528\u5e76\u52a0\u901f\u6e32\u67d3\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a213D\u9ad8\u65af\u6cfc\u6e85\u573a\u666f\u5728\u5b58\u50a8\u548c\u6e32\u67d3\u65f6\u7684\u9ad8\u5185\u5b58\u9700\u6c42\u95ee\u9898\uff0c\u63d0\u5347\u590d\u6742\u573a\u666f\u7684\u5b9e\u65f6\u6e32\u67d3\u6548\u7387\u3002", "method": "\u7ed3\u5408\u865a\u62df\u5185\u5b58\u548c\u865a\u62df\u7eb9\u7406\u6280\u672f\uff0c\u52a8\u6001\u8bc6\u522b\u5e76\u6d41\u5f0f\u4f20\u8f93\u53ef\u89c1\u9ad8\u65af\u5143\u7d20\uff0c\u540c\u65f6\u96c6\u6210\u7ec6\u8282\u5c42\u6b21\uff08LOD\uff09\u4f18\u5316\u6e32\u67d3\u901f\u5ea6\u3002", "result": "\u663e\u8457\u51cf\u5c11\u4e86\u5185\u5b58\u4f7f\u7528\uff0c\u52a0\u901f\u4e86\u6e32\u67d3\u8fc7\u7a0b\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u573a\u666f\u4e2d\uff0c\u5e76\u5728\u684c\u9762\u548c\u79fb\u52a8\u8bbe\u5907\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a213D\u9ad8\u65af\u6cfc\u6e85\u573a\u666f\u7684\u5b9e\u65f6\u6e32\u67d3\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.19708", "pdf": "https://arxiv.org/pdf/2506.19708", "abs": "https://arxiv.org/abs/2506.19708", "authors": ["Matyas Bohacek", "Thomas Fel", "Maneesh Agrawala", "Ekdeep Singh Lubana"], "title": "Uncovering Conceptual Blindspots in Generative Image Models Using Sparse Autoencoders", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Despite their impressive performance, generative image models trained on large-scale datasets frequently fail to produce images with seemingly simple concepts -- e.g., human hands or objects appearing in groups of four -- that are reasonably expected to appear in the training data. These failure modes have largely been documented anecdotally, leaving open the question of whether they reflect idiosyncratic anomalies or more structural limitations of these models. To address this, we introduce a systematic approach for identifying and characterizing \"conceptual blindspots\" -- concepts present in the training data but absent or misrepresented in a model's generations. Our method leverages sparse autoencoders (SAEs) to extract interpretable concept embeddings, enabling a quantitative comparison of concept prevalence between real and generated images. We train an archetypal SAE (RA-SAE) on DINOv2 features with 32,000 concepts -- the largest such SAE to date -- enabling fine-grained analysis of conceptual disparities. Applied to four popular generative models (Stable Diffusion 1.5/2.1, PixArt, and Kandinsky), our approach reveals specific suppressed blindspots (e.g., bird feeders, DVD discs, and whitespaces on documents) and exaggerated blindspots (e.g., wood background texture and palm trees). At the individual datapoint level, we further isolate memorization artifacts -- instances where models reproduce highly specific visual templates seen during training. Overall, we propose a theoretically grounded framework for systematically identifying conceptual blindspots in generative models by assessing their conceptual fidelity with respect to the underlying data-generating process.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u8bc6\u522b\u751f\u6210\u56fe\u50cf\u6a21\u578b\u4e2d\u7684\u201c\u6982\u5ff5\u76f2\u70b9\u201d\uff0c\u5373\u8bad\u7ec3\u6570\u636e\u4e2d\u5b58\u5728\u4f46\u5728\u751f\u6210\u56fe\u50cf\u4e2d\u7f3a\u5931\u6216\u9519\u8bef\u8868\u73b0\u7684\u6982\u5ff5\u3002", "motivation": "\u5c3d\u7ba1\u751f\u6210\u56fe\u50cf\u6a21\u578b\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u751f\u6210\u67d0\u4e9b\u7b80\u5355\u6982\u5ff5\uff08\u5982\u4eba\u624b\u6216\u56db\u4ef6\u4e00\u7ec4\u7269\u4f53\uff09\u65f6\u4ecd\u5b58\u5728\u660e\u663e\u7f3a\u9677\u3002\u8fd9\u4e9b\u7f3a\u9677\u662f\u5426\u53cd\u6620\u6a21\u578b\u7684\u7ed3\u6784\u6027\u9650\u5236\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u63d0\u53d6\u53ef\u89e3\u91ca\u7684\u6982\u5ff5\u5d4c\u5165\uff0c\u901a\u8fc7\u91cf\u5316\u6bd4\u8f83\u771f\u5b9e\u4e0e\u751f\u6210\u56fe\u50cf\u4e2d\u6982\u5ff5\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u8bc6\u522b\u76f2\u70b9\u3002\u8bad\u7ec3\u4e86\u4e00\u4e2a\u5305\u542b32,000\u4e2a\u6982\u5ff5\u7684RA-SAE\u6a21\u578b\u3002", "result": "\u5728\u56db\u79cd\u6d41\u884c\u751f\u6210\u6a21\u578b\uff08Stable Diffusion 1.5/2.1\u3001PixArt\u3001Kandinsky\uff09\u4e2d\u53d1\u73b0\u4e86\u7279\u5b9a\u88ab\u6291\u5236\u6216\u5938\u5927\u7684\u76f2\u70b9\uff08\u5982\u9e1f\u98df\u5668\u3001DVD\u5149\u76d8\u3001\u6728\u8d28\u80cc\u666f\u7eb9\u7406\uff09\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u4e0e\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u7684\u6982\u5ff5\u4fdd\u771f\u5ea6\uff0c\u7cfb\u7edf\u6027\u8bc6\u522b\u6982\u5ff5\u76f2\u70b9\u3002"}}
{"id": "2506.18999", "pdf": "https://arxiv.org/pdf/2506.18999", "abs": "https://arxiv.org/abs/2506.18999", "authors": ["Yuan Yao", "Yicong Hong", "Difan Liu", "Long Mai", "Feng Liu", "Jiebo Luo"], "title": "Diffusion Transformer-to-Mamba Distillation for High-Resolution Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "The quadratic computational complexity of self-attention in diffusion transformers (DiT) introduces substantial computational costs in high-resolution image generation. While the linear-complexity Mamba model emerges as a potential alternative, direct Mamba training remains empirically challenging. To address this issue, this paper introduces diffusion transformer-to-mamba distillation (T2MD), forming an efficient training pipeline that facilitates the transition from the self-attention-based transformer to the linear complexity state-space model Mamba. We establish a diffusion self-attention and Mamba hybrid model that simultaneously achieves efficiency and global dependencies. With the proposed layer-level teacher forcing and feature-based knowledge distillation, T2MD alleviates the training difficulty and high cost of a state space model from scratch. Starting from the distilled 512$\\times$512 resolution base model, we push the generation towards 2048$\\times$2048 images via lightweight adaptation and high-resolution fine-tuning. Experiments demonstrate that our training path leads to low overhead but high-quality text-to-image generation. Importantly, our results also justify the feasibility of using sequential and causal Mamba models for generating non-causal visual output, suggesting the potential for future exploration.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u6269\u6563Transformer\u5230Mamba\u7684\u84b8\u998f\u65b9\u6cd5\uff08T2MD\uff09\uff0c\u4ee5\u89e3\u51b3\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u4f9d\u8d56\u6027\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u6269\u6563Transformer\u4e2d\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u540c\u65f6\u63a2\u7d22Mamba\u6a21\u578b\u5728\u975e\u56e0\u679c\u89c6\u89c9\u751f\u6210\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faT2MD\u65b9\u6cd5\uff0c\u7ed3\u5408\u81ea\u6ce8\u610f\u529b\u548cMamba\u7684\u6df7\u5408\u6a21\u578b\uff0c\u91c7\u7528\u5c42\u7ea7\u6559\u5e08\u5f3a\u5236\u548c\u57fa\u4e8e\u7279\u5f81\u7684\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728512\u00d7512\u5206\u8fa8\u7387\u4e0b\u8bad\u7ec3\u540e\uff0c\u53ef\u6269\u5c55\u52302048\u00d72048\u5206\u8fa8\u7387\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u3002", "conclusion": "T2MD\u8bc1\u660e\u4e86Mamba\u6a21\u578b\u5728\u975e\u56e0\u679c\u89c6\u89c9\u751f\u6210\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2506.19072", "pdf": "https://arxiv.org/pdf/2506.19072", "abs": "https://arxiv.org/abs/2506.19072", "authors": ["Yimu Wang", "Mozhgan Nasr Azadani", "Sean Sedwards", "Krzysztof Czarnecki"], "title": "HAWAII: Hierarchical Visual Knowledge Transfer for Efficient Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Improving the visual understanding ability of vision-language models (VLMs) is crucial for enhancing their performance across various tasks. While using multiple pretrained visual experts has shown great promise, it often incurs significant computational costs during training and inference. To address this challenge, we propose HAWAII, a novel framework that distills knowledge from multiple visual experts into a single vision encoder, enabling it to inherit the complementary strengths of several experts with minimal computational overhead. To mitigate conflicts among different teachers and switch between different teacher-specific knowledge, instead of using a fixed set of adapters for multiple teachers, we propose to use teacher-specific Low-Rank Adaptation (LoRA) adapters with a corresponding router. Each adapter is aligned with a specific teacher, avoiding noisy guidance during distillation. To enable efficient knowledge distillation, we propose fine-grained and coarse-grained distillation. At the fine-grained level, token importance scores are employed to emphasize the most informative tokens from each teacher adaptively. At the coarse-grained level, we summarize the knowledge from multiple teachers and transfer it to the student using a set of general-knowledge LoRA adapters with a router. Extensive experiments on various vision-language tasks demonstrate the superiority of HAWAII, compared to the popular open-source VLMs.", "AI": {"tldr": "HAWAII\u6846\u67b6\u901a\u8fc7\u5c06\u591a\u4e2a\u89c6\u89c9\u4e13\u5bb6\u7684\u77e5\u8bc6\u84b8\u998f\u5230\u5355\u4e00\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\uff0c\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u7559\u4e92\u8865\u4f18\u52bf\u3002", "motivation": "\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u540c\u65f6\u907f\u514d\u591a\u4e13\u5bb6\u7cfb\u7edf\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u4f7f\u7528\u6559\u5e08\u7279\u5b9a\u7684LoRA\u9002\u914d\u5668\u548c\u8def\u7531\u5668\uff0c\u7ed3\u5408\u7ec6\u7c92\u5ea6\u548c\u7c97\u7c92\u5ea6\u84b8\u998f\u7b56\u7565\u3002", "result": "\u5728\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4e3b\u6d41\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "HAWAII\u6846\u67b6\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\uff0c\u9002\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u84b8\u998f\u3002"}}
{"id": "2506.19117", "pdf": "https://arxiv.org/pdf/2506.19117", "abs": "https://arxiv.org/abs/2506.19117", "authors": ["Christina Ourania Tze", "Daniel Dauner", "Yiyi Liao", "Dzmitry Tsishkou", "Andreas Geiger"], "title": "PrITTI: Primitive-based Generation of Controllable and Editable 3D Semantic Scenes", "categories": ["cs.CV"], "comment": "Project page: https://raniatze.github.io/pritti/", "summary": "Large-scale 3D semantic scene generation has predominantly relied on voxel-based representations, which are memory-intensive, bound by fixed resolutions, and challenging to edit. In contrast, primitives represent semantic entities using compact, coarse 3D structures that are easy to manipulate and compose, making them an ideal representation for this task. In this paper, we introduce PrITTI, a latent diffusion-based framework that leverages primitives as the main foundational elements for generating compositional, controllable, and editable 3D semantic scene layouts. Our method adopts a hybrid representation, modeling ground surfaces in a rasterized format while encoding objects as vectorized 3D primitives. This decomposition is also reflected in a structured latent representation that enables flexible scene manipulation of ground and object components. To overcome the orientation ambiguities in conventional encoding methods, we introduce a stable Cholesky-based parameterization that jointly encodes object size and orientation. Experiments on the KITTI-360 dataset show that PrITTI outperforms a voxel-based baseline in generation quality, while reducing memory requirements by up to $3\\times$. In addition, PrITTI enables direct instance-level manipulation of objects in the scene and supports a range of downstream applications, including scene inpainting, outpainting, and photo-realistic street-view synthesis.", "AI": {"tldr": "PrITTI\u662f\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u7684\u6846\u67b6\uff0c\u5229\u75283D\u57fa\u5143\u751f\u6210\u53ef\u63a7\u3001\u53ef\u7f16\u8f91\u76843D\u8bed\u4e49\u573a\u666f\u5e03\u5c40\uff0c\u4f18\u4e8e\u4f20\u7edf\u4f53\u7d20\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u4f53\u7d20\u8868\u793a\u5185\u5b58\u5bc6\u96c6\u3001\u5206\u8fa8\u7387\u56fa\u5b9a\u4e14\u96be\u4ee5\u7f16\u8f91\uff0c\u800c\u57fa\u5143\u8868\u793a\u66f4\u7d27\u51d1\u3001\u6613\u4e8e\u64cd\u4f5c\uff0c\u9002\u5408\u5927\u89c4\u6a213D\u573a\u666f\u751f\u6210\u3002", "method": "\u91c7\u7528\u6df7\u5408\u8868\u793a\uff0c\u5730\u9762\u4ee5\u6805\u683c\u5316\u5f62\u5f0f\u5efa\u6a21\uff0c\u5bf9\u8c61\u4ee5\u5411\u91cf\u53163D\u57fa\u5143\u7f16\u7801\uff0c\u5e76\u5f15\u5165\u7a33\u5b9a\u7684Cholesky\u53c2\u6570\u5316\u89e3\u7801\u5bf9\u8c61\u5c3a\u5bf8\u548c\u65b9\u5411\u3002", "result": "\u5728KITTI-360\u6570\u636e\u96c6\u4e0a\uff0cPrITTI\u751f\u6210\u8d28\u91cf\u4f18\u4e8e\u4f53\u7d20\u57fa\u7ebf\uff0c\u5185\u5b58\u9700\u6c42\u964d\u4f4e3\u500d\uff0c\u652f\u6301\u5b9e\u4f8b\u7ea7\u64cd\u4f5c\u548c\u591a\u79cd\u4e0b\u6e38\u5e94\u7528\u3002", "conclusion": "PrITTI\u8bc1\u660e\u4e86\u57fa\u5143\u8868\u793a\u57283D\u573a\u666f\u751f\u6210\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u7075\u6d3b\u6027\uff0c\u4e3a\u53ef\u63a7\u573a\u666f\u7f16\u8f91\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.19261", "pdf": "https://arxiv.org/pdf/2506.19261", "abs": "https://arxiv.org/abs/2506.19261", "authors": ["Quang-Binh Nguyen", "Trong-Vu Hoang", "Ngoc-Do Tran", "Tam V. Nguyen", "Minh-Triet Tran", "Trung-Nghia Le"], "title": "Automated Image Recognition Framework", "categories": ["cs.CV"], "comment": "ICCCI 2025", "summary": "While the efficacy of deep learning models heavily relies on data, gathering and annotating data for specific tasks, particularly when addressing novel or sensitive subjects lacking relevant datasets, poses significant time and resource challenges. In response to this, we propose a novel Automated Image Recognition (AIR) framework that harnesses the power of generative AI. AIR empowers end-users to synthesize high-quality, pre-annotated datasets, eliminating the necessity for manual labeling. It also automatically trains deep learning models on the generated datasets with robust image recognition performance. Our framework includes two main data synthesis processes, AIR-Gen and AIR-Aug. The AIR-Gen enables end-users to seamlessly generate datasets tailored to their specifications. To improve image quality, we introduce a novel automated prompt engineering module that leverages the capabilities of large language models. We also introduce a distribution adjustment algorithm to eliminate duplicates and outliers, enhancing the robustness and reliability of generated datasets. On the other hand, the AIR-Aug enhances a given dataset, thereby improving the performance of deep classifier models. AIR-Aug is particularly beneficial when users have limited data for specific tasks. Through comprehensive experiments, we demonstrated the efficacy of our generated data in training deep learning models and showcased the system's potential to provide image recognition models for a wide range of objects. We also conducted a user study that achieved an impressive score of 4.4 out of 5.0, underscoring the AI community's positive perception of AIR.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210AI\u7684\u81ea\u52a8\u56fe\u50cf\u8bc6\u522b\u6846\u67b6\uff08AIR\uff09\uff0c\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u9884\u6807\u6ce8\u6570\u636e\u96c6\u548c\u81ea\u52a8\u8bad\u7ec3\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u6536\u96c6\u548c\u6807\u6ce8\u7684\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u7279\u5b9a\u4efb\u52a1\u4e2d\u6570\u636e\u6536\u96c6\u548c\u6807\u6ce8\u7684\u9ad8\u6210\u672c\u95ee\u9898\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u65b0\u9896\u6216\u654f\u611f\u4e3b\u9898\u65f6\u7f3a\u4e4f\u76f8\u5173\u6570\u636e\u96c6\u7684\u60c5\u51b5\u3002", "method": "\u5305\u62ecAIR-Gen\uff08\u751f\u6210\u5b9a\u5236\u6570\u636e\u96c6\uff09\u548cAIR-Aug\uff08\u589e\u5f3a\u73b0\u6709\u6570\u636e\u96c6\uff09\uff0c\u7ed3\u5408\u81ea\u52a8\u63d0\u793a\u5de5\u7a0b\u548c\u5206\u5e03\u8c03\u6574\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u751f\u6210\u7684\u6570\u636e\u80fd\u6709\u6548\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u6237\u7814\u7a76\u663e\u793aAIR\u83b7\u5f974.4/5\u7684\u9ad8\u8bc4\u5206\u3002", "conclusion": "AIR\u6846\u67b6\u4e3a\u56fe\u50cf\u8bc6\u522b\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.19291", "pdf": "https://arxiv.org/pdf/2506.19291", "abs": "https://arxiv.org/abs/2506.19291", "authors": ["Xiaoyuan Wang", "Yizhou Zhao", "Botao Ye", "Xiaojun Shan", "Weijie Lyu", "Lu Qi", "Kelvin C. K. Chan", "Yinxiao Li", "Ming-Hsuan Yang"], "title": "HoliGS: Holistic Gaussian Splatting for Embodied View Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "We propose HoliGS, a novel deformable Gaussian splatting framework that addresses embodied view synthesis from long monocular RGB videos. Unlike prior 4D Gaussian splatting and dynamic NeRF pipelines, which struggle with training overhead in minute-long captures, our method leverages invertible Gaussian Splatting deformation networks to reconstruct large-scale, dynamic environments accurately. Specifically, we decompose each scene into a static background plus time-varying objects, each represented by learned Gaussian primitives undergoing global rigid transformations, skeleton-driven articulation, and subtle non-rigid deformations via an invertible neural flow. This hierarchical warping strategy enables robust free-viewpoint novel-view rendering from various embodied camera trajectories by attaching Gaussians to a complete canonical foreground shape (\\eg, egocentric or third-person follow), which may involve substantial viewpoint changes and interactions between multiple actors. Our experiments demonstrate that \\ourmethod~ achieves superior reconstruction quality on challenging datasets while significantly reducing both training and rendering time compared to state-of-the-art monocular deformable NeRFs. These results highlight a practical and scalable solution for EVS in real-world scenarios. The source code will be released.", "AI": {"tldr": "HoliGS\u662f\u4e00\u79cd\u65b0\u9896\u7684\u53ef\u53d8\u5f62\u9ad8\u65af\u6e85\u5c04\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u957f\u5355\u76eeRGB\u89c6\u9891\u4e2d\u8fdb\u884c\u5b9e\u4f53\u5316\u89c6\u56fe\u5408\u6210\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u67094D\u9ad8\u65af\u6e85\u5c04\u548c\u52a8\u6001NeRF\u65b9\u6cd5\u5728\u957f\u65f6\u95f4\u6355\u83b7\u4e2d\u8bad\u7ec3\u5f00\u9500\u5927\u7684\u95ee\u9898\u3002", "method": "\u5c06\u573a\u666f\u5206\u89e3\u4e3a\u9759\u6001\u80cc\u666f\u548c\u65f6\u53d8\u5bf9\u8c61\uff0c\u901a\u8fc7\u53ef\u9006\u9ad8\u65af\u6e85\u5c04\u53d8\u5f62\u7f51\u7edc\u5b9e\u73b0\u5168\u5c40\u521a\u6027\u53d8\u6362\u3001\u9aa8\u67b6\u9a71\u52a8\u5173\u8282\u548c\u7ec6\u5fae\u975e\u521a\u6027\u53d8\u5f62\u3002", "result": "\u5728\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u66f4\u9ad8\u8d28\u91cf\u7684\u91cd\u5efa\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u548c\u6e32\u67d3\u65f6\u95f4\u3002", "conclusion": "HoliGS\u4e3a\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5b9e\u4f53\u5316\u89c6\u56fe\u5408\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.19348", "pdf": "https://arxiv.org/pdf/2506.19348", "abs": "https://arxiv.org/abs/2506.19348", "authors": ["Jintao Rong", "Xin Xie", "Xinyi Yu", "Linlin Ou", "Xinyu Zhang", "Chunhua Shen", "Dong Gong"], "title": "Training-Free Motion Customization for Distilled Video Generators with Adaptive Test-Time Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Distilled video generation models offer fast and efficient synthesis but struggle with motion customization when guided by reference videos, especially under training-free settings. Existing training-free methods, originally designed for standard diffusion models, fail to generalize due to the accelerated generative process and large denoising steps in distilled models. To address this, we propose MotionEcho, a novel training-free test-time distillation framework that enables motion customization by leveraging diffusion teacher forcing. Our approach uses high-quality, slow teacher models to guide the inference of fast student models through endpoint prediction and interpolation. To maintain efficiency, we dynamically allocate computation across timesteps according to guidance needs. Extensive experiments across various distilled video generation models and benchmark datasets demonstrate that our method significantly improves motion fidelity and generation quality while preserving high efficiency. Project page: https://euminds.github.io/motionecho/", "AI": {"tldr": "MotionEcho\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u6307\u5bfc\u5b66\u751f\u6a21\u578b\uff0c\u63d0\u5347\u84b8\u998f\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u8fd0\u52a8\u5b9a\u5236\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u84b8\u998f\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u5feb\u901f\u5408\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u53c2\u8003\u89c6\u9891\u5f15\u5bfc\u7684\u8fd0\u52a8\u5b9a\u5236\u4e0a\u5b58\u5728\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u3002\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u84b8\u998f\u6a21\u578b\u7684\u52a0\u901f\u751f\u6210\u8fc7\u7a0b\u548c\u5927\u53bb\u566a\u6b65\u957f\u3002", "method": "\u63d0\u51faMotionEcho\u6846\u67b6\uff0c\u5229\u7528\u6269\u6563\u6559\u5e08\u5f3a\u5236\u6280\u672f\uff0c\u901a\u8fc7\u7aef\u70b9\u9884\u6d4b\u548c\u63d2\u503c\uff0c\u7528\u9ad8\u8d28\u91cf\u6162\u901f\u6559\u5e08\u6a21\u578b\u6307\u5bfc\u5feb\u901f\u5b66\u751f\u6a21\u578b\u7684\u63a8\u7406\u3002\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u4ee5\u4fdd\u6301\u6548\u7387\u3002", "result": "\u5728\u591a\u79cd\u84b8\u998f\u89c6\u9891\u751f\u6210\u6a21\u578b\u548c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u8fd0\u52a8\u4fdd\u771f\u5ea6\u548c\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u7387\u3002", "conclusion": "MotionEcho\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u84b8\u998f\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u8fd0\u52a8\u5b9a\u5236\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u751f\u6210\u6548\u679c\u548c\u6548\u7387\u3002"}}
{"id": "2506.19391", "pdf": "https://arxiv.org/pdf/2506.19391", "abs": "https://arxiv.org/abs/2506.19391", "authors": ["Declan J. Curran", "Sanaa Hobeichi", "Hira Saleem", "Hao Xue", "Flora D. Salim"], "title": "Generate the Forest before the Trees -- A Hierarchical Diffusion model for Climate Downscaling", "categories": ["cs.CV"], "comment": "8 pages", "summary": "Downscaling is essential for generating the high-resolution climate data needed for local planning, but traditional methods remain computationally demanding. Recent years have seen impressive results from AI downscaling models, particularly diffusion models, which have attracted attention due to their ability to generate ensembles and overcome the smoothing problem common in other AI methods. However, these models typically remain computationally intensive. We introduce a Hierarchical Diffusion Downscaling (HDD) model, which introduces an easily-extensible hierarchical sampling process to the diffusion framework. A coarse-to-fine hierarchy is imposed via a simple downsampling scheme. HDD achieves competitive accuracy on ERA5 reanalysis datasets and CMIP6 models, significantly reducing computational load by running on up to half as many pixels with competitive results. Additionally, a single model trained at 0.25{\\deg} resolution transfers seamlessly across multiple CMIP6 models with much coarser resolution. HDD thus offers a lightweight alternative for probabilistic climate downscaling, facilitating affordable large-ensemble high-resolution climate projections. See a full code implementation at: https://github.com/HDD-Hierarchical-Diffusion-Downscaling/HDD-Hierarchical-Diffusion-Downscaling.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHDD\u7684\u5206\u5c42\u6269\u6563\u964d\u5c3a\u5ea6\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d1f\u62c5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u964d\u5c3a\u5ea6\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\uff0cAI\u964d\u5c3a\u5ea6\u6a21\u578b\uff08\u5982\u6269\u6563\u6a21\u578b\uff09\u867d\u6709\u6548\u4f46\u4ecd\u8ba1\u7b97\u5bc6\u96c6\uff0c\u9700\u8981\u66f4\u8f7b\u91cf\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165\u5206\u5c42\u91c7\u6837\u8fc7\u7a0b\uff0c\u91c7\u7528\u4ece\u7c97\u5230\u7ec6\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u4e0b\u91c7\u6837\u65b9\u6848\u5b9e\u73b0\u3002", "result": "\u5728ERA5\u548cCMIP6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u8d1f\u8f7d\u663e\u8457\u964d\u4f4e\uff0c\u4e14\u6a21\u578b\u53ef\u8de8\u4e0d\u540c\u5206\u8fa8\u7387\u8fc1\u79fb\u3002", "conclusion": "HDD\u4e3a\u6982\u7387\u6c14\u5019\u964d\u5c3a\u5ea6\u63d0\u4f9b\u4e86\u8f7b\u91cf\u5316\u66ff\u4ee3\u65b9\u6848\uff0c\u652f\u6301\u5927\u89c4\u6a21\u9ad8\u5206\u8fa8\u7387\u6c14\u5019\u9884\u6d4b\u3002"}}
{"id": "2506.19406", "pdf": "https://arxiv.org/pdf/2506.19406", "abs": "https://arxiv.org/abs/2506.19406", "authors": ["Chen Yi", "Shan LianLei"], "title": "A Global-Local Cross-Attention Network for Ultra-high Resolution Remote Sensing Image Semantic Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the rapid development of ultra-high resolution (UHR) remote sensing technology, the demand for accurate and efficient semantic segmentation has increased significantly. However, existing methods face challenges in computational efficiency and multi-scale feature fusion. To address these issues, we propose GLCANet (Global-Local Cross-Attention Network), a lightweight segmentation framework designed for UHR remote sensing imagery.GLCANet employs a dual-stream architecture to efficiently fuse global semantics and local details while minimizing GPU usage. A self-attention mechanism enhances long-range dependencies, refines global features, and preserves local details for better semantic consistency. A masked cross-attention mechanism also adaptively fuses global-local features, selectively enhancing fine-grained details while exploiting global context to improve segmentation accuracy. Experimental results show that GLCANet outperforms state-of-the-art methods regarding accuracy and computational efficiency. The model effectively processes large, high-resolution images with a small memory footprint, providing a promising solution for real-world remote sensing applications.", "AI": {"tldr": "GLCANet\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5206\u5272\u6846\u67b6\uff0c\u7528\u4e8e\u8d85\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\uff0c\u901a\u8fc7\u53cc\u6d41\u67b6\u6784\u548c\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u8bed\u4e49\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u8d85\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u6280\u672f\u7684\u53d1\u5c55\u5bf9\u51c6\u786e\u9ad8\u6548\u7684\u8bed\u4e49\u5206\u5272\u9700\u6c42\u589e\u52a0\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "GLCANet\u91c7\u7528\u53cc\u6d41\u67b6\u6784\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408\u5168\u5c40\u8bed\u4e49\u4e0e\u5c40\u90e8\u7ec6\u8282\uff0c\u5e76\u901a\u8fc7\u63a9\u7801\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u81ea\u9002\u5e94\u878d\u5408\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGLCANet\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u9ad8\u6548\u5904\u7406\u5927\u5c3a\u5bf8\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u3002", "conclusion": "GLCANet\u4e3a\u5b9e\u9645\u9065\u611f\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.19445", "pdf": "https://arxiv.org/pdf/2506.19445", "abs": "https://arxiv.org/abs/2506.19445", "authors": ["Mahdi Mohd Hossain Noki", "Syed Mumtahin Mahmud", "Prothito Shovon Majumder", "Abdul Mohaimen Al Radi", "Md. Haider Ali", "Md. Mosaddek Khan"], "title": "Deblurring in the Wild: A Real-World Dataset from Smartphone High-Speed Videos", "categories": ["cs.CV"], "comment": "8 pages (without references), 3 figures. Dataset   https://huggingface.co/datasets/masterda/SloMoBlur", "summary": "We introduce the largest real-world image deblurring dataset constructed from smartphone slow-motion videos. Using 240 frames captured over one second, we simulate realistic long-exposure blur by averaging frames to produce blurry images, while using the temporally centered frame as the sharp reference. Our dataset contains over 42,000 high-resolution blur-sharp image pairs, making it approximately 10 times larger than widely used datasets, with 8 times the amount of different scenes, including indoor and outdoor environments, with varying object and camera motions. We benchmark multiple state-of-the-art (SOTA) deblurring models on our dataset and observe significant performance degradation, highlighting the complexity and diversity of our benchmark. Our dataset serves as a challenging new benchmark to facilitate robust and generalizable deblurring models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u667a\u80fd\u624b\u673a\u6162\u52a8\u4f5c\u89c6\u9891\u6784\u5efa\u7684\u5927\u89c4\u6a21\u771f\u5b9e\u56fe\u50cf\u53bb\u6a21\u7cca\u6570\u636e\u96c6\uff0c\u5305\u542b42,000\u5bf9\u9ad8\u5206\u8fa8\u7387\u6a21\u7cca-\u6e05\u6670\u56fe\u50cf\uff0c\u7528\u4e8e\u8bc4\u4f30\u73b0\u6709\u53bb\u6a21\u7cca\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u53bb\u6a21\u7cca\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u4e14\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u96be\u4ee5\u8bc4\u4f30\u6a21\u578b\u5728\u771f\u5b9e\u590d\u6742\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u5e73\u5747240\u5e27\u6162\u52a8\u4f5c\u89c6\u9891\u6a21\u62df\u957f\u66dd\u5149\u6a21\u7cca\uff0c\u751f\u6210\u6a21\u7cca\u56fe\u50cf\uff0c\u5e76\u4f7f\u7528\u4e2d\u95f4\u5e27\u4f5c\u4e3a\u6e05\u6670\u53c2\u8003\u3002", "result": "\u6570\u636e\u96c6\u89c4\u6a21\u662f\u73b0\u6709\u5e38\u7528\u6570\u636e\u96c6\u768410\u500d\uff0c\u573a\u666f\u591a\u6837\u6027\u66f4\u9ad8\uff0c\u73b0\u6709SOTA\u6a21\u578b\u5728\u5176\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u5f00\u53d1\u9c81\u68d2\u4e14\u6cdb\u5316\u6027\u5f3a\u7684\u53bb\u6a21\u7cca\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u6311\u6218\u6027\u57fa\u51c6\u3002"}}
{"id": "2506.19465", "pdf": "https://arxiv.org/pdf/2506.19465", "abs": "https://arxiv.org/abs/2506.19465", "authors": ["Farnood Salehi", "Vandit Sharma", "Amirhossein Askari Farsangi", "Tun\u00e7 Ozan Ayd\u0131n"], "title": "Stylized Structural Patterns for Improved Neural Network Pre-training", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Modern deep learning models in computer vision require large datasets of real images, which are difficult to curate and pose privacy and legal concerns, limiting their commercial use. Recent works suggest synthetic data as an alternative, yet models trained with it often underperform. This paper proposes a two-step approach to bridge this gap. First, we propose an improved neural fractal formulation through which we introduce a new class of synthetic data. Second, we propose reverse stylization, a technique that transfers visual features from a small, license-free set of real images onto synthetic datasets, enhancing their effectiveness. We analyze the domain gap between our synthetic datasets and real images using Kernel Inception Distance (KID) and show that our method achieves a significantly lower distributional gap compared to existing synthetic datasets. Furthermore, our experiments across different tasks demonstrate the practical impact of this reduced gap. We show that pretraining the EDM2 diffusion model on our synthetic dataset leads to an 11% reduction in FID during image generation, compared to models trained on existing synthetic datasets, and a 20% decrease in autoencoder reconstruction error, indicating improved performance in data representation. Furthermore, a ViT-S model trained for classification on this synthetic data achieves over a 10% improvement in ImageNet-100 accuracy. Our work opens up exciting possibilities for training practical models when sufficiently large real training sets are not available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u6b65\u6cd5\u6765\u7f29\u5c0f\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff1a\u6539\u8fdb\u7684\u795e\u7ecf\u5206\u5f62\u516c\u5f0f\u548c\u53cd\u5411\u98ce\u683c\u5316\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5206\u5e03\u5dee\u8ddd\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4f9d\u8d56\u5927\u89c4\u6a21\u771f\u5b9e\u56fe\u50cf\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u96be\u4ee5\u83b7\u53d6\u4e14\u5b58\u5728\u9690\u79c1\u548c\u6cd5\u5f8b\u98ce\u9669\uff0c\u800c\u73b0\u6709\u5408\u6210\u6570\u636e\u8bad\u7ec3\u6548\u679c\u4e0d\u4f73\u3002", "method": "1. \u63d0\u51fa\u6539\u8fdb\u7684\u795e\u7ecf\u5206\u5f62\u516c\u5f0f\u751f\u6210\u65b0\u7c7b\u522b\u7684\u5408\u6210\u6570\u636e\uff1b2. \u63d0\u51fa\u53cd\u5411\u98ce\u683c\u5316\u6280\u672f\uff0c\u5c06\u5c0f\u89c4\u6a21\u771f\u5b9e\u56fe\u50cf\u7684\u7279\u5f81\u8fc1\u79fb\u5230\u5408\u6210\u6570\u636e\u4e2d\u3002", "result": "\u901a\u8fc7KID\u6307\u6807\u9a8c\u8bc1\u4e86\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u7684\u5206\u5e03\u5dee\u8ddd\u663e\u8457\u964d\u4f4e\uff1b\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5982EDM2\u6269\u6563\u6a21\u578b\u7684FID\u964d\u4f4e11%\uff0cViT-S\u5206\u7c7b\u6a21\u578b\u5728ImageNet-100\u4e0a\u51c6\u786e\u7387\u63d0\u534710%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5728\u7f3a\u4e4f\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u96c6\u65f6\u8bad\u7ec3\u5b9e\u7528\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.19615", "pdf": "https://arxiv.org/pdf/2506.19615", "abs": "https://arxiv.org/abs/2506.19615", "authors": ["Gaurav Sharma", "Ravi Kothari", "Josef Schmid"], "title": "Self-Supervised Multimodal NeRF for Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we propose a Neural Radiance Fields (NeRF) based framework, referred to as Novel View Synthesis Framework (NVSF). It jointly learns the implicit neural representation of space and time-varying scene for both LiDAR and Camera. We test this on a real-world autonomous driving scenario containing both static and dynamic scenes. Compared to existing multimodal dynamic NeRFs, our framework is self-supervised, thus eliminating the need for 3D labels. For efficient training and faster convergence, we introduce heuristic-based image pixel sampling to focus on pixels with rich information. To preserve the local features of LiDAR points, a Double Gradient based mask is employed. Extensive experiments on the KITTI-360 dataset show that, compared to the baseline models, our framework has reported best performance on both LiDAR and Camera domain. Code of the model is available at https://github.com/gaurav00700/Selfsupervised-NVSF", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eNeRF\u7684\u81ea\u76d1\u7763\u591a\u6a21\u6001\u52a8\u6001\u573a\u666f\u65b0\u89c6\u89d2\u5408\u6210\u6846\u67b6\uff08NVSF\uff09\uff0c\u65e0\u97003D\u6807\u7b7e\uff0c\u5728LiDAR\u548c\u76f8\u673a\u9886\u57df\u5747\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u52a8\u6001NeRF\u65b9\u6cd5\u5bf93D\u6807\u7b7e\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u591a\u6a21\u6001\u573a\u666f\u4e0b\u7684\u65b0\u89c6\u89d2\u5408\u6210\u6027\u80fd\u3002", "method": "\u8054\u5408\u5b66\u4e60\u65f6\u7a7a\u573a\u666f\u7684\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff0c\u91c7\u7528\u542f\u53d1\u5f0f\u56fe\u50cf\u50cf\u7d20\u91c7\u6837\u548c\u53cc\u68af\u5ea6\u63a9\u7801\u4f18\u5316\u8bad\u7ec3\u3002", "result": "\u5728KITTI-360\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0cLiDAR\u548c\u76f8\u673a\u9886\u57df\u5747\u53d6\u5f97\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "NVSF\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u81ea\u76d1\u7763\u7684\u591a\u6a21\u6001\u52a8\u6001\u573a\u666f\u65b0\u89c6\u89d2\u5408\u6210\u6846\u67b6\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.19656", "pdf": "https://arxiv.org/pdf/2506.19656", "abs": "https://arxiv.org/abs/2506.19656", "authors": ["Oscar J. Pellicer-Valero", "Cesar Aybar", "Gustau Camps Valls"], "title": "Video Compression for Spatiotemporal Earth System Data", "categories": ["cs.CV", "cs.DL", "eess.IV", "physics.geo-ph"], "comment": null, "summary": "Large-scale Earth system datasets, from high-resolution remote sensing imagery to spatiotemporal climate model outputs, exhibit characteristics analogous to those of standard videos. Their inherent spatial, temporal, and spectral redundancies can thus be readily exploited by established video compression techniques. Here, we present xarrayvideo, a Python library for compressing multichannel spatiotemporal datasets by encoding them as videos. Our approach achieves compression ratios of up to 250x while maintaining high fidelity by leveraging standard, well-optimized video codecs through ffmpeg. We demonstrate the library's effectiveness on four real-world multichannel spatiotemporal datasets: DynamicEarthNet (very high resolution Planet images), DeepExtremeCubes (high resolution Sentinel-2 images), ERA5 (weather reanalysis data), and the SimpleS2 dataset (high resolution multichannel Sentinel-2 images), achieving Peak Signal-to-Noise Ratios (PSNRs) of 55.86, 40.60, 46.58, and 43.23 dB at 0.1 bits per pixel per band (bpppb) and 65.91, 54.28, 62.90, and 55.04 dB at 1 bpppb. We are redistributing two of these datasets, DeepExtremeCubes (2.3 Tb) and DynamicEarthNet (525 Gb), in the machine-learning-ready and cloud-ready TACO format through HuggingFace at significantly reduced sizes (270 Gb and 8.5 Gb, respectively) without compromising quality (PSNR 55.77-56.65 and 60.15). No performance loss is observed when the compressed versions of these datasets are used in their respective deep learning-based downstream tasks (next step reflectance prediction and landcover segmentation). In conclusion, xarrayvideo presents an efficient solution for handling the rapidly growing size of Earth observation datasets, making advanced compression techniques accessible and practical to the Earth science community. The library is available for use at https://github.com/IPL-UV/xarrayvideo", "AI": {"tldr": "xarrayvideo\u662f\u4e00\u4e2aPython\u5e93\uff0c\u5229\u7528\u89c6\u9891\u538b\u7f29\u6280\u672f\u9ad8\u6548\u538b\u7f29\u591a\u901a\u9053\u65f6\u7a7a\u6570\u636e\u96c6\uff0c\u538b\u7f29\u6bd4\u9ad8\u8fbe250\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u3002", "motivation": "\u968f\u7740\u5730\u7403\u89c2\u6d4b\u6570\u636e\u96c6\u89c4\u6a21\u7684\u5feb\u901f\u589e\u957f\uff0c\u9700\u8981\u9ad8\u6548\u7684\u538b\u7f29\u65b9\u6cd5\u6765\u5904\u7406\u8fd9\u4e9b\u6570\u636e\u3002", "method": "\u901a\u8fc7ffmpeg\u5229\u7528\u6807\u51c6\u89c6\u9891\u7f16\u89e3\u7801\u5668\u5c06\u591a\u901a\u9053\u65f6\u7a7a\u6570\u636e\u96c6\u7f16\u7801\u4e3a\u89c6\u9891\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u538b\u7f29\u6bd4\uff08\u5982250\u500d\uff09\u548c\u9ad8\u4fdd\u771f\u5ea6\uff08PSNR 55.86-65.91 dB\uff09\u3002", "conclusion": "xarrayvideo\u4e3a\u5730\u7403\u79d1\u5b66\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u538b\u7f29\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u3002"}}
{"id": "2506.19658", "pdf": "https://arxiv.org/pdf/2506.19658", "abs": "https://arxiv.org/abs/2506.19658", "authors": ["Yang Xing", "Jiong Wu", "Yuheng Bu", "Kuang Gong"], "title": "SAM2-SGP: Enhancing SAM2 for Medical Image Segmentation via Support-Set Guided Prompting", "categories": ["cs.CV"], "comment": null, "summary": "Although new vision foundation models such as Segment Anything Model 2 (SAM2) have significantly enhanced zero-shot image segmentation capabilities, reliance on human-provided prompts poses significant challenges in adapting SAM2 to medical image segmentation tasks. Moreover, SAM2's performance in medical image segmentation was limited by the domain shift issue, since it was originally trained on natural images and videos. To address these challenges, we proposed SAM2 with support-set guided prompting (SAM2-SGP), a framework that eliminated the need for manual prompts. The proposed model leveraged the memory mechanism of SAM2 to generate pseudo-masks using image-mask pairs from a support set via a Pseudo-mask Generation (PMG) module. We further introduced a novel Pseudo-mask Attention (PMA) module, which used these pseudo-masks to automatically generate bounding boxes and enhance localized feature extraction by guiding attention to relevant areas. Furthermore, a low-rank adaptation (LoRA) strategy was adopted to mitigate the domain shift issue. The proposed framework was evaluated on both 2D and 3D datasets across multiple medical imaging modalities, including fundus photography, X-ray, computed tomography (CT), magnetic resonance imaging (MRI), positron emission tomography (PET), and ultrasound. The results demonstrated a significant performance improvement over state-of-the-art models, such as nnUNet and SwinUNet, as well as foundation models, such as SAM2 and MedSAM2, underscoring the effectiveness of the proposed approach. Our code is publicly available at https://github.com/astlian9/SAM_Support.", "AI": {"tldr": "SAM2-SGP\u6846\u67b6\u901a\u8fc7\u652f\u6301\u96c6\u5f15\u5bfc\u63d0\u793a\u548c\u4f4e\u79e9\u9002\u5e94\u7b56\u7565\uff0c\u89e3\u51b3\u4e86SAM2\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u624b\u52a8\u63d0\u793a\u4f9d\u8d56\u548c\u9886\u57df\u504f\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3SAM2\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u4f9d\u8d56\u4eba\u5de5\u63d0\u793a\u548c\u9886\u57df\u504f\u79fb\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faSAM2-SGP\u6846\u67b6\uff0c\u5305\u62ec\u4f2a\u63a9\u6a21\u751f\u6210\u6a21\u5757\uff08PMG\uff09\u548c\u4f2a\u63a9\u6a21\u6ce8\u610f\u529b\u6a21\u5757\uff08PMA\uff09\uff0c\u5e76\u91c7\u7528\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u7b56\u7565\u3002", "result": "\u5728\u591a\u79cd\u533b\u5b66\u5f71\u50cf\u6a21\u6001\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff08\u5982nnUNet\u3001SwinUNet\u3001SAM2\u548cMedSAM2\uff09\u3002", "conclusion": "SAM2-SGP\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u8d8a\u4e14\u4ee3\u7801\u5f00\u6e90\u3002"}}
{"id": "2506.19798", "pdf": "https://arxiv.org/pdf/2506.19798", "abs": "https://arxiv.org/abs/2506.19798", "authors": ["Junwei Zhou", "Xueting Li", "Lu Qi", "Ming-Hsuan Yang"], "title": "CoCo4D: Comprehensive and Complex 4D Scene Generation", "categories": ["cs.CV"], "comment": "16 pages,10 figures", "summary": "Existing 4D synthesis methods primarily focus on object-level generation or dynamic scene synthesis with limited novel views, restricting their ability to generate multi-view consistent and immersive dynamic 4D scenes. To address these constraints, we propose a framework (dubbed as CoCo4D) for generating detailed dynamic 4D scenes from text prompts, with the option to include images. Our method leverages the crucial observation that articulated motion typically characterizes foreground objects, whereas background alterations are less pronounced. Consequently, CoCo4D divides 4D scene synthesis into two responsibilities: modeling the dynamic foreground and creating the evolving background, both directed by a reference motion sequence. Given a text prompt and an optional reference image, CoCo4D first generates an initial motion sequence utilizing video diffusion models. This motion sequence then guides the synthesis of both the dynamic foreground object and the background using a novel progressive outpainting scheme. To ensure seamless integration of the moving foreground object within the dynamic background, CoCo4D optimizes a parametric trajectory for the foreground, resulting in realistic and coherent blending. Extensive experiments show that CoCo4D achieves comparable or superior performance in 4D scene generation compared to existing methods, demonstrating its effectiveness and efficiency. More results are presented on our website https://colezwhy.github.io/coco4d/.", "AI": {"tldr": "CoCo4D\u662f\u4e00\u4e2a\u4ece\u6587\u672c\u63d0\u793a\u751f\u6210\u52a8\u60014D\u573a\u666f\u7684\u6846\u67b6\uff0c\u652f\u6301\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u6c89\u6d78\u611f\uff0c\u901a\u8fc7\u5206\u79bb\u52a8\u6001\u524d\u666f\u548c\u80cc\u666f\u4f18\u5316\u5408\u6210\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u73b0\u67094D\u5408\u6210\u65b9\u6cd5\u5728\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u52a8\u6001\u573a\u666f\u751f\u6210\u4e0a\u7684\u5c40\u9650\u6027\u3002", "method": "\u5229\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u521d\u59cb\u8fd0\u52a8\u5e8f\u5217\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u5916\u63a8\u65b9\u6848\u5408\u6210\u52a8\u6001\u524d\u666f\u548c\u80cc\u666f\uff0c\u5e76\u4f18\u5316\u524d\u666f\u8f68\u8ff9\u4ee5\u5b9e\u73b0\u65e0\u7f1d\u878d\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCoCo4D\u57284D\u573a\u666f\u751f\u6210\u4e2d\u6027\u80fd\u4f18\u4e8e\u6216\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "CoCo4D\u6709\u6548\u4e14\u9ad8\u6548\u5730\u751f\u6210\u4e86\u591a\u89c6\u89d2\u4e00\u81f4\u7684\u52a8\u60014D\u573a\u666f\u3002"}}
{"id": "2506.19833", "pdf": "https://arxiv.org/pdf/2506.19833", "abs": "https://arxiv.org/abs/2506.19833", "authors": ["Yubo Huang", "Weiqiang Wang", "Sirui Zhao", "Tong Xu", "Lin Liu", "Enhong Chen"], "title": "Bind-Your-Avatar: Multi-Talking-Character Video Generation with Dynamic 3D-mask-based Embedding Router", "categories": ["cs.CV"], "comment": null, "summary": "Recent years have witnessed remarkable advances in audio-driven talking head generation. However, existing approaches predominantly focus on single-character scenarios. While some methods can create separate conversation videos between two individuals, the critical challenge of generating unified conversation videos with multiple physically co-present characters sharing the same spatial environment remains largely unaddressed. This setting presents two key challenges: audio-to-character correspondence control and the lack of suitable datasets featuring multi-character talking videos within the same scene. To address these challenges, we introduce Bind-Your-Avatar, an MM-DiT-based model specifically designed for multi-talking-character video generation in the same scene. Specifically, we propose (1) A novel framework incorporating a fine-grained Embedding Router that binds `who' and `speak what' together to address the audio-to-character correspondence control. (2) Two methods for implementing a 3D-mask embedding router that enables frame-wise, fine-grained control of individual characters, with distinct loss functions based on observed geometric priors and a mask refinement strategy to enhance the accuracy and temporal smoothness of the predicted masks. (3) The first dataset, to the best of our knowledge, specifically constructed for multi-talking-character video generation, and accompanied by an open-source data processing pipeline, and (4) A benchmark for the dual-talking-characters video generation, with extensive experiments demonstrating superior performance over multiple state-of-the-art methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBind-Your-Avatar\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u540c\u4e00\u573a\u666f\u4e2d\u591a\u89d2\u8272\u5bf9\u8bdd\u89c6\u9891\uff0c\u89e3\u51b3\u4e86\u97f3\u9891\u4e0e\u89d2\u8272\u5bf9\u5e94\u63a7\u5236\u7684\u6311\u6218\uff0c\u5e76\u63d0\u4f9b\u4e86\u9996\u4e2a\u591a\u89d2\u8272\u5bf9\u8bdd\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u89d2\u8272\u573a\u666f\uff0c\u800c\u591a\u89d2\u8272\u5728\u540c\u4e00\u7a7a\u95f4\u73af\u5883\u4e2d\u7684\u5bf9\u8bdd\u89c6\u9891\u751f\u6210\u95ee\u9898\u5c1a\u672a\u89e3\u51b3\uff0c\u5c24\u5176\u662f\u97f3\u9891\u4e0e\u89d2\u8272\u5bf9\u5e94\u63a7\u5236\u548c\u7f3a\u4e4f\u76f8\u5173\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eMM-DiT\u7684\u6a21\u578b\uff0c\u5305\u62ec\u7ec6\u7c92\u5ea6Embedding Router\u30013D-mask\u5d4c\u5165\u8def\u7531\u5668\u548c\u63a9\u7801\u4f18\u5316\u7b56\u7565\uff0c\u5e76\u6784\u5efa\u4e86\u9996\u4e2a\u591a\u89d2\u8272\u5bf9\u8bdd\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u53cc\u89d2\u8272\u5bf9\u8bdd\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "Bind-Your-Avatar\u4e3a\u591a\u89d2\u8272\u5bf9\u8bdd\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u586b\u8865\u4e86\u6570\u636e\u96c6\u7a7a\u767d\u3002"}}
{"id": "2506.19838", "pdf": "https://arxiv.org/pdf/2506.19838", "abs": "https://arxiv.org/abs/2506.19838", "authors": ["Liangbin Xie", "Yu Li", "Shian Du", "Menghan Xia", "Xintao Wang", "Fanghua Yu", "Ziyan Chen", "Pengfei Wan", "Jiantao Zhou", "Chao Dong"], "title": "SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution", "categories": ["cs.CV"], "comment": "Project webpage available at https://simplegvr.github.io/", "summary": "Latent diffusion models have emerged as a leading paradigm for efficient video generation. However, as user expectations shift toward higher-resolution outputs, relying solely on latent computation becomes inadequate. A promising approach involves decoupling the process into two stages: semantic content generation and detail synthesis. The former employs a computationally intensive base model at lower resolutions, while the latter leverages a lightweight cascaded video super-resolution (VSR) model to achieve high-resolution output. In this work, we focus on studying key design principles for latter cascaded VSR models, which are underexplored currently. First, we propose two degradation strategies to generate training pairs that better mimic the output characteristics of the base model, ensuring alignment between the VSR model and its upstream generator. Second, we provide critical insights into VSR model behavior through systematic analysis of (1) timestep sampling strategies, (2) noise augmentation effects on low-resolution (LR) inputs. These findings directly inform our architectural and training innovations. Finally, we introduce interleaving temporal unit and sparse local attention to achieve efficient training and inference, drastically reducing computational overhead. Extensive experiments demonstrate the superiority of our framework over existing methods, with ablation studies confirming the efficacy of each design choice. Our work establishes a simple yet effective baseline for cascaded video super-resolution generation, offering practical insights to guide future advancements in efficient cascaded synthesis systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u89c6\u9891\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u79bb\u8bed\u4e49\u5185\u5bb9\u751f\u6210\u548c\u7ec6\u8282\u5408\u6210\uff0c\u91cd\u70b9\u7814\u7a76\u4e86\u8f7b\u91cf\u7ea7\u7ea7\u8054\u89c6\u9891\u8d85\u5206\u8fa8\u7387\uff08VSR\uff09\u6a21\u578b\u7684\u8bbe\u8ba1\u539f\u5219\u3002", "motivation": "\u968f\u7740\u7528\u6237\u5bf9\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u751f\u6210\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4ec5\u4f9d\u8d56\u6f5c\u5728\u8ba1\u7b97\u5df2\u4e0d\u8db3\uff0c\u9700\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u9000\u5316\u7b56\u7565\u751f\u6210\u8bad\u7ec3\u5bf9\uff0c\u5206\u6790\u65f6\u95f4\u6b65\u91c7\u6837\u7b56\u7565\u548c\u566a\u58f0\u589e\u5f3a\u6548\u679c\uff0c\u5e76\u5f15\u5165\u4ea4\u9519\u65f6\u95f4\u5355\u5143\u548c\u7a00\u758f\u5c40\u90e8\u6ce8\u610f\u529b\u4ee5\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u8bbe\u8ba1\u9009\u62e9\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7ea7\u8054\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u751f\u6210\u63d0\u4f9b\u4e86\u7b80\u5355\u6709\u6548\u7684\u57fa\u7ebf\uff0c\u4e3a\u672a\u6765\u9ad8\u6548\u7ea7\u8054\u5408\u6210\u7cfb\u7edf\u7684\u8fdb\u6b65\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2506.19839", "pdf": "https://arxiv.org/pdf/2506.19839", "abs": "https://arxiv.org/abs/2506.19839", "authors": ["Moayed Haji-Ali", "Willi Menapace", "Ivan Skorokhodov", "Arpit Sahni", "Sergey Tulyakov", "Vicente Ordonez", "Aliaksandr Siarohin"], "title": "Improving Progressive Generation with Decomposable Flow Matching", "categories": ["cs.CV", "cs.AI"], "comment": "Project Webpage: https://snap-research.github.io/dfm/", "summary": "Generating high-dimensional visual modalities is a computationally intensive task. A common solution is progressive generation, where the outputs are synthesized in a coarse-to-fine spectral autoregressive manner. While diffusion models benefit from the coarse-to-fine nature of denoising, explicit multi-stage architectures are rarely adopted. These architectures have increased the complexity of the overall approach, introducing the need for a custom diffusion formulation, decomposition-dependent stage transitions, add-hoc samplers, or a model cascade. Our contribution, Decomposable Flow Matching (DFM), is a simple and effective framework for the progressive generation of visual media. DFM applies Flow Matching independently at each level of a user-defined multi-scale representation (such as Laplacian pyramid). As shown by our experiments, our approach improves visual quality for both images and videos, featuring superior results compared to prior multistage frameworks. On Imagenet-1k 512px, DFM achieves 35.2% improvements in FDD scores over the base architecture and 26.4% over the best-performing baseline, under the same training compute. When applied to finetuning of large models, such as FLUX, DFM shows faster convergence speed to the training distribution. Crucially, all these advantages are achieved with a single model, architectural simplicity, and minimal modifications to existing training pipelines.", "AI": {"tldr": "Decomposable Flow Matching (DFM) \u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6e10\u8fdb\u5f0f\u751f\u6210\u89c6\u89c9\u5a92\u4f53\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u72ec\u7acb\u5e94\u7528 Flow Matching \u5728\u591a\u5c3a\u5ea6\u8868\u793a\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u548c\u89c6\u9891\u7684\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u9ad8\u7ef4\u89c6\u89c9\u6a21\u6001\u751f\u6210\u7684\u8ba1\u7b97\u5bc6\u96c6\u578b\u95ee\u9898\uff0c\u907f\u514d\u73b0\u6709\u591a\u9636\u6bb5\u67b6\u6784\u7684\u590d\u6742\u6027\u548c\u989d\u5916\u9700\u6c42\u3002", "method": "DFM \u5728\u7528\u6237\u5b9a\u4e49\u7684\u591a\u5c3a\u5ea6\u8868\u793a\uff08\u5982\u62c9\u666e\u62c9\u65af\u91d1\u5b57\u5854\uff09\u7684\u6bcf\u4e2a\u5c42\u7ea7\u4e0a\u72ec\u7acb\u5e94\u7528 Flow Matching\u3002", "result": "\u5728 Imagenet-1k 512px \u4e0a\uff0cDFM \u7684 FDD \u5206\u6570\u6bd4\u57fa\u7840\u67b6\u6784\u63d0\u9ad8\u4e86 35.2%\uff0c\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u9ad8\u4e86 26.4%\uff0c\u4e14\u5728\u5927\u578b\u6a21\u578b\u5fae\u8c03\u4e2d\u6536\u655b\u66f4\u5feb\u3002", "conclusion": "DFM \u901a\u8fc7\u5355\u4e00\u6a21\u578b\u3001\u67b6\u6784\u7b80\u5355\u6027\u548c\u5bf9\u73b0\u6709\u8bad\u7ec3\u6d41\u7a0b\u7684\u6700\u5c0f\u4fee\u6539\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2506.19840", "pdf": "https://arxiv.org/pdf/2506.19840", "abs": "https://arxiv.org/abs/2506.19840", "authors": ["Zekun Li", "Rui Zhou", "Rahul Sajnani", "Xiaoyan Cong", "Daniel Ritchie", "Srinath Sridhar"], "title": "GenHSI: Controllable Generation of Human-Scene Interaction Videos", "categories": ["cs.CV"], "comment": null, "summary": "Large-scale pre-trained video diffusion models have exhibited remarkable capabilities in diverse video generation. However, existing solutions face several challenges in using these models to generate long movie-like videos with rich human-object interactions that include unrealistic human-scene interaction, lack of subject identity preservation, and require expensive training. We propose GenHSI, a training-free method for controllable generation of long human-scene interaction videos (HSI). Taking inspiration from movie animation, our key insight is to overcome the limitations of previous work by subdividing the long video generation task into three stages: (1) script writing, (2) pre-visualization, and (3) animation. Given an image of a scene, a user description, and multiple images of a person, we use these three stages to generate long-videos that preserve human-identity and provide rich human-scene interactions. Script writing converts complex human tasks into simple atomic tasks that are used in the pre-visualization stage to generate 3D keyframes (storyboards). These 3D keyframes are rendered and animated by off-the-shelf video diffusion models for consistent long video generation with rich contacts in a 3D-aware manner. A key advantage of our work is that we alleviate the need for scanned, accurate scenes and create 3D keyframes from single-view images. We are the first to generate a long video sequence with a consistent camera pose that contains arbitrary numbers of character actions without training. Experiments demonstrate that our method can generate long videos that effectively preserve scene content and character identity with plausible human-scene interaction from a single image scene. Visit our project homepage https://kunkun0w0.github.io/project/GenHSI/ for more information.", "AI": {"tldr": "GenHSI\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u957f\u65f6\u95f4\u7684\u4eba\u7c7b-\u573a\u666f\u4ea4\u4e92\u89c6\u9891\uff08HSI\uff09\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u5904\u7406\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u957f\u7535\u5f71\u5f0f\u89c6\u9891\u65f6\u9762\u4e34\u4eba\u7c7b-\u573a\u666f\u4ea4\u4e92\u4e0d\u771f\u5b9e\u3001\u4e3b\u4f53\u8eab\u4efd\u4fdd\u5b58\u4e0d\u8db3\u4ee5\u53ca\u8bad\u7ec3\u6210\u672c\u9ad8\u7b49\u95ee\u9898\u3002", "method": "GenHSI\u5c06\u957f\u89c6\u9891\u751f\u6210\u4efb\u52a1\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u811a\u672c\u7f16\u5199\u3001\u9884\u89c6\u89c9\u5316\u548c\u52a8\u753b\u3002\u5229\u7528\u573a\u666f\u56fe\u50cf\u3001\u7528\u6237\u63cf\u8ff0\u548c\u4eba\u7269\u56fe\u50cf\u751f\u62103D\u5173\u952e\u5e27\uff0c\u5e76\u901a\u8fc7\u73b0\u6210\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u6e32\u67d3\u52a8\u753b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGenHSI\u80fd\u751f\u6210\u4fdd\u7559\u573a\u666f\u5185\u5bb9\u548c\u89d2\u8272\u8eab\u4efd\u7684\u957f\u89c6\u9891\uff0c\u4e14\u5177\u6709\u5408\u7406\u7684\u4eba\u7c7b-\u573a\u666f\u4ea4\u4e92\u3002", "conclusion": "GenHSI\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u751f\u6210\u957f\u89c6\u9891\u5e8f\u5217\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u591a\u4e2a\u6311\u6218\u3002"}}
{"id": "2506.19845", "pdf": "https://arxiv.org/pdf/2506.19845", "abs": "https://arxiv.org/abs/2506.19845", "authors": ["Vladislav Esaulov", "M. Moein Esfahani"], "title": "A Comparative Study of NAFNet Baselines for Image Restoration", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We study NAFNet (Nonlinear Activation Free Network), a simple and efficient deep learning baseline for image restoration. By using CIFAR10 images corrupted with noise and blur, we conduct an ablation study of NAFNet's core components. Our baseline model implements SimpleGate activation, Simplified Channel Activation (SCA), and LayerNormalization. We compare this baseline to different variants that replace or remove components. Quantitative results (PSNR, SSIM) and examples illustrate how each modification affects restoration performance. Our findings support the NAFNet design: the SimpleGate and simplified attention mechanisms yield better results than conventional activations and attention, while LayerNorm proves to be important for stable training. We conclude with recommendations for model design, discuss potential improvements, and future work.", "AI": {"tldr": "NAFNet\u662f\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u56fe\u50cf\u4fee\u590d\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6838\u5fc3\u7ec4\u4ef6\uff08SimpleGate\u6fc0\u6d3b\u3001SCA\u548cLayerNorm\uff09\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76NAFNet\u7684\u6838\u5fc3\u7ec4\u4ef6\u5bf9\u56fe\u50cf\u4fee\u590d\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u9a8c\u8bc1\u5176\u8bbe\u8ba1\u5408\u7406\u6027\u3002", "method": "\u4f7f\u7528CIFAR10\u566a\u58f0\u548c\u6a21\u7cca\u56fe\u50cf\u8fdb\u884c\u6d88\u878d\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e0d\u540c\u7ec4\u4ef6\u66ff\u6362\u6216\u79fb\u9664\u540e\u7684\u6548\u679c\u3002", "result": "SimpleGate\u548c\u7b80\u5316\u6ce8\u610f\u529b\u673a\u5236\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0cLayerNorm\u5bf9\u8bad\u7ec3\u7a33\u5b9a\u6027\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u652f\u6301NAFNet\u8bbe\u8ba1\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2506.19852", "pdf": "https://arxiv.org/pdf/2506.19852", "abs": "https://arxiv.org/abs/2506.19852", "authors": ["Xingyang Li", "Muyang Li", "Tianle Cai", "Haocheng Xi", "Shuo Yang", "Yujun Lin", "Lvmin Zhang", "Songlin Yang", "Jinbo Hu", "Kelly Peng", "Maneesh Agrawala", "Ion Stoica", "Kurt Keutzer", "Song Han"], "title": "Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Code: https://github.com/mit-han-lab/radial-attention", "summary": "Recent advances in diffusion models have enabled high-quality video generation, but the additional temporal dimension significantly increases computational costs, making training and inference on long videos prohibitively expensive. In this paper, we identify a phenomenon we term Spatiotemporal Energy Decay in video diffusion models: post-softmax attention scores diminish as spatial and temporal distance between tokens increase, akin to the physical decay of signal or waves over space and time in nature. Motivated by this, we propose Radial Attention, a scalable sparse attention mechanism with $O(n \\log n)$ complexity that translates energy decay into exponentially decaying compute density, which is significantly more efficient than standard $O(n^2)$ dense attention and more expressive than linear attention. Specifically, Radial Attention employs a simple, static attention mask where each token attends to spatially nearby tokens, with the attention window size shrinking with temporal distance. Moreover, it allows pre-trained video diffusion models to extend their generation length with efficient LoRA-based fine-tuning. Extensive experiments show that Radial Attention maintains video quality across Wan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9$\\times$ speedup over the original dense attention. With minimal tuning, it enables video generation up to 4$\\times$ longer while reducing training costs by up to 4.4$\\times$ compared to direct fine-tuning and accelerating inference by up to 3.7$\\times$ compared to dense attention inference.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRadial Attention\uff0c\u4e00\u79cd\u9ad8\u6548\u7684\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u6a21\u62df\u65f6\u7a7a\u80fd\u91cf\u8870\u51cf\u73b0\u8c61\uff0c\u663e\u8457\u964d\u4f4e\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u9650\u5236\u4e86\u957f\u89c6\u9891\u7684\u751f\u6210\uff0c\u7814\u7a76\u53d1\u73b0\u65f6\u7a7a\u6ce8\u610f\u529b\u5206\u6570\u968f\u8ddd\u79bb\u8870\u51cf\u7684\u73b0\u8c61\uff0c\u542f\u53d1\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u6ce8\u610f\u529b\u673a\u5236\u3002", "method": "\u63d0\u51faRadial Attention\uff0c\u5229\u7528\u9759\u6001\u6ce8\u610f\u529b\u63a9\u7801\uff0c\u4f7f\u6bcf\u4e2atoken\u4ec5\u5173\u6ce8\u7a7a\u95f4\u90bb\u8fd1\u7684token\uff0c\u4e14\u7a97\u53e3\u5927\u5c0f\u968f\u65f6\u95f4\u8ddd\u79bb\u7f29\u5c0f\uff0c\u590d\u6742\u5ea6\u4e3aO(n log n)\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRadial Attention\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u89c6\u9891\u8d28\u91cf\uff0c\u8ba1\u7b97\u901f\u5ea6\u63d0\u53471.9\u500d\uff0c\u652f\u6301\u751f\u6210\u957f\u5ea64\u500d\u7684\u89c6\u9891\uff0c\u8bad\u7ec3\u6210\u672c\u964d\u4f4e4.4\u500d\uff0c\u63a8\u7406\u52a0\u901f3.7\u500d\u3002", "conclusion": "Radial Attention\u901a\u8fc7\u6a21\u62df\u80fd\u91cf\u8870\u51cf\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u957f\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2506.19360", "pdf": "https://arxiv.org/pdf/2506.19360", "abs": "https://arxiv.org/abs/2506.19360", "authors": ["Yunsung Chung", "Yunbei Zhang", "Nassir Marrouche", "Jihun Hamm"], "title": "SoK: Can Synthetic Images Replace Real Data? A Survey of Utility and Privacy of Synthetic Image Generation", "categories": ["cs.CR", "cs.CV"], "comment": "Accepted at the 34th USENIX Security Symposium (USENIX Security '25).   21 pages, plus a 6-page appendix", "summary": "Advances in generative models have transformed the field of synthetic image generation for privacy-preserving data synthesis (PPDS). However, the field lacks a comprehensive survey and comparison of synthetic image generation methods across diverse settings. In particular, when we generate synthetic images for the purpose of training a classifier, there is a pipeline of generation-sampling-classification which takes private training as input and outputs the final classifier of interest. In this survey, we systematically categorize existing image synthesis methods, privacy attacks, and mitigations along this generation-sampling-classification pipeline. To empirically compare diverse synthesis approaches, we provide a benchmark with representative generative methods and use model-agnostic membership inference attacks (MIAs) as a measure of privacy risk. Through this study, we seek to answer critical questions in PPDS: Can synthetic data effectively replace real data? Which release strategy balances utility and privacy? Do mitigations improve the utility-privacy tradeoff? Which generative models perform best across different scenarios? With a systematic evaluation of diverse methods, our study provides actionable insights into the utility-privacy tradeoffs of synthetic data generation methods and guides the decision on optimal data releasing strategies for real-world applications.", "AI": {"tldr": "\u672c\u6587\u5bf9\u9690\u79c1\u4fdd\u62a4\u6570\u636e\u5408\u6210\uff08PPDS\uff09\u4e2d\u7684\u5408\u6210\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u5206\u7c7b\u548c\u6bd4\u8f83\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u4ee5\u8bc4\u4f30\u4e0d\u540c\u65b9\u6cd5\u7684\u9690\u79c1\u98ce\u9669\u548c\u6548\u7528\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5bf9\u5408\u6210\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u7684\u5168\u9762\u8c03\u67e5\u548c\u6bd4\u8f83\uff0c\u5c24\u5176\u662f\u5728\u8bad\u7ec3\u5206\u7c7b\u5668\u65f6\u5982\u4f55\u5e73\u8861\u6548\u7528\u548c\u9690\u79c1\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u7c7b\u73b0\u6709\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\u3001\u9690\u79c1\u653b\u51fb\u548c\u7f13\u89e3\u63aa\u65bd\uff0c\u5e76\u4f7f\u7528\u6a21\u578b\u65e0\u5173\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\uff08MIA\uff09\u4f5c\u4e3a\u9690\u79c1\u98ce\u9669\u8861\u91cf\u6807\u51c6\u3002", "result": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5173\u4e8e\u5408\u6210\u6570\u636e\u662f\u5426\u80fd\u66ff\u4ee3\u771f\u5b9e\u6570\u636e\u3001\u6700\u4f73\u53d1\u5e03\u7b56\u7565\u4ee5\u53ca\u4e0d\u540c\u751f\u6210\u6a21\u578b\u8868\u73b0\u7684\u5b9e\u8bc1\u7ed3\u679c\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u7684\u6548\u7528-\u9690\u79c1\u6743\u8861\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u5e76\u6307\u5bfc\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6570\u636e\u53d1\u5e03\u7b56\u7565\u3002"}}
{"id": "2506.19387", "pdf": "https://arxiv.org/pdf/2506.19387", "abs": "https://arxiv.org/abs/2506.19387", "authors": ["Khuram Naveed", "Bruna Neves de Freitas", "Ruben Pauwels"], "title": "NAADA: A Noise-Aware Attention Denoising Autoencoder for Dental Panoramic Radiographs", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "10 pages, 8 figures", "summary": "Convolutional denoising autoencoders (DAEs) are powerful tools for image restoration. However, they inherit a key limitation of convolutional neural networks (CNNs): they tend to recover low-frequency features, such as smooth regions, more effectively than high-frequency details. This leads to the loss of fine details, which is particularly problematic in dental radiographs where preserving subtle anatomical structures is crucial. While self-attention mechanisms can help mitigate this issue by emphasizing important features, conventional attention methods often prioritize features corresponding to cleaner regions and may overlook those obscured by noise. To address this limitation, we propose a noise-aware self-attention method, which allows the model to effectively focus on and recover key features even within noisy regions. Building on this approach, we introduce the noise-aware attention-enhanced denoising autoencoder (NAADA) network for enhancing noisy panoramic dental radiographs. Compared with the recent state of the art (and much heavier) methods like Uformer, MResDNN etc., our method improves the reconstruction of fine details, ensuring better image quality and diagnostic accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u566a\u58f0\u611f\u77e5\u81ea\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u5377\u79ef\u53bb\u566a\u81ea\u7f16\u7801\u5668\u5728\u7259\u79d1X\u5149\u56fe\u50cf\u4e2d\u7684\u9ad8\u9891\u7ec6\u8282\u6062\u590d\u80fd\u529b\u3002", "motivation": "\u5377\u79ef\u53bb\u566a\u81ea\u7f16\u7801\u5668\u5728\u56fe\u50cf\u6062\u590d\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u503e\u5411\u4e8e\u6062\u590d\u4f4e\u9891\u7279\u5f81\uff0c\u5bfc\u81f4\u9ad8\u9891\u7ec6\u8282\u4e22\u5931\uff0c\u8fd9\u5728\u7259\u79d1X\u5149\u56fe\u50cf\u4e2d\u5c24\u4e3a\u5173\u952e\u3002\u4f20\u7edf\u6ce8\u610f\u529b\u65b9\u6cd5\u53ef\u80fd\u5ffd\u7565\u566a\u58f0\u533a\u57df\u7684\u91cd\u8981\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u566a\u58f0\u611f\u77e5\u81ea\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efa\u566a\u58f0\u611f\u77e5\u6ce8\u610f\u529b\u589e\u5f3a\u53bb\u566a\u81ea\u7f16\u7801\u5668\uff08NAADA\uff09\u7f51\u7edc\uff0c\u7528\u4e8e\u589e\u5f3a\u566a\u58f0\u7259\u79d1\u5168\u666fX\u5149\u56fe\u50cf\u3002", "result": "\u4e0e\u73b0\u6709\u65b9\u6cd5\uff08\u5982Uformer\u3001MResDNN\u7b49\uff09\u76f8\u6bd4\uff0cNAADA\u80fd\u66f4\u597d\u5730\u91cd\u5efa\u7ec6\u8282\uff0c\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548c\u8bca\u65ad\u51c6\u786e\u6027\u3002", "conclusion": "\u566a\u58f0\u611f\u77e5\u81ea\u6ce8\u610f\u529b\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u9891\u7ec6\u8282\u6062\u590d\u95ee\u9898\uff0c\u4e3a\u7259\u79d1X\u5149\u56fe\u50cf\u7684\u53bb\u566a\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.19455", "pdf": "https://arxiv.org/pdf/2506.19455", "abs": "https://arxiv.org/abs/2506.19455", "authors": ["Zhifeng Wang", "Renjiao Yi", "Xin Wen", "Chenyang Zhu", "Kai Xu", "Kunlun He"], "title": "Angio-Diff: Learning a Self-Supervised Adversarial Diffusion Model for Angiographic Geometry Generation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Vascular diseases pose a significant threat to human health, with X-ray angiography established as the gold standard for diagnosis, allowing for detailed observation of blood vessels. However, angiographic X-rays expose personnel and patients to higher radiation levels than non-angiographic X-rays, which are unwanted. Thus, modality translation from non-angiographic to angiographic X-rays is desirable. Data-driven deep approaches are hindered by the lack of paired large-scale X-ray angiography datasets. While making high-quality vascular angiography synthesis crucial, it remains challenging. We find that current medical image synthesis primarily operates at pixel level and struggles to adapt to the complex geometric structure of blood vessels, resulting in unsatisfactory quality of blood vessel image synthesis, such as disconnections or unnatural curvatures. To overcome this issue, we propose a self-supervised method via diffusion models to transform non-angiographic X-rays into angiographic X-rays, mitigating data shortages for data-driven approaches. Our model comprises a diffusion model that learns the distribution of vascular data from diffusion latent, a generator for vessel synthesis, and a mask-based adversarial module. To enhance geometric accuracy, we propose a parametric vascular model to fit the shape and distribution of blood vessels. The proposed method contributes a pipeline and a synthetic dataset for X-ray angiography. We conducted extensive comparative and ablation experiments to evaluate the Angio-Diff. The results demonstrate that our method achieves state-of-the-art performance in synthetic angiography image quality and more accurately synthesizes the geometric structure of blood vessels. The code is available at https://github.com/zfw-cv/AngioDiff.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u5c06\u975e\u8840\u7ba1\u9020\u5f71X\u5c04\u7ebf\u8f6c\u6362\u4e3a\u8840\u7ba1\u9020\u5f71X\u5c04\u7ebf\uff0c\u89e3\u51b3\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff0c\u63d0\u5347\u8840\u7ba1\u51e0\u4f55\u7ed3\u6784\u5408\u6210\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u8840\u7ba1\u9020\u5f71X\u5c04\u7ebf\u8bca\u65ad\u8f90\u5c04\u9ad8\uff0c\u975e\u8840\u7ba1\u9020\u5f71\u6570\u636e\u4e30\u5bcc\u4f46\u7f3a\u4e4f\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8840\u7ba1\u51e0\u4f55\u7ed3\u6784\u5408\u6210\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u7ed3\u5408\u6269\u6563\u6a21\u578b\u5b66\u4e60\u8840\u7ba1\u6570\u636e\u5206\u5e03\uff0c\u751f\u6210\u5668\u5408\u6210\u8840\u7ba1\uff0c\u63a9\u7801\u5bf9\u6297\u6a21\u5757\u63d0\u5347\u8d28\u91cf\uff0c\u5f15\u5165\u53c2\u6570\u5316\u8840\u7ba1\u6a21\u578b\u589e\u5f3a\u51e0\u4f55\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAngio-Diff\u5728\u5408\u6210\u8840\u7ba1\u9020\u5f71\u56fe\u50cf\u8d28\u91cf\u548c\u51e0\u4f55\u7ed3\u6784\u51c6\u786e\u6027\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8840\u7ba1\u9020\u5f71\u5408\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u548c\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5408\u6210\u8d28\u91cf\u3002"}}
{"id": "2506.19491", "pdf": "https://arxiv.org/pdf/2506.19491", "abs": "https://arxiv.org/abs/2506.19491", "authors": ["Gen\u00eds Castillo G\u00f3mez-Raya", "\u00c1lmos Veres-Vit\u00e1lyos", "Filip Lemic", "Pablo Royo", "Mario Montagud", "Sergi Fern\u00e1ndez", "Sergi Abadal", "Xavier Costa-P\u00e9rez"], "title": "Experimental Assessment of Neural 3D Reconstruction for Small UAV-based Applications", "categories": ["cs.ET", "cs.AI", "cs.CV", "cs.NI", "eess.IV"], "comment": "6 pages, 7 figures, 2 tables, accepted at IEEE International   Symposium on Personal, Indoor and Mobile Radio Communications 2025", "summary": "The increasing miniaturization of Unmanned Aerial Vehicles (UAVs) has expanded their deployment potential to indoor and hard-to-reach areas. However, this trend introduces distinct challenges, particularly in terms of flight dynamics and power consumption, which limit the UAVs' autonomy and mission capabilities. This paper presents a novel approach to overcoming these limitations by integrating Neural 3D Reconstruction (N3DR) with small UAV systems for fine-grained 3-Dimensional (3D) digital reconstruction of small static objects. Specifically, we design, implement, and evaluate an N3DR-based pipeline that leverages advanced models, i.e., Instant-ngp, Nerfacto, and Splatfacto, to improve the quality of 3D reconstructions using images of the object captured by a fleet of small UAVs. We assess the performance of the considered models using various imagery and pointcloud metrics, comparing them against the baseline Structure from Motion (SfM) algorithm. The experimental results demonstrate that the N3DR-enhanced pipeline significantly improves reconstruction quality, making it feasible for small UAVs to support high-precision 3D mapping and anomaly detection in constrained environments. In more general terms, our results highlight the potential of N3DR in advancing the capabilities of miniaturized UAV systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u795e\u7ecf3D\u91cd\u5efa\uff08N3DR\uff09\u96c6\u6210\u5230\u5c0f\u578b\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u5bf9\u9759\u6001\u5c0f\u7269\u4f53\u7684\u7cbe\u7ec63D\u91cd\u5efa\u8d28\u91cf\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edfSfM\u7b97\u6cd5\u3002", "motivation": "\u5c0f\u578b\u65e0\u4eba\u673a\u5728\u5ba4\u5185\u548c\u96be\u4ee5\u5230\u8fbe\u533a\u57df\u7684\u90e8\u7f72\u6f5c\u529b\u589e\u52a0\uff0c\u4f46\u5176\u98de\u884c\u52a8\u529b\u5b66\u548c\u529f\u8017\u95ee\u9898\u9650\u5236\u4e86\u81ea\u4e3b\u6027\u548c\u4efb\u52a1\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u5e76\u8bc4\u4f30\u4e86\u4e00\u4e2a\u57fa\u4e8eN3DR\u7684\u6d41\u7a0b\uff0c\u5229\u7528Instant-ngp\u3001Nerfacto\u548cSplatfacto\u7b49\u5148\u8fdb\u6a21\u578b\uff0c\u901a\u8fc7\u5c0f\u578b\u65e0\u4eba\u673a\u62cd\u6444\u7684\u56fe\u50cf\u63d0\u53473D\u91cd\u5efa\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cN3DR\u589e\u5f3a\u7684\u6d41\u7a0b\u663e\u8457\u63d0\u9ad8\u4e86\u91cd\u5efa\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u9ad8\u7cbe\u5ea63D\u6620\u5c04\u548c\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u3002", "conclusion": "N3DR\u6280\u672f\u6709\u671b\u63a8\u52a8\u5c0f\u578b\u65e0\u4eba\u673a\u7cfb\u7edf\u7684\u80fd\u529b\u53d1\u5c55\u3002"}}
{"id": "2506.19741", "pdf": "https://arxiv.org/pdf/2506.19741", "abs": "https://arxiv.org/abs/2506.19741", "authors": ["Yihong Luo", "Shuchen Xue", "Tianyang Hu", "Jing Tang"], "title": "Noise Consistency Training: A Native Approach for One-Step Generator in Learning Additional Controls", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": null, "summary": "The pursuit of efficient and controllable high-quality content generation remains a central challenge in artificial intelligence-generated content (AIGC). While one-step generators, enabled by diffusion distillation techniques, offer excellent generation quality and computational efficiency, adapting them to new control conditions--such as structural constraints, semantic guidelines, or external inputs--poses a significant challenge. Conventional approaches often necessitate computationally expensive modifications to the base model and subsequent diffusion distillation. This paper introduces Noise Consistency Training (NCT), a novel and lightweight approach to directly integrate new control signals into pre-trained one-step generators without requiring access to original training images or retraining the base diffusion model. NCT operates by introducing an adapter module and employs a noise consistency loss in the noise space of the generator. This loss aligns the adapted model's generation behavior across noises that are conditionally dependent to varying degrees, implicitly guiding it to adhere to the new control. Theoretically, this training objective can be understood as minimizing the distributional distance between the adapted generator and the conditional distribution induced by the new conditions. NCT is modular, data-efficient, and easily deployable, relying only on the pre-trained one-step generator and a control signal model. Extensive experiments demonstrate that NCT achieves state-of-the-art controllable generation in a single forward pass, surpassing existing multi-step and distillation-based methods in both generation quality and computational efficiency. Code is available at https://github.com/Luo-Yihong/NCT", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNoise Consistency Training (NCT)\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c06\u65b0\u7684\u63a7\u5236\u4fe1\u53f7\u76f4\u63a5\u96c6\u6210\u5230\u9884\u8bad\u7ec3\u7684\u4e00\u6b65\u751f\u6210\u5668\u4e2d\uff0c\u65e0\u9700\u539f\u59cb\u8bad\u7ec3\u56fe\u50cf\u6216\u91cd\u65b0\u8bad\u7ec3\u57fa\u7840\u6269\u6563\u6a21\u578b\u3002", "motivation": "\u9ad8\u6548\u4e14\u53ef\u63a7\u7684\u9ad8\u8d28\u91cf\u5185\u5bb9\u751f\u6210\u662fAIGC\u7684\u6838\u5fc3\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u9002\u5e94\u65b0\u63a7\u5236\u6761\u4ef6\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "NCT\u901a\u8fc7\u5f15\u5165\u9002\u914d\u5668\u6a21\u5757\u548c\u566a\u58f0\u4e00\u81f4\u6027\u635f\u5931\uff0c\u5728\u751f\u6210\u5668\u7684\u566a\u58f0\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u9690\u5f0f\u5f15\u5bfc\u6a21\u578b\u9075\u5faa\u65b0\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNCT\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u53ef\u63a7\u751f\u6210\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u591a\u6b65\u548c\u57fa\u4e8e\u84b8\u998f\u7684\u65b9\u6cd5\u3002", "conclusion": "NCT\u662f\u4e00\u79cd\u6a21\u5757\u5316\u3001\u6570\u636e\u9ad8\u6548\u4e14\u6613\u4e8e\u90e8\u7f72\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2506.19742", "pdf": "https://arxiv.org/pdf/2506.19742", "abs": "https://arxiv.org/abs/2506.19742", "authors": ["Zhuowei Xu", "Han Li", "Dai Sun", "Zhicheng Li", "Yujia Li", "Qingpeng Kong", "Zhiwei Cheng", "Nassir Navab", "S. Kevin Zhou"], "title": "NeRF-based CBCT Reconstruction needs Normalization and Initialization", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Cone Beam Computed Tomography (CBCT) is widely used in medical imaging. However, the limited number and intensity of X-ray projections make reconstruction an ill-posed problem with severe artifacts. NeRF-based methods have achieved great success in this task. However, they suffer from a local-global training mismatch between their two key components: the hash encoder and the neural network. Specifically, in each training step, only a subset of the hash encoder's parameters is used (local sparse), whereas all parameters in the neural network participate (global dense). Consequently, hash features generated in each step are highly misaligned, as they come from different subsets of the hash encoder. These misalignments from different training steps are then fed into the neural network, causing repeated inconsistent global updates in training, which leads to unstable training, slower convergence, and degraded reconstruction quality. Aiming to alleviate the impact of this local-global optimization mismatch, we introduce a Normalized Hash Encoder, which enhances feature consistency and mitigates the mismatch. Additionally, we propose a Mapping Consistency Initialization(MCI) strategy that initializes the neural network before training by leveraging the global mapping property from a well-trained model. The initialized neural network exhibits improved stability during early training, enabling faster convergence and enhanced reconstruction performance. Our method is simple yet effective, requiring only a few lines of code while substantially improving training efficiency on 128 CT cases collected from 4 different datasets, covering 7 distinct anatomical regions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f52\u4e00\u5316\u54c8\u5e0c\u7f16\u7801\u5668\u548c\u6620\u5c04\u4e00\u81f4\u6027\u521d\u59cb\u5316\u7b56\u7565\uff0c\u4ee5\u89e3\u51b3CBCT\u91cd\u5efa\u4e2d\u5c40\u90e8-\u5168\u5c40\u8bad\u7ec3\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "CBCT\u91cd\u5efa\u56e0\u6295\u5f71\u6570\u636e\u6709\u9650\u800c\u6210\u4e3a\u75c5\u6001\u95ee\u9898\uff0c\u73b0\u6709NeRF\u65b9\u6cd5\u56e0\u54c8\u5e0c\u7f16\u7801\u5668\u4e0e\u795e\u7ecf\u7f51\u7edc\u7684\u5c40\u90e8-\u5168\u5c40\u8bad\u7ec3\u4e0d\u5339\u914d\u5bfc\u81f4\u7279\u5f81\u4e0d\u4e00\u81f4\u3001\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002", "method": "\u5f15\u5165\u5f52\u4e00\u5316\u54c8\u5e0c\u7f16\u7801\u5668\u589e\u5f3a\u7279\u5f81\u4e00\u81f4\u6027\uff0c\u5e76\u63d0\u51fa\u6620\u5c04\u4e00\u81f4\u6027\u521d\u59cb\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u6a21\u578b\u521d\u59cb\u5316\u795e\u7ecf\u7f51\u7edc\u4ee5\u63d0\u5347\u65e9\u671f\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u65b9\u6cd5\u5728128\u4e2aCT\u6848\u4f8b\u4e0a\u9a8c\u8bc1\uff0c\u8986\u76d67\u4e2a\u89e3\u5256\u533a\u57df\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u91cd\u5efa\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u4ec5\u9700\u5c11\u91cf\u4ee3\u7801\u5373\u53ef\u663e\u8457\u6539\u5584CBCT\u91cd\u5efa\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u8d28\u91cf\u3002"}}
