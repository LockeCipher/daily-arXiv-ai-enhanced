{"id": "2509.00040", "pdf": "https://arxiv.org/pdf/2509.00040", "abs": "https://arxiv.org/abs/2509.00040", "authors": ["Chengkai Dai", "Tao Liu", "Dezhao Guo", "Binzhi Sun", "Guoxin Fang", "Yeung Yam", "Charlie C. L. Wang"], "title": "Curve-based slicer for multi-axis DLP 3D printing", "categories": ["cs.GR", "cs.RO"], "comment": null, "summary": "This paper introduces a novel curve-based slicing method for generating planar layers with dynamically varying orientations in digital light processing (DLP) 3D printing. Our approach effectively addresses key challenges in DLP printing, such as regions with large overhangs and staircase artifacts, while preserving its intrinsic advantages of high resolution and fast printing speeds. We formulate the slicing problem as an optimization task, in which parametric curves are computed to define both the slicing layers and the model partitioning through their tangent planes. These curves inherently define motion trajectories for the build platform and can be optimized to meet critical manufacturing objectives, including collision-free motion and floating-free deposition. We validate our method through physical experiments on a robotic multi-axis DLP printing setup, demonstrating that the optimized curves can robustly guide smooth, high-quality fabrication of complex geometries.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u66f2\u7ebf\u57fa\u5207\u7247\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u53d8\u5316\u5c42\u65b9\u5411\u6765\u89e3\u51b3DLP 3D\u6253\u5370\u4e2d\u7684\u60ac\u7a7a\u533a\u57df\u548c\u9636\u68af\u6548\u5e94\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u5206\u8fa8\u7387\u548c\u5feb\u901f\u6253\u5370\u7684\u4f18\u52bf\u3002", "motivation": "\u89e3\u51b3DLP 3D\u6253\u5370\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u5927\u89d2\u5ea6\u60ac\u7a7a\u533a\u57df\u548c\u9636\u68af\u6548\u5e94\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u5206\u8fa8\u7387\u548c\u5feb\u901f\u6253\u5370\u7684\u4f18\u52bf\u3002", "method": "\u5c06\u5207\u7247\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u4f18\u5316\u4efb\u52a1\uff0c\u901a\u8fc7\u8ba1\u7b97\u53c2\u6570\u66f2\u7ebf\u6765\u5b9a\u4e49\u5207\u7247\u5c42\u548c\u6a21\u578b\u5206\u5272\uff0c\u8fd9\u4e9b\u66f2\u7ebf\u540c\u65f6\u5b9a\u4e49\u4e86\u6784\u5efa\u5e73\u53f0\u7684\u8fd0\u52a8\u8f68\u8ff9\uff0c\u5e76\u53ef\u4ee5\u4f18\u5316\u4ee5\u6ee1\u8db3\u65e0\u78b0\u649e\u8fd0\u52a8\u548c\u65e0\u6d6e\u52a8\u6c89\u79ef\u7b49\u5236\u9020\u76ee\u6807\u3002", "result": "\u901a\u8fc7\u5728\u673a\u5668\u4eba\u591a\u8f74DLP\u6253\u5370\u8bbe\u5907\u4e0a\u8fdb\u884c\u5b9e\u9645\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u4f18\u5316\u66f2\u7ebf\u53ef\u4ee5\u7a33\u56fa\u5730\u6307\u5bfc\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u7684\u5e73\u6ed1\u9ad8\u8d28\u91cf\u5236\u9020\u3002", "conclusion": "\u8be5\u66f2\u7ebf\u57fa\u5207\u7247\u65b9\u6cd5\u4e3aDLP 3D\u6253\u5370\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u9ad8\u5206\u8fa8\u7387\u548c\u5feb\u901f\u6253\u5370\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u6709\u6548\u51cf\u5c11\u60ac\u7a7a\u95ee\u9898\u548c\u9636\u68af\u6548\u5e94\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u590d\u6742\u5f62\u72b6\u7684\u5236\u9020\u3002"}}
{"id": "2509.00052", "pdf": "https://arxiv.org/pdf/2509.00052", "abs": "https://arxiv.org/abs/2509.00052", "authors": ["Jianzhi Long", "Wenhao Sun", "Rongcheng Tu", "Dacheng Tao"], "title": "Lightning Fast Caching-based Parallel Denoising Prediction for Accelerating Talking Head Generation", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Diffusion-based talking head models generate high-quality, photorealistic videos but suffer from slow inference, limiting practical applications. Existing acceleration methods for general diffusion models fail to exploit the temporal and spatial redundancies unique to talking head generation. In this paper, we propose a task-specific framework addressing these inefficiencies through two key innovations. First, we introduce Lightning-fast Caching-based Parallel denoising prediction (LightningCP), caching static features to bypass most model layers in inference time. We also enable parallel prediction using cached features and estimated noisy latents as inputs, efficiently bypassing sequential sampling. Second, we propose Decoupled Foreground Attention (DFA) to further accelerate attention computations, exploiting the spatial decoupling in talking head videos to restrict attention to dynamic foreground regions. Additionally, we remove reference features in certain layers to bring extra speedup. Extensive experiments demonstrate that our framework significantly improves inference speed while preserving video quality.", "AI": {"tldr": "\u63d0\u51faLightningCP\u548cDFA\u4e24\u79cd\u521b\u65b0\u65b9\u6cd5\uff0c\u9488\u5bf9\u8bf4\u8bdd\u5934\u751f\u6210\u4efb\u52a1\u4f18\u5316\u6269\u6563\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u540c\u65f6\u4fdd\u6301\u89c6\u9891\u8d28\u91cf", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u52a0\u901f\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u8bf4\u8bdd\u5934\u751f\u6210\u4efb\u52a1\u7279\u6709\u7684\u65f6\u7a7a\u5197\u4f59\u6027\uff0c\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u6162\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528", "method": "1. LightningCP\uff1a\u7f13\u5b58\u9759\u6001\u7279\u5f81\uff0c\u5728\u63a8\u7406\u65f6\u7ed5\u8fc7\u5927\u591a\u6570\u6a21\u578b\u5c42\uff1b\u4f7f\u7528\u7f13\u5b58\u7279\u5f81\u548c\u4f30\u8ba1\u566a\u58f0\u6f5c\u5728\u4f5c\u4e3a\u8f93\u5165\u5b9e\u73b0\u5e76\u884c\u9884\u6d4b\uff0c\u7ed5\u8fc7\u987a\u5e8f\u91c7\u6837\u30022. DFA\uff1a\u5229\u7528\u8bf4\u8bdd\u5934\u89c6\u9891\u7684\u7a7a\u95f4\u89e3\u8026\u7279\u6027\uff0c\u5c06\u6ce8\u610f\u529b\u8ba1\u7b97\u9650\u5236\u5728\u52a8\u6001\u524d\u666f\u533a\u57df\uff1b\u5728\u67d0\u4e9b\u5c42\u4e2d\u79fb\u9664\u53c2\u8003\u7279\u5f81\u4ee5\u83b7\u5f97\u989d\u5916\u52a0\u901f", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u89c6\u9891\u8d28\u91cf", "conclusion": "\u63d0\u51fa\u7684\u4efb\u52a1\u7279\u5b9a\u6846\u67b6\u901a\u8fc7\u5229\u7528\u8bf4\u8bdd\u5934\u751f\u6210\u7684\u72ec\u7279\u7279\u6027\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.00269", "pdf": "https://arxiv.org/pdf/2509.00269", "abs": "https://arxiv.org/abs/2509.00269", "authors": ["Maria Parelli", "Michael Oechsle", "Michael Niemeyer", "Federico Tombari", "Andreas Geiger"], "title": "3D-LATTE: Latent Space 3D Editing from Textual Instructions", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Despite the recent success of multi-view diffusion models for text/image-based 3D asset generation, instruction-based editing of 3D assets lacks surprisingly far behind the quality of generation models. The main reason is that recent approaches using 2D priors suffer from view-inconsistent editing signals. Going beyond 2D prior distillation methods and multi-view editing strategies, we propose a training-free editing method that operates within the latent space of a native 3D diffusion model, allowing us to directly manipulate 3D geometry. We guide the edit synthesis by blending 3D attention maps from the generation with the source object. Coupled with geometry-aware regularization guidance, a spectral modulation strategy in the Fourier domain and a refinement step for 3D enhancement, our method outperforms previous 3D editing methods enabling high-fidelity, precise, and robust edits across a wide range of shapes and semantic manipulations.", "AI": {"tldr": "\u57fa\u4e8e\u672c\u57303D\u6db2\u5316\u6a21\u578b\u7684\u8bad\u7ec3\u514d\u8d39\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc73D\u6ce8\u610f\u529b\u5730\u56fe\u6df7\u5408\u548c\u51e0\u4f55\u610f\u8bc6\u6b63\u5219\u5316\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u7684\u4e00\u81f4\u60273D\u7f16\u8f91", "motivation": "\u5f53\u524d\u57fa\u4e8e2D\u5148\u9a8c\u76843D\u8d44\u4ea7\u7f16\u8f91\u65b9\u6cd5\u5b58\u5728\u89c6\u56fe\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u7f16\u8f91\u8d28\u91cf\u8fdc\u843d\u540e\u4e8e\u751f\u6210\u6a21\u578b", "method": "\u5728\u672c\u57303D\u6db2\u5316\u6a21\u578b\u6f5c\u7a7a\u95f4\u76f4\u63a5\u64cd\u4f5c3D\u51e0\u4f55\uff0c\u901a\u8fc73D\u6ce8\u610f\u529b\u5730\u56fe\u6df7\u5408\u3001\u51e0\u4f55\u610f\u8bc6\u6b63\u5219\u5316\u3001\u5080\u91cc\u53f6\u57df\u8c03\u5236\u7b56\u7565\u548c3D\u589e\u5f3a\u7cbe\u7ec6\u5316\u6b65\u9aa4", "result": "\u65b9\u6cd5\u5728\u5e7f\u6cdb\u7684\u5f62\u72b6\u548c\u8bed\u4e49\u64cd\u4f5c\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u3001\u7cbe\u786e\u548c\u7a33\u5065\u7684\u7f16\u8f91\u6548\u679c\uff0c\u8d85\u8d8a\u4e86\u4e4b\u524d\u76843D\u7f16\u8f91\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u76f4\u63a5\u64cd\u4f5c3D\u6db2\u5316\u6a21\u578b\u7684\u672c\u5730\u6f5c\u7a7a\u95f4\uff0c\u53ef\u4ee5\u514d\u9664\u89c6\u56fe\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u9ad8\u8d28\u91cf\u76843D\u8d44\u4ea7\u7f16\u8f91"}}
{"id": "2509.00033", "pdf": "https://arxiv.org/pdf/2509.00033", "abs": "https://arxiv.org/abs/2509.00033", "authors": ["Tahoshin Alam Ishat"], "title": "Deep Learning-Driven Multimodal Detection and Movement Analysis of Objects in Culinary", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 9 figures", "summary": "This is a research exploring existing models and fine tuning them to combine a YOLOv8 segmentation model, a LSTM model trained on hand point motion sequence and a ASR (whisper-base) to extract enough data for a LLM (TinyLLaMa) to predict the recipe and generate text creating a step by step guide for the cooking procedure. All the data were gathered by the author for a robust task specific system to perform best in complex and challenging environments proving the extension and endless application of computer vision in daily activities such as kitchen work. This work extends the field for many more crucial task of our day to day life.", "AI": {"tldr": "\u7ed3\u5408YOLOv8\u5206\u5272\u6a21\u578b\u3001LSTM\u624b\u52bf\u52a8\u4f5c\u5e8f\u5217\u6a21\u578b\u548cWhisper\u8bed\u97f3\u8bc6\u522b\uff0c\u4e3aTinyLLaMA\u63d0\u4f9b\u591a\u6a21\u6001\u6570\u636e\u4ee5\u751f\u6210\u70f9\u996a\u6b65\u9aa4\u6307\u5357", "motivation": "\u63a2\u7d22\u8ba1\u7b97\u673a\u89c6\u89c9\u5728\u65e5\u5e38\u53a8\u623f\u6d3b\u52a8\u4e2d\u7684\u6269\u5c55\u5e94\u7528\uff0c\u6784\u5efa\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u6267\u884c\u7279\u5b9a\u4efb\u52a1\u7684\u9c81\u68d2\u7cfb\u7edf", "method": "\u96c6\u6210YOLOv8\u56fe\u50cf\u5206\u5272\u3001LSTM\u624b\u52bf\u52a8\u4f5c\u5e8f\u5217\u5206\u6790\u548cWhisper\u8bed\u97f3\u8bc6\u522b\uff0c\u4e3aTinyLLaMA\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u591a\u6a21\u6001\u8f93\u5165\u6570\u636e", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u9884\u6d4b\u98df\u8c31\u5e76\u751f\u6210\u9010\u6b65\u70f9\u996a\u6307\u5357\u7684\u4efb\u52a1\u4e13\u7528\u7cfb\u7edf", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u6269\u5c55\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u5728\u65e5\u5e38\u5173\u952e\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u9886\u57df\uff0c\u8bc1\u660e\u4e86\u591a\u6a21\u6001AI\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u65e0\u9650\u5e94\u7528\u6f5c\u529b"}}
{"id": "2509.01134", "pdf": "https://arxiv.org/pdf/2509.01134", "abs": "https://arxiv.org/abs/2509.01134", "authors": ["Xilong Zhou", "Pedro Figueiredo", "Milo\u0161 Ha\u0161an", "Valentin Deschaintre", "Paul Guerrero", "Yiwei Hu", "Nima Khademi Kalantari"], "title": "RealMat: Realistic Materials with Diffusion and Reinforcement Learning", "categories": ["cs.GR", "cs.CV"], "comment": "11 pages, 11 figures", "summary": "Generative models for high-quality materials are particularly desirable to make 3D content authoring more accessible. However, the majority of material generation methods are trained on synthetic data. Synthetic data provides precise supervision for material maps, which is convenient but also tends to create a significant visual gap with real-world materials. Alternatively, recent work used a small dataset of real flash photographs to guarantee realism, however such data is limited in scale and diversity. To address these limitations, we propose RealMat, a diffusion-based material generator that leverages realistic priors, including a text-to-image model and a dataset of realistic material photos under natural lighting. In RealMat, we first finetune a pretrained Stable Diffusion XL (SDXL) with synthetic material maps arranged in $2 \\times 2$ grids. This way, our model inherits some realism of SDXL while learning the data distribution of the synthetic material grids. Still, this creates a realism gap, with some generated materials appearing synthetic. We propose to further finetune our model through reinforcement learning (RL), encouraging the generation of realistic materials. We develop a realism reward function for any material image under natural lighting, by collecting a large-scale dataset of realistic material images. We show that this approach increases generated materials' realism compared to our base model and related work.", "AI": {"tldr": "RealMat\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6750\u8d28\u751f\u6210\u5668\uff0c\u901a\u8fc7\u7ed3\u5408\u5408\u6210\u6750\u8d28\u6570\u636e\u548c\u771f\u5b9e\u6750\u8d28\u7167\u7247\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u751f\u6210\u6750\u8d28\u7684\u771f\u5b9e\u611f\uff0c\u89e3\u51b3\u4e86\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6750\u8d28\u4e4b\u95f4\u7684\u89c6\u89c9\u5dee\u8ddd\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6750\u8d28\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u5408\u6210\u6570\u636e\u8bad\u7ec3\uff0c\u867d\u7136\u80fd\u63d0\u4f9b\u7cbe\u786e\u7684\u6750\u8d28\u56fe\u76d1\u7763\uff0c\u4f46\u4e0e\u771f\u5b9e\u4e16\u754c\u6750\u8d28\u5b58\u5728\u663e\u8457\u89c6\u89c9\u5dee\u8ddd\u3002\u800c\u57fa\u4e8e\u771f\u5b9e\u95ea\u5149\u7167\u7247\u7684\u65b9\u6cd5\u867d\u7136\u80fd\u4fdd\u8bc1\u771f\u5b9e\u611f\uff0c\u4f46\u6570\u636e\u89c4\u6a21\u548c\u591a\u6837\u6027\u6709\u9650\u3002", "method": "1. \u5728\u5408\u6210\u6750\u8d28\u56fe2\u00d72\u7f51\u683c\u4e0a\u5fae\u8c03\u9884\u8bad\u7ec3\u7684Stable Diffusion XL\u6a21\u578b\uff1b2. \u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u4e00\u6b65\u5fae\u8c03\u6a21\u578b\uff0c\u4f7f\u7528\u57fa\u4e8e\u5927\u89c4\u6a21\u771f\u5b9e\u6750\u8d28\u56fe\u50cf\u6570\u636e\u96c6\u6784\u5efa\u7684\u771f\u5b9e\u611f\u5956\u52b1\u51fd\u6570\u6765\u9f13\u52b1\u751f\u6210\u66f4\u771f\u5b9e\u7684\u6750\u8d28\u3002", "result": "\u8be5\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u548c\u76f8\u5173\u5de5\u4f5c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u6750\u8d28\u7684\u771f\u5b9e\u611f\uff0c\u6709\u6548\u7f29\u5c0f\u4e86\u5408\u6210\u6750\u8d28\u4e0e\u771f\u5b9e\u6750\u8d28\u4e4b\u95f4\u7684\u89c6\u89c9\u5dee\u8ddd\u3002", "conclusion": "RealMat\u901a\u8fc7\u7ed3\u5408\u5408\u6210\u6570\u636e\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u771f\u5b9e\u6027\u4f18\u5316\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u9ad8\u771f\u5b9e\u611f\u7684\u6750\u8d28\u751f\u6210\uff0c\u4e3a3D\u5185\u5bb9\u521b\u4f5c\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u5de5\u5177\u3002"}}
{"id": "2509.00108", "pdf": "https://arxiv.org/pdf/2509.00108", "abs": "https://arxiv.org/abs/2509.00108", "authors": ["Anas M. Ali", "Anis Koubaa", "Bilel Benjdira"], "title": "Dual-Stage Global and Local Feature Framework for Image Dehazing", "categories": ["cs.CV"], "comment": null, "summary": "Addressing the challenge of removing atmospheric fog or haze from digital images, known as image dehazing, has recently gained significant traction in the computer vision community. Although contemporary dehazing models have demonstrated promising performance, few have thoroughly investigated high-resolution imagery. In such scenarios, practitioners often resort to downsampling the input image or processing it in smaller patches, which leads to a notable performance degradation. This drop is primarily linked to the difficulty of effectively combining global contextual information with localized, fine-grained details as the spatial resolution grows. In this chapter, we propose a novel framework, termed the Streamlined Global and Local Features Combinator (SGLC), to bridge this gap and enable robust dehazing for high-resolution inputs. Our approach is composed of two principal components: the Global Features Generator (GFG) and the Local Features Enhancer (LFE). The GFG produces an initial dehazed output by focusing on broad contextual understanding of the scene. Subsequently, the LFE refines this preliminary output by enhancing localized details and pixel-level features, thereby capturing the interplay between global appearance and local structure. To evaluate the effectiveness of SGLC, we integrated it with the Uformer architecture, a state-of-the-art dehazing model. Experimental results on high-resolution datasets reveal a considerable improvement in peak signal-to-noise ratio (PSNR) when employing SGLC, indicating its potency in addressing haze in large-scale imagery. Moreover, the SGLC design is model-agnostic, allowing any dehazing network to be augmented with the proposed global-and-local feature fusion mechanism. Through this strategy, practitioners can harness both scene-level cues and granular details, significantly improving visual fidelity in high-resolution environments.", "AI": {"tldr": "\u63d0\u51faSGLC\u6846\u67b6\u89e3\u51b3\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u53bb\u96fe\u95ee\u9898\uff0c\u901a\u8fc7\u5168\u5c40\u7279\u5f81\u751f\u6210\u5668\u548c\u5c40\u90e8\u7279\u5f81\u589e\u5f3a\u5668\u7684\u7ec4\u5408\uff0c\u6709\u6548\u878d\u5408\u5168\u5c40\u4e0a\u4e0b\u6587\u548c\u5c40\u90e8\u7ec6\u8282\uff0c\u663e\u8457\u63d0\u5347\u53bb\u96fe\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u53bb\u96fe\u6a21\u578b\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u901a\u5e38\u9700\u8981\u964d\u91c7\u6837\u6216\u5206\u5757\u5904\u7406\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u96be\u4ee5\u6709\u6548\u7ed3\u5408\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u5c40\u90e8\u7cbe\u7ec6\u7ec6\u8282\u3002", "method": "\u63d0\u51faSGLC\u6846\u67b6\uff0c\u5305\u542b\u5168\u5c40\u7279\u5f81\u751f\u6210\u5668(GFG)\u548c\u5c40\u90e8\u7279\u5f81\u589e\u5f3a\u5668(LFE)\u3002GFG\u751f\u6210\u521d\u6b65\u53bb\u96fe\u7ed3\u679c\uff0cLFE\u7ec6\u5316\u5c40\u90e8\u7ec6\u8282\uff0c\u6355\u83b7\u5168\u5c40\u5916\u89c2\u4e0e\u5c40\u90e8\u7ed3\u6784\u7684\u4ea4\u4e92\u4f5c\u7528\u3002", "result": "\u5728\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0cSGLC\u663e\u8457\u63d0\u5347\u4e86PSNR\u6307\u6807\uff0c\u8bc1\u660e\u5176\u5728\u5927\u89c4\u6a21\u56fe\u50cf\u53bb\u96fe\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "SGLC\u662f\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u53ef\u4e0e\u4efb\u4f55\u53bb\u96fe\u7f51\u7edc\u7ed3\u5408\uff0c\u901a\u8fc7\u5168\u5c40-\u5c40\u90e8\u7279\u5f81\u878d\u5408\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u9ad8\u5206\u8fa8\u7387\u73af\u5883\u4e0b\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2509.02141", "pdf": "https://arxiv.org/pdf/2509.02141", "abs": "https://arxiv.org/abs/2509.02141", "authors": ["Mohit Mendiratta", "Mayur Deshmukh", "Kartik Teotia", "Vladislav Golyanik", "Adam Kortylewski", "Christian Theobalt"], "title": "GRMM: Real-Time High-Fidelity Gaussian Morphable Head Model with Learned Residuals", "categories": ["cs.GR", "cs.CV"], "comment": "Project page: https://mohitm1994.github.io/GRMM/", "summary": "3D Morphable Models (3DMMs) enable controllable facial geometry and expression editing for reconstruction, animation, and AR/VR, but traditional PCA-based mesh models are limited in resolution, detail, and photorealism. Neural volumetric methods improve realism but remain too slow for interactive use. Recent Gaussian Splatting (3DGS) based facial models achieve fast, high-quality rendering but still depend solely on a mesh-based 3DMM prior for expression control, limiting their ability to capture fine-grained geometry, expressions, and full-head coverage. We introduce GRMM, the first full-head Gaussian 3D morphable model that augments a base 3DMM with residual geometry and appearance components, additive refinements that recover high-frequency details such as wrinkles, fine skin texture, and hairline variations. GRMM provides disentangled control through low-dimensional, interpretable parameters (e.g., identity shape, facial expressions) while separately modelling residuals that capture subject- and expression-specific detail beyond the base model's capacity. Coarse decoders produce vertex-level mesh deformations, fine decoders represent per-Gaussian appearance, and a lightweight CNN refines rasterised images for enhanced realism, all while maintaining 75 FPS real-time rendering. To learn consistent, high-fidelity residuals, we present EXPRESS-50, the first dataset with 60 aligned expressions across 50 identities, enabling robust disentanglement of identity and expression in Gaussian-based 3DMMs. Across monocular 3D face reconstruction, novel-view synthesis, and expression transfer, GRMM surpasses state-of-the-art methods in fidelity and expression accuracy while delivering interactive real-time performance.", "AI": {"tldr": "GRMM\u662f\u9996\u4e2a\u57fa\u4e8e\u9ad8\u65af\u6e85\u5c04\u7684\u5168\u59343D\u53ef\u53d8\u5f62\u6a21\u578b\uff0c\u901a\u8fc7\u6b8b\u5dee\u51e0\u4f55\u548c\u5916\u89c2\u7ec4\u4ef6\u589e\u5f3a\u4f20\u7edf3DMM\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u7ec6\u8282\u6355\u6349\u548c\u5b9e\u65f6\u6e32\u67d3", "motivation": "\u4f20\u7edfPCA\u7f51\u683c\u6a21\u578b\u5206\u8fa8\u7387\u6709\u9650\uff0c\u795e\u7ecf\u4f53\u79ef\u65b9\u6cd5\u6e32\u67d3\u901f\u5ea6\u6162\uff0c\u73b0\u6709\u9ad8\u65af\u6e85\u5c04\u9762\u90e8\u6a21\u578b\u4f9d\u8d56\u7f51\u683c\u5148\u9a8c\uff0c\u65e0\u6cd5\u6355\u6349\u7ec6\u7c92\u5ea6\u51e0\u4f55\u548c\u5b8c\u6574\u5934\u90e8\u7ec6\u8282", "method": "\u5728\u57fa\u78403DMM\u4e0a\u6dfb\u52a0\u6b8b\u5dee\u51e0\u4f55\u548c\u5916\u89c2\u7ec4\u4ef6\uff0c\u4f7f\u7528\u7c97\u7c92\u5ea6\u89e3\u7801\u5668\u751f\u6210\u7f51\u683c\u53d8\u5f62\uff0c\u7ec6\u7c92\u5ea6\u89e3\u7801\u5668\u8868\u793a\u9ad8\u65af\u5916\u89c2\uff0c\u8f7b\u91cfCNN\u589e\u5f3a\u56fe\u50cf\u771f\u5b9e\u611f", "result": "\u5728\u5355\u76ee3D\u4eba\u8138\u91cd\u5efa\u3001\u65b0\u89c6\u89d2\u5408\u6210\u548c\u8868\u60c5\u8fc1\u79fb\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u4fdd\u630175FPS\u5b9e\u65f6\u6e32\u67d3\u6027\u80fd", "conclusion": "GRMM\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u7ec6\u8282\u6355\u6349\u4e0e\u5b9e\u65f6\u6e32\u67d3\u7684\u5e73\u8861\uff0cEXPRESS-50\u6570\u636e\u96c6\u4e3a\u9ad8\u65af\u57fa3DMM\u7684\u8eab\u4efd\u8868\u60c5\u89e3\u8026\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6491"}}
{"id": "2509.02474", "pdf": "https://arxiv.org/pdf/2509.02474", "abs": "https://arxiv.org/abs/2509.02474", "authors": ["Nina Wiedemann", "Sainan Liu", "Quentin Leboutet", "Katelyn Gao", "Benjamin Ummenhofer", "Michael Paulitsch", "Kai Yuan"], "title": "Unifi3D: A Study on 3D Representations for Generation and Reconstruction in a Common Framework", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": null, "summary": "Following rapid advancements in text and image generation, research has increasingly shifted towards 3D generation. Unlike the well-established pixel-based representation in images, 3D representations remain diverse and fragmented, encompassing a wide variety of approaches such as voxel grids, neural radiance fields, signed distance functions, point clouds, or octrees, each offering distinct advantages and limitations. In this work, we present a unified evaluation framework designed to assess the performance of 3D representations in reconstruction and generation. We compare these representations based on multiple criteria: quality, computational efficiency, and generalization performance. Beyond standard model benchmarking, our experiments aim to derive best practices over all steps involved in the 3D generation pipeline, including preprocessing, mesh reconstruction, compression with autoencoders, and generation. Our findings highlight that reconstruction errors significantly impact overall performance, underscoring the need to evaluate generation and reconstruction jointly. We provide insights that can inform the selection of suitable 3D models for various applications, facilitating the development of more robust and application-specific solutions in 3D generation. The code for our framework is available at https://github.com/isl-org/unifi3d.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u6bd4\u8f83\u4e0d\u540c3D\u8868\u793a\u65b9\u6cd5\uff08\u5982\u4f53\u7d20\u7f51\u683c\u3001\u795e\u7ecf\u8f90\u5c04\u573a\u7b49\uff09\u5728\u91cd\u5efa\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u91cd\u70b9\u5173\u6ce8\u8d28\u91cf\u3001\u8ba1\u7b97\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u6587\u672c\u548c\u56fe\u50cf\u751f\u6210\u7684\u5feb\u901f\u53d1\u5c55\uff0c3D\u751f\u6210\u7814\u7a76\u65e5\u76ca\u91cd\u8981\uff0c\u4f463D\u8868\u793a\u65b9\u6cd5\u591a\u6837\u4e14\u5206\u6563\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u8bc4\u4f30\u6807\u51c6\u6765\u6bd4\u8f83\u4e0d\u540c\u65b9\u6cd5\u7684\u4f18\u52a3\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5bf9\u591a\u79cd3D\u8868\u793a\u65b9\u6cd5\u8fdb\u884c\u7cfb\u7edf\u6027\u6bd4\u8f83\uff0c\u5305\u62ec\u9884\u5904\u7406\u3001\u7f51\u683c\u91cd\u5efa\u3001\u81ea\u7f16\u7801\u5668\u538b\u7f29\u548c\u751f\u6210\u7b49\u5b8c\u6574\u6d41\u7a0b\u7684\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u91cd\u5efa\u8bef\u5dee\u5bf9\u6574\u4f53\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u9700\u8981\u5c06\u751f\u6210\u548c\u91cd\u5efa\u8054\u5408\u8bc4\u4f30\uff0c\u4e3a\u4e0d\u540c\u5e94\u7528\u573a\u666f\u9009\u62e9\u5408\u9002\u76843D\u6a21\u578b\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a3D\u751f\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u7279\u5b9a\u5e94\u7528\u76843D\u751f\u6210\u89e3\u51b3\u65b9\u6848\uff0c\u76f8\u5173\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.00177", "pdf": "https://arxiv.org/pdf/2509.00177", "abs": "https://arxiv.org/abs/2509.00177", "authors": ["Faizan Farooq Khan", "Vladan Stojni\u0107", "Zakaria Laskar", "Mohamed Elhoseiny", "Giorgos Tolias"], "title": "Category-level Text-to-Image Retrieval Improved: Bridging the Domain Gap with Diffusion Models and Vision Encoders", "categories": ["cs.CV"], "comment": "BMVC 2025", "summary": "This work explores text-to-image retrieval for queries that specify or describe a semantic category. While vision-and-language models (VLMs) like CLIP offer a straightforward open-vocabulary solution, they map text and images to distant regions in the representation space, limiting retrieval performance. To bridge this modality gap, we propose a two-step approach. First, we transform the text query into a visual query using a generative diffusion model. Then, we estimate image-to-image similarity with a vision model. Additionally, we introduce an aggregation network that combines multiple generated images into a single vector representation and fuses similarity scores across both query modalities. Our approach leverages advancements in vision encoders, VLMs, and text-to-image generation models. Extensive evaluations show that it consistently outperforms retrieval methods relying solely on text queries. Source code is available at: https://github.com/faixan-khan/cletir", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u6b65\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5\uff1a\u5148\u7528\u6269\u6563\u6a21\u578b\u5c06\u6587\u672c\u67e5\u8be2\u8f6c\u6362\u4e3a\u89c6\u89c9\u67e5\u8be2\uff0c\u518d\u7528\u89c6\u89c9\u6a21\u578b\u8ba1\u7b97\u56fe\u50cf\u76f8\u4f3c\u5ea6\uff0c\u901a\u8fc7\u805a\u5408\u7f51\u7edc\u878d\u5408\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6027\u80fd", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u5c06\u6587\u672c\u548c\u56fe\u50cf\u6620\u5c04\u5230\u8868\u793a\u7a7a\u95f4\u4e2d\u8f83\u8fdc\u7684\u533a\u57df\uff0c\u5b58\u5728\u6a21\u6001\u9e3f\u6c9f\u95ee\u9898\uff0c\u9650\u5236\u4e86\u68c0\u7d22\u6027\u80fd", "method": "1. \u4f7f\u7528\u751f\u6210\u6269\u6563\u6a21\u578b\u5c06\u6587\u672c\u67e5\u8be2\u8f6c\u6362\u4e3a\u89c6\u89c9\u67e5\u8be2\u56fe\u50cf\uff1b2. \u7528\u89c6\u89c9\u6a21\u578b\u4f30\u8ba1\u56fe\u50cf\u5230\u56fe\u50cf\u76f8\u4f3c\u5ea6\uff1b3. \u5f15\u5165\u805a\u5408\u7f51\u7edc\u5c06\u591a\u4e2a\u751f\u6210\u56fe\u50cf\u878d\u5408\u4e3a\u5355\u4e00\u5411\u91cf\u8868\u793a\uff0c\u5e76\u8de8\u4e24\u79cd\u67e5\u8be2\u6a21\u6001\u878d\u5408\u76f8\u4f3c\u5ea6\u5206\u6570", "result": "\u7ecf\u8fc7\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u5728\u4f9d\u8d56\u7eaf\u6587\u672c\u67e5\u8be2\u7684\u68c0\u7d22\u65b9\u6cd5\u4e0a\u8868\u73b0\u4e00\u81f4\u66f4\u4f18", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5229\u7528\u4e86\u89c6\u89c9\u7f16\u7801\u5668\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u8fdb\u5c55\uff0c\u6210\u529f\u5f25\u5408\u4e86\u6a21\u6001\u9e3f\u6c9f\uff0c\u63d0\u5347\u4e86\u8bed\u4e49\u7c7b\u522b\u6307\u5b9a\u67e5\u8be2\u7684\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u6027\u80fd"}}
{"id": "2509.00284", "pdf": "https://arxiv.org/pdf/2509.00284", "abs": "https://arxiv.org/abs/2509.00284", "authors": ["Liang Gong", "Tommy", "Wang", "Sara Chaker", "Yanchen Dong", "Fouad Bousetouane", "Brenden Morton", "Mark Mendez"], "title": "Generative AI for Industrial Contour Detection: A Language-Guided Vision System", "categories": ["cs.CV", "cs.AI"], "comment": "20 pages, 5 figures", "summary": "Industrial computer vision systems often struggle with noise, material variability, and uncontrolled imaging conditions, limiting the effectiveness of classical edge detectors and handcrafted pipelines. In this work, we present a language-guided generative vision system for remnant contour detection in manufacturing, designed to achieve CAD-level precision. The system is organized into three stages: data acquisition and preprocessing, contour generation using a conditional GAN, and multimodal contour refinement through vision-language modeling, where standardized prompts are crafted in a human-in-the-loop process and applied through image-text guided synthesis. On proprietary FabTrack datasets, the proposed system improved contour fidelity, enhancing edge continuity and geometric alignment while reducing manual tracing. For the refinement stage, we benchmarked several vision-language models, including Google's Gemini 2.0 Flash, OpenAI's GPT-image-1 integrated within a VLM-guided workflow, and open-source baselines. Under standardized conditions, GPT-image-1 consistently outperformed Gemini 2.0 Flash in both structural accuracy and perceptual quality. These findings demonstrate the promise of VLM-guided generative workflows for advancing industrial computer vision beyond the limitations of classical pipelines.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u8a00\u5bfc\u5411\u7684\u751f\u6210\u5f0f\u89c6\u89c9\u7cfb\u7edf\uff0c\u7528\u4e8e\u5236\u9020\u4e1a\u4e2d\u7684\u6b8b\u4f59\u8f6e\u5ed3\u68c0\u6d4b\uff0c\u901a\u8fc7GAN\u548c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u4e86CAD\u7ea7\u522b\u7cbe\u5ea6\u7684\u8f6e\u5ed3\u68c0\u6d4b\u3002", "motivation": "\u5de5\u4e1a\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u9047\u5230\u566a\u58f0\u3001\u6750\u6599\u53d8\u5f02\u6027\u548c\u975e\u63a7\u5236\u6210\u50cf\u6761\u4ef6\u7684\u6311\u6218\uff0c\u4f20\u7edf\u8fb9\u7f18\u68c0\u6d4b\u5668\u548c\u624b\u5de5\u7ba1\u9053\u6548\u679c\u6709\u9650\u3002", "method": "\u7cfb\u7edf\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u6570\u636e\u83b7\u53d6\u548c\u9884\u5904\u7406\u3001\u4f7f\u7528\u6761\u4ef6GAN\u751f\u6210\u8f6e\u5ed3\u3001\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u591a\u6a21\u6001\u8f6e\u5ed3\u7cbe\u7ec6\u5316\uff0c\u5176\u4e2d\u6807\u51c6\u5316\u63d0\u793a\u901a\u8fc7\u4eba\u5728\u5faa\u73af\u8fc7\u7a0b\u5236\u4f5c\u5e76\u901a\u8fc7\u56fe\u50cf-\u6587\u672c\u5bfc\u5411\u5408\u6210\u5e94\u7528\u3002", "result": "\u5728\u4e13\u6709FabTrack\u6570\u636e\u96c6\u4e0a\uff0c\u7cfb\u7edf\u63d0\u9ad8\u4e86\u8f6e\u5ed3\u4fdd\u771f\u5ea6\uff0c\u589e\u5f3a\u4e86\u8fb9\u7f18\u8fde\u7eed\u6027\u548c\u51e0\u4f55\u5bf9\u9f50\uff0c\u51cf\u5c11\u4e86\u624b\u5de5\u8ffd\u8e2a\u3002\u5728\u7cbe\u7ec6\u5316\u9636\u6bb5\uff0cGPT-image-1\u5728\u7ed3\u6784\u51c6\u786e\u6027\u548c\u611f\u77e5\u8d28\u91cf\u65b9\u9762\u5747\u8d85\u8fc7Gemini 2.0 Flash\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u8bc1\u660e\u4e86VLM\u5bfc\u5411\u7684\u751f\u6210\u5f0f\u5de5\u4f5c\u6d41\u5728\u63a8\u8fdb\u5de5\u4e1a\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u8d85\u8d8a\u4f20\u7edf\u7ba1\u9053\u9650\u5236\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.00356", "pdf": "https://arxiv.org/pdf/2509.00356", "abs": "https://arxiv.org/abs/2509.00356", "authors": ["Jin Ye", "Fengchao Xiong", "Jun Zhou", "Yuntao Qian"], "title": "Iterative Low-rank Network for Hyperspectral Image Denoising", "categories": ["cs.CV"], "comment": null, "summary": "Hyperspectral image (HSI) denoising is a crucial preprocessing step for subsequent tasks. The clean HSI usually reside in a low-dimensional subspace, which can be captured by low-rank and sparse representation, known as the physical prior of HSI. It is generally challenging to adequately use such physical properties for effective denoising while preserving image details. This paper introduces a novel iterative low-rank network (ILRNet) to address these challenges. ILRNet integrates the strengths of model-driven and data-driven approaches by embedding a rank minimization module (RMM) within a U-Net architecture. This module transforms feature maps into the wavelet domain and applies singular value thresholding (SVT) to the low-frequency components during the forward pass, leveraging the spectral low-rankness of HSIs in the feature domain. The parameter, closely related to the hyperparameter of the singular vector thresholding algorithm, is adaptively learned from the data, allowing for flexible and effective capture of low-rankness across different scenarios. Additionally, ILRNet features an iterative refinement process that adaptively combines intermediate denoised HSIs with noisy inputs. This manner ensures progressive enhancement and superior preservation of image details. Experimental results demonstrate that ILRNet achieves state-of-the-art performance in both synthetic and real-world noise removal tasks.", "AI": {"tldr": "ILRNet\u662f\u4e00\u79cd\u7ed3\u5408\u6a21\u578b\u9a71\u52a8\u548c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u8fed\u4ee3\u4f4e\u79e9\u7f51\u7edc\uff0c\u901a\u8fc7\u5c06\u79e9\u6700\u5c0f\u5316\u6a21\u5757\u5d4c\u5165U-Net\u67b6\u6784\uff0c\u5728\u7279\u5f81\u57df\u5229\u7528HSI\u7684\u5149\u8c31\u4f4e\u79e9\u7279\u6027\u8fdb\u884c\u6709\u6548\u53bb\u566a\u3002", "motivation": "\u9ad8\u5149\u8c31\u56fe\u50cf\u53bb\u566a\u662f\u540e\u7eed\u4efb\u52a1\u7684\u5173\u952e\u9884\u5904\u7406\u6b65\u9aa4\u3002\u5e72\u51c0\u7684HSI\u901a\u5e38\u5b58\u5728\u4e8e\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u4e2d\uff0c\u53ef\u4ee5\u901a\u8fc7\u4f4e\u79e9\u548c\u7a00\u758f\u8868\u793a\u6765\u6355\u6349\uff0c\u4f46\u5982\u4f55\u5728\u6709\u6548\u53bb\u566a\u7684\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u7ec6\u8282\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51faILRNet\uff0c\u5728U-Net\u67b6\u6784\u4e2d\u5d4c\u5165\u79e9\u6700\u5c0f\u5316\u6a21\u5757(RMM)\uff0c\u5c06\u7279\u5f81\u56fe\u8f6c\u6362\u5230\u5c0f\u6ce2\u57df\u5e76\u5bf9\u4f4e\u9891\u5206\u91cf\u5e94\u7528\u5947\u5f02\u503c\u9608\u503c\u5904\u7406(SVT)\uff0c\u81ea\u9002\u5e94\u5b66\u4e60\u53c2\u6570\u4ee5\u7075\u6d3b\u6355\u6349\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u4f4e\u79e9\u7279\u6027\uff0c\u5e76\u91c7\u7528\u8fed\u4ee3\u7ec6\u5316\u8fc7\u7a0b\u81ea\u9002\u5e94\u7ed3\u5408\u4e2d\u95f4\u53bb\u566a\u7ed3\u679c\u548c\u566a\u58f0\u8f93\u5165\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cILRNet\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u566a\u58f0\u53bb\u9664\u4efb\u52a1\u4e2d\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "ILRNet\u6210\u529f\u6574\u5408\u4e86\u6a21\u578b\u9a71\u52a8\u548c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u7279\u5f81\u57df\u7684\u4f4e\u79e9\u7279\u6027\u548c\u8fed\u4ee3\u7ec6\u5316\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\u53bb\u566a\u548c\u7ec6\u8282\u4fdd\u6301\u3002"}}
{"id": "2509.00378", "pdf": "https://arxiv.org/pdf/2509.00378", "abs": "https://arxiv.org/abs/2509.00378", "authors": ["Shumpei Takezaki", "Ryoma Bise", "Shinnosuke Matsuo"], "title": "NoiseCutMix: A Novel Data Augmentation Approach by Mixing Estimated Noise in Diffusion Models", "categories": ["cs.CV"], "comment": "Accepted at ICCV2025 Workshop LIMIT", "summary": "In this study, we propose a novel data augmentation method that introduces the concept of CutMix into the generation process of diffusion models, thereby exploiting both the ability of diffusion models to generate natural and high-resolution images and the characteristic of CutMix, which combines features from two classes to create diverse augmented data. Representative data augmentation methods for combining images from multiple classes include CutMix and MixUp. However, techniques like CutMix often result in unnatural boundaries between the two images due to contextual differences. Therefore, in this study, we propose a method, called NoiseCutMix, to achieve natural, high-resolution image generation featuring the fused characteristics of two classes by partially combining the estimated noise corresponding to two different classes in a diffusion model. In the classification experiments, we verified the effectiveness of the proposed method by comparing it with conventional data augmentation techniques that combine multiple classes, random image generation using Stable Diffusion, and combinations of these methods. Our codes are available at: https://github.com/shumpei-takezaki/NoiseCutMix", "AI": {"tldr": "\u63d0\u51faNoiseCutMix\u65b9\u6cd5\uff0c\u5c06CutMix\u6982\u5ff5\u5f15\u5165\u6269\u6563\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\uff0c\u901a\u8fc7\u878d\u5408\u4e24\u4e2a\u7c7b\u522b\u7684\u4f30\u8ba1\u566a\u58f0\u6765\u751f\u6210\u81ea\u7136\u3001\u9ad8\u5206\u8fa8\u7387\u7684\u8de8\u7c7b\u522b\u878d\u5408\u56fe\u50cf", "motivation": "\u4f20\u7edfCutMix\u7b49\u65b9\u6cd5\u5728\u7ec4\u5408\u4e0d\u540c\u7c7b\u522b\u56fe\u50cf\u65f6\u4f1a\u4ea7\u751f\u4e0d\u81ea\u7136\u7684\u8fb9\u754c\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u751f\u6210\u81ea\u7136\u878d\u5408\u56fe\u50cf\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5", "method": "\u5728\u6269\u6563\u6a21\u578b\u4e2d\u90e8\u5206\u7ec4\u5408\u4e24\u4e2a\u4e0d\u540c\u7c7b\u522b\u5bf9\u5e94\u7684\u4f30\u8ba1\u566a\u58f0\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684\u80fd\u529b\u548cCutMix\u7684\u7279\u5f81\u878d\u5408\u7279\u6027", "result": "\u5728\u5206\u7c7b\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u4f20\u7edf\u591a\u7c7b\u522b\u7ec4\u5408\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u548c\u968f\u673a\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u8868\u73b0\u66f4\u597d", "conclusion": "NoiseCutMix\u80fd\u591f\u751f\u6210\u81ea\u7136\u4e14\u9ad8\u5206\u8fa8\u7387\u7684\u8de8\u7c7b\u522b\u878d\u5408\u56fe\u50cf\uff0c\u4e3a\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u65b9\u6cd5"}}
{"id": "2509.00381", "pdf": "https://arxiv.org/pdf/2509.00381", "abs": "https://arxiv.org/abs/2509.00381", "authors": ["Runtong Wu", "Jiayao Song", "Fei Teng", "Xianhao Ren", "Yuyan Gao", "Kailun Yang"], "title": "Visually Grounded Narratives: Reducing Cognitive Burden in Researcher-Participant Interaction", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Narrative inquiry has been one of the prominent application domains for the analysis of human experience, aiming to know more about the complexity of human society. However, researchers are often required to transform various forms of data into coherent hand-drafted narratives in storied form throughout narrative analysis, which brings an immense burden of data analysis. Participants, too, are expected to engage in member checking and presentation of these narrative products, which involves reviewing and responding to large volumes of documents. Given the dual burden and the need for more efficient and participant-friendly approaches to narrative making and representation, we made a first attempt: (i) a new paradigm is proposed, NAME, as the initial attempt to push the field of narrative inquiry. Name is able to transfer research documents into coherent story images, alleviating the cognitive burden of interpreting extensive text-based materials during member checking for both researchers and participants. (ii) We develop an actor location and shape module to facilitate plausible image generation. (iii) We have designed a set of robust evaluation metrics comprising three key dimensions to objectively measure the perceptual quality and narrative consistency of generated characters. Our approach consistently demonstrates state-of-the-art performance across different data partitioning schemes. Remarkably, while the baseline relies on the full 100% of the available data, our method requires only 0.96% yet still reduces the FID score from 195 to 152. Under identical data volumes, our method delivers substantial improvements: for the 70:30 split, the FID score decreases from 175 to 152, and for the 95:5 split, it is nearly halved from 96 to 49. Furthermore, the proposed model achieves a score of 3.62 on the newly introduced metric, surpassing the baseline score of 2.66.", "AI": {"tldr": "\u63d0\u51faNAME\u65b0\u8303\u5f0f\uff0c\u5c06\u7814\u7a76\u6587\u6863\u8f6c\u5316\u4e3a\u8fde\u8d2f\u7684\u6545\u4e8b\u56fe\u50cf\uff0c\u51cf\u8f7b\u53d9\u4e8b\u5206\u6790\u4e2d\u6587\u672c\u89e3\u8bfb\u7684\u8ba4\u77e5\u8d1f\u62c5\uff0c\u4ec5\u97000.96%\u6570\u636e\u5373\u53ef\u663e\u8457\u63d0\u5347\u751f\u6210\u8d28\u91cf", "motivation": "\u4f20\u7edf\u53d9\u4e8b\u5206\u6790\u9700\u8981\u5c06\u5404\u79cd\u6570\u636e\u8f6c\u5316\u4e3a\u624b\u5199\u53d9\u4e8b\u6545\u4e8b\uff0c\u7ed9\u7814\u7a76\u8005\u548c\u53c2\u4e0e\u8005\u5e26\u6765\u5de8\u5927\u6570\u636e\u5206\u6790\u8d1f\u62c5\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u53c2\u4e0e\u8005\u53cb\u597d\u7684\u53d9\u4e8b\u5236\u4f5c\u548c\u5448\u73b0\u65b9\u6cd5", "method": "\u5f00\u53d1NAME\u8303\u5f0f\uff0c\u5305\u542b\u6f14\u5458\u5b9a\u4f4d\u548c\u5f62\u72b6\u6a21\u5757\u4ee5\u4fc3\u8fdb\u5408\u7406\u56fe\u50cf\u751f\u6210\uff0c\u8bbe\u8ba1\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\u7684\u8bc4\u4f30\u6307\u6807\u6765\u5ba2\u89c2\u8861\u91cf\u751f\u6210\u89d2\u8272\u7684\u611f\u77e5\u8d28\u91cf\u548c\u53d9\u4e8b\u4e00\u81f4\u6027", "result": "\u5728\u4e0d\u540c\u6570\u636e\u5212\u5206\u65b9\u6848\u4e0b\u5747\u8868\u73b0\u6700\u4f18\uff0c\u4ec5\u75280.96%\u6570\u636e\u5c31\u5c06FID\u5206\u6570\u4ece195\u964d\u81f3152\uff1b\u76f8\u540c\u6570\u636e\u91cf\u4e0b\u663e\u8457\u6539\u8fdb\uff1a70:30\u5212\u5206\u4ece175\u964d\u81f3152\uff0c95:5\u5212\u5206\u4ece96\u964d\u81f349\uff1b\u65b0\u6307\u6807\u5f97\u52063.62\u8d85\u8d8a\u57fa\u7ebf2.66", "conclusion": "NAME\u8303\u5f0f\u6210\u529f\u51cf\u8f7b\u4e86\u53d9\u4e8b\u5206\u6790\u4e2d\u7684\u8ba4\u77e5\u8d1f\u62c5\uff0c\u4e3a\u53d9\u4e8b\u63a2\u7a76\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u53c2\u4e0e\u8005\u53cb\u597d\u7684\u65b9\u6cd5\uff0c\u5728\u6570\u636e\u6548\u7387\u548c\u751f\u6210\u8d28\u91cf\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7a81\u7834"}}
{"id": "2509.00395", "pdf": "https://arxiv.org/pdf/2509.00395", "abs": "https://arxiv.org/abs/2509.00395", "authors": ["Mengxiao Geng", "Ran Hong", "Bingxuan Li", "Qiegen Liu"], "title": "Double-Constraint Diffusion Model with Nuclear Regularization for Ultra-low-dose PET Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Ultra-low-dose positron emission tomography (PET) reconstruction holds significant potential for reducing patient radiation exposure and shortening examination times. However, it may also lead to increased noise and reduced imaging detail, which could decrease the image quality. In this study, we present a Double-Constraint Diffusion Model (DCDM), which freezes the weights of a pre-trained diffusion model and injects a trainable double-constraint controller into the encoding architecture, greatly reducing the number of trainable parameters for ultra-low-dose PET reconstruction. Unlike full fine-tuning models, DCDM can adapt to different dose levels without retraining all model parameters, thereby improving reconstruction flexibility. Specifically, the two constraint modules, named the Nuclear Transformer Constraint (NTC) and the Encoding Nexus Constraint (ENC), serve to refine the pre-trained diffusion model. The NTC leverages the nuclear norm as an approximation for matrix rank minimization, integrates the low-rank property into the Transformer architecture, and enables efficient information extraction from low-dose images and conversion into compressed feature representations in the latent space. Subsequently, the ENC utilizes these compressed feature representations to encode and control the pre-trained diffusion model, ultimately obtaining reconstructed PET images in the pixel space. In clinical reconstruction, the compressed feature representations from NTC help select the most suitable ENC for efficient unknown low-dose PET reconstruction. Experiments conducted on the UDPET public dataset and the Clinical dataset demonstrated that DCDM outperforms state-of-the-art methods on known dose reduction factors (DRF) and generalizes well to unknown DRF scenarios, proving valuable even at ultra-low dose levels, such as 1% of the full dose.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u7ea6\u675f\u6269\u6563\u6a21\u578b(DCDM)\u7528\u4e8e\u8d85\u4f4e\u5242\u91cfPET\u91cd\u5efa\uff0c\u901a\u8fc7\u51bb\u7ed3\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u6743\u91cd\u5e76\u6ce8\u5165\u53ef\u8bad\u7ec3\u7684\u53cc\u7ea6\u675f\u63a7\u5236\u5668\uff0c\u5927\u5e45\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\uff0c\u5728\u5df2\u77e5\u548c\u672a\u77e5\u5242\u91cf\u6c34\u5e73\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8d85\u4f4e\u5242\u91cfPET\u91cd\u5efa\u53ef\u51cf\u5c11\u60a3\u8005\u8f90\u5c04\u66b4\u9732\u548c\u68c0\u67e5\u65f6\u95f4\uff0c\u4f46\u4f1a\u5bfc\u81f4\u566a\u58f0\u589e\u52a0\u548c\u56fe\u50cf\u7ec6\u8282\u51cf\u5c11\uff0c\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u53c8\u5177\u6709\u7075\u6d3b\u6027\u7684\u91cd\u5efa\u65b9\u6cd5\u3002", "method": "DCDM\u51bb\u7ed3\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u6743\u91cd\uff0c\u6ce8\u5165\u6838\u53d8\u6362\u5668\u7ea6\u675f(NTC)\u548c\u7f16\u7801\u8fde\u63a5\u7ea6\u675f(ENC)\u4e24\u4e2a\u53ef\u8bad\u7ec3\u6a21\u5757\u3002NTC\u5229\u7528\u6838\u8303\u6570\u8fd1\u4f3c\u77e9\u9635\u79e9\u6700\u5c0f\u5316\uff0c\u5c06\u4f4e\u79e9\u7279\u6027\u6574\u5408\u5230Transformer\u67b6\u6784\u4e2d\uff1bENC\u5229\u7528\u538b\u7f29\u7279\u5f81\u8868\u793a\u7f16\u7801\u548c\u63a7\u5236\u6269\u6563\u6a21\u578b\u3002", "result": "\u5728UDPET\u516c\u5171\u6570\u636e\u96c6\u548c\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDCDM\u5728\u5df2\u77e5\u5242\u91cf\u51cf\u5c11\u56e0\u5b50(DRF)\u548c\u672a\u77e5DRF\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5373\u4f7f\u57281%\u5168\u5242\u91cf\u7684\u8d85\u4f4e\u5242\u91cf\u6c34\u5e73\u4e0b\u4e5f\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DCDM\u901a\u8fc7\u53cc\u7ea6\u675f\u63a7\u5236\u5668\u6709\u6548\u51cf\u5c11\u4e86\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\uff0c\u63d0\u9ad8\u4e86\u91cd\u5efa\u7075\u6d3b\u6027\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u5242\u91cf\u6c34\u5e73\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6240\u6709\u53c2\u6570\uff0c\u5728\u8d85\u4f4e\u5242\u91cfPET\u91cd\u5efa\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2509.00396", "pdf": "https://arxiv.org/pdf/2509.00396", "abs": "https://arxiv.org/abs/2509.00396", "authors": ["Ryosuke Seshimo", "Mariko Isogawa"], "title": "DAOVI: Distortion-Aware Omnidirectional Video Inpainting", "categories": ["cs.CV", "cs.AI"], "comment": "BMVC 2025", "summary": "Omnidirectional videos that capture the entire surroundings are employed in a variety of fields such as VR applications and remote sensing. However, their wide field of view often causes unwanted objects to appear in the videos. This problem can be addressed by video inpainting, which enables the natural removal of such objects while preserving both spatial and temporal consistency. Nevertheless, most existing methods assume processing ordinary videos with a narrow field of view and do not tackle the distortion in equirectangular projection of omnidirectional videos. To address this issue, this paper proposes a novel deep learning model for omnidirectional video inpainting, called Distortion-Aware Omnidirectional Video Inpainting (DAOVI). DAOVI introduces a module that evaluates temporal motion information in the image space considering geodesic distance, as well as a depth-aware feature propagation module in the feature space that is designed to address the geometric distortion inherent to omnidirectional videos. The experimental results demonstrate that our proposed method outperforms existing methods both quantitatively and qualitatively.", "AI": {"tldr": "\u63d0\u51fa\u4e86DAOVI\u6a21\u578b\uff0c\u4e13\u95e8\u9488\u5bf9\u5168\u666f\u89c6\u9891\u4fee\u590d\u4e2d\u7684\u51e0\u4f55\u7578\u53d8\u95ee\u9898\uff0c\u901a\u8fc7\u6d4b\u5730\u8ddd\u79bb\u8bc4\u4f30\u8fd0\u52a8\u4fe1\u606f\u548c\u6df1\u5ea6\u611f\u77e5\u7279\u5f81\u4f20\u64ad\u6a21\u5757\uff0c\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5168\u666f\u89c6\u9891\u5177\u6709\u5bbd\u89c6\u573a\u89d2\uff0c\u4f46\u7ecf\u5e38\u5305\u542b\u4e0d\u9700\u8981\u7684\u7269\u4f53\u3002\u73b0\u6709\u89c6\u9891\u4fee\u590d\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u666e\u901a\u7a84\u89c6\u573a\u89c6\u9891\uff0c\u65e0\u6cd5\u5904\u7406\u5168\u666f\u89c6\u9891\u5728\u7b49\u8ddd\u67f1\u72b6\u6295\u5f71\u4e2d\u7684\u7578\u53d8\u95ee\u9898\u3002", "method": "\u63d0\u51faDAOVI\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1) \u57fa\u4e8e\u6d4b\u5730\u8ddd\u79bb\u7684\u56fe\u50cf\u7a7a\u95f4\u65f6\u5e8f\u8fd0\u52a8\u4fe1\u606f\u8bc4\u4f30\u6a21\u5757\uff1b2) \u7279\u5f81\u7a7a\u95f4\u4e2d\u5904\u7406\u51e0\u4f55\u7578\u53d8\u7684\u6df1\u5ea6\u611f\u77e5\u7279\u5f81\u4f20\u64ad\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u90fd\u4f18\u4e8e\u73b0\u6709\u7684\u89c6\u9891\u4fee\u590d\u65b9\u6cd5\u3002", "conclusion": "DAOVI\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u5168\u666f\u89c6\u9891\u4fee\u590d\u4e2d\u7684\u51e0\u4f55\u7578\u53d8\u95ee\u9898\uff0c\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u6a21\u5757\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4fdd\u6301\u3002"}}
{"id": "2509.00403", "pdf": "https://arxiv.org/pdf/2509.00403", "abs": "https://arxiv.org/abs/2509.00403", "authors": ["Yushuo Chen", "Ruizhi Shao", "Youxin Pang", "Hongwen Zhang", "Xinyi Wu", "Rihui Wu", "Yebin Liu"], "title": "DevilSight: Augmenting Monocular Human Avatar Reconstruction through a Virtual Perspective", "categories": ["cs.CV"], "comment": null, "summary": "We present a novel framework to reconstruct human avatars from monocular videos. Recent approaches have struggled either to capture the fine-grained dynamic details from the input or to generate plausible details at novel viewpoints, which mainly stem from the limited representational capacity of the avatar model and insufficient observational data. To overcome these challenges, we propose to leverage the advanced video generative model, Human4DiT, to generate the human motions from alternative perspective as an additional supervision signal. This approach not only enriches the details in previously unseen regions but also effectively regularizes the avatar representation to mitigate artifacts. Furthermore, we introduce two complementary strategies to enhance video generation: To ensure consistent reproduction of human motion, we inject the physical identity into the model through video fine-tuning. For higher-resolution outputs with finer details, a patch-based denoising algorithm is employed. Experimental results demonstrate that our method outperforms recent state-of-the-art approaches and validate the effectiveness of our proposed strategies.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89c6\u9891\u751f\u6210\u6a21\u578bHuman4DiT\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u591a\u89c6\u89d2\u4eba\u4f53\u8fd0\u52a8\u4f5c\u4e3a\u989d\u5916\u76d1\u7763\u4fe1\u53f7\uff0c\u89e3\u51b3\u5355\u76ee\u89c6\u9891\u91cd\u5efa\u4eba\u4f53avatar\u65f6\u7ec6\u8282\u7f3a\u5931\u548c\u89c6\u89d2\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4ece\u5355\u76ee\u89c6\u9891\u91cd\u5efa\u4eba\u4f53avatar\u65f6\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u65e0\u6cd5\u6355\u6349\u8f93\u5165\u4e2d\u7684\u7ec6\u7c92\u5ea6\u52a8\u6001\u7ec6\u8282\uff0c\u4ee5\u53ca\u5728\u65b0\u9896\u89c6\u89d2\u4e0b\u751f\u6210\u5408\u7406\u7ec6\u8282\u7684\u80fd\u529b\u4e0d\u8db3\uff0c\u8fd9\u4e3b\u8981\u6e90\u4e8eavatar\u6a21\u578b\u8868\u793a\u80fd\u529b\u6709\u9650\u548c\u89c2\u6d4b\u6570\u636e\u4e0d\u8db3\u3002", "method": "1) \u5229\u7528\u5148\u8fdb\u89c6\u9891\u751f\u6210\u6a21\u578bHuman4DiT\u751f\u6210\u66ff\u4ee3\u89c6\u89d2\u7684\u4eba\u4f53\u8fd0\u52a8\u4f5c\u4e3a\u989d\u5916\u76d1\u7763\u4fe1\u53f7\uff1b2) \u901a\u8fc7\u89c6\u9891\u5fae\u8c03\u6ce8\u5165\u7269\u7406\u8eab\u4efd\u786e\u4fdd\u8fd0\u52a8\u4e00\u81f4\u6027\uff1b3) \u91c7\u7528\u57fa\u4e8epatch\u7684\u53bb\u566a\u7b97\u6cd5\u83b7\u5f97\u66f4\u9ad8\u5206\u8fa8\u7387\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u89c6\u9891\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u591a\u89c6\u89d2\u76d1\u7763\u4fe1\u53f7\uff0c\u7ed3\u5408\u8eab\u4efd\u4fdd\u6301\u548c\u7ec6\u8282\u589e\u5f3a\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5355\u76ee\u89c6\u9891\u4eba\u4f53avatar\u91cd\u5efa\u4e2d\u7684\u7ec6\u8282\u7f3a\u5931\u548c\u89c6\u89d2\u4e00\u81f4\u6027\u95ee\u9898\u3002"}}
{"id": "2509.00442", "pdf": "https://arxiv.org/pdf/2509.00442", "abs": "https://arxiv.org/abs/2509.00442", "authors": ["Lubin Gan", "Xiaoman Wu", "Jing Zhang", "Zhifeng Wang", "Linhao Qu", "Siying Wu", "Xiaoyan Sun"], "title": "SemaMIL: Semantic Reordering with Retrieval-Guided State Space Modeling for Whole Slide Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Multiple instance learning (MIL) has become the leading approach for extracting discriminative features from whole slide images (WSIs) in computational pathology. Attention-based MIL methods can identify key patches but tend to overlook contextual relationships. Transformer models are able to model interactions but require quadratic computational cost and are prone to overfitting. State space models (SSMs) offer linear complexity, yet shuffling patch order disrupts histological meaning and reduces interpretability. In this work, we introduce SemaMIL, which integrates Semantic Reordering (SR), an adaptive method that clusters and arranges semantically similar patches in sequence through a reversible permutation, with a Semantic-guided Retrieval State Space Module (SRSM) that chooses a representative subset of queries to adjust state space parameters for improved global modeling. Evaluation on four WSI subtype datasets shows that, compared to strong baselines, SemaMIL achieves state-of-the-art accuracy with fewer FLOPs and parameters.", "AI": {"tldr": "SemaMIL\u662f\u4e00\u79cd\u65b0\u7684\u591a\u5b9e\u4f8b\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u91cd\u6392\u5e8f\u548c\u8bed\u4e49\u5f15\u5bfc\u68c0\u7d22\u72b6\u6001\u7a7a\u95f4\u6a21\u5757\uff0c\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u6ce8\u610f\u529b\u673a\u5236MIL\u65b9\u6cd5\u5ffd\u7565\u4e86\u4e0a\u4e0b\u6587\u5173\u7cfb\uff0cTransformer\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u867d\u7136\u8ba1\u7b97\u590d\u6742\u5ea6\u4f4e\u4f46\u6253\u4e71\u8865\u4e01\u987a\u5e8f\u4f1a\u7834\u574f\u7ec4\u7ec7\u5b66\u610f\u4e49\u3002", "method": "\u63d0\u51faSemaMIL\u6846\u67b6\uff0c\u5305\u542b\u8bed\u4e49\u91cd\u6392\u5e8f\uff08SR\uff09\u65b9\u6cd5\u5bf9\u8bed\u4e49\u76f8\u4f3c\u7684\u8865\u4e01\u8fdb\u884c\u805a\u7c7b\u548c\u6392\u5e8f\uff0c\u4ee5\u53ca\u8bed\u4e49\u5f15\u5bfc\u68c0\u7d22\u72b6\u6001\u7a7a\u95f4\u6a21\u5757\uff08SRSM\uff09\u9009\u62e9\u4ee3\u8868\u6027\u67e5\u8be2\u5b50\u96c6\u6765\u8c03\u6574\u72b6\u6001\u7a7a\u95f4\u53c2\u6570\u3002", "result": "\u5728\u56db\u4e2aWSI\u4e9a\u578b\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cSemaMIL\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4f7f\u7528\u4e86\u66f4\u5c11\u7684FLOPs\u548c\u53c2\u6570\u3002", "conclusion": "SemaMIL\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u91cd\u6392\u5e8f\u548c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u7ec4\u7ec7\u5b66\u610f\u4e49\u548c\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5168\u5c40\u5efa\u6a21\u80fd\u529b\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2509.00508", "pdf": "https://arxiv.org/pdf/2509.00508", "abs": "https://arxiv.org/abs/2509.00508", "authors": ["Nhat-Tuong Do-Tran", "Ngoc-Hoang-Lam Le", "Ian Chiu", "Po-Tsun Paul Kuo", "Ching-Chun Huang"], "title": "TRUST: Token-dRiven Ultrasound Style Transfer for Cross-Device Adaptation", "categories": ["cs.CV"], "comment": "Accepted to APSIPA ASC 2025", "summary": "Ultrasound images acquired from different devices exhibit diverse styles, resulting in decreased performance of downstream tasks. To mitigate the style gap, unpaired image-to-image (UI2I) translation methods aim to transfer images from a source domain, corresponding to new device acquisitions, to a target domain where a frozen task model has been trained for downstream applications. However, existing UI2I methods have not explicitly considered filtering the most relevant style features, which may result in translated images misaligned with the needs of downstream tasks. In this work, we propose TRUST, a token-driven dual-stream framework that preserves source content while transferring the common style of the target domain, ensuring that content and style remain unblended. Given multiple styles in the target domain, we introduce a Token-dRiven (TR) module that operates from two perspectives: (1) a data view--selecting \"suitable\" target tokens corresponding to each source token, and (2) a model view--identifying ``optimal\" target tokens for the downstream model, guided by a behavior mirror loss. Additionally, we inject auxiliary prompts into the source encoder to match content representation with downstream behavior. Experimental results on ultrasound datasets demonstrate that TRUST outperforms existing UI2I methods in both visual quality and downstream task performance.", "AI": {"tldr": "TRUST\u662f\u4e00\u4e2a\u9762\u5411\u8d85\u58f0\u56fe\u50cf\u7684\u53cc\u6d41\u6846\u67b6\uff0c\u901a\u8fc7token\u9a71\u52a8\u673a\u5236\u5728\u4fdd\u6301\u6e90\u5185\u5bb9\u7684\u540c\u65f6\u8f6c\u79fb\u76ee\u6807\u57df\u98ce\u683c\uff0c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd", "motivation": "\u89e3\u51b3\u4e0d\u540c\u8bbe\u5907\u83b7\u53d6\u7684\u8d85\u58f0\u56fe\u50cf\u98ce\u683c\u5dee\u5f02\u5bfc\u81f4\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u7b5b\u9009\u4e0e\u4e0b\u6e38\u4efb\u52a1\u76f8\u5173\u7684\u98ce\u683c\u7279\u5f81", "method": "\u63d0\u51fatoken\u9a71\u52a8\u7684\u53cc\u6d41\u6846\u67b6\uff0c\u5305\u542b\u6570\u636e\u89c6\u89d2\u9009\u62e9\u5408\u9002\u76ee\u6807token\u548c\u6a21\u578b\u89c6\u89d2\u8bc6\u522b\u6700\u4f18\u76ee\u6807token\uff0c\u5e76\u5f15\u5165\u884c\u4e3a\u955c\u50cf\u635f\u5931\u548c\u8f85\u52a9\u63d0\u793a", "result": "\u5728\u8d85\u58f0\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTRUST\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709UI2I\u65b9\u6cd5", "conclusion": "TRUST\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8d85\u58f0\u56fe\u50cf\u98ce\u683c\u8fc1\u79fb\u4e2d\u7684\u5185\u5bb9\u4fdd\u6301\u548c\u98ce\u683c\u5bf9\u9f50\u95ee\u9898\uff0c\u4e3a\u591a\u8bbe\u5907\u8d85\u58f0\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.00549", "pdf": "https://arxiv.org/pdf/2509.00549", "abs": "https://arxiv.org/abs/2509.00549", "authors": ["Peirong Liu", "Oula Puonti", "Xiaoling Hu", "Karthik Gopinath", "Annabel Sorby-Adams", "Daniel C. Alexander", "W. Taylor Kimberly", "Juan E. Iglesias"], "title": "A Modality-agnostic Multi-task Foundation Model for Human Brain Imaging", "categories": ["cs.CV"], "comment": "16 pages", "summary": "Recent learning-based approaches have made astonishing advances in calibrated medical imaging like computerized tomography (CT), yet they struggle to generalize in uncalibrated modalities -- notably magnetic resonance (MR) imaging, where performance is highly sensitive to the differences in MR contrast, resolution, and orientation. This prevents broad applicability to diverse real-world clinical protocols. Here we introduce BrainFM, a modality-agnostic, multi-task vision foundation model for human brain imaging. With the proposed \"mild-to-severe\" intra-subject generation and \"real-synth\" mix-up training strategy, BrainFM is resilient to the appearance of acquired images (e.g., modality, contrast, deformation, resolution, artifacts), and can be directly applied to five fundamental brain imaging tasks, including image synthesis for CT and T1w/T2w/FLAIR MRI, anatomy segmentation, scalp-to-cortical distance, bias field estimation, and registration. We evaluate the efficacy of BrainFM on eleven public datasets, and demonstrate its robustness and effectiveness across all tasks and input modalities. Code is available at https://github.com/jhuldr/BrainFM.", "AI": {"tldr": "BrainFM\u662f\u4e00\u4e2a\u6a21\u6001\u65e0\u5173\u7684\u591a\u4efb\u52a1\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\"\u8f7b\u5ea6\u5230\u91cd\u5ea6\"\u751f\u6210\u548c\"\u771f\u5b9e-\u5408\u6210\"\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\uff0c\u80fd\u591f\u5904\u7406\u591a\u79cd\u8111\u6210\u50cf\u4efb\u52a1\uff0c\u5bf9\u56fe\u50cf\u5916\u89c2\u53d8\u5316\u5177\u6709\u5f3a\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u5728\u6821\u51c6\u533b\u5b66\u6210\u50cf\uff08\u5982CT\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u672a\u6821\u51c6\u6a21\u6001\uff08\u7279\u522b\u662fMRI\uff09\u4e2d\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u5bf9MR\u5bf9\u6bd4\u5ea6\u3001\u5206\u8fa8\u7387\u548c\u65b9\u5411\u7684\u5dee\u5f02\u654f\u611f\uff0c\u9650\u5236\u4e86\u5728\u591a\u6837\u5316\u4e34\u5e8a\u534f\u8bae\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u63d0\u51fa\"\u8f7b\u5ea6\u5230\u91cd\u5ea6\"\u4e3b\u4f53\u5185\u751f\u6210\u548c\"\u771f\u5b9e-\u5408\u6210\"\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\uff0c\u4f7f\u6a21\u578b\u5bf9\u83b7\u53d6\u56fe\u50cf\u7684\u5916\u89c2\uff08\u5982\u6a21\u6001\u3001\u5bf9\u6bd4\u5ea6\u3001\u53d8\u5f62\u3001\u5206\u8fa8\u7387\u3001\u4f2a\u5f71\uff09\u5177\u6709\u5f39\u6027\u3002", "result": "\u572811\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u8bc1\u660eBrainFM\u5728\u6240\u6709\u4efb\u52a1\u548c\u8f93\u5165\u6a21\u6001\u4e2d\u90fd\u5177\u6709\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "BrainFM\u4f5c\u4e3a\u4e00\u4e2a\u6a21\u6001\u65e0\u5173\u7684\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u76f4\u63a5\u5e94\u7528\u4e8e\u4e94\u79cd\u57fa\u672c\u8111\u6210\u50cf\u4efb\u52a1\uff0c\u4e3a\u591a\u6837\u5316\u4e34\u5e8a\u534f\u8bae\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2509.00578", "pdf": "https://arxiv.org/pdf/2509.00578", "abs": "https://arxiv.org/abs/2509.00578", "authors": ["Abdellah Zakaria Sellam", "Ilyes Benaissa", "Salah Eddine Bekhouche", "Abdenour Hadid", "Vito Ren\u00f3", "Cosimo Distante"], "title": "C-DiffDet+: Fusing Global Scene Context with Generative Denoising for High-Fidelity Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Fine-grained object detection in challenging visual domains, such as vehicle damage assessment, presents a formidable challenge even for human experts to resolve reliably. While DiffusionDet has advanced the state-of-the-art through conditional denoising diffusion, its performance remains limited by local feature conditioning in context-dependent scenarios. We address this fundamental limitation by introducing Context-Aware Fusion (CAF), which leverages cross-attention mechanisms to integrate global scene context with local proposal features directly. The global context is generated using a separate dedicated encoder that captures comprehensive environmental information, enabling each object proposal to attend to scene-level understanding. Our framework significantly enhances the generative detection paradigm by enabling each object proposal to attend to comprehensive environmental information. Experimental results demonstrate an improvement over state-of-the-art models on the CarDD benchmark, establishing new performance benchmarks for context-aware object detection in fine-grained domains", "AI": {"tldr": "\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u878d\u5408(CAF)\u6280\u672f\uff0c\u5728\u6c7d\u8f66\u635f\u574f\u68c0\u6d4b\u9886\u57df\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u7ec6\u7c92\u5ea6\u7269\u4f53\u68c0\u6d4b\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7684DiffusionDet\u6a21\u578b", "motivation": "\u89e3\u51b3DiffusionDet\u5728\u4e0a\u4e0b\u6587\u4f9d\u8d56\u573a\u666f\u4e2d\u4ec5\u4f7f\u7528\u5c40\u90e8\u7279\u5f81\u6761\u4ef6\u5316\u7684\u6027\u80fd\u9650\u5236\u95ee\u9898", "method": "\u63d0\u51fa\u4e0a\u4e0b\u6587\u611f\u77e5\u878d\u5408(CAF)\u673a\u5236\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u96c6\u6210\u5168\u5c40\u573a\u666f\u4e0a\u4e0b\u6587\u548c\u5c40\u90e8\u5efa\u8bae\u7279\u5f81\uff0c\u4f7f\u7528\u72ec\u7acb\u7f16\u7801\u5668\u83b7\u53d6\u5168\u9762\u73af\u5883\u4fe1\u606f", "result": "\u5728CarDD\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f73\u6a21\u578b\uff0c\u4e3a\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7ec6\u7c92\u5ea6\u7269\u4f53\u68c0\u6d4b\u5efa\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u6807\u6746", "conclusion": "CAF\u6846\u67b6\u901a\u8fc7\u5168\u5c40\u4e0a\u4e0b\u6587\u878d\u5408\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u5f0f\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u4e3a\u5177\u6709\u6311\u6218\u6027\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.00665", "pdf": "https://arxiv.org/pdf/2509.00665", "abs": "https://arxiv.org/abs/2509.00665", "authors": ["Weilong Yan", "Xin Zhang", "Robby T. Tan"], "title": "ER-LoRA: Effective-Rank Guided Adaptation for Weather-Generalized Depth Estimation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Monocular depth estimation under adverse weather conditions (e.g.\\ rain, fog, snow, and nighttime) remains highly challenging due to the lack of reliable ground truth and the difficulty of learning from unlabeled real-world data. Existing methods often rely on synthetic adverse data with pseudo-labels, which suffer from domain gaps, or employ self-supervised learning, which violates photometric assumptions in adverse scenarios. In this work, we propose to achieve weather--generalized depth estimation by Parameter--Efficient Fine--Tuning (PEFT) of Vision Foundation Models (VFMs), using only a small amount of high--visibility (normal) data. While PEFT has shown strong performance in semantic tasks such as segmentation, it remains underexplored for geometry--centric tasks like depth estimation -- especially in terms of balancing effective adaptation with the preservation of pretrained knowledge. To this end, we introduce the Selecting--Tuning--Maintaining (STM) strategy, which structurally decomposes the pretrained weights of VFMs based on two kinds of effective ranks (entropy--rank and stable--rank). In the tuning phase, we adaptively select the proper rank number as well as the task--aware singular directions for initialization, based on the entropy--rank and full--tuned weight; while in the maintaining stage, we enforce a principal direction regularization based on the stable--rank. This design guarantees flexible task adaptation while preserving the strong generalization capability of the pretrained VFM. Extensive experiments on four real--world benchmarks across diverse weather conditions demonstrate that STM not only outperforms existing PEFT methods and full fine--tuning but also surpasses methods trained with adverse synthetic data, and even the depth foundation model", "AI": {"tldr": "\u63d0\u51faSTM\u7b56\u7565\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u4ec5\u4f7f\u7528\u5c11\u91cf\u6b63\u5e38\u5929\u6c14\u6570\u636e\u5b9e\u73b0\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff0c\u5728\u591a\u4e2a\u771f\u5b9e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u9762\u4e34\u7f3a\u4e4f\u53ef\u9760\u771f\u503c\u548c\u96be\u4ee5\u4ece\u672a\u6807\u6ce8\u771f\u5b9e\u6570\u636e\u5b66\u4e60\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u57df\u5dee\u8ddd\u6216\u8fdd\u53cd\u5149\u5ea6\u5047\u8bbe\u7684\u95ee\u9898", "method": "\u5f15\u5165\u9009\u62e9-\u8c03\u4f18-\u4fdd\u6301(STM)\u7b56\u7565\uff0c\u57fa\u4e8e\u71b5\u79e9\u548c\u7a33\u5b9a\u79e9\u7ed3\u6784\u5206\u89e3\u9884\u8bad\u7ec3\u6743\u91cd\uff0c\u81ea\u9002\u5e94\u9009\u62e9\u79e9\u6570\u548c\u4efb\u52a1\u611f\u77e5\u5947\u5f02\u65b9\u5411\u8fdb\u884c\u521d\u59cb\u5316\uff0c\u5e76\u65bd\u52a0\u4e3b\u65b9\u5411\u6b63\u5219\u5316", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSTM\u4e0d\u4ec5\u4f18\u4e8e\u73b0\u6709PEFT\u65b9\u6cd5\u548c\u5168\u5fae\u8c03\uff0c\u8fd8\u8d85\u8d8a\u4e86\u4f7f\u7528\u5408\u6210\u6076\u52a3\u6570\u636e\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86\u6df1\u5ea6\u57fa\u7840\u6a21\u578b", "conclusion": "STM\u7b56\u7565\u80fd\u591f\u7075\u6d3b\u9002\u5e94\u4efb\u52a1\u540c\u65f6\u4fdd\u6301\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u6df1\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.00757", "pdf": "https://arxiv.org/pdf/2509.00757", "abs": "https://arxiv.org/abs/2509.00757", "authors": ["Xiufeng Huang", "Ziyuan Luo", "Qi Song", "Ruofei Wang", "Renjie Wan"], "title": "MarkSplatter: Generalizable Watermarking for 3D Gaussian Splatting Model via Splatter Image Structure", "categories": ["cs.CV"], "comment": null, "summary": "The growing popularity of 3D Gaussian Splatting (3DGS) has intensified the need for effective copyright protection. Current 3DGS watermarking methods rely on computationally expensive fine-tuning procedures for each predefined message. We propose the first generalizable watermarking framework that enables efficient protection of Splatter Image-based 3DGS models through a single forward pass. We introduce GaussianBridge that transforms unstructured 3D Gaussians into Splatter Image format, enabling direct neural processing for arbitrary message embedding. To ensure imperceptibility, we design a Gaussian-Uncertainty-Perceptual heatmap prediction strategy for preserving visual quality. For robust message recovery, we develop a dense segmentation-based extraction mechanism that maintains reliable extraction even when watermarked objects occupy minimal regions in rendered views. Project page: https://kevinhuangxf.github.io/marksplatter.", "AI": {"tldr": "\u9996\u4e2a\u901a\u7528\u53163D\u9ad8\u65af\u6cfc\u6e85\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u5b9e\u73b0\u9ad8\u6548\u7248\u6743\u4fdd\u62a4\uff0c\u65e0\u9700\u5bf9\u6bcf\u4e2a\u9884\u5b9a\u4e49\u6d88\u606f\u8fdb\u884c\u6602\u8d35\u5fae\u8c03", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u65e5\u76ca\u6d41\u884c\uff0c\u9700\u8981\u6709\u6548\u7684\u7248\u6743\u4fdd\u62a4\u65b9\u6cd5\u3002\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u9700\u8981\u5bf9\u6bcf\u4e2a\u9884\u5b9a\u4e49\u6d88\u606f\u8fdb\u884c\u8ba1\u7b97\u6602\u8d35\u7684\u5fae\u8c03", "method": "\u63d0\u51faGaussianBridge\u5c06\u975e\u7ed3\u6784\u53163D\u9ad8\u65af\u8f6c\u6362\u4e3aSplatter Image\u683c\u5f0f\uff0c\u5b9e\u73b0\u76f4\u63a5\u795e\u7ecf\u5904\u7406\uff1b\u8bbe\u8ba1\u9ad8\u65af\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u70ed\u56fe\u9884\u6d4b\u7b56\u7565\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\uff1b\u5f00\u53d1\u57fa\u4e8e\u5bc6\u96c6\u5206\u5272\u7684\u63d0\u53d6\u673a\u5236\u786e\u4fdd\u9c81\u68d2\u6027", "result": "\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5355\u6b21\u524d\u5411\u4f20\u64ad\u6c34\u5370\u5d4c\u5165\uff0c\u5728\u6e32\u67d3\u89c6\u56fe\u4e2d\u6c34\u5370\u5bf9\u8c61\u5360\u636e\u6700\u5c0f\u533a\u57df\u65f6\u4ecd\u80fd\u4fdd\u6301\u53ef\u9760\u63d0\u53d6", "conclusion": "\u8be5\u6846\u67b6\u4e3aSplatter Image-based 3DGS\u6a21\u578b\u63d0\u4f9b\u4e86\u9996\u4e2a\u901a\u7528\u5316\u6c34\u5370\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u4e0d\u53ef\u611f\u77e5\u4e14\u9c81\u68d2\u7684\u7248\u6743\u4fdd\u62a4"}}
{"id": "2509.00787", "pdf": "https://arxiv.org/pdf/2509.00787", "abs": "https://arxiv.org/abs/2509.00787", "authors": ["Ganxi Xu", "Jinyi Long", "Jia Zhang"], "title": "Diffusion-Based Image-to-Brain Signal Generation with Cross-Attention Mechanisms for Visual Prostheses", "categories": ["cs.CV"], "comment": null, "summary": "Visual prostheses have shown great potential in restoring vision for blind individuals. On the one hand, researchers have been continuously improving the brain decoding framework of visual prostheses by leveraging the powerful image generation capabilities of diffusion models. On the other hand, the brain encoding stage of visual prostheses struggles to generate brain signals with sufficient biological similarity. Although existing works have recognized this problem, the quality of predicted stimuli still remains a critical issue, as existing approaches typically lack supervised signals from real brain responses to validate the biological plausibility of predicted stimuli. To address this issue, we propose a novel image-to-brain framework based on denoising diffusion probabilistic models (DDPMs) enhanced with cross-attention mechanisms. Our framework consists of two key architectural components: a pre-trained CLIP visual encoder that extracts rich semantic representations from input images, and a cross-attention enhanced U-Net diffusion model that learns to reconstruct biologically plausible brain signals through iterative denoising. Unlike conventional generative models that rely on simple concatenation for conditioning, our cross-attention modules enable dynamic interaction between visual features and brain signal representations, facilitating fine-grained alignment during the generation process. We evaluate our framework on two multimodal datasets (THINGS-EEG2 and THINGS-MEG) to demonstrate its effectiveness in generating biologically plausible brain signals. Moreover, we visualize the training and test M/EEG topographies for all subjects on both datasets to intuitively demonstrate the intra-subject variations and inter-subject variations in M/EEG signals.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53bb\u566a\u6db2\u4f53\u6a21\u578b(DDPM)\u7684\u56fe\u50cf-\u5927\u8111\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u89c6\u89c9\u4e49\u80ce\u4fee\u590d\u88c5\u7f6e\u4e2d\u5927\u8111\u4fe1\u53f7\u7684\u751f\u7269\u5b66\u53ef\u4fe1\u6027\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u4e49\u80ce\u4fee\u590d\u88c5\u7f6e\u5728\u5927\u8111\u7f16\u7801\u9636\u6bb5\u9047\u5230\u56f0\u96be\uff0c\u9884\u6d4b\u523a\u6fc0\u7684\u751f\u7269\u5b66\u53ef\u4fe1\u6027\u8d28\u91cf\u4e0d\u9ad8\uff0c\u7f3a\u4e4f\u6765\u81ea\u771f\u5b9e\u5927\u8111\u54cd\u5e94\u7684\u76d1\u7763\u4fe1\u53f7\u3002", "method": "\u6784\u5efa\u4e86\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u9884\u8bad\u7ec3\u7684CLIP\u89c6\u89c9\u7f16\u7801\u5668\u63d0\u53d6\u8bed\u4e49\u8868\u5f81\uff0c\u4ee5\u53ca\u8de8\u6ce8\u610f\u529b\u589e\u5f3a\u7684U-Net\u6db2\u4f53\u6a21\u578b\u901a\u8fc7\u8fed\u4ee3\u53bb\u566a\u5b66\u4e60\u91cd\u5efa\u751f\u7269\u53ef\u4fe1\u7684\u5927\u8111\u4fe1\u53f7\u3002\u8de8\u6ce8\u610f\u529b\u6a21\u5757\u5141\u8bb8\u89c6\u89c9\u7279\u5f81\u4e0e\u5927\u8111\u4fe1\u53f7\u8868\u5f81\u4e4b\u95f4\u7684\u52a8\u6001\u4ea4\u4e92\u3002", "result": "\u5728THINGS-EEG2\u548cTHINGS-MEG\u4e24\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u5728\u751f\u6210\u751f\u7269\u53ef\u4fe1\u7684\u5927\u8111\u4fe1\u53f7\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u53ef\u89c6\u5316\u4e86\u8bad\u7ec3\u548c\u6d4b\u8bd5\u9636\u6bb5\u7684M/EEG\u5730\u5f62\u56fe\uff0c\u5c55\u793a\u4e86\u4e3b\u4f53\u5185\u548c\u4e3b\u4f53\u95f4\u7684\u53d8\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u4e86\u89c6\u89c9\u7279\u5f81\u4e0e\u5927\u8111\u4fe1\u53f7\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u4e49\u80ce\u4fee\u590d\u88c5\u7f6e\u4e2d\u9884\u6d4b\u5927\u8111\u4fe1\u53f7\u7684\u751f\u7269\u5b66\u53ef\u4fe1\u6027\u3002"}}
{"id": "2509.00800", "pdf": "https://arxiv.org/pdf/2509.00800", "abs": "https://arxiv.org/abs/2509.00800", "authors": ["Zhuodong Jiang", "Haoran Wang", "Guoxi Huang", "Brett Seymour", "Nantheera Anantrasirichai"], "title": "SWAGSplatting: Semantic-guided Water-scene Augmented Gaussian Splatting", "categories": ["cs.CV"], "comment": "Submitted to SIGGRAPH Asia 2025 Technical Communications", "summary": "Accurate 3D reconstruction in underwater environments remains a complex challenge due to issues such as light distortion, turbidity, and limited visibility. AI-based techniques have been applied to address these issues, however, existing methods have yet to fully exploit the potential of AI, particularly in integrating language models with visual processing. In this paper, we propose a novel framework that leverages multimodal cross-knowledge to create semantic-guided 3D Gaussian Splatting for robust and high-fidelity deep-sea scene reconstruction. By embedding an extra semantic feature into each Gaussian primitive and supervised by the CLIP extracted semantic feature, our method enforces semantic and structural awareness throughout the training. The dedicated semantic consistency loss ensures alignment with high-level scene understanding. Besides, we propose a novel stage-wise training strategy, combining coarse-to-fine learning with late-stage parameter refinement, to further enhance both stability and reconstruction quality. Extensive results show that our approach consistently outperforms state-of-the-art methods on SeaThru-NeRF and Submerged3D datasets across three metrics, with an improvement of up to 3.09 dB on average in terms of PSNR, making it a strong candidate for applications in underwater exploration and marine perception.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u5f15\u5bfc\u76843D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u77e5\u8bc6\u878d\u5408\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u4e0b3D\u91cd\u5efa\u7684\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u6c34\u4e0b3D\u91cd\u5efa\u9762\u4e34\u5149\u7ebf\u5931\u771f\u3001\u6d51\u6d4a\u548c\u80fd\u89c1\u5ea6\u4f4e\u7b49\u6311\u6218\uff0c\u73b0\u6709AI\u65b9\u6cd5\u5c1a\u672a\u5145\u5206\u5229\u7528\u8bed\u8a00\u6a21\u578b\u4e0e\u89c6\u89c9\u5904\u7406\u7684\u6574\u5408\u6f5c\u529b\u3002", "method": "\u57283D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\u4e2d\u4e3a\u6bcf\u4e2a\u9ad8\u65af\u57fa\u5143\u5d4c\u5165\u8bed\u4e49\u7279\u5f81\uff0c\u4f7f\u7528CLIP\u63d0\u53d6\u7684\u8bed\u4e49\u7279\u5f81\u8fdb\u884c\u76d1\u7763\uff0c\u5e76\u91c7\u7528\u5206\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff08\u4ece\u7c97\u5230\u7ec6\u5b66\u4e60\u52a0\u540e\u671f\u53c2\u6570\u4f18\u5316\uff09\u3002", "result": "\u5728SeaThru-NeRF\u548cSubmerged3D\u6570\u636e\u96c6\u4e0a\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0cPSNR\u5e73\u5747\u63d0\u5347\u9ad8\u8fbe3.09 dB\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6c34\u4e0b\u63a2\u7d22\u548c\u6d77\u6d0b\u611f\u77e5\u5e94\u7528\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u8bed\u4e49\u5f15\u5bfc\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2509.00831", "pdf": "https://arxiv.org/pdf/2509.00831", "abs": "https://arxiv.org/abs/2509.00831", "authors": ["Zhijing Wu", "Longguang Wang"], "title": "UPGS: Unified Pose-aware Gaussian Splatting for Dynamic Scene Deblurring", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing dynamic 3D scenes from monocular video has broad applications in AR/VR, robotics, and autonomous navigation, but often fails due to severe motion blur caused by camera and object motion. Existing methods commonly follow a two-step pipeline, where camera poses are first estimated and then 3D Gaussians are optimized. Since blurring artifacts usually undermine pose estimation, pose errors could be accumulated to produce inferior reconstruction results. To address this issue, we introduce a unified optimization framework by incorporating camera poses as learnable parameters complementary to 3DGS attributes for end-to-end optimization. Specifically, we recast camera and object motion as per-primitive SE(3) affine transformations on 3D Gaussians and formulate a unified optimization objective. For stable optimization, we introduce a three-stage training schedule that optimizes camera poses and Gaussians alternatively. Particularly, 3D Gaussians are first trained with poses being fixed, and then poses are optimized with 3D Gaussians being untouched. Finally, all learnable parameters are optimized together. Extensive experiments on the Stereo Blur dataset and challenging real-world sequences demonstrate that our method achieves significant gains in reconstruction quality and pose estimation accuracy over prior dynamic deblurring methods.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u76f8\u673a\u4f4d\u59ff\u4f5c\u4e3a\u53ef\u5b66\u4e60\u53c2\u6570\u4e0e3D\u9ad8\u65af\u5c5e\u6027\u8054\u5408\u4f18\u5316\uff0c\u89e3\u51b3\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e2d\u8fd0\u52a8\u6a21\u7cca\u5bfc\u81f4\u7684\u4f4d\u59ff\u4f30\u8ba1\u8bef\u5dee\u95ee\u9898", "motivation": "\u5355\u76ee\u89c6\u9891\u52a8\u60013D\u91cd\u5efa\u4e2d\uff0c\u76f8\u673a\u548c\u7269\u4f53\u8fd0\u52a8\u5bfc\u81f4\u7684\u4e25\u91cd\u8fd0\u52a8\u6a21\u7cca\u4f1a\u7834\u574f\u4f4d\u59ff\u4f30\u8ba1\uff0c\u73b0\u6709\u4e24\u6b65\u6d41\u6c34\u7ebf\u65b9\u6cd5\u4f1a\u7d2f\u79ef\u4f4d\u59ff\u8bef\u5dee\uff0c\u5bfc\u81f4\u91cd\u5efa\u8d28\u91cf\u4e0b\u964d", "method": "\u5c06\u76f8\u673a\u548c\u7269\u4f53\u8fd0\u52a8\u5efa\u6a21\u4e3a3D\u9ad8\u65af\u7684SE(3)\u4eff\u5c04\u53d8\u6362\uff0c\u63d0\u51fa\u7edf\u4e00\u4f18\u5316\u76ee\u6807\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u5148\u56fa\u5b9a\u4f4d\u59ff\u4f18\u5316\u9ad8\u65af\uff0c\u518d\u56fa\u5b9a\u9ad8\u65af\u4f18\u5316\u4f4d\u59ff\uff0c\u6700\u540e\u8054\u5408\u4f18\u5316\u6240\u6709\u53c2\u6570", "result": "\u5728Stereo Blur\u6570\u636e\u96c6\u548c\u771f\u5b9e\u573a\u666f\u5e8f\u5217\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u4f4d\u59ff\u4f30\u8ba1\u7cbe\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u52a8\u6001\u53bb\u6a21\u7cca\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u7aef\u5230\u7aef\u8054\u5408\u4f18\u5316\u76f8\u673a\u4f4d\u59ff\u548c3D\u9ad8\u65af\u5c5e\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8fd0\u52a8\u6a21\u7cca\u5bfc\u81f4\u7684\u4f4d\u59ff\u4f30\u8ba1\u548c\u91cd\u5efa\u8d28\u91cf\u95ee\u9898\uff0c\u4e3a\u52a8\u6001\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2509.00833", "pdf": "https://arxiv.org/pdf/2509.00833", "abs": "https://arxiv.org/abs/2509.00833", "authors": ["Sicheng Yang", "Hongqiu Wang", "Zhaohu Xing", "Sixiang Chen", "Lei Zhu"], "title": "SegDINO: An Efficient Design for Medical and Natural Image Segmentation with DINO-V3", "categories": ["cs.CV"], "comment": null, "summary": "The DINO family of self-supervised vision models has shown remarkable transferability, yet effectively adapting their representations for segmentation remains challenging. Existing approaches often rely on heavy decoders with multi-scale fusion or complex upsampling, which introduce substantial parameter overhead and computational cost. In this work, we propose SegDINO, an efficient segmentation framework that couples a frozen DINOv3 backbone with a lightweight decoder. SegDINO extracts multi-level features from the pretrained encoder, aligns them to a common resolution and channel width, and utilizes a lightweight MLP head to directly predict segmentation masks. This design minimizes trainable parameters while preserving the representational power of foundation features. Extensive experiments across six benchmarks, including three medical datasets (TN3K, Kvasir-SEG, ISIC) and three natural image datasets (MSD, VMD-D, ViSha), demonstrate that SegDINO consistently achieves state-of-the-art performance compared to existing methods. Code is available at https://github.com/script-Yang/SegDINO.", "AI": {"tldr": "SegDINO\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u5206\u5272\u6846\u67b6\uff0c\u5c06\u51bb\u7ed3\u7684DINOv3\u4e3b\u5e72\u7f51\u7edc\u4e0e\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\u7ed3\u5408\uff0c\u5728\u591a\u4e2a\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684DINO\u6a21\u578b\u81ea\u9002\u5e94\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u91cd\u578b\u89e3\u7801\u5668\uff0c\u5e26\u6765\u5927\u91cf\u53c2\u6570\u5f00\u9500\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u51bb\u7ed3\u7684DINOv3\u4e3b\u5e72\u63d0\u53d6\u591a\u7ea7\u7279\u5f81\uff0c\u5bf9\u9f50\u5230\u7edf\u4e00\u5206\u8fa8\u7387\u548c\u901a\u9053\u5bbd\u5ea6\uff0c\u7136\u540e\u901a\u8fc7\u8f7b\u91cf\u7ea7MLP\u5934\u76f4\u63a5\u9884\u6d4b\u5206\u5272\u63a9\u7801\u3002", "result": "\u57286\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec3\u4e2a\u533b\u5b66\u6570\u636e\u96c6\u548c3\u4e2a\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\uff09\u4e0a\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "conclusion": "SegDINO\u8bc1\u660e\u4e86\u5728\u4fdd\u6301\u57fa\u7840\u7279\u5f81\u8868\u793a\u80fd\u529b\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u5206\u5272\u4efb\u52a1\u3002"}}
{"id": "2509.00835", "pdf": "https://arxiv.org/pdf/2509.00835", "abs": "https://arxiv.org/abs/2509.00835", "authors": ["Jongwook Si", "Sungyoung Kim"], "title": "Satellite Image Utilization for Dehazing with Swin Transformer-Hybrid U-Net and Watershed loss", "categories": ["cs.CV"], "comment": null, "summary": "Satellite imagery plays a crucial role in various fields; however, atmospheric interference and haze significantly degrade image clarity and reduce the accuracy of information extraction. To address these challenges, this paper proposes a hybrid dehazing framework that integrates Swin Transformer and U-Net to balance global context learning and local detail restoration, called SUFERNOBWA. The proposed network employs SwinRRDB, a Swin Transformer-based Residual-in-Residual Dense Block, in both the encoder and decoder to effectively extract features. This module enables the joint learning of global contextual information and fine spatial structures, which is crucial for structural preservation in satellite image. Furthermore, we introduce a composite loss function that combines L2 loss, guided loss, and a novel watershed loss, which enhances structural boundary preservation and ensures pixel-level accuracy. This architecture enables robust dehazing under diverse atmospheric conditions while maintaining structural consistency across restored images. Experimental results demonstrate that the proposed method outperforms state-of-the-art models on both the RICE and SateHaze1K datasets. Specifically, on the RICE dataset, the proposed approach achieved a PSNR of 33.24 dB and an SSIM of 0.967, which is a significant improvement over existing method. This study provides an effective solution for mitigating atmospheric interference in satellite imagery and highlights its potential applicability across diverse remote sensing applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Swin Transformer\u548cU-Net\u7684\u6df7\u5408\u53bb\u96fe\u6846\u67b6SUFERNOBWA\uff0c\u7528\u4e8e\u536b\u661f\u56fe\u50cf\u53bb\u96fe\uff0c\u5728RICE\u548cSateHaze1K\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u536b\u661f\u56fe\u50cf\u5728\u5927\u6c14\u5e72\u6270\u548c\u96fe\u973e\u5f71\u54cd\u4e0b\u6e05\u6670\u5ea6\u4e0b\u964d\uff0c\u4fe1\u606f\u63d0\u53d6\u51c6\u786e\u6027\u964d\u4f4e\uff0c\u9700\u8981\u6709\u6548\u7684\u53bb\u96fe\u65b9\u6cd5\u6765\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002", "method": "\u4f7f\u7528Swin Transformer\u548cU-Net\u7ed3\u5408\u7684\u6df7\u5408\u67b6\u6784\uff0c\u91c7\u7528SwinRRDB\u6a21\u5757\u5728\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u4e2d\u63d0\u53d6\u7279\u5f81\uff0c\u7ed3\u5408\u5168\u5c40\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u5c40\u90e8\u7ec6\u8282\u6062\u590d\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5305\u542bL2\u635f\u5931\u3001\u5f15\u5bfc\u635f\u5931\u548c\u65b0\u578b\u5206\u6c34\u5cad\u635f\u5931\u7684\u590d\u5408\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728RICE\u6570\u636e\u96c6\u4e0a\u8fbe\u5230PSNR 33.24 dB\u548cSSIM 0.967\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728SateHaze1K\u6570\u636e\u96c6\u4e0a\u4e5f\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u8f7b\u536b\u661f\u56fe\u50cf\u4e2d\u7684\u5927\u6c14\u5e72\u6270\uff0c\u5728\u4e0d\u540c\u5927\u6c14\u6761\u4ef6\u4e0b\u4fdd\u6301\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u5728\u9065\u611f\u5e94\u7528\u4e2d\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.00843", "pdf": "https://arxiv.org/pdf/2509.00843", "abs": "https://arxiv.org/abs/2509.00843", "authors": ["Xueyang Kang", "Zhengkang Xiang", "Zezheng Zhang", "Kourosh Khoshelham"], "title": "Look Beyond: Two-Stage Scene View Generation via Panorama and Video Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": "26 pages, 30 figures, 2025 ACM Multimedia", "summary": "Novel view synthesis (NVS) from a single image is highly ill-posed due to large unobserved regions, especially for views that deviate significantly from the input. While existing methods focus on consistency between the source and generated views, they often fail to maintain coherence and correct view alignment across long-range or looped trajectories. We propose a model that addresses this by decomposing single-view NVS into a 360-degree scene extrapolation followed by novel view interpolation. This design ensures long-term view and scene consistency by conditioning on keyframes extracted and warped from a generated panoramic representation. In the first stage, a panorama diffusion model learns the scene prior from the input perspective image. Perspective keyframes are then sampled and warped from the panorama and used as anchor frames in a pre-trained video diffusion model, which generates novel views through a proposed spatial noise diffusion process. Compared to prior work, our method produces globally consistent novel views -- even in loop closure scenarios -- while enabling flexible camera control. Experiments on diverse scene datasets demonstrate that our approach outperforms existing methods in generating coherent views along user-defined trajectories. Our implementation is available at https://github.com/YiGuYT/LookBeyond.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5355\u89c6\u56fe\u65b0\u89c6\u89d2\u5408\u6210\u5206\u89e3\u4e3a360\u5ea6\u573a\u666f\u5916\u63a8\u548c\u89c6\u89d2\u63d2\u503c\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u666f\u6269\u6563\u6a21\u578b\u548c\u89c6\u9891\u6269\u6563\u6a21\u578b\u786e\u4fdd\u957f\u671f\u89c6\u89d2\u4e00\u81f4\u6027", "motivation": "\u73b0\u6709\u5355\u56fe\u50cf\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\u5728\u5904\u7406\u5927\u5e45\u504f\u79bb\u8f93\u5165\u89c6\u89d2\u6216\u5faa\u73af\u8f68\u8ff9\u65f6\uff0c\u96be\u4ee5\u4fdd\u6301\u573a\u666f\u4e00\u81f4\u6027\u548c\u6b63\u786e\u7684\u89c6\u89d2\u5bf9\u9f50", "method": "\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u5168\u666f\u6269\u6563\u6a21\u578b\u4ece\u8f93\u5165\u89c6\u89d2\u56fe\u50cf\u5b66\u4e60\u573a\u666f\u5148\u9a8c\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4ece\u751f\u6210\u7684\u5168\u666f\u56fe\u4e2d\u91c7\u6837\u5e76\u626d\u66f2\u5173\u952e\u5e27\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u8fdb\u884c\u7a7a\u95f4\u566a\u58f0\u6269\u6563\u751f\u6210\u65b0\u89c6\u89d2", "result": "\u5728\u591a\u6837\u5316\u573a\u666f\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7528\u6237\u5b9a\u4e49\u8f68\u8ff9\u4e0a\u751f\u6210\u8fde\u8d2f\u89c6\u56fe\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u5faa\u73af\u95ed\u5408\u573a\u666f\u4e2d\u4e5f\u80fd\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5206\u89e3\u7b56\u7565\u548c\u5173\u952e\u5e27\u6761\u4ef6\u5316\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5355\u89c6\u56fe\u65b0\u89c6\u89d2\u5408\u6210\u7684\u957f\u671f\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u540c\u65f6\u652f\u6301\u7075\u6d3b\u7684\u76f8\u673a\u63a7\u5236"}}
{"id": "2509.00917", "pdf": "https://arxiv.org/pdf/2509.00917", "abs": "https://arxiv.org/abs/2509.00917", "authors": ["Youngjin Oh", "Junhyeong Kwon", "Junyoung Park", "Nam Ik Cho"], "title": "DarkVRAI: Capture-Condition Conditioning and Burst-Order Selective Scan for Low-light RAW Video Denoising", "categories": ["cs.CV"], "comment": null, "summary": "Low-light RAW video denoising is a fundamentally challenging task due to severe signal degradation caused by high sensor gain and short exposure times, which are inherently limited by video frame rate requirements. To address this, we propose DarkVRAI, a novel framework that achieved first place in the AIM 2025 Low-light RAW Video Denoising Challenge. Our method introduces two primary contributions: (1) a successful application of a conditioning scheme for image denoising, which explicitly leverages capture metadata, to video denoising to guide the alignment and denoising processes, and (2) a Burst-Order Selective Scan (BOSS) mechanism that effectively models long-range temporal dependencies within the noisy video sequence. By synergistically combining these components, DarkVRAI demonstrates state-of-the-art performance on a rigorous and realistic benchmark dataset, setting a new standard for low-light video denoising.", "AI": {"tldr": "DarkVRAI\u662f\u4e00\u4e2a\u9488\u5bf9\u4f4e\u5149\u7167RAW\u89c6\u9891\u53bb\u566a\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6761\u4ef6\u5316\u65b9\u6848\u548cBOSS\u673a\u5236\uff0c\u5728AIM 2025\u6311\u6218\u8d5b\u4e2d\u53d6\u5f97\u7b2c\u4e00\u540d\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f4e\u5149\u7167RAW\u89c6\u9891\u7531\u4e8e\u9ad8\u4f20\u611f\u5668\u589e\u76ca\u548c\u77ed\u66dd\u5149\u65f6\u95f4\u5bfc\u81f4\u7684\u4e25\u91cd\u4fe1\u53f7\u9000\u5316\u95ee\u9898\uff0c\u8fd9\u4e9b\u9650\u5236\u6e90\u4e8e\u89c6\u9891\u5e27\u7387\u8981\u6c42\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u4e3b\u8981\u8d21\u732e\uff1a1) \u5c06\u56fe\u50cf\u53bb\u566a\u7684\u6761\u4ef6\u5316\u65b9\u6848\u6210\u529f\u5e94\u7528\u4e8e\u89c6\u9891\u53bb\u566a\uff0c\u5229\u7528\u6355\u83b7\u5143\u6570\u636e\u6307\u5bfc\u5bf9\u9f50\u548c\u53bb\u566a\u8fc7\u7a0b\uff1b2) \u63d0\u51faBurst-Order Selective Scan (BOSS)\u673a\u5236\uff0c\u6709\u6548\u5efa\u6a21\u566a\u58f0\u89c6\u9891\u5e8f\u5217\u4e2d\u7684\u957f\u7a0b\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "result": "\u5728\u4e25\u683c\u4e14\u771f\u5b9e\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e3a\u4f4e\u5149\u7167\u89c6\u9891\u53bb\u566a\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\u3002", "conclusion": "\u901a\u8fc7\u534f\u540c\u7ed3\u5408\u6761\u4ef6\u5316\u65b9\u6848\u548cBOSS\u673a\u5236\uff0cDarkVRAI\u6846\u67b6\u5728\u4f4e\u5149\u7167RAW\u89c6\u9891\u53bb\u566a\u65b9\u9762\u53d6\u5f97\u4e86\u7a81\u7834\u6027\u8fdb\u5c55\uff0c\u8d62\u5f97\u4e86AIM 2025\u6311\u6218\u8d5b\u7b2c\u4e00\u540d\u3002"}}
{"id": "2509.00989", "pdf": "https://arxiv.org/pdf/2509.00989", "abs": "https://arxiv.org/abs/2509.00989", "authors": ["Josef Gr\u00fcn", "Lukas Meyer", "Maximilian Weiherer", "Bernhard Egger", "Marc Stamminger", "Linus Franke"], "title": "Towards Integrating Multi-Spectral Imaging with Gaussian Splatting", "categories": ["cs.CV"], "comment": "for project page, see   https://meyerls.github.io/towards_multi_spec_splat/", "summary": "We present a study of how to integrate color (RGB) and multi-spectral imagery (red, green, red-edge, and near-infrared) into the 3D Gaussian Splatting (3DGS) framework, a state-of-the-art explicit radiance-field-based method for fast and high-fidelity 3D reconstruction from multi-view images. While 3DGS excels on RGB data, naive per-band optimization of additional spectra yields poor reconstructions due to inconsistently appearing geometry in the spectral domain. This problem is prominent, even though the actual geometry is the same, regardless of spectral modality. To investigate this, we evaluate three strategies: 1) Separate per-band reconstruction with no shared structure. 2) Splitting optimization, in which we first optimize RGB geometry, copy it, and then fit each new band to the model by optimizing both geometry and band representation. 3) Joint, in which the modalities are jointly optimized, optionally with an initial RGB-only phase. We showcase through quantitative metrics and qualitative novel-view renderings on multi-spectral datasets the effectiveness of our dedicated optimized Joint strategy, increasing overall spectral reconstruction as well as enhancing RGB results through spectral cross-talk. We therefore suggest integrating multi-spectral data directly into the spherical harmonics color components to compactly model each Gaussian's multi-spectral reflectance. Moreover, our analysis reveals several key trade-offs in when and how to introduce spectral bands during optimization, offering practical insights for robust multi-modal 3DGS reconstruction.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u57283D\u9ad8\u65af\u6cfc\u6e85(3DGS)\u6846\u67b6\u4e2d\u6709\u6548\u6574\u5408RGB\u548c\u591a\u5149\u8c31\u56fe\u50cf\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u4f18\u5316\u7b56\u7565\u5e76\u8bc1\u660e\u4e86\u8054\u5408\u4f18\u5316\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u867d\u71363DGS\u5728RGB\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u76f4\u63a5\u5c06\u591a\u5149\u8c31\u6ce2\u6bb5\u8fdb\u884c\u72ec\u7acb\u4f18\u5316\u4f1a\u5bfc\u81f4\u91cd\u5efa\u8d28\u91cf\u4e0d\u4f73\uff0c\u56e0\u4e3a\u4e0d\u540c\u5149\u8c31\u57df\u4e2d\u7684\u51e0\u4f55\u7ed3\u6784\u4f1a\u51fa\u73b0\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "\u8bc4\u4f30\u4e86\u4e09\u79cd\u7b56\u7565\uff1a1) \u72ec\u7acb\u6ce2\u6bb5\u91cd\u5efa\uff1b2) \u5206\u5272\u4f18\u5316\uff08\u5148\u4f18\u5316RGB\u51e0\u4f55\u518d\u590d\u5236\u5230\u5176\u4ed6\u6ce2\u6bb5\uff09\uff1b3) \u8054\u5408\u4f18\u5316\uff08\u53ef\u9009\u7684RGB\u521d\u59cb\u9636\u6bb5\uff09\u3002\u63d0\u51fa\u4e86\u5c06\u591a\u5149\u8c31\u6570\u636e\u76f4\u63a5\u6574\u5408\u5230\u7403\u8c10\u989c\u8272\u7ec4\u4ef6\u4e2d\u7684\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5b9a\u91cf\u6307\u6807\u548c\u5b9a\u6027\u65b0\u9896\u89c6\u89d2\u6e32\u67d3\u9a8c\u8bc1\u4e86\u8054\u5408\u4f18\u5316\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6574\u4f53\u5149\u8c31\u91cd\u5efa\u8d28\u91cf\uff0c\u8fd8\u901a\u8fc7\u5149\u8c31\u4e32\u6270\u589e\u5f3a\u4e86RGB\u7ed3\u679c\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u5f15\u5165\u5149\u8c31\u6ce2\u6bb5\u7684\u5173\u952e\u6743\u8861\uff0c\u4e3a\u9c81\u68d2\u7684\u591a\u6a21\u60013DGS\u91cd\u5efa\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u5efa\u8bae\u5c06\u591a\u5149\u8c31\u6570\u636e\u76f4\u63a5\u6574\u5408\u5230\u7403\u8c10\u989c\u8272\u7ec4\u4ef6\u4e2d\u4ee5\u7d27\u51d1\u5efa\u6a21\u6bcf\u4e2a\u9ad8\u65af\u7684\u53cd\u5c04\u7279\u6027\u3002"}}
{"id": "2509.01028", "pdf": "https://arxiv.org/pdf/2509.01028", "abs": "https://arxiv.org/abs/2509.01028", "authors": ["Zixin Zhu", "Kevin Duarte", "Mamshad Nayeem Rizve", "Chengyuan Xu", "Ratheesh Kalarot", "Junsong Yuan"], "title": "CompSlider: Compositional Slider for Disentangled Multiple-Attribute Image Generation", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "In text-to-image (T2I) generation, achieving fine-grained control over attributes - such as age or smile - remains challenging, even with detailed text prompts. Slider-based methods offer a solution for precise control of image attributes. Existing approaches typically train individual adapter for each attribute separately, overlooking the entanglement among multiple attributes. As a result, interference occurs among different attributes, preventing precise control of multiple attributes together. To address this challenge, we aim to disentangle multiple attributes in slider-based generation to enbale more reliable and independent attribute manipulation. Our approach, CompSlider, can generate a conditional prior for the T2I foundation model to control multiple attributes simultaneously. Furthermore, we introduce novel disentanglement and structure losses to compose multiple attribute changes while maintaining structural consistency within the image. Since CompSlider operates in the latent space of the conditional prior and does not require retraining the foundation model, it reduces the computational burden for both training and inference. We evaluate our approach on a variety of image attributes and highlight its generality by extending to video generation.", "AI": {"tldr": "CompSlider\u662f\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6ed1\u5757\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u80fd\u591f\u540c\u65f6\u7cbe\u786e\u63a7\u5236\u591a\u4e2a\u56fe\u50cf\u5c5e\u6027\uff0c\u901a\u8fc7\u89e3\u8026\u591a\u4e2a\u5c5e\u6027\u548c\u4fdd\u6301\u7ed3\u6784\u4e00\u81f4\u6027\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u5c5e\u6027\u5e72\u6270\u95ee\u9898\u3002", "motivation": "\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\uff0c\u5373\u4f7f\u4f7f\u7528\u8be6\u7ec6\u7684\u6587\u672c\u63d0\u793a\uff0c\u4e5f\u96be\u4ee5\u5b9e\u73b0\u5bf9\u5c5e\u6027\uff08\u5982\u5e74\u9f84\u6216\u5fae\u7b11\uff09\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002\u73b0\u6709\u57fa\u4e8e\u6ed1\u5757\u7684\u65b9\u6cd5\u901a\u5e38\u4e3a\u6bcf\u4e2a\u5c5e\u6027\u5355\u72ec\u8bad\u7ec3\u9002\u914d\u5668\uff0c\u5ffd\u7565\u4e86\u591a\u4e2a\u5c5e\u6027\u4e4b\u95f4\u7684\u7ea0\u7f20\uff0c\u5bfc\u81f4\u4e0d\u540c\u5c5e\u6027\u4e4b\u95f4\u4ea7\u751f\u5e72\u6270\uff0c\u65e0\u6cd5\u540c\u65f6\u7cbe\u786e\u63a7\u5236\u591a\u4e2a\u5c5e\u6027\u3002", "method": "\u63d0\u51faCompSlider\u65b9\u6cd5\uff0c\u4e3aT2I\u57fa\u7840\u6a21\u578b\u751f\u6210\u6761\u4ef6\u5148\u9a8c\u4ee5\u540c\u65f6\u63a7\u5236\u591a\u4e2a\u5c5e\u6027\u3002\u5f15\u5165\u65b0\u9896\u7684\u89e3\u8026\u635f\u5931\u548c\u7ed3\u6784\u635f\u5931\u6765\u7ec4\u5408\u591a\u4e2a\u5c5e\u6027\u53d8\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u5185\u7684\u7ed3\u6784\u4e00\u81f4\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u6761\u4ef6\u5148\u9a8c\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u3002", "result": "CompSlider\u51cf\u5c11\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u80fd\u591f\u53ef\u9760\u4e14\u72ec\u7acb\u5730\u64cd\u7eb5\u591a\u4e2a\u5c5e\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u56fe\u50cf\u5c5e\u6027\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7\u6269\u5c55\u5230\u89c6\u9891\u751f\u6210\u5c55\u793a\u4e86\u5176\u901a\u7528\u6027\u3002", "conclusion": "CompSlider\u901a\u8fc7\u89e3\u8026\u591a\u4e2a\u5c5e\u6027\u548c\u5f15\u5165\u7ed3\u6784\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u57fa\u4e8e\u6ed1\u5757\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u591a\u5c5e\u6027\u63a7\u5236\u7684\u5e72\u6270\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u9760\u548c\u72ec\u7acb\u7684\u5c5e\u6027\u64cd\u4f5c\uff0c\u540c\u65f6\u5177\u6709\u8ba1\u7b97\u6548\u7387\u548c\u901a\u7528\u6027\u4f18\u52bf\u3002"}}
{"id": "2509.01071", "pdf": "https://arxiv.org/pdf/2509.01071", "abs": "https://arxiv.org/abs/2509.01071", "authors": ["Ziyi Liu", "Zhe Xu", "Jiabo Ma", "Wenqaing Li", "Junlin Hou", "Fuxiang Huang", "Xi Wang", "Ronald Cheong Kin Chan", "Terence Tsz Wai Wong", "Hao Chen"], "title": "A Unified Low-level Foundation Model for Enhancing Pathology Image Quality", "categories": ["cs.CV"], "comment": null, "summary": "Foundation models have revolutionized computational pathology by achieving remarkable success in high-level diagnostic tasks, yet the critical challenge of low-level image enhancement remains largely unaddressed. Real-world pathology images frequently suffer from degradations such as noise, blur, and low resolution due to slide preparation artifacts, staining variability, and imaging constraints, while the reliance on physical staining introduces significant costs, delays, and inconsistency. Although existing methods target individual problems like denoising or super-resolution, their task-specific designs lack the versatility to handle the diverse low-level vision challenges encountered in practice. To bridge this gap, we propose the first unified Low-level Pathology Foundation Model (LPFM), capable of enhancing image quality in restoration tasks, including super-resolution, deblurring, and denoising, as well as facilitating image translation tasks like virtual staining (H&E and special stains), all through a single adaptable architecture. Our approach introduces a contrastive pre-trained encoder that learns transferable, stain-invariant feature representations from 190 million unlabeled pathology images, enabling robust identification of degradation patterns. A unified conditional diffusion process dynamically adapts to specific tasks via textual prompts, ensuring precise control over output quality. Trained on a curated dataset of 87,810 whole slied images (WSIs) across 34 tissue types and 5 staining protocols, LPFM demonstrates statistically significant improvements (p<0.01) over state-of-the-art methods in most tasks (56/66), achieving Peak Signal-to-Noise Ratio (PSNR) gains of 10-15% for image restoration and Structural Similarity Index Measure (SSIM) improvements of 12-18% for virtual staining.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7edf\u4e00\u7684\u4f4e\u7ea7\u522b\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578bLPFM\uff0c\u901a\u8fc7\u5355\u4e00\u67b6\u6784\u5904\u7406\u56fe\u50cf\u6062\u590d\uff08\u8d85\u5206\u8fa8\u7387\u3001\u53bb\u6a21\u7cca\u3001\u53bb\u566a\uff09\u548c\u865a\u62df\u67d3\u8272\u4efb\u52a1\uff0c\u5728190\u4e07\u65e0\u6807\u7b7e\u75c5\u7406\u56fe\u50cf\u4e0a\u9884\u8bad\u7ec3\uff0c\u5728\u5927\u591a\u6570\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u75c5\u7406\u56fe\u50cf\u5b58\u5728\u566a\u58f0\u3001\u6a21\u7cca\u3001\u4f4e\u5206\u8fa8\u7387\u7b49\u9000\u5316\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u9488\u5bf9\u5355\u4e00\u4efb\u52a1\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u5904\u7406\u591a\u6837\u5316\u4f4e\u7ea7\u522b\u89c6\u89c9\u6311\u6218\u7684\u901a\u7528\u6027\uff0c\u4e14\u7269\u7406\u67d3\u8272\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\u3001\u4e0d\u4e00\u81f4\u3002", "method": "\u4f7f\u7528\u5bf9\u6bd4\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u5b66\u4e60\u53ef\u8fc1\u79fb\u7684\u67d3\u8272\u4e0d\u53d8\u7279\u5f81\u8868\u793a\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u6761\u4ef6\u6269\u6563\u8fc7\u7a0b\u6839\u636e\u6587\u672c\u63d0\u793a\u52a8\u6001\u9002\u5e94\u7279\u5b9a\u4efb\u52a1\uff0c\u572887,810\u5f20\u5168\u5207\u7247\u56fe\u50cf\u4e0a\u8bad\u7ec3\u3002", "result": "\u5728\u5927\u591a\u6570\u4efb\u52a1\uff0856/66\uff09\u4e2d\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff08p<0.01\uff09\uff0c\u56fe\u50cf\u6062\u590d\u4efb\u52a1PSNR\u63d0\u534710-15%\uff0c\u865a\u62df\u67d3\u8272\u4efb\u52a1SSIM\u63d0\u534712-18%\u3002", "conclusion": "LPFM\u4f5c\u4e3a\u9996\u4e2a\u7edf\u4e00\u7684\u4f4e\u7ea7\u522b\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u79cd\u56fe\u50cf\u589e\u5f3a\u4efb\u52a1\uff0c\u4e3a\u75c5\u7406\u5b66\u56fe\u50cf\u5904\u7406\u63d0\u4f9b\u4e86\u901a\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.01107", "pdf": "https://arxiv.org/pdf/2509.01107", "abs": "https://arxiv.org/abs/2509.01107", "authors": ["Wenzhuang Wang", "Yifan Zhao", "Mingcan Ma", "Ming Liu", "Zhonglin Jiang", "Yong Chen", "Jia Li"], "title": "FICGen: Frequency-Inspired Contextual Disentanglement for Layout-driven Degraded Image Generation", "categories": ["cs.CV"], "comment": "21 pages, 19 figures, ICCV 2025", "summary": "Layout-to-image (L2I) generation has exhibited promising results in natural domains, but suffers from limited generative fidelity and weak alignment with user-provided layouts when applied to degraded scenes (i.e., low-light, underwater). We primarily attribute these limitations to the \"contextual illusion dilemma\" in degraded conditions, where foreground instances are overwhelmed by context-dominant frequency distributions. Motivated by this, our paper proposes a new Frequency-Inspired Contextual Disentanglement Generative (FICGen) paradigm, which seeks to transfer frequency knowledge of degraded images into the latent diffusion space, thereby facilitating the rendering of degraded instances and their surroundings via contextual frequency-aware guidance. To be specific, FICGen consists of two major steps. Firstly, we introduce a learnable dual-query mechanism, each paired with a dedicated frequency resampler, to extract contextual frequency prototypes from pre-collected degraded exemplars in the training set. Secondly, a visual-frequency enhanced attention is employed to inject frequency prototypes into the degraded generation process. To alleviate the contextual illusion and attribute leakage, an instance coherence map is developed to regulate latent-space disentanglement between individual instances and their surroundings, coupled with an adaptive spatial-frequency aggregation module to reconstruct spatial-frequency mixed degraded representations. Extensive experiments on 5 benchmarks involving a variety of degraded scenarios-from severe low-light to mild blur-demonstrate that FICGen consistently surpasses existing L2I methods in terms of generative fidelity, alignment and downstream auxiliary trainability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFICGen\u65b9\u6cd5\uff0c\u901a\u8fc7\u9891\u7387\u542f\u53d1\u7684\u4e0a\u4e0b\u6587\u89e3\u8026\u751f\u6210\u8303\u5f0f\uff0c\u89e3\u51b3\u9000\u5316\u573a\u666f\u4e0b\u5e03\u5c40\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u4e0a\u4e0b\u6587\u5e7b\u89c9\u56f0\u5883\uff0c\u63d0\u5347\u751f\u6210\u4fdd\u771f\u5ea6\u548c\u5e03\u5c40\u5bf9\u9f50\u6027\u3002", "motivation": "\u5728\u9000\u5316\u573a\u666f\uff08\u5982\u4f4e\u5149\u7167\u3001\u6c34\u4e0b\uff09\u4e2d\uff0c\u5e03\u5c40\u5230\u56fe\u50cf\u751f\u6210\u5b58\u5728\u751f\u6210\u4fdd\u771f\u5ea6\u6709\u9650\u548c\u4e0e\u7528\u6237\u63d0\u4f9b\u5e03\u5c40\u5bf9\u9f50\u6027\u5f31\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u524d\u666f\u5b9e\u4f8b\u88ab\u4e0a\u4e0b\u6587\u4e3b\u5bfc\u7684\u9891\u7387\u5206\u5e03\u6240\u6df9\u6ca1\u7684\"\u4e0a\u4e0b\u6587\u5e7b\u89c9\u56f0\u5883\"\u3002", "method": "\u63d0\u51fa\u9891\u7387\u542f\u53d1\u7684\u4e0a\u4e0b\u6587\u89e3\u8026\u751f\u6210\u8303\u5f0f\uff08FICGen\uff09\uff0c\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u6b65\u9aa4\uff1a1\uff09\u53ef\u5b66\u4e60\u7684\u53cc\u67e5\u8be2\u673a\u5236\u914d\u5408\u4e13\u7528\u9891\u7387\u91cd\u91c7\u6837\u5668\uff0c\u4ece\u8bad\u7ec3\u96c6\u9000\u5316\u6837\u672c\u4e2d\u63d0\u53d6\u4e0a\u4e0b\u6587\u9891\u7387\u539f\u578b\uff1b2\uff09\u89c6\u89c9\u9891\u7387\u589e\u5f3a\u6ce8\u610f\u529b\u673a\u5236\u6ce8\u5165\u9891\u7387\u539f\u578b\u5230\u9000\u5316\u751f\u6210\u8fc7\u7a0b\uff0c\u4f7f\u7528\u5b9e\u4f8b\u4e00\u81f4\u6027\u56fe\u548c\u81ea\u9002\u5e94\u7a7a\u95f4\u9891\u7387\u805a\u5408\u6a21\u5757\u6765\u8c03\u8282\u89e3\u8026\u548c\u91cd\u5efa\u8868\u793a\u3002", "result": "\u57285\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFICGen\u5728\u5404\u79cd\u9000\u5316\u573a\u666f\uff08\u4ece\u4e25\u91cd\u4f4e\u5149\u7167\u5230\u8f7b\u5fae\u6a21\u7cca\uff09\u4e2d\uff0c\u5728\u751f\u6210\u4fdd\u771f\u5ea6\u3001\u5bf9\u9f50\u6027\u548c\u4e0b\u6e38\u8f85\u52a9\u53ef\u8bad\u7ec3\u6027\u65b9\u9762 consistently \u8d85\u8d8a\u73b0\u6709L2I\u65b9\u6cd5\u3002", "conclusion": "FICGen\u901a\u8fc7\u5c06\u9000\u5316\u56fe\u50cf\u7684\u9891\u7387\u77e5\u8bc6\u8f6c\u79fb\u5230\u6f5c\u5728\u6269\u6563\u7a7a\u95f4\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9000\u5316\u573a\u666f\u4e0b\u7684\u4e0a\u4e0b\u6587\u5e7b\u89c9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e03\u5c40\u5230\u56fe\u50cf\u751f\u6210\u7684\u6027\u80fd\u3002"}}
{"id": "2509.01177", "pdf": "https://arxiv.org/pdf/2509.01177", "abs": "https://arxiv.org/abs/2509.01177", "authors": ["Junxiang Liu", "Junming Lin", "Jiangtong Li", "Jie Li"], "title": "DynaMind: Reconstructing Dynamic Visual Scenes from EEG by Aligning Temporal Dynamics and Multimodal Semantics to Guided Diffusion", "categories": ["cs.CV", "cs.AI", "cs.HC", "eess.SP"], "comment": "14 pages, 6 figures", "summary": "Reconstruction dynamic visual scenes from electroencephalography (EEG) signals remains a primary challenge in brain decoding, limited by the low spatial resolution of EEG, a temporal mismatch between neural recordings and video dynamics, and the insufficient use of semantic information within brain activity. Therefore, existing methods often inadequately resolve both the dynamic coherence and the complex semantic context of the perceived visual stimuli. To overcome these limitations, we introduce DynaMind, a novel framework that reconstructs video by jointly modeling neural dynamics and semantic features via three core modules: a Regional-aware Semantic Mapper (RSM), a Temporal-aware Dynamic Aligner (TDA), and a Dual-Guidance Video Reconstructor (DGVR). The RSM first utilizes a regional-aware encoder to extract multimodal semantic features from EEG signals across distinct brain regions, aggregating them into a unified diffusion prior. In the mean time, the TDA generates a dynamic latent sequence, or blueprint, to enforce temporal consistency between the feature representations and the original neural recordings. Together, guided by the semantic diffusion prior, the DGVR translates the temporal-aware blueprint into a high-fidelity video reconstruction. On the SEED-DV dataset, DynaMind sets a new state-of-the-art (SOTA), boosting reconstructed video accuracies (video- and frame-based) by 12.5 and 10.3 percentage points, respectively. It also achieves a leap in pixel-level quality, showing exceptional visual fidelity and temporal coherence with a 9.4% SSIM improvement and a 19.7% FVMD reduction. This marks a critical advancement, bridging the gap between neural dynamics and high-fidelity visual semantics.", "AI": {"tldr": "DynaMind\u662f\u4e00\u4e2a\u4eceEEG\u4fe1\u53f7\u91cd\u5efa\u52a8\u6001\u89c6\u89c9\u573a\u666f\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u795e\u7ecf\u52a8\u529b\u5b66\u548c\u8bed\u4e49\u7279\u5f81\uff0c\u5728\u89c6\u9891\u91cd\u5efa\u7cbe\u5ea6\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u5b9e\u73b0\u4e86\u663e\u8457\u63d0\u5347", "motivation": "\u89e3\u51b3EEG\u4fe1\u53f7\u4f4e\u7a7a\u95f4\u5206\u8fa8\u7387\u3001\u795e\u7ecf\u8bb0\u5f55\u4e0e\u89c6\u9891\u52a8\u6001\u65f6\u95f4\u4e0d\u5339\u914d\u3001\u4ee5\u53ca\u8111\u6d3b\u52a8\u4e2d\u8bed\u4e49\u4fe1\u606f\u5229\u7528\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5904\u7406\u52a8\u6001\u8fde\u8d2f\u6027\u548c\u590d\u6742\u8bed\u4e49\u4e0a\u4e0b\u6587", "method": "\u91c7\u7528\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u533a\u57df\u611f\u77e5\u8bed\u4e49\u6620\u5c04\u5668(RSM)\u63d0\u53d6\u591a\u6a21\u6001\u8bed\u4e49\u7279\u5f81\uff0c\u65f6\u95f4\u611f\u77e5\u52a8\u6001\u5bf9\u9f50\u5668(TDA)\u751f\u6210\u52a8\u6001\u6f5c\u5728\u5e8f\u5217\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u53cc\u5f15\u5bfc\u89c6\u9891\u91cd\u5efa\u5668(DGVR)\u5c06\u65f6\u5e8f\u84dd\u56fe\u8f6c\u6362\u4e3a\u9ad8\u4fdd\u771f\u89c6\u9891", "result": "\u5728SEED-DV\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u65b0\u7684SOTA\uff0c\u89c6\u9891\u548c\u5e27\u7ea7\u51c6\u786e\u7387\u5206\u522b\u63d0\u534712.5\u548c10.3\u4e2a\u767e\u5206\u70b9\uff0cSSIM\u63d0\u53479.4%\uff0cFVMD\u964d\u4f4e19.7%\uff0c\u663e\u793a\u51fa\u5353\u8d8a\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u65f6\u95f4\u8fde\u8d2f\u6027", "conclusion": "\u8be5\u6846\u67b6\u5728\u795e\u7ecf\u52a8\u529b\u5b66\u548c\u9ad8\u4fdd\u771f\u89c6\u89c9\u8bed\u4e49\u4e4b\u95f4\u67b6\u8d77\u4e86\u5173\u952e\u6865\u6881\uff0c\u6807\u5fd7\u7740\u8111\u89e3\u7801\u9886\u57df\u7684\u91cd\u5927\u8fdb\u5c55"}}
{"id": "2509.01181", "pdf": "https://arxiv.org/pdf/2509.01181", "abs": "https://arxiv.org/abs/2509.01181", "authors": ["Qiaoqiao Jin", "Siming Fu", "Dong She", "Weinan Jia", "Hualiang Wang", "Mu Liu", "Jidong Jiang"], "title": "FocusDPO: Dynamic Preference Optimization for Multi-Subject Personalized Image Generation via Adaptive Focus", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multi-subject personalized image generation aims to synthesize customized images containing multiple specified subjects without requiring test-time optimization. However, achieving fine-grained independent control over multiple subjects remains challenging due to difficulties in preserving subject fidelity and preventing cross-subject attribute leakage. We present FocusDPO, a framework that adaptively identifies focus regions based on dynamic semantic correspondence and supervision image complexity. During training, our method progressively adjusts these focal areas across noise timesteps, implementing a weighted strategy that rewards information-rich patches while penalizing regions with low prediction confidence. The framework dynamically adjusts focus allocation during the DPO process according to the semantic complexity of reference images and establishes robust correspondence mappings between generated and reference subjects. Extensive experiments demonstrate that our method substantially enhances the performance of existing pre-trained personalized generation models, achieving state-of-the-art results on both single-subject and multi-subject personalized image synthesis benchmarks. Our method effectively mitigates attribute leakage while preserving superior subject fidelity across diverse generation scenarios, advancing the frontier of controllable multi-subject image synthesis.", "AI": {"tldr": "FocusDPO\u662f\u4e00\u4e2a\u591a\u4e3b\u4f53\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8bed\u4e49\u5bf9\u5e94\u548c\u590d\u6742\u5ea6\u611f\u77e5\u7684\u7126\u70b9\u533a\u57df\u8bc6\u522b\uff0c\u5728DPO\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u81ea\u9002\u5e94\u7126\u70b9\u5206\u914d\uff0c\u6709\u6548\u9632\u6b62\u5c5e\u6027\u6cc4\u6f0f\u5e76\u4fdd\u6301\u4e3b\u4f53\u4fdd\u771f\u5ea6\u3002", "motivation": "\u591a\u4e3b\u4f53\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u9700\u8981\u5728\u4e0d\u8fdb\u884c\u6d4b\u8bd5\u65f6\u4f18\u5316\u7684\u524d\u63d0\u4e0b\u5408\u6210\u5305\u542b\u591a\u4e2a\u6307\u5b9a\u4e3b\u4f53\u7684\u5b9a\u5236\u56fe\u50cf\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u6301\u4e3b\u4f53\u4fdd\u771f\u5ea6\u548c\u9632\u6b62\u8de8\u4e3b\u4f53\u5c5e\u6027\u6cc4\u6f0f\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "\u57fa\u4e8e\u52a8\u6001\u8bed\u4e49\u5bf9\u5e94\u548c\u76d1\u7763\u56fe\u50cf\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u8bc6\u522b\u7126\u70b9\u533a\u57df\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8de8\u566a\u58f0\u65f6\u95f4\u6b65\u6e10\u8fdb\u8c03\u6574\u7126\u70b9\u533a\u57df\uff0c\u91c7\u7528\u52a0\u6743\u7b56\u7565\u5956\u52b1\u4fe1\u606f\u4e30\u5bcc\u533a\u57df\u5e76\u60e9\u7f5a\u4f4e\u7f6e\u4fe1\u5ea6\u533a\u57df\uff0c\u6839\u636e\u53c2\u8003\u56fe\u50cf\u8bed\u4e49\u590d\u6742\u5ea6\u52a8\u6001\u8c03\u6574DPO\u8fc7\u7a0b\u4e2d\u7684\u7126\u70b9\u5206\u914d\u3002", "result": "\u5728\u73b0\u6709\u9884\u8bad\u7ec3\u4e2a\u6027\u5316\u751f\u6210\u6a21\u578b\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5728\u5355\u4e3b\u4f53\u548c\u591a\u4e3b\u4f53\u4e2a\u6027\u5316\u56fe\u50cf\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u6709\u6548\u7f13\u89e3\u5c5e\u6027\u6cc4\u6f0f\u95ee\u9898\u5e76\u5728\u591a\u6837\u5316\u751f\u6210\u573a\u666f\u4e2d\u4fdd\u6301\u4f18\u5f02\u7684\u4e3b\u4f53\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63a8\u8fdb\u4e86\u53ef\u63a7\u591a\u4e3b\u4f53\u56fe\u50cf\u5408\u6210\u7684\u524d\u6cbf\uff0c\u901a\u8fc7\u52a8\u6001\u7126\u70b9\u5206\u914d\u673a\u5236\u5b9e\u73b0\u4e86\u5bf9\u591a\u4e2a\u4e3b\u4f53\u7684\u7ec6\u7c92\u5ea6\u72ec\u7acb\u63a7\u5236\u3002"}}
{"id": "2509.01214", "pdf": "https://arxiv.org/pdf/2509.01214", "abs": "https://arxiv.org/abs/2509.01214", "authors": ["Yizhe Yuan", "Bingsen Xue", "Bangzheng Pu", "Chengxiang Wang", "Cheng Jin"], "title": "PRINTER:Deformation-Aware Adversarial Learning for Virtual IHC Staining with In Situ Fidelity", "categories": ["cs.CV", "cs.MM"], "comment": "10 pages, 4 figures", "summary": "Tumor spatial heterogeneity analysis requires precise correlation between Hematoxylin and Eosin H&E morphology and immunohistochemical (IHC) biomarker expression, yet current methods suffer from spatial misalignment in consecutive sections, severely compromising in situ pathological interpretation. In order to obtain a more accurate virtual staining pattern, We propose PRINTER, a weakly-supervised framework that integrates PRototype-drIven content and staiNing patTERn decoupling and deformation-aware adversarial learning strategies designed to accurately learn IHC staining patterns while preserving H&E staining details. Our approach introduces three key innovations: (1) A prototype-driven staining pattern transfer with explicit content-style decoupling; and (2) A cyclic registration-synthesis framework GapBridge that bridges H&E and IHC domains through deformable structural alignment, where registered features guide cross-modal style transfer while synthesized outputs iteratively refine the registration;(3) Deformation-Aware Adversarial Learning: We propose a training framework where a generator and deformation-aware registration network jointly adversarially optimize a style-focused discriminator. Extensive experiments demonstrate that PRINTER effectively achieves superior performance in preserving H&E staining details and virtual staining fidelity, outperforming state-of-the-art methods. Our work provides a robust and scalable solution for virtual staining, advancing the field of computational pathology.", "AI": {"tldr": "PRINTER\u662f\u4e00\u4e2a\u5f31\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u539f\u578b\u9a71\u52a8\u7684\u5185\u5bb9-\u67d3\u8272\u6a21\u5f0f\u89e3\u8026\u548c\u53d8\u5f62\u611f\u77e5\u5bf9\u6297\u5b66\u4e60\uff0c\u5b9e\u73b0H&E\u5230IHC\u7684\u7cbe\u786e\u865a\u62df\u67d3\u8272\uff0c\u89e3\u51b3\u7ec4\u7ec7\u5207\u7247\u7a7a\u95f4\u9519\u4f4d\u95ee\u9898", "motivation": "\u80bf\u7624\u7a7a\u95f4\u5f02\u8d28\u6027\u5206\u6790\u9700\u8981\u7cbe\u786e\u5173\u8054H&E\u5f62\u6001\u5b66\u548cIHC\u751f\u7269\u6807\u5fd7\u7269\u8868\u8fbe\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u8fde\u7eed\u5207\u7247\u4e2d\u5b58\u5728\u7a7a\u95f4\u9519\u4f4d\u95ee\u9898\uff0c\u4e25\u91cd\u5f71\u54cd\u75c5\u7406\u5b66\u539f\u4f4d\u89e3\u91ca", "method": "\u63d0\u51fa\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1\uff09\u539f\u578b\u9a71\u52a8\u7684\u67d3\u8272\u6a21\u5f0f\u8f6c\u79fb\u4e0e\u663e\u5f0f\u5185\u5bb9-\u98ce\u683c\u89e3\u8026\uff1b2\uff09\u5faa\u73af\u914d\u51c6-\u5408\u6210\u6846\u67b6GapBridge\uff0c\u901a\u8fc7\u53ef\u53d8\u5f62\u7ed3\u6784\u5bf9\u9f50\u6865\u63a5H&E\u548cIHC\u57df\uff1b3\uff09\u53d8\u5f62\u611f\u77e5\u5bf9\u6297\u5b66\u4e60\uff0c\u751f\u6210\u5668\u548c\u53d8\u5f62\u611f\u77e5\u914d\u51c6\u7f51\u7edc\u8054\u5408\u5bf9\u6297\u4f18\u5316\u98ce\u683c\u805a\u7126\u5224\u522b\u5668", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660ePRINTER\u5728\u4fdd\u7559H&E\u67d3\u8272\u7ec6\u8282\u548c\u865a\u62df\u67d3\u8272\u4fdd\u771f\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u865a\u62df\u67d3\u8272\u63d0\u4f9b\u4e86\u7a33\u5065\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u8ba1\u7b97\u75c5\u7406\u5b66\u9886\u57df\u7684\u53d1\u5c55"}}
{"id": "2509.01232", "pdf": "https://arxiv.org/pdf/2509.01232", "abs": "https://arxiv.org/abs/2509.01232", "authors": ["Lingzhou Mu", "Qiang Wang", "Fan Jiang", "Mengchao Wang", "Yaqi Fan", "Mu Xu", "Kai Zhang"], "title": "FantasyHSI: Video-Generation-Centric 4D Human Synthesis In Any Scene through A Graph-based Multi-Agent Framework", "categories": ["cs.CV"], "comment": "https://fantasy-amap.github.io/fantasy-hsi/", "summary": "Human-Scene Interaction (HSI) seeks to generate realistic human behaviors within complex environments, yet it faces significant challenges in handling long-horizon, high-level tasks and generalizing to unseen scenes. To address these limitations, we introduce FantasyHSI, a novel HSI framework centered on video generation and multi-agent systems that operates without paired data. We model the complex interaction process as a dynamic directed graph, upon which we build a collaborative multi-agent system. This system comprises a scene navigator agent for environmental perception and high-level path planning, and a planning agent that decomposes long-horizon goals into atomic actions. Critically, we introduce a critic agent that establishes a closed-loop feedback mechanism by evaluating the deviation between generated actions and the planned path. This allows for the dynamic correction of trajectory drifts caused by the stochasticity of the generative model, thereby ensuring long-term logical consistency. To enhance the physical realism of the generated motions, we leverage Direct Preference Optimization (DPO) to train the action generator, significantly reducing artifacts such as limb distortion and foot-sliding. Extensive experiments on our custom SceneBench benchmark demonstrate that FantasyHSI significantly outperforms existing methods in terms of generalization, long-horizon task completion, and physical realism. Ours project page: https://fantasy-amap.github.io/fantasy-hsi/", "AI": {"tldr": "FantasyHSI\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u9891\u751f\u6210\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u4eba\u7c7b-\u573a\u666f\u4ea4\u4e92\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6709\u5411\u56fe\u5efa\u6a21\u4ea4\u4e92\u8fc7\u7a0b\uff0c\u5305\u542b\u573a\u666f\u5bfc\u822a\u5668\u3001\u89c4\u5212\u5668\u548c\u6279\u8bc4\u5668\u4e09\u4e2a\u667a\u80fd\u4f53\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u914d\u5bf9\u6570\u636e\u7684\u957f\u65f6\u7a0b\u3001\u9ad8\u5c42\u6b21\u4efb\u52a1\u751f\u6210\uff0c\u5e76\u5728\u7269\u7406\u771f\u5b9e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4eba\u7c7b-\u573a\u666f\u4ea4\u4e92\u4e2d\u957f\u65f6\u7a0b\u3001\u9ad8\u5c42\u6b21\u4efb\u52a1\u5904\u7406\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u6a21\u578b\u5728\u672a\u89c1\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u5bf9\u914d\u5bf9\u6570\u636e\u7684\u4f9d\u8d56\u3002", "method": "1) \u5c06\u590d\u6742\u4ea4\u4e92\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u52a8\u6001\u6709\u5411\u56fe\uff1b2) \u6784\u5efa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff1a\u573a\u666f\u5bfc\u822a\u5668\uff08\u73af\u5883\u611f\u77e5\u548c\u8def\u5f84\u89c4\u5212\uff09\u3001\u89c4\u5212\u5668\uff08\u76ee\u6807\u5206\u89e3\u4e3a\u539f\u5b50\u52a8\u4f5c\uff09\u3001\u6279\u8bc4\u5668\uff08\u95ed\u73af\u53cd\u9988\u673a\u5236\uff09\uff1b3) \u4f7f\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\u8bad\u7ec3\u52a8\u4f5c\u751f\u6210\u5668\u63d0\u5347\u7269\u7406\u771f\u5b9e\u6027\u3002", "result": "\u5728\u81ea\u5b9a\u4e49SceneBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFantasyHSI\u5728\u6cdb\u5316\u80fd\u529b\u3001\u957f\u65f6\u7a0b\u4efb\u52a1\u5b8c\u6210\u5ea6\u548c\u7269\u7406\u771f\u5b9e\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u80a2\u4f53\u626d\u66f2\u548c\u811a\u90e8\u6ed1\u52a8\u7b49\u4f2a\u5f71\u3002", "conclusion": "FantasyHSI\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u95ed\u73af\u53cd\u9988\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86HSI\u4e2d\u7684\u957f\u65f6\u7a0b\u4e00\u81f4\u6027\u548c\u7269\u7406\u771f\u5b9e\u6027\u95ee\u9898\uff0c\u4e3a\u65e0\u9700\u914d\u5bf9\u6570\u636e\u7684\u4eba\u7c7b\u884c\u4e3a\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.01317", "pdf": "https://arxiv.org/pdf/2509.01317", "abs": "https://arxiv.org/abs/2509.01317", "authors": ["Alexandros Gkillas", "Nikos Piperigkos", "Aris S. Lalos"], "title": "Guided Model-based LiDAR Super-Resolution for Resource-Efficient Automotive scene Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "High-resolution LiDAR data plays a critical role in 3D semantic segmentation for autonomous driving, but the high cost of advanced sensors limits large-scale deployment. In contrast, low-cost sensors such as 16-channel LiDAR produce sparse point clouds that degrade segmentation accuracy. To overcome this, we introduce the first end-to-end framework that jointly addresses LiDAR super-resolution (SR) and semantic segmentation. The framework employs joint optimization during training, allowing the SR module to incorporate semantic cues and preserve fine details, particularly for smaller object classes. A new SR loss function further directs the network to focus on regions of interest. The proposed lightweight, model-based SR architecture uses significantly fewer parameters than existing LiDAR SR approaches, while remaining easily compatible with segmentation networks. Experiments show that our method achieves segmentation performance comparable to models operating on high-resolution and costly 64-channel LiDAR data.", "AI": {"tldr": "\u9996\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u8054\u5408\u5904\u7406LiDAR\u8d85\u5206\u8fa8\u7387\u548c\u8bed\u4e49\u5206\u5272\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u548c\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u5728\u51cf\u5c11\u53c2\u6570\u7684\u540c\u65f6\u8fbe\u5230\u4e0e\u9ad8\u5206\u8fa8\u738764\u901a\u9053LiDAR\u76f8\u5f53\u7684\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387LiDAR\u6210\u672c\u9ad8\u6602\u9650\u5236\u4e86\u5927\u89c4\u6a21\u90e8\u7f72\uff0c\u800c\u4f4e\u6210\u672c16\u901a\u9053LiDAR\u4ea7\u751f\u7684\u7a00\u758f\u70b9\u4e91\u4f1a\u964d\u4f4e\u5206\u5272\u7cbe\u5ea6\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u77db\u76fe\u3002", "method": "\u91c7\u7528\u7aef\u5230\u7aef\u6846\u67b6\u8fdb\u884c\u8054\u5408\u4f18\u5316\u8bad\u7ec3\uff0cSR\u6a21\u5757\u878d\u5165\u8bed\u4e49\u7ebf\u7d22\u5e76\u4fdd\u7559\u7ec6\u8282\uff1b\u8bbe\u8ba1\u65b0\u7684SR\u635f\u5931\u51fd\u6570\u5173\u6ce8\u611f\u5174\u8da3\u533a\u57df\uff1b\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6a21\u578b\u67b6\u6784\u51cf\u5c11\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u8bed\u4e49\u5206\u5272\u6027\u80fd\u4e0a\u8fbe\u5230\u4e86\u4e0e\u6602\u8d3564\u901a\u9053LiDAR\u6570\u636e\u76f8\u5f53\u7684\u6c34\u5e73\u3002", "conclusion": "\u63d0\u51fa\u7684\u8054\u5408\u4f18\u5316\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u6210\u672cLiDAR\u4f20\u611f\u5668\u6027\u80fd\u53d7\u9650\u7684\u95ee\u9898\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5927\u89c4\u6a21\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.01362", "pdf": "https://arxiv.org/pdf/2509.01362", "abs": "https://arxiv.org/abs/2509.01362", "authors": ["Jiayi Gao", "Changcheng Hua", "Qingchao Chen", "Yuxin Peng", "Yang Liu"], "title": "Identity-Preserving Text-to-Video Generation via Training-Free Prompt, Image, and Guidance Enhancement", "categories": ["cs.CV", "cs.MM"], "comment": "7 pages, 3 figures", "summary": "Identity-preserving text-to-video (IPT2V) generation creates videos faithful to both a reference subject image and a text prompt. While fine-tuning large pretrained video diffusion models on ID-matched data achieves state-of-the-art results on IPT2V, data scarcity and high tuning costs hinder broader improvement. We thus introduce a Training-Free Prompt, Image, and Guidance Enhancement (TPIGE) framework that bridges the semantic gap between the video description and the reference image and design sampling guidance that enhances identity preservation and video quality, achieving performance gains at minimal cost.Specifically, we first propose Face Aware Prompt Enhancement, using GPT-4o to enhance the text prompt with facial details derived from the reference image. We then propose Prompt Aware Reference Image Enhancement, leveraging an identity-preserving image generator to refine the reference image, rectifying conflicts with the text prompt. The above mutual refinement significantly improves input quality before video generation. Finally, we propose ID-Aware Spatiotemporal Guidance Enhancement, utilizing unified gradients to optimize identity preservation and video quality jointly during generation.Our method outperforms prior work and is validated by automatic and human evaluations on a 1000 video test set, winning first place in the ACM Multimedia 2025 Identity-Preserving Video Generation Challenge, demonstrating state-of-the-art performance and strong generality. The code is available at https://github.com/Andyplus1/IPT2V.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u63d0\u793a\u3001\u56fe\u50cf\u548c\u5f15\u5bfc\u589e\u5f3a\u6846\u67b6(TPIGE)\uff0c\u901a\u8fc7\u76f8\u4e92\u589e\u5f3a\u6587\u672c\u63d0\u793a\u548c\u53c2\u8003\u56fe\u50cf\uff0c\u7ed3\u5408\u65f6\u7a7a\u5f15\u5bfc\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u8eab\u4efd\u4fdd\u6301\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u7684\u8eab\u4efd\u4fdd\u6301\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u5bf9\u5927\u578b\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u5fae\u8c03\uff0c\u4f46\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u548c\u9ad8\u8c03\u4f18\u6210\u672c\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u4f7f\u7528GPT-4o\u8fdb\u884c\u9762\u90e8\u611f\u77e5\u63d0\u793a\u589e\u5f3a\uff0c\u4ece\u53c2\u8003\u56fe\u50cf\u63d0\u53d6\u9762\u90e8\u7ec6\u8282\u4e30\u5bcc\u6587\u672c\u63d0\u793a\uff1b2) \u4f7f\u7528\u8eab\u4efd\u4fdd\u6301\u56fe\u50cf\u751f\u6210\u5668\u8fdb\u884c\u63d0\u793a\u611f\u77e5\u53c2\u8003\u56fe\u50cf\u589e\u5f3a\uff0c\u4fee\u6b63\u4e0e\u6587\u672c\u63d0\u793a\u7684\u51b2\u7a81\uff1b3) \u63d0\u51fa\u8eab\u4efd\u611f\u77e5\u65f6\u7a7a\u5f15\u5bfc\u589e\u5f3a\uff0c\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u8054\u5408\u4f18\u5316\u8eab\u4efd\u4fdd\u6301\u548c\u89c6\u9891\u8d28\u91cf\u3002", "result": "\u57281000\u4e2a\u89c6\u9891\u6d4b\u8bd5\u96c6\u4e0a\u901a\u8fc7\u81ea\u52a8\u548c\u4eba\u5de5\u8bc4\u4f30\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u4f18\u4e8e\u5148\u524d\u5de5\u4f5c\uff0c\u5728ACM Multimedia 2025\u8eab\u4efd\u4fdd\u6301\u89c6\u9891\u751f\u6210\u6311\u6218\u8d5b\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\uff0c\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "TPIGE\u6846\u67b6\u4ee5\u6700\u5c0f\u6210\u672c\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u89e3\u51b3\u4e86\u8eab\u4efd\u4fdd\u6301\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u9ad8\u6210\u672c\u95ee\u9898\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bad\u7ec3\u514d\u8d39\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.01373", "pdf": "https://arxiv.org/pdf/2509.01373", "abs": "https://arxiv.org/abs/2509.01373", "authors": ["Wei Lu", "Lingyu Zhu", "Si-Bao Chen"], "title": "Unsupervised Ultra-High-Resolution UAV Low-Light Image Enhancement: A Benchmark, Metric and Framework", "categories": ["cs.CV"], "comment": "18 pages, 10 figures", "summary": "Low light conditions significantly degrade Unmanned Aerial Vehicles (UAVs) performance in critical applications. Existing Low-light Image Enhancement (LIE) methods struggle with the unique challenges of aerial imagery, including Ultra-High Resolution (UHR), lack of paired data, severe non-uniform illumination, and deployment constraints. To address these issues, we propose three key contributions. First, we present U3D, the first unsupervised UHR UAV dataset for LIE, with a unified evaluation toolkit. Second, we introduce the Edge Efficiency Index (EEI), a novel metric balancing perceptual quality with key deployment factors: speed, resolution, model complexity, and memory footprint. Third, we develop U3LIE, an efficient framework with two training-only designs-Adaptive Pre-enhancement Augmentation (APA) for input normalization and a Luminance Interval Loss (L_int) for exposure control. U3LIE achieves SOTA results, processing 4K images at 23.8 FPS on a single GPU, making it ideal for real-time on-board deployment. In summary, these contributions provide a holistic solution (dataset, metric, and method) for advancing robust 24/7 UAV vision. The code and datasets are available at https://github.com/lwCVer/U3D_Toolkit.", "AI": {"tldr": "\u63d0\u51fa\u4e86U3D\u65e0\u4eba\u673a\u4f4e\u5149\u589e\u5f3a\u6570\u636e\u96c6\u3001EEI\u8bc4\u4f30\u6307\u6807\u548cU3LIE\u9ad8\u6548\u6846\u67b6\uff0c\u89e3\u51b3\u65e0\u4eba\u673a\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u7684\u72ec\u7279\u6311\u6218\uff0c\u5b9e\u73b0\u5b9e\u65f64K\u5904\u7406\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u4f4e\u5149\u6761\u4ef6\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u6709\u4f4e\u5149\u589e\u5f3a\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u65e0\u4eba\u673a\u56fe\u50cf\u7684\u8d85\u9ad8\u5206\u8fa8\u7387\u3001\u65e0\u914d\u5bf9\u6570\u636e\u3001\u975e\u5747\u5300\u5149\u7167\u548c\u90e8\u7f72\u9650\u5236\u7b49\u72ec\u7279\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86U3D\u65e0\u76d1\u7763\u8d85\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u96c6\uff1b\u63d0\u51faEEI\u6307\u6807\u5e73\u8861\u611f\u77e5\u8d28\u91cf\u548c\u90e8\u7f72\u6548\u7387\uff1b\u8bbe\u8ba1\u4e86U3LIE\u6846\u67b6\uff0c\u5305\u542b\u81ea\u9002\u5e94\u9884\u589e\u5f3a\u589e\u5f3a(APA)\u548c\u4eae\u5ea6\u533a\u95f4\u635f\u5931(L_int)\u4e24\u4e2a\u8bad\u7ec3\u7b56\u7565\u3002", "result": "U3LIE\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u5355GPU\u4e0a\u4ee523.8 FPS\u5904\u74064K\u56fe\u50cf\uff0c\u9002\u5408\u5b9e\u65f6\u673a\u8f7d\u90e8\u7f72\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6307\u6807\u548c\u65b9\u6cd5\u4e09\u4f4d\u4e00\u4f53\u7684\u5b8c\u6574\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u65e0\u4eba\u673a24/7\u5168\u5929\u5019\u89c6\u89c9\u80fd\u529b\u53d1\u5c55\u3002"}}
{"id": "2509.01405", "pdf": "https://arxiv.org/pdf/2509.01405", "abs": "https://arxiv.org/abs/2509.01405", "authors": ["Jianman Lin", "Tianshui Chen", "Chunmei Qing", "Zhijing Yang", "Shuangping Huang", "Yuheng Ren", "Liang Lin"], "title": "Neural Scene Designer: Self-Styled Semantic Image Manipulation", "categories": ["cs.CV"], "comment": null, "summary": "Maintaining stylistic consistency is crucial for the cohesion and aesthetic appeal of images, a fundamental requirement in effective image editing and inpainting. However, existing methods primarily focus on the semantic control of generated content, often neglecting the critical task of preserving this consistency. In this work, we introduce the Neural Scene Designer (NSD), a novel framework that enables photo-realistic manipulation of user-specified scene regions while ensuring both semantic alignment with user intent and stylistic consistency with the surrounding environment. NSD leverages an advanced diffusion model, incorporating two parallel cross-attention mechanisms that separately process text and style information to achieve the dual objectives of semantic control and style consistency. To capture fine-grained style representations, we propose the Progressive Self-style Representational Learning (PSRL) module. This module is predicated on the intuitive premise that different regions within a single image share a consistent style, whereas regions from different images exhibit distinct styles. The PSRL module employs a style contrastive loss that encourages high similarity between representations from the same image while enforcing dissimilarity between those from different images. Furthermore, to address the lack of standardized evaluation protocols for this task, we establish a comprehensive benchmark. This benchmark includes competing algorithms, dedicated style-related metrics, and diverse datasets and settings to facilitate fair comparisons. Extensive experiments conducted on our benchmark demonstrate the effectiveness of the proposed framework.", "AI": {"tldr": "NSD\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5e76\u884c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u548c\u6e10\u8fdb\u5f0f\u81ea\u98ce\u683c\u8868\u793a\u5b66\u4e60\u6a21\u5757\uff0c\u5b9e\u73b0\u56fe\u50cf\u7f16\u8f91\u4e2d\u7684\u8bed\u4e49\u63a7\u5236\u548c\u98ce\u683c\u4e00\u81f4\u6027\u4fdd\u6301\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8bed\u4e49\u63a7\u5236\uff0c\u4f46\u5ffd\u89c6\u4e86\u98ce\u683c\u4e00\u81f4\u6027\u7684\u91cd\u8981\u6027\uff0c\u8fd9\u5f71\u54cd\u4e86\u56fe\u50cf\u7684\u6574\u4f53\u7f8e\u611f\u548c\u8fde\u8d2f\u6027\u3002", "method": "\u63d0\u51faNeural Scene Designer\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u5e76\u884c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5206\u522b\u5904\u7406\u6587\u672c\u548c\u98ce\u683c\u4fe1\u606f\uff0c\u5e76\u8bbe\u8ba1PSRL\u6a21\u5757\u901a\u8fc7\u98ce\u683c\u5bf9\u6bd4\u635f\u5931\u5b66\u4e60\u7ec6\u7c92\u5ea6\u98ce\u683c\u8868\u793a\u3002", "result": "\u5728\u5efa\u7acb\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNSD\u6846\u67b6\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5b9e\u73b0\u903c\u771f\u7684\u56fe\u50cf\u64cd\u4f5c\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u5bf9\u9f50\u548c\u98ce\u683c\u4e00\u81f4\u6027\u3002", "conclusion": "NSD\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u50cf\u7f16\u8f91\u4e2d\u7684\u98ce\u683c\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u7684PSRL\u6a21\u5757\u548c\u8bc4\u4f30\u57fa\u51c6\u4e3a\u8be5\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d21\u732e\u3002"}}
{"id": "2509.01411", "pdf": "https://arxiv.org/pdf/2509.01411", "abs": "https://arxiv.org/abs/2509.01411", "authors": ["U\u011fur \u00c7o\u011falan", "Mojtaba Bemana", "Karol Myszkowski", "Hans-Peter Seidel", "Colin Groth"], "title": "MILO: A Lightweight Perceptual Quality Metric for Image and Latent-Space Optimization", "categories": ["cs.CV"], "comment": "11 pages, 7 figures", "summary": "We present MILO (Metric for Image- and Latent-space Optimization), a lightweight, multiscale, perceptual metric for full-reference image quality assessment (FR-IQA). MILO is trained using pseudo-MOS (Mean Opinion Score) supervision, in which reproducible distortions are applied to diverse images and scored via an ensemble of recent quality metrics that account for visual masking effects. This approach enables accurate learning without requiring large-scale human-labeled datasets. Despite its compact architecture, MILO outperforms existing metrics across standard FR-IQA benchmarks and offers fast inference suitable for real-time applications. Beyond quality prediction, we demonstrate the utility of MILO as a perceptual loss in both image and latent domains. In particular, we show that spatial masking modeled by MILO, when applied to latent representations from a VAE encoder within Stable Diffusion, enables efficient and perceptually aligned optimization. By combining spatial masking with a curriculum learning strategy, we first process perceptually less relevant regions before progressively shifting the optimization to more visually distorted areas. This strategy leads to significantly improved performance in tasks like denoising, super-resolution, and face restoration, while also reducing computational overhead. MILO thus functions as both a state-of-the-art image quality metric and as a practical tool for perceptual optimization in generative pipelines.", "AI": {"tldr": "MILO\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u591a\u5c3a\u5ea6\u611f\u77e5\u5ea6\u91cf\u6807\u51c6\uff0c\u7528\u4e8e\u5168\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff0c\u901a\u8fc7\u4f2aMOS\u76d1\u7763\u8bad\u7ec3\uff0c\u5728\u56fe\u50cf\u548c\u6f5c\u5728\u7a7a\u95f4\u4e2d\u4f5c\u4e3a\u611f\u77e5\u635f\u5931\u51fd\u6570\u8868\u73b0\u51fa\u8272", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u9700\u8981\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u5ea6\u91cf\u6807\u51c6\u5728\u611f\u77e5\u5bf9\u9f50\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u7684\u4e0d\u8db3", "method": "\u4f7f\u7528\u53ef\u91cd\u73b0\u7684\u5931\u771f\u56fe\u50cf\u548c\u96c6\u6210\u8d28\u91cf\u5ea6\u91cf\u751f\u6210\u4f2aMOS\u76d1\u7763\u6570\u636e\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u591a\u5c3a\u5ea6\u7f51\u7edc\uff0c\u7ed3\u5408\u7a7a\u95f4\u63a9\u853d\u548c\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u8fdb\u884c\u611f\u77e5\u4f18\u5316", "result": "\u5728\u6807\u51c6FR-IQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u5ea6\u91cf\u6807\u51c6\uff0c\u5728\u53bb\u566a\u3001\u8d85\u5206\u8fa8\u7387\u548c\u4eba\u8138\u4fee\u590d\u7b49\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500", "conclusion": "MILO\u4e0d\u4ec5\u662f\u6700\u5148\u8fdb\u7684\u56fe\u50cf\u8d28\u91cf\u5ea6\u91cf\u6807\u51c6\uff0c\u8fd8\u662f\u751f\u6210\u7ba1\u9053\u4e2d\u611f\u77e5\u4f18\u5316\u7684\u5b9e\u7528\u5de5\u5177\uff0c\u5177\u6709\u5feb\u901f\u63a8\u7406\u548c\u5b9e\u65f6\u5e94\u7528\u80fd\u529b"}}
{"id": "2509.01421", "pdf": "https://arxiv.org/pdf/2509.01421", "abs": "https://arxiv.org/abs/2509.01421", "authors": ["Guohui Zhang", "Jiangtong Tan", "Linjiang Huang", "Zhonghang Yuan", "Naishan Zheng", "Jie Huang", "Feng Zhao"], "title": "InfoScale: Unleashing Training-free Variable-scaled Image Generation via Effective Utilization of Information", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models (DMs) have become dominant in visual generation but suffer performance drop when tested on resolutions that differ from the training scale, whether lower or higher. In fact, the key challenge in generating variable-scale images lies in the differing amounts of information across resolutions, which requires information conversion procedures to be varied for generating variable-scaled images. In this paper, we investigate the issues of three critical aspects in DMs for a unified analysis in variable-scaled generation: dilated convolution, attention mechanisms, and initial noise. Specifically, 1) dilated convolution in DMs for the higher-resolution generation loses high-frequency information. 2) Attention for variable-scaled image generation struggles to adjust the information aggregation adaptively. 3) The spatial distribution of information in the initial noise is misaligned with variable-scaled image. To solve the above problems, we propose \\textbf{InfoScale}, an information-centric framework for variable-scaled image generation by effectively utilizing information from three aspects correspondingly. For information loss in 1), we introduce Progressive Frequency Compensation module to compensate for high-frequency information lost by dilated convolution in higher-resolution generation. For information aggregation inflexibility in 2), we introduce Adaptive Information Aggregation module to adaptively aggregate information in lower-resolution generation and achieve an effective balance between local and global information in higher-resolution generation. For information distribution misalignment in 3), we design Noise Adaptation module to re-distribute information in initial noise for variable-scaled generation. Our method is plug-and-play for DMs and extensive experiments demonstrate the effectiveness in variable-scaled image generation.", "AI": {"tldr": "InfoScale\u662f\u4e00\u4e2a\u4fe1\u606f\u4e2d\u5fc3\u5316\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6e10\u8fdb\u9891\u7387\u8865\u507f\u3001\u81ea\u9002\u5e94\u4fe1\u606f\u805a\u5408\u548c\u566a\u58f0\u9002\u5e94\u4e09\u4e2a\u6a21\u5757\uff0c\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u4fe1\u606f\u635f\u5931\u3001\u805a\u5408\u4e0d\u7075\u6d3b\u548c\u5206\u5e03\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u8bad\u7ec3\u5c3a\u5ea6\u4ee5\u5916\u7684\u5206\u8fa8\u7387\u4e0a\u751f\u6210\u56fe\u50cf\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u4e0d\u540c\u5206\u8fa8\u7387\u7684\u4fe1\u606f\u91cf\u5dee\u5f02\u9700\u8981\u4e0d\u540c\u7684\u4fe1\u606f\u8f6c\u6362\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faInfoScale\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1\uff09\u6e10\u8fdb\u9891\u7387\u8865\u507f\u6a21\u5757\u89e3\u51b3\u6269\u5f20\u5377\u79ef\u5bfc\u81f4\u7684\u9ad8\u9891\u4fe1\u606f\u635f\u5931\uff1b2\uff09\u81ea\u9002\u5e94\u4fe1\u606f\u805a\u5408\u6a21\u5757\u5b9e\u73b0\u4fe1\u606f\u7075\u6d3b\u805a\u5408\uff1b3\uff09\u566a\u58f0\u9002\u5e94\u6a21\u5757\u91cd\u65b0\u5206\u5e03\u521d\u59cb\u566a\u58f0\u4e2d\u7684\u4fe1\u606f\u3002", "result": "\u8be5\u65b9\u6cd5\u5373\u63d2\u5373\u7528\uff0c\u5728\u53d8\u5c3a\u5ea6\u56fe\u50cf\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "InfoScale\u901a\u8fc7\u4fe1\u606f\u4e2d\u5fc3\u5316\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u53d8\u5c3a\u5ea6\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u4e3a\u591a\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.01492", "pdf": "https://arxiv.org/pdf/2509.01492", "abs": "https://arxiv.org/abs/2509.01492", "authors": ["Sebastian Eilermann", "Ren\u00e9 Heesch", "Oliver Niggemann"], "title": "A Continuous-Time Consistency Model for 3D Point Cloud Generation", "categories": ["cs.CV"], "comment": null, "summary": "Fast and accurate 3D shape generation from point clouds is essential for applications in robotics, AR/VR, and digital content creation. We introduce ConTiCoM-3D, a continuous-time consistency model that synthesizes 3D shapes directly in point space, without discretized diffusion steps, pre-trained teacher models, or latent-space encodings. The method integrates a TrigFlow-inspired continuous noise schedule with a Chamfer Distance-based geometric loss, enabling stable training on high-dimensional point sets while avoiding expensive Jacobian-vector products. This design supports efficient one- to two-step inference with high geometric fidelity. In contrast to previous approaches that rely on iterative denoising or latent decoders, ConTiCoM-3D employs a time-conditioned neural network operating entirely in continuous time, thereby achieving fast generation. Experiments on the ShapeNet benchmark show that ConTiCoM-3D matches or outperforms state-of-the-art diffusion and latent consistency models in both quality and efficiency, establishing it as a practical framework for scalable 3D shape generation.", "AI": {"tldr": "ConTiCoM-3D\u662f\u4e00\u4e2a\u8fde\u7eed\u65f6\u95f4\u4e00\u81f4\u6027\u6a21\u578b\uff0c\u76f4\u63a5\u4ece\u70b9\u7a7a\u95f4\u5408\u62103D\u5f62\u72b6\uff0c\u65e0\u9700\u79bb\u6563\u5316\u6269\u6563\u6b65\u9aa4\u3001\u9884\u8bad\u7ec3\u6559\u5e08\u6a21\u578b\u6216\u6f5c\u5728\u7a7a\u95f4\u7f16\u7801\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4e00\u5230\u4e24\u6b65\u63a8\u7406\u3002", "motivation": "\u5feb\u901f\u51c6\u786e\u76843D\u70b9\u4e91\u5f62\u72b6\u751f\u6210\u5bf9\u4e8e\u673a\u5668\u4eba\u3001AR/VR\u548c\u6570\u5b57\u5185\u5bb9\u521b\u5efa\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u8fed\u4ee3\u53bb\u566a\u548c\u6f5c\u5728\u89e3\u7801\u5668\u4f9d\u8d56\u3002", "method": "\u6574\u5408TrigFlow\u542f\u53d1\u7684\u8fde\u7eed\u566a\u58f0\u8c03\u5ea6\u548c\u57fa\u4e8eChamfer\u8ddd\u79bb\u7684\u51e0\u4f55\u635f\u5931\uff0c\u4f7f\u7528\u65f6\u95f4\u6761\u4ef6\u795e\u7ecf\u7f51\u7edc\u5728\u8fde\u7eed\u65f6\u95f4\u4e2d\u64cd\u4f5c\uff0c\u907f\u514d\u6602\u8d35\u7684\u96c5\u53ef\u6bd4\u5411\u91cf\u4e58\u79ef\u8ba1\u7b97\u3002", "result": "\u5728ShapeNet\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cConTiCoM-3D\u5728\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u6269\u6563\u548c\u6f5c\u5728\u4e00\u81f4\u6027\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u53ef\u6269\u5c55\u76843D\u5f62\u72b6\u751f\u6210\u5efa\u7acb\u4e86\u4e00\u4e2a\u5b9e\u7528\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u51e0\u4f55\u4fdd\u771f\u5ea6\u7684\u5feb\u901f\u751f\u6210\u3002"}}
{"id": "2509.01557", "pdf": "https://arxiv.org/pdf/2509.01557", "abs": "https://arxiv.org/abs/2509.01557", "authors": ["Dejia Cai", "Yao Ran", "Kun Yang", "Xinwang Shi", "Yingying Zhou", "Kexian Wu", "Yang Xu", "Yi Hu", "Xiaowei Zhou"], "title": "Acoustic Interference Suppression in Ultrasound images for Real-Time HIFU Monitoring Using an Image-Based Latent Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "High-Intensity Focused Ultrasound (HIFU) is a non-invasive therapeutic technique widely used for treating various diseases. However, the success and safety of HIFU treatments depend on real-time monitoring, which is often hindered by interference when using ultrasound to guide HIFU treatment. To address these challenges, we developed HIFU-ILDiff, a novel deep learning-based approach leveraging latent diffusion models to suppress HIFU-induced interference in ultrasound images. The HIFU-ILDiff model employs a Vector Quantized Variational Autoencoder (VQ-VAE) to encode noisy ultrasound images into a lower-dimensional latent space, followed by a latent diffusion model that iteratively removes interference. The denoised latent vectors are then decoded to reconstruct high-resolution, interference-free ultrasound images. We constructed a comprehensive dataset comprising 18,872 image pairs from in vitro phantoms, ex vivo tissues, and in vivo animal data across multiple imaging modalities and HIFU power levels to train and evaluate the model. Experimental results demonstrate that HIFU-ILDiff significantly outperforms the commonly used Notch Filter method, achieving a Structural Similarity Index (SSIM) of 0.796 and Peak Signal-to-Noise Ratio (PSNR) of 23.780 compared to SSIM of 0.443 and PSNR of 14.420 for the Notch Filter under in vitro scenarios. Additionally, HIFU-ILDiff achieves real-time processing at 15 frames per second, markedly faster than the Notch Filter's 5 seconds per frame. These findings indicate that HIFU-ILDiff is able to denoise HIFU interference in ultrasound guiding images for real-time monitoring during HIFU therapy, which will greatly improve the treatment precision in current clinical applications.", "AI": {"tldr": "HIFU-ILDiff\u662f\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u6df1\u5ea6\u5b66\u4e60\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u65f6\u6291\u5236HIFU\u6cbb\u7597\u4e2d\u8d85\u58f0\u56fe\u50cf\u7684\u5e72\u6270\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u9677\u6ce2\u6ee4\u6ce2\u5668\uff0c\u5b9e\u73b015\u5e27/\u79d2\u7684\u5b9e\u65f6\u5904\u7406\u3002", "motivation": "HIFU\u6cbb\u7597\u7684\u6210\u529f\u548c\u5b89\u5168\u6027\u4f9d\u8d56\u4e8e\u5b9e\u65f6\u76d1\u6d4b\uff0c\u4f46\u4f7f\u7528\u8d85\u58f0\u5f15\u5bfcHIFU\u6cbb\u7597\u65f6\u5b58\u5728\u5e72\u6270\u95ee\u9898\uff0c\u5f71\u54cd\u6cbb\u7597\u6548\u679c\u548c\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528VQ-VAE\u5c06\u542b\u566a\u8d85\u58f0\u56fe\u50cf\u7f16\u7801\u5230\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4\uff0c\u7136\u540e\u4f7f\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u8fed\u4ee3\u53bb\u9664\u5e72\u6270\uff0c\u6700\u540e\u89e3\u7801\u91cd\u5efa\u9ad8\u5206\u8fa8\u7387\u65e0\u5e72\u6270\u8d85\u58f0\u56fe\u50cf\u3002", "result": "\u5728\u4f53\u5916\u5b9e\u9a8c\u4e2d\uff0cHIFU-ILDiff\u8fbe\u5230SSIM 0.796\u548cPSNR 23.780\uff0c\u663e\u8457\u4f18\u4e8e\u9677\u6ce2\u6ee4\u6ce2\u5668\u7684SSIM 0.443\u548cPSNR 14.420\uff0c\u5904\u7406\u901f\u5ea6\u8fbe\u523015\u5e27/\u79d2\u3002", "conclusion": "HIFU-ILDiff\u80fd\u591f\u5b9e\u65f6\u53bb\u566aHIFU\u5e72\u6270\uff0c\u63d0\u9ad8HIFU\u6cbb\u7597\u7cbe\u5ea6\uff0c\u5177\u6709\u91cd\u8981\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.01624", "pdf": "https://arxiv.org/pdf/2509.01624", "abs": "https://arxiv.org/abs/2509.01624", "authors": ["Natalia Frumkin", "Diana Marculescu"], "title": "Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-image diffusion models are computationally intensive, often requiring dozens of forward passes through large transformer backbones. For instance, Stable Diffusion XL generates high-quality images with 50 evaluations of a 2.6B-parameter model, an expensive process even for a single batch. Few-step diffusion models reduce this cost to 2-8 denoising steps but still depend on large, uncompressed U-Net or diffusion transformer backbones, which are often too costly for full-precision inference without datacenter GPUs. These requirements also limit existing post-training quantization methods that rely on full-precision calibration. We introduce Q-Sched, a new paradigm for post-training quantization that modifies the diffusion model scheduler rather than model weights. By adjusting the few-step sampling trajectory, Q-Sched achieves full-precision accuracy with a 4x reduction in model size. To learn quantization-aware pre-conditioning coefficients, we propose the JAQ loss, which combines text-image compatibility with an image quality metric for fine-grained optimization. JAQ is reference-free and requires only a handful of calibration prompts, avoiding full-precision inference during calibration. Q-Sched delivers substantial gains: a 15.5% FID improvement over the FP16 4-step Latent Consistency Model and a 16.6% improvement over the FP16 8-step Phased Consistency Model, showing that quantization and few-step distillation are complementary for high-fidelity generation. A large-scale user study with more than 80,000 annotations further confirms Q-Sched's effectiveness on both FLUX.1[schnell] and SDXL-Turbo.", "AI": {"tldr": "Q-Sched\u662f\u4e00\u79cd\u65b0\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u6269\u6563\u6a21\u578b\u8c03\u5ea6\u5668\u800c\u975e\u6a21\u578b\u6743\u91cd\uff0c\u57284\u500d\u6a21\u578b\u538b\u7f29\u4e0b\u5b9e\u73b0\u5168\u7cbe\u5ea6\u51c6\u786e\u7387\uff0c\u663e\u8457\u63d0\u5347few-step\u6269\u6563\u6a21\u578b\u7684\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0cfew-step\u65b9\u6cd5\u867d\u7136\u51cf\u5c11\u4e86\u53bb\u566a\u6b65\u9aa4\u4f46\u4ecd\u4f9d\u8d56\u5927\u578b\u672a\u538b\u7f29\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5728\u975e\u6570\u636e\u4e2d\u5fc3GPU\u4e0a\u7684\u90e8\u7f72\u548c\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faQ-Sched\u8303\u5f0f\uff0c\u901a\u8fc7\u4fee\u6539\u6269\u6563\u8c03\u5ea6\u5668\u6765\u8c03\u6574few-step\u91c7\u6837\u8f68\u8ff9\uff1b\u5f15\u5165JAQ\u635f\u5931\u51fd\u6570\uff0c\u7ed3\u5408\u6587\u672c-\u56fe\u50cf\u517c\u5bb9\u6027\u548c\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u8fdb\u884c\u7ec6\u7c92\u5ea6\u4f18\u5316\uff0c\u65e0\u9700\u5168\u7cbe\u5ea6\u63a8\u7406\u6821\u51c6\u3002", "result": "\u5728FP16 4-step Latent Consistency Model\u4e0aFID\u63d0\u534715.5%\uff0c\u5728FP16 8-step Phased Consistency Model\u4e0a\u63d0\u534716.6%\uff1b\u5927\u89c4\u6a21\u7528\u6237\u7814\u7a76\uff0880,000+\u6807\u6ce8\uff09\u8bc1\u5b9e\u4e86\u5728FLUX.1\u548cSDXL-Turbo\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u91cf\u5316\u548cfew-step\u84b8\u998f\u5728\u9ad8\u4fdd\u771f\u751f\u6210\u65b9\u9762\u5177\u6709\u4e92\u8865\u6027\uff0cQ-Sched\u4e3a\u9ad8\u6548\u6269\u6563\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.01681", "pdf": "https://arxiv.org/pdf/2509.01681", "abs": "https://arxiv.org/abs/2509.01681", "authors": ["Mohamed Ilyes Lakhal", "Richard Bowden"], "title": "GaussianGAN: Real-Time Photorealistic controllable Human Avatars", "categories": ["cs.CV"], "comment": "IEEE conference series on Automatic Face and Gesture Recognition 2025", "summary": "Photorealistic and controllable human avatars have gained popularity in the research community thanks to rapid advances in neural rendering, providing fast and realistic synthesis tools. However, a limitation of current solutions is the presence of noticeable blurring. To solve this problem, we propose GaussianGAN, an animatable avatar approach developed for photorealistic rendering of people in real-time. We introduce a novel Gaussian splatting densification strategy to build Gaussian points from the surface of cylindrical structures around estimated skeletal limbs. Given the camera calibration, we render an accurate semantic segmentation with our novel view segmentation module. Finally, a UNet generator uses the rendered Gaussian splatting features and the segmentation maps to create photorealistic digital avatars. Our method runs in real-time with a rendering speed of 79 FPS. It outperforms previous methods regarding visual perception and quality, achieving a state-of-the-art results in terms of a pixel fidelity of 32.94db on the ZJU Mocap dataset and 33.39db on the Thuman4 dataset.", "AI": {"tldr": "GaussianGAN\u662f\u4e00\u79cd\u5b9e\u65f6\u53ef\u52a8\u753b\u5316\u7684\u4eba\u4f53avatar\u6e32\u67d3\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u65af\u6cfc\u6e85\u5bc6\u5ea6\u5316\u7b56\u7565\u548c\u8bed\u4e49\u5206\u5272\u6a21\u5757\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u6a21\u7cca\u95ee\u9898\uff0c\u572879FPS\u4e0b\u5b9e\u73b0\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u6e32\u67d3\u3002", "motivation": "\u5f53\u524d\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\u5b58\u5728\u660e\u663e\u6a21\u7cca\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5b9e\u65f6\u751f\u6210\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u4e14\u53ef\u63a7\u7684\u4eba\u4f53avatar\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u9ad8\u65af\u6cfc\u6e85\u5bc6\u5ea6\u5316\u7b56\u7565\u4ece\u9aa8\u9abc\u80a2\u4f53\u5468\u56f4\u7684\u5706\u67f1\u7ed3\u6784\u8868\u9762\u6784\u5efa\u9ad8\u65af\u70b9\uff1b\u5f15\u5165\u65b0\u9896\u89c6\u89d2\u5206\u5272\u6a21\u5757\u6e32\u67d3\u51c6\u786e\u8bed\u4e49\u5206\u5272\uff1b\u4f7f\u7528UNet\u751f\u6210\u5668\u7ed3\u5408\u9ad8\u65af\u6cfc\u6e85\u7279\u5f81\u548c\u5206\u5272\u56fe\u751f\u6210avatar\u3002", "result": "\u5728ZJU Mocap\u6570\u636e\u96c6\u4e0a\u8fbe\u523032.94db\u50cf\u7d20\u4fdd\u771f\u5ea6\uff0c\u5728Thuman4\u6570\u636e\u96c6\u4e0a\u8fbe\u523033.39db\uff0c\u4ee579FPS\u5b9e\u65f6\u8fd0\u884c\uff0c\u89c6\u89c9\u611f\u77e5\u548c\u8d28\u91cf\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "GaussianGAN\u5728\u5b9e\u65f6\u7167\u7247\u7ea7\u4eba\u4f53avatar\u6e32\u67d3\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u7cca\u95ee\u9898\u5e76\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u89c6\u89c9\u6548\u679c\u3002"}}
{"id": "2509.01752", "pdf": "https://arxiv.org/pdf/2509.01752", "abs": "https://arxiv.org/abs/2509.01752", "authors": ["Yu Shi", "Shuyi Fan", "Changsheng Fang", "Shuo Han", "Haodong Li", "Li Zhou", "Bahareh Morovati", "Dayang Wang", "Hengyong Yu"], "title": "Clinical Metadata Guided Limited-Angle CT Image Reconstruction", "categories": ["cs.CV", "physics.med-ph"], "comment": null, "summary": "Limited-angle computed tomography (LACT) offers improved temporal resolution and reduced radiation dose for cardiac imaging, but suffers from severe artifacts due to truncated projections. To address the ill-posedness of LACT reconstruction, we propose a two-stage diffusion framework guided by structured clinical metadata. In the first stage, a transformer-based diffusion model conditioned exclusively on metadata, including acquisition parameters, patient demographics, and diagnostic impressions, generates coarse anatomical priors from noise. The second stage further refines the images by integrating both the coarse prior and metadata to produce high-fidelity results. Physics-based data consistency is enforced at each sampling step in both stages using an Alternating Direction Method of Multipliers module, ensuring alignment with the measured projections. Extensive experiments on both synthetic and real cardiac CT datasets demonstrate that incorporating metadata significantly improves reconstruction fidelity, particularly under severe angular truncation. Compared to existing metadata-free baselines, our method achieves superior performance in SSIM, PSNR, nMI, and PCC. Ablation studies confirm that different types of metadata contribute complementary benefits, particularly diagnostic and demographic priors under limited-angle conditions. These findings highlight the dual role of clinical metadata in improving both reconstruction quality and efficiency, supporting their integration into future metadata-guided medical imaging frameworks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7ed3\u6784\u5316\u4e34\u5e8a\u5143\u6570\u636e\u7684\u4e24\u9636\u6bb5\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6709\u9650\u89d2\u5ea6CT\u91cd\u5efa\u95ee\u9898\uff0c\u901a\u8fc7\u5143\u6570\u636e\u5f15\u5bfc\u751f\u6210\u5148\u9a8c\u5e76\u63d0\u5347\u91cd\u5efa\u8d28\u91cf", "motivation": "\u6709\u9650\u89d2\u5ea6CT\u867d\u7136\u80fd\u63d0\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u964d\u4f4e\u8f90\u5c04\u5242\u91cf\uff0c\u4f46\u7531\u4e8e\u6295\u5f71\u622a\u65ad\u4f1a\u4ea7\u751f\u4e25\u91cd\u4f2a\u5f71\uff0c\u9700\u8981\u89e3\u51b3\u5176\u75c5\u6001\u91cd\u5efa\u95ee\u9898", "method": "\u4e24\u9636\u6bb5\u6269\u6563\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u57fa\u4e8etransformer\u7684\u6269\u6563\u6a21\u578b\u4ec5\u4f7f\u7528\u5143\u6570\u636e\u751f\u6210\u7c97\u7565\u89e3\u5256\u5148\u9a8c\uff1b\u7b2c\u4e8c\u9636\u6bb5\u6574\u5408\u7c97\u7565\u5148\u9a8c\u548c\u5143\u6570\u636e\u751f\u6210\u9ad8\u4fdd\u771f\u7ed3\u679c\uff0c\u6bcf\u4e2a\u91c7\u6837\u6b65\u9aa4\u90fd\u4f7f\u7528ADMM\u6a21\u5757\u5f3a\u5236\u6267\u884c\u7269\u7406\u6570\u636e\u4e00\u81f4\u6027", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u5fc3\u810fCT\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u52a0\u5165\u5143\u6570\u636e\u663e\u8457\u63d0\u9ad8\u4e86\u91cd\u5efa\u4fdd\u771f\u5ea6\uff0c\u7279\u522b\u662f\u5728\u4e25\u91cd\u89d2\u5ea6\u622a\u65ad\u60c5\u51b5\u4e0b\uff0c\u5728SSIM\u3001PSNR\u3001nMI\u548cPCC\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65e0\u5143\u6570\u636e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u4e34\u5e8a\u5143\u6570\u636e\u5728\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u548c\u6548\u7387\u65b9\u9762\u5177\u6709\u53cc\u91cd\u4f5c\u7528\uff0c\u652f\u6301\u5c06\u5176\u6574\u5408\u5230\u672a\u6765\u7684\u5143\u6570\u636e\u5f15\u5bfc\u533b\u5b66\u6210\u50cf\u6846\u67b6\u4e2d\uff0c\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e0d\u540c\u7c7b\u578b\u5143\u6570\u636e\u5177\u6709\u4e92\u8865\u4f18\u52bf"}}
{"id": "2509.01837", "pdf": "https://arxiv.org/pdf/2509.01837", "abs": "https://arxiv.org/abs/2509.01837", "authors": ["Yotam Erel", "Rishabh Dabral", "Vladislav Golyanik", "Amit H. Bermano", "Christian Theobalt"], "title": "PractiLight: Practical Light Control Using Foundational Diffusion Models", "categories": ["cs.CV"], "comment": "Project page: https://yoterel.github.io/PractiLight-project-page/", "summary": "Light control in generated images is a difficult task, posing specific challenges, spanning over the entire image and frequency spectrum. Most approaches tackle this problem by training on extensive yet domain-specific datasets, limiting the inherent generalization and applicability of the foundational backbones used. Instead, PractiLight is a practical approach, effectively leveraging foundational understanding of recent generative models for the task. Our key insight is that lighting relationships in an image are similar in nature to token interaction in self-attention layers, and hence are best represented there. Based on this and other analyses regarding the importance of early diffusion iterations, PractiLight trains a lightweight LoRA regressor to produce the direct irradiance map for a given image, using a small set of training images. We then employ this regressor to incorporate the desired lighting into the generation process of another image using Classifier Guidance. This careful design generalizes well to diverse conditions and image domains. We demonstrate state-of-the-art performance in terms of quality and control with proven parameter and data efficiency compared to leading works over a wide variety of scenes types. We hope this work affirms that image lighting can feasibly be controlled by tapping into foundational knowledge, enabling practical and general relighting.", "AI": {"tldr": "PractiLight\u662f\u4e00\u4e2a\u5b9e\u7528\u7684\u56fe\u50cf\u5149\u7167\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7LoRA\u56de\u5f52\u5668\u751f\u6210\u8f90\u7167\u5ea6\u56fe\uff0c\u5e76\u5229\u7528\u5206\u7c7b\u5668\u5f15\u5bfc\u5c06\u6240\u9700\u5149\u7167\u878d\u5165\u751f\u6210\u8fc7\u7a0b\uff0c\u5728\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u5149\u7167\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5728\u7279\u5b9a\u9886\u57df\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u57fa\u7840\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9002\u7528\u6027\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u751f\u6210\u6a21\u578b\u7684\u57fa\u7840\u77e5\u8bc6\uff0c\u5f00\u53d1\u4e00\u4e2a\u5b9e\u7528\u4e14\u901a\u7528\u7684\u5149\u7167\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u5173\u952e\u53d1\u73b0\u662f\u56fe\u50cf\u4e2d\u7684\u5149\u7167\u5173\u7cfb\u4e0e\u81ea\u6ce8\u610f\u529b\u5c42\u7684token\u4ea4\u4e92\u76f8\u4f3c\u3002\u57fa\u4e8e\u6b64\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7LoRA\u56de\u5f52\u5668\u751f\u6210\u76f4\u63a5\u8f90\u7167\u5ea6\u56fe\uff0c\u7136\u540e\u4f7f\u7528\u5206\u7c7b\u5668\u5f15\u5bfc\u5c06\u6240\u9700\u5149\u7167\u878d\u5165\u53e6\u4e00\u56fe\u50cf\u7684\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u79cd\u573a\u666f\u7c7b\u578b\u4e0a\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u8d28\u91cf\u548c\u63a7\u5236\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u6bd4\u9886\u5148\u5de5\u4f5c\u5177\u6709\u66f4\u597d\u7684\u53c2\u6570\u548c\u6570\u636e\u6548\u7387\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8bc1\u5b9e\u4e86\u901a\u8fc7\u6316\u6398\u57fa\u7840\u77e5\u8bc6\u53ef\u4ee5\u53ef\u884c\u5730\u63a7\u5236\u56fe\u50cf\u5149\u7167\uff0c\u5b9e\u73b0\u4e86\u5b9e\u7528\u4e14\u901a\u7528\u7684\u91cd\u5149\u7167\u529f\u80fd\u3002"}}
{"id": "2509.01864", "pdf": "https://arxiv.org/pdf/2509.01864", "abs": "https://arxiv.org/abs/2509.01864", "authors": ["Paula C\u00e1rdenas", "Leonardo Manrique", "Daniela Vega", "Daniela Ruiz", "Pablo Arbel\u00e1ez"], "title": "Latent Gene Diffusion for Spatial Transcriptomics Completion", "categories": ["cs.CV"], "comment": "10 pages, 8 figures. Accepted to CVAMD Workshop, ICCV 2025", "summary": "Computer Vision has proven to be a powerful tool for analyzing Spatial Transcriptomics (ST) data. However, current models that predict spatially resolved gene expression from histopathology images suffer from significant limitations due to data dropout. Most existing approaches rely on single-cell RNA sequencing references, making them dependent on alignment quality and external datasets while also risking batch effects and inherited dropout. In this paper, we address these limitations by introducing LGDiST, the first reference-free latent gene diffusion model for ST data dropout. We show that LGDiST outperforms the previous state-of-the-art in gene expression completion, with an average Mean Squared Error that is 18% lower across 26 datasets. Furthermore, we demonstrate that completing ST data with LGDiST improves gene expression prediction performance on six state-of-the-art methods up to 10% in MSE. A key innovation of LGDiST is using context genes previously considered uninformative to build a rich and biologically meaningful genetic latent space. Our experiments show that removing key components of LGDiST, such as the context genes, the ST latent space, and the neighbor conditioning, leads to considerable drops in performance. These findings underscore that the full architecture of LGDiST achieves substantially better performance than any of its isolated components.", "AI": {"tldr": "LGDiST\u662f\u9996\u4e2a\u65e0\u53c2\u8003\u7684\u6f5c\u5728\u57fa\u56e0\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u5904\u7406\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u6570\u636e\u7f3a\u5931\u95ee\u9898\uff0c\u572826\u4e2a\u6570\u636e\u96c6\u4e0a\u5e73\u5747MSE\u964d\u4f4e18%\uff0c\u5e76\u63d0\u53476\u79cd\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u57fa\u56e0\u8868\u8fbe\u9884\u6d4b\u6027\u80fd\u8fbe10%\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u5206\u6790\u65b9\u6cd5\u53d7\u6570\u636e\u7f3a\u5931\u9650\u5236\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u53c2\u8003\uff0c\u5b58\u5728\u5bf9\u9f50\u8d28\u91cf\u4f9d\u8d56\u3001\u5916\u90e8\u6570\u636e\u96c6\u4f9d\u8d56\u3001\u6279\u6b21\u6548\u5e94\u548c\u7ee7\u627f\u7f3a\u5931\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faLGDiST\u6a21\u578b\uff0c\u4f7f\u7528\u4e4b\u524d\u88ab\u8ba4\u4e3a\u65e0\u4fe1\u606f\u6027\u7684\u4e0a\u4e0b\u6587\u57fa\u56e0\u6784\u5efa\u4e30\u5bcc\u4e14\u5177\u6709\u751f\u7269\u5b66\u610f\u4e49\u7684\u9057\u4f20\u6f5c\u5728\u7a7a\u95f4\uff0c\u91c7\u7528\u6269\u6563\u6a21\u578b\u67b6\u6784\uff0c\u5305\u542b\u90bb\u5c45\u6761\u4ef6\u7b49\u5173\u952e\u7ec4\u4ef6\u3002", "result": "\u572826\u4e2a\u6570\u636e\u96c6\u4e0a\u5e73\u5747MSE\u964d\u4f4e18%\uff0c\u57286\u79cd\u6700\u5148\u8fdb\u65b9\u6cd5\u4e0a\u57fa\u56e0\u8868\u8fbe\u9884\u6d4b\u6027\u80fd\u63d0\u5347\u8fbe10%\u3002\u79fb\u9664\u4e0a\u4e0b\u6587\u57fa\u56e0\u3001ST\u6f5c\u5728\u7a7a\u95f4\u6216\u90bb\u5c45\u6761\u4ef6\u7b49\u7ec4\u4ef6\u4f1a\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "LGDiST\u5b8c\u6574\u67b6\u6784\u7684\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4efb\u4f55\u5b64\u7acb\u7ec4\u4ef6\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u6570\u636e\u8865\u5168\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2509.01898", "pdf": "https://arxiv.org/pdf/2509.01898", "abs": "https://arxiv.org/abs/2509.01898", "authors": ["Zhipeng Weng", "Xiaopeng Liu", "Ce Liu", "Xingyuan Guo", "Yukai Shi", "Liang Lin"], "title": "DroneSR: Rethinking Few-shot Thermal Image Super-Resolution from Drone-based Perspective", "categories": ["cs.CV"], "comment": null, "summary": "Although large scale models achieve significant improvements in performance, the overfitting challenge still frequently undermines their generalization ability. In super resolution tasks on images, diffusion models as representatives of generative models typically adopt large scale architectures. However, few-shot drone-captured infrared training data frequently induces severe overfitting in large-scale architectures. To address this key challenge, our method proposes a new Gaussian quantization representation learning method oriented to diffusion models that alleviates overfitting and enhances robustness. At the same time, an effective monitoring mechanism tracks large scale architectures during training to detect signs of overfitting. By introducing Gaussian quantization representation learning, our method effectively reduces overfitting while maintaining architecture complexity. On this basis, we construct a multi source drone-based infrared image benchmark dataset for detection and use it to emphasize overfitting issues of large scale architectures in few sample, drone-based diverse drone-based image reconstruction scenarios. To verify the efficacy of the method in mitigating overfitting, experiments are conducted on the constructed benchmark. Experimental results demonstrate that our method outperforms existing super resolution approaches and significantly mitigates overfitting of large scale architectures under complex conditions. The code and DroneSR dataset will be available at: https://github.com/wengzp1/GARLSR.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9762\u5411\u6269\u6563\u6a21\u578b\u7684\u9ad8\u65af\u91cf\u5316\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u9ad8\u65af\u91cf\u5316\u8868\u793a\u5b66\u4e60\u548c\u6709\u6548\u7684\u76d1\u63a7\u673a\u5236\uff0c\u89e3\u51b3\u5927\u89c4\u6a21\u67b6\u6784\u5728\u65e0\u4eba\u673a\u7ea2\u5916\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u7684\u8fc7\u62df\u5408\u95ee\u9898\u3002", "motivation": "\u5927\u89c4\u6a21\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u4f46\u8fc7\u62df\u5408\u95ee\u9898\u7ecf\u5e38\u524a\u5f31\u5176\u6cdb\u5316\u80fd\u529b\u3002\u5728\u65e0\u4eba\u673a\u62cd\u6444\u7684\u7ea2\u5916\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\uff0c\u6269\u6563\u6a21\u578b\u7b49\u751f\u6210\u6a21\u578b\u901a\u5e38\u91c7\u7528\u5927\u89c4\u6a21\u67b6\u6784\uff0c\u4f46\u5c11\u6837\u672c\u8bad\u7ec3\u6570\u636e\u7ecf\u5e38\u5bfc\u81f4\u4e25\u91cd\u7684\u8fc7\u62df\u5408\u3002", "method": "\u63d0\u51fa\u9ad8\u65af\u91cf\u5316\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u8bbe\u8ba1\u65b0\u7684\u8868\u793a\u5b66\u4e60\u7b56\u7565\uff0c\u540c\u65f6\u5efa\u7acb\u6709\u6548\u7684\u76d1\u63a7\u673a\u5236\u6765\u8ddf\u8e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u8fc7\u62df\u5408\u8ff9\u8c61\u3002\u6784\u5efa\u4e86\u591a\u6e90\u65e0\u4eba\u673a\u7ea2\u5916\u56fe\u50cf\u57fa\u51c6\u6570\u636e\u96c6\u7528\u4e8e\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6784\u5efa\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u8f7b\u4e86\u5927\u89c4\u6a21\u67b6\u6784\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u7684\u8fc7\u62df\u5408\u95ee\u9898\u3002", "conclusion": "\u9ad8\u65af\u91cf\u5316\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u67b6\u6784\u590d\u6742\u6027\uff0c\u4e3a\u89e3\u51b3\u5c11\u6837\u672c\u65e0\u4eba\u673a\u56fe\u50cf\u91cd\u5efa\u573a\u666f\u4e2d\u7684\u8fc7\u62df\u5408\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.01964", "pdf": "https://arxiv.org/pdf/2509.01964", "abs": "https://arxiv.org/abs/2509.01964", "authors": ["Hongyu Li", "Chaofeng Chen", "Xiaoming Li", "Guangming Lu"], "title": "2D Gaussian Splatting with Semantic Alignment for Image Inpainting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Gaussian Splatting (GS), a recent technique for converting discrete points into continuous spatial representations, has shown promising results in 3D scene modeling and 2D image super-resolution. In this paper, we explore its untapped potential for image inpainting, which demands both locally coherent pixel synthesis and globally consistent semantic restoration. We propose the first image inpainting framework based on 2D Gaussian Splatting, which encodes incomplete images into a continuous field of 2D Gaussian splat coefficients and reconstructs the final image via a differentiable rasterization process. The continuous rendering paradigm of GS inherently promotes pixel-level coherence in the inpainted results. To improve efficiency and scalability, we introduce a patch-wise rasterization strategy that reduces memory overhead and accelerates inference. For global semantic consistency, we incorporate features from a pretrained DINO model. We observe that DINO's global features are naturally robust to small missing regions and can be effectively adapted to guide semantic alignment in large-mask scenarios, ensuring that the inpainted content remains contextually consistent with the surrounding scene. Extensive experiments on standard benchmarks demonstrate that our method achieves competitive performance in both quantitative metrics and perceptual quality, establishing a new direction for applying Gaussian Splatting to 2D image processing.", "AI": {"tldr": "\u57fa\u4e8e2D\u9ad8\u65af\u6cfc\u6e85\u7684\u56fe\u50cf\u4fee\u590d\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8fde\u7eed\u6e32\u67d3\u8303\u5f0f\u5b9e\u73b0\u50cf\u7d20\u7ea7\u8fde\u8d2f\u6027\uff0c\u7ed3\u5408DINO\u7279\u5f81\u786e\u4fdd\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u63a2\u7d22\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5728\u56fe\u50cf\u4fee\u590d\u9886\u57df\u7684\u6f5c\u529b\uff0c\u8be5\u4efb\u52a1\u9700\u8981\u5c40\u90e8\u50cf\u7d20\u5408\u6210\u7684\u8fde\u8d2f\u6027\u548c\u5168\u5c40\u8bed\u4e49\u6062\u590d\u7684\u4e00\u81f4\u6027", "method": "\u63d0\u51fa\u9996\u4e2a\u57fa\u4e8e2D\u9ad8\u65af\u6cfc\u6e85\u7684\u56fe\u50cf\u4fee\u590d\u6846\u67b6\uff0c\u5c06\u4e0d\u5b8c\u6574\u56fe\u50cf\u7f16\u7801\u4e3a2D\u9ad8\u65af\u6cfc\u6e85\u7cfb\u6570\u7684\u8fde\u7eed\u573a\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u5149\u6805\u5316\u8fc7\u7a0b\u91cd\u5efa\u56fe\u50cf\uff1b\u5f15\u5165\u5206\u5757\u5149\u6805\u5316\u7b56\u7565\u63d0\u9ad8\u6548\u7387\uff1b\u6574\u5408\u9884\u8bad\u7ec3DINO\u6a21\u578b\u7684\u5168\u5c40\u7279\u5f81\u786e\u4fdd\u8bed\u4e49\u4e00\u81f4\u6027", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u7684\u5b9a\u91cf\u6307\u6807\u548c\u611f\u77e5\u8d28\u91cf\u8868\u73b0", "conclusion": "\u4e3a\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u57282D\u56fe\u50cf\u5904\u7406\u4e2d\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027"}}
{"id": "2509.01984", "pdf": "https://arxiv.org/pdf/2509.01984", "abs": "https://arxiv.org/abs/2509.01984", "authors": ["Quan Dao", "Xiaoxiao He", "Ligong Han", "Ngan Hoai Nguyen", "Amin Heyrani Nobar", "Faez Ahmed", "Han Zhang", "Viet Anh Nguyen", "Dimitris Metaxas"], "title": "Discrete Noise Inversion for Next-scale Autoregressive Text-based Image Editing", "categories": ["cs.CV"], "comment": "preprint", "summary": "Visual autoregressive models (VAR) have recently emerged as a promising class of generative models, achieving performance comparable to diffusion models in text-to-image generation tasks. While conditional generation has been widely explored, the ability to perform prompt-guided image editing without additional training is equally critical, as it supports numerous practical real-world applications. This paper investigates the text-to-image editing capabilities of VAR by introducing Visual AutoRegressive Inverse Noise (VARIN), the first noise inversion-based editing technique designed explicitly for VAR models. VARIN leverages a novel pseudo-inverse function for argmax sampling, named Location-aware Argmax Inversion (LAI), to generate inverse Gumbel noises. These inverse noises enable precise reconstruction of the source image and facilitate targeted, controllable edits aligned with textual prompts. Extensive experiments demonstrate that VARIN effectively modifies source images according to specified prompts while significantly preserving the original background and structural details, thus validating its efficacy as a practical editing approach.", "AI": {"tldr": "VARIN\u662f\u9996\u4e2a\u4e3a\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u8bbe\u8ba1\u7684\u566a\u58f0\u53cd\u6f14\u7f16\u8f91\u6280\u672f\uff0c\u901a\u8fc7\u4f4d\u7f6e\u611f\u77e5Argmax\u53cd\u6f14\u751f\u6210\u9006Gumbel\u566a\u58f0\uff0c\u5b9e\u73b0\u7cbe\u786e\u56fe\u50cf\u91cd\u5efa\u548c\u6587\u672c\u5f15\u5bfc\u7f16\u8f91\u3002", "motivation": "\u867d\u7136\u6761\u4ef6\u751f\u6210\u5df2\u88ab\u5e7f\u6cdb\u63a2\u7d22\uff0c\u4f46\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u8fdb\u884c\u63d0\u793a\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u7684\u80fd\u529b\u540c\u6837\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u652f\u6301\u4f17\u591a\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5f15\u5165VARIN\u6280\u672f\uff0c\u5229\u7528\u65b0\u9896\u7684\u4f4d\u7f6e\u611f\u77e5Argmax\u53cd\u6f14(LAI)\u4f2a\u9006\u51fd\u6570\u751f\u6210\u9006Gumbel\u566a\u58f0\uff0c\u5b9e\u73b0\u6e90\u56fe\u50cf\u7cbe\u786e\u91cd\u5efa\u548c\u6587\u672c\u5bf9\u9f50\u7684\u9488\u5bf9\u6027\u7f16\u8f91\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eVARIN\u80fd\u6709\u6548\u6839\u636e\u6307\u5b9a\u63d0\u793a\u4fee\u6539\u6e90\u56fe\u50cf\uff0c\u540c\u65f6\u663e\u8457\u4fdd\u7559\u539f\u59cb\u80cc\u666f\u548c\u7ed3\u6784\u7ec6\u8282\u3002", "conclusion": "VARIN\u88ab\u9a8c\u8bc1\u4e3a\u4e00\u79cd\u5b9e\u7528\u7684\u7f16\u8f91\u65b9\u6cd5\uff0c\u4e3a\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6587\u672c\u5230\u56fe\u50cf\u7f16\u8f91\u80fd\u529b\u3002"}}
{"id": "2509.02000", "pdf": "https://arxiv.org/pdf/2509.02000", "abs": "https://arxiv.org/abs/2509.02000", "authors": ["Elad Aharoni", "Noy Porat", "Dani Lischinski", "Ariel Shamir"], "title": "Palette Aligned Image Diffusion", "categories": ["cs.CV"], "comment": "14 pages, 19 figures", "summary": "We introduce the Palette-Adapter, a novel method for conditioning text-to-image diffusion models on a user-specified color palette. While palettes are a compact and intuitive tool widely used in creative workflows, they introduce significant ambiguity and instability when used for conditioning image generation. Our approach addresses this challenge by interpreting palettes as sparse histograms and introducing two scalar control parameters: histogram entropy and palette-to-histogram distance, which allow flexible control over the degree of palette adherence and color variation. We further introduce a negative histogram mechanism that allows users to suppress specific undesired hues, improving adherence to the intended palette under the standard classifier-free guidance mechanism. To ensure broad generalization across the color space, we train on a carefully curated dataset with balanced coverage of rare and common colors. Our method enables stable, semantically coherent generation across a wide range of palettes and prompts. We evaluate our method qualitatively, quantitatively, and through a user study, and show that it consistently outperforms existing approaches in achieving both strong palette adherence and high image quality.", "AI": {"tldr": "Palette-Adapter\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u8c03\u8272\u677f\u89e3\u91ca\u4e3a\u7a00\u758f\u76f4\u65b9\u56fe\u5e76\u5f15\u5165\u4e24\u4e2a\u63a7\u5236\u53c2\u6570\uff0c\u5728\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u5b9e\u73b0\u7a33\u5b9a\u7684\u8c03\u8272\u677f\u6761\u4ef6\u63a7\u5236\u3002", "motivation": "\u8c03\u8272\u677f\u662f\u521b\u610f\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u76f4\u89c2\u5de5\u5177\uff0c\u4f46\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u4f5c\u4e3a\u6761\u4ef6\u4f7f\u7528\u65f6\u5b58\u5728\u663e\u8457\u7684\u6a21\u7cca\u6027\u548c\u4e0d\u7a33\u5b9a\u6027\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u5c06\u8c03\u8272\u677f\u89e3\u91ca\u4e3a\u7a00\u758f\u76f4\u65b9\u56fe\uff0c\u5f15\u5165\u76f4\u65b9\u56fe\u71b5\u548c\u8c03\u8272\u677f\u5230\u76f4\u65b9\u56fe\u8ddd\u79bb\u4e24\u4e2a\u6807\u91cf\u63a7\u5236\u53c2\u6570\uff0c\u5e76\u91c7\u7528\u8d1f\u76f4\u65b9\u56fe\u673a\u5236\u6765\u6291\u5236\u4e0d\u9700\u8981\u7684\u8272\u8c03\uff0c\u5728\u7cbe\u5fc3\u7b56\u5212\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u5e7f\u6cdb\u7684\u8c03\u8272\u677f\u548c\u63d0\u793a\u8303\u56f4\u5185\u5b9e\u73b0\u7a33\u5b9a\u3001\u8bed\u4e49\u8fde\u8d2f\u7684\u751f\u6210\uff0c\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4ee5\u53ca\u7528\u6237\u7814\u7a76\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Palette-Adapter\u901a\u8fc7\u521b\u65b0\u7684\u8c03\u8272\u677f\u89e3\u91ca\u548c\u63a7\u5236\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8c03\u8272\u677f\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u6a21\u7cca\u6027\u548c\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8c03\u8272\u677f\u9075\u5faa\u5ea6\u548c\u56fe\u50cf\u8d28\u91cf\u3002"}}
{"id": "2509.02161", "pdf": "https://arxiv.org/pdf/2509.02161", "abs": "https://arxiv.org/abs/2509.02161", "authors": ["Pablo Ayuso-Albizu", "Juan C. SanMiguel", "Pablo Carballeira"], "title": "Enhancing Zero-Shot Pedestrian Attribute Recognition with Synthetic Data Generation: A Comparative Study with Image-To-Image Diffusion Models", "categories": ["cs.CV"], "comment": "Paper accepted at AVSS 2025 conference", "summary": "Pedestrian Attribute Recognition (PAR) involves identifying various human attributes from images with applications in intelligent monitoring systems. The scarcity of large-scale annotated datasets hinders the generalization of PAR models, specially in complex scenarios involving occlusions, varying poses, and diverse environments. Recent advances in diffusion models have shown promise for generating diverse and realistic synthetic images, allowing to expand the size and variability of training data. However, the potential of diffusion-based data expansion for generating PAR-like images remains underexplored. Such expansion may enhance the robustness and adaptability of PAR models in real-world scenarios. This paper investigates the effectiveness of diffusion models in generating synthetic pedestrian images tailored to PAR tasks. We identify key parameters of img2img diffusion-based data expansion; including text prompts, image properties, and the latest enhancements in diffusion-based data augmentation, and examine their impact on the quality of generated images for PAR. Furthermore, we employ the best-performing expansion approach to generate synthetic images for training PAR models, by enriching the zero-shot datasets. Experimental results show that prompt alignment and image properties are critical factors in image generation, with optimal selection leading to a 4.5% improvement in PAR recognition performance.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u7528\u4e8e\u884c\u4eba\u5c5e\u6027\u8bc6\u522b(PAR)\u4efb\u52a1\u7684\u5408\u6210\u884c\u4eba\u56fe\u50cf\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u4f18\u5316\u6587\u672c\u63d0\u793a\u548c\u56fe\u50cf\u5c5e\u6027\u7b49\u5173\u952e\u53c2\u6570\uff0c\u5b9e\u73b0\u4e864.5%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u884c\u4eba\u5c5e\u6027\u8bc6\u522b\u9762\u4e34\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u573a\u666f\u4e2d\u3002\u6269\u6563\u6a21\u578b\u80fd\u591f\u751f\u6210\u591a\u6837\u4e14\u903c\u771f\u7684\u5408\u6210\u56fe\u50cf\uff0c\u4f46\u5176\u5728PAR\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u8bc6\u522bimg2img\u6269\u6563\u6570\u636e\u6269\u5c55\u7684\u5173\u952e\u53c2\u6570\uff08\u6587\u672c\u63d0\u793a\u3001\u56fe\u50cf\u5c5e\u6027\u7b49\uff09\uff0c\u7814\u7a76\u5b83\u4eec\u5bf9PAR\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u5e76\u91c7\u7528\u6700\u4f73\u6269\u5c55\u65b9\u6cd5\u901a\u8fc7\u4e30\u5bcc\u96f6\u6837\u672c\u6570\u636e\u96c6\u6765\u751f\u6210\u5408\u6210\u56fe\u50cf\u8bad\u7ec3PAR\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u63d0\u793a\u5bf9\u9f50\u548c\u56fe\u50cf\u5c5e\u6027\u662f\u56fe\u50cf\u751f\u6210\u7684\u5173\u952e\u56e0\u7d20\uff0c\u6700\u4f18\u9009\u62e9\u4f7fPAR\u8bc6\u522b\u6027\u80fd\u63d0\u9ad8\u4e864.5%\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u80fd\u591f\u6709\u6548\u751f\u6210\u9002\u7528\u4e8ePAR\u4efb\u52a1\u7684\u5408\u6210\u884c\u4eba\u56fe\u50cf\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u53c2\u6570\u4f18\u5316\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2509.02295", "pdf": "https://arxiv.org/pdf/2509.02295", "abs": "https://arxiv.org/abs/2509.02295", "authors": ["Sapir Esther Yiflach", "Yuval Atzmon", "Gal Chechik"], "title": "Data-Driven Loss Functions for Inference-Time Optimization in Text-to-Image Generation", "categories": ["cs.CV"], "comment": "Project page is at https://learn-to-steer-paper.github.io/", "summary": "Text-to-image diffusion models can generate stunning visuals, yet they often fail at tasks children find trivial--like placing a dog to the right of a teddy bear rather than to the left. When combinations get more unusual--a giraffe above an airplane--these failures become even more pronounced. Existing methods attempt to fix these spatial reasoning failures through model fine-tuning or test-time optimization with handcrafted losses that are suboptimal. Rather than imposing our assumptions about spatial encoding, we propose learning these objectives directly from the model's internal representations. We introduce Learn-to-Steer, a novel framework that learns data-driven objectives for test-time optimization rather than handcrafting them. Our key insight is to train a lightweight classifier that decodes spatial relationships from the diffusion model's cross-attention maps, then deploy this classifier as a learned loss function during inference. Training such classifiers poses a surprising challenge: they can take shortcuts by detecting linguistic traces rather than learning true spatial patterns. We solve this with a dual-inversion strategy that enforces geometric understanding. Our method dramatically improves spatial accuracy: from 0.20 to 0.61 on FLUX.1-dev and from 0.07 to 0.54 on SD2.1 across standard benchmarks. Moreover, our approach generalizes to multiple relations and significantly improves accuracy.", "AI": {"tldr": "Learn-to-Steer\u6846\u67b6\u901a\u8fc7\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u4ece\u6269\u6563\u6a21\u578b\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u56fe\u4e2d\u89e3\u7801\u7a7a\u95f4\u5173\u7cfb\uff0c\u4f5c\u4e3a\u5b66\u4e60\u635f\u5931\u51fd\u6570\u5728\u63a8\u7406\u65f6\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5982\u5de6\u53f3\u4f4d\u7f6e\u5173\u7cfb\u7b49\u7b80\u5355\u4efb\u52a1\u7ecf\u5e38\u5931\u8d25\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u6a21\u578b\u5fae\u8c03\u6216\u624b\u5de5\u8bbe\u8ba1\u7684\u635f\u5931\u51fd\u6570\u8fdb\u884c\u4f18\u5316\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u4e0d\u591f\u7406\u60f3\u3002", "method": "\u63d0\u51faLearn-to-Steer\u6846\u67b6\uff1a1\uff09\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u4ece\u4ea4\u53c9\u6ce8\u610f\u529b\u56fe\u4e2d\u89e3\u7801\u7a7a\u95f4\u5173\u7cfb\uff1b2\uff09\u4f7f\u7528\u53cc\u91cd\u53cd\u8f6c\u7b56\u7565\u907f\u514d\u5206\u7c7b\u5668\u8d70\u6377\u5f84\uff1b3\uff09\u5c06\u8bad\u7ec3\u597d\u7684\u5206\u7c7b\u5668\u4f5c\u4e3a\u5b66\u4e60\u635f\u5931\u51fd\u6570\u5728\u63a8\u7406\u65f6\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u95f4\u51c6\u786e\u6027\uff1a\u5728FLUX.1-dev\u4e0a\u4ece0.20\u63d0\u5347\u52300.61\uff0c\u5728SD2.1\u4e0a\u4ece0.07\u63d0\u5347\u52300.54\u3002\u540c\u65f6\u80fd\u591f\u6cdb\u5316\u5230\u591a\u79cd\u7a7a\u95f4\u5173\u7cfb\u5e76\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u901a\u8fc7\u76f4\u63a5\u4ece\u6a21\u578b\u5185\u90e8\u8868\u793a\u5b66\u4e60\u76ee\u6807\u800c\u975e\u624b\u5de5\u8bbe\u8ba1\u635f\u5931\u51fd\u6570\uff0cLearn-to-Steer\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u95ee\u9898\uff0c\u4e3a\u6539\u8fdb\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u7a7a\u95f4\u51c6\u786e\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.02419", "pdf": "https://arxiv.org/pdf/2509.02419", "abs": "https://arxiv.org/abs/2509.02419", "authors": ["Tao Wang", "Zhenxuan Zhang", "Yuanbo Zhou", "Xinlin Zhang", "Yuanbin Chen", "Tao Tan", "Guang Yang", "Tong Tong"], "title": "From Noisy Labels to Intrinsic Structure: A Geometric-Structural Dual-Guided Framework for Noise-Robust Medical Image Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The effectiveness of convolutional neural networks in medical image segmentation relies on large-scale, high-quality annotations, which are costly and time-consuming to obtain. Even expert-labeled datasets inevitably contain noise arising from subjectivity and coarse delineations, which disrupt feature learning and adversely impact model performance. To address these challenges, this study propose a Geometric-Structural Dual-Guided Network (GSD-Net), which integrates geometric and structural cues to improve robustness against noisy annotations. It incorporates a Geometric Distance-Aware module that dynamically adjusts pixel-level weights using geometric features, thereby strengthening supervision in reliable regions while suppressing noise. A Structure-Guided Label Refinement module further refines labels with structural priors, and a Knowledge Transfer module enriches supervision and improves sensitivity to local details. To comprehensively assess its effectiveness, we evaluated GSD-Net on six publicly available datasets: four containing three types of simulated label noise, and two with multi-expert annotations that reflect real-world subjectivity and labeling inconsistencies. Experimental results demonstrate that GSD-Net achieves state-of-the-art performance under noisy annotations, achieving improvements of 2.52% on Kvasir, 22.76% on Shenzhen, 8.87% on BU-SUC, and 4.59% on BraTS2020 under SR simulated noise. The codes of this study are available at https://github.com/ortonwang/GSD-Net.", "AI": {"tldr": "GSD-Net\u662f\u4e00\u4e2a\u51e0\u4f55\u7ed3\u6784\u53cc\u5f15\u5bfc\u7f51\u7edc\uff0c\u901a\u8fc7\u6574\u5408\u51e0\u4f55\u548c\u7ed3\u6784\u7ebf\u7d22\u6765\u63d0\u9ad8\u5bf9\u566a\u58f0\u6807\u6ce8\u7684\u9c81\u68d2\u6027\uff0c\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9700\u8981\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6807\u6ce8\uff0c\u4f46\u4e13\u5bb6\u6807\u6ce8\u4e5f\u5b58\u5728\u4e3b\u89c2\u6027\u548c\u7c97\u7cd9\u6807\u6ce8\u5e26\u6765\u7684\u566a\u58f0\uff0c\u8fd9\u4e9b\u566a\u58f0\u4f1a\u7834\u574f\u7279\u5f81\u5b66\u4e60\u5e76\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51faGSD-Net\u7f51\u7edc\uff0c\u5305\u542b\u51e0\u4f55\u8ddd\u79bb\u611f\u77e5\u6a21\u5757\u52a8\u6001\u8c03\u6574\u50cf\u7d20\u7ea7\u6743\u91cd\uff0c\u7ed3\u6784\u5f15\u5bfc\u6807\u7b7e\u7ec6\u5316\u6a21\u5757\u5229\u7528\u7ed3\u6784\u5148\u9a8c\u7cbe\u70bc\u6807\u7b7e\uff0c\u4ee5\u53ca\u77e5\u8bc6\u8f6c\u79fb\u6a21\u5757\u589e\u5f3a\u76d1\u7763\u548c\u5bf9\u5c40\u90e8\u7ec6\u8282\u7684\u654f\u611f\u6027\u3002", "result": "\u5728\u516d\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u5728\u566a\u58f0\u6807\u6ce8\u4e0b\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728Kvasir\u3001Shenzhen\u3001BU-SUC\u548cBraTS2020\u6570\u636e\u96c6\u4e0a\u5206\u522b\u63d0\u53472.52%\u300122.76%\u30018.87%\u548c4.59%\u3002", "conclusion": "GSD-Net\u901a\u8fc7\u51e0\u4f55\u548c\u7ed3\u6784\u53cc\u5f15\u5bfc\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u5bf9\u566a\u58f0\u6807\u6ce8\u7684\u9c81\u68d2\u6027\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u566a\u58f0\u73af\u5883\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2509.02445", "pdf": "https://arxiv.org/pdf/2509.02445", "abs": "https://arxiv.org/abs/2509.02445", "authors": ["Lydia Kin Ching Chau", "Zhi Yu", "Ruo Wei Jiang"], "title": "Towards High-Fidelity, Identity-Preserving Real-Time Makeup Transfer: Decoupling Style Generation", "categories": ["cs.CV"], "comment": null, "summary": "We present a novel framework for real-time virtual makeup try-on that achieves high-fidelity, identity-preserving cosmetic transfer with robust temporal consistency. In live makeup transfer applications, it is critical to synthesize temporally coherent results that accurately replicate fine-grained makeup and preserve user's identity. However, existing methods often struggle to disentangle semitransparent cosmetics from skin tones and other identify features, causing identity shifts and raising fairness concerns. Furthermore, current methods lack real-time capabilities and fail to maintain temporal consistency, limiting practical adoption. To address these challenges, we decouple makeup transfer into two steps: transparent makeup mask extraction and graphics-based mask rendering. After the makeup extraction step, the makeup rendering can be performed in real time, enabling live makeup try-on. Our makeup extraction model trained on pseudo-ground-truth data generated via two complementary methods: a graphics-based rendering pipeline and an unsupervised k-means clustering approach. To further enhance transparency estimation and color fidelity, we propose specialized training objectives, including alpha-weighted reconstruction and lip color losses. Our method achieves robust makeup transfer across diverse poses, expressions, and skin tones while preserving temporal smoothness. Extensive experiments demonstrate that our approach outperforms existing baselines in capturing fine details, maintaining temporal stability, and preserving identity integrity.", "AI": {"tldr": "\u9ad8\u4fdd\u771f\u5b9e\u65f6\u865a\u62df\u5316\u5986\u8bd5\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u900f\u660e\u5316\u5986\u63d0\u53d6\u548c\u56fe\u5f62\u6e32\u67d3\u4e24\u6b65\u9aa4\u5b9e\u73b0\u8eab\u4efd\u4fdd\u6301\u548c\u65f6\u95f4\u4e00\u81f4\u6027", "motivation": "\u89e3\u51b3\u73b0\u6709\u5316\u5986\u8f6c\u79fb\u65b9\u6cd5\u5728\u8eab\u4efd\u4fdd\u6301\u3001\u8d28\u611f\u91cd\u73b0\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u65e0\u6cd5\u6709\u6548\u5206\u79bb\u534a\u900f\u660e\u5316\u5986\u7269\u4e0e\u76ae\u80a4\u989c\u8272\u7684\u95ee\u9898", "method": "\u4e24\u6b65\u6cd5\uff1a\u5148\u8fdb\u884c\u900f\u660e\u5316\u5986\u63d0\u53d6\uff0c\u7136\u540e\u4f7f\u7528\u56fe\u5f62\u5b66\u65b9\u6cd5\u8fdb\u884c\u5b9e\u65f6\u6e32\u67d3\u3002\u8bad\u7ec3\u6570\u636e\u901a\u8fc7\u56fe\u5f62\u6e32\u67d3\u7ba1\u7ebf\u548c\u65e0\u76d1\u7763k-means\u805a\u7c7b\u751f\u6210\uff0c\u91c7\u7528\u4e13\u95e8\u7684\u8bad\u7ec3\u76ee\u6807\u5305\u62ecalpha\u6743\u91cd\u91cd\u5efa\u548c\u5507\u5f69\u635f\u5931", "result": "\u65b9\u6cd5\u5728\u591a\u6837\u5316\u59ff\u6001\u3001\u8868\u60c5\u548c\u76ae\u8272\u60c5\u51b5\u4e0b\u90fd\u80fd\u5b9e\u73b0\u7a33\u5065\u7684\u5316\u5986\u8f6c\u79fb\uff0c\u4fdd\u6301\u65f6\u95f4\u5e73\u6ed1\u6027\uff0c\u5728\u7ec6\u8282\u6350\u6349\u3001\u65f6\u95f4\u7a33\u5b9a\u6027\u548c\u8eab\u4efd\u4fdd\u6301\u65b9\u9762\u8d85\u8fc7\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u4e24\u6b65\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u5b9e\u65f6\u865a\u62df\u5316\u5986\u8bd5\u7528\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u9ad8\u4fdd\u771f\u3001\u8eab\u4efd\u4fdd\u6301\u7684\u5b9e\u65f6\u5316\u5986\u6548\u679c\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.02466", "pdf": "https://arxiv.org/pdf/2509.02466", "abs": "https://arxiv.org/abs/2509.02466", "authors": ["Yanwen Wang", "Yiyu Zhuang", "Jiawei Zhang", "Li Wang", "Yifei Zeng", "Xun Cao", "Xinxin Zuo", "Hao Zhu"], "title": "TeRA: Rethinking Text-driven Realistic 3D Avatar Generation", "categories": ["cs.CV"], "comment": "Accepted by ICCV2025", "summary": "In this paper, we rethink text-to-avatar generative models by proposing TeRA, a more efficient and effective framework than the previous SDS-based models and general large 3D generative models. Our approach employs a two-stage training strategy for learning a native 3D avatar generative model. Initially, we distill a decoder to derive a structured latent space from a large human reconstruction model. Subsequently, a text-controlled latent diffusion model is trained to generate photorealistic 3D human avatars within this latent space. TeRA enhances the model performance by eliminating slow iterative optimization and enables text-based partial customization through a structured 3D human representation. Experiments have proven our approach's superiority over previous text-to-avatar generative models in subjective and objective evaluation.", "AI": {"tldr": "TeRA\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u8bad\u7ec3\u7684\u6587\u672c\u52303D\u5934\u50cf\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5728\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\u4e2d\u751f\u6210\u903c\u771f3D\u4eba\u4f53\u5934\u50cf\uff0c\u76f8\u6bd4\u4f20\u7edfSDS\u65b9\u6cd5\u66f4\u9ad8\u6548\u4e14\u652f\u6301\u6587\u672c\u5c40\u90e8\u5b9a\u5236", "motivation": "\u4f20\u7edf\u57fa\u4e8eSDS\u7684\u6587\u672c\u5230\u5934\u50cf\u751f\u6210\u6a21\u578b\u5b58\u5728\u8fed\u4ee3\u4f18\u5316\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u4e14\u652f\u6301\u5c40\u90e8\u5b9a\u5236\u76843D\u5934\u50cf\u751f\u6210\u65b9\u6848", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a1\uff09\u4ece\u5927\u578b\u4eba\u4f53\u91cd\u5efa\u6a21\u578b\u4e2d\u63d0\u53d6\u89e3\u7801\u5668\u6784\u5efa\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\uff1b2\uff09\u8bad\u7ec3\u6587\u672c\u63a7\u5236\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5728\u8be5\u7a7a\u95f4\u4e2d\u751f\u62103D\u5934\u50cf", "result": "\u5b9e\u9a8c\u8bc1\u660eTeRA\u5728\u4e3b\u89c2\u548c\u5ba2\u89c2\u8bc4\u4f30\u4e0a\u5747\u4f18\u4e8e\u4e4b\u524d\u7684\u6587\u672c\u5230\u5934\u50cf\u751f\u6210\u6a21\u578b\uff0c\u6d88\u9664\u4e86\u7f13\u6162\u7684\u8fed\u4ee3\u4f18\u5316\u8fc7\u7a0b", "conclusion": "TeRA\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\u548c\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u66f4\u6709\u6548\u7684\u6587\u672c\u52303D\u5934\u50cf\u751f\u6210\uff0c\u652f\u6301\u6587\u672c\u9a71\u52a8\u7684\u5c40\u90e8\u5b9a\u5236\u529f\u80fd"}}
{"id": "2509.00036", "pdf": "https://arxiv.org/pdf/2509.00036", "abs": "https://arxiv.org/abs/2509.00036", "authors": ["Cheng Jin", "Zhenyu Xiao", "Yuantao Gu"], "title": "A-FloPS: Accelerating Diffusion Sampling with Adaptive Flow Path Sampler", "categories": ["cs.LG", "cs.CV", "68T07, 60H10, 65C30", "I.2.6; G.1.7"], "comment": "14 pages,9 figures", "summary": "Diffusion models deliver state-of-the-art generative performance across diverse modalities but remain computationally expensive due to their inherently iterative sampling process. Existing training-free acceleration methods typically improve numerical solvers for the reverse-time ODE, yet their effectiveness is fundamentally constrained by the inefficiency of the underlying sampling trajectories. We propose A-FloPS (Adaptive Flow Path Sampler), a principled, training-free framework that reparameterizes the sampling trajectory of any pre-trained diffusion model into a flow-matching form and augments it with an adaptive velocity decomposition. The reparameterization analytically maps diffusion scores to flow-compatible velocities, yielding integration-friendly trajectories without retraining. The adaptive mechanism further factorizes the velocity field into a linear drift term and a residual component whose temporal variation is actively suppressed, restoring the accuracy benefits of high-order integration even in extremely low-NFE regimes. Extensive experiments on conditional image generation and text-to-image synthesis show that A-FloPS consistently outperforms state-of-the-art training-free samplers in both sample quality and efficiency. Notably, with as few as $5$ function evaluations, A-FloPS achieves substantially lower FID and generates sharper, more coherent images. The adaptive mechanism also improves native flow-based generative models, underscoring its generality. These results position A-FloPS as a versatile and effective solution for high-quality, low-latency generative modeling.", "AI": {"tldr": "A-FloPS\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u6548\u6269\u6563\u6a21\u578b\u91c7\u6837\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u65b0\u53c2\u6570\u5316\u91c7\u6837\u8f68\u8ff9\u548c\u81ea\u9002\u5e94\u901f\u5ea6\u5206\u89e3\uff0c\u5728\u6781\u4f4e\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210", "motivation": "\u6269\u6563\u6a21\u578b\u867d\u7136\u751f\u6210\u6027\u80fd\u4f18\u5f02\uff0c\u4f46\u8fed\u4ee3\u91c7\u6837\u8fc7\u7a0b\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709\u7684\u514d\u8bad\u7ec3\u52a0\u901f\u65b9\u6cd5\u53d7\u9650\u4e8e\u91c7\u6837\u8f68\u8ff9\u6548\u7387\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51faA-FloPS\u6846\u67b6\uff1a1\uff09\u5c06\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u8f68\u8ff9\u91cd\u65b0\u53c2\u6570\u5316\u4e3a\u6d41\u5339\u914d\u5f62\u5f0f\uff1b2\uff09\u91c7\u7528\u81ea\u9002\u5e94\u901f\u5ea6\u5206\u89e3\u673a\u5236\uff0c\u5c06\u901f\u5ea6\u573a\u5206\u89e3\u4e3a\u7ebf\u6027\u6f02\u79fb\u9879\u548c\u6b8b\u5dee\u5206\u91cf\uff1b3\uff09\u6291\u5236\u6b8b\u5dee\u5206\u91cf\u7684\u65f6\u95f4\u53d8\u5316\uff0c\u6062\u590d\u9ad8\u9636\u79ef\u5206\u4f18\u52bf", "result": "\u5728\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u548c\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u4efb\u52a1\u4e2d\uff0cA-FloPS\u5728\u6837\u672c\u8d28\u91cf\u548c\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u514d\u8bad\u7ec3\u91c7\u6837\u5668\u3002\u4ec5\u97005\u6b21\u51fd\u6570\u8bc4\u4f30\u5373\u53ef\u83b7\u5f97\u663e\u8457\u66f4\u4f4e\u7684FID\u5206\u6570\uff0c\u751f\u6210\u66f4\u6e05\u6670\u3001\u66f4\u8fde\u8d2f\u7684\u56fe\u50cf", "conclusion": "A-FloPS\u4e3a\u9ad8\u8d28\u91cf\u3001\u4f4e\u5ef6\u8fdf\u751f\u6210\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u81ea\u9002\u5e94\u673a\u5236\u8fd8\u80fd\u63d0\u5347\u539f\u751f\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578b\u6027\u80fd"}}
{"id": "2509.00065", "pdf": "https://arxiv.org/pdf/2509.00065", "abs": "https://arxiv.org/abs/2509.00065", "authors": ["Zhitao Wang", "Yirong Xiong", "Roberto Horowitz", "Yanke Wang", "Yuxing Han"], "title": "Hybrid Perception and Equivariant Diffusion for Robust Multi-Node Rebar Tying", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted by The IEEE International Conference on Automation Science   and Engineering (CASE) 2025", "summary": "Rebar tying is a repetitive but critical task in reinforced concrete construction, typically performed manually at considerable ergonomic risk. Recent advances in robotic manipulation hold the potential to automate the tying process, yet face challenges in accurately estimating tying poses in congested rebar nodes. In this paper, we introduce a hybrid perception and motion planning approach that integrates geometry-based perception with Equivariant Denoising Diffusion on SE(3) (Diffusion-EDFs) to enable robust multi-node rebar tying with minimal training data. Our perception module utilizes density-based clustering (DBSCAN), geometry-based node feature extraction, and principal component analysis (PCA) to segment rebar bars, identify rebar nodes, and estimate orientation vectors for sequential ranking, even in complex, unstructured environments. The motion planner, based on Diffusion-EDFs, is trained on as few as 5-10 demonstrations to generate sequential end-effector poses that optimize collision avoidance and tying efficiency. The proposed system is validated on various rebar meshes, including single-layer, multi-layer, and cluttered configurations, demonstrating high success rates in node detection and accurate sequential tying. Compared with conventional approaches that rely on large datasets or extensive manual parameter tuning, our method achieves robust, efficient, and adaptable multi-node tying while significantly reducing data requirements. This result underscores the potential of hybrid perception and diffusion-driven planning to enhance automation in on-site construction tasks, improving both safety and labor efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u51e0\u4f55\u611f\u77e5\u548cSE(3)\u7b49\u53d8\u53bb\u566a\u6269\u6563\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u94a2\u7b4b\u7ed1\u624e\uff0c\u4ec5\u9700\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u9c81\u68d2\u7684\u591a\u8282\u70b9\u7ed1\u624e\u3002", "motivation": "\u94a2\u7b4b\u7ed1\u624e\u662f\u6df7\u51dd\u571f\u65bd\u5de5\u4e2d\u7684\u91cd\u590d\u6027\u5173\u952e\u4efb\u52a1\uff0c\u4f20\u7edf\u624b\u5de5\u64cd\u4f5c\u5b58\u5728\u4eba\u4f53\u5de5\u7a0b\u5b66\u98ce\u9669\u3002\u73b0\u6709\u673a\u5668\u4eba\u65b9\u6cd5\u5728\u62e5\u6324\u94a2\u7b4b\u8282\u70b9\u4e2d\u51c6\u786e\u4f30\u8ba1\u7ed1\u624e\u59ff\u6001\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u6216\u624b\u52a8\u53c2\u6570\u8c03\u6574\u3002", "method": "\u91c7\u7528\u6df7\u5408\u611f\u77e5\u548c\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff1a\u611f\u77e5\u6a21\u5757\u4f7f\u7528DBSCAN\u805a\u7c7b\u3001\u51e0\u4f55\u7279\u5f81\u63d0\u53d6\u548cPCA\u8fdb\u884c\u94a2\u7b4b\u5206\u5272\u3001\u8282\u70b9\u8bc6\u522b\u548c\u65b9\u5411\u5411\u91cf\u4f30\u8ba1\uff1b\u8fd0\u52a8\u89c4\u5212\u57fa\u4e8eDiffusion-EDFs\uff0c\u4ec5\u97005-10\u4e2a\u6f14\u793a\u6837\u672c\u5373\u53ef\u751f\u6210\u4f18\u5316\u7684\u672b\u7aef\u6267\u884c\u5668\u59ff\u6001\u5e8f\u5217\u3002", "result": "\u5728\u5355\u5c42\u3001\u591a\u5c42\u548c\u6742\u4e71\u914d\u7f6e\u7b49\u591a\u79cd\u94a2\u7b4b\u7f51\u683c\u4e0a\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u9ad8\u6210\u529f\u7387\u7684\u8282\u70b9\u68c0\u6d4b\u548c\u51c6\u786e\u7684\u987a\u5e8f\u7ed1\u624e\u6027\u80fd\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u6570\u636e\u9700\u6c42\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u6df7\u5408\u611f\u77e5\u548c\u6269\u6563\u9a71\u52a8\u89c4\u5212\u5728\u65bd\u5de5\u73b0\u573a\u81ea\u52a8\u5316\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u52b3\u52a8\u6548\u7387\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u94a2\u7b4b\u7ed1\u624e\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.00641", "pdf": "https://arxiv.org/pdf/2509.00641", "abs": "https://arxiv.org/abs/2509.00641", "authors": ["Zhipeng Yin", "Zichong Wang", "Avash Palikhe", "Zhen Liu", "Jun Liu", "Wenbin Zhang"], "title": "AMCR: A Framework for Assessing and Mitigating Copyright Risks in Generative Models", "categories": ["cs.LG", "cs.CR", "cs.CV"], "comment": null, "summary": "Generative models have achieved impressive results in text to image tasks, significantly advancing visual content creation. However, this progress comes at a cost, as such models rely heavily on large-scale training data and may unintentionally replicate copyrighted elements, creating serious legal and ethical challenges for real-world deployment. To address these concerns, researchers have proposed various strategies to mitigate copyright risks, most of which are prompt based methods that filter or rewrite user inputs to prevent explicit infringement. While effective in handling obvious cases, these approaches often fall short in more subtle situations, where seemingly benign prompts can still lead to infringing outputs. To address these limitations, this paper introduces Assessing and Mitigating Copyright Risks (AMCR), a comprehensive framework which i) builds upon prompt-based strategies by systematically restructuring risky prompts into safe and non-sensitive forms, ii) detects partial infringements through attention-based similarity analysis, and iii) adaptively mitigates risks during generation to reduce copyright violations without compromising image quality. Extensive experiments validate the effectiveness of AMCR in revealing and mitigating latent copyright risks, offering practical insights and benchmarks for the safer deployment of generative models.", "AI": {"tldr": "AMCR\u6846\u67b6\u901a\u8fc7\u63d0\u793a\u91cd\u6784\u3001\u6ce8\u610f\u529b\u76f8\u4f3c\u6027\u5206\u6790\u548c\u81ea\u9002\u5e94\u98ce\u9669\u7f13\u89e3\uff0c\u6709\u6548\u68c0\u6d4b\u548c\u51cf\u8f7b\u751f\u6210\u6a21\u578b\u4e2d\u7684\u6f5c\u5728\u7248\u6743\u98ce\u9669", "motivation": "\u751f\u6210\u6a21\u578b\u5728\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4e25\u91cd\u4f9d\u8d56\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\uff0c\u53ef\u80fd\u65e0\u610f\u4e2d\u590d\u5236\u53d7\u7248\u6743\u4fdd\u62a4\u7684\u5185\u5bb9\uff0c\u5e26\u6765\u6cd5\u5f8b\u548c\u4f26\u7406\u6311\u6218\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u5728\u5904\u7406\u5fae\u5999\u4fb5\u6743\u60c5\u51b5\u65f6\u6548\u679c\u6709\u9650", "method": "\u63d0\u51faAMCR\u7efc\u5408\u6846\u67b6\uff1a1\uff09\u7cfb\u7edf\u91cd\u6784\u98ce\u9669\u63d0\u793a\u4e3a\u5b89\u5168\u5f62\u5f0f\uff1b2\uff09\u901a\u8fc7\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u76f8\u4f3c\u6027\u5206\u6790\u68c0\u6d4b\u90e8\u5206\u4fb5\u6743\uff1b3\uff09\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u81ea\u9002\u5e94\u7f13\u89e3\u98ce\u9669\uff0c\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf", "result": "\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86AMCR\u5728\u63ed\u793a\u548c\u51cf\u8f7b\u6f5c\u5728\u7248\u6743\u98ce\u9669\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u751f\u6210\u6a21\u578b\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u548c\u57fa\u51c6", "conclusion": "AMCR\u6846\u67b6\u4e3a\u89e3\u51b3\u751f\u6210\u6a21\u578b\u4e2d\u7684\u7248\u6743\u95ee\u9898\u63d0\u4f9b\u4e86\u5168\u9762\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u51cf\u5c11\u7248\u6743\u4fb5\u6743\u7684\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\uff0c\u4fc3\u8fdb\u751f\u6210\u6280\u672f\u7684\u66f4\u5b89\u5168\u5e94\u7528"}}
{"id": "2509.00911", "pdf": "https://arxiv.org/pdf/2509.00911", "abs": "https://arxiv.org/abs/2509.00911", "authors": ["Joongho Jo", "Jongsun Park"], "title": "GS-TG: 3D Gaussian Splatting Accelerator with Tile Grouping for Reducing Redundant Sorting while Preserving Rasterization Efficiency", "categories": ["cs.AR", "cs.CV"], "comment": "DAC 2025", "summary": "3D Gaussian Splatting (3D-GS) has emerged as a promising alternative to neural radiance fields (NeRF) as it offers high speed as well as high image quality in novel view synthesis. Despite these advancements, 3D-GS still struggles to meet the frames per second (FPS) demands of real-time applications. In this paper, we introduce GS-TG, a tile-grouping-based accelerator that enhances 3D-GS rendering speed by reducing redundant sorting operations and preserving rasterization efficiency. GS-TG addresses a critical trade-off issue in 3D-GS rendering: increasing the tile size effectively reduces redundant sorting operations, but it concurrently increases unnecessary rasterization computations. So, during sorting of the proposed approach, GS-TG groups small tiles (for making large tiles) to share sorting operations across tiles within each group, significantly reducing redundant computations. During rasterization, a bitmask assigned to each Gaussian identifies relevant small tiles, to enable efficient sharing of sorting results. Consequently, GS-TG enables sorting to be performed as if a large tile size is used by grouping tiles during the sorting stage, while allowing rasterization to proceed with the original small tiles by using bitmasks in the rasterization stage. GS-TG is a lossless method requiring no retraining or fine-tuning and it can be seamlessly integrated with previous 3D-GS optimization techniques. Experimental results show that GS-TG achieves an average speed-up of 1.54 times over state-of-the-art 3D-GS accelerators.", "AI": {"tldr": "GS-TG\u662f\u4e00\u79cd\u57fa\u4e8e\u74e6\u7247\u5206\u7ec4\u76843D\u9ad8\u65af\u6e85\u5c04\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u51cf\u5c11\u5197\u4f59\u6392\u5e8f\u64cd\u4f5c\u548c\u4fdd\u6301\u5149\u6805\u5316\u6548\u7387\uff0c\u5b9e\u73b01.54\u500d\u7684\u5e73\u5747\u52a0\u901f\u6548\u679c\u3002", "motivation": "3D\u9ad8\u65af\u6e85\u5c04\u867d\u7136\u6bd4NeRF\u901f\u5ea6\u5feb\uff0c\u4f46\u4ecd\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u5e94\u7528\u7684\u9ad8\u5e27\u7387\u9700\u6c42\uff0c\u5b58\u5728\u74e6\u7247\u5927\u5c0f\u4e0e\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u5728\u6392\u5e8f\u9636\u6bb5\u5c06\u5c0f\u74e6\u7247\u5206\u7ec4\u5f62\u6210\u5927\u74e6\u7247\u6765\u5171\u4eab\u6392\u5e8f\u64cd\u4f5c\uff0c\u5728\u5149\u6805\u5316\u9636\u6bb5\u4f7f\u7528\u4f4d\u63a9\u7801\u6807\u8bc6\u76f8\u5173\u5c0f\u74e6\u7247\uff0c\u5b9e\u73b0\u6392\u5e8f\u7ed3\u679c\u7684\u9ad8\u6548\u5171\u4eab\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aGS-TG\u76f8\u6bd4\u6700\u5148\u8fdb\u76843D-GS\u52a0\u901f\u5668\u5e73\u5747\u52a0\u901f1.54\u500d\uff0c\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\u3002", "conclusion": "GS-TG\u901a\u8fc7\u521b\u65b0\u7684\u74e6\u7247\u5206\u7ec4\u548c\u4f4d\u63a9\u7801\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e863D-GS\u6e32\u67d3\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u662f\u4e00\u79cd\u65e0\u635f\u4e14\u53ef\u4e0e\u5176\u4ed6\u4f18\u5316\u6280\u672f\u65e0\u7f1d\u96c6\u6210\u7684\u52a0\u901f\u65b9\u6848\u3002"}}
{"id": "2509.02154", "pdf": "https://arxiv.org/pdf/2509.02154", "abs": "https://arxiv.org/abs/2509.02154", "authors": ["Aymene Mohammed Bouayed", "Samuel Deslauriers-Gauthier", "Adrian Iaccovelli", "David Naccache"], "title": "Conditional-$t^3$VAE: Equitable Latent Space Allocation for Fair Generation", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": null, "summary": "Variational Autoencoders (VAEs) with global priors mirror the training set's class frequency in latent space, underrepresenting tail classes and reducing generative fairness on imbalanced datasets. While $t^3$VAE improves robustness via heavy-tailed Student's t-distribution priors, it still allocates latent volume proportionally to the class frequency.In this work, we address this issue by explicitly enforcing equitable latent space allocation across classes. To this end, we propose Conditional-$t^3$VAE, which defines a per-class \\mbox{Student's t} joint prior over latent and output variables, preventing dominance by majority classes. Our model is optimized using a closed-form objective derived from the $\\gamma$-power divergence. Moreover, for class-balanced generation, we derive an equal-weight latent mixture of Student's t-distributions. On SVHN-LT, CIFAR100-LT, and CelebA, Conditional-$t^3$VAE consistently achieves lower FID scores than both $t^3$VAE and Gaussian-based VAE baselines, particularly under severe class imbalance. In per-class F1 evaluations, Conditional-$t^3$VAE also outperforms the conditional Gaussian VAE across all highly imbalanced settings. While Gaussian-based models remain competitive under mild imbalance ratio ($\\rho \\lesssim 3$), our approach substantially improves generative fairness and diversity in more extreme regimes.", "AI": {"tldr": "Conditional-$t^3$VAE\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u7c7b\u522b\u5b9a\u4e49Student's t\u8054\u5408\u5148\u9a8c\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfVAE\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u6f5c\u5728\u7a7a\u95f4\u5206\u914d\u4e0d\u5747\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u516c\u5e73\u6027\u548c\u591a\u6837\u6027\u3002", "motivation": "\u4f20\u7edfVAE\u53ca\u5176\u53d8\u4f53$t^3$VAE\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\uff0c\u6f5c\u5728\u7a7a\u95f4\u5206\u914d\u4e0e\u8bad\u7ec3\u96c6\u7c7b\u522b\u9891\u7387\u6210\u6b63\u6bd4\uff0c\u5bfc\u81f4\u5c3e\u90e8\u7c7b\u522b\u8868\u793a\u4e0d\u8db3\uff0c\u751f\u6210\u516c\u5e73\u6027\u964d\u4f4e\u3002", "method": "\u63d0\u51faConditional-$t^3$VAE\uff0c\u4e3a\u6bcf\u4e2a\u7c7b\u522b\u5b9a\u4e49Student's t\u8054\u5408\u5148\u9a8c\uff0c\u9632\u6b62\u591a\u6570\u7c7b\u522b\u4e3b\u5bfc\u3002\u4f7f\u7528\u03b3-power\u6563\u5ea6\u63a8\u5bfc\u7684\u95ed\u5f0f\u76ee\u6807\u8fdb\u884c\u4f18\u5316\uff0c\u5e76\u63a8\u5bfc\u7b49\u6743\u91cd\u6f5c\u5728\u6df7\u5408Student's t\u5206\u5e03\u7528\u4e8e\u7c7b\u522b\u5e73\u8861\u751f\u6210\u3002", "result": "\u5728SVHN-LT\u3001CIFAR100-LT\u548cCelebA\u6570\u636e\u96c6\u4e0a\uff0cConditional-$t^3$VAE\u59cb\u7ec8\u83b7\u5f97\u6bd4$t^3$VAE\u548c\u57fa\u4e8e\u9ad8\u65af\u5206\u5e03\u7684VAE\u57fa\u7ebf\u66f4\u4f4e\u7684FID\u5206\u6570\uff0c\u7279\u522b\u662f\u5728\u4e25\u91cd\u7c7b\u522b\u4e0d\u5e73\u8861\u60c5\u51b5\u4e0b\u3002\u5728\u6bcf\u7c7bF1\u8bc4\u4f30\u4e2d\u4e5f\u4f18\u4e8e\u6761\u4ef6\u9ad8\u65afVAE\u3002", "conclusion": "\u5728\u8f7b\u5fae\u4e0d\u5e73\u8861\u60c5\u51b5\u4e0b\uff08\u03c1\u22723\uff09\uff0c\u57fa\u4e8e\u9ad8\u65af\u5206\u5e03\u7684\u6a21\u578b\u4ecd\u6709\u7ade\u4e89\u529b\uff0c\u4f46\u5728\u66f4\u6781\u7aef\u7684\u4e0d\u5e73\u8861\u60c5\u51b5\u4e0b\uff0cConditional-$t^3$VAE\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u516c\u5e73\u6027\u548c\u591a\u6837\u6027\u3002"}}
