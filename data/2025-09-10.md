<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 17]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Neural Cone Radiosity for Interactive Global Illumination with Glossy Materials](https://arxiv.org/abs/2509.07522)
*Jierui Ren,Haojie Jin,Bo Pang,Yisong Chen,Guoping Wang,Sheng Li*

Main category: cs.GR

TL;DR: 提出神经锥辐射度方法，通过反射感知的射线锥编码有效解决高频出射辐射度分布的建模难题，在保持紧凑网络结构的同时实现高质量实时渲染。


<details>
  <summary>Details</summary>
Motivation: 现有神经辐射度方法主要依赖位置特征编码，在捕捉高频、强视角依赖的辐射度分布方面存在明显局限，特别是对于光泽材质。

Method: 基于预滤波多分辨率哈希网格，通过连续空间聚合将视角依赖的反射特性直接嵌入编码过程，准确近似光泽BSDF波瓣。

Result: 在各种光泽度条件下实时生成高质量、无噪声的渲染结果，相比基线方法具有更高的保真度和真实感。

Conclusion: 该方法显著提升了网络对高频反射分布的建模能力，有效处理从高光到低光等各种光泽度表面，同时减轻了网络拟合复杂辐射度分布的负担。

Abstract: Modeling of high-frequency outgoing radiance distributions has long been a key challenge in rendering, particularly for glossy material. Such distributions concentrate radiative energy within a narrow lobe and are highly sensitive to changes in view direction. However, existing neural radiosity methods, which primarily rely on positional feature encoding, exhibit notable limitations in capturing these high-frequency, strongly view-dependent radiance distributions. To address this, we propose a highly-efficient approach by reflectance-aware ray cone encoding based on the neural radiosity framework, named neural cone radiosity. The core idea is to employ a pre-filtered multi-resolution hash grid to accurately approximate the glossy BSDF lobe, embedding view-dependent reflectance characteristics directly into the encoding process through continuous spatial aggregation. Our design not only significantly improves the network's ability to model high-frequency reflection distributions but also effectively handles surfaces with a wide range of glossiness levels, from highly glossy to low-gloss finishes. Meanwhile, our method reduces the network's burden in fitting complex radiance distributions, allowing the overall architecture to remain compact and efficient. Comprehensive experimental results demonstrate that our method consistently produces high-quality, noise-free renderings in real time under various glossiness conditions, and delivers superior fidelity and realism compared to baseline approaches.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [MEGS$^{2}$: Memory-Efficient Gaussian Splatting via Spherical Gaussians and Unified Pruning](https://arxiv.org/abs/2509.07021)
*Jiarui Chen,Yikeng Chen,Yingshuang Zou,Ye Huang,Peng Wang,Yuan Liu,Yujing Sun,Wenping Wang*

Main category: cs.CV

TL;DR: MEGS²是一个内存高效的3D高斯泼溅框架，通过联合优化基元数量和每个基元的参数，实现了前所未有的内存压缩，相比现有方法减少50%静态VRAM和40%渲染VRAM。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅技术虽然在新视角合成中表现出色，但其高内存消耗严重限制了在边缘设备上的应用。现有压缩方法大多只关注存储压缩，未能解决渲染内存的关键瓶颈问题。

Method: 1) 用轻量级的任意方向球面高斯瓣替代内存密集的球谐函数作为颜色表示；2) 提出统一的软剪枝框架，将基元数量和瓣数量剪枝建模为单一约束优化问题。

Result: 实验表明，MEGS²相比现有方法实现了50%的静态VRAM减少和40%的渲染VRAM减少，同时保持可比的渲染质量。

Conclusion: MEGS²通过联合优化基元数量和参数效率，成功解决了3DGS的内存瓶颈问题，为边缘设备部署提供了可行的解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a dominant novel-view synthesis technique, but its high memory consumption severely limits its applicability on edge devices. A growing number of 3DGS compression methods have been proposed to make 3DGS more efficient, yet most only focus on storage compression and fail to address the critical bottleneck of rendering memory. To address this problem, we introduce MEGS$^{2}$, a novel memory-efficient framework that tackles this challenge by jointly optimizing two key factors: the total primitive number and the parameters per primitive, achieving unprecedented memory compression. Specifically, we replace the memory-intensive spherical harmonics with lightweight arbitrarily-oriented spherical Gaussian lobes as our color representations. More importantly, we propose a unified soft pruning framework that models primitive-number and lobe-number pruning as a single constrained optimization problem. Experiments show that MEGS$^{2}$ achieves a 50% static VRAM reduction and a 40% rendering VRAM reduction compared to existing methods, while maintaining comparable rendering quality.

</details>


### [3] [Automated Evaluation of Gender Bias Across 13 Large Multimodal Models](https://arxiv.org/abs/2509.07050)
*Juan Manuel Contreras*

Main category: cs.CV

TL;DR: Aymara Image Fairness Evaluation基准测试显示，大型多模态模型(LMMs)在图像生成中系统性放大职业性别刻板印象，存在显著的默认男性偏见，且不同模型间的偏见程度差异很大。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽然识别了多模态模型中的性别偏见，但方法学限制阻碍了大规模、可比较的跨模型分析，需要开发标准化评估工具来促进AI公平性。

Method: 使用75个程序生成的性别中性提示词，测试13个商业LMM模型生成刻板男性、刻板女性和非刻板职业的人物图像，然后使用经过验证的LLM-as-a-judge系统对965张图像进行性别表征评分。

Result: LMMs系统性放大职业性别刻板印象：男性刻板职业中93.0%生成男性，女性刻板职业中仅22.5%生成男性；非刻板职业中68.3%默认生成男性；不同模型男性表征比例从46.7%到73.3%不等。

Conclusion: 偏见程度因模型设计选择而异，高性能模型可实现性别平等，表明高偏见并非不可避免。研究提供了最全面的跨模型性别偏见基准，强调标准化自动化评估工具对AI公平性的必要性。

Abstract: Large multimodal models (LMMs) have revolutionized text-to-image generation, but they risk perpetuating the harmful social biases in their training data. Prior work has identified gender bias in these models, but methodological limitations prevented large-scale, comparable, cross-model analysis. To address this gap, we introduce the Aymara Image Fairness Evaluation, a benchmark for assessing social bias in AI-generated images. We test 13 commercially available LMMs using 75 procedurally-generated, gender-neutral prompts to generate people in stereotypically-male, stereotypically-female, and non-stereotypical professions. We then use a validated LLM-as-a-judge system to score the 965 resulting images for gender representation. Our results reveal (p < .001 for all): 1) LMMs systematically not only reproduce but actually amplify occupational gender stereotypes relative to real-world labor data, generating men in 93.0% of images for male-stereotyped professions but only 22.5% for female-stereotyped professions; 2) Models exhibit a strong default-male bias, generating men in 68.3% of the time for non-stereotyped professions; and 3) The extent of bias varies dramatically across models, with overall male representation ranging from 46.7% to 73.3%. Notably, the top-performing model de-amplified gender stereotypes and approached gender parity, achieving the highest fairness scores. This variation suggests high bias is not an inevitable outcome but a consequence of design choices. Our work provides the most comprehensive cross-model benchmark of gender bias to date and underscores the necessity of standardized, automated evaluation tools for promoting accountability and fairness in AI development.

</details>


### [4] [Realism to Deception: Investigating Deepfake Detectors Against Face Enhancement](https://arxiv.org/abs/2509.07178)
*Muhammad Saad Saeed,Ijaz Ul Haq,Khalid Malik*

Main category: cs.CV

TL;DR: 这篇论文研究了面部增强技术对深度伪造检测器的影响，发现即使基础增强筛镜也能显著降低检测准确性，GAN基增强技术更能完全刺活检测器的脏漏。


<details>
  <summary>Details</summary>
Motivation: 面部增强技术虽能改善面部外观，但可能无意中扭曲生物识别特征，导致深度伪造检测器准确性下降。本研究想要识别这些增强技术是否可以作为反例证工具使用。

Method: 系统性评估传统图像处理方法和GAN基增强技术对深度伪造检测器稳健性的影响，分析对Naïve、Spatial和Frequency-based检测方法的效果，并进行对抗性训练实验。

Result: 实验结果显示，基础增强筛镜能将检测准确性降低至64.63%，GAN基技术更能达到75.12%的攻击成功率。对抗性训练并不能有效提升模型稳健性。

Conclusion: 面部增强技术可以有效地作为反例证工具，强调了开发更加弹性和适应性强的例证方法的必要性。

Abstract: Face enhancement techniques are widely used to enhance facial appearance. However, they can inadvertently distort biometric features, leading to significant decrease in the accuracy of deepfake detectors. This study hypothesizes that these techniques, while improving perceptual quality, can degrade the performance of deepfake detectors. To investigate this, we systematically evaluate whether commonly used face enhancement methods can serve an anti-forensic role by reducing detection accuracy. We use both traditional image processing methods and advanced GAN-based enhancements to evaluate the robustness of deepfake detectors. We provide a comprehensive analysis of the effectiveness of these enhancement techniques, focusing on their impact on Na\"ive, Spatial, and Frequency-based detection methods. Furthermore, we conduct adversarial training experiments to assess whether exposure to face enhancement transformations improves model robustness. Experiments conducted on the FaceForensics++, DeepFakeDetection, and CelebDF-v2 datasets indicate that even basic enhancement filters can significantly reduce detection accuracy achieving ASR up to 64.63\%. In contrast, GAN-based techniques further exploit these vulnerabilities, achieving ASR up to 75.12\%. Our results demonstrate that face enhancement methods can effectively function as anti-forensic tools, emphasizing the need for more resilient and adaptive forensic methods.

</details>


### [5] [Reconstruction Alignment Improves Unified Multimodal Models](https://arxiv.org/abs/2509.07295)
*Ji Xie,Trevor Darrell,Luke Zettlemoyer,XuDong Wang*

Main category: cs.CV

TL;DR: RecA是一种资源高效的后训练方法，通过利用视觉理解编码器嵌入作为密集文本提示，无需标注即可提供丰富监督，显著提升多模态模型的生成和编辑性能


<details>
  <summary>Details</summary>
Motivation: 传统多模态模型训练依赖稀疏的图像-文本对，缺乏细粒度视觉细节监督，导致理解和生成能力不对齐

Method: 提出重建对齐(RecA)方法，让模型基于自身的视觉理解嵌入重建输入图像，使用自监督重建损失来对齐理解和生成

Result: 仅用27 GPU小时，在GenEval(0.73→0.90)和DPGBench(80.93→88.15)上显著提升生成性能，编辑基准也有改善(ImgEdit 3.38→3.75, GEdit 6.94→7.25)，超越更大开源模型

Conclusion: RecA是一种高效通用的后训练对齐策略，适用于各种多模态模型架构，能有效提升视觉生成和编辑的保真度

Abstract: Unified multimodal models (UMMs) unify visual understanding and generation within a single architecture. However, conventional training relies on image-text pairs (or sequences) whose captions are typically sparse and miss fine-grained visual details--even when they use hundreds of words to describe a simple image. We introduce Reconstruction Alignment (RecA), a resource-efficient post-training method that leverages visual understanding encoder embeddings as dense "text prompts," providing rich supervision without captions. Concretely, RecA conditions a UMM on its own visual understanding embeddings and optimizes it to reconstruct the input image with a self-supervised reconstruction loss, thereby realigning understanding and generation. Despite its simplicity, RecA is broadly applicable: across autoregressive, masked-autoregressive, and diffusion-based UMMs, it consistently improves generation and editing fidelity. With only 27 GPU-hours, post-training with RecA substantially improves image generation performance on GenEval (0.73$\rightarrow$0.90) and DPGBench (80.93$\rightarrow$88.15), while also boosting editing benchmarks (ImgEdit 3.38$\rightarrow$3.75, GEdit 6.94$\rightarrow$7.25). Notably, RecA surpasses much larger open-source models and applies broadly across diverse UMM architectures, establishing it as an efficient and general post-training alignment strategy for UMMs

</details>


### [6] [DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset Generation](https://arxiv.org/abs/2509.07435)
*Ze-Xin Yin,Jiaxiong Qiu,Liu Liu,Xinjie Wang,Wei Sui,Zhizhong Su,Jian Yang,Jin Xie*

Main category: cs.CV

TL;DR: LGAA是一个端到端的PBR就绪3D资产生成框架，通过多视角扩散先验统一建模几何和材质，使用模块化设计实现高效训练和高质量可重光照网格生成。


<details>
  <summary>Details</summary>
Motivation: 现有的3D生成方法主要关注几何建模，将纹理烘焙为简单顶点颜色或留给后处理，缺乏端到端的PBR材质生成能力，需要自主的3D资产创建流程。

Method: 采用模块化设计：LGAA Wrapper重用多视角扩散模型层，LGAA Switcher对齐多个扩散先验，LGAA Decoder预测带PBR通道的2D高斯溅射，最后通过后处理提取高质量可重光照网格。

Result: 实验证明LGAA在文本和图像条件多视角扩散模型中表现优异，模块化设计支持灵活集成多个扩散先验，仅需69k多视角实例即可高效收敛。

Conclusion: LGAA实现了端到端的PBR就绪3D资产生成，统一了几何和材质的建模，具有高效收敛和高质量输出的优势。

Abstract: The labor- and experience-intensive creation of 3D assets with physically based rendering (PBR) materials demands an autonomous 3D asset creation pipeline. However, most existing 3D generation methods focus on geometry modeling, either baking textures into simple vertex colors or leaving texture synthesis to post-processing with image diffusion models. To achieve end-to-end PBR-ready 3D asset generation, we present Lightweight Gaussian Asset Adapter (LGAA), a novel framework that unifies the modeling of geometry and PBR materials by exploiting multi-view (MV) diffusion priors from a novel perspective. The LGAA features a modular design with three components. Specifically, the LGAA Wrapper reuses and adapts network layers from MV diffusion models, which encapsulate knowledge acquired from billions of images, enabling better convergence in a data-efficient manner. To incorporate multiple diffusion priors for geometry and PBR synthesis, the LGAA Switcher aligns multiple LGAA Wrapper layers encapsulating different knowledge. Then, a tamed variational autoencoder (VAE), termed LGAA Decoder, is designed to predict 2D Gaussian Splatting (2DGS) with PBR channels. Finally, we introduce a dedicated post-processing procedure to effectively extract high-quality, relightable mesh assets from the resulting 2DGS. Extensive quantitative and qualitative experiments demonstrate the superior performance of LGAA with both text-and image-conditioned MV diffusion models. Additionally, the modular design enables flexible incorporation of multiple diffusion priors, and the knowledge-preserving scheme leads to efficient convergence trained on merely 69k multi-view instances. Our code, pre-trained weights, and the dataset used will be publicly available via our project page: https://zx-yin.github.io/dreamlifting/.

</details>


### [7] [Bias-Aware Machine Unlearning: Towards Fairer Vision Models via Controllable Forgetting](https://arxiv.org/abs/2509.07456)
*Sai Siddhartha Chary Aylapuram,Veeraraju Elluru,Shivang Agarwal*

Main category: cs.CV

TL;DR: 该论文提出了一种基于机器遗忘技术的偏置感知方法，通过在训练后选择性移除偏置样本或特征表示来减少视觉模型中的各种偏置，无需重新训练即可显著提升模型公平性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络容易依赖训练数据中的虚假相关性，导致在医疗和自动驾驶等安全关键领域产生有偏或不公平的预测。传统偏置缓解方法需要从头重新训练或重新设计数据管道，成本高昂。

Method: 基于隐私保护遗忘技术，评估了梯度上升、LoRA和师生蒸馏等多种策略，在CUB-200-2011（姿态偏置）、CIFAR-10（合成补丁偏置）和CelebA（微笑检测中的性别偏置）三个基准数据集上进行实证分析。

Result: 后训练遗忘方法显著减少了子组差异，在CUB-200上人口统计公平性提升94.86%，CIFAR-10提升30.28%，CelebA提升97.37%，准确率损失最小，在效用、公平性、质量和隐私的联合评估中平均得分0.62。

Conclusion: 机器遗忘技术为增强已部署视觉系统的公平性提供了一个实用框架，无需进行完整的重新训练。

Abstract: Deep neural networks often rely on spurious correlations in training data, leading to biased or unfair predictions in safety-critical domains such as medicine and autonomous driving. While conventional bias mitigation typically requires retraining from scratch or redesigning data pipelines, recent advances in machine unlearning provide a promising alternative for post-hoc model correction. In this work, we investigate \textit{Bias-Aware Machine Unlearning}, a paradigm that selectively removes biased samples or feature representations to mitigate diverse forms of bias in vision models. Building on privacy-preserving unlearning techniques, we evaluate various strategies including Gradient Ascent, LoRA, and Teacher-Student distillation. Through empirical analysis on three benchmark datasets, CUB-200-2011 (pose bias), CIFAR-10 (synthetic patch bias), and CelebA (gender bias in smile detection), we demonstrate that post-hoc unlearning can substantially reduce subgroup disparities, with improvements in demographic parity of up to \textbf{94.86\%} on CUB-200, \textbf{30.28\%} on CIFAR-10, and \textbf{97.37\%} on CelebA. These gains are achieved with minimal accuracy loss and with methods scoring an average of 0.62 across the 3 settings on the joint evaluation of utility, fairness, quality, and privacy. Our findings establish machine unlearning as a practical framework for enhancing fairness in deployed vision systems without necessitating full retraining.

</details>


### [8] [ANYPORTAL: Zero-Shot Consistent Video Background Replacement](https://arxiv.org/abs/2509.07472)
*Wenshuo Gao,Xicheng Lan,Shuai Yang*

Main category: cs.CV

TL;DR: ANYPORTAL是一个零样本视频背景替换框架，利用预训练扩散模型实现高质量视频编辑，无需训练即可保持前景一致性和时间连贯性


<details>
  <summary>Details</summary>
Motivation: 现有视频生成技术在精确控制视频细节方面存在不足，难以实现用户意图的精确对齐，限制了实际应用

Method: 提出零样本框架ANYPORTAL，结合视频扩散模型的时间先验和图像扩散模型的重新光照能力，使用精炼投影算法实现像素级细节操作

Result: 实验结果表明ANYPORTAL在消费级GPU上实现了高质量结果，为视频内容创作和编辑提供了实用高效的解决方案

Conclusion: ANYPORTAL框架成功解决了视频背景替换中的前景一致性和时间连贯光照挑战，为零样本视频编辑提供了有效方法

Abstract: Despite the rapid advancements in video generation technology, creating high-quality videos that precisely align with user intentions remains a significant challenge. Existing methods often fail to achieve fine-grained control over video details, limiting their practical applicability. We introduce ANYPORTAL, a novel zero-shot framework for video background replacement that leverages pre-trained diffusion models. Our framework collaboratively integrates the temporal prior of video diffusion models with the relighting capabilities of image diffusion models in a zero-shot setting. To address the critical challenge of foreground consistency, we propose a Refinement Projection Algorithm, which enables pixel-level detail manipulation to ensure precise foreground preservation. ANYPORTAL is training-free and overcomes the challenges of achieving foreground consistency and temporally coherent relighting. Experimental results demonstrate that ANYPORTAL achieves high-quality results on consumer-grade GPUs, offering a practical and efficient solution for video content creation and editing.

</details>


### [9] [Fine-Tuning Vision-Language Models for Visual Navigation Assistance](https://arxiv.org/abs/2509.07488)
*Xiao Li,Bharat Gandhi,Ming Zhan,Mohit Nehra,Zhicheng Zhang,Yuchen Sun,Meijia Song,Naisheng Zhang,Xi Wang*

Main category: cs.CV

TL;DR: 通过细调BLIP-2模型使用LoRA技术，提出了一种给视障人士提供室内导航的视觉-语言导航系统，并设计了专门的评估指标来量化导航指令的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统导航系统在室内环境中效果差，缺乏精确位置数据，需要为视障人士提供更有效的室内导航帮助。

Method: 使用低秩适配(LoRA)技术对BLIP-2模型进行细调，基于手动注释的室内导航数据集生成步骤式导航指令。

Result: 细调后的模型在生成方向指令方面显著提升，充分充分克服了原始BLIP-2模型的限制。

Conclusion: 该方法通过结合视觉和语言模型，为视障人士提供了更加准确的室内导航解决方案，提高了访问性和独立性。

Abstract: We address vision-language-driven indoor navigation to assist visually impaired individuals in reaching a target location using images and natural language guidance. Traditional navigation systems are ineffective indoors due to the lack of precise location data. Our approach integrates vision and language models to generate step-by-step navigational instructions, enhancing accessibility and independence. We fine-tune the BLIP-2 model with Low Rank Adaptation (LoRA) on a manually annotated indoor navigation dataset. We propose an evaluation metric that refines the BERT F1 score by emphasizing directional and sequential variables, providing a more comprehensive measure of navigational performance. After applying LoRA, the model significantly improved in generating directional instructions, overcoming limitations in the original BLIP-2 model.

</details>


### [10] [DiGS: Accurate and Complete Surface Reconstruction from 3D Gaussians via Direct SDF Learning](https://arxiv.org/abs/2509.07493)
*Wenzhi Guo,Bing Wang*

Main category: cs.CV

TL;DR: DiGS将符号距离场(SDF)学习嵌入3D高斯泼溅框架，通过几何引导的网格增长策略提升表面重建的准确性和完整性，同时保持高渲染质量


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅在渲染方面效果出色，但由于表示的无结构性和缺乏显式几何监督，难以实现准确完整的表面重建

Method: 为每个高斯关联可学习的SDF值，设计几何引导的多尺度网格增长策略，自适应地在几何一致区域分布高斯

Result: 在DTU、Mip-NeRF 360和Tanks&Temples等标准基准测试中，DiGS持续提升了重建准确性和完整性，同时保持了高渲染保真度

Conclusion: DiGS通过将SDF学习与3DGS结合，实现了更好的几何一致性和表面重建质量，为3D重建提供了统一的解决方案

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a powerful paradigm for photorealistic view synthesis, representing scenes with spatially distributed Gaussian primitives. While highly effective for rendering, achieving accurate and complete surface reconstruction remains challenging due to the unstructured nature of the representation and the absence of explicit geometric supervision. In this work, we propose DiGS, a unified framework that embeds Signed Distance Field (SDF) learning directly into the 3DGS pipeline, thereby enforcing strong and interpretable surface priors. By associating each Gaussian with a learnable SDF value, DiGS explicitly aligns primitives with underlying geometry and improves cross-view consistency. To further ensure dense and coherent coverage, we design a geometry-guided grid growth strategy that adaptively distributes Gaussians along geometry-consistent regions under a multi-scale hierarchy. Extensive experiments on standard benchmarks, including DTU, Mip-NeRF 360, and Tanks& Temples, demonstrate that DiGS consistently improves reconstruction accuracy and completeness while retaining high rendering fidelity.

</details>


### [11] [TextlessRAG: End-to-End Visual Document RAG by Speech Without Text](https://arxiv.org/abs/2509.07538)
*Peijin Xie,Shun Qian,Bingquan Liu,Dexin Wang,Lin Sun,Xiangzheng Zhang*

Main category: cs.CV

TL;DR: TextlessRAG是首个端到端的语音文档问答框架，无需ASR、TTS和OCR，直接在语音和视觉文档之间进行问答，并发布了首个双语语音-文档RAG数据集。


<details>
  <summary>Details</summary>
Motivation: 文档图像包含丰富知识，语音查询具有便携性和灵活性，但之前没有工作探索过直接在语音查询下对视觉文档图像进行知识库问答。

Method: 提出TextlessRAG端到端框架，消除ASR、TTS和OCR，直接解释语音、检索相关视觉知识并生成答案，采用完全无文本的流程，并集成了布局感知重排序机制来优化检索。

Result: 实验证明在效率和准确性方面都有显著提升。

Conclusion: 该框架为语音-文档问答开辟了新方向，发布的双语数据集和完整流程将促进该领域研究发展。

Abstract: Document images encapsulate a wealth of knowledge, while the portability of spoken queries enables broader and flexible application scenarios. Yet, no prior work has explored knowledge base question answering over visual document images with queries provided directly in speech. We propose TextlessRAG, the first end-to-end framework for speech-based question answering over large-scale document images. Unlike prior methods, TextlessRAG eliminates ASR, TTS and OCR, directly interpreting speech, retrieving relevant visual knowledge, and generating answers in a fully textless pipeline. To further boost performance, we integrate a layout-aware reranking mechanism to refine retrieval. Experiments demonstrate substantial improvements in both efficiency and accuracy. To advance research in this direction, we also release the first bilingual speech--document RAG dataset, featuring Chinese and English voice queries paired with multimodal document content. Both the dataset and our pipeline will be made available at repository:https://github.com/xiepeijinhit-hue/textlessrag

</details>


### [12] [PanoLAM: Large Avatar Model for Gaussian Full-Head Synthesis from One-shot Unposed Image](https://arxiv.org/abs/2509.07552)
*Peng Li,Yisheng He,Yingdong Hu,Yuan Dong,Weihao Yuan,Yuan Liu,Zilong Dong,Yike Guo*

Main category: cs.CV

TL;DR: 基于单张无姿态图片通过前向传播框架快速合成高保真的高斯全头模型，避免了GAN逆向和测试时优化的耗时问题。


<details>
  <summary>Details</summary>
Motivation: 解决从单张无姿态图片快速重建9AD全头模型的挑战，避免传统方法中GAN逆向和测试时优化的高计算成本。

Method: 使用大规模合成数据集训练，提出由粗到细的高斯头部生成流水线，通过FLAME模型稀疏点与图像特征交互，并使用双分支框架聚合结构化球面三平面特征和非结构化点特征。

Result: 实验结果显示该框架能够在单次前向传播中完成高保真的全头重建，在速度和效果上都显著优于现有方法。

Conclusion: 该前向传播框架为单图3D头部重建提供了高效、高质量的解决方案，为快速渲染和实时应用创造了条件。

Abstract: We present a feed-forward framework for Gaussian full-head synthesis from a single unposed image. Unlike previous work that relies on time-consuming GAN inversion and test-time optimization, our framework can reconstruct the Gaussian full-head model given a single unposed image in a single forward pass. This enables fast reconstruction and rendering during inference. To mitigate the lack of large-scale 3D head assets, we propose a large-scale synthetic dataset from trained 3D GANs and train our framework using only synthetic data. For efficient high-fidelity generation, we introduce a coarse-to-fine Gaussian head generation pipeline, where sparse points from the FLAME model interact with the image features by transformer blocks for feature extraction and coarse shape reconstruction, which are then densified for high-fidelity reconstruction. To fully leverage the prior knowledge residing in pretrained 3D GANs for effective reconstruction, we propose a dual-branch framework that effectively aggregates the structured spherical triplane feature and unstructured point-based features for more effective Gaussian head reconstruction. Experimental results show the effectiveness of our framework towards existing work.

</details>


### [13] [HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting](https://arxiv.org/abs/2509.07774)
*Yimin Pan,Matthias Nießner,Tobias Kirschstein*

Main category: cs.CV

TL;DR: 基于3D高斯泼溅技术，提出了一种从多视角图像重建发丝级头发几何的端到端方法，通过多阶段流程实现高效精确的头发重建


<details>
  <summary>Details</summary>
Motivation: 虚拟现实和数字人建模应用中需要高质量的头发重建，现有方法往往忽略发丝的连接性和拓扑结构，3DGS的显式表示与发丝结构天然契合

Method: 多阶段流水线：首先使用可微分高斯光栅化器重建详细头发几何，然后通过新颖的合并方案将高斯段合并为连贯发丝，最后在光度监督下优化和生长发丝

Result: 在合成和真实数据集上的广泛实验表明，该方法能稳健处理各种发型，实现高效重建（通常在一小时内完成），并提出了评估拓扑准确性的新指标

Conclusion: 该方法成功扩展了3DGS框架用于发丝级头发重建，解决了现有方法忽略拓扑结构的问题，为头发重建提供了新的解决方案

Abstract: Human hair reconstruction is a challenging problem in computer vision, with growing importance for applications in virtual reality and digital human modeling. Recent advances in 3D Gaussians Splatting (3DGS) provide efficient and explicit scene representations that naturally align with the structure of hair strands. In this work, we extend the 3DGS framework to enable strand-level hair geometry reconstruction from multi-view images. Our multi-stage pipeline first reconstructs detailed hair geometry using a differentiable Gaussian rasterizer, then merges individual Gaussian segments into coherent strands through a novel merging scheme, and finally refines and grows the strands under photometric supervision.   While existing methods typically evaluate reconstruction quality at the geometric level, they often neglect the connectivity and topology of hair strands. To address this, we propose a new evaluation metric that serves as a proxy for assessing topological accuracy in strand reconstruction. Extensive experiments on both synthetic and real-world datasets demonstrate that our method robustly handles a wide range of hairstyles and achieves efficient reconstruction, typically completing within one hour.   The project page can be found at: https://yimin-pan.github.io/hair-gs/

</details>


### [14] [RayGaussX: Accelerating Gaussian-Based Ray Marching for Real-Time and High-Quality Novel View Synthesis](https://arxiv.org/abs/2509.07782)
*Hugo Blanc,Jean-Emmanuel Deschaud,Alexis Paljic*

Main category: cs.CV

TL;DR: RayGaussX在RayGauss基础上进行优化，通过引入体积渲染加速策略、增强光线一致性、尺度正则化和新的致密化准则，实现了5-12倍训练加速和50-80倍渲染速度提升，同时提高视觉质量。


<details>
  <summary>Details</summary>
Motivation: RayGauss虽然在合成和室内场景的新视角合成中达到了最先进的渲染质量，但其计算成本阻碍了在真实世界场景中的实时渲染。

Method: 引入体积渲染加速策略（空空间跳过和自适应采样）、增强光线一致性、尺度正则化以减少误报交集，并提出新的致密化准则来改善远距离区域的密度分布。

Result: 在真实世界数据集上实现了5-12倍训练加速和50-80倍渲染速度提升（FPS），视觉质量提升高达+0.56 dB PSNR。

Conclusion: RayGaussX成功解决了RayGauss的计算效率问题，在保持高质量渲染的同时实现了显著的性能提升，适用于更大规模的场景。

Abstract: RayGauss has achieved state-of-the-art rendering quality for novel-view synthesis on synthetic and indoor scenes by representing radiance and density fields with irregularly distributed elliptical basis functions, rendered via volume ray casting using a Bounding Volume Hierarchy (BVH). However, its computational cost prevents real-time rendering on real-world scenes. Our approach, RayGaussX, builds on RayGauss by introducing key contributions that accelerate both training and inference. Specifically, we incorporate volumetric rendering acceleration strategies such as empty-space skipping and adaptive sampling, enhance ray coherence, and introduce scale regularization to reduce false-positive intersections. Additionally, we propose a new densification criterion that improves density distribution in distant regions, leading to enhanced graphical quality on larger scenes. As a result, RayGaussX achieves 5x to 12x faster training and 50x to 80x higher rendering speeds (FPS) on real-world datasets while improving visual quality by up to +0.56 dB in PSNR. Project page with videos and code: https://raygaussx.github.io/.

</details>


### [15] [Faster, Self-Supervised Super-Resolution for Anisotropic Multi-View MRI Using a Sparse Coordinate Loss](https://arxiv.org/abs/2509.07798)
*Maja Schlereth,Moritz Schillinger,Katharina Breininger*

Main category: cs.CV

TL;DR: 通过自监督学习方法，将两个正交各异性低分辨率磁共振成像融合，重建高分辨率图像，无需高分辨率数据训练，重建速度提升10倍


<details>
  <summary>Details</summary>
Motivation: 医学MR成像中，高分辨率扫描耗时长且病人不舒适，低分辨率图像分析粗糖易错，需要一种无需HR数据的超分辨率重建方法

Method: 提出多视角神经网络，采用自监督方式训练，不需高分辨率数据。引入稀疏坐标基损失函数，支持任意缩放比例的LR图像融合。结合病人无关离线和病人特定在线两个阶段

Result: 在两个独立的MR图像数据集上评估，超分辨率性能与现有自监督SOTA方法相比或更优，病人特定重建速度提升达10倍，质量保持或提升

Conclusion: 该方法能够在无需高分辨率数据的情况下，通过融合多个低分辨率扫描快速重建高质量MR图像，显著提高了医学图像处理效率和诊断准确性

Abstract: Acquiring images in high resolution is often a challenging task. Especially in the medical sector, image quality has to be balanced with acquisition time and patient comfort. To strike a compromise between scan time and quality for Magnetic Resonance (MR) imaging, two anisotropic scans with different low-resolution (LR) orientations can be acquired. Typically, LR scans are analyzed individually by radiologists, which is time consuming and can lead to inaccurate interpretation. To tackle this, we propose a novel approach for fusing two orthogonal anisotropic LR MR images to reconstruct anatomical details in a unified representation. Our multi-view neural network is trained in a self-supervised manner, without requiring corresponding high-resolution (HR) data. To optimize the model, we introduce a sparse coordinate-based loss, enabling the integration of LR images with arbitrary scaling. We evaluate our method on MR images from two independent cohorts. Our results demonstrate comparable or even improved super-resolution (SR) performance compared to state-of-the-art (SOTA) self-supervised SR methods for different upsampling scales. By combining a patient-agnostic offline and a patient-specific online phase, we achieve a substantial speed-up of up to ten times for patient-specific reconstruction while achieving similar or better SR quality. Code is available at https://github.com/MajaSchle/tripleSR.

</details>


### [16] [SplatFill: 3D Scene Inpainting via Depth-Guided Gaussian Splatting](https://arxiv.org/abs/2509.07809)
*Mahtab Dahaghin,Milind G. Padalkar,Matteo Toso,Alessio Del Bue*

Main category: cs.CV

TL;DR: SplatFill是一种新颖的深度引导3D高斯溅射场景修复方法，通过联合深度监督和一致性感知细化，在视觉保真度和效率方面达到最先进水平


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射(3DGS)虽然能创建高度逼真的3D场景表示，但在修复因遮挡或场景编辑导致的缺失区域时仍存在挑战，常导致模糊细节、伪影和几何不一致

Method: 结合两种关键思想：(1)联合深度基和对象基监督，确保修复的高斯准确放置在3D空间中并与周围几何对齐；(2)一致性感知细化方案，选择性识别和修正不一致区域而不破坏场景其他部分

Result: 在SPIn-NeRF数据集上的评估显示，SplatFill在视觉保真度上超越现有NeRF基和3DGS基修复方法，同时减少24.5%的训练时间，提供更清晰的细节、更少伪影和更好的视角一致性

Conclusion: SplatFill通过深度引导和监督策略，成功解决了3DGS场景修复中的关键挑战，实现了高质量和高效率的场景修复

Abstract: 3D Gaussian Splatting (3DGS) has enabled the creation of highly realistic 3D scene representations from sets of multi-view images. However, inpainting missing regions, whether due to occlusion or scene editing, remains a challenging task, often leading to blurry details, artifacts, and inconsistent geometry. In this work, we introduce SplatFill, a novel depth-guided approach for 3DGS scene inpainting that achieves state-of-the-art perceptual quality and improved efficiency. Our method combines two key ideas: (1) joint depth-based and object-based supervision to ensure inpainted Gaussians are accurately placed in 3D space and aligned with surrounding geometry, and (2) we propose a consistency-aware refinement scheme that selectively identifies and corrects inconsistent regions without disrupting the rest of the scene. Evaluations on the SPIn-NeRF dataset demonstrate that SplatFill not only surpasses existing NeRF-based and 3DGS-based inpainting methods in visual fidelity but also reduces training time by 24.5%. Qualitative results show our method delivers sharper details, fewer artifacts, and greater coherence across challenging viewpoints.

</details>


### [17] [ScoreHOI: Physically Plausible Reconstruction of Human-Object Interaction via Score-Guided Diffusion](https://arxiv.org/abs/2509.07920)
*Ao Li,Jinpeng Liu,Yixuan Zhu,Yansong Tang*

Main category: cs.CV

TL;DR: ScoreHOI是一个基于扩散模型的优化器，通过引入扩散先验和物理约束来改进人-物交互的联合重建，在标准基准测试中表现出优越性能


<details>
  <summary>Details</summary>
Motivation: 以往优化方法由于缺乏人-物交互先验知识，难以实现物理上合理的重建结果，需要引入先验知识来提升重建质量

Method: 提出ScoreHOI扩散优化器，利用分数引导采样的可控性重建条件分布；提出接触驱动的迭代细化方法增强接触合理性；在去噪过程中使用特定物理约束指导

Result: 在标准基准测试中表现出优于现有最先进方法的性能，实现了精确和鲁棒的联合人-物交互重建改进

Conclusion: ScoreHOI通过扩散先验和物理约束有效提升了人-物交互重建的准确性和物理合理性，为理解人-环境复杂相互关系提供了重要技术手段

Abstract: Joint reconstruction of human-object interaction marks a significant milestone in comprehending the intricate interrelations between humans and their surrounding environment. Nevertheless, previous optimization methods often struggle to achieve physically plausible reconstruction results due to the lack of prior knowledge about human-object interactions. In this paper, we introduce ScoreHOI, an effective diffusion-based optimizer that introduces diffusion priors for the precise recovery of human-object interactions. By harnessing the controllability within score-guided sampling, the diffusion model can reconstruct a conditional distribution of human and object pose given the image observation and object feature. During inference, the ScoreHOI effectively improves the reconstruction results by guiding the denoising process with specific physical constraints. Furthermore, we propose a contact-driven iterative refinement approach to enhance the contact plausibility and improve the reconstruction accuracy. Extensive evaluations on standard benchmarks demonstrate ScoreHOI's superior performance over state-of-the-art methods, highlighting its ability to achieve a precise and robust improvement in joint human-object interaction reconstruction.

</details>


### [18] [Feature Space Analysis by Guided Diffusion Model](https://arxiv.org/abs/2509.07936)
*Kimiaki Shirahama,Miki Yanobu,Kaduki Yamashita,Miho Ohsaki*

Main category: cs.CV

TL;DR: 通过导向温度模型实现的解码器，生成与指定特征密切匹配的图像，用于解析DNN特征空间的黑盒性质


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络特征提取过程的黑盒性问题，通过生成特征相似图像来揭示DNN编码的图像属性

Method: 使用导向温度模型实现解码器，在预训练温度模型的反向图像生成过程中指导，最小化清洁图像估计特征与用户指定特征的欧几里得距离

Result: 在CLIP图像编码器、ResNet-50和视觉Transformer上验证，生成图像的特征与指定特征高度相似，揭示了这些DNN特征空间的价值见解

Conclusion: 该解码器无需额外训练即可分析不同DNN特征空间，在单卡GPU上运行，为解释DNN黑盒提供了实用方法

Abstract: One of the key issues in Deep Neural Networks (DNNs) is the black-box nature of their internal feature extraction process. Targeting vision-related domains, this paper focuses on analysing the feature space of a DNN by proposing a decoder that can generate images whose features are guaranteed to closely match a user-specified feature. Owing to this guarantee that is missed in past studies, our decoder allows us to evidence which of various attributes in an image are encoded into a feature by the DNN, by generating images whose features are in proximity to that feature. Our decoder is implemented as a guided diffusion model that guides the reverse image generation of a pre-trained diffusion model to minimise the Euclidean distance between the feature of a clean image estimated at each step and the user-specified feature. One practical advantage of our decoder is that it can analyse feature spaces of different DNNs with no additional training and run on a single COTS GPU. The experimental results targeting CLIP's image encoder, ResNet-50 and vision transformer demonstrate that images generated by our decoder have features remarkably similar to the user-specified ones and reveal valuable insights into these DNNs' feature spaces.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [19] [DepthVision: Robust Vision-Language Understanding through GAN-Based LiDAR-to-RGB Synthesis](https://arxiv.org/abs/2509.07463)
*Sven Kirchner,Nils Purschke,Ross Greer,Alois C. Knoll*

Main category: cs.RO

TL;DR: DepthVision是一个多模态场景理解框架，通过条件GAN从稀疏LiDAR点云合成RGB图像，并结合真实RGB数据，在光照条件不佳时提升机器人视觉性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉输入退化或不足时机器人可靠操作的挑战，特别是在黑暗、运动模糊等传感器退化情况下。

Method: 使用条件生成对抗网络（GAN）从稀疏LiDAR点云合成RGB图像，通过Luminance-Aware Modality Adaptation（LAMA）根据环境光照条件动态融合合成图像和真实RGB数据。

Result: 在真实和模拟数据集上的评估显示，该方法在低光照条件下显著提升性能，相比仅使用RGB的基线有大幅改进，同时保持与冻结视觉语言模型的兼容性。

Conclusion: 这项工作展示了LiDAR引导的RGB合成在实现真实环境中鲁棒机器人操作方面的潜力，无需对下游视觉语言模型进行微调。

Abstract: Ensuring reliable robot operation when visual input is degraded or insufficient remains a central challenge in robotics. This letter introduces DepthVision, a framework for multimodal scene understanding designed to address this problem. Unlike existing Vision-Language Models (VLMs), which use only camera-based visual input alongside language, DepthVision synthesizes RGB images from sparse LiDAR point clouds using a conditional generative adversarial network (GAN) with an integrated refiner network. These synthetic views are then combined with real RGB data using a Luminance-Aware Modality Adaptation (LAMA), which blends the two types of data dynamically based on ambient lighting conditions. This approach compensates for sensor degradation, such as darkness or motion blur, without requiring any fine-tuning of downstream vision-language models. We evaluate DepthVision on real and simulated datasets across various models and tasks, with particular attention to safety-critical tasks. The results demonstrate that our approach improves performance in low-light conditions, achieving substantial gains over RGB-only baselines while preserving compatibility with frozen VLMs. This work highlights the potential of LiDAR-guided RGB synthesis for achieving robust robot operation in real-world environments.

</details>
