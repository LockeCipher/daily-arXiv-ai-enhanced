{"id": "2510.02390", "pdf": "https://arxiv.org/pdf/2510.02390", "abs": "https://arxiv.org/abs/2510.02390", "authors": ["Zilai Li"], "title": "Hyperparameters are all you need: Using five-step inference for an original diffusion model to generate images comparable to the latest distillation model", "categories": ["cs.GR", "cs.AI", "eess.IV"], "comment": "10 pages, 5 figures, conference", "summary": "The diffusion model is a state-of-the-art generative model that generates an image by applying a neural network iteratively. Moreover, this generation process is regarded as an algorithm solving an ordinary differential equation or a stochastic differential equation. Based on the analysis of the truncation error of the diffusion ODE and SDE, our study proposes a training-free algorithm that generates high-quality 512 x 512 and 1024 x 1024 images in eight steps, with flexible guidance scales. To the best of my knowledge, our algorithm is the first one that samples a 1024 x 1024 resolution image in 8 steps with an FID performance comparable to that of the latest distillation model, but without additional training. Meanwhile, our algorithm can also generate a 512 x 512 image in 8 steps, and its FID performance is better than the inference result using state-of-the-art ODE solver DPM++ 2m in 20 steps. We validate our eight-step image generation algorithm using the COCO 2014, COCO 2017, and LAION datasets. And our best FID performance is 15.7, 22.35, and 17.52. While the FID performance of DPM++2m is 17.3, 23.75, and 17.33. Further, it also outperforms the state-of-the-art AMED-plugin solver, whose FID performance is 19.07, 25.50, and 18.06. We also apply the algorithm in five-step inference without additional training, for which the best FID performance in the datasets mentioned above is 19.18, 23.24, and 19.61, respectively, and is comparable to the performance of the state-of-the-art AMED Pulgin solver in eight steps, SDXL-turbo in four steps, and the state-of-the-art diffusion distillation model Flash Diffusion in five steps. We also validate our algorithm in synthesizing 1024 * 1024 images within 6 steps, whose FID performance only has a limited distance to the latest distillation algorithm. The code is in repo: https://github.com/TheLovesOfLadyPurple/Hyperparameters-are-all-you-need", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u9ad8\u6548\u6269\u6563\u6a21\u578b\u91c7\u6837\u7b97\u6cd5\uff0c\u80fd\u57288\u6b65\u5185\u751f\u6210\u9ad8\u8d28\u91cf512\u00d7512\u548c1024\u00d71024\u56fe\u50cf\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709ODE\u6c42\u89e3\u5668\u3002", "motivation": "\u57fa\u4e8e\u6269\u6563ODE\u548cSDE\u7684\u622a\u65ad\u8bef\u5dee\u5206\u6790\uff0c\u65e8\u5728\u5f00\u53d1\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u9ad8\u6548\u91c7\u6837\u7b97\u6cd5\uff0c\u5b9e\u73b0\u5feb\u901f\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6269\u6563ODE\u548cSDE\u7684\u622a\u65ad\u8bef\u5dee\uff0c\u8bbe\u8ba1\u8bad\u7ec3\u81ea\u7531\u7684\u91c7\u6837\u7b97\u6cd5\uff0c\u652f\u6301\u7075\u6d3b\u5f15\u5bfc\u5c3a\u5ea6\u3002", "result": "\u5728COCO\u548cLAION\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c8\u6b65\u751f\u6210512\u00d7512\u56fe\u50cf\u7684FID\u6027\u80fd\uff0815.7, 22.35, 17.52\uff09\u4f18\u4e8eDPM++ 2m\uff0820\u6b65\uff09\u548cAMED-plugin\uff1b1024\u00d71024\u56fe\u50cf\u751f\u6210\u57288\u6b65\u5185\u8fbe\u5230\u4e0e\u6700\u65b0\u84b8\u998f\u6a21\u578b\u76f8\u5f53\u7684FID\u6027\u80fd\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5c11\u6b65\u6570\u56fe\u50cf\u751f\u6210\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709ODE\u6c42\u89e3\u5668\uff0c\u4e14\u4e0e\u84b8\u998f\u6a21\u578b\u76f8\u5f53\u3002"}}
{"id": "2510.02884", "pdf": "https://arxiv.org/pdf/2510.02884", "abs": "https://arxiv.org/abs/2510.02884", "authors": ["Xinran Zhang", "Hanqi Zhu", "Yifan Duan", "Yanyong Zhang"], "title": "GS-Share: Enabling High-fidelity Map Sharing with Incremental Gaussian Splatting", "categories": ["cs.GR"], "comment": "11 pages, 11 figures", "summary": "Constructing and sharing 3D maps is essential for many applications, including autonomous driving and augmented reality. Recently, 3D Gaussian splatting has emerged as a promising approach for accurate 3D reconstruction. However, a practical map-sharing system that features high-fidelity, continuous updates, and network efficiency remains elusive. To address these challenges, we introduce GS-Share, a photorealistic map-sharing system with a compact representation. The core of GS-Share includes anchor-based global map construction, virtual-image-based map enhancement, and incremental map update. We evaluate GS-Share against state-of-the-art methods, demonstrating that our system achieves higher fidelity, particularly for extrapolated views, with improvements of 11%, 22%, and 74% in PSNR, LPIPS, and Depth L1, respectively. Furthermore, GS-Share is significantly more compact, reducing map transmission overhead by 36%.", "AI": {"tldr": "GS-Share\u662f\u4e00\u4e2a\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u9ad8\u4fdd\u771f\u5730\u56fe\u5171\u4eab\u7cfb\u7edf\uff0c\u901a\u8fc7\u7d27\u51d1\u8868\u793a\u5b9e\u73b0\u8fde\u7eed\u66f4\u65b0\u548c\u7f51\u7edc\u6548\u7387\uff0c\u5728\u4fdd\u771f\u5ea6\u548c\u4f20\u8f93\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u67093D\u5730\u56fe\u5171\u4eab\u7cfb\u7edf\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u3001\u8fde\u7eed\u66f4\u65b0\u548c\u7f51\u7edc\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u589e\u5f3a\u73b0\u5b9e\u7b49\u5e94\u7528\u4e2d\u9700\u8981\u9ad8\u6548\u5171\u4eab\u548c\u66f4\u65b03D\u5730\u56fe\u3002", "method": "\u91c7\u7528\u951a\u70b9\u57fa\u7840\u7684\u5168\u5c40\u5730\u56fe\u6784\u5efa\u3001\u57fa\u4e8e\u865a\u62df\u56fe\u50cf\u7684\u5730\u56fe\u589e\u5f3a\u548c\u589e\u91cf\u5730\u56fe\u66f4\u65b0\u7b49\u6838\u5fc3\u6280\u672f\uff0c\u6784\u5efa\u7d27\u51d1\u76843D\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u3002", "result": "\u5728PSNR\u3001LPIPS\u548cDepth L1\u6307\u6807\u4e0a\u5206\u522b\u63d0\u534711%\u300122%\u548c74%\uff0c\u5730\u56fe\u4f20\u8f93\u5f00\u9500\u51cf\u5c1136%\uff0c\u7279\u522b\u5728\u5916\u63a8\u89c6\u56fe\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "GS-Share\u6210\u529f\u89e3\u51b3\u4e863D\u5730\u56fe\u5171\u4eab\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u3001\u7d27\u51d1\u4e14\u652f\u6301\u8fde\u7eed\u66f4\u65b0\u7684\u5b9e\u7528\u5730\u56fe\u5171\u4eab\u7cfb\u7edf\u3002"}}
{"id": "2510.02691", "pdf": "https://arxiv.org/pdf/2510.02691", "abs": "https://arxiv.org/abs/2510.02691", "authors": ["Yibin Zhao", "Yihan Pan", "Jun Nan", "Jianjun Yi"], "title": "FSFSplatter: Build Surface and Novel Views with Sparse-Views within 3min", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Gaussian Splatting has become a leading reconstruction technique, known for its high-quality novel view synthesis and detailed reconstruction. However, most existing methods require dense, calibrated views. Reconstructing from free sparse images often leads to poor surface due to limited overlap and overfitting. We introduce FSFSplatter, a new approach for fast surface reconstruction from free sparse images. Our method integrates end-to-end dense Gaussian initialization, camera parameter estimation, and geometry-enhanced scene optimization. Specifically, FSFSplatter employs a large Transformer to encode multi-view images and generates a dense and geometrically consistent Gaussian scene initialization via a self-splitting Gaussian head. It eliminates local floaters through contribution-based pruning and mitigates overfitting to limited views by leveraging depth and multi-view feature supervision with differentiable camera parameters during rapid optimization. FSFSplatter outperforms current state-of-the-art methods on widely used DTU and Replica.", "AI": {"tldr": "FSFSplatter\u662f\u4e00\u79cd\u4ece\u81ea\u7531\u7a00\u758f\u56fe\u50cf\u8fdb\u884c\u5feb\u901f\u8868\u9762\u91cd\u5efa\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u5bc6\u96c6\u9ad8\u65af\u521d\u59cb\u5316\u3001\u76f8\u673a\u53c2\u6570\u4f30\u8ba1\u548c\u51e0\u4f55\u589e\u5f3a\u573a\u666f\u4f18\u5316\u6765\u89e3\u51b3\u4f20\u7edf\u9ad8\u65af\u6e85\u5c04\u65b9\u6cd5\u9700\u8981\u5bc6\u96c6\u6821\u51c6\u89c6\u56fe\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u9ad8\u65af\u6e85\u5c04\u65b9\u6cd5\u5927\u591a\u9700\u8981\u5bc6\u96c6\u6821\u51c6\u89c6\u56fe\uff0c\u4ece\u81ea\u7531\u7a00\u758f\u56fe\u50cf\u91cd\u5efa\u65f6\u7531\u4e8e\u91cd\u53e0\u6709\u9650\u548c\u8fc7\u62df\u5408\u4f1a\u5bfc\u81f4\u8868\u9762\u8d28\u91cf\u5dee\u3002", "method": "\u4f7f\u7528\u5927\u578bTransformer\u7f16\u7801\u591a\u89c6\u56fe\u56fe\u50cf\uff0c\u901a\u8fc7\u81ea\u5206\u88c2\u9ad8\u65af\u5934\u751f\u6210\u5bc6\u96c6\u51e0\u4f55\u4e00\u81f4\u7684\u9ad8\u65af\u573a\u666f\u521d\u59cb\u5316\uff0c\u91c7\u7528\u8d21\u732e\u5ea6\u526a\u679d\u6d88\u9664\u5c40\u90e8\u6f02\u6d6e\u7269\uff0c\u5229\u7528\u6df1\u5ea6\u548c\u591a\u89c6\u56fe\u7279\u5f81\u76d1\u7763\u7ed3\u5408\u53ef\u5fae\u5206\u76f8\u673a\u53c2\u6570\u5728\u5feb\u901f\u4f18\u5316\u4e2d\u7f13\u89e3\u8fc7\u62df\u5408\u3002", "result": "FSFSplatter\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684DTU\u548cReplica\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4ece\u81ea\u7531\u7a00\u758f\u56fe\u50cf\u8fdb\u884c\u9ad8\u8d28\u91cf\u8868\u9762\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03163", "pdf": "https://arxiv.org/pdf/2510.03163", "abs": "https://arxiv.org/abs/2510.03163", "authors": ["Jiapeng Tang", "Matthew Lavine", "Dor Verbin", "Stephan J. Garbin", "Matthias Nie\u00dfner", "Ricardo Martin Brualla", "Pratul P. Srinivasan", "Philipp Henzler"], "title": "ROGR: Relightable 3D Objects using Generative Relighting", "categories": ["cs.CV", "cs.GR"], "comment": "NeurIPS 2025 Spotlight. Project page:   https://tangjiapeng.github.io/ROGR", "summary": "We introduce ROGR, a novel approach that reconstructs a relightable 3D model of an object captured from multiple views, driven by a generative relighting model that simulates the effects of placing the object under novel environment illuminations. Our method samples the appearance of the object under multiple lighting environments, creating a dataset that is used to train a lighting-conditioned Neural Radiance Field (NeRF) that outputs the object's appearance under any input environmental lighting. The lighting-conditioned NeRF uses a novel dual-branch architecture to encode the general lighting effects and specularities separately. The optimized lighting-conditioned NeRF enables efficient feed-forward relighting under arbitrary environment maps without requiring per-illumination optimization or light transport simulation. We evaluate our approach on the established TensoIR and Stanford-ORB datasets, where it improves upon the state-of-the-art on most metrics, and showcase our approach on real-world object captures.", "AI": {"tldr": "ROGR\u662f\u4e00\u79cd\u65b0\u9896\u7684\u53ef\u91cd\u5149\u71673D\u91cd\u5efa\u65b9\u6cd5\uff0c\u4f7f\u7528\u751f\u6210\u5f0f\u91cd\u5149\u7167\u6a21\u578b\u4ece\u591a\u89c6\u89d2\u56fe\u50cf\u91cd\u5efa\u7269\u4f53\u6a21\u578b\uff0c\u80fd\u591f\u5728\u4efb\u610f\u73af\u5883\u5149\u7167\u4e0b\u8fdb\u884c\u9ad8\u6548\u524d\u5411\u91cd\u5149\u7167\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u4efb\u610f\u73af\u5883\u5149\u7167\u4e0b\u8fdb\u884c\u91cd\u5149\u7167\u65f6\u9700\u8981\u6bcf\u6b21\u4f18\u5316\u6216\u5149\u7ebf\u4f20\u8f93\u6a21\u62df\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u524d\u5411\u91cd\u5149\u7167\u3002", "method": "\u4f7f\u7528\u751f\u6210\u5f0f\u91cd\u5149\u7167\u6a21\u578b\u91c7\u6837\u7269\u4f53\u5728\u591a\u79cd\u5149\u7167\u73af\u5883\u4e0b\u7684\u5916\u89c2\uff0c\u8bad\u7ec3\u5149\u7167\u6761\u4ef6\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\uff0c\u91c7\u7528\u53cc\u5206\u652f\u67b6\u6784\u5206\u522b\u7f16\u7801\u4e00\u822c\u5149\u7167\u6548\u679c\u548c\u955c\u9762\u53cd\u5c04\u3002", "result": "\u5728TensoIR\u548cStanford-ORB\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u5728\u5927\u591a\u6570\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u5728\u771f\u5b9e\u7269\u4f53\u6355\u6349\u4e0a\u5c55\u793a\u4e86\u826f\u597d\u6548\u679c\u3002", "conclusion": "ROGR\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u91cd\u5efa\u53ef\u91cd\u5149\u7167\u76843D\u6a21\u578b\uff0c\u5728\u4efb\u610f\u73af\u5883\u5149\u7167\u4e0b\u5b9e\u73b0\u524d\u5411\u91cd\u5149\u7167\uff0c\u65e0\u9700\u6bcf\u6b21\u4f18\u5316\u6216\u5149\u7ebf\u4f20\u8f93\u6a21\u62df\u3002"}}
{"id": "2510.02571", "pdf": "https://arxiv.org/pdf/2510.02571", "abs": "https://arxiv.org/abs/2510.02571", "authors": ["Zhiting Mei", "Ola Shorinwa", "Anirudha Majumdar"], "title": "How Confident are Video Models? Empowering Video Models to Express their Uncertainty", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Generative video models demonstrate impressive text-to-video capabilities, spurring widespread adoption in many real-world applications. However, like large language models (LLMs), video generation models tend to hallucinate, producing plausible videos even when they are factually wrong. Although uncertainty quantification (UQ) of LLMs has been extensively studied in prior work, no UQ method for video models exists, raising critical safety concerns. To our knowledge, this paper represents the first work towards quantifying the uncertainty of video models. We present a framework for uncertainty quantification of generative video models, consisting of: (i) a metric for evaluating the calibration of video models based on robust rank correlation estimation with no stringent modeling assumptions; (ii) a black-box UQ method for video models (termed S-QUBED), which leverages latent modeling to rigorously decompose predictive uncertainty into its aleatoric and epistemic components; and (iii) a UQ dataset to facilitate benchmarking calibration in video models. By conditioning the generation task in the latent space, we disentangle uncertainty arising due to vague task specifications from that arising from lack of knowledge. Through extensive experiments on benchmark video datasets, we demonstrate that S-QUBED computes calibrated total uncertainty estimates that are negatively correlated with the task accuracy and effectively computes the aleatoric and epistemic constituents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u751f\u6210\u89c6\u9891\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u6846\u67b6S-QUBED\uff0c\u5305\u542b\u8bc4\u4f30\u6307\u6807\u3001\u9ed1\u76d2UQ\u65b9\u6cd5\u548c\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u80fd\u5206\u89e3\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u4e3a\u5076\u7136\u6027\u548c\u8ba4\u77e5\u6027\u6210\u5206\u3002", "motivation": "\u751f\u6210\u89c6\u9891\u6a21\u578b\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u4f1a\u4ea7\u751f\u770b\u4f3c\u5408\u7406\u4f46\u4e8b\u5b9e\u9519\u8bef\u7684\u89c6\u9891\uff0c\u800c\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u89c6\u9891\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u5b58\u5728\u5b89\u5168\u9690\u60a3\u3002", "method": "\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u5efa\u6a21\uff0c\u5c06\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u5206\u89e3\u4e3a\u5076\u7136\u6027\uff08\u4efb\u52a1\u6a21\u7cca\u6027\uff09\u548c\u8ba4\u77e5\u6027\uff08\u77e5\u8bc6\u7f3a\u4e4f\uff09\u6210\u5206\uff0c\u63d0\u51fa\u57fa\u4e8e\u9c81\u68d2\u79e9\u76f8\u5173\u4f30\u8ba1\u7684\u6821\u51c6\u8bc4\u4f30\u6307\u6807\u548c\u9ed1\u76d2UQ\u65b9\u6cd5S-QUBED\u3002", "result": "\u5728\u57fa\u51c6\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cS-QUBED\u80fd\u8ba1\u7b97\u4e0e\u4efb\u52a1\u51c6\u786e\u5ea6\u8d1f\u76f8\u5173\u7684\u6821\u51c6\u603b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5e76\u6709\u6548\u5206\u89e3\u4e0d\u786e\u5b9a\u6027\u7684\u4e24\u4e2a\u6210\u5206\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u751f\u6210\u89c6\u9891\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5de5\u4f5c\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u53ef\u9760\u5730\u8bc4\u4f30\u548c\u5206\u89e3\u89c6\u9891\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u89c6\u9891\u6a21\u578b\u7684\u5b89\u5168\u5e94\u7528\u63d0\u4f9b\u91cd\u8981\u4fdd\u969c\u3002"}}
{"id": "2510.02599", "pdf": "https://arxiv.org/pdf/2510.02599", "abs": "https://arxiv.org/abs/2510.02599", "authors": ["Hovhannes Margaryan", "Bo Wan", "Tinne Tuytelaars"], "title": "PEO: Training-Free Aesthetic Quality Enhancement in Pre-Trained Text-to-Image Diffusion Models with Prompt Embedding Optimization", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces a novel approach to aesthetic quality improvement in pre-trained text-to-image diffusion models when given a simple prompt. Our method, dubbed Prompt Embedding Optimization (PEO), leverages a pre-trained text-to-image diffusion model as a backbone and optimizes the text embedding of a given simple and uncurated prompt to enhance the visual quality of the generated image. We achieve this by a tripartite objective function that improves the aesthetic fidelity of the generated image, ensures adherence to the optimized text embedding, and minimal divergence from the initial prompt. The latter is accomplished through a prompt preservation term. Additionally, PEO is training-free and backbone-independent. Quantitative and qualitative evaluations confirm the effectiveness of the proposed method, exceeding or equating the performance of state-of-the-art text-to-image and prompt adaptation methods.", "AI": {"tldr": "\u63d0\u51faPrompt Embedding Optimization (PEO)\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6587\u672c\u5d4c\u5165\u6765\u63d0\u5347\u9884\u8bad\u7ec3\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\uff0c\u65e0\u9700\u8bad\u7ec3\u4e14\u4e0e\u4e3b\u5e72\u6a21\u578b\u65e0\u5173\u3002", "motivation": "\u89e3\u51b3\u9884\u8bad\u7ec3\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u7b80\u5355\u63d0\u793a\u8bcd\u4e0b\u751f\u6210\u56fe\u50cf\u7f8e\u5b66\u8d28\u91cf\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4e09\u91cd\u76ee\u6807\u51fd\u6570\u4f18\u5316\u6587\u672c\u5d4c\u5165\uff1a\u63d0\u5347\u751f\u6210\u56fe\u50cf\u7684\u7f8e\u5b66\u4fdd\u771f\u5ea6\u3001\u786e\u4fdd\u4e0e\u4f18\u5316\u6587\u672c\u5d4c\u5165\u7684\u4e00\u81f4\u6027\u3001\u901a\u8fc7\u63d0\u793a\u8bcd\u4fdd\u7559\u9879\u6700\u5c0f\u5316\u4e0e\u521d\u59cb\u63d0\u793a\u8bcd\u7684\u5dee\u5f02\u3002", "result": "\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u6027\u80fd\u8fbe\u5230\u6216\u8d85\u8fc7\u6700\u5148\u8fdb\u7684\u6587\u672c\u5230\u56fe\u50cf\u548c\u63d0\u793a\u8bcd\u9002\u5e94\u65b9\u6cd5\u3002", "conclusion": "PEO\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u4e3b\u5e72\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u63d0\u5347\u7b80\u5355\u63d0\u793a\u8bcd\u4e0b\u751f\u6210\u56fe\u50cf\u7684\u7f8e\u5b66\u8d28\u91cf\u3002"}}
{"id": "2510.02617", "pdf": "https://arxiv.org/pdf/2510.02617", "abs": "https://arxiv.org/abs/2510.02617", "authors": ["Beijia Lu", "Ziyi Chen", "Jing Xiao", "Jun-Yan Zhu"], "title": "Input-Aware Sparse Attention for Real-Time Co-Speech Video Generation", "categories": ["cs.CV"], "comment": "Project Page: https://beijia11.github.io/IASA", "summary": "Diffusion models can synthesize realistic co-speech video from audio for various applications, such as video creation and virtual agents. However, existing diffusion-based methods are slow due to numerous denoising steps and costly attention mechanisms, preventing real-time deployment. In this work, we distill a many-step diffusion video model into a few-step student model. Unfortunately, directly applying recent diffusion distillation methods degrades video quality and falls short of real-time performance. To address these issues, our new video distillation method leverages input human pose conditioning for both attention and loss functions. We first propose using accurate correspondence between input human pose keypoints to guide attention to relevant regions, such as the speaker's face, hands, and upper body. This input-aware sparse attention reduces redundant computations and strengthens temporal correspondences of body parts, improving inference efficiency and motion coherence. To further enhance visual quality, we introduce an input-aware distillation loss that improves lip synchronization and hand motion realism. By integrating our input-aware sparse attention and distillation loss, our method achieves real-time performance with improved visual quality compared to recent audio-driven and input-driven methods. We also conduct extensive experiments showing the effectiveness of our algorithmic design choices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f93\u5165\u611f\u77e5\u7a00\u758f\u6ce8\u610f\u529b\u548c\u84b8\u998f\u635f\u5931\u7684\u5b9e\u65f6\u8bed\u97f3\u9a71\u52a8\u89c6\u9891\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u591a\u6b65\u6269\u6563\u6a21\u578b\u84b8\u998f\u4e3a\u5c11\u6b65\u5b66\u751f\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6027\u80fd\u5e76\u63d0\u5347\u4e86\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u8bed\u97f3\u9a71\u52a8\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u7531\u4e8e\u9700\u8981\u5927\u91cf\u53bb\u566a\u6b65\u9aa4\u548c\u6602\u8d35\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901f\u5ea6\u7f13\u6162\uff0c\u65e0\u6cd5\u5b9e\u73b0\u5b9e\u65f6\u90e8\u7f72\u3002", "method": "1) \u4f7f\u7528\u8f93\u5165\u4eba\u4f53\u59ff\u6001\u5173\u952e\u70b9\u7684\u51c6\u786e\u5bf9\u5e94\u5173\u7cfb\u6765\u5f15\u5bfc\u6ce8\u610f\u529b\u5230\u76f8\u5173\u533a\u57df\uff1b2) \u63d0\u51fa\u8f93\u5165\u611f\u77e5\u7a00\u758f\u6ce8\u610f\u529b\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff1b3) \u5f15\u5165\u8f93\u5165\u611f\u77e5\u84b8\u998f\u635f\u5931\u63d0\u5347\u5507\u90e8\u540c\u6b65\u548c\u624b\u90e8\u8fd0\u52a8\u771f\u5b9e\u6027\u3002", "result": "\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6027\u80fd\uff0c\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u4f18\u4e8e\u6700\u8fd1\u7684\u97f3\u9891\u9a71\u52a8\u548c\u8f93\u5165\u9a71\u52a8\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u8f93\u5165\u611f\u77e5\u7a00\u758f\u6ce8\u610f\u529b\u548c\u84b8\u998f\u635f\u5931\uff0c\u6210\u529f\u5c06\u591a\u6b65\u6269\u6563\u6a21\u578b\u84b8\u998f\u4e3a\u5c11\u6b65\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5b9e\u65f6\u8bed\u97f3\u9a71\u52a8\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2510.02631", "pdf": "https://arxiv.org/pdf/2510.02631", "abs": "https://arxiv.org/abs/2510.02631", "authors": ["Victor Enescu", "Hichem Sahbi"], "title": "Deep Generative Continual Learning using Functional LoRA: FunLoRA", "categories": ["cs.CV"], "comment": null, "summary": "Continual adaptation of deep generative models holds tremendous potential and critical importance, given their rapid and expanding usage in text and vision based applications. Incremental training, however, remains highly challenging due to catastrophic forgetting phenomenon, which makes it difficult for neural networks to effectively incorporate new knowledge. A common strategy consists in retraining the generative model on its own synthetic data in order to mitigate forgetting. Yet, such an approach faces two major limitations: (i) the continually increasing training time eventually becomes intractable, and (ii) reliance on synthetic data inevitably leads to long-term performance degradation, since synthetic samples lack the richness of real training data. In this paper, we attenuate these issues by designing a novel and more expressive conditioning mechanism for generative models based on low rank adaptation (LoRA), that exclusively employs rank 1 matrices, whose reparametrized matrix rank is functionally increased using carefully selected functions -- and dubbed functional LoRA: FunLoRA. Using this dynamic conditioning, the generative model is guaranteed to avoid catastrophic forgetting and needs only to be trained on data from the current task. Extensive experiments using flow-matching based models trained from scratch, showcase that our proposed parameter-efficient fine-tuning (PEFT) method surpasses prior state-of-the-art results based on diffusion models, reaching higher classification accuracy scores, while only requiring a fraction of the memory cost and sampling time.", "AI": {"tldr": "\u63d0\u51faFunLoRA\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\u548c\u52a8\u6001\u6761\u4ef6\u673a\u5236\u89e3\u51b3\u751f\u6210\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4ec5\u9700\u5728\u5f53\u524d\u4efb\u52a1\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u6210\u672c\u548c\u91c7\u6837\u65f6\u95f4\u3002", "motivation": "\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u5728\u6301\u7eed\u9002\u5e94\u4e2d\u9762\u4e34\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4f20\u7edf\u57fa\u4e8e\u5408\u6210\u6570\u636e\u7684\u518d\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\u8bad\u7ec3\u65f6\u95f4\u4e0d\u53ef\u6301\u7eed\u548c\u6027\u80fd\u957f\u671f\u9000\u5316\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u4f4e\u79e9\u9002\u5e94(LoRA)\u7684\u529f\u80fd\u6027\u6761\u4ef6\u673a\u5236FunLoRA\uff0c\u4f7f\u7528\u79e91\u77e9\u9635\u5e76\u901a\u8fc7\u7cbe\u5fc3\u9009\u62e9\u7684\u51fd\u6570\u589e\u52a0\u91cd\u53c2\u6570\u5316\u77e9\u9635\u79e9\uff0c\u5b9e\u73b0\u52a8\u6001\u6761\u4ef6\u3002", "result": "\u5728\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u6a21\u578b\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cFunLoRA\u8d85\u8d8a\u4e86\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5148\u524d\u6700\u4f18\u7ed3\u679c\uff0c\u8fbe\u5230\u66f4\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4ec5\u9700\u5c11\u91cf\u5185\u5b58\u6210\u672c\u548c\u91c7\u6837\u65f6\u95f4\u3002", "conclusion": "FunLoRA\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u751f\u6210\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.02642", "pdf": "https://arxiv.org/pdf/2510.02642", "abs": "https://arxiv.org/abs/2510.02642", "authors": ["Abhishek Joshi", "Jahnavi Krishna Koda", "Abhishek Phadke"], "title": "Sequence-Preserving Dual-FoV Defense for Traffic Sign and Light Recognition in Autonomous Vehicles", "categories": ["cs.CV"], "comment": null, "summary": "Traffic light and sign recognition are key for Autonomous Vehicles (AVs) because perception mistakes directly influence navigation and safety. In addition to digital adversarial attacks, models are vulnerable to existing perturbations (glare, rain, dirt, or graffiti), which could lead to dangerous misclassifications. The current work lacks consideration of temporal continuity, multistatic field-of-view (FoV) sensing, and robustness to both digital and natural degradation. This study proposes a dual FoV, sequence-preserving robustness framework for traffic lights and signs in the USA based on a multi-source dataset built on aiMotive, Udacity, Waymo, and self-recorded videos from the region of Texas. Mid and long-term sequences of RGB images are temporally aligned for four operational design domains (ODDs): highway, night, rainy, and urban. Over a series of experiments on a real-life application of anomaly detection, this study outlines a unified three-layer defense stack framework that incorporates feature squeezing, defensive distillation, and entropy-based anomaly detection, as well as sequence-wise temporal voting for further enhancement. The evaluation measures included accuracy, attack success rate (ASR), risk-weighted misclassification severity, and confidence stability. Physical transferability was confirmed using probes for recapture. The results showed that the Unified Defense Stack achieved 79.8mAP and reduced the ASR to 18.2%, which is superior to YOLOv8, YOLOv9, and BEVFormer, while reducing the high-risk misclassification to 32%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u89c6\u573a\u3001\u5e8f\u5217\u4fdd\u6301\u7684\u9c81\u68d2\u6027\u6846\u67b6\uff0c\u7528\u4e8e\u7f8e\u56fd\u4ea4\u901a\u706f\u548c\u6807\u5fd7\u8bc6\u522b\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u4e09\u5c42\u9632\u5fa1\u5806\u6808\uff08\u7279\u5f81\u538b\u7f29\u3001\u9632\u5fa1\u84b8\u998f\u3001\u57fa\u4e8e\u71b5\u7684\u5f02\u5e38\u68c0\u6d4b\uff09\u7ed3\u5408\u65f6\u5e8f\u6295\u7968\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u6570\u5b57\u548c\u81ea\u7136\u6270\u52a8\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e2d\u4ea4\u901a\u706f\u548c\u6807\u5fd7\u8bc6\u522b\u9519\u8bef\u76f4\u63a5\u5f71\u54cd\u5bfc\u822a\u548c\u5b89\u5168\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u65f6\u95f4\u8fde\u7eed\u6027\u3001\u591a\u9759\u6001\u89c6\u573a\u611f\u77e5\u4ee5\u53ca\u6570\u5b57\u548c\u81ea\u7136\u9000\u5316\u9c81\u68d2\u6027\u7684\u8003\u8651\u3002", "method": "\u57fa\u4e8e\u591a\u6e90\u6570\u636e\u96c6\u6784\u5efa\u53cc\u89c6\u573a\u5e8f\u5217\u4fdd\u6301\u6846\u67b6\uff0c\u91c7\u7528\u7edf\u4e00\u4e09\u5c42\u9632\u5fa1\u5806\u6808\uff08\u7279\u5f81\u538b\u7f29\u3001\u9632\u5fa1\u84b8\u998f\u3001\u57fa\u4e8e\u71b5\u7684\u5f02\u5e38\u68c0\u6d4b\uff09\u548c\u5e8f\u5217\u65f6\u5e8f\u6295\u7968\u673a\u5236\uff0c\u5728\u56db\u79cd\u64cd\u4f5c\u8bbe\u8ba1\u57df\u4e0b\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u7edf\u4e00\u9632\u5fa1\u5806\u6808\u8fbe\u523079.8mAP\uff0c\u5c06\u653b\u51fb\u6210\u529f\u7387\u964d\u81f318.2%\uff0c\u4f18\u4e8eYOLOv8\u3001YOLOv9\u548cBEVFormer\uff0c\u540c\u65f6\u5c06\u9ad8\u98ce\u9669\u8bef\u5206\u7c7b\u964d\u81f332%\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u771f\u5b9e\u573a\u666f\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u901a\u8fc7\u7269\u7406\u53ef\u8f6c\u79fb\u6027\u9a8c\u8bc1\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u9c81\u68d2\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.02654", "pdf": "https://arxiv.org/pdf/2510.02654", "abs": "https://arxiv.org/abs/2510.02654", "authors": ["Benjamin Yu", "Jackie Liu", "Justin Cui"], "title": "Smart-GRPO: Smartly Sampling Noise for Efficient RL of Flow-Matching Models", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in flow-matching have enabled high-quality text-to-image generation. However, the deterministic nature of flow-matching models makes them poorly suited for reinforcement learning, a key tool for improving image quality and human alignment. Prior work has introduced stochasticity by perturbing latents with random noise, but such perturbations are inefficient and unstable. We propose Smart-GRPO, the first method to optimize noise perturbations for reinforcement learning in flow-matching models. Smart-GRPO employs an iterative search strategy that decodes candidate perturbations, evaluates them with a reward function, and refines the noise distribution toward higher-reward regions. Experiments demonstrate that Smart-GRPO improves both reward optimization and visual quality compared to baseline methods. Our results suggest a practical path toward reinforcement learning in flow-matching frameworks, bridging the gap between efficient training and human-aligned generation.", "AI": {"tldr": "Smart-GRPO\u662f\u9996\u4e2a\u4f18\u5316\u6d41\u5339\u914d\u6a21\u578b\u4e2d\u5f3a\u5316\u5b66\u4e60\u566a\u58f0\u6270\u52a8\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u641c\u7d22\u7b56\u7565\u6539\u8fdb\u566a\u58f0\u5206\u5e03\uff0c\u63d0\u9ad8\u5956\u52b1\u4f18\u5316\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u6d41\u5339\u914d\u6a21\u578b\u867d\u7136\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u6587\u672c\u5230\u56fe\u50cf\uff0c\u4f46\u5176\u786e\u5b9a\u6027\u7279\u6027\u4e0d\u9002\u5408\u5f3a\u5316\u5b66\u4e60\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u968f\u673a\u566a\u58f0\u6270\u52a8\u5f15\u5165\u968f\u673a\u6027\uff0c\u4f46\u6548\u7387\u4f4e\u4e14\u4e0d\u7a33\u5b9a\u3002", "method": "\u91c7\u7528\u8fed\u4ee3\u641c\u7d22\u7b56\u7565\uff1a\u89e3\u7801\u5019\u9009\u6270\u52a8\uff0c\u7528\u5956\u52b1\u51fd\u6570\u8bc4\u4f30\uff0c\u7136\u540e\u5411\u9ad8\u5956\u52b1\u533a\u57df\u4f18\u5316\u566a\u58f0\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSmart-GRPO\u5728\u5956\u52b1\u4f18\u5316\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u4e3a\u6d41\u5339\u914d\u6846\u67b6\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\uff0c\u5f25\u5408\u4e86\u9ad8\u6548\u8bad\u7ec3\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u751f\u6210\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2510.02732", "pdf": "https://arxiv.org/pdf/2510.02732", "abs": "https://arxiv.org/abs/2510.02732", "authors": ["Jianing Chen", "Zehao Li", "Yujun Cai", "Hao Jiang", "Shuqin Gao", "Honglong Zhao", "Tianlu Mao", "Yucheng Zhang"], "title": "From Tokens to Nodes: Semantic-Guided Motion Control for Dynamic 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Dynamic 3D reconstruction from monocular videos remains difficult due to the ambiguity inferring 3D motion from limited views and computational demands of modeling temporally varying scenes. While recent sparse control methods alleviate computation by reducing millions of Gaussians to thousands of control points, they suffer from a critical limitation: they allocate points purely by geometry, leading to static redundancy and dynamic insufficiency. We propose a motion-adaptive framework that aligns control density with motion complexity. Leveraging semantic and motion priors from vision foundation models, we establish patch-token-node correspondences and apply motion-adaptive compression to concentrate control points in dynamic regions while suppressing redundancy in static backgrounds. Our approach achieves flexible representational density adaptation through iterative voxelization and motion tendency scoring, directly addressing the fundamental mismatch between control point allocation and motion complexity. To capture temporal evolution, we introduce spline-based trajectory parameterization initialized by 2D tracklets, replacing MLP-based deformation fields to achieve smoother motion representation and more stable optimization. Extensive experiments demonstrate significant improvements in reconstruction quality and efficiency over existing state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd0\u52a8\u81ea\u9002\u5e94\u76843D\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u548c\u8fd0\u52a8\u5148\u9a8c\u5c06\u63a7\u5236\u70b9\u5bc6\u5ea6\u4e0e\u8fd0\u52a8\u590d\u6742\u5ea6\u5bf9\u9f50\uff0c\u5728\u52a8\u6001\u533a\u57df\u96c6\u4e2d\u63a7\u5236\u70b9\uff0c\u6291\u5236\u9759\u6001\u80cc\u666f\u5197\u4f59\uff0c\u5e76\u4f7f\u7528\u6837\u6761\u8f68\u8ff9\u53c2\u6570\u5316\u5b9e\u73b0\u66f4\u5e73\u6ed1\u7684\u8fd0\u52a8\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u63a7\u5236\u65b9\u6cd5\u4ec5\u57fa\u4e8e\u51e0\u4f55\u5206\u914d\u63a7\u5236\u70b9\uff0c\u5bfc\u81f4\u9759\u6001\u533a\u57df\u5197\u4f59\u548c\u52a8\u6001\u533a\u57df\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u74063D\u8fd0\u52a8\u91cd\u5efa\u4e2d\u7684\u8ba1\u7b97\u9700\u6c42\u548c\u89c6\u56fe\u9650\u5236\u95ee\u9898\u3002", "method": "\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u8bed\u4e49\u548c\u8fd0\u52a8\u5148\u9a8c\u5efa\u7acb\u8865\u4e01-\u6807\u8bb0-\u8282\u70b9\u5bf9\u5e94\u5173\u7cfb\uff0c\u901a\u8fc7\u8fd0\u52a8\u81ea\u9002\u5e94\u538b\u7f29\u548c\u8fed\u4ee3\u4f53\u7d20\u5316\u5b9e\u73b0\u63a7\u5236\u70b9\u5bc6\u5ea6\u9002\u5e94\uff0c\u91c7\u7528\u57fa\u4e8e2D\u8f68\u8ff9\u521d\u59cb\u5316\u7684\u6837\u6761\u8f68\u8ff9\u53c2\u6570\u5316\u66ff\u4ee3MLP\u53d8\u5f62\u573a\u3002", "result": "\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u6548\u7387\u65b9\u9762\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\uff0c\u5b9e\u73b0\u4e86\u66f4\u5e73\u6ed1\u7684\u8fd0\u52a8\u8868\u793a\u548c\u66f4\u7a33\u5b9a\u7684\u4f18\u5316\u3002", "conclusion": "\u63d0\u51fa\u7684\u8fd0\u52a8\u81ea\u9002\u5e94\u6846\u67b6\u901a\u8fc7\u5c06\u63a7\u5236\u70b9\u5206\u914d\u4e0e\u8fd0\u52a8\u590d\u6742\u5ea6\u5bf9\u9f50\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u60013D\u91cd\u5efa\u4e2d\u7684\u5173\u952e\u9650\u5236\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u91cd\u5efa\u8d28\u91cf\u4e0a\u90fd\u53d6\u5f97\u4e86\u7a81\u7834\u3002"}}
{"id": "2510.02733", "pdf": "https://arxiv.org/pdf/2510.02733", "abs": "https://arxiv.org/abs/2510.02733", "authors": ["Weimin Yuan", "Cai Meng"], "title": "Net2Net: When Un-trained Meets Pre-trained Networks for Robust Real-World Denoising", "categories": ["cs.CV"], "comment": null, "summary": "Traditional denoising methods for noise removal have largely relied on handcrafted priors, often perform well in controlled environments but struggle to address the complexity and variability of real noise. In contrast, deep learning-based approaches have gained prominence for learning noise characteristics from large datasets, but these methods frequently require extensive labeled data and may not generalize effectively across diverse noise types and imaging conditions. In this paper, we present an innovative method, termed as Net2Net, that combines the strengths of untrained and pre-trained networks to tackle the challenges of real-world noise removal. The innovation of Net2Net lies in its combination of unsupervised DIP and supervised pre-trained model DRUNet by regularization by denoising (RED). The untrained network adapts to the unique noise characteristics of each input image without requiring labeled data, while the pre-trained network leverages learned representations from large-scale datasets to deliver robust denoising performance. This hybrid framework enhances generalization across varying noise patterns and improves performance, particularly in scenarios with limited training data. Extensive experiments on benchmark datasets demonstrate the superiority of our method for real-world noise removal.", "AI": {"tldr": "Net2Net\u662f\u4e00\u79cd\u7ed3\u5408\u65e0\u76d1\u7763DIP\u548c\u9884\u8bad\u7ec3DRUNet\u7f51\u7edc\u7684\u6df7\u5408\u53bb\u566a\u65b9\u6cd5\uff0c\u901a\u8fc7RED\u6b63\u5219\u5316\u5b9e\u73b0\uff0c\u65e0\u9700\u6807\u8bb0\u6570\u636e\u5373\u53ef\u9002\u5e94\u5404\u79cd\u771f\u5b9e\u566a\u58f0\u6a21\u5f0f\u3002", "motivation": "\u4f20\u7edf\u53bb\u566a\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u5148\u9a8c\uff0c\u5728\u590d\u6742\u771f\u5b9e\u566a\u58f0\u4e2d\u8868\u73b0\u4e0d\u4f73\uff1b\u6df1\u5ea6\u5b66\u4e60\u9700\u8981\u5927\u91cf\u6807\u8bb0\u6570\u636e\u4e14\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u9700\u8981\u4e00\u79cd\u80fd\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u65b9\u6cd5\u6765\u5904\u7406\u771f\u5b9e\u4e16\u754c\u566a\u58f0\u3002", "method": "\u7ed3\u5408\u65e0\u76d1\u7763DIP\u7f51\u7edc\u548c\u9884\u8bad\u7ec3DRUNet\u7f51\u7edc\uff0c\u901a\u8fc7RED\u6b63\u5219\u5316\u6784\u5efa\u6df7\u5408\u6846\u67b6\u3002\u65e0\u76d1\u7763\u7f51\u7edc\u9002\u5e94\u8f93\u5165\u56fe\u50cf\u7684\u72ec\u7279\u566a\u58f0\u7279\u5f81\uff0c\u9884\u8bad\u7ec3\u7f51\u7edc\u5229\u7528\u5927\u89c4\u6a21\u6570\u636e\u96c6\u5b66\u4e60\u5230\u7684\u8868\u793a\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u566a\u58f0\u53bb\u9664\u65b9\u9762\u5177\u6709\u4f18\u8d8a\u6027\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "Net2Net\u901a\u8fc7\u7ed3\u5408\u65e0\u76d1\u7763\u548c\u9884\u8bad\u7ec3\u7f51\u7edc\u7684\u6df7\u5408\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u771f\u5b9e\u4e16\u754c\u566a\u58f0\u53bb\u9664\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u8de8\u4e0d\u540c\u566a\u58f0\u6a21\u5f0f\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.02815", "pdf": "https://arxiv.org/pdf/2510.02815", "abs": "https://arxiv.org/abs/2510.02815", "authors": ["Feng Yuan", "Yifan Gao", "Yuehua Ye", "Haoyue Li", "Xin Gao"], "title": "Med-K2N: Flexible K-to-N Modality Translation for Medical Image Synthesis", "categories": ["cs.CV"], "comment": "ICLR2026 under review", "summary": "Cross-modal medical image synthesis research focuses on reconstructing missing imaging modalities from available ones to support clinical diagnosis. Driven by clinical necessities for flexible modality reconstruction, we explore K to N medical generation, where three critical challenges emerge: How can we model the heterogeneous contributions of different modalities to various target tasks? How can we ensure fusion quality control to prevent degradation from noisy information? How can we maintain modality identity consistency in multi-output generation? Driven by these clinical necessities, and drawing inspiration from SAM2's sequential frame paradigm and clinicians' progressive workflow of incrementally adding and selectively integrating multi-modal information, we treat multi-modal medical data as sequential frames with quality-driven selection mechanisms. Our key idea is to \"learn\" adaptive weights for each modality-task pair and \"memorize\" beneficial fusion patterns through progressive enhancement. To achieve this, we design three collaborative modules: PreWeightNet for global contribution assessment, ThresholdNet for adaptive filtering, and EffiWeightNet for effective weight computation. Meanwhile, to maintain modality identity consistency, we propose the Causal Modality Identity Module (CMIM) that establishes causal constraints between generated images and target modality descriptions using vision-language modeling. Extensive experimental results demonstrate that our proposed Med-K2N outperforms state-of-the-art methods by significant margins on multiple benchmarks. Source code is available.", "AI": {"tldr": "\u63d0\u51faMed-K2N\u65b9\u6cd5\u89e3\u51b3K\u5230N\u533b\u5b66\u56fe\u50cf\u5408\u6210\u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u5f02\u8d28\u6a21\u6001\u8d21\u732e\u5efa\u6a21\u3001\u878d\u5408\u8d28\u91cf\u63a7\u5236\u3001\u591a\u8f93\u51fa\u6a21\u6001\u4e00\u81f4\u6027\u4fdd\u6301\uff0c\u901a\u8fc7\u4e09\u4e2a\u534f\u4f5c\u6a21\u5757\u548c\u56e0\u679c\u6a21\u6001\u8eab\u4efd\u6a21\u5757\u5b9e\u73b0\u81ea\u9002\u5e94\u6743\u91cd\u5b66\u4e60\u548c\u6e10\u8fdb\u589e\u5f3a\u3002", "motivation": "\u4e34\u5e8a\u8bca\u65ad\u9700\u8981\u7075\u6d3b\u91cd\u5efa\u7f3a\u5931\u7684\u6210\u50cf\u6a21\u6001\uff0c\u4f46\u9762\u4e34\u5f02\u8d28\u6a21\u6001\u8d21\u732e\u5efa\u6a21\u3001\u878d\u5408\u8d28\u91cf\u63a7\u5236\u3001\u591a\u8f93\u51fa\u6a21\u6001\u4e00\u81f4\u6027\u4fdd\u6301\u4e09\u5927\u6311\u6218\u3002", "method": "\u5c06\u591a\u6a21\u6001\u533b\u5b66\u6570\u636e\u89c6\u4e3a\u5177\u6709\u8d28\u91cf\u9a71\u52a8\u9009\u62e9\u673a\u5236\u7684\u5e8f\u5217\u5e27\uff0c\u8bbe\u8ba1PreWeightNet\u3001ThresholdNet\u3001EffiWeightNet\u4e09\u4e2a\u534f\u4f5c\u6a21\u5757\u8fdb\u884c\u81ea\u9002\u5e94\u6743\u91cd\u5b66\u4e60\u548c\u6e10\u8fdb\u589e\u5f3a\uff0c\u5e76\u63d0\u51fa\u56e0\u679c\u6a21\u6001\u8eab\u4efd\u6a21\u5757(CMIM)\u4fdd\u6301\u6a21\u6001\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "Med-K2N\u901a\u8fc7\u81ea\u9002\u5e94\u6743\u91cd\u5b66\u4e60\u548c\u56e0\u679c\u7ea6\u675f\u6709\u6548\u89e3\u51b3\u4e86K\u5230N\u533b\u5b66\u56fe\u50cf\u5408\u6210\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u4e86\u7075\u6d3b\u53ef\u9760\u7684\u6a21\u6001\u91cd\u5efa\u80fd\u529b\u3002"}}
{"id": "2510.02922", "pdf": "https://arxiv.org/pdf/2510.02922", "abs": "https://arxiv.org/abs/2510.02922", "authors": ["Daphne Tsolissou", "Theofanis Ganitidis", "Konstantinos Mitsis", "Stergios CHristodoulidis", "Maria Vakalopoulou", "Konstantina Nikita"], "title": "Multimodal Carotid Risk Stratification with Large Vision-Language Models: Benchmarking, Fine-Tuning, and Clinical Insights", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Reliable risk assessment for carotid atheromatous disease remains a major clinical challenge, as it requires integrating diverse clinical and imaging information in a manner that is transparent and interpretable to clinicians. This study investigates the potential of state-of-the-art and recent large vision-language models (LVLMs) for multimodal carotid plaque assessment by integrating ultrasound imaging (USI) with structured clinical, demographic, laboratory, and protein biomarker data. A framework that simulates realistic diagnostic scenarios through interview-style question sequences is proposed, comparing a range of open-source LVLMs, including both general-purpose and medically tuned models. Zero-shot experiments reveal that even if they are very powerful, not all LVLMs can accurately identify imaging modality and anatomy, while all of them perform poorly in accurate risk classification. To address this limitation, LLaVa-NeXT-Vicuna is adapted to the ultrasound domain using low-rank adaptation (LoRA), resulting in substantial improvements in stroke risk stratification. The integration of multimodal tabular data in the form of text further enhances specificity and balanced accuracy, yielding competitive performance compared to prior convolutional neural network (CNN) baselines trained on the same dataset. Our findings highlight both the promise and limitations of LVLMs in ultrasound-based cardiovascular risk prediction, underscoring the importance of multimodal integration, model calibration, and domain adaptation for clinical translation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6574\u5408\u8d85\u58f0\u6210\u50cf\u548c\u4e34\u5e8a\u6570\u636e\u7528\u4e8e\u9888\u52a8\u8109\u6591\u5757\u98ce\u9669\u8bc4\u4f30\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u98ce\u9669\u5206\u7c7b\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u901a\u8fc7\u9886\u57df\u9002\u5e94\u548c\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u9888\u52a8\u8109\u7ca5\u6837\u786c\u5316\u75be\u75c5\u7684\u98ce\u9669\u8bc4\u4f30\u9700\u8981\u6574\u5408\u591a\u6837\u5316\u7684\u4e34\u5e8a\u548c\u5f71\u50cf\u4fe1\u606f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u9888\u52a8\u8109\u6591\u5757\u8bc4\u4f30\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u8fc7\u8bbf\u8c08\u5f0f\u95ee\u9898\u5e8f\u5217\u6a21\u62df\u771f\u5b9e\u8bca\u65ad\u573a\u666f\u7684\u6846\u67b6\uff0c\u6bd4\u8f83\u4e86\u591a\u79cd\u5f00\u6e90LVLMs\u3002\u4f7f\u7528\u4f4e\u79e9\u9002\u5e94( LoRA )\u5c06LLaVa-NeXT-Vicuna\u6a21\u578b\u9002\u914d\u5230\u8d85\u58f0\u9886\u57df\uff0c\u5e76\u6574\u5408\u591a\u6a21\u6001\u8868\u683c\u6570\u636e\u3002", "result": "\u96f6\u6837\u672c\u5b9e\u9a8c\u663e\u793a\uff0c\u5e76\u975e\u6240\u6709LVLMs\u90fd\u80fd\u51c6\u786e\u8bc6\u522b\u6210\u50cf\u6a21\u6001\u548c\u89e3\u5256\u7ed3\u6784\uff0c\u6240\u6709\u6a21\u578b\u5728\u98ce\u9669\u5206\u7c7b\u65b9\u9762\u8868\u73b0\u5747\u4e0d\u4f73\u3002\u9886\u57df\u9002\u5e94\u540e\u5352\u4e2d\u98ce\u9669\u5206\u5c42\u663e\u8457\u6539\u5584\uff0c\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u7279\u5f02\u6027\u548c\u5e73\u8861\u51c6\u786e\u7387\u3002", "conclusion": "LVLMs\u5728\u8d85\u58f0\u5fc3\u8840\u7ba1\u98ce\u9669\u9884\u6d4b\u4e2d\u65e2\u6709\u524d\u666f\u4e5f\u6709\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u6574\u5408\u3001\u6a21\u578b\u6821\u51c6\u548c\u9886\u57df\u9002\u5e94\u5bf9\u4e8e\u4e34\u5e8a\u8f6c\u5316\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.02970", "pdf": "https://arxiv.org/pdf/2510.02970", "abs": "https://arxiv.org/abs/2510.02970", "authors": ["Xiaoyan Kui", "Qianmu Xiao", "Qqinsong Li", "Zexin Ji", "JIelin Zhang", "Beiji Zou"], "title": "Flip Distribution Alignment VAE for Multi-Phase MRI Synthesis", "categories": ["cs.CV"], "comment": "This paper has been early accept by MICCAI 2025", "summary": "Separating shared and independent features is crucial for multi-phase contrast-enhanced (CE) MRI synthesis. However, existing methods use deep autoencoder generators with low parameter efficiency and lack interpretable training strategies. In this paper, we propose Flip Distribution Alignment Variational Autoencoder (FDA-VAE), a lightweight feature-decoupled VAE model for multi-phase CE MRI synthesis. Our method encodes input and target images into two latent distributions that are symmetric concerning a standard normal distribution, effectively separating shared and independent features. The Y-shaped bidirectional training strategy further enhances the interpretability of feature separation. Experimental results show that compared to existing deep autoencoder-based end-to-end synthesis methods, FDA-VAE significantly reduces model parameters and inference time while effectively improving synthesis quality. The source code is publicly available at https://github.com/QianMuXiao/FDA-VAE.", "AI": {"tldr": "\u63d0\u51faFDA-VAE\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7279\u5f81\u89e3\u8026VAE\u6a21\u578b\uff0c\u7528\u4e8e\u591a\u671f\u76f8\u589e\u5f3aMRI\u5408\u6210\uff0c\u901a\u8fc7\u5bf9\u79f0\u6f5c\u5728\u5206\u5e03\u7f16\u7801\u6709\u6548\u5206\u79bb\u5171\u4eab\u548c\u72ec\u7acb\u7279\u5f81\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u6df1\u5ea6\u81ea\u7f16\u7801\u5668\u751f\u6210\u5668\u53c2\u6570\u6548\u7387\u4f4e\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u9700\u8981\u6539\u8fdb\u591a\u671f\u76f8\u589e\u5f3aMRI\u5408\u6210\u4e2d\u7684\u7279\u5f81\u5206\u79bb\u3002", "method": "\u4f7f\u7528Flip Distribution Alignment VAE\uff0c\u5c06\u8f93\u5165\u548c\u76ee\u6807\u56fe\u50cf\u7f16\u7801\u4e3a\u5173\u4e8e\u6807\u51c6\u6b63\u6001\u5206\u5e03\u5bf9\u79f0\u7684\u4e24\u4e2a\u6f5c\u5728\u5206\u5e03\uff0c\u91c7\u7528Y\u5f62\u53cc\u5411\u8bad\u7ec3\u7b56\u7565\u589e\u5f3a\u7279\u5f81\u5206\u79bb\u7684\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u6df1\u5ea6\u81ea\u7f16\u7801\u5668\u7aef\u5230\u7aef\u5408\u6210\u65b9\u6cd5\uff0cFDA-VAE\u663e\u8457\u51cf\u5c11\u6a21\u578b\u53c2\u6570\u548c\u63a8\u7406\u65f6\u95f4\uff0c\u540c\u65f6\u6709\u6548\u63d0\u9ad8\u5408\u6210\u8d28\u91cf\u3002", "conclusion": "FDA-VAE\u662f\u4e00\u79cd\u53c2\u6570\u6548\u7387\u9ad8\u3001\u63a8\u7406\u901f\u5ea6\u5feb\u4e14\u5408\u6210\u8d28\u91cf\u597d\u7684\u591a\u671f\u76f8\u589e\u5f3aMRI\u5408\u6210\u65b9\u6cd5\u3002"}}
{"id": "2510.02987", "pdf": "https://arxiv.org/pdf/2510.02987", "abs": "https://arxiv.org/abs/2510.02987", "authors": ["Juntong Wang", "Huiyu Duan", "Jiarui Wang", "Ziheng Jia", "Guangtao Zhai", "Xiongkuo Min"], "title": "TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid advancement of large multimodal models (LMMs), recent text-to-image (T2I) models can generate high-quality images and demonstrate great alignment to short prompts. However, they still struggle to effectively understand and follow long and detailed prompts, displaying inconsistent generation. To address this challenge, we introduce LPG-Bench, a comprehensive benchmark for evaluating long-prompt-based text-to-image generation. LPG-Bench features 200 meticulously crafted prompts with an average length of over 250 words, approaching the input capacity of several leading commercial models. Using these prompts, we generate 2,600 images from 13 state-of-the-art models and further perform comprehensive human-ranked annotations. Based on LPG-Bench, we observe that state-of-the-art T2I alignment evaluation metrics exhibit poor consistency with human preferences on long-prompt-based image generation. To address the gap, we introduce a novel zero-shot metric based on text-to-image-to-text consistency, termed TIT, for evaluating long-prompt-generated images. The core concept of TIT is to quantify T2I alignment by directly comparing the consistency between the raw prompt and the LMM-produced description on the generated image, which includes an efficient score-based instantiation TIT-Score and a large-language-model (LLM) based instantiation TIT-Score-LLM. Extensive experiments demonstrate that our framework achieves superior alignment with human judgment compared to CLIP-score, LMM-score, etc., with TIT-Score-LLM attaining a 7.31% absolute improvement in pairwise accuracy over the strongest baseline. LPG-Bench and TIT methods together offer a deeper perspective to benchmark and foster the development of T2I models. All resources will be made publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86LPG-Bench\u57fa\u51c6\u6d4b\u8bd5\u548cTIT\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u957f\u63d0\u793a\u751f\u6210\u4e2d\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5bf9\u957f\u8be6\u7ec6\u63d0\u793a\u7406\u89e3\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u77ed\u63d0\u793a\u4e0b\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u4f46\u5728\u957f\u8be6\u7ec6\u63d0\u793a\u4e0b\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u8bc4\u4f30\u57fa\u51c6\u548c\u6307\u6807\u3002", "method": "\u6784\u5efa\u5305\u542b200\u4e2a\u5e73\u5747\u8d85\u8fc7250\u8bcd\u7684\u957f\u63d0\u793a\u57fa\u51c6LPG-Bench\uff0c\u751f\u62102600\u5f20\u56fe\u50cf\u5e76\u8fdb\u884c\u4eba\u5de5\u6807\u6ce8\uff1b\u63d0\u51fa\u57fa\u4e8e\u6587\u672c-\u56fe\u50cf-\u6587\u672c\u4e00\u81f4\u6027\u7684TIT\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5305\u62ecTIT-Score\u548cTIT-Score-LLM\u4e24\u79cd\u5b9e\u73b0\u3002", "result": "\u73b0\u6709T2I\u8bc4\u4f30\u6307\u6807\u5728\u957f\u63d0\u793a\u751f\u6210\u4e2d\u4e0e\u4eba\u7c7b\u504f\u597d\u4e00\u81f4\u6027\u5dee\uff1bTIT\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0cTIT-Score-LLM\u5728\u6210\u5bf9\u51c6\u786e\u7387\u4e0a\u6bd4\u6700\u5f3a\u57fa\u7ebf\u63d0\u53477.31%\u3002", "conclusion": "LPG-Bench\u548cTIT\u65b9\u6cd5\u4e3a\u8bc4\u4f30\u548c\u4fc3\u8fdbT2I\u6a21\u578b\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u6240\u6709\u8d44\u6e90\u5c06\u516c\u5f00\u3002"}}
{"id": "2510.03012", "pdf": "https://arxiv.org/pdf/2510.03012", "abs": "https://arxiv.org/abs/2510.03012", "authors": ["Haoze Sun", "Linfeng Jiang", "Fan Li", "Renjing Pei", "Zhixin Wang", "Yong Guo", "Jiaqi Xu", "Haoyu Chen", "Jin Han", "Fenglong Song", "Yujiu Yang", "Wenbo Li"], "title": "PocketSR: The Super-Resolution Expert in Your Pocket Mobiles", "categories": ["cs.CV"], "comment": null, "summary": "Real-world image super-resolution (RealSR) aims to enhance the visual quality of in-the-wild images, such as those captured by mobile phones. While existing methods leveraging large generative models demonstrate impressive results, the high computational cost and latency make them impractical for edge deployment. In this paper, we introduce PocketSR, an ultra-lightweight, single-step model that brings generative modeling capabilities to RealSR while maintaining high fidelity. To achieve this, we design LiteED, a highly efficient alternative to the original computationally intensive VAE in SD, reducing parameters by 97.5% while preserving high-quality encoding and decoding. Additionally, we propose online annealing pruning for the U-Net, which progressively shifts generative priors from heavy modules to lightweight counterparts, ensuring effective knowledge transfer and further optimizing efficiency. To mitigate the loss of prior knowledge during pruning, we incorporate a multi-layer feature distillation loss. Through an in-depth analysis of each design component, we provide valuable insights for future research. PocketSR, with a model size of 146M parameters, processes 4K images in just 0.8 seconds, achieving a remarkable speedup over previous methods. Notably, it delivers performance on par with state-of-the-art single-step and even multi-step RealSR models, making it a highly practical solution for edge-device applications.", "AI": {"tldr": "PocketSR\u662f\u4e00\u4e2a\u8d85\u8f7b\u91cf\u7ea7\u7684\u5355\u6b65\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6a21\u578b\uff0c\u901a\u8fc7\u9ad8\u6548\u7684LiteED\u7f16\u7801\u5668\u548c\u5728\u7ebf\u9000\u706b\u526a\u679d\u6280\u672f\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\uff0c\u9002\u5408\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u751f\u6210\u6a21\u578b\u7684RealSR\u65b9\u6cd5\u867d\u7136\u6548\u679c\u597d\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u5ef6\u8fdf\u5927\uff0c\u96be\u4ee5\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002\u9700\u8981\u5f00\u53d1\u65e2\u9ad8\u6548\u53c8\u80fd\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u8d85\u5206\u8fa8\u7387\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u4e86LiteED\u4f5c\u4e3aSD\u4e2dVAE\u7684\u9ad8\u6548\u66ff\u4ee3\u54c1\uff0c\u53c2\u6570\u51cf\u5c1197.5%\uff1b\u63d0\u51fa\u5728\u7ebf\u9000\u706b\u526a\u679d\u6280\u672f\uff0c\u9010\u6b65\u5c06\u751f\u6210\u5148\u9a8c\u4ece\u91cd\u6a21\u5757\u8f6c\u79fb\u5230\u8f7b\u6a21\u5757\uff1b\u4f7f\u7528\u591a\u5c42\u7279\u5f81\u84b8\u998f\u635f\u5931\u6765\u7f13\u89e3\u526a\u679d\u8fc7\u7a0b\u4e2d\u7684\u77e5\u8bc6\u635f\u5931\u3002", "result": "PocketSR\u6a21\u578b\u4ec5146M\u53c2\u6570\uff0c\u5904\u74064K\u56fe\u50cf\u4ec5\u97000.8\u79d2\uff0c\u901f\u5ea6\u663e\u8457\u63d0\u5347\u3002\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7684\u5355\u6b65\u548c\u591a\u6b65RealSR\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "PocketSR\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6548\u7387\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2510.03049", "pdf": "https://arxiv.org/pdf/2510.03049", "abs": "https://arxiv.org/abs/2510.03049", "authors": ["Ruotong Liao", "Guowen Huang", "Qing Cheng", "Thomas Seidl", "Daniel Cremers", "Volker Tresp"], "title": "When and Where do Events Switch in Multi-Event Video Generation?", "categories": ["cs.CV", "cs.AI"], "comment": "Work in Progress. Accepted to ICCV2025 @ LongVid-Foundations", "summary": "Text-to-video (T2V) generation has surged in response to challenging questions, especially when a long video must depict multiple sequential events with temporal coherence and controllable content. Existing methods that extend to multi-event generation omit an inspection of the intrinsic factor in event shifting. The paper aims to answer the central question: When and where multi-event prompts control event transition during T2V generation. This work introduces MEve, a self-curated prompt suite for evaluating multi-event text-to-video (T2V) generation, and conducts a systematic study of two representative model families, i.e., OpenSora and CogVideoX. Extensive experiments demonstrate the importance of early intervention in denoising steps and block-wise model layers, revealing the essential factor for multi-event video generation and highlighting the possibilities for multi-event conditioning in future models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MEve\u8bc4\u4f30\u5957\u4ef6\uff0c\u7cfb\u7edf\u7814\u7a76\u4e86\u591a\u4e8b\u4ef6\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u4e2d\u7684\u4e8b\u4ef6\u8f6c\u6362\u63a7\u5236\u95ee\u9898\uff0c\u53d1\u73b0\u65e9\u671f\u5e72\u9884\u53bb\u566a\u6b65\u9aa4\u548c\u5757\u7ea7\u6a21\u578b\u5c42\u5bf9\u591a\u4e8b\u4ef6\u89c6\u9891\u751f\u6210\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6269\u5c55\u5230\u591a\u4e8b\u4ef6\u751f\u6210\u65f6\u5ffd\u7565\u4e86\u4e8b\u4ef6\u8f6c\u6362\u7684\u5185\u5728\u56e0\u7d20\uff0c\u672c\u6587\u65e8\u5728\u56de\u7b54\u591a\u4e8b\u4ef6\u63d0\u793a\u4f55\u65f6\u4f55\u5730\u63a7\u5236\u4e8b\u4ef6\u8f6c\u6362\u8fd9\u4e00\u6838\u5fc3\u95ee\u9898\u3002", "method": "\u5f15\u5165MEve\u81ea\u5efa\u63d0\u793a\u5957\u4ef6\uff0c\u7cfb\u7edf\u7814\u7a76OpenSora\u548cCogVideoX\u4e24\u4e2a\u4ee3\u8868\u6027\u6a21\u578b\u5bb6\u65cf\uff0c\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u5206\u6790\u53bb\u566a\u6b65\u9aa4\u548c\u6a21\u578b\u5c42\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u65e9\u671f\u5e72\u9884\u53bb\u566a\u6b65\u9aa4\u548c\u5757\u7ea7\u6a21\u578b\u5c42\u5bf9\u591a\u4e8b\u4ef6\u89c6\u9891\u751f\u6210\u81f3\u5173\u91cd\u8981\uff0c\u63ed\u793a\u4e86\u591a\u4e8b\u4ef6\u751f\u6210\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u591a\u4e8b\u4ef6\u8c03\u8282\u5728\u672a\u6765\u6a21\u578b\u4e2d\u7684\u53ef\u80fd\u6027\uff0c\u4e3a\u591a\u4e8b\u4ef6\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2510.03075", "pdf": "https://arxiv.org/pdf/2510.03075", "abs": "https://arxiv.org/abs/2510.03075", "authors": ["Karim Farid", "Rajat Sahay", "Yumna Ali Alnaggar", "Simon Schrodi", "Volker Fischer", "Cordelia Schmid", "Thomas Brox"], "title": "What Drives Compositional Generalization in Visual Generative Models?", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Compositional generalization, the ability to generate novel combinations of known concepts, is a key ingredient for visual generative models. Yet, not all mechanisms that enable or inhibit it are fully understood. In this work, we conduct a systematic study of how various design choices influence compositional generalization in image and video generation in a positive or negative way. Through controlled experiments, we identify two key factors: (i) whether the training objective operates on a discrete or continuous distribution, and (ii) to what extent conditioning provides information about the constituent concepts during training. Building on these insights, we show that relaxing the MaskGIT discrete loss with an auxiliary continuous JEPA-based objective can improve compositional performance in discrete models like MaskGIT.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u89c6\u89c9\u751f\u6210\u6a21\u578b\u4e2d\u5f71\u54cd\u7ec4\u5408\u6cdb\u5316\u7684\u8bbe\u8ba1\u9009\u62e9\uff0c\u53d1\u73b0\u79bb\u6563/\u8fde\u7eed\u8bad\u7ec3\u76ee\u6807\u548c\u6761\u4ef6\u4fe1\u606f\u662f\u4e24\u5927\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u6dfb\u52a0\u8fde\u7eedJEPA\u76ee\u6807\u6765\u6539\u8fdbMaskGIT\u7684\u7ec4\u5408\u6027\u80fd\u3002", "motivation": "\u7ec4\u5408\u6cdb\u5316\u662f\u89c6\u89c9\u751f\u6210\u6a21\u578b\u7684\u5173\u952e\u80fd\u529b\uff0c\u4f46\u76ee\u524d\u5bf9\u5176\u4fc3\u8fdb\u548c\u6291\u5236\u673a\u5236\u7684\u7406\u89e3\u8fd8\u4e0d\u591f\u5145\u5206\uff0c\u9700\u8981\u8fdb\u884c\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u53d7\u63a7\u5b9e\u9a8c\u7814\u7a76\u4e0d\u540c\u8bbe\u8ba1\u9009\u62e9\u5bf9\u7ec4\u5408\u6cdb\u5316\u7684\u5f71\u54cd\uff0c\u91cd\u70b9\u5173\u6ce8\u8bad\u7ec3\u76ee\u6807\u7684\u79bb\u6563/\u8fde\u7eed\u6027\u8d28\u4ee5\u53ca\u6761\u4ef6\u4fe1\u606f\u7684\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u5728MaskGIT\u4e2d\u5f15\u5165\u8fde\u7eedJEPA\u76ee\u6807\u7684\u65b9\u6cd5\u3002", "result": "\u8bc6\u522b\u51fa\u4e24\u4e2a\u5173\u952e\u56e0\u7d20\uff1a(i)\u8bad\u7ec3\u76ee\u6807\u7684\u79bb\u6563\u6216\u8fde\u7eed\u5206\u5e03\uff1b(ii)\u8bad\u7ec3\u671f\u95f4\u6761\u4ef6\u4fe1\u606f\u5bf9\u7ec4\u6210\u6982\u5ff5\u7684\u63d0\u4f9b\u7a0b\u5ea6\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u8bc1\u660e\u4e86\u5728MaskGIT\u4e2d\u6dfb\u52a0\u8fde\u7eedJEPA\u76ee\u6807\u53ef\u4ee5\u6539\u5584\u7ec4\u5408\u6027\u80fd\u3002", "conclusion": "\u8bad\u7ec3\u76ee\u6807\u7684\u79bb\u6563/\u8fde\u7eed\u6027\u8d28\u548c\u6761\u4ef6\u4fe1\u606f\u662f\u5f71\u54cd\u7ec4\u5408\u6cdb\u5316\u7684\u5173\u952e\u56e0\u7d20\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u8bbe\u8ba1\u9009\u62e9\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u89c6\u89c9\u751f\u6210\u6a21\u578b\u7684\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.03089", "pdf": "https://arxiv.org/pdf/2510.03089", "abs": "https://arxiv.org/abs/2510.03089", "authors": ["Naresh Kumar Devulapally", "Shruti Agarwal", "Tejas Gokhale", "Vishnu Suresh Lokhande"], "title": "Latent Diffusion Unlearning: Protecting Against Unauthorized Personalization Through Trajectory Shifted Perturbations", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image diffusion models have demonstrated remarkable effectiveness in rapid and high-fidelity personalization, even when provided with only a few user images. However, the effectiveness of personalization techniques has lead to concerns regarding data privacy, intellectual property protection, and unauthorized usage. To mitigate such unauthorized usage and model replication, the idea of generating ``unlearnable'' training samples utilizing image poisoning techniques has emerged. Existing methods for this have limited imperceptibility as they operate in the pixel space which results in images with noise and artifacts. In this work, we propose a novel model-based perturbation strategy that operates within the latent space of diffusion models. Our method alternates between denoising and inversion while modifying the starting point of the denoising trajectory: of diffusion models. This trajectory-shifted sampling ensures that the perturbed images maintain high visual fidelity to the original inputs while being resistant to inversion and personalization by downstream generative models. This approach integrates unlearnability into the framework of Latent Diffusion Models (LDMs), enabling a practical and imperceptible defense against unauthorized model adaptation. We validate our approach on four benchmark datasets to demonstrate robustness against state-of-the-art inversion attacks. Results demonstrate that our method achieves significant improvements in imperceptibility ($\\sim 8 \\% -10\\%$ on perceptual metrics including PSNR, SSIM, and FID) and robustness ( $\\sim 10\\%$ on average across five adversarial settings), highlighting its effectiveness in safeguarding sensitive data.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u4e0d\u53ef\u5b66\u4e60\u6837\u672c\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u4ea4\u66ff\u8fdb\u884c\u53bb\u566a\u548c\u53cd\u8f6c\u64cd\u4f5c\uff0c\u4fee\u6539\u53bb\u566a\u8f68\u8ff9\u7684\u8d77\u70b9\uff0c\u5b9e\u73b0\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\u4e14\u80fd\u62b5\u6297\u4e0b\u6e38\u751f\u6210\u6a21\u578b\u53cd\u6f14\u548c\u4e2a\u6027\u5316\u7684\u56fe\u50cf\u6270\u52a8\u3002", "motivation": "\u9488\u5bf9\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u4e2a\u6027\u5316\u5e94\u7528\u4e2d\u53ef\u80fd\u5f15\u53d1\u7684\u6570\u636e\u9690\u79c1\u3001\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u548c\u672a\u7ecf\u6388\u6743\u4f7f\u7528\u7b49\u95ee\u9898\uff0c\u73b0\u6709\u57fa\u4e8e\u50cf\u7d20\u7a7a\u95f4\u7684\u56fe\u50cf\u6bd2\u5316\u65b9\u6cd5\u5b58\u5728\u89c6\u89c9\u8d28\u91cf\u5dee\u7684\u95ee\u9898\uff0c\u9700\u8981\u5728\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u6709\u6548\u7684\u9632\u5fa1\u3002", "method": "\u5728\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u91c7\u7528\u4ea4\u66ff\u53bb\u566a\u548c\u53cd\u8f6c\u7684\u7b56\u7565\uff0c\u4fee\u6539\u53bb\u566a\u8f68\u8ff9\u7684\u8d77\u70b9\uff0c\u901a\u8fc7\u8f68\u8ff9\u504f\u79fb\u91c7\u6837\u751f\u6210\u4e0d\u53ef\u5b66\u4e60\u7684\u8bad\u7ec3\u6837\u672c\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u611f\u77e5\u6307\u6807\uff08PSNR\u3001SSIM\u3001FID\uff09\u4e0a\u63d0\u5347\u4e86\u7ea68%-10%\uff0c\u5728\u4e94\u79cd\u5bf9\u6297\u8bbe\u7f6e\u4e0b\u5e73\u5747\u9c81\u68d2\u6027\u63d0\u5347\u4e86\u7ea610%\uff0c\u80fd\u6709\u6548\u62b5\u6297\u6700\u5148\u8fdb\u7684\u53cd\u6f14\u653b\u51fb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c06\u4e0d\u53ef\u5b66\u4e60\u7279\u6027\u96c6\u6210\u5230\u6f5c\u5728\u6269\u6563\u6a21\u578b\u6846\u67b6\u4e2d\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u96be\u4ee5\u5bdf\u89c9\u7684\u9632\u5fa1\u673a\u5236\uff0c\u6709\u6548\u4fdd\u62a4\u654f\u611f\u6570\u636e\u514d\u53d7\u672a\u7ecf\u6388\u6743\u7684\u6a21\u578b\u9002\u5e94\u3002"}}
{"id": "2510.03104", "pdf": "https://arxiv.org/pdf/2510.03104", "abs": "https://arxiv.org/abs/2510.03104", "authors": ["Zhiting Mei", "Ola Shorinwa", "Anirudha Majumdar"], "title": "Geometry Meets Vision: Revisiting Pretrained Semantics in Distilled Fields", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Semantic distillation in radiance fields has spurred significant advances in open-vocabulary robot policies, e.g., in manipulation and navigation, founded on pretrained semantics from large vision models. While prior work has demonstrated the effectiveness of visual-only semantic features (e.g., DINO and CLIP) in Gaussian Splatting and neural radiance fields, the potential benefit of geometry-grounding in distilled fields remains an open question. In principle, visual-geometry features seem very promising for spatial tasks such as pose estimation, prompting the question: Do geometry-grounded semantic features offer an edge in distilled fields? Specifically, we ask three critical questions: First, does spatial-grounding produce higher-fidelity geometry-aware semantic features? We find that image features from geometry-grounded backbones contain finer structural details compared to their counterparts. Secondly, does geometry-grounding improve semantic object localization? We observe no significant difference in this task. Thirdly, does geometry-grounding enable higher-accuracy radiance field inversion? Given the limitations of prior work and their lack of semantics integration, we propose a novel framework SPINE for inverting radiance fields without an initial guess, consisting of two core components: coarse inversion using distilled semantics, and fine inversion using photometric-based optimization. Surprisingly, we find that the pose estimation accuracy decreases with geometry-grounded features. Our results suggest that visual-only features offer greater versatility for a broader range of downstream tasks, although geometry-grounded features contain more geometric detail. Notably, our findings underscore the necessity of future research on effective strategies for geometry-grounding that augment the versatility and performance of pretrained semantic features.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u8f90\u5c04\u573a\u8bed\u4e49\u84b8\u998f\u4e2d\u51e0\u4f55\u63a5\u5730\u7279\u5f81\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u89c6\u89c9\u7279\u5f81\u6bd4\u51e0\u4f55\u63a5\u5730\u7279\u5f81\u5728\u59ff\u6001\u4f30\u8ba1\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u5c3d\u7ba1\u540e\u8005\u5305\u542b\u66f4\u591a\u51e0\u4f55\u7ec6\u8282\u3002", "motivation": "\u63a2\u7d22\u51e0\u4f55\u63a5\u5730\u8bed\u4e49\u7279\u5f81\u5728\u84b8\u998f\u8f90\u5c04\u573a\u4e2d\u7684\u6f5c\u5728\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u7a7a\u95f4\u4efb\u52a1\u5982\u59ff\u6001\u4f30\u8ba1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86SPINE\u6846\u67b6\uff0c\u5305\u542b\u57fa\u4e8e\u84b8\u998f\u8bed\u4e49\u7684\u7c97\u53cd\u6f14\u548c\u57fa\u4e8e\u5149\u5ea6\u4f18\u5316\u7684\u7cbe\u7ec6\u53cd\u6f14\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\u3002", "result": "\u51e0\u4f55\u63a5\u5730\u7279\u5f81\u5305\u542b\u66f4\u7cbe\u7ec6\u7684\u7ed3\u6784\u7ec6\u8282\uff0c\u4f46\u5728\u8bed\u4e49\u76ee\u6807\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u65e0\u663e\u8457\u5dee\u5f02\uff0c\u4e14\u5728\u59ff\u6001\u4f30\u8ba1\u4e2d\u7cbe\u5ea6\u53cd\u800c\u4e0b\u964d\u3002", "conclusion": "\u89c6\u89c9\u7279\u5f81\u5728\u66f4\u5e7f\u6cdb\u7684\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5177\u6709\u66f4\u597d\u7684\u901a\u7528\u6027\uff0c\u672a\u6765\u9700\u8981\u7814\u7a76\u66f4\u6709\u6548\u7684\u51e0\u4f55\u63a5\u5730\u7b56\u7565\u6765\u589e\u5f3a\u9884\u8bad\u7ec3\u8bed\u4e49\u7279\u5f81\u7684\u6027\u80fd\u3002"}}
{"id": "2510.03110", "pdf": "https://arxiv.org/pdf/2510.03110", "abs": "https://arxiv.org/abs/2510.03110", "authors": ["Beibei Lin", "Tingting Chen", "Robby T. Tan"], "title": "GeoComplete: Geometry-Aware Diffusion for Reference-Driven Image Completion", "categories": ["cs.CV"], "comment": "Accepted by NeurIPS 2025. Project page:   https://bb12346.github.io/GeoComplete/", "summary": "Reference-driven image completion, which restores missing regions in a target view using additional images, is particularly challenging when the target view differs significantly from the references. Existing generative methods rely solely on diffusion priors and, without geometric cues such as camera pose or depth, often produce misaligned or implausible content. We propose GeoComplete, a novel framework that incorporates explicit 3D structural guidance to enforce geometric consistency in the completed regions, setting it apart from prior image-only approaches. GeoComplete introduces two key ideas: conditioning the diffusion process on projected point clouds to infuse geometric information, and applying target-aware masking to guide the model toward relevant reference cues. The framework features a dual-branch diffusion architecture. One branch synthesizes the missing regions from the masked target, while the other extracts geometric features from the projected point cloud. Joint self-attention across branches ensures coherent and accurate completion. To address regions visible in references but absent in the target, we project the target view into each reference to detect occluded areas, which are then masked during training. This target-aware masking directs the model to focus on useful cues, enhancing performance in difficult scenarios. By integrating a geometry-aware dual-branch diffusion architecture with a target-aware masking strategy, GeoComplete offers a unified and robust solution for geometry-conditioned image completion. Experiments show that GeoComplete achieves a 17.1 PSNR improvement over state-of-the-art methods, significantly boosting geometric accuracy while maintaining high visual quality.", "AI": {"tldr": "GeoComplete\u662f\u4e00\u4e2a\u7528\u4e8e\u53c2\u8003\u9a71\u52a8\u56fe\u50cf\u8865\u5168\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u663e\u5f0f3D\u7ed3\u6784\u6307\u5bfc\u6765\u589e\u5f3a\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u751f\u6210\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u6269\u6563\u5148\u9a8c\uff0c\u7f3a\u4e4f\u51e0\u4f55\u7ebf\u7d22\uff08\u5982\u76f8\u673a\u59ff\u6001\u6216\u6df1\u5ea6\uff09\uff0c\u5728\u76ee\u6807\u89c6\u56fe\u4e0e\u53c2\u8003\u56fe\u50cf\u5dee\u5f02\u8f83\u5927\u65f6\u7ecf\u5e38\u4ea7\u751f\u9519\u4f4d\u6216\u4e0d\u5408\u7406\u7684\u5185\u5bb9\u3002", "method": "\u63d0\u51fa\u53cc\u5206\u652f\u6269\u6563\u67b6\u6784\uff1a\u4e00\u4e2a\u5206\u652f\u4ece\u63a9\u7801\u76ee\u6807\u5408\u6210\u7f3a\u5931\u533a\u57df\uff0c\u53e6\u4e00\u4e2a\u5206\u652f\u4ece\u6295\u5f71\u70b9\u4e91\u63d0\u53d6\u51e0\u4f55\u7279\u5f81\uff1b\u5f15\u5165\u76ee\u6807\u611f\u77e5\u63a9\u7801\u7b56\u7565\uff0c\u5c06\u76ee\u6807\u89c6\u56fe\u6295\u5f71\u5230\u6bcf\u4e2a\u53c2\u8003\u4e2d\u4ee5\u68c0\u6d4b\u906e\u6321\u533a\u57df\u5e76\u5728\u8bad\u7ec3\u65f6\u63a9\u7801\u3002", "result": "\u5b9e\u9a8c\u663e\u793aGeoComplete\u5728PSNR\u6307\u6807\u4e0a\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u534717.1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51e0\u4f55\u51c6\u786e\u6027\u540c\u65f6\u4fdd\u6301\u9ad8\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u51e0\u4f55\u611f\u77e5\u7684\u53cc\u5206\u652f\u6269\u6563\u67b6\u6784\u548c\u76ee\u6807\u611f\u77e5\u63a9\u7801\u7b56\u7565\uff0cGeoComplete\u4e3a\u51e0\u4f55\u6761\u4ef6\u56fe\u50cf\u8865\u5168\u63d0\u4f9b\u4e86\u7edf\u4e00\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03117", "pdf": "https://arxiv.org/pdf/2510.03117", "abs": "https://arxiv.org/abs/2510.03117", "authors": ["Kaisi Guan", "Xihua Wang", "Zhengfeng Lai", "Xin Cheng", "Peng Zhang", "XiaoJiang Liu", "Ruihua Song", "Meng Cao"], "title": "Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction", "categories": ["cs.CV", "cs.SD"], "comment": null, "summary": "This study focuses on a challenging yet promising task, Text-to-Sounding-Video (T2SV) generation, which aims to generate a video with synchronized audio from text conditions, meanwhile ensuring both modalities are aligned with text. Despite progress in joint audio-video training, two critical challenges still remain unaddressed: (1) a single, shared text caption where the text for video is equal to the text for audio often creates modal interference, confusing the pretrained backbones, and (2) the optimal mechanism for cross-modal feature interaction remains unclear. To address these challenges, we first propose the Hierarchical Visual-Grounded Captioning (HVGC) framework that generates pairs of disentangled captions, a video caption, and an audio caption, eliminating interference at the conditioning stage. Based on HVGC, we further introduce BridgeDiT, a novel dual-tower diffusion transformer, which employs a Dual CrossAttention (DCA) mechanism that acts as a robust ``bridge\" to enable a symmetric, bidirectional exchange of information, achieving both semantic and temporal synchronization. Extensive experiments on three benchmark datasets, supported by human evaluations, demonstrate that our method achieves state-of-the-art results on most metrics. Comprehensive ablation studies further validate the effectiveness of our contributions, offering key insights for the future T2SV task. All the codes and checkpoints will be publicly released.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u6587\u672c\u5230\u58f0\u97f3\u89c6\u9891\u751f\u6210\u4efb\u52a1\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u89c6\u89c9\u57fa\u7840\u5b57\u5e55\u6846\u67b6\u751f\u6210\u89e3\u8026\u7684\u89c6\u9891\u548c\u97f3\u9891\u5b57\u5e55\uff0c\u5e76\u5f15\u5165\u6865\u63a5\u6269\u6563\u53d8\u6362\u5668\u5b9e\u73b0\u8de8\u6a21\u6001\u7279\u5f81\u4ea4\u4e92\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6587\u672c\u5230\u58f0\u97f3\u89c6\u9891\u751f\u6210\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u5355\u4e00\u5171\u4eab\u6587\u672c\u5b57\u5e55\u5bfc\u81f4\u7684\u6a21\u6001\u5e72\u6270\uff0c\u4ee5\u53ca\u8de8\u6a21\u6001\u7279\u5f81\u4ea4\u4e92\u673a\u5236\u4e0d\u660e\u786e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u89c6\u89c9\u57fa\u7840\u5b57\u5e55\u6846\u67b6\u751f\u6210\u89e3\u8026\u7684\u89c6\u9891\u548c\u97f3\u9891\u5b57\u5e55\uff0c\u5e76\u8bbe\u8ba1\u6865\u63a5\u6269\u6563\u53d8\u6362\u5668\uff0c\u91c7\u7528\u53cc\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u53cc\u5411\u4fe1\u606f\u4ea4\u6362\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u548c\u4eba\u5de5\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5927\u591a\u6570\u6307\u6807\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u6001\u5e72\u6270\u548c\u8de8\u6a21\u6001\u4ea4\u4e92\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u6587\u672c\u5230\u58f0\u97f3\u89c6\u9891\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2510.03122", "pdf": "https://arxiv.org/pdf/2510.03122", "abs": "https://arxiv.org/abs/2510.03122", "authors": ["Shiyi Zhang", "Dong Liang", "Hairong Zheng", "Yihang Zhou"], "title": "HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The reconstruction of visual information from brain activity fosters interdisciplinary integration between neuroscience and computer vision. However, existing methods still face challenges in accurately recovering highly complex visual stimuli. This difficulty stems from the characteristics of natural scenes: low-level features exhibit heterogeneity, while high-level features show semantic entanglement due to contextual overlaps. Inspired by the hierarchical representation theory of the visual cortex, we propose the HAVIR model, which separates the visual cortex into two hierarchical regions and extracts distinct features from each. Specifically, the Structural Generator extracts structural information from spatial processing voxels and converts it into latent diffusion priors, while the Semantic Extractor converts semantic processing voxels into CLIP embeddings. These components are integrated via the Versatile Diffusion model to synthesize the final image. Experimental results demonstrate that HAVIR enhances both the structural and semantic quality of reconstructions, even in complex scenes, and outperforms existing models.", "AI": {"tldr": "HAVIR\u6a21\u578b\u901a\u8fc7\u5206\u79bb\u89c6\u89c9\u76ae\u5c42\u7684\u4e24\u4e2a\u5c42\u6b21\u533a\u57df\u6765\u63d0\u53d6\u4e0d\u540c\u7279\u5f81\uff0c\u7ed3\u5408\u7ed3\u6784\u751f\u6210\u5668\u548c\u8bed\u4e49\u63d0\u53d6\u5668\uff0c\u4f7f\u7528Versatile Diffusion\u6a21\u578b\u5408\u6210\u6700\u7ec8\u56fe\u50cf\uff0c\u5728\u590d\u6742\u573a\u666f\u4e2d\u63d0\u5347\u4e86\u91cd\u5efa\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u51c6\u786e\u6062\u590d\u9ad8\u5ea6\u590d\u6742\u89c6\u89c9\u523a\u6fc0\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u56e0\u4e3a\u81ea\u7136\u573a\u666f\u4e2d\u4f4e\u5c42\u7279\u5f81\u5f02\u8d28\u6027\u9ad8\uff0c\u9ad8\u5c42\u7279\u5f81\u56e0\u4e0a\u4e0b\u6587\u91cd\u53e0\u800c\u8bed\u4e49\u7ea0\u7f20\u3002\u53d7\u89c6\u89c9\u76ae\u5c42\u5c42\u6b21\u8868\u793a\u7406\u8bba\u542f\u53d1\uff0c\u9700\u8981\u5206\u79bb\u4e0d\u540c\u5c42\u6b21\u7684\u7279\u5f81\u63d0\u53d6\u3002", "method": "\u5c06\u89c6\u89c9\u76ae\u5c42\u5206\u4e3a\u4e24\u4e2a\u5c42\u6b21\u533a\u57df\uff1a\u7ed3\u6784\u751f\u6210\u5668\u4ece\u7a7a\u95f4\u5904\u7406\u4f53\u7d20\u63d0\u53d6\u7ed3\u6784\u4fe1\u606f\u5e76\u8f6c\u6362\u4e3a\u6f5c\u5728\u6269\u6563\u5148\u9a8c\uff0c\u8bed\u4e49\u63d0\u53d6\u5668\u5c06\u8bed\u4e49\u5904\u7406\u4f53\u7d20\u8f6c\u6362\u4e3aCLIP\u5d4c\u5165\uff0c\u901a\u8fc7Versatile Diffusion\u6a21\u578b\u6574\u5408\u8fd9\u4e9b\u7ec4\u4ef6\u5408\u6210\u6700\u7ec8\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eHAVIR\u5728\u590d\u6742\u573a\u666f\u4e2d\u63d0\u5347\u4e86\u91cd\u5efa\u56fe\u50cf\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u8d28\u91cf\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "HAVIR\u6a21\u578b\u901a\u8fc7\u5c42\u6b21\u5316\u5206\u79bb\u89c6\u89c9\u76ae\u5c42\u7279\u5f81\u63d0\u53d6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u89c6\u89c9\u523a\u6fc0\u91cd\u5efa\u4e2d\u7684\u6311\u6218\uff0c\u5728\u7ed3\u6784\u548c\u8bed\u4e49\u8d28\u91cf\u65b9\u9762\u5747\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2510.03135", "pdf": "https://arxiv.org/pdf/2510.03135", "abs": "https://arxiv.org/abs/2510.03135", "authors": ["Gen Li", "Bo Zhao", "Jianfei Yang", "Laura Sevilla-Lara"], "title": "Mask2IV: Interaction-Centric Video Generation via Mask Trajectories", "categories": ["cs.CV", "cs.RO"], "comment": "Project page: https://reagan1311.github.io/mask2iv", "summary": "Generating interaction-centric videos, such as those depicting humans or robots interacting with objects, is crucial for embodied intelligence, as they provide rich and diverse visual priors for robot learning, manipulation policy training, and affordance reasoning. However, existing methods often struggle to model such complex and dynamic interactions. While recent studies show that masks can serve as effective control signals and enhance generation quality, obtaining dense and precise mask annotations remains a major challenge for real-world use. To overcome this limitation, we introduce Mask2IV, a novel framework specifically designed for interaction-centric video generation. It adopts a decoupled two-stage pipeline that first predicts plausible motion trajectories for both actor and object, then generates a video conditioned on these trajectories. This design eliminates the need for dense mask inputs from users while preserving the flexibility to manipulate the interaction process. Furthermore, Mask2IV supports versatile and intuitive control, allowing users to specify the target object of interaction and guide the motion trajectory through action descriptions or spatial position cues. To support systematic training and evaluation, we curate two benchmarks covering diverse action and object categories across both human-object interaction and robotic manipulation scenarios. Extensive experiments demonstrate that our method achieves superior visual realism and controllability compared to existing baselines.", "AI": {"tldr": "Mask2IV\u662f\u4e00\u4e2a\u7528\u4e8e\u4ea4\u4e92\u4e2d\u5fc3\u89c6\u9891\u751f\u6210\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u65e0\u9700\u5bc6\u96c6\u63a9\u7801\u8f93\u5165\uff0c\u901a\u8fc7\u9884\u6d4b\u52a8\u4f5c\u8f68\u8ff9\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4eba\u7c7b/\u673a\u5668\u4eba\u4ea4\u4e92\u89c6\u9891", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5efa\u6a21\u590d\u6742\u7684\u52a8\u6001\u4ea4\u4e92\uff0c\u800c\u83b7\u53d6\u7cbe\u786e\u7684\u63a9\u7801\u6807\u6ce8\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u5177\u6709\u6311\u6218\u6027", "method": "\u91c7\u7528\u89e3\u8026\u7684\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a\u9996\u5148\u9884\u6d4b\u52a8\u4f5c\u548c\u7269\u4f53\u7684\u5408\u7406\u8fd0\u52a8\u8f68\u8ff9\uff0c\u7136\u540e\u57fa\u4e8e\u8fd9\u4e9b\u8f68\u8ff9\u751f\u6210\u89c6\u9891", "result": "\u5728\u591a\u6837\u5316\u7684\u52a8\u4f5c\u548c\u7269\u4f53\u7c7b\u522b\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u65b9\u6cd5\u5728\u89c6\u89c9\u771f\u5b9e\u6027\u548c\u53ef\u63a7\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf", "conclusion": "Mask2IV\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4ea4\u4e92\u4e2d\u5fc3\u89c6\u9891\u751f\u6210\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u63a7\u5236\u65b9\u5f0f\uff0c\u65e0\u9700\u5bc6\u96c6\u63a9\u7801\u8f93\u5165"}}
{"id": "2510.03161", "pdf": "https://arxiv.org/pdf/2510.03161", "abs": "https://arxiv.org/abs/2510.03161", "authors": ["Qing Huang", "Zhipei Xu", "Xuanyu Zhang", "Jian Zhang"], "title": "UniShield: An Adaptive Multi-Agent Framework for Unified Forgery Image Detection and Localization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the rapid advancements in image generation, synthetic images have become increasingly realistic, posing significant societal risks, such as misinformation and fraud. Forgery Image Detection and Localization (FIDL) thus emerges as essential for maintaining information integrity and societal security. Despite impressive performances by existing domain-specific detection methods, their practical applicability remains limited, primarily due to their narrow specialization, poor cross-domain generalization, and the absence of an integrated adaptive framework. To address these issues, we propose UniShield, the novel multi-agent-based unified system capable of detecting and localizing image forgeries across diverse domains, including image manipulation, document manipulation, DeepFake, and AI-generated images. UniShield innovatively integrates a perception agent with a detection agent. The perception agent intelligently analyzes image features to dynamically select suitable detection models, while the detection agent consolidates various expert detectors into a unified framework and generates interpretable reports. Extensive experiments show that UniShield achieves state-of-the-art results, surpassing both existing unified approaches and domain-specific detectors, highlighting its superior practicality, adaptiveness, and scalability.", "AI": {"tldr": "\u63d0\u51fa\u4e86UniShield\u591a\u667a\u80fd\u4f53\u7edf\u4e00\u7cfb\u7edf\uff0c\u7528\u4e8e\u8de8\u57df\u68c0\u6d4b\u548c\u5b9a\u4f4d\u56fe\u50cf\u4f2a\u9020\uff0c\u5305\u62ec\u56fe\u50cf\u7be1\u6539\u3001\u6587\u6863\u7be1\u6539\u3001DeepFake\u548cAI\u751f\u6210\u56fe\u50cf\uff0c\u901a\u8fc7\u611f\u77e5\u667a\u80fd\u4f53\u548c\u68c0\u6d4b\u667a\u80fd\u4f53\u7684\u534f\u540c\u5de5\u4f5c\u5b9e\u73b0\u81ea\u9002\u5e94\u68c0\u6d4b\u3002", "motivation": "\u968f\u7740\u56fe\u50cf\u751f\u6210\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5408\u6210\u56fe\u50cf\u8d8a\u6765\u8d8a\u903c\u771f\uff0c\u5e26\u6765\u4e86\u9519\u8bef\u4fe1\u606f\u548c\u6b3a\u8bc8\u7b49\u793e\u4f1a\u98ce\u9669\u3002\u73b0\u6709\u9886\u57df\u7279\u5b9a\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u4e13\u4e1a\u5316\u8fc7\u7a84\u3001\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u5dee\u3001\u7f3a\u4e4f\u96c6\u6210\u81ea\u9002\u5e94\u6846\u67b6\u7b49\u5c40\u9650\u6027\u3002", "method": "UniShield\u7cfb\u7edf\u521b\u65b0\u6027\u5730\u96c6\u6210\u4e86\u611f\u77e5\u667a\u80fd\u4f53\u548c\u68c0\u6d4b\u667a\u80fd\u4f53\u3002\u611f\u77e5\u667a\u80fd\u4f53\u667a\u80fd\u5206\u6790\u56fe\u50cf\u7279\u5f81\u4ee5\u52a8\u6001\u9009\u62e9\u5408\u9002\u7684\u68c0\u6d4b\u6a21\u578b\uff0c\u68c0\u6d4b\u667a\u80fd\u4f53\u5c06\u5404\u79cd\u4e13\u5bb6\u68c0\u6d4b\u5668\u6574\u5408\u5230\u7edf\u4e00\u6846\u67b6\u4e2d\u5e76\u751f\u6210\u53ef\u89e3\u91ca\u62a5\u544a\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cUniShield\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u7edf\u4e00\u65b9\u6cd5\u548c\u9886\u57df\u7279\u5b9a\u68c0\u6d4b\u5668\uff0c\u7a81\u51fa\u4e86\u5176\u5353\u8d8a\u7684\u5b9e\u7528\u6027\u3001\u81ea\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "UniShield\u4e3a\u89e3\u51b3\u56fe\u50cf\u4f2a\u9020\u68c0\u6d4b\u7684\u5b9e\u9645\u5e94\u7528\u9650\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u7684\u793e\u4f1a\u5b89\u5168\u610f\u4e49\u3002"}}
{"id": "2510.03191", "pdf": "https://arxiv.org/pdf/2510.03191", "abs": "https://arxiv.org/abs/2510.03191", "authors": ["Denis Zavadski", "Nikita Philip Tatsch", "Carsten Rother"], "title": "Product-Quantised Image Representation for High-Quality Image Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Product quantisation (PQ) is a classical method for scalable vector encoding, yet it has seen limited usage for latent representations in high-fidelity image generation. In this work, we introduce PQGAN, a quantised image autoencoder that integrates PQ into the well-known vector quantisation (VQ) framework of VQGAN. PQGAN achieves a noticeable improvement over state-of-the-art methods in terms of reconstruction performance, including both quantisation methods and their continuous counterparts. We achieve a PSNR score of 37dB, where prior work achieves 27dB, and are able to reduce the FID, LPIPS, and CMMD score by up to 96%. Our key to success is a thorough analysis of the interaction between codebook size, embedding dimensionality, and subspace factorisation, with vector and scalar quantisation as special cases. We obtain novel findings, such that the performance of VQ and PQ behaves in opposite ways when scaling the embedding dimension. Furthermore, our analysis shows performance trends for PQ that help guide optimal hyperparameter selection. Finally, we demonstrate that PQGAN can be seamlessly integrated into pre-trained diffusion models. This enables either a significantly faster and more compute-efficient generation, or a doubling of the output resolution at no additional cost, positioning PQ as a strong extension for discrete latent representation in image synthesis.", "AI": {"tldr": "PQGAN\u5c06\u4ea7\u54c1\u91cf\u5316(PQ)\u96c6\u6210\u5230VQGAN\u6846\u67b6\u4e2d\uff0c\u5728\u56fe\u50cf\u91cd\u5efa\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cPSNR\u8fbe\u523037dB\uff0c\u5e76\u80fd\u65e0\u7f1d\u96c6\u6210\u5230\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u4ea7\u54c1\u91cf\u5316(PQ)\u5728\u53ef\u6269\u5c55\u5411\u91cf\u7f16\u7801\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5728\u9ad8\u4fdd\u771f\u56fe\u50cf\u751f\u6210\u7684\u6f5c\u5728\u8868\u793a\u4e2d\u5e94\u7528\u6709\u9650\uff0c\u4f5c\u8005\u5e0c\u671b\u5c06PQ\u96c6\u6210\u5230VQGAN\u6846\u67b6\u4e2d\u63d0\u5347\u6027\u80fd\u3002", "method": "\u63d0\u51faPQGAN\uff0c\u5728VQGAN\u7684\u5411\u91cf\u91cf\u5316\u6846\u67b6\u4e2d\u96c6\u6210\u4ea7\u54c1\u91cf\u5316\uff0c\u6df1\u5165\u5206\u6790\u7801\u672c\u5927\u5c0f\u3001\u5d4c\u5165\u7ef4\u5ea6\u548c\u5b50\u7a7a\u95f4\u5206\u89e3\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "PSNR\u8fbe\u523037dB\uff08\u5148\u524d\u5de5\u4f5c\u4e3a27dB\uff09\uff0cFID\u3001LPIPS\u548cCMMD\u5206\u6570\u964d\u4f4e\u9ad8\u8fbe96%\uff0c\u5e76\u80fd\u5b9e\u73b0\u66f4\u5feb\u7684\u751f\u6210\u901f\u5ea6\u6216\u53cc\u500d\u8f93\u51fa\u5206\u8fa8\u7387\u3002", "conclusion": "PQGAN\u5c55\u793a\u4e86\u4ea7\u54c1\u91cf\u5316\u5728\u56fe\u50cf\u5408\u6210\u4e2d\u4f5c\u4e3a\u79bb\u6563\u6f5c\u5728\u8868\u793a\u7684\u5f3a\u5927\u6269\u5c55\u80fd\u529b\uff0c\u4e3a\u8d85\u53c2\u6570\u9009\u62e9\u63d0\u4f9b\u4e86\u6307\u5bfc\u6027\u8d8b\u52bf\u5206\u6790\u3002"}}
{"id": "2510.02425", "pdf": "https://arxiv.org/pdf/2510.02425", "abs": "https://arxiv.org/abs/2510.02425", "authors": ["Sophie L. Wang", "Phillip Isola", "Brian Cheung"], "title": "Words That Make Language Models Perceive", "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) trained purely on text ostensibly lack any direct perceptual experience, yet their internal representations are implicitly shaped by multimodal regularities encoded in language. We test the hypothesis that explicit sensory prompting can surface this latent structure, bringing a text-only LLM into closer representational alignment with specialist vision and audio encoders. When a sensory prompt tells the model to 'see' or 'hear', it cues the model to resolve its next-token predictions as if they were conditioned on latent visual or auditory evidence that is never actually supplied. Our findings reveal that lightweight prompt engineering can reliably activate modality-appropriate representations in purely text-trained LLMs.", "AI": {"tldr": "\u901a\u8fc7\u611f\u5b98\u63d0\u793a\uff08\u5982'\u770b'\u6216'\u542c'\uff09\u53ef\u4ee5\u6fc0\u6d3b\u7eaf\u6587\u672c\u8bad\u7ec3\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u591a\u6a21\u6001\u8868\u5f81\u80fd\u529b\uff0c\u4f7f\u5176\u4e0e\u4e13\u4e1a\u89c6\u89c9\u548c\u97f3\u9891\u7f16\u7801\u5668\u7684\u8868\u5f81\u5bf9\u9f50\u3002", "motivation": "\u63a2\u7d22\u7eaf\u6587\u672c\u8bad\u7ec3\u7684LLMs\u662f\u5426\u5305\u542b\u9690\u5f0f\u7684\u591a\u6a21\u6001\u7ed3\u6784\uff0c\u4ee5\u53ca\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u611f\u5b98\u63d0\u793a\u6765\u6fc0\u6d3b\u8fd9\u4e9b\u6f5c\u5728\u8868\u5f81\u3002", "method": "\u4f7f\u7528\u611f\u5b98\u63d0\u793a\uff08\u5982'\u770b'\u6216'\u542c'\uff09\u6765\u5f15\u5bfc\u6a21\u578b\uff0c\u8ba9\u6a21\u578b\u5728\u751f\u6210\u4e0b\u4e00\u4e2atoken\u65f6\u8868\u73b0\u5f97\u597d\u50cf\u57fa\u4e8e\u6f5c\u5728\u7684\u89c6\u89c9\u6216\u542c\u89c9\u8bc1\u636e\u3002", "result": "\u8f7b\u91cf\u7ea7\u7684\u63d0\u793a\u5de5\u7a0b\u53ef\u4ee5\u53ef\u9760\u5730\u6fc0\u6d3b\u7eaf\u6587\u672c\u8bad\u7ec3LLMs\u4e2d\u4e0e\u6a21\u6001\u76f8\u9002\u5e94\u7684\u8868\u5f81\u3002", "conclusion": "\u7eaf\u6587\u672c\u8bad\u7ec3\u7684LLMs\u786e\u5b9e\u5305\u542b\u9690\u5f0f\u7684\u591a\u6a21\u6001\u7ed3\u6784\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u611f\u5b98\u63d0\u793a\u53ef\u4ee5\u6fc0\u6d3b\u8fd9\u4e9b\u8868\u5f81\u80fd\u529b\u3002"}}
{"id": "2510.02469", "pdf": "https://arxiv.org/pdf/2510.02469", "abs": "https://arxiv.org/abs/2510.02469", "authors": ["Sung-Yeon Park", "Adam Lee", "Juanwu Lu", "Can Cui", "Luyang Jiang", "Rohit Gupta", "Kyungtae Han", "Ahmadreza Moradipari", "Ziran Wang"], "title": "SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D Gaussian Splatting", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Driving scene manipulation with sensor data is emerging as a promising alternative to traditional virtual driving simulators. However, existing frameworks struggle to generate realistic scenarios efficiently due to limited editing capabilities. To address these challenges, we present SIMSplat, a predictive driving scene editor with language-aligned Gaussian splatting. As a language-controlled editor, SIMSplat enables intuitive manipulation using natural language prompts. By aligning language with Gaussian-reconstructed scenes, it further supports direct querying of road objects, allowing precise and flexible editing. Our method provides detailed object-level editing, including adding new objects and modifying the trajectories of both vehicles and pedestrians, while also incorporating predictive path refinement through multi-agent motion prediction to generate realistic interactions among all agents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's extensive editing capabilities and adaptability across a wide range of scenarios. Project page: https://sungyeonparkk.github.io/simsplat/", "AI": {"tldr": "SIMSplat\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bed\u8a00\u5bf9\u9f50\u9ad8\u65af\u6cfc\u6e85\u7684\u9884\u6d4b\u6027\u9a7e\u9a76\u573a\u666f\u7f16\u8f91\u5668\uff0c\u652f\u6301\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u76f4\u89c2\u5730\u64cd\u4f5c\u9a7e\u9a76\u573a\u666f\uff0c\u80fd\u591f\u6dfb\u52a0\u65b0\u5bf9\u8c61\u3001\u4fee\u6539\u8f66\u8f86\u548c\u884c\u4eba\u8f68\u8ff9\uff0c\u5e76\u96c6\u6210\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u9884\u6d4b\u6765\u751f\u6210\u903c\u771f\u7684\u4ea4\u4e92\u3002", "motivation": "\u73b0\u6709\u7684\u9a7e\u9a76\u573a\u666f\u7f16\u8f91\u6846\u67b6\u7531\u4e8e\u7f16\u8f91\u80fd\u529b\u6709\u9650\uff0c\u96be\u4ee5\u9ad8\u6548\u751f\u6210\u903c\u771f\u573a\u666f\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u76f4\u89c2\u64cd\u4f5c\u4e14\u652f\u6301\u7cbe\u786e\u7f16\u8f91\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u8bed\u8a00\u5bf9\u9f50\u7684\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u4e0e\u9ad8\u65af\u91cd\u5efa\u573a\u666f\u5bf9\u9f50\uff0c\u652f\u6301\u76f4\u63a5\u67e5\u8be2\u9053\u8def\u5bf9\u8c61\u3002\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u9884\u6d4b\u8fdb\u884c\u9884\u6d4b\u6027\u8def\u5f84\u4f18\u5316\uff0c\u5b9e\u73b0\u903c\u771f\u7684\u4ea4\u4e92\u751f\u6210\u3002", "result": "\u5728Waymo\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSIMSplat\u5177\u6709\u5e7f\u6cdb\u7684\u7f16\u8f91\u80fd\u529b\u548c\u8de8\u573a\u666f\u9002\u5e94\u6027\u3002", "conclusion": "SIMSplat\u63d0\u4f9b\u4e86\u4e00\u4e2a\u76f4\u89c2\u4e14\u529f\u80fd\u5f3a\u5927\u7684\u9a7e\u9a76\u573a\u666f\u7f16\u8f91\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u751f\u6210\u903c\u771f\u7684\u4ea4\u4e92\u573a\u666f\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u6a21\u62df\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2510.02514", "pdf": "https://arxiv.org/pdf/2510.02514", "abs": "https://arxiv.org/abs/2510.02514", "authors": ["Guy Ohayon", "Pierre-Etienne H. Fiquet", "Florentin Guth", "Jona Ball\u00e9", "Eero P. Simoncelli"], "title": "Learning a distance measure from the information-estimation geometry of data", "categories": ["eess.IV", "cs.CV", "cs.IT", "eess.SP", "math.IT", "stat.ML"], "comment": "Code available at   https://github.com/ohayonguy/information-estimation-metric", "summary": "We introduce the Information-Estimation Metric (IEM), a novel form of distance function derived from an underlying continuous probability density over a domain of signals. The IEM is rooted in a fundamental relationship between information theory and estimation theory, which links the log-probability of a signal with the errors of an optimal denoiser, applied to noisy observations of the signal. In particular, the IEM between a pair of signals is obtained by comparing their denoising error vectors over a range of noise amplitudes. Geometrically, this amounts to comparing the score vector fields of the blurred density around the signals over a range of blur levels. We prove that the IEM is a valid global metric and derive a closed-form expression for its local second-order approximation, which yields a Riemannian metric. For Gaussian-distributed signals, the IEM coincides with the Mahalanobis distance. But for more complex distributions, it adapts, both locally and globally, to the geometry of the distribution. In practice, the IEM can be computed using a learned denoiser (analogous to generative diffusion models) and solving a one-dimensional integral. To demonstrate the value of our framework, we learn an IEM on the ImageNet database. Experiments show that this IEM is competitive with or outperforms state-of-the-art supervised image quality metrics in predicting human perceptual judgments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u8bba\u548c\u4f30\u8ba1\u7406\u8bba\u7684\u65b0\u578b\u8ddd\u79bb\u5ea6\u91cfIEM\uff0c\u901a\u8fc7\u6bd4\u8f83\u4fe1\u53f7\u5728\u4e0d\u540c\u566a\u58f0\u6c34\u5e73\u4e0b\u7684\u53bb\u566a\u8bef\u5dee\u5411\u91cf\u6765\u5b9a\u4e49\u8ddd\u79bb\uff0c\u80fd\u591f\u9002\u5e94\u590d\u6742\u5206\u5e03\u7684\u51e0\u4f55\u7ed3\u6784\u3002", "motivation": "\u4f20\u7edf\u8ddd\u79bb\u5ea6\u91cf\u65e0\u6cd5\u5f88\u597d\u5730\u9002\u5e94\u590d\u6742\u6982\u7387\u5206\u5e03\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4ece\u6570\u636e\u5206\u5e03\u4e2d\u5b66\u4e60\u5e76\u9002\u5e94\u5176\u51e0\u4f55\u7279\u6027\u7684\u8ddd\u79bb\u5ea6\u91cf\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u4fe1\u606f\u8bba\u4e0e\u4f30\u8ba1\u7406\u8bba\u7684\u57fa\u672c\u5173\u7cfb\uff0c\u901a\u8fc7\u6bd4\u8f83\u4fe1\u53f7\u5728\u4e0d\u540c\u566a\u58f0\u632f\u5e45\u4e0b\u7684\u6700\u4f18\u53bb\u566a\u5668\u8bef\u5dee\u5411\u91cf\u6765\u5b9a\u4e49IEM\uff0c\u4f7f\u7528\u5b66\u4e60\u5230\u7684\u53bb\u566a\u5668\u8ba1\u7b97\u8be5\u5ea6\u91cf\u3002", "result": "\u5728ImageNet\u6570\u636e\u5e93\u4e0a\u5b66\u4e60\u7684IEM\u5728\u9884\u6d4b\u4eba\u7c7b\u611f\u77e5\u5224\u65ad\u65b9\u9762\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u76d1\u7763\u56fe\u50cf\u8d28\u91cf\u5ea6\u91cf\u65b9\u6cd5\u7ade\u4e89\u6216\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "IEM\u662f\u4e00\u79cd\u6709\u6548\u7684\u5168\u5c40\u5ea6\u91cf\uff0c\u80fd\u591f\u9002\u5e94\u590d\u6742\u5206\u5e03\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5728\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u7b49\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.02713", "pdf": "https://arxiv.org/pdf/2510.02713", "abs": "https://arxiv.org/abs/2510.02713", "authors": ["Se-Ho Lee", "Keunsoo Ko", "Seung-Wook Kim"], "title": "Image Enhancement Based on Pigment Representation", "categories": ["eess.IV", "cs.CV"], "comment": "14 pages, 9 figures, accepted at IEEE Transactions on Multimedia   (TMM)", "summary": "This paper presents a novel and efficient image enhancement method based on pigment representation. Unlike conventional methods where the color transformation is restricted to pre-defined color spaces like RGB, our method dynamically adapts to input content by transforming RGB colors into a high-dimensional feature space referred to as \\textit{pigments}. The proposed pigment representation offers adaptability and expressiveness, achieving superior image enhancement performance. The proposed method involves transforming input RGB colors into high-dimensional pigments, which are then reprojected individually and blended to refine and aggregate the information of the colors in pigment spaces. Those pigments are then transformed back into RGB colors to generate an enhanced output image. The transformation and reprojection parameters are derived from the visual encoder which adaptively estimates such parameters based on the content in the input image. Extensive experimental results demonstrate the superior performance of the proposed method over state-of-the-art methods in image enhancement tasks, including image retouching and tone mapping, while maintaining relatively low computational complexity and small model size.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u989c\u6599\u8868\u793a\u7684\u65b0\u578b\u9ad8\u6548\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06RGB\u989c\u8272\u8f6c\u6362\u4e3a\u9ad8\u7ef4\u989c\u6599\u7279\u5f81\u7a7a\u95f4\uff0c\u52a8\u6001\u9002\u5e94\u8f93\u5165\u5185\u5bb9\uff0c\u5b9e\u73b0\u5353\u8d8a\u7684\u56fe\u50cf\u589e\u5f3a\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c40\u9650\u4e8e\u9884\u5b9a\u4e49\u8272\u5f69\u7a7a\u95f4\uff08\u5982RGB\uff09\uff0c\u65e0\u6cd5\u52a8\u6001\u9002\u5e94\u8f93\u5165\u5185\u5bb9\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u8868\u8fbe\u80fd\u529b\u66f4\u5f3a\u7684\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u3002", "method": "\u5c06\u8f93\u5165RGB\u989c\u8272\u8f6c\u6362\u4e3a\u9ad8\u7ef4\u989c\u6599\u8868\u793a\uff0c\u5728\u989c\u6599\u7a7a\u95f4\u4e2d\u8fdb\u884c\u5355\u72ec\u91cd\u6295\u5f71\u548c\u6df7\u5408\uff0c\u7136\u540e\u8f6c\u6362\u56deRGB\u751f\u6210\u589e\u5f3a\u56fe\u50cf\u3002\u8f6c\u6362\u548c\u91cd\u6295\u5f71\u53c2\u6570\u7531\u89c6\u89c9\u7f16\u7801\u5668\u6839\u636e\u8f93\u5165\u56fe\u50cf\u5185\u5bb9\u81ea\u9002\u5e94\u4f30\u8ba1\u3002", "result": "\u5728\u56fe\u50cf\u4fee\u9970\u548c\u8272\u8c03\u6620\u5c04\u7b49\u56fe\u50cf\u589e\u5f3a\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u8f83\u5c0f\u7684\u6a21\u578b\u5c3a\u5bf8\u3002", "conclusion": "\u989c\u6599\u8868\u793a\u65b9\u6cd5\u4e3a\u56fe\u50cf\u589e\u5f3a\u63d0\u4f9b\u4e86\u9002\u5e94\u6027\u548c\u8868\u8fbe\u80fd\u529b\uff0c\u5728\u4fdd\u6301\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2510.02730", "pdf": "https://arxiv.org/pdf/2510.02730", "abs": "https://arxiv.org/abs/2510.02730", "authors": ["Nishanth Shetty", "Madhava Prasath", "Chandra Sekhar Seelamantula"], "title": "Dale meets Langevin: A Multiplicative Denoising Diffusion Model", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Gradient descent has proven to be a powerful and effective technique for optimization in numerous machine learning applications. Recent advances in computational neuroscience have shown that learning in standard gradient descent optimization formulation is not consistent with learning in biological systems. This has opened up interesting avenues for building biologically inspired learning techniques. One such approach is inspired by Dale's law, which states that inhibitory and excitatory synapses do not swap roles during the course of learning. The resulting exponential gradient descent optimization scheme leads to log-normally distributed synaptic weights. Interestingly, the density that satisfies the Fokker-Planck equation corresponding to the stochastic differential equation (SDE) with geometric Brownian motion (GBM) is the log-normal density. Leveraging this connection, we start with the SDE governing geometric Brownian motion, and show that discretizing the corresponding reverse-time SDE yields a multiplicative update rule, which surprisingly, coincides with the sampling equivalent of the exponential gradient descent update founded on Dale's law. Furthermore, we propose a new formalism for multiplicative denoising score-matching, subsuming the loss function proposed by Hyvaerinen for non-negative data. Indeed, log-normally distributed data is positive and the proposed score-matching formalism turns out to be a natural fit. This allows for training of score-based models for image data and results in a novel multiplicative update scheme for sample generation starting from a log-normal density. Experimental results on MNIST, Fashion MNIST, and Kuzushiji datasets demonstrate generative capability of the new scheme. To the best of our knowledge, this is the first instance of a biologically inspired generative model employing multiplicative updates, founded on geometric Brownian motion.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u5e03\u6717\u8fd0\u52a8\u7684\u751f\u7269\u542f\u53d1\u5f0f\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u4e58\u6027\u66f4\u65b0\u65b9\u6848\u5b9e\u73b0\u56fe\u50cf\u751f\u6210\uff0c\u5c06\u6307\u6570\u68af\u5ea6\u4e0b\u964d\u4e0e\u5206\u6570\u5339\u914d\u76f8\u7ed3\u5408\u3002", "motivation": "\u6807\u51c6\u68af\u5ea6\u4e0b\u964d\u4e0e\u751f\u7269\u5b66\u4e60\u673a\u5236\u4e0d\u4e00\u81f4\uff0c\u53d7Dale\u5b9a\u5f8b\u542f\u53d1\uff0c\u7814\u7a76\u751f\u7269\u542f\u53d1\u7684\u5b66\u4e60\u6280\u672f\uff0c\u7279\u522b\u662f\u57fa\u4e8e\u51e0\u4f55\u5e03\u6717\u8fd0\u52a8\u7684\u4e58\u6027\u66f4\u65b0\u65b9\u6848\u3002", "method": "\u4ece\u51e0\u4f55\u5e03\u6717\u8fd0\u52a8\u7684\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u51fa\u53d1\uff0c\u79bb\u6563\u5316\u53cd\u5411\u65f6\u95f4SDE\u5f97\u5230\u4e58\u6027\u66f4\u65b0\u89c4\u5219\uff0c\u63d0\u51fa\u4e58\u6027\u53bb\u566a\u5206\u6570\u5339\u914d\u5f62\u5f0f\u5316\u65b9\u6cd5\u3002", "result": "\u5728MNIST\u3001Fashion MNIST\u548cKuzushiji\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u65b0\u65b9\u6848\u7684\u751f\u6210\u80fd\u529b\uff0c\u6743\u91cd\u5448\u5bf9\u6570\u6b63\u6001\u5206\u5e03\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8e\u51e0\u4f55\u5e03\u6717\u8fd0\u52a8\u3001\u91c7\u7528\u4e58\u6027\u66f4\u65b0\u7684\u751f\u7269\u542f\u53d1\u5f0f\u751f\u6210\u6a21\u578b\uff0c\u4e3a\u751f\u7269\u542f\u53d1\u7684\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
