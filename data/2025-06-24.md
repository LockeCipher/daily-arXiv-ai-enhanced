<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 6]
- [cs.CV](#cs.CV) [Total: 43]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing](https://arxiv.org/abs/2506.17450)
*Jiacheng Chen,Ramin Mehran,Xuhui Jia,Saining Xie,Sanghyun Woo*

Main category: cs.GR

TL;DR: BlenderFusion是一个生成式视觉合成框架，通过重新组合物体、相机和背景来合成新场景。


<details>
  <summary>Details</summary>
Motivation: 旨在解决复杂场景编辑任务中的灵活性和控制性问题。

Method: 采用分层-编辑-合成流程，结合预训练的扩散模型和两种关键训练策略（源掩码和模拟物体抖动）。

Result: 在复杂场景编辑任务中显著优于现有方法。

Conclusion: BlenderFusion为生成式场景编辑提供了高效且灵活的工具。

Abstract: We present BlenderFusion, a generative visual compositing framework that synthesizes new scenes by recomposing objects, camera, and background. It follows a layering-editing-compositing pipeline: (i) segmenting and converting visual inputs into editable 3D entities (layering), (ii) editing them in Blender with 3D-grounded control (editing), and (iii) fusing them into a coherent scene using a generative compositor (compositing). Our generative compositor extends a pre-trained diffusion model to process both the original (source) and edited (target) scenes in parallel. It is fine-tuned on video frames with two key training strategies: (i) source masking, enabling flexible modifications like background replacement; (ii) simulated object jittering, facilitating disentangled control over objects and camera. BlenderFusion significantly outperforms prior methods in complex compositional scene editing tasks.

</details>


### [2] [3D Gaussian Splatting for Fine-Detailed Surface Reconstruction in Large-Scale Scene](https://arxiv.org/abs/2506.17636)
*Shihan Chen,Zhaojin Li,Zeyu Chen,Qingsong Yan,Gaoyang Shen,Ran Duan*

Main category: cs.GR

TL;DR: 提出了一种用于大规模场景表面重建的新方法，通过粗到细策略、自适应分区和外观解耦模型，解决了计算复杂性和动态外观问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法在大规模场景中面临计算复杂性和动态外观的挑战，限制了其在航测和自动驾驶中的应用。

Method: 采用粗到细策略、自适应场景分区、外观解耦模型和瞬态掩模模型，结合多视角约束和单视角正则化。

Result: 在GauU-Scene V2数据集上表现优于现有NeRF和高斯方法，实现了高保真视觉效果和精确表面重建。

Conclusion: 该方法有效解决了大规模场景重建的挑战，具有实际应用潜力，代码将开源。

Abstract: Recent developments in 3D Gaussian Splatting have made significant advances in surface reconstruction. However, scaling these methods to large-scale scenes remains challenging due to high computational demands and the complex dynamic appearances typical of outdoor environments. These challenges hinder the application in aerial surveying and autonomous driving. This paper proposes a novel solution to reconstruct large-scale surfaces with fine details, supervised by full-sized images. Firstly, we introduce a coarse-to-fine strategy to reconstruct a coarse model efficiently, followed by adaptive scene partitioning and sub-scene refining from image segments. Additionally, we integrate a decoupling appearance model to capture global appearance variations and a transient mask model to mitigate interference from moving objects. Finally, we expand the multi-view constraint and introduce a single-view regularization for texture-less areas. Our experiments were conducted on the publicly available dataset GauU-Scene V2, which was captured using unmanned aerial vehicles. To the best of our knowledge, our method outperforms existing NeRF-based and Gaussian-based methods, achieving high-fidelity visual results and accurate surface from full-size image optimization. Open-source code will be available on GitHub.

</details>


### [3] [Morse: Dual-Sampling for Lossless Acceleration of Diffusion Models](https://arxiv.org/abs/2506.18251)
*Chao Li,Jiawei Fan,Anbang Yao*

Main category: cs.GR

TL;DR: Morse是一个无损加速扩散模型的双采样框架，通过快速跳跃采样和自适应残差反馈策略提升效率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成过程通常耗时较长，Morse旨在无损加速这一过程，同时保持生成质量。

Method: Morse包含Dash和Dot两个模型：Dash为预训练扩散模型，采用跳跃采样；Dot生成残差反馈，与Dash交替运行以提升效率。

Result: 在6个图像生成任务中，Morse实现了1.78X到3.31X的无损加速，且适用于Latent Consistency Model。

Conclusion: Morse通过双采样框架和权重共享策略，显著提升了扩散模型的运行效率，同时保持生成质量。

Abstract: In this paper, we present Morse, a simple dual-sampling framework for accelerating diffusion models losslessly. The key insight of Morse is to reformulate the iterative generation (from noise to data) process via taking advantage of fast jump sampling and adaptive residual feedback strategies. Specifically, Morse involves two models called Dash and Dot that interact with each other. The Dash model is just the pre-trained diffusion model of any type, but operates in a jump sampling regime, creating sufficient space for sampling efficiency improvement. The Dot model is significantly faster than the Dash model, which is learnt to generate residual feedback conditioned on the observations at the current jump sampling point on the trajectory of the Dash model, lifting the noise estimate to easily match the next-step estimate of the Dash model without jump sampling. By chaining the outputs of the Dash and Dot models run in a time-interleaved fashion, Morse exhibits the merit of flexibly attaining desired image generation performance while improving overall runtime efficiency. With our proposed weight sharing strategy between the Dash and Dot models, Morse is efficient for training and inference. Our method shows a lossless speedup of 1.78X to 3.31X on average over a wide range of sampling step budgets relative to 9 baseline diffusion models on 6 image generation tasks. Furthermore, we show that our method can be also generalized to improve the Latent Consistency Model (LCM-SDXL, which is already accelerated with consistency distillation technique) tailored for few-step text-to-image synthesis. The code and models are available at https://github.com/deep-optimization/Morse.

</details>


### [4] [What You Think Is What You Get: Bridge User Intent and Transfer Function Design through Multimodal Large Language Models](https://arxiv.org/abs/2506.18407)
*Yiyao Wang,Bo Pan,Ke Wang,Han Liu,Jinyuan Mao,Yuxin Liu,Minfeng Zhu,Bo Zhang,Weifeng Chen,Xiuqi Huang,Wei Chen*

Main category: cs.GR

TL;DR: 论文提出了一种名为WYTWYG的框架，利用多模态大语言模型（MLLMs）基于用户意图指导传递函数（TF）优化，解决了现有方法探索空间大和泛化能力弱的问题。


<details>
  <summary>Details</summary>
Motivation: 传递函数（TFs）在直接体积渲染（DVR）中至关重要，但设计有效的TFs仍不直观，现有优化方法面临探索空间大和泛化能力弱的挑战。

Method: 提出WYTWYG框架，包含基于进化的探索器和基于MLLMs的体积渲染质量评估器，并开发了交互式TF设计系统。

Result: 通过三个案例研究展示了框架的通用性，并通过实验验证了各组件有效性。

Conclusion: WYTWYG框架成功解决了TF优化中的探索空间和泛化问题，为DVR提供了更直观的设计工具。

Abstract: Direct volume rendering (DVR) is a fundamental technique for visualizing volumetric data, with transfer functions (TFs) playing a crucial role in extracting meaningful structures. However, designing effective TFs remains unintuitive due to the semantic gap between user intent and TF parameter space. Researchers have developed numerous TF optimization methods to bridge this gap. However, existing methods still face two challenges: large exploration space and weak generalizability. To address these issues, we propose What You Think is What You Get (WYTWYG) framework, which leveraging Multi-model Large Language Models (MLLMs) to guide the TF optimization based on user intent. Specifically, we first introduce a novel TF optimization approach comprising two core components: (1) an evolution-based explorer for effective exploration of the TF space, and (2) a volume rendering quality evaluator based on MLLMs to provide generalizable visual guidance. We further propose a TF interactive design system based on this approach. We demonstrate the general applicability of our framework through three case studies, and validate the effectiveness of each component through extensive experiments. Our code is available at: https://github.com/wyysteelhead/TFevolve.

</details>


### [5] [BulletGen: Improving 4D Reconstruction with Bullet-Time Generation](https://arxiv.org/abs/2506.18601)
*Denys Rozumnyi,Jonathon Luiten,Numair Khan,Johannes Schönberger,Peter Kontschieder*

Main category: cs.GR

TL;DR: BulletGen利用生成模型纠正高斯动态场景表示中的错误并补全缺失信息，通过扩散视频生成模型与4D重建对齐，实现沉浸式动态体验。


<details>
  <summary>Details</summary>
Motivation: 解决从单目视频重建动态场景时的未观测区域重建和深度估计模糊问题。

Method: 结合生成模型与高斯动态场景表示，通过扩散模型生成帧监督4D高斯模型优化。

Result: 在新视角合成和2D/3D跟踪任务中达到最先进水平。

Conclusion: BulletGen成功融合生成内容与静态/动态场景组件，提升动态场景重建质量。

Abstract: Transforming casually captured, monocular videos into fully immersive dynamic experiences is a highly ill-posed task, and comes with significant challenges, e.g., reconstructing unseen regions, and dealing with the ambiguity in monocular depth estimation. In this work we introduce BulletGen, an approach that takes advantage of generative models to correct errors and complete missing information in a Gaussian-based dynamic scene representation. This is done by aligning the output of a diffusion-based video generation model with the 4D reconstruction at a single frozen "bullet-time" step. The generated frames are then used to supervise the optimization of the 4D Gaussian model. Our method seamlessly blends generative content with both static and dynamic scene components, achieving state-of-the-art results on both novel-view synthesis, and 2D/3D tracking tasks.

</details>


### [6] [DuetGen: Music Driven Two-Person Dance Generation via Hierarchical Masked Modeling](https://arxiv.org/abs/2506.18680)
*Anindita Ghosh,Bing Zhou,Rishabh Dabral,Jian Wang,Vladislav Golyanik,Christian Theobalt,Philipp Slusallek,Chuan Guo*

Main category: cs.GR

TL;DR: DuetGen是一个新颖的框架，用于从音乐生成交互式双人舞蹈，通过两阶段方法（编码和生成）和分层掩码建模实现同步和互动。


<details>
  <summary>Details</summary>
Motivation: 双人舞蹈的同步和互动复杂性是主要挑战，需要解决舞伴之间以及与音乐的协调问题。

Method: 采用两阶段方法：1) 使用VQ-VAE将双人动作编码为离散标记；2) 使用掩码变换器从音乐生成这些标记，分层处理高、低级别特征。

Result: 在基准数据集上，DuetGen在动作真实性、音乐舞蹈对齐和舞伴协调方面表现优异。

Conclusion: DuetGen通过分层建模和交互表示，成功生成了同步且互动的双人舞蹈，性能领先。

Abstract: We present DuetGen, a novel framework for generating interactive two-person dances from music. The key challenge of this task lies in the inherent complexities of two-person dance interactions, where the partners need to synchronize both with each other and with the music. Inspired by the recent advances in motion synthesis, we propose a two-stage solution: encoding two-person motions into discrete tokens and then generating these tokens from music. To effectively capture intricate interactions, we represent both dancers' motions as a unified whole to learn the necessary motion tokens, and adopt a coarse-to-fine learning strategy in both the stages. Our first stage utilizes a VQ-VAE that hierarchically separates high-level semantic features at a coarse temporal resolution from low-level details at a finer resolution, producing two discrete token sequences at different abstraction levels. Subsequently, in the second stage, two generative masked transformers learn to map music signals to these dance tokens: the first producing high-level semantic tokens, and the second, conditioned on music and these semantic tokens, producing the low-level tokens. We train both transformers to learn to predict randomly masked tokens within the sequence, enabling them to iteratively generate motion tokens by filling an empty token sequence during inference. Through the hierarchical masked modeling and dedicated interaction representation, DuetGen achieves the generation of synchronized and interactive two-person dances across various genres. Extensive experiments and user studies on a benchmark duet dance dataset demonstrate state-of-the-art performance of DuetGen in motion realism, music-dance alignment, and partner coordination.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [7] [Mechanistic Interpretability of Diffusion Models: Circuit-Level Analysis and Causal Validation](https://arxiv.org/abs/2506.17237)
*Dip Roy*

Main category: cs.CV

TL;DR: 该论文通过量化电路分析扩散模型，揭示了合成与自然数据处理的算法差异，并识别了八种功能不同的注意力机制。


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型在图像生成中的计算路径和机制原理，以理解其对合成与自然数据的不同处理方式。

Method: 通过对2,000张合成图像和2,000张CelebA人脸图像进行系统性干预实验，分析计算复杂性和注意力模式。

Result: 发现真实人脸处理需要更高计算复杂度（1.084±0.008），并识别了八种功能不同的注意力机制（如边缘检测、纹理分析等）。

Conclusion: 研究为生成模型行为的算法理解和控制提供了定量基础，支持通过机制干预策略优化模型性能。

Abstract: We present a quantitative circuit-level analysis of diffusion models, establishing computational pathways and mechanistic principles underlying image generation processes. Through systematic intervention experiments across 2,000 synthetic and 2,000 CelebA facial images, we discover fundamental algorithmic differences in how diffusion architectures process synthetic versus naturalistic data distributions. Our investigation reveals that real-world face processing requires circuits with measurably higher computational complexity (complexity ratio = 1.084 plus/minus 0.008, p < 0.001), exhibiting distinct attention specialization patterns with entropy divergence ranging from 0.015 to 0.166 across denoising timesteps. We identify eight functionally distinct attention mechanisms showing specialized computational roles: edge detection (entropy = 3.18 plus/minus 0.12), texture analysis (entropy = 4.16 plus/minus 0.08), and semantic understanding (entropy = 2.67 plus/minus 0.15). Intervention analysis demonstrates critical computational bottlenecks where targeted ablations produce 25.6% to 128.3% performance degradation, providing causal evidence for identified circuit functions. These findings establish quantitative foundations for algorithmic understanding and control of generative model behavior through mechanistic intervention strategies.

</details>


### [8] [Efficient Feedback Gate Network for Hyperspectral Image Super-Resolution](https://arxiv.org/abs/2506.17361)
*Xufei Wang,Mingjian Zhang,Fei Ge,Jinchen Zhu,Wen Sha,Jifen Ren,Zhimeng Hou,Shouguo Zheng,ling Zheng,Shizhuang Weng*

Main category: cs.CV

TL;DR: 本文提出了一种基于分组的高效反馈门网络（EFGN），用于单幅高光谱图像超分辨率（SHSR），通过反馈门操作和大核卷积提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有SHSR方法未能充分利用波段间和空间-光谱信息的连贯性，导致性能受限。

Method: 提出EFGN，结合反馈门操作、大核卷积和光谱交互；引入SPDFM模块学习波段信息和层次空间信息；开发SSRGM模块增强空间-光谱特征。

Result: 在三个高光谱数据集上，EFGN在光谱保真度和空间内容重建方面优于现有方法。

Conclusion: EFGN通过高效利用波段和空间-光谱信息，显著提升了SHSR的性能。

Abstract: Even without auxiliary images, single hyperspectral image super-resolution (SHSR) methods can be designed to improve the spatial resolution of hyperspectral images. However, failing to explore coherence thoroughly along bands and spatial-spectral information leads to the limited performance of the SHSR. In this study, we propose a novel group-based SHSR method termed the efficient feedback gate network, which uses various feedbacks and gate operations involving large kernel convolutions and spectral interactions. In particular, by providing different guidance for neighboring groups, we can learn rich band information and hierarchical hyperspectral spatial information using channel shuffling and dilatation convolution in shuffled and progressive dilated fusion module(SPDFM). Moreover, we develop a wide-bound perception gate block and a spectrum enhancement gate block to construct the spatial-spectral reinforcement gate module (SSRGM) and obtain highly representative spatial-spectral features efficiently. Additionally, we apply a three-dimensional SSRGM to enhance holistic information and coherence for hyperspectral data. The experimental results on three hyperspectral datasets demonstrate the superior performance of the proposed network over the state-of-the-art methods in terms of spectral fidelity and spatial content reconstruction.

</details>


### [9] [HIRE: Lightweight High-Resolution Image Feature Enrichment for Multimodal LLMs](https://arxiv.org/abs/2506.17608)
*Nikitha SR,Aradhya Neeraj Mathur,Tarun Ram Menta,Rishabh Jain,Mausoom Sarkar*

Main category: cs.CV

TL;DR: 提出一种浅层特征增强器，通过特征上采样实现高分辨率特征生成，显著降低计算成本，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 高分辨率图像特征在多模态大语言模型中表现优异，但计算成本高昂，需多次调用大型图像编码器。

Method: 提出浅层特征增强器，通过特征上采样生成高分辨率特征，减少计算负担。

Result: 实验表明，该方法在训练和推理时间上大幅减少，计算成本降低1.5倍FLOPs，同时保持竞争力。

Conclusion: 浅层特征增强器是一种高效的高分辨率特征生成方法，显著降低计算成本。

Abstract: The integration of high-resolution image features in modern multimodal large language models has demonstrated significant improvements in fine-grained visual understanding tasks, achieving high performance across multiple benchmarks. Since these features are obtained from large image encoders like ViT, they come with a significant increase in computational costs due to multiple calls to these encoders. In this work, we first develop an intuition for feature upsampling as a natural extension of high-resolution feature generation. Through extensive experiments and ablations, we demonstrate how a shallow feature enricher can achieve competitive results with tremendous reductions in training and inference time as well as computational cost, with upto 1.5x saving in FLOPs.

</details>


### [10] [DreamJourney: Perpetual View Generation with Video Diffusion Models](https://arxiv.org/abs/2506.17705)
*Bo Pan,Yang Chen,Yingwei Pan,Ting Yao,Wei Chen,Tao Mei*

Main category: cs.CV

TL;DR: DreamJourney提出了一种两阶段框架，利用视频扩散模型实现动态场景的长期视图生成，解决了现有方法在3D感知和动态对象运动方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法基于2D扩散模型，缺乏3D感知能力，且无法捕捉动态4D世界中的对象运动，导致生成视图失真且静态。

Method: DreamJourney分为两阶段：第一阶段通过3D点云和视频扩散模型生成一致的跨视图视频；第二阶段利用多模态大语言模型描述对象运动并动态生成视图。

Result: 实验表明，DreamJourney在定量和定性上均优于现有方法。

Conclusion: DreamJourney通过结合3D感知和动态对象运动，实现了更高质量的长期动态场景视图生成。

Abstract: Perpetual view generation aims to synthesize a long-term video corresponding to an arbitrary camera trajectory solely from a single input image. Recent methods commonly utilize a pre-trained text-to-image diffusion model to synthesize new content of previously unseen regions along camera movement. However, the underlying 2D diffusion model lacks 3D awareness and results in distorted artifacts. Moreover, they are limited to generating views of static 3D scenes, neglecting to capture object movements within the dynamic 4D world. To alleviate these issues, we present DreamJourney, a two-stage framework that leverages the world simulation capacity of video diffusion models to trigger a new perpetual scene view generation task with both camera movements and object dynamics. Specifically, in stage I, DreamJourney first lifts the input image to 3D point cloud and renders a sequence of partial images from a specific camera trajectory. A video diffusion model is then utilized as generative prior to complete the missing regions and enhance visual coherence across the sequence, producing a cross-view consistent video adheres to the 3D scene and camera trajectory. Meanwhile, we introduce two simple yet effective strategies (early stopping and view padding) to further stabilize the generation process and improve visual quality. Next, in stage II, DreamJourney leverages a multimodal large language model to produce a text prompt describing object movements in current view, and uses video diffusion model to animate current view with object movements. Stage I and II are repeated recurrently, enabling perpetual dynamic scene view generation. Extensive experiments demonstrate the superiority of our DreamJourney over state-of-the-art methods both quantitatively and qualitatively. Our project page: https://dream-journey.vercel.app.

</details>


### [11] [Programmable-Room: Interactive Textured 3D Room Meshes Generation Empowered by Large Language Models](https://arxiv.org/abs/2506.17707)
*Jihyun Kim,Junho Park,Kyeongbo Kong,Suk-Ju Kang*

Main category: cs.CV

TL;DR: Programmable-Room是一个通过自然语言指令交互生成和编辑3D房间网格的框架，结合视觉编程和预训练扩散模型，实现了高质量的房间生成和编辑。


<details>
  <summary>Details</summary>
Motivation: 解决通过自然语言精确控制3D房间网格生成和编辑的挑战，将复杂任务分解为多个简单步骤，并通过统一框架支持这些任务。

Method: 将任务分解为生成3D坐标、全景图像纹理、构建3D网格和家具布置，利用视觉编程（VP）和预训练扩散模型（优化训练目标）实现。

Result: 展示了框架在生成和编辑3D房间网格方面的灵活性，并在定量和定性上优于现有模型。

Conclusion: Programmable-Room通过自然语言指令和模块化设计，为3D房间生成和编辑提供了高效且灵活的解决方案。

Abstract: We present Programmable-Room, a framework which interactively generates and edits a 3D room mesh, given natural language instructions. For precise control of a room's each attribute, we decompose the challenging task into simpler steps such as creating plausible 3D coordinates for room meshes, generating panorama images for the texture, constructing 3D meshes by integrating the coordinates and panorama texture images, and arranging furniture. To support the various decomposed tasks with a unified framework, we incorporate visual programming (VP). VP is a method that utilizes a large language model (LLM) to write a Python-like program which is an ordered list of necessary modules for the various tasks given in natural language. We develop most of the modules. Especially, for the texture generating module, we utilize a pretrained large-scale diffusion model to generate panorama images conditioned on text and visual prompts (i.e., layout, depth, and semantic map) simultaneously. Specifically, we enhance the panorama image generation quality by optimizing the training objective with a 1D representation of a panorama scene obtained from bidirectional LSTM. We demonstrate Programmable-Room's flexibility in generating and editing 3D room meshes, and prove our framework's superiority to an existing model quantitatively and qualitatively. Project page is available in https://jihyun0510.github.io/Programmable_Room_Page/.

</details>


### [12] [PhysID: Physics-based Interactive Dynamics from a Single-view Image](https://arxiv.org/abs/2506.17746)
*Sourabh Vasant Gothe,Ayon Chattopadhyay,Gunturi Venkata Sai Phani Kiran,Pratik,Vibhav Agarwal,Jayesh Rajkumar Vachhani,Sourav Ghosh,Parameswaranath VM,Barath Raj KR*

Main category: cs.CV

TL;DR: PhysID利用生成模型从单视角图像创建物理交互动态，简化3D建模和物理属性校准，实现实时交互和个性化。


<details>
  <summary>Details</summary>
Motivation: 提升移动用户体验，通过交互和AR/VR应用，解决静态图像转化为交互体验的挑战。

Method: 利用大型生成模型进行3D网格生成和物理属性预测，结合设备端物理引擎实现实时渲染。

Result: 实验验证了多模态大语言模型的零样本能力和3D重建模型性能，证明了端到端框架的有效性。

Conclusion: PhysID在移动端交互动态领域取得显著进展，支持实时、非确定性交互和低内存消耗。

Abstract: Transforming static images into interactive experiences remains a challenging task in computer vision. Tackling this challenge holds the potential to elevate mobile user experiences, notably through interactive and AR/VR applications. Current approaches aim to achieve this either using pre-recorded video responses or requiring multi-view images as input. In this paper, we present PhysID, that streamlines the creation of physics-based interactive dynamics from a single-view image by leveraging large generative models for 3D mesh generation and physical property prediction. This significantly reduces the expertise required for engineering-intensive tasks like 3D modeling and intrinsic property calibration, enabling the process to be scaled with minimal manual intervention. We integrate an on-device physics-based engine for physically plausible real-time rendering with user interactions. PhysID represents a leap forward in mobile-based interactive dynamics, offering real-time, non-deterministic interactions and user-personalization with efficient on-device memory consumption. Experiments evaluate the zero-shot capabilities of various Multimodal Large Language Models (MLLMs) on diverse tasks and the performance of 3D reconstruction models. These results demonstrate the cohesive functioning of all modules within the end-to-end framework, contributing to its effectiveness.

</details>


### [13] [LoLA-SpecViT: Local Attention SwiGLU Vision Transformer with LoRA for Hyperspectral Imaging](https://arxiv.org/abs/2506.17759)
*Fadi Abdeladhim Zidi,Djamel Eddine Boukhari,Abdellah Zakaria Sellam,Abdelkrim Ouafi,Cosimo Distante,Salah Eddine Bekhouche,Abdelmalik Taleb-Ahmed*

Main category: cs.CV

TL;DR: LoLA-SpecViT是一种轻量级光谱视觉变换器，通过参数高效架构解决高光谱图像分类中的挑战，结合3D卷积和局部自注意力，显著减少参数并提升性能。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类面临高维数据、冗余和标注样本稀缺的挑战，现有变换器模型在标签稀缺条件下的扩展性和适应性不足。

Method: 提出LoLA-SpecViT，结合3D卷积前端和局部自注意力，集成低秩适应（LoRA）以减少参数，并采用循环学习率调度器优化训练。

Result: 在三个基准数据集上表现优异，最高达99.91%准确率，参数大幅减少，且在低标签条件下鲁棒性更强。

Conclusion: LoLA-SpecViT为高光谱图像应用提供了可扩展和通用的解决方案，适用于农业、环境监测等领域。

Abstract: Hyperspectral image classification remains a challenging task due to the high dimensionality of spectral data, significant inter-band redundancy, and the limited availability of annotated samples. While recent transformer-based models have improved the global modeling of spectral-spatial dependencies, their scalability and adaptability under label-scarce conditions remain limited. In this work, we propose \textbf{LoLA-SpecViT}(Low-rank adaptation Local Attention Spectral Vision Transformer), a lightweight spectral vision transformer that addresses these limitations through a parameter-efficient architecture tailored to the unique characteristics of hyperspectral imagery. Our model combines a 3D convolutional spectral front-end with local window-based self-attention, enhancing both spectral feature extraction and spatial consistency while reducing computational complexity. To further improve adaptability, we integrate low-rank adaptation (LoRA) into attention and projection layers, enabling fine-tuning with over 80\% fewer trainable parameters. A novel cyclical learning rate scheduler modulates LoRA adaptation strength during training, improving convergence and generalisation. Extensive experiments on three benchmark datasets WHU-Hi LongKou, WHU-Hi HongHu, and Salinas demonstrate that LoLA-SpecViT consistently outperforms state-of-the-art baselines, achieving up to 99.91\% accuracy with substantially fewer parameters and enhanced robustness under low-label regimes. The proposed framework provides a scalable and generalizable solution for real-world HSI applications in agriculture, environmental monitoring, and remote sensing analytics. Our code is available in the following \href{https://github.com/FadiZidiDz/LoLA-SpecViT}{GitHub Repository}.

</details>


### [14] [EgoWorld: Translating Exocentric View to Egocentric View using Rich Exocentric Observations](https://arxiv.org/abs/2506.17896)
*Junho Park,Andrew Sangwoo Ye,Taein Kwon*

Main category: cs.CV

TL;DR: EgoWorld是一个新颖的两阶段框架，通过外中心观测重建自我中心视角，解决了现有方法依赖2D线索和多视图同步的限制。


<details>
  <summary>Details</summary>
Motivation: 自我中心视角对于AR、VR和机器人应用至关重要，但现有方法依赖2D线索和同步多视图设置，且假设不切实际。

Method: EgoWorld通过外中心深度图重建点云，将其重投影到自我中心视角，并应用扩散修复生成密集且语义一致的图像。

Result: 在H2O和TACO数据集上表现优异，泛化能力强，适用于新对象、动作、场景和主体，甚至在未标记的真实世界示例中表现良好。

Conclusion: EgoWorld通过创新的两阶段框架，显著提升了外中心到自我中心视角转换的性能和实用性。

Abstract: Egocentric vision is essential for both human and machine visual understanding, particularly in capturing the detailed hand-object interactions needed for manipulation tasks. Translating third-person views into first-person views significantly benefits augmented reality (AR), virtual reality (VR) and robotics applications. However, current exocentric-to-egocentric translation methods are limited by their dependence on 2D cues, synchronized multi-view settings, and unrealistic assumptions such as necessity of initial egocentric frame and relative camera poses during inference. To overcome these challenges, we introduce EgoWorld, a novel two-stage framework that reconstructs an egocentric view from rich exocentric observations, including projected point clouds, 3D hand poses, and textual descriptions. Our approach reconstructs a point cloud from estimated exocentric depth maps, reprojects it into the egocentric perspective, and then applies diffusion-based inpainting to produce dense, semantically coherent egocentric images. Evaluated on the H2O and TACO datasets, EgoWorld achieves state-of-the-art performance and demonstrates robust generalization to new objects, actions, scenes, and subjects. Moreover, EgoWorld shows promising results even on unlabeled real-world examples.

</details>


### [15] [Enabling PSO-Secure Synthetic Data Sharing Using Diversity-Aware Diffusion Models](https://arxiv.org/abs/2506.17975)
*Mischa Dombrowski,Bernhard Kainz*

Main category: cs.CV

TL;DR: 论文提出了一种通过最大化合成数据多样性来保护隐私的方法，确保数据无法被用于识别个人，同时性能接近真实数据。


<details>
  <summary>Details</summary>
Motivation: 当前合成数据在医学影像中隐私保护潜力大，但法律合规性和性能不足，需改进。

Method: 提出了一种基于扩散模型的通用框架，生成非个性化合成数据，确保隐私。

Result: 合成数据性能接近真实数据（差距1%以内），优于现有非隐私保护方法。

Conclusion: 最大化多样性不仅提升性能，还能实现隐私保护，为合成数据共享提供新思路。

Abstract: Synthetic data has recently reached a level of visual fidelity that makes it nearly indistinguishable from real data, offering great promise for privacy-preserving data sharing in medical imaging. However, fully synthetic datasets still suffer from significant limitations: First and foremost, the legal aspect of sharing synthetic data is often neglected and data regulations, such as the GDPR, are largley ignored. Secondly, synthetic models fall short of matching the performance of real data, even for in-domain downstream applications. Recent methods for image generation have focused on maximising image diversity instead of fidelity solely to improve the mode coverage and therefore the downstream performance of synthetic data. In this work, we shift perspective and highlight how maximizing diversity can also be interpreted as protecting natural persons from being singled out, which leads to predicate singling-out (PSO) secure synthetic datasets. Specifically, we propose a generalisable framework for training diffusion models on personal data which leads to unpersonal synthetic datasets achieving performance within one percentage point of real-data models while significantly outperforming state-of-the-art methods that do not ensure privacy. Our code is available at https://github.com/MischaD/Trichotomy.

</details>


### [16] [ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image Generation](https://arxiv.org/abs/2506.18095)
*Junying Chen,Zhenyang Cai,Pengcheng Chen,Shunian Chen,Ke Ji,Xidong Wang,Yunjin Yang,Benyou Wang*

Main category: cs.CV

TL;DR: ShareGPT-4o-Image数据集和Janus-4o模型的发布旨在推动开放研究，实现高质量的文本到图像和文本加图像到图像的生成。


<details>
  <summary>Details</summary>
Motivation: 当前领先的多模态生成模型（如GPT-4o-Image）是专有的，无法公开访问。为了普及这些能力，作者提出了ShareGPT-4o-Image数据集和Janus-4o模型。

Method: 利用GPT-4o生成的45K文本到图像和46K文本加图像到图像数据，训练了Janus-4o模型，支持两种生成任务。

Result: Janus-4o在文本到图像生成上显著优于前代模型，并新增了文本加图像到图像生成功能，仅需91K合成样本和6小时训练即取得优异表现。

Conclusion: ShareGPT-4o-Image和Janus-4o的发布将促进开放研究，推动高质量图像生成的发展。

Abstract: Recent advances in multimodal generative models have unlocked photorealistic, instruction-aligned image generation, yet leading systems like GPT-4o-Image remain proprietary and inaccessible. To democratize these capabilities, we present ShareGPT-4o-Image, the first dataset comprising 45K text-to-image and 46K text-and-image-to-image data, all synthesized using GPT-4o's image generation capabilities for distilling its advanced image generation abilities. Leveraging this dataset, we develop Janus-4o, a multimodal large language model capable of both text-to-image and text-and-image-to-image generation. Janus-4o not only significantly improves text-to-image generation over its predecessor, Janus-Pro, but also newly supports text-and-image-to-image generation. Notably, it achieves impressive performance in text-and-image-to-image generation from scratch, using only 91K synthetic samples and 6 hours of training on an 8 A800-GPU machine. We hope the release of ShareGPT-4o-Image and Janus-4o will foster open research in photorealistic, instruction-aligned image generation.

</details>


### [17] [Targeted False Positive Synthesis via Detector-guided Adversarial Diffusion Attacker for Robust Polyp Detection](https://arxiv.org/abs/2506.18134)
*Quan Zhou,Gan Luo,Qiang Hu,Qingyong Zhang,Jinhua Zhang,Yinjiao Tian,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: 提出了一种对抗性扩散框架，用于合成高价值的假阳性样本，以改进结肠息肉检测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型在假阳性问题上表现不足，生成模型多关注息肉多样性而忽略假阳性问题。

Method: 设计了区域噪声匹配策略和Detector-guided Adversarial Diffusion Attacker (DADA)模块，用于合成多样背景和生成高价值假阳性样本。

Result: 在公开和内部数据集上验证了方法的优越性，合成数据使检测器的F1分数分别提高了至少2.6%和2.7%。

Conclusion: 该方法为病灶检测中的假阳性合成提供了新范式，有望提升结肠癌筛查的临床可靠性。

Abstract: Polyp detection is crucial for colorectal cancer screening, yet existing models are limited by the scale and diversity of available data. While generative models show promise for data augmentation, current methods mainly focus on enhancing polyp diversity, often overlooking the critical issue of false positives. In this paper, we address this gap by proposing an adversarial diffusion framework to synthesize high-value false positives. The extensive variability of negative backgrounds presents a significant challenge in false positive synthesis. To overcome this, we introduce two key innovations: First, we design a regional noise matching strategy to construct a negative synthesis space using polyp detection datasets. This strategy trains a negative-centric diffusion model by masking polyp regions, ensuring the model focuses exclusively on learning diverse background patterns. Second, we introduce the Detector-guided Adversarial Diffusion Attacker (DADA) module, which perturbs the negative synthesis process to disrupt a pre-trained detector's decision, guiding the negative-centric diffusion model to generate high-value, detector-confusing false positives instead of low-value, ordinary backgrounds. Our approach is the first to apply adversarial diffusion to lesion detection, establishing a new paradigm for targeted false positive synthesis and paving the way for more reliable clinical applications in colorectal cancer screening. Extensive results on public and in-house datasets verify the superiority of our method over the current state-of-the-arts, with our synthesized data improving the detectors by at least 2.6% and 2.7% in F1-score, respectively, over the baselines. Codes are at https://github.com/Huster-Hq/DADA.

</details>


### [18] [CDG-MAE: Learning Correspondences from Diffusion Generated Views](https://arxiv.org/abs/2506.18164)
*Varun Belagali,Pierre Marza,Srikar Yellapragada,Zilinghan Li,Tarak Nath Nandi,Ravi K Madduri,Joel Saltz,Stergios Christodoulidis,Maria Vakalopoulou,Dimitris Samaras*

Main category: cs.CV

TL;DR: CDG-MAE提出了一种基于MAE的自监督方法，利用扩散模型生成多样合成视图，克服了视频和图像裁剪的局限性，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决密集对应学习中的训练数据获取难题，避免依赖昂贵且有限的视频数据或缺乏多样性的图像裁剪。

Method: 使用图像条件扩散模型生成多样合成视图，提出多锚点策略调节任务难度，并评估生成图像的局部和全局一致性。

Result: CDG-MAE显著优于仅依赖图像的MAE方法，并大幅缩小与基于视频方法的性能差距。

Conclusion: 通过合成视图和多锚点策略，CDG-MAE为自监督学习提供了更丰富的数据和更灵活的任务设计。

Abstract: Learning dense correspondences, critical for application such as video label propagation, is hindered by tedious and unscalable manual annotation. Self-supervised methods address this by using a cross-view pretext task, often modeled with a masked autoencoder, where a masked target view is reconstructed from an anchor view. However, acquiring effective training data remains a challenge - collecting diverse video datasets is difficult and costly, while simple image crops lack necessary pose variations. This paper introduces CDG-MAE, a novel MAE-based self-supervised method that uses diverse synthetic views generated from static images via an image-conditioned diffusion model. These generated views exhibit substantial changes in pose and perspective, providing a rich training signal that overcomes the limitations of video and crop-based anchors. We present a quantitative method to evaluate local and global consistency of generated images, discussing their use for cross-view self-supervised pretraining. Furthermore, we enhance the standard single-anchor MAE setting to a multi-anchor strategy to effectively modulate the difficulty of pretext task. CDG-MAE significantly outperforms state-of-the-art MAE methods reliant only on images and substantially narrows the performance gap to video-based approaches.

</details>


### [19] [Limitations of NERF with pre-trained Vision Features for Few-Shot 3D Reconstruction](https://arxiv.org/abs/2506.18208)
*Ankit Sanjyal*

Main category: cs.CV

TL;DR: DINO-enhanced NeRF模型在极端少样本场景下表现不如基线NeRF，预训练视觉特征可能对少样本3D重建无益甚至有害。


<details>
  <summary>Details</summary>
Motivation: 探索预训练视觉特征（如DINO）对少样本3D重建的效果，验证其在极端少样本场景下的有效性。

Method: 系统评估DINO增强的NeRF模型，包括基线NeRF、冻结DINO特征、LoRA微调特征和多尺度特征融合。

Result: 所有DINO变体表现均差于基线NeRF（PSNR 12.9-13.0 vs. 14.71），表明预训练特征可能引入有害偏差。

Conclusion: 预训练视觉特征在少样本3D重建中可能无效，建议关注几何一致性的简单架构。

Abstract: Neural Radiance Fields (NeRF) have revolutionized 3D scene reconstruction from sparse image collections. Recent work has explored integrating pre-trained vision features, particularly from DINO, to enhance few-shot reconstruction capabilities. However, the effectiveness of such approaches remains unclear, especially in extreme few-shot scenarios. In this paper, we present a systematic evaluation of DINO-enhanced NeRF models, comparing baseline NeRF, frozen DINO features, LoRA fine-tuned features, and multi-scale feature fusion. Surprisingly, our experiments reveal that all DINO variants perform worse than the baseline NeRF, achieving PSNR values around 12.9 to 13.0 compared to the baseline's 14.71. This counterintuitive result suggests that pre-trained vision features may not be beneficial for few-shot 3D reconstruction and may even introduce harmful biases. We analyze potential causes including feature-task mismatch, overfitting to limited data, and integration challenges. Our findings challenge common assumptions in the field and suggest that simpler architectures focusing on geometric consistency may be more effective for few-shot scenarios.

</details>


### [20] [Make It Efficient: Dynamic Sparse Attention for Autoregressive Image Generation](https://arxiv.org/abs/2506.18226)
*Xunzhi Xiang,Qi Fan*

Main category: cs.CV

TL;DR: 提出了一种无需训练的上下文优化方法ADSA，通过动态稀疏注意力减少内存和计算开销。


<details>
  <summary>Details</summary>
Motivation: 自回归图像生成模型在推理时因长上下文导致内存和计算效率低下。

Method: 提出ADSA方法，动态识别关键历史token，优化注意力计算，并引入动态KV-cache更新机制。

Result: 实验表明ADSA在生成质量和资源效率上均表现优越，GPU内存消耗减少约50%。

Conclusion: ADSA是一种高效且无需训练的方法，显著提升了文本到图像合成的性能。

Abstract: Autoregressive conditional image generation models have emerged as a dominant paradigm in text-to-image synthesis. These methods typically convert images into one-dimensional token sequences and leverage the self-attention mechanism, which has achieved remarkable success in natural language processing, to capture long-range dependencies, model global context, and ensure semantic coherence. However, excessively long contexts during inference lead to significant memory overhead caused by KV-cache and computational delays. To alleviate these challenges, we systematically analyze how global semantics, spatial layouts, and fine-grained textures are formed during inference, and propose a novel training-free context optimization method called Adaptive Dynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies historical tokens crucial for maintaining local texture consistency and those essential for ensuring global semantic coherence, thereby efficiently streamlining attention computation. Additionally, we introduce a dynamic KV-cache update mechanism tailored for ADSA, reducing GPU memory consumption during inference by approximately $50\%$. Extensive qualitative and quantitative experiments demonstrate the effectiveness and superiority of our approach in terms of both generation quality and resource efficiency.

</details>


### [21] [Adaptive Mask-guided K-space Diffusion for Accelerated MRI Reconstruction](https://arxiv.org/abs/2506.18270)
*Qinrong Cai,Yu Guan,Zhibo Chen,Dong Liang,Qiuyun Fan,Qiegen Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于自适应掩码的扩散模型（AMDM），用于MRI重建，通过自适应调整k空间数据的频率分布，有效分离高低频成分，提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统MRI重建方法未充分考虑k空间不同频率区域的重要性，导致重建效果受限。

Method: 采用自适应掩码机制，根据k空间数据的频率分布生成动态掩码，指导扩散过程，分离高低频成分。

Result: 实验证明该方法能有效学习特定频率信息，显著提升MRI重建质量。

Conclusion: AMDM为未来基于掩码优化k空间数据提供了灵活框架。

Abstract: As the deep learning revolution marches on, masked modeling has emerged as a distinctive approach that involves predicting parts of the original data that are proportionally masked during training, and has demonstrated exceptional performance in multiple fields. Magnetic Resonance Imaging (MRI) reconstruction is a critical task in medical imaging that seeks to recover high-quality images from under-sampled k-space data. However, previous MRI reconstruction strategies usually optimized the entire image domain or k-space, without considering the importance of different frequency regions in the k-space This work introduces a diffusion model based on adaptive masks (AMDM), which utilizes the adaptive adjustment of frequency distribution based on k-space data to develop a hybrid masks mechanism that adapts to different k-space inputs. This enables the effective separation of high-frequency and low-frequency components, producing diverse frequency-specific representations. Additionally, the k-space frequency distribution informs the generation of adaptive masks, which, in turn, guide a closed-loop diffusion process. Experimental results verified the ability of this method to learn specific frequency information and thereby improved the quality of MRI reconstruction, providing a flexible framework for optimizing k-space data using masks in the future.

</details>


### [22] [A Multi-Scale Spatial Attention-Based Zero-Shot Learning Framework for Low-Light Image Enhancement](https://arxiv.org/abs/2506.18323)
*Muhammad Azeem Aslam,Hassan Khalid,Nisar Ahmed*

Main category: cs.CV

TL;DR: LucentVisionNet是一种零样本学习框架，用于低光图像增强，结合多尺度空间注意力和深度曲线估计网络，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决低光图像增强任务中缺乏配对训练数据的挑战，超越传统和深度学习方法。

Method: 集成多尺度空间注意力与深度曲线估计网络，采用循环增强策略和复合损失函数优化。

Result: 在多个基准数据集上优于监督、无监督和零样本方法，提升视觉质量和计算效率。

Conclusion: LucentVisionNet适用于移动摄影、监控和自主导航等实际应用，具有高质量和高效性。

Abstract: Low-light image enhancement remains a challenging task, particularly in the absence of paired training data. In this study, we present LucentVisionNet, a novel zero-shot learning framework that addresses the limitations of traditional and deep learning-based enhancement methods. The proposed approach integrates multi-scale spatial attention with a deep curve estimation network, enabling fine-grained enhancement while preserving semantic and perceptual fidelity. To further improve generalization, we adopt a recurrent enhancement strategy and optimize the model using a composite loss function comprising six tailored components, including a novel no-reference image quality loss inspired by human visual perception. Extensive experiments on both paired and unpaired benchmark datasets demonstrate that LucentVisionNet consistently outperforms state-of-the-art supervised, unsupervised, and zero-shot methods across multiple full-reference and no-reference image quality metrics. Our framework achieves high visual quality, structural consistency, and computational efficiency, making it well-suited for deployment in real-world applications such as mobile photography, surveillance, and autonomous navigation.

</details>


### [23] [NSFW-Classifier Guided Prompt Sanitization for Safe Text-to-Image Generation](https://arxiv.org/abs/2506.18325)
*Yu Xie,Chengjie Zeng,Lingyun Zhang,Yanwei Fu*

Main category: cs.CV

TL;DR: 本文提出PromptSan方法，通过文本NSFW分类器指导的提示净化技术，减少T2I模型生成有害内容的风险。


<details>
  <summary>Details</summary>
Motivation: T2I模型（如Stable Diffusion）的快速发展带来了生成有害内容的风险，如色情、暴力等，违背伦理目标并阻碍可持续发展。

Method: 提出PromptSan方法，包括PromptSan-Modify（迭代识别并替换有害标记）和PromptSan-Suffix（训练优化后缀标记序列中和有害意图）。

Result: 实验表明，PromptSan在多指标上显著减少有害内容生成，平衡安全性与可用性。

Conclusion: PromptSan在不改变模型架构或降低生成能力的情况下，有效净化有害提示，为T2I技术的可持续发展提供支持。

Abstract: The rapid advancement of text-to-image (T2I) models, such as Stable Diffusion, has enhanced their capability to synthesize images from textual prompts. However, this progress also raises significant risks of misuse, including the generation of harmful content (e.g., pornography, violence, discrimination), which contradicts the ethical goals of T2I technology and hinders its sustainable development. Inspired by "jailbreak" attacks in large language models, which bypass restrictions through subtle prompt modifications, this paper proposes NSFW-Classifier Guided Prompt Sanitization (PromptSan), a novel approach to detoxify harmful prompts without altering model architecture or degrading generation capability. PromptSan includes two variants: PromptSan-Modify, which iteratively identifies and replaces harmful tokens in input prompts using text NSFW classifiers during inference, and PromptSan-Suffix, which trains an optimized suffix token sequence to neutralize harmful intent while passing both text and image NSFW classifier checks. Extensive experiments demonstrate that PromptSan achieves state-of-the-art performance in reducing harmful content generation across multiple metrics, effectively balancing safety and usability.

</details>


### [24] [BSMamba: Brightness and Semantic Modeling for Long-Range Interaction in Low-Light Image Enhancement](https://arxiv.org/abs/2506.18346)
*Tongshun Zhang,Pingping Liu,Mengen Cai,Zijian Zhang,Yubing Lu,Qiuzhan Zhou*

Main category: cs.CV

TL;DR: BSMamba是一种新型视觉Mamba架构，通过亮度Mamba和语义Mamba组件，解决了低光图像增强中亮度恢复和语义一致性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有低光图像增强方法在提升亮度的同时难以保持语义一致性和细节，且传统视觉Mamba方法因固定扫描规则限制了远距离标记的交互。

Method: BSMamba包含亮度Mamba和语义Mamba，分别基于亮度和语义相似性建模标记交互，避免固定扫描规则的局限性。

Result: 实验表明BSMamba在低光图像增强任务中达到了最先进的性能，同时保持了语义一致性。

Conclusion: BSMamba通过亮度和语义引导的标记交互，显著提升了低光图像增强的效果。

Abstract: Current low-light image enhancement (LLIE) methods face significant limitations in simultaneously improving brightness while preserving semantic consistency, fine details, and computational efficiency. With the emergence of state-space models, particularly Mamba, image restoration has achieved remarkable performance, yet existing visual Mamba approaches flatten 2D images into 1D token sequences using fixed scanning rules, critically limiting interactions between distant tokens with causal relationships and constraining their ability to capture meaningful long-range dependencies. To address these fundamental limitations, we propose BSMamba, a novel visual Mamba architecture comprising two specially designed components: Brightness Mamba and Semantic Mamba. The Brightness Mamba revolutionizes token interaction patterns by prioritizing connections between distant tokens with similar brightness levels, effectively addressing the challenge of brightness restoration in LLIE tasks through brightness-guided selective attention. Complementing this, the Semantic Mamba establishes priority interactions between tokens sharing similar semantic meanings, allowing the model to maintain contextual consistency by connecting semantically related regions across the image, thus preserving the hierarchical nature of image semantics during enhancement. By intelligently modeling tokens based on brightness and semantic similarity rather than arbitrary scanning patterns, BSMamba transcends the constraints of conventional token sequencing while adhering to the principles of causal modeling. Extensive experiments demonstrate that BSMamba achieves state-of-the-art performance in LLIE while preserving semantic consistency.

</details>


### [25] [Latent Space Analysis for Melanoma Prevention](https://arxiv.org/abs/2506.18414)
*Ciro Listone,Aniello Murano*

Main category: cs.CV

TL;DR: 提出了一种基于条件变分自编码器的可解释性风险建模方法，用于皮肤病变分类，支持连续评估和可视化解释。


<details>
  <summary>Details</summary>
Motivation: 黑色素瘤的高致死率需要早期可解释的诊断工具，现有深度学习模型仅提供二元输出，临床价值有限。

Method: 使用条件变分自编码器学习结构化潜在空间，结合SVM分类器，实现病变的连续评估和风险指示。

Result: 方法在区分良性痣和黑色素瘤上表现优异，潜在空间支持可视化和几何解释，增强临床适用性。

Conclusion: 该方法结合预测性能与临床可解释性，促进早期检测和AI诊断的信任。

Abstract: Melanoma represents a critical health risk due to its aggressive progression and high mortality, underscoring the need for early, interpretable diagnostic tools. While deep learning has advanced in skin lesion classification, most existing models provide only binary outputs, offering limited clinical insight. This work introduces a novel approach that extends beyond classification, enabling interpretable risk modelling through a Conditional Variational Autoencoder. The proposed method learns a structured latent space that captures semantic relationships among lesions, allowing for a nuanced, continuous assessment of morphological differences. An SVM is also trained on this representation effectively differentiating between benign nevi and melanomas, demonstrating strong and consistent performance. More importantly, the learned latent space supports visual and geometric interpretation of malignancy, with the spatial proximity of a lesion to known melanomas serving as a meaningful indicator of risk. This approach bridges predictive performance with clinical applicability, fostering early detection, highlighting ambiguous cases, and enhancing trust in AI-assisted diagnosis through transparent and interpretable decision-making.

</details>


### [26] [Frequency-Domain Fusion Transformer for Image Inpainting](https://arxiv.org/abs/2506.18437)
*Sijin He,Guangfeng Lin,Tao Li,Yajun Chen*

Main category: cs.CV

TL;DR: 提出了一种结合频域融合的Transformer图像修复方法，通过小波变换和Gabor滤波增强细节保留，并设计了可学习的频域滤波器以降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以处理复杂纹理和大面积遮挡，而Transformer方法因自注意力的低通特性无法保留高频细节且计算成本高。

Method: 结合小波变换和Gabor滤波的注意力机制，设计可学习的频域滤波器替代前馈网络，采用四层编码器-解码器结构和新损失策略。

Result: 实验表明，该方法能有效保留更多高频信息，提升图像修复质量。

Conclusion: 提出的方法在保留高频细节和降低计算成本方面优于现有方法。

Abstract: Image inpainting plays a vital role in restoring missing image regions and supporting high-level vision tasks, but traditional methods struggle with complex textures and large occlusions. Although Transformer-based approaches have demonstrated strong global modeling capabilities, they often fail to preserve high-frequency details due to the low-pass nature of self-attention and suffer from high computational costs. To address these challenges, this paper proposes a Transformer-based image inpainting method incorporating frequency-domain fusion. Specifically, an attention mechanism combining wavelet transform and Gabor filtering is introduced to enhance multi-scale structural modeling and detail preservation. Additionally, a learnable frequency-domain filter based on the fast Fourier transform is designed to replace the feedforward network, enabling adaptive noise suppression and detail retention. The model adopts a four-level encoder-decoder structure and is guided by a novel loss strategy to balance global semantics and fine details. Experimental results demonstrate that the proposed method effectively improves the quality of image inpainting by preserving more high-frequency information.

</details>


### [27] [DIP: Unsupervised Dense In-Context Post-training of Visual Representations](https://arxiv.org/abs/2506.18463)
*Sophia Sirko-Galouchenko,Spyros Gidaris,Antonin Vobecky,Andrei Bursuc,Nicolas Thome*

Main category: cs.CV

TL;DR: DIP是一种无监督的后训练方法，通过模拟下游场景任务提升密集图像表示，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提升大规模预训练视觉编码器在上下文场景理解中的密集表示能力，避免复杂自蒸馏架构。

Method: 利用伪任务模拟下游场景，结合扩散模型和视觉编码器自动生成任务，基于元学习原理。

Result: 在多种下游任务中表现优异，计算高效（单A100 GPU耗时<9小时）。

Conclusion: DIP提供了一种简单、高效且有效的密集表示改进方案。

Abstract: We introduce DIP, a novel unsupervised post-training method designed to enhance dense image representations in large-scale pretrained vision encoders for in-context scene understanding. Unlike prior approaches that rely on complex self-distillation architectures, our method trains the vision encoder using pseudo-tasks that explicitly simulate downstream in-context scenarios, inspired by meta-learning principles. To enable post-training on unlabeled data, we propose an automatic mechanism for generating in-context tasks that combines a pretrained diffusion model and the vision encoder itself. DIP is simple, unsupervised, and computationally efficient, requiring less than 9 hours on a single A100 GPU. By learning dense representations through pseudo in-context tasks, it achieves strong performance across a wide variety of downstream real-world in-context scene understanding tasks. It outperforms both the initial vision encoder and prior methods, offering a practical and effective solution for improving dense representations. Code available here: https://github.com/sirkosophia/DIP

</details>


### [28] [GANs vs. Diffusion Models for virtual staining with the HER2match dataset](https://arxiv.org/abs/2506.18484)
*Pascal Klöckner,José Teixeira,Diana Montezuma,Jaime S. Cardoso,Hugo M. Horlings,Sara P. Oliveira*

Main category: cs.CV

TL;DR: 论文介绍了首个公开的H&E-HER2染色配对数据集HER2match，并比较了GANs和DMs在H&E-HER2染色转换任务中的表现，发现GANs整体优于DMs，同时强调了数据对齐的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决H&E-HER2染色转换任务中缺乏公开数据集和模型性能不明确的问题。

Method: 引入HER2match数据集，比较多种GANs和DMs，并提出一种新的Brownian Bridge Diffusion Model（BBDM）。

Result: GANs整体表现优于DMs，仅BBDM与之相当；数据对齐显著提升模型效果。

Conclusion: HER2match数据集和模型比较为H&E-HER2染色转换研究提供了新资源和指导。

Abstract: Virtual staining is a promising technique that uses deep generative models to recreate histological stains, providing a faster and more cost-effective alternative to traditional tissue chemical staining. Specifically for H&E-HER2 staining transfer, despite a rising trend in publications, the lack of sufficient public datasets has hindered progress in the topic. Additionally, it is currently unclear which model frameworks perform best for this particular task. In this paper, we introduce the HER2match dataset, the first publicly available dataset with the same breast cancer tissue sections stained with both H&E and HER2. Furthermore, we compare the performance of several Generative Adversarial Networks (GANs) and Diffusion Models (DMs), and implement a novel Brownian Bridge Diffusion Model for H&E-HER2 translation. Our findings indicate that, overall, GANs perform better than DMs, with only the BBDM achieving comparable results. Furthermore, we emphasize the importance of data alignment, as all models trained on HER2match produced vastly improved visuals compared to the widely used consecutive-slide BCI dataset. This research provides a new high-quality dataset ([available upon publication acceptance]), improving both model training and evaluation. In addition, our comparison of frameworks offers valuable guidance for researchers working on the topic.

</details>


### [29] [ShowFlow: From Robust Single Concept to Condition-Free Multi-Concept Generation](https://arxiv.org/abs/2506.18493)
*Trong-Vu Hoang,Quang-Binh Nguyen,Thanh-Toan Do,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: ShowFlow是一个框架，用于解决单概念和多概念图像生成中的身份保持和提示对齐问题。


<details>
  <summary>Details</summary>
Motivation: 解决可控图像合成中单概念和多概念生成的挑战，如身份丢失和概念遗漏。

Method: ShowFlow-S使用KronA-WED适配器和解耦学习方法；ShowFlow-M重用ShowFlow-S模型，加入SAMA和布局一致性策略。

Result: 实验和用户研究验证了ShowFlow在广告和虚拟试衣等实际应用中的有效性。

Conclusion: ShowFlow在单概念和多概念图像生成中表现出色，具有实际应用潜力。

Abstract: Customizing image generation remains a core challenge in controllable image synthesis. For single-concept generation, maintaining both identity preservation and prompt alignment is challenging. In multi-concept scenarios, relying solely on a prompt without additional conditions like layout boxes or semantic masks, often leads to identity loss and concept omission. In this paper, we introduce ShowFlow, a comprehensive framework designed to tackle these challenges. We propose ShowFlow-S for single-concept image generation, and ShowFlow-M for handling multiple concepts. ShowFlow-S introduces a KronA-WED adapter, which integrates a Kronecker adapter with weight and embedding decomposition, and employs a disentangled learning approach with a novel attention regularization objective to enhance single-concept generation. Building on this foundation, ShowFlow-M directly reuses the learned models from ShowFlow-S to support multi-concept generation without extra conditions, incorporating a Subject-Adaptive Matching Attention (SAMA) and a layout consistency strategy as the plug-and-play module. Extensive experiments and user studies validate ShowFlow's effectiveness, highlighting its potential in real-world applications like advertising and virtual dressing.

</details>


### [30] [Enhancing Image Restoration Transformer via Adaptive Translation Equivariance](https://arxiv.org/abs/2506.18520)
*JiaKui Hu,Zhengjian Yao,Lujia Jin,Hangzhou He,Yanye Lu*

Main category: cs.CV

TL;DR: 论文提出了一种名为TEAFormer的网络，通过滑动索引和组件堆叠策略解决图像修复中注意力机制破坏平移等变性的问题，并引入自适应滑动索引机制以平衡计算成本和感受野。


<details>
  <summary>Details</summary>
Motivation: 现代修复变换器中的注意力机制破坏了平移等变性，影响了训练收敛和泛化能力，需要解决这一问题。

Method: 提出滑动索引和组件堆叠策略，并设计自适应滑动索引机制，结合全局和局部信息。

Result: TEAFormer在多种图像修复任务中表现出色，效果、训练收敛和泛化能力均优于其他方法。

Conclusion: TEAFormer通过自适应滑动索引机制有效平衡了计算成本和感受野，提升了图像修复任务的性能。

Abstract: Translation equivariance is a fundamental inductive bias in image restoration, ensuring that translated inputs produce translated outputs. Attention mechanisms in modern restoration transformers undermine this property, adversely impacting both training convergence and generalization. To alleviate this issue, we propose two key strategies for incorporating translation equivariance: slide indexing and component stacking. Slide indexing maintains operator responses at fixed positions, with sliding window attention being a notable example, while component stacking enables the arrangement of translation-equivariant operators in parallel or sequentially, thereby building complex architectures while preserving translation equivariance. However, these strategies still create a dilemma in model design between the high computational cost of self-attention and the fixed receptive field associated with sliding window attention. To address this, we develop an adaptive sliding indexing mechanism to efficiently select key-value pairs for each query, which are then concatenated in parallel with globally aggregated key-value pairs. The designed network, called the Translation Equivariance Adaptive Transformer (TEAFormer), is assessed across a variety of image restoration tasks. The results highlight its superiority in terms of effectiveness, training convergence, and generalization.

</details>


### [31] [Auto-Regressively Generating Multi-View Consistent Images](https://arxiv.org/abs/2506.18527)
*JiaKui Hu,Yuxiao Yang,Jialun Liu,Jinbo Wu,Chen Zhao,Yanye Lu*

Main category: cs.CV

TL;DR: 提出了一种多视图自回归（MV-AR）方法，通过自回归模型逐步生成一致的多视图图像，解决了多视图一致性和多样化条件下的形状与纹理合成问题。


<details>
  <summary>Details</summary>
Motivation: 多视图图像生成在3D内容创作中至关重要，但面临多视图一致性和多样化条件下合成的挑战。

Method: 利用自回归模型的逐令牌预测能力，设计统一模型处理多样化提示，并引入条件注入模块和渐进训练策略。

Result: 实验表明MV-AR能生成一致的多视图图像，性能与领先的扩散模型相当。

Conclusion: MV-AR在多视图生成任务中表现出色，代码和模型将开源。

Abstract: Generating multi-view images from human instructions is crucial for 3D content creation. The primary challenges involve maintaining consistency across multiple views and effectively synthesizing shapes and textures under diverse conditions. In this paper, we propose the Multi-View Auto-Regressive (MV-AR) method, which leverages an auto-regressive model to progressively generate consistent multi-view images from arbitrary prompts. Firstly, the next-token-prediction capability of the AR model significantly enhances its effectiveness in facilitating progressive multi-view synthesis. When generating widely-separated views, MV-AR can utilize all its preceding views to extract effective reference information. Subsequently, we propose a unified model that accommodates various prompts via architecture designing and training strategies. To address multiple conditions, we introduce condition injection modules for text, camera pose, image, and shape. To manage multi-modal conditions simultaneously, a progressive training strategy is employed. This strategy initially adopts the text-to-multi-view (t2mv) model as a baseline to enhance the development of a comprehensive X-to-multi-view (X2mv) model through the randomly dropping and combining conditions. Finally, to alleviate the overfitting problem caused by limited high-quality data, we propose the "Shuffle View" data augmentation technique, thus significantly expanding the training data by several magnitudes. Experiments demonstrate the performance and versatility of our MV-AR, which consistently generates consistent multi-view images across a range of conditions and performs on par with leading diffusion-based multi-view image generation models. Code and models will be released at https://github.com/MILab-PKU/MVAR.

</details>


### [32] [VQ-Insight: Teaching VLMs for AI-Generated Video Quality Understanding via Progressive Visual Reinforcement Learning](https://arxiv.org/abs/2506.18564)
*Xuanyu Zhang,Weiqi Li,Shijie Zhao,Junlin Li,Li Zhang,Jian Zhang*

Main category: cs.CV

TL;DR: VQ-Insight是一个新型的推理式视觉语言模型框架，用于评估AI生成视频的质量，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成视频质量评估存在泛化能力有限、缺乏时间感知、依赖大规模标注数据等问题。

Method: 提出VQ-Insight框架，包括渐进式视频质量学习方案和多维度评分奖励设计。

Result: 实验表明VQ-Insight在多维度评分和自然视频评分中优于现有方法。

Conclusion: VQ-Insight显著提升了视频生成任务的质量评估效果。

Abstract: Recent advances in AI-generated content (AIGC) have led to the emergence of powerful text-to-video generation models. Despite these successes, evaluating the quality of AIGC-generated videos remains challenging due to limited generalization, lack of temporal awareness, heavy reliance on large-scale annotated datasets, and the lack of effective interaction with generation models. Most current approaches rely on supervised finetuning of vision-language models (VLMs), which often require large-scale annotated datasets and tend to decouple understanding and generation. To address these shortcomings, we propose VQ-Insight, a novel reasoning-style VLM framework for AIGC video quality assessment. Our approach features: (1) a progressive video quality learning scheme that combines image quality warm-up, general task-specific temporal learning, and joint optimization with the video generation model; (2) the design of multi-dimension scoring rewards, preference comparison rewards, and temporal modeling rewards to enhance both generalization and specialization in video quality evaluation. Extensive experiments demonstrate that VQ-Insight consistently outperforms state-of-the-art baselines in preference comparison, multi-dimension scoring, and natural video scoring, bringing significant improvements for video generation tasks.

</details>


### [33] [VisualChef: Generating Visual Aids in Cooking via Mask Inpainting](https://arxiv.org/abs/2506.18569)
*Oleh Kuzyk,Zuoyue Li,Marc Pollefeys,Xi Wang*

Main category: cs.CV

TL;DR: VisualChef是一种生成烹饪场景上下文视觉辅助的方法，通过基于掩码的视觉定位简化对齐，生成执行动作和结果的图像。


<details>
  <summary>Details</summary>
Motivation: 烹饪过程中缺乏一致的视觉指导，现有方法依赖复杂的文本-视觉对齐，VisualChef旨在简化这一过程。

Method: 通过识别动作相关对象并分类，进行针对性修改，同时保持环境一致；提出自动化流程提取高质量帧。

Result: 在三个第一人称视频数据集上定量和定性评估，优于现有方法。

Conclusion: VisualChef通过简化视觉-文本对齐，有效生成烹饪场景的视觉辅助。

Abstract: Cooking requires not only following instructions but also understanding, executing, and monitoring each step - a process that can be challenging without visual guidance. Although recipe images and videos offer helpful cues, they often lack consistency in focus, tools, and setup. To better support the cooking process, we introduce VisualChef, a method for generating contextual visual aids tailored to cooking scenarios. Given an initial frame and a specified action, VisualChef generates images depicting both the action's execution and the resulting appearance of the object, while preserving the initial frame's environment. Previous work aims to integrate knowledge extracted from large language models by generating detailed textual descriptions to guide image generation, which requires fine-grained visual-textual alignment and involves additional annotations. In contrast, VisualChef simplifies alignment through mask-based visual grounding. Our key insight is identifying action-relevant objects and classifying them to enable targeted modifications that reflect the intended action and outcome while maintaining a consistent environment. In addition, we propose an automated pipeline to extract high-quality initial, action, and final state frames. We evaluate VisualChef quantitatively and qualitatively on three egocentric video datasets and show its improvements over state-of-the-art methods.

</details>


### [34] [2D Triangle Splatting for Direct Differentiable Mesh Training](https://arxiv.org/abs/2506.18575)
*Kaifeng Sheng,Zheng Zhou,Yingliang Peng,Qianwei Wang*

Main category: cs.CV

TL;DR: 提出了一种名为2D Triangle Splatting（2DTS）的新方法，用2D三角形面片替代3D高斯基元，以提升渲染速度和高级渲染效果。


<details>
  <summary>Details</summary>
Motivation: 尽管基于3D高斯基元的可微分渲染在多视角图像重建中表现出色，但在渲染速度和高级效果（如重光照和阴影渲染）上仍存在不足。

Method: 采用2D三角形面片作为基元，结合紧凑性参数，直接训练出逼真的网格。

Result: 实验表明，该方法在未优化紧凑性的情况下，比现有高斯基元方法具有更高的保真度，且重建网格的视觉质量更优。

Conclusion: 2DTS方法在保留连续体积建模优势的同时，显著提升了渲染速度和效果，为高质量3D场景重建提供了新思路。

Abstract: Differentiable rendering with 3D Gaussian primitives has emerged as a powerful method for reconstructing high-fidelity 3D scenes from multi-view images. While it offers improvements over NeRF-based methods, this representation still encounters challenges with rendering speed and advanced rendering effects, such as relighting and shadow rendering, compared to mesh-based models. In this paper, we propose 2D Triangle Splatting (2DTS), a novel method that replaces 3D Gaussian primitives with 2D triangle facelets. This representation naturally forms a discrete mesh-like structure while retaining the benefits of continuous volumetric modeling. By incorporating a compactness parameter into the triangle primitives, we enable direct training of photorealistic meshes. Our experimental results demonstrate that our triangle-based method, in its vanilla version (without compactness tuning), achieves higher fidelity compared to state-of-the-art Gaussian-based methods. Furthermore, our approach produces reconstructed meshes with superior visual quality compared to existing mesh reconstruction methods.

</details>


### [35] [Resampling Augmentation for Time Series Contrastive Learning: Application to Remote Sensing](https://arxiv.org/abs/2506.18587)
*Antoine Saget,Baptiste Lafabregue,Antoine Cornuéjols,Pierre Gançarski*

Main category: cs.CV

TL;DR: 论文提出了一种基于重采样的对比学习增强策略，用于卫星图像时间序列（SITS），在农业分类任务中表现优于常见方法。


<details>
  <summary>Details</summary>
Motivation: 利用大量未标记的卫星图像时间序列数据，解决标记数据稀缺的问题，同时克服时间序列对比学习中数据增强的挑战。

Method: 采用重采样策略生成正样本对，通过上采样时间序列并提取不重叠的子序列，保持时间覆盖范围。

Result: 在多个农业分类基准测试中表现优于常见方法（如抖动、调整大小和掩码），并在S2-Agri100数据集上达到最先进性能。

Conclusion: 该方法为遥感时间序列提供了一种简单而有效的对比学习增强策略。

Abstract: Given the abundance of unlabeled Satellite Image Time Series (SITS) and the scarcity of labeled data, contrastive self-supervised pretraining emerges as a natural tool to leverage this vast quantity of unlabeled data. However, designing effective data augmentations for contrastive learning remains challenging for time series. We introduce a novel resampling-based augmentation strategy that generates positive pairs by upsampling time series and extracting disjoint subsequences while preserving temporal coverage. We validate our approach on multiple agricultural classification benchmarks using Sentinel-2 imagery, showing that it outperforms common alternatives such as jittering, resizing, and masking. Further, we achieve state-of-the-art performance on the S2-Agri100 dataset without employing spatial information or temporal encodings, surpassing more complex masked-based SSL frameworks. Our method offers a simple, yet effective, contrastive learning augmentation for remote sensing time series.

</details>


### [36] [RDPO: Real Data Preference Optimization for Physics Consistency Video Generation](https://arxiv.org/abs/2506.18655)
*Wenxu Qian,Chaoyue Wang,Hou Peng,Zhiyu Tan,Hao Li,Anxiang Zeng*

Main category: cs.CV

TL;DR: 提出了一种无需标注的框架RDPO，通过从真实视频中提取物理先验，显著提升了生成视频的动作连贯性和物理真实感。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成技术在视觉质量上取得了显著进展，但在真实物理模拟方面仍存在不足，而基于偏好的后训练方法需要昂贵的人工标注数据或奖励模型。

Method: RDPO通过反向采样真实视频序列自动构建偏好对，并采用多阶段迭代训练计划逐步提升生成器的物理一致性。

Result: 在多个基准测试和人工评估中，RDPO在动作连贯性和物理真实感方面均取得了显著提升。

Conclusion: RDPO通过直接从真实视频中提取物理先验，无需人工标注，显著提升了生成视频的物理一致性。

Abstract: Video generation techniques have achieved remarkable advancements in visual quality, yet faithfully reproducing real-world physics remains elusive. Preference-based model post-training may improve physical consistency, but requires costly human-annotated datasets or reward models that are not yet feasible. To address these challenges, we present Real Data Preference Optimisation (RDPO), an annotation-free framework that distills physical priors directly from real-world videos. Specifically, the proposed RDPO reverse-samples real video sequences with a pre-trained generator to automatically build preference pairs that are statistically distinguishable in terms of physical correctness. A multi-stage iterative training schedule then guides the generator to obey physical laws increasingly well. Benefiting from the dynamic information explored from real videos, our proposed RDPO significantly improves the action coherence and physical realism of the generated videos. Evaluations on multiple benchmarks and human evaluations have demonstrated that RDPO achieves improvements across multiple dimensions. The source code and demonstration of this paper are available at: https://wwenxu.github.io/RDPO/

</details>


### [37] [Reconstructing Tornadoes in 3D with Gaussian Splatting](https://arxiv.org/abs/2506.18677)
*Adam Yang,Nadula Kadawedduwa,Tianfu Wang,Maria Molina,Christopher Metzler*

Main category: cs.CV

TL;DR: 论文提出了一种基于实验室的小型龙卷风多视角数据集，并展示了使用3D高斯溅射（3DGS）技术有效重建其3D结构的方法。


<details>
  <summary>Details</summary>
Motivation: 准确重建龙卷风的3D结构对理解和应对这一破坏性天气现象至关重要，但目前缺乏用于开发和验证相关工具的受控数据集。

Method: 捕获并发布了一个实验室小型龙卷风的多视角数据集，并应用3D高斯溅射（3DGS）技术进行3D重建。

Result: 成功重建并可视化了实验室龙卷风的3D结构。

Conclusion: 该数据集和方法为未来研究龙卷风的3D结构提供了有价值的工具和基准。

Abstract: Accurately reconstructing the 3D structure of tornadoes is critically important for understanding and preparing for this highly destructive weather phenomenon. While modern 3D scene reconstruction techniques, such as 3D Gaussian splatting (3DGS), could provide a valuable tool for reconstructing the 3D structure of tornados, at present we are critically lacking a controlled tornado dataset with which to develop and validate these tools. In this work we capture and release a novel multiview dataset of a small lab-based tornado. We demonstrate one can effectively reconstruct and visualize the 3D structure of this tornado using 3DGS.

</details>


### [38] [MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit Neural Scene Representation](https://arxiv.org/abs/2506.18678)
*Tianchen Deng,Guole Shen,Xun Chen,Shenghai Yuan,Hongming Shen,Guohao Peng,Zhenyu Wu,Jingchuan Wang,Lihua Xie,Danwei Wang,Hesheng Wang,Weidong Chen*

Main category: cs.CV

TL;DR: 提出首个分布式多智能体协作神经SLAM框架，结合混合场景表示、分布式相机跟踪、局部到全局闭环以及在线蒸馏技术，解决现有方法在大规模场景和多智能体协作中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有神经隐式SLAM方法局限于单智能体场景，且在大规模场景和长序列中表现不佳，而基于NeRF的多智能体SLAM框架无法满足通信带宽限制。

Method: 提出三平面网格联合场景表示方法、局部到全局闭环方法以及在线蒸馏技术，用于多子地图融合和全局一致性。

Result: 实验证明该方法在映射、跟踪和通信方面优于现有方法，并发布了首个包含单智能体和多智能体场景的真实世界数据集。

Conclusion: 该框架在多智能体协作SLAM中表现出色，新数据集将推动SLAM、3D重建和视觉基础模型的研究。

Abstract: Neural implicit scene representations have recently shown promising results in dense visual SLAM. However, existing implicit SLAM algorithms are constrained to single-agent scenarios, and fall difficulties in large-scale scenes and long sequences. Existing NeRF-based multi-agent SLAM frameworks cannot meet the constraints of communication bandwidth. To this end, we propose the first distributed multi-agent collaborative neural SLAM framework with hybrid scene representation, distributed camera tracking, intra-to-inter loop closure, and online distillation for multiple submap fusion. A novel triplane-grid joint scene representation method is proposed to improve scene reconstruction. A novel intra-to-inter loop closure method is designed to achieve local (single-agent) and global (multi-agent) consistency. We also design a novel online distillation method to fuse the information of different submaps to achieve global consistency. Furthermore, to the best of our knowledge, there is no real-world dataset for NeRF-based/GS-based SLAM that provides both continuous-time trajectories groundtruth and high-accuracy 3D meshes groundtruth. To this end, we propose the first real-world Dense slam (DES) dataset covering both single-agent and multi-agent scenarios, ranging from small rooms to large-scale outdoor scenes, with high-accuracy ground truth for both 3D mesh and continuous-time camera trajectory. This dataset can advance the development of the research in both SLAM, 3D reconstruction, and visual foundation model. Experiments on various datasets demonstrate the superiority of the proposed method in both mapping, tracking, and communication. The dataset and code will open-source on https://github.com/dtc111111/mcnslam.

</details>


### [39] [Matrix-Game: Interactive World Foundation Model](https://arxiv.org/abs/2506.18701)
*Yifan Zhang,Chunli Peng,Boyang Wang,Puyi Wang,Qingcheng Zhu,Fei Kang,Biao Jiang,Zedong Gao,Eric Li,Yang Liu,Yahui Zhou*

Main category: cs.CV

TL;DR: Matrix-Game是一个用于可控游戏世界生成的交互式基础模型，通过两阶段训练实现高质量视频生成，并在Minecraft数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够精确控制角色动作和相机移动的游戏世界生成模型，同时保持高视觉质量和时间连贯性。

Method: 采用两阶段训练流程：大规模无标签预训练和环境理解，随后进行带标签的交互式视频生成训练。

Result: Matrix-Game在视觉质量、时间质量、动作可控性和物理规则理解方面均优于现有模型，特别是在可控性和物理一致性上表现突出。

Conclusion: Matrix-Game为交互式图像到世界生成提供了高效解决方案，并开源模型和评测基准以推动未来研究。

Abstract: We introduce Matrix-Game, an interactive world foundation model for controllable game world generation. Matrix-Game is trained using a two-stage pipeline that first performs large-scale unlabeled pretraining for environment understanding, followed by action-labeled training for interactive video generation. To support this, we curate Matrix-Game-MC, a comprehensive Minecraft dataset comprising over 2,700 hours of unlabeled gameplay video clips and over 1,000 hours of high-quality labeled clips with fine-grained keyboard and mouse action annotations. Our model adopts a controllable image-to-world generation paradigm, conditioned on a reference image, motion context, and user actions. With over 17 billion parameters, Matrix-Game enables precise control over character actions and camera movements, while maintaining high visual quality and temporal coherence. To evaluate performance, we develop GameWorld Score, a unified benchmark measuring visual quality, temporal quality, action controllability, and physical rule understanding for Minecraft world generation. Extensive experiments show that Matrix-Game consistently outperforms prior open-source Minecraft world models (including Oasis and MineWorld) across all metrics, with particularly strong gains in controllability and physical consistency. Double-blind human evaluations further confirm the superiority of Matrix-Game, highlighting its ability to generate perceptually realistic and precisely controllable videos across diverse game scenarios. To facilitate future research on interactive image-to-world generation, we will open-source the Matrix-Game model weights and the GameWorld Score benchmark at https://github.com/SkyworkAI/Matrix-Game.

</details>


### [40] [ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs](https://arxiv.org/abs/2506.18792)
*Michal Nazarczuk,Sibi Catley-Chandar,Thomas Tanay,Zhensong Zhang,Gregory Slabaugh,Eduardo Pérez-Pellitero*

Main category: cs.CV

TL;DR: ViDAR利用个性化扩散模型生成伪多视角监督信号，通过高斯泼溅表示训练，提升动态场景下的新视角合成质量。


<details>
  <summary>Details</summary>
Motivation: 解决单目视频中动态场景新视角合成的挑战，尤其是结构-运动解耦的模糊性和监督信号稀缺的问题。

Method: 提出ViDAR框架，结合个性化扩散模型生成伪多视角监督信号，并引入扩散感知损失函数和相机位姿优化策略。

Result: 在DyCheck基准测试中，ViDAR在视觉质量和几何一致性上优于现有方法，尤其在动态区域表现突出。

Conclusion: ViDAR通过扩散模型和优化策略，显著提升了动态场景新视角合成的性能，为相关领域提供了新基准。

Abstract: Dynamic Novel View Synthesis aims to generate photorealistic views of moving subjects from arbitrary viewpoints. This task is particularly challenging when relying on monocular video, where disentangling structure from motion is ill-posed and supervision is scarce. We introduce Video Diffusion-Aware Reconstruction (ViDAR), a novel 4D reconstruction framework that leverages personalised diffusion models to synthesise a pseudo multi-view supervision signal for training a Gaussian splatting representation. By conditioning on scene-specific features, ViDAR recovers fine-grained appearance details while mitigating artefacts introduced by monocular ambiguity. To address the spatio-temporal inconsistency of diffusion-based supervision, we propose a diffusion-aware loss function and a camera pose optimisation strategy that aligns synthetic views with the underlying scene geometry. Experiments on DyCheck, a challenging benchmark with extreme viewpoint variation, show that ViDAR outperforms all state-of-the-art baselines in visual quality and geometric consistency. We further highlight ViDAR's strong improvement over baselines on dynamic regions and provide a new benchmark to compare performance in reconstructing motion-rich parts of the scene. Project page: https://vidar-4d.github.io

</details>


### [41] [Phantom-Data : Towards a General Subject-Consistent Video Generation Dataset](https://arxiv.org/abs/2506.18851)
*Zhuowei Chen,Bingchuan Li,Tianxiang Ma,Lijie Liu,Mingcong Liu,Yi Zhang,Gen Li,Xinghui Li,Siyu Zhou,Qian He,Xinglong Wu*

Main category: cs.CV

TL;DR: 论文提出了Phantom-Data数据集，解决了现有主题到视频生成模型中的复制粘贴问题，通过跨场景数据提升文本指令的忠实度。


<details>
  <summary>Details</summary>
Motivation: 现有模型在主题到视频生成中存在复制粘贴问题，原因是训练时使用同场景参考图像，导致主题身份与背景属性纠缠。

Method: 构建Phantom-Data数据集，通过三阶段流程：主题检测、跨上下文主题检索和身份验证，确保视觉一致性。

Result: 实验表明，使用Phantom-Data训练显著提升了提示对齐和视觉质量，同时保持身份一致性。

Conclusion: Phantom-Data是首个通用跨场景主题到视频一致性数据集，有效解决了现有模型的局限性。

Abstract: Subject-to-video generation has witnessed substantial progress in recent years. However, existing models still face significant challenges in faithfully following textual instructions. This limitation, commonly known as the copy-paste problem, arises from the widely used in-pair training paradigm. This approach inherently entangles subject identity with background and contextual attributes by sampling reference images from the same scene as the target video. To address this issue, we introduce \textbf{Phantom-Data, the first general-purpose cross-pair subject-to-video consistency dataset}, containing approximately one million identity-consistent pairs across diverse categories. Our dataset is constructed via a three-stage pipeline: (1) a general and input-aligned subject detection module, (2) large-scale cross-context subject retrieval from more than 53 million videos and 3 billion images, and (3) prior-guided identity verification to ensure visual consistency under contextual variation. Comprehensive experiments show that training with Phantom-Data significantly improves prompt alignment and visual quality while preserving identity consistency on par with in-pair baselines.

</details>


### [42] [TAMMs: Temporal-Aware Multimodal Model for Satellite Image Change Understanding and Forecasting](https://arxiv.org/abs/2506.18862)
*Zhongbin Guo,Yuhao Wang,Ping Jian,Xinyue Chen,Wei Peng,Ertai E*

Main category: cs.CV

TL;DR: TAMMs模型通过轻量级时序模块和语义融合机制，提升了多模态大语言模型在卫星图像时序分析和未来场景生成中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在卫星图像时序分析中缺乏细粒度时空推理能力，本研究旨在评估MLLMs在复杂多模态动态建模中的潜力。

Method: 提出TAMMs模型，结合轻量级时序模块和语义融合控制注入机制（SFCI），增强MLLMs的时空推理能力。

Result: 实验表明TAMMs在时序变化理解和未来图像预测任务中优于基线模型。

Conclusion: 精心设计的时序推理和语义融合能充分发挥MLLMs在时空理解中的潜力。

Abstract: Satellite image time-series analysis demands fine-grained spatial-temporal reasoning, which remains a challenge for existing multimodal large language models (MLLMs). In this work, we study the capabilities of MLLMs on a novel task that jointly targets temporal change understanding and future scene generation, aiming to assess their potential for modeling complex multimodal dynamics over time. We propose TAMMs, a Temporal-Aware Multimodal Model for satellite image change understanding and forecasting, which enhances frozen MLLMs with lightweight temporal modules for structured sequence encoding and contextual prompting. To guide future image generation, TAMMs introduces a Semantic-Fused Control Injection (SFCI) mechanism that adaptively combines high-level semantic reasoning and structural priors within an enhanced ControlNet. This dual-path conditioning enables temporally consistent and semantically grounded image synthesis. Experiments demonstrate that TAMMs outperforms strong MLLM baselines in both temporal change understanding and future image forecasting tasks, highlighting how carefully designed temporal reasoning and semantic fusion can unlock the full potential of MLLMs for spatio-temporal understanding.

</details>


### [43] [OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation](https://arxiv.org/abs/2506.18866)
*Qijun Gan,Ruizi Yang,Jianke Zhu,Shaofei Xue,Steven Hoi*

Main category: cs.CV

TL;DR: OmniAvatar是一个创新的音频驱动全身视频生成模型，通过多层级音频嵌入策略和LoRA训练方法，提升了唇同步和自然动作，同时支持精确的文本控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注面部动作，难以生成自然同步的全身动画，且缺乏精细的提示控制能力。

Method: 采用像素级多层级音频嵌入策略和LoRA训练方法，结合音频特征和基础模型的提示控制能力。

Result: 实验表明，OmniAvatar在面部和半身视频生成上优于现有模型，支持多种场景的精确控制。

Conclusion: OmniAvatar在音频驱动动画中实现了更高的同步性和灵活性，同时保留了文本控制的精确性。

Abstract: Significant progress has been made in audio-driven human animation, while most existing methods focus mainly on facial movements, limiting their ability to create full-body animations with natural synchronization and fluidity. They also struggle with precise prompt control for fine-grained generation. To tackle these challenges, we introduce OmniAvatar, an innovative audio-driven full-body video generation model that enhances human animation with improved lip-sync accuracy and natural movements. OmniAvatar introduces a pixel-wise multi-hierarchical audio embedding strategy to better capture audio features in the latent space, enhancing lip-syncing across diverse scenes. To preserve the capability for prompt-driven control of foundation models while effectively incorporating audio features, we employ a LoRA-based training approach. Extensive experiments show that OmniAvatar surpasses existing models in both facial and semi-body video generation, offering precise text-based control for creating videos in various domains, such as podcasts, human interactions, dynamic scenes, and singing. Our project page is https://omni-avatar.github.io/.

</details>


### [44] [OmniGen2: Exploration to Advanced Multimodal Generation](https://arxiv.org/abs/2506.18871)
*Chenyuan Wu,Pengfei Zheng,Ruiran Yan,Shitao Xiao,Xin Luo,Yueze Wang,Wanli Li,Xiyan Jiang,Yexin Liu,Junjie Zhou,Ze Liu,Ziyi Xia,Chaofan Li,Haoge Deng,Jiahao Wang,Kun Luo,Bo Zhang,Defu Lian,Xinlong Wang,Zhongyuan Wang,Tiejun Huang,Zheng Liu*

Main category: cs.CV

TL;DR: OmniGen2是一个多功能开源生成模型，支持文本到图像、图像编辑和上下文生成任务，通过双解码路径和独立参数设计提升性能。


<details>
  <summary>Details</summary>
Motivation: 为解决多任务生成模型的统一性和性能问题，OmniGen2在OmniGen v1基础上改进，避免重新适应VAE输入并保留文本生成能力。

Method: 采用双解码路径（文本和图像模态）、独立参数和分离的图像标记器，结合反射机制和专用数据集进行训练。

Result: 在文本到图像和图像编辑任务中表现优异，并在新基准OmniContext中达到开源模型的最先进一致性。

Conclusion: OmniGen2通过创新设计和数据支持，为多任务生成提供了高效解决方案，并将开源模型和工具以促进未来研究。

Abstract: In this work, we introduce OmniGen2, a versatile and open-source generative model designed to provide a unified solution for diverse generation tasks, including text-to-image, image editing, and in-context generation. Unlike OmniGen v1, OmniGen2 features two distinct decoding pathways for text and image modalities, utilizing unshared parameters and a decoupled image tokenizer. This design enables OmniGen2 to build upon existing multimodal understanding models without the need to re-adapt VAE inputs, thereby preserving the original text generation capabilities. To facilitate the training of OmniGen2, we developed comprehensive data construction pipelines, encompassing image editing and in-context generation data. Additionally, we introduce a reflection mechanism tailored for image generation tasks and curate a dedicated reflection dataset based on OmniGen2. Despite its relatively modest parameter size, OmniGen2 achieves competitive results on multiple task benchmarks, including text-to-image and image editing. To further evaluate in-context generation, also referred to as subject-driven tasks, we introduce a new benchmark named OmniContext. OmniGen2 achieves state-of-the-art performance among open-source models in terms of consistency. We will release our models, training code, datasets, and data construction pipeline to support future research in this field. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link: https://github.com/VectorSpaceLab/OmniGen2

</details>


### [45] [Let Your Video Listen to Your Music!](https://arxiv.org/abs/2506.18881)
*Xinyu Zhang,Dong Gong,Zicheng Duan,Anton van den Hengel,Lingqiao Liu*

Main category: cs.CV

TL;DR: 提出了一种名为MVAA的框架，自动将视频与音乐节拍对齐，同时保留原始视觉内容。


<details>
  <summary>Details</summary>
Motivation: 多媒体制作中需要将视频运动节奏与音乐对齐，现有方法依赖人工或启发式编辑，缺乏灵活性。

Method: 采用两步法：1) 将运动关键帧与音频节拍对齐；2) 使用扩散模型生成连贯中间帧。

Result: 实验表明，该方法能高质量实现节拍对齐和视觉流畅性。

Conclusion: MVAA框架高效灵活，适用于视频与音乐的自动对齐。

Abstract: Aligning the rhythm of visual motion in a video with a given music track is a practical need in multimedia production, yet remains an underexplored task in autonomous video editing. Effective alignment between motion and musical beats enhances viewer engagement and visual appeal, particularly in music videos, promotional content, and cinematic editing. Existing methods typically depend on labor-intensive manual cutting, speed adjustments, or heuristic-based editing techniques to achieve synchronization. While some generative models handle joint video and music generation, they often entangle the two modalities, limiting flexibility in aligning video to music beats while preserving the full visual content. In this paper, we propose a novel and efficient framework, termed MVAA (Music-Video Auto-Alignment), that automatically edits video to align with the rhythm of a given music track while preserving the original visual content. To enhance flexibility, we modularize the task into a two-step process in our MVAA: aligning motion keyframes with audio beats, followed by rhythm-aware video inpainting. Specifically, we first insert keyframes at timestamps aligned with musical beats, then use a frame-conditioned diffusion model to generate coherent intermediate frames, preserving the original video's semantic content. Since comprehensive test-time training can be time-consuming, we adopt a two-stage strategy: pretraining the inpainting module on a small video set to learn general motion priors, followed by rapid inference-time fine-tuning for video-specific adaptation. This hybrid approach enables adaptation within 10 minutes with one epoch on a single NVIDIA 4090 GPU using CogVideoX-5b-I2V as the backbone. Extensive experiments show that our approach can achieve high-quality beat alignment and visual smoothness.

</details>


### [46] [FilMaster: Bridging Cinematic Principles and Generative AI for Automated Film Generation](https://arxiv.org/abs/2506.18899)
*Kaiyi Huang,Yukun Huang,Xintao Wang,Zinan Lin,Xuefei Ning,Pengfei Wan,Di Zhang,Yu Wang,Xihui Liu*

Main category: cs.CV

TL;DR: FilMaster是一个端到端的AI系统，通过整合真实世界的电影原则生成专业级电影，解决了现有系统在镜头语言和节奏上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有AI电影生成系统缺乏多样化的镜头语言和电影节奏，导致视觉效果模板化和叙事不吸引人。

Method: FilMaster基于两个关键原则：从大量电影数据中学习摄影技术，并模拟以观众为中心的后期制作流程。系统分为参考引导生成阶段和生成后期制作阶段。

Result: FilMaster在镜头语言设计和电影节奏控制方面表现优异，并通过FilmEval基准验证。

Conclusion: FilMaster推动了生成式AI在专业电影制作中的应用，提供了可编辑、行业标准的输出。

Abstract: AI-driven content creation has shown potential in film production. However, existing film generation systems struggle to implement cinematic principles and thus fail to generate professional-quality films, particularly lacking diverse camera language and cinematic rhythm. This results in templated visuals and unengaging narratives. To address this, we introduce FilMaster, an end-to-end AI system that integrates real-world cinematic principles for professional-grade film generation, yielding editable, industry-standard outputs. FilMaster is built on two key principles: (1) learning cinematography from extensive real-world film data and (2) emulating professional, audience-centric post-production workflows. Inspired by these principles, FilMaster incorporates two stages: a Reference-Guided Generation Stage which transforms user input to video clips, and a Generative Post-Production Stage which transforms raw footage into audiovisual outputs by orchestrating visual and auditory elements for cinematic rhythm. Our generation stage highlights a Multi-shot Synergized RAG Camera Language Design module to guide the AI in generating professional camera language by retrieving reference clips from a vast corpus of 440,000 film clips. Our post-production stage emulates professional workflows by designing an Audience-Centric Cinematic Rhythm Control module, including Rough Cut and Fine Cut processes informed by simulated audience feedback, for effective integration of audiovisual elements to achieve engaging content. The system is empowered by generative AI models like (M)LLMs and video generation models. Furthermore, we introduce FilmEval, a comprehensive benchmark for evaluating AI-generated films. Extensive experiments show FilMaster's superior performance in camera language design and cinematic rhythm control, advancing generative AI in professional filmmaking.

</details>


### [47] [Audit & Repair: An Agentic Framework for Consistent Story Visualization in Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.18900)
*Kiymet Akdemir,Tahira Kazimi,Pinar Yanardag*

Main category: cs.CV

TL;DR: 提出了一种多智能体协作框架，用于解决故事可视化中角色和对象的一致性保持问题。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在故事可视化中难以保持角色和对象的一致性，导致叙事不连贯。

Method: 采用多智能体协作框架，通过迭代循环自主识别、修正和优化不一致性，支持细粒度面板级更新。

Result: 定量和定性实验表明，该方法在多面板一致性上优于现有方法。

Conclusion: 提出的框架模型无关，可灵活集成多种扩散模型，有效提升故事可视化的一致性。

Abstract: Story visualization has become a popular task where visual scenes are generated to depict a narrative across multiple panels. A central challenge in this setting is maintaining visual consistency, particularly in how characters and objects persist and evolve throughout the story. Despite recent advances in diffusion models, current approaches often fail to preserve key character attributes, leading to incoherent narratives. In this work, we propose a collaborative multi-agent framework that autonomously identifies, corrects, and refines inconsistencies across multi-panel story visualizations. The agents operate in an iterative loop, enabling fine-grained, panel-level updates without re-generating entire sequences. Our framework is model-agnostic and flexibly integrates with a variety of diffusion models, including rectified flow transformers such as Flux and latent diffusion models such as Stable Diffusion. Quantitative and qualitative experiments show that our method outperforms prior approaches in terms of multi-panel consistency.

</details>


### [48] [From Virtual Games to Real-World Play](https://arxiv.org/abs/2506.18901)
*Wenqiang Sun,Fangyun Wei,Jinjing Zhao,Xi Chen,Zilong Chen,Hongyang Zhang,Jun Zhang,Yan Lu*

Main category: cs.CV

TL;DR: RealPlay是一个基于神经网络的实时游戏引擎，通过用户控制信号生成交互式视频，专注于逼真且时间一致的视频序列。


<details>
  <summary>Details</summary>
Motivation: 旨在生成逼真的视频序列，而非传统游戏风格的视觉效果，同时解决低延迟反馈、时间一致性和准确控制响应的挑战。

Method: 采用迭代分块预测技术，结合标记的游戏数据和无标记的真实世界视频进行训练，无需真实世界动作标注。

Result: 实现了控制信号从虚拟到真实场景的映射，并能够泛化控制多种真实世界实体（如自行车和行人）。

Conclusion: RealPlay展示了在逼真视频生成和交互控制方面的强大能力，具有广泛的应用潜力。

Abstract: We introduce RealPlay, a neural network-based real-world game engine that enables interactive video generation from user control signals. Unlike prior works focused on game-style visuals, RealPlay aims to produce photorealistic, temporally consistent video sequences that resemble real-world footage. It operates in an interactive loop: users observe a generated scene, issue a control command, and receive a short video chunk in response. To enable such realistic and responsive generation, we address key challenges including iterative chunk-wise prediction for low-latency feedback, temporal consistency across iterations, and accurate control response. RealPlay is trained on a combination of labeled game data and unlabeled real-world videos, without requiring real-world action annotations. Notably, we observe two forms of generalization: (1) control transfer-RealPlay effectively maps control signals from virtual to real-world scenarios; and (2) entity transfer-although training labels originate solely from a car racing game, RealPlay generalizes to control diverse real-world entities, including bicycles and pedestrians, beyond vehicles. Project page can be found: https://wenqsun.github.io/RealPlay/

</details>


### [49] [TC-Light: Temporally Consistent Relighting for Dynamic Long Videos](https://arxiv.org/abs/2506.18904)
*Yang Liu,Chuanchen Luo,Zimo Tang,Yingyan Li,Yuran Yang,Yuanyong Ning,Lue Fan,Junran Peng,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: TC-Light提出了一种两阶段优化机制，用于长视频中的光照编辑，解决了现有方法在时间一致性和计算效率上的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 长视频中复杂动态的光照编辑在视觉内容创作和AI数据扩展方面具有重要价值，但现有技术局限于肖像视频或面临时间一致性和计算效率问题。

Method: TC-Light采用两阶段后优化机制：第一阶段优化外观嵌入以对齐全局光照；第二阶段优化提出的唯一视频张量（UVT）以对齐细粒度纹理和光照。

Result: 实验表明，该方法能够实现物理上合理的光照编辑结果，具有优异的时间一致性和低计算成本。

Conclusion: TC-Light为长视频光照编辑提供了一种高效且一致的解决方案，适用于多种下游任务。

Abstract: Editing illumination in long videos with complex dynamics has significant value in various downstream tasks, including visual content creation and manipulation, as well as data scaling up for embodied AI through sim2real and real2real transfer. Nevertheless, existing video relighting techniques are predominantly limited to portrait videos or fall into the bottleneck of temporal consistency and computation efficiency. In this paper, we propose TC-Light, a novel paradigm characterized by the proposed two-stage post optimization mechanism. Starting from the video preliminarily relighted by an inflated video relighting model, it optimizes appearance embedding in the first stage to align global illumination. Then it optimizes the proposed canonical video representation, i.e., Unique Video Tensor (UVT), to align fine-grained texture and lighting in the second stage. To comprehensively evaluate performance, we also establish a long and highly dynamic video benchmark. Extensive experiments show that our method enables physically plausible relighting results with superior temporal coherence and low computation cost. The code and video demos are available at https://dekuliutesla.github.io/tclight/.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [50] [Can Common VLMs Rival Medical VLMs? Evaluation and Strategic Insights](https://arxiv.org/abs/2506.17337)
*Yuan Zhong,Ruinan Jin,Xiaoxiao Li,Qi Dou*

Main category: eess.IV

TL;DR: 研究表明，经过轻量级微调的通用视觉语言模型（VLMs）在特定医学成像任务中表现优于或与专用医学VLMs相当，挑战了医学专用预训练的必要性。


<details>
  <summary>Details</summary>
Motivation: 探讨通用VLMs（如CLIP、LLaVA）是否可以通过微调在医学成像任务中与专用医学VLMs竞争，以节省计算和数据资源。

Method: 系统评估通用和医学VLMs在疾病诊断和视觉问答（VQA）任务中的表现，包括域内（ID）和域外（OOD）任务，并采用LoRA微调方法。

Result: 通用VLMs在微调后表现优异，尤其在域内任务中，甚至超越专用医学VLMs；在域外任务中也展现出较强的适应性。

Conclusion: 通用VLMs结合微调是一种可扩展且经济高效的替代方案，为医学成像领域的未来研究提供了重要启示。

Abstract: Medical vision-language models (VLMs) leverage large-scale pretraining for diverse imaging tasks but require substantial computational and data resources. Meanwhile, common or general-purpose VLMs (e.g., CLIP, LLaVA), though not trained for medical use, show promise with fine-tuning. This raises a key question: Can efficient fine-tuned common VLMs rival generalist medical VLMs for solving specific medical imaging tasks? This study systematically evaluates common and medical VLMs across disease diagnosis and visual question answering (VQA). Using CLIP-based and LLaVA-based models, we examine (1) off-the-shelf performance gaps in in-domain (ID) settings, (2) whether fine-tuning bridges these gaps, and (3) generalization to out-of-domain (OOD) tasks on unseen medical modalities. While medical-specific pretraining provides advantages in ID settings, common VLMs match or surpass medical-specific models after lightweight fine-tuning, with LoRA-based adaptation proving highly effective among different tasks. In OOD tasks, common VLMs demonstrate strong adaptability in some tasks, challenging the assumption that medical-specific pre-training is essential. These findings suggest that leveraging common VLMs with fine-tuning offers a scalable and cost-effective alternative to developing large-scale medical VLMs, providing crucial insights for future research in the medical imaging field.

</details>


### [51] [MTSIC: Multi-stage Transformer-based GAN for Spectral Infrared Image Colorization](https://arxiv.org/abs/2506.17540)
*Tingting Liu,Yuan Liu,Jinhui Tang,Liyin Yuan,Chengyu Liu,Chunlai Li,Xiubao Sui,Qian Chen*

Main category: eess.IV

TL;DR: 提出了一种基于GAN的多波段红外图像彩色化方法，通过多阶段光谱自注意力Transformer网络（MTSIC）提升图像质量和语义准确性。


<details>
  <summary>Details</summary>
Motivation: 热红外（TIR）图像缺乏颜色和纹理信息，限制了其应用并可能导致视觉疲劳。现有方法因单波段信息不足导致图像失真和语义模糊。

Method: 采用GAN框架，结合多波段光谱信息，设计了MTSIC生成器，利用空间-光谱注意力残差块（SARB）和U形架构提取上下文信息，并通过多尺度小波块（MSWB）对齐语义。

Result: 实验表明，该方法显著优于传统技术，有效提升了红外图像的视觉质量。

Conclusion: 提出的MTSIC框架通过多波段光谱信息融合，显著改善了红外图像彩色化的效果。

Abstract: Thermal infrared (TIR) images, acquired through thermal radiation imaging, are unaffected by variations in lighting conditions and atmospheric haze. However, TIR images inherently lack color and texture information, limiting downstream tasks and potentially causing visual fatigue. Existing colorization methods primarily rely on single-band images with limited spectral information and insufficient feature extraction capabilities, which often result in image distortion and semantic ambiguity. In contrast, multiband infrared imagery provides richer spectral data, facilitating the preservation of finer details and enhancing semantic accuracy. In this paper, we propose a generative adversarial network (GAN)-based framework designed to integrate spectral information to enhance the colorization of infrared images. The framework employs a multi-stage spectral self-attention Transformer network (MTSIC) as the generator. Each spectral feature is treated as a token for self-attention computation, and a multi-head self-attention mechanism forms a spatial-spectral attention residual block (SARB), achieving multi-band feature mapping and reducing semantic confusion. Multiple SARB units are integrated into a Transformer-based single-stage network (STformer), which uses a U-shaped architecture to extract contextual information, combined with multi-scale wavelet blocks (MSWB) to align semantic information in the spatial-frequency dual domain. Multiple STformer modules are cascaded to form MTSIC, progressively optimizing the reconstruction quality. Experimental results demonstrate that the proposed method significantly outperforms traditional techniques and effectively enhances the visual quality of infrared images.

</details>


### [52] [Transforming H&E images into IHC: A Variance-Penalized GAN for Precision Oncology](https://arxiv.org/abs/2506.18371)
*Sara Rehmat,Hafeez Ur Rehman*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的图像翻译框架，从H&E染色样本生成高质量IHC图像，用于低成本、可扩展的HER2评估。


<details>
  <summary>Details</summary>
Motivation: HER2阳性乳腺癌的诊断依赖昂贵的IHC技术，而H&E染色虽普及但缺乏特异性。本研究旨在通过AI技术解决这一矛盾。

Method: 改进金字塔pix2pix的损失函数，引入基于方差的惩罚项，提升生成图像的多样性和质量，特别针对HER2阳性（IHC 3+）图像。

Result: 在BCI数据集上，模型在PSNR、SSIM和FID指标上优于现有方法，尤其在HER2阳性图像翻译中表现突出。

Conclusion: 该模型为HER2诊断提供了高效替代方案，并展示了在通用图像翻译任务中的潜力，推动了AI驱动的精准肿瘤学发展。

Abstract: The overexpression of the human epidermal growth factor receptor 2 (HER2) in breast cells is a key driver of HER2-positive breast cancer, a highly aggressive subtype requiring precise diagnosis and targeted therapy. Immunohistochemistry (IHC) is the standard technique for HER2 assessment but is costly, labor-intensive, and highly dependent on antibody selection. In contrast, hematoxylin and eosin (H&E) staining, a routine histopathological procedure, offers broader accessibility but lacks HER2 specificity. This study proposes an advanced deep learning-based image translation framework to generate highfidelity IHC images from H&E-stained tissue samples, enabling cost-effective and scalable HER2 assessment. By modifying the loss function of pyramid pix2pix, we mitigate mode collapse, a fundamental limitation in generative adversarial networks (GANs), and introduce a novel variance-based penalty that enforces structural diversity in generated images. Our model particularly excels in translating HER2-positive (IHC 3+) images, which have remained challenging for existing methods due to their complex morphological variations. Extensive evaluations on the BCI histopathological dataset demonstrate that our model surpasses state-of-the-art methods in terms of peak signal-tonoise ratio (PSNR), structural similarity index (SSIM), and Frechet Inception Distance (FID), particularly in accurately translating HER2-positive (IHC 3+) images. Beyond medical imaging, our model exhibits superior performance in general image-to-image translation tasks, showcasing its potential across multiple domains. This work marks a significant step toward AI-driven precision oncology, offering a reliable and efficient alternative to traditional HER2 diagnostics.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [53] [Origins of Creativity in Attention-Based Diffusion Models](https://arxiv.org/abs/2506.17324)
*Emma Finn,T. Anderson Keller,Manos Theodosis,Demba E. Ba*

Main category: cs.LG

TL;DR: 本文探讨了扩散模型中创造性的起源，特别是自注意力机制如何影响生成图像的全局一致性。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型在图像生成中的广泛应用，理解其创造性来源变得尤为重要。现有理论未能解释自注意力在生成过程中的作用。

Method: 扩展了现有理论，研究了由CNN和自注意力层参数化的扩散模型，并通过实验验证了自注意力的影响。

Result: 理论表明自注意力能促进生成图像中局部特征的全局一致性，实验验证了这一行为。

Conclusion: 自注意力在扩散模型中起到了关键作用，使生成的图像在全局上更一致。

Abstract: As diffusion models have become the tool of choice for image generation and as the quality of the images continues to improve, the question of how `creativity' originates in diffusion has become increasingly important. The score matching perspective on diffusion has proven particularly fruitful for understanding how and why diffusion models generate images that remain plausible while differing significantly from their training images. In particular, as explained in (Kamb \& Ganguli, 2024) and others, e.g., (Ambrogioni, 2023), theory suggests that if our score matching were optimal, we would only be able to recover training samples through our diffusion process. However, as shown by Kamb \& Ganguli, (2024), in diffusion models where the score is parametrized by a simple CNN, the inductive biases of the CNN itself (translation equivariance and locality) allow the model to generate samples that globally do not match any training samples, but are rather patch-wise `mosaics'. Notably, however, this theory does not extend to describe the role of self-attention in this process. In this work, we take a preliminary step in this direction to extend this theory to the case of diffusion models whose score is parametrized by a CNN with a final self-attention layer. We show that our theory suggests that self-attention will induce a globally image-consistent arrangement of local features beyond the patch-level in generated samples, and we verify this behavior empirically on a carefully crafted dataset.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [54] [TCDiff++: An End-to-end Trajectory-Controllable Diffusion Model for Harmonious Music-Driven Group Choreography](https://arxiv.org/abs/2506.18671)
*Yuqin Dai,Wanlu Zhu,Ronghui Li,Xiu Li,Zhenyu Zhang,Jun Li,Jian Yang*

Main category: cs.SD

TL;DR: TCDiff++是一个音乐驱动的端到端框架，旨在解决群舞生成中的碰撞、脚滑和位置突变问题，通过嵌入和损失函数优化，实现高质量群舞生成。


<details>
  <summary>Details</summary>
Motivation: 现有群舞生成方法存在多舞者碰撞、单舞者脚滑和长序列位置突变等问题，限制了实际应用效果。

Method: 提出TCDiff++框架，包括舞者定位嵌入、距离一致性损失、交换模式嵌入、步法适配器和长序列扩散采样策略。

Result: 实验表明，TCDiff++在长序列场景下表现优异，生成高质量且连贯的群舞。

Conclusion: TCDiff++通过创新方法有效解决了群舞生成中的关键问题，提升了生成效果。

Abstract: Music-driven dance generation has garnered significant attention due to its wide range of industrial applications, particularly in the creation of group choreography. During the group dance generation process, however, most existing methods still face three primary issues: multi-dancer collisions, single-dancer foot sliding and abrupt swapping in the generation of long group dance. In this paper, we propose TCDiff++, a music-driven end-to-end framework designed to generate harmonious group dance. Specifically, to mitigate multi-dancer collisions, we utilize a dancer positioning embedding to better maintain the relative positioning among dancers. Additionally, we incorporate a distance-consistency loss to ensure that inter-dancer distances remain within plausible ranges. To address the issue of single-dancer foot sliding, we introduce a swap mode embedding to indicate dancer swapping patterns and design a Footwork Adaptor to refine raw motion, thereby minimizing foot sliding. For long group dance generation, we present a long group diffusion sampling strategy that reduces abrupt position shifts by injecting positional information into the noisy input. Furthermore, we integrate a Sequence Decoder layer to enhance the model's ability to selectively process long sequences. Extensive experiments demonstrate that our TCDiff++ achieves state-of-the-art performance, particularly in long-duration scenarios, ensuring high-quality and coherent group dance generation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [55] [GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale Multi-Agent Gaussian SLAM](https://arxiv.org/abs/2506.18885)
*Annika Thomas,Aneesa Sonawalla,Alex Rose,Jonathan P. How*

Main category: cs.RO

TL;DR: GRAND-SLAM是一种多智能体高斯泼溅SLAM方法，适用于大尺度户外环境，结合了局部优化和闭环检测，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有高斯泼溅SLAM方法仅适用于小规模室内环境，无法满足大尺度多智能体户外场景的需求。

Method: 提出GRAND-SLAM方法，包括基于局部优化的隐式跟踪模块和多智能体闭环检测的位姿图优化框架。

Result: 在Replica室内数据集上，GRAND-SLAM的跟踪性能和PSNR分别优于现有方法28%；在Kimera-Multi户外数据集上，多智能体跟踪误差降低91%。

Conclusion: GRAND-SLAM在大尺度多智能体户外环境中表现出色，为场景重建和探索提供了高效解决方案。

Abstract: 3D Gaussian splatting has emerged as an expressive scene representation for RGB-D visual SLAM, but its application to large-scale, multi-agent outdoor environments remains unexplored. Multi-agent Gaussian SLAM is a promising approach to rapid exploration and reconstruction of environments, offering scalable environment representations, but existing approaches are limited to small-scale, indoor environments. To that end, we propose Gaussian Reconstruction via Multi-Agent Dense SLAM, or GRAND-SLAM, a collaborative Gaussian splatting SLAM method that integrates i) an implicit tracking module based on local optimization over submaps and ii) an approach to inter- and intra-robot loop closure integrated into a pose-graph optimization framework. Experiments show that GRAND-SLAM provides state-of-the-art tracking performance and 28% higher PSNR than existing methods on the Replica indoor dataset, as well as 91% lower multi-agent tracking error and improved rendering over existing multi-agent methods on the large-scale, outdoor Kimera-Multi dataset.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [56] [Can Generated Images Serve as a Viable Modality for Text-Centric Multimodal Learning?](https://arxiv.org/abs/2506.17623)
*Yuesheng Huang,Peng Zhang,Riliang Liu,Jiaqi Liang*

Main category: cs.MM

TL;DR: 研究探讨了通过文本生成图像（T2I）模型为文本任务提供补充模态的可行性，发现其性能提升显著但条件苛刻。


<details>
  <summary>Details</summary>
Motivation: 解决文本数据与多模态模型之间的模态差距，探索T2I模型在文本任务中的潜在价值。

Method: 通过文本分类任务，评估T2I模型质量、提示工程策略和多模态融合架构的影响。

Result: 合成感知能显著提升性能，但效果高度依赖文本与图像的语义对齐、任务的可视化基础及T2I模型的生成质量。

Conclusion: 研究为这一范式建立了首个严格基准，明确了其潜力与局限，展示了在传统单模态场景中丰富语言理解的可行性。

Abstract: A significant ``modality gap" exists between the abundance of text-only data and the increasing power of multimodal models. This work systematically investigates whether images generated on-the-fly by Text-to-Image (T2I) models can serve as a valuable complementary modality for text-centric tasks. Through a comprehensive evaluation framework on text classification, we analyze the impact of critical variables, including T2I model quality, prompt engineering strategies, and multimodal fusion architectures. Our findings demonstrate that this``synthetic perception" can yield significant performance gains, even when augmenting strong large language model baselines. However, we find the effectiveness of this approach is highly conditional, depending critically on the semantic alignment between text and the generated image, the inherent ``visual groundability" of the task, and the generative fidelity of the T2I model. Our work establishes the first rigorous benchmark for this paradigm, providing a clear analysis of its potential and current limitations, and demonstrating its viability as a pathway to enrich language understanding in traditionally unimodal scenarios.

</details>
