<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 3]
- [cs.CV](#cs.CV) [Total: 27]
- [eess.IV](#eess.IV) [Total: 6]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Zero-Shot Dynamic Concept Personalization with Grid-Based LoRA](https://arxiv.org/abs/2507.17963)
*Rameen Abdal,Or Patashnik,Ekaterina Deyneka,Hao Chen,Aliaksandr Siarohin,Sergey Tulyakov,Daniel Cohen-Or,Kfir Aberman*

Main category: cs.GR

TL;DR: 提出了一种零样本框架，用于文本到视频模型中的动态概念个性化，无需微调即可推广到新概念。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要针对每个实例进行微调，限制了可扩展性，因此需要一种零样本方法。

Method: 利用结构化2x2视频网格训练轻量级Grid-LoRA适配器，并通过Grid Fill模块完成部分观察的布局。

Result: 实验表明，该方法在未见过的动态概念和编辑场景中均能产生高质量且一致的结果。

Conclusion: 该方法通过单次前向传播实现动态概念个性化，具有高效性和可扩展性。

Abstract: Recent advances in text-to-video generation have enabled high-quality synthesis from text and image prompts. While the personalization of dynamic concepts, which capture subject-specific appearance and motion from a single video, is now feasible, most existing methods require per-instance fine-tuning, limiting scalability. We introduce a fully zero-shot framework for dynamic concept personalization in text-to-video models. Our method leverages structured 2x2 video grids that spatially organize input and output pairs, enabling the training of lightweight Grid-LoRA adapters for editing and composition within these grids. At inference, a dedicated Grid Fill module completes partially observed layouts, producing temporally coherent and identity preserving outputs. Once trained, the entire system operates in a single forward pass, generalizing to previously unseen dynamic concepts without any test-time optimization. Extensive experiments demonstrate high-quality and consistent results across a wide range of subjects beyond trained concepts and editing scenarios.

</details>


### [2] [GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar](https://arxiv.org/abs/2507.18155)
*SeungJun Moon,Hah Min Lew,Seungeun Lee,Ji-Su Kang,Gyeong-Moon Park*

Main category: cs.GR

TL;DR: GeoAvatar提出了一种自适应几何高斯泼溅框架，通过APS分割高斯为刚性和柔性集，结合嘴部结构和部分变形策略，提升动画质量。


<details>
  <summary>Details</summary>
Motivation: 解决3D头部头像生成中身份保持与新颖姿态和表情动画之间的平衡问题。

Method: 采用自适应预分配阶段（APS）分割高斯，提出嘴部结构和部分变形策略，并引入正则化损失。

Result: GeoAvatar在重建和新动画场景中优于现有方法。

Conclusion: GeoAvatar通过自适应几何高斯泼溅和嘴部优化，显著提升了头像生成质量。

Abstract: Despite recent progress in 3D head avatar generation, balancing identity preservation, i.e., reconstruction, with novel poses and expressions, i.e., animation, remains a challenge. Existing methods struggle to adapt Gaussians to varying geometrical deviations across facial regions, resulting in suboptimal quality. To address this, we propose GeoAvatar, a framework for adaptive geometrical Gaussian Splatting. GeoAvatar leverages Adaptive Pre-allocation Stage (APS), an unsupervised method that segments Gaussians into rigid and flexible sets for adaptive offset regularization. Then, based on mouth anatomy and dynamics, we introduce a novel mouth structure and the part-wise deformation strategy to enhance the animation fidelity of the mouth. Finally, we propose a regularization loss for precise rigging between Gaussians and 3DMM faces. Moreover, we release DynamicFace, a video dataset with highly expressive facial motions. Extensive experiments show the superiority of GeoAvatar compared to state-of-the-art methods in reconstruction and novel animation scenarios.

</details>


### [3] [PS-GS: Gaussian Splatting for Multi-View Photometric Stereo](https://arxiv.org/abs/2507.18231)
*Yixiao Chen,Bin Liang,Hanzhi Guo,Yongqing Cheng,Jiayi Zhao,Dongdong Weng*

Main category: cs.GR

TL;DR: PS-GS方法通过结合高斯点云和逆渲染技术，在多光源条件下高效估计几何、材质和光照，显著提升了3D重建的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有逆渲染方法依赖固定环境光照，导致3D重建精度不足。PS-GS旨在通过多光源和多视角数据解决这一问题。

Method: 首先构建2D高斯点云模型作为初始几何，随后通过包含光照计算的多层感知机进行逆渲染，并结合未标定的光度立体法估计法线进行正则化。

Result: 实验表明，PS-GS在合成和真实数据集上均优于现有方法，重建精度和计算效率更高。

Conclusion: PS-GS为多光源条件下的3D重建提供了高效且准确的解决方案，适用于新视角合成、重光照及材质编辑等应用。

Abstract: Integrating inverse rendering with multi-view photometric stereo (MVPS) yields more accurate 3D reconstructions than the inverse rendering approaches that rely on fixed environment illumination. However, efficient inverse rendering with MVPS remains challenging. To fill this gap, we introduce the Gaussian Splatting for Multi-view Photometric Stereo (PS-GS), which efficiently and jointly estimates the geometry, materials, and lighting of the object that is illuminated by diverse directional lights (multi-light). Our method first reconstructs a standard 2D Gaussian splatting model as the initial geometry. Based on the initialization model, it then proceeds with the deferred inverse rendering by the full rendering equation containing a lighting-computing multi-layer perceptron. During the whole optimization, we regularize the rendered normal maps by the uncalibrated photometric stereo estimated normals. We also propose the 2D Gaussian ray-tracing for single directional light to refine the incident lighting. The regularizations and the use of multi-view and multi-light images mitigate the ill-posed problem of inverse rendering. After optimization, the reconstructed object can be used for novel-view synthesis, relighting, and material and shape editing. Experiments on both synthetic and real datasets demonstrate that our method outperforms prior works in terms of reconstruction accuracy and computational efficiency.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [4] [Lumina-mGPT 2.0: Stand-Alone AutoRegressive Image Modeling](https://arxiv.org/abs/2507.17801)
*Yi Xin,Juncheng Yan,Qi Qin,Zhen Li,Dongyang Liu,Shicheng Li,Victor Shea-Jay Huang,Yupeng Zhou,Renrui Zhang,Le Zhuo,Tiancheng Han,Xiaoqing Sun,Siqi Luo,Mengmeng Wang,Bin Fu,Yuewen Cao,Hongsheng Li,Guangtao Zhai,Xiaohong Liu,Yu Qiao,Peng Gao*

Main category: cs.CV

TL;DR: Lumina-mGPT 2.0是一个独立的解码器自回归模型，专注于高质量图像生成，无需依赖预训练组件或混合架构，实现了与扩散模型相媲美的生成质量，并支持多任务处理。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预训练组件或混合架构，限制了设计自由和许可灵活性。Lumina-mGPT 2.0旨在通过完全从头训练，提供更灵活、可组合的自回归模型。

Method: 采用统一的分词方案，支持多任务处理（如生成、编辑、可控合成等），并结合高效解码策略（如推理时间缩放和推测性Jacobi采样）提升质量和速度。

Result: 在标准文本到图像基准测试中表现优异，甚至超越扩散模型；在Graph200K基准测试中展示了多任务能力。

Conclusion: Lumina-mGPT 2.0是一个强大、灵活的统一多模态生成基础模型，已开源训练细节、代码和模型。

Abstract: We present Lumina-mGPT 2.0, a stand-alone, decoder-only autoregressive model that revisits and revitalizes the autoregressive paradigm for high-quality image generation and beyond. Unlike existing approaches that rely on pretrained components or hybrid architectures, Lumina-mGPT 2.0 is trained entirely from scratch, enabling unrestricted architectural design and licensing freedom. It achieves generation quality on par with state-of-the-art diffusion models such as DALL-E 3 and SANA, while preserving the inherent flexibility and compositionality of autoregressive modeling. Our unified tokenization scheme allows the model to seamlessly handle a wide spectrum of tasks-including subject-driven generation, image editing, controllable synthesis, and dense prediction-within a single generative framework. To further boost usability, we incorporate efficient decoding strategies like inference-time scaling and speculative Jacobi sampling to improve quality and speed, respectively. Extensive evaluations on standard text-to-image benchmarks (e.g., GenEval, DPG) demonstrate that Lumina-mGPT 2.0 not only matches but in some cases surpasses diffusion-based models. Moreover, we confirm its multi-task capabilities on the Graph200K benchmark, with the native Lumina-mGPT 2.0 performing exceptionally well. These results position Lumina-mGPT 2.0 as a strong, flexible foundation model for unified multimodal generation. We have released our training details, code, and models at https://github.com/Alpha-VLLM/Lumina-mGPT-2.0.

</details>


### [5] [SV3.3B: A Sports Video Understanding Model for Action Recognition](https://arxiv.org/abs/2507.17844)
*Sai Varun Kodathala,Yashwanth Reddy Vutukoori,Rakesh Vunnam*

Main category: cs.CV

TL;DR: SV3.3B是一个轻量级的3.3B参数视频理解模型，用于自动化体育视频分析，结合了时间运动差异采样和自监督学习，显著提升了运动动作的细粒度理解。


<details>
  <summary>Details</summary>
Motivation: 传统体育视频分析模型计算量大且缺乏对运动动作的细粒度理解，无法捕捉关键的生物力学过渡阶段。

Method: 采用DWT-VGG16-LDA关键帧提取机制和V-DWT-JEPA2编码器，结合LLM解码器，实现高效设备端部署。

Result: 在NSVA篮球数据集上表现优异，生成技术上详细且分析丰富的体育描述，性能超过GPT-4o变体。

Conclusion: SV3.3B在计算需求低的情况下，显著提升了体育视频分析的性能和信息密度。

Abstract: This paper addresses the challenge of automated sports video analysis, which has traditionally been limited by computationally intensive models requiring server-side processing and lacking fine-grained understanding of athletic movements. Current approaches struggle to capture the nuanced biomechanical transitions essential for meaningful sports analysis, often missing critical phases like preparation, execution, and follow-through that occur within seconds. To address these limitations, we introduce SV3.3B, a lightweight 3.3B parameter video understanding model that combines novel temporal motion difference sampling with self-supervised learning for efficient on-device deployment. Our approach employs a DWT-VGG16-LDA based keyframe extraction mechanism that intelligently identifies the 16 most representative frames from sports sequences, followed by a V-DWT-JEPA2 encoder pretrained through mask-denoising objectives and an LLM decoder fine-tuned for sports action description generation. Evaluated on a subset of the NSVA basketball dataset, SV3.3B achieves superior performance across both traditional text generation metrics and sports-specific evaluation criteria, outperforming larger closed-source models including GPT-4o variants while maintaining significantly lower computational requirements. Our model demonstrates exceptional capability in generating technically detailed and analytically rich sports descriptions, achieving 29.2% improvement over GPT-4o in ground truth validation metrics, with substantial improvements in information density, action complexity, and measurement precision metrics essential for comprehensive athletic analysis. Model Available at https://huggingface.co/sportsvision/SV3.3B.

</details>


### [6] [Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis](https://arxiv.org/abs/2507.17860)
*Ko Watanabe. Stanislav Frolov. Adriano Lucieri. Andreas Dengel*

Main category: cs.CV

TL;DR: 利用生成式AI（GenAI）模型评估皮肤癌分类器的公平性，发现合成数据在公平性评估中具有潜力，但需注意数据一致性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在皮肤癌筛查中的应用潜力巨大，但存在潜在偏见，需评估和改进公平性。

Method: 使用最先进的生成式AI模型（LightningDiT）评估公开黑色素瘤分类器的公平性。

Result: 合成数据在公平性评估中表现良好，但若评估模型与合成数据训练集不一致，公平性验证会变得困难。

Conclusion: 合成数据为医学影像AI系统的公平性评估提供了新途径，但需确保数据一致性。

Abstract: Recent advancements in Deep Learning and its application on the edge hold great potential for the revolution of routine screenings for skin cancers like Melanoma. Along with the anticipated benefits of this technology, potential dangers arise from unforseen and inherent biases. Thus, assessing and improving the fairness of such systems is of utmost importance. A key challenge in fairness assessment is to ensure that the evaluation dataset is sufficiently representative of different Personal Identifiable Information (PII) (sex, age, and race) and other minority groups. Against the backdrop of this challenge, this study leverages the state-of-the-art Generative AI (GenAI) LightningDiT model to assess the fairness of publicly available melanoma classifiers. The results suggest that fairness assessment using highly realistic synthetic data is a promising direction. Yet, our findings indicate that verifying fairness becomes difficult when the melanoma-detection model used for evaluation is trained on data that differ from the dataset underpinning the synthetic images. Nonetheless, we propose that our approach offers a valuable new avenue for employing synthetic data to gauge and enhance fairness in medical-imaging GenAI systems.

</details>


### [7] [DiNAT-IR: Exploring Dilated Neighborhood Attention for High-Quality Image Restoration](https://arxiv.org/abs/2507.17892)
*Hanzhou Liu,Binghan Li,Chengkai Liu,Mi Lu*

Main category: cs.CV

TL;DR: 论文提出了一种基于Transformer的图像修复方法DiNAT-IR，通过结合局部和全局注意力机制，解决了现有方法在高效性和质量之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer方法在图像修复任务中因自注意力机制的高计算成本而难以扩展到高分辨率图像，且可能忽略局部细节。

Method: 引入Dilated Neighborhood Attention (DiNA)结合滑动窗口注意力和混合膨胀因子，并加入通道感知模块以整合全局上下文。

Result: DiNAT-IR在多个基准测试中取得竞争性结果，为低层计算机视觉问题提供了高质量解决方案。

Conclusion: DiNAT-IR通过平衡全局和局部注意力，显著提升了图像修复的质量和效率。

Abstract: Transformers, with their self-attention mechanisms for modeling long-range dependencies, have become a dominant paradigm in image restoration tasks. However, the high computational cost of self-attention limits scalability to high-resolution images, making efficiency-quality trade-offs a key research focus. To address this, Restormer employs channel-wise self-attention, which computes attention across channels instead of spatial dimensions. While effective, this approach may overlook localized artifacts that are crucial for high-quality image restoration. To bridge this gap, we explore Dilated Neighborhood Attention (DiNA) as a promising alternative, inspired by its success in high-level vision tasks. DiNA balances global context and local precision by integrating sliding-window attention with mixed dilation factors, effectively expanding the receptive field without excessive overhead. However, our preliminary experiments indicate that directly applying this global-local design to the classic deblurring task hinders accurate visual restoration, primarily due to the constrained global context understanding within local attention. To address this, we introduce a channel-aware module that complements local attention, effectively integrating global context without sacrificing pixel-level precision. The proposed DiNAT-IR, a Transformer-based architecture specifically designed for image restoration, achieves competitive results across multiple benchmarks, offering a high-quality solution for diverse low-level computer vision problems.

</details>


### [8] [AFRDA: Attentive Feature Refinement for Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2507.17957)
*Md. Al-Masrur Khan,Durgakant Pushp,Lantao Liu*

Main category: cs.CV

TL;DR: 提出了一种自适应特征细化（AFR）模块，通过结合高低分辨率特征和高频组件，提升无监督域自适应语义分割（UDA-SS）的性能。


<details>
  <summary>Details</summary>
Motivation: 现有UDA-SS方法在平衡局部细节和全局上下文信息方面表现不佳，导致复杂区域分割错误。

Method: 引入AFR模块，利用低分辨率语义先验细化高分辨率特征，并整合高频组件以捕捉细粒度结构。通过不确定性驱动注意力平衡局部与全局信息。

Result: 在GTA V→Cityscapes和Synthia→Cityscapes上分别提升1.05%和1.04%的mIoU。

Conclusion: AFR模块轻量高效，显著提升了UDA-SS的分割精度。

Abstract: In Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS), a model is trained on labeled source domain data (e.g., synthetic images) and adapted to an unlabeled target domain (e.g., real-world images) without access to target annotations. Existing UDA-SS methods often struggle to balance fine-grained local details with global contextual information, leading to segmentation errors in complex regions. To address this, we introduce the Adaptive Feature Refinement (AFR) module, which enhances segmentation accuracy by refining highresolution features using semantic priors from low-resolution logits. AFR also integrates high-frequency components, which capture fine-grained structures and provide crucial boundary information, improving object delineation. Additionally, AFR adaptively balances local and global information through uncertaintydriven attention, reducing misclassifications. Its lightweight design allows seamless integration into HRDA-based UDA methods, leading to state-of-the-art segmentation performance. Our approach improves existing UDA-SS methods by 1.05% mIoU on GTA V --> Cityscapes and 1.04% mIoU on Synthia-->Cityscapes. The implementation of our framework is available at: https://github.com/Masrur02/AFRDA

</details>


### [9] [High-fidelity 3D Gaussian Inpainting: preserving multi-view consistency and photorealistic details](https://arxiv.org/abs/2507.18023)
*Jun Zhou,Dinghao Li,Nannan Li,Mingjie Wang*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯分布的修复框架，通过稀疏修复视图重建完整3D场景，结合自动掩模优化和不确定性引导优化，显著提升了修复效果。


<details>
  <summary>Details</summary>
Motivation: 3D场景修复因结构不规则和多视角一致性需求而具有挑战性，现有方法难以满足高保真需求。

Method: 采用自动掩模优化（高斯场景过滤和反投影）和不确定性引导的细粒度优化策略，提升修复精度和多视角一致性。

Result: 在多个数据集上实验表明，该方法在视觉质量和视角一致性上优于现有方法。

Conclusion: 提出的框架有效解决了3D场景修复中的多视角一致性和细节保真问题，具有显著优势。

Abstract: Recent advancements in multi-view 3D reconstruction and novel-view synthesis, particularly through Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have greatly enhanced the fidelity and efficiency of 3D content creation. However, inpainting 3D scenes remains a challenging task due to the inherent irregularity of 3D structures and the critical need for maintaining multi-view consistency. In this work, we propose a novel 3D Gaussian inpainting framework that reconstructs complete 3D scenes by leveraging sparse inpainted views. Our framework incorporates an automatic Mask Refinement Process and region-wise Uncertainty-guided Optimization. Specifically, we refine the inpainting mask using a series of operations, including Gaussian scene filtering and back-projection, enabling more accurate localization of occluded regions and realistic boundary restoration. Furthermore, our Uncertainty-guided Fine-grained Optimization strategy, which estimates the importance of each region across multi-view images during training, alleviates multi-view inconsistencies and enhances the fidelity of fine details in the inpainted results. Comprehensive experiments conducted on diverse datasets demonstrate that our approach outperforms existing state-of-the-art methods in both visual quality and view consistency.

</details>


### [10] [ViGText: Deepfake Image Detection with Vision-Language Model Explanations and Graph Neural Networks](https://arxiv.org/abs/2507.18031)
*Ahmad ALBarqawi,Mahmoud Nazzal,Issa Khalil,Abdallah Khreishah,NhatHai Phan*

Main category: cs.CV

TL;DR: ViGText是一种结合视觉大语言模型（VLLM）文本解释和图神经网络（GNNs）的新方法，用于提升深度伪造检测的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术的快速发展威胁媒体真实性，传统检测方法难以应对复杂、定制化的深度伪造内容。

Method: ViGText通过将图像分块、构建图像和文本图，并利用GNNs整合分析，结合空间和频域的多层次特征提取。

Result: 实验显示ViGText显著提升泛化性（F1分数从72.45%升至98.32%）和鲁棒性（召回率提高11.1%），对抗攻击性能下降小于4%。

Conclusion: ViGText通过详细的视觉和文本分析，为深度伪造检测设定了新标准，有助于保障媒体真实性和信息完整性。

Abstract: The rapid rise of deepfake technology, which produces realistic but fraudulent digital content, threatens the authenticity of media. Traditional deepfake detection approaches often struggle with sophisticated, customized deepfakes, especially in terms of generalization and robustness against malicious attacks. This paper introduces ViGText, a novel approach that integrates images with Vision Large Language Model (VLLM) Text explanations within a Graph-based framework to improve deepfake detection. The novelty of ViGText lies in its integration of detailed explanations with visual data, as it provides a more context-aware analysis than captions, which often lack specificity and fail to reveal subtle inconsistencies. ViGText systematically divides images into patches, constructs image and text graphs, and integrates them for analysis using Graph Neural Networks (GNNs) to identify deepfakes. Through the use of multi-level feature extraction across spatial and frequency domains, ViGText captures details that enhance its robustness and accuracy to detect sophisticated deepfakes. Extensive experiments demonstrate that ViGText significantly enhances generalization and achieves a notable performance boost when it detects user-customized deepfakes. Specifically, average F1 scores rise from 72.45% to 98.32% under generalization evaluation, and reflects the model's superior ability to generalize to unseen, fine-tuned variations of stable diffusion models. As for robustness, ViGText achieves an increase of 11.1% in recall compared to other deepfake detection approaches. When facing targeted attacks that exploit its graph-based architecture, ViGText limits classification performance degradation to less than 4%. ViGText uses detailed visual and textual analysis to set a new standard for detecting deepfakes, helping ensure media authenticity and information integrity.

</details>


### [11] [Enhancing Scene Transition Awareness in Video Generation via Post-Training](https://arxiv.org/abs/2507.18046)
*Hanwen Shen,Jiajie Lu,Yupeng Cao,Xiaonan Yang*

Main category: cs.CV

TL;DR: 论文提出了一种名为TAV的数据集，用于提升AI生成视频中场景转换的连贯性，解决了当前模型在生成多场景视频时的不足。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成的视频在单场景任务中表现良好，但在多场景视频生成中缺乏连贯的场景转换能力，主要因为模型无法从提示中推断何时需要转换。

Method: 提出了Transition-Aware Video (TAV)数据集，包含多场景转换的视频片段，并通过后训练提升模型对场景转换的理解。

Result: 实验表明，使用TAV数据集后训练能够改善场景转换理解，缩小所需与生成场景之间的差距，同时保持图像质量。

Conclusion: TAV数据集为多场景视频生成提供了有效解决方案，提升了模型的场景转换能力。

Abstract: Recent advances in AI-generated video have shown strong performance on \emph{text-to-video} tasks, particularly for short clips depicting a single scene. However, current models struggle to generate longer videos with coherent scene transitions, primarily because they cannot infer when a transition is needed from the prompt. Most open-source models are trained on datasets consisting of single-scene video clips, which limits their capacity to learn and respond to prompts requiring multiple scenes. Developing scene transition awareness is essential for multi-scene generation, as it allows models to identify and segment videos into distinct clips by accurately detecting transitions.   To address this, we propose the \textbf{Transition-Aware Video} (TAV) dataset, which consists of preprocessed video clips with multiple scene transitions. Our experiment shows that post-training on the \textbf{TAV} dataset improves prompt-based scene transition understanding, narrows the gap between required and generated scenes, and maintains image quality.

</details>


### [12] [BokehDiff: Neural Lens Blur with One-Step Diffusion](https://arxiv.org/abs/2507.18060)
*Chengxuan Zhu,Qingnan Fan,Qi Zhang,Jinwei Chen,Huaqi Zhang,Chao Xu,Boxin Shi*

Main category: cs.CV

TL;DR: BokehDiff是一种新颖的镜头模糊渲染方法，结合生成扩散先验，实现物理准确且视觉吸引人的效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于深度估计的准确性，在深度不连续处产生伪影。

Method: 采用物理启发的自注意力模块，结合深度依赖的模糊圈约束和自遮挡效应，并适配扩散模型到一步推理方案。

Result: 生成高质量和高保真的结果。

Conclusion: 通过扩散模型合成逼真前景，解决可扩展配对数据不足的问题，平衡真实性与场景多样性。

Abstract: We introduce BokehDiff, a novel lens blur rendering method that achieves physically accurate and visually appealing outcomes, with the help of generative diffusion prior. Previous methods are bounded by the accuracy of depth estimation, generating artifacts in depth discontinuities. Our method employs a physics-inspired self-attention module that aligns with the image formation process, incorporating depth-dependent circle of confusion constraint and self-occlusion effects. We adapt the diffusion model to the one-step inference scheme without introducing additional noise, and achieve results of high quality and fidelity. To address the lack of scalable paired data, we propose to synthesize photorealistic foregrounds with transparency with diffusion models, balancing authenticity and scene diversity.

</details>


### [13] [Adapting Large VLMs with Iterative and Manual Instructions for Generative Low-light Enhancement](https://arxiv.org/abs/2507.18064)
*Xiaoran Sun,Liyan Wang,Cong Wang,Yeying Jin,Kin-man Lam,Zhixun Su,Yang Yang,Jinshan Pan*

Main category: cs.CV

TL;DR: VLM-IMI是一种利用视觉语言模型和迭代手动指令的低光图像增强框架，通过语义指导提升复杂光照条件下的效果。


<details>
  <summary>Details</summary>
Motivation: 现有低光图像增强方法忽视正常光图像的语义指导，限制了其在复杂光照条件下的表现。

Method: 结合视觉语言模型和文本描述，通过指令先验融合模块动态对齐图像与文本特征，采用迭代手动指令策略优化结果。

Result: 在多样场景中，VLM-IMI在定量指标和感知质量上均优于现有方法。

Conclusion: VLM-IMI通过语义指导和动态融合显著提升了低光图像增强的效果。

Abstract: Most existing low-light image enhancement (LLIE) methods rely on pre-trained model priors, low-light inputs, or both, while neglecting the semantic guidance available from normal-light images. This limitation hinders their effectiveness in complex lighting conditions. In this paper, we propose VLM-IMI, a novel framework that leverages large vision-language models (VLMs) with iterative and manual instructions (IMIs) for LLIE. VLM-IMI incorporates textual descriptions of the desired normal-light content as enhancement cues, enabling semantically informed restoration. To effectively integrate cross-modal priors, we introduce an instruction prior fusion module, which dynamically aligns and fuses image and text features, promoting the generation of detailed and semantically coherent outputs. During inference, we adopt an iterative and manual instruction strategy to refine textual instructions, progressively improving visual quality. This refinement enhances structural fidelity, semantic alignment, and the recovery of fine details under extremely low-light conditions. Extensive experiments across diverse scenarios demonstrate that VLM-IMI outperforms state-of-the-art methods in both quantitative metrics and perceptual quality. The source code is available at https://github.com/sunxiaoran01/VLM-IMI.

</details>


### [14] [TextSAM-EUS: Text Prompt Learning for SAM to Accurately Segment Pancreatic Tumor in Endoscopic Ultrasound](https://arxiv.org/abs/2507.18082)
*Pascal Spiegler,Taha Koleilat,Arash Harirpoush,Corey S. Miller,Hassan Rivaz,Marta Kersten-Oertel,Yiming Xiao*

Main category: cs.CV

TL;DR: TextSAM-EUS是一种基于文本驱动的轻量级Segment Anything Model（SAM）改进方法，用于自动分割胰腺肿瘤，无需手动几何提示，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 胰腺癌预后差，依赖内镜超声（EUS）进行活检和放疗，但EUS图像噪声多、对比度低，传统深度学习模型分割效果差且依赖大量标注数据。

Method: 通过BiomedCLIP文本编码器和LoRA架构改进SAM，实现自动胰腺肿瘤分割，仅需调整0.86%的参数。

Result: 在公开数据集上，TextSAM-EUS的Dice和NSD分别达到82.69%和85.28%（自动提示），优于现有方法。

Conclusion: TextSAM-EUS是首个将提示学习引入SAM医学图像分割的方法，为EUS分割提供了高效、鲁棒的解决方案。

Abstract: Pancreatic cancer carries a poor prognosis and relies on endoscopic ultrasound (EUS) for targeted biopsy and radiotherapy. However, the speckle noise, low contrast, and unintuitive appearance of EUS make segmentation of pancreatic tumors with fully supervised deep learning (DL) models both error-prone and dependent on large, expert-curated annotation datasets. To address these challenges, we present TextSAM-EUS, a novel, lightweight, text-driven adaptation of the Segment Anything Model (SAM) that requires no manual geometric prompts at inference. Our approach leverages text prompt learning (context optimization) through the BiomedCLIP text encoder in conjunction with a LoRA-based adaptation of SAM's architecture to enable automatic pancreatic tumor segmentation in EUS, tuning only 0.86% of the total parameters. On the public Endoscopic Ultrasound Database of the Pancreas, TextSAM-EUS with automatic prompts attains 82.69% Dice and 85.28% normalized surface distance (NSD), and with manual geometric prompts reaches 83.10% Dice and 85.70% NSD, outperforming both existing state-of-the-art (SOTA) supervised DL models and foundation models (e.g., SAM and its variants). As the first attempt to incorporate prompt learning in SAM-based medical image segmentation, TextSAM-EUS offers a practical option for efficient and robust automatic EUS segmentation. Our code will be publicly available upon acceptance.

</details>


### [15] [T2VWorldBench: A Benchmark for Evaluating World Knowledge in Text-to-Video Generation](https://arxiv.org/abs/2507.18107)
*Yubin Chen,Xuyang Guo,Zhenmei Shi,Zhao Song,Jiahao Zhang*

Main category: cs.CV

TL;DR: T2VWorldBench是一个评估文本到视频模型世界知识生成能力的框架，发现当前模型在语义一致性和事实准确性方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 研究文本到视频模型在利用世界知识方面的能力不足，提出系统性评估方法。

Method: 提出T2VWorldBench框架，涵盖6大类、60子类和1200个提示，结合人工和自动化评估。

Result: 评估10个先进模型，发现多数无法正确生成符合世界知识的视频。

Conclusion: 当前模型在世界知识利用方面存在显著差距，为未来研究提供了方向。

Abstract: Text-to-video (T2V) models have shown remarkable performance in generating visually reasonable scenes, while their capability to leverage world knowledge for ensuring semantic consistency and factual accuracy remains largely understudied. In response to this challenge, we propose T2VWorldBench, the first systematic evaluation framework for evaluating the world knowledge generation abilities of text-to-video models, covering 6 major categories, 60 subcategories, and 1,200 prompts across a wide range of domains, including physics, nature, activity, culture, causality, and object. To address both human preference and scalable evaluation, our benchmark incorporates both human evaluation and automated evaluation using vision-language models (VLMs). We evaluated the 10 most advanced text-to-video models currently available, ranging from open source to commercial models, and found that most models are unable to understand world knowledge and generate truly correct videos. These findings point out a critical gap in the capability of current text-to-video models to leverage world knowledge, providing valuable research opportunities and entry points for constructing models with robust capabilities for commonsense reasoning and factual generation.

</details>


### [16] [Degradation-Consistent Learning via Bidirectional Diffusion for Low-Light Image Enhancement](https://arxiv.org/abs/2507.18144)
*Jinhong He,Minglong Xue,Zhipu Liu,Mingliang Zhou,Aoxiang Ning,Palaiahnakote Shivakumara*

Main category: cs.CV

TL;DR: 该论文提出了一种双向扩散优化机制，通过联合建模低光与正常光图像的退化过程，提升图像增强质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散方法因单向建模退化而难以捕捉真实世界复杂退化模式的问题。

Method: 采用双向扩散（低光到正常光及正常光到低光）训练，引入自适应特征交互块（AFI）和反射感知校正模块（RACM）。

Result: 在多个基准数据集上定量和定性评估均优于现有方法，且能泛化到多样化退化场景。

Conclusion: 提出的方法通过双向建模和特征优化，显著提升了低光图像增强的质量和一致性。

Abstract: Low-light image enhancement aims to improve the visibility of degraded images to better align with human visual perception. While diffusion-based methods have shown promising performance due to their strong generative capabilities. However, their unidirectional modelling of degradation often struggles to capture the complexity of real-world degradation patterns, leading to structural inconsistencies and pixel misalignments. To address these challenges, we propose a bidirectional diffusion optimization mechanism that jointly models the degradation processes of both low-light and normal-light images, enabling more precise degradation parameter matching and enhancing generation quality. Specifically, we perform bidirectional diffusion-from low-to-normal light and from normal-to-low light during training and introduce an adaptive feature interaction block (AFI) to refine feature representation. By leveraging the complementarity between these two paths, our approach imposes an implicit symmetry constraint on illumination attenuation and noise distribution, facilitating consistent degradation learning and improving the models ability to perceive illumination and detail degradation. Additionally, we design a reflection-aware correction module (RACM) to guide color restoration post-denoising and suppress overexposed regions, ensuring content consistency and generating high-quality images that align with human visual perception. Extensive experiments on multiple benchmark datasets demonstrate that our method outperforms state-of-the-art methods in both quantitative and qualitative evaluations while generalizing effectively to diverse degradation scenarios. Code at https://github.com/hejh8/BidDiff

</details>


### [17] [TeEFusion: Blending Text Embeddings to Distill Classifier-Free Guidance](https://arxiv.org/abs/2507.18192)
*Minghao Fu,Guo-Hua Wang,Xiaohao Chen,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CV

TL;DR: TeEFusion是一种高效的蒸馏方法，通过将引导幅度直接融入文本嵌入并蒸馏复杂采样策略，显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 解决CFG依赖两次前向传播和复杂采样策略导致的高推理成本问题。

Method: 通过线性操作融合条件和非条件文本嵌入，无需额外参数，学生模型学习教师模型的输出。

Result: 学生模型推理速度提升6倍，图像质量接近教师模型。

Conclusion: TeEFusion在保持图像质量的同时显著提升了效率。

Abstract: Recent advances in text-to-image synthesis largely benefit from sophisticated sampling strategies and classifier-free guidance (CFG) to ensure high-quality generation. However, CFG's reliance on two forward passes, especially when combined with intricate sampling algorithms, results in prohibitively high inference costs. To address this, we introduce TeEFusion (\textbf{Te}xt \textbf{E}mbeddings \textbf{Fusion}), a novel and efficient distillation method that directly incorporates the guidance magnitude into the text embeddings and distills the teacher model's complex sampling strategy. By simply fusing conditional and unconditional text embeddings using linear operations, TeEFusion reconstructs the desired guidance without adding extra parameters, simultaneously enabling the student model to learn from the teacher's output produced via its sophisticated sampling approach. Extensive experiments on state-of-the-art models such as SD3 demonstrate that our method allows the student to closely mimic the teacher's performance with a far simpler and more efficient sampling strategy. Consequently, the student model achieves inference speeds up to 6$\times$ faster than the teacher model, while maintaining image quality at levels comparable to those obtained through the teacher's complex sampling approach. The code is publicly available at \href{https://github.com/AIDC-AI/TeEFusion}{github.com/AIDC-AI/TeEFusion}.

</details>


### [18] [LEAF: Latent Diffusion with Efficient Encoder Distillation for Aligned Features in Medical Image Segmentation](https://arxiv.org/abs/2507.18214)
*Qilin Huang,Tianyu Lin,Zhiguang Chen,Fudan Zheng*

Main category: cs.CV

TL;DR: LEAF是一种基于潜在扩散模型的医学图像分割方法，通过直接预测分割图和使用特征蒸馏技术，提升了分割性能，同时保持了模型的高效性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在医学图像分割任务中未针对任务特性进行调整，且特征提取能力不足。

Method: 在微调过程中，将噪声预测模式替换为直接预测分割图，并采用特征蒸馏方法对齐卷积层和Transformer编码器的特征。

Result: 实验表明，LEAF在多种疾病类型的分割数据集上提升了原始扩散模型的性能，且未增加推理时的参数或计算量。

Conclusion: LEAF在不改变模型架构或增加计算负担的情况下，显著提升了医学图像分割的效果。

Abstract: Leveraging the powerful capabilities of diffusion models has yielded quite effective results in medical image segmentation tasks. However, existing methods typically transfer the original training process directly without specific adjustments for segmentation tasks. Furthermore, the commonly used pre-trained diffusion models still have deficiencies in feature extraction. Based on these considerations, we propose LEAF, a medical image segmentation model grounded in latent diffusion models. During the fine-tuning process, we replace the original noise prediction pattern with a direct prediction of the segmentation map, thereby reducing the variance of segmentation results. We also employ a feature distillation method to align the hidden states of the convolutional layers with the features from a transformer-based vision encoder. Experimental results demonstrate that our method enhances the performance of the original diffusion model across multiple segmentation datasets for different disease types. Notably, our approach does not alter the model architecture, nor does it increase the number of parameters or computation during the inference phase, making it highly efficient.

</details>


### [19] [MVG4D: Image Matrix-Based Multi-View and Motion Generation for 4D Content Creation from a Single Image](https://arxiv.org/abs/2507.18371)
*Xiaotian Chen,DongFu Yin,Fei Richard Yu,Xuanchen Li,Xinhao Zhang*

Main category: cs.CV

TL;DR: MVG4D是一个从单张静态图像生成动态4D内容的新框架，结合多视图合成和4D高斯泼溅技术，显著提升了时间一致性和视觉真实感。


<details>
  <summary>Details</summary>
Motivation: 尽管生成模型在数字内容创作方面取得了进展，但生成高保真且时间一致的动态4D内容仍具挑战性。

Method: MVG4D通过图像矩阵模块合成时间相干且空间多样化的多视图图像，并利用轻量级变形网络将3D高斯点云扩展到时间域。

Result: 在Objaverse数据集上的实验表明，MVG4D在CLIP-I、PSNR、FVD和时间效率上优于现有方法，减少了闪烁伪影并增强了结构细节。

Conclusion: MVG4D为从最小输入高效可控地生成4D内容开辟了新方向，提升了AR/VR体验。

Abstract: Advances in generative modeling have significantly enhanced digital content creation, extending from 2D images to complex 3D and 4D scenes. Despite substantial progress, producing high-fidelity and temporally consistent dynamic 4D content remains a challenge. In this paper, we propose MVG4D, a novel framework that generates dynamic 4D content from a single still image by combining multi-view synthesis with 4D Gaussian Splatting (4D GS). At its core, MVG4D employs an image matrix module that synthesizes temporally coherent and spatially diverse multi-view images, providing rich supervisory signals for downstream 3D and 4D reconstruction. These multi-view images are used to optimize a 3D Gaussian point cloud, which is further extended into the temporal domain via a lightweight deformation network. Our method effectively enhances temporal consistency, geometric fidelity, and visual realism, addressing key challenges in motion discontinuity and background degradation that affect prior 4D GS-based methods. Extensive experiments on the Objaverse dataset demonstrate that MVG4D outperforms state-of-the-art baselines in CLIP-I, PSNR, FVD, and time efficiency. Notably, it reduces flickering artifacts and sharpens structural details across views and time, enabling more immersive AR/VR experiences. MVG4D sets a new direction for efficient and controllable 4D generation from minimal inputs.

</details>


### [20] [HumanMaterial: Human Material Estimation from a Single Image via Progressive Training](https://arxiv.org/abs/2507.18385)
*Yu Jiang,Jiahao Xia,Jiongming Qin,Yusen Wang,Tuo Cao,Chunxia Xiao*

Main category: cs.CV

TL;DR: 论文提出了一种基于物理渲染的全人体逆向渲染方法，通过构建高质量数据集和设计渐进式训练模型，提升材质估计的准确性，尤其是皮肤的真实感。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法因材质数据简化和渲染方程限制导致的渲染结果真实感不足，尤其是皮肤渲染的问题。

Method: 构建OpenHumanBRDF数据集，包含位移和次表面散射等材质；设计HumanMaterial模型，采用渐进式训练策略和CPR损失函数优化材质估计。

Result: 在OpenHumanBRDF数据集和真实数据上实验表明，该方法在材质估计上达到最优性能。

Conclusion: 通过高质量数据集和渐进式训练策略，显著提升了全人体逆向渲染的真实感，尤其是皮肤渲染效果。

Abstract: Full-body Human inverse rendering based on physically-based rendering aims to acquire high-quality materials, which helps achieve photo-realistic rendering under arbitrary illuminations. This task requires estimating multiple material maps and usually relies on the constraint of rendering result. The absence of constraints on the material maps makes inverse rendering an ill-posed task. Previous works alleviated this problem by building material dataset for training, but their simplified material data and rendering equation lead to rendering results with limited realism, especially that of skin. To further alleviate this problem, we construct a higher-quality dataset (OpenHumanBRDF) based on scanned real data and statistical material data. In addition to the normal, diffuse albedo, roughness, specular albedo, we produce displacement and subsurface scattering to enhance the realism of rendering results, especially for the skin. With the increase in prediction tasks for more materials, using an end-to-end model as in the previous work struggles to balance the importance among various material maps, and leads to model underfitting. Therefore, we design a model (HumanMaterial) with progressive training strategy to make full use of the supervision information of the material maps and improve the performance of material estimation. HumanMaterial first obtain the initial material results via three prior models, and then refine the results by a finetuning model. Prior models estimate different material maps, and each map has different significance for rendering results. Thus, we design a Controlled PBR Rendering (CPR) loss, which enhances the importance of the materials to be optimized during the training of prior models. Extensive experiments on OpenHumanBRDF dataset and real data demonstrate that our method achieves state-of-the-art performance.

</details>


### [21] [Iwin Transformer: Hierarchical Vision Transformer using Interleaved Windows](https://arxiv.org/abs/2507.18405)
*Simin Huo,Ning Li*

Main category: cs.CV

TL;DR: Iwin Transformer是一种无需位置嵌入的分层视觉Transformer，通过创新的交错窗口注意力和深度可分离卷积，可直接从低分辨率到高分辨率进行微调。


<details>
  <summary>Details</summary>
Motivation: 解决Swin Transformer需要两个连续块才能近似全局注意力的限制，实现单模块内的全局信息交换。

Method: 结合交错窗口注意力（连接远距离标记）和深度可分离卷积（连接邻近标记），实现全局信息交换。

Result: 在ImageNet-1K上达到87.4%的top-1准确率，在语义分割和视频动作识别任务中表现优异。

Conclusion: Iwin Transformer的核心组件可作为独立模块替换自注意力模块，其方法有望启发未来研究，如视频生成中的Iwin 3D注意力。

Abstract: We introduce Iwin Transformer, a novel position-embedding-free hierarchical vision transformer, which can be fine-tuned directly from low to high resolution, through the collaboration of innovative interleaved window attention and depthwise separable convolution. This approach uses attention to connect distant tokens and applies convolution to link neighboring tokens, enabling global information exchange within a single module, overcoming Swin Transformer's limitation of requiring two consecutive blocks to approximate global attention. Extensive experiments on visual benchmarks demonstrate that Iwin Transformer exhibits strong competitiveness in tasks such as image classification (87.4 top-1 accuracy on ImageNet-1K), semantic segmentation and video action recognition. We also validate the effectiveness of the core component in Iwin as a standalone module that can seamlessly replace the self-attention module in class-conditional image generation. The concepts and methods introduced by the Iwin Transformer have the potential to inspire future research, like Iwin 3D Attention in video generation. The code and models are available at https://github.com/cominder/Iwin-Transformer.

</details>


### [22] [CRUISE: Cooperative Reconstruction and Editing in V2X Scenarios using Gaussian Splatting](https://arxiv.org/abs/2507.18473)
*Haoran Xu,Saining Zhang,Peishuo Li,Baijun Ye,Xiaoxue Chen,Huan-ang Gao,Jv Zheng,Xiaowei Song,Ziqiao Peng,Run Miao,Jinrang Jia,Yifeng Shi,Guangqi Yi,Hang Zhao,Hao Tang,Hongyang Li,Kaicheng Yu,Hao Zhao*

Main category: cs.CV

TL;DR: CRUISE是一个用于V2X驾驶环境的综合重建与合成框架，通过分解高斯散射技术实现高保真重建和灵活编辑，提升3D检测和跟踪性能，并生成具有挑战性的极端案例。


<details>
  <summary>Details</summary>
Motivation: 探索仿真在V2X场景中数据生成和增强的潜力，以支持自动驾驶中的车辆与基础设施协作。

Method: 采用分解高斯散射技术重建真实场景，支持动态交通参与者的编辑和图像渲染，用于数据集增强。

Result: CRUISE实现高保真重建，提升3D检测和跟踪性能，并能生成极端案例。

Conclusion: CRUISE为V2X场景提供了一种高效的数据生成和增强方法，推动了自动驾驶技术的发展。

Abstract: Vehicle-to-everything (V2X) communication plays a crucial role in autonomous driving, enabling cooperation between vehicles and infrastructure. While simulation has significantly contributed to various autonomous driving tasks, its potential for data generation and augmentation in V2X scenarios remains underexplored. In this paper, we introduce CRUISE, a comprehensive reconstruction-and-synthesis framework designed for V2X driving environments. CRUISE employs decomposed Gaussian Splatting to accurately reconstruct real-world scenes while supporting flexible editing. By decomposing dynamic traffic participants into editable Gaussian representations, CRUISE allows for seamless modification and augmentation of driving scenes. Furthermore, the framework renders images from both ego-vehicle and infrastructure views, enabling large-scale V2X dataset augmentation for training and evaluation. Our experimental results demonstrate that: 1) CRUISE reconstructs real-world V2X driving scenes with high fidelity; 2) using CRUISE improves 3D detection across ego-vehicle, infrastructure, and cooperative views, as well as cooperative 3D tracking on the V2X-Seq benchmark; and 3) CRUISE effectively generates challenging corner cases.

</details>


### [23] [COT-AD: Cotton Analysis Dataset](https://arxiv.org/abs/2507.18532)
*Akbar Ali,Mahek Vyas,Soumyaratna Debnath,Chanda Grover Kamra,Jaidev Sanjay Khalane,Reuben Shibu Devanesan,Indra Deep Mastan,Subramanian Sankaranarayanan,Pankaj Khanna,Shanmuganathan Raman*

Main category: cs.CV

TL;DR: COT-AD是一个用于棉花作物分析的计算机视觉数据集，包含25,000多张图像和5,000张标注图像，支持分类、分割、图像修复等多种任务。


<details>
  <summary>Details</summary>
Motivation: 解决棉花农业数据集的不足，提供全面的棉花生长周期图像和标注数据。

Method: 通过航拍和高分辨率DSLR图像收集数据，标注包括病虫害识别、植被和杂草分析。

Result: COT-AD数据集支持多种计算机视觉任务，推动了数据驱动的作物管理。

Conclusion: COT-AD填补了棉花农业数据集的空白，为作物管理和疾病早期防治提供了重要资源。

Abstract: This paper presents COT-AD, a comprehensive Dataset designed to enhance cotton crop analysis through computer vision. Comprising over 25,000 images captured throughout the cotton growth cycle, with 5,000 annotated images, COT-AD includes aerial imagery for field-scale detection and segmentation and high-resolution DSLR images documenting key diseases. The annotations cover pest and disease recognition, vegetation, and weed analysis, addressing a critical gap in cotton-specific agricultural datasets. COT-AD supports tasks such as classification, segmentation, image restoration, enhancement, deep generative model-based cotton crop synthesis, and early disease management, advancing data-driven crop management

</details>


### [24] [Elucidating the Design Space of Arbitrary-Noise-Based Diffusion Models](https://arxiv.org/abs/2507.18534)
*Xingyu Qiu,Mengying Yang,Xinghua Ma,Dong Liang,Yuzhen Li,Fanding Li,Gongning Luo,Wei Wang,Kuanquan Wang,Shuo Li*

Main category: cs.CV

TL;DR: EDA扩展了基于任意噪声的扩散模型设计空间，解决了EDM固定高斯噪声限制的问题，提升了图像恢复性能。


<details>
  <summary>Details</summary>
Motivation: EDM的固定高斯噪声模式限制了图像恢复的进展，强制注入高斯噪声会破坏退化图像并增加恢复复杂度。

Method: 提出了EDA，扩展噪声模式的自由度，同时保持EDM模块的灵活性，并证明噪声复杂度增加不会带来额外计算开销。

Result: 在MRI偏置场校正、CT金属伪影减少和自然图像阴影去除任务中，仅需5次采样步骤，EDA表现优于大多数任务专用方法，并在偏置场校正和阴影去除中达到SOTA。

Conclusion: EDA通过引入任意噪声模式，显著提升了扩散模型在图像恢复任务中的性能，且计算效率高。

Abstract: EDM elucidates the unified design space of diffusion models, yet its fixed noise patterns restricted to pure Gaussian noise, limit advancements in image restoration. Our study indicates that forcibly injecting Gaussian noise corrupts the degraded images, overextends the image transformation distance, and increases restoration complexity. To address this problem, our proposed EDA Elucidates the Design space of Arbitrary-noise-based diffusion models. Theoretically, EDA expands the freedom of noise pattern while preserving the original module flexibility of EDM, with rigorous proof that increased noise complexity incurs no additional computational overhead during restoration. EDA is validated on three typical tasks: MRI bias field correction (global smooth noise), CT metal artifact reduction (global sharp noise), and natural image shadow removal (local boundary-aware noise). With only 5 sampling steps, EDA outperforms most task-specific methods and achieves state-of-the-art performance in bias field correction and shadow removal.

</details>


### [25] [Unposed 3DGS Reconstruction with Probabilistic Procrustes Mapping](https://arxiv.org/abs/2507.18541)
*Chong Cheng,Zijian Wang,Sicheng Yu,Yu Hu,Nanjie Yao,Hao Wang*

Main category: cs.CV

TL;DR: 提出了一种新的无姿态3D高斯分布重建框架，结合预训练MVS先验和概率Procrustes映射策略，解决了大规模户外图像重建中的内存和精度问题。


<details>
  <summary>Details</summary>
Motivation: 现有MVS模型在大规模无姿态图像重建中面临内存限制和精度下降问题，需要一种更高效的方法。

Method: 将输入图像划分为子集，通过概率Procrustes映射全局对齐点云和姿态，并联合优化3D高斯分布和相机姿态。

Result: 在Waymo和KITTI数据集上实现了高精度重建，成为无姿态3D高斯分布重建的新标杆。

Conclusion: 该方法有效解决了大规模无姿态图像重建的挑战，提升了重建精度和效率。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a core technique for 3D representation. Its effectiveness largely depends on precise camera poses and accurate point cloud initialization, which are often derived from pretrained Multi-View Stereo (MVS) models. However, in unposed reconstruction task from hundreds of outdoor images, existing MVS models may struggle with memory limits and lose accuracy as the number of input images grows. To address this limitation, we propose a novel unposed 3DGS reconstruction framework that integrates pretrained MVS priors with the probabilistic Procrustes mapping strategy. The method partitions input images into subsets, maps submaps into a global space, and jointly optimizes geometry and poses with 3DGS. Technically, we formulate the mapping of tens of millions of point clouds as a probabilistic Procrustes problem and solve a closed-form alignment. By employing probabilistic coupling along with a soft dustbin mechanism to reject uncertain correspondences, our method globally aligns point clouds and poses within minutes across hundreds of images. Moreover, we propose a joint optimization framework for 3DGS and camera poses. It constructs Gaussians from confidence-aware anchor points and integrates 3DGS differentiable rendering with an analytical Jacobian to jointly refine scene and poses, enabling accurate reconstruction and pose estimation. Experiments on Waymo and KITTI datasets show that our method achieves accurate reconstruction from unposed image sequences, setting a new state of the art for unposed 3DGS reconstruction.

</details>


### [26] [Facial Demorphing from a Single Morph Using a Latent Conditional GAN](https://arxiv.org/abs/2507.18566)
*Nitish Shukla,Arun Ross*

Main category: cs.CV

TL;DR: 本文提出了一种新的去变形方法，解决了现有方法在去变形过程中容易复制变形图像或依赖相同变形技术的问题。该方法在潜在空间分解变形图像，适用于未见过的变形技术和人脸风格。


<details>
  <summary>Details</summary>
Motivation: 现有的去变形方法存在变形复制问题，且通常假设训练和测试使用相同的变形技术。本文旨在克服这些问题，提供更通用的去变形解决方案。

Method: 通过在潜在空间分解变形图像，该方法能够处理未见过的变形技术和人脸风格。训练使用合成人脸生成的变形图像，测试时应用于真实人脸和任意变形技术。

Result: 该方法显著优于现有方法，能够生成高保真度的去变形人脸图像。

Conclusion: 提出的方法在去变形任务中表现出色，解决了现有技术的局限性，适用于更广泛的变形技术和人脸风格。

Abstract: A morph is created by combining two (or more) face images from two (or more) identities to create a composite image that is highly similar to both constituent identities, allowing the forged morph to be biometrically associated with more than one individual. Morph Attack Detection (MAD) can be used to detect a morph, but does not reveal the constituent images. Demorphing - the process of deducing the constituent images - is thus vital to provide additional evidence about a morph. Existing demorphing methods suffer from the morph replication problem, where the outputs tend to look very similar to the morph itself, or assume that train and test morphs are generated using the same morph technique. The proposed method overcomes these issues. The method decomposes a morph in latent space allowing it to demorph images created from unseen morph techniques and face styles. We train our method on morphs created from synthetic faces and test on morphs created from real faces using arbitrary morph techniques. Our method outperforms existing methods by a considerable margin and produces high fidelity demorphed face images.

</details>


### [27] [Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis](https://arxiv.org/abs/2507.18569)
*Yanzuo Lu,Yuxi Ren,Xin Xia,Shanchuan Lin,Xing Wang,Xuefeng Xiao,Andy J. Ma,Xiaohua Xie,Jian-Huang Lai*

Main category: cs.CV

TL;DR: DMDX是一种通过对抗性蒸馏预训练和ADM微调的统一框架，显著提升了一步生成性能，并在图像和视频合成中设定了新基准。


<details>
  <summary>Details</summary>
Motivation: 解决DMD中反向KL散度最小化导致的模式崩溃问题，提出更有效的分数蒸馏方法。

Method: 提出ADM框架，利用扩散判别器在潜空间中对齐真实和虚假分数估计器；结合对抗性蒸馏预训练和ADM微调为DMDX。

Result: 在SDXL上的一步生成性能优于DMD2，GPU时间更少；在SD3-Medium、SD3.5-Large和CogVideoX上设定了新基准。

Conclusion: DMDX通过对抗性蒸馏和ADM微调，显著提升了分数蒸馏的效率和质量。

Abstract: Distribution Matching Distillation (DMD) is a promising score distillation technique that compresses pre-trained teacher diffusion models into efficient one-step or multi-step student generators. Nevertheless, its reliance on the reverse Kullback-Leibler (KL) divergence minimization potentially induces mode collapse (or mode-seeking) in certain applications. To circumvent this inherent drawback, we propose Adversarial Distribution Matching (ADM), a novel framework that leverages diffusion-based discriminators to align the latent predictions between real and fake score estimators for score distillation in an adversarial manner. In the context of extremely challenging one-step distillation, we further improve the pre-trained generator by adversarial distillation with hybrid discriminators in both latent and pixel spaces. Different from the mean squared error used in DMD2 pre-training, our method incorporates the distributional loss on ODE pairs collected from the teacher model, and thus providing a better initialization for score distillation fine-tuning in the next stage. By combining the adversarial distillation pre-training with ADM fine-tuning into a unified pipeline termed DMDX, our proposed method achieves superior one-step performance on SDXL compared to DMD2 while consuming less GPU time. Additional experiments that apply multi-step ADM distillation on SD3-Medium, SD3.5-Large, and CogVideoX set a new benchmark towards efficient image and video synthesis.

</details>


### [28] [DRWKV: Focusing on Object Edges for Low-Light Image Enhancement](https://arxiv.org/abs/2507.18594)
*Xuecheng Bai,Yuxiang Wang,Boyu Hu,Qinyuan Jie,Chuanzhi Xu,Hongru Xiao,Kechen Li,Vera Chung*

Main category: cs.CV

TL;DR: DRWKV模型通过GER理论、Evolving WKV Attention和Bi-SAB机制，提升了低光图像增强的边缘保真度和视觉效果，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决低光图像增强中边缘连续性和细节保留的挑战。

Method: 提出DRWKV模型，结合GER理论、Evolving WKV Attention和Bi-SAB机制，优化边缘和结构分离。

Result: 在PSNR、SSIM和NIQE指标上表现领先，计算复杂度低，且能提升下游任务性能。

Conclusion: DRWKV在低光图像增强中具有高效性和泛化能力。

Abstract: Low-light image enhancement remains a challenging task, particularly in preserving object edge continuity and fine structural details under extreme illumination degradation. In this paper, we propose a novel model, DRWKV (Detailed Receptance Weighted Key Value), which integrates our proposed Global Edge Retinex (GER) theory, enabling effective decoupling of illumination and edge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV Attention, a spiral-scanning mechanism that captures spatial edge continuity and models irregular structures more effectively. Thirdly, we design the Bilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align luminance and chrominance features, improving visual naturalness and mitigating artifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV achieves leading performance in PSNR, SSIM, and NIQE while maintaining low computational complexity. Furthermore, DRWKV enhances downstream performance in low-light multi-object tracking tasks, validating its generalization capabilities.

</details>


### [29] [SIDA: Synthetic Image Driven Zero-shot Domain Adaptation](https://arxiv.org/abs/2507.18632)
*Ye-Chan Kim,SeungJu Cha,Si-Woo Kim,Taewhan Kim,Dong-Jin Kim*

Main category: cs.CV

TL;DR: 提出了一种基于合成图像的零样本域适应方法SIDA，通过生成目标域风格的合成图像来替代文本描述，提高了适应效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本描述的零样本域适应方法难以捕捉复杂现实变化且耗时较长，因此探索利用图像数据提供更细粒度的风格线索。

Method: 通过生成源域风格的详细图像并应用图像翻译生成目标域风格的合成图像，利用其风格特征作为目标域代理，并引入Domain Mix和Patch Style Transfer模块。

Result: 在多种零样本适应场景中表现优异，尤其在挑战性领域，同时显著减少了适应时间。

Conclusion: SIDA方法通过合成图像有效解决了文本驱动方法的局限性，实现了高效且高性能的零样本域适应。

Abstract: Zero-shot domain adaptation is a method for adapting a model to a target domain without utilizing target domain image data. To enable adaptation without target images, existing studies utilize CLIP's embedding space and text description to simulate target-like style features. Despite the previous achievements in zero-shot domain adaptation, we observe that these text-driven methods struggle to capture complex real-world variations and significantly increase adaptation time due to their alignment process. Instead of relying on text descriptions, we explore solutions leveraging image data, which provides diverse and more fine-grained style cues. In this work, we propose SIDA, a novel and efficient zero-shot domain adaptation method leveraging synthetic images. To generate synthetic images, we first create detailed, source-like images and apply image translation to reflect the style of the target domain. We then utilize the style features of these synthetic images as a proxy for the target domain. Based on these features, we introduce Domain Mix and Patch Style Transfer modules, which enable effective modeling of real-world variations. In particular, Domain Mix blends multiple styles to expand the intra-domain representations, and Patch Style Transfer assigns different styles to individual patches. We demonstrate the effectiveness of our method by showing state-of-the-art performance in diverse zero-shot adaptation scenarios, particularly in challenging domains. Moreover, our approach achieves high efficiency by significantly reducing the overall adaptation time.

</details>


### [30] [Captain Cinema: Towards Short Movie Generation](https://arxiv.org/abs/2507.18634)
*Junfei Xiao,Ceyuan Yang,Lvmin Zhang,Shengqu Cai,Yang Zhao,Yuwei Guo,Gordon Wetzstein,Maneesh Agrawala,Alan Yuille,Lu Jiang*

Main category: cs.CV

TL;DR: Captain Cinema是一个用于生成短电影的框架，通过结合自上而下的关键帧规划和自下而上的视频合成，实现了高质量、高效率的自动化电影创作。


<details>
  <summary>Details</summary>
Motivation: 解决自动化生成视觉连贯且叙事一致的短电影的挑战。

Method: 采用多模态扩散变换器（MM-DiT）和交错训练策略，结合关键帧规划和视频合成。

Result: 实验表明，Captain Cinema能够高效生成视觉连贯且叙事一致的短电影。

Conclusion: 该框架为自动化电影创作提供了高质量和高效率的解决方案。

Abstract: We present Captain Cinema, a generation framework for short movie generation. Given a detailed textual description of a movie storyline, our approach firstly generates a sequence of keyframes that outline the entire narrative, which ensures long-range coherence in both the storyline and visual appearance (e.g., scenes and characters). We refer to this step as top-down keyframe planning. These keyframes then serve as conditioning signals for a video synthesis model, which supports long context learning, to produce the spatio-temporal dynamics between them. This step is referred to as bottom-up video synthesis. To support stable and efficient generation of multi-scene long narrative cinematic works, we introduce an interleaved training strategy for Multimodal Diffusion Transformers (MM-DiT), specifically adapted for long-context video data. Our model is trained on a specially curated cinematic dataset consisting of interleaved data pairs. Our experiments demonstrate that Captain Cinema performs favorably in the automated creation of visually coherent and narrative consistent short movies in high quality and efficiency. Project page: https://thecinema.ai

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [31] [Improving Multislice Electron Ptychography with a Generative Prior](https://arxiv.org/abs/2507.17800)
*Christian K. Belardi,Chia-Hao Lee,Yingheng Wang,Justin Lovelace,Kilian Q. Weinberger,David A. Muller,Carla P. Gomes*

Main category: eess.IV

TL;DR: MEP-Diffusion结合扩散模型与迭代求解器，显著提升多切片电子衍射成像质量。


<details>
  <summary>Details</summary>
Motivation: 现有迭代算法耗时长且解不理想，需改进。

Method: 开发MEP-Diffusion扩散模型，作为生成先验集成到现有方法中。

Result: SSIM提升90.50%，重建3D体积质量显著提高。

Conclusion: 混合方法有效优化了多切片电子衍射成像的重建效果。

Abstract: Multislice electron ptychography (MEP) is an inverse imaging technique that computationally reconstructs the highest-resolution images of atomic crystal structures from diffraction patterns. Available algorithms often solve this inverse problem iteratively but are both time consuming and produce suboptimal solutions due to their ill-posed nature. We develop MEP-Diffusion, a diffusion model trained on a large database of crystal structures specifically for MEP to augment existing iterative solvers. MEP-Diffusion is easily integrated as a generative prior into existing reconstruction methods via Diffusion Posterior Sampling (DPS). We find that this hybrid approach greatly enhances the quality of the reconstructed 3D volumes, achieving a 90.50% improvement in SSIM over existing methods.

</details>


### [32] [Hierarchical Diffusion Framework for Pseudo-Healthy Brain MRI Inpainting with Enhanced 3D Consistency](https://arxiv.org/abs/2507.17911)
*Dou Hoon Kwark,Shirui Luo,Xiyue Zhu,Yudu Li,Zhi-Pei Liang,Volodymyr Kindratenko*

Main category: eess.IV

TL;DR: 提出了一种分层扩散框架，通过两个垂直的2D阶段（轴向和冠状）实现伪健康MRI图像修复，平衡数据效率和体积一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，2D模型切片间不连续，3D模型需要大量训练数据。本文旨在解决这些问题。

Method: 采用轴向扩散模型生成全局一致的粗修复，再用冠状扩散模型细化解剖细节。结合垂直空间视图和自适应重采样。

Result: 实验表明，该方法在真实性和体积一致性上优于现有基线。

Conclusion: 该方法为伪健康图像修复提供了高效且一致的解决方案。

Abstract: Pseudo-healthy image inpainting is an essential preprocessing step for analyzing pathological brain MRI scans. Most current inpainting methods favor slice-wise 2D models for their high in-plane fidelity, but their independence across slices produces discontinuities in the volume. Fully 3D models alleviate this issue, but their high model capacity demands extensive training data for reliable, high-fidelity synthesis -- often impractical in medical settings. We address these limitations with a hierarchical diffusion framework by replacing direct 3D modeling with two perpendicular coarse-to-fine 2D stages. An axial diffusion model first yields a coarse, globally consistent inpainting; a coronal diffusion model then refines anatomical details. By combining perpendicular spatial views with adaptive resampling, our method balances data efficiency and volumetric consistency. Our experiments show our approach outperforms state-of-the-art baselines in both realism and volumetric consistency, making it a promising solution for pseudo-healthy image inpainting. Code is available at https://github.com/dou0000/3dMRI-Consistent-Inpaint.

</details>


### [33] [Direct Dual-Energy CT Material Decomposition using Model-based Denoising Diffusion Model](https://arxiv.org/abs/2507.18012)
*Hang Xu,Alexandre Bousse,Alessandro Perelli*

Main category: eess.IV

TL;DR: 论文提出了一种名为DEcomp-MoD的深度学习方法，用于直接从DECT投影数据生成材料图像，解决了传统方法中图像域分解的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统DECT材料分解方法在图像域进行后处理，未考虑射束硬化效应，导致结果不理想。

Method: 结合谱DECT模型知识到深度学习训练损失中，并在材料图像域引入基于评分的去噪扩散先验。

Result: 在低剂量AAPM数据集上，DEcomp-MoD在定量和定性评估中表现优于现有方法。

Conclusion: DEcomp-MoD具有临床诊断潜力，优于现有无监督和监督深度学习方法。

Abstract: Dual-energy X-ray Computed Tomography (DECT) constitutes an advanced technology which enables automatic decomposition of materials in clinical images without manual segmentation using the dependency of the X-ray linear attenuation with energy. However, most methods perform material decomposition in the image domain as a post-processing step after reconstruction but this procedure does not account for the beam-hardening effect and it results in sub-optimal results. In this work, we propose a deep learning procedure called Dual-Energy Decomposition Model-based Diffusion (DEcomp-MoD) for quantitative material decomposition which directly converts the DECT projection data into material images. The algorithm is based on incorporating the knowledge of the spectral DECT model into the deep learning training loss and combining a score-based denoising diffusion learned prior in the material image domain. Importantly the inference optimization loss takes as inputs directly the sinogram and converts to material images through a model-based conditional diffusion model which guarantees consistency of the results. We evaluate the performance with both quantitative and qualitative estimation of the proposed DEcomp-MoD method on synthetic DECT sinograms from the low-dose AAPM dataset. Finally, we show that DEcomp-MoD outperform state-of-the-art unsupervised score-based model and supervised deep learning networks, with the potential to be deployed for clinical diagnosis.

</details>


### [34] [Parameter-Efficient Fine-Tuning of 3D DDPM for MRI Image Generation Using Tensor Networks](https://arxiv.org/abs/2507.18112)
*Binghua Li,Ziqing Chang,Tong Liang,Chao Li,Toshihisa Tanaka,Shigeki Aoki,Qibin Zhao,Zhe Sun*

Main category: eess.IV

TL;DR: 提出了一种名为TenVOO的参数高效微调方法，用于3D U-Net基础的DDPM在MRI图像生成中的应用，显著减少了参数需求并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决3D卷积操作参数高效表示的研究不足问题，特别是在MRI图像生成中。

Method: 利用张量网络建模，将3D卷积核表示为低维张量，减少参数需求。

Result: 在三个脑MRI数据集上，TenVOO仅需原始模型0.3%的可训练参数，即达到最佳性能。

Conclusion: TenVOO是一种高效的PEFT方法，适用于3D卷积模型的微调，具有实际应用潜力。

Abstract: We address the challenge of parameter-efficient fine-tuning (PEFT) for three-dimensional (3D) U-Net-based denoising diffusion probabilistic models (DDPMs) in magnetic resonance imaging (MRI) image generation. Despite its practical significance, research on parameter-efficient representations of 3D convolution operations remains limited. To bridge this gap, we propose Tensor Volumetric Operator (TenVOO), a novel PEFT method specifically designed for fine-tuning DDPMs with 3D convolutional backbones. Leveraging tensor network modeling, TenVOO represents 3D convolution kernels with lower-dimensional tensors, effectively capturing complex spatial dependencies during fine-tuning with few parameters. We evaluate TenVOO on three downstream brain MRI datasets-ADNI, PPMI, and BraTS2021-by fine-tuning a DDPM pretrained on 59,830 T1-weighted brain MRI scans from the UK Biobank. Our results demonstrate that TenVOO achieves state-of-the-art performance in multi-scale structural similarity index measure (MS-SSIM), outperforming existing approaches in capturing spatial dependencies while requiring only 0.3% of the trainable parameters of the original model. Our code is available at: https://github.com/xiaovhua/tenvoo

</details>


### [35] [U-Net Based Healthy 3D Brain Tissue Inpainting](https://arxiv.org/abs/2507.18126)
*Juexin Zhang,Ying Weng,Ke Chen*

Main category: eess.IV

TL;DR: 提出了一种基于U-Net的新方法，用于从掩码输入图像合成健康的3D脑组织，并在ASNR-MICCAI BraTS挑战赛中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决脑MRI扫描中缺失或损坏区域的重建问题，提升模型的泛化能力和鲁棒性。

Method: 采用U-Net架构，结合随机掩码健康图像的数据增强策略，训练于BraTS-Local-Inpainting数据集。

Result: 在验证集上取得SSIM 0.841、PSNR 23.257、MSE 0.007的优异表现，且标准差较低，模型可靠性和一致性高。

Conclusion: 该方法在挑战赛中夺冠，证明了其在脑组织合成任务中的高效性和稳定性。

Abstract: This paper introduces a novel approach to synthesize healthy 3D brain tissue from masked input images, specifically focusing on the task of 'ASNR-MICCAI BraTS Local Synthesis of Tissue via Inpainting'. Our proposed method employs a U-Net-based architecture, which is designed to effectively reconstruct the missing or corrupted regions of brain MRI scans. To enhance our model's generalization capabilities and robustness, we implement a comprehensive data augmentation strategy that involves randomly masking healthy images during training. Our model is trained on the BraTS-Local-Inpainting dataset and demonstrates the exceptional performance in recovering healthy brain tissue. The evaluation metrics employed, including Structural Similarity Index (SSIM), Peak Signal-to-Noise Ratio (PSNR), and Mean Squared Error (MSE), consistently yields impressive results. On the BraTS-Local-Inpainting validation set, our model achieved an SSIM score of 0.841, a PSNR score of 23.257, and an MSE score of 0.007. Notably, these evaluation metrics exhibit relatively low standard deviations, i.e., 0.103 for SSIM score, 4.213 for PSNR score and 0.007 for MSE score, which indicates that our model's reliability and consistency across various input scenarios. Our method also secured first place in the challenge.

</details>


### [36] [UniSegDiff: Boosting Unified Lesion Segmentation via a Staged Diffusion Model](https://arxiv.org/abs/2507.18362)
*Yilong Hu,Shijie Chang,Lihe Zhang,Feng Tian,Weibing Sun,Huchuan Lu*

Main category: eess.IV

TL;DR: UniSegDiff是一种新型扩散概率模型框架，通过分阶段训练和推理动态调整目标，解决了传统扩散模型在病灶分割中注意力分布不均的问题，并在多模态和多器官任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 扩散概率模型在生成任务中表现优异，但在病灶分割中存在注意力分布不均、训练时间长和效果不佳的问题。

Method: 提出UniSegDiff框架，采用分阶段训练和推理策略，动态调整预测目标，并通过预训练特征提取网络实现统一的病灶分割。

Result: 在六种不同器官和多种成像模态上的实验表明，UniSegDiff显著优于现有最优方法。

Conclusion: UniSegDiff通过改进训练和推理策略，在多模态和多器官病灶分割任务中取得了显著优势。

Abstract: The Diffusion Probabilistic Model (DPM) has demonstrated remarkable performance across a variety of generative tasks. The inherent randomness in diffusion models helps address issues such as blurring at the edges of medical images and labels, positioning Diffusion Probabilistic Models (DPMs) as a promising approach for lesion segmentation. However, we find that the current training and inference strategies of diffusion models result in an uneven distribution of attention across different timesteps, leading to longer training times and suboptimal solutions. To this end, we propose UniSegDiff, a novel diffusion model framework designed to address lesion segmentation in a unified manner across multiple modalities and organs. This framework introduces a staged training and inference approach, dynamically adjusting the prediction targets at different stages, forcing the model to maintain high attention across all timesteps, and achieves unified lesion segmentation through pre-training the feature extraction network for segmentation. We evaluate performance on six different organs across various imaging modalities. Comprehensive experimental results demonstrate that UniSegDiff significantly outperforms previous state-of-the-art (SOTA) approaches. The code is available at https://github.com/HUYILONG-Z/UniSegDiff.

</details>
