<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 24]
- [cs.LG](#cs.LG) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance](https://arxiv.org/abs/2509.15130)
*Chenxi Song,Yanming Yang,Tong Zhao,Ruibo Li,Chi Zhang*

Main category: cs.GR

TL;DR: WorldForge是一个无需训练的推理时框架，通过三个耦合模块实现精确的运动轨迹控制，解决了视频扩散模型在3D/4D任务中的可控性和几何一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型虽然具有丰富的空间智能先验知识，但受限于有限的可控性和几何不一致性，导致其强先验知识难以实际应用于3D/4D任务。现有方法通常需要重新训练或微调，这会降低预训练知识质量并带来高计算成本。

Method: 提出三个紧密耦合的模块：1）步内递归细化机制，在每个去噪步骤内重复优化网络预测以实现精确轨迹注入；2）流门控潜在融合，利用光流相似性在潜在空间中将运动与外观解耦，并选择性地将轨迹引导注入运动相关通道；3）双路径自校正引导，比较有引导和无引导的去噪路径以自适应校正轨迹漂移。

Result: 在多个基准测试上的广泛实验验证了该方法在真实性、轨迹一致性和视觉保真度方面的优越性，能够在不训练的情况下实现精确的运动控制和逼真的内容生成。

Conclusion: 这项工作为可控视频合成引入了一种新颖的即插即用范式，为利用生成先验进行空间智能提供了新的视角。

Abstract: Recent video diffusion models demonstrate strong potential in spatial intelligence tasks due to their rich latent world priors. However, this potential is hindered by their limited controllability and geometric inconsistency, creating a gap between their strong priors and their practical use in 3D/4D tasks. As a result, current approaches often rely on retraining or fine-tuning, which risks degrading pretrained knowledge and incurs high computational costs. To address this, we propose WorldForge, a training-free, inference-time framework composed of three tightly coupled modules. Intra-Step Recursive Refinement introduces a recursive refinement mechanism during inference, which repeatedly optimizes network predictions within each denoising step to enable precise trajectory injection. Flow-Gated Latent Fusion leverages optical flow similarity to decouple motion from appearance in the latent space and selectively inject trajectory guidance into motion-related channels. Dual-Path Self-Corrective Guidance compares guided and unguided denoising paths to adaptively correct trajectory drift caused by noisy or misaligned structural signals. Together, these components inject fine-grained, trajectory-aligned guidance without training, achieving both accurate motion control and photorealistic content generation. Extensive experiments across diverse benchmarks validate our method's superiority in realism, trajectory consistency, and visual fidelity. This work introduces a novel plug-and-play paradigm for controllable video synthesis, offering a new perspective on leveraging generative priors for spatial intelligence.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [AToken: A Unified Tokenizer for Vision](https://arxiv.org/abs/2509.14476)
*Jiasen Lu,Liangchen Song,Mingze Xu,Byeongjoo Ahn,Yanjun Wang,Chen Chen,Afshin Dehghan,Yinfei Yang*

Main category: cs.CV

TL;DR: AToken是首个统一的视觉分词器，能够在图像、视频和3D资产上同时实现高保真重建和语义理解，通过4D潜在空间统一多种视觉模态和处理任务。


<details>
  <summary>Details</summary>
Motivation: 现有分词器通常专注于单一模态的重建或理解任务，缺乏统一的框架来处理多种视觉输入类型。AToken旨在解决这一局限性，为下一代多模态AI系统提供统一的视觉分词基础。

Method: 采用纯Transformer架构，引入4D旋转位置编码处理任意分辨率和时长的视觉输入。使用无对抗训练目标，结合感知损失和Gram矩阵损失确保稳定训练。通过渐进式训练课程从单图像扩展到视频和3D，支持连续和离散潜在token。

Result: 在图像上达到0.21 rFID和82.2% ImageNet准确率；视频上达到3.01 rFVD和32.6% MSRVTT检索率；3D上达到28.19 PSNR和90.9%分类准确率。在下游任务中支持视觉生成和理解任务，在所有基准测试中表现优异。

Conclusion: AToken展示了统一视觉分词在构建下一代多模态AI系统中的潜力，为同时处理重建和理解任务提供了有效的解决方案。

Abstract: We present AToken, the first unified visual tokenizer that achieves both high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets. Unlike existing tokenizers that specialize in either reconstruction or understanding for single modalities, AToken encodes these diverse visual inputs into a shared 4D latent space, unifying both tasks and modalities in a single framework. Specifically, we introduce a pure transformer architecture with 4D rotary position embeddings to process visual inputs of arbitrary resolutions and temporal durations. To ensure stable training, we introduce an adversarial-free training objective that combines perceptual and Gram matrix losses, achieving state-of-the-art reconstruction quality. By employing a progressive training curriculum, AToken gradually expands from single images, videos, and 3D, and supports both continuous and discrete latent tokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01 rFVD with 32.6% MSRVTT retrieval for videos, and 28.19 PSNR with 90.9% classification accuracy for 3D. In downstream applications, AToken enables both visual generation tasks (e.g., image generation with continuous and discrete tokens, text-to-video generation, image-to-3D synthesis) and understanding tasks (e.g., multimodal LLMs), achieving competitive performance across all benchmarks. These results shed light on the next-generation multimodal AI systems built upon unified visual tokenization.

</details>


### [3] [Edge-Aware Normalized Attention for Efficient and Detail-Preserving Single Image Super-Resolution](https://arxiv.org/abs/2509.14550)
*Penghao Rao,Tieyong Zeng*

Main category: cs.CV

TL;DR: 提出了一种边缘引导的注意力机制，通过自适应调制图选择性增强结构显著区域，结合轻量级残差设计和复合损失函数，在保持模型复杂度的情况下提升了超分辨率的结构清晰度和感知质量。


<details>
  <summary>Details</summary>
Motivation: 单图像超分辨率(SISR)是一个高度不适定问题，现有边缘感知方法往往在复杂骨干网络上附加边缘先验或注意力分支，但临时融合经常引入冗余、优化不稳定或结构增益有限的问题。

Method: 开发边缘引导注意力机制，从联合编码的边缘特征和中间特征激活中推导自适应调制图，用于归一化和重加权响应；集成到轻量级残差设计中，使用结合像素级、感知和对抗项的复合目标函数进行训练。

Result: 在标准SISR基准测试中，相比SRGAN、ESRGAN和现有边缘注意力基线，在可比模型复杂度下实现了结构清晰度和感知质量的一致提升。

Conclusion: 该公式提供了参数高效的边缘先验注入路径、通过定制多术语损失稳定对抗细化，以及在不依赖更深或过度参数化架构的情况下增强边缘保真度，证明了原则性边缘条件调制在推进感知超分辨率方面的有效性。

Abstract: Single-image super-resolution (SISR) remains highly ill-posed because recovering structurally faithful high-frequency content from a single low-resolution observation is ambiguous. Existing edge-aware methods often attach edge priors or attention branches onto increasingly complex backbones, yet ad hoc fusion frequently introduces redundancy, unstable optimization, or limited structural gains. We address this gap with an edge-guided attention mechanism that derives an adaptive modulation map from jointly encoded edge features and intermediate feature activations, then applies it to normalize and reweight responses, selectively amplifying structurally salient regions while suppressing spurious textures. In parallel, we integrate this mechanism into a lightweight residual design trained under a composite objective combining pixel-wise, perceptual, and adversarial terms to balance fidelity, perceptual realism, and training stability. Extensive experiments on standard SISR benchmarks demonstrate consistent improvements in structural sharpness and perceptual quality over SRGAN, ESRGAN, and prior edge-attention baselines at comparable model complexity. The proposed formulation provides (i) a parameter-efficient path to inject edge priors, (ii) stabilized adversarial refinement through a tailored multiterm loss, and (iii) enhanced edge fidelity without resorting to deeper or heavily overparameterized architectures. These results highlight the effectiveness of principled edge-conditioned modulation for advancing perceptual super-resolution.

</details>


### [4] [Adaptive and Iterative Point Cloud Denoising with Score-Based Diffusion Model](https://arxiv.org/abs/2509.14560)
*Zhaonan Wang,Manyi Li,ShiQing Xin,Changhe Tu*

Main category: cs.CV

TL;DR: 基于评分模型的适应性迭代点云去噪方法，通过估计噪声强度制定适应迭代计划，并设计了具有特征融合和梯度融合的网络结构


<details>
  <summary>Details</summary>
Motivation: 解决现有点云去噪方法在迭代次数和噪声模式适应性上的不足，无法高效处理不同水平或模式的噪声

Method: 使用评分模型框架，首先估计噪声强度并制定适应性迭代计划，然后通过训练好的网络进行迭代更新。设计了具有特征融合和梯度融合的网络结构以支持迭代去噪

Result: 在合成和实际扫描数据集上都取得了超过现有最佳方法的结果，能够获得清洁平滑的去噪点云，同时保留形状边界和细节

Conclusion: 该方法通过适应性迭代计划和特征融合网络设计，有效解决了不同噪声模式的去噪问题，在质量和数量上都表现优异

Abstract: Point cloud denoising task aims to recover the clean point cloud from the scanned data coupled with different levels or patterns of noise. The recent state-of-the-art methods often train deep neural networks to update the point locations towards the clean point cloud, and empirically repeat the denoising process several times in order to obtain the denoised results. It is not clear how to efficiently arrange the iterative denoising processes to deal with different levels or patterns of noise. In this paper, we propose an adaptive and iterative point cloud denoising method based on the score-based diffusion model. For a given noisy point cloud, we first estimate the noise variation and determine an adaptive denoising schedule with appropriate step sizes, then invoke the trained network iteratively to update point clouds following the adaptive schedule. To facilitate this adaptive and iterative denoising process, we design the network architecture and a two-stage sampling strategy for the network training to enable feature fusion and gradient fusion for iterative denoising. Compared to the state-of-the-art point cloud denoising methods, our approach obtains clean and smooth denoised point clouds, while preserving the shape boundary and details better. Our results not only outperform the other methods both qualitatively and quantitatively, but also are preferable on the synthetic dataset with different patterns of noises, as well as the real-scanned dataset.

</details>


### [5] [DiffVL: Diffusion-Based Visual Localization on 2D Maps via BEV-Conditioned GPS Denoising](https://arxiv.org/abs/2509.14565)
*Li Gao,Hongyang Sun,Liu Liu,Yunhao Li,Yang Cai*

Main category: cs.CV

TL;DR: DiffVL是一个基于扩散模型的视觉定位框架，将视觉定位重新定义为GPS去噪任务，利用SD地图和视觉BEV特征来恢复噪声GPS轨迹中的真实位姿分布，实现了亚米级精度而无需HD地图。


<details>
  <summary>Details</summary>
Motivation: 现有视觉定位方法面临HD地图成本高、SD地图方法忽略噪声GPS信号的问题。GPS在城市场景中容易受到多路径误差影响，但仍然是普遍可用的信号源。

Method: 提出DiffVL框架，使用扩散模型将噪声GPS轨迹与视觉BEV特征和SD地图联合建模，通过迭代扩散细化来恢复真实位姿分布，反向学习GPS噪声扰动。

Result: 在多个数据集上的实验表明，该方法相比BEV匹配基线方法达到了最先进的精度水平，实现了亚米级定位精度。

Conclusion: 扩散模型能够通过将噪声GPS作为生成先验来实现可扩展的定位，这代表了从传统基于匹配方法向生成式方法的范式转变。

Abstract: Accurate visual localization is crucial for autonomous driving, yet existing methods face a fundamental dilemma: While high-definition (HD) maps provide high-precision localization references, their costly construction and maintenance hinder scalability, which drives research toward standard-definition (SD) maps like OpenStreetMap. Current SD-map-based approaches primarily focus on Bird's-Eye View (BEV) matching between images and maps, overlooking a ubiquitous signal-noisy GPS. Although GPS is readily available, it suffers from multipath errors in urban environments. We propose DiffVL, the first framework to reformulate visual localization as a GPS denoising task using diffusion models. Our key insight is that noisy GPS trajectory, when conditioned on visual BEV features and SD maps, implicitly encode the true pose distribution, which can be recovered through iterative diffusion refinement. DiffVL, unlike prior BEV-matching methods (e.g., OrienterNet) or transformer-based registration approaches, learns to reverse GPS noise perturbations by jointly modeling GPS, SD map, and visual signals, achieving sub-meter accuracy without relying on HD maps. Experiments on multiple datasets demonstrate that our method achieves state-of-the-art accuracy compared to BEV-matching baselines. Crucially, our work proves that diffusion models can enable scalable localization by treating noisy GPS as a generative prior-making a paradigm shift from traditional matching-based methods.

</details>


### [6] [MultiEdit: Advancing Instruction-based Image Editing on Diverse and Challenging Tasks](https://arxiv.org/abs/2509.14638)
*Mingsong Li,Lin Liu,Hongjun Wang,Haoxing Chen,Xijun Gu,Shizhan Liu,Dong Gong,Junbo Zhao,Zhenzhong Lan,Jianguo Li*

Main category: cs.CV

TL;DR: MultiEdit是一个包含10.7万高质量图像编辑样本的综合数据集，涵盖6个挑战性编辑任务，通过创新的多模态大语言模型流水线生成视觉自适应编辑指令和高质量编辑图像。


<details>
  <summary>Details</summary>
Motivation: 解决当前基于指令的图像编辑方法在处理复杂编辑任务时的局限性，现有数据集编辑类型和样本数量有限，且传统数据集构建存在噪声图像-标题对，导致模型在复杂编辑场景中能力受限。

Method: 采用新颖的数据集构建流水线，利用两个多模态大语言模型分别生成视觉自适应编辑指令和制作高保真度编辑图像，涵盖18种非风格转换编辑类型和38种风格转换操作。

Result: 实验表明，使用MultiEdit-Train集微调基础开源模型显著提升了在复杂编辑任务上的性能，同时有效保持了在标准编辑基准上的能力。

Conclusion: MultiEdit为推进更多样化、更具挑战性的基于指令的图像编辑能力研究提供了宝贵资源，数据集已在HuggingFace上公开。

Abstract: Current instruction-based image editing (IBIE) methods struggle with challenging editing tasks, as both editing types and sample counts of existing datasets are limited. Moreover, traditional dataset construction often contains noisy image-caption pairs, which may introduce biases and limit model capabilities in complex editing scenarios. To address these limitations, we introduce MultiEdit, a comprehensive dataset featuring over 107K high-quality image editing samples. It encompasses 6 challenging editing tasks through a diverse collection of 18 non-style-transfer editing types and 38 style transfer operations, covering a spectrum from sophisticated style transfer to complex semantic operations like person reference editing and in-image text editing. We employ a novel dataset construction pipeline that utilizes two multi-modal large language models (MLLMs) to generate visual-adaptive editing instructions and produce high-fidelity edited images, respectively. Extensive experiments demonstrate that fine-tuning foundational open-source models with our MultiEdit-Train set substantially improves models' performance on sophisticated editing tasks in our proposed MultiEdit-Test benchmark, while effectively preserving their capabilities on the standard editing benchmark. We believe MultiEdit provides a valuable resource for advancing research into more diverse and challenging IBIE capabilities. Our dataset is available at https://huggingface.co/datasets/inclusionAI/MultiEdit.

</details>


### [7] [DACoN: DINO for Anime Paint Bucket Colorization with Any Number of Reference Images](https://arxiv.org/abs/2509.14685)
*Kazuma Nagata,Naoshi Kaneko*

Main category: cs.CV

TL;DR: DACoN是一个利用基础模型进行线条画自动上色的框架，通过融合基础模型的低分辨率语义特征和CNN的高分辨率空间特征，支持任意数量的参考图像，解决了遮挡、姿态变化和视角变化等挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习自动上色方法在处理遮挡、姿态变化和视角变化时存在困难，且通常只能支持1-2个参考图像，限制了上色效果。

Method: 提出DACoN框架，利用基础模型提取部分级语义特征（即使在线条画中），并将其与CNN提取的高分辨率空间特征融合，实现细粒度且鲁棒的特征提取，同时移除了对参考图像数量的限制。

Result: 定量和定性评估表明，使用多个参考图像能带来显著优势，实现了卓越的上色性能。

Conclusion: DACoN通过结合基础模型的语义理解能力和CNN的空间细节提取能力，提供了一个更强大和灵活的自动上色解决方案，代码和模型已开源。

Abstract: Automatic colorization of line drawings has been widely studied to reduce the labor cost of hand-drawn anime production. Deep learning approaches, including image/video generation and feature-based correspondence, have improved accuracy but struggle with occlusions, pose variations, and viewpoint changes. To address these challenges, we propose DACoN, a framework that leverages foundation models to capture part-level semantics, even in line drawings. Our method fuses low-resolution semantic features from foundation models with high-resolution spatial features from CNNs for fine-grained yet robust feature extraction. In contrast to previous methods that rely on the Multiplex Transformer and support only one or two reference images, DACoN removes this constraint, allowing any number of references. Quantitative and qualitative evaluations demonstrate the benefits of using multiple reference images, achieving superior colorization performance. Our code and model are available at https://github.com/kzmngt/DACoN.

</details>


### [8] [FMGS-Avatar: Mesh-Guided 2D Gaussian Splatting with Foundation Model Priors for 3D Monocular Avatar Reconstruction](https://arxiv.org/abs/2509.14739)
*Jinlong Fan,Bingyu Hu,Xingguang Li,Yuxiang Yang,Jing Zhang*

Main category: cs.CV

TL;DR: FMGS-Avatar是一种从单目视频重建高保真可动画人体化身的创新方法，通过网格引导的2D高斯泼溅和基础模型知识蒸馏，解决了几何信息不足和表示限制的问题。


<details>
  <summary>Details</summary>
Motivation: 从单目视频重建高保真可动画人体化身面临几何信息不足的挑战，现有3D高斯泼溅方法由于自由形式的3D高斯基元而难以保持表面细节。

Method: 提出网格引导的2D高斯泼溅，将2D高斯基元直接附加到模板网格面上，并利用基础模型（如Sapiens）补充单目视频的有限视觉线索，通过选择性梯度隔离的协调训练策略解决多模态优化冲突。

Result: 实验评估显示相比现有方法具有优越的重建质量，在几何精度和外观保真度方面显著提升，同时提供丰富的语义信息，支持新颖视角和姿态下时空一致的渲染。

Conclusion: 通过增强表示和协调信息蒸馏的结合，该方法显著推进了3D单目人体化身重建，实现了高质量的几何细节保持和语义信息提取。

Abstract: Reconstructing high-fidelity animatable human avatars from monocular videos remains challenging due to insufficient geometric information in single-view observations. While recent 3D Gaussian Splatting methods have shown promise, they struggle with surface detail preservation due to the free-form nature of 3D Gaussian primitives. To address both the representation limitations and information scarcity, we propose a novel method, \textbf{FMGS-Avatar}, that integrates two key innovations. First, we introduce Mesh-Guided 2D Gaussian Splatting, where 2D Gaussian primitives are attached directly to template mesh faces with constrained position, rotation, and movement, enabling superior surface alignment and geometric detail preservation. Second, we leverage foundation models trained on large-scale datasets, such as Sapiens, to complement the limited visual cues from monocular videos. However, when distilling multi-modal prior knowledge from foundation models, conflicting optimization objectives can emerge as different modalities exhibit distinct parameter sensitivities. We address this through a coordinated training strategy with selective gradient isolation, enabling each loss component to optimize its relevant parameters without interference. Through this combination of enhanced representation and coordinated information distillation, our approach significantly advances 3D monocular human avatar reconstruction. Experimental evaluation demonstrates superior reconstruction quality compared to existing methods, with notable gains in geometric accuracy and appearance fidelity while providing rich semantic information. Additionally, the distilled prior knowledge within a shared canonical space naturally enables spatially and temporally consistent rendering under novel views and poses.

</details>


### [9] [Dataset Distillation for Super-Resolution without Class Labels and Pre-trained Models](https://arxiv.org/abs/2509.14777)
*Sunwoo Cho,Yejin Jung,Nam Ik Cho,Jae Woong Soh*

Main category: cs.CV

TL;DR: 一种无需类标签或预训练SR模型的新型数据萌蓬方法，通过CLIP特征分类和模型微调来生成超分辨率训练数据，在仅使用0.68%原始数据的情况下仍能保持高性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统GAN反向数据萌蓬方法对预训练SR模型和类别标签的依赖性，提高方法的通用性和适用性。

Method: 首先提取高梯度补丁并使用CLIP特征进行图像分类，然后对选择的补丁微调模型以学习其分布并合成萌蓬训练图像。

Result: 在仅使用0.68%原始数据训练Transformer SR模型时，性能下降仅0.3dB，模型微调耗时4小时，SR模型训练耗时1小时，远少于使用全量数据的11小时训练时间。

Conclusion: 该方法实现了独特的数据萌蓬效果，在显著减少训练数据和计算成本的同时保持了独制的性能水平。

Abstract: Training deep neural networks has become increasingly demanding, requiring large datasets and significant computational resources, especially as model complexity advances. Data distillation methods, which aim to improve data efficiency, have emerged as promising solutions to this challenge. In the field of single image super-resolution (SISR), the reliance on large training datasets highlights the importance of these techniques. Recently, a generative adversarial network (GAN) inversion-based data distillation framework for SR was proposed, showing potential for better data utilization. However, the current method depends heavily on pre-trained SR networks and class-specific information, limiting its generalizability and applicability. To address these issues, we introduce a new data distillation approach for image SR that does not need class labels or pre-trained SR models. In particular, we first extract high-gradient patches and categorize images based on CLIP features, then fine-tune a diffusion model on the selected patches to learn their distribution and synthesize distilled training images. Experimental results show that our method achieves state-of-the-art performance while using significantly less training data and requiring less computational time. Specifically, when we train a baseline Transformer model for SR with only 0.68\% of the original dataset, the performance drop is just 0.3 dB. In this case, diffusion model fine-tuning takes 4 hours, and SR model training completes within 1 hour, much shorter than the 11-hour training time with the full dataset.

</details>


### [10] [Radiology Report Conditional 3D CT Generation with Multi Encoder Latent diffusion Model](https://arxiv.org/abs/2509.14780)
*Sina Amirrajab,Zohaib Salahuddin,Sheng Kuang,Henry C. Woodruff,Philippe Lambin*

Main category: cs.CV

TL;DR: Report2CT是一个基于放射报告条件化的3D潜在扩散模型，能够直接从完整的放射报告中生成高质量的3D胸部CT图像，在MICCAI 2025 VLM3D挑战赛中取得第一名。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成模型在3D CT生成方面应用有限，且通常使用简化的提示词，忽略了放射报告中丰富的语义细节，导致文本-图像对齐度和临床保真度降低。

Method: 使用三个预训练的医学文本编码器（BiomedVLP CXR BERT、MedEmbed和ClinicalBERT）捕获临床上下文，结合放射报告和体素间距信息，在CT RATE数据集的20000个CT体积上训练3D潜在扩散模型。

Result: 生成的CT体积具有解剖一致性和优异的视觉质量，多编码器条件化提高了CLIP分数，无分类器引导进一步增强了对齐度，在所有评估指标上达到最先进性能。

Conclusion: 通过利用完整的放射报告和多编码器文本条件化，Report2CT推进了3D CT合成技术，能够生成临床保真度高、质量优异的合成数据。

Abstract: Text to image latent diffusion models have recently advanced medical image synthesis, but applications to 3D CT generation remain limited. Existing approaches rely on simplified prompts, neglecting the rich semantic detail in full radiology reports, which reduces text image alignment and clinical fidelity. We propose Report2CT, a radiology report conditional latent diffusion framework for synthesizing 3D chest CT volumes directly from free text radiology reports, incorporating both findings and impression sections using multiple text encoder. Report2CT integrates three pretrained medical text encoders (BiomedVLP CXR BERT, MedEmbed, and ClinicalBERT) to capture nuanced clinical context. Radiology reports and voxel spacing information condition a 3D latent diffusion model trained on 20000 CT volumes from the CT RATE dataset. Model performance was evaluated using Frechet Inception Distance (FID) for real synthetic distributional similarity and CLIP based metrics for semantic alignment, with additional qualitative and quantitative comparisons against GenerateCT model. Report2CT generated anatomically consistent CT volumes with excellent visual quality and text image alignment. Multi encoder conditioning improved CLIP scores, indicating stronger preservation of fine grained clinical details in the free text radiology reports. Classifier free guidance further enhanced alignment with only a minor trade off in FID. We ranked first in the VLM3D Challenge at MICCAI 2025 on Text Conditional CT Generation and achieved state of the art performance across all evaluation metrics. By leveraging complete radiology reports and multi encoder text conditioning, Report2CT advances 3D CT synthesis, producing clinically faithful and high quality synthetic data.

</details>


### [11] [Not All Degradations Are Equal: A Targeted Feature Denoising Framework for Generalizable Image Super-Resolution](https://arxiv.org/abs/2509.14841)
*Hongjun Wang,Jiyuan Chen,Zhengwei Yin,Xuan Song,Yinqiang Zheng*

Main category: cs.CV

TL;DR: 这篇论文提出了一种针对性的特征去噪框架，专门处理超分辨中模型过拟合噪声的问题，无需改动现有模型结构即可提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设模型会过拟合所有降级类型，但经详细研究发现模型主要过拟合噪声，因为噪声具有与其他降级不同的特征模式。

Method: 提出包含噪声检测和去噪模块的针对性特征去噪框架，可以无缝集成到现有超分辨模型中而无需改动网络结构。

Result: 在5个传统测试集和数据集上表现超过之前的正则化方法，包括合成和实际场景。

Conclusion: 该框架通过针对性地处理噪声过拟合问题，为通用性图像超分辨提供了有效解决方案，显示了更好的性能。

Abstract: Generalizable Image Super-Resolution aims to enhance model generalization capabilities under unknown degradations. To achieve this goal, the models are expected to focus only on image content-related features instead of overfitting degradations. Recently, numerous approaches such as Dropout and Feature Alignment have been proposed to suppress models' natural tendency to overfit degradations and yield promising results. Nevertheless, these works have assumed that models overfit to all degradation types (e.g., blur, noise, JPEG), while through careful investigations in this paper, we discover that models predominantly overfit to noise, largely attributable to its distinct degradation pattern compared to other degradation types. In this paper, we propose a targeted feature denoising framework, comprising noise detection and denoising modules. Our approach presents a general solution that can be seamlessly integrated with existing super-resolution models without requiring architectural modifications. Our framework demonstrates superior performance compared to previous regularization-based methods across five traditional benchmarks and datasets, encompassing both synthetic and real-world scenarios.

</details>


### [12] [Controllable Localized Face Anonymization Via Diffusion Inpainting](https://arxiv.org/abs/2509.14866)
*Ali Salar,Qing Liu,Guoying Zhao*

Main category: cs.CV

TL;DR: 基于潜在扩散模型的统一匿名化框架，通过自适应属性引导模块控制匿名化过程，支持局部匿名化，无需额外训练即可超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着肖像图像在计算机视觉中的广泛应用，需要保护个人身份隐私，同时确保匿名化后的图像仍可用于下游计算机视觉任务。

Method: 利用潜在扩散模型的修复能力，设计自适应属性引导模块，在反向去噪过程中应用梯度校正，使生成图像的面部属性与合成目标图像对齐，支持局部匿名化。

Result: 在CelebA-HQ和FFHQ数据集上的大量实验表明，该方法优于最先进的方法，且无需额外模型训练。

Conclusion: 提出的统一框架能够有效生成逼真的匿名化图像，在保护隐私的同时保持图像实用性，代码已开源。

Abstract: The growing use of portrait images in computer vision highlights the need to protect personal identities. At the same time, anonymized images must remain useful for downstream computer vision tasks. In this work, we propose a unified framework that leverages the inpainting ability of latent diffusion models to generate realistic anonymized images. Unlike prior approaches, we have complete control over the anonymization process by designing an adaptive attribute-guidance module that applies gradient correction during the reverse denoising process, aligning the facial attributes of the generated image with those of the synthesized target image. Our framework also supports localized anonymization, allowing users to specify which facial regions are left unchanged. Extensive experiments conducted on the public CelebA-HQ and FFHQ datasets show that our method outperforms state-of-the-art approaches while requiring no additional model training. The source code is available on our page.

</details>


### [13] [NeRF-based Visualization of 3D Cues Supporting Data-Driven Spacecraft Pose Estimation](https://arxiv.org/abs/2509.14890)
*Antoine Legrand,Renaud Detry,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: 本文提出一种可视化方法，通过训练NeRF基于梯度的图像生成器，来呈现空间设备位置估计网络依赖的3D视觉线索，以提高其决策过程的可解释性。


<details>
  <summary>Details</summary>
Motivation: 实际定位任务需要6D相对位置估计，但目前的数据驱动方法因决策过程不可解释而限制了实际应用。需要一种方法来可视化空间设备位置估计网络所依赖的3D视觉特征。

Method: 训练基于NeRF的图像生成器，通过位置估计网络的梯度反向传播来强化生成器渲染出空间设备位置估计网络主要利用的3D特征。

Result: 实验结果证明该方法能够恢复关键的3D视觉线索，并为位置估计网络的监督方式与隐式表征之间的关系提供了更深入的见解。

Conclusion: 该研究提供了一种有效的可视化方法，能够揭示空间设备位置估计网络的决策基础，促进了数据驱动方法在实际定位任务中的应用。

Abstract: On-orbit operations require the estimation of the relative 6D pose, i.e., position and orientation, between a chaser spacecraft and its target. While data-driven spacecraft pose estimation methods have been developed, their adoption in real missions is hampered by the lack of understanding of their decision process. This paper presents a method to visualize the 3D visual cues on which a given pose estimator relies. For this purpose, we train a NeRF-based image generator using the gradients back-propagated through the pose estimation network. This enforces the generator to render the main 3D features exploited by the spacecraft pose estimation network. Experiments demonstrate that our method recovers the relevant 3D cues. Furthermore, they offer additional insights on the relationship between the pose estimation network supervision and its implicit representation of the target spacecraft.

</details>


### [14] [SPATIALGEN: Layout-guided 3D Indoor Scene Generation](https://arxiv.org/abs/2509.14981)
*Chuan Fang,Heng Li,Yixun Liang,Jia Zheng,Yongsen Mao,Yuan Liu,Rui Tang,Zihan Zhou,Ping Tan*

Main category: cs.CV

TL;DR: 提出了SpatialGen多视图多模态扩散模型，使用大规模合成数据集生成高质量、语义一致的3D室内场景


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在视觉质量、多样性、语义一致性和用户控制方面的平衡问题，填补缺乏大规模高质量数据集的空白

Method: 基于12,328个结构化标注场景的大规模合成数据集，开发多视图多模态扩散模型，从3D布局和参考图像生成外观、几何和语义信息

Result: 在实验中始终生成优于先前方法的结果，能够从任意视角合成保持跨模态空间一致性的场景

Conclusion: 通过开源数据和模型推动室内场景理解和生成领域的发展，为设计、虚拟现实和机器人应用提供高效解决方案

Abstract: Creating high-fidelity 3D models of indoor environments is essential for applications in design, virtual reality, and robotics. However, manual 3D modeling remains time-consuming and labor-intensive. While recent advances in generative AI have enabled automated scene synthesis, existing methods often face challenges in balancing visual quality, diversity, semantic consistency, and user control. A major bottleneck is the lack of a large-scale, high-quality dataset tailored to this task. To address this gap, we introduce a comprehensive synthetic dataset, featuring 12,328 structured annotated scenes with 57,440 rooms, and 4.7M photorealistic 2D renderings. Leveraging this dataset, we present SpatialGen, a novel multi-view multi-modal diffusion model that generates realistic and semantically consistent 3D indoor scenes. Given a 3D layout and a reference image (derived from a text prompt), our model synthesizes appearance (color image), geometry (scene coordinate map), and semantic (semantic segmentation map) from arbitrary viewpoints, while preserving spatial consistency across modalities. SpatialGen consistently generates superior results to previous methods in our experiments. We are open-sourcing our data and models to empower the community and advance the field of indoor scene understanding and generation.

</details>


### [15] [Sea-ing Through Scattered Rays: Revisiting the Image Formation Model for Realistic Underwater Image Generation](https://arxiv.org/abs/2509.15011)
*Vasiliki Ismiroglou,Malte Pedersen,Stefan H. Bengtson,Andreas Aakerberg,Thomas B. Moeslund*

Main category: cs.CV

TL;DR: 本文提出了一种改进的水下合成数据生成方法，包含前向散射项和非均匀介质，并收集了BUCKET数据集验证效果。


<details>
  <summary>Details</summary>
Motivation: 现有水下图像生成模型主要关注变色问题，忽略了高浊度环境中距离相关的能见度损失复杂性。

Method: 提出包含前向散射项和非均匀介质的合成数据生成流程，并收集受控浊度条件下的真实数据集BUCKET。

Result: 在增加浊度条件下显示出定性改进，调查参与者选择率达到82.5%。

Conclusion: 改进的合成数据生成方法能更好地模拟高浊度水下环境，为水下视觉研究提供更真实的数据支持。

Abstract: In recent years, the underwater image formation model has found extensive use in the generation of synthetic underwater data. Although many approaches focus on scenes primarily affected by discoloration, they often overlook the model's ability to capture the complex, distance-dependent visibility loss present in highly turbid environments. In this work, we propose an improved synthetic data generation pipeline that includes the commonly omitted forward scattering term, while also considering a nonuniform medium. Additionally, we collected the BUCKET dataset under controlled turbidity conditions to acquire real turbid footage with the corresponding reference images. Our results demonstrate qualitative improvements over the reference model, particularly under increasing turbidity, with a selection rate of 82. 5\% by survey participants. Data and code can be accessed on the project page: vap.aau.dk/sea-ing-through-scattered-rays.

</details>


### [16] [AutoEdit: Automatic Hyperparameter Tuning for Image Editing](https://arxiv.org/abs/2509.15031)
*Chau Pham,Quan Dao,Mahesh Bhosale,Yunjie Tian,Dimitris Metaxas,David Doermann*

Main category: cs.CV

TL;DR: 通过强化学习框架动态调整散布模型图像编辑的超参数，避免暴力搜索并提高效率


<details>
  <summary>Details</summary>
Motivation: 现有散布模型图像编辑方法需要暴力调整多个相互依赖的超参数，计算成本高，应用遍历困难

Method: 使用强化学习框架，建立马尔可夫决策过程，动态调整去噪步骤中的超参数，通过近端策略优化实现时间效率

Result: 实验表明方法显著减少了搜索时间和计算开销，超越现有暴力搜索方法

Conclusion: 该强化学习框架推进了散布模型图像编辑在实际中的部署应用

Abstract: Recent advances in diffusion models have revolutionized text-guided image editing, yet existing editing methods face critical challenges in hyperparameter identification. To get the reasonable editing performance, these methods often require the user to brute-force tune multiple interdependent hyperparameters, such as inversion timesteps and attention modification, \textit{etc.} This process incurs high computational costs due to the huge hyperparameter search space. We consider searching optimal editing's hyperparameters as a sequential decision-making task within the diffusion denoising process. Specifically, we propose a reinforcement learning framework, which establishes a Markov Decision Process that dynamically adjusts hyperparameters across denoising steps, integrating editing objectives into a reward function. The method achieves time efficiency through proximal policy optimization while maintaining optimal hyperparameter configurations. Experiments demonstrate significant reduction in search time and computational overhead compared to existing brute-force approaches, advancing the practical deployment of a diffusion-based image editing framework in the real world.

</details>


### [17] [RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes](https://arxiv.org/abs/2509.15123)
*Fang Li,Hao Zhang,Narendra Ahuja*

Main category: cs.CV

TL;DR: 提出了一种仅需单RGB视频即可在动态场景中进行更准确、高效相机参数优化的新方法，包含补丁跟踪滤波器、异常值感知联合优化和两阶段优化策略三个关键组件。


<details>
  <summary>Details</summary>
Motivation: COLMAP在动态场景中需要地面真实运动掩码且运行时间长，现有方法需要额外监督信息如GT焦距、3D点云等，而这些在普通RGB视频中通常不可得。

Method: 1) 补丁跟踪滤波器建立稀疏铰链关系；2) 异常值感知联合优化自适应降权移动异常值；3) 两阶段优化策略平衡Softplus限制和凸最小值。

Result: 在4个真实数据集和1个合成数据集上的实验表明，该方法仅用单RGB视频就能更高效、准确地估计相机参数。

Conclusion: 该方法在仅使用单RGB视频的情况下，实现了动态场景中相机参数优化的效率和准确性提升，无需依赖运动先验或其他额外监督信息。

Abstract: Although COLMAP has long remained the predominant method for camera parameter optimization in static scenes, it is constrained by its lengthy runtime and reliance on ground truth (GT) motion masks for application to dynamic scenes. Many efforts attempted to improve it by incorporating more priors as supervision such as GT focal length, motion masks, 3D point clouds, camera poses, and metric depth, which, however, are typically unavailable in casually captured RGB videos. In this paper, we propose a novel method for more accurate and efficient camera parameter optimization in dynamic scenes solely supervised by a single RGB video. Our method consists of three key components: (1) Patch-wise Tracking Filters, to establish robust and maximally sparse hinge-like relations across the RGB video. (2) Outlier-aware Joint Optimization, for efficient camera parameter optimization by adaptive down-weighting of moving outliers, without reliance on motion priors. (3) A Two-stage Optimization Strategy, to enhance stability and optimization speed by a trade-off between the Softplus limits and convex minima in losses. We visually and numerically evaluate our camera estimates. To further validate accuracy, we feed the camera estimates into a 4D reconstruction method and assess the resulting 3D scenes, and rendered 2D RGB and depth maps. We perform experiments on 4 real-world datasets (NeRF-DS, DAVIS, iPhone, and TUM-dynamics) and 1 synthetic dataset (MPI-Sintel), demonstrating that our method estimates camera parameters more efficiently and accurately with a single RGB video as the only supervision.

</details>


### [18] [AIP: Subverting Retrieval-Augmented Generation via Adversarial Instructional Prompt](https://arxiv.org/abs/2509.15159)
*Saket S. Chaturvedi,Gaurav Bagwe,Lan Zhang,Xiaoyong Yuan*

Main category: cs.CV

TL;DR: 通过攻击指令提示来操控RAG系统的检索行为，实现高敌攻成功率且保持正常功能的隐藕攻击


<details>
  <summary>Details</summary>
Motivation: 现有RAG攻击主要依赖操控用户查询，而指令提示作为广泛重用、公开共享且少被审计的组件，成为更现实的攻击面

Method: 使用多样化查询生成策略模拟真实语言变化，基于遗传算法进行聚合优化，平衡攻击成功率、清洁任务效用和隐藕性

Result: AIP攻击达到最95.23%的攻击成功率，同时保持了正常的功能性能

Conclusion: 发现了RAG系统中被忽视的关键漏洞，强调重新评估共享指令提示的安全性

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by retrieving relevant documents from external sources to improve factual accuracy and verifiability. However, this reliance introduces new attack surfaces within the retrieval pipeline, beyond the LLM itself. While prior RAG attacks have exposed such vulnerabilities, they largely rely on manipulating user queries, which is often infeasible in practice due to fixed or protected user inputs. This narrow focus overlooks a more realistic and stealthy vector: instructional prompts, which are widely reused, publicly shared, and rarely audited. Their implicit trust makes them a compelling target for adversaries to manipulate RAG behavior covertly.   We introduce a novel attack for Adversarial Instructional Prompt (AIP) that exploits adversarial instructional prompts to manipulate RAG outputs by subtly altering retrieval behavior. By shifting the attack surface to the instructional prompts, AIP reveals how trusted yet seemingly benign interface components can be weaponized to degrade system integrity. The attack is crafted to achieve three goals: (1) naturalness, to evade user detection; (2) utility, to encourage use of prompts; and (3) robustness, to remain effective across diverse query variations. We propose a diverse query generation strategy that simulates realistic linguistic variation in user queries, enabling the discovery of prompts that generalize across paraphrases and rephrasings. Building on this, a genetic algorithm-based joint optimization is developed to evolve adversarial prompts by balancing attack success, clean-task utility, and stealthiness. Experimental results show that AIP achieves up to 95.23% ASR while preserving benign functionality. These findings uncover a critical and previously overlooked vulnerability in RAG systems, emphasizing the need to reassess the shared instructional prompts.

</details>


### [19] [A Race Bias Free Face Aging Model for Reliable Kinship Verification](https://arxiv.org/abs/2509.15177)
*Ali Nazari,Bardiya Kariminia,Mohsen Ebrahimi Moghaddam*

Main category: cs.CV

TL;DR: 这篇论文提出了一种称为RA-GAN的无种族偏见面部老化模型，通过RACEpSp和特征混合器模块，生成无种族偏见的同龄化图像，从而提高亲籍验证的准确性。


<details>
  <summary>Details</summary>
Motivation: 亲籍验证中存在年龄差异问题，而现有面部老化模型存在种族偏见，影响图像相似度认定，需要解决这一问题。

Method: 设计RA-GAN模型，包含RACEpSp模块和特征混合器，生成无种族偏见的面部老化图像，并将其应用于KinFaceW-I和KinFaceW-II数据集的亲子图像同龄化处理。

Result: RA-GAN在所有年龄段比SAM-GAN均年13.14%，在60+年龄段比CUSP-GAN高9.1%的种族准确性，并更好保留主体身份。在KinFaceW-I数据集上，父子、父女、母子、母女关系的验证准确性分别提高5.22、5.12、1.63和0.41；在KinFaceW-II上，父女、父子、母子关系的准确性分别提高2.9、0.39和1.6。

Conclusion: RA-GAN能够有效减少面部老化模型的种族偏见，生成更准确的同龄化图像，显著提升亲籍验证的性能，尤其在不同亲籍关系中都取得了明显改善。

Abstract: The age gap in kinship verification addresses the time difference between the photos of the parent and the child. Moreover, their same-age photos are often unavailable, and face aging models are racially biased, which impacts the likeness of photos. Therefore, we propose a face aging GAN model, RA-GAN, consisting of two new modules, RACEpSp and a feature mixer, to produce racially unbiased images. The unbiased synthesized photos are used in kinship verification to investigate the results of verifying same-age parent-child images. The experiments demonstrate that our RA-GAN outperforms SAM-GAN on an average of 13.14\% across all age groups, and CUSP-GAN in the 60+ age group by 9.1\% in terms of racial accuracy. Moreover, RA-GAN can preserve subjects' identities better than SAM-GAN and CUSP-GAN across all age groups. Additionally, we demonstrate that transforming parent and child images from the KinFaceW-I and KinFaceW-II datasets to the same age can enhance the verification accuracy across all age groups. The accuracy increases with our RA-GAN for the kinship relationships of father-son and father-daughter, mother-son, and mother-daughter, which are 5.22, 5.12, 1.63, and 0.41, respectively, on KinFaceW-I. Additionally, the accuracy for the relationships of father-daughter, father-son, and mother-son is 2.9, 0.39, and 1.6 on KinFaceW-II, respectively. The code is available at~\href{https://github.com/bardiya2254kariminia/An-Age-Transformation-whitout-racial-bias-for-Kinship-verification}{Github}

</details>


### [20] [Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding](https://arxiv.org/abs/2509.15178)
*Zaiquan Yang,Yuhao Liu,Gerhard Hancke,Rynson W. H. Lau*

Main category: cs.CV

TL;DR: 本文探索了使用多模态大语言模型(MLLMs)的零梭击方案来解决时空视频基准(STVG)问题，通过分解式时空高亮策略和时间增强组装策略来提升MLLMs的推理能力，在三个STVG标准数据集上超越了当前最好方法。


<details>
  <summary>Details</summary>
Motivation: 发现MLLMs在STVG任务中存在两个关键问题：(1)MLLMs往往动态分配特殊标记(基准标记)来基准文本查询；(2)MLLMs因无法充分整合文本查询中的线索(如属性、动作)而导致基准效果次优。需要一种方法来充分发挥MLLMs的推理能力。

Method: 提出了基于MLLMs的零梭击STVG框架，包括：
1. 分解式时空高亮(DSTH)策略：将原始查询解耦为属性和动作子查询，使用新的对数引导重关注(LRA)模块学习潜在变量作为空间和时间提示，按子查询正则化标记预测
2. 时间增强组装(TAS)策略：使用原始视频帧和时间增强帧作为输入来组装预测，提高时间一致性

Result: 在多个MLLMs上评估了该方法，在三个常见的STVG标准数据集上超越了当前最好的方法

Conclusion: 通过DSTH和TAS策略，成功地充分发挥了MLLMs在STVG任务中的推理能力，提供了一种有效的零梭击解决方案，显示了MLLMs在视觉语言理解任务中的强大潜力

Abstract: Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal tube of a video, as specified by the input text query. In this paper, we utilize multimodal large language models (MLLMs) to explore a zero-shot solution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to dynamically assign special tokens, referred to as \textit{grounding tokens}, for grounding the text query; and (2) MLLMs often suffer from suboptimal grounding due to the inability to fully integrate the cues in the text query (\textit{e.g.}, attributes, actions) for inference. Based on these insights, we propose a MLLM-based zero-shot framework for STVG, which includes novel decomposed spatio-temporal highlighting (DSTH) and temporal-augmented assembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH strategy first decouples the original query into attribute and action sub-queries for inquiring the existence of the target both spatially and temporally. It then uses a novel logit-guided re-attention (LRA) module to learn latent variables as spatial and temporal prompts, by regularizing token predictions for each sub-query. These prompts highlight attribute and action cues, respectively, directing the model's attention to reliable spatial and temporal related visual regions. In addition, as the spatial grounding by the attribute sub-query should be temporally consistent, we introduce the TAS strategy to assemble the predictions using the original video frames and the temporal-augmented frames as inputs to help improve temporal consistency. We evaluate our method on various MLLMs, and show that it outperforms SOTA methods on three common STVG benchmarks.   The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG.

</details>


### [21] [Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation](https://arxiv.org/abs/2509.15185)
*Xiaoyu Yue,Zidong Wang,Yuqing Wang,Wenlong Zhang,Xihui Liu,Wanli Ouyang,Lei Bai,Luping Zhou*

Main category: cs.CV

TL;DR: 本文系统研究了自回归模型在视觉领域的应用机制，发现了阻碍高级视觉语义学习的三个关键问题，并提出了自引导训练框架ST-AR来有效解决这些问题，显著提升了图像理解和生成质量。


<details>
  <summary>Details</summary>
Motivation: 近期研究表明高质量视觉表征在图像生成中的重要性，同时揭示了生成模型在图像理解方面的局限性。作为原本为自然语言设计的生成范式，自回归模型在视觉领域面临类似挑战。

Method: 提出了自引导训练框架ST-AR，通过在训练过程中引入自监督目标来解决自回归模型在视觉领域面临的三个关键问题：局部条件依赖、步间语义不一致性和空间不变性缺失。

Result: ST-AR显著提升了自回归模型的图像理解能力并改善了生成质量，在不改变采样策略的情况下，为LlamaGen-L带来约42%的FID提升，为LlamaGen-XL带来49%的FID提升。

Conclusion: 自引导训练框架ST-AR有效解决了自回归模型在视觉领域的核心挑战，证明了在不依赖预训练表征模型的情况下，通过适当的训练策略可以显著提升视觉自回归模型的性能。

Abstract: Recent studies have demonstrated the importance of high-quality visual representations in image generation and have highlighted the limitations of generative models in image understanding. As a generative paradigm originally designed for natural language, autoregressive models face similar challenges. In this work, we present the first systematic investigation into the mechanisms of applying the next-token prediction paradigm to the visual domain. We identify three key properties that hinder the learning of high-level visual semantics: local and conditional dependence, inter-step semantic inconsistency, and spatial invariance deficiency. We show that these issues can be effectively addressed by introducing self-supervised objectives during training, leading to a novel training framework, Self-guided Training for AutoRegressive models (ST-AR). Without relying on pre-trained representation models, ST-AR significantly enhances the image understanding ability of autoregressive models and leads to improved generation quality. Specifically, ST-AR brings approximately 42% FID improvement for LlamaGen-L and 49% FID improvement for LlamaGen-XL, while maintaining the same sampling strategy.

</details>


### [22] [RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation](https://arxiv.org/abs/2509.15212)
*Yuming Jiang,Siteng Huang,Shengke Xue,Yaxi Zhao,Jun Cen,Sicong Leng,Kehan Li,Jiayan Guo,Kexiang Wang,Mingxiu Chen,Fan Wang,Deli Zhao,Xin Li*

Main category: cs.CV

TL;DR: RynnVLA-001是一个基于大规模视频生成预训练的视觉-语言-动作模型，采用两阶段预训练方法：第一阶段通过自我中心视频生成预训练预测未来帧，第二阶段通过人类中心轨迹感知建模联合预测关键点轨迹，并使用ActionVAE压缩动作序列，在下游机器人任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决视觉-语言-动作模型中动作预测的复杂性，并建立视觉帧预测与动作预测之间的有效桥梁，需要开发更有效的预训练策略来提升模型初始化效果。

Method: 提出两阶段预训练方法：1）自我中心视频生成预训练：在1200万自我中心操作视频上训练图像到视频模型，基于初始帧和语言指令预测未来帧；2）人类中心轨迹感知建模：联合预测未来关键点轨迹；同时提出ActionVAE变分自编码器压缩动作序列为紧凑潜在嵌入。

Result: 在相同下游机器人数据集上微调后，RynnVLA-001超越了最先进的基线模型，证明了所提预训练策略为VLA模型提供了更有效的初始化。

Conclusion: 该研究提出的两阶段预训练方法和ActionVAE动作表示增强技术，有效提升了视觉-语言-动作模型的性能，为VLA模型提供了更好的初始化方案。

Abstract: This paper presents RynnVLA-001, a vision-language-action(VLA) model built upon large-scale video generative pretraining from human demonstrations. We propose a novel two-stage pretraining methodology. The first stage, Ego-Centric Video Generative Pretraining, trains an Image-to-Video model on 12M ego-centric manipulation videos to predict future frames conditioned on an initial frame and a language instruction. The second stage, Human-Centric Trajectory-Aware Modeling, extends this by jointly predicting future keypoint trajectories, thereby effectively bridging visual frame prediction with action prediction. Furthermore, to enhance action representation, we propose ActionVAE, a variational autoencoder that compresses sequences of actions into compact latent embeddings, reducing the complexity of the VLA output space. When finetuned on the same downstream robotics datasets, RynnVLA-001 achieves superior performance over state-of-the-art baselines, demonstrating that the proposed pretraining strategy provides a more effective initialization for VLA models.

</details>


### [23] [Out-of-Sight Trajectories: Tracking, Fusion, and Prediction](https://arxiv.org/abs/2509.15219)
*Haichao Zhang,Yi Xu,Yun Fu*

Main category: cs.CV

TL;DR: 提出了OST任务，通过视觉-定位映射和去噪模块，在无监督方式下预测视线外物体的无噪声轨迹，在Vi-Fi和JRDB数据集上达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖完整无噪声观测数据，忽略了视线外物体和传感器噪声的挑战，这些限制在现实场景中带来安全风险并阻碍可靠预测

Method: 增强的视觉-定位去噪模块利用相机标定建立视觉-定位映射，以无监督方式有效去噪传感器数据，扩展应用到行人和车辆

Result: 在Vi-Fi和JRDB数据集上实现了轨迹去噪和预测的最先进性能，显著超越先前基线，并与卡尔曼滤波等传统方法进行比较

Conclusion: 这是首个整合视觉-定位投影来去噪视线外智能体噪声传感器轨迹的工作，为未来进展铺平了道路

Abstract: Trajectory prediction is a critical task in computer vision and autonomous systems, playing a key role in autonomous driving, robotics, surveillance, and virtual reality. Existing methods often rely on complete and noise-free observational data, overlooking the challenges associated with out-of-sight objects and the inherent noise in sensor data caused by limited camera coverage, obstructions, and the absence of ground truth for denoised trajectories. These limitations pose safety risks and hinder reliable prediction in real-world scenarios. In this extended work, we present advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the noise-free visual trajectories of out-of-sight objects using noisy sensor data. Building on our previous research, we broaden the scope of Out-of-Sight Trajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending its applicability to autonomous driving, robotics, surveillance, and virtual reality. Our enhanced Vision-Positioning Denoising Module leverages camera calibration to establish a vision-positioning mapping, addressing the lack of visual references, while effectively denoising noisy sensor data in an unsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB datasets, our approach achieves state-of-the-art performance in both trajectory denoising and prediction, significantly surpassing previous baselines. Additionally, we introduce comparisons with traditional denoising methods, such as Kalman filtering, and adapt recent trajectory prediction models to our task, providing a comprehensive benchmark. This work represents the first initiative to integrate vision-positioning projection for denoising noisy sensor trajectories of out-of-sight agents, paving the way for future advances. The code and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST

</details>


### [24] [Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model](https://arxiv.org/abs/2509.15220)
*Fangjinhua Wang,Qingshan Xu,Yew-Soon Ong,Marc Pollefeys*

Main category: cs.CV

TL;DR: 提出基于扩散模型的多视图立体视觉框架，将深度图细化建模为条件扩散过程，实现了高效且高性能的3D重建


<details>
  <summary>Details</summary>
Motivation: 传统多视图立体视觉方法计算效率低，而扩散模型在生成任务中表现出色，希望将扩散模型的迭代去噪过程应用于深度图细化

Method: 设计条件编码器指导扩散过程，结合轻量级2D U-Net和卷积GRU的扩散网络，提出基于置信度的自适应采样策略，开发了DiffMVS和CasDiffMVS两种方法

Result: DiffMVS在运行时间和GPU内存使用方面达到最先进效率，CasDiffMVS在DTU、Tanks & Temples和ETH3D数据集上达到最先进性能

Conclusion: 将扩散模型引入多视图立体视觉是有效的，提出的框架在效率和性能方面都取得了显著提升，为3D重建任务提供了新的解决方案

Abstract: To reconstruct the 3D geometry from calibrated images, learning-based multi-view stereo (MVS) methods typically perform multi-view depth estimation and then fuse depth maps into a mesh or point cloud. To improve the computational efficiency, many methods initialize a coarse depth map and then gradually refine it in higher resolutions. Recently, diffusion models achieve great success in generation tasks. Starting from a random noise, diffusion models gradually recover the sample with an iterative denoising process. In this paper, we propose a novel MVS framework, which introduces diffusion models in MVS. Specifically, we formulate depth refinement as a conditional diffusion process. Considering the discriminative characteristic of depth estimation, we design a condition encoder to guide the diffusion process. To improve efficiency, we propose a novel diffusion network combining lightweight 2D U-Net and convolutional GRU. Moreover, we propose a novel confidence-based sampling strategy to adaptively sample depth hypotheses based on the confidence estimated by diffusion model. Based on our novel MVS framework, we propose two novel MVS methods, DiffMVS and CasDiffMVS. DiffMVS achieves competitive performance with state-of-the-art efficiency in run-time and GPU memory. CasDiffMVS achieves state-of-the-art performance on DTU, Tanks & Temples and ETH3D. Code is available at: https://github.com/cvg/diffmvs.

</details>


### [25] [Lost in Translation? Vocabulary Alignment for Source-Free Domain Adaptation in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2509.15225)
*Silvio Mazzucco,Carl Persson,Mattia Segu,Pier Luigi Dovesi,Federico Tombari,Luc Van Gool,Matteo Poggi*

Main category: cs.CV

TL;DR: VocAlign是一个针对开放词汇语义分割的源自由领域自适应框架，采用师生范式结合词汇对齐策略，通过LoRA微调和Top-K类别选择机制，在CityScapes数据集上实现了6.11 mIoU的显著提升。


<details>
  <summary>Details</summary>
Motivation: 为了解决视觉语言模型在开放词汇语义分割中的源自由领域自适应问题，需要在不访问源数据的情况下提升模型在新领域的性能，同时保持计算效率。

Method: 采用师生范式，结合词汇对齐策略改进伪标签生成；使用LoRA进行高效微调以保持原始能力；提出Top-K类别选择机制降低内存需求。

Result: 在CityScapes数据集上实现了6.11 mIoU的显著改进，在零样本分割基准测试中表现出优越性能。

Conclusion: VocAlign为开放词汇设置中的源自由自适应设立了新标准，在保持计算效率的同时显著提升了分割性能。

Abstract: We introduce VocAlign, a novel source-free domain adaptation framework specifically designed for VLMs in open-vocabulary semantic segmentation. Our method adopts a student-teacher paradigm enhanced with a vocabulary alignment strategy, which improves pseudo-label generation by incorporating additional class concepts. To ensure efficiency, we use Low-Rank Adaptation (LoRA) to fine-tune the model, preserving its original capabilities while minimizing computational overhead. In addition, we propose a Top-K class selection mechanism for the student model, which significantly reduces memory requirements while further improving adaptation performance. Our approach achieves a notable 6.11 mIoU improvement on the CityScapes dataset and demonstrates superior performance on zero-shot segmentation benchmarks, setting a new standard for source-free adaptation in the open-vocabulary setting.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [26] [Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models](https://arxiv.org/abs/2509.15076)
*Mohammad Saleh Vahdatpour,Maryam Eyvazi,Yanqing Zhang*

Main category: cs.LG

TL;DR: 基于天空图像的AI空气质量预测与可视化系统，结合统计纹理分析和生成模型，提供可解释的空气污染可视化


<details>
  <summary>Details</summary>
Motivation: 传统空气质量监测系统空间覆盖有限且可访问性差，需要开发更直观、透明的空气质量可视化方法

Method: 结合统计纹理分析和监督学习进行污染分类，利用视觉语言模型引导的图像生成技术合成污染场景可视化

Result: 在城市场景数据集上验证了污染水平估计和语义一致视觉合成的有效性

Conclusion: 该系统为面向用户的智能应用提供基础，未来将集成绿色CNN架构和FPGA增量学习以实现边缘平台实时推理

Abstract: Air pollution remains a critical threat to public health and environmental sustainability, yet conventional monitoring systems are often constrained by limited spatial coverage and accessibility. This paper proposes an AI-driven agent that predicts ambient air pollution levels from sky images and synthesizes realistic visualizations of pollution scenarios using generative modeling. Our approach combines statistical texture analysis with supervised learning for pollution classification, and leverages vision-language model (VLM)-guided image generation to produce interpretable representations of air quality conditions. The generated visuals simulate varying degrees of pollution, offering a foundation for user-facing interfaces that improve transparency and support informed environmental decision-making. These outputs can be seamlessly integrated into intelligent applications aimed at enhancing situational awareness and encouraging behavioral responses based on real-time forecasts. We validate our method using a dataset of urban sky images and demonstrate its effectiveness in both pollution level estimation and semantically consistent visual synthesis. The system design further incorporates human-centered user experience principles to ensure accessibility, clarity, and public engagement in air quality forecasting. To support scalable and energy-efficient deployment, future iterations will incorporate a green CNN architecture enhanced with FPGA-based incremental learning, enabling real-time inference on edge platforms.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [27] [Learning Mechanistic Subtypes of Neurodegeneration with a Physics-Informed Variational Autoencoder Mixture Model](https://arxiv.org/abs/2509.15124)
*Sanduni Pinnawala,Annabelle Hartanto,Ivor J. A. Simpson,Peter A. Wijeratne*

Main category: eess.IV

TL;DR: 一种深度生成模型，通过结合偏微方程物理知识和变分自动编码器混合模型，从神经影像数据中推断神经退行性疾病的机制子型


<details>
  <summary>Details</summary>
Motivation: 现有的物理集成机器学习方法只考虑单一偏微方程，无法处理多机制导致的异质性神经退行性疾病

Method: 将反应-扩散偏微方程集成到变分自动编码器混合模型框架中，支持从神经影像数据推断解释性隐变量

Result: 在合成数据集上评估了方法，并在阿尔茨海默病PET数据中展示了发现机制子型的潜力

Conclusion: 该方法能够从稀疏高维神经影像数据中推断多重物理机制子型，提供了更好的解释性和应用价值

Abstract: Modelling the underlying mechanisms of neurodegenerative diseases demands methods that capture heterogeneous and spatially varying dynamics from sparse, high-dimensional neuroimaging data. Integrating partial differential equation (PDE) based physics knowledge with machine learning provides enhanced interpretability and utility over classic numerical methods. However, current physics-integrated machine learning methods are limited to considering a single PDE, severely limiting their application to diseases where multiple mechanisms are responsible for different groups (i.e., subtypes) and aggravating problems with model misspecification and degeneracy. Here, we present a deep generative model for learning mixtures of latent dynamic models governed by physics-based PDEs, going beyond traditional approaches that assume a single PDE structure. Our method integrates reaction-diffusion PDEs within a variational autoencoder (VAE) mixture model framework, supporting inference of subtypes of interpretable latent variables (e.g. diffusivity and reaction rates) from neuroimaging data. We evaluate our method on synthetic benchmarks and demonstrate its potential for uncovering mechanistic subtypes of Alzheimer's disease progression from positron emission tomography (PET) data.

</details>
