{"id": "2507.10689", "pdf": "https://arxiv.org/pdf/2507.10689", "abs": "https://arxiv.org/abs/2507.10689", "authors": ["Tongshun Zhang", "Pingping Liu", "Yubing Lu", "Mengen Cai", "Zijian Zhang", "Zhe Zhang", "Qiuzhan Zhou"], "title": "CWNet: Causal Wavelet Network for Low-Light Image Enhancement", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Traditional Low-Light Image Enhancement (LLIE) methods primarily focus on uniform brightness adjustment, often neglecting instance-level semantic information and the inherent characteristics of different features. To address these limitations, we propose CWNet (Causal Wavelet Network), a novel architecture that leverages wavelet transforms for causal reasoning. Specifically, our approach comprises two key components: 1) Inspired by the concept of intervention in causality, we adopt a causal reasoning perspective to reveal the underlying causal relationships in low-light enhancement. From a global perspective, we employ a metric learning strategy to ensure causal embeddings adhere to causal principles, separating them from non-causal confounding factors while focusing on the invariance of causal factors. At the local level, we introduce an instance-level CLIP semantic loss to precisely maintain causal factor consistency. 2) Based on our causal analysis, we present a wavelet transform-based backbone network that effectively optimizes the recovery of frequency information, ensuring precise enhancement tailored to the specific attributes of wavelet transforms. Extensive experiments demonstrate that CWNet significantly outperforms current state-of-the-art methods across multiple datasets, showcasing its robust performance across diverse scenes. Code is available at https://github.com/bywlzts/CWNet-Causal-Wavelet-Network.", "AI": {"tldr": "CWNet\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u53d8\u6362\u548c\u56e0\u679c\u63a8\u7406\u7684\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40\u548c\u5c40\u90e8\u7b56\u7565\u4f18\u5316\u8bed\u4e49\u548c\u9891\u7387\u4fe1\u606f\u6062\u590d\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u5ffd\u7565\u5b9e\u4f8b\u7ea7\u8bed\u4e49\u4fe1\u606f\u548c\u7279\u5f81\u7279\u6027\uff0cCWNet\u65e8\u5728\u901a\u8fc7\u56e0\u679c\u63a8\u7406\u548c\u5c0f\u6ce2\u53d8\u6362\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u56e0\u679c\u63a8\u7406\u89c6\u89d2\u548c\u5c0f\u6ce2\u53d8\u6362\uff0c\u5305\u62ec\u5168\u5c40\u5ea6\u91cf\u5b66\u4e60\u548c\u5c40\u90e8CLIP\u8bed\u4e49\u635f\u5931\uff0c\u4f18\u5316\u9891\u7387\u4fe1\u606f\u6062\u590d\u3002", "result": "CWNet\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CWNet\u901a\u8fc7\u56e0\u679c\u63a8\u7406\u548c\u5c0f\u6ce2\u53d8\u6362\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.10943", "pdf": "https://arxiv.org/pdf/2507.10943", "abs": "https://arxiv.org/abs/2507.10943", "authors": ["Yushun Fang", "Lu Liu", "Xiang Gao", "Qiang Hu", "Ning Cao", "Jianghe Cui", "Gang Chen", "Xiaoyun Zhang"], "title": "Robust ID-Specific Face Restoration via Alignment Learning", "categories": ["cs.CV"], "comment": "17 pages, 8 figures", "summary": "The latest developments in Face Restoration have yielded significant advancements in visual quality through the utilization of diverse diffusion priors. Nevertheless, the uncertainty of face identity introduced by identity-obscure inputs and stochastic generative processes remains unresolved. To address this challenge, we present Robust ID-Specific Face Restoration (RIDFR), a novel ID-specific face restoration framework based on diffusion models. Specifically, RIDFR leverages a pre-trained diffusion model in conjunction with two parallel conditioning modules. The Content Injection Module inputs the severely degraded image, while the Identity Injection Module integrates the specific identity from a given image. Subsequently, RIDFR incorporates Alignment Learning, which aligns the restoration results from multiple references with the same identity in order to suppress the interference of ID-irrelevant face semantics (e.g. pose, expression, make-up, hair style). Experiments demonstrate that our framework outperforms the state-of-the-art methods, reconstructing high-quality ID-specific results with high identity fidelity and demonstrating strong robustness.", "AI": {"tldr": "RIDFR\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b0\u578bID\u7279\u5b9a\u4eba\u8138\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u5185\u5bb9\u6ce8\u5165\u548c\u8eab\u4efd\u6ce8\u5165\u6a21\u5757\uff0c\u7ed3\u5408\u5bf9\u9f50\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4fee\u590d\u7ed3\u679c\u7684ID\u4fdd\u771f\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u4eba\u8138\u4fee\u590d\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u8eab\u4efd\u6a21\u7cca\u8f93\u5165\u548c\u968f\u673a\u751f\u6210\u8fc7\u7a0b\u5bfc\u81f4\u7684\u4e0d\u786e\u5b9a\u6027\u4ecd\u672a\u89e3\u51b3\u3002", "method": "RIDFR\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u5185\u5bb9\u6ce8\u5165\u6a21\u5757\u548c\u8eab\u4efd\u6ce8\u5165\u6a21\u5757\uff0c\u5e76\u901a\u8fc7\u5bf9\u9f50\u5b66\u4e60\u6291\u5236ID\u65e0\u5173\u8bed\u4e49\u7684\u5e72\u6270\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRIDFR\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u91cd\u5efa\u9ad8\u8d28\u91cf\u4e14ID\u4fdd\u771f\u5ea6\u9ad8\u7684\u7ed3\u679c\uff0c\u5e76\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "RIDFR\u4e3a\u89e3\u51b3\u8eab\u4efd\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u8138\u4fee\u590d\u7684ID\u4fdd\u771f\u5ea6\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.11025", "pdf": "https://arxiv.org/pdf/2507.11025", "abs": "https://arxiv.org/abs/2507.11025", "authors": ["Sung Ho Kang", "Hyun-Cheol Park"], "title": "Human-Guided Shade Artifact Suppression in CBCT-to-MDCT Translation via Schr\u00f6dinger Bridge with Conditional Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "We present a novel framework for CBCT-to-MDCT translation, grounded in the Schrodinger Bridge (SB) formulation, which integrates GAN-derived priors with human-guided conditional diffusion. Unlike conventional GANs or diffusion models, our approach explicitly enforces boundary consistency between CBCT inputs and pseudo targets, ensuring both anatomical fidelity and perceptual controllability. Binary human feedback is incorporated via classifier-free guidance (CFG), effectively steering the generative process toward clinically preferred outcomes. Through iterative refinement and tournament-based preference selection, the model internalizes human preferences without relying on a reward model. Subtraction image visualizations reveal that the proposed method selectively attenuates shade artifacts in key anatomical regions while preserving fine structural detail. Quantitative evaluations further demonstrate superior performance across RMSE, SSIM, LPIPS, and Dice metrics on clinical datasets -- outperforming prior GAN- and fine-tuning-based feedback methods -- while requiring only 10 sampling steps. These findings underscore the effectiveness and efficiency of our framework for real-time, preference-aligned medical image translation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSchrodinger Bridge\u6846\u67b6\u7684CBCT-to-MDCT\u8f6c\u6362\u65b9\u6cd5\uff0c\u7ed3\u5408GAN\u5148\u9a8c\u548c\u4eba\u7c7b\u5f15\u5bfc\u7684\u6761\u4ef6\u6269\u6563\uff0c\u901a\u8fc7\u8fb9\u754c\u4e00\u81f4\u6027\u548c\u4eba\u7c7b\u53cd\u9988\u4f18\u5316\u751f\u6210\u7ed3\u679c\u3002", "motivation": "\u4f20\u7edfGAN\u6216\u6269\u6563\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u8f6c\u6362\u4e2d\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u89e3\u5256\u5b66\u4fdd\u771f\u5ea6\u548c\u611f\u77e5\u53ef\u63a7\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u7b26\u5408\u4e34\u5e8a\u504f\u597d\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408Schrodinger Bridge\u6846\u67b6\uff0c\u5229\u7528GAN\u5148\u9a8c\u548c\u4eba\u7c7b\u53cd\u9988\uff08\u901a\u8fc7CFG\uff09\u4f18\u5316\u751f\u6210\u8fc7\u7a0b\uff0c\u901a\u8fc7\u8fed\u4ee3\u548c\u504f\u597d\u9009\u62e9\u5b66\u4e60\u4eba\u7c7b\u504f\u597d\u3002", "result": "\u5728\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728RMSE\u3001SSIM\u3001LPIPS\u548cDice\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4ec5\u970010\u6b21\u91c7\u6837\u6b65\u9aa4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u7b26\u5408\u4e34\u5e8a\u504f\u597d\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u533b\u5b66\u56fe\u50cf\u8f6c\u6362\u3002"}}
{"id": "2507.11035", "pdf": "https://arxiv.org/pdf/2507.11035", "abs": "https://arxiv.org/abs/2507.11035", "authors": ["Lirong Zheng", "Yanshan Li", "Rui Yu", "Kaihao Zhang"], "title": "Efficient Dual-domain Image Dehazing with Haze Prior Perception", "categories": ["cs.CV"], "comment": "12 pages", "summary": "Transformer-based models exhibit strong global modeling capabilities in single-image dehazing, but their high computational cost limits real-time applicability. Existing methods predominantly rely on spatial-domain features to capture long-range dependencies, which are computationally expensive and often inadequate under complex haze conditions. While some approaches introduce frequency-domain cues, the weak coupling between spatial and frequency branches limits the overall performance. To overcome these limitations, we propose the Dark Channel Guided Frequency-aware Dehazing Network (DGFDNet), a novel dual-domain framework that performs physically guided degradation alignment across spatial and frequency domains. At its core, the DGFDBlock comprises two key modules: 1) the Haze-Aware Frequency Modulator (HAFM), which generates a pixel-level haze confidence map from dark channel priors to adaptively enhance haze-relevant frequency components, thereby achieving global degradation-aware spectral modulation; 2) the Multi-level Gating Aggregation Module (MGAM), which fuses multi-scale features through diverse convolutional kernels and hybrid gating mechanisms to recover fine structural details. Additionally, a Prior Correction Guidance Branch (PCGB) incorporates a closed-loop feedback mechanism, enabling iterative refinement of the prior by intermediate dehazed features and significantly improving haze localization accuracy, especially in challenging outdoor scenes. Extensive experiments on four benchmark haze datasets demonstrate that DGFDNet achieves state-of-the-art performance with superior robustness and real-time efficiency. Code is available at: https://github.com/Dilizlr/DGFDNet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u57df\u53bb\u96fe\u7f51\u7edcDGFDNet\uff0c\u7ed3\u5408\u7a7a\u95f4\u57df\u548c\u9891\u57df\u7279\u5f81\uff0c\u901a\u8fc7\u7269\u7406\u5f15\u5bfc\u7684\u9000\u5316\u5bf9\u9f50\u63d0\u5347\u6027\u80fd\u3002", "motivation": "Transformer\u6a21\u578b\u5728\u5355\u56fe\u50cf\u53bb\u96fe\u4e2d\u8868\u73b0\u4f18\u5f02\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7a7a\u95f4\u57df\u7279\u5f81\u6216\u9891\u57df\u7279\u5f81\u8026\u5408\u4e0d\u8db3\u3002", "method": "DGFDNet\u5305\u542bHAFM\u6a21\u5757\uff08\u57fa\u4e8e\u6697\u901a\u9053\u5148\u9a8c\u751f\u6210\u96fe\u973e\u7f6e\u4fe1\u56fe\uff09\u548cMGAM\u6a21\u5757\uff08\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\uff09\uff0c\u5e76\u5f15\u5165PCGB\u5206\u652f\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u517c\u5177\u9c81\u68d2\u6027\u548c\u5b9e\u65f6\u6027\u3002", "conclusion": "DGFDNet\u901a\u8fc7\u53cc\u57df\u534f\u540c\u548c\u7269\u7406\u5f15\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53bb\u96fe\u6548\u679c\u548c\u6548\u7387\u3002"}}
{"id": "2507.11040", "pdf": "https://arxiv.org/pdf/2507.11040", "abs": "https://arxiv.org/abs/2507.11040", "authors": ["Nicolas Drapier", "Aladine Chetouani", "Aur\u00e9lien Chateigner"], "title": "Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery", "categories": ["cs.CV"], "comment": "11 pages, 9 figures", "summary": "We present GLOD, a transformer-first architecture for object detection in high-resolution satellite imagery. GLOD replaces CNN backbones with a Swin Transformer for end-to-end feature extraction, combined with novel UpConvMixer blocks for robust upsampling and Fusion Blocks for multi-scale feature integration. Our approach achieves 32.95\\% on xView, outperforming SOTA methods by 11.46\\%. Key innovations include asymmetric fusion with CBAM attention and a multi-path head design capturing objects across scales. The architecture is optimized for satellite imagery challenges, leveraging spatial priors while maintaining computational efficiency.", "AI": {"tldr": "GLOD\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff0c\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\uff0c\u901a\u8fc7Swin Transformer\u548c\u65b0\u578b\u6a21\u5757\u5b9e\u73b0\u9ad8\u6548\u7279\u5f81\u63d0\u53d6\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\u4e2d\u76ee\u6807\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u63d0\u5347\u6027\u80fd\u5e76\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u91c7\u7528Swin Transformer\u66ff\u4ee3CNN\u4e3b\u5e72\uff0c\u7ed3\u5408UpConvMixer\u5757\u548cFusion Blocks\u8fdb\u884c\u591a\u5c3a\u5ea6\u7279\u5f81\u6574\u5408\uff0c\u521b\u65b0\u6027\u5730\u4f7f\u7528CBAM\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u8def\u5f84\u5934\u8bbe\u8ba1\u3002", "result": "\u5728xView\u6570\u636e\u96c6\u4e0a\u8fbe\u523032.95%\u7684\u51c6\u786e\u7387\uff0c\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u9ad8\u51fa11.46%\u3002", "conclusion": "GLOD\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u536b\u661f\u56fe\u50cf\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2507.11061", "pdf": "https://arxiv.org/pdf/2507.11061", "abs": "https://arxiv.org/abs/2507.11061", "authors": ["Hayeon Kim", "Ji Ha Jang", "Se Young Chun"], "title": "Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in 3D neural representations and instance-level editing models have enabled the efficient creation of high-quality 3D content. However, achieving precise local 3D edits remains challenging, especially for Gaussian Splatting, due to inconsistent multi-view 2D part segmentations and inherently ambiguous nature of Score Distillation Sampling (SDS) loss. To address these limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that enables precise and drastic part-level modifications. First, we introduce a robust 3D mask generation module with our 3D-Geometry Aware Label Prediction (3D-GALP), which uses spherical harmonics (SH) coefficients to model view-dependent label variations and soft-label property, yielding accurate and consistent part segmentations across viewpoints. Second, we propose a regularized SDS loss that combines the standard SDS loss with additional regularizers. In particular, an L1 anchor loss is introduced via our Scheduled Latent Mixing and Part (SLaMP) editing method, which generates high-quality part-edited 2D images and confines modifications only to the target region while preserving contextual coherence. Additional regularizers, such as Gaussian prior removal, further improve flexibility by allowing changes beyond the existing context, and robust 3D masking prevents unintended edits. Experimental results demonstrate that our RoMaP achieves state-of-the-art local 3D editing on both reconstructed and generated Gaussian scenes and objects qualitatively and quantitatively, making it possible for more robust and flexible part-level 3D Gaussian editing.", "AI": {"tldr": "RoMaP\u662f\u4e00\u79cd\u65b0\u578b\u7684\u5c40\u90e83D\u9ad8\u65af\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc73D-GALP\u6a21\u5757\u548c\u6b63\u5219\u5316SDS\u635f\u5931\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u4e14\u5927\u5e45\u5ea6\u7684\u90e8\u5206\u7ea7\u522b3D\u7f16\u8f91\u3002", "motivation": "\u5c3d\u7ba13D\u795e\u7ecf\u8868\u793a\u548c\u5b9e\u4f8b\u7ea7\u7f16\u8f91\u6a21\u578b\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5b9e\u73b0\u7cbe\u786e\u7684\u5c40\u90e83D\u7f16\u8f91\u4ecd\u5177\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u65af\u6cfc\u6e85\u4e2d\u3002", "method": "RoMaP\u7ed3\u5408\u4e863D-GALP\u6a21\u5757\uff08\u5229\u7528\u7403\u8c10\u7cfb\u6570\u5efa\u6a21\u89c6\u56fe\u4f9d\u8d56\u6807\u7b7e\uff09\u548c\u6b63\u5219\u5316SDS\u635f\u5931\uff08\u5305\u62ecL1\u951a\u5b9a\u635f\u5931\u548c\u5176\u4ed6\u6b63\u5219\u5316\u5668\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRoMaP\u5728\u91cd\u5efa\u548c\u751f\u6210\u7684\u9ad8\u65af\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5c40\u90e83D\u7f16\u8f91\u6548\u679c\u3002", "conclusion": "RoMaP\u4e3a\u90e8\u5206\u7ea7\u522b3D\u9ad8\u65af\u7f16\u8f91\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u548c\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11152", "pdf": "https://arxiv.org/pdf/2507.11152", "abs": "https://arxiv.org/abs/2507.11152", "authors": ["Duoyou Chen", "Yunqing Chen", "Can Zhang", "Zhou Wang", "Cheng Chen", "Ruoxiu Xiao"], "title": "Latent Space Consistency for Sparse-View CT Reconstruction", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "ACMMM2025 Accepted", "summary": "Computed Tomography (CT) is a widely utilized imaging modality in clinical settings. Using densely acquired rotational X-ray arrays, CT can capture 3D spatial features. However, it is confronted with challenged such as significant time consumption and high radiation exposure. CT reconstruction methods based on sparse-view X-ray images have garnered substantial attention from researchers as they present a means to mitigate costs and risks. In recent years, diffusion models, particularly the Latent Diffusion Model (LDM), have demonstrated promising potential in the domain of 3D CT reconstruction. Nonetheless, due to the substantial differences between the 2D latent representation of X-ray modalities and the 3D latent representation of CT modalities, the vanilla LDM is incapable of achieving effective alignment within the latent space. To address this issue, we propose the Consistent Latent Space Diffusion Model (CLS-DM), which incorporates cross-modal feature contrastive learning to efficiently extract latent 3D information from 2D X-ray images and achieve latent space alignment between modalities. Experimental results indicate that CLS-DM outperforms classical and state-of-the-art generative models in terms of standard voxel-level metrics (PSNR, SSIM) on the LIDC-IDRI and CTSpine1K datasets. This methodology not only aids in enhancing the effectiveness and economic viability of sparse X-ray reconstructed CT but can also be generalized to other cross-modal transformation tasks, such as text-to-image synthesis. We have made our code publicly available at https://anonymous.4open.science/r/CLS-DM-50D6/ to facilitate further research and applications in other domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCLS-DM\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u6bd4\u5b66\u4e60\uff0c\u89e3\u51b32D X\u5c04\u7ebf\u56fe\u50cf\u4e0e3D CT\u56fe\u50cf\u6f5c\u5728\u7a7a\u95f4\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u5347\u7a00\u758fX\u5c04\u7ebf\u91cd\u5efaCT\u7684\u6548\u679c\u3002", "motivation": "\u7a00\u758fX\u5c04\u7ebf\u56fe\u50cf\u91cd\u5efaCT\u53ef\u51cf\u5c11\u65f6\u95f4\u548c\u8f90\u5c04\u98ce\u9669\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u6f5c\u5728\u7a7a\u95f4\u5bf9\u9f50\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faCLS-DM\u6a21\u578b\uff0c\u5229\u7528\u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4ece2D X\u5c04\u7ebf\u56fe\u50cf\u4e2d\u63d0\u53d63D\u4fe1\u606f\u5e76\u5b9e\u73b0\u6f5c\u5728\u7a7a\u95f4\u5bf9\u9f50\u3002", "result": "\u5728LIDC-IDRI\u548cCTSpine1K\u6570\u636e\u96c6\u4e0a\uff0cCLS-DM\u5728PSNR\u548cSSIM\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u751f\u6210\u6a21\u578b\u3002", "conclusion": "CLS-DM\u4e0d\u4ec5\u63d0\u5347\u4e86\u7a00\u758fX\u5c04\u7ebf\u91cd\u5efaCT\u7684\u6548\u679c\uff0c\u8fd8\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u8de8\u6a21\u6001\u4efb\u52a1\uff0c\u5982\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u3002"}}
{"id": "2507.11245", "pdf": "https://arxiv.org/pdf/2507.11245", "abs": "https://arxiv.org/abs/2507.11245", "authors": ["X. Feng", "H. Yu", "M. Wu", "S. Hu", "J. Chen", "C. Zhu", "J. Wu", "X. Chu", "K. Huang"], "title": "NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models", "categories": ["cs.CV"], "comment": "Project Page: https://amap-ml.github.io/NarrLV-Website/", "summary": "With the rapid development of foundation video generation technologies, long video generation models have exhibited promising research potential thanks to expanded content creation space. Recent studies reveal that the goal of long video generation tasks is not only to extend video duration but also to accurately express richer narrative content within longer videos. However, due to the lack of evaluation benchmarks specifically designed for long video generation models, the current assessment of these models primarily relies on benchmarks with simple narrative prompts (e.g., VBench). To the best of our knowledge, our proposed NarrLV is the first benchmark to comprehensively evaluate the Narrative expression capabilities of Long Video generation models. Inspired by film narrative theory, (i) we first introduce the basic narrative unit maintaining continuous visual presentation in videos as Temporal Narrative Atom (TNA), and use its count to quantitatively measure narrative richness. Guided by three key film narrative elements influencing TNA changes, we construct an automatic prompt generation pipeline capable of producing evaluation prompts with a flexibly expandable number of TNAs. (ii) Then, based on the three progressive levels of narrative content expression, we design an effective evaluation metric using the MLLM-based question generation and answering framework. (iii) Finally, we conduct extensive evaluations on existing long video generation models and the foundation generation models. Experimental results demonstrate that our metric aligns closely with human judgments. The derived evaluation outcomes reveal the detailed capability boundaries of current video generation models in narrative content expression.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30\u957f\u89c6\u9891\u751f\u6210\u6a21\u578b\u53d9\u4e8b\u8868\u8fbe\u80fd\u529b\u7684\u57fa\u51c6NarrLV\uff0c\u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u53d9\u4e8b\u539f\u5b50\uff08TNA\uff09\u548c\u57fa\u4e8eMLLM\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\u3002", "motivation": "\u73b0\u6709\u957f\u89c6\u9891\u751f\u6210\u6a21\u578b\u7f3a\u4e4f\u4e13\u95e8\u8bc4\u4f30\u53d9\u4e8b\u8868\u8fbe\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5bfc\u81f4\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u7b80\u5355\u53d9\u4e8b\u63d0\u793a\u7684\u57fa\u51c6\uff08\u5982VBench\uff09\u3002", "method": "1. \u5f15\u5165\u65f6\u95f4\u53d9\u4e8b\u539f\u5b50\uff08TNA\uff09\u4f5c\u4e3a\u57fa\u672c\u53d9\u4e8b\u5355\u5143\uff1b2. \u8bbe\u8ba1\u81ea\u52a8\u63d0\u793a\u751f\u6210\u7ba1\u9053\uff1b3. \u57fa\u4e8eMLLM\u6846\u67b6\u8bbe\u8ba1\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cNarrLV\u7684\u8bc4\u4f30\u6307\u6807\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u53d9\u4e8b\u8868\u8fbe\u4e0a\u7684\u80fd\u529b\u8fb9\u754c\u3002", "conclusion": "NarrLV\u662f\u9996\u4e2a\u5168\u9762\u8bc4\u4f30\u957f\u89c6\u9891\u751f\u6210\u6a21\u578b\u53d9\u4e8b\u8868\u8fbe\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2507.11252", "pdf": "https://arxiv.org/pdf/2507.11252", "abs": "https://arxiv.org/abs/2507.11252", "authors": ["Guanghao Wu", "Chen Xu", "Hai Song", "Chong Wang", "Qixing Zhang"], "title": "MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection", "categories": ["cs.CV", "eess.IV"], "comment": "18 pages, 11 figures", "summary": "Smoke is the first visible indicator of a wildfire.With the advancement of deep learning, image-based smoke detection has become a crucial method for detecting and preventing forest fires. However, the scarcity of smoke image data from forest fires is one of the significant factors hindering the detection of forest fire smoke. Image generation models offer a promising solution for synthesizing realistic smoke images. However, current inpainting models exhibit limitations in generating high-quality smoke representations, particularly manifesting as inconsistencies between synthesized smoke and background contexts. To solve these problems, we proposed a comprehensive framework for generating forest fire smoke images. Firstly, we employed the pre-trained segmentation model and the multimodal model to obtain smoke masks and image captions.Then, to address the insufficient utilization of masks and masked images by inpainting models, we introduced a network architecture guided by mask and masked image features. We also proposed a new loss function, the mask random difference loss, which enhances the consistency of the generated effects around the mask by randomly expanding and eroding the mask edges.Finally, to generate a smoke image dataset using random masks for subsequent detection tasks, we incorporated smoke characteristics and use a multimodal large language model as a filtering tool to select diverse and reasonable smoke images, thereby improving the quality of the synthetic dataset. Experiments showed that our generated smoke images are realistic and diverse, and effectively enhance the performance of forest fire smoke detection models. Code is available at https://github.com/wghr123/MFGDiffusion.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u68ee\u6797\u706b\u707e\u70df\u96fe\u56fe\u50cf\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u4fee\u590d\u6a21\u578b\u548c\u5f15\u5165\u65b0\u635f\u5931\u51fd\u6570\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u70df\u96fe\u56fe\u50cf\uff0c\u63d0\u5347\u70df\u96fe\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u68ee\u6797\u706b\u707e\u70df\u96fe\u56fe\u50cf\u6570\u636e\u7a00\u7f3a\uff0c\u73b0\u6709\u4fee\u590d\u6a21\u578b\u751f\u6210\u70df\u96fe\u4e0e\u80cc\u666f\u4e0d\u4e00\u81f4\uff0c\u9650\u5236\u4e86\u70df\u96fe\u68c0\u6d4b\u6548\u679c\u3002", "method": "\u7ed3\u5408\u9884\u8bad\u7ec3\u5206\u5272\u6a21\u578b\u548c\u591a\u6a21\u6001\u6a21\u578b\u83b7\u53d6\u70df\u96fe\u63a9\u7801\u548c\u56fe\u50cf\u63cf\u8ff0\uff0c\u8bbe\u8ba1\u63a9\u7801\u548c\u63a9\u7801\u56fe\u50cf\u7279\u5f81\u5f15\u5bfc\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u63d0\u51fa\u63a9\u7801\u968f\u673a\u5dee\u5f02\u635f\u5931\u51fd\u6570\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7b5b\u9009\u56fe\u50cf\u3002", "result": "\u751f\u6210\u7684\u70df\u96fe\u56fe\u50cf\u771f\u5b9e\u591a\u6837\uff0c\u663e\u8457\u63d0\u5347\u4e86\u70df\u96fe\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u70df\u96fe\u56fe\u50cf\u751f\u6210\u95ee\u9898\uff0c\u4e3a\u70df\u96fe\u68c0\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002"}}
{"id": "2507.11321", "pdf": "https://arxiv.org/pdf/2507.11321", "abs": "https://arxiv.org/abs/2507.11321", "authors": ["Haoxuan Qu", "Yujun Cai", "Hossein Rahmani", "Ajay Kumar", "Junsong Yuan", "Jun Liu"], "title": "A Mixed-Primitive-based Gaussian Splatting Method for Surface Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Recently, Gaussian Splatting (GS) has received a lot of attention in surface reconstruction. However, while 3D objects can be of complex and diverse shapes in the real world, existing GS-based methods only limitedly use a single type of splatting primitive (Gaussian ellipse or Gaussian ellipsoid) to represent object surfaces during their reconstruction. In this paper, we highlight that this can be insufficient for object surfaces to be represented in high quality. Thus, we propose a novel framework that, for the first time, enables Gaussian Splatting to incorporate multiple types of (geometrical) primitives during its surface reconstruction process. Specifically, in our framework, we first propose a compositional splatting strategy, enabling the splatting and rendering of different types of primitives in the Gaussian Splatting pipeline. In addition, we also design our framework with a mixed-primitive-based initialization strategy and a vertex pruning mechanism to further promote its surface representation learning process to be well executed leveraging different types of primitives. Extensive experiments show the efficacy of our framework and its accurate surface reconstruction performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u9996\u6b21\u5728\u8868\u9762\u91cd\u5efa\u8fc7\u7a0b\u4e2d\u5f15\u5165\u591a\u79cd\u51e0\u4f55\u57fa\u5143\uff0c\u63d0\u9ad8\u4e86\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u4ec5\u4f7f\u7528\u5355\u4e00\u7c7b\u578b\u7684\u57fa\u5143\uff08\u9ad8\u65af\u692d\u5706\u6216\u692d\u7403\uff09\u8868\u793a\u590d\u6742\u591a\u6837\u7684\u7269\u4f53\u8868\u9762\uff0c\u5bfc\u81f4\u91cd\u5efa\u8d28\u91cf\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u7ec4\u5408\u6cfc\u6e85\u7b56\u7565\u3001\u6df7\u5408\u57fa\u5143\u521d\u59cb\u5316\u7b56\u7565\u548c\u9876\u70b9\u4fee\u526a\u673a\u5236\uff0c\u652f\u6301\u591a\u79cd\u57fa\u5143\u7684\u9ad8\u65af\u6cfc\u6e85\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u6846\u67b6\u6709\u6548\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u8868\u9762\u91cd\u5efa\u3002", "conclusion": "\u591a\u57fa\u5143\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u8868\u9762\u91cd\u5efa\u8d28\u91cf\uff0c\u4e3a\u590d\u6742\u7269\u4f53\u8868\u793a\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.11441", "pdf": "https://arxiv.org/pdf/2507.11441", "abs": "https://arxiv.org/abs/2507.11441", "authors": ["Kaif Shaikh", "Antoni Kowalczuk", "Franziska Boenisch", "Adam Dziedzic"], "title": "Implementing Adaptations for Vision AutoRegressive Model", "categories": ["cs.CV", "cs.LG", "I.2.6; I.5.1; I.4.8; I.2.10"], "comment": "Accepted at DIG-BUGS: Data in Generative Models Workshop @ ICML 2025", "summary": "Vision AutoRegressive model (VAR) was recently introduced as an alternative to Diffusion Models (DMs) in image generation domain. In this work we focus on its adaptations, which aim to fine-tune pre-trained models to perform specific downstream tasks, like medical data generation. While for DMs there exist many techniques, adaptations for VAR remain underexplored. Similarly, differentially private (DP) adaptations-ones that aim to preserve privacy of the adaptation data-have been extensively studied for DMs, while VAR lacks such solutions. In our work, we implement and benchmark many strategies for VAR, and compare them to state-of-the-art DM adaptation strategies. We observe that VAR outperforms DMs for non-DP adaptations, however, the performance of DP suffers, which necessitates further research in private adaptations for VAR. Code is available at https://github.com/sprintml/finetuning_var_dp.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Vision AutoRegressive\u6a21\u578b\uff08VAR\uff09\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u7279\u522b\u662f\u4e0eDiffusion Models\uff08DMs\uff09\u7684\u6bd4\u8f83\uff0c\u5e76\u63a2\u8ba8\u4e86\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u9002\u5e94\u6027\u5728VAR\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "VAR\u4f5c\u4e3aDMs\u7684\u66ff\u4ee3\u65b9\u6848\u5728\u56fe\u50cf\u751f\u6210\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u9002\u5e94\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u5b9e\u73b0\u5e76\u6bd4\u8f83\u4e86VAR\u7684\u591a\u79cd\u9002\u5e94\u6027\u7b56\u7565\uff0c\u5305\u62ec\u5dee\u5206\u9690\u79c1\u9002\u5e94\u6027\uff0c\u5e76\u4e0eDMs\u7684\u73b0\u6709\u7b56\u7565\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "VAR\u5728\u975e\u5dee\u5206\u9690\u79c1\u9002\u5e94\u6027\u4efb\u52a1\u4e2d\u4f18\u4e8eDMs\uff0c\u4f46\u5728\u5dee\u5206\u9690\u79c1\u9002\u5e94\u6027\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "VAR\u5728\u9002\u5e94\u6027\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5dee\u5206\u9690\u79c1\u9002\u5e94\u6027\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.11474", "pdf": "https://arxiv.org/pdf/2507.11474", "abs": "https://arxiv.org/abs/2507.11474", "authors": ["Pan Du", "Mingqi Xu", "Xiaozhi Zhu", "Jian-xun Wang"], "title": "HUG-VAS: A Hierarchical NURBS-Based Generative Model for Aortic Geometry Synthesis and Controllable Editing", "categories": ["cs.CV"], "comment": "59 pages, 9 figures", "summary": "Accurate characterization of vascular geometry is essential for cardiovascular diagnosis and treatment planning. Traditional statistical shape modeling (SSM) methods rely on linear assumptions, limiting their expressivity and scalability to complex topologies such as multi-branch vascular structures. We introduce HUG-VAS, a Hierarchical NURBS Generative model for Vascular geometry Synthesis, which integrates NURBS surface parameterization with diffusion-based generative modeling to synthesize realistic, fine-grained aortic geometries. Trained with 21 patient-specific samples, HUG-VAS generates anatomically faithful aortas with supra-aortic branches, yielding biomarker distributions that closely match those of the original dataset. HUG-VAS adopts a hierarchical architecture comprising a denoising diffusion model that generates centerlines and a guided diffusion model that synthesizes radial profiles conditioned on those centerlines, thereby capturing two layers of anatomical variability. Critically, the framework supports zero-shot conditional generation from image-derived priors, enabling practical applications such as interactive semi-automatic segmentation, robust reconstruction under degraded imaging conditions, and implantable device optimization. To our knowledge, HUG-VAS is the first SSM framework to bridge image-derived priors with generative shape modeling via a unified integration of NURBS parameterization and hierarchical diffusion processes.", "AI": {"tldr": "HUG-VAS\u662f\u4e00\u79cd\u57fa\u4e8eNURBS\u548c\u6269\u6563\u6a21\u578b\u7684\u5c42\u6b21\u5316\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u5408\u6210\u771f\u5b9e\u3001\u7ec6\u7c92\u5ea6\u7684\u4e3b\u52a8\u8109\u51e0\u4f55\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7ebf\u6027SSM\u65b9\u6cd5\u5728\u590d\u6742\u62d3\u6251\u7ed3\u6784\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u7edf\u8ba1\u5f62\u72b6\u5efa\u6a21\uff08SSM\uff09\u65b9\u6cd5\u4f9d\u8d56\u7ebf\u6027\u5047\u8bbe\uff0c\u96be\u4ee5\u8868\u8fbe\u590d\u6742\u7684\u591a\u5206\u652f\u8840\u7ba1\u7ed3\u6784\u3002HUG-VAS\u65e8\u5728\u901a\u8fc7\u7ed3\u5408NURBS\u53c2\u6570\u5316\u548c\u6269\u6563\u6a21\u578b\uff0c\u751f\u6210\u66f4\u771f\u5b9e\u7684\u8840\u7ba1\u51e0\u4f55\u3002", "method": "HUG-VAS\u91c7\u7528\u5c42\u6b21\u5316\u67b6\u6784\uff0c\u5305\u62ec\u751f\u6210\u4e2d\u5fc3\u7ebf\u7684\u53bb\u566a\u6269\u6563\u6a21\u578b\u548c\u57fa\u4e8e\u4e2d\u5fc3\u7ebf\u751f\u6210\u5f84\u5411\u5256\u9762\u7684\u5f15\u5bfc\u6269\u6563\u6a21\u578b\uff0c\u652f\u6301\u96f6\u6837\u672c\u6761\u4ef6\u751f\u6210\u3002", "result": "\u6a21\u578b\u572821\u4e2a\u60a3\u8005\u6837\u672c\u4e0a\u8bad\u7ec3\uff0c\u751f\u6210\u7684\u4e3b\u52a8\u8109\u7ed3\u6784\u5177\u6709\u89e3\u5256\u5b66\u4fdd\u771f\u5ea6\uff0c\u751f\u7269\u6807\u5fd7\u7269\u5206\u5e03\u4e0e\u539f\u59cb\u6570\u636e\u96c6\u9ad8\u5ea6\u5339\u914d\u3002", "conclusion": "HUG-VAS\u9996\u6b21\u901a\u8fc7NURBS\u53c2\u6570\u5316\u548c\u5c42\u6b21\u5316\u6269\u6563\u8fc7\u7a0b\uff0c\u5c06\u56fe\u50cf\u5148\u9a8c\u4e0e\u751f\u6210\u5f62\u72b6\u5efa\u6a21\u7edf\u4e00\u8d77\u6765\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.11522", "pdf": "https://arxiv.org/pdf/2507.11522", "abs": "https://arxiv.org/abs/2507.11522", "authors": ["Tariq Mehmood", "Hamza Ahmad", "Muhammad Haroon Shakeel", "Murtaza Taj"], "title": "CATVis: Context-Aware Thought Visualization", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at MICCAI 2025. This is the submitted version prior to peer   review. The final Version of Record will appear in the MICCAI 2025   proceedings (Springer LNCS)", "summary": "EEG-based brain-computer interfaces (BCIs) have shown promise in various applications, such as motor imagery and cognitive state monitoring. However, decoding visual representations from EEG signals remains a significant challenge due to their complex and noisy nature. We thus propose a novel 5-stage framework for decoding visual representations from EEG signals: (1) an EEG encoder for concept classification, (2) cross-modal alignment of EEG and text embeddings in CLIP feature space, (3) caption refinement via re-ranking, (4) weighted interpolation of concept and caption embeddings for richer semantics, and (5) image generation using a pre-trained Stable Diffusion model. We enable context-aware EEG-to-image generation through cross-modal alignment and re-ranking. Experimental results demonstrate that our method generates high-quality images aligned with visual stimuli, outperforming SOTA approaches by 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and reducing Fr\\'echet Inception Distance by 36.61%, indicating superior semantic alignment and image quality.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd5\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7EEG\u4fe1\u53f7\u89e3\u7801\u89c6\u89c9\u8868\u5f81\uff0c\u7ed3\u5408\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u56fe\u50cf\u751f\u6210\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548c\u8bed\u4e49\u5bf9\u9f50\u3002", "motivation": "EEG\u4fe1\u53f7\u89e3\u7801\u89c6\u89c9\u8868\u5f81\u7684\u590d\u6742\u6027\u548c\u566a\u58f0\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u8bed\u4e49\u5bf9\u9f50\u7684\u56fe\u50cf\u3002", "method": "1) EEG\u7f16\u7801\u5668\u5206\u7c7b\u6982\u5ff5\uff1b2) EEG\u4e0e\u6587\u672c\u5d4c\u5165\u5728CLIP\u7279\u5f81\u7a7a\u95f4\u5bf9\u9f50\uff1b3) \u901a\u8fc7\u91cd\u6392\u4f18\u5316\u6807\u9898\uff1b4) \u52a0\u6743\u63d2\u503c\u6982\u5ff5\u4e0e\u6807\u9898\u5d4c\u5165\uff1b5) \u4f7f\u7528Stable Diffusion\u751f\u6210\u56fe\u50cf\u3002", "result": "\u65b9\u6cd5\u5728\u5206\u7c7b\u51c6\u786e\u7387\u3001\u751f\u6210\u51c6\u786e\u7387\u548c\u56fe\u50cf\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5206\u522b\u63d0\u534713.43%\u300115.21%\u548c\u964d\u4f4e36.61%\u7684FID\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5728EEG\u4fe1\u53f7\u89e3\u7801\u89c6\u89c9\u8868\u5f81\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u8111\u673a\u63a5\u53e3\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2507.11533", "pdf": "https://arxiv.org/pdf/2507.11533", "abs": "https://arxiv.org/abs/2507.11533", "authors": ["Mengyu Wang", "Henghui Ding", "Jianing Peng", "Yao Zhao", "Yunpeng Chen", "Yunchao Wei"], "title": "CharaConsist: Fine-Grained Consistent Character Generation", "categories": ["cs.CV"], "comment": "ICCV 2025 accepted paper, project page:   https://murray-wang.github.io/CharaConsist/", "summary": "In text-to-image generation, producing a series of consistent contents that preserve the same identity is highly valuable for real-world applications. Although a few works have explored training-free methods to enhance the consistency of generated subjects, we observe that they suffer from the following problems. First, they fail to maintain consistent background details, which limits their applicability. Furthermore, when the foreground character undergoes large motion variations, inconsistencies in identity and clothing details become evident. To address these problems, we propose CharaConsist, which employs point-tracking attention and adaptive token merge along with decoupled control of the foreground and background. CharaConsist enables fine-grained consistency for both foreground and background, supporting the generation of one character in continuous shots within a fixed scene or in discrete shots across different scenes. Moreover, CharaConsist is the first consistent generation method tailored for text-to-image DiT model. Its ability to maintain fine-grained consistency, combined with the larger capacity of latest base model, enables it to produce high-quality visual outputs, broadening its applicability to a wider range of real-world scenarios. The source code has been released at https://github.com/Murray-Wang/CharaConsist", "AI": {"tldr": "CharaConsist\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u70b9\u8ddf\u8e2a\u6ce8\u610f\u529b\u548c\u81ea\u9002\u5e94\u4ee4\u724c\u5408\u5e76\uff0c\u89e3\u51b3\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u524d\u540e\u666f\u4e00\u81f4\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u4e00\u81f4\u5185\u5bb9\u65f6\u65e0\u6cd5\u4fdd\u6301\u80cc\u666f\u7ec6\u8282\u548c\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u70b9\u8ddf\u8e2a\u6ce8\u610f\u529b\u548c\u81ea\u9002\u5e94\u4ee4\u724c\u5408\u5e76\uff0c\u89e3\u8026\u524d\u540e\u666f\u63a7\u5236\u3002", "result": "\u5b9e\u73b0\u4e86\u524d\u540e\u666f\u7684\u7ec6\u7c92\u5ea6\u4e00\u81f4\u6027\uff0c\u652f\u6301\u8fde\u7eed\u6216\u79bb\u6563\u573a\u666f\u4e2d\u7684\u89d2\u8272\u751f\u6210\u3002", "conclusion": "CharaConsist\u662f\u9996\u4e2a\u9488\u5bf9DiT\u6a21\u578b\u7684\u4e00\u81f4\u6027\u751f\u6210\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u5b9e\u9645\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2507.10623", "pdf": "https://arxiv.org/pdf/2507.10623", "abs": "https://arxiv.org/abs/2507.10623", "authors": ["Daniel Saragih", "Deyu Cao", "Tejas Balaji"], "title": "Flows and Diffusions on the Neural Manifold", "categories": ["cs.LG", "cs.CV"], "comment": "40 pages, 6 figures, 13 tables", "summary": "Diffusion and flow-based generative models have achieved remarkable success in domains such as image synthesis, video generation, and natural language modeling. In this work, we extend these advances to weight space learning by leveraging recent techniques to incorporate structural priors derived from optimization dynamics. Central to our approach is modeling the trajectory induced by gradient descent as a trajectory inference problem. We unify several trajectory inference techniques under the framework of gradient flow matching, providing a theoretical framework for treating optimization paths as inductive bias. We further explore architectural and algorithmic choices, including reward fine-tuning by adjoint matching, the use of autoencoders for latent weight representation, conditioning on task-specific context data, and adopting informative source distributions such as Kaiming uniform. Experiments demonstrate that our method matches or surpasses baselines in generating in-distribution weights, improves initialization for downstream training, and supports fine-tuning to enhance performance. Finally, we illustrate a practical application in safety-critical systems: detecting harmful covariate shifts, where our method outperforms the closest comparable baseline.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u548c\u6d41\u6a21\u578b\u7684\u6743\u91cd\u7a7a\u95f4\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u52a8\u6001\u7684\u7ed3\u6784\u5148\u9a8c\uff0c\u7edf\u4e00\u4e86\u8f68\u8ff9\u63a8\u65ad\u6280\u672f\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u5c06\u6269\u6563\u548c\u6d41\u751f\u6210\u6a21\u578b\u7684\u6210\u529f\u6269\u5c55\u5230\u6743\u91cd\u7a7a\u95f4\u5b66\u4e60\uff0c\u5229\u7528\u4f18\u5316\u52a8\u6001\u7684\u7ed3\u6784\u5148\u9a8c\u63d0\u5347\u751f\u6210\u548c\u521d\u59cb\u5316\u6548\u679c\u3002", "method": "\u5c06\u68af\u5ea6\u4e0b\u964d\u8f68\u8ff9\u5efa\u6a21\u4e3a\u8f68\u8ff9\u63a8\u65ad\u95ee\u9898\uff0c\u63d0\u51fa\u68af\u5ea6\u6d41\u5339\u914d\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u7f16\u7801\u5668\u3001\u4efb\u52a1\u4e0a\u4e0b\u6587\u6761\u4ef6\u548cKaiming\u5747\u5300\u5206\u5e03\u7b49\u6280\u672f\u3002", "result": "\u65b9\u6cd5\u5728\u751f\u6210\u5206\u5e03\u5185\u6743\u91cd\u3001\u4e0b\u6e38\u8bad\u7ec3\u521d\u59cb\u5316\u548c\u6027\u80fd\u5fae\u8c03\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5e76\u5728\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u4e2d\u68c0\u6d4b\u6709\u5bb3\u534f\u53d8\u91cf\u504f\u79fb\u65f6\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8bba\u6587\u4e3a\u6743\u91cd\u7a7a\u95f4\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u7528\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5728\u751f\u6210\u548c\u521d\u59cb\u5316\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.10768", "pdf": "https://arxiv.org/pdf/2507.10768", "abs": "https://arxiv.org/abs/2507.10768", "authors": ["Bart Pogodzinski", "Christopher Wewer", "Bernt Schiele", "Jan Eric Lenssen"], "title": "Spatial Reasoners for Continuous Variables in Any Domain", "categories": ["cs.LG", "cs.CV"], "comment": "For the project documentation see https://spatialreasoners.github.io/   . The SRM project website is available at   https://geometric-rl.mpi-inf.mpg.de/srm/ . The work was published on ICML   2025 CODEML workshop", "summary": "We present Spatial Reasoners, a software framework to perform spatial reasoning over continuous variables with generative denoising models. Denoising generative models have become the de-facto standard for image generation, due to their effectiveness in sampling from complex, high-dimensional distributions. Recently, they have started being explored in the context of reasoning over multiple continuous variables. Providing infrastructure for generative reasoning with such models requires a high effort, due to a wide range of different denoising formulations, samplers, and inference strategies. Our presented framework aims to facilitate research in this area, providing easy-to-use interfaces to control variable mapping from arbitrary data domains, generative model paradigms, and inference strategies. Spatial Reasoners are openly available at https://spatialreasoners.github.io/", "AI": {"tldr": "Spatial Reasoners\u662f\u4e00\u4e2a\u7528\u4e8e\u5904\u7406\u8fde\u7eed\u53d8\u91cf\u7a7a\u95f4\u63a8\u7406\u7684\u8f6f\u4ef6\u6846\u67b6\uff0c\u57fa\u4e8e\u751f\u6210\u53bb\u566a\u6a21\u578b\u3002", "motivation": "\u7531\u4e8e\u751f\u6210\u53bb\u566a\u6a21\u578b\u5728\u590d\u6742\u9ad8\u7ef4\u5206\u5e03\u91c7\u6837\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5df2\u6210\u4e3a\u56fe\u50cf\u751f\u6210\u7684\u6807\u51c6\u65b9\u6cd5\uff0c\u4f46\u5176\u5728\u591a\u8fde\u7eed\u53d8\u91cf\u63a8\u7406\u4e2d\u7684\u5e94\u7528\u7814\u7a76\u57fa\u7840\u8bbe\u65bd\u9700\u6c42\u9ad8\u3002", "method": "\u6846\u67b6\u63d0\u4f9b\u6613\u7528\u63a5\u53e3\uff0c\u652f\u6301\u53d8\u91cf\u6620\u5c04\u3001\u751f\u6210\u6a21\u578b\u8303\u5f0f\u548c\u63a8\u7406\u7b56\u7565\u7684\u63a7\u5236\u3002", "result": "Spatial Reasoners\u6846\u67b6\u5df2\u5f00\u6e90\uff0c\u65e8\u5728\u4fc3\u8fdb\u8be5\u9886\u57df\u7814\u7a76\u3002", "conclusion": "\u8be5\u6846\u67b6\u7b80\u5316\u4e86\u751f\u6210\u63a8\u7406\u7814\u7a76\uff0c\u652f\u6301\u591a\u6837\u5316\u5e94\u7528\u3002"}}
{"id": "2507.11001", "pdf": "https://arxiv.org/pdf/2507.11001", "abs": "https://arxiv.org/abs/2507.11001", "authors": ["Yanbo Wang", "Zipeng Fang", "Lei Zhao", "Weidong Chen"], "title": "Learning to Tune Like an Expert: Interpretable and Scene-Aware Navigation via MLLM Reasoning and CVAE-Based Adaptation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Service robots are increasingly deployed in diverse and dynamic environments, where both physical layouts and social contexts change over time and across locations. In these unstructured settings, conventional navigation systems that rely on fixed parameters often fail to generalize across scenarios, resulting in degraded performance and reduced social acceptance. Although recent approaches have leveraged reinforcement learning to enhance traditional planners, these methods often fail in real-world deployments due to poor generalization and limited simulation diversity, which hampers effective sim-to-real transfer. To tackle these issues, we present LE-Nav, an interpretable and scene-aware navigation framework that leverages multi-modal large language model reasoning and conditional variational autoencoders to adaptively tune planner hyperparameters. To achieve zero-shot scene understanding, we utilize one-shot exemplars and chain-of-thought prompting strategies. Additionally, a conditional variational autoencoder captures the mapping between natural language instructions and navigation hyperparameters, enabling expert-level tuning. Experiments show that LE-Nav can generate hyperparameters achieving human-level tuning across diverse planners and scenarios. Real-world navigation trials and a user study on a smart wheelchair platform demonstrate that it outperforms state-of-the-art methods on quantitative metrics such as success rate, efficiency, safety, and comfort, while receiving higher subjective scores for perceived safety and social acceptance. Code is available at https://github.com/Cavendish518/LE-Nav.", "AI": {"tldr": "LE-Nav\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u89c4\u5212\u5668\u8d85\u53c2\u6570\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5bfc\u822a\u7cfb\u7edf\u5728\u52a8\u6001\u548c\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u56e0\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u6a21\u62df\u591a\u6837\u6027\u4e0d\u8db3\u800c\u96be\u4ee5\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u5229\u7528\u5355\u6b21\u793a\u4f8b\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u7b56\u7565\u5b9e\u73b0\u96f6\u6837\u672c\u573a\u666f\u7406\u89e3\uff0c\u81ea\u9002\u5e94\u8c03\u6574\u8d85\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLE-Nav\u751f\u6210\u7684\u8d85\u53c2\u6570\u5728\u591a\u6837\u573a\u666f\u4e2d\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\uff0c\u5b9e\u9645\u5bfc\u822a\u6d4b\u8bd5\u548c\u7528\u6237\u7814\u7a76\u663e\u793a\u5176\u5728\u6210\u529f\u7387\u3001\u6548\u7387\u3001\u5b89\u5168\u6027\u548c\u8212\u9002\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "LE-Nav\u901a\u8fc7\u81ea\u9002\u5e94\u8d85\u53c2\u6570\u8c03\u6574\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u6027\u80fd\u548c\u793e\u4f1a\u63a5\u53d7\u5ea6\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u548c\u975e\u7ed3\u6784\u5316\u73af\u5883\u3002"}}
{"id": "2507.11069", "pdf": "https://arxiv.org/pdf/2507.11069", "abs": "https://arxiv.org/abs/2507.11069", "authors": ["Jeongyun Kim", "Seunghoon Jeong", "Giseop Kim", "Myung-Hwan Jeon", "Eunji Jun", "Ayoung Kim"], "title": "TRAN-D: 2D Gaussian Splatting-based Sparse-view Transparent Object Depth Reconstruction via Physics Simulation for Scene Update", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Understanding the 3D geometry of transparent objects from RGB images is challenging due to their inherent physical properties, such as reflection and refraction. To address these difficulties, especially in scenarios with sparse views and dynamic environments, we introduce TRAN-D, a novel 2D Gaussian Splatting-based depth reconstruction method for transparent objects. Our key insight lies in separating transparent objects from the background, enabling focused optimization of Gaussians corresponding to the object. We mitigate artifacts with an object-aware loss that places Gaussians in obscured regions, ensuring coverage of invisible surfaces while reducing overfitting. Furthermore, we incorporate a physics-based simulation that refines the reconstruction in just a few seconds, effectively handling object removal and chain-reaction movement of remaining objects without the need for rescanning. TRAN-D is evaluated on both synthetic and real-world sequences, and it consistently demonstrated robust improvements over existing GS-based state-of-the-art methods. In comparison with baselines, TRAN-D reduces the mean absolute error by over 39% for the synthetic TRansPose sequences. Furthermore, despite being updated using only one image, TRAN-D reaches a {\\delta} < 2.5 cm accuracy of 48.46%, over 1.5 times that of baselines, which uses six images. Code and more results are available at https://jeongyun0609.github.io/TRAN-D/.", "AI": {"tldr": "TRAN-D\u662f\u4e00\u79cd\u57fa\u4e8e2D\u9ad8\u65af\u6cfc\u6e85\u7684\u900f\u660e\u7269\u4f53\u6df1\u5ea6\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u79bb\u900f\u660e\u7269\u4f53\u4e0e\u80cc\u666f\u5e76\u4f18\u5316\u9ad8\u65af\u5206\u5e03\uff0c\u7ed3\u5408\u7269\u7406\u6a21\u62df\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u89c6\u56fe\u548c\u52a8\u6001\u73af\u5883\u4e0b\u7684\u91cd\u5efa\u6548\u679c\u3002", "motivation": "\u900f\u660e\u7269\u4f53\u76843D\u51e0\u4f55\u91cd\u5efa\u56e0\u53cd\u5c04\u548c\u6298\u5c04\u7b49\u7269\u7406\u7279\u6027\u800c\u6781\u5177\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5728\u7a00\u758f\u89c6\u56fe\u548c\u52a8\u6001\u73af\u5883\u4e2d\u3002", "method": "TRAN-D\u901a\u8fc7\u5206\u79bb\u900f\u660e\u7269\u4f53\u4e0e\u80cc\u666f\uff0c\u4f18\u5316\u5bf9\u5e94\u9ad8\u65af\u5206\u5e03\uff0c\u5e76\u5f15\u5165\u7269\u4f53\u611f\u77e5\u635f\u5931\u548c\u7269\u7406\u6a21\u62df\uff0c\u51cf\u5c11\u4f2a\u5f71\u5e76\u63d0\u5347\u91cd\u5efa\u7cbe\u5ea6\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u573a\u666f\u4e2d\uff0cTRAN-D\u6bd4\u73b0\u6709\u65b9\u6cd5\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u964d\u4f4e39%\uff0c\u5355\u56fe\u50cf\u66f4\u65b0\u65f6\u7cbe\u5ea6\u63d0\u53471.5\u500d\u3002", "conclusion": "TRAN-D\u5728\u900f\u660e\u7269\u4f53\u6df1\u5ea6\u91cd\u5efa\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u548c\u7a00\u758f\u89c6\u56fe\u573a\u666f\u3002"}}
{"id": "2507.11071", "pdf": "https://arxiv.org/pdf/2507.11071", "abs": "https://arxiv.org/abs/2507.11071", "authors": ["Isaiah Thompson Ocansey", "Ritwik Bhattacharya", "Tanmay Sen"], "title": "LogTinyLLM: Tiny Large Language Models Based Contextual Log Anomaly Detection", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Log anomaly detection using traditional rule based or deep learning based methods is often challenging due to the large volume and highly complex nature of log sequence. So effective way of detection of anomalous sequence of logs is crucial for system maintenance and development. This paper proposes parameter efficient finetuning specifically low rank adaptation (LoRA) and adapter based approaches for finding contextual anomalies in sequence of logs in large log data set. It compares different tiny large language models (LLMs) on the Thunderbird dataset. The results show that LoRA based finetuning provides substantial performance improvements of 18 to 19 percentage over LogBert based full finetuning approach, achieving accuracy scores between 97.76% and 98.83% compared to 79.37%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLoRA\u548c\u9002\u914d\u5668\u7684\u9ad8\u6548\u53c2\u6570\u5fae\u8c03\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u5927\u89c4\u6a21\u65e5\u5fd7\u6570\u636e\u4e2d\u7684\u5f02\u5e38\u5e8f\u5217\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u6216\u6df1\u5ea6\u5b66\u4e60\u7684\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u590d\u6742\u65e5\u5fd7\u5e8f\u5217\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff08\u5982LoRA\u548c\u9002\u914d\u5668\uff09\uff0c\u5728Thunderbird\u6570\u636e\u96c6\u4e0a\u6bd4\u8f83\u4e0d\u540c\u5c0f\u578b\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002", "result": "LoRA\u5fae\u8c03\u6bd4LogBert\u5168\u5fae\u8c03\u6027\u80fd\u63d0\u534718-19%\uff0c\u51c6\u786e\u7387\u8fbe\u523097.76%-98.83%\uff0c\u800c\u540e\u8005\u4ec5\u4e3a79.37%\u3002", "conclusion": "LoRA\u5fae\u8c03\u65b9\u6cd5\u5728\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002"}}
