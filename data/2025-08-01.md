<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 28]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Robust and Efficient 3D Gaussian Splatting for Urban Scene Reconstruction](https://arxiv.org/abs/2507.23006)
*Zhensheng Yuan,Haozhi Huang,Zhen Xiong,Di Wang,Guanghua Yang*

Main category: cs.CV

TL;DR: 提出了一种快速重建和实时渲染城市规模场景的框架，通过并行训练和可控细节策略优化效率与质量。


<details>
  <summary>Details</summary>
Motivation: 解决多视角捕获中外观变化对重建和渲染的负面影响，同时提升城市规模场景处理的效率与质量。

Method: 采用场景分区并行训练、可见性图像选择、可控细节策略和外观变换模块，结合深度、尺度和抗锯齿增强模块。

Result: 实验表明，该方法高效重建城市规模场景，在效率和质量上优于先前方法。

Conclusion: 该框架为城市规模场景的重建和渲染提供了高效且高质量的解决方案。

Abstract: We present a framework that enables fast reconstruction and real-time rendering of urban-scale scenes while maintaining robustness against appearance variations across multi-view captures. Our approach begins with scene partitioning for parallel training, employing a visibility-based image selection strategy to optimize training efficiency. A controllable level-of-detail (LOD) strategy explicitly regulates Gaussian density under a user-defined budget, enabling efficient training and rendering while maintaining high visual fidelity. The appearance transformation module mitigates the negative effects of appearance inconsistencies across images while enabling flexible adjustments. Additionally, we utilize enhancement modules, such as depth regularization, scale regularization, and antialiasing, to improve reconstruction fidelity. Experimental results demonstrate that our method effectively reconstructs urban-scale scenes and outperforms previous approaches in both efficiency and quality. The source code is available at: https://yzslab.github.io/REUrbanGS.

</details>


### [2] [Recovering Diagnostic Value: Super-Resolution-Aided Echocardiographic Classification in Resource-Constrained Imaging](https://arxiv.org/abs/2507.23027)
*Krishan Agyakari Raja Babu,Om Prabhu,Annu,Mohanasankar Sivaprakasam*

Main category: cs.CV

TL;DR: 论文探讨了深度学习超分辨率技术在低质量超声心动图上的应用，显著提升了分类任务的表现，尤其是SRResNet模型。


<details>
  <summary>Details</summary>
Motivation: 资源受限环境下超声心动图质量差，限制了诊断模型的效能，超分辨率技术在此领域的应用尚未充分探索。

Method: 使用CAMUS数据集，按图像质量分层，评估两种分类任务（2CH vs. 4CH和ED vs. ES），并应用SRGAN和SRResNet模型增强图像。

Result: SRResNet在提升分类准确率上表现显著且计算高效，超分辨率技术能有效恢复低质量超声心动图的诊断价值。

Conclusion: 超分辨率技术可提升资源受限环境下AI辅助诊断的效能，实现更高效的医疗资源利用。

Abstract: Automated cardiac interpretation in resource-constrained settings (RCS) is often hindered by poor-quality echocardiographic imaging, limiting the effectiveness of downstream diagnostic models. While super-resolution (SR) techniques have shown promise in enhancing magnetic resonance imaging (MRI) and computed tomography (CT) scans, their application to echocardiography-a widely accessible but noise-prone modality-remains underexplored. In this work, we investigate the potential of deep learning-based SR to improve classification accuracy on low-quality 2D echocardiograms. Using the publicly available CAMUS dataset, we stratify samples by image quality and evaluate two clinically relevant tasks of varying complexity: a relatively simple Two-Chamber vs. Four-Chamber (2CH vs. 4CH) view classification and a more complex End-Diastole vs. End-Systole (ED vs. ES) phase classification. We apply two widely used SR models-Super-Resolution Generative Adversarial Network (SRGAN) and Super-Resolution Residual Network (SRResNet), to enhance poor-quality images and observe significant gains in performance metric-particularly with SRResNet, which also offers computational efficiency. Our findings demonstrate that SR can effectively recover diagnostic value in degraded echo scans, making it a viable tool for AI-assisted care in RCS, achieving more with less.

</details>


### [3] [Adaptive Time-step Training for Enhancing Spike-Based Neural Radiance Fields](https://arxiv.org/abs/2507.23033)
*Ranxi Lin,Canming Yao,Jiayi Li,Weihang Liu,Xin Lou,Pingqiang Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种基于脉冲神经网络的动态时间步训练策略（PATA），用于神经辐射场（NeRF）模型，以在保持渲染质量的同时显著减少计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: NeRF模型在训练和推理中依赖密集点采样，导致计算资源需求高，限制了其在资源受限场景中的应用。脉冲神经网络（SNNs）因其高效能特性成为潜在解决方案。

Method: 提出PATA框架，结合动态时间步训练策略，自动平衡渲染质量和时间步长，实现场景自适应推理。基于Instant-NGP架构进行实验验证。

Result: 实验结果显示，PATA在保持渲染质量的同时，减少了64%的推理时间步和61.55%的运行功耗。

Conclusion: PATA通过动态时间步策略有效降低了NeRF模型的资源消耗，为资源受限场景提供了可行的解决方案。

Abstract: Neural Radiance Fields (NeRF)-based models have achieved remarkable success in 3D reconstruction and rendering tasks. However, during both training and inference, these models rely heavily on dense point sampling along rays from multiple viewpoints, resulting in a surge in floating-point operations and severely limiting their use in resource-constrained scenarios like edge computing. Spiking Neural Networks (SNNs), which communicate via binary spikes over discrete time steps, offer a promising alternative due to their energy-efficient nature. Given the inherent variability in scene scale and texture complexity in neural rendering and the prevailing practice of training separate models per scene, we propose a spike-based NeRF framework with a dynamic time step training strategy, termed Pretrain-Adaptive Time-step Adjustment (PATA). This approach automatically explores the trade-off between rendering quality and time step length during training. Consequently, it enables scene-adaptive inference with variable time steps and reduces the additional consumption of computational resources in the inference process. Anchoring to the established Instant-NGP architecture, we evaluate our method across diverse datasets. The experimental results show that PATA can preserve rendering fidelity while reducing inference time steps by 64\% and running power by 61.55\%.

</details>


### [4] [Reference-Guided Diffusion Inpainting For Multimodal Counterfactual Generation](https://arxiv.org/abs/2507.23058)
*Alexandru Buburuzan*

Main category: cs.CV

TL;DR: 论文介绍了两种合成数据生成方法（MObI和AnydoorMed），分别用于自动驾驶和医学图像分析，通过扩散模型实现高真实感和可控性。


<details>
  <summary>Details</summary>
Motivation: 由于真实数据采集成本高且复杂，合成数据方法在安全关键应用中需求增加，但需要高真实感和可控性。

Method: MObI利用扩散模型实现多模态对象修复，AnydoorMed则专注于医学图像的参考引导修复。

Result: 两种方法均能生成高真实感且可控的合成数据，适用于多模态场景和医学图像。

Conclusion: 这些方法展示了基础模型在多模态和医学图像中的适应性，为构建高真实感合成数据提供了新途径。

Abstract: Safety-critical applications, such as autonomous driving and medical image analysis, require extensive multimodal data for rigorous testing. Synthetic data methods are gaining prominence due to the cost and complexity of gathering real-world data, but they demand a high degree of realism and controllability to be useful. This work introduces two novel methods for synthetic data generation in autonomous driving and medical image analysis, namely MObI and AnydoorMed, respectively. MObI is a first-of-its-kind framework for Multimodal Object Inpainting that leverages a diffusion model to produce realistic and controllable object inpaintings across perceptual modalities, demonstrated simultaneously for camera and lidar. Given a single reference RGB image, MObI enables seamless object insertion into existing multimodal scenes at a specified 3D location, guided by a bounding box, while maintaining semantic consistency and multimodal coherence. Unlike traditional inpainting methods that rely solely on edit masks, this approach uses 3D bounding box conditioning to ensure accurate spatial positioning and realistic scaling. AnydoorMed extends this paradigm to the medical imaging domain, focusing on reference-guided inpainting for mammography scans. It leverages a diffusion-based model to inpaint anomalies with impressive detail preservation, maintaining the reference anomaly's structural integrity while semantically blending it with the surrounding tissue. Together, these methods demonstrate that foundation models for reference-guided inpainting in natural images can be readily adapted to diverse perceptual modalities, paving the way for the next generation of systems capable of constructing highly realistic, controllable and multimodal counterfactual scenarios.

</details>


### [5] [X-NeMo: Expressive Neural Motion Reenactment via Disentangled Latent Attention](https://arxiv.org/abs/2507.23143)
*Xiaochen Zhao,Hongyi Xu,Guoxian Song,You Xie,Chenxu Zhang,Xiu Li,Linjie Luo,Jinli Suo,Yebin Liu*

Main category: cs.CV

TL;DR: X-NeMo是一种基于扩散的零样本肖像动画方法，通过驱动视频的面部动作生成静态肖像动画，解决了身份泄漏和表情捕捉问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法中身份泄漏和难以捕捉细微及极端表情的问题。

Method: 提出端到端训练框架，提取1D身份无关运动描述符，通过交叉注意力控制图像生成。

Result: 实验表明X-NeMo优于现有方法，生成高表现力且身份相似的动画。

Conclusion: X-NeMo通过1D运动描述符和交叉注意力设计，有效解决了身份泄漏问题，提升了动画表现力。

Abstract: We propose X-NeMo, a novel zero-shot diffusion-based portrait animation pipeline that animates a static portrait using facial movements from a driving video of a different individual. Our work first identifies the root causes of the key issues in prior approaches, such as identity leakage and difficulty in capturing subtle and extreme expressions. To address these challenges, we introduce a fully end-to-end training framework that distills a 1D identity-agnostic latent motion descriptor from driving image, effectively controlling motion through cross-attention during image generation. Our implicit motion descriptor captures expressive facial motion in fine detail, learned end-to-end from a diverse video dataset without reliance on pretrained motion detectors. We further enhance expressiveness and disentangle motion latents from identity cues by supervising their learning with a dual GAN decoder, alongside spatial and color augmentations. By embedding the driving motion into a 1D latent vector and controlling motion via cross-attention rather than additive spatial guidance, our design eliminates the transmission of spatial-aligned structural clues from the driving condition to the diffusion backbone, substantially mitigating identity leakage. Extensive experiments demonstrate that X-NeMo surpasses state-of-the-art baselines, producing highly expressive animations with superior identity resemblance. Our code and models are available for research.

</details>


### [6] [Neural Multi-View Self-Calibrated Photometric Stereo without Photometric Stereo Cues](https://arxiv.org/abs/2507.23162)
*Xu Cao,Takafumi Taketomi*

Main category: cs.CV

TL;DR: 提出了一种神经逆向渲染方法，从多视角图像中联合重建几何、空间变化反射率和光照条件。


<details>
  <summary>Details</summary>
Motivation: 传统多视角光度立体方法需要光照校准或中间线索（如每视角法线图），而本方法直接从原始图像中联合优化所有场景参数。

Method: 使用神经隐式场表示几何和反射率，并应用阴影感知体积渲染。空间网络预测有符号距离和反射率潜在码，反射率网络基于潜在码和角度编码的法线、视角和光照方向估计反射率值。

Result: 在形状和光照估计精度上优于现有基于法线的方法，适用于视角未对齐的多光照图像，并能处理复杂几何和反射率物体。

Conclusion: 该方法无需中间线索，单阶段优化所有参数，表现出优越的性能和泛化能力。

Abstract: We propose a neural inverse rendering approach that jointly reconstructs geometry, spatially varying reflectance, and lighting conditions from multi-view images captured under varying directional lighting. Unlike prior multi-view photometric stereo methods that require light calibration or intermediate cues such as per-view normal maps, our method jointly optimizes all scene parameters from raw images in a single stage. We represent both geometry and reflectance as neural implicit fields and apply shadow-aware volume rendering. A spatial network first predicts the signed distance and a reflectance latent code for each scene point. A reflectance network then estimates reflectance values conditioned on the latent code and angularly encoded surface normal, view, and light directions. The proposed method outperforms state-of-the-art normal-guided approaches in shape and lighting estimation accuracy, generalizes to view-unaligned multi-light images, and handles objects with challenging geometry and reflectance.

</details>


### [7] [Single Image Rain Streak Removal Using Harris Corner Loss and R-CBAM Network](https://arxiv.org/abs/2507.23185)
*Jongwook Si,Sungyoung Kim*

Main category: cs.CV

TL;DR: 提出了一种结合Corner Loss和R-CBAM模块的图像去雨网络，显著提升了去雨效果。


<details>
  <summary>Details</summary>
Motivation: 单图像去雨问题不仅需要抑制噪声，还需保留细节和视觉质量。

Method: 引入Corner Loss防止边界和纹理信息丢失，并采用R-CBAM模块动态调整特征重要性。

Result: 在Rain100L和Rain100H数据集上PSNR分别达到33.29 dB和26.16 dB。

Conclusion: 该方法在去雨任务中表现优于现有方法，有效保留了图像细节。

Abstract: The problem of single-image rain streak removal goes beyond simple noise suppression, requiring the simultaneous preservation of fine structural details and overall visual quality. In this study, we propose a novel image restoration network that effectively constrains the restoration process by introducing a Corner Loss, which prevents the loss of object boundaries and detailed texture information during restoration. Furthermore, we propose a Residual Convolutional Block Attention Module (R-CBAM) Block into the encoder and decoder to dynamically adjust the importance of features in both spatial and channel dimensions, enabling the network to focus more effectively on regions heavily affected by rain streaks. Quantitative evaluations conducted on the Rain100L and Rain100H datasets demonstrate that the proposed method significantly outperforms previous approaches, achieving a PSNR of 33.29 dB on Rain100L and 26.16 dB on Rain100H.

</details>


### [8] [Adversarial-Guided Diffusion for Multimodal LLM Attacks](https://arxiv.org/abs/2507.23202)
*Chengwei Xia,Fan Ma,Ruijie Quan,Kun Zhan,Yi Yang*

Main category: cs.CV

TL;DR: 提出了一种对抗引导扩散（AGD）方法，通过扩散模型生成对抗图像欺骗多模态大语言模型（MLLMs），同时避免显著扭曲原始图像。


<details>
  <summary>Details</summary>
Motivation: 传统对抗攻击通常在高频段嵌入扰动，容易被防御方法（如低通滤波）抑制。AGD旨在通过将对抗信号嵌入扩散模型的噪声中，提高攻击的鲁棒性。

Method: AGD在反向扩散过程中将目标语义注入噪声组件，利用噪声的全频谱特性，使对抗信号不易被防御方法抑制。

Result: 实验表明，AGD在攻击性能和对抗防御鲁棒性上优于现有方法。

Conclusion: AGD通过噪声组件嵌入对抗信号，显著提高了攻击的鲁棒性和有效性。

Abstract: This paper addresses the challenge of generating adversarial image using a diffusion model to deceive multimodal large language models (MLLMs) into generating the targeted responses, while avoiding significant distortion of the clean image. To address the above challenges, we propose an adversarial-guided diffusion (AGD) approach for adversarial attack MLLMs. We introduce adversarial-guided noise to ensure attack efficacy. A key observation in our design is that, unlike most traditional adversarial attacks which embed high-frequency perturbations directly into the clean image, AGD injects target semantics into the noise component of the reverse diffusion. Since the added noise in a diffusion model spans the entire frequency spectrum, the adversarial signal embedded within it also inherits this full-spectrum property. Importantly, during reverse diffusion, the adversarial image is formed as a linear combination of the clean image and the noise. Thus, when applying defenses such as a simple low-pass filtering, which act independently on each component, the adversarial image within the noise component is less likely to be suppressed, as it is not confined to the high-frequency band. This makes AGD inherently robust to variety defenses. Extensive experiments demonstrate that our AGD outperforms state-of-the-art methods in attack performance as well as in model robustness to some defenses.

</details>


### [9] [PixNerd: Pixel Neural Field Diffusion](https://arxiv.org/abs/2507.23268)
*Shuai Wang,Ziteng Gao,Chenhui Zhu,Weilin Huang,Limin Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为PixelNerd的单尺度、单阶段、高效的端到端解决方案，通过神经场建模补丁解码，避免了传统两阶段训练中的累积误差和解码伪影。


<details>
  <summary>Details</summary>
Motivation: 当前扩散变换器的成功依赖于预训练VAE的压缩潜在空间，但两阶段训练会引入累积误差和解码伪影。现有方法通过复杂级联管道和增加标记复杂度回到像素空间，但效率低下。

Method: 提出PixelNerd框架，利用神经场高效表示补丁解码，实现单尺度、单阶段的端到端训练，无需复杂级联管道或VAE。

Result: 在ImageNet 256×256和512×512上分别达到2.15和2.84 FID，并在文本到图像应用中表现优异（GenEval 0.73，DPG 80.9）。

Conclusion: PixelNerd通过神经场高效建模，显著提升了图像生成质量，同时简化了训练流程。

Abstract: The current success of diffusion transformers heavily depends on the compressed latent space shaped by the pre-trained variational autoencoder(VAE). However, this two-stage training paradigm inevitably introduces accumulated errors and decoding artifacts. To address the aforementioned problems, researchers return to pixel space at the cost of complicated cascade pipelines and increased token complexity. In contrast to their efforts, we propose to model the patch-wise decoding with neural field and present a single-scale, single-stage, efficient, end-to-end solution, coined as pixel neural field diffusion~(PixelNerd). Thanks to the efficient neural field representation in PixNerd, we directly achieved 2.15 FID on ImageNet $256\times256$ and 2.84 FID on ImageNet $512\times512$ without any complex cascade pipeline or VAE. We also extend our PixNerd framework to text-to-image applications. Our PixNerd-XXL/16 achieved a competitive 0.73 overall score on the GenEval benchmark and 80.9 overall score on the DPG benchmark.

</details>


### [10] [iLRM: An Iterative Large 3D Reconstruction Model](https://arxiv.org/abs/2507.23277)
*Gyeongjin Kang,Seungtae Nam,Xiangyu Sun,Sameh Khamis,Abdelrahman Mohamed,Eunbyung Park*

Main category: cs.CV

TL;DR: iLRM提出了一种迭代式大规模3D重建模型，通过解耦场景表示、两阶段注意力机制和高分辨率信息注入，解决了现有方法在计算成本和可扩展性上的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于transformer的3D重建方法因全注意力机制在多视图或高分辨率下计算成本过高，限制了其可扩展性。

Method: iLRM采用迭代细化机制，通过解耦场景表示、两阶段注意力方案和逐层高分辨率信息注入，实现高效3D重建。

Result: 在RE10K和DL3DV数据集上，iLRM在重建质量和速度上均优于现有方法，且具有更高的可扩展性。

Conclusion: iLRM通过创新设计显著提升了3D重建的效率和质量，适用于多视图输入场景。

Abstract: Feed-forward 3D modeling has emerged as a promising approach for rapid and high-quality 3D reconstruction. In particular, directly generating explicit 3D representations, such as 3D Gaussian splatting, has attracted significant attention due to its fast and high-quality rendering, as well as numerous applications. However, many state-of-the-art methods, primarily based on transformer architectures, suffer from severe scalability issues because they rely on full attention across image tokens from multiple input views, resulting in prohibitive computational costs as the number of views or image resolution increases. Toward a scalable and efficient feed-forward 3D reconstruction, we introduce an iterative Large 3D Reconstruction Model (iLRM) that generates 3D Gaussian representations through an iterative refinement mechanism, guided by three core principles: (1) decoupling the scene representation from input-view images to enable compact 3D representations; (2) decomposing fully-attentional multi-view interactions into a two-stage attention scheme to reduce computational costs; and (3) injecting high-resolution information at every layer to achieve high-fidelity reconstruction. Experimental results on widely used datasets, such as RE10K and DL3DV, demonstrate that iLRM outperforms existing methods in both reconstruction quality and speed. Notably, iLRM exhibits superior scalability, delivering significantly higher reconstruction quality under comparable computational cost by efficiently leveraging a larger number of input views.

</details>


### [11] [UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing](https://arxiv.org/abs/2507.23278)
*Hao Tang,Chenwei Xie,Xiaoyi Bao,Tingyu Weng,Pandeng Li,Yun Zheng,Liwei Wang*

Main category: cs.CV

TL;DR: UniLIP扩展了CLIP的功能，支持重建、生成和编辑，通过两阶段训练和自蒸馏策略保持原始理解能力，同时在生成和编辑任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP统一方法需要额外模块支持重建和生成任务，导致性能下降或理解能力不一致。UniLIP旨在解决这一问题。

Method: 采用两阶段训练和自蒸馏策略，引入双条件架构连接MLLM和扩散变换器。

Result: 在生成任务中，GenEval和WISE得分分别为0.87和0.53；在编辑任务中，ImgEdit得分为3.62，均超越同类模型。

Conclusion: UniLIP成功扩展了CLIP的应用范围，在理解和生成/编辑任务中均表现优异。

Abstract: In this paper, we propose UniLIP, which extends CLIP to reconstruction, generation and editing, thereby building a unified tokenizer upon its exceptional comprehension capabilities. Previous CLIP-based unified methods often require additional diffusion decoders or quantization to support reconstruction and generation tasks, leading to inconsistent reconstruction or degradation of original comprehension performance.In contrast, we introduce a two-stage training scheme and a self-distillation strategy that progressively integrates reconstruction capabilities into CLIP, allowing it to maintain original comprehension performance while achieving effective image reconstruction. Furthermore, we propose a dual-condition architecture to connect the MLLM and diffusion transformer, using both learnable queries and the last layer multimodal hidden states as joint conditions. This method not only enables the utilization of the MLLM's strong reasoning capabilities in generation tasks, but also maximizes the exploitation of the rich information in UniLIP features during editing tasks. In text-to-image generation tasks, UniLIP obtains scores of 0.87 and 0.53 on GenEval and WISE benchmark respectively, surpassing all previous unified models of similar scale. In image editing, UniLIP also achieves a score of 3.62 on the ImgEdit Benchmark, surpassing recent state-of-the-art models such as BAGEL and UniWorld-V1. UniLIP effectively expand the application scope of CLIP, enabling continuous CLIP features to not only serve as the optimal choice for understanding tasks but also achieve highly competitive performance in generation and editing tasks.

</details>


### [12] [Training-free Geometric Image Editing on Diffusion Models](https://arxiv.org/abs/2507.23300)
*Hanshen Zhu,Zhen Zhu,Kaile Zhang,Yiming Gong,Yuliang Liu,Xiang Bai*

Main category: cs.CV

TL;DR: 该论文提出了一种解耦的图像几何编辑方法FreeFine，通过分离对象变换、源区域修复和目标区域细化，显著提升了编辑效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的编辑方法在处理大或复杂的几何变换时效果不佳，需要一种更高效的方法。

Method: 采用解耦管道，分别处理对象变换、源区域修复和目标区域细化，并使用无需训练的扩散方法FreeFine实现后两步。

Result: 在GeoBench基准测试中，FreeFine在图像保真度和编辑精度上优于现有方法，尤其在复杂变换下表现突出。

Conclusion: FreeFine通过解耦和训练自由的方法，显著提升了图像几何编辑的效果和效率。

Abstract: We tackle the task of geometric image editing, where an object within an image is repositioned, reoriented, or reshaped while preserving overall scene coherence. Previous diffusion-based editing methods often attempt to handle all relevant subtasks in a single step, proving difficult when transformations become large or structurally complex. We address this by proposing a decoupled pipeline that separates object transformation, source region inpainting, and target region refinement. Both inpainting and refinement are implemented using a training-free diffusion approach, FreeFine. In experiments on our new GeoBench benchmark, which contains both 2D and 3D editing scenarios, FreeFine outperforms state-of-the-art alternatives in image fidelity, and edit precision, especially under demanding transformations. Code and benchmark are available at: https://github.com/CIawevy/FreeFine

</details>


### [13] [MagicRoad: Semantic-Aware 3D Road Surface Reconstruction via Obstacle Inpainting](https://arxiv.org/abs/2507.23340)
*Xingyue Peng,Yuandong Lyu,Lang Zhang,Jian Zhu,Songtao Wang,Jiaxin Deng,Songxin Lu,Weiliang Ma,Dangen She,Peng Jia,XianPeng Lang*

Main category: cs.CV

TL;DR: 提出了一种鲁棒的道路表面重建框架，结合了遮挡感知的2D高斯曲面和语义引导的颜色增强，以应对动态遮挡和光照变化。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态遮挡、视觉杂乱和光照变化下表现不佳，需要一种更鲁棒的道路重建方法。

Method: 采用平面适应的高斯表示进行大规模建模，利用分割引导的视频修复去除动态和静态前景物体，并通过语义感知的HSV空间校正增强颜色一致性。

Result: 在城市规模数据集上的实验表明，该方法在真实条件下显著优于现有方法。

Conclusion: 该框架能够生成视觉一致且几何精确的道路重建结果。

Abstract: Road surface reconstruction is essential for autonomous driving, supporting centimeter-accurate lane perception and high-definition mapping in complex urban environments.While recent methods based on mesh rendering or 3D Gaussian splatting (3DGS) achieve promising results under clean and static conditions, they remain vulnerable to occlusions from dynamic agents, visual clutter from static obstacles, and appearance degradation caused by lighting and weather changes. We present a robust reconstruction framework that integrates occlusion-aware 2D Gaussian surfels with semantic-guided color enhancement to recover clean, consistent road surfaces. Our method leverages a planar-adapted Gaussian representation for efficient large-scale modeling, employs segmentation-guided video inpainting to remove both dynamic and static foreground objects, and enhances color coherence via semantic-aware correction in HSV space. Extensive experiments on urban-scale datasets demonstrate that our framework produces visually coherent and geometrically faithful reconstructions, significantly outperforming prior methods under real-world conditions.

</details>


### [14] [IN45023 Neural Network Design Patterns in Computer Vision Seminar Report, Summer 2025](https://arxiv.org/abs/2507.23357)
*Radu-Andrei Bourceanu,Neil De La Fuente,Jan Grimm,Andrei Jardan,Andriy Manucharyan,Cornelius Weiss,Roman Pflugfelder*

Main category: cs.CV

TL;DR: 分析六篇有影响力的论文，探讨计算机视觉中关键设计模式的演变，包括ResNet、ViT、GANs、LDMs、DINO和MAE。


<details>
  <summary>Details</summary>
Motivation: 研究计算机视觉领域的关键设计模式及其演变，为未来研究提供参考。

Method: 通过分析六篇代表性论文，总结其核心方法和技术创新。

Result: 展示了从ResNet到MAE的技术进步，包括深度学习、生成模型和自监督学习的发展。

Conclusion: 计算机视觉领域的设计模式不断演进，新技术如Transformer和自监督学习正推动该领域发展。

Abstract: This report analyzes the evolution of key design patterns in computer vision by examining six influential papers. The analy- sis begins with foundational architectures for image recognition. We review ResNet, which introduced residual connections to overcome the vanishing gradient problem and enable effective training of significantly deeper convolutional networks. Subsequently, we examine the Vision Transformer (ViT), which established a new paradigm by applying the Transformer ar- chitecture to sequences of image patches, demonstrating the efficacy of attention-based models for large-scale image recogni- tion. Building on these visual representation backbones, we investigate generative models. Generative Adversarial Networks (GANs) are analyzed for their novel adversarial training process, which challenges a generator against a discriminator to learn complex data distributions. Then, Latent Diffusion Models (LDMs) are covered, which improve upon prior generative methods by performing a sequential denoising process in a perceptually compressed latent space. LDMs achieve high-fidelity synthesis with greater computational efficiency, representing the current state-of-the-art for image generation. Finally, we explore self-supervised learning techniques that reduce dependency on labeled data. DINO is a self-distillation framework in which a student network learns to match the output of a momentum-updated teacher, yielding features with strong k-NN classification performance. We conclude with Masked Autoencoders (MAE), which utilize an asymmetric encoder-decoder design to reconstruct heavily masked inputs, providing a highly scalable and effective method for pre-training large-scale vision models.

</details>


### [15] [UniEmo: Unifying Emotional Understanding and Generation with Learnable Expert Queries](https://arxiv.org/abs/2507.23372)
*Yijie Zhu,Lingsen Zhang,Zitong Yu,Rui Shao,Tao Tan,Liqiang Nie*

Main category: cs.CV

TL;DR: UniEmo是一个统一框架，将情感理解与生成任务结合，通过层次化情感理解链和多尺度特征提取实现互补增强，并通过扩散模型生成情感图像。


<details>
  <summary>Details</summary>
Motivation: 情感理解与生成任务通常被分开处理，但它们本质上是互补的，可以相互增强。

Method: 提出层次化情感理解链和可学习专家查询，提取多尺度情感特征；融合这些特征指导扩散模型生成情感图像；引入情感相关系数和情感条件损失增强生成多样性。

Result: UniEmo在情感理解和生成任务上显著优于现有方法。

Conclusion: 通过联合训练和生成驱动的双重反馈机制，UniEmo提升了模型的情感理解能力。

Abstract: Emotional understanding and generation are often treated as separate tasks, yet they are inherently complementary and can mutually enhance each other. In this paper, we propose the UniEmo, a unified framework that seamlessly integrates these two tasks. The key challenge lies in the abstract nature of emotions, necessitating the extraction of visual representations beneficial for both tasks. To address this, we propose a hierarchical emotional understanding chain with learnable expert queries that progressively extracts multi-scale emotional features, thereby serving as a foundational step for unification. Simultaneously, we fuse these expert queries and emotional representations to guide the diffusion model in generating emotion-evoking images. To enhance the diversity and fidelity of the generated emotional images, we further introduce the emotional correlation coefficient and emotional condition loss into the fusion process. This step facilitates fusion and alignment for emotional generation guided by the understanding. In turn, we demonstrate that joint training allows the generation component to provide implicit feedback to the understanding part. Furthermore, we propose a novel data filtering algorithm to select high-quality and diverse emotional images generated by the well-trained model, which explicitly feedback into the understanding part. Together, these generation-driven dual feedback processes enhance the model's understanding capacity. Extensive experiments show that UniEmo significantly outperforms state-of-the-art methods in both emotional understanding and generation tasks. The code for the proposed method is available at https://github.com/JiuTian-VL/UniEmo.

</details>


### [16] [NeRF Is a Valuable Assistant for 3D Gaussian Splatting](https://arxiv.org/abs/2507.23374)
*Shuangkang Fang,I-Chao Shen,Takeo Igarashi,Yufeng Wang,ZeSheng Wang,Yi Yang,Wenrui Ding,Shuchang Zhou*

Main category: cs.CV

TL;DR: NeRF-GS结合NeRF和3DGS，通过联合优化解决3DGS的局限性，提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决3DGS对高斯初始化敏感、空间感知有限和缺乏高斯间关联的问题。

Method: 通过共享3D空间信息对齐NeRF和3DGS的空间特征，并优化残差向量。

Result: 在基准数据集上表现优于现有方法，达到最优性能。

Conclusion: NeRF和3DGS是互补的，为混合方法提供了新思路。

Abstract: We introduce NeRF-GS, a novel framework that jointly optimizes Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework leverages the inherent continuous spatial representation of NeRF to mitigate several limitations of 3DGS, including sensitivity to Gaussian initialization, limited spatial awareness, and weak inter-Gaussian correlations, thereby enhancing its performance. In NeRF-GS, we revisit the design of 3DGS and progressively align its spatial features with NeRF, enabling both representations to be optimized within the same scene through shared 3D spatial information. We further address the formal distinctions between the two approaches by optimizing residual vectors for both implicit features and Gaussian positions to enhance the personalized capabilities of 3DGS. Experimental results on benchmark datasets show that NeRF-GS surpasses existing methods and achieves state-of-the-art performance. This outcome confirms that NeRF and 3DGS are complementary rather than competing, offering new insights into hybrid approaches that combine 3DGS and NeRF for efficient 3D scene representation.

</details>


### [17] [Out-of-Distribution Detection in Medical Imaging via Diffusion Trajectories](https://arxiv.org/abs/2507.23411)
*Lemar Abdi,Francisco Caetano,Amaan Valiuddin,Christiaan Viviers,Hamdi Joudeh,Fons van der Sommen*

Main category: cs.CV

TL;DR: 提出了一种基于Stein分数的去噪扩散模型（SBDDM）的无监督OOD检测方法，通过扩散轨迹曲率实现高效异常检测，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决现有生成方法在计算成本高、不可靠且需要重新训练的问题，提升OOD检测的效率和鲁棒性。

Method: 利用预训练的SBDDM模型，通过前向扩散轨迹的Stein分数计算异常分数，仅需五步扩散。

Result: 在Near-OOD和Far-OOD检测中分别实现10.43%和18.10%的相对改进，计算成本大幅降低。

Conclusion: SBDDM是一种高效、可靠的OOD检测方法，适用于实时计算机辅助诊断。

Abstract: In medical imaging, unsupervised out-of-distribution (OOD) detection offers an attractive approach for identifying pathological cases with extremely low incidence rates. In contrast to supervised methods, OOD-based approaches function without labels and are inherently robust to data imbalances. Current generative approaches often rely on likelihood estimation or reconstruction error, but these methods can be computationally expensive, unreliable, and require retraining if the inlier data changes. These limitations hinder their ability to distinguish nominal from anomalous inputs efficiently, consistently, and robustly. We propose a reconstruction-free OOD detection method that leverages the forward diffusion trajectories of a Stein score-based denoising diffusion model (SBDDM). By capturing trajectory curvature via the estimated Stein score, our approach enables accurate anomaly scoring with only five diffusion steps. A single SBDDM pre-trained on a large, semantically aligned medical dataset generalizes effectively across multiple Near-OOD and Far-OOD benchmarks, achieving state-of-the-art performance while drastically reducing computational cost during inference. Compared to existing methods, SBDDM achieves a relative improvement of up to 10.43% and 18.10% for Near-OOD and Far-OOD detection, making it a practical building block for real-time, reliable computer-aided diagnosis.

</details>


### [18] [Stable-Sim2Real: Exploring Simulation of Real-Captured 3D Data with Two-Stage Depth Diffusion](https://arxiv.org/abs/2507.23483)
*Mutian Xu,Chongjie Ye,Haolin Liu,Yushuang Wu,Jiahao Chang,Xiaoguang Han*

Main category: cs.CV

TL;DR: 论文提出了一种名为Stable-Sim2Real的两阶段深度扩散模型，用于数据驱动的3D数据模拟，显著提升了真实世界3D视觉任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D数据模拟方法难以完全捕捉真实数据复杂性的问题，探索数据驱动的解决方案。

Method: 采用两阶段深度扩散模型：第一阶段生成合成与真实深度之间的残差，第二阶段通过调整扩散损失优化局部区域。

Result: 实验表明，该方法生成的3D模拟数据显著提升了真实世界任务的性能，且与真实数据高度相似。

Conclusion: Stable-Sim2Real为3D数据模拟提供了高效的数据驱动解决方案，具有实际应用潜力。

Abstract: 3D data simulation aims to bridge the gap between simulated and real-captured 3D data, which is a fundamental problem for real-world 3D visual tasks. Most 3D data simulation methods inject predefined physical priors but struggle to capture the full complexity of real data. An optimal approach involves learning an implicit mapping from synthetic to realistic data in a data-driven manner, but progress in this solution has met stagnation in recent studies. This work explores a new solution path of data-driven 3D simulation, called Stable-Sim2Real, based on a novel two-stage depth diffusion model. The initial stage finetunes Stable-Diffusion to generate the residual between the real and synthetic paired depth, producing a stable but coarse depth, where some local regions may deviate from realistic patterns. To enhance this, both the synthetic and initial output depth are fed into a second-stage diffusion, where diffusion loss is adjusted to prioritize these distinct areas identified by a 3D discriminator. We provide a new benchmark scheme to evaluate 3D data simulation methods. Extensive experiments show that training the network with the 3D simulated data derived from our method significantly enhances performance in real-world 3D visual tasks. Moreover, the evaluation demonstrates the high similarity between our 3D simulated data and real-captured patterns. Project page: https://mutianxu.github.io/stable-sim2real/.

</details>


### [19] [Online Estimation of Table-Top Grown Strawberry Mass in Field Conditions with Occlusions](https://arxiv.org/abs/2507.23487)
*Jinshan Zhen,Yuanyue Ge,Tianxiao Zhu,Hui Zhao,Ya Xiong*

Main category: cs.CV

TL;DR: 提出了一种基于RGB-D和深度学习的草莓质量估计方法，解决了遮挡和姿态变化问题，误差率低。


<details>
  <summary>Details</summary>
Motivation: 解决田间草莓质量估计中因遮挡和姿态变化带来的挑战。

Method: 结合YOLOv8-Seg实例分割、CycleGAN遮挡修复和倾斜角校正，通过多项式回归模型估计质量。

Result: 孤立草莓和遮挡情况下的平均质量估计误差分别为8.11%和10.47%，CycleGAN在遮挡修复上优于LaMa。

Conclusion: 该方法为复杂遮挡条件下的自动化收获和产量监测提供了可靠解决方案。

Abstract: Accurate mass estimation of table-top grown strawberries under field conditions remains challenging due to frequent occlusions and pose variations. This study proposes a vision-based pipeline integrating RGB-D sensing and deep learning to enable non-destructive, real-time and online mass estimation. The method employed YOLOv8-Seg for instance segmentation, Cycle-consistent generative adversarial network (CycleGAN) for occluded region completion, and tilt-angle correction to refine frontal projection area calculations. A polynomial regression model then mapped the geometric features to mass. Experiments demonstrated mean mass estimation errors of 8.11% for isolated strawberries and 10.47% for occluded cases. CycleGAN outperformed large mask inpainting (LaMa) model in occlusion recovery, achieving superior pixel area ratios (PAR) (mean: 0.978 vs. 1.112) and higher intersection over union (IoU) scores (92.3% vs. 47.7% in the [0.9-1] range). This approach addresses critical limitations of traditional methods, offering a robust solution for automated harvesting and yield monitoring with complex occlusion patterns.

</details>


### [20] [Gaussian Splatting Feature Fields for Privacy-Preserving Visual Localization](https://arxiv.org/abs/2507.23569)
*Maxime Pietrantoni,Gabriela Csurka,Torsten Sattler*

Main category: cs.CV

TL;DR: 论文提出了一种基于3D高斯泼溅（3DGS）的视觉定位方法，结合显式几何模型和隐式特征场，实现高精度且隐私保护的定位。


<details>
  <summary>Details</summary>
Motivation: 解决视觉定位任务中隐私保护和精度需求的问题。

Method: 提出高斯泼溅特征场（GSFFs），结合3DGS的几何信息和对比学习框架，学习鲁棒特征表示，并通过3D结构聚类生成隐私保护的分割结果。

Result: 在多个真实数据集上验证，隐私和非隐私保护定位均达到最优性能。

Conclusion: GSFFs方法在视觉定位中实现了高精度和隐私保护的平衡。

Abstract: Visual localization is the task of estimating a camera pose in a known environment. In this paper, we utilize 3D Gaussian Splatting (3DGS)-based representations for accurate and privacy-preserving visual localization. We propose Gaussian Splatting Feature Fields (GSFFs), a scene representation for visual localization that combines an explicit geometry model (3DGS) with an implicit feature field. We leverage the dense geometric information and differentiable rasterization algorithm from 3DGS to learn robust feature representations grounded in 3D. In particular, we align a 3D scale-aware feature field and a 2D feature encoder in a common embedding space through a contrastive framework. Using a 3D structure-informed clustering procedure, we further regularize the representation learning and seamlessly convert the features to segmentations, which can be used for privacy-preserving visual localization. Pose refinement, which involves aligning either feature maps or segmentations from a query image with those rendered from the GSFFs scene representation, is used to achieve localization. The resulting privacy- and non-privacy-preserving localization pipelines, evaluated on multiple real-world datasets, show state-of-the-art performances.

</details>


### [21] [DivControl: Knowledge Diversion for Controllable Image Generation](https://arxiv.org/abs/2507.23620)
*Yucheng Xie,Fu Feng,Ruixiao Shi,Jing Wang,Yong Rui,Xin Geng*

Main category: cs.CV

TL;DR: DivControl提出了一种可分解的预训练框架，通过知识转移和动态门控实现统一可控生成和高效适应，显著降低训练成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多条件生成中泛化能力差且适应成本高，需要一种更高效和通用的解决方案。

Method: 通过SVD分解ControlNet为基本组件，利用知识转移和动态门控实现条件无关和条件特定的解耦，并通过表示对齐损失提升条件保真度。

Result: DivControl在可控性上达到SOTA，训练成本降低36.4倍，并在未见条件上表现出强大的零样本和小样本性能。

Conclusion: DivControl展示了优越的可扩展性、模块化和迁移能力，为多条件生成提供了高效解决方案。

Abstract: Diffusion models have advanced from text-to-image (T2I) to image-to-image (I2I) generation by incorporating structured inputs such as depth maps, enabling fine-grained spatial control. However, existing methods either train separate models for each condition or rely on unified architectures with entangled representations, resulting in poor generalization and high adaptation costs for novel conditions. To this end, we propose DivControl, a decomposable pretraining framework for unified controllable generation and efficient adaptation. DivControl factorizes ControlNet via SVD into basic components-pairs of singular vectors-which are disentangled into condition-agnostic learngenes and condition-specific tailors through knowledge diversion during multi-condition training. Knowledge diversion is implemented via a dynamic gate that performs soft routing over tailors based on the semantics of condition instructions, enabling zero-shot generalization and parameter-efficient adaptation to novel conditions. To further improve condition fidelity and training efficiency, we introduce a representation alignment loss that aligns condition embeddings with early diffusion features. Extensive experiments demonstrate that DivControl achieves state-of-the-art controllability with 36.4$\times$ less training cost, while simultaneously improving average performance on basic conditions. It also delivers strong zero-shot and few-shot performance on unseen conditions, demonstrating superior scalability, modularity, and transferability.

</details>


### [22] [Adaptively Distilled ControlNet: Accelerated Training and Superior Sampling for Medical Image Synthesis](https://arxiv.org/abs/2507.23652)
*Kunpeng Qiu,Zhiying Zhou,Yongxin Guo*

Main category: cs.CV

TL;DR: 提出了一种任务无关的框架Adaptively Distilled ControlNet，通过双模型蒸馏加速训练和优化，解决了医学图像标注中的隐私和标注效率问题。


<details>
  <summary>Details</summary>
Motivation: 医学图像标注受隐私和标注效率限制，影响分割模型的性能和泛化能力。

Method: 采用双模型蒸馏框架，教师模型通过掩码-图像对指导学生模型，并通过自适应正则化优化。

Result: 在KiTS19和Polyps数据集上，TransUNet和SANet分别实现了mDice/mIoU的显著提升。

Conclusion: 该框架在隐私保护和性能提升方面表现出色，代码已开源。

Abstract: Medical image annotation is constrained by privacy concerns and labor-intensive labeling, significantly limiting the performance and generalization of segmentation models. While mask-controllable diffusion models excel in synthesis, they struggle with precise lesion-mask alignment. We propose \textbf{Adaptively Distilled ControlNet}, a task-agnostic framework that accelerates training and optimization through dual-model distillation. Specifically, during training, a teacher model, conditioned on mask-image pairs, regularizes a mask-only student model via predicted noise alignment in parameter space, further enhanced by adaptive regularization based on lesion-background ratios. During sampling, only the student model is used, enabling privacy-preserving medical image generation. Comprehensive evaluations on two distinct medical datasets demonstrate state-of-the-art performance: TransUNet improves mDice/mIoU by 2.4%/4.2% on KiTS19, while SANet achieves 2.6%/3.5% gains on Polyps, highlighting its effectiveness and superiority. Code is available at GitHub.

</details>


### [23] [I2V-GS: Infrastructure-to-Vehicle View Transformation with Gaussian Splatting for Autonomous Driving Data Generation](https://arxiv.org/abs/2507.23683)
*Jialei Chen,Wuhao Xu,Sipeng He,Baoru Huang,Dongchun Ren*

Main category: cs.CV

TL;DR: I2V-GS是一种通过高斯散射将基础设施视图转换为车辆视图的新方法，用于生成自动驾驶数据。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶数据主要通过车辆采集，成本高且效率低，而通过合成数据可以解决这一问题。

Method: 采用自适应深度扭曲生成密集训练视图，并通过级联策略扩展视图范围，同时利用跨视图信息进行置信度优化。

Result: I2V-GS在车辆视图下的合成质量显著提升，NTA-Iou、NTL-Iou和FID分别提高了45.7%、34.2%和14.9%。

Conclusion: I2V-GS是首个实现基础设施-车辆视图转换的框架，为自动驾驶数据生成提供了高效解决方案。

Abstract: Vast and high-quality data are essential for end-to-end autonomous driving systems. However, current driving data is mainly collected by vehicles, which is expensive and inefficient. A potential solution lies in synthesizing data from real-world images. Recent advancements in 3D reconstruction demonstrate photorealistic novel view synthesis, highlighting the potential of generating driving data from images captured on the road. This paper introduces a novel method, I2V-GS, to transfer the Infrastructure view To the Vehicle view with Gaussian Splatting. Reconstruction from sparse infrastructure viewpoints and rendering under large view transformations is a challenging problem. We adopt the adaptive depth warp to generate dense training views. To further expand the range of views, we employ a cascade strategy to inpaint warped images, which also ensures inpainting content is consistent across views. To further ensure the reliability of the diffusion model, we utilize the cross-view information to perform a confidenceguided optimization. Moreover, we introduce RoadSight, a multi-modality, multi-view dataset from real scenarios in infrastructure views. To our knowledge, I2V-GS is the first framework to generate autonomous driving datasets with infrastructure-vehicle view transformation. Experimental results demonstrate that I2V-GS significantly improves synthesis quality under vehicle view, outperforming StreetGaussian in NTA-Iou, NTL-Iou, and FID by 45.7%, 34.2%, and 14.9%, respectively.

</details>


### [24] [UniLDiff: Unlocking the Power of Diffusion Priors for All-in-One Image Restoration](https://arxiv.org/abs/2507.23685)
*Zihan Cheng,Liangtai Zhou,Dian Chen,Ni Tang,Xiaotong Luo,Yanyun Qu*

Main category: cs.CV

TL;DR: 提出了一种基于潜在扩散模型（LDM）的统一图像修复框架，通过Degradation-Aware Feature Fusion（DAFF）模块和Detail-Aware Expert Module（DAEM）提升多任务和混合退化场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 解决All-in-One Image Restoration（AiOIR）中的核心挑战，利用扩散模型的强大生成能力处理多样化退化问题。

Method: 结合低质量视觉先验到扩散过程，设计DAFF模块自适应处理不同退化类型，并通过DAEM模块减少细节损失。

Result: 在多任务和混合退化设置下，方法表现优于现有技术。

Conclusion: 扩散模型先验在统一图像修复中具有实际潜力。

Abstract: All-in-One Image Restoration (AiOIR) has emerged as a promising yet challenging research direction. To address its core challenges, we propose a novel unified image restoration framework based on latent diffusion models (LDMs). Our approach structurally integrates low-quality visual priors into the diffusion process, unlocking the powerful generative capacity of diffusion models for diverse degradations. Specifically, we design a Degradation-Aware Feature Fusion (DAFF) module to enable adaptive handling of diverse degradation types. Furthermore, to mitigate detail loss caused by the high compression and iterative sampling of LDMs, we design a Detail-Aware Expert Module (DAEM) in the decoder to enhance texture and fine-structure recovery. Extensive experiments across multi-task and mixed degradation settings demonstrate that our method consistently achieves state-of-the-art performance, highlighting the practical potential of diffusion priors for unified image restoration. Our code will be released.

</details>


### [25] [Enhanced Velocity Field Modeling for Gaussian Video Reconstruction](https://arxiv.org/abs/2507.23704)
*Zhenyang Li,Xiaoyang Bai,Tongchen Zhang,Pengfei Shen,Weiwei Xu,Yifan Peng*

Main category: cs.CV

TL;DR: 论文提出了一种名为FlowGaussian-VR的方法，通过速度场建模和流辅助自适应致密化策略，解决了复杂运动和尺度变化下3D高斯溅射重建的视觉质量不足问题。


<details>
  <summary>Details</summary>
Motivation: 高保真3D视频重建对VR/AR中的动态场景实时渲染至关重要，但现有方法在复杂运动和尺度变化下表现不佳。

Method: FlowGaussian-VR包含速度场渲染（VFR）管道和流辅助自适应致密化（FAD）策略，优化动态内容重建。

Result: 在多视图动态重建和新视角合成任务中，该方法显著提升了视觉质量（PSNR增益超过2.5 dB），减少了动态纹理的模糊伪影。

Conclusion: FlowGaussian-VR通过速度场建模和自适应致密化，有效提升了动态场景重建的视觉质量和轨迹跟踪能力。

Abstract: High-fidelity 3D video reconstruction is essential for enabling real-time rendering of dynamic scenes with realistic motion in virtual and augmented reality (VR/AR). The deformation field paradigm of 3D Gaussian splatting has achieved near-photorealistic results in video reconstruction due to the great representation capability of deep deformation networks. However, in videos with complex motion and significant scale variations, deformation networks often overfit to irregular Gaussian trajectories, leading to suboptimal visual quality. Moreover, the gradient-based densification strategy designed for static scene reconstruction proves inadequate to address the absence of dynamic content. In light of these challenges, we propose a flow-empowered velocity field modeling scheme tailored for Gaussian video reconstruction, dubbed FlowGaussian-VR. It consists of two core components: a velocity field rendering (VFR) pipeline which enables optical flow-based optimization, and a flow-assisted adaptive densification (FAD) strategy that adjusts the number and size of Gaussians in dynamic regions. We validate our model's effectiveness on multi-view dynamic reconstruction and novel view synthesis with multiple real-world datasets containing challenging motion scenarios, demonstrating not only notable visual improvements (over 2.5 dB gain in PSNR) and less blurry artifacts in dynamic textures, but also regularized and trackable per-Gaussian trajectories.

</details>


### [26] [SeqAffordSplat: Scene-level Sequential Affordance Reasoning on 3D Gaussian Splatting](https://arxiv.org/abs/2507.23772)
*Di Li,Jie Feng,Jiahao Chen,Weisheng Dong,Guanbin Li,Yuhui Zheng,Mingtao Feng,Guangming Shi*

Main category: cs.CV

TL;DR: 论文提出SeqAffordSplat基准和SeqSplatNet框架，用于解决3D高斯泼溅环境中的长时程、多物体功能推理任务。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法局限于单物体、单步交互，无法满足复杂现实场景的需求。

Method: 提出SeqSplatNet框架，结合大语言模型和条件解码器，生成3D功能掩码序列；引入预训练策略和特征注入机制。

Result: 在SeqAffordSplat基准上取得最优性能，支持复杂场景的长时程功能推理。

Conclusion: SeqSplatNet成功将功能推理从单步扩展到场景级复杂任务。

Abstract: 3D affordance reasoning, the task of associating human instructions with the functional regions of 3D objects, is a critical capability for embodied agents. Current methods based on 3D Gaussian Splatting (3DGS) are fundamentally limited to single-object, single-step interactions, a paradigm that falls short of addressing the long-horizon, multi-object tasks required for complex real-world applications. To bridge this gap, we introduce the novel task of Sequential 3D Gaussian Affordance Reasoning and establish SeqAffordSplat, a large-scale benchmark featuring 1800+ scenes to support research on long-horizon affordance understanding in complex 3DGS environments. We then propose SeqSplatNet, an end-to-end framework that directly maps an instruction to a sequence of 3D affordance masks. SeqSplatNet employs a large language model that autoregressively generates text interleaved with special segmentation tokens, guiding a conditional decoder to produce the corresponding 3D mask. To handle complex scene geometry, we introduce a pre-training strategy, Conditional Geometric Reconstruction, where the model learns to reconstruct complete affordance region masks from known geometric observations, thereby building a robust geometric prior. Furthermore, to resolve semantic ambiguities, we design a feature injection mechanism that lifts rich semantic features from 2D Vision Foundation Models (VFM) and fuses them into the 3D decoder at multiple scales. Extensive experiments demonstrate that our method sets a new state-of-the-art on our challenging benchmark, effectively advancing affordance reasoning from single-step interactions to complex, sequential tasks at the scene level.

</details>


### [27] [SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions](https://arxiv.org/abs/2507.23784)
*Jessica Bader,Leander Girrbach,Stephan Alaniz,Zeynep Akata*

Main category: cs.CV

TL;DR: 论文提出了一种新的基准SUB，用于评估概念瓶颈模型（CBMs）在分布变化下的鲁棒性，并开发了Tied Diffusion Guidance（TDG）方法来精确控制生成图像。


<details>
  <summary>Details</summary>
Motivation: 尽管CBMs在提高AI透明度方面表现出色，但在分布变化下难以可靠识别正确概念，因此需要更严格的评估方法。

Method: 通过SUB基准（基于CUB数据集的38,400张合成图像）和TDG方法（通过共享噪声控制生成图像）来评估CBMs。

Result: SUB和TDG方法为CBMs的鲁棒性评估提供了新工具，推动了更稳健的模型发展。

Conclusion: SUB基准和TDG方法为概念模型的鲁棒性研究提供了重要支持，有助于开发更可靠的AI解释方法。

Abstract: Concept Bottleneck Models (CBMs) and other concept-based interpretable models show great promise for making AI applications more transparent, which is essential in fields like medicine. Despite their success, we demonstrate that CBMs struggle to reliably identify the correct concepts under distribution shifts. To assess the robustness of CBMs to concept variations, we introduce SUB: a fine-grained image and concept benchmark containing 38,400 synthetic images based on the CUB dataset. To create SUB, we select a CUB subset of 33 bird classes and 45 concepts to generate images which substitute a specific concept, such as wing color or belly pattern. We introduce a novel Tied Diffusion Guidance (TDG) method to precisely control generated images, where noise sharing for two parallel denoising processes ensures that both the correct bird class and the correct attribute are generated. This novel benchmark enables rigorous evaluation of CBMs and similar interpretable models, contributing to the development of more robust methods. Our code is available at https://github.com/ExplainableML/sub and the dataset at http://huggingface.co/datasets/Jessica-bader/SUB.

</details>


### [28] [Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis](https://arxiv.org/abs/2507.23785)
*Bowen Zhang,Sicheng Xu,Chuxin Wang,Jiaolong Yang,Feng Zhao,Dong Chen,Baining Guo*

Main category: cs.CV

TL;DR: 提出了一种从单视频输入生成高质量动态3D内容的新框架，通过直接编码高斯溅射（GS）及其时间变化，解决了4D扩散建模的高维挑战。


<details>
  <summary>Details</summary>
Motivation: 直接4D扩散建模因数据构建成本高和3D形状、外观及运动联合表示的高维性而极具挑战性。

Method: 引入Direct 4DMesh-to-GS Variation Field VAE，直接编码规范高斯溅射及其时间变化，无需逐实例拟合，并将高维动画压缩到紧凑潜在空间。基于此表示，训练了高斯变化场扩散模型。

Result: 模型在Objaverse数据集上训练，生成质量优于现有方法，并对野外视频输入表现出显著泛化能力。

Conclusion: 该框架为生成高质量动画3D内容开辟了新途径。

Abstract: In this paper, we present a novel framework for video-to-4D generation that creates high-quality dynamic 3D content from single video inputs. Direct 4D diffusion modeling is extremely challenging due to costly data construction and the high-dimensional nature of jointly representing 3D shape, appearance, and motion. We address these challenges by introducing a Direct 4DMesh-to-GS Variation Field VAE that directly encodes canonical Gaussian Splats (GS) and their temporal variations from 3D animation data without per-instance fitting, and compresses high-dimensional animations into a compact latent space. Building upon this efficient representation, we train a Gaussian Variation Field diffusion model with temporal-aware Diffusion Transformer conditioned on input videos and canonical GS. Trained on carefully-curated animatable 3D objects from the Objaverse dataset, our model demonstrates superior generation quality compared to existing methods. It also exhibits remarkable generalization to in-the-wild video inputs despite being trained exclusively on synthetic data, paving the way for generating high-quality animated 3D content. Project page: https://gvfdiffusion.github.io/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [29] [Investigating the Invertibility of Multimodal Latent Spaces: Limitations of Optimization-Based Methods](https://arxiv.org/abs/2507.23010)
*Siwoo Park*

Main category: cs.LG

TL;DR: 本文研究了多模态潜在空间在任务特定AI模型中的逆向能力和广泛用途，发现其逆向映射能力有限且语义不连贯。


<details>
  <summary>Details</summary>
Motivation: 探索多模态潜在空间在逆向任务中的潜力，填补现有研究空白。

Method: 提出基于优化的框架，双向应用于文本-图像和文本-音频模态，验证逆向映射的可行性。

Result: 实验表明，逆向映射虽能对齐目标输出，但感知质量差且语义不连贯。

Conclusion: 多模态潜在空间缺乏稳健逆向映射所需结构，需进一步研究开发语义丰富且可逆的空间。

Abstract: This paper investigates the inverse capabilities and broader utility of multimodal latent spaces within task-specific AI (Artificial Intelligence) models. While these models excel at their designed forward tasks (e.g., text-to-image generation, audio-to-text transcription), their potential for inverse mappings remains largely unexplored. We propose an optimization-based framework to infer input characteristics from desired outputs, applying it bidirectionally across Text-Image (BLIP, Flux.1-dev) and Text-Audio (Whisper-Large-V3, Chatterbox-TTS) modalities.   Our central hypothesis posits that while optimization can guide models towards inverse tasks, their multimodal latent spaces will not consistently support semantically meaningful and perceptually coherent inverse mappings. Experimental results consistently validate this hypothesis. We demonstrate that while optimization can force models to produce outputs that align textually with targets (e.g., a text-to-image model generating an image that an image captioning model describes correctly, or an ASR model transcribing optimized audio accurately), the perceptual quality of these inversions is chaotic and incoherent. Furthermore, when attempting to infer the original semantic input from generative models, the reconstructed latent space embeddings frequently lack semantic interpretability, aligning with nonsensical vocabulary tokens.   These findings highlight a critical limitation. multimodal latent spaces, primarily optimized for specific forward tasks, do not inherently possess the structure required for robust and interpretable inverse mappings. Our work underscores the need for further research into developing truly semantically rich and invertible multimodal latent spaces.

</details>


### [30] [DepMicroDiff: Diffusion-Based Dependency-Aware Multimodal Imputation for Microbiome Data](https://arxiv.org/abs/2507.23676)
*Rabeya Tus Sadia,Qiang Cheng*

Main category: cs.LG

TL;DR: DepMicroDiff是一种结合扩散模型和依赖感知Transformer的新框架，用于微生物组数据插补，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 微生物组数据的稀疏性和噪声对准确插补构成挑战，现有方法未能充分捕捉微生物间的复杂依赖关系和上下文元数据。

Method: DepMicroDiff结合扩散生成模型和依赖感知Transformer，利用VAE预训练和基于LLM的元数据编码。

Result: 在TCGA数据集上，DepMicroDiff在Pearson相关性、余弦相似度和误差指标上显著优于基线方法。

Conclusion: DepMicroDiff在微生物组数据插补中表现出鲁棒性和泛化能力，适用于多种癌症类型。

Abstract: Microbiome data analysis is essential for understanding host health and disease, yet its inherent sparsity and noise pose major challenges for accurate imputation, hindering downstream tasks such as biomarker discovery. Existing imputation methods, including recent diffusion-based models, often fail to capture the complex interdependencies between microbial taxa and overlook contextual metadata that can inform imputation. We introduce DepMicroDiff, a novel framework that combines diffusion-based generative modeling with a Dependency-Aware Transformer (DAT) to explicitly capture both mutual pairwise dependencies and autoregressive relationships. DepMicroDiff is further enhanced by VAE-based pretraining across diverse cancer datasets and conditioning on patient metadata encoded via a large language model (LLM). Experiments on TCGA microbiome datasets show that DepMicroDiff substantially outperforms state-of-the-art baselines, achieving higher Pearson correlation (up to 0.712), cosine similarity (up to 0.812), and lower RMSE and MAE across multiple cancer types, demonstrating its robustness and generalizability for microbiome imputation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [31] [GSFusion:Globally Optimized LiDAR-Inertial-Visual Mapping for Gaussian Splatting](https://arxiv.org/abs/2507.23273)
*Jaeseok Park,Chanoh Park,Minsu Kim,Soohwan Kim*

Main category: cs.RO

TL;DR: GSFusion是一种在线LiDAR-惯性-视觉映射系统，通过全局位姿图优化中的surfel-to-surfel约束，解决了3D高斯泼溅（3DGS）在稀疏数据和全局对齐方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统基于相机传感器（包括RGB-D）的3DGS方法存在计算负载高、在纹理或光照差的环境中失效、操作范围短等问题，而LiDAR虽为替代方案，但其与3DGS结合时面临全局对齐和稀疏数据优化的挑战。

Method: 提出GSFusion系统，采用surfel-to-surfel约束实现高精度地图一致性，并引入像素感知的高斯初始化策略和有界Sigmoid约束以处理稀疏数据。

Result: 在公开和自有数据集上的实验表明，GSFusion在渲染质量和地图构建效率上优于现有3DGS SLAM系统。

Conclusion: GSFusion通过创新的约束和初始化策略，成功解决了LiDAR与3DGS结合时的关键问题，提升了系统性能。

Abstract: While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic mapping, conventional approaches based on camera sensor, even RGB-D, suffer from fundamental limitations such as high computational load, failure in environments with poor texture or illumination, and short operational ranges. LiDAR emerges as a robust alternative, but its integration with 3DGS introduces new challenges, such as the need for exceptional global alignment for photorealistic quality and prolonged optimization times caused by sparse data. To address these challenges, we propose GSFusion, an online LiDAR-Inertial-Visual mapping system that ensures high-precision map consistency through a surfel-to-surfel constraint in the global pose-graph optimization. To handle sparse data, our system employs a pixel-aware Gaussian initialization strategy for efficient representation and a bounded sigmoid constraint to prevent uncontrolled Gaussian growth. Experiments on public and our datasets demonstrate our system outperforms existing 3DGS SLAM systems in terms of rendering quality and map-building efficiency.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [32] [Towards High-Resolution Alignment and Super-Resolution of Multi-Sensor Satellite Imagery](https://arxiv.org/abs/2507.23150)
*Philip Wootaek Shin,Vishal Gaur,Rahul Ramachandran,Manil Maskey,Jack Sampson,Vijaykrishnan Narayanan,Sujit Roy*

Main category: eess.IV

TL;DR: 论文提出了一种框架，用于对齐和融合30米分辨率的HLS影像与10米分辨率的HLS影像，以提升超分辨率效果。


<details>
  <summary>Details</summary>
Motivation: 解决不同卫星传感器间分辨率差异对数据融合和应用的挑战，现有方法依赖人工降尺度影像，不适用于异质传感器。

Method: 开发了一个初步框架，以HLS10为参考，对齐和融合HLS30影像。

Result: 定量和定性评估表明该方法有效，提升了超分辨率影像质量。

Conclusion: 研究为异质卫星影像超分辨率提供了可行性见解，并指出了未来改进的关键方向。

Abstract: High-resolution satellite imagery is essential for geospatial analysis, yet differences in spatial resolution across satellite sensors present challenges for data fusion and downstream applications. Super-resolution techniques can help bridge this gap, but existing methods rely on artificially downscaled images rather than real sensor data and are not well suited for heterogeneous satellite sensors with differing spectral, temporal characteristics. In this work, we develop a preliminary framework to align and Harmonized Landsat Sentinel 30m(HLS 30) imagery using Harmonized Landsat Sentinel 10m(HLS10) as a reference from the HLS dataset. Our approach aims to bridge the resolution gap between these sensors and improve the quality of super-resolved Landsat imagery. Quantitative and qualitative evaluations demonstrate the effectiveness of our method, showing its potential for enhancing satellite-based sensing applications. This study provides insights into the feasibility of heterogeneous satellite image super-resolution and highlights key considerations for future advancements in the field.

</details>


### [33] [LesionGen: A Concept-Guided Diffusion Model for Dermatology Image Synthesis](https://arxiv.org/abs/2507.23001)
*Jamil Fayyad,Nourhan Bayasi,Ziyang Yu,Homayoun Najjaran*

Main category: eess.IV

TL;DR: LesionGen是一种基于文本到图像扩散概率模型（T2I-DPMs）的皮肤病图像合成框架，通过高质量图像-描述对生成多样且真实的皮肤病图像，弥补了现有数据集的不足。


<details>
  <summary>Details</summary>
Motivation: 皮肤病分类的深度学习模型需要大量多样且标注良好的数据集，但现有数据集因隐私、标注成本高和人口代表性不足而受限。

Method: LesionGen利用专家标注和伪生成的概念丰富描述，训练预训练的扩散模型，生成基于皮肤病学描述的图像。

Result: 仅使用合成数据训练的模型分类准确率接近真实图像训练的模型，且在少数群体表现上有显著提升。

Conclusion: LesionGen为皮肤病图像合成提供了有效解决方案，弥补了数据稀缺问题。

Abstract: Deep learning models for skin disease classification require large, diverse, and well-annotated datasets. However, such resources are often limited due to privacy concerns, high annotation costs, and insufficient demographic representation. While text-to-image diffusion probabilistic models (T2I-DPMs) offer promise for medical data synthesis, their use in dermatology remains underexplored, largely due to the scarcity of rich textual descriptions in existing skin image datasets. In this work, we introduce LesionGen, a clinically informed T2I-DPM framework for dermatology image synthesis. Unlike prior methods that rely on simplistic disease labels, LesionGen is trained on structured, concept-rich dermatological captions derived from expert annotations and pseudo-generated, concept-guided reports. By fine-tuning a pretrained diffusion model on these high-quality image-caption pairs, we enable the generation of realistic and diverse skin lesion images conditioned on meaningful dermatological descriptions. Our results demonstrate that models trained solely on our synthetic dataset achieve classification accuracy comparable to those trained on real images, with notable gains in worst-case subgroup performance. Code and data are available here.

</details>
