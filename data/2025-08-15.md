<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 29]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MANGO: Multimodal Attention-based Normalizing Flow Approach to Fusion Learning](https://arxiv.org/abs/2508.10133)
*Thanh-Dat Truong,Christophe Bobda,Nitin Agarwal,Khoa Luu*

Main category: cs.CV

TL;DR: 论文提出了一种新型的多模态注意力归一化流（MANGO）方法，通过可逆交叉注意力层（ICA）和三种新的交叉注意力机制，显式、可解释地学习多模态数据的复杂相关性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态融合方法依赖Transformer的注意力机制隐式学习特征相关性，难以捕捉模态本质特征和复杂结构。

Method: 提出可逆交叉注意力层（ICA）和三种新交叉注意力机制（MMCA、IMCA、LICA），构建归一化流模型。

Result: 在语义分割、图像翻译和电影分类任务上达到最先进性能。

Conclusion: MANGO方法显式、可解释地学习多模态特征，性能优越。

Abstract: Multimodal learning has gained much success in recent years. However, current multimodal fusion methods adopt the attention mechanism of Transformers to implicitly learn the underlying correlation of multimodal features. As a result, the multimodal model cannot capture the essential features of each modality, making it difficult to comprehend complex structures and correlations of multimodal inputs. This paper introduces a novel Multimodal Attention-based Normalizing Flow (MANGO) approach\footnote{The source code of this work will be publicly available.} to developing explicit, interpretable, and tractable multimodal fusion learning. In particular, we propose a new Invertible Cross-Attention (ICA) layer to develop the Normalizing Flow-based Model for multimodal data. To efficiently capture the complex, underlying correlations in multimodal data in our proposed invertible cross-attention layer, we propose three new cross-attention mechanisms: Modality-to-Modality Cross-Attention (MMCA), Inter-Modality Cross-Attention (IMCA), and Learnable Inter-Modality Cross-Attention (LICA). Finally, we introduce a new Multimodal Attention-based Normalizing Flow to enable the scalability of our proposed method to high-dimensional multimodal data. Our experimental results on three different multimodal learning tasks, i.e., semantic segmentation, image-to-image translation, and movie genre classification, have illustrated the state-of-the-art (SoTA) performance of the proposed approach.

</details>


### [2] [EntropyGS: An Efficient Entropy Coding on 3D Gaussian Splatting](https://arxiv.org/abs/2508.10227)
*Yuning Huang,Jiahao Pang,Fengqing Zhu,Dong Tian*

Main category: cs.CV

TL;DR: 3D高斯泼溅（3DGS）是一种新兴的视图合成方法，具有快速训练/渲染和高质量视觉效果的优点。本文提出了一种名为EntropyGS的熵编码方法，用于压缩3DGS高斯属性，实现了30倍的码率降低，同时保持渲染质量。


<details>
  <summary>Details</summary>
Motivation: 3DGS的高斯创建和视图渲染任务通常需要在不同时间或设备上完成，因此高斯属性的存储、传输和压缩变得必要。本文通过分析高斯属性的统计特性，发现其符合特定分布规律，从而提出了一种高效的压缩方法。

Method: 首先对3DGS高斯属性进行相关性和统计分析，发现球谐AC属性符合拉普拉斯分布，而旋转、缩放和不透明度可由高斯混合模型近似。基于此，提出了一种因子化和参数化的熵编码方法EntropyGS，根据属性类型自适应量化并进行熵编码。

Result: EntropyGS在基准数据集上实现了约30倍的码率降低，同时保持与输入3DGS数据相似的渲染质量，且编码和解码速度快。

Conclusion: 本文通过统计分析和熵编码方法，有效解决了3DGS高斯属性的压缩问题，为3D视图合成的存储和传输提供了高效解决方案。

Abstract: As an emerging novel view synthesis approach, 3D Gaussian Splatting (3DGS) demonstrates fast training/rendering with superior visual quality. The two tasks of 3DGS, Gaussian creation and view rendering, are typically separated over time or devices, and thus storage/transmission and finally compression of 3DGS Gaussians become necessary. We begin with a correlation and statistical analysis of 3DGS Gaussian attributes. An inspiring finding in this work reveals that spherical harmonic AC attributes precisely follow Laplace distributions, while mixtures of Gaussian distributions can approximate rotation, scaling, and opacity. Additionally, harmonic AC attributes manifest weak correlations with other attributes except for inherited correlations from a color space. A factorized and parameterized entropy coding method, EntropyGS, is hereinafter proposed. During encoding, distribution parameters of each Gaussian attribute are estimated to assist their entropy coding. The quantization for entropy coding is adaptively performed according to Gaussian attribute types. EntropyGS demonstrates about 30x rate reduction on benchmark datasets while maintaining similar rendering quality compared to input 3DGS data, with a fast encoding and decoding time.

</details>


### [3] [High Fidelity Text to Image Generation with Contrastive Alignment and Structural Guidance](https://arxiv.org/abs/2508.10280)
*Danyi Gao*

Main category: cs.CV

TL;DR: 提出了一种结合文本-图像对比约束和结构引导机制的高保真图像生成方法，解决了现有方法在语义对齐和结构一致性上的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动图像生成方法在语义对齐准确性和结构一致性上存在性能瓶颈，需要改进。

Method: 集成文本-图像对比约束与结构引导机制，引入对比学习模块和结构先验（如语义布局图或边缘草图），通过多目标监督机制优化损失函数。

Result: 在COCO-2014数据集上验证，定量指标（CLIP Score、FID、SSIM）表现优越，有效平衡语义对齐与结构保真度。

Conclusion: 该方法在语义清晰和结构完整图像生成上表现优异，为联合文本-图像建模提供了可行技术路径。

Abstract: This paper addresses the performance bottlenecks of existing text-driven image generation methods in terms of semantic alignment accuracy and structural consistency. A high-fidelity image generation method is proposed by integrating text-image contrastive constraints with structural guidance mechanisms. The approach introduces a contrastive learning module that builds strong cross-modal alignment constraints to improve semantic matching between text and image. At the same time, structural priors such as semantic layout maps or edge sketches are used to guide the generator in spatial-level structural modeling. This enhances the layout completeness and detail fidelity of the generated images. Within the overall framework, the model jointly optimizes contrastive loss, structural consistency loss, and semantic preservation loss. A multi-objective supervision mechanism is adopted to improve the semantic consistency and controllability of the generated content. Systematic experiments are conducted on the COCO-2014 dataset. Sensitivity analyses are performed on embedding dimensions, text length, and structural guidance strength. Quantitative metrics confirm the superior performance of the proposed method in terms of CLIP Score, FID, and SSIM. The results show that the method effectively bridges the gap between semantic alignment and structural fidelity without increasing computational complexity. It demonstrates a strong ability to generate semantically clear and structurally complete images, offering a viable technical path for joint text-image modeling and image generation.

</details>


### [4] [AtomDiffuser: Time-Aware Degradation Modeling for Drift and Beam Damage in STEM Imaging](https://arxiv.org/abs/2508.10359)
*Hao Wang,Hongkui Zheng,Kai He,Abolfazl Razi*

Main category: cs.CV

TL;DR: AtomDiffuser是一个时间感知的降解建模框架，用于分离STEM数据中的样本漂移和辐射衰减，通过预测仿射变换和空间衰减图来解析原子结构动态。


<details>
  <summary>Details</summary>
Motivation: 解释时间分辨STEM数据时，空间漂移和辐射损伤导致的信号损失是两大挑战，现有方法难以明确分离这些效应或建模原子分辨率下的材料动态。

Method: 提出AtomDiffuser框架，通过预测仿射变换和空间衰减图来分离样本漂移和辐射衰减，利用降解作为物理启发的时间条件过程。

Result: AtomDiffuser在合成降解过程上训练，能很好地推广到真实世界的cryo-STEM数据，支持高分辨率降解推断和漂移对齐。

Conclusion: AtomDiffuser提供了解释性结构演变的工具，可用于可视化和量化与辐射诱导原子不稳定性相关的降解模式。

Abstract: Scanning transmission electron microscopy (STEM) plays a critical role in modern materials science, enabling direct imaging of atomic structures and their evolution under external interferences. However, interpreting time-resolved STEM data remains challenging due to two entangled degradation effects: spatial drift caused by mechanical and thermal instabilities, and beam-induced signal loss resulting from radiation damage. These factors distort both geometry and intensity in complex, temporally correlated ways, making it difficult for existing methods to explicitly separate their effects or model material dynamics at atomic resolution. In this work, we present AtomDiffuser, a time-aware degradation modeling framework that disentangles sample drift and radiometric attenuation by predicting an affine transformation and a spatially varying decay map between any two STEM frames. Unlike traditional denoising or registration pipelines, our method leverages degradation as a physically heuristic, temporally conditioned process, enabling interpretable structural evolutions across time. Trained on synthetic degradation processes, AtomDiffuser also generalizes well to real-world cryo-STEM data. It further supports high-resolution degradation inference and drift alignment, offering tools for visualizing and quantifying degradation patterns that correlate with radiation-induced atomic instabilities.

</details>


### [5] [Towards Spatially Consistent Image Generation: On Incorporating Intrinsic Scene Properties into Diffusion Models](https://arxiv.org/abs/2508.10382)
*Hyundo Lee,Suhyung Choi,Byoung-Tak Zhang,Inwoo Hwang*

Main category: cs.CV

TL;DR: 论文提出了一种通过共同生成图像及其对应的场景内在属性（如深度、分割图）的方法，以解决传统图像生成模型在空间一致性和布局上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统图像生成模型依赖图像-文本对或单独使用场景内在属性作为条件输入，导致生成的图像空间不一致和失真。本文利用场景内在属性提供丰富的底层信息，以生成更一致和真实的图像。

Method: 首先通过预训练估计器从大型图像数据集中提取场景内在属性，无需额外场景信息或显式3D表示；然后将多种内在属性聚合为单一潜在变量；基于预训练的潜在扩散模型（LDM），同时去噪图像和内在属性域，共享信息以保持一致性。

Result: 实验表明，该方法能纠正空间不一致性，生成更自然的场景布局，同时保持基础模型（如Stable Diffusion）的保真度和文本对齐性。

Conclusion: 通过共同生成图像和场景内在属性，模型能隐式捕捉底层场景结构，生成更一致和真实的图像。

Abstract: Image generation models trained on large datasets can synthesize high-quality images but often produce spatially inconsistent and distorted images due to limited information about the underlying structures and spatial layouts. In this work, we leverage intrinsic scene properties (e.g., depth, segmentation maps) that provide rich information about the underlying scene, unlike prior approaches that solely rely on image-text pairs or use intrinsics as conditional inputs. Our approach aims to co-generate both images and their corresponding intrinsics, enabling the model to implicitly capture the underlying scene structure and generate more spatially consistent and realistic images. Specifically, we first extract rich intrinsic scene properties from a large image dataset with pre-trained estimators, eliminating the need for additional scene information or explicit 3D representations. We then aggregate various intrinsic scene properties into a single latent variable using an autoencoder. Building upon pre-trained large-scale Latent Diffusion Models (LDMs), our method simultaneously denoises the image and intrinsic domains by carefully sharing mutual information so that the image and intrinsic reflect each other without degrading image quality. Experimental results demonstrate that our method corrects spatial inconsistencies and produces a more natural layout of scenes while maintaining the fidelity and textual alignment of the base model (e.g., Stable Diffusion).

</details>


### [6] [PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce Driver Distraction Detection](https://arxiv.org/abs/2508.10397)
*Haibin Sun,Xinghui Song*

Main category: cs.CV

TL;DR: 提出了一种基于姿态驱动的质量控制数据增强框架（PQ-DAF），通过视觉语言模型筛选样本，提升少样本驾驶员分心检测的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有模型在真实场景中泛化能力不足的问题，主要源于数据标注成本高和训练数据与目标场景的域偏移。

Method: 结合渐进条件扩散模型（PCDMs）捕捉关键姿态特征，并利用CogVLM视觉语言模型过滤低质量合成样本。

Result: PQ-DAF显著提升了少样本条件下的模型性能，增强了跨域泛化能力。

Conclusion: PQ-DAF为少样本驾驶员分心检测提供了一种高效且可靠的数据增强解决方案。

Abstract: Driver distraction detection is essential for improving traffic safety and reducing road accidents. However, existing models often suffer from degraded generalization when deployed in real-world scenarios. This limitation primarily arises from the few-shot learning challenge caused by the high cost of data annotation in practical environments, as well as the substantial domain shift between training datasets and target deployment conditions. To address these issues, we propose a Pose-driven Quality-controlled Data Augmentation Framework (PQ-DAF) that leverages a vision-language model for sample filtering to cost-effectively expand training data and enhance cross-domain robustness. Specifically, we employ a Progressive Conditional Diffusion Model (PCDMs) to accurately capture key driver pose features and synthesize diverse training examples. A sample quality assessment module, built upon the CogVLM vision-language model, is then introduced to filter out low-quality synthetic samples based on a confidence threshold, ensuring the reliability of the augmented dataset. Extensive experiments demonstrate that PQ-DAF substantially improves performance in few-shot driver distraction detection, achieving significant gains in model generalization under data-scarce conditions.

</details>


### [7] [NanoControl: A Lightweight Framework for Precise and Efficient Control in Diffusion Transformer](https://arxiv.org/abs/2508.10424)
*Shanyuan Liu,Jian Zhu,Junda Lu,Yue Gong,Liuzhuozheng Li,Bo Cheng,Yuhang Ma,Liebucha Wu,Xiaoyu Wu,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: NanoControl是一种基于Flux的高效可控文本到图像生成模型，通过LoRA风格控制模块和KV-Context增强机制，显著减少计算开销，同时保持高质量生成和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有基于ControlNet的方法在DiTs中引入过多参数和计算成本，需高效替代方案。

Method: 采用Flux作为主干网络，设计LoRA风格控制模块和KV-Context增强机制，直接学习控制信号并融合条件特征。

Result: 仅增加0.024%参数和0.029% GFLOPs，实现高效可控生成，性能优于传统方法。

Conclusion: NanoControl在减少计算开销的同时，提升了生成质量和可控性，为高效可控文本到图像生成提供了新思路。

Abstract: Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in text-to-image synthesis. However, in the domain of controllable text-to-image generation using DiTs, most existing methods still rely on the ControlNet paradigm originally designed for UNet-based diffusion models. This paradigm introduces significant parameter overhead and increased computational costs. To address these challenges, we propose the Nano Control Diffusion Transformer (NanoControl), which employs Flux as the backbone network. Our model achieves state-of-the-art controllable text-to-image generation performance while incurring only a 0.024\% increase in parameter count and a 0.029\% increase in GFLOPs, thus enabling highly efficient controllable generation. Specifically, rather than duplicating the DiT backbone for control, we design a LoRA-style (low-rank adaptation) control module that directly learns control signals from raw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation mechanism that integrates condition-specific key-value information into the backbone in a simple yet highly effective manner, facilitating deep fusion of conditional features. Extensive benchmark experiments demonstrate that NanoControl significantly reduces computational overhead compared to conventional control approaches, while maintaining superior generation quality and achieving improved controllability.

</details>


### [8] [CRISP: Contrastive Residual Injection and Semantic Prompting for Continual Video Instance Segmentation](https://arxiv.org/abs/2508.10432)
*Baichen Liu,Qi Lyu,Xudong Wang,Jiahua Dong,Lianqing Liu,Zhi Han*

Main category: cs.CV

TL;DR: CRISP方法通过对比残差注入和语义提示，解决了持续视频实例分割中的实例、类别和任务混淆问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决持续视频实例分割中吸收新类别和保留旧类别的挑战，同时保持时间一致性。

Method: 采用实例跟踪建模、自适应残差语义提示框架和对比学习语义一致性损失，以及简洁的增量提示初始化策略。

Result: 在YouTube-VIS数据集上显著优于现有方法，避免了灾难性遗忘，提升了分割和分类性能。

Conclusion: CRISP是一种有效的持续视频实例分割方法，具有广泛的应用潜力。

Abstract: Continual video instance segmentation demands both the plasticity to absorb new object categories and the stability to retain previously learned ones, all while preserving temporal consistency across frames. In this work, we introduce Contrastive Residual Injection and Semantic Prompting (CRISP), an earlier attempt tailored to address the instance-wise, category-wise, and task-wise confusion in continual video instance segmentation. For instance-wise learning, we model instance tracking and construct instance correlation loss, which emphasizes the correlation with the prior query space while strengthening the specificity of the current task query. For category-wise learning, we build an adaptive residual semantic prompt (ARSP) learning framework, which constructs a learnable semantic residual prompt pool generated by category text and uses an adjustive query-prompt matching mechanism to build a mapping relationship between the query of the current task and the semantic residual prompt. Meanwhile, a semantic consistency loss based on the contrastive learning is introduced to maintain semantic coherence between object queries and residual prompts during incremental training. For task-wise learning, to ensure the correlation at the inter-task level within the query space, we introduce a concise yet powerful initialization strategy for incremental prompts. Extensive experiments on YouTube-VIS-2019 and YouTube-VIS-2021 datasets demonstrate that CRISP significantly outperforms existing continual segmentation methods in the long-term continual video instance segmentation task, avoiding catastrophic forgetting and effectively improving segmentation and classification performance. The code is available at https://github.com/01upup10/CRISP.

</details>


### [9] [From Images to Perception: Emergence of Perceptual Properties by Reconstructing Images](https://arxiv.org/abs/2508.10450)
*Pablo Hernández-Cámara,Jesus Malo,Valero Laparra*

Main category: cs.CV

TL;DR: PerceptNet是一种受生物启发的架构，通过端到端优化完成图像重建任务，其编码器阶段与人类感知判断高度相关，表明视觉系统可能针对特定失真水平进行优化。


<details>
  <summary>Details</summary>
Motivation: 探索人类视觉感知是否源于图像统计，并验证生物启发模型能否无监督学习感知指标。

Method: 使用PerceptNet架构进行端到端优化，完成自编码、去噪、去模糊和稀疏正则化任务。

Result: 编码器阶段（V1-like层）与人类感知判断高度相关，且在适度噪声、模糊和稀疏性下表现最佳。

Conclusion: 视觉系统可能针对特定失真水平进行优化，生物启发模型可无监督学习感知指标。

Abstract: A number of scientists suggested that human visual perception may emerge from image statistics, shaping efficient neural representations in early vision. In this work, a bio-inspired architecture that can accommodate several known facts in the retina-V1 cortex, the PerceptNet, has been end-to-end optimized for different tasks related to image reconstruction: autoencoding, denoising, deblurring, and sparsity regularization. Our results show that the encoder stage (V1-like layer) consistently exhibits the highest correlation with human perceptual judgments on image distortion despite not using perceptual information in the initialization or training. This alignment exhibits an optimum for moderate noise, blur and sparsity. These findings suggest that the visual system may be tuned to remove those particular levels of distortion with that level of sparsity and that biologically inspired models can learn perceptual metrics without human supervision.

</details>


### [10] [Trajectory-aware Shifted State Space Models for Online Video Super-Resolution](https://arxiv.org/abs/2508.10453)
*Qiang Zhu,Xiandong Meng,Yuxian Jiang,Fan Zhang,David Bull,Shuyuan Zhu,Bing Zeng*

Main category: cs.CV

TL;DR: 提出了一种基于轨迹感知移位SSMs（TS-Mamba）的在线视频超分辨率方法，结合长程轨迹建模和低复杂度Mamba，实现高效时空信息聚合。


<details>
  <summary>Details</summary>
Motivation: 现有在线VSR方法仅使用相邻前一帧进行时间对齐，限制了长程时间建模能力。SSMs具有线性计算复杂度和全局感受野，能显著提升效率和性能。

Method: TS-Mamba通过视频轨迹选择相似token，利用轨迹感知移位Mamba聚合模块（TSMA）进行信息聚合，并设计了基于Hilbert扫描的移位SSMs块以增强空间连续性。

Result: 在三个VSR测试数据集上，TS-Mamba在多数情况下优于六个基准模型，计算复杂度降低22.7%。

Conclusion: TS-Mamba通过长程轨迹建模和低复杂度设计，实现了高效且高性能的在线视频超分辨率。

Abstract: Online video super-resolution (VSR) is an important technique for many real-world video processing applications, which aims to restore the current high-resolution video frame based on temporally previous frames. Most of the existing online VSR methods solely employ one neighboring previous frame to achieve temporal alignment, which limits long-range temporal modeling of videos. Recently, state space models (SSMs) have been proposed with linear computational complexity and a global receptive field, which significantly improve computational efficiency and performance. In this context, this paper presents a novel online VSR method based on Trajectory-aware Shifted SSMs (TS-Mamba), leveraging both long-term trajectory modeling and low-complexity Mamba to achieve efficient spatio-temporal information aggregation. Specifically, TS-Mamba first constructs the trajectories within a video to select the most similar tokens from the previous frames. Then, a Trajectory-aware Shifted Mamba Aggregation (TSMA) module consisting of proposed shifted SSMs blocks is employed to aggregate the selected tokens. The shifted SSMs blocks are designed based on Hilbert scannings and corresponding shift operations to compensate for scanning losses and strengthen the spatial continuity of Mamba. Additionally, we propose a trajectory-aware loss function to supervise the trajectory generation, ensuring the accuracy of token selection when training our model. Extensive experiments on three widely used VSR test datasets demonstrate that compared with six online VSR benchmark models, our TS-Mamba achieves state-of-the-art performance in most cases and over 22.7\% complexity reduction (in MACs). The source code for TS-Mamba will be available at https://github.com.

</details>


### [11] [TweezeEdit: Consistent and Efficient Image Editing with Path Regularization](https://arxiv.org/abs/2508.10498)
*Jianda Mao,Kaibo Wang,Yang Xiang,Kani Chen*

Main category: cs.CV

TL;DR: TweezeEdit是一种无需调优和反转的图像编辑框架，通过正则化整个去噪路径而非依赖反转锚点，实现了高效的语义保留和目标对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法在图像编辑中过度依赖目标提示而忽略源图像语义，且编辑路径长效率低。

Method: 提出TweezeEdit框架，通过梯度驱动正则化直接在一致性模型中注入目标提示语义，缩短编辑路径。

Result: 实验显示TweezeEdit在语义保留和目标对齐上优于现有方法，仅需12步（1.6秒/次编辑）。

Conclusion: TweezeEdit高效且适用于实时应用，解决了现有方法的局限性。

Abstract: Large-scale pre-trained diffusion models empower users to edit images through text guidance. However, existing methods often over-align with target prompts while inadequately preserving source image semantics. Such approaches generate target images explicitly or implicitly from the inversion noise of the source images, termed the inversion anchors. We identify this strategy as suboptimal for semantic preservation and inefficient due to elongated editing paths. We propose TweezeEdit, a tuning- and inversion-free framework for consistent and efficient image editing. Our method addresses these limitations by regularizing the entire denoising path rather than relying solely on the inversion anchors, ensuring source semantic retention and shortening editing paths. Guided by gradient-driven regularization, we efficiently inject target prompt semantics along a direct path using a consistency model. Extensive experiments demonstrate TweezeEdit's superior performance in semantic preservation and target alignment, outperforming existing methods. Remarkably, it requires only 12 steps (1.6 seconds per edit), underscoring its potential for real-time applications.

</details>


### [12] [Multi-Sample Anti-Aliasing and Constrained Optimization for 3D Gaussian Splatting](https://arxiv.org/abs/2508.10507)
*Zheng Zhou,Jia-Chen Zhang,Yu-Jie Xiong,Chun-Ming Xia*

Main category: cs.CV

TL;DR: 提出了一种结合多重采样抗锯齿（MSAA）和双重几何约束的优化框架，显著提升了3D高斯泼溅在细节重建和实时渲染中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅技术在优化过程中缺乏足够的几何约束，导致高频纹理和锐利不连续区域的细节模糊。

Method: 通过自适应混合四重子样本计算像素颜色，并引入两种几何约束：动态梯度分析的权重策略和边界梯度微分约束。

Result: 在多个基准测试中，该方法在细节保留和高频纹理重建方面达到最优性能，同时保持实时渲染效率。

Conclusion: 实验证明该方法在结构相似性和感知质量上显著优于基线方法，为3D高斯泼溅提供了更精细的优化方案。

Abstract: Recent advances in 3D Gaussian splatting have significantly improved real-time novel view synthesis, yet insufficient geometric constraints during scene optimization often result in blurred reconstructions of fine-grained details, particularly in regions with high-frequency textures and sharp discontinuities. To address this, we propose a comprehensive optimization framework integrating multisample anti-aliasing (MSAA) with dual geometric constraints. Our system computes pixel colors through adaptive blending of quadruple subsamples, effectively reducing aliasing artifacts in high-frequency components. The framework introduces two constraints: (a) an adaptive weighting strategy that prioritizes under-reconstructed regions through dynamic gradient analysis, and (b) gradient differential constraints enforcing geometric regularization at object boundaries. This targeted optimization enables the model to allocate computational resources preferentially to critical regions requiring refinement while maintaining global consistency. Extensive experimental evaluations across multiple benchmarks demonstrate that our method achieves state-of-the-art performance in detail preservation, particularly in preserving high-frequency textures and sharp discontinuities, while maintaining real-time rendering efficiency. Quantitative metrics and perceptual studies confirm statistically significant improvements over baseline approaches in both structural similarity (SSIM) and perceptual quality (LPIPS).

</details>


### [13] [A Segmentation-driven Editing Method for Bolt Defect Augmentation and Detection](https://arxiv.org/abs/2508.10509)
*Yangjie Xiao,Ke Zhang,Jiacun Wang,Xin Sheng,Yurong Guo,Meijuan Chen,Zehua Ren,Zhaoye Zheng,Zhenbing Zhao*

Main category: cs.CV

TL;DR: 提出了一种基于分割的螺栓缺陷编辑方法（SBDE），通过增强数据集解决缺陷图像稀缺和数据分布不平衡问题，显著提升了螺栓缺陷检测性能。


<details>
  <summary>Details</summary>
Motivation: 螺栓缺陷检测对输电线路安全至关重要，但缺陷图像稀缺和数据分布不平衡限制了检测性能。

Method: 1. 提出螺栓属性分割模型（Bolt-SAM），通过CLAHE-FFT适配器和多部分感知掩码解码器生成高质量掩码；2. 结合掩码优化模块（MOD）和图像修复模型（LaMa）构建缺陷属性编辑模型（MOD-LaMa）；3. 提出编辑恢复增强（ERA）策略，将编辑后的缺陷螺栓放回原始场景以扩展数据集。

Result: 实验表明，SBDE生成的缺陷图像优于现有图像编辑模型，并显著提升缺陷检测性能。

Conclusion: SBDE方法有效且具有应用潜力，代码已开源。

Abstract: Bolt defect detection is critical to ensure the safety of transmission lines. However, the scarcity of defect images and imbalanced data distributions significantly limit detection performance. To address this problem, we propose a segmentationdriven bolt defect editing method (SBDE) to augment the dataset. First, a bolt attribute segmentation model (Bolt-SAM) is proposed, which enhances the segmentation of complex bolt attributes through the CLAHE-FFT Adapter (CFA) and Multipart- Aware Mask Decoder (MAMD), generating high-quality masks for subsequent editing tasks. Second, a mask optimization module (MOD) is designed and integrated with the image inpainting model (LaMa) to construct the bolt defect attribute editing model (MOD-LaMa), which converts normal bolts into defective ones through attribute editing. Finally, an editing recovery augmentation (ERA) strategy is proposed to recover and put the edited defect bolts back into the original inspection scenes and expand the defect detection dataset. We constructed multiple bolt datasets and conducted extensive experiments. Experimental results demonstrate that the bolt defect images generated by SBDE significantly outperform state-of-the-art image editing models, and effectively improve the performance of bolt defect detection, which fully verifies the effectiveness and application potential of the proposed method. The code of the project is available at https://github.com/Jay-xyj/SBDE.

</details>


### [14] [HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head Synthesis](https://arxiv.org/abs/2508.10566)
*Shiyu Liu,Kui Jiang,Xianming Liu,Hongxun Yao,Xiaocheng Feng*

Main category: cs.CV

TL;DR: HM-Talker提出了一种结合隐式和显式运动特征的混合运动表示方法，通过跨模态解缠模块和混合运动建模模块，显著提升了说话头视频的生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前音频驱动的说话头视频生成方法因缺乏显式发音先验，常导致运动模糊和唇部抖动，HM-Talker旨在解决这一问题。

Method: HM-Talker结合隐式和显式运动特征，使用跨模态解缠模块提取互补特征，并通过混合运动建模模块动态合并特征以增强跨主体泛化能力。

Result: 实验表明HM-Talker在视觉质量和唇同步准确性上优于现有方法。

Conclusion: HM-Talker通过混合运动表示和身份无关学习，显著提升了说话头视频的生成效果。

Abstract: Audio-driven talking head video generation enhances user engagement in human-computer interaction. However, current methods frequently produce videos with motion blur and lip jitter, primarily due to their reliance on implicit modeling of audio-facial motion correlations--an approach lacking explicit articulatory priors (i.e., anatomical guidance for speech-related facial movements). To overcome this limitation, we propose HM-Talker, a novel framework for generating high-fidelity, temporally coherent talking heads. HM-Talker leverages a hybrid motion representation combining both implicit and explicit motion cues. Explicit cues use Action Units (AUs), anatomically defined facial muscle movements, alongside implicit features to minimize phoneme-viseme misalignment. Specifically, our Cross-Modal Disentanglement Module (CMDM) extracts complementary implicit/explicit motion features while predicting AUs directly from audio input aligned to visual cues. To mitigate identity-dependent biases in explicit features and enhance cross-subject generalization, we introduce the Hybrid Motion Modeling Module (HMMM). This module dynamically merges randomly paired implicit/explicit features, enforcing identity-agnostic learning. Together, these components enable robust lip synchronization across diverse identities, advancing personalized talking head synthesis. Extensive experiments demonstrate HM-Talker's superiority over state-of-the-art methods in visual quality and lip-sync accuracy.

</details>


### [15] [SpaRC-AD: A Baseline for Radar-Camera Fusion in End-to-End Autonomous Driving](https://arxiv.org/abs/2508.10567)
*Philipp Wolters,Johannes Gilg,Torben Teepe,Gerhard Rigoll*

Main category: cs.CV

TL;DR: SpaRC-AD是一种基于查询的端到端相机-雷达融合框架，用于规划导向的自动驾驶，解决了视觉方法在恶劣天气、部分遮挡和精确速度估计中的局限性。


<details>
  <summary>Details</summary>
Motivation: 视觉方法在恶劣天气、部分遮挡和精确速度估计方面存在局限性，而这些问题在安全敏感场景中至关重要。

Method: 通过稀疏3D特征对齐和基于多普勒的速度估计，优化了3D场景表示，改进了代理锚点、地图多段线和运动建模。

Result: 在多个自动驾驶任务中表现优异，包括3D检测（+4.8% mAP）、多目标跟踪（+8.3% AMOTA）、在线地图（+1.8% mAP）、运动预测（-4.0% mADE）和轨迹规划（-0.1m L2和-9% TPC）。

Conclusion: SpaRC-AD在安全关键场景中表现出色，证明了雷达融合在精确运动理解和长时程轨迹预测中的有效性。

Abstract: End-to-end autonomous driving systems promise stronger performance through unified optimization of perception, motion forecasting, and planning. However, vision-based approaches face fundamental limitations in adverse weather conditions, partial occlusions, and precise velocity estimation - critical challenges in safety-sensitive scenarios where accurate motion understanding and long-horizon trajectory prediction are essential for collision avoidance. To address these limitations, we propose SpaRC-AD, a query-based end-to-end camera-radar fusion framework for planning-oriented autonomous driving. Through sparse 3D feature alignment, and doppler-based velocity estimation, we achieve strong 3D scene representations for refinement of agent anchors, map polylines and motion modelling. Our method achieves strong improvements over the state-of-the-art vision-only baselines across multiple autonomous driving tasks, including 3D detection (+4.8% mAP), multi-object tracking (+8.3% AMOTA), online mapping (+1.8% mAP), motion prediction (-4.0% mADE), and trajectory planning (-0.1m L2 and -9% TPC). We achieve both spatial coherence and temporal consistency on multiple challenging benchmarks, including real-world open-loop nuScenes, long-horizon T-nuScenes, and closed-loop simulator Bench2Drive. We show the effectiveness of radar-based fusion in safety-critical scenarios where accurate motion understanding and long-horizon trajectory prediction are essential for collision avoidance. The source code of all experiments is available at https://phi-wol.github.io/sparcad/

</details>


### [16] [Fourier-Guided Attention Upsampling for Image Super-Resolution](https://arxiv.org/abs/2508.10616)
*Daejune Choi,Youchan No,Jinhyung Lee,Duksu Kim*

Main category: cs.CV

TL;DR: Frequency-Guided Attention (FGA) 是一种轻量级上采样模块，通过傅里叶特征、跨分辨率相关注意力层和频率域L1损失，显著提升单图像超分辨率性能。


<details>
  <summary>Details</summary>
Motivation: 传统上采样方法（如子像素卷积）效率高但难以重建高频细节并易引入混叠伪影，FGA旨在解决这些问题。

Method: FGA结合了基于傅里叶特征的多层感知器、跨分辨率相关注意力层和频率域L1损失，以提升高频细节重建和减少混叠。

Result: 实验显示FGA在五种超分辨率骨干网络上平均PSNR提升0.12~0.14 dB，频率域一致性提升29%，尤其在纹理丰富数据集上表现突出。

Conclusion: FGA是一种实用且可扩展的上采样替代方案，能有效减少混叠并保留细节。

Abstract: We propose Frequency-Guided Attention (FGA), a lightweight upsampling module for single image super-resolution. Conventional upsamplers, such as Sub-Pixel Convolution, are efficient but frequently fail to reconstruct high-frequency details and introduce aliasing artifacts. FGA addresses these issues by integrating (1) a Fourier feature-based Multi-Layer Perceptron (MLP) for positional frequency encoding, (2) a cross-resolution Correlation Attention Layer for adaptive spatial alignment, and (3) a frequency-domain L1 loss for spectral fidelity supervision. Adding merely 0.3M parameters, FGA consistently enhances performance across five diverse super-resolution backbones in both lightweight and full-capacity scenarios. Experimental results demonstrate average PSNR gains of 0.12~0.14 dB and improved frequency-domain consistency by up to 29%, particularly evident on texture-rich datasets. Visual and spectral evaluations confirm FGA's effectiveness in reducing aliasing and preserving fine details, establishing it as a practical, scalable alternative to traditional upsampling methods.

</details>


### [17] [ChatENV: An Interactive Vision-Language Model for Sensor-Guided Environmental Monitoring and Scenario Simulation](https://arxiv.org/abs/2508.10635)
*Hosam Elgendy,Ahmed Sharshar,Ahmed Aboeitta,Mohsen Guizani*

Main category: cs.CV

TL;DR: ChatENV是一个交互式视觉语言模型，结合卫星图像和传感器数据，用于环境监测和推理。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型忽视环境传感器的因果信号，依赖单一来源的标注，缺乏交互式场景推理能力。

Method: 构建包含177k图像和152k时间对的多样化数据集，利用GPT-4o和Gemini 2.0标注，并通过LoRA适配器微调Qwen-2.5-VL模型。

Result: ChatENV在时间推理和假设分析中表现优异（BERT-F1 0.903），优于现有时间模型。

Conclusion: ChatENV是一个强大的工具，支持基于传感器的环境监测和交互式分析。

Abstract: Understanding environmental changes from aerial imagery is vital for climate resilience, urban planning, and ecosystem monitoring. Yet, current vision language models (VLMs) overlook causal signals from environmental sensors, rely on single-source captions prone to stylistic bias, and lack interactive scenario-based reasoning. We present ChatENV, the first interactive VLM that jointly reasons over satellite image pairs and real-world sensor data. Our framework: (i) creates a 177k-image dataset forming 152k temporal pairs across 62 land-use classes in 197 countries with rich sensor metadata (e.g., temperature, PM10, CO); (ii) annotates data using GPT- 4o and Gemini 2.0 for stylistic and semantic diversity; and (iii) fine-tunes Qwen-2.5-VL using efficient Low-Rank Adaptation (LoRA) adapters for chat purposes. ChatENV achieves strong performance in temporal and "what-if" reasoning (e.g., BERT-F1 0.903) and rivals or outperforms state-of-the-art temporal models, while supporting interactive scenario-based analysis. This positions ChatENV as a powerful tool for grounded, sensor-aware environmental monitoring.

</details>


### [18] [Hybrid Generative Fusion for Efficient and Privacy-Preserving Face Recognition Dataset Generation](https://arxiv.org/abs/2508.10672)
*Feiran Li,Qianqian Xu,Shilong Bao,Boyu Han,Zhiyong Yang,Qingming Huang*

Main category: cs.CV

TL;DR: 本文提出了一种构建高质量人脸数据集的方法，通过清理基线数据集、生成合成身份并采用课程学习策略，最终在竞赛中取得第一名。


<details>
  <summary>Details</summary>
Motivation: 解决构建无重叠身份的高质量人脸数据集的挑战，以提升人脸识别模型的性能。

Method: 结合Mixture-of-Experts策略清理数据，使用Stable Diffusion生成合成身份，并通过Vec2Face扩展数据，采用课程学习策略训练模型。

Result: 构建的数据集在竞赛中排名第一，并在不同规模的身份测试中显著提升模型性能。

Conclusion: 混合真实与合成数据的策略有效构建了高质量数据集，显著提升了人脸识别模型的性能。

Abstract: In this paper, we present our approach to the DataCV ICCV Challenge, which centers on building a high-quality face dataset to train a face recognition model. The constructed dataset must not contain identities overlapping with any existing public face datasets. To handle this challenge, we begin with a thorough cleaning of the baseline HSFace dataset, identifying and removing mislabeled or inconsistent identities through a Mixture-of-Experts (MoE) strategy combining face embedding clustering and GPT-4o-assisted verification. We retain the largest consistent identity cluster and apply data augmentation up to a fixed number of images per identity. To further diversify the dataset, we generate synthetic identities using Stable Diffusion with prompt engineering. As diffusion models are computationally intensive, we generate only one reference image per identity and efficiently expand it using Vec2Face, which rapidly produces 49 identity-consistent variants. This hybrid approach fuses GAN-based and diffusion-based samples, enabling efficient construction of a diverse and high-quality dataset. To address the high visual similarity among synthetic identities, we adopt a curriculum learning strategy by placing them early in the training schedule, allowing the model to progress from easier to harder samples. Our final dataset contains 50 images per identity, and all newly generated identities are checked with mainstream face datasets to ensure no identity leakage. Our method achieves \textbf{1st place} in the competition, and experimental results show that our dataset improves model performance across 10K, 20K, and 100K identity scales. Code is available at https://github.com/Ferry-Li/datacv_fr.

</details>


### [19] [Physics-Informed Joint Multi-TE Super-Resolution with Implicit Neural Representation for Robust Fetal T2 Mapping](https://arxiv.org/abs/2508.10680)
*Busra Bulut,Maik Dannecker,Thomas Sanchez,Sara Neves Silva,Vladyslav Zalevskyi,Steven Jia,Jean-Baptiste Ledoux,Guillaume Auzias,François Rousseau,Jana Hutter,Daniel Rueckert,Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: 提出了一种联合重建跨TE数据的方法，用于胎儿脑MRI中的T2映射，解决了运动伪影问题，并在0.55T下首次实现了胎儿T2映射。


<details>
  <summary>Details</summary>
Motivation: 胎儿脑MRI中T2映射有助于更好地表征发育中的大脑，但现有方法因多次采集和运动伪影导致扫描时间长且不稳定。

Method: 结合隐式神经表示和物理知识正则化，建模T2衰减，实现跨TE信息共享，同时保持解剖和定量T2保真度。

Result: 在模拟胎儿脑和成人数据集上表现优异，首次在0.55T下实现胎儿T2映射，并展示了减少每TE堆栈数量的潜力。

Conclusion: 该方法显著提升了胎儿T2映射的效率和准确性，为临床提供了新工具。

Abstract: T2 mapping in fetal brain MRI has the potential to improve characterization of the developing brain, especially at mid-field (0.55T), where T2 decay is slower. However, this is challenging as fetal MRI acquisition relies on multiple motion-corrupted stacks of thick slices, requiring slice-to-volume reconstruction (SVR) to estimate a high-resolution (HR) 3D volume. Currently, T2 mapping involves repeated acquisitions of these stacks at each echo time (TE), leading to long scan times and high sensitivity to motion. We tackle this challenge with a method that jointly reconstructs data across TEs, addressing severe motion. Our approach combines implicit neural representations with a physics-informed regularization that models T2 decay, enabling information sharing across TEs while preserving anatomical and quantitative T2 fidelity. We demonstrate state-of-the-art performance on simulated fetal brain and in vivo adult datasets with fetal-like motion. We also present the first in vivo fetal T2 mapping results at 0.55T. Our study shows potential for reducing the number of stacks per TE in T2 mapping by leveraging anatomical redundancy.

</details>


### [20] [Novel View Synthesis using DDIM Inversion](https://arxiv.org/abs/2508.10688)
*Sehajdeep SIngh,A V Subramanyam*

Main category: cs.CV

TL;DR: 提出了一种轻量级视图转换框架TUNet，利用预训练扩散模型的高保真生成能力，通过融合策略解决模糊重建问题，并在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单张图像合成新视图任务中，现有方法需微调大型扩散模型或从头训练，成本高且重建模糊、泛化能力差。

Method: 基于DDIM反演潜在空间，使用相机姿态条件转换U-Net（TUNet）预测目标视图潜在，提出融合策略保留细节，并利用预训练扩散模型生成新视图。

Result: 在MVImgNet数据集上，方法表现优于现有方法。

Conclusion: 提出的轻量级框架有效解决了模糊重建问题，并利用预训练模型实现了高质量新视图合成。

Abstract: Synthesizing novel views from a single input image is a challenging task. It requires extrapolating the 3D structure of a scene while inferring details in occluded regions, and maintaining geometric consistency across viewpoints. Many existing methods must fine-tune large diffusion backbones using multiple views or train a diffusion model from scratch, which is extremely expensive. Additionally, they suffer from blurry reconstruction and poor generalization. This gap presents the opportunity to explore an explicit lightweight view translation framework that can directly utilize the high-fidelity generative capabilities of a pretrained diffusion model while reconstructing a scene from a novel view. Given the DDIM-inverted latent of a single input image, we employ a camera pose-conditioned translation U-Net, TUNet, to predict the inverted latent corresponding to the desired target view. However, the image sampled using the predicted latent may result in a blurry reconstruction. To this end, we propose a novel fusion strategy that exploits the inherent noise correlation structure observed in DDIM inversion. The proposed fusion strategy helps preserve the texture and fine-grained details. To synthesize the novel view, we use the fused latent as the initial condition for DDIM sampling, leveraging the generative prior of the pretrained diffusion model. Extensive experiments on MVImgNet demonstrate that our method outperforms existing methods.

</details>


### [21] [Beyond conventional vision: RGB-event fusion for robust object detection in dynamic traffic scenarios](https://arxiv.org/abs/2508.10704)
*Zhanwen Liu,Yujing Sun,Yang Wang,Nan Yang,Shengbo Eben Li,Xiangmo Zhao*

Main category: cs.CV

TL;DR: 论文提出了一种结合事件相机和RGB相机的方法（MCFNet），通过动态范围信息和运动线索融合，提升复杂交通环境下的目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统RGB相机在动态范围上的限制导致复杂交通环境（如夜间驾驶、隧道）中的全局对比度降低和高频细节丢失，影响目标检测性能。

Method: 提出MCFNet，包括事件校正模块（ECM）进行时间对齐，事件动态上采样模块（EDUM）提升空间分辨率，以及跨模态融合模块（CMM）实现自适应特征融合。

Result: 在DSEC-Det和PKU-DAVIS-SOD数据集上，MCFNet显著优于现有方法，尤其在DSEC-Det上mAP50提升7.4%，mAP提升1.7%。

Conclusion: MCFNet通过结合事件相机和RGB相机，有效解决了复杂光照条件下的目标检测问题，性能显著提升。

Abstract: The dynamic range limitation of conventional RGB cameras reduces global contrast and causes loss of high-frequency details such as textures and edges in complex traffic environments (e.g., nighttime driving, tunnels), hindering discriminative feature extraction and degrading frame-based object detection. To address this, we integrate a bio-inspired event camera with an RGB camera to provide high dynamic range information and propose a motion cue fusion network (MCFNet), which achieves optimal spatiotemporal alignment and adaptive cross-modal feature fusion under challenging lighting. Specifically, an event correction module (ECM) temporally aligns asynchronous event streams with image frames via optical-flow-based warping, jointly optimized with the detection network to learn task-aware event representations. The event dynamic upsampling module (EDUM) enhances spatial resolution of event frames to match image structures, ensuring precise spatiotemporal alignment. The cross-modal mamba fusion module (CMM) uses adaptive feature fusion with a novel interlaced scanning mechanism, effectively integrating complementary information for robust detection. Experiments conducted on the DSEC-Det and PKU-DAVIS-SOD datasets demonstrate that MCFNet significantly outperforms existing methods in various poor lighting and fast moving traffic scenarios. Notably, on the DSEC-Det dataset, MCFNet achieves a remarkable improvement, surpassing the best existing methods by 7.4% in mAP50 and 1.7% in mAP metrics, respectively. The code is available at https://github.com/Charm11492/MCFNet.

</details>


### [22] [CountCluster: Training-Free Object Quantity Guidance with Cross-Attention Map Clustering for Text-to-Image Generation](https://arxiv.org/abs/2508.10710)
*Joohyeon Lee,Jin-Seop Lee,Jee-Hyong Lee*

Main category: cs.CV

TL;DR: 论文提出CountCluster方法，通过优化对象交叉注意力图的分簇，提升文本到图像生成中对象数量的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在生成图像时难以准确反映输入提示中的对象数量，且现有方法依赖外部工具或训练，效果有限。

Method: CountCluster在推理时根据注意力分数将对象交叉注意力图分簇，优化潜在空间以匹配目标分布。

Result: 方法在对象数量准确性上平均提升18.5%，优于现有方法。

Conclusion: CountCluster无需外部工具或额外训练，显著提升了文本到图像生成中对象数量的控制能力。

Abstract: Diffusion-based text-to-image generation models have demonstrated strong performance in terms of image quality and diversity. However, they still struggle to generate images that accurately reflect the number of objects specified in the input prompt. Several approaches have been proposed that rely on either external counting modules for iterative refinement or quantity representations derived from learned tokens or latent features. However, they still have limitations in accurately reflecting the specified number of objects and overlook an important structural characteristic--The number of object instances in the generated image is largely determined in the early timesteps of the denoising process. To correctly reflect the object quantity for image generation, the highly activated regions in the object cross-attention map at the early timesteps should match the input object quantity, while each region should be clearly separated. To address this issue, we propose \textit{CountCluster}, a method that guides the object cross-attention map to be clustered according to the specified object count in the input, without relying on any external tools or additional training. The proposed method partitions the object cross-attention map into $k$ clusters at inference time based on attention scores, defines an ideal distribution in which each cluster is spatially well-separated, and optimizes the latent to align with this target distribution. Our method achieves an average improvement of 18.5\%p in object count accuracy compared to existing methods, and demonstrates superior quantity control performance across a variety of prompts. Code will be released at: https://github.com/JoohyeonL22/CountCluster .

</details>


### [23] [NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale](https://arxiv.org/abs/2508.10711)
*NextStep Team,Chunrui Han,Guopeng Li,Jingwei Wu,Quan Sun,Yan Cai,Yuang Peng,Zheng Ge,Deyu Zhou,Haomiao Tang,Hongyu Zhou,Kenkun Liu,Ailin Huang,Bin Wang,Changxin Miao,Deshan Sun,En Yu,Fukun Yin,Gang Yu,Hao Nie,Haoran Lv,Hanpeng Hu,Jia Wang,Jian Zhou,Jianjian Sun,Kaijun Tan,Kang An,Kangheng Lin,Liang Zhao,Mei Chen,Peng Xing,Rui Wang,Shiyu Liu,Shutao Xia,Tianhao You,Wei Ji,Xianfang Zeng,Xin Han,Xuelin Zhang,Yana Wei,Yanming Xu,Yimin Jiang,Yingming Wang,Yu Zhou,Yucheng Han,Ziyang Meng,Binxing Jiao,Daxin Jiang,Xiangyu Zhang,Yibo Zhu*

Main category: cs.CV

TL;DR: NextStep-1是一种14B自回归模型，结合157M流匹配头，通过离散文本和连续图像令牌的下一令牌预测目标，在文本到图像生成任务中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有自回归模型在文本到图像生成中依赖计算密集型扩散模型或量化损失的问题。

Method: 采用14B自回归模型和157M流匹配头，训练离散文本和连续图像令牌的下一令牌预测目标。

Result: 在文本到图像生成任务中表现优异，支持高保真图像合成和图像编辑。

Conclusion: NextStep-1展示了自回归模型的强大能力，并计划开源代码和模型。

Abstract: Prevailing autoregressive (AR) models for text-to-image generation either rely on heavy, computationally-intensive diffusion models to process continuous image tokens, or employ vector quantization (VQ) to obtain discrete tokens with quantization loss. In this paper, we push the autoregressive paradigm forward with NextStep-1, a 14B autoregressive model paired with a 157M flow matching head, training on discrete text tokens and continuous image tokens with next-token prediction objectives. NextStep-1 achieves state-of-the-art performance for autoregressive models in text-to-image generation tasks, exhibiting strong capabilities in high-fidelity image synthesis. Furthermore, our method shows strong performance in image editing, highlighting the power and versatility of our unified approach. To facilitate open research, we will release our code and models to the community.

</details>


### [24] [Exploiting Discriminative Codebook Prior for Autoregressive Image Generation](https://arxiv.org/abs/2508.10719)
*Longxiang Tang,Ruihang Chu,Xiang Wang,Yujin Han,Pingyu Wu,Chunming He,Yingya Zhang,Shiwei Zhang,Jiaya Jia*

Main category: cs.CV

TL;DR: 论文提出了一种名为DCPE的方法，替代k-means聚类，更有效地利用代码本中的令牌相似性信息，提升自回归模型的训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归图像生成系统未充分利用代码本中的令牌相似性信息，而k-means聚类在代码本特征空间中表现不佳。

Method: 提出DCPE方法，采用基于实例的距离替代基于质心的距离，并使用凝聚合并技术解决令牌空间差异问题。

Result: DCPE将LlamaGen-B的训练速度提升42%，并改善了FID和IS性能。

Conclusion: DCPE是一种即插即用的方法，能有效提取和利用代码本中的判别性先验信息。

Abstract: Advanced discrete token-based autoregressive image generation systems first tokenize images into sequences of token indices with a codebook, and then model these sequences in an autoregressive paradigm. While autoregressive generative models are trained only on index values, the prior encoded in the codebook, which contains rich token similarity information, is not exploited. Recent studies have attempted to incorporate this prior by performing naive k-means clustering on the tokens, helping to facilitate the training of generative models with a reduced codebook. However, we reveal that k-means clustering performs poorly in the codebook feature space due to inherent issues, including token space disparity and centroid distance inaccuracy. In this work, we propose the Discriminative Codebook Prior Extractor (DCPE) as an alternative to k-means clustering for more effectively mining and utilizing the token similarity information embedded in the codebook. DCPE replaces the commonly used centroid-based distance, which is found to be unsuitable and inaccurate for the token feature space, with a more reasonable instance-based distance. Using an agglomerative merging technique, it further addresses the token space disparity issue by avoiding splitting high-density regions and aggregating low-density ones. Extensive experiments demonstrate that DCPE is plug-and-play and integrates seamlessly with existing codebook prior-based paradigms. With the discriminative prior extracted, DCPE accelerates the training of autoregressive models by 42% on LlamaGen-B and improves final FID and IS performance.

</details>


### [25] [Video-BLADE: Block-Sparse Attention Meets Step Distillation for Efficient Video Generation](https://arxiv.org/abs/2508.10774)
*Youping Gu,Xiaolong Li,Yuhao Hu,Bohan Zhuang*

Main category: cs.CV

TL;DR: BLADE提出了一种数据无关的联合训练框架，通过自适应块稀疏注意力和稀疏感知的步蒸馏，显著加速扩散变换器的视频生成，同时提升质量。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器在高质量视频生成中表现优异，但其迭代去噪过程缓慢且长序列的二次注意力成本高，导致推理瓶颈。

Method: BLADE结合自适应块稀疏注意力（ASA）和基于轨迹分布匹配（TDM）的稀疏感知步蒸馏，动态生成内容感知的稀疏掩码，并将稀疏性直接融入蒸馏过程。

Result: 在CogVideoX-5B和Wan2.1-1.3B上，BLADE分别实现8.89x和14.10x的加速，VBench-2.0分数显著提升。

Conclusion: BLADE在加速视频生成的同时提高了质量，为扩散变换器的实际应用提供了高效解决方案。

Abstract: Diffusion transformers currently lead the field in high-quality video generation, but their slow iterative denoising process and prohibitive quadratic attention costs for long sequences create significant inference bottlenecks. While both step distillation and sparse attention mechanisms have shown promise as independent acceleration strategies, effectively combining these approaches presents critical challenges -- training-free integration yields suboptimal results, while separately training sparse attention after step distillation requires prohibitively expensive high-quality video data. To overcome these limitations, we propose BLADE, an innovative data-free joint training framework that introduces: (1) an Adaptive Block-Sparse Attention (ASA) mechanism for dynamically generating content-aware sparsity masks to focus computation on salient spatiotemporal features, and (2) a sparsity-aware step distillation paradigm built upon Trajectory Distribution Matching (TDM) that directly incorporates sparsity into the distillation process rather than treating it as a separate compression step, with fast convergence. We validate BLADE on text-to-video models like CogVideoX-5B and Wan2.1-1.3B. Our framework demonstrates remarkable efficiency gains across different scales. On Wan2.1-1.3B, BLADE achieves a 14.10x end-to-end inference acceleration over a 50-step baseline. Moreover, on models such as CogVideoX-5B with short video sequence lengths, our framework delivers a robust 8.89x speedup. Crucially, the acceleration is accompanied by a consistent quality improvement. On the VBench-2.0 benchmark, BLADE boosts the score of CogVideoX-5B to 0.569 (from 0.534) and Wan2.1-1.3B to 0.570 (from 0.563), results that are further corroborated by superior ratings in human evaluations. Our code and model weights are publicly available at: http://ziplab.co/BLADE-Homepage/.

</details>


### [26] [Ultra-High-Definition Reference-Based Landmark Image Super-Resolution with Generative Diffusion Prior](https://arxiv.org/abs/2508.10779)
*Zhenning Shi,Zizheng Yan,Yuhang Yu,Clara Xue,Jingyu Zhuang,Qi Zhang,Jinwei Chen,Tao Li,Qingnan Fan*

Main category: cs.CV

TL;DR: TriFlowSR是一种新的框架，通过显式模式匹配解决LR图像与参考HR图像对齐问题，并引入首个UHD地标场景RefSR数据集Landmark-4K。


<details>
  <summary>Details</summary>
Motivation: 现有基于ControlNet的RefSR方法难以有效对齐LR与参考HR图像信息，且数据集分辨率低、质量差，导致参考图像缺乏细节。

Method: 提出TriFlowSR框架，设计参考匹配策略，并构建Landmark-4K数据集，支持UHD地标场景下的高质量恢复。

Result: 实验表明，TriFlowSR能更有效地利用参考HR图像的语义和纹理信息，优于现有方法。

Conclusion: TriFlowSR是首个针对真实世界退化UHD地标场景的基于扩散的RefSR方法，代码和模型将开源。

Abstract: Reference-based Image Super-Resolution (RefSR) aims to restore a low-resolution (LR) image by utilizing the semantic and texture information from an additional reference high-resolution (reference HR) image. Existing diffusion-based RefSR methods are typically built upon ControlNet, which struggles to effectively align the information between the LR image and the reference HR image. Moreover, current RefSR datasets suffer from limited resolution and poor image quality, resulting in the reference images lacking sufficient fine-grained details to support high-quality restoration. To overcome the limitations above, we propose TriFlowSR, a novel framework that explicitly achieves pattern matching between the LR image and the reference HR image. Meanwhile, we introduce Landmark-4K, the first RefSR dataset for Ultra-High-Definition (UHD) landmark scenarios. Considering the UHD scenarios with real-world degradation, in TriFlowSR, we design a Reference Matching Strategy to effectively match the LR image with the reference HR image. Experimental results show that our approach can better utilize the semantic and texture information of the reference HR image compared to previous methods. To the best of our knowledge, we propose the first diffusion-based RefSR pipeline for ultra-high definition landmark scenarios under real-world degradation. Our code and model will be available at https://github.com/nkicsl/TriFlowSR.

</details>


### [27] [Object Fidelity Diffusion for Remote Sensing Image Generation](https://arxiv.org/abs/2508.10801)
*Ziqi Ye,Shuran Ma,Jie Yang,Xiaoyi Yang,Ziyang Gong,Xue Yang,Haipeng Wang*

Main category: cs.CV

TL;DR: OF-Diff提出了一种高保真遥感图像生成方法，通过提取对象先验形状和双分支扩散模型提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在遥感图像生成中保真度不足，影响目标检测模型的鲁棒性和可靠性。

Method: 提出OF-Diff，首次基于布局提取对象先验形状，并引入双分支扩散模型和扩散一致性损失，结合DDPO优化生成多样性。

Result: OF-Diff在多项关键指标上超越现有方法，尤其对多态和小目标类提升显著（如飞机、船只、车辆的mAP分别提高8.3%、7.7%、4.0%）。

Conclusion: OF-Diff显著提升了遥感图像生成的保真度和多样性，为高精度目标检测提供了更可靠的数据支持。

Abstract: High-precision controllable remote sensing image generation is both meaningful and challenging. Existing diffusion models often produce low-fidelity images due to their inability to adequately capture morphological details, which may affect the robustness and reliability of object detection models. To enhance the accuracy and fidelity of generated objects in remote sensing, this paper proposes Object Fidelity Diffusion (OF-Diff), which effectively improves the fidelity of generated objects. Specifically, we are the first to extract the prior shapes of objects based on the layout for diffusion models in remote sensing. Then, we introduce a dual-branch diffusion model with diffusion consistency loss, which can generate high-fidelity remote sensing images without providing real images during the sampling phase. Furthermore, we introduce DDPO to fine-tune the diffusion process, making the generated remote sensing images more diverse and semantically consistent. Comprehensive experiments demonstrate that OF-Diff outperforms state-of-the-art methods in the remote sensing across key quality metrics. Notably, the performance of several polymorphic and small object classes shows significant improvement. For instance, the mAP increases by 8.3%, 7.7%, and 4.0% for airplanes, ships, and vehicles, respectively.

</details>


### [28] [Hierarchical Fine-grained Preference Optimization for Physically Plausible Video Generation](https://arxiv.org/abs/2508.10858)
*Harold Haodong Chen,Haojian Huang,Qifeng Chen,Harry Yang,Ser-Nam Lim*

Main category: cs.CV

TL;DR: PhysHPO提出了一种分层跨模态直接偏好优化框架，通过四个层次的对齐提升物理合理性视频生成的质量，并引入自动数据选择流程。


<details>
  <summary>Details</summary>
Motivation: 解决视频生成中物理合理性不足的问题，提升真实感和准确性。

Method: 分层跨模态直接偏好优化（PhysHPO），包括实例、状态、运动和语义四个层次的对齐，并利用自动数据选择流程。

Result: 实验表明PhysHPO显著提升了物理合理性和视频生成质量。

Conclusion: PhysHPO是首个探索细粒度偏好对齐和数据选择的工作，为更真实的视频生成开辟了新方向。

Abstract: Recent advancements in video generation have enabled the creation of high-quality, visually compelling videos. However, generating videos that adhere to the laws of physics remains a critical challenge for applications requiring realism and accuracy. In this work, we propose PhysHPO, a novel framework for Hierarchical Cross-Modal Direct Preference Optimization, to tackle this challenge by enabling fine-grained preference alignment for physically plausible video generation. PhysHPO optimizes video alignment across four hierarchical granularities: a) Instance Level, aligning the overall video content with the input prompt; b) State Level, ensuring temporal consistency using boundary frames as anchors; c) Motion Level, modeling motion trajectories for realistic dynamics; and d) Semantic Level, maintaining logical consistency between narrative and visuals. Recognizing that real-world videos are the best reflections of physical phenomena, we further introduce an automated data selection pipeline to efficiently identify and utilize "good data" from existing large-scale text-video datasets, thereby eliminating the need for costly and time-intensive dataset construction. Extensive experiments on both physics-focused and general capability benchmarks demonstrate that PhysHPO significantly improves physical plausibility and overall video generation quality of advanced models. To the best of our knowledge, this is the first work to explore fine-grained preference alignment and data selection for video generation, paving the way for more realistic and human-preferred video generation paradigms.

</details>


### [29] [TexVerse: A Universe of 3D Objects with High-Resolution Textures](https://arxiv.org/abs/2508.10868)
*Yibo Zhang,Li Zhang,Rui Ma,Nan Cao*

Main category: cs.CV

TL;DR: TexVerse是一个大规模高分辨率纹理3D数据集，填补了现有数据集的空白，包含超过858K独特模型和1.6M实例，支持纹理合成、PBR材料开发等应用。


<details>
  <summary>Details</summary>
Motivation: 当前大规模3D数据集主要关注高分辨率几何生成，而高分辨率纹理生成缺乏合适的数据集，TexVerse旨在解决这一问题。

Method: TexVerse从Sketchfab收集了858K高分辨率3D模型，包括158K PBR材质模型，并提供骨架和动画子集（TexVerse-Skeleton和TexVerse-Animation）。

Result: 数据集包含1.6M实例，并附带详细注释，支持纹理合成、PBR材料开发、动画等多种3D任务。

Conclusion: TexVerse为高分辨率纹理生成和相关3D任务提供了高质量的数据资源。

Abstract: We introduce TexVerse, a large-scale 3D dataset featuring high-resolution textures. While recent advances in large-scale 3D datasets have enhanced high-resolution geometry generation, creating high-resolution textures end-to-end remains underexplored due to the lack of suitable datasets. TexVerse fills this gap with a curated collection of over 858K unique high-resolution 3D models sourced from Sketchfab, including more than 158K models with physically based rendering (PBR) materials. Each model encompasses all of its high-resolution variants, bringing the total to 1.6M 3D instances. TexVerse also includes specialized subsets: TexVerse-Skeleton, with 69K rigged models, and TexVerse-Animation, with 54K animated models, both preserving original skeleton and animation data uploaded by the user. We also provide detailed model annotations describing overall characteristics, structural components, and intricate features. TexVerse offers a high-quality data resource with wide-ranging potential applications in texture synthesis, PBR material development, animation, and various 3D vision and graphics tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [30] [Geospatial Diffusion for Land Cover Imperviousness Change Forecasting](https://arxiv.org/abs/2508.10649)
*Debvrat Varshney,Vibhas Vats,Bhartendu Pandey,Christa Brelsford,Philipe Dias*

Main category: cs.LG

TL;DR: 本文提出了一种利用生成式AI（GenAI）预测土地覆盖变化的新方法，通过将LULC预测视为基于历史和辅助数据的数据合成问题，展示了其可行性。


<details>
  <summary>Details</summary>
Motivation: 土地覆盖变化对地球系统过程有重要影响，但目前预测能力滞后于其他领域，如水文和大气过程。

Method: 采用扩散模型进行十年期不透水表面预测，并与无变化基线进行比较。

Result: 在12个大都市区的测试中，模型在分辨率≥0.7×0.7km²时表现优于基线。

Conclusion: 生成模型能捕捉历史数据中的时空模式，未来研究将纳入更多地球物理属性和驱动变量。

Abstract: Land cover, both present and future, has a significant effect on several important Earth system processes. For example, impervious surfaces heat up and speed up surface water runoff and reduce groundwater infiltration, with concomitant effects on regional hydrology and flood risk. While regional Earth System models have increasing skill at forecasting hydrologic and atmospheric processes at high resolution in future climate scenarios, our ability to forecast land-use and land-cover change (LULC), a critical input to risk and consequences assessment for these scenarios, has lagged behind. In this paper, we propose a new paradigm exploiting Generative AI (GenAI) for land cover change forecasting by framing LULC forecasting as a data synthesis problem conditioned on historical and auxiliary data-sources. We discuss desirable properties of generative models that fundament our research premise, and demonstrate the feasibility of our methodology through experiments on imperviousness forecasting using historical data covering the entire conterminous United States. Specifically, we train a diffusion model for decadal forecasting of imperviousness and compare its performance to a baseline that assumes no change at all. Evaluation across 12 metropolitan areas for a year held-out during training indicate that for average resolutions $\geq 0.7\times0.7km^2$ our model yields MAE lower than such a baseline. This finding corroborates that such a generative model can capture spatiotemporal patterns from historical data that are significant for projecting future change. Finally, we discuss future research to incorporate auxiliary information on physical properties about the Earth, as well as supporting simulation of different scenarios by means of driver variables.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [31] [Invisible Watermarks, Visible Gains: Steering Machine Unlearning with Bi-Level Watermarking Design](https://arxiv.org/abs/2508.10065)
*Yuhao Sun,Yihua Zhang,Gaowen Liu,Hongtao Xie,Sijia Liu*

Main category: cs.CR

TL;DR: 论文提出了一种基于数字水印的机器遗忘（MU）新方法Water4MU，通过数据级调整提升遗忘效果，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着‘被遗忘权’需求增加，机器遗忘成为提升信任和合规性的重要工具，但现有方法主要依赖模型权重调整，数据级调整的潜力未被充分探索。

Method: 提出Water4MU框架，结合数字水印和双层优化（BLO），上层优化水印网络以降低遗忘难度，下层独立训练模型。

Result: 实验证明Water4MU在图像分类和生成任务中均有效，尤其在‘挑战性遗忘’场景中表现优于现有方法。

Conclusion: Water4MU通过数据级水印调整实现了高效机器遗忘，为敏感数据移除提供了新思路。

Abstract: With the increasing demand for the right to be forgotten, machine unlearning (MU) has emerged as a vital tool for enhancing trust and regulatory compliance by enabling the removal of sensitive data influences from machine learning (ML) models. However, most MU algorithms primarily rely on in-training methods to adjust model weights, with limited exploration of the benefits that data-level adjustments could bring to the unlearning process. To address this gap, we propose a novel approach that leverages digital watermarking to facilitate MU by strategically modifying data content. By integrating watermarking, we establish a controlled unlearning mechanism that enables precise removal of specified data while maintaining model utility for unrelated tasks. We first examine the impact of watermarked data on MU, finding that MU effectively generalizes to watermarked data. Building on this, we introduce an unlearning-friendly watermarking framework, termed Water4MU, to enhance unlearning effectiveness. The core of Water4MU is a bi-level optimization (BLO) framework: at the upper level, the watermarking network is optimized to minimize unlearning difficulty, while at the lower level, the model itself is trained independently of watermarking. Experimental results demonstrate that Water4MU is effective in MU across both image classification and image generation tasks. Notably, it outperforms existing methods in challenging MU scenarios, known as "challenging forgets".

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [32] [DINOMotion: advanced robust tissue motion tracking with DINOv2 in 2D-Cine MRI-guided radiotherapy](https://arxiv.org/abs/2508.10260)
*Soorena Salari,Catherine Spino,Laurie-Anne Pharand,Fabienne Lathuiliere,Hassan Rivaz,Silvain Beriault,Yiming Xiao*

Main category: eess.IV

TL;DR: DINOMotion是一种基于DINOv2和LoRA层的新型深度学习框架，用于2D-Cine MRI引导放疗中的运动跟踪，具有鲁棒性、高效性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在大偏差和对齐解释性方面存在不足，DINOMotion旨在解决这些问题。

Method: 结合DINOv2的强大特征表示和LoRA层的参数优化，直接计算图像配准，提供视觉对应关系。

Result: 在肾脏、肝脏和肺部的实验中，DINOMotion的Dice分数分别为92.07%、90.90%和95.23%，处理时间约30ms。

Conclusion: DINOMotion在实时运动跟踪中表现优异，特别是在大偏差处理方面，具有临床应用潜力。

Abstract: Accurate tissue motion tracking is critical to ensure treatment outcome and safety in 2D-Cine MRI-guided radiotherapy. This is typically achieved by registration of sequential images, but existing methods often face challenges with large misalignments and lack of interpretability. In this paper, we introduce DINOMotion, a novel deep learning framework based on DINOv2 with Low-Rank Adaptation (LoRA) layers for robust, efficient, and interpretable motion tracking. DINOMotion automatically detects corresponding landmarks to derive optimal image registration, enhancing interpretability by providing explicit visual correspondences between sequential images. The integration of LoRA layers reduces trainable parameters, improving training efficiency, while DINOv2's powerful feature representations offer robustness against large misalignments. Unlike iterative optimization-based methods, DINOMotion directly computes image registration at test time. Our experiments on volunteer and patient datasets demonstrate its effectiveness in estimating both linear and nonlinear transformations, achieving Dice scores of 92.07% for the kidney, 90.90% for the liver, and 95.23% for the lung, with corresponding Hausdorff distances of 5.47 mm, 8.31 mm, and 6.72 mm, respectively. DINOMotion processes each scan in approximately 30ms and consistently outperforms state-of-the-art methods, particularly in handling large misalignments. These results highlight its potential as a robust and interpretable solution for real-time motion tracking in 2D-Cine MRI-guided radiotherapy.

</details>


### [33] [Efficient Image Denoising Using Global and Local Circulant Representation](https://arxiv.org/abs/2508.10307)
*Zhaoming Kong,Jiahuan Zhang,Xiaowei Yang*

Main category: eess.IV

TL;DR: 提出了一种名为Haar-tSVD的简单去噪算法，利用非局部自相似性和Haar变换与PCA的联系，通过统一的t-SVD投影捕获全局和局部相关性，实现高效并行化去噪。


<details>
  <summary>Details</summary>
Motivation: 成像设备的进步和每日海量图像数据对高效去噪的需求日益增长。

Method: 结合Haar变换和t-SVD投影，利用非局部自相似性，提出一步式并行化滤波方法，无需学习局部基。引入基于CNN和特征值分析的自适应噪声估计方案。

Result: 在不同真实去噪任务中验证了Haar-tSVD的高效性和有效性，实现了噪声去除和细节保留的平衡。

Conclusion: Haar-tSVD是一种快速且性能优异的去噪方法，适用于实际应用。

Abstract: The advancement of imaging devices and countless image data generated everyday impose an increasingly high demand on efficient and effective image denoising. In this paper, we present a computationally simple denoising algorithm, termed Haar-tSVD, aiming to explore the nonlocal self-similarity prior and leverage the connection between principal component analysis (PCA) and the Haar transform under circulant representation. We show that global and local patch correlations can be effectively captured through a unified tensor-singular value decomposition (t-SVD) projection with the Haar transform. This results in a one-step, highly parallelizable filtering method that eliminates the need for learning local bases to represent image patches, striking a balance between denoising speed and performance. Furthermore, we introduce an adaptive noise estimation scheme based on a CNN estimator and eigenvalue analysis to enhance the robustness and adaptability of the proposed method. Experiments on different real-world denoising tasks validate the efficiency and effectiveness of Haar-tSVD for noise removal and detail preservation. Datasets, code and results are publicly available at https://github.com/ZhaomingKong/Haar-tSVD.

</details>
