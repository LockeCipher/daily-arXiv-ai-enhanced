{"id": "2509.03680", "pdf": "https://arxiv.org/pdf/2509.03680", "abs": "https://arxiv.org/abs/2509.03680", "authors": ["Ruofan Liang", "Kai He", "Zan Gojcic", "Igor Gilitschenski", "Sanja Fidler", "Nandita Vijaykumar", "Zian Wang"], "title": "LuxDiT: Lighting Estimation with Video Diffusion Transformer", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Project page: https://research.nvidia.com/labs/toronto-ai/LuxDiT/", "summary": "Estimating scene lighting from a single image or video remains a longstanding challenge in computer vision and graphics. Learning-based approaches are constrained by the scarcity of ground-truth HDR environment maps, which are expensive to capture and limited in diversity. While recent generative models offer strong priors for image synthesis, lighting estimation remains difficult due to its reliance on indirect visual cues, the need to infer global (non-local) context, and the recovery of high-dynamic-range outputs. We propose LuxDiT, a novel data-driven approach that fine-tunes a video diffusion transformer to generate HDR environment maps conditioned on visual input. Trained on a large synthetic dataset with diverse lighting conditions, our model learns to infer illumination from indirect visual cues and generalizes effectively to real-world scenes. To improve semantic alignment between the input and the predicted environment map, we introduce a low-rank adaptation finetuning strategy using a collected dataset of HDR panoramas. Our method produces accurate lighting predictions with realistic angular high-frequency details, outperforming existing state-of-the-art techniques in both quantitative and qualitative evaluations.", "AI": {"tldr": "LuxDiT\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u9891\u6269\u6563\u53d8\u6362\u5668\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03\u751f\u6210HDR\u73af\u5883\u5149\u7167\u56fe\uff0c\u80fd\u591f\u4ece\u5355\u5f20\u56fe\u50cf\u6216\u89c6\u9891\u4e2d\u51c6\u786e\u4f30\u8ba1\u573a\u666f\u5149\u7167\u6761\u4ef6\u3002", "motivation": "\u73b0\u6709\u7684\u5b66\u4e60\u578b\u5149\u7167\u4f30\u8ba1\u65b9\u6cd5\u53d7\u9650\u4e8e\u771f\u5b9eHDR\u73af\u5883\u56fe\u7684\u7a00\u7f3a\u6027\uff0c\u8fd9\u4e9b\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u4e14\u591a\u6837\u6027\u6709\u9650\u3002\u867d\u7136\u751f\u6210\u6a21\u578b\u5728\u56fe\u50cf\u5408\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5149\u7167\u4f30\u8ba1\u4ecd\u9762\u4e34\u95f4\u63a5\u89c6\u89c9\u7ebf\u7d22\u4f9d\u8d56\u3001\u5168\u5c40\u4e0a\u4e0b\u6587\u63a8\u65ad\u548c\u9ad8\u52a8\u6001\u8303\u56f4\u8f93\u51fa\u6062\u590d\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faLuxDiT\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u89c6\u9891\u6269\u6563\u53d8\u6362\u5668\uff0c\u4f7f\u5176\u80fd\u591f\u6839\u636e\u89c6\u89c9\u8f93\u5165\u751f\u6210HDR\u73af\u5883\u56fe\u3002\u91c7\u7528\u4f4e\u79e9\u9002\u5e94\u5fae\u8c03\u7b56\u7565\u6765\u63d0\u9ad8\u8f93\u5165\u4e0e\u9884\u6d4b\u73af\u5883\u56fe\u4e4b\u95f4\u7684\u8bed\u4e49\u5bf9\u9f50\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ea7\u751f\u5177\u6709\u771f\u5b9e\u89d2\u5ea6\u9ad8\u9891\u7ec6\u8282\u7684\u51c6\u786e\u5149\u7167\u9884\u6d4b\uff0c\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u90fd\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u6280\u672f\uff0c\u5e76\u80fd\u6709\u6548\u6cdb\u5316\u5230\u771f\u5b9e\u4e16\u754c\u573a\u666f\u3002", "conclusion": "LuxDiT\u901a\u8fc7\u7ed3\u5408\u89c6\u9891\u6269\u6563\u53d8\u6362\u5668\u548c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5fae\u8c03\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5355\u56fe\u50cf/\u89c6\u9891\u5149\u7167\u4f30\u8ba1\u7684\u6311\u6218\uff0c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u56fe\u5f62\u5b66\u4e2d\u7684\u5149\u7167\u4f30\u8ba1\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.03775", "pdf": "https://arxiv.org/pdf/2509.03775", "abs": "https://arxiv.org/abs/2509.03775", "authors": ["Sankeerth Durvasula", "Sharanshangar Muhunthan", "Zain Moustafa", "Richard Chen", "Ruofan Liang", "Yushi Guan", "Nilesh Ahuja", "Nilesh Jain", "Selvakumar Panneer", "Nandita Vijaykumar"], "title": "ContraGS: Codebook-Condensed and Trainable Gaussian Splatting for Fast, Memory-Efficient Reconstruction", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) is a state-of-art technique to model real-world scenes with high quality and real-time rendering. Typically, a higher quality representation can be achieved by using a large number of 3D Gaussians. However, using large 3D Gaussian counts significantly increases the GPU device memory for storing model parameters. A large model thus requires powerful GPUs with high memory capacities for training and has slower training/rendering latencies due to the inefficiencies of memory access and data movement. In this work, we introduce ContraGS, a method to enable training directly on compressed 3DGS representations without reducing the Gaussian Counts, and thus with a little loss in model quality. ContraGS leverages codebooks to compactly store a set of Gaussian parameter vectors throughout the training process, thereby significantly reducing memory consumption. While codebooks have been demonstrated to be highly effective at compressing fully trained 3DGS models, directly training using codebook representations is an unsolved challenge. ContraGS solves the problem of learning non-differentiable parameters in codebook-compressed representations by posing parameter estimation as a Bayesian inference problem. To this end, ContraGS provides a framework that effectively uses MCMC sampling to sample over a posterior distribution of these compressed representations. With ContraGS, we demonstrate that ContraGS significantly reduces the peak memory during training (on average 3.49X) and accelerated training and rendering (1.36X and 1.88X on average, respectively), while retraining close to state-of-art quality.", "AI": {"tldr": "ContraGS\u662f\u4e00\u79cd\u76f4\u63a5\u5728\u538b\u7f29\u76843D\u9ad8\u65af\u5206\u5e03\u8868\u793a\u4e0a\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u7801\u672c\u5b58\u50a8\u9ad8\u65af\u53c2\u6570\u5411\u91cf\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u6d88\u8017\u548c\u52a0\u901f\u8bad\u7ec3\u6e32\u67d3\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1SOTA\u7684\u8d28\u91cf\u3002", "motivation": "3D\u9ad8\u65af\u5206\u5e03\u6280\u672f\u9700\u8981\u5927\u91cf\u9ad8\u65af\u5206\u5e03\u6765\u5b9e\u73b0\u9ad8\u8d28\u91cf\u8868\u793a\uff0c\u4f46\u8fd9\u4f1a\u663e\u8457\u589e\u52a0GPU\u5185\u5b58\u6d88\u8017\u548c\u8bad\u7ec3/\u6e32\u67d3\u5ef6\u8fdf\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u591f\u5728\u4fdd\u6301\u9ad8\u65af\u6570\u91cf\u7684\u540c\u65f6\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u3002", "method": "\u4f7f\u7528\u7801\u672c\u7d27\u51d1\u5b58\u50a8\u9ad8\u65af\u53c2\u6570\u5411\u91cf\uff0c\u5c06\u53c2\u6570\u4f30\u8ba1\u5efa\u6a21\u4e3a\u8d1d\u53f6\u65af\u63a8\u65ad\u95ee\u9898\uff0c\u91c7\u7528MCMC\u91c7\u6837\u5728\u538b\u7f29\u8868\u793a\u7684\u540e\u9a8c\u5206\u5e03\u4e0a\u8fdb\u884c\u91c7\u6837\u3002", "result": "\u8bad\u7ec3\u5cf0\u503c\u5185\u5b58\u5e73\u5747\u51cf\u5c113.49\u500d\uff0c\u8bad\u7ec3\u548c\u6e32\u67d3\u901f\u5ea6\u5206\u522b\u5e73\u5747\u52a0\u901f1.36\u500d\u548c1.88\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1SOTA\u7684\u8d28\u91cf\u3002", "conclusion": "ContraGS\u6210\u529f\u89e3\u51b3\u4e86\u5728\u7801\u672c\u538b\u7f29\u8868\u793a\u4e2d\u8bad\u7ec3\u4e0d\u53ef\u5fae\u5206\u53c2\u6570\u7684\u6311\u6218\uff0c\u4e3a3D\u9ad8\u65af\u5206\u5e03\u6280\u672f\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5185\u5b58\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.04047", "pdf": "https://arxiv.org/pdf/2509.04047", "abs": "https://arxiv.org/abs/2509.04047", "authors": ["Ashish Tiwari", "Satyam Bhardwaj", "Yash Bachwana", "Parag Sarvoday Sahu", "T. M. Feroz Ali", "Bhargava Chintalapati", "Shanmuganathan Raman"], "title": "TensoIS: A Step Towards Feed-Forward Tensorial Inverse Subsurface Scattering for Perlin Distributed Heterogeneous Media", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "To appear in Pacific Graphics 2025 (CGF Journal Track), Project page:   https://yashbachwana.github.io/TensoIS/", "summary": "Estimating scattering parameters of heterogeneous media from images is a severely under-constrained and challenging problem. Most of the existing approaches model BSSRDF either through an analysis-by-synthesis approach, approximating complex path integrals, or using differentiable volume rendering techniques to account for heterogeneity. However, only a few studies have applied learning-based methods to estimate subsurface scattering parameters, but they assume homogeneous media. Interestingly, no specific distribution is known to us that can explicitly model the heterogeneous scattering parameters in the real world. Notably, procedural noise models such as Perlin and Fractal Perlin noise have been effective in representing intricate heterogeneities of natural, organic, and inorganic surfaces. Leveraging this, we first create HeteroSynth, a synthetic dataset comprising photorealistic images of heterogeneous media whose scattering parameters are modeled using Fractal Perlin noise. Furthermore, we propose Tensorial Inverse Scattering (TensoIS), a learning-based feed-forward framework to estimate these Perlin-distributed heterogeneous scattering parameters from sparse multi-view image observations. Instead of directly predicting the 3D scattering parameter volume, TensoIS uses learnable low-rank tensor components to represent the scattering volume. We evaluate TensoIS on unseen heterogeneous variations over shapes from the HeteroSynth test set, smoke and cloud geometries obtained from open-source realistic volumetric simulations, and some real-world samples to establish its effectiveness for inverse scattering. Overall, this study is an attempt to explore Perlin noise distribution, given the lack of any such well-defined distribution in literature, to potentially model real-world heterogeneous scattering in a feed-forward manner.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u524d\u5411\u6846\u67b6TensoIS\uff0c\u901a\u8fc7Perlin\u566a\u58f0\u6a21\u578b\u548c\u4f4e\u79e9\u5f20\u91cf\u8868\u793a\u6765\u4f30\u8ba1\u5f02\u8d28\u4ecb\u8d28\u7684\u6563\u5c04\u53c2\u6570\uff0c\u89e3\u51b3\u4e86\u4ece\u7a00\u758f\u591a\u89c6\u89d2\u56fe\u50cf\u4e2d\u9006\u5411\u6563\u5c04\u7684\u6311\u6218\u6027\u95ee\u9898\u3002", "motivation": "\u5f02\u8d28\u4ecb\u8d28\u7684\u6563\u5c04\u53c2\u6570\u4f30\u8ba1\u662f\u4e2a\u6781\u5176\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u5047\u8bbe\u5747\u8d28\u4ecb\u8d28\uff0c\u800c\u771f\u5b9e\u4e16\u754c\u4e2d\u7f3a\u5c11\u660e\u786e\u7684\u5f02\u8d28\u6027\u6563\u5c04\u53c2\u6570\u5206\u5e03\u6a21\u578b\u3002", "method": "\u9996\u5148\u521b\u5efa\u4e86HeteroSynth\u5408\u6210\u6570\u636e\u96c6\uff0c\u4f7f\u7528Fractal Perlin\u566a\u58f0\u6a21\u578b\u5f02\u8d28\u6563\u5c04\u53c2\u6570\uff1b\u63d0\u51faTensoIS\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u4f4e\u79e9\u5f20\u91cf\u7ec4\u4ef6\u6765\u8868\u793a3D\u6563\u5c04\u53c2\u6570\u4f53\u79ef\uff0c\u4ece\u7a00\u758f\u591a\u89c6\u89d2\u56fe\u50cf\u4e2d\u8fdb\u884c\u53cd\u5411\u4f30\u8ba1\u3002", "result": "\u5728HeteroSynth\u6d4b\u8bd5\u96c6\u3001\u6709\u8da3\u7684\u70df\u96fe\u548c\u4e91\u5c42\u51e0\u4f55\u4f53\u4ee5\u53ca\u5b9e\u9645\u6837\u54c1\u4e0a\u8bc4\u4f30\u4e86TensoIS\u7684\u6548\u679c\uff0c\u8bc1\u660e\u5176\u5728\u9006\u5411\u6563\u5c04\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u63a2\u7d22\u4e86Perlin\u566a\u58f0\u5206\u5e03\u5728\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u5f02\u8d28\u6563\u5c04\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u524d\u5411\u5f0f\u5f02\u8d28\u6563\u5c04\u53c2\u6570\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.04058", "pdf": "https://arxiv.org/pdf/2509.04058", "abs": "https://arxiv.org/abs/2509.04058", "authors": ["Lei Zhong", "Yi Yang", "Changjian Li"], "title": "SMooGPT: Stylized Motion Generation using Large Language Models", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Stylized motion generation is actively studied in computer graphics, especially benefiting from the rapid advances in diffusion models. The goal of this task is to produce a novel motion respecting both the motion content and the desired motion style, e.g., ``walking in a loop like a Monkey''. Existing research attempts to address this problem via motion style transfer or conditional motion generation. They typically embed the motion style into a latent space and guide the motion implicitly in a latent space as well. Despite the progress, their methods suffer from low interpretability and control, limited generalization to new styles, and fail to produce motions other than ``walking'' due to the strong bias in the public stylization dataset. In this paper, we propose to solve the stylized motion generation problem from a new perspective of reasoning-composition-generation, based on our observations: i) human motion can often be effectively described using natural language in a body-part centric manner, ii) LLMs exhibit a strong ability to understand and reason about human motion, and iii) human motion has an inherently compositional nature, facilitating the new motion content or style generation via effective recomposing. We thus propose utilizing body-part text space as an intermediate representation, and present SMooGPT, a fine-tuned LLM, acting as a reasoner, composer, and generator when generating the desired stylized motion. Our method executes in the body-part text space with much higher interpretability, enabling fine-grained motion control, effectively resolving potential conflicts between motion content and style, and generalizes well to new styles thanks to the open-vocabulary ability of LLMs. Comprehensive experiments and evaluations, and a user perceptual study, demonstrate the effectiveness of our approach, especially under the pure text-driven stylized motion generation.", "AI": {"tldr": "\u901a\u8fc7\u5c06LLM\u4f5c\u4e3a\u7406\u89e3\u5668\u3001\u7ec4\u5408\u5668\u548c\u751f\u6210\u5668\uff0cSMooGPT\u65b9\u6cd5\u5728\u8eab\u4f53\u90e8\u4f4d\u6587\u672c\u7a7a\u95f4\u4e2d\u751f\u6210\u9ad8\u89e3\u91ca\u6027\u3001\u7ec6\u7c92\u5ea6\u63a7\u5236\u7684\u98ce\u683c\u5316\u52a8\u4f5c\uff0c\u5145\u5206\u5229\u7528\u4e86LLM\u7684\u5f00\u653f\u8bcd\u80fd\u529b\u548c\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u98ce\u683c\u5316\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\u5728\u89e3\u91ca\u6027\u3001\u63a7\u5236\u6027\u3001\u666e\u904d\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5bf9\u65b0\u98ce\u683c\u7684\u9002\u5e94\u80fd\u529b\u5dee\u4ee5\u53ca\u751f\u6210\u52a8\u4f5c\u7c7b\u578b\u504f\u5dee\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u7406\u7531-\u7ec4\u5408-\u751f\u6210\u7684\u65b0\u89c6\u89d2\uff0c\u4f7f\u7528\u8eab\u4f53\u90e8\u4f4d\u6587\u672c\u7a7a\u95f4\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u5e76\u7ec6\u8c03LLM\u6784\u5efaSMooGPT\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u540c\u65f6\u626e\u6f14\u7406\u89e3\u5668\u3001\u7ec4\u5408\u5668\u548c\u751f\u6210\u5668\u7684\u89d2\u8272\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u3001\u8bc4\u4f30\u548c\u7528\u6237\u611f\u77e5\u7814\u7a76\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728\u7eaf\u6587\u672c\u9a71\u52a8\u7684\u98ce\u683c\u5316\u52a8\u4f5c\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u7279\u522b\u7a81\u51fa\u3002", "conclusion": "SMooGPT\u901a\u8fc7\u5229\u7528LLM\u5728\u8eab\u4f53\u90e8\u4f4d\u6587\u672c\u7a7a\u95f4\u7684\u5f3a\u5927\u7406\u89e3\u548c\u7ec4\u5408\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u89e3\u91ca\u6027\u3001\u7ec6\u7c92\u5ea6\u63a7\u5236\u548c\u826f\u597d\u666e\u904d\u6027\u7684\u98ce\u683c\u5316\u52a8\u4f5c\u751f\u6210\u3002"}}
{"id": "2509.04145", "pdf": "https://arxiv.org/pdf/2509.04145", "abs": "https://arxiv.org/abs/2509.04145", "authors": ["Dongliang Cao", "Guoxing Sun", "Marc Habermann", "Florian Bernard"], "title": "Hyper Diffusion Avatars: Dynamic Human Avatar Generation using Network Weight Space Diffusion", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Creating human avatars is a highly desirable yet challenging task. Recent advancements in radiance field rendering have achieved unprecedented photorealism and real-time performance for personalized dynamic human avatars. However, these approaches are typically limited to person-specific rendering models trained on multi-view video data for a single individual, limiting their ability to generalize across different identities. On the other hand, generative approaches leveraging prior knowledge from pre-trained 2D diffusion models can produce cartoonish, static human avatars, which are animated through simple skeleton-based articulation. Therefore, the avatars generated by these methods suffer from lower rendering quality compared to person-specific rendering methods and fail to capture pose-dependent deformations such as cloth wrinkles. In this paper, we propose a novel approach that unites the strengths of person-specific rendering and diffusion-based generative modeling to enable dynamic human avatar generation with both high photorealism and realistic pose-dependent deformations. Our method follows a two-stage pipeline: first, we optimize a set of person-specific UNets, with each network representing a dynamic human avatar that captures intricate pose-dependent deformations. In the second stage, we train a hyper diffusion model over the optimized network weights. During inference, our method generates network weights for real-time, controllable rendering of dynamic human avatars. Using a large-scale, cross-identity, multi-view video dataset, we demonstrate that our approach outperforms state-of-the-art human avatar generation methods.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u4eba\u5458\u7279\u5b9a\u6e32\u67d3\u4e0e\u53cc\u5411\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u52a8\u6001\u4eba\u7c7b\u6f14\u5458\u751f\u6210\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u5b9e\u4f53\u611f\u548c\u771f\u5b9e\u59ff\u52bf\u4f9d\u8d56\u53d8\u5f62\u7684\u6e32\u67d3\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u4eba\u5458\u7279\u5b9a\u6e32\u67d3\u6a21\u578b\u65e0\u6cd5\u6a21\u62df\u4e0d\u540c\u8eab\u4efd\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u751f\u6210\u5f0f\u65b9\u6cd5\u6e32\u67d3\u8d28\u91cf\u4f4e\u4e14\u65e0\u6cd5\u6293\u53d6\u59ff\u52bf\u4f9d\u8d56\u53d8\u5f62\u7684\u9650\u5236\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u7ba1\u7ebf\uff1a\u9996\u5148\u4f18\u5316\u4e00\u7ec4\u4eba\u5458\u7279\u5b9aUNet\u7f51\u7edc\u6765\u6293\u53d6\u7ec6\u817b\u7684\u59ff\u52bf\u4f9d\u8d56\u53d8\u5f62\uff0c\u7136\u540e\u8bad\u7ec3\u4e00\u4e2a\u8d85\u53cc\u5411\u6a21\u578b\u6765\u751f\u6210\u7f51\u7edc\u6743\u91cd\uff0c\u5b9e\u73b0\u5b9e\u65f6\u53ef\u63a7\u6e32\u67d3\u3002", "result": "\u5728\u5927\u89c4\u6a21\u8de8\u8eab\u4efd\u591a\u89c6\u89d2\u89c6\u9891\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u4eba\u7c7b\u6f14\u5458\u751f\u6210\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u7ed3\u5408\u4e86\u4eba\u5458\u7279\u5b9a\u6e32\u67d3\u548c\u751f\u6210\u5f0f\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u80fd\u591f\u751f\u6210\u5177\u6709\u9ad8\u5b9e\u4f53\u611f\u548c\u771f\u5b9e\u59ff\u52bf\u4f9d\u8d56\u53d8\u5f62\u7684\u52a8\u6001\u4eba\u7c7b\u6f14\u5458\u3002"}}
{"id": "2509.03542", "pdf": "https://arxiv.org/pdf/2509.03542", "abs": "https://arxiv.org/abs/2509.03542", "authors": ["Guosheng Hu"], "title": "The Chaotic Art: Quantum Representation and Manipulation of Color", "categories": ["quant-ph", "cs.GR"], "comment": "9 pages, 8 figures", "summary": "Due to its unique computing principles, quantum computing technology will profoundly change the spectacle of color art. Focusing on experimental exploration of color qubit representation, color channel processing, and color image generation via quantum computing, this article proposes a new technical path for color computing in quantum computing environment, by which digital color is represented, operated, and measured in quantum bits, and then restored for classical computers as computing results. This method has been proved practicable as an artistic technique of color qubit representation and quantum computing via programming experiments in Qiskit and IBM Q. By building a bridge between classical chromatics and quantum graphics, quantum computers can be used for information visualization, image processing, and more color computing tasks. Furthermore, quantum computing can be expected to facilitate new color theories and artistic concepts.", "AI": {"tldr": "\u91cf\u5b50\u8ba1\u7b97\u6280\u672f\u901a\u8fc7\u989c\u8272\u5361\u5229\u8868\u793a\u3001\u989c\u8272\u901a\u9053\u5904\u7406\u548c\u91cf\u5b50\u56fe\u50cf\u751f\u6210\u7684\u65b9\u5f0f\uff0c\u4e3a\u989c\u8272\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u6280\u672f\u8def\u5f84\uff0c\u5efa\u7acb\u4e86\u7ecf\u5178\u8272\u5f69\u5b66\u4e0e\u91cf\u5b50\u56fe\u5f62\u5b66\u4e4b\u95f4\u7684\u6865\u6881\u3002", "motivation": "\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u7684\u72ec\u7279\u8ba1\u7b97\u539f\u7406\uff0c\u6df1\u523b\u6539\u53d8\u989c\u8272\u827a\u672f\u7684\u666f\u89c2\uff0c\u4e3a\u989c\u8272\u8ba1\u7b97\u63d0\u4f9b\u65b0\u7684\u6280\u672f\u53ef\u80fd\u6027\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u63a2\u7d22\u989c\u8272\u5361\u5229\u8868\u793a\u3001\u989c\u8272\u901a\u9053\u5904\u7406\u548c\u91cf\u5b50\u56fe\u50cf\u751f\u6210\uff0c\u5728Qiskit\u548cIBM Q\u4e2d\u8fdb\u884c\u7f16\u7a0b\u5b9e\u9a8c\uff0c\u5c06\u6570\u5b57\u989c\u8272\u7528\u91cf\u5b50\u4f4d\u8868\u793a\u3001\u64cd\u4f5c\u548c\u6d4b\u91cf\uff0c\u7136\u540e\u6062\u590d\u4e3a\u7ecf\u5178\u8ba1\u7b97\u673a\u7684\u8ba1\u7b97\u7ed3\u679c\u3002", "result": "\u8be5\u65b9\u6cd5\u5df2\u7ecf\u88ab\u8bc1\u660e\u4f5c\u4e3a\u989c\u8272\u5361\u5229\u8868\u793a\u548c\u91cf\u5b50\u8ba1\u7b97\u7684\u827a\u672f\u6280\u672f\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e3a\u4fe1\u606f\u53ef\u89c6\u5316\u3001\u56fe\u50cf\u5904\u7406\u548c\u66f4\u591a\u989c\u8272\u8ba1\u7b97\u4efb\u52a1\u63d0\u4f9b\u4e86\u91cf\u5b50\u8ba1\u7b97\u673a\u7684\u5e94\u7528\u53ef\u80fd\u3002", "conclusion": "\u91cf\u5b50\u8ba1\u7b97\u6709\u671b\u63a8\u52a8\u65b0\u7684\u989c\u8272\u7406\u8bba\u548c\u827a\u672f\u6982\u5ff5\u7684\u53d1\u5c55\uff0c\u901a\u8fc7\u5efa\u7acb\u7ecf\u5178\u8272\u5f69\u5b66\u4e0e\u91cf\u5b50\u56fe\u5f62\u5b66\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u4e3a\u989c\u8272\u8ba1\u7b97\u9886\u57df\u5e26\u6765\u9769\u547d\u6027\u7684\u53d8\u5316\u3002"}}
{"id": "2509.03740", "pdf": "https://arxiv.org/pdf/2509.03740", "abs": "https://arxiv.org/abs/2509.03740", "authors": ["Taha Koleilat", "Hassan Rivaz", "Yiming Xiao"], "title": "Singular Value Few-shot Adaptation of Vision-Language Models", "categories": ["cs.CV", "cs.CL"], "comment": "10 pages, 2 figures, 8 tables", "summary": "Vision-language models (VLMs) like CLIP have shown impressive zero-shot and few-shot learning capabilities across diverse applications. However, adapting these models to new fine-grained domains remains difficult due to reliance on prompt engineering and the high cost of full model fine-tuning. Existing adaptation approaches rely on augmented components, such as prompt tokens and adapter modules, which could limit adaptation quality, destabilize the model, and compromise the rich knowledge learned during pretraining. In this work, we present \\textbf{CLIP-SVD}, a novel \\textit{multi-modal} and \\textit{parameter-efficient} adaptation technique that leverages Singular Value Decomposition (SVD) to modify the internal parameter space of CLIP without injecting additional modules. Specifically, we fine-tune only the singular values of the CLIP parameter matrices to rescale the basis vectors for domain adaptation while retaining the pretrained model. This design enables enhanced adaptation performance using only \\textbf{0.04\\%} of the model's total parameters and better preservation of its generalization ability. CLIP-SVD achieves state-of-the-art classification results on 11 natural and 10 biomedical datasets, outperforming previous methods in both accuracy and generalization under few-shot settings. Additionally, we leverage a natural language-based approach to analyze the effectiveness and dynamics of the CLIP adaptation to allow interpretability of CLIP-SVD. The code is publicly available at https://github.com/HealthX-Lab/CLIP-SVD.", "AI": {"tldr": "CLIP-SVD\u662f\u4e00\u79cd\u57fa\u4e8e\u5947\u5f02\u503c\u5206\u89e3\u7684\u591a\u6a21\u6001\u53c2\u6570\u9ad8\u6548\u9002\u5e94\u6280\u672f\uff0c\u4ec5\u9700\u8c03\u6574CLIP\u6a21\u578b\u53c2\u6570\u77e9\u9635\u7684\u5947\u5f02\u503c\uff0c\u75280.04%\u7684\u53c2\u6570\u5b9e\u73b0\u9886\u57df\u9002\u5e94\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9002\u5e94\u65b9\u6cd5\u4f9d\u8d56\u63d0\u793a\u5de5\u7a0b\u548c\u5b8c\u6574\u5fae\u8c03\uff0c\u6210\u672c\u9ad8\u4e14\u53ef\u80fd\u7834\u574f\u9884\u8bad\u7ec3\u77e5\u8bc6\u3002\u9700\u8981\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u4e14\u80fd\u4fdd\u6301\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u9002\u5e94\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5947\u5f02\u503c\u5206\u89e3(SVD)\u4fee\u6539CLIP\u5185\u90e8\u53c2\u6570\u7a7a\u95f4\uff0c\u4ec5\u5fae\u8c03\u53c2\u6570\u77e9\u9635\u7684\u5947\u5f02\u503c\u6765\u91cd\u65b0\u7f29\u653e\u57fa\u5411\u91cf\uff0c\u4e0d\u6ce8\u5165\u989d\u5916\u6a21\u5757\u3002", "result": "\u572811\u4e2a\u81ea\u7136\u6570\u636e\u96c6\u548c10\u4e2a\u751f\u7269\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u5206\u7c7b\u7ed3\u679c\uff0c\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u51c6\u786e\u7387\u548c\u6cdb\u5316\u80fd\u529b\u5747\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "CLIP-SVD\u901a\u8fc7SVD\u5b9e\u73b0\u53c2\u6570\u9ad8\u6548\u7684\u9886\u57df\u9002\u5e94\uff0c\u5728\u4fdd\u6301\u9884\u8bad\u7ec3\u77e5\u8bc6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002"}}
{"id": "2509.03803", "pdf": "https://arxiv.org/pdf/2509.03803", "abs": "https://arxiv.org/abs/2509.03803", "authors": ["Mengyu Gao", "Qiulei Dong"], "title": "Causality-guided Prompt Learning for Vision-language Models via Visual Granulation", "categories": ["cs.CV"], "comment": "ICCV 2025 Accepted", "summary": "Prompt learning has recently attracted much attention for adapting pre-trained vision-language models (e.g., CLIP) to downstream recognition tasks. However, most of the existing CLIP-based prompt learning methods only show a limited ability for handling fine-grained datasets. To address this issue, we propose a causality-guided text prompt learning method via visual granulation for CLIP, called CaPL, where the explored visual granulation technique could construct sets of visual granules for the text prompt to capture subtle discrepancies among different fine-grained classes through casual inference. The CaPL method contains the following two modules: (1) An attribute disentanglement module is proposed to decompose visual features into non-individualized attributes (shared by some classes) and individualized attributes (specific to single classes) using a Brownian Bridge Diffusion Model; (2) A granule learning module is proposed to construct visual granules by integrating the aforementioned attributes for recognition under two causal inference strategies. Thanks to the learned visual granules, more discriminative text prompt is expected to be learned. Extensive experimental results on 15 datasets demonstrate that our CaPL method significantly outperforms the state-of-the-art prompt learning methods, especially on fine-grained datasets.", "AI": {"tldr": "CaPL\u662f\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u63a8\u7406\u7684\u89c6\u89c9\u7c92\u5ea6\u5316\u6587\u672c\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c5e\u6027\u89e3\u8026\u548c\u7c92\u5ea6\u5b66\u4e60\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86CLIP\u5728\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684CLIP\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6709\u9650\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6355\u6349\u7ec6\u7c92\u5ea6\u7c7b\u522b\u95f4\u7ec6\u5fae\u5dee\u5f02\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCaPL\u65b9\u6cd5\uff0c\u5305\u542b\u5c5e\u6027\u89e3\u8026\u6a21\u5757\uff08\u4f7f\u7528\u5e03\u6717\u6865\u6269\u6563\u6a21\u578b\u5206\u89e3\u89c6\u89c9\u7279\u5f81\uff09\u548c\u7c92\u5ea6\u5b66\u4e60\u6a21\u5757\uff08\u901a\u8fc7\u56e0\u679c\u63a8\u7406\u7b56\u7565\u6784\u5efa\u89c6\u89c9\u7c92\u5ea6\uff09\u3002", "result": "\u572815\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCaPL\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u3002", "conclusion": "\u901a\u8fc7\u89c6\u89c9\u7c92\u5ea6\u5316\u548c\u56e0\u679c\u63a8\u7406\uff0cCaPL\u80fd\u591f\u5b66\u4e60\u66f4\u5177\u533a\u5206\u6027\u7684\u6587\u672c\u63d0\u793a\uff0c\u6709\u6548\u63d0\u5347CLIP\u5728\u7ec6\u7c92\u5ea6\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2509.03883", "pdf": "https://arxiv.org/pdf/2509.03883", "abs": "https://arxiv.org/abs/2509.03883", "authors": ["Haiwei Xue", "Xiangyang Luo", "Zhanghao Hu", "Xin Zhang", "Xunzhi Xiang", "Yuqin Dai", "Jianzhuang Liu", "Zhensong Zhang", "Minglei Li", "Jian Yang", "Fei Ma", "Zhiyong Wu", "Changpeng Yang", "Zonghong Dai", "Fei Richard Yu"], "title": "Human Motion Video Generation: A Survey", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted by TPAMI. Github Repo:   https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation IEEE Access:   https://ieeexplore.ieee.org/document/11106267", "summary": "Human motion video generation has garnered significant research interest due to its broad applications, enabling innovations such as photorealistic singing heads or dynamic avatars that seamlessly dance to music. However, existing surveys in this field focus on individual methods, lacking a comprehensive overview of the entire generative process. This paper addresses this gap by providing an in-depth survey of human motion video generation, encompassing over ten sub-tasks, and detailing the five key phases of the generation process: input, motion planning, motion video generation, refinement, and output. Notably, this is the first survey that discusses the potential of large language models in enhancing human motion video generation. Our survey reviews the latest developments and technological trends in human motion video generation across three primary modalities: vision, text, and audio. By covering over two hundred papers, we offer a thorough overview of the field and highlight milestone works that have driven significant technological breakthroughs. Our goal for this survey is to unveil the prospects of human motion video generation and serve as a valuable resource for advancing the comprehensive applications of digital humans. A complete list of the models examined in this survey is available in Our Repository https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.", "AI": {"tldr": "\u8fd9\u662f\u4e00\u4efd\u5173\u4e8e\u4eba\u4f53\u8fd0\u52a8\u89c6\u9891\u751f\u6210\u7684\u7efc\u8ff0\u6027\u8bba\u6587\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u8be5\u9886\u57df\u7684\u751f\u6210\u8fc7\u7a0b\u4e94\u5927\u9636\u6bb5\u3001\u4e09\u79cd\u4e3b\u8981\u6a21\u6001\uff0c\u5e76\u9996\u6b21\u8ba8\u8bba\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u8c03\u67e5\u592a\u8fc7\u96c6\u4e2d\u4e8e\u5355\u4e2a\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u5bf9\u6574\u4e2a\u4eba\u4f53\u8fd0\u52a8\u89c6\u9891\u751f\u6210\u8fc7\u7a0b\u7684\u7cfb\u7edf\u6027\u6982\u89c8\u3002\u672c\u6587\u5f25\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u89c6\u89d2\u3002", "method": "\u8bba\u6587\u5c06\u751f\u6210\u8fc7\u7a0b\u5206\u4e3a\u4e94\u4e2a\u5173\u952e\u9636\u6bb5\uff1a\u8f93\u5165\u3001\u8fd0\u52a8\u89c4\u5212\u3001\u8fd0\u52a8\u89c6\u9891\u751f\u6210\u3001\u7cbe\u70bc\u548c\u8f93\u51fa\u3002\u7efc\u8ff0\u4e86\u89c6\u89c9\u3001\u6587\u672c\u548c\u97f3\u9891\u4e09\u79cd\u4e3b\u8981\u6a21\u6001\uff0c\u6db5\u76d6\u4e86\u8d85\u8fc7200\u7bc7\u8bba\u6587\u3002", "result": "\u8be5\u7efc\u8ff0\u63d0\u4f9b\u4e86\u4eba\u4f53\u8fd0\u52a8\u89c6\u9891\u751f\u6210\u9886\u57df\u7684\u5168\u9762\u6982\u89c8\uff0c\u6807\u8bb0\u4e86\u91cd\u8981\u7684\u91cc\u7a0b\u7891\u6027\u7814\u7a76\uff0c\u5e76\u9996\u6b21\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u3002", "conclusion": "\u8fd9\u4efd\u7efc\u8ff0\u63ed\u793a\u4e86\u4eba\u4f53\u8fd0\u52a8\u89c6\u9891\u751f\u6210\u7684\u5e7b\u60f3\u524d\u666f\uff0c\u5e76\u4e3a\u63a8\u8fdb\u6570\u5b57\u4eba\u7c7b\u7684\u5168\u9762\u5e94\u7528\u63d0\u4f9b\u4e86\u4ef7\u503c\u8f83\u9ad8\u7684\u8d44\u6e90\u3002"}}
{"id": "2509.04126", "pdf": "https://arxiv.org/pdf/2509.04126", "abs": "https://arxiv.org/abs/2509.04126", "authors": ["Yuan Zhao", "Liu Lin"], "title": "MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-image diffusion models have achieved remarkable image quality, but they still struggle with complex, multiele ment prompts, and limited stylistic diversity. To address these limitations, we propose a Multi-Expert Planning and Gen eration Framework (MEPG) that synergistically integrates position- and style-aware large language models (LLMs) with spatial-semantic expert modules. The framework comprises two core components: (1) a Position-Style-Aware (PSA) module that utilizes a supervised fine-tuned LLM to decom pose input prompts into precise spatial coordinates and style encoded semantic instructions; and (2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera tion through dynamic expert routing across both local regions and global areas. During the generation process for each lo cal region, specialized models (e.g., realism experts, styliza tion specialists) are selectively activated for each spatial par tition via attention-based gating mechanisms. The architec ture supports lightweight integration and replacement of ex pert models, providing strong extensibility. Additionally, an interactive interface enables real-time spatial layout editing and per-region style selection from a portfolio of experts. Ex periments show that MEPG significantly outperforms base line models with the same backbone in both image quality   and style diversity.", "AI": {"tldr": "\u63d0\u51fa\u4e86MEPG\u6846\u67b6\uff0c\u901a\u8fc7\u4f4d\u7f6e-\u98ce\u683c\u611f\u77e5LLM\u548c\u591a\u4e13\u5bb6\u6269\u6563\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u590d\u6742\u591a\u5143\u7d20\u63d0\u793a\u548c\u98ce\u683c\u591a\u6837\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u591a\u5143\u7d20\u63d0\u793a\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u98ce\u683c\u591a\u6837\u6027\u6709\u9650\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7cbe\u786e\u63a7\u5236\u7a7a\u95f4\u5e03\u5c40\u548c\u98ce\u683c\u7684\u4e13\u4e1a\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\u7684LLM\u5206\u89e3\u8f93\u5165\u63d0\u793a\u4e3a\u7cbe\u786e\u7a7a\u95f4\u5750\u6807\u548c\u98ce\u683c\u7f16\u7801\u8bed\u4e49\u6307\u4ee4\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u95e8\u63a7\u673a\u5236\u5728\u5c40\u90e8\u533a\u57df\u548c\u5168\u5c40\u533a\u57df\u52a8\u6001\u8def\u7531\u4e13\u5bb6\u6a21\u578b\uff08\u5982\u5199\u5b9e\u4e13\u5bb6\u3001\u98ce\u683c\u5316\u4e13\u5bb6\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMEPG\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u98ce\u683c\u591a\u6837\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u76f8\u540c\u9aa8\u5e72\u7f51\u7edc\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "MEPG\u6846\u67b6\u901a\u8fc7\u4e13\u4e1a\u5316\u6a21\u5757\u7684\u534f\u540c\u6574\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86\u590d\u6742\u591a\u5143\u7d20\u56fe\u50cf\u751f\u6210\u7684\u8d28\u91cf\u548c\u98ce\u683c\u63a7\u5236\u80fd\u529b\uff0c\u5177\u6709\u8f7b\u91cf\u7ea7\u96c6\u6210\u548c\u5f3a\u6269\u5c55\u6027\u4f18\u52bf\u3002"}}
{"id": "2509.04269", "pdf": "https://arxiv.org/pdf/2509.04269", "abs": "https://arxiv.org/abs/2509.04269", "authors": ["Yuxin Gong", "Se-in Jang", "Wei Shao", "Yi Su", "Kuang Gong"], "title": "TauGenNet: Plasma-Driven Tau PET Image Synthesis via Text-Guided 3D Diffusion Models", "categories": ["cs.CV"], "comment": "9 pages, 4 figures, submitted to IEEE Transactions on Radiation and   Plasma Medical Sciences", "summary": "Accurate quantification of tau pathology via tau positron emission tomography (PET) scan is crucial for diagnosing and monitoring Alzheimer's disease (AD). However, the high cost and limited availability of tau PET restrict its widespread use. In contrast, structural magnetic resonance imaging (MRI) and plasma-based biomarkers provide non-invasive and widely available complementary information related to brain anatomy and disease progression. In this work, we propose a text-guided 3D diffusion model for 3D tau PET image synthesis, leveraging multimodal conditions from both structural MRI and plasma measurement. Specifically, the textual prompt is from the plasma p-tau217 measurement, which is a key indicator of AD progression, while MRI provides anatomical structure constraints. The proposed framework is trained and evaluated using clinical AV1451 tau PET data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. Experimental results demonstrate that our approach can generate realistic, clinically meaningful 3D tau PET across a range of disease stages. The proposed framework can help perform tau PET data augmentation under different settings, provide a non-invasive, cost-effective alternative for visualizing tau pathology, and support the simulation of disease progression under varying plasma biomarker levels and cognitive conditions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6587\u672c\u5f15\u5bfc\u76843D\u6269\u6563\u6a21\u578b\uff0c\u5229\u7528\u7ed3\u6784MRI\u548c\u8840\u6d46p-tau217\u6d4b\u91cf\u503c\u5408\u62103D tau PET\u56fe\u50cf\uff0c\u4e3a\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\u63d0\u4f9b\u4f4e\u6210\u672c\u66ff\u4ee3\u65b9\u6848", "motivation": "tau PET\u6210\u50cf\u6210\u672c\u9ad8\u4e14\u53ef\u7528\u6027\u6709\u9650\uff0c\u800c\u7ed3\u6784MRI\u548c\u8840\u6d46\u751f\u7269\u6807\u5fd7\u7269\u5177\u6709\u975e\u4fb5\u5165\u6027\u548c\u5e7f\u6cdb\u53ef\u7528\u6027\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u7ecf\u6d4e\u6709\u6548\u7684tau\u75c5\u7406\u53ef\u89c6\u5316\u65b9\u6cd5", "method": "\u4f7f\u7528\u6587\u672c\u5f15\u5bfc\u76843D\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u8840\u6d46p-tau217\u6d4b\u91cf\u503c\u4f5c\u4e3a\u6587\u672c\u63d0\u793a\uff0c\u7ed3\u6784MRI\u63d0\u4f9b\u89e3\u5256\u7ed3\u6784\u7ea6\u675f\uff0c\u5728ADNI\u6570\u636e\u5e93\u7684AV1451 tau PET\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30", "result": "\u80fd\u591f\u751f\u6210\u8de8\u4e0d\u540c\u75be\u75c5\u9636\u6bb5\u7684\u771f\u5b9e\u3001\u4e34\u5e8a\u6709\u610f\u4e49\u76843D tau PET\u56fe\u50cf\uff0c\u652f\u6301\u6570\u636e\u589e\u5f3a\u548c\u75be\u75c5\u8fdb\u5c55\u6a21\u62df", "conclusion": "\u8be5\u6846\u67b6\u53ef\u4f5c\u4e3atau PET\u7684\u975e\u4fb5\u5165\u6027\u3001\u7ecf\u6d4e\u6709\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u5728\u4e0d\u540c\u8840\u6d46\u751f\u7269\u6807\u5fd7\u7269\u6c34\u5e73\u548c\u8ba4\u77e5\u6761\u4ef6\u4e0b\u53ef\u89c6\u5316tau\u75c5\u7406\u548c\u6a21\u62df\u75be\u75c5\u8fdb\u5c55"}}
{"id": "2509.04379", "pdf": "https://arxiv.org/pdf/2509.04379", "abs": "https://arxiv.org/abs/2509.04379", "authors": ["Jimin Xu", "Bosheng Qin", "Tao Jin", "Zhou Zhao", "Zhenhui Ye", "Jun Yu", "Fei Wu"], "title": "SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in neural representations, such as Neural Radiance Fields and 3D Gaussian Splatting, have increased interest in applying style transfer to 3D scenes. While existing methods can transfer style patterns onto 3D-consistent neural representations, they struggle to effectively extract and transfer high-level style semantics from the reference style image. Additionally, the stylized results often lack structural clarity and separation, making it difficult to distinguish between different instances or objects within the 3D scene. To address these limitations, we propose a novel 3D style transfer pipeline that effectively integrates prior knowledge from pretrained 2D diffusion models. Our pipeline consists of two key stages: First, we leverage diffusion priors to generate stylized renderings of key viewpoints. Then, we transfer the stylized key views onto the 3D representation. This process incorporates two innovative designs. The first is cross-view style alignment, which inserts cross-view attention into the last upsampling block of the UNet, allowing feature interactions across multiple key views. This ensures that the diffusion model generates stylized key views that maintain both style fidelity and instance-level consistency. The second is instance-level style transfer, which effectively leverages instance-level consistency across stylized key views and transfers it onto the 3D representation. This results in a more structured, visually coherent, and artistically enriched stylization. Extensive qualitative and quantitative experiments demonstrate that our 3D style transfer pipeline significantly outperforms state-of-the-art methods across a wide range of scenes, from forward-facing to challenging 360-degree environments. Visit our project page https://jm-xu.github.io/SSGaussian for immersive visualization.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u76843D\u98ce\u683c\u8f6c\u6362\u6d41\u6c34\u7ebf\uff0c\u5229\u7528\u9884\u8bad\u7ec32D\u6269\u6563\u6a21\u578b\u7684\u77e5\u8bc6\uff0c\u901a\u8fc7\u8de8\u89c6\u56fe\u98ce\u683c\u5bf9\u9f50\u548c\u5b9e\u4f8b\u7ea7\u98ce\u683c\u8f6c\u6362\uff0c\u5b9e\u73b0\u4e86\u66f4\u7ed3\u6784\u5316\u3001\u89c6\u89c9\u4e00\u81f4\u76843D\u573a\u666f\u98ce\u683c\u5316\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u76843D\u98ce\u683c\u8f6c\u6362\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u63d0\u53d6\u548c\u8f6c\u6362\u9ad8\u7ea7\u522b\u7684\u98ce\u683c\u8bed\u4e49\uff0c\u5e76\u4e14\u98ce\u683c\u5316\u7ed3\u679c\u7f3a\u4e4f\u7ed3\u6784\u6e05\u6670\u5ea6\u548c\u5206\u79bb\u6027\uff0c\u5bfc\u81f4\u4e0d\u540c\u5b9e\u4f8b\u6216\u7269\u4f53\u96be\u4ee5\u533a\u5206\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6d41\u6c34\u7ebf\uff1a\u9996\u5148\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u5173\u952e\u89c6\u70b9\u7684\u98ce\u683c\u5316\u6e32\u67d3\uff0c\u7136\u540e\u5c06\u98ce\u683c\u5316\u7684\u5173\u952e\u89c6\u56fe\u8f6c\u6362\u52303D\u8868\u793a\u4e2d\u3002\u5305\u542b\u8de8\u89c6\u56fe\u98ce\u683c\u5bf9\u9f50\uff08\u5728UNet\u6700\u540e\u4e00\u4e2a\u4e0a\u91c7\u6837\u5757\u4e2d\u63d2\u5165\u8de8\u89c6\u56fe\u6ce8\u610f\u529b\uff09\u548c\u5b9e\u4f8b\u7ea7\u98ce\u683c\u8f6c\u6362\u4e24\u4e2a\u521b\u65b0\u8bbe\u8ba1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u573a\u666f\u4e0a\uff08\u4ece\u524d\u5411\u89c6\u89d2\u5230\u5177\u6709\u6311\u6218\u6027\u7684360\u5ea6\u73af\u5883\uff09\u90fd\u663e\u8457\u8d85\u8fc7\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u65b0\u65b93D\u98ce\u683c\u8f6c\u6362\u6d41\u6c34\u7ebf\u901a\u8fc7\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u77e5\u8bc6\uff0c\u5b9e\u73b0\u4e86\u66f4\u7ed3\u6784\u5316\u3001\u89c6\u89c9\u4e00\u81f4\u548c\u827a\u672f\u4e30\u5bcc\u76843D\u573a\u666f\u98ce\u683c\u5316\u6548\u679c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u9ad8\u7ea7\u98ce\u683c\u8bed\u4e49\u63d0\u53d6\u548c\u5b9e\u4f8b\u533a\u5206\u65b9\u9762\u7684\u95ee\u9898\u3002"}}
{"id": "2509.04434", "pdf": "https://arxiv.org/pdf/2509.04434", "abs": "https://arxiv.org/abs/2509.04434", "authors": ["Hyunsoo Cha", "Byungjun Kim", "Hanbyul Joo"], "title": "Durian: Dual Reference-guided Portrait Animation with Attribute Transfer", "categories": ["cs.CV"], "comment": "Project Page: https://hyunsoocha.github.io/durian", "summary": "We present Durian, the first method for generating portrait animation videos with facial attribute transfer from a given reference image to a target portrait in a zero-shot manner. To enable high-fidelity and spatially consistent attribute transfer across frames, we introduce dual reference networks that inject spatial features from both the portrait and attribute images into the denoising process of a diffusion model. We train the model using a self-reconstruction formulation, where two frames are sampled from the same portrait video: one is treated as the attribute reference and the other as the target portrait, and the remaining frames are reconstructed conditioned on these inputs and their corresponding masks. To support the transfer of attributes with varying spatial extent, we propose a mask expansion strategy using keypoint-conditioned image generation for training. In addition, we further augment the attribute and portrait images with spatial and appearance-level transformations to improve robustness to positional misalignment between them. These strategies allow the model to effectively generalize across diverse attributes and in-the-wild reference combinations, despite being trained without explicit triplet supervision. Durian achieves state-of-the-art performance on portrait animation with attribute transfer, and notably, its dual reference design enables multi-attribute composition in a single generation pass without additional training.", "AI": {"tldr": "Durian\u662f\u9996\u4e2a\u96f6\u6837\u672c\u8096\u50cf\u52a8\u753b\u751f\u6210\u65b9\u6cd5\uff0c\u80fd\u591f\u4ece\u53c2\u8003\u56fe\u50cf\u5411\u76ee\u6807\u8096\u50cf\u8fdb\u884c\u9762\u90e8\u5c5e\u6027\u8fc1\u79fb\uff0c\u901a\u8fc7\u53cc\u53c2\u8003\u7f51\u7edc\u548c\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u3001\u7a7a\u95f4\u4e00\u81f4\u7684\u8de8\u5e27\u5c5e\u6027\u8fc1\u79fb\u3002", "motivation": "\u73b0\u6709\u7684\u8096\u50cf\u52a8\u753b\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u6709\u6548\u7684\u96f6\u6837\u672c\u9762\u90e8\u5c5e\u6027\u8fc1\u79fb\u80fd\u529b\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5728\u4e0d\u540c\u5e27\u95f4\u4fdd\u6301\u7a7a\u95f4\u4e00\u81f4\u6027\u7684\u9ad8\u4fdd\u771f\u5c5e\u6027\u8fc1\u79fb\u6280\u672f\u3002", "method": "\u91c7\u7528\u53cc\u53c2\u8003\u7f51\u7edc\u5c06\u8096\u50cf\u548c\u5c5e\u6027\u56fe\u50cf\u7684\u7a7a\u95f4\u7279\u5f81\u6ce8\u5165\u6269\u6563\u6a21\u578b\u53bb\u566a\u8fc7\u7a0b\uff1b\u4f7f\u7528\u81ea\u91cd\u5efa\u8bad\u7ec3\u6846\u67b6\uff0c\u4ece\u540c\u4e00\u89c6\u9891\u91c7\u6837\u5e27\u4f5c\u4e3a\u5c5e\u6027\u548c\u76ee\u6807\u53c2\u8003\uff1b\u63d0\u51fa\u57fa\u4e8e\u5173\u952e\u70b9\u7684\u63a9\u7801\u6269\u5c55\u7b56\u7565\u548c\u7a7a\u95f4\u5916\u89c2\u53d8\u6362\u589e\u5f3a\u65b9\u6cd5\u3002", "result": "Durian\u5728\u8096\u50cf\u52a8\u753b\u5c5e\u6027\u8fc1\u79fb\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5176\u53cc\u53c2\u8003\u8bbe\u8ba1\u652f\u6301\u5355\u6b21\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u591a\u5c5e\u6027\u7ec4\u5408\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u521b\u65b0\u7684\u53cc\u53c2\u8003\u7f51\u7edc\u8bbe\u8ba1\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u6761\u4ef6\u4e0b\u7684\u9ad8\u8d28\u91cf\u8096\u50cf\u52a8\u753b\u751f\u6210\u548c\u5c5e\u6027\u8fc1\u79fb\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.04450", "pdf": "https://arxiv.org/pdf/2509.04450", "abs": "https://arxiv.org/abs/2509.04450", "authors": ["Jun-Kun Chen", "Aayush Bansal", "Minh Phuoc Vo", "Yu-Xiong Wang"], "title": "Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual Try-On from a Single Image -- Technical Preview", "categories": ["cs.CV", "cs.LG"], "comment": "Project Page: https://immortalco.github.io/VirtualFittingRoom/", "summary": "We introduce the Virtual Fitting Room (VFR), a novel video generative model that produces arbitrarily long virtual try-on videos. Our VFR models long video generation tasks as an auto-regressive, segment-by-segment generation process, eliminating the need for resource-intensive generation and lengthy video data, while providing the flexibility to generate videos of arbitrary length. The key challenges of this task are twofold: ensuring local smoothness between adjacent segments and maintaining global temporal consistency across different segments. To address these challenges, we propose our VFR framework, which ensures smoothness through a prefix video condition and enforces consistency with the anchor video -- a 360-degree video that comprehensively captures the human's wholebody appearance. Our VFR generates minute-scale virtual try-on videos with both local smoothness and global temporal consistency under various motions, making it a pioneering work in long virtual try-on video generation.", "AI": {"tldr": "VFR\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u80fd\u591f\u751f\u6210\u4efb\u610f\u957f\u5ea6\u7684\u865a\u62df\u8bd5\u7a7f\u89c6\u9891\uff0c\u901a\u8fc7\u5206\u6bb5\u81ea\u56de\u5f52\u751f\u6210\u65b9\u5f0f\u89e3\u51b3\u957f\u89c6\u9891\u751f\u6210\u95ee\u9898\uff0c\u786e\u4fdd\u5c40\u90e8\u5e73\u6ed1\u6027\u548c\u5168\u5c40\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edf\u865a\u62df\u8bd5\u7a7f\u89c6\u9891\u751f\u6210\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u548c\u957f\u89c6\u9891\u6570\u636e\uff0c\u4e14\u96be\u4ee5\u751f\u6210\u4efb\u610f\u957f\u5ea6\u7684\u89c6\u9891\u3002\u9700\u8981\u89e3\u51b3\u76f8\u90bb\u7247\u6bb5\u95f4\u7684\u5c40\u90e8\u5e73\u6ed1\u6027\u548c\u4e0d\u540c\u7247\u6bb5\u95f4\u7684\u5168\u5c40\u65f6\u95f4\u4e00\u81f4\u6027\u6311\u6218\u3002", "method": "\u91c7\u7528\u81ea\u56de\u5f52\u7684\u5206\u6bb5\u751f\u6210\u8fc7\u7a0b\uff0c\u4f7f\u7528\u524d\u7f00\u89c6\u9891\u6761\u4ef6\u786e\u4fdd\u5c40\u90e8\u5e73\u6ed1\u6027\uff0c\u901a\u8fc7\u951a\u70b9\u89c6\u9891\uff08360\u5ea6\u5168\u8eab\u89c6\u9891\uff09\u6765\u4fdd\u6301\u5168\u5c40\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u6210\u529f\u751f\u6210\u4e86\u5206\u949f\u7ea7\u522b\u7684\u865a\u62df\u8bd5\u7a7f\u89c6\u9891\uff0c\u5728\u5404\u79cd\u52a8\u4f5c\u4e0b\u90fd\u80fd\u4fdd\u6301\u5c40\u90e8\u5e73\u6ed1\u6027\u548c\u5168\u5c40\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "conclusion": "VFR\u662f\u957f\u865a\u62df\u8bd5\u7a7f\u89c6\u9891\u751f\u6210\u9886\u57df\u7684\u5f00\u521b\u6027\u5de5\u4f5c\uff0c\u4e3a\u4efb\u610f\u957f\u5ea6\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
