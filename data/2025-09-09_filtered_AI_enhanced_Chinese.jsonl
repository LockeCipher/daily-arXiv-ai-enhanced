{"id": "2509.06573", "pdf": "https://arxiv.org/pdf/2509.06573", "abs": "https://arxiv.org/abs/2509.06573", "authors": ["Jie Zhou", "Linzi Qu", "Miu-Ling Lam", "Hongbo Fu"], "title": "From Rigging to Waving: 3D-Guided Diffusion for Natural Animation of Hand-Drawn Characters", "categories": ["cs.GR"], "comment": null, "summary": "Hand-drawn character animation is a vibrant field in computer graphics, presenting challenges in achieving geometric consistency while conveying expressive motion. Traditional skeletal animation methods maintain geometric consistency but struggle with complex non-rigid elements like flowing hair and skirts, leading to unnatural deformation. Conversely, video diffusion models synthesize realistic dynamics but often create geometric distortions in stylized drawings due to domain gaps. This work proposes a hybrid animation system that combines skeletal animation and video diffusion. Initially, coarse images are generated from characters retargeted with skeletal animations for geometric guidance. These images are then enhanced in texture and secondary dynamics using video diffusion priors, framing this enhancement as an inpainting task. A domain-adapted diffusion model refines user-masked regions needing improvement, especially for secondary dynamics. To enhance motion realism further, we introduce a Secondary Dynamics Injection (SDI) strategy in the denoising process, incorporating features from a pre-trained diffusion model enriched with human motion priors. Additionally, to tackle unnatural deformations from low-poly single-mesh character modeling, we present a Hair Layering Modeling (HLM) technique that uses segmentation maps to separate hair from the body, allowing for more natural animation of long-haired characters. Extensive experiments show that our system outperforms state-of-the-art methods in both quantitative and qualitative evaluations.", "AI": {"tldr": "\u7ed3\u5408\u9aa8\u9abc\u52a8\u753b\u548c\u89c6\u9891\u6f0f\u6dbc\u6a21\u578b\u7684\u6df7\u5408\u52a8\u753b\u7cfb\u7edf\uff0c\u901a\u8fc7\u57fa\u4e8e\u9aa8\u9abc\u7684\u51e0\u4f55\u6307\u5bfc\u548c\u6f0f\u6dbc\u6a21\u578b\u7684\u7eb9\u7406\u589e\u5f3a\uff0c\u89e3\u51b3\u624b\u7ed8\u5f62\u8c61\u52a8\u753b\u4e2d\u7684\u51e0\u4f55\u4e0d\u4e00\u81f4\u548c\u975e\u67d0\u6027\u53d8\u5f62\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u9aa8\u9abc\u52a8\u753b\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u975e\u67d0\u6027\u5143\u7d20\uff08\u5982\u5934\u53d1\u3001\u88d9\u636e\uff09\u65f6\u5bb9\u6613\u4ea7\u751f\u4e0d\u81ea\u7136\u53d8\u5f62\uff0c\u800c\u89c6\u9891\u6f0f\u6dbc\u6a21\u578b\u867d\u80fd\u751f\u6210\u771f\u5b9e\u52a8\u6001\u4f46\u5bb9\u6613\u9020\u6210\u98ce\u683c\u5316\u7ed8\u753b\u7684\u51e0\u4f55\u626d\u66f2\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u540c\u65f6\u4fdd\u6301\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u751f\u52a8\u8868\u73b0\u529b\u3002", "method": "1\uff09\u4f7f\u7528\u9aa8\u9abc\u52a8\u753b\u751f\u6210\u57fa\u7840\u56fe\u50cf\u4f5c\u4e3a\u51e0\u4f55\u6307\u5bfc 2\uff09\u901a\u8fc7\u57df\u9002\u914d\u6f0f\u6dbc\u6a21\u578b\u5c06\u7eb9\u7406\u589e\u5f3a\u6846\u67b6\u4e3a\u4fee\u590d\u4efb\u52a1 3\uff09\u4e8c\u7ea7\u52a8\u6001\u6ce8\u5165\uff08SDI\uff09\u7b56\u7565\uff1a\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u6ce8\u5165\u4eba\u4f53\u8fd0\u52a8\u5148\u9a8c\u77e5\u8bc6 4\uff09\u5934\u53d1\u5c42\u6b21\u5efa\u6a21\uff08HLM\uff09\u6280\u672f\uff1a\u4f7f\u7528\u5206\u5272\u5730\u56fe\u5c06\u5934\u53d1\u4e0e\u8eab\u4f53\u5206\u79bb\u52a8\u753b", "result": "\u7ecf\u8fc7\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8be5\u7cfb\u7edf\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u90fd\u8d85\u8fc7\u4e86\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u66f4\u81ea\u7136\u548c\u8868\u73b0\u529b\u66f4\u5f3a\u7684\u624b\u7ed8\u5f62\u8c61\u52a8\u753b\u3002", "conclusion": "\u8be5\u6df7\u5408\u52a8\u753b\u7cfb\u7edf\u6709\u6548\u5730\u7ed3\u5408\u4e86\u9aa8\u9abc\u52a8\u753b\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u4f18\u52bf\u548c\u6f0f\u6dbc\u6a21\u578b\u7684\u52a8\u6001\u751f\u6210\u80fd\u529b\uff0c\u901a\u8fc7\u4e8c\u7ea7\u52a8\u6001\u6ce8\u5165\u548c\u5934\u53d1\u5c42\u6b21\u5efa\u6a21\u6280\u672f\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u624b\u7ed8\u5f62\u8c61\u52a8\u753b\u4e2d\u7684\u5173\u952e\u6280\u672f\u6311\u6218\u3002"}}
{"id": "2509.05321", "pdf": "https://arxiv.org/pdf/2509.05321", "abs": "https://arxiv.org/abs/2509.05321", "authors": ["Yunfei Guo", "Tao Zhang", "Wu Huang", "Yao Song"], "title": "A Dataset Generation Scheme Based on Video2EEG-SPGN-Diffusion for SEED-VD", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper introduces an open-source framework, Video2EEG-SPGN-Diffusion, that leverages the SEED-VD dataset to generate a multimodal dataset of EEG signals conditioned on video stimuli. Additionally, we disclose an engineering pipeline for aligning video and EEG data pairs, facilitating the training of multimodal large models with EEG alignment capabilities. Personalized EEG signals are generated using a self-play graph network (SPGN) integrated with a diffusion model. As a major contribution, we release a new dataset comprising over 1000 samples of SEED-VD video stimuli paired with generated 62-channel EEG signals at 200 Hz and emotion labels, enabling video-EEG alignment and advancing multimodal research. This framework offers novel tools for emotion analysis, data augmentation, and brain-computer interface applications, with substantial research and engineering significance.", "AI": {"tldr": "Video2EEG-SPGN-Diffusion\u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u5229\u7528SEED-VD\u6570\u636e\u96c6\u751f\u6210\u57fa\u4e8e\u89c6\u9891\u523a\u6fc0\u7684\u591a\u6a21\u6001EEG\u4fe1\u53f7\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u89c6\u9891-EEG\u6570\u636e\u5bf9\u9f50\u7684\u5de5\u7a0b\u6d41\u7a0b\u3002", "motivation": "\u4e3a\u591a\u6a21\u6001\u5927\u6a21\u578b\u63d0\u4f9bEEG\u5bf9\u9f50\u80fd\u529b\uff0c\u63a8\u52a8\u89c6\u9891-EEG\u5bf9\u9f50\u7814\u7a76\uff0c\u652f\u6301\u60c5\u611f\u5206\u6790\u3001\u6570\u636e\u589e\u5f3a\u548c\u8111\u673a\u63a5\u53e3\u5e94\u7528\u3002", "method": "\u91c7\u7528\u81ea\u535a\u5f08\u56fe\u7f51\u7edc\uff08SPGN\uff09\u4e0e\u6269\u6563\u6a21\u578b\u7ed3\u5408\u7684\u65b9\u6cd5\u751f\u6210\u4e2a\u6027\u5316EEG\u4fe1\u53f7\uff0c\u6784\u5efa\u5305\u542b1000\u591a\u4e2a\u6837\u672c\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b62\u901a\u9053200Hz\u7684EEG\u4fe1\u53f7\u548c\u60c5\u611f\u6807\u7b7e\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u89c6\u9891-EEG\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u5de5\u7a0b\u6d41\u7a0b\uff0c\u652f\u6301\u591a\u6a21\u6001\u7814\u7a76\u548c\u5e94\u7528\u5f00\u53d1\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u60c5\u611f\u5206\u6790\u3001\u6570\u636e\u589e\u5f3a\u548c\u8111\u673a\u63a5\u53e3\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u548c\u65b9\u6cd5\uff0c\u5177\u6709\u91cd\u8981\u7684\u7814\u7a76\u548c\u5de5\u7a0b\u4ef7\u503c\u3002"}}
{"id": "2509.05342", "pdf": "https://arxiv.org/pdf/2509.05342", "abs": "https://arxiv.org/abs/2509.05342", "authors": ["Gaspard Beaudouin", "Minghan Li", "Jaeyeon Kim", "Sunghoon Yoon", "Mengyu Wang"], "title": "Delta Velocity Rectified Flow for Text-to-Image Editing", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We propose Delta Velocity Rectified Flow (DVRF), a novel inversion-free, path-aware editing framework within rectified flow models for text-to-image editing. DVRF is a distillation-based method that explicitly models the discrepancy between the source and target velocity fields in order to mitigate over-smoothing artifacts rampant in prior distillation sampling approaches. We further introduce a time-dependent shift term to push noisy latents closer to the target trajectory, enhancing the alignment with the target distribution. We theoretically demonstrate that when this shift is disabled, DVRF reduces to Delta Denoising Score, thereby bridging score-based diffusion optimization and velocity-based rectified-flow optimization. Moreover, when the shift term follows a linear schedule under rectified-flow dynamics, DVRF generalizes the Inversion-free method FlowEdit and provides a principled theoretical interpretation for it. Experimental results indicate that DVRF achieves superior editing quality, fidelity, and controllability while requiring no architectural modifications, making it efficient and broadly applicable to text-to-image editing tasks. Code is available at https://github.com/gaspardbd/DeltaVelocityRectifiedFlow.", "AI": {"tldr": "DVRF\u662f\u4e00\u79cd\u57fa\u4e8e\u84b8\u998f\u7684\u65e0\u53cd\u6f14\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u6e90\u548c\u76ee\u6807\u901f\u5ea6\u573a\u5dee\u5f02\u6765\u51cf\u5c11\u4f2a\u5f71\uff0c\u5e76\u5f15\u5165\u65f6\u95f4\u76f8\u5173\u504f\u79fb\u9879\u6765\u63d0\u5347\u76ee\u6807\u5206\u5e03\u5bf9\u9f50\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u84b8\u998f\u91c7\u6837\u65b9\u6cd5\u4e2d\u666e\u904d\u5b58\u5728\u7684\u8fc7\u5ea6\u5e73\u6ed1\u4f2a\u5f71\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u9ad8\u8d28\u91cf\u7684\u65e0\u53cd\u6f14\u6587\u672c\u5230\u56fe\u50cf\u7f16\u8f91\u3002", "method": "\u57fa\u4e8e\u6574\u6d41\u6d41\u6a21\u578b\u7684\u84b8\u998f\u65b9\u6cd5\uff0c\u663e\u5f0f\u5efa\u6a21\u6e90\u548c\u76ee\u6807\u901f\u5ea6\u573a\u5dee\u5f02\uff0c\u5f15\u5165\u65f6\u95f4\u76f8\u5173\u504f\u79fb\u9879\u5c06\u566a\u58f0\u6f5c\u5728\u7a7a\u95f4\u63a8\u5411\u76ee\u6807\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eDVRF\u5728\u7f16\u8f91\u8d28\u91cf\u3001\u4fdd\u771f\u5ea6\u548c\u53ef\u63a7\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u65e0\u9700\u67b6\u6784\u4fee\u6539\u5373\u53ef\u9ad8\u6548\u5e94\u7528\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u3002", "conclusion": "DVRF\u6210\u529f\u8fde\u63a5\u4e86\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u4f18\u5316\u548c\u57fa\u4e8e\u901f\u5ea6\u7684\u6574\u6d41\u6d41\u4f18\u5316\uff0c\u4e3aFlowEdit\u7b49\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u65e0\u53cd\u6f14\u56fe\u50cf\u7f16\u8f91\u3002"}}
{"id": "2509.05441", "pdf": "https://arxiv.org/pdf/2509.05441", "abs": "https://arxiv.org/abs/2509.05441", "authors": ["Tejaswini Medi", "Hsien-Yi Wang", "Arianna Rampini", "Margret Keuper"], "title": "FAVAE-Effective Frequency Aware Latent Tokenizer", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Latent generative models have shown remarkable progress in high-fidelity image synthesis, typically using a two-stage training process that involves compressing images into latent embeddings via learned tokenizers in the first stage. The quality of generation strongly depends on how expressive and well-optimized these latent embeddings are. While various methods have been proposed to learn effective latent representations, the reconstructed images often lack realism, particularly in textured regions with sharp transitions, due to loss of fine details governed by high frequencies. We conduct a detailed frequency decomposition of existing state-of-the-art (SOTA) latent tokenizers and show that conventional objectives inherently prioritize low-frequency reconstruction, often at the expense of high-frequency fidelity. Our analysis reveals these latent tokenizers exhibit a bias toward low-frequency information, when jointly optimized, leading to over-smoothed outputs and visual artifacts that diminish perceptual quality. To address this, we propose a wavelet-based, frequency-aware variational autoencoder (FA-VAE) framework that explicitly decouples the optimization of low- and high-frequency components. This decoupling enables improved reconstruction of fine textures while preserving global structure. Our approach bridges the fidelity gap in current latent tokenizers and emphasizes the importance of frequency-aware optimization for realistic image representation, with broader implications for applications in content creation, neural rendering, and medical imaging.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u73b0\u6709\u6f5c\u5728\u6807\u8bb0\u5668\u5728\u9891\u7387\u91cd\u5efa\u65b9\u9762\u7684\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u5c0f\u6ce2\u7684\u9891\u7387\u611f\u77e5\u53d8\u5206\u81ea\u7f16\u7801\u5668(FA-VAE)\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u4f4e\u9891\u548c\u9ad8\u9891\u7ec4\u4ef6\u7684\u4f18\u5316\u6765\u6539\u5584\u7eb9\u7406\u7ec6\u8282\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u6f5c\u5728\u751f\u6210\u6a21\u578b\u5728\u56fe\u50cf\u91cd\u5efa\u65f6\u5f80\u5f80\u7f3a\u4e4f\u771f\u5b9e\u611f\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u5c16\u9510\u8fc7\u6e21\u7684\u7eb9\u7406\u533a\u57df\uff0c\u56e0\u4e3a\u4f20\u7edf\u76ee\u6807\u51fd\u6570\u56fa\u6709\u5730\u4f18\u5148\u8003\u8651\u4f4e\u9891\u91cd\u5efa\uff0c\u800c\u727a\u7272\u4e86\u9ad8\u9891\u4fdd\u771f\u5ea6\uff0c\u5bfc\u81f4\u8f93\u51fa\u8fc7\u5ea6\u5e73\u6ed1\u548c\u89c6\u89c9\u4f2a\u5f71\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5c0f\u6ce2\u7684\u9891\u7387\u611f\u77e5\u53d8\u5206\u81ea\u7f16\u7801\u5668(FA-VAE)\u6846\u67b6\uff0c\u663e\u5f0f\u89e3\u8026\u4f4e\u9891\u548c\u9ad8\u9891\u7ec4\u4ef6\u7684\u4f18\u5316\uff0c\u901a\u8fc7\u9891\u7387\u611f\u77e5\u4f18\u5316\u6539\u5584\u7cbe\u7ec6\u7eb9\u7406\u7684\u91cd\u5efa\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u7ed3\u6784\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6539\u5584\u5f53\u524d\u6f5c\u5728\u6807\u8bb0\u5668\u7684\u4fdd\u771f\u5ea6\u5dee\u8ddd\uff0c\u5728\u7eb9\u7406\u7ec6\u8282\u91cd\u5efa\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u7ed3\u6784\u7684\u5b8c\u6574\u6027\u3002", "conclusion": "\u9891\u7387\u611f\u77e5\u4f18\u5316\u5bf9\u4e8e\u5b9e\u73b0\u771f\u5b9e\u56fe\u50cf\u8868\u793a\u81f3\u5173\u91cd\u8981\uff0c\u8be5\u65b9\u6cd5\u5728\u5185\u5bb9\u521b\u4f5c\u3001\u795e\u7ecf\u6e32\u67d3\u548c\u533b\u5b66\u6210\u50cf\u7b49\u5e94\u7528\u4e2d\u5177\u6709\u5e7f\u6cdb\u610f\u4e49\u3002"}}
{"id": "2509.05515", "pdf": "https://arxiv.org/pdf/2509.05515", "abs": "https://arxiv.org/abs/2509.05515", "authors": ["Sen Wang", "Kunyi Li", "Siyun Liang", "Elena Alegret", "Jing Ma", "Nassir Navab", "Stefano Gasperini"], "title": "Visibility-Aware Language Aggregation for Open-Vocabulary Segmentation in 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Recently, distilling open-vocabulary language features from 2D images into 3D Gaussians has attracted significant attention. Although existing methods achieve impressive language-based interactions of 3D scenes, we observe two fundamental issues: background Gaussians contributing negligibly to a rendered pixel get the same feature as the dominant foreground ones, and multi-view inconsistencies due to view-specific noise in language embeddings. We introduce Visibility-Aware Language Aggregation (VALA), a lightweight yet effective method that computes marginal contributions for each ray and applies a visibility-aware gate to retain only visible Gaussians. Moreover, we propose a streaming weighted geometric median in cosine space to merge noisy multi-view features. Our method yields a robust, view-consistent language feature embedding in a fast and memory-efficient manner. VALA improves open-vocabulary localization and segmentation across reference datasets, consistently surpassing existing works.", "AI": {"tldr": "VALA\u65b9\u6cd5\u901a\u8fc7\u53ef\u89c1\u6027\u611f\u77e5\u8bed\u8a00\u805a\u5408\u548c\u6d41\u5f0f\u52a0\u6743\u51e0\u4f55\u4e2d\u503c\u878d\u5408\uff0c\u89e3\u51b3\u4e863D\u9ad8\u65af\u5206\u5e03\u4e2d\u80cc\u666f\u9ad8\u65af\u566a\u58f0\u548c\u591a\u89c6\u89d2\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u8bcd\u6c47\u5b9a\u4f4d\u548c\u5206\u5272\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u57fa\u672c\u95ee\u9898\uff1a\u80cc\u666f\u9ad8\u65af\u5bf9\u6e32\u67d3\u50cf\u7d20\u8d21\u732e\u5f88\u5c0f\u5374\u83b7\u5f97\u4e0e\u524d\u666f\u76f8\u540c\u7684\u7279\u5f81\uff0c\u4ee5\u53ca\u8bed\u8a00\u5d4c\u5165\u4e2d\u89c6\u89d2\u7279\u5b9a\u566a\u58f0\u5bfc\u81f4\u7684\u591a\u89c6\u89d2\u4e0d\u4e00\u81f4\u6027", "method": "\u63d0\u51fa\u53ef\u89c1\u6027\u611f\u77e5\u8bed\u8a00\u805a\u5408(VALA)\uff0c\u8ba1\u7b97\u6bcf\u6761\u5149\u7ebf\u7684\u8fb9\u9645\u8d21\u732e\u5e76\u5e94\u7528\u53ef\u89c1\u6027\u611f\u77e5\u95e8\u63a7\u4fdd\u7559\u53ef\u89c1\u9ad8\u65af\uff1b\u63d0\u51fa\u5728\u4f59\u5f26\u7a7a\u95f4\u4e2d\u4f7f\u7528\u6d41\u5f0f\u52a0\u6743\u51e0\u4f55\u4e2d\u503c\u6765\u878d\u5408\u591a\u89c6\u89d2\u566a\u58f0\u7279\u5f81", "result": "VALA\u5728\u53c2\u8003\u6570\u636e\u96c6\u4e0a\u6539\u5584\u4e86\u5f00\u653e\u8bcd\u6c47\u5b9a\u4f4d\u548c\u5206\u5272\u6027\u80fd\uff0c\u4e00\u81f4\u8d85\u8d8a\u73b0\u6709\u5de5\u4f5c", "conclusion": "\u8be5\u65b9\u6cd5\u4ee5\u5feb\u901f\u4e14\u5185\u5b58\u9ad8\u6548\u7684\u65b9\u5f0f\u4ea7\u751f\u4e86\u9c81\u68d2\u3001\u89c6\u89d2\u4e00\u81f4\u7684\u8bed\u8a00\u7279\u5f81\u5d4c\u5165"}}
{"id": "2509.05554", "pdf": "https://arxiv.org/pdf/2509.05554", "abs": "https://arxiv.org/abs/2509.05554", "authors": ["Yihong Leng", "Siming Zheng", "Jinwei Chen", "Bo Li", "Jiaojiao Li", "Peng-Tao Jiang"], "title": "RED: Robust Event-Guided Motion Deblurring with Modality-Specific Disentangled Representation", "categories": ["cs.CV", "cs.IR"], "comment": null, "summary": "Event cameras provide sparse yet temporally high-temporal-resolution motion information, demonstrating great potential for motion deblurring. Existing methods focus on cross-modal interaction, overlooking the inherent incompleteness of event streams, which arises from the trade-off between sensitivity and noise introduced by the thresholding mechanism of Dynamic Vision Sensors (DVS). Such degradation compromises the integrity of motion priors and limits the effectiveness of event-guided deblurring. To tackle these challenges, we propose a Robust Event-guided Deblurring (RED) network with modality-specific disentangled representation. First, we introduce a Robustness-Oriented Perturbation Strategy (RPS) that applies random masking to events, which exposes RED to incomplete patterns and then foster robustness against various unknown scenario conditions.Next, a disentangled OmniAttention is presented to explicitly model intra-motion, inter-motion, and cross-modality correlations from two inherently distinct but complementary sources: blurry images and partially disrupted events. Building on these reliable features, two interactive modules are designed to enhance motion-sensitive areas in blurry images and inject semantic context into incomplete event representations. Extensive experiments on synthetic and real-world datasets demonstrate RED consistently achieves state-of-the-art performance in both accuracy and robustness.", "AI": {"tldr": "\u901a\u8fc7\u6a21\u6001\u7279\u5f02\u89e3\u8026\u8868\u5f81\u548c\u968f\u673a\u6295\u5f71\u7b56\u7565\uff0c\u7f51\u7edc\u80fd\u591f\u5728\u4e8b\u4ef6\u6d41\u4e0d\u5b8c\u6574\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u7a33\u5065\u7684\u8fd0\u52a8\u53bb\u6a21\u7cca\u6548\u679c", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u4e8b\u4ef6\u6d41\u7684\u672c\u8d28\u6027\u4e0d\u5b8c\u6574\u6027\uff0c\u8fd9\u79cd\u9000\u5316\u5f71\u54cd\u8fd0\u52a8\u5148\u9a8c\u77e5\u8bc6\u7684\u5b8c\u6574\u6027\u5e76\u9650\u5236\u4e8b\u4ef6\u5bfc\u5411\u53bb\u6a21\u7cca\u7684\u6548\u679c", "method": "\u63d0\u51fa\u7a33\u5065\u6027\u5bfc\u5411\u7684\u968f\u673a\u6295\u5f71\u7b56\u7565(RPS)\u901a\u8fc7\u968f\u673a\u63a9\u7801\u4e8b\u4ef6\u6765\u57f9\u517b\u7f51\u7edc\u5bf9\u4e0d\u5b8c\u6574\u6a21\u5f0f\u7684\u9002\u5e94\u6027\uff1b\u8bbe\u8ba1\u89e3\u8026\u7684OmniAttention\u6a21\u5757\u6765\u663e\u5f0f\u5efa\u6a21\u5185\u90e8\u8fd0\u52a8\u3001\u8fd0\u52a8\u95f4\u548c\u8de8\u6a21\u6001\u76f8\u5173\u6027\uff1b\u4f7f\u7528\u4e24\u4e2a\u4ea4\u4e92\u6a21\u5757\u6765\u589e\u5f3a\u6a21\u7cca\u56fe\u50cf\u4e2d\u7684\u8fd0\u52a8\u654f\u611f\u533a\u57df\u5e76\u5411\u4e8b\u4ef6\u8868\u5f81\u6ce8\u5165\u8bed\u4e49\u4e0a\u4e0b\u6587", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cRED\u7f51\u7edc\u5728\u51c6\u786e\u6027\u548c\u7a33\u5065\u6027\u65b9\u9762\u90fd\u4e00\u81f4\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "\u901a\u8fc7\u5904\u7406\u4e8b\u4ef6\u6d41\u7684\u672c\u8d28\u6027\u4e0d\u5b8c\u6574\u6027\uff0c\u7f51\u7edc\u80fd\u591f\u5728\u5404\u79cd\u672a\u77e5\u573a\u666f\u6761\u4ef6\u4e0b\u5b9e\u73b0\u66f4\u7a33\u5065\u548c\u6709\u6548\u7684\u8fd0\u52a8\u53bb\u6a21\u7cca"}}
{"id": "2509.05659", "pdf": "https://arxiv.org/pdf/2509.05659", "abs": "https://arxiv.org/abs/2509.05659", "authors": ["Guandong Li", "Zhaobin Chu"], "title": "EditIDv2: Editable ID Customization with Data-Lubricated ID Feature Integration for Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "We propose EditIDv2, a tuning-free solution specifically designed for high-complexity narrative scenes and long text inputs. Existing character editing methods perform well under simple prompts, but often suffer from degraded editing capabilities, semantic understanding biases, and identity consistency breakdowns when faced with long text narratives containing multiple semantic layers, temporal logic, and complex contextual relationships. In EditID, we analyzed the impact of the ID integration module on editability. In EditIDv2, we further explore and address the influence of the ID feature integration module. The core of EditIDv2 is to discuss the issue of editability injection under minimal data lubrication. Through a sophisticated decomposition of PerceiverAttention, the introduction of ID loss and joint dynamic training with the diffusion model, as well as an offline fusion strategy for the integration module, we achieve deep, multi-level semantic editing while maintaining identity consistency in complex narrative environments using only a small amount of data lubrication. This meets the demands of long prompts and high-quality image generation, and achieves excellent results in the IBench evaluation.", "AI": {"tldr": "EditIDv2\u662f\u4e00\u4e2a\u65e0\u9700\u8c03\u4f18\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e13\u95e8\u9488\u5bf9\u9ad8\u590d\u6742\u5ea6\u53d9\u4e8b\u573a\u666f\u548c\u957f\u6587\u672c\u8f93\u5165\u8bbe\u8ba1\uff0c\u901a\u8fc7\u6539\u8fdbID\u7279\u5f81\u6574\u5408\u6a21\u5757\u548c\u5f15\u5165\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u6df1\u5ea6\u591a\u5c42\u7ea7\u8bed\u4e49\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u89d2\u8272\u7f16\u8f91\u65b9\u6cd5\u5728\u7b80\u5355\u63d0\u793a\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9762\u5bf9\u5305\u542b\u591a\u8bed\u4e49\u5c42\u3001\u65f6\u95f4\u903b\u8f91\u548c\u590d\u6742\u4e0a\u4e0b\u6587\u5173\u7cfb\u7684\u957f\u6587\u672c\u53d9\u4e8b\u65f6\uff0c\u5f80\u5f80\u51fa\u73b0\u7f16\u8f91\u80fd\u529b\u4e0b\u964d\u3001\u8bed\u4e49\u7406\u89e3\u504f\u5dee\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u5d29\u6e83\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7PerceiverAttention\u7684\u7cbe\u7ec6\u5206\u89e3\u3001\u5f15\u5165ID\u635f\u5931\u548c\u4e0e\u6269\u6563\u6a21\u578b\u7684\u8054\u5408\u52a8\u6001\u8bad\u7ec3\uff0c\u4ee5\u53ca\u6574\u5408\u6a21\u5757\u7684\u79bb\u7ebf\u878d\u5408\u7b56\u7565\uff0c\u5728\u5c11\u91cf\u6570\u636e\u6da6\u6ed1\u4e0b\u5b9e\u73b0\u53ef\u7f16\u8f91\u6027\u6ce8\u5165\u3002", "result": "\u5728\u590d\u6742\u53d9\u4e8b\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u6df1\u5ea6\u591a\u5c42\u7ea7\u8bed\u4e49\u7f16\u8f91\uff0c\u540c\u65f6\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u6ee1\u8db3\u957f\u63d0\u793a\u548c\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u7684\u9700\u6c42\uff0c\u5728IBench\u8bc4\u4f30\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7ed3\u679c\u3002", "conclusion": "EditIDv2\u6210\u529f\u89e3\u51b3\u4e86\u957f\u6587\u672c\u53d9\u4e8b\u573a\u666f\u4e0b\u7684\u89d2\u8272\u7f16\u8f91\u6311\u6218\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u8bbe\u8ba1\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u6700\u5c0f\u6570\u636e\u9700\u6c42\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u8bed\u4e49\u7f16\u8f91\u548c\u8eab\u4efd\u4fdd\u6301\u3002"}}
{"id": "2509.05662", "pdf": "https://arxiv.org/pdf/2509.05662", "abs": "https://arxiv.org/abs/2509.05662", "authors": ["Wasikul Islam"], "title": "WIPUNet: A Physics-inspired Network with Weighted Inductive Biases for Image Denoising", "categories": ["cs.CV", "hep-ex"], "comment": "13 pages, 4 figures", "summary": "In high-energy particle physics, collider measurements are contaminated by \"pileup\", overlapping soft interactions that obscure the hard-scatter signal of interest. Dedicated subtraction strategies exploit physical priors such as conservation, locality, and isolation. Inspired by this analogy, we investigate how such principles can inform image denoising by embedding physics-guided inductive biases into neural architectures. This paper is a proof of concept: rather than targeting state-of-the-art (SOTA) benchmarks, we ask whether physics-inspired priors improve robustness under strong corruption.   We introduce a hierarchy of PU-inspired denoisers: a residual CNN with conservation constraints, its Gaussian-noise variants, and the Weighted Inductive Pileup-physics-inspired U-Network for Denoising (WIPUNet), which integrates these ideas into a UNet backbone. On CIFAR-10 with Gaussian noise at $\\sigma\\in\\{15,25,50,75,100\\}$, PU-inspired CNNs are competitive with standard baselines, while WIPUNet shows a \\emph{widening margin} at higher noise. Complementary BSD500 experiments show the same trend, suggesting physics-inspired priors provide stability where purely data-driven models degrade. Our contributions are: (i) translating pileup-mitigation principles into modular inductive biases; (ii) integrating them into UNet; and (iii) demonstrating robustness gains at high noise without relying on heavy SOTA machinery.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5c06\u9ad8\u80fd\u7269\u7406\u4e2d\u7684\u5806\u79ef\u5e72\u6270\u51cf\u5c11\u539f\u7406\u8f6c\u5316\u4e3a\u56fe\u50cf\u53bb\u566a\u7684\u7269\u7406\u5bfc\u5411\u5f52\u7eb3\u504f\u89c1\uff0c\u63d0\u51faWIPUNet\u7f51\u7edc\uff0c\u5728\u9ad8\u566a\u58f0\u6761\u4ef6\u4e0b\u663e\u793a\u4e86\u66f4\u597d\u7684\u7a33\u5065\u6027\u3002", "motivation": "\u53d7\u9ad8\u80fd\u7269\u7406\u4e2d\u5806\u79ef\u5e72\u6270\u51cf\u5c11\u7b56\u7565\u7684\u542f\u53d1\uff0c\u901a\u8fc7\u5d4c\u5165\u7269\u7406\u5bfc\u5411\u7684\u5f52\u7eb3\u504f\u89c1\uff08\u5982\u5b88\u6052\u3001\u5c40\u57df\u6027\u3001\u9694\u79bb\u6027\uff09\u6765\u63d0\u5347\u56fe\u50cf\u53bb\u566a\u6a21\u578b\u5728\u5f3a\u5ea6\u6c61\u67d3\u4e0b\u7684\u7a33\u5065\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u5806\u79ef\u5e72\u6270\u53bb\u566a\u5668\uff1a\u5305\u542b\u5b88\u6052\u7ea6\u675f\u7684\u6e05\u6d41CNN\u3001\u5176\u9ad8\u65af\u566a\u58f0\u53d8\u79cd\uff0c\u4ee5\u53caWIPUNet\uff08\u7ed3\u5408\u7269\u7406\u504f\u89c1\u7684UNet\u7ed3\u6784\uff09\u3002\u5728CIFAR-10\u548cBSD500\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5728\u9ad8\u65af\u566a\u58f0\u6761\u4ef6\u4e0b\uff08\u03c3=15-100\uff09\uff0cPU\u53bb\u566a\u5668\u4e0e\u6807\u51c6\u57fa\u7ebf\u76f8\u5f53\uff0c\u800cWIPUNet\u5728\u9ad8\u566a\u58f0\u6c34\u5e73\u4e0b\u663e\u793a\u51f8\u51fa\u4f18\u52bf\uff0c\u8bf4\u660e\u7269\u7406\u504f\u89c1\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u7a33\u5065\u6027\u3002", "conclusion": "\u7269\u7406\u53d7\u542f\u53d1\u7684\u5f52\u7eb3\u504f\u89c1\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u590d\u6742SOTA\u673a\u5236\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u56fe\u50cf\u53bb\u566a\u6a21\u578b\u5e26\u6765\u5728\u9ad8\u5ea6\u6c61\u67d3\u6761\u4ef6\u4e0b\u7684\u7a33\u5065\u6027\u6539\u5584\u3002"}}
{"id": "2509.05695", "pdf": "https://arxiv.org/pdf/2509.05695", "abs": "https://arxiv.org/abs/2509.05695", "authors": ["Jingwei Peng", "Zhixuan Qiu", "Boyu Jin", "Surasakdi Siripong"], "title": "Leveraging Vision-Language Large Models for Interpretable Video Action Recognition with Semantic Tokenization", "categories": ["cs.CV"], "comment": null, "summary": "Human action recognition often struggles with deep semantic understanding, complex contextual information, and fine-grained distinction, limitations that traditional methods frequently encounter when dealing with diverse video data. Inspired by the remarkable capabilities of large language models, this paper introduces LVLM-VAR, a novel framework that pioneers the application of pre-trained Vision-Language Large Models (LVLMs) to video action recognition, emphasizing enhanced accuracy and interpretability. Our method features a Video-to-Semantic-Tokens (VST) Module, which innovatively transforms raw video sequences into discrete, semantically and temporally consistent \"semantic action tokens,\" effectively crafting an \"action narrative\" that is comprehensible to an LVLM. These tokens, combined with natural language instructions, are then processed by a LoRA-fine-tuned LVLM (e.g., LLaVA-13B) for robust action classification and semantic reasoning. LVLM-VAR not only achieves state-of-the-art or highly competitive performance on challenging benchmarks such as NTU RGB+D and NTU RGB+D 120, demonstrating significant improvements (e.g., 94.1% on NTU RGB+D X-Sub and 90.0% on NTU RGB+D 120 X-Set), but also substantially boosts model interpretability by generating natural language explanations for its predictions.", "AI": {"tldr": "LVLM-VAR\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u5927\u6a21\u578b\u5c06\u89c6\u9891\u8f6c\u6362\u4e3a\u8bed\u4e49\u52a8\u4f5c\u6807\u8bb0\uff0c\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8fdb\u884c\u52a8\u4f5c\u5206\u7c7b\u548c\u8bed\u4e49\u63a8\u7406\uff0c\u5728\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u4eba\u7c7b\u52a8\u4f5c\u8bc6\u522b\u65b9\u6cd5\u5728\u5904\u7406\u591a\u6837\u5316\u89c6\u9891\u6570\u636e\u65f6\uff0c\u7ecf\u5e38\u9762\u4e34\u6df1\u5ea6\u8bed\u4e49\u7406\u89e3\u3001\u590d\u6742\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u7ec6\u7c92\u5ea6\u533a\u5206\u7684\u6311\u6218\u3002\u53d7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5f3a\u5927\u80fd\u529b\u7684\u542f\u53d1\uff0c\u672c\u6587\u65e8\u5728\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u5927\u6a21\u578b\u6765\u63d0\u5347\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86Video-to-Semantic-Tokens (VST)\u6a21\u5757\uff0c\u5c06\u539f\u59cb\u89c6\u9891\u5e8f\u5217\u8f6c\u6362\u4e3a\u79bb\u6563\u7684\u3001\u8bed\u4e49\u548c\u65f6\u95f4\u4e00\u81f4\u7684\"\u8bed\u4e49\u52a8\u4f5c\u6807\u8bb0\"\uff0c\u521b\u5efaLVLM\u53ef\u7406\u89e3\u7684\"\u52a8\u4f5c\u53d9\u8ff0\"\u3002\u8fd9\u4e9b\u6807\u8bb0\u4e0e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e00\u8d77\u7531LoRA\u5fae\u8c03\u7684LVLM\uff08\u5982LLaVA-13B\uff09\u5904\u7406\uff0c\u8fdb\u884c\u52a8\u4f5c\u5206\u7c7b\u548c\u8bed\u4e49\u63a8\u7406\u3002", "result": "\u5728NTU RGB+D\u548cNTU RGB+D 120\u7b49\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6216\u6781\u5177\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff1aNTU RGB+D X-Sub\u8fbe\u523094.1%\uff0cNTU RGB+D 120 X-Set\u8fbe\u523090.0%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "LVLM-VAR\u4e0d\u4ec5\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u901a\u8fc7\u751f\u6210\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u5927\u5927\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.05728", "pdf": "https://arxiv.org/pdf/2509.05728", "abs": "https://arxiv.org/abs/2509.05728", "authors": ["Niels Balemans", "Ali Anwar", "Jan Steckel", "Siegfried Mercelis"], "title": "LiDAR-BIND-T: Improving SLAM with Temporally Consistent Cross-Modal LiDAR Reconstruction", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "This paper extends LiDAR-BIND, a modular multi-modal fusion framework that binds heterogeneous sensors (radar, sonar) to a LiDAR-defined latent space, with mechanisms that explicitly enforce temporal consistency. We introduce three contributions: (i) temporal embedding similarity that aligns consecutive latents, (ii) a motion-aligned transformation loss that matches displacement between predictions and ground truth LiDAR, and (iii) windows temporal fusion using a specialised temporal module. We further update the model architecture to better preserve spatial structure. Evaluations on radar/sonar-to-LiDAR translation demonstrate improved temporal and spatial coherence, yielding lower absolute trajectory error and better occupancy map accuracy in Cartographer-based SLAM (Simultaneous Localisation and Mapping). We propose different metrics based on the Fr\\'echet Video Motion Distance (FVMD) and a correlation-peak distance metric providing practical temporal quality indicators to evaluate SLAM performance. The proposed temporal LiDAR-BIND, or LiDAR-BIND-T, maintains plug-and-play modality fusion while substantially enhancing temporal stability, resulting in improved robustness and performance for downstream SLAM.", "AI": {"tldr": "LiDAR-BIND-T\u6269\u5c55\u4e86\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u4e00\u81f4\u6027\u673a\u5236\uff0c\u5728\u96f7\u8fbe/\u58f0\u7eb3\u5230LiDAR\u7684\u8f6c\u6362\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u6539\u5584\u4e86SLAM\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u4f20\u611f\u5668\u878d\u5408\u65b9\u6cd5\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5f71\u54cd\u4e86\u4e0b\u6e38SLAM\u4efb\u52a1\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u663e\u5f0f\u5f3a\u5236\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u4e2a\u4e3b\u8981\u8d21\u732e\uff1a(i)\u65f6\u95f4\u5d4c\u5165\u76f8\u4f3c\u6027\u5bf9\u9f50\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\uff0c(ii)\u8fd0\u52a8\u5bf9\u9f50\u53d8\u6362\u635f\u5931\u5339\u914d\u9884\u6d4b\u4e0e\u771f\u5b9eLiDAR\u7684\u4f4d\u79fb\uff0c(iii)\u7a97\u53e3\u65f6\u95f4\u878d\u5408\u4f7f\u7528\u4e13\u95e8\u7684\u65f6\u95f4\u6a21\u5757\uff0c\u5e76\u66f4\u65b0\u6a21\u578b\u67b6\u6784\u4ee5\u66f4\u597d\u5730\u4fdd\u6301\u7a7a\u95f4\u7ed3\u6784\u3002", "result": "\u5728\u96f7\u8fbe/\u58f0\u7eb3\u5230LiDAR\u7684\u8f6c\u6362\u8bc4\u4f30\u4e2d\u663e\u793a\u51fa\u6539\u8fdb\u7684\u65f6\u95f4\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u964d\u4f4e\u4e86\u7edd\u5bf9\u8f68\u8ff9\u8bef\u5dee\uff0c\u63d0\u9ad8\u4e86\u57fa\u4e8eCartographer\u7684SLAM\u7684\u5360\u7528\u5730\u56fe\u7cbe\u5ea6\u3002", "conclusion": "LiDAR-BIND-T\u4fdd\u6301\u4e86\u5373\u63d2\u5373\u7528\u7684\u6a21\u6001\u878d\u5408\u80fd\u529b\uff0c\u540c\u65f6\u663e\u8457\u589e\u5f3a\u4e86\u65f6\u95f4\u7a33\u5b9a\u6027\uff0c\u4e3a\u4e0b\u6e38SLAM\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2509.05746", "pdf": "https://arxiv.org/pdf/2509.05746", "abs": "https://arxiv.org/abs/2509.05746", "authors": ["Tianhao Guo", "Bingjie Lu", "Feng Wang", "Zhengyang Lu"], "title": "Depth-Aware Super-Resolution via Distance-Adaptive Variational Formulation", "categories": ["cs.CV"], "comment": null, "summary": "Single image super-resolution traditionally assumes spatially-invariant degradation models, yet real-world imaging systems exhibit complex distance-dependent effects including atmospheric scattering, depth-of-field variations, and perspective distortions. This fundamental limitation necessitates spatially-adaptive reconstruction strategies that explicitly incorporate geometric scene understanding for optimal performance. We propose a rigorous variational framework that characterizes super-resolution as a spatially-varying inverse problem, formulating the degradation operator as a pseudodifferential operator with distance-dependent spectral characteristics that enable theoretical analysis of reconstruction limits across depth ranges. Our neural architecture implements discrete gradient flow dynamics through cascaded residual blocks with depth-conditional convolution kernels, ensuring convergence to stationary points of the theoretical energy functional while incorporating learned distance-adaptive regularization terms that dynamically adjust smoothness constraints based on local geometric structure. Spectral constraints derived from atmospheric scattering theory prevent bandwidth violations and noise amplification in far-field regions, while adaptive kernel generation networks learn continuous mappings from depth to reconstruction filters. Comprehensive evaluation across five benchmark datasets demonstrates state-of-the-art performance, achieving 36.89/0.9516 and 30.54/0.8721 PSNR/SSIM at 2 and 4 scales on KITTI outdoor scenes, outperforming existing methods by 0.44dB and 0.36dB respectively. This work establishes the first theoretically-grounded distance-adaptive super-resolution framework and demonstrates significant improvements on depth-variant scenarios while maintaining competitive performance across traditional benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7406\u8bba\u57fa\u7840\u7684\u6df1\u5ea6\u81ea\u9002\u5e94\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u8ddd\u79bb\u76f8\u5173\u7684\u4f2a\u5fae\u5206\u7b97\u5b50\u5efa\u6a21\u9000\u5316\u8fc7\u7a0b\uff0c\u4f7f\u7528\u6df1\u5ea6\u6761\u4ef6\u5377\u79ef\u6838\u5b9e\u73b0\u7a7a\u95f4\u81ea\u9002\u5e94\u91cd\u5efa\uff0c\u5728\u6df1\u5ea6\u53d8\u5316\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u5047\u8bbe\u7a7a\u95f4\u4e0d\u53d8\u7684\u9000\u5316\u6a21\u578b\uff0c\u4f46\u771f\u5b9e\u6210\u50cf\u7cfb\u7edf\u5b58\u5728\u5927\u6c14\u6563\u5c04\u3001\u666f\u6df1\u53d8\u5316\u548c\u900f\u89c6\u7578\u53d8\u7b49\u590d\u6742\u8ddd\u79bb\u76f8\u5173\u6548\u5e94\uff0c\u9700\u8981\u7a7a\u95f4\u81ea\u9002\u5e94\u91cd\u5efa\u7b56\u7565\u3002", "method": "\u53d8\u5206\u6846\u67b6\u5c06\u8d85\u5206\u8fa8\u7387\u5efa\u6a21\u4e3a\u7a7a\u95f4\u53d8\u5316\u7684\u9006\u95ee\u9898\uff0c\u9000\u5316\u7b97\u5b50\u4e3a\u8ddd\u79bb\u76f8\u5173\u7684\u4f2a\u5fae\u5206\u7b97\u5b50\u3002\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u901a\u8fc7\u7ea7\u8054\u6b8b\u5dee\u5757\u5b9e\u73b0\u79bb\u6563\u68af\u5ea6\u6d41\u52a8\u529b\u5b66\uff0c\u4f7f\u7528\u6df1\u5ea6\u6761\u4ef6\u5377\u79ef\u6838\uff0c\u7ed3\u5408\u5b66\u4e60\u5230\u7684\u8ddd\u79bb\u81ea\u9002\u5e94\u6b63\u5219\u5316\u9879\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0cKITTI\u6237\u5916\u573a\u666f2\u500d\u548c4\u500d\u7f29\u653e\u5206\u522b\u83b7\u5f9736.89/0.9516\u548c30.54/0.8721\u7684PSNR/SSIM\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u5206\u522b\u9ad8\u51fa0.44dB\u548c0.36dB\u3002", "conclusion": "\u5efa\u7acb\u4e86\u9996\u4e2a\u7406\u8bba\u57fa\u7840\u7684\u6df1\u5ea6\u81ea\u9002\u5e94\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u5728\u6df1\u5ea6\u53d8\u5316\u573a\u666f\u4e2d\u663e\u8457\u6539\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u5728\u4f20\u7edf\u57fa\u51c6\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\u3002"}}
{"id": "2509.05952", "pdf": "https://arxiv.org/pdf/2509.05952", "abs": "https://arxiv.org/abs/2509.05952", "authors": ["Feng Wang", "Zihao Yu"], "title": "Coefficients-Preserving Sampling for Reinforcement Learning with Flow Matching", "categories": ["cs.CV"], "comment": "work in progress", "summary": "Reinforcement Learning (RL) has recently emerged as a powerful technique for improving image and video generation in Diffusion and Flow Matching models, specifically for enhancing output quality and alignment with prompts. A critical step for applying online RL methods on Flow Matching is the introduction of stochasticity into the deterministic framework, commonly realized by Stochastic Differential Equation (SDE). Our investigation reveals a significant drawback to this approach: SDE-based sampling introduces pronounced noise artifacts in the generated images, which we found to be detrimental to the reward learning process. A rigorous theoretical analysis traces the origin of this noise to an excess of stochasticity injected during inference. To address this, we draw inspiration from Denoising Diffusion Implicit Models (DDIM) to reformulate the sampling process. Our proposed method, Coefficients-Preserving Sampling (CPS), eliminates these noise artifacts. This leads to more accurate reward modeling, ultimately enabling faster and more stable convergence for reinforcement learning-based optimizers like Flow-GRPO and Dance-GRPO. Code will be released at https://github.com/IamCreateAI/FlowCPS", "AI": {"tldr": "\u901a\u8fc7CPS\u91c7\u6837\u65b9\u6cd5\u6d88\u9664SDE\u57fa\u4e8e\u6d41\u5339\u914d\u6a21\u578b\u4e2d\u7684\u566a\u58f0\u9664\u7591\uff0c\u63d0\u9ad8\u5956\u52b1\u5b66\u4e60\u6548\u679c\u548c\u751f\u6210\u8d28\u91cf", "motivation": "\u89e3\u51b3SDE\u91c7\u6837\u5728\u6d41\u5339\u914d\u6a21\u578b\u4e2d\u5f15\u5165\u7684\u566a\u58f0\u9664\u7591\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u5bf9\u52b1\u5f3a\u5b66\u4e60\u5956\u52b1\u5b66\u4e60\u8fc7\u7a0b\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd", "method": "\u63d0\u51faCoefficients-Preserving Sampling (CPS)\u65b9\u6cd5\uff0c\u53d7DDIM\u542f\u53d1\u91cd\u6784\u91c7\u6837\u8fc7\u7a0b\uff0c\u6d88\u9664\u8fc7\u591a\u968f\u673a\u6027\u5bfc\u81f4\u7684\u566a\u58f0", "result": "CPS\u6d88\u9664\u4e86\u566a\u58f0\u9664\u7591\uff0c\u63d0\u9ad8\u4e86\u5956\u52b1\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u4fc3\u8fdb\u4e86Flow-GRPO\u548cDance-GRPO\u7b49RL\u4f18\u5316\u5668\u7684\u66f4\u5feb\u66f4\u7a33\u5b9a\u6536\u655b", "conclusion": "CPS\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86SDE\u91c7\u6837\u7684\u566a\u58f0\u95ee\u9898\uff0c\u4e3a\u6d41\u5339\u914d\u6a21\u578b\u7684\u52b1\u5f3a\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u57fa\u7840"}}
{"id": "2509.05953", "pdf": "https://arxiv.org/pdf/2509.05953", "abs": "https://arxiv.org/abs/2509.05953", "authors": ["Jeonghyun Noh", "Wangsu Jeon", "Jinsun Park"], "title": "Dual Interaction Network with Cross-Image Attention for Medical Image Segmentation", "categories": ["cs.CV"], "comment": "16pages", "summary": "Medical image segmentation is a crucial method for assisting professionals in diagnosing various diseases through medical imaging. However, various factors such as noise, blurriness, and low contrast often hinder the accurate diagnosis of diseases. While numerous image enhancement techniques can mitigate these issues, they may also alter crucial information needed for accurate diagnosis in the original image. Conventional image fusion strategies, such as feature concatenation can address this challenge. However, they struggle to fully leverage the advantages of both original and enhanced images while suppressing the side effects of the enhancements. To overcome the problem, we propose a dual interactive fusion module (DIFM) that effectively exploits mutual complementary information from the original and enhanced images. DIFM employs cross-attention bidirectionally to simultaneously attend to corresponding spatial information across different images, subsequently refining the complementary features via global spatial attention. This interaction leverages low- to high-level features implicitly associated with diverse structural attributes like edges, blobs, and object shapes, resulting in enhanced features that embody important spatial characteristics. In addition, we introduce a multi-scale boundary loss based on gradient extraction to improve segmentation accuracy at object boundaries. Experimental results on the ACDC and Synapse datasets demonstrate the superiority of the proposed method quantitatively and qualitatively. Code available at: https://github.com/JJeong-Gari/DIN", "AI": {"tldr": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u53cc\u5411\u4ea4\u4e92\u878d\u5408\u6a21\u5757(DIFM)\uff0c\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u5229\u7528\u539f\u59cb\u56fe\u50cf\u548c\u589e\u5f3a\u56fe\u50cf\u7684\u4e92\u8865\u4fe1\u606f\uff0c\u63d0\u9ad8\u75be\u75c5\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u566a\u58f0\u3001\u6a21\u7cca\u3001\u5bf9\u6bd4\u5ea6\u4f4e\u7b49\u95ee\u9898\u5f71\u54cd\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u800c\u4f20\u7edf\u56fe\u50cf\u589e\u5f3a\u6280\u672f\u53ef\u80fd\u4f1a\u4fee\u6539\u539f\u59cb\u56fe\u50cf\u7684\u5173\u952e\u4fe1\u606f\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5145\u5206\u5229\u7528\u4e24\u79cd\u56fe\u50cf\u4f18\u52bf\u7684\u878d\u5408\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u53cc\u5411\u4ea4\u4e92\u878d\u5408\u6a21\u5757(DIFM)\uff0c\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u673a\u5236\u53cc\u5411\u5173\u6ce8\u4e0d\u540c\u56fe\u50cf\u95f4\u7684\u5bf9\u5e94\u7a7a\u95f4\u4fe1\u606f\uff0c\u5e76\u4f7f\u7528\u5168\u5c40\u7a7a\u95f4\u6ce8\u610f\u529b\u7cbe\u70bc\u4e92\u8865\u7279\u5f81\u3002\u540c\u65f6\u91c7\u7528\u57fa\u4e8e\u68af\u5ea6\u63d0\u53d6\u7684\u591a\u5c3a\u5ea8\u8fb9\u754c\u635f\u5931\u51fd\u6570\u6765\u63d0\u9ad8\u8fb9\u754c\u5206\u5272\u51c6\u786e\u6027\u3002", "result": "\u5728ACDC\u548cSynapse\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u65b9\u6cd5\u5728\u6570\u91cf\u548c\u8d28\u91cf\u4e0a\u90fd\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5229\u7528\u539f\u59cb\u56fe\u50cf\u548c\u589e\u5f3a\u56fe\u50cf\u7684\u4e92\u8865\u4fe1\u606f\uff0c\u63d0\u9ad8\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u8fb9\u754c\u533a\u57df\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2509.05963", "pdf": "https://arxiv.org/pdf/2509.05963", "abs": "https://arxiv.org/abs/2509.05963", "authors": ["Rafal Karp", "Dawid Gruszka", "Tomasz Trzcinski"], "title": "Neural Bloom: A Deep Learning Approach to Real-Time Lighting", "categories": ["cs.CV"], "comment": null, "summary": "We propose a novel method to generate bloom lighting effect in real time using neural networks. Our solution generate brightness mask from given 3D scene view up to 30% faster than state-of-the-art methods. The existing traditional techniques rely on multiple blur appliances and texture sampling, also very often have existing conditional branching in its implementation. These operations occupy big portion of the execution time. We solve this problem by proposing two neural network-based bloom lighting methods, Neural Bloom Lighting (NBL) and Fast Neural Bloom Lighting (FastNBL), focusing on their quality and performance. Both methods were tested on a variety of 3D scenes, with evaluations conducted on brightness mask accuracy and inference speed. The main contribution of this work is that both methods produce high-quality bloom effects while outperforming the standard state-of-the-art bloom implementation, with FastNBL being faster by 28% and NBL faster by 12%. These findings highlight that we can achieve realistic bloom lighting phenomena faster, moving us towards more realism in real-time environments in the future. This improvement saves computational resources, which is a major bottleneck in real-time rendering. Furthermore, it is crucial for sustaining immersion and ensuring smooth experiences in high FPS environments, while maintaining high-quality realism.", "AI": {"tldr": "\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u5b9e\u65f6\u751f\u6210\u6e05\u6670\u7684\u714c\u5149\u706f\u5149\u6548\u679c\uff0c\u65b9\u6cd5\u901f\u5ea6\u6bd4\u73b0\u6709\u6280\u672f\u63d0\u9ad830%\uff0c\u8282\u7701\u8ba1\u7b97\u8d44\u6e90", "motivation": "\u4f20\u7edf\u714c\u5149\u706f\u5149\u6280\u672f\u4f9d\u9760\u591a\u6b21\u6a21\u7cca\u548c\u7eb9\u7406\u91c7\u6837\uff0c\u5305\u542b\u6761\u4ef6\u5206\u652f\uff0c\u5360\u7528\u5927\u91cf\u6267\u884c\u65f6\u95f4\uff0c\u6210\u4e3a\u5b9e\u65f6\u6e32\u67d3\u7684\u6025\u9700\u89e3\u51b3\u7684\u6025\u9700\u89e3\u51b3\u7684\u7b49\u95ee\u9898", "method": "\u63d0\u51fa\u4e24\u79cd\u795e\u7ecf\u7f51\u7edc\u57fa\u714c\u5149\u706f\u5149\u65b9\u6cd5\uff1aNeural Bloom Lighting (NBL) \u548c Fast Neural Bloom Lighting (FastNBL)\uff0c\u5206\u522b\u91cd\u70b9\u5173\u6ce8\u8d28\u91cf\u548c\u6027\u80fd", "result": "\u5728\u591a\u79cd3D\u573a\u666f\u4e2d\u8fdb\u884c\u6d4b\u8bd5\uff0cFastNBL\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6848\u5feb28%\uff0cNBL\u5feb12%\uff0c\u540c\u65f6\u4ea7\u751f\u9ad8\u8d28\u91cf\u714c\u5149\u6548\u679c", "conclusion": "\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u80fd\u591f\u66f4\u5feb\u901f\u5730\u5b9e\u73b0\u73b0\u5b9e\u7684\u714c\u5149\u706f\u5149\u73b0\u8c61\uff0c\u4e3a\u5b9e\u65f6\u73af\u5883\u63d0\u4f9b\u66f4\u9ad8\u7684\u771f\u5b9e\u611f\u548c\u6d41\u7545\u4f53\u9a8c\uff0c\u89e3\u51b3\u8ba1\u7b97\u8d44\u6e90\u74f6\u9888\u95ee\u9898"}}
{"id": "2509.05970", "pdf": "https://arxiv.org/pdf/2509.05970", "abs": "https://arxiv.org/abs/2509.05970", "authors": ["Ye Wang", "Zili Yi", "Yibo Zhang", "Peng Zheng", "Xuping Xie", "Jiang Lin", "Yilin Wang", "Rui Ma"], "title": "OmniStyle2: Scalable and High Quality Artistic Style Transfer Data Generation via Destylization", "categories": ["cs.CV"], "comment": "Project Page: https://wangyephd.github.io/projects/omnistyle2.html", "summary": "OmniStyle2 introduces a novel approach to artistic style transfer by reframing it as a data problem. Our key insight is destylization, reversing style transfer by removing stylistic elements from artworks to recover natural, style-free counterparts. This yields DST-100K, a large-scale dataset that provides authentic supervision signals by aligning real artistic styles with their underlying content. To build DST-100K, we develop (1) DST, a text-guided destylization model that reconstructs stylefree content, and (2) DST-Filter, a multi-stage evaluation model that employs Chain-of-Thought reasoning to automatically discard low-quality pairs while ensuring content fidelity and style accuracy. Leveraging DST-100K, we train OmniStyle2, a simple feed-forward model based on FLUX.1-dev. Despite its simplicity, OmniStyle2 consistently surpasses state-of-the-art methods across both qualitative and quantitative benchmarks. Our results demonstrate that scalable data generation via destylization provides a reliable supervision paradigm, overcoming the fundamental challenge posed by the lack of ground-truth data in artistic style transfer.", "AI": {"tldr": "OmniStyle2\u901a\u8fc7\u53bb\u98ce\u683c\u5316\u65b9\u6cd5\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6DST-100K\uff0c\u8bad\u7ec3\u51fa\u57fa\u4e8eFLUX.1-dev\u7684\u524d\u9988\u6a21\u578b\uff0c\u5728\u827a\u672f\u98ce\u683c\u8fc1\u79fb\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u65b9\u6cd5", "motivation": "\u89e3\u51b3\u827a\u672f\u98ce\u683c\u8fc1\u79fb\u4e2d\u7f3a\u4e4f\u771f\u5b9e\u76d1\u7763\u6570\u636e\u7684\u57fa\u672c\u6311\u6218\uff0c\u901a\u8fc7\u53bb\u98ce\u683c\u5316\u65b9\u6cd5\u83b7\u5f97\u98ce\u683c\u5316\u827a\u672f\u4f5c\u54c1\u4e0e\u5176\u5e95\u5c42\u5185\u5bb9\u7684\u771f\u5b9e\u5bf9\u5e94\u5173\u7cfb", "method": "\u5f00\u53d1\u6587\u672c\u5f15\u5bfc\u7684\u53bb\u98ce\u683c\u5316\u6a21\u578bDST\u91cd\u5efa\u65e0\u98ce\u683c\u5185\u5bb9\uff0c\u4f7f\u7528\u591a\u9636\u6bb5\u8bc4\u4f30\u6a21\u578bDST-Filter\u81ea\u52a8\u7b5b\u9009\u9ad8\u8d28\u91cf\u6570\u636e\u5bf9\uff0c\u57fa\u4e8eDST-100K\u6570\u636e\u96c6\u8bad\u7ec3FLUX.1-dev\u524d\u9988\u6a21\u578b", "result": "OmniStyle2\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u8d85\u8d8a\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u53bb\u98ce\u683c\u5316\u8fdb\u884c\u53ef\u6269\u5c55\u6570\u636e\u751f\u6210\u7684\u53ef\u9760\u6027", "conclusion": "\u53bb\u98ce\u683c\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u7684\u76d1\u7763\u8303\u5f0f\uff0c\u514b\u670d\u4e86\u827a\u672f\u98ce\u683c\u8fc1\u79fb\u4e2d\u7f3a\u4e4f\u771f\u5b9e\u6570\u636e\u7684\u57fa\u672c\u6311\u6218"}}
{"id": "2509.05992", "pdf": "https://arxiv.org/pdf/2509.05992", "abs": "https://arxiv.org/abs/2509.05992", "authors": ["Zekun Zhou", "Yanru Gong", "Liu Shi", "Qiegen Liu"], "title": "Multi-Strategy Guided Diffusion via Sparse Masking Temporal Reweighting Distribution Correction", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have demonstrated remarkable generative capabilities in image processing tasks. We propose a Sparse condition Temporal Rewighted Integrated Distribution Estimation guided diffusion model (STRIDE) for sparse-view CT reconstruction. Specifically, we design a joint training mechanism guided by sparse conditional probabilities to facilitate the model effective learning of missing projection view completion and global information modeling. Based on systematic theoretical analysis, we propose a temporally varying sparse condition reweighting guidance strategy to dynamically adjusts weights during the progressive denoising process from pure noise to the real image, enabling the model to progressively perceive sparse-view information. The linear regression is employed to correct distributional shifts between known and generated data, mitigating inconsistencies arising during the guidance process. Furthermore, we construct a dual-network parallel architecture to perform global correction and optimization across multiple sub-frequency components, thereby effectively improving the model capability in both detail restoration and structural preservation, ultimately achieving high-quality image reconstruction. Experimental results on both public and real datasets demonstrate that the proposed method achieves the best improvement of 2.58 dB in PSNR, increase of 2.37\\% in SSIM, and reduction of 0.236 in MSE compared to the best-performing baseline methods. The reconstructed images exhibit excellent generalization and robustness in terms of structural consistency, detail restoration, and artifact suppression.", "AI": {"tldr": "STRIDE\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7a00\u758f\u89c6\u56feCT\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u95f4\u53d8\u5316\u7684\u7a00\u758f\u6761\u4ef6\u91cd\u52a0\u6743\u7b56\u7565\u548c\u53cc\u7f51\u7edc\u5e76\u884c\u67b6\u6784\uff0c\u5728PSNR\u3001SSIM\u548cMSE\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u5904\u7406\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u751f\u6210\u80fd\u529b\uff0c\u4f46\u5982\u4f55\u6709\u6548\u5904\u7406\u7a00\u758f\u89c6\u56feCT\u91cd\u5efa\u4e2d\u7684\u7f3a\u5931\u6295\u5f71\u89c6\u56fe\u5b8c\u6210\u548c\u5168\u5c40\u4fe1\u606f\u5efa\u6a21\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51fa\u65f6\u95f4\u53d8\u5316\u7684\u7a00\u758f\u6761\u4ef6\u91cd\u52a0\u6743\u5f15\u5bfc\u7b56\u7565\uff0c\u52a8\u6001\u8c03\u6574\u53bb\u566a\u8fc7\u7a0b\u4e2d\u7684\u6743\u91cd\uff1b\u4f7f\u7528\u7ebf\u6027\u56de\u5f52\u6821\u6b63\u5206\u5e03\u504f\u79fb\uff1b\u6784\u5efa\u53cc\u7f51\u7edc\u5e76\u884c\u67b6\u6784\u8fdb\u884c\u5168\u5c40\u6821\u6b63\u548c\u591a\u5b50\u9891\u7387\u5206\u91cf\u4f18\u5316\u3002", "result": "\u5728\u516c\u5f00\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cPSNR\u63d0\u53472.58dB\uff0cSSIM\u63d0\u9ad82.37%\uff0cMSE\u964d\u4f4e0.236\uff0c\u91cd\u5efa\u56fe\u50cf\u5728\u7ed3\u6784\u4e00\u81f4\u6027\u3001\u7ec6\u8282\u6062\u590d\u548c\u4f2a\u5f71\u6291\u5236\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u79c0\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "STRIDE\u65b9\u6cd5\u901a\u8fc7\u521b\u65b0\u7684\u5f15\u5bfc\u7b56\u7565\u548c\u7f51\u7edc\u67b6\u6784\u8bbe\u8ba1\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u7a00\u758f\u89c6\u56feCT\u56fe\u50cf\u91cd\u5efa\uff0c\u5728\u591a\u9879\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002"}}
{"id": "2509.06010", "pdf": "https://arxiv.org/pdf/2509.06010", "abs": "https://arxiv.org/abs/2509.06010", "authors": ["Wanyin Cheng", "Zanxi Ruan"], "title": "BLaVe-CoT: Consistency-Aware Visual Question Answering for Blind and Low Vision Users", "categories": ["cs.CV"], "comment": null, "summary": "Visual Question Answering (VQA) holds great potential for assisting Blind and Low Vision (BLV) users, yet real-world usage remains challenging. Due to visual impairments, BLV users often take blurry or poorly framed photos and face difficulty in articulating specific questions about what they cannot fully see. As a result, their visual questions are frequently ambiguous, and different users may interpret them in diverse ways. This leads to multiple valid answers, each grounded in different image regions-posing a mismatch with conventional VQA systems that assume a single answer and region. To bridge this gap, we present BLaVe-CoT, a VQA framework designed to reason about answer consistency in the face of ambiguity. Our method proposes diverse candidate answers using a LoRA-tuned BLIP-2 model, then grounds each answer spatially using PolyFormer, and finally applies a chain-of-thought reasoning module to assess whether the answers refer to the same or different regions. Evaluated on the VQA-AnswerTherapy benchmark, BLaVe-CoT outperforms previous methods and proves more robust to the ambiguity and visual noise common in assistive settings. This work highlights the need for VQA systems that can adapt to real human uncertainty and provide inclusive support for BLV users. To foster further research and accessibility applications, we have made the code publicly available at https://github.com/Accecwan/BLaVe-CoT.", "AI": {"tldr": "BLaVe-CoT\u662f\u4e00\u4e2a\u9488\u5bf9\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u7528\u6237\u7684VQA\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5019\u9009\u7b54\u6848\u751f\u6210\u3001\u7a7a\u95f4\u5b9a\u4f4d\u548c\u601d\u7ef4\u94fe\u63a8\u7406\u6765\u5904\u7406\u89c6\u89c9\u95ee\u7b54\u4e2d\u7684\u6a21\u7cca\u6027\u95ee\u9898\u3002", "motivation": "\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u7528\u6237\u62cd\u6444\u7684\u7167\u7247\u5f80\u5f80\u6a21\u7cca\u6216\u6784\u56fe\u4e0d\u4f73\uff0c\u63d0\u51fa\u7684\u89c6\u89c9\u95ee\u9898\u4e5f\u7ecf\u5e38\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b58\u5728\u591a\u79cd\u6709\u6548\u7b54\u6848\uff0c\u8fd9\u4e0e\u4f20\u7edfVQA\u7cfb\u7edf\u5047\u8bbe\u5355\u4e00\u7b54\u6848\u548c\u533a\u57df\u4e0d\u5339\u914d\u3002", "method": "\u4f7f\u7528LoRA\u8c03\u4f18\u7684BLIP-2\u6a21\u578b\u751f\u6210\u591a\u6837\u5019\u9009\u7b54\u6848\uff0c\u901a\u8fc7PolyFormer\u5bf9\u6bcf\u4e2a\u7b54\u6848\u8fdb\u884c\u7a7a\u95f4\u5b9a\u4f4d\uff0c\u6700\u540e\u5e94\u7528\u601d\u7ef4\u94fe\u63a8\u7406\u6a21\u5757\u8bc4\u4f30\u7b54\u6848\u662f\u5426\u6307\u5411\u76f8\u540c\u6216\u4e0d\u540c\u533a\u57df\u3002", "result": "\u5728VQA-AnswerTherapy\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBLaVe-CoT\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u5bf9\u8f85\u52a9\u573a\u666f\u4e2d\u5e38\u89c1\u7684\u6a21\u7cca\u6027\u548c\u89c6\u89c9\u566a\u58f0\u66f4\u5177\u9c81\u68d2\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86VQA\u7cfb\u7edf\u9700\u8981\u9002\u5e94\u771f\u5b9e\u4eba\u7c7b\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u7528\u6237\u63d0\u4f9b\u5305\u5bb9\u6027\u652f\u6301\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.06068", "pdf": "https://arxiv.org/pdf/2509.06068", "abs": "https://arxiv.org/abs/2509.06068", "authors": ["Shih-Ying Yeh"], "title": "Home-made Diffusion Model from Scratch to Hatch", "categories": ["cs.CV"], "comment": null, "summary": "We introduce Home-made Diffusion Model (HDM), an efficient yet powerful text-to-image diffusion model optimized for training (and inferring) on consumer-grade hardware. HDM achieves competitive 1024x1024 generation quality while maintaining a remarkably low training cost of $535-620 using four RTX5090 GPUs, representing a significant reduction in computational requirements compared to traditional approaches. Our key contributions include: (1) Cross-U-Transformer (XUT), a novel U-shape transformer, Cross-U-Transformer (XUT), that employs cross-attention for skip connections, providing superior feature integration that leads to remarkable compositional consistency; (2) a comprehensive training recipe that incorporates TREAD acceleration, a novel shifted square crop strategy for efficient arbitrary aspect-ratio training, and progressive resolution scaling; and (3) an empirical demonstration that smaller models (343M parameters) with carefully crafted architectures can achieve high-quality results and emergent capabilities, such as intuitive camera control. Our work provides an alternative paradigm of scaling, demonstrating a viable path toward democratizing high-quality text-to-image generation for individual researchers and smaller organizations with limited computational resources.", "AI": {"tldr": "HDM\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u4f4e\u6210\u672c\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0c\u4ec5\u9700535-620\u7f8e\u5143\u57284\u5757RTX5090\u4e0a\u8bad\u7ec3\uff0c\u5c31\u80fd\u751f\u62101024x1024\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u5927\u5927\u964d\u4f4e\u4e86\u8ba1\u7b97\u9700\u6c42\u3002", "motivation": "\u4e3a\u4e86\u8ba9\u4e2a\u4eba\u7814\u7a76\u8005\u548c\u5c0f\u578b\u7ec4\u7ec7\u4e5f\u80fd\u8d1f\u62c5\u5f97\u8d77\u9ad8\u8d28\u91cf\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faCross-U-Transformer\u67b6\u6784\u4f7f\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u8fdb\u884c\u8df3\u8dc3\u8fde\u63a5\uff0c\u7ed3\u5408TREAD\u52a0\u901f\u3001\u79fb\u4f4d\u65b9\u5f62\u88c1\u526a\u7b56\u7565\u548c\u6e10\u8fdb\u5206\u8fa8\u7387\u7f29\u653e\u7684\u9ad8\u6548\u8bad\u7ec3\u65b9\u6848\u3002", "result": "\u4ec5\u7528343M\u53c2\u6570\u7684\u5c0f\u6a21\u578b\u5c31\u80fd\u5b9e\u73b0\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u548c\u6d8c\u73b0\u80fd\u529b\uff08\u5982\u76f4\u89c2\u76f8\u673a\u63a7\u5236\uff09\uff0c\u5728\u6709\u9650\u786c\u4ef6\u4e0a\u8fbe\u5230\u7ade\u4e89\u6027\u6548\u679c\u3002", "conclusion": "\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u67b6\u6784\u800c\u975e\u5355\u7eaf\u6269\u5927\u89c4\u6a21\uff0c\u8bc1\u660e\u4e86\u9ad8\u8d28\u91cf\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u53ef\u4ee5\u6c11\u4e3b\u5316\uff0c\u4e3a\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2509.06096", "pdf": "https://arxiv.org/pdf/2509.06096", "abs": "https://arxiv.org/abs/2509.06096", "authors": ["Yiwen Ye", "Yicheng Wu", "Xiangde Luo", "He Zhang", "Ziyang Chen", "Ting Dang", "Yanning Zhang", "Yong Xia"], "title": "MedSeqFT: Sequential Fine-tuning Foundation Models for 3D Medical Image Segmentation", "categories": ["cs.CV"], "comment": "10 pages, 5 figures", "summary": "Foundation models have become a promising paradigm for advancing medical image analysis, particularly for segmentation tasks where downstream applications often emerge sequentially. Existing fine-tuning strategies, however, remain limited: parallel fine-tuning isolates tasks and fails to exploit shared knowledge, while multi-task fine-tuning requires simultaneous access to all datasets and struggles with incremental task integration. To address these challenges, we propose MedSeqFT, a sequential fine-tuning framework that progressively adapts pre-trained models to new tasks while refining their representational capacity. MedSeqFT introduces two core components: (1) Maximum Data Similarity (MDS) selection, which identifies downstream samples most representative of the original pre-training distribution to preserve general knowledge, and (2) Knowledge and Generalization Retention Fine-Tuning (K&G RFT), a LoRA-based knowledge distillation scheme that balances task-specific adaptation with the retention of pre-trained knowledge. Extensive experiments on two multi-task datasets covering ten 3D segmentation tasks demonstrate that MedSeqFT consistently outperforms state-of-the-art fine-tuning strategies, yielding substantial performance gains (e.g., an average Dice improvement of 3.0%). Furthermore, evaluations on two unseen tasks (COVID-19-20 and Kidney) verify that MedSeqFT enhances transferability, particularly for tumor segmentation. Visual analyses of loss landscapes and parameter variations further highlight the robustness of MedSeqFT. These results establish sequential fine-tuning as an effective, knowledge-retentive paradigm for adapting foundation models to evolving clinical tasks. Code will be released.", "AI": {"tldr": "MedSeqFT\u662f\u4e00\u4e2a\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u987a\u5e8f\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7MDS\u6837\u672c\u9009\u62e9\u548cK&G RFT\u77e5\u8bc6\u84b8\u998f\uff0c\u5728\u4fdd\u6301\u9884\u8bad\u7ec3\u77e5\u8bc6\u7684\u540c\u65f6\u9002\u5e94\u65b0\u4efb\u52a1\uff0c\u572810\u4e2a3D\u5206\u5272\u4efb\u52a1\u4e0a\u5e73\u5747Dice\u63d0\u53473.0%", "motivation": "\u73b0\u6709\u5fae\u8c03\u7b56\u7565\u5b58\u5728\u5c40\u9650\uff1a\u5e76\u884c\u5fae\u8c03\u9694\u79bb\u4efb\u52a1\u65e0\u6cd5\u5171\u4eab\u77e5\u8bc6\uff0c\u591a\u4efb\u52a1\u5fae\u8c03\u9700\u8981\u540c\u65f6\u8bbf\u95ee\u6240\u6709\u6570\u636e\u96c6\u4e14\u96be\u4ee5\u589e\u91cf\u96c6\u6210\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u987a\u5e8f\u9002\u5e94\u65b0\u4efb\u52a1\u540c\u65f6\u4fdd\u6301\u9884\u8bad\u7ec3\u77e5\u8bc6\u7684\u6846\u67b6", "method": "\u63d0\u51faMedSeqFT\u987a\u5e8f\u5fae\u8c03\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) MDS\u9009\u62e9\u8bc6\u522b\u6700\u80fd\u4ee3\u8868\u9884\u8bad\u7ec3\u5206\u5e03\u7684\u4e0b\u6e38\u6837\u672c\uff1b2) K&G RFT\u57fa\u4e8eLoRA\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6848\uff0c\u5e73\u8861\u4efb\u52a1\u7279\u5b9a\u9002\u5e94\u548c\u9884\u8bad\u7ec3\u77e5\u8bc6\u4fdd\u6301", "result": "\u5728\u4e24\u4e2a\u591a\u4efb\u52a1\u6570\u636e\u96c6\u4e0a\u768410\u4e2a3D\u5206\u5272\u4efb\u52a1\u4e2d consistently\u4f18\u4e8e\u6700\u5148\u8fdb\u5fae\u8c03\u7b56\u7565\uff0c\u5e73\u5747Dice\u63d0\u53473.0%\u3002\u5728\u672a\u89c1\u4efb\u52a1(COVID-19-20\u548cKidney)\u4e0a\u9a8c\u8bc1\u4e86\u66f4\u597d\u7684\u8fc1\u79fb\u6027\uff0c\u7279\u522b\u662f\u80bf\u7624\u5206\u5272", "conclusion": "MedSeqFT\u5efa\u7acb\u4e86\u987a\u5e8f\u5fae\u8c03\u4f5c\u4e3a\u9002\u5e94\u57fa\u7840\u6a21\u578b\u5230\u6f14\u5316\u4e34\u5e8a\u4efb\u52a1\u7684\u6709\u6548\u3001\u77e5\u8bc6\u4fdd\u6301\u8303\u5f0f\uff0c\u635f\u5931\u666f\u89c2\u548c\u53c2\u6570\u53d8\u5316\u7684\u53ef\u89c6\u5316\u5206\u6790\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u5176\u9c81\u68d2\u6027"}}
{"id": "2509.06116", "pdf": "https://arxiv.org/pdf/2509.06116", "abs": "https://arxiv.org/abs/2509.06116", "authors": ["Giulia Bonino", "Luca Alberto Rizzo"], "title": "CARDIE: clustering algorithm on relevant descriptors for image enhancement", "categories": ["cs.CV", "I.4.8"], "comment": null, "summary": "Automatic image clustering is a cornerstone of computer vision, yet its application to image enhancement remains limited, primarily due to the difficulty of defining clusters that are meaningful for this specific task. To address this issue, we introduce CARDIE, an unsupervised algorithm that clusters images based on their color and luminosity content. In addition, we introduce a method to quantify the impact of image enhancement algorithms on luminance distribution and local variance. Using this method, we demonstrate that CARDIE produces clusters more relevant to image enhancement than those derived from semantic image attributes. Furthermore, we demonstrate that CARDIE clusters can be leveraged to resample image enhancement datasets, leading to improved performance for tone mapping and denoising algorithms. To encourage adoption and ensure reproducibility, we publicly release CARDIE code on our GitHub.", "AI": {"tldr": "CARDIE\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u56fe\u50cf\u805a\u7c7b\u7b97\u6cd5\uff0c\u57fa\u4e8e\u989c\u8272\u548c\u4eae\u5ea6\u5185\u5bb9\u8fdb\u884c\u805a\u7c7b\uff0c\u4e13\u95e8\u7528\u4e8e\u56fe\u50cf\u589e\u5f3a\u4efb\u52a1\uff0c\u80fd\u591f\u63d0\u5347\u8c03\u5f69\u6620\u5c04\u548c\u53bb\u566a\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u56fe\u50cf\u81ea\u52a8\u805a\u7c7b\u5728\u56fe\u50cf\u589e\u5f3a\u4efb\u52a1\u4e2d\u5e94\u7528\u6709\u9650\uff0c\u4e3b\u8981\u56e0\u4e3a\u96be\u4ee5\u5b9a\u4e49\u5bf9\u8be5\u4efb\u52a1\u6709\u610f\u4e49\u7684\u805a\u7c7b\u3002", "method": "\u63d0\u51faCARDIE\u65e0\u76d1\u7763\u7b97\u6cd5\uff0c\u57fa\u4e8e\u989c\u8272\u548c\u4eae\u5ea6\u5185\u5bb9\u805a\u7c7b\u56fe\u50cf\uff1b\u5e76\u63d0\u51fa\u91cf\u5316\u56fe\u50cf\u589e\u5f3a\u7b97\u6cd5\u5f71\u54cd\u7684\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4eae\u5ea6\u5206\u5e03\u548c\u5c40\u90e8\u65b9\u5dee\u3002", "result": "CARDIE\u4ea7\u751f\u7684\u805a\u7c7b\u6bd4\u8bed\u4e49\u56fe\u50cf\u5c5e\u6027\u805a\u7c7b\u66f4\u9002\u5408\u56fe\u50cf\u589e\u5f3a\uff0c\u53ef\u7528\u4e8e\u91cd\u65b0\u91c7\u6837\u56fe\u50cf\u589e\u5f3a\u6570\u636e\u96c6\uff0c\u63d0\u5347\u8c03\u5f69\u6620\u5c04\u548c\u53bb\u566a\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "CARDIE\u4e3a\u56fe\u50cf\u589e\u5f3a\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u805a\u7c7b\u65b9\u6cd5\uff0c\u5df2\u5f00\u6e90\u53d1\u5e03\u4ee3\u7801\u4ee5\u4fbf\u91cd\u73b0\u548c\u5e94\u7528\u3002"}}
{"id": "2509.06155", "pdf": "https://arxiv.org/pdf/2509.06155", "abs": "https://arxiv.org/abs/2509.06155", "authors": ["Duomin Wang", "Wei Zuo", "Aojie Li", "Ling-Hao Chen", "Xinyao Liao", "Deyu Zhou", "Zixin Yin", "Xili Dai", "Daxin Jiang", "Gang Yu"], "title": "UniVerse-1: Unified Audio-Video Generation via Stitching of Experts", "categories": ["cs.CV"], "comment": "Project page: https://dorniwang.github.io/UniVerse-1/", "summary": "We introduce UniVerse-1, a unified, Veo-3-like model capable of simultaneously generating coordinated audio and video. To enhance training efficiency, we bypass training from scratch and instead employ a stitching of experts (SoE) technique. This approach deeply fuses the corresponding blocks of pre-trained video and music generation experts models, thereby fully leveraging their foundational capabilities. To ensure accurate annotations and temporal alignment for both ambient sounds and speech with video content, we developed an online annotation pipeline that processes the required training data and generates labels during training process. This strategy circumvents the performance degradation often caused by misalignment text-based annotations. Through the synergy of these techniques, our model, after being finetuned on approximately 7,600 hours of audio-video data, produces results with well-coordinated audio-visuals for ambient sounds generation and strong alignment for speech generation. To systematically evaluate our proposed method, we introduce Verse-Bench, a new benchmark dataset. In an effort to advance research in audio-video generation and to close the performance gap with state-of-the-art models such as Veo3, we make our model and code publicly available. We hope this contribution will benefit the broader research community. Project page: https://dorniwang.github.io/UniVerse-1/.", "AI": {"tldr": "UniVerse-1\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u4e13\u5bb6\u6a21\u578b\u62fc\u63a5\u6280\u672f\u5b9e\u73b0\u97f3\u9891\u548c\u89c6\u9891\u7684\u534f\u540c\u751f\u6210\uff0c\u907f\u514d\u4e86\u4ece\u5934\u8bad\u7ec3\uff0c\u57287600\u5c0f\u65f6\u6570\u636e\u4e0a\u5fae\u8c03\u540e\u80fd\u591f\u4ea7\u751f\u534f\u8c03\u7684\u97f3\u89c6\u9891\u5185\u5bb9\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u97f3\u9891-\u89c6\u9891\u751f\u6210\u4e2d\u8bad\u7ec3\u6548\u7387\u4f4e\u548c\u6587\u672c\u6807\u6ce8\u4e0d\u5bf9\u9f50\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u540c\u65f6\u751f\u6210\u534f\u8c03\u97f3\u9891\u548c\u89c6\u9891\u7684\u7edf\u4e00\u6a21\u578b\uff0c\u907f\u514d\u4ece\u5934\u8bad\u7ec3\u5e76\u5145\u5206\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u57fa\u7840\u80fd\u529b\u3002", "method": "\u91c7\u7528\u4e13\u5bb6\u6a21\u578b\u62fc\u63a5(SoE)\u6280\u672f\uff0c\u6df1\u5ea6\u878d\u5408\u9884\u8bad\u7ec3\u7684\u89c6\u9891\u548c\u97f3\u4e50\u751f\u6210\u4e13\u5bb6\u6a21\u578b\u7684\u5bf9\u5e94\u5757\uff1b\u5f00\u53d1\u5728\u7ebf\u6807\u6ce8\u6d41\u7a0b\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5904\u7406\u8bad\u7ec3\u6570\u636e\u5e76\u751f\u6210\u6807\u7b7e\uff0c\u907f\u514d\u57fa\u4e8e\u6587\u672c\u6807\u6ce8\u7684\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002", "result": "\u6a21\u578b\u57287600\u5c0f\u65f6\u97f3\u89c6\u9891\u6570\u636e\u4e0a\u5fae\u8c03\u540e\uff0c\u80fd\u591f\u4e3a\u73af\u5883\u58f0\u97f3\u751f\u6210\u4ea7\u751f\u826f\u597d\u534f\u8c03\u7684\u97f3\u89c6\u9891\u5185\u5bb9\uff0c\u4e3a\u8bed\u97f3\u751f\u6210\u63d0\u4f9b\u5f3a\u5bf9\u9f50\u6548\u679c\uff1b\u5e76\u5f15\u5165\u4e86Verse-Bench\u57fa\u51c6\u6570\u636e\u96c6\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "conclusion": "UniVerse-1\u901a\u8fc7\u521b\u65b0\u7684\u4e13\u5bb6\u62fc\u63a5\u548c\u5728\u7ebf\u6807\u6ce8\u6280\u672f\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u97f3\u89c6\u9891\u534f\u540c\u751f\u6210\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u548c\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u7f29\u5c0f\u4e0eVeo3\u7b49\u6700\u5148\u8fdb\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\u3002"}}
{"id": "2509.06165", "pdf": "https://arxiv.org/pdf/2509.06165", "abs": "https://arxiv.org/abs/2509.06165", "authors": ["Huy Le", "Nhat Chung", "Tung Kieu", "Jingkang Yang", "Ngan Le"], "title": "UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual Representation Learning", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 7 figures", "summary": "Video Scene Graph Generation (VidSGG) aims to represent dynamic visual content by detecting objects and modeling their temporal interactions as structured graphs. Prior studies typically target either coarse-grained box-level or fine-grained panoptic pixel-level VidSGG, often requiring task-specific architectures and multi-stage training pipelines. In this paper, we present UNO (UNified Object-centric VidSGG), a single-stage, unified framework that jointly addresses both tasks within an end-to-end architecture. UNO is designed to minimize task-specific modifications and maximize parameter sharing, enabling generalization across different levels of visual granularity. The core of UNO is an extended slot attention mechanism that decomposes visual features into object and relation slots. To ensure robust temporal modeling, we introduce object temporal consistency learning, which enforces consistent object representations across frames without relying on explicit tracking modules. Additionally, a dynamic triplet prediction module links relation slots to corresponding object pairs, capturing evolving interactions over time. We evaluate UNO on standard box-level and pixel-level VidSGG benchmarks. Results demonstrate that UNO not only achieves competitive performance across both tasks but also offers improved efficiency through a unified, object-centric design.", "AI": {"tldr": "UNO\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5355\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u89c6\u9891\u573a\u666f\u56fe\u751f\u6210\uff0c\u53ef\u540c\u65f6\u5904\u7406\u7c97\u7c92\u5ea6\u8fb9\u754c\u6846\u7ea7\u548c\u7ec6\u7c92\u5ea6\u5168\u666f\u50cf\u7d20\u7ea7\u4efb\u52a1\uff0c\u901a\u8fc7\u6269\u5c55\u7684slot attention\u673a\u5236\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u7c92\u5ea6\u89c6\u89c9\u5185\u5bb9\u5efa\u6a21\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u573a\u666f\u56fe\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u7c92\u5ea6\uff08\u8fb9\u754c\u6846\u7ea7\u6216\u50cf\u7d20\u7ea7\uff09\u8bbe\u8ba1\uff0c\u9700\u8981\u4efb\u52a1\u7279\u5b9a\u7684\u67b6\u6784\u548c\u591a\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faUNO\u6846\u67b6\uff1a1\uff09\u6269\u5c55slot attention\u673a\u5236\u5c06\u89c6\u89c9\u7279\u5f81\u5206\u89e3\u4e3a\u5bf9\u8c61\u548c\u5173\u7cfbslot\uff1b2\uff09\u5bf9\u8c61\u65f6\u5e8f\u4e00\u81f4\u6027\u5b66\u4e60\u786e\u4fdd\u8de8\u5e27\u8868\u793a\u4e00\u81f4\u6027\uff1b3\uff09\u52a8\u6001\u4e09\u5143\u7ec4\u9884\u6d4b\u6a21\u5757\u94fe\u63a5\u5173\u7cfbslot\u5230\u5bf9\u8c61\u5bf9\u3002", "result": "\u5728\u6807\u51c6\u8fb9\u754c\u6846\u7ea7\u548c\u50cf\u7d20\u7ea7VidSGG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u540c\u65f6\u901a\u8fc7\u7edf\u4e00\u7684\u5bf9\u8c61\u4e2d\u5fc3\u8bbe\u8ba1\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "conclusion": "UNO\u8bc1\u660e\u4e86\u7edf\u4e00\u6846\u67b6\u5728\u89c6\u9891\u573a\u666f\u56fe\u751f\u6210\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u80fd\u591f\u4ee5\u6700\u5c0f\u4efb\u52a1\u7279\u5b9a\u4fee\u6539\u5904\u7406\u4e0d\u540c\u89c6\u89c9\u7c92\u5ea6\u4efb\u52a1\uff0c\u4e3a\u591a\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.06387", "pdf": "https://arxiv.org/pdf/2509.06387", "abs": "https://arxiv.org/abs/2509.06387", "authors": ["Dongsik Yoon", "Jongeun Kim"], "title": "Your Super Resolution Model is not Enough for Tackling Real-World Scenarios", "categories": ["cs.CV"], "comment": "To appear in Workshop on Efficient Computing under Limited Resources:   Visual Computing (ICCV 2025)", "summary": "Despite remarkable progress in Single Image Super-Resolution (SISR), traditional models often struggle to generalize across varying scale factors, limiting their real-world applicability. To address this, we propose a plug-in Scale-Aware Attention Module (SAAM) designed to retrofit modern fixed-scale SR models with the ability to perform arbitrary-scale SR. SAAM employs lightweight, scale-adaptive feature extraction and upsampling, incorporating the Simple parameter-free Attention Module (SimAM) for efficient guidance and gradient variance loss to enhance sharpness in image details. Our method integrates seamlessly into multiple state-of-the-art SR backbones (e.g., SCNet, HiT-SR, OverNet), delivering competitive or superior performance across a wide range of integer and non-integer scale factors. Extensive experiments on benchmark datasets demonstrate that our approach enables robust multi-scale upscaling with minimal computational overhead, offering a practical solution for real-world scenarios.", "AI": {"tldr": "\u63d0\u51faSAAM\u63d2\u4ef6\u6a21\u5757\uff0c\u8ba9\u56fa\u5b9a\u7f29\u653e\u6bd4\u7684\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u80fd\u591f\u5904\u7406\u4efb\u610f\u7f29\u653e\u6bd4\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u6e17\u900f\u5f0f\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u591a\u6bd4\u4f8b\u5347\u7ea7\u3002", "motivation": "\u4f20\u7edf\u5355\u56fe\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u5bf9\u4e0d\u540c\u7f29\u653e\u6bd4\u4e4b\u95f4\u7684\u6cbf\u7528\u6027\u5dee\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8ba9\u73b0\u6709\u56fa\u5b9a\u6bd4\u4f8b\u6a21\u578b\u5904\u7406\u4efb\u610f\u7f29\u653e\u6bd4\u7684\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1Scale-Aware Attention Module (SAAM)\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u7684\u6bd4\u4f8b\u81ea\u9002\u5e94\u7279\u5f81\u63d0\u53d6\u548c\u4e0a\u91c7\u6837\uff0c\u7ed3\u5408SimAM\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u9ad8\u6548\u6307\u5bfc\uff0c\u4f7f\u7528\u68af\u5ea6\u65b9\u5dee\u635f\u5931\u63d0\u5347\u56fe\u50cf\u7ec6\u8282\u9510\u5229\u5ea6\u3002", "result": "\u65b9\u6848\u80fd\u5916\u63a5\u591a\u4e2a\u72ec\u7acb\u7684SISR\u6a21\u578b\uff08\u5982SCNet\u3001HiT-SR\u3001OverNet\uff09\uff0c\u5728\u5e7f\u6cdb\u7684\u6574\u6570\u548c\u975e\u6574\u6570\u7f29\u653e\u6bd4\u4e0b\u90fd\u53d6\u5f97\u4e86\u7ade\u4e89\u529b\u6216\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u8ba1\u7b97\u5f00\u9500\u6700\u5c0f\u3002", "conclusion": "SAAM\u4e3a\u73b0\u6709\u56fa\u5b9a\u6bd4\u4f8b\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u6bd4\u4f8b\u5347\u7ea7\u80fd\u529b\uff0c\u5177\u6709\u5f3a\u70c8\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.06400", "pdf": "https://arxiv.org/pdf/2509.06400", "abs": "https://arxiv.org/abs/2509.06400", "authors": ["Matthieu Gendrin", "St\u00e9phane Pateux", "Th\u00e9o Ladune"], "title": "3DOF+Quantization: 3DGS quantization for large scenes with limited Degrees of Freedom", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) is a major breakthrough in 3D scene reconstruction. With a number of views of a given object or scene, the algorithm trains a model composed of 3D gaussians, which enables the production of novel views from arbitrary points of view. This freedom of movement is referred to as 6DoF for 6 degrees of freedom: a view is produced for any position (3 degrees), orientation of camera (3 other degrees). On large scenes, though, the input views are acquired from a limited zone in space, and the reconstruction is valuable for novel views from the same zone, even if the scene itself is almost unlimited in size. We refer to this particular case as 3DoF+, meaning that the 3 degrees of freedom of camera position are limited to small offsets around the central position. Considering the problem of coordinate quantization, the impact of position error on the projection error in pixels is studied. It is shown that the projection error is proportional to the squared inverse distance of the point being projected. Consequently, a new quantization scheme based on spherical coordinates is proposed. Rate-distortion performance of the proposed method are illustrated on the well-known Garden scene.", "AI": {"tldr": "3D\u9ad8\u65af\u6cfc\u6e85\u5728\u6709\u9650\u89c6\u89d2\u533a\u57df\uff083DoF+\uff09\u4e0b\u63d0\u51fa\u57fa\u4e8e\u7403\u5750\u6807\u7684\u65b0\u91cf\u5316\u65b9\u6848\uff0c\u89e3\u51b3\u4f4d\u7f6e\u8bef\u5dee\u5bf9\u6295\u5f71\u8bef\u5dee\u7684\u5f71\u54cd", "motivation": "3DGS\u5728\u5927\u573a\u666f\u4e2d\u7531\u4e8e\u8f93\u5165\u89c6\u89d2\u6709\u9650\uff0c\u91cd\u5efa\u4e3b\u8981\u9002\u7528\u4e8e\u76f8\u540c\u533a\u57df\u7684\u89c6\u89d2\u53d8\u5316\uff083DoF+\uff09\uff0c\u9700\u8981\u89e3\u51b3\u5750\u6807\u91cf\u5316\u5e26\u6765\u7684\u4f4d\u7f6e\u8bef\u5dee\u95ee\u9898", "method": "\u5206\u6790\u4f4d\u7f6e\u8bef\u5dee\u5bf9\u50cf\u7d20\u6295\u5f71\u8bef\u5dee\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6295\u5f71\u8bef\u5dee\u4e0e\u6295\u5f71\u70b9\u8ddd\u79bb\u7684\u5e73\u65b9\u53cd\u6bd4\u6210\u6b63\u6bd4\uff0c\u636e\u6b64\u63d0\u51fa\u57fa\u4e8e\u7403\u5750\u6807\u7684\u91cf\u5316\u65b9\u6848", "result": "\u5728Garden\u573a\u666f\u4e0a\u5c55\u793a\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u7387\u5931\u771f\u6027\u80fd", "conclusion": "\u57fa\u4e8e\u7403\u5750\u6807\u7684\u91cf\u5316\u65b9\u6848\u80fd\u6709\u6548\u5904\u74063DoF+\u573a\u666f\u4e0b\u7684\u5750\u6807\u91cf\u5316\u95ee\u9898\uff0c\u6539\u5584\u6295\u5f71\u7cbe\u5ea6"}}
{"id": "2509.06413", "pdf": "https://arxiv.org/pdf/2509.06413", "abs": "https://arxiv.org/abs/2509.06413", "authors": ["Yixiao Li", "Xin Li", "Chris Wei Zhou", "Shuo Xing", "Hadi Amirpour", "Xiaoshuai Hao", "Guanghui Yue", "Baoquan Zhao", "Weide Liu", "Xiaoyuan Yang", "Zhengzhong Tu", "Xinyu Li", "Chuanbiao Song", "Chenqi Zhang", "Jun Lan", "Huijia Zhu", "Weiqiang Wang", "Xiaoyan Sun", "Shishun Tian", "Dongyang Yan", "Weixia Zhang", "Junlin Chen", "Wei Sun", "Zhihua Wang", "Zhuohang Shi", "Zhizun Luo", "Hang Ouyang", "Tianxin Xiao", "Fan Yang", "Zhaowang Wu", "Kaixin Deng"], "title": "VQualA 2025 Challenge on Image Super-Resolution Generated Content Quality Assessment: Methods and Results", "categories": ["cs.CV", "eess.IV"], "comment": "11 pages, 12 figures, VQualA ICCV Workshop", "summary": "This paper presents the ISRGC-Q Challenge, built upon the Image Super-Resolution Generated Content Quality Assessment (ISRGen-QA) dataset, and organized as part of the Visual Quality Assessment (VQualA) Competition at the ICCV 2025 Workshops. Unlike existing Super-Resolution Image Quality Assessment (SR-IQA) datasets, ISRGen-QA places a greater emphasis on SR images generated by the latest generative approaches, including Generative Adversarial Networks (GANs) and diffusion models. The primary goal of this challenge is to analyze the unique artifacts introduced by modern super-resolution techniques and to evaluate their perceptual quality effectively. A total of 108 participants registered for the challenge, with 4 teams submitting valid solutions and fact sheets for the final testing phase. These submissions demonstrated state-of-the-art (SOTA) performance on the ISRGen-QA dataset. The project is publicly available at: https://github.com/Lighting-YXLI/ISRGen-QA.", "AI": {"tldr": "ISRGC-Q\u6311\u6218\u8d5b\u57fa\u4e8eISRGen-QA\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u8bc4\u4f30\u751f\u6210\u5f0f\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff08GAN\u548c\u6269\u6563\u6a21\u578b\uff09\u4ea7\u751f\u7684\u56fe\u50cf\u8d28\u91cf\uff0c\u5171\u6709108\u4eba\u6ce8\u518c\uff0c4\u4e2a\u56e2\u961f\u63d0\u4ea4\u4e86SOTA\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u8d85\u5206\u8fa8\u7387\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6570\u636e\u96c6\u672a\u80fd\u5145\u5206\u6db5\u76d6\u6700\u65b0\u751f\u6210\u5f0f\u65b9\u6cd5\uff08\u5982GAN\u548c\u6269\u6563\u6a21\u578b\uff09\u4ea7\u751f\u7684\u72ec\u7279\u4f2a\u5f71\uff0c\u9700\u8981\u4e13\u95e8\u7684\u6570\u636e\u96c6\u548c\u6311\u6218\u6765\u6709\u6548\u8bc4\u4f30\u8fd9\u4e9b\u73b0\u4ee3\u6280\u672f\u7684\u611f\u77e5\u8d28\u91cf\u3002", "method": "\u6784\u5efaISRGen-QA\u6570\u636e\u96c6\uff0c\u91cd\u70b9\u5305\u542b\u751f\u6210\u5f0f\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u4ea7\u751f\u7684\u56fe\u50cf\uff0c\u7ec4\u7ec7\u6311\u6218\u8d5b\u8ba9\u53c2\u4e0e\u8005\u5f00\u53d1\u8d28\u91cf\u8bc4\u4f30\u7b97\u6cd5\uff0c\u6700\u7ec8\u67094\u4e2a\u56e2\u961f\u63d0\u4ea4\u6709\u6548\u89e3\u51b3\u65b9\u6848\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "108\u540d\u53c2\u4e0e\u8005\u6ce8\u518c\u6311\u6218\uff0c4\u4e2a\u56e2\u961f\u63d0\u4ea4\u7684\u89e3\u51b3\u65b9\u6848\u5728ISRGen-QA\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "ISRGC-Q\u6311\u6218\u8d5b\u6210\u529f\u5efa\u7acb\u4e86\u9488\u5bf9\u751f\u6210\u5f0f\u8d85\u5206\u8fa8\u7387\u6280\u672f\u7684\u8d28\u91cf\u8bc4\u4f30\u57fa\u51c6\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\uff0c\u76f8\u5173\u9879\u76ee\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u63d0\u4f9b\u3002"}}
{"id": "2509.06442", "pdf": "https://arxiv.org/pdf/2509.06442", "abs": "https://arxiv.org/abs/2509.06442", "authors": ["Yixiao Li", "Xiaoyuan Yang", "Guanghui Yue", "Jun Fu", "Qiuping Jiang", "Xu Jia", "Paul L. Rosin", "Hantao Liu", "Wei Zhou"], "title": "Perception-oriented Bidirectional Attention Network for Image Super-resolution Quality Assessment", "categories": ["cs.CV", "eess.IV"], "comment": "16 pages, 6 figures, IEEE Transactions on Image Processing", "summary": "Many super-resolution (SR) algorithms have been proposed to increase image resolution. However, full-reference (FR) image quality assessment (IQA) metrics for comparing and evaluating different SR algorithms are limited. In this work, we propose the Perception-oriented Bidirectional Attention Network (PBAN) for image SR FR-IQA, which is composed of three modules: an image encoder module, a perception-oriented bidirectional attention (PBA) module, and a quality prediction module. First, we encode the input images for feature representations. Inspired by the characteristics of the human visual system, we then construct the perception-oriented PBA module. Specifically, different from existing attention-based SR IQA methods, we conceive a Bidirectional Attention to bidirectionally construct visual attention to distortion, which is consistent with the generation and evaluation processes of SR images. To further guide the quality assessment towards the perception of distorted information, we propose Grouped Multi-scale Deformable Convolution, enabling the proposed method to adaptively perceive distortion. Moreover, we design Sub-information Excitation Convolution to direct visual perception to both sub-pixel and sub-channel attention. Finally, the quality prediction module is exploited to integrate quality-aware features and regress quality scores. Extensive experiments demonstrate that our proposed PBAN outperforms state-of-the-art quality assessment methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86PBAN\uff08\u611f\u77e5\u5bfc\u5411\u53cc\u5411\u6ce8\u610f\u529b\u7f51\u7edc\uff09\u7528\u4e8e\u8d85\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u5168\u53c2\u8003\u8d28\u91cf\u8bc4\u4f30\uff0c\u901a\u8fc7\u53cc\u5411\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u5c3a\u5ea6\u53ef\u53d8\u5f62\u5377\u79ef\u7b49\u65b9\u6cd5\uff0c\u5728\u611f\u77e5\u5931\u771f\u4fe1\u606f\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u7684\u8d85\u5206\u8fa8\u7387\u7b97\u6cd5\u7f3a\u4e4f\u6709\u6548\u7684\u5168\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u6765\u6bd4\u8f83\u548c\u8bc4\u4f30\u4e0d\u540c\u7b97\u6cd5\u7684\u6027\u80fd", "method": "\u6784\u5efa\u4e86\u4e09\u4e2a\u6a21\u5757\uff1a\u56fe\u50cf\u7f16\u7801\u5668\u3001\u611f\u77e5\u5bfc\u5411\u53cc\u5411\u6ce8\u610f\u529b\u6a21\u5757\uff08\u5305\u542b\u53cc\u5411\u6ce8\u610f\u529b\u3001\u5206\u7ec4\u591a\u5c3a\u5ea6\u53ef\u53d8\u5f62\u5377\u79ef\u548c\u5b50\u4fe1\u606f\u6fc0\u52b1\u5377\u79ef\uff09\u548c\u8d28\u91cf\u9884\u6d4b\u6a21\u5757", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660ePBAN\u5728\u8d28\u91cf\u8bc4\u4f30\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5", "conclusion": "PBAN\u901a\u8fc7\u521b\u65b0\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u611f\u77e5\u5bfc\u5411\u8bbe\u8ba1\uff0c\u4e3a\u8d85\u5206\u8fa8\u7387\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.06499", "pdf": "https://arxiv.org/pdf/2509.06499", "abs": "https://arxiv.org/abs/2509.06499", "authors": ["Jibai Lin", "Bo Ma", "Yating Yang", "Rong Ma", "Turghun Osman", "Ahtamjan Ahmat", "Rui Dong", "Lei Wang", "Xi Zhou"], "title": "TIDE: Achieving Balanced Subject-Driven Image Generation via Target-Instructed Diffusion Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Subject-driven image generation (SDIG) aims to manipulate specific subjects within images while adhering to textual instructions, a task crucial for advancing text-to-image diffusion models. SDIG requires reconciling the tension between maintaining subject identity and complying with dynamic edit instructions, a challenge inadequately addressed by existing methods. In this paper, we introduce the Target-Instructed Diffusion Enhancing (TIDE) framework, which resolves this tension through target supervision and preference learning without test-time fine-tuning. TIDE pioneers target-supervised triplet alignment, modelling subject adaptation dynamics using a (reference image, instruction, target images) triplet. This approach leverages the Direct Subject Diffusion (DSD) objective, training the model with paired \"winning\" (balanced preservation-compliance) and \"losing\" (distorted) targets, systematically generated and evaluated via quantitative metrics. This enables implicit reward modelling for optimal preservation-compliance balance. Experimental results on standard benchmarks demonstrate TIDE's superior performance in generating subject-faithful outputs while maintaining instruction compliance, outperforming baseline methods across multiple quantitative metrics. TIDE's versatility is further evidenced by its successful application to diverse tasks, including structural-conditioned generation, image-to-image generation, and text-image interpolation. Our code is available at https://github.com/KomJay520/TIDE.", "AI": {"tldr": "TIDE\u6846\u67b6\u901a\u8fc7\u76ee\u6807\u76d1\u7763\u548c\u504f\u597d\u5b66\u4e60\u89e3\u51b3\u4e3b\u9898\u9a71\u52a8\u56fe\u50cf\u751f\u6210\u4e2d\u4fdd\u6301\u4e3b\u4f53\u8eab\u4efd\u4e0e\u9075\u5faa\u7f16\u8f91\u6307\u4ee4\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u65e0\u9700\u6d4b\u8bd5\u65f6\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u66f4\u597d\u7684\u5e73\u8861\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u4e3b\u9898\u9a71\u52a8\u56fe\u50cf\u751f\u6210\u65f6\uff0c\u65e0\u6cd5\u5145\u5206\u89e3\u51b3\u4fdd\u6301\u4e3b\u4f53\u8eab\u4efd\u548c\u9075\u5faa\u52a8\u6001\u7f16\u8f91\u6307\u4ee4\u4e4b\u95f4\u7684\u5f20\u529b\u95ee\u9898\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faTIDE\u6846\u67b6\uff0c\u91c7\u7528\u76ee\u6807\u76d1\u7763\u4e09\u5143\u7ec4\u5bf9\u9f50\u65b9\u6cd5\uff0c\u4f7f\u7528(\u53c2\u8003\u56fe\u50cf\u3001\u6307\u4ee4\u3001\u76ee\u6807\u56fe\u50cf)\u4e09\u5143\u7ec4\u5efa\u6a21\u4e3b\u4f53\u9002\u5e94\u52a8\u6001\uff0c\u901a\u8fc7DSD\u76ee\u6807\u51fd\u6570\u8bad\u7ec3\u6a21\u578b\uff0c\u4f7f\u7528\u7cfb\u7edf\u751f\u6210\u7684\"\u83b7\u80dc\"\u548c\"\u5931\u8d25\"\u76ee\u6807\u8fdb\u884c\u504f\u597d\u5b66\u4e60\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTIDE\u5728\u751f\u6210\u4e3b\u4f53\u5fe0\u5b9e\u8f93\u51fa\u540c\u65f6\u4fdd\u6301\u6307\u4ee4\u9075\u5faa\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u591a\u4e2a\u91cf\u5316\u6307\u6807\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u80fd\u6210\u529f\u5e94\u7528\u4e8e\u7ed3\u6784\u6761\u4ef6\u751f\u6210\u3001\u56fe\u50cf\u5230\u56fe\u50cf\u751f\u6210\u548c\u6587\u672c\u56fe\u50cf\u63d2\u503c\u7b49\u591a\u6837\u5316\u4efb\u52a1\u3002", "conclusion": "TIDE\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u76ee\u6807\u76d1\u7763\u548c\u504f\u597d\u5b66\u4e60\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e3b\u9898\u9a71\u52a8\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2509.06579", "pdf": "https://arxiv.org/pdf/2509.06579", "abs": "https://arxiv.org/abs/2509.06579", "authors": ["Xin Kong", "Daniel Watson", "Yannick Str\u00fcmpler", "Michael Niemeyer", "Federico Tombari"], "title": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Multi-view diffusion models have shown promise in 3D novel view synthesis, but most existing methods adopt a non-autoregressive formulation. This limits their applicability in world modeling, as they only support a fixed number of views and suffer from slow inference due to denoising all frames simultaneously. To address these limitations, we propose CausNVS, a multi-view diffusion model in an autoregressive setting, which supports arbitrary input-output view configurations and generates views sequentially. We train CausNVS with causal masking and per-frame noise, using pairwise-relative camera pose encodings (CaPE) for precise camera control. At inference time, we combine a spatially-aware sliding-window with key-value caching and noise conditioning augmentation to mitigate drift. Our experiments demonstrate that CausNVS supports a broad range of camera trajectories, enables flexible autoregressive novel view synthesis, and achieves consistently strong visual quality across diverse settings. Project page: https://kxhit.github.io/CausNVS.html.", "AI": {"tldr": "CausNVS\u662f\u4e00\u4e2a\u81ea\u56de\u5f52\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\uff0c\u652f\u6301\u4efb\u610f\u8f93\u5165\u8f93\u51fa\u89c6\u89d2\u914d\u7f6e\uff0c\u901a\u8fc7\u5e8f\u5217\u5316\u751f\u6210\u89c6\u89d2\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u56fa\u5b9a\u89c6\u89d2\u6570\u548c\u63a8\u7406\u6162\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\u5927\u591a\u91c7\u7528\u975e\u81ea\u56de\u5f52\u5f62\u5f0f\uff0c\u9650\u5236\u4e86\u5728\u4e16\u754c\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\uff0c\u53ea\u652f\u6301\u56fa\u5b9a\u89c6\u89d2\u6570\u4e14\u7531\u4e8e\u540c\u65f6\u53bb\u566a\u6240\u6709\u5e27\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u6162\u3002", "method": "\u4f7f\u7528\u56e0\u679c\u63a9\u7801\u548c\u9010\u5e27\u566a\u58f0\u8bad\u7ec3\uff0c\u91c7\u7528\u6210\u5bf9\u76f8\u5bf9\u76f8\u673a\u4f4d\u59ff\u7f16\u7801(CaPE)\u5b9e\u73b0\u7cbe\u786e\u76f8\u673a\u63a7\u5236\u3002\u63a8\u7406\u65f6\u7ed3\u5408\u7a7a\u95f4\u611f\u77e5\u6ed1\u52a8\u7a97\u53e3\u3001\u952e\u503c\u7f13\u5b58\u548c\u566a\u58f0\u6761\u4ef6\u589e\u5f3a\u6765\u51cf\u8f7b\u6f02\u79fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCausNVS\u652f\u6301\u5e7f\u6cdb\u7684\u76f8\u673a\u8f68\u8ff9\uff0c\u5b9e\u73b0\u7075\u6d3b\u7684\u81ea\u56de\u5f52\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u4fdd\u6301\u4e00\u81f4\u7684\u5f3a\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "CausNVS\u901a\u8fc7\u81ea\u56de\u5f52\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\u5728\u89c6\u89d2\u7075\u6d3b\u6027\u548c\u63a8\u7406\u6548\u7387\u65b9\u9762\u7684\u9650\u5236\uff0c\u4e3a\u4e16\u754c\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.06591", "pdf": "https://arxiv.org/pdf/2509.06591", "abs": "https://arxiv.org/abs/2509.06591", "authors": ["Yichao Liu", "YueYang Teng"], "title": "Hybrid Swin Attention Networks for Simultaneously Low-Dose PET and CT Denoising", "categories": ["cs.CV"], "comment": null, "summary": "Low-dose computed tomography (LDCT) and positron emission tomography (PET) have emerged as safer alternatives to conventional imaging modalities by significantly reducing radiation exposure. However, this reduction often results in increased noise and artifacts, which can compromise diagnostic accuracy. Consequently, denoising for LDCT/PET has become a vital area of research aimed at enhancing image quality while maintaining radiation safety. In this study, we introduce a novel Hybrid Swin Attention Network (HSANet), which incorporates Efficient Global Attention (EGA) modules and a hybrid upsampling module. The EGA modules enhance both spatial and channel-wise interaction, improving the network's capacity to capture relevant features, while the hybrid upsampling module mitigates the risk of overfitting to noise. We validate the proposed approach using a publicly available LDCT/PET dataset. Experimental results demonstrate that HSANet achieves superior denoising performance compared to existing methods, while maintaining a lightweight model size suitable for deployment on GPUs with standard memory configurations. This makes our approach highly practical for real-world clinical applications.", "AI": {"tldr": "\u63d0\u51faHSANet\u7f51\u7edc\u7528\u4e8eLDCT/PET\u56fe\u50cf\u964d\u566a\uff0c\u7ed3\u5408\u9ad8\u6548\u5168\u5c40\u6ce8\u610f\u529b\u6a21\u5757\u548c\u6df7\u5408\u4e0a\u91c7\u6837\u6a21\u5757\uff0c\u5728\u4fdd\u6301\u8f7b\u91cf\u7ea7\u6a21\u578b\u7684\u540c\u65f6\u5b9e\u73b0\u4f18\u8d8a\u7684\u964d\u566a\u6027\u80fd\u3002", "motivation": "\u4f4e\u5242\u91cfCT\u548cPET\u6210\u50cf\u867d\u7136\u964d\u4f4e\u4e86\u8f90\u5c04\u66b4\u9732\uff0c\u4f46\u4f1a\u5bfc\u81f4\u566a\u58f0\u548c\u4f2a\u5f71\u589e\u52a0\uff0c\u5f71\u54cd\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u56e0\u6b64\u9700\u8981\u6709\u6548\u7684\u964d\u566a\u65b9\u6cd5\u6765\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u6df7\u5408Swin\u6ce8\u610f\u529b\u7f51\u7edc(HSANet)\uff0c\u5305\u542b\u9ad8\u6548\u5168\u5c40\u6ce8\u610f\u529b(EGA)\u6a21\u5757\u589e\u5f3a\u7a7a\u95f4\u548c\u901a\u9053\u4ea4\u4e92\uff0c\u4ee5\u53ca\u6df7\u5408\u4e0a\u91c7\u6837\u6a21\u5757\u9632\u6b62\u8fc7\u62df\u5408\u566a\u58f0\u3002", "result": "\u5728\u516c\u5f00LDCT/PET\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cHSANet\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u83b7\u5f97\u66f4\u4f18\u8d8a\u7684\u964d\u566a\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u9002\u5408\u6807\u51c6GPU\u5185\u5b58\u914d\u7f6e\u90e8\u7f72\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u5f88\u9ad8\u7684\u5b9e\u9645\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u8f90\u5c04\u5b89\u5168\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4f4e\u5242\u91cf\u533b\u5b66\u56fe\u50cf\u8d28\u91cf\u3002"}}
{"id": "2509.06685", "pdf": "https://arxiv.org/pdf/2509.06685", "abs": "https://arxiv.org/abs/2509.06685", "authors": ["Shengkai Zhang", "Yuhe Liu", "Guanjun Wu", "Jianhua He", "Xinggang Wang", "Mozi Chen", "Kezhong Liu"], "title": "VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level Guidance in Large Scenes", "categories": ["cs.CV"], "comment": null, "summary": "VIM-GS is a Gaussian Splatting (GS) framework using monocular images for novel-view synthesis (NVS) in large scenes. GS typically requires accurate depth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limited depth sensing range makes it difficult for GS to work in large scenes. Monocular images, however, lack depth to guide the learning and lead to inferior NVS results. Although large foundation models (LFMs) for monocular depth estimation are available, they suffer from cross-frame inconsistency, inaccuracy for distant scenes, and ambiguity in deceptive texture cues. This paper aims to generate dense, accurate depth images from monocular RGB inputs for high-definite GS rendering. The key idea is to leverage the accurate but sparse depth from visual-inertial Structure-from-Motion (SfM) to refine the dense but coarse depth from LFMs. To bridge the sparse input and dense output, we propose an object-segmented depth propagation algorithm that renders the depth of pixels of structured objects. Then we develop a dynamic depth refinement module to handle the crippled SfM depth of dynamic objects and refine the coarse LFM depth. Experiments using public and customized datasets demonstrate the superior rendering quality of VIM-GS in large scenes.", "AI": {"tldr": "VIM-GS\u662f\u4e00\u4e2a\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u7684\u5355\u76ee\u56fe\u50cf\u65b0\u89c6\u89d2\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u60ef\u6027SfM\u7684\u7a00\u758f\u6df1\u5ea6\u548c\u5927\u57fa\u7840\u6a21\u578b\u7684\u7a20\u5bc6\u6df1\u5ea6\uff0c\u89e3\u51b3\u4e86\u5927\u573a\u666f\u4e2d\u6df1\u5ea6\u4f30\u8ba1\u7684\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u9700\u8981RGB-D/\u7acb\u4f53\u76f8\u673a\u63d0\u4f9b\u51c6\u786e\u6df1\u5ea6\uff0c\u4f46\u6df1\u5ea6\u4f20\u611f\u8303\u56f4\u6709\u9650\uff0c\u96be\u4ee5\u5904\u7406\u5927\u573a\u666f\u3002\u5355\u76ee\u56fe\u50cf\u7f3a\u4e4f\u6df1\u5ea6\u4fe1\u606f\uff0c\u800c\u73b0\u6709\u5927\u57fa\u7840\u6a21\u578b\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5b58\u5728\u8de8\u5e27\u4e0d\u4e00\u81f4\u3001\u8fdc\u8ddd\u79bb\u4e0d\u51c6\u786e\u548c\u7eb9\u7406\u6b67\u4e49\u7b49\u95ee\u9898\u3002", "method": "\u5229\u7528\u89c6\u89c9\u60ef\u6027SfM\u7684\u51c6\u786e\u7a00\u758f\u6df1\u5ea6\u6765\u4f18\u5316\u5927\u57fa\u7840\u6a21\u578b\u7684\u7a20\u5bc6\u4f46\u7c97\u7cd9\u6df1\u5ea6\u3002\u63d0\u51fa\u5bf9\u8c61\u5206\u5272\u6df1\u5ea6\u4f20\u64ad\u7b97\u6cd5\u6765\u8fde\u63a5\u7a00\u758f\u8f93\u5165\u548c\u7a20\u5bc6\u8f93\u51fa\uff0c\u5e76\u5f00\u53d1\u52a8\u6001\u6df1\u5ea6\u4f18\u5316\u6a21\u5757\u5904\u7406\u52a8\u6001\u5bf9\u8c61\u7684SfM\u6df1\u5ea6\u7f3a\u9677\u3002", "result": "\u5728\u516c\u5f00\u548c\u5b9a\u5236\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cVIM-GS\u5728\u5927\u573a\u666f\u4e2d\u5177\u6709\u4f18\u8d8a\u7684\u6e32\u67d3\u8d28\u91cf\u3002", "conclusion": "VIM-GS\u901a\u8fc7\u878d\u5408\u7a00\u758f\u51c6\u786e\u6df1\u5ea6\u548c\u7a20\u5bc6\u7c97\u7cd9\u6df1\u5ea6\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5927\u573a\u666f\u4e2d\u9ad8\u8d28\u91cf\u7684\u5355\u76ee\u56fe\u50cf\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6df1\u5ea6\u4f30\u8ba1\u9650\u5236\u3002"}}
{"id": "2509.06693", "pdf": "https://arxiv.org/pdf/2509.06693", "abs": "https://arxiv.org/abs/2509.06693", "authors": ["Xichen Xu", "Yanshu Wang", "Jinbao Wang", "Qunyi Zhang", "Xiaoning Lei", "Guoyang Xie", "Guannan Jiang", "Zhichao Lu"], "title": "STAGE: Segmentation-oriented Industrial Anomaly Synthesis via Graded Diffusion with Explicit Mask Alignment", "categories": ["cs.CV"], "comment": null, "summary": "Segmentation-oriented Industrial Anomaly Synthesis (SIAS) plays a pivotal role in enhancing the performance of downstream anomaly segmentation, as it provides an effective means of expanding abnormal data. However, existing SIAS methods face several critical limitations: (i) the synthesized anomalies often lack intricate texture details and fail to align precisely with the surrounding background, and (ii) they struggle to generate fine-grained, pixel-level anomalies. To address these challenges, we propose Segmentation-oriented Anomaly synthesis via Graded diffusion with Explicit mask alignment, termed STAGE. STAGE introduces a novel anomaly inference strategy that incorporates clean background information as a prior to guide the denoising distribution, enabling the model to more effectively distinguish and highlight abnormal foregrounds. Furthermore, it employs a graded diffusion framework with an anomaly-only branch to explicitly record local anomalies during both the forward and reverse processes, ensuring that subtle anomalies are not overlooked. Finally, STAGE incorporates the explicit mask alignment (EMA) strategy to progressively align the synthesized anomalies with the background, resulting in context-consistent and structurally coherent generations. Extensive experiments on the MVTec and BTAD datasets demonstrate that STAGE achieves state-of-the-art performance in SIAS, which in turn enhances downstream anomaly segmentation.", "AI": {"tldr": "STAGE\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u7ea7\u6ef4\u679c\u6a21\u578b\u7684\u5f02\u5e38\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u660e\u786e\u63a9\u7801\u5bf9\u9f50\u7b56\u7565\u548c\u5f02\u5e38\u4ec5\u5206\u652f\u6765\u63d0\u9ad8\u5f02\u5e38\u5206\u5272\u7684\u7cbe\u5ea6\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u5272\u5bfc\u5411\u5de5\u4e1a\u5f02\u5e38\u5408\u6210\u65b9\u6cd5\u5b58\u5728\u7ec6\u8282\u7f3a\u5931\u3001\u80cc\u666f\u5bf9\u9f50\u4e0d\u51c6\u548c\u50cf\u7d20\u7ea7\u5f02\u5e38\u751f\u6210\u56f0\u96be\u7b49\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6e05\u6d01\u80cc\u666f\u4f5c\u4e3a\u5148\u9a8c\u4fe1\u606f\u6307\u5bfc\u53bb\u566a\u5206\u5e03\uff0c\u91c7\u7528\u5206\u7ea7\u6ef4\u679c\u6846\u67b6\u4e0e\u5f02\u5e38\u4ec5\u5206\u652f\u8bb0\u5f55\u5c40\u90e8\u5f02\u5e38\uff0c\u901a\u8fc7EMA\u7b56\u7565\u8fdb\u884c\u6e10\u8fdb\u5f0f\u63a9\u7801\u5bf9\u9f50\u3002", "result": "\u5728MVTec\u548cBTAD\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684SIAS\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e0b\u6e38\u5f02\u5e38\u5206\u5272\u6548\u679c\u3002", "conclusion": "STAGE\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u5177\u6709\u7ec6\u81f4\u7eb9\u7406\u548c\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u7684\u5de5\u4e1a\u5f02\u5e38\u6570\u636e\uff0c\u4e3a\u5f02\u5e38\u68c0\u6d4b\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6570\u636e\u6269\u5145\u65b9\u6848\u3002"}}
{"id": "2509.06723", "pdf": "https://arxiv.org/pdf/2509.06723", "abs": "https://arxiv.org/abs/2509.06723", "authors": ["Ruicheng Zhang", "Jun Zhou", "Zunnan Xu", "Zihao Liu", "Jiehui Huang", "Mingyang Zhang", "Yu Sun", "Xiu Li"], "title": "Zero-shot 3D-Aware Trajectory-Guided image-to-video generation via Test-Time Training", "categories": ["cs.CV"], "comment": null, "summary": "Trajectory-Guided image-to-video (I2V) generation aims to synthesize videos that adhere to user-specified motion instructions. Existing methods typically rely on computationally expensive fine-tuning on scarce annotated datasets. Although some zero-shot methods attempt to trajectory control in the latent space, they may yield unrealistic motion by neglecting 3D perspective and creating a misalignment between the manipulated latents and the network's noise predictions. To address these challenges, we introduce Zo3T, a novel zero-shot test-time-training framework for trajectory-guided generation with three core innovations: First, we incorporate a 3D-Aware Kinematic Projection, leveraging inferring scene depth to derive perspective-correct affine transformations for target regions. Second, we introduce Trajectory-Guided Test-Time LoRA, a mechanism that dynamically injects and optimizes ephemeral LoRA adapters into the denoising network alongside the latent state. Driven by a regional feature consistency loss, this co-adaptation effectively enforces motion constraints while allowing the pre-trained model to locally adapt its internal representations to the manipulated latent, thereby ensuring generative fidelity and on-manifold adherence. Finally, we develop Guidance Field Rectification, which refines the denoising evolutionary path by optimizing the conditional guidance field through a one-step lookahead strategy, ensuring efficient generative progression towards the target trajectory. Zo3T significantly enhances 3D realism and motion accuracy in trajectory-controlled I2V generation, demonstrating superior performance over existing training-based and zero-shot approaches.", "AI": {"tldr": "Zo3T\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc73D\u611f\u77e5\u8fd0\u52a8\u6295\u5f71\u3001\u8f68\u8ff9\u5f15\u5bfc\u6d4b\u8bd5\u65f6LoRA\u9002\u914d\u5668\u548c\u5f15\u5bfc\u573a\u6821\u6b63\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u8f68\u8ff9\u5f15\u5bfc\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\uff0c\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u5fae\u8c03\u3002", "motivation": "\u73b0\u6709\u7684\u8f68\u8ff9\u5f15\u5bfc\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u8ba1\u7b97\u6602\u8d35\u7684\u5fae\u8c03\u548c\u7a00\u7f3a\u7684\u6807\u6ce8\u6570\u636e\uff0c\u8981\u4e48\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u8f68\u8ff9\u63a7\u5236\u65f6\u5ffd\u75653D\u900f\u89c6\uff0c\u5bfc\u81f4\u4e0d\u771f\u5b9e\u7684\u8fd0\u52a8\u548c\u6f5c\u5728\u7a7a\u95f4\u4e0e\u7f51\u7edc\u566a\u58f0\u9884\u6d4b\u4e4b\u95f4\u7684\u4e0d\u5bf9\u9f50\u3002", "method": "1. 3D\u611f\u77e5\u8fd0\u52a8\u6295\u5f71\uff1a\u5229\u7528\u63a8\u65ad\u7684\u573a\u666f\u6df1\u5ea6\u63a8\u5bfc\u900f\u89c6\u6b63\u786e\u7684\u4eff\u5c04\u53d8\u6362\uff1b2. \u8f68\u8ff9\u5f15\u5bfc\u6d4b\u8bd5\u65f6LoRA\uff1a\u52a8\u6001\u6ce8\u5165\u548c\u4f18\u5316\u4e34\u65f6LoRA\u9002\u914d\u5668\uff0c\u901a\u8fc7\u533a\u57df\u7279\u5f81\u4e00\u81f4\u6027\u635f\u5931\u5b9e\u73b0\u8fd0\u52a8\u7ea6\u675f\uff1b3. \u5f15\u5bfc\u573a\u6821\u6b63\uff1a\u901a\u8fc7\u4e00\u6b65\u524d\u77bb\u7b56\u7565\u4f18\u5316\u6761\u4ef6\u5f15\u5bfc\u573a\uff0c\u786e\u4fdd\u751f\u6210\u8fc7\u7a0b\u671d\u5411\u76ee\u6807\u8f68\u8ff9\u3002", "result": "Zo3T\u663e\u8457\u63d0\u9ad8\u4e86\u8f68\u8ff9\u63a7\u5236\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u76843D\u771f\u5b9e\u611f\u548c\u8fd0\u52a8\u51c6\u786e\u6027\uff0c\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u8bad\u7ec3\u548c\u96f6\u6837\u672c\u65b9\u6cd5\u3002", "conclusion": "Zo3T\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u76843D\u611f\u77e5\u6280\u672f\u548c\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8f68\u8ff9\u5f15\u5bfc\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u9ad8\u8d28\u91cf\u89c6\u9891\u5408\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u96f6\u6837\u672c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.06793", "pdf": "https://arxiv.org/pdf/2509.06793", "abs": "https://arxiv.org/abs/2509.06793", "authors": ["George Ciubotariu", "Florin-Alexandru Vasluianu", "Zhuyun Zhou", "Nancy Mehta", "Radu Timofte", "Ke Wu", "Long Sun", "Lingshun Kong", "Zhongbao Yang", "Jinshan Pan", "Jiangxin Dong", "Jinhui Tang", "Hao Chen", "Yinghui Fang", "Dafeng Zhang", "Yongqi Song", "Jiangbo Guo", "Shuhua Jin", "Zeyu Xiao", "Rui Zhao", "Zhuoyuan Li", "Cong Zhang", "Yufeng Peng", "Xin Lu", "Zhijing Sun", "Chengjie Ge", "Zihao Li", "Zishun Liao", "Ziang Zhou", "Qiyu Kang", "Xueyang Fu", "Zheng-Jun Zha", "Yuqian Zhang", "Shuai Liu", "Jie Liu", "Zhuhao Zhang", "Lishen Qu", "Zhihao Liu", "Shihao Zhou", "Yaqi Luo", "Juncheng Zhou", "Jufeng Yang", "Qianfeng Yang", "Qiyuan Guan", "Xiang Chen", "Guiyue Jin", "Jiyu Jin"], "title": "AIM 2025 Challenge on High FPS Motion Deblurring: Methods and Results", "categories": ["cs.CV"], "comment": "ICCVW AIM 2025", "summary": "This paper presents a comprehensive review of the AIM 2025 High FPS Non-Uniform Motion Deblurring Challenge, highlighting the proposed solutions and final results. The objective of this challenge is to identify effective networks capable of producing clearer and visually compelling images in diverse and challenging conditions, by learning representative visual cues for complex aggregations of motion types. A total of 68 participants registered for the competition, and 9 teams ultimately submitted valid entries. This paper thoroughly evaluates the state-of-the-art advances in high-FPS single image motion deblurring, showcasing the significant progress in the field, while leveraging samples of the novel dataset, MIORe, that introduces challenging examples of movement patterns.", "AI": {"tldr": "AIM 2025\u9ad8\u5e27\u7387\u975e\u5747\u5300\u8fd0\u52a8\u53bb\u6a21\u7cca\u6311\u6218\u8d5b\u7efc\u8ff0\uff0c\u8bc4\u4f30\u4e869\u4e2a\u56e2\u961f\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5355\u56fe\u50cf\u8fd0\u52a8\u53bb\u6a21\u7cca\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u4f7f\u7528\u4e86\u65b0\u7684MIORe\u6570\u636e\u96c6\u3002", "motivation": "\u8bc6\u522b\u80fd\u591f\u5728\u591a\u6837\u5316\u6311\u6218\u6761\u4ef6\u4e0b\u901a\u8fc7\u5b66\u4e60\u590d\u6742\u8fd0\u52a8\u7c7b\u578b\u7684\u4ee3\u8868\u6027\u89c6\u89c9\u7ebf\u7d22\uff0c\u4ea7\u751f\u66f4\u6e05\u6670\u548c\u89c6\u89c9\u5438\u5f15\u529b\u56fe\u50cf\u7684\u6709\u6548\u7f51\u7edc\u3002", "method": "\u901a\u8fc7\u7ade\u8d5b\u5f62\u5f0f\uff0c68\u540d\u53c2\u4e0e\u8005\u6ce8\u518c\uff0c9\u4e2a\u56e2\u961f\u63d0\u4ea4\u6709\u6548\u65b9\u6848\uff0c\u4f7f\u7528\u65b0\u9896\u7684MIORe\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u5177\u6709\u6311\u6218\u6027\u7684\u8fd0\u52a8\u6a21\u5f0f\u6837\u672c\u3002", "result": "\u5c55\u793a\u4e86\u9ad8\u5e27\u7387\u5355\u56fe\u50cf\u8fd0\u52a8\u53bb\u6a21\u7cca\u9886\u57df\u7684\u6700\u5148\u8fdb\u8fdb\u5c55\uff0c\u8bc1\u660e\u4e86\u8be5\u9886\u57df\u7684\u663e\u8457\u8fdb\u6b65\u3002", "conclusion": "\u6311\u6218\u8d5b\u6210\u529f\u8bc6\u522b\u4e86\u6709\u6548\u7684\u53bb\u6a21\u7cca\u7f51\u7edc\uff0cMIORe\u6570\u636e\u96c6\u4e3a\u590d\u6742\u8fd0\u52a8\u6a21\u5f0f\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6d4b\u8bd5\u57fa\u51c6\uff0c\u63a8\u52a8\u4e86\u975e\u5747\u5300\u8fd0\u52a8\u53bb\u6a21\u7cca\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.06803", "pdf": "https://arxiv.org/pdf/2509.06803", "abs": "https://arxiv.org/abs/2509.06803", "authors": ["George Ciubotariu", "Zhuyun Zhou", "Zongwei Wu", "Radu Timofte"], "title": "MIORe & VAR-MIORe: Benchmarks to Push the Boundaries of Restoration", "categories": ["cs.CV"], "comment": "ICCV 2025 Oral", "summary": "We introduce MIORe and VAR-MIORe, two novel multi-task datasets that address critical limitations in current motion restoration benchmarks. Designed with high-frame-rate (1000 FPS) acquisition and professional-grade optics, our datasets capture a broad spectrum of motion scenarios, which include complex ego-camera movements, dynamic multi-subject interactions, and depth-dependent blur effects. By adaptively averaging frames based on computed optical flow metrics, MIORe generates consistent motion blur, and preserves sharp inputs for video frame interpolation and optical flow estimation. VAR-MIORe further extends by spanning a variable range of motion magnitudes, from minimal to extreme, establishing the first benchmark to offer explicit control over motion amplitude. We provide high-resolution, scalable ground truths that challenge existing algorithms under both controlled and adverse conditions, paving the way for next-generation research of various image and video restoration tasks.", "AI": {"tldr": "MIORe\u548cVAR-MIORe\u662f\u4e24\u4e2a\u65b0\u9896\u7684\u591a\u4efb\u52a1\u8fd0\u52a8\u6062\u590d\u6570\u636e\u96c6\uff0c\u901a\u8fc71000FPS\u9ad8\u901f\u91c7\u96c6\u548c\u4e13\u4e1a\u5149\u5b66\u8bbe\u5907\u6355\u6349\u590d\u6742\u8fd0\u52a8\u573a\u666f\uff0c\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u8fd0\u52a8\u6a21\u7cca\u548c\u6e05\u6670\u5e27\u6570\u636e\uff0c\u7528\u4e8e\u89c6\u9891\u5e27\u63d2\u503c\u548c\u5149\u6d41\u4f30\u8ba1\u7b49\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u8fd0\u52a8\u6062\u590d\u57fa\u51c6\u6d4b\u8bd5\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u8fd0\u52a8\u573a\u666f\u6570\u636e\u548c\u7cbe\u786e\u7684\u8fd0\u52a8\u5e45\u5ea6\u63a7\u5236\u3002", "method": "\u91c7\u75281000FPS\u9ad8\u901f\u91c7\u96c6\u548c\u4e13\u4e1a\u5149\u5b66\u8bbe\u5907\uff0c\u57fa\u4e8e\u8ba1\u7b97\u7684\u5149\u6d41\u6307\u6807\u81ea\u9002\u5e94\u5e73\u5747\u5e27\u6765\u751f\u6210\u4e00\u81f4\u7684\u8fd0\u52a8\u6a21\u7cca\uff0c\u540c\u65f6\u4fdd\u7559\u6e05\u6670\u8f93\u5165\u5e27\u3002VAR-MIORe\u8fdb\u4e00\u6b65\u6269\u5c55\u4e86\u53ef\u53d8\u8fd0\u52a8\u5e45\u5ea6\u8303\u56f4\u3002", "result": "\u521b\u5efa\u4e86\u9996\u4e2a\u80fd\u591f\u63d0\u4f9b\u8fd0\u52a8\u5e45\u5ea6\u663e\u5f0f\u63a7\u5236\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63d0\u4f9b\u9ad8\u5206\u8fa8\u7387\u3001\u53ef\u6269\u5c55\u7684\u771f\u5b9e\u6570\u636e\uff0c\u5728\u53d7\u63a7\u548c\u6076\u52a3\u6761\u4ef6\u4e0b\u6311\u6218\u73b0\u6709\u7b97\u6cd5\u3002", "conclusion": "\u8fd9\u4e9b\u6570\u636e\u96c6\u4e3a\u4e0b\u4e00\u4ee3\u56fe\u50cf\u548c\u89c6\u9891\u6062\u590d\u4efb\u52a1\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u8fd0\u52a8\u6062\u590d\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.06868", "pdf": "https://arxiv.org/pdf/2509.06868", "abs": "https://arxiv.org/abs/2509.06868", "authors": ["Behnoud Shafiezadeh", "Amir Mashmool", "Farshad Eshghi", "Manoochehr Kelarestaghi"], "title": "A New Hybrid Model of Generative Adversarial Network and You Only Look Once Algorithm for Automatic License-Plate Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Automatic License-Plate Recognition (ALPR) plays a pivotal role in Intelligent Transportation Systems (ITS) as a fundamental element of Smart Cities. However, due to its high variability, ALPR faces challenging issues more efficiently addressed by deep learning techniques. In this paper, a selective Generative Adversarial Network (GAN) is proposed for deblurring in the preprocessing step, coupled with the state-of-the-art You-Only-Look-Once (YOLO)v5 object detection architectures for License-Plate Detection (LPD), and the integrated Character Segmentation (CS) and Character Recognition (CR) steps. The selective preprocessing bypasses unnecessary and sometimes counter-productive input manipulations, while YOLOv5 LPD/CS+CR delivers high accuracy and low computing cost. As a result, YOLOv5 achieves a detection time of 0.026 seconds for both LP and CR detection stages, facilitating real-time applications with exceptionally rapid responsiveness. Moreover, the proposed model achieves accuracy rates of 95\\% and 97\\% in the LPD and CR detection phases, respectively. Furthermore, the inclusion of the Deblur-GAN pre-processor significantly improves detection accuracy by nearly 40\\%, especially when encountering blurred License Plates (LPs).To train and test the learning components, we generated and publicly released our blur and ALPR datasets (using Iranian license plates as a use-case), which are more representative of close-to-real-life ad-hoc situations. The findings demonstrate that employing the state-of-the-art YOLO model results in excellent overall precision and detection time, making it well-suited for portable applications. Additionally, integrating the Deblur-GAN model as a preliminary processing step enhances the overall effectiveness of our comprehensive model, particularly when confronted with blurred scenes captured by the camera as input.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9009\u62e9\u6027GAN\u53bb\u6a21\u7cca\u9884\u5904\u7406\u548cYOLOv5\u76ee\u6807\u68c0\u6d4b\u7684\u81ea\u52a8\u8f66\u724c\u8bc6\u522b\u7cfb\u7edf\uff0c\u5728\u4f0a\u6717\u8f66\u724c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8695%\u7684\u8f66\u724c\u68c0\u6d4b\u51c6\u786e\u7387\u548c97%\u7684\u5b57\u7b26\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u68c0\u6d4b\u65f6\u95f4\u4ec50.026\u79d2\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6a21\u7cca\u573a\u666f\u3002", "motivation": "\u81ea\u52a8\u8f66\u724c\u8bc6\u522b(ALPR)\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u8f66\u724c\u7684\u9ad8\u53d8\u5f02\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u3002\u7279\u522b\u662f\u6a21\u7cca\u8f66\u724c\u7684\u5904\u7406\u6210\u4e3a\u6280\u672f\u96be\u70b9\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6765\u89e3\u51b3\u3002", "method": "\u91c7\u7528\u9009\u62e9\u6027GAN\u8fdb\u884c\u53bb\u6a21\u7cca\u9884\u5904\u7406\uff0c\u7ed3\u5408YOLOv5\u8fdb\u884c\u8f66\u724c\u68c0\u6d4b\u548c\u5b57\u7b26\u5206\u5272\u8bc6\u522b\u3002\u9009\u62e9\u6027\u9884\u5904\u7406\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u8f93\u5165\u64cd\u4f5c\uff0cYOLOv5\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "result": "YOLOv5\u5728\u8f66\u724c\u68c0\u6d4b\u548c\u5b57\u7b26\u8bc6\u522b\u9636\u6bb5\u4ec5\u97000.026\u79d2\u68c0\u6d4b\u65f6\u95f4\uff0c\u8f66\u724c\u68c0\u6d4b\u51c6\u786e\u738795%\uff0c\u5b57\u7b26\u8bc6\u522b\u51c6\u786e\u738797%\u3002Deblur-GAN\u9884\u5904\u7406\u4f7f\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u5347\u8fd140%\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6a21\u7cca\u8f66\u724c\u573a\u666f\u3002", "conclusion": "\u7ed3\u5408YOLOv5\u548cDeblur-GAN\u7684\u96c6\u6210\u6a21\u578b\u5728\u7cbe\u5ea6\u548c\u68c0\u6d4b\u65f6\u95f4\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u9002\u5408\u4fbf\u643a\u5f0f\u5e94\u7528\u548c\u6a21\u7cca\u8f93\u5165\u573a\u666f\uff0c\u4e3a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.06904", "pdf": "https://arxiv.org/pdf/2509.06904", "abs": "https://arxiv.org/abs/2509.06904", "authors": ["Cem Eteke", "Alexander Griessel", "Wolfgang Kellerer", "Eckehard Steinbach"], "title": "BIR-Adapter: A Low-Complexity Diffusion Model Adapter for Blind Image Restoration", "categories": ["cs.CV"], "comment": "20 pages, 14 figures", "summary": "This paper introduces BIR-Adapter, a low-complexity blind image restoration adapter for diffusion models. The BIR-Adapter enables the utilization of the prior of pre-trained large-scale diffusion models on blind image restoration without training any auxiliary feature extractor. We take advantage of the robustness of pretrained models. We extract features from degraded images via the model itself and extend the self-attention mechanism with these degraded features. We introduce a sampling guidance mechanism to reduce hallucinations. We perform experiments on synthetic and real-world degradations and demonstrate that BIR-Adapter achieves competitive or better performance compared to state-of-the-art methods while having significantly lower complexity. Additionally, its adapter-based design enables integration into other diffusion models, enabling broader applications in image restoration tasks. We showcase this by extending a super-resolution-only model to perform better under additional unknown degradations.", "AI": {"tldr": "BIR-Adapter\u662f\u4e00\u4e2a\u4f4e\u590d\u6742\u5ea6\u7684\u76f2\u56fe\u50cf\u6062\u590d\u9002\u914d\u5668\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u5229\u7528\u9884\u8bad\u7ec3\u5927\u89c4\u6a21\u6269\u6563\u6a21\u578b\u7684\u5148\u9a8c\u77e5\u8bc6\u8fdb\u884c\u76f2\u56fe\u50cf\u6062\u590d\uff0c\u907f\u514d\u8bad\u7ec3\u989d\u5916\u7684\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "method": "\u4ece\u9000\u5316\u56fe\u50cf\u4e2d\u901a\u8fc7\u6a21\u578b\u672c\u8eab\u63d0\u53d6\u7279\u5f81\uff0c\u6269\u5c55\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5f15\u5165\u9000\u5316\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u91c7\u6837\u5f15\u5bfc\u673a\u5236\u51cf\u5c11\u5e7b\u89c9\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u9000\u5316\u6570\u636e\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8a\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u540c\u65f6\u8ba1\u7b97\u590d\u6742\u5ea6\u663e\u8457\u964d\u4f4e\uff0c\u53ef\u96c6\u6210\u5230\u5176\u4ed6\u6269\u6563\u6a21\u578b\u4e2d\u6269\u5c55\u5e94\u7528\u3002", "conclusion": "BIR-Adapter\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u76f2\u56fe\u50cf\u6062\u590d\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u5177\u6709\u826f\u597d\u7684\u6269\u5c55\u6027\u548c\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.06945", "pdf": "https://arxiv.org/pdf/2509.06945", "abs": "https://arxiv.org/abs/2509.06945", "authors": ["Wenxuan Huang", "Shuang Chen", "Zheyong Xie", "Shaosheng Cao", "Shixiang Tang", "Yufan Shen", "Qingyu Yin", "Wenbo Hu", "Xiaoman Wang", "Yuntian Tang", "Junbo Qiao", "Yue Guo", "Yao Hu", "Zhenfei Yin", "Philip Torr", "Yu Cheng", "Wanli Ouyang", "Shaohui Lin"], "title": "Interleaving Reasoning for Better Text-to-Image Generation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Unified multimodal understanding and generation models recently have achieve significant improvement in image generation capability, yet a large gap remains in instruction following and detail preservation compared to systems that tightly couple comprehension with generation such as GPT-4o. Motivated by recent advances in interleaving reasoning, we explore whether such reasoning can further improve Text-to-Image (T2I) generation. We introduce Interleaving Reasoning Generation (IRG), a framework that alternates between text-based thinking and image synthesis: the model first produces a text-based thinking to guide an initial image, then reflects on the result to refine fine-grained details, visual quality, and aesthetics while preserving semantics. To train IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL), which targets two sub-goals: (1) strengthening the initial think-and-generate stage to establish core content and base quality, and (2) enabling high-quality textual reflection and faithful implementation of those refinements in a subsequent image. We curate IRGL-300K, a dataset organized into six decomposed learning modes that jointly cover learning text-based thinking, and full thinking-image trajectories. Starting from a unified foundation model that natively emits interleaved text-image outputs, our two-stage training first builds robust thinking and reflection, then efficiently tunes the IRG pipeline in the full thinking-image trajectory data. Extensive experiments show SoTA performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality and fine-grained fidelity. The code, model weights and datasets will be released in: https://github.com/Osilly/Interleaving-Reasoning-Generation .", "AI": {"tldr": "IRG\u6846\u67b6\u901a\u8fc7\u4ea4\u66ff\u6587\u672c\u63a8\u7406\u548c\u56fe\u50cf\u5408\u6210\u6765\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u8d28\u91cf\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f975-10\u4e2a\u767e\u5206\u70b9\u7684\u7edd\u5bf9\u63d0\u5347", "motivation": "\u73b0\u6709\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u80fd\u529b\u4e0a\u867d\u6709\u8fdb\u6b65\uff0c\u4f46\u5728\u6307\u4ee4\u9075\u5faa\u548c\u7ec6\u8282\u4fdd\u6301\u65b9\u9762\u4ecd\u843d\u540e\u4e8eGPT-4o\u7b49\u7406\u89e3-\u751f\u6210\u7d27\u5bc6\u8026\u5408\u7684\u7cfb\u7edf\uff0c\u56e0\u6b64\u63a2\u7d22\u4ea4\u66ff\u63a8\u7406\u662f\u5426\u80fd\u8fdb\u4e00\u6b65\u6539\u8fdbT2I\u751f\u6210", "method": "\u63d0\u51faIRG\u6846\u67b6\uff1a\u9996\u5148\u751f\u6210\u6587\u672c\u63a8\u7406\u6307\u5bfc\u521d\u59cb\u56fe\u50cf\uff0c\u7136\u540e\u5bf9\u7ed3\u679c\u8fdb\u884c\u53cd\u601d\u4ee5\u7ec6\u5316\u7ec6\u8282\u3001\u89c6\u89c9\u8d28\u91cf\u548c\u7f8e\u5b66\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u3002\u4f7f\u7528IRGL-300K\u6570\u636e\u96c6\u8fdb\u884c\u4e24\u9636\u6bb5\u8bad\u7ec3\uff0c\u5148\u5efa\u7acb\u7a33\u5065\u7684\u601d\u8003\u548c\u53cd\u601d\u80fd\u529b\uff0c\u7136\u540e\u5728\u5b8c\u6574\u601d\u8003-\u56fe\u50cf\u8f68\u8ff9\u6570\u636e\u4e0a\u5fae\u8c03", "result": "\u5728GenEval\u3001WISE\u3001TIIF\u3001GenAI-Bench\u548cOneIG-EN\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5b9e\u73b0state-of-the-art\u6027\u80fd\uff0c\u83b7\u5f975-10\u4e2a\u767e\u5206\u70b9\u7684\u7edd\u5bf9\u589e\u76ca\uff0c\u89c6\u89c9\u8d28\u91cf\u548c\u7ec6\u7c92\u5ea6\u4fdd\u771f\u5ea6\u663e\u8457\u63d0\u5347", "conclusion": "\u4ea4\u66ff\u63a8\u7406\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u8d28\u91cf\u548c\u7ec6\u8282\u4fdd\u6301\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5f15\u5165\u6587\u672c\u63a8\u7406\u53cd\u601d\u673a\u5236\u7684\u91cd\u8981\u4ef7\u503c"}}
{"id": "2509.05314", "pdf": "https://arxiv.org/pdf/2509.05314", "abs": "https://arxiv.org/abs/2509.05314", "authors": ["Ying Li", "Xiaobao Wei", "Xiaowei Chi", "Yuming Li", "Zhongyu Zhao", "Hao Wang", "Ningning Ma", "Ming Lu", "Shanghang Zhang"], "title": "ManipDreamer3D : Synthesizing Plausible Robotic Manipulation Video with Occupancy-aware 3D Trajectory", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "8pages; 7figures; 4 tables", "summary": "Data scarcity continues to be a major challenge in the field of robotic manipulation. Although diffusion models provide a promising solution for generating robotic manipulation videos, existing methods largely depend on 2D trajectories, which inherently face issues with 3D spatial ambiguity. In this work, we present a novel framework named ManipDreamer3D for generating plausible 3D-aware robotic manipulation videos from the input image and the text instruction. Our method combines 3D trajectory planning with a reconstructed 3D occupancy map created from a third-person perspective, along with a novel trajectory-to-video diffusion model. Specifically, ManipDreamer3D first reconstructs the 3D occupancy representation from the input image and then computes an optimized 3D end-effector trajectory, minimizing path length while avoiding collisions. Next, we employ a latent editing technique to create video sequences from the initial image latent and the optimized 3D trajectory. This process conditions our specially trained trajectory-to-video diffusion model to produce robotic pick-and-place videos. Our method generates robotic videos with autonomously planned plausible 3D trajectories, significantly reducing human intervention requirements. Experimental results demonstrate superior visual quality compared to existing methods.", "AI": {"tldr": "ManipDreamer3D\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u54083D\u8f68\u8ff9\u89c4\u5212\u548c\u8f68\u8ff9\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u4ece\u8f93\u5165\u56fe\u50cf\u548c\u6587\u672c\u6307\u4ee4\u751f\u6210\u903c\u771f\u76843D\u611f\u77e5\u673a\u5668\u4eba\u64cd\u4f5c\u89c6\u9891\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d562D\u8f68\u8ff9\u5bfc\u81f4\u76843D\u7a7a\u95f4\u6a21\u7cca\u95ee\u9898\u3002", "motivation": "\u673a\u5668\u4eba\u64cd\u4f5c\u9886\u57df\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u73b0\u6709\u6269\u6563\u6a21\u578b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d562D\u8f68\u8ff9\uff0c\u5b58\u57283D\u7a7a\u95f4\u6a21\u7cca\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u751f\u6210\u5177\u6709\u5408\u74063D\u8f68\u8ff9\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u89c6\u9891\u7684\u65b9\u6cd5\u3002", "method": "1) \u4ece\u8f93\u5165\u56fe\u50cf\u91cd\u5efa3D\u5360\u636e\u56fe\u8868\u793a\uff1b2) \u8ba1\u7b97\u4f18\u5316\u76843D\u672b\u7aef\u6267\u884c\u5668\u8f68\u8ff9\uff0c\u6700\u5c0f\u5316\u8def\u5f84\u957f\u5ea6\u5e76\u907f\u514d\u78b0\u649e\uff1b3) \u4f7f\u7528\u6f5c\u5728\u7f16\u8f91\u6280\u672f\u4ece\u521d\u59cb\u56fe\u50cf\u6f5c\u5728\u548c\u4f18\u5316\u8f68\u8ff9\u521b\u5efa\u89c6\u9891\u5e8f\u5217\uff1b4) \u901a\u8fc7\u4e13\u95e8\u8bad\u7ec3\u7684\u8f68\u8ff9\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u62fe\u53d6\u653e\u7f6e\u89c6\u9891\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u673a\u5668\u4eba\u89c6\u9891\u5177\u6709\u81ea\u4e3b\u89c4\u5212\u7684\u5408\u74063D\u8f68\u8ff9\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u5e72\u9884\u9700\u6c42\uff0c\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ManipDreamer3D\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e863D\u7a7a\u95f4\u6a21\u7cca\u95ee\u9898\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u76843D\u611f\u77e5\u673a\u5668\u4eba\u64cd\u4f5c\u89c6\u9891\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u6570\u636e\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.05315", "pdf": "https://arxiv.org/pdf/2509.05315", "abs": "https://arxiv.org/abs/2509.05315", "authors": ["Petros Loukas", "David Bassir", "Savvas Chatzichristofis", "Angelos Amanatiadis"], "title": "Evaluation of Large Language Models for Anomaly Detection in Autonomous Vehicles", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "The rapid evolution of large language models (LLMs) has pushed their boundaries to many applications in various domains. Recently, the research community has started to evaluate their potential adoption in autonomous vehicles and especially as complementary modules in the perception and planning software stacks. However, their evaluation is limited in synthetic datasets or manually driving datasets without the ground truth knowledge and more precisely, how the current perception and planning algorithms would perform in the cases under evaluation. For this reason, this work evaluates LLMs on real-world edge cases where current autonomous vehicles have been proven to fail. The proposed architecture consists of an open vocabulary object detector coupled with prompt engineering and large language model contextual reasoning. We evaluate several state-of-the-art models against real edge cases and provide qualitative comparison results along with a discussion on the findings for the potential application of LLMs as anomaly detectors in autonomous vehicles.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u771f\u5b9e\u8fb9\u7f18\u6848\u4f8b\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86\u7ed3\u5408\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u548c\u63d0\u793a\u5de5\u7a0b\u7684\u67b6\u6784\uff0c\u7528\u4e8e\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u5f53\u524dLLM\u8bc4\u4f30\u4e3b\u8981\u5c40\u9650\u4e8e\u5408\u6210\u6570\u636e\u96c6\u6216\u4eba\u5de5\u9a7e\u9a76\u6570\u636e\u96c6\uff0c\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u8fb9\u7f18\u6848\u4f8b\u7684\u6d4b\u8bd5\uff0c\u7279\u522b\u662f\u73b0\u6709\u611f\u77e5\u89c4\u5212\u7b97\u6cd5\u5728\u8fd9\u4e9b\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\u8bc4\u4f30\u3002", "method": "\u91c7\u7528\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u5668\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u67b6\u6784\uff0c\u5728\u771f\u5b9e\u8fb9\u7f18\u6848\u4f8b\u4e0a\u8bc4\u4f30\u591a\u4e2a\u5148\u8fdb\u6a21\u578b\u3002", "result": "\u63d0\u4f9b\u4e86\u5b9a\u6027\u6bd4\u8f83\u7ed3\u679c\uff0c\u8ba8\u8bba\u4e86LLM\u4f5c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5f02\u5e38\u68c0\u6d4b\u5668\u7684\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "LLM\u5728\u81ea\u52a8\u9a7e\u9a76\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u5177\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u771f\u5b9e\u4e16\u754c\u8fb9\u7f18\u6848\u4f8b\u65f6\u80fd\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2509.05374", "pdf": "https://arxiv.org/pdf/2509.05374", "abs": "https://arxiv.org/abs/2509.05374", "authors": ["Zhiqiang Yuan", "Jinchao Zhang", "Jie Zhou"], "title": "A Synthetic-to-Real Dehazing Method based on Domain Unification", "categories": ["eess.IV", "cs.CV"], "comment": "ICME 2025 Accept", "summary": "Due to distribution shift, the performance of deep learning-based method for image dehazing is adversely affected when applied to real-world hazy images. In this paper, we find that such deviation in dehazing task between real and synthetic domains may come from the imperfect collection of clean data. Owing to the complexity of the scene and the effect of depth, the collected clean data cannot strictly meet the ideal conditions, which makes the atmospheric physics model in the real domain inconsistent with that in the synthetic domain. For this reason, we come up with a synthetic-to-real dehazing method based on domain unification, which attempts to unify the relationship between the real and synthetic domain, thus to let the dehazing model more in line with the actual situation. Extensive experiments qualitatively and quantitatively demonstrate that the proposed dehazing method significantly outperforms state-of-the-art methods on real-world images.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9886\u57df\u7edf\u4e00\u7684\u5408\u6210\u5230\u771f\u5b9e\u53bb\u96fe\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u4e00\u771f\u5b9e\u548c\u5408\u6210\u9886\u57df\u7684\u5173\u7cfb\uff0c\u4f7f\u53bb\u96fe\u6a21\u578b\u66f4\u7b26\u5408\u5b9e\u9645\u60c5\u51b5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u771f\u5b9e\u96fe\u973e\u56fe\u50cf\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u5206\u5e03\u504f\u79fb\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u56fe\u50cf\u53bb\u96fe\u65b9\u6cd5\u5728\u5e94\u7528\u4e8e\u771f\u5b9e\u96fe\u973e\u56fe\u50cf\u65f6\u6027\u80fd\u4e0b\u964d\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u79cd\u504f\u5dee\u6e90\u4e8e\u5e72\u51c0\u6570\u636e\u6536\u96c6\u7684\u4e0d\u5b8c\u7f8e\uff0c\u771f\u5b9e\u9886\u57df\u7684\u5927\u6c14\u7269\u7406\u6a21\u578b\u4e0e\u5408\u6210\u9886\u57df\u4e0d\u4e00\u81f4\u3002", "method": "\u57fa\u4e8e\u9886\u57df\u7edf\u4e00\u7684\u5408\u6210\u5230\u771f\u5b9e\u53bb\u96fe\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u4e00\u771f\u5b9e\u548c\u5408\u6210\u9886\u57df\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4f7f\u53bb\u96fe\u6a21\u578b\u66f4\u7b26\u5408\u5b9e\u9645\u573a\u666f\u7684\u590d\u6742\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u5b9a\u6027\u548c\u5b9a\u91cf\u5730\u8bc1\u660e\uff0c\u6240\u63d0\u51fa\u7684\u53bb\u96fe\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u9886\u57df\u7edf\u4e00\u7684\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u5408\u6210\u4e0e\u771f\u5b9e\u9886\u57df\u4e4b\u95f4\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u5347\u53bb\u96fe\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2509.05469", "pdf": "https://arxiv.org/pdf/2509.05469", "abs": "https://arxiv.org/abs/2509.05469", "authors": ["Chenguang Wang", "Xiang Yan", "Yilong Dai", "Ziyi Wang", "Susu Xu"], "title": "From Image Generation to Infrastructure Design: a Multi-agent Pipeline for Street Design Generation", "categories": ["cs.AI", "cs.CV", "cs.CY", "cs.HC"], "comment": "21 pages, 8 figures", "summary": "Realistic visual renderings of street-design scenarios are essential for public engagement in active transportation planning. Traditional approaches are labor-intensive, hindering collective deliberation and collaborative decision-making. While AI-assisted generative design shows transformative potential by enabling rapid creation of design scenarios, existing generative approaches typically require large amounts of domain-specific training data and struggle to enable precise spatial variations of design/configuration in complex street-view scenes. We introduce a multi-agent system that edits and redesigns bicycle facilities directly on real-world street-view imagery. The framework integrates lane localization, prompt optimization, design generation, and automated evaluation to synthesize realistic, contextually appropriate designs. Experiments across diverse urban scenarios demonstrate that the system can adapt to varying road geometries and environmental conditions, consistently yielding visually coherent and instruction-compliant results. This work establishes a foundation for applying multi-agent pipelines to transportation infrastructure planning and facility design.", "AI": {"tldr": "\u4e00\u4e2a\u591a\u6a21\u6001\u591a\u6ee1\u610f\u7cfb\u7edf\uff0c\u901a\u8fc7\u8f66\u9053\u5b9a\u4f4d\u3001\u63d0\u793a\u4f18\u5316\u3001\u8bbe\u8ba1\u751f\u6210\u548c\u81ea\u52a8\u8bc4\u4f30\u7b49\u6b65\u9aa4\uff0c\u5728\u771f\u5b9e\u8857\u9053\u56fe\u50cf\u4e0a\u7f16\u8f91\u548c\u91cd\u65b0\u8bbe\u8ba1\u81ea\u884c\u8f66\u8bbe\u65bd\uff0c\u751f\u6210\u73b0\u5b9e\u800c\u4e0a\u4e0b\u6587\u9002\u5b9c\u7684\u8bbe\u8ba1\u3002", "motivation": "\u73b0\u6709\u7684\u8857\u9053\u8bbe\u8ba1\u6e32\u67d3\u65b9\u6cd5\u52b3\u52a8\u5bc6\u96c6\uff0c\u5f71\u54cd\u516c\u4f17\u53c2\u4e0e\u548c\u534f\u4f5c\u51b3\u7b56\uff0c\u800cAI\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u9886\u57df\u7279\u5b9a\u6570\u636e\u4e14\u65e0\u6cd5\u7cbe\u786e\u63a7\u5236\u590d\u6742\u8857\u9053\u573a\u666f\u4e2d\u7684\u7a7a\u95f4\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u591a\u6ee1\u610f\u7cfb\u7edf\uff0c\u96c6\u6210\u4e86\u8f66\u9053\u5b9a\u4f4d\u3001\u63d0\u793a\u4f18\u5316\u3001\u8bbe\u8ba1\u751f\u6210\u548c\u81ea\u52a8\u8bc4\u4f30\u7b49\u6a21\u5757\uff0c\u76f4\u63a5\u5728\u771f\u5b9e\u8857\u9053\u56fe\u50cf\u4e0a\u7f16\u8f91\u548c\u91cd\u65b0\u8bbe\u8ba1\u81ea\u884c\u8f66\u8bbe\u65bd\u3002", "result": "\u5728\u591a\u6837\u5316\u57ce\u5e02\u573a\u666f\u4e2d\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc1\u660e\u7cfb\u7edf\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u8def\u7f51\u51e0\u4f55\u5f62\u72b6\u548c\u73af\u5883\u6761\u4ef6\uff0c\u4e00\u8d28\u6027\u5730\u4ea7\u751f\u89c6\u89c9\u4e00\u81f4\u4e14\u7b26\u5408\u6307\u4ee4\u8981\u6c42\u7684\u7ed3\u679c\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5e94\u7528\u591a\u6ee1\u610f\u7ba1\u9053\u5230\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u548c\u8bbe\u65bd\u8bbe\u8ba1\u9886\u57df\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.05978", "pdf": "https://arxiv.org/pdf/2509.05978", "abs": "https://arxiv.org/abs/2509.05978", "authors": ["Mohamed Mohamed", "Brennan Nichyporuk", "Douglas L. Arnold", "Tal Arbel"], "title": "Imagining Alternatives: Towards High-Resolution 3D Counterfactual Medical Image Generation via Language Guidance", "categories": ["eess.IV", "cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Vision-language models have demonstrated impressive capabilities in generating 2D images under various conditions; however the impressive performance of these models in 2D is largely enabled by extensive, readily available pretrained foundation models. Critically, comparable pretrained foundation models do not exist for 3D, significantly limiting progress in this domain. As a result, the potential of vision-language models to produce high-resolution 3D counterfactual medical images conditioned solely on natural language descriptions remains completely unexplored. Addressing this gap would enable powerful clinical and research applications, such as personalized counterfactual explanations, simulation of disease progression scenarios, and enhanced medical training by visualizing hypothetical medical conditions in realistic detail. Our work takes a meaningful step toward addressing this challenge by introducing a framework capable of generating high-resolution 3D counterfactual medical images of synthesized patients guided by free-form language prompts. We adapt state-of-the-art 3D diffusion models with enhancements from Simple Diffusion and incorporate augmented conditioning to improve text alignment and image quality. To our knowledge, this represents the first demonstration of a language-guided native-3D diffusion model applied specifically to neurological imaging data, where faithful three-dimensional modeling is essential to represent the brain's three-dimensional structure. Through results on two distinct neurological MRI datasets, our framework successfully simulates varying counterfactual lesion loads in Multiple Sclerosis (MS), and cognitive states in Alzheimer's disease, generating high-quality images while preserving subject fidelity in synthetically generated medical images. Our results lay the groundwork for prompt-driven disease progression analysis within 3D medical imaging.", "AI": {"tldr": "\u9996\u4e2a\u57fa\u4e8e\u8bed\u8a00\u63d0\u793a\u751f\u6210\u9ad8\u8d28\u91cf3D\u533b\u5b66\u53cd\u4e8b\u5b9e\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u9488\u5bf9\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u5f00\u53d1\u4e86\u539f\u751f3D\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u6a21\u62df\u591a\u53d1\u6027\u786c\u5316\u75c7\u548c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u4e0d\u540c\u75c5\u7406\u72b6\u6001\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57282D\u56fe\u50cf\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f3D\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff0c\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u751f\u6210\u9ad8\u5206\u8fa8\u73873D\u533b\u5b66\u53cd\u4e8b\u5b9e\u56fe\u50cf\u7684\u80fd\u529b\u5c1a\u672a\u88ab\u63a2\u7d22\uff0c\u8fd9\u9650\u5236\u4e86\u4e34\u5e8a\u548c\u7814\u7a76\u5e94\u7528\u7684\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u6700\u5148\u8fdb\u76843D\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408Simple Diffusion\u7684\u589e\u5f3a\u6280\u672f\uff0c\u5f15\u5165\u589e\u5f3a\u6761\u4ef6\u673a\u5236\u6765\u6539\u5584\u6587\u672c\u5bf9\u9f50\u548c\u56fe\u50cf\u8d28\u91cf\uff0c\u5f00\u53d1\u4e86\u4e13\u95e8\u9488\u5bf9\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u7684\u539f\u751f3D\u6269\u6563\u6a21\u578b\u3002", "result": "\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u795e\u7ecfMRI\u6570\u636e\u96c6\u4e0a\u6210\u529f\u6a21\u62df\u4e86\u591a\u53d1\u6027\u786c\u5316\u75c7\u7684\u4e0d\u540c\u75c5\u7076\u8d1f\u8377\u548c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u8ba4\u77e5\u72b6\u6001\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u5408\u6210\u533b\u5b66\u56fe\u50cf\u4e2d\u53d7\u8bd5\u8005\u7684\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a3D\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u63d0\u793a\u9a71\u52a8\u75be\u75c5\u8fdb\u5c55\u5206\u6790\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u8bed\u8a00\u5f15\u5bfc\u76843D\u533b\u5b66\u56fe\u50cf\u751f\u6210\u7684\u53ef\u884c\u6027\uff0c\u5177\u6709\u91cd\u8981\u7684\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
