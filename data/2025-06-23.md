<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 4]
- [cs.CV](#cs.CV) [Total: 28]
- [cs.RO](#cs.RO) [Total: 1]
- [eess.IV](#eess.IV) [Total: 6]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [FLUX.1 Kontext: Flow Matching for In-Context Image Generation and Editing in Latent Space](https://arxiv.org/abs/2506.15742)
*Black Forest Labs,Stephen Batifol,Andreas Blattmann,Frederic Boesel,Saksham Consul,Cyril Diagne,Tim Dockhorn,Jack English,Zion English,Patrick Esser,Sumith Kulal,Kyle Lacey,Yam Levi,Cheng Li,Dominik Lorenz,Jonas Müller,Dustin Podell,Robin Rombach,Harry Saini,Axel Sauer,Luke Smith*

Main category: cs.GR

TL;DR: FLUX.1 Kontext是一种生成流匹配模型，统一了图像生成和编辑任务，通过结合文本和图像的语义上下文生成新视图。它在单次和多轮任务中均表现出色，速度快且性能优越。


<details>
  <summary>Details</summary>
Motivation: 当前图像编辑模型在多轮任务中存在角色一致性和稳定性下降的问题，FLUX.1 Kontext旨在解决这一问题，同时提升生成速度和任务统一性。

Method: 采用简单的序列拼接方法，统一处理局部编辑和生成任务，结合文本和图像输入的语义上下文。

Result: 模型在单次和多轮任务中均表现出更高的对象和角色保留能力，生成速度快，性能优于现有技术。

Conclusion: FLUX.1 Kontext为统一图像处理模型设定了新标准，适用于交互式应用和快速原型设计。

Abstract: We present evaluation results for FLUX.1 Kontext, a generative flow matching model that unifies image generation and editing. The model generates novel output views by incorporating semantic context from text and image inputs. Using a simple sequence concatenation approach, FLUX.1 Kontext handles both local editing and generative in-context tasks within a single unified architecture. Compared to current editing models that exhibit degradation in character consistency and stability across multiple turns, we observe that FLUX.1 Kontext improved preservation of objects and characters, leading to greater robustness in iterative workflows.The model achieves competitive performance with current state-of-the-art systems while delivering significantly faster generation times, enabling interactive applications and rapid prototyping workflows. To validate these improvements, we introduce KontextBench, a comprehensive benchmark with 1026 image-prompt pairs covering five task categories: local editing, global editing, character reference, style reference and text editing. Detailed evaluations show the superior performance of FLUX.1 Kontext in terms of both single-turn quality and multi-turn consistency, setting new standards for unified image processing models.

</details>


### [2] [VEIGAR: View-consistent Explicit Inpainting and Geometry Alignment for 3D object Removal](https://arxiv.org/abs/2506.15821)
*Pham Khai Nguyen Do,Bao Nguyen Tran,Nam Nguyen,Duc Dung Nguyen*

Main category: cs.GR

TL;DR: VEIGAR是一种高效的新视角合成框架，无需初始3D重建阶段，通过轻量级基础模型和尺度不变深度损失实现高质量重建和跨视角一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖初始3D重建阶段，计算成本高且重建质量不佳，VEIGAR旨在解决这一问题。

Method: VEIGAR使用轻量级基础模型在像素空间显式对齐先验，并引入尺度不变深度损失监督策略。

Result: VEIGAR在重建质量和跨视角一致性上达到新SOTA，训练时间减少三倍。

Conclusion: VEIGAR在效率和效果上优于现有方法，为NVS任务提供了更优解决方案。

Abstract: Recent advances in Novel View Synthesis (NVS) and 3D generation have significantly improved editing tasks, with a primary emphasis on maintaining cross-view consistency throughout the generative process. Contemporary methods typically address this challenge using a dual-strategy framework: performing consistent 2D inpainting across all views guided by embedded priors either explicitly in pixel space or implicitly in latent space; and conducting 3D reconstruction with additional consistency guidance. Previous strategies, in particular, often require an initial 3D reconstruction phase to establish geometric structure, introducing considerable computational overhead. Even with the added cost, the resulting reconstruction quality often remains suboptimal. In this paper, we present VEIGAR, a computationally efficient framework that outperforms existing methods without relying on an initial reconstruction phase. VEIGAR leverages a lightweight foundation model to reliably align priors explicitly in the pixel space. In addition, we introduce a novel supervision strategy based on scale-invariant depth loss, which removes the need for traditional scale-and-shift operations in monocular depth regularization. Through extensive experimentation, VEIGAR establishes a new state-of-the-art benchmark in reconstruction quality and cross-view consistency, while achieving a threefold reduction in training time compared to the fastest existing method, highlighting its superior balance of efficiency and effectiveness.

</details>


### [3] [Beyond Blur: A Fluid Perspective on Generative Diffusion Models](https://arxiv.org/abs/2506.16827)
*Grzegorz Gruszczynski,Michal Jan Wlodarczyk,Jakub J Meixner,Przemyslaw Musialski*

Main category: cs.GR

TL;DR: 本文提出了一种基于PDE的图像生成方法，通过物理驱动的腐蚀过程结合对流-扩散和噪声，实现了高质量的图像生成。


<details>
  <summary>Details</summary>
Motivation: 旨在通过物理启发的PDE方法，结合流体动力学和深度学习，改进现有基于PDE的图像生成技术。

Method: 使用GPU加速的Lattice Boltzmann求解器实现PDE，结合随机速度场模拟湍流，神经网络学习反转腐蚀过程。

Result: 该方法能够生成多样且高质量的图像，同时保持色彩一致性，并推广了现有的PDE腐蚀技术。

Conclusion: 本文为基于扩散的图像合成提供了物理启发的创新视角，结合了流体动力学和深度学习。

Abstract: We propose a novel PDE-driven corruption process for generative image synthesis based on advection-diffusion processes which generalizes existing PDE-based approaches. Our forward pass formulates image corruption via a physically motivated PDE that couples directional advection with isotropic diffusion and Gaussian noise, controlled by dimensionless numbers (Peclet, Fourier). We implement this PDE numerically through a GPU-accelerated custom Lattice Boltzmann solver for fast evaluation. To induce realistic turbulence, we generate stochastic velocity fields that introduce coherent motion and capture multi-scale mixing. In the generative process, a neural network learns to reverse the advection-diffusion operator thus constituting a novel generative model. We discuss how previous methods emerge as specific cases of our operator, demonstrating that our framework generalizes prior PDE-based corruption techniques. We illustrate how advection improves the diversity and quality of the generated images while keeping the overall color palette unaffected. This work bridges fluid dynamics, dimensionless PDE theory, and deep generative modeling, offering a fresh perspective on physically informed image corruption processes for diffusion-based synthesis.

</details>


### [4] [DreamCube: 3D Panorama Generation via Multi-plane Synchronization](https://arxiv.org/abs/2506.17206)
*Yukun Huang,Yanning Zhou,Jianan Wang,Kaiyi Huang,Xihui Liu*

Main category: cs.GR

TL;DR: 论文提出DreamCube，通过多平面同步技术将2D基础模型扩展到全景领域，实现高质量3D全景生成。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法因3D全景数据稀缺而依赖2D基础模型，但2D单视图与3D全景不兼容的问题。

Method: 采用多平面同步技术，设计DreamCube（多平面RGB-D扩散模型），最大化利用2D基础模型先验。

Result: 实验证明方法在全景图像生成、深度估计和3D场景生成中有效。

Conclusion: DreamCube通过多平面同步技术成功扩展2D基础模型能力，实现多样化外观和精确几何的3D全景生成。

Abstract: 3D panorama synthesis is a promising yet challenging task that demands high-quality and diverse visual appearance and geometry of the generated omnidirectional content. Existing methods leverage rich image priors from pre-trained 2D foundation models to circumvent the scarcity of 3D panoramic data, but the incompatibility between 3D panoramas and 2D single views limits their effectiveness. In this work, we demonstrate that by applying multi-plane synchronization to the operators from 2D foundation models, their capabilities can be seamlessly extended to the omnidirectional domain. Based on this design, we further introduce DreamCube, a multi-plane RGB-D diffusion model for 3D panorama generation, which maximizes the reuse of 2D foundation model priors to achieve diverse appearances and accurate geometry while maintaining multi-view consistency. Extensive experiments demonstrate the effectiveness of our approach in panoramic image generation, panoramic depth estimation, and 3D scene generation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [5] [ADAM-Dehaze: Adaptive Density-Aware Multi-Stage Dehazing for Improved Object Detection in Foggy Conditions](https://arxiv.org/abs/2506.15837)
*Fatmah AlHindaassi,Mohammed Talha Alam,Fakhri Karray*

Main category: cs.CV

TL;DR: ADAM-Dehaze是一个自适应、密度感知的去雾框架，通过动态路由和自适应损失优化图像恢复和目标检测，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 雾天气严重影响视觉信息，对自动驾驶和监控系统等安全关键应用构成挑战。

Method: 使用HDEN网络分类雾密度，动态路由到三个CORUN分支，结合自适应损失平衡物理模型和感知保真度。

Result: 在Cityscapes和RTTS基准上，PSNR提升2.1 dB，FADE减少30%，目标检测mAP提高13点，推理时间减少20%。

Conclusion: ADAM-Dehaze展示了密度特定处理和与下游视觉任务无缝集成的重要性。

Abstract: Adverse weather conditions, particularly fog, pose a significant challenge to autonomous vehicles, surveillance systems, and other safety-critical applications by severely degrading visual information. We introduce ADAM-Dehaze, an adaptive, density-aware dehazing framework that jointly optimizes image restoration and object detection under varying fog intensities. A lightweight Haze Density Estimation Network (HDEN) classifies each input as light, medium, or heavy fog. Based on this score, the system dynamically routes the image through one of three CORUN branches: Light, Medium, or Complex, each tailored to its haze regime. A novel adaptive loss balances physical-model coherence and perceptual fidelity, ensuring both accurate defogging and preservation of fine details. On Cityscapes and the real-world RTTS benchmark, ADAM-Dehaze improves PSNR by up to 2.1 dB, reduces FADE by 30 percent, and increases object detection mAP by up to 13 points, while cutting inference time by 20 percent. These results highlight the importance of intensity-specific processing and seamless integration with downstream vision tasks. Code available at: https://github.com/talha-alam/ADAM-Dehaze.

</details>


### [6] [EchoShot: Multi-Shot Portrait Video Generation](https://arxiv.org/abs/2506.15838)
*Jiahao Wang,Hualian Sheng,Sijia Cai,Weizhan Zhang,Caixia Yan,Yachuang Feng,Bing Deng,Jieping Ye*

Main category: cs.CV

TL;DR: EchoShot是一个基于视频扩散模型的多镜头肖像定制框架，通过位置嵌入机制和高质量数据集PortraitGala，实现了身份一致性和内容可控性。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型仅限于单镜头生成，而实际应用需要多镜头且身份一致的视频生成。

Method: 提出EchoShot框架，采用镜头感知位置嵌入机制，结合PortraitGala数据集进行训练，支持基于参考图像的个性化生成和长视频合成。

Result: EchoShot在多镜头肖像视频生成中表现出卓越的身份一致性和属性级可控性。

Conclusion: EchoShot为通用多镜头视频建模提供了潜在的基础范式。

Abstract: Video diffusion models substantially boost the productivity of artistic workflows with high-quality portrait video generative capacity. However, prevailing pipelines are primarily constrained to single-shot creation, while real-world applications urge for multiple shots with identity consistency and flexible content controllability. In this work, we propose EchoShot, a native and scalable multi-shot framework for portrait customization built upon a foundation video diffusion model. To start with, we propose shot-aware position embedding mechanisms within video diffusion transformer architecture to model inter-shot variations and establish intricate correspondence between multi-shot visual content and their textual descriptions. This simple yet effective design enables direct training on multi-shot video data without introducing additional computational overhead. To facilitate model training within multi-shot scenario, we construct PortraitGala, a large-scale and high-fidelity human-centric video dataset featuring cross-shot identity consistency and fine-grained captions such as facial attributes, outfits, and dynamic motions. To further enhance applicability, we extend EchoShot to perform reference image-based personalized multi-shot generation and long video synthesis with infinite shot counts. Extensive evaluations demonstrate that EchoShot achieves superior identity consistency as well as attribute-level controllability in multi-shot portrait video generation. Notably, the proposed framework demonstrates potential as a foundational paradigm for general multi-shot video modeling.

</details>


### [7] [MoiréXNet: Adaptive Multi-Scale Demoiréing with Linear Attention Test-Time Training and Truncated Flow Matching Prior](https://arxiv.org/abs/2506.15929)
*Liangyan Li,Yimo Ning,Kevin Le,Wei Dong,Yunzhe Li,Jun Chen,Xiaohong Liu*

Main category: cs.CV

TL;DR: 提出了一种结合MAP估计与深度学习的图像和视频去摩尔纹框架，解决了现有方法在非线性退化过程中的不足。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习方法无法完全去除摩尔纹或导致过度平滑，生成模型在非线性退化中表现不佳且易引入伪影。

Method: 提出混合MAP框架，结合监督学习模型（带线性注意力TTT模块）和TFMP先验，高效学习非线性映射并细化输出。

Result: 框架结合了线性注意力的计算效率和生成模型的细化能力，显著提升了恢复性能。

Conclusion: 该框架有效解决了去摩尔纹问题，恢复了高频细节并抑制了伪影。

Abstract: This paper introduces a novel framework for image and video demoir\'eing by integrating Maximum A Posteriori (MAP) estimation with advanced deep learning techniques. Demoir\'eing addresses inherently nonlinear degradation processes, which pose significant challenges for existing methods.   Traditional supervised learning approaches either fail to remove moir\'e patterns completely or produce overly smooth results. This stems from constrained model capacity and scarce training data, which inadequately represent the clean image distribution and hinder accurate reconstruction of ground-truth images. While generative models excel in image restoration for linear degradations, they struggle with nonlinear cases such as demoir\'eing and often introduce artifacts.   To address these limitations, we propose a hybrid MAP-based framework that integrates two complementary components. The first is a supervised learning model enhanced with efficient linear attention Test-Time Training (TTT) modules, which directly learn nonlinear mappings for RAW-to-sRGB demoir\'eing. The second is a Truncated Flow Matching Prior (TFMP) that further refines the outputs by aligning them with the clean image distribution, effectively restoring high-frequency details and suppressing artifacts. These two components combine the computational efficiency of linear attention with the refinement abilities of generative models, resulting in improved restoration performance.

</details>


### [8] [Advanced Sign Language Video Generation with Compressed and Quantized Multi-Condition Tokenization](https://arxiv.org/abs/2506.15980)
*Cong Wang,Zexuan Deng,Zhiwei Jiang,Fei Shen,Yafeng Yin,Shiwei Gan,Zifeng Cheng,Shiping Ge,Qing Gu*

Main category: cs.CV

TL;DR: SignViP是一个新的手语视频生成框架，通过多细粒度条件提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单一粗糙条件（如骨架序列），限制了生成视频的自然性和表现力。

Method: SignViP包含三个核心组件：Sign Video Diffusion Model、FSQ Autoencoder和Multi-Condition Token Translator，通过离散化范式整合细粒度条件。

Result: 实验表明SignViP在视频质量、时间一致性和语义保真度上达到最优性能。

Conclusion: SignViP通过多细粒度条件显著提升了手语视频生成的质量和表现力。

Abstract: Sign Language Video Generation (SLVG) seeks to generate identity-preserving sign language videos from spoken language texts. Existing methods primarily rely on the single coarse condition (\eg, skeleton sequences) as the intermediary to bridge the translation model and the video generation model, which limits both the naturalness and expressiveness of the generated videos. To overcome these limitations, we propose SignViP, a novel SLVG framework that incorporates multiple fine-grained conditions for improved generation fidelity. Rather than directly translating error-prone high-dimensional conditions, SignViP adopts a discrete tokenization paradigm to integrate and represent fine-grained conditions (\ie, fine-grained poses and 3D hands). SignViP contains three core components. (1) Sign Video Diffusion Model is jointly trained with a multi-condition encoder to learn continuous embeddings that encapsulate fine-grained motion and appearance. (2) Finite Scalar Quantization (FSQ) Autoencoder is further trained to compress and quantize these embeddings into discrete tokens for compact representation of the conditions. (3) Multi-Condition Token Translator is trained to translate spoken language text to discrete multi-condition tokens. During inference, Multi-Condition Token Translator first translates the spoken language text into discrete multi-condition tokens. These tokens are then decoded to continuous embeddings by FSQ Autoencoder, which are subsequently injected into Sign Video Diffusion Model to guide video generation. Experimental results show that SignViP achieves state-of-the-art performance across metrics, including video quality, temporal coherence, and semantic fidelity. The code is available at https://github.com/umnooob/signvip/.

</details>


### [9] [PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models](https://arxiv.org/abs/2506.16054)
*Tianchen Zhao,Ke Hong,Xinhao Yang,Xuefeng Xiao,Huixia Li,Feng Ling,Ruiqi Xie,Siqi Chen,Hongyu Zhu,Yichong Zhang,Yu Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为PARO的技术，通过重新组织注意力模式来降低视觉生成中的计算和内存成本，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 视觉生成中注意力机制的二次复杂度导致高计算和内存成本，尤其是在高分辨率图像或多帧视频生成中。现有稀疏化和量化技术面临低密度和低比特宽度的挑战。

Method: 设计了一种名为PARO的技术，通过重新组织注意力模式为硬件友好的块状模式，简化并增强稀疏化和量化。

Result: PAROAttention在视频和图像生成中实现了无损指标，性能接近全精度基线，同时显著降低密度（20%-30%）和比特宽度（INT8/INT4），端到端延迟加速1.9x至2.7x。

Conclusion: PARO技术通过重新组织注意力模式，有效解决了视觉生成中的计算和内存问题，同时保持高性能。

Abstract: In visual generation, the quadratic complexity of attention mechanisms results in high memory and computational costs, especially for longer token sequences required in high-resolution image or multi-frame video generation. To address this, prior research has explored techniques such as sparsification and quantization. However, these techniques face significant challenges under low density and reduced bitwidths. Through systematic analysis, we identify that the core difficulty stems from the dispersed and irregular characteristics of visual attention patterns. Therefore, instead of introducing specialized sparsification and quantization design to accommodate such patterns, we propose an alternative strategy: *reorganizing* the attention pattern to alleviate the challenges. Inspired by the local aggregation nature of visual feature extraction, we design a novel **Pattern-Aware token ReOrdering (PARO)** technique, which unifies the diverse attention patterns into a hardware-friendly block-wise pattern. This unification substantially simplifies and enhances both sparsification and quantization. We evaluate the performance-efficiency trade-offs of various design choices and finalize a methodology tailored for the unified pattern. Our approach, **PAROAttention**, achieves video and image generation with lossless metrics, and nearly identical results from full-precision (FP) baselines, while operating at notably lower density (~20%-30%) and bitwidth (**INT8/INT4**), achieving a **1.9x** to **2.7x** end-to-end latency speedup.

</details>


### [10] [STAR-Pose: Efficient Low-Resolution Video Human Pose Estimation via Spatial-Temporal Adaptive Super-Resolution](https://arxiv.org/abs/2506.16061)
*Yucheng Jin,Jinyan Chen,Ziyue He,Baojun Han,Furan An*

Main category: cs.CV

TL;DR: STAR-Pose是一种针对低分辨率视频中人体姿态估计的空间-时间自适应超分辨率框架，通过改进的Transformer和自适应融合模块，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 低分辨率视频中的人体姿态估计存在挑战，传统方法依赖高质量输入或计算密集型处理，难以在资源受限环境中部署。

Method: 提出STAR-Pose框架，结合空间-时间Transformer（带LeakyReLU的线性注意力）和自适应融合模块，设计姿态感知复合损失函数。

Result: 在多个主流视频HPE数据集上表现优异，64x48分辨率下mAP提升5.2%，推理速度比级联方法快2.8x至4.4x。

Conclusion: STAR-Pose在低分辨率视频中高效且准确地实现了人体姿态估计，为资源受限场景提供了实用解决方案。

Abstract: Human pose estimation in low-resolution videos presents a fundamental challenge in computer vision. Conventional methods either assume high-quality inputs or employ computationally expensive cascaded processing, which limits their deployment in resource-constrained environments. We propose STAR-Pose, a spatial-temporal adaptive super-resolution framework specifically designed for video-based human pose estimation. Our method features a novel spatial-temporal Transformer with LeakyReLU-modified linear attention, which efficiently captures long-range temporal dependencies. Moreover, it is complemented by an adaptive fusion module that integrates parallel CNN branch for local texture enhancement. We also design a pose-aware compound loss to achieve task-oriented super-resolution. This loss guides the network to reconstruct structural features that are most beneficial for keypoint localization, rather than optimizing purely for visual quality. Extensive experiments on several mainstream video HPE datasets demonstrate that STAR-Pose outperforms existing approaches. It achieves up to 5.2% mAP improvement under extremely low-resolution (64x48) conditions while delivering 2.8x to 4.4x faster inference than cascaded approaches.

</details>


### [11] [FastInit: Fast Noise Initialization for Temporally Consistent Video Generation](https://arxiv.org/abs/2506.16119)
*Chengyu Bai,Yuming Li,Zhongyu Zhao,Jintao Chen,Peidong Jia,Qi She,Ming Lu,Shanghang Zhang*

Main category: cs.CV

TL;DR: FastInit提出了一种快速噪声初始化方法，通过单次前向传播生成高质量视频噪声，显著提升视频生成效率和时序一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如FreeInit）通过迭代优化噪声提高时序一致性，但计算成本高，FastInit旨在解决这一问题。

Method: FastInit训练了一个视频噪声预测网络（VNPNet），输入随机噪声和文本提示，单次生成优化后的噪声。

Result: 实验表明，FastInit显著提升了视频生成的质量和时序一致性，同时大幅降低了计算成本。

Conclusion: FastInit为视频生成提供了一种高效且实用的解决方案，可直接应用于推理阶段。

Abstract: Video generation has made significant strides with the development of diffusion models; however, achieving high temporal consistency remains a challenging task. Recently, FreeInit identified a training-inference gap and introduced a method to iteratively refine the initial noise during inference. However, iterative refinement significantly increases the computational cost associated with video generation. In this paper, we introduce FastInit, a fast noise initialization method that eliminates the need for iterative refinement. FastInit learns a Video Noise Prediction Network (VNPNet) that takes random noise and a text prompt as input, generating refined noise in a single forward pass. Therefore, FastInit greatly enhances the efficiency of video generation while achieving high temporal consistency across frames. To train the VNPNet, we create a large-scale dataset consisting of pairs of text prompts, random noise, and refined noise. Extensive experiments with various text-to-video models show that our method consistently improves the quality and temporal consistency of the generated videos. FastInit not only provides a substantial improvement in video generation but also offers a practical solution that can be applied directly during inference. The code and dataset will be released.

</details>


### [12] [Integrating Generative Adversarial Networks and Convolutional Neural Networks for Enhanced Traffic Accidents Detection and Analysis](https://arxiv.org/abs/2506.16186)
*Zhenghao Xi,Xiang Liu,Yaqi Liu,Yitong Cai,Yangyu Zheng*

Main category: cs.CV

TL;DR: 论文提出了一种基于深度学习的交通事故检测框架，结合GANs生成数据和CNNs训练模型，实现了高精度的实时事故检测。


<details>
  <summary>Details</summary>
Motivation: 全球交通事故数量上升，需要智能、高效的自动化事故检测系统以提高交通安全。

Method: 使用GANs合成数据，CNNs训练模型，结合FTCNN和VIT模型，从YouTube视频中收集事故和非事故帧并进行预处理。

Result: FTCNN和VIT模型分别达到94%和95%的准确率，CNN模型为88%，验证了框架的高效性。

Conclusion: 该框架适用于实时交通监控和智能城市系统，为未来智能监控系统奠定了基础。

Abstract: Accident detection using Closed Circuit Television (CCTV) footage is one of the most imperative features for enhancing transport safety and efficient traffic control. To this end, this research addresses the issues of supervised monitoring and data deficiency in accident detection systems by adapting excellent deep learning technologies. The motivation arises from rising statistics in the number of car accidents worldwide; this calls for innovation and the establishment of a smart, efficient and automated way of identifying accidents and calling for help to save lives. Addressing the problem of the scarcity of data, the presented framework joins Generative Adversarial Networks (GANs) for synthesizing data and Convolutional Neural Networks (CNN) for model training. Video frames for accidents and non-accidents are collected from YouTube videos, and we perform resizing, image enhancement and image normalisation pixel range adjustments. Three models are used: CNN, Fine-tuned Convolutional Neural Network (FTCNN) and Vision Transformer (VIT) worked best for detecting accidents from CCTV, obtaining an accuracy rate of 94% and 95%, while the CNN model obtained 88%. Such results show that the proposed framework suits traffic safety applications due to its high real-time accident detection capabilities and broad-scale applicability. This work lays the foundation for intelligent surveillance systems in the future for real-time traffic monitoring, smart city framework, and integration of intelligent surveillance systems into emergency management systems.

</details>


### [13] [VideoGAN-based Trajectory Proposal for Automated Vehicles](https://arxiv.org/abs/2506.16209)
*Annajoyce Mariani,Kira Maag,Hanno Gottschalk*

Main category: cs.CV

TL;DR: 使用GAN生成鸟瞰视角交通场景视频，从中提取轨迹数据，验证其统计准确性和物理真实性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉未来轨迹的复杂多模态分布，因此探索GAN是否能生成准确且空间关系正确的轨迹。

Method: 提出一种管道，利用低分辨率鸟瞰占用网格视频训练视频生成模型，并通过单帧目标检测和帧间目标匹配提取轨迹数据。

Result: 在100 GPU小时内完成训练，推理时间低于20毫秒，生成的轨迹在空间和动态参数上与真实数据分布一致。

Conclusion: GAN能高效生成物理真实的轨迹，适用于自动驾驶场景。

Abstract: Being able to generate realistic trajectory options is at the core of increasing the degree of automation of road vehicles. While model-driven, rule-based, and classical learning-based methods are widely used to tackle these tasks at present, they can struggle to effectively capture the complex, multimodal distributions of future trajectories. In this paper we investigate whether a generative adversarial network (GAN) trained on videos of bird's-eye view (BEV) traffic scenarios can generate statistically accurate trajectories that correctly capture spatial relationships between the agents. To this end, we propose a pipeline that uses low-resolution BEV occupancy grid videos as training data for a video generative model. From the generated videos of traffic scenarios we extract abstract trajectory data using single-frame object detection and frame-to-frame object matching. We particularly choose a GAN architecture for the fast training and inference times with respect to diffusion models. We obtain our best results within 100 GPU hours of training, with inference times under 20\,ms. We demonstrate the physical realism of the proposed trajectories in terms of distribution alignment of spatial and dynamic parameters with respect to the ground truth videos from the Waymo Open Motion Dataset.

</details>


### [14] [R3eVision: A Survey on Robust Rendering, Restoration, and Enhancement for 3D Low-Level Vision](https://arxiv.org/abs/2506.16262)
*Weeyoung Kwon,Jeahun Sung,Minkyu Jeon,Chanho Eom,Jihyong Oh*

Main category: cs.CV

TL;DR: 该论文综述了3D低层视觉（3D LLV）领域，探讨了如何在退化条件下实现高保真的3D重建和视图合成。


<details>
  <summary>Details</summary>
Motivation: 现有神经渲染方法（如NeRF和3DGS）通常假设输入为高质量多视图图像，但在实际场景中常面临噪声、模糊、低分辨率等问题，限制了其鲁棒性。

Method: 论文通过形式化退化感知渲染问题，分类总结了将低层视觉任务（如超分辨率、去模糊等）融入神经渲染框架的现有方法。

Result: 综述展示了这些方法如何提升在退化条件下的3D重建质量，并讨论了在自动驾驶、AR/VR等领域的应用。

Conclusion: 3D LLV被视为实现真实环境中鲁棒3D内容生成和场景重建的重要方向。

Abstract: Neural rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have achieved significant progress in photorealistic 3D scene reconstruction and novel view synthesis. However, most existing models assume clean and high-resolution (HR) multi-view inputs, which limits their robustness under real-world degradations such as noise, blur, low-resolution (LR), and weather-induced artifacts. To address these limitations, the emerging field of 3D Low-Level Vision (3D LLV) extends classical 2D Low-Level Vision tasks including super-resolution (SR), deblurring, weather degradation removal, restoration, and enhancement into the 3D spatial domain. This survey, referred to as R\textsuperscript{3}eVision, provides a comprehensive overview of robust rendering, restoration, and enhancement for 3D LLV by formalizing the degradation-aware rendering problem and identifying key challenges related to spatio-temporal consistency and ill-posed optimization. Recent methods that integrate LLV into neural rendering frameworks are categorized to illustrate how they enable high-fidelity 3D reconstruction under adverse conditions. Application domains such as autonomous driving, AR/VR, and robotics are also discussed, where reliable 3D perception from degraded inputs is critical. By reviewing representative methods, datasets, and evaluation protocols, this work positions 3D LLV as a fundamental direction for robust 3D content generation and scene-level reconstruction in real-world environments.

</details>


### [15] [Learning Multi-scale Spatial-frequency Features for Image Denoising](https://arxiv.org/abs/2506.16307)
*Xu Zhao,Chen Zhao,Xiantao Hu,Hongliang Zhang,Ying Tai,Jian Yang*

Main category: cs.CV

TL;DR: 提出了一种多尺度自适应双域网络（MADNet），用于图像去噪，通过图像金字塔输入和自适应空间频率学习单元（ASFU）提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖固定单输入单输出的Unet架构，忽略了像素级多尺度表示，且未区分高频和低频噪声特性。

Method: 使用图像金字塔输入，设计ASFU单元分离高低频信息，并在跳跃连接中引入全局特征融合块。

Result: 在合成和真实噪声图像数据集上的实验验证了MADNet优于当前最先进的去噪方法。

Conclusion: MADNet通过多尺度自适应双域设计，显著提升了图像去噪性能。

Abstract: Recent advancements in multi-scale architectures have demonstrated exceptional performance in image denoising tasks. However, existing architectures mainly depends on a fixed single-input single-output Unet architecture, ignoring the multi-scale representations of pixel level. In addition, previous methods treat the frequency domain uniformly, ignoring the different characteristics of high-frequency and low-frequency noise. In this paper, we propose a novel multi-scale adaptive dual-domain network (MADNet) for image denoising. We use image pyramid inputs to restore noise-free results from low-resolution images. In order to realize the interaction of high-frequency and low-frequency information, we design an adaptive spatial-frequency learning unit (ASFU), where a learnable mask is used to separate the information into high-frequency and low-frequency components. In the skip connections, we design a global feature fusion block to enhance the features at different scales. Extensive experiments on both synthetic and real noisy image datasets verify the effectiveness of MADNet compared with current state-of-the-art denoising approaches.

</details>


### [16] [Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate Details](https://arxiv.org/abs/2506.16504)
*Zeqiang Lai,Yunfei Zhao,Haolin Liu,Zibo Zhao,Qingxiang Lin,Huiwen Shi,Xianghui Yang,Mingxin Yang,Shuhui Yang,Yifei Feng,Sheng Zhang,Xin Huang,Di Luo,Fan Yang,Fang Yang,Lifu Wang,Sicong Liu,Yixuan Tang,Yulin Cai,Zebin He,Tian Liu,Yuhong Liu,Jie Jiang,Linus,Jingwei Huang,Chunchao Guo*

Main category: cs.CV

TL;DR: Hunyuan3D 2.5是一个强大的3D扩散模型套件，用于生成高保真和细节丰富的3D资产。它在形状和纹理生成方面均有显著改进。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D生成模型在形状和纹理细节上的不足，缩小生成与手工制作3D形状之间的差距。

Method: 采用两阶段流程，引入新的形状基础模型LATTICE（10B参数），并升级纹理生成模型，支持基于物理的渲染（PBR）。

Result: 在形状和端到端纹理生成方面显著优于先前方法。

Conclusion: Hunyuan3D 2.5在3D资产生成领域取得了重要进展，提升了生成质量。

Abstract: In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion models aimed at generating high-fidelity and detailed textured 3D assets. Hunyuan3D 2.5 follows two-stages pipeline of its previous version Hunyuan3D 2.0, while demonstrating substantial advancements in both shape and texture generation. In terms of shape generation, we introduce a new shape foundation model -- LATTICE, which is trained with scaled high-quality datasets, model-size, and compute. Our largest model reaches 10B parameters and generates sharp and detailed 3D shape with precise image-3D following while keeping mesh surface clean and smooth, significantly closing the gap between generated and handcrafted 3D shapes. In terms of texture generation, it is upgraded with phyiscal-based rendering (PBR) via a novel multi-view architecture extended from Hunyuan3D 2.0 Paint model. Our extensive evaluation shows that Hunyuan3D 2.5 significantly outperforms previous methods in both shape and end-to-end texture generation.

</details>


### [17] [How to Train your Text-to-Image Model: Evaluating Design Choices for Synthetic Training Captions](https://arxiv.org/abs/2506.16679)
*Manuel Brack,Sudeep Katakol,Felix Friedrich,Patrick Schramowski,Hareesh Ravi,Kristian Kersting,Ajinkya Kale*

Main category: cs.CV

TL;DR: 本文研究了合成字幕策略对文本到图像模型性能的影响，发现高质量字幕能提升文本对齐但可能牺牲美学和多样性，而随机长度字幕则能平衡美学和对齐。


<details>
  <summary>Details</summary>
Motivation: 由于网络抓取数据集的噪声和不一致性，合成字幕成为训练文本到图像模型的新趋势，但其设计选择缺乏研究。

Method: 通过系统实验，研究不同合成字幕策略对模型性能的影响。

Result: 高质量字幕提升文本对齐但可能降低美学和多样性；随机长度字幕平衡美学和对齐；不同字幕分布会显著改变模型输出偏差。

Conclusion: 字幕设计对模型性能至关重要，研究结果为文本到图像生成提供了更有效的训练数据策略。

Abstract: Training data is at the core of any successful text-to-image models. The quality and descriptiveness of image text are crucial to a model's performance. Given the noisiness and inconsistency in web-scraped datasets, recent works shifted towards synthetic training captions. While this setup is generally believed to produce more capable models, current literature does not provide any insights into its design choices. This study closes this gap by systematically investigating how different synthetic captioning strategies impact the downstream performance of text-to-image models. Our experiments demonstrate that dense, high-quality captions enhance text alignment but may introduce trade-offs in output aesthetics and diversity. Conversely, captions of randomized lengths yield balanced improvements across aesthetics and alignment without compromising sample diversity. We also demonstrate that varying caption distributions introduce significant shifts in the output bias of a trained model. Our findings underscore the importance of caption design in achieving optimal model performance and provide practical insights for more effective training data strategies in text-to-image generation.

</details>


### [18] [TeSG: Textual Semantic Guidance for Infrared and Visible Image Fusion](https://arxiv.org/abs/2506.16730)
*Mingrui Zhu,Xiru Chen,Xin Wei,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: 提出了一种基于文本语义指导的红外与可见光图像融合方法TeSG，通过掩码和文本语义信息优化融合过程，提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导的红外与可见光图像融合方法对文本语义信息的利用不足，影响了融合效果和下游任务表现。

Method: TeSG包含语义信息生成器（SIG）、掩码引导交叉注意力模块（MGCA）和文本驱动注意力融合模块（TDAF），分别生成语义信息、初步融合视觉特征和优化融合过程。

Result: 实验表明TeSG在下游任务中表现优于现有方法。

Conclusion: TeSG通过多层次文本语义指导，有效提升了图像融合的质量和实用性。

Abstract: Infrared and visible image fusion (IVF) aims to combine complementary information from both image modalities, producing more informative and comprehensive outputs. Recently, text-guided IVF has shown great potential due to its flexibility and versatility. However, the effective integration and utilization of textual semantic information remains insufficiently studied. To tackle these challenges, we introduce textual semantics at two levels: the mask semantic level and the text semantic level, both derived from textual descriptions extracted by large Vision-Language Models (VLMs). Building on this, we propose Textual Semantic Guidance for infrared and visible image fusion, termed TeSG, which guides the image synthesis process in a way that is optimized for downstream tasks such as detection and segmentation. Specifically, TeSG consists of three core components: a Semantic Information Generator (SIG), a Mask-Guided Cross-Attention (MGCA) module, and a Text-Driven Attentional Fusion (TDAF) module. The SIG generates mask and text semantics based on textual descriptions. The MGCA module performs initial attention-based fusion of visual features from both infrared and visible images, guided by mask semantics. Finally, the TDAF module refines the fusion process with gated attention driven by text semantics. Extensive experiments demonstrate the competitiveness of our approach, particularly in terms of performance on downstream tasks, compared to existing state-of-the-art methods.

</details>


### [19] [3DeepRep: 3D Deep Low-rank Tensor Representation for Hyperspectral Image Inpainting](https://arxiv.org/abs/2506.16735)
*Yunshan Li,Wenwu Gong,Qianqian Wang,Chao Wang,Lili Yang*

Main category: cs.CV

TL;DR: 提出了一种新型3方向深度低秩张量表示模型（3DeepRep），通过多方向非线性变换提升HSI修复性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注光谱模式的低秩特性，忽略了其他张量模式的低秩结构。

Method: 3DeepRep模型在HSI张量的三个方向上执行深度非线性变换，并通过核范数最小化实现低秩表示，最终通过可学习聚合模块融合结果。

Result: 在真实HSI数据集上的实验表明，该方法在定性和定量上均优于现有技术。

Conclusion: 3DeepRep通过多方向低秩表示显著提升了HSI修复性能。

Abstract: Recent approaches based on transform-based tensor nuclear norm (TNN) have demonstrated notable effectiveness in hyperspectral image (HSI) inpainting by leveraging low-rank structures in latent representations. Recent developments incorporate deep transforms to improve low-rank tensor representation; however, existing approaches typically restrict the transform to the spectral mode, neglecting low-rank properties along other tensor modes. In this paper, we propose a novel 3-directional deep low-rank tensor representation (3DeepRep) model, which performs deep nonlinear transforms along all three modes of the HSI tensor. To enforce low-rankness, the model minimizes the nuclear norms of mode-i frontal slices in the corresponding latent space for each direction (i=1,2,3), forming a 3-directional TNN regularization. The outputs from the three directional branches are subsequently fused via a learnable aggregation module to produce the final result. An efficient gradient-based optimization algorithm is developed to solve the model in a self-supervised manner. Extensive experiments on real-world HSI datasets demonstrate that the proposed method achieves superior inpainting performance compared to existing state-of-the-art techniques, both qualitatively and quantitatively.

</details>


### [20] [Noise-Informed Diffusion-Generated Image Detection with Anomaly Attention](https://arxiv.org/abs/2506.16743)
*Weinan Guan,Wei Wang,Bo Peng,Ziwen He,Jing Dong,Haonan Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种基于噪声感知自注意力模块（NASA）的新方法，用于检测扩散模型生成的图像，解决了训练中未见扩散模型的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型生成图像质量的提升，信息安全隐患增加，需要一种能泛化到未见扩散模型的检测方法。

Method: 通过分析图像噪声模式，提出NASA模块，结合Swin Transformer构建NASA-Swin架构，并采用跨模态融合嵌入和通道掩码策略。

Result: 实验表明，该方法在检测扩散生成图像方面表现优异，对未见生成方法具有SOTA性能。

Conclusion: NASA-Swin架构通过噪声感知和跨模态融合，显著提升了扩散生成图像的检测能力。

Abstract: With the rapid development of image generation technologies, especially the advancement of Diffusion Models, the quality of synthesized images has significantly improved, raising concerns among researchers about information security. To mitigate the malicious abuse of diffusion models, diffusion-generated image detection has proven to be an effective countermeasure.However, a key challenge for forgery detection is generalising to diffusion models not seen during training. In this paper, we address this problem by focusing on image noise. We observe that images from different diffusion models share similar noise patterns, distinct from genuine images. Building upon this insight, we introduce a novel Noise-Aware Self-Attention (NASA) module that focuses on noise regions to capture anomalous patterns. To implement a SOTA detection model, we incorporate NASA into Swin Transformer, forming an novel detection architecture NASA-Swin. Additionally, we employ a cross-modality fusion embedding to combine RGB and noise images, along with a channel mask strategy to enhance feature learning from both modalities. Extensive experiments demonstrate the effectiveness of our approach in enhancing detection capabilities for diffusion-generated images. When encountering unseen generation methods, our approach achieves the state-of-the-art performance.Our code is available at https://github.com/WeinanGuan/NASA-Swin.

</details>


### [21] [Infrared and Visible Image Fusion Based on Implicit Neural Representations](https://arxiv.org/abs/2506.16773)
*Shuchen Sun,Ligen Shi,Chang Liu,Lina Wu,Jun Qiu*

Main category: cs.CV

TL;DR: 本文提出了一种基于隐式神经表示（INR）的图像融合方法INRFuse，通过神经网络参数化连续函数，突破传统依赖离散像素或显式特征的局限，实现红外与可见光图像的高质量融合。


<details>
  <summary>Details</summary>
Motivation: 结合红外与可见光图像的优势，生成信息丰富且满足视觉或计算需求的图像。

Method: 利用归一化空间坐标作为输入，通过多层感知器自适应融合两种模态的特征，设计多损失函数联合优化相似性。

Result: INRFuse在主观视觉质量和客观评价指标上优于现有方法，融合图像结构清晰、细节自然、信息丰富。

Conclusion: INRFuse无需训练数据集，支持不同分辨率图像直接融合，并能通过高密度坐标查询实现超分辨率重建。

Abstract: Infrared and visible light image fusion aims to combine the strengths of both modalities to generate images that are rich in information and fulfill visual or computational requirements. This paper proposes an image fusion method based on Implicit Neural Representations (INR), referred to as INRFuse. This method parameterizes a continuous function through a neural network to implicitly represent the multimodal information of the image, breaking through the traditional reliance on discrete pixels or explicit features. The normalized spatial coordinates of the infrared and visible light images serve as inputs, and multi-layer perceptrons is utilized to adaptively fuse the features of both modalities, resulting in the output of the fused image. By designing multiple loss functions, the method jointly optimizes the similarity between the fused image and the original images, effectively preserving the thermal radiation information of the infrared image while maintaining the texture details of the visible light image. Furthermore, the resolution-independent characteristic of INR allows for the direct fusion of images with varying resolutions and achieves super-resolution reconstruction through high-density coordinate queries. Experimental results indicate that INRFuse outperforms existing methods in both subjective visual quality and objective evaluation metrics, producing fused images with clear structures, natural details, and rich information without the necessity for a training dataset.

</details>


### [22] [PQCAD-DM: Progressive Quantization and Calibration-Assisted Distillation for Extremely Efficient Diffusion Model](https://arxiv.org/abs/2506.16776)
*Beomseok Ko,Hyeryung Jang*

Main category: cs.CV

TL;DR: PQCAD-DM是一种结合渐进量化（PQ）和校准辅助蒸馏（CAD）的混合压缩框架，旨在解决扩散模型的计算和资源密集型问题，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中表现优异，但其依赖迭代马尔可夫链过程导致计算和资源密集，且误差累积限制了简单压缩技术的有效性。

Method: PQ采用两阶段量化，通过动量机制自适应调整位宽，减少低精度下的权重扰动；CAD利用全精度校准数据集进行蒸馏，使量化学生模型能匹配全精度性能。

Result: PQCAD-DM在计算效率和生成质量之间取得平衡，推理时间减半，同时保持竞争力。实验表明其在多样数据集上优于固定位量化方法。

Conclusion: PQCAD-DM通过PQ和CAD的结合，显著提升了扩散模型的效率和性能，为压缩技术提供了新方向。

Abstract: Diffusion models excel in image generation but are computational and resource-intensive due to their reliance on iterative Markov chain processes, leading to error accumulation and limiting the effectiveness of naive compression techniques. In this paper, we propose PQCAD-DM, a novel hybrid compression framework combining Progressive Quantization (PQ) and Calibration-Assisted Distillation (CAD) to address these challenges. PQ employs a two-stage quantization with adaptive bit-width transitions guided by a momentum-based mechanism, reducing excessive weight perturbations in low-precision. CAD leverages full-precision calibration datasets during distillation, enabling the student to match full-precision performance even with a quantized teacher. As a result, PQCAD-DM achieves a balance between computational efficiency and generative quality, halving inference time while maintaining competitive performance. Extensive experiments validate PQCAD-DM's superior generative capabilities and efficiency across diverse datasets, outperforming fixed-bit quantization methods.

</details>


### [23] [RealSR-R1: Reinforcement Learning for Real-World Image Super-Resolution with Vision-Language Chain-of-Thought](https://arxiv.org/abs/2506.16796)
*Junbo Qiao,Miaomiao Cai,Wei Li,Yutong Liu,Xudong Huang,Gaoqi He,Jiao Xie,Jie Hu,Xinghao Chen,Shaohui Lin*

Main category: cs.CV

TL;DR: RealSR-R1提出了一种结合视觉与语言推理的VLCoT框架，并引入GRPO优化方法，显著提升了真实世界图像超分辨率的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在理解退化图像内容方面表现不佳，导致重建结果低保真且不自然。

Method: 提出VLCoT框架模拟人类处理退化图像的过程，并首次引入GRPO优化方法，设计了四种奖励函数。

Result: 实验表明，RealSR-R1能生成更真实的细节并准确理解图像内容，尤其在语义丰富或严重退化的场景中表现突出。

Conclusion: VLCoT-GRPO框架为真实世界图像超分辨率任务提供了有效的解决方案。

Abstract: Real-World Image Super-Resolution is one of the most challenging task in image restoration. However, existing methods struggle with an accurate understanding of degraded image content, leading to reconstructed results that are both low-fidelity and unnatural. We present RealSR-R1 in this work, which empowers the RealSR models with understanding and reasoning capabilities. Inspired by the success of Chain of Thought (CoT) in large language models (LLMs), we simulate the human process of handling degraded images and propose the VLCoT framework, which integrates vision and language reasoning. The framework aims to precisely restore image details by progressively generating more comprehensive text and higher-resolution images. To overcome the challenge of traditional supervised learning CoT failing to generalize to real-world scenarios, we introduce, for the first time, Group Relative Policy Optimization (GRPO) into the Real-World Image Super-Resolution task. We propose VLCoT-GRPO as a solution, which designs four reward functions: (1) Format reward, used to standardize the CoT process; (2) Degradation reward, to incentivize accurate degradation estimation; (3) Understanding reward, to ensure the accuracy of the generated content; and (4) Generation reward, where we propose using a visual expert model to evaluate the quality of generated images, encouraging the model to generate more realistic images. Extensive experiments demonstrate that our proposed RealSR-R1 can generate realistic details and accurately understand image content, particularly in semantically rich scenes or images with severe degradation.

</details>


### [24] [Seeing What Matters: Generalizable AI-generated Video Detection with Forensic-Oriented Augmentation](https://arxiv.org/abs/2506.16802)
*Riccardo Corvi,Davide Cozzolino,Ekta Prashnani,Shalini De Mello,Koki Nagano,Luisa Verdoliva*

Main category: cs.CV

TL;DR: 论文提出了一种新的视频伪造检测方法，通过引导分类器关注生成模型引入的低级伪影而非高级语义缺陷，提高了检测器的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频伪造检测器泛化能力差，限制了其在实际场景中的应用。

Method: 研究不同生成架构，识别共享的判别特征；提出基于小波分解的数据增强策略，引导模型利用更相关的伪造线索。

Result: 新方法显著提高了检测器的泛化能力，在多种生成模型上表现优异。

Conclusion: 该方法简单有效，无需复杂算法或大规模数据集，显著提升了检测器的性能。

Abstract: Synthetic video generation is progressing very rapidly. The latest models can produce very realistic high-resolution videos that are virtually indistinguishable from real ones. Although several video forensic detectors have been recently proposed, they often exhibit poor generalization, which limits their applicability in a real-world scenario. Our key insight to overcome this issue is to guide the detector towards seeing what really matters. In fact, a well-designed forensic classifier should focus on identifying intrinsic low-level artifacts introduced by a generative architecture rather than relying on high-level semantic flaws that characterize a specific model. In this work, first, we study different generative architectures, searching and identifying discriminative features that are unbiased, robust to impairments, and shared across models. Then, we introduce a novel forensic-oriented data augmentation strategy based on the wavelet decomposition and replace specific frequency-related bands to drive the model to exploit more relevant forensic cues. Our novel training paradigm improves the generalizability of AI-generated video detectors, without the need for complex algorithms and large datasets that include multiple synthetic generators. To evaluate our approach, we train the detector using data from a single generative model and test it against videos produced by a wide range of other models. Despite its simplicity, our method achieves a significant accuracy improvement over state-of-the-art detectors and obtains excellent results even on very recent generative models, such as NOVA and FLUX. Code and data will be made publicly available.

</details>


### [25] [FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation](https://arxiv.org/abs/2506.16806)
*Fan Yang,Yousong Zhu,Xin Li,Yufei Zhan,Hongyin Zhao,Shurong Zheng,Yaowei Wang,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: FOCUS是一个统一的LVLM模型，通过端到端框架整合了分割感知和可控对象生成，显著提升了视觉理解和生成能力。


<details>
  <summary>Details</summary>
Motivation: 当前方法将视觉理解和生成分离，依赖多个独立模型，FOCUS旨在统一这两者。

Method: 采用双分支视觉编码器、MoVQGAN视觉分词器和多阶段训练管道，结合分割掩码指导扩散解码器。

Result: 在多项任务中表现优异，验证了视觉感知与生成能力的联合优化效果。

Conclusion: FOCUS通过统一框架有效整合了分割感知与生成，为视觉语言模型提供了新方向。

Abstract: Recent Large Vision Language Models (LVLMs) demonstrate promising capabilities in unifying visual understanding and generative modeling, enabling both accurate content understanding and flexible editing. However, current approaches treat "what to see" and "how to edit" separately: they either perform isolated object segmentation or utilize segmentation masks merely as conditional prompts for local edit generation tasks, often relying on multiple disjointed models. To bridge these gaps, we introduce FOCUS, a unified LVLM that integrates segmentation-aware perception and controllable object-centric generation within an end-to-end framework. FOCUS employs a dual-branch visual encoder to simultaneously capture global semantic context and fine-grained spatial details. In addition, we leverage a MoVQGAN-based visual tokenizer to produce discrete visual tokens that enhance generation quality. To enable accurate and controllable image editing, we propose a progressive multi-stage training pipeline, where segmentation masks are jointly optimized and used as spatial condition prompts to guide the diffusion decoder. This strategy aligns visual encoding, segmentation, and generation modules, effectively bridging segmentation-aware perception with fine-grained visual synthesis. Extensive experiments across three core tasks, including multimodal understanding, referring segmentation accuracy, and controllable image generation, demonstrate that FOCUS achieves strong performance by jointly optimizing visual perception and generative capabilities.

</details>


### [26] [Visual-Instructed Degradation Diffusion for All-in-One Image Restoration](https://arxiv.org/abs/2506.16960)
*Wenyang Luo,Haina Qin,Zewen Chen,Libin Wang,Dandan Zheng,Yuming Li,Yufan Liu,Bing Li,Weiming Hu*

Main category: cs.CV

TL;DR: Defusion是一种基于视觉指令引导退化扩散的全能图像修复框架，通过构建明确的视觉指令来指导扩散模型，在退化空间中直接操作，显著提升稳定性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有图像修复方法通常需要针对每种退化类型单独设计模型，限制了其在混合或未知退化场景中的泛化能力。

Method: Defusion通过构建视觉指令（基于标准化视觉元素的退化）来捕捉退化特征，并利用这些指令引导扩散模型在退化空间中直接操作，实现高质量图像重建。

Result: 实验表明，Defusion在多种图像修复任务中（包括复杂和真实世界的退化）优于现有最先进方法。

Conclusion: Defusion通过视觉指令引导的退化扩散，提供了一种高效、通用的图像修复解决方案，适用于多样化的退化场景。

Abstract: Image restoration tasks like deblurring, denoising, and dehazing usually need distinct models for each degradation type, restricting their generalization in real-world scenarios with mixed or unknown degradations. In this work, we propose \textbf{Defusion}, a novel all-in-one image restoration framework that utilizes visual instruction-guided degradation diffusion. Unlike existing methods that rely on task-specific models or ambiguous text-based priors, Defusion constructs explicit \textbf{visual instructions} that align with the visual degradation patterns. These instructions are grounded by applying degradations to standardized visual elements, capturing intrinsic degradation features while agnostic to image semantics. Defusion then uses these visual instructions to guide a diffusion-based model that operates directly in the degradation space, where it reconstructs high-quality images by denoising the degradation effects with enhanced stability and generalizability. Comprehensive experiments demonstrate that Defusion outperforms state-of-the-art methods across diverse image restoration tasks, including complex and real-world degradations.

</details>


### [27] [Reversing Flow for Image Restoration](https://arxiv.org/abs/2506.16961)
*Haina Qin,Wenyang Luo,Libin Wang,Dandan Zheng,Jingdong Chen,Ming Yang,Bing Li,Weiming Hu*

Main category: cs.CV

TL;DR: ResFlow是一种新型图像修复框架，通过确定性路径建模退化过程，显著提升性能和速度。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型将退化过程视为随机变换，导致效率低下和复杂性增加。

Method: 使用连续归一化流建模退化过程，引入辅助过程消除HQ预测的不确定性，采用熵保持流路径并匹配速度场。

Result: 在少于四个采样步骤内完成任务，并在多个图像修复基准测试中达到最先进水平。

Conclusion: ResFlow为实际应用提供了一种高效实用的图像修复解决方案。

Abstract: Image restoration aims to recover high-quality (HQ) images from degraded low-quality (LQ) ones by reversing the effects of degradation. Existing generative models for image restoration, including diffusion and score-based models, often treat the degradation process as a stochastic transformation, which introduces inefficiency and complexity. In this work, we propose ResFlow, a novel image restoration framework that models the degradation process as a deterministic path using continuous normalizing flows. ResFlow augments the degradation process with an auxiliary process that disambiguates the uncertainty in HQ prediction to enable reversible modeling of the degradation process. ResFlow adopts entropy-preserving flow paths and learns the augmented degradation flow by matching the velocity field. ResFlow significantly improves the performance and speed of image restoration, completing the task in fewer than four sampling steps. Extensive experiments demonstrate that ResFlow achieves state-of-the-art results across various image restoration benchmarks, offering a practical and efficient solution for real-world applications.

</details>


### [28] [Unsupervised Image Super-Resolution Reconstruction Based on Real-World Degradation Patterns](https://arxiv.org/abs/2506.17027)
*Yiyang Tie,Hong Zhu,Yunyun Luo,Jing Shi*

Main category: cs.CV

TL;DR: 提出了一种TripleGAN框架，通过两个GAN组件分别处理模糊特性和其他退化模式，第三个GAN用于重建真实低分辨率图像，显著提升了超分辨率重建性能。


<details>
  <summary>Details</summary>
Motivation: 解决仅依赖真实低分辨率图像提取退化模式时的挑战，如模糊和噪声多样性，以及合成数据与真实数据间的退化域差距。

Method: TripleGAN框架：FirstGAN缩小模糊域差距，SecondGAN学习目标域模糊特性和其他退化模式，ThirdGAN基于伪真实数据重建低分辨率图像。

Result: 在RealSR和DRealSR数据集上表现优异，量化指标明显优势，重建图像清晰无过平滑伪影。

Conclusion: TripleGAN能有效学习真实退化模式并合成对齐数据集，显著提升超分辨率重建质量。

Abstract: The training of real-world super-resolution reconstruction models heavily relies on datasets that reflect real-world degradation patterns. Extracting and modeling degradation patterns for super-resolution reconstruction using only real-world low-resolution (LR) images remains a challenging task. When synthesizing datasets to simulate real-world degradation, relying solely on degradation extraction methods fails to capture both blur and diverse noise characteristics across varying LR distributions, as well as more implicit degradations such as color gamut shifts. Conversely, domain translation alone cannot accurately approximate real-world blur characteristics due to the significant degradation domain gap between synthetic and real data. To address these challenges, we propose a novel TripleGAN framework comprising two strategically designed components: The FirstGAN primarily focuses on narrowing the domain gap in blur characteristics, while the SecondGAN performs domain-specific translation to approximate target-domain blur properties and learn additional degradation patterns. The ThirdGAN is trained on pseudo-real data generated by the FirstGAN and SecondGAN to reconstruct real-world LR images. Extensive experiments on the RealSR and DRealSR datasets demonstrate that our method exhibits clear advantages in quantitative metrics while maintaining sharp reconstructions without over-smoothing artifacts. The proposed framework effectively learns real-world degradation patterns from LR observations and synthesizes aligned datasets with corresponding degradation characteristics, thereby enabling the trained network to achieve superior performance in reconstructing high-quality SR images from real-world LR inputs.

</details>


### [29] [Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition](https://arxiv.org/abs/2506.17201)
*Jiaqi Li,Junshu Tang,Zhiyong Xu,Longhuang Wu,Yuan Zhou,Shuai Shao,Tianbao Yu,Zhiguo Cao,Qinglin Lu*

Main category: cs.CV

TL;DR: Hunyuan-GameCraft是一个用于游戏环境中高动态交互视频生成的新框架，通过统一输入、混合历史条件训练和模型蒸馏解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态性、通用性、长期一致性和效率方面存在不足，限制了多样化游戏视频的生成能力。

Method: 统一键盘和鼠标输入到共享相机表示空间，提出混合历史条件训练策略，并进行模型蒸馏以提高效率。

Result: 在大型数据集上训练和微调后，Hunyuan-GameCraft在视觉保真度、真实感和动作可控性上显著优于现有模型。

Conclusion: Hunyuan-GameCraft提升了交互游戏视频的真实感和可玩性，适用于复杂实时环境。

Abstract: Recent advances in diffusion-based and controllable video generation have enabled high-quality and temporally coherent video synthesis, laying the groundwork for immersive interactive gaming experiences. However, current methods face limitations in dynamics, generality, long-term consistency, and efficiency, which limit the ability to create various gameplay videos. To address these gaps, we introduce Hunyuan-GameCraft, a novel framework for high-dynamic interactive video generation in game environments. To achieve fine-grained action control, we unify standard keyboard and mouse inputs into a shared camera representation space, facilitating smooth interpolation between various camera and movement operations. Then we propose a hybrid history-conditioned training strategy that extends video sequences autoregressively while preserving game scene information. Additionally, to enhance inference efficiency and playability, we achieve model distillation to reduce computational overhead while maintaining consistency across long temporal sequences, making it suitable for real-time deployment in complex interactive environments. The model is trained on a large-scale dataset comprising over one million gameplay recordings across over 100 AAA games, ensuring broad coverage and diversity, then fine-tuned on a carefully annotated synthetic dataset to enhance precision and control. The curated game scene data significantly improves the visual fidelity, realism and action controllability. Extensive experiments demonstrate that Hunyuan-GameCraft significantly outperforms existing models, advancing the realism and playability of interactive game video generation.

</details>


### [30] [Part$^{2}$GS: Part-aware Modeling of Articulated Objects using 3D Gaussian Splatting](https://arxiv.org/abs/2506.17212)
*Tianjiao Yu,Vedant Shah,Muntasir Wahed,Ying Shen,Kiet A. Nguyen,Ismini Lourentzou*

Main category: cs.CV

TL;DR: Part$^{2}$GS是一个新颖的框架，用于建模多部分物体的高保真几何和物理一致性的数字孪生。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的关节物体建模仍然是一个挑战，现有方法难以同时保持高保真几何和物理一致性。

Method: 采用部分感知的3D高斯表示，结合物理约束（如接触强制、速度一致性和矢量场对齐）和排斥点场来防止碰撞。

Result: 在合成和真实数据集上，Part$^{2}$GS在可移动部分的Chamfer距离上比现有方法提升高达10倍。

Conclusion: Part$^{2}$GS通过结合几何和物理约束，显著提升了关节物体建模的性能和一致性。

Abstract: Articulated objects are common in the real world, yet modeling their structure and motion remains a challenging task for 3D reconstruction methods. In this work, we introduce Part$^{2}$GS, a novel framework for modeling articulated digital twins of multi-part objects with high-fidelity geometry and physically consistent articulation. Part$^{2}$GS leverages a part-aware 3D Gaussian representation that encodes articulated components with learnable attributes, enabling structured, disentangled transformations that preserve high-fidelity geometry. To ensure physically consistent motion, we propose a motion-aware canonical representation guided by physics-based constraints, including contact enforcement, velocity consistency, and vector-field alignment. Furthermore, we introduce a field of repel points to prevent part collisions and maintain stable articulation paths, significantly improving motion coherence over baselines. Extensive evaluations on both synthetic and real-world datasets show that Part$^{2}$GS consistently outperforms state-of-the-art methods by up to 10$\times$ in Chamfer Distance for movable parts.

</details>


### [31] [Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens](https://arxiv.org/abs/2506.17218)
*Zeyuan Yang,Xueyang Yu,Delin Chen,Maohao Shen,Chuang Gan*

Main category: cs.CV

TL;DR: Mirage框架通过引入潜在视觉标记，增强视觉语言模型的多模态推理能力，无需生成显式图像。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在文本解码时需将视觉推理转化为语言表达，限制了需要视觉想象的任务性能。现有方法通过生成显式图像解决，但预训练负担重且影响推理能力。

Method: 提出Mirage框架，在解码时引入潜在视觉标记，通过蒸馏监督和强化学习优化多模态推理。

Result: 实验表明，Mirage在不生成显式图像的情况下提升了多模态推理能力。

Conclusion: Mirage为视觉语言模型提供了一种高效的多模态推理方法，避免了显式图像生成的负担。

Abstract: Vision-language models (VLMs) excel at multimodal understanding, yet their text-only decoding forces them to verbalize visual reasoning, limiting performance on tasks that demand visual imagination. Recent attempts train VLMs to render explicit images, but the heavy image-generation pre-training often hinders the reasoning ability. Inspired by the way humans reason with mental imagery-the internal construction and manipulation of visual cues-we investigate whether VLMs can reason through interleaved multimodal trajectories without producing explicit images. To this end, we present a Machine Mental Imagery framework, dubbed as Mirage, which augments VLM decoding with latent visual tokens alongside ordinary text. Concretely, whenever the model chooses to ``think visually'', it recasts its hidden states as next tokens, thereby continuing a multimodal trajectory without generating pixel-level images. Begin by supervising the latent tokens through distillation from ground-truth image embeddings, we then switch to text-only supervision to make the latent trajectory align tightly with the task objective. A subsequent reinforcement learning stage further enhances the multimodal reasoning capability. Experiments on diverse benchmarks demonstrate that Mirage unlocks stronger multimodal reasoning without explicit image generation.

</details>


### [32] [Emergent Temporal Correspondences from Video Diffusion Transformers](https://arxiv.org/abs/2506.17220)
*Jisu Nam,Soowon Son,Dahyun Chung,Jiyoung Kim,Siyoon Jin,Junhwa Hur,Seungryong Kim*

Main category: cs.CV

TL;DR: DiffTrack是一个定量分析框架，用于研究视频扩散模型（DiTs）如何建立时间对应关系，揭示了特定层在时间匹配中的关键作用，并在零样本点跟踪和视频生成中展示了实际应用。


<details>
  <summary>Details</summary>
Motivation: 研究视频扩散模型内部如何建立和表示时间对应关系，填补了现有研究的空白。

Method: 构建带有伪真实跟踪注释的数据集，提出新评估指标，分析3D注意力机制中各组件的作用。

Result: 特定层的查询-键相似性在时间匹配中起关键作用，且在去噪过程中逐渐显著；DiffTrack在零样本点跟踪中表现优异。

Conclusion: DiffTrack为理解视频DiTs的内部机制提供了重要见解，并为未来研究和应用奠定了基础。

Abstract: Recent advancements in video diffusion models based on Diffusion Transformers (DiTs) have achieved remarkable success in generating temporally coherent videos. Yet, a fundamental question persists: how do these models internally establish and represent temporal correspondences across frames? We introduce DiffTrack, the first quantitative analysis framework designed to answer this question. DiffTrack constructs a dataset of prompt-generated video with pseudo ground-truth tracking annotations and proposes novel evaluation metrics to systematically analyze how each component within the full 3D attention mechanism of DiTs (e.g., representations, layers, and timesteps) contributes to establishing temporal correspondences. Our analysis reveals that query-key similarities in specific, but not all, layers play a critical role in temporal matching, and that this matching becomes increasingly prominent during the denoising process. We demonstrate practical applications of DiffTrack in zero-shot point tracking, where it achieves state-of-the-art performance compared to existing vision foundation and self-supervised video models. Further, we extend our findings to motion-enhanced video generation with a novel guidance method that improves temporal consistency of generated videos without additional training. We believe our work offers crucial insights into the inner workings of video DiTs and establishes a foundation for further research and applications leveraging their temporal understanding.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [33] [FlowRAM: Grounding Flow Matching Policy with Region-Aware Mamba Framework for Robotic Manipulation](https://arxiv.org/abs/2506.16201)
*Sen Wang,Le Wang,Sanping Zhou,Jingyi Tian,Jiayi Li,Haowen Sun,Wei Tang*

Main category: cs.RO

TL;DR: FlowRAM提出了一种基于生成模型的高效区域感知框架，用于提升机器人高精度操作的性能与速度。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散的策略学习方法在推理时计算效率低，且未充分利用生成模型在3D环境中的信息探索潜力。

Method: FlowRAM采用动态半径调度实现自适应感知，结合状态空间模型处理多模态信息，并利用条件流匹配学习动作位姿。

Result: 在RLBench基准测试中，FlowRAM的平均成功率提升12.0%，且推理速度显著提高（少于4步）。

Conclusion: FlowRAM在高精度任务中表现出色，为机器人操作提供了高效且性能优越的解决方案。

Abstract: Robotic manipulation in high-precision tasks is essential for numerous industrial and real-world applications where accuracy and speed are required. Yet current diffusion-based policy learning methods generally suffer from low computational efficiency due to the iterative denoising process during inference. Moreover, these methods do not fully explore the potential of generative models for enhancing information exploration in 3D environments. In response, we propose FlowRAM, a novel framework that leverages generative models to achieve region-aware perception, enabling efficient multimodal information processing. Specifically, we devise a Dynamic Radius Schedule, which allows adaptive perception, facilitating transitions from global scene comprehension to fine-grained geometric details. Furthermore, we integrate state space models to integrate multimodal information, while preserving linear computational complexity. In addition, we employ conditional flow matching to learn action poses by regressing deterministic vector fields, simplifying the learning process while maintaining performance. We verify the effectiveness of the FlowRAM in the RLBench, an established manipulation benchmark, and achieve state-of-the-art performance. The results demonstrate that FlowRAM achieves a remarkable improvement, particularly in high-precision tasks, where it outperforms previous methods by 12.0% in average success rate. Additionally, FlowRAM is able to generate physically plausible actions for a variety of real-world tasks in less than 4 time steps, significantly increasing inference speed.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [34] [Diffusion-based Counterfactual Augmentation: Towards Robust and Interpretable Knee Osteoarthritis Grading](https://arxiv.org/abs/2506.15748)
*Zhe Wang,Yuhua Ru,Aladine Chetouani,Tina Shiang,Fang Chen,Fabian Bauer,Liping Zhang,Didier Hans,Rachid Jennane,William Ewing Palmer,Mohamed Jarraya,Yung Hsin Chen*

Main category: eess.IV

TL;DR: 论文提出了一种基于扩散模型的反事实增强框架（DCA），通过生成针对性反事实样本提升深度学习模型对膝骨关节炎（KOA）分级的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决膝骨关节炎分级中观察者间差异大及深度学习模型在关键决策边界附近鲁棒性不足的问题。

Method: 利用扩散模型的潜在空间，通过随机微分方程（SDE）生成反事实样本，并结合自校正学习策略优化分类器。

Result: 在OAI和MOST数据集上的实验表明，该方法显著提高了分类准确性，并提供了与临床知识一致的可解释性。

Conclusion: DCA框架通过将模型不确定性转化为训练信号，为开发更准确、可信的自动化诊断系统提供了新途径。

Abstract: Automated grading of Knee Osteoarthritis (KOA) from radiographs is challenged by significant inter-observer variability and the limited robustness of deep learning models, particularly near critical decision boundaries. To address these limitations, this paper proposes a novel framework, Diffusion-based Counterfactual Augmentation (DCA), which enhances model robustness and interpretability by generating targeted counterfactual examples. The method navigates the latent space of a diffusion model using a Stochastic Differential Equation (SDE), governed by balancing a classifier-informed boundary drive with a manifold constraint. The resulting counterfactuals are then used within a self-corrective learning strategy to improve the classifier by focusing on its specific areas of uncertainty. Extensive experiments on the public Osteoarthritis Initiative (OAI) and Multicenter Osteoarthritis Study (MOST) datasets demonstrate that this approach significantly improves classification accuracy across multiple model architectures. Furthermore, the method provides interpretability by visualizing minimal pathological changes and revealing that the learned latent space topology aligns with clinical knowledge of KOA progression. The DCA framework effectively converts model uncertainty into a robust training signal, offering a promising pathway to developing more accurate and trustworthy automated diagnostic systems. Our code is available at https://github.com/ZWang78/DCA.

</details>


### [35] [DiffO: Single-step Diffusion for Image Compression at Ultra-Low Bitrates](https://arxiv.org/abs/2506.16572)
*Chanung Park,Joo Chan Lee,Jong Hwan Ko*

Main category: eess.IV

TL;DR: DiffO是一种单步扩散模型，用于图像压缩，在极低比特率下提供高感知质量和快速解码。


<details>
  <summary>Details</summary>
Motivation: 现有图像压缩方法在极低比特率下质量下降严重，而基于扩散的模型解码延迟高且感知质量有限。

Method: DiffO结合了VQ残差训练和速率自适应噪声调制，分别捕捉全局几何与高频细节，并动态调整去噪强度以匹配目标比特率。

Result: 实验表明，DiffO在压缩性能上超越现有方法，解码速度提升约50倍。

Conclusion: DiffO显著提升了生成式编解码器的实用性，适用于极低比特率场景。

Abstract: Although image compression is fundamental to visual data processing and has inspired numerous standard and learned codecs, these methods still suffer severe quality degradation at extremely low bits per pixel. While recent diffusion based models provided enhanced generative performance at low bitrates, they still yields limited perceptual quality and prohibitive decoding latency due to multiple denoising steps. In this paper, we propose the first single step diffusion model for image compression (DiffO) that delivers high perceptual quality and fast decoding at ultra low bitrates. DiffO achieves these goals by coupling two key innovations: (i) VQ Residual training, which factorizes a structural base code and a learned residual in latent space, capturing both global geometry and high frequency details; and (ii) rate adaptive noise modulation, which tunes denoising strength on the fly to match the desired bitrate. Extensive experiments show that DiffO surpasses state of the art compression performance while improving decoding speed by about 50x compared to prior diffusion-based methods, greatly improving the practicality of generative codecs. The code will be available at https://github.com/Freemasti/DiffO.

</details>


### [36] [A Prior-Guided Joint Diffusion Model in Projection Domain for PET Tracer Conversion](https://arxiv.org/abs/2506.16733)
*Fang Chen,Weifeng Zhang,Xingyu Ai,BingXuan Li,An Li,Qiegen Liu*

Main category: eess.IV

TL;DR: 该研究提出了一种基于先验引导的联合扩散模型（PJDM），用于在投影域中将18F-FDG PET图像转换为18F-DOPA PET图像，以提高图像质量和合成效果。


<details>
  <summary>Details</summary>
Motivation: 18F-FDG PET在特定肿瘤中的效果有限，而18F-DOPA虽然特异性更高，但合成复杂且临床应用受限。因此，研究旨在通过投影域建模直接利用原始信息，减少重建过程中的误差累积。

Method: 研究采用先验引导的联合扩散模型（PJDM），包括粗估计模型和先验细化模型。通过高阶混合采样器生成初始合成图像，并利用学习到的先验进行迭代细化。

Result: 实验结果表明，PJDM显著提高了投影域图像质量和合成效果。

Conclusion: PJDM为18F-DOPA PET图像的合成提供了一种有效方法，克服了传统方法的局限性。

Abstract: Positron emission tomography (PET) is widely used to assess metabolic activity, but its application is limited by the availability of radiotracers. 18F-labeled fluorodeoxyglucose (18F-FDG) is the most commonly used tracer but shows limited effectiveness for certain tumors. In contrast, 6-18F-fluoro-3,4-dihydroxy-L-phenylalanine (18F-DOPA) offers higher specificity for neuroendocrine tumors and neurological disorders. However, its complex synthesis and limitations in transportation and clinical use hinder widespread adoption. During PET imaging, the sinogram represents a form of raw data acquired by the scanner. Therefore, modeling in projection domain enables more direct utilization of the original information, potentially reducing the accumulation of errors introduced during the image reconstruction process. Inspired by these factors, this study proposes a prior-guided joint diffusion model (PJDM) for transforming 18F-FDG PET images into 18F-DOPA PET images in projection domain. Specifically, a coarse estimation model and a prior refinement model are trained independently. During inference, an initial synthetic 18F-DOPA PET sinogram is generated using a higher-order hybrid sampler. This sinogram is then degraded and serves as an additional condition to guide the iterative refinement process using learned prior. Experimental results demonstrated that PJDM effectively improved both sinogram quality and synthetic outcomes. The code is available at: https://github.com/yqx7150/PJDM.

</details>


### [37] [Temperature calibration of surface emissivities with an improved thermal image enhancement network](https://arxiv.org/abs/2506.16803)
*Ning Chu,Siya Zheng,Shanqing Zhang,Li Li,Caifang Cai,Ali Mohammad-Djafari,Feng Zhao,Yuanbo Song*

Main category: eess.IV

TL;DR: 本文提出了一种物理引导的神经框架，通过对称跳跃-CNN架构和发射率感知注意力模块，统一温度校正和图像增强，解决了红外热成像中因材料发射率变化导致的温度精度问题。


<details>
  <summary>Details</summary>
Motivation: 红外热成像因材料发射率变化导致温度精度问题，现有方法常忽略辐射校准和图像退化的联合优化。

Method: 采用对称跳跃-CNN架构和发射率感知注意力模块，通过双约束损失函数（均值-方差对齐和基于KL散度的直方图匹配）优化温度校正和图像增强。

Result: 在工业鼓风机系统验证中，模型实现了热辐射特征与空间背景的动态融合，并在多种工业条件下获得准确校准结果。

Conclusion: 该方法有效抑制发射率伪影并恢复结构细节，为红外热成像的温度精度问题提供了创新解决方案。

Abstract: Infrared thermography faces persistent challenges in temperature accuracy due to material emissivity variations, where existing methods often neglect the joint optimization of radiometric calibration and image degradation. This study introduces a physically guided neural framework that unifies temperature correction and image enhancement through a symmetric skip-CNN architecture and an emissivity-aware attention module. The pre-processing stage segments the ROIs of the image and and initially corrected the firing rate. A novel dual-constrained loss function strengthens the statistical consistency between the target and reference regions through mean-variance alignment and histogram matching based on Kullback-Leibler dispersion. The method works by dynamically fusing thermal radiation features and spatial context, and the model suppresses emissivity artifacts while recovering structural details. After validating the industrial blower system under different conditions, the improved network realizes the dynamic fusion of thermal radiation characteristics and spatial background, with accurate calibration results in various industrial conditions.

</details>


### [38] [MeDi: Metadata-Guided Diffusion Models for Mitigating Biases in Tumor Classification](https://arxiv.org/abs/2506.17140)
*David Jacob Drexlin,Jonas Dippel,Julius Hense,Niklas Prenißl,Grégoire Montavon,Frederick Klauschen,Klaus-Robert Müller*

Main category: eess.IV

TL;DR: 提出了一种名为MeDi的生成模型框架，通过显式建模元数据来增强代表性不足的子群体数据，以减少下游模型的偏见。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在组织学预测任务中表现优异，但在临床实践中因对染色、扫描仪、医院和人口统计等条件缺乏鲁棒性而受限，导致学习捷径和偏见预测。

Method: 提出Metadata-guided generative Diffusion model（MeDi），通过生成合成数据平衡训练数据，减少偏见。

Result: MeDi在TCGA中为未见子群体生成高质量图像，提升生成图像保真度，并改善下游分类器在子群体偏移数据集上的性能。

Conclusion: MeDi是减少数据偏见的生成模型概念验证，为临床实践提供了潜在解决方案。

Abstract: Deep learning models have made significant advances in histological prediction tasks in recent years. However, for adaptation in clinical practice, their lack of robustness to varying conditions such as staining, scanner, hospital, and demographics is still a limiting factor: if trained on overrepresented subpopulations, models regularly struggle with less frequent patterns, leading to shortcut learning and biased predictions. Large-scale foundation models have not fully eliminated this issue. Therefore, we propose a novel approach explicitly modeling such metadata into a Metadata-guided generative Diffusion model framework (MeDi). MeDi allows for a targeted augmentation of underrepresented subpopulations with synthetic data, which balances limited training data and mitigates biases in downstream models. We experimentally show that MeDi generates high-quality histopathology images for unseen subpopulations in TCGA, boosts the overall fidelity of the generated images, and enables improvements in performance for downstream classifiers on datasets with subpopulation shifts. Our work is a proof-of-concept towards better mitigating data biases with generative models.

</details>


### [39] [Proportional Sensitivity in Generative Adversarial Network (GAN)-Augmented Brain Tumor Classification Using Convolutional Neural Network](https://arxiv.org/abs/2506.17165)
*Mahin Montasir Afif,Abdullah Al Noman,K. M. Tahsin Kabir,Md. Mortuza Ahmmed,Md. Mostafizur Rahman,Mufti Mahmud,Md. Ashraful Babu*

Main category: eess.IV

TL;DR: 研究探讨了GAN生成的脑肿瘤MRI图像与真实图像不同比例对CNN分类性能的影响，发现少量GAN数据能显著提升模型性能，但过多会降低效果。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像数据稀缺问题，探索GAN生成数据在增强数据集中的作用。

Method: 使用DCGAN生成合成图像，与真实图像按不同比例混合训练CNN，并在真实测试集上评估性能。

Result: 少量GAN数据（如100张）与900张真实数据混合时，模型准确率达95.2%，但GAN比例过高时性能下降。

Conclusion: GAN生成数据在有限真实数据时有用，但过多会引入伪影影响模型泛化能力。

Abstract: Generative Adversarial Networks (GAN) have shown potential in expanding limited medical imaging datasets. This study explores how different ratios of GAN-generated and real brain tumor MRI images impact the performance of a CNN in classifying healthy vs. tumorous scans. A DCGAN was used to create synthetic images which were mixed with real ones at various ratios to train a custom CNN. The CNN was then evaluated on a separate real-world test set. Our results indicate that the model maintains high sensitivity and precision in tumor classification, even when trained predominantly on synthetic data. When only a small portion of GAN data was added, such as 900 real images and 100 GAN images, the model achieved excellent performance, with test accuracy reaching 95.2%, and precision, recall, and F1-score all exceeding 95%. However, as the proportion of GAN images increased further, performance gradually declined. This study suggests that while GANs are useful for augmenting limited datasets especially when real data is scarce, too much synthetic data can introduce artifacts that affect the model's ability to generalize to real world cases.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [40] [DT-UFC: Universal Large Model Feature Coding via Peaky-to-Balanced Distribution Transformation](https://arxiv.org/abs/2506.16495)
*Changsheng Gao,Zijie Liu,Li Li,Dong Liu,Xiaoyan Sun,Weisi Lin*

Main category: cs.MM

TL;DR: 本文首次系统研究了大型模型的通用特征编码，提出了一种学习峰值到平衡分布变换的方法，解决了特征分布异构性问题，显著提升了压缩效率和跨模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究多针对特定任务或模型，而跨大型模型的通用特征编码问题尚未解决，特征分布异构性严重影响了压缩效率和泛化能力。

Method: 提出了一种非均匀、数据驱动的峰值到平衡分布变换方法，将异构特征分布对齐到共同的目标空间，无需修改下游编解码器。

Result: 在LLaMA3、DINOv2和SD3等多个模型和任务上的实验表明，该方法在压缩效率和跨模型泛化方面显著优于任务特定基线。

Conclusion: 该方法为大型模型的通用特征编码提供了有效解决方案，未来将开源代码以促进进一步研究。

Abstract: Like image coding in visual data transmission, feature coding is essential for the distributed deployment of large models by significantly reducing transmission and storage overhead. However, prior studies have mostly targeted task- or model-specific scenarios, leaving the challenge of universal feature coding across diverse large models largely unaddressed. In this paper, we present the first systematic study on universal feature coding for large models. The key challenge lies in the inherently diverse and distributionally incompatible nature of features extracted from different models. For example, features from DINOv2 exhibit highly peaky, concentrated distributions, while those from Stable Diffusion 3 (SD3) are more dispersed and uniform. This distributional heterogeneity severely hampers both compression efficiency and cross-model generalization. To address this, we propose a learned peaky-to-balanced distribution transformation, which reshapes highly skewed feature distributions into a common, balanced target space. This transformation is non-uniform, data-driven, and plug-and-play, enabling effective alignment of heterogeneous distributions without modifying downstream codecs. With this alignment, a universal codec trained on the balanced target distribution can effectively generalize to features from different models and tasks. We validate our approach on three representative large models-LLaMA3, DINOv2, and SD3-across multiple tasks and modalities. Extensive experiments show that our method achieves notable improvements in both compression efficiency and cross-model generalization over task-specific baselines. All source code will be released for future research.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [41] [Watermarking Autoregressive Image Generation](https://arxiv.org/abs/2506.16349)
*Nikola Jovanović,Ismail Labiad,Tomáš Souček,Martin Vechev,Pierre Fernandez*

Main category: cs.LG

TL;DR: 本文提出了一种针对自回归图像生成模型输出的水印方法，解决了重新标记图像令牌时水印丢失的问题，并通过实验验证了其可靠性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 自回归图像生成模型的输出可能被滥用，但目前缺乏在令牌级别上水印的方法，因此需要一种可靠的水印技术来追踪其来源。

Method: 通过调整语言模型水印技术，引入自定义的标记器-解标记器微调程序以提高反向循环一致性（RCC），并增加水印同步层。

Result: 实验表明，该方法能够可靠且鲁棒地检测水印，并提供理论支持的p值。

Conclusion: 该方法为自回归图像生成模型的输出提供了一种有效的水印解决方案，解决了重新标记导致的水印丢失问题。

Abstract: Watermarking the outputs of generative models has emerged as a promising approach for tracking their provenance. Despite significant interest in autoregressive image generation models and their potential for misuse, no prior work has attempted to watermark their outputs at the token level. In this work, we present the first such approach by adapting language model watermarking techniques to this setting. We identify a key challenge: the lack of reverse cycle-consistency (RCC), wherein re-tokenizing generated image tokens significantly alters the token sequence, effectively erasing the watermark. To address this and to make our method robust to common image transformations, neural compression, and removal attacks, we introduce (i) a custom tokenizer-detokenizer finetuning procedure that improves RCC, and (ii) a complementary watermark synchronization layer. As our experiments demonstrate, our approach enables reliable and robust watermark detection with theoretically grounded p-values.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [42] [AI's Blind Spots: Geographic Knowledge and Diversity Deficit in Generated Urban Scenario](https://arxiv.org/abs/2506.16898)
*Ciro Beneduce,Massimiliano Luca,Bruno Lepri*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Image generation models are revolutionizing many domains, and urban analysis and design is no exception. While such models are widely adopted, there is a limited literature exploring their geographic knowledge, along with the biases they embed. In this work, we generated 150 synthetic images for each state in the USA and related capitals using FLUX 1 and Stable Diffusion 3.5, two state-of-the-art models for image generation. We embed each image using DINO-v2 ViT-S/14 and the Fr\'echet Inception Distances to measure the similarity between the generated images. We found that while these models have implicitly learned aspects of USA geography, if we prompt the models to generate an image for "United States" instead of specific cities or states, the models exhibit a strong representative bias toward metropolis-like areas, excluding rural states and smaller cities. {\color{black} In addition, we found that models systematically exhibit some entity-disambiguation issues with European-sounding names like Frankfort or Devon.

</details>
