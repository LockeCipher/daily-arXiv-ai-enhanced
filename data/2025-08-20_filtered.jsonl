{"id": "2508.13228", "pdf": "https://arxiv.org/pdf/2508.13228", "abs": "https://arxiv.org/abs/2508.13228", "authors": ["Yuyan Ye", "Hang Xu", "Yanghang Huang", "Jiali Huang", "Qian Weng"], "title": "PreSem-Surf: RGB-D Surface Reconstruction with Progressive Semantic Modeling and SG-MLP Pre-Rendering Mechanism", "categories": ["cs.GR", "cs.AI", "cs.CV", "eess.IV"], "comment": "2025 International Joint Conference on Neural Networks (IJCNN 2025)", "summary": "This paper proposes PreSem-Surf, an optimized method based on the Neural Radiance Field (NeRF) framework, capable of reconstructing high-quality scene surfaces from RGB-D sequences in a short time. The method integrates RGB, depth, and semantic information to improve reconstruction performance. Specifically, a novel SG-MLP sampling structure combined with PR-MLP (Preconditioning Multilayer Perceptron) is introduced for voxel pre-rendering, allowing the model to capture scene-related information earlier and better distinguish noise from local details. Furthermore, progressive semantic modeling is adopted to extract semantic information at increasing levels of precision, reducing training time while enhancing scene understanding. Experiments on seven synthetic scenes with six evaluation metrics show that PreSem-Surf achieves the best performance in C-L1, F-score, and IoU, while maintaining competitive results in NC, Accuracy, and Completeness, demonstrating its effectiveness and practical applicability."}
{"id": "2508.13386", "pdf": "https://arxiv.org/pdf/2508.13386", "abs": "https://arxiv.org/abs/2508.13386", "authors": ["Ty Trusty", "David I. W. Levin", "Danny M. Kaufman"], "title": "Sparse, Geometry- and Material-Aware Bases for Multilevel Elastodynamic Simulation", "categories": ["cs.GR"], "comment": "15 pages,22 figures", "summary": "We present a multi-level elastodynamics timestep solver for accelerating incremental potential contact (IPC) simulations. Our method retains the robustness of gold standard IPC in the face of intricate geometry, complex heterogeneous material distributions and high resolution input data without sacrificing visual fidelity (per-timestep relative displacement error of $\\approx1\\%$). The success of our method is enabled by a novel, sparse, geometry- and material-aware basis construction method which allows for the use of fast preconditioned conjugate gradient solvers (in place of a sparse direct solver), but without suffering convergence issues due to stiff or heterogeneous materials. The end result is a solver that produces results visually indistinguishable and quantitatively very close to gold-standard IPC methods but up to $13\\times$ faster on identical hardware."}
{"id": "2508.13738", "pdf": "https://arxiv.org/pdf/2508.13738", "abs": "https://arxiv.org/abs/2508.13738", "authors": ["Shidong Wang", "Renato Pajarola"], "title": "Eliminating Rasterization: Direct Vector Floor Plan Generation with DiffPlanner", "categories": ["cs.GR"], "comment": "accepted to IEEE Transactions on Visualization and Computer Graphics", "summary": "The boundary-constrained floor plan generation problem aims to generate the topological and geometric properties of a set of rooms within a given boundary. Recently, learning-based methods have made significant progress in generating realistic floor plans. However, these methods involve a workflow of converting vector data into raster images, using image-based generative models, and then converting the results back into vector data. This process is complex and redundant, often resulting in information loss. Raster images, unlike vector data, cannot scale without losing detail and precision. To address these issues, we propose a novel deep learning framework called DiffPlanner for boundary-constrained floor plan generation, which operates entirely in vector space. Our framework is a Transformer-based conditional diffusion model that integrates an alignment mechanism in training, aligning the optimization trajectory of the model with the iterative design processes of designers. This enables our model to handle complex vector data, better fit the distribution of the predicted targets, accomplish the challenging task of floor plan layout design, and achieve user-controllable generation. We conduct quantitative comparisons, qualitative evaluations, ablation experiments, and perceptual studies to evaluate our method. Extensive experiments demonstrate that DiffPlanner surpasses existing state-of-the-art methods in generating floor plans and bubble diagrams in the creative stages, offering more controllability to users and producing higher-quality results that closely match the ground truths."}
{"id": "2508.13797", "pdf": "https://arxiv.org/pdf/2508.13797", "abs": "https://arxiv.org/abs/2508.13797", "authors": ["Feng-Lin Liu", "Shi-Yang Li", "Yan-Pei Cao", "Hongbo Fu", "Lin Gao"], "title": "Sketch3DVE: Sketch-based 3D-Aware Scene Video Editing", "categories": ["cs.GR", "cs.CV"], "comment": "SIGGRAPH 2025", "summary": "Recent video editing methods achieve attractive results in style transfer or appearance modification. However, editing the structural content of 3D scenes in videos remains challenging, particularly when dealing with significant viewpoint changes, such as large camera rotations or zooms. Key challenges include generating novel view content that remains consistent with the original video, preserving unedited regions, and translating sparse 2D inputs into realistic 3D video outputs. To address these issues, we propose Sketch3DVE, a sketch-based 3D-aware video editing method to enable detailed local manipulation of videos with significant viewpoint changes. To solve the challenge posed by sparse inputs, we employ image editing methods to generate edited results for the first frame, which are then propagated to the remaining frames of the video. We utilize sketching as an interaction tool for precise geometry control, while other mask-based image editing methods are also supported. To handle viewpoint changes, we perform a detailed analysis and manipulation of the 3D information in the video. Specifically, we utilize a dense stereo method to estimate a point cloud and the camera parameters of the input video. We then propose a point cloud editing approach that uses depth maps to represent the 3D geometry of newly edited components, aligning them effectively with the original 3D scene. To seamlessly merge the newly edited content with the original video while preserving the features of unedited regions, we introduce a 3D-aware mask propagation strategy and employ a video diffusion model to produce realistic edited videos. Extensive experiments demonstrate the superiority of Sketch3DVE in video editing. Homepage and code: http://http://geometrylearning.com/Sketch3DVE/"}
{"id": "2508.13808", "pdf": "https://arxiv.org/pdf/2508.13808", "abs": "https://arxiv.org/abs/2508.13808", "authors": ["Nan Luo", "Chenglin Ye", "Jiaxu Li", "Gang Liu", "Bo Wan", "Di Wang", "Lupeng Liu", "Jun Xiao"], "title": "Is-NeRF: In-scattering Neural Radiance Field for Blurred Images", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Neural Radiance Fields (NeRF) has gained significant attention for its prominent implicit 3D representation and realistic novel view synthesis capabilities. Available works unexceptionally employ straight-line volume rendering, which struggles to handle sophisticated lightpath scenarios and introduces geometric ambiguities during training, particularly evident when processing motion-blurred images. To address these challenges, this work proposes a novel deblur neural radiance field, Is-NeRF, featuring explicit lightpath modeling in real-world environments. By unifying six common light propagation phenomena through an in-scattering representation, we establish a new scattering-aware volume rendering pipeline adaptable to complex lightpaths. Additionally, we introduce an adaptive learning strategy that enables autonomous determining of scattering directions and sampling intervals to capture finer object details. The proposed network jointly optimizes NeRF parameters, scattering parameters, and camera motions to recover fine-grained scene representations from blurry images. Comprehensive evaluations demonstrate that it effectively handles complex real-world scenarios, outperforming state-of-the-art approaches in generating high-fidelity images with accurate geometric details."}
{"id": "2508.13300", "pdf": "https://arxiv.org/pdf/2508.13300", "abs": "https://arxiv.org/abs/2508.13300", "authors": ["Sirshapan Mitra", "Yogesh S. Rawat"], "title": "GaitCrafter: Diffusion Model for Biometric Preserving Gait Synthesis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Gait recognition is a valuable biometric task that enables the identification of individuals from a distance based on their walking patterns. However, it remains limited by the lack of large-scale labeled datasets and the difficulty of collecting diverse gait samples for each individual while preserving privacy. To address these challenges, we propose GaitCrafter, a diffusion-based framework for synthesizing realistic gait sequences in the silhouette domain. Unlike prior works that rely on simulated environments or alternative generative models, GaitCrafter trains a video diffusion model from scratch, exclusively on gait silhouette data. Our approach enables the generation of temporally consistent and identity-preserving gait sequences. Moreover, the generation process is controllable-allowing conditioning on various covariates such as clothing, carried objects, and view angle. We show that incorporating synthetic samples generated by GaitCrafter into the gait recognition pipeline leads to improved performance, especially under challenging conditions. Additionally, we introduce a mechanism to generate novel identities-synthetic individuals not present in the original dataset-by interpolating identity embeddings. These novel identities exhibit unique, consistent gait patterns and are useful for training models while maintaining privacy of real subjects. Overall, our work takes an important step toward leveraging diffusion models for high-quality, controllable, and privacy-aware gait data generation."}
{"id": "2508.13378", "pdf": "https://arxiv.org/pdf/2508.13378", "abs": "https://arxiv.org/abs/2508.13378", "authors": ["Yiting Wang", "Ziwei Wang", "Jiachen Zhong", "Di Zhu", "Weiyi Li"], "title": "Applications of Small Language Models in Medical Imaging Classification with a Focus on Prompt Strategies", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Large language models (LLMs) have shown remarkable capabilities in natural language processing and multi-modal understanding. However, their high computational cost, limited accessibility, and data privacy concerns hinder their adoption in resource-constrained healthcare environments. This study investigates the performance of small language models (SLMs) in a medical imaging classification task, comparing different models and prompt designs to identify the optimal combination for accuracy and usability. Using the NIH Chest X-ray dataset, we evaluate multiple SLMs on the task of classifying chest X-ray positions (anteroposterior [AP] vs. posteroanterior [PA]) under three prompt strategies: baseline instruction, incremental summary prompts, and correction-based reflective prompts. Our results show that certain SLMs achieve competitive accuracy with well-crafted prompts, suggesting that prompt engineering can substantially enhance SLM performance in healthcare applications without requiring deep AI expertise from end users."}
{"id": "2508.13471", "pdf": "https://arxiv.org/pdf/2508.13471", "abs": "https://arxiv.org/abs/2508.13471", "authors": ["Wenyong Zhou", "Taiqiang Wu", "Zhengwu Liu", "Yuxin Cheng", "Chen Zhang", "Ngai Wong"], "title": "MINR: Efficient Implicit Neural Representations for Multi-Image Encoding", "categories": ["cs.CV"], "comment": "4 pages, 4 figures", "summary": "Implicit Neural Representations (INRs) aim to parameterize discrete signals through implicit continuous functions. However, formulating each image with a separate neural network~(typically, a Multi-Layer Perceptron (MLP)) leads to computational and storage inefficiencies when encoding multi-images. To address this issue, we propose MINR, sharing specific layers to encode multi-image efficiently. We first compare the layer-wise weight distributions for several trained INRs and find that corresponding intermediate layers follow highly similar distribution patterns. Motivated by this, we share these intermediate layers across multiple images while preserving the input and output layers as input-specific. In addition, we design an extra novel projection layer for each image to capture its unique features. Experimental results on image reconstruction and super-resolution tasks demonstrate that MINR can save up to 60\\% parameters while maintaining comparable performance. Particularly, MINR scales effectively to handle 100 images, maintaining an average peak signal-to-noise ratio (PSNR) of 34 dB. Further analysis of various backbones proves the robustness of the proposed MINR."}
{"id": "2508.13479", "pdf": "https://arxiv.org/pdf/2508.13479", "abs": "https://arxiv.org/abs/2508.13479", "authors": ["Chao Wang", "Francesco Banterle", "Bin Ren", "Radu Timofte", "Xin Lu", "Yufeng Peng", "Chengjie Ge", "Zhijing Sun", "Ziang Zhou", "Zihao Li", "Zishun Liao", "Qiyu Kang", "Xueyang Fu", "Zheng-Jun Zha", "Zhijing Sun", "Xingbo Wang", "Kean Liu", "Senyan Xu", "Yang Qiu", "Yifan Ding", "Gabriel Eilertsen", "Jonas Unger", "Zihao Wang", "Ke Wu", "Jinshan Pan", "Zhen Liu", "Zhongyang Li", "Shuaicheng Liu", "S. M Nadim Uddin"], "title": "AIM 2025 challenge on Inverse Tone Mapping Report: Methods and Results", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "This paper presents a comprehensive review of the AIM 2025 Challenge on Inverse Tone Mapping (ITM). The challenge aimed to push forward the development of effective ITM algorithms for HDR image reconstruction from single LDR inputs, focusing on perceptual fidelity and numerical consistency. A total of \\textbf{67} participants submitted \\textbf{319} valid results, from which the best five teams were selected for detailed analysis. This report consolidates their methodologies and performance, with the lowest PU21-PSNR among the top entries reaching 29.22 dB. The analysis highlights innovative strategies for enhancing HDR reconstruction quality and establishes strong benchmarks to guide future research in inverse tone mapping."}
{"id": "2508.13485", "pdf": "https://arxiv.org/pdf/2508.13485", "abs": "https://arxiv.org/abs/2508.13485", "authors": ["Fuyang Liu", "Jilin Mei", "Fangyuan Mao", "Chen Min", "Yan Xing", "Yu Hu"], "title": "CORENet: Cross-Modal 4D Radar Denoising Network with LiDAR Supervision for Autonomous Driving", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 5 figures, Accepted to IROS 2025", "summary": "4D radar-based object detection has garnered great attention for its robustness in adverse weather conditions and capacity to deliver rich spatial information across diverse driving scenarios. Nevertheless, the sparse and noisy nature of 4D radar point clouds poses substantial challenges for effective perception. To address the limitation, we present CORENet, a novel cross-modal denoising framework that leverages LiDAR supervision to identify noise patterns and extract discriminative features from raw 4D radar data. Designed as a plug-and-play architecture, our solution enables seamless integration into voxel-based detection frameworks without modifying existing pipelines. Notably, the proposed method only utilizes LiDAR data for cross-modal supervision during training while maintaining full radar-only operation during inference. Extensive evaluation on the challenging Dual-Radar dataset, which is characterized by elevated noise level, demonstrates the effectiveness of our framework in enhancing detection robustness. Comprehensive experiments validate that CORENet achieves superior performance compared to existing mainstream approaches."}
{"id": "2508.13503", "pdf": "https://arxiv.org/pdf/2508.13503", "abs": "https://arxiv.org/abs/2508.13503", "authors": ["Tianyi Xu", "Fan Zhang", "Boxin Shi", "Tianfan Xue", "Yujin Wang"], "title": "AdaptiveAE: An Adaptive Exposure Strategy for HDR Capturing in Dynamic Scenes", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted to ICCV 2025", "summary": "Mainstream high dynamic range imaging techniques typically rely on fusing multiple images captured with different exposure setups (shutter speed and ISO). A good balance between shutter speed and ISO is crucial for achieving high-quality HDR, as high ISO values introduce significant noise, while long shutter speeds can lead to noticeable motion blur. However, existing methods often overlook the complex interaction between shutter speed and ISO and fail to account for motion blur effects in dynamic scenes.   In this work, we propose AdaptiveAE, a reinforcement learning-based method that optimizes the selection of shutter speed and ISO combinations to maximize HDR reconstruction quality in dynamic environments. AdaptiveAE integrates an image synthesis pipeline that incorporates motion blur and noise simulation into our training procedure, leveraging semantic information and exposure histograms. It can adaptively select optimal ISO and shutter speed sequences based on a user-defined exposure time budget, and find a better exposure schedule than traditional solutions. Experimental results across multiple datasets demonstrate that it achieves the state-of-the-art performance."}
{"id": "2508.13515", "pdf": "https://arxiv.org/pdf/2508.13515", "abs": "https://arxiv.org/abs/2508.13515", "authors": ["Yiang Shi", "Xiaoyang Guo", "Wei Yin", "Mingkai Jia", "Qian Zhang", "Xiaolin Hu", "Wenyu Liu", "Xinggang Wan"], "title": "2D Gaussians Meet Visual Tokenizer", "categories": ["cs.CV"], "comment": null, "summary": "The image tokenizer is a critical component in AR image generation, as it determines how rich and structured visual content is encoded into compact representations. Existing quantization-based tokenizers such as VQ-GAN primarily focus on appearance features like texture and color, often neglecting geometric structures due to their patch-based design. In this work, we explored how to incorporate more visual information into the tokenizer and proposed a new framework named Visual Gaussian Quantization (VGQ), a novel tokenizer paradigm that explicitly enhances structural modeling by integrating 2D Gaussians into traditional visual codebook quantization frameworks. Our approach addresses the inherent limitations of naive quantization methods such as VQ-GAN, which struggle to model structured visual information due to their patch-based design and emphasis on texture and color. In contrast, VGQ encodes image latents as 2D Gaussian distributions, effectively capturing geometric and spatial structures by directly modeling structure-related parameters such as position, rotation and scale. We further demonstrate that increasing the density of 2D Gaussians within the tokens leads to significant gains in reconstruction fidelity, providing a flexible trade-off between token efficiency and visual richness. On the ImageNet 256x256 benchmark, VGQ achieves strong reconstruction quality with an rFID score of 1.00. Furthermore, by increasing the density of 2D Gaussians within the tokens, VGQ gains a significant boost in reconstruction capability and achieves a state-of-the-art reconstruction rFID score of 0.556 and a PSNR of 24.93, substantially outperforming existing methods. Codes will be released soon."}
{"id": "2508.13524", "pdf": "https://arxiv.org/pdf/2508.13524", "abs": "https://arxiv.org/abs/2508.13524", "authors": ["Vamsi Krishna Mulukutla", "Sai Supriya Pavarala", "Srinivasa Raju Rudraraju", "Sridevi Bonthu"], "title": "Evaluating Open-Source Vision Language Models for Facial Emotion Recognition against Traditional Deep Learning Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Facial Emotion Recognition (FER) is crucial for applications such as human-computer interaction and mental health diagnostics. This study presents the first empirical comparison of open-source Vision-Language Models (VLMs), including Phi-3.5 Vision and CLIP, against traditional deep learning models VGG19, ResNet-50, and EfficientNet-B0 on the challenging FER-2013 dataset, which contains 35,887 low-resolution grayscale images across seven emotion classes. To address the mismatch between VLM training assumptions and the noisy nature of FER data, we introduce a novel pipeline that integrates GFPGAN-based image restoration with FER evaluation. Results show that traditional models, particularly EfficientNet-B0 (86.44%) and ResNet-50 (85.72%), significantly outperform VLMs like CLIP (64.07%) and Phi-3.5 Vision (51.66%), highlighting the limitations of VLMs in low-quality visual tasks. In addition to performance evaluation using precision, recall, F1-score, and accuracy, we provide a detailed computational cost analysis covering preprocessing, training, inference, and evaluation phases, offering practical insights for deployment. This work underscores the need for adapting VLMs to noisy environments and provides a reproducible benchmark for future research in emotion recognition."}
{"id": "2508.13537", "pdf": "https://arxiv.org/pdf/2508.13537", "abs": "https://arxiv.org/abs/2508.13537", "authors": ["Shikun Zhang", "Cunjian Chen", "Yiqun Wang", "Qiuhong Ke", "Yong Li"], "title": "EAvatar: Expression-Aware Head Avatar Reconstruction with Generative Geometry Priors", "categories": ["cs.CV", "cs.AI"], "comment": "20 pages, 11 figures", "summary": "High-fidelity head avatar reconstruction plays a crucial role in AR/VR, gaming, and multimedia content creation. Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated effectiveness in modeling complex geometry with real-time rendering capability and are now widely used in high-fidelity head avatar reconstruction tasks. However, existing 3DGS-based methods still face significant challenges in capturing fine-grained facial expressions and preserving local texture continuity, especially in highly deformable regions. To mitigate these limitations, we propose a novel 3DGS-based framework termed EAvatar for head reconstruction that is both expression-aware and deformation-aware. Our method introduces a sparse expression control mechanism, where a small number of key Gaussians are used to influence the deformation of their neighboring Gaussians, enabling accurate modeling of local deformations and fine-scale texture transitions. Furthermore, we leverage high-quality 3D priors from pretrained generative models to provide a more reliable facial geometry, offering structural guidance that improves convergence stability and shape accuracy during training. Experimental results demonstrate that our method produces more accurate and visually coherent head reconstructions with improved expression controllability and detail fidelity."}
{"id": "2508.13584", "pdf": "https://arxiv.org/pdf/2508.13584", "abs": "https://arxiv.org/abs/2508.13584", "authors": ["Ruixin Zhang", "Jiaqing Fan", "Yifan Liao", "Qian Qiao", "Fanzhang Li"], "title": "Temporal-Conditional Referring Video Object Segmentation with Noise-Free Text-to-Video Diffusion Model", "categories": ["cs.CV"], "comment": "11 pages, 7 figures", "summary": "Referring Video Object Segmentation (RVOS) aims to segment specific objects in a video according to textual descriptions. We observe that recent RVOS approaches often place excessive emphasis on feature extraction and temporal modeling, while relatively neglecting the design of the segmentation head. In fact, there remains considerable room for improvement in segmentation head design. To address this, we propose a Temporal-Conditional Referring Video Object Segmentation model, which innovatively integrates existing segmentation methods to effectively enhance boundary segmentation capability. Furthermore, our model leverages a text-to-video diffusion model for feature extraction. On top of this, we remove the traditional noise prediction module to avoid the randomness of noise from degrading segmentation accuracy, thereby simplifying the model while improving performance. Finally, to overcome the limited feature extraction capability of the VAE, we design a Temporal Context Mask Refinement (TCMR) module, which significantly improves segmentation quality without introducing complex designs. We evaluate our method on four public RVOS benchmarks, where it consistently achieves state-of-the-art performance."}
{"id": "2508.13592", "pdf": "https://arxiv.org/pdf/2508.13592", "abs": "https://arxiv.org/abs/2508.13592", "authors": ["Yoel Shapiro", "Yahia Showgan", "Koustav Mullick"], "title": "Bridging Clear and Adverse Driving Conditions", "categories": ["cs.CV"], "comment": null, "summary": "Autonomous Driving (AD) systems exhibit markedly degraded performance under adverse environmental conditions, such as low illumination and precipitation. The underrepresentation of adverse conditions in AD datasets makes it challenging to address this deficiency. To circumvent the prohibitive cost of acquiring and annotating adverse weather data, we propose a novel Domain Adaptation (DA) pipeline that transforms clear-weather images into fog, rain, snow, and nighttime images. Here, we systematically develop and evaluate several novel data-generation pipelines, including simulation-only, GAN-based, and hybrid diffusion-GAN approaches, to synthesize photorealistic adverse images from labelled clear images. We leverage an existing DA GAN, extend it to support auxiliary inputs, and develop a novel training recipe that leverages both simulated and real images. The simulated images facilitate exact supervision by providing perfectly matched image pairs, while the real images help bridge the simulation-to-real (sim2real) gap. We further introduce a method to mitigate hallucinations and artifacts in Stable-Diffusion Image-to-Image (img2img) outputs by blending them adaptively with their progenitor images. We finetune downstream models on our synthetic data and evaluate them on the Adverse Conditions Dataset with Correspondences (ACDC). We achieve 1.85 percent overall improvement in semantic segmentation, and 4.62 percent on nighttime, demonstrating the efficacy of our hybrid method for robust AD perception under challenging conditions."}
{"id": "2508.13628", "pdf": "https://arxiv.org/pdf/2508.13628", "abs": "https://arxiv.org/abs/2508.13628", "authors": ["Ao Chen", "Lihe Ding", "Tianfan Xue"], "title": "DiffIER: Optimizing Diffusion Models with Iterative Error Reduction", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have demonstrated remarkable capabilities in generating high-quality samples and enhancing performance across diverse domains through Classifier-Free Guidance (CFG). However, the quality of generated samples is highly sensitive to the selection of the guidance weight. In this work, we identify a critical ``training-inference gap'' and we argue that it is the presence of this gap that undermines the performance of conditional generation and renders outputs highly sensitive to the guidance weight. We quantify this gap by measuring the accumulated error during the inference stage and establish a correlation between the selection of guidance weight and minimizing this gap. Furthermore, to mitigate this gap, we propose DiffIER, an optimization-based method for high-quality generation. We demonstrate that the accumulated error can be effectively reduced by an iterative error minimization at each step during inference. By introducing this novel plug-and-play optimization framework, we enable the optimization of errors at every single inference step and enhance generation quality. Empirical results demonstrate that our proposed method outperforms baseline approaches in conditional generation tasks. Furthermore, the method achieves consistent success in text-to-image generation, image super-resolution, and text-to-speech generation, underscoring its versatility and potential for broad applications in future research."}
{"id": "2508.13632", "pdf": "https://arxiv.org/pdf/2508.13632", "abs": "https://arxiv.org/abs/2508.13632", "authors": ["Yutong Feng", "Linlin Zhang", "Hengyuan Cao", "Yiming Chen", "Xiaoduan Feng", "Jian Cao", "Yuxiong Wu", "Bin Wang"], "title": "OmniTry: Virtual Try-On Anything without Masks", "categories": ["cs.CV"], "comment": null, "summary": "Virtual Try-ON (VTON) is a practical and widely-applied task, for which most of existing works focus on clothes. This paper presents OmniTry, a unified framework that extends VTON beyond garment to encompass any wearable objects, e.g., jewelries and accessories, with mask-free setting for more practical application. When extending to various types of objects, data curation is challenging for obtaining paired images, i.e., the object image and the corresponding try-on result. To tackle this problem, we propose a two-staged pipeline: For the first stage, we leverage large-scale unpaired images, i.e., portraits with any wearable items, to train the model for mask-free localization. Specifically, we repurpose the inpainting model to automatically draw objects in suitable positions given an empty mask. For the second stage, the model is further fine-tuned with paired images to transfer the consistency of object appearance. We observed that the model after the first stage shows quick convergence even with few paired samples. OmniTry is evaluated on a comprehensive benchmark consisting of 12 common classes of wearable objects, with both in-shop and in-the-wild images. Experimental results suggest that OmniTry shows better performance on both object localization and ID-preservation compared with existing methods. The code, model weights, and evaluation benchmark of OmniTry will be made publicly available at https://omnitry.github.io/."}
{"id": "2508.13866", "pdf": "https://arxiv.org/pdf/2508.13866", "abs": "https://arxiv.org/abs/2508.13866", "authors": ["Paul Grimal", "Micha\u00ebl Soumm", "Herv\u00e9 Le Borgne", "Olivier Ferret", "Akihiro Sugimoto"], "title": "SAGA: Learning Signal-Aligned Distributions for Improved Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "State-of-the-art text-to-image models produce visually impressive results but often struggle with precise alignment to text prompts, leading to missing critical elements or unintended blending of distinct concepts. We propose a novel approach that learns a high-success-rate distribution conditioned on a target prompt, ensuring that generated images faithfully reflect the corresponding prompts. Our method explicitly models the signal component during the denoising process, offering fine-grained control that mitigates over-optimization and out-of-distribution artifacts. Moreover, our framework is training-free and seamlessly integrates with both existing diffusion and flow matching architectures. It also supports additional conditioning modalities -- such as bounding boxes -- for enhanced spatial alignment. Extensive experiments demonstrate that our approach outperforms current state-of-the-art methods. The code is available at https://github.com/grimalPaul/gsn-factory."}
{"id": "2508.13911", "pdf": "https://arxiv.org/pdf/2508.13911", "abs": "https://arxiv.org/abs/2508.13911", "authors": ["Chunji Lv", "Zequn Chen", "Donglin Di", "Weinan Zhang", "Hao Li", "Wei Chen", "Changsheng Li"], "title": "PhysGM: Large Physical Gaussian Model for Feed-Forward 4D Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "While physics-grounded 3D motion synthesis has seen significant progress, current methods face critical limitations. They typically rely on pre-reconstructed 3D Gaussian Splatting (3DGS) representations, while physics integration depends on either inflexible, manually defined physical attributes or unstable, optimization-heavy guidance from video models. To overcome these challenges, we introduce PhysGM, a feed-forward framework that jointly predicts a 3D Gaussian representation and its physical properties from a single image, enabling immediate, physical simulation and high-fidelity 4D rendering. We first establish a base model by jointly optimizing for Gaussian reconstruction and probabilistic physics prediction. The model is then refined with physically plausible reference videos to enhance both rendering fidelity and physics prediction accuracy. We adopt the Direct Preference Optimization (DPO) to align its simulations with reference videos, circumventing Score Distillation Sampling (SDS) optimization which needs back-propagating gradients through the complex differentiable simulation and rasterization. To facilitate the training, we introduce a new dataset PhysAssets of over 24,000 3D assets, annotated with physical properties and corresponding guiding videos. Experimental results demonstrate that our method effectively generates high-fidelity 4D simulations from a single image in one minute. This represents a significant speedup over prior works while delivering realistic rendering results. Our project page is at:https://hihixiaolv.github.io/PhysGM.github.io/"}
{"id": "2508.14014", "pdf": "https://arxiv.org/pdf/2508.14014", "abs": "https://arxiv.org/abs/2508.14014", "authors": ["Byeonggwon Lee", "Junkyu Park", "Khang Truong Giang", "Soohwan Song"], "title": "Online 3D Gaussian Splatting Modeling with Novel View Selection", "categories": ["cs.CV"], "comment": null, "summary": "This study addresses the challenge of generating online 3D Gaussian Splatting (3DGS) models from RGB-only frames. Previous studies have employed dense SLAM techniques to estimate 3D scenes from keyframes for 3DGS model construction. However, these methods are limited by their reliance solely on keyframes, which are insufficient to capture an entire scene, resulting in incomplete reconstructions. Moreover, building a generalizable model requires incorporating frames from diverse viewpoints to achieve broader scene coverage. However, online processing restricts the use of many frames or extensive training iterations. Therefore, we propose a novel method for high-quality 3DGS modeling that improves model completeness through adaptive view selection. By analyzing reconstruction quality online, our approach selects optimal non-keyframes for additional training. By integrating both keyframes and selected non-keyframes, the method refines incomplete regions from diverse viewpoints, significantly enhancing completeness. We also present a framework that incorporates an online multi-view stereo approach, ensuring consistency in 3D information throughout the 3DGS modeling process. Experimental results demonstrate that our method outperforms state-of-the-art methods, delivering exceptional performance in complex outdoor scenes."}
{"id": "2508.14033", "pdf": "https://arxiv.org/pdf/2508.14033", "abs": "https://arxiv.org/abs/2508.14033", "authors": ["Shaoshu Yang", "Zhe Kong", "Feng Gao", "Meng Cheng", "Xiangyu Liu", "Yong Zhang", "Zhuoliang Kang", "Wenhan Luo", "Xunliang Cai", "Ran He", "Xiaoming Wei"], "title": "InfiniteTalk: Audio-driven Video Generation for Sparse-Frame Video Dubbing", "categories": ["cs.CV"], "comment": "11 pages, 7 figures", "summary": "Recent breakthroughs in video AIGC have ushered in a transformative era for audio-driven human animation. However, conventional video dubbing techniques remain constrained to mouth region editing, resulting in discordant facial expressions and body gestures that compromise viewer immersion. To overcome this limitation, we introduce sparse-frame video dubbing, a novel paradigm that strategically preserves reference keyframes to maintain identity, iconic gestures, and camera trajectories while enabling holistic, audio-synchronized full-body motion editing. Through critical analysis, we identify why naive image-to-video models fail in this task, particularly their inability to achieve adaptive conditioning. Addressing this, we propose InfiniteTalk, a streaming audio-driven generator designed for infinite-length long sequence dubbing. This architecture leverages temporal context frames for seamless inter-chunk transitions and incorporates a simple yet effective sampling strategy that optimizes control strength via fine-grained reference frame positioning. Comprehensive evaluations on HDTF, CelebV-HQ, and EMTD datasets demonstrate state-of-the-art performance. Quantitative metrics confirm superior visual realism, emotional coherence, and full-body motion synchronization."}
{"id": "2508.14037", "pdf": "https://arxiv.org/pdf/2508.14037", "abs": "https://arxiv.org/abs/2508.14037", "authors": ["Lintao Xiang", "Xinkai Chen", "Jianhuang Lai", "Guangcong Wang"], "title": "Distilled-3DGS:Distilled 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "Project page: https://distilled3dgs.github.io Code:   https://github.com/lt-xiang/Distilled-3DGS", "summary": "3D Gaussian Splatting (3DGS) has exhibited remarkable efficacy in novel view synthesis (NVS). However, it suffers from a significant drawback: achieving high-fidelity rendering typically necessitates a large number of 3D Gaussians, resulting in substantial memory consumption and storage requirements. To address this challenge, we propose the first knowledge distillation framework for 3DGS, featuring various teacher models, including vanilla 3DGS, noise-augmented variants, and dropout-regularized versions. The outputs of these teachers are aggregated to guide the optimization of a lightweight student model. To distill the hidden geometric structure, we propose a structural similarity loss to boost the consistency of spatial geometric distributions between the student and teacher model. Through comprehensive quantitative and qualitative evaluations across diverse datasets, the proposed Distilled-3DGS, a simple yet effective framework without bells and whistles, achieves promising rendering results in both rendering quality and storage efficiency compared to state-of-the-art methods. Project page: https://distilled3dgs.github.io . Code: https://github.com/lt-xiang/Distilled-3DGS ."}
{"id": "2508.14041", "pdf": "https://arxiv.org/pdf/2508.14041", "abs": "https://arxiv.org/abs/2508.14041", "authors": ["Chin-Yang Lin", "Cheng Sun", "Fu-En Yang", "Min-Hung Chen", "Yen-Yu Lin", "Yu-Lun Liu"], "title": "LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos", "categories": ["cs.CV"], "comment": "ICCV 2025. Project page: https://linjohnss.github.io/longsplat/", "summary": "LongSplat addresses critical challenges in novel view synthesis (NVS) from casually captured long videos characterized by irregular camera motion, unknown camera poses, and expansive scenes. Current methods often suffer from pose drift, inaccurate geometry initialization, and severe memory limitations. To address these issues, we introduce LongSplat, a robust unposed 3D Gaussian Splatting framework featuring: (1) Incremental Joint Optimization that concurrently optimizes camera poses and 3D Gaussians to avoid local minima and ensure global consistency; (2) a robust Pose Estimation Module leveraging learned 3D priors; and (3) an efficient Octree Anchor Formation mechanism that converts dense point clouds into anchors based on spatial density. Extensive experiments on challenging benchmarks demonstrate that LongSplat achieves state-of-the-art results, substantially improving rendering quality, pose accuracy, and computational efficiency compared to prior approaches. Project page: https://linjohnss.github.io/longsplat/"}
{"id": "2508.13228", "pdf": "https://arxiv.org/pdf/2508.13228", "abs": "https://arxiv.org/abs/2508.13228", "authors": ["Yuyan Ye", "Hang Xu", "Yanghang Huang", "Jiali Huang", "Qian Weng"], "title": "PreSem-Surf: RGB-D Surface Reconstruction with Progressive Semantic Modeling and SG-MLP Pre-Rendering Mechanism", "categories": ["cs.GR", "cs.AI", "cs.CV", "eess.IV"], "comment": "2025 International Joint Conference on Neural Networks (IJCNN 2025)", "summary": "This paper proposes PreSem-Surf, an optimized method based on the Neural Radiance Field (NeRF) framework, capable of reconstructing high-quality scene surfaces from RGB-D sequences in a short time. The method integrates RGB, depth, and semantic information to improve reconstruction performance. Specifically, a novel SG-MLP sampling structure combined with PR-MLP (Preconditioning Multilayer Perceptron) is introduced for voxel pre-rendering, allowing the model to capture scene-related information earlier and better distinguish noise from local details. Furthermore, progressive semantic modeling is adopted to extract semantic information at increasing levels of precision, reducing training time while enhancing scene understanding. Experiments on seven synthetic scenes with six evaluation metrics show that PreSem-Surf achieves the best performance in C-L1, F-score, and IoU, while maintaining competitive results in NC, Accuracy, and Completeness, demonstrating its effectiveness and practical applicability."}
{"id": "2508.13287", "pdf": "https://arxiv.org/pdf/2508.13287", "abs": "https://arxiv.org/abs/2508.13287", "authors": ["Shuxin Liang", "Yihan Xiao", "Wenlu Tang"], "title": "InnerGS: Internal Scenes Rendering via Factorized 3D Gaussian Splatting", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has recently gained popularity for efficient scene rendering by representing scenes as explicit sets of anisotropic 3D Gaussians. However, most existing work focuses primarily on modeling external surfaces. In this work, we target the reconstruction of internal scenes, which is crucial for applications that require a deep understanding of an object's interior. By directly modeling a continuous volumetric density through the inner 3D Gaussian distribution, our model effectively reconstructs smooth and detailed internal structures from sparse sliced data. Our approach eliminates the need for camera poses, is plug-and-play, and is inherently compatible with any data modalities. We provide cuda implementation at: https://github.com/Shuxin-Liang/InnerGS."}
{"id": "2508.13776", "pdf": "https://arxiv.org/pdf/2508.13776", "abs": "https://arxiv.org/abs/2508.13776", "authors": ["Sebastian Ibarra", "Javier del Riego", "Alessandro Catanese", "Julian Cuba", "Julian Cardona", "Nataly Leon", "Jonathan Infante", "Karim Lekadir", "Oliver Diaz", "Richard Osuala"], "title": "Comparing Conditional Diffusion Models for Synthesizing Contrast-Enhanced Breast MRI from Pre-Contrast Images", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "13 pages, 5 figures, submitted and accepted to MICCAI Deepbreath   workshop 2025", "summary": "Dynamic contrast-enhanced (DCE) MRI is essential for breast cancer diagnosis and treatment. However, its reliance on contrast agents introduces safety concerns, contraindications, increased cost, and workflow complexity. To this end, we present pre-contrast conditioned denoising diffusion probabilistic models to synthesize DCE-MRI, introducing, evaluating, and comparing a total of 22 generative model variants in both single-breast and full breast settings. Towards enhancing lesion fidelity, we introduce both tumor-aware loss functions and explicit tumor segmentation mask conditioning. Using a public multicenter dataset and comparing to respective pre-contrast baselines, we observe that subtraction image-based models consistently outperform post-contrast-based models across five complementary evaluation metrics. Apart from assessing the entire image, we also separately evaluate the region of interest, where both tumor-aware losses and segmentation mask inputs improve evaluation metrics. The latter notably enhance qualitative results capturing contrast uptake, albeit assuming access to tumor localization inputs that are not guaranteed to be available in screening settings. A reader study involving 2 radiologists and 4 MRI technologists confirms the high realism of the synthetic images, indicating an emerging clinical potential of generative contrast-enhancement. We share our codebase at https://github.com/sebastibar/conditional-diffusion-breast-MRI."}
{"id": "2508.13797", "pdf": "https://arxiv.org/pdf/2508.13797", "abs": "https://arxiv.org/abs/2508.13797", "authors": ["Feng-Lin Liu", "Shi-Yang Li", "Yan-Pei Cao", "Hongbo Fu", "Lin Gao"], "title": "Sketch3DVE: Sketch-based 3D-Aware Scene Video Editing", "categories": ["cs.GR", "cs.CV"], "comment": "SIGGRAPH 2025", "summary": "Recent video editing methods achieve attractive results in style transfer or appearance modification. However, editing the structural content of 3D scenes in videos remains challenging, particularly when dealing with significant viewpoint changes, such as large camera rotations or zooms. Key challenges include generating novel view content that remains consistent with the original video, preserving unedited regions, and translating sparse 2D inputs into realistic 3D video outputs. To address these issues, we propose Sketch3DVE, a sketch-based 3D-aware video editing method to enable detailed local manipulation of videos with significant viewpoint changes. To solve the challenge posed by sparse inputs, we employ image editing methods to generate edited results for the first frame, which are then propagated to the remaining frames of the video. We utilize sketching as an interaction tool for precise geometry control, while other mask-based image editing methods are also supported. To handle viewpoint changes, we perform a detailed analysis and manipulation of the 3D information in the video. Specifically, we utilize a dense stereo method to estimate a point cloud and the camera parameters of the input video. We then propose a point cloud editing approach that uses depth maps to represent the 3D geometry of newly edited components, aligning them effectively with the original 3D scene. To seamlessly merge the newly edited content with the original video while preserving the features of unedited regions, we introduce a 3D-aware mask propagation strategy and employ a video diffusion model to produce realistic edited videos. Extensive experiments demonstrate the superiority of Sketch3DVE in video editing. Homepage and code: http://http://geometrylearning.com/Sketch3DVE/"}
{"id": "2508.13808", "pdf": "https://arxiv.org/pdf/2508.13808", "abs": "https://arxiv.org/abs/2508.13808", "authors": ["Nan Luo", "Chenglin Ye", "Jiaxu Li", "Gang Liu", "Bo Wan", "Di Wang", "Lupeng Liu", "Jun Xiao"], "title": "Is-NeRF: In-scattering Neural Radiance Field for Blurred Images", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Neural Radiance Fields (NeRF) has gained significant attention for its prominent implicit 3D representation and realistic novel view synthesis capabilities. Available works unexceptionally employ straight-line volume rendering, which struggles to handle sophisticated lightpath scenarios and introduces geometric ambiguities during training, particularly evident when processing motion-blurred images. To address these challenges, this work proposes a novel deblur neural radiance field, Is-NeRF, featuring explicit lightpath modeling in real-world environments. By unifying six common light propagation phenomena through an in-scattering representation, we establish a new scattering-aware volume rendering pipeline adaptable to complex lightpaths. Additionally, we introduce an adaptive learning strategy that enables autonomous determining of scattering directions and sampling intervals to capture finer object details. The proposed network jointly optimizes NeRF parameters, scattering parameters, and camera motions to recover fine-grained scene representations from blurry images. Comprehensive evaluations demonstrate that it effectively handles complex real-world scenarios, outperforming state-of-the-art approaches in generating high-fidelity images with accurate geometric details."}
{"id": "2508.13826", "pdf": "https://arxiv.org/pdf/2508.13826", "abs": "https://arxiv.org/abs/2508.13826", "authors": ["Niklas Bubeck", "Suprosanna Shit", "Chen Chen", "Can Zhao", "Pengfei Guo", "Dong Yang", "Georg Zitzlsberger", "Daguang Xu", "Bernhard Kainz", "Daniel Rueckert", "Jiazhen Pan"], "title": "Latent Interpolation Learning Using Diffusion Models for Cardiac Volume Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Cardiac Magnetic Resonance (CMR) imaging is a critical tool for diagnosing and managing cardiovascular disease, yet its utility is often limited by the sparse acquisition of 2D short-axis slices, resulting in incomplete volumetric information. Accurate 3D reconstruction from these sparse slices is essential for comprehensive cardiac assessment, but existing methods face challenges, including reliance on predefined interpolation schemes (e.g., linear or spherical), computational inefficiency, and dependence on additional semantic inputs such as segmentation labels or motion data. To address these limitations, we propose a novel \\textbf{Ca}rdiac \\textbf{L}atent \\textbf{I}nterpolation \\textbf{D}iffusion (CaLID) framework that introduces three key innovations. First, we present a data-driven interpolation scheme based on diffusion models, which can capture complex, non-linear relationships between sparse slices and improves reconstruction accuracy. Second, we design a computationally efficient method that operates in the latent space and speeds up 3D whole-heart upsampling time by a factor of 24, reducing computational overhead compared to previous methods. Third, with only sparse 2D CMR images as input, our method achieves SOTA performance against baseline methods, eliminating the need for auxiliary input such as morphological guidance, thus simplifying workflows. We further extend our method to 2D+T data, enabling the effective modeling of spatiotemporal dynamics and ensuring temporal coherence. Extensive volumetric evaluations and downstream segmentation tasks demonstrate that CaLID achieves superior reconstruction quality and efficiency. By addressing the fundamental limitations of existing approaches, our framework advances the state of the art for spatio and spatiotemporal whole-heart reconstruction, offering a robust and clinically practical solution for cardiovascular imaging."}
{"id": "2508.13907", "pdf": "https://arxiv.org/pdf/2508.13907", "abs": "https://arxiv.org/abs/2508.13907", "authors": ["Xiaopeng Peng", "Heath Gemar", "Erin Fleet", "Kyle Novak", "Abbie Watnik", "Grover Swartzlander"], "title": "Learning to See Through Flare", "categories": ["eess.IV", "cs.CV"], "comment": "accepted by ICCVW 2025", "summary": "Machine vision systems are susceptible to laser flare, where unwanted intense laser illumination blinds and distorts its perception of the environment through oversaturation or permanent damage to sensor pixels. We introduce NeuSee, the first computational imaging framework for high-fidelity sensor protection across the full visible spectrum. It jointly learns a neural representation of a diffractive optical element (DOE) and a frequency-space Mamba-GAN network for image restoration. NeuSee system is adversarially trained end-to-end on 100K unique images to suppress the peak laser irradiance as high as $10^6$ times the sensor saturation threshold $I_{\\textrm{sat}}$, the point at which camera sensors may experience damage without the DOE. Our system leverages heterogeneous data and model parallelism for distributed computing, integrating hyperspectral information and multiple neural networks for realistic simulation and image restoration. NeuSee takes into account open-world scenes with dynamically varying laser wavelengths, intensities, and positions, as well as lens flare effects, unknown ambient lighting conditions, and sensor noises. It outperforms other learned DOEs, achieving full-spectrum imaging and laser suppression for the first time, with a 10.1\\% improvement in restored image quality."}
