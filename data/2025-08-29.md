<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 24]
- [cs.RO](#cs.RO) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Mixture of Contexts for Long Video Generation](https://arxiv.org/abs/2508.21058)
*Shengqu Cai,Ceyuan Yang,Lvmin Zhang,Yuwei Guo,Junfei Xiao,Ziyan Yang,Yinghao Xu,Zhenheng Yang,Alan Yuille,Leonidas Guibas,Maneesh Agrawala,Lu Jiang,Gordon Wetzstein*

Main category: cs.GR

TL;DR: 提出了Mixture of Contexts (MoC)稀疏注意力路由模块，通过动态选择信息块和强制锚点来解决长视频生成中的长上下文记忆问题，实现近线性计算复杂度扩展


<details>
  <summary>Details</summary>
Motivation: 解决扩散变换器在生成长视频时因自注意力二次计算成本导致的内存和计算不可行问题，以及长序列优化困难

Method: 将长上下文视频生成重新定义为内部信息检索任务，使用可学习的稀疏注意力路由模块MoC，每个查询动态选择少量信息块和强制锚点（标题、局部窗口）进行注意力计算，采用因果路由防止循环闭合

Result: 随着数据规模扩展和路由稀疏化，模型能够将计算资源分配给显著历史信息，在数分钟内容中保持身份、动作和场景的一致性，实现近线性计算扩展

Conclusion: MoC模块通过信息检索的方式有效解决了长视频生成中的长上下文记忆问题，计算效率的提升使得分钟级视频的实用训练和合成成为可能

Abstract: Long video generation is fundamentally a long context memory problem: models must retain and retrieve salient events across a long range without collapsing or drifting. However, scaling diffusion transformers to generate long-context videos is fundamentally limited by the quadratic cost of self-attention, which makes memory and computation intractable and difficult to optimize for long sequences. We recast long-context video generation as an internal information retrieval task and propose a simple, learnable sparse attention routing module, Mixture of Contexts (MoC), as an effective long-term memory retrieval engine. In MoC, each query dynamically selects a few informative chunks plus mandatory anchors (caption, local windows) to attend to, with causal routing that prevents loop closures. As we scale the data and gradually sparsify the routing, the model allocates compute to salient history, preserving identities, actions, and scenes over minutes of content. Efficiency follows as a byproduct of retrieval (near-linear scaling), which enables practical training and synthesis, and the emergence of memory and consistency at the scale of minutes.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [VSF: Simple, Efficient, and Effective Negative Guidance in Few-Step Image Generation Models By Value Sign Flip](https://arxiv.org/abs/2508.10931)
*Wenqi Guo,Shan Du*

Main category: cs.CV

TL;DR: VSF是一种简单高效的负向提示引导方法，通过在扩散和流匹配模型中翻转负向提示的注意力值符号来抑制不需要的内容。


<details>
  <summary>Details</summary>
Motivation: 现有的负向提示引导方法如CFG、NASA和NAG在少步生成模型中效果有限，需要一种更有效的方法来动态抑制不需要的内容。

Method: VSF通过翻转负向提示的注意力值符号来动态抑制不需要的内容，计算开销小，适用于MMDiT架构和基于交叉注意力的模型。

Result: 实验表明VSF在复杂提示对数据集上表现优异，在少步模型中显著优于现有方法，在非少步模型中也优于CFG，同时保持图像质量。

Conclusion: VSF是一种高效且有效的负向提示引导方法，适用于静态图像和视频生成，代码已开源。

Abstract: We introduce Value Sign Flip (VSF), a simple and efficient method for incorporating negative prompt guidance in few-step diffusion and flow-matching image generation models. Unlike existing approaches such as classifier-free guidance (CFG), NASA, and NAG, VSF dynamically suppresses undesired content by flipping the sign of attention values from negative prompts. Our method requires only small computational overhead and integrates effectively with MMDiT-style architectures such as Stable Diffusion 3.5 Turbo, as well as cross-attention-based models like Wan. We validate VSF on challenging datasets with complex prompt pairs and demonstrate superior performance in both static image and video generation tasks. Experimental results show that VSF significantly improves negative prompt adherence compared to prior methods in few-step models, and even CFG in non-few-step models, while maintaining competitive image quality. Code and ComfyUI node are available in https://github.com/weathon/VSF/tree/main.

</details>


### [3] [SDiFL: Stable Diffusion-Driven Framework for Image Forgery Localization](https://arxiv.org/abs/2508.20182)
*Yang Su,Shunquan Tan,Jiwu Huang*

Main category: cs.CV

TL;DR: 基于Stable Diffusion V3的多模态框架，通过将伪造残差作为显式模态融合到潜空间，实现了无需涉及成本标注数据的高效准确图片伪造定位方法


<details>
  <summary>Details</summary>
Motivation: 现有图片伪造定位方法依赖人工标注数据，成本高且难以跟上多模态大模型带来的图片操纷技术发展

Method: 利用SD3多模态处理能力，将高速筛波器提取的伪造残差作为显式模态融合到潜空间，保留SD3提取的丰富语义信息

Result: 在广泛使用的案例数据集上较现有最先进方法提升12%，在未见过的真实文档伪造和自然场景图片上也表现突出

Conclusion: 该框架有效解决了依赖标注数据的限制，为应对新兴图片操纷技术提供了高效准确的伪造定位方案

Abstract: Driven by the new generation of multi-modal large models, such as Stable Diffusion (SD), image manipulation technologies have advanced rapidly, posing significant challenges to image forensics. However, existing image forgery localization methods, which heavily rely on labor-intensive and costly annotated data, are struggling to keep pace with these emerging image manipulation technologies. To address these challenges, we are the first to integrate both image generation and powerful perceptual capabilities of SD into an image forensic framework, enabling more efficient and accurate forgery localization. First, we theoretically show that the multi-modal architecture of SD can be conditioned on forgery-related information, enabling the model to inherently output forgery localization results. Then, building on this foundation, we specifically leverage the multimodal framework of Stable DiffusionV3 (SD3) to enhance forgery localization performance.We leverage the multi-modal processing capabilities of SD3 in the latent space by treating image forgery residuals -- high-frequency signals extracted using specific highpass filters -- as an explicit modality. This modality is fused into the latent space during training to enhance forgery localization performance. Notably, our method fully preserves the latent features extracted by SD3, thereby retaining the rich semantic information of the input image. Experimental results show that our framework achieves up to 12% improvements in performance on widely used benchmarking datasets compared to current state-of-the-art image forgery localization models. Encouragingly, the model demonstrates strong performance on forensic tasks involving real-world document forgery images and natural scene forging images, even when such data were entirely unseen during training.

</details>


### [4] [Audio-Guided Visual Editing with Complex Multi-Modal Prompts](https://arxiv.org/abs/2508.20379)
*Hyeonyu Kim,Seokhoon Jeong,Seonghee Han,Chanhyuk Choi,Taehwan Kim*

Main category: cs.CV

TL;DR: 基于先训多模态编码器的音频导航视觉编辑框架，无需额外训练即可处理复杂的多模态编辑任务


<details>
  <summary>Details</summary>
Motivation: 现有的文本导向编辑在复杂场景中存在限制，需要音频等非文本提示来提供更丰富的编辑信息

Method: 利用先训多模态编码器的零样本能力，通过消除音频编码空间与推淌模型提示编码空间的差异来集成音频，并提出分离噪声分支和适配性片选择来处理复杂多模态提示

Result: 在多样化编辑任务中表现优异，能够在仅依靠文本方法失败的场景中通过音频提供丰富信息来完成复杂编辑

Conclusion: 该框架为复杂多模态编辑任务提供了一种无需训练、具有良好普适性的解决方案，显示了音频在视觉编辑中的重要价值

Abstract: Visual editing with diffusion models has made significant progress but often struggles with complex scenarios that textual guidance alone could not adequately describe, highlighting the need for additional non-text editing prompts. In this work, we introduce a novel audio-guided visual editing framework that can handle complex editing tasks with multiple text and audio prompts without requiring additional training. Existing audio-guided visual editing methods often necessitate training on specific datasets to align audio with text, limiting their generalization to real-world situations. We leverage a pre-trained multi-modal encoder with strong zero-shot capabilities and integrate diverse audio into visual editing tasks, by alleviating the discrepancy between the audio encoder space and the diffusion model's prompt encoder space. Additionally, we propose a novel approach to handle complex scenarios with multiple and multi-modal editing prompts through our separate noise branching and adaptive patch selection. Our comprehensive experiments on diverse editing tasks demonstrate that our framework excels in handling complicated editing scenarios by incorporating rich information from audio, where text-only approaches fail.

</details>


### [5] [Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation](https://arxiv.org/abs/2508.20471)
*Jiusi Li,Jackson Jiang,Jinyu Miao,Miao Long,Tuopu Wen,Peijin Jia,Shengxiang Liu,Chunlei Yu,Maolin Liu,Yuzhan Cai,Kun Jiang,Mengmeng Yang,Diange Yang*

Main category: cs.CV

TL;DR: G^2Editor是一个用于驾驶视频中逼真精确物体编辑的框架，通过3D高斯表示和层次化细粒度特征实现精确姿态控制和视觉质量提升


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统需要大量角点案例进行训练验证，但真实世界收集成本高且危险。现有3D高斯溅射和图像生成模型方法存在视觉保真度有限和姿态控制不精确的问题

Method: 利用编辑对象的3D高斯表示作为密集先验注入去噪过程，使用场景级3D边界框布局重建非目标对象的遮挡区域，并引入层次化细粒度特征指导外观细节

Result: 在Waymo Open Dataset上的实验表明，G^2Editor在姿态可控性和视觉质量方面优于现有方法，有效支持物体重新定位、插入和删除操作

Conclusion: G^2Editor提供了一个统一框架，能够生成逼真的驾驶场景编辑结果，同时有利于下游数据驱动任务

Abstract: Corner cases are crucial for training and validating autonomous driving systems, yet collecting them from the real world is often costly and hazardous. Editing objects within captured sensor data offers an effective alternative for generating diverse scenarios, commonly achieved through 3D Gaussian Splatting or image generative models. However, these approaches often suffer from limited visual fidelity or imprecise pose control. To address these issues, we propose G^2Editor, a framework designed for photorealistic and precise object editing in driving videos. Our method leverages a 3D Gaussian representation of the edited object as a dense prior, injected into the denoising process to ensure accurate pose control and spatial consistency. A scene-level 3D bounding box layout is employed to reconstruct occluded areas of non-target objects. Furthermore, to guide the appearance details of the edited object, we incorporate hierarchical fine-grained features as additional conditions during generation. Experiments on the Waymo Open Dataset demonstrate that G^2Editor effectively supports object repositioning, insertion, and deletion within a unified framework, outperforming existing methods in both pose controllability and visual quality, while also benefiting downstream data-driven tasks.

</details>


### [6] [Towards Inclusive Communication: A Unified LLM-Based Framework for Sign Language, Lip Movements, and Audio Understanding](https://arxiv.org/abs/2508.20476)
*Jeong Hun Yeo,Hyeongseop Rha,Sungjune Park,Junil Won,Yong Man Ro*

Main category: cs.CV

TL;DR: 首个统一框架，能够处理手语、唇部动作和音频等多模态组合，用于语音文本生成，性能超越专门模型


<details>
  <summary>Details</summary>
Motivation: 解决ASR系统对聋人群体的访问性问题，探索视觉代替方案如手语和唇读的统一集成，开发能处理异构输入的模态无关结构

Method: 设计统一的模态无关架构，能够有效处理异质性输入，明确将唇部动作模型化为独立模态，探索模态间协同作用

Result: 在SLT、VSR、ASR和AVSR任务上达到或超过专门模型的最先进性能，明确模型化唇部动作显著提升了SLT性能

Conclusion: 该统一框架成功展示了多模态集成在无音频通信中的潜力，唇部动作作为非手势线索对手语理解具有重要价值

Abstract: Audio is the primary modality for human communication and has driven the success of Automatic Speech Recognition (ASR) technologies. However, such systems remain inherently inaccessible to individuals who are deaf or hard of hearing. Visual alternatives such as sign language and lip reading offer effective substitutes, and recent advances in Sign Language Translation (SLT) and Visual Speech Recognition (VSR) have improved audio-less communication. Yet, these modalities have largely been studied in isolation, and their integration within a unified framework remains underexplored. In this paper, we introduce the first unified framework capable of handling diverse combinations of sign language, lip movements, and audio for spoken-language text generation. We focus on three main objectives: (i) designing a unified, modality-agnostic architecture capable of effectively processing heterogeneous inputs; (ii) exploring the underexamined synergy among modalities, particularly the role of lip movements as non-manual cues in sign language comprehension; and (iii) achieving performance on par with or superior to state-of-the-art models specialized for individual tasks. Building on this framework, we achieve performance on par with or better than task-specific state-of-the-art models across SLT, VSR, ASR, and AVSR. Furthermore, our analysis reveals that explicitly modeling lip movements as a separate modality significantly improves SLT performance.

</details>


### [7] [Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent](https://arxiv.org/abs/2508.20505)
*En Ci,Shanyan Guan,Yanhao Ge,Yilin Zhang,Wei Li,Zhenyu Zhang,Jian Yang,Ying Tai*

Main category: cs.CV

TL;DR: DescriptiveEdit将基于指令的图像编辑重新定义为基于参考图像的文本到图像生成，通过交叉注意力UNet注入参考图像特征，避免了重建误差和数据集质量问题，在Emu Edit基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的语义图像编辑方法存在重建误差（基于反演的方法）和数据集质量限制（基于指令的方法）的问题，需要一种既能保持生成质量又不受数据集限制的解决方案。

Method: 提出DescriptiveEdit框架，将指令编辑重构为基于参考图像的文本生成任务。采用Cross-Attentive UNet架构，通过注意力桥接机制将参考图像特征注入到提示词到编辑图像的生成过程中。

Result: 在Emu Edit基准测试中，该方法提高了编辑准确性和一致性，能够无缝集成ControlNet、IP-Adapter等扩展，展现出更好的可扩展性。

Conclusion: DescriptiveEdit通过重新定义问题框架，有效解决了现有语义图像编辑方法的局限性，在保持生成质量的同时避免了数据集依赖问题，为图像编辑提供了新的解决方案。

Abstract: Despite the progress in text-to-image generation, semantic image editing remains a challenge. Inversion-based algorithms unavoidably introduce reconstruction errors, while instruction-based models mainly suffer from limited dataset quality and scale. To address these problems, we propose a descriptive-prompt-based editing framework, named DescriptiveEdit. The core idea is to re-frame `instruction-based image editing' as `reference-image-based text-to-image generation', which preserves the generative power of well-trained Text-to-Image models without architectural modifications or inversion. Specifically, taking the reference image and a prompt as input, we introduce a Cross-Attentive UNet, which newly adds attention bridges to inject reference image features into the prompt-to-edit-image generation process. Owing to its text-to-image nature, DescriptiveEdit overcomes limitations in instruction dataset quality, integrates seamlessly with ControlNet, IP-Adapter, and other extensions, and is more scalable. Experiments on the Emu Edit benchmark show it improves editing accuracy and consistency.

</details>


### [8] [Adam SLAM - the last mile of camera calibration with 3DGS](https://arxiv.org/abs/2508.20526)
*Matthieu Gendrin,Stéphane Pateux,Xiaoran Jiang,Théo Ladune,Luce Morin*

Main category: cs.CV

TL;DR: 使用3DGS模型通过反向传播新视角颜色损失来微调相机标定，在3DGS参考数据集上平均提升0.4 dB PSNR


<details>
  <summary>Details</summary>
Motivation: 相机标定质量对新颖视角合成至关重要，1像素误差就会显著影响重建质量，但真实场景缺乏地面真值，需要通过新视角合成质量来评估标定质量

Method: 提出使用3D高斯散射(3DGS)模型，通过反向传播新视角颜色损失来微调相机参数，优化相机标定

Result: 在3DGS参考数据集上，新的标定方法平均带来0.4 dB PSNR的改进

Conclusion: 虽然微调过程可能耗时且适用性取决于训练时间要求，但对于参考场景（如Mip-NeRF 360）的标定，新视角质量是最重要的考量因素

Abstract: The quality of the camera calibration is of major importance for evaluating progresses in novel view synthesis, as a 1-pixel error on the calibration has a significant impact on the reconstruction quality. While there is no ground truth for real scenes, the quality of the calibration is assessed by the quality of the novel view synthesis. This paper proposes to use a 3DGS model to fine tune calibration by backpropagation of novel view color loss with respect to the cameras parameters. The new calibration alone brings an average improvement of 0.4 dB PSNR on the dataset used as reference by 3DGS. The fine tuning may be long and its suitability depends on the criticity of training time, but for calibration of reference scenes, such as Mip-NeRF 360, the stake of novel view quality is the most important.

</details>


### [9] [FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models](https://arxiv.org/abs/2508.20586)
*Zheng Chong,Yanwei Lei,Shiyue Zhang,Zhuandi He,Zhen Wang,Xujie Zhang,Xiao Dong,Yiling Wu,Dongmei Jiang,Xiaodan Liang*

Main category: cs.CV

TL;DR: FastFit是一个基于可缓存扩散架构的高速多参考虚拟试穿框架，通过半注意力机制和类别嵌入替代时间步嵌入，实现了参考特征的一次计算多次复用，平均加速3.5倍，并在多个数据集上达到最先进的保真度指标。


<details>
  <summary>Details</summary>
Motivation: 当前虚拟试穿技术面临两大挑战：无法支持多参考服装组合（包括服装和配饰），以及在每个去噪步骤中重复计算参考特征导致的显著低效。

Method: 提出基于可缓存扩散架构的FastFit框架，采用半注意力机制，用类别嵌入替代传统时间步嵌入，完全解耦参考特征编码与去噪过程，实现参考特征的一次计算多次复用。

Result: 在VITON-HD、DressCode和新建的DressCode-MR数据集上，FastFit在关键保真度指标上超越最先进方法，同时实现平均3.5倍的推理加速。

Conclusion: FastFit通过创新的可缓存架构有效解决了多参考虚拟试穿的效率和性能问题，为复杂多参考虚拟试穿研究提供了新的解决方案和数据集支持。

Abstract: Despite its great potential, virtual try-on technology is hindered from real-world application by two major challenges: the inability of current methods to support multi-reference outfit compositions (including garments and accessories), and their significant inefficiency caused by the redundant re-computation of reference features in each denoising step. To address these challenges, we propose FastFit, a high-speed multi-reference virtual try-on framework based on a novel cacheable diffusion architecture. By employing a Semi-Attention mechanism and substituting traditional timestep embeddings with class embeddings for reference items, our model fully decouples reference feature encoding from the denoising process with negligible parameter overhead. This allows reference features to be computed only once and losslessly reused across all steps, fundamentally breaking the efficiency bottleneck and achieving an average 3.5x speedup over comparable methods. Furthermore, to facilitate research on complex, multi-reference virtual try-on, we introduce DressCode-MR, a new large-scale dataset. It comprises 28,179 sets of high-quality, paired images covering five key categories (tops, bottoms, dresses, shoes, and bags), constructed through a pipeline of expert models and human feedback refinement. Extensive experiments on the VITON-HD, DressCode, and our DressCode-MR datasets show that FastFit surpasses state-of-the-art methods on key fidelity metrics while offering its significant advantage in inference efficiency.

</details>


### [10] [Physics Informed Generative Models for Magnetic Field Images](https://arxiv.org/abs/2508.20612)
*Aye Phyu Phyu Aung,Lucas Lum,Zhansen Shi,Wen Qiu,Bernice Zee,JM Chin,Yeow Kheng Lim,J. Senthilnath*

Main category: cs.CV

TL;DR: 提出基于扩散模型和物理约束的PI-GenMFI方法，生成合成磁場圖像來解決MFI數據稀缺問題，用於半導體缺陷定位


<details>
  <summary>Details</summary>
Motivation: 半導體製造中MFI數據稀缺導致機器學習模型訓練困難，需要生成合成數據來優化缺陷定位流程

Method: 使用擴散模型結合兩個物理約束，生成常見電源短路缺陷類型的MFI合成圖像

Result: 與VAE和擴散模型等SOTA生成模型比較，通過領域專家評估和定量指標顯示出有希望的結果

Conclusion: PI-GenMFI方法能有效生成合成MFI數據，為ML算法提供訓練數據，優化半導體缺陷定位過程

Abstract: In semiconductor manufacturing, defect detection and localization are critical to ensuring product quality and yield. While X-ray imaging is a reliable non-destructive testing method, it is memory-intensive and time-consuming for large-scale scanning, Magnetic Field Imaging (MFI) offers a more efficient means to localize regions of interest (ROI) for targeted X-ray scanning. However, the limited availability of MFI datasets due to proprietary concerns presents a significant bottleneck for training machine learning (ML) models using MFI. To address this challenge, we consider an ML-driven approach leveraging diffusion models with two physical constraints. We propose Physics Informed Generative Models for Magnetic Field Images (PI-GenMFI) to generate synthetic MFI samples by integrating specific physical information. We generate MFI images for the most common defect types: power shorts. These synthetic images will serve as training data for ML algorithms designed to localize defect areas efficiently. To evaluate generated MFIs, we compare our model to SOTA generative models from both variational autoencoder (VAE) and diffusion methods. We present a domain expert evaluation to assess the generated samples. In addition, we present qualitative and quantitative evaluation using various metrics used for image generation and signal processing, showing promising results to optimize the defect localization process.

</details>


### [11] [Revisiting the Privacy Risks of Split Inference: A GAN-Based Data Reconstruction Attack via Progressive Feature Optimization](https://arxiv.org/abs/2508.20613)
*Yixiang Qiu,Yanhan Liu,Hongyao Yu,Hao Fang,Bin Chen,Shu-Tao Xia,Ke Xu*

Main category: cs.CV

TL;DR: 提出了一种基于GAN的渐进式特征优化数据重建攻击框架，能够有效从深度神经网络的分割推理中间特征中恢复高保真度的敏感输入数据。


<details>
  <summary>Details</summary>
Motivation: 现有的数据重建攻击通常只在浅层模型上有效，且未能充分利用语义先验，限制了重建质量和跨数据集、跨模型架构的泛化能力。

Method: 采用GAN框架，通过渐进式特征优化(PFO)将生成器分解为分层块，逐步细化中间表示；引入L1-ball约束来稳定优化并提高图像真实感。

Result: 在大量实验中，该方法显著优于现有攻击方法，特别是在高分辨率场景、分布外设置以及针对更深更复杂的DNN时表现突出。

Conclusion: 所提出的PFO框架有效解决了现有数据重建攻击的局限性，能够从分割推理的中间特征中高质量地恢复敏感数据，揭示了深度神经网络分割推理中的严重隐私风险。

Abstract: The growing complexity of Deep Neural Networks (DNNs) has led to the adoption of Split Inference (SI), a collaborative paradigm that partitions computation between edge devices and the cloud to reduce latency and protect user privacy. However, recent advances in Data Reconstruction Attacks (DRAs) reveal that intermediate features exchanged in SI can be exploited to recover sensitive input data, posing significant privacy risks. Existing DRAs are typically effective only on shallow models and fail to fully leverage semantic priors, limiting their reconstruction quality and generalizability across datasets and model architectures. In this paper, we propose a novel GAN-based DRA framework with Progressive Feature Optimization (PFO), which decomposes the generator into hierarchical blocks and incrementally refines intermediate representations to enhance the semantic fidelity of reconstructed images. To stabilize the optimization and improve image realism, we introduce an L1-ball constraint during reconstruction. Extensive experiments show that our method outperforms prior attacks by a large margin, especially in high-resolution scenarios, out-of-distribution settings, and against deeper and more complex DNNs.

</details>


### [12] [AvatarBack: Back-Head Generation for Complete 3D Avatars from Front-View Images](https://arxiv.org/abs/2508.20623)
*Shiqi Xin,Xiaolin Zhang,Yanbin Liu,Peng Zhang,Caifeng Shan*

Main category: cs.CV

TL;DR: AvatarBack是一个即插即用的框架，通过生成身份一致的后视图伪图像和自适应空间对齐策略，解决现有高斯溅射方法中后脑重建质量差的问题，实现完整一致的3D高斯头像重建。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯溅射的头像重建方法主要依赖正面视图图像，导致后脑区域重建质量差，存在几何不一致、结构模糊和真实感降低等问题，限制了重建头像的保真度。

Method: 提出AvatarBack框架，包含两个核心技术：1）特定主体生成器（SSG）利用生成先验从稀疏正面输入合成身份一致的后视图伪图像；2）自适应空间对齐策略（ASA）使用可学习的变换矩阵优化合成视图与3D高斯表示之间的几何对齐。

Result: 在NeRSemble和K-hairstyle数据集上的实验表明，AvatarBack显著提升了后脑重建质量，同时保持了正面保真度，重建的头像在不同运动下保持一致的视觉真实感且完全可动画化。

Conclusion: AvatarBack通过显式建模缺失的后脑区域，成功解决了3D高斯头像重建中的后脑质量问题，为完整一致的头像重建提供了有效解决方案。

Abstract: Recent advances in Gaussian Splatting have significantly boosted the reconstruction of head avatars, enabling high-quality facial modeling by representing an 3D avatar as a collection of 3D Gaussians. However, existing methods predominantly rely on frontal-view images, leaving the back-head poorly constructed. This leads to geometric inconsistencies, structural blurring, and reduced realism in the rear regions, ultimately limiting the fidelity of reconstructed avatars. To address this challenge, we propose AvatarBack, a novel plug-and-play framework specifically designed to reconstruct complete and consistent 3D Gaussian avatars by explicitly modeling the missing back-head regions. AvatarBack integrates two core technical innovations,i.e., the Subject-specific Generator (SSG) and the Adaptive Spatial Alignment Strategy (ASA). The former leverages a generative prior to synthesize identity-consistent, plausible back-view pseudo-images from sparse frontal inputs, providing robust multi-view supervision. To achieve precise geometric alignment between these synthetic views and the 3D Gaussian representation, the later employs learnable transformation matrices optimized during training, effectively resolving inherent pose and coordinate discrepancies. Extensive experiments on NeRSemble and K-hairstyle datasets, evaluated using geometric, photometric, and GPT-4o-based perceptual metrics, demonstrate that AvatarBack significantly enhances back-head reconstruction quality while preserving frontal fidelity. Moreover, the reconstructed avatars maintain consistent visual realism under diverse motions and remain fully animatable.

</details>


### [13] [CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models](https://arxiv.org/abs/2508.20640)
*Ayan Banerjee,Fernando Vilariño,Josep Lladós*

Main category: cs.CV

TL;DR: CraftGraffiti是一个端到端的文本引导涂鸦生成框架，通过风格优先、身份保持的方法解决极端风格转换中面部身份保持的挑战，在保持面部可识别性的同时实现高质量涂鸦艺术生成。


<details>
  <summary>Details</summary>
Motivation: 解决在极端风格转换（如涂鸦艺术）中保持面部身份识别性的挑战，因为细微的面部特征扭曲会消除主体的可识别性，影响个人和文化真实性。

Method: 采用端到端文本引导框架：1）使用LoRA微调的预训练扩散变压器进行涂鸦风格迁移；2）通过面部一致性自注意力机制增强身份嵌入；3）使用CLIP引导的提示扩展实现无关键点的姿态定制；4）实施"风格优先、身份保持"的生成范式。

Result: 定量结果显示竞争性的面部特征一致性和最先进的美学评分，人类偏好评分优异。在Cruilla音乐节的现场部署验证了系统的实际创作影响力。

Conclusion: CraftGraffiti推进了身份尊重的AI辅助艺术目标，为创造性AI应用中融合风格自由和可识别性提供了原则性方法。

Abstract: Preserving facial identity under extreme stylistic transformation remains a major challenge in generative art. In graffiti, a high-contrast, abstract medium, subtle distortions to the eyes, nose, or mouth can erase the subject's recognizability, undermining both personal and cultural authenticity. We present CraftGraffiti, an end-to-end text-guided graffiti generation framework designed with facial feature preservation as a primary objective. Given an input image and a style and pose descriptive prompt, CraftGraffiti first applies graffiti style transfer via LoRA-fine-tuned pretrained diffusion transformer, then enforces identity fidelity through a face-consistent self-attention mechanism that augments attention layers with explicit identity embeddings. Pose customization is achieved without keypoints, using CLIP-guided prompt extension to enable dynamic re-posing while retaining facial coherence. We formally justify and empirically validate the "style-first, identity-after" paradigm, showing it reduces attribute drift compared to the reverse order. Quantitative results demonstrate competitive facial feature consistency and state-of-the-art aesthetic and human preference scores, while qualitative analyses and a live deployment at the Cruilla Festival highlight the system's real-world creative impact. CraftGraffiti advances the goal of identity-respectful AI-assisted artistry, offering a principled approach for blending stylistic freedom with recognizability in creative AI applications.

</details>


### [14] ["Humor, Art, or Misinformation?": A Multimodal Dataset for Intent-Aware Synthetic Image Detection](https://arxiv.org/abs/2508.20670)
*Anastasios Skoularikis,Stefanos-Iordanis Papadopoulos,Symeon Papadopoulos,Panagiotis C. Petrantonakis*

Main category: cs.CV

TL;DR: S-HArM是一个多模态数据集，用于意图感知分类，包含来自Twitter/X和Reddit的9,576个图像-文本对，标注为幽默/讽刺、艺术或虚假信息。研究探索了三种提示策略来构建合成训练数据，发现图像和多模态引导的数据在泛化到真实内容方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多忽视了AI生成图像背后的意图，为了填补这一空白，需要开发能够识别内容创作意图的多模态分类方法。

Method: 构建S-HArM数据集，探索三种提示策略（图像引导、描述引导、多模态引导）使用Stable Diffusion生成合成训练数据，比较多种模型架构包括模态融合、对比学习、重建网络、注意力机制和大规模视觉语言模型。

Result: 使用图像和多模态引导数据训练的模型在泛化到真实内容方面表现更好，因为保留了视觉上下文信息。但整体性能仍然有限。

Conclusion: 推断意图的任务具有复杂性，需要专门的架构来解决这一挑战，当前方法的性能还有待提升。

Abstract: Recent advances in multimodal AI have enabled progress in detecting synthetic and out-of-context content. However, existing efforts largely overlook the intent behind AI-generated images. To fill this gap, we introduce S-HArM, a multimodal dataset for intent-aware classification, comprising 9,576 "in the wild" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art, or Misinformation. Additionally, we explore three prompting strategies (image-guided, description-guided, and multimodally-guided) to construct a large-scale synthetic training dataset with Stable Diffusion. We conduct an extensive comparative study including modality fusion, contrastive learning, reconstruction networks, attention mechanisms, and large vision-language models. Our results show that models trained on image- and multimodally-guided data generalize better to "in the wild" content, due to preserved visual context. However, overall performance remains limited, highlighting the complexity of inferring intent and the need for specialized architectures.

</details>


### [15] [CardioMorphNet: Cardiac Motion Prediction Using a Shape-Guided Bayesian Recurrent Deep Network](https://arxiv.org/abs/2508.20734)
*Reza Akbari Movahed,Abuzar Rezaee,Arezoo Zakeri,Colin Berry,Edmond S. L. Ho,Ali Gooya*

Main category: cs.CV

TL;DR: CardioMorphNet是一种基于贝叶斯深度学习的3D心脏形状引导形变配准框架，通过循环变分自编码器建模心脏周期时空依赖关系，不使用基于强度的图像配准相似性损失，在心脏运动估计方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖基于强度的图像配准相似性损失，可能忽略心脏解剖区域，难以准确捕捉心脏运动。需要一种能够专注于解剖区域的心脏运动估计方法。

Method: 提出CardioMorphNet循环贝叶斯深度学习框架，使用循环变分自编码器建模时空依赖，通过两个后验模型进行双心室分割和运动估计，利用分割图进行递归配准而不使用强度相似性损失。

Result: 在UK Biobank数据集上验证，CardioMorphNet在心脏运动估计方面优于最先进方法，且相比其他概率性心脏配准方法，在心脏区域产生更低的不确定性值，预测置信度更高。

Conclusion: CardioMorphNet通过形状引导的贝叶斯深度学习框架，有效解决了心脏运动估计问题，提供了更准确的运动场估计和不确定性量化，为心脏功能评估提供了可靠工具。

Abstract: Accurate cardiac motion estimation from cine cardiac magnetic resonance (CMR) images is vital for assessing cardiac function and detecting its abnormalities. Existing methods often struggle to capture heart motion accurately because they rely on intensity-based image registration similarity losses that may overlook cardiac anatomical regions. To address this, we propose CardioMorphNet, a recurrent Bayesian deep learning framework for 3D cardiac shape-guided deformable registration using short-axis (SAX) CMR images. It employs a recurrent variational autoencoder to model spatio-temporal dependencies over the cardiac cycle and two posterior models for bi-ventricular segmentation and motion estimation. The derived loss function from the Bayesian formulation guides the framework to focus on anatomical regions by recursively registering segmentation maps without using intensity-based image registration similarity loss, while leveraging sequential SAX volumes and spatio-temporal features. The Bayesian modelling also enables computation of uncertainty maps for the estimated motion fields. Validated on the UK Biobank dataset by comparing warped mask shapes with ground truth masks, CardioMorphNet demonstrates superior performance in cardiac motion estimation, outperforming state-of-the-art methods. Uncertainty assessment shows that it also yields lower uncertainty values for estimated motion fields in the cardiac region compared with other probabilistic-based cardiac registration methods, indicating higher confidence in its predictions.

</details>


### [16] [Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning](https://arxiv.org/abs/2508.20751)
*Yibin Wang,Zhimin Li,Yuhang Zang,Yujie Zhou,Jiazi Bu,Chunyu Wang,Qinglin Lu,Cheng Jin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 该论文提出了Pref-GRPO方法来解决文本到图像生成中的奖励破解问题，通过从分数最大化转向偏好拟合来稳定训练，并引入了UniGenBench综合评估基准。


<details>
  <summary>Details</summary>
Motivation: 当前基于点式奖励模型的文本到图像生成方法容易受到奖励破解的影响，即微小分数差异经过放大后导致模型过度优化琐碎增益，破坏生成稳定性。现有评估基准也因标准粗糙而限制了全面模型评估。

Method: 提出Pref-GRPO方法：使用成对偏好奖励模型，在每组图像中进行成对比较，以胜率作为奖励信号，将优化目标从分数最大化转向偏好拟合。同时开发UniGenBench基准，包含600个提示词，通过10个主要标准和27个子标准评估语义一致性。

Result: 实验表明Pref-GRPO能够区分细微的图像质量差异，提供更稳定的优势并缓解奖励破解问题。UniGenBench基准揭示了开源和闭源文本到图像模型的优缺点，并验证了Pref-GRPO的有效性。

Conclusion: Pref-GRPO通过偏好拟合方法有效解决了奖励破解问题，UniGenBench为文本到图像生成提供了更全面的评估框架，两者共同推动了该领域的发展。

Abstract: Recent advancements highlight the importance of GRPO-based reinforcement learning methods and benchmarking in enhancing text-to-image (T2I) generation. However, current methods using pointwise reward models (RM) for scoring generated images are susceptible to reward hacking. We reveal that this happens when minimal score differences between images are amplified after normalization, creating illusory advantages that drive the model to over-optimize for trivial gains, ultimately destabilizing the image generation process. To address this, we propose Pref-GRPO, a pairwise preference reward-based GRPO method that shifts the optimization objective from score maximization to preference fitting, ensuring more stable training. In Pref-GRPO, images are pairwise compared within each group using preference RM, and the win rate is used as the reward signal. Extensive experiments demonstrate that PREF-GRPO differentiates subtle image quality differences, providing more stable advantages and mitigating reward hacking. Additionally, existing T2I benchmarks are limited by coarse evaluation criteria, hindering comprehensive model assessment. To solve this, we introduce UniGenBench, a unified T2I benchmark comprising 600 prompts across 5 main themes and 20 subthemes. It evaluates semantic consistency through 10 primary and 27 sub-criteria, leveraging MLLM for benchmark construction and evaluation. Our benchmarks uncover the strengths and weaknesses of both open and closed-source T2I models and validate the effectiveness of Pref-GRPO.

</details>


### [17] [${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale Feature for Generalizable Gaussian Splatting](https://arxiv.org/abs/2508.20754)
*Yuxi Hu,Jun Zhang,Kuangyi Chen,Zhe Zhang,Friedrich Fraundorfer*

Main category: cs.CV

TL;DR: C3-GS是一个用于通用高斯溅射的框架，通过上下文感知、跨维度和跨尺度约束增强特征学习，实现无需逐场景优化的高质量新视图合成


<details>
  <summary>Details</summary>
Motivation: 现有方法在稀疏输入视图下难以编码判别性、多视图一致的特征来预测高斯参数，导致几何构建不准确

Method: 提出C3-GS框架，集成三个轻量级模块到统一渲染管道中，包含上下文感知、跨维度和跨尺度约束来改进特征融合

Result: 在基准数据集上的广泛实验验证了C3-GS实现了最先进的渲染质量和泛化能力

Conclusion: 该框架能够实现逼真的合成效果，无需额外监督，代码已开源

Abstract: Generalizable Gaussian Splatting aims to synthesize novel views for unseen scenes without per-scene optimization. In particular, recent advancements utilize feed-forward networks to predict per-pixel Gaussian parameters, enabling high-quality synthesis from sparse input views. However, existing approaches fall short in encoding discriminative, multi-view consistent features for Gaussian predictions, which struggle to construct accurate geometry with sparse views. To address this, we propose $\mathbf{C}^{3}$-GS, a framework that enhances feature learning by incorporating context-aware, cross-dimension, and cross-scale constraints. Our architecture integrates three lightweight modules into a unified rendering pipeline, improving feature fusion and enabling photorealistic synthesis without requiring additional supervision. Extensive experiments on benchmark datasets validate that $\mathbf{C}^{3}$-GS achieves state-of-the-art rendering quality and generalization ability. Code is available at: https://github.com/YuhsiHu/C3-GS.

</details>


### [18] [Estimating 2D Keypoints of Surgical Tools Using Vision-Language Models with Low-Rank Adaptation](https://arxiv.org/abs/2508.20830)
*Krit Duangprom,Tryphon Lambrou,Binod Bhattarai*

Main category: cs.CV

TL;DR: 使用LoRA微调视觉语言模型进行手术工具2D关键点估计，仅需2个epoch即可超越传统方法，在小规模医疗数据上表现优异


<details>
  <summary>Details</summary>
Motivation: 传统CNN和Transformer方法在小规模医疗数据集上容易过拟合，需要利用预训练视觉语言模型的泛化能力来解决这个问题

Method: 设计提示词创建指令调优数据集，使用LoRA低秩调整技术微调视觉语言模型，将视觉特征与语义关键点描述对齐

Result: 仅需2个epoch微调，适应后的VLM就超越了基线模型，在低资源场景下表现出色

Conclusion: 该方法不仅提高了关键点检测性能，还为未来3D手术手和工具姿态估计工作铺平了道路

Abstract: This paper presents a novel pipeline for 2D keypoint estima- tion of surgical tools by leveraging Vision Language Models (VLMs) fine- tuned using a low rank adjusting (LoRA) technique. Unlike traditional Convolutional Neural Network (CNN) or Transformer-based approaches, which often suffer from overfitting in small-scale medical datasets, our method harnesses the generalization capabilities of pre-trained VLMs. We carefully design prompts to create an instruction-tuning dataset and use them to align visual features with semantic keypoint descriptions. Experimental results show that with only two epochs of fine tuning, the adapted VLM outperforms the baseline models, demonstrating the ef- fectiveness of LoRA in low-resource scenarios. This approach not only improves keypoint detection performance, but also paves the way for future work in 3D surgical hands and tools pose estimation.

</details>


### [19] [Understanding and evaluating computer vision models through the lens of counterfactuals](https://arxiv.org/abs/2508.20881)
*Pushkar Shukla*

Main category: cs.CV

TL;DR: 该论文提出了基于反事实推理的框架，用于解释、审计和缓解视觉分类器和生成模型中的偏见，包括CAVLI、ASAC、TIBET、BiasConnect和InterMit等方法。


<details>
  <summary>Details</summary>
Motivation: 反事实推理已成为可解释和公平AI的核心方法，但需要系统化的框架来发现虚假相关性、探索因果依赖关系，并构建更鲁棒的系统。

Method: 开发了多个反事实方法：CAVLI整合归因和概念级分析；ASAC使用对抗性反事实扰动保护属性；TIBET提供可扩展的偏见评估流程；BiasConnect构建因果图诊断交叉偏见；InterMit提供无需训练的偏见缓解算法。

Result: 这些方法能够量化模型对可解释概念的依赖程度，提高模型的公平性和准确性，避免刻板印象，并实现对种族、性别、年龄等身份相关偏见的因果审计。

Conclusion: 反事实推理为判别性和生成性模型中的可解释性、公平性和因果性提供了统一的视角，建立了原则性、可扩展的社会责任偏见评估和缓解方法。

Abstract: Counterfactual reasoning -- the practice of asking ``what if'' by varying inputs and observing changes in model behavior -- has become central to interpretable and fair AI. This thesis develops frameworks that use counterfactuals to explain, audit, and mitigate bias in vision classifiers and generative models. By systematically altering semantically meaningful attributes while holding others fixed, these methods uncover spurious correlations, probe causal dependencies, and help build more robust systems.   The first part addresses vision classifiers. CAVLI integrates attribution (LIME) with concept-level analysis (TCAV) to quantify how strongly decisions rely on human-interpretable concepts. With localized heatmaps and a Concept Dependency Score, CAVLI shows when models depend on irrelevant cues like backgrounds. Extending this, ASAC introduces adversarial counterfactuals that perturb protected attributes while preserving semantics. Through curriculum learning, ASAC fine-tunes biased models for improved fairness and accuracy while avoiding stereotype-laden artifacts.   The second part targets generative Text-to-Image (TTI) models. TIBET provides a scalable pipeline for evaluating prompt-sensitive biases by varying identity-related terms, enabling causal auditing of how race, gender, and age affect image generation. To capture interactions, BiasConnect builds causal graphs diagnosing intersectional biases. Finally, InterMit offers a modular, training-free algorithm that mitigates intersectional bias via causal sensitivity scores and user-defined fairness goals.   Together, these contributions show counterfactuals as a unifying lens for interpretability, fairness, and causality in both discriminative and generative models, establishing principled, scalable methods for socially responsible bias evaluation and mitigation.

</details>


### [20] [COMETH: Convex Optimization for Multiview Estimation and Tracking of Humans](https://arxiv.org/abs/2508.20920)
*Enrico Martini,Ho Jin Choi,Nadia Figueroa,Nicola Bombieri*

Main category: cs.CV

TL;DR: COMETH是一个轻量级的多视角人体姿态融合算法，通过凸优化和状态观测器实现实时人体运动跟踪，在工业和安全关键应用中表现出色


<details>
  <summary>Details</summary>
Motivation: 解决工业5.0时代多摄像头系统中计算成本高、带宽需求大、边缘设备资源受限导致的精度下降和时空不一致性问题

Method: 集成运动学和生物力学约束提高关节定位精度，使用凸优化逆向运动学进行空间融合，实现状态观测器改善时间一致性

Result: 在公共和工业数据集上，COMETH在定位、检测和跟踪精度方面优于最先进方法

Conclusion: 该融合管道实现了准确且可扩展的人体运动跟踪，特别适合工业和安全关键应用

Abstract: In the era of Industry 5.0, monitoring human activity is essential for ensuring both ergonomic safety and overall well-being. While multi-camera centralized setups improve pose estimation accuracy, they often suffer from high computational costs and bandwidth requirements, limiting scalability and real-time applicability. Distributing processing across edge devices can reduce network bandwidth and computational load. On the other hand, the constrained resources of edge devices lead to accuracy degradation, and the distribution of computation leads to temporal and spatial inconsistencies. We address this challenge by proposing COMETH (Convex Optimization for Multiview Estimation and Tracking of Humans), a lightweight algorithm for real-time multi-view human pose fusion that relies on three concepts: it integrates kinematic and biomechanical constraints to increase the joint positioning accuracy; it employs convex optimization-based inverse kinematics for spatial fusion; and it implements a state observer to improve temporal consistency. We evaluate COMETH on both public and industrial datasets, where it outperforms state-of-the-art methods in localization, detection, and tracking accuracy. The proposed fusion pipeline enables accurate and scalable human motion tracking, making it well-suited for industrial and safety-critical applications. The code is publicly available at https://github.com/PARCO-LAB/COMETH.

</details>


### [21] [POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models](https://arxiv.org/abs/2508.21019)
*Jiaxiang Cheng,Bing Ma,Xuhua Ren,Hongyi Jin,Kai Yu,Peng Zhang,Wenyue Li,Yuan Zhou,Tianxiang Zheng,Qinglin Lu*

Main category: cs.CV

TL;DR: POSE是一个单步蒸馏框架，通过两阶段过程（稳定性预热和统一对抗均衡）来加速大规模视频扩散模型，将采样步骤从1000步减少到1步，延迟降低100倍，同时保持视频质量。


<details>
  <summary>Details</summary>
Motivation: 视频扩散生成领域在采样效率方面存在瓶颈，现有图像加速方法无法建模视频帧的时间连贯性，也无法为大规模视频模型提供单步蒸馏。

Method: 采用两阶段蒸馏过程：1）稳定性预热机制稳定对抗蒸馏，优化单步映射的视频质量；2）统一对抗均衡机制促进稳定的单步对抗训练达到纳什均衡；对于条件生成还提出条件对抗一致性方法。

Result: 在VBench-I2V上平均提升7.15%的语义对齐、时间一致性和帧质量，将预训练模型的延迟从1000秒降低到10秒（100倍加速）。

Conclusion: POSE成功解决了视频扩散模型采样效率低的问题，实现了高质量单步视频生成，在保持性能的同时大幅降低了计算延迟。

Abstract: The field of video diffusion generation faces critical bottlenecks in sampling efficiency, especially for large-scale models and long sequences. Existing video acceleration methods adopt image-based techniques but suffer from fundamental limitations: they neither model the temporal coherence of video frames nor provide single-step distillation for large-scale video models. To bridge this gap, we propose POSE (Phased One-Step Equilibrium), a distillation framework that reduces the sampling steps of large-scale video diffusion models, enabling the generation of high-quality videos in a single step. POSE employs a carefully designed two-phase process to distill video models:(i) stability priming: a warm-up mechanism to stabilize adversarial distillation that adapts the high-quality trajectory of the one-step generator from high to low signal-to-noise ratio regimes, optimizing the video quality of single-step mappings near the endpoints of flow trajectories. (ii) unified adversarial equilibrium: a flexible self-adversarial distillation mechanism that promotes stable single-step adversarial training towards a Nash equilibrium within the Gaussian noise space, generating realistic single-step videos close to real videos. For conditional video generation, we propose (iii) conditional adversarial consistency, a method to improve both semantic consistency and frame consistency between conditional frames and generated frames. Comprehensive experiments demonstrate that POSE outperforms other acceleration methods on VBench-I2V by average 7.15% in semantic alignment, temporal conference and frame quality, reducing the latency of the pre-trained model by 100$\times$, from 1000 seconds to 10 seconds, while maintaining competitive performance.

</details>


### [22] [Reusing Computation in Text-to-Image Diffusion for Efficient Generation of Image Sets](https://arxiv.org/abs/2508.21032)
*Dale Decatur,Thibault Groueix,Wang Yifan,Rana Hanocka,Vladimir Kim,Matheus Gadelha*

Main category: cs.CV

TL;DR: 提出一种无需训练的方法，通过聚类语义相似的提示词并在早期扩散步骤中共享计算，显著降低文本到图像生成的算力成本并提升图像质量


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型计算成本高昂，但相关提示词之间存在冗余，特别是在早期去噪步骤中捕获共享结构

Method: 基于提示词语义相似性进行聚类，在早期扩散步骤中共享计算，利用UnClip的文本到图像先验优化扩散步骤分配

Result: 显著降低计算成本，同时提高图像质量，可无缝集成到现有流程中并随提示词集规模扩展

Conclusion: 该方法通过利用提示词间的相关性，有效减轻大规模文本到图像生成的环境和经济负担

Abstract: Text-to-image diffusion models enable high-quality image generation but are computationally expensive. While prior work optimizes per-inference efficiency, we explore an orthogonal approach: reducing redundancy across correlated prompts. Our method leverages the coarse-to-fine nature of diffusion models, where early denoising steps capture shared structures among similar prompts. We propose a training-free approach that clusters prompts based on semantic similarity and shares computation in early diffusion steps. Experiments show that for models trained conditioned on image embeddings, our approach significantly reduces compute cost while improving image quality. By leveraging UnClip's text-to-image prior, we enhance diffusion step allocation for greater efficiency. Our method seamlessly integrates with existing pipelines, scales with prompt sets, and reduces the environmental and financial burden of large-scale text-to-image generation. Project page: https://ddecatur.github.io/hierarchical-diffusion/

</details>


### [23] [FW-GAN: Frequency-Driven Handwriting Synthesis with Wave-Modulated MLP Generator](https://arxiv.org/abs/2508.21040)
*Huynh Tong Dang Khoa,Dang Hoai Nam,Vo Nguyen Le Duy*

Main category: cs.CV

TL;DR: FW-GAN是一个基于频率感知的单样本手写合成框架，通过Wave-MLP和频率引导鉴别器生成高质量、风格一致的手写文本，有效解决了传统方法在长距离依赖和频率信息建模方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 标记手写数据稀缺限制了识别系统的效果，现有合成方法存在两个主要问题：传统卷积架构难以建模长距离依赖和复杂笔画模式，且忽视了频率信息在捕捉手写细节中的关键作用。

Method: 提出FW-GAN框架，包含相位感知Wave-MLP生成器来捕捉空间关系并保留风格线索，频率引导鉴别器利用高频成分增强真实性检测，以及新颖的频率分布损失函数对齐合成与真实手写的频率特征。

Result: 在越南语和英语手写数据集上的实验表明，FW-GAN能够生成高质量、风格一致的手写文本，有效提升低资源手写识别系统的数据增强效果。

Conclusion: FW-GAN通过频率感知方法显著提升了手写合成的真实性和风格一致性，为低资源手写识别提供了有价值的数据增强工具。

Abstract: Labeled handwriting data is often scarce, limiting the effectiveness of recognition systems that require diverse, style-consistent training samples. Handwriting synthesis offers a promising solution by generating artificial data to augment training. However, current methods face two major limitations. First, most are built on conventional convolutional architectures, which struggle to model long-range dependencies and complex stroke patterns. Second, they largely ignore the crucial role of frequency information, which is essential for capturing fine-grained stylistic and structural details in handwriting. To address these challenges, we propose FW-GAN, a one-shot handwriting synthesis framework that generates realistic, writer-consistent text from a single example. Our generator integrates a phase-aware Wave-MLP to better capture spatial relationships while preserving subtle stylistic cues. We further introduce a frequency-guided discriminator that leverages high-frequency components to enhance the authenticity detection of generated samples. Additionally, we introduce a novel Frequency Distribution Loss that aligns the frequency characteristics of synthetic and real handwriting, thereby enhancing visual fidelity. Experiments on Vietnamese and English handwriting datasets demonstrate that FW-GAN generates high-quality, style-consistent handwriting, making it a valuable tool for augmenting data in low-resource handwriting recognition (HTR) pipelines. Official implementation is available at https://github.com/DAIR-Group/FW-GAN

</details>


### [24] [OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning](https://arxiv.org/abs/2508.21066)
*Yuan Gong,Xionghui Wang,Jie Wu,Shiyin Wang,Yitong Wang,Xinglong Wu*

Main category: cs.CV

TL;DR: OneReward是一个统一的强化学习框架，使用单一奖励模型提升多任务生成能力，应用于掩码引导图像生成任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖任务特定的监督微调，限制了泛化能力和训练效率，需要统一的框架来处理不同数据分布和评估标准的多种任务。

Method: 使用单一视觉语言模型作为生成奖励模型，通过多任务强化学习直接训练预训练基础模型，无需任务特定的监督微调。

Result: 实验表明，统一的编辑模型在多个评估维度上优于商业和开源竞争对手，如Ideogram、Adobe Photoshop和FLUX Fill [Pro]。

Conclusion: OneReward框架通过单一奖励模型有效提升了多任务生成能力，证明了其在处理多样化任务和数据分布方面的优势。

Abstract: In this paper, we introduce OneReward, a unified reinforcement learning framework that enhances the model's generative capabilities across multiple tasks under different evaluation criteria using only \textit{One Reward} model. By employing a single vision-language model (VLM) as the generative reward model, which can distinguish the winner and loser for a given task and a given evaluation criterion, it can be effectively applied to multi-task generation models, particularly in contexts with varied data and diverse task objectives. We utilize OneReward for mask-guided image generation, which can be further divided into several sub-tasks such as image fill, image extend, object removal, and text rendering, involving a binary mask as the edit area. Although these domain-specific tasks share same conditioning paradigm, they differ significantly in underlying data distributions and evaluation metrics. Existing methods often rely on task-specific supervised fine-tuning (SFT), which limits generalization and training efficiency. Building on OneReward, we develop Seedream 3.0 Fill, a mask-guided generation model trained via multi-task reinforcement learning directly on a pre-trained base model, eliminating the need for task-specific SFT. Experimental results demonstrate that our unified edit model consistently outperforms both commercial and open-source competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across multiple evaluation dimensions. Code and model are available at: https://one-reward.github.io

</details>


### [25] [First-Place Solution to NeurIPS 2024 Invisible Watermark Removal Challenge](https://arxiv.org/abs/2508.21072)
*Fahad Shamshad,Tameem Bakr,Yahia Shaaban,Noor Hussein,Karthik Nandakumar,Nils Lukas*

Main category: cs.CV

TL;DR: 本文提出了针对NeurIPS 2024 Erasing the Invisible挑战的获胜解决方案，展示了在黑白盒攻击场景下近乎完美的水印去除效果（95.7%），同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有数字水印技术在对抗性攻击下的鲁棒性尚不明确，需要通过挑战赛来压力测试水印的鲁棒性。

Method: 针对beige-box跟踪使用基于自适应VAE的规避攻击，结合测试时优化和CIELAB空间的颜色对比度恢复；针对black-box跟踪采用基于空间或频域伪影的图像聚类，然后应用图像到图像扩散模型，结合受控噪声注入和ChatGPT生成的语义先验。

Result: 实证评估显示该方法成功实现了近乎完美的水印去除（95.7%），对剩余图像质量的影响可以忽略不计。

Conclusion: 希望这些攻击方法能够激发更鲁棒的图像水印技术的发展。

Abstract: Content watermarking is an important tool for the authentication and copyright protection of digital media. However, it is unclear whether existing watermarks are robust against adversarial attacks. We present the winning solution to the NeurIPS 2024 Erasing the Invisible challenge, which stress-tests watermark robustness under varying degrees of adversary knowledge. The challenge consisted of two tracks: a black-box and beige-box track, depending on whether the adversary knows which watermarking method was used by the provider. For the beige-box track, we leverage an adaptive VAE-based evasion attack, with a test-time optimization and color-contrast restoration in CIELAB space to preserve the image's quality. For the black-box track, we first cluster images based on their artifacts in the spatial or frequency-domain. Then, we apply image-to-image diffusion models with controlled noise injection and semantic priors from ChatGPT-generated captions to each cluster with optimized parameter settings. Empirical evaluations demonstrate that our method successfully achieves near-perfect watermark removal (95.7%) with negligible impact on the residual image's quality. We hope that our attacks inspire the development of more robust image watermarking methods.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [26] [SPGrasp: Spatiotemporal Prompt-driven Grasp Synthesis in Dynamic Scenes](https://arxiv.org/abs/2508.20547)
*Yunpeng Mei,Hongjie Cao,Yinqiu Xia,Wei Xiao,Zhaohan Feng,Gang Wang,Jie Chen*

Main category: cs.RO

TL;DR: SPGrasp是一个基于SAMv2的实时交互式抓取合成框架，通过整合用户提示和时空上下文，在动态物体上实现低延迟（59ms）且保持时间一致性的抓取估计。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态物体的实时交互抓取合成中无法同时实现低延迟推理和提示能力，存在延迟与交互性之间的权衡问题。

Method: 扩展SAMv2模型用于视频流抓取估计，核心创新是将用户提示与时空上下文整合，确保动态物体的时间一致性。

Result: 在OCID上达到90.6%的实例级抓取准确率，Jacquard上93.8%，GraspNet-1Billion连续跟踪下92.0%准确率，延迟73.1ms，比之前最佳方法降低58.5%。真实世界实验中13个移动物体的交互抓取成功率达94.8%。

Conclusion: SPGrasp有效解决了动态抓取合成中的延迟-交互性权衡问题，实现了实时交互式抓取合成。

Abstract: Real-time interactive grasp synthesis for dynamic objects remains challenging as existing methods fail to achieve low-latency inference while maintaining promptability. To bridge this gap, we propose SPGrasp (spatiotemporal prompt-driven dynamic grasp synthesis), a novel framework extending segment anything model v2 (SAMv2) for video stream grasp estimation. Our core innovation integrates user prompts with spatiotemporal context, enabling real-time interaction with end-to-end latency as low as 59 ms while ensuring temporal consistency for dynamic objects. In benchmark evaluations, SPGrasp achieves instance-level grasp accuracies of 90.6% on OCID and 93.8% on Jacquard. On the challenging GraspNet-1Billion dataset under continuous tracking, SPGrasp achieves 92.0% accuracy with 73.1 ms per-frame latency, representing a 58.5% reduction compared to the prior state-of-the-art promptable method RoG-SAM while maintaining competitive accuracy. Real-world experiments involving 13 moving objects demonstrate a 94.8% success rate in interactive grasping scenarios. These results confirm SPGrasp effectively resolves the latency-interactivity trade-off in dynamic grasp synthesis. Code is available at https://github.com/sejmoonwei/SPGrasp.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [27] [Efficient and Privacy-Protecting Background Removal for 2D Video Streaming using iPhone 15 Pro Max LiDAR](https://arxiv.org/abs/2508.20250)
*Jessica Kinnevan,Naifa Alqahtani,Toral Chauhan*

Main category: eess.IV

TL;DR: 这篇论文探讨了使用iPhone 15 Pro Max的LiDAR技术进行背景移除和合成的新方法，利用深度信息充分发挥了在不同光照条件下的稳定性优势。


<details>
  <summary>Details</summary>
Motivation: 传统背景移除技术如色度键控和AI模型很容易受光照条件影响，需要找到一种更稳定、不受光线限制的解决方案。

Method: 集成iPhone 15 Pro Max的LiDAR和颜色摄像头，使用SwiftUI和Swift框架开发用户界面和后端，采用Metal Shader Language进行实时图像增强，达到60fps的流式框率。

Result: 系统能够在低光和光线充足环境下都表现良好，但深度数据的流媒体带宽限制了深度图分辨率（320x240），且LiDAR红外激光对某些材料的深度反射有限制。

Conclusion: 如果iPhone等移动设备的LiDAR分辨率能提升到与色彩图像相等的水平，LiDAR技术有望成为视频应用和摄影领域的领先背景移除方法。

Abstract: Light Detection and Ranging (LiDAR) technology in consumer-grade mobile devices can be used as a replacement for traditional background removal and compositing techniques. Unlike approaches such as chroma keying and trained AI models, LiDAR's depth information is independent of subject lighting, and performs equally well in low-light and well-lit environments. We integrate the LiDAR and color cameras on the iPhone 15 Pro Max with GPU-based image processing. We use Apple's SwiftUI and Swift frameworks for user interface and backend development, and Metal Shader Language (MSL) for realtime image enhancement at the standard iPhone streaming frame rate of 60 frames per second. The only meaningful limitations of the technology are the streaming bandwidth of the depth data, which currently reduces the depth map resolution to 320x240, and any pre-existing limitations of the LiDAR IR laser to reflect accurate depth from some materials. If the LiDAR resolution on a mobile device like the iPhone can be improved to match the color image resolution, LiDAR could feasibly become the preeminent method of background removal for video applications and photography.

</details>


### [28] [GENRE-CMR: Generalizable Deep Learning for Diverse Multi-Domain Cardiac MRI Reconstruction](https://arxiv.org/abs/2508.20600)
*Kian Anvari Hamedani,Narges Razizadeh,Shahabedin Nabavi,Mohsen Ebrahimi Moghaddam*

Main category: eess.IV

TL;DR: GENRE-CMR是一种基于GAN的生成对抗网络架构，采用残差深度展开重建框架，通过边缘感知区域损失和统计分布对齐损失，显著提升心脏磁共振图像重建质量和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 加速心脏磁共振成像在扫描时间与图像质量之间存在权衡，特别是在不同采集设置下的泛化能力方面存在挑战。需要开发能够适应异构采集协议的统一且鲁棒的解决方案。

Method: 提出基于GAN的残差深度展开重建框架，将迭代优化展开为卷积子网络级联，集成残差连接实现渐进特征传播。引入两种损失函数：边缘感知区域损失（关注结构信息区域）和统计分布对齐损失（通过对称KL散度正则化特征空间）。

Result: 在未见数据分布上达到0.9552 SSIM和38.90 dB PSNR，在各种加速因子和采样轨迹下超越现有最先进方法。消融研究证实各组件对重建质量和泛化能力的贡献。

Conclusion: 该框架为高质量心脏磁共振重建提供了统一且鲁棒的解决方案，为临床适应性部署铺平了道路。

Abstract: Accelerated Cardiovascular Magnetic Resonance (CMR) image reconstruction remains a critical challenge due to the trade-off between scan time and image quality, particularly when generalizing across diverse acquisition settings. We propose GENRE-CMR, a generative adversarial network (GAN)-based architecture employing a residual deep unrolled reconstruction framework to enhance reconstruction fidelity and generalization. The architecture unrolls iterative optimization into a cascade of convolutional subnetworks, enriched with residual connections to enable progressive feature propagation from shallow to deeper stages. To further improve performance, we integrate two loss functions: (1) an Edge-Aware Region (EAR) loss, which guides the network to focus on structurally informative regions and helps prevent common reconstruction blurriness; and (2) a Statistical Distribution Alignment (SDA) loss, which regularizes the feature space across diverse data distributions via a symmetric KL divergence formulation. Extensive experiments confirm that GENRE-CMR surpasses state-of-the-art methods on training and unseen data, achieving 0.9552 SSIM and 38.90 dB PSNR on unseen distributions across various acceleration factors and sampling trajectories. Ablation studies confirm the contribution of each proposed component to reconstruction quality and generalization. Our framework presents a unified and robust solution for high-quality CMR reconstruction, paving the way for clinically adaptable deployment across heterogeneous acquisition protocols.

</details>


### [29] [Efficient Fine-Tuning of DINOv3 Pretrained on Natural Images for Atypical Mitotic Figure Classification in MIDOG 2025](https://arxiv.org/abs/2508.21041)
*Guillaume Balezo,Raphaël Bourgade,Thomas Walter*

Main category: eess.IV

TL;DR: DINOv3-H+视觉变换器通过LoRA微调和数据增强，在MIDOG 2025挑战中实现了0.8871的平衡准确率，证明了其在组织病理学图像中检测非典型有丝分裂的有效性。


<details>
  <summary>Details</summary>
Motivation: 非典型有丝分裂图像(AMFs)是预后不良的重要标志物，但由于出现频率低、形态细微且观察者间差异大，检测难度很高。MIDOG 2025挑战旨在建立跨领域的AMF分类基准。

Method: 使用在自然图像上预训练的DINOv3-H+视觉变换器，通过低秩适应(LoRA)进行参数高效微调(仅65万可训练参数)，并采用广泛的数据增强技术。

Result: 尽管存在领域差异，DINOv3在组织病理学图像上表现出良好的迁移能力，在初步测试集上达到了0.8871的平衡准确率。

Conclusion: DINOv3预训练模型具有强大的鲁棒性，结合参数高效微调方法，为MIDOG 2025的非典型有丝分裂分类提供了强有力的基线方案。

Abstract: Atypical mitotic figures (AMFs) are markers of abnormal cell division associated with poor prognosis, yet their detection remains difficult due to low prevalence, subtle morphology, and inter-observer variability. The MIDOG 2025 challenge introduces a benchmark for AMF classification across multiple domains. In this work, we evaluate the recently published DINOv3-H+ vision transformer, pretrained on natural images, which we fine-tuned using low-rank adaptation (LoRA, 650k trainable parameters) and extensive augmentation. Despite the domain gap, DINOv3 transfers effectively to histopathology, achieving a balanced accuracy of 0.8871 on the preliminary test set. These results highlight the robustness of DINOv3 pretraining and show that, when combined with parameter-efficient fine-tuning, it provides a strong baseline for atypical mitosis classification in MIDOG 2025.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [30] [Unleashing Uncertainty: Efficient Machine Unlearning for Generative AI](https://arxiv.org/abs/2508.20773)
*Christoforos N. Spartalis,Theodoros Semertzidis,Petros Daras,Efstratios Gavves*

Main category: cs.LG

TL;DR: SAFEMax是一种基于信息论原理的扩散模型机器遗忘方法，通过最大化生成图像的熵，使模型在条件为不允许类别时生成高斯噪声并停止去噪过程。


<details>
  <summary>Details</summary>
Motivation: 为了解决扩散模型中特定类别内容的遗忘问题，同时保持对其他类别的记忆，需要一种高效的机器遗忘方法。

Method: 基于信息论原理，通过最大化生成图像的熵，在早期扩散步骤中控制遗忘与保留的平衡，使模型在条件为不允许类别时生成高斯噪声并停止去噪。

Result: SAFEMax方法表现出有效性，并在效率方面显著优于现有最先进方法。

Conclusion: SAFEMax为扩散模型提供了一种高效且有效的机器遗忘解决方案，通过信息论方法实现了选择性遗忘。

Abstract: We introduce SAFEMax, a novel method for Machine Unlearning in diffusion models. Grounded in information-theoretic principles, SAFEMax maximizes the entropy in generated images, causing the model to generate Gaussian noise when conditioned on impermissible classes by ultimately halting its denoising process. Also, our method controls the balance between forgetting and retention by selectively focusing on the early diffusion steps, where class-specific information is prominent. Our results demonstrate the effectiveness of SAFEMax and highlight its substantial efficiency gains over state-of-the-art methods.

</details>
