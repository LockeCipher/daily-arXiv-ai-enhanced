{"id": "2508.03415", "pdf": "https://arxiv.org/pdf/2508.03415", "abs": "https://arxiv.org/abs/2508.03415", "authors": ["Shivangi Nigam", "Adarsh Prasad Behera", "Shekhar Verma", "P. Nagabhushan"], "title": "Learning Latent Representations for Image Translation using Frequency Distributed CycleGAN", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "This paper is currently under review for publication in an IEEE   Transactions. If accepted, the copyright will be transferred to IEEE", "summary": "This paper presents Fd-CycleGAN, an image-to-image (I2I) translation framework that enhances latent representation learning to approximate real data distributions. Building upon the foundation of CycleGAN, our approach integrates Local Neighborhood Encoding (LNE) and frequency-aware supervision to capture fine-grained local pixel semantics while preserving structural coherence from the source domain. We employ distribution-based loss metrics, including KL/JS divergence and log-based similarity measures, to explicitly quantify the alignment between real and generated image distributions in both spatial and frequency domains. To validate the efficacy of Fd-CycleGAN, we conduct experiments on diverse datasets -- Horse2Zebra, Monet2Photo, and a synthetically augmented Strike-off dataset. Compared to baseline CycleGAN and other state-of-the-art methods, our approach demonstrates superior perceptual quality, faster convergence, and improved mode diversity, particularly in low-data regimes. By effectively capturing local and global distribution characteristics, Fd-CycleGAN achieves more visually coherent and semantically consistent translations. Our results suggest that frequency-guided latent learning significantly improves generalization in image translation tasks, with promising applications in document restoration, artistic style transfer, and medical image synthesis. We also provide comparative insights with diffusion-based generative models, highlighting the advantages of our lightweight adversarial approach in terms of training efficiency and qualitative output.", "AI": {"tldr": "Fd-CycleGAN\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u56fe\u50cf\u5230\u56fe\u50cf\u7ffb\u8bd1\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u6f5c\u5728\u8868\u793a\u5b66\u4e60\u548c\u9891\u7387\u611f\u77e5\u76d1\u7763\uff0c\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002", "motivation": "\u6539\u8fdbCycleGAN\u5728\u4f4e\u6570\u636e\u91cf\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\uff0c\u540c\u65f6\u66f4\u597d\u5730\u6355\u6349\u5c40\u90e8\u548c\u5168\u5c40\u6570\u636e\u5206\u5e03\u7279\u5f81\u3002", "method": "\u7ed3\u5408\u5c40\u90e8\u90bb\u57df\u7f16\u7801\uff08LNE\uff09\u548c\u9891\u7387\u611f\u77e5\u76d1\u7763\uff0c\u4f7f\u7528KL/JS\u6563\u5ea6\u548c\u57fa\u4e8e\u5bf9\u6570\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\u6765\u91cf\u5316\u771f\u5b9e\u4e0e\u751f\u6210\u56fe\u50cf\u7684\u5206\u5e03\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebfCycleGAN\u548c\u5176\u4ed6\u5148\u8fdb\u65b9\u6cd5\uff0c\u751f\u6210\u56fe\u50cf\u5177\u6709\u66f4\u9ad8\u7684\u611f\u77e5\u8d28\u91cf\u3001\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6a21\u5f0f\u591a\u6837\u6027\u3002", "conclusion": "Fd-CycleGAN\u901a\u8fc7\u9891\u7387\u5f15\u5bfc\u7684\u6f5c\u5728\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u7ffb\u8bd1\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u6587\u6863\u4fee\u590d\u3001\u827a\u672f\u98ce\u683c\u8fc1\u79fb\u548c\u533b\u5b66\u56fe\u50cf\u5408\u6210\u7b49\u9886\u57df\u3002"}}
{"id": "2508.02807", "pdf": "https://arxiv.org/pdf/2508.02807", "abs": "https://arxiv.org/abs/2508.02807", "authors": ["Tongchun Zuo", "Zaiyu Huang", "Shuliang Ning", "Ente Lin", "Chao Liang", "Zerong Zheng", "Jianwen Jiang", "Yuan Zhang", "Mingyuan Gao", "Xin Dong"], "title": "DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a Stage-Wise Diffusion Transformer Framework", "categories": ["cs.CV"], "comment": "18 pages, 12 figures", "summary": "Video virtual try-on (VVT) technology has garnered considerable academic interest owing to its promising applications in e-commerce advertising and entertainment. However, most existing end-to-end methods rely heavily on scarce paired garment-centric datasets and fail to effectively leverage priors of advanced visual models and test-time inputs, making it challenging to accurately preserve fine-grained garment details and maintain temporal consistency in unconstrained scenarios. To address these challenges, we propose DreamVVT, a carefully designed two-stage framework built upon Diffusion Transformers (DiTs), which is inherently capable of leveraging diverse unpaired human-centric data to enhance adaptability in real-world scenarios. To further leverage prior knowledge from pretrained models and test-time inputs, in the first stage, we sample representative frames from the input video and utilize a multi-frame try-on model integrated with a vision-language model (VLM), to synthesize high-fidelity and semantically consistent keyframe try-on images. These images serve as complementary appearance guidance for subsequent video generation. \\textbf{In the second stage}, skeleton maps together with fine-grained motion and appearance descriptions are extracted from the input content, and these along with the keyframe try-on images are then fed into a pretrained video generation model enhanced with LoRA adapters. This ensures long-term temporal coherence for unseen regions and enables highly plausible dynamic motions. Extensive quantitative and qualitative experiments demonstrate that DreamVVT surpasses existing methods in preserving detailed garment content and temporal stability in real-world scenarios. Our project page https://virtu-lab.github.io/", "AI": {"tldr": "DreamVVT\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\uff08DiTs\uff09\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u89c6\u9891\u865a\u62df\u8bd5\u7a7f\uff08VVT\uff09\u4e2d\u7684\u7ec6\u7c92\u5ea6\u7ec6\u8282\u4fdd\u7559\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7a00\u7f3a\u7684\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u4e14\u672a\u80fd\u5145\u5206\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u6d4b\u8bd5\u8f93\u5165\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u5bfc\u81f4\u5728\u65e0\u7ea6\u675f\u573a\u666f\u4e2d\u96be\u4ee5\u4fdd\u6301\u7ec6\u8282\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\u4ece\u8f93\u5165\u89c6\u9891\u4e2d\u91c7\u6837\u5173\u952e\u5e27\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u751f\u6210\u9ad8\u4fdd\u771f\u8bd5\u7a7f\u56fe\u50cf\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5229\u7528\u9aa8\u67b6\u56fe\u548c\u52a8\u6001\u63cf\u8ff0\uff0c\u901a\u8fc7\u589e\u5f3a\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u7684\u89c6\u9891\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDreamVVT\u5728\u7ec6\u8282\u4fdd\u7559\u548c\u65f6\u95f4\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DreamVVT\u901a\u8fc7\u4e24\u9636\u6bb5\u8bbe\u8ba1\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6709\u6548\u5229\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u865a\u62df\u8bd5\u7a7f\u7684\u6027\u80fd\u3002"}}
{"id": "2508.02831", "pdf": "https://arxiv.org/pdf/2508.02831", "abs": "https://arxiv.org/abs/2508.02831", "authors": ["Miko\u0142aj Zieli\u0144ski", "Krzysztof Byrski", "Tomasz Szczepanik", "Przemys\u0142aw Spurek"], "title": "GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing", "categories": ["cs.CV"], "comment": null, "summary": "Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have recently transformed 3D scene representation and rendering. NeRF achieves high-fidelity novel view synthesis by learning volumetric representations through neural networks, but its implicit encoding makes editing and physical interaction challenging. In contrast, GS represents scenes as explicit collections of Gaussian primitives, enabling real-time rendering, faster training, and more intuitive manipulation. This explicit structure has made GS particularly well-suited for interactive editing and integration with physics-based simulation. In this paper, we introduce GENIE (Gaussian Encoding for Neural Radiance Fields Interactive Editing), a hybrid model that combines the photorealistic rendering quality of NeRF with the editable and structured representation of GS. Instead of using spherical harmonics for appearance modeling, we assign each Gaussian a trainable feature embedding. These embeddings are used to condition a NeRF network based on the k nearest Gaussians to each query point. To make this conditioning efficient, we introduce Ray-Traced Gaussian Proximity Search (RT-GPS), a fast nearest Gaussian search based on a modified ray-tracing pipeline. We also integrate a multi-resolution hash grid to initialize and update Gaussian features. Together, these components enable real-time, locality-aware editing: as Gaussian primitives are repositioned or modified, their interpolated influence is immediately reflected in the rendered output. By combining the strengths of implicit and explicit representations, GENIE supports intuitive scene manipulation, dynamic interaction, and compatibility with physical simulation, bridging the gap between geometry-based editing and neural rendering. The code can be found under (https://github.com/MikolajZielinski/genie)", "AI": {"tldr": "GENIE\u7ed3\u5408\u4e86NeRF\u7684\u9ad8\u4fdd\u771f\u6e32\u67d3\u548cGS\u7684\u53ef\u7f16\u8f91\u6027\uff0c\u901a\u8fc7\u9ad8\u65af\u7279\u5f81\u5d4c\u5165\u548c\u9ad8\u6548\u641c\u7d22\u5b9e\u73b0\u5b9e\u65f6\u7f16\u8f91\u3002", "motivation": "\u89e3\u51b3NeRF\u96be\u4ee5\u7f16\u8f91\u548cGS\u6e32\u67d3\u8d28\u91cf\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u7279\u5f81\u5d4c\u5165\u548cRT-GPS\u641c\u7d22\uff0c\u7ed3\u5408\u591a\u5206\u8fa8\u7387\u54c8\u5e0c\u7f51\u683c\u3002", "result": "\u5b9e\u73b0\u4e86\u5b9e\u65f6\u7f16\u8f91\u548c\u9ad8\u4fdd\u771f\u6e32\u67d3\u7684\u5e73\u8861\u3002", "conclusion": "GENIE\u5728\u53ef\u7f16\u8f91\u6027\u548c\u6e32\u67d3\u8d28\u91cf\u4e0a\u53d6\u5f97\u4e86\u7a81\u7834\u3002"}}
{"id": "2508.02903", "pdf": "https://arxiv.org/pdf/2508.02903", "abs": "https://arxiv.org/abs/2508.02903", "authors": ["Mehrdad Moradi", "Kamran Paynabar"], "title": "RDDPM: Robust Denoising Diffusion Probabilistic Model for Unsupervised Anomaly Segmentation", "categories": ["cs.CV", "68T07", "I.4.9; I.2.10"], "comment": "10 pages, 5 figures. Accepted to the ICCV 2025 Workshop on   Vision-based Industrial InspectiON (VISION)", "summary": "Recent advancements in diffusion models have demonstrated significant success in unsupervised anomaly segmentation. For anomaly segmentation, these models are first trained on normal data; then, an anomalous image is noised to an intermediate step, and the normal image is reconstructed through backward diffusion. Unlike traditional statistical methods, diffusion models do not rely on specific assumptions about the data or target anomalies, making them versatile for use across different domains. However, diffusion models typically assume access to normal data for training, limiting their applicability in realistic settings. In this paper, we propose novel robust denoising diffusion models for scenarios where only contaminated (i.e., a mix of normal and anomalous) unlabeled data is available. By casting maximum likelihood estimation of the data as a nonlinear regression problem, we reinterpret the denoising diffusion probabilistic model through a regression lens. Using robust regression, we derive a robust version of denoising diffusion probabilistic models. Our novel framework offers flexibility in constructing various robust diffusion models. Our experiments show that our approach outperforms current state of the art diffusion models, for unsupervised anomaly segmentation when only contaminated data is available. Our method outperforms existing diffusion-based approaches, achieving up to 8.08\\% higher AUROC and 10.37\\% higher AUPRC on MVTec datasets. The implementation code is available at: https://github.com/mehrdadmoradi124/RDDPM", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u964d\u566a\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u4ec5\u6709\u6c61\u67d3\uff08\u6df7\u5408\u6b63\u5e38\u548c\u5f02\u5e38\uff09\u672a\u6807\u8bb0\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u65e0\u76d1\u7763\u5f02\u5e38\u5206\u5272\u3002", "motivation": "\u4f20\u7edf\u6269\u6563\u6a21\u578b\u4f9d\u8d56\u6b63\u5e38\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c06\u6570\u636e\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u8f6c\u5316\u4e3a\u975e\u7ebf\u6027\u56de\u5f52\u95ee\u9898\uff0c\u63d0\u51fa\u9c81\u68d2\u56de\u5f52\u89c6\u89d2\u4e0b\u7684\u964d\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6c61\u67d3\u6570\u636e\u4e0b\u4f18\u4e8e\u73b0\u6709\u6269\u6563\u6a21\u578b\uff0cAUROC\u548cAUPRC\u5206\u522b\u63d0\u53478.08%\u548c10.37%\u3002", "conclusion": "\u63d0\u51fa\u7684\u9c81\u68d2\u6269\u6563\u6a21\u578b\u6846\u67b6\u7075\u6d3b\u4e14\u9ad8\u6548\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5f02\u5e38\u5206\u5272\u4efb\u52a1\u3002"}}
{"id": "2508.02923", "pdf": "https://arxiv.org/pdf/2508.02923", "abs": "https://arxiv.org/abs/2508.02923", "authors": ["Minh-Hai Nguyen", "Edouard Pauwels", "Pierre Weiss"], "title": "How Diffusion Prior Landscapes Shape the Posterior in Blind Deconvolution", "categories": ["cs.CV"], "comment": null, "summary": "The Maximum A Posteriori (MAP) estimation is a widely used framework in blind deconvolution to recover sharp images from blurred observations. The estimated image and blur filter are defined as the maximizer of the posterior distribution. However, when paired with sparsity-promoting image priors, MAP estimation has been shown to favors blurry solutions, limiting its effectiveness. In this paper, we revisit this result using diffusion-based priors, a class of models that capture realistic image distributions. Through an empirical examination of the prior's likelihood landscape, we uncover two key properties: first, blurry images tend to have higher likelihoods; second, the landscape contains numerous local minimizers that correspond to natural images. Building on these insights, we provide a theoretical analysis of the blind deblurring posterior. This reveals that the MAP estimator tends to produce sharp filters (close to the Dirac delta function) and blurry solutions. However local minimizers of the posterior, which can be obtained with gradient descent, correspond to realistic, natural images, effectively solving the blind deconvolution problem. Our findings suggest that overcoming MAP's limitations requires good local initialization to local minima in the posterior landscape. We validate our analysis with numerical experiments, demonstrating the practical implications of our insights for designing improved priors and optimization techniques.", "AI": {"tldr": "MAP\u4f30\u8ba1\u5728\u76f2\u53bb\u5377\u79ef\u4e2d\u503e\u5411\u4e8e\u6a21\u7cca\u89e3\uff0c\u4f46\u6269\u6563\u5148\u9a8c\u63ed\u793a\u4e86\u5c40\u90e8\u6781\u5c0f\u503c\u5bf9\u5e94\u81ea\u7136\u56fe\u50cf\uff0c\u9700\u826f\u597d\u521d\u59cb\u5316\u4ee5\u89e3\u51b3\u76f2\u53bb\u5377\u79ef\u95ee\u9898\u3002", "motivation": "\u7814\u7a76MAP\u4f30\u8ba1\u5728\u76f2\u53bb\u5377\u79ef\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63a2\u7d22\u6269\u6563\u5148\u9a8c\u5982\u4f55\u63ed\u793a\u66f4\u4f18\u89e3\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6269\u6563\u5148\u9a8c\u7684\u4f3c\u7136\u666f\u89c2\uff0c\u53d1\u73b0\u6a21\u7cca\u56fe\u50cf\u4f3c\u7136\u66f4\u9ad8\uff0c\u5c40\u90e8\u6781\u5c0f\u503c\u5bf9\u5e94\u81ea\u7136\u56fe\u50cf\u3002\u7406\u8bba\u5206\u6790\u76f2\u53bb\u6a21\u7cca\u540e\u9a8c\uff0c\u9a8c\u8bc1\u68af\u5ea6\u4e0b\u964d\u53ef\u627e\u5230\u5b9e\u9645\u89e3\u3002", "result": "MAP\u4f30\u8ba1\u503e\u5411\u4e8e\u4ea7\u751f\u6a21\u7cca\u89e3\uff0c\u4f46\u5c40\u90e8\u6781\u5c0f\u503c\u5bf9\u5e94\u81ea\u7136\u56fe\u50cf\uff0c\u9700\u826f\u597d\u521d\u59cb\u5316\u4ee5\u4f18\u5316\u7ed3\u679c\u3002", "conclusion": "\u6269\u6563\u5148\u9a8c\u548c\u5c40\u90e8\u521d\u59cb\u5316\u53ef\u514b\u670dMAP\u9650\u5236\uff0c\u4e3a\u76f2\u53bb\u5377\u79ef\u63d0\u4f9b\u66f4\u4f18\u89e3\u3002"}}
{"id": "2508.02944", "pdf": "https://arxiv.org/pdf/2508.02944", "abs": "https://arxiv.org/abs/2508.02944", "authors": ["Chenxu Zhang", "Zenan Li", "Hongyi Xu", "You Xie", "Xiaochen Zhao", "Tianpei Gu", "Guoxian Song", "Xin Chen", "Chao Liang", "Jianwen Jiang", "Linjie Luo"], "title": "X-Actor: Emotional and Expressive Long-Range Portrait Acting from Audio", "categories": ["cs.CV"], "comment": "Project Page at https://byteaigc.github.io/X-Actor/", "summary": "We present X-Actor, a novel audio-driven portrait animation framework that generates lifelike, emotionally expressive talking head videos from a single reference image and an input audio clip. Unlike prior methods that emphasize lip synchronization and short-range visual fidelity in constrained speaking scenarios, X-Actor enables actor-quality, long-form portrait performance capturing nuanced, dynamically evolving emotions that flow coherently with the rhythm and content of speech. Central to our approach is a two-stage decoupled generation pipeline: an audio-conditioned autoregressive diffusion model that predicts expressive yet identity-agnostic facial motion latent tokens within a long temporal context window, followed by a diffusion-based video synthesis module that translates these motions into high-fidelity video animations. By operating in a compact facial motion latent space decoupled from visual and identity cues, our autoregressive diffusion model effectively captures long-range correlations between audio and facial dynamics through a diffusion-forcing training paradigm, enabling infinite-length emotionally-rich motion prediction without error accumulation. Extensive experiments demonstrate that X-Actor produces compelling, cinematic-style performances that go beyond standard talking head animations and achieves state-of-the-art results in long-range, audio-driven emotional portrait acting.", "AI": {"tldr": "X-Actor\u662f\u4e00\u4e2a\u97f3\u9891\u9a71\u52a8\u7684\u8096\u50cf\u52a8\u753b\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u5f20\u53c2\u8003\u56fe\u50cf\u548c\u97f3\u9891\u751f\u6210\u903c\u771f\u3001\u60c5\u611f\u4e30\u5bcc\u7684\u8bf4\u8bdd\u5934\u90e8\u89c6\u9891\u3002\u5176\u6838\u5fc3\u662f\u4e24\u9636\u6bb5\u89e3\u8026\u751f\u6210\u6d41\u7a0b\uff0c\u7ed3\u5408\u81ea\u56de\u5f52\u6269\u6563\u6a21\u578b\u548c\u89c6\u9891\u5408\u6210\u6a21\u5757\uff0c\u5b9e\u73b0\u957f\u65f6\u60c5\u611f\u8fde\u8d2f\u7684\u52a8\u753b\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u5173\u6ce8\u5507\u90e8\u540c\u6b65\u548c\u77ed\u65f6\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u800cX-Actor\u65e8\u5728\u751f\u6210\u5177\u6709\u52a8\u6001\u60c5\u611f\u6f14\u53d8\u7684\u6f14\u5458\u7ea7\u957f\u65f6\u8096\u50cf\u8868\u6f14\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u89e3\u8026\u751f\u6210\u6d41\u7a0b\uff1a1\uff09\u97f3\u9891\u6761\u4ef6\u81ea\u56de\u5f52\u6269\u6563\u6a21\u578b\u9884\u6d4b\u9762\u90e8\u8fd0\u52a8\u6f5c\u5728\u6807\u8bb0\uff1b2\uff09\u6269\u6563\u89c6\u9891\u5408\u6210\u6a21\u5757\u5c06\u5176\u8f6c\u5316\u4e3a\u9ad8\u4fdd\u771f\u89c6\u9891\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cX-Actor\u80fd\u751f\u6210\u8d85\u8d8a\u6807\u51c6\u8bf4\u8bdd\u5934\u90e8\u52a8\u753b\u7684\u5f71\u9662\u7ea7\u8868\u6f14\uff0c\u5e76\u5728\u957f\u65f6\u97f3\u9891\u9a71\u52a8\u60c5\u611f\u8096\u50cf\u8868\u6f14\u4e2d\u8fbe\u5230\u6700\u4f18\u6548\u679c\u3002", "conclusion": "X-Actor\u901a\u8fc7\u89e3\u8026\u8bbe\u8ba1\u548c\u6269\u6563\u8bad\u7ec3\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u957f\u65f6\u60c5\u611f\u8fde\u8d2f\u7684\u9ad8\u8d28\u91cf\u8096\u50cf\u52a8\u753b\uff0c\u4e3a\u97f3\u9891\u9a71\u52a8\u8868\u6f14\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.02967", "pdf": "https://arxiv.org/pdf/2508.02967", "abs": "https://arxiv.org/abs/2508.02967", "authors": ["Dawei Zhang", "Xiaojie Guo"], "title": "Towards Robust Image Denoising with Scale Equivariance", "categories": ["cs.CV"], "comment": null, "summary": "Despite notable advances in image denoising, existing models often struggle to generalize beyond in-distribution noise patterns, particularly when confronted with out-of-distribution (OOD) conditions characterized by spatially variant noise. This generalization gap remains a fundamental yet underexplored challenge. In this work, we investigate \\emph{scale equivariance} as a core inductive bias for improving OOD robustness. We argue that incorporating scale-equivariant structures enables models to better adapt from training on spatially uniform noise to inference on spatially non-uniform degradations. Building on this insight, we propose a robust blind denoising framework equipped with two key components: a Heterogeneous Normalization Module (HNM) and an Interactive Gating Module (IGM). HNM stabilizes feature distributions and dynamically corrects features under varying noise intensities, while IGM facilitates effective information modulation via gated interactions between signal and feature paths. Extensive evaluations demonstrate that our model consistently outperforms state-of-the-art methods on both synthetic and real-world benchmarks, especially under spatially heterogeneous noise. Code will be made publicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c3a\u5ea6\u7b49\u53d8\u6027\u7684\u76f2\u53bb\u566a\u6846\u67b6\uff0c\u901a\u8fc7HNM\u548cIGM\u6a21\u5757\u63d0\u5347\u6a21\u578b\u5728\u7a7a\u95f4\u975e\u5747\u5300\u566a\u58f0\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u53bb\u566a\u6a21\u578b\u5728\u7a7a\u95f4\u975e\u5747\u5300\u566a\u58f0\uff08OOD\u6761\u4ef6\uff09\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542bHNM\uff08\u7a33\u5b9a\u7279\u5f81\u5206\u5e03\uff09\u548cIGM\uff08\u4fe1\u606f\u8c03\u5236\uff09\u7684\u5c3a\u5ea6\u7b49\u53d8\u6846\u67b6\u3002", "result": "\u6a21\u578b\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u7a7a\u95f4\u5f02\u8d28\u566a\u58f0\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u5c3a\u5ea6\u7b49\u53d8\u6027\u662f\u63d0\u5347\u53bb\u566a\u6a21\u578bOOD\u9c81\u68d2\u6027\u7684\u6709\u6548\u5f52\u7eb3\u504f\u7f6e\uff0cHNM\u548cIGM\u6a21\u5757\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2508.02973", "pdf": "https://arxiv.org/pdf/2508.02973", "abs": "https://arxiv.org/abs/2508.02973", "authors": ["Alakh Desai", "Nuno Vasconcelos"], "title": "Diffusion Models with Adaptive Negative Sampling Without External Resources", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models (DMs) have demonstrated an unparalleled ability to create diverse and high-fidelity images from text prompts. However, they are also well-known to vary substantially regarding both prompt adherence and quality. Negative prompting was introduced to improve prompt compliance by specifying what an image must not contain. Previous works have shown the existence of an ideal negative prompt that can maximize the odds of the positive prompt. In this work, we explore relations between negative prompting and classifier-free guidance (CFG) to develop a sampling procedure, {\\it Adaptive Negative Sampling Without External Resources} (ANSWER), that accounts for both positive and negative conditions from a single prompt. This leverages the internal understanding of negation by the diffusion model to increase the odds of generating images faithful to the prompt. ANSWER is a training-free technique, applicable to any model that supports CFG, and allows for negative grounding of image concepts without an explicit negative prompts, which are lossy and incomplete. Experiments show that adding ANSWER to existing DMs outperforms the baselines on multiple benchmarks and is preferred by humans 2x more over the other methods.", "AI": {"tldr": "ANSWER\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u8d1f\u63d0\u793a\u548c\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\uff08CFG\uff09\uff0c\u63d0\u9ad8\u6269\u6563\u6a21\u578b\u751f\u6210\u56fe\u50cf\u4e0e\u6587\u672c\u63d0\u793a\u7684\u5339\u914d\u5ea6\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u65f6\uff0c\u5bf9\u63d0\u793a\u7684\u9075\u5faa\u7a0b\u5ea6\u548c\u8d28\u91cf\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u8d1f\u63d0\u793a\u88ab\u7528\u4e8e\u6539\u5584\u63d0\u793a\u9075\u5faa\u6027\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u8d44\u6e90\u4e14\u4e0d\u5b8c\u6574\u3002", "method": "\u63d0\u51faANSWER\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u5185\u90e8\u5bf9\u5426\u5b9a\u7684\u7406\u89e3\uff0c\u7ed3\u5408CFG\uff0c\u65e0\u9700\u663e\u5f0f\u8d1f\u63d0\u793a\u5373\u53ef\u5b9e\u73b0\u8d1f\u6761\u4ef6\u91c7\u6837\u3002", "result": "ANSWER\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4eba\u7c7b\u8bc4\u4f30\u663e\u793a\u5176\u53d7\u6b22\u8fce\u7a0b\u5ea6\u662f\u5176\u4ed6\u65b9\u6cd5\u76842\u500d\u3002", "conclusion": "ANSWER\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u6280\u672f\uff0c\u9002\u7528\u4e8e\u652f\u6301CFG\u7684\u4efb\u4f55\u6269\u6563\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u4e0e\u63d0\u793a\u7684\u5339\u914d\u5ea6\u3002"}}
{"id": "2508.02978", "pdf": "https://arxiv.org/pdf/2508.02978", "abs": "https://arxiv.org/abs/2508.02978", "authors": ["Yusaku Takama", "Ning Ding", "Tatsuya Yokota", "Toru Tamaki"], "title": "Separating Shared and Domain-Specific LoRAs for Multi-Domain Learning", "categories": ["cs.CV"], "comment": "9 pages", "summary": "Existing architectures of multi-domain learning have two types of adapters: shared LoRA for all domains and domain-specific LoRA for each particular domain. However, it remains unclear whether this structure effectively captures domain-specific information. In this paper, we propose a method that ensures that shared and domain-specific LoRAs exist in different subspaces; specifically, the column and left null subspaces of the pre-trained weights. We apply the proposed method to action recognition with three datasets (UCF101, Kinetics400, and HMDB51) and demonstrate its effectiveness in some cases along with the analysis of the dimensions of LoRA weights.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u786e\u4fdd\u5171\u4eab\u548c\u9886\u57df\u7279\u5b9a\u7684LoRAs\u5b58\u5728\u4e8e\u4e0d\u540c\u7684\u5b50\u7a7a\u95f4\u4e2d\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u83b7\u9886\u57df\u7279\u5b9a\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u591a\u9886\u57df\u5b66\u4e60\u67b6\u6784\u4e2d\uff0c\u5171\u4eab\u548c\u9886\u57df\u7279\u5b9aLoRA\u7684\u7ed3\u6784\u662f\u5426\u6709\u6548\u6355\u83b7\u9886\u57df\u4fe1\u606f\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u5c06\u5171\u4eab\u548c\u9886\u57df\u7279\u5b9aLoRAs\u5206\u522b\u7f6e\u4e8e\u9884\u8bad\u7ec3\u6743\u91cd\u7684\u5217\u548c\u5de6\u96f6\u5b50\u7a7a\u95f4\u4e2d\u3002", "result": "\u5728\u4e09\u4e2a\u52a8\u4f5c\u8bc6\u522b\u6570\u636e\u96c6\uff08UCF101\u3001Kinetics400\u548cHMDB51\uff09\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5206\u6790\u4e86LoRA\u6743\u91cd\u7684\u7ef4\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u6709\u6548\uff0c\u5e76\u901a\u8fc7LoRA\u6743\u91cd\u7ef4\u5ea6\u5206\u6790\u63d0\u4f9b\u4e86\u8fdb\u4e00\u6b65\u89c1\u89e3\u3002"}}
{"id": "2508.03006", "pdf": "https://arxiv.org/pdf/2508.03006", "abs": "https://arxiv.org/abs/2508.03006", "authors": ["Fan Yang", "Yihao Huang", "Jiayi Zhu", "Ling Shi", "Geguang Pu", "Jin Song Dong", "Kailong Wang"], "title": "Seeing It Before It Happens: In-Generation NSFW Detection for Diffusion-Based Text-to-Image Models", "categories": ["cs.CV"], "comment": "8 pages", "summary": "Diffusion-based text-to-image (T2I) models enable high-quality image generation but also pose significant risks of misuse, particularly in producing not-safe-for-work (NSFW) content. While prior detection methods have focused on filtering prompts before generation or moderating images afterward, the in-generation phase of diffusion models remains largely unexplored for NSFW detection. In this paper, we introduce In-Generation Detection (IGD), a simple yet effective approach that leverages the predicted noise during the diffusion process as an internal signal to identify NSFW content. This approach is motivated by preliminary findings suggesting that the predicted noise may capture semantic cues that differentiate NSFW from benign prompts, even when the prompts are adversarially crafted. Experiments conducted on seven NSFW categories show that IGD achieves an average detection accuracy of 91.32% over naive and adversarial NSFW prompts, outperforming seven baseline methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86In-Generation Detection (IGD)\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u9884\u6d4b\u566a\u58f0\u68c0\u6d4bNSFW\u5185\u5bb9\uff0c\u51c6\u786e\u7387\u8fbe91.32%\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684\u540c\u65f6\u53ef\u80fd\u88ab\u6ee5\u7528\u751f\u6210NSFW\u5185\u5bb9\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u751f\u6210\u524d\u6216\u751f\u6210\u540e\u7684\u68c0\u6d4b\uff0c\u800c\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u68c0\u6d4b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5229\u7528\u6269\u6563\u8fc7\u7a0b\u4e2d\u7684\u9884\u6d4b\u566a\u58f0\u4f5c\u4e3a\u5185\u90e8\u4fe1\u53f7\uff0c\u533a\u5206NSFW\u4e0e\u826f\u6027\u63d0\u793a\u3002", "result": "\u5728\u4e03\u79cdNSFW\u7c7b\u522b\u4e0a\uff0cIGD\u7684\u5e73\u5747\u68c0\u6d4b\u51c6\u786e\u7387\u4e3a91.32%\uff0c\u4f18\u4e8e\u4e03\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "IGD\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u6269\u6563\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u4e2dNSFW\u68c0\u6d4b\u7684\u7a7a\u767d\u3002"}}
{"id": "2508.03017", "pdf": "https://arxiv.org/pdf/2508.03017", "abs": "https://arxiv.org/abs/2508.03017", "authors": ["Liheng Zhang", "Weihao Yu", "Zubo Lu", "Haozhi Gu", "Jin Huang"], "title": "SA-3DGS: A Self-Adaptive Compression Method for 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "9 pages, 7 figures. Under review at AAAI 2026", "summary": "Recent advancements in 3D Gaussian Splatting have enhanced efficient and high-quality novel view synthesis. However, representing scenes requires a large number of Gaussian points, leading to high storage demands and limiting practical deployment. The latest methods facilitate the compression of Gaussian models but struggle to identify truly insignificant Gaussian points in the scene, leading to a decline in subsequent Gaussian pruning, compression quality, and rendering performance. To address this issue, we propose SA-3DGS, a method that significantly reduces storage costs while maintaining rendering quality. SA-3DGS learns an importance score to automatically identify the least significant Gaussians in scene reconstruction, thereby enabling effective pruning and redundancy reduction. Next, the importance-aware clustering module compresses Gaussians attributes more accurately into the codebook, improving the codebook's expressive capability while reducing model size. Finally, the codebook repair module leverages contextual scene information to repair the codebook, thereby recovering the original Gaussian point attributes and mitigating the degradation in rendering quality caused by information loss. Experimental results on several benchmark datasets show that our method achieves up to 66x compression while maintaining or even improving rendering quality. The proposed Gaussian pruning approach is not only adaptable to but also improves other pruning-based methods (e.g., LightGaussian), showcasing excellent performance and strong generalization ability.", "AI": {"tldr": "SA-3DGS\u901a\u8fc7\u91cd\u8981\u6027\u8bc4\u5206\u548c\u805a\u7c7b\u538b\u7f29\uff0c\u663e\u8457\u51cf\u5c113D\u9ad8\u65af\u6a21\u578b\u7684\u5b58\u50a8\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u8bc6\u522b\u573a\u666f\u4e2d\u771f\u6b63\u4e0d\u91cd\u8981\u7684\u9ad8\u65af\u70b9\uff0c\u5bfc\u81f4\u538b\u7f29\u548c\u6e32\u67d3\u6027\u80fd\u4e0b\u964d\u3002", "method": "SA-3DGS\u901a\u8fc7\u5b66\u4e60\u91cd\u8981\u6027\u8bc4\u5206\u8bc6\u522b\u4e0d\u91cd\u8981\u9ad8\u65af\u70b9\uff0c\u7ed3\u5408\u805a\u7c7b\u548c\u4fee\u590d\u6a21\u5757\u4f18\u5316\u538b\u7f29\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u9ad8\u8fbe66\u500d\u538b\u7f29\uff0c\u4e14\u6e32\u67d3\u8d28\u91cf\u4e0d\u964d\u53cd\u5347\u3002", "conclusion": "SA-3DGS\u5728\u538b\u7f29\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u5176\u4ed6\u57fa\u4e8e\u526a\u679d\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.03034", "pdf": "https://arxiv.org/pdf/2508.03034", "abs": "https://arxiv.org/abs/2508.03034", "authors": ["Qi Xie", "Yongjia Ma", "Donglin Di", "Xuehao Gao", "Xun Yang"], "title": "MoCA: Identity-Preserving Text-to-Video Generation via Mixture of Cross Attention", "categories": ["cs.CV"], "comment": null, "summary": "Achieving ID-preserving text-to-video (T2V) generation remains challenging despite recent advances in diffusion-based models. Existing approaches often fail to capture fine-grained facial dynamics or maintain temporal identity coherence. To address these limitations, we propose MoCA, a novel Video Diffusion Model built on a Diffusion Transformer (DiT) backbone, incorporating a Mixture of Cross-Attention mechanism inspired by the Mixture-of-Experts paradigm. Our framework improves inter-frame identity consistency by embedding MoCA layers into each DiT block, where Hierarchical Temporal Pooling captures identity features over varying timescales, and Temporal-Aware Cross-Attention Experts dynamically model spatiotemporal relationships. We further incorporate a Latent Video Perceptual Loss to enhance identity coherence and fine-grained details across video frames. To train this model, we collect CelebIPVid, a dataset of 10,000 high-resolution videos from 1,000 diverse individuals, promoting cross-ethnicity generalization. Extensive experiments on CelebIPVid show that MoCA outperforms existing T2V methods by over 5% across Face similarity.", "AI": {"tldr": "MoCA\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u6df7\u5408\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u6587\u672c\u5230\u89c6\u9891\uff08T2V\uff09\u751f\u6210\u4e2d\u7684\u8eab\u4efd\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u9762\u90e8\u52a8\u6001\u6355\u6349\u548c\u65f6\u5e8f\u8eab\u4efd\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u91c7\u7528DiT\u6846\u67b6\uff0c\u5d4c\u5165MoCA\u5c42\uff0c\u7ed3\u5408\u5206\u5c42\u65f6\u5e8f\u6c60\u5316\u548c\u65f6\u5e8f\u611f\u77e5\u4ea4\u53c9\u6ce8\u610f\u529b\u4e13\u5bb6\uff0c\u5e76\u5f15\u5165\u6f5c\u5728\u89c6\u9891\u611f\u77e5\u635f\u5931\u3002", "result": "\u5728CelebIPVid\u6570\u636e\u96c6\u4e0a\uff0cMoCA\u5728\u9762\u90e8\u76f8\u4f3c\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd55%\u4ee5\u4e0a\u3002", "conclusion": "MoCA\u901a\u8fc7\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u548c\u65f6\u5e8f\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86T2V\u751f\u6210\u4e2d\u7684\u8eab\u4efd\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.03050", "pdf": "https://arxiv.org/pdf/2508.03050", "abs": "https://arxiv.org/abs/2508.03050", "authors": ["Zeyu Zhu", "Weijia Wu", "Mike Zheng Shou"], "title": "Multi-human Interactive Talking Dataset", "categories": ["cs.CV"], "comment": "9 pages, 4 figures, 4 tables", "summary": "Existing studies on talking video generation have predominantly focused on single-person monologues or isolated facial animations, limiting their applicability to realistic multi-human interactions. To bridge this gap, we introduce MIT, a large-scale dataset specifically designed for multi-human talking video generation. To this end, we develop an automatic pipeline that collects and annotates multi-person conversational videos. The resulting dataset comprises 12 hours of high-resolution footage, each featuring two to four speakers, with fine-grained annotations of body poses and speech interactions. It captures natural conversational dynamics in multi-speaker scenario, offering a rich resource for studying interactive visual behaviors. To demonstrate the potential of MIT, we furthur propose CovOG, a baseline model for this novel task. It integrates a Multi-Human Pose Encoder (MPE) to handle varying numbers of speakers by aggregating individual pose embeddings, and an Interactive Audio Driver (IAD) to modulate head dynamics based on speaker-specific audio features. Together, these components showcase the feasibility and challenges of generating realistic multi-human talking videos, establishing MIT as a valuable benchmark for future research. The code is avalibale at: https://github.com/showlab/Multi-human-Talking-Video-Dataset.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86MIT\u6570\u636e\u96c6\u548cCovOG\u6a21\u578b\uff0c\u7528\u4e8e\u591a\u4eba\u7c7b\u5bf9\u8bdd\u89c6\u9891\u751f\u6210\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u4eba\u7c7b\u72ec\u767d\u6216\u5b64\u7acb\u9762\u90e8\u52a8\u753b\uff0c\u7f3a\u4e4f\u5bf9\u591a\u4eba\u7c7b\u4ea4\u4e92\u7684\u9002\u7528\u6027\u3002", "method": "\u5f00\u53d1\u81ea\u52a8\u7ba1\u9053\u6536\u96c6\u548c\u6807\u6ce8\u591a\u4eba\u5bf9\u8bdd\u89c6\u9891\uff0c\u6784\u5efaMIT\u6570\u636e\u96c6\uff1b\u63d0\u51faCovOG\u6a21\u578b\uff0c\u6574\u5408\u591a\u4eba\u7c7b\u59ff\u6001\u7f16\u7801\u5668\u548c\u4ea4\u4e92\u97f3\u9891\u9a71\u52a8\u3002", "result": "MIT\u6570\u636e\u96c6\u5305\u542b12\u5c0f\u65f6\u9ad8\u6e05\u89c6\u9891\uff0c\u6807\u6ce8\u7cbe\u7ec6\uff1bCovOG\u6a21\u578b\u5c55\u793a\u4e86\u751f\u6210\u591a\u4eba\u7c7b\u5bf9\u8bdd\u89c6\u9891\u7684\u53ef\u884c\u6027\u3002", "conclusion": "MIT\u6570\u636e\u96c6\u548cCovOG\u6a21\u578b\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u51c6\uff0c\u89e3\u51b3\u4e86\u591a\u4eba\u7c7b\u4ea4\u4e92\u89c6\u9891\u751f\u6210\u7684\u6311\u6218\u3002"}}
{"id": "2508.03077", "pdf": "https://arxiv.org/pdf/2508.03077", "abs": "https://arxiv.org/abs/2508.03077", "authors": ["Anran Wu", "Long Peng", "Xin Di", "Xueyuan Dai", "Chen Wu", "Yang Wang", "Xueyang Fu", "Yang Cao", "Zheng-Jun Zha"], "title": "RobustGS: Unified Boosting of Feedforward 3D Gaussian Splatting under Low-Quality Conditions", "categories": ["cs.CV"], "comment": null, "summary": "Feedforward 3D Gaussian Splatting (3DGS) overcomes the limitations of optimization-based 3DGS by enabling fast and high-quality reconstruction without the need for per-scene optimization. However, existing feedforward approaches typically assume that input multi-view images are clean and high-quality. In real-world scenarios, images are often captured under challenging conditions such as noise, low light, or rain, resulting in inaccurate geometry and degraded 3D reconstruction. To address these challenges, we propose a general and efficient multi-view feature enhancement module, RobustGS, which substantially improves the robustness of feedforward 3DGS methods under various adverse imaging conditions, enabling high-quality 3D reconstruction. The RobustGS module can be seamlessly integrated into existing pretrained pipelines in a plug-and-play manner to enhance reconstruction robustness. Specifically, we introduce a novel component, Generalized Degradation Learner, designed to extract generic representations and distributions of multiple degradations from multi-view inputs, thereby enhancing degradation-awareness and improving the overall quality of 3D reconstruction. In addition, we propose a novel semantic-aware state-space model. It first leverages the extracted degradation representations to enhance corrupted inputs in the feature space. Then, it employs a semantic-aware strategy to aggregate semantically similar information across different views, enabling the extraction of fine-grained cross-view correspondences and further improving the quality of 3D representations. Extensive experiments demonstrate that our approach, when integrated into existing methods in a plug-and-play manner, consistently achieves state-of-the-art reconstruction quality across various types of degradations.", "AI": {"tldr": "\u63d0\u51faRobustGS\u6a21\u5757\uff0c\u63d0\u53473D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u5728\u6076\u52a3\u6210\u50cf\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf3D\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u524d\u99883DGS\u65b9\u6cd5\u5047\u8bbe\u8f93\u5165\u591a\u89c6\u56fe\u56fe\u50cf\u5e72\u51c0\u9ad8\u8d28\u91cf\uff0c\u4f46\u5b9e\u9645\u573a\u666f\u4e2d\u56fe\u50cf\u5e38\u53d7\u566a\u58f0\u3001\u4f4e\u5149\u6216\u96e8\u7b49\u5f71\u54cd\uff0c\u5bfc\u81f4\u91cd\u5efa\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u5f15\u5165\u901a\u7528\u591a\u89c6\u56fe\u7279\u5f81\u589e\u5f3a\u6a21\u5757RobustGS\uff0c\u5305\u62ec\u5e7f\u4e49\u9000\u5316\u5b66\u4e60\u5668\u548c\u8bed\u4e49\u611f\u77e5\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u4ee5\u589e\u5f3a\u9000\u5316\u611f\u77e5\u548c\u8bed\u4e49\u4fe1\u606f\u805a\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRobustGS\u80fd\u663e\u8457\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u9000\u5316\u7c7b\u578b\u3002", "conclusion": "RobustGS\u6a21\u5757\u53ef\u65e0\u7f1d\u96c6\u6210\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u53473D\u91cd\u5efa\u7684\u9c81\u68d2\u6027\u548c\u8d28\u91cf\u3002"}}
{"id": "2508.03118", "pdf": "https://arxiv.org/pdf/2508.03118", "abs": "https://arxiv.org/abs/2508.03118", "authors": ["Heng Jia", "Linchao Zhu", "Na Zhao"], "title": "H3R: Hybrid Multi-view Correspondence for Generalizable 3D Reconstruction", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "Despite recent advances in feed-forward 3D Gaussian Splatting, generalizable 3D reconstruction remains challenging, particularly in multi-view correspondence modeling. Existing approaches face a fundamental trade-off: explicit methods achieve geometric precision but struggle with ambiguous regions, while implicit methods provide robustness but suffer from slow convergence. We present H3R, a hybrid framework that addresses this limitation by integrating volumetric latent fusion with attention-based feature aggregation. Our framework consists of two complementary components: an efficient latent volume that enforces geometric consistency through epipolar constraints, and a camera-aware Transformer that leverages Pl\\\"ucker coordinates for adaptive correspondence refinement. By integrating both paradigms, our approach enhances generalization while converging 2$\\times$ faster than existing methods. Furthermore, we show that spatial-aligned foundation models (e.g., SD-VAE) substantially outperform semantic-aligned models (e.g., DINOv2), resolving the mismatch between semantic representations and spatial reconstruction requirements. Our method supports variable-number and high-resolution input views while demonstrating robust cross-dataset generalization. Extensive experiments show that our method achieves state-of-the-art performance across multiple benchmarks, with significant PSNR improvements of 0.59 dB, 1.06 dB, and 0.22 dB on the RealEstate10K, ACID, and DTU datasets, respectively. Code is available at https://github.com/JiaHeng-DLUT/H3R.", "AI": {"tldr": "H3R\u662f\u4e00\u79cd\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u4f53\u79ef\u6f5c\u5728\u878d\u5408\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7279\u5f81\u805a\u5408\uff0c\u89e3\u51b3\u4e863D\u91cd\u5efa\u4e2d\u7684\u591a\u89c6\u89d2\u5bf9\u5e94\u5efa\u6a21\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u76843D\u91cd\u5efa\u65b9\u6cd5\u5728\u51e0\u4f55\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0cH3R\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u663e\u5f0f\u548c\u9690\u5f0f\u65b9\u6cd5\u7684\u4f18\u52bf\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "H3R\u7531\u4e24\u90e8\u5206\u7ec4\u6210\uff1a\u4e00\u4e2a\u901a\u8fc7\u6781\u7ebf\u7ea6\u675f\u589e\u5f3a\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u6f5c\u5728\u4f53\u79ef\u6a21\u5757\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5229\u7528Pl\u00fccker\u5750\u6807\u8fdb\u884c\u81ea\u9002\u5e94\u5bf9\u5e94\u4f18\u5316\u7684\u76f8\u673a\u611f\u77e5Transformer\u3002", "result": "H3R\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cPSNR\u5206\u522b\u63d0\u9ad8\u4e860.59 dB\u30011.06 dB\u548c0.22 dB\uff0c\u4e14\u6536\u655b\u901f\u5ea6\u5feb2\u500d\u3002", "conclusion": "H3R\u901a\u8fc7\u6df7\u5408\u6846\u67b6\u548c\u7a7a\u95f4\u5bf9\u9f50\u7684\u57fa\u7840\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u91cd\u5efa\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.03142", "pdf": "https://arxiv.org/pdf/2508.03142", "abs": "https://arxiv.org/abs/2508.03142", "authors": ["Chengyu Bai", "Jintao Chen", "Xiang Bai", "Yilong Chen", "Qi She", "Ming Lu", "Shanghang Zhang"], "title": "UniEdit-I: Training-free Image Editing for Unified VLM via Iterative Understanding, Editing and Verifying", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, unified vision-language models (VLMs) have rapidly advanced, effectively tackling both visual understanding and generation tasks within a single design. While many unified VLMs have explored various design choices, the recent hypothesis from OpenAI's GPT-4o suggests a promising generation pipeline: Understanding VLM->Visual Feature->Projector->Diffusion Model->Image. The understanding VLM is frozen, and only the generation-related modules are trained. This pipeline maintains the strong capability of understanding VLM while enabling the image generation ability of the unified VLM. Although this pipeline has shown very promising potential for the future development of unified VLM, how to easily enable image editing capability is still unexplored. In this paper, we introduce a novel training-free framework named UniEdit-I to enable the unified VLM with image editing capability via three iterative steps: understanding, editing, and verifying. 1. The understanding step analyzes the source image to create a source prompt through structured semantic analysis and makes minimal word replacements to form the target prompt based on the editing instruction. 2. The editing step introduces a time-adaptive offset, allowing for coherent editing from coarse to fine throughout the denoising process. 3. The verification step checks the alignment between the target prompt and the intermediate edited image, provides automatic consistency scores and corrective feedback, and determines whether to stop early or continue the editing loop. This understanding, editing, and verifying loop iterates until convergence, delivering high-fidelity editing in a training-free manner. We implemented our method based on the latest BLIP3-o and achieved state-of-the-art (SOTA) performance on the GEdit-Bench benchmark.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u56fe\u50cf\u7f16\u8f91\u6846\u67b6UniEdit-I\uff0c\u901a\u8fc7\u7406\u89e3\u3001\u7f16\u8f91\u548c\u9a8c\u8bc1\u4e09\u4e2a\u8fed\u4ee3\u6b65\u9aa4\uff0c\u4e3a\u7edf\u4e00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u63d0\u4f9b\u56fe\u50cf\u7f16\u8f91\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7edf\u4e00VLM\u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7f3a\u4e4f\u56fe\u50cf\u7f16\u8f91\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u4e09\u4e2a\u6b65\u9aa4\u5b9e\u73b0\u56fe\u50cf\u7f16\u8f91\uff1a1\uff09\u7406\u89e3\u6b65\u9aa4\u751f\u6210\u6e90\u63d0\u793a\u548c\u76ee\u6807\u63d0\u793a\uff1b2\uff09\u7f16\u8f91\u6b65\u9aa4\u5f15\u5165\u65f6\u95f4\u81ea\u9002\u5e94\u504f\u79fb\u8fdb\u884c\u8fde\u8d2f\u7f16\u8f91\uff1b3\uff09\u9a8c\u8bc1\u6b65\u9aa4\u68c0\u67e5\u5bf9\u9f50\u5e76\u63d0\u4f9b\u53cd\u9988\u3002", "result": "\u5728BLIP3-o\u4e0a\u5b9e\u73b0\uff0c\u5e76\u5728GEdit-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "UniEdit-I\u4e3a\u7edf\u4e00VLM\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u56fe\u50cf\u7f16\u8f91\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.03143", "pdf": "https://arxiv.org/pdf/2508.03143", "abs": "https://arxiv.org/abs/2508.03143", "authors": ["Yanshu Wang", "Xichen Xu", "Xiaoning Lei", "Guoyang Xie"], "title": "SARD: Segmentation-Aware Anomaly Synthesis via Region-Constrained Diffusion with Discriminative Mask Guidance", "categories": ["cs.CV"], "comment": "Accepted by The 2025 International Conference on Machine Intelligence   and Nature-InspireD Computing (MIND)", "summary": "Synthesizing realistic and spatially precise anomalies is essential for enhancing the robustness of industrial anomaly detection systems. While recent diffusion-based methods have demonstrated strong capabilities in modeling complex defect patterns, they often struggle with spatial controllability and fail to maintain fine-grained regional fidelity. To overcome these limitations, we propose SARD (Segmentation-Aware anomaly synthesis via Region-constrained Diffusion with discriminative mask Guidance), a novel diffusion-based framework specifically designed for anomaly generation. Our approach introduces a Region-Constrained Diffusion (RCD) process that preserves the background by freezing it and selectively updating only the foreground anomaly regions during the reverse denoising phase, thereby effectively reducing background artifacts. Additionally, we incorporate a Discriminative Mask Guidance (DMG) module into the discriminator, enabling joint evaluation of both global realism and local anomaly fidelity, guided by pixel-level masks. Extensive experiments on the MVTec-AD and BTAD datasets show that SARD surpasses existing methods in segmentation accuracy and visual quality, setting a new state-of-the-art for pixel-level anomaly synthesis.", "AI": {"tldr": "SARD\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u5f02\u5e38\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u533a\u57df\u7ea6\u675f\u6269\u6563\u548c\u5224\u522b\u6027\u63a9\u7801\u5f15\u5bfc\uff0c\u63d0\u5347\u4e86\u5f02\u5e38\u5408\u6210\u7684\u7a7a\u95f4\u53ef\u63a7\u6027\u548c\u533a\u57df\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u65b9\u6cd5\u5728\u7a7a\u95f4\u53ef\u63a7\u6027\u548c\u533a\u57df\u4fdd\u771f\u5ea6\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u63d0\u5347\u3002", "method": "\u63d0\u51faSARD\u6846\u67b6\uff0c\u7ed3\u5408\u533a\u57df\u7ea6\u675f\u6269\u6563\uff08RCD\uff09\u548c\u5224\u522b\u6027\u63a9\u7801\u5f15\u5bfc\uff08DMG\uff09\uff0c\u9009\u62e9\u6027\u66f4\u65b0\u5f02\u5e38\u533a\u57df\u5e76\u8bc4\u4f30\u5168\u5c40\u4e0e\u5c40\u90e8\u4fdd\u771f\u5ea6\u3002", "result": "\u5728MVTec-AD\u548cBTAD\u6570\u636e\u96c6\u4e0a\uff0cSARD\u5728\u5206\u5272\u7cbe\u5ea6\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u65b0SOTA\u3002", "conclusion": "SARD\u901a\u8fc7\u6539\u8fdb\u6269\u6563\u8fc7\u7a0b\u7684\u7a7a\u95f4\u63a7\u5236\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u5408\u6210\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.03180", "pdf": "https://arxiv.org/pdf/2508.03180", "abs": "https://arxiv.org/abs/2508.03180", "authors": ["Weihang Liu", "Yuke Li", "Yuxuan Li", "Jingyi Yu", "Xin Lou"], "title": "Duplex-GS: Proxy-Guided Weighted Blending for Real-Time Order-Independent Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated remarkable rendering fidelity and efficiency. However, these methods still rely on computationally expensive sequential alpha-blending operations, resulting in significant overhead, particularly on resource-constrained platforms. In this paper, we propose Duplex-GS, a dual-hierarchy framework that integrates proxy Gaussian representations with order-independent rendering techniques to achieve photorealistic results while sustaining real-time performance. To mitigate the overhead caused by view-adaptive radix sort, we introduce cell proxies for local Gaussians management and propose cell search rasterization for further acceleration. By seamlessly combining our framework with Order-Independent Transparency (OIT), we develop a physically inspired weighted sum rendering technique that simultaneously eliminates \"popping\" and \"transparency\" artifacts, yielding substantial improvements in both accuracy and efficiency. Extensive experiments on a variety of real-world datasets demonstrate the robustness of our method across diverse scenarios, including multi-scale training views and large-scale environments. Our results validate the advantages of the OIT rendering paradigm in Gaussian Splatting, achieving high-quality rendering with an impressive 1.5 to 4 speedup over existing OIT based Gaussian Splatting approaches and 52.2% to 86.9% reduction of the radix sort overhead without quality degradation.", "AI": {"tldr": "Duplex-GS\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5c42\u6b21\u6846\u67b6\uff0c\u7ed3\u5408\u4ee3\u7406\u9ad8\u65af\u8868\u793a\u548c\u987a\u5e8f\u65e0\u5173\u6e32\u67d3\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u9ad8\u65af\u6cfc\u6e85\u7684\u6e32\u67d3\u6548\u7387\u548c\u6548\u679c\u3002", "motivation": "\u73b0\u67093D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u4f9d\u8d56\u8ba1\u7b97\u5bc6\u96c6\u7684\u987a\u5e8falpha\u6df7\u5408\u64cd\u4f5c\uff0c\u5bfc\u81f4\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u7684\u6027\u80fd\u74f6\u9888\u3002", "method": "\u901a\u8fc7\u4ee3\u7406\u9ad8\u65af\u8868\u793a\u548c\u5355\u5143\u641c\u7d22\u5149\u6805\u5316\u4f18\u5316\u89c6\u56fe\u81ea\u9002\u5e94\u57fa\u6570\u6392\u5e8f\u7684\u5f00\u9500\uff0c\u7ed3\u5408\u987a\u5e8f\u65e0\u5173\u900f\u660e\u6280\u672f\u5b9e\u73b0\u9ad8\u6548\u6e32\u67d3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u573a\u666f\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u901f\u5ea6\u63d0\u53471.5\u81f34\u500d\uff0c\u57fa\u6570\u6392\u5e8f\u5f00\u9500\u51cf\u5c1152.2%\u81f386.9%\u3002", "conclusion": "Duplex-GS\u9a8c\u8bc1\u4e86\u987a\u5e8f\u65e0\u5173\u900f\u660e\u6280\u672f\u5728\u9ad8\u65af\u6cfc\u6e85\u4e2d\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u5b9e\u65f6\u6e32\u67d3\u3002"}}
{"id": "2508.03218", "pdf": "https://arxiv.org/pdf/2508.03218", "abs": "https://arxiv.org/abs/2508.03218", "authors": ["Shanshan Guo", "Xiwen Liang", "Junfan Lin", "Yuzheng Zhuang", "Liang Lin", "Xiaodan Liang"], "title": "ActionSink: Toward Precise Robot Manipulation with Dynamic Integration of Action Flow", "categories": ["cs.CV"], "comment": null, "summary": "Language-instructed robot manipulation has garnered significant interest due to the potential of learning from collected data. While the challenges in high-level perception and planning are continually addressed along the progress of general large pre-trained models, the low precision of low-level action estimation has emerged as the key limiting factor in manipulation performance. To this end, this paper introduces a novel robot manipulation framework, i.e., ActionSink, to pave the way toward precise action estimations in the field of learning-based robot manipulation. As the name suggests, ActionSink reformulates the actions of robots as action-caused optical flows from videos, called \"action flow\", in a self-supervised manner, which are then used to be retrieved and integrated to enhance the action estimation. Specifically, ActionSink incorporates two primary modules. The first module is a coarse-to-fine action flow matcher, which continuously refines the accuracy of action flow via iterative retrieval and denoising process. The second module is a dynamic action flow integrator, which employs a working memory pool that dynamically and efficiently manages the historical action flows that should be used to integrate to enhance the current action estimation. In this module, a multi-layer fusion module is proposed to integrate direct estimation and action flows from both the current and the working memory, achieving highly accurate action estimation through a series of estimation-integration processes. Our ActionSink framework outperformed prior SOTA on the LIBERO benchmark by a 7.9\\% success rate, and obtained nearly an 8\\% accuracy gain on the challenging long-horizon visual task LIBERO-Long.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aActionSink\u7684\u65b0\u578b\u673a\u5668\u4eba\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u65b9\u5f0f\u5c06\u673a\u5668\u4eba\u52a8\u4f5c\u91cd\u65b0\u5b9a\u4e49\u4e3a\u89c6\u9891\u4e2d\u7684\u5149\u6d41\uff08\u79f0\u4e3a\u201c\u52a8\u4f5c\u6d41\u201d\uff09\uff0c\u5e76\u901a\u8fc7\u68c0\u7d22\u548c\u6574\u5408\u52a8\u4f5c\u6d41\u6765\u63d0\u9ad8\u52a8\u4f5c\u4f30\u8ba1\u7684\u7cbe\u5ea6\u3002", "motivation": "\u5c3d\u7ba1\u901a\u7528\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u9ad8\u5c42\u6b21\u611f\u77e5\u548c\u89c4\u5212\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u4f4e\u5c42\u6b21\u52a8\u4f5c\u4f30\u8ba1\u7684\u4f4e\u7cbe\u5ea6\u4ecd\u7136\u662f\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd\u7684\u5173\u952e\u9650\u5236\u56e0\u7d20\u3002", "method": "ActionSink\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u6a21\u5757\uff1a1\uff09\u7c97\u5230\u7ec6\u7684\u52a8\u4f5c\u6d41\u5339\u914d\u5668\uff0c\u901a\u8fc7\u8fed\u4ee3\u68c0\u7d22\u548c\u53bb\u566a\u8fc7\u7a0b\u63d0\u9ad8\u52a8\u4f5c\u6d41\u7cbe\u5ea6\uff1b2\uff09\u52a8\u6001\u52a8\u4f5c\u6d41\u6574\u5408\u5668\uff0c\u5229\u7528\u5de5\u4f5c\u8bb0\u5fc6\u6c60\u52a8\u6001\u7ba1\u7406\u5386\u53f2\u52a8\u4f5c\u6d41\uff0c\u5e76\u901a\u8fc7\u591a\u5c42\u878d\u5408\u6a21\u5757\u6574\u5408\u5f53\u524d\u548c\u5386\u53f2\u7684\u52a8\u4f5c\u6d41\u3002", "result": "ActionSink\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u9ad8\u4e867.9%\u7684\u6210\u529f\u7387\uff0c\u5728LIBERO-Long\u4efb\u52a1\u4e2d\u83b7\u5f97\u4e86\u8fd18%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "ActionSink\u901a\u8fc7\u81ea\u76d1\u7763\u7684\u52a8\u4f5c\u6d41\u548c\u52a8\u6001\u6574\u5408\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u52a8\u4f5c\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u4e3a\u5b66\u4e60\u578b\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.03227", "pdf": "https://arxiv.org/pdf/2508.03227", "abs": "https://arxiv.org/abs/2508.03227", "authors": ["Hongyu Shen", "Junfeng Ni", "Yixin Chen", "Weishuo Li", "Mingtao Pei", "Siyuan Huang"], "title": "Trace3D: Consistent Segmentation Lifting via Gaussian Instance Tracing", "categories": ["cs.CV"], "comment": null, "summary": "We address the challenge of lifting 2D visual segmentation to 3D in Gaussian Splatting. Existing methods often suffer from inconsistent 2D masks across viewpoints and produce noisy segmentation boundaries as they neglect these semantic cues to refine the learned Gaussians. To overcome this, we introduce Gaussian Instance Tracing (GIT), which augments the standard Gaussian representation with an instance weight matrix across input views. Leveraging the inherent consistency of Gaussians in 3D, we use this matrix to identify and correct 2D segmentation inconsistencies. Furthermore, since each Gaussian ideally corresponds to a single object, we propose a GIT-guided adaptive density control mechanism to split and prune ambiguous Gaussians during training, resulting in sharper and more coherent 2D and 3D segmentation boundaries. Experimental results show that our method extracts clean 3D assets and consistently improves 3D segmentation in both online (e.g., self-prompting) and offline (e.g., contrastive lifting) settings, enabling applications such as hierarchical segmentation, object extraction, and scene editing.", "AI": {"tldr": "\u63d0\u51faGaussian Instance Tracing (GIT)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u4f8b\u6743\u91cd\u77e9\u9635\u548c\u81ea\u9002\u5e94\u5bc6\u5ea6\u63a7\u5236\uff0c\u89e3\u51b32D\u52303D\u5206\u5272\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u63d0\u5347\u5206\u5272\u8fb9\u754c\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u57282D\u52303D\u5206\u5272\u4e2d\u5b58\u5728\u89c6\u89d2\u95f4\u63a9\u7801\u4e0d\u4e00\u81f4\u548c\u8fb9\u754c\u566a\u58f0\u95ee\u9898\uff0c\u7f3a\u4e4f\u8bed\u4e49\u7ebf\u7d22\u4f18\u5316\u9ad8\u65af\u6a21\u578b\u3002", "method": "\u5f15\u5165GIT\uff0c\u6269\u5c55\u9ad8\u65af\u8868\u793a\uff0c\u52a0\u5165\u5b9e\u4f8b\u6743\u91cd\u77e9\u9635\uff0c\u5229\u7528\u9ad8\u65af\u4e00\u81f4\u6027\u4fee\u6b632D\u5206\u5272\u4e0d\u4e00\u81f4\uff1b\u63d0\u51fa\u81ea\u9002\u5e94\u5bc6\u5ea6\u63a7\u5236\u673a\u5236\uff0c\u4f18\u5316\u9ad8\u65af\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGIT\u5728\u5728\u7ebf\u548c\u79bb\u7ebf\u573a\u666f\u4e0b\u5747\u63d0\u53473D\u5206\u5272\u8d28\u91cf\uff0c\u652f\u6301\u5206\u5c42\u5206\u5272\u3001\u5bf9\u8c61\u63d0\u53d6\u548c\u573a\u666f\u7f16\u8f91\u7b49\u5e94\u7528\u3002", "conclusion": "GIT\u901a\u8fc7\u8bed\u4e49\u4f18\u5316\u548c\u5bc6\u5ea6\u63a7\u5236\uff0c\u663e\u8457\u6539\u5584\u4e862D\u52303D\u5206\u5272\u7684\u8fb9\u754c\u4e00\u81f4\u6027\u548c\u6e05\u6670\u5ea6\u3002"}}
{"id": "2508.03241", "pdf": "https://arxiv.org/pdf/2508.03241", "abs": "https://arxiv.org/abs/2508.03241", "authors": ["Xingchao Yang", "Shiori Ueda", "Yuantian Huang", "Tomoya Akiyama", "Takafumi Taketomi"], "title": "FFHQ-Makeup: Paired Synthetic Makeup Dataset with Facial Consistency Across Multiple Styles", "categories": ["cs.CV"], "comment": "Project: https://yangxingchao.github.io/FFHQ-Makeup-page, Datasets:   https://huggingface.co/datasets/cyberagent/FFHQ-Makeup", "summary": "Paired bare-makeup facial images are essential for a wide range of beauty-related tasks, such as virtual try-on, facial privacy protection, and facial aesthetics analysis. However, collecting high-quality paired makeup datasets remains a significant challenge. Real-world data acquisition is constrained by the difficulty of collecting large-scale paired images, while existing synthetic approaches often suffer from limited realism or inconsistencies between bare and makeup images. Current synthetic methods typically fall into two categories: warping-based transformations, which often distort facial geometry and compromise the precision of makeup; and text-to-image generation, which tends to alter facial identity and expression, undermining consistency. In this work, we present FFHQ-Makeup, a high-quality synthetic makeup dataset that pairs each identity with multiple makeup styles while preserving facial consistency in both identity and expression. Built upon the diverse FFHQ dataset, our pipeline transfers real-world makeup styles from existing datasets onto 18K identities by introducing an improved makeup transfer method that disentangles identity and makeup. Each identity is paired with 5 different makeup styles, resulting in a total of 90K high-quality bare-makeup image pairs. To the best of our knowledge, this is the first work that focuses specifically on constructing a makeup dataset. We hope that FFHQ-Makeup fills the gap of lacking high-quality bare-makeup paired datasets and serves as a valuable resource for future research in beauty-related tasks.", "AI": {"tldr": "FFHQ-Makeup\u662f\u4e00\u4e2a\u9ad8\u8d28\u91cf\u5408\u6210\u7684\u5316\u5986\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u914d\u5bf9\u56fe\u50cf\u4e0d\u8db3\u548c\u771f\u5b9e\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u5316\u5986\u8f6c\u79fb\u65b9\u6cd5\u751f\u621018K\u8eab\u4efd\u768490K\u914d\u5bf9\u56fe\u50cf\u3002", "motivation": "\u73b0\u6709\u5316\u5986\u6570\u636e\u96c6\u5728\u771f\u5b9e\u6027\u548c\u914d\u5bf9\u4e00\u81f4\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u96be\u4ee5\u6ee1\u8db3\u7f8e\u5bb9\u76f8\u5173\u4efb\u52a1\u7684\u9700\u6c42\u3002", "method": "\u57fa\u4e8eFFHQ\u6570\u636e\u96c6\uff0c\u91c7\u7528\u6539\u8fdb\u7684\u5316\u5986\u8f6c\u79fb\u65b9\u6cd5\uff0c\u5c06\u771f\u5b9e\u5316\u5986\u98ce\u683c\u8f6c\u79fb\u523018K\u8eab\u4efd\u4e0a\uff0c\u6bcf\u4e2a\u8eab\u4efd\u914d\u5bf95\u79cd\u5316\u5986\u98ce\u683c\u3002", "result": "\u751f\u6210\u4e8690K\u9ad8\u8d28\u91cf\u914d\u5bf9\u56fe\u50cf\uff0c\u4fdd\u6301\u4e86\u8eab\u4efd\u548c\u8868\u60c5\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "FFHQ-Makeup\u586b\u8865\u4e86\u9ad8\u8d28\u91cf\u914d\u5bf9\u5316\u5986\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4e3a\u7f8e\u5bb9\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002"}}
{"id": "2508.03244", "pdf": "https://arxiv.org/pdf/2508.03244", "abs": "https://arxiv.org/abs/2508.03244", "authors": ["Chuanzhi Xu", "Haoxian Zhou", "Langyi Chen", "Yuk Ying Chung", "Qiang Qu"], "title": "Ultralight Polarity-Split Neuromorphic SNN for Event-Stream Super-Resolution", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Event cameras offer unparalleled advantages such as high temporal resolution, low latency, and high dynamic range. However, their limited spatial resolution poses challenges for fine-grained perception tasks. In this work, we propose an ultra-lightweight, stream-based event-to-event super-resolution method based on Spiking Neural Networks (SNNs), designed for real-time deployment on resource-constrained devices. To further reduce model size, we introduce a novel Dual-Forward Polarity-Split Event Encoding strategy that decouples positive and negative events into separate forward paths through a shared SNN. Furthermore, we propose a Learnable Spatio-temporal Polarity-aware Loss (LearnSTPLoss) that adaptively balances temporal, spatial, and polarity consistency using learnable uncertainty-based weights. Experimental results demonstrate that our method achieves competitive super-resolution performance on multiple datasets while significantly reducing model size and inference time. The lightweight design enables embedding the module into event cameras or using it as an efficient front-end preprocessing for downstream vision tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u7684\u8d85\u8f7b\u91cf\u7ea7\u4e8b\u4ef6\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\uff0c\u901a\u8fc7\u53cc\u5411\u524d\u6781\u6027\u5206\u5272\u4e8b\u4ef6\u7f16\u7801\u548c\u53ef\u5b66\u4e60\u65f6\u7a7a\u6781\u6027\u611f\u77e5\u635f\u5931\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u3001\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u52a8\u6001\u8303\u56f4\u4f18\u52bf\u53d7\u9650\u4e8e\u7a7a\u95f4\u5206\u8fa8\u7387\uff0c\u5f71\u54cd\u7ec6\u7c92\u5ea6\u611f\u77e5\u4efb\u52a1\u3002", "method": "\u91c7\u7528SNN\u8bbe\u8ba1\u5b9e\u65f6\u6d41\u5f0f\u4e8b\u4ef6\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u5f15\u5165\u53cc\u5411\u524d\u6781\u6027\u5206\u5272\u4e8b\u4ef6\u7f16\u7801\u548c\u53ef\u5b66\u4e60\u65f6\u7a7a\u6781\u6027\u611f\u77e5\u635f\u5931\uff08LearnSTPLoss\uff09\u3002", "result": "\u5728\u591a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u7ade\u4e89\u6027\u8d85\u5206\u8fa8\u7387\u6027\u80fd\uff0c\u663e\u8457\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u548c\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u9002\u5408\u5d4c\u5165\u4e8b\u4ef6\u76f8\u673a\u6216\u4f5c\u4e3a\u4e0b\u6e38\u89c6\u89c9\u4efb\u52a1\u7684\u9ad8\u6548\u9884\u5904\u7406\u6a21\u5757\u3002"}}
{"id": "2508.03252", "pdf": "https://arxiv.org/pdf/2508.03252", "abs": "https://arxiv.org/abs/2508.03252", "authors": ["Wentao Qu", "Guofeng Mei", "Jing Wang", "Yujiao Wu", "Xiaoshui Huang", "Liang Xiao"], "title": "Robust Single-Stage Fully Sparse 3D Object Detection via Detachable Latent Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Denoising Diffusion Probabilistic Models (DDPMs) have shown success in robust 3D object detection tasks. Existing methods often rely on the score matching from 3D boxes or pre-trained diffusion priors. However, they typically require multi-step iterations in inference, which limits efficiency. To address this, we propose a \\textbf{R}obust single-stage fully \\textbf{S}parse 3D object \\textbf{D}etection \\textbf{Net}work with a Detachable Latent Framework (DLF) of DDPMs, named RSDNet. Specifically, RSDNet learns the denoising process in latent feature spaces through lightweight denoising networks like multi-level denoising autoencoders (DAEs). This enables RSDNet to effectively understand scene distributions under multi-level perturbations, achieving robust and reliable detection. Meanwhile, we reformulate the noising and denoising mechanisms of DDPMs, enabling DLF to construct multi-type and multi-level noise samples and targets, enhancing RSDNet robustness to multiple perturbations. Furthermore, a semantic-geometric conditional guidance is introduced to perceive the object boundaries and shapes, alleviating the center feature missing problem in sparse representations, enabling RSDNet to perform in a fully sparse detection pipeline. Moreover, the detachable denoising network design of DLF enables RSDNet to perform single-step detection in inference, further enhancing detection efficiency. Extensive experiments on public benchmarks show that RSDNet can outperform existing methods, achieving state-of-the-art detection.", "AI": {"tldr": "RSDNet\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDDPM\u7684\u5355\u9636\u6bb5\u7a00\u758f3D\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u53bb\u566a\u7f51\u7edc\u548c\u53ef\u5206\u79bb\u6f5c\u5728\u6846\u67b6\u63d0\u9ad8\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u591a\u6b65\u8fed\u4ee3\u63a8\u7406\uff0c\u6548\u7387\u4f4e\uff0c\u4e14\u5bf9\u591a\u7ea7\u6270\u52a8\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "RSDNet\u91c7\u7528\u8f7b\u91cf\u7ea7\u53bb\u566a\u7f51\u7edc\uff08\u5982\u591a\u7ea7DAEs\uff09\u5728\u6f5c\u5728\u7279\u5f81\u7a7a\u95f4\u5b66\u4e60\u53bb\u566a\u8fc7\u7a0b\uff0c\u5e76\u5f15\u5165\u8bed\u4e49-\u51e0\u4f55\u6761\u4ef6\u6307\u5bfc\u4ee5\u589e\u5f3a\u7a00\u758f\u8868\u793a\u7684\u8fb9\u754c\u611f\u77e5\u3002", "result": "\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRSDNet\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "RSDNet\u901a\u8fc7\u5355\u6b65\u63a8\u7406\u548c\u53ef\u5206\u79bb\u6f5c\u5728\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u76ee\u6807\u68c0\u6d4b\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.03254", "pdf": "https://arxiv.org/pdf/2508.03254", "abs": "https://arxiv.org/abs/2508.03254", "authors": ["Jisoo Kim", "Wooseok Seo", "Junwan Kim", "Seungho Park", "Sooyeon Park", "Youngjae Yu"], "title": "V.I.P. : Iterative Online Preference Distillation for Efficient Video Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV2025 accepted", "summary": "With growing interest in deploying text-to-video (T2V) models in resource-constrained environments, reducing their high computational cost has become crucial, leading to extensive research on pruning and knowledge distillation methods while maintaining performance. However, existing distillation methods primarily rely on supervised fine-tuning (SFT), which often leads to mode collapse as pruned models with reduced capacity fail to directly match the teacher's outputs, ultimately resulting in degraded quality. To address this challenge, we propose an effective distillation method, ReDPO, that integrates DPO and SFT. Our approach leverages DPO to guide the student model to focus on recovering only the targeted properties, rather than passively imitating the teacher, while also utilizing SFT to enhance overall performance. We additionally propose V.I.P., a novel framework for filtering and curating high-quality pair datasets, along with a step-by-step online approach for calibrated training. We validate our method on two leading T2V models, VideoCrafter2 and AnimateDiff, achieving parameter reduction of 36.2% and 67.5% each, while maintaining or even surpassing the performance of full models. Further experiments demonstrate the effectiveness of both ReDPO and V.I.P. framework in enabling efficient and high-quality video generation. Our code and videos are available at https://jiiiisoo.github.io/VIP.github.io/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408DPO\u548cSFT\u7684\u84b8\u998f\u65b9\u6cd5ReDPO\uff0c\u4ee5\u53ca\u6570\u636e\u7b5b\u9009\u6846\u67b6V.I.P.\uff0c\u7528\u4e8e\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u84b8\u998f\u65b9\u6cd5\u56e0\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5bfc\u81f4\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u63d0\u5347\u526a\u679d\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u7ed3\u5408DPO\u548cSFT\uff0c\u5229\u7528DPO\u6307\u5bfc\u5b66\u751f\u6a21\u578b\u6062\u590d\u76ee\u6807\u5c5e\u6027\uff0c\u800c\u975e\u88ab\u52a8\u6a21\u4eff\u6559\u5e08\u6a21\u578b\uff1b\u540c\u65f6\u63d0\u51faV.I.P.\u6846\u67b6\u7b5b\u9009\u9ad8\u8d28\u91cf\u6570\u636e\u5bf9\u3002", "result": "\u5728VideoCrafter2\u548cAnimateDiff\u4e0a\u5206\u522b\u51cf\u5c1136.2%\u548c67.5%\u7684\u53c2\u6570\uff0c\u6027\u80fd\u4fdd\u6301\u6216\u8d85\u8d8a\u5b8c\u6574\u6a21\u578b\u3002", "conclusion": "ReDPO\u548cV.I.P.\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u526a\u679d\u6a21\u578b\u7684\u89c6\u9891\u751f\u6210\u6548\u7387\u548c\u8d28\u91cf\u3002"}}
{"id": "2508.03300", "pdf": "https://arxiv.org/pdf/2508.03300", "abs": "https://arxiv.org/abs/2508.03300", "authors": ["Jun Luo", "Zijing Zhao", "Yang Liu"], "title": "Zero Shot Domain Adaptive Semantic Segmentation by Synthetic Data Generation and Progressive Adaptation", "categories": ["cs.CV"], "comment": "Accepted to IROS 2025", "summary": "Deep learning-based semantic segmentation models achieve impressive results yet remain limited in handling distribution shifts between training and test data. In this paper, we present SDGPA (Synthetic Data Generation and Progressive Adaptation), a novel method that tackles zero-shot domain adaptive semantic segmentation, in which no target images are available, but only a text description of the target domain's style is provided. To compensate for the lack of target domain training data, we utilize a pretrained off-the-shelf text-to-image diffusion model, which generates training images by transferring source domain images to target style. Directly editing source domain images introduces noise that harms segmentation because the layout of source images cannot be precisely maintained. To address inaccurate layouts in synthetic data, we propose a method that crops the source image, edits small patches individually, and then merges them back together, which helps improve spatial precision. Recognizing the large domain gap, SDGPA constructs an augmented intermediate domain, leveraging easier adaptation subtasks to enable more stable model adaptation to the target domain. Additionally, to mitigate the impact of noise in synthetic data, we design a progressive adaptation strategy, ensuring robust learning throughout the training process. Extensive experiments demonstrate that our method achieves state-of-the-art performance in zero-shot semantic segmentation. The code is available at https://github.com/ROUJINN/SDGPA", "AI": {"tldr": "SDGPA\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u57df\u9002\u5e94\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u6e10\u8fdb\u9002\u5e94\u89e3\u51b3\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u6570\u636e\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u8bed\u4e49\u5206\u5272\u6a21\u578b\u5728\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u6570\u636e\u5206\u5e03\u504f\u79fb\u65f6\u7684\u6027\u80fd\u9650\u5236\uff0c\u5c24\u5176\u662f\u5728\u96f6\u6837\u672c\u57df\u9002\u5e94\u573a\u666f\u4e0b\uff08\u65e0\u76ee\u6807\u57df\u56fe\u50cf\uff0c\u4ec5\u6709\u6587\u672c\u63cf\u8ff0\uff09\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u751f\u6210\u76ee\u6807\u98ce\u683c\u56fe\u50cf\uff0c\u901a\u8fc7\u88c1\u526a\u548c\u7f16\u8f91\u5c0f\u533a\u57df\u63d0\u9ad8\u7a7a\u95f4\u7cbe\u5ea6\uff0c\u6784\u5efa\u4e2d\u95f4\u57df\u5e76\u91c7\u7528\u6e10\u8fdb\u9002\u5e94\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSDGPA\u5728\u96f6\u6837\u672c\u8bed\u4e49\u5206\u5272\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "SDGPA\u901a\u8fc7\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u6e10\u8fdb\u9002\u5e94\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u96f6\u6837\u672c\u57df\u9002\u5e94\u8bed\u4e49\u5206\u5272\u95ee\u9898\u3002"}}
{"id": "2508.03320", "pdf": "https://arxiv.org/pdf/2508.03320", "abs": "https://arxiv.org/abs/2508.03320", "authors": ["Peiyu Wang", "Yi Peng", "Yimeng Gan", "Liang Hu", "Tianyidan Xie", "Xiaokun Wang", "Yichen Wei", "Chuanxin Tang", "Bo Zhu", "Changshi Li", "Hongyang Wei", "Eric Li", "Xuchen Song", "Yang Liu", "Yahui Zhou"], "title": "Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding and Generation", "categories": ["cs.CV"], "comment": null, "summary": "We introduce Skywork UniPic, a 1.5 billion-parameter autoregressive model that unifies image understanding, text-to-image generation, and image editing within a single architecture-eliminating the need for task-specific adapters or inter-module connectors-and demonstrate that compact multimodal systems can achieve state-of-the-art performance on commodity hardware. Skywork UniPic achieves a GenEval score of 0.86, surpassing most existing unified models; sets a new DPG-Bench complex-generation record of 85.5; attains 5.83 on GEditBench-EN and 3.49 on ImgEdit-Bench for image editing; and generates 1024 x 1024 images with under 15 GB of GPU memory (e.g., RTX 4090). (1) a decoupled encoding strategy that leverages a masked autoregressive encoder for synthesis and a SigLIP2 encoder for understanding, all feeding a shared autoregressive decoder; (2) a progressive, resolution-aware training schedule scaling from 256 x 256 to 1024 x 1024 while dynamically unfreezing parameters to balance capacity and stability; and (3) meticulously curated, 100 million-scale datasets augmented with task-specific reward models to refine generation and editing objectives. By demonstrating that high-fidelity multimodal integration need not incur prohibitive resource demands, Skywork UniPic establishes a practical paradigm for deployable, high-fidelity multimodal AI. Code and weights are publicly available at https://huggingface.co/Skywork/Skywork-UniPic-1.5B.", "AI": {"tldr": "Skywork UniPic\u662f\u4e00\u4e2a15\u4ebf\u53c2\u6570\u7684\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u7edf\u4e00\u4e86\u56fe\u50cf\u7406\u89e3\u3001\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u548c\u56fe\u50cf\u7f16\u8f91\u529f\u80fd\uff0c\u6027\u80fd\u4f18\u5f02\u4e14\u8d44\u6e90\u9700\u6c42\u4f4e\u3002", "motivation": "\u65e8\u5728\u8bc1\u660e\u7d27\u51d1\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u53ef\u4ee5\u5728\u5546\u54c1\u786c\u4ef6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u907f\u514d\u4efb\u52a1\u7279\u5b9a\u7684\u9002\u914d\u5668\u6216\u6a21\u5757\u95f4\u8fde\u63a5\u5668\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u89e3\u8026\u7f16\u7801\u7b56\u7565\u3001\u6e10\u8fdb\u5f0f\u5206\u8fa8\u7387\u611f\u77e5\u8bad\u7ec3\u8ba1\u5212\u4ee5\u53ca\u7cbe\u5fc3\u7b56\u5212\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u5956\u52b1\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5982GenEval\u5f97\u52060.86\uff0cDPG-Bench\u590d\u6742\u751f\u6210\u8bb0\u5f5585.5\uff0c\u5e76\u80fd\u5728\u4f4eGPU\u5185\u5b58\u4e0b\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u3002", "conclusion": "Skywork UniPic\u4e3a\u53ef\u90e8\u7f72\u7684\u9ad8\u4fdd\u771f\u591a\u6a21\u6001AI\u63d0\u4f9b\u4e86\u5b9e\u7528\u8303\u4f8b\uff0c\u4ee3\u7801\u548c\u6743\u91cd\u5df2\u516c\u5f00\u3002"}}
{"id": "2508.03334", "pdf": "https://arxiv.org/pdf/2508.03334", "abs": "https://arxiv.org/abs/2508.03334", "authors": ["Xunzhi Xiang", "Yabo Chen", "Guiyu Zhang", "Zhongyu Wang", "Zhe Gao", "Quanming Xiang", "Gonghu Shang", "Junqi Liu", "Haibin Huang", "Yang Gao", "Chi Zhang", "Qi Fan", "Xuelong Li"], "title": "Macro-from-Micro Planning for High-Quality and Parallelized Autoregressive Long Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Current autoregressive diffusion models excel at video generation but are generally limited to short temporal durations. Our theoretical analysis indicates that the autoregressive modeling typically suffers from temporal drift caused by error accumulation and hinders parallelization in long video synthesis. To address these limitations, we propose a novel planning-then-populating framework centered on Macro-from-Micro Planning (MMPL) for long video generation. MMPL sketches a global storyline for the entire video through two hierarchical stages: Micro Planning and Macro Planning. Specifically, Micro Planning predicts a sparse set of future keyframes within each short video segment, offering motion and appearance priors to guide high-quality video segment generation. Macro Planning extends the in-segment keyframes planning across the entire video through an autoregressive chain of micro plans, ensuring long-term consistency across video segments. Subsequently, MMPL-based Content Populating generates all intermediate frames in parallel across segments, enabling efficient parallelization of autoregressive generation. The parallelization is further optimized by Adaptive Workload Scheduling for balanced GPU execution and accelerated autoregressive video generation. Extensive experiments confirm that our method outperforms existing long video generation models in quality and stability. Generated videos and comparison results are in our project page.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMMPL\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u89c4\u5212\u89e3\u51b3\u81ea\u56de\u5f52\u6269\u6563\u6a21\u578b\u5728\u957f\u89c6\u9891\u751f\u6210\u4e2d\u7684\u65f6\u95f4\u6f02\u79fb\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u5e76\u884c\u5316\u751f\u6210\u3002", "motivation": "\u81ea\u56de\u5f52\u6269\u6563\u6a21\u578b\u5728\u957f\u89c6\u9891\u751f\u6210\u4e2d\u5b58\u5728\u65f6\u95f4\u6f02\u79fb\u548c\u5e76\u884c\u5316\u56f0\u96be\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u63d0\u51faMMPL\u6846\u67b6\uff0c\u5206Micro Planning\u548cMacro Planning\u4e24\u9636\u6bb5\u89c4\u5212\u5168\u5c40\u60c5\u8282\uff0c\u5e76\u901a\u8fc7\u5e76\u884c\u586b\u5145\u4e2d\u95f4\u5e27\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u957f\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002", "conclusion": "MMPL\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u751f\u6210\u4e2d\u7684\u65f6\u95f4\u6f02\u79fb\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u6548\u5e76\u884c\u5316\u3002"}}
{"id": "2508.03336", "pdf": "https://arxiv.org/pdf/2508.03336", "abs": "https://arxiv.org/abs/2508.03336", "authors": ["Tongshun Zhang", "Pingping Liu", "Zixuan Zhong", "Zijian Zhang", "Qiuzhan Zhou"], "title": "Beyond Illumination: Fine-Grained Detail Preservation in Extreme Dark Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Recovering fine-grained details in extremely dark images remains challenging due to severe structural information loss and noise corruption. Existing enhancement methods often fail to preserve intricate details and sharp edges, limiting their effectiveness in downstream applications like text and edge detection. To address these deficiencies, we propose an efficient dual-stage approach centered on detail recovery for dark images. In the first stage, we introduce a Residual Fourier-Guided Module (RFGM) that effectively restores global illumination in the frequency domain. RFGM captures inter-stage and inter-channel dependencies through residual connections, providing robust priors for high-fidelity frequency processing while mitigating error accumulation risks from unreliable priors. The second stage employs complementary Mamba modules specifically designed for textural structure refinement: (1) Patch Mamba operates on channel-concatenated non-downsampled patches, meticulously modeling pixel-level correlations to enhance fine-grained details without resolution loss. (2) Grad Mamba explicitly focuses on high-gradient regions, alleviating state decay in state space models and prioritizing reconstruction of sharp edges and boundaries. Extensive experiments on multiple benchmark datasets and downstream applications demonstrate that our method significantly improves detail recovery performance while maintaining efficiency. Crucially, the proposed modules are lightweight and can be seamlessly integrated into existing Fourier-based frameworks with minimal computational overhead. Code is available at https://github.com/bywlzts/RFGM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u9636\u6bb5\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b8b\u5dee\u5085\u91cc\u53f6\u5f15\u5bfc\u6a21\u5757\uff08RFGM\uff09\u548cMamba\u6a21\u5757\uff0c\u6709\u6548\u6062\u590d\u6781\u6697\u56fe\u50cf\u4e2d\u7684\u7ec6\u8282\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6062\u590d\u6697\u56fe\u50cf\u7ec6\u8282\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u65e0\u6cd5\u4fdd\u7559\u7cbe\u7ec6\u7ed3\u6784\u548c\u9510\u5229\u8fb9\u7f18\uff0c\u5f71\u54cd\u4e0b\u6e38\u5e94\u7528\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528RFGM\u5728\u9891\u57df\u6062\u590d\u5168\u5c40\u5149\u7167\uff1b\u7b2c\u4e8c\u9636\u6bb5\u91c7\u7528Patch Mamba\u548cGrad Mamba\u6a21\u5757\u7ec6\u5316\u7eb9\u7406\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7ec6\u8282\u6062\u590d\u6027\u80fd\uff0c\u4e14\u6a21\u5757\u8f7b\u91cf\u9ad8\u6548\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6062\u590d\u6697\u56fe\u50cf\u7ec6\u8282\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u6846\u67b6\u4e2d\u3002"}}
{"id": "2508.03338", "pdf": "https://arxiv.org/pdf/2508.03338", "abs": "https://arxiv.org/abs/2508.03338", "authors": ["Tongshun Zhang", "Pingping Liu", "Zhe Zhang", "Qiuzhan Zhou"], "title": "CIVQLLIE: Causal Intervention with Vector Quantization for Low-Light Image Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Images captured in nighttime scenes suffer from severely reduced visibility, hindering effective content perception. Current low-light image enhancement (LLIE) methods face significant challenges: data-driven end-to-end mapping networks lack interpretability or rely on unreliable prior guidance, struggling under extremely dark conditions, while physics-based methods depend on simplified assumptions that often fail in complex real-world scenarios. To address these limitations, we propose CIVQLLIE, a novel framework that leverages the power of discrete representation learning through causal reasoning. We achieve this through Vector Quantization (VQ), which maps continuous image features to a discrete codebook of visual tokens learned from large-scale high-quality images. This codebook serves as a reliable prior, encoding standardized brightness and color patterns that are independent of degradation. However, direct application of VQ to low-light images fails due to distribution shifts between degraded inputs and the learned codebook. Therefore, we propose a multi-level causal intervention approach to systematically correct these shifts. First, during encoding, our Pixel-level Causal Intervention (PCI) module intervenes to align low-level features with the brightness and color distributions expected by the codebook. Second, a Feature-aware Causal Intervention (FCI) mechanism with Low-frequency Selective Attention Gating (LSAG) identifies and enhances channels most affected by illumination degradation, facilitating accurate codebook token matching while enhancing the encoder's generalization performance through flexible feature-level intervention. Finally, during decoding, the High-frequency Detail Reconstruction Module (HDRM) leverages structural information preserved in the matched codebook representations to reconstruct fine details using deformable convolution techniques.", "AI": {"tldr": "CIVQLLIE\u662f\u4e00\u79cd\u57fa\u4e8e\u79bb\u6563\u8868\u793a\u5b66\u4e60\u548c\u56e0\u679c\u63a8\u7406\u7684\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u5411\u91cf\u91cf\u5316\u548c\u591a\u7ea7\u56e0\u679c\u5e72\u9884\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u591c\u95f4\u573a\u666f\u56fe\u50cf\u53ef\u89c1\u6027\u5dee\uff0c\u73b0\u6709\u4f4e\u5149\u589e\u5f3a\u65b9\u6cd5\u7f3a\u4e4f\u89e3\u91ca\u6027\u6216\u4f9d\u8d56\u4e0d\u53ef\u9760\u7684\u5148\u9a8c\uff0c\u7269\u7406\u65b9\u6cd5\u5219\u7b80\u5316\u5047\u8bbe\u96be\u4ee5\u9002\u5e94\u590d\u6742\u573a\u666f\u3002", "method": "\u5229\u7528\u5411\u91cf\u91cf\u5316\u5c06\u56fe\u50cf\u7279\u5f81\u6620\u5c04\u5230\u79bb\u6563\u89c6\u89c9\u6807\u8bb0\u4ee3\u7801\u672c\uff0c\u901a\u8fc7\u50cf\u7d20\u7ea7\u548c\u7279\u5f81\u7ea7\u56e0\u679c\u5e72\u9884\u6821\u6b63\u5206\u5e03\u504f\u79fb\uff0c\u5e76\u7ed3\u5408\u9ad8\u9891\u7ec6\u8282\u91cd\u5efa\u6a21\u5757\u3002", "result": "CIVQLLIE\u901a\u8fc7\u53ef\u9760\u7684\u5148\u9a8c\u548c\u591a\u7ea7\u5e72\u9884\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u5149\u56fe\u50cf\u7684\u589e\u5f3a\u6548\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u7ed3\u5408\u79bb\u6563\u8868\u793a\u5b66\u4e60\u548c\u56e0\u679c\u63a8\u7406\uff0c\u4e3a\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.03373", "pdf": "https://arxiv.org/pdf/2508.03373", "abs": "https://arxiv.org/abs/2508.03373", "authors": ["Ni Tang", "Xiaotong Luo", "Zihan Cheng", "Liangtai Zhou", "Dongxiao Zhang", "Yanyun Qu"], "title": "Diffusion Once and Done: Degradation-Aware LoRA for Efficient All-in-One Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have revealed powerful potential in all-in-one image restoration (AiOIR), which is talented in generating abundant texture details. The existing AiOIR methods either retrain a diffusion model or fine-tune the pretrained diffusion model with extra conditional guidance. However, they often suffer from high inference costs and limited adaptability to diverse degradation types. In this paper, we propose an efficient AiOIR method, Diffusion Once and Done (DOD), which aims to achieve superior restoration performance with only one-step sampling of Stable Diffusion (SD) models. Specifically, multi-degradation feature modulation is first introduced to capture different degradation prompts with a pretrained diffusion model. Then, parameter-efficient conditional low-rank adaptation integrates the prompts to enable the fine-tuning of the SD model for adapting to different degradation types. Besides, a high-fidelity detail enhancement module is integrated into the decoder of SD to improve structural and textural details. Experiments demonstrate that our method outperforms existing diffusion-based restoration approaches in both visual quality and inference efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5DOD\uff0c\u901a\u8fc7\u5355\u6b65\u91c7\u6837\u5b9e\u73b0\u9ad8\u6027\u80fd\u4fee\u590d\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u9002\u5e94\u6027\u6709\u9650\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u5f15\u5165\u591a\u9000\u5316\u7279\u5f81\u8c03\u5236\u548c\u53c2\u6570\u9ad8\u6548\u7684\u4f4e\u79e9\u9002\u5e94\uff0c\u7ed3\u5408\u9ad8\u4fdd\u771f\u7ec6\u8282\u589e\u5f3a\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDOD\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u63a8\u7406\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DOD\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u3002"}}
{"id": "2508.03402", "pdf": "https://arxiv.org/pdf/2508.03402", "abs": "https://arxiv.org/abs/2508.03402", "authors": ["Pingchuan Ma", "Xiaopei Yang", "Yusong Li", "Ming Gui", "Felix Krause", "Johannes Schusterbauer", "Bj\u00f6rn Ommer"], "title": "SCFlow: Implicitly Learning Style and Content Disentanglement with Flow Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ICCV 2025, Project Page: https://compvis.github.io/SCFlow/", "summary": "Explicitly disentangling style and content in vision models remains challenging due to their semantic overlap and the subjectivity of human perception. Existing methods propose separation through generative or discriminative objectives, but they still face the inherent ambiguity of disentangling intertwined concepts. Instead, we ask: Can we bypass explicit disentanglement by learning to merge style and content invertibly, allowing separation to emerge naturally? We propose SCFlow, a flow-matching framework that learns bidirectional mappings between entangled and disentangled representations. Our approach is built upon three key insights: 1) Training solely to merge style and content, a well-defined task, enables invertible disentanglement without explicit supervision; 2) flow matching bridges on arbitrary distributions, avoiding the restrictive Gaussian priors of diffusion models and normalizing flows; and 3) a synthetic dataset of 510,000 samples (51 styles $\\times$ 10,000 content samples) was curated to simulate disentanglement through systematic style-content pairing. Beyond controllable generation tasks, we demonstrate that SCFlow generalizes to ImageNet-1k and WikiArt in zero-shot settings and achieves competitive performance, highlighting that disentanglement naturally emerges from the invertible merging process.", "AI": {"tldr": "SCFlow\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u53ef\u9006\u5408\u5e76\u98ce\u683c\u4e0e\u5185\u5bb9\u7684\u65b9\u6cd5\uff0c\u907f\u514d\u663e\u5f0f\u89e3\u8026\u7684\u6311\u6218\uff0c\u5229\u7528\u6d41\u5339\u914d\u6846\u67b6\u5b9e\u73b0\u81ea\u7136\u89e3\u8026\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u98ce\u683c\u4e0e\u5185\u5bb9\u89e3\u8026\u65f6\u9762\u4e34\u8bed\u4e49\u91cd\u53e0\u548c\u4e3b\u89c2\u6027\u7684\u6311\u6218\uff0cSCFlow\u5c1d\u8bd5\u901a\u8fc7\u53ef\u9006\u5408\u5e76\u7ed5\u8fc7\u663e\u5f0f\u89e3\u8026\u7684\u56f0\u96be\u3002", "method": "SCFlow\u91c7\u7528\u6d41\u5339\u914d\u6846\u67b6\uff0c\u5b66\u4e60\u98ce\u683c\u4e0e\u5185\u5bb9\u7684\u53cc\u5411\u6620\u5c04\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u65e0\u9700\u663e\u5f0f\u76d1\u7763\u3002", "result": "SCFlow\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u6cdb\u5316\u81f3ImageNet-1k\u548cWikiArt\uff0c\u6027\u80fd\u7ade\u4e89\u6027\uff0c\u8868\u660e\u89e3\u8026\u53ef\u4ece\u53ef\u9006\u5408\u5e76\u4e2d\u81ea\u7136\u6d8c\u73b0\u3002", "conclusion": "SCFlow\u901a\u8fc7\u53ef\u9006\u5408\u5e76\u98ce\u683c\u4e0e\u5185\u5bb9\uff0c\u5b9e\u73b0\u4e86\u81ea\u7136\u89e3\u8026\uff0c\u4e3a\u53ef\u63a7\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.03437", "pdf": "https://arxiv.org/pdf/2508.03437", "abs": "https://arxiv.org/abs/2508.03437", "authors": ["Hongjun Liu", "Chao Yao", "Yalan Zhang", "Xiaokun wang", "Xiaojuan Ban"], "title": "Spatial Imputation Drives Cross-Domain Alignment for EEG Classification", "categories": ["cs.CV", "cs.AI", "62M10", "I.5.1; J.3"], "comment": "ACMMM 2025 poster", "summary": "Electroencephalogram (EEG) signal classification faces significant challenges due to data distribution shifts caused by heterogeneous electrode configurations, acquisition protocols, and hardware discrepancies across domains. This paper introduces IMAC, a novel channel-dependent mask and imputation self-supervised framework that formulates the alignment of cross-domain EEG data shifts as a spatial time series imputation task. To address heterogeneous electrode configurations in cross-domain scenarios, IMAC first standardizes different electrode layouts using a 3D-to-2D positional unification mapping strategy, establishing unified spatial representations. Unlike previous mask-based self-supervised representation learning methods, IMAC introduces spatio-temporal signal alignment. This involves constructing a channel-dependent mask and reconstruction task framed as a low-to-high resolution EEG spatial imputation problem. Consequently, this approach simulates cross-domain variations such as channel omissions and temporal instabilities, thus enabling the model to leverage the proposed imputer for robust signal alignment during inference. Furthermore, IMAC incorporates a disentangled structure that separately models the temporal and spatial information of the EEG signals separately, reducing computational complexity while enhancing flexibility and adaptability. Comprehensive evaluations across 10 publicly available EEG datasets demonstrate IMAC's superior performance, achieving state-of-the-art classification accuracy in both cross-subject and cross-center validation scenarios. Notably, IMAC shows strong robustness under both simulated and real-world distribution shifts, surpassing baseline methods by up to $35$\\% in integrity scores while maintaining consistent classification accuracy.", "AI": {"tldr": "IMAC\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u901a\u9053\u4f9d\u8d56\u63a9\u7801\u548c\u63d2\u503c\u4efb\u52a1\u89e3\u51b3\u8de8\u57dfEEG\u4fe1\u53f7\u5206\u7c7b\u4e2d\u7684\u6570\u636e\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7a7a\u95f4\u65f6\u95f4\u5e8f\u5217\u5bf9\u9f50\u3002", "motivation": "EEG\u4fe1\u53f7\u5206\u7c7b\u9762\u4e34\u56e0\u7535\u6781\u914d\u7f6e\u3001\u91c7\u96c6\u534f\u8bae\u548c\u786c\u4ef6\u5dee\u5f02\u5bfc\u81f4\u7684\u6570\u636e\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u9c81\u68d2\u7684\u65b9\u6cd5\u8fdb\u884c\u8de8\u57df\u5bf9\u9f50\u3002", "method": "IMAC\u91c7\u75283D-to-2D\u4f4d\u7f6e\u7edf\u4e00\u6620\u5c04\u6807\u51c6\u5316\u7535\u6781\u5e03\u5c40\uff0c\u63d0\u51fa\u901a\u9053\u4f9d\u8d56\u63a9\u7801\u548c\u91cd\u5efa\u4efb\u52a1\uff0c\u6a21\u62df\u8de8\u57df\u53d8\u5316\uff0c\u5e76\u901a\u8fc7\u89e3\u8026\u7ed3\u6784\u5206\u522b\u5efa\u6a21\u65f6\u7a7a\u4fe1\u606f\u3002", "result": "\u572810\u4e2a\u516c\u5f00EEG\u6570\u636e\u96c6\u4e0a\uff0cIMAC\u5728\u8de8\u4e3b\u4f53\u548c\u8de8\u4e2d\u5fc3\u9a8c\u8bc1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u9886\u5148\u57fa\u7ebf\u65b9\u6cd535%\u3002", "conclusion": "IMAC\u901a\u8fc7\u7a7a\u95f4\u65f6\u95f4\u5e8f\u5217\u5bf9\u9f50\u548c\u89e3\u8026\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86EEG\u4fe1\u53f7\u5206\u7c7b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2508.03442", "pdf": "https://arxiv.org/pdf/2508.03442", "abs": "https://arxiv.org/abs/2508.03442", "authors": ["Shangwen Zhu", "Qianyu Peng", "Yuting Hu", "Zhantao Yang", "Han Zhang", "Zhao Pu", "Ruili Feng", "Fan Cheng"], "title": "RAAG: Ratio Aware Adaptive Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Flow-based generative models have recently achieved remarkable progress in image and video synthesis, with classifier-free guidance (CFG) becoming the standard tool for high-fidelity, controllable generation. However, despite their practical success, little is known about how guidance interacts with different stages of the sampling process-especially in the fast, low-step regimes typical of modern flow-based pipelines. In this work, we uncover and analyze a fundamental instability: the earliest reverse steps are acutely sensitive to the guidance scale, owing to a pronounced spike in the relative strength (RATIO) of conditional to unconditional predictions. Through rigorous theoretical analysis and empirical validation, we show that this RATIO spike is intrinsic to the data distribution, independent of the model architecture, and causes exponential error amplification when paired with strong guidance. To address this, we propose a simple, theoretically grounded, RATIO-aware adaptive guidance schedule that automatically dampens the guidance scale at early steps based on the evolving RATIO, using a closed-form exponential decay. Our method is lightweight, requires no additional inference overhead, and is compatible with standard flow frameworks. Experiments across state-of-the-art image (SD3.5, Lumina) and video (WAN2.1) models demonstrate that our approach enables up to 3x faster sampling while maintaining or improving generation quality, robustness, and semantic alignment. Extensive ablation studies further confirm the generality and stability of our schedule across models, datasets, and hyperparameters. Our findings highlight the critical role of stepwise guidance adaptation in unlocking the full potential of fast flow-based generative models.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6d41\u5f0f\u751f\u6210\u6a21\u578b\u5728\u65e9\u671f\u91c7\u6837\u9636\u6bb5\u5bf9\u5f15\u5bfc\u5c3a\u5ea6\u654f\u611f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u5f15\u5bfc\u8c03\u5ea6\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u91c7\u6837\u901f\u5ea6\u4e0e\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u63a2\u7d22\u5f15\u5bfc\u673a\u5236\u5728\u4e0d\u540c\u91c7\u6837\u9636\u6bb5\u7684\u4f5c\u7528\uff0c\u5c24\u5176\u662f\u5feb\u901f\u4f4e\u6b65\u957f\u6a21\u5f0f\u4e0b\uff0c\u4ee5\u89e3\u51b3\u65e9\u671f\u6b65\u9aa4\u5bf9\u5f15\u5bfc\u5c3a\u5ea6\u7684\u654f\u611f\u6027\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u63d0\u51fa\u57fa\u4e8eRATIO\u7684\u81ea\u9002\u5e94\u5f15\u5bfc\u8c03\u5ea6\u65b9\u6cd5\uff0c\u81ea\u52a8\u8c03\u6574\u65e9\u671f\u6b65\u9aa4\u7684\u5f15\u5bfc\u5f3a\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u548c\u89c6\u9891\u6a21\u578b\u4e2d\u53ef\u63d0\u53473\u500d\u91c7\u6837\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u6539\u8fdb\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "\u81ea\u9002\u5e94\u5f15\u5bfc\u8c03\u5ea6\u662f\u91ca\u653e\u5feb\u901f\u6d41\u5f0f\u751f\u6210\u6a21\u578b\u6f5c\u529b\u7684\u5173\u952e\u3002"}}
{"id": "2508.03447", "pdf": "https://arxiv.org/pdf/2508.03447", "abs": "https://arxiv.org/abs/2508.03447", "authors": ["Qiyu Chen", "Zhen Qu", "Wei Luo", "Haiming Yao", "Yunkang Cao", "Yuxin Jiang", "Yinan Duan", "Huiyuan Luo", "Chengkan Lv", "Zhengtao Zhang"], "title": "CoPS: Conditional Prompt Synthesis for Zero-Shot Anomaly Detection", "categories": ["cs.CV"], "comment": "19 pages, 33 figures, 14 tables", "summary": "Recently, large pre-trained vision-language models have shown remarkable performance in zero-shot anomaly detection (ZSAD). With fine-tuning on a single auxiliary dataset, the model enables cross-category anomaly detection on diverse datasets covering industrial defects and medical lesions. Compared to manually designed prompts, prompt learning eliminates the need for expert knowledge and trial-and-error. However, it still faces the following challenges: (i) static learnable tokens struggle to capture the continuous and diverse patterns of normal and anomalous states, limiting generalization to unseen categories; (ii) fixed textual labels provide overly sparse category information, making the model prone to overfitting to a specific semantic subspace. To address these issues, we propose Conditional Prompt Synthesis (CoPS), a novel framework that synthesizes dynamic prompts conditioned on visual features to enhance ZSAD performance. Specifically, we extract representative normal and anomaly prototypes from fine-grained patch features and explicitly inject them into prompts, enabling adaptive state modeling. Given the sparsity of class labels, we leverage a variational autoencoder to model semantic image features and implicitly fuse varied class tokens into prompts. Additionally, integrated with our spatially-aware alignment mechanism, extensive experiments demonstrate that CoPS surpasses state-of-the-art methods by 2.5% AUROC in both classification and segmentation across 13 industrial and medical datasets. Code will be available at https://github.com/cqylunlun/CoPS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoPS\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u57fa\u4e8e\u89c6\u89c9\u7279\u5f81\u7684\u63d0\u793a\uff0c\u63d0\u5347\u4e86\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u4e2d\u9762\u4e34\u9759\u6001\u63d0\u793a\u65e0\u6cd5\u6355\u6349\u8fde\u7eed\u591a\u6837\u72b6\u6001\u548c\u56fa\u5b9a\u6807\u7b7e\u4fe1\u606f\u7a00\u758f\u7684\u95ee\u9898\u3002", "method": "CoPS\u901a\u8fc7\u63d0\u53d6\u6b63\u5e38\u548c\u5f02\u5e38\u539f\u578b\u5e76\u6ce8\u5165\u63d0\u793a\u4e2d\uff0c\u540c\u65f6\u5229\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u878d\u5408\u591a\u6837\u7c7b\u522b\u4fe1\u606f\u3002", "result": "\u572813\u4e2a\u5de5\u4e1a\u548c\u533b\u5b66\u6570\u636e\u96c6\u4e0a\uff0cCoPS\u5728\u5206\u7c7b\u548c\u5206\u5272\u4efb\u52a1\u4e2dAUROC\u63d0\u5347\u4e862.5%\u3002", "conclusion": "CoPS\u901a\u8fc7\u52a8\u6001\u63d0\u793a\u548c\u8bed\u4e49\u878d\u5408\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.03449", "pdf": "https://arxiv.org/pdf/2508.03449", "abs": "https://arxiv.org/abs/2508.03449", "authors": ["Xuan Dong", "Xiangyuan Sun", "Xia Wang", "Jian Song", "Ya Li", "Weixin Li"], "title": "Video Demoireing using Focused-Defocused Dual-Camera System", "categories": ["cs.CV"], "comment": null, "summary": "Moire patterns, unwanted color artifacts in images and videos, arise from the interference between spatially high-frequency scene contents and the spatial discrete sampling of digital cameras. Existing demoireing methods primarily rely on single-camera image/video processing, which faces two critical challenges: 1) distinguishing moire patterns from visually similar real textures, and 2) preserving tonal consistency and temporal coherence while removing moire artifacts. To address these issues, we propose a dual-camera framework that captures synchronized videos of the same scene: one in focus (retaining high-quality textures but may exhibit moire patterns) and one defocused (with significantly reduced moire patterns but blurred textures). We use the defocused video to help distinguish moire patterns from real texture, so as to guide the demoireing of the focused video. We propose a frame-wise demoireing pipeline, which begins with an optical flow based alignment step to address any discrepancies in displacement and occlusion between the focused and defocused frames. Then, we leverage the aligned defocused frame to guide the demoireing of the focused frame using a multi-scale CNN and a multi-dimensional training loss. To maintain tonal and temporal consistency, our final step involves a joint bilateral filter to leverage the demoireing result from the CNN as the guide to filter the input focused frame to obtain the final output. Experimental results demonstrate that our proposed framework largely outperforms state-of-the-art image and video demoireing methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u6444\u50cf\u5934\u7684\u53bb\u6469\u5c14\u7eb9\u6846\u67b6\uff0c\u901a\u8fc7\u540c\u6b65\u62cd\u6444\u805a\u7126\u548c\u6563\u7126\u89c6\u9891\uff0c\u5229\u7528\u6563\u7126\u89c6\u9891\u6307\u5bfc\u805a\u7126\u89c6\u9891\u7684\u53bb\u6469\u5c14\u7eb9\u5904\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u53bb\u6469\u5c14\u7eb9\u65b9\u6cd5\u96be\u4ee5\u533a\u5206\u6469\u5c14\u7eb9\u4e0e\u771f\u5b9e\u7eb9\u7406\uff0c\u4e14\u96be\u4ee5\u4fdd\u6301\u8272\u8c03\u4e00\u81f4\u6027\u548c\u65f6\u95f4\u8fde\u8d2f\u6027\u3002", "method": "\u4f7f\u7528\u53cc\u6444\u50cf\u5934\u540c\u6b65\u62cd\u6444\u805a\u7126\u548c\u6563\u7126\u89c6\u9891\uff0c\u901a\u8fc7\u5149\u6d41\u5bf9\u9f50\u548c\u591a\u5c3a\u5ea6CNN\u7ed3\u5408\u591a\u7ef4\u8bad\u7ec3\u635f\u5931\u8fdb\u884c\u53bb\u6469\u5c14\u7eb9\u5904\u7406\uff0c\u6700\u540e\u4f7f\u7528\u8054\u5408\u53cc\u8fb9\u6ee4\u6ce2\u4fdd\u6301\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u56fe\u50cf\u548c\u89c6\u9891\u53bb\u6469\u5c14\u7eb9\u65b9\u6cd5\u3002", "conclusion": "\u53cc\u6444\u50cf\u5934\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6469\u5c14\u7eb9\u4e0e\u771f\u5b9e\u7eb9\u7406\u7684\u533a\u5206\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8272\u8c03\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.03457", "pdf": "https://arxiv.org/pdf/2508.03457", "abs": "https://arxiv.org/abs/2508.03457", "authors": ["Haotian Wang", "Yuzhe Weng", "Jun Du", "Haoran Xu", "Xiaoyan Wu", "Shan He", "Bing Yin", "Cong Liu", "Jianqing Gao", "Qingfeng Liu"], "title": "READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven Talking Head Generation", "categories": ["cs.GR", "cs.CV", "cs.SD", "eess.AS"], "comment": "9 pages", "summary": "The introduction of diffusion models has brought significant advances to the field of audio-driven talking head generation. However, the extremely slow inference speed severely limits the practical implementation of diffusion-based talking head generation models. In this study, we propose READ, the first real-time diffusion-transformer-based talking head generation framework. Our approach first learns a spatiotemporal highly compressed video latent space via a temporal VAE, significantly reducing the token count to accelerate generation. To achieve better audio-visual alignment within this compressed latent space, a pre-trained Speech Autoencoder (SpeechAE) is proposed to generate temporally compressed speech latent codes corresponding to the video latent space. These latent representations are then modeled by a carefully designed Audio-to-Video Diffusion Transformer (A2V-DiT) backbone for efficient talking head synthesis. Furthermore, to ensure temporal consistency and accelerated inference in extended generation, we propose a novel asynchronous noise scheduler (ANS) for both the training and inference process of our framework. The ANS leverages asynchronous add-noise and asynchronous motion-guided generation in the latent space, ensuring consistency in generated video clips. Experimental results demonstrate that READ outperforms state-of-the-art methods by generating competitive talking head videos with significantly reduced runtime, achieving an optimal balance between quality and speed while maintaining robust metric stability in long-time generation.", "AI": {"tldr": "READ\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563-\u53d8\u6362\u5668\u7684\u5b9e\u65f6\u8bf4\u8bdd\u5934\u90e8\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u538b\u7f29\u89c6\u9891\u548c\u8bed\u97f3\u6f5c\u5728\u7a7a\u95f4\u53ca\u5f02\u6b65\u566a\u58f0\u8c03\u5ea6\u5668\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u901f\u5ea6\u548c\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u8bf4\u8bdd\u5934\u90e8\u751f\u6210\u4e2d\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u63a8\u7406\u901f\u5ea6\u8fc7\u6162\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u65f6\u95f4VAE\u538b\u7f29\u89c6\u9891\u6f5c\u5728\u7a7a\u95f4\uff0c\u9884\u8bad\u7ec3SpeechAE\u751f\u6210\u8bed\u97f3\u6f5c\u5728\u4ee3\u7801\uff0c\u8bbe\u8ba1A2V-DiT\u5efa\u6a21\uff0c\u5e76\u5f15\u5165\u5f02\u6b65\u566a\u58f0\u8c03\u5ea6\u5668\uff08ANS\uff09\u3002", "result": "READ\u5728\u8d28\u91cf\u548c\u901f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u957f\u65f6\u95f4\u751f\u6210\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "READ\u5728\u5b9e\u65f6\u6027\u548c\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u4e3a\u8bf4\u8bdd\u5934\u90e8\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.03458", "pdf": "https://arxiv.org/pdf/2508.03458", "abs": "https://arxiv.org/abs/2508.03458", "authors": ["Zilin Chen", "Shengnan Lu"], "title": "AVPDN: Learning Motion-Robust and Scale-Adaptive Representations for Video-Based Polyp Detection", "categories": ["cs.CV"], "comment": null, "summary": "Accurate detection of polyps is of critical importance for the early and intermediate stages of colorectal cancer diagnosis. Compared to static images, dynamic colonoscopy videos provide more comprehensive visual information, which can facilitate the development of effective treatment plans. However, unlike fixed-camera recordings, colonoscopy videos often exhibit rapid camera movement, introducing substantial background noise that disrupts the structural integrity of the scene and increases the risk of false positives. To address these challenges, we propose the Adaptive Video Polyp Detection Network (AVPDN), a robust framework for multi-scale polyp detection in colonoscopy videos. AVPDN incorporates two key components: the Adaptive Feature Interaction and Augmentation (AFIA) module and the Scale-Aware Context Integration (SACI) module. The AFIA module adopts a triple-branch architecture to enhance feature representation. It employs dense self-attention for global context modeling, sparse self-attention to mitigate the influence of low query-key similarity in feature aggregation, and channel shuffle operations to facilitate inter-branch information exchange. In parallel, the SACI module is designed to strengthen multi-scale feature integration. It utilizes dilated convolutions with varying receptive fields to capture contextual information at multiple spatial scales, thereby improving the model's denoising capability. Experiments conducted on several challenging public benchmarks demonstrate the effectiveness and generalization ability of the proposed method, achieving competitive performance in video-based polyp detection tasks.", "AI": {"tldr": "AVPDN\u662f\u4e00\u79cd\u7528\u4e8e\u7ed3\u80a0\u955c\u89c6\u9891\u4e2d\u591a\u5c3a\u5ea6\u606f\u8089\u68c0\u6d4b\u7684\u9c81\u68d2\u6846\u67b6\uff0c\u901a\u8fc7AFIA\u548cSACI\u6a21\u5757\u63d0\u5347\u7279\u5f81\u8868\u793a\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u6574\u5408\u3002", "motivation": "\u7ed3\u80a0\u955c\u89c6\u9891\u7684\u52a8\u6001\u6027\u548c\u5feb\u901f\u76f8\u673a\u79fb\u52a8\u5bfc\u81f4\u80cc\u666f\u566a\u58f0\u548c\u5047\u9633\u6027\u98ce\u9669\u589e\u52a0\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u606f\u8089\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "AVPDN\u7ed3\u5408AFIA\u6a21\u5757\uff08\u4e09\u652f\u67b6\u6784\u589e\u5f3a\u7279\u5f81\u8868\u793a\uff09\u548cSACI\u6a21\u5757\uff08\u591a\u5c3a\u5ea6\u7279\u5f81\u6574\u5408\uff09\uff0c\u5229\u7528\u81ea\u6ce8\u610f\u529b\u548c\u6269\u5f20\u5377\u79ef\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AVPDN\u5728\u89c6\u9891\u606f\u8089\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5177\u6709\u7ade\u4e89\u529b\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u89c6\u9891\u4e2d\u7684\u566a\u58f0\u548c\u5047\u9633\u6027\u95ee\u9898\u3002"}}
{"id": "2508.03483", "pdf": "https://arxiv.org/pdf/2508.03483", "abs": "https://arxiv.org/abs/2508.03483", "authors": ["Dasol Choi Jihwan Lee", "Minjae Lee", "Minsuk Kahng"], "title": "When Cars Have Stereotypes: Auditing Demographic Bias in Objects from Text-to-Image Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "While prior research on text-to-image generation has predominantly focused on biases in human depictions, we investigate a more subtle yet pervasive phenomenon: demographic bias in generated objects (e.g., cars). We introduce SODA (Stereotyped Object Diagnostic Audit), a novel framework for systematically measuring such biases. Our approach compares visual attributes of objects generated with demographic cues (e.g., \"for young people'') to those from neutral prompts, across 2,700 images produced by three state-of-the-art models (GPT Image-1, Imagen 4, and Stable Diffusion) in five object categories. Through a comprehensive analysis, we uncover strong associations between specific demographic groups and visual attributes, such as recurring color patterns prompted by gender or ethnicity cues. These patterns reflect and reinforce not only well-known stereotypes but also more subtle and unintuitive biases. We also observe that some models generate less diverse outputs, which in turn amplifies the visual disparities compared to neutral prompts. Our proposed auditing framework offers a practical approach for testing, revealing how stereotypes still remain embedded in today's generative models. We see this as an essential step toward more systematic and responsible AI development.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5bf9\u8c61\uff08\u5982\u6c7d\u8f66\uff09\u7684\u9690\u6027\u4eba\u53e3\u7edf\u8ba1\u504f\u89c1\uff0c\u63d0\u51fa\u4e86SODA\u6846\u67b6\u6765\u7cfb\u7edf\u6d4b\u91cf\u8fd9\u4e9b\u504f\u89c1\uff0c\u53d1\u73b0\u6a21\u578b\u751f\u6210\u7684\u56fe\u50cf\u4e2d\u5b58\u5728\u4e0e\u4eba\u53e3\u7edf\u8ba1\u76f8\u5173\u7684\u89c6\u89c9\u5c5e\u6027\u504f\u89c1\u3002", "motivation": "\u63a2\u7d22\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u5bf9\u8c61\u7684\u4eba\u53e3\u7edf\u8ba1\u504f\u89c1\uff0c\u63ed\u793a\u6a21\u578b\u5982\u4f55\u53cd\u6620\u548c\u5f3a\u5316\u523b\u677f\u5370\u8c61\u3002", "method": "\u5f15\u5165SODA\u6846\u67b6\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e2d\u6027\u63d0\u793a\u548c\u4eba\u53e3\u7edf\u8ba1\u63d0\u793a\u751f\u6210\u7684\u56fe\u50cf\uff08\u51712,700\u5f20\uff09\uff0c\u5206\u6790\u89c6\u89c9\u5c5e\u6027\u7684\u5dee\u5f02\u3002", "result": "\u53d1\u73b0\u4eba\u53e3\u7edf\u8ba1\u63d0\u793a\u5bfc\u81f4\u7279\u5b9a\u89c6\u89c9\u5c5e\u6027\uff08\u5982\u989c\u8272\u6a21\u5f0f\uff09\u7684\u504f\u89c1\uff0c\u67d0\u4e9b\u6a21\u578b\u751f\u6210\u591a\u6837\u6027\u8f83\u4f4e\uff0c\u52a0\u5267\u4e86\u89c6\u89c9\u5dee\u5f02\u3002", "conclusion": "SODA\u6846\u67b6\u4e3a\u63ed\u793a\u751f\u6210\u6a21\u578b\u4e2d\u7684\u523b\u677f\u5370\u8c61\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5\uff0c\u662f\u8fc8\u5411\u66f4\u8d1f\u8d23\u4efbAI\u5f00\u53d1\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2508.03485", "pdf": "https://arxiv.org/pdf/2508.03485", "abs": "https://arxiv.org/abs/2508.03485", "authors": ["Lianwei Yang", "Haokun Lin", "Tianchen Zhao", "Yichen Wu", "Hongyu Zhu", "Ruiqi Xie", "Zhenan Sun", "Yu Wang", "Qingyi Gu"], "title": "LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion Transformers (DiTs) have achieved impressive performance in text-to-image generation. However, their high computational cost and large parameter sizes pose significant challenges for usage in resource-constrained scenarios. Post-training quantization (PTQ) is a promising solution to reduce memory usage and accelerate inference, but existing PTQ methods suffer from severe performance degradation under extreme low-bit settings. We identify two key obstacles to low-bit post-training quantization for DiT models: (1) model weights follow a Gaussian-like distribution with long tails, causing uniform quantization to poorly allocate intervals and leading to significant errors; (2) two types of activation outliers: (i) Mild Outliers with slightly elevated values, and (ii) Salient Outliers with large magnitudes concentrated in specific channels, which disrupt activation quantization. To address these issues, we propose LRQ-DiT, an efficient and accurate PTQ framework. We introduce Twin-Log Quantization (TLQ), a log-based method that aligns well with the weight distribution and reduces quantization errors. We also propose an Adaptive Rotation Scheme (ARS) that dynamically applies Hadamard or outlier-aware rotations based on activation fluctuation, effectively mitigating the impact of both types of outliers. We evaluate LRQ-DiT on PixArt and FLUX under various bit-width settings, and validate the performance on COCO, MJHQ, and sDCI datasets. LRQ-DiT achieves low-bit quantization of DiT models while preserving image quality, outperforming existing PTQ baselines.", "AI": {"tldr": "LRQ-DiT\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4f4e\u6bd4\u7279\u540e\u8bad\u7ec3\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7Twin-Log\u91cf\u5316\u548c\u81ea\u9002\u5e94\u65cb\u8f6c\u65b9\u6848\uff0c\u89e3\u51b3\u4e86DiT\u6a21\u578b\u5728\u6781\u7aef\u4f4e\u6bd4\u7279\u91cf\u5316\u4e2d\u7684\u6743\u91cd\u5206\u5e03\u548c\u6fc0\u6d3b\u5f02\u5e38\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "DiT\u6a21\u578b\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u53c2\u6570\u91cf\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e2d\u96be\u4ee5\u5e94\u7528\u3002\u73b0\u6709PTQ\u65b9\u6cd5\u5728\u6781\u7aef\u4f4e\u6bd4\u7279\u91cf\u5316\u4e0b\u6027\u80fd\u4e0b\u964d\u4e25\u91cd\u3002", "method": "\u63d0\u51faLRQ-DiT\u6846\u67b6\uff0c\u5305\u62ecTwin-Log\u91cf\u5316\uff08TLQ\uff09\u548c\u81ea\u9002\u5e94\u65cb\u8f6c\u65b9\u6848\uff08ARS\uff09\uff0c\u5206\u522b\u9488\u5bf9\u6743\u91cd\u5206\u5e03\u548c\u6fc0\u6d3b\u5f02\u5e38\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728PixArt\u3001FLUX\u7b49\u6a21\u578b\u53ca\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cLRQ-DiT\u5728\u4f4e\u6bd4\u7279\u91cf\u5316\u4e0b\u4fdd\u6301\u4e86\u56fe\u50cf\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709PTQ\u57fa\u7ebf\u3002", "conclusion": "LRQ-DiT\u4e3aDiT\u6a21\u578b\u7684\u4f4e\u6bd4\u7279\u91cf\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8d44\u6e90\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2508.03539", "pdf": "https://arxiv.org/pdf/2508.03539", "abs": "https://arxiv.org/abs/2508.03539", "authors": ["Long Qian", "Bingke Zhu", "Yingying Chen", "Ming Tang", "Jinqiao Wang"], "title": "Quality-Aware Language-Conditioned Local Auto-Regressive Anomaly Synthesis and Detection", "categories": ["cs.CV"], "comment": null, "summary": "Despite substantial progress in anomaly synthesis methods, existing diffusion-based and coarse inpainting pipelines commonly suffer from structural deficiencies such as micro-structural discontinuities, limited semantic controllability, and inefficient generation. To overcome these limitations, we introduce ARAS, a language-conditioned, auto-regressive anomaly synthesis approach that precisely injects local, text-specified defects into normal images via token-anchored latent editing. Leveraging a hard-gated auto-regressive operator and a training-free, context-preserving masked sampling kernel, ARAS significantly enhances defect realism, preserves fine-grained material textures, and provides continuous semantic control over synthesized anomalies. Integrated within our Quality-Aware Re-weighted Anomaly Detection (QARAD) framework, we further propose a dynamic weighting strategy that emphasizes high-quality synthetic samples by computing an image-text similarity score with a dual-encoder model. Extensive experiments across three benchmark datasets-MVTec AD, VisA, and BTAD, demonstrate that our QARAD outperforms SOTA methods in both image- and pixel-level anomaly detection tasks, achieving improved accuracy, robustness, and a 5 times synthesis speedup compared to diffusion-based alternatives. Our complete code and synthesized dataset will be publicly available.", "AI": {"tldr": "ARAS\u662f\u4e00\u79cd\u8bed\u8a00\u6761\u4ef6\u81ea\u56de\u5f52\u5f02\u5e38\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ee4\u724c\u951a\u5b9a\u7684\u6f5c\u5728\u7f16\u8f91\u7cbe\u786e\u6ce8\u5165\u5c40\u90e8\u7f3a\u9677\uff0c\u7ed3\u5408QARAD\u6846\u67b6\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6269\u6563\u548c\u7c97\u4fee\u590d\u65b9\u6cd5\u5728\u7ed3\u6784\u7f3a\u9677\u3001\u8bed\u4e49\u63a7\u5236\u548c\u751f\u6210\u6548\u7387\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u8bed\u8a00\u6761\u4ef6\u81ea\u56de\u5f52\u65b9\u6cd5\uff0c\u7ed3\u5408\u786c\u95e8\u63a7\u81ea\u56de\u5f52\u64cd\u4f5c\u7b26\u548c\u65e0\u8bad\u7ec3\u4e0a\u4e0b\u6587\u4fdd\u7559\u63a9\u7801\u91c7\u6837\u6838\u3002", "result": "\u5728MVTec AD\u3001VisA\u548cBTAD\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5408\u6210\u901f\u5ea6\u63d0\u53475\u500d\u3002", "conclusion": "ARAS\u548cQARAD\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u5408\u6210\u7684\u8d28\u91cf\u548c\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2508.03542", "pdf": "https://arxiv.org/pdf/2508.03542", "abs": "https://arxiv.org/abs/2508.03542", "authors": ["Dmitrii Korzh", "Dmitrii Tarasov", "Artyom Iudin", "Elvir Karimov", "Matvey Skripkin", "Nikita Kuzmin", "Andrey Kuznetsov", "Oleg Y. Rogov", "Ivan Oseledets"], "title": "Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations and Sentences", "categories": ["cs.CV"], "comment": null, "summary": "Conversion of spoken mathematical expressions is a challenging task that involves transcribing speech into a strictly structured symbolic representation while addressing the ambiguity inherent in the pronunciation of equations. Although significant progress has been achieved in automatic speech recognition (ASR) and language models (LM), the problem of converting spoken mathematics into LaTeX remains underexplored. This task directly applies to educational and research domains, such as lecture transcription or note creation. Based on ASR post-correction, prior work requires 2 transcriptions, focuses only on isolated equations, has a limited test set, and provides neither training data nor multilingual coverage. To address these issues, we present the first fully open-source large-scale dataset, comprising over 66,000 human-annotated audio samples of mathematical equations and sentences in both English and Russian, drawn from diverse scientific domains. In addition to the ASR post-correction models and few-shot prompting, we apply audio language models, demonstrating comparable character error rate (CER) results on the MathSpeech benchmark (28% vs. 30%) for the equations conversion. In contrast, on the proposed S2L-equations benchmark, our models outperform the MathSpeech model by a substantial margin of more than 40 percentage points, even after accounting for LaTeX formatting artifacts (27% vs. 64%). We establish the first benchmark for mathematical sentence recognition (S2L-sentences) and achieve an equation CER of 40%. This work lays the groundwork for future advances in multimodal AI, with a particular focus on mathematical content recognition.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f00\u6e90\u6570\u636e\u96c6\u548c\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c06\u53e3\u8bed\u6570\u5b66\u8868\u8fbe\u5f0f\u8f6c\u6362\u4e3aLaTeX\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u53e3\u8bed\u6570\u5b66\u8868\u8fbe\u5f0f\u8f6c\u6362\u4e3aLaTeX\u7684\u6311\u6218\uff0c\u586b\u8865\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u96c6\u89c4\u6a21\u3001\u591a\u8bed\u8a00\u652f\u6301\u548c\u5e94\u7528\u573a\u666f\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8eASR\u540e\u6821\u6b63\u6a21\u578b\u548c\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u5408\u5f00\u6e90\u6570\u636e\u96c6\uff0866,000\u4e2a\u6807\u6ce8\u6837\u672c\uff09\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "result": "\u5728MathSpeech\u57fa\u51c6\u6d4b\u8bd5\u4e2dCER\u4e3a28%\uff0c\u5728S2L-equations\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8eMathSpeech\u6a21\u578b\uff0827% vs. 64%\uff09\uff0c\u5e76\u5728S2L-sentences\u57fa\u51c6\u6d4b\u8bd5\u4e2dCER\u4e3a40%\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u591a\u6a21\u6001AI\u5728\u6570\u5b66\u5185\u5bb9\u8bc6\u522b\u9886\u57df\u7684\u672a\u6765\u8fdb\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.03643", "pdf": "https://arxiv.org/pdf/2508.03643", "abs": "https://arxiv.org/abs/2508.03643", "authors": ["Xiangyu Sun", "Haoyi jiang", "Liu Liu", "Seungtae Nam", "Gyeongjin Kang", "Xinjie wang", "Wei Sui", "Zhizhong Su", "Wenyu Liu", "Xinggang Wang", "Eunbyung Park"], "title": "Uni3R: Unified 3D Reconstruction and Semantic Understanding via Generalizable Gaussian Splatting from Unposed Multi-View Images", "categories": ["cs.CV"], "comment": "The code is available at https://github.com/HorizonRobotics/Uni3R", "summary": "Reconstructing and semantically interpreting 3D scenes from sparse 2D views remains a fundamental challenge in computer vision. Conventional methods often decouple semantic understanding from reconstruction or necessitate costly per-scene optimization, thereby restricting their scalability and generalizability. In this paper, we introduce Uni3R, a novel feed-forward framework that jointly reconstructs a unified 3D scene representation enriched with open-vocabulary semantics, directly from unposed multi-view images. Our approach leverages a Cross-View Transformer to robustly integrate information across arbitrary multi-view inputs, which then regresses a set of 3D Gaussian primitives endowed with semantic feature fields. This unified representation facilitates high-fidelity novel view synthesis, open-vocabulary 3D semantic segmentation, and depth prediction, all within a single, feed-forward pass. Extensive experiments demonstrate that Uni3R establishes a new state-of-the-art across multiple benchmarks, including 25.07 PSNR on RE10K and 55.84 mIoU on ScanNet. Our work signifies a novel paradigm towards generalizable, unified 3D scene reconstruction and understanding. The code is available at https://github.com/HorizonRobotics/Uni3R.", "AI": {"tldr": "Uni3R\u662f\u4e00\u4e2a\u65b0\u7684\u524d\u9988\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u65e0\u59ff\u6001\u591a\u89c6\u56fe\u56fe\u50cf\u4e2d\u8054\u5408\u91cd\u5efa\u5e26\u6709\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u7684\u7edf\u4e003D\u573a\u666f\u8868\u793a\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u89e3\u8026\u548c\u4f18\u5316\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u57283D\u573a\u666f\u91cd\u5efa\u4e2d\u8bed\u4e49\u7406\u89e3\u4e0e\u91cd\u5efa\u89e3\u8026\u6216\u9700\u8981\u6602\u8d35\u4f18\u5316\u7684\u95ee\u9898\uff0c\u63d0\u5347\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u6027\u3002", "method": "\u5229\u7528\u8de8\u89c6\u56feTransformer\u6574\u5408\u591a\u89c6\u56fe\u4fe1\u606f\uff0c\u56de\u5f52\u5e26\u6709\u8bed\u4e49\u7279\u5f81\u573a\u76843D\u9ad8\u65af\u57fa\u5143\uff0c\u5b9e\u73b0\u7edf\u4e00\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0SOTA\uff0c\u5982RE10K\u4e0a25.07 PSNR\u548cScanNet\u4e0a55.84 mIoU\u3002", "conclusion": "Uni3R\u4e3a\u53ef\u6cdb\u5316\u7684\u7edf\u4e003D\u573a\u666f\u91cd\u5efa\u548c\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.03692", "pdf": "https://arxiv.org/pdf/2508.03692", "abs": "https://arxiv.org/abs/2508.03692", "authors": ["Ao Liang", "Youquan Liu", "Yu Yang", "Dongyue Lu", "Linfeng Li", "Lingdong Kong", "Huaici Zhao", "Wei Tsang Ooi"], "title": "LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences", "categories": ["cs.CV", "cs.RO"], "comment": "Preprint; 28 pages, 18 figures, 12 tables; Project Page at   https://lidarcrafter.github.io", "summary": "Generative world models have become essential data engines for autonomous driving, yet most existing efforts focus on videos or occupancy grids, overlooking the unique LiDAR properties. Extending LiDAR generation to dynamic 4D world modeling presents challenges in controllability, temporal coherence, and evaluation standardization. To this end, we present LiDARCrafter, a unified framework for 4D LiDAR generation and editing. Given free-form natural language inputs, we parse instructions into ego-centric scene graphs, which condition a tri-branch diffusion network to generate object structures, motion trajectories, and geometry. These structured conditions enable diverse and fine-grained scene editing. Additionally, an autoregressive module generates temporally coherent 4D LiDAR sequences with smooth transitions. To support standardized evaluation, we establish a comprehensive benchmark with diverse metrics spanning scene-, object-, and sequence-level aspects. Experiments on the nuScenes dataset using this benchmark demonstrate that LiDARCrafter achieves state-of-the-art performance in fidelity, controllability, and temporal consistency across all levels, paving the way for data augmentation and simulation. The code and benchmark are released to the community.", "AI": {"tldr": "LiDARCrafter\u662f\u4e00\u4e2a\u7edf\u4e00\u76844D LiDAR\u751f\u6210\u548c\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u751f\u6210\u52a8\u6001\u573a\u666f\uff0c\u5e76\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u4e16\u754c\u6a21\u578b\u591a\u5173\u6ce8\u89c6\u9891\u6216\u5360\u7528\u7f51\u683c\uff0c\u5ffd\u7565\u4e86LiDAR\u7279\u6027\uff0c\u52a8\u60014D LiDAR\u5efa\u6a21\u5728\u53ef\u63a7\u6027\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u8bc4\u4f30\u6807\u51c6\u5316\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "LiDARCrafter\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u89e3\u6790\u4e3a\u573a\u666f\u56fe\uff0c\u901a\u8fc7\u4e09\u5206\u652f\u6269\u6563\u7f51\u7edc\u751f\u6210\u5bf9\u8c61\u7ed3\u6784\u3001\u8fd0\u52a8\u8f68\u8ff9\u548c\u51e0\u4f55\u5f62\u72b6\uff0c\u5e76\u5229\u7528\u81ea\u56de\u5f52\u6a21\u5757\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u76844D LiDAR\u5e8f\u5217\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLiDARCrafter\u5728\u4fdd\u771f\u5ea6\u3001\u53ef\u63a7\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u8fbe\u5230\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "LiDARCrafter\u4e3a\u6570\u636e\u589e\u5f3a\u548c\u4eff\u771f\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u4ee3\u7801\u548c\u57fa\u51c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.03694", "pdf": "https://arxiv.org/pdf/2508.03694", "abs": "https://arxiv.org/abs/2508.03694", "authors": ["Jianxiong Gao", "Zhaoxi Chen", "Xian Liu", "Jianfeng Feng", "Chenyang Si", "Yanwei Fu", "Yu Qiao", "Ziwei Liu"], "title": "LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation", "categories": ["cs.CV"], "comment": "Project page: https://vchitect.github.io/LongVie-project/", "summary": "Controllable ultra-long video generation is a fundamental yet challenging task. Although existing methods are effective for short clips, they struggle to scale due to issues such as temporal inconsistency and visual degradation. In this paper, we initially investigate and identify three key factors: separate noise initialization, independent control signal normalization, and the limitations of single-modality guidance. To address these issues, we propose LongVie, an end-to-end autoregressive framework for controllable long video generation. LongVie introduces two core designs to ensure temporal consistency: 1) a unified noise initialization strategy that maintains consistent generation across clips, and 2) global control signal normalization that enforces alignment in the control space throughout the entire video. To mitigate visual degradation, LongVie employs 3) a multi-modal control framework that integrates both dense (e.g., depth maps) and sparse (e.g., keypoints) control signals, complemented by 4) a degradation-aware training strategy that adaptively balances modality contributions over time to preserve visual quality. We also introduce LongVGenBench, a comprehensive benchmark consisting of 100 high-resolution videos spanning diverse real-world and synthetic environments, each lasting over one minute. Extensive experiments show that LongVie achieves state-of-the-art performance in long-range controllability, consistency, and quality.", "AI": {"tldr": "LongVie\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u7528\u4e8e\u53ef\u63a7\u957f\u89c6\u9891\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u89c6\u9891\u4e2d\u7684\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u9000\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u77ed\u7247\u6bb5\u751f\u6210\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u957f\u89c6\u9891\u4e2d\u9762\u4e34\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u9000\u5316\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faLongVie\u6846\u67b6\uff0c\u91c7\u7528\u7edf\u4e00\u566a\u58f0\u521d\u59cb\u5316\u3001\u5168\u5c40\u63a7\u5236\u4fe1\u53f7\u5f52\u4e00\u5316\u3001\u591a\u6a21\u6001\u63a7\u5236\u6846\u67b6\u548c\u9000\u5316\u611f\u77e5\u8bad\u7ec3\u7b56\u7565\u3002", "result": "LongVie\u5728\u957f\u8303\u56f4\u53ef\u63a7\u6027\u3001\u4e00\u81f4\u6027\u548c\u8d28\u91cf\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "LongVie\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u751f\u6210\u7684\u6311\u6218\uff0c\u5e76\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2508.03073", "pdf": "https://arxiv.org/pdf/2508.03073", "abs": "https://arxiv.org/abs/2508.03073", "authors": ["Bo Zhang", "JianFei Huo", "Zheng Zhang", "Wufan Wang", "Hui Gao", "Xiangyang Gong", "Wendong Wang"], "title": "Nexus-INR: Diverse Knowledge-guided Arbitrary-Scale Multimodal Medical Image Super-Resolution", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Arbitrary-resolution super-resolution (ARSR) provides crucial flexibility for medical image analysis by adapting to diverse spatial resolutions. However, traditional CNN-based methods are inherently ill-suited for ARSR, as they are typically designed for fixed upsampling factors. While INR-based methods overcome this limitation, they still struggle to effectively process and leverage multi-modal images with varying resolutions and details. In this paper, we propose Nexus-INR, a Diverse Knowledge-guided ARSR framework, which employs varied information and downstream tasks to achieve high-quality, adaptive-resolution medical image super-resolution. Specifically, Nexus-INR contains three key components. A dual-branch encoder with an auxiliary classification task to effectively disentangle shared anatomical structures and modality-specific features; a knowledge distillation module using cross-modal attention that guides low-resolution modality reconstruction with high-resolution reference, enhanced by self-supervised consistency loss; an integrated segmentation module that embeds anatomical semantics to improve both reconstruction quality and downstream segmentation performance. Experiments on the BraTS2020 dataset for both super-resolution and downstream segmentation demonstrate that Nexus-INR outperforms state-of-the-art methods across various metrics.", "AI": {"tldr": "Nexus-INR\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u6837\u5316\u77e5\u8bc6\u5f15\u5bfc\u7684\u4efb\u610f\u5206\u8fa8\u7387\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u7f16\u7801\u5668\u3001\u77e5\u8bc6\u84b8\u998f\u6a21\u5757\u548c\u96c6\u6210\u5206\u5272\u6a21\u5757\uff0c\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u548c\u4e0b\u6e38\u5206\u5272\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfCNN\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u4efb\u610f\u5206\u8fa8\u7387\u8d85\u5206\u8fa8\u7387\uff08ARSR\uff09\uff0c\u800c\u57fa\u4e8eINR\u7684\u65b9\u6cd5\u867d\u89e3\u51b3\u4e86\u56fa\u5b9a\u4e0a\u91c7\u6837\u95ee\u9898\uff0c\u4f46\u5728\u5904\u7406\u591a\u6a21\u6001\u56fe\u50cf\u65f6\u4ecd\u6709\u5c40\u9650\u3002", "method": "Nexus-INR\u5305\u542b\u53cc\u5206\u652f\u7f16\u7801\u5668\uff08\u5206\u79bb\u89e3\u5256\u7ed3\u6784\u548c\u6a21\u6001\u7279\u5f81\uff09\u3001\u77e5\u8bc6\u84b8\u998f\u6a21\u5757\uff08\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5f15\u5bfc\u91cd\u5efa\uff09\u548c\u96c6\u6210\u5206\u5272\u6a21\u5757\uff08\u5d4c\u5165\u89e3\u5256\u8bed\u4e49\uff09\u3002", "result": "\u5728BraTS2020\u6570\u636e\u96c6\u4e0a\uff0cNexus-INR\u5728\u8d85\u5206\u8fa8\u7387\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Nexus-INR\u901a\u8fc7\u591a\u6837\u5316\u77e5\u8bc6\u5f15\u5bfc\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u533b\u5b66\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2508.03221", "pdf": "https://arxiv.org/pdf/2508.03221", "abs": "https://arxiv.org/abs/2508.03221", "authors": ["Yu Pan", "Jiahao Chen", "Lin Wang", "Bingrong Dai", "Yi Du"], "title": "BadBlocks: Low-Cost and Stealthy Backdoor Attacks Tailored for Text-to-Image Diffusion Models", "categories": ["cs.CR", "cs.CV"], "comment": null, "summary": "In recent years,Diffusion models have achieved remarkable progress in the field of image generation.However,recent studies have shown that diffusion models are susceptible to backdoor attacks,in which attackers can manipulate the output by injecting covert triggers such as specific visual patterns or textual phrases into the training dataset.Fortunately,with the continuous advancement of defense techniques,defenders have become increasingly capable of identifying and mitigating most backdoor attacks using visual inspection and neural network-based detection methods.However,in this paper,we identify a novel type of backdoor threat that is more lightweight and covert than existing approaches,which we name BadBlocks,requires only about 30\\% of the computational resources and 20\\% GPU time typically needed by previous backdoor attacks,yet it successfully injects backdoors and evades the most advanced defense frameworks.BadBlocks enables attackers to selectively contaminate specific blocks within the UNet architecture of diffusion models while maintaining normal functionality in the remaining components.Experimental results demonstrate that BadBlocks achieves a high attack success rate (ASR) and low perceptual quality loss (as measured by FID Score),even under extremely constrained computational resources and GPU time.Moreover,BadBlocks is able to bypass existing defense frameworks,especially the attention-based backdoor detection method, highlighting it as a novel and noteworthy threat.Ablation studies further demonstrate that effective backdoor injection does not require fine-tuning the entire network and highlight the pivotal role of certain neural network layers in backdoor mapping.Overall,BadBlocks significantly reduces the barrier to conducting backdoor attacks in all aspects.It enables attackers to inject backdoors into large-scale diffusion models even using consumer-grade GPUs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u8f7b\u91cf\u7ea7\u540e\u95e8\u653b\u51fb\u65b9\u6cd5BadBlocks\uff0c\u80fd\u5728\u6269\u6563\u6a21\u578b\u4e2d\u9ad8\u6548\u6ce8\u5165\u540e\u95e8\u5e76\u7ed5\u8fc7\u73b0\u6709\u9632\u5fa1\u6846\u67b6\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u6613\u53d7\u540e\u95e8\u653b\u51fb\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5df2\u80fd\u5e94\u5bf9\u591a\u6570\u653b\u51fb\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u66f4\u8f7b\u91cf\u3001\u9690\u853d\u653b\u51fb\u7684\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u9009\u62e9\u6027\u6c61\u67d3UNet\u67b6\u6784\u7684\u7279\u5b9a\u6a21\u5757\uff0cBadBlocks\u4ee5\u4f4e\u8ba1\u7b97\u8d44\u6e90\uff0830%\u8ba1\u7b97\u91cf\uff0c20%GPU\u65f6\u95f4\uff09\u5b9e\u73b0\u540e\u95e8\u6ce8\u5165\u3002", "result": "\u5b9e\u9a8c\u663e\u793aBadBlocks\u653b\u51fb\u6210\u529f\u7387\u9ad8\uff08ASR\uff09\uff0c\u611f\u77e5\u8d28\u91cf\u635f\u5931\u4f4e\uff08FID\u8bc4\u5206\uff09\uff0c\u5e76\u80fd\u7ed5\u8fc7\u5148\u8fdb\u9632\u5fa1\u6846\u67b6\u3002", "conclusion": "BadBlocks\u663e\u8457\u964d\u4f4e\u4e86\u540e\u95e8\u653b\u51fb\u95e8\u69db\uff0c\u653b\u51fb\u8005\u53ef\u7528\u6d88\u8d39\u7ea7GPU\u5bf9\u5927\u89c4\u6a21\u6269\u6563\u6a21\u578b\u6ce8\u5165\u540e\u95e8\u3002"}}
{"id": "2508.03291", "pdf": "https://arxiv.org/pdf/2508.03291", "abs": "https://arxiv.org/abs/2508.03291", "authors": ["Hengxin Ruan", "Qiufan Lin", "Shupei Chen", "Yang Wang", "Wei Zhang"], "title": "Investigation on deep learning-based galaxy image translation models", "categories": ["astro-ph.IM", "astro-ph.GA", "cs.CV"], "comment": "Accepted at A&A; 18+6 pages; 12+6 figures", "summary": "Galaxy image translation is an important application in galaxy physics and cosmology. With deep learning-based generative models, image translation has been performed for image generation, data quality enhancement, information extraction, and generalized for other tasks such as deblending and anomaly detection. However, most endeavors on image translation primarily focus on the pixel-level and morphology-level statistics of galaxy images. There is a lack of discussion on the preservation of complex high-order galaxy physical information, which would be more challenging but crucial for studies that rely on high-fidelity image translation. Therefore, we investigated the effectiveness of generative models in preserving high-order physical information (represented by spectroscopic redshift) along with pixel-level and morphology-level information. We tested four representative models, i.e. a Swin Transformer, an SRGAN, a capsule network, and a diffusion model, using the SDSS and CFHTLS galaxy images. We found that these models show different levels of incapabilities in retaining redshift information, even if the global structures of galaxies and morphology-level statistics can be roughly reproduced. In particular, the cross-band peak fluxes of galaxies were found to contain meaningful redshift information, whereas they are subject to noticeable uncertainties in the translation of images, which may substantially be due to the nature of many-to-many mapping. Nonetheless, imperfect translated images may still contain a considerable amount of information and thus hold promise for downstream applications for which high image fidelity is not strongly required. Our work can facilitate further research on how complex physical information is manifested on galaxy images, and it provides implications on the development of image translation models for scientific use.", "AI": {"tldr": "\u7814\u7a76\u4e86\u751f\u6210\u6a21\u578b\u5728\u661f\u7cfb\u56fe\u50cf\u7ffb\u8bd1\u4e2d\u4fdd\u7559\u9ad8\u9636\u7269\u7406\u4fe1\u606f\uff08\u5982\u7ea2\u79fb\uff09\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u4fdd\u7559\u7ea2\u79fb\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4f46\u4ecd\u5bf9\u67d0\u4e9b\u5e94\u7528\u6709\u4ef7\u503c\u3002", "motivation": "\u661f\u7cfb\u56fe\u50cf\u7ffb\u8bd1\u5728\u661f\u7cfb\u7269\u7406\u548c\u5b87\u5b99\u5b66\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u50cf\u7d20\u548c\u5f62\u6001\u7edf\u8ba1\uff0c\u7f3a\u4e4f\u5bf9\u9ad8\u9636\u7269\u7406\u4fe1\u606f\u4fdd\u7559\u7684\u8ba8\u8bba\u3002", "method": "\u6d4b\u8bd5\u4e86\u56db\u79cd\u751f\u6210\u6a21\u578b\uff08Swin Transformer\u3001SRGAN\u3001\u80f6\u56ca\u7f51\u7edc\u3001\u6269\u6563\u6a21\u578b\uff09\u5728SDSS\u548cCFHTLS\u661f\u7cfb\u56fe\u50cf\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6a21\u578b\u5728\u4fdd\u7559\u7ea2\u79fb\u4fe1\u606f\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u8de8\u6ce2\u6bb5\u5cf0\u503c\u901a\u91cf\u5305\u542b\u6709\u7528\u4fe1\u606f\uff0c\u7ffb\u8bd1\u56fe\u50cf\u4ecd\u53ef\u7528\u4e8e\u67d0\u4e9b\u4e0b\u6e38\u4efb\u52a1\u3002", "conclusion": "\u7814\u7a76\u4e3a\u661f\u7cfb\u56fe\u50cf\u4e2d\u590d\u6742\u7269\u7406\u4fe1\u606f\u7684\u4f53\u73b0\u63d0\u4f9b\u4e86\u542f\u793a\uff0c\u5e76\u6307\u5bfc\u79d1\u5b66\u7528\u9014\u7684\u56fe\u50cf\u7ffb\u8bd1\u6a21\u578b\u5f00\u53d1\u3002"}}
{"id": "2508.03594", "pdf": "https://arxiv.org/pdf/2508.03594", "abs": "https://arxiv.org/abs/2508.03594", "authors": ["Ana Lawry Aguila", "Ayodeji Ijishakin", "Juan Eugenio Iglesias", "Tomomi Takenaga", "Yukihiro Nomura", "Takeharu Yoshikawa", "Osamu Abe", "Shouhei Hanaoka"], "title": "CADD: Context aware disease deviations via restoration of brain images using normative conditional diffusion models", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Applying machine learning to real-world medical data, e.g. from hospital archives, has the potential to revolutionize disease detection in brain images. However, detecting pathology in such heterogeneous cohorts is a difficult challenge. Normative modeling, a form of unsupervised anomaly detection, offers a promising approach to studying such cohorts where the ``normal'' behavior is modeled and can be used at subject level to detect deviations relating to disease pathology. Diffusion models have emerged as powerful tools for anomaly detection due to their ability to capture complex data distributions and generate high-quality images. Their performance relies on image restoration; differences between the original and restored images highlight potential abnormalities. However, unlike normative models, these diffusion model approaches do not incorporate clinical information which provides important context to guide the disease detection process. Furthermore, standard approaches often poorly restore healthy regions, resulting in poor reconstructions and suboptimal detection performance. We present CADD, the first conditional diffusion model for normative modeling in 3D images. To guide the healthy restoration process, we propose a novel inference inpainting strategy which balances anomaly removal with retention of subject-specific features. Evaluated on three challenging datasets, including clinical scans, which may have lower contrast, thicker slices, and motion artifacts, CADD achieves state-of-the-art performance in detecting neurological abnormalities in heterogeneous cohorts.", "AI": {"tldr": "CADD\u662f\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u76843D\u56fe\u50cf\u89c4\u8303\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u4e34\u5e8a\u4fe1\u606f\u548c\u521b\u65b0\u7684\u4fee\u590d\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f02\u8d28\u6027\u961f\u5217\u4e2d\u795e\u7ecf\u5f02\u5e38\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5229\u7528\u673a\u5668\u5b66\u4e60\u5206\u6790\u771f\u5b9e\u4e16\u754c\u533b\u5b66\u6570\u636e\uff08\u5982\u533b\u9662\u6863\u6848\uff09\u6709\u671b\u9769\u65b0\u8111\u90e8\u56fe\u50cf\u7684\u75be\u75c5\u68c0\u6d4b\uff0c\u4f46\u5f02\u8d28\u6027\u961f\u5217\u4e2d\u7684\u75c5\u7406\u68c0\u6d4b\u5177\u6709\u6311\u6218\u6027\u3002\u89c4\u8303\u5efa\u6a21\u548c\u6269\u6563\u6a21\u578b\u4e3a\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u4e34\u5e8a\u4fe1\u606f\u6574\u5408\u4e14\u4fee\u590d\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faCADD\uff0c\u9996\u4e2a\u7528\u4e8e3D\u56fe\u50cf\u89c4\u8303\u5efa\u6a21\u7684\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u91c7\u7528\u521b\u65b0\u7684\u63a8\u7406\u4fee\u590d\u7b56\u7565\uff0c\u5e73\u8861\u5f02\u5e38\u53bb\u9664\u4e0e\u4fdd\u7559\u4e2a\u4f53\u7279\u5f81\u3002", "result": "\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\uff08\u5305\u62ec\u4f4e\u5bf9\u6bd4\u5ea6\u3001\u539a\u5207\u7247\u548c\u8fd0\u52a8\u4f2a\u5f71\u7684\u4e34\u5e8a\u626b\u63cf\uff09\u4e0a\uff0cCADD\u5b9e\u73b0\u4e86\u795e\u7ecf\u5f02\u5e38\u68c0\u6d4b\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "CADD\u901a\u8fc7\u7ed3\u5408\u4e34\u5e8a\u4fe1\u606f\u548c\u4f18\u5316\u4fee\u590d\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f02\u8d28\u6027\u961f\u5217\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u80fd\u529b\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.03645", "pdf": "https://arxiv.org/pdf/2508.03645", "abs": "https://arxiv.org/abs/2508.03645", "authors": ["Akshay L Chandra", "Iman Nematollahi", "Chenguang Huang", "Tim Welschehold", "Wolfram Burgard", "Abhinav Valada"], "title": "DiWA: Diffusion Policy Adaptation with World Models", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Accepted at the 2025 Conference on Robot Learning (CoRL)", "summary": "Fine-tuning diffusion policies with reinforcement learning (RL) presents significant challenges. The long denoising sequence for each action prediction impedes effective reward propagation. Moreover, standard RL methods require millions of real-world interactions, posing a major bottleneck for practical fine-tuning. Although prior work frames the denoising process in diffusion policies as a Markov Decision Process to enable RL-based updates, its strong dependence on environment interaction remains highly inefficient. To bridge this gap, we introduce DiWA, a novel framework that leverages a world model for fine-tuning diffusion-based robotic skills entirely offline with reinforcement learning. Unlike model-free approaches that require millions of environment interactions to fine-tune a repertoire of robot skills, DiWA achieves effective adaptation using a world model trained once on a few hundred thousand offline play interactions. This results in dramatically improved sample efficiency, making the approach significantly more practical and safer for real-world robot learning. On the challenging CALVIN benchmark, DiWA improves performance across eight tasks using only offline adaptation, while requiring orders of magnitude fewer physical interactions than model-free baselines. To our knowledge, this is the first demonstration of fine-tuning diffusion policies for real-world robotic skills using an offline world model. We make the code publicly available at https://diwa.cs.uni-freiburg.de.", "AI": {"tldr": "DiWA\u6846\u67b6\u5229\u7528\u4e16\u754c\u6a21\u578b\u79bb\u7ebf\u5fae\u8c03\u6269\u6563\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u6837\u672c\u6548\u7387\uff0c\u51cf\u5c11\u5b9e\u9645\u4ea4\u4e92\u9700\u6c42\u3002", "motivation": "\u6269\u6563\u7b56\u7565\u7684\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u5b58\u5728\u5956\u52b1\u4f20\u64ad\u56f0\u96be\u548c\u5b9e\u9645\u4ea4\u4e92\u9700\u6c42\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faDiWA\u6846\u67b6\uff0c\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u79bb\u7ebf\u5fae\u8c03\u6269\u6563\u7b56\u7565\uff0c\u51cf\u5c11\u73af\u5883\u4ea4\u4e92\u4f9d\u8d56\u3002", "result": "\u5728CALVIN\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDiWA\u4ec5\u9700\u5c11\u91cf\u79bb\u7ebf\u4ea4\u4e92\u5373\u63d0\u5347\u516b\u9879\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "DiWA\u9996\u6b21\u5b9e\u73b0\u57fa\u4e8e\u79bb\u7ebf\u4e16\u754c\u6a21\u578b\u7684\u6269\u6563\u7b56\u7565\u5fae\u8c03\uff0c\u4e3a\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u63d0\u4f9b\u9ad8\u6548\u5b89\u5168\u65b9\u6848\u3002"}}
