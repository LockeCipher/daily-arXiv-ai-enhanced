<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 36]
- [cs.AI](#cs.AI) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [From Rigging to Waving: 3D-Guided Diffusion for Natural Animation of Hand-Drawn Characters](https://arxiv.org/abs/2509.06573)
*Jie Zhou,Linzi Qu,Miu-Ling Lam,Hongbo Fu*

Main category: cs.GR

TL;DR: 结合骨骼动画和视频漏涼模型的混合动画系统，通过基于骨骼的几何指导和漏涼模型的纹理增强，解决手绘形象动画中的几何不一致和非某性变形问题。


<details>
  <summary>Details</summary>
Motivation: 传统骨骼动画方法在处理复杂非某性元素（如头发、裙据）时容易产生不自然变形，而视频漏涼模型虽能生成真实动态但容易造成风格化绘画的几何扭曲。需要一种方法能同时保持几何一致性和生动表现力。

Method: 1）使用骨骼动画生成基础图像作为几何指导 2）通过域适配漏涼模型将纹理增强框架为修复任务 3）二级动态注入（SDI）策略：在去噪过程中注入人体运动先验知识 4）头发层次建模（HLM）技术：使用分割地图将头发与身体分离动画

Result: 经过广泛实验，该系统在定量和定性评估中都超过了现有最优方法，能够生成更自然和表现力更强的手绘形象动画。

Conclusion: 该混合动画系统有效地结合了骨骼动画的几何一致性优势和漏涼模型的动态生成能力，通过二级动态注入和头发层次建模技术，成功解决了手绘形象动画中的关键技术挑战。

Abstract: Hand-drawn character animation is a vibrant field in computer graphics, presenting challenges in achieving geometric consistency while conveying expressive motion. Traditional skeletal animation methods maintain geometric consistency but struggle with complex non-rigid elements like flowing hair and skirts, leading to unnatural deformation. Conversely, video diffusion models synthesize realistic dynamics but often create geometric distortions in stylized drawings due to domain gaps. This work proposes a hybrid animation system that combines skeletal animation and video diffusion. Initially, coarse images are generated from characters retargeted with skeletal animations for geometric guidance. These images are then enhanced in texture and secondary dynamics using video diffusion priors, framing this enhancement as an inpainting task. A domain-adapted diffusion model refines user-masked regions needing improvement, especially for secondary dynamics. To enhance motion realism further, we introduce a Secondary Dynamics Injection (SDI) strategy in the denoising process, incorporating features from a pre-trained diffusion model enriched with human motion priors. Additionally, to tackle unnatural deformations from low-poly single-mesh character modeling, we present a Hair Layering Modeling (HLM) technique that uses segmentation maps to separate hair from the body, allowing for more natural animation of long-haired characters. Extensive experiments show that our system outperforms state-of-the-art methods in both quantitative and qualitative evaluations.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [A Dataset Generation Scheme Based on Video2EEG-SPGN-Diffusion for SEED-VD](https://arxiv.org/abs/2509.05321)
*Yunfei Guo,Tao Zhang,Wu Huang,Yao Song*

Main category: cs.CV

TL;DR: Video2EEG-SPGN-Diffusion是一个开源框架，利用SEED-VD数据集生成基于视频刺激的多模态EEG信号数据集，并提供了视频-EEG数据对齐的工程流程。


<details>
  <summary>Details</summary>
Motivation: 为多模态大模型提供EEG对齐能力，推动视频-EEG对齐研究，支持情感分析、数据增强和脑机接口应用。

Method: 采用自博弈图网络（SPGN）与扩散模型结合的方法生成个性化EEG信号，构建包含1000多个样本的数据集，包含62通道200Hz的EEG信号和情感标签。

Result: 成功构建了大规模视频-EEG配对数据集，提供了完整的工程流程，支持多模态研究和应用开发。

Conclusion: 该框架为情感分析、数据增强和脑机接口领域提供了新的工具和方法，具有重要的研究和工程价值。

Abstract: This paper introduces an open-source framework, Video2EEG-SPGN-Diffusion, that leverages the SEED-VD dataset to generate a multimodal dataset of EEG signals conditioned on video stimuli. Additionally, we disclose an engineering pipeline for aligning video and EEG data pairs, facilitating the training of multimodal large models with EEG alignment capabilities. Personalized EEG signals are generated using a self-play graph network (SPGN) integrated with a diffusion model. As a major contribution, we release a new dataset comprising over 1000 samples of SEED-VD video stimuli paired with generated 62-channel EEG signals at 200 Hz and emotion labels, enabling video-EEG alignment and advancing multimodal research. This framework offers novel tools for emotion analysis, data augmentation, and brain-computer interface applications, with substantial research and engineering significance.

</details>


### [3] [Delta Velocity Rectified Flow for Text-to-Image Editing](https://arxiv.org/abs/2509.05342)
*Gaspard Beaudouin,Minghan Li,Jaeyeon Kim,Sunghoon Yoon,Mengyu Wang*

Main category: cs.CV

TL;DR: DVRF是一种基于蒸馏的无反演图像编辑框架，通过显式建模源和目标速度场差异来减少伪影，并引入时间相关偏移项来提升目标分布对齐。


<details>
  <summary>Details</summary>
Motivation: 解决现有蒸馏采样方法中普遍存在的过度平滑伪影问题，提供更高质量的无反演文本到图像编辑。

Method: 基于整流流模型的蒸馏方法，显式建模源和目标速度场差异，引入时间相关偏移项将噪声潜在空间推向目标轨迹。

Result: 实验结果表明DVRF在编辑质量、保真度和可控性方面表现优异，无需架构修改即可高效应用于文本到图像编辑任务。

Conclusion: DVRF成功连接了基于分数的扩散优化和基于速度的整流流优化，为FlowEdit等方法提供了理论解释，实现了高质量的无反演图像编辑。

Abstract: We propose Delta Velocity Rectified Flow (DVRF), a novel inversion-free, path-aware editing framework within rectified flow models for text-to-image editing. DVRF is a distillation-based method that explicitly models the discrepancy between the source and target velocity fields in order to mitigate over-smoothing artifacts rampant in prior distillation sampling approaches. We further introduce a time-dependent shift term to push noisy latents closer to the target trajectory, enhancing the alignment with the target distribution. We theoretically demonstrate that when this shift is disabled, DVRF reduces to Delta Denoising Score, thereby bridging score-based diffusion optimization and velocity-based rectified-flow optimization. Moreover, when the shift term follows a linear schedule under rectified-flow dynamics, DVRF generalizes the Inversion-free method FlowEdit and provides a principled theoretical interpretation for it. Experimental results indicate that DVRF achieves superior editing quality, fidelity, and controllability while requiring no architectural modifications, making it efficient and broadly applicable to text-to-image editing tasks. Code is available at https://github.com/gaspardbd/DeltaVelocityRectifiedFlow.

</details>


### [4] [FAVAE-Effective Frequency Aware Latent Tokenizer](https://arxiv.org/abs/2509.05441)
*Tejaswini Medi,Hsien-Yi Wang,Arianna Rampini,Margret Keuper*

Main category: cs.CV

TL;DR: 本文分析了现有潜在标记器在频率重建方面的偏差问题，提出了基于小波的频率感知变分自编码器(FA-VAE)框架，通过解耦低频和高频组件的优化来改善纹理细节重建。


<details>
  <summary>Details</summary>
Motivation: 现有潜在生成模型在图像重建时往往缺乏真实感，特别是在具有尖锐过渡的纹理区域，因为传统目标函数固有地优先考虑低频重建，而牺牲了高频保真度，导致输出过度平滑和视觉伪影。

Method: 提出基于小波的频率感知变分自编码器(FA-VAE)框架，显式解耦低频和高频组件的优化，通过频率感知优化改善精细纹理的重建同时保持全局结构。

Result: 该方法能够改善当前潜在标记器的保真度差距，在纹理细节重建方面表现更好，同时保持全局结构的完整性。

Conclusion: 频率感知优化对于实现真实图像表示至关重要，该方法在内容创作、神经渲染和医学成像等应用中具有广泛意义。

Abstract: Latent generative models have shown remarkable progress in high-fidelity image synthesis, typically using a two-stage training process that involves compressing images into latent embeddings via learned tokenizers in the first stage. The quality of generation strongly depends on how expressive and well-optimized these latent embeddings are. While various methods have been proposed to learn effective latent representations, the reconstructed images often lack realism, particularly in textured regions with sharp transitions, due to loss of fine details governed by high frequencies. We conduct a detailed frequency decomposition of existing state-of-the-art (SOTA) latent tokenizers and show that conventional objectives inherently prioritize low-frequency reconstruction, often at the expense of high-frequency fidelity. Our analysis reveals these latent tokenizers exhibit a bias toward low-frequency information, when jointly optimized, leading to over-smoothed outputs and visual artifacts that diminish perceptual quality. To address this, we propose a wavelet-based, frequency-aware variational autoencoder (FA-VAE) framework that explicitly decouples the optimization of low- and high-frequency components. This decoupling enables improved reconstruction of fine textures while preserving global structure. Our approach bridges the fidelity gap in current latent tokenizers and emphasizes the importance of frequency-aware optimization for realistic image representation, with broader implications for applications in content creation, neural rendering, and medical imaging.

</details>


### [5] [Visibility-Aware Language Aggregation for Open-Vocabulary Segmentation in 3D Gaussian Splatting](https://arxiv.org/abs/2509.05515)
*Sen Wang,Kunyi Li,Siyun Liang,Elena Alegret,Jing Ma,Nassir Navab,Stefano Gasperini*

Main category: cs.CV

TL;DR: VALA方法通过可见性感知语言聚合和流式加权几何中值融合，解决了3D高斯分布中背景高斯噪声和多视角不一致性问题，显著提升了开放词汇定位和分割性能


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个基本问题：背景高斯对渲染像素贡献很小却获得与前景相同的特征，以及语言嵌入中视角特定噪声导致的多视角不一致性

Method: 提出可见性感知语言聚合(VALA)，计算每条光线的边际贡献并应用可见性感知门控保留可见高斯；提出在余弦空间中使用流式加权几何中值来融合多视角噪声特征

Result: VALA在参考数据集上改善了开放词汇定位和分割性能，一致超越现有工作

Conclusion: 该方法以快速且内存高效的方式产生了鲁棒、视角一致的语言特征嵌入

Abstract: Recently, distilling open-vocabulary language features from 2D images into 3D Gaussians has attracted significant attention. Although existing methods achieve impressive language-based interactions of 3D scenes, we observe two fundamental issues: background Gaussians contributing negligibly to a rendered pixel get the same feature as the dominant foreground ones, and multi-view inconsistencies due to view-specific noise in language embeddings. We introduce Visibility-Aware Language Aggregation (VALA), a lightweight yet effective method that computes marginal contributions for each ray and applies a visibility-aware gate to retain only visible Gaussians. Moreover, we propose a streaming weighted geometric median in cosine space to merge noisy multi-view features. Our method yields a robust, view-consistent language feature embedding in a fast and memory-efficient manner. VALA improves open-vocabulary localization and segmentation across reference datasets, consistently surpassing existing works.

</details>


### [6] [RED: Robust Event-Guided Motion Deblurring with Modality-Specific Disentangled Representation](https://arxiv.org/abs/2509.05554)
*Yihong Leng,Siming Zheng,Jinwei Chen,Bo Li,Jiaojiao Li,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: 通过模态特异解耦表征和随机投影策略，网络能够在事件流不完整的情况下实现稳健的运动去模糊效果


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了事件流的本质性不完整性，这种退化影响运动先验知识的完整性并限制事件导向去模糊的效果

Method: 提出稳健性导向的随机投影策略(RPS)通过随机掩码事件来培养网络对不完整模式的适应性；设计解耦的OmniAttention模块来显式建模内部运动、运动间和跨模态相关性；使用两个交互模块来增强模糊图像中的运动敏感区域并向事件表征注入语义上下文

Result: 在合成和真实数据集上的广泛实验表明，RED网络在准确性和稳健性方面都一致达到了最先进的性能

Conclusion: 通过处理事件流的本质性不完整性，网络能够在各种未知场景条件下实现更稳健和有效的运动去模糊

Abstract: Event cameras provide sparse yet temporally high-temporal-resolution motion information, demonstrating great potential for motion deblurring. Existing methods focus on cross-modal interaction, overlooking the inherent incompleteness of event streams, which arises from the trade-off between sensitivity and noise introduced by the thresholding mechanism of Dynamic Vision Sensors (DVS). Such degradation compromises the integrity of motion priors and limits the effectiveness of event-guided deblurring. To tackle these challenges, we propose a Robust Event-guided Deblurring (RED) network with modality-specific disentangled representation. First, we introduce a Robustness-Oriented Perturbation Strategy (RPS) that applies random masking to events, which exposes RED to incomplete patterns and then foster robustness against various unknown scenario conditions.Next, a disentangled OmniAttention is presented to explicitly model intra-motion, inter-motion, and cross-modality correlations from two inherently distinct but complementary sources: blurry images and partially disrupted events. Building on these reliable features, two interactive modules are designed to enhance motion-sensitive areas in blurry images and inject semantic context into incomplete event representations. Extensive experiments on synthetic and real-world datasets demonstrate RED consistently achieves state-of-the-art performance in both accuracy and robustness.

</details>


### [7] [EditIDv2: Editable ID Customization with Data-Lubricated ID Feature Integration for Text-to-Image Generation](https://arxiv.org/abs/2509.05659)
*Guandong Li,Zhaobin Chu*

Main category: cs.CV

TL;DR: EditIDv2是一个无需调优的解决方案，专门针对高复杂度叙事场景和长文本输入设计，通过改进ID特征整合模块和引入新的训练策略，在保持身份一致性的同时实现深度多层级语义编辑。


<details>
  <summary>Details</summary>
Motivation: 现有角色编辑方法在简单提示下表现良好，但在面对包含多语义层、时间逻辑和复杂上下文关系的长文本叙事时，往往出现编辑能力下降、语义理解偏差和身份一致性崩溃的问题。

Method: 通过PerceiverAttention的精细分解、引入ID损失和与扩散模型的联合动态训练，以及整合模块的离线融合策略，在少量数据润滑下实现可编辑性注入。

Result: 在复杂叙事环境中实现了深度多层级语义编辑，同时保持身份一致性，满足长提示和高质量图像生成的需求，在IBench评估中取得了优异结果。

Conclusion: EditIDv2成功解决了长文本叙事场景下的角色编辑挑战，通过创新的架构设计和训练策略，在最小数据需求下实现了高质量的语义编辑和身份保持。

Abstract: We propose EditIDv2, a tuning-free solution specifically designed for high-complexity narrative scenes and long text inputs. Existing character editing methods perform well under simple prompts, but often suffer from degraded editing capabilities, semantic understanding biases, and identity consistency breakdowns when faced with long text narratives containing multiple semantic layers, temporal logic, and complex contextual relationships. In EditID, we analyzed the impact of the ID integration module on editability. In EditIDv2, we further explore and address the influence of the ID feature integration module. The core of EditIDv2 is to discuss the issue of editability injection under minimal data lubrication. Through a sophisticated decomposition of PerceiverAttention, the introduction of ID loss and joint dynamic training with the diffusion model, as well as an offline fusion strategy for the integration module, we achieve deep, multi-level semantic editing while maintaining identity consistency in complex narrative environments using only a small amount of data lubrication. This meets the demands of long prompts and high-quality image generation, and achieves excellent results in the IBench evaluation.

</details>


### [8] [WIPUNet: A Physics-inspired Network with Weighted Inductive Biases for Image Denoising](https://arxiv.org/abs/2509.05662)
*Wasikul Islam*

Main category: cs.CV

TL;DR: 这篇论文将高能物理中的堆积干扰减少原理转化为图像去噪的物理导向归纳偏见，提出WIPUNet网络，在高噪声条件下显示了更好的稳健性。


<details>
  <summary>Details</summary>
Motivation: 受高能物理中堆积干扰减少策略的启发，通过嵌入物理导向的归纳偏见（如守恒、局域性、隔离性）来提升图像去噪模型在强度污染下的稳健性。

Method: 提出了一系列堆积干扰去噪器：包含守恒约束的清流CNN、其高斯噪声变种，以及WIPUNet（结合物理偏见的UNet结构）。在CIFAR-10和BSD500数据集上进行实验。

Result: 在高斯噪声条件下（σ=15-100），PU去噪器与标准基线相当，而WIPUNet在高噪声水平下显示凸出优势，说明物理偏见提供了更好的稳健性。

Conclusion: 物理受启发的归纳偏见能够在不依赖复杂SOTA机制的情况下，为图像去噪模型带来在高度污染条件下的稳健性改善。

Abstract: In high-energy particle physics, collider measurements are contaminated by "pileup", overlapping soft interactions that obscure the hard-scatter signal of interest. Dedicated subtraction strategies exploit physical priors such as conservation, locality, and isolation. Inspired by this analogy, we investigate how such principles can inform image denoising by embedding physics-guided inductive biases into neural architectures. This paper is a proof of concept: rather than targeting state-of-the-art (SOTA) benchmarks, we ask whether physics-inspired priors improve robustness under strong corruption.   We introduce a hierarchy of PU-inspired denoisers: a residual CNN with conservation constraints, its Gaussian-noise variants, and the Weighted Inductive Pileup-physics-inspired U-Network for Denoising (WIPUNet), which integrates these ideas into a UNet backbone. On CIFAR-10 with Gaussian noise at $\sigma\in\{15,25,50,75,100\}$, PU-inspired CNNs are competitive with standard baselines, while WIPUNet shows a \emph{widening margin} at higher noise. Complementary BSD500 experiments show the same trend, suggesting physics-inspired priors provide stability where purely data-driven models degrade. Our contributions are: (i) translating pileup-mitigation principles into modular inductive biases; (ii) integrating them into UNet; and (iii) demonstrating robustness gains at high noise without relying on heavy SOTA machinery.

</details>


### [9] [Leveraging Vision-Language Large Models for Interpretable Video Action Recognition with Semantic Tokenization](https://arxiv.org/abs/2509.05695)
*Jingwei Peng,Zhixuan Qiu,Boyu Jin,Surasakdi Siripong*

Main category: cs.CV

TL;DR: LVLM-VAR是一个创新的视频动作识别框架，利用预训练视觉语言大模型将视频转换为语义动作标记，结合自然语言指令进行动作分类和语义推理，在准确性和可解释性方面都取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统人类动作识别方法在处理多样化视频数据时，经常面临深度语义理解、复杂上下文信息和细粒度区分的挑战。受大型语言模型强大能力的启发，本文旨在利用视觉语言大模型来提升视频动作识别的性能。

Method: 提出了Video-to-Semantic-Tokens (VST)模块，将原始视频序列转换为离散的、语义和时间一致的"语义动作标记"，创建LVLM可理解的"动作叙述"。这些标记与自然语言指令一起由LoRA微调的LVLM（如LLaVA-13B）处理，进行动作分类和语义推理。

Result: 在NTU RGB+D和NTU RGB+D 120等挑战性基准测试中达到最先进或极具竞争力的性能：NTU RGB+D X-Sub达到94.1%，NTU RGB+D 120 X-Set达到90.0%，显著优于现有方法。

Conclusion: LVLM-VAR不仅显著提升了视频动作识别的准确性，还通过生成自然语言解释大大增强了模型的可解释性，为视频理解任务开辟了新途径。

Abstract: Human action recognition often struggles with deep semantic understanding, complex contextual information, and fine-grained distinction, limitations that traditional methods frequently encounter when dealing with diverse video data. Inspired by the remarkable capabilities of large language models, this paper introduces LVLM-VAR, a novel framework that pioneers the application of pre-trained Vision-Language Large Models (LVLMs) to video action recognition, emphasizing enhanced accuracy and interpretability. Our method features a Video-to-Semantic-Tokens (VST) Module, which innovatively transforms raw video sequences into discrete, semantically and temporally consistent "semantic action tokens," effectively crafting an "action narrative" that is comprehensible to an LVLM. These tokens, combined with natural language instructions, are then processed by a LoRA-fine-tuned LVLM (e.g., LLaVA-13B) for robust action classification and semantic reasoning. LVLM-VAR not only achieves state-of-the-art or highly competitive performance on challenging benchmarks such as NTU RGB+D and NTU RGB+D 120, demonstrating significant improvements (e.g., 94.1% on NTU RGB+D X-Sub and 90.0% on NTU RGB+D 120 X-Set), but also substantially boosts model interpretability by generating natural language explanations for its predictions.

</details>


### [10] [LiDAR-BIND-T: Improving SLAM with Temporally Consistent Cross-Modal LiDAR Reconstruction](https://arxiv.org/abs/2509.05728)
*Niels Balemans,Ali Anwar,Jan Steckel,Siegfried Mercelis*

Main category: cs.CV

TL;DR: LiDAR-BIND-T扩展了多模态融合框架，通过引入时间一致性机制，在雷达/声纳到LiDAR的转换中显著提升了时间和空间一致性，改善了SLAM性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态传感器融合方法在时间一致性方面存在不足，影响了下游SLAM任务的鲁棒性和性能，需要开发能够显式强制时间一致性的机制。

Method: 提出了三个主要贡献：(i)时间嵌入相似性对齐连续潜在空间，(ii)运动对齐变换损失匹配预测与真实LiDAR的位移，(iii)窗口时间融合使用专门的时间模块，并更新模型架构以更好地保持空间结构。

Result: 在雷达/声纳到LiDAR的转换评估中显示出改进的时间和空间一致性，降低了绝对轨迹误差，提高了基于Cartographer的SLAM的占用地图精度。

Conclusion: LiDAR-BIND-T保持了即插即用的模态融合能力，同时显著增强了时间稳定性，为下游SLAM任务提供了更好的鲁棒性和性能。

Abstract: This paper extends LiDAR-BIND, a modular multi-modal fusion framework that binds heterogeneous sensors (radar, sonar) to a LiDAR-defined latent space, with mechanisms that explicitly enforce temporal consistency. We introduce three contributions: (i) temporal embedding similarity that aligns consecutive latents, (ii) a motion-aligned transformation loss that matches displacement between predictions and ground truth LiDAR, and (iii) windows temporal fusion using a specialised temporal module. We further update the model architecture to better preserve spatial structure. Evaluations on radar/sonar-to-LiDAR translation demonstrate improved temporal and spatial coherence, yielding lower absolute trajectory error and better occupancy map accuracy in Cartographer-based SLAM (Simultaneous Localisation and Mapping). We propose different metrics based on the Fr\'echet Video Motion Distance (FVMD) and a correlation-peak distance metric providing practical temporal quality indicators to evaluate SLAM performance. The proposed temporal LiDAR-BIND, or LiDAR-BIND-T, maintains plug-and-play modality fusion while substantially enhancing temporal stability, resulting in improved robustness and performance for downstream SLAM.

</details>


### [11] [Depth-Aware Super-Resolution via Distance-Adaptive Variational Formulation](https://arxiv.org/abs/2509.05746)
*Tianhao Guo,Bingjie Lu,Feng Wang,Zhengyang Lu*

Main category: cs.CV

TL;DR: 提出了首个理论基础的深度自适应超分辨率框架，通过距离相关的伪微分算子建模退化过程，使用深度条件卷积核实现空间自适应重建，在深度变化场景中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统单图像超分辨率假设空间不变的退化模型，但真实成像系统存在大气散射、景深变化和透视畸变等复杂距离相关效应，需要空间自适应重建策略。

Method: 变分框架将超分辨率建模为空间变化的逆问题，退化算子为距离相关的伪微分算子。神经网络架构通过级联残差块实现离散梯度流动力学，使用深度条件卷积核，结合学习到的距离自适应正则化项。

Result: 在五个基准数据集上达到最先进性能，KITTI户外场景2倍和4倍缩放分别获得36.89/0.9516和30.54/0.8721的PSNR/SSIM，比现有方法分别高出0.44dB和0.36dB。

Conclusion: 建立了首个理论基础的深度自适应超分辨率框架，在深度变化场景中显著改进性能，同时在传统基准上保持竞争力。

Abstract: Single image super-resolution traditionally assumes spatially-invariant degradation models, yet real-world imaging systems exhibit complex distance-dependent effects including atmospheric scattering, depth-of-field variations, and perspective distortions. This fundamental limitation necessitates spatially-adaptive reconstruction strategies that explicitly incorporate geometric scene understanding for optimal performance. We propose a rigorous variational framework that characterizes super-resolution as a spatially-varying inverse problem, formulating the degradation operator as a pseudodifferential operator with distance-dependent spectral characteristics that enable theoretical analysis of reconstruction limits across depth ranges. Our neural architecture implements discrete gradient flow dynamics through cascaded residual blocks with depth-conditional convolution kernels, ensuring convergence to stationary points of the theoretical energy functional while incorporating learned distance-adaptive regularization terms that dynamically adjust smoothness constraints based on local geometric structure. Spectral constraints derived from atmospheric scattering theory prevent bandwidth violations and noise amplification in far-field regions, while adaptive kernel generation networks learn continuous mappings from depth to reconstruction filters. Comprehensive evaluation across five benchmark datasets demonstrates state-of-the-art performance, achieving 36.89/0.9516 and 30.54/0.8721 PSNR/SSIM at 2 and 4 scales on KITTI outdoor scenes, outperforming existing methods by 0.44dB and 0.36dB respectively. This work establishes the first theoretically-grounded distance-adaptive super-resolution framework and demonstrates significant improvements on depth-variant scenarios while maintaining competitive performance across traditional benchmarks.

</details>


### [12] [Coefficients-Preserving Sampling for Reinforcement Learning with Flow Matching](https://arxiv.org/abs/2509.05952)
*Feng Wang,Zihao Yu*

Main category: cs.CV

TL;DR: 通过CPS采样方法消除SDE基于流匹配模型中的噪声除疑，提高奖励学习效果和生成质量


<details>
  <summary>Details</summary>
Motivation: 解决SDE采样在流匹配模型中引入的噪声除疑问题，该问题对励强学习奖励学习过程产生负面影响

Method: 提出Coefficients-Preserving Sampling (CPS)方法，受DDIM启发重构采样过程，消除过多随机性导致的噪声

Result: CPS消除了噪声除疑，提高了奖励模型的准确性，促进了Flow-GRPO和Dance-GRPO等RL优化器的更快更稳定收敛

Conclusion: CPS方法有效解决了SDE采样的噪声问题，为流匹配模型的励强学习应用提供了更好的基础

Abstract: Reinforcement Learning (RL) has recently emerged as a powerful technique for improving image and video generation in Diffusion and Flow Matching models, specifically for enhancing output quality and alignment with prompts. A critical step for applying online RL methods on Flow Matching is the introduction of stochasticity into the deterministic framework, commonly realized by Stochastic Differential Equation (SDE). Our investigation reveals a significant drawback to this approach: SDE-based sampling introduces pronounced noise artifacts in the generated images, which we found to be detrimental to the reward learning process. A rigorous theoretical analysis traces the origin of this noise to an excess of stochasticity injected during inference. To address this, we draw inspiration from Denoising Diffusion Implicit Models (DDIM) to reformulate the sampling process. Our proposed method, Coefficients-Preserving Sampling (CPS), eliminates these noise artifacts. This leads to more accurate reward modeling, ultimately enabling faster and more stable convergence for reinforcement learning-based optimizers like Flow-GRPO and Dance-GRPO. Code will be released at https://github.com/IamCreateAI/FlowCPS

</details>


### [13] [Dual Interaction Network with Cross-Image Attention for Medical Image Segmentation](https://arxiv.org/abs/2509.05953)
*Jeonghyun Noh,Wangsu Jeon,Jinsun Park*

Main category: cs.CV

TL;DR: 医学图像分割中的双向交互融合模块(DIFM)，通过跨注意力机制有效利用原始图像和增强图像的互补信息，提高疾病诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 医学图像中的噪声、模糊、对比度低等问题影响诊断准确性，而传统图像增强技术可能会修改原始图像的关键信息，需要一种能够充分利用两种图像优势的融合方法。

Method: 提出双向交互融合模块(DIFM)，通过跨注意力机制双向关注不同图像间的对应空间信息，并使用全局空间注意力精炼互补特征。同时采用基于梯度提取的多尺庨边界损失函数来提高边界分割准确性。

Result: 在ACDC和Synapse数据集上进行实验，结果显示方法在数量和质量上都表现优异。

Conclusion: 该方法能够有效地利用原始图像和增强图像的互补信息，提高医学图像分割的准确性，尤其在边界区域表现更优。

Abstract: Medical image segmentation is a crucial method for assisting professionals in diagnosing various diseases through medical imaging. However, various factors such as noise, blurriness, and low contrast often hinder the accurate diagnosis of diseases. While numerous image enhancement techniques can mitigate these issues, they may also alter crucial information needed for accurate diagnosis in the original image. Conventional image fusion strategies, such as feature concatenation can address this challenge. However, they struggle to fully leverage the advantages of both original and enhanced images while suppressing the side effects of the enhancements. To overcome the problem, we propose a dual interactive fusion module (DIFM) that effectively exploits mutual complementary information from the original and enhanced images. DIFM employs cross-attention bidirectionally to simultaneously attend to corresponding spatial information across different images, subsequently refining the complementary features via global spatial attention. This interaction leverages low- to high-level features implicitly associated with diverse structural attributes like edges, blobs, and object shapes, resulting in enhanced features that embody important spatial characteristics. In addition, we introduce a multi-scale boundary loss based on gradient extraction to improve segmentation accuracy at object boundaries. Experimental results on the ACDC and Synapse datasets demonstrate the superiority of the proposed method quantitatively and qualitatively. Code available at: https://github.com/JJeong-Gari/DIN

</details>


### [14] [Neural Bloom: A Deep Learning Approach to Real-Time Lighting](https://arxiv.org/abs/2509.05963)
*Rafal Karp,Dawid Gruszka,Tomasz Trzcinski*

Main category: cs.CV

TL;DR: 使用神经网络实时生成清晰的煌光灯光效果，方法速度比现有技术提高30%，节省计算资源


<details>
  <summary>Details</summary>
Motivation: 传统煌光灯光技术依靠多次模糊和纹理采样，包含条件分支，占用大量执行时间，成为实时渲染的急需解决的急需解决的等问题

Method: 提出两种神经网络基煌光灯光方法：Neural Bloom Lighting (NBL) 和 Fast Neural Bloom Lighting (FastNBL)，分别重点关注质量和性能

Result: 在多种3D场景中进行测试，FastNBL比现有最佳方案快28%，NBL快12%，同时产生高质量煌光效果

Conclusion: 神经网络方法能够更快速地实现现实的煌光灯光现象，为实时环境提供更高的真实感和流畅体验，解决计算资源瓶颈问题

Abstract: We propose a novel method to generate bloom lighting effect in real time using neural networks. Our solution generate brightness mask from given 3D scene view up to 30% faster than state-of-the-art methods. The existing traditional techniques rely on multiple blur appliances and texture sampling, also very often have existing conditional branching in its implementation. These operations occupy big portion of the execution time. We solve this problem by proposing two neural network-based bloom lighting methods, Neural Bloom Lighting (NBL) and Fast Neural Bloom Lighting (FastNBL), focusing on their quality and performance. Both methods were tested on a variety of 3D scenes, with evaluations conducted on brightness mask accuracy and inference speed. The main contribution of this work is that both methods produce high-quality bloom effects while outperforming the standard state-of-the-art bloom implementation, with FastNBL being faster by 28% and NBL faster by 12%. These findings highlight that we can achieve realistic bloom lighting phenomena faster, moving us towards more realism in real-time environments in the future. This improvement saves computational resources, which is a major bottleneck in real-time rendering. Furthermore, it is crucial for sustaining immersion and ensuring smooth experiences in high FPS environments, while maintaining high-quality realism.

</details>


### [15] [OmniStyle2: Scalable and High Quality Artistic Style Transfer Data Generation via Destylization](https://arxiv.org/abs/2509.05970)
*Ye Wang,Zili Yi,Yibo Zhang,Peng Zheng,Xuping Xie,Jiang Lin,Yilin Wang,Rui Ma*

Main category: cs.CV

TL;DR: OmniStyle2通过去风格化方法构建大规模数据集DST-100K，训练出基于FLUX.1-dev的前馈模型，在艺术风格迁移任务中超越现有最佳方法


<details>
  <summary>Details</summary>
Motivation: 解决艺术风格迁移中缺乏真实监督数据的基本挑战，通过去风格化方法获得风格化艺术作品与其底层内容的真实对应关系

Method: 开发文本引导的去风格化模型DST重建无风格内容，使用多阶段评估模型DST-Filter自动筛选高质量数据对，基于DST-100K数据集训练FLUX.1-dev前馈模型

Result: OmniStyle2在定性和定量基准测试中持续超越最先进方法，证明了通过去风格化进行可扩展数据生成的可靠性

Conclusion: 去风格化提供了一种可靠的监督范式，克服了艺术风格迁移中缺乏真实数据的基本挑战

Abstract: OmniStyle2 introduces a novel approach to artistic style transfer by reframing it as a data problem. Our key insight is destylization, reversing style transfer by removing stylistic elements from artworks to recover natural, style-free counterparts. This yields DST-100K, a large-scale dataset that provides authentic supervision signals by aligning real artistic styles with their underlying content. To build DST-100K, we develop (1) DST, a text-guided destylization model that reconstructs stylefree content, and (2) DST-Filter, a multi-stage evaluation model that employs Chain-of-Thought reasoning to automatically discard low-quality pairs while ensuring content fidelity and style accuracy. Leveraging DST-100K, we train OmniStyle2, a simple feed-forward model based on FLUX.1-dev. Despite its simplicity, OmniStyle2 consistently surpasses state-of-the-art methods across both qualitative and quantitative benchmarks. Our results demonstrate that scalable data generation via destylization provides a reliable supervision paradigm, overcoming the fundamental challenge posed by the lack of ground-truth data in artistic style transfer.

</details>


### [16] [Multi-Strategy Guided Diffusion via Sparse Masking Temporal Reweighting Distribution Correction](https://arxiv.org/abs/2509.05992)
*Zekun Zhou,Yanru Gong,Liu Shi,Qiegen Liu*

Main category: cs.CV

TL;DR: STRIDE是一种基于扩散模型的稀疏视图CT重建方法，通过时间变化的稀疏条件重加权策略和双网络并行架构，在PSNR、SSIM和MSE指标上显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像处理任务中展现出强大的生成能力，但如何有效处理稀疏视图CT重建中的缺失投影视图完成和全局信息建模仍是一个挑战。

Method: 提出时间变化的稀疏条件重加权引导策略，动态调整去噪过程中的权重；使用线性回归校正分布偏移；构建双网络并行架构进行全局校正和多子频率分量优化。

Result: 在公开和真实数据集上，PSNR提升2.58dB，SSIM提高2.37%，MSE降低0.236，重建图像在结构一致性、细节恢复和伪影抑制方面表现出优秀的泛化性和鲁棒性。

Conclusion: STRIDE方法通过创新的引导策略和网络架构设计，成功实现了高质量的稀疏视图CT图像重建，在多项指标上显著优于现有最佳方法。

Abstract: Diffusion models have demonstrated remarkable generative capabilities in image processing tasks. We propose a Sparse condition Temporal Rewighted Integrated Distribution Estimation guided diffusion model (STRIDE) for sparse-view CT reconstruction. Specifically, we design a joint training mechanism guided by sparse conditional probabilities to facilitate the model effective learning of missing projection view completion and global information modeling. Based on systematic theoretical analysis, we propose a temporally varying sparse condition reweighting guidance strategy to dynamically adjusts weights during the progressive denoising process from pure noise to the real image, enabling the model to progressively perceive sparse-view information. The linear regression is employed to correct distributional shifts between known and generated data, mitigating inconsistencies arising during the guidance process. Furthermore, we construct a dual-network parallel architecture to perform global correction and optimization across multiple sub-frequency components, thereby effectively improving the model capability in both detail restoration and structural preservation, ultimately achieving high-quality image reconstruction. Experimental results on both public and real datasets demonstrate that the proposed method achieves the best improvement of 2.58 dB in PSNR, increase of 2.37\% in SSIM, and reduction of 0.236 in MSE compared to the best-performing baseline methods. The reconstructed images exhibit excellent generalization and robustness in terms of structural consistency, detail restoration, and artifact suppression.

</details>


### [17] [BLaVe-CoT: Consistency-Aware Visual Question Answering for Blind and Low Vision Users](https://arxiv.org/abs/2509.06010)
*Wanyin Cheng,Zanxi Ruan*

Main category: cs.CV

TL;DR: BLaVe-CoT是一个针对盲人和低视力用户的VQA框架，通过多候选答案生成、空间定位和思维链推理来处理视觉问答中的模糊性问题。


<details>
  <summary>Details</summary>
Motivation: 盲人和低视力用户拍摄的照片往往模糊或构图不佳，提出的视觉问题也经常模糊不清，存在多种有效答案，这与传统VQA系统假设单一答案和区域不匹配。

Method: 使用LoRA调优的BLIP-2模型生成多样候选答案，通过PolyFormer对每个答案进行空间定位，最后应用思维链推理模块评估答案是否指向相同或不同区域。

Result: 在VQA-AnswerTherapy基准测试中，BLaVe-CoT优于先前方法，对辅助场景中常见的模糊性和视觉噪声更具鲁棒性。

Conclusion: 这项工作强调了VQA系统需要适应真实人类不确定性，为盲人和低视力用户提供包容性支持的重要性。

Abstract: Visual Question Answering (VQA) holds great potential for assisting Blind and Low Vision (BLV) users, yet real-world usage remains challenging. Due to visual impairments, BLV users often take blurry or poorly framed photos and face difficulty in articulating specific questions about what they cannot fully see. As a result, their visual questions are frequently ambiguous, and different users may interpret them in diverse ways. This leads to multiple valid answers, each grounded in different image regions-posing a mismatch with conventional VQA systems that assume a single answer and region. To bridge this gap, we present BLaVe-CoT, a VQA framework designed to reason about answer consistency in the face of ambiguity. Our method proposes diverse candidate answers using a LoRA-tuned BLIP-2 model, then grounds each answer spatially using PolyFormer, and finally applies a chain-of-thought reasoning module to assess whether the answers refer to the same or different regions. Evaluated on the VQA-AnswerTherapy benchmark, BLaVe-CoT outperforms previous methods and proves more robust to the ambiguity and visual noise common in assistive settings. This work highlights the need for VQA systems that can adapt to real human uncertainty and provide inclusive support for BLV users. To foster further research and accessibility applications, we have made the code publicly available at https://github.com/Accecwan/BLaVe-CoT.

</details>


### [18] [Home-made Diffusion Model from Scratch to Hatch](https://arxiv.org/abs/2509.06068)
*Shih-Ying Yeh*

Main category: cs.CV

TL;DR: HDM是一个高效的低成本文本到图像扩散模型，仅需535-620美元在4块RTX5090上训练，就能生成1024x1024高质量图像，大大降低了计算需求。


<details>
  <summary>Details</summary>
Motivation: 为了让个人研究者和小型组织也能负担得起高质量文本到图像生成，解决传统方法计算成本过高的问题。

Method: 提出Cross-U-Transformer架构使用交叉注意力进行跳跃连接，结合TREAD加速、移位方形裁剪策略和渐进分辨率缩放的高效训练方案。

Result: 仅用343M参数的小模型就能实现高质量图像生成和涌现能力（如直观相机控制），在有限硬件上达到竞争性效果。

Conclusion: 通过精心设计的架构而非单纯扩大规模，证明了高质量文本到图像生成可以民主化，为计算资源有限的研究者提供了可行路径。

Abstract: We introduce Home-made Diffusion Model (HDM), an efficient yet powerful text-to-image diffusion model optimized for training (and inferring) on consumer-grade hardware. HDM achieves competitive 1024x1024 generation quality while maintaining a remarkably low training cost of $535-620 using four RTX5090 GPUs, representing a significant reduction in computational requirements compared to traditional approaches. Our key contributions include: (1) Cross-U-Transformer (XUT), a novel U-shape transformer, Cross-U-Transformer (XUT), that employs cross-attention for skip connections, providing superior feature integration that leads to remarkable compositional consistency; (2) a comprehensive training recipe that incorporates TREAD acceleration, a novel shifted square crop strategy for efficient arbitrary aspect-ratio training, and progressive resolution scaling; and (3) an empirical demonstration that smaller models (343M parameters) with carefully crafted architectures can achieve high-quality results and emergent capabilities, such as intuitive camera control. Our work provides an alternative paradigm of scaling, demonstrating a viable path toward democratizing high-quality text-to-image generation for individual researchers and smaller organizations with limited computational resources.

</details>


### [19] [MedSeqFT: Sequential Fine-tuning Foundation Models for 3D Medical Image Segmentation](https://arxiv.org/abs/2509.06096)
*Yiwen Ye,Yicheng Wu,Xiangde Luo,He Zhang,Ziyang Chen,Ting Dang,Yanning Zhang,Yong Xia*

Main category: cs.CV

TL;DR: MedSeqFT是一个用于医学图像分割的顺序微调框架，通过MDS样本选择和K&G RFT知识蒸馏，在保持预训练知识的同时适应新任务，在10个3D分割任务上平均Dice提升3.0%


<details>
  <summary>Details</summary>
Motivation: 现有微调策略存在局限：并行微调隔离任务无法共享知识，多任务微调需要同时访问所有数据集且难以增量集成。需要一种能够顺序适应新任务同时保持预训练知识的框架

Method: 提出MedSeqFT顺序微调框架，包含两个核心组件：1) MDS选择识别最能代表预训练分布的下游样本；2) K&G RFT基于LoRA的知识蒸馏方案，平衡任务特定适应和预训练知识保持

Result: 在两个多任务数据集上的10个3D分割任务中 consistently优于最先进微调策略，平均Dice提升3.0%。在未见任务(COVID-19-20和Kidney)上验证了更好的迁移性，特别是肿瘤分割

Conclusion: MedSeqFT建立了顺序微调作为适应基础模型到演化临床任务的有效、知识保持范式，损失景观和参数变化的可视化分析进一步证明了其鲁棒性

Abstract: Foundation models have become a promising paradigm for advancing medical image analysis, particularly for segmentation tasks where downstream applications often emerge sequentially. Existing fine-tuning strategies, however, remain limited: parallel fine-tuning isolates tasks and fails to exploit shared knowledge, while multi-task fine-tuning requires simultaneous access to all datasets and struggles with incremental task integration. To address these challenges, we propose MedSeqFT, a sequential fine-tuning framework that progressively adapts pre-trained models to new tasks while refining their representational capacity. MedSeqFT introduces two core components: (1) Maximum Data Similarity (MDS) selection, which identifies downstream samples most representative of the original pre-training distribution to preserve general knowledge, and (2) Knowledge and Generalization Retention Fine-Tuning (K&G RFT), a LoRA-based knowledge distillation scheme that balances task-specific adaptation with the retention of pre-trained knowledge. Extensive experiments on two multi-task datasets covering ten 3D segmentation tasks demonstrate that MedSeqFT consistently outperforms state-of-the-art fine-tuning strategies, yielding substantial performance gains (e.g., an average Dice improvement of 3.0%). Furthermore, evaluations on two unseen tasks (COVID-19-20 and Kidney) verify that MedSeqFT enhances transferability, particularly for tumor segmentation. Visual analyses of loss landscapes and parameter variations further highlight the robustness of MedSeqFT. These results establish sequential fine-tuning as an effective, knowledge-retentive paradigm for adapting foundation models to evolving clinical tasks. Code will be released.

</details>


### [20] [CARDIE: clustering algorithm on relevant descriptors for image enhancement](https://arxiv.org/abs/2509.06116)
*Giulia Bonino,Luca Alberto Rizzo*

Main category: cs.CV

TL;DR: CARDIE是一种无监督图像聚类算法，基于颜色和亮度内容进行聚类，专门用于图像增强任务，能够提升调彩映射和去噪算法的性能。


<details>
  <summary>Details</summary>
Motivation: 图像自动聚类在图像增强任务中应用有限，主要因为难以定义对该任务有意义的聚类。

Method: 提出CARDIE无监督算法，基于颜色和亮度内容聚类图像；并提出量化图像增强算法影响的方法，评估亮度分布和局部方差。

Result: CARDIE产生的聚类比语义图像属性聚类更适合图像增强，可用于重新采样图像增强数据集，提升调彩映射和去噪算法的性能。

Conclusion: CARDIE为图像增强任务提供了更有效的聚类方法，已开源发布代码以便重现和应用。

Abstract: Automatic image clustering is a cornerstone of computer vision, yet its application to image enhancement remains limited, primarily due to the difficulty of defining clusters that are meaningful for this specific task. To address this issue, we introduce CARDIE, an unsupervised algorithm that clusters images based on their color and luminosity content. In addition, we introduce a method to quantify the impact of image enhancement algorithms on luminance distribution and local variance. Using this method, we demonstrate that CARDIE produces clusters more relevant to image enhancement than those derived from semantic image attributes. Furthermore, we demonstrate that CARDIE clusters can be leveraged to resample image enhancement datasets, leading to improved performance for tone mapping and denoising algorithms. To encourage adoption and ensure reproducibility, we publicly release CARDIE code on our GitHub.

</details>


### [21] [UniVerse-1: Unified Audio-Video Generation via Stitching of Experts](https://arxiv.org/abs/2509.06155)
*Duomin Wang,Wei Zuo,Aojie Li,Ling-Hao Chen,Xinyao Liao,Deyu Zhou,Zixin Yin,Xili Dai,Daxin Jiang,Gang Yu*

Main category: cs.CV

TL;DR: UniVerse-1是一个统一的多模态生成模型，通过专家模型拼接技术实现音频和视频的协同生成，避免了从头训练，在7600小时数据上微调后能够产生协调的音视频内容。


<details>
  <summary>Details</summary>
Motivation: 为了解决音频-视频生成中训练效率低和文本标注不对齐的问题，开发一个能够同时生成协调音频和视频的统一模型，避免从头训练并充分利用预训练模型的基础能力。

Method: 采用专家模型拼接(SoE)技术，深度融合预训练的视频和音乐生成专家模型的对应块；开发在线标注流程，在训练过程中处理训练数据并生成标签，避免基于文本标注的不对齐问题。

Result: 模型在7600小时音视频数据上微调后，能够为环境声音生成产生良好协调的音视频内容，为语音生成提供强对齐效果；并引入了Verse-Bench基准数据集进行系统评估。

Conclusion: UniVerse-1通过创新的专家拼接和在线标注技术，成功实现了高效的音视频协同生成，为研究社区提供了有价值的资源和工具，有助于缩小与Veo3等最先进模型的性能差距。

Abstract: We introduce UniVerse-1, a unified, Veo-3-like model capable of simultaneously generating coordinated audio and video. To enhance training efficiency, we bypass training from scratch and instead employ a stitching of experts (SoE) technique. This approach deeply fuses the corresponding blocks of pre-trained video and music generation experts models, thereby fully leveraging their foundational capabilities. To ensure accurate annotations and temporal alignment for both ambient sounds and speech with video content, we developed an online annotation pipeline that processes the required training data and generates labels during training process. This strategy circumvents the performance degradation often caused by misalignment text-based annotations. Through the synergy of these techniques, our model, after being finetuned on approximately 7,600 hours of audio-video data, produces results with well-coordinated audio-visuals for ambient sounds generation and strong alignment for speech generation. To systematically evaluate our proposed method, we introduce Verse-Bench, a new benchmark dataset. In an effort to advance research in audio-video generation and to close the performance gap with state-of-the-art models such as Veo3, we make our model and code publicly available. We hope this contribution will benefit the broader research community. Project page: https://dorniwang.github.io/UniVerse-1/.

</details>


### [22] [UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual Representation Learning](https://arxiv.org/abs/2509.06165)
*Huy Le,Nhat Chung,Tung Kieu,Jingkang Yang,Ngan Le*

Main category: cs.CV

TL;DR: UNO是一个统一的单阶段框架，用于视频场景图生成，可同时处理粗粒度边界框级和细粒度全景像素级任务，通过扩展的slot attention机制和时序一致性学习实现高效的多粒度视觉内容建模。


<details>
  <summary>Details</summary>
Motivation: 现有的视频场景图生成方法通常针对特定粒度（边界框级或像素级）设计，需要任务特定的架构和多阶段训练流程，缺乏统一的解决方案。

Method: 提出UNO框架：1）扩展slot attention机制将视觉特征分解为对象和关系slot；2）对象时序一致性学习确保跨帧表示一致性；3）动态三元组预测模块链接关系slot到对象对。

Result: 在标准边界框级和像素级VidSGG基准测试中取得竞争性性能，同时通过统一的对象中心设计提高了效率。

Conclusion: UNO证明了统一框架在视频场景图生成中的可行性，能够以最小任务特定修改处理不同视觉粒度任务，为多粒度视觉理解提供了有效解决方案。

Abstract: Video Scene Graph Generation (VidSGG) aims to represent dynamic visual content by detecting objects and modeling their temporal interactions as structured graphs. Prior studies typically target either coarse-grained box-level or fine-grained panoptic pixel-level VidSGG, often requiring task-specific architectures and multi-stage training pipelines. In this paper, we present UNO (UNified Object-centric VidSGG), a single-stage, unified framework that jointly addresses both tasks within an end-to-end architecture. UNO is designed to minimize task-specific modifications and maximize parameter sharing, enabling generalization across different levels of visual granularity. The core of UNO is an extended slot attention mechanism that decomposes visual features into object and relation slots. To ensure robust temporal modeling, we introduce object temporal consistency learning, which enforces consistent object representations across frames without relying on explicit tracking modules. Additionally, a dynamic triplet prediction module links relation slots to corresponding object pairs, capturing evolving interactions over time. We evaluate UNO on standard box-level and pixel-level VidSGG benchmarks. Results demonstrate that UNO not only achieves competitive performance across both tasks but also offers improved efficiency through a unified, object-centric design.

</details>


### [23] [Your Super Resolution Model is not Enough for Tackling Real-World Scenarios](https://arxiv.org/abs/2509.06387)
*Dongsik Yoon,Jongeun Kim*

Main category: cs.CV

TL;DR: 提出SAAM插件模块，让固定缩放比的超分辨率模型能够处理任意缩放比，通过轻量级的渗透式注意力机制实现多比例升级。


<details>
  <summary>Details</summary>
Motivation: 传统单图超分辨率模型对不同缩放比之间的沿用性差，限制了实际应用场景。需要一种能够让现有固定比例模型处理任意缩放比的方案。

Method: 设计Scale-Aware Attention Module (SAAM)，采用轻量级的比例自适应特征提取和上采样，结合SimAM注意力机制进行高效指导，使用梯度方差损失提升图像细节锐利度。

Result: 方案能外接多个独立的SISR模型（如SCNet、HiT-SR、OverNet），在广泛的整数和非整数缩放比下都取得了竞争力或更优的性能，计算开销最小。

Conclusion: SAAM为现有固定比例超分辨率模型提供了一种高效的多比例升级能力，具有强烈的实际应用价值。

Abstract: Despite remarkable progress in Single Image Super-Resolution (SISR), traditional models often struggle to generalize across varying scale factors, limiting their real-world applicability. To address this, we propose a plug-in Scale-Aware Attention Module (SAAM) designed to retrofit modern fixed-scale SR models with the ability to perform arbitrary-scale SR. SAAM employs lightweight, scale-adaptive feature extraction and upsampling, incorporating the Simple parameter-free Attention Module (SimAM) for efficient guidance and gradient variance loss to enhance sharpness in image details. Our method integrates seamlessly into multiple state-of-the-art SR backbones (e.g., SCNet, HiT-SR, OverNet), delivering competitive or superior performance across a wide range of integer and non-integer scale factors. Extensive experiments on benchmark datasets demonstrate that our approach enables robust multi-scale upscaling with minimal computational overhead, offering a practical solution for real-world scenarios.

</details>


### [24] [3DOF+Quantization: 3DGS quantization for large scenes with limited Degrees of Freedom](https://arxiv.org/abs/2509.06400)
*Matthieu Gendrin,Stéphane Pateux,Théo Ladune*

Main category: cs.CV

TL;DR: 3D高斯泼溅在有限视角区域（3DoF+）下提出基于球坐标的新量化方案，解决位置误差对投影误差的影响


<details>
  <summary>Details</summary>
Motivation: 3DGS在大场景中由于输入视角有限，重建主要适用于相同区域的视角变化（3DoF+），需要解决坐标量化带来的位置误差问题

Method: 分析位置误差对像素投影误差的影响，发现投影误差与投影点距离的平方反比成正比，据此提出基于球坐标的量化方案

Result: 在Garden场景上展示了所提方法的率失真性能

Conclusion: 基于球坐标的量化方案能有效处理3DoF+场景下的坐标量化问题，改善投影精度

Abstract: 3D Gaussian Splatting (3DGS) is a major breakthrough in 3D scene reconstruction. With a number of views of a given object or scene, the algorithm trains a model composed of 3D gaussians, which enables the production of novel views from arbitrary points of view. This freedom of movement is referred to as 6DoF for 6 degrees of freedom: a view is produced for any position (3 degrees), orientation of camera (3 other degrees). On large scenes, though, the input views are acquired from a limited zone in space, and the reconstruction is valuable for novel views from the same zone, even if the scene itself is almost unlimited in size. We refer to this particular case as 3DoF+, meaning that the 3 degrees of freedom of camera position are limited to small offsets around the central position. Considering the problem of coordinate quantization, the impact of position error on the projection error in pixels is studied. It is shown that the projection error is proportional to the squared inverse distance of the point being projected. Consequently, a new quantization scheme based on spherical coordinates is proposed. Rate-distortion performance of the proposed method are illustrated on the well-known Garden scene.

</details>


### [25] [VQualA 2025 Challenge on Image Super-Resolution Generated Content Quality Assessment: Methods and Results](https://arxiv.org/abs/2509.06413)
*Yixiao Li,Xin Li,Chris Wei Zhou,Shuo Xing,Hadi Amirpour,Xiaoshuai Hao,Guanghui Yue,Baoquan Zhao,Weide Liu,Xiaoyuan Yang,Zhengzhong Tu,Xinyu Li,Chuanbiao Song,Chenqi Zhang,Jun Lan,Huijia Zhu,Weiqiang Wang,Xiaoyan Sun,Shishun Tian,Dongyang Yan,Weixia Zhang,Junlin Chen,Wei Sun,Zhihua Wang,Zhuohang Shi,Zhizun Luo,Hang Ouyang,Tianxin Xiao,Fan Yang,Zhaowang Wu,Kaixin Deng*

Main category: cs.CV

TL;DR: ISRGC-Q挑战赛基于ISRGen-QA数据集，专注于评估生成式超分辨率方法（GAN和扩散模型）产生的图像质量，共有108人注册，4个团队提交了SOTA性能的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有超分辨率图像质量评估数据集未能充分涵盖最新生成式方法（如GAN和扩散模型）产生的独特伪影，需要专门的数据集和挑战来有效评估这些现代技术的感知质量。

Method: 构建ISRGen-QA数据集，重点包含生成式超分辨率方法产生的图像，组织挑战赛让参与者开发质量评估算法，最终有4个团队提交有效解决方案进行测试。

Result: 108名参与者注册挑战，4个团队提交的解决方案在ISRGen-QA数据集上达到了最先进的性能表现。

Conclusion: ISRGC-Q挑战赛成功建立了针对生成式超分辨率技术的质量评估基准，推动了该领域的发展，相关项目和数据集已公开提供。

Abstract: This paper presents the ISRGC-Q Challenge, built upon the Image Super-Resolution Generated Content Quality Assessment (ISRGen-QA) dataset, and organized as part of the Visual Quality Assessment (VQualA) Competition at the ICCV 2025 Workshops. Unlike existing Super-Resolution Image Quality Assessment (SR-IQA) datasets, ISRGen-QA places a greater emphasis on SR images generated by the latest generative approaches, including Generative Adversarial Networks (GANs) and diffusion models. The primary goal of this challenge is to analyze the unique artifacts introduced by modern super-resolution techniques and to evaluate their perceptual quality effectively. A total of 108 participants registered for the challenge, with 4 teams submitting valid solutions and fact sheets for the final testing phase. These submissions demonstrated state-of-the-art (SOTA) performance on the ISRGen-QA dataset. The project is publicly available at: https://github.com/Lighting-YXLI/ISRGen-QA.

</details>


### [26] [Perception-oriented Bidirectional Attention Network for Image Super-resolution Quality Assessment](https://arxiv.org/abs/2509.06442)
*Yixiao Li,Xiaoyuan Yang,Guanghui Yue,Jun Fu,Qiuping Jiang,Xu Jia,Paul L. Rosin,Hantao Liu,Wei Zhou*

Main category: cs.CV

TL;DR: 提出了PBAN（感知导向双向注意力网络）用于超分辨率图像的全参考质量评估，通过双向注意力机制和多尺度可变形卷积等方法，在感知失真信息方面优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有的超分辨率算法缺乏有效的全参考图像质量评估指标来比较和评估不同算法的性能

Method: 构建了三个模块：图像编码器、感知导向双向注意力模块（包含双向注意力、分组多尺度可变形卷积和子信息激励卷积）和质量预测模块

Result: 大量实验表明PBAN在质量评估方面优于最先进的方法

Conclusion: PBAN通过创新的注意力机制和感知导向设计，为超分辨率图像质量评估提供了有效的解决方案

Abstract: Many super-resolution (SR) algorithms have been proposed to increase image resolution. However, full-reference (FR) image quality assessment (IQA) metrics for comparing and evaluating different SR algorithms are limited. In this work, we propose the Perception-oriented Bidirectional Attention Network (PBAN) for image SR FR-IQA, which is composed of three modules: an image encoder module, a perception-oriented bidirectional attention (PBA) module, and a quality prediction module. First, we encode the input images for feature representations. Inspired by the characteristics of the human visual system, we then construct the perception-oriented PBA module. Specifically, different from existing attention-based SR IQA methods, we conceive a Bidirectional Attention to bidirectionally construct visual attention to distortion, which is consistent with the generation and evaluation processes of SR images. To further guide the quality assessment towards the perception of distorted information, we propose Grouped Multi-scale Deformable Convolution, enabling the proposed method to adaptively perceive distortion. Moreover, we design Sub-information Excitation Convolution to direct visual perception to both sub-pixel and sub-channel attention. Finally, the quality prediction module is exploited to integrate quality-aware features and regress quality scores. Extensive experiments demonstrate that our proposed PBAN outperforms state-of-the-art quality assessment methods.

</details>


### [27] [TIDE: Achieving Balanced Subject-Driven Image Generation via Target-Instructed Diffusion Enhancement](https://arxiv.org/abs/2509.06499)
*Jibai Lin,Bo Ma,Yating Yang,Rong Ma,Turghun Osman,Ahtamjan Ahmat,Rui Dong,Lei Wang,Xi Zhou*

Main category: cs.CV

TL;DR: TIDE框架通过目标监督和偏好学习解决主题驱动图像生成中保持主体身份与遵循编辑指令之间的冲突，无需测试时微调即可实现更好的平衡效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理主题驱动图像生成时，无法充分解决保持主体身份和遵循动态编辑指令之间的张力问题，需要新的解决方案。

Method: 提出TIDE框架，采用目标监督三元组对齐方法，使用(参考图像、指令、目标图像)三元组建模主体适应动态，通过DSD目标函数训练模型，使用系统生成的"获胜"和"失败"目标进行偏好学习。

Result: 在标准基准测试中，TIDE在生成主体忠实输出同时保持指令遵循方面表现优异，在多个量化指标上超越基线方法，并能成功应用于结构条件生成、图像到图像生成和文本图像插值等多样化任务。

Conclusion: TIDE框架通过创新的目标监督和偏好学习方法，有效解决了主题驱动图像生成中的关键挑战，为文本到图像扩散模型的发展提供了有力工具。

Abstract: Subject-driven image generation (SDIG) aims to manipulate specific subjects within images while adhering to textual instructions, a task crucial for advancing text-to-image diffusion models. SDIG requires reconciling the tension between maintaining subject identity and complying with dynamic edit instructions, a challenge inadequately addressed by existing methods. In this paper, we introduce the Target-Instructed Diffusion Enhancing (TIDE) framework, which resolves this tension through target supervision and preference learning without test-time fine-tuning. TIDE pioneers target-supervised triplet alignment, modelling subject adaptation dynamics using a (reference image, instruction, target images) triplet. This approach leverages the Direct Subject Diffusion (DSD) objective, training the model with paired "winning" (balanced preservation-compliance) and "losing" (distorted) targets, systematically generated and evaluated via quantitative metrics. This enables implicit reward modelling for optimal preservation-compliance balance. Experimental results on standard benchmarks demonstrate TIDE's superior performance in generating subject-faithful outputs while maintaining instruction compliance, outperforming baseline methods across multiple quantitative metrics. TIDE's versatility is further evidenced by its successful application to diverse tasks, including structural-conditioned generation, image-to-image generation, and text-image interpolation. Our code is available at https://github.com/KomJay520/TIDE.

</details>


### [28] [CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesis](https://arxiv.org/abs/2509.06579)
*Xin Kong,Daniel Watson,Yannick Strümpler,Michael Niemeyer,Federico Tombari*

Main category: cs.CV

TL;DR: CausNVS是一个自回归多视角扩散模型，支持任意输入输出视角配置，通过序列化生成视角来解决现有方法固定视角数和推理慢的问题。


<details>
  <summary>Details</summary>
Motivation: 现有多视角扩散模型大多采用非自回归形式，限制了在世界建模中的应用，只支持固定视角数且由于同时去噪所有帧导致推理速度慢。

Method: 使用因果掩码和逐帧噪声训练，采用成对相对相机位姿编码(CaPE)实现精确相机控制。推理时结合空间感知滑动窗口、键值缓存和噪声条件增强来减轻漂移。

Result: 实验表明CausNVS支持广泛的相机轨迹，实现灵活的自回归新视角合成，在不同设置下保持一致的强视觉质量。

Conclusion: CausNVS通过自回归方法成功解决了多视角扩散模型在视角灵活性和推理效率方面的限制，为世界建模提供了更好的解决方案。

Abstract: Multi-view diffusion models have shown promise in 3D novel view synthesis, but most existing methods adopt a non-autoregressive formulation. This limits their applicability in world modeling, as they only support a fixed number of views and suffer from slow inference due to denoising all frames simultaneously. To address these limitations, we propose CausNVS, a multi-view diffusion model in an autoregressive setting, which supports arbitrary input-output view configurations and generates views sequentially. We train CausNVS with causal masking and per-frame noise, using pairwise-relative camera pose encodings (CaPE) for precise camera control. At inference time, we combine a spatially-aware sliding-window with key-value caching and noise conditioning augmentation to mitigate drift. Our experiments demonstrate that CausNVS supports a broad range of camera trajectories, enables flexible autoregressive novel view synthesis, and achieves consistently strong visual quality across diverse settings. Project page: https://kxhit.github.io/CausNVS.html.

</details>


### [29] [Hybrid Swin Attention Networks for Simultaneously Low-Dose PET and CT Denoising](https://arxiv.org/abs/2509.06591)
*Yichao Liu,YueYang Teng*

Main category: cs.CV

TL;DR: 提出HSANet网络用于LDCT/PET图像降噪，结合高效全局注意力模块和混合上采样模块，在保持轻量级模型的同时实现优越的降噪性能。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT和PET成像虽然降低了辐射暴露，但会导致噪声和伪影增加，影响诊断准确性，因此需要有效的降噪方法来提升图像质量。

Method: 提出混合Swin注意力网络(HSANet)，包含高效全局注意力(EGA)模块增强空间和通道交互，以及混合上采样模块防止过拟合噪声。

Result: 在公开LDCT/PET数据集上验证，HSANet相比现有方法获得更优越的降噪性能，同时保持轻量级模型，适合标准GPU内存配置部署。

Conclusion: 该方法具有很高的实际临床应用价值，能够在保持辐射安全的同时显著提升低剂量医学图像质量。

Abstract: Low-dose computed tomography (LDCT) and positron emission tomography (PET) have emerged as safer alternatives to conventional imaging modalities by significantly reducing radiation exposure. However, this reduction often results in increased noise and artifacts, which can compromise diagnostic accuracy. Consequently, denoising for LDCT/PET has become a vital area of research aimed at enhancing image quality while maintaining radiation safety. In this study, we introduce a novel Hybrid Swin Attention Network (HSANet), which incorporates Efficient Global Attention (EGA) modules and a hybrid upsampling module. The EGA modules enhance both spatial and channel-wise interaction, improving the network's capacity to capture relevant features, while the hybrid upsampling module mitigates the risk of overfitting to noise. We validate the proposed approach using a publicly available LDCT/PET dataset. Experimental results demonstrate that HSANet achieves superior denoising performance compared to existing methods, while maintaining a lightweight model size suitable for deployment on GPUs with standard memory configurations. This makes our approach highly practical for real-world clinical applications.

</details>


### [30] [VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level Guidance in Large Scenes](https://arxiv.org/abs/2509.06685)
*Shengkai Zhang,Yuhe Liu,Guanjun Wu,Jianhua He,Xinggang Wang,Mozi Chen,Kezhong Liu*

Main category: cs.CV

TL;DR: VIM-GS是一个基于高斯泼溅的单目图像新视角合成框架，通过结合视觉惯性SfM的稀疏深度和大基础模型的稠密深度，解决了大场景中深度估计的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统高斯泼溅方法需要RGB-D/立体相机提供准确深度，但深度传感范围有限，难以处理大场景。单目图像缺乏深度信息，而现有大基础模型的单目深度估计存在跨帧不一致、远距离不准确和纹理歧义等问题。

Method: 利用视觉惯性SfM的准确稀疏深度来优化大基础模型的稠密但粗糙深度。提出对象分割深度传播算法来连接稀疏输入和稠密输出，并开发动态深度优化模块处理动态对象的SfM深度缺陷。

Result: 在公开和定制数据集上的实验表明，VIM-GS在大场景中具有优越的渲染质量。

Conclusion: VIM-GS通过融合稀疏准确深度和稠密粗糙深度，成功实现了大场景中高质量的单目图像新视角合成，解决了现有方法的深度估计限制。

Abstract: VIM-GS is a Gaussian Splatting (GS) framework using monocular images for novel-view synthesis (NVS) in large scenes. GS typically requires accurate depth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limited depth sensing range makes it difficult for GS to work in large scenes. Monocular images, however, lack depth to guide the learning and lead to inferior NVS results. Although large foundation models (LFMs) for monocular depth estimation are available, they suffer from cross-frame inconsistency, inaccuracy for distant scenes, and ambiguity in deceptive texture cues. This paper aims to generate dense, accurate depth images from monocular RGB inputs for high-definite GS rendering. The key idea is to leverage the accurate but sparse depth from visual-inertial Structure-from-Motion (SfM) to refine the dense but coarse depth from LFMs. To bridge the sparse input and dense output, we propose an object-segmented depth propagation algorithm that renders the depth of pixels of structured objects. Then we develop a dynamic depth refinement module to handle the crippled SfM depth of dynamic objects and refine the coarse LFM depth. Experiments using public and customized datasets demonstrate the superior rendering quality of VIM-GS in large scenes.

</details>


### [31] [STAGE: Segmentation-oriented Industrial Anomaly Synthesis via Graded Diffusion with Explicit Mask Alignment](https://arxiv.org/abs/2509.06693)
*Xichen Xu,Yanshu Wang,Jinbao Wang,Qunyi Zhang,Xiaoning Lei,Guoyang Xie,Guannan Jiang,Zhichao Lu*

Main category: cs.CV

TL;DR: STAGE提出了一种基于分级滴果模型的异常合成方法，通过明确掩码对齐策略和异常仅分支来提高异常分割的精度和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的分割导向工业异常合成方法存在细节缺失、背景对齐不准和像素级异常生成困难等问题。

Method: 使用清洁背景作为先验信息指导去噪分布，采用分级滴果框架与异常仅分支记录局部异常，通过EMA策略进行渐进式掩码对齐。

Result: 在MVTec和BTAD数据集上达到了最先进的SIAS性能，显著提升了下游异常分割效果。

Conclusion: STAGE方法能够生成具有细致纹理和上下文一致性的工业异常数据，为异常检测领域提供了有效的数据扩充方案。

Abstract: Segmentation-oriented Industrial Anomaly Synthesis (SIAS) plays a pivotal role in enhancing the performance of downstream anomaly segmentation, as it provides an effective means of expanding abnormal data. However, existing SIAS methods face several critical limitations: (i) the synthesized anomalies often lack intricate texture details and fail to align precisely with the surrounding background, and (ii) they struggle to generate fine-grained, pixel-level anomalies. To address these challenges, we propose Segmentation-oriented Anomaly synthesis via Graded diffusion with Explicit mask alignment, termed STAGE. STAGE introduces a novel anomaly inference strategy that incorporates clean background information as a prior to guide the denoising distribution, enabling the model to more effectively distinguish and highlight abnormal foregrounds. Furthermore, it employs a graded diffusion framework with an anomaly-only branch to explicitly record local anomalies during both the forward and reverse processes, ensuring that subtle anomalies are not overlooked. Finally, STAGE incorporates the explicit mask alignment (EMA) strategy to progressively align the synthesized anomalies with the background, resulting in context-consistent and structurally coherent generations. Extensive experiments on the MVTec and BTAD datasets demonstrate that STAGE achieves state-of-the-art performance in SIAS, which in turn enhances downstream anomaly segmentation.

</details>


### [32] [Zero-shot 3D-Aware Trajectory-Guided image-to-video generation via Test-Time Training](https://arxiv.org/abs/2509.06723)
*Ruicheng Zhang,Jun Zhou,Zunnan Xu,Zihao Liu,Jiehui Huang,Mingyang Zhang,Yu Sun,Xiu Li*

Main category: cs.CV

TL;DR: Zo3T是一个零样本测试时训练框架，通过3D感知运动投影、轨迹引导测试时LoRA适配器和引导场校正，实现了高质量的轨迹引导图像到视频生成，无需大量标注数据和微调。


<details>
  <summary>Details</summary>
Motivation: 现有的轨迹引导图像到视频生成方法要么需要计算昂贵的微调和稀缺的标注数据，要么在潜在空间中进行轨迹控制时忽略3D透视，导致不真实的运动和潜在空间与网络噪声预测之间的不对齐。

Method: 1. 3D感知运动投影：利用推断的场景深度推导透视正确的仿射变换；2. 轨迹引导测试时LoRA：动态注入和优化临时LoRA适配器，通过区域特征一致性损失实现运动约束；3. 引导场校正：通过一步前瞻策略优化条件引导场，确保生成过程朝向目标轨迹。

Result: Zo3T显著提高了轨迹控制图像到视频生成的3D真实感和运动准确性，在性能上优于现有的基于训练和零样本方法。

Conclusion: Zo3T框架通过创新的3D感知技术和测试时自适应机制，成功解决了轨迹引导图像到视频生成中的关键挑战，为高质量视频合成提供了有效的零样本解决方案。

Abstract: Trajectory-Guided image-to-video (I2V) generation aims to synthesize videos that adhere to user-specified motion instructions. Existing methods typically rely on computationally expensive fine-tuning on scarce annotated datasets. Although some zero-shot methods attempt to trajectory control in the latent space, they may yield unrealistic motion by neglecting 3D perspective and creating a misalignment between the manipulated latents and the network's noise predictions. To address these challenges, we introduce Zo3T, a novel zero-shot test-time-training framework for trajectory-guided generation with three core innovations: First, we incorporate a 3D-Aware Kinematic Projection, leveraging inferring scene depth to derive perspective-correct affine transformations for target regions. Second, we introduce Trajectory-Guided Test-Time LoRA, a mechanism that dynamically injects and optimizes ephemeral LoRA adapters into the denoising network alongside the latent state. Driven by a regional feature consistency loss, this co-adaptation effectively enforces motion constraints while allowing the pre-trained model to locally adapt its internal representations to the manipulated latent, thereby ensuring generative fidelity and on-manifold adherence. Finally, we develop Guidance Field Rectification, which refines the denoising evolutionary path by optimizing the conditional guidance field through a one-step lookahead strategy, ensuring efficient generative progression towards the target trajectory. Zo3T significantly enhances 3D realism and motion accuracy in trajectory-controlled I2V generation, demonstrating superior performance over existing training-based and zero-shot approaches.

</details>


### [33] [AIM 2025 Challenge on High FPS Motion Deblurring: Methods and Results](https://arxiv.org/abs/2509.06793)
*George Ciubotariu,Florin-Alexandru Vasluianu,Zhuyun Zhou,Nancy Mehta,Radu Timofte,Ke Wu,Long Sun,Lingshun Kong,Zhongbao Yang,Jinshan Pan,Jiangxin Dong,Jinhui Tang,Hao Chen,Yinghui Fang,Dafeng Zhang,Yongqi Song,Jiangbo Guo,Shuhua Jin,Zeyu Xiao,Rui Zhao,Zhuoyuan Li,Cong Zhang,Yufeng Peng,Xin Lu,Zhijing Sun,Chengjie Ge,Zihao Li,Zishun Liao,Ziang Zhou,Qiyu Kang,Xueyang Fu,Zheng-Jun Zha,Yuqian Zhang,Shuai Liu,Jie Liu,Zhuhao Zhang,Lishen Qu,Zhihao Liu,Shihao Zhou,Yaqi Luo,Juncheng Zhou,Jufeng Yang,Qianfeng Yang,Qiyuan Guan,Xiang Chen,Guiyue Jin,Jiyu Jin*

Main category: cs.CV

TL;DR: AIM 2025高帧率非均匀运动去模糊挑战赛综述，评估了9个团队的有效解决方案，展示了单图像运动去模糊领域的最新进展，并使用了新的MIORe数据集。


<details>
  <summary>Details</summary>
Motivation: 识别能够在多样化挑战条件下通过学习复杂运动类型的代表性视觉线索，产生更清晰和视觉吸引力图像的有效网络。

Method: 通过竞赛形式，68名参与者注册，9个团队提交有效方案，使用新颖的MIORe数据集进行评估，该数据集包含具有挑战性的运动模式样本。

Result: 展示了高帧率单图像运动去模糊领域的最先进进展，证明了该领域的显著进步。

Conclusion: 挑战赛成功识别了有效的去模糊网络，MIORe数据集为复杂运动模式提供了有价值的测试基准，推动了非均匀运动去模糊技术的发展。

Abstract: This paper presents a comprehensive review of the AIM 2025 High FPS Non-Uniform Motion Deblurring Challenge, highlighting the proposed solutions and final results. The objective of this challenge is to identify effective networks capable of producing clearer and visually compelling images in diverse and challenging conditions, by learning representative visual cues for complex aggregations of motion types. A total of 68 participants registered for the competition, and 9 teams ultimately submitted valid entries. This paper thoroughly evaluates the state-of-the-art advances in high-FPS single image motion deblurring, showcasing the significant progress in the field, while leveraging samples of the novel dataset, MIORe, that introduces challenging examples of movement patterns.

</details>


### [34] [MIORe & VAR-MIORe: Benchmarks to Push the Boundaries of Restoration](https://arxiv.org/abs/2509.06803)
*George Ciubotariu,Zhuyun Zhou,Zongwei Wu,Radu Timofte*

Main category: cs.CV

TL;DR: MIORe和VAR-MIORe是两个新颖的多任务运动恢复数据集，通过1000FPS高速采集和专业光学设备捕捉复杂运动场景，提供高质量的运动模糊和清晰帧数据，用于视频帧插值和光流估计等任务。


<details>
  <summary>Details</summary>
Motivation: 解决当前运动恢复基准测试的关键局限性，缺乏高质量、多样化的运动场景数据和精确的运动幅度控制。

Method: 采用1000FPS高速采集和专业光学设备，基于计算的光流指标自适应平均帧来生成一致的运动模糊，同时保留清晰输入帧。VAR-MIORe进一步扩展了可变运动幅度范围。

Result: 创建了首个能够提供运动幅度显式控制的基准测试，提供高分辨率、可扩展的真实数据，在受控和恶劣条件下挑战现有算法。

Conclusion: 这些数据集为下一代图像和视频恢复任务的研究奠定了基础，推动了运动恢复领域的发展。

Abstract: We introduce MIORe and VAR-MIORe, two novel multi-task datasets that address critical limitations in current motion restoration benchmarks. Designed with high-frame-rate (1000 FPS) acquisition and professional-grade optics, our datasets capture a broad spectrum of motion scenarios, which include complex ego-camera movements, dynamic multi-subject interactions, and depth-dependent blur effects. By adaptively averaging frames based on computed optical flow metrics, MIORe generates consistent motion blur, and preserves sharp inputs for video frame interpolation and optical flow estimation. VAR-MIORe further extends by spanning a variable range of motion magnitudes, from minimal to extreme, establishing the first benchmark to offer explicit control over motion amplitude. We provide high-resolution, scalable ground truths that challenge existing algorithms under both controlled and adverse conditions, paving the way for next-generation research of various image and video restoration tasks.

</details>


### [35] [A New Hybrid Model of Generative Adversarial Network and You Only Look Once Algorithm for Automatic License-Plate Recognition](https://arxiv.org/abs/2509.06868)
*Behnoud Shafiezadeh,Amir Mashmool,Farshad Eshghi,Manoochehr Kelarestaghi*

Main category: cs.CV

TL;DR: 本文提出了一种结合选择性GAN去模糊预处理和YOLOv5目标检测的自动车牌识别系统，在伊朗车牌数据集上实现了95%的车牌检测准确率和97%的字符识别准确率，检测时间仅0.026秒，特别适用于模糊场景。


<details>
  <summary>Details</summary>
Motivation: 自动车牌识别(ALPR)在智能交通系统中至关重要，但由于车牌的高变异性，传统方法效率低下。特别是模糊车牌的处理成为技术难点，需要更高效的深度学习方法来解决。

Method: 采用选择性GAN进行去模糊预处理，结合YOLOv5进行车牌检测和字符分割识别。选择性预处理避免不必要的输入操作，YOLOv5提供高精度和低计算成本。

Result: YOLOv5在车牌检测和字符识别阶段仅需0.026秒检测时间，车牌检测准确率95%，字符识别准确率97%。Deblur-GAN预处理使检测准确率提升近40%，特别适用于模糊车牌场景。

Conclusion: 结合YOLOv5和Deblur-GAN的集成模型在精度和检测时间方面表现优异，特别适合便携式应用和模糊输入场景，为智能交通系统提供了有效的解决方案。

Abstract: Automatic License-Plate Recognition (ALPR) plays a pivotal role in Intelligent Transportation Systems (ITS) as a fundamental element of Smart Cities. However, due to its high variability, ALPR faces challenging issues more efficiently addressed by deep learning techniques. In this paper, a selective Generative Adversarial Network (GAN) is proposed for deblurring in the preprocessing step, coupled with the state-of-the-art You-Only-Look-Once (YOLO)v5 object detection architectures for License-Plate Detection (LPD), and the integrated Character Segmentation (CS) and Character Recognition (CR) steps. The selective preprocessing bypasses unnecessary and sometimes counter-productive input manipulations, while YOLOv5 LPD/CS+CR delivers high accuracy and low computing cost. As a result, YOLOv5 achieves a detection time of 0.026 seconds for both LP and CR detection stages, facilitating real-time applications with exceptionally rapid responsiveness. Moreover, the proposed model achieves accuracy rates of 95\% and 97\% in the LPD and CR detection phases, respectively. Furthermore, the inclusion of the Deblur-GAN pre-processor significantly improves detection accuracy by nearly 40\%, especially when encountering blurred License Plates (LPs).To train and test the learning components, we generated and publicly released our blur and ALPR datasets (using Iranian license plates as a use-case), which are more representative of close-to-real-life ad-hoc situations. The findings demonstrate that employing the state-of-the-art YOLO model results in excellent overall precision and detection time, making it well-suited for portable applications. Additionally, integrating the Deblur-GAN model as a preliminary processing step enhances the overall effectiveness of our comprehensive model, particularly when confronted with blurred scenes captured by the camera as input.

</details>


### [36] [BIR-Adapter: A Low-Complexity Diffusion Model Adapter for Blind Image Restoration](https://arxiv.org/abs/2509.06904)
*Cem Eteke,Alexander Griessel,Wolfgang Kellerer,Eckehard Steinbach*

Main category: cs.CV

TL;DR: BIR-Adapter是一个低复杂度的盲图像恢复适配器，利用预训练扩散模型的先验知识，无需额外训练特征提取器，在保持竞争力的同时显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 利用预训练大规模扩散模型的先验知识进行盲图像恢复，避免训练额外的特征提取器，降低计算复杂度。

Method: 从退化图像中通过模型本身提取特征，扩展自注意力机制引入退化特征，并采用采样引导机制减少幻觉。

Result: 在合成和真实退化数据上达到或超越最先进方法的性能，同时计算复杂度显著降低，可集成到其他扩散模型中扩展应用。

Conclusion: BIR-Adapter提供了一种高效且通用的盲图像恢复解决方案，能够有效利用预训练扩散模型的先验知识，具有良好的扩展性和应用前景。

Abstract: This paper introduces BIR-Adapter, a low-complexity blind image restoration adapter for diffusion models. The BIR-Adapter enables the utilization of the prior of pre-trained large-scale diffusion models on blind image restoration without training any auxiliary feature extractor. We take advantage of the robustness of pretrained models. We extract features from degraded images via the model itself and extend the self-attention mechanism with these degraded features. We introduce a sampling guidance mechanism to reduce hallucinations. We perform experiments on synthetic and real-world degradations and demonstrate that BIR-Adapter achieves competitive or better performance compared to state-of-the-art methods while having significantly lower complexity. Additionally, its adapter-based design enables integration into other diffusion models, enabling broader applications in image restoration tasks. We showcase this by extending a super-resolution-only model to perform better under additional unknown degradations.

</details>


### [37] [Interleaving Reasoning for Better Text-to-Image Generation](https://arxiv.org/abs/2509.06945)
*Wenxuan Huang,Shuang Chen,Zheyong Xie,Shaosheng Cao,Shixiang Tang,Yufan Shen,Qingyu Yin,Wenbo Hu,Xiaoman Wang,Yuntian Tang,Junbo Qiao,Yue Guo,Yao Hu,Zhenfei Yin,Philip Torr,Yu Cheng,Wanli Ouyang,Shaohui Lin*

Main category: cs.CV

TL;DR: IRG框架通过交替文本推理和图像合成来提升文本到图像生成质量，在多个基准测试中取得5-10个百分点的绝对提升


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型在图像生成能力上虽有进步，但在指令遵循和细节保持方面仍落后于GPT-4o等理解-生成紧密耦合的系统，因此探索交替推理是否能进一步改进T2I生成

Method: 提出IRG框架：首先生成文本推理指导初始图像，然后对结果进行反思以细化细节、视觉质量和美学，同时保持语义。使用IRGL-300K数据集进行两阶段训练，先建立稳健的思考和反思能力，然后在完整思考-图像轨迹数据上微调

Result: 在GenEval、WISE、TIIF、GenAI-Bench和OneIG-EN等基准测试上实现state-of-the-art性能，获得5-10个百分点的绝对增益，视觉质量和细粒度保真度显著提升

Conclusion: 交替推理框架有效提升了文本到图像生成的质量和细节保持能力，证明了在生成过程中引入文本推理反思机制的重要价值

Abstract: Unified multimodal understanding and generation models recently have achieve significant improvement in image generation capability, yet a large gap remains in instruction following and detail preservation compared to systems that tightly couple comprehension with generation such as GPT-4o. Motivated by recent advances in interleaving reasoning, we explore whether such reasoning can further improve Text-to-Image (T2I) generation. We introduce Interleaving Reasoning Generation (IRG), a framework that alternates between text-based thinking and image synthesis: the model first produces a text-based thinking to guide an initial image, then reflects on the result to refine fine-grained details, visual quality, and aesthetics while preserving semantics. To train IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL), which targets two sub-goals: (1) strengthening the initial think-and-generate stage to establish core content and base quality, and (2) enabling high-quality textual reflection and faithful implementation of those refinements in a subsequent image. We curate IRGL-300K, a dataset organized into six decomposed learning modes that jointly cover learning text-based thinking, and full thinking-image trajectories. Starting from a unified foundation model that natively emits interleaved text-image outputs, our two-stage training first builds robust thinking and reflection, then efficiently tunes the IRG pipeline in the full thinking-image trajectory data. Extensive experiments show SoTA performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality and fine-grained fidelity. The code, model weights and datasets will be released in: https://github.com/Osilly/Interleaving-Reasoning-Generation .

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [38] [From Image Generation to Infrastructure Design: a Multi-agent Pipeline for Street Design Generation](https://arxiv.org/abs/2509.05469)
*Chenguang Wang,Xiang Yan,Yilong Dai,Ziyi Wang,Susu Xu*

Main category: cs.AI

TL;DR: 一个多模态多满意系统，通过车道定位、提示优化、设计生成和自动评估等步骤，在真实街道图像上编辑和重新设计自行车设施，生成现实而上下文适宜的设计。


<details>
  <summary>Details</summary>
Motivation: 现有的街道设计渲染方法劳动密集，影响公众参与和协作决策，而AI生成方法通常需要大量领域特定数据且无法精确控制复杂街道场景中的空间变化。

Method: 提出一个多满意系统，集成了车道定位、提示优化、设计生成和自动评估等模块，直接在真实街道图像上编辑和重新设计自行车设施。

Result: 在多样化城市场景中进行实验，证明系统能够适应不同路网几何形状和环境条件，一质性地产生视觉一致且符合指令要求的结果。

Conclusion: 这项工作为应用多满意管道到交通基础设施规划和设施设计领域奠定了基础。

Abstract: Realistic visual renderings of street-design scenarios are essential for public engagement in active transportation planning. Traditional approaches are labor-intensive, hindering collective deliberation and collaborative decision-making. While AI-assisted generative design shows transformative potential by enabling rapid creation of design scenarios, existing generative approaches typically require large amounts of domain-specific training data and struggle to enable precise spatial variations of design/configuration in complex street-view scenes. We introduce a multi-agent system that edits and redesigns bicycle facilities directly on real-world street-view imagery. The framework integrates lane localization, prompt optimization, design generation, and automated evaluation to synthesize realistic, contextually appropriate designs. Experiments across diverse urban scenarios demonstrate that the system can adapt to varying road geometries and environmental conditions, consistently yielding visually coherent and instruction-compliant results. This work establishes a foundation for applying multi-agent pipelines to transportation infrastructure planning and facility design.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [39] [A Synthetic-to-Real Dehazing Method based on Domain Unification](https://arxiv.org/abs/2509.05374)
*Zhiqiang Yuan,Jinchao Zhang,Jie Zhou*

Main category: eess.IV

TL;DR: 提出了一种基于领域统一的合成到真实去雾方法，通过统一真实和合成领域的关系，使去雾模型更符合实际情况，显著提升了在真实雾霾图像上的性能。


<details>
  <summary>Details</summary>
Motivation: 由于分布偏移，基于深度学习的图像去雾方法在应用于真实雾霾图像时性能下降。研究发现这种偏差源于干净数据收集的不完美，真实领域的大气物理模型与合成领域不一致。

Method: 基于领域统一的合成到真实去雾方法，通过统一真实和合成领域之间的关系，使去雾模型更符合实际场景的复杂性。

Result: 大量实验定性和定量地证明，所提出的去雾方法在真实世界图像上显著优于最先进的方法。

Conclusion: 通过领域统一的方法可以有效解决合成与真实领域之间的分布偏移问题，提升去雾模型在真实场景中的性能表现。

Abstract: Due to distribution shift, the performance of deep learning-based method for image dehazing is adversely affected when applied to real-world hazy images. In this paper, we find that such deviation in dehazing task between real and synthetic domains may come from the imperfect collection of clean data. Owing to the complexity of the scene and the effect of depth, the collected clean data cannot strictly meet the ideal conditions, which makes the atmospheric physics model in the real domain inconsistent with that in the synthetic domain. For this reason, we come up with a synthetic-to-real dehazing method based on domain unification, which attempts to unify the relationship between the real and synthetic domain, thus to let the dehazing model more in line with the actual situation. Extensive experiments qualitatively and quantitatively demonstrate that the proposed dehazing method significantly outperforms state-of-the-art methods on real-world images.

</details>


### [40] [Imagining Alternatives: Towards High-Resolution 3D Counterfactual Medical Image Generation via Language Guidance](https://arxiv.org/abs/2509.05978)
*Mohamed Mohamed,Brennan Nichyporuk,Douglas L. Arnold,Tal Arbel*

Main category: eess.IV

TL;DR: 首个基于语言提示生成高质量3D医学反事实图像的方法，针对神经影像数据开发了原生3D扩散模型，能够模拟多发性硬化症和阿尔茨海默病的不同病理状态。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在2D图像生成方面表现出色，但由于缺乏3D预训练基础模型，基于自然语言生成高分辨率3D医学反事实图像的能力尚未被探索，这限制了临床和研究应用的发展。

Method: 采用最先进的3D扩散模型，结合Simple Diffusion的增强技术，引入增强条件机制来改善文本对齐和图像质量，开发了专门针对神经影像数据的原生3D扩散模型。

Result: 在两个不同的神经MRI数据集上成功模拟了多发性硬化症的不同病灶负荷和阿尔茨海默病的认知状态，生成高质量图像的同时保持了合成医学图像中受试者的保真度。

Conclusion: 这项工作为3D医学影像中的提示驱动疾病进展分析奠定了基础，展示了语言引导的3D医学图像生成的可行性，具有重要的临床应用潜力。

Abstract: Vision-language models have demonstrated impressive capabilities in generating 2D images under various conditions; however the impressive performance of these models in 2D is largely enabled by extensive, readily available pretrained foundation models. Critically, comparable pretrained foundation models do not exist for 3D, significantly limiting progress in this domain. As a result, the potential of vision-language models to produce high-resolution 3D counterfactual medical images conditioned solely on natural language descriptions remains completely unexplored. Addressing this gap would enable powerful clinical and research applications, such as personalized counterfactual explanations, simulation of disease progression scenarios, and enhanced medical training by visualizing hypothetical medical conditions in realistic detail. Our work takes a meaningful step toward addressing this challenge by introducing a framework capable of generating high-resolution 3D counterfactual medical images of synthesized patients guided by free-form language prompts. We adapt state-of-the-art 3D diffusion models with enhancements from Simple Diffusion and incorporate augmented conditioning to improve text alignment and image quality. To our knowledge, this represents the first demonstration of a language-guided native-3D diffusion model applied specifically to neurological imaging data, where faithful three-dimensional modeling is essential to represent the brain's three-dimensional structure. Through results on two distinct neurological MRI datasets, our framework successfully simulates varying counterfactual lesion loads in Multiple Sclerosis (MS), and cognitive states in Alzheimer's disease, generating high-quality images while preserving subject fidelity in synthetically generated medical images. Our results lay the groundwork for prompt-driven disease progression analysis within 3D medical imaging.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [41] [ManipDreamer3D : Synthesizing Plausible Robotic Manipulation Video with Occupancy-aware 3D Trajectory](https://arxiv.org/abs/2509.05314)
*Ying Li,Xiaobao Wei,Xiaowei Chi,Yuming Li,Zhongyu Zhao,Hao Wang,Ningning Ma,Ming Lu,Shanghang Zhang*

Main category: cs.RO

TL;DR: ManipDreamer3D是一个新颖的框架，通过结合3D轨迹规划和轨迹到视频扩散模型，从输入图像和文本指令生成逼真的3D感知机器人操作视频，解决了现有方法依赖2D轨迹导致的3D空间模糊问题。


<details>
  <summary>Details</summary>
Motivation: 机器人操作领域面临数据稀缺的挑战，现有扩散模型方法主要依赖2D轨迹，存在3D空间模糊问题，需要开发能够生成具有合理3D轨迹的机器人操作视频的方法。

Method: 1) 从输入图像重建3D占据图表示；2) 计算优化的3D末端执行器轨迹，最小化路径长度并避免碰撞；3) 使用潜在编辑技术从初始图像潜在和优化轨迹创建视频序列；4) 通过专门训练的轨迹到视频扩散模型生成拾取放置视频。

Result: 实验结果表明，该方法生成的机器人视频具有自主规划的合理3D轨迹，显著减少了人工干预需求，在视觉质量上优于现有方法。

Conclusion: ManipDreamer3D框架成功解决了3D空间模糊问题，能够生成高质量的3D感知机器人操作视频，为机器人操作数据生成提供了有效的解决方案。

Abstract: Data scarcity continues to be a major challenge in the field of robotic manipulation. Although diffusion models provide a promising solution for generating robotic manipulation videos, existing methods largely depend on 2D trajectories, which inherently face issues with 3D spatial ambiguity. In this work, we present a novel framework named ManipDreamer3D for generating plausible 3D-aware robotic manipulation videos from the input image and the text instruction. Our method combines 3D trajectory planning with a reconstructed 3D occupancy map created from a third-person perspective, along with a novel trajectory-to-video diffusion model. Specifically, ManipDreamer3D first reconstructs the 3D occupancy representation from the input image and then computes an optimized 3D end-effector trajectory, minimizing path length while avoiding collisions. Next, we employ a latent editing technique to create video sequences from the initial image latent and the optimized 3D trajectory. This process conditions our specially trained trajectory-to-video diffusion model to produce robotic pick-and-place videos. Our method generates robotic videos with autonomously planned plausible 3D trajectories, significantly reducing human intervention requirements. Experimental results demonstrate superior visual quality compared to existing methods.

</details>


### [42] [Evaluation of Large Language Models for Anomaly Detection in Autonomous Vehicles](https://arxiv.org/abs/2509.05315)
*Petros Loukas,David Bassir,Savvas Chatzichristofis,Angelos Amanatiadis*

Main category: cs.RO

TL;DR: 本文评估了大型语言模型在自动驾驶车辆真实边缘案例中的表现，提出了结合开放词汇目标检测和提示工程的架构，用于异常检测。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估主要局限于合成数据集或人工驾驶数据集，缺乏真实世界边缘案例的测试，特别是现有感知规划算法在这些情况下的表现评估。

Method: 采用开放词汇目标检测器结合提示工程和大型语言模型上下文推理的架构，在真实边缘案例上评估多个先进模型。

Result: 提供了定性比较结果，讨论了LLM作为自动驾驶车辆异常检测器的潜在应用价值。

Conclusion: LLM在自动驾驶异常检测方面具有应用潜力，特别是在处理真实世界边缘案例时能提供有价值的上下文推理能力。

Abstract: The rapid evolution of large language models (LLMs) has pushed their boundaries to many applications in various domains. Recently, the research community has started to evaluate their potential adoption in autonomous vehicles and especially as complementary modules in the perception and planning software stacks. However, their evaluation is limited in synthetic datasets or manually driving datasets without the ground truth knowledge and more precisely, how the current perception and planning algorithms would perform in the cases under evaluation. For this reason, this work evaluates LLMs on real-world edge cases where current autonomous vehicles have been proven to fail. The proposed architecture consists of an open vocabulary object detector coupled with prompt engineering and large language model contextual reasoning. We evaluate several state-of-the-art models against real edge cases and provide qualitative comparison results along with a discussion on the findings for the potential application of LLMs as anomaly detectors in autonomous vehicles.

</details>
