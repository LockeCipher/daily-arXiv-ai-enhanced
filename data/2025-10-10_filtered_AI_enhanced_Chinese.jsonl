{"id": "2510.07340", "pdf": "https://arxiv.org/pdf/2510.07340", "abs": "https://arxiv.org/abs/2510.07340", "authors": ["Yongzhi Li", "Saining Zhang", "Yibing Chen", "Boying Li", "Yanxin Zhang", "Xiaoyu Du"], "title": "SpotDiff: Spotting and Disentangling Interference in Feature Space for Subject-Preserving Image Generation", "categories": ["cs.GR", "cs.LG"], "comment": null, "summary": "Personalized image generation aims to faithfully preserve a reference subject's identity while adapting to diverse text prompts. Existing optimization-based methods ensure high fidelity but are computationally expensive, while learning-based approaches offer efficiency at the cost of entangled representations influenced by nuisance factors. We introduce SpotDiff, a novel learning-based method that extracts subject-specific features by spotting and disentangling interference. Leveraging a pre-trained CLIP image encoder and specialized expert networks for pose and background, SpotDiff isolates subject identity through orthogonality constraints in the feature space. To enable principled training, we introduce SpotDiff10k, a curated dataset with consistent pose and background variations. Experiments demonstrate that SpotDiff achieves more robust subject preservation and controllable editing than prior methods, while attaining competitive performance with only 10k training samples.", "AI": {"tldr": "SpotDiff\u662f\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u89e3\u7f20\u5e72\u6270\u6765\u63d0\u53d6\u7279\u5b9a\u4e3b\u9898\u7684\u7279\u5f81\uff0c\u5728\u4fdd\u6301\u4e3b\u9898\u8eab\u4efd\u7684\u540c\u65f6\u5b9e\u73b0\u53ef\u63a7\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u5b66\u4e60\u65b9\u6cd5\u6548\u7387\u9ad8\u4f46\u53d7\u5e72\u6270\u56e0\u7d20\u5f71\u54cd\u5bfc\u81f4\u8868\u793a\u7ea0\u7f20\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u53c8\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3CLIP\u56fe\u50cf\u7f16\u7801\u5668\u548c\u4e13\u95e8\u7684\u59ff\u6001\u3001\u80cc\u666f\u4e13\u5bb6\u7f51\u7edc\uff0c\u901a\u8fc7\u7279\u5f81\u7a7a\u95f4\u6b63\u4ea4\u6027\u7ea6\u675f\u6765\u5206\u79bb\u4e3b\u9898\u8eab\u4efd\u3002\u6784\u5efaSpotDiff10k\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "SpotDiff\u5728\u4e3b\u9898\u4fdd\u6301\u548c\u53ef\u63a7\u7f16\u8f91\u65b9\u9762\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u9c81\u68d2\uff0c\u4ec5\u752810k\u8bad\u7ec3\u6837\u672c\u5c31\u8fbe\u5230\u7ade\u4e89\u6027\u80fd\u3002", "conclusion": "SpotDiff\u901a\u8fc7\u89e3\u7f20\u5e72\u6270\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\uff0c\u5728\u4fdd\u771f\u5ea6\u548c\u53ef\u63a7\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.07343", "pdf": "https://arxiv.org/pdf/2510.07343", "abs": "https://arxiv.org/abs/2510.07343", "authors": ["Shaorong Zhang", "Rob Brekelmans", "Greg Ver Steeg"], "title": "Local MAP Sampling for Diffusion Models", "categories": ["cs.GR", "cs.AI", "eess.IV"], "comment": null, "summary": "Diffusion Posterior Sampling (DPS) provides a principled Bayesian approach to inverse problems by sampling from $p(x_0 \\mid y)$. However, in practice, the goal of inverse problem solving is not to cover the posterior but to recover the most accurate reconstruction, where optimization-based diffusion solvers often excel despite lacking a clear probabilistic foundation. We introduce Local MAP Sampling (LMAPS), a new inference framework that iteratively solving local MAP subproblems along the diffusion trajectory. This perspective clarifies their connection to global MAP estimation and DPS, offering a unified probabilistic interpretation for optimization-based methods. Building on this foundation, we develop practical algorithms with a probabilistically interpretable covariance approximation, a reformulated objective for stability and interpretability, and a gradient approximation for non-differentiable operators. Across a broad set of image restoration and scientific tasks, LMAPS achieves state-of-the-art performance, including $\\geq 2$ dB gains on motion deblurring, JPEG restoration, and quantization, and $>1.5$ dB improvements on inverse scattering benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86Local MAP Sampling (LMAPS)\u6846\u67b6\uff0c\u901a\u8fc7\u6cbf\u6269\u6563\u8f68\u8ff9\u8fed\u4ee3\u6c42\u89e3\u5c40\u90e8MAP\u5b50\u95ee\u9898\uff0c\u4e3a\u57fa\u4e8e\u4f18\u5316\u7684\u6269\u6563\u6c42\u89e3\u5668\u63d0\u4f9b\u7edf\u4e00\u7684\u6982\u7387\u89e3\u91ca\uff0c\u5e76\u5728\u591a\u4e2a\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u867d\u7136DPS\u63d0\u4f9b\u4e86\u89e3\u51b3\u9006\u95ee\u9898\u7684\u8d1d\u53f6\u65af\u65b9\u6cd5\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u4f18\u5316\u65b9\u6cd5\u5f80\u5f80\u8868\u73b0\u66f4\u597d\u5374\u7f3a\u4e4f\u660e\u786e\u7684\u6982\u7387\u57fa\u7840\u3002\u9700\u8981\u4e3a\u4f18\u5316\u65b9\u6cd5\u63d0\u4f9b\u7edf\u4e00\u7684\u6982\u7387\u89e3\u91ca\u3002", "method": "\u5f15\u5165LMAPS\u6846\u67b6\uff0c\u8fed\u4ee3\u6c42\u89e3\u6269\u6563\u8f68\u8ff9\u4e0a\u7684\u5c40\u90e8MAP\u5b50\u95ee\u9898\uff1b\u5f00\u53d1\u4e86\u6982\u7387\u53ef\u89e3\u91ca\u7684\u534f\u65b9\u5dee\u8fd1\u4f3c\u3001\u91cd\u65b0\u8868\u8ff0\u7684\u76ee\u6807\u51fd\u6570\u4ee5\u63d0\u9ad8\u7a33\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4ee5\u53ca\u975e\u53ef\u5fae\u7b97\u5b50\u7684\u68af\u5ea6\u8fd1\u4f3c\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u56fe\u50cf\u6062\u590d\u548c\u79d1\u5b66\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff1a\u8fd0\u52a8\u53bb\u6a21\u7cca\u3001JPEG\u6062\u590d\u548c\u91cf\u5316\u4efb\u52a1\u4e0a\u83b7\u5f97\u22652dB\u589e\u76ca\uff0c\u9006\u6563\u5c04\u57fa\u51c6\u4e0a\u83b7\u5f97>1.5dB\u6539\u8fdb\u3002", "conclusion": "LMAPS\u4e3a\u4f18\u5316\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6982\u7387\u89e3\u91ca\uff0c\u8fde\u63a5\u4e86\u5168\u5c40MAP\u4f30\u8ba1\u548cDPS\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2510.07868", "pdf": "https://arxiv.org/pdf/2510.07868", "abs": "https://arxiv.org/abs/2510.07868", "authors": ["Haojie Jin", "Jierui Ren", "Yisong Chen", "Guoping Wang", "Sheng Li"], "title": "NRRS: Neural Russian Roulette and Splitting", "categories": ["cs.GR"], "comment": "15 pages", "summary": "We propose a novel framework for Russian Roulette and Splitting (RRS) tailored to wavefront path tracing, a highly parallel rendering architecture that processes path states in batched, stage-wise execution for efficient GPU utilization. Traditional RRS methods, with unpredictable path counts, are fundamentally incompatible with wavefront's preallocated memory and scheduling requirements. To resolve this, we introduce a normalized RRS formulation with a bounded path count, enabling stable and memory-efficient execution.   Furthermore, we pioneer the use of neural networks to learn RRS factors, presenting two models: NRRS and AID-NRRS. At a high level, both feature a carefully designed RRSNet that explicitly incorporates RRS normalization, with only subtle differences in their implementation. To balance computational cost and inference accuracy, we introduce Mix-Depth, a path-depth-aware mechanism that adaptively regulates neural evaluation, further improving efficiency.   Extensive experiments demonstrate that our method outperforms traditional heuristics and recent RRS techniques in both rendering quality and performance across a variety of complex scenes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u4e3a\u6ce2\u524d\u8def\u5f84\u8ffd\u8e2a\u8bbe\u8ba1\u7684\u4fc4\u7f57\u65af\u8f6e\u76d8\u8d4c\u548c\u5206\u88c2\uff08RRS\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5f52\u4e00\u5316RRS\u516c\u5f0f\u548c\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60RRS\u56e0\u5b50\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfRRS\u65b9\u6cd5\u4e0e\u6ce2\u524d\u67b6\u6784\u7684\u5185\u5b58\u9884\u5206\u914d\u9700\u6c42\u4e0d\u517c\u5bb9\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfRRS\u65b9\u6cd5\u7531\u4e8e\u8def\u5f84\u6570\u91cf\u4e0d\u53ef\u9884\u6d4b\uff0c\u4e0e\u6ce2\u524d\u8def\u5f84\u8ffd\u8e2a\u7684\u9ad8\u5ea6\u5e76\u884c\u3001\u6279\u5904\u7406\u67b6\u6784\u7684\u5185\u5b58\u9884\u5206\u914d\u548c\u8c03\u5ea6\u8981\u6c42\u5b58\u5728\u6839\u672c\u6027\u4e0d\u517c\u5bb9\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165\u5f52\u4e00\u5316RRS\u516c\u5f0f\u786e\u4fdd\u8def\u5f84\u6570\u91cf\u6709\u754c\uff1b\u63d0\u51fa\u4e24\u79cd\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff08NRRS\u548cAID-NRRS\uff09\u5b66\u4e60RRS\u56e0\u5b50\uff1b\u8bbe\u8ba1RRSNet\u663e\u5f0f\u6574\u5408RRS\u5f52\u4e00\u5316\uff1b\u5f15\u5165Mix-Depth\u673a\u5236\u6839\u636e\u8def\u5f84\u6df1\u5ea6\u81ea\u9002\u5e94\u8c03\u8282\u795e\u7ecf\u8bc4\u4f30\u3002", "result": "\u5728\u591a\u79cd\u590d\u6742\u573a\u666f\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u6e32\u67d3\u8d28\u91cf\u548c\u6027\u80fd\u65b9\u9762\u5747\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u8fd1\u671fRRS\u6280\u672f\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u6ce2\u524d\u8def\u5f84\u8ffd\u8e2a\u4e2dRRS\u7684\u5185\u5b58\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60RRS\u56e0\u5b50\u548c\u81ea\u9002\u5e94\u8c03\u8282\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6e32\u67d3\u6548\u679c\u548c\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2510.08166", "pdf": "https://arxiv.org/pdf/2510.08166", "abs": "https://arxiv.org/abs/2510.08166", "authors": ["Elias Kristmann", "Markus Sch\u00fctz", "Michael Wimmer"], "title": "Variable-Rate Texture Compression: Real-Time Rendering with JPEG", "categories": ["cs.GR"], "comment": null, "summary": "Although variable-rate compressed image formats such as JPEG are widely used to efficiently encode images, they have not found their way into real-time rendering due to special requirements such as random access to individual texels. In this paper, we investigate the feasibility of variable-rate texture compression on modern GPUs using the JPEG format, and how it compares to the GPU-friendly fixed-rate compression approaches BC1 and ASTC. Using a deferred rendering pipeline, we are able to identify the subset of blocks that are needed for a given frame, decode these, and colorize the framebuffer's pixels. Despite the additional $\\sim$0.17 bit per pixel that we require for our approach, JPEG maintains significantly better quality and compression rates compared to BC1, and depending on the type of image, outperforms or competes with ASTC. The JPEG rendering pipeline increases rendering duration by less than 0.3 ms on an RTX 4090, demonstrating that sophisticated variable-rate compression schemes are feasible on modern GPUs, even in VR. Source code and data sets are available at: https://github.com/elias1518693/jpeg_textures", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u73b0\u4ee3GPU\u4e0a\u4f7f\u7528JPEG\u683c\u5f0f\u8fdb\u884c\u53ef\u53d8\u901f\u7387\u7eb9\u7406\u538b\u7f29\u7684\u53ef\u884c\u6027\uff0c\u5e76\u4e0e\u56fa\u5b9a\u901f\u7387\u538b\u7f29\u65b9\u6cd5BC1\u548cASTC\u8fdb\u884c\u6bd4\u8f83\u3002\u901a\u8fc7\u5ef6\u8fdf\u6e32\u67d3\u7ba1\u7ebf\uff0c\u80fd\u591f\u8bc6\u522b\u6240\u9700\u7eb9\u7406\u5757\u5e76\u89e3\u7801\uff0c\u5728RTX 4090\u4e0a\u4ec5\u589e\u52a00.3ms\u6e32\u67d3\u65f6\u95f4\u3002", "motivation": "\u867d\u7136JPEG\u7b49\u53ef\u53d8\u901f\u7387\u538b\u7f29\u56fe\u50cf\u683c\u5f0f\u5e7f\u6cdb\u7528\u4e8e\u9ad8\u6548\u7f16\u7801\u56fe\u50cf\uff0c\u4f46\u7531\u4e8e\u9700\u8981\u968f\u673a\u8bbf\u95ee\u5355\u4e2a\u7eb9\u7406\u50cf\u7d20\u7b49\u7279\u6b8a\u8981\u6c42\uff0c\u5c1a\u672a\u5728\u5b9e\u65f6\u6e32\u67d3\u4e2d\u5f97\u5230\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5728\u73b0\u4ee3GPU\u4e0a\u5b9e\u73b0\u53ef\u53d8\u901f\u7387\u7eb9\u7406\u538b\u7f29\u7684\u53ef\u80fd\u6027\u3002", "method": "\u4f7f\u7528\u5ef6\u8fdf\u6e32\u67d3\u7ba1\u7ebf\u8bc6\u522b\u6bcf\u5e27\u6240\u9700\u7684\u7eb9\u7406\u5757\u5b50\u96c6\uff0c\u89e3\u7801\u8fd9\u4e9b\u5757\u5e76\u4e3a\u5e27\u7f13\u51b2\u533a\u7684\u50cf\u7d20\u7740\u8272\u3002\u4e0e\u56fa\u5b9a\u901f\u7387\u538b\u7f29\u65b9\u6cd5BC1\u548cASTC\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u5c3d\u7ba1\u9700\u8981\u989d\u5916\u7ea60.17\u4f4d/\u50cf\u7d20\u7684\u5f00\u9500\uff0c\u4f46JPEG\u5728\u8d28\u91cf\u548c\u538b\u7f29\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8eBC1\uff0c\u6839\u636e\u56fe\u50cf\u7c7b\u578b\uff0c\u8868\u73b0\u4f18\u4e8e\u6216\u4e0eASTC\u76f8\u5f53\u3002\u5728RTX 4090\u4e0a\u6e32\u67d3\u65f6\u95f4\u4ec5\u589e\u52a0\u4e0d\u52300.3ms\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u5728VR\u5e94\u7528\u4e2d\uff0c\u73b0\u4ee3GPU\u4e0a\u5b9e\u73b0\u590d\u6742\u7684\u53ef\u53d8\u901f\u7387\u538b\u7f29\u65b9\u6848\u662f\u53ef\u884c\u7684\uff0c\u4e3a\u5b9e\u65f6\u6e32\u67d3\u4e2d\u7684\u9ad8\u6548\u7eb9\u7406\u538b\u7f29\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.07441", "pdf": "https://arxiv.org/pdf/2510.07441", "abs": "https://arxiv.org/abs/2510.07441", "authors": ["Nithin C. Babu", "Aniruddha Mahapatra", "Harsh Rangwani", "Rajiv Soundararajan", "Kuldeep Kulkarni"], "title": "DynamicEval: Rethinking Evaluation for Dynamic Text-to-Video Synthesis", "categories": ["cs.CV"], "comment": "Preprint. Under review. 26 pages, 11 figures, 11 tables. Access the   project page in https://nithincbabu7.github.io/DynamicEval", "summary": "Existing text-to-video (T2V) evaluation benchmarks, such as VBench and EvalCrafter, suffer from two limitations. (i) While the emphasis is on subject-centric prompts or static camera scenes, camera motion essential for producing cinematic shots and existing metrics under dynamic motion are largely unexplored. (ii) These benchmarks typically aggregate video-level scores into a single model-level score for ranking generative models. Such aggregation, however, overlook video-level evaluation, which is vital to selecting the better video among the candidate videos generated for a given prompt. To address these gaps, we introduce DynamicEval, a benchmark consisting of systematically curated prompts emphasizing dynamic camera motion, paired with 45k human annotations on video pairs from 3k videos generated by ten T2V models. DynamicEval evaluates two key dimensions of video quality: background scene consistency and foreground object consistency. For background scene consistency, we obtain the interpretable error maps based on the Vbench motion smoothness metric. We observe that while the Vbench motion smoothness metric shows promising alignment with human judgments, it fails in two cases: occlusions/disocclusions arising from camera and foreground object movements. Building on this, we propose a new background consistency metric that leverages object error maps to correct two failure cases in a principled manner. Our second innovation is the introduction of a foreground consistency metric that tracks points and their neighbors within each object instance to assess object fidelity. Extensive experiments demonstrate that our proposed metrics achieve stronger correlations with human preferences at both the video level and the model level (an improvement of more than 2% points), establishing DynamicEval as a more comprehensive benchmark for evaluating T2V models under dynamic camera motion.", "AI": {"tldr": "DynamicEval\u662f\u4e00\u4e2a\u65b0\u7684\u6587\u672c\u5230\u89c6\u9891\u8bc4\u4f30\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u52a8\u6001\u76f8\u673a\u8fd0\u52a8\u8bc4\u4f30\uff0c\u5305\u542b\u7cbe\u5fc3\u7b56\u5212\u7684\u63d0\u793a\u8bed\u548c45k\u4eba\u5de5\u6807\u6ce8\uff0c\u63d0\u51fa\u4e86\u80cc\u666f\u573a\u666f\u4e00\u81f4\u6027\u548c\u524d\u666f\u5bf9\u8c61\u4e00\u81f4\u6027\u7684\u65b0\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u73b0\u6709T2V\u8bc4\u4f30\u57fa\u51c6\u5b58\u5728\u4e24\u4e2a\u5c40\u9650\uff1a\u5ffd\u89c6\u52a8\u6001\u76f8\u673a\u8fd0\u52a8\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u5c06\u89c6\u9891\u7ea7\u8bc4\u5206\u805a\u5408\u4e3a\u6a21\u578b\u7ea7\u8bc4\u5206\u65f6\u5ffd\u7565\u4e86\u89c6\u9891\u7ea7\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002", "method": "\u6784\u5efaDynamicEval\u57fa\u51c6\uff0c\u5305\u542b\u5f3a\u8c03\u52a8\u6001\u76f8\u673a\u8fd0\u52a8\u7684\u63d0\u793a\u8bed\u548c45k\u4eba\u5de5\u6807\u6ce8\uff1b\u63d0\u51fa\u80cc\u666f\u4e00\u81f4\u6027\u6307\u6807\uff0c\u57fa\u4e8eVBench\u8fd0\u52a8\u5e73\u6ed1\u5ea6\u6307\u6807\u751f\u6210\u53ef\u89e3\u91ca\u9519\u8bef\u56fe\uff0c\u5e76\u901a\u8fc7\u5bf9\u8c61\u9519\u8bef\u56fe\u4fee\u6b63\u906e\u6321/\u53bb\u906e\u6321\u95ee\u9898\uff1b\u63d0\u51fa\u524d\u666f\u4e00\u81f4\u6027\u6307\u6807\uff0c\u8ddf\u8e2a\u5bf9\u8c61\u5b9e\u4f8b\u5185\u7684\u70b9\u53ca\u5176\u90bb\u5c45\u6765\u8bc4\u4f30\u5bf9\u8c61\u4fdd\u771f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u6307\u6807\u5728\u89c6\u9891\u7ea7\u548c\u6a21\u578b\u7ea7\u4e0e\u4eba\u7c7b\u504f\u597d\u76f8\u5173\u6027\u66f4\u5f3a\uff08\u63d0\u5347\u8d85\u8fc72\u4e2a\u767e\u5206\u70b9\uff09\uff0c\u5efa\u7acb\u4e86\u66f4\u5168\u9762\u7684\u52a8\u6001\u76f8\u673a\u8fd0\u52a8T2V\u6a21\u578b\u8bc4\u4f30\u57fa\u51c6\u3002", "conclusion": "DynamicEval\u901a\u8fc7\u5173\u6ce8\u52a8\u6001\u76f8\u673a\u8fd0\u52a8\u548c\u89c6\u9891\u7ea7\u8bc4\u4f30\uff0c\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684T2V\u6a21\u578b\u8bc4\u4f30\u6846\u67b6\uff0c\u65b0\u63d0\u51fa\u7684\u80cc\u666f\u548c\u524d\u666f\u4e00\u81f4\u6027\u6307\u6807\u663e\u8457\u63d0\u5347\u4e86\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2510.08271", "pdf": "https://arxiv.org/pdf/2510.08271", "abs": "https://arxiv.org/abs/2510.08271", "authors": ["Andreas Engelhardt", "Mark Boss", "Vikram Voletti", "Chun-Han Yao", "Hendrik P. A. Lensch", "Varun Jampani"], "title": "SViM3D: Stable Video Material Diffusion for Single Image 3D Generation", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted by International Conference on Computer Vision (ICCV 2025).   Project page: http://svim3d.aengelhardt.com", "summary": "We present Stable Video Materials 3D (SViM3D), a framework to predict multi-view consistent physically based rendering (PBR) materials, given a single image. Recently, video diffusion models have been successfully used to reconstruct 3D objects from a single image efficiently. However, reflectance is still represented by simple material models or needs to be estimated in additional steps to enable relighting and controlled appearance edits. We extend a latent video diffusion model to output spatially varying PBR parameters and surface normals jointly with each generated view based on explicit camera control. This unique setup allows for relighting and generating a 3D asset using our model as neural prior. We introduce various mechanisms to this pipeline that improve quality in this ill-posed setting. We show state-of-the-art relighting and novel view synthesis performance on multiple object-centric datasets. Our method generalizes to diverse inputs, enabling the generation of relightable 3D assets useful in AR/VR, movies, games and other visual media.", "AI": {"tldr": "SViM3D\u662f\u4e00\u4e2a\u4ece\u5355\u5f20\u56fe\u50cf\u9884\u6d4b\u591a\u89c6\u89d2\u4e00\u81f4PBR\u6750\u8d28\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u5c55\u89c6\u9891\u6269\u6563\u6a21\u578b\u6765\u8054\u5408\u8f93\u51faPBR\u53c2\u6570\u548c\u8868\u9762\u6cd5\u7ebf\uff0c\u652f\u6301\u91cd\u5149\u7167\u548c3D\u8d44\u4ea7\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f7f\u7528\u7b80\u5355\u6750\u8d28\u6a21\u578b\uff0c\u8981\u4e48\u9700\u8981\u989d\u5916\u6b65\u9aa4\u4f30\u8ba1\u53cd\u5c04\u7387\uff0c\u65e0\u6cd5\u6709\u6548\u652f\u6301\u91cd\u5149\u7167\u548c\u53ef\u63a7\u5916\u89c2\u7f16\u8f91\u3002", "method": "\u6269\u5c55\u6f5c\u5728\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u57fa\u4e8e\u663e\u5f0f\u76f8\u673a\u63a7\u5236\u8054\u5408\u8f93\u51fa\u7a7a\u95f4\u53d8\u5316\u7684PBR\u53c2\u6570\u548c\u8868\u9762\u6cd5\u7ebf\uff0c\u5f15\u5165\u591a\u79cd\u673a\u5236\u6539\u8fdb\u8d28\u91cf\u3002", "result": "\u5728\u591a\u4e2a\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u91cd\u5149\u7167\u548c\u65b0\u89c6\u89d2\u5408\u6210\u6027\u80fd\uff0c\u80fd\u6cdb\u5316\u5230\u591a\u6837\u5316\u8f93\u5165\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u53ef\u7528\u4e8eAR/VR\u3001\u7535\u5f71\u3001\u6e38\u620f\u7b49\u89c6\u89c9\u5a92\u4f53\u7684\u53ef\u91cd\u5149\u71673D\u8d44\u4ea7\u3002"}}
{"id": "2510.07470", "pdf": "https://arxiv.org/pdf/2510.07470", "abs": "https://arxiv.org/abs/2510.07470", "authors": ["Marien Renaud", "Julien Hermant", "Deliang Wei", "Yu Sun"], "title": "Provably Accelerated Imaging with Restarted Inertia and Score-based Image Priors", "categories": ["cs.CV", "94A08, 68U10"], "comment": "62 pages", "summary": "Fast convergence and high-quality image recovery are two essential features of algorithms for solving ill-posed imaging inverse problems. Existing methods, such as regularization by denoising (RED), often focus on designing sophisticated image priors to improve reconstruction quality, while leaving convergence acceleration to heuristics. To bridge the gap, we propose Restarted Inertia with Score-based Priors (RISP) as a principled extension of RED. RISP incorporates a restarting inertia for fast convergence, while still allowing score-based image priors for high-quality reconstruction. We prove that RISP attains a faster stationary-point convergence rate than RED, without requiring the convexity of the image prior. We further derive and analyze the associated continuous-time dynamical system, offering insight into the connection between RISP and the heavy-ball ordinary differential equation (ODE). Experiments across a range of imaging inverse problems demonstrate that RISP enables fast convergence while achieving high-quality reconstructions.", "AI": {"tldr": "RISP\u662f\u4e00\u79cd\u6539\u8fdb\u7684RED\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u91cd\u542f\u60ef\u6027\u548c\u57fa\u4e8e\u5206\u6570\u7684\u56fe\u50cf\u5148\u9a8c\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u91cd\u5efa\u7684\u540c\u65f6\u5b9e\u73b0\u5feb\u901f\u6536\u655b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982RED\u4e13\u6ce8\u4e8e\u8bbe\u8ba1\u590d\u6742\u56fe\u50cf\u5148\u9a8c\u6765\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\uff0c\u4f46\u6536\u655b\u52a0\u901f\u4f9d\u8d56\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002\u9700\u8981\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u63d0\u51faRISP\u7b97\u6cd5\uff0c\u7ed3\u5408\u91cd\u542f\u60ef\u6027\u5b9e\u73b0\u5feb\u901f\u6536\u655b\uff0c\u540c\u65f6\u4f7f\u7528\u57fa\u4e8e\u5206\u6570\u7684\u56fe\u50cf\u5148\u9a8c\u8fdb\u884c\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eRISP\u5728\u591a\u79cd\u6210\u50cf\u9006\u95ee\u9898\u4e2d\u5b9e\u73b0\u5feb\u901f\u6536\u655b\u548c\u9ad8\u8d28\u91cf\u91cd\u5efa\uff0c\u7406\u8bba\u5206\u6790\u663e\u793a\u5176\u5177\u6709\u6bd4RED\u66f4\u5feb\u7684\u9a7b\u70b9\u6536\u655b\u901f\u7387\u3002", "conclusion": "RISP\u4e3a\u6210\u50cf\u9006\u95ee\u9898\u63d0\u4f9b\u4e86\u65e2\u5feb\u901f\u6536\u655b\u53c8\u9ad8\u8d28\u91cf\u91cd\u5efa\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5efa\u7acb\u4e86\u4e0e\u91cd\u7403ODE\u7684\u7406\u8bba\u8054\u7cfb\u3002"}}
{"id": "2510.07492", "pdf": "https://arxiv.org/pdf/2510.07492", "abs": "https://arxiv.org/abs/2510.07492", "authors": ["Guoliang Gong", "Man Yu"], "title": "A Denoising Framework for Real-World Ultra-Low Dose Lung CT Images Based on an Image Purification Strategy", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Ultra-low dose CT (uLDCT) significantly reduces radiation exposure but introduces severe noise and artifacts. It also leads to substantial spatial misalignment between uLDCT and normal dose CT (NDCT) image pairs. This poses challenges for directly applying existing denoising networks trained on synthetic noise or aligned data. To address this core challenge in uLDCT denoising, this paper proposes an innovative denoising framework based on an Image Purification (IP) strategy. First, we construct a real clinical uLDCT lung dataset. Then, we propose an Image Purification strategy that generates structurally aligned uLDCT-NDCT image pairs, providing a high-quality data foundation for network training. Building upon this, we propose a Frequency-domain Flow Matching (FFM) model, which works synergistically with the IP strategy to excellently preserve the anatomical structure integrity of denoised images. Experiments on the real clinical dataset demonstrate that our IP strategy significantly enhances the performance of multiple mainstream denoising models on the uLDCT task. Notably, our proposed FFM model combined with the IP strategy achieves state-of-the-art (SOTA) results in anatomical structure preservation. This study provides an effective solution to the data mismatch problem in real-world uLDCT denoising. Code and dataset are available at https://github.com/MonkeyDadLufy/flow-matching.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u51c0\u5316\u7b56\u7565\u7684\u8d85\u4f4e\u5242\u91cfCT\u53bb\u566a\u6846\u67b6\uff0c\u89e3\u51b3\u771f\u5b9e\u4e34\u5e8auLDCT\u4e0e\u6b63\u5e38\u5242\u91cfCT\u56fe\u50cf\u95f4\u7684\u7a7a\u95f4\u9519\u4f4d\u95ee\u9898\uff0c\u901a\u8fc7\u9891\u7387\u57df\u6d41\u5339\u914d\u6a21\u578b\u5b9e\u73b0\u4f18\u5f02\u7684\u89e3\u5256\u7ed3\u6784\u4fdd\u6301\u6548\u679c\u3002", "motivation": "\u8d85\u4f4e\u5242\u91cfCT\u663e\u8457\u964d\u4f4e\u8f90\u5c04\u5242\u91cf\u4f46\u5f15\u5165\u4e25\u91cd\u566a\u58f0\u548c\u4f2a\u5f71\uff0c\u5bfc\u81f4uLDCT\u4e0eNDCT\u56fe\u50cf\u5bf9\u5b58\u5728\u663e\u8457\u7a7a\u95f4\u9519\u4f4d\uff0c\u73b0\u6709\u53bb\u566a\u7f51\u7edc\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u3002", "method": "\u6784\u5efa\u771f\u5b9e\u4e34\u5e8auLDCT\u80ba\u90e8\u6570\u636e\u96c6\uff1b\u63d0\u51fa\u56fe\u50cf\u51c0\u5316\u7b56\u7565\u751f\u6210\u7ed3\u6784\u5bf9\u9f50\u7684\u56fe\u50cf\u5bf9\uff1b\u8bbe\u8ba1\u9891\u7387\u57df\u6d41\u5339\u914d\u6a21\u578b\u4e0eIP\u7b56\u7565\u534f\u540c\u5de5\u4f5c\u3002", "result": "\u5728\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\uff0cIP\u7b56\u7565\u663e\u8457\u63d0\u5347\u591a\u79cd\u4e3b\u6d41\u53bb\u566a\u6a21\u578b\u6027\u80fd\uff1bFFM\u6a21\u578b\u7ed3\u5408IP\u7b56\u7565\u5728\u89e3\u5256\u7ed3\u6784\u4fdd\u6301\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u771f\u5b9e\u4e16\u754cuLDCT\u53bb\u566a\u4e2d\u7684\u6570\u636e\u4e0d\u5339\u914d\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.08491", "pdf": "https://arxiv.org/pdf/2510.08491", "abs": "https://arxiv.org/abs/2510.08491", "authors": ["Xilong Zhou", "Bao-Huy Nguyen", "Lo\u00efc Magne", "Vladislav Golyanik", "Thomas Leimk\u00fchler", "Christian Theobalt"], "title": "Splat the Net: Radiance Fields with Splattable Neural Primitives", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Radiance fields have emerged as a predominant representation for modeling 3D scene appearance. Neural formulations such as Neural Radiance Fields provide high expressivity but require costly ray marching for rendering, whereas primitive-based methods such as 3D Gaussian Splatting offer real-time efficiency through splatting, yet at the expense of representational power. Inspired by advances in both these directions, we introduce splattable neural primitives, a new volumetric representation that reconciles the expressivity of neural models with the efficiency of primitive-based splatting. Each primitive encodes a bounded neural density field parameterized by a shallow neural network. Our formulation admits an exact analytical solution for line integrals, enabling efficient computation of perspectively accurate splatting kernels. As a result, our representation supports integration along view rays without the need for costly ray marching. The primitives flexibly adapt to scene geometry and, being larger than prior analytic primitives, reduce the number required per scene. On novel-view synthesis benchmarks, our approach matches the quality and speed of 3D Gaussian Splatting while using $10\\times$ fewer primitives and $6\\times$ fewer parameters. These advantages arise directly from the representation itself, without reliance on complex control or adaptation frameworks. The project page is https://vcai.mpi-inf.mpg.de/projects/SplatNet/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53efsplat\u7684\u795e\u7ecf\u57fa\u5143\u8868\u793a\uff0c\u7ed3\u5408\u4e86\u795e\u7ecf\u8f90\u5c04\u573a\u7684\u9ad8\u8868\u8fbe\u80fd\u529b\u548c3D\u9ad8\u65af\u6cfc\u6e85\u7684\u5b9e\u65f6\u6e32\u67d3\u6548\u7387\uff0c\u4f7f\u7528\u66f4\u5c11\u7684\u57fa\u5143\u548c\u53c2\u6570\u8fbe\u5230\u76f8\u540c\u8d28\u91cf", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e2d\uff0c\u795e\u7ecf\u8f90\u5c04\u573a\u8868\u8fbe\u80fd\u529b\u5f3a\u4f46\u6e32\u67d3\u6210\u672c\u9ad8\uff0c3D\u9ad8\u65af\u6cfc\u6e85\u5b9e\u65f6\u9ad8\u6548\u4f46\u8868\u8fbe\u80fd\u529b\u6709\u9650\u3002\u9700\u8981\u4e00\u79cd\u80fd\u517c\u987e\u8868\u8fbe\u80fd\u529b\u548c\u6e32\u67d3\u6548\u7387\u7684\u65b0\u8868\u793a\u65b9\u6cd5", "method": "\u4f7f\u7528\u53efsplat\u7684\u795e\u7ecf\u57fa\u5143\uff0c\u6bcf\u4e2a\u57fa\u5143\u7f16\u7801\u4e00\u4e2a\u6709\u754c\u7684\u795e\u7ecf\u5bc6\u5ea6\u573a\uff0c\u901a\u8fc7\u6d45\u5c42\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5316\u3002\u8be5\u8868\u793a\u5141\u8bb8\u89e3\u6790\u6c42\u89e3\u7ebf\u79ef\u5206\uff0c\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684splatting\u6838\u8ba1\u7b97", "result": "\u5728\u65b0\u89c6\u89d2\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5339\u914d3D\u9ad8\u65af\u6cfc\u6e85\u7684\u8d28\u91cf\u548c\u901f\u5ea6\uff0c\u540c\u65f6\u4f7f\u752810\u500d\u66f4\u5c11\u7684\u57fa\u5143\u548c6\u500d\u66f4\u5c11\u7684\u53c2\u6570", "conclusion": "\u8be5\u8868\u793a\u76f4\u63a5\u5b9e\u73b0\u4e86\u8868\u8fbe\u80fd\u529b\u548c\u6548\u7387\u7684\u5e73\u8861\uff0c\u65e0\u9700\u4f9d\u8d56\u590d\u6742\u7684\u63a7\u5236\u6216\u9002\u914d\u6846\u67b6"}}
{"id": "2510.08530", "pdf": "https://arxiv.org/pdf/2510.08530", "abs": "https://arxiv.org/abs/2510.08530", "authors": ["Zhitong Huang", "Mohan Zhang", "Renhan Wang", "Rui Tang", "Hao Zhu", "Jing Liao"], "title": "X2Video: Adapting Diffusion Models for Multimodal Controllable Neural Video Rendering", "categories": ["cs.GR", "cs.CV", "68U05", "I.3.3; I.3.6"], "comment": "Code, model, and dataset will be released at project page soon:   https://luckyhzt.github.io/x2video", "summary": "We present X2Video, the first diffusion model for rendering photorealistic videos guided by intrinsic channels including albedo, normal, roughness, metallicity, and irradiance, while supporting intuitive multi-modal controls with reference images and text prompts for both global and local regions. The intrinsic guidance allows accurate manipulation of color, material, geometry, and lighting, while reference images and text prompts provide intuitive adjustments in the absence of intrinsic information. To enable these functionalities, we extend the intrinsic-guided image generation model XRGB to video generation by employing a novel and efficient Hybrid Self-Attention, which ensures temporal consistency across video frames and also enhances fidelity to reference images. We further develop a Masked Cross-Attention to disentangle global and local text prompts, applying them effectively onto respective local and global regions. For generating long videos, our novel Recursive Sampling method incorporates progressive frame sampling, combining keyframe prediction and frame interpolation to maintain long-range temporal consistency while preventing error accumulation. To support the training of X2Video, we assembled a video dataset named InteriorVideo, featuring 1,154 rooms from 295 interior scenes, complete with reliable ground-truth intrinsic channel sequences and smooth camera trajectories. Both qualitative and quantitative evaluations demonstrate that X2Video can produce long, temporally consistent, and photorealistic videos guided by intrinsic conditions. Additionally, X2Video effectively accommodates multi-modal controls with reference images, global and local text prompts, and simultaneously supports editing on color, material, geometry, and lighting through parametric tuning. Project page: https://luckyhzt.github.io/x2video", "AI": {"tldr": "X2Video\u662f\u9996\u4e2a\u57fa\u4e8e\u5185\u5728\u901a\u9053\uff08\u53cd\u7167\u7387\u3001\u6cd5\u7ebf\u3001\u7c97\u7cd9\u5ea6\u3001\u91d1\u5c5e\u5ea6\u3001\u8f90\u7167\u5ea6\uff09\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u751f\u6210\u903c\u771f\u89c6\u9891\uff0c\u5e76\u652f\u6301\u53c2\u8003\u56fe\u50cf\u548c\u6587\u672c\u63d0\u793a\u7684\u591a\u6a21\u6001\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u7f3a\u4e4f\u5bf9\u5185\u5728\u901a\u9053\u7684\u7cbe\u786e\u63a7\u5236\u80fd\u529b\uff0c\u65e0\u6cd5\u51c6\u786e\u64cd\u7eb5\u989c\u8272\u3001\u6750\u8d28\u3001\u51e0\u4f55\u548c\u5149\u7167\u7b49\u5c5e\u6027\u3002X2Video\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u5185\u5728\u901a\u9053\u6307\u5bfc\u5b9e\u73b0\u7cbe\u786e\u7684\u5c5e\u6027\u63a7\u5236\u3002", "method": "1. \u6269\u5c55XRGB\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5230\u89c6\u9891\u9886\u57df\uff1b2. \u63d0\u51fa\u6df7\u5408\u81ea\u6ce8\u610f\u529b\u673a\u5236\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\uff1b3. \u5f00\u53d1\u63a9\u7801\u4ea4\u53c9\u6ce8\u610f\u529b\u5206\u79bb\u5168\u5c40\u548c\u5c40\u90e8\u6587\u672c\u63d0\u793a\uff1b4. \u91c7\u7528\u9012\u5f52\u91c7\u6837\u65b9\u6cd5\u751f\u6210\u957f\u89c6\u9891\uff0c\u7ed3\u5408\u5173\u952e\u5e27\u9884\u6d4b\u548c\u5e27\u63d2\u503c\u3002", "result": "X2Video\u80fd\u591f\u751f\u6210\u957f\u65f6\u95f4\u3001\u65f6\u95f4\u4e00\u81f4\u4e14\u903c\u771f\u7684\u89c6\u9891\uff0c\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u540c\u65f6\u652f\u6301\u901a\u8fc7\u53c2\u6570\u8c03\u8282\u5bf9\u989c\u8272\u3001\u6750\u8d28\u3001\u51e0\u4f55\u548c\u5149\u7167\u8fdb\u884c\u7f16\u8f91\u3002", "conclusion": "X2Video\u901a\u8fc7\u5185\u5728\u901a\u9053\u6307\u5bfc\u548c\u521b\u65b0\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\uff0c\u5e76\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u591a\u6a21\u6001\u63a7\u5236\u80fd\u529b\uff0c\u4e3a\u89c6\u9891\u751f\u6210\u9886\u57df\u5e26\u6765\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2510.07546", "pdf": "https://arxiv.org/pdf/2510.07546", "abs": "https://arxiv.org/abs/2510.07546", "authors": ["Soroush Mehraban", "Vida Adeli", "Jacob Rommann", "Babak Taati", "Kyryl Truskovskyi"], "title": "PickStyle: Video-to-Video Style Transfer with Context-Style Adapters", "categories": ["cs.CV"], "comment": null, "summary": "We address the task of video style transfer with diffusion models, where the goal is to preserve the context of an input video while rendering it in a target style specified by a text prompt. A major challenge is the lack of paired video data for supervision. We propose PickStyle, a video-to-video style transfer framework that augments pretrained video diffusion backbones with style adapters and benefits from paired still image data with source-style correspondences for training. PickStyle inserts low-rank adapters into the self-attention layers of conditioning modules, enabling efficient specialization for motion-style transfer while maintaining strong alignment between video content and style. To bridge the gap between static image supervision and dynamic video, we construct synthetic training clips from paired images by applying shared augmentations that simulate camera motion, ensuring temporal priors are preserved. In addition, we introduce Context-Style Classifier-Free Guidance (CS-CFG), a novel factorization of classifier-free guidance into independent text (style) and video (context) directions. CS-CFG ensures that context is preserved in generated video while the style is effectively transferred. Experiments across benchmarks show that our approach achieves temporally coherent, style-faithful, and content-preserving video translations, outperforming existing baselines both qualitatively and quantitatively.", "AI": {"tldr": "PickStyle\u662f\u4e00\u4e2a\u89c6\u9891\u98ce\u683c\u8fc1\u79fb\u6846\u67b6\uff0c\u901a\u8fc7\u63d2\u5165\u4f4e\u79e9\u9002\u914d\u5668\u5230\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u5229\u7528\u914d\u5bf9\u7684\u9759\u6001\u56fe\u50cf\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5b9e\u73b0\u5185\u5bb9\u4fdd\u6301\u548c\u98ce\u683c\u8f6c\u6362\u7684\u89c6\u9891\u98ce\u683c\u8fc1\u79fb\u3002", "motivation": "\u89e3\u51b3\u89c6\u9891\u98ce\u683c\u8fc1\u79fb\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u914d\u5bf9\u89c6\u9891\u6570\u636e\u76d1\u7763\u7684\u6311\u6218\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u9891\u5185\u5bb9\u4e0e\u76ee\u6807\u98ce\u683c\u7684\u5f3a\u5bf9\u9f50\u3002", "method": "\u5728\u6761\u4ef6\u6a21\u5757\u7684\u81ea\u6ce8\u610f\u529b\u5c42\u63d2\u5165\u4f4e\u79e9\u9002\u914d\u5668\uff1b\u4ece\u914d\u5bf9\u56fe\u50cf\u6784\u5efa\u5408\u6210\u8bad\u7ec3\u7247\u6bb5\uff0c\u5e94\u7528\u5171\u4eab\u589e\u5f3a\u6a21\u62df\u76f8\u673a\u8fd0\u52a8\uff1b\u63d0\u51fa\u4e0a\u4e0b\u6587-\u98ce\u683c\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\uff08CS-CFG\uff09\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u65f6\u95f4\u4e00\u81f4\u3001\u98ce\u683c\u5fe0\u5b9e\u4e14\u5185\u5bb9\u4fdd\u6301\u7684\u89c6\u9891\u8f6c\u6362\uff0c\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "PickStyle\u6846\u67b6\u901a\u8fc7\u6709\u6548\u7684\u9002\u914d\u5668\u8bbe\u8ba1\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u9891\u98ce\u683c\u8fc1\u79fb\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u98ce\u683c\u8f6c\u6362\u3002"}}
{"id": "2510.07631", "pdf": "https://arxiv.org/pdf/2510.07631", "abs": "https://arxiv.org/abs/2510.07631", "authors": ["Shreshth Saini", "Shashank Gupta", "Alan C. Bovik"], "title": "Rectified-CFG++ for Flow Based Models", "categories": ["cs.CV"], "comment": "Accepted at NeurIPS 2025", "summary": "Classifier-free guidance (CFG) is the workhorse for steering large diffusion models toward text-conditioned targets, yet its native application to rectified flow (RF) based models provokes severe off-manifold drift, yielding visual artifacts, text misalignment, and brittle behaviour. We present Rectified-CFG++, an adaptive predictor-corrector guidance that couples the deterministic efficiency of rectified flows with a geometry-aware conditioning rule. Each inference step first executes a conditional RF update that anchors the sample near the learned transport path, then applies a weighted conditional correction that interpolates between conditional and unconditional velocity fields. We prove that the resulting velocity field is marginally consistent and that its trajectories remain within a bounded tubular neighbourhood of the data manifold, ensuring stability across a wide range of guidance strengths. Extensive experiments on large-scale text-to-image models (Flux, Stable Diffusion 3/3.5, Lumina) show that Rectified-CFG++ consistently outperforms standard CFG on benchmark datasets such as MS-COCO, LAION-Aesthetic, and T2I-CompBench. Project page: https://rectified-cfgpp.github.io/", "AI": {"tldr": "\u63d0\u51faRectified-CFG++\u65b9\u6cd5\uff0c\u89e3\u51b3\u6807\u51c6CFG\u5728\u6574\u6d41\u6d41\u6a21\u578b\u4e2d\u7684\u79bb\u6d41\u5f62\u6f02\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9884\u6d4b-\u6821\u6b63\u5f15\u5bfc\u786e\u4fdd\u8f68\u8ff9\u5728\u6570\u636e\u6d41\u5f62\u9644\u8fd1\u7a33\u5b9a\u8fd0\u884c\u3002", "motivation": "\u6807\u51c6CFG\u5728\u6574\u6d41\u6d41\u6a21\u578b\u4e2d\u4f1a\u5f15\u8d77\u4e25\u91cd\u7684\u79bb\u6d41\u5f62\u6f02\u79fb\uff0c\u5bfc\u81f4\u89c6\u89c9\u4f2a\u5f71\u3001\u6587\u672c\u4e0d\u5bf9\u9f50\u548c\u4e0d\u7a33\u5b9a\u884c\u4e3a\uff0c\u9700\u8981\u4e00\u79cd\u51e0\u4f55\u611f\u77e5\u7684\u5f15\u5bfc\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u9884\u6d4b-\u6821\u6b63\u5f15\u5bfc\uff0c\u7ed3\u5408\u6574\u6d41\u6d41\u7684\u786e\u5b9a\u6027\u6548\u7387\u4e0e\u51e0\u4f55\u611f\u77e5\u6761\u4ef6\u89c4\u5219\u3002\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u5148\u6267\u884c\u6761\u4ef6RF\u66f4\u65b0\u951a\u5b9a\u6837\u672c\uff0c\u7136\u540e\u5e94\u7528\u52a0\u6743\u6761\u4ef6\u6821\u6b63\u3002", "result": "\u5728Flux\u3001Stable Diffusion 3/3.5\u3001Lumina\u7b49\u5927\u89c4\u6a21\u6587\u751f\u56fe\u6a21\u578b\u4e0a\uff0c\u5728MS-COCO\u3001LAION-Aesthetic\u3001T2I-CompBench\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4e00\u81f4\u4f18\u4e8e\u6807\u51c6CFG\u3002", "conclusion": "Rectified-CFG++\u901a\u8fc7\u786e\u4fdd\u8f68\u8ff9\u5728\u6709\u754c\u7ba1\u72b6\u90bb\u57df\u5185\u8fd0\u884c\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u6574\u6d41\u6d41\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5f15\u5bfc\u65b9\u6cd5\u3002"}}
{"id": "2510.07654", "pdf": "https://arxiv.org/pdf/2510.07654", "abs": "https://arxiv.org/abs/2510.07654", "authors": ["Yanjie Pan", "Qingdong He", "Lidong Wang", "Bo Peng", "Mingmin Chi"], "title": "Once Is Enough: Lightweight DiT-Based Video Virtual Try-On via One-Time Garment Appearance Injection", "categories": ["cs.CV"], "comment": "5 pages (including references), 4 figures. Code and models will be   released upon publication", "summary": "Video virtual try-on aims to replace the clothing of a person in a video with a target garment. Current dual-branch architectures have achieved significant success in diffusion models based on the U-Net; however, adapting them to diffusion models built upon the Diffusion Transformer remains challenging. Initially, introducing latent space features from the garment reference branch requires adding or modifying the backbone network, leading to a large number of trainable parameters. Subsequently, the latent space features of garments lack inherent temporal characteristics and thus require additional learning. To address these challenges, we propose a novel approach, OIE (Once is Enough), a virtual try-on strategy based on first-frame clothing replacement: specifically, we employ an image-based clothing transfer model to replace the clothing in the initial frame, and then, under the content control of the edited first frame, utilize pose and mask information to guide the temporal prior of the video generation model in synthesizing the remaining frames sequentially. Experiments show that our method achieves superior parameter efficiency and computational efficiency while still maintaining leading performance under these constraints.", "AI": {"tldr": "\u63d0\u51faOIE\u65b9\u6cd5\uff0c\u57fa\u4e8e\u9996\u5e27\u670d\u88c5\u66ff\u6362\u7684\u89c6\u9891\u865a\u62df\u8bd5\u7a7f\u7b56\u7565\uff0c\u901a\u8fc7\u56fe\u50cf\u6a21\u578b\u66ff\u6362\u9996\u5e27\u670d\u88c5\uff0c\u7136\u540e\u5229\u7528\u59ff\u6001\u548c\u63a9\u7801\u4fe1\u606f\u5f15\u5bfc\u89c6\u9891\u751f\u6210\u6a21\u578b\u5408\u6210\u540e\u7eed\u5e27\uff0c\u5b9e\u73b0\u9ad8\u6548\u53c2\u6570\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eU-Net\u7684\u53cc\u5206\u652f\u67b6\u6784\u96be\u4ee5\u9002\u5e94\u57fa\u4e8eDiffusion Transformer\u7684\u6269\u6563\u6a21\u578b\uff0c\u5f15\u5165\u670d\u88c5\u53c2\u8003\u5206\u652f\u7684\u6f5c\u5728\u7a7a\u95f4\u7279\u5f81\u9700\u8981\u4fee\u6539\u9aa8\u5e72\u7f51\u7edc\u5bfc\u81f4\u53c2\u6570\u8fc7\u591a\uff0c\u4e14\u670d\u88c5\u6f5c\u5728\u7279\u5f81\u7f3a\u4e4f\u65f6\u95f4\u7279\u6027\u9700\u8981\u989d\u5916\u5b66\u4e60\u3002", "method": "\u4f7f\u7528\u56fe\u50cf\u670d\u88c5\u8f6c\u79fb\u6a21\u578b\u66ff\u6362\u9996\u5e27\u670d\u88c5\uff0c\u5728\u7f16\u8f91\u540e\u7684\u9996\u5e27\u5185\u5bb9\u63a7\u5236\u4e0b\uff0c\u5229\u7528\u59ff\u6001\u548c\u63a9\u7801\u4fe1\u606f\u5f15\u5bfc\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u65f6\u95f4\u5148\u9a8c\uff0c\u987a\u5e8f\u5408\u6210\u5269\u4f59\u5e27\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9886\u5148\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u53c2\u6570\u6548\u7387\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "OIE\u65b9\u6cd5\u901a\u8fc7\u9996\u5e27\u66ff\u6362\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u53cc\u5206\u652f\u67b6\u6784\u5728Diffusion Transformer\u4e2d\u7684\u9002\u5e94\u95ee\u9898\uff0c\u5728\u53c2\u6570\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.07670", "pdf": "https://arxiv.org/pdf/2510.07670", "abs": "https://arxiv.org/abs/2510.07670", "authors": ["Haoyi Duan", "Yunzhi Zhang", "Yilun Du", "Jiajun Wu"], "title": "Controllable Video Synthesis via Variational Inference", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://video-synthesis-variational.github.io/", "summary": "Many video workflows benefit from a mixture of user controls with varying granularity, from exact 4D object trajectories and camera paths to coarse text prompts, while existing video generative models are typically trained for fixed input formats. We develop a video synthesis method that addresses this need and generates samples with high controllability for specified elements while maintaining diversity for under-specified ones. We cast the task as variational inference to approximate a composed distribution, leveraging multiple video generation backbones to account for all task constraints collectively. To address the optimization challenge, we break down the problem into step-wise KL divergence minimization over an annealed sequence of distributions, and further propose a context-conditioned factorization technique that reduces modes in the solution space to circumvent local optima. Experiments suggest that our method produces samples with improved controllability, diversity, and 3D consistency compared to prior works.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u9891\u5408\u6210\u65b9\u6cd5\uff0c\u652f\u6301\u4ece\u7cbe\u786e4D\u5bf9\u8c61\u8f68\u8ff9\u5230\u7c97\u7c92\u5ea6\u6587\u672c\u63d0\u793a\u7684\u591a\u7c92\u5ea6\u7528\u6237\u63a7\u5236\uff0c\u901a\u8fc7\u53d8\u5206\u63a8\u7406\u548c\u9010\u6b65KL\u6563\u5ea6\u6700\u5c0f\u5316\u5b9e\u73b0\u9ad8\u53ef\u63a7\u6027\u548c\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u901a\u5e38\u9488\u5bf9\u56fa\u5b9a\u8f93\u5165\u683c\u5f0f\u8bad\u7ec3\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9700\u8981\u6df7\u5408\u7c92\u5ea6\u7528\u6237\u63a7\u5236\u7684\u5de5\u4f5c\u6d41\u7a0b\u9700\u6c42\u3002", "method": "\u5c06\u4efb\u52a1\u5efa\u6a21\u4e3a\u53d8\u5206\u63a8\u7406\uff0c\u5229\u7528\u591a\u4e2a\u89c6\u9891\u751f\u6210\u9aa8\u5e72\u7f51\u7edc\u5171\u540c\u5904\u7406\u6240\u6709\u4efb\u52a1\u7ea6\u675f\uff0c\u901a\u8fc7\u9010\u6b65KL\u6563\u5ea6\u6700\u5c0f\u5316\u548c\u4e0a\u4e0b\u6587\u6761\u4ef6\u5206\u89e3\u6280\u672f\u89e3\u51b3\u4f18\u5316\u6311\u6218\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u76f8\u6bd4\u5148\u524d\u5de5\u4f5c\uff0c\u5728\u53ef\u63a7\u6027\u3001\u591a\u6837\u6027\u548c3D\u4e00\u81f4\u6027\u65b9\u9762\u4ea7\u751f\u66f4\u597d\u7684\u6837\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u89c6\u9891\u751f\u6210\u4e2d\u591a\u7c92\u5ea6\u63a7\u5236\u7684\u9700\u6c42\uff0c\u5728\u4fdd\u6301\u672a\u6307\u5b9a\u5143\u7d20\u591a\u6837\u6027\u7684\u540c\u65f6\u786e\u4fdd\u6307\u5b9a\u5143\u7d20\u7684\u9ad8\u53ef\u63a7\u6027\u3002"}}
{"id": "2510.07721", "pdf": "https://arxiv.org/pdf/2510.07721", "abs": "https://arxiv.org/abs/2510.07721", "authors": ["Zipeng Guo", "Lichen Ma", "Xiaolong Fu", "Gaojing Zhou", "Lan Yang", "Yuchen Zhou", "Linkai Liu", "Yu He", "Ximan Liu", "Shiping Dong", "Jingling Fu", "Zhen Chen", "Yu Shi", "Junshi Huang", "Jason Li", "Chao Gou"], "title": "RePainter: Empowering E-commerce Object Removal via Spatial-matting Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "In web data, product images are central to boosting user engagement and advertising efficacy on e-commerce platforms, yet the intrusive elements such as watermarks and promotional text remain major obstacles to delivering clear and appealing product visuals. Although diffusion-based inpainting methods have advanced, they still face challenges in commercial settings due to unreliable object removal and limited domain-specific adaptation. To tackle these challenges, we propose Repainter, a reinforcement learning framework that integrates spatial-matting trajectory refinement with Group Relative Policy Optimization (GRPO). Our approach modulates attention mechanisms to emphasize background context, generating higher-reward samples and reducing unwanted object insertion. We also introduce a composite reward mechanism that balances global, local, and semantic constraints, effectively reducing visual artifacts and reward hacking. Additionally, we contribute EcomPaint-100K, a high-quality, large-scale e-commerce inpainting dataset, and a standardized benchmark EcomPaint-Bench for fair evaluation. Extensive experiments demonstrate that Repainter significantly outperforms state-of-the-art methods, especially in challenging scenes with intricate compositions. We will release our code and weights upon acceptance.", "AI": {"tldr": "Repainter\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u56fe\u50cf\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u906e\u7f69\u8f68\u8ff9\u4f18\u5316\u548cGRPO\u7b97\u6cd5\uff0c\u4e13\u95e8\u89e3\u51b3\u7535\u5546\u4ea7\u54c1\u56fe\u50cf\u4e2d\u6c34\u5370\u548c\u4fc3\u9500\u6587\u672c\u7684\u53bb\u9664\u95ee\u9898\uff0c\u5728\u590d\u6742\u573a\u666f\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7535\u5546\u5e73\u53f0\u4e2d\u4ea7\u54c1\u56fe\u50cf\u7684\u6c34\u5370\u548c\u4fc3\u9500\u6587\u672c\u7b49\u4fb5\u5165\u6027\u5143\u7d20\u5f71\u54cd\u4e86\u89c6\u89c9\u6548\u679c\u548c\u5e7f\u544a\u6548\u679c\uff0c\u73b0\u6709\u6269\u6563\u4fee\u590d\u65b9\u6cd5\u5728\u5546\u4e1a\u573a\u666f\u4e2d\u5b58\u5728\u5bf9\u8c61\u79fb\u9664\u4e0d\u53ef\u9760\u548c\u9886\u57df\u9002\u5e94\u6027\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faRepainter\u6846\u67b6\uff0c\u96c6\u6210\u7a7a\u95f4\u906e\u7f69\u8f68\u8ff9\u4f18\u5316\u548cGroup Relative Policy Optimization (GRPO)\uff0c\u901a\u8fc7\u8c03\u8282\u6ce8\u610f\u529b\u673a\u5236\u5f3a\u8c03\u80cc\u666f\u4e0a\u4e0b\u6587\uff0c\u5f15\u5165\u590d\u5408\u5956\u52b1\u673a\u5236\u5e73\u8861\u5168\u5c40\u3001\u5c40\u90e8\u548c\u8bed\u4e49\u7ea6\u675f\u3002", "result": "\u5728EcomPaint-100K\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRepainter\u5728\u590d\u6742\u6784\u56fe\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u6709\u6548\u51cf\u5c11\u89c6\u89c9\u4f2a\u5f71\u548c\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\u3002", "conclusion": "Repainter\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u7535\u5546\u56fe\u50cf\u4fee\u590d\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u4fee\u590d\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.07729", "pdf": "https://arxiv.org/pdf/2510.07729", "abs": "https://arxiv.org/abs/2510.07729", "authors": ["Jian Gao", "Mengqi Yuan", "Yifei Zeng", "Chang Zeng", "Zhihao Li", "Zhenyu Chen", "Weichao Qiu", "Xiao-Xiao Long", "Hao Zhu", "Xun Cao", "Yao Yao"], "title": "ComGS: Efficient 3D Object-Scene Composition via Surface Octahedral Probes", "categories": ["cs.CV"], "comment": null, "summary": "Gaussian Splatting (GS) enables immersive rendering, but realistic 3D object-scene composition remains challenging. Baked appearance and shadow information in GS radiance fields cause inconsistencies when combining objects and scenes. Addressing this requires relightable object reconstruction and scene lighting estimation. For relightable object reconstruction, existing Gaussian-based inverse rendering methods often rely on ray tracing, leading to low efficiency. We introduce Surface Octahedral Probes (SOPs), which store lighting and occlusion information and allow efficient 3D querying via interpolation, avoiding expensive ray tracing. SOPs provide at least a 2x speedup in reconstruction and enable real-time shadow computation in Gaussian scenes. For lighting estimation, existing Gaussian-based inverse rendering methods struggle to model intricate light transport and often fail in complex scenes, while learning-based methods predict lighting from a single image and are viewpoint-sensitive. We observe that 3D object-scene composition primarily concerns the object's appearance and nearby shadows. Thus, we simplify the challenging task of full scene lighting estimation by focusing on the environment lighting at the object's placement. Specifically, we capture a 360 degrees reconstructed radiance field of the scene at the location and fine-tune a diffusion model to complete the lighting. Building on these advances, we propose ComGS, a novel 3D object-scene composition framework. Our method achieves high-quality, real-time rendering at around 28 FPS, produces visually harmonious results with vivid shadows, and requires only 36 seconds for editing. Code and dataset are available at https://nju-3dv.github.io/projects/ComGS/.", "AI": {"tldr": "\u63d0\u51fa\u4e86ComGS\u6846\u67b6\uff0c\u901a\u8fc7\u8868\u9762\u516b\u9762\u4f53\u63a2\u9488(SOPs)\u5b9e\u73b0\u9ad8\u6548\u53ef\u91cd\u5149\u7167\u7269\u4f53\u91cd\u5efa\uff0c\u7ed3\u5408\u573a\u666f\u5149\u7167\u4f30\u8ba1\u6280\u672f\uff0c\u89e3\u51b33D\u7269\u4f53-\u573a\u666f\u5408\u6210\u4e2d\u7684\u5149\u7167\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6e32\u67d3\u548c\u5feb\u901f\u7f16\u8f91\u3002", "motivation": "\u9ad8\u65af\u6cfc\u6e85(GS)\u6280\u672f\u867d\u7136\u80fd\u5b9e\u73b0\u6c89\u6d78\u5f0f\u6e32\u67d3\uff0c\u4f46\u57283D\u7269\u4f53-\u573a\u666f\u5408\u6210\u65f6\u5b58\u5728\u70d8\u57f9\u5916\u89c2\u548c\u9634\u5f71\u4fe1\u606f\u5bfc\u81f4\u7684\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u9700\u8981\u53ef\u91cd\u5149\u7167\u7269\u4f53\u91cd\u5efa\u548c\u573a\u666f\u5149\u7167\u4f30\u8ba1\u3002", "method": "1. \u4f7f\u7528\u8868\u9762\u516b\u9762\u4f53\u63a2\u9488(SOPs)\u5b58\u50a8\u5149\u7167\u548c\u906e\u6321\u4fe1\u606f\uff0c\u901a\u8fc7\u63d2\u503c\u5b9e\u73b0\u9ad8\u65483D\u67e5\u8be2\uff0c\u907f\u514d\u6602\u8d35\u7684\u5c04\u7ebf\u8ffd\u8e2a\uff1b2. \u5728\u7269\u4f53\u653e\u7f6e\u4f4d\u7f6e\u6355\u83b7360\u5ea6\u91cd\u5efa\u8f90\u5c04\u573a\uff0c\u5fae\u8c03\u6269\u6563\u6a21\u578b\u5b8c\u6210\u5149\u7167\u4f30\u8ba1\u3002", "result": "\u5b9e\u73b0\u4e86\u81f3\u5c112\u500d\u7684\u91cd\u5efa\u52a0\u901f\uff0c\u652f\u6301\u9ad8\u65af\u573a\u666f\u4e2d\u7684\u5b9e\u65f6\u9634\u5f71\u8ba1\u7b97\uff0c\u6e32\u67d3\u8d28\u91cf\u9ad8\uff0c\u89c6\u89c9\u548c\u8c10\uff0c\u5177\u6709\u751f\u52a8\u7684\u9634\u5f71\u6548\u679c\uff0c\u5b9e\u65f6\u6e32\u67d3\u7ea628 FPS\uff0c\u7f16\u8f91\u4ec5\u970036\u79d2\u3002", "conclusion": "ComGS\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e863D\u7269\u4f53-\u573a\u666f\u5408\u6210\u4e2d\u7684\u5149\u7167\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u5b9e\u65f6\u7684\u6e32\u67d3\u548c\u5feb\u901f\u7f16\u8f91\uff0c\u4e3a\u6c89\u6d78\u5f0f3D\u573a\u666f\u5408\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.07741", "pdf": "https://arxiv.org/pdf/2510.07741", "abs": "https://arxiv.org/abs/2510.07741", "authors": ["Yuang Meng", "Xin Jin", "Lina Lei", "Chun-Le Guo", "Chongyi Li"], "title": "UltraLED: Learning to See Everything in Ultra-High Dynamic Range Scenes", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Ultra-high dynamic range (UHDR) scenes exhibit significant exposure disparities between bright and dark regions. Such conditions are commonly encountered in nighttime scenes with light sources. Even with standard exposure settings, a bimodal intensity distribution with boundary peaks often emerges, making it difficult to preserve both highlight and shadow details simultaneously. RGB-based bracketing methods can capture details at both ends using short-long exposure pairs, but are susceptible to misalignment and ghosting artifacts. We found that a short-exposure image already retains sufficient highlight detail. The main challenge of UHDR reconstruction lies in denoising and recovering information in dark regions. In comparison to the RGB images, RAW images, thanks to their higher bit depth and more predictable noise characteristics, offer greater potential for addressing this challenge. This raises a key question: can we learn to see everything in UHDR scenes using only a single short-exposure RAW image? In this study, we rely solely on a single short-exposure frame, which inherently avoids ghosting and motion blur, making it particularly robust in dynamic scenes. To achieve that, we introduce UltraLED, a two-stage framework that performs exposure correction via a ratio map to balance dynamic range, followed by a brightness-aware RAW denoiser to enhance detail recovery in dark regions. To support this setting, we design a 9-stop bracketing pipeline to synthesize realistic UHDR images and contribute a corresponding dataset based on diverse scenes, using only the shortest exposure as input for reconstruction. Extensive experiments show that UltraLED significantly outperforms existing single-frame approaches. Our code and dataset are made publicly available at https://srameo.github.io/projects/ultraled.", "AI": {"tldr": "\u63d0\u51faUltraLED\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u5355\u5e27\u77ed\u66dd\u5149RAW\u56fe\u50cf\u5b9e\u73b0\u8d85\u9ad8\u52a8\u6001\u8303\u56f4\u573a\u666f\u91cd\u5efa\uff0c\u901a\u8fc7\u66dd\u5149\u6821\u6b63\u548c\u4eae\u5ea6\u611f\u77e5RAW\u53bb\u566a\u6765\u6062\u590d\u6697\u90e8\u7ec6\u8282\u3002", "motivation": "\u89e3\u51b3\u8d85\u9ad8\u52a8\u6001\u8303\u56f4\u573a\u666f\u4e2d\u540c\u65f6\u4fdd\u7559\u9ad8\u5149\u548c\u9634\u5f71\u7ec6\u8282\u7684\u6311\u6218\uff0c\u907f\u514d\u4f20\u7edfRGB\u5305\u56f4\u66dd\u5149\u65b9\u6cd5\u7684\u9519\u4f4d\u548c\u91cd\u5f71\u95ee\u9898\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u9996\u5148\u901a\u8fc7\u6bd4\u7387\u56fe\u8fdb\u884c\u66dd\u5149\u6821\u6b63\u5e73\u8861\u52a8\u6001\u8303\u56f4\uff0c\u7136\u540e\u4f7f\u7528\u4eae\u5ea6\u611f\u77e5RAW\u53bb\u566a\u5668\u589e\u5f3a\u6697\u90e8\u7ec6\u8282\u6062\u590d\u3002", "result": "\u5728\u5408\u6210UHDR\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cUltraLED\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u5355\u5e27\u65b9\u6cd5\u3002", "conclusion": "\u4ec5\u4f7f\u7528\u5355\u5e27\u77ed\u66dd\u5149RAW\u56fe\u50cf\u5373\u53ef\u6709\u6548\u91cd\u5efaUHDR\u573a\u666f\uff0c\u907f\u514d\u4e86\u52a8\u6001\u573a\u666f\u4e2d\u7684\u91cd\u5f71\u548c\u8fd0\u52a8\u6a21\u7cca\u95ee\u9898\u3002"}}
{"id": "2510.07752", "pdf": "https://arxiv.org/pdf/2510.07752", "abs": "https://arxiv.org/abs/2510.07752", "authors": ["Junhao He", "Jiaxu Wang", "Jia Li", "Mingyuan Sun", "Qiang Zhang", "Jiahang Cao", "Ziyi Zhang", "Yi Gu", "Jingkai Sun", "Renjing Xu"], "title": "DEGS: Deformable Event-based 3D Gaussian Splatting from RGB and Event Stream", "categories": ["cs.CV"], "comment": "Accepted by TVCG", "summary": "Reconstructing Dynamic 3D Gaussian Splatting (3DGS) from low-framerate RGB videos is challenging. This is because large inter-frame motions will increase the uncertainty of the solution space. For example, one pixel in the first frame might have more choices to reach the corresponding pixel in the second frame. Event cameras can asynchronously capture rapid visual changes and are robust to motion blur, but they do not provide color information. Intuitively, the event stream can provide deterministic constraints for the inter-frame large motion by the event trajectories. Hence, combining low-temporal-resolution images with high-framerate event streams can address this challenge. However, it is challenging to jointly optimize Dynamic 3DGS using both RGB and event modalities due to the significant discrepancy between these two data modalities. This paper introduces a novel framework that jointly optimizes dynamic 3DGS from the two modalities. The key idea is to adopt event motion priors to guide the optimization of the deformation fields. First, we extract the motion priors encoded in event streams by using the proposed LoCM unsupervised fine-tuning framework to adapt an event flow estimator to a certain unseen scene. Then, we present the geometry-aware data association method to build the event-Gaussian motion correspondence, which is the primary foundation of the pipeline, accompanied by two useful strategies, namely motion decomposition and inter-frame pseudo-label. Extensive experiments show that our method outperforms existing image and event-based approaches across synthetic and real scenes and prove that our method can effectively optimize dynamic 3DGS with the help of event data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4f4e\u5e27\u7387RGB\u89c6\u9891\u548c\u9ad8\u5e27\u7387\u4e8b\u4ef6\u6d41\u6765\u91cd\u5efa\u52a8\u60013D\u9ad8\u65af\u6e85\u5c04\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4e8b\u4ef6\u8fd0\u52a8\u5148\u9a8c\u6307\u5bfc\u53d8\u5f62\u573a\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u5927\u5e27\u95f4\u8fd0\u52a8\u5e26\u6765\u7684\u4e0d\u786e\u5b9a\u6027\u6311\u6218\u3002", "motivation": "\u4ece\u4f4e\u5e27\u7387RGB\u89c6\u9891\u91cd\u5efa\u52a8\u60013D\u9ad8\u65af\u6e85\u5c04\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5927\u5e27\u95f4\u8fd0\u52a8\u4f1a\u589e\u52a0\u89e3\u7a7a\u95f4\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u4e8b\u4ef6\u76f8\u673a\u80fd\u5f02\u6b65\u6355\u6349\u5feb\u901f\u89c6\u89c9\u53d8\u5316\u4e14\u5bf9\u8fd0\u52a8\u6a21\u7cca\u9c81\u68d2\uff0c\u4f46\u7f3a\u4e4f\u989c\u8272\u4fe1\u606f\u3002\u7ed3\u5408\u4e24\u79cd\u6a21\u6001\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e8b\u4ef6\u8fd0\u52a8\u5148\u9a8c\u6307\u5bfc\u53d8\u5f62\u573a\u4f18\u5316\uff1a1\uff09\u4f7f\u7528LoCM\u65e0\u76d1\u7763\u5fae\u8c03\u6846\u67b6\u4ece\u4e8b\u4ef6\u6d41\u4e2d\u63d0\u53d6\u8fd0\u52a8\u5148\u9a8c\uff1b2\uff09\u63d0\u51fa\u51e0\u4f55\u611f\u77e5\u6570\u636e\u5173\u8054\u65b9\u6cd5\u5efa\u7acb\u4e8b\u4ef6-\u9ad8\u65af\u8fd0\u52a8\u5bf9\u5e94\u5173\u7cfb\uff1b3\uff09\u4f7f\u7528\u8fd0\u52a8\u5206\u89e3\u548c\u5e27\u95f4\u4f2a\u6807\u7b7e\u7b56\u7565\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u573a\u666f\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u56fe\u50cf\u548c\u4e8b\u4ef6\u7684\u65b9\u6cd5\uff0c\u8bc1\u660e\u80fd\u6709\u6548\u5229\u7528\u4e8b\u4ef6\u6570\u636e\u4f18\u5316\u52a8\u60013D\u9ad8\u65af\u6e85\u5c04\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408RGB\u548c\u4e8b\u4ef6\u6a21\u6001\uff0c\u5229\u7528\u4e8b\u4ef6\u8fd0\u52a8\u5148\u9a8c\u6307\u5bfc\u53d8\u5f62\u573a\u4f18\u5316\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4f4e\u5e27\u7387\u89c6\u9891\u91cd\u5efa\u52a8\u60013D\u9ad8\u65af\u6e85\u5c04\u4e2d\u5927\u5e27\u95f4\u8fd0\u52a8\u5e26\u6765\u7684\u6311\u6218\u3002"}}
{"id": "2510.07830", "pdf": "https://arxiv.org/pdf/2510.07830", "abs": "https://arxiv.org/abs/2510.07830", "authors": ["Houqiang Zhong", "Zhenglong Wu", "Sihua Fu", "Zihan Zheng", "Xin Jin", "Xiaoyun Zhang", "Li Song", "Qiang Hu"], "title": "PrismGS: Physically-Grounded Anti-Aliasing for High-Fidelity Large-Scale 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has recently enabled real-time photorealistic rendering in compact scenes, but scaling to large urban environments introduces severe aliasing artifacts and optimization instability, especially under high-resolution (e.g., 4K) rendering. These artifacts, manifesting as flickering textures and jagged edges, arise from the mismatch between Gaussian primitives and the multi-scale nature of urban geometry. While existing ``divide-and-conquer'' pipelines address scalability, they fail to resolve this fidelity gap. In this paper, we propose PrismGS, a physically-grounded regularization framework that improves the intrinsic rendering behavior of 3D Gaussians. PrismGS integrates two synergistic regularizers. The first is pyramidal multi-scale supervision, which enforces consistency by supervising the rendering against a pre-filtered image pyramid. This compels the model to learn an inherently anti-aliased representation that remains coherent across different viewing scales, directly mitigating flickering textures. This is complemented by an explicit size regularization that imposes a physically-grounded lower bound on the dimensions of the 3D Gaussians. This prevents the formation of degenerate, view-dependent primitives, leading to more stable and plausible geometric surfaces and reducing jagged edges. Our method is plug-and-play and compatible with existing pipelines. Extensive experiments on MatrixCity, Mill-19, and UrbanScene3D demonstrate that PrismGS achieves state-of-the-art performance, yielding significant PSNR gains around 1.5 dB against CityGaussian, while maintaining its superior quality and robustness under demanding 4K rendering.", "AI": {"tldr": "PrismGS\u662f\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u6b63\u5219\u5316\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb3D\u9ad8\u65af\u6cfc\u6e85\u5728\u5927\u89c4\u6a21\u57ce\u5e02\u573a\u666f\u4e2d\u7684\u6e32\u67d3\u8d28\u91cf\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u76d1\u7763\u548c\u5c3a\u5bf8\u6b63\u5219\u5316\u89e3\u51b3\u9ad8\u5206\u8fa8\u7387\u6e32\u67d3\u65f6\u7684\u8d70\u6837\u95ee\u9898\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u5728\u5927\u578b\u57ce\u5e02\u573a\u666f\u4e2d\u4f1a\u51fa\u73b0\u4e25\u91cd\u7684\u8d70\u6837\u4f2a\u5f71\u548c\u4f18\u5316\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u7279\u522b\u662f\u57284K\u9ad8\u5206\u8fa8\u7387\u6e32\u67d3\u65f6\uff0c\u8868\u73b0\u4e3a\u95ea\u70c1\u7eb9\u7406\u548c\u952f\u9f7f\u8fb9\u7f18\u3002\u73b0\u6709\u65b9\u6cd5\u867d\u7136\u89e3\u51b3\u4e86\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u4f46\u672a\u80fd\u89e3\u51b3\u8fd9\u79cd\u4fdd\u771f\u5ea6\u5dee\u8ddd\u3002", "method": "\u63d0\u51faPrismGS\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u534f\u540c\u7684\u6b63\u5219\u5316\u5668\uff1a1) \u91d1\u5b57\u5854\u591a\u5c3a\u5ea6\u76d1\u7763\uff0c\u901a\u8fc7\u5bf9\u9884\u6ee4\u6ce2\u56fe\u50cf\u91d1\u5b57\u5854\u8fdb\u884c\u76d1\u7763\u6765\u5f3a\u5236\u6a21\u578b\u5b66\u4e60\u6297\u8d70\u6837\u8868\u793a\uff1b2) \u663e\u5f0f\u5c3a\u5bf8\u6b63\u5219\u5316\uff0c\u5bf93D\u9ad8\u65af\u7684\u5c3a\u5bf8\u65bd\u52a0\u7269\u7406\u57fa\u7840\u7684\u4e0b\u754c\u7ea6\u675f\uff0c\u9632\u6b62\u5f62\u6210\u9000\u5316\u7684\u89c6\u56fe\u4f9d\u8d56\u57fa\u5143\u3002", "result": "\u5728MatrixCity\u3001Mill-19\u548cUrbanScene3D\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPrismGS\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u76f8\u6bd4CityGaussian\u83b7\u5f97\u4e86\u7ea61.5dB\u7684PSNR\u63d0\u5347\uff0c\u540c\u65f6\u57284K\u6e32\u67d3\u4e0b\u4fdd\u6301\u4f18\u8d8a\u7684\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "PrismGS\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u6846\u67b6\uff0c\u80fd\u591f\u663e\u8457\u6539\u55843D\u9ad8\u65af\u6cfc\u6e85\u5728\u5927\u89c4\u6a21\u57ce\u5e02\u573a\u666f\u4e2d\u7684\u6e32\u67d3\u8d28\u91cf\uff0c\u6709\u6548\u89e3\u51b3\u9ad8\u5206\u8fa8\u7387\u6e32\u67d3\u65f6\u7684\u8d70\u6837\u95ee\u9898\uff0c\u540c\u65f6\u4e0e\u73b0\u6709\u6d41\u6c34\u7ebf\u517c\u5bb9\u3002"}}
{"id": "2510.07940", "pdf": "https://arxiv.org/pdf/2510.07940", "abs": "https://arxiv.org/abs/2510.07940", "authors": ["Leigang Qu", "Ziyang Wang", "Na Zheng", "Wenjie Wang", "Liqiang Nie", "Tat-Seng Chua"], "title": "TTOM: Test-Time Optimization and Memorization for Compositional Video Generation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Project page: https://ttom-t2v.github.io/", "summary": "Video Foundation Models (VFMs) exhibit remarkable visual generation performance, but struggle in compositional scenarios (e.g., motion, numeracy, and spatial relation). In this work, we introduce Test-Time Optimization and Memorization (TTOM), a training-free framework that aligns VFM outputs with spatiotemporal layouts during inference for better text-image alignment. Rather than direct intervention to latents or attention per-sample in existing work, we integrate and optimize new parameters guided by a general layout-attention objective. Furthermore, we formulate video generation within a streaming setting, and maintain historical optimization contexts with a parametric memory mechanism that supports flexible operations, such as insert, read, update, and delete. Notably, we found that TTOM disentangles compositional world knowledge, showing powerful transferability and generalization. Experimental results on the T2V-CompBench and Vbench benchmarks establish TTOM as an effective, practical, scalable, and efficient framework to achieve cross-modal alignment for compositional video generation on the fly.", "AI": {"tldr": "TTOM\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6d4b\u8bd5\u65f6\u4f18\u5316\u548c\u8bb0\u5fc6\u673a\u5236\uff0c\u5728\u63a8\u7406\u65f6\u5bf9\u9f50\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e0e\u65f6\u7a7a\u5e03\u5c40\uff0c\u63d0\u5347\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u89c6\u9891\u57fa\u7840\u6a21\u578b\u5728\u7ec4\u5408\u573a\u666f\uff08\u5982\u8fd0\u52a8\u3001\u6570\u5b57\u3001\u7a7a\u95f4\u5173\u7cfb\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u6539\u8fdb\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u80fd\u529b\u3002", "method": "\u63d0\u51faTTOM\u6846\u67b6\uff1a1\uff09\u96c6\u6210\u5e76\u4f18\u5316\u65b0\u53c2\u6570\uff0c\u4f7f\u7528\u901a\u7528\u5e03\u5c40-\u6ce8\u610f\u529b\u76ee\u6807\uff1b2\uff09\u5728\u6d41\u5f0f\u8bbe\u7f6e\u4e2d\u751f\u6210\u89c6\u9891\uff1b3\uff09\u4f7f\u7528\u53c2\u6570\u5316\u8bb0\u5fc6\u673a\u5236\u7ef4\u62a4\u5386\u53f2\u4f18\u5316\u4e0a\u4e0b\u6587\uff0c\u652f\u6301\u63d2\u5165\u3001\u8bfb\u53d6\u3001\u66f4\u65b0\u548c\u5220\u9664\u64cd\u4f5c\u3002", "result": "\u5728T2V-CompBench\u548cVbench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTTOM\u88ab\u8bc1\u660e\u662f\u6709\u6548\u3001\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5b9e\u73b0\u7ec4\u5408\u89c6\u9891\u751f\u6210\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u3002", "conclusion": "TTOM\u80fd\u591f\u89e3\u8026\u7ec4\u5408\u6027\u4e16\u754c\u77e5\u8bc6\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u53ef\u8fc1\u79fb\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u7ec4\u5408\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5b9e\u65f6\u5bf9\u9f50\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.07944", "pdf": "https://arxiv.org/pdf/2510.07944", "abs": "https://arxiv.org/abs/2510.07944", "authors": ["Tianrui Zhang", "Yichen Liu", "Zilin Guo", "Yuxin Guo", "Jingcheng Ni", "Chenjing Ding", "Dan Xu", "Lewei Lu", "Zehuan Wu"], "title": "CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Generative models have been widely applied to world modeling for environment simulation and future state prediction. With advancements in autonomous driving, there is a growing demand not only for high-fidelity video generation under various controls, but also for producing diverse and meaningful information such as depth estimation. To address this, we propose CVD-STORM, a cross-view video diffusion model utilizing a spatial-temporal reconstruction Variational Autoencoder (VAE) that generates long-term, multi-view videos with 4D reconstruction capabilities under various control inputs. Our approach first fine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing its ability to encode 3D structures and temporal dynamics. Subsequently, we integrate this VAE into the video diffusion process to significantly improve generation quality. Experimental results demonstrate that our model achieves substantial improvements in both FID and FVD metrics. Additionally, the jointly-trained Gaussian Splatting Decoder effectively reconstructs dynamic scenes, providing valuable geometric information for comprehensive scene understanding.", "AI": {"tldr": "\u63d0\u51fa\u4e86CVD-STORM\u6a21\u578b\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u7a7a\u95f4-\u65f6\u95f4\u91cd\u5efaVAE\u7684\u8de8\u89c6\u89d2\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u751f\u6210\u5177\u67094D\u91cd\u5efa\u80fd\u529b\u7684\u591a\u89c6\u89d2\u957f\u89c6\u9891\uff0c\u5e76\u5728\u5404\u79cd\u63a7\u5236\u8f93\u5165\u4e0b\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4e0d\u4ec5\u9700\u8981\u9ad8\u4fdd\u771f\u5ea6\u7684\u53ef\u63a7\u89c6\u9891\u751f\u6210\uff0c\u8fd8\u9700\u8981\u4ea7\u751f\u6df1\u5ea6\u4f30\u8ba1\u7b49\u591a\u6837\u4e14\u6709\u610f\u4e49\u7684\u51e0\u4f55\u4fe1\u606f\uff0c\u4ee5\u652f\u6301\u5168\u9762\u7684\u573a\u666f\u7406\u89e3\u3002", "method": "\u9996\u5148\u901a\u8fc7\u8f85\u52a94D\u91cd\u5efa\u4efb\u52a1\u5fae\u8c03VAE\uff0c\u589e\u5f3a\u5176\u7f16\u78013D\u7ed3\u6784\u548c\u65f6\u95f4\u52a8\u6001\u7684\u80fd\u529b\uff0c\u7136\u540e\u5c06\u8be5VAE\u96c6\u6210\u5230\u89c6\u9891\u6269\u6563\u8fc7\u7a0b\u4e2d\u4ee5\u663e\u8457\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u8054\u5408\u8bad\u7ec3\u9ad8\u65af\u6cfc\u6e85\u89e3\u7801\u5668\u8fdb\u884c\u52a8\u6001\u573a\u666f\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728FID\u548cFVD\u6307\u6807\u4e0a\u5747\u5b9e\u73b0\u4e86\u663e\u8457\u63d0\u5347\uff0c\u5e76\u4e14\u8054\u5408\u8bad\u7ec3\u7684\u9ad8\u65af\u6cfc\u6e85\u89e3\u7801\u5668\u80fd\u591f\u6709\u6548\u91cd\u5efa\u52a8\u6001\u573a\u666f\uff0c\u4e3a\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u51e0\u4f55\u4fe1\u606f\u3002", "conclusion": "CVD-STORM\u6a21\u578b\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u591a\u89c6\u89d2\u89c6\u9891\u751f\u6210\u548c4D\u573a\u666f\u91cd\u5efa\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u573a\u666f\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2510.07961", "pdf": "https://arxiv.org/pdf/2510.07961", "abs": "https://arxiv.org/abs/2510.07961", "authors": ["Yidi Liu", "Xueyang Fu", "Jie Huang", "Jie Xiao", "Dong Li", "Wenlong Zhang", "Lei Bai", "Zheng-Jun Zha"], "title": "Latent Harmony: Synergistic Unified UHD Image Restoration via Latent Space Regularization and Controllable Refinement", "categories": ["cs.CV"], "comment": "Accepted by NeurIPS 2025", "summary": "Ultra-High Definition (UHD) image restoration faces a trade-off between computational efficiency and high-frequency detail retention. While Variational Autoencoders (VAEs) improve efficiency via latent-space processing, their Gaussian constraint often discards degradation-specific high-frequency information, hurting reconstruction fidelity. To overcome this, we propose Latent Harmony, a two-stage framework that redefines VAEs for UHD restoration by jointly regularizing the latent space and enforcing high-frequency-aware reconstruction.In Stage One, we introduce LH-VAE, which enhances semantic robustness through visual semantic constraints and progressive degradation perturbations, while latent equivariance strengthens high-frequency reconstruction.Stage Two jointly trains this refined VAE with a restoration model using High-Frequency Low-Rank Adaptation (HF-LoRA): an encoder LoRA guided by a fidelity-oriented high-frequency alignment loss to recover authentic details, and a decoder LoRA driven by a perception-oriented loss to synthesize realistic textures. Both LoRA modules are trained via alternating optimization with selective gradient propagation to preserve the pretrained latent structure.At inference, a tunable parameter {\\alpha} enables flexible fidelity-perception trade-offs.Experiments show Latent Harmony achieves state-of-the-art performance across UHD and standard-resolution tasks, effectively balancing efficiency, perceptual quality, and reconstruction accuracy.", "AI": {"tldr": "Latent Harmony\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5UHD\u56fe\u50cf\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u6b63\u5219\u5316\u6f5c\u5728\u7a7a\u95f4\u548c\u5f3a\u5236\u9ad8\u9891\u611f\u77e5\u91cd\u5efa\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u9ad8\u9891\u7ec6\u8282\u4fdd\u7559\u80fd\u529b\u3002", "motivation": "UHD\u56fe\u50cf\u4fee\u590d\u9762\u4e34\u8ba1\u7b97\u6548\u7387\u4e0e\u9ad8\u9891\u7ec6\u8282\u4fdd\u7559\u7684\u6743\u8861\u95ee\u9898\uff0c\u4f20\u7edfVAE\u7684\u9ad8\u65af\u7ea6\u675f\u4f1a\u4e22\u5f03\u9000\u5316\u7279\u5b9a\u7684\u9ad8\u9891\u4fe1\u606f\uff0c\u5f71\u54cd\u91cd\u5efa\u4fdd\u771f\u5ea6\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u63d0\u51faLH-VAE\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u4e49\u7ea6\u675f\u548c\u6e10\u8fdb\u9000\u5316\u6270\u52a8\u589e\u5f3a\u8bed\u4e49\u9c81\u68d2\u6027\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528HF-LoRA\u8054\u5408\u8bad\u7ec3\uff0c\u5305\u542b\u7f16\u7801\u5668LoRA\uff08\u4fdd\u771f\u5ea6\u5bfc\u5411\uff09\u548c\u89e3\u7801\u5668LoRA\uff08\u611f\u77e5\u5bfc\u5411\uff09\uff0c\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u548c\u9009\u62e9\u6027\u68af\u5ea6\u4f20\u64ad\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLatent Harmony\u5728UHD\u548c\u6807\u51c6\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u6709\u6548\u5e73\u8861\u6548\u7387\u3001\u611f\u77e5\u8d28\u91cf\u548c\u91cd\u5efa\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u91cd\u65b0\u5b9a\u4e49VAE\u7528\u4e8eUHD\u4fee\u590d\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u9ad8\u9891\u7ec6\u8282\u4fdd\u7559\u7684\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7075\u6d3b\u7684\u4fdd\u771f\u5ea6-\u611f\u77e5\u6743\u8861\u3002"}}
{"id": "2510.08096", "pdf": "https://arxiv.org/pdf/2510.08096", "abs": "https://arxiv.org/abs/2510.08096", "authors": ["Ankit Gahlawat", "Anirban Mukherjee", "Dinesh Babu Jayagopi"], "title": "Efficient Label Refinement for Face Parsing Under Extreme Poses Using 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "Accepted to VCIP 2025 (International Conference on Visual   Communications and Image Processing 2025)", "summary": "Accurate face parsing under extreme viewing angles remains a significant challenge due to limited labeled data in such poses. Manual annotation is costly and often impractical at scale. We propose a novel label refinement pipeline that leverages 3D Gaussian Splatting (3DGS) to generate accurate segmentation masks from noisy multiview predictions. By jointly fitting two 3DGS models, one to RGB images and one to their initial segmentation maps, our method enforces multiview consistency through shared geometry, enabling the synthesis of pose-diverse training data with only minimal post-processing. Fine-tuning a face parsing model on this refined dataset significantly improves accuracy on challenging head poses, while maintaining strong performance on standard views. Extensive experiments, including human evaluations, demonstrate that our approach achieves superior results compared to state-of-the-art methods, despite requiring no ground-truth 3D annotations and using only a small set of initial images. Our method offers a scalable and effective solution for improving face parsing robustness in real-world settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u75283D\u9ad8\u65af\u6e85\u5c04\u4ece\u591a\u89c6\u89d2\u566a\u58f0\u9884\u6d4b\u4e2d\u751f\u6210\u7cbe\u786e\u5206\u5272\u63a9\u7801\u7684\u6807\u7b7e\u7ec6\u5316\u6d41\u7a0b\uff0c\u65e0\u97003D\u6807\u6ce8\u5373\u53ef\u663e\u8457\u63d0\u5347\u6781\u7aef\u89c6\u89d2\u4e0b\u7684\u4eba\u8138\u89e3\u6790\u7cbe\u5ea6\u3002", "motivation": "\u6781\u7aef\u89c6\u89d2\u4e0b\u7684\u4eba\u8138\u89e3\u6790\u7531\u4e8e\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u800c\u9762\u4e34\u6311\u6218\uff0c\u624b\u52a8\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u89c4\u6a21\u5316\u3002", "method": "\u8054\u5408\u62df\u5408\u4e24\u4e2a3D\u9ad8\u65af\u6e85\u5c04\u6a21\u578b\uff08RGB\u56fe\u50cf\u548c\u521d\u59cb\u5206\u5272\u56fe\uff09\uff0c\u901a\u8fc7\u5171\u4eab\u51e0\u4f55\u5b9e\u73b0\u591a\u89c6\u89d2\u4e00\u81f4\u6027\uff0c\u5408\u6210\u59ff\u6001\u591a\u6837\u7684\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5934\u90e8\u59ff\u6001\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u89e3\u6790\u7cbe\u5ea6\uff0c\u540c\u65f6\u5728\u6807\u51c6\u89c6\u89d2\u4e0a\u4fdd\u6301\u5f3a\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u63d0\u5347\u771f\u5b9e\u573a\u666f\u4e2d\u4eba\u8138\u89e3\u6790\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.08131", "pdf": "https://arxiv.org/pdf/2510.08131", "abs": "https://arxiv.org/abs/2510.08131", "authors": ["Kesen Zhao", "Jiaxin Shi", "Beier Zhu", "Junbao Zhou", "Xiaolong Shen", "Yuan Zhou", "Qianru Sun", "Hanwang Zhang"], "title": "Real-Time Motion-Controllable Autoregressive Video Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Real-time motion-controllable video generation remains challenging due to the inherent latency of bidirectional diffusion models and the lack of effective autoregressive (AR) approaches. Existing AR video diffusion models are limited to simple control signals or text-to-video generation, and often suffer from quality degradation and motion artifacts in few-step generation. To address these challenges, we propose AR-Drag, the first RL-enhanced few-step AR video diffusion model for real-time image-to-video generation with diverse motion control. We first fine-tune a base I2V model to support basic motion control, then further improve it via reinforcement learning with a trajectory-based reward model. Our design preserves the Markov property through a Self-Rollout mechanism and accelerates training by selectively introducing stochasticity in denoising steps. Extensive experiments demonstrate that AR-Drag achieves high visual fidelity and precise motion alignment, significantly reducing latency compared with state-of-the-art motion-controllable VDMs, while using only 1.3B parameters. Additional visualizations can be found on our project page: https://kesenzhao.github.io/AR-Drag.github.io/.", "AI": {"tldr": "AR-Drag\u662f\u9996\u4e2aRL\u589e\u5f3a\u7684\u5c11\u6b65\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u5b9e\u65f6\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\uff0c\u652f\u6301\u591a\u6837\u5316\u8fd0\u52a8\u63a7\u5236\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u3002", "motivation": "\u89e3\u51b3\u5b9e\u65f6\u8fd0\u52a8\u53ef\u63a7\u89c6\u9891\u751f\u6210\u7684\u6311\u6218\uff1a\u53cc\u5411\u6269\u6563\u6a21\u578b\u7684\u56fa\u6709\u5ef6\u8fdf\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u81ea\u56de\u5f52\u65b9\u6cd5\u5728\u5c11\u6b65\u751f\u6210\u4e2d\u7684\u8d28\u91cf\u4e0b\u964d\u548c\u8fd0\u52a8\u4f2a\u5f71\u95ee\u9898\u3002", "method": "\u9996\u5148\u5fae\u8c03\u57fa\u7840I2V\u6a21\u578b\u652f\u6301\u57fa\u672c\u8fd0\u52a8\u63a7\u5236\uff0c\u7136\u540e\u901a\u8fc7\u57fa\u4e8e\u8f68\u8ff9\u7684\u5956\u52b1\u6a21\u578b\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u6539\u8fdb\u3002\u901a\u8fc7\u81ea\u6eda\u52a8\u673a\u5236\u4fdd\u6301\u9a6c\u5c14\u53ef\u592b\u6027\u8d28\uff0c\u5e76\u5728\u53bb\u566a\u6b65\u9aa4\u4e2d\u9009\u62e9\u6027\u5f15\u5165\u968f\u673a\u6027\u6765\u52a0\u901f\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAR-Drag\u5b9e\u73b0\u4e86\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u7cbe\u786e\u7684\u8fd0\u52a8\u5bf9\u9f50\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u8fd0\u52a8\u53ef\u63a7VDM\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\uff0c\u4ec5\u4f7f\u752813\u4ebf\u53c2\u6570\u3002", "conclusion": "AR-Drag\u6210\u529f\u89e3\u51b3\u4e86\u5b9e\u65f6\u8fd0\u52a8\u53ef\u63a7\u89c6\u9891\u751f\u6210\u7684\u6311\u6218\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u5ef6\u8fdf\uff0c\u4e3a\u5b9e\u65f6\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.08143", "pdf": "https://arxiv.org/pdf/2510.08143", "abs": "https://arxiv.org/abs/2510.08143", "authors": ["Shian Du", "Menghan Xia", "Chang Liu", "Quande Liu", "Xintao Wang", "Pengfei Wan", "Xiangyang Ji"], "title": "UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Cascaded video super-resolution has emerged as a promising technique for decoupling the computational burden associated with generating high-resolution videos using large foundation models. Existing studies, however, are largely confined to text-to-video tasks and fail to leverage additional generative conditions beyond text, which are crucial for ensuring fidelity in multi-modal video generation. We address this limitation by presenting UniMMVSR, the first unified generative video super-resolution framework to incorporate hybrid-modal conditions, including text, images, and videos. We conduct a comprehensive exploration of condition injection strategies, training schemes, and data mixture techniques within a latent video diffusion model. A key challenge was designing distinct data construction and condition utilization methods to enable the model to precisely utilize all condition types, given their varied correlations with the target video. Our experiments demonstrate that UniMMVSR significantly outperforms existing methods, producing videos with superior detail and a higher degree of conformity to multi-modal conditions. We also validate the feasibility of combining UniMMVSR with a base model to achieve multi-modal guided generation of 4K video, a feat previously unattainable with existing techniques.", "AI": {"tldr": "UniMMVSR\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u80fd\u591f\u6574\u5408\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u7b49\u591a\u79cd\u6761\u4ef6\uff0c\u5728\u6f5c\u5728\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u5b9e\u73b0\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u7ea7\u8054\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8e\u6587\u672c\u5230\u89c6\u9891\u4efb\u52a1\uff0c\u65e0\u6cd5\u5229\u7528\u6587\u672c\u4ee5\u5916\u7684\u751f\u6210\u6761\u4ef6\uff0c\u8fd9\u5728\u591a\u6a21\u6001\u89c6\u9891\u751f\u6210\u4e2d\u5bf9\u4e8e\u4fdd\u8bc1\u4fdd\u771f\u5ea6\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5728\u6f5c\u5728\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u63a2\u7d22\u6761\u4ef6\u6ce8\u5165\u7b56\u7565\u3001\u8bad\u7ec3\u65b9\u6848\u548c\u6570\u636e\u6df7\u5408\u6280\u672f\uff0c\u8bbe\u8ba1\u4e0d\u540c\u7684\u6570\u636e\u6784\u5efa\u548c\u6761\u4ef6\u5229\u7528\u65b9\u6cd5\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u7cbe\u786e\u5229\u7528\u6240\u6709\u6761\u4ef6\u7c7b\u578b\u3002", "result": "UniMMVSR\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u5177\u6709\u66f4\u4f18\u7ec6\u8282\u548c\u66f4\u9ad8\u591a\u6a21\u6001\u6761\u4ef6\u7b26\u5408\u5ea6\u7684\u89c6\u9891\uff0c\u5e76\u80fd\u4e0e\u57fa\u7840\u6a21\u578b\u7ed3\u5408\u5b9e\u73b04K\u89c6\u9891\u7684\u591a\u6a21\u6001\u5f15\u5bfc\u751f\u6210\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u6761\u4ef6\u5229\u7528\u7684\u6311\u6218\uff0c\u4e3a\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2510.08273", "pdf": "https://arxiv.org/pdf/2510.08273", "abs": "https://arxiv.org/abs/2510.08273", "authors": ["Haipeng Liu", "Yang Wang", "Meng Wang"], "title": "One Stone with Two Birds: A Null-Text-Null Frequency-Aware Diffusion Models for Text-Guided Image Inpainting", "categories": ["cs.CV"], "comment": "25 pages, 11 figures, to appear NeurIPS 2025", "summary": "Text-guided image inpainting aims at reconstructing the masked regions as per text prompts, where the longstanding challenges lie in the preservation for unmasked regions, while achieving the semantics consistency between unmasked and inpainted masked regions. Previous arts failed to address both of them, always with either of them to be remedied. Such facts, as we observed, stem from the entanglement of the hybrid (e.g., mid-and-low) frequency bands that encode varied image properties, which exhibit different robustness to text prompts during the denoising process. In this paper, we propose a null-text-null frequency-aware diffusion models, dubbed \\textbf{NTN-Diff}, for text-guided image inpainting, by decomposing the semantics consistency across masked and unmasked regions into the consistencies as per each frequency band, while preserving the unmasked regions, to circumvent two challenges in a row. Based on the diffusion process, we further divide the denoising process into early (high-level noise) and late (low-level noise) stages, where the mid-and-low frequency bands are disentangled during the denoising process. As observed, the stable mid-frequency band is progressively denoised to be semantically aligned during text-guided denoising process, which, meanwhile, serves as the guidance to the null-text denoising process to denoise low-frequency band for the masked regions, followed by a subsequent text-guided denoising process at late stage, to achieve the semantics consistency for mid-and-low frequency bands across masked and unmasked regions, while preserve the unmasked regions. Extensive experiments validate the superiority of NTN-Diff over the state-of-the-art diffusion models to text-guided diffusion models. Our code can be accessed from https://github.com/htyjers/NTN-Diff.", "AI": {"tldr": "\u63d0\u51faNTN-Diff\u6a21\u578b\uff0c\u901a\u8fc7\u9891\u7387\u611f\u77e5\u7684\u6269\u6563\u6a21\u578b\u89e3\u51b3\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u4fee\u590d\u4e2d\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u672a\u63a9\u7801\u533a\u57df\u4fdd\u62a4\u95ee\u9898", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u89e3\u51b3\u672a\u63a9\u7801\u533a\u57df\u4fdd\u62a4\u548c\u63a9\u7801/\u672a\u63a9\u7801\u533a\u57df\u8bed\u4e49\u4e00\u81f4\u6027\u4e24\u4e2a\u6311\u6218\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u6df7\u5408\u9891\u7387\u5e26\u7684\u7ea0\u7f20\u5bfc\u81f4\u5bf9\u6587\u672c\u63d0\u793a\u7684\u9c81\u68d2\u6027\u4e0d\u540c", "method": "\u5c06\u53bb\u566a\u8fc7\u7a0b\u5206\u4e3a\u65e9\u671f\u548c\u665a\u671f\u9636\u6bb5\uff0c\u5728\u65e9\u671f\u9636\u6bb5\u4f7f\u7528\u6587\u672c\u5f15\u5bfc\u53bb\u566a\u7a33\u5b9a\u4e2d\u9891\u5e26\uff0c\u7136\u540e\u4f7f\u7528\u7a7a\u6587\u672c\u53bb\u566a\u5904\u7406\u4f4e\u9891\u5e26\uff0c\u6700\u540e\u5728\u665a\u671f\u9636\u6bb5\u8fdb\u884c\u6587\u672c\u5f15\u5bfc\u53bb\u566a", "result": "\u5b9e\u9a8c\u9a8c\u8bc1NTN-Diff\u5728\u6587\u672c\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u9891\u7387\u5206\u89e3\u548c\u5206\u9636\u6bb5\u53bb\u566a\u7b56\u7565\uff0cNTN-Diff\u6210\u529f\u89e3\u51b3\u4e86\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u4fee\u590d\u4e2d\u7684\u5173\u952e\u6311\u6218"}}
{"id": "2510.08318", "pdf": "https://arxiv.org/pdf/2510.08318", "abs": "https://arxiv.org/abs/2510.08318", "authors": ["Yushi Huang", "Xingtong Ge", "Ruihao Gong", "Chengtao Lv", "Jun Zhang"], "title": "LinVideo: A Post-Training Framework towards O(n) Attention in Efficient Video Generation", "categories": ["cs.CV"], "comment": "Code will be released upon acceptance", "summary": "Video diffusion models (DMs) have enabled high-quality video synthesis. However, their computation costs scale quadratically with sequence length because self-attention has quadratic complexity. While linear attention lowers the cost, fully replacing quadratic attention requires expensive pretraining due to the limited expressiveness of linear attention and the complexity of spatiotemporal modeling in video generation. In this paper, we present LinVideo, an efficient data-free post-training framework that replaces a target number of self-attention modules with linear attention while preserving the original model's performance. First, we observe a significant disparity in the replaceability of different layers. Instead of manual or heuristic choices, we frame layer selection as a binary classification problem and propose selective transfer, which automatically and progressively converts layers to linear attention with minimal performance impact. Additionally, to overcome the ineffectiveness and inefficiency of existing objectives for this transfer process, we introduce an anytime distribution matching (ADM) objective that aligns the distributions of samples across any timestep along the sampling trajectory. This objective is efficient and recovers model performance. Extensive experiments show that our method achieves a 1.25-2.00x speedup while preserving generation quality, and our 4-step distilled model further delivers a 15.92x latency reduction with minimal visual quality drop.", "AI": {"tldr": "LinVideo\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u6570\u636e\u65e0\u5173\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u66ff\u6362\u4e3a\u7ebf\u6027\u6ce8\u610f\u529b\u6765\u52a0\u901f\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u8ba1\u7b97\u6210\u672c\u968f\u5e8f\u5217\u957f\u5ea6\u5448\u4e8c\u6b21\u65b9\u589e\u957f\uff0c\u800c\u5b8c\u5168\u66ff\u6362\u4e8c\u6b21\u65b9\u6ce8\u610f\u529b\u9700\u8981\u6602\u8d35\u7684\u9884\u8bad\u7ec3\u3002\u73b0\u6709\u7ebf\u6027\u6ce8\u610f\u529b\u65b9\u6cd5\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u65f6\u7a7a\u5efa\u6a21\u590d\u6742\u3002", "method": "\u63d0\u51fa\u9009\u62e9\u6027\u8fc1\u79fb\u65b9\u6cd5\uff0c\u5c06\u5c42\u9009\u62e9\u4f5c\u4e3a\u4e8c\u5143\u5206\u7c7b\u95ee\u9898\u81ea\u52a8\u6e10\u8fdb\u8f6c\u6362\uff1b\u5f15\u5165\u4efb\u610f\u65f6\u95f4\u5206\u5e03\u5339\u914d\u76ee\u6807\uff0c\u5728\u91c7\u6837\u8f68\u8ff9\u7684\u4efb\u4f55\u65f6\u95f4\u6b65\u5bf9\u9f50\u6837\u672c\u5206\u5e03\u3002", "result": "\u5b9e\u73b01.25-2.00\u500d\u52a0\u901f\u5e76\u4fdd\u6301\u751f\u6210\u8d28\u91cf\uff0c4\u6b65\u84b8\u998f\u6a21\u578b\u8fdb\u4e00\u6b65\u5b9e\u73b015.92\u500d\u5ef6\u8fdf\u964d\u4f4e\u4e14\u89c6\u89c9\u8d28\u91cf\u4e0b\u964d\u6700\u5c0f\u3002", "conclusion": "LinVideo\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2510.08358", "pdf": "https://arxiv.org/pdf/2510.08358", "abs": "https://arxiv.org/abs/2510.08358", "authors": ["Alexander Belyaev", "Pierre-Alain Fayolle", "Michael Cohen"], "title": "SPICE: Simple and Practical Image Clarification and Enhancement", "categories": ["cs.CV"], "comment": "5 pages, 8 figures", "summary": "We introduce a simple and efficient method to enhance and clarify images. More specifically, we deal with low light image enhancement and clarification of hazy imagery (hazy/foggy images, images containing sand dust, and underwater images). Our method involves constructing an image filter to simulate low-light or hazy conditions and deriving approximate reverse filters to minimize distortions in the enhanced images. Experimental results show that our approach is highly competitive and often surpasses state-of-the-art techniques in handling extremely dark images and in enhancing hazy images. A key advantage of our approach lies in its simplicity: Our method is implementable with just a few lines of MATLAB code.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\u548c\u96fe\u973e\u56fe\u50cf\uff08\u96fe\u5929\u3001\u6c99\u5c18\u3001\u6c34\u4e0b\u56fe\u50cf\uff09\u7684\u6e05\u6670\u5316\u5904\u7406", "motivation": "\u89e3\u51b3\u4f4e\u5149\u7167\u56fe\u50cf\u548c\u96fe\u973e\u56fe\u50cf\u7684\u8d28\u91cf\u95ee\u9898\uff0c\u63d0\u4f9b\u7b80\u5355\u6709\u6548\u7684\u589e\u5f3a\u65b9\u6848", "method": "\u6784\u5efa\u56fe\u50cf\u6ee4\u6ce2\u5668\u6a21\u62df\u4f4e\u5149\u6216\u96fe\u973e\u6761\u4ef6\uff0c\u63a8\u5bfc\u8fd1\u4f3c\u53cd\u5411\u6ee4\u6ce2\u5668\u4ee5\u6700\u5c0f\u5316\u589e\u5f3a\u56fe\u50cf\u4e2d\u7684\u5931\u771f", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u6781\u6697\u56fe\u50cf\u548c\u589e\u5f3a\u96fe\u973e\u56fe\u50cf\u65b9\u9762\u5177\u6709\u9ad8\u5ea6\u7ade\u4e89\u529b\uff0c\u751a\u81f3\u4f18\u4e8e\u6700\u5148\u8fdb\u6280\u672f", "conclusion": "\u8be5\u65b9\u6cd5\u7684\u5173\u952e\u4f18\u52bf\u5728\u4e8e\u5176\u7b80\u5355\u6027\uff0c\u4ec5\u9700\u51e0\u884cMATLAB\u4ee3\u7801\u5373\u53ef\u5b9e\u73b0"}}
{"id": "2510.08363", "pdf": "https://arxiv.org/pdf/2510.08363", "abs": "https://arxiv.org/abs/2510.08363", "authors": ["Mattia Ferrari", "Lorenzo Bruzzone"], "title": "Hyperspectral data augmentation with transformer-based diffusion models", "categories": ["cs.CV"], "comment": "10 pages, 2 figures, accepted at SPIE REMOTE SENSING conference 16-20   September 2024 Edinburgh, United Kingdom", "summary": "The introduction of new generation hyperspectral satellite sensors, combined with advancements in deep learning methodologies, has significantly enhanced the ability to discriminate detailed land-cover classes at medium-large scales. However, a significant challenge in deep learning methods is the risk of overfitting when training networks with small labeled datasets. In this work, we propose a data augmentation technique that leverages a guided diffusion model. To effectively train the model with a limited number of labeled samples and to capture complex patterns in the data, we implement a lightweight transformer network. Additionally, we introduce a modified weighted loss function and an optimized cosine variance scheduler, which facilitate fast and effective training on small datasets. We evaluate the effectiveness of the proposed method on a forest classification task with 10 different forest types using hyperspectral images acquired by the PRISMA satellite. The results demonstrate that the proposed method outperforms other data augmentation techniques in both average and weighted average accuracy. The effectiveness of the method is further highlighted by the stable training behavior of the model, which addresses a common limitation in the practical application of deep generative models for data augmentation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7Transformer\u7f51\u7edc\u3001\u6539\u8fdb\u7684\u52a0\u6743\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u7684\u4f59\u5f26\u65b9\u5dee\u8c03\u5ea6\u5668\uff0c\u7528\u4e8e\u89e3\u51b3\u5c0f\u6837\u672c\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8fc7\u62df\u5408\u95ee\u9898\u3002", "motivation": "\u65b0\u4e00\u4ee3\u9ad8\u5149\u8c31\u536b\u661f\u4f20\u611f\u5668\u4e0e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7ed3\u5408\uff0c\u80fd\u591f\u5728\u5927\u4e2d\u5c3a\u5ea6\u4e0a\u533a\u5206\u8be6\u7ec6\u7684\u5730\u7269\u7c7b\u522b\uff0c\u4f46\u5c0f\u6837\u672c\u8bad\u7ec3\u65f6\u5bb9\u6613\u8fc7\u62df\u5408\u3002", "method": "\u4f7f\u7528\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u8fdb\u884c\u6570\u636e\u589e\u5f3a\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7Transformer\u7f51\u7edc\u6355\u83b7\u590d\u6742\u6570\u636e\u6a21\u5f0f\uff0c\u91c7\u7528\u6539\u8fdb\u7684\u52a0\u6743\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u7684\u4f59\u5f26\u65b9\u5dee\u8c03\u5ea6\u5668\u5b9e\u73b0\u5c0f\u6570\u636e\u96c6\u4e0a\u7684\u5feb\u901f\u6709\u6548\u8bad\u7ec3\u3002", "result": "\u5728PRISMA\u536b\u661f\u83b7\u53d6\u768410\u79cd\u68ee\u6797\u7c7b\u578b\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u5e73\u5747\u7cbe\u5ea6\u548c\u52a0\u6743\u5e73\u5747\u7cbe\u5ea6\u4e0a\u5747\u4f18\u4e8e\u5176\u4ed6\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u4e14\u8bad\u7ec3\u8fc7\u7a0b\u7a33\u5b9a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5e38\u89c1\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u4e3a\u5c0f\u6837\u672c\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.08377", "pdf": "https://arxiv.org/pdf/2510.08377", "abs": "https://arxiv.org/abs/2510.08377", "authors": ["Cong Wei", "Quande Liu", "Zixuan Ye", "Qiulin Wang", "Xintao Wang", "Pengfei Wan", "Kun Gai", "Wenhu Chen"], "title": "UniVideo: Unified Understanding, Generation, and Editing for Videos", "categories": ["cs.CV"], "comment": "Project Website https://congwei1230.github.io/UniVideo/", "summary": "Unified multimodal models have shown promising results in multimodal content generation and editing but remain largely limited to the image domain. In this work, we present UniVideo, a versatile framework that extends unified modeling to the video domain. UniVideo adopts a dual-stream design, combining a Multimodal Large Language Model (MLLM) for instruction understanding with a Multimodal DiT (MMDiT) for video generation. This design enables accurate interpretation of complex multimodal instructions while preserving visual consistency. Built on this architecture, UniVideo unifies diverse video generation and editing tasks under a single multimodal instruction paradigm and is jointly trained across them. Extensive experiments demonstrate that UniVideo matches or surpasses state-of-the-art task-specific baselines in text/image-to-video generation, in-context video generation and in-context video editing. Notably, the unified design of UniVideo enables two forms of generalization. First, UniVideo supports task composition, such as combining editing with style transfer, by integrating multiple capabilities within a single instruction. Second, even without explicit training on free-form video editing, UniVideo transfers its editing capability from large-scale image editing data to this setting, handling unseen instructions such as green-screening characters or changing materials within a video. Beyond these core capabilities, UniVideo also supports visual-prompt-based video generation, where the MLLM interprets visual prompts and guides the MMDiT during synthesis. To foster future research, we will release our model and code.", "AI": {"tldr": "UniVideo\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u89c6\u9891\u751f\u6210\u548c\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u6d41\u8bbe\u8ba1\uff08MLLM\u7406\u89e3\u6307\u4ee4+MMDiT\u751f\u6210\u89c6\u9891\uff09\u5b9e\u73b0\u591a\u79cd\u89c6\u9891\u4efb\u52a1\u7684\u7edf\u4e00\u5904\u7406\uff0c\u652f\u6301\u4efb\u52a1\u7ec4\u5408\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u4e3b\u8981\u5c40\u9650\u4e8e\u56fe\u50cf\u9886\u57df\uff0c\u672c\u6587\u65e8\u5728\u5c06\u7edf\u4e00\u5efa\u6a21\u6269\u5c55\u5230\u89c6\u9891\u9886\u57df\uff0c\u89e3\u51b3\u590d\u6742\u591a\u6a21\u6001\u6307\u4ee4\u7406\u89e3\u548c\u89c6\u89c9\u4e00\u81f4\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53cc\u6d41\u67b6\u6784\uff1a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u8d1f\u8d23\u7406\u89e3\u6307\u4ee4\uff0c\u591a\u6a21\u6001DiT\uff08MMDiT\uff09\u8d1f\u8d23\u89c6\u9891\u751f\u6210\u3002\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u7edf\u4e00\u5904\u7406\u6587\u672c/\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u3001\u4e0a\u4e0b\u6587\u89c6\u9891\u751f\u6210\u548c\u7f16\u8f91\u7b49\u4efb\u52a1\u3002", "result": "UniVideo\u5728\u6587\u672c/\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u3001\u4e0a\u4e0b\u6587\u89c6\u9891\u751f\u6210\u548c\u7f16\u8f91\u7b49\u4efb\u52a1\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u7279\u5b9a\u4efb\u52a1\u57fa\u7ebf\uff0c\u5e76\u5c55\u73b0\u51fa\u4efb\u52a1\u7ec4\u5408\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u7684\u80fd\u529b\u3002", "conclusion": "UniVideo\u8bc1\u660e\u4e86\u7edf\u4e00\u591a\u6a21\u6001\u5efa\u6a21\u5728\u89c6\u9891\u9886\u57df\u7684\u53ef\u884c\u6027\uff0c\u652f\u6301\u4efb\u52a1\u7ec4\u5408\u548c\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u4e3a\u672a\u6765\u89c6\u9891\u751f\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.08398", "pdf": "https://arxiv.org/pdf/2510.08398", "abs": "https://arxiv.org/abs/2510.08398", "authors": ["Zeqing Wang", "Xinyu Wei", "Bairui Li", "Zhen Guo", "Jinrui Zhang", "Hongyang Wei", "Keze Wang", "Lei Zhang"], "title": "VideoVerse: How Far is Your T2V Generator from a World Model?", "categories": ["cs.CV"], "comment": "24 Pages, 8 Figures, 11 Tables", "summary": "The recent rapid advancement of Text-to-Video (T2V) generation technologies, which are critical to build ``world models'', makes the existing benchmarks increasingly insufficient to evaluate state-of-the-art T2V models. First, current evaluation dimensions, such as per-frame aesthetic quality and temporal consistency, are no longer able to differentiate state-of-the-art T2V models. Second, event-level temporal causality, which not only distinguishes video from other modalities but also constitutes a crucial component of world models, is severely underexplored in existing benchmarks. Third, existing benchmarks lack a systematic assessment of world knowledge, which are essential capabilities for building world models. To address these issues, we introduce VideoVerse, a comprehensive benchmark that focuses on evaluating whether a T2V model could understand complex temporal causality and world knowledge in the real world. We collect representative videos across diverse domains (e.g., natural landscapes, sports, indoor scenes, science fiction, chemical and physical experiments) and extract their event-level descriptions with inherent temporal causality, which are then rewritten into text-to-video prompts by independent annotators. For each prompt, we design a suite of binary evaluation questions from the perspective of dynamic and static properties, with a total of ten carefully defined evaluation dimensions. In total, our VideoVerse comprises 300 carefully curated prompts, involving 815 events and 793 binary evaluation questions. Consequently, a human preference aligned QA-based evaluation pipeline is developed by using modern vision-language models. Finally, we perform a systematic evaluation of state-of-the-art open-source and closed-source T2V models on VideoVerse, providing in-depth analysis on how far the current T2V generators are from world models.", "AI": {"tldr": "VideoVerse\u662f\u4e00\u4e2a\u65b0\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u6ce8\u4e8e\u8bc4\u4f30T2V\u6a21\u578b\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u590d\u6742\u65f6\u95f4\u56e0\u679c\u5173\u7cfb\u548c\u4e16\u754c\u77e5\u8bc6\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5305\u542b300\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u63d0\u793a\u3001815\u4e2a\u4e8b\u4ef6\u548c793\u4e2a\u4e8c\u5143\u8bc4\u4f30\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u8bc4\u4f30\u6700\u5148\u8fdb\u7684T2V\u6a21\u578b\uff0c\u56e0\u4e3a\u5f53\u524d\u8bc4\u4f30\u7ef4\u5ea6\uff08\u5982\u9010\u5e27\u7f8e\u5b66\u8d28\u91cf\u548c\u65f6\u95f4\u4e00\u81f4\u6027\uff09\u5df2\u65e0\u6cd5\u533a\u5206\u9876\u7ea7\u6a21\u578b\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u4e8b\u4ef6\u7ea7\u65f6\u95f4\u56e0\u679c\u5173\u7cfb\u548c\u4e16\u754c\u77e5\u8bc6\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u6536\u96c6\u8de8\u591a\u4e2a\u9886\u57df\u7684\u4ee3\u8868\u6027\u89c6\u9891\uff0c\u63d0\u53d6\u5177\u6709\u5185\u5728\u65f6\u95f4\u56e0\u679c\u5173\u7cfb\u7684\u4e8b\u4ef6\u7ea7\u63cf\u8ff0\uff0c\u7531\u72ec\u7acb\u6807\u6ce8\u8005\u91cd\u5199\u4e3a\u6587\u672c\u5230\u89c6\u9891\u63d0\u793a\uff0c\u8bbe\u8ba1\u5305\u542b10\u4e2a\u7ef4\u5ea6\u7684\u4e8c\u5143\u8bc4\u4f30\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u57fa\u4e8e\u4eba\u7c7b\u504f\u597d\u7684\u95ee\u7b54\u8bc4\u4f30\u6d41\u7a0b\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b300\u4e2a\u63d0\u793a\u3001815\u4e2a\u4e8b\u4ef6\u548c793\u4e2a\u8bc4\u4f30\u95ee\u9898\u7684VideoVerse\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u5bf9\u5f00\u6e90\u548c\u95ed\u6e90\u7684\u5148\u8fdbT2V\u6a21\u578b\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "conclusion": "VideoVerse\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u8bc4\u4f30T2V\u6a21\u578b\u5728\u6784\u5efa\u4e16\u754c\u6a21\u578b\u65b9\u9762\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u5168\u9762\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5f53\u524dT2V\u751f\u6210\u5668\u4e0e\u4e16\u754c\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2510.08431", "pdf": "https://arxiv.org/pdf/2510.08431", "abs": "https://arxiv.org/abs/2510.08431", "authors": ["Kaiwen Zheng", "Yuji Wang", "Qianli Ma", "Huayu Chen", "Jintao Zhang", "Yogesh Balaji", "Jianfei Chen", "Ming-Yu Liu", "Jun Zhu", "Qinsheng Zhang"], "title": "Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "This work represents the first effort to scale up continuous-time consistency distillation to general application-level image and video diffusion models. Although continuous-time consistency model (sCM) is theoretically principled and empirically powerful for accelerating academic-scale diffusion, its applicability to large-scale text-to-image and video tasks remains unclear due to infrastructure challenges in Jacobian-vector product (JVP) computation and the limitations of standard evaluation benchmarks. We first develop a parallelism-compatible FlashAttention-2 JVP kernel, enabling sCM training on models with over 10 billion parameters and high-dimensional video tasks. Our investigation reveals fundamental quality limitations of sCM in fine-detail generation, which we attribute to error accumulation and the \"mode-covering\" nature of its forward-divergence objective. To remedy this, we propose the score-regularized continuous-time consistency model (rCM), which incorporates score distillation as a long-skip regularizer. This integration complements sCM with the \"mode-seeking\" reverse divergence, effectively improving visual quality while maintaining high generation diversity. Validated on large-scale models (Cosmos-Predict2, Wan2.1) up to 14B parameters and 5-second videos, rCM matches or surpasses the state-of-the-art distillation method DMD2 on quality metrics while offering notable advantages in diversity, all without GAN tuning or extensive hyperparameter searches. The distilled models generate high-fidelity samples in only $1\\sim4$ steps, accelerating diffusion sampling by $15\\times\\sim50\\times$. These results position rCM as a practical and theoretically grounded framework for advancing large-scale diffusion distillation.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5c06\u8fde\u7eed\u65f6\u95f4\u4e00\u81f4\u6027\u84b8\u998f\u6269\u5c55\u5230\u901a\u7528\u5e94\u7528\u7ea7\u56fe\u50cf\u548c\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u5206\u6570\u6b63\u5219\u5316\u8fde\u7eed\u65f6\u95f4\u4e00\u81f4\u6027\u6a21\u578b(rCM)\uff0c\u89e3\u51b3\u4e86sCM\u5728\u7ec6\u8282\u751f\u6210\u4e2d\u7684\u8d28\u91cf\u9650\u5236\uff0c\u572814B\u53c2\u6570\u6a21\u578b\u548c5\u79d2\u89c6\u9891\u4e0a\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u9ad8\u591a\u6837\u6027\u76841-4\u6b65\u751f\u6210\u3002", "motivation": "\u8fde\u7eed\u65f6\u95f4\u4e00\u81f4\u6027\u6a21\u578b(sCM)\u867d\u7136\u5728\u5b66\u672f\u89c4\u6a21\u6269\u6563\u52a0\u901f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u548c\u89c6\u9891\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u4ecd\u4e0d\u660e\u786e\uff0c\u4e3b\u8981\u9762\u4e34\u96c5\u53ef\u6bd4\u5411\u91cf\u79ef\u8ba1\u7b97\u7684\u57fa\u7840\u8bbe\u65bd\u6311\u6218\u548c\u6807\u51c6\u8bc4\u4f30\u57fa\u51c6\u7684\u5c40\u9650\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u5e76\u884c\u517c\u5bb9\u7684FlashAttention-2 JVP\u5185\u6838\uff0c\u652f\u6301\u8d85\u8fc7100\u4ebf\u53c2\u6570\u6a21\u578b\u8bad\u7ec3\uff1b\u63d0\u51fa\u5206\u6570\u6b63\u5219\u5316\u8fde\u7eed\u65f6\u95f4\u4e00\u81f4\u6027\u6a21\u578b(rCM)\uff0c\u901a\u8fc7\u5206\u6570\u84b8\u998f\u4f5c\u4e3a\u957f\u8df3\u8dc3\u6b63\u5219\u5668\uff0c\u7ed3\u5408sCM\u7684\u524d\u5411\u6563\u5ea6\u548c\u53cd\u5411\u6563\u5ea6\u7684\u4f18\u52bf\u3002", "result": "\u5728Cosmos-Predict2\u3001Wan2.1\u7b49\u9ad8\u8fbe14B\u53c2\u6570\u6a21\u578b\u548c5\u79d2\u89c6\u9891\u4e0a\u9a8c\u8bc1\uff0crCM\u5728\u8d28\u91cf\u6307\u6807\u4e0a\u5339\u914d\u6216\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u84b8\u998f\u65b9\u6cd5DMD2\uff0c\u540c\u65f6\u5728\u591a\u6837\u6027\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u65e0\u9700GAN\u8c03\u4f18\u6216\u5927\u91cf\u8d85\u53c2\u6570\u641c\u7d22\uff0c\u4ec5\u97001-4\u6b65\u751f\u6210\u9ad8\u4fdd\u771f\u6837\u672c\uff0c\u52a0\u901f\u6269\u6563\u91c7\u683715-50\u500d\u3002", "conclusion": "rCM\u4e3a\u63a8\u8fdb\u5927\u89c4\u6a21\u6269\u6563\u84b8\u998f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u7406\u8bba\u57fa\u7840\u7684\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u548c\u9ad8\u591a\u6837\u6027\u7684\u5feb\u901f\u751f\u6210\u3002"}}
{"id": "2510.08449", "pdf": "https://arxiv.org/pdf/2510.08449", "abs": "https://arxiv.org/abs/2510.08449", "authors": ["Noor Islam S. Mohammad"], "title": "Hierarchical Spatial Algorithms for High-Resolution Image Quantization and Feature Extraction", "categories": ["cs.CV", "68T45, 68U10", "I.4.8; I.2.10"], "comment": "There are 14 pages journal paper", "summary": "This study introduces a modular framework for spatial image processing, integrating grayscale quantization, color and brightness enhancement, image sharpening, bidirectional transformation pipelines, and geometric feature extraction. A stepwise intensity transformation quantizes grayscale images into eight discrete levels, producing a posterization effect that simplifies representation while preserving structural detail. Color enhancement is achieved via histogram equalization in both RGB and YCrCb color spaces, with the latter improving contrast while maintaining chrominance fidelity. Brightness adjustment is implemented through HSV value-channel manipulation, and image sharpening is performed using a 3 * 3 convolution kernel to enhance high-frequency details. A bidirectional transformation pipeline that integrates unsharp masking, gamma correction, and noise amplification achieved accuracy levels of 76.10% and 74.80% for the forward and reverse processes, respectively. Geometric feature extraction employed Canny edge detection, Hough-based line estimation (e.g., 51.50{\\deg} for billiard cue alignment), Harris corner detection, and morphological window localization. Cue isolation further yielded 81.87\\% similarity against ground truth images. Experimental evaluation across diverse datasets demonstrates robust and deterministic performance, highlighting its potential for real-time image analysis and computer vision.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6a21\u5757\u5316\u7a7a\u95f4\u56fe\u50cf\u5904\u7406\u6846\u67b6\uff0c\u5305\u542b\u7070\u5ea6\u91cf\u5316\u3001\u8272\u5f69\u4eae\u5ea6\u589e\u5f3a\u3001\u56fe\u50cf\u9510\u5316\u3001\u53cc\u5411\u53d8\u6362\u7ba1\u9053\u548c\u51e0\u4f55\u7279\u5f81\u63d0\u53d6\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u51fa\u7a33\u5065\u7684\u786e\u5b9a\u6027\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u7efc\u5408\u7684\u56fe\u50cf\u5904\u7406\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u591a\u79cd\u56fe\u50cf\u589e\u5f3a\u548c\u7279\u5f81\u63d0\u53d6\u4efb\u52a1\uff0c\u6ee1\u8db3\u5b9e\u65f6\u56fe\u50cf\u5206\u6790\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u5305\u62ec\u516b\u7ea7\u7070\u5ea6\u91cf\u5316\u3001RGB\u548cYCrCb\u8272\u5f69\u7a7a\u95f4\u7684\u76f4\u65b9\u56fe\u5747\u8861\u5316\u3001HSV\u4eae\u5ea6\u8c03\u6574\u30013\u00d73\u5377\u79ef\u6838\u9510\u5316\u3001\u53cc\u5411\u53d8\u6362\u7ba1\u9053\uff08\u5305\u542b\u53cd\u9510\u5316\u63a9\u6a21\u3001\u4f3d\u9a6c\u6821\u6b63\u548c\u566a\u58f0\u653e\u5927\uff09\uff0c\u4ee5\u53caCanny\u8fb9\u7f18\u68c0\u6d4b\u3001Hough\u7ebf\u4f30\u8ba1\u3001Harris\u89d2\u70b9\u68c0\u6d4b\u7b49\u51e0\u4f55\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u3002", "result": "\u53cc\u5411\u53d8\u6362\u7ba1\u9053\u6b63\u5411\u548c\u53cd\u5411\u8fc7\u7a0b\u51c6\u786e\u7387\u5206\u522b\u4e3a76.10%\u548c74.80%\uff0c\u7403\u6746\u5bf9\u9f50\u89d2\u5ea6\u4f30\u8ba1\u4e3a51.50\u5ea6\uff0c\u7403\u6746\u9694\u79bb\u4e0e\u771f\u5b9e\u56fe\u50cf\u7684\u76f8\u4f3c\u5ea6\u8fbe\u523081.87%\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u7a33\u5065\u548c\u786e\u5b9a\u6027\u7684\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u65f6\u56fe\u50cf\u5206\u6790\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.08508", "pdf": "https://arxiv.org/pdf/2510.08508", "abs": "https://arxiv.org/abs/2510.08508", "authors": ["Lu Liu", "Chunlei Cai", "Shaocheng Shen", "Jianfeng Liang", "Weimin Ouyang", "Tianxiao Ye", "Jian Mao", "Huiyu Duan", "Jiangchao Yao", "Xiaoyun Zhang", "Qiang Hu", "Guangtao Zhai"], "title": "MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Real-world videos often suffer from complex degradations, such as noise, compression artifacts, and low-light distortions, due to diverse acquisition and transmission conditions. Existing restoration methods typically require professional manual selection of specialized models or rely on monolithic architectures that fail to generalize across varying degradations. Inspired by expert experience, we propose MoA-VR, the first \\underline{M}ixture-\\underline{o}f-\\underline{A}gents \\underline{V}ideo \\underline{R}estoration system that mimics the reasoning and processing procedures of human professionals through three coordinated agents: Degradation Identification, Routing and Restoration, and Restoration Quality Assessment. Specifically, we construct a large-scale and high-resolution video degradation recognition benchmark and build a vision-language model (VLM) driven degradation identifier. We further introduce a self-adaptive router powered by large language models (LLMs), which autonomously learns effective restoration strategies by observing tool usage patterns. To assess intermediate and final processed video quality, we construct the \\underline{Res}tored \\underline{V}ideo \\underline{Q}uality (Res-VQ) dataset and design a dedicated VLM-based video quality assessment (VQA) model tailored for restoration tasks. Extensive experiments demonstrate that MoA-VR effectively handles diverse and compound degradations, consistently outperforming existing baselines in terms of both objective metrics and perceptual quality. These results highlight the potential of integrating multimodal intelligence and modular reasoning in general-purpose video restoration systems.", "AI": {"tldr": "MoA-VR\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u89c6\u9891\u4fee\u590d\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e09\u4e2a\u534f\u8c03\u7684\u667a\u80fd\u4f53\uff08\u9000\u5316\u8bc6\u522b\u3001\u8def\u7531\u4fee\u590d\u3001\u8d28\u91cf\u8bc4\u4f30\uff09\u6765\u6a21\u62df\u4eba\u7c7b\u4e13\u5bb6\u7684\u5904\u7406\u6d41\u7a0b\uff0c\u6709\u6548\u5904\u7406\u590d\u6742\u7684\u89c6\u9891\u9000\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u89c6\u9891\u5e38\u56e0\u91c7\u96c6\u548c\u4f20\u8f93\u6761\u4ef6\u800c\u906d\u53d7\u590d\u6742\u9000\u5316\uff08\u5982\u566a\u58f0\u3001\u538b\u7f29\u4f2a\u5f71\u3001\u4f4e\u5149\u5931\u771f\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u4e13\u4e1a\u624b\u52a8\u9009\u62e9\u4e13\u7528\u6a21\u578b\u6216\u91c7\u7528\u5355\u4e00\u67b6\u6784\uff0c\u96be\u4ee5\u6cdb\u5316\u5904\u7406\u5404\u79cd\u9000\u5316\u7c7b\u578b\u3002", "method": "\u63d0\u51faMoA-VR\u7cfb\u7edf\uff0c\u5305\u542b\u4e09\u4e2a\u667a\u80fd\u4f53\uff1a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u9000\u5316\u8bc6\u522b\u5668\u3001\u7531\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u8def\u7531\u5668\u3001\u4ee5\u53ca\u4e13\u95e8\u4e3a\u4fee\u590d\u4efb\u52a1\u8bbe\u8ba1\u7684VLM\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMoA-VR\u80fd\u6709\u6548\u5904\u7406\u591a\u6837\u5316\u548c\u590d\u5408\u9000\u5316\uff0c\u5728\u5ba2\u89c2\u6307\u6807\u548c\u611f\u77e5\u8d28\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u591a\u6a21\u6001\u667a\u80fd\u548c\u6a21\u5757\u5316\u63a8\u7406\u5728\u901a\u7528\u89c6\u9891\u4fee\u590d\u7cfb\u7edf\u4e2d\u7684\u6574\u5408\u6f5c\u529b\u3002"}}
{"id": "2510.08527", "pdf": "https://arxiv.org/pdf/2510.08527", "abs": "https://arxiv.org/abs/2510.08527", "authors": ["Zhiyuan Zhang", "Can Wang", "Dongdong Chen", "Jing Liao"], "title": "FlexTraj: Image-to-Video Generation with Flexible Point Trajectory Control", "categories": ["cs.CV"], "comment": "Project Page: https://bestzzhang.github.io/FlexTraj", "summary": "We present FlexTraj, a framework for image-to-video generation with flexible point trajectory control. FlexTraj introduces a unified point-based motion representation that encodes each point with a segmentation ID, a temporally consistent trajectory ID, and an optional color channel for appearance cues, enabling both dense and sparse trajectory control. Instead of injecting trajectory conditions into the video generator through token concatenation or ControlNet, FlexTraj employs an efficient sequence-concatenation scheme that achieves faster convergence, stronger controllability, and more efficient inference, while maintaining robustness under unaligned conditions. To train such a unified point trajectory-controlled video generator, FlexTraj adopts an annealing training strategy that gradually reduces reliance on complete supervision and aligned condition. Experimental results demonstrate that FlexTraj enables multi-granularity, alignment-agnostic trajectory control for video generation, supporting various applications such as motion cloning, drag-based image-to-video, motion interpolation, camera redirection, flexible action control and mesh animations.", "AI": {"tldr": "FlexTraj\u662f\u4e00\u4e2a\u7528\u4e8e\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u57fa\u4e8e\u70b9\u7684\u8fd0\u52a8\u8868\u793a\u5b9e\u73b0\u7075\u6d3b\u8f68\u8ff9\u63a7\u5236\uff0c\u652f\u6301\u5bc6\u96c6\u548c\u7a00\u758f\u8f68\u8ff9\u63a7\u5236\uff0c\u91c7\u7528\u5e8f\u5217\u62fc\u63a5\u65b9\u6848\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u548c\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u5728\u8f68\u8ff9\u63a7\u5236\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u3001\u591a\u7c92\u5ea6\u7684\u8f68\u8ff9\u63a7\u5236\u80fd\u529b\uff0c\u540c\u65f6\u89e3\u51b3\u6761\u4ef6\u5bf9\u9f50\u548c\u8bad\u7ec3\u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u70b9\u8fd0\u52a8\u8868\u793a\uff08\u5206\u5272ID\u3001\u8f68\u8ff9ID\u3001\u989c\u8272\u901a\u9053\uff09\uff0c\u91c7\u7528\u5e8f\u5217\u62fc\u63a5\u65b9\u6848\u6ce8\u5165\u8f68\u8ff9\u6761\u4ef6\uff0c\u4f7f\u7528\u9000\u706b\u8bad\u7ec3\u7b56\u7565\u9010\u6b65\u51cf\u5c11\u5bf9\u5b8c\u6574\u76d1\u7763\u548c\u5bf9\u9f50\u6761\u4ef6\u7684\u4f9d\u8d56\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eFlexTraj\u652f\u6301\u591a\u7c92\u5ea6\u3001\u5bf9\u9f50\u65e0\u5173\u7684\u8f68\u8ff9\u63a7\u5236\uff0c\u9002\u7528\u4e8e\u8fd0\u52a8\u514b\u9686\u3001\u62d6\u62fd\u5f0f\u56fe\u50cf\u5230\u89c6\u9891\u3001\u8fd0\u52a8\u63d2\u503c\u3001\u76f8\u673a\u91cd\u5b9a\u5411\u3001\u7075\u6d3b\u52a8\u4f5c\u63a7\u5236\u548c\u7f51\u683c\u52a8\u753b\u7b49\u591a\u79cd\u5e94\u7528\u3002", "conclusion": "FlexTraj\u6846\u67b6\u5728\u8f68\u8ff9\u63a7\u5236\u7684\u7075\u6d3b\u6027\u3001\u8bad\u7ec3\u6548\u7387\u548c\u63a8\u7406\u6027\u80fd\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u8f68\u8ff9\u63a7\u5236\u80fd\u529b\u3002"}}
{"id": "2510.08555", "pdf": "https://arxiv.org/pdf/2510.08555", "abs": "https://arxiv.org/abs/2510.08555", "authors": ["Minghong Cai", "Qiulin Wang", "Zongli Ye", "Wenze Liu", "Quande Liu", "Weicai Ye", "Xintao Wang", "Pengfei Wan", "Kun Gai", "Xiangyu Yue"], "title": "VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning", "categories": ["cs.CV"], "comment": "Project page: https://onevfall.github.io/project_page/videocanvas", "summary": "We introduce the task of arbitrary spatio-temporal video completion, where a video is generated from arbitrary, user-specified patches placed at any spatial location and timestamp, akin to painting on a video canvas. This flexible formulation naturally unifies many existing controllable video generation tasks--including first-frame image-to-video, inpainting, extension, and interpolation--under a single, cohesive paradigm. Realizing this vision, however, faces a fundamental obstacle in modern latent video diffusion models: the temporal ambiguity introduced by causal VAEs, where multiple pixel frames are compressed into a single latent representation, making precise frame-level conditioning structurally difficult. We address this challenge with VideoCanvas, a novel framework that adapts the In-Context Conditioning (ICC) paradigm to this fine-grained control task with zero new parameters. We propose a hybrid conditioning strategy that decouples spatial and temporal control: spatial placement is handled via zero-padding, while temporal alignment is achieved through Temporal RoPE Interpolation, which assigns each condition a continuous fractional position within the latent sequence. This resolves the VAE's temporal ambiguity and enables pixel-frame-aware control on a frozen backbone. To evaluate this new capability, we develop VideoCanvasBench, the first benchmark for arbitrary spatio-temporal video completion, covering both intra-scene fidelity and inter-scene creativity. Experiments demonstrate that VideoCanvas significantly outperforms existing conditioning paradigms, establishing a new state of the art in flexible and unified video generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86VideoCanvas\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u6761\u4ef6\u7b56\u7565\u89e3\u51b3\u6f5c\u5728\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u7684\u65f6\u95f4\u6a21\u7cca\u95ee\u9898\uff0c\u5b9e\u73b0\u4efb\u610f\u65f6\u7a7a\u89c6\u9891\u8865\u5168\u4efb\u52a1\u3002", "motivation": "\u7edf\u4e00\u73b0\u6709\u53ef\u63a7\u89c6\u9891\u751f\u6210\u4efb\u52a1\uff08\u5982\u56fe\u50cf\u5230\u89c6\u9891\u3001\u4fee\u590d\u3001\u6269\u5c55\u3001\u63d2\u503c\uff09\u5230\u4e00\u4e2a\u7075\u6d3b\u8303\u5f0f\uff0c\u4f46\u9762\u4e34\u56e0\u679cVAE\u5f15\u5165\u7684\u65f6\u95f4\u6a21\u7cca\u95ee\u9898\u3002", "method": "\u91c7\u7528\u96f6\u53c2\u6570\u589e\u52a0\u7684\u4e0a\u4e0b\u6587\u6761\u4ef6\u8303\u5f0f\uff0c\u901a\u8fc7\u96f6\u586b\u5145\u5904\u7406\u7a7a\u95f4\u63a7\u5236\uff0c\u901a\u8fc7\u65f6\u95f4RoPE\u63d2\u503c\u5b9e\u73b0\u65f6\u95f4\u5bf9\u9f50\uff0c\u89e3\u8026\u65f6\u7a7a\u63a7\u5236\u3002", "result": "\u5728VideoCanvasBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6761\u4ef6\u8303\u5f0f\uff0c\u5efa\u7acb\u4e86\u7075\u6d3b\u7edf\u4e00\u89c6\u9891\u751f\u6210\u7684\u65b0SOTA\u3002", "conclusion": "VideoCanvas\u6210\u529f\u89e3\u51b3\u4e86\u6f5c\u5728\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u65f6\u95f4\u6a21\u7cca\u95ee\u9898\uff0c\u4e3a\u4efb\u610f\u65f6\u7a7a\u89c6\u9891\u8865\u5168\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.08561", "pdf": "https://arxiv.org/pdf/2510.08561", "abs": "https://arxiv.org/abs/2510.08561", "authors": ["Maham Tanveer", "Yang Zhou", "Simon Niklaus", "Ali Mahdavi Amiri", "Hao Zhang", "Krishna Kumar Singh", "Nanxuan Zhao"], "title": "MultiCOIN: Multi-Modal COntrollable Video INbetweening", "categories": ["cs.CV"], "comment": "Project website: https://multicoinx.github.io/multicoin/", "summary": "Video inbetweening creates smooth and natural transitions between two image frames, making it an indispensable tool for video editing and long-form video synthesis. Existing works in this domain are unable to generate large, complex, or intricate motions. In particular, they cannot accommodate the versatility of user intents and generally lack fine control over the details of intermediate frames, leading to misalignment with the creative mind. To fill these gaps, we introduce \\modelname{}, a video inbetweening framework that allows multi-modal controls, including depth transition and layering, motion trajectories, text prompts, and target regions for movement localization, while achieving a balance between flexibility, ease of use, and precision for fine-grained video interpolation. To achieve this, we adopt the Diffusion Transformer (DiT) architecture as our video generative model, due to its proven capability to generate high-quality long videos. To ensure compatibility between DiT and our multi-modal controls, we map all motion controls into a common sparse and user-friendly point-based representation as the video/noise input. Further, to respect the variety of controls which operate at varying levels of granularity and influence, we separate content controls and motion controls into two branches to encode the required features before guiding the denoising process, resulting in two generators, one for motion and the other for content. Finally, we propose a stage-wise training strategy to ensure that our model learns the multi-modal controls smoothly. Extensive qualitative and quantitative experiments demonstrate that multi-modal controls enable a more dynamic, customizable, and contextually accurate visual narrative.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u652f\u6301\u591a\u6a21\u6001\u63a7\u5236\u7684\u89c6\u9891\u63d2\u5e27\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u6df1\u5ea6\u8fc7\u6e21\u3001\u8fd0\u52a8\u8f68\u8ff9\u3001\u6587\u672c\u63d0\u793a\u7b49\u591a\u79cd\u65b9\u5f0f\u7cbe\u786e\u63a7\u5236\u4e2d\u95f4\u5e27\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u590d\u6742\u8fd0\u52a8\u548c\u7f3a\u4e4f\u7cbe\u7ec6\u63a7\u5236\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u63d2\u5e27\u65b9\u6cd5\u65e0\u6cd5\u751f\u6210\u5927\u89c4\u6a21\u3001\u590d\u6742\u6216\u7cbe\u7ec6\u7684\u8fd0\u52a8\uff0c\u7f3a\u4e4f\u5bf9\u4e2d\u95f4\u5e27\u7ec6\u8282\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u96be\u4ee5\u6ee1\u8db3\u7528\u6237\u591a\u6837\u5316\u7684\u521b\u4f5c\u610f\u56fe\u3002", "method": "\u91c7\u7528Diffusion Transformer\u67b6\u6784\u4f5c\u4e3a\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u5c06\u6240\u6709\u8fd0\u52a8\u63a7\u5236\u6620\u5c04\u4e3a\u7edf\u4e00\u7684\u57fa\u4e8e\u70b9\u7684\u7a00\u758f\u8868\u793a\uff0c\u5c06\u5185\u5bb9\u63a7\u5236\u548c\u8fd0\u52a8\u63a7\u5236\u5206\u79bb\u4e3a\u4e24\u4e2a\u5206\u652f\u8fdb\u884c\u7279\u5f81\u7f16\u7801\uff0c\u5e76\u91c7\u7528\u5206\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0c\u591a\u6a21\u6001\u63a7\u5236\u80fd\u591f\u5b9e\u73b0\u66f4\u52a8\u6001\u3001\u53ef\u5b9a\u5236\u548c\u4e0a\u4e0b\u6587\u51c6\u786e\u7684\u89c6\u89c9\u53d9\u4e8b\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u6a21\u6001\u63a7\u5236\u89c6\u9891\u63d2\u5e27\u6846\u67b6\u5728\u7075\u6d3b\u6027\u3001\u6613\u7528\u6027\u548c\u7cbe\u786e\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u4e3a\u89c6\u9891\u7f16\u8f91\u548c\u957f\u89c6\u9891\u5408\u6210\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5de5\u5177\u3002"}}
{"id": "2510.08562", "pdf": "https://arxiv.org/pdf/2510.08562", "abs": "https://arxiv.org/abs/2510.08562", "authors": ["Zhiyu Zheng", "Shaoyu Chen", "Haoran Yin", "Xinbang Zhang", "Jialv Zou", "Xinggang Wang", "Qian Zhang", "Lefei Zhang"], "title": "ResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous Driving", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "End-to-end autonomous driving (E2EAD) systems, which learn to predict future trajectories directly from sensor data, are fundamentally challenged by the inherent spatio-temporal imbalance of trajectory data. This imbalance creates a significant optimization burden, causing models to learn spurious correlations instead of causal inference, while also prioritizing uncertain, distant predictions, thereby compromising immediate safety. To address these issues, we propose ResAD, a novel Normalized Residual Trajectory Modeling framework. Instead of predicting the future trajectory directly, our approach reframes the learning task to predict the residual deviation from a deterministic inertial reference. The inertial reference serves as a counterfactual, forcing the model to move beyond simple pattern recognition and instead identify the underlying causal factors (e.g., traffic rules, obstacles) that necessitate deviations from a default, inertially-guided path. To deal with the optimization imbalance caused by uncertain, long-term horizons, ResAD further incorporates Point-wise Normalization of the predicted residual. It re-weights the optimization objective, preventing large-magnitude errors associated with distant, uncertain waypoints from dominating the learning signal. Extensive experiments validate the effectiveness of our framework. On the NAVSIM benchmark, ResAD achieves a state-of-the-art PDMS of 88.6 using a vanilla diffusion policy with only two denoising steps, demonstrating that our approach significantly simplifies the learning task and improves model performance. The code will be released to facilitate further research.", "AI": {"tldr": "ResAD\u662f\u4e00\u4e2a\u7528\u4e8e\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7684\u5f52\u4e00\u5316\u6b8b\u5dee\u8f68\u8ff9\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u4e0e\u786e\u5b9a\u6027\u60ef\u6027\u53c2\u8003\u7684\u504f\u5dee\u6765\u89e3\u51b3\u8f68\u8ff9\u6570\u636e\u65f6\u7a7a\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u7b80\u5316\u5b66\u4e60\u4efb\u52a1\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u9762\u4e34\u8f68\u8ff9\u6570\u636e\u56fa\u6709\u7684\u65f6\u7a7a\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u8fd9\u5bfc\u81f4\u6a21\u578b\u5b66\u4e60\u865a\u5047\u76f8\u5173\u6027\u800c\u975e\u56e0\u679c\u63a8\u7406\uff0c\u5e76\u4f18\u5148\u8003\u8651\u4e0d\u786e\u5b9a\u7684\u8fdc\u8ddd\u79bb\u9884\u6d4b\uff0c\u4ece\u800c\u5371\u53ca\u5373\u65f6\u5b89\u5168\u3002", "method": "\u63d0\u51faResAD\u6846\u67b6\uff0c\u5c06\u5b66\u4e60\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u9884\u6d4b\u4e0e\u786e\u5b9a\u6027\u60ef\u6027\u53c2\u8003\u7684\u6b8b\u5dee\u504f\u5dee\uff0c\u800c\u975e\u76f4\u63a5\u9884\u6d4b\u672a\u6765\u8f68\u8ff9\u3002\u91c7\u7528\u9010\u70b9\u5f52\u4e00\u5316\u6765\u91cd\u65b0\u52a0\u6743\u4f18\u5316\u76ee\u6807\uff0c\u9632\u6b62\u4e0e\u8fdc\u8ddd\u79bb\u4e0d\u786e\u5b9a\u8def\u5f84\u70b9\u76f8\u5173\u7684\u5927\u5e45\u5ea6\u8bef\u5dee\u4e3b\u5bfc\u5b66\u4e60\u4fe1\u53f7\u3002", "result": "\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cResAD\u4f7f\u7528\u4ec5\u6709\u4e24\u4e2a\u53bb\u566a\u6b65\u9aa4\u7684\u666e\u901a\u6269\u6563\u7b56\u7565\u5b9e\u73b0\u4e8688.6\u7684\u6700\u5148\u8fdbPDMS\u5206\u6570\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u7b80\u5316\u4e86\u5b66\u4e60\u4efb\u52a1\u5e76\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "ResAD\u901a\u8fc7\u91cd\u65b0\u5b9a\u4e49\u5b66\u4e60\u4efb\u52a1\u548c\u5f15\u5165\u5f52\u4e00\u5316\u6b8b\u5dee\u5efa\u6a21\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8f68\u8ff9\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u56e0\u679c\u63a8\u7406\u548c\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.08566", "pdf": "https://arxiv.org/pdf/2510.08566", "abs": "https://arxiv.org/abs/2510.08566", "authors": ["Meixi Song", "Xin Lin", "Dizhe Zhang", "Haodong Li", "Xiangtai Li", "Bo Du", "Lu Qi"], "title": "D$^2$GS: Depth-and-Density Guided Gaussian Splatting for Stable and Accurate Sparse-View Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in 3D Gaussian Splatting (3DGS) enable real-time, high-fidelity novel view synthesis (NVS) with explicit 3D representations. However, performance degradation and instability remain significant under sparse-view conditions. In this work, we identify two key failure modes under sparse-view conditions: overfitting in regions with excessive Gaussian density near the camera, and underfitting in distant areas with insufficient Gaussian coverage. To address these challenges, we propose a unified framework D$^2$GS, comprising two key components: a Depth-and-Density Guided Dropout strategy that suppresses overfitting by adaptively masking redundant Gaussians based on density and depth, and a Distance-Aware Fidelity Enhancement module that improves reconstruction quality in under-fitted far-field areas through targeted supervision. Moreover, we introduce a new evaluation metric to quantify the stability of learned Gaussian distributions, providing insights into the robustness of the sparse-view 3DGS. Extensive experiments on multiple datasets demonstrate that our method significantly improves both visual quality and robustness under sparse view conditions. The project page can be found at: https://insta360-research-team.github.io/DDGS-website/.", "AI": {"tldr": "\u63d0\u51faD\u00b2GS\u6846\u67b6\u89e3\u51b3\u7a00\u758f\u89c6\u56fe\u4e0b3D\u9ad8\u65af\u6cfc\u6e85\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u901a\u8fc7\u5bc6\u5ea6-\u6df1\u5ea6\u5f15\u5bfc\u7684dropout\u7b56\u7565\u6291\u5236\u8fc7\u62df\u5408\uff0c\u4ee5\u53ca\u8ddd\u79bb\u611f\u77e5\u7684\u4fdd\u771f\u5ea6\u589e\u5f3a\u6a21\u5757\u6539\u5584\u8fdc\u573a\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u5728\u7a00\u758f\u89c6\u56fe\u6761\u4ef6\u4e0b\u5b58\u5728\u6027\u80fd\u4e0b\u964d\u548c\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4e3b\u8981\u7531\u4e8e\u8fd1\u76f8\u673a\u533a\u57df\u7684\u8fc7\u5ea6\u62df\u5408\u548c\u8fdc\u8ddd\u79bb\u533a\u57df\u7684\u62df\u5408\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u548c\u5bc6\u5ea6\u5f15\u5bfc\u7684dropout\u7b56\u7565\u81ea\u9002\u5e94\u63a9\u7801\u5197\u4f59\u9ad8\u65af\uff0c\u4ee5\u53ca\u8ddd\u79bb\u611f\u77e5\u4fdd\u771f\u5ea6\u589e\u5f3a\u6a21\u5757\u5bf9\u8fdc\u573a\u533a\u57df\u8fdb\u884c\u9488\u5bf9\u6027\u76d1\u7763\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u89c6\u56fe\u6761\u4ef6\u4e0b\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "D\u00b2GS\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u89c6\u56fe3D\u9ad8\u65af\u6cfc\u6e85\u7684\u8fc7\u62df\u5408\u548c\u6b20\u62df\u5408\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2510.08575", "pdf": "https://arxiv.org/pdf/2510.08575", "abs": "https://arxiv.org/abs/2510.08575", "authors": ["Haofei Xu", "Daniel Barath", "Andreas Geiger", "Marc Pollefeys"], "title": "ReSplat: Learning Recurrent Gaussian Splats", "categories": ["cs.CV"], "comment": "Project page: https://haofeixu.github.io/resplat/", "summary": "While feed-forward Gaussian splatting models provide computational efficiency and effectively handle sparse input settings, their performance is fundamentally limited by the reliance on a single forward pass during inference. We propose ReSplat, a feed-forward recurrent Gaussian splatting model that iteratively refines 3D Gaussians without explicitly computing gradients. Our key insight is that the Gaussian splatting rendering error serves as a rich feedback signal, guiding the recurrent network to learn effective Gaussian updates. This feedback signal naturally adapts to unseen data distributions at test time, enabling robust generalization. To initialize the recurrent process, we introduce a compact reconstruction model that operates in a $16 \\times$ subsampled space, producing $16 \\times$ fewer Gaussians than previous per-pixel Gaussian models. This substantially reduces computational overhead and allows for efficient Gaussian updates. Extensive experiments across varying of input views (2, 8, 16), resolutions ($256 \\times 256$ to $540 \\times 960$), and datasets (DL3DV and RealEstate10K) demonstrate that our method achieves state-of-the-art performance while significantly reducing the number of Gaussians and improving the rendering speed. Our project page is at https://haofeixu.github.io/resplat/.", "AI": {"tldr": "ReSplat\u662f\u4e00\u79cd\u524d\u9988\u5faa\u73af\u9ad8\u65af\u6e85\u5c04\u6a21\u578b\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u53163D\u9ad8\u65af\u800c\u4e0d\u663e\u5f0f\u8ba1\u7b97\u68af\u5ea6\uff0c\u5229\u7528\u6e32\u67d3\u8bef\u5dee\u4f5c\u4e3a\u53cd\u9988\u4fe1\u53f7\u6765\u6307\u5bfc\u9ad8\u65af\u66f4\u65b0\uff0c\u5728\u51cf\u5c11\u9ad8\u65af\u6570\u91cf\u548c\u63d0\u5347\u6e32\u67d3\u901f\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u524d\u9988\u9ad8\u65af\u6e85\u5c04\u6a21\u578b\u56e0\u4f9d\u8d56\u5355\u6b21\u524d\u5411\u4f20\u64ad\u800c\u6027\u80fd\u53d7\u9650\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u8fed\u4ee3\u4f18\u53163D\u9ad8\u65af\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u5faa\u73af\u9ad8\u65af\u6e85\u5c04\u6a21\u578b\uff0c\u4f7f\u7528\u6e32\u67d3\u8bef\u5dee\u4f5c\u4e3a\u53cd\u9988\u4fe1\u53f7\u6307\u5bfc\u9ad8\u65af\u66f4\u65b0\uff1b\u5f15\u5165\u7d27\u51d1\u91cd\u5efa\u6a21\u578b\u572816\u500d\u4e0b\u91c7\u6837\u7a7a\u95f4\u521d\u59cb\u5316\u9ad8\u65af\uff0c\u5927\u5e45\u51cf\u5c11\u9ad8\u65af\u6570\u91cf\u3002", "result": "\u5728\u591a\u79cd\u8f93\u5165\u89c6\u56fe\u3001\u5206\u8fa8\u7387\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u9ad8\u65af\u6570\u91cf\u548c\u63d0\u5347\u6e32\u67d3\u901f\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "ReSplat\u901a\u8fc7\u5faa\u73af\u4f18\u5316\u673a\u5236\u548c\u7d27\u51d1\u521d\u59cb\u5316\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u524d\u9988\u9ad8\u65af\u6e85\u5c04\u6a21\u578b\u7684\u6027\u80fd\u9650\u5236\u95ee\u9898\u3002"}}
{"id": "2510.07878", "pdf": "https://arxiv.org/pdf/2510.07878", "abs": "https://arxiv.org/abs/2510.07878", "authors": ["Hamees Sayed", "Pranath Reddy", "Michael W. Toomey", "Sergei Gleyzer"], "title": "FlowLensing: Simulating Gravitational Lensing with Flow Matching", "categories": ["astro-ph.IM", "cs.CV"], "comment": "6 pages, 2 figures, 3 tables", "summary": "Gravitational lensing is one of the most powerful probes of dark matter, yet creating high-fidelity lensed images at scale remains a bottleneck. Existing tools rely on ray-tracing or forward-modeling pipelines that, while precise, are prohibitively slow. We introduce FlowLensing, a Diffusion Transformer-based compact and efficient flow-matching model for strong gravitational lensing simulation. FlowLensing operates in both discrete and continuous regimes, handling classes such as different dark matter models as well as continuous model parameters ensuring physical consistency. By enabling scalable simulations, our model can advance dark matter studies, specifically for probing dark matter substructure in cosmological surveys. We find that our model achieves a speedup of over 200$\\times$ compared to classical simulators for intensive dark matter models, with high fidelity and low inference latency. FlowLensing enables rapid, scalable, and physically consistent image synthesis, offering a practical alternative to traditional forward-modeling pipelines.", "AI": {"tldr": "FlowLensing\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u7684\u7d27\u51d1\u9ad8\u6548\u6d41\u5339\u914d\u6a21\u578b\uff0c\u7528\u4e8e\u5f3a\u5f15\u529b\u900f\u955c\u6a21\u62df\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u52a0\u901f200\u500d\u4ee5\u4e0a\uff0c\u652f\u6301\u79bb\u6563\u548c\u8fde\u7eed\u53c2\u6570\u5904\u7406\uff0c\u786e\u4fdd\u7269\u7406\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u5f15\u529b\u900f\u955c\u6a21\u62df\u5de5\u5177\u4f9d\u8d56\u5149\u7ebf\u8ffd\u8e2a\u6216\u524d\u5411\u5efa\u6a21\u6d41\u7a0b\uff0c\u867d\u7136\u7cbe\u786e\u4f46\u901f\u5ea6\u6781\u6162\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5927\u89c4\u6a21\u6a21\u62df\u9700\u6c42\uff0c\u963b\u788d\u4e86\u6697\u7269\u8d28\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u7684\u6d41\u5339\u914d\u6a21\u578b\uff0c\u5728\u79bb\u6563\u548c\u8fde\u7eed\u4e24\u79cd\u673a\u5236\u4e0b\u8fd0\u884c\uff0c\u5904\u7406\u4e0d\u540c\u6697\u7269\u8d28\u6a21\u578b\u7c7b\u522b\u548c\u8fde\u7eed\u6a21\u578b\u53c2\u6570\uff0c\u786e\u4fdd\u7269\u7406\u4e00\u81f4\u6027\u3002", "result": "\u6a21\u578b\u76f8\u6bd4\u4f20\u7edf\u6a21\u62df\u5668\u5728\u5bc6\u96c6\u6697\u7269\u8d28\u6a21\u578b\u4e0a\u5b9e\u73b0200\u500d\u4ee5\u4e0a\u7684\u52a0\u901f\uff0c\u5177\u6709\u9ad8\u4fdd\u771f\u5ea6\u548c\u4f4e\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "FlowLensing\u80fd\u591f\u5b9e\u73b0\u5feb\u901f\u3001\u53ef\u6269\u5c55\u4e14\u7269\u7406\u4e00\u81f4\u7684\u56fe\u50cf\u5408\u6210\uff0c\u4e3a\u4f20\u7edf\u524d\u5411\u5efa\u6a21\u6d41\u7a0b\u63d0\u4f9b\u4e86\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\uff0c\u53ef\u63a8\u8fdb\u6697\u7269\u8d28\u5b50\u7ed3\u6784\u63a2\u6d4b\u7814\u7a76\u3002"}}
{"id": "2510.07905", "pdf": "https://arxiv.org/pdf/2510.07905", "abs": "https://arxiv.org/abs/2510.07905", "authors": ["Yufei Tong", "Guanjie Cheng", "Peihan Wu", "Yicheng Zhu", "Kexu Lu", "Feiyi Chen", "Meng Xi", "Junqin Huang", "Shuiguang Deng"], "title": "SatFusion: A Unified Framework for Enhancing Satellite IoT Images via Multi-Temporal and Multi-Source Data Fusion", "categories": ["eess.IV", "cs.CV", "cs.MM"], "comment": null, "summary": "With the rapid advancement of the digital society, the proliferation of satellites in the Satellite Internet of Things (Sat-IoT) has led to the continuous accumulation of large-scale multi-temporal and multi-source images across diverse application scenarios. However, existing methods fail to fully exploit the complementary information embedded in both temporal and source dimensions. For example, Multi-Image Super-Resolution (MISR) enhances reconstruction quality by leveraging temporal complementarity across multiple observations, yet the limited fine-grained texture details in input images constrain its performance. Conversely, pansharpening integrates multi-source images by injecting high-frequency spatial information from panchromatic data, but typically relies on pre-interpolated low-resolution inputs and assumes noise-free alignment, making it highly sensitive to noise and misregistration. To address these issues, we propose SatFusion: A Unified Framework for Enhancing Satellite IoT Images via Multi-Temporal and Multi-Source Data Fusion. Specifically, SatFusion first employs a Multi-Temporal Image Fusion (MTIF) module to achieve deep feature alignment with the panchromatic image. Then, a Multi-Source Image Fusion (MSIF) module injects fine-grained texture information from the panchromatic data. Finally, a Fusion Composition module adaptively integrates the complementary advantages of both modalities while dynamically refining spectral consistency, supervised by a weighted combination of multiple loss functions. Extensive experiments on the WorldStrat, WV3, QB, and GF2 datasets demonstrate that SatFusion significantly improves fusion quality, robustness under challenging conditions, and generalizability to real-world Sat-IoT scenarios. The code is available at: https://github.com/dllgyufei/SatFusion.git.", "AI": {"tldr": "SatFusion\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u536b\u661f\u7269\u8054\u7f51\u56fe\u50cf\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u65f6\u76f8\u548c\u591a\u6e90\u6570\u636e\u878d\u5408\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u65f6\u76f8\u548c\u6e90\u7ef4\u5ea6\u7684\u4e92\u8865\u4fe1\u606f\u3002MISR\u65b9\u6cd5\u53d7\u9650\u4e8e\u8f93\u5165\u56fe\u50cf\u7684\u7ec6\u7c92\u5ea6\u7eb9\u7406\u7ec6\u8282\uff0c\u800c\u5168\u8272\u9510\u5316\u65b9\u6cd5\u5bf9\u566a\u58f0\u548c\u914d\u51c6\u8bef\u5dee\u654f\u611f\u3002", "method": "SatFusion\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u591a\u65f6\u76f8\u56fe\u50cf\u878d\u5408\u6a21\u5757\u5b9e\u73b0\u6df1\u5ea6\u7279\u5f81\u5bf9\u9f50\uff0c\u591a\u6e90\u56fe\u50cf\u878d\u5408\u6a21\u5757\u6ce8\u5165\u5168\u8272\u6570\u636e\u7684\u7eb9\u7406\u4fe1\u606f\uff0c\u878d\u5408\u7ec4\u5408\u6a21\u5757\u81ea\u9002\u5e94\u6574\u5408\u4e24\u79cd\u6a21\u6001\u7684\u4f18\u52bf\u5e76\u52a8\u6001\u4f18\u5316\u5149\u8c31\u4e00\u81f4\u6027\u3002", "result": "\u5728WorldStrat\u3001WV3\u3001QB\u548cGF2\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSatFusion\u663e\u8457\u63d0\u5347\u4e86\u878d\u5408\u8d28\u91cf\u3001\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u4ee5\u53ca\u5bf9\u771f\u5b9eSat-IoT\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SatFusion\u901a\u8fc7\u7edf\u4e00\u7684\u591a\u65f6\u76f8\u548c\u591a\u6e90\u6570\u636e\u878d\u5408\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u536b\u661f\u7269\u8054\u7f51\u56fe\u50cf\u589e\u5f3a\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5177\u6709\u4f18\u5f02\u7684\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.08407", "pdf": "https://arxiv.org/pdf/2510.08407", "abs": "https://arxiv.org/abs/2510.08407", "authors": ["Lauren Anderson", "Lucas Chatelain", "Nicolas Tremblay", "Kathryn Grandfield", "David Rousseau", "Aur\u00e9lien Gourrier"], "title": "Biology-driven assessment of deep learning super-resolution imaging of the porosity network in dentin", "categories": ["cs.LG", "cs.CV", "q-bio.TO"], "comment": null, "summary": "The mechanosensory system of teeth is currently believed to partly rely on Odontoblast cells stimulation by fluid flow through a porosity network extending through dentin. Visualizing the smallest sub-microscopic porosity vessels therefore requires the highest achievable resolution from confocal fluorescence microscopy, the current gold standard. This considerably limits the extent of the field of view to very small sample regions. To overcome this limitation, we tested different deep learning (DL) super-resolution (SR) models to allow faster experimental acquisitions of lower resolution images and restore optimal image quality by post-processing. Three supervised 2D SR models (RCAN, pix2pix, FSRCNN) and one unsupervised (CycleGAN) were applied to a unique set of experimentally paired high- and low-resolution confocal images acquired with different sampling schemes, resulting in a pixel size increase of x2, x4, x8. Model performance was quantified using a broad set of similarity and distribution-based image quality assessment (IQA) metrics, which yielded inconsistent results that mostly contradicted our visual perception. This raises the question of the relevance of such generic metrics to efficiently target the specific structure of dental porosity. To resolve this conflicting information, the generated SR images were segmented taking into account the specific scales and morphology of the porosity network and analysed by comparing connected components. Additionally, the capacity of the SR models to preserve 3D porosity connectivity throughout the confocal image stacks was evaluated using graph analysis. This biology-driven assessment allowed a far better mechanistic interpretation of SR performance, highlighting differences in model sensitivity to weak intensity features and the impact of non-linearity in image generation, which explains the failure of standard IQA metrics.", "AI": {"tldr": "\u672c\u7814\u7a76\u6d4b\u8bd5\u4e86\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u8d85\u5206\u8fa8\u7387\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u4f4e\u5206\u8fa8\u7387\u5171\u805a\u7126\u56fe\u50cf\u6062\u590d\u7259\u672c\u8d28\u5b54\u9699\u7f51\u7edc\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u751f\u7269\u5b66\u9a71\u52a8\u7684\u8bc4\u4f30\u65b9\u6cd5\u9a8c\u8bc1\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7259\u672c\u8d28\u5b54\u9699\u7f51\u7edc\u53ef\u89c6\u5316\u53d7\u9650\u4e8e\u5171\u805a\u7126\u663e\u5fae\u955c\u7684\u5206\u8fa8\u7387\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u4ece\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u6062\u590d\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u4ee5\u6269\u5927\u89c6\u91ce\u8303\u56f4\u5e76\u52a0\u5feb\u5b9e\u9a8c\u91c7\u96c6\u901f\u5ea6\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u6709\u76d1\u77632D\u8d85\u5206\u8fa8\u7387\u6a21\u578b(RCAN\u3001pix2pix\u3001FSRCNN)\u548c\u4e00\u79cd\u65e0\u76d1\u7763\u6a21\u578b(CycleGAN)\uff0c\u5bf9\u4e0d\u540c\u91c7\u6837\u65b9\u6848\u83b7\u53d6\u7684\u5b9e\u9a8c\u914d\u5bf9\u9ad8\u4f4e\u5206\u8fa8\u7387\u5171\u805a\u7126\u56fe\u50cf\u8fdb\u884c\u5904\u7406\uff0c\u5b9e\u73b02\u500d\u30014\u500d\u30018\u500d\u7684\u50cf\u7d20\u5c3a\u5bf8\u589e\u52a0\u3002", "result": "\u6807\u51c6\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u4e0e\u89c6\u89c9\u611f\u77e5\u4e0d\u4e00\u81f4\uff0c\u800c\u57fa\u4e8e\u5b54\u9699\u7f51\u7edc\u7ed3\u6784\u548c\u8fde\u901a\u6027\u7684\u751f\u7269\u5b66\u9a71\u52a8\u8bc4\u4f30\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u89e3\u91ca\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u7684\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "\u9488\u5bf9\u7279\u5b9a\u751f\u7269\u7ed3\u6784\u7684\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u8bc4\u4f30\u9700\u8981\u91c7\u7528\u751f\u7269\u5b66\u9a71\u52a8\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u800c\u975e\u901a\u7528\u7684\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\uff0c\u8fd9\u80fd\u66f4\u51c6\u786e\u5730\u53cd\u6620\u6a21\u578b\u5728\u4fdd\u7559\u5173\u952e\u7ed3\u6784\u7279\u5f81\u65b9\u9762\u7684\u80fd\u529b\u3002"}}
{"id": "2510.08568", "pdf": "https://arxiv.org/pdf/2510.08568", "abs": "https://arxiv.org/abs/2510.08568", "authors": ["Hongyu Li", "Lingfeng Sun", "Yafei Hu", "Duy Ta", "Jennifer Barry", "George Konidaris", "Jiahui Fu"], "title": "NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Enabling robots to execute novel manipulation tasks zero-shot is a central goal in robotics. Most existing methods assume in-distribution tasks or rely on fine-tuning with embodiment-matched data, limiting transfer across platforms. We present NovaFlow, an autonomous manipulation framework that converts a task description into an actionable plan for a target robot without any demonstrations. Given a task description, NovaFlow synthesizes a video using a video generation model and distills it into 3D actionable object flow using off-the-shelf perception modules. From the object flow, it computes relative poses for rigid objects and realizes them as robot actions via grasp proposals and trajectory optimization. For deformable objects, this flow serves as a tracking objective for model-based planning with a particle-based dynamics model. By decoupling task understanding from low-level control, NovaFlow naturally transfers across embodiments. We validate on rigid, articulated, and deformable object manipulation tasks using a table-top Franka arm and a Spot quadrupedal mobile robot, and achieve effective zero-shot execution without demonstrations or embodiment-specific training. Project website: https://novaflow.lhy.xyz/.", "AI": {"tldr": "NovaFlow\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u673a\u5668\u4eba\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u9891\u751f\u6210\u548c3D\u7269\u4f53\u6d41\u5206\u6790\uff0c\u5c06\u4efb\u52a1\u63cf\u8ff0\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u8ba1\u5212\uff0c\u65e0\u9700\u6f14\u793a\u6216\u7279\u5b9a\u673a\u5668\u4eba\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u4efb\u52a1\u5206\u5e03\u5185\u6216\u9700\u8981\u7279\u5b9a\u673a\u5668\u4eba\u7684\u5fae\u8c03\u6570\u636e\uff0c\u9650\u5236\u4e86\u8de8\u5e73\u53f0\u8fc1\u79fb\u80fd\u529b\u3002\u76ee\u6807\u662f\u5b9e\u73b0\u673a\u5668\u4eba\u96f6\u6837\u672c\u6267\u884c\u65b0\u4efb\u52a1\u3002", "method": "\u4f7f\u7528\u89c6\u9891\u751f\u6210\u6a21\u578b\u5408\u6210\u4efb\u52a1\u89c6\u9891\uff0c\u901a\u8fc7\u611f\u77e5\u6a21\u5757\u63d0\u53d63D\u7269\u4f53\u6d41\uff0c\u4e3a\u521a\u6027\u7269\u4f53\u8ba1\u7b97\u76f8\u5bf9\u4f4d\u59ff\u5e76\u901a\u8fc7\u6293\u53d6\u63d0\u8bae\u548c\u8f68\u8ff9\u4f18\u5316\u5b9e\u73b0\uff0c\u4e3a\u53ef\u53d8\u5f62\u7269\u4f53\u4f7f\u7528\u57fa\u4e8e\u7c92\u5b50\u7684\u52a8\u529b\u5b66\u6a21\u578b\u8fdb\u884c\u8ddf\u8e2a\u89c4\u5212\u3002", "result": "\u5728\u684c\u9762Franka\u673a\u68b0\u81c2\u548cSpot\u56db\u8db3\u79fb\u52a8\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u521a\u6027\u3001\u5173\u8282\u5f0f\u548c\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u64cd\u4f5c\u4efb\u52a1\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u96f6\u6837\u672c\u6267\u884c\u3002", "conclusion": "\u901a\u8fc7\u5c06\u4efb\u52a1\u7406\u89e3\u4e0e\u5e95\u5c42\u63a7\u5236\u89e3\u8026\uff0cNovaFlow\u80fd\u591f\u81ea\u7136\u5730\u8de8\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\u8fc1\u79fb\uff0c\u65e0\u9700\u6f14\u793a\u6216\u7279\u5b9a\u673a\u5668\u4eba\u8bad\u7ec3\u3002"}}
