{"id": "2509.12279", "pdf": "https://arxiv.org/pdf/2509.12279", "abs": "https://arxiv.org/abs/2509.12279", "authors": ["He Gao", "Baoxiang Huang", "Milena Radenkovic", "Borui Li", "Ge Chen"], "title": "Domain Adaptive SAR Wake Detection: Leveraging Similarity Filtering and Memory Guidance", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Synthetic Aperture Radar (SAR), with its all-weather and wide-area observation capabilities, serves as a crucial tool for wake detection. However, due to its complex imaging mechanism, wake features in SAR images often appear abstract and noisy, posing challenges for accurate annotation. In contrast, optical images provide more distinct visual cues, but models trained on optical data suffer from performance degradation when applied to SAR images due to domain shift. To address this cross-modal domain adaptation challenge, we propose a Similarity-Guided and Memory-Guided Domain Adaptation (termed SimMemDA) framework for unsupervised domain adaptive ship wake detection via instance-level feature similarity filtering and feature memory guidance. Specifically, to alleviate the visual discrepancy between optical and SAR images, we first utilize WakeGAN to perform style transfer on optical images, generating pseudo-images close to the SAR style. Then, instance-level feature similarity filtering mechanism is designed to identify and prioritize source samples with target-like distributions, minimizing negative transfer. Meanwhile, a Feature-Confidence Memory Bank combined with a K-nearest neighbor confidence-weighted fusion strategy is introduced to dynamically calibrate pseudo-labels in the target domain, improving the reliability and stability of pseudo-labels. Finally, the framework further enhances generalization through region-mixed training, strategically combining source annotations with calibrated target pseudo-labels. Experimental results demonstrate that the proposed SimMemDA method can improve the accuracy and robustness of cross-modal ship wake detection tasks, validating the effectiveness and feasibility of the proposed method."}
{"id": "2509.12501", "pdf": "https://arxiv.org/pdf/2509.12501", "abs": "https://arxiv.org/abs/2509.12501", "authors": ["Yao He", "Youngjoong Kwon", "Wenxiao Cai", "Ehsan Adeli"], "title": "Artist-Created Mesh Generation from Raw Observation", "categories": ["cs.CV"], "comment": null, "summary": "We present an end-to-end framework for generating artist-style meshes from noisy or incomplete point clouds, such as those captured by real-world sensors like LiDAR or mobile RGB-D cameras. Artist-created meshes are crucial for commercial graphics pipelines due to their compatibility with animation and texturing tools and their efficiency in rendering. However, existing approaches often assume clean, complete inputs or rely on complex multi-stage pipelines, limiting their applicability in real-world scenarios. To address this, we propose an end-to-end method that refines the input point cloud and directly produces high-quality, artist-style meshes. At the core of our approach is a novel reformulation of 3D point cloud refinement as a 2D inpainting task, enabling the use of powerful generative models. Preliminary results on the ShapeNet dataset demonstrate the promise of our framework in producing clean, complete meshes."}
{"id": "2509.12569", "pdf": "https://arxiv.org/pdf/2509.12569", "abs": "https://arxiv.org/abs/2509.12569", "authors": ["Qi Wang", "Shuliang Zhu", "Jinjia Zhou"], "title": "Adaptive Sampling Scheduler", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 10 figures,2 Tables, 18 Equations", "summary": "Consistent distillation methods have evolved into effective techniques that significantly accelerate the sampling process of diffusion models. Although existing methods have achieved remarkable results, the selection of target timesteps during distillation mainly relies on deterministic or stochastic strategies, which often require sampling schedulers to be designed specifically for different distillation processes. Moreover, this pattern severely limits flexibility, thereby restricting the full sampling potential of diffusion models in practical applications. To overcome these limitations, this paper proposes an adaptive sampling scheduler that is applicable to various consistency distillation frameworks. The scheduler introduces three innovative strategies: (i) dynamic target timestep selection, which adapts to different consistency distillation frameworks by selecting timesteps based on their computed importance; (ii) Optimized alternating sampling along the solution trajectory by guiding forward denoising and backward noise addition based on the proposed time step importance, enabling more effective exploration of the solution space to enhance generation performance; and (iii) Utilization of smoothing clipping and color balancing techniques to achieve stable and high-quality generation results at high guidance scales, thereby expanding the applicability of consistency distillation models in complex generation scenarios. We validated the effectiveness and flexibility of the adaptive sampling scheduler across various consistency distillation methods through comprehensive experimental evaluations. Experimental results consistently demonstrated significant improvements in generative performance, highlighting the strong adaptability achieved by our method."}
{"id": "2509.12627", "pdf": "https://arxiv.org/pdf/2509.12627", "abs": "https://arxiv.org/abs/2509.12627", "authors": ["Pengbo Guo", "Chengxu Liu", "Guoshuai Zhao", "Xingsong Hou", "Jialie Shen", "Xueming Qian"], "title": "Exploring Spectral Characteristics for Single Image Reflection Removal", "categories": ["cs.CV"], "comment": null, "summary": "Eliminating reflections caused by incident light interacting with reflective medium remains an ill-posed problem in the image restoration area. The primary challenge arises from the overlapping of reflection and transmission components in the captured images, which complicates the task of accurately distinguishing and recovering the clean background. Existing approaches typically address reflection removal solely in the image domain, ignoring the spectral property variations of reflected light, which hinders their ability to effectively discern reflections. In this paper, we start with a new perspective on spectral learning, and propose the Spectral Codebook to reconstruct the optical spectrum of the reflection image. The reflections can be effectively distinguished by perceiving the wavelength differences between different light sources in the spectrum. To leverage the reconstructed spectrum, we design two spectral prior refinement modules to re-distribute pixels in the spatial dimension and adaptively enhance the spectral differences along the wavelength dimension. Furthermore, we present the Spectrum-Aware Transformer to jointly recover the transmitted content in spectral and pixel domains. Experimental results on three different reflection benchmarks demonstrate the superiority and generalization ability of our method compared to state-of-the-art models."}
{"id": "2509.12742", "pdf": "https://arxiv.org/pdf/2509.12742", "abs": "https://arxiv.org/abs/2509.12742", "authors": ["Jiateng Liu", "Hao Gao", "Jiu-Cheng Xie", "Chi-Man Pun", "Jian Xiong", "Haolun Li", "Feng Xu"], "title": "Effective Gaussian Management for High-fidelity Object Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "This paper proposes an effective Gaussian management approach for high-fidelity object reconstruction. Departing from recent Gaussian Splatting (GS) methods that employ indiscriminate attribute assignment, our approach introduces a novel densification strategy that dynamically activates spherical harmonics (SHs) or normals under the supervision of a surface reconstruction module, which effectively mitigates the gradient conflicts caused by dual supervision and achieves superior reconstruction results. To further improve representation efficiency, we develop a lightweight Gaussian representation that adaptively adjusts the SH orders of each Gaussian based on gradient magnitudes and performs task-decoupled pruning to remove Gaussian with minimal impact on a reconstruction task without sacrificing others, which balances the representational capacity with parameter quantity. Notably, our management approach is model-agnostic and can be seamlessly integrated into other frameworks, enhancing performance while reducing model size. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art approaches in both reconstruction quality and efficiency, achieving superior performance with significantly fewer parameters."}
{"id": "2509.12763", "pdf": "https://arxiv.org/pdf/2509.12763", "abs": "https://arxiv.org/abs/2509.12763", "authors": ["Yican Zhao", "Ce Wang", "You Hao", "Lei Li", "Tianli Liao"], "title": "DyGLNet: Hybrid Global-Local Feature Fusion with Dynamic Upsampling for Medical Image Segmentation", "categories": ["cs.CV"], "comment": "18pages, under review", "summary": "Medical image segmentation grapples with challenges including multi-scale lesion variability, ill-defined tissue boundaries, and computationally intensive processing demands. This paper proposes the DyGLNet, which achieves efficient and accurate segmentation by fusing global and local features with a dynamic upsampling mechanism. The model innovatively designs a hybrid feature extraction module (SHDCBlock), combining single-head self-attention and multi-scale dilated convolutions to model local details and global context collaboratively. We further introduce a dynamic adaptive upsampling module (DyFusionUp) to realize high-fidelity reconstruction of feature maps based on learnable offsets. Then, a lightweight design is adopted to reduce computational overhead. Experiments on seven public datasets demonstrate that DyGLNet outperforms existing methods, particularly excelling in boundary accuracy and small-object segmentation. Meanwhile, it exhibits lower computation complexity, enabling an efficient and reliable solution for clinical medical image analysis. The code will be made available soon."}
{"id": "2509.12787", "pdf": "https://arxiv.org/pdf/2509.12787", "abs": "https://arxiv.org/abs/2509.12787", "authors": ["Linchun Wu", "Qin Zou", "Xianbiao Qi", "Bo Du", "Zhongyuan Wang", "Qingquan Li"], "title": "Double Helix Diffusion for Cross-Domain Anomaly Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Visual anomaly inspection is critical in manufacturing, yet hampered by the scarcity of real anomaly samples for training robust detectors. Synthetic data generation presents a viable strategy for data augmentation; however, current methods remain constrained by two principal limitations: 1) the generation of anomalies that are structurally inconsistent with the normal background, and 2) the presence of undesirable feature entanglement between synthesized images and their corresponding annotation masks, which undermines the perceptual realism of the output. This paper introduces Double Helix Diffusion (DH-Diff), a novel cross-domain generative framework designed to simultaneously synthesize high-fidelity anomaly images and their pixel-level annotation masks, explicitly addressing these challenges. DH-Diff employs a unique architecture inspired by a double helix, cycling through distinct modules for feature separation, connection, and merging. Specifically, a domain-decoupled attention mechanism mitigates feature entanglement by enhancing image and annotation features independently, and meanwhile a semantic score map alignment module ensures structural authenticity by coherently integrating anomaly foregrounds. DH-Diff offers flexible control via text prompts and optional graphical guidance. Extensive experiments demonstrate that DH-Diff significantly outperforms state-of-the-art methods in diversity and authenticity, leading to significant improvements in downstream anomaly detection performance."}
{"id": "2509.12815", "pdf": "https://arxiv.org/pdf/2509.12815", "abs": "https://arxiv.org/abs/2509.12815", "authors": ["Biwen Lei", "Yang Li", "Xinhai Liu", "Shuhui Yang", "Lixin Xu", "Jingwei Huang", "Ruining Tang", "Haohan Weng", "Jian Liu", "Jing Xu", "Zhen Zhou", "Yiling Zhu", "Jiankai Xing", "Jiachen Xu", "Changfeng Ma", "Xinhao Yan", "Yunhan Yang", "Chunshi Wang", "Duoteng Xu", "Xueqi Ma", "Yuguang Chen", "Jing Li", "Mingxin Yang", "Sheng Zhang", "Yifei Feng", "Xin Huang", "Di Luo", "Zebin He", "Puhua Jiang", "Changrong Hu", "Zihan Qin", "Shiwei Miao", "Haolin Liu", "Yunfei Zhao", "Zeqiang Lai", "Qingxiang Lin", "Zibo Zhao", "Kunhong Li", "Xianghui Yang", "Huiwen Shi", "Xin Yang", "Yuxuan Wang", "Zebin Yao", "Yihang Lian", "Sicong Liu", "Xintong Han", "Wangchen Qin", "Caisheng Ouyang", "Jianyin Liu", "Tianwen Yuan", "Shuai Jiang", "Hong Duan", "Yanqi Niu", "Wencong Lin", "Yifu Sun", "Shirui Huang", "Lin Niu", "Gu Gong", "Guojian Xiao", "Bojian Zheng", "Xiang Yuan", "Qi Chen", "Jie Xiao", "Dongyang Zheng", "Xiaofeng Yang", "Kai Liu", "Jianchen Zhu", "Lifu Wang", "Qinglin Lu", "Jie Liu", "Liang Dong", "Fan Jiang", "Ruibin Chen", "Lei Wang", "Chao Zhang", "Jiaxin Lin", "Hao Zhang", "Zheng Ye", "Peng He", "Runzhou Wu", "Yinhe Wu", "Jiayao Du", "Jupeng Chen", "Xinyue Mao", "Dongyuan Guo", "Yixuan Tang", "Yulin Tsai", "Yonghao Tan", "Jiaao Yu", "Junlin Yu", "Keren Zhang", "Yifan Li", "Peng Chen", "Tian Liu", "Di Wang", "Yuhong Liu", "Linus", "Jie Jiang", "Zhuo Chen", "Chunchao Guo"], "title": "Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation", "categories": ["cs.CV"], "comment": "Technical Report", "summary": "The creation of high-quality 3D assets, a cornerstone of modern game development, has long been characterized by labor-intensive and specialized workflows. This paper presents Hunyuan3D Studio, an end-to-end AI-powered content creation platform designed to revolutionize the game production pipeline by automating and streamlining the generation of game-ready 3D assets. At its core, Hunyuan3D Studio integrates a suite of advanced neural modules (such as Part-level 3D Generation, Polygon Generation, Semantic UV, etc.) into a cohesive and user-friendly system. This unified framework allows for the rapid transformation of a single concept image or textual description into a fully-realized, production-quality 3D model complete with optimized geometry and high-fidelity PBR textures. We demonstrate that assets generated by Hunyuan3D Studio are not only visually compelling but also adhere to the stringent technical requirements of contemporary game engines, significantly reducing iteration time and lowering the barrier to entry for 3D content creation. By providing a seamless bridge from creative intent to technical asset, Hunyuan3D Studio represents a significant leap forward for AI-assisted workflows in game development and interactive media."}
{"id": "2509.12836", "pdf": "https://arxiv.org/pdf/2509.12836", "abs": "https://arxiv.org/abs/2509.12836", "authors": ["Shreyas Shivakumara", "Gabriel Eilertsen", "Karljohan Lundin Palmerius"], "title": "Exploring Metric Fusion for Evaluation of NeRFs", "categories": ["cs.CV"], "comment": "Accepted for 17th International Conference on Quality of Multimedia   Experience (QoMEX 25)", "summary": "Neural Radiance Fields (NeRFs) have demonstrated significant potential in synthesizing novel viewpoints. Evaluating the NeRF-generated outputs, however, remains a challenge due to the unique artifacts they exhibit, and no individual metric performs well across all datasets. We hypothesize that combining two successful metrics, Deep Image Structure and Texture Similarity (DISTS) and Video Multi-Method Assessment Fusion (VMAF), based on different perceptual methods, can overcome the limitations of individual metrics and achieve improved correlation with subjective quality scores. We experiment with two normalization strategies for the individual metrics and two fusion strategies to evaluate their impact on the resulting correlation with the subjective scores. The proposed pipeline is tested on two distinct datasets, Synthetic and Outdoor, and its performance is evaluated across three different configurations. We present a detailed analysis comparing the correlation coefficients of fusion methods and individual scores with subjective scores to demonstrate the robustness and generalizability of the fusion metrics."}
{"id": "2509.12878", "pdf": "https://arxiv.org/pdf/2509.12878", "abs": "https://arxiv.org/abs/2509.12878", "authors": ["Qianguang Zhao", "Dongli Wang", "Yan Zhou", "Jianxun Li", "Richard Irampa"], "title": "Few to Big: Prototype Expansion Network via Diffusion Learner for Point Cloud Few-shot Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Few-shot 3D point cloud semantic segmentation aims to segment novel categories using a minimal number of annotated support samples. While existing prototype-based methods have shown promise, they are constrained by two critical challenges: (1) Intra-class Diversity, where a prototype's limited representational capacity fails to cover a class's full variations, and (2) Inter-set Inconsistency, where prototypes derived from the support set are misaligned with the query feature space. Motivated by the powerful generative capability of diffusion model, we re-purpose its pre-trained conditional encoder to provide a novel source of generalizable features for expanding the prototype's representational range. Under this setup, we introduce the Prototype Expansion Network (PENet), a framework that constructs big-capacity prototypes from two complementary feature sources. PENet employs a dual-stream learner architecture: it retains a conventional fully supervised Intrinsic Learner (IL) to distill representative features, while introducing a novel Diffusion Learner (DL) to provide rich generalizable features. The resulting dual prototypes are then processed by a Prototype Assimilation Module (PAM), which adopts a novel push-pull cross-guidance attention block to iteratively align the prototypes with the query space. Furthermore, a Prototype Calibration Mechanism (PCM) regularizes the final big capacity prototype to prevent semantic drift. Extensive experiments on the S3DIS and ScanNet datasets demonstrate that PENet significantly outperforms state-of-the-art methods across various few-shot settings."}
{"id": "2509.12905", "pdf": "https://arxiv.org/pdf/2509.12905", "abs": "https://arxiv.org/abs/2509.12905", "authors": ["Branko Mitic", "Philipp Seeb\u00f6ck", "Helmut Prosch", "Georg Langs"], "title": "AREPAS: Anomaly Detection in Fine-Grained Anatomy with Reconstruction-Based Semantic Patch-Scoring", "categories": ["cs.CV"], "comment": null, "summary": "Early detection of newly emerging diseases, lesion severity assessment, differentiation of medical conditions and automated screening are examples for the wide applicability and importance of anomaly detection (AD) and unsupervised segmentation in medicine. Normal fine-grained tissue variability such as present in pulmonary anatomy is a major challenge for existing generative AD methods. Here, we propose a novel generative AD approach addressing this issue. It consists of an image-to-image translation for anomaly-free reconstruction and a subsequent patch similarity scoring between observed and generated image-pairs for precise anomaly localization. We validate the new method on chest computed tomography (CT) scans for the detection and segmentation of infectious disease lesions. To assess generalizability, we evaluate the method on an ischemic stroke lesion segmentation task in T1-weighted brain MRI. Results show improved pixel-level anomaly segmentation in both chest CTs and brain MRIs, with relative DICE score improvements of +1.9% and +4.4%, respectively, compared to other state-of-the-art reconstruction-based methods."}
{"id": "2509.12913", "pdf": "https://arxiv.org/pdf/2509.12913", "abs": "https://arxiv.org/abs/2509.12913", "authors": ["Hojat Ardi", "Amir Jahanshahi", "Ali Diba"], "title": "T-SiamTPN: Temporal Siamese Transformer Pyramid Networks for Robust and Efficient UAV Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Aerial object tracking remains a challenging task due to scale variations, dynamic backgrounds, clutter, and frequent occlusions. While most existing trackers emphasize spatial cues, they often overlook temporal dependencies, resulting in limited robustness in long-term tracking and under occlusion. Furthermore, correlation-based Siamese trackers are inherently constrained by the linear nature of correlation operations, making them ineffective against complex, non-linear appearance changes. To address these limitations, we introduce T-SiamTPN, a temporal-aware Siamese tracking framework that extends the SiamTPN architecture with explicit temporal modeling. Our approach incorporates temporal feature fusion and attention-based interactions, strengthening temporal consistency and enabling richer feature representations. These enhancements yield significant improvements over the baseline and achieve performance competitive with state-of-the-art trackers. Crucially, despite the added temporal modules, T-SiamTPN preserves computational efficiency. Deployed on the resource-constrained Jetson Nano, the tracker runs in real time at 7.1 FPS, demonstrating its suitability for real-world embedded applications without notable runtime overhead. Experimental results highlight substantial gains: compared to the baseline, T-SiamTPN improves success rate by 13.7% and precision by 14.7%. These findings underscore the importance of temporal modeling in Siamese tracking frameworks and establish T-SiamTPN as a strong and efficient solution for aerial object tracking. Code is available at: https://github.com/to/be/released"}
{"id": "2509.12931", "pdf": "https://arxiv.org/pdf/2509.12931", "abs": "https://arxiv.org/abs/2509.12931", "authors": ["Xiao Tang", "Guirong Zhuo", "Cong Wang", "Boyuan Zheng", "Minqing Huang", "Lianqing Zheng", "Long Chen", "Shouyi Lu"], "title": "4DRadar-GS: Self-Supervised Dynamic Driving Scene Reconstruction with 4D Radar", "categories": ["cs.CV"], "comment": null, "summary": "3D reconstruction and novel view synthesis are critical for validating autonomous driving systems and training advanced perception models. Recent self-supervised methods have gained significant attention due to their cost-effectiveness and enhanced generalization in scenarios where annotated bounding boxes are unavailable. However, existing approaches, which often rely on frequency-domain decoupling or optical flow, struggle to accurately reconstruct dynamic objects due to imprecise motion estimation and weak temporal consistency, resulting in incomplete or distorted representations of dynamic scene elements. To address these challenges, we propose 4DRadar-GS, a 4D Radar-augmented self-supervised 3D reconstruction framework tailored for dynamic driving scenes. Specifically, we first present a 4D Radar-assisted Gaussian initialization scheme that leverages 4D Radar's velocity and spatial information to segment dynamic objects and recover monocular depth scale, generating accurate Gaussian point representations. In addition, we propose a Velocity-guided PointTrack (VGPT) model, which is jointly trained with the reconstruction pipeline under scene flow supervision, to track fine-grained dynamic trajectories and construct temporally consistent representations. Evaluated on the OmniHD-Scenes dataset, 4DRadar-GS achieves state-of-the-art performance in dynamic driving scene 3D reconstruction."}
{"id": "2509.12938", "pdf": "https://arxiv.org/pdf/2509.12938", "abs": "https://arxiv.org/abs/2509.12938", "authors": ["Abdalla Arafa", "Didier Stricker"], "title": "Beyond Averages: Open-Vocabulary 3D Scene Understanding with Gaussian Splatting and Bag of Embeddings", "categories": ["cs.CV"], "comment": null, "summary": "Novel view synthesis has seen significant advancements with 3D Gaussian Splatting (3DGS), enabling real-time photorealistic rendering. However, the inherent fuzziness of Gaussian Splatting presents challenges for 3D scene understanding, restricting its broader applications in AR/VR and robotics. While recent works attempt to learn semantics via 2D foundation model distillation, they inherit fundamental limitations: alpha blending averages semantics across objects, making 3D-level understanding impossible. We propose a paradigm-shifting alternative that bypasses differentiable rendering for semantics entirely. Our key insight is to leverage predecomposed object-level Gaussians and represent each object through multiview CLIP feature aggregation, creating comprehensive \"bags of embeddings\" that holistically describe objects. This allows: (1) accurate open-vocabulary object retrieval by comparing text queries to object-level (not Gaussian-level) embeddings, and (2) seamless task adaptation: propagating object IDs to pixels for 2D segmentation or to Gaussians for 3D extraction. Experiments demonstrate that our method effectively overcomes the challenges of 3D open-vocabulary object extraction while remaining comparable to state-of-the-art performance in 2D open-vocabulary segmentation, ensuring minimal compromise."}
{"id": "2509.12990", "pdf": "https://arxiv.org/pdf/2509.12990", "abs": "https://arxiv.org/abs/2509.12990", "authors": ["Boyu Han", "Qianqian Xu", "Shilong Bao", "Zhiyong Yang", "Sicong Li", "Qingming Huang"], "title": "Dual-Stage Reweighted MoE for Long-Tailed Egocentric Mistake Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "In this report, we address the problem of determining whether a user performs an action incorrectly from egocentric video data. To handle the challenges posed by subtle and infrequent mistakes, we propose a Dual-Stage Reweighted Mixture-of-Experts (DR-MoE) framework. In the first stage, features are extracted using a frozen ViViT model and a LoRA-tuned ViViT model, which are combined through a feature-level expert module. In the second stage, three classifiers are trained with different objectives: reweighted cross-entropy to mitigate class imbalance, AUC loss to improve ranking under skewed distributions, and label-aware loss with sharpness-aware minimization to enhance calibration and generalization. Their predictions are fused using a classification-level expert module. The proposed method achieves strong performance, particularly in identifying rare and ambiguous mistake instances. The code is available at https://github.com/boyuh/DR-MoE."}
{"id": "2509.13083", "pdf": "https://arxiv.org/pdf/2509.13083", "abs": "https://arxiv.org/abs/2509.13083", "authors": ["Yan Xingyang", "Huang Xiaohong", "Zhang Zhao", "You Tian", "Xu Ziheng"], "title": "Using KL-Divergence to Focus Frequency Information in Low-Light Image Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "In the Fourier domain, luminance information is primarily encoded in the amplitude spectrum, while spatial structures are captured in the phase components. The traditional Fourier Frequency information fitting employs pixel-wise loss functions, which tend to focus excessively on local information and may lead to global information loss. In this paper, we present LLFDisc, a U-shaped deep enhancement network that integrates cross-attention and gating mechanisms tailored for frequency-aware enhancement. We propose a novel distribution-aware loss that directly fits the Fourier-domain information and minimizes their divergence using a closed-form KL-Divergence objective. This enables the model to align Fourier-domain information more robustly than with conventional MSE-based losses. Furthermore, we enhance the perceptual loss based on VGG by embedding KL-Divergence on extracted deep features, enabling better structural fidelity. Extensive experiments across multiple benchmarks demonstrate that LLFDisc achieves state-of-the-art performance in both qualitative and quantitative evaluations. Our code will be released at: https://github.com/YanXY000/LLFDisc"}
{"id": "2509.13149", "pdf": "https://arxiv.org/pdf/2509.13149", "abs": "https://arxiv.org/abs/2509.13149", "authors": ["Minqing Huang", "Shouyi Lu", "Boyuan Zheng", "Ziyao Li", "Xiao Tang", "Guirong Zhuo"], "title": "MSDNet: Efficient 4D Radar Super-Resolution via Multi-Stage Distillation", "categories": ["cs.CV"], "comment": "8 pages, 5 figures", "summary": "4D radar super-resolution, which aims to reconstruct sparse and noisy point clouds into dense and geometrically consistent representations, is a foundational problem in autonomous perception. However, existing methods often suffer from high training cost or rely on complex diffusion-based sampling, resulting in high inference latency and poor generalization, making it difficult to balance accuracy and efficiency. To address these limitations, we propose MSDNet, a multi-stage distillation framework that efficiently transfers dense LiDAR priors to 4D radar features to achieve both high reconstruction quality and computational efficiency. The first stage performs reconstruction-guided feature distillation, aligning and densifying the student's features through feature reconstruction. In the second stage, we propose diffusion-guided feature distillation, which treats the stage-one distilled features as a noisy version of the teacher's representations and refines them via a lightweight diffusion network. Furthermore, we introduce a noise adapter that adaptively aligns the noise level of the feature with a predefined diffusion timestep, enabling a more precise denoising. Extensive experiments on the VoD and in-house datasets demonstrate that MSDNet achieves both high-fidelity reconstruction and low-latency inference in the task of 4D radar point cloud super-resolution, and consistently improves performance on downstream tasks. The code will be publicly available upon publication."}
{"id": "2509.13175", "pdf": "https://arxiv.org/pdf/2509.13175", "abs": "https://arxiv.org/abs/2509.13175", "authors": ["Yingtai Li", "Haoran Lai", "Xiaoqian Zhou", "Shuai Ming", "Wenxin Ma", "Wei Wei", "Shaohua Kevin Zhou"], "title": "More performant and scalable: Rethinking contrastive vision-language pre-training of radiology in the LLM era", "categories": ["cs.CV"], "comment": "MICCAI 2025", "summary": "The emergence of Large Language Models (LLMs) presents unprecedented opportunities to revolutionize medical contrastive vision-language pre-training. In this paper, we show how LLMs can facilitate large-scale supervised pre-training, thereby advancing vision-language alignment. We begin by demonstrate that modern LLMs can automatically extract diagnostic labels from radiology reports with remarkable precision (>96\\% AUC in our experiments) without complex prompt engineering, enabling the creation of large-scale \"silver-standard\" datasets at a minimal cost (~\\$3 for 50k CT image-report pairs). Further, we find that vision encoder trained on this \"silver-standard\" dataset achieves performance comparable to those trained on labels extracted by specialized BERT-based models, thereby democratizing the access to large-scale supervised pre-training. Building on this foundation, we proceed to reveal that supervised pre-training fundamentally improves contrastive vision-language alignment. Our approach achieves state-of-the-art performance using only a 3D ResNet-18 with vanilla CLIP training, including 83.8\\% AUC for zero-shot diagnosis on CT-RATE, 77.3\\% AUC on RAD-ChestCT, and substantial improvements in cross-modal retrieval (MAP@50=53.7\\% for image-image, Recall@100=52.2\\% for report-image). These results demonstrate the potential of utilizing LLMs to facilitate {\\bf more performant and scalable} medical AI systems. Our code is avaiable at https://github.com/SadVoxel/More-performant-and-scalable."}
{"id": "2509.13214", "pdf": "https://arxiv.org/pdf/2509.13214", "abs": "https://arxiv.org/abs/2509.13214", "authors": ["Fei Wang", "Xuecheng Wu", "Zheng Zhang", "Danlei Huang", "Yuheng Huang", "BoWang"], "title": "End4: End-to-end Denoising Diffusion for Diffusion-Based Inpainting Detection", "categories": ["cs.CV"], "comment": null, "summary": "The powerful generative capabilities of diffusion models have significantly advanced the field of image synthesis, enhancing both full image generation and inpainting-based image editing. Despite their remarkable advancements, diffusion models also raise concerns about potential misuse for malicious purposes. However, existing approaches struggle to identify images generated by diffusion-based inpainting models, even when similar inpainted images are included in their training data. To address this challenge, we propose a novel detection method based on End-to-end denoising diffusion (End4). Specifically, End4 designs a denoising reconstruction model to improve the alignment degree between the latent spaces of the reconstruction and detection processes, thus reconstructing features that are more conducive to detection. Meanwhile, it leverages a Scale-aware Pyramid-like Fusion Module (SPFM) that refines local image features under the guidance of attention pyramid layers at different scales, enhancing feature discriminability. Additionally, to evaluate detection performance on inpainted images, we establish a comprehensive benchmark comprising images generated from five distinct masked regions. Extensive experiments demonstrate that our End4 effectively generalizes to unseen masking patterns and remains robust under various perturbations. Our code and dataset will be released soon."}
{"id": "2509.13255", "pdf": "https://arxiv.org/pdf/2509.13255", "abs": "https://arxiv.org/abs/2509.13255", "authors": ["Mattia Soldan", "Fabian Caba Heilbron", "Bernard Ghanem", "Josef Sivic", "Bryan Russell"], "title": "ResidualViT for Efficient Temporally Dense Video Encoding", "categories": ["cs.CV", "cs.AI", "cs.IR", "eess.IV"], "comment": null, "summary": "Several video understanding tasks, such as natural language temporal video grounding, temporal activity localization, and audio description generation, require \"temporally dense\" reasoning over frames sampled at high temporal resolution. However, computing frame-level features for these tasks is computationally expensive given the temporal resolution requirements. In this paper, we make three contributions to reduce the cost of computing features for temporally dense tasks. First, we introduce a vision transformer (ViT) architecture, dubbed ResidualViT, that leverages the large temporal redundancy in videos to efficiently compute temporally dense frame-level features. Our architecture incorporates (i) learnable residual connections that ensure temporal consistency across consecutive frames and (ii) a token reduction module that enhances processing speed by selectively discarding temporally redundant information while reusing weights of a pretrained foundation model. Second, we propose a lightweight distillation strategy to approximate the frame-level features of the original foundation model. Finally, we evaluate our approach across four tasks and five datasets, in both zero-shot and fully supervised settings, demonstrating significant reductions in computational cost (up to 60%) and improvements in inference speed (up to 2.5x faster), all while closely approximating the accuracy of the original foundation model."}
{"id": "2509.13317", "pdf": "https://arxiv.org/pdf/2509.13317", "abs": "https://arxiv.org/abs/2509.13317", "authors": ["An-Chieh Cheng", "Yang Fu", "Yukang Chen", "Zhijian Liu", "Xiaolong Li", "Subhashree Radhakrishnan", "Song Han", "Yao Lu", "Jan Kautz", "Pavlo Molchanov", "Hongxu Yin", "Xiaolong Wang", "Sifei Liu"], "title": "3D Aware Region Prompted Vision Language Model", "categories": ["cs.CV"], "comment": "Project Website: https://www.anjiecheng.me/sr3d", "summary": "We present Spatial Region 3D (SR-3D) aware vision-language model that connects single-view 2D images and multi-view 3D data through a shared visual token space. SR-3D supports flexible region prompting, allowing users to annotate regions with bounding boxes, segmentation masks on any frame, or directly in 3D, without the need for exhaustive multi-frame labeling. We achieve this by enriching 2D visual features with 3D positional embeddings, which allows the 3D model to draw upon strong 2D priors for more accurate spatial reasoning across frames, even when objects of interest do not co-occur within the same view. Extensive experiments on both general 2D vision language and specialized 3D spatial benchmarks demonstrate that SR-3D achieves state-of-the-art performance, underscoring its effectiveness for unifying 2D and 3D representation space on scene understanding. Moreover, we observe applicability to in-the-wild videos without sensory 3D inputs or ground-truth 3D annotations, where SR-3D accurately infers spatial relationships and metric measurements."}
{"id": "2509.12239", "pdf": "https://arxiv.org/pdf/2509.12239", "abs": "https://arxiv.org/abs/2509.12239", "authors": ["Sanyam Jain", "Khuram Naveed", "Illia Oleksiienko", "Alexandros Iosifidis", "Ruben Pauwels"], "title": "InJecteD: Analyzing Trajectories and Drift Dynamics in Denoising Diffusion Probabilistic Models for 2D Point Cloud Generation", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "This work introduces InJecteD, a framework for interpreting Denoising Diffusion Probabilistic Models (DDPMs) by analyzing sample trajectories during the denoising process of 2D point cloud generation. We apply this framework to three datasets from the Datasaurus Dozen bullseye, dino, and circle using a simplified DDPM architecture with customizable input and time embeddings. Our approach quantifies trajectory properties, including displacement, velocity, clustering, and drift field dynamics, using statistical metrics such as Wasserstein distance and cosine similarity. By enhancing model transparency, InJecteD supports human AI collaboration by enabling practitioners to debug and refine generative models. Experiments reveal distinct denoising phases: initial noise exploration, rapid shape formation, and final refinement, with dataset-specific behaviors example, bullseyes concentric convergence vs. dinos complex contour formation. We evaluate four model configurations, varying embeddings and noise schedules, demonstrating that Fourier based embeddings improve trajectory stability and reconstruction quality"}
{"id": "2509.12458", "pdf": "https://arxiv.org/pdf/2509.12458", "abs": "https://arxiv.org/abs/2509.12458", "authors": ["\u00c0lmos Veres-Vit\u00e0lyos", "Genis Castillo Gomez-Raya", "Filip Lemic", "Daniel Johannes Bugelnig", "Bernhard Rinner", "Sergi Abadal", "Xavier Costa-P\u00e9rez"], "title": "Neural 3D Object Reconstruction with Small-Scale Unmanned Aerial Vehicles", "categories": ["cs.RO", "cs.AR", "cs.CV", "cs.ET", "cs.SY", "eess.SY"], "comment": "13 pages, 16 figures, 3 tables, 45 references", "summary": "Small Unmanned Aerial Vehicles (UAVs) exhibit immense potential for navigating indoor and hard-to-reach areas, yet their significant constraints in payload and autonomy have largely prevented their use for complex tasks like high-quality 3-Dimensional (3D) reconstruction. To overcome this challenge, we introduce a novel system architecture that enables fully autonomous, high-fidelity 3D scanning of static objects using UAVs weighing under 100 grams. Our core innovation lies in a dual-reconstruction pipeline that creates a real-time feedback loop between data capture and flight control. A near-real-time (near-RT) process uses Structure from Motion (SfM) to generate an instantaneous pointcloud of the object. The system analyzes the model quality on the fly and dynamically adapts the UAV's trajectory to intelligently capture new images of poorly covered areas. This ensures comprehensive data acquisition. For the final, detailed output, a non-real-time (non-RT) pipeline employs a Neural Radiance Fields (NeRF)-based Neural 3D Reconstruction (N3DR) approach, fusing SfM-derived camera poses with precise Ultra Wide-Band (UWB) location data to achieve superior accuracy. We implemented and validated this architecture using Crazyflie 2.1 UAVs. Our experiments, conducted in both single- and multi-UAV configurations, conclusively show that dynamic trajectory adaptation consistently improves reconstruction quality over static flight paths. This work demonstrates a scalable and autonomous solution that unlocks the potential of miniaturized UAVs for fine-grained 3D reconstruction in constrained environments, a capability previously limited to much larger platforms."}
{"id": "2509.12543", "pdf": "https://arxiv.org/pdf/2509.12543", "abs": "https://arxiv.org/abs/2509.12543", "authors": ["Harshit Rajgarhia", "Shivali Dalmia", "Mengyang Zhao", "Mukherji Abhishek", "Kiran Ganesh"], "title": "Human + AI for Accelerating Ad Localization Evaluation", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Adapting advertisements for multilingual audiences requires more than simple text translation; it demands preservation of visual consistency, spatial alignment, and stylistic integrity across diverse languages and formats. We introduce a structured framework that combines automated components with human oversight to address the complexities of advertisement localization. To the best of our knowledge, this is the first work to integrate scene text detection, inpainting, machine translation (MT), and text reimposition specifically for accelerating ad localization evaluation workflows. Qualitative results across six locales demonstrate that our approach produces semantically accurate and visually coherent localized advertisements, suitable for deployment in real-world workflows."}
{"id": "2509.12728", "pdf": "https://arxiv.org/pdf/2509.12728", "abs": "https://arxiv.org/abs/2509.12728", "authors": ["Jeongsol Kim", "Chanseok Lee", "Jong Chul Ye", "Mooseok Jang"], "title": "Generalizable Holographic Reconstruction via Amplitude-Only Diffusion Priors", "categories": ["physics.optics", "cs.CV", "cs.LG"], "comment": "Keywords: Diffusion model, phase retrieval, inline-holography,   inverse problem", "summary": "Phase retrieval in inline holography is a fundamental yet ill-posed inverse problem due to the nonlinear coupling between amplitude and phase in coherent imaging. We present a novel off-the-shelf solution that leverages a diffusion model trained solely on object amplitude to recover both amplitude and phase from diffraction intensities. Using a predictor-corrector sampling framework with separate likelihood gradients for amplitude and phase, our method enables complex field reconstruction without requiring ground-truth phase data for training. We validate the proposed approach through extensive simulations and experiments, demonstrating robust generalization across diverse object shapes, imaging system configurations, and modalities, including lensless setups. Notably, a diffusion prior trained on simple amplitude data (e.g., polystyrene beads) successfully reconstructs complex biological tissue structures, highlighting the method's adaptability. This framework provides a cost-effective, generalizable solution for nonlinear inverse problems in computational imaging, and establishes a foundation for broader coherent imaging applications beyond holography."}
