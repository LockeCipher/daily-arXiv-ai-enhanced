<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 3]
- [cs.CV](#cs.CV) [Total: 67]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.AI](#cs.AI) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 7]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [MoDA: Multi-modal Diffusion Architecture for Talking Head Generation](https://arxiv.org/abs/2507.03256)
*Xinyang Li,Gen Li,Zhihui Lin,Yichen Qian,GongXin Yao,Weinan Jia,Weihua Chen,Fan Wang*

Main category: cs.GR

TL;DR: MoDA提出了一种基于扩散模型的多模态方法，用于生成具有任意身份和语音音频的逼真说话头部视频，解决了现有方法中的低效推理和视觉伪影问题。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在说话头部生成中的低效推理、视觉伪影以及多模态信息交互不足导致的真实性问题。

Method: 1) 定义联合参数空间连接运动生成与神经渲染，利用流匹配简化扩散学习；2) 引入多模态扩散架构建模噪声运动、音频和辅助条件的交互。

Result: 实验表明，MoDA显著提升了视频的多样性、真实性和效率，适用于实际应用。

Conclusion: MoDA通过联合参数空间和多模态架构，有效解决了扩散模型在说话头部生成中的挑战，提升了生成质量。

Abstract: Talking head generation with arbitrary identities and speech audio remains a crucial problem in the realm of digital humans and the virtual metaverse. Recently, diffusion models have become a popular generative technique in this field with their strong generation and generalization capabilities. However, several challenges remain for diffusion-based methods: 1) inefficient inference and visual artifacts, which arise from the implicit latent space of Variational Auto-Encoders (VAE), complicating the diffusion process; 2) authentic facial expressions and head movements, resulting from insufficient multi-modal information interaction. In this paper, MoDA handle these challenges by 1) defines a joint parameter space to bridge motion generation and neural rendering, and leverages flow matching to simplify the diffusion learning process; 2) introduces a multi-modal diffusion architecture to model the interaction among noisy motion, audio, and auxiliary conditions, ultimately enhancing overall facial expressiveness. Subsequently, a coarse-to-fine fusion strategy is adopted to progressively integrate different modalities, ensuring effective integration across feature spaces. Experimental results demonstrate that MoDA significantly improves video diversity, realism, and efficiency, making it suitable for real-world applications.

</details>


### [2] [F-Hash: Feature-Based Hash Design for Time-Varying Volume Visualization via Multi-Resolution Tesseract Encoding](https://arxiv.org/abs/2507.03836)
*Jianxin Sun,David Lenz,Hongfeng Yu,Tom Peterka*

Main category: cs.GR

TL;DR: F-Hash是一种基于特征的多分辨率Tesseract编码架构，显著提升了时间变化体积数据的建模收敛速度，同时支持特征跟踪和演化可视化。


<details>
  <summary>Details</summary>
Motivation: 时间变化体积数据的交互式可视化因复杂的时空特征和大规模数据集而具有挑战性。现有方法使用隐式神经表示（INR）但训练收敛速度慢。

Method: 提出F-Hash，一种基于特征的多分辨率Tesseract编码架构，结合多级无碰撞哈希函数，高效映射动态4D多分辨率嵌入网格。

Result: F-Hash在多种时间变化体积数据集上实现了最先进的训练收敛速度，并支持特征跟踪和演化可视化。

Conclusion: F-Hash为时间变化体积数据的建模和渲染提供了高效且通用的编码解决方案。

Abstract: Interactive time-varying volume visualization is challenging due to its complex spatiotemporal features and sheer size of the dataset. Recent works transform the original discrete time-varying volumetric data into continuous Implicit Neural Representations (INR) to address the issues of compression, rendering, and super-resolution in both spatial and temporal domains. However, training the INR takes a long time to converge, especially when handling large-scale time-varying volumetric datasets. In this work, we proposed F-Hash, a novel feature-based multi-resolution Tesseract encoding architecture to greatly enhance the convergence speed compared with existing input encoding methods for modeling time-varying volumetric data. The proposed design incorporates multi-level collision-free hash functions that map dynamic 4D multi-resolution embedding grids without bucket waste, achieving high encoding capacity with compact encoding parameters. Our encoding method is agnostic to time-varying feature detection methods, making it a unified encoding solution for feature tracking and evolution visualization. Experiments show the F-Hash achieves state-of-the-art convergence speed in training various time-varying volumetric datasets for diverse features. We also proposed an adaptive ray marching algorithm to optimize the sample streaming for faster rendering of the time-varying neural representation.

</details>


### [3] [A3FR: Agile 3D Gaussian Splatting with Incremental Gaze Tracked Foveated Rendering in Virtual Reality](https://arxiv.org/abs/2507.04147)
*Shuo Xin,Haiyu Wang,Sai Qian Zhang*

Main category: cs.GR

TL;DR: 论文提出了一种名为A3FR的高效渲染框架，通过并行化视线跟踪和注视点渲染过程，显著降低了渲染延迟。


<details>
  <summary>Details</summary>
Motivation: 虚拟现实中的图像渲染对计算资源要求高，而现有的注视点渲染技术虽然能减少渲染成本，但视线跟踪过程本身的计算开销可能抵消其优势，导致延迟增加。

Method: 采用3D高斯泼溅作为渲染算法，并设计A3FR框架，通过并行化视线跟踪和注视点渲染来优化性能。

Result: 评估结果显示，A3FR能将端到端渲染延迟降低至多2倍，同时保持视觉质量。

Conclusion: A3FR框架有效解决了注视点渲染中的延迟问题，为虚拟现实中的实时图像渲染提供了高效解决方案。

Abstract: Virtual reality (VR) significantly transforms immersive digital interfaces, greatly enhancing education, professional practices, and entertainment by increasing user engagement and opening up new possibilities in various industries. Among its numerous applications, image rendering is crucial. Nevertheless, rendering methodologies like 3D Gaussian Splatting impose high computational demands, driven predominantly by user expectations for superior visual quality. This results in notable processing delays for real-time image rendering, which greatly affects the user experience. Additionally, VR devices such as head-mounted displays (HMDs) are intricately linked to human visual behavior, leveraging knowledge from perception and cognition to improve user experience. These insights have spurred the development of foveated rendering, a technique that dynamically adjusts rendering resolution based on the user's gaze direction. The resultant solution, known as gaze-tracked foveated rendering, significantly reduces the computational burden of the rendering process.   Although gaze-tracked foveated rendering can reduce rendering costs, the computational overhead of the gaze tracking process itself can sometimes outweigh the rendering savings, leading to increased processing latency. To address this issue, we propose an efficient rendering framework called~\textit{A3FR}, designed to minimize the latency of gaze-tracked foveated rendering via the parallelization of gaze tracking and foveated rendering processes. For the rendering algorithm, we utilize 3D Gaussian Splatting, a state-of-the-art neural rendering technique. Evaluation results demonstrate that A3FR can reduce end-to-end rendering latency by up to $2\times$ while maintaining visual quality.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [4] [Advancing Talking Head Generation: A Comprehensive Survey of Multi-Modal Methodologies, Datasets, Evaluation Metrics, and Loss Functions](https://arxiv.org/abs/2507.02900)
*Vineet Kumar Rakesh,Soumya Mazumdar,Research Pratim Maity,Sarbajit Pal,Amitabha Das,Tapas Samanta*

Main category: cs.CV

TL;DR: 本文综述了说话头生成（THG）的技术，分类了多种方法，并评估了算法、数据集和指标，同时指出了挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: THG技术在计算机视觉中具有重要意义，应用广泛，但存在技术挑战，需要系统总结和未来方向探讨。

Method: 分类了2D、3D、NeRF、扩散模型等多种方法，并评估了相关算法、数据集和指标。

Result: 总结了THG的进展，指出了依赖预训练模型、极端姿态处理等挑战。

Conclusion: 未来方向包括模块化架构、多语言数据集等，为研究者和从业者提供了实用见解。

Abstract: Talking Head Generation (THG) has emerged as a transformative technology in computer vision, enabling the synthesis of realistic human faces synchronized with image, audio, text, or video inputs. This paper provides a comprehensive review of methodologies and frameworks for talking head generation, categorizing approaches into 2D--based, 3D--based, Neural Radiance Fields (NeRF)--based, diffusion--based, parameter-driven techniques and many other techniques. It evaluates algorithms, datasets, and evaluation metrics while highlighting advancements in perceptual realism and technical efficiency critical for applications such as digital avatars, video dubbing, ultra-low bitrate video conferencing, and online education. The study identifies challenges such as reliance on pre--trained models, extreme pose handling, multilingual synthesis, and temporal consistency. Future directions include modular architectures, multilingual datasets, hybrid models blending pre--trained and task-specific layers, and innovative loss functions. By synthesizing existing research and exploring emerging trends, this paper aims to provide actionable insights for researchers and practitioners in the field of talking head generation. For the complete survey, code, and curated resource list, visit our GitHub repository: https://github.com/VineetKumarRakesh/thg.

</details>


### [5] [Mimesis, Poiesis, and Imagination: Exploring Text-to-Image Generation of Biblical Narratives](https://arxiv.org/abs/2507.02973)
*Willem Th. van Peursen,Samuel E. Entsua-Mensah*

Main category: cs.CV

TL;DR: 研究探讨AI如何通过MidJourney生成《出埃及记》2:5-9的图像，分析其模仿与创造性，并评估其风格、神学和文化维度。


<details>
  <summary>Details</summary>
Motivation: 探索AI在重现或重新想象圣经叙事中的潜力，以及其在神圣艺术中的角色。

Method: 通过比较视觉分析（包括Google图像和古典绘画），评估AI生成图像的风格、神学和文化的表现。

Result: AI能生成美学丰富且富有想象力的图像，但也反映其训练数据的偏见和局限性。

Conclusion: AI可作为重新解读圣经文本的创意伙伴，但其在神圣艺术中的角色仍复杂且有争议。

Abstract: This study explores the intersection of artificial intelligence and the visualization of Biblical narratives by analyzing AI-generated images of Exodus 2:5-9 (Moses found in River Nile) using MidJourney. Drawing on the classical concepts of mimesis (imitation) and poiesis (creative generation), the authors investigate how text-to-image (T2I) models reproduce or reimagine sacred narratives. Through comparative visual analysis, including Google image results and classical paintings, the research evaluates the stylistic, theological, and cultural dimensions of AI-generated depictions. Findings show that while AI excels in producing aesthetically rich and imaginative visuals, it also reflects the biases and limitations of its training data. The study highlights AI's potential to augment human imagination but questions its capacity for genuine creativity, authorial intent, and theological depth. It concludes by suggesting that AI can serve as a creative partner in reinterpreting biblical texts, though its role in sacred art remains complex and contested.

</details>


### [6] [FreqCross: A Multi-Modal Frequency-Spatial Fusion Network for Robust Detection of Stable Diffusion 3.5 Generated Images](https://arxiv.org/abs/2507.02995)
*Guang Yang*

Main category: cs.CV

TL;DR: FreqCross是一种新型多模态融合网络，结合空间RGB特征、频域伪影和径向能量分布模式，用于检测AI生成图像，准确率达97.8%。


<details>
  <summary>Details</summary>
Motivation: 随着Stable Diffusion 3.5等扩散模型的快速发展，生成的合成图像高度逼真，现有检测方法面临挑战。

Method: 采用三分支架构：ResNet-18提取空间特征，轻量CNN处理2D FFT频谱，多层感知机分析径向能量分布。通过特征拼接和分类头实现多模态融合。

Result: 在10,000对真实与合成图像数据集上，FreqCross准确率达97.8%，优于现有方法5.2%。频域分析显示合成图像在0.1--0.4归一化频率范围内具有独特特征。

Conclusion: FreqCross通过多模态融合和频域分析，实现了高效的AI生成图像检测，为相关研究提供了理论基础和实用工具。

Abstract: The rapid advancement of diffusion models, particularly Stable Diffusion 3.5, has enabled the generation of highly photorealistic synthetic images that pose significant challenges to existing detection methods. This paper presents FreqCross, a novel multi-modal fusion network that combines spatial RGB features, frequency domain artifacts, and radial energy distribution patterns to achieve robust detection of AI-generated images. Our approach leverages a three-branch architecture: (1) a ResNet-18 backbone for spatial feature extraction, (2) a lightweight CNN for processing 2D FFT magnitude spectra, and (3) a multi-layer perceptron for analyzing radial energy profiles. We introduce a novel radial energy distribution analysis that captures characteristic frequency artifacts inherent in diffusion-generated images, and fuse it with spatial and spectral cues via simple feature concatenation followed by a compact classification head. Extensive experiments on a dataset of 10,000 paired real (MS-COCO) and synthetic (Stable Diffusion 3.5) images demonstrate that FreqCross achieves 97.8\% accuracy, outperforming state-of-the-art baselines by 5.2\%. The frequency analysis further reveals that synthetic images exhibit distinct spectral signatures in the 0.1--0.4 normalised frequency range, providing theoretical foundation for our approach. Code and pre-trained models are publicly available to facilitate reproducible research.

</details>


### [7] [LATTE: Latent Trajectory Embedding for Diffusion-Generated Image Detection](https://arxiv.org/abs/2507.03054)
*Ana Vasilcoiu,Ivona Najdenkoska,Zeno Geradts,Marcel Worring*

Main category: cs.CV

TL;DR: LATTE提出了一种基于潜在轨迹嵌入的方法，通过建模去噪过程中的潜在嵌入演化，有效区分生成图像与真实图像。


<details>
  <summary>Details</summary>
Motivation: 随着基于扩散的图像生成器快速发展，区分生成图像与真实图像变得困难，亟需开发通用的检测器。

Method: LATTE通过建模潜在嵌入在多步去噪中的轨迹，结合潜在-视觉特征细化模块和轻量级分类器，实现高效检测。

Result: LATTE在多个基准测试（如GenImage和DiffusionFake）上超越基线方法，并在跨生成器和跨数据集场景中表现优异。

Conclusion: LATTE展示了潜在嵌入轨迹在生成图像检测中的潜力，为未来研究提供了新方向。

Abstract: The rapid advancement of diffusion-based image generators has made it increasingly difficult to distinguish generated from real images. This can erode trust in digital media, making it critical to develop generalizable detectors for generated images. Recent methods leverage diffusion denoising cues, but mainly focus on single-step reconstruction errors, ignoring the inherent sequential nature of the denoising process. In this work, we propose LATTE - Latent Trajectory Embedding - a novel approach that models the evolution of latent embeddings across several denoising timesteps. By modeling the trajectory of such embeddings rather than single-step errors, LATTE captures subtle, discriminative patterns that distinguish real from generated images. Each latent is refined by employing our latent-visual feature refinement module and aggregated into a unified representation. Afterwards, it is fused with the visual features and finally passed into a lightweight classifier. Our experiments demonstrate that LATTE surpasses the baselines on several established benchmarks, such as GenImage and DiffusionFake. Moreover, it demonstrates strong performance in cross-generator and cross-datasets settings, highlighting the potential of using the trajectory of latent embeddings for generated image detection. The code is available on the following link: https://github.com/AnaMVasilcoiu/LATTE-Diffusion-Detector.

</details>


### [8] [LACONIC: A 3D Layout Adapter for Controllable Image Creation](https://arxiv.org/abs/2507.03257)
*Léopold Maillard,Tom Durand,Adrien Ramanana Rahary,Maks Ovsjanikov*

Main category: cs.CV

TL;DR: 提出一种新的3D感知生成方法，通过适配器网络增强预训练扩散模型，支持相机控制和3D几何条件，实现场景一致的图像合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖2D控制，难以保持3D几何一致性，限制了场景合成的真实性和丰富性。

Method: 提出条件方法、训练策略和适配器网络，结合预训练扩散模型，支持3D几何条件和相机控制，并考虑场景整体上下文。

Result: 模型轻量、泛化能力强，支持直观的图像编辑（如对象定位、旋转和缩放），丰富了应用场景。

Conclusion: 该方法显著提升了3D感知的图像合成能力，扩展了生成模型的应用范围。

Abstract: Existing generative approaches for guided image synthesis of multi-object scenes typically rely on 2D controls in the image or text space. As a result, these methods struggle to maintain and respect consistent three-dimensional geometric structure, underlying the scene. In this paper, we propose a novel conditioning approach, training method and adapter network that can be plugged into pretrained text-to-image diffusion models. Our approach provides a way to endow such models with 3D-awareness, while leveraging their rich prior knowledge. Our method supports camera control, conditioning on explicit 3D geometries and, for the first time, accounts for the entire context of a scene, i.e., both on and off-screen items, to synthesize plausible and semantically rich images. Despite its multi-modal nature, our model is lightweight, requires a reasonable number of data for supervised learning and shows remarkable generalization power. We also introduce methods for intuitive and consistent image editing and restyling, e.g., by positioning, rotating or resizing individual objects in a scene. Our method integrates well within various image creation workflows and enables a richer set of applications compared to previous approaches.

</details>


### [9] [MolVision: Molecular Property Prediction with Vision Language Models](https://arxiv.org/abs/2507.03283)
*Deepan Adak,Yogesh Singh Rawat,Shruti Vyas*

Main category: cs.CV

TL;DR: MolVision利用视觉语言模型（VLMs）结合分子结构图像和文本描述，提升分子属性预测性能。实验表明，视觉信息与高效微调策略（如LoRA）结合效果显著。


<details>
  <summary>Details</summary>
Motivation: 分子属性预测在药物发现和材料科学中至关重要，但现有方法主要依赖文本表示（如SMILES/SELFIES），缺乏结构信息。

Method: 提出MolVision，整合分子结构图像和文本描述，构建涵盖10个数据集的基准，评估9种VLMs在零样本、少样本和微调设置下的表现。

Result: 视觉信息单独不足，但多模态融合显著提升泛化能力；结合LoRA微调视觉编码器进一步优化性能。

Conclusion: MolVision通过多模态融合和高效微调策略，显著提升了分子属性预测的准确性和泛化能力。

Abstract: Molecular property prediction is a fundamental task in computational chemistry with critical applications in drug discovery and materials science. While recent works have explored Large Language Models (LLMs) for this task, they primarily rely on textual molecular representations such as SMILES/SELFIES, which can be ambiguous and structurally less informative. In this work, we introduce MolVision, a novel approach that leverages Vision-Language Models (VLMs) by integrating both molecular structure as images and textual descriptions to enhance property prediction. We construct a benchmark spanning ten diverse datasets, covering classification, regression and description tasks. Evaluating nine different VLMs in zero-shot, few-shot, and fine-tuned settings, we find that visual information improves prediction performance, particularly when combined with efficient fine-tuning strategies such as LoRA. Our results reveal that while visual information alone is insufficient, multimodal fusion significantly enhances generalization across molecular properties. Adaptation of vision encoder for molecular images in conjunction with LoRA further improves the performance. The code and data is available at : $\href{https://molvision.github.io/MolVision/}{https://molvision.github.io/MolVision/}$.

</details>


### [10] [CPKD: Clinical Prior Knowledge-Constrained Diffusion Models for Surgical Phase Recognition in Endoscopic Submucosal Dissection](https://arxiv.org/abs/2507.03295)
*Xiangning Zhang,Jinnan Chen,Qingwei Zhang,Yaqi Wang,Chengfeng Zhou,Xiaobo Li,Dahong Qian*

Main category: cs.CV

TL;DR: 本文提出了一种名为CPKD的新生成框架，通过去噪扩散原理重新构想手术阶段识别，同时保留核心迭代优化理念。


<details>
  <summary>Details</summary>
Motivation: 胃肠道恶性肿瘤是全球癌症相关死亡的主要原因，而内镜黏膜下剥离术（ESD）的临床应用中，可靠的手术阶段识别是关键瓶颈。

Method: CPKD框架基于去噪扩散原理，逐步从随机噪声重建阶段序列，并结合视觉-时间特征。设计了条件掩码策略以捕捉领域特性，并融入临床先验知识以纠正逻辑错误。

Result: 在ESD820、Cholec80及外部多中心数据集上的评估表明，CPKD性能优于或媲美现有最佳方法。

Conclusion: 扩散生成范式在手术阶段识别中具有有效性，CPKD为复杂内镜工作流提供了可靠解决方案。

Abstract: Gastrointestinal malignancies constitute a leading cause of cancer-related mortality worldwide, with advanced-stage prognosis remaining particularly dismal. Originating as a groundbreaking technique for early gastric cancer treatment, Endoscopic Submucosal Dissection has evolved into a versatile intervention for diverse gastrointestinal lesions. While computer-assisted systems significantly enhance procedural precision and safety in ESD, their clinical adoption faces a critical bottleneck: reliable surgical phase recognition within complex endoscopic workflows. Current state-of-the-art approaches predominantly rely on multi-stage refinement architectures that iteratively optimize temporal predictions. In this paper, we present Clinical Prior Knowledge-Constrained Diffusion (CPKD), a novel generative framework that reimagines phase recognition through denoising diffusion principles while preserving the core iterative refinement philosophy. This architecture progressively reconstructs phase sequences starting from random noise and conditioned on visual-temporal features. To better capture three domain-specific characteristics, including positional priors, boundary ambiguity, and relation dependency, we design a conditional masking strategy. Furthermore, we incorporate clinical prior knowledge into the model training to improve its ability to correct phase logical errors. Comprehensive evaluations on ESD820, Cholec80, and external multi-center demonstrate that our proposed CPKD achieves superior or comparable performance to state-of-the-art approaches, validating the effectiveness of diffusion-based generative paradigms for surgical phase recognition.

</details>


### [11] [Personalized Image Generation from an Author Writing Style](https://arxiv.org/abs/2507.03313)
*Sagar Gandhi,Vishal Gandhi*

Main category: cs.CV

TL;DR: 论文提出了一种将作者写作风格转化为视觉表示的方法，使用AWS和LLM生成文本提示，再通过扩散模型生成图像。评估显示生成图像与作者风格匹配度较高。


<details>
  <summary>Details</summary>
Motivation: 解决将文本定义的作者写作风格转化为视觉表示的挑战，探索生成AI在创作辅助和跨模态理解中的应用。

Method: 利用AWS作为输入，通过LLM生成文本提示，再使用扩散模型（Stable Diffusion）生成图像。

Result: 生成图像与作者风格匹配度较高（平均4.08/5），视觉独特性中等，能捕捉情绪和氛围。

Conclusion: 提出了一种端到端的视觉作者风格个性化方法，并提供了初步实证验证，为创意辅助和跨模态理解开辟了新途径。

Abstract: Translating nuanced, textually-defined authorial writing styles into compelling visual representations presents a novel challenge in generative AI. This paper introduces a pipeline that leverages Author Writing Sheets (AWS) - structured summaries of an author's literary characteristics - as input to a Large Language Model (LLM, Claude 3.7 Sonnet). The LLM interprets the AWS to generate three distinct, descriptive text-to-image prompts, which are then rendered by a diffusion model (Stable Diffusion 3.5 Medium). We evaluated our approach using 49 author styles from Reddit data, with human evaluators assessing the stylistic match and visual distinctiveness of the generated images. Results indicate a good perceived alignment between the generated visuals and the textual authorial profiles (mean style match: $4.08/5$), with images rated as moderately distinctive. Qualitative analysis further highlighted the pipeline's ability to capture mood and atmosphere, while also identifying challenges in representing highly abstract narrative elements. This work contributes a novel end-to-end methodology for visual authorial style personalization and provides an initial empirical validation, opening avenues for applications in creative assistance and cross-modal understanding.

</details>


### [12] [Mirror in the Model: Ad Banner Image Generation via Reflective Multi-LLM and Multi-modal Agents](https://arxiv.org/abs/2507.03326)
*Zhao Wang,Bowen Chen,Yotaro Shimose,Sota Moriyama,Heng Wang,Shingo Takamatsu*

Main category: cs.CV

TL;DR: MIMO框架通过多模态代理系统和协调循环，显著提升了广告横幅的自动生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型（如GPT-4o）在广告横幅设计中无法满足结构化布局、精确排版和品牌一致性等需求。

Method: MIMO结合了分层多模态代理系统（MIMO-Core）和协调循环（MIMO-Loop），通过自然语言提示和标志图像输入，自动检测并纠正生成中的错误。

Result: 实验表明，MIMO在真实横幅设计场景中优于现有基于扩散和LLM的基线方法。

Conclusion: MIMO为广告横幅生成提供了高效且高质量的解决方案。

Abstract: Recent generative models such as GPT-4o have shown strong capabilities in producing high-quality images with accurate text rendering. However, commercial design tasks like advertising banners demand more than visual fidelity -- they require structured layouts, precise typography, consistent branding, and more. In this paper, we introduce MIMO (Mirror In-the-Model), an agentic refinement framework for automatic ad banner generation. MIMO combines a hierarchical multi-modal agent system (MIMO-Core) with a coordination loop (MIMO-Loop) that explores multiple stylistic directions and iteratively improves design quality. Requiring only a simple natural language based prompt and logo image as input, MIMO automatically detects and corrects multiple types of errors during generation. Experiments show that MIMO significantly outperforms existing diffusion and LLM-based baselines in real-world banner design scenarios.

</details>


### [13] [DESign: Dynamic Context-Aware Convolution and Efficient Subnet Regularization for Continuous Sign Language Recognition](https://arxiv.org/abs/2507.03339)
*Sheng Liu,Yiheng Yu,Yuan Feng,Min Xu,Zhelun Jin,Yining Jiang,Tiantian Yuan*

Main category: cs.CV

TL;DR: DESign框架通过动态上下文感知卷积（DCAC）和子网正则化CTC（SR-CTC）提升连续手语识别（CSLR）性能，解决现有方法在时空建模和过拟合上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有CSLR方法难以处理多样样本，且动态卷积仅关注空间建模，忽略时间动态和上下文依赖。

Method: 提出DCAC动态捕捉帧间运动线索并自适应卷积权重，SR-CTC通过子网监督防止CTC过拟合。

Result: 在PHOENIX14等主流数据集上达到SOTA性能。

Conclusion: DESign有效提升CSLR的泛化能力和准确性，且SR-CTC无需额外推理开销。

Abstract: Current continuous sign language recognition (CSLR) methods struggle with handling diverse samples. Although dynamic convolutions are ideal for this task, they mainly focus on spatial modeling and fail to capture the temporal dynamics and contextual dependencies. To address this, we propose DESign, a novel framework that incorporates Dynamic Context-Aware Convolution (DCAC) and Subnet Regularization Connectionist Temporal Classification (SR-CTC). DCAC dynamically captures the inter-frame motion cues that constitute signs and uniquely adapts convolutional weights in a fine-grained manner based on contextual information, enabling the model to better generalize across diverse signing behaviors and boost recognition accuracy. Furthermore, we observe that existing methods still rely on only a limited number of frames for parameter updates during training, indicating that CTC learning overfits to a dominant path. To address this, SR-CTC regularizes training by applying supervision to subnetworks, encouraging the model to explore diverse CTC alignment paths and effectively preventing overfitting. A classifier-sharing strategy in SR-CTC further strengthens multi-scale consistency. Notably, SR-CTC introduces no inference overhead and can be seamlessly integrated into existing CSLR models to boost performance. Extensive ablations and visualizations further validate the effectiveness of the proposed methods. Results on mainstream CSLR datasets (i.e., PHOENIX14, PHOENIX14-T, CSL-Daily) demonstrate that DESign achieves state-of-the-art performance.

</details>


### [14] [Masked Temporal Interpolation Diffusion for Procedure Planning in Instructional Videos](https://arxiv.org/abs/2507.03393)
*Yufan Zhou,Zhaobo Qi,Lingshuai Lin,Junqi Jing,Tingting Chai,Beichen Zhang,Shuhui Wang,Weigang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为MTID的模型，用于解决教学视频中的程序规划问题，通过潜在空间时间插值模块和任务感知掩码机制，生成连贯且任务对齐的动作序列。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖文本级监督，难以捕捉动作间复杂的时序关系，因此需要一种能够增强视觉监督并生成更精准动作序列的模型。

Method: 提出MTID模型，结合潜在空间时间插值模块和动作感知掩码投影机制，利用扩散模型生成中间潜在特征，并通过任务自适应掩码邻近损失优化动作生成。

Result: 在三个基准数据集上的实验表明，MTID在大多数指标上取得了显著的动作规划性能提升。

Conclusion: MTID通过增强视觉监督和任务感知机制，能够生成更连贯且任务对齐的动作序列，为程序规划任务提供了有效解决方案。

Abstract: In this paper, we address the challenge of procedure planning in instructional videos, aiming to generate coherent and task-aligned action sequences from start and end visual observations. Previous work has mainly relied on text-level supervision to bridge the gap between observed states and unobserved actions, but it struggles with capturing intricate temporal relationships among actions. Building on these efforts, we propose the Masked Temporal Interpolation Diffusion (MTID) model that introduces a latent space temporal interpolation module within the diffusion model. This module leverages a learnable interpolation matrix to generate intermediate latent features, thereby augmenting visual supervision with richer mid-state details. By integrating this enriched supervision into the model, we enable end-to-end training tailored to task-specific requirements, significantly enhancing the model's capacity to predict temporally coherent action sequences. Additionally, we introduce an action-aware mask projection mechanism to restrict the action generation space, combined with a task-adaptive masked proximity loss to prioritize more accurate reasoning results close to the given start and end states over those in intermediate steps. Simultaneously, it filters out task-irrelevant action predictions, leading to contextually aware action sequences. Experimental results across three widely used benchmark datasets demonstrate that our MTID achieves promising action planning performance on most metrics. The code is available at https://github.com/WiserZhou/MTID.

</details>


### [15] [Learning Normals of Noisy Points by Local Gradient-Aware Surface Filtering](https://arxiv.org/abs/2507.03394)
*Qing Li,Huifang Feng,Xun Gong,Yu-Shen Liu*

Main category: cs.CV

TL;DR: 提出了一种通过局部梯度感知表面滤波从噪声点云中学习法线的新方法，结合全局表面拟合和局部梯度约束，显著提升了法线估计和去噪性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常针对较干净数据且依赖监督先验，无法有效处理噪声点云的法线估计问题。

Method: 通过隐函数约束局部梯度，利用法线和距离将噪声点投影到潜在表面，结合全局距离测量和局部梯度一致性约束。

Result: 在法线估计、表面重建和点云去噪任务中表现出最先进的性能。

Conclusion: 该方法通过梯度感知滤波和局部约束，有效解决了噪声点云的法线估计问题。

Abstract: Estimating normals for noisy point clouds is a persistent challenge in 3D geometry processing, particularly for end-to-end oriented normal estimation. Existing methods generally address relatively clean data and rely on supervised priors to fit local surfaces within specific neighborhoods. In this paper, we propose a novel approach for learning normals from noisy point clouds through local gradient-aware surface filtering. Our method projects noisy points onto the underlying surface by utilizing normals and distances derived from an implicit function constrained by local gradients. We start by introducing a distance measurement operator for global surface fitting on noisy data, which integrates projected distances along normals. Following this, we develop an implicit field-based filtering approach for surface point construction, adding projection constraints on these points during filtering. To address issues of over-smoothing and gradient degradation, we further incorporate local gradient consistency constraints, as well as local gradient orientation and aggregation. Comprehensive experiments on normal estimation, surface reconstruction, and point cloud denoising demonstrate the state-of-the-art performance of our method. The source code and trained models are available at https://github.com/LeoQLi/LGSF.

</details>


### [16] [Helping CLIP See Both the Forest and the Trees: A Decomposition and Description Approach](https://arxiv.org/abs/2507.03458)
*Leyan Xue,Zongbo Han,Guangyu Wang,Qinghua Hu,Mingyue Cheng,Changqing Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种通过随机多裁剪增强激活CLIP局部特征分析能力的方法，解决了其偏向全局图像模式的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统提示工程依赖粗粒度类别标签，忽视细粒度局部语义，而现有方法假设VLMs能识别局部细节。然而实验发现CLIP对全局模式的偏见限制了其处理局部描述符的能力。

Method: 采用随机多裁剪增强，通过裁剪部分区域约束模型感受野并重新校准注意力机制，从而激活CLIP的局部特征分析能力。

Result: 在零样本、少样本和测试时适应设置下，D&D方法表现出色。

Conclusion: 该方法简单有效，可即插即用，显著提升了CLIP处理局部视觉描述符的能力。

Abstract: Vision-Language Models (VLMs) like CLIP achieve cross-modal semantic alignment through contrastive learning, exhibiting robust zero-shot generalization. Traditional prompt engineering, however, predominantly relies on coarse-grained category labels, neglecting fine-grained local semantics. Existing approaches assume that VLMs inherently recognize localized visual details and attempt to enhance classification by augmenting text prompts with attribute descriptors generated by large language models. However, our systematic experiments reveal critical limitations: CLIP's strong bias toward global image patterns hinders its ability to process localized visual descriptors. To address this fundamental constraint, we propose a simple, effective, and plug-and-play solution that enables CLIP to ``See Both the Forest and the Trees." Specifically, we employ stochastic multi-crop augmentation to activate CLIP's latent capacity for localized feature analysis. By cropping only partial regions, the approach effectively constrains the model's receptive field and recalibrates its attention mechanism, thereby mitigating its inherent bias. We evaluate the proposed method under zero-shot, few-shot, and test-time adaptation settings, and extensive experiments demonstrate that D&D achieves promising performance.

</details>


### [17] [Radar Velocity Transformer: Single-scan Moving Object Segmentation in Noisy Radar Point Clouds](https://arxiv.org/abs/2507.03463)
*Matthias Zeller,Vardeep S. Sandhu,Benedikt Mersch,Jens Behley,Michael Heidingsfeld,Cyrill Stachniss*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的单次雷达扫描移动物体分割方法，利用雷达的多普勒速度信息，无需依赖时序数据，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆需实时感知周围移动物体，但传统方法依赖时序数据，雷达可直接提供速度信息，因此开发单次扫描分割方法。

Method: 提出Radar Velocity Transformer，将速度信息融入网络各模块，并设计基于Transformer的上采样方法，提升稀疏点云的分割性能。

Result: 方法在RadarScenes数据集上表现优异，速度快于传感器帧率，仅需单次扫描即可实现精确分割。

Conclusion: 该方法为雷达移动物体分割提供了高效解决方案，适用于实时自动驾驶场景。

Abstract: The awareness about moving objects in the surroundings of a self-driving vehicle is essential for safe and reliable autonomous navigation. The interpretation of LiDAR and camera data achieves exceptional results but typically requires to accumulate and process temporal sequences of data in order to extract motion information. In contrast, radar sensors, which are already installed in most recent vehicles, can overcome this limitation as they directly provide the Doppler velocity of the detections and, hence incorporate instantaneous motion information within a single measurement. % In this paper, we tackle the problem of moving object segmentation in noisy radar point clouds. We also consider differentiating parked from moving cars, to enhance scene understanding. Instead of exploiting temporal dependencies to identify moving objects, we develop a novel transformer-based approach to perform single-scan moving object segmentation in sparse radar scans accurately. The key to our Radar Velocity Transformer is to incorporate the valuable velocity information throughout each module of the network, thereby enabling the precise segmentation of moving and non-moving objects. Additionally, we propose a transformer-based upsampling, which enhances the performance by adaptively combining information and overcoming the limitation of interpolation of sparse point clouds. Finally, we create a new radar moving object segmentation benchmark based on the RadarScenes dataset and compare our approach to other state-of-the-art methods. Our network runs faster than the frame rate of the sensor and shows superior segmentation results using only single-scan radar data.

</details>


### [18] [Outdoor Monocular SLAM with Global Scale-Consistent 3D Gaussian Pointmaps](https://arxiv.org/abs/2507.03737)
*Chong Cheng,Sicheng Yu,Zijian Wang,Yifan Zhou,Hao Wang*

Main category: cs.CV

TL;DR: S3PO-GS是一种针对户外场景的RGB-only 3D高斯溅射SLAM方法，通过自一致的跟踪模块和基于补丁的动态映射模块，解决了尺度漂移和几何先验缺失问题，提升了跟踪精度和场景重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS SLAM方法在户外场景中缺乏几何先验或存在尺度漂移问题，限制了其性能。

Method: 提出自一致的跟踪模块和基于补丁的动态映射模块，避免尺度漂移并引入几何先验。

Result: 在Waymo、KITTI和DL3DV数据集上，S3PO-GS在新视角合成和跟踪精度上达到最优。

Conclusion: S3PO-GS在复杂户外环境中表现出色，解决了现有方法的局限性。

Abstract: 3D Gaussian Splatting (3DGS) has become a popular solution in SLAM due to its high-fidelity and real-time novel view synthesis performance. However, some previous 3DGS SLAM methods employ a differentiable rendering pipeline for tracking, \textbf{lack geometric priors} in outdoor scenes. Other approaches introduce separate tracking modules, but they accumulate errors with significant camera movement, leading to \textbf{scale drift}. To address these challenges, we propose a robust RGB-only outdoor 3DGS SLAM method: S3PO-GS. Technically, we establish a self-consistent tracking module anchored in the 3DGS pointmap, which avoids cumulative scale drift and achieves more precise and robust tracking with fewer iterations. Additionally, we design a patch-based pointmap dynamic mapping module, which introduces geometric priors while avoiding scale ambiguity. This significantly enhances tracking accuracy and the quality of scene reconstruction, making it particularly suitable for complex outdoor environments. Our experiments on the Waymo, KITTI, and DL3DV datasets demonstrate that S3PO-GS achieves state-of-the-art results in novel view synthesis and outperforms other 3DGS SLAM methods in tracking accuracy. Project page: https://3dagentworld.github.io/S3PO-GS/.

</details>


### [19] [StreamDiT: Real-Time Streaming Text-to-Video Generation](https://arxiv.org/abs/2507.03745)
*Akio Kodaira,Tingbo Hou,Ji Hou,Masayoshi Tomizuka,Yue Zhao*

Main category: cs.CV

TL;DR: StreamDiT是一种基于流匹配和移动缓冲区的实时视频生成模型，通过混合训练和分区方案提升内容一致性和视觉质量，最终在单GPU上实现16 FPS的实时性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成模型通常仅能离线生成短视频，限制了其在交互式和实时应用中的使用。

Method: 提出StreamDiT，基于流匹配和移动缓冲区训练，采用混合训练和分区方案，结合adaLN DiT建模和多步蒸馏方法。

Result: 训练了一个4B参数的模型，通过蒸馏将函数评估次数减少到缓冲区块数，实现16 FPS的实时性能。

Conclusion: StreamDiT支持实时应用，如流生成和交互生成，并通过定量指标和人工评估验证了其有效性。

Abstract: Recently, great progress has been achieved in text-to-video (T2V) generation by scaling transformer-based diffusion models to billions of parameters, which can generate high-quality videos. However, existing models typically produce only short clips offline, restricting their use cases in interactive and real-time applications. This paper addresses these challenges by proposing StreamDiT, a streaming video generation model. StreamDiT training is based on flow matching by adding a moving buffer. We design mixed training with different partitioning schemes of buffered frames to boost both content consistency and visual quality. StreamDiT modeling is based on adaLN DiT with varying time embedding and window attention. To practice the proposed method, we train a StreamDiT model with 4B parameters. In addition, we propose a multistep distillation method tailored for StreamDiT. Sampling distillation is performed in each segment of a chosen partitioning scheme. After distillation, the total number of function evaluations (NFEs) is reduced to the number of chunks in a buffer. Finally, our distilled model reaches real-time performance at 16 FPS on one GPU, which can generate video streams at 512p resolution. We evaluate our method through both quantitative metrics and human evaluation. Our model enables real-time applications, e.g. streaming generation, interactive generation, and video-to-video. We provide video results and more examples in our project website: <a href="https://cumulo-autumn.github.io/StreamDiT/">this https URL.</a>

</details>


### [20] [Interpretable Diffusion Models with B-cos Networks](https://arxiv.org/abs/2507.03846)
*Nicola Bernold,Moritz Vandenhirtz,Alice Bizeul,Julia E. Vogt*

Main category: cs.CV

TL;DR: 提出了一种基于B-cos模块的可解释性扩散模型架构，能够生成高质量图像并揭示提示词对图像的影响。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型难以准确反映提示中的所有语义信息，且缺乏自动检测失败的能力。

Method: 采用B-cos模块构建扩散模型，生成解释性信息，显示每个提示词对图像像素区域的影响。

Result: 模型在生成高质量图像的同时，提供了提示与图像对齐的有意义见解。

Conclusion: B-cos扩散模型兼具生成能力和可解释性，为提示-图像对齐提供了新视角。

Abstract: Text-to-image diffusion models generate images by iteratively denoising random noise, conditioned on a prompt. While these models have enabled impressive progress in image generation, they often fail to accurately reflect all semantic information described in the prompt -- failures that are difficult to detect automatically. In this work, we introduce a diffusion model architecture built with B-cos modules that offers inherent interpretability. Our approach provides insight into how individual prompt tokens affect the generated image by producing explanations that highlight the pixel regions influenced by each token. We demonstrate that B-cos diffusion models can produce high-quality images while providing meaningful insights into prompt-image alignment.

</details>


### [21] [ArmGS: Composite Gaussian Appearance Refinement for Modeling Dynamic Urban Environments](https://arxiv.org/abs/2507.03886)
*Guile Wu,Dongfeng Bai,Bingbing Liu*

Main category: cs.CV

TL;DR: 提出了一种名为ArmGS的新方法，用于动态城市环境建模，通过多粒度外观优化提升自动驾驶场景的渲染效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态城市场景建模中忽略了帧间和视角间的细粒度变化，导致效果不佳。

Method: 采用多级外观建模方案，从局部高斯到全局图像和动态角色级别优化变换参数。

Result: 在多个自动驾驶数据集上表现优于现有方法。

Conclusion: ArmGS方法在动态场景建模中实现了高保真重建和实时渲染。

Abstract: This work focuses on modeling dynamic urban environments for autonomous driving simulation. Contemporary data-driven methods using neural radiance fields have achieved photorealistic driving scene modeling, but they suffer from low rendering efficacy. Recently, some approaches have explored 3D Gaussian splatting for modeling dynamic urban scenes, enabling high-fidelity reconstruction and real-time rendering. However, these approaches often neglect to model fine-grained variations between frames and camera viewpoints, leading to suboptimal results. In this work, we propose a new approach named ArmGS that exploits composite driving Gaussian splatting with multi-granularity appearance refinement for autonomous driving scene modeling. The core idea of our approach is devising a multi-level appearance modeling scheme to optimize a set of transformation parameters for composite Gaussian refinement from multiple granularities, ranging from local Gaussian level to global image level and dynamic actor level. This not only models global scene appearance variations between frames and camera viewpoints, but also models local fine-grained changes of background and objects. Extensive experiments on multiple challenging autonomous driving datasets, namely, Waymo, KITTI, NOTR and VKITTI2, demonstrate the superiority of our approach over the state-of-the-art methods.

</details>


### [22] [Hierarchical Semantic-Visual Fusion of Visible and Near-infrared Images for Long-range Haze Removal](https://arxiv.org/abs/2507.03893)
*Yi Li,Xiaoxiong Wang,Jiawei Wang,Yi Chang,Kai Cao,Luxin Yan*

Main category: cs.CV

TL;DR: 本文提出了一种层次化语义-视觉融合（HSVF）框架，用于解决长距离图像去雾问题，通过结合近红外和可见光模态的优势，实现高对比度和丰富纹理的去雾效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注短距离去雾，而长距离去雾由于散射增强和信号丢失问题被忽视。近红外模态具有更好的雾穿透能力，但现有方法在融合时忽略了可见光图像中的雾残留。

Method: 提出HSVF框架，包含语义流和视觉流。语义流通过模态不变的内在表示重建无雾场景，视觉流从近红外模态恢复结构细节。

Result: 实验表明，HSVF在真实长距离去雾任务中优于现有方法，生成高对比度和丰富纹理的结果。

Conclusion: HSVF通过语义和视觉流的协作，有效解决了长距离去雾问题，并提供了新的数据集支持未来研究。

Abstract: While image dehazing has advanced substantially in the past decade, most efforts have focused on short-range scenarios, leaving long-range haze removal under-explored. As distance increases, intensified scattering leads to severe haze and signal loss, making it impractical to recover distant details solely from visible images. Near-infrared, with superior fog penetration, offers critical complementary cues through multimodal fusion. However, existing methods focus on content integration while often neglecting haze embedded in visible images, leading to results with residual haze. In this work, we argue that the infrared and visible modalities not only provide complementary low-level visual features, but also share high-level semantic consistency. Motivated by this, we propose a Hierarchical Semantic-Visual Fusion (HSVF) framework, comprising a semantic stream to reconstruct haze-free scenes and a visual stream to incorporate structural details from the near-infrared modality. The semantic stream first acquires haze-robust semantic prediction by aligning modality-invariant intrinsic representations. Then the shared semantics act as strong priors to restore clear and high-contrast distant scenes under severe haze degradation. In parallel, the visual stream focuses on recovering lost structural details from near-infrared by fusing complementary cues from both visible and near-infrared images. Through the cooperation of dual streams, HSVF produces results that exhibit both high-contrast scenes and rich texture details. Moreover, we introduce a novel pixel-aligned visible-infrared haze dataset with semantic labels to facilitate benchmarking. Extensive experiments demonstrate the superiority of our method over state-of-the-art approaches in real-world long-range haze removal.

</details>


### [23] [EchoMimicV3: 1.3B Parameters are All You Need for Unified Multi-Modal and Multi-Task Human Animation](https://arxiv.org/abs/2507.03905)
*Rang Meng,Yan Wang,Weipeng Wu,Ruobing Zheng,Yuming Li,Chenguang Ma*

Main category: cs.CV

TL;DR: 提出了一种统一的多任务范式EchoMimicV3，通过空间-时间局部重建处理多样化生成任务，结合多模态解耦交叉注意力模块和SFT+Reward交替训练范式，实现了高效、高质量、多功能的数字人生成。


<details>
  <summary>Details</summary>
Motivation: 解决大规模视频生成模型在推理速度、计算成本和任务多样性上的挑战，探索如何实现更快、更高质、更强泛化且多任务统一的人类动画生成。

Method: 1. 将多样化生成任务视为空间-时间局部重建；2. 引入多模态解耦交叉注意力模块；3. 提出SFT+Reward交替训练范式。

Result: EchoMimicV3在面部和半身视频生成中表现优异，生成质量媲美参数量大10倍的模型，并提供精确的文本控制。

Conclusion: 通过创新方法，实现了高效、高质量、多功能的数字人生成，解决了性能和实用性的双重挑战。

Abstract: Human animation recently has advanced rapidly, achieving increasingly realistic and vivid results, especially with the integration of large-scale video generation models. However, the slow inference speed and high computational cost of these large models bring significant challenges for practical applications. Additionally, various tasks in human animation, such as lip-syncing, audio-driven full-body animation, and video generation from start and end frames, often require different specialized models. The introduction of large video models has not alleviated this dilemma. This raises an important question: Can we make human animation Faster, Higher in quality, Stronger in generalization, and make various tasks Together in one model? To address this, we dive into video generation models and discover that the devil lies in the details: Inspired by MAE, we propose a novel unified Multi-Task paradigm for human animation, treating diverse generation tasks as spatial-temporal local reconstructions, requiring modifications only on the input side; Given the interplay and division among multi-modal conditions including text, image, and audio, we introduce a multi-modal decoupled cross-attention module to fuse multi-modals in a divide-and-conquer manner; We propose a new SFT+Reward alternating training paradigm, enabling the minimal model with 1.3B parameters to achieve generation quality comparable to models with 10 times the parameters count. Through these innovations, our work paves the way for efficient, high-quality, and versatile digital human generation, addressing both performance and practicality challenges in the field. Extensive experiments demonstrate that EchoMimicV3 outperforms existing models in both facial and semi-body video generation, providing precise text-based control for creating videos in a wide range of scenarios.

</details>


### [24] [DNF-Intrinsic: Deterministic Noise-Free Diffusion for Indoor Inverse Rendering](https://arxiv.org/abs/2507.03924)
*Rongjia Zheng,Qing Zhang,Chengjiang Long,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: DNF-Intrinsic提出了一种基于预训练扩散模型的逆渲染方法，通过直接使用源图像而非噪声输入，提升了生成结果的鲁棒性和质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用噪声图像进行逆渲染，导致结构信息退化，影响结果质量。DNF-Intrinsic旨在解决这一问题。

Method: 采用源图像作为输入，通过流匹配直接预测确定性本征属性，并设计生成渲染器确保物理真实性。

Result: 在合成和真实数据集上，DNF-Intrinsic明显优于现有方法。

Conclusion: DNF-Intrinsic通过改进输入和约束机制，显著提升了逆渲染的质量和鲁棒性。

Abstract: Recent methods have shown that pre-trained diffusion models can be fine-tuned to enable generative inverse rendering by learning image-conditioned noise-to-intrinsic mapping. Despite their remarkable progress, they struggle to robustly produce high-quality results as the noise-to-intrinsic paradigm essentially utilizes noisy images with deteriorated structure and appearance for intrinsic prediction, while it is common knowledge that structure and appearance information in an image are crucial for inverse rendering. To address this issue, we present DNF-Intrinsic, a robust yet efficient inverse rendering approach fine-tuned from a pre-trained diffusion model, where we propose to take the source image rather than Gaussian noise as input to directly predict deterministic intrinsic properties via flow matching. Moreover, we design a generative renderer to constrain that the predicted intrinsic properties are physically faithful to the source image. Experiments on both synthetic and real-world datasets show that our method clearly outperforms existing state-of-the-art methods.

</details>


### [25] [Evaluating Adversarial Protections for Diffusion Personalization: A Comprehensive Study](https://arxiv.org/abs/2507.03953)
*Kai Ye,Tianyi Chen,Zhen Wang*

Main category: cs.CV

TL;DR: 本文对八种基于扰动的保护方法（AdvDM、ASPL、FSGM、MetaCloak、Mist、PhotoGuard、SDS和SimAC）在肖像和艺术作品领域进行了全面比较，评估了视觉不可感知性和保护效果，为方法选择提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型在图像生成和个性化中的广泛应用，隐私泄露和内容滥用问题日益突出，因此需要评估和比较现有的保护方法。

Method: 研究比较了八种基于扰动的保护方法，在不同扰动预算下，使用多种指标评估视觉不可感知性和保护效果。

Result: 研究结果为方法选择提供了实用指导，并公开了代码。

Conclusion: 该研究为扩散模型中的隐私保护方法选择提供了重要参考，并促进了相关领域的进一步发展。

Abstract: With the increasing adoption of diffusion models for image generation and personalization, concerns regarding privacy breaches and content misuse have become more pressing. In this study, we conduct a comprehensive comparison of eight perturbation based protection methods: AdvDM, ASPL, FSGM, MetaCloak, Mist, PhotoGuard, SDS, and SimAC--across both portrait and artwork domains. These methods are evaluated under varying perturbation budgets, using a range of metrics to assess visual imperceptibility and protective efficacy. Our results offer practical guidance for method selection. Code is available at: https://github.com/vkeilo/DiffAdvPerturbationBench.

</details>


### [26] [Robust Low-light Scene Restoration via Illumination Transition](https://arxiv.org/abs/2507.03976)
*Ze Li,Feng Zhang,Xiatian Zhu,Meng Zhang,Yanghong Zhou,P. Y. Mok*

Main category: cs.CV

TL;DR: 提出了一种名为RoSe的框架，用于从低光多视角图像合成正常光照的新视角图像，解决了现有方法在预处理和去噪方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有低光增强方法未能有效处理多视角相关性，且其他先进方法存在色彩失真和去噪效果有限的问题。

Method: 通过将任务建模为3D空间中的光照过渡估计问题，利用低秩特性约束过渡表示，设计了双分支架构和低秩去噪模块。

Result: 实验表明，RoSe在渲染质量和多视角一致性上显著优于现有方法。

Conclusion: RoSe框架有效解决了低光多视角图像合成问题，提供了高质量的渲染结果。

Abstract: Synthesizing normal-light novel views from low-light multiview images is an important yet challenging task, given the low visibility and high ISO noise present in the input images. Existing low-light enhancement methods often struggle to effectively preprocess such low-light inputs, as they fail to consider correlations among multiple views. Although other state-of-the-art methods have introduced illumination-related components offering alternative solutions to the problem, they often result in drawbacks such as color distortions and artifacts, and they provide limited denoising effectiveness. In this paper, we propose a novel Robust Low-light Scene Restoration framework (RoSe), which enables effective synthesis of novel views in normal lighting conditions from low-light multiview image inputs, by formulating the task as an illuminance transition estimation problem in 3D space, conceptualizing it as a specialized rendering task. This multiview-consistent illuminance transition field establishes a robust connection between low-light and normal-light conditions. By further exploiting the inherent low-rank property of illumination to constrain the transition representation, we achieve more effective denoising without complex 2D techniques or explicit noise modeling. To implement RoSe, we design a concise dual-branch architecture and introduce a low-rank denoising module. Experiments demonstrate that RoSe significantly outperforms state-of-the-art models in both rendering quality and multiview consistency on standard benchmarks. The codes and data are available at https://pegasus2004.github.io/RoSe.

</details>


### [27] [Flux-Sculptor: Text-Driven Rich-Attribute Portrait Editing through Decomposed Spatial Flow Control](https://arxiv.org/abs/2507.03979)
*Tianyao He,Runqi Wang,Yang Chen,Dejia Song,Nemo Chen,Xu Tang,Yao Hu*

Main category: cs.CV

TL;DR: Flux-Sculptor是一个基于通量的框架，用于精确的文本驱动肖像编辑，通过Prompt-Aligned Spatial Locator和Structure-to-Detail Edit Control策略实现高精度编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法在重建保真度和编辑灵活性之间难以平衡，因此需要一种更精确的文本驱动肖像编辑方法。

Method: 提出Flux-Sculptor框架，包含Prompt-Aligned Spatial Locator（PASL）和Structure-to-Detail Edit Control（S2D-EC）策略，通过潜在表示和注意力值的序列掩码引导融合实现编辑。

Result: 实验表明，Flux-Sculptor在丰富属性编辑和面部信息保留方面优于现有方法。

Conclusion: Flux-Sculptor是一个实用的肖像编辑工具，具有高精度和灵活性。

Abstract: Text-driven portrait editing holds significant potential for various applications but also presents considerable challenges. An ideal text-driven portrait editing approach should achieve precise localization and appropriate content modification, yet existing methods struggle to balance reconstruction fidelity and editing flexibility. To address this issue, we propose Flux-Sculptor, a flux-based framework designed for precise text-driven portrait editing. Our framework introduces a Prompt-Aligned Spatial Locator (PASL) to accurately identify relevant editing regions and a Structure-to-Detail Edit Control (S2D-EC) strategy to spatially guide the denoising process through sequential mask-guided fusion of latent representations and attention values. Extensive experiments demonstrate that Flux-Sculptor surpasses existing methods in rich-attribute editing and facial information preservation, making it a strong candidate for practical portrait editing applications. Project page is available at https://flux-sculptor.github.io/.

</details>


### [28] [PresentAgent: Multimodal Agent for Presentation Video Generation](https://arxiv.org/abs/2507.04036)
*Jingwei Shi,Zeyu Zhang,Biao Wu,Yanjie Liang,Meng Fang,Ling Chen,Yang Zhao*

Main category: cs.CV

TL;DR: PresentAgent是一个多模态代理，将长文档转换为带旁白的演示视频，超越静态幻灯片或文本摘要的限制，实现视听同步的人类风格演示。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅能生成静态幻灯片或文本摘要，无法满足动态、视听同步的演示需求。

Method: 采用模块化流程，包括文档分段、幻灯片视觉帧规划与渲染、上下文旁白生成（使用大语言模型和TTS模型），以及精确的视听对齐合成。

Result: 在30个文档-演示对的数据集上，PresentAgent接近人类水平的质量，评估指标包括内容保真度、视觉清晰度和受众理解度。

Conclusion: 可控多模态代理在将静态文本转换为动态、高效且易访问的演示格式方面具有巨大潜力。

Abstract: We present PresentAgent, a multimodal agent that transforms long-form documents into narrated presentation videos. While existing approaches are limited to generating static slides or text summaries, our method advances beyond these limitations by producing fully synchronized visual and spoken content that closely mimics human-style presentations. To achieve this integration, PresentAgent employs a modular pipeline that systematically segments the input document, plans and renders slide-style visual frames, generates contextual spoken narration with large language models and Text-to-Speech models, and seamlessly composes the final video with precise audio-visual alignment. Given the complexity of evaluating such multimodal outputs, we introduce PresentEval, a unified assessment framework powered by Vision-Language Models that comprehensively scores videos across three critical dimensions: content fidelity, visual clarity, and audience comprehension through prompt-based evaluation. Our experimental validation on a curated dataset of 30 document-presentation pairs demonstrates that PresentAgent approaches human-level quality across all evaluation metrics. These results highlight the significant potential of controllable multimodal agents in transforming static textual materials into dynamic, effective, and accessible presentation formats. Code will be available at https://github.com/AIGeeksGroup/PresentAgent.

</details>


### [29] [Consistent and Invariant Generalization Learning for Short-video Misinformation Detection](https://arxiv.org/abs/2507.04061)
*Hanghui Guo,Weijie Shi,Mengze Li,Juncheng Li,Hao Chen,Yue Cui,Jiajie Xu,Jia Zhu,Jiawei Shen,Zhangze Chen,Sirui Han*

Main category: cs.CV

TL;DR: 论文提出DOCTOR模型，通过跨模态一致性学习和不变性学习，提升短视频虚假信息检测的领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前模型在特定领域训练后，在未见领域表现不佳，主要由于领域差距和模态依赖问题。

Method: DOCTOR模型包含跨模态特征插值和插值蒸馏模块，以及扩散模型增强领域不变特征。

Result: 实验证明DOCTOR模型在跨领域短视频虚假信息检测中表现优异。

Conclusion: DOCTOR通过多模态一致性学习和特征增强，有效解决了领域泛化问题。

Abstract: Short-video misinformation detection has attracted wide attention in the multi-modal domain, aiming to accurately identify the misinformation in the video format accompanied by the corresponding audio. Despite significant advancements, current models in this field, trained on particular domains (source domains), often exhibit unsatisfactory performance on unseen domains (target domains) due to domain gaps. To effectively realize such domain generalization on the short-video misinformation detection task, we propose deep insights into the characteristics of different domains: (1) The detection on various domains may mainly rely on different modalities (i.e., mainly focusing on videos or audios). To enhance domain generalization, it is crucial to achieve optimal model performance on all modalities simultaneously. (2) For some domains focusing on cross-modal joint fraud, a comprehensive analysis relying on cross-modal fusion is necessary. However, domain biases located in each modality (especially in each frame of videos) will be accumulated in this fusion process, which may seriously damage the final identification of misinformation. To address these issues, we propose a new DOmain generalization model via ConsisTency and invariance learning for shORt-video misinformation detection (named DOCTOR), which contains two characteristic modules: (1) We involve the cross-modal feature interpolation to map multiple modalities into a shared space and the interpolation distillation to synchronize multi-modal learning; (2) We design the diffusion model to add noise to retain core features of multi modal and enhance domain invariant features through cross-modal guided denoising. Extensive experiments demonstrate the effectiveness of our proposed DOCTOR model. Our code is public available at https://github.com/ghh1125/DOCTOR.

</details>


### [30] [PromptSR: Cascade Prompting for Lightweight Image Super-Resolution](https://arxiv.org/abs/2507.04118)
*Wenyang Liu,Chen Cai,Jianjun Gao,Kejun Wu,Yi Wang,Kim-Hui Yap,Lap-Pui Chau*

Main category: cs.CV

TL;DR: PromptSR提出了一种轻量级图像超分辨率方法，通过级联提示块（CPB）结合全局和局部信息，扩大感受野并保持低计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决基于窗口的自注意力模型在轻量级Vision Transformer中感受野受限的问题，同时避免计算复杂度的增加。

Method: 提出CPB模块，包含全局锚点提示层（GAPL）和两个局部提示层（LPLs），通过跨尺度注意力构建低维锚点提示，结合全局和局部细化。

Result: 实验结果表明，PromptSR在定量、定性和复杂度评估上优于现有轻量级超分辨率方法。

Conclusion: PromptSR通过结合全局先验和局部细节，有效扩大了感受野并保持低计算成本，具有优越性能。

Abstract: Although the lightweight Vision Transformer has significantly advanced image super-resolution (SR), it faces the inherent challenge of a limited receptive field due to the window-based self-attention modeling. The quadratic computational complexity relative to window size restricts its ability to use a large window size for expanding the receptive field while maintaining low computational costs. To address this challenge, we propose PromptSR, a novel prompt-empowered lightweight image SR method. The core component is the proposed cascade prompting block (CPB), which enhances global information access and local refinement via three cascaded prompting layers: a global anchor prompting layer (GAPL) and two local prompting layers (LPLs). The GAPL leverages downscaled features as anchors to construct low-dimensional anchor prompts (APs) through cross-scale attention, significantly reducing computational costs. These APs, with enhanced global perception, are then used to provide global prompts, efficiently facilitating long-range token connections. The two LPLs subsequently combine category-based self-attention and window-based self-attention to refine the representation in a coarse-to-fine manner. They leverage attention maps from the GAPL as additional global prompts, enabling them to perceive features globally at different granularities for adaptive local refinement. In this way, the proposed CPB effectively combines global priors and local details, significantly enlarging the receptive field while maintaining the low computational costs of our PromptSR. The experimental results demonstrate the superiority of our method, which outperforms state-of-the-art lightweight SR methods in quantitative, qualitative, and complexity evaluations. Our code will be released at https://github.com/wenyang001/PromptSR.

</details>


### [31] [Pedestrian Intention Prediction via Vision-Language Foundation Models](https://arxiv.org/abs/2507.04141)
*Mohsen Azarmi,Mahdi Rezaei,He Wang*

Main category: cs.CV

TL;DR: 该研究利用视觉-语言基础模型（VLFMs）通过分层提示模板整合多模态数据，显著提升了行人过马路意图预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统基于视觉的行人过马路意图预测方法在泛化性、上下文理解和因果推理方面存在不足，研究探索VLFMs的潜力以解决这些问题。

Method: 通过分层提示模板整合视觉帧、物理线索观察和自车动态等多模态数据，并利用自动提示工程框架优化提示。

Result: 实验表明，结合车速及其时间变化以及时间敏感的提示，预测准确性提升19.8%；优化提示进一步带来12.5%的准确性提升。

Conclusion: VLFMs在行人过马路意图预测中表现优于传统视觉模型，为自动驾驶应用提供了更好的泛化能力和上下文理解。

Abstract: Prediction of pedestrian crossing intention is a critical function in autonomous vehicles. Conventional vision-based methods of crossing intention prediction often struggle with generalizability, context understanding, and causal reasoning. This study explores the potential of vision-language foundation models (VLFMs) for predicting pedestrian crossing intentions by integrating multimodal data through hierarchical prompt templates. The methodology incorporates contextual information, including visual frames, physical cues observations, and ego-vehicle dynamics, into systematically refined prompts to guide VLFMs effectively in intention prediction. Experiments were conducted on three common datasets-JAAD, PIE, and FU-PIP. Results demonstrate that incorporating vehicle speed, its variations over time, and time-conscious prompts significantly enhances the prediction accuracy up to 19.8%. Additionally, optimised prompts generated via an automatic prompt engineering framework yielded 12.5% further accuracy gains. These findings highlight the superior performance of VLFMs compared to conventional vision-based models, offering enhanced generalisation and contextual understanding for autonomous driving applications.

</details>


### [32] [Unlocking Compositional Control: Self-Supervision for LVLM-Based Image Generation](https://arxiv.org/abs/2507.04151)
*Fernando Gabriela Garcia,Spencer Burns,Ryan Shaw,Hunter Young*

Main category: cs.CV

TL;DR: Hi-SSLVLM是一种新型生成模型，通过两阶段自监督学习策略提升复杂文本到图像合成的性能，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖昂贵的人工标注数据集，且难以精确控制细粒度视觉属性和复杂空间关系。Hi-SSLVLM旨在解决这些问题。

Method: 采用两阶段策略：1) 多粒度视觉-语言对齐，自主生成层次化标题；2) 自优化和引导图像生成，结合内部组合规划机制和语义一致性损失。

Result: 在多个基准测试中表现优于Janus-Pro-1B、Stable Diffusion XL等，人类评估也证实其更高的生成质量和语义一致性。

Conclusion: Hi-SSLVLM在可控性和语义一致性方面取得显著进展，为开放式文本到图像生成提供了新方向。

Abstract: This paper introduces Hierarchical Self-Supervised LVLM (Hi-SSLVLM), a novel generative model designed to significantly advance text-to-image synthesis, particularly for complex and compositionally challenging prompts. Traditional methods often grapple with the high cost of meticulously curated paired image-text datasets and struggle with precise control over fine-grained visual attributes and intricate spatial relationships. Our Hi-SSLVLM addresses these limitations through a unique two-stage self-supervised learning strategy. The first stage, Multi-Granularity Visual-Language Grounding, enables the Large Vision-Language Model (LVLM) backbone to autonomously generate and align hierarchical captions (global and local) to images, cultivating a deep internal semantic understanding without reliance on extensive human annotation. The second stage, Self-Refinement and Guided Image Generation, leverages this acquired knowledge by an Internal Compositional Planning (ICP) mechanism, where the LVLM first formulates detailed textual sub-prompts to guide the image generation process, complemented by a novel Semantic Consistency Loss for precise output alignment. Comprehensive experiments against leading baselines, including Janus-Pro-1B, Stable Diffusion XL 1.0, DeepFloyd IF v1.0, and ControlNet-XL, on multi-dimensional benchmarks such as Gemini-2.0-Flash and InternVL3-78B, demonstrate Hi-SSLVLM's superior performance across all fine-grained metrics. An in-depth ablation study confirms the critical role of each proposed component. Furthermore, human evaluations corroborate our quantitative findings, highlighting Hi-SSLVLM's enhanced fidelity to prompt, compositional accuracy, and overall aesthetic quality, marking a significant step towards more controllable and semantically consistent open-ended text-to-image generation.

</details>


### [33] [LVLM-Composer's Explicit Planning for Image Generation](https://arxiv.org/abs/2507.04152)
*Spencer Ramsey,Jeffrey Lee,Amina Grant*

Main category: cs.CV

TL;DR: LVLM-Composer是一种新型的大规模视觉语言模型，通过分层语义规划和细粒度特征对齐机制，显著提升了复杂文本描述的图像生成能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型在复杂文本描述的图像生成中存在局限性，如多对象、属性和空间关系的准确渲染。

Method: 提出LVLM-Composer，采用分层语义规划模块和细粒度特征对齐机制，结合多阶段训练范式。

Result: 在LongBench-T2I基准测试中表现优异，显著优于现有方法。

Conclusion: LVLM-Composer为可控且准确的开放式文本到图像生成迈出了重要一步。

Abstract: The burgeoning field of generative artificial intelligence has fundamentally reshaped our approach to content creation, with Large Vision-Language Models (LVLMs) standing at its forefront. While current LVLMs have demonstrated impressive capabilities in text-to-image generation, they often falter when confronted with complex textual descriptions demanding precise compositional understanding and visual planning. This limitation particularly impacts the accurate rendering of multiple objects, their attributes, spatial relationships, and specific poses within intricate scenes, as evidenced by benchmarks like LongBench-T2I. To address these challenges, we introduce LVLM-Composer, a novel 10-billion parameter scale LVLM specifically engineered for enhanced compositional image synthesis. Our method incorporates a Hierarchical Semantic Planning Module for structured prompt decomposition and a Fine-Grained Feature Alignment Mechanism for precise visual guidance during generation. We propose a multi-stage training paradigm, featuring Hierarchical Semantic-Visual Grounding Pre-training and Compositional Planning Reinforcement Learning with Self-Correction, to instill robust compositional reasoning. Extensive experiments on the LongBench-T2I benchmark, utilizing automatic evaluation by Gemini-2.0-Flash and InternVL3-78B, demonstrate LVLM-Composer's superior performance across critical compositional dimensions including object accuracy, composition fidelity, and pose accuracy, significantly outperforming state-of-the-art baselines. An in-depth ablation study further validates the indispensable contribution of our proposed modules, while human evaluations confirm the perceptual superiority of our generated images. LVLM-Composer represents a significant step towards truly controllable and compositionally accurate open-ended text-to-image generation.

</details>


### [34] [Voyaging into Unbounded Dynamic Scenes from a Single View](https://arxiv.org/abs/2507.04183)
*Fengrui Tian,Tianjiao Ding,Jinqi Luo,Hancheng Min,René Vidal*

Main category: cs.CV

TL;DR: 论文提出DynamicVoyager，通过将动态场景生成重新定义为场景外绘过程，解决了单视角生成无界动态场景的3D运动一致性问题。


<details>
  <summary>Details</summary>
Motivation: 研究单视角生成无界动态场景的问题，应用于增强/虚拟现实和机器人领域，需确保生成视图与3D运动一致。

Method: 将单视角视频输入映射为动态点云，利用射线上下文渲染部分视频并外绘，生成3D一致运动，更新点云以支持未来视图。

Result: 实验表明模型能生成无界场景，运动一致且可通过场景提示控制生成内容。

Conclusion: DynamicVoyager通过射线上下文和点云更新，实现了单视角下无界动态场景的3D一致生成。

Abstract: This paper studies the problem of generating an unbounded dynamic scene from a single view, which has wide applications in augmented/virtual reality and robotics. Since the scene is changing over time, different generated views need to be consistent with the underlying 3D motions. While previous works learn such consistency by training from multiple views, the generated scene regions are bounded to be close to the training views with limited camera movements. To address this issue, we propose DynamicVoyager that reformulates the dynamic scene generation as a scene outpainting process for new dynamic content. As 2D outpainting models can hardly generate 3D consistent motions from only 2D pixels at a single view, we consider pixels as rays to enrich the pixel input with the ray context, so that the 3D motion consistency can be learned from the ray information. More specifically, we first map the single-view video input to a dynamic point cloud with the estimated video depths. Then we render the partial video at a novel view and outpaint the video with ray contexts from the point cloud to generate 3D consistent motions. We employ the outpainted video to update the point cloud, which is used for scene outpainting from future novel views. Experiments show that our model is able to generate unbounded scenes with consistent motions along fly-through cameras, and the generated contents can be controlled with scene prompts.

</details>


### [35] [Quick Bypass Mechanism of Zero-Shot Diffusion-Based Image Restoration](https://arxiv.org/abs/2507.04207)
*Yu-Shan Tai,An-Yeu,Wu*

Main category: cs.CV

TL;DR: 提出了一种快速绕过机制（QBM）和修订反向过程（RRP），显著加速扩散模型在图像恢复任务中的去噪过程，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本方法在图像恢复任务中因去噪过程耗时较长而受限，需改进以提升效率。

Method: 通过QBM从中间近似初始化以绕过早期去噪步骤，并通过RRP调整随机噪声权重以增强随机性。

Result: 在ImageNet-1K和CelebA-HQ数据集上验证，QBM和RRP能有效加速现有方法且不降低性能。

Conclusion: QBM和RRP为扩散模型在图像恢复任务中的高效应用提供了可行解决方案。

Abstract: Recent advancements in diffusion models have demonstrated remarkable success in various image generation tasks. Building upon these achievements, diffusion models have also been effectively adapted to image restoration tasks, e.g., super-resolution and deblurring, aiming to recover high-quality images from degraded inputs. Although existing zero-shot approaches enable pretrained diffusion models to perform restoration tasks without additional fine-tuning, these methods often suffer from prolonged iteration times in the denoising process. To address this limitation, we propose a Quick Bypass Mechanism (QBM), a strategy that significantly accelerates the denoising process by initializing from an intermediate approximation, effectively bypassing early denoising steps. Furthermore, recognizing that approximation may introduce inconsistencies, we introduce a Revised Reverse Process (RRP), which adjusts the weighting of random noise to enhance the stochasticity and mitigate potential disharmony. We validate proposed methods on ImageNet-1K and CelebA-HQ across multiple image restoration tasks, e.g., super-resolution, deblurring, and compressed sensing. Our experimental results show that the proposed methods can effectively accelerate existing methods while maintaining original performance.

</details>


### [36] [DreamPoster: A Unified Framework for Image-Conditioned Generative Poster Design](https://arxiv.org/abs/2507.04218)
*Xiwei Hu,Haokun Chen,Zhongqi Qi,Hui Zhang,Dexiang Hong,Jie Shao,Xinglong Wu*

Main category: cs.CV

TL;DR: DreamPoster是一个基于文本到图像生成的框架，能够从用户提供的图像和文本提示中智能合成高质量海报，同时保持内容保真度并支持灵活的分辨率和布局输出。


<details>
  <summary>Details</summary>
Motivation: 现有的海报生成方法在内容保真度和灵活性上存在不足，DreamPoster旨在解决这些问题。

Method: 基于T2I模型Seedream3.0，采用系统化的数据标注流程和渐进式训练策略，构建多任务生成能力。

Result: 在测试基准中，DreamPoster的可用性达到88.55%，显著优于GPT-4o（47.56%）和SeedEdit3.0（25.96%）。

Conclusion: DreamPoster展示了在海报生成任务中的优越性能，并将应用于字节跳动的多个平台。

Abstract: We present DreamPoster, a Text-to-Image generation framework that intelligently synthesizes high-quality posters from user-provided images and text prompts while maintaining content fidelity and supporting flexible resolution and layout outputs. Specifically, DreamPoster is built upon our T2I model, Seedream3.0 to uniformly process different poster generating types. For dataset construction, we propose a systematic data annotation pipeline that precisely annotates textual content and typographic hierarchy information within poster images, while employing comprehensive methodologies to construct paired datasets comprising source materials (e.g., raw graphics/text) and their corresponding final poster outputs. Additionally, we implement a progressive training strategy that enables the model to hierarchically acquire multi-task generation capabilities while maintaining high-quality generation. Evaluations on our testing benchmarks demonstrate DreamPoster's superiority over existing methods, achieving a high usability rate of 88.55\%, compared to GPT-4o (47.56\%) and SeedEdit3.0 (25.96\%). DreamPoster will be online in Jimeng and other Bytedance Apps.

</details>


### [37] [Domain Generalizable Portrait Style Transfer](https://arxiv.org/abs/2507.04243)
*Xinbo Wang,Wenju Xu,Qing Zhang,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: 提出一种肖像风格迁移方法，通过语义对齐和AdaIN-Wavelet变换实现高质量风格化。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在不同领域泛化能力不足和语义对齐不精确的问题。

Method: 基于预训练模型和语义适配器建立密集语义对应，使用AdaIN-Wavelet变换平衡内容保留与风格化，结合双条件扩散模型生成结果。

Result: 实验表明该方法在风格迁移质量和语义对齐上表现优越。

Conclusion: 该方法实现了高质量、可控的肖像风格迁移，并在多个区域（如头发、眼睛等）表现出色。

Abstract: This paper presents a portrait style transfer method that generalizes well to various different domains while enabling high-quality semantic-aligned stylization on regions including hair, eyes, eyelashes, skins, lips, and background. To this end, we propose to establish dense semantic correspondence between the given input and reference portraits based on a pre-trained model and a semantic adapter, with which we obtain a warped reference semantically aligned with the input. To ensure effective yet controllable style transfer, we devise an AdaIN-Wavelet transform to balance content preservation and stylization by blending low-frequency information of the warped reference with high-frequency information of the input in the latent space. A style adapter is also designed to provide style guidance from the warped reference. With the stylized latent from AdaIN-Wavelet transform, we employ a dual-conditional diffusion model that integrates a ControlNet recording high-frequency information and the style guidance to generate the final result. Extensive experiments demonstrate the superiority of our method. Our code and trained model are available at https://github.com/wangxb29/DGPST.

</details>


### [38] [Towards Lightest Low-Light Image Enhancement Architecture for Mobile Devices](https://arxiv.org/abs/2507.04277)
*Guangrui Bai,Hailong Yan,Wenhai Liu,Yahui Deng,Erbao Dong*

Main category: cs.CV

TL;DR: LiteIE是一种超轻量级无监督增强框架，适用于移动和嵌入式设备，平衡视觉质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖大网络和标注数据，限制了在资源受限平台上的部署。

Method: 设计了一个仅含两个卷积层的骨干无关特征提取器，以及无参数的迭代恢复模块，结合无监督训练目标。

Result: 在LOL数据集上PSNR达19.04 dB，参数仅0.07%，在移动设备上实现30 FPS的4K图像处理。

Conclusion: LiteIE是资源有限平台上低光增强的高效实用解决方案。

Abstract: Real-time low-light image enhancement on mobile and embedded devices requires models that balance visual quality and computational efficiency. Existing deep learning methods often rely on large networks and labeled datasets, limiting their deployment on resource-constrained platforms. In this paper, we propose LiteIE, an ultra-lightweight unsupervised enhancement framework that eliminates dependence on large-scale supervision and generalizes well across diverse conditions. We design a backbone-agnostic feature extractor with only two convolutional layers to produce compact image features enhancement tensors. In addition, we develop a parameter-free Iterative Restoration Module, which reuses the extracted features to progressively recover fine details lost in earlier enhancement steps, without introducing any additional learnable parameters. We further propose an unsupervised training objective that integrates exposure control, edge-aware smoothness, and multi-scale color consistency losses. Experiments on the LOL dataset, LiteIE achieves 19.04 dB PSNR, surpassing SOTA by 1.4 dB while using only 0.07\% of its parameters. On a Snapdragon 8 Gen 3 mobile processor, LiteIE runs at 30 FPS for 4K images with just 58 parameters, enabling real-time deployment on edge devices. These results establish LiteIE as an efficient and practical solution for low-light enhancement on resource-limited platforms.

</details>


### [39] [MPQ-DMv2: Flexible Residual Mixed Precision Quantization for Low-Bit Diffusion Models with Temporal Distillation](https://arxiv.org/abs/2507.04290)
*Weilun Feng,Chuanguang Yang,Haotong Qin,Yuqi Li,Xiangqi Li,Zhulin An,Libo Huang,Boyu Diao,Fuzhen Zhuang,Michele Magno,Yongjun Xu,Yingli Tian,Tingwen Huang*

Main category: cs.CV

TL;DR: MPQ-DMv2提出了一种改进的混合精度量化框架，用于极低比特扩散模型，解决了现有量化方法在极低比特下的性能退化问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在视觉生成任务中表现优异，但高计算复杂度限制了其在边缘设备上的应用。现有量化方法在极低比特（2-4位）下泛化能力不足，导致性能严重下降。

Method: 提出Flexible Z-Order Residual Mixed Quantization处理异常值，采用Object-Oriented Low-Rank Initialization优化初始化，并通过Memory-based Temporal Relation Distillation保持时间一致性。

Result: 在各种生成任务上的实验表明，MPQ-DMv2在不同架构下显著优于现有方法，尤其是在极低比特宽度下。

Conclusion: MPQ-DMv2通过改进量化设计和优化策略，显著提升了极低比特扩散模型的性能，为边缘设备上的应用提供了可行方案。

Abstract: Diffusion models have demonstrated remarkable performance on vision generation tasks. However, the high computational complexity hinders its wide application on edge devices. Quantization has emerged as a promising technique for inference acceleration and memory reduction. However, existing quantization methods do not generalize well under extremely low-bit (2-4 bit) quantization. Directly applying these methods will cause severe performance degradation. We identify that the existing quantization framework suffers from the outlier-unfriendly quantizer design, suboptimal initialization, and optimization strategy. We present MPQ-DMv2, an improved \textbf{M}ixed \textbf{P}recision \textbf{Q}uantization framework for extremely low-bit \textbf{D}iffusion \textbf{M}odels. For the quantization perspective, the imbalanced distribution caused by salient outliers is quantization-unfriendly for uniform quantizer. We propose \textit{Flexible Z-Order Residual Mixed Quantization} that utilizes an efficient binary residual branch for flexible quant steps to handle salient error. For the optimization framework, we theoretically analyzed the convergence and optimality of the LoRA module and propose \textit{Object-Oriented Low-Rank Initialization} to use prior quantization error for informative initialization. We then propose \textit{Memory-based Temporal Relation Distillation} to construct an online time-aware pixel queue for long-term denoising temporal information distillation, which ensures the overall temporal consistency between quantized and full-precision model. Comprehensive experiments on various generation tasks show that our MPQ-DMv2 surpasses current SOTA methods by a great margin on different architectures, especially under extremely low-bit widths.

</details>


### [40] [Sat2City: 3D City Generation from A Single Satellite Image with Cascaded Latent Diffusion](https://arxiv.org/abs/2507.04403)
*Tongyan Hua,Lutao Jiang,Ying-Cong Chen,Wufan Zhao*

Main category: cs.CV

TL;DR: Sat2City框架结合稀疏体素网格和潜在扩散模型，从卫星图像生成详细3D城市场景，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖神经渲染技术，难以从有限的2D观测中生成大规模详细3D结构。

Method: 提出Sat2City框架，包含级联潜在扩散模型、Re-Hash操作和逆采样策略，结合合成数据集。

Result: 在合成数据集上验证，生成高保真3D结构，优于现有城市生成模型。

Conclusion: Sat2City为3D城市生成提供了高效解决方案，具有广泛应用潜力。

Abstract: Recent advancements in generative models have enabled 3D urban scene generation from satellite imagery, unlocking promising applications in gaming, digital twins, and beyond. However, most existing methods rely heavily on neural rendering techniques, which hinder their ability to produce detailed 3D structures on a broader scale, largely due to the inherent structural ambiguity derived from relatively limited 2D observations. To address this challenge, we propose Sat2City, a novel framework that synergizes the representational capacity of sparse voxel grids with latent diffusion models, tailored specifically for our novel 3D city dataset. Our approach is enabled by three key components: (1) A cascaded latent diffusion framework that progressively recovers 3D city structures from satellite imagery, (2) a Re-Hash operation at its Variational Autoencoder (VAE) bottleneck to compute multi-scale feature grids for stable appearance optimization and (3) an inverse sampling strategy enabling implicit supervision for smooth appearance transitioning.To overcome the challenge of collecting real-world city-scale 3D models with high-quality geometry and appearance, we introduce a dataset of synthesized large-scale 3D cities paired with satellite-view height maps. Validated on this dataset, our framework generates detailed 3D structures from a single satellite image, achieving superior fidelity compared to existing city generation models.

</details>


### [41] [A View-consistent Sampling Method for Regularized Training of Neural Radiance Fields](https://arxiv.org/abs/2507.04408)
*Aoxiang Fan,Corentin Dumery,Nicolas Talabot,Pascal Fua*

Main category: cs.CV

TL;DR: 论文提出了一种基于视图一致性分布的深度正则化方法，用于改进NeRF在真实世界数据上的表现，避免了传统深度估计模型的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 传统深度估计模型需要昂贵的3D监督训练且泛化能力差，尤其在户外无界场景中表现不佳。

Method: 利用视图一致性分布（结合低级颜色特征和高级蒸馏特征）和深度推动损失，对NeRF训练进行隐式正则化。

Result: 在多个公开数据集上的实验表明，该方法显著优于现有NeRF变体和深度正则化方法。

Conclusion: 提出的方法通过视图一致性分布和深度推动损失，有效提升了NeRF在复杂场景中的表现。

Abstract: Neural Radiance Fields (NeRF) has emerged as a compelling framework for scene representation and 3D recovery. To improve its performance on real-world data, depth regularizations have proven to be the most effective ones. However, depth estimation models not only require expensive 3D supervision in training, but also suffer from generalization issues. As a result, the depth estimations can be erroneous in practice, especially for outdoor unbounded scenes. In this paper, we propose to employ view-consistent distributions instead of fixed depth value estimations to regularize NeRF training. Specifically, the distribution is computed by utilizing both low-level color features and high-level distilled features from foundation models at the projected 2D pixel-locations from per-ray sampled 3D points. By sampling from the view-consistency distributions, an implicit regularization is imposed on the training of NeRF. We also utilize a depth-pushing loss that works in conjunction with the sampling technique to jointly provide effective regularizations for eliminating the failure modes. Extensive experiments conducted on various scenes from public datasets demonstrate that our proposed method can generate significantly better novel view synthesis results than state-of-the-art NeRF variants as well as different depth regularization methods.

</details>


### [42] [DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge](https://arxiv.org/abs/2507.04447)
*Wenyao Zhang,Hongsi Liu,Zekun Qi,Yunnan Wang,XinQiang Yu,Jiazhao Zhang,Runpei Dong,Jiawei He,He Wang,Zhizheng Zhang,Li Yi,Wenjun Zeng,Xin Jin*

Main category: cs.CV

TL;DR: DreamVLA提出了一种新的视觉-语言-动作（VLA）框架，通过整合全面的世界知识预测，解决了现有方法在动态、空间和语义信息上的不足，实现了感知-预测-动作的闭环。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在图像生成与动作预测结合中存在冗余信息和世界知识不全面的问题，限制了机器人操作的泛化和推理能力。

Method: DreamVLA引入了动态区域引导的世界知识预测，结合空间和语义线索，并采用块状结构化注意力机制防止信息干扰，同时使用扩散变换器解耦动作表示。

Result: 在真实和仿真环境中，DreamVLA在真实机器人任务中达到76.7%的成功率，在CALVIN ABC-D基准测试中平均长度为4.44。

Conclusion: DreamVLA通过整合全面的世界知识预测和解耦表示，显著提升了机器人操作的性能和泛化能力。

Abstract: Recent advances in vision-language-action (VLA) models have shown promise in integrating image generation with action prediction to improve generalization and reasoning in robot manipulation. However, existing methods are limited to challenging image-based forecasting, which suffers from redundant information and lacks comprehensive and critical world knowledge, including dynamic, spatial and semantic information. To address these limitations, we propose DreamVLA, a novel VLA framework that integrates comprehensive world knowledge forecasting to enable inverse dynamics modeling, thereby establishing a perception-prediction-action loop for manipulation tasks. Specifically, DreamVLA introduces a dynamic-region-guided world knowledge prediction, integrated with the spatial and semantic cues, which provide compact yet comprehensive representations for action planning. This design aligns with how humans interact with the world by first forming abstract multimodal reasoning chains before acting. To mitigate interference among the dynamic, spatial and semantic information during training, we adopt a block-wise structured attention mechanism that masks their mutual attention, preventing information leakage and keeping each representation clean and disentangled. Moreover, to model the conditional distribution over future actions, we employ a diffusion-based transformer that disentangles action representations from shared latent features. Extensive experiments on both real-world and simulation environments demonstrate that DreamVLA achieves 76.7% success rate on real robot tasks and 4.44 average length on the CALVIN ABC-D benchmarks.

</details>


### [43] [CoT-lized Diffusion: Let's Reinforce T2I Generation Step-by-step](https://arxiv.org/abs/2507.04451)
*Zheyuan Liu,Munan Ning,Qihui Zhang,Shuo Yang,Zhongrui Wang,Yiwei Yang,Xianzhe Xu,Yibing Song,Weihua Chen,Fan Wang,Li Yuan*

Main category: cs.CV

TL;DR: CoT-Diff通过将MLLM驱动的3D布局规划与扩散过程紧密结合，显著提升了文本到图像生成的空间对齐和组合保真度。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型在复杂场景中难以实现空间对齐，布局规划与生成过程脱节导致控制不足。

Method: CoT-Diff框架将MLLM驱动的3D布局规划与扩散过程结合，通过动态更新布局和条件感知注意力机制实现精确控制。

Result: 在3D场景基准测试中，CoT-Diff的空间对齐和组合保真度显著提升，复杂场景空间准确率比现有方法高34.7%。

Conclusion: CoT-Diff验证了这种紧密耦合生成范式的有效性。

Abstract: Current text-to-image (T2I) generation models struggle to align spatial composition with the input text, especially in complex scenes. Even layout-based approaches yield suboptimal spatial control, as their generation process is decoupled from layout planning, making it difficult to refine the layout during synthesis. We present CoT-Diff, a framework that brings step-by-step CoT-style reasoning into T2I generation by tightly integrating Multimodal Large Language Model (MLLM)-driven 3D layout planning with the diffusion process. CoT-Diff enables layout-aware reasoning inline within a single diffusion round: at each denoising step, the MLLM evaluates intermediate predictions, dynamically updates the 3D scene layout, and continuously guides the generation process. The updated layout is converted into semantic conditions and depth maps, which are fused into the diffusion model via a condition-aware attention mechanism, enabling precise spatial control and semantic injection. Experiments on 3D Scene benchmarks show that CoT-Diff significantly improves spatial alignment and compositional fidelity, and outperforms the state-of-the-art method by 34.7% in complex scene spatial accuracy, thereby validating the effectiveness of this entangled generation paradigm.

</details>


### [44] [BiVM: Accurate Binarized Neural Network for Efficient Video Matting](https://arxiv.org/abs/2507.04456)
*Haotong Qin,Xianglong Liu,Xudong Ma,Lei Ke,Yulun Zhang,Jie Luo,Michele Magno*

Main category: cs.CV

TL;DR: BiVM是一种高效的二值化神经网络，用于实时视频抠图，通过弹性快捷连接和可进化拓扑结构提升编码器性能，并通过稀疏解码器特征减少计算负担，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的实时视频抠图因计算限制难以广泛应用，现有二值化方法在精度和效率上存在不足。

Method: 提出弹性快捷连接和可进化拓扑结构的编码器，稀疏解码器特征，并采用信息引导的局部二值化模仿框架。

Result: BiVM在精度和效率上显著优于现有方法，计算和存储成本分别节省14.3倍和21.6倍。

Conclusion: BiVM通过优化编码器和解码器设计，解决了二值化视频抠图的精度和效率问题，适用于边缘设备。

Abstract: Deep neural networks for real-time video matting suffer significant computational limitations on edge devices, hindering their adoption in widespread applications such as online conferences and short-form video production. Binarization emerges as one of the most common compression approaches with compact 1-bit parameters and efficient bitwise operations. However, accuracy and efficiency limitations exist in the binarized video matting network due to its degenerated encoder and redundant decoder. Following a theoretical analysis based on the information bottleneck principle, the limitations are mainly caused by the degradation of prediction-relevant information in the intermediate features and the redundant computation in prediction-irrelevant areas. We present BiVM, an accurate and resource-efficient Binarized neural network for Video Matting. First, we present a series of binarized computation structures with elastic shortcuts and evolvable topologies, enabling the constructed encoder backbone to extract high-quality representation from input videos for accurate prediction. Second, we sparse the intermediate feature of the binarized decoder by masking homogeneous parts, allowing the decoder to focus on representation with diverse details while alleviating the computation burden for efficient inference. Furthermore, we construct a localized binarization-aware mimicking framework with the information-guided strategy, prompting matting-related representation in full-precision counterparts to be accurately and fully utilized. Comprehensive experiments show that the proposed BiVM surpasses alternative binarized video matting networks, including state-of-the-art (SOTA) binarization methods, by a substantial margin. Moreover, our BiVM achieves significant savings of 14.3x and 21.6x in computation and storage costs, respectively. We also evaluate BiVM on ARM CPU hardware.

</details>


### [45] [A Training-Free Style-Personalization via Scale-wise Autoregressive Model](https://arxiv.org/abs/2507.04482)
*Kyoungmin Lee,Jihun Park,Jongmin Gim,Wonhyeok Choi,Kyumin Hwang,Jaeyeul Kim,Sunghoon Im*

Main category: cs.CV

TL;DR: 提出一种无需训练的个性化图像生成框架，通过多路径设计和干预分析实现内容和风格控制。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要额外训练的问题，提供更灵活高效的图像生成控制。

Method: 采用三路径设计（内容、风格、生成），结合关键阶段注意力共享和自适应查询共享机制。

Result: 在风格保真度和提示保真度上表现优异，推理速度更快，部署更灵活。

Conclusion: 该方法在无需训练的情况下实现了高效的个性化图像生成，具有实际应用潜力。

Abstract: We present a training-free framework for style-personalized image generation that controls content and style information during inference using a scale-wise autoregressive model. Our method employs a three-path design--content, style, and generation--each guided by a corresponding text prompt, enabling flexible and efficient control over image semantics without any additional training. A central contribution of this work is a step-wise and attention-wise intervention analysis. Through systematic prompt and feature injection, we find that early-to-middle generation steps play a pivotal role in shaping both content and style, and that query features predominantly encode content-specific information. Guided by these insights, we introduce two targeted mechanisms: Key Stage Attention Sharing, which aligns content and style during the semantically critical steps, and Adaptive Query Sharing, which reinforces content semantics in later steps through similarity-aware query blending. Extensive experiments demonstrate that our method achieves competitive style fidelity and prompt fidelity compared to fine-tuned baselines, while offering faster inference and greater deployment flexibility.

</details>


### [46] [MambaVideo for Discrete Video Tokenization with Channel-Split Quantization](https://arxiv.org/abs/2507.04559)
*Dawit Mureja Argaw,Xian Liu,Joon Son Chung,Ming-Yu Liu,Fitsum Reda*

Main category: cs.CV

TL;DR: 提出了一种基于Mamba的编码器-解码器架构和通道分割量化方案的新型离散视频标记化方法，显著提升了生成模型的效率和表现力。


<details>
  <summary>Details</summary>
Motivation: 视频数据的高维度特性使得离散标记化成为自回归生成建模的关键需求，现有方法存在局限性。

Method: 采用Mamba架构的编码器-解码器，并提出通道分割量化方案以增强量化潜在表示的能力。

Result: 模型在多个数据集上表现优异，超越了基于因果3D卷积和Transformer的方法。

Conclusion: 该模型为自回归视频生成提供了一种高效且强大的标记化工具。

Abstract: Discrete video tokenization is essential for efficient autoregressive generative modeling due to the high dimensionality of video data. This work introduces a state-of-the-art discrete video tokenizer with two key contributions. First, we propose a novel Mamba-based encoder-decoder architecture that overcomes the limitations of previous sequencebased tokenizers. Second, we introduce a new quantization scheme, channel-split quantization, which significantly enhances the representational power of quantized latents while preserving the token count. Our model sets a new state-of-the-art, outperforming both causal 3D convolutionbased and Transformer-based approaches across multiple datasets. Experimental results further demonstrate its robustness as a tokenizer for autoregressive video generation.

</details>


### [47] [S$^2$Edit: Text-Guided Image Editing with Precise Semantic and Spatial Control](https://arxiv.org/abs/2507.04584)
*Xudong Liu,Zikun Chen,Ruowei Jiang,Ziyi Wu,Kejia Yin,Han Zhao,Parham Aarabi,Igor Gilitschenski*

Main category: cs.CV

TL;DR: S$^2$Edit是一种基于预训练文本到图像扩散模型的新方法，支持个性化编辑，具有精确的语义和空间控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法在需要细粒度控制的编辑任务（如人脸编辑）中表现不佳，容易丢失身份信息或改变无关区域。

Method: 通过微调模型将身份信息嵌入可学习的文本标记，并在文本特征空间中施加正交约束以解耦身份与属性。使用对象掩码引导交叉注意力图。

Result: 实验表明S$^2$Edit在定量和定性上优于现有方法，并能实现如化妆转移等应用。

Conclusion: S$^2$Edit在保持身份信息的同时实现了精确的局部编辑，具有广泛的应用潜力。

Abstract: Recent advances in diffusion models have enabled high-quality generation and manipulation of images guided by texts, as well as concept learning from images. However, naive applications of existing methods to editing tasks that require fine-grained control, e.g., face editing, often lead to suboptimal solutions with identity information and high-frequency details lost during the editing process, or irrelevant image regions altered due to entangled concepts. In this work, we propose S$^2$Edit, a novel method based on a pre-trained text-to-image diffusion model that enables personalized editing with precise semantic and spatial control. We first fine-tune our model to embed the identity information into a learnable text token. During fine-tuning, we disentangle the learned identity token from attributes to be edited by enforcing an orthogonality constraint in the textual feature space. To ensure that the identity token only affects regions of interest, we apply object masks to guide the cross-attention maps. At inference time, our method performs localized editing while faithfully preserving the original identity with semantically disentangled and spatially focused identity token learned. Extensive experiments demonstrate the superiority of S$^2$Edit over state-of-the-art methods both quantitatively and qualitatively. Additionally, we showcase several compositional image editing applications of S$^2$Edit such as makeup transfer.

</details>


### [48] [QR-LoRA: Efficient and Disentangled Fine-tuning via QR Decomposition for Customized Generation](https://arxiv.org/abs/2507.04599)
*Jiahui Yang,Yongjia Ma,Donglin Di,Hao Li,Wei Chen,Yan Xie,Jianxun Cui,Xun Yang,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 提出QR-LoRA框架，通过QR分解实现结构化参数更新，有效分离视觉属性，减少可训练参数并避免特征纠缠。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型在结合多个LoRA模型时，权重矩阵的非结构化修改会导致内容与风格属性的特征纠缠。

Method: 利用QR分解，固定Q和R矩阵，仅训练额外的任务特定ΔR矩阵，减少参数并增强属性分离。

Result: QR-LoRA在内容-风格融合任务中表现出优异的解耦能力，参数效率更高。

Conclusion: QR-LoRA为生成模型提供了一种参数高效、解耦的微调新范式。

Abstract: Existing text-to-image models often rely on parameter fine-tuning techniques such as Low-Rank Adaptation (LoRA) to customize visual attributes. However, when combining multiple LoRA models for content-style fusion tasks, unstructured modifications of weight matrices often lead to undesired feature entanglement between content and style attributes. We propose QR-LoRA, a novel fine-tuning framework leveraging QR decomposition for structured parameter updates that effectively separate visual attributes. Our key insight is that the orthogonal Q matrix naturally minimizes interference between different visual features, while the upper triangular R matrix efficiently encodes attribute-specific transformations. Our approach fixes both Q and R matrices while only training an additional task-specific $\Delta R$ matrix. This structured design reduces trainable parameters to half of conventional LoRA methods and supports effective merging of multiple adaptations without cross-contamination due to the strong disentanglement properties between $\Delta R$ matrices. Experiments demonstrate that QR-LoRA achieves superior disentanglement in content-style fusion tasks, establishing a new paradigm for parameter-efficient, disentangled fine-tuning in generative models.

</details>


### [49] [Learning Robust Stereo Matching in the Wild with Selective Mixture-of-Experts](https://arxiv.org/abs/2507.04631)
*Yun Wang,Longguang Wang,Chenghao Zhang,Yongjian Zhang,Zhanjie Zhang,Ao Ma,Chenyou Fan,Tin Lun Lam,Junjie Hu*

Main category: cs.CV

TL;DR: SMoEStereo提出了一种结合LoRA和MoE模块的新框架，通过动态选择专家和自适应核大小，提升了立体匹配的跨域性能，同时通过轻量级决策网络平衡效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的立体匹配网络在跨域性能上表现不足，主要受限于域偏移和不平衡的视差分布。利用视觉基础模型（VFMs）可以增强鲁棒性，但如何高效整合仍具挑战性。

Method: 提出SMoEStereo框架，结合LoRA和MoE模块，包括动态选择专家的MoE-LoRA和自适应核大小的MoE-Adapter，并通过轻量级决策网络选择性激活模块。

Result: 实验表明，该方法在多个基准测试中实现了最先进的跨域和联合泛化性能，无需特定数据集适配。

Conclusion: SMoEStereo通过创新模块设计和轻量级决策网络，有效提升了立体匹配的鲁棒性和效率。

Abstract: Recently, learning-based stereo matching networks have advanced significantly. However, they often lack robustness and struggle to achieve impressive cross-domain performance due to domain shifts and imbalanced disparity distributions among diverse datasets. Leveraging Vision Foundation Models (VFMs) can intuitively enhance the model's robustness, but integrating such a model into stereo matching cost-effectively to fully realize their robustness remains a key challenge. To address this, we propose SMoEStereo, a novel framework that adapts VFMs for stereo matching through a tailored, scene-specific fusion of Low-Rank Adaptation (LoRA) and Mixture-of-Experts (MoE) modules. SMoEStereo introduces MoE-LoRA with adaptive ranks and MoE-Adapter with adaptive kernel sizes. The former dynamically selects optimal experts within MoE to adapt varying scenes across domains, while the latter injects inductive bias into frozen VFMs to improve geometric feature extraction. Importantly, to mitigate computational overhead, we further propose a lightweight decision network that selectively activates MoE modules based on input complexity, balancing efficiency with accuracy. Extensive experiments demonstrate that our method exhibits state-of-the-art cross-domain and joint generalization across multiple benchmarks without dataset-specific adaptation. The code is available at \textcolor{red}{https://github.com/cocowy1/SMoE-Stereo}.

</details>


### [50] [ChangeBridge: Spatiotemporal Image Generation with Multimodal Controls for Remote Sensing](https://arxiv.org/abs/2507.04678)
*Zhenghui Zhao,Chen Wu,Di Wang,Hongruixuan Chen,Zhuo Zheng*

Main category: cs.CV

TL;DR: ChangeBridge是一种基于多模态控制的时空扩散模型，用于从给定场景图像模拟未来场景，适用于城市规划等领域。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法未探索基于给定场景图像模拟未来场景的能力，而这一能力在城市规划等领域有广泛应用。

Method: 提出ChangeBridge，一种条件时空扩散模型，利用多模态空间控制（如文本提示、实例布局和语义图）从事件前图像合成事件后图像。

Result: 实验证明ChangeBridge能模拟高保真未来场景，并与给定条件（如事件和事件驱动的背景变化）对齐。

Conclusion: ChangeBridge是首个具有多模态控制的时空生成模型，为遥感领域提供了新的模拟能力。

Abstract: Recent advancements in generative methods, especially diffusion models, have made great progress in remote sensing image synthesis. Despite these advancements, existing methods have not explored the simulation of future scenarios based on given scenario images. This simulation capability has wide applications for urban planning, land managementChangeBridge: Spatiotemporal Image Generation with Multimodal Controls, and beyond. In this work, we propose ChangeBridge, a conditional spatiotemporal diffusion model. Given pre-event images and conditioned on multimodal spatial controls (e.g., text prompts, instance layouts, and semantic maps), ChangeBridge can synthesize post-event images. The core idea behind ChangeBridge is to modeling the noise-to-image diffusion model, as a pre-to-post diffusion bridge. Conditioned on multimodal controls, ChangeBridge leverages a stochastic Brownian-bridge diffusion, directly modeling the spatiotemporal evolution between pre-event and post-event states. To the best of our knowledge, ChangeBridge is the first spatiotemporal generative model with multimodal controls for remote sensing. Experimental results demonstrate that ChangeBridge can simulate high-fidelity future scenarios aligned with given conditions, including event and event-driven background variations. Code will be available.

</details>


### [51] [TeethGenerator: A two-stage framework for paired pre- and post-orthodontic 3D dental data generation](https://arxiv.org/abs/2507.04685)
*Changsong Lei,Yaqian Liang,Shaofeng Wang,Jiajia Dai,Yong-Jin Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为TeethGenerator的两阶段框架，用于生成正畸前后的配对3D牙齿模型，以支持牙齿排列神经网络的训练。


<details>
  <summary>Details</summary>
Motivation: 当前3D形状生成方法多关注单对象生成，无法满足生成解剖结构复杂的牙齿模型需求，且临床数据收集耗时。

Method: 采用两阶段框架：牙齿形状生成模块（基于扩散模型学习牙齿形态分布）和牙齿风格生成模块（以风格为条件生成正畸前模型）。

Result: 合成数据与真实数据分布高度一致，结合真实数据训练显著提升了牙齿排列性能。

Conclusion: TeethGenerator为牙齿排列网络提供了高质量合成数据，解决了临床数据不足的问题。

Abstract: Digital orthodontics represents a prominent and critical application of computer vision technology in the medical field. So far, the labor-intensive process of collecting clinical data, particularly in acquiring paired 3D orthodontic teeth models, constitutes a crucial bottleneck for developing tooth arrangement neural networks. Although numerous general 3D shape generation methods have been proposed, most of them focus on single-object generation and are insufficient for generating anatomically structured teeth models, each comprising 24-32 segmented teeth. In this paper, we propose TeethGenerator, a novel two-stage framework designed to synthesize paired 3D teeth models pre- and post-orthodontic, aiming to facilitate the training of downstream tooth arrangement networks. Specifically, our approach consists of two key modules: (1) a teeth shape generation module that leverages a diffusion model to learn the distribution of morphological characteristics of teeth, enabling the generation of diverse post-orthodontic teeth models; and (2) a teeth style generation module that synthesizes corresponding pre-orthodontic teeth models by incorporating desired styles as conditional inputs. Extensive qualitative and quantitative experiments demonstrate that our synthetic dataset aligns closely with the distribution of real orthodontic data, and promotes tooth alignment performance significantly when combined with real data for training. The code and dataset are available at https://github.com/lcshhh/teeth_generator.

</details>


### [52] [Structure-Guided Diffusion Models for High-Fidelity Portrait Shadow Removal](https://arxiv.org/abs/2507.04692)
*Wanchang Yu,Qing Zhang,Rongjia Zheng,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: 提出了一种基于扩散的肖像阴影去除方法，通过生成式修复和细节恢复，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在去除肖像阴影时容易出现面部身份篡改、阴影残留、颜色失真等问题，需要更鲁棒且高保真的解决方案。

Method: 1. 训练阴影无关的结构提取网络生成结构图；2. 训练结构引导的扩散修复模型去除阴影；3. 训练细节恢复扩散模型优化结果。

Result: 在基准数据集上表现优异，避免了常见问题如身份篡改、阴影残留等。

Conclusion: 该方法通过扩散模型和结构引导，实现了高质量的肖像阴影去除，代码已开源。

Abstract: We present a diffusion-based portrait shadow removal approach that can robustly produce high-fidelity results. Unlike previous methods, we cast shadow removal as diffusion-based inpainting. To this end, we first train a shadow-independent structure extraction network on a real-world portrait dataset with various synthetic lighting conditions, which allows to generate a shadow-independent structure map including facial details while excluding the unwanted shadow boundaries. The structure map is then used as condition to train a structure-guided inpainting diffusion model for removing shadows in a generative manner. Finally, to restore the fine-scale details (e.g., eyelashes, moles and spots) that may not be captured by the structure map, we take the gradients inside the shadow regions as guidance and train a detail restoration diffusion model to refine the shadow removal result. Extensive experiments on the benchmark datasets show that our method clearly outperforms existing methods, and is effective to avoid previously common issues such as facial identity tampering, shadow residual, color distortion, structure blurring, and loss of details. Our code is available at https://github.com/wanchang-yu/Structure-Guided-Diffusion-for-Portrait-Shadow-Removal.

</details>


### [53] [Identity-Preserving Text-to-Video Generation Guided by Simple yet Effective Spatial-Temporal Decoupled Representations](https://arxiv.org/abs/2507.04705)
*Yuji Wang,Moran Li,Xiaobin Hu,Ran Yi,Jiangning Zhang,Han Feng,Weijian Cao,Yabiao Wang,Chengjie Wang,Lizhuang Ma*

Main category: cs.CV

TL;DR: 提出了一种空间-时间解耦框架，通过语义提示优化和分阶段生成方法，解决了文本到视频生成中的身份一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有端到端框架在空间布局和时间动态之间存在权衡，难以同时保证身份一致性和动态流畅性。

Method: 采用空间-时间解耦框架，将表示分解为空间特征（布局）和时间特征（动态），并通过语义提示优化和分阶段生成实现。

Result: 实验表明，该方法在时空一致性、身份保留、文本相关性和视频质量方面表现优异，获得2025年ACM MultiMedia Challenge亚军。

Conclusion: 该框架简单有效，成功解决了文本到视频生成中的关键问题，具有广泛应用潜力。

Abstract: Identity-preserving text-to-video (IPT2V) generation, which aims to create high-fidelity videos with consistent human identity, has become crucial for downstream applications. However, current end-to-end frameworks suffer a critical spatial-temporal trade-off: optimizing for spatially coherent layouts of key elements (e.g., character identity preservation) often compromises instruction-compliant temporal smoothness, while prioritizing dynamic realism risks disrupting the spatial coherence of visual structures. To tackle this issue, we propose a simple yet effective spatial-temporal decoupled framework that decomposes representations into spatial features for layouts and temporal features for motion dynamics. Specifically, our paper proposes a semantic prompt optimization mechanism and stage-wise decoupled generation paradigm. The former module decouples the prompt into spatial and temporal components. Aligned with the subsequent stage-wise decoupled approach, the spatial prompts guide the text-to-image (T2I) stage to generate coherent spatial features, while the temporal prompts direct the sequential image-to-video (I2V) stage to ensure motion consistency. Experimental results validate that our approach achieves excellent spatiotemporal consistency, demonstrating outstanding performance in identity preservation, text relevance, and video quality. By leveraging this simple yet robust mechanism, our algorithm secures the runner-up position in 2025 ACM MultiMedia Challenge.

</details>


### [54] [Losing Control: Data Poisoning Attack on Guided Diffusion via ControlNet](https://arxiv.org/abs/2507.04726)
*Raz Lapid,Almog Dubin*

Main category: cs.CV

TL;DR: 本文提出了一种针对ControlNets的新型数据投毒攻击方法，能够在无文本触发的情况下生成特定内容，揭示了开源ControlNets管道的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: ControlNets依赖公开数据集和社区共享数据进行微调，容易受到隐蔽的数据投毒攻击，本文旨在揭示这一漏洞。

Method: 通过注入毒化样本（将轻微触发的输入与NSFW目标配对），使模型在触发时生成NSFW内容，同时保持干净提示的保真度。

Result: 在大规模高质量数据集上，攻击成功率高且触发输入难以察觉。

Conclusion: 研究揭示了开源ControlNets管道的关键漏洞，强调了数据净化和防御机制的重要性。

Abstract: Text-to-image diffusion models have achieved remarkable success in translating textual prompts into high-fidelity images. ControlNets further extend these models by allowing precise, image-based conditioning (e.g., edge maps, depth, pose), enabling fine-grained control over structure and style. However, their dependence on large, publicly scraped datasets -- and the increasing use of community-shared data for fine-tuning -- exposes them to stealthy data poisoning attacks. In this work, we introduce a novel data poisoning method that manipulates ControlNets to generate images containing specific content without any text triggers. By injecting poisoned samples -- each pairing a subtly triggered input with an NSFW target -- the model retains clean-prompt fidelity yet reliably produces NSFW outputs when the trigger is present. On large-scale, high-quality datasets, our backdoor achieves high attack success rate while remaining imperceptible in raw inputs. These results reveal a critical vulnerability in open-source ControlNets pipelines and underscore the need for robust data sanitization and defense mechanisms.

</details>


### [55] [MatDecompSDF: High-Fidelity 3D Shape and PBR Material Decomposition from Multi-View Images](https://arxiv.org/abs/2507.04749)
*Chengyu Wang,Isabella Bennett,Henry Scott,Liang Zhang,Mei Chen,Hao Li,Rui Zhao*

Main category: cs.CV

TL;DR: MatDecompSDF是一个新框架，用于从多视角图像中恢复高保真3D形状并分解其基于物理的材料属性。


<details>
  <summary>Details</summary>
Motivation: 逆向渲染的核心挑战是从2D观测中解耦几何、材料和光照的欠定问题。

Method: 方法通过联合优化三个神经组件：神经SDF表示几何、神经场预测PBR材料参数、MLP模型捕获环境光照，并结合物理可微渲染层进行端到端优化。

Result: 在合成和真实数据集（如DTU）上，MatDecompSDF在几何精度、材料保真度和新视角合成方面超越现有方法。

Conclusion: 该方法生成可编辑和可重照明的资产，适用于数字内容创作。

Abstract: We present MatDecompSDF, a novel framework for recovering high-fidelity 3D shapes and decomposing their physically-based material properties from multi-view images. The core challenge of inverse rendering lies in the ill-posed disentanglement of geometry, materials, and illumination from 2D observations. Our method addresses this by jointly optimizing three neural components: a neural Signed Distance Function (SDF) to represent complex geometry, a spatially-varying neural field for predicting PBR material parameters (albedo, roughness, metallic), and an MLP-based model for capturing unknown environmental lighting. The key to our approach is a physically-based differentiable rendering layer that connects these 3D properties to the input images, allowing for end-to-end optimization. We introduce a set of carefully designed physical priors and geometric regularizations, including a material smoothness loss and an Eikonal loss, to effectively constrain the problem and achieve robust decomposition. Extensive experiments on both synthetic and real-world datasets (e.g., DTU) demonstrate that MatDecompSDF surpasses state-of-the-art methods in geometric accuracy, material fidelity, and novel view synthesis. Crucially, our method produces editable and relightable assets that can be seamlessly integrated into standard graphics pipelines, validating its practical utility for digital content creation.

</details>


### [56] [GraphBrep: Learning B-Rep in Graph Structure for Efficient CAD Generation](https://arxiv.org/abs/2507.04765)
*Weilin Lai,Tie Xu,Hu Wang*

Main category: cs.CV

TL;DR: GraphBrep提出了一种显式表示和学习紧凑拓扑的B-Rep生成模型，通过图扩散模型减少冗余，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 直接B-Rep生成在CAD工作流程中日益重要，但现有方法将拓扑隐式嵌入几何特征，导致冗余和计算成本高。

Method: 构建无向加权图表示表面拓扑，利用图扩散模型学习拓扑条件，显式表示确保数据结构紧凑。

Result: 在两个大规模无条件数据集和一个类别条件数据集上，训练和推理时间分别减少31.3%和56.3%，同时保持高质量生成。

Conclusion: GraphBrep通过显式拓扑表示有效降低计算成本，为CAD生成提供了高效解决方案。

Abstract: Direct B-Rep generation is increasingly important in CAD workflows, eliminating costly modeling sequence data and supporting complex features. A key challenge is modeling joint distribution of the misaligned geometry and topology. Existing methods tend to implicitly embed topology into the geometric features of edges. Although this integration ensures feature alignment, it also causes edge geometry to carry more redundant structural information compared to the original B-Rep, leading to significantly higher computational cost. To reduce redundancy, we propose GraphBrep, a B-Rep generation model that explicitly represents and learns compact topology. Following the original structure of B-Rep, we construct an undirected weighted graph to represent surface topology. A graph diffusion model is employed to learn topology conditioned on surface features, serving as the basis for determining connectivity between primitive surfaces. The explicit representation ensures a compact data structure, effectively reducing computational cost during both training and inference. Experiments on two large-scale unconditional datasets and one category-conditional dataset demonstrate the proposed method significantly reduces training and inference times (up to 31.3% and 56.3% for given datasets, respectively) while maintaining high-quality CAD generation compared with SOTA.

</details>


### [57] [CMET: Clustering guided METric for quantifying embedding quality](https://arxiv.org/abs/2507.04840)
*Sourav Ghosh,Chayan Maitra,Rajat K. De*

Main category: cs.CV

TL;DR: 提出了一种名为CMET的新度量标准，用于量化嵌入质量，通过局部和全局评分（CMET_L和CMET_G）评估数据嵌入的保形能力。


<details>
  <summary>Details</summary>
Motivation: 现有度量标准在时间和空间复杂度上较高，且缺乏对数据嵌入局部和全局结构的统计验证。

Method: 开发了CMET度量标准，包含CMET_L和CMET_G两个评分，分别衡量局部和全局形状保留能力。

Result: 在多种数据集上验证了CMET的优越性能，包括合成、生物和图像数据。

Conclusion: CMET因其高效性、稳定性和广泛的适用性，成为一种可靠的嵌入质量度量标准。

Abstract: Due to rapid advancements in technology, datasets are available from various domains. In order to carry out more relevant and appropriate analysis, it is often necessary to project the dataset into a higher or lower dimensional space based on requirement. Projecting the data in a higher-dimensional space helps in unfolding intricate patterns, enhancing the performance of the underlying models. On the other hand, dimensionality reduction is helpful in denoising data while capturing maximal information, as well as reducing execution time and memory.In this context, it is not always statistically evident whether the transformed embedding retains the local and global structure of the original data. Most of the existing metrics that are used for comparing the local and global shape of the embedding against the original one are highly expensive in terms of time and space complexity. In order to address this issue, the objective of this study is to formulate a novel metric, called Clustering guided METric (CMET), for quantifying embedding quality. It is effective to serve the purpose of quantitative comparison between an embedding and the original data. CMET consists of two scores, viz., CMET_L and CMET_G, that measure the degree of local and global shape preservation capability, respectively. The efficacy of CMET has been demonstrated on a wide variety of datasets, including four synthetic, two biological, and two image datasets. Results reflect the favorable performance of CMET against the state-of-the-art methods. Capability to handle both small and large data, low algorithmic complexity, better and stable performance across all kinds of data, and different choices of hyper-parameters feature CMET as a reliable metric.

</details>


### [58] [HV-MMBench: Benchmarking MLLMs for Human-Centric Video Understanding](https://arxiv.org/abs/2507.04909)
*Yuxuan Cai,Jiangning Zhang,Zhenye Gan,Qingdong He,Xiaobin Hu,Junwei Zhu,Yabiao Wang,Chengjie Wang,Zhucun Xue,Xinwei He,Xiang Bai*

Main category: cs.CV

TL;DR: HV-MMBench是一个针对多模态大语言模型（MLLMs）在人类中心视频理解中的全面评估基准，涵盖15个任务、多种数据格式和多领域视频场景。


<details>
  <summary>Details</summary>
Motivation: 现有的人类中心视频基准过于关注生成质量和动作识别，缺乏对感知和认知能力的全面评估，且评价指标单一。

Method: 提出HV-MMBench，包含多样化的任务（如属性感知、认知推理）、多种数据格式（选择题、填空题等）和多领域视频场景。

Result: HV-MMBench支持对MLLMs在人类中心视频理解中的全面评估，涵盖短期到长期视频，并引入多样化的评价指标。

Conclusion: HV-MMBench填补了现有基准的不足，为MLLMs在人类中心视频理解中的能力评估提供了更全面的工具。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant advances in visual understanding tasks involving both images and videos. However, their capacity to comprehend human-centric video data remains underexplored, primarily due to the absence of comprehensive and high-quality evaluation benchmarks. Existing human-centric benchmarks predominantly emphasize video generation quality and action recognition, while overlooking essential perceptual and cognitive abilities required in human-centered scenarios. Furthermore, they are often limited by single-question paradigms and overly simplistic evaluation metrics. To address above limitations, we propose a modern HV-MMBench, a rigorously curated benchmark designed to provide a more holistic evaluation of MLLMs in human-centric video understanding. Compared to existing human-centric video benchmarks, our work offers the following key features: (1) Diverse evaluation dimensions: HV-MMBench encompasses 15 tasks, ranging from basic attribute perception (e.g., age estimation, emotion recognition) to advanced cognitive reasoning (e.g., social relationship prediction, intention prediction), enabling comprehensive assessment of model capabilities; (2) Varied data types: The benchmark includes multiple-choice, fill-in-blank, true/false, and open-ended question formats, combined with diverse evaluation metrics, to more accurately and robustly reflect model performance; (3) Multi-domain video coverage: The benchmark spans 50 distinct visual scenarios, enabling comprehensive evaluation across fine-grained scene variations; (4) Temporal coverage: The benchmark covers videos from short-term (10 seconds) to long-term (up to 30min) durations, supporting systematic analysis of models temporal reasoning abilities across diverse contextual lengths.

</details>


### [59] [RainShift: A Benchmark for Precipitation Downscaling Across Geographies](https://arxiv.org/abs/2507.04930)
*Paula Harder,Luca Schmidt,Francis Pelletier,Nicole Ludwig,Matthew Chantry,Christian Lessig,Alex Hernandez-Garcia,David Rolnick*

Main category: cs.CV

TL;DR: RainShift数据集和基准测试评估了深度学习超分辨率模型在地理分布变化下的降尺度性能，发现模型在分布外区域性能显著下降，数据对齐等方法可改善泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决地球系统模型（ESM）在局部尺度风险评估中高分辨率计算不可行的问题，并评估深度学习降尺度模型在地理区域间的泛化能力。

Method: 引入RainShift数据集和基准，评估GAN和扩散模型等降尺度方法在南北半球数据差异下的泛化表现。

Result: 模型在分布外区域性能显著下降，扩展训练域可部分改善但不足以克服地理差异，数据对齐等方法能提升泛化能力。

Conclusion: 研究提升了降尺度方法的全球适用性，为减少高分辨率气候信息获取的不平等迈出一步。

Abstract: Earth System Models (ESM) are our main tool for projecting the impacts of climate change. However, running these models at sufficient resolution for local-scale risk-assessments is not computationally feasible. Deep learning-based super-resolution models offer a promising solution to downscale ESM outputs to higher resolutions by learning from data. Yet, due to regional variations in climatic processes, these models typically require retraining for each geographical area-demanding high-resolution observational data, which is unevenly available across the globe. This highlights the need to assess how well these models generalize across geographic regions. To address this, we introduce RainShift, a dataset and benchmark for evaluating downscaling under geographic distribution shifts. We evaluate state-of-the-art downscaling approaches including GANs and diffusion models in generalizing across data gaps between the Global North and Global South. Our findings reveal substantial performance drops in out-of-distribution regions, depending on model and geographic area. While expanding the training domain generally improves generalization, it is insufficient to overcome shifts between geographically distinct regions. We show that addressing these shifts through, for example, data alignment can improve spatial generalization. Our work advances the global applicability of downscaling methods and represents a step toward reducing inequities in access to high-resolution climate information.

</details>


### [60] [Taming the Tri-Space Tension: ARC-Guided Hallucination Modeling and Control for Text-to-Image Generation](https://arxiv.org/abs/2507.04946)
*Jianjiang Yang,Ziyan Huang*

Main category: cs.CV

TL;DR: 论文提出了一种认知启发的视角，将文本到图像（T2I）扩散模型中的“幻觉”重新解释为潜在对齐空间中的轨迹漂移，并引入了对齐风险码（ARC）和TensionModulator（TM-ARC）来量化和干预生成过程中的对齐问题。


<details>
  <summary>Details</summary>
Motivation: 尽管T2I扩散模型在图像质量和提示保真度方面取得了显著进展，但仍存在“幻觉”问题，即生成内容与提示语义不一致。作者认为这些失败反映了生成过程中更深层次的结构性错位。

Method: 通过实证观察，作者将生成过程描述为多轴认知张力场中的动态平衡，并提出了“幻觉三空间”概念和ARC动态向量表示。基于此，开发了TM-ARC控制器，在潜在空间中进行干预。

Result: 实验表明，TM-ARC显著减少了幻觉，同时保持了图像质量和多样性。

Conclusion: 该框架为理解和缓解T2I扩散模型中的生成失败提供了一种统一且可解释的方法。

Abstract: Despite remarkable progress in image quality and prompt fidelity, text-to-image (T2I) diffusion models continue to exhibit persistent "hallucinations", where generated content subtly or significantly diverges from the intended prompt semantics. While often regarded as unpredictable artifacts, we argue that these failures reflect deeper, structured misalignments within the generative process. In this work, we propose a cognitively inspired perspective that reinterprets hallucinations as trajectory drift within a latent alignment space. Empirical observations reveal that generation unfolds within a multiaxial cognitive tension field, where the model must continuously negotiate competing demands across three key critical axes: semantic coherence, structural alignment, and knowledge grounding. We then formalize this three-axis space as the \textbf{Hallucination Tri-Space} and introduce the Alignment Risk Code (ARC): a dynamic vector representation that quantifies real-time alignment tension during generation. The magnitude of ARC captures overall misalignment, its direction identifies the dominant failure axis, and its imbalance reflects tension asymmetry. Based on this formulation, we develop the TensionModulator (TM-ARC): a lightweight controller that operates entirely in latent space. TM-ARC monitors ARC signals and applies targeted, axis-specific interventions during the sampling process. Extensive experiments on standard T2I benchmarks demonstrate that our approach significantly reduces hallucination without compromising image quality or diversity. This framework offers a unified and interpretable approach for understanding and mitigating generative failures in diffusion-based T2I systems.

</details>


### [61] [DC-AR: Efficient Masked Autoregressive Image Generation with Deep Compression Hybrid Tokenizer](https://arxiv.org/abs/2507.04947)
*Yecheng Wu,Junyu Chen,Zhuoyang Zhang,Enze Xie,Jincheng Yu,Junsong Chen,Jinyi Hu,Yao Lu,Song Han,Han Cai*

Main category: cs.CV

TL;DR: DC-AR是一种新型的掩码自回归文本到图像生成框架，通过DC-HT深度压缩混合分词器实现高效高质量图像生成。


<details>
  <summary>Details</summary>
Motivation: 解决现有掩码自回归模型因分词器限制在质量和效率上落后于扩散模型的问题。

Method: 引入DC-HT分词器实现32倍空间压缩，并基于MaskGIT构建混合掩码自回归框架，先通过离散令牌生成结构元素，再通过残差令牌进行细化。

Result: 在MJHQ-30K上gFID为5.49，GenEval得分为0.69，吞吐量提高1.5-7.9倍，延迟降低2.0-3.5倍。

Conclusion: DC-AR在图像生成质量和计算效率上均达到领先水平。

Abstract: We introduce DC-AR, a novel masked autoregressive (AR) text-to-image generation framework that delivers superior image generation quality with exceptional computational efficiency. Due to the tokenizers' limitations, prior masked AR models have lagged behind diffusion models in terms of quality or efficiency. We overcome this limitation by introducing DC-HT - a deep compression hybrid tokenizer for AR models that achieves a 32x spatial compression ratio while maintaining high reconstruction fidelity and cross-resolution generalization ability. Building upon DC-HT, we extend MaskGIT and create a new hybrid masked autoregressive image generation framework that first produces the structural elements through discrete tokens and then applies refinements via residual tokens. DC-AR achieves state-of-the-art results with a gFID of 5.49 on MJHQ-30K and an overall score of 0.69 on GenEval, while offering 1.5-7.9x higher throughput and 2.0-3.5x lower latency compared to prior leading diffusion and autoregressive models.

</details>


### [62] [InterGSEdit: Interactive 3D Gaussian Splatting Editing with 3D Geometry-Consistent Attention Prior](https://arxiv.org/abs/2507.04961)
*Minghao Wen,Shengjie Wu,Kangkan Wang,Dong Liang*

Main category: cs.CV

TL;DR: InterGSEdit提出了一种基于交互式关键视图选择的3D高斯泼溅编辑框架，解决了多视图编辑中的局部不一致性问题，并通过CLIP语义一致性选择和注意力融合网络实现了高质量的3D编辑。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅编辑方法在多视图编辑中存在局部不一致性问题，且依赖文本提示的编辑方式缺乏灵活性。

Method: 提出CLIP语义一致性选择策略筛选参考视图，构建3D几何一致性注意力先验，并通过注意力融合网络结合2D和3D注意力。

Result: 实验表明，InterGSEdit在3D高斯泼溅编辑中实现了最先进的性能，提供了高质量且一致的编辑效果。

Conclusion: InterGSEdit通过交互式选择和注意力融合，显著提升了3D编辑的几何一致性和用户体验。

Abstract: 3D Gaussian Splatting based 3D editing has demonstrated impressive performance in recent years. However, the multi-view editing often exhibits significant local inconsistency, especially in areas of non-rigid deformation, which lead to local artifacts, texture blurring, or semantic variations in edited 3D scenes. We also found that the existing editing methods, which rely entirely on text prompts make the editing process a "one-shot deal", making it difficult for users to control the editing degree flexibly. In response to these challenges, we present InterGSEdit, a novel framework for high-quality 3DGS editing via interactively selecting key views with users' preferences. We propose a CLIP-based Semantic Consistency Selection (CSCS) strategy to adaptively screen a group of semantically consistent reference views for each user-selected key view. Then, the cross-attention maps derived from the reference views are used in a weighted Gaussian Splatting unprojection to construct the 3D Geometry-Consistent Attention Prior ($GAP^{3D}$). We project $GAP^{3D}$ to obtain 3D-constrained attention, which are fused with 2D cross-attention via Attention Fusion Network (AFN). AFN employs an adaptive attention strategy that prioritizes 3D-constrained attention for geometric consistency during early inference, and gradually prioritizes 2D cross-attention maps in diffusion for fine-grained features during the later inference. Extensive experiments demonstrate that InterGSEdit achieves state-of-the-art performance, delivering consistent, high-fidelity 3DGS editing with improved user experience.

</details>


### [63] [TLB-VFI: Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation](https://arxiv.org/abs/2507.04984)
*Zonglin Lyu,Chen Chen*

Main category: cs.CV

TL;DR: TLB-VFI是一种高效的基于视频的扩散模型，通过3D小波门控和时间感知自编码器提取时间信息，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有图像和视频扩散模型在视频帧插值任务中的效率和信息提取不足问题。

Method: 提出Temporal-Aware Latent Brownian Bridge Diffusion，结合3D-wavelet gating和temporal-aware autoencoder提取时间信息，并引入光流指导。

Result: 在最具挑战性的数据集上FID提升20%，参数减少3倍，速度提升2.3倍，训练数据需求减少9000倍。

Conclusion: TLB-VFI在性能和效率上均优于现有方法，为视频帧插值任务提供了高效解决方案。

Abstract: Video Frame Interpolation (VFI) aims to predict the intermediate frame $I_n$ (we use n to denote time in videos to avoid notation overload with the timestep $t$ in diffusion models) based on two consecutive neighboring frames $I_0$ and $I_1$. Recent approaches apply diffusion models (both image-based and video-based) in this task and achieve strong performance. However, image-based diffusion models are unable to extract temporal information and are relatively inefficient compared to non-diffusion methods. Video-based diffusion models can extract temporal information, but they are too large in terms of training scale, model size, and inference time. To mitigate the above issues, we propose Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI), an efficient video-based diffusion model. By extracting rich temporal information from video inputs through our proposed 3D-wavelet gating and temporal-aware autoencoder, our method achieves 20% improvement in FID on the most challenging datasets over recent SOTA of image-based diffusion models. Meanwhile, due to the existence of rich temporal information, our method achieves strong performance while having 3times fewer parameters. Such a parameter reduction results in 2.3x speed up. By incorporating optical flow guidance, our method requires 9000x less training data and achieves over 20x fewer parameters than video-based diffusion models. Codes and results are available at our project page: https://zonglinl.github.io/tlbvfi_page.

</details>


### [64] [Estimating Object Physical Properties from RGB-D Vision and Depth Robot Sensors Using Deep Learning](https://arxiv.org/abs/2507.05029)
*Ricardo Cardoso,Plinio Moreno*

Main category: cs.CV

TL;DR: 论文提出了一种结合稀疏点云数据和RGB图像来估计物体质量的新方法，显著优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 在机器人应用中，准确估计物体质量对任务性能至关重要，但目前仅依赖视觉传感器的质量估计研究较少。

Method: 使用ShapeNetSem 3D模型生成合成RGBD数据，训练图像生成模型以估计密集深度图，并结合现有数据集训练质量估计器。

Result: 该方法在所有评估指标上显著优于现有基准。

Conclusion: 结合点云和RGB数据的方法在物体质量估计任务中表现出色，且开源了数据和模型。

Abstract: Inertial mass plays a crucial role in robotic applications such as object grasping, manipulation, and simulation, providing a strong prior for planning and control. Accurately estimating an object's mass before interaction can significantly enhance the performance of various robotic tasks. However, mass estimation using only vision sensors is a relatively underexplored area. This paper proposes a novel approach combining sparse point-cloud data from depth images with RGB images to estimate the mass of objects. We evaluate a range of point-cloud processing architectures, alongside RGB-only methods. To overcome the limited availability of training data, we create a synthetic dataset using ShapeNetSem 3D models, simulating RGBD images via a Kinect camera. This synthetic data is used to train an image generation model for estimating dense depth maps, which we then use to augment an existing dataset of images paired with mass values. Our approach significantly outperforms existing benchmarks across all evaluated metrics. The data generation (https://github.com/RavineWindteer/ShapenetSem-to-RGBD) as well as the training of the depth estimator (https://github.com/RavineWindteer/GLPDepth-Edited) and the mass estimator (https://github.com/RavineWindteer/Depth-mass-estimator) are available online.

</details>


### [65] [AI-Driven Cytomorphology Image Synthesis for Medical Diagnostics](https://arxiv.org/abs/2507.05063)
*Jan Carreras Boada,Rao Muhammad Umer,Carsten Marr*

Main category: cs.CV

TL;DR: 使用微调的稳定扩散模型生成合成图像，显著提升单核白细胞分类器的性能，解决了生物医学数据不平衡和隐私限制问题。


<details>
  <summary>Details</summary>
Motivation: 生物医学数据集通常存在样本不平衡和隐私限制，阻碍了机器学习模型的开发。合成图像可以改善数据可用性并保护隐私，但生成高质量图像仍具挑战性。

Method: 通过微调的稳定扩散模型（使用LoRA权重）生成合成图像，并结合少量真实样本指导，用于训练ResNet和CLIP分类器。

Result: 添加5000张合成图像后，ResNet分类器准确率从27.3%提升至78.4%，CLIP分类器从61.8%提升至76.8%。合成图像与真实图像高度相似。

Conclusion: 合成图像是生物医学研究中克服数据限制、提升模型泛化能力的有效工具，有助于医学诊断和研究。

Abstract: Biomedical datasets often contain a large sample imbalance and are subject to strict privacy constraints, which together hinder the development of accurate machine learning models. One potential solution is to generate synthetic images, as this can improve data availability while preserving patient privacy. However, it remains difficult to generate synthetic images of sufficient quality for training robust classifiers. In this work, we focus on the classification of single white blood cells, a key component in the diagnosis of hematological diseases such as acute myeloid leukemia (AML), a severe blood cancer. We demonstrate how synthetic images generated with a fine-tuned stable diffusion model using LoRA weights when guided by real few-shot samples of the target white blood cell classes, can enhance classifier performance for limited data. When training a ResNet classifier, accuracy increased from 27.3\% to 78.4\% (+51.1\%) by adding 5000 synthetic images per class to a small and highly imbalanced real dataset. For a CLIP-based classifier, the accuracy improved from 61.8\% to 76.8\% (+15.0\%). The synthetic images are highly similar to real images, and they can help overcome dataset limitations, enhancing model generalization. Our results establish synthetic images as a tool in biomedical research, improving machine learning models, and facilitating medical diagnosis and research.

</details>


### [66] [ICAS: Detecting Training Data from Autoregressive Image Generative Models](https://arxiv.org/abs/2507.05068)
*Hongyao Yu,Yixiang Qiu,Yiheng Yang,Hao Fang,Tianqu Zhuang,Jiaxin Hong,Bin Chen,Hao Wu,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 该论文首次研究了自回归图像生成模型在成员推理攻击中的脆弱性，提出了一种基于隐式分类和自适应分数聚合的方法，并验证了其在数据隐私和版权保护中的有效性。


<details>
  <summary>Details</summary>
Motivation: 自回归图像生成模型的快速发展引发了数据隐私和版权问题，需要一种方法来检测未经授权的训练数据使用。

Method: 结合隐式分类和自适应分数聚合策略，计算查询图像的隐式分类分数，并通过自适应聚合生成最终分数以判断样本是否属于训练集。

Result: 实验验证了方法的有效性，特别是在类别条件和文本到图像场景中，并揭示了大型基础模型的脆弱性以及尺度视觉自回归模型数据的易检测性。

Conclusion: 该方法在数据隐私保护中具有潜力，并为自回归模型的脆弱性提供了新见解。

Abstract: Autoregressive image generation has witnessed rapid advancements, with prominent models such as scale-wise visual auto-regression pushing the boundaries of visual synthesis. However, these developments also raise significant concerns regarding data privacy and copyright. In response, training data detection has emerged as a critical task for identifying unauthorized data usage in model training. To better understand the vulnerability of autoregressive image generative models to such detection, we conduct the first study applying membership inference to this domain. Our approach comprises two key components: implicit classification and an adaptive score aggregation strategy. First, we compute the implicit token-wise classification score within the query image. Then we propose an adaptive score aggregation strategy to acquire a final score, which places greater emphasis on the tokens with lower scores. A higher final score indicates that the sample is more likely to be involved in the training set. To validate the effectiveness of our method, we adapt existing detection algorithms originally designed for LLMs to visual autoregressive models. Extensive experiments demonstrate the superiority of our method in both class-conditional and text-to-image scenarios. Moreover, our approach exhibits strong robustness and generalization under various data transformations. Furthermore, sufficient experiments suggest two novel key findings: (1) A linear scaling law on membership inference, exposing the vulnerability of large foundation models. (2) Training data from scale-wise visual autoregressive models is easier to detect than other autoregressive paradigms.Our code is available at https://github.com/Chrisqcwx/ImageAR-MIA.

</details>


### [67] [MoDiT: Learning Highly Consistent 3D Motion Coefficients with Diffusion Transformer for Talking Head Generation](https://arxiv.org/abs/2507.05092)
*Yucheng Wang,Dan Xu*

Main category: cs.CV

TL;DR: 论文提出了一种名为MoDiT的新框架，结合3DMM和基于扩散的Transformer，解决了音频驱动说话头生成中的时间抖动、身份漂移和不自然眨眼问题。


<details>
  <summary>Details</summary>
Motivation: 音频驱动说话头生成在虚拟助手、视频游戏和电影中至关重要，但现有方法存在时间抖动、身份漂移和不自然眨眼等问题。

Method: MoDiT框架结合3DMM和扩散Transformer，采用分层去噪策略、3DMM系数集成和改进的眨眼策略。

Result: 模型有效减少了时间抖动，保持了身份一致性，并生成了更自然的眨眼行为。

Conclusion: MoDiT框架在音频驱动说话头生成中表现出色，解决了现有方法的局限性。

Abstract: Audio-driven talking head generation is critical for applications such as virtual assistants, video games, and films, where natural lip movements are essential. Despite progress in this field, challenges remain in producing both consistent and realistic facial animations. Existing methods, often based on GANs or UNet-based diffusion models, face three major limitations: (i) temporal jittering caused by weak temporal constraints, resulting in frame inconsistencies; (ii) identity drift due to insufficient 3D information extraction, leading to poor preservation of facial identity; and (iii) unnatural blinking behavior due to inadequate modeling of realistic blink dynamics. To address these issues, we propose MoDiT, a novel framework that combines the 3D Morphable Model (3DMM) with a Diffusion-based Transformer. Our contributions include: (i) A hierarchical denoising strategy with revised temporal attention and biased self/cross-attention mechanisms, enabling the model to refine lip synchronization and progressively enhance full-face coherence, effectively mitigating temporal jittering. (ii) The integration of 3DMM coefficients to provide explicit spatial constraints, ensuring accurate 3D-informed optical flow prediction and improved lip synchronization using Wav2Lip results, thereby preserving identity consistency. (iii) A refined blinking strategy to model natural eye movements, with smoother and more realistic blinking behaviors.

</details>


### [68] [4DSloMo: 4D Reconstruction for High Speed Scene with Asynchronous Capture](https://arxiv.org/abs/2507.05163)
*Yutian Chen,Shi Guo,Tianshuo Yang,Lihe Ding,Xiuyuan Yu,Jinwei Gu,Tianfan Xue*

Main category: cs.CV

TL;DR: 提出一种利用低帧率相机实现高速4D场景重建的系统，通过异步捕获和生成模型提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有4D捕捉系统帧率低，直接重建高速运动效果不佳，需改进。

Method: 采用异步捕获方案提高有效帧率，结合视频扩散模型修复稀疏视图重建的伪影。

Result: 实验显示，该方法显著优于同步捕获，实现等效100-200 FPS的高质量重建。

Conclusion: 通过异步捕获和生成模型，成功实现低成本高速4D重建。

Abstract: Reconstructing fast-dynamic scenes from multi-view videos is crucial for high-speed motion analysis and realistic 4D reconstruction. However, the majority of 4D capture systems are limited to frame rates below 30 FPS (frames per second), and a direct 4D reconstruction of high-speed motion from low FPS input may lead to undesirable results. In this work, we propose a high-speed 4D capturing system only using low FPS cameras, through novel capturing and processing modules. On the capturing side, we propose an asynchronous capture scheme that increases the effective frame rate by staggering the start times of cameras. By grouping cameras and leveraging a base frame rate of 25 FPS, our method achieves an equivalent frame rate of 100-200 FPS without requiring specialized high-speed cameras. On processing side, we also propose a novel generative model to fix artifacts caused by 4D sparse-view reconstruction, as asynchrony reduces the number of viewpoints at each timestamp. Specifically, we propose to train a video-diffusion-based artifact-fix model for sparse 4D reconstruction, which refines missing details, maintains temporal consistency, and improves overall reconstruction quality. Experimental results demonstrate that our method significantly enhances high-speed 4D reconstruction compared to synchronous capture.

</details>


### [69] [Semantic Frame Interpolation](https://arxiv.org/abs/2507.05173)
*Yijia Hong,Jiangning Zhang,Ran Yi,Yuji Wang,Weijian Cao,Xiaobin Hu,Zhucun Xue,Yabiao Wang,Chengjie Wang,Lizhuang Ma*

Main category: cs.CV

TL;DR: 论文提出了一种新的语义帧插值（SFI）任务，并开发了SemFi模型和SFI-300K数据集，以解决传统帧插值任务在文本控制和多帧率生成上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统帧插值任务缺乏文本控制和多帧率支持，且生成效果受限于固定帧数。本文旨在填补这一空白。

Method: 基于Wan2.1构建SemFi模型，引入Mixture-of-LoRA模块，支持多帧率生成，并创建SFI-300K数据集和评估标准。

Result: 实验表明，SemFi在SFI任务中表现优异，尤其在一致性和多样性方面。

Conclusion: SemFi和SFI-300K为语义帧插值任务提供了有效解决方案，推动了该领域的发展。

Abstract: Generating intermediate video content of varying lengths based on given first and last frames, along with text prompt information, offers significant research and application potential. However, traditional frame interpolation tasks primarily focus on scenarios with a small number of frames, no text control, and minimal differences between the first and last frames. Recent community developers have utilized large video models represented by Wan to endow frame-to-frame capabilities. However, these models can only generate a fixed number of frames and often fail to produce satisfactory results for certain frame lengths, while this setting lacks a clear official definition and a well-established benchmark. In this paper, we first propose a new practical Semantic Frame Interpolation (SFI) task from the perspective of academic definition, which covers the above two settings and supports inference at multiple frame rates. To achieve this goal, we propose a novel SemFi model building upon Wan2.1, which incorporates a Mixture-of-LoRA module to ensure the generation of high-consistency content that aligns with control conditions across various frame length limitations. Furthermore, we propose SFI-300K, the first general-purpose dataset and benchmark specifically designed for SFI. To support this, we collect and process data from the perspective of SFI, carefully designing evaluation metrics and methods to assess the model's performance across multiple dimensions, encompassing image and video, and various aspects, including consistency and diversity. Through extensive experiments on SFI-300K, we demonstrate that our method is particularly well-suited to meet the requirements of the SFI task.

</details>


### [70] [SegmentDreamer: Towards High-fidelity Text-to-3D Synthesis with Segmented Consistency Trajectory Distillation](https://arxiv.org/abs/2507.05256)
*Jiahao Zhu,Zixuan Chen,Guangcong Wang,Xiaohua Xie,Yi Zhou*

Main category: cs.CV

TL;DR: SegmentDreamer通过Segmented Consistency Trajectory Distillation (SCTD)解决CD-based方法在text-to-3D生成中的条件指导不平衡问题，提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于CD的方法因自一致性和交叉一致性不平衡导致生成结果不理想，需要改进。

Method: 提出SCTD，重新定义自一致性和交叉一致性关系，并分割PF-ODE轨迹以确保段内一致性，降低蒸馏误差。

Result: SegmentDreamer在视觉质量上优于现有方法，支持高保真3D生成。

Conclusion: SegmentDreamer通过SCTD有效解决了条件指导不平衡问题，显著提升了text-to-3D生成的质量和效率。

Abstract: Recent advancements in text-to-3D generation improve the visual quality of Score Distillation Sampling (SDS) and its variants by directly connecting Consistency Distillation (CD) to score distillation. However, due to the imbalance between self-consistency and cross-consistency, these CD-based methods inherently suffer from improper conditional guidance, leading to sub-optimal generation results. To address this issue, we present SegmentDreamer, a novel framework designed to fully unleash the potential of consistency models for high-fidelity text-to-3D generation. Specifically, we reformulate SDS through the proposed Segmented Consistency Trajectory Distillation (SCTD), effectively mitigating the imbalance issues by explicitly defining the relationship between self- and cross-consistency. Moreover, SCTD partitions the Probability Flow Ordinary Differential Equation (PF-ODE) trajectory into multiple sub-trajectories and ensures consistency within each segment, which can theoretically provide a significantly tighter upper bound on distillation error. Additionally, we propose a distillation pipeline for a more swift and stable generation. Extensive experiments demonstrate that our SegmentDreamer outperforms state-of-the-art methods in visual quality, enabling high-fidelity 3D asset creation through 3D Gaussian Splatting (3DGS).

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [71] [Image-driven Robot Drawing with Rapid Lognormal Movements](https://arxiv.org/abs/2507.03166)
*Daniel Berio,Guillaume Clivaz,Michael Stroh,Oliver Deussen,Réjean Plamondon,Sylvain Calinon,Frederic Fol Leymarie*

Main category: cs.RO

TL;DR: 论文提出了一种结合人类手/臂运动模型和可微分渲染技术的方法，用于生成机器人绘画的自然动作轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模图像生成和视觉模型忽略了人类绘画/书写行为的物理特性，影响视觉效果和艺术家-机器人协作的直观性。

Method: 使用sigma-lognormal模型描述人类手/臂运动，并结合可微分矢量图形渲染器（DiffVG），通过梯度优化生成自然动作轨迹。

Result: 该方法成功生成了机器人可行的轨迹，并应用于合成涂鸦和图像抽象的生成与再现。

Conclusion: 通过结合图像驱动目标和最小时间平滑准则，该方法为机器人绘画提供了更自然和直观的解决方案。

Abstract: Large image generation and vision models, combined with differentiable rendering technologies, have become powerful tools for generating paths that can be drawn or painted by a robot. However, these tools often overlook the intrinsic physicality of the human drawing/writing act, which is usually executed with skillful hand/arm gestures. Taking this into account is important for the visual aesthetics of the results and for the development of closer and more intuitive artist-robot collaboration scenarios. We present a method that bridges this gap by enabling gradient-based optimization of natural human-like motions guided by cost functions defined in image space. To this end, we use the sigma-lognormal model of human hand/arm movements, with an adaptation that enables its use in conjunction with a differentiable vector graphics (DiffVG) renderer. We demonstrate how this pipeline can be used to generate feasible trajectories for a robot by combining image-driven objectives with a minimum-time smoothing criterion. We demonstrate applications with generation and robotic reproduction of synthetic graffiti as well as image abstraction.

</details>


### [72] [EmbodieDreamer: Advancing Real2Sim2Real Transfer for Policy Training via Embodied World Modeling](https://arxiv.org/abs/2507.05198)
*Boyuan Wang,Xinpan Meng,Xiaofeng Wang,Zheng Zhu,Angen Ye,Yang Wang,Zhiqin Yang,Chaojun Ni,Guan Huang,Xingang Wang*

Main category: cs.RO

TL;DR: EmbodieDreamer框架通过物理和视觉对齐模块减少Real2Sim2Real差距，提升机器人策略训练效果。


<details>
  <summary>Details</summary>
Motivation: 解决Embodied AI中大规模真实数据收集成本高、效率低的问题，以及仿真环境与真实世界在物理动态和视觉外观上的差距。

Method: 提出PhysAligner（可微分物理模块）优化物理参数，以及VisAligner（条件视频扩散模型）提升视觉逼真度。

Result: PhysAligner降低物理参数估计误差3.74%，优化速度提升89.91%；VisAligner使任务成功率提高29.17%。

Conclusion: EmbodieDreamer有效缩小Real2Sim2Real差距，提升机器人策略在真实世界的表现。

Abstract: The rapid advancement of Embodied AI has led to an increasing demand for large-scale, high-quality real-world data. However, collecting such embodied data remains costly and inefficient. As a result, simulation environments have become a crucial surrogate for training robot policies. Yet, the significant Real2Sim2Real gap remains a critical bottleneck, particularly in terms of physical dynamics and visual appearance. To address this challenge, we propose EmbodieDreamer, a novel framework that reduces the Real2Sim2Real gap from both the physics and appearance perspectives. Specifically, we propose PhysAligner, a differentiable physics module designed to reduce the Real2Sim physical gap. It jointly optimizes robot-specific parameters such as control gains and friction coefficients to better align simulated dynamics with real-world observations. In addition, we introduce VisAligner, which incorporates a conditional video diffusion model to bridge the Sim2Real appearance gap by translating low-fidelity simulated renderings into photorealistic videos conditioned on simulation states, enabling high-fidelity visual transfer. Extensive experiments validate the effectiveness of EmbodieDreamer. The proposed PhysAligner reduces physical parameter estimation error by 3.74% compared to simulated annealing methods while improving optimization speed by 89.91\%. Moreover, training robot policies in the generated photorealistic environment leads to a 29.17% improvement in the average task success rate across real-world tasks after reinforcement learning. Code, model and data will be publicly available.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [73] [Animation Needs Attention: A Holistic Approach to Slides Animation Comprehension with Visual-Language Models](https://arxiv.org/abs/2507.03916)
*Yifan Jiang,Yibo Xue,Yukun Kang,Pin Zheng,Jian Peng,Feiran Wu,Changliang Xu*

Main category: cs.AI

TL;DR: 论文提出了首个公开的幻灯片动画数据集，并利用LoRA微调Qwen-2.5-VL-7B模型，在动画生成任务中显著优于GPT-4.1和Gemini-2.5-Pro。


<details>
  <summary>Details</summary>
Motivation: 解决AI驱动的幻灯片生成工具缺乏动画支持的问题，以及现有视觉语言模型在动画任务中的局限性。

Method: 发布包含12,000个三元组（自然语言描述、动画JSON文件、渲染视频）的数据集，并使用LoRA微调Qwen-2.5-VL-7B模型。

Result: 模型在BLEU-4、ROUGE-L、SPICE和CODA指标上显著提升，尤其在CODA细节方面表现突出。

Conclusion: 数据集、LoRA模型和CODA指标为未来基于VLM的动态幻灯片生成研究提供了基准和基础。

Abstract: Slide animations, such as fade-ins, fly-ins, and wipes, are critical for audience engagement, efficient information delivery, and vivid visual expression. However, most AI-driven slide-generation tools still lack native animation support, and existing vision-language models (VLMs) struggle with animation tasks due to the absence of public datasets and limited temporal-reasoning capabilities. To address this gap, we release the first public dataset for slide-animation modeling: 12,000 triplets of natural-language descriptions, animation JSON files, and rendered videos, collectively covering every built-in PowerPoint effect. Using this resource, we fine-tune Qwen-2.5-VL-7B with Low-Rank Adaptation (LoRA) and achieve consistent improvements over GPT-4.1 and Gemini-2.5-Pro in BLEU-4, ROUGE-L, SPICE, and our Coverage-Order-Detail Assessment (CODA) metric, which evaluates action coverage, temporal order, and detail fidelity. On a manually curated test set of slides, the LoRA model increases BLEU-4 by around 60%, ROUGE-L by 30%, and shows significant improvements in CODA-detail. This demonstrates that low-rank adaptation enables reliable temporal reasoning and generalization beyond synthetic data. Overall, our dataset, LoRA-enhanced model, and CODA metric provide a rigorous benchmark and foundation for future research on VLM-based dynamic slide generation.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [74] [Differentiable High-Performance Ray Tracing-Based Simulation of Radio Propagation with Point Clouds](https://arxiv.org/abs/2507.04021)
*Niklas Vaara,Pekka Sangi,Miguel Bordallo López,Janne Heikkilä*

Main category: eess.SP

TL;DR: 提出了一种基于点云的可微分射线追踪无线电传播模拟器，展示了其高效性，并利用分割标签学习环境的电磁特性。


<details>
  <summary>Details</summary>
Motivation: 射线追踪是无线电传播模拟的常用方法，但其准确性依赖于环境模型的质量。计算机视觉和机器学习的进步使得重建带有语义分割标签的详细环境模型成为可能。

Method: 提出了一种直接操作点云的可微分射线追踪模拟器，模拟了多达五次反射和散射的多路径传播。

Result: 在两个室内场景中，模拟时间少于90毫秒，展示了方法的高效性。

Conclusion: 通过结合可微分电磁计算和分割标签，能够学习环境的电磁特性。

Abstract: Ray tracing is a widely used deterministic method for radio propagation simulations, capable of producing physically accurate multipath components. The accuracy depends on the quality of the environment model and its electromagnetic properties. Recent advances in computer vision and machine learning have made it possible to reconstruct detailed environment models augmented with semantic segmentation labels.   In this letter, we propose a differentiable ray tracing-based radio propagation simulator that operates directly on point clouds. We showcase the efficiency of our method by simulating multi-bounce propagation paths with up to five interactions with specular reflections and diffuse scattering in two indoor scenarios, each completing in less than 90 ms. Lastly, we demonstrate how the differentiability of electromagnetic computations can be combined with segmentation labels to learn the electromagnetic properties of the environment.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [75] [SPATIA: Multimodal Model for Prediction and Generation of Spatial Cell Phenotypes](https://arxiv.org/abs/2507.04704)
*Zhenglun Kong,Mufan Qiu,John Boesen,Xiang Lin,Sukwon Yun,Tianlong Chen,Manolis Kellis,Marinka Zitnik*

Main category: q-bio.QM

TL;DR: SPATIA是一个多尺度生成和预测模型，用于整合细胞形态、基因表达和空间背景，通过跨模态学习和空间依赖建模，在多个任务上优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 理解细胞形态、基因表达和空间组织如何共同影响组织功能是生物学中的核心问题，现有方法通常仅分析单一模态或分辨率有限。

Method: SPATIA通过跨注意力融合细胞形态和基因表达特征，使用Transformer模块在多个尺度（细胞、生态位、组织）上建模空间依赖，并通过生成扩散解码器合成高分辨率细胞图像。

Result: SPATIA在12个任务上优于13个基线模型，能够生成反映转录组扰动的真实细胞形态。

Conclusion: SPATIA为空间转录组学提供了统一的、空间感知的多尺度表示学习方法，具有广泛的应用潜力。

Abstract: Understanding how cellular morphology, gene expression, and spatial organization jointly shape tissue function is a central challenge in biology. Image-based spatial transcriptomics technologies now provide high-resolution measurements of cell images and gene expression profiles, but machine learning methods typically analyze these modalities in isolation or at limited resolution. We address the problem of learning unified, spatially aware representations that integrate cell morphology, gene expression, and spatial context across biological scales. This requires models that can operate at single-cell resolution, reason across spatial neighborhoods, and generalize to whole-slide tissue organization. Here, we introduce SPATIA, a multi-scale generative and predictive model for spatial transcriptomics. SPATIA learns cell-level embeddings by fusing image-derived morphological tokens and transcriptomic vector tokens using cross-attention and then aggregates them at niche and tissue levels using transformer modules to capture spatial dependencies. SPATIA incorporates token merging in its generative diffusion decoder to synthesize high-resolution cell images conditioned on gene expression. We assembled a multi-scale dataset consisting of 17 million cell-gene pairs, 1 million niche-gene pairs, and 10,000 tissue-gene pairs across 49 donors, 17 tissue types, and 12 disease states. We benchmark SPATIA against 13 existing models across 12 individual tasks, which span several categories including cell annotation, cell clustering, gene imputation, cross-modal prediction, and image generation. SPATIA achieves improved performance over all baselines and generates realistic cell morphologies that reflect transcriptomic perturbations.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [76] [EvRWKV: A RWKV Framework for Effective Event-guided Low-Light Image Enhancement](https://arxiv.org/abs/2507.03184)
*WenJie Cai,Qingguo Meng,Zhenyu Wang,Xingbo Dong,Zhe Jin*

Main category: eess.IV

TL;DR: EvRWKV是一种新型框架，通过双域处理和跨模态交互，有效解决低光条件下的图像增强问题，显著提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 低光条件下图像质量差（噪声、模糊、曝光不足）影响下游应用性能，传统方法效果有限，事件相机虽有潜力但现有融合方法存在不足。

Method: 提出EvRWKV框架，包含Cross-RWKV模块（用于精细时空和跨模态融合）和EISFE模块（自适应频域噪声抑制和空间域对齐）。

Result: 在真实低光数据集（SDE、SDSD、RELED）上表现优异，噪声抑制、细节恢复和视觉清晰度提升显著。

Conclusion: EvRWKV在低光图像增强中达到领先水平，为实际应用提供了有效解决方案。

Abstract: Capturing high-quality visual content under low-light conditions remains a challenging problem due to severe noise, motion blur, and underexposure, which degrade the performance of downstream applications. Traditional frame-based low-light enhancement methods often amplify noise or fail to preserve structural details, especially in real-world scenarios. Event cameras, offering high dynamic range and microsecond temporal resolution by asynchronously capturing brightness changes, emerge as promising alternatives for low-light imaging. However, existing event-image fusion methods suffer from simplistic fusion strategies and inadequate handling of spatial-temporal misalignment and noise. To address these challenges, we propose EvRWKV, a novel framework that enables continuous cross-modal interaction through dual-domain processing. Our approach incorporates a Cross-RWKV module, leveraging the Receptance Weighted Key Value (RWKV) architecture for fine-grained temporal and cross-modal fusion, and an Event Image Spectral Fusion Enhancer (EISFE) module, which jointly performs adaptive frequency-domain noise suppression and spatial-domain deformable convolution alignment. Extensive qualitative and quantitative evaluations on real-world low-light datasets(SDE, SDSD, RELED) demonstrate that EvRWKV achieves state-of-the-art performance, effectively enhancing image quality by suppressing noise, restoring structural details, and improving visual clarity in challenging low-light conditions.

</details>


### [77] [UltraDfeGAN: Detail-Enhancing Generative Adversarial Networks for High-Fidelity Functional Ultrasound Synthesis](https://arxiv.org/abs/2507.03341)
*Zhuo Li,Xuhang Chen,Shuqiang Wang*

Main category: eess.IV

TL;DR: 本文提出了一种基于GAN的fUS图像生成方法，通过架构改进提升生成图像的质量和生理合理性，并在下游任务中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: fUS技术在临床应用中面临数据稀缺和图像真实性不足的挑战，需要一种有效的图像生成方法。

Method: 采用GAN框架，结合特征增强模块和归一化技术，优化fUS图像合成。

Result: 实验表明该方法能生成高质量fUS图像，并在数据增强任务中提升分类准确性。

Conclusion: 该框架有效解决了fUS数据稀缺问题，为临床应用提供了支持。

Abstract: Functional ultrasound (fUS) is a neuroimaging technique known for its high spatiotemporal resolution, enabling non-invasive observation of brain activity through neurovascular coupling. Despite its potential in clinical applications such as neonatal monitoring and intraoperative guidance, the development of fUS faces challenges related to data scarcity and limitations in generating realistic fUS images. This paper explores the use of a generative adversarial network (GAN) framework tailored for fUS image synthesis. The proposed method incorporates architectural enhancements, including feature enhancement modules and normalization techniques, aiming to improve the fidelity and physiological plausibility of generated images. The study evaluates the performance of the framework against existing generative models, demonstrating its capability to produce high-quality fUS images under various experimental conditions. Additionally, the synthesized images are assessed for their utility in downstream tasks, showing improvements in classification accuracy when used for data augmentation. Experimental results are based on publicly available fUS datasets, highlighting the framework's effectiveness in addressing data limitations.

</details>


### [78] [EdgeSRIE: A hybrid deep learning framework for real-time speckle reduction and image enhancement on portable ultrasound systems](https://arxiv.org/abs/2507.03937)
*Hyunwoo Cho,Jongsoo Lee,Jinbum Kang,Yangmo Yoo*

Main category: eess.IV

TL;DR: EdgeSRIE是一种轻量级混合深度学习框架，用于便携式超声成像中的实时斑点减少和图像增强，具有低计算成本和高效性能。


<details>
  <summary>Details</summary>
Motivation: 超声图像中的斑点模式会掩盖解剖细节，导致诊断不确定性。现有深度学习方法计算成本高，难以在低资源设备上应用。

Method: EdgeSRIE包含两个分支：无监督去斑点分支和去模糊分支，网络量化后部署在低功耗SoC上。

Result: 在性能评估中，EdgeSRIE在CNR和AGM上优于其他基线方法，并实现60帧/秒的实时推理。

Conclusion: EdgeSRIE在资源有限环境下实现了实时高质量超声成像的可行性。

Abstract: Speckle patterns in ultrasound images often obscure anatomical details, leading to diagnostic uncertainty. Recently, various deep learning (DL)-based techniques have been introduced to effectively suppress speckle; however, their high computational costs pose challenges for low-resource devices, such as portable ultrasound systems. To address this issue, EdgeSRIE, which is a lightweight hybrid DL framework for real-time speckle reduction and image enhancement in portable ultrasound imaging, is introduced. The proposed framework consists of two main branches: an unsupervised despeckling branch, which is trained by minimizing a loss function between speckled images, and a deblurring branch, which restores blurred images to sharp images. For hardware implementation, the trained network is quantized to 8-bit integer precision and deployed on a low-resource system-on-chip (SoC) with limited power consumption. In the performance evaluation with phantom and in vivo analyses, EdgeSRIE achieved the highest contrast-to-noise ratio (CNR) and average gradient magnitude (AGM) compared with the other baselines (different 2-rule-based methods and other 4-DL-based methods). Furthermore, EdgeSRIE enabled real-time inference at over 60 frames per second while satisfying computational requirements (< 20K parameters) on actual portable ultrasound hardware. These results demonstrated the feasibility of EdgeSRIE for real-time, high-quality ultrasound imaging in resource-limited environments.

</details>


### [79] [Deep-Learning-Assisted Highly-Accurate COVID-19 Diagnosis on Lung Computed Tomography Images](https://arxiv.org/abs/2507.04252)
*Yinuo Wang,Juhyun Bae,Ka Ho Chow,Shenyang Chen,Shreyash Gupta*

Main category: eess.IV

TL;DR: 提出了一种基于GAN和滑动窗口的CT图像质量控制流程，并采用LDAM Loss和CB Loss解决数据长尾问题，模型在测试集上MCC超过0.983。


<details>
  <summary>Details</summary>
Motivation: COVID-19的CT诊断需要高质量的图像，但现有数据存在长尾分布问题，影响模型性能。

Method: 使用GAN和滑动窗口优化CT图像质量，结合LDAM Loss和CB Loss解决数据不平衡问题。

Result: 模型在基准测试数据集上的MCC达到0.983以上。

Conclusion: 提出的方法有效提升了CT图像质量和模型性能，为COVID-19诊断提供了可靠支持。

Abstract: COVID-19 is a severe and acute viral disease that can cause symptoms consistent with pneumonia in which inflammation is caused in the alveolous regions of the lungs leading to a build-up of fluid and breathing difficulties. Thus, the diagnosis of COVID using CT scans has been effective in assisting with RT-PCR diagnosis and severity classifications. In this paper, we proposed a new data quality control pipeline to refine the quality of CT images based on GAN and sliding windows. Also, we use class-sensitive cost functions including Label Distribution Aware Loss(LDAM Loss) and Class-balanced(CB) Loss to solve the long-tail problem existing in datasets. Our model reaches more than 0.983 MCC in the benchmark test dataset.

</details>


### [80] [FB-Diff: Fourier Basis-guided Diffusion for Temporal Interpolation of 4D Medical Imaging](https://arxiv.org/abs/2507.04547)
*Xin You,Runze Yang,Chuyan Zhang,Zhongliang Jiang,Jie Yang,Nassir Navab*

Main category: eess.IV

TL;DR: FB-Diff是一种基于傅里叶基的扩散模型，用于4D医学图像的时间插值任务，通过结合生理运动先验和频率信息，实现了更好的呼吸运动建模。


<details>
  <summary>Details</summary>
Motivation: 现有方法基于线性运动假设，但实际呼吸运动是非线性和准周期的，因此需要从频率角度解决时间插值问题。

Method: 提出FB-Diff模型，结合生理运动先验和傅里叶基，通过扩散模型生成中间帧。

Result: FB-Diff在感知性能和时序一致性上达到SOTA，同时保持良好重建指标。

Conclusion: FB-Diff为非线性呼吸运动建模提供了有效解决方案，代码已开源。

Abstract: The temporal interpolation task for 4D medical imaging, plays a crucial role in clinical practice of respiratory motion modeling. Following the simplified linear-motion hypothesis, existing approaches adopt optical flow-based models to interpolate intermediate frames. However, realistic respiratory motions should be nonlinear and quasi-periodic with specific frequencies. Intuited by this property, we resolve the temporal interpolation task from the frequency perspective, and propose a Fourier basis-guided Diffusion model, termed FB-Diff. Specifically, due to the regular motion discipline of respiration, physiological motion priors are introduced to describe general characteristics of temporal data distributions. Then a Fourier motion operator is elaborately devised to extract Fourier bases by incorporating physiological motion priors and case-specific spectral information in the feature space of Variational Autoencoder. Well-learned Fourier bases can better simulate respiratory motions with motion patterns of specific frequencies. Conditioned on starting and ending frames, the diffusion model further leverages well-learned Fourier bases via the basis interaction operator, which promotes the temporal interpolation task in a generative manner. Extensive results demonstrate that FB-Diff achieves state-of-the-art (SOTA) perceptual performance with better temporal consistency while maintaining promising reconstruction metrics. Codes are available.

</details>


### [81] [Sequential Attention-based Sampling for Histopathological Analysis](https://arxiv.org/abs/2507.05077)
*Tarun G,Naman Malpani,Gugan Thoppe,Sridharan Devarajan*

Main category: eess.IV

TL;DR: SASHA是一种基于深度强化学习的方法，用于高效分析组织病理学图像，通过智能采样和选择性放大实现可靠诊断，显著降低计算和内存成本。


<details>
  <summary>Details</summary>
Motivation: 全幻灯片图像（WSI）通常以千兆像素大小获取，计算上难以完全分析，且诊断标签通常仅在幻灯片级别可用，精细标注成本高。

Method: SASHA结合轻量级分层注意力多实例学习（MIL）模型学习特征，并通过智能采样选择性放大高分辨率区域（10-20%）。

Result: SASHA在计算和内存成本显著降低的情况下，性能与全高分辨率分析方法相当，并优于其他稀疏采样方法。

Conclusion: SASHA是一种适用于医学图像自动诊断的智能采样模型，特别适用于包含稀疏信息特征的大图像。

Abstract: Deep neural networks are increasingly applied for automated histopathology. Yet, whole-slide images (WSIs) are often acquired at gigapixel sizes, rendering it computationally infeasible to analyze them entirely at high resolution. Diagnostic labels are largely available only at the slide-level, because expert annotation of images at a finer (patch) level is both laborious and expensive. Moreover, regions with diagnostic information typically occupy only a small fraction of the WSI, making it inefficient to examine the entire slide at full resolution. Here, we propose SASHA -- {\it S}equential {\it A}ttention-based {\it S}ampling for {\it H}istopathological {\it A}nalysis -- a deep reinforcement learning approach for efficient analysis of histopathological images. First, SASHA learns informative features with a lightweight hierarchical, attention-based multiple instance learning (MIL) model. Second, SASHA samples intelligently and zooms selectively into a small fraction (10-20\%) of high-resolution patches, to achieve reliable diagnosis. We show that SASHA matches state-of-the-art methods that analyze the WSI fully at high-resolution, albeit at a fraction of their computational and memory costs. In addition, it significantly outperforms competing, sparse sampling methods. We propose SASHA as an intelligent sampling model for medical imaging challenges that involve automated diagnosis with exceptionally large images containing sparsely informative features.

</details>


### [82] [SV-DRR: High-Fidelity Novel View X-Ray Synthesis Using Diffusion Model](https://arxiv.org/abs/2507.05148)
*Chun Xie,Yuichi Yoshii,Itaru Kitahara*

Main category: eess.IV

TL;DR: 提出了一种基于扩散变换器的单视角X射线图像生成多视角图像的方法，解决了传统方法在角度范围、分辨率和图像质量上的限制。


<details>
  <summary>Details</summary>
Motivation: 多视角X射线成像虽能提供互补信息，但会增加辐射暴露和临床工作流程复杂性，因此需要一种高效的单视角生成方法。

Method: 采用扩散变换器（Diffusion Transformer）和弱到强训练策略，以生成高分辨率、多视角的X射线图像。

Result: 实验结果表明，该方法能生成更高分辨率、视角可控的多视角图像，优于现有方法。

Conclusion: 该方法在临床、医学教育和数据扩展方面具有重要应用潜力，可生成高质量多样化数据集。

Abstract: X-ray imaging is a rapid and cost-effective tool for visualizing internal human anatomy. While multi-view X-ray imaging provides complementary information that enhances diagnosis, intervention, and education, acquiring images from multiple angles increases radiation exposure and complicates clinical workflows. To address these challenges, we propose a novel view-conditioned diffusion model for synthesizing multi-view X-ray images from a single view. Unlike prior methods, which are limited in angular range, resolution, and image quality, our approach leverages the Diffusion Transformer to preserve fine details and employs a weak-to-strong training strategy for stable high-resolution image generation. Experimental results demonstrate that our method generates higher-resolution outputs with improved control over viewing angles. This capability has significant implications not only for clinical applications but also for medical education and data extension, enabling the creation of diverse, high-quality datasets for training and analysis. Our code is available at GitHub.

</details>
