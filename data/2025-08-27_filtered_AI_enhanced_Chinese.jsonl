{"id": "2508.18540", "pdf": "https://arxiv.org/pdf/2508.18540", "abs": "https://arxiv.org/abs/2508.18540", "authors": ["Jonghyun Kim", "Cheng Sun", "Michael Stengel", "Matthew Chan", "Andrew Russell", "Jaehyun Jung", "Wil Braithwaite", "Shalini De Mello", "David Luebke"], "title": "Real-time 3D Visualization of Radiance Fields on Light Field Displays", "categories": ["cs.GR", "eess.IV"], "comment": "10 pages, 14 figures. J. Kim, C. Sun, and M. Stengel contributed   equally", "summary": "Radiance fields have revolutionized photo-realistic 3D scene visualization by enabling high-fidelity reconstruction of complex environments, making them an ideal match for light field displays. However, integrating these technologies presents significant computational challenges, as light field displays require multiple high-resolution renderings from slightly shifted viewpoints, while radiance fields rely on computationally intensive volume rendering. In this paper, we propose a unified and efficient framework for real-time radiance field rendering on light field displays. Our method supports a wide range of radiance field representations, including NeRFs, 3D Gaussian Splatting, and Sparse Voxels, within a shared architecture based on a single-pass plane sweeping strategy and caching of shared, non-directional components. The framework generalizes across different scene formats without retraining, and avoids redundant computation across views. We further demonstrate a real-time interactive application on a Looking Glass display, achieving 200+ FPS at 512p across 45 views, enabling seamless, immersive 3D interaction. On standard benchmarks, our method achieves up to 22x speedup compared to independently rendering each view, while preserving image quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u9ad8\u6548\u6846\u67b6\uff0c\u7528\u4e8e\u5149\u573a\u663e\u793a\u5668\u4e0a\u7684\u5b9e\u65f6\u8f90\u5c04\u573a\u6e32\u67d3\uff0c\u652f\u6301\u591a\u79cd\u8f90\u5c04\u573a\u8868\u793a\uff0c\u5b9e\u73b045\u89c6\u89d2\u4e0b200+FPS\u7684\u5b9e\u65f6\u4ea4\u4e92", "motivation": "\u8f90\u5c04\u573a\u6280\u672f\u867d\u7136\u80fd\u591f\u5b9e\u73b0\u9ad8\u8d28\u91cf3D\u573a\u666f\u91cd\u5efa\uff0c\u4f46\u5c06\u5176\u4e0e\u5149\u573a\u663e\u793a\u5668\u96c6\u6210\u9762\u4e34\u5de8\u5927\u8ba1\u7b97\u6311\u6218\uff0c\u56e0\u4e3a\u5149\u573a\u663e\u793a\u5668\u9700\u8981\u4ece\u591a\u4e2a\u89c6\u89d2\u8fdb\u884c\u9ad8\u5206\u8fa8\u7387\u6e32\u67d3\uff0c\u800c\u8f90\u5c04\u573a\u6e32\u67d3\u672c\u8eab\u8ba1\u7b97\u5bc6\u96c6", "method": "\u57fa\u4e8e\u5355\u6b21\u5e73\u9762\u626b\u63cf\u7b56\u7565\u548c\u5171\u4eab\u975e\u65b9\u5411\u6027\u7ec4\u4ef6\u7f13\u5b58\u7684\u7edf\u4e00\u67b6\u6784\uff0c\u652f\u6301NeRFs\u30013D\u9ad8\u65af\u6cfc\u6e85\u548c\u7a00\u758f\u4f53\u7d20\u7b49\u591a\u79cd\u8f90\u5c04\u573a\u8868\u793a\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u6cdb\u5316\u5230\u4e0d\u540c\u573a\u666f\u683c\u5f0f", "result": "\u5728Looking Glass\u663e\u793a\u5668\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u4ea4\u4e92\u5e94\u7528\uff0c45\u4e2a\u89c6\u89d2512p\u5206\u8fa8\u7387\u4e0b\u8fbe\u5230200+FPS\uff0c\u76f8\u6bd4\u72ec\u7acb\u6e32\u67d3\u6bcf\u4e2a\u89c6\u89d2\u5b9e\u73b0\u6700\u9ad822\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u8f90\u5c04\u573a\u5728\u5149\u573a\u663e\u793a\u5668\u4e0a\u7684\u5b9e\u65f6\u6e32\u67d3\u95ee\u9898\uff0c\u4e3a\u6c89\u6d78\u5f0f3D\u4ea4\u4e92\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u548c\u4f18\u5f02\u7684\u6027\u80fd\u8868\u73b0"}}
{"id": "2508.18597", "pdf": "https://arxiv.org/pdf/2508.18597", "abs": "https://arxiv.org/abs/2508.18597", "authors": ["Xiaohao Sun", "Divyam Goel", "Angle X. Chang"], "title": "SemLayoutDiff: Semantic Layout Generation with Diffusion Model for Indoor Scene Synthesis", "categories": ["cs.GR", "cs.CV"], "comment": "Project page: https://3dlg-hcvc.github.io/SemLayoutDiff/", "summary": "We present SemLayoutDiff, a unified model for synthesizing diverse 3D indoor scenes across multiple room types. The model introduces a scene layout representation combining a top-down semantic map and attributes for each object. Unlike prior approaches, which cannot condition on architectural constraints, SemLayoutDiff employs a categorical diffusion model capable of conditioning scene synthesis explicitly on room masks. It first generates a coherent semantic map, followed by a cross-attention-based network to predict furniture placements that respect the synthesized layout. Our method also accounts for architectural elements such as doors and windows, ensuring that generated furniture arrangements remain practical and unobstructed. Experiments on the 3D-FRONT dataset show that SemLayoutDiff produces spatially coherent, realistic, and varied scenes, outperforming previous methods.", "AI": {"tldr": "SemLayoutDiff\u662f\u4e00\u4e2a\u7edf\u4e00\u76843D\u5ba4\u5185\u573a\u666f\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u5730\u56fe\u548c\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u6839\u636e\u623f\u95f4\u63a9\u7801\u7ea6\u675f\u751f\u6210\u591a\u6837\u5316\u7684\u5bb6\u5177\u5e03\u5c40\u3002", "motivation": "\u73b0\u6709\u76843D\u573a\u666f\u751f\u6210\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u8003\u8651\u5efa\u7b51\u7ea6\u675f\u6761\u4ef6\uff08\u5982\u95e8\u7a97\u4f4d\u7f6e\uff09\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u5bb6\u5177\u5e03\u5c40\u53ef\u80fd\u4e0d\u5b9e\u7528\u6216\u88ab\u963b\u6321\u3002", "method": "\u63d0\u51fa\u7ed3\u5408\u81ea\u4e0a\u800c\u4e0b\u8bed\u4e49\u5730\u56fe\u548c\u5bf9\u8c61\u5c5e\u6027\u7684\u573a\u666f\u8868\u793a\uff0c\u4f7f\u7528\u5206\u7c7b\u6269\u6563\u6a21\u578b\u751f\u6210\u8bed\u4e49\u5730\u56fe\uff0c\u518d\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u7f51\u7edc\u9884\u6d4b\u5bb6\u5177\u4f4d\u7f6e\uff0c\u540c\u65f6\u8003\u8651\u95e8\u7a97\u7b49\u5efa\u7b51\u5143\u7d20\u3002", "result": "\u57283D-FRONT\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u7a7a\u95f4\u8fde\u8d2f\u3001\u771f\u5b9e\u4e14\u591a\u6837\u7684\u573a\u666f\uff0c\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "SemLayoutDiff\u901a\u8fc7\u663e\u5f0f\u8003\u8651\u5efa\u7b51\u7ea6\u675f\u6761\u4ef6\uff0c\u5b9e\u73b0\u4e86\u66f4\u5b9e\u7528\u548c\u5408\u7406\u76843D\u5ba4\u5185\u573a\u666f\u5408\u6210\u3002"}}
{"id": "2508.18389", "pdf": "https://arxiv.org/pdf/2508.18389", "abs": "https://arxiv.org/abs/2508.18389", "authors": ["Hao Liang", "Zhixuan Ge", "Ashish Tiwari", "Soumendu Majee", "G. M. Dilshan Godaliyadda", "Ashok Veeraraghavan", "Guha Balakrishnan"], "title": "FastAvatar: Instant 3D Gaussian Splatting for Faces from Single Unconstrained Poses", "categories": ["cs.CV"], "comment": "11 pages, 5 figures", "summary": "We present FastAvatar, a pose-invariant, feed-forward framework that can generate a 3D Gaussian Splatting (3DGS) model from a single face image from an arbitrary pose in near-instant time (<10ms). FastAvatar uses a novel encoder-decoder neural network design to achieve both fast fitting and identity preservation regardless of input pose. First, FastAvatar constructs a 3DGS face ``template'' model from a training dataset of faces with multi-view captures. Second, FastAvatar encodes the input face image into an identity-specific and pose-invariant latent embedding, and decodes this embedding to predict residuals to the structural and appearance parameters of each Gaussian in the template 3DGS model. By only inferring residuals in a feed-forward fashion, model inference is fast and robust. FastAvatar significantly outperforms existing feed-forward face 3DGS methods (e.g., GAGAvatar) in reconstruction quality, and runs 1000x faster than per-face optimization methods (e.g., FlashAvatar, GaussianAvatars and GASP). In addition, FastAvatar's novel latent space design supports real-time identity interpolation and attribute editing which is not possible with any existing feed-forward 3DGS face generation framework. FastAvatar's combination of excellent reconstruction quality and speed expands the scope of 3DGS for photorealistic avatar applications in consumer and interactive systems.", "AI": {"tldr": "FastAvatar\u662f\u4e00\u4e2a\u5feb\u901f\u524d\u9988\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u4efb\u610f\u59ff\u6001\u7684\u5355\u5f20\u4eba\u8138\u56fe\u50cf\u572810\u6beb\u79d2\u5185\u751f\u62103D\u9ad8\u65af\u6cfc\u6e85\u6a21\u578b\uff0c\u5b9e\u73b0\u59ff\u6001\u4e0d\u53d8\u7684\u8eab\u4efd\u4fdd\u6301\u548c\u5b9e\u65f6\u8eab\u4efd\u63d2\u503c\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u81383D\u91cd\u5efa\u65b9\u6cd5\u8981\u4e48\u901f\u5ea6\u6162\uff08\u9700\u8981\u9010\u4eba\u8138\u4f18\u5316\uff09\uff0c\u8981\u4e48\u91cd\u5efa\u8d28\u91cf\u4e0d\u4f73\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u8d28\u91cf\u91cd\u5efa\u53c8\u80fd\u5b9e\u73b0\u5b9e\u65f6\u63a8\u7406\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\uff0c\u5148\u6784\u5efa3DGS\u4eba\u8138\u6a21\u677f\u6a21\u578b\uff0c\u7136\u540e\u5c06\u8f93\u5165\u56fe\u50cf\u7f16\u7801\u4e3a\u8eab\u4efd\u7279\u5b9a\u7684\u59ff\u6001\u4e0d\u53d8\u6f5c\u5728\u5d4c\u5165\uff0c\u89e3\u7801\u9884\u6d4b\u6a21\u677f\u9ad8\u65af\u53c2\u6570\u6b8b\u5dee\u3002", "result": "\u5728\u91cd\u5efa\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u524d\u9988\u65b9\u6cd5\uff08\u5982GAGAvatar\uff09\uff0c\u901f\u5ea6\u6bd4\u9010\u4eba\u8138\u4f18\u5316\u65b9\u6cd5\u5feb1000\u500d\uff0c\u652f\u6301\u5b9e\u65f6\u8eab\u4efd\u63d2\u503c\u548c\u5c5e\u6027\u7f16\u8f91\u3002", "conclusion": "FastAvatar\u7ed3\u5408\u4e86\u4f18\u79c0\u7684\u91cd\u5efa\u8d28\u91cf\u548c\u901f\u5ea6\uff0c\u6269\u5c55\u4e863D\u9ad8\u65af\u6cfc\u6e85\u5728\u6d88\u8d39\u7ea7\u548c\u4ea4\u4e92\u5f0f\u7cfb\u7edf\u4e2d\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u5934\u50cf\u5e94\u7528\u7684\u8303\u56f4\u3002"}}
{"id": "2508.19140", "pdf": "https://arxiv.org/pdf/2508.19140", "abs": "https://arxiv.org/abs/2508.19140", "authors": ["Florian Hahlbohm", "Linus Franke", "Leon Overk\u00e4mping", "Paula Wespe", "Susana Castillo", "Martin Eisemann", "Marcus Magnor"], "title": "A Bag of Tricks for Efficient Implicit Neural Point Clouds", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Project page: https://fhahlbohm.github.io/inpc_v2/", "summary": "Implicit Neural Point Cloud (INPC) is a recent hybrid representation that combines the expressiveness of neural fields with the efficiency of point-based rendering, achieving state-of-the-art image quality in novel view synthesis. However, as with other high-quality approaches that query neural networks during rendering, the practical usability of INPC is limited by comparatively slow rendering. In this work, we present a collection of optimizations that significantly improve both the training and inference performance of INPC without sacrificing visual fidelity. The most significant modifications are an improved rasterizer implementation, more effective sampling techniques, and the incorporation of pre-training for the convolutional neural network used for hole-filling. Furthermore, we demonstrate that points can be modeled as small Gaussians during inference to further improve quality in extrapolated, e.g., close-up views of the scene. We design our implementations to be broadly applicable beyond INPC and systematically evaluate each modification in a series of experiments. Our optimized INPC pipeline achieves up to 25% faster training, 2x faster rendering, and 20% reduced VRAM usage paired with slight image quality improvements.", "AI": {"tldr": "\u901f\u5ea6\u4f18\u5316\u9690\u5f0f\u795e\u7ecf\u70b9\u4e91(INPC)\u8868\u793a\uff0c\u901a\u8fc7\u6539\u8fdb\u5149\u6805\u5316\u5668\u3001\u6837\u672c\u91c7\u96c6\u548c\u9884\u8bad\u7ec3\u7b49\u6280\u672f\uff0c\u5b9e\u73b0\u8bad\u7ec3\u901f\u5ea6\u63d0\u534725%\uff0c\u6e32\u67d3\u901f\u5ea6\u63d0\u53472\u500d\uff0cVRAM\u4f7f\u7528\u91cf\u51cf\u5c1120%\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf", "motivation": "INPC\u7ed3\u5408\u4e86\u795e\u7ecf\u573a\u7684\u8868\u8fbe\u529b\u548c\u70b9\u57fa\u6e32\u67d3\u7684\u6548\u7387\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9047\u5230\u6e32\u67d3\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u5176\u5b9e\u7528\u6027", "method": "\u91c7\u7528\u4e09\u4e2a\u4e3b\u8981\u4f18\u5316\u6280\u672f\uff1a1)\u6539\u8fdb\u7684\u5149\u6805\u5316\u5668\u5b9e\u73b0 2)\u66f4\u6709\u6548\u7684\u6837\u672c\u91c7\u96c6\u6280\u672f 3)\u4e3a\u6d1e\u6d1e\u586b\u5145\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6dfb\u52a0\u9884\u8bad\u7ec3\u3002\u8fd8\u5c06\u70b9\u6a21\u578b\u5316\u4e3a\u5c0f\u9ad8\u65af\u5206\u5e03\u6765\u63d0\u5347\u5916\u63a8\u89c6\u89d2\u7684\u8d28\u91cf", "result": "\u4f18\u5316\u540e\u7684INPC\u6d41\u6c34\u7ebf\u5b9e\u73b0\u4e86\u8bad\u7ec3\u901f\u5ea6\u63d0\u534725%\uff0c\u6e32\u67d3\u901f\u5ea6\u63d0\u53472\u500d\uff0cVRAM\u4f7f\u7528\u91cf\u51cf\u5c1120%\uff0c\u540c\u65f6\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u6027\u80fd\u4f18\u5316\u65b9\u6848\uff0cINPC\u5728\u4fdd\u6301\u9ad8\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u4e86\u8bad\u7ec3\u548c\u6e32\u67d3\u6548\u7387\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u6027\u3002\u8fd9\u4e9b\u4f18\u5316\u6280\u672f\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027"}}
{"id": "2508.18425", "pdf": "https://arxiv.org/pdf/2508.18425", "abs": "https://arxiv.org/abs/2508.18425", "authors": ["Lucas Wojcik", "Gabriel E. Lima", "Valfride Nascimento", "Eduil Nascimento Jr.", "Rayson Laroca", "David Menotti"], "title": "LPLC: A Dataset for License Plate Legibility Classification", "categories": ["cs.CV"], "comment": "Accepted for presentation at the Conference on Graphics, Patterns and   Images (SIBGRAPI) 2025", "summary": "Automatic License Plate Recognition (ALPR) faces a major challenge when dealing with illegible license plates (LPs). While reconstruction methods such as super-resolution (SR) have emerged, the core issue of recognizing these low-quality LPs remains unresolved. To optimize model performance and computational efficiency, image pre-processing should be applied selectively to cases that require enhanced legibility. To support research in this area, we introduce a novel dataset comprising 10,210 images of vehicles with 12,687 annotated LPs for legibility classification (the LPLC dataset). The images span a wide range of vehicle types, lighting conditions, and camera/image quality levels. We adopt a fine-grained annotation strategy that includes vehicle- and LP-level occlusions, four legibility categories (perfect, good, poor, and illegible), and character labels for three categories (excluding illegible LPs). As a benchmark, we propose a classification task using three image recognition networks to determine whether an LP image is good enough, requires super-resolution, or is completely unrecoverable. The overall F1 score, which remained below 80% for all three baseline models (ViT, ResNet, and YOLO), together with the analyses of SR and LP recognition methods, highlights the difficulty of the task and reinforces the need for further research. The proposed dataset is publicly available at https://github.com/lmlwojcik/lplc-dataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u8f66\u724c\u53ef\u8bfb\u6027\u5206\u7c7b\u6570\u636e\u96c6LPLC\uff0c\u5305\u542b10,210\u5f20\u8f66\u8f86\u56fe\u50cf\u548c12,687\u4e2a\u6807\u6ce8\u8f66\u724c\uff0c\u7528\u4e8e\u5224\u65ad\u8f66\u724c\u662f\u5426\u9700\u8981\u8d85\u5206\u8fa8\u7387\u5904\u7406\u6216\u662f\u5426\u5b8c\u5168\u4e0d\u53ef\u8bfb\u3002\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u73b0\u6709\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff08F1\u5206\u6570\u4f4e\u4e8e80%\uff09\uff0c\u51f8\u663e\u4e86\u8be5\u4efb\u52a1\u7684\u6311\u6218\u6027\u3002", "motivation": "\u81ea\u52a8\u8f66\u724c\u8bc6\u522b\u7cfb\u7edf\u5728\u5904\u7406\u4f4e\u8d28\u91cf\u8f66\u724c\u65f6\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u65e0\u6cd5\u6839\u672c\u89e3\u51b3\u95ee\u9898\u3002\u9700\u8981\u9009\u62e9\u6027\u5bf9\u9700\u8981\u589e\u5f3a\u53ef\u8bfb\u6027\u7684\u8f66\u724c\u8fdb\u884c\u9884\u5904\u7406\uff0c\u4f46\u7f3a\u4e4f\u4e13\u95e8\u7684\u6570\u636e\u96c6\u652f\u6301\u76f8\u5173\u7814\u7a76\u3002", "method": "\u6784\u5efa\u5305\u542b10,210\u5f20\u8f66\u8f86\u56fe\u50cf\u548c12,687\u4e2a\u6807\u6ce8\u8f66\u724c\u7684\u65b0\u6570\u636e\u96c6\uff0c\u91c7\u7528\u7ec6\u7c92\u5ea6\u6807\u6ce8\u7b56\u7565\uff08\u5305\u62ec\u906e\u6321\u60c5\u51b5\u30014\u4e2a\u53ef\u8bfb\u6027\u7b49\u7ea7\u548c\u5b57\u7b26\u6807\u7b7e\uff09\u3002\u4f7f\u7528ViT\u3001ResNet\u548cYOLO\u4e09\u79cd\u56fe\u50cf\u8bc6\u522b\u7f51\u7edc\u4f5c\u4e3a\u57fa\u51c6\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\u4efb\u52a1\u3002", "result": "\u6240\u6709\u4e09\u4e2a\u57fa\u51c6\u6a21\u578b\u7684\u6574\u4f53F1\u5206\u6570\u90fd\u4f4e\u4e8e80%\uff0c\u8868\u660e\u8be5\u4efb\u52a1\u5177\u6709\u76f8\u5f53\u96be\u5ea6\u3002\u8d85\u5206\u8fa8\u7387\u548c\u8f66\u724c\u8bc6\u522b\u65b9\u6cd5\u7684\u5206\u6790\u7ed3\u679c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u4efb\u52a1\u7684\u6311\u6218\u6027\u3002", "conclusion": "\u8f66\u724c\u53ef\u8bfb\u6027\u5206\u7c7b\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u3002\u63d0\u51fa\u7684LPLC\u6570\u636e\u96c6\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u9009\u62e9\u6027\u56fe\u50cf\u9884\u5904\u7406\u65b9\u6cd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.18621", "pdf": "https://arxiv.org/pdf/2508.18621", "abs": "https://arxiv.org/abs/2508.18621", "authors": ["Xin Gao", "Li Hu", "Siqi Hu", "Mingyang Huang", "Chaonan Ji", "Dechao Meng", "Jinwei Qi", "Penchong Qiao", "Zhen Shen", "Yafei Song", "Ke Sun", "Linrui Tian", "Guangyuan Wang", "Qi Wang", "Zhongjian Wang", "Jiayu Xiao", "Sheng Xu", "Bang Zhang", "Peng Zhang", "Xindi Zhang", "Zhe Zhang", "Jingren Zhou", "Lian Zhuo"], "title": "Wan-S2V: Audio-Driven Cinematic Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Current state-of-the-art (SOTA) methods for audio-driven character animation demonstrate promising performance for scenarios primarily involving speech and singing. However, they often fall short in more complex film and television productions, which demand sophisticated elements such as nuanced character interactions, realistic body movements, and dynamic camera work. To address this long-standing challenge of achieving film-level character animation, we propose an audio-driven model, which we refere to as Wan-S2V, built upon Wan. Our model achieves significantly enhanced expressiveness and fidelity in cinematic contexts compared to existing approaches. We conducted extensive experiments, benchmarking our method against cutting-edge models such as Hunyuan-Avatar and Omnihuman. The experimental results consistently demonstrate that our approach significantly outperforms these existing solutions. Additionally, we explore the versatility of our method through its applications in long-form video generation and precise video lip-sync editing.", "AI": {"tldr": "Wan-S2V\u662f\u4e00\u4e2a\u57fa\u4e8eWan\u6784\u5efa\u7684\u97f3\u9891\u9a71\u52a8\u89d2\u8272\u52a8\u753b\u6a21\u578b\uff0c\u5728\u7535\u5f71\u7ea7\u52a8\u753b\u5236\u4f5c\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u89d2\u8272\u4e92\u52a8\u3001\u8eab\u4f53\u52a8\u4f5c\u548c\u955c\u5934\u8fd0\u52a8\u7b49\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u97f3\u9891\u9a71\u52a8\u89d2\u8272\u52a8\u753b\u65b9\u6cd5\u4e3b\u8981\u9002\u7528\u4e8e\u8bed\u97f3\u548c\u6b4c\u5531\u573a\u666f\uff0c\u4f46\u5728\u9700\u8981\u590d\u6742\u89d2\u8272\u4e92\u52a8\u3001\u7ec6\u817b\u8eab\u4f53\u52a8\u4f5c\u548c\u52a8\u6001\u955c\u5934\u5de5\u4f5c\u7684\u5f71\u89c6\u5236\u4f5c\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6ee1\u8db3\u7535\u5f71\u7ea7\u52a8\u753b\u7684\u9700\u6c42\u3002", "method": "\u57fa\u4e8eWan\u6846\u67b6\u6784\u5efaWan-S2V\u97f3\u9891\u9a71\u52a8\u6a21\u578b\uff0c\u901a\u8fc7\u589e\u5f3a\u8868\u73b0\u529b\u548c\u4fdd\u771f\u5ea6\u6765\u9002\u5e94\u7535\u5f71\u7ea7\u52a8\u753b\u5236\u4f5c\u9700\u6c42\uff0c\u652f\u6301\u957f\u89c6\u9891\u751f\u6210\u548c\u7cbe\u786e\u7684\u53e3\u578b\u540c\u6b65\u7f16\u8f91\u3002", "result": "\u5728Hunyuan-Avatar\u548cOmnihuman\u7b49\u5148\u8fdb\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cWan-S2V\u65b9\u6cd5\u59cb\u7ec8\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u7535\u5f71\u7ea7\u52a8\u753b\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "Wan-S2V\u6210\u529f\u89e3\u51b3\u4e86\u7535\u5f71\u7ea7\u89d2\u8272\u52a8\u753b\u7684\u957f\u4e45\u6311\u6218\uff0c\u4e3a\u590d\u6742\u5f71\u89c6\u5236\u4f5c\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u97f3\u9891\u9a71\u52a8\u52a8\u753b\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.18633", "pdf": "https://arxiv.org/pdf/2508.18633", "abs": "https://arxiv.org/abs/2508.18633", "authors": ["Chenxuan Miao", "Yutong Feng", "Jianshu Zeng", "Zixiang Gao", "Hantang Liu", "Yunfeng Yan", "Donglian Qi", "Xi Chen", "Bin Wang", "Hengshuang Zhao"], "title": "ROSE: Remove Objects with Side Effects in Videos", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Video object removal has achieved advanced performance due to the recent success of video generative models. However, when addressing the side effects of objects, e.g., their shadows and reflections, existing works struggle to eliminate these effects for the scarcity of paired video data as supervision. This paper presents ROSE, termed Remove Objects with Side Effects, a framework that systematically studies the object's effects on environment, which can be categorized into five common cases: shadows, reflections, light, translucency and mirror. Given the challenges of curating paired videos exhibiting the aforementioned effects, we leverage a 3D rendering engine for synthetic data generation. We carefully construct a fully-automatic pipeline for data preparation, which simulates a large-scale paired dataset with diverse scenes, objects, shooting angles, and camera trajectories. ROSE is implemented as an video inpainting model built on diffusion transformer. To localize all object-correlated areas, the entire video is fed into the model for reference-based erasing. Moreover, additional supervision is introduced to explicitly predict the areas affected by side effects, which can be revealed through the differential mask between the paired videos. To fully investigate the model performance on various side effect removal, we presents a new benchmark, dubbed ROSE-Bench, incorporating both common scenarios and the five special side effects for comprehensive evaluation. Experimental results demonstrate that ROSE achieves superior performance compared to existing video object erasing models and generalizes well to real-world video scenarios. The project page is https://rose2025-inpaint.github.io/.", "AI": {"tldr": "ROSE\u662f\u4e00\u4e2a\u89c6\u9891\u5bf9\u8c61\u79fb\u9664\u6846\u67b6\uff0c\u4e13\u95e8\u5904\u7406\u5bf9\u8c61\u79fb\u9664\u540e\u7684\u526f\u4f5c\u7528\uff08\u9634\u5f71\u3001\u53cd\u5c04\u3001\u5149\u7ebf\u3001\u534a\u900f\u660e\u548c\u955c\u50cf\u6548\u679c\uff09\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u6269\u6563\u53d8\u6362\u5668\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u4fee\u590d\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5bf9\u8c61\u79fb\u9664\u65b9\u6cd5\u5728\u5904\u7406\u5bf9\u8c61\u526f\u4f5c\u7528\uff08\u5982\u9634\u5f71\u3001\u53cd\u5c04\u7b49\uff09\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u914d\u5bf9\u7684\u76d1\u7763\u6570\u636e\u6765\u8bad\u7ec3\u6a21\u578b\u8bc6\u522b\u548c\u6d88\u9664\u8fd9\u4e9b\u526f\u4f5c\u7528\u3002", "method": "\u4f7f\u75283D\u6e32\u67d3\u5f15\u64ce\u751f\u6210\u5927\u89c4\u6a21\u5408\u6210\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u6784\u5efa\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u7684\u89c6\u9891\u4fee\u590d\u6a21\u578b\uff0c\u901a\u8fc7\u53c2\u8003\u6574\u4e2a\u89c6\u9891\u8fdb\u884c\u57fa\u4e8e\u53c2\u8003\u7684\u64e6\u9664\uff0c\u5e76\u5f15\u5165\u989d\u5916\u76d1\u7763\u6765\u663e\u5f0f\u9884\u6d4b\u53d7\u526f\u4f5c\u7528\u5f71\u54cd\u7684\u533a\u57df\u3002", "result": "ROSE\u5728\u63d0\u51fa\u7684ROSE-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u76f8\u6bd4\u73b0\u6709\u89c6\u9891\u5bf9\u8c61\u64e6\u9664\u6a21\u578b\u5177\u6709\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u573a\u666f\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7cfb\u7edf\u6027\u5904\u7406\u5bf9\u8c61\u526f\u4f5c\u7528\u548c\u5408\u6210\u6570\u636e\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5bf9\u8c61\u79fb\u9664\u7684\u8d28\u91cf\uff0c\u4e3a\u5904\u7406\u590d\u6742\u89c6\u89c9\u526f\u4f5c\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.18664", "pdf": "https://arxiv.org/pdf/2508.18664", "abs": "https://arxiv.org/abs/2508.18664", "authors": ["Xin Tian", "Yingtie Lei", "Xiujun Zhang", "Zimeng Li", "Chi-Man Pun", "Xuhang Chen"], "title": "SFormer: SNR-guided Transformer for Underwater Image Enhancement from the Frequency Domain", "categories": ["cs.CV"], "comment": "Accepted by PRICAI2025", "summary": "Recent learning-based underwater image enhancement (UIE) methods have advanced by incorporating physical priors into deep neural networks, particularly using the signal-to-noise ratio (SNR) prior to reduce wavelength-dependent attenuation. However, spatial domain SNR priors have two limitations: (i) they cannot effectively separate cross-channel interference, and (ii) they provide limited help in amplifying informative structures while suppressing noise. To overcome these, we propose using the SNR prior in the frequency domain, decomposing features into amplitude and phase spectra for better channel modulation. We introduce the Fourier Attention SNR-prior Transformer (FAST), combining spectral interactions with SNR cues to highlight key spectral components. Additionally, the Frequency Adaptive Transformer (FAT) bottleneck merges low- and high-frequency branches using a gated attention mechanism to enhance perceptual quality. Embedded in a unified U-shaped architecture, these modules integrate a conventional RGB stream with an SNR-guided branch, forming SFormer. Trained on 4,800 paired images from UIEB, EUVP, and LSUI, SFormer surpasses recent methods with a 3.1 dB gain in PSNR and 0.08 in SSIM, successfully restoring colors, textures, and contrast in underwater scenes.", "AI": {"tldr": "\u63d0\u51fa\u4e86SFormer\u65b9\u6cd5\uff0c\u5728\u9891\u57df\u4f7f\u7528SNR\u5148\u9a8c\uff0c\u7ed3\u5408\u5085\u91cc\u53f6\u6ce8\u610f\u529bSNR\u5148\u9a8c\u53d8\u6362\u5668\u548c\u9891\u7387\u81ea\u9002\u5e94\u53d8\u6362\u5668\uff0c\u663e\u8457\u63d0\u5347\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\u6548\u679c", "motivation": "\u73b0\u6709\u57fa\u4e8eSNR\u5148\u9a8c\u7684\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u5728\u7a7a\u95f4\u57df\u5b58\u5728\u4e24\u4e2a\u5c40\u9650\uff1a\u65e0\u6cd5\u6709\u6548\u5206\u79bb\u8de8\u901a\u9053\u5e72\u6270\uff0c\u4ee5\u53ca\u5728\u653e\u5927\u4fe1\u606f\u7ed3\u6784\u540c\u65f6\u6291\u5236\u566a\u58f0\u65b9\u9762\u5e2e\u52a9\u6709\u9650", "method": "\u63d0\u51fa\u9891\u57dfSNR\u5148\u9a8c\uff0c\u5c06\u7279\u5f81\u5206\u89e3\u4e3a\u5e45\u5ea6\u548c\u76f8\u4f4d\u8c31\uff1b\u8bbe\u8ba1\u5085\u91cc\u53f6\u6ce8\u610f\u529bSNR\u5148\u9a8c\u53d8\u6362\u5668(FAST)\u548c\u9891\u7387\u81ea\u9002\u5e94\u53d8\u6362\u5668(FAT)\uff1b\u91c7\u7528U\u578b\u67b6\u6784\u6574\u5408\u5e38\u89c4RGB\u6d41\u548cSNR\u5f15\u5bfc\u5206\u652f", "result": "\u5728UIEB\u3001EUVP\u548cLSUI\u6570\u636e\u96c6\u76844800\u5bf9\u56fe\u50cf\u4e0a\u8bad\u7ec3\uff0cPSNR\u63d0\u53473.1dB\uff0cSSIM\u63d0\u53470.08\uff0c\u6210\u529f\u6062\u590d\u6c34\u4e0b\u573a\u666f\u7684\u989c\u8272\u3001\u7eb9\u7406\u548c\u5bf9\u6bd4\u5ea6", "conclusion": "\u9891\u57dfSNR\u5148\u9a8c\u6bd4\u7a7a\u95f4\u57df\u65b9\u6cd5\u66f4\u6709\u6548\uff0cSFormer\u65b9\u6cd5\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u6700\u65b0\u65b9\u6cd5"}}
{"id": "2508.18696", "pdf": "https://arxiv.org/pdf/2508.18696", "abs": "https://arxiv.org/abs/2508.18696", "authors": ["Qun Ji", "Peng Li", "Mingqiang Wei"], "title": "ColorGS: High-fidelity Surgical Scene Reconstruction with Colored Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "High-fidelity reconstruction of deformable tissues from endoscopic videos remains challenging due to the limitations of existing methods in capturing subtle color variations and modeling global deformations. While 3D Gaussian Splatting (3DGS) enables efficient dynamic reconstruction, its fixed per-Gaussian color assignment struggles with intricate textures, and linear deformation modeling fails to model consistent global deformation. To address these issues, we propose ColorGS, a novel framework that integrates spatially adaptive color encoding and enhanced deformation modeling for surgical scene reconstruction. First, we introduce Colored Gaussian Primitives, which employ dynamic anchors with learnable color parameters to adaptively encode spatially varying textures, significantly improving color expressiveness under complex lighting and tissue similarity. Second, we design an Enhanced Deformation Model (EDM) that combines time-aware Gaussian basis functions with learnable time-independent deformations, enabling precise capture of both localized tissue deformations and global motion consistency caused by surgical interactions. Extensive experiments on DaVinci robotic surgery videos and benchmark datasets (EndoNeRF, StereoMIS) demonstrate that ColorGS achieves state-of-the-art performance, attaining a PSNR of 39.85 (1.5 higher than prior 3DGS-based methods) and superior SSIM (97.25\\%) while maintaining real-time rendering efficiency. Our work advances surgical scene reconstruction by balancing high fidelity with computational practicality, critical for intraoperative guidance and AR/VR applications.", "AI": {"tldr": "ColorGS\u662f\u4e00\u4e2a\u7528\u4e8e\u5185\u7aa5\u955c\u89c6\u9891\u4e2d\u53ef\u53d8\u5f62\u7ec4\u7ec7\u91cd\u5efa\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u989c\u8272\u7f16\u7801\u548c\u589e\u5f3a\u53d8\u5f62\u5efa\u6a21\uff0c\u5728\u4fdd\u6301\u5b9e\u65f6\u6e32\u67d3\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6355\u6349\u7ec6\u5fae\u989c\u8272\u53d8\u5316\u548c\u5efa\u6a21\u5168\u5c40\u53d8\u5f62\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c3D\u9ad8\u65af\u6cfc\u6e85\u7684\u56fa\u5b9a\u989c\u8272\u5206\u914d\u548c\u7ebf\u6027\u53d8\u5f62\u5efa\u6a21\u65e0\u6cd5\u5904\u7406\u590d\u6742\u7eb9\u7406\u548c\u4e00\u81f4\u5168\u5c40\u53d8\u5f62\u3002", "method": "\u63d0\u51faColored Gaussian Primitives\u4f7f\u7528\u52a8\u6001\u951a\u70b9\u548c\u53ef\u5b66\u4e60\u989c\u8272\u53c2\u6570\u81ea\u9002\u5e94\u7f16\u7801\u7a7a\u95f4\u53d8\u5316\u7eb9\u7406\uff1b\u8bbe\u8ba1Enhanced Deformation Model\u7ed3\u5408\u65f6\u95f4\u611f\u77e5\u9ad8\u65af\u57fa\u51fd\u6570\u548c\u53ef\u5b66\u4e60\u65f6\u95f4\u65e0\u5173\u53d8\u5f62\u3002", "result": "\u5728DaVinci\u673a\u5668\u4eba\u624b\u672f\u89c6\u9891\u548c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cPSNR\u8fbe\u523039.85\uff08\u6bd4\u4e4b\u524d\u65b9\u6cd5\u9ad81.5\uff09\uff0cSSIM\u8fbe\u523097.25%\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6e32\u67d3\u6548\u7387\u3002", "conclusion": "ColorGS\u901a\u8fc7\u5e73\u8861\u9ad8\u4fdd\u771f\u5ea6\u548c\u8ba1\u7b97\u5b9e\u7528\u6027\uff0c\u63a8\u8fdb\u4e86\u624b\u672f\u573a\u666f\u91cd\u5efa\u6280\u672f\uff0c\u5bf9\u672f\u4e2d\u6307\u5bfc\u548cAR/VR\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2508.18788", "pdf": "https://arxiv.org/pdf/2508.18788", "abs": "https://arxiv.org/abs/2508.18788", "authors": ["Christian L\u00f6wens", "Thorben Funke", "Jingchao Xie", "Alexandru Paul Condurache"], "title": "PseudoMapTrainer: Learning Online Mapping without HD Maps", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "Accepted at ICCV 2025", "summary": "Online mapping models show remarkable results in predicting vectorized maps from multi-view camera images only. However, all existing approaches still rely on ground-truth high-definition maps during training, which are expensive to obtain and often not geographically diverse enough for reliable generalization. In this work, we propose PseudoMapTrainer, a novel approach to online mapping that uses pseudo-labels generated from unlabeled sensor data. We derive those pseudo-labels by reconstructing the road surface from multi-camera imagery using Gaussian splatting and semantics of a pre-trained 2D segmentation network. In addition, we introduce a mask-aware assignment algorithm and loss function to handle partially masked pseudo-labels, allowing for the first time the training of online mapping models without any ground-truth maps. Furthermore, our pseudo-labels can be effectively used to pre-train an online model in a semi-supervised manner to leverage large-scale unlabeled crowdsourced data. The code is available at github.com/boschresearch/PseudoMapTrainer.", "AI": {"tldr": "PseudoMapTrainer\u662f\u4e00\u79cd\u65e0\u9700\u5730\u9762\u771f\u5b9e\u9ad8\u7cbe\u5730\u56fe\u7684\u5728\u7ebf\u5730\u56fe\u6784\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u56fe\u50cf\u548c\u9884\u8bad\u7ec3\u5206\u5272\u7f51\u7edc\u751f\u6210\u4f2a\u6807\u7b7e\u8fdb\u884c\u8bad\u7ec3", "motivation": "\u73b0\u6709\u5728\u7ebf\u5730\u56fe\u6784\u5efa\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u4e14\u5730\u7406\u591a\u6837\u6027\u4e0d\u8db3\u7684\u9ad8\u7cbe\u5730\u56fe\u6807\u6ce8\u6570\u636e\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b", "method": "\u4f7f\u7528\u9ad8\u65af\u6e85\u5c04\u4ece\u591a\u76f8\u673a\u56fe\u50cf\u91cd\u5efa\u9053\u8def\u8868\u9762\uff0c\u7ed3\u5408\u9884\u8bad\u7ec32D\u5206\u5272\u7f51\u7edc\u751f\u6210\u4f2a\u6807\u7b7e\uff1b\u63d0\u51fa\u63a9\u7801\u611f\u77e5\u5206\u914d\u7b97\u6cd5\u548c\u635f\u5931\u51fd\u6570\u5904\u7406\u90e8\u5206\u63a9\u7801\u7684\u4f2a\u6807\u7b7e", "result": "\u9996\u6b21\u5b9e\u73b0\u4e86\u65e0\u9700\u5730\u9762\u771f\u5b9e\u5730\u56fe\u7684\u5728\u7ebf\u5730\u56fe\u6a21\u578b\u8bad\u7ec3\uff0c\u5e76\u80fd\u5229\u7528\u5927\u89c4\u6a21\u65e0\u6807\u6ce8\u4f17\u5305\u6570\u636e\u8fdb\u884c\u534a\u76d1\u7763\u9884\u8bad\u7ec3", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5728\u7ebf\u5730\u56fe\u6784\u5efa\u63d0\u4f9b\u4e86\u66f4\u7ecf\u6d4e\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2508.18859", "pdf": "https://arxiv.org/pdf/2508.18859", "abs": "https://arxiv.org/abs/2508.18859", "authors": ["Muhammad Kashif Ali", "Eun Woo Im", "Dongjin Kim", "Tae Hyun Kim", "Vivek Gupta", "Haonan Luo", "Tianrui Li"], "title": "Harnessing Meta-Learning for Controllable Full-Frame Video Stabilization", "categories": ["cs.CV"], "comment": null, "summary": "Video stabilization remains a fundamental problem in computer vision, particularly pixel-level synthesis solutions for video stabilization, which synthesize full-frame outputs, add to the complexity of this task. These methods aim to enhance stability while synthesizing full-frame videos, but the inherent diversity in motion profiles and visual content present in each video sequence makes robust generalization with fixed parameters difficult. To address this, we present a novel method that improves pixel-level synthesis video stabilization methods by rapidly adapting models to each input video at test time. The proposed approach takes advantage of low-level visual cues available during inference to improve both the stability and visual quality of the output. Notably, the proposed rapid adaptation achieves significant performance gains even with a single adaptation pass. We further propose a jerk localization module and a targeted adaptation strategy, which focuses the adaptation on high-jerk segments for maximizing stability with fewer adaptation steps. The proposed methodology enables modern stabilizers to overcome the longstanding SOTA approaches while maintaining the full frame nature of the modern methods, while offering users with control mechanisms akin to classical approaches. Extensive experiments on diverse real-world datasets demonstrate the versatility of the proposed method. Our approach consistently improves the performance of various full-frame synthesis models in both qualitative and quantitative terms, including results on downstream applications.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5feb\u901f\u9002\u914d\u7684\u89c6\u9891\u7a33\u50b3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6d4b\u8bd5\u65f6\u9002\u914d\u6a21\u578b\u6765\u6539\u5584\u50cf\u7d20\u7ea7\u5408\u6210\u89c6\u9891\u7a33\u50b3\u7684\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u4f4e\u7ea7\u89c6\u89c9\u7ebf\u7d22\u548c\u4e13\u95e8\u7684\u5197\u52d2\u5b9a\u4f4d\u6a21\u5757\uff0c\u5728\u4fdd\u6301\u5168\u5e27\u8f93\u51fa\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u7a33\u5b9a\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u50cf\u7d20\u7ea7\u5408\u6210\u89c6\u9891\u7a33\u50b3\u65b9\u6cd5\u867d\u80fd\u751f\u6210\u5168\u5e27\u8f93\u51fa\uff0c\u4f46\u56e0\u4e3a\u89c6\u9891\u8fd0\u52a8\u548c\u5185\u5bb9\u7684\u591a\u6837\u6027\uff0c\u56fa\u5b9a\u53c2\u6570\u7684\u6a21\u578b\u5f88\u96be\u5b9e\u73b0\u7a33\u5065\u7684\u901a\u7528\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6839\u636e\u6bcf\u4e2a\u8f93\u5165\u89c6\u9891\u8fdb\u884c\u5feb\u901f\u9002\u914d\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u7a33\u5b9a\u6027\u548c\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9898\u7684\u5feb\u901f\u9002\u914d\u65b9\u6cd5\uff0c\u5728\u63a8\u7406\u65f6\u5229\u7528\u4f4e\u7ea7\u89c6\u89c9\u7ebf\u7d22\u6765\u6539\u5584\u8f93\u51fa\u7684\u7a33\u5b9a\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002\u8fd8\u63d0\u51fa\u4e86\u5197\u52d2\u5b9a\u4f4d\u6a21\u5757\u548c\u9488\u5bf9\u6027\u9002\u914d\u7b56\u7565\uff0c\u5c06\u9002\u914d\u91cd\u70b9\u653e\u5728\u9ad8\u5197\u52d2\u6bb5\u843d\u4e0a\uff0c\u4ee5\u6700\u5c11\u7684\u9002\u914d\u6b65\u9aa4\u5b9e\u73b0\u6700\u5927\u5316\u7a33\u5b9a\u6027\u3002", "result": "\u901a\u8fc7\u5355\u6b21\u9002\u914d\u5c31\u80fd\u83b7\u5f97\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u5728\u591a\u6837\u5316\u7684\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u591a\u7528\u9014\u6027\u3002\u8be5\u65b9\u6cd5\u4e00\u8d2f\u5730\u6539\u5584\u4e86\u5404\u79cd\u5168\u5e27\u5408\u6210\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5305\u62ec\u5b9a\u6027\u548c\u5b9a\u91cf\u6307\u6807\uff0c\u4ee5\u53ca\u4e0b\u6e38\u5e94\u7528\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u8ba9\u73b0\u4ee3\u7a33\u50b3\u5668\u8d85\u8d8a\u957f\u671f\u4ee5\u6765\u7684SOTA\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u73b0\u4ee3\u65b9\u6cd5\u7684\u5168\u5e27\u7279\u6027\uff0c\u5e76\u63d0\u4f9b\u7c7b\u4f3c\u4e8e\u7ecf\u5178\u65b9\u6cd5\u7684\u7528\u6237\u63a7\u5236\u673a\u5236\u3002"}}
{"id": "2508.18971", "pdf": "https://arxiv.org/pdf/2508.18971", "abs": "https://arxiv.org/abs/2508.18971", "authors": ["Maxime Pietrantoni", "Martin Humenberger", "Torsten Sattler", "Gabriela Csurka"], "title": "Can we make NeRF-based visual localization privacy-preserving?", "categories": ["cs.CV"], "comment": null, "summary": "Visual localization (VL) is the task of estimating the camera pose in a known scene. VL methods, a.o., can be distinguished based on how they represent the scene, e.g., explicitly through a (sparse) point cloud or a collection of images or implicitly through the weights of a neural network. Recently, NeRF-based methods have become popular for VL. While NeRFs offer high-quality novel view synthesis, they inadvertently encode fine scene details, raising privacy concerns when deployed in cloud-based localization services as sensitive information could be recovered. In this paper, we tackle this challenge on two ends. We first propose a new protocol to assess privacy-preservation of NeRF-based representations. We show that NeRFs trained with photometric losses store fine-grained details in their geometry representations, making them vulnerable to privacy attacks, even if the head that predicts colors is removed. Second, we propose ppNeSF (Privacy-Preserving Neural Segmentation Field), a NeRF variant trained with segmentation supervision instead of RGB images. These segmentation labels are learned in a self-supervised manner, ensuring they are coarse enough to obscure identifiable scene details while remaining discriminativeness in 3D. The segmentation space of ppNeSF can be used for accurate visual localization, yielding state-of-the-art results.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u795e\u7ecf\u5206\u5272\u573a(ppNeSF)\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u57fa\u4e8eNeRF\u7684\u89c6\u89c9\u5b9a\u4f4d\u4e2d\u7684\u9690\u79c1\u6cc4\u9732\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u65b0\u7684\u8bc4\u4f30\u534f\u8bae\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u57fa\u4e8eNeRF\u7684\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u867d\u7136\u80fd\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u4f46\u4f1a\u65e0\u610f\u4e2d\u7f16\u7801\u7cbe\u7ec6\u7684\u573a\u666f\u7ec6\u8282\uff0c\u5728\u4e91\u670d\u52a1\u90e8\u7f72\u65f6\u5b58\u5728\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002", "method": "\u63d0\u51fappNeSF\u65b9\u6cd5\uff0c\u4f7f\u7528\u5206\u5272\u76d1\u7763\u800c\u975eRGB\u56fe\u50cf\u8bad\u7ec3NeRF\u53d8\u4f53\uff0c\u5206\u5272\u6807\u7b7e\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u83b7\u5f97\uff0c\u65e2\u80fd\u4fdd\u63013D\u533a\u5206\u6027\u53c8\u80fd\u6a21\u7cca\u53ef\u8bc6\u522b\u7ec6\u8282\u3002", "result": "ppNeSF\u7684\u5206\u5272\u7a7a\u95f4\u53ef\u7528\u4e8e\u7cbe\u786e\u7684\u89c6\u89c9\u5b9a\u4f4d\uff0c\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u6709\u6548\u4fdd\u62a4\u9690\u79c1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86NeRF-based\u89c6\u89c9\u5b9a\u4f4d\u4e2d\u7684\u9690\u79c1\u95ee\u9898\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u76843D\u573a\u666f\u8868\u793a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19154", "pdf": "https://arxiv.org/pdf/2508.19154", "abs": "https://arxiv.org/abs/2508.19154", "authors": ["Yan Chen", "Yi Wen", "Wei Li", "Junchao Liu", "Yong Guo", "Jie Hu", "Xinghao Chen"], "title": "RDDM: Practicing RAW Domain Diffusion Model for Real-world Image Restoration", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "We present the RAW domain diffusion model (RDDM), an end-to-end diffusion model that restores photo-realistic images directly from the sensor RAW data. While recent sRGB-domain diffusion methods achieve impressive results, they are caught in a dilemma between high fidelity and realistic generation. As these models process lossy sRGB inputs and neglect the accessibility of the sensor RAW images in many scenarios, e.g., in image and video capturing in edge devices, resulting in sub-optimal performance. RDDM bypasses this limitation by directly restoring images in the RAW domain, replacing the conventional two-stage image signal processing (ISP) + IR pipeline. However, a simple adaptation of pre-trained diffusion models to the RAW domain confronts the out-of-distribution (OOD) issues. To this end, we propose: (1) a RAW-domain VAE (RVAE) learning optimal latent representations, (2) a differentiable Post Tone Processing (PTP) module enabling joint RAW and sRGB space optimization. To compensate for the deficiency in the dataset, we develop a scalable degradation pipeline synthesizing RAW LQ-HQ pairs from existing sRGB datasets for large-scale training. Furthermore, we devise a configurable multi-bayer (CMB) LoRA module handling diverse RAW patterns such as RGGB, BGGR, etc. Extensive experiments demonstrate RDDM's superiority over state-of-the-art sRGB diffusion methods, yielding higher fidelity results with fewer artifacts.", "AI": {"tldr": "RDDM\u662f\u4e00\u79cd\u76f4\u63a5\u5728RAW\u57df\u8fdb\u884c\u56fe\u50cf\u6062\u590d\u7684\u7aef\u5230\u7aef\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7RAW\u57dfVAE\u548c\u53ef\u5fae\u5206\u540e\u8272\u8c03\u5904\u7406\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfsRGB\u57df\u6269\u6563\u6a21\u578b\u7684\u9ad8\u4fdd\u771f\u5ea6\u4e0e\u771f\u5b9e\u611f\u751f\u6210\u4e4b\u95f4\u7684\u56f0\u5883\u3002", "motivation": "\u73b0\u6709\u7684sRGB\u57df\u6269\u6563\u6a21\u578b\u5904\u7406\u6709\u635f\u7684sRGB\u8f93\u5165\uff0c\u5ffd\u7565\u4e86\u4f20\u611f\u5668RAW\u56fe\u50cf\u5728\u8bb8\u591a\u573a\u666f\u4e2d\u7684\u53ef\u8bbf\u95ee\u6027\uff08\u5982\u8fb9\u7f18\u8bbe\u5907\u4e2d\u7684\u56fe\u50cf\u548c\u89c6\u9891\u6355\u83b7\uff09\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002RDDM\u65e8\u5728\u901a\u8fc7\u76f4\u63a5\u5728RAW\u57df\u6062\u590d\u56fe\u50cf\u6765\u7ed5\u8fc7\u8fd9\u4e00\u9650\u5236\u3002", "method": "1) \u63d0\u51faRAW\u57dfVAE\uff08RVAE\uff09\u5b66\u4e60\u6700\u4f18\u6f5c\u5728\u8868\u793a\uff1b2) \u8bbe\u8ba1\u53ef\u5fae\u5206\u540e\u8272\u8c03\u5904\u7406\uff08PTP\uff09\u6a21\u5757\u5b9e\u73b0RAW\u548csRGB\u7a7a\u95f4\u7684\u8054\u5408\u4f18\u5316\uff1b3) \u5f00\u53d1\u53ef\u6269\u5c55\u7684\u9000\u5316\u7ba1\u9053\u4ece\u73b0\u6709sRGB\u6570\u636e\u96c6\u5408\u6210RAW LQ-HQ\u5bf9\uff1b4) \u8bbe\u8ba1\u53ef\u914d\u7f6e\u591a\u62dc\u8033\uff08CMB\uff09LoRA\u6a21\u5757\u5904\u7406\u4e0d\u540c\u7684RAW\u6a21\u5f0f\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eRDDM\u4f18\u4e8e\u6700\u5148\u8fdb\u7684sRGB\u6269\u6563\u65b9\u6cd5\uff0c\u4ee5\u66f4\u5c11\u7684\u4f2a\u5f71\u4ea7\u751f\u66f4\u9ad8\u4fdd\u771f\u5ea6\u7684\u7ed3\u679c\u3002", "conclusion": "RDDM\u901a\u8fc7\u76f4\u63a5\u5728RAW\u57df\u64cd\u4f5c\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u9ad8\u4fdd\u771f\u5ea6\u4e0e\u771f\u5b9e\u611f\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u56fe\u50cf\u6062\u590d\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2508.19243", "pdf": "https://arxiv.org/pdf/2508.19243", "abs": "https://arxiv.org/abs/2508.19243", "authors": ["Beiqi Chen", "Shuai Shao", "Haitang Feng", "Jianhuang Lai", "Jianlou Si", "Guangcong Wang"], "title": "Style4D-Bench: A Benchmark Suite for 4D Stylization", "categories": ["cs.CV"], "comment": "Project page: https://becky-catherine.github.io/Style4D . Code:   https://github.com/Becky-catherine/Style4D-Bench", "summary": "We introduce Style4D-Bench, the first benchmark suite specifically designed for 4D stylization, with the goal of standardizing evaluation and facilitating progress in this emerging area. Style4D-Bench comprises: 1) a comprehensive evaluation protocol measuring spatial fidelity, temporal coherence, and multi-view consistency through both perceptual and quantitative metrics, 2) a strong baseline that make an initial attempt for 4D stylization, and 3) a curated collection of high-resolution dynamic 4D scenes with diverse motions and complex backgrounds. To establish a strong baseline, we present Style4D, a novel framework built upon 4D Gaussian Splatting. It consists of three key components: a basic 4DGS scene representation to capture reliable geometry, a Style Gaussian Representation that leverages lightweight per-Gaussian MLPs for temporally and spatially aware appearance control, and a Holistic Geometry-Preserved Style Transfer module designed to enhance spatio-temporal consistency via contrastive coherence learning and structural content preservation. Extensive experiments on Style4D-Bench demonstrate that Style4D achieves state-of-the-art performance in 4D stylization, producing fine-grained stylistic details with stable temporal dynamics and consistent multi-view rendering. We expect Style4D-Bench to become a valuable resource for benchmarking and advancing research in stylized rendering of dynamic 3D scenes. Project page: https://becky-catherine.github.io/Style4D . Code: https://github.com/Becky-catherine/Style4D-Bench .", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a4D\u98ce\u683c\u5316\u57fa\u51c6Style4D-Bench\uff0c\u5305\u542b\u8bc4\u4f30\u534f\u8bae\u3001\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u548c\u9ad8\u8d28\u91cf4D\u573a\u666f\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e4D\u9ad8\u65af\u6e85\u5c04\u7684Style4D\u6846\u67b6\u5b9e\u73b0SOTA\u6027\u80fd", "motivation": "4D\u98ce\u683c\u5316\u662f\u4e00\u4e2a\u65b0\u5174\u9886\u57df\uff0c\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u57fa\u51c6\u548c\u65b9\u6cd5\uff0c\u9700\u8981\u5efa\u7acb\u7edf\u4e00\u7684\u8bc4\u4f30\u6807\u51c6\u548c\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55", "method": "\u57fa\u4e8e4D\u9ad8\u65af\u6e85\u5c04\u6784\u5efaStyle4D\u6846\u67b6\uff0c\u5305\u542b\u57fa\u78404DGS\u573a\u666f\u8868\u793a\u3001\u98ce\u683c\u9ad8\u65af\u8868\u793a\uff08\u8f7b\u91cf\u7ea7MLP\uff09\u548c\u6574\u4f53\u51e0\u4f55\u4fdd\u6301\u98ce\u683c\u8fc1\u79fb\u6a21\u5757\uff0c\u901a\u8fc7\u5bf9\u6bd4\u4e00\u81f4\u6027\u5b66\u4e60\u548c\u7ed3\u6784\u5185\u5bb9\u4fdd\u6301\u589e\u5f3a\u65f6\u7a7a\u4e00\u81f4\u6027", "result": "\u5728Style4D-Bench\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0cStyle4D\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u76844D\u98ce\u683c\u5316\u6027\u80fd\uff0c\u751f\u6210\u7ec6\u7c92\u5ea6\u98ce\u683c\u7ec6\u8282\uff0c\u5177\u6709\u7a33\u5b9a\u7684\u65f6\u95f4\u52a8\u6001\u548c\u4e00\u81f4\u7684\u591a\u89c6\u89d2\u6e32\u67d3", "conclusion": "Style4D-Bench\u5c06\u6210\u4e3a\u52a8\u60013D\u573a\u666f\u98ce\u683c\u5316\u6e32\u67d3\u7814\u7a76\u7684\u91cd\u8981\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u548c\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55"}}
{"id": "2508.19247", "pdf": "https://arxiv.org/pdf/2508.19247", "abs": "https://arxiv.org/abs/2508.19247", "authors": ["Lin Li", "Zehuan Huang", "Haoran Feng", "Gengxiong Zhuang", "Rui Chen", "Chunchao Guo", "Lu Sheng"], "title": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space", "categories": ["cs.CV"], "comment": "Project page: https://huanngzh.github.io/VoxHammer-Page/", "summary": "3D local editing of specified regions is crucial for game industry and robot interaction. Recent methods typically edit rendered multi-view images and then reconstruct 3D models, but they face challenges in precisely preserving unedited regions and overall coherence. Inspired by structured 3D generative models, we propose VoxHammer, a novel training-free approach that performs precise and coherent editing in 3D latent space. Given a 3D model, VoxHammer first predicts its inversion trajectory and obtains its inverted latents and key-value tokens at each timestep. Subsequently, in the denoising and editing phase, we replace the denoising features of preserved regions with the corresponding inverted latents and cached key-value tokens. By retaining these contextual features, this approach ensures consistent reconstruction of preserved areas and coherent integration of edited parts. To evaluate the consistency of preserved regions, we constructed Edit3D-Bench, a human-annotated dataset comprising hundreds of samples, each with carefully labeled 3D editing regions. Experiments demonstrate that VoxHammer significantly outperforms existing methods in terms of both 3D consistency of preserved regions and overall quality. Our method holds promise for synthesizing high-quality edited paired data, thereby laying the data foundation for in-context 3D generation. See our project page at https://huanngzh.github.io/VoxHammer-Page/.", "AI": {"tldr": "VoxHammer\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u76843D\u5c40\u90e8\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u57283D\u6f5c\u5728\u7a7a\u95f4\u4e2d\u4fdd\u7559\u672a\u7f16\u8f91\u533a\u57df\u7684\u7279\u5f81\uff0c\u5b9e\u73b0\u7cbe\u786e\u4e14\u8fde\u8d2f\u7684\u7f16\u8f91\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u7f16\u8f91\u591a\u89c6\u89d2\u56fe\u50cf\u518d\u91cd\u5efa3D\u6a21\u578b\uff0c\u96be\u4ee5\u7cbe\u786e\u4fdd\u6301\u672a\u7f16\u8f91\u533a\u57df\u7684\u5b8c\u6574\u6027\u548c\u6574\u4f53\u4e00\u81f4\u6027\u3002", "method": "\u9996\u5148\u9884\u6d4b3D\u6a21\u578b\u7684\u53cd\u8f6c\u8f68\u8ff9\u83b7\u53d6\u6f5c\u5728\u8868\u793a\u548c\u952e\u503c\u4ee4\u724c\uff0c\u7136\u540e\u5728\u53bb\u566a\u7f16\u8f91\u9636\u6bb5\u7528\u53cd\u8f6c\u7279\u5f81\u66ff\u6362\u4fdd\u7559\u533a\u57df\u7684\u7279\u5f81\uff0c\u786e\u4fdd\u4e00\u81f4\u6027\u3002", "result": "\u5728\u6784\u5efa\u7684Edit3D-Bench\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cVoxHammer\u5728\u4fdd\u6301\u533a\u57df3D\u4e00\u81f4\u6027\u548c\u6574\u4f53\u8d28\u91cf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5408\u6210\u9ad8\u8d28\u91cf\u7f16\u8f91\u914d\u5bf9\u6570\u636e\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u4e3a\u4e0a\u4e0b\u65873D\u751f\u6210\u5960\u5b9a\u4e86\u6570\u636e\u57fa\u7840\u3002"}}
{"id": "2508.18569", "pdf": "https://arxiv.org/pdf/2508.18569", "abs": "https://arxiv.org/abs/2508.18569", "authors": ["Girish A. Koushik", "Fatemeh Nazarieh", "Katherine Birch", "Shenbin Qian", "Diptesh Kanojia"], "title": "The Mind's Eye: A Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation", "categories": ["cs.CL", "cs.CV"], "comment": "Under Review", "summary": "Visual metaphor generation is a challenging task that aims to generate an image given an input text metaphor. Inherently, it needs language understanding to bind a source concept with a target concept, in a way that preserves meaning while ensuring visual coherence. We propose a self-evaluating visual metaphor generation framework that focuses on metaphor alignment. Our self-evaluation approach combines existing metrics with our newly proposed metaphor decomposition score and a meaning alignment (MA) metric. Within this setup, we explore two novel approaches: a training-free pipeline that explicitly decomposes prompts into source-target-meaning (S-T-M) mapping for image synthesis, and a complementary training-based pipeline that improves alignment using our proposed self-evaluation reward schema, without any large-scale retraining. On the held-out test set, the training-free approach surpasses strong closed baselines (GPT-4o, Imagen) on decomposition, CLIP, and MA scores, with the training-based approach close behind. We evaluate our framework output using a user-facing study, and observed that participants preferred GPT-4o overall, while our training-free pipeline led open-source methods and edged Imagen on abstract metaphors. Our analyses show S-T-M prompting helps longer or more abstract metaphors, with closed models excelling on short, concrete cases; we also observe sensitivity to sampler settings. Overall, structured prompting and lightweight RL perform metaphor alignment well under modest compute, and remaining gaps to human preference appear driven by aesthetics and sampling.", "AI": {"tldr": "\u8fd9\u662f\u4e00\u4e2a\u81ea\u6211\u8bc4\u4f30\u7684\u89c6\u89c9\u9690\u55bb\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6e90-\u76ee\u6807-\u542b\u4e49\u6620\u5c04\u548c\u8f7b\u91cf\u7ea7\u5f3a\u5316\u5b66\u4e60\u6765\u63d0\u9ad8\u9690\u55bb\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u89c6\u89c9\u9690\u55bb\u751f\u6210\u9700\u8981\u5408\u5e76\u8bed\u8a00\u7406\u89e3\u548c\u89c6\u89c9\u4e00\u81f4\u6027\uff0c\u76ee\u524d\u7684\u65b9\u6cd5\u5728\u9690\u55bb\u5bf9\u9f50\u65b9\u9762\u9047\u5230\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a\u8bad\u7ec3\u514d\u8d39\u6d41\u6c34\u7ebf\uff08\u663e\u5f0f\u5206\u89e3\u63d0\u793a\u4e3a\u6e90-\u76ee\u6807-\u542b\u4e49\u6620\u5c04\uff09\u548c\u8bad\u7ec3\u57fa\u4e8e\u6d41\u6c34\u7ebf\uff08\u4f7f\u7528\u81ea\u6211\u8bc4\u4f30\u5956\u52b1\u8fdb\u884c\u8f7b\u91cf\u7ea7\u5f3a\u5316\u5b66\u4e60\uff09\u3002", "result": "\u5728\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u8bad\u7ec3\u514d\u8d39\u65b9\u6cd5\u5728\u5206\u89e3\u3001CLIP\u548c\u542b\u4e49\u5bf9\u9f50\u6307\u6807\u4e0a\u8d85\u8d8aGPT-4o\u548cImagen\uff0c\u8bad\u7ec3\u57fa\u4e8e\u65b9\u6cd5\u4e5f\u8868\u73b0\u4f18\u5f02\u3002\u7528\u6237\u7814\u7a76\u663e\u793a\u7528\u6237\u66f4\u504f\u597dGPT-4o\uff0c\u4f46\u5f00\u6e90\u65b9\u6cd5\u5728\u62bd\u8c61\u9690\u55bb\u4e0a\u66f4\u4f18\u3002", "conclusion": "\u7ed3\u6784\u5316\u63d0\u793a\u548c\u8f7b\u91cf\u7ea7RL\u80fd\u591f\u5728\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u5b9e\u73b0\u9690\u55bb\u5bf9\u9f50\uff0c\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5dee\u8ddd\u4e3b\u8981\u6765\u81ea\u7f8e\u5b66\u8d28\u91cf\u548c\u91c7\u6837\u8bbe\u7f6e\u3002"}}
{"id": "2508.18835", "pdf": "https://arxiv.org/pdf/2508.18835", "abs": "https://arxiv.org/abs/2508.18835", "authors": ["Hillol Biswas"], "title": "Quantum-Circuit-Based Visual Fractal Image Generation in Qiskit and Analytics", "categories": ["quant-ph", "cs.CV"], "comment": null, "summary": "As nature is ascribed as quantum, the fractals also pose some intriguing appearance which is found in many micro and macro observable entities or phenomena. Fractals show self-similarity across sizes; structures that resemble the entire are revealed when zoomed in. In Quantum systems, the probability density or wavefunction may exhibit recurring interference patterns at various energy or length scales. Fractals are produced by basic iterative rules (such as Mandelbrot or Julia sets), and they provide limitless complexity. Despite its simplicity, the Schr\\\"odinger equation in quantum mechanics produces incredibly intricate patterns of interference and entanglement, particularly in chaotic quantum systems. Quantum computing, the root where lies to the using the principles of quantum-mechanical phenomenon, when applied in fractal image generation, what outcomes are expected? The paper outlines the generation of a Julia set dataset using an approach coupled with building quantum circuit, highlighting the concepts of superposition, randomness, and entanglement as foundational elements to manipulate the generated dataset patterns. As Quantum computing is finding many application areas, the possibility of using quantum circuits for fractal Julia image generation posits a unique direction of future research where it can be applied to quantum generative arts across various ecosystems with a customised approach, such as producing an exciting landscape based on a quantum art theme.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u91cf\u5b50\u8ba1\u7b97\u5728\u5206\u5f62\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662fJulia\u96c6\u5408\u7684\u751f\u6210\uff0c\u5229\u7528\u91cf\u5b50\u53e0\u52a0\u3001\u968f\u673a\u6027\u548c\u7ea0\u7f20\u7b49\u7279\u6027\u6765\u64cd\u7eb5\u751f\u6210\u7684\u6570\u636e\u96c6\u6a21\u5f0f\u3002", "motivation": "\u81ea\u7136\u754c\u5177\u6709\u91cf\u5b50\u7279\u6027\uff0c\u800c\u5206\u5f62\u5728\u5fae\u89c2\u548c\u5b8f\u89c2\u53ef\u89c2\u6d4b\u5b9e\u4f53\u6216\u73b0\u8c61\u4e2d\u5c55\u73b0\u51fa\u8ff7\u4eba\u7684\u81ea\u76f8\u4f3c\u6027\u3002\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u6982\u7387\u5bc6\u5ea6\u6216\u6ce2\u51fd\u6570\u5728\u4e0d\u540c\u80fd\u91cf\u6216\u957f\u5ea6\u5c3a\u5ea6\u4e0a\u53ef\u80fd\u5448\u73b0\u91cd\u590d\u7684\u5e72\u6d89\u6a21\u5f0f\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u91cf\u5b50\u8ba1\u7b97\u5982\u4f55\u5e94\u7528\u4e8e\u5206\u5f62\u56fe\u50cf\u751f\u6210\u8fd9\u4e00\u65b0\u5174\u9886\u57df\u3002", "method": "\u91c7\u7528\u91cf\u5b50\u7535\u8def\u6784\u5efa\u65b9\u6cd5\u751f\u6210Julia\u96c6\u5408\u6570\u636e\u96c6\uff0c\u5229\u7528\u91cf\u5b50\u53e0\u52a0\u3001\u968f\u673a\u6027\u548c\u7ea0\u7f20\u7b49\u91cf\u5b50\u529b\u5b66\u57fa\u672c\u539f\u7406\u6765\u64cd\u7eb5\u751f\u6210\u7684\u6a21\u5f0f\u3002", "result": "\u63d0\u51fa\u4e86\u4f7f\u7528\u91cf\u5b50\u7535\u8def\u8fdb\u884c\u5206\u5f62Julia\u56fe\u50cf\u751f\u6210\u7684\u65b0\u65b9\u6cd5\uff0c\u4e3a\u91cf\u5b50\u751f\u6210\u827a\u672f\u5728\u5404\u4e2a\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5b9a\u5236\u5316\u5e94\u7528\u5f00\u8f9f\u4e86\u72ec\u7279\u7684\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u91cf\u5b50\u8ba1\u7b97\u5728\u5206\u5f62\u56fe\u50cf\u751f\u6210\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u91cf\u5b50\u751f\u6210\u827a\u672f\u65b9\u9762\uff0c\u53ef\u4ee5\u57fa\u4e8e\u91cf\u5b50\u827a\u672f\u4e3b\u9898\u521b\u5efa\u4ee4\u4eba\u5174\u594b\u7684\u666f\u89c2\uff0c\u8fd9\u4ee3\u8868\u4e86\u4e00\u4e2a\u72ec\u7279\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
