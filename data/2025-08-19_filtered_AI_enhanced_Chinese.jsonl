{"id": "2508.11695", "pdf": "https://arxiv.org/pdf/2508.11695", "abs": "https://arxiv.org/abs/2508.11695", "authors": ["Yiyun Chen", "Weikai Yang"], "title": "RefAdGen: High-Fidelity Advertising Image Generation", "categories": ["cs.GR", "cs.AI"], "comment": null, "summary": "The rapid advancement of Artificial Intelligence Generated Content (AIGC) techniques has unlocked opportunities in generating diverse and compelling advertising images based on referenced product images and textual scene descriptions. This capability substantially reduces human labor and production costs in traditional marketing workflows. However, existing AIGC techniques either demand extensive fine-tuning for each referenced image to achieve high fidelity, or they struggle to maintain fidelity across diverse products, making them impractical for e-commerce and marketing industries. To tackle this limitation, we first construct AdProd-100K, a large-scale advertising image generation dataset. A key innovation in its construction is our dual data augmentation strategy, which fosters robust, 3D-aware representations crucial for realistic and high-fidelity image synthesis. Leveraging this dataset, we propose RefAdGen, a generation framework that achieves high fidelity through a decoupled design. The framework enforces precise spatial control by injecting a product mask at the U-Net input, and employs an efficient Attention Fusion Module (AFM) to integrate product features. This design effectively resolves the fidelity-efficiency dilemma present in existing methods. Extensive experiments demonstrate that RefAdGen achieves state-of-the-art performance, showcasing robust generalization by maintaining high fidelity and remarkable visual results for both unseen products and challenging real-world, in-the-wild images. This offers a scalable and cost-effective alternative to traditional workflows. Code and datasets are publicly available at https://github.com/Anonymous-Name-139/RefAdgen.", "AI": {"tldr": "\u63d0\u51fa\u4e86RefAdGen\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u8bbe\u8ba1\u548c\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\u89e3\u51b3AIGC\u5e7f\u544a\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u4fdd\u771f\u5ea6-\u6548\u7387\u56f0\u5883\uff0c\u5728\u5927\u578b\u6570\u636e\u96c6AdProd-100K\u4e0a\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u548c\u9ad8\u6548\u751f\u6210\u3002", "motivation": "\u73b0\u6709AIGC\u6280\u672f\u5728\u5e7f\u544a\u56fe\u50cf\u751f\u6210\u4e2d\u8981\u4e48\u9700\u8981\u5bf9\u6bcf\u4e2a\u53c2\u8003\u56fe\u50cf\u8fdb\u884c\u5927\u91cf\u5fae\u8c03\uff0c\u8981\u4e48\u96be\u4ee5\u5728\u4e0d\u540c\u4ea7\u54c1\u4e0a\u4fdd\u6301\u4fdd\u771f\u5ea6\uff0c\u65e0\u6cd5\u6ee1\u8db3\u7535\u5546\u548c\u8425\u9500\u884c\u4e1a\u7684\u5b9e\u9645\u9700\u6c42\u3002", "method": "\u6784\u5efaAdProd-100K\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u91c7\u7528\u53cc\u91cd\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff1b\u63d0\u51faRefAdGen\u6846\u67b6\uff0c\u901a\u8fc7\u4ea7\u54c1\u63a9\u7801\u6ce8\u5165\u5b9e\u73b0\u7cbe\u786e\u7a7a\u95f4\u63a7\u5236\uff0c\u4f7f\u7528\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\u6574\u5408\u4ea7\u54c1\u7279\u5f81\u3002", "result": "RefAdGen\u5728\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5bf9\u672a\u89c1\u4ea7\u54c1\u548c\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u90fd\u80fd\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u548c\u4f18\u5f02\u89c6\u89c9\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4f20\u7edf\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4fdd\u771f\u5ea6-\u6548\u7387\u56f0\u5883\u3002"}}
{"id": "2508.12691", "pdf": "https://arxiv.org/pdf/2508.12691", "abs": "https://arxiv.org/abs/2508.12691", "authors": ["Yuanxin Wei", "Lansong Diao", "Bujiao Chen", "Shenggan Cheng", "Zhengping Qian", "Wenyuan Yu", "Nong Xiao", "Wei Lin", "Jiangsu Du"], "title": "MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "7 pages, 10 figures", "summary": "Leveraging the Transformer architecture and the diffusion process, video DiT models have emerged as a dominant approach for high-quality video generation. However, their multi-step iterative denoising process incurs high computational cost and inference latency. Caching, a widely adopted optimization method in DiT models, leverages the redundancy in the diffusion process to skip computations in different granularities (e.g., step, cfg, block). Nevertheless, existing caching methods are limited to single-granularity strategies, struggling to balance generation quality and inference speed in a flexible manner. In this work, we propose MixCache, a training-free caching-based framework for efficient video DiT inference. It first distinguishes the interference and boundary between different caching strategies, and then introduces a context-aware cache triggering strategy to determine when caching should be enabled, along with an adaptive hybrid cache decision strategy for dynamically selecting the optimal caching granularity. Extensive experiments on diverse models demonstrate that, MixCache can significantly accelerate video generation (e.g., 1.94$\\times$ speedup on Wan 14B, 1.97$\\times$ speedup on HunyuanVideo) while delivering both superior generation quality and inference efficiency compared to baseline methods.", "AI": {"tldr": "MixCache\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u6548\u89c6\u9891DiT\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7c92\u5ea6\u7f13\u5b58\u7b56\u7565\u548c\u81ea\u9002\u5e94\u51b3\u7b56\u673a\u5236\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u89c6\u9891\u751f\u6210", "motivation": "\u73b0\u6709\u7684\u89c6\u9891DiT\u6a21\u578b\u867d\u7136\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u4f46\u591a\u6b65\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u63a8\u7406\u5ef6\u8fdf\u5927\u3002\u73b0\u6709\u7f13\u5b58\u65b9\u6cd5\u4ec5\u9650\u4e8e\u5355\u7c92\u5ea6\u7b56\u7565\uff0c\u96be\u4ee5\u7075\u6d3b\u5e73\u8861\u751f\u6210\u8d28\u91cf\u548c\u63a8\u7406\u901f\u5ea6", "method": "\u63d0\u51faMixCache\u6846\u67b6\uff1a1\uff09\u533a\u5206\u4e0d\u540c\u7f13\u5b58\u7b56\u7565\u7684\u5e72\u6270\u548c\u8fb9\u754c\uff1b2\uff09\u5f15\u5165\u4e0a\u4e0b\u6587\u611f\u77e5\u7f13\u5b58\u89e6\u53d1\u7b56\u7565\u51b3\u5b9a\u4f55\u65f6\u542f\u7528\u7f13\u5b58\uff1b3\uff09\u81ea\u9002\u5e94\u6df7\u5408\u7f13\u5b58\u51b3\u7b56\u7b56\u7565\u52a8\u6001\u9009\u62e9\u6700\u4f18\u7f13\u5b58\u7c92\u5ea6", "result": "\u5728\u591a\u79cd\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMixCache\u80fd\u663e\u8457\u52a0\u901f\u89c6\u9891\u751f\u6210\uff08Wan 14B\u4e0a1.94\u500d\u52a0\u901f\uff0cHunyuanVideo\u4e0a1.97\u500d\u52a0\u901f\uff09\uff0c\u540c\u65f6\u63d0\u4f9b\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u751f\u6210\u8d28\u91cf\u548c\u63a8\u7406\u6548\u7387", "conclusion": "MixCache\u901a\u8fc7\u521b\u65b0\u7684\u591a\u7c92\u5ea6\u7f13\u5b58\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891DiT\u6a21\u578b\u63a8\u7406\u6548\u7387\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u751f\u6210\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\u6548\u679c"}}
{"id": "2508.11728", "pdf": "https://arxiv.org/pdf/2508.11728", "abs": "https://arxiv.org/abs/2508.11728", "authors": ["Chunxia Ren", "Ning Zhu", "Yue Lai", "Gui Chen", "Ruijie Wang", "Yangyi Hu", "Suyao Liu", "Shuwen Mao", "Hong Su", "Yu Zhang", "Li Xiao"], "title": "UniDCF: A Foundation Model for Comprehensive Dentocraniofacial Hard Tissue Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": "23 pages, 6 figures", "summary": "Dentocraniofacial hard tissue defects profoundly affect patients' physiological functions, facial aesthetics, and psychological well-being, posing significant challenges for precise reconstruction. Current deep learning models are limited to single-tissue scenarios and modality-specific imaging inputs, resulting in poor generalizability and trade-offs between anatomical fidelity, computational efficiency, and cross-tissue adaptability. Here we introduce UniDCF, a unified framework capable of reconstructing multiple dentocraniofacial hard tissues through multimodal fusion encoding of point clouds and multi-view images. By leveraging the complementary strengths of each modality and incorporating a score-based denoising module to refine surface smoothness, UniDCF overcomes the limitations of prior single-modality approaches. We curated the largest multimodal dataset, comprising intraoral scans, CBCT, and CT from 6,609 patients, resulting in 54,555 annotated instances. Evaluations demonstrate that UniDCF outperforms existing state-of-the-art methods in terms of geometric precision, structural completeness, and spatial accuracy. Clinical simulations indicate UniDCF reduces reconstruction design time by 99% and achieves clinician-rated acceptability exceeding 94%. Overall, UniDCF enables rapid, automated, and high-fidelity reconstruction, supporting personalized and precise restorative treatments, streamlining clinical workflows, and enhancing patient outcomes.", "AI": {"tldr": "UniDCF\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u70b9\u4e91\u548c\u591a\u89c6\u89d2\u56fe\u50cf\u7684\u878d\u5408\u7f16\u7801\uff0c\u91cd\u5efa\u591a\u79cd\u7259\u988c\u9762\u786c\u7ec4\u7ec7\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5355\u6a21\u6001\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u51e0\u4f55\u7cbe\u5ea6\u3001\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u7a7a\u95f4\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7259\u988c\u9762\u786c\u7ec4\u7ec7\u7f3a\u635f\u4e25\u91cd\u5f71\u54cd\u60a3\u8005\u7684\u751f\u7406\u529f\u80fd\u3001\u9762\u90e8\u7f8e\u89c2\u548c\u5fc3\u7406\u5065\u5eb7\uff0c\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4ec5\u9650\u4e8e\u5355\u7ec4\u7ec7\u573a\u666f\u548c\u7279\u5b9a\u6a21\u6001\u6210\u50cf\u8f93\u5165\uff0c\u5bfc\u81f4\u6cdb\u5316\u6027\u5dee\uff0c\u9700\u8981\u5728\u89e3\u5256\u4fdd\u771f\u5ea6\u3001\u8ba1\u7b97\u6548\u7387\u548c\u8de8\u7ec4\u7ec7\u9002\u5e94\u6027\u4e4b\u95f4\u505a\u51fa\u6743\u8861\u3002", "method": "\u63d0\u51faUniDCF\u6846\u67b6\uff0c\u901a\u8fc7\u70b9\u4e91\u548c\u591a\u89c6\u89d2\u56fe\u50cf\u7684\u591a\u6a21\u6001\u878d\u5408\u7f16\u7801\uff0c\u5229\u7528\u5404\u6a21\u6001\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u5206\u6570\u7684\u53bb\u566a\u6a21\u5757\u6765\u4f18\u5316\u8868\u9762\u5e73\u6ed1\u5ea6\u3002\u6784\u5efa\u4e86\u5305\u542b6,609\u540d\u60a3\u8005\u7684\u53e3\u5185\u626b\u63cf\u3001CBCT\u548cCT\u6570\u636e\u7684\u5927\u578b\u591a\u6a21\u6001\u6570\u636e\u96c6\u3002", "result": "UniDCF\u5728\u51e0\u4f55\u7cbe\u5ea6\u3001\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u7a7a\u95f4\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u4e34\u5e8a\u6a21\u62df\u663e\u793a\u91cd\u5efa\u8bbe\u8ba1\u65f6\u95f4\u51cf\u5c1199%\uff0c\u4e34\u5e8a\u533b\u751f\u63a5\u53d7\u5ea6\u8d85\u8fc794%\u3002", "conclusion": "UniDCF\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u81ea\u52a8\u5316\u3001\u9ad8\u4fdd\u771f\u5ea6\u7684\u91cd\u5efa\uff0c\u652f\u6301\u4e2a\u6027\u5316\u548c\u7cbe\u786e\u7684\u4fee\u590d\u6cbb\u7597\uff0c\u7b80\u5316\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6539\u5584\u60a3\u8005\u6cbb\u7597\u6548\u679c\u3002"}}
{"id": "2508.11854", "pdf": "https://arxiv.org/pdf/2508.11854", "abs": "https://arxiv.org/abs/2508.11854", "authors": ["Matthew Hull", "Haoyang Yang", "Pratham Mehta", "Mansi Phute", "Aeree Cho", "Haorang Wang", "Matthew Lau", "Wenke Lee", "Wilian Lunardi", "Martin Andreoni", "Polo Chau"], "title": "ComplicitSplat: Downstream Models are Vulnerable to Blackbox Attacks by 3D Gaussian Splat Camouflages", "categories": ["cs.CV", "cs.LG"], "comment": "7 pages, 6 figures", "summary": "As 3D Gaussian Splatting (3DGS) gains rapid adoption in safety-critical tasks for efficient novel-view synthesis from static images, how might an adversary tamper images to cause harm? We introduce ComplicitSplat, the first attack that exploits standard 3DGS shading methods to create viewpoint-specific camouflage - colors and textures that change with viewing angle - to embed adversarial content in scene objects that are visible only from specific viewpoints and without requiring access to model architecture or weights. Our extensive experiments show that ComplicitSplat generalizes to successfully attack a variety of popular detector - both single-stage, multi-stage, and transformer-based models on both real-world capture of physical objects and synthetic scenes. To our knowledge, this is the first black-box attack on downstream object detectors using 3DGS, exposing a novel safety risk for applications like autonomous navigation and other mission-critical robotic systems.", "AI": {"tldr": "ComplicitSplat\u662f\u4e00\u79cd\u9488\u5bf93D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u7684\u9ed1\u76d2\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7279\u5b9a\u89c6\u89d2\u4e0b\u5d4c\u5165\u5bf9\u6297\u6027\u5185\u5bb9\u6765\u6b3a\u9a97\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u67b6\u6784\u6216\u6743\u91cd\u3002", "motivation": "\u968f\u77403D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5728\u5b89\u5168\u5173\u952e\u4efb\u52a1\u4e2d\u7684\u5feb\u901f\u5e94\u7528\uff0c\u9700\u8981\u7814\u7a76\u653b\u51fb\u8005\u5982\u4f55\u901a\u8fc7\u7be1\u6539\u56fe\u50cf\u6765\u9020\u6210\u5371\u5bb3\uff0c\u7279\u522b\u662f\u9488\u5bf9\u81ea\u4e3b\u5bfc\u822a\u7b49\u5173\u952e\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5b89\u5168\u98ce\u9669\u3002", "method": "\u5229\u7528\u6807\u51c63DGS\u7740\u8272\u65b9\u6cd5\u521b\u5efa\u89c6\u89d2\u7279\u5b9a\u7684\u4f2a\u88c5\uff08\u968f\u89c6\u89d2\u53d8\u5316\u7684\u989c\u8272\u548c\u7eb9\u7406\uff09\uff0c\u5728\u573a\u666f\u5bf9\u8c61\u4e2d\u5d4c\u5165\u4ec5\u4ece\u7279\u5b9a\u89c6\u89d2\u53ef\u89c1\u7684\u5bf9\u6297\u6027\u5185\u5bb9\uff0c\u5b9e\u73b0\u9ed1\u76d2\u653b\u51fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660eComplicitSplat\u80fd\u6210\u529f\u653b\u51fb\u5404\u79cd\u6d41\u884c\u7684\u68c0\u6d4b\u5668\uff08\u5355\u9636\u6bb5\u3001\u591a\u9636\u6bb5\u548c\u57fa\u4e8etransformer\u7684\u6a21\u578b\uff09\uff0c\u5728\u771f\u5b9e\u7269\u7406\u5bf9\u8c61\u6355\u83b7\u548c\u5408\u6210\u573a\u666f\u4e2d\u5747\u6709\u6548\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u4e0b\u6e38\u76ee\u6807\u68c0\u6d4b\u5668\u76843DGS\u9ed1\u76d2\u653b\u51fb\uff0c\u63ed\u793a\u4e86\u81ea\u4e3b\u5bfc\u822a\u7b49\u5173\u952e\u5e94\u7528\u7684\u65b0\u578b\u5b89\u5168\u98ce\u9669\u3002"}}
{"id": "2508.11893", "pdf": "https://arxiv.org/pdf/2508.11893", "abs": "https://arxiv.org/abs/2508.11893", "authors": ["Quanwei Hu", "Yinggan Tang", "Xuguang Zhang"], "title": "Large Kernel Modulation Network for Efficient Image Super-Resolution", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Image super-resolution (SR) in resource-constrained scenarios demands lightweight models balancing performance and latency. Convolutional neural networks (CNNs) offer low latency but lack non-local feature capture, while Transformers excel at non-local modeling yet suffer slow inference. To address this trade-off, we propose the Large Kernel Modulation Network (LKMN), a pure CNN-based model. LKMN has two core components: Enhanced Partial Large Kernel Block (EPLKB) and Cross-Gate Feed-Forward Network (CGFN). The EPLKB utilizes channel shuffle to boost inter-channel interaction, incorporates channel attention to focus on key information, and applies large kernel strip convolutions on partial channels for non-local feature extraction with reduced complexity. The CGFN dynamically adjusts discrepancies between input, local, and non-local features via a learnable scaling factor, then employs a cross-gate strategy to modulate and fuse these features, enhancing their complementarity. Extensive experiments demonstrate that our method outperforms existing state-of-the-art (SOTA) lightweight SR models while balancing quality and efficiency. Specifically, LKMN-L achieves 0.23 dB PSNR improvement over DAT-light on the Manga109 dataset at $\\times$4 upscale, with nearly $\\times$4.8 times faster. Codes are in the supplementary materials. The code is available at https://github.com/Supereeeee/LKMN.", "AI": {"tldr": "LKMN\u662f\u4e00\u4e2a\u7eafCNN\u7684\u8f7b\u91cf\u7ea7\u8d85\u5206\u8fa8\u7387\u6a21\u578b\uff0c\u901a\u8fc7\u5927\u6838\u8c03\u5236\u5757\u548c\u4ea4\u53c9\u95e8\u524d\u9988\u7f51\u7edc\uff0c\u5728\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u975e\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u9700\u6c42\uff0c\u4f20\u7edfCNN\u6a21\u578b\u7f3a\u4e4f\u975e\u5c40\u90e8\u7279\u5f81\u6355\u83b7\u80fd\u529b\uff0c\u800cTransformer\u867d\u7136\u64c5\u957f\u975e\u5c40\u90e8\u5efa\u6a21\u4f46\u63a8\u7406\u901f\u5ea6\u6162\uff0c\u9700\u8981\u5728\u6027\u80fd\u4e0e\u5ef6\u8fdf\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002", "method": "\u63d0\u51faLarge Kernel Modulation Network (LKMN)\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1aEnhanced Partial Large Kernel Block (EPLKB) \u4f7f\u7528\u901a\u9053\u6df7\u6d17\u589e\u5f3a\u901a\u9053\u95f4\u4ea4\u4e92\uff0c\u7ed3\u5408\u901a\u9053\u6ce8\u610f\u529b\u5173\u6ce8\u5173\u952e\u4fe1\u606f\uff0c\u5728\u90e8\u5206\u901a\u9053\u4e0a\u5e94\u7528\u5927\u6838\u6761\u5e26\u5377\u79ef\u8fdb\u884c\u975e\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\uff1bCross-Gate Feed-Forward Network (CGFN) \u901a\u8fc7\u53ef\u5b66\u4e60\u7f29\u653e\u56e0\u5b50\u52a8\u6001\u8c03\u6574\u8f93\u5165\u3001\u5c40\u90e8\u548c\u975e\u5c40\u90e8\u7279\u5f81\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u91c7\u7528\u4ea4\u53c9\u95e8\u7b56\u7565\u8c03\u5236\u548c\u878d\u5408\u8fd9\u4e9b\u7279\u5f81\u3002", "result": "\u5728Manga109\u6570\u636e\u96c6\u4e0a4\u500d\u653e\u5927\u65f6\uff0cLKMN-L\u6bd4DAT-light\u63d0\u53470.23 dB PSNR\uff0c\u63a8\u7406\u901f\u5ea6\u5feb\u7ea64.8\u500d\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u8f7b\u91cf\u7ea7\u8d85\u5206\u8fa8\u7387SOTA\u6a21\u578b\u3002", "conclusion": "LKMN\u8bc1\u660e\u4e86\u7eafCNN\u67b6\u6784\u53ef\u4ee5\u5728\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u7684\u540c\u65f6\u5b9e\u73b0\u6709\u6548\u7684\u975e\u5c40\u90e8\u7279\u5f81\u5efa\u6a21\uff0c\u4e3a\u8f7b\u91cf\u7ea7\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u63d0\u4f9b\u4e86\u6027\u80fd\u4e0e\u6548\u7387\u5e73\u8861\u7684\u4f18\u79c0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11922", "pdf": "https://arxiv.org/pdf/2508.11922", "abs": "https://arxiv.org/abs/2508.11922", "authors": ["Aditi Jahagirdar", "Sameer Joshi"], "title": "Assessment of Using Synthetic Data in Brain Tumor Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Manual brain tumor segmentation from MRI scans is challenging due to tumor heterogeneity, scarcity of annotated data, and class imbalance in medical imaging datasets. Synthetic data generated by generative models has the potential to mitigate these issues by improving dataset diversity. This study investigates, as a proof of concept, the impact of incorporating synthetic MRI data, generated using a pre-trained GAN model, into training a U-Net segmentation network. Experiments were conducted using real data from the BraTS 2020 dataset, synthetic data generated with the medigan library, and hybrid datasets combining real and synthetic samples in varying proportions. While overall quantitative performance (Dice coefficient, IoU, precision, recall, accuracy) was comparable between real-only and hybrid-trained models, qualitative inspection suggested that hybrid datasets, particularly with 40% real and 60% synthetic data, improved whole tumor boundary delineation. However, region-wise accuracy for the tumor core and the enhancing tumor remained lower, indicating a persistent class imbalance. The findings support the feasibility of synthetic data as an augmentation strategy for brain tumor segmentation, while highlighting the need for larger-scale experiments, volumetric data consistency, and mitigating class imbalance in future work.", "AI": {"tldr": "\u4f7f\u7528GAN\u751f\u6210\u7684\u5408\u6210MRI\u6570\u636e\u6765\u8865\u5145\u771f\u5b9e\u6570\u636e\u8fdb\u884c\u8111\u90e8\u813e\u7624\u5206\u5272\uff0c\u5728\u6574\u4f53\u6027\u80fd\u76f8\u4f3c\u7684\u60c5\u51b5\u4e0b\uff0c6:4\u7684\u5408\u6210\u771f\u5b9e\u6df7\u5408\u6bd4\u4f8b\u5728\u813e\u7624\u8fb9\u754c\u5206\u5272\u65b9\u9762\u663e\u793a\u51fa\u6539\u5584\u6548\u679c", "motivation": "\u89e3\u51b3\u8111\u813e\u7624\u624b\u52a8\u5206\u5272\u9762\u4e34\u7684\u6311\u6218\uff1a\u813e\u7624\u5f02\u8d28\u6027\u3001\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u3002\u901a\u8fc7\u751f\u6210\u5f0f\u6a21\u578b\u4ea7\u751f\u5408\u6210\u6570\u636e\u6765\u63d0\u5347\u6570\u636e\u96c6\u591a\u6837\u6027", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3GAN\u6a21\u578b(medigan\u5e93)\u751f\u6210\u5408\u6210MRI\u6570\u636e\uff0c\u5c06\u5176\u4e0eBraTS 2020\u771f\u5b9e\u6570\u636e\u6309\u4e0d\u540c\u6bd4\u4f8b\u6df7\u5408\uff0c\u8bad\u7ec3U-Net\u5206\u5272\u7f51\u7edc\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c", "result": "\u6574\u4f53\u91cf\u5316\u6307\u6807(Dice\u7cfb\u6570\u3001IoU\u7b49)\u5728\u771f\u5b9e\u6570\u636e\u548c\u6df7\u5408\u6570\u636e\u4e4b\u95f4\u76f8\u4f3c\uff0c\u4f46\u5b9a\u6027\u5206\u6790\u663e\u793a40%\u771f\u5b9e+60%\u5408\u6210\u7684\u6df7\u5408\u6bd4\u4f8b\u5728\u6574\u4f53\u813e\u7624\u8fb9\u754c\u5206\u5272\u65b9\u9762\u6709\u6539\u5584\uff0c\u4f46\u813e\u7624\u6838\u5fc3\u548c\u589e\u5f3a\u533a\u57df\u7684\u51c6\u786e\u6027\u4ecd\u8f83\u4f4e", "conclusion": "\u5408\u6210\u6570\u636e\u4f5c\u4e3a\u6570\u636e\u589e\u5f3a\u7b56\u7565\u5728\u8111\u813e\u7624\u5206\u5272\u4e2d\u5177\u6709\u53ef\u884c\u6027\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u8fdb\u884c\u66f4\u5927\u89c4\u6a21\u7684\u5b9e\u9a8c\u9a8c\u8bc1"}}
{"id": "2508.11932", "pdf": "https://arxiv.org/pdf/2508.11932", "abs": "https://arxiv.org/abs/2508.11932", "authors": ["Chengwei Zhang", "Xueyi Zhang", "Mingrui Lao", "Tao Jiang", "Xinhao Xu", "Wenjie Li", "Fubo Zhang", "Longyong Chen"], "title": "Deep Learning For Point Cloud Denoising: A Survey", "categories": ["cs.CV"], "comment": null, "summary": "Real-world environment-derived point clouds invariably exhibit noise across varying modalities and intensities. Hence, point cloud denoising (PCD) is essential as a preprocessing step to improve downstream task performance. Deep learning (DL)-based PCD models, known for their strong representation capabilities and flexible architectures, have surpassed traditional methods in denoising performance. To our best knowledge, despite recent advances in performance, no comprehensive survey systematically summarizes the developments of DL-based PCD. To fill the gap, this paper seeks to identify key challenges in DL-based PCD, summarizes the main contributions of existing methods, and proposes a taxonomy tailored to denoising tasks. To achieve this goal, we formulate PCD as a two-step process: outlier removal and surface noise restoration, encompassing most scenarios and requirements of PCD. Additionally, we compare methods in terms of similarities, differences, and respective advantages. Finally, we discuss research limitations and future directions, offering insights for further advancements in PCD.", "AI": {"tldr": "\u8fd9\u662f\u4e00\u7bc7\u5173\u4e8e\u6df1\u5ea6\u5b66\u4e60\u57fa\u4e8e\u70b9\u4e91\u53bb\u566a\u7684\u7efc\u8ff0\u6027\u8bba\u6587\uff0c\u7cfb\u7edf\u603b\u7ed3\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u72b6\u51b5\u3001\u6316\u6398\u5173\u952e\u6311\u6218\u3001\u63d0\u51fa\u5206\u7c7b\u4f53\u7cfb\u5e76\u5c55\u671b\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u70b9\u4e91\u6570\u636e\u5b58\u5728\u5404\u79cd\u566a\u58f0\uff0c\u53bb\u566a\u662f\u4e0b\u6e38\u4efb\u52a1\u7684\u5173\u952e\u9884\u5904\u7406\u6b65\u9aa4\u3002\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5df2\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u7684\u7814\u7a76\u7efc\u8ff0\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5c06\u70b9\u4e91\u53bb\u566a\u6a21\u578b\u5316\u4e3a\u4e24\u6b65\u8fc7\u7a0b\uff1a\u79bb\u7fa4\u70b9\u79fb\u9664\u548c\u8868\u9762\u566a\u58f0\u6062\u590d\u3002\u901a\u8fc7\u8fd9\u79cd\u5206\u6790\u6846\u67b6\uff0c\u5bf9\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u7cfb\u7edf\u5206\u7c7b\u548c\u6bd4\u8f83\u5206\u6790\uff0c\u63d0\u51fa\u4e13\u95e8\u9002\u7528\u4e8e\u53bb\u566a\u4efb\u52a1\u7684\u5206\u7c7b\u4f53\u7cfb\u3002", "result": "\u8bba\u6587\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u5206\u6790\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u6df1\u5ea6\u5b66\u4e60\u57fa\u4e8e\u70b9\u4e91\u53bb\u566a\u7684\u5173\u952e\u6311\u6218\uff0c\u7cfb\u7edf\u603b\u7ed3\u4e86\u5404\u79cd\u65b9\u6cd5\u7684\u4e3b\u8981\u8d21\u732e\u548c\u7279\u70b9\uff0c\u5e76\u5b8c\u6574\u5730\u6bd4\u8f83\u4e86\u4e0d\u540c\u65b9\u6cd5\u7684\u4f18\u52bf\u548c\u5dee\u5f02\u3002", "conclusion": "\u8bba\u6587\u4e3a\u6df1\u5ea6\u5b66\u4e60\u57fa\u4e8e\u70b9\u4e91\u53bb\u566a\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u7814\u7a76\u6846\u67b6\uff0c\u4e0d\u4ec5\u7cfb\u7edf\u603b\u7ed3\u4e86\u73b0\u6709\u6280\u672f\u72b6\u51b5\uff0c\u8fd8\u63d0\u51fa\u4e86\u7814\u7a76\u9650\u5236\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u5bf9\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u5177\u6709\u91cd\u8981\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2508.11952", "pdf": "https://arxiv.org/pdf/2508.11952", "abs": "https://arxiv.org/abs/2508.11952", "authors": ["Yueming Xu", "Jiahui Zhang", "Ze Huang", "Yurui Chen", "Yanpeng Zhou", "Zhenyu Chen", "Yu-Jie Yuan", "Pengxiang Xia", "Guowei Huang", "Xinyue Cai", "Zhongang Qi", "Xingyue Quan", "Jianye Hao", "Hang Xu", "Li Zhang"], "title": "UniUGG: Unified 3D Understanding and Generation via Geometric-Semantic Encoding", "categories": ["cs.CV"], "comment": null, "summary": "Despite the impressive progress on understanding and generating images shown by the recent unified architectures, the integration of 3D tasks remains challenging and largely unexplored. In this paper, we introduce UniUGG, the first unified understanding and generation framework for 3D modalities. Our unified framework employs an LLM to comprehend and decode sentences and 3D representations. At its core, we propose a spatial decoder leveraging a latent diffusion model to generate high-quality 3D representations. This allows for the generation and imagination of 3D scenes based on a reference image and an arbitrary view transformation, while remaining supports for spatial visual question answering (VQA) tasks. Additionally, we propose a geometric-semantic learning strategy to pretrain the vision encoder. This design jointly captures the input's semantic and geometric cues, enhancing both spatial understanding and generation. Extensive experimental results demonstrate the superiority of our method in visual representation, spatial understanding, and 3D generation. The source code will be released upon paper acceptance.", "AI": {"tldr": "UniUGG\u662f\u9996\u4e2a\u7edf\u4e00\u7406\u89e3\u548c\u751f\u62103D\u6a21\u6001\u7684\u6846\u67b6\uff0c\u4f7f\u7528LLM\u7406\u89e3\u53e5\u5b50\u548c3D\u8868\u793a\uff0c\u901a\u8fc7\u7a7a\u95f4\u89e3\u7801\u5668\u548c\u6f5c\u5728\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf3D\u5185\u5bb9\uff0c\u652f\u6301\u7a7a\u95f4VQA\u4efb\u52a1\u548c\u57fa\u4e8e\u53c2\u8003\u56fe\u50cf\u76843D\u573a\u666f\u751f\u6210\u3002", "motivation": "\u5c3d\u7ba1\u7edf\u4e00\u67b6\u6784\u5728\u56fe\u50cf\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f463D\u4efb\u52a1\u7684\u6574\u5408\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u4e14\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u5904\u74063D\u7406\u89e3\u548c\u751f\u6210\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u63d0\u51faUniUGG\u6846\u67b6\uff0c\u4f7f\u7528LLM\u8fdb\u884c\u53e5\u5b50\u548c3D\u8868\u793a\u7684\u89e3\u7801\uff0c\u6838\u5fc3\u662f\u91c7\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u7a7a\u95f4\u89e3\u7801\u5668\u751f\u62103D\u8868\u793a\uff0c\u5e76\u63d0\u51fa\u51e0\u4f55\u8bed\u4e49\u5b66\u4e60\u7b56\u7565\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u8054\u5408\u6355\u83b7\u8f93\u5165\u7684\u8bed\u4e49\u548c\u51e0\u4f55\u7ebf\u7d22\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u8868\u793a\u3001\u7a7a\u95f4\u7406\u89e3\u548c3D\u751f\u6210\u65b9\u9762\u5177\u6709\u4f18\u8d8a\u6027\u3002", "conclusion": "UniUGG\u6210\u529f\u5b9e\u73b0\u4e863D\u6a21\u6001\u7684\u7edf\u4e00\u7406\u89e3\u548c\u751f\u6210\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u7a7a\u95f4\u89e3\u7801\u5668\u548c\u51e0\u4f55\u8bed\u4e49\u5b66\u4e60\u7b56\u7565\uff0c\u5728\u591a\u4e2a3D\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e3a3D\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12015", "pdf": "https://arxiv.org/pdf/2508.12015", "abs": "https://arxiv.org/abs/2508.12015", "authors": ["Hongyuan Liu", "Haochen Yu", "Jianfei Jiang", "Qiankun Liu", "Jiansheng Chen", "Huimin Ma"], "title": "InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing dynamic driving scenes from dashcam videos has attracted increasing attention due to its significance in autonomous driving and scene understanding. While recent advances have made impressive progress, most methods still unify all background elements into a single representation, hindering both instance-level understanding and flexible scene editing. Some approaches attempt to lift 2D segmentation into 3D space, but often rely on pre-processed instance IDs or complex pipelines to map continuous features to discrete identities. Moreover, these methods are typically designed for indoor scenes with rich viewpoints, making them less applicable to outdoor driving scenarios. In this paper, we present InstDrive, an instance-aware 3D Gaussian Splatting framework tailored for the interactive reconstruction of dynamic driving scene. We use masks generated by SAM as pseudo ground-truth to guide 2D feature learning via contrastive loss and pseudo-supervised objectives. At the 3D level, we introduce regularization to implicitly encode instance identities and enforce consistency through a voxel-based loss. A lightweight static codebook further bridges continuous features and discrete identities without requiring data pre-processing or complex optimization. Quantitative and qualitative experiments demonstrate the effectiveness of InstDrive, and to the best of our knowledge, it is the first framework to achieve 3D instance segmentation in dynamic, open-world driving scenes.More visualizations are available at our project page.", "AI": {"tldr": "InstDrive\u662f\u4e00\u4e2a\u9488\u5bf9\u52a8\u6001\u9a7e\u9a76\u573a\u666f\u7684\u5b9e\u4f8b\u611f\u77e53D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u5f00\u653e\u4e16\u754c\u52a8\u6001\u9a7e\u9a76\u573a\u666f\u76843D\u5b9e\u4f8b\u5206\u5272\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u80cc\u666f\u5143\u7d20\u7edf\u4e00\u4e3a\u5355\u4e00\u8868\u793a\uff0c\u963b\u788d\u4e86\u5b9e\u4f8b\u7ea7\u7406\u89e3\u548c\u7075\u6d3b\u573a\u666f\u7f16\u8f91\uff1b\u73b0\u6709\u5ba4\u5185\u573a\u666f\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u6237\u5916\u9a7e\u9a76\u573a\u666f\uff1b\u9700\u8981\u89e3\u51b32D\u5206\u5272\u52303D\u7a7a\u95f4\u7684\u6620\u5c04\u95ee\u9898\u3002", "method": "\u4f7f\u7528SAM\u751f\u6210\u7684\u63a9\u7801\u4f5c\u4e3a\u4f2a\u771f\u503c\uff0c\u901a\u8fc7\u5bf9\u6bd4\u635f\u5931\u548c\u4f2a\u76d1\u7763\u76ee\u6807\u6307\u5bfc2D\u7279\u5f81\u5b66\u4e60\uff1b\u57283D\u5c42\u9762\u5f15\u5165\u6b63\u5219\u5316\u9690\u5f0f\u7f16\u7801\u5b9e\u4f8b\u8eab\u4efd\uff0c\u901a\u8fc7\u4f53\u7d20\u635f\u5931\u5f3a\u5236\u4e00\u81f4\u6027\uff1b\u4f7f\u7528\u8f7b\u91cf\u7ea7\u9759\u6001\u7801\u672c\u6865\u63a5\u8fde\u7eed\u7279\u5f81\u548c\u79bb\u6563\u8eab\u4efd\u3002", "result": "\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u8bc1\u660e\u4e86InstDrive\u7684\u6709\u6548\u6027\uff0c\u9996\u6b21\u5728\u52a8\u6001\u5f00\u653e\u4e16\u754c\u9a7e\u9a76\u573a\u666f\u4e2d\u5b9e\u73b03D\u5b9e\u4f8b\u5206\u5272\u3002", "conclusion": "InstDrive\u4e3a\u52a8\u6001\u9a7e\u9a76\u573a\u666f\u7684\u4ea4\u4e92\u5f0f\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b9e\u4f8b\u611f\u77e5\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u6570\u636e\u9884\u5904\u7406\u6216\u590d\u6742\u4f18\u5316\u5373\u53ef\u5b9e\u73b03D\u5b9e\u4f8b\u5206\u5272\u3002"}}
{"id": "2508.12084", "pdf": "https://arxiv.org/pdf/2508.12084", "abs": "https://arxiv.org/abs/2508.12084", "authors": ["Jaejun Hwang", "Dayoung Gong", "Manjin Kim", "Minsu Cho"], "title": "Generic Event Boundary Detection via Denoising Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to ICCV 2025", "summary": "Generic event boundary detection (GEBD) aims to identify natural boundaries in a video, segmenting it into distinct and meaningful chunks. Despite the inherent subjectivity of event boundaries, previous methods have focused on deterministic predictions, overlooking the diversity of plausible solutions. In this paper, we introduce a novel diffusion-based boundary detection model, dubbed DiffGEBD, that tackles the problem of GEBD from a generative perspective. The proposed model encodes relevant changes across adjacent frames via temporal self-similarity and then iteratively decodes random noise into plausible event boundaries being conditioned on the encoded features. Classifier-free guidance allows the degree of diversity to be controlled in denoising diffusion. In addition, we introduce a new evaluation metric to assess the quality of predictions considering both diversity and fidelity. Experiments show that our method achieves strong performance on two standard benchmarks, Kinetics-GEBD and TAPOS, generating diverse and plausible event boundaries.", "AI": {"tldr": "DiffGEBD\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u901a\u7528\u4e8b\u4ef6\u8fb9\u754c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u89c6\u89d2\u89e3\u51b3\u4e8b\u4ef6\u8fb9\u754c\u68c0\u6d4b\u95ee\u9898\uff0c\u80fd\u591f\u4ea7\u751f\u591a\u6837\u5316\u7684\u8fb9\u754c\u9884\u6d4b\u7ed3\u679c\u3002", "motivation": "\u4f20\u7edf\u7684\u4e8b\u4ef6\u8fb9\u754c\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u786e\u5b9a\u6027\u9884\u6d4b\uff0c\u5ffd\u7565\u4e86\u4e8b\u4ef6\u8fb9\u754c\u7684\u4e3b\u89c2\u6027\u548c\u89e3\u51b3\u65b9\u6848\u7684\u591a\u6837\u6027\u3002\u672c\u6587\u65e8\u5728\u4ece\u751f\u6210\u5f0f\u89d2\u5ea6\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u4f9b\u591a\u79cd\u5408\u7406\u7684\u8fb9\u754c\u9884\u6d4b\u3002", "method": "\u63d0\u51faDiffGEBD\u6a21\u578b\uff1a1\uff09\u901a\u8fc7\u65f6\u95f4\u81ea\u76f8\u4f3c\u6027\u7f16\u7801\u76f8\u90bb\u5e27\u4e4b\u95f4\u7684\u76f8\u5173\u53d8\u5316\uff1b2\uff09\u4f7f\u7528\u6269\u6563\u6a21\u578b\u8fed\u4ee3\u5730\u5c06\u968f\u673a\u566a\u58f0\u89e3\u7801\u4e3a\u5408\u7406\u7684\u4e8b\u4ef6\u8fb9\u754c\uff1b3\uff09\u91c7\u7528\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u6765\u63a7\u5236\u53bb\u566a\u8fc7\u7a0b\u4e2d\u7684\u591a\u6837\u6027\u7a0b\u5ea6\u3002", "result": "\u5728Kinetics-GEBD\u548cTAPOS\u4e24\u4e2a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u5f3a\u52b2\u6027\u80fd\uff0c\u80fd\u591f\u751f\u6210\u591a\u6837\u4e14\u5408\u7406\u7684\u4e8b\u4ef6\u8fb9\u754c\u3002", "conclusion": "DiffGEBD\u6210\u529f\u5730\u5c06\u6269\u6563\u6a21\u578b\u5e94\u7528\u4e8e\u4e8b\u4ef6\u8fb9\u754c\u68c0\u6d4b\u4efb\u52a1\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u80fd\u591f\u4ea7\u751f\u591a\u6837\u5316\u9884\u6d4b\u7684\u65b0\u65b9\u6cd5\uff0c\u540c\u65f6\u5f15\u5165\u4e86\u8003\u8651\u591a\u6837\u6027\u548c\u4fdd\u771f\u5ea6\u7684\u65b0\u8bc4\u4f30\u6307\u6807\u3002"}}
{"id": "2508.12094", "pdf": "https://arxiv.org/pdf/2508.12094", "abs": "https://arxiv.org/abs/2508.12094", "authors": ["Songwei Liu", "Hong Liu", "Fangmin Chen", "Xurui Peng", "Chenqian Yan", "Lean Fu", "Xing Mei"], "title": "Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have transformed image synthesis by establishing unprecedented quality and creativity benchmarks. Nevertheless, their large-scale deployment faces challenges due to computationally intensive iterative denoising processes. Although post-training quantization(PTQ) provides an effective pathway for accelerating sampling, the iterative nature of diffusion models causes stepwise quantization errors to accumulate progressively during generation, inevitably compromising output fidelity. To address this challenge, we develop a theoretical framework that mathematically formulates error propagation in Diffusion Models (DMs), deriving per-step quantization error propagation equations and establishing the first closed-form solution for cumulative error. Building on this theoretical foundation, we propose a timestep-aware cumulative error compensation scheme. Extensive experiments across multiple image datasets demonstrate that our compensation strategy effectively mitigates error propagation, significantly enhancing existing PTQ methods to achieve state-of-the-art(SOTA) performance on low-precision diffusion models.", "AI": {"tldr": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63a8\u5bfc\u53d8\u6362\u6a21\u578b\u4e2d\u91cf\u5316\u9519\u8bef\u4f20\u64ad\u65b9\u7a0b\uff0c\u63d0\u51fa\u65f6\u95f4\u6b65\u611f\u77e5\u7684\u7d2f\u8ba1\u9519\u8bef\u8865\u507f\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4f4e\u7cbe\u5ea6\u53d8\u6362\u6a21\u578b\u7684\u6027\u80fd", "motivation": "\u53d8\u6362\u6a21\u578b\u5728\u56fe\u50cf\u5408\u6210\u9886\u57df\u8fbe\u5230\u4e86\u7a81\u7834\u6027\u8d28\u91cf\uff0c\u4f46\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u8f83\u5927\uff0c\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u5728\u52a0\u901f\u91c7\u6837\u65f6\u5b58\u5728\u6b65\u8fdb\u5f0f\u91cf\u5316\u9519\u8bef\u7d2f\u79ef\u95ee\u9898\uff0c\u5f71\u54cd\u8f93\u51fa\u4fdd\u771f\u5ea6", "method": "\u5efa\u7acb\u7406\u8bba\u6846\u67b6\u6570\u5b66\u5f62\u5f0f\u5316\u53d8\u6362\u6a21\u578b\u4e2d\u7684\u9519\u8bef\u4f20\u64ad\u673a\u5236\uff0c\u63a8\u5bfc\u6bcf\u6b65\u91cf\u5316\u9519\u8bef\u4f20\u64ad\u65b9\u7a0b\uff0c\u5e76\u5f97\u51fa\u7d2f\u8ba1\u9519\u8bef\u7684\u95ed\u5f0f\u89e3\u3002\u57fa\u4e8e\u7406\u8bba\u57fa\u7840\u63d0\u51fa\u65f6\u95f4\u6b65\u611f\u77e5\u7684\u7d2f\u8ba1\u9519\u8bef\u8865\u507f\u7b56\u7565", "result": "\u5728\u591a\u4e2a\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u8865\u507f\u7b56\u7565\u80fd\u591f\u6709\u6548\u51cf\u5c11\u9519\u8bef\u4f20\u64ad\uff0c\u663e\u8457\u63d0\u5347\u73b0\u6709PTQ\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5728\u4f4e\u7cbe\u5ea6\u53d8\u6362\u6a21\u578b\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u65f6\u95f4\u6b65\u611f\u77e5\u7684\u7d2f\u8ba1\u9519\u8bef\u8865\u507f\u7b56\u7565\u80fd\u591f\u6709\u6548\u89e3\u51b3\u53d8\u6362\u6a21\u578b\u91cf\u5316\u4e2d\u7684\u9519\u8bef\u7d2f\u79ef\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u52a0\u901f\u65b9\u6848"}}
{"id": "2508.12131", "pdf": "https://arxiv.org/pdf/2508.12131", "abs": "https://arxiv.org/abs/2508.12131", "authors": ["Minh Tran", "Johnmark Clements", "Annie Prasanna", "Tri Nguyen", "Ngan Le"], "title": "DualFit: A Two-Stage Virtual Try-On via Warping and Synthesis", "categories": ["cs.CV"], "comment": "Retail Vision, ICCV 2025", "summary": "Virtual Try-On technology has garnered significant attention for its potential to transform the online fashion retail experience by allowing users to visualize how garments would look on them without physical trials. While recent advances in diffusion-based warping-free methods have improved perceptual quality, they often fail to preserve fine-grained garment details such as logos and printed text elements that are critical for brand integrity and customer trust. In this work, we propose DualFit, a hybrid VTON pipeline that addresses this limitation by two-stage approach. In the first stage, DualFit warps the target garment to align with the person image using a learned flow field, ensuring high-fidelity preservation. In the second stage, a fidelity-preserving try-on module synthesizes the final output by blending the warped garment with preserved human regions. Particularly, to guide this process, we introduce a preserved-region input and an inpainting mask, enabling the model to retain key areas and regenerate only where necessary, particularly around garment seams. Extensive qualitative results show that DualFit achieves visually seamless try-on results while faithfully maintaining high-frequency garment details, striking an effective balance between reconstruction accuracy and perceptual realism.", "AI": {"tldr": "DualFit\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u865a\u62df\u8bd5\u7a7f\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d8\u5f62\u548c\u4fdd\u771f\u5ea6\u4fdd\u6301\u6a21\u5757\uff0c\u5728\u4fdd\u6301\u670d\u88c5\u7ec6\u8282\u7684\u540c\u65f6\u5b9e\u73b0\u65e0\u7f1d\u8bd5\u7a7f\u6548\u679c", "motivation": "\u89e3\u51b3\u73b0\u6709\u6269\u6563\u5f0f\u865a\u62df\u8bd5\u7a7f\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u6301\u670d\u88c5\u7cbe\u7ec6\u7ec6\u8282\uff08\u5982logo\u548c\u5370\u5237\u6587\u5b57\uff09\u7684\u95ee\u9898\uff0c\u8fd9\u5bf9\u54c1\u724c\u5b8c\u6574\u6027\u548c\u5ba2\u6237\u4fe1\u4efb\u81f3\u5173\u91cd\u8981", "method": "\u4e24\u9636\u6bb5\u6df7\u5408\u7ba1\u9053\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u5b66\u4e60\u6d41\u573a\u53d8\u5f62\u76ee\u6807\u670d\u88c5\u4e0e\u4eba\u50cf\u5bf9\u9f50\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u4fdd\u771f\u5ea6\u4fdd\u6301\u8bd5\u7a7f\u6a21\u5757\uff0c\u7ed3\u5408\u4fdd\u7559\u533a\u57df\u8f93\u5165\u548c\u4fee\u590d\u63a9\u7801\uff0c\u5408\u6210\u6700\u7ec8\u8f93\u51fa", "result": "\u5e7f\u6cdb\u7684\u5b9a\u6027\u7ed3\u679c\u663e\u793aDualFit\u5b9e\u73b0\u4e86\u89c6\u89c9\u65e0\u7f1d\u7684\u8bd5\u7a7f\u6548\u679c\uff0c\u540c\u65f6\u5fe0\u5b9e\u5730\u4fdd\u6301\u4e86\u9ad8\u9891\u670d\u88c5\u7ec6\u8282\uff0c\u5728\u91cd\u5efa\u51c6\u786e\u6027\u548c\u611f\u77e5\u771f\u5b9e\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u6709\u6548\u5e73\u8861", "conclusion": "DualFil\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u865a\u62df\u8bd5\u7a7f\u4e2d\u7ec6\u8282\u4fdd\u6301\u7684\u6311\u6218\uff0c\u4e3a\u5728\u7ebf\u65f6\u5c1a\u96f6\u552e\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u865a\u62df\u8bd5\u7a7f\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12132", "pdf": "https://arxiv.org/pdf/2508.12132", "abs": "https://arxiv.org/abs/2508.12132", "authors": ["Amira Guesmi", "Bassem Ouni", "Muhammad Shafique"], "title": "TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "Quantized Neural Networks (QNNs) are increasingly deployed in edge and resource-constrained environments due to their efficiency in computation and memory usage. While shown to distort the gradient landscape and weaken conventional pixel-level attacks, it provides limited robustness against patch-based adversarial attacks-localized, high-saliency perturbations that remain surprisingly transferable across bit-widths. Existing defenses either overfit to fixed quantization settings or fail to address this cross-bit generalization vulnerability. We introduce \\textbf{TriQDef}, a tri-level quantization-aware defense framework designed to disrupt the transferability of patch-based adversarial attacks across QNNs. TriQDef consists of: (1) a Feature Disalignment Penalty (FDP) that enforces semantic inconsistency by penalizing perceptual similarity in intermediate representations; (2) a Gradient Perceptual Dissonance Penalty (GPDP) that explicitly misaligns input gradients across bit-widths by minimizing structural and directional agreement via Edge IoU and HOG Cosine metrics; and (3) a Joint Quantization-Aware Training Protocol that unifies these penalties within a shared-weight training scheme across multiple quantization levels. Extensive experiments on CIFAR-10 and ImageNet demonstrate that TriQDef reduces Attack Success Rates (ASR) by over 40\\% on unseen patch and quantization combinations, while preserving high clean accuracy. Our findings underscore the importance of disrupting both semantic and perceptual gradient alignment to mitigate patch transferability in QNNs.", "AI": {"tldr": "TriQDef\u662f\u4e00\u4e2a\u9488\u5bf9\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u7684\u4e09\u7ea7\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5f81\u4e0d\u5bf9\u9f50\u60e9\u7f5a\u548c\u68af\u5ea6\u611f\u77e5\u5931\u8c10\u60e9\u7f5a\u6765\u7834\u574f\u8de8\u6bd4\u7279\u5bbd\u5ea6\u7684\u8865\u4e01\u5f0f\u5bf9\u6297\u653b\u51fb\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u867d\u7136\u63d0\u9ad8\u4e86\u8ba1\u7b97\u548c\u5185\u5b58\u6548\u7387\uff0c\u4f46\u5bf9\u57fa\u4e8e\u8865\u4e01\u7684\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u6709\u9650\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u8981\u4e48\u8fc7\u62df\u5408\u4e8e\u56fa\u5b9a\u91cf\u5316\u8bbe\u7f6e\uff0c\u8981\u4e48\u65e0\u6cd5\u89e3\u51b3\u8de8\u6bd4\u7279\u6cdb\u5316\u6f0f\u6d1e\u3002", "method": "TriQDef\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u7279\u5f81\u4e0d\u5bf9\u9f50\u60e9\u7f5a(FDP)\u901a\u8fc7\u60e9\u7f5a\u4e2d\u95f4\u8868\u793a\u7684\u611f\u77e5\u76f8\u4f3c\u6027\u6765\u5f3a\u5236\u8bed\u4e49\u4e0d\u4e00\u81f4\uff1b\u68af\u5ea6\u611f\u77e5\u5931\u8c10\u60e9\u7f5a(GPDP)\u901a\u8fc7\u8fb9\u7f18IoU\u548cHOG\u4f59\u5f26\u5ea6\u91cf\u6700\u5c0f\u5316\u7ed3\u6784\u6027\u548c\u65b9\u5411\u6027\u4e00\u81f4\u6027\u6765\u663e\u5f0f\u9519\u4f4d\u8f93\u5165\u68af\u5ea6\uff1b\u8054\u5408\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u534f\u8bae\u5728\u591a\u4e2a\u91cf\u5316\u7ea7\u522b\u4e0a\u7edf\u4e00\u8fd9\u4e9b\u60e9\u7f5a\u3002", "result": "\u5728CIFAR-10\u548cImageNet\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTriQDef\u5728\u672a\u89c1\u8fc7\u7684\u8865\u4e01\u548c\u91cf\u5316\u7ec4\u5408\u4e0a\u5c06\u653b\u51fb\u6210\u529f\u7387(ASR)\u964d\u4f4e\u4e8640%\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6e05\u6d01\u51c6\u786e\u7387\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u7834\u574f\u8bed\u4e49\u548c\u611f\u77e5\u68af\u5ea6\u5bf9\u9f50\u5bf9\u4e8e\u51cf\u8f7b\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u4e2d\u8865\u4e01\u53ef\u8fc1\u79fb\u6027\u7684\u91cd\u8981\u6027\uff0cTriQDef\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u6bd4\u7279\u6cdb\u5316\u6f0f\u6d1e\u95ee\u9898\u3002"}}
{"id": "2508.12163", "pdf": "https://arxiv.org/pdf/2508.12163", "abs": "https://arxiv.org/abs/2508.12163", "authors": ["Wenqing Wang", "Yun Fu"], "title": "RealTalk: Realistic Emotion-Aware Lifelike Talking-Head Synthesis", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG", "I.4; I.3; I.2"], "comment": "Accepted to the ICCV 2025 Workshop on Artificial Social Intelligence", "summary": "Emotion is a critical component of artificial social intelligence. However, while current methods excel in lip synchronization and image quality, they often fail to generate accurate and controllable emotional expressions while preserving the subject's identity. To address this challenge, we introduce RealTalk, a novel framework for synthesizing emotional talking heads with high emotion accuracy, enhanced emotion controllability, and robust identity preservation. RealTalk employs a variational autoencoder (VAE) to generate 3D facial landmarks from driving audio, which are concatenated with emotion-label embeddings using a ResNet-based landmark deformation model (LDM) to produce emotional landmarks. These landmarks and facial blendshape coefficients jointly condition a novel tri-plane attention Neural Radiance Field (NeRF) to synthesize highly realistic emotional talking heads. Extensive experiments demonstrate that RealTalk outperforms existing methods in emotion accuracy, controllability, and identity preservation, advancing the development of socially intelligent AI systems.", "AI": {"tldr": "RealTalk\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u60c5\u611f\u8bf4\u8bdd\u5934\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7VAE\u751f\u62103D\u9762\u90e8\u6807\u5fd7\u70b9\uff0c\u7ed3\u5408\u60c5\u611f\u6807\u7b7e\u5d4c\u5165\u548cResNet-based LDM\u4ea7\u751f\u60c5\u611f\u6807\u5fd7\u70b9\uff0c\u518d\u901a\u8fc7tri-plane attention NeRF\u5408\u6210\u9ad8\u8d28\u91cf\u60c5\u611f\u8bf4\u8bdd\u5934\uff0c\u5728\u60c5\u611f\u51c6\u786e\u6027\u3001\u53ef\u63a7\u6027\u548c\u8eab\u4efd\u4fdd\u6301\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u867d\u7136\u5728\u5507\u5f62\u540c\u6b65\u548c\u56fe\u50cf\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u751f\u6210\u51c6\u786e\u53ef\u63a7\u7684\u60c5\u611f\u8868\u8fbe\u540c\u65f6\u4fdd\u6301\u4e3b\u4f53\u8eab\u4efd\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u8fd9\u9650\u5236\u4e86\u4eba\u5de5\u667a\u80fd\u793e\u4ea4\u667a\u80fd\u7684\u53d1\u5c55\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668(VAE)\u4ece\u9a71\u52a8\u97f3\u9891\u751f\u62103D\u9762\u90e8\u6807\u5fd7\u70b9\uff0c\u901a\u8fc7ResNet-based\u5730\u6807\u53d8\u5f62\u6a21\u578b(LDM)\u5c06\u60c5\u611f\u6807\u7b7e\u5d4c\u5165\u4e0e\u6807\u5fd7\u70b9\u8fde\u63a5\uff0c\u4ea7\u751f\u60c5\u611f\u6807\u5fd7\u70b9\u3002\u8fd9\u4e9b\u6807\u5fd7\u70b9\u548c\u9762\u90e8\u6df7\u5408\u5f62\u72b6\u7cfb\u6570\u5171\u540c\u6761\u4ef6\u5316\u65b0\u9896\u7684\u4e09\u5e73\u9762\u6ce8\u610f\u529b\u795e\u7ecf\u8f90\u5c04\u573a(NeRF)\u6765\u5408\u6210\u60c5\u611f\u8bf4\u8bdd\u5934\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cRealTalk\u5728\u60c5\u611f\u51c6\u786e\u6027\u3001\u53ef\u63a7\u6027\u548c\u8eab\u4efd\u4fdd\u6301\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u793e\u4ea4\u667a\u80fdAI\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "conclusion": "RealTalk\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u60c5\u611f\u8bf4\u8bdd\u5934\u5408\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u51c6\u786e\u7684\u60c5\u611f\u8868\u8fbe\u548c\u8eab\u4efd\u4fdd\u6301\uff0c\u4e3a\u4eba\u5de5\u667a\u80fd\u793e\u4ea4\u667a\u80fd\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u652f\u6491\u3002"}}
{"id": "2508.12176", "pdf": "https://arxiv.org/pdf/2508.12176", "abs": "https://arxiv.org/abs/2508.12176", "authors": ["Zhiwei Zheng", "Dongyin Hu", "Mingmin Zhao"], "title": "Scalable RF Simulation in Generative 4D Worlds", "categories": ["cs.CV"], "comment": null, "summary": "Radio Frequency (RF) sensing has emerged as a powerful, privacy-preserving alternative to vision-based methods for indoor perception tasks. However, collecting high-quality RF data in dynamic and diverse indoor environments remains a major challenge. To address this, we introduce WaveVerse, a prompt-based, scalable framework that simulates realistic RF signals from generated indoor scenes with human motions. WaveVerse introduces a language-guided 4D world generator, which includes a state-aware causal transformer for human motion generation conditioned on spatial constraints and texts, and a phase-coherent ray tracing simulator that enables the simulation of accurate and coherent RF signals. Experiments demonstrate the effectiveness of our approach in conditioned human motion generation and highlight how phase coherence is applied to beamforming and respiration monitoring. We further present two case studies in ML-based high-resolution imaging and human activity recognition, demonstrating that WaveVerse not only enables data generation for RF imaging for the first time, but also consistently achieves performance gain in both data-limited and data-adequate scenarios.", "AI": {"tldr": "WaveVerse\u662f\u4e00\u4e2a\u57fa\u4e8e\u63d0\u793a\u7684RF\u4fe1\u53f7\u4eff\u771f\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u8a00\u5f15\u5bfc\u751f\u6210\u5ba4\u5185\u573a\u666f\u548c\u4eba\u4f53\u8fd0\u52a8\uff0c\u4f7f\u7528\u76f8\u4f4d\u76f8\u5e72\u5c04\u7ebf\u8ffd\u8e2a\u6a21\u62df\u771f\u5b9eRF\u4fe1\u53f7\uff0c\u89e3\u51b3\u4e86RF\u6570\u636e\u91c7\u96c6\u96be\u9898\u3002", "motivation": "RF\u4f20\u611f\u4f5c\u4e3a\u9690\u79c1\u4fdd\u62a4\u7684\u5ba4\u5185\u611f\u77e5\u66ff\u4ee3\u65b9\u6848\u9762\u4e34\u9ad8\u8d28\u91cf\u6570\u636e\u91c7\u96c6\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u591a\u6837\u7684\u5ba4\u5185\u73af\u5883\u4e2d\u96be\u4ee5\u83b7\u53d6\u8db3\u591f\u7684\u8bad\u7ec3\u6570\u636e\u3002", "method": "\u91c7\u7528\u8bed\u8a00\u5f15\u5bfc\u76844D\u4e16\u754c\u751f\u6210\u5668\uff0c\u5305\u62ec\u72b6\u6001\u611f\u77e5\u56e0\u679c\u53d8\u6362\u5668\u7528\u4e8e\u57fa\u4e8e\u7a7a\u95f4\u7ea6\u675f\u548c\u6587\u672c\u7684\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\uff0c\u4ee5\u53ca\u76f8\u4f4d\u76f8\u5e72\u5c04\u7ebf\u8ffd\u8e2a\u6a21\u62df\u5668\u6765\u4ea7\u751f\u51c6\u786e\u7684RF\u4fe1\u53f7\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u6761\u4ef6\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\u65b9\u9762\u6709\u6548\uff0c\u76f8\u4f4d\u76f8\u5e72\u6027\u6210\u529f\u5e94\u7528\u4e8e\u6ce2\u675f\u6210\u5f62\u548c\u547c\u5438\u76d1\u6d4b\u3002\u5728RF\u6210\u50cf\u548c\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u6848\u4f8b\u4e2d\uff0cWaveVerse\u9996\u6b21\u5b9e\u73b0\u4e86RF\u6210\u50cf\u6570\u636e\u751f\u6210\uff0c\u5e76\u5728\u6570\u636e\u6709\u9650\u548c\u5145\u8db3\u573a\u666f\u4e0b\u5747\u83b7\u5f97\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "WaveVerse\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684RF\u4fe1\u53f7\u4eff\u771f\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684RF\u8bad\u7ec3\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u5b9e\u9645\u6570\u636e\u91c7\u96c6\u96be\u9898\uff0c\u4e3aRF\u611f\u77e5\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6570\u636e\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12271", "pdf": "https://arxiv.org/pdf/2508.12271", "abs": "https://arxiv.org/abs/2508.12271", "authors": ["Ronghua Xu", "Jin Xie", "Jing Nie", "Jiale Cao", "Yanwei Pang"], "title": "SNNSIR: A Simple Spiking Neural Network for Stereo Image Restoration", "categories": ["cs.CV"], "comment": "11 pages", "summary": "Spiking Neural Networks (SNNs), characterized by discrete binary activations, offer high computational efficiency and low energy consumption, making them well-suited for computation-intensive tasks such as stereo image restoration. In this work, we propose SNNSIR, a simple yet effective Spiking Neural Network for Stereo Image Restoration, specifically designed under the spike-driven paradigm where neurons transmit information through sparse, event-based binary spikes. In contrast to existing hybrid SNN-ANN models that still rely on operations such as floating-point matrix division or exponentiation, which are incompatible with the binary and event-driven nature of SNNs, our proposed SNNSIR adopts a fully spike-driven architecture to achieve low-power and hardware-friendly computation. To address the expressiveness limitations of binary spiking neurons, we first introduce a lightweight Spike Residual Basic Block (SRBB) to enhance information flow via spike-compatible residual learning. Building on this, the Spike Stereo Convolutional Modulation (SSCM) module introduces simplified nonlinearity through element-wise multiplication and highlights noise-sensitive regions via cross-view-aware modulation. Complementing this, the Spike Stereo Cross-Attention (SSCA) module further improves stereo correspondence by enabling efficient bidirectional feature interaction across views within a spike-compatible framework. Extensive experiments on diverse stereo image restoration tasks, including rain streak removal, raindrop removal, low-light enhancement, and super-resolution demonstrate that our model achieves competitive restoration performance while significantly reducing computational overhead. These results highlight the potential for real-time, low-power stereo vision applications. The code will be available after the article is accepted.", "AI": {"tldr": "\u63d0\u51faSNNSIR\uff0c\u4e00\u79cd\u5b8c\u5168\u8109\u51b2\u9a71\u52a8\u7684\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u7acb\u4f53\u56fe\u50cf\u6062\u590d\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u6062\u590d\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7684\u6df7\u5408SNN-ANN\u6a21\u578b\u4ecd\u4f9d\u8d56\u6d6e\u70b9\u77e9\u9635\u9664\u6cd5\u6216\u6307\u6570\u8fd0\u7b97\uff0c\u4e0eSNN\u7684\u4e8c\u8fdb\u5236\u548c\u4e8b\u4ef6\u9a71\u52a8\u7279\u6027\u4e0d\u517c\u5bb9\u3002\u9700\u8981\u5f00\u53d1\u5b8c\u5168\u8109\u51b2\u9a71\u52a8\u7684\u67b6\u6784\u6765\u5b9e\u73b0\u4f4e\u529f\u8017\u548c\u786c\u4ef6\u53cb\u597d\u7684\u8ba1\u7b97\u3002", "method": "1) \u5f15\u5165\u8f7b\u91cf\u7ea7\u8109\u51b2\u6b8b\u5dee\u57fa\u672c\u5757(SRBB)\u901a\u8fc7\u8109\u51b2\u517c\u5bb9\u7684\u6b8b\u5dee\u5b66\u4e60\u589e\u5f3a\u4fe1\u606f\u6d41\uff1b2) \u8109\u51b2\u7acb\u4f53\u5377\u79ef\u8c03\u5236(SSCM)\u6a21\u5757\u901a\u8fc7\u9010\u5143\u7d20\u4e58\u6cd5\u5f15\u5165\u7b80\u5316\u975e\u7ebf\u6027\uff0c\u5e76\u901a\u8fc7\u8de8\u89c6\u56fe\u611f\u77e5\u8c03\u5236\u7a81\u51fa\u566a\u58f0\u654f\u611f\u533a\u57df\uff1b3) \u8109\u51b2\u7acb\u4f53\u4ea4\u53c9\u6ce8\u610f\u529b(SSCA)\u6a21\u5757\u5728\u8109\u51b2\u517c\u5bb9\u6846\u67b6\u5185\u5b9e\u73b0\u9ad8\u6548\u7684\u8de8\u89c6\u56fe\u53cc\u5411\u7279\u5f81\u4ea4\u4e92\u3002", "result": "\u5728\u591a\u79cd\u7acb\u4f53\u56fe\u50cf\u6062\u590d\u4efb\u52a1\uff08\u96e8\u7eb9\u53bb\u9664\u3001\u96e8\u6ef4\u53bb\u9664\u3001\u4f4e\u5149\u589e\u5f3a\u548c\u8d85\u5206\u8fa8\u7387\uff09\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u6062\u590d\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5b9e\u65f6\u3001\u4f4e\u529f\u8017\u7acb\u4f53\u89c6\u89c9\u5e94\u7528\u7684\u6f5c\u529b\uff0c\u4e3a\u5b8c\u5168\u8109\u51b2\u9a71\u52a8\u7684\u7acb\u4f53\u56fe\u50cf\u6062\u590d\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12313", "pdf": "https://arxiv.org/pdf/2508.12313", "abs": "https://arxiv.org/abs/2508.12313", "authors": ["Xiaobin Deng", "Changyu Diao", "Min Li", "Ruohan Yu", "Duanqing Xu"], "title": "Improving Densification in 3D Gaussian Splatting for High-Fidelity Rendering", "categories": ["cs.CV"], "comment": "Project page: https://xiaobin2001.github.io/improved-gs-web", "summary": "Although 3D Gaussian Splatting (3DGS) has achieved impressive performance in real-time rendering, its densification strategy often results in suboptimal reconstruction quality. In this work, we present a comprehensive improvement to the densification pipeline of 3DGS from three perspectives: when to densify, how to densify, and how to mitigate overfitting. Specifically, we propose an Edge-Aware Score to effectively select candidate Gaussians for splitting. We further introduce a Long-Axis Split strategy that reduces geometric distortions introduced by clone and split operations. To address overfitting, we design a set of techniques, including Recovery-Aware Pruning, Multi-step Update, and Growth Control. Our method enhances rendering fidelity without introducing additional training or inference overhead, achieving state-of-the-art performance with fewer Gaussians.", "AI": {"tldr": "\u57fa\u4e8e3D\u9ad8\u65af\u62d3\u6251\u7684\u5bc6\u5316\u7b56\u7565\u5168\u9762\u6539\u8fdb\uff0c\u901a\u8fc7\u8fb9\u7f18\u611f\u77e5\u9009\u62e9\u3001\u957f\u8f74\u5206\u5272\u7b56\u7565\u548c\u6291\u5236\u8fc7\u62df\u5408\u6280\u672f\uff0c\u5728\u4e0d\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6e32\u67d3\u8d28\u91cf", "motivation": "3D\u9ad8\u65af\u62d3\u6251\u6280\u672f\u867d\u7136\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6e32\u67d3\uff0c\u4f46\u5176\u5bc6\u5316\u7b56\u7565\u5bfc\u81f4\u91cd\u5efa\u8d28\u91cf\u4e0d\u4f73\uff0c\u9700\u8981\u4ece\u5bc6\u5316\u65f6\u673a\u3001\u65b9\u5f0f\u548c\u6291\u5236\u8fc7\u62df\u5408\u7b49\u591a\u4e2a\u89d2\u5ea6\u8fdb\u884c\u5168\u9762\u6539\u8fdb", "method": "\u63d0\u51fa\u8fb9\u7f18\u611f\u77e5\u8bc4\u5206\u9009\u62e9\u5206\u5272\u5019\u9009\u9ad8\u65af\u51fd\u6570\uff0c\u91c7\u7528\u957f\u8f74\u5206\u5272\u7b56\u7565\u51cf\u5c11\u514b\u9686\u548c\u5206\u5272\u64cd\u4f5c\u5f15\u5165\u7684\u51e0\u4f55\u5f02\u5e38\uff0c\u8bbe\u8ba1\u6062\u590d\u611f\u77e5\u526a\u679d\u3001\u591a\u6b65\u66f4\u65b0\u548c\u589e\u957f\u63a7\u5236\u6280\u672f\u6765\u5e94\u5bf9\u8fc7\u62df\u5408\u95ee\u9898", "result": "\u65b9\u6cd5\u5728\u4e0d\u589e\u52a0\u8bad\u7ec3\u6216\u63a8\u7406\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u6e32\u67d3\u4fdd\u771f\u5ea6\uff0c\u4ee5\u66f4\u5c11\u7684\u9ad8\u65af\u51fd\u6570\u6570\u91cf\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u5bc6\u5316\u7ba1\u9053\u6539\u8fdb\uff0c\u8be5\u7814\u7a76\u4e3a3D\u9ad8\u65af\u62d3\u6251\u6280\u672f\u63d0\u4f9b\u4e86\u4e00\u5957\u6709\u6548\u7684\u91cd\u5efa\u8d28\u91cf\u4f18\u5316\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u5b9e\u65f6\u6e32\u67d3\u4f18\u52bf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u6548\u679c"}}
{"id": "2508.12336", "pdf": "https://arxiv.org/pdf/2508.12336", "abs": "https://arxiv.org/abs/2508.12336", "authors": ["Fatemeh Ghorbani Lohesara", "Karen Eguiazarian", "Sebastian Knorr"], "title": "Geometry-Aware Video Inpainting for Joint Headset Occlusion Removal and Face Reconstruction in Social XR", "categories": ["cs.CV"], "comment": null, "summary": "Head-mounted displays (HMDs) are essential for experiencing extended reality (XR) environments and observing virtual content. However, they obscure the upper part of the user's face, complicating external video recording and significantly impacting social XR applications such as teleconferencing, where facial expressions and eye gaze details are crucial for creating an immersive experience. This study introduces a geometry-aware learning-based framework to jointly remove HMD occlusions and reconstruct complete 3D facial geometry from RGB frames captured from a single viewpoint. The method integrates a GAN-based video inpainting network, guided by dense facial landmarks and a single occlusion-free reference frame, to restore missing facial regions while preserving identity. Subsequently, a SynergyNet-based module regresses 3D Morphable Model (3DMM) parameters from the inpainted frames, enabling accurate 3D face reconstruction. Dense landmark optimization is incorporated throughout the pipeline to improve both the inpainting quality and the fidelity of the recovered geometry. Experimental results demonstrate that the proposed framework can successfully remove HMDs from RGB facial videos while maintaining facial identity and realism, producing photorealistic 3D face geometry outputs. Ablation studies further show that the framework remains robust across different landmark densities, with only minor quality degradation under sparse landmark configurations.", "AI": {"tldr": "\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u51e0\u4f55\u611f\u77e5\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u5355\u89c6\u89d2RGB\u89c6\u9891\u4e2d\u540c\u65f6\u53bb\u9664\u5934\u663e\u8bbe\u5907\u906e\u6321\u5e76\u91cd\u5efa\u5b8c\u6574\u76843D\u9762\u90e8\u51e0\u4f55\uff0c\u4e3a\u793e\u4ea4XR\u5e94\u7528\u63d0\u4f9b\u6d89\u53ca\u9762\u90e8\u8868\u60c5\u7684\u6d1e\u5bdf\u4f53\u9a8c\u3002", "motivation": "\u5934\u663e\u8bbe\u5907(HMDs)\u906e\u6321\u4e86\u7528\u6237\u9762\u90e8\u4e0a\u90e8\u5206\uff0c\u5f71\u54cd\u5916\u90e8\u89c6\u9891\u5f55\u5236\u548c\u793e\u4ea4XR\u5e94\u7528\uff08\u5982\u8fdc\u7a0b\u4f1a\u8bae\uff09\u7684\u6c1b\u56f4\u521b\u9020\uff0c\u56e0\u4e3a\u9762\u90e8\u8868\u60c5\u548c\u773c\u795e\u7ec6\u8282\u5bf9\u4e8e\u6c1b\u56f4\u6d1e\u5bdf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u96c6\u6210GAN\u57fa\u7840\u7684\u89c6\u9891\u4fee\u590d\u7f51\u7edc\uff0c\u5728\u5bc6\u96c6\u9762\u90e8\u5173\u952e\u70b9\u548c\u5355\u5f20\u65e0\u906e\u6321\u53c2\u7167\u5e27\u7684\u6307\u5bfc\u4e0b\u6062\u590d\u7f3a\u5931\u9762\u90e8\u533a\u57df\u4fdd\u6301\u8eab\u4efd\u8bc6\u522b\uff1b\u7ee7\u800c\u4f7f\u7528SynergyNet\u57fa\u7840\u6a21\u5757\u4ece\u4fee\u590d\u540e\u7684\u5e27\u4e2d\u56de\u5f523D\u53ef\u53d8\u5f62\u6a21\u578b(3DMM)\u53c2\u6570\uff0c\u5b9e\u73b0\u51c6\u786e\u76843D\u9762\u90e8\u91cd\u5efa\uff1b\u901a\u8fc7\u5bc6\u96c6\u5173\u952e\u70b9\u4f18\u5316\u63d0\u5347\u4fee\u590d\u8d28\u91cf\u548c\u51e0\u4f55\u91cd\u5efa\u4fdd\u771f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u6846\u67b6\u80fd\u6210\u529f\u4eceRGB\u9762\u90e8\u89c6\u9891\u4e2d\u53bb\u9664HMD\u906e\u6321\uff0c\u4fdd\u6301\u9762\u90e8\u8eab\u4efd\u8bc6\u522b\u548c\u771f\u5b9e\u6027\uff0c\u751f\u6210\u4ee5\u7167\u7247\u4e3a\u771f\u5b9e\u76843D\u9762\u90e8\u51e0\u4f55\u8f93\u51fa\u3002\u5206\u79bb\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u660e\u6846\u67b6\u5728\u4e0d\u540c\u5173\u952e\u70b9\u5bc6\u5ea6\u4e0b\u5747\u4fdd\u6301\u7a33\u5065\uff0c\u5728\u7a00\u758f\u5173\u952e\u70b9\u914d\u7f6e\u4e0b\u4ec5\u6709\u8f7b\u5fae\u8d28\u91cf\u4e0b\u964d\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u51e0\u4f55\u611f\u77e5\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3HMD\u906e\u6321\u95ee\u9898\uff0c\u4e3a\u793e\u4ea4XR\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u9762\u90e8\u8868\u60c5\u6062\u590d\u548c3D\u91cd\u5efa\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u8f93\u51fa\u7684\u540c\u65f6\u5177\u6709\u826f\u597d\u7684\u7a33\u5065\u6027\u3002"}}
{"id": "2508.12341", "pdf": "https://arxiv.org/pdf/2508.12341", "abs": "https://arxiv.org/abs/2508.12341", "authors": ["Ziye Wang", "Minghang Yu", "Chunyan Xu", "Zhen Cui"], "title": "Semantic Discrepancy-aware Detector for Image Forgery Identification", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 5 figures", "summary": "With the rapid advancement of image generation techniques, robust forgery detection has become increasingly imperative to ensure the trustworthiness of digital media. Recent research indicates that the learned semantic concepts of pre-trained models are critical for identifying fake images. However, the misalignment between the forgery and semantic concept spaces hinders the model's forgery detection performance. To address this problem, we propose a novel Semantic Discrepancy-aware Detector (SDD) that leverages reconstruction learning to align the two spaces at a fine-grained visual level. By exploiting the conceptual knowledge embedded in the pre-trained vision language model, we specifically design a semantic token sampling module to mitigate the space shifts caused by features irrelevant to both forgery traces and semantic concepts. A concept-level forgery discrepancy learning module, built upon a visual reconstruction paradigm, is proposed to strengthen the interaction between visual semantic concepts and forgery traces, effectively capturing discrepancies under the concepts' guidance. Finally, the low-level forgery feature enhancemer integrates the learned concept level forgery discrepancies to minimize redundant forgery information. Experiments conducted on two standard image forgery datasets demonstrate the efficacy of the proposed SDD, which achieves superior results compared to existing methods. The code is available at https://github.com/wzy1111111/SSD.", "AI": {"tldr": "\u63d0\u51faSDD\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u91cd\u5efa\u5b66\u4e60\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5c42\u9762\u5bf9\u9f50\u4f2a\u9020\u75d5\u8ff9\u548c\u8bed\u4e49\u6982\u5ff5\u7a7a\u95f4\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6982\u5ff5\u77e5\u8bc6\u6765\u63d0\u5347\u4f2a\u9020\u56fe\u50cf\u68c0\u6d4b\u6027\u80fd", "motivation": "\u968f\u7740\u56fe\u50cf\u751f\u6210\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u786e\u4fdd\u6570\u5b57\u5a92\u4f53\u53ef\u4fe1\u5ea6\u9700\u8981\u5f3a\u5927\u7684\u4f2a\u9020\u68c0\u6d4b\u3002\u9884\u8bad\u7ec3\u6a21\u578b\u5b66\u4e60\u7684\u8bed\u4e49\u6982\u5ff5\u5bf9\u8bc6\u522b\u5047\u56fe\u50cf\u5f88\u5173\u952e\uff0c\u4f46\u4f2a\u9020\u7a7a\u95f4\u4e0e\u8bed\u4e49\u6982\u5ff5\u7a7a\u95f4\u7684\u4e0d\u5bf9\u9f50\u963b\u788d\u4e86\u68c0\u6d4b\u6027\u80fd", "method": "\u63d0\u51fa\u8bed\u4e49\u5dee\u5f02\u611f\u77e5\u68c0\u6d4b\u5668(SDD)\uff1a1)\u8bed\u4e49\u6807\u8bb0\u91c7\u6837\u6a21\u5757\u7f13\u89e3\u4e0e\u4f2a\u9020\u75d5\u8ff9\u548c\u8bed\u4e49\u6982\u5ff5\u65e0\u5173\u7684\u7279\u5f81\u9020\u6210\u7684\u7a7a\u95f4\u504f\u79fb\uff1b2)\u57fa\u4e8e\u89c6\u89c9\u91cd\u5efa\u8303\u5f0f\u7684\u6982\u5ff5\u7ea7\u4f2a\u9020\u5dee\u5f02\u5b66\u4e60\u6a21\u5757\uff1b3)\u4f4e\u7ea7\u4f2a\u9020\u7279\u5f81\u589e\u5f3a\u5668\u6574\u5408\u5b66\u4e60\u5230\u7684\u6982\u5ff5\u7ea7\u5dee\u5f02", "result": "\u5728\u4e24\u4e2a\u6807\u51c6\u56fe\u50cf\u4f2a\u9020\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660eSDD\u7684\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u53d6\u5f97\u4e86\u66f4\u4f18\u8d8a\u7684\u7ed3\u679c", "conclusion": "SDD\u901a\u8fc7\u91cd\u5efa\u5b66\u4e60\u6709\u6548\u5bf9\u9f50\u4f2a\u9020\u548c\u8bed\u4e49\u6982\u5ff5\u7a7a\u95f4\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6982\u5ff5\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f2a\u9020\u56fe\u50cf\u68c0\u6d4b\u6027\u80fd"}}
{"id": "2508.12343", "pdf": "https://arxiv.org/pdf/2508.12343", "abs": "https://arxiv.org/abs/2508.12343", "authors": ["Emanuel C. Silva", "Tatiana T. Schein", "Stephanie L. Bri\u00e3o", "Guilherme L. M. Costa", "Felipe G. Oliveira", "Gustavo P. Almeida", "Eduardo L. Silva", "Sam S. Devincenzi", "Karina S. Machado", "Paulo L. J. Drews-Jr"], "title": "AquaFeat: A Features-Based Image Enhancement Model for Underwater Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "The severe image degradation in underwater environments impairs object detection models, as traditional image enhancement methods are often not optimized for such downstream tasks. To address this, we propose AquaFeat, a novel, plug-and-play module that performs task-driven feature enhancement. Our approach integrates a multi-scale feature enhancement network trained end-to-end with the detector's loss function, ensuring the enhancement process is explicitly guided to refine features most relevant to the detection task. When integrated with YOLOv8m on challenging underwater datasets, AquaFeat achieves state-of-the-art Precision (0.877) and Recall (0.624), along with competitive mAP scores (mAP@0.5 of 0.677 and mAP@[0.5:0.95] of 0.421). By delivering these accuracy gains while maintaining a practical processing speed of 46.5 FPS, our model provides an effective and computationally efficient solution for real-world applications, such as marine ecosystem monitoring and infrastructure inspection.", "AI": {"tldr": "AquaFeat\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u4efb\u52a1\u9a71\u52a8\u7279\u5f81\u589e\u5f3a\u6a21\u5757\uff0c\u4e13\u95e8\u9488\u5bf9\u6c34\u4e0b\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u8bbe\u8ba1\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u5b9e\u73b0\u591a\u5c3a\u5ea6\u7279\u5f81\u589e\u5f3a\uff0c\u5728YOLOv8m\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u6c34\u4e0b\u73af\u5883\u7684\u4e25\u91cd\u56fe\u50cf\u9000\u5316\u4f1a\u635f\u5bb3\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f20\u7edf\u7684\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u901a\u5e38\u6ca1\u6709\u9488\u5bf9\u4e0b\u6e38\u4efb\u52a1\u8fdb\u884c\u4f18\u5316\u3002", "method": "\u63d0\u51faAquaFeat\u6a21\u5757\uff0c\u96c6\u6210\u591a\u5c3a\u5ea6\u7279\u5f81\u589e\u5f3a\u7f51\u7edc\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u4e0e\u68c0\u6d4b\u5668\u635f\u5931\u51fd\u6570\u7ed3\u5408\uff0c\u786e\u4fdd\u589e\u5f3a\u8fc7\u7a0b\u660e\u786e\u6307\u5bfc\u4f18\u5316\u4e0e\u68c0\u6d4b\u4efb\u52a1\u6700\u76f8\u5173\u7684\u7279\u5f81\u3002", "result": "\u5728\u6311\u6218\u6027\u6c34\u4e0b\u6570\u636e\u96c6\u4e0a\uff0cAquaFeat\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6(0.877)\u548c\u53ec\u56de\u7387(0.624)\uff0c\u4ee5\u53ca\u7ade\u4e89\u529b\u7684mAP\u5206\u6570(mAP@0.5\u4e3a0.677\uff0cmAP@[0.5:0.95]\u4e3a0.421)\uff0c\u5904\u7406\u901f\u5ea6\u4e3a46.5 FPS\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u4fdd\u6301\u5b9e\u7528\u5904\u7406\u901f\u5ea6\u7684\u540c\u65f6\u63d0\u4f9b\u51c6\u786e\u6027\u63d0\u5347\uff0c\u4e3a\u6d77\u6d0b\u751f\u6001\u7cfb\u7edf\u76d1\u6d4b\u548c\u57fa\u7840\u8bbe\u65bd\u68c0\u67e5\u7b49\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12346", "pdf": "https://arxiv.org/pdf/2508.12346", "abs": "https://arxiv.org/abs/2508.12346", "authors": ["Hu Gao", "Depeng Dang"], "title": "MBMamba: When Memory Buffer Meets Mamba for Structure-Aware Image Deblurring", "categories": ["cs.CV"], "comment": null, "summary": "The Mamba architecture has emerged as a promising alternative to CNNs and Transformers for image deblurring. However, its flatten-and-scan strategy often results in local pixel forgetting and channel redundancy, limiting its ability to effectively aggregate 2D spatial information. Although existing methods mitigate this by modifying the scan strategy or incorporating local feature modules, it increase computational complexity and hinder real-time performance. In this paper, we propose a structure-aware image deblurring network without changing the original Mamba architecture. Specifically, we design a memory buffer mechanism to preserve historical information for later fusion, enabling reliable modeling of relevance between adjacent features. Additionally, we introduce an Ising-inspired regularization loss that simulates the energy minimization of the physical system's \"mutual attraction\" between pixels, helping to maintain image structure and coherence. Building on this, we develop MBMamba. Experimental results show that our method outperforms state-of-the-art approaches on widely used benchmarks.", "AI": {"tldr": "MBMamba\uff1a\u4e00\u79cd\u57fa\u4e8eMamba\u67b6\u6784\u7684\u56fe\u50cf\u53bb\u6a21\u7cca\u7f51\u7edc\uff0c\u901a\u8fc7\u5185\u5b58\u7f13\u51b2\u673a\u5236\u548cIsing\u6b63\u5219\u5316\u635f\u5931\u89e3\u51b3\u5c40\u90e8\u50cf\u7d20\u9057\u5fd8\u548c\u901a\u9053\u5197\u4f59\u95ee\u9898\uff0c\u5728\u4e0d\u6539\u53d8\u539f\u59cb\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6027\u80fd", "motivation": "Mamba\u67b6\u6784\u5728\u56fe\u50cf\u53bb\u6a21\u7cca\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176flatten-and-scan\u7b56\u7565\u5bfc\u81f4\u5c40\u90e8\u50cf\u7d20\u9057\u5fd8\u548c\u901a\u9053\u5197\u4f59\uff0c\u73b0\u6709\u6539\u8fdb\u65b9\u6cd5\u589e\u52a0\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u5f71\u54cd\u5b9e\u65f6\u6027\u80fd", "method": "\u63d0\u51fa\u5185\u5b58\u7f13\u51b2\u673a\u5236\u4fdd\u5b58\u5386\u53f2\u4fe1\u606f\u7528\u4e8e\u540e\u7eed\u878d\u5408\uff0c\u5f15\u5165Ising\u542f\u53d1\u7684\u6b63\u5219\u5316\u635f\u5931\u6a21\u62df\u50cf\u7d20\u95f4\"\u76f8\u4e92\u5438\u5f15\"\u7684\u80fd\u91cf\u6700\u5c0f\u5316\uff0c\u4fdd\u6301\u56fe\u50cf\u7ed3\u6784\u548c\u8fde\u8d2f\u6027", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "MBMamba\u5728\u4e0d\u6539\u53d8Mamba\u539f\u59cb\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u89e3\u51b3\u4e86\u5c40\u90e8\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u56fe\u50cf\u53bb\u6a21\u7cca\u6027\u80fd"}}
{"id": "2508.12356", "pdf": "https://arxiv.org/pdf/2508.12356", "abs": "https://arxiv.org/abs/2508.12356", "authors": ["Ahmet H. G\u00fczel", "Ilija Bogunovic", "Jack Parker-Holder"], "title": "Synthetic Data is Sufficient for Zero-Shot Visual Generalization from Offline Data", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Offline reinforcement learning (RL) offers a promising framework for training agents using pre-collected datasets without the need for further environment interaction. However, policies trained on offline data often struggle to generalise due to limited exposure to diverse states. The complexity of visual data introduces additional challenges such as noise, distractions, and spurious correlations, which can misguide the policy and increase the risk of overfitting if the training data is not sufficiently diverse. Indeed, this makes it challenging to leverage vision-based offline data in training robust agents that can generalize to unseen environments. To solve this problem, we propose a simple approach generating additional synthetic training data. We propose a two-step process, first augmenting the originally collected offline data to improve zero-shot generalization by introducing diversity, then using a diffusion model to generate additional data in latent space. We test our method across both continuous action spaces (Visual D4RL) and discrete action spaces (Procgen), demonstrating that it significantly improves generalization without requiring any algorithmic changes to existing model-free offline RL methods. We show that our method not only increases the diversity of the training data but also significantly reduces the generalization gap at test time while maintaining computational efficiency. We believe this approach could fuel additional progress in generating synthetic data to train more general agents in the future.", "AI": {"tldr": "\u901a\u8fc7\u4e24\u6b65\u6b65\u9aa4\uff1a\u9996\u5148\u5bf9\u79bb\u7ebf\u6570\u636e\u8fdb\u884c\u589e\u5e06\u63d0\u5347\u96f6\u68ad\u6cd5\u901a\u7528\u6027\uff0c\u7136\u540e\u4f7f\u7528\u6b7b\u5143\u6a21\u578b\u5728\u6f5c\u7a7a\u95f4\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u4ece\u800c\u63d0\u9ad8\u89c6\u89c9\u57fa\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6f14\u7b56\u901a\u7528\u6027\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u89c6\u89c9\u6570\u636e\u7684\u566a\u58f0\u3001\u5e72\u6270\u548c\u504f\u76f8\u5173\u8054\u95ee\u9898\uff0c\u5145\u5206\u5229\u7528\u89c6\u89c9\u57fa\u79bb\u7ebf\u6570\u636e\u8bad\u7ec3\u66f4\u7a33\u5065\u4e14\u5177\u6709\u826f\u597d\u901a\u7528\u6027\u7684\u6f14\u7b56\u3002", "method": "\u9996\u5148\u5bf9\u539f\u59cb\u79bb\u7ebf\u6570\u636e\u8fdb\u884c\u589e\u5e06\u63d0\u5347\u591a\u6837\u6027\uff0c\u7136\u540e\u4f7f\u7528\u6b7b\u5143\u6a21\u578b\u5728\u6f5c\u7a7a\u95f4\u751f\u6210\u989d\u5916\u7684\u5408\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u65e0\u9700\u6539\u53d8\u73b0\u6709\u6a21\u578b\u514d\u8d39\u79bb\u7ebfRL\u7b97\u6cd5\u3002", "result": "\u5728\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4(Visual D4RL)\u548c\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4(Procgen)\u4e0a\u90fd\u663e\u793a\u51fa\u663e\u8457\u7684\u901a\u7528\u6027\u63d0\u5347\uff0c\u540c\u65f6\u589e\u52a0\u4e86\u8bad\u7ec3\u6570\u636e\u591a\u6837\u6027\u5e76\u51cf\u5c0f\u4e86\u6d4b\u8bd5\u65f6\u7684\u901a\u7528\u6027\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u5177\u6709\u66f4\u5f3a\u901a\u7528\u6027\u7684\u6f14\u7b56\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u4e14\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2508.12396", "pdf": "https://arxiv.org/pdf/2508.12396", "abs": "https://arxiv.org/abs/2508.12396", "authors": ["Xiaochuan Lin", "Xiangyong Chen", "Xuan Li", "Yichen Su"], "title": "DeCoT: Decomposing Complex Instructions for Enhanced Text-to-Image Generation with Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Despite remarkable advancements, current Text-to-Image (T2I) models struggle with complex, long-form textual instructions, frequently failing to accurately render intricate details, spatial relationships, or specific constraints. This limitation is highlighted by benchmarks such as LongBench-T2I, which reveal deficiencies in handling composition, specific text, and fine textures. To address this, we propose DeCoT (Decomposition-CoT), a novel framework that leverages Large Language Models (LLMs) to significantly enhance T2I models' understanding and execution of complex instructions. DeCoT operates in two core stages: first, Complex Instruction Decomposition and Semantic Enhancement, where an LLM breaks down raw instructions into structured, actionable semantic units and clarifies ambiguities; second, Multi-Stage Prompt Integration and Adaptive Generation, which transforms these units into a hierarchical or optimized single prompt tailored for existing T2I models. Extensive experiments on the LongBench-T2I dataset demonstrate that DeCoT consistently and substantially improves the performance of leading T2I models across all evaluated dimensions, particularly in challenging aspects like \"Text\" and \"Composition\". Quantitative results, validated by multiple MLLM evaluators (Gemini-2.0-Flash and InternVL3-78B), show that DeCoT, when integrated with Infinity-8B, achieves an average score of 3.52, outperforming the baseline Infinity-8B (3.44). Ablation studies confirm the critical contribution of each DeCoT component and the importance of sophisticated LLM prompting. Furthermore, human evaluations corroborate these findings, indicating superior perceptual quality and instruction fidelity. DeCoT effectively bridges the gap between high-level user intent and T2I model requirements, leading to more faithful and accurate image generation.", "AI": {"tldr": "DeCoT\u662f\u4e00\u4e2a\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u5206\u89e3\u590d\u6742\u6587\u672c\u6307\u4ee4\u6765\u63d0\u5347\u6587\u751f\u56fe\u6a21\u578b\u6027\u80fd\u7684\u65b0\u6846\u67b6\uff0c\u5728LongBench-T2I\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u6539\u5584\u4e86\u6587\u672c\u548c\u6784\u56fe\u7b49\u6311\u6218\u6027\u65b9\u9762\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u6587\u751f\u56fe\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u957f\u6587\u672c\u6307\u4ee4\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u7ecf\u5e38\u65e0\u6cd5\u51c6\u786e\u6e32\u67d3\u7ec6\u8282\u3001\u7a7a\u95f4\u5173\u7cfb\u548c\u7279\u5b9a\u7ea6\u675f\uff0c\u9700\u8981\u66f4\u597d\u7684\u65b9\u6cd5\u6765\u7406\u89e3\u548c\u6267\u884c\u590d\u6742\u6307\u4ee4\u3002", "method": "DeCoT\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u590d\u6742\u6307\u4ee4\u5206\u89e3\u548c\u8bed\u4e49\u589e\u5f3a\uff0c\u7528LLM\u5c06\u539f\u59cb\u6307\u4ee4\u5206\u89e3\u4e3a\u7ed3\u6784\u5316\u8bed\u4e49\u5355\u5143\uff1b2\uff09\u591a\u9636\u6bb5\u63d0\u793a\u96c6\u6210\u548c\u81ea\u9002\u5e94\u751f\u6210\uff0c\u5c06\u8fd9\u4e9b\u5355\u5143\u8f6c\u6362\u4e3a\u9002\u5408\u73b0\u6709T2I\u6a21\u578b\u7684\u5c42\u6b21\u5316\u6216\u4f18\u5316\u63d0\u793a\u3002", "result": "\u5728LongBench-T2I\u6570\u636e\u96c6\u4e0a\uff0cDeCoT\u663e\u8457\u63d0\u5347\u4e86\u4e3b\u6d41T2I\u6a21\u578b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\"\u6587\u672c\"\u548c\"\u6784\u56fe\"\u65b9\u9762\u3002\u4e0eInfinity-8B\u96c6\u6210\u65f6\u5e73\u5747\u5f97\u52063.52\uff0c\u4f18\u4e8e\u57fa\u7ebf3.44\u3002\u4eba\u5de5\u8bc4\u4f30\u4e5f\u8bc1\u5b9e\u4e86\u611f\u77e5\u8d28\u91cf\u548c\u6307\u4ee4\u4fdd\u771f\u5ea6\u7684\u63d0\u5347\u3002", "conclusion": "DeCoT\u6709\u6548\u5f25\u5408\u4e86\u9ad8\u7ea7\u7528\u6237\u610f\u56fe\u4e0eT2I\u6a21\u578b\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5b9e\u73b0\u4e86\u66f4\u5fe0\u5b9e\u548c\u51c6\u786e\u7684\u56fe\u50cf\u751f\u6210\uff0c\u6bcf\u4e2a\u7ec4\u4ef6\u90fd\u53d1\u6325\u4e86\u5173\u952e\u4f5c\u7528\uff0c\u590d\u6742\u7684LLM\u63d0\u793a\u7b56\u7565\u4e5f\u5f88\u91cd\u8981\u3002"}}
{"id": "2508.12415", "pdf": "https://arxiv.org/pdf/2508.12415", "abs": "https://arxiv.org/abs/2508.12415", "authors": ["Ke Xing", "Hanwen Liang", "Dejia Xu", "Yuyang Yin", "Konstantinos N. Plataniotis", "Yao Zhao", "Yunchao Wei"], "title": "TiP4GEN: Text to Immersive Panorama 4D Scene Generation", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid advancement and widespread adoption of VR/AR technologies, there is a growing demand for the creation of high-quality, immersive dynamic scenes. However, existing generation works predominantly concentrate on the creation of static scenes or narrow perspective-view dynamic scenes, falling short of delivering a truly 360-degree immersive experience from any viewpoint. In this paper, we introduce \\textbf{TiP4GEN}, an advanced text-to-dynamic panorama scene generation framework that enables fine-grained content control and synthesizes motion-rich, geometry-consistent panoramic 4D scenes. TiP4GEN integrates panorama video generation and dynamic scene reconstruction to create 360-degree immersive virtual environments. For video generation, we introduce a \\textbf{Dual-branch Generation Model} consisting of a panorama branch and a perspective branch, responsible for global and local view generation, respectively. A bidirectional cross-attention mechanism facilitates comprehensive information exchange between the branches. For scene reconstruction, we propose a \\textbf{Geometry-aligned Reconstruction Model} based on 3D Gaussian Splatting. By aligning spatial-temporal point clouds using metric depth maps and initializing scene cameras with estimated poses, our method ensures geometric consistency and temporal coherence for the reconstructed scenes. Extensive experiments demonstrate the effectiveness of our proposed designs and the superiority of TiP4GEN in generating visually compelling and motion-coherent dynamic panoramic scenes. Our project page is at https://ke-xing.github.io/TiP4GEN/.", "AI": {"tldr": "TiP4GEN\u662f\u4e00\u4e2a\u6587\u672c\u5230\u52a8\u6001\u5168\u666f\u573a\u666f\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u751f\u6210\u6a21\u578b\u548c\u51e0\u4f55\u5bf9\u9f50\u91cd\u5efa\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf360\u5ea6\u6c89\u6d78\u5f0f\u52a8\u6001\u573a\u666f\u7684\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u4e8e\u9759\u6001\u573a\u666f\u6216\u7a84\u89c6\u89d2\u52a8\u6001\u573a\u666f\uff0c\u65e0\u6cd5\u63d0\u4f9b\u771f\u6b63\u7684360\u5ea6\u6c89\u6d78\u5f0f\u4f53\u9a8c\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u4ece\u4efb\u610f\u89c6\u89d2\u751f\u6210\u52a8\u6001\u5168\u666f\u573a\u666f\u7684\u6280\u672f\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u751f\u6210\u6a21\u578b\uff08\u5168\u666f\u5206\u652f\u548c\u900f\u89c6\u5206\u652f\uff09\u8fdb\u884c\u89c6\u9891\u751f\u6210\uff0c\u901a\u8fc7\u53cc\u5411\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u4fe1\u606f\u4ea4\u6362\uff1b\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u51e0\u4f55\u5bf9\u9f50\u91cd\u5efa\u6a21\u578b\uff0c\u5229\u7528\u5ea6\u91cf\u6df1\u5ea6\u56fe\u5bf9\u9f50\u65f6\u7a7a\u70b9\u4e91\u5e76\u521d\u59cb\u5316\u573a\u666f\u76f8\u673a\u4f4d\u59ff\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u6240\u63d0\u8bbe\u8ba1\u7684\u6709\u6548\u6027\uff0cTiP4GEN\u5728\u751f\u6210\u89c6\u89c9\u5438\u5f15\u4eba\u4e14\u8fd0\u52a8\u8fde\u8d2f\u7684\u52a8\u6001\u5168\u666f\u573a\u666f\u65b9\u9762\u5177\u6709\u4f18\u8d8a\u6027\u3002", "conclusion": "TiP4GEN\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u52a8\u6001\u5168\u666f\u573a\u666f\u751f\u6210\u7684\u6311\u6218\uff0c\u4e3a\u521b\u5efa\u9ad8\u8d28\u91cf360\u5ea6\u6c89\u6d78\u5f0f\u865a\u62df\u73af\u5883\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12473", "pdf": "https://arxiv.org/pdf/2508.12473", "abs": "https://arxiv.org/abs/2508.12473", "authors": ["Eranga Bandara", "Ross Gore", "Sachin Shetty", "Ravi Mukkamala", "Christopher Rhea", "Atmaram Yarlagadda", "Shaifali Kaushik", "L. H. M. P. De Silva", "Andriy Maznychenko", "Inna Sokolowska", "Amin Hass", "Kasun De Zoysa"], "title": "Standardization of Neuromuscular Reflex Analysis -- Role of Fine-Tuned Vision-Language Model Consortium and OpenAI gpt-oss Reasoning LLM Enabled Decision Support System", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate assessment of neuromuscular reflexes, such as the H-reflex, plays a critical role in sports science, rehabilitation, and clinical neurology. Traditional analysis of H-reflex EMG waveforms is subject to variability and interpretation bias among clinicians and researchers, limiting reliability and standardization. To address these challenges, we propose a Fine-Tuned Vision-Language Model (VLM) Consortium and a reasoning Large-Language Model (LLM)-enabled Decision Support System for automated H-reflex waveform interpretation and diagnosis. Our approach leverages multiple VLMs, each fine-tuned on curated datasets of H-reflex EMG waveform images annotated with clinical observations, recovery timelines, and athlete metadata. These models are capable of extracting key electrophysiological features and predicting neuromuscular states, including fatigue, injury, and recovery, directly from EMG images and contextual metadata. Diagnostic outputs from the VLM consortium are aggregated using a consensus-based method and refined by a specialized reasoning LLM, which ensures robust, transparent, and explainable decision support for clinicians and sports scientists. The end-to-end platform orchestrates seamless communication between the VLM ensemble and the reasoning LLM, integrating prompt engineering strategies and automated reasoning workflows using LLM Agents. Experimental results demonstrate that this hybrid system delivers highly accurate, consistent, and interpretable H-reflex assessments, significantly advancing the automation and standardization of neuromuscular diagnostics. To our knowledge, this work represents the first integration of a fine-tuned VLM consortium with a reasoning LLM for image-based H-reflex analysis, laying the foundation for next-generation AI-assisted neuromuscular assessment and athlete monitoring platforms.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7cbe\u8c03\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8054\u76df\u548c\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u5316H-\u53cd\u5c04\u7535\u673a\u56fe\u5f62\u5206\u6790\u548c\u8bca\u65ad\u3002", "motivation": "\u4f20\u7edfH-\u53cd\u5c04EMG\u6ce2\u5f62\u5206\u6790\u5b58\u5728\u53d8\u5f02\u6027\u5927\u3001\u89e3\u91ca\u504f\u5dee\u548c\u53ef\u9760\u6027\u4f4e\u7684\u95ee\u9898\uff0c\u9700\u8981\u63d0\u9ad8\u795e\u7ecf\u808c\u8089\u8bca\u65ad\u7684\u6807\u51c6\u5316\u548c\u81ea\u52a8\u5316\u6c34\u5e73\u3002", "method": "\u4f7f\u7528\u591a\u4e2a\u7ec6\u8c03\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7ec4\u6210\u8054\u76df\uff0c\u6bcf\u4e2a\u6a21\u578b\u57fa\u4e8e\u6e05\u5355\u6807\u6ce8\u7684H-\u53cd\u5c04EMG\u56fe\u50cf\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u7136\u540e\u901a\u8fc7\u5171\u8bc6\u673a\u5236\u805a\u5408\u8f93\u51fa\uff0c\u518d\u7531\u4e13\u95e8\u7684\u7406\u89e3LLM\u8fdb\u884c\u7cbe\u70bc\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u6df7\u5408\u7cfb\u7edf\u80fd\u591f\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u3001\u4e00\u81f4\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684H-\u53cd\u5c04\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u5347\u4e86\u795e\u7ecf\u808c\u8089\u8bca\u65ad\u7684\u81ea\u52a8\u5316\u548c\u6807\u51c6\u5316\u6c34\u5e73\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5c06\u7cbe\u8c03VLM\u805a\u5408\u4e0e\u7406\u89e3LLM\u96c6\u6210\u7528\u4e8e\u56fe\u50cf\u57faH-\u53cd\u5c04\u5206\u6790\uff0c\u4e3a\u4e0b\u4e00\u4ee3AI\u8f85\u52a9\u795e\u7ecf\u808c\u8089\u8bc4\u4f30\u548c\u8fd0\u52a8\u5458\u76d1\u6d4b\u5e73\u53f0\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.12512", "pdf": "https://arxiv.org/pdf/2508.12512", "abs": "https://arxiv.org/abs/2508.12512", "authors": ["Krishna Teja Chitty-Venkata", "Murali Emani", "Venkatram Vishwanath"], "title": "LangVision-LoRA-NAS: Neural Architecture Search for Variable LoRA Rank in Vision Language Models", "categories": ["cs.CV"], "comment": "Accepted by ICIP 2025 Conference", "summary": "Vision Language Models (VLMs) integrate visual and text modalities to enable multimodal understanding and generation. These models typically combine a Vision Transformer (ViT) as an image encoder and a Large Language Model (LLM) for text generation. LoRA (Low-Rank Adaptation) is an efficient fine-tuning method to adapt pre-trained models to new tasks by introducing low-rank updates to their weights. While LoRA has emerged as a powerful technique for fine-tuning large models by introducing low-rank updates, current implementations assume a fixed rank, potentially limiting flexibility and efficiency across diverse tasks. This paper introduces \\textit{LangVision-LoRA-NAS}, a novel framework that integrates Neural Architecture Search (NAS) with LoRA to optimize VLMs for variable-rank adaptation. Our approach leverages NAS to dynamically search for the optimal LoRA rank configuration tailored to specific multimodal tasks, balancing performance and computational efficiency. Through extensive experiments using the LLaMA-3.2-11B model on several datasets, LangVision-LoRA-NAS demonstrates notable improvement in model performance while reducing fine-tuning costs. Our Base and searched fine-tuned models on LLaMA-3.2-11B-Vision-Instruct can be found \\href{https://huggingface.co/collections/krishnateja95/llama-32-11b-vision-instruct-langvision-lora-nas-6786cac480357a6a6fcc59ee}{\\textcolor{blue}{here}} and the code for LangVision-LoRA-NAS can be found \\href{https://github.com/krishnateja95/LangVision-NAS}{\\textcolor{blue}{here}}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LangVision-LoRA-NAS\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u52a8\u6001\u4f18\u5316LoRA\u79e9\u914d\u7f6e\uff0c\u5728\u63d0\u5347\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u5fae\u8c03\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684LoRA\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u79e9\u8fdb\u884c\u5fae\u8c03\uff0c\u9650\u5236\u4e86\u5728\u4e0d\u540c\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u8c03\u6574\u79e9\u914d\u7f6e\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u795e\u7ecf\u67b6\u6784\u641c\u7d22(NAS)\u4e0eLoRA\u7ed3\u5408\uff0c\u52a8\u6001\u641c\u7d22\u6700\u4f18\u7684LoRA\u79e9\u914d\u7f6e\uff0c\u9488\u5bf9\u7279\u5b9a\u591a\u6a21\u6001\u4efb\u52a1\u5e73\u8861\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728LLaMA-3.2-11B\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u5fae\u8c03\u6210\u672c\u3002", "conclusion": "LangVision-LoRA-NAS\u6846\u67b6\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u53d8\u79e9\u9002\u5e94\u65b9\u6cd5\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2508.12603", "pdf": "https://arxiv.org/pdf/2508.12603", "abs": "https://arxiv.org/abs/2508.12603", "authors": ["Can Cui", "Yupeng Zhou", "Juntong Peng", "Sung-Yeon Park", "Zichong Yang", "Prashanth Sankaranarayanan", "Jiaru Zhang", "Ruqi Zhang", "Ziran Wang"], "title": "ViLaD: A Large Vision Language Diffusion Framework for End-to-End Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "End-to-end autonomous driving systems built on Vision Language Models (VLMs) have shown significant promise, yet their reliance on autoregressive architectures introduces some limitations for real-world applications. The sequential, token-by-token generation process of these models results in high inference latency and cannot perform bidirectional reasoning, making them unsuitable for dynamic, safety-critical environments. To overcome these challenges, we introduce ViLaD, a novel Large Vision Language Diffusion (LVLD) framework for end-to-end autonomous driving that represents a paradigm shift. ViLaD leverages a masked diffusion model that enables parallel generation of entire driving decision sequences, significantly reducing computational latency. Moreover, its architecture supports bidirectional reasoning, allowing the model to consider both past and future simultaneously, and supports progressive easy-first generation to iteratively improve decision quality. We conduct comprehensive experiments on the nuScenes dataset, where ViLaD outperforms state-of-the-art autoregressive VLM baselines in both planning accuracy and inference speed, while achieving a near-zero failure rate. Furthermore, we demonstrate the framework's practical viability through a real-world deployment on an autonomous vehicle for an interactive parking task, confirming its effectiveness and soundness for practical applications.", "AI": {"tldr": "ViLaD\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b0\u578b\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u5e76\u884c\u751f\u6210\u9a7e\u9a76\u51b3\u7b56\u5e8f\u5217\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\uff0c\u652f\u6301\u53cc\u5411\u63a8\u7406\uff0c\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u81ea\u56de\u5f52VLM\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4f9d\u8d56\u81ea\u56de\u5f52\u67b6\u6784\uff0c\u5b58\u5728\u63a8\u7406\u5ef6\u8fdf\u9ad8\u3001\u65e0\u6cd5\u8fdb\u884c\u53cc\u5411\u63a8\u7406\u7684\u95ee\u9898\uff0c\u4e0d\u9002\u5408\u52a8\u6001\u7684\u5b89\u5168\u5173\u952e\u73af\u5883\u3002", "method": "\u91c7\u7528\u63a9\u7801\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u9a7e\u9a76\u51b3\u7b56\u5e8f\u5217\u7684\u5e76\u884c\u751f\u6210\uff0c\u652f\u6301\u53cc\u5411\u63a8\u7406\u548c\u6e10\u8fdb\u5f0f\u7b80\u5355\u4f18\u5148\u751f\u6210\u7b56\u7565\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0cViLaD\u5728\u89c4\u5212\u7cbe\u5ea6\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u81ea\u56de\u5f52VLM\u57fa\u7ebf\uff0c\u63a5\u8fd1\u96f6\u5931\u8d25\u7387\uff0c\u5e76\u5728\u771f\u5b9e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0a\u6210\u529f\u90e8\u7f72\u9a8c\u8bc1\u3002", "conclusion": "ViLaD\u6846\u67b6\u4ee3\u8868\u4e86\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u67b6\u6784\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12605", "pdf": "https://arxiv.org/pdf/2508.12605", "abs": "https://arxiv.org/abs/2508.12605", "authors": ["Wenjie Liao", "Jieyu Yuan", "Yifang Xu", "Chunle Guo", "Zilong Zhang", "Jihong Li", "Jiachen Fu", "Haotian Fan", "Tao Li", "Junhui Cui", "Chongyi Li"], "title": "ViDA-UGC: Detailed Image Quality Analysis via Visual Distortion Assessment for UGC Images", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have introduced a paradigm shift for Image Quality Assessment (IQA) from unexplainable image quality scoring to explainable IQA, demonstrating practical applications like quality control and optimization guidance. However, current explainable IQA methods not only inadequately use the same distortion criteria to evaluate both User-Generated Content (UGC) and AI-Generated Content (AIGC) images, but also lack detailed quality analysis for monitoring image quality and guiding image restoration. In this study, we establish the first large-scale Visual Distortion Assessment Instruction Tuning Dataset for UGC images, termed ViDA-UGC, which comprises 11K images with fine-grained quality grounding, detailed quality perception, and reasoning quality description data. This dataset is constructed through a distortion-oriented pipeline, which involves human subject annotation and a Chain-of-Thought (CoT) assessment framework. This framework guides GPT-4o to generate quality descriptions by identifying and analyzing UGC distortions, which helps capturing rich low-level visual features that inherently correlate with distortion patterns. Moreover, we carefully select 476 images with corresponding 6,149 question answer pairs from ViDA-UGC and invite a professional team to ensure the accuracy and quality of GPT-generated information. The selected and revised data further contribute to the first UGC distortion assessment benchmark, termed ViDA-UGC-Bench. Experimental results demonstrate the effectiveness of the ViDA-UGC and CoT framework for consistently enhancing various image quality analysis abilities across multiple base MLLMs on ViDA-UGC-Bench and Q-Bench, even surpassing GPT-4o.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6784\u5efa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u89c6\u89c9\u5931\u771f\u8bc4\u4f30\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6ViDA-UGC\uff0c\u4e13\u95e8\u9488\u5bf9\u7528\u6237\u751f\u6210\u5185\u5bb9\u56fe\u50cf\uff0c\u5305\u542b\u7ec6\u7c92\u5ea6\u8d28\u91cf\u6807\u6ce8\u548c\u8be6\u7ec6\u8d28\u91cf\u63cf\u8ff0\uff0c\u901a\u8fc7CoT\u6846\u67b6\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u53ef\u89e3\u91ca\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u4e00\u662f\u5bf9\u7528\u6237\u751f\u6210\u5185\u5bb9\u548cAI\u751f\u6210\u5185\u5bb9\u4f7f\u7528\u76f8\u540c\u7684\u5931\u771f\u6807\u51c6\u8fdb\u884c\u8bc4\u4f30\u4e0d\u591f\u5408\u7406\uff1b\u4e8c\u662f\u7f3a\u4e4f\u8be6\u7ec6\u7684\u56fe\u50cf\u8d28\u91cf\u5206\u6790\u6765\u76d1\u63a7\u8d28\u91cf\u548c\u6307\u5bfc\u56fe\u50cf\u6062\u590d\u3002", "method": "\u901a\u8fc7\u5931\u771f\u5bfc\u5411\u7684\u6d41\u7a0b\u6784\u5efaViDA-UGC\u6570\u636e\u96c6\uff0c\u5305\u542b11K\u56fe\u50cf\uff0c\u91c7\u7528\u4eba\u7c7b\u6807\u6ce8\u548cChain-of-Thought\u8bc4\u4f30\u6846\u67b6\u5f15\u5bfcGPT-4o\u751f\u6210\u8d28\u91cf\u63cf\u8ff0\uff0c\u5e76\u7cbe\u9009476\u56fe\u50cf\u6784\u5efaViDA-UGC-Bench\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cViDA-UGC\u548cCoT\u6846\u67b6\u80fd\u6301\u7eed\u589e\u5f3a\u591a\u79cd\u57fa\u7840MLLM\u5728\u56fe\u50cf\u8d28\u91cf\u5206\u6790\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5728ViDA-UGC-Bench\u548cQ-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u751a\u81f3\u8d85\u8d8a\u4e86GPT-4o\u7684\u8868\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aUGC\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7CoT\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86MLLM\u7684\u56fe\u50cf\u8d28\u91cf\u5206\u6790\u80fd\u529b\uff0c\u4e3a\u56fe\u50cf\u8d28\u91cf\u76d1\u63a7\u548c\u6062\u590d\u6307\u5bfc\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2508.12610", "pdf": "https://arxiv.org/pdf/2508.12610", "abs": "https://arxiv.org/abs/2508.12610", "authors": ["Chen Qian", "Danyang Li", "Xinran Yu", "Zheng Yang", "Qiang Ma"], "title": "OpenMoCap: Rethinking Optical Motion Capture under Real-world Occlusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Optical motion capture is a foundational technology driving advancements in cutting-edge fields such as virtual reality and film production. However, system performance suffers severely under large-scale marker occlusions common in real-world applications. An in-depth analysis identifies two primary limitations of current models: (i) the lack of training datasets accurately reflecting realistic marker occlusion patterns, and (ii) the absence of training strategies designed to capture long-range dependencies among markers. To tackle these challenges, we introduce the CMU-Occlu dataset, which incorporates ray tracing techniques to realistically simulate practical marker occlusion patterns. Furthermore, we propose OpenMoCap, a novel motion-solving model designed specifically for robust motion capture in environments with significant occlusions. Leveraging a marker-joint chain inference mechanism, OpenMoCap enables simultaneous optimization and construction of deep constraints between markers and joints. Extensive comparative experiments demonstrate that OpenMoCap consistently outperforms competing methods across diverse scenarios, while the CMU-Occlu dataset opens the door for future studies in robust motion solving. The proposed OpenMoCap is integrated into the MoSen MoCap system for practical deployment. The code is released at: https://github.com/qianchen214/OpenMoCap.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u89e3\u51b3\u4e86\u5149\u5b66\u52a8\u4f5c\u6293\u53d6\u4e2d\u6807\u8bb0\u70b9\u906e\u6321\u95ee\u9898\uff0c\u901a\u8fc7\u65b0\u7684CMU-Occlu\u6570\u636e\u96c6\u548cOpenMoCap\u6a21\u578b\u63d0\u5347\u4e86\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u52a8\u4f5c\u89e3\u6790\u7cbe\u5ea6", "motivation": "\u73b0\u6709\u5149\u5b66\u52a8\u4f5c\u6293\u53d6\u7cfb\u7edf\u5728\u5927\u89c4\u6a21\u6807\u8bb0\u70b9\u906e\u6321\u60c5\u51b5\u4e0b\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\uff0c\u4e3b\u8981\u95ee\u9898\u662f\u7f3a\u4e4f\u53cd\u6620\u5b9e\u9645\u906e\u6321\u6a21\u5f0f\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u65e0\u6cd5\u6293\u53d6\u6807\u8bb0\u70b9\u95f4\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb", "method": "\u63d0\u51faCMU-Occlu\u6570\u636e\u96c6\uff0c\u5229\u7528\u5149\u7ebf\u8ffd\u8e2a\u6280\u672f\u6a21\u62df\u5b9e\u9645\u906e\u6321\u6a21\u5f0f\uff1b\u8bbe\u8ba1OpenMoCap\u6a21\u578b\uff0c\u901a\u8fc7\u6807\u8bb0-\u5173\u8282\u94fe\u63a8\u7406\u673a\u5236\u5b9e\u73b0\u6807\u8bb0\u4e0e\u5173\u8282\u95f4\u6df1\u5ea6\u7ea6\u675f\u7684\u540c\u65f6\u4f18\u5316", "result": "\u5bf9\u6bd4\u5b9e\u9a8c\u663e\u793aOpenMoCap\u5728\u591a\u79cd\u573a\u666f\u4e0b\u90fd\u660e\u663e\u8d85\u8fc7\u7ade\u4e89\u65b9\u6cd5\uff0cCMU-Occlu\u6570\u636e\u96c6\u4e3a\u7a33\u5065\u52a8\u4f5c\u89e3\u6790\u7814\u7a76\u6253\u5f00\u4e86\u65b0\u65b9\u5411", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u89e3\u51b3\u4e86\u5149\u5b66\u52a8\u4f5c\u6293\u53d6\u4e2d\u906e\u6321\u95ee\u9898\uff0c\u901a\u8fc7\u5b9e\u9645\u7684\u6570\u636e\u6a21\u62df\u548c\u521b\u65b0\u7684\u6a21\u578b\u8bbe\u8ba1\uff0c\u4e3a\u865a\u62df\u73b0\u5b9e\u548c\u7535\u5f71\u5236\u4f5c\u7b49\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u52a8\u4f5c\u6293\u53d6\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12644", "pdf": "https://arxiv.org/pdf/2508.12644", "abs": "https://arxiv.org/abs/2508.12644", "authors": ["Hao Wen", "Hongbo Kang", "Jian Ma", "Jing Huang", "Yuanwang Yang", "Haozhe Lin", "Yu-Kun Lai", "Kun Li"], "title": "DyCrowd: Towards Dynamic Crowd Reconstruction from a Large-scene Video", "categories": ["cs.CV"], "comment": null, "summary": "3D reconstruction of dynamic crowds in large scenes has become increasingly important for applications such as city surveillance and crowd analysis. However, current works attempt to reconstruct 3D crowds from a static image, causing a lack of temporal consistency and inability to alleviate the typical impact caused by occlusions. In this paper, we propose DyCrowd, the first framework for spatio-temporally consistent 3D reconstruction of hundreds of individuals' poses, positions and shapes from a large-scene video. We design a coarse-to-fine group-guided motion optimization strategy for occlusion-robust crowd reconstruction in large scenes. To address temporal instability and severe occlusions, we further incorporate a VAE (Variational Autoencoder)-based human motion prior along with a segment-level group-guided optimization. The core of our strategy leverages collective crowd behavior to address long-term dynamic occlusions. By jointly optimizing the motion sequences of individuals with similar motion segments and combining this with the proposed Asynchronous Motion Consistency (AMC) loss, we enable high-quality unoccluded motion segments to guide the motion recovery of occluded ones, ensuring robust and plausible motion recovery even in the presence of temporal desynchronization and rhythmic inconsistencies. Additionally, in order to fill the gap of no existing well-annotated large-scene video dataset, we contribute a virtual benchmark dataset, VirtualCrowd, for evaluating dynamic crowd reconstruction from large-scene videos. Experimental results demonstrate that the proposed method achieves state-of-the-art performance in the large-scene dynamic crowd reconstruction task. The code and dataset will be available for research purposes.", "AI": {"tldr": "DyCrowd\u662f\u9996\u4e2a\u4ece\u5927\u573a\u666f\u89c6\u9891\u4e2d\u8fdb\u884c\u65f6\u7a7a\u4e00\u81f43D\u4eba\u7fa4\u91cd\u5efa\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7c97\u5230\u7ec6\u7684\u7fa4\u4f53\u5f15\u5bfc\u8fd0\u52a8\u4f18\u5316\u7b56\u7565\u548cVAE\u8fd0\u52a8\u5148\u9a8c\uff0c\u6709\u6548\u89e3\u51b3\u906e\u6321\u95ee\u9898\uff0c\u5e76\u8d21\u732e\u4e86VirtualCrowd\u865a\u62df\u6570\u636e\u96c6\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u4ece\u9759\u6001\u56fe\u50cf\u91cd\u5efa3D\u4eba\u7fa4\u7f3a\u4e4f\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u65e0\u6cd5\u89e3\u51b3\u906e\u6321\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u5927\u573a\u666f\u89c6\u9891\u4e2d\u52a8\u6001\u4eba\u7fa4\u91cd\u5efa\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u7c97\u5230\u7ec6\u7684\u7fa4\u4f53\u5f15\u5bfc\u8fd0\u52a8\u4f18\u5316\u7b56\u7565\uff0c\u7ed3\u5408VAE\u4eba\u7c7b\u8fd0\u52a8\u5148\u9a8c\u548c\u5206\u6bb5\u7ea7\u7fa4\u4f53\u5f15\u5bfc\u4f18\u5316\uff0c\u5229\u7528\u5f02\u6b65\u8fd0\u52a8\u4e00\u81f4\u6027\u635f\u5931(AMC)\u5b9e\u73b0\u906e\u6321\u6062\u590d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5927\u573a\u666f\u52a8\u6001\u4eba\u7fa4\u91cd\u5efa\u4efb\u52a1\u4e2d\u8fbe\u5230state-of-the-art\u6027\u80fd\u3002", "conclusion": "DyCrowd\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u5927\u573a\u666f\u89c6\u9891\u4e2d\u7684\u52a8\u6001\u4eba\u7fa43D\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u906e\u6321\u95ee\u9898\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2508.12663", "pdf": "https://arxiv.org/pdf/2508.12663", "abs": "https://arxiv.org/abs/2508.12663", "authors": ["Seung Young Noh", "Ju Yong Chang"], "title": "Stable Diffusion-Based Approach for Human De-Occlusion", "categories": ["cs.CV"], "comment": "MM 2025", "summary": "Humans can infer the missing parts of an occluded object by leveraging prior knowledge and visible cues. However, enabling deep learning models to accurately predict such occluded regions remains a challenging task. De-occlusion addresses this problem by reconstructing both the mask and RGB appearance. In this work, we focus on human de-occlusion, specifically targeting the recovery of occluded body structures and appearances. Our approach decomposes the task into two stages: mask completion and RGB completion. The first stage leverages a diffusion-based human body prior to provide a comprehensive representation of body structure, combined with occluded joint heatmaps that offer explicit spatial cues about missing regions. The reconstructed amodal mask then serves as a conditioning input for the second stage, guiding the model on which areas require RGB reconstruction. To further enhance RGB generation, we incorporate human-specific textual features derived using a visual question answering (VQA) model and encoded via a CLIP encoder. RGB completion is performed using Stable Diffusion, with decoder fine-tuning applied to mitigate pixel-level degradation in visible regions -- a known limitation of prior diffusion-based de-occlusion methods caused by latent space transformations. Our method effectively reconstructs human appearances even under severe occlusions and consistently outperforms existing methods in both mask and RGB completion. Moreover, the de-occluded images generated by our approach can improve the performance of downstream human-centric tasks, such as 2D pose estimation and 3D human reconstruction. The code will be made publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a9\u7801\u5b8c\u6210\u548cRGB\u5b8c\u6210\u6765\u6062\u590d\u906e\u6321\u4eba\u4f53\u7684\u7ed3\u6784\u548c\u5916\u89c2\u3002\u65b9\u6cd5\u7ed3\u5408\u4e86\u53cc\u5411\u6a21\u578b\u3001\u5173\u8282\u70ed\u529b\u56fe\u548c\u4eba\u4f53\u7279\u5f81\u63cf\u8ff0\uff0c\u5728\u4e25\u91cd\u906e\u6321\u60c5\u51b5\u4e0b\u4e5f\u80fd\u51c6\u786e\u91cd\u5efa\u4eba\u4f53\u5916\u89c2\uff0c\u5e76\u80fd\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u4eba\u7c7b\u80fd\u591f\u5229\u7528\u5148\u9a8c\u77e5\u8bc6\u548c\u53ef\u89c1\u7ebf\u7d22\u63a8\u65ad\u906e\u6321\u7269\u4f53\u7684\u7f3a\u5931\u90e8\u5206\uff0c\u4f46\u8ba9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u51c6\u786e\u9884\u6d4b\u906e\u6321\u533a\u57df\u4ecd\u662f\u4e00\u9879\u6311\u6218\u6027\u4efb\u52a1\u3002\u7279\u522b\u662f\u5728\u4eba\u4f53\u53bb\u906e\u6321\u9886\u57df\uff0c\u9700\u8981\u6062\u590d\u906e\u6321\u7684\u8eab\u4f53\u7ed3\u6784\u548c\u5916\u89c2\u3002", "method": "\u65b9\u6cd5\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u63a9\u7801\u5b8c\u6210\u548cRGB\u5b8c\u6210\u3002\u7b2c\u4e00\u9636\u6bb5\u5229\u7528\u57fa\u4e8e\u53cd\u5411\u6a21\u578b\u7684\u4eba\u4f53\u5148\u9a8c\u77e5\u8bc6\uff0c\u7ed3\u5408\u906e\u6321\u5173\u8282\u70ed\u529b\u56fe\u63d0\u4f9b\u7f3a\u5931\u533a\u57df\u7684\u663e\u5f0f\u7a7a\u95f4\u7ebf\u7d22\u3002\u91cd\u5efa\u7684\u65e0\u6a21\u6001\u63a9\u7801\u4f5c\u4e3a\u7b2c\u4e8c\u9636\u6bb5\u7684\u6761\u4ef6\u8f93\u5165\uff0c\u6307\u5bfcRGB\u91cd\u5efa\u3002\u8fd8\u7ed3\u5408\u4e86\u901a\u8fc7VQA\u6a21\u578b\u63d0\u53d6\u7684\u4eba\u4f53\u7279\u5f81\u6587\u672c\u63cf\u8ff0\uff0c\u5e76\u4f7f\u7528Stable Diffusion\u8fdb\u884cRGB\u5b8c\u6210\uff0c\u901a\u8fc7\u89e3\u7801\u5668\u5fae\u8c03\u51cf\u5c11\u53ef\u89c1\u533a\u57df\u7684\u50cf\u7d20\u7ea7\u9000\u5316\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e25\u91cd\u906e\u6321\u60c5\u51b5\u4e0b\u4e5f\u80fd\u6709\u6548\u5730\u91cd\u5efa\u4eba\u4f53\u5916\u89c2\uff0c\u5e76\u5728\u63a9\u7801\u5b8c\u6210\u548cRGB\u5b8c\u6210\u4e0a\u90fd\u6301\u7eed\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002\u751a\u81f3\uff0c\u901a\u8fc7\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u53bb\u906e\u6321\u56fe\u50cf\u8fd8\u80fd\u63d0\u9ad8\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u59822D\u59ff\u52bf\u4f30\u8ba1\u548c3D\u4eba\u4f53\u91cd\u5efa\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4eba\u4f53\u53bb\u906e\u6321\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5206\u89e3\u3001\u7ed3\u5408\u591a\u79cd\u63d0\u793a\u4fe1\u606f\uff08\u5305\u62ec\u63a9\u7801\u3001\u5173\u8282\u70ed\u529b\u56fe\u3001\u6587\u672c\u7279\u5f81\uff09\u548c\u4f18\u5316\u7684\u53cd\u5411\u6a21\u578b\u5b9e\u73b0\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4e25\u91cd\u906e\u6321\u4e0b\u4eba\u4f53\u5916\u89c2\u91cd\u5efa\u7684\u6311\u6218\uff0c\u5e76\u4e3a\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u8f93\u5165\u6570\u636e\u3002"}}
{"id": "2508.12668", "pdf": "https://arxiv.org/pdf/2508.12668", "abs": "https://arxiv.org/abs/2508.12668", "authors": ["Abhijay Ghildyal", "Li-Yun Wang", "Feng Liu"], "title": "WP-CLIP: Leveraging CLIP to Predict W\u00f6lfflin's Principles in Visual Art", "categories": ["cs.CV"], "comment": "ICCV 2025 AI4VA workshop (oral), Code:   https://github.com/abhijay9/wpclip", "summary": "W\\\"olfflin's five principles offer a structured approach to analyzing stylistic variations for formal analysis. However, no existing metric effectively predicts all five principles in visual art. Computationally evaluating the visual aspects of a painting requires a metric that can interpret key elements such as color, composition, and thematic choices. Recent advancements in vision-language models (VLMs) have demonstrated their ability to evaluate abstract image attributes, making them promising candidates for this task. In this work, we investigate whether CLIP, pre-trained on large-scale data, can understand and predict W\\\"olfflin's principles. Our findings indicate that it does not inherently capture such nuanced stylistic elements. To address this, we fine-tune CLIP on annotated datasets of real art images to predict a score for each principle. We evaluate our model, WP-CLIP, on GAN-generated paintings and the Pandora-18K art dataset, demonstrating its ability to generalize across diverse artistic styles. Our results highlight the potential of VLMs for automated art analysis.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u4f7f\u7528CLIP\u6a21\u578b\u9884\u6d4bW\u00f6lfflin\u4e94\u5927\u827a\u672f\u539f\u5219\uff0c\u901a\u8fc7\u7ec6\u8c03\u5728\u7ed8\u753b\u6570\u636e\u96c6\u4e0a\u7684\u65b9\u6cd5\u63d0\u5347\u4e86\u5bf9\u827a\u672f\u98ce\u683c\u7684\u7406\u89e3\u80fd\u529b\u3002", "motivation": "W\u00f6lfflin\u4e94\u5927\u539f\u5219\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u7684\u827a\u672f\u5206\u6790\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u8ba1\u91cf\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u9884\u6d4b\u6240\u6709\u539f\u5219\u3002\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u62bd\u8c61\u56fe\u50cf\u5c5e\u6027\u8bc4\u4f30\u65b9\u9762\u7684\u8fdb\u5c55\u4f7f\u5f97\u5b83\u4eec\u6210\u4e3a\u6f5c\u5728\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u9996\u5148\u8bc4\u4f30\u9884\u8bad\u7ec3\u7684CLIP\u6a21\u578b\u662f\u5426\u80fd\u7406\u89e3W\u00f6lfflin\u539f\u5219\uff0c\u7136\u540e\u5728\u6ce8\u91ca\u8fc7\u7684\u771f\u5b9e\u827a\u672f\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u5bf9CLIP\u8fdb\u884c\u7ec6\u8c03\uff0c\u4ee5\u9884\u6d4b\u6bcf\u4e2a\u539f\u5219\u7684\u5f97\u5206\u3002\u5f00\u53d1\u4e86WP-CLIP\u6a21\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u9884\u8bad\u7ec3\u7684CLIP\u5e76\u4e0d\u80fd\u5185\u5728\u7406\u89e3\u8fd9\u4e9b\u7ec6\u81f4\u7684\u827a\u672f\u5143\u7d20\u3002\u7ec6\u8c03\u540e\u7684WP-CLIP\u6a21\u578b\u80fd\u591f\u5728GAN\u751f\u6210\u7684\u7ed8\u753b\u548cPandora-18K\u827a\u672f\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u826f\u597d\u7684\u6cdb\u5316\uff0c\u9002\u7528\u4e8e\u591a\u6837\u7684\u827a\u672f\u98ce\u683c\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u9ad8\u4eae\u4e86\u89c6\u89c6-\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u5316\u827a\u672f\u5206\u6790\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u6709\u76ee\u6807\u7684\u7ec6\u8c03\u53ef\u4ee5\u8ba9\u6a21\u578b\u638c\u63e1\u66f4\u6df1\u5165\u7684\u827a\u672f\u98ce\u683c\u7279\u5f81\u3002"}}
{"id": "2508.12718", "pdf": "https://arxiv.org/pdf/2508.12718", "abs": "https://arxiv.org/abs/2508.12718", "authors": ["Syed Muhmmad Israr", "Feng Zhao"], "title": "Single-Reference Text-to-Image Manipulation with Dual Contrastive Denoising Score", "categories": ["cs.CV"], "comment": null, "summary": "Large-scale text-to-image generative models have shown remarkable ability to synthesize diverse and high-quality images. However, it is still challenging to directly apply these models for editing real images for two reasons. First, it is difficult for users to come up with a perfect text prompt that accurately describes every visual detail in the input image. Second, while existing models can introduce desirable changes in certain regions, they often dramatically alter the input content and introduce unexpected changes in unwanted regions. To address these challenges, we present Dual Contrastive Denoising Score, a simple yet powerful framework that leverages the rich generative prior of text-to-image diffusion models. Inspired by contrastive learning approaches for unpaired image-to-image translation, we introduce a straightforward dual contrastive loss within the proposed framework. Our approach utilizes the extensive spatial information from the intermediate representations of the self-attention layers in latent diffusion models without depending on auxiliary networks. Our method achieves both flexible content modification and structure preservation between input and output images, as well as zero-shot image-to-image translation. Through extensive experiments, we show that our approach outperforms existing methods in real image editing while maintaining the capability to directly utilize pretrained text-to-image diffusion models without further training.", "AI": {"tldr": "\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u53cc\u5bf9\u6bd4\u53bb\u566a\u6846\u67b6\uff0c\u5229\u7528\u6f5c\u5728\u6f5c\u6563\u6a21\u578b\u7684\u81ea\u6ce8\u610f\u529b\u5c42\u4e2d\u95f4\u8868\u5f81\uff0c\u5b9e\u73b0\u771f\u5b9e\u56fe\u50cf\u7f16\u8f91\u4e2d\u7684\u7075\u6d3b\u5185\u5bb9\u4fee\u6539\u548c\u7ed3\u6784\u4fdd\u6301", "motivation": "\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u771f\u5b9e\u56fe\u50cf\u7f16\u8f91\u4e2d\u7684\u4e24\u5927\u6311\u6218\uff1a\u7528\u6237\u96be\u4ee5\u51c6\u786e\u63cf\u8ff0\u6240\u6709\u89c6\u89c9\u7ec6\u8282\u7684\u63d0\u793a\u8bcd\uff0c\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\u5728\u4fee\u6539\u7279\u5b9a\u533a\u57df\u65f6\u5bb9\u6613\u9020\u6210\u610f\u5916\u53d8\u5316", "method": "\u63d0\u51fa\u53cc\u5bf9\u6bd4\u53bb\u566a\u5f97\u5206\u6846\u67b6\uff0c\u5728\u6f5c\u5728\u6f5c\u6563\u6a21\u578b\u7684\u81ea\u6ce8\u610f\u529b\u5c42\u4e2d\u95f4\u8868\u5f81\u4e2d\u5f15\u5165\u7b80\u5355\u7684\u53cc\u5bf9\u6bd4\u635f\u5931\uff0c\u65e0\u9700\u9644\u52a0\u7f51\u7edc", "result": "\u65b9\u6cd5\u5728\u771f\u5b9e\u56fe\u50cf\u7f16\u8f91\u4e0a\u8d85\u8fc7\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u7ef4\u6301\u4e86\u76f4\u63a5\u4f7f\u7528\u9884\u8bad\u7ec3\u6587\u672c\u5230\u56fe\u50cf\u6f5c\u6563\u6a21\u578b\u7684\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u7075\u6d3b\u5185\u5bb9\u4fee\u6539\u548c\u7ed3\u6784\u4fdd\u6301", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5229\u7528\u6f5c\u5728\u6f5c\u6563\u6a21\u578b\u7684\u4e2d\u95f4\u8868\u5f81\u548c\u5bf9\u6bd4\u5b66\u4e60\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u771f\u5b9e\u56fe\u50cf\u7f16\u8f91\u7684\u5173\u952e\u6311\u6218\uff0c\u5177\u6709\u96f6\u6837\u672c\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\u80fd\u529b"}}
{"id": "2508.12720", "pdf": "https://arxiv.org/pdf/2508.12720", "abs": "https://arxiv.org/abs/2508.12720", "authors": ["Kangjie Chen", "Yingji Zhong", "Zhihao Li", "Jiaqi Lin", "Youyu Chen", "Minghan Qin", "Haoqian Wang"], "title": "Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "Under review. Project page:   https://chenkangjie1123.github.io/Co-Adaptation-3DGS/", "summary": "3D Gaussian Splatting (3DGS) has demonstrated impressive performance in novel view synthesis under dense-view settings. However, in sparse-view scenarios, despite the realistic renderings in training views, 3DGS occasionally manifests appearance artifacts in novel views. This paper investigates the appearance artifacts in sparse-view 3DGS and uncovers a core limitation of current approaches: the optimized Gaussians are overly-entangled with one another to aggressively fit the training views, which leads to a neglect of the real appearance distribution of the underlying scene and results in appearance artifacts in novel views. The analysis is based on a proposed metric, termed Co-Adaptation Score (CA), which quantifies the entanglement among Gaussians, i.e., co-adaptation, by computing the pixel-wise variance across multiple renderings of the same viewpoint, with different random subsets of Gaussians. The analysis reveals that the degree of co-adaptation is naturally alleviated as the number of training views increases. Based on the analysis, we propose two lightweight strategies to explicitly mitigate the co-adaptation in sparse-view 3DGS: (1) random gaussian dropout; (2) multiplicative noise injection to the opacity. Both strategies are designed to be plug-and-play, and their effectiveness is validated across various methods and benchmarks. We hope that our insights into the co-adaptation effect will inspire the community to achieve a more comprehensive understanding of sparse-view 3DGS.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u7a00\u758f\u89c6\u56fe\u4e0b3D\u9ad8\u65af\u6cfc\u6e85(3DGS)\u7684\u5916\u89c2\u4f2a\u5f71\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u91cf\u5316\u9ad8\u65af\u7ea0\u7f20\u7a0b\u5ea6\u7684\u5171\u9002\u5e94\u8bc4\u5206(CA)\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u8f7b\u91cf\u7ea7\u7b56\u7565\u6765\u7f13\u89e3\u5171\u9002\u5e94\u6548\u5e94\u3002", "motivation": "3DGS\u5728\u5bc6\u96c6\u89c6\u56fe\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u7a00\u758f\u89c6\u56fe\u573a\u666f\u4e2d\uff0c\u5c3d\u7ba1\u8bad\u7ec3\u89c6\u56fe\u6e32\u67d3\u6548\u679c\u771f\u5b9e\uff0c\u4f46\u5728\u65b0\u89c6\u56fe\u4e2d\u4f1a\u51fa\u73b0\u5916\u89c2\u4f2a\u5f71\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76\u8fd9\u4e9b\u4f2a\u5f71\u7684\u6839\u672c\u539f\u56e0\u3002", "method": "\u63d0\u51fa\u4e86\u5171\u9002\u5e94\u8bc4\u5206(CA)\u6765\u91cf\u5316\u9ad8\u65af\u4e4b\u95f4\u7684\u7ea0\u7f20\u7a0b\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u7b56\u7565\uff1a(1)\u968f\u673a\u9ad8\u65af\u4e22\u5f03\uff1b(2)\u5bf9\u4e0d\u900f\u660e\u5ea6\u6ce8\u5165\u4e58\u6027\u566a\u58f0\u3002\u8fd9\u4e24\u79cd\u7b56\u7565\u90fd\u662f\u5373\u63d2\u5373\u7528\u7684\u3002", "result": "\u5206\u6790\u53d1\u73b0\u9ad8\u65af\u4e4b\u95f4\u7684\u5171\u9002\u5e94\u7a0b\u5ea6\u968f\u7740\u8bad\u7ec3\u89c6\u56fe\u6570\u91cf\u7684\u589e\u52a0\u800c\u81ea\u7136\u7f13\u89e3\u3002\u63d0\u51fa\u7684\u4e24\u79cd\u7b56\u7565\u5728\u5404\u79cd\u65b9\u6cd5\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u8bc1\u660e\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u5bf9\u5171\u9002\u5e94\u6548\u5e94\u7684\u6df1\u5165\u7406\u89e3\u5c06\u6709\u52a9\u4e8e\u793e\u533a\u66f4\u5168\u9762\u5730\u7406\u89e3\u7a00\u758f\u89c6\u56fe3DGS\uff0c\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u7b56\u7565\u80fd\u6709\u6548\u7f13\u89e3\u5916\u89c2\u4f2a\u5f71\u95ee\u9898\u3002"}}
{"id": "2508.12736", "pdf": "https://arxiv.org/pdf/2508.12736", "abs": "https://arxiv.org/abs/2508.12736", "authors": ["Ying Zhang", "Xiongxin Tang", "Chongyi Li", "Qiao Chen", "Yuquan Wu"], "title": "Frequency-Driven Inverse Kernel Prediction for Single Image Defocus Deblurring", "categories": ["cs.CV"], "comment": null, "summary": "Single image defocus deblurring aims to recover an all-in-focus image from a defocus counterpart, where accurately modeling spatially varying blur kernels remains a key challenge. Most existing methods rely on spatial features for kernel estimation, but their performance degrades in severely blurry regions where local high-frequency details are missing. To address this, we propose a Frequency-Driven Inverse Kernel Prediction network (FDIKP) that incorporates frequency-domain representations to enhance structural identifiability in kernel modeling. Given the superior discriminative capability of the frequency domain for blur modeling, we design a Dual-Branch Inverse Kernel Prediction (DIKP) strategy that improves the accuracy of kernel estimation while maintaining stability. Moreover, considering the limited number of predicted inverse kernels, we introduce a Position Adaptive Convolution (PAC) to enhance the adaptability of the deconvolution process. Finally, we propose a Dual-Domain Scale Recurrent Module (DSRM) to fuse deconvolution results and progressively improve deblurring quality from coarse to fine. Extensive experiments demonstrate that our method outperforms existing approaches. Code will be made publicly available.", "AI": {"tldr": "\u9891\u57df\u9a71\u52a8\u7684\u9006\u6838\u9884\u6d4b\u7f51\u7edc(FDIKP)\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u9006\u6838\u9884\u6d4b\u7b56\u7565\u548c\u4f4d\u7f6e\u9002\u914d\u5377\u79ef\uff0c\u63d0\u5347\u4e86\u5355\u56fe\u6563\u7126\u53bb\u6a21\u7cca\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u9760\u7a7a\u95f4\u7279\u5f81\u4f30\u8ba1\u6838\uff0c\u4f46\u5728\u4e25\u91cd\u6a21\u7cca\u533a\u57df\u6027\u80fd\u4f1a\u964d\u7ea7\uff0c\u56e0\u4e3a\u5c40\u90e8\u9ad8\u9891\u7ec6\u8282\u7f3a\u5931\u3002\u9891\u57df\u57fa\u7840\u8868\u793a\u80fd\u591f\u63d0\u4f9b\u66f4\u597d\u7684\u7ed3\u6784\u53ef\u8bc6\u522b\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u53cc\u5206\u652f\u9006\u6838\u9884\u6d4b\u7b56\u7565(DIKP)\u6765\u63d0\u9ad8\u6838\u4f30\u8ba1\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\uff1b\u4f7f\u7528\u4f4d\u7f6e\u9002\u914d\u5377\u79ef(PAC)\u589e\u5f3a\u53cd\u5377\u79ef\u8fc7\u7a0b\u7684\u9002\u5e94\u6027\uff1b\u63d0\u51fa\u53cc\u57df\u5c3a\u5ea6\u9012\u5f52\u6a21\u5757(DSRM)\u6765\u878d\u5408\u53cd\u5377\u79ef\u7ed3\u679c\u5e76\u6e10\u8fdb\u6539\u5584\u53bb\u6a21\u7cca\u8d28\u91cf\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5355\u56fe\u6563\u7126\u53bb\u6a21\u7cca\u4efb\u52a1\u4e0a\u8d85\u8fc7\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u9891\u57df\u57fa\u7840\u8868\u793a\u548c\u521b\u65b0\u7684\u6a21\u5757\u8bbe\u8ba1\uff0c\u672c\u6587\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u4e25\u91cd\u6a21\u7cca\u533a\u57df\u7684\u6838\u4f30\u8ba1\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u79c0\u7684\u53bb\u6a21\u7cca\u6548\u679c\u3002"}}
{"id": "2508.12784", "pdf": "https://arxiv.org/pdf/2508.12784", "abs": "https://arxiv.org/abs/2508.12784", "authors": ["Dan Ruta", "Abdelaziz Djelouah", "Raphael Ortiz", "Christopher Schroers"], "title": "Leveraging Diffusion Models for Stylization using Multiple Style Images", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in latent diffusion models have enabled exciting progress in image style transfer. However, several key issues remain. For example, existing methods still struggle to accurately match styles. They are often limited in the number of style images that can be used. Furthermore, they tend to entangle content and style in undesired ways. To address this, we propose leveraging multiple style images which helps better represent style features and prevent content leaking from the style images. We design a method that leverages both image prompt adapters and statistical alignment of the features during the denoising process. With this, our approach is designed such that it can intervene both at the cross-attention and the self-attention layers of the denoising UNet. For the statistical alignment, we employ clustering to distill a small representative set of attention features from the large number of attention values extracted from the style samples. As demonstrated in our experimental section, the resulting method achieves state-of-the-art results for stylization.", "AI": {"tldr": "\u57fa\u4e8e\u591a\u6837\u5f0f\u56fe\u7247\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u98ce\u683c\u8f6c\u6362\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u7247\u63d0\u793a\u9002\u914d\u5668\u548c\u7edf\u8ba1\u5bf9\u9f50\u6280\u672f\uff0c\u5728\u53bb\u566aUNet\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u81ea\u6ce8\u610f\u529b\u5c42\u8fdb\u884c\u5e72\u9884\uff0c\u5b9e\u73b0\u4e86\u72ec\u7279\u7684\u98ce\u683c\u8f6c\u6362\u6548\u679c", "motivation": "\u73b0\u6709\u98ce\u683c\u8f6c\u6362\u65b9\u6cd5\u5728\u51c6\u786e\u5339\u914d\u98ce\u683c\u3001\u53ef\u4f7f\u7528\u6837\u5f0f\u56fe\u7247\u6570\u91cf\u9650\u5236\u4ee5\u53ca\u5185\u5bb9\u4e0e\u98ce\u683c\u6050\u5408\u95ee\u9898\u65b9\u9762\u4ecd\u9047\u5230\u6311\u6218", "method": "\u5229\u7528\u591a\u6837\u5f0f\u56fe\u7247\u8868\u5f81\u98ce\u683c\u7279\u5f81\u5e76\u9632\u6b62\u5185\u5bb9\u6cc4\u6f0f\uff0c\u7ed3\u5408\u56fe\u7247\u63d0\u793a\u9002\u914d\u5668\u548c\u53bb\u566a\u8fc7\u7a0b\u4e2d\u7684\u7279\u5f81\u7edf\u8ba1\u5bf9\u9f50\uff0c\u5728UNet\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u81ea\u6ce8\u610f\u529b\u5c42\u8fdb\u884c\u5e72\u9884\uff0c\u901a\u8fc7\u805a\u7c7b\u6280\u672f\u4ece\u5927\u91cf\u6837\u5f0f\u6837\u672c\u4e2d\u7cbe\u70bc\u5c0f\u89c4\u6a21\u4ee3\u8868\u6027\u6ce8\u610f\u529b\u7279\u5f81\u96c6", "result": "\u8be5\u65b9\u6cd5\u5728\u98ce\u683c\u5316\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73", "conclusion": "\u901a\u8fc7\u591a\u6837\u5f0f\u56fe\u7247\u7ed3\u5408\u56fe\u50cf\u63d0\u793a\u9002\u914d\u5668\u548c\u7edf\u8ba1\u5bf9\u9f50\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u98ce\u683c\u8f6c\u6362\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u98ce\u683c\u5339\u914d\u548c\u66f4\u597d\u7684\u5185\u5bb9-\u98ce\u683c\u5206\u79bb"}}
{"id": "2508.12811", "pdf": "https://arxiv.org/pdf/2508.12811", "abs": "https://arxiv.org/abs/2508.12811", "authors": ["Yikai Wang", "Zhouxia Wang", "Zhonghua Wu", "Qingyi Tao", "Kang Liao", "Chen Change Loy"], "title": "Next Visual Granularity Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We propose a novel approach to image generation by decomposing an image into a structured sequence, where each element in the sequence shares the same spatial resolution but differs in the number of unique tokens used, capturing different level of visual granularity. Image generation is carried out through our newly introduced Next Visual Granularity (NVG) generation framework, which generates a visual granularity sequence beginning from an empty image and progressively refines it, from global layout to fine details, in a structured manner. This iterative process encodes a hierarchical, layered representation that offers fine-grained control over the generation process across multiple granularity levels. We train a series of NVG models for class-conditional image generation on the ImageNet dataset and observe clear scaling behavior. Compared to the VAR series, NVG consistently outperforms it in terms of FID scores (3.30 -> 3.03, 2.57 ->2.44, 2.09 -> 2.06). We also conduct extensive analysis to showcase the capability and potential of the NVG framework. Our code and models will be released.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u751f\u6210\u65b9\u6cd5NVG\uff0c\u901a\u8fc7\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u7ed3\u6784\u5316\u5e8f\u5217\uff0c\u4ece\u5168\u5c40\u5e03\u5c40\u5230\u7ec6\u8282\u9010\u6b65\u7ec6\u5316\u751f\u6210\uff0c\u5728ImageNet\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8eVAR\u7cfb\u5217\u7684FID\u5206\u6570\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u4e2d\u5bf9\u591a\u7c92\u5ea6\u5c42\u6b21\u63a7\u5236\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u4ece\u7c97\u5230\u7ec6\u9010\u6b65\u7ec6\u5316\u7684\u7ed3\u6784\u5316\u751f\u6210\u6846\u67b6\u3002", "method": "\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u5177\u6709\u76f8\u540c\u7a7a\u95f4\u5206\u8fa8\u7387\u4f46\u4e0d\u540c\u89c6\u89c9\u7c92\u5ea6\u7684\u7ed3\u6784\u5316\u5e8f\u5217\uff0c\u4f7f\u7528Next Visual Granularity (NVG)\u6846\u67b6\u4ece\u7a7a\u56fe\u50cf\u5f00\u59cb\u9010\u6b65\u751f\u6210\uff0c\u5b9e\u73b0\u4ece\u5168\u5c40\u5e03\u5c40\u5230\u7ec6\u8282\u7684\u5c42\u6b21\u5316\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5728ImageNet\u6570\u636e\u96c6\u4e0a\u7684\u7c7b\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0cNVG\u6a21\u578b\u76f8\u6bd4VAR\u7cfb\u5217\u5728FID\u5206\u6570\u4e0a\u6301\u7eed\u63d0\u5347\uff083.30\u21923.03\uff0c2.57\u21922.44\uff0c2.09\u21922.06\uff09\uff0c\u5e76\u663e\u793a\u51fa\u6e05\u6670\u7684\u7f29\u653e\u884c\u4e3a\u3002", "conclusion": "NVG\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u5e8f\u5217\u5206\u89e3\u548c\u6e10\u8fdb\u5f0f\u7ec6\u5316\u751f\u6210\uff0c\u5b9e\u73b0\u4e86\u5bf9\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u7684\u591a\u7c92\u5ea6\u5c42\u6b21\u63a7\u5236\uff0c\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u80fd\u529b\u548c\u6f5c\u529b\u3002"}}
{"id": "2508.12824", "pdf": "https://arxiv.org/pdf/2508.12824", "abs": "https://arxiv.org/abs/2508.12824", "authors": ["Shuang Chen", "Ronald Thenius", "Farshad Arvin", "Amir Atapour-Abarghouei"], "title": "DEEP-SEA: Deep-Learning Enhancement for Environmental Perception in Submerged Aquatics", "categories": ["cs.CV"], "comment": null, "summary": "Continuous and reliable underwater monitoring is essential for assessing marine biodiversity, detecting ecological changes and supporting autonomous exploration in aquatic environments. Underwater monitoring platforms rely on mainly visual data for marine biodiversity analysis, ecological assessment and autonomous exploration. However, underwater environments present significant challenges due to light scattering, absorption and turbidity, which degrade image clarity and distort colour information, which makes accurate observation difficult. To address these challenges, we propose DEEP-SEA, a novel deep learning-based underwater image restoration model to enhance both low- and high-frequency information while preserving spatial structures. The proposed Dual-Frequency Enhanced Self-Attention Spatial and Frequency Modulator aims to adaptively refine feature representations in frequency domains and simultaneously spatial information for better structural preservation. Our comprehensive experiments on EUVP and LSUI datasets demonstrate the superiority over the state of the art in restoring fine-grained image detail and structural consistency. By effectively mitigating underwater visual degradation, DEEP-SEA has the potential to improve the reliability of underwater monitoring platforms for more accurate ecological observation, species identification and autonomous navigation.", "AI": {"tldr": "DEEP-SEA\u662f\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6d77\u5e95\u56fe\u50cf\u6062\u590d\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u9891\u589e\u5f3a\u81ea\u6ce8\u610f\u529b\u673a\u5236\u540c\u65f6\u4f18\u5316\u7a7a\u95f4\u548c\u9891\u7387\u4fe1\u606f\uff0c\u6709\u6548\u89e3\u51b3\u6c34\u4e0b\u56fe\u50cf\u9000\u5316\u95ee\u9898", "motivation": "\u6c34\u4e0b\u73af\u5883\u5b58\u5728\u5149\u6563\u5c04\u3001\u5438\u6536\u548c\u6d51\u6d4a\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u56fe\u50cf\u6e05\u6670\u5ea6\u4e0b\u964d\u548c\u989c\u8272\u5931\u771f\uff0c\u5f71\u54cd\u6d77\u6d0b\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u548c\u751f\u6001\u8bc4\u4f30\u7684\u51c6\u786e\u6027", "method": "\u63d0\u51fa\u53cc\u9891\u589e\u5f3a\u81ea\u6ce8\u610f\u529b\u7a7a\u95f4\u548c\u9891\u7387\u8c03\u5236\u5668\uff0c\u81ea\u9002\u5e94\u5730\u5728\u9891\u57df\u4e2d\u4f18\u5316\u7279\u5f81\u8868\u793a\uff0c\u540c\u65f6\u4fdd\u6301\u7a7a\u95f4\u4fe1\u606f\u4ee5\u66f4\u597d\u5730\u4fdd\u62a4\u7ed3\u6784", "result": "\u5728EUVP\u548cLSUI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u6062\u590d\u7cbe\u7ec6\u56fe\u50cf\u7ec6\u8282\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "DEEP-SEA\u901a\u8fc7\u6709\u6548\u7f13\u89e3\u6c34\u4e0b\u89c6\u89c9\u9000\u5316\u95ee\u9898\uff0c\u6709\u6f5c\u529b\u63d0\u9ad8\u6c34\u4e0b\u76d1\u6d4b\u5e73\u53f0\u7684\u53ef\u9760\u6027\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u751f\u6001\u89c2\u6d4b\u3001\u7269\u79cd\u8bc6\u522b\u548c\u81ea\u4e3b\u5bfc\u822a"}}
{"id": "2508.12880", "pdf": "https://arxiv.org/pdf/2508.12880", "abs": "https://arxiv.org/abs/2508.12880", "authors": ["Chubin Chen", "Jiashu Zhu", "Xiaokun Feng", "Nisha Huang", "Meiqi Wu", "Fangyuan Mao", "Jiahong Wu", "Xiangxiang Chu", "Xiu Li"], "title": "S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Classifier-free Guidance (CFG) is a widely used technique in modern diffusion models for enhancing sample quality and prompt adherence. However, through an empirical analysis on Gaussian mixture modeling with a closed-form solution, we observe a discrepancy between the suboptimal results produced by CFG and the ground truth. The model's excessive reliance on these suboptimal predictions often leads to semantic incoherence and low-quality outputs. To address this issue, we first empirically demonstrate that the model's suboptimal predictions can be effectively refined using sub-networks of the model itself. Building on this insight, we propose S^2-Guidance, a novel method that leverages stochastic block-dropping during the forward process to construct stochastic sub-networks, effectively guiding the model away from potential low-quality predictions and toward high-quality outputs. Extensive qualitative and quantitative experiments on text-to-image and text-to-video generation tasks demonstrate that S^2-Guidance delivers superior performance, consistently surpassing CFG and other advanced guidance strategies. Our code will be released.", "AI": {"tldr": "S\u00b2-Guidance\u662f\u4e00\u79cd\u65b0\u7684\u5bfc\u5411\u65b9\u6cd5\uff0c\u901a\u8fc7\u968f\u673a\u5757\u6295\u5f03\u6784\u5efa\u5b50\u7f51\u7edc\uff0c\u6539\u5584\u4e86CFG\u5728\u6a21\u7cca\u6a21\u578b\u4e2d\u7684\u6b21\u4f18\u9884\u6d4b\u95ee\u9898\uff0c\u5728\u6587\u672c\u5230\u56fe\u50cf/\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u7ecf\u9a8c\u5206\u6790\u53d1\u73b0CFG\u5728\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u4e2d\u4ea7\u751f\u4e0e\u771f\u5b9e\u503c\u7684\u504f\u5dee\uff0c\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u6b21\u4f18\u9884\u6d4b\u5bfc\u81f4\u8bed\u4e49\u4e0d\u4e00\u81f4\u548c\u4f4e\u8d28\u91cf\u8f93\u51fa\u3002", "method": "\u901a\u8fc7\u5728\u524d\u5411\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u968f\u673a\u5757\u6295\u5f03\u6784\u5efa\u968f\u673a\u5b50\u7f51\u7edc\uff0c\u7528\u4e8e\u7cbe\u70bc\u6a21\u578b\u7684\u6b21\u4f18\u9884\u6d4b\uff0c\u5f15\u5bfc\u6a21\u578b\u907f\u514d\u4f4e\u8d28\u91cf\u9884\u6d4b\u5e76\u5411\u9ad8\u8d28\u91cf\u8f93\u51fa\u53d1\u5c55\u3002", "result": "\u5728\u6587\u672c\u5230\u56fe\u50cf\u548c\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\uff0cS\u00b2-Guidance\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u4e2d\u90fd\u663e\u793a\u51fa\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u4e00\u8d35\u8d85\u8fc7CFG\u548c\u5176\u4ed6\u5148\u8fdb\u5bfc\u5411\u7b56\u7565\u3002", "conclusion": "S\u00b2-Guidance\u901a\u8fc7\u5229\u7528\u6a21\u578b\u81ea\u8eab\u7684\u5b50\u7f51\u7edc\u7cbe\u70bc\u9884\u6d4b\uff0c\u6709\u6548\u89e3\u51b3\u4e86CFG\u7684\u9650\u5236\uff0c\u4e3a\u6a21\u7cca\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5bfc\u5411\u65b9\u6cd5\u3002"}}
{"id": "2508.12900", "pdf": "https://arxiv.org/pdf/2508.12900", "abs": "https://arxiv.org/abs/2508.12900", "authors": ["Jiayi Wang", "Hadrien Reynaud", "Franciskus Xaverius Erick", "Bernhard Kainz"], "title": "CTFlow: Video-Inspired Latent Flow Matching for 3D CT Synthesis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Generative modelling of entire CT volumes conditioned on clinical reports has the potential to accelerate research through data augmentation, privacy-preserving synthesis and reducing regulator-constraints on patient data while preserving diagnostic signals. With the recent release of CT-RATE, a large-scale collection of 3D CT volumes paired with their respective clinical reports, training large text-conditioned CT volume generation models has become achievable. In this work, we introduce CTFlow, a 0.5B latent flow matching transformer model, conditioned on clinical reports. We leverage the A-VAE from FLUX to define our latent space, and rely on the CT-Clip text encoder to encode the clinical reports. To generate consistent whole CT volumes while keeping the memory constraints tractable, we rely on a custom autoregressive approach, where the model predicts the first sequence of slices of the volume from text-only, and then relies on the previously generated sequence of slices and the text, to predict the following sequence. We evaluate our results against state-of-the-art generative CT model, and demonstrate the superiority of our approach in terms of temporal coherence, image diversity and text-image alignment, with FID, FVD, IS scores and CLIP score.", "AI": {"tldr": "CTFlow\u662f\u4e00\u4e2a0.5B\u53c2\u6570\u7684\u6f5c\u5728\u6d41\u5339\u914d\u53d8\u6362\u5668\u6a21\u578b\uff0c\u901a\u8fc7\u4e34\u5e8a\u62a5\u544a\u6587\u672c\u6761\u4ef6\u751f\u6210\u5b8c\u6574\u76843D CT\u4f53\u79ef\uff0c\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u3001\u56fe\u50cf\u591a\u6837\u6027\u548c\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u57fa\u4e8e\u4e34\u5e8a\u62a5\u544a\u751f\u6210\u5b8c\u6574CT\u4f53\u79ef\u53ef\u4ee5\u52a0\u901f\u533b\u5b66\u7814\u7a76\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u3001\u9690\u79c1\u4fdd\u62a4\u5408\u6210\u548c\u51cf\u5c11\u60a3\u8005\u6570\u636e\u76d1\u7ba1\u7ea6\u675f\uff0c\u540c\u65f6\u4fdd\u7559\u8bca\u65ad\u4fe1\u53f7\u3002CT-RATE\u6570\u636e\u96c6\u7684\u53d1\u5e03\u4f7f\u5f97\u8bad\u7ec3\u5927\u578b\u6587\u672c\u6761\u4ef6CT\u751f\u6210\u6a21\u578b\u6210\u4e3a\u53ef\u80fd\u3002", "method": "\u4f7f\u7528FLUX\u7684A-VAE\u5b9a\u4e49\u6f5c\u5728\u7a7a\u95f4\uff0cCT-Clip\u6587\u672c\u7f16\u7801\u5668\u7f16\u7801\u4e34\u5e8a\u62a5\u544a\u3002\u91c7\u7528\u81ea\u5b9a\u4e49\u81ea\u56de\u5f52\u65b9\u6cd5\uff1a\u6a21\u578b\u9996\u5148\u4ece\u7eaf\u6587\u672c\u9884\u6d4b\u7b2c\u4e00\u4e2a\u5207\u7247\u5e8f\u5217\uff0c\u7136\u540e\u57fa\u4e8e\u5148\u524d\u751f\u6210\u7684\u5207\u7247\u5e8f\u5217\u548c\u6587\u672c\u9884\u6d4b\u540e\u7eed\u5e8f\u5217\uff0c\u4ee5\u4fdd\u6301\u6574\u4e2aCT\u4f53\u79ef\u7684\u4e00\u81f4\u6027\u5e76\u63a7\u5236\u5185\u5b58\u7ea6\u675f\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u7684CT\u751f\u6210\u6a21\u578b\u76f8\u6bd4\uff0cCTFlow\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u3001\u56fe\u50cf\u591a\u6837\u6027\u548c\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u901a\u8fc7FID\u3001FVD\u3001IS\u5206\u6570\u548cCLIP\u5206\u6570\u8fdb\u884c\u8bc4\u4f30\u3002", "conclusion": "CTFlow\u6210\u529f\u5b9e\u73b0\u4e86\u57fa\u4e8e\u4e34\u5e8a\u62a5\u544a\u7684\u5b8c\u65743D CT\u4f53\u79ef\u751f\u6210\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u751f\u6210\u6a21\u578b\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.12919", "pdf": "https://arxiv.org/pdf/2508.12919", "abs": "https://arxiv.org/abs/2508.12919", "authors": ["Elena Izzo", "Luca Parolari", "Davide Vezzaro", "Lamberto Ballan"], "title": "7Bench: a Comprehensive Benchmark for Layout-guided Text-to-image Models", "categories": ["cs.CV"], "comment": "Accepted to ICIAP 2025", "summary": "Layout-guided text-to-image models offer greater control over the generation process by explicitly conditioning image synthesis on the spatial arrangement of elements. As a result, their adoption has increased in many computer vision applications, ranging from content creation to synthetic data generation. A critical challenge is achieving precise alignment between the image, textual prompt, and layout, ensuring semantic fidelity and spatial accuracy. Although recent benchmarks assess text alignment, layout alignment remains overlooked, and no existing benchmark jointly evaluates both. This gap limits the ability to evaluate a model's spatial fidelity, which is crucial when using layout-guided generation for synthetic data, as errors can introduce noise and degrade data quality. In this work, we introduce 7Bench, the first benchmark to assess both semantic and spatial alignment in layout-guided text-to-image generation. It features text-and-layout pairs spanning seven challenging scenarios, investigating object generation, color fidelity, attribute recognition, inter-object relationships, and spatial control. We propose an evaluation protocol that builds on existing frameworks by incorporating the layout alignment score to assess spatial accuracy. Using 7Bench, we evaluate several state-of-the-art diffusion models, uncovering their respective strengths and limitations across diverse alignment tasks. The benchmark is available at https://github.com/Elizzo/7Bench.", "AI": {"tldr": "7Bench\u662f\u9996\u4e2a\u8bc4\u4f30\u5e03\u5c40\u5f15\u5bfc\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u8bed\u4e49\u548c\u7a7a\u95f4\u5bf9\u9f50\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u4e03\u4e2a\u6311\u6218\u6027\u573a\u666f\uff0c\u7528\u4e8e\u8bc4\u4f30\u5bf9\u8c61\u751f\u6210\u3001\u989c\u8272\u4fdd\u771f\u5ea6\u3001\u5c5e\u6027\u8bc6\u522b\u3001\u5bf9\u8c61\u95f4\u5173\u7cfb\u548c\u7a7a\u95f4\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u8bc4\u4f30\u6587\u672c\u5bf9\u9f50\uff0c\u800c\u5ffd\u7565\u4e86\u5e03\u5c40\u5bf9\u9f50\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u7684\u7a7a\u95f4\u4fdd\u771f\u5ea6\uff0c\u8fd9\u5728\u5408\u6210\u6570\u636e\u751f\u6210\u7b49\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u534f\u8bae\uff0c\u5728\u73b0\u6709\u6846\u67b6\u57fa\u7840\u4e0a\u52a0\u5165\u5e03\u5c40\u5bf9\u9f50\u5206\u6570\u6765\u8bc4\u4f30\u7a7a\u95f4\u51c6\u786e\u6027\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b\u4e03\u4e2a\u6311\u6218\u6027\u573a\u666f\u7684\u6587\u672c-\u5e03\u5c40\u5bf9\u6570\u636e\u96c6\u3002", "result": "\u4f7f\u75287Bench\u8bc4\u4f30\u4e86\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u6269\u6563\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u4e0d\u540c\u5bf9\u9f50\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "conclusion": "7Bench\u586b\u8865\u4e86\u5e03\u5c40\u5f15\u5bfc\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u6a21\u578b\u7684\u7a7a\u95f4\u4fdd\u771f\u5ea6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u5408\u6210\u6570\u636e\u8d28\u91cf\u3002"}}
{"id": "2508.12969", "pdf": "https://arxiv.org/pdf/2508.12969", "abs": "https://arxiv.org/abs/2508.12969", "authors": ["Qirui Li", "Guangcong Zheng", "Qi Zhao", "Jie Li", "Bin Dong", "Yiwu Yao", "Xi Li"], "title": "Compact Attention: Exploiting Structured Spatio-Temporal Sparsity for Fast Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "The computational demands of self-attention mechanisms pose a critical challenge for transformer-based video generation, particularly in synthesizing ultra-long sequences. Current approaches, such as factorized attention and fixed sparse patterns, fail to fully exploit the inherent spatio-temporal redundancies in video data. Through systematic analysis of video diffusion transformers (DiT), we uncover a key insight: Attention matrices exhibit structured, yet heterogeneous sparsity patterns, where specialized heads dynamically attend to distinct spatiotemporal regions (e.g., local pattern, cross-shaped pattern, or global pattern). Existing sparse attention methods either impose rigid constraints or introduce significant overhead, limiting their effectiveness. To address this, we propose Compact Attention, a hardware-aware acceleration framework featuring three innovations: 1) Adaptive tiling strategies that approximate diverse spatial interaction patterns via dynamic tile grouping, 2) Temporally varying windows that adjust sparsity levels based on frame proximity, and 3) An automated configuration search algorithm that optimizes sparse patterns while preserving critical attention pathways. Our method achieves 1.6~2.5x acceleration in attention computation on single-GPU setups while maintaining comparable visual quality with full-attention baselines. This work provides a principled approach to unlocking efficient long-form video generation through structured sparsity exploitation. Project Page: https://yo-ava.github.io/Compact-Attention.github.io/", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86Compact Attention\u6846\u67b6\uff0c\u901a\u8fc7\u8ba4\u522b\u548c\u5229\u7528\u89c6\u9891\u6570\u636e\u4e2d\u7684\u7ed3\u6784\u5316\u7a00\u758f\u6a21\u5f0f\uff0c\u5728\u4fdd\u6301\u8ba4\u77e5\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6ce8\u610f\u529b\u8ba1\u7b97\u76841.6~2.5\u500d\u52a0\u901f\u3002", "motivation": "\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u8ba1\u7b97\u8981\u6c42\u7ed9\u57fa\u4e8etransformer\u7684\u89c6\u9891\u751f\u6210\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5408\u6210\u8d85\u957f\u5e8f\u5217\u65f6\u3002\u73b0\u6709\u7684\u56e0\u5b50\u5316\u6ce8\u610f\u529b\u548c\u56fa\u5b9a\u7a00\u758f\u6a21\u5f0f\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u5229\u7528\u89c6\u9891\u6570\u636e\u4e2d\u5185\u5728\u7684\u65f6\u7a7a\u5197\u4f59\u6027\u3002", "method": "\u8ba1\u7b97\u673a\u786e\u77e5\u7684\u52a0\u901f\u6846\u67b6Compact Attention\uff0c\u5305\u542b\u4e09\u9879\u521b\u65b0\uff1a1)\u901a\u8fc7\u52a8\u6001\u5236\u56fe\u5206\u7ec4\u8fd1\u4f3c\u591a\u6837\u5316\u7a7a\u95f4\u4ea4\u4e92\u6a21\u5f0f\u7684\u9002\u5e94\u6027\u5236\u56fe\u7b56\u7565\uff1b2)\u6839\u636e\u5e27\u8ddd\u79bb\u8c03\u6574\u7a00\u758f\u7a0b\u5ea6\u7684\u65f6\u53d8\u7a97\u53e3\uff1b3)\u5728\u4fdd\u7559\u5173\u952e\u6ce8\u610f\u529b\u9014\u5f84\u7684\u540c\u65f6\u4f18\u5316\u7a00\u758f\u6a21\u5f0f\u7684\u81ea\u52a8\u5316\u914d\u7f6e\u641c\u7d22\u7b97\u6cd5\u3002", "result": "\u5728\u5355GPU\u73af\u5883\u4e0b\uff0c\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6ce8\u610f\u529b\u8ba1\u7b971.6~2.5\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u5168\u6ce8\u610f\u529b\u57fa\u7ebf\u76f8\u5f53\u7684\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u7ed3\u6784\u5316\u7a00\u758f\u5229\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u7406\u8bba\u57fa\u7840\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5f00\u542f\u9ad8\u6548\u7684\u957f\u5f62\u5f0f\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2508.13009", "pdf": "https://arxiv.org/pdf/2508.13009", "abs": "https://arxiv.org/abs/2508.13009", "authors": ["Xianglong He", "Chunli Peng", "Zexiang Liu", "Boyang Wang", "Yifan Zhang", "Qi Cui", "Fei Kang", "Biao Jiang", "Mengyin An", "Yangyang Ren", "Baixin Xu", "Hao-Xiang Guo", "Kaixiong Gong", "Cyrus Wu", "Wei Li", "Xuchen Song", "Yang Liu", "Eric Li", "Yahui Zhou"], "title": "Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model", "categories": ["cs.CV"], "comment": "Project Page: https://matrix-game-v2.github.io", "summary": "Recent advances in interactive video generations have demonstrated diffusion model's potential as world models by capturing complex physical dynamics and interactive behaviors. However, existing interactive world models depend on bidirectional attention and lengthy inference steps, severely limiting real-time performance. Consequently, they are hard to simulate real-world dynamics, where outcomes must update instantaneously based on historical context and current actions. To address this, we present Matrix-Game 2.0, an interactive world model generates long videos on-the-fly via few-step auto-regressive diffusion. Our framework consists of three key components: (1) A scalable data production pipeline for Unreal Engine and GTA5 environments to effectively produce massive amounts (about 1200 hours) of video data with diverse interaction annotations; (2) An action injection module that enables frame-level mouse and keyboard inputs as interactive conditions; (3) A few-step distillation based on the casual architecture for real-time and streaming video generation. Matrix Game 2.0 can generate high-quality minute-level videos across diverse scenes at an ultra-fast speed of 25 FPS. We open-source our model weights and codebase to advance research in interactive world modeling.", "AI": {"tldr": "Matrix-Game 2.0\u662f\u4e00\u4e2a\u5b9e\u65f6\u4ea4\u4e92\u5f0f\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u5c11\u6b65\u81ea\u56de\u5f52\u6269\u6563\u751f\u6210\u9ad8\u8d28\u91cf\u957f\u89c6\u9891\uff0c\u901f\u5ea6\u8fbe\u523025FPS", "motivation": "\u73b0\u6709\u4ea4\u4e92\u5f0f\u4e16\u754c\u6a21\u578b\u4f9d\u8d56\u53cc\u5411\u6ce8\u610f\u529b\u548c\u5197\u957f\u63a8\u7406\u6b65\u9aa4\uff0c\u4e25\u91cd\u9650\u5236\u5b9e\u65f6\u6027\u80fd\uff0c\u96be\u4ee5\u6a21\u62df\u9700\u8981\u5373\u65f6\u66f4\u65b0\u7684\u771f\u5b9e\u4e16\u754c\u52a8\u6001", "method": "\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1)\u53ef\u6269\u5c55\u7684\u6570\u636e\u751f\u4ea7\u6d41\u6c34\u7ebf\uff1b2)\u52a8\u4f5c\u6ce8\u5165\u6a21\u5757\uff1b3)\u57fa\u4e8e\u56e0\u679c\u67b6\u6784\u7684\u5c11\u6b65\u84b8\u998f\u6280\u672f", "result": "\u80fd\u591f\u4ee5\u8d85\u5feb\u901f\u5ea625FPS\u751f\u6210\u5206\u949f\u7ea7\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u6db5\u76d6\u591a\u6837\u5316\u573a\u666f", "conclusion": "\u8be5\u6a21\u578b\u5728\u4ea4\u4e92\u5f0f\u4e16\u754c\u5efa\u6a21\u65b9\u9762\u53d6\u5f97\u91cd\u8981\u8fdb\u5c55\uff0c\u5f00\u6e90\u6a21\u578b\u6743\u91cd\u548c\u4ee3\u7801\u5e93\u4ee5\u63a8\u52a8\u76f8\u5173\u7814\u7a76"}}
{"id": "2508.13013", "pdf": "https://arxiv.org/pdf/2508.13013", "abs": "https://arxiv.org/abs/2508.13013", "authors": ["Jingqiao Xiu", "Fangzhou Hong", "Yicong Li", "Mengze Li", "Wentao Wang", "Sirui Han", "Liang Pan", "Ziwei Liu"], "title": "EgoTwin: Dreaming Body and View in First Person", "categories": ["cs.CV"], "comment": null, "summary": "While exocentric video synthesis has achieved great progress, egocentric video generation remains largely underexplored, which requires modeling first-person view content along with camera motion patterns induced by the wearer's body movements. To bridge this gap, we introduce a novel task of joint egocentric video and human motion generation, characterized by two key challenges: 1) Viewpoint Alignment: the camera trajectory in the generated video must accurately align with the head trajectory derived from human motion; 2) Causal Interplay: the synthesized human motion must causally align with the observed visual dynamics across adjacent video frames. To address these challenges, we propose EgoTwin, a joint video-motion generation framework built on the diffusion transformer architecture. Specifically, EgoTwin introduces a head-centric motion representation that anchors the human motion to the head joint and incorporates a cybernetics-inspired interaction mechanism that explicitly captures the causal interplay between video and motion within attention operations. For comprehensive evaluation, we curate a large-scale real-world dataset of synchronized text-video-motion triplets and design novel metrics to assess video-motion consistency. Extensive experiments demonstrate the effectiveness of the EgoTwin framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86EgoTwin\u6846\u67b6\uff0c\u7528\u4e8e\u8054\u5408\u751f\u6210\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c6\u9891\u548c\u4eba\u4f53\u8fd0\u52a8\uff0c\u89e3\u51b3\u4e86\u89c6\u89d2\u5bf9\u9f50\u548c\u56e0\u679c\u4ea4\u4e92\u4e24\u4e2a\u5173\u952e\u6311\u6218\u3002", "motivation": "\u867d\u7136\u5916\u4e2d\u5fc3\u89c6\u9891\u5408\u6210\u53d6\u5f97\u4e86\u5f88\u5927\u8fdb\u5c55\uff0c\u4f46\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c6\u9891\u751f\u6210\u4ecd\u7136\u5f88\u5c11\u88ab\u63a2\u7d22\uff0c\u9700\u8981\u540c\u65f6\u5efa\u6a21\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u5185\u5bb9\u548c\u7a7f\u6234\u8005\u8eab\u4f53\u8fd0\u52a8\u5f15\u8d77\u7684\u76f8\u673a\u8fd0\u52a8\u6a21\u5f0f\u3002", "method": "\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\u6784\u5efaEgoTwin\u6846\u67b6\uff0c\u5f15\u5165\u4ee5\u5934\u90e8\u4e3a\u4e2d\u5fc3\u7684\u8fd0\u52a8\u8868\u793a\uff0c\u5e76\u91c7\u7528\u63a7\u5236\u8bba\u542f\u53d1\u7684\u4ea4\u4e92\u673a\u5236\u5728\u6ce8\u610f\u529b\u64cd\u4f5c\u4e2d\u663e\u5f0f\u6355\u6349\u89c6\u9891\u4e0e\u8fd0\u52a8\u4e4b\u95f4\u7684\u56e0\u679c\u4ea4\u4e92\u3002", "result": "\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u540c\u6b65\u6587\u672c-\u89c6\u9891-\u8fd0\u52a8\u4e09\u5143\u7ec4\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86EgoTwin\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "EgoTwin\u6210\u529f\u89e3\u51b3\u4e86\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u4e0e\u4eba\u4f53\u8fd0\u52a8\u8054\u5408\u751f\u6210\u7684\u4efb\u52a1\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8868\u793a\u548c\u4ea4\u4e92\u673a\u5236\u5b9e\u73b0\u4e86\u89c6\u9891\u4e0e\u8fd0\u52a8\u4e4b\u95f4\u7684\u7cbe\u786e\u5bf9\u9f50\u548c\u56e0\u679c\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.13043", "pdf": "https://arxiv.org/pdf/2508.13043", "abs": "https://arxiv.org/abs/2508.13043", "authors": ["Ayaka Yasunaga", "Hideo Saito", "Dieter Schmalstieg", "Shohei Mori"], "title": "IntelliCap: Intelligent Guidance for Consistent View Sampling", "categories": ["cs.CV"], "comment": "This work is a pre-print version of a paper that has been accepted to   the IEEE International Symposium on Mixed and Augmented Reality for future   publication. Project Page:   https://mediated-reality.github.io/projects/yasunaga_ismar25/", "summary": "Novel view synthesis from images, for example, with 3D Gaussian splatting, has made great progress. Rendering fidelity and speed are now ready even for demanding virtual reality applications. However, the problem of assisting humans in collecting the input images for these rendering algorithms has received much less attention. High-quality view synthesis requires uniform and dense view sampling. Unfortunately, these requirements are not easily addressed by human camera operators, who are in a hurry, impatient, or lack understanding of the scene structure and the photographic process. Existing approaches to guide humans during image acquisition concentrate on single objects or neglect view-dependent material characteristics. We propose a novel situated visualization technique for scanning at multiple scales. During the scanning of a scene, our method identifies important objects that need extended image coverage to properly represent view-dependent appearance. To this end, we leverage semantic segmentation and category identification, ranked by a vision-language model. Spherical proxies are generated around highly ranked objects to guide the user during scanning. Our results show superior performance in real scenes compared to conventional view sampling strategies.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u624b\u673a\u626b\u63cf\u5bfc\u822a\u6280\u672f\uff0c\u901a\u8fc7\u8bc6\u522b\u91cd\u8981\u7269\u4f53\u5e76\u751f\u6210\u7403\u9762\u4ee3\u7406\u6765\u6307\u5bfc\u7528\u6237\u91c7\u96c6\u66f4\u5305\u5bb9\u89c6\u89d2\u4f9d\u8d56\u5916\u89c2\u7684\u56fe\u50cf\uff0c\u63d0\u5347\u65b0\u89c6\u89d2\u5408\u6210\u8d28\u91cf\u3002", "motivation": "\u867d\u71363D\u5411\u91cf\u6563\u5c04\u7b49\u65b0\u89c6\u89d2\u5408\u6210\u6280\u672f\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u4f46\u5728\u56fe\u50cf\u91c7\u96c6\u8fc7\u7a0b\u4e2d\u5bfc\u822a\u4eba\u7c7b\u7528\u6237\u4ecd\u7f3a\u4e4f\u6709\u6548\u5de5\u5177\u3002\u4eba\u7c7b\u62cd\u6444\u8005\u5e38\u56e0\u6025\u5fd9\u3001\u8010\u5fc3\u4e0d\u8db3\u6216\u4e0d\u7406\u89e3\u573a\u666f\u7ed3\u6784\uff0c\u65e0\u6cd5\u5b8c\u6210\u5747\u5300\u5bc6\u96c6\u7684\u89c6\u89d2\u91c7\u6837\uff0c\u5f71\u54cd\u5408\u6210\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u591a\u5c3a\u5ea6\u626b\u63cf\u7684\u60ef\u4f4d\u53ef\u89c6\u5316\u6280\u672f\u3002\u5728\u626b\u63cf\u8fc7\u7a0b\u4e2d\uff0c\u901a\u8fc7\u8bed\u4e49\u5206\u5272\u548c\u7c7b\u522b\u8bc6\u522b\u68c0\u6d4b\u91cd\u8981\u7269\u4f53\uff0c\u4f7f\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6392\u540d\u3002\u4e3a\u6392\u540d\u8f83\u9ad8\u7684\u7269\u4f53\u751f\u6210\u7403\u9762\u4ee3\u7406\uff0c\u7528\u4e8e\u5728\u626b\u63cf\u65f6\u5bfc\u822a\u7528\u6237\u8981\u6c42\u66f4\u5145\u5206\u7684\u56fe\u50cf\u8986\u76d6\u3002", "result": "\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u65b0\u89c6\u89d2\u5408\u6210\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684\u89c6\u89d2\u91c7\u6837\u7b56\u7565\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u80fd\u591f\u5728\u56fe\u50cf\u91c7\u96c6\u8fc7\u7a0b\u4e2d\u6709\u6548\u5bfc\u822a\u7528\u6237\u7684\u60ef\u4f4d\u53ef\u89c6\u5316\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u52a8\u8bc6\u522b\u91cd\u8981\u7269\u4f53\u5e76\u63d0\u4f9b\u6709\u9488\u5bf9\u6027\u7684\u626b\u63cf\u5efa\u8bae\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65b0\u89c6\u89d2\u5408\u6210\u7684\u8d28\u91cf\u548c\u6548\u679c\u3002"}}
{"id": "2508.13065", "pdf": "https://arxiv.org/pdf/2508.13065", "abs": "https://arxiv.org/abs/2508.13065", "authors": ["Siddharth Khandelwal", "Sridhar Kamath", "Arjun Jain"], "title": "Odo: Depth-Guided Diffusion for Identity-Preserving Body Reshaping", "categories": ["cs.CV"], "comment": null, "summary": "Human shape editing enables controllable transformation of a person's body shape, such as thin, muscular, or overweight, while preserving pose, identity, clothing, and background. Unlike human pose editing, which has advanced rapidly, shape editing remains relatively underexplored. Current approaches typically rely on 3D morphable models or image warping, often introducing unrealistic body proportions, texture distortions, and background inconsistencies due to alignment errors and deformations. A key limitation is the lack of large-scale, publicly available datasets for training and evaluating body shape manipulation methods. In this work, we introduce the first large-scale dataset of 18,573 images across 1523 subjects, specifically designed for controlled human shape editing. It features diverse variations in body shape, including fat, muscular and thin, captured under consistent identity, clothing, and background conditions. Using this dataset, we propose Odo, an end-to-end diffusion-based method that enables realistic and intuitive body reshaping guided by simple semantic attributes. Our approach combines a frozen UNet that preserves fine-grained appearance and background details from the input image with a ControlNet that guides shape transformation using target SMPL depth maps. Extensive experiments demonstrate that our method outperforms prior approaches, achieving per-vertex reconstruction errors as low as 7.5mm, significantly lower than the 13.6mm observed in baseline methods, while producing realistic results that accurately match the desired target shapes.", "AI": {"tldr": "\u9996\u4e2a\u5927\u89c4\u6a21\u4eba\u4f53\u5f62\u72b6\u7f16\u8f91\u6570\u636e\u96c6\uff0c\u63d0\u51faOdo\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u51bb\u7ed3UNet\u548cControlNet\u5b9e\u73b0\u5b9e\u65f6\u4eba\u4f53\u5f62\u72b6\u7f16\u8f91\uff0c\u91cd\u5efa\u8bef\u5dee\u4ece13.6mm\u964d\u4f4e\u52307.5mm", "motivation": "\u4eba\u4f53\u5f62\u72b6\u7f16\u8f91\u9886\u57df\u7f3a\u4e4f\u5927\u89c4\u6a21\u516c\u5f00\u6570\u636e\u96c6\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4f53\u6bd4\u4e0d\u534f\u8c03\u3001\u7eb9\u7406\u626d\u66f2\u548c\u80cc\u666f\u4e0d\u4e00\u81f4\u7b49\u95ee\u9898", "method": "\u6784\u5efa18,573\u5f20\u56fe\u7247\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u63d0\u51faOdo\u7aef\u5230\u7aef\u6ef4\u6e0f\u65b9\u6cd5\uff0c\u7ed3\u5408\u51bb\u7ed3UNet\u4fdd\u4fdd\u5916\u89c2\u7ec6\u8282\u548cControlNet\u901a\u8fc7SMPL\u6df1\u5ea6\u5730\u56fe\u5bfc\u822a\u5f62\u72b6\u53d8\u6362", "result": "\u5728\u91cd\u5efa\u8bef\u5dee\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4ece13.6mm\u964d\u4f4e\u52307.5mm\uff0c\u751f\u6210\u7684\u7ed3\u679c\u66f4\u52a0\u5b9e\u9645\u4e14\u51c6\u786e\u5339\u914d\u76ee\u6807\u5f62\u72b6", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4eba\u4f53\u5f62\u72b6\u7f16\u8f91\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u521b\u65b0\u7684\u6a21\u578b\u7ed3\u6784\u5b9e\u73b0\u4e86\u66f4\u52a0\u5b9e\u9645\u7684\u5f62\u72b6\u7f16\u8f91\u6548\u679c"}}
{"id": "2508.13078", "pdf": "https://arxiv.org/pdf/2508.13078", "abs": "https://arxiv.org/abs/2508.13078", "authors": ["Qingwen Zeng", "Juan E. Tapia", "Izan Garcia", "Juan M. Espin", "Christoph Busch"], "title": "ID-Card Synthetic Generation: Toward a Simulated Bona fide Dataset", "categories": ["cs.CV"], "comment": null, "summary": "Nowadays, the development of a Presentation Attack Detection (PAD) system for ID cards presents a challenge due to the lack of images available to train a robust PAD system and the increase in diversity of possible attack instrument species. Today, most algorithms focus on generating attack samples and do not take into account the limited number of bona fide images. This work is one of the first to propose a method for mimicking bona fide images by generating synthetic versions of them using Stable Diffusion, which may help improve the generalisation capabilities of the detector. Furthermore, the new images generated are evaluated in a system trained from scratch and in a commercial solution. The PAD system yields an interesting result, as it identifies our images as bona fide, which has a positive impact on detection performance and data restrictions.", "AI": {"tldr": "\u4f7f\u7528Stable Diffusion\u751f\u6210\u5408\u6210\u771f\u5b9eID\u5361\u56fe\u50cf\u6765\u89e3\u51b3\u5b9e\u9645\u771f\u5b9e\u6837\u672c\u7f3a\u4e4f\u95ee\u9898\uff0c\u63d0\u5347\u963b\u6b62\u653b\u51fb\u68c0\u6d4b\u7cfb\u7edf\u7684\u6027\u80fd", "motivation": "ID\u5361\u963b\u6b62\u653b\u51fb\u68c0\u6d4b(PAD)\u7cfb\u7edf\u9762\u4e34\u771f\u5b9e\u6837\u672c\u6570\u91cf\u6709\u9650\u548c\u653b\u51fb\u624b\u6bb5\u591a\u6837\u5316\u7684\u6311\u6218\uff0c\u5f53\u524d\u7b97\u6cd5\u591a\u5173\u6ce8\u653b\u51fb\u6837\u672c\u751f\u6210\u800c\u5ffd\u89c6\u4e86\u771f\u5b9e\u56fe\u50cf\u7684\u7f3a\u4e4f\u95ee\u9898", "method": "\u63d0\u51fa\u4f7f\u7528Stable Diffusion\u751f\u6210\u5408\u6210\u771f\u5b9eID\u5361\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u4ece\u5934\u8bad\u7ec3\u7684\u7cfb\u7edf\u548c\u5546\u4e1a\u89e3\u51b3\u65b9\u6848\u4e2d\u8bc4\u4f30\u8fd9\u4e9b\u65b0\u56fe\u50cf", "result": "\u751f\u6210\u7684\u5408\u6210\u56fe\u50cf\u88abPAD\u7cfb\u7edf\u8bc6\u522b\u4e3a\u771f\u5b9e\u56fe\u50cf\uff0c\u5bf9\u68c0\u6d4b\u6027\u80fd\u548c\u6570\u636e\u9650\u5236\u95ee\u9898\u4ea7\u751f\u4e86\u79ef\u6781\u5f71\u54cd", "conclusion": "\u8fd9\u662f\u9996\u6b21\u63d0\u51fa\u4f7f\u7528\u751f\u6210\u5f0fAI\u6280\u672f\u6765\u6a21\u4eff\u771f\u5b9eID\u5361\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8PAD\u7cfb\u7edf\u7684\u901a\u7528\u6027\u80fd\u529b"}}
{"id": "2508.13091", "pdf": "https://arxiv.org/pdf/2508.13091", "abs": "https://arxiv.org/abs/2508.13091", "authors": ["Zihua Liu", "Yizhou Li", "Songyan Zhang", "Masatoshi Okutomi"], "title": "DMS:Diffusion-Based Multi-Baseline Stereo Generation for Improving Self-Supervised Depth Estimation", "categories": ["cs.CV"], "comment": null, "summary": "While supervised stereo matching and monocular depth estimation have advanced significantly with learning-based algorithms, self-supervised methods using stereo images as supervision signals have received relatively less focus and require further investigation. A primary challenge arises from ambiguity introduced during photometric reconstruction, particularly due to missing corresponding pixels in ill-posed regions of the target view, such as occlusions and out-of-frame areas. To address this and establish explicit photometric correspondences, we propose DMS, a model-agnostic approach that utilizes geometric priors from diffusion models to synthesize novel views along the epipolar direction, guided by directional prompts. Specifically, we finetune a Stable Diffusion model to simulate perspectives at key positions: left-left view shifted from the left camera, right-right view shifted from the right camera, along with an additional novel view between the left and right cameras. These synthesized views supplement occluded pixels, enabling explicit photometric reconstruction. Our proposed DMS is a cost-free, ''plug-and-play'' method that seamlessly enhances self-supervised stereo matching and monocular depth estimation, and relies solely on unlabeled stereo image pairs for both training and synthesizing. Extensive experiments demonstrate the effectiveness of our approach, with up to 35% outlier reduction and state-of-the-art performance across multiple benchmark datasets.", "AI": {"tldr": "DMS\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u51e0\u4f55\u5148\u9a8c\u5408\u6210\u6cbf\u6781\u7ebf\u65b9\u5411\u7684\u65b0\u89c6\u89d2\uff0c\u901a\u8fc7\u65b9\u5411\u63d0\u793a\u6765\u8865\u5145\u906e\u6321\u50cf\u7d20\uff0c\u63d0\u5347\u81ea\u76d1\u7763\u7acb\u4f53\u5339\u914d\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6027\u80fd\u3002", "motivation": "\u81ea\u76d1\u7763\u65b9\u6cd5\u4f7f\u7528\u7acb\u4f53\u56fe\u50cf\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\u7684\u7814\u7a76\u76f8\u5bf9\u8f83\u5c11\uff0c\u4e3b\u8981\u6311\u6218\u6765\u81ea\u5149\u5ea6\u91cd\u5efa\u4e2d\u7684\u6a21\u7cca\u6027\uff0c\u7279\u522b\u662f\u5728\u76ee\u6807\u89c6\u89d2\u7684\u906e\u6321\u533a\u57df\u548c\u51fa\u6846\u533a\u57df\u4e2d\u5bf9\u5e94\u50cf\u7d20\u7f3a\u5931\u7684\u95ee\u9898\u3002", "method": "\u5fae\u8c03Stable Diffusion\u6a21\u578b\u6765\u6a21\u62df\u5173\u952e\u4f4d\u7f6e\u7684\u89c6\u89d2\uff1a\u4ece\u5de6\u76f8\u673a\u504f\u79fb\u7684\u5de6-\u5de6\u89c6\u89d2\u3001\u4ece\u53f3\u76f8\u673a\u504f\u79fb\u7684\u53f3-\u53f3\u89c6\u89d2\uff0c\u4ee5\u53ca\u5de6\u53f3\u76f8\u673a\u4e4b\u95f4\u7684\u989d\u5916\u65b0\u89c6\u89d2\u3002\u8fd9\u4e9b\u5408\u6210\u89c6\u89d2\u8865\u5145\u906e\u6321\u50cf\u7d20\uff0c\u5b9e\u73b0\u663e\u5f0f\u5149\u5ea6\u91cd\u5efa\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5f02\u5e38\u503c\u51cf\u5c11\u9ad8\u8fbe35%\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "DMS\u662f\u4e00\u4e2a\u65e0\u9700\u6210\u672c\u7684\u5373\u63d2\u5373\u7528\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u672a\u6807\u8bb0\u7684\u7acb\u4f53\u56fe\u50cf\u5bf9\u8fdb\u884c\u8bad\u7ec3\u548c\u5408\u6210\uff0c\u80fd\u65e0\u7f1d\u589e\u5f3a\u81ea\u76d1\u7763\u7acb\u4f53\u5339\u914d\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u3002"}}
{"id": "2508.13104", "pdf": "https://arxiv.org/pdf/2508.13104", "abs": "https://arxiv.org/abs/2508.13104", "authors": ["Yuang Wang", "Chao Wen", "Haoyu Guo", "Sida Peng", "Minghan Qin", "Hujun Bao", "Xiaowei Zhou", "Ruizhen Hu"], "title": "Precise Action-to-Video Generation Through Visual Action Prompts", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted to ICCV 2025. Project page: https://zju3dv.github.io/VAP/", "summary": "We present visual action prompts, a unified action representation for action-to-video generation of complex high-DoF interactions while maintaining transferable visual dynamics across domains. Action-driven video generation faces a precision-generality trade-off: existing methods using text, primitive actions, or coarse masks offer generality but lack precision, while agent-centric action signals provide precision at the cost of cross-domain transferability. To balance action precision and dynamic transferability, we propose to \"render\" actions into precise visual prompts as domain-agnostic representations that preserve both geometric precision and cross-domain adaptability for complex actions; specifically, we choose visual skeletons for their generality and accessibility. We propose robust pipelines to construct skeletons from two interaction-rich data sources - human-object interactions (HOI) and dexterous robotic manipulation - enabling cross-domain training of action-driven generative models. By integrating visual skeletons into pretrained video generation models via lightweight fine-tuning, we enable precise action control of complex interaction while preserving the learning of cross-domain dynamics. Experiments on EgoVid, RT-1 and DROID demonstrate the effectiveness of our proposed approach. Project page: https://zju3dv.github.io/VAP/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u89c6\u89c9\u52a8\u4f5c\u63d0\u793a\uff08visual action prompts\uff09\uff0c\u4e00\u79cd\u7edf\u4e00\u7684\u52a8\u4f5c\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u590d\u6742\u9ad8\u81ea\u7531\u5ea6\u4ea4\u4e92\u7684\u52a8\u4f5c\u5230\u89c6\u9891\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u8de8\u9886\u57df\u7684\u53ef\u8f6c\u79fb\u89c6\u89c9\u52a8\u6001\u3002", "motivation": "\u73b0\u6709\u7684\u52a8\u4f5c\u9a71\u52a8\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u9762\u4e34\u7cbe\u5ea6\u4e0e\u901a\u7528\u6027\u7684\u6743\u8861\uff1a\u6587\u672c\u3001\u539f\u59cb\u52a8\u4f5c\u6216\u7c97\u7cd9\u63a9\u7801\u65b9\u6cd5\u5177\u6709\u901a\u7528\u6027\u4f46\u7f3a\u4e4f\u7cbe\u5ea6\uff0c\u800c\u667a\u80fd\u4f53\u4e2d\u5fc3\u52a8\u4f5c\u4fe1\u53f7\u63d0\u4f9b\u7cbe\u5ea6\u4f46\u7f3a\u4e4f\u8de8\u9886\u57df\u53ef\u8f6c\u79fb\u6027\u3002", "method": "\u5c06\u52a8\u4f5c\"\u6e32\u67d3\"\u4e3a\u7cbe\u786e\u7684\u89c6\u89c9\u63d0\u793a\u4f5c\u4e3a\u9886\u57df\u65e0\u5173\u8868\u793a\uff0c\u9009\u62e9\u89c6\u89c9\u9aa8\u67b6\u4f5c\u4e3a\u901a\u7528\u4e14\u6613\u83b7\u53d6\u7684\u8868\u793a\u5f62\u5f0f\u3002\u6784\u5efa\u4ece\u4eba-\u7269\u4ea4\u4e92\u548c\u7075\u5de7\u673a\u5668\u4eba\u64cd\u4f5c\u6570\u636e\u4e2d\u63d0\u53d6\u9aa8\u67b6\u7684\u9c81\u68d2\u6d41\u7a0b\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5fae\u8c03\u5c06\u89c6\u89c9\u9aa8\u67b6\u96c6\u6210\u5230\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e2d\u3002", "result": "\u5728EgoVid\u3001RT-1\u548cDROID\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u89c6\u89c9\u52a8\u4f5c\u63d0\u793a\u80fd\u591f\u5728\u4fdd\u6301\u590d\u6742\u4ea4\u4e92\u7cbe\u786e\u52a8\u4f5c\u63a7\u5236\u7684\u540c\u65f6\uff0c\u4fdd\u7559\u8de8\u9886\u57df\u52a8\u6001\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u5e73\u8861\u4e86\u52a8\u4f5c\u7cbe\u5ea6\u548c\u52a8\u6001\u53ef\u8f6c\u79fb\u6027\u3002"}}
{"id": "2508.13153", "pdf": "https://arxiv.org/pdf/2508.13153", "abs": "https://arxiv.org/abs/2508.13153", "authors": ["Wenhao Hu", "Zesheng Li", "Haonan Zhou", "Liu Liu", "Xuexiang Wen", "Zhizhong Su", "Xi Li", "Gaoang Wang"], "title": "IGFuse: Interactive 3D Gaussian Scene Reconstruction via Multi-Scans Fusion", "categories": ["cs.CV"], "comment": "Project page: https://whhu7.github.io/IGFuse", "summary": "Reconstructing complete and interactive 3D scenes remains a fundamental challenge in computer vision and robotics, particularly due to persistent object occlusions and limited sensor coverage. Multiview observations from a single scene scan often fail to capture the full structural details. Existing approaches typically rely on multi stage pipelines, such as segmentation, background completion, and inpainting or require per-object dense scanning, both of which are error-prone, and not easily scalable. We propose IGFuse, a novel framework that reconstructs interactive Gaussian scene by fusing observations from multiple scans, where natural object rearrangement between captures reveal previously occluded regions. Our method constructs segmentation aware Gaussian fields and enforces bi-directional photometric and semantic consistency across scans. To handle spatial misalignments, we introduce a pseudo-intermediate scene state for unified alignment, alongside collaborative co-pruning strategies to refine geometry. IGFuse enables high fidelity rendering and object level scene manipulation without dense observations or complex pipelines. Extensive experiments validate the framework's strong generalization to novel scene configurations, demonstrating its effectiveness for real world 3D reconstruction and real-to-simulation transfer. Our project page is available online.", "AI": {"tldr": "IGFuse\u662f\u4e00\u4e2a\u65b0\u9896\u76843D\u573a\u666f\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u591a\u89c6\u89d2\u626b\u63cf\u6570\u636e\u6765\u91cd\u5efa\u4ea4\u4e92\u5f0f\u9ad8\u65af\u573a\u666f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u56e0\u906e\u6321\u548c\u4f20\u611f\u5668\u9650\u5236\u5bfc\u81f4\u7684\u5b8c\u6574\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u76843D\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\u9762\u4e34\u7269\u4f53\u906e\u6321\u548c\u4f20\u611f\u5668\u8986\u76d6\u6709\u9650\u7684\u95ee\u9898\uff0c\u591a\u9636\u6bb5\u5904\u7406\u6d41\u7a0b\u5bb9\u6613\u51fa\u9519\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u626b\u63cf\u6570\u636e\u5e76\u63ed\u793a\u88ab\u906e\u6321\u533a\u57df\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faIGFuse\u6846\u67b6\uff0c\u6784\u5efa\u5206\u5272\u611f\u77e5\u7684\u9ad8\u65af\u573a\uff0c\u901a\u8fc7\u53cc\u5411\u5149\u5ea6\u4e00\u81f4\u6027\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u7ea6\u675f\u591a\u626b\u63cf\u6570\u636e\uff0c\u5f15\u5165\u4f2a\u4e2d\u95f4\u573a\u666f\u72b6\u6001\u5904\u7406\u7a7a\u95f4\u9519\u4f4d\uff0c\u91c7\u7528\u534f\u4f5c\u5171\u526a\u679d\u7b56\u7565\u4f18\u5316\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\u8be5\u65b9\u6cd5\u5bf9\u65b0\u573a\u666f\u914d\u7f6e\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u4fdd\u771f\u6e32\u67d3\u548c\u5bf9\u8c61\u7ea7\u573a\u666f\u64cd\u4f5c\uff0c\u65e0\u9700\u5bc6\u96c6\u89c2\u6d4b\u6216\u590d\u6742\u5904\u7406\u6d41\u7a0b\u3002", "conclusion": "IGFuse\u4e3a\u771f\u5b9e\u4e16\u754c3D\u91cd\u5efa\u548c\u771f\u5b9e\u5230\u4eff\u771f\u7684\u8f6c\u6362\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u591a\u626b\u63cf\u878d\u5408\u6210\u529f\u89e3\u51b3\u4e86\u906e\u6321\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5b8c\u6574\u7684\u4ea4\u4e92\u5f0f3D\u573a\u666f\u91cd\u5efa\u3002"}}
{"id": "2508.13154", "pdf": "https://arxiv.org/pdf/2508.13154", "abs": "https://arxiv.org/abs/2508.13154", "authors": ["Zhaoxi Chen", "Tianqi Liu", "Long Zhuo", "Jiawei Ren", "Zeng Tao", "He Zhu", "Fangzhou Hong", "Liang Pan", "Ziwei Liu"], "title": "4DNeX: Feed-Forward 4D Generative Modeling Made Easy", "categories": ["cs.CV"], "comment": "Project Page: https://4dnex.github.io/", "summary": "We present 4DNeX, the first feed-forward framework for generating 4D (i.e., dynamic 3D) scene representations from a single image. In contrast to existing methods that rely on computationally intensive optimization or require multi-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D generation by fine-tuning a pretrained video diffusion model. Specifically, 1) to alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale dataset with high-quality 4D annotations generated using advanced reconstruction approaches. 2) we introduce a unified 6D video representation that jointly models RGB and XYZ sequences, facilitating structured learning of both appearance and geometry. 3) we propose a set of simple yet effective adaptation strategies to repurpose pretrained video diffusion models for 4D modeling. 4DNeX produces high-quality dynamic point clouds that enable novel-view video synthesis. Extensive experiments demonstrate that 4DNeX outperforms existing 4D generation methods in efficiency and generalizability, offering a scalable solution for image-to-4D modeling and laying the foundation for generative 4D world models that simulate dynamic scene evolution.", "AI": {"tldr": "4DNeX\u662f\u9996\u4e2a\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u62104D\uff08\u52a8\u60013D\uff09\u573a\u666f\u8868\u793a\u7684\u7aef\u5230\u7aef\u524d\u9988\u6846\u67b6\uff0c\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u56fe\u50cf\u52304D\u751f\u6210", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8ba1\u7b97\u5bc6\u96c6\u578b\u4f18\u5316\u6216\u9700\u8981\u591a\u5e27\u89c6\u9891\u8f93\u5165\u7684\u95ee\u9898\uff0c\u4e3a\u751f\u6210\u5f0f4D\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848", "method": "1)\u6784\u5efa\u5927\u89c4\u6a214D\u6570\u636e\u96c64DNeX-10M\uff1b2)\u5f15\u5165\u7edf\u4e006D\u89c6\u9891\u8868\u793a\u8054\u5408\u5efa\u6a21RGB\u548cXYZ\u5e8f\u5217\uff1b3)\u63d0\u51fa\u9002\u914d\u7b56\u7565\u5c06\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u7528\u4e8e4D\u5efa\u6a21", "result": "\u751f\u6210\u9ad8\u8d28\u91cf\u52a8\u6001\u70b9\u4e91\uff0c\u652f\u6301\u65b0\u9896\u89c6\u89d2\u89c6\u9891\u5408\u6210\uff0c\u5728\u6548\u7387\u548c\u6cdb\u5316\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u67094D\u751f\u6210\u65b9\u6cd5", "conclusion": "4DNeX\u4e3a\u56fe\u50cf\u52304D\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u6a21\u62df\u52a8\u6001\u573a\u666f\u6f14\u5316\u7684\u751f\u6210\u5f0f4D\u4e16\u754c\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840"}}
{"id": "2508.11673", "pdf": "https://arxiv.org/pdf/2508.11673", "abs": "https://arxiv.org/abs/2508.11673", "authors": ["Haojie Zhang", "Yixiong Liang", "Hulin Kuang", "Lihui Cen", "Zhe Qu", "Yigang Cen", "Min Zeng", "Shichao Kan"], "title": "Contrastive Regularization over LoRA for Multimodal Biomedical Image Incremental Learning", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MM"], "comment": "10 pages, 3 figures, submitted to ACM Multimedia 2025", "summary": "Multimodal Biomedical Image Incremental Learning (MBIIL) is essential for handling diverse tasks and modalities in the biomedical domain, as training separate models for each modality or task significantly increases inference costs. Existing incremental learning methods focus on task expansion within a single modality, whereas MBIIL seeks to train a unified model incrementally across modalities. The MBIIL faces two challenges: I) How to preserve previously learned knowledge during incremental updates? II) How to effectively leverage knowledge acquired from existing modalities to support new modalities? To address these challenges, we propose MSLoRA-CR, a method that fine-tunes Modality-Specific LoRA modules while incorporating Contrastive Regularization to enhance intra-modality knowledge sharing and promote inter-modality knowledge differentiation. Our approach builds upon a large vision-language model (LVLM), keeping the pretrained model frozen while incrementally adapting new LoRA modules for each modality or task. Experiments on the incremental learning of biomedical images demonstrate that MSLoRA-CR outperforms both the state-of-the-art (SOTA) approach of training separate models for each modality and the general incremental learning method (incrementally fine-tuning LoRA). Specifically, MSLoRA-CR achieves a 1.88% improvement in overall performance compared to unconstrained incremental learning methods while maintaining computational efficiency. Our code is publicly available at https://github.com/VentusAislant/MSLoRA_CR.", "AI": {"tldr": "\u63d0\u51faMSLoRA-CR\u65b9\u6cd5\u89e3\u51b3\u591a\u6a21\u6001\u751f\u7269\u533b\u5b66\u56fe\u50cf\u589e\u91cf\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u6001\u7279\u5b9aLoRA\u6a21\u5757\u548c\u5bf9\u6bd4\u6b63\u5219\u5316\u5b9e\u73b0\u77e5\u8bc6\u4fdd\u6301\u4e0e\u5171\u4eab\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u4e0b\u6027\u80fd\u63d0\u53471.88%", "motivation": "\u751f\u7269\u533b\u5b66\u9886\u57df\u9700\u8981\u5904\u7406\u591a\u79cd\u6a21\u6001\u548c\u4efb\u52a1\uff0c\u4e3a\u6bcf\u4e2a\u6a21\u6001\u5355\u72ec\u8bad\u7ec3\u6a21\u578b\u4f1a\u663e\u8457\u589e\u52a0\u63a8\u7406\u6210\u672c\uff0c\u800c\u73b0\u6709\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u6a21\u6001\u4efb\u52a1\u6269\u5c55", "method": "\u57fa\u4e8e\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u51bb\u7ed3\u9884\u8bad\u7ec3\u53c2\u6570\uff0c\u4e3a\u6bcf\u4e2a\u6a21\u6001\u589e\u91cf\u9002\u914d\u7279\u5b9aLoRA\u6a21\u5757\uff0c\u5e76\u5f15\u5165\u5bf9\u6bd4\u6b63\u5219\u5316\u4fc3\u8fdb\u6a21\u6001\u5185\u77e5\u8bc6\u5171\u4eab\u548c\u6a21\u6001\u95f4\u77e5\u8bc6\u533a\u5206", "result": "\u5728\u751f\u7269\u533b\u5b66\u56fe\u50cf\u589e\u91cf\u5b66\u4e60\u5b9e\u9a8c\u4e2d\uff0cMSLoRA-CR\u4f18\u4e8e\u4e3a\u6bcf\u4e2a\u6a21\u6001\u5355\u72ec\u8bad\u7ec3\u6a21\u578b\u7684\u65b9\u6cd5\u548c\u901a\u7528\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\uff0c\u6574\u4f53\u6027\u80fd\u63d0\u53471.88%", "conclusion": "MSLoRA-CR\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u751f\u7269\u533b\u5b66\u56fe\u50cf\u589e\u91cf\u5b66\u4e60\u7684\u4e24\u4e2a\u6838\u5fc3\u6311\u6218\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd"}}
{"id": "2508.12986", "pdf": "https://arxiv.org/pdf/2508.12986", "abs": "https://arxiv.org/abs/2508.12986", "authors": ["Jinyi Liu", "Guoyang Zhao", "Lijun Liu", "Yiguang Hong", "Weiping Zhang", "Shuming Cheng"], "title": "Point upsampling networks for single-photon sensing", "categories": ["physics.optics", "cs.CV"], "comment": "13 pages, 8 figures, any comments are welcome", "summary": "Single-photon sensing has generated great interest as a prominent technique of long-distance and ultra-sensitive imaging, however, it tends to yield sparse and spatially biased point clouds, thus limiting its practical utility. In this work, we propose using point upsampling networks to increase point density and reduce spatial distortion in single-photon point cloud. Particularly, our network is built on the state space model which integrates a multi-path scanning mechanism to enrich spatial context, a bidirectional Mamba backbone to capture global geometry and local details, and an adaptive upsample shift module to correct offset-induced distortions. Extensive experiments are implemented on commonly-used datasets to confirm its high reconstruction accuracy and strong robustness to the distortion noise, and also on real-world data to demonstrate that our model is able to generate visually consistent, detail-preserving, and noise suppressed point clouds. Our work is the first to establish the upsampling framework for single-photon sensing, and hence opens a new avenue for single-photon sensing and its practical applications in the downstreaming tasks.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
