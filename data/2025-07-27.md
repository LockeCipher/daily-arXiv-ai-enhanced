<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 3]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Zero-Shot Dynamic Concept Personalization with Grid-Based LoRA](https://arxiv.org/abs/2507.17963)
*Rameen Abdal,Or Patashnik,Ekaterina Deyneka,Hao Chen,Aliaksandr Siarohin,Sergey Tulyakov,Daniel Cohen-Or,Kfir Aberman*

Main category: cs.GR

TL;DR: 提出了一种零样本框架，用于文本到视频模型中的动态概念个性化，无需实例微调，通过结构化视频网格和轻量级Grid-LoRA适配器实现高效编辑和合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要针对每个实例进行微调，限制了可扩展性，因此需要一种无需微调即可实现动态概念个性化的方法。

Method: 利用结构化2x2视频网格组织输入输出对，训练轻量级Grid-LoRA适配器，并通过Grid Fill模块在推理时完成部分观察的布局。

Result: 实验表明，该方法在未见过的动态概念和编辑场景中均能产生高质量且一致的结果。

Conclusion: 该方法通过单次前向传播实现动态概念个性化，具有高效性和泛化能力。

Abstract: Recent advances in text-to-video generation have enabled high-quality synthesis from text and image prompts. While the personalization of dynamic concepts, which capture subject-specific appearance and motion from a single video, is now feasible, most existing methods require per-instance fine-tuning, limiting scalability. We introduce a fully zero-shot framework for dynamic concept personalization in text-to-video models. Our method leverages structured 2x2 video grids that spatially organize input and output pairs, enabling the training of lightweight Grid-LoRA adapters for editing and composition within these grids. At inference, a dedicated Grid Fill module completes partially observed layouts, producing temporally coherent and identity preserving outputs. Once trained, the entire system operates in a single forward pass, generalizing to previously unseen dynamic concepts without any test-time optimization. Extensive experiments demonstrate high-quality and consistent results across a wide range of subjects beyond trained concepts and editing scenarios.

</details>


### [2] [GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar](https://arxiv.org/abs/2507.18155)
*SeungJun Moon,Hah Min Lew,Seungeun Lee,Ji-Su Kang,Gyeong-Moon Park*

Main category: cs.GR

TL;DR: GeoAvatar提出了一种自适应几何高斯泼溅框架，通过APS分割高斯为刚性和柔性集，结合嘴部结构和部分变形策略，提升动画质量，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决3D头部化身生成中身份保持与新颖姿势和表情动画之间的平衡问题。

Method: 使用自适应预分配阶段（APS）分割高斯，引入嘴部结构和部分变形策略，提出正则化损失。

Result: 在重建和新动画场景中优于现有方法。

Conclusion: GeoAvatar通过自适应几何高斯泼溅和嘴部优化，显著提升了3D头部化身的生成质量。

Abstract: Despite recent progress in 3D head avatar generation, balancing identity preservation, i.e., reconstruction, with novel poses and expressions, i.e., animation, remains a challenge. Existing methods struggle to adapt Gaussians to varying geometrical deviations across facial regions, resulting in suboptimal quality. To address this, we propose GeoAvatar, a framework for adaptive geometrical Gaussian Splatting. GeoAvatar leverages Adaptive Pre-allocation Stage (APS), an unsupervised method that segments Gaussians into rigid and flexible sets for adaptive offset regularization. Then, based on mouth anatomy and dynamics, we introduce a novel mouth structure and the part-wise deformation strategy to enhance the animation fidelity of the mouth. Finally, we propose a regularization loss for precise rigging between Gaussians and 3DMM faces. Moreover, we release DynamicFace, a video dataset with highly expressive facial motions. Extensive experiments show the superiority of GeoAvatar compared to state-of-the-art methods in reconstruction and novel animation scenarios.

</details>


### [3] [PS-GS: Gaussian Splatting for Multi-View Photometric Stereo](https://arxiv.org/abs/2507.18231)
*Yixiao Chen,Bin Liang,Hanzhi Guo,Yongqing Cheng,Jiayi Zhao,Dongdong Weng*

Main category: cs.GR

TL;DR: 提出了一种结合高斯点云与多视角光度立体（MVPS）的高效逆渲染方法PS-GS，显著提升了3D重建的准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有逆渲染方法依赖固定光照环境，而结合MVPS虽能提升精度，但效率仍不足。

Method: 基于2D高斯点云初始化几何模型，通过延迟逆渲染和光照计算多层感知机联合优化几何、材质和光照，并利用未标定光度立体法估计的法线图进行正则化。

Result: 实验表明，该方法在合成和真实数据集上均优于现有方法，重建精度和计算效率更高。

Conclusion: PS-GS为逆渲染提供了一种高效解决方案，支持新视角合成、重光照及材质形状编辑。

Abstract: Integrating inverse rendering with multi-view photometric stereo (MVPS) yields more accurate 3D reconstructions than the inverse rendering approaches that rely on fixed environment illumination. However, efficient inverse rendering with MVPS remains challenging. To fill this gap, we introduce the Gaussian Splatting for Multi-view Photometric Stereo (PS-GS), which efficiently and jointly estimates the geometry, materials, and lighting of the object that is illuminated by diverse directional lights (multi-light). Our method first reconstructs a standard 2D Gaussian splatting model as the initial geometry. Based on the initialization model, it then proceeds with the deferred inverse rendering by the full rendering equation containing a lighting-computing multi-layer perceptron. During the whole optimization, we regularize the rendered normal maps by the uncalibrated photometric stereo estimated normals. We also propose the 2D Gaussian ray-tracing for single directional light to refine the incident lighting. The regularizations and the use of multi-view and multi-light images mitigate the ill-posed problem of inverse rendering. After optimization, the reconstructed object can be used for novel-view synthesis, relighting, and material and shape editing. Experiments on both synthetic and real datasets demonstrate that our method outperforms prior works in terms of reconstruction accuracy and computational efficiency.

</details>
