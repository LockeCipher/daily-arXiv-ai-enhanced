{"id": "2508.05755", "pdf": "https://arxiv.org/pdf/2508.05755", "abs": "https://arxiv.org/abs/2508.05755", "authors": ["Agnieszka Polowczyk", "Alicja Polowczyk", "Dawid Malarz", "Artur Kasymov", "Marcin Mazur", "Jacek Tabor", "Przemys\u0142aw Spurek"], "title": "UnGuide: Learning to Forget with LoRA-Guided Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in large-scale text-to-image diffusion models have heightened concerns about their potential misuse, especially in generating harmful or misleading content. This underscores the urgent need for effective machine unlearning, i.e., removing specific knowledge or concepts from pretrained models without compromising overall performance. One possible approach is Low-Rank Adaptation (LoRA), which offers an efficient means to fine-tune models for targeted unlearning. However, LoRA often inadvertently alters unrelated content, leading to diminished image fidelity and realism. To address this limitation, we introduce UnGuide -- a novel approach which incorporates UnGuidance, a dynamic inference mechanism that leverages Classifier-Free Guidance (CFG) to exert precise control over the unlearning process. UnGuide modulates the guidance scale based on the stability of a few first steps of denoising processes, enabling selective unlearning by LoRA adapter. For prompts containing the erased concept, the LoRA module predominates and is counterbalanced by the base model; for unrelated prompts, the base model governs generation, preserving content fidelity. Empirical results demonstrate that UnGuide achieves controlled concept removal and retains the expressive power of diffusion models, outperforming existing LoRA-based methods in both object erasure and explicit content removal tasks.", "AI": {"tldr": "UnGuide\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u63a8\u7406\u673a\u5236UnGuidance\u7ed3\u5408LoRA\uff0c\u5b9e\u73b0\u5bf9\u6269\u6563\u6a21\u578b\u4e2d\u7279\u5b9a\u77e5\u8bc6\u7684\u7cbe\u786e\u53bb\u9664\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u53ef\u80fd\u88ab\u6ee5\u7528\u751f\u6210\u6709\u5bb3\u5185\u5bb9\uff0c\u56e0\u6b64\u9700\u8981\u6709\u6548\u7684\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\uff0c\u53bb\u9664\u7279\u5b9a\u77e5\u8bc6\u800c\u4e0d\u5f71\u54cd\u6574\u4f53\u6027\u80fd\u3002", "method": "UnGuide\u7ed3\u5408LoRA\u548cClassifier-Free Guidance\uff08CFG\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5f15\u5bfc\u5c3a\u5ea6\uff0c\u9009\u62e9\u6027\u53bb\u9664\u77e5\u8bc6\u3002LoRA\u6a21\u5757\u4e3b\u5bfc\u76ee\u6807\u6982\u5ff5\u53bb\u9664\uff0c\u57fa\u7840\u6a21\u578b\u4fdd\u6301\u65e0\u5173\u5185\u5bb9\u751f\u6210\u3002", "result": "UnGuide\u5728\u5bf9\u8c61\u64e6\u9664\u548c\u663e\u5f0f\u5185\u5bb9\u53bb\u9664\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709LoRA\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u53ef\u63a7\u6982\u5ff5\u53bb\u9664\u5e76\u4fdd\u7559\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u3002", "conclusion": "UnGuide\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86LoRA\u5728\u65e0\u5173\u5185\u5bb9\u4e0a\u7684\u526f\u4f5c\u7528\uff0c\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2508.05769", "pdf": "https://arxiv.org/pdf/2508.05769", "abs": "https://arxiv.org/abs/2508.05769", "authors": ["Seyed Hadi Seyed", "Ayberk Cansever", "David Hart"], "title": "Improving Masked Style Transfer using Blended Partial Convolution", "categories": ["cs.CV"], "comment": null, "summary": "Artistic style transfer has long been possible with the advancements of convolution- and transformer-based neural networks. Most algorithms apply the artistic style transfer to the whole image, but individual users may only need to apply a style transfer to a specific region in the image. The standard practice is to simply mask the image after the stylization. This work shows that this approach tends to improperly capture the style features in the region of interest. We propose a partial-convolution-based style transfer network that accurately applies the style features exclusively to the region of interest. Additionally, we present network-internal blending techniques that account for imperfections in the region selection. We show that this visually and quantitatively improves stylization using examples from the SA-1B dataset. Code is publicly available at https://github.com/davidmhart/StyleTransferMasked.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u90e8\u5206\u5377\u79ef\u7684\u98ce\u683c\u8fc1\u79fb\u7f51\u7edc\uff0c\u4e13\u6ce8\u4e8e\u5bf9\u56fe\u50cf\u7279\u5b9a\u533a\u57df\u8fdb\u884c\u98ce\u683c\u8fc1\u79fb\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u533a\u57df\u9009\u62e9\u4e0a\u7684\u7f3a\u9677\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5bf9\u6574\u4e2a\u56fe\u50cf\u8fdb\u884c\u98ce\u683c\u8fc1\u79fb\u540e\u901a\u8fc7\u63a9\u7801\u5904\u7406\uff0c\u4f46\u4f1a\u5bfc\u81f4\u611f\u5174\u8da3\u533a\u57df\u7684\u98ce\u683c\u7279\u5f81\u6355\u6349\u4e0d\u51c6\u786e\u3002", "method": "\u91c7\u7528\u90e8\u5206\u5377\u79ef\u7f51\u7edc\uff0c\u7ed3\u5408\u5185\u90e8\u6df7\u5408\u6280\u672f\uff0c\u7cbe\u786e\u5730\u5c06\u98ce\u683c\u7279\u5f81\u5e94\u7528\u4e8e\u611f\u5174\u8da3\u533a\u57df\u3002", "result": "\u5728SA-1B\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u89c6\u89c9\u548c\u91cf\u5316\u4e0a\u7684\u6539\u8fdb\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5c40\u90e8\u98ce\u683c\u8fc1\u79fb\u7684\u51c6\u786e\u6027\u548c\u89c6\u89c9\u6548\u679c\u3002"}}
{"id": "2508.05772", "pdf": "https://arxiv.org/pdf/2508.05772", "abs": "https://arxiv.org/abs/2508.05772", "authors": ["Can Zhao", "Pengfei Guo", "Dong Yang", "Yucheng Tang", "Yufan He", "Benjamin Simon", "Mason Belue", "Stephanie Harmon", "Baris Turkbey", "Daguang Xu"], "title": "MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with Rectified Flow and Region-specific Contrastive Loss", "categories": ["cs.CV"], "comment": null, "summary": "Medical image synthesis is an important topic for both clinical and research applications. Recently, diffusion models have become a leading approach in this area. Despite their strengths, many existing methods struggle with (1) limited generalizability that only work for specific body regions or voxel spacings, (2) slow inference, which is a common issue for diffusion models, and (3) weak alignment with input conditions, which is a critical issue for medical imaging. MAISI, a previously proposed framework, addresses generalizability issues but still suffers from slow inference and limited condition consistency. In this work, we present MAISI-v2, the first accelerated 3D medical image synthesis framework that integrates rectified flow to enable fast and high quality generation. To further enhance condition fidelity, we introduce a novel region-specific contrastive loss to enhance the sensitivity to region of interest. Our experiments show that MAISI-v2 can achieve SOTA image quality with $33 \\times$ acceleration for latent diffusion model. We also conducted a downstream segmentation experiment to show that the synthetic images can be used for data augmentation. We release our code, training details, model weights, and a GUI demo to facilitate reproducibility and promote further development within the community.", "AI": {"tldr": "MAISI-v2\u662f\u4e00\u4e2a\u52a0\u901f\u76843D\u533b\u5b66\u56fe\u50cf\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u6821\u6b63\u6d41\u548c\u533a\u57df\u7279\u5f02\u6027\u5bf9\u6bd4\u635f\u5931\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u901a\u7528\u6027\u3001\u63a8\u7406\u901f\u5ea6\u548c\u6761\u4ef6\u4e00\u81f4\u6027\u4e0a\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\u5728\u901a\u7528\u6027\u3001\u63a8\u7406\u901f\u5ea6\u548c\u8f93\u5165\u6761\u4ef6\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cMAISI-v2\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "MAISI-v2\u6574\u5408\u4e86\u6821\u6b63\u6d41\u4ee5\u52a0\u901f\u751f\u6210\uff0c\u5e76\u5f15\u5165\u533a\u57df\u7279\u5f02\u6027\u5bf9\u6bd4\u635f\u5931\u589e\u5f3a\u6761\u4ef6\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMAISI-v2\u5728\u56fe\u50cf\u8d28\u91cf\u4e0a\u8fbe\u5230SOTA\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u534733\u500d\uff0c\u4e14\u5408\u6210\u56fe\u50cf\u53ef\u7528\u4e8e\u6570\u636e\u589e\u5f3a\u3002", "conclusion": "MAISI-v2\u5728\u533b\u5b66\u56fe\u50cf\u5408\u6210\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u751f\u6210\uff0c\u5e76\u63d0\u4f9b\u4e86\u5f00\u6e90\u8d44\u6e90\u4ee5\u4fc3\u8fdb\u793e\u533a\u53d1\u5c55\u3002"}}
{"id": "2508.05813", "pdf": "https://arxiv.org/pdf/2508.05813", "abs": "https://arxiv.org/abs/2508.05813", "authors": ["Raphael Du Sablon", "David Hart"], "title": "Optimization-Free Style Transfer for 3D Gaussian Splats", "categories": ["cs.CV"], "comment": null, "summary": "The task of style transfer for 3D Gaussian splats has been explored in many previous works, but these require reconstructing or fine-tuning the splat while incorporating style information or optimizing a feature extraction network on the splat representation. We propose a reconstruction- and optimization-free approach to stylizing 3D Gaussian splats. This is done by generating a graph structure across the implicit surface of the splat representation. A feed-forward, surface-based stylization method is then used and interpolated back to the individual splats in the scene. This allows for any style image and 3D Gaussian splat to be used without any additional training or optimization. This also allows for fast stylization of splats, achieving speeds under 2 minutes even on consumer-grade hardware. We demonstrate the quality results this approach achieves and compare to other 3D Gaussian splat style transfer methods. Code is publicly available at https://github.com/davidmhart/FastSplatStyler.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u5efa\u6216\u4f18\u5316\u76843D\u9ad8\u65af\u6837\u6761\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u56fe\u7ed3\u6784\u5e76\u63d2\u503c\u5b9e\u73b0\u5feb\u901f\u98ce\u683c\u5316\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u91cd\u5efa\u6216\u4f18\u5316\u6837\u6761\uff0c\u6548\u7387\u4f4e\u4e14\u9650\u5236\u591a\uff0c\u56e0\u6b64\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u4f18\u5316\u7684\u5feb\u901f\u98ce\u683c\u5316\u65b9\u6cd5\u3002", "method": "\u5728\u6837\u6761\u9690\u5f0f\u8868\u9762\u751f\u6210\u56fe\u7ed3\u6784\uff0c\u4f7f\u7528\u524d\u9988\u8868\u9762\u98ce\u683c\u5316\u65b9\u6cd5\u5e76\u63d2\u503c\u56de\u6837\u6761\u3002", "result": "\u5b9e\u73b0\u4e86\u5feb\u901f\u98ce\u683c\u5316\uff082\u5206\u949f\u5185\uff09\uff0c\u652f\u6301\u4efb\u610f\u98ce\u683c\u56fe\u50cf\u548c3D\u9ad8\u65af\u6837\u6761\uff0c\u6548\u679c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u3001\u7075\u6d3b\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.05819", "pdf": "https://arxiv.org/pdf/2508.05819", "abs": "https://arxiv.org/abs/2508.05819", "authors": ["Jong-Ik Park", "Carlee Joe-Wong", "Gary K. Fedder"], "title": "MZEN: Multi-Zoom Enhanced NeRF for 3-D Reconstruction with Unknown Camera Poses", "categories": ["cs.CV"], "comment": null, "summary": "Neural Radiance Fields (NeRF) methods excel at 3D reconstruction from multiple 2D images, even those taken with unknown camera poses. However, they still miss the fine-detailed structures that matter in industrial inspection, e.g., detecting sub-micron defects on a production line or analyzing chips with Scanning Electron Microscopy (SEM). In these scenarios, the sensor resolution is fixed and compute budgets are tight, so the only way to expose fine structure is to add zoom-in images; yet, this breaks the multi-view consistency that pose-free NeRF training relies on. We propose Multi-Zoom Enhanced NeRF (MZEN), the first NeRF framework that natively handles multi-zoom image sets. MZEN (i) augments the pin-hole camera model with an explicit, learnable zoom scalar that scales the focal length, and (ii) introduces a novel pose strategy: wide-field images are solved first to establish a global metric frame, and zoom-in images are then pose-primed to the nearest wide-field counterpart via a zoom-consistent crop-and-match procedure before joint refinement. Across eight forward-facing scenes$\\unicode{x2013}$synthetic TCAD models, real SEM of micro-structures, and BLEFF objects$\\unicode{x2013}$MZEN consistently outperforms pose-free baselines and even high-resolution variants, boosting PSNR by up to $28 \\%$, SSIM by $10 \\%$, and reducing LPIPS by up to $222 \\%$. MZEN, therefore, extends NeRF to real-world factory settings, preserving global accuracy while capturing the micron-level details essential for industrial inspection.", "AI": {"tldr": "MZEN\u662f\u4e00\u79cd\u6539\u8fdb\u7684NeRF\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u591a\u7f29\u653e\u56fe\u50cf\u96c6\uff0c\u663e\u8457\u63d0\u5347\u5de5\u4e1a\u68c0\u6d4b\u4e2d\u7684\u7ec6\u8282\u91cd\u5efa\u6548\u679c\u3002", "motivation": "\u73b0\u6709NeRF\u65b9\u6cd5\u5728\u5de5\u4e1a\u68c0\u6d4b\u4e2d\u65e0\u6cd5\u6355\u6349\u5fae\u7c73\u7ea7\u7ec6\u8282\uff0c\u4e14\u591a\u7f29\u653e\u56fe\u50cf\u4f1a\u7834\u574f\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u3002", "method": "MZEN\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u7f29\u653e\u6807\u91cf\u6539\u8fdb\u76f8\u673a\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u59ff\u6001\u7b56\u7565\uff0c\u5148\u89e3\u51b3\u5e7f\u89d2\u56fe\u50cf\u518d\u5904\u7406\u7f29\u653e\u56fe\u50cf\u3002", "result": "\u5728\u591a\u79cd\u573a\u666f\u4e0b\uff0cMZEN\u663e\u8457\u63d0\u5347\u4e86PSNR\u3001SSIM\u548cLPIPS\u6307\u6807\u3002", "conclusion": "MZEN\u6210\u529f\u5c06NeRF\u5e94\u7528\u4e8e\u5de5\u4e1a\u68c0\u6d4b\uff0c\u517c\u987e\u5168\u5c40\u7cbe\u5ea6\u548c\u5fae\u7c73\u7ea7\u7ec6\u8282\u3002"}}
{"id": "2508.05950", "pdf": "https://arxiv.org/pdf/2508.05950", "abs": "https://arxiv.org/abs/2508.05950", "authors": ["Yanxing Liang", "Yinghui Wang", "Jinlong Yang", "Wei Li"], "title": "A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The lack of spatial dimensional information remains a challenge in normal estimation from a single image. Recent diffusion-based methods have demonstrated significant potential in 2D-to-3D implicit mapping, they rely on data-driven statistical priors and miss the explicit modeling of light-surface interaction, leading to multi-view normal direction conflicts. Moreover, the discrete sampling mechanism of diffusion models causes gradient discontinuity in differentiable rendering reconstruction modules, preventing 3D geometric errors from being backpropagated to the normal generation network, thereby forcing existing methods to depend on dense normal annotations. This paper proposes SINGAD, a novel Self-supervised framework from a single Image for Normal estimation via 3D GAussian splatting guided Diffusion. By integrating physics-driven light-interaction modeling and a differentiable rendering-based reprojection strategy, our framework directly converts 3D geometric errors into normal optimization signals, solving the challenges of multi-view geometric inconsistency and data dependency. Specifically, the framework constructs a light-interaction-driven 3DGS reparameterization model to generate multi-scale geometric features consistent with light transport principles, ensuring multi-view normal consistency. A cross-domain feature fusion module is designed within a conditional diffusion model, embedding geometric priors to constrain normal generation while maintaining accurate geometric error propagation. Furthermore, a differentiable 3D reprojection loss strategy is introduced for self-supervised optimization that minimizes geometric error between the reconstructed and input image, eliminating dependence on annotated normal datasets. Quantitative evaluations on the Google Scanned Objects dataset demonstrate that our method outperforms state-of-the-art approaches across multiple metrics.", "AI": {"tldr": "SINGAD\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc73D\u9ad8\u65af\u6269\u6563\u4f30\u8ba1\u5355\u5f20\u56fe\u50cf\u7684\u6cd5\u7ebf\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u89d2\u51e0\u4f55\u4e0d\u4e00\u81f4\u6027\u548c\u6570\u636e\u4f9d\u8d56\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6570\u636e\u9a71\u52a8\u7684\u7edf\u8ba1\u5148\u9a8c\uff0c\u7f3a\u4e4f\u5bf9\u5149-\u8868\u9762\u4ea4\u4e92\u7684\u663e\u5f0f\u5efa\u6a21\uff0c\u5bfc\u81f4\u591a\u89c6\u89d2\u6cd5\u7ebf\u65b9\u5411\u51b2\u7a81\uff0c\u4e14\u65e0\u6cd5\u901a\u8fc7\u68af\u5ea6\u4f20\u64ad\u4f18\u5316\u51e0\u4f55\u8bef\u5dee\u3002", "method": "\u7ed3\u5408\u7269\u7406\u9a71\u52a8\u7684\u5149\u4ea4\u4e92\u5efa\u6a21\u548c\u53ef\u5fae\u5206\u6e32\u67d3\u91cd\u6295\u5f71\u7b56\u7565\uff0c\u6784\u5efa3D\u9ad8\u65af\u91cd\u53c2\u6570\u5316\u6a21\u578b\u548c\u8de8\u57df\u7279\u5f81\u878d\u5408\u6a21\u5757\uff0c\u5b9e\u73b0\u81ea\u76d1\u7763\u4f18\u5316\u3002", "result": "\u5728Google Scanned Objects\u6570\u636e\u96c6\u4e0a\uff0cSINGAD\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SINGAD\u901a\u8fc7\u51e0\u4f55\u4e00\u81f4\u6027\u7ea6\u675f\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u56fe\u50cf\u6cd5\u7ebf\u4f30\u8ba1\u7684\u6027\u80fd\u3002"}}
{"id": "2508.05954", "pdf": "https://arxiv.org/pdf/2508.05954", "abs": "https://arxiv.org/abs/2508.05954", "authors": ["Han Lin", "Jaemin Cho", "Amir Zadeh", "Chuan Li", "Mohit Bansal"], "title": "Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project Page: https://bifrost-1.github.io", "summary": "There is growing interest in integrating high-fidelity visual synthesis capabilities into large language models (LLMs) without compromising their strong reasoning capabilities. Existing methods that directly train LLMs or bridge LLMs and diffusion models usually suffer from costly training since the backbone LLMs have not seen image representations during pretraining. We present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs (MLLMs) and diffusion models using patch-level CLIP image embeddings as latent variables, which are natively aligned with the MLLM's CLIP visual encoder. These patch-level image embeddings are integrated into the diffusion model with a lightweight adaptation of its ControlNet. To retain the original multimodal reasoning capabilities of MLLMs, we equip the MLLM with a visual generation branch initialized from the original MLLM parameters when predicting the patch-level image embeddings. By seamlessly integrating pretrained MLLMs and diffusion models with patch-level CLIP latents, our framework enables high-fidelity controllable image generation with significant training efficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or better performance than previous methods in terms of visual fidelity and multimodal understanding, with substantially lower compute during training. We also provide comprehensive ablation studies showing the effectiveness of our design choices.", "AI": {"tldr": "Bifrost-1\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9884\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4e0e\u6269\u6563\u6a21\u578b\u7ed3\u5408\uff0c\u5229\u7528CLIP\u56fe\u50cf\u5d4c\u5165\u4f5c\u4e3a\u6f5c\u5728\u53d8\u91cf\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u9ad8\u4fdd\u771f\u53ef\u63a7\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8bad\u7ec3\u6210\u672c\u9ad8\u4e14\u672a\u5145\u5206\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u89c6\u89c9\u8868\u793a\u80fd\u529b\uff0cBifrost-1\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528patch\u7ea7CLIP\u56fe\u50cf\u5d4c\u5165\u4f5c\u4e3a\u6f5c\u5728\u53d8\u91cf\uff0c\u8f7b\u91cf\u7ea7ControlNet\u9002\u914d\u6269\u6563\u6a21\u578b\uff0c\u5e76\u521d\u59cb\u5316MLLM\u7684\u89c6\u89c9\u751f\u6210\u5206\u652f\u3002", "result": "Bifrost-1\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u591a\u6a21\u6001\u7406\u89e3\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u8bad\u7ec3\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "Bifrost-1\u901a\u8fc7\u9ad8\u6548\u6574\u5408\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2508.06014", "pdf": "https://arxiv.org/pdf/2508.06014", "abs": "https://arxiv.org/abs/2508.06014", "authors": ["Minsu Kim", "Subin Jeon", "In Cho", "Mijin Yoo", "Seon Joo Kim"], "title": "ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera Samplings and Diffusion Priors", "categories": ["cs.CV"], "comment": "10 pages, 6 Figures, ICCV 2025", "summary": "Recent advances in novel view synthesis (NVS) have enabled real-time rendering with 3D Gaussian Splatting (3DGS). However, existing methods struggle with artifacts and missing regions when rendering from viewpoints that deviate from the training trajectory, limiting seamless scene exploration. To address this, we propose a 3DGS-based pipeline that generates additional training views to enhance reconstruction. We introduce an information-gain-driven virtual camera placement strategy to maximize scene coverage, followed by video diffusion priors to refine rendered results. Fine-tuning 3D Gaussians with these enhanced views significantly improves reconstruction quality. To evaluate our method, we present Wild-Explore, a benchmark designed for challenging scene exploration. Experiments demonstrate that our approach outperforms existing 3DGS-based methods, enabling high-quality, artifact-free rendering from arbitrary viewpoints.   https://exploregs.github.io", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u7684\u7ba1\u9053\uff0c\u901a\u8fc7\u751f\u6210\u989d\u5916\u8bad\u7ec3\u89c6\u56fe\u548c\u865a\u62df\u76f8\u673a\u653e\u7f6e\u7b56\u7565\uff0c\u63d0\u5347\u65b0\u89c6\u89d2\u5408\u6210\uff08NVS\uff09\u7684\u8d28\u91cf\uff0c\u51cf\u5c11\u4f2a\u5f71\u548c\u7f3a\u5931\u533a\u57df\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u504f\u79bb\u8bad\u7ec3\u8f68\u8ff9\u7684\u89c6\u89d2\u4e0b\u6e32\u67d3\u65f6\u5b58\u5728\u4f2a\u5f71\u548c\u7f3a\u5931\u533a\u57df\uff0c\u9650\u5236\u4e86\u65e0\u7f1d\u573a\u666f\u63a2\u7d22\u3002", "method": "\u91c7\u7528\u4fe1\u606f\u589e\u76ca\u9a71\u52a8\u7684\u865a\u62df\u76f8\u673a\u653e\u7f6e\u7b56\u7565\u6700\u5927\u5316\u573a\u666f\u8986\u76d6\uff0c\u7ed3\u5408\u89c6\u9891\u6269\u6563\u5148\u9a8c\u4f18\u5316\u6e32\u67d3\u7ed3\u679c\uff0c\u5e76\u901a\u8fc7\u589e\u5f3a\u89c6\u56fe\u5fae\u8c033D\u9ad8\u65af\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u67093DGS\u65b9\u6cd5\uff0c\u652f\u6301\u9ad8\u8d28\u91cf\u3001\u65e0\u4f2a\u5f71\u7684\u4efb\u610f\u89c6\u89d2\u6e32\u67d3\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e863DGS\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u6311\u6218\u6027\u573a\u666f\u63a2\u7d22\u3002"}}
{"id": "2508.06021", "pdf": "https://arxiv.org/pdf/2508.06021", "abs": "https://arxiv.org/abs/2508.06021", "authors": ["Utku Ozbulak", "Michaela Cohrs", "Hristo L. Svilenov", "Joris Vankerschaver", "Wesley De Neve"], "title": "Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Sub-visible particle analysis using flow imaging microscopy combined with deep learning has proven effective in identifying particle types, enabling the distinction of harmless components such as silicone oil from protein particles. However, the scarcity of available data and severe imbalance between particle types within datasets remain substantial hurdles when applying multi-class classifiers to such problems, often forcing researchers to rely on less effective methods. The aforementioned issue is particularly challenging for particle types that appear unintentionally and in lower numbers, such as silicone oil and air bubbles, as opposed to protein particles, where obtaining large numbers of images through controlled settings is comparatively straightforward. In this work, we develop a state-of-the-art diffusion model to address data imbalance by generating high-fidelity images that can augment training datasets, enabling the effective training of multi-class deep neural networks. We validate this approach by demonstrating that the generated samples closely resemble real particle images in terms of visual quality and structure. To assess the effectiveness of using diffusion-generated images in training datasets, we conduct large-scale experiments on a validation dataset comprising 500,000 protein particle images and demonstrate that this approach improves classification performance with no negligible downside. Finally, to promote open research and reproducibility, we publicly release both our diffusion models and the trained multi-class deep neural network classifiers, along with a straightforward interface for easy integration into future studies, at https://github.com/utkuozbulak/svp-generative-ai.", "AI": {"tldr": "\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u4ee5\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u591a\u7c7b\u5206\u7c7b\u5668\u7684\u6027\u80fd\u3002", "motivation": "\u6570\u636e\u7a00\u7f3a\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u662f\u591a\u7c7b\u5206\u7c7b\u5668\u5728\u4e9a\u53ef\u89c1\u9897\u7c92\u5206\u6790\u4e2d\u7684\u4e3b\u8981\u969c\u788d\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u7f55\u89c1\u9897\u7c92\u7c7b\u578b\uff08\u5982\u7845\u6cb9\u548c\u6c14\u6ce1\uff09\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5148\u8fdb\u7684\u6269\u6563\u6a21\u578b\uff0c\u751f\u6210\u9ad8\u4fdd\u771f\u56fe\u50cf\u4ee5\u6269\u5145\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u751f\u6210\u7684\u6837\u672c\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u7ed3\u6784\u4e0a\u4e0e\u771f\u5b9e\u9897\u7c92\u56fe\u50cf\u76f8\u4f3c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "\u516c\u5f00\u4e86\u6269\u6563\u6a21\u578b\u548c\u591a\u7c7b\u5206\u7c7b\u5668\uff0c\u4fc3\u8fdb\u5f00\u653e\u7814\u7a76\u548c\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2508.06032", "pdf": "https://arxiv.org/pdf/2508.06032", "abs": "https://arxiv.org/abs/2508.06032", "authors": ["Kiran Chhatre", "Christopher Peters", "Srikrishna Karanam"], "title": "Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts", "categories": ["cs.CV"], "comment": "16 pages, 11 figures", "summary": "Existing methods for human parsing into body parts and clothing often use fixed mask categories with broad labels that obscure fine-grained clothing types. Recent open-vocabulary segmentation approaches leverage pretrained text-to-image (T2I) diffusion model features for strong zero-shot transfer, but typically group entire humans into a single person category, failing to distinguish diverse clothing or detailed body parts. To address this, we propose Spectrum, a unified network for part-level pixel parsing (body parts and clothing) and instance-level grouping. While diffusion-based open-vocabulary models generalize well across tasks, their internal representations are not specialized for detailed human parsing. We observe that, unlike diffusion models with broad representations, image-driven 3D texture generators maintain faithful correspondence to input images, enabling stronger representations for parsing diverse clothing and body parts. Spectrum introduces a novel repurposing of an Image-to-Texture (I2Tx) diffusion model -- obtained by fine-tuning a T2I model on 3D human texture maps -- for improved alignment with body parts and clothing. From an input image, we extract human-part internal features via the I2Tx diffusion model and generate semantically valid masks aligned to diverse clothing categories through prompt-guided grounding. Once trained, Spectrum produces semantic segmentation maps for every visible body part and clothing category, ignoring standalone garments or irrelevant objects, for any number of humans in the scene. We conduct extensive cross-dataset experiments -- separately assessing body parts, clothing parts, unseen clothing categories, and full-body masks -- and demonstrate that Spectrum consistently outperforms baseline methods in prompt-based segmentation.", "AI": {"tldr": "Spectrum\u662f\u4e00\u4e2a\u7edf\u4e00\u7f51\u7edc\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u7684\u4eba\u4f53\u90e8\u4f4d\u548c\u8863\u7269\u89e3\u6790\uff0c\u901a\u8fc7\u6539\u8fdb\u7684I2Tx\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u66f4\u597d\u7684\u8bed\u4e49\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4eba\u4f53\u89e3\u6790\u4e2d\u5e38\u4f7f\u7528\u5bbd\u6cdb\u7684\u6807\u7b7e\uff0c\u65e0\u6cd5\u533a\u5206\u7ec6\u7c92\u5ea6\u7684\u8863\u7269\u7c7b\u578b\u6216\u8be6\u7ec6\u7684\u8eab\u4f53\u90e8\u4f4d\u3002\u6269\u6563\u6a21\u578b\u867d\u80fd\u6cdb\u5316\uff0c\u4f46\u5176\u5185\u90e8\u8868\u793a\u4e0d\u9002\u5408\u8be6\u7ec6\u89e3\u6790\u3002", "method": "Spectrum\u5229\u7528\u7ecf\u8fc7\u5fae\u8c03\u7684I2Tx\u6269\u6563\u6a21\u578b\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u63d0\u793a\u5f15\u5bfc\u751f\u6210\u8bed\u4e49\u6709\u6548\u7684\u63a9\u7801\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u89e3\u6790\u3002", "result": "Spectrum\u5728\u8de8\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u63d0\u793a\u5f15\u5bfc\u7684\u5206\u5272\u4efb\u52a1\u4e2d\u3002", "conclusion": "Spectrum\u901a\u8fc7\u6539\u8fdb\u7684\u6269\u6563\u6a21\u578b\u548c\u63d0\u793a\u5f15\u5bfc\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u4eba\u4f53\u90e8\u4f4d\u548c\u8863\u7269\u89e3\u6790\u3002"}}
{"id": "2508.06033", "pdf": "https://arxiv.org/pdf/2508.06033", "abs": "https://arxiv.org/abs/2508.06033", "authors": ["Yiming Gong", "Zhen Zhu", "Minjia Zhang"], "title": "InstantEdit: Text-Guided Few-Step Image Editing with Piecewise Rectified Flow", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "We propose a fast text-guided image editing method called InstantEdit based on the RectifiedFlow framework, which is structured as a few-step editing process that preserves critical content while following closely to textual instructions. Our approach leverages the straight sampling trajectories of RectifiedFlow by introducing a specialized inversion strategy called PerRFI. To maintain consistent while editable results for RectifiedFlow model, we further propose a novel regeneration method, Inversion Latent Injection, which effectively reuses latent information obtained during inversion to facilitate more coherent and detailed regeneration. Additionally, we propose a Disentangled Prompt Guidance technique to balance editability with detail preservation, and integrate a Canny-conditioned ControlNet to incorporate structural cues and suppress artifacts. Evaluation on the PIE image editing dataset demonstrates that InstantEdit is not only fast but also achieves better qualitative and quantitative results compared to state-of-the-art few-step editing methods.", "AI": {"tldr": "InstantEdit\u662f\u4e00\u79cd\u57fa\u4e8eRectifiedFlow\u6846\u67b6\u7684\u5feb\u901f\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7PerRFI\u53cd\u8f6c\u7b56\u7565\u548cInversion Latent Injection\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u4e00\u81f4\u7684\u7f16\u8f91\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5728\u5feb\u901f\u6027\u548c\u7f16\u8f91\u4e00\u81f4\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u540c\u65f6\u4fdd\u7559\u5173\u952e\u5185\u5bb9\u5e76\u7d27\u5bc6\u9075\u5faa\u6587\u672c\u6307\u4ee4\u3002", "method": "\u7ed3\u5408RectifiedFlow\u7684\u76f4\u7ebf\u91c7\u6837\u8f68\u8ff9\uff0c\u63d0\u51faPerRFI\u53cd\u8f6c\u7b56\u7565\u548cInversion Latent Injection\u6280\u672f\uff0c\u5e76\u5f15\u5165Disentangled Prompt Guidance\u548cCanny-conditioned ControlNet\u3002", "result": "\u5728PIE\u6570\u636e\u96c6\u4e0a\uff0cInstantEdit\u5728\u901f\u5ea6\u548c\u7f16\u8f91\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "InstantEdit\u901a\u8fc7\u521b\u65b0\u6280\u672f\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u4e00\u81f4\u4e14\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u3002"}}
{"id": "2508.06044", "pdf": "https://arxiv.org/pdf/2508.06044", "abs": "https://arxiv.org/abs/2508.06044", "authors": ["Huimin Wu", "Xiaojian Ma", "Haozhe Zhao", "Yanpeng Zhao", "Qing Li"], "title": "NEP: Autoregressive Image Editing via Next Editing Token Prediction", "categories": ["cs.CV"], "comment": "The project page is: https://nep-bigai.github.io/", "summary": "Text-guided image editing involves modifying a source image based on a language instruction and, typically, requires changes to only small local regions. However, existing approaches generate the entire target image rather than selectively regenerate only the intended editing areas. This results in (1) unnecessary computational costs and (2) a bias toward reconstructing non-editing regions, which compromises the quality of the intended edits. To resolve these limitations, we propose to formulate image editing as Next Editing-token Prediction (NEP) based on autoregressive image generation, where only regions that need to be edited are regenerated, thus avoiding unintended modification to the non-editing areas. To enable any-region editing, we propose to pre-train an any-order autoregressive text-to-image (T2I) model. Once trained, it is capable of zero-shot image editing and can be easily adapted to NEP for image editing, which achieves a new state-of-the-art on widely used image editing benchmarks. Moreover, our model naturally supports test-time scaling (TTS) through iteratively refining its generation in a zero-shot manner. The project page is: https://nep-bigai.github.io/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u7684Next Editing-token Prediction (NEP)\u65b9\u6cd5\uff0c\u4ec5\u7f16\u8f91\u9700\u8981\u4fee\u6539\u7684\u533a\u57df\uff0c\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u548c\u975e\u7f16\u8f91\u533a\u57df\u7684\u504f\u5dee\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u6574\u4e2a\u76ee\u6807\u56fe\u50cf\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u975e\u7f16\u8f91\u533a\u57df\u91cd\u5efa\u504f\u5dee\u5f71\u54cd\u7f16\u8f91\u8d28\u91cf\u3002", "method": "\u901a\u8fc7\u9884\u8bad\u7ec3\u4efb\u610f\u987a\u5e8f\u81ea\u56de\u5f52\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u56fe\u50cf\u7f16\u8f91\uff0c\u5e76\u9002\u5e94NEP\u65b9\u6cd5\u3002", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u56fe\u50cf\u7f16\u8f91\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u652f\u6301\u96f6\u6837\u672c\u8fed\u4ee3\u4f18\u5316\uff08TTS\uff09\u3002", "conclusion": "NEP\u65b9\u6cd5\u901a\u8fc7\u9009\u62e9\u6027\u7f16\u8f91\u533a\u57df\u89e3\u51b3\u4e86\u73b0\u6709\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u7f16\u8f91\u3002"}}
{"id": "2508.06051", "pdf": "https://arxiv.org/pdf/2508.06051", "abs": "https://arxiv.org/abs/2508.06051", "authors": ["Linhan Cao", "Wei Sun", "Weixia Zhang", "Xiangyang Zhu", "Jun Jia", "Kaiwei Zhang", "Dandan Zhu", "Guangtao Zhai", "Xiongkuo Min"], "title": "VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Video quality assessment (VQA) aims to objectively quantify perceptual quality degradation in alignment with human visual perception. Despite recent advances, existing VQA models still suffer from two critical limitations: \\textit{poor generalization to out-of-distribution (OOD) videos} and \\textit{limited explainability}, which restrict their applicability in real-world scenarios. To address these challenges, we propose \\textbf{VQAThinker}, a reasoning-based VQA framework that leverages large multimodal models (LMMs) with reinforcement learning to jointly model video quality understanding and scoring, emulating human perceptual decision-making. Specifically, we adopt group relative policy optimization (GRPO), a rule-guided reinforcement learning algorithm that enables reasoning over video quality under score-level supervision, and introduce three VQA-specific rewards: (1) a \\textbf{bell-shaped regression reward} that increases rapidly as the prediction error decreases and becomes progressively less sensitive near the ground truth; (2) a \\textbf{pairwise ranking reward} that guides the model to correctly determine the relative quality between video pairs; and (3) a \\textbf{temporal consistency reward} that encourages the model to prefer temporally coherent videos over their perturbed counterparts. Extensive experiments demonstrate that VQAThinker achieves state-of-the-art performance on both in-domain and OOD VQA benchmarks, showing strong generalization for video quality scoring. Furthermore, evaluations on video quality understanding tasks validate its superiority in distortion attribution and quality description compared to existing explainable VQA models and LMMs. These findings demonstrate that reinforcement learning offers an effective pathway toward building generalizable and explainable VQA models solely with score-level supervision.", "AI": {"tldr": "VQAThinker\u662f\u4e00\u4e2a\u57fa\u4e8e\u63a8\u7406\u7684\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u7684\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\uff08VQA\uff09\u6a21\u578b\u5728\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u548c\u4e09\u79cdVQA\u7279\u5b9a\u5956\u52b1\uff08\u56de\u5f52\u3001\u6392\u5e8f\u548c\u65f6\u95f4\u4e00\u81f4\u6027\uff09\u6765\u6a21\u62df\u4eba\u7c7b\u611f\u77e5\u51b3\u7b56\u3002", "result": "VQAThinker\u5728\u57df\u5185\u548c\u57df\u5916VQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u5728\u8d28\u91cf\u7406\u89e3\u548c\u63cf\u8ff0\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u4e3a\u4ec5\u4f9d\u8d56\u5206\u6570\u7ea7\u76d1\u7763\u6784\u5efa\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u5f3a\u7684VQA\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2508.06058", "pdf": "https://arxiv.org/pdf/2508.06058", "abs": "https://arxiv.org/abs/2508.06058", "authors": ["Shiyang Zhou", "Haijin Zeng", "Yunfan Lu", "Yongyong Chen", "Jie Liu", "Jingyong Su"], "title": "Lightweight Quad Bayer HybridEVS Demosaicing via State Space Augmented Cross-Attention", "categories": ["cs.CV"], "comment": null, "summary": "Event cameras like the Hybrid Event-based Vision Sensor (HybridEVS) camera capture brightness changes as asynchronous \"events\" instead of frames, offering advanced application on mobile photography. However, challenges arise from combining a Quad Bayer Color Filter Array (CFA) sensor with event pixels lacking color information, resulting in aliasing and artifacts on the demosaicing process before downstream application. Current methods struggle to address these issues, especially on resource-limited mobile devices. In response, we introduce \\textbf{TSANet}, a lightweight \\textbf{T}wo-stage network via \\textbf{S}tate space augmented cross-\\textbf{A}ttention, which can handle event pixels inpainting and demosaicing separately, leveraging the benefits of dividing complex tasks into manageable subtasks. Furthermore, we introduce a lightweight Cross-Swin State Block that uniquely utilizes positional prior for demosaicing and enhances global dependencies through the state space model with linear complexity. In summary, TSANet demonstrates excellent demosaicing performance on both simulated and real data of HybridEVS while maintaining a lightweight model, averaging better results than the previous state-of-the-art method DemosaicFormer across seven diverse datasets in both PSNR and SSIM, while respectively reducing parameter and computation costs by $1.86\\times$ and $3.29\\times$. Our approach presents new possibilities for efficient image demosaicing on mobile devices. Code is available in the supplementary materials.", "AI": {"tldr": "TSANet\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u4e24\u9636\u6bb5\u7f51\u7edc\uff0c\u901a\u8fc7\u72b6\u6001\u7a7a\u95f4\u589e\u5f3a\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\uff0c\u5206\u522b\u5904\u7406\u4e8b\u4ef6\u50cf\u7d20\u4fee\u590d\u548c\u53bb\u9a6c\u8d5b\u514b\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86HybridEVS\u76f8\u673a\u7684\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "HybridEVS\u76f8\u673a\u7ed3\u5408Quad Bayer CFA\u4f20\u611f\u5668\u548c\u4e8b\u4ef6\u50cf\u7d20\u65f6\uff0c\u7f3a\u4e4f\u989c\u8272\u4fe1\u606f\u5bfc\u81f4\u53bb\u9a6c\u8d5b\u514b\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u4f2a\u5f71\u548c\u6df7\u53e0\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faTSANet\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u7f51\u7edc\u8bbe\u8ba1\uff0c\u5206\u522b\u5904\u7406\u4e8b\u4ef6\u50cf\u7d20\u4fee\u590d\u548c\u53bb\u9a6c\u8d5b\u514b\uff1b\u5f15\u5165\u8f7b\u91cf\u7ea7\u7684Cross-Swin State Block\uff0c\u5229\u7528\u4f4d\u7f6e\u5148\u9a8c\u548c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u589e\u5f3a\u5168\u5c40\u4f9d\u8d56\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9eHybridEVS\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cPSNR\u548cSSIM\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5DemosaicFormer\uff0c\u53c2\u6570\u548c\u8ba1\u7b97\u6210\u672c\u5206\u522b\u964d\u4f4e1.86\u500d\u548c3.29\u500d\u3002", "conclusion": "TSANet\u4e3a\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548\u56fe\u50cf\u53bb\u9a6c\u8d5b\u514b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u6a21\u578b\u8f7b\u91cf\u4e14\u6027\u80fd\u4f18\u8d8a\u3002"}}
{"id": "2508.06076", "pdf": "https://arxiv.org/pdf/2508.06076", "abs": "https://arxiv.org/abs/2508.06076", "authors": ["Michael Wehrli", "Alicia Durrer", "Paul Friedrich", "Sidaty El Hadramy", "Edwin Li", "Luana Brahaj", "Carol C. Hasler", "Philippe C. Cattin"], "title": "Towards MR-Based Trochleoplasty Planning", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at MICCAI COLAS Workshop 2025. Code:   https://wehrlimi.github.io/sr-3d-planning/", "summary": "To treat Trochlear Dysplasia (TD), current approaches rely mainly on low-resolution clinical Magnetic Resonance (MR) scans and surgical intuition. The surgeries are planned based on surgeons experience, have limited adoption of minimally invasive techniques, and lead to inconsistent outcomes. We propose a pipeline that generates super-resolved, patient-specific 3D pseudo-healthy target morphologies from conventional clinical MR scans. First, we compute an isotropic super-resolved MR volume using an Implicit Neural Representation (INR). Next, we segment femur, tibia, patella, and fibula with a multi-label custom-trained network. Finally, we train a Wavelet Diffusion Model (WDM) to generate pseudo-healthy target morphologies of the trochlear region. In contrast to prior work producing pseudo-healthy low-resolution 3D MR images, our approach enables the generation of sub-millimeter resolved 3D shapes compatible for pre- and intraoperative use. These can serve as preoperative blueprints for reshaping the femoral groove while preserving the native patella articulation. Furthermore, and in contrast to other work, we do not require a CT for our pipeline - reducing the amount of radiation. We evaluated our approach on 25 TD patients and could show that our target morphologies significantly improve the sulcus angle (SA) and trochlear groove depth (TGD). The code and interactive visualization are available at https://wehrlimi.github.io/sr-3d-planning/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e34\u5e8aMR\u626b\u63cf\u751f\u6210\u9ad8\u5206\u8fa8\u73873D\u4f2a\u5065\u5eb7\u76ee\u6807\u5f62\u6001\u7684\u6d41\u7a0b\uff0c\u7528\u4e8e\u6cbb\u7597\u6ed1\u8f66\u53d1\u80b2\u4e0d\u826f\uff08TD\uff09\uff0c\u65e0\u9700CT\u626b\u63cf\uff0c\u51cf\u5c11\u8f90\u5c04\u3002", "motivation": "\u5f53\u524d\u6cbb\u7597TD\u7684\u65b9\u6cd5\u4f9d\u8d56\u4f4e\u5206\u8fa8\u7387MR\u626b\u63cf\u548c\u5916\u79d1\u533b\u751f\u7ecf\u9a8c\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u4e00\u81f4\u4e14\u5fae\u521b\u6280\u672f\u5e94\u7528\u6709\u9650\u3002", "method": "\u4f7f\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u751f\u6210\u5404\u5411\u540c\u6027\u8d85\u5206\u8fa8\u7387MR\u4f53\u79ef\uff0c\u591a\u6807\u7b7e\u7f51\u7edc\u5206\u5272\u9aa8\u9abc\uff0c\u5c0f\u6ce2\u6269\u6563\u6a21\u578b\uff08WDM\uff09\u751f\u6210\u4f2a\u5065\u5eb7\u76ee\u6807\u5f62\u6001\u3002", "result": "\u572825\u540dTD\u60a3\u8005\u4e2d\u9a8c\u8bc1\uff0c\u663e\u8457\u6539\u5584\u4e86\u6ed1\u8f66\u89d2\u5ea6\uff08SA\uff09\u548c\u6ed1\u8f66\u6c9f\u6df1\u5ea6\uff08TGD\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u672f\u524d\u89c4\u5212\u63d0\u4f9b\u4e86\u9ad8\u5206\u8fa8\u73873D\u5f62\u6001\uff0c\u51cf\u5c11\u8f90\u5c04\uff0c\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2508.06080", "pdf": "https://arxiv.org/pdf/2508.06080", "abs": "https://arxiv.org/abs/2508.06080", "authors": ["Bin Xia", "Jiyang Liu", "Yuechen Zhang", "Bohao Peng", "Ruihang Chu", "Yitong Wang", "Xinglong Wu", "Bei Yu", "Jiaya Jia"], "title": "DreamVE: Unified Instruction-based Image and Video Editing", "categories": ["cs.CV"], "comment": null, "summary": "Instruction-based editing holds vast potential due to its simple and efficient interactive editing format. However, instruction-based editing, particularly for video, has been constrained by limited training data, hindering its practical application. To this end, we introduce DreamVE, a unified model for instruction-based image and video editing. Specifically, We propose a two-stage training strategy: first image editing, then video editing. This offers two main benefits: (1) Image data scales more easily, and models are more efficient to train, providing useful priors for faster and better video editing training. (2) Unifying image and video generation is natural and aligns with current trends. Moreover, we present comprehensive training data synthesis pipelines, including collage-based and generative model-based data synthesis. The collage-based data synthesis combines foreground objects and backgrounds to generate diverse editing data, such as object manipulation, background changes, and text modifications. It can easily generate billions of accurate, consistent, realistic, and diverse editing pairs. We pretrain DreamVE on extensive collage-based data to achieve strong performance in key editing types and enhance generalization and transfer capabilities. However, collage-based data lacks some attribute editing cases, leading to a relative drop in performance. In contrast, the generative model-based pipeline, despite being hard to scale up, offers flexibility in handling attribute editing cases. Therefore, we use generative model-based data to further fine-tune DreamVE. Besides, we design an efficient and powerful editing framework for DreamVE. We build on the SOTA T2V model and use a token concatenation with early drop approach to inject source image guidance, ensuring strong consistency and editability. The codes and models will be released.", "AI": {"tldr": "DreamVE\u662f\u4e00\u4e2a\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u548c\u89c6\u9891\u7f16\u8f91\u7edf\u4e00\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff08\u5148\u56fe\u50cf\u540e\u89c6\u9891\uff09\u548c\u7efc\u5408\u6570\u636e\u5408\u6210\u65b9\u6cd5\uff08\u62fc\u8d34\u548c\u751f\u6210\u6a21\u578b\uff09\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u6307\u4ee4\u7684\u89c6\u9891\u7f16\u8f91\u56e0\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u800c\u53d7\u9650\u7684\u95ee\u9898\uff0c\u63a8\u52a8\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff08\u56fe\u50cf\u7f16\u8f91\u540e\u89c6\u9891\u7f16\u8f91\uff09\uff0c\u7ed3\u5408\u62fc\u8d34\u548c\u751f\u6210\u6a21\u578b\u7684\u6570\u636e\u5408\u6210\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u9ad8\u6548\u7684\u7f16\u8f91\u6846\u67b6\u3002", "result": "DreamVE\u5728\u5173\u952e\u7f16\u8f91\u7c7b\u578b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u548c\u8fc1\u79fb\u80fd\u529b\uff0c\u4f46\u5c5e\u6027\u7f16\u8f91\u6027\u80fd\u7a0d\u5f31\u3002", "conclusion": "DreamVE\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u548c\u591a\u6837\u5316\u6570\u636e\u5408\u6210\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e\u6307\u4ee4\u7684\u7f16\u8f91\u6027\u80fd\uff0c\u672a\u6765\u5c06\u5f00\u6e90\u4ee3\u7801\u548c\u6a21\u578b\u3002"}}
{"id": "2508.06082", "pdf": "https://arxiv.org/pdf/2508.06082", "abs": "https://arxiv.org/abs/2508.06082", "authors": ["Yanxiao Sun", "Jiafu Wu", "Yun Cao", "Chengming Xu", "Yabiao Wang", "Weijian Cao", "Donghao Luo", "Chengjie Wang", "Yanwei Fu"], "title": "SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion-based or flow-based models have achieved significant progress in video synthesis but require multiple iterative sampling steps, which incurs substantial computational overhead. While many distillation methods that are solely based on trajectory-preserving or distribution-matching have been developed to accelerate video generation models, these approaches often suffer from performance breakdown or increased artifacts under few-step settings. To address these limitations, we propose \\textbf{\\emph{SwiftVideo}}, a unified and stable distillation framework that combines the advantages of trajectory-preserving and distribution-matching strategies. Our approach introduces continuous-time consistency distillation to ensure precise preservation of ODE trajectories. Subsequently, we propose a dual-perspective alignment that includes distribution alignment between synthetic and real data along with trajectory alignment across different inference steps. Our method maintains high-quality video generation while substantially reducing the number of inference steps. Quantitative evaluations on the OpenVid-1M benchmark demonstrate that our method significantly outperforms existing approaches in few-step video generation.", "AI": {"tldr": "SwiftVideo\u662f\u4e00\u79cd\u7ed3\u5408\u8f68\u8ff9\u4fdd\u6301\u548c\u5206\u5e03\u5339\u914d\u7b56\u7565\u7684\u7edf\u4e00\u7a33\u5b9a\u84b8\u998f\u6846\u67b6\uff0c\u7528\u4e8e\u52a0\u901f\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u51cf\u5c11\u63a8\u7406\u6b65\u9aa4\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8f68\u8ff9\u4fdd\u6301\u6216\u5206\u5e03\u5339\u914d\u7684\u84b8\u998f\u65b9\u6cd5\u5728\u5c11\u6b65\u8bbe\u7f6e\u4e0b\u6027\u80fd\u4e0b\u964d\u6216\u4ea7\u751f\u66f4\u591a\u4f2a\u5f71\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u8fde\u7eed\u65f6\u95f4\u4e00\u81f4\u6027\u84b8\u998f\u786e\u4fddODE\u8f68\u8ff9\u7cbe\u786e\u4fdd\u6301\uff0c\u5e76\u5f15\u5165\u53cc\u89c6\u89d2\u5bf9\u9f50\uff08\u5206\u5e03\u5bf9\u9f50\u548c\u8f68\u8ff9\u5bf9\u9f50\uff09\u4ee5\u4f18\u5316\u6027\u80fd\u3002", "result": "\u5728OpenVid-1M\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSwiftVideo\u5728\u5c11\u6b65\u89c6\u9891\u751f\u6210\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SwiftVideo\u901a\u8fc7\u7ed3\u5408\u4e24\u79cd\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u751f\u6210\uff0c\u4e3a\u5c11\u6b65\u63a8\u7406\u63d0\u4f9b\u4e86\u7a33\u5b9a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06093", "pdf": "https://arxiv.org/pdf/2508.06093", "abs": "https://arxiv.org/abs/2508.06093", "authors": ["Chen Zhu", "Buzhen Huang", "Zijing Wu", "Binghui Zuo", "Yangang Wang"], "title": "E-React: Towards Emotionally Controlled Synthesis of Human Reactions", "categories": ["cs.CV"], "comment": null, "summary": "Emotion serves as an essential component in daily human interactions. Existing human motion generation frameworks do not consider the impact of emotions, which reduces naturalness and limits their application in interactive tasks, such as human reaction synthesis. In this work, we introduce a novel task: generating diverse reaction motions in response to different emotional cues. However, learning emotion representation from limited motion data and incorporating it into a motion generation framework remains a challenging problem. To address the above obstacles, we introduce a semi-supervised emotion prior in an actor-reactor diffusion model to facilitate emotion-driven reaction synthesis. Specifically, based on the observation that motion clips within a short sequence tend to share the same emotion, we first devise a semi-supervised learning framework to train an emotion prior. With this prior, we further train an actor-reactor diffusion model to generate reactions by considering both spatial interaction and emotional response. Finally, given a motion sequence of an actor, our approach can generate realistic reactions under various emotional conditions. Experimental results demonstrate that our model outperforms existing reaction generation methods. The code and data will be made publicly available at https://ereact.github.io/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u60c5\u611f\u9a71\u52a8\u7684\u4eba\u4f53\u53cd\u5e94\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u534a\u76d1\u7763\u60c5\u611f\u5148\u9a8c\u548c\u6269\u6563\u6a21\u578b\u63d0\u5347\u751f\u6210\u7684\u81ea\u7136\u6027\u548c\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u4f53\u52a8\u4f5c\u751f\u6210\u6846\u67b6\u672a\u8003\u8651\u60c5\u611f\u5f71\u54cd\uff0c\u5bfc\u81f4\u751f\u6210\u52a8\u4f5c\u4e0d\u591f\u81ea\u7136\uff0c\u9650\u5236\u4e86\u5728\u4ea4\u4e92\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u534a\u76d1\u7763\u5b66\u4e60\u8bad\u7ec3\u60c5\u611f\u5148\u9a8c\uff0c\u5e76\u7ed3\u5408\u6269\u6563\u6a21\u578b\u751f\u6210\u8003\u8651\u7a7a\u95f4\u4ea4\u4e92\u548c\u60c5\u611f\u54cd\u5e94\u7684\u53cd\u5e94\u52a8\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u591a\u6837\u60c5\u611f\u9a71\u52a8\u7684\u53cd\u5e94\u52a8\u4f5c\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u60c5\u611f\u9a71\u52a8\u52a8\u4f5c\u751f\u6210\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u751f\u6210\u52a8\u4f5c\u7684\u81ea\u7136\u6027\u548c\u591a\u6837\u6027\u3002"}}
{"id": "2508.06136", "pdf": "https://arxiv.org/pdf/2508.06136", "abs": "https://arxiv.org/abs/2508.06136", "authors": ["YoungChan Choi", "HengFei Wang", "YiHua Cheng", "Boeun Kim", "Hyung Jin Chang", "YoungGeun Choi", "Sang-Il Choi"], "title": "Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 5 figures, ACM Multimeida 2025 accepted", "summary": "We propose a novel 3D gaze redirection framework that leverages an explicit 3D eyeball structure. Existing gaze redirection methods are typically based on neural radiance fields, which employ implicit neural representations via volume rendering. Unlike these NeRF-based approaches, where the rotation and translation of 3D representations are not explicitly modeled, we introduce a dedicated 3D eyeball structure to represent the eyeballs with 3D Gaussian Splatting (3DGS). Our method generates photorealistic images that faithfully reproduce the desired gaze direction by explicitly rotating and translating the 3D eyeball structure. In addition, we propose an adaptive deformation module that enables the replication of subtle muscle movements around the eyes. Through experiments conducted on the ETH-XGaze dataset, we demonstrate that our framework is capable of generating diverse novel gaze images, achieving superior image quality and gaze estimation accuracy compared to previous state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u663e\u5f0f3D\u773c\u7403\u7ed3\u6784\u7684\u65b0\u578b3D\u89c6\u7ebf\u91cd\u5b9a\u5411\u6846\u67b6\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8eNeRF\u7684\u9690\u5f0f\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u7ebf\u91cd\u5b9a\u5411\u65b9\u6cd5\u591a\u57fa\u4e8e\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\uff0c\u5176\u9690\u5f0f\u8868\u793a\u96be\u4ee5\u663e\u5f0f\u5efa\u6a21\u773c\u7403\u65cb\u8f6c\u548c\u5e73\u79fb\u3002", "method": "\u91c7\u75283D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u663e\u5f0f\u8868\u793a\u773c\u7403\u7ed3\u6784\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u53d8\u5f62\u6a21\u5757\u6a21\u62df\u773c\u90e8\u808c\u8089\u8fd0\u52a8\u3002", "result": "\u5728ETH-XGaze\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u9ad8\u4e14\u89c6\u7ebf\u4f30\u8ba1\u51c6\u786e\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u663e\u5f0f3D\u773c\u7403\u7ed3\u6784\u7ed3\u5408\u81ea\u9002\u5e94\u53d8\u5f62\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u7ebf\u91cd\u5b9a\u5411\u7684\u903c\u771f\u5ea6\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2508.06139", "pdf": "https://arxiv.org/pdf/2508.06139", "abs": "https://arxiv.org/abs/2508.06139", "authors": ["Shaohua Pan", "Xinyu Yi", "Yan Zhou", "Weihua Jian", "Yuan Zhang", "Pengfei Wan", "Feng Xu"], "title": "DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera", "categories": ["cs.CV"], "comment": null, "summary": "Combining sparse IMUs and a monocular camera is a new promising setting to perform real-time human motion capture. This paper proposes a diffusion-based solution to learn human motion priors and fuse the two modalities of signals together seamlessly in a unified framework. By delicately considering the characteristics of the two signals, the sequential visual information is considered as a whole and transformed into a condition embedding, while the inertial measurement is concatenated with the noisy body pose frame by frame to construct a sequential input for the diffusion model. Firstly, we observe that the visual information may be unavailable in some frames due to occlusions or subjects moving out of the camera view. Thus incorporating the sequential visual features as a whole to get a single feature embedding is robust to the occasional degenerations of visual information in those frames. On the other hand, the IMU measurements are robust to occlusions and always stable when signal transmission has no problem. So incorporating them frame-wisely could better explore the temporal information for the system. Experiments have demonstrated the effectiveness of the system design and its state-of-the-art performance in pose estimation compared with the previous works. Our codes are available for research at https://shaohua-pan.github.io/diffcap-page.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u7a00\u758fIMU\u548c\u5355\u76ee\u6444\u50cf\u5934\u8fdb\u884c\u5b9e\u65f6\u4eba\u4f53\u8fd0\u52a8\u6355\u6349\uff0c\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u878d\u5408\u4e24\u79cd\u4fe1\u53f7\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u7ed3\u5408\u7a00\u758fIMU\u548c\u5355\u76ee\u6444\u50cf\u5934\u8fdb\u884c\u5b9e\u65f6\u8fd0\u52a8\u6355\u6349\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u89c6\u89c9\u4fe1\u606f\u5076\u5c14\u5931\u6548\u548cIMU\u4fe1\u53f7\u7a33\u5b9a\u6027\u7684\u95ee\u9898\u3002", "method": "\u5c06\u89c6\u89c9\u4fe1\u606f\u6574\u4f53\u8f6c\u5316\u4e3a\u6761\u4ef6\u5d4c\u5165\uff0c\u800cIMU\u6d4b\u91cf\u503c\u9010\u5e27\u4e0e\u566a\u58f0\u59ff\u6001\u62fc\u63a5\u4f5c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u8f93\u5165\uff0c\u4ee5\u5145\u5206\u5229\u7528\u4e24\u79cd\u4fe1\u53f7\u7684\u7279\u70b9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u878d\u5408\u89c6\u89c9\u548cIMU\u4fe1\u53f7\uff0c\u8be5\u65b9\u6cd5\u5728\u8fd0\u52a8\u6355\u6349\u4e2d\u5b9e\u73b0\u4e86\u9c81\u68d2\u6027\u548c\u9ad8\u6027\u80fd\u3002"}}
{"id": "2508.06160", "pdf": "https://arxiv.org/pdf/2508.06160", "abs": "https://arxiv.org/abs/2508.06160", "authors": ["Zhenbang Du", "Yonggan Fu", "Lifu Wang", "Jiayi Qian", "Xiao Luo", "Yingyan", "Lin"], "title": "Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Diffusion models have shown remarkable success across generative tasks, yet their high computational demands challenge deployment on resource-limited platforms. This paper investigates a critical question for compute-optimal diffusion model deployment: Under a post-training setting without fine-tuning, is it more effective to reduce the number of denoising steps or to use a cheaper per-step inference? Intuitively, reducing the number of denoising steps increases the variability of the distributions across steps, making the model more sensitive to compression. In contrast, keeping more denoising steps makes the differences smaller, preserving redundancy, and making post-training compression more feasible. To systematically examine this, we propose PostDiff, a training-free framework for accelerating pre-trained diffusion models by reducing redundancy at both the input level and module level in a post-training manner. At the input level, we propose a mixed-resolution denoising scheme based on the insight that reducing generation resolution in early denoising steps can enhance low-frequency components and improve final generation fidelity. At the module level, we employ a hybrid module caching strategy to reuse computations across denoising steps. Extensive experiments and ablation studies demonstrate that (1) PostDiff can significantly improve the fidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to boost efficiency while maintaining decent generation fidelity, reducing per-step inference cost is often more effective than reducing the number of denoising steps. Our code is available at https://github.com/GATECH-EIC/PostDiff.", "AI": {"tldr": "PostDiff\u6846\u67b6\u901a\u8fc7\u6df7\u5408\u5206\u8fa8\u7387\u53bb\u566a\u548c\u6a21\u5757\u7ea7\u7f13\u5b58\u7b56\u7565\uff0c\u5728\u65e0\u9700\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u4f18\u5316\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u6548\u7387\u4e0e\u4fdd\u771f\u5ea6\u6743\u8861\u3002", "motivation": "\u7814\u7a76\u5728\u8d44\u6e90\u6709\u9650\u5e73\u53f0\u4e0a\u90e8\u7f72\u6269\u6563\u6a21\u578b\u65f6\uff0c\u51cf\u5c11\u53bb\u566a\u6b65\u9aa4\u6216\u964d\u4f4e\u6bcf\u6b65\u8ba1\u7b97\u6210\u672c\u54ea\u79cd\u65b9\u5f0f\u66f4\u6709\u6548\u3002", "method": "\u63d0\u51faPostDiff\u6846\u67b6\uff0c\u5305\u62ec\u6df7\u5408\u5206\u8fa8\u7387\u53bb\u566a\u65b9\u6848\u548c\u6a21\u5757\u7ea7\u7f13\u5b58\u7b56\u7565\uff0c\u4ee5\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPostDiff\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u4e0e\u4fdd\u771f\u5ea6\u7684\u5e73\u8861\uff0c\u4e14\u964d\u4f4e\u6bcf\u6b65\u8ba1\u7b97\u6210\u672c\u6bd4\u51cf\u5c11\u53bb\u566a\u6b65\u9aa4\u66f4\u6709\u6548\u3002", "conclusion": "PostDiff\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6269\u6563\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u65e0\u9700\u5fae\u8c03\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06169", "pdf": "https://arxiv.org/pdf/2508.06169", "abs": "https://arxiv.org/abs/2508.06169", "authors": ["Wenpeng Xing", "Jie Chen", "Zaifeng Yang", "Changting Lin", "Jianfeng Dong", "Chaochao Chen", "Xun Zhou", "Meng Han"], "title": "UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Underwater 3D scene reconstruction faces severe challenges from light absorption, scattering, and turbidity, which degrade geometry and color fidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF extensions such as SeaThru-NeRF incorporate physics-based models, their MLP reliance limits efficiency and spatial resolution in hazy environments. We introduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for robust underwater reconstruction. Key innovations include: (1) a plug-and-play learnable underwater image formation module using voxel-based regression for spatially varying attenuation and backscatter; and (2) a Physics-Aware Uncertainty Pruning (PAUP) branch that adaptively removes noisy floating Gaussians via uncertainty scoring, ensuring artifact-free geometry. The pipeline operates in training and rendering stages. During training, noisy Gaussians are optimized end-to-end with underwater parameters, guided by PAUP pruning and scattering modeling. In rendering, refined Gaussians produce clean Unattenuated Radiance Images (URIs) free from media effects, while learned physics enable realistic Underwater Images (UWIs) with accurate light transport. Experiments on SeaThru-NeRF and UWBundle datasets show superior performance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on SeaThru-NeRF, with ~65% reduction in floating artifacts.", "AI": {"tldr": "UW-3DGS\u662f\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u7684\u6c34\u4e0b3D\u573a\u666f\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u7269\u7406\u6a21\u5757\u548c\u81ea\u9002\u5e94\u566a\u58f0\u4fee\u526a\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u4e0b\u91cd\u5efa\u7684\u51e0\u4f55\u548c\u989c\u8272\u4fdd\u771f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\uff08\u5982NeRF\u53ca\u5176\u6269\u5c55\uff09\u5728\u6c34\u4e0b\u73af\u5883\u4e2d\u56e0\u5149\u7ebf\u5438\u6536\u3001\u6563\u5c04\u548c\u6d51\u6d4a\u95ee\u9898\u5bfc\u81f4\u91cd\u5efa\u6548\u679c\u4e0d\u4f73\uff0c\u4e14\u6548\u7387\u8f83\u4f4e\u3002", "method": "UW-3DGS\u7ed3\u5408\u4e86\u53ef\u5b66\u4e60\u7684\u6c34\u4e0b\u56fe\u50cf\u5f62\u6210\u6a21\u5757\u548c\u7269\u7406\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u4fee\u526a\uff08PAUP\uff09\u5206\u652f\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u4f18\u5316\u9ad8\u65af\u5206\u5e03\u5e76\u53bb\u9664\u566a\u58f0\u3002", "result": "\u5728SeaThru-NeRF\u548cUWBundle\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cPSNR\u8fbe27.604\uff0cSSIM\u4e3a0.868\uff0cLPIPS\u4e3a0.104\uff0c\u6d6e\u52a8\u7269\u4f53\u51cf\u5c11\u7ea665%\u3002", "conclusion": "UW-3DGS\u901a\u8fc7\u7269\u7406\u5efa\u6a21\u548c\u81ea\u9002\u5e94\u4fee\u526a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u4e0b3D\u91cd\u5efa\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2508.06170", "pdf": "https://arxiv.org/pdf/2508.06170", "abs": "https://arxiv.org/abs/2508.06170", "authors": ["Ojonugwa Oluwafemi Ejiga Peter", "Akingbola Oluwapemiisin", "Amalahu Chetachi", "Adeniran Opeyemi", "Fahmi Khalifa", "Md Mahmudur Rahman"], "title": "Synthetic Data-Driven Multi-Architecture Framework for Automated Polyp Segmentation Through Integrated Detection and Mask Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Colonoscopy is a vital tool for the early diagnosis of colorectal cancer, which is one of the main causes of cancer-related mortality globally; hence, it is deemed an essential technique for the prevention and early detection of colorectal cancer. The research introduces a unique multidirectional architectural framework to automate polyp detection within colonoscopy images while helping resolve limited healthcare dataset sizes and annotation complexities. The research implements a comprehensive system that delivers synthetic data generation through Stable Diffusion enhancements together with detection and segmentation algorithms. This detection approach combines Faster R-CNN for initial object localization while the Segment Anything Model (SAM) refines the segmentation masks. The faster R-CNN detection algorithm achieved a recall of 93.08% combined with a precision of 88.97% and an F1 score of 90.98%.SAM is then used to generate the image mask. The research evaluated five state-of-the-art segmentation models that included U-Net, PSPNet, FPN, LinkNet, and MANet using ResNet34 as a base model. The results demonstrate the superior performance of FPN with the highest scores of PSNR (7.205893) and SSIM (0.492381), while UNet excels in recall (84.85%) and LinkNet shows balanced performance in IoU (64.20%) and Dice score (77.53%).", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u65b9\u5411\u67b6\u6784\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u7ed3\u80a0\u955c\u56fe\u50cf\u4e2d\u7684\u606f\u8089\u68c0\u6d4b\uff0c\u7ed3\u5408\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u68c0\u6d4b\u5206\u5272\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u548c\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u7ed3\u80a0\u955c\u68c0\u67e5\u662f\u7ed3\u76f4\u80a0\u764c\u65e9\u671f\u8bca\u65ad\u7684\u5173\u952e\u5de5\u5177\uff0c\u4f46\u533b\u7597\u6570\u636e\u96c6\u6709\u9650\u4e14\u6807\u6ce8\u590d\u6742\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408Faster R-CNN\u8fdb\u884c\u521d\u59cb\u76ee\u6807\u5b9a\u4f4d\uff0cSegment Anything Model\uff08SAM\uff09\u4f18\u5316\u5206\u5272\u63a9\u7801\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e94\u79cd\u5206\u5272\u6a21\u578b\uff08U-Net\u3001PSPNet\u3001FPN\u3001LinkNet\u3001MANet\uff09\u3002", "result": "Faster R-CNN\u7684\u53ec\u56de\u7387\u4e3a93.08%\uff0c\u7cbe\u786e\u7387\u4e3a88.97%\uff0cF1\u5206\u6570\u4e3a90.98%\uff1bFPN\u5728PSNR\u548cSSIM\u4e0a\u8868\u73b0\u6700\u4f73\uff0cUNet\u5728\u53ec\u56de\u7387\u4e0a\u9886\u5148\uff0cLinkNet\u5728IoU\u548cDice\u5206\u6570\u4e0a\u8868\u73b0\u5747\u8861\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u96c6\u548c\u6807\u6ce8\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u606f\u8089\u68c0\u6d4b\u548c\u5206\u5272\u7684\u81ea\u52a8\u5316\u6027\u80fd\u3002"}}
{"id": "2508.06202", "pdf": "https://arxiv.org/pdf/2508.06202", "abs": "https://arxiv.org/abs/2508.06202", "authors": ["Chang Che", "Ziqi Wang", "Pengwan Yang", "Qi Wang", "Hui Ma", "Zenglin Shi"], "title": "LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language Models (MLLMs) to incrementally learn new tasks over time. However, this process is challenged by catastrophic forgetting, where performance on previously learned tasks deteriorates as the model adapts to new ones. A common approach to mitigate forgetting is architecture expansion, which introduces task-specific modules to prevent interference. Yet, existing methods often expand entire layers for each task, leading to significant parameter overhead and poor scalability. To overcome these issues, we introduce LoRA in LoRA (LiLoRA), a highly efficient architecture expansion method tailored for CVIT in MLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy, applies an additional low-rank decomposition to matrix B to minimize task-specific parameters, and incorporates a cosine-regularized stability loss to preserve consistency in shared representations over time. Extensive experiments on a diverse CVIT benchmark show that LiLoRA consistently achieves superior performance in sequential task learning while significantly improving parameter efficiency compared to existing approaches.", "AI": {"tldr": "LiLoRA\u662f\u4e00\u79cd\u9ad8\u6548\u67b6\u6784\u6269\u5c55\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u6301\u7eed\u89c6\u89c9\u6307\u4ee4\u8c03\u6574\uff08CVIT\uff09\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u901a\u8fc7\u5171\u4eabLoRA\u77e9\u9635\u548c\u4f4e\u79e9\u5206\u89e3\u51cf\u5c11\u53c2\u6570\u5197\u4f59\uff0c\u540c\u65f6\u5f15\u5165\u4f59\u5f26\u6b63\u5219\u5316\u635f\u5931\u4fdd\u6301\u8868\u793a\u4e00\u81f4\u6027\u3002", "motivation": "\u6301\u7eed\u89c6\u89c9\u6307\u4ee4\u8c03\u6574\uff08CVIT\uff09\u4e2d\uff0c\u707e\u96be\u6027\u9057\u5fd8\u548c\u53c2\u6570\u6548\u7387\u4f4e\u662f\u4e3b\u8981\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u5168\u5c42\u6269\u5c55\u5bfc\u81f4\u53c2\u6570\u5197\u4f59\u548c\u53ef\u6269\u5c55\u6027\u5dee\u3002", "method": "\u63d0\u51faLiLoRA\u65b9\u6cd5\uff0c\u5171\u4eabLoRA\u77e9\u9635A\uff0c\u5bf9\u77e9\u9635B\u8fdb\u884c\u4f4e\u79e9\u5206\u89e3\u4ee5\u51cf\u5c11\u4efb\u52a1\u7279\u5b9a\u53c2\u6570\uff0c\u5e76\u5f15\u5165\u4f59\u5f26\u6b63\u5219\u5316\u7a33\u5b9a\u6027\u635f\u5931\u3002", "result": "\u5728\u591a\u6837\u5316CVIT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLiLoRA\u5728\u987a\u5e8f\u4efb\u52a1\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u53c2\u6570\u6548\u7387\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "LiLoRA\u4e3aCVIT\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u5e73\u8861\u4e86\u4efb\u52a1\u5b66\u4e60\u548c\u53c2\u6570\u6548\u7387\u3002"}}
{"id": "2508.06228", "pdf": "https://arxiv.org/pdf/2508.06228", "abs": "https://arxiv.org/abs/2508.06228", "authors": ["Daniel Feijoo", "Paula Garrido-Mellado", "Jaesung Rim", "Alvaro Garcia", "Marcos V. Conde"], "title": "Towards Unified Image Deblurring using a Mixture-of-Experts Decoder", "categories": ["cs.CV"], "comment": "Preprint. Under review", "summary": "Image deblurring, removing blurring artifacts from images, is a fundamental task in computational photography and low-level computer vision. Existing approaches focus on specialized solutions tailored to particular blur types, thus, these solutions lack generalization. This limitation in current methods implies requiring multiple models to cover several blur types, which is not practical in many real scenarios. In this paper, we introduce the first all-in-one deblurring method capable of efficiently restoring images affected by diverse blur degradations, including global motion, local motion, blur in low-light conditions, and defocus blur. We propose a mixture-of-experts (MoE) decoding module, which dynamically routes image features based on the recognized blur degradation, enabling precise and efficient restoration in an end-to-end manner. Our unified approach not only achieves performance comparable to dedicated task-specific models, but also demonstrates remarkable robustness and generalization capabilities on unseen blur degradation scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u56fe\u50cf\u53bb\u6a21\u7cca\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u591a\u79cd\u6a21\u7cca\u7c7b\u578b\uff0c\u5305\u62ec\u5168\u5c40\u8fd0\u52a8\u3001\u5c40\u90e8\u8fd0\u52a8\u3001\u4f4e\u5149\u6a21\u7cca\u548c\u6563\u7126\u6a21\u7cca\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9488\u5bf9\u7279\u5b9a\u6a21\u7cca\u7c7b\u578b\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u8981\u591a\u4e2a\u6a21\u578b\u8986\u76d6\u4e0d\u540c\u6a21\u7cca\u7c7b\u578b\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\u4e0d\u5b9e\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u89e3\u7801\u6a21\u5757\uff0c\u6839\u636e\u8bc6\u522b\u7684\u6a21\u7cca\u7c7b\u578b\u52a8\u6001\u8def\u7531\u56fe\u50cf\u7279\u5f81\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u7cbe\u786e\u6062\u590d\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u5ab2\u7f8e\u4e13\u7528\u6a21\u578b\uff0c\u5e76\u5728\u672a\u89c1\u8fc7\u7684\u6a21\u7cca\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u65b9\u6cd5\u4e3a\u56fe\u50cf\u53bb\u6a21\u7cca\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06318", "pdf": "https://arxiv.org/pdf/2508.06318", "abs": "https://arxiv.org/abs/2508.06318", "authors": ["Giacomo D'Amicantonio", "Snehashis Majhi", "Quan Kong", "Lorenzo Garattoni", "Gianpiero Francesca", "Fran\u00e7ois Bremond", "Egor Bondarev"], "title": "Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video Anomaly Detection (VAD) is a challenging task due to the variability of anomalous events and the limited availability of labeled data. Under the Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided during training, while predictions are made at the frame level. Although state-of-the-art models perform well on simple anomalies (e.g., explosions), they struggle with complex real-world events (e.g., shoplifting). This difficulty stems from two key issues: (1) the inability of current models to address the diversity of anomaly types, as they process all categories with a shared model, overlooking category-specific features; and (2) the weak supervision signal, which lacks precise temporal information, limiting the ability to capture nuanced anomalous patterns blended with normal events. To address these challenges, we propose Gaussian Splatting-guided Mixture of Experts (GS-MoE), a novel framework that employs a set of expert models, each specialized in capturing specific anomaly types. These experts are guided by a temporal Gaussian splatting loss, enabling the model to leverage temporal consistency and enhance weak supervision. The Gaussian splatting approach encourages a more precise and comprehensive representation of anomalies by focusing on temporal segments most likely to contain abnormal events. The predictions from these specialized experts are integrated through a mixture-of-experts mechanism to model complex relationships across diverse anomaly patterns. Our approach achieves state-of-the-art performance, with a 91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on XD-Violence and MSAD datasets. By leveraging category-specific expertise and temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6e85\u5c04\u5f15\u5bfc\u7684\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff08GS-MoE\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u591a\u6837\u6027\u548c\u5f31\u76d1\u7763\u4fe1\u53f7\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u5f02\u5e38\u4e8b\u4ef6\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u4e3a\u6a21\u578b\u65e0\u6cd5\u533a\u5206\u5f02\u5e38\u7c7b\u578b\u7684\u591a\u6837\u6027\uff0c\u4e14\u5f31\u76d1\u7763\u4fe1\u53f7\u7f3a\u4e4f\u7cbe\u786e\u7684\u65f6\u95f4\u4fe1\u606f\u3002", "method": "GS-MoE\u6846\u67b6\u91c7\u7528\u4e00\u7ec4\u4e13\u5bb6\u6a21\u578b\uff0c\u6bcf\u4e2a\u6a21\u578b\u4e13\u6ce8\u4e8e\u7279\u5b9a\u5f02\u5e38\u7c7b\u578b\uff0c\u5e76\u901a\u8fc7\u65f6\u95f4\u9ad8\u65af\u6e85\u5c04\u635f\u5931\u589e\u5f3a\u5f31\u76d1\u7763\u4fe1\u53f7\u3002", "result": "\u5728UCF-Crime\u6570\u636e\u96c6\u4e0a\u8fbe\u523091.58%\u7684AUC\uff0c\u5728XD-Violence\u548cMSAD\u6570\u636e\u96c6\u4e0a\u4e5f\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "GS-MoE\u901a\u8fc7\u7c7b\u522b\u7279\u5b9a\u4e13\u5bb6\u548c\u65f6\u95f4\u5f15\u5bfc\uff0c\u4e3a\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2508.06327", "pdf": "https://arxiv.org/pdf/2508.06327", "abs": "https://arxiv.org/abs/2508.06327", "authors": ["Xin Ci Wong", "Duygu Sarikaya", "Kieran Zucker", "Marc De Kamps", "Nishant Ravikumar"], "title": "Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?", "categories": ["cs.CV"], "comment": "ICONIP 2025", "summary": "Magnetic resonance (MR) imaging, including cardiac MR, is prone to domain shift due to variations in imaging devices and acquisition protocols. This challenge limits the deployment of trained AI models in real-world scenarios, where performance degrades on unseen domains. Traditional solutions involve increasing the size of the dataset through ad-hoc image augmentation or additional online training/transfer learning, which have several limitations. Synthetic data offers a promising alternative, but anatomical/structural consistency constraints limit the effectiveness of generative models in creating image-label pairs. To address this, we propose a diffusion model (DM) trained on a source domain that generates synthetic cardiac MR images that resemble a given reference. The synthetic data maintains spatial and structural fidelity, ensuring similarity to the source domain and compatibility with the segmentation mask. We assess the utility of our generative approach in multi-centre cardiac MR segmentation, using the 2D nnU-Net, 3D nnU-Net and vanilla U-Net segmentation networks. We explore domain generalisation, where, domain-invariant segmentation models are trained on synthetic source domain data, and domain adaptation, where, we shift target domain data towards the source domain using the DM. Both strategies significantly improved segmentation performance on data from an unseen target domain, in terms of surface-based metrics (Welch's t-test, p < 0.01), compared to training segmentation models on real data alone. The proposed method ameliorates the need for transfer learning or online training to address domain shift challenges in cardiac MR image analysis, especially useful in data-scarce settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u751f\u6210\u5408\u6210\u5fc3\u810fMR\u56fe\u50cf\u4ee5\u89e3\u51b3\u57df\u504f\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u591a\u4e2d\u5fc3\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u5fc3\u810fMR\u6210\u50cf\u56e0\u8bbe\u5907\u548c\u534f\u8bae\u5dee\u5f02\u5bfc\u81f4\u57df\u504f\u79fb\uff0c\u9650\u5236\u4e86AI\u6a21\u578b\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u4f20\u7edf\u65b9\u6cd5\u5982\u6570\u636e\u589e\u5f3a\u6216\u8fc1\u79fb\u5b66\u4e60\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u5408\u6210\u56fe\u50cf\uff0c\u4fdd\u6301\u7ed3\u6784\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u591a\u4e2d\u5fc3\u5206\u5272\u4e2d\u7684\u6548\u679c\u3002", "result": "\u5408\u6210\u6570\u636e\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\uff08p < 0.01\uff09\uff0c\u4f18\u4e8e\u4ec5\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u57df\u504f\u79fb\u95ee\u9898\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u573a\u666f\uff0c\u65e0\u9700\u4f9d\u8d56\u8fc1\u79fb\u5b66\u4e60\u6216\u5728\u7ebf\u8bad\u7ec3\u3002"}}
{"id": "2508.06335", "pdf": "https://arxiv.org/pdf/2508.06335", "abs": "https://arxiv.org/abs/2508.06335", "authors": ["Patrick Takenaka", "Johannes Maucher", "Marco F. Huber"], "title": "ViPro-2: Unsupervised State Estimation via Integrated Dynamics for Guiding Video Prediction", "categories": ["cs.CV"], "comment": "Published in 2025 International Joint Conference on Neural Networks   (IJCNN)", "summary": "Predicting future video frames is a challenging task with many downstream applications. Previous work has shown that procedural knowledge enables deep models for complex dynamical settings, however their model ViPro assumed a given ground truth initial symbolic state. We show that this approach led to the model learning a shortcut that does not actually connect the observed environment with the predicted symbolic state, resulting in the inability to estimate states given an observation if previous states are noisy. In this work, we add several improvements to ViPro that enables the model to correctly infer states from observations without providing a full ground truth state in the beginning. We show that this is possible in an unsupervised manner, and extend the original Orbits dataset with a 3D variant to close the gap to real world scenarios.", "AI": {"tldr": "\u6539\u8fdbViPro\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u4ece\u672a\u77e5\u521d\u59cb\u72b6\u6001\u6b63\u786e\u63a8\u65ad\u89c6\u9891\u5e27\uff0c\u5e76\u5728\u65e0\u76d1\u7763\u4e0b\u5b8c\u6210\uff0c\u540c\u65f6\u6269\u5c55\u6570\u636e\u96c6\u4ee5\u9002\u5e94\u771f\u5b9e\u573a\u666f\u3002", "motivation": "\u89e3\u51b3ViPro\u6a21\u578b\u56e0\u4f9d\u8d56\u521d\u59cb\u771f\u5b9e\u72b6\u6001\u800c\u65e0\u6cd5\u5904\u7406\u566a\u58f0\u89c2\u6d4b\u7684\u95ee\u9898\uff0c\u63d0\u5347\u5176\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\u80fd\u529b\u3002", "method": "\u5728ViPro\u57fa\u7840\u4e0a\u6dfb\u52a0\u6539\u8fdb\uff0c\u4f7f\u5176\u80fd\u4ece\u672a\u77e5\u521d\u59cb\u72b6\u6001\u63a8\u65ad\u72b6\u6001\uff0c\u5e76\u4ee5\u65e0\u76d1\u7763\u65b9\u5f0f\u5b9e\u73b0\uff0c\u540c\u65f6\u6269\u5c553D\u6570\u636e\u96c6\u3002", "result": "\u6a21\u578b\u80fd\u591f\u4ece\u672a\u77e5\u521d\u59cb\u72b6\u6001\u6b63\u786e\u63a8\u65ad\u72b6\u6001\uff0c\u4e14\u5728\u566a\u58f0\u89c2\u6d4b\u4e0b\u8868\u73b0\u66f4\u7a33\u5065\u3002", "conclusion": "\u6539\u8fdb\u540e\u7684ViPro\u6a21\u578b\u5728\u65e0\u76d1\u7763\u4e0b\u80fd\u66f4\u51c6\u786e\u63a8\u65ad\u72b6\u6001\uff0c\u9002\u7528\u4e8e\u66f4\u590d\u6742\u7684\u771f\u5b9e\u573a\u666f\u3002"}}
{"id": "2508.06392", "pdf": "https://arxiv.org/pdf/2508.06392", "abs": "https://arxiv.org/abs/2508.06392", "authors": ["Wenbin Teng", "Gonglin Chen", "Haiwei Chen", "Yajie Zhao"], "title": "FVGen: Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Recent progress in 3D reconstruction has enabled realistic 3D models from dense image captures, yet challenges persist with sparse views, often leading to artifacts in unseen areas. Recent works leverage Video Diffusion Models (VDMs) to generate dense observations, filling the gaps when only sparse views are available for 3D reconstruction tasks. A significant limitation of these methods is their slow sampling speed when using VDMs. In this paper, we present FVGen, a novel framework that addresses this challenge by enabling fast novel view synthesis using VDMs in as few as four sampling steps. We propose a novel video diffusion model distillation method that distills a multi-step denoising teacher model into a few-step denoising student model using Generative Adversarial Networks (GANs) and softened reverse KL-divergence minimization. Extensive experiments on real-world datasets show that, compared to previous works, our framework generates the same number of novel views with similar (or even better) visual quality while reducing sampling time by more than 90%. FVGen significantly improves time efficiency for downstream reconstruction tasks, particularly when working with sparse input views (more than 2) where pre-trained VDMs need to be run multiple times to achieve better spatial coverage.", "AI": {"tldr": "FVGen\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u65b0\u89c6\u89d2\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u9891\u6269\u6563\u6a21\u578b\u84b8\u998f\u6280\u672f\uff0c\u5728\u4ec5\u9700\u56db\u6b65\u91c7\u6837\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u9ad8\u8d28\u91cf3D\u89c6\u56fe\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u6548\u7387\u3002", "motivation": "\u7a00\u758f\u89c6\u89d2\u4e0b\u76843D\u91cd\u5efa\u5b58\u5728\u672a\u89c2\u5bdf\u533a\u57df\u7684\u4f2a\u5f71\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08VDMs\uff09\u751f\u6210\u5bc6\u96c6\u89c2\u6d4b\uff0c\u4f46\u91c7\u6837\u901f\u5ea6\u6162\u3002", "method": "\u63d0\u51faFVGen\u6846\u67b6\uff0c\u5229\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GANs\uff09\u548c\u8f6f\u5316\u53cd\u5411KL\u6563\u5ea6\u6700\u5c0f\u5316\uff0c\u5c06\u591a\u6b65\u53bb\u566a\u6559\u5e08\u6a21\u578b\u84b8\u998f\u4e3a\u5c11\u6b65\u53bb\u566a\u5b66\u751f\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFVGen\u5728\u751f\u6210\u76f8\u540c\u6570\u91cf\u65b0\u89c6\u89d2\u65f6\uff0c\u89c6\u89c9\u8d28\u91cf\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u91c7\u6837\u65f6\u95f4\u51cf\u5c1190%\u4ee5\u4e0a\u3002", "conclusion": "FVGen\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u8f93\u5165\u89c6\u89d2\u4e0b3D\u91cd\u5efa\u4efb\u52a1\u7684\u65f6\u95f4\u6548\u7387\u3002"}}
{"id": "2508.06407", "pdf": "https://arxiv.org/pdf/2508.06407", "abs": "https://arxiv.org/abs/2508.06407", "authors": ["Ch Muhammad Awais", "Marco Reggiannini", "Davide Moroni", "Oktay Karakus"], "title": "A Classification-Aware Super-Resolution Framework for Ship Targets in SAR Imagery", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "High-resolution imagery plays a critical role in improving the performance of visual recognition tasks such as classification, detection, and segmentation. In many domains, including remote sensing and surveillance, low-resolution images can limit the accuracy of automated analysis. To address this, super-resolution (SR) techniques have been widely adopted to attempt to reconstruct high-resolution images from low-resolution inputs. Related traditional approaches focus solely on enhancing image quality based on pixel-level metrics, leaving the relationship between super-resolved image fidelity and downstream classification performance largely underexplored. This raises a key question: can integrating classification objectives directly into the super-resolution process further improve classification accuracy? In this paper, we try to respond to this question by investigating the relationship between super-resolution and classification through the deployment of a specialised algorithmic strategy. We propose a novel methodology that increases the resolution of synthetic aperture radar imagery by optimising loss functions that account for both image quality and classification performance. Our approach improves image quality, as measured by scientifically ascertained image quality indicators, while also enhancing classification accuracy.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5c06\u5206\u7c7b\u76ee\u6807\u76f4\u63a5\u878d\u5165\u8d85\u5206\u8fa8\u7387\u8fc7\u7a0b\u662f\u5426\u80fd\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u635f\u5931\u51fd\u6570\u540c\u65f6\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548c\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u9650\u5236\u4e86\u81ea\u52a8\u5316\u5206\u6790\u7684\u51c6\u786e\u6027\uff0c\u4f20\u7edf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u4ec5\u5173\u6ce8\u50cf\u7d20\u7ea7\u6307\u6807\uff0c\u672a\u5145\u5206\u63a2\u7d22\u8d85\u5206\u8fa8\u7387\u56fe\u50cf\u4fdd\u771f\u5ea6\u4e0e\u4e0b\u6e38\u5206\u7c7b\u6027\u80fd\u7684\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u540c\u65f6\u8003\u8651\u56fe\u50cf\u8d28\u91cf\u548c\u5206\u7c7b\u6027\u80fd\u7684\u635f\u5931\u51fd\u6570\uff0c\u63d0\u5347\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u56fe\u50cf\u7684\u5206\u8fa8\u7387\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u56fe\u50cf\u8d28\u91cf\uff08\u901a\u8fc7\u79d1\u5b66\u9a8c\u8bc1\u7684\u56fe\u50cf\u8d28\u91cf\u6307\u6807\uff09\uff0c\u8fd8\u589e\u5f3a\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5c06\u5206\u7c7b\u76ee\u6807\u878d\u5165\u8d85\u5206\u8fa8\u7387\u8fc7\u7a0b\u53ef\u4ee5\u540c\u65f6\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548c\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2508.06429", "pdf": "https://arxiv.org/pdf/2508.06429", "abs": "https://arxiv.org/abs/2508.06429", "authors": ["Guido Manni", "Clemente Lauretti", "Loredana Zollo", "Paolo Soda"], "title": "SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deep learning has revolutionized medical imaging, but its effectiveness is severely limited by insufficient labeled training data. This paper introduces a novel GAN-based semi-supervised learning framework specifically designed for low labeled-data regimes, evaluated across settings with 5 to 50 labeled samples per class. Our approach integrates three specialized neural networks -- a generator for class-conditioned image translation, a discriminator for authenticity assessment and classification, and a dedicated classifier -- within a three-phase training framework. The method alternates between supervised training on limited labeled data and unsupervised learning that leverages abundant unlabeled images through image-to-image translation rather than generation from noise. We employ ensemble-based pseudo-labeling that combines confidence-weighted predictions from the discriminator and classifier with temporal consistency through exponential moving averaging, enabling reliable label estimation for unlabeled data. Comprehensive evaluation across eleven MedMNIST datasets demonstrates that our approach achieves statistically significant improvements over six state-of-the-art GAN-based semi-supervised methods, with particularly strong performance in the extreme 5-shot setting where the scarcity of labeled data is most challenging. The framework maintains its superiority across all evaluated settings (5, 10, 20, and 50 shots per class). Our approach offers a practical solution for medical imaging applications where annotation costs are prohibitive, enabling robust classification performance even with minimal labeled data. Code is available at https://github.com/GuidoManni/SPARSE.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eGAN\u7684\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u9488\u5bf9\u533b\u5b66\u5f71\u50cf\u4e2d\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u96c6\u6210\u751f\u6210\u5668\u3001\u5224\u522b\u5668\u548c\u5206\u7c7b\u5668\uff0c\u7ed3\u5408\u4f2a\u6807\u7b7e\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u6837\u672c\u573a\u666f\u4e0b\u7684\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u5f71\u50cf\u4e2d\u56e0\u6807\u8bb0\u6570\u636e\u4e0d\u8db3\u800c\u5bfc\u81f4\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u53d7\u9650\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u548c\u4f2a\u6807\u7b7e\u6280\u672f\uff0c\u5229\u7528\u5c11\u91cf\u6807\u8bb0\u6570\u636e\u548c\u5927\u91cf\u672a\u6807\u8bb0\u6570\u636e\u8fdb\u884c\u534a\u76d1\u7763\u5b66\u4e60\u3002", "result": "\u572811\u4e2aMedMNIST\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u516d\u79cd\u6700\u5148\u8fdb\u7684GAN\u534a\u76d1\u7763\u65b9\u6cd5\uff0c\u5c24\u5176\u57285-shot\u6781\u7aef\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6807\u8bb0\u6210\u672c\u9ad8\u7684\u533b\u5b66\u5f71\u50cf\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5373\u4f7f\u5728\u6781\u5c11\u91cf\u6807\u8bb0\u6570\u636e\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u7a33\u5065\u5206\u7c7b\u3002"}}
{"id": "2508.06430", "pdf": "https://arxiv.org/pdf/2508.06430", "abs": "https://arxiv.org/abs/2508.06430", "authors": ["Om Patil", "Jinesh Modi", "Suryabha Mukhopadhyay", "Meghaditya Giri", "Chhavi Malhotra"], "title": "MotionSwap", "categories": ["cs.CV"], "comment": "8 pages, 7 figures, 5 tables. This is a student research submission   from BITS Pilani, Hyderabad Campus. Our implementation enhances SimSwap with   attention modules and dynamic training strategies", "summary": "Face swapping technology has gained significant attention in both academic research and commercial applications. This paper presents our implementation and enhancement of SimSwap, an efficient framework for high fidelity face swapping. We introduce several improvements to the original model, including the integration of self and cross-attention mechanisms in the generator architecture, dynamic loss weighting, and cosine annealing learning rate scheduling. These enhancements lead to significant improvements in identity preservation, attribute consistency, and overall visual quality.   Our experimental results, spanning 400,000 training iterations, demonstrate progressive improvements in generator and discriminator performance. The enhanced model achieves better identity similarity, lower FID scores, and visibly superior qualitative results compared to the baseline. Ablation studies confirm the importance of each architectural and training improvement. We conclude by identifying key future directions, such as integrating StyleGAN3, improving lip synchronization, incorporating 3D facial modeling, and introducing temporal consistency for video-based applications.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5bf9SimSwap\u6846\u67b6\u7684\u6539\u8fdb\uff0c\u901a\u8fc7\u5f15\u5165\u81ea\u6ce8\u610f\u529b\u4e0e\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u3001\u52a8\u6001\u635f\u5931\u52a0\u6743\u548c\u4f59\u5f26\u9000\u706b\u5b66\u4e60\u7387\u8c03\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6362\u8138\u6548\u679c\u3002", "motivation": "\u63d0\u5347\u6362\u8138\u6280\u672f\u5728\u8eab\u4efd\u4fdd\u7559\u3001\u5c5e\u6027\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u5728\u751f\u6210\u5668\u4e2d\u96c6\u6210\u81ea\u6ce8\u610f\u529b\u4e0e\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u91c7\u7528\u52a8\u6001\u635f\u5931\u52a0\u6743\u548c\u4f59\u5f26\u9000\u706b\u5b66\u4e60\u7387\u8c03\u5ea6\u3002", "result": "\u6539\u8fdb\u540e\u7684\u6a21\u578b\u5728\u8eab\u4efd\u76f8\u4f3c\u6027\u3001FID\u5206\u6570\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u6539\u8fdb\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u672a\u6765\u65b9\u5411\u5305\u62ec\u6574\u5408StyleGAN3\u3001\u6539\u8fdb\u5507\u540c\u6b65\u3001\u5f15\u51653D\u9762\u90e8\u5efa\u6a21\u548c\u89c6\u9891\u5e94\u7528\u4e2d\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.06494", "pdf": "https://arxiv.org/pdf/2508.06494", "abs": "https://arxiv.org/abs/2508.06494", "authors": ["Yehonathan Litman", "Fernando De la Torre", "Shubham Tulsiani"], "title": "LightSwitch: Multi-view Relighting with Material-guided Diffusion", "categories": ["cs.CV"], "comment": "ICCV 2025, Project page & Code:   https://yehonathanlitman.github.io/light_switch/", "summary": "Recent approaches for 3D relighting have shown promise in integrating 2D image relighting generative priors to alter the appearance of a 3D representation while preserving the underlying structure. Nevertheless, generative priors used for 2D relighting that directly relight from an input image do not take advantage of intrinsic properties of the subject that can be inferred or cannot consider multi-view data at scale, leading to subpar relighting. In this paper, we propose Lightswitch, a novel finetuned material-relighting diffusion framework that efficiently relights an arbitrary number of input images to a target lighting condition while incorporating cues from inferred intrinsic properties. By using multi-view and material information cues together with a scalable denoising scheme, our method consistently and efficiently relights dense multi-view data of objects with diverse material compositions. We show that our 2D relighting prediction quality exceeds previous state-of-the-art relighting priors that directly relight from images. We further demonstrate that LightSwitch matches or outperforms state-of-the-art diffusion inverse rendering methods in relighting synthetic and real objects in as little as 2 minutes.", "AI": {"tldr": "Lightswitch\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6846\u67b6\u76843D\u91cd\u5149\u7167\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u591a\u89c6\u89d2\u548c\u6750\u8d28\u4fe1\u606f\uff0c\u9ad8\u6548\u5730\u5c06\u8f93\u5165\u56fe\u50cf\u91cd\u5149\u7167\u5230\u76ee\u6807\u5149\u7167\u6761\u4ef6\u3002", "motivation": "\u73b0\u67092D\u91cd\u5149\u7167\u751f\u6210\u5148\u9a8c\u672a\u5145\u5206\u5229\u7528\u7269\u4f53\u56fa\u6709\u5c5e\u6027\u6216\u5927\u89c4\u6a21\u591a\u89c6\u89d2\u6570\u636e\uff0c\u5bfc\u81f4\u91cd\u5149\u7167\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faLightswitch\uff0c\u4e00\u79cd\u5fae\u8c03\u7684\u6750\u6599\u91cd\u5149\u7167\u6269\u6563\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u89c6\u89d2\u548c\u6750\u8d28\u4fe1\u606f\uff0c\u91c7\u7528\u53ef\u6269\u5c55\u7684\u53bb\u566a\u65b9\u6848\u3002", "result": "Lightswitch\u57282D\u91cd\u5149\u7167\u9884\u6d4b\u8d28\u91cf\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u7269\u4f53\u91cd\u5149\u7167\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Lightswitch\u901a\u8fc7\u7ed3\u5408\u56fa\u6709\u5c5e\u6027\u548c\u591a\u89c6\u89d2\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u4e00\u81f4\u7684\u91cd\u5149\u7167\u6548\u679c\u3002"}}
{"id": "2508.05658", "pdf": "https://arxiv.org/pdf/2508.05658", "abs": "https://arxiv.org/abs/2508.05658", "authors": ["Song Yan", "Hui Wei", "Jinlong Fei", "Guoliang Yang", "Zhengyu Zhao", "Zheng Wamg"], "title": "Universally Unfiltered and Unseen:Input-Agnostic Multimodal Jailbreaks against Text-to-Image Model Safeguards", "categories": ["cs.CR", "cs.CV", "cs.MM"], "comment": "ACM MM 2025", "summary": "Various (text) prompt filters and (image) safety checkers have been implemented to mitigate the misuse of Text-to-Image (T2I) models in creating Not-Safe-For-Work (NSFW) content.In order to expose potential security vulnerabilities of such safeguards, multimodal jailbreaks have been studied.However, existing jailbreaks are limited to prompt-specific and image-specific perturbations, which suffer from poor scalability and time-consuming optimization.To address these limitations, we propose Universally Unfiltered and Unseen (U3)-Attack, a multimodal jailbreak attack method against T2I safeguards.Specifically, U3-Attack optimizes an adversarial patch on the image background to universally bypass safety checkers and optimizes a safe paraphrase set from a sensitive word to universally bypass prompt filters while eliminating redundant computations.Extensive experimental results demonstrate the superiority of our U3-Attack on both open-source and commercial T2I models.For example, on the commercial Runway-inpainting model with both prompt filter and safety checker, our U3-Attack achieves $~4\\times$ higher success rates than the state-of-the-art multimodal jailbreak attack, MMA-Diffusion.Content Warning: This paper includes examples of NSFW content.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aU3-Attack\u7684\u591a\u6a21\u6001\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff0c\u7528\u4e8e\u7ed5\u8fc7\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u7684\u5b89\u5168\u68c0\u67e5\u5668\u548c\u63d0\u793a\u8fc7\u6ee4\u5668\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u6269\u5c55\u6027\u5dee\u548c\u4f18\u5316\u8017\u65f6\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\u5c40\u9650\u4e8e\u7279\u5b9a\u63d0\u793a\u548c\u56fe\u50cf\u7684\u6270\u52a8\uff0c\u6269\u5c55\u6027\u5dee\u4e14\u4f18\u5316\u8017\u65f6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u901a\u7528\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "U3-Attack\u901a\u8fc7\u4f18\u5316\u56fe\u50cf\u80cc\u666f\u7684\u5bf9\u6297\u6027\u8865\u4e01\u6765\u7ed5\u8fc7\u5b89\u5168\u68c0\u67e5\u5668\uff0c\u5e76\u4f18\u5316\u654f\u611f\u8bcd\u7684\u5b89\u5168\u91ca\u4e49\u96c6\u6765\u7ed5\u8fc7\u63d0\u793a\u8fc7\u6ee4\u5668\uff0c\u540c\u65f6\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cU3-Attack\u5728\u5f00\u6e90\u548c\u5546\u4e1aT2I\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f8b\u5982\u5728Runway-inpainting\u6a21\u578b\u4e0a\uff0c\u5176\u6210\u529f\u7387\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u9ad84\u500d\u3002", "conclusion": "U3-Attack\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u591a\u6a21\u6001\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ed5\u8fc7T2I\u6a21\u578b\u5b89\u5168\u63aa\u65bd\u7684\u6210\u529f\u7387\u3002"}}
{"id": "2508.05669", "pdf": "https://arxiv.org/pdf/2508.05669", "abs": "https://arxiv.org/abs/2508.05669", "authors": ["Jin Khye Tan", "En Jun Choong", "Ethan Jeremiah Chitty", "Yan Pheng Choo", "John Hsin Yang Wong", "Chern Eu Cheah"], "title": "Fine-Tuning Vision-Language Models for Markdown Conversion of Financial Tables in Malaysian Audited Financial Reports", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CV", "cs.LG", "I.2.7; I.7.2; J.1"], "comment": "28 pages, 14 figures, 5 tables. Evaluation code (LLM-as-a-judge and   Markdown TEDS) is available at https://github.com/jinkhye/MyFinMarkdown. The   development dataset and evaluation benchmark are available on Hugging Face at   https://huggingface.co/datasets/jinkhye/MyFinMarkdown-sample and   https://huggingface.co/datasets/jinkhye/MyFinMarkdown-bench respectively", "summary": "Accurately extracting and representing the structure of tabular data from financial documents remains a critical challenge in document understanding, particularly for regulatory and analytical use cases. This study addresses the complexity of converting financial tables from Malaysian audited financial reports into Markdown format, a task complicated by rotated layouts, multi-level headers, and implicit structural cues. We propose a fine-tuned vision-language model (VLM), based on Qwen2.5-VL-7B, optimized for high-fidelity Markdown generation from document images. Our approach includes a curated dataset of 2,152 image-text pairs with augmentations and a supervised fine-tuning strategy using LoRA. To assess performance, we evaluated our model on 100 out-of-sample tables using a dual framework: a criteria-based LLM-as-a-judge for fine-grained accuracy and our novel Markdown Tree-Edit-Distance-based Similarity (TEDS) metric for holistic structural fidelity. Our model achieves a 92.20% overall accuracy on the criteria-based assessment and a 96.53% Markdown TEDS score. This performance significantly surpasses its Qwen2.5-VL-7B base model, larger-scale VLMs, and specialized reasoning-enabled models. Compared to these self-hosted alternatives, it also significantly reduces inference time. Furthermore, its accuracy exceeds that of widely used proprietary models such as OpenAI's GPT-4o and Gemini 2.5 Flash. These results demonstrate that domain-specific fine-tuning provides an effective and efficient method to bridge the gap between unstructured financial documents and downstream automation, rivalling much larger and more general models without their computational overhead.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eQwen2.5-VL-7B\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u5c06\u9a6c\u6765\u897f\u4e9a\u5ba1\u8ba1\u8d22\u52a1\u62a5\u8868\u4e2d\u7684\u8868\u683c\u8f6c\u6362\u4e3aMarkdown\u683c\u5f0f\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u4ece\u8d22\u52a1\u6587\u6863\u4e2d\u51c6\u786e\u63d0\u53d6\u548c\u8868\u793a\u8868\u683c\u7ed3\u6784\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u76d1\u7ba1\u548c\u5206\u6790\u7528\u4f8b\u7684\u9700\u6c42\u3002", "method": "\u4f7f\u7528Qwen2.5-VL-7B\u5fae\u8c03\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u54082,152\u4e2a\u56fe\u50cf-\u6587\u672c\u5bf9\u7684\u6570\u636e\u96c6\u548cLoRA\u76d1\u7763\u5fae\u8c03\u7b56\u7565\u3002", "result": "\u6a21\u578b\u5728\u6807\u51c6\u8bc4\u4f30\u4e2d\u8fbe\u523092.20%\u7684\u51c6\u786e\u7387\u548c96.53%\u7684Markdown TEDS\u5206\u6570\uff0c\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "conclusion": "\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u662f\u8fde\u63a5\u975e\u7ed3\u6784\u5316\u8d22\u52a1\u6587\u6863\u4e0e\u4e0b\u6e38\u81ea\u52a8\u5316\u7684\u9ad8\u6548\u65b9\u6cd5\uff0c\u6027\u80fd\u4f18\u4e8e\u5927\u578b\u901a\u7528\u6a21\u578b\u3002"}}
{"id": "2508.06065", "pdf": "https://arxiv.org/pdf/2508.06065", "abs": "https://arxiv.org/abs/2508.06065", "authors": ["Daniel Lee", "Nikhil Sharma", "Donghoon Shin", "DaEun Choi", "Harsh Sharma", "Jeonghwan Kim", "Heng Ji"], "title": "ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CV", "H.5.2; I.2.7"], "comment": null, "summary": "Generative AI has made image creation more accessible, yet aligning outputs with nuanced creative intent remains challenging, particularly for non-experts. Existing tools often require users to externalize ideas through prompts or references, limiting fluid exploration. We introduce ThematicPlane, a system that enables users to navigate and manipulate high-level semantic concepts (e.g., mood, style, or narrative tone) within an interactive thematic design plane. This interface bridges the gap between tacit creative intent and system control. In our exploratory study (N=6), participants engaged in divergent and convergent creative modes, often embracing unexpected results as inspiration or iteration cues. While they grounded their exploration in familiar themes, differing expectations of how themes mapped to outputs revealed a need for more explainable controls. Overall, ThematicPlane fosters expressive, iterative workflows and highlights new directions for intuitive, semantics-driven interaction in generative design tools.", "AI": {"tldr": "ThematicPlane\u7cfb\u7edf\u901a\u8fc7\u4ea4\u4e92\u5f0f\u4e3b\u9898\u8bbe\u8ba1\u5e73\u9762\u5e2e\u52a9\u7528\u6237\u63a2\u7d22\u548c\u64cd\u4f5c\u9ad8\u7ea7\u8bed\u4e49\u6982\u5ff5\uff0c\u5f25\u5408\u521b\u610f\u610f\u56fe\u4e0e\u7cfb\u7edf\u63a7\u5236\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u751f\u6210\u5f0fAI\u4f7f\u56fe\u50cf\u521b\u4f5c\u66f4\u6613\u83b7\u53d6\uff0c\u4f46\u975e\u4e13\u5bb6\u7528\u6237\u96be\u4ee5\u7cbe\u786e\u8868\u8fbe\u521b\u610f\u610f\u56fe\uff0c\u73b0\u6709\u5de5\u5177\u9650\u5236\u4e86\u6d41\u7545\u63a2\u7d22\u3002", "method": "\u5f15\u5165ThematicPlane\u7cfb\u7edf\uff0c\u652f\u6301\u7528\u6237\u901a\u8fc7\u4ea4\u4e92\u5f0f\u4e3b\u9898\u8bbe\u8ba1\u5e73\u9762\u64cd\u4f5c\u8bed\u4e49\u6982\u5ff5\uff08\u5982\u60c5\u7eea\u3001\u98ce\u683c\u6216\u53d9\u4e8b\u57fa\u8c03\uff09\u3002", "result": "\u63a2\u7d22\u6027\u7814\u7a76\uff08N=6\uff09\u663e\u793a\uff0c\u7528\u6237\u80fd\u5728\u53d1\u6563\u548c\u6536\u655b\u521b\u610f\u6a21\u5f0f\u4e2d\u5de5\u4f5c\uff0c\u4f46\u9700\u66f4\u53ef\u89e3\u91ca\u7684\u63a7\u5236\u3002", "conclusion": "ThematicPlane\u652f\u6301\u8fed\u4ee3\u5f0f\u521b\u610f\u6d41\u7a0b\uff0c\u4e3a\u751f\u6210\u8bbe\u8ba1\u5de5\u5177\u63d0\u4f9b\u4e86\u8bed\u4e49\u9a71\u52a8\u4ea4\u4e92\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.06151", "pdf": "https://arxiv.org/pdf/2508.06151", "abs": "https://arxiv.org/abs/2508.06151", "authors": ["Yong Oh Lee", "JeeEun Kim", "Jung Woo Lee"], "title": "Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "In oral cancer diagnostics, the limited availability of annotated datasets frequently constrains the performance of diagnostic models, particularly due to the variability and insufficiency of training data. To address these challenges, this study proposed a novel approach to enhance diagnostic accuracy by synthesizing realistic oral cancer lesions using an inpainting technique with a fine-tuned diffusion model. We compiled a comprehensive dataset from multiple sources, featuring a variety of oral cancer images. Our method generated synthetic lesions that exhibit a high degree of visual fidelity to actual lesions, thereby significantly enhancing the performance of diagnostic algorithms. The results show that our classification model achieved a diagnostic accuracy of 0.97 in differentiating between cancerous and non-cancerous tissues, while our detection model accurately identified lesion locations with 0.85 accuracy. This method validates the potential for synthetic image generation in medical diagnostics and paves the way for further research into extending these methods to other types of cancer diagnostics.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u4fee\u590d\u6280\u672f\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u53e3\u8154\u764c\u75c5\u53d8\u5408\u6210\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u53e3\u8154\u764c\u8bca\u65ad\u4e2d\uff0c\u6807\u6ce8\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u6027\u548c\u591a\u6837\u6027\u4e0d\u8db3\u9650\u5236\u4e86\u8bca\u65ad\u6a21\u578b\u7684\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u589e\u5f3a\u6570\u636e\u3002", "method": "\u4f7f\u7528\u5fae\u8c03\u7684\u6269\u6563\u6a21\u578b\u548c\u56fe\u50cf\u4fee\u590d\u6280\u672f\u751f\u6210\u903c\u771f\u7684\u53e3\u8154\u764c\u75c5\u53d8\u5408\u6210\u56fe\u50cf\uff0c\u5e76\u7ed3\u5408\u591a\u6e90\u6570\u636e\u96c6\u8bad\u7ec3\u8bca\u65ad\u6a21\u578b\u3002", "result": "\u5206\u7c7b\u6a21\u578b\u5728\u533a\u5206\u764c\u53d8\u548c\u975e\u764c\u53d8\u7ec4\u7ec7\u65f6\u51c6\u786e\u7387\u8fbe0.97\uff0c\u68c0\u6d4b\u6a21\u578b\u5b9a\u4f4d\u75c5\u53d8\u7684\u51c6\u786e\u7387\u4e3a0.85\u3002", "conclusion": "\u5408\u6210\u56fe\u50cf\u751f\u6210\u5728\u533b\u5b66\u8bca\u65ad\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u764c\u75c7\u8bca\u65ad\u9886\u57df\u3002"}}
{"id": "2508.06182", "pdf": "https://arxiv.org/pdf/2508.06182", "abs": "https://arxiv.org/abs/2508.06182", "authors": ["Chiara Baldini", "Kaisar Kushibar", "Richard Osuala", "Simone Balocco", "Oliver Diaz", "Karim Lekadir", "Leonardo S. Mattos"], "title": "Clinically-guided Data Synthesis for Laryngeal Lesion Detection", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Although computer-aided diagnosis (CADx) and detection (CADe) systems have made significant progress in various medical domains, their application is still limited in specialized fields such as otorhinolaryngology. In the latter, current assessment methods heavily depend on operator expertise, and the high heterogeneity of lesions complicates diagnosis, with biopsy persisting as the gold standard despite its substantial costs and risks. A critical bottleneck for specialized endoscopic CADx/e systems is the lack of well-annotated datasets with sufficient variability for real-world generalization. This study introduces a novel approach that exploits a Latent Diffusion Model (LDM) coupled with a ControlNet adapter to generate laryngeal endoscopic image-annotation pairs, guided by clinical observations. The method addresses data scarcity by conditioning the diffusion process to produce realistic, high-quality, and clinically relevant image features that capture diverse anatomical conditions. The proposed approach can be leveraged to expand training datasets for CADx/e models, empowering the assessment process in laryngology. Indeed, during a downstream task of detection, the addition of only 10% synthetic data improved the detection rate of laryngeal lesions by 9% when the model was internally tested and 22.1% on out-of-domain external data. Additionally, the realism of the generated images was evaluated by asking 5 expert otorhinolaryngologists with varying expertise to rate their confidence in distinguishing synthetic from real images. This work has the potential to accelerate the development of automated tools for laryngeal disease diagnosis, offering a solution to data scarcity and demonstrating the applicability of synthetic data in real-world scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\u548cControlNet\u9002\u914d\u5668\u751f\u6210\u5589\u955c\u56fe\u50cf-\u6807\u6ce8\u5bf9\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5589\u955cCADx/e\u7cfb\u7edf\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u6dfb\u52a010%\u5408\u6210\u6570\u636e\u5373\u53ef\u663e\u8457\u63d0\u5347\u75c5\u53d8\u68c0\u6d4b\u7387\u3002", "motivation": "\u5589\u955cCADx/e\u7cfb\u7edf\u56e0\u6570\u636e\u7a00\u7f3a\u548c\u75c5\u53d8\u5f02\u8d28\u6027\u9ad8\u800c\u53d1\u5c55\u53d7\u9650\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u7ecf\u9a8c\u548c\u6d3b\u68c0\uff0c\u6210\u672c\u9ad8\u4e14\u98ce\u9669\u5927\u3002", "method": "\u7ed3\u5408LDM\u548cControlNet\u9002\u914d\u5668\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u4e34\u5e8a\u76f8\u5173\u7684\u5589\u955c\u56fe\u50cf-\u6807\u6ce8\u5bf9\uff0c\u6269\u5c55\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "result": "\u6dfb\u52a010%\u5408\u6210\u6570\u636e\u540e\uff0c\u75c5\u53d8\u68c0\u6d4b\u7387\u5728\u5185\u90e8\u6d4b\u8bd5\u4e2d\u63d0\u53479%\uff0c\u5916\u90e8\u6570\u636e\u4e2d\u63d0\u534722.1%\uff1b\u751f\u6210\u56fe\u50cf\u7684\u771f\u5b9e\u6027\u5f97\u5230\u4e13\u5bb6\u8ba4\u53ef\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5408\u6210\u6570\u636e\u5728\u5589\u955cCADx/e\u7cfb\u7edf\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.06325", "pdf": "https://arxiv.org/pdf/2508.06325", "abs": "https://arxiv.org/abs/2508.06325", "authors": ["Zelin Li", "Ruohan Zong", "Yifan Liu", "Ruichen Yao", "Yaokun Liu", "Yang Zhang", "Dong Wang"], "title": "Anti-Tamper Protection for Unauthorized Individual Image Generation", "categories": ["cs.CR", "cs.CV"], "comment": "22 pages ,22 figures, Paper has been accepted by ICCV'2025", "summary": "With the advancement of personalized image generation technologies, concerns about forgery attacks that infringe on portrait rights and privacy are growing. To address these concerns, protection perturbation algorithms have been developed to disrupt forgery generation. However, the protection algorithms would become ineffective when forgery attackers apply purification techniques to bypass the protection. To address this issue, we present a novel approach, Anti-Tamper Perturbation (ATP). ATP introduces a tamper-proof mechanism within the perturbation. It consists of protection and authorization perturbations, where the protection perturbation defends against forgery attacks, while the authorization perturbation detects purification-based tampering. Both protection and authorization perturbations are applied in the frequency domain under the guidance of a mask, ensuring that the protection perturbation does not disrupt the authorization perturbation. This design also enables the authorization perturbation to be distributed across all image pixels, preserving its sensitivity to purification-based tampering. ATP demonstrates its effectiveness in defending forgery attacks across various attack settings through extensive experiments, providing a robust solution for protecting individuals' portrait rights and privacy. Our code is available at: https://github.com/Seeyn/Anti-Tamper-Perturbation .", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u6297\u7be1\u6539\u6270\u52a8\uff08ATP\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u4fdd\u62a4\u6270\u52a8\u548c\u6388\u6743\u6270\u52a8\uff0c\u6709\u6548\u9632\u5fa1\u4f2a\u9020\u653b\u51fb\u5e76\u68c0\u6d4b\u51c0\u5316\u7be1\u6539\u3002", "motivation": "\u968f\u7740\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4f2a\u9020\u653b\u51fb\u4fb5\u72af\u8096\u50cf\u6743\u548c\u9690\u79c1\u7684\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\uff0c\u73b0\u6709\u4fdd\u62a4\u6270\u52a8\u7b97\u6cd5\u6613\u88ab\u51c0\u5316\u6280\u672f\u7ed5\u8fc7\u3002", "method": "ATP\u5728\u9891\u57df\u4e2d\u5f15\u5165\u4fdd\u62a4\u6270\u52a8\u548c\u6388\u6743\u6270\u52a8\uff0c\u901a\u8fc7\u63a9\u6a21\u6307\u5bfc\u786e\u4fdd\u4e24\u8005\u4e92\u4e0d\u5e72\u6270\uff0c\u6388\u6743\u6270\u52a8\u5206\u5e03\u5728\u5168\u56fe\u50cf\u7d20\u4ee5\u4fdd\u6301\u5bf9\u51c0\u5316\u7be1\u6539\u7684\u654f\u611f\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cATP\u5728\u5404\u79cd\u653b\u51fb\u573a\u666f\u4e0b\u5747\u80fd\u6709\u6548\u9632\u5fa1\u4f2a\u9020\u653b\u51fb\uff0c\u4e3a\u8096\u50cf\u6743\u548c\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u9c81\u68d2\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "ATP\u901a\u8fc7\u53cc\u91cd\u6270\u52a8\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4fdd\u62a4\u7b97\u6cd5\u6613\u88ab\u7ed5\u8fc7\u7684\u95ee\u9898\uff0c\u4e3a\u56fe\u50cf\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.06490", "pdf": "https://arxiv.org/pdf/2508.06490", "abs": "https://arxiv.org/abs/2508.06490", "authors": ["Stanislas Ducotterd", "Michael Unser"], "title": "Multivariate Fields of Experts", "categories": ["eess.IV", "cs.CV", "cs.LG", "eess.SP"], "comment": null, "summary": "We introduce the multivariate fields of experts, a new framework for the learning of image priors. Our model generalizes existing fields of experts methods by incorporating multivariate potential functions constructed via Moreau envelopes of the $\\ell_\\infty$-norm. We demonstrate the effectiveness of our proposal across a range of inverse problems that include image denoising, deblurring, compressed-sensing magnetic-resonance imaging, and computed tomography. The proposed approach outperforms comparable univariate models and achieves performance close to that of deep-learning-based regularizers while being significantly faster, requiring fewer parameters, and being trained on substantially fewer data. In addition, our model retains a relatively high level of interpretability due to its structured design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u5143\u4e13\u5bb6\u573a\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u56fe\u50cf\u5148\u9a8c\uff0c\u901a\u8fc7\u5f15\u5165\u591a\u5143\u52bf\u51fd\u6570\u6539\u8fdb\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u9006\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6539\u8fdb\u73b0\u6709\u4e13\u5bb6\u573a\u65b9\u6cd5\uff0c\u63d0\u5347\u56fe\u50cf\u5148\u9a8c\u5b66\u4e60\u7684\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5229\u7528Moreau\u5305\u7edc\u6784\u9020\u591a\u5143\u52bf\u51fd\u6570\uff0c\u6269\u5c55\u4e86\u4e13\u5bb6\u573a\u6a21\u578b\uff0c\u5e94\u7528\u4e8e\u56fe\u50cf\u53bb\u566a\u3001\u53bb\u6a21\u7cca\u3001\u538b\u7f29\u611f\u77e5MRI\u548cCT\u7b49\u9006\u95ee\u9898\u3002", "result": "\u6027\u80fd\u4f18\u4e8e\u5355\u53d8\u91cf\u6a21\u578b\uff0c\u63a5\u8fd1\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u4f46\u901f\u5ea6\u66f4\u5feb\u3001\u53c2\u6570\u66f4\u5c11\u3001\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u66f4\u4f4e\uff0c\u4e14\u4fdd\u6301\u8f83\u9ad8\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u591a\u5143\u4e13\u5bb6\u573a\u6846\u67b6\u5728\u9006\u95ee\u9898\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u517c\u5177\u9ad8\u6548\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u56fe\u50cf\u5148\u9a8c\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
