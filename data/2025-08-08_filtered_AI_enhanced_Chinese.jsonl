{"id": "2508.04965", "pdf": "https://arxiv.org/pdf/2508.04965", "abs": "https://arxiv.org/abs/2508.04965", "authors": ["Zijian Wang", "Beizhen Zhao", "Hao Wang"], "title": "Perceive-Sample-Compress: Towards Real-Time 3D Gaussian Splatting", "categories": ["cs.GR", "cs.CV", "cs.MM"], "comment": null, "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated remarkable capabilities in real-time and photorealistic novel view synthesis. However, traditional 3DGS representations often struggle with large-scale scene management and efficient storage, particularly when dealing with complex environments or limited computational resources. To address these limitations, we introduce a novel perceive-sample-compress framework for 3D Gaussian Splatting. Specifically, we propose a scene perception compensation algorithm that intelligently refines Gaussian parameters at each level. This algorithm intelligently prioritizes visual importance for higher fidelity rendering in critical areas, while optimizing resource usage and improving overall visible quality. Furthermore, we propose a pyramid sampling representation to manage Gaussian primitives across hierarchical levels. Finally, to facilitate efficient storage of proposed hierarchical pyramid representations, we develop a Generalized Gaussian Mixed model compression algorithm to achieve significant compression ratios without sacrificing visual fidelity. The extensive experiments demonstrate that our method significantly improves memory efficiency and high visual quality while maintaining real-time rendering speed.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u611f\u77e5-\u91c7\u6837-\u538b\u7f29\u6846\u67b6\uff0c\u4f18\u53163D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u63d0\u5347\u5927\u573a\u666f\u7ba1\u7406\u548c\u5b58\u50a8\u6548\u7387\u3002", "motivation": "\u4f20\u7edf3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5728\u5927\u573a\u666f\u7ba1\u7406\u548c\u5b58\u50a8\u6548\u7387\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u5728\u590d\u6742\u73af\u5883\u6216\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u573a\u666f\u611f\u77e5\u8865\u507f\u7b97\u6cd5\u548c\u91d1\u5b57\u5854\u91c7\u6837\u8868\u793a\uff0c\u7ed3\u5408\u5e7f\u4e49\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u538b\u7f29\u7b97\u6cd5\uff0c\u4f18\u5316\u8d44\u6e90\u5206\u914d\u548c\u5b58\u50a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u5185\u5b58\u6548\u7387\u548c\u89c6\u89c9\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6e32\u67d3\u901f\u5ea6\u3002", "conclusion": "\u65b0\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e863D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u7684\u5b58\u50a8\u548c\u7ba1\u7406\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002"}}
{"id": "2508.04966", "pdf": "https://arxiv.org/pdf/2508.04966", "abs": "https://arxiv.org/abs/2508.04966", "authors": ["Yifan Zhou", "Beizhen Zhao", "Pengcheng Wu", "Hao Wang"], "title": "Laplacian Analysis Meets Dynamics Modelling: Gaussian Splatting for 4D Reconstruction", "categories": ["cs.GR", "cs.CV", "cs.MM"], "comment": null, "summary": "While 3D Gaussian Splatting (3DGS) excels in static scene modeling, its extension to dynamic scenes introduces significant challenges. Existing dynamic 3DGS methods suffer from either over-smoothing due to low-rank decomposition or feature collision from high-dimensional grid sampling. This is because of the inherent spectral conflicts between preserving motion details and maintaining deformation consistency at different frequency. To address these challenges, we propose a novel dynamic 3DGS framework with hybrid explicit-implicit functions. Our approach contains three key innovations: a spectral-aware Laplacian encoding architecture which merges Hash encoding and Laplacian-based module for flexible frequency motion control, an enhanced Gaussian dynamics attribute that compensates for photometric distortions caused by geometric deformation, and an adaptive Gaussian split strategy guided by KDTree-based primitive control to efficiently query and optimize dynamic areas. Through extensive experiments, our method demonstrates state-of-the-art performance in reconstructing complex dynamic scenes, achieving better reconstruction fidelity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u663e\u5f0f-\u9690\u5f0f\u51fd\u6570\u7684\u52a8\u60013D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u573a\u666f\u4e2d\u7684\u8fc7\u5e73\u6ed1\u548c\u7279\u5f81\u51b2\u7a81\u95ee\u9898\u3002", "motivation": "\u52a8\u60013D\u9ad8\u65af\u6cfc\u6e85\u5728\u5efa\u6a21\u52a8\u6001\u573a\u666f\u65f6\u5b58\u5728\u8fc7\u5e73\u6ed1\u6216\u7279\u5f81\u51b2\u7a81\u7684\u6311\u6218\uff0c\u9700\u5e73\u8861\u8fd0\u52a8\u7ec6\u8282\u548c\u53d8\u5f62\u4e00\u81f4\u6027\u3002", "method": "\u91c7\u7528\u6df7\u5408\u663e\u5f0f-\u9690\u5f0f\u51fd\u6570\uff0c\u5305\u62ec\u9891\u8c31\u611f\u77e5\u7684\u62c9\u666e\u62c9\u65af\u7f16\u7801\u67b6\u6784\u3001\u589e\u5f3a\u7684\u9ad8\u65af\u52a8\u6001\u5c5e\u6027\u53ca\u81ea\u9002\u5e94\u9ad8\u65af\u5206\u5272\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u5347\u4e86\u91cd\u5efa\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u60013D\u9ad8\u65af\u6cfc\u6e85\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u52a8\u6001\u573a\u666f\u5efa\u6a21\u3002"}}
{"id": "2508.05064", "pdf": "https://arxiv.org/pdf/2508.05064", "abs": "https://arxiv.org/abs/2508.05064", "authors": ["Mahmoud Chick Zaouali", "Todd Charter", "Yehor Karpichev", "Brandon Haworth", "Homayoun Najjjaran"], "title": "A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding", "categories": ["cs.GR", "cs.CL", "cs.CV"], "comment": null, "summary": "Gaussian Splatting has rapidly emerged as a transformative technique for real-time 3D scene representation, offering a highly efficient and expressive alternative to Neural Radiance Fields (NeRF). Its ability to render complex scenes with high fidelity has enabled progress across domains such as scene reconstruction, robotics, and interactive content creation. More recently, the integration of Large Language Models (LLMs) and language embeddings into Gaussian Splatting pipelines has opened new possibilities for text-conditioned generation, editing, and semantic scene understanding. Despite these advances, a comprehensive overview of this emerging intersection has been lacking. This survey presents a structured review of current research efforts that combine language guidance with 3D Gaussian Splatting, detailing theoretical foundations, integration strategies, and real-world use cases. We highlight key limitations such as computational bottlenecks, generalizability, and the scarcity of semantically annotated 3D Gaussian data and outline open challenges and future directions for advancing language-guided 3D scene understanding using Gaussian Splatting.", "AI": {"tldr": "\u7efc\u8ff0\u63a2\u8ba8\u4e86\u8bed\u8a00\u6a21\u578b\u4e0e3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u7684\u7ed3\u5408\uff0c\u5206\u6790\u4e86\u5176\u7406\u8bba\u3001\u5e94\u7528\u53ca\u6311\u6218\u3002", "motivation": "\u586b\u8865\u8bed\u8a00\u5f15\u5bfc\u4e0e3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u7ed3\u5408\u7684\u7efc\u8ff0\u7a7a\u767d\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u3002", "method": "\u7ed3\u6784\u5316\u56de\u987e\u73b0\u6709\u7814\u7a76\uff0c\u5305\u62ec\u7406\u8bba\u57fa\u7840\u3001\u96c6\u6210\u7b56\u7565\u548c\u5b9e\u9645\u7528\u4f8b\u3002", "result": "\u603b\u7ed3\u4e86\u6280\u672f\u4f18\u52bf\uff08\u5982\u9ad8\u4fdd\u771f\u6e32\u67d3\uff09\u548c\u5c40\u9650\u6027\uff08\u5982\u8ba1\u7b97\u74f6\u9888\u3001\u6570\u636e\u7a00\u7f3a\uff09\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u63a8\u52a8\u8bed\u8a00\u5f15\u5bfc\u76843D\u573a\u666f\u7406\u89e3\u3002"}}
{"id": "2508.04797", "pdf": "https://arxiv.org/pdf/2508.04797", "abs": "https://arxiv.org/abs/2508.04797", "authors": ["Mohab Kishawy", "Ali Abdellatif Hussein", "Jun Chen"], "title": "RetinexDual: Retinex-based Dual Nature Approach for Generalized Ultra-High-Definition Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Advancements in image sensing have elevated the importance of Ultra-High-Definition Image Restoration (UHD IR). Traditional methods, such as extreme downsampling or transformation from the spatial to the frequency domain, encounter significant drawbacks: downsampling induces irreversible information loss in UHD images, while our frequency analysis reveals that pure frequency-domain approaches are ineffective for spatially confined image artifacts, primarily due to the loss of degradation locality. To overcome these limitations, we present RetinexDual, a novel Retinex theory-based framework designed for generalized UHD IR tasks. RetinexDual leverages two complementary sub-networks: the Scale-Attentive maMBA (SAMBA) and the Frequency Illumination Adaptor (FIA). SAMBA, responsible for correcting the reflectance component, utilizes a coarse-to-fine mechanism to overcome the causal modeling of mamba, which effectively reduces artifacts and restores intricate details. On the other hand, FIA ensures precise correction of color and illumination distortions by operating in the frequency domain and leveraging the global context provided by it. Evaluating RetinexDual on four UHD IR tasks, namely deraining, deblurring, dehazing, and Low-Light Image Enhancement (LLIE), shows that it outperforms recent methods qualitatively and quantitatively. Ablation studies demonstrate the importance of employing distinct designs for each branch in RetinexDual, as well as the effectiveness of its various components.", "AI": {"tldr": "RetinexDual\u662f\u4e00\u79cd\u57fa\u4e8eRetinex\u7406\u8bba\u7684\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u8d85\u9ad8\u6e05\u56fe\u50cf\u6062\u590d\u4efb\u52a1\uff0c\u901a\u8fc7\u4e24\u4e2a\u4e92\u8865\u5b50\u7f51\u7edc\uff08SAMBA\u548cFIA\uff09\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u4e0b\u91c7\u6837\u6216\u9891\u57df\u8f6c\u6362\uff09\u5728\u8d85\u9ad8\u6e05\u56fe\u50cf\u6062\u590d\u4e2d\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u6216\u5c40\u90e8\u9000\u5316\u95ee\u9898\uff0cRetinexDual\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "RetinexDual\u7ed3\u5408\u4e86Scale-Attentive maMBA\uff08SAMBA\uff09\u548cFrequency Illumination Adaptor\uff08FIA\uff09\u4e24\u4e2a\u5b50\u7f51\u7edc\uff0c\u5206\u522b\u5904\u7406\u53cd\u5c04\u5206\u91cf\u548c\u9891\u57df\u4e2d\u7684\u989c\u8272\u4e0e\u5149\u7167\u6821\u6b63\u3002", "result": "\u5728\u53bb\u96e8\u3001\u53bb\u6a21\u7cca\u3001\u53bb\u96fe\u548c\u4f4e\u5149\u589e\u5f3a\u7b49\u4efb\u52a1\u4e2d\uff0cRetinexDual\u5728\u8d28\u91cf\u548c\u6570\u91cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RetinexDual\u901a\u8fc7\u4e92\u8865\u5b50\u7f51\u7edc\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u8d85\u9ad8\u6e05\u56fe\u50cf\u6062\u590d\u95ee\u9898\uff0c\u5404\u7ec4\u4ef6\u7684\u4f5c\u7528\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2508.05187", "pdf": "https://arxiv.org/pdf/2508.05187", "abs": "https://arxiv.org/abs/2508.05187", "authors": ["Mohamed Abdul Gafoor", "Marius Preda", "Titus Zaharia"], "title": "Refining Gaussian Splatting: A Volumetric Densification Approach", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Achieving high-quality novel view synthesis in 3D Gaussian Splatting (3DGS) often depends on effective point primitive management. The underlying Adaptive Density Control (ADC) process addresses this issue by automating densification and pruning. Yet, the vanilla 3DGS densification strategy shows key shortcomings. To address this issue, in this paper we introduce a novel density control method, which exploits the volumes of inertia associated to each Gaussian function to guide the refinement process. Furthermore, we study the effect of both traditional Structure from Motion (SfM) and Deep Image Matching (DIM) methods for point cloud initialization. Extensive experimental evaluations on the Mip-NeRF 360 dataset demonstrate that our approach surpasses 3DGS in reconstruction quality, delivering encouraging performance across diverse scenes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u60ef\u6027\u4f53\u79ef\u7684\u65b0\u578b\u5bc6\u5ea6\u63a7\u5236\u65b9\u6cd5\uff0c\u6539\u8fdb\u4e863D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u4e2d\u7684\u70b9\u57fa\u5143\u7ba1\u7406\uff0c\u63d0\u5347\u4e86\u65b0\u89c6\u89d2\u5408\u6210\u7684\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b33DGS\u4e2d\u81ea\u9002\u5e94\u5bc6\u5ea6\u63a7\u5236\uff08ADC\uff09\u7684\u4e0d\u8db3\uff0c\u4f18\u5316\u70b9\u57fa\u5143\u7ba1\u7406\u4ee5\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u3002", "method": "\u5229\u7528\u9ad8\u65af\u51fd\u6570\u7684\u60ef\u6027\u4f53\u79ef\u6307\u5bfc\u7ec6\u5316\u8fc7\u7a0b\uff0c\u5e76\u6bd4\u8f83\u4e86\u4f20\u7edfSfM\u548c\u6df1\u5ea6\u56fe\u50cf\u5339\u914d\uff08DIM\uff09\u5bf9\u70b9\u4e91\u521d\u59cb\u5316\u7684\u5f71\u54cd\u3002", "result": "\u5728Mip-NeRF 360\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u91cd\u5efa\u8d28\u91cf\u4e0a\u4f18\u4e8e3DGS\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\u3002", "conclusion": "\u63d0\u51fa\u7684\u5bc6\u5ea6\u63a7\u5236\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e863DGS\u7684\u6027\u80fd\uff0c\u4e3a\u65b0\u89c6\u89d2\u5408\u6210\u63d0\u4f9b\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.04818", "pdf": "https://arxiv.org/pdf/2508.04818", "abs": "https://arxiv.org/abs/2508.04818", "authors": ["Mehrdad Moradi", "Marco Grasso", "Bianca Maria Colosimo", "Kamran Paynabar"], "title": "Single-Step Reconstruction-Free Anomaly Detection and Segmentation via Diffusion Models", "categories": ["cs.CV", "eess.IV", "stat.ML", "62H35, 68T07, 62M40, 68T45", "I.2.6; I.2.10; I.4.6; I.4.8; I.5.1; I.5.4"], "comment": "9 pages, 8 figures, 2 tables. Submitted to an IEEE conference", "summary": "Generative models have demonstrated significant success in anomaly detection and segmentation over the past decade. Recently, diffusion models have emerged as a powerful alternative, outperforming previous approaches such as GANs and VAEs. In typical diffusion-based anomaly detection, a model is trained on normal data, and during inference, anomalous images are perturbed to a predefined intermediate step in the forward diffusion process. The corresponding normal image is then reconstructed through iterative reverse sampling.   However, reconstruction-based approaches present three major challenges: (1) the reconstruction process is computationally expensive due to multiple sampling steps, making real-time applications impractical; (2) for complex or subtle patterns, the reconstructed image may correspond to a different normal pattern rather than the original input; and (3) Choosing an appropriate intermediate noise level is challenging because it is application-dependent and often assumes prior knowledge of anomalies, an assumption that does not hold in unsupervised settings.   We introduce Reconstruction-free Anomaly Detection with Attention-based diffusion models in Real-time (RADAR), which overcomes the limitations of reconstruction-based anomaly detection. Unlike current SOTA methods that reconstruct the input image, RADAR directly produces anomaly maps from the diffusion model, improving both detection accuracy and computational efficiency. We evaluate RADAR on real-world 3D-printed material and the MVTec-AD dataset. Our approach surpasses state-of-the-art diffusion-based and statistical machine learning models across all key metrics, including accuracy, precision, recall, and F1 score. Specifically, RADAR improves F1 score by 7% on MVTec-AD and 13% on the 3D-printed material dataset compared to the next best model.   Code available at: https://github.com/mehrdadmoradi124/RADAR", "AI": {"tldr": "RADAR\u662f\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u6269\u6563\u6a21\u578b\u7684\u65e0\u91cd\u5efa\u5b9e\u65f6\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6269\u6563\u6a21\u578b\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u91cd\u5efa\u6548\u679c\u5dee\u548c\u566a\u58f0\u9009\u62e9\u96be\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u91cd\u5efa\u7684\u6269\u6563\u6a21\u578b\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u91cd\u5efa\u6548\u679c\u4e0d\u7406\u60f3\u548c\u566a\u58f0\u9009\u62e9\u4f9d\u8d56\u5148\u9a8c\u77e5\u8bc6\u7684\u95ee\u9898\uff0cRADAR\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "RADAR\u76f4\u63a5\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u5f02\u5e38\u56fe\uff0c\u907f\u514d\u4e86\u91cd\u5efa\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "result": "\u5728MVTec-AD\u548c3D\u6253\u5370\u6750\u6599\u6570\u636e\u96c6\u4e0a\uff0cRADAR\u5728\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cF1\u5206\u6570\u5206\u522b\u63d0\u9ad8\u4e867%\u548c13%\u3002", "conclusion": "RADAR\u901a\u8fc7\u65e0\u91cd\u5efa\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2508.04732", "pdf": "https://arxiv.org/pdf/2508.04732", "abs": "https://arxiv.org/abs/2508.04732", "authors": ["Xiaoqi Dong", "Xiangyu Zhou", "Nicholas Evans", "Yujia Lin"], "title": "LumiGen: An LVLM-Enhanced Iterative Framework for Fine-Grained Text-to-Image Generation", "categories": ["cs.LG", "cs.GR"], "comment": null, "summary": "Text-to-Image (T2I) generation has made significant advancements with diffusion models, yet challenges persist in handling complex instructions, ensuring fine-grained content control, and maintaining deep semantic consistency. Existing T2I models often struggle with tasks like accurate text rendering, precise pose generation, or intricate compositional coherence. Concurrently, Vision-Language Models (LVLMs) have demonstrated powerful capabilities in cross-modal understanding and instruction following. We propose LumiGen, a novel LVLM-enhanced iterative framework designed to elevate T2I model performance, particularly in areas requiring fine-grained control, through a closed-loop, LVLM-driven feedback mechanism. LumiGen comprises an Intelligent Prompt Parsing & Augmentation (IPPA) module for proactive prompt enhancement and an Iterative Visual Feedback & Refinement (IVFR) module, which acts as a \"visual critic\" to iteratively correct and optimize generated images. Evaluated on the challenging LongBench-T2I Benchmark, LumiGen achieves a superior average score of 3.08, outperforming state-of-the-art baselines. Notably, our framework demonstrates significant improvements in critical dimensions such as text rendering and pose expression, validating the effectiveness of LVLM integration for more controllable and higher-quality image generation.", "AI": {"tldr": "LumiGen\u662f\u4e00\u4e2a\u57fa\u4e8eLVLM\u7684\u8fed\u4ee3\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u63d0\u793a\u89e3\u6790\u548c\u89c6\u89c9\u53cd\u9988\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86T2I\u6a21\u578b\u5728\u590d\u6742\u6307\u4ee4\u548c\u7cbe\u7ec6\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709T2I\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u6307\u4ee4\u3001\u7cbe\u7ec6\u5185\u5bb9\u63a7\u5236\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cLVLM\u7684\u8de8\u6a21\u6001\u7406\u89e3\u548c\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002", "method": "LumiGen\u5305\u542bIPPA\u6a21\u5757\uff08\u667a\u80fd\u63d0\u793a\u89e3\u6790\u4e0e\u589e\u5f3a\uff09\u548cIVFR\u6a21\u5757\uff08\u8fed\u4ee3\u89c6\u89c9\u53cd\u9988\u4e0e\u4f18\u5316\uff09\uff0c\u901a\u8fc7\u95ed\u73af\u53cd\u9988\u673a\u5236\u63d0\u5347\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002", "result": "\u5728LongBench-T2I Benchmark\u4e0a\uff0cLumiGen\u5e73\u5747\u5f97\u52063.08\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5c24\u5176\u5728\u6587\u672c\u6e32\u67d3\u548c\u59ff\u6001\u8868\u8fbe\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "LVLM\u7684\u96c6\u6210\u663e\u8457\u63d0\u5347\u4e86T2I\u6a21\u578b\u7684\u53ef\u63a7\u6027\u548c\u751f\u6210\u8d28\u91cf\uff0c\u9a8c\u8bc1\u4e86LumiGen\u6846\u67b6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.04968", "pdf": "https://arxiv.org/pdf/2508.04968", "abs": "https://arxiv.org/abs/2508.04968", "authors": ["Zhihao Guo", "Peng Wang", "Zidong Chen", "Xiangyu Kong", "Yan Lyu", "Guanyu Gao", "Liangxiu Han"], "title": "UGOD: Uncertainty-Guided Differentiable Opacity and Soft Dropout for Enhanced Sparse-View 3DGS", "categories": ["cs.CV", "cs.AI", "I.4.8; I.2.10; I.5.1"], "comment": "11 pages, 5 figures", "summary": "3D Gaussian Splatting (3DGS) has become a competitive approach for novel view synthesis (NVS) due to its advanced rendering efficiency through 3D Gaussian projection and blending. However, Gaussians are treated equally weighted for rendering in most 3DGS methods, making them prone to overfitting, which is particularly the case in sparse-view scenarios. To address this, we investigate how adaptive weighting of Gaussians affects rendering quality, which is characterised by learned uncertainties proposed. This learned uncertainty serves two key purposes: first, it guides the differentiable update of Gaussian opacity while preserving the 3DGS pipeline integrity; second, the uncertainty undergoes soft differentiable dropout regularisation, which strategically transforms the original uncertainty into continuous drop probabilities that govern the final Gaussian projection and blending process for rendering. Extensive experimental results over widely adopted datasets demonstrate that our method outperforms rivals in sparse-view 3D synthesis, achieving higher quality reconstruction with fewer Gaussians in most datasets compared to existing sparse-view approaches, e.g., compared to DropGaussian, our method achieves 3.27\\% PSNR improvements on the MipNeRF 360 dataset.", "AI": {"tldr": "3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u5728\u7a00\u758f\u89c6\u89d2\u573a\u666f\u4e2d\u6613\u8fc7\u62df\u5408\uff0c\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u81ea\u9002\u5e94\u6743\u91cd\u548c\u4e0d\u786e\u5b9a\u6027\u5b66\u4e60\u63d0\u5347\u6e32\u67d3\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b33DGS\u5728\u7a00\u758f\u89c6\u89d2\u4e0b\u56e0\u9ad8\u65af\u6743\u91cd\u5747\u7b49\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\u95ee\u9898\u3002", "method": "\u5f15\u5165\u5b66\u4e60\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u6307\u5bfc\u9ad8\u65af\u4e0d\u900f\u660e\u5ea6\u7684\u53ef\u5fae\u66f4\u65b0\uff0c\u5e76\u901a\u8fc7\u8f6f\u53ef\u5fae\u4e22\u5f03\u6b63\u5219\u5316\u4f18\u5316\u6e32\u67d3\u8fc7\u7a0b\u3002", "result": "\u5728\u7a00\u758f\u89c6\u89d23D\u5408\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cPSNR\u63d0\u53473.27%\uff08MipNeRF 360\u6570\u636e\u96c6\uff09\u3002", "conclusion": "\u81ea\u9002\u5e94\u6743\u91cd\u548c\u4e0d\u786e\u5b9a\u6027\u5b66\u4e60\u663e\u8457\u63d0\u5347\u7a00\u758f\u89c6\u89d2\u4e0b\u7684\u6e32\u67d3\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2508.04979", "pdf": "https://arxiv.org/pdf/2508.04979", "abs": "https://arxiv.org/abs/2508.04979", "authors": ["Zheng Chen", "Mingde Zhou", "Jinpei Guo", "Jiale Yuan", "Yifei Ji", "Yulun Zhang"], "title": "Steering One-Step Diffusion Model with Fidelity-Rich Decoder for Fast Image Compression", "categories": ["cs.CV"], "comment": "Code is available at: https://github.com/zhengchen1999/SODEC", "summary": "Diffusion-based image compression has demonstrated impressive perceptual performance. However, it suffers from two critical drawbacks: (1) excessive decoding latency due to multi-step sampling, and (2) poor fidelity resulting from over-reliance on generative priors. To address these issues, we propose SODEC, a novel single-step diffusion image compression model. We argue that in image compression, a sufficiently informative latent renders multi-step refinement unnecessary. Based on this insight, we leverage a pre-trained VAE-based model to produce latents with rich information, and replace the iterative denoising process with a single-step decoding. Meanwhile, to improve fidelity, we introduce the fidelity guidance module, encouraging output that is faithful to the original image. Furthermore, we design the rate annealing training strategy to enable effective training under extremely low bitrates. Extensive experiments show that SODEC significantly outperforms existing methods, achieving superior rate-distortion-perception performance. Moreover, compared to previous diffusion-based compression models, SODEC improves decoding speed by more than 20$\\times$. Code is released at: https://github.com/zhengchen1999/SODEC.", "AI": {"tldr": "SODEC\u662f\u4e00\u79cd\u65b0\u578b\u7684\u5355\u6b65\u6269\u6563\u56fe\u50cf\u538b\u7f29\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6269\u6563\u6a21\u578b\u89e3\u7801\u5ef6\u8fdf\u9ad8\u548c\u4fdd\u771f\u5ea6\u4f4e\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5355\u6b65\u89e3\u7801\u548c\u4fdd\u771f\u5ea6\u5f15\u5bfc\u6a21\u5757\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6269\u6563\u56fe\u50cf\u538b\u7f29\u6a21\u578b\u5b58\u5728\u89e3\u7801\u5ef6\u8fdf\u9ad8\u548c\u4fdd\u771f\u5ea6\u4f4e\u7684\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684VAE\u6a21\u578b\u751f\u6210\u4fe1\u606f\u4e30\u5bcc\u7684\u6f5c\u5728\u8868\u793a\uff0c\u91c7\u7528\u5355\u6b65\u89e3\u7801\u66ff\u4ee3\u591a\u6b65\u53bb\u566a\uff0c\u5e76\u5f15\u5165\u4fdd\u771f\u5ea6\u5f15\u5bfc\u6a21\u5757\u548c\u901f\u7387\u9000\u706b\u8bad\u7ec3\u7b56\u7565\u3002", "result": "SODEC\u5728\u901f\u7387-\u5931\u771f-\u611f\u77e5\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u89e3\u7801\u901f\u5ea6\u63d0\u534720\u500d\u4ee5\u4e0a\u3002", "conclusion": "SODEC\u901a\u8fc7\u5355\u6b65\u89e3\u7801\u548c\u4fdd\u771f\u5ea6\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u538b\u7f29\u3002"}}
{"id": "2508.04988", "pdf": "https://arxiv.org/pdf/2508.04988", "abs": "https://arxiv.org/abs/2508.04988", "authors": ["Yue Li", "Weifan Wang", "Tai Sing Lee"], "title": "Modeling Rapid Contextual Learning in the Visual Cortex with Fast-Weight Deep Autoencoder Networks", "categories": ["cs.CV"], "comment": null, "summary": "Recent neurophysiological studies have revealed that the early visual cortex can rapidly learn global image context, as evidenced by a sparsification of population responses and a reduction in mean activity when exposed to familiar versus novel image contexts. This phenomenon has been attributed primarily to local recurrent interactions, rather than changes in feedforward or feedback pathways, supported by both empirical findings and circuit-level modeling. Recurrent neural circuits capable of simulating these effects have been shown to reshape the geometry of neural manifolds, enhancing robustness and invariance to irrelevant variations. In this study, we employ a Vision Transformer (ViT)-based autoencoder to investigate, from a functional perspective, how familiarity training can induce sensitivity to global context in the early layers of a deep neural network. We hypothesize that rapid learning operates via fast weights, which encode transient or short-term memory traces, and we explore the use of Low-Rank Adaptation (LoRA) to implement such fast weights within each Transformer layer. Our results show that (1) The proposed ViT-based autoencoder's self-attention circuit performs a manifold transform similar to a neural circuit model of the familiarity effect. (2) Familiarity training aligns latent representations in early layers with those in the top layer that contains global context information. (3) Familiarity training broadens the self-attention scope within the remembered image context. (4) These effects are significantly amplified by LoRA-based fast weights. Together, these findings suggest that familiarity training introduces global sensitivity to earlier layers in a hierarchical network, and that a hybrid fast-and-slow weight architecture may provide a viable computational model for studying rapid global context learning in the brain.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u65e9\u671f\u89c6\u89c9\u76ae\u5c42\u80fd\u5feb\u901f\u5b66\u4e60\u5168\u5c40\u56fe\u50cf\u4e0a\u4e0b\u6587\uff0c\u901a\u8fc7\u7a00\u758f\u5316\u79cd\u7fa4\u54cd\u5e94\u548c\u964d\u4f4e\u5e73\u5747\u6d3b\u52a8\u5b9e\u73b0\u3002\u4f7f\u7528ViT\u81ea\u7f16\u7801\u5668\u63a2\u7d22\u5feb\u901f\u6743\u91cd\u673a\u5236\uff0c\u53d1\u73b0LoRA\u80fd\u663e\u8457\u589e\u5f3a\u5168\u5c40\u654f\u611f\u6027\u3002", "motivation": "\u63a2\u8ba8\u5feb\u901f\u5b66\u4e60\u5982\u4f55\u901a\u8fc7\u5feb\u901f\u6743\u91cd\u673a\u5236\u5728\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u65e9\u671f\u5c42\u4e2d\u5f15\u5165\u5168\u5c40\u4e0a\u4e0b\u6587\u654f\u611f\u6027\u3002", "method": "\u91c7\u7528ViT\u81ea\u7f16\u7801\u5668\u548cLoRA\u6280\u672f\uff0c\u5206\u6790\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u5feb\u901f\u6743\u91cd\u7684\u4f5c\u7528\u3002", "result": "ViT\u81ea\u7f16\u7801\u5668\u6a21\u62df\u795e\u7ecf\u56de\u8def\uff0cLoRA\u589e\u5f3a\u5168\u5c40\u654f\u611f\u6027\uff0c\u65e9\u671f\u5c42\u4e0e\u9876\u5c42\u8868\u5f81\u5bf9\u9f50\u3002", "conclusion": "\u5feb\u901f\u6743\u91cd\u67b6\u6784\u4e3a\u7814\u7a76\u5927\u8111\u5feb\u901f\u5168\u5c40\u4e0a\u4e0b\u6587\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u884c\u8ba1\u7b97\u6a21\u578b\u3002"}}
{"id": "2508.05016", "pdf": "https://arxiv.org/pdf/2508.05016", "abs": "https://arxiv.org/abs/2508.05016", "authors": ["Shushi Wang", "Chunyi Li", "Zicheng Zhang", "Han Zhou", "Wei Dong", "Jun Chen", "Guangtao Zhai", "Xiaohong Liu"], "title": "AU-IQA: A Benchmark Dataset for Perceptual Quality Assessment of AI-Enhanced User-Generated Content", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "AI-based image enhancement techniques have been widely adopted in various visual applications, significantly improving the perceptual quality of user-generated content (UGC). However, the lack of specialized quality assessment models has become a significant limiting factor in this field, limiting user experience and hindering the advancement of enhancement methods. While perceptual quality assessment methods have shown strong performance on UGC and AIGC individually, their effectiveness on AI-enhanced UGC (AI-UGC) which blends features from both, remains largely unexplored. To address this gap, we construct AU-IQA, a benchmark dataset comprising 4,800 AI-UGC images produced by three representative enhancement types which include super-resolution, low-light enhancement, and denoising. On this dataset, we further evaluate a range of existing quality assessment models, including traditional IQA methods and large multimodal models. Finally, we provide a comprehensive analysis of how well current approaches perform in assessing the perceptual quality of AI-UGC. The access link to the AU-IQA is https://github.com/WNNGGU/AU-IQA-Dataset.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86AU-IQA\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u73b0\u6709\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\u5728AI\u589e\u5f3aUGC\u56fe\u50cf\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u7f3a\u4e4f\u4e13\u95e8\u7684\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\u9650\u5236\u4e86AI\u589e\u5f3aUGC\u56fe\u50cf\u9886\u57df\u7684\u53d1\u5c55\uff0c\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\u548c\u65b9\u6cd5\u8fdb\u6b65\u3002", "method": "\u6784\u5efa\u5305\u542b4,800\u5f20AI-UGC\u56fe\u50cf\u7684AU-IQA\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u8d85\u5206\u8fa8\u7387\u3001\u4f4e\u5149\u589e\u5f3a\u548c\u53bb\u566a\u4e09\u79cd\u589e\u5f3a\u7c7b\u578b\uff0c\u5e76\u8bc4\u4f30\u73b0\u6709\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\u3002", "result": "\u5bf9\u5f53\u524d\u65b9\u6cd5\u5728\u8bc4\u4f30AI-UGC\u611f\u77e5\u8d28\u91cf\u65b9\u9762\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\u3002", "conclusion": "AU-IQA\u6570\u636e\u96c6\u586b\u8865\u4e86AI-UGC\u8d28\u91cf\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002"}}
{"id": "2508.05060", "pdf": "https://arxiv.org/pdf/2508.05060", "abs": "https://arxiv.org/abs/2508.05060", "authors": ["Yifeng Huang", "Zhang Chen", "Yi Xu", "Minh Hoai", "Zhong Li"], "title": "DualMat: PBR Material Estimation via Coherent Dual-Path Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "We present DualMat, a novel dual-path diffusion framework for estimating Physically Based Rendering (PBR) materials from single images under complex lighting conditions. Our approach operates in two distinct latent spaces: an albedo-optimized path leveraging pretrained visual knowledge through RGB latent space, and a material-specialized path operating in a compact latent space designed for precise metallic and roughness estimation. To ensure coherent predictions between the albedo-optimized and material-specialized paths, we introduce feature distillation during training. We employ rectified flow to enhance efficiency by reducing inference steps while maintaining quality. Our framework extends to high-resolution and multi-view inputs through patch-based estimation and cross-view attention, enabling seamless integration into image-to-3D pipelines. DualMat achieves state-of-the-art performance on both Objaverse and real-world data, significantly outperforming existing methods with up to 28% improvement in albedo estimation and 39% reduction in metallic-roughness prediction errors.", "AI": {"tldr": "DualMat\u662f\u4e00\u79cd\u53cc\u8def\u5f84\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5355\u5f20\u56fe\u50cf\u4e2d\u4f30\u8ba1\u590d\u6742\u5149\u7167\u6761\u4ef6\u4e0b\u7684PBR\u6750\u8d28\uff0c\u901a\u8fc7\u4e24\u4e2a\u6f5c\u5728\u7a7a\u95f4\u548c\u7279\u5f81\u84b8\u998f\u5b9e\u73b0\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u9884\u6d4b\u3002", "motivation": "\u89e3\u51b3\u4ece\u5355\u5f20\u56fe\u50cf\u4e2d\u4f30\u8ba1PBR\u6750\u8d28\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u5149\u7167\u6761\u4ef6\u4e0b\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u7cbe\u786e\u6027\u548c\u6548\u7387\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u53cc\u8def\u5f84\u6846\u67b6\uff1a\u4e00\u6761\u8def\u5f84\u4f18\u5316\u53cd\u7167\u7387\uff08RGB\u6f5c\u5728\u7a7a\u95f4\uff09\uff0c\u53e6\u4e00\u6761\u8def\u5f84\u4e13\u6ce8\u4e8e\u91d1\u5c5e\u548c\u7c97\u7cd9\u5ea6\uff08\u7d27\u51d1\u6f5c\u5728\u7a7a\u95f4\uff09\uff0c\u5e76\u901a\u8fc7\u7279\u5f81\u84b8\u998f\u548c\u6574\u6d41\u6d41\u63d0\u5347\u6548\u7387\u3002", "result": "\u5728Objaverse\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u53cd\u7167\u7387\u4f30\u8ba1\u63d0\u534728%\uff0c\u91d1\u5c5e-\u7c97\u7cd9\u5ea6\u9884\u6d4b\u8bef\u5dee\u51cf\u5c1139%\u3002", "conclusion": "DualMat\u901a\u8fc7\u53cc\u8def\u5f84\u8bbe\u8ba1\u548c\u9ad8\u6548\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86PBR\u6750\u8d28\u4f30\u8ba1\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u3002"}}
{"id": "2508.05065", "pdf": "https://arxiv.org/pdf/2508.05065", "abs": "https://arxiv.org/abs/2508.05065", "authors": ["Yifu Guo", "Yuquan Lu", "Wentao Zhang", "Zishan Xu", "Dexia Chen", "Siyu Zhang", "Yizhe Zhang", "Ruixuan Wang"], "title": "Decoupling Continual Semantic Segmentation", "categories": ["cs.CV"], "comment": "https://github.com/euyis1019/Decoupling-Continual-Semantic-Segmentation", "summary": "Continual Semantic Segmentation (CSS) requires learning new classes without forgetting previously acquired knowledge, addressing the fundamental challenge of catastrophic forgetting in dense prediction tasks. However, existing CSS methods typically employ single-stage encoder-decoder architectures where segmentation masks and class labels are tightly coupled, leading to interference between old and new class learning and suboptimal retention-plasticity balance. We introduce DecoupleCSS, a novel two-stage framework for CSS. By decoupling class-aware detection from class-agnostic segmentation, DecoupleCSS enables more effective continual learning, preserving past knowledge while learning new classes. The first stage leverages pre-trained text and image encoders, adapted using LoRA, to encode class-specific information and generate location-aware prompts. In the second stage, the Segment Anything Model (SAM) is employed to produce precise segmentation masks, ensuring that segmentation knowledge is shared across both new and previous classes. This approach improves the balance between retention and adaptability in CSS, achieving state-of-the-art performance across a variety of challenging tasks. Our code is publicly available at: https://github.com/euyis1019/Decoupling-Continual-Semantic-Segmentation.", "AI": {"tldr": "DecoupleCSS\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u7c7b\u611f\u77e5\u68c0\u6d4b\u548c\u7c7b\u65e0\u5173\u5206\u5272\uff0c\u89e3\u51b3\u4e86\u6301\u7eed\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u4fdd\u7559\u4e0e\u9002\u5e94\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6301\u7eed\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u4e2d\u56e0\u5355\u9636\u6bb5\u67b6\u6784\u5bfc\u81f4\u7684\u65e7\u7c7b\u4e0e\u65b0\u7c7b\u5b66\u4e60\u5e72\u6270\u548c\u4fdd\u7559-\u53ef\u5851\u6027\u5e73\u8861\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6587\u672c\u548c\u56fe\u50cf\u7f16\u7801\u5668\uff08\u901a\u8fc7LoRA\u9002\u914d\uff09\u751f\u6210\u7c7b\u7279\u5b9a\u4fe1\u606f\u548c\u4f4d\u7f6e\u611f\u77e5\u63d0\u793a\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5229\u7528Segment Anything Model (SAM)\u751f\u6210\u7cbe\u786e\u5206\u5272\u63a9\u7801\u3002", "result": "\u5728\u591a\u79cd\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6709\u6548\u5e73\u8861\u4e86\u4fdd\u7559\u4e0e\u9002\u5e94\u6027\u3002", "conclusion": "DecoupleCSS\u901a\u8fc7\u89e3\u8026\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u6301\u7eed\u8bed\u4e49\u5206\u5272\u7684\u6548\u679c\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.05069", "pdf": "https://arxiv.org/pdf/2508.05069", "abs": "https://arxiv.org/abs/2508.05069", "authors": ["Jian Zhu", "Shanyuan Liu", "Liuzhuozheng Li", "Yue Gong", "He Wang", "Bo Cheng", "Yuhang Ma", "Liebucha Wu", "Xiaoyu Wu", "Dawei Leng", "Yuhui Yin", "Yang Xu"], "title": "FLUX-Makeup: High-Fidelity, Identity-Consistent, and Robust Makeup Transfer via Diffusion Transformer", "categories": ["cs.CV"], "comment": null, "summary": "Makeup transfer aims to apply the makeup style from a reference face to a target face and has been increasingly adopted in practical applications. Existing GAN-based approaches typically rely on carefully designed loss functions to balance transfer quality and facial identity consistency, while diffusion-based methods often depend on additional face-control modules or algorithms to preserve identity. However, these auxiliary components tend to introduce extra errors, leading to suboptimal transfer results. To overcome these limitations, we propose FLUX-Makeup, a high-fidelity, identity-consistent, and robust makeup transfer framework that eliminates the need for any auxiliary face-control components. Instead, our method directly leverages source-reference image pairs to achieve superior transfer performance. Specifically, we build our framework upon FLUX-Kontext, using the source image as its native conditional input. Furthermore, we introduce RefLoRAInjector, a lightweight makeup feature injector that decouples the reference pathway from the backbone, enabling efficient and comprehensive extraction of makeup-related information. In parallel, we design a robust and scalable data generation pipeline to provide more accurate supervision during training. The paired makeup datasets produced by this pipeline significantly surpass the quality of all existing datasets. Extensive experiments demonstrate that FLUX-Makeup achieves state-of-the-art performance, exhibiting strong robustness across diverse scenarios.", "AI": {"tldr": "FLUX-Makeup\u662f\u4e00\u79cd\u65e0\u9700\u8f85\u52a9\u9762\u90e8\u63a7\u5236\u7ec4\u4ef6\u7684\u9ad8\u4fdd\u771f\u3001\u8eab\u4efd\u4e00\u81f4\u4e14\u9c81\u68d2\u7684\u5316\u5986\u8fc1\u79fb\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u5229\u7528\u6e90-\u53c2\u8003\u56fe\u50cf\u5bf9\u5b9e\u73b0\u5353\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709GAN\u548c\u6269\u6563\u6a21\u578b\u65b9\u6cd5\u4f9d\u8d56\u989d\u5916\u7ec4\u4ef6\u6216\u590d\u6742\u635f\u5931\u51fd\u6570\uff0c\u6613\u5f15\u5165\u8bef\u5dee\uff0c\u5bfc\u81f4\u8fc1\u79fb\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u57fa\u4e8eFLUX-Kontext\u6846\u67b6\uff0c\u5f15\u5165RefLoRAInjector\u8f7b\u91cf\u7ea7\u5316\u5986\u7279\u5f81\u6ce8\u5165\u5668\uff0c\u5e76\u8bbe\u8ba1\u9ad8\u6548\u6570\u636e\u751f\u6210\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFLUX-Makeup\u5728\u591a\u6837\u573a\u666f\u4e0b\u8868\u73b0\u6700\u4f18\uff0c\u4e14\u751f\u6210\u7684\u6570\u636e\u96c6\u8d28\u91cf\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u96c6\u3002", "conclusion": "FLUX-Makeup\u901a\u8fc7\u7b80\u5316\u6846\u67b6\u548c\u9ad8\u6548\u6570\u636e\u751f\u6210\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u5316\u5986\u8fc1\u79fb\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.05091", "pdf": "https://arxiv.org/pdf/2508.05091", "abs": "https://arxiv.org/abs/2508.05091", "authors": ["Jingxuan He", "Busheng Su", "Finn Wong"], "title": "PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Generating long, temporally coherent videos with precise control over subject identity and motion is a formidable challenge for current diffusion models, which often suffer from identity drift and are limited to short clips. We introduce PoseGen, a novel framework that generates arbitrarily long videos of a specific subject from a single reference image and a driving pose sequence. Our core innovation is an in-context LoRA finetuning strategy that injects subject appearance at the token level for identity preservation, while simultaneously conditioning on pose information at the channel level for fine-grained motion control. To overcome duration limits, PoseGen pioneers an interleaved segment generation method that seamlessly stitches video clips together, using a shared KV cache mechanism and a specialized transition process to ensure background consistency and temporal smoothness. Trained on a remarkably small 33-hour video dataset, extensive experiments show that PoseGen significantly outperforms state-of-the-art methods in identity fidelity, pose accuracy, and its unique ability to produce coherent, artifact-free videos of unlimited duration.", "AI": {"tldr": "PoseGen\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u5f20\u53c2\u8003\u56fe\u50cf\u548c\u9a71\u52a8\u59ff\u6001\u5e8f\u5217\u751f\u6210\u4efb\u610f\u957f\u5ea6\u7684\u89c6\u9891\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6269\u6563\u6a21\u578b\u7684\u8eab\u4efd\u6f02\u79fb\u548c\u65f6\u957f\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u89c6\u9891\u65f6\u5b58\u5728\u8eab\u4efd\u6f02\u79fb\u548c\u77ed\u65f6\u957f\u9650\u5236\uff0c\u96be\u4ee5\u5b9e\u73b0\u957f\u89c6\u9891\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u548c\u7cbe\u786e\u63a7\u5236\u3002", "method": "\u91c7\u7528in-context LoRA\u5fae\u8c03\u7b56\u7565\uff0c\u5728token\u7ea7\u522b\u6ce8\u5165\u4e3b\u9898\u5916\u89c2\u4ee5\u4fdd\u6301\u8eab\u4efd\uff0c\u540c\u65f6\u5728\u901a\u9053\u7ea7\u522b\u6761\u4ef6\u5316\u59ff\u6001\u4fe1\u606f\u4ee5\u63a7\u5236\u8fd0\u52a8\u3002\u901a\u8fc7\u5206\u6bb5\u751f\u6210\u548c\u5171\u4eabKV\u7f13\u5b58\u673a\u5236\u5b9e\u73b0\u65e0\u7f1d\u62fc\u63a5\u3002", "result": "\u5728\u4ec533\u5c0f\u65f6\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0cPoseGen\u5728\u8eab\u4efd\u4fdd\u771f\u5ea6\u3001\u59ff\u6001\u51c6\u786e\u6027\u548c\u957f\u89c6\u9891\u751f\u6210\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PoseGen\u901a\u8fc7\u521b\u65b0\u7684\u5fae\u8c03\u548c\u5206\u6bb5\u751f\u6210\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u957f\u89c6\u9891\u7684\u9ad8\u8d28\u91cf\u751f\u6210\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.05160", "pdf": "https://arxiv.org/pdf/2508.05160", "abs": "https://arxiv.org/abs/2508.05160", "authors": ["Qi Xie", "Jiahong Fu", "Zongben Xu", "Deyu Meng"], "title": "Rotation Equivariant Arbitrary-scale Image Super-Resolution", "categories": ["cs.CV"], "comment": "Accepted by IEEE TPAMI, code and supplementary material is available   at https://github.com/XieQi2015/Equivariant-ASISR", "summary": "The arbitrary-scale image super-resolution (ASISR), a recent popular topic in computer vision, aims to achieve arbitrary-scale high-resolution recoveries from a low-resolution input image. This task is realized by representing the image as a continuous implicit function through two fundamental modules, a deep-network-based encoder and an implicit neural representation (INR) module. Despite achieving notable progress, a crucial challenge of such a highly ill-posed setting is that many common geometric patterns, such as repetitive textures, edges, or shapes, are seriously warped and deformed in the low-resolution images, naturally leading to unexpected artifacts appearing in their high-resolution recoveries. Embedding rotation equivariance into the ASISR network is thus necessary, as it has been widely demonstrated that this enhancement enables the recovery to faithfully maintain the original orientations and structural integrity of geometric patterns underlying the input image. Motivated by this, we make efforts to construct a rotation equivariant ASISR method in this study. Specifically, we elaborately redesign the basic architectures of INR and encoder modules, incorporating intrinsic rotation equivariance capabilities beyond those of conventional ASISR networks. Through such amelioration, the ASISR network can, for the first time, be implemented with end-to-end rotational equivariance maintained from input to output. We also provide a solid theoretical analysis to evaluate its intrinsic equivariance error, demonstrating its inherent nature of embedding such an equivariance structure. The superiority of the proposed method is substantiated by experiments conducted on both simulated and real datasets. We also validate that the proposed framework can be readily integrated into current ASISR methods in a plug \\& play manner to further enhance their performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65cb\u8f6c\u7b49\u53d8\u7684\u4efb\u610f\u5c3a\u5ea6\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u7f16\u7801\u5668\u548c\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u4ece\u8f93\u5165\u5230\u8f93\u51fa\u7684\u7aef\u5230\u7aef\u65cb\u8f6c\u7b49\u53d8\u6027\u3002", "motivation": "\u89e3\u51b3\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u51e0\u4f55\u6a21\u5f0f\uff08\u5982\u7eb9\u7406\u3001\u8fb9\u7f18\uff09\u53d8\u5f62\u5bfc\u81f4\u7684\u8d85\u5206\u8fa8\u7387\u6062\u590d\u4e2d\u51fa\u73b0\u4f2a\u5f71\u7684\u95ee\u9898\u3002", "method": "\u91cd\u65b0\u8bbe\u8ba1\u7f16\u7801\u5668\u548c\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u6a21\u5757\uff0c\u5d4c\u5165\u65cb\u8f6c\u7b49\u53d8\u80fd\u529b\uff0c\u5e76\u7406\u8bba\u5206\u6790\u5176\u56fa\u6709\u7b49\u53d8\u8bef\u5dee\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u5e76\u53ef\u5373\u63d2\u5373\u7528\u5730\u63d0\u5347\u73b0\u6709ASISR\u65b9\u6cd5\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65cb\u8f6c\u7b49\u53d8ASISR\u65b9\u6cd5\u6709\u6548\u4fdd\u6301\u4e86\u8f93\u5165\u56fe\u50cf\u7684\u51e0\u4f55\u7ed3\u6784\u5b8c\u6574\u6027\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.05162", "pdf": "https://arxiv.org/pdf/2508.05162", "abs": "https://arxiv.org/abs/2508.05162", "authors": ["Xuan Wang", "Kai Ruan", "Liyang Qian", "Zhizhi Guo", "Chang Su", "Gaoang Wang"], "title": "X-MoGen: Unified Motion Generation across Humans and Animals", "categories": ["cs.CV"], "comment": null, "summary": "Text-driven motion generation has attracted increasing attention due to its broad applications in virtual reality, animation, and robotics. While existing methods typically model human and animal motion separately, a joint cross-species approach offers key advantages, such as a unified representation and improved generalization. However, morphological differences across species remain a key challenge, often compromising motion plausibility. To address this, we propose \\textbf{X-MoGen}, the first unified framework for cross-species text-driven motion generation covering both humans and animals. X-MoGen adopts a two-stage architecture. First, a conditional graph variational autoencoder learns canonical T-pose priors, while an autoencoder encodes motion into a shared latent space regularized by morphological loss. In the second stage, we perform masked motion modeling to generate motion embeddings conditioned on textual descriptions. During training, a morphological consistency module is employed to promote skeletal plausibility across species. To support unified modeling, we construct \\textbf{UniMo4D}, a large-scale dataset of 115 species and 119k motion sequences, which integrates human and animal motions under a shared skeletal topology for joint training. Extensive experiments on UniMo4D demonstrate that X-MoGen outperforms state-of-the-art methods on both seen and unseen species.", "AI": {"tldr": "X-MoGen\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u8de8\u7269\u79cd\u6587\u672c\u9a71\u52a8\u8fd0\u52a8\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u67b6\u6784\u548c\u5f62\u6001\u4e00\u81f4\u6027\u6a21\u5757\u89e3\u51b3\u5f62\u6001\u5dee\u5f02\u95ee\u9898\uff0c\u5e76\u5728UniMo4D\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5206\u522b\u5efa\u6a21\u4eba\u7c7b\u548c\u52a8\u7269\u8fd0\u52a8\uff0c\u800c\u8de8\u7269\u79cd\u8054\u5408\u65b9\u6cd5\u80fd\u63d0\u4f9b\u7edf\u4e00\u8868\u793a\u548c\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5f62\u6001\u5dee\u5f02\u4ecd\u662f\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u67b6\u6784\uff1a\u7b2c\u4e00\u9636\u6bb5\u5b66\u4e60T-pose\u5148\u9a8c\u548c\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff1b\u7b2c\u4e8c\u9636\u6bb5\u57fa\u4e8e\u6587\u672c\u63cf\u8ff0\u751f\u6210\u8fd0\u52a8\u5d4c\u5165\u3002\u8bad\u7ec3\u65f6\u4f7f\u7528\u5f62\u6001\u4e00\u81f4\u6027\u6a21\u5757\u3002", "result": "\u5728UniMo4D\u6570\u636e\u96c6\u4e0a\uff0cX-MoGen\u5728\u5df2\u77e5\u548c\u672a\u77e5\u7269\u79cd\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "X-MoGen\u9996\u6b21\u5b9e\u73b0\u4e86\u8de8\u7269\u79cd\u6587\u672c\u9a71\u52a8\u8fd0\u52a8\u751f\u6210\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u5f62\u6001\u5dee\u5f02\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.05236", "pdf": "https://arxiv.org/pdf/2508.05236", "abs": "https://arxiv.org/abs/2508.05236", "authors": ["Yatong Lan", "Jingfeng Chen", "Yiru Wang", "Lei He"], "title": "ArbiViewGen: Controllable Arbitrary Viewpoint Camera Data Generation for Autonomous Driving via Stable Diffusion Models", "categories": ["cs.CV"], "comment": "11 pages, 6 figures", "summary": "Arbitrary viewpoint image generation holds significant potential for autonomous driving, yet remains a challenging task due to the lack of ground-truth data for extrapolated views, which hampers the training of high-fidelity generative models. In this work, we propose Arbiviewgen, a novel diffusion-based framework for the generation of controllable camera images from arbitrary points of view. To address the absence of ground-truth data in unseen views, we introduce two key components: Feature-Aware Adaptive View Stitching (FAVS) and Cross-View Consistency Self-Supervised Learning (CVC-SSL). FAVS employs a hierarchical matching strategy that first establishes coarse geometric correspondences using camera poses, then performs fine-grained alignment through improved feature matching algorithms, and identifies high-confidence matching regions via clustering analysis. Building upon this, CVC-SSL adopts a self-supervised training paradigm where the model reconstructs the original camera views from the synthesized stitched images using a diffusion model, enforcing cross-view consistency without requiring supervision from extrapolated data. Our framework requires only multi-camera images and their associated poses for training, eliminating the need for additional sensors or depth maps. To our knowledge, Arbiviewgen is the first method capable of controllable arbitrary view camera image generation in multiple vehicle configurations.", "AI": {"tldr": "Arbiviewgen\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u4efb\u610f\u89c6\u89d2\u751f\u6210\u53ef\u63a7\u76f8\u673a\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u7f3a\u4e4f\u5916\u63a8\u89c6\u56fe\u771f\u5b9e\u6570\u636e\u7684\u95ee\u9898\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u4efb\u610f\u89c6\u89d2\u56fe\u50cf\u751f\u6210\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u7f3a\u4e4f\u5916\u63a8\u89c6\u56fe\u7684\u771f\u5b9e\u6570\u636e\uff0c\u963b\u788d\u4e86\u9ad8\u4fdd\u771f\u751f\u6210\u6a21\u578b\u7684\u8bad\u7ec3\u3002", "method": "\u63d0\u51faFAVS\uff08\u7279\u5f81\u611f\u77e5\u81ea\u9002\u5e94\u89c6\u56fe\u62fc\u63a5\uff09\u548cCVC-SSL\uff08\u8de8\u89c6\u56fe\u4e00\u81f4\u6027\u81ea\u76d1\u7763\u5b66\u4e60\uff09\uff0c\u901a\u8fc7\u5206\u5c42\u5339\u914d\u548c\u81ea\u76d1\u7763\u8bad\u7ec3\u5b9e\u73b0\u3002", "result": "\u6846\u67b6\u4ec5\u9700\u591a\u76f8\u673a\u56fe\u50cf\u53ca\u5176\u4f4d\u59ff\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e0\u9700\u989d\u5916\u4f20\u611f\u5668\u6216\u6df1\u5ea6\u56fe\uff0c\u9996\u6b21\u5b9e\u73b0\u591a\u8f66\u8f86\u914d\u7f6e\u4e0b\u7684\u53ef\u63a7\u4efb\u610f\u89c6\u89d2\u56fe\u50cf\u751f\u6210\u3002", "conclusion": "Arbiviewgen\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u4efb\u610f\u89c6\u89d2\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05254", "pdf": "https://arxiv.org/pdf/2508.05254", "abs": "https://arxiv.org/abs/2508.05254", "authors": ["Hyunjoon Lee", "Joonkyu Min", "Jaesik Park"], "title": "CF3: Compact and Fast 3D Feature Fields", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV 2025", "summary": "3D Gaussian Splatting (3DGS) has begun incorporating rich information from 2D foundation models. However, most approaches rely on a bottom-up optimization process that treats raw 2D features as ground truth, incurring increased computational costs. We propose a top-down pipeline for constructing compact and fast 3D Gaussian feature fields, namely, CF3. We first perform a fast weighted fusion of multi-view 2D features with pre-trained Gaussians. This approach enables training a per-Gaussian autoencoder directly on the lifted features, instead of training autoencoders in the 2D domain. As a result, the autoencoder better aligns with the feature distribution. More importantly, we introduce an adaptive sparsification method that optimizes the Gaussian attributes of the feature field while pruning and merging the redundant Gaussians, constructing an efficient representation with preserved geometric details. Our approach achieves a competitive 3D feature field using as little as 5% of the Gaussians compared to Feature-3DGS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCF3\u76843D\u9ad8\u65af\u7279\u5f81\u573a\u6784\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u4e0a\u800c\u4e0b\u7684\u6d41\u7a0b\u4f18\u5316\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u7a00\u758f\u5316\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u81ea\u4e0b\u800c\u4e0a\u7684\u4f18\u5316\u8fc7\u7a0b\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e14\u5c062D\u7279\u5f81\u89c6\u4e3a\u7edd\u5bf9\u771f\u5b9e\u503c\u3002", "method": "\u91c7\u7528\u5feb\u901f\u52a0\u6743\u878d\u5408\u591a\u89c6\u89d22D\u7279\u5f81\u4e0e\u9884\u8bad\u7ec3\u9ad8\u65af\u6a21\u578b\uff0c\u8bad\u7ec3\u9ad8\u65af\u81ea\u7f16\u7801\u5668\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u7a00\u758f\u5316\u65b9\u6cd5\u4f18\u5316\u9ad8\u65af\u5c5e\u6027\u3002", "result": "\u4ec5\u97005%\u7684\u9ad8\u65af\u70b9\u5373\u53ef\u6784\u5efa\u5177\u6709\u7ade\u4e89\u529b\u76843D\u7279\u5f81\u573a\uff0c\u4fdd\u7559\u51e0\u4f55\u7ec6\u8282\u3002", "conclusion": "CF3\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u7279\u5f81\u8868\u793a\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.05264", "pdf": "https://arxiv.org/pdf/2508.05264", "abs": "https://arxiv.org/abs/2508.05264", "authors": ["Xiaoyang Zhang", "Zhen Hua", "Yakun Ju", "Wei Zhou", "Jun Liu", "Alex C. Kot"], "title": "SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible Image Fusion", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to TCSVT", "summary": "Infrared and visible image fusion (IVIF) aims to combine the thermal radiation information from infrared images with the rich texture details from visible images to enhance perceptual capabilities for downstream visual tasks. However, existing methods often fail to preserve key targets due to a lack of deep semantic understanding of the scene, while the fusion process itself can also introduce artifacts and detail loss, severely compromising both image quality and task performance. To address these issues, this paper proposes SGDFuse, a conditional diffusion model guided by the Segment Anything Model (SAM), to achieve high-fidelity and semantically-aware image fusion. The core of our method is to utilize high-quality semantic masks generated by SAM as explicit priors to guide the optimization of the fusion process via a conditional diffusion model. Specifically, the framework operates in a two-stage process: it first performs a preliminary fusion of multi-modal features, and then utilizes the semantic masks from SAM jointly with the preliminary fused image as a condition to drive the diffusion model's coarse-to-fine denoising generation. This ensures the fusion process not only has explicit semantic directionality but also guarantees the high fidelity of the final result. Extensive experiments demonstrate that SGDFuse achieves state-of-the-art performance in both subjective and objective evaluations, as well as in its adaptability to downstream tasks, providing a powerful solution to the core challenges in image fusion. The code of SGDFuse is available at https://github.com/boshizhang123/SGDFuse.", "AI": {"tldr": "SGDFuse\u662f\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u7ea2\u5916\u4e0e\u53ef\u89c1\u5149\u56fe\u50cf\u878d\u5408\u65b9\u6cd5\uff0c\u5229\u7528Segment Anything Model (SAM)\u751f\u6210\u7684\u8bed\u4e49\u63a9\u7801\u4f5c\u4e3a\u663e\u5f0f\u5148\u9a8c\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u548c\u8bed\u4e49\u611f\u77e5\u7684\u56fe\u50cf\u878d\u5408\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u5bf9\u573a\u666f\u7684\u6df1\u5c42\u8bed\u4e49\u7406\u89e3\uff0c\u5e38\u5bfc\u81f4\u5173\u952e\u76ee\u6807\u4e22\u5931\uff0c\u4e14\u878d\u5408\u8fc7\u7a0b\u53ef\u80fd\u5f15\u5165\u4f2a\u5f71\u548c\u7ec6\u8282\u635f\u5931\uff0c\u5f71\u54cd\u56fe\u50cf\u8d28\u91cf\u548c\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u5148\u8fdb\u884c\u591a\u6a21\u6001\u7279\u5f81\u7684\u521d\u6b65\u878d\u5408\uff0c\u518d\u5229\u7528SAM\u751f\u6210\u7684\u8bed\u4e49\u63a9\u7801\u4e0e\u521d\u6b65\u878d\u5408\u56fe\u50cf\u4f5c\u4e3a\u6761\u4ef6\uff0c\u9a71\u52a8\u6269\u6563\u6a21\u578b\u8fdb\u884c\u7c97\u5230\u7ec6\u7684\u53bb\u566a\u751f\u6210\u3002", "result": "SGDFuse\u5728\u4e3b\u89c2\u548c\u5ba2\u89c2\u8bc4\u4f30\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4e14\u5bf9\u4e0b\u6e38\u4efb\u52a1\u5177\u6709\u826f\u597d\u9002\u5e94\u6027\u3002", "conclusion": "SGDFuse\u901a\u8fc7\u8bed\u4e49\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u50cf\u878d\u5408\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u878d\u5408\u65b9\u6848\u3002"}}
{"id": "2508.05323", "pdf": "https://arxiv.org/pdf/2508.05323", "abs": "https://arxiv.org/abs/2508.05323", "authors": ["Frank Ruis", "Gertjan Burghouts", "Hugo Kuijf"], "title": "Textual Inversion for Efficient Adaptation of Open-Vocabulary Object Detectors Without Forgetting", "categories": ["cs.CV"], "comment": null, "summary": "Recent progress in large pre-trained vision language models (VLMs) has reached state-of-the-art performance on several object detection benchmarks and boasts strong zero-shot capabilities, but for optimal performance on specific targets some form of finetuning is still necessary. While the initial VLM weights allow for great few-shot transfer learning, this usually involves the loss of the original natural language querying and zero-shot capabilities. Inspired by the success of Textual Inversion (TI) in personalizing text-to-image diffusion models, we propose a similar formulation for open-vocabulary object detection. TI allows extending the VLM vocabulary by learning new or improving existing tokens to accurately detect novel or fine-grained objects from as little as three examples. The learned tokens are completely compatible with the original VLM weights while keeping them frozen, retaining the original model's benchmark performance, and leveraging its existing capabilities such as zero-shot domain transfer (e.g., detecting a sketch of an object after training only on real photos). The storage and gradient calculations are limited to the token embedding dimension, requiring significantly less compute than full-model fine-tuning. We evaluated whether the method matches or outperforms the baseline methods that suffer from forgetting in a wide variety of quantitative and qualitative experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u53cd\u8f6c\uff08TI\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6269\u5c55\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u8bcd\u6c47\u8868\uff0c\u4ee5\u68c0\u6d4b\u65b0\u5bf9\u8c61\uff0c\u540c\u65f6\u4fdd\u7559\u539f\u59cb\u6a21\u578b\u7684\u96f6\u6837\u672c\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9488\u5bf9\u7279\u5b9a\u76ee\u6807\u7684\u5fae\u8c03\u901a\u5e38\u4f1a\u4e27\u5931\u5176\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u548c\u96f6\u6837\u672c\u80fd\u529b\u3002", "method": "\u91c7\u7528\u6587\u672c\u53cd\u8f6c\uff08TI\uff09\u6280\u672f\uff0c\u901a\u8fc7\u5b66\u4e60\u65b0\u7684\u6216\u6539\u8fdb\u73b0\u6709\u6807\u8bb0\uff0c\u4ece\u5c11\u91cf\u793a\u4f8b\u4e2d\u68c0\u6d4b\u65b0\u5bf9\u8c61\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u6a21\u578b\u6743\u91cd\u51bb\u7ed3\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u539f\u59cb\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u9700\u6c42\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5fae\u8c03\u5bfc\u81f4\u7684\u9057\u5fd8\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u6a21\u578b\u7684\u96f6\u6837\u672c\u80fd\u529b\u3002"}}
{"id": "2508.05343", "pdf": "https://arxiv.org/pdf/2508.05343", "abs": "https://arxiv.org/abs/2508.05343", "authors": ["Junyu Zhou", "Yuyang Huang", "Wenrui Dai", "Junni Zou", "Ziyang Zheng", "Nuowen Kan", "Chenglin Li", "Hongkai Xiong"], "title": "3DGabSplat: 3D Gabor Splatting for Frequency-adaptive Radiance Field Rendering", "categories": ["cs.CV"], "comment": "Accepted by ACM MM'25", "summary": "Recent prominence in 3D Gaussian Splatting (3DGS) has enabled real-time rendering while maintaining high-fidelity novel view synthesis. However, 3DGS resorts to the Gaussian function that is low-pass by nature and is restricted in representing high-frequency details in 3D scenes. Moreover, it causes redundant primitives with degraded training and rendering efficiency and excessive memory overhead. To overcome these limitations, we propose 3D Gabor Splatting (3DGabSplat) that leverages a novel 3D Gabor-based primitive with multiple directional 3D frequency responses for radiance field representation supervised by multi-view images. The proposed 3D Gabor-based primitive forms a filter bank incorporating multiple 3D Gabor kernels at different frequencies to enhance flexibility and efficiency in capturing fine 3D details. Furthermore, to achieve novel view rendering, an efficient CUDA-based rasterizer is developed to project the multiple directional 3D frequency components characterized by 3D Gabor-based primitives onto the 2D image plane, and a frequency-adaptive mechanism is presented for adaptive joint optimization of primitives. 3DGabSplat is scalable to be a plug-and-play kernel for seamless integration into existing 3DGS paradigms to enhance both efficiency and quality of novel view synthesis. Extensive experiments demonstrate that 3DGabSplat outperforms 3DGS and its variants using alternative primitives, and achieves state-of-the-art rendering quality across both real-world and synthetic scenes. Remarkably, we achieve up to 1.35 dB PSNR gain over 3DGS with simultaneously reduced number of primitives and memory consumption.", "AI": {"tldr": "3D Gabor Splatting (3DGabSplat) \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D Gabor\u6838\u7684\u65b0\u578b\u57fa\u5143\uff0c\u7528\u4e8e\u63d0\u53473D\u573a\u666f\u7684\u9ad8\u9891\u7ec6\u8282\u8868\u793a\uff0c\u540c\u65f6\u51cf\u5c11\u5197\u4f59\u57fa\u5143\uff0c\u63d0\u9ad8\u8bad\u7ec3\u548c\u6e32\u67d3\u6548\u7387\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u867d\u7136\u80fd\u5b9e\u73b0\u5b9e\u65f6\u6e32\u67d3\u548c\u9ad8\u4fdd\u771f\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u4f46\u5176\u9ad8\u65af\u51fd\u6570\u672c\u8d28\u4e0a\u662f\u4f4e\u901a\u7684\uff0c\u96be\u4ee5\u6355\u6349\u9ad8\u9891\u7ec6\u8282\uff0c\u4e14\u5b58\u5728\u5197\u4f59\u57fa\u5143\u548c\u5185\u5b58\u5f00\u9500\u5927\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa3D Gabor Splatting\uff0c\u5229\u7528\u591a\u65b9\u54113D\u9891\u7387\u54cd\u5e94\u76843D Gabor\u57fa\u5143\u8868\u793a\u8f90\u5c04\u573a\uff0c\u5e76\u5f00\u53d1\u4e86\u9ad8\u6548\u7684CUDA\u5149\u6805\u5316\u5668\u548c\u9891\u7387\u81ea\u9002\u5e94\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c3DGabSplat\u5728\u6e32\u67d3\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u4f18\u4e8e3DGS\u53ca\u5176\u53d8\u4f53\uff0cPSNR\u63d0\u5347\u8fbe1.35 dB\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u57fa\u5143\u6570\u91cf\u548c\u5185\u5b58\u6d88\u8017\u3002", "conclusion": "3DGabSplat\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u5373\u63d2\u5373\u7528\u65b9\u6cd5\uff0c\u80fd\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u67093DGS\u6846\u67b6\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u65b0\u89c6\u89d2\u5408\u6210\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2508.05399", "pdf": "https://arxiv.org/pdf/2508.05399", "abs": "https://arxiv.org/abs/2508.05399", "authors": ["Wonjun Kang", "Byeongkeun Ahn", "Minjae Lee", "Kevin Galim", "Seunghyuk Oh", "Hyung Il Koo", "Nam Ik Cho"], "title": "UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Code is available at https://github.com/furiosa-ai/uncage", "summary": "Text-to-image (T2I) generation has been actively studied using Diffusion Models and Autoregressive Models. Recently, Masked Generative Transformers have gained attention as an alternative to Autoregressive Models to overcome the inherent limitations of causal attention and autoregressive decoding through bidirectional attention and parallel decoding, enabling efficient and high-quality image generation. However, compositional T2I generation remains challenging, as even state-of-the-art Diffusion Models often fail to accurately bind attributes and achieve proper text-image alignment. While Diffusion Models have been extensively studied for this issue, Masked Generative Transformers exhibit similar limitations but have not been explored in this context. To address this, we propose Unmasking with Contrastive Attention Guidance (UNCAGE), a novel training-free method that improves compositional fidelity by leveraging attention maps to prioritize the unmasking of tokens that clearly represent individual objects. UNCAGE consistently improves performance in both quantitative and qualitative evaluations across multiple benchmarks and metrics, with negligible inference overhead. Our code is available at https://github.com/furiosa-ai/uncage.", "AI": {"tldr": "UNCAGE\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u6ce8\u610f\u529b\u56fe\u4f18\u5148\u89e3\u63a9\u4ee3\u8868\u5355\u4e2a\u5bf9\u8c61\u7684\u6807\u8bb0\uff0c\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u7ec4\u5408\u4fdd\u771f\u5ea6\u3002", "motivation": "\u89e3\u51b3Masked Generative Transformers\u5728\u7ec4\u5408\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u5c5e\u6027\u7ed1\u5b9a\u548c\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51faUNCAGE\uff0c\u5229\u7528\u6ce8\u610f\u529b\u56fe\u6307\u5bfc\u89e3\u63a9\u8fc7\u7a0b\uff0c\u4f18\u5148\u5904\u7406\u660e\u786e\u4ee3\u8868\u5bf9\u8c61\u7684\u6807\u8bb0\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u548c\u6307\u6807\u4e0a\uff0cUNCAGE\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u63a8\u7406\u5f00\u9500\u53ef\u5ffd\u7565\u3002", "conclusion": "UNCAGE\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ec4\u5408\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u6027\u80fd\u3002"}}
{"id": "2508.05414", "pdf": "https://arxiv.org/pdf/2508.05414", "abs": "https://arxiv.org/abs/2508.05414", "authors": ["Jiawei Liang", "Siyuan Liang", "Jianjie Huang", "Chenxi Si", "Ming Zhang", "Xiaochun Cao"], "title": "Physical Adversarial Camouflage through Gradient Calibration and Regularization", "categories": ["cs.CV"], "comment": "Accepted to IJCAI 2025", "summary": "The advancement of deep object detectors has greatly affected safety-critical fields like autonomous driving. However, physical adversarial camouflage poses a significant security risk by altering object textures to deceive detectors. Existing techniques struggle with variable physical environments, facing two main challenges: 1) inconsistent sampling point densities across distances hinder the gradient optimization from ensuring local continuity, and 2) updating texture gradients from multiple angles causes conflicts, reducing optimization stability and attack effectiveness. To address these issues, we propose a novel adversarial camouflage framework based on gradient optimization. First, we introduce a gradient calibration strategy, which ensures consistent gradient updates across distances by propagating gradients from sparsely to unsampled texture points. Additionally, we develop a gradient decorrelation method, which prioritizes and orthogonalizes gradients based on loss values, enhancing stability and effectiveness in multi-angle optimization by eliminating redundant or conflicting updates. Extensive experimental results on various detection models, angles and distances show that our method significantly exceeds the state of the art, with an average increase in attack success rate (ASR) of 13.46% across distances and 11.03% across angles. Furthermore, empirical evaluation in real-world scenarios highlights the need for more robust system design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u4f18\u5316\u7684\u5bf9\u6297\u6027\u4f2a\u88c5\u6846\u67b6\uff0c\u901a\u8fc7\u68af\u5ea6\u6821\u51c6\u548c\u53bb\u76f8\u5173\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u7269\u7406\u5bf9\u6297\u4f2a\u88c5\u5bf9\u5b89\u5168\u5173\u952e\u9886\u57df\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\uff09\u6784\u6210\u5a01\u80c1\uff0c\u73b0\u6709\u6280\u672f\u56e0\u73af\u5883\u53d8\u5316\u5bfc\u81f4\u68af\u5ea6\u4f18\u5316\u4e0d\u7a33\u5b9a\u548c\u653b\u51fb\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u5f15\u5165\u68af\u5ea6\u6821\u51c6\u7b56\u7565\u548c\u68af\u5ea6\u53bb\u76f8\u5173\u65b9\u6cd5\uff0c\u786e\u4fdd\u68af\u5ea6\u66f4\u65b0\u7684\u8fde\u7eed\u6027\u548c\u591a\u89d2\u5ea6\u4f18\u5316\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u653b\u51fb\u6210\u529f\u7387\u4e0a\u5e73\u5747\u63d0\u534713.46%\uff08\u8ddd\u79bb\uff09\u548c11.03%\uff08\u89d2\u5ea6\uff09\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u7269\u7406\u5bf9\u6297\u4f2a\u88c5\u4e2d\u7684\u4f18\u5316\u95ee\u9898\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u73b0\u5b9e\u573a\u666f\u4e2d\u7cfb\u7edf\u8bbe\u8ba1\u7684\u9c81\u68d2\u6027\u9700\u6c42\u3002"}}
{"id": "2508.05599", "pdf": "https://arxiv.org/pdf/2508.05599", "abs": "https://arxiv.org/abs/2508.05599", "authors": ["Shaobin Zhuang", "Yiwei Guo", "Canmiao Fu", "Zhipeng Huang", "Zeyue Tian", "Ying Zhang", "Chen Li", "Yali Wang"], "title": "WeTok: Powerful Discrete Tokenization for High-Fidelity Visual Reconstruction", "categories": ["cs.CV"], "comment": "23 pages, 10 figures, 37 tables", "summary": "Visual tokenizer is a critical component for vision generation. However, the existing tokenizers often face unsatisfactory trade-off between compression ratios and reconstruction fidelity. To fill this gap, we introduce a powerful and concise WeTok tokenizer, which surpasses the previous leading tokenizers via two core innovations. (1) Group-wise lookup-free Quantization (GQ). We partition the latent features into groups, and perform lookup-free quantization for each group. As a result, GQ can efficiently overcome memory and computation limitations of prior tokenizers, while achieving a reconstruction breakthrough with more scalable codebooks. (2) Generative Decoding (GD). Different from prior tokenizers, we introduce a generative decoder with a prior of extra noise variable. In this case, GD can probabilistically model the distribution of visual data conditioned on discrete tokens, allowing WeTok to reconstruct visual details, especially at high compression ratios. Extensive experiments on mainstream benchmarks show superior performance of our WeTok. On the ImageNet 50k validation set, WeTok achieves a record-low zero-shot rFID (WeTok: 0.12 vs. FLUX-VAE: 0.18 vs. SD-VAE 3.5: 0.19). Furthermore, our highest compression model achieves a zero-shot rFID of 3.49 with a compression ratio of 768, outperforming Cosmos (384) 4.57 which has only 50% compression rate of ours. Code and models are available: https://github.com/zhuangshaobin/WeTok.", "AI": {"tldr": "WeTok\u662f\u4e00\u79cd\u65b0\u578b\u89c6\u89c9\u5206\u8bcd\u5668\uff0c\u901a\u8fc7\u5206\u7ec4\u65e0\u67e5\u627e\u91cf\u5316\u548c\u751f\u6210\u89e3\u7801\u6280\u672f\uff0c\u5728\u538b\u7f29\u6bd4\u548c\u91cd\u5efa\u4fdd\u771f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u5206\u8bcd\u5668\u5728\u538b\u7f29\u6bd4\u548c\u91cd\u5efa\u4fdd\u771f\u5ea6\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0cWeTok\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u7ec4\u65e0\u67e5\u627e\u91cf\u5316\uff08GQ\uff09\u548c\u751f\u6210\u89e3\u7801\uff08GD\uff09\u6280\u672f\uff0cGQ\u63d0\u5347\u6548\u7387\u4e0e\u6269\u5c55\u6027\uff0cGD\u901a\u8fc7\u566a\u58f0\u53d8\u91cf\u5efa\u6a21\u89c6\u89c9\u6570\u636e\u5206\u5e03\u3002", "result": "\u5728ImageNet 50k\u9a8c\u8bc1\u96c6\u4e0a\uff0cWeTok\u521b\u4e0b\u96f6\u6837\u672crFID\u6700\u4f4e\u8bb0\u5f55\uff080.12\uff09\uff0c\u9ad8\u538b\u7f29\u6bd4\u6a21\u578b\uff08768\uff09\u6027\u80fd\u4f18\u4e8e\u7ade\u54c1\u3002", "conclusion": "WeTok\u5728\u89c6\u89c9\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u4e3a\u89c6\u89c9\u5206\u8bcd\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.05606", "pdf": "https://arxiv.org/pdf/2508.05606", "abs": "https://arxiv.org/abs/2508.05606", "authors": ["Luozheng Qin", "Jia Gong", "Yuqing Sun", "Tianjiao Li", "Mengping Yang", "Xiaomeng Yang", "Chao Qu", "Zhiyu Tan", "Hao Li"], "title": "Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and Vision", "categories": ["cs.CV", "cs.CL"], "comment": "https://sais-fuxi.github.io/projects/uni-cot/", "summary": "Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large Language Models (LLMs) by decomposing complex tasks into simpler, sequential subtasks. However, extending CoT to vision-language reasoning tasks remains challenging, as it often requires interpreting transitions of visual states to support reasoning. Existing methods often struggle with this due to limited capacity of modeling visual state transitions or incoherent visual trajectories caused by fragmented architectures.   To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought framework that enables coherent and grounded multimodal reasoning within a single unified model. The key idea is to leverage a model capable of both image understanding and generation to reason over visual content and model evolving visual states. However, empowering a unified model to achieve that is non-trivial, given the high computational cost and the burden of training. To address this, Uni-CoT introduces a novel two-level reasoning paradigm: A Macro-Level CoT for high-level task planning and A Micro-Level CoT for subtask execution. This design significantly reduces the computational overhead. Furthermore, we introduce a structured training paradigm that combines interleaved image-text supervision for macro-level CoT with multi-task objectives for micro-level CoT. Together, these innovations allow Uni-CoT to perform scalable and coherent multi-modal reasoning. Furthermore, thanks to our design, all experiments can be efficiently completed using only 8 A100 GPUs with 80GB VRAM each. Experimental results on reasoning-driven image generation benchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT demonstrates SOTA performance and strong generalization, establishing Uni-CoT as a promising solution for multi-modal reasoning. Project Page and Code: https://sais-fuxi.github.io/projects/uni-cot/", "AI": {"tldr": "Uni-CoT\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u94fe\u5f0f\u601d\u7ef4\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u56fe\u50cf\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\uff0c\u5b9e\u73b0\u8fde\u8d2f\u7684\u591a\u6a21\u6001\u63a8\u7406\uff0c\u5e76\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6269\u5c55\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u5230\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5efa\u6a21\u89c6\u89c9\u72b6\u6001\u8f6c\u6362\u6216\u8fde\u8d2f\u89c6\u89c9\u8f68\u8ff9\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u63d0\u51faUni-CoT\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u7ea7\u63a8\u7406\u8303\u5f0f\uff08\u5b8f\u89c2\u7ea7CoT\u7528\u4e8e\u4efb\u52a1\u89c4\u5212\uff0c\u5fae\u89c2\u7ea7CoT\u7528\u4e8e\u5b50\u4efb\u52a1\u6267\u884c\uff09\uff0c\u5e76\u7ed3\u5408\u7ed3\u6784\u5316\u8bad\u7ec3\u8303\u5f0f\u3002", "result": "\u5728WISE\u3001RISE\u548cKRIS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\uff08\u4ec5\u97008\u5757A100 GPU\uff09\u3002", "conclusion": "Uni-CoT\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05609", "pdf": "https://arxiv.org/pdf/2508.05609", "abs": "https://arxiv.org/abs/2508.05609", "authors": ["Yuhan Zhang", "Long Zhuo", "Ziyang Chu", "Tong Wu", "Zhibing Li", "Liang Pan", "Dahua Lin", "Ziwei Liu"], "title": "Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity", "categories": ["cs.CV"], "comment": "Page: https://zyh482.github.io/Hi3DEval/", "summary": "Despite rapid advances in 3D content generation, quality assessment for the generated 3D assets remains challenging. Existing methods mainly rely on image-based metrics and operate solely at the object level, limiting their ability to capture spatial coherence, material authenticity, and high-fidelity local details. 1) To address these challenges, we introduce Hi3DEval, a hierarchical evaluation framework tailored for 3D generative content. It combines both object-level and part-level evaluation, enabling holistic assessments across multiple dimensions as well as fine-grained quality analysis. Additionally, we extend texture evaluation beyond aesthetic appearance by explicitly assessing material realism, focusing on attributes such as albedo, saturation, and metallicness. 2) To support this framework, we construct Hi3DBench, a large-scale dataset comprising diverse 3D assets and high-quality annotations, accompanied by a reliable multi-agent annotation pipeline. We further propose a 3D-aware automated scoring system based on hybrid 3D representations. Specifically, we leverage video-based representations for object-level and material-subject evaluations to enhance modeling of spatio-temporal consistency and employ pretrained 3D features for part-level perception. Extensive experiments demonstrate that our approach outperforms existing image-based metrics in modeling 3D characteristics and achieves superior alignment with human preference, providing a scalable alternative to manual evaluations. The project page is available at https://zyh482.github.io/Hi3DEval/.", "AI": {"tldr": "Hi3DEval\u662f\u4e00\u4e2a\u9488\u5bf93D\u751f\u6210\u5185\u5bb9\u7684\u5206\u5c42\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u5408\u5bf9\u8c61\u7ea7\u548c\u90e8\u5206\u7ea7\u8bc4\u4f30\uff0c\u5e76\u6269\u5c55\u7eb9\u7406\u8bc4\u4f30\u4ee5\u5305\u62ec\u6750\u6599\u771f\u5b9e\u6027\u3002Hi3DBench\u6570\u636e\u96c6\u548c\u81ea\u52a8\u5316\u8bc4\u5206\u7cfb\u7edf\u652f\u6301\u8be5\u6846\u67b6\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u56fe\u50cf\u6307\u6807\u3002", "motivation": "\u73b0\u67093D\u5185\u5bb9\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u56fe\u50cf\u6307\u6807\u4e14\u4ec5\u5173\u6ce8\u5bf9\u8c61\u7ea7\uff0c\u65e0\u6cd5\u6355\u6349\u7a7a\u95f4\u4e00\u81f4\u6027\u3001\u6750\u6599\u771f\u5b9e\u6027\u548c\u5c40\u90e8\u7ec6\u8282\u3002", "method": "\u63d0\u51faHi3DEval\u6846\u67b6\uff0c\u7ed3\u5408\u5bf9\u8c61\u7ea7\u548c\u90e8\u5206\u7ea7\u8bc4\u4f30\uff0c\u6269\u5c55\u7eb9\u7406\u8bc4\u4f30\u81f3\u6750\u6599\u5c5e\u6027\uff1b\u6784\u5efaHi3DBench\u6570\u636e\u96c6\u548c3D\u611f\u77e5\u81ea\u52a8\u5316\u8bc4\u5206\u7cfb\u7edf\u3002", "result": "\u5b9e\u9a8c\u663e\u793aHi3DEval\u5728\u5efa\u6a213D\u7279\u6027\u548c\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u56fe\u50cf\u6307\u6807\u3002", "conclusion": "Hi3DEval\u4e3a3D\u751f\u6210\u5185\u5bb9\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4f18\u4e8e\u4f20\u7edf\u56fe\u50cf\u6307\u6807\u3002"}}
{"id": "2508.05631", "pdf": "https://arxiv.org/pdf/2508.05631", "abs": "https://arxiv.org/abs/2508.05631", "authors": ["Weiqi Zhang", "Junsheng Zhou", "Haotian Geng", "Wenyuan Zhang", "Yu-Shen Liu"], "title": "GAP: Gaussianize Any Point Clouds with Text Guidance", "categories": ["cs.CV"], "comment": "ICCV 2025. Project page: https://weiqi-zhang.github.io/GAP", "summary": "3D Gaussian Splatting (3DGS) has demonstrated its advantages in achieving fast and high-quality rendering. As point clouds serve as a widely-used and easily accessible form of 3D representation, bridging the gap between point clouds and Gaussians becomes increasingly important. Recent studies have explored how to convert the colored points into Gaussians, but directly generating Gaussians from colorless 3D point clouds remains an unsolved challenge. In this paper, we propose GAP, a novel approach that gaussianizes raw point clouds into high-fidelity 3D Gaussians with text guidance. Our key idea is to design a multi-view optimization framework that leverages a depth-aware image diffusion model to synthesize consistent appearances across different viewpoints. To ensure geometric accuracy, we introduce a surface-anchoring mechanism that effectively constrains Gaussians to lie on the surfaces of 3D shapes during optimization. Furthermore, GAP incorporates a diffuse-based inpainting strategy that specifically targets at completing hard-to-observe regions. We evaluate GAP on the Point-to-Gaussian generation task across varying complexity levels, from synthetic point clouds to challenging real-world scans, and even large-scale scenes. Project Page: https://weiqi-zhang.github.io/GAP.", "AI": {"tldr": "GAP\u662f\u4e00\u79cd\u5c06\u539f\u59cb\u70b9\u4e91\u901a\u8fc7\u6587\u672c\u5f15\u5bfc\u8f6c\u5316\u4e3a\u9ad8\u4fdd\u771f3D\u9ad8\u65af\u5206\u5e03\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u65e0\u8272\u70b9\u4e91\u76f4\u63a5\u751f\u6210\u9ad8\u65af\u7684\u6311\u6218\u3002", "motivation": "\u70b9\u4e91\u662f\u5e7f\u6cdb\u4f7f\u7528\u76843D\u8868\u793a\u5f62\u5f0f\uff0c\u4f46\u5982\u4f55\u5c06\u65e0\u8272\u70b9\u4e91\u8f6c\u5316\u4e3a\u9ad8\u65af\u5206\u5e03\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u591a\u89c6\u89d2\u4f18\u5316\u6846\u67b6\uff0c\u5229\u7528\u6df1\u5ea6\u611f\u77e5\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5408\u6210\u4e00\u81f4\u5916\u89c2\uff0c\u5e76\u901a\u8fc7\u8868\u9762\u951a\u5b9a\u673a\u5236\u7ea6\u675f\u9ad8\u65af\u5206\u5e03\u3002", "result": "\u5728\u4ece\u5408\u6210\u70b9\u4e91\u5230\u771f\u5b9e\u626b\u63cf\u7684\u4e0d\u540c\u590d\u6742\u5ea6\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86GAP\u7684\u6709\u6548\u6027\u3002", "conclusion": "GAP\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u8272\u70b9\u4e91\u5230\u9ad8\u4fdd\u771f3D\u9ad8\u65af\u7684\u8f6c\u5316\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\u3002"}}
{"id": "2508.04929", "pdf": "https://arxiv.org/pdf/2508.04929", "abs": "https://arxiv.org/abs/2508.04929", "authors": ["Suyi Chen", "Haibin Ling"], "title": "CryoGS: Gaussian Splatting for Cryo-EM Homogeneous Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "As a critical modality for structural biology, cryogenic electron microscopy (cryo-EM) facilitates the determination of macromolecular structures at near-atomic resolution. The core computational task in single-particle cryo-EM is to reconstruct the 3D electrostatic potential of a molecule from a large collection of noisy 2D projections acquired at unknown orientations. Gaussian mixture models (GMMs) provide a continuous, compact, and physically interpretable representation for molecular density and have recently gained interest in cryo-EM reconstruction. However, existing methods rely on external consensus maps or atomic models for initialization, limiting their use in self-contained pipelines. Addressing this issue, we introduce cryoGS, a GMM-based method that integrates Gaussian splatting with the physics of cryo-EM image formation. In particular, we develop an orthogonal projection-aware Gaussian splatting, with adaptations such as a normalization term and FFT-aligned coordinate system tailored for cryo-EM imaging. All these innovations enable stable and efficient homogeneous reconstruction directly from raw cryo-EM particle images using random initialization. Experimental results on real datasets validate the effectiveness and robustness of cryoGS over representative baselines. The code will be released upon publication.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\u7684cryoGS\u65b9\u6cd5\uff0c\u7528\u4e8e\u76f4\u63a5\u4ece\u539f\u59cb\u51b7\u51bb\u7535\u955c\u56fe\u50cf\u4e2d\u91cd\u5efa\u5206\u5b50\u7ed3\u6784\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u521d\u59cb\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u5171\u8bc6\u56fe\u6216\u539f\u5b50\u6a21\u578b\u521d\u59cb\u5316\uff0c\u9650\u5236\u4e86\u5176\u5728\u72ec\u7acb\u6d41\u7a0b\u4e2d\u7684\u5e94\u7528\u3002", "method": "cryoGS\u7ed3\u5408\u9ad8\u65af\u6295\u5c04\u4e0e\u51b7\u51bb\u7535\u955c\u6210\u50cf\u7269\u7406\uff0c\u5f00\u53d1\u4e86\u6b63\u4ea4\u6295\u5f71\u611f\u77e5\u7684\u9ad8\u65af\u6295\u5c04\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u5f52\u4e00\u5316\u9879\u548cFFT\u5bf9\u9f50\u5750\u6807\u7cfb\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86cryoGS\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "cryoGS\u4e3a\u51b7\u51bb\u7535\u955c\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5b9a\u4e14\u9ad8\u6548\u7684\u81ea\u5305\u542b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05240", "pdf": "https://arxiv.org/pdf/2508.05240", "abs": "https://arxiv.org/abs/2508.05240", "authors": ["Junyi Wang", "Xi Zhu", "Yikun Guo", "Zixi Wang", "Haichuan Gao", "Le Zhang", "Fan Zhang"], "title": "Coarse-to-Fine Joint Registration of MR and Ultrasound Images via Imaging Style Transfer", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "We developed a pipeline for registering pre-surgery Magnetic Resonance (MR) images and post-resection Ultrasound (US) images. Our approach leverages unpaired style transfer using 3D CycleGAN to generate synthetic T1 images, thereby enhancing registration performance. Additionally, our registration process employs both affine and local deformable transformations for a coarse-to-fine registration. The results demonstrate that our approach improves the consistency between MR and US image pairs in most cases.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D CycleGAN\u7684MR\u4e0eUS\u56fe\u50cf\u914d\u51c6\u6d41\u7a0b\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210T1\u56fe\u50cf\u63d0\u5347\u914d\u51c6\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u672f\u524dMR\u4e0e\u672f\u540eUS\u56fe\u50cf\u914d\u51c6\u7684\u6311\u6218\uff0c\u63d0\u5347\u914d\u51c6\u4e00\u81f4\u6027\u3002", "method": "\u4f7f\u75283D CycleGAN\u8fdb\u884c\u975e\u914d\u5bf9\u98ce\u683c\u8fc1\u79fb\u751f\u6210\u5408\u6210T1\u56fe\u50cf\uff0c\u7ed3\u5408\u4eff\u5c04\u548c\u5c40\u90e8\u53ef\u53d8\u5f62\u53d8\u6362\u5b9e\u73b0\u7c97\u5230\u7ec6\u914d\u51c6\u3002", "result": "\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e86MR\u4e0eUS\u56fe\u50cf\u5bf9\u7684\u914d\u51c6\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86MR\u4e0eUS\u56fe\u50cf\u7684\u914d\u51c6\u6027\u80fd\u3002"}}
{"id": "2508.05635", "pdf": "https://arxiv.org/pdf/2508.05635", "abs": "https://arxiv.org/abs/2508.05635", "authors": ["Yue Liao", "Pengfei Zhou", "Siyuan Huang", "Donglin Yang", "Shengcong Chen", "Yuxin Jiang", "Yue Hu", "Jingbin Cai", "Si Liu", "Jianlan Luo", "Liliang Chen", "Shuicheng Yan", "Maoqing Yao", "Guanghui Ren"], "title": "Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": "https://genie-envisioner.github.io/", "summary": "We introduce Genie Envisioner (GE), a unified world foundation platform for robotic manipulation that integrates policy learning, evaluation, and simulation within a single video-generative framework. At its core, GE-Base is a large-scale, instruction-conditioned video diffusion model that captures the spatial, temporal, and semantic dynamics of real-world robotic interactions in a structured latent space. Built upon this foundation, GE-Act maps latent representations to executable action trajectories through a lightweight, flow-matching decoder, enabling precise and generalizable policy inference across diverse embodiments with minimal supervision. To support scalable evaluation and training, GE-Sim serves as an action-conditioned neural simulator, producing high-fidelity rollouts for closed-loop policy development. The platform is further equipped with EWMBench, a standardized benchmark suite measuring visual fidelity, physical consistency, and instruction-action alignment. Together, these components establish Genie Envisioner as a scalable and practical foundation for instruction-driven, general-purpose embodied intelligence. All code, models, and benchmarks will be released publicly.", "AI": {"tldr": "Genie Envisioner (GE) \u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u5e73\u53f0\uff0c\u96c6\u6210\u4e86\u7b56\u7565\u5b66\u4e60\u3001\u8bc4\u4f30\u548c\u4eff\u771f\uff0c\u57fa\u4e8e\u89c6\u9891\u751f\u6210\u6846\u67b6\u3002", "motivation": "\u65e8\u5728\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u5b9e\u7528\u7684\u57fa\u7840\u5e73\u53f0\uff0c\u652f\u6301\u6307\u4ee4\u9a71\u52a8\u7684\u901a\u7528\u667a\u80fd\u4f53\u3002", "method": "GE-Base \u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u6307\u4ee4\u6761\u4ef6\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0cGE-Act \u901a\u8fc7\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\u5c06\u6f5c\u5728\u8868\u793a\u6620\u5c04\u4e3a\u53ef\u6267\u884c\u52a8\u4f5c\u8f68\u8ff9\uff0cGE-Sim \u4f5c\u4e3a\u795e\u7ecf\u6a21\u62df\u5668\u751f\u6210\u9ad8\u4fdd\u771f\u4eff\u771f\u3002", "result": "\u5e73\u53f0\u901a\u8fc7 EWMBench \u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u89c6\u89c9\u4fdd\u771f\u5ea6\u3001\u7269\u7406\u4e00\u81f4\u6027\u548c\u6307\u4ee4-\u52a8\u4f5c\u5bf9\u9f50\u3002", "conclusion": "Genie Envisioner \u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u901a\u7528\u667a\u80fd\u4f53\u57fa\u7840\u5e73\u53f0\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u516c\u5f00\u3002"}}
