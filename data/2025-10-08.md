<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 21]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Teamwork: Collaborative Diffusion with Low-rank Coordination and Adaptation](https://arxiv.org/abs/2510.05532)
*Sam Sartor,Pieter Peers*

Main category: cs.CV

TL;DR: Teamwork是一个灵活高效的统一解决方案，通过协调多个基础扩散模型实例来扩展输入输出通道，无需修改预训练模型架构，支持多种图形任务。


<details>
  <summary>Details</summary>
Motivation: 现有通道扩展方法通常是特定于应用的，难以适应不同的扩散模型或新任务，需要一种更灵活通用的解决方案。

Method: 使用多个基础扩散模型实例（队友）进行协调，采用改进的低秩适应（LoRA）方法同时处理适应和协调问题，支持队友的动态激活/停用。

Result: 在多种生成和逆向图形任务中表现出色，包括修复、单图像SVBRDF估计、本征分解、神经着色和本征图像合成。

Conclusion: Teamwork提供了一个统一且高效的框架，能够灵活扩展扩散模型的输入输出能力，适用于广泛的图形应用。

Abstract: Large pretrained diffusion models can provide strong priors beneficial for many graphics applications. However, generative applications such as neural rendering and inverse methods such as SVBRDF estimation and intrinsic image decomposition require additional input or output channels. Current solutions for channel expansion are often application specific and these solutions can be difficult to adapt to different diffusion models or new tasks. This paper introduces Teamwork: a flexible and efficient unified solution for jointly increasing the number of input and output channels as well as adapting a pretrained diffusion model to new tasks. Teamwork achieves channel expansion without altering the pretrained diffusion model architecture by coordinating and adapting multiple instances of the base diffusion model (\ie, teammates). We employ a novel variation of Low Rank-Adaptation (LoRA) to jointly address both adaptation and coordination between the different teammates. Furthermore Teamwork supports dynamic (de)activation of teammates. We demonstrate the flexibility and efficiency of Teamwork on a variety of generative and inverse graphics tasks such as inpainting, single image SVBRDF estimation, intrinsic decomposition, neural shading, and intrinsic image synthesis.

</details>


### [2] [Mitigating Diffusion Model Hallucinations with Dynamic Guidance](https://arxiv.org/abs/2510.05356)
*Kostas Triaridis,Alexandros Graikos,Aggelina Chatziagapi,Grigorios G. Chrysos,Dimitris Samaras*

Main category: cs.CV

TL;DR: 提出动态引导方法，在生成时选择性锐化导致伪影的分数函数方向，减少扩散模型的幻觉生成，同时保持语义多样性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成效果惊艳，但常常产生结构不一致的幻觉样本，这些样本位于真实数据分布的支持之外。这些幻觉可归因于数据分布模式间的过度平滑。

Method: 引入动态引导方法，通过仅沿已知导致伪影的预定方向选择性锐化分数函数，来减轻幻觉，同时保留有效的语义变化。

Result: 动态引导在受控和自然图像数据集上显著减少幻觉，明显优于基线方法。

Conclusion: 这是首个在生成时而非通过后处理过滤来解决幻觉问题的方法，动态引导能有效减少扩散模型的幻觉生成。

Abstract: Diffusion models, despite their impressive demos, often produce hallucinatory samples with structural inconsistencies that lie outside of the support of the true data distribution. Such hallucinations can be attributed to excessive smoothing between modes of the data distribution. However, semantic interpolations are often desirable and can lead to generation diversity, thus we believe a more nuanced solution is required. In this work, we introduce Dynamic Guidance, which tackles this issue. Dynamic Guidance mitigates hallucinations by selectively sharpening the score function only along the pre-determined directions known to cause artifacts, while preserving valid semantic variations. To our knowledge, this is the first approach that addresses hallucinations at generation time rather than through post-hoc filtering. Dynamic Guidance substantially reduces hallucinations on both controlled and natural image datasets, significantly outperforming baselines.

</details>


### [3] [LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation](https://arxiv.org/abs/2510.05367)
*Yang Xiao,Gen Li,Kaiyuan Deng,Yushu Wu,Zheng Zhan,Yanzhi Wang,Xiaolong Ma,Bo Hui*

Main category: cs.CV

TL;DR: 提出了一种训练免费的视频生成加速方法LightCache，通过分析扩散模型推理过程中的编码、去噪和解码三个阶段，设计了异步缓存交换、特征分块和切片解码等策略来降低内存消耗，同时保持加速效果。


<details>
  <summary>Details</summary>
Motivation: 基于扩散模型的视频生成中，推理过程中的潜在变量冗余为加速提供了切入点。现有基于缓存的加速方法在后两个阶段（去噪和解码）会导致显著的内存激增，需要解决这一内存消耗问题。

Method: 1) 异步缓存交换：优化缓存管理策略；2) 特征分块：将特征分割处理；3) 切片解码：对潜在变量进行切片解码。这些策略确保引入的时间开销低于加速收益。

Result: 与基线相比，该方法实现了更快的推理速度和更低的内存使用，同时将质量下降控制在可接受范围内。

Conclusion: LightCache方法有效解决了视频生成扩散模型推理中的内存消耗问题，在保持加速效果的同时显著降低了内存需求，为训练免费加速提供了实用解决方案。

Abstract: Training-free acceleration has emerged as an advanced research area in video generation based on diffusion models. The redundancy of latents in diffusion model inference provides a natural entry point for acceleration. In this paper, we decompose the inference process into the encoding, denoising, and decoding stages, and observe that cache-based acceleration methods often lead to substantial memory surges in the latter two stages. To address this problem, we analyze the characteristics of inference across different stages and propose stage-specific strategies for reducing memory consumption: 1) Asynchronous Cache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same time, we ensure that the time overhead introduced by these three strategies remains lower than the acceleration gains themselves. Compared with the baseline, our approach achieves faster inference speed and lower memory usage, while maintaining quality degradation within an acceptable range. The Code is available at https://github.com/NKUShaw/LightCache .

</details>


### [4] [ArchitectHead: Continuous Level of Detail Control for 3D Gaussian Head Avatars](https://arxiv.org/abs/2510.05488)
*Peizhi Yan,Rabab Ward,Qiang Tang,Shan Du*

Main category: cs.CV

TL;DR: ArchitectHead是首个支持连续控制细节级别的3D高斯头部化身框架，通过UV特征场和动态重采样实现渲染效率与视觉质量的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS化身的高斯点数量固定，无法根据应用需求调整细节级别来平衡渲染效率和视觉质量。

Method: 在2D UV特征空间中参数化高斯点，构建多级可学习特征图组成的UV特征场，通过轻量级神经网络解码器将潜在特征转换为3D高斯属性，通过动态重采样特征图控制高斯点数量。

Result: 在最高细节级别实现SOTA质量，最低细节级别仅使用6.2%高斯点，质量适度下降（L1损失+7.9%，PSNR -0.97%，SSIM -0.6%，LPIPS损失+24.1%），渲染速度几乎翻倍。

Conclusion: ArchitectHead实现了无需重新训练的连续细节级别控制，在保持高质量的同时显著提升渲染效率。

Abstract: 3D Gaussian Splatting (3DGS) has enabled photorealistic and real-time rendering of 3D head avatars. Existing 3DGS-based avatars typically rely on tens of thousands of 3D Gaussian points (Gaussians), with the number of Gaussians fixed after training. However, many practical applications require adjustable levels of detail (LOD) to balance rendering efficiency and visual quality. In this work, we propose "ArchitectHead", the first framework for creating 3D Gaussian head avatars that support continuous control over LOD. Our key idea is to parameterize the Gaussians in a 2D UV feature space and propose a UV feature field composed of multi-level learnable feature maps to encode their latent features. A lightweight neural network-based decoder then transforms these latent features into 3D Gaussian attributes for rendering. ArchitectHead controls the number of Gaussians by dynamically resampling feature maps from the UV feature field at the desired resolutions. This method enables efficient and continuous control of LOD without retraining. Experimental results show that ArchitectHead achieves state-of-the-art (SOTA) quality in self and cross-identity reenactment tasks at the highest LOD, while maintaining near SOTA performance at lower LODs. At the lowest LOD, our method uses only 6.2\% of the Gaussians while the quality degrades moderately (L1 Loss +7.9\%, PSNR --0.97\%, SSIM --0.6\%, LPIPS Loss +24.1\%), and the rendering speed nearly doubles.

</details>


### [5] [Improving Chain-of-Thought Efficiency for Autoregressive Image Generation](https://arxiv.org/abs/2510.05593)
*Zeqi Gu,Markos Georgopoulos,Xiaoliang Dai,Marjan Ghazvininejad,Chu Wang,Felix Juefei-Xu,Kunpeng Li,Yujun Shi,Zecheng He,Zijian He,Jiawei Zhou,Abe Davis,Jialiang Wang*

Main category: cs.CV

TL;DR: ShortCoTI是一个轻量级优化框架，通过强化学习减少图像生成中的思维链冗余，在保持图像质量的同时将提示推理长度减少54%。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用思维链推理来增强图像生成的对齐性和细节，但会导致视觉过度思考现象，增加计算成本并可能引入与原始提示矛盾的细节。

Method: 提出ShortCoTI框架，使用自适应奖励函数鼓励更简洁的思维链，并通过强化学习范式优化提示推理长度。

Result: 在多个基准测试（T2I-CompBench、GenEval）上，推理长度减少54%，同时保持或略微提高质量指标，消除了冗余解释和重复优化。

Conclusion: ShortCoTI在不影响生成图像保真度或视觉吸引力的前提下，显著提高了计算效率。

Abstract: Autoregressive multimodal large language models have recently gained popularity for image generation, driven by advances in foundation models. To enhance alignment and detail, newer approaches employ chain-of-thought (CoT) reasoning, expanding user inputs into elaborated prompts prior to image synthesis. However, this strategy can introduce unnecessary redundancy -- a phenomenon we call visual overthinking -- which increases computational costs and can introduce details that contradict the original prompt. In this work, we explore how to generate more concise CoT sequences for more efficient image generation. We introduce ShortCoTI, a lightweight optimization framework that encourages more concise CoT while preserving output image quality. ShortCoTI rewards more concise prompts with an adaptive function that scales according to an estimated difficulty for each task. Incorporating this reward into a reinforcement learning paradigm reduces prompt reasoning length by 54% while maintaining or slightly improving quality metrics across multiple benchmarks (T2I-CompBench, GenEval). Qualitative analysis shows that our method eliminates verbose explanations and repetitive refinements, producing reasoning prompts that are both concise and semantically rich. As a result, ShortCoTI improves computational efficiency without compromising the fidelity or visual appeal of generated images.

</details>


### [6] [Efficient Conditional Generation on Scale-based Visual Autoregressive Models](https://arxiv.org/abs/2510.05610)
*Jiaqi Liu,Tao Huang,Chang Xu*

Main category: cs.CV

TL;DR: 提出ECM框架，通过轻量级控制模块实现高效的空间条件图像生成，无需微调预训练模型，显著降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 当前自回归模型在复杂空间条件生成时需要微调预训练模型，导致高昂训练成本，需要更高效的解决方案。

Method: 使用分布式架构的轻量控制模块，包含上下文感知注意力层和共享门控FFN，配合早期中心采样策略和温度调度。

Result: 在基于尺度的自回归模型上验证，实现了高保真度和多样性的图像生成控制，超越现有基线方法。

Conclusion: ECM框架在保持生成质量的同时显著提升了训练和推理效率，为自回归模型的条件生成提供了高效解决方案。

Abstract: Recent advances in autoregressive (AR) models have demonstrated their potential to rival diffusion models in image synthesis. However, for complex spatially-conditioned generation, current AR approaches rely on fine-tuning the pre-trained model, leading to significant training costs. In this paper, we propose the Efficient Control Model (ECM), a plug-and-play framework featuring a lightweight control module that introduces control signals via a distributed architecture. This architecture consists of context-aware attention layers that refine conditional features using real-time generated tokens, and a shared gated feed-forward network (FFN) designed to maximize the utilization of its limited capacity and ensure coherent control feature learning. Furthermore, recognizing the critical role of early-stage generation in determining semantic structure, we introduce an early-centric sampling strategy that prioritizes learning early control sequences. This approach reduces computational cost by lowering the number of training tokens per iteration, while a complementary temperature scheduling during inference compensates for the resulting insufficient training of late-stage tokens. Extensive experiments on scale-based AR models validate that our method achieves high-fidelity and diverse control over image generation, surpassing existing baselines while significantly improving both training and inference efficiency.

</details>


### [7] [Beyond Spectral Peaks: Interpreting the Cues Behind Synthetic Image Detection](https://arxiv.org/abs/2510.05633)
*Sara Mandelli,Diego Vila-Portela,David Vázquez-Padín,Paolo Bestagini,Fernando Pérez-González*

Main category: cs.CV

TL;DR: 该研究系统分析了基于频率域伪影的深度伪造检测器是否真正依赖频谱峰值，发现大多数检测器并不从根本上依赖这些峰值，挑战了该领域的普遍假设。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的伪造检测器通常被视为黑盒，不清楚它们是否真正依赖频谱峰值作为合成图像生成的强指标，这限制了检测器的可解释性和可信度。

Method: 提出了一种从图像中移除频谱峰值的策略，并分析该操作对多个检测器的影响；同时引入了一个仅依赖频率峰值的简单线性检测器作为完全可解释的基线。

Result: 研究发现大多数检测器并不从根本上依赖频谱峰值，这与该领域的普遍假设相矛盾。

Conclusion: 这项工作挑战了关于频谱峰值在伪造检测中重要性的普遍假设，为开发更透明可靠的取证工具铺平了道路。

Abstract: Over the years, the forensics community has proposed several deep learning-based detectors to mitigate the risks of generative AI. Recently, frequency-domain artifacts (particularly periodic peaks in the magnitude spectrum), have received significant attention, as they have been often considered a strong indicator of synthetic image generation. However, state-of-the-art detectors are typically used as black-boxes, and it still remains unclear whether they truly rely on these peaks. This limits their interpretability and trust. In this work, we conduct a systematic study to address this question. We propose a strategy to remove spectral peaks from images and analyze the impact of this operation on several detectors. In addition, we introduce a simple linear detector that relies exclusively on frequency peaks, providing a fully interpretable baseline free from the confounding influence of deep learning. Our findings reveal that most detectors are not fundamentally dependent on spectral peaks, challenging a widespread assumption in the field and paving the way for more transparent and reliable forensic tools.

</details>


### [8] [When and How to Cut Classical Concerts? A Multimodal Automated Video Editing Approach](https://arxiv.org/abs/2510.05661)
*Daniel Gonzálbez-Biosca,Josep Cabacas-Maso,Carles Ventura,Ismael Benito-Altamirano*

Main category: cs.CV

TL;DR: 提出了一种用于古典音乐会多摄像机录像自动编辑的多模态架构，将问题分解为时间分割（何时剪切）和空间选择（如何剪切）两个子任务。


<details>
  <summary>Details</summary>
Motivation: 自动视频编辑在计算机视觉和多媒体领域仍未被充分探索，特别是与视频生成和场景理解相比。本文专注于编辑古典音乐会的多摄像机录像。

Method: 使用轻量级卷积-Transformer管道整合音频对数梅尔频谱图、可选图像嵌入和标量时间特征进行时间分割；使用CLIP编码器改进空间选择，并将干扰项选择限制在同一音乐会片段内。

Result: 模型在检测剪切点方面优于先前基线，并在视觉镜头选择方面具有竞争力，推进了多模态自动视频编辑的技术水平。

Conclusion: 该方法通过分解编辑任务和采用多模态架构，在古典音乐会视频自动编辑方面取得了先进成果，为自动视频编辑领域做出了贡献。

Abstract: Automated video editing remains an underexplored task in the computer vision and multimedia domains, especially when contrasted with the growing interest in video generation and scene understanding. In this work, we address the specific challenge of editing multicamera recordings of classical music concerts by decomposing the problem into two key sub-tasks: when to cut and how to cut. Building on recent literature, we propose a novel multimodal architecture for the temporal segmentation task (when to cut), which integrates log-mel spectrograms from the audio signals, plus an optional image embedding, and scalar temporal features through a lightweight convolutional-transformer pipeline. For the spatial selection task (how to cut), we improve the literature by updating from old backbones, e.g. ResNet, with a CLIP-based encoder and constraining distractor selection to segments from the same concert. Our dataset was constructed following a pseudo-labeling approach, in which raw video data was automatically clustered into coherent shot segments. We show that our models outperformed previous baselines in detecting cut points and provide competitive visual shot selection, advancing the state of the art in multimodal automated video editing.

</details>


### [9] [AgeBooth: Controllable Facial Aging and Rejuvenation via Diffusion Models](https://arxiv.org/abs/2510.05715)
*Shihao Zhu,Bohan Cao,Ziheng Ouyang,Zhen Li,Peng-Tao Jiang,Qibin Hou*

Main category: cs.CV

TL;DR: AgeBooth是一种新颖的年龄特定微调方法，通过年龄条件提示混合和年龄特定LoRA融合策略，无需大量年龄标注数据即可增强基于适配器的身份个性化模型的年龄控制能力。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在生成身份一致图像时难以准确控制年龄，且微调需要昂贵的跨年龄配对图像。

Method: 利用年龄的线性特性，引入年龄条件提示混合和基于SVDMix矩阵融合的年龄特定LoRA融合策略，实现中间年龄肖像的高质量生成。

Result: AgeBooth从单张参考图像生成不同年龄段的真实且身份一致的人脸图像，在年龄控制和视觉质量上优于现有最先进的编辑方法。

Conclusion: AgeBooth提供了一种无需大量年龄标注数据的有效年龄控制解决方案，显著提升了身份个性化模型的年龄编辑能力。

Abstract: Recent diffusion model research focuses on generating identity-consistent images from a reference photo, but they struggle to accurately control age while preserving identity, and fine-tuning such models often requires costly paired images across ages. In this paper, we propose AgeBooth, a novel age-specific finetuning approach that can effectively enhance the age control capability of adapterbased identity personalization models without the need for expensive age-varied datasets. To reduce dependence on a large amount of age-labeled data, we exploit the linear nature of aging by introducing age-conditioned prompt blending and an age-specific LoRA fusion strategy that leverages SVDMix, a matrix fusion technique. These techniques enable high-quality generation of intermediate-age portraits. Our AgeBooth produces realistic and identity-consistent face images across different ages from a single reference image. Experiments show that AgeBooth achieves superior age control and visual quality compared to previous state-of-the-art editing-based methods.

</details>


### [10] [Data Factory with Minimal Human Effort Using VLMs](https://arxiv.org/abs/2510.05722)
*Jiaojiao Ye,Jiaxing Zhong,Qian Xie,Yuzhou Zhou,Niki Trigoni,Andrew Markham*

Main category: cs.CV

TL;DR: 提出了一种无需训练的管道，集成预训练的ControlNet和视觉语言模型，生成带有像素级标签的合成图像，用于提升少样本语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 传统数据增强方法难以操作高级语义属性（如材质和纹理），而现有基于扩散模型的方法要么计算昂贵，要么性能不足。

Method: 使用预训练的ControlNet和视觉语言模型构建训练免费管道，包含多路提示生成器、掩码生成器和高质量图像选择模块，生成带有像素级标签的合成图像。

Result: 在PASCAL-5i和COCO-20i数据集上表现出有希望的性能，并在一次性语义分割任务中优于同期工作。

Conclusion: 该方法无需手动标注，显著改善下游任务，通过提高保真度和多样性来增强少样本语义分割性能。

Abstract: Generating enough and diverse data through augmentation offers an efficient solution to the time-consuming and labour-intensive process of collecting and annotating pixel-wise images. Traditional data augmentation techniques often face challenges in manipulating high-level semantic attributes, such as materials and textures. In contrast, diffusion models offer a robust alternative, by effectively utilizing text-to-image or image-to-image transformation. However, existing diffusion-based methods are either computationally expensive or compromise on performance. To address this issue, we introduce a novel training-free pipeline that integrates pretrained ControlNet and Vision-Language Models (VLMs) to generate synthetic images paired with pixel-level labels. This approach eliminates the need for manual annotations and significantly improves downstream tasks. To improve the fidelity and diversity, we add a Multi-way Prompt Generator, Mask Generator and High-quality Image Selection module. Our results on PASCAL-5i and COCO-20i present promising performance and outperform concurrent work for one-shot semantic segmentation.

</details>


### [11] [Rasterized Steered Mixture of Experts for Efficient 2D Image Regression](https://arxiv.org/abs/2510.05814)
*Yi-Hsin Li,Thomas Sikora,Sebastian Knorr,Mårten Sjöström*

Main category: cs.CV

TL;DR: 提出一种基于光栅化的优化策略，结合光栅化高斯核渲染效率和Steered Mixture of Experts的边缘感知门控机制，加速二维图像回归，同时保持模型稀疏性和重建质量。


<details>
  <summary>Details</summary>
Motivation: Steered Mixture of Experts回归框架在图像重建、压缩、去噪和超分辨率方面表现出色，但高计算成本限制了实际应用，需要更高效的优化方法。

Method: 使用光栅化高斯核渲染替代全局迭代优化，结合Steered Mixture of Experts的边缘感知门控机制，实现快速参数更新和内存高效的模型表示。

Result: 该方法显著加速了参数更新，支持原生超分辨率和图像去噪等应用，在计算效率和重建保真度之间达到新的平衡。

Conclusion: 光栅化优化与边缘感知结构的结合为二维图像处理任务提供了计算效率和重建质量之间的良好权衡。

Abstract: The Steered Mixture of Experts regression framework has demonstrated strong performance in image reconstruction, compression, denoising, and super-resolution. However, its high computational cost limits practical applications. This work introduces a rasterization-based optimization strategy that combines the efficiency of rasterized Gaussian kernel rendering with the edge-aware gating mechanism of the Steered Mixture of Experts. The proposed method is designed to accelerate two-dimensional image regression while maintaining the model's inherent sparsity and reconstruction quality. By replacing global iterative optimization with a rasterized formulation, the method achieves significantly faster parameter updates and more memory-efficient model representations. In addition, the proposed framework supports applications such as native super-resolution and image denoising, which are not directly achievable with standard rasterized Gaussian kernel approaches. The combination of fast rasterized optimization with the edge-aware structure of the Steered Mixture of Experts provides a new balance between computational efficiency and reconstruction fidelity for two-dimensional image processing tasks.

</details>


### [12] [$\bf{D^3}$QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection](https://arxiv.org/abs/2510.05891)
*Yanran Zhang,Bingyao Yu,Yu Zheng,Wenzhao Zheng,Yueqi Duan,Lei Chen,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 提出D3QE方法，利用视觉自回归模型生成图像时存在的离散分布差异和量化误差，通过离散分布感知变换器检测自回归生成的伪造图像。


<details>
  <summary>Details</summary>
Motivation: 视觉自回归模型通过离散token预测生成图像，与传统GAN或扩散模型不同，在图像合成质量和向量量化表示方面具有独特特征，这为伪造图像检测带来了新的挑战和机会。

Method: 提出离散分布差异感知量化误差(D3QE)方法，引入动态码本频率统计到注意力机制中的变换器，融合语义特征和量化误差潜在表示。

Result: 在包含7种主流视觉自回归模型的ARForensics数据集上实验表明，D3QE具有优越的检测精度和强泛化能力，对真实世界扰动具有鲁棒性。

Conclusion: D3QE方法有效利用了自回归模型生成图像的独特特征，在自回归生成图像检测方面表现出色，为应对新兴视觉生成技术的伪造检测提供了有效解决方案。

Abstract: The emergence of visual autoregressive (AR) models has revolutionized image generation while presenting new challenges for synthetic image detection. Unlike previous GAN or diffusion-based methods, AR models generate images through discrete token prediction, exhibiting both marked improvements in image synthesis quality and unique characteristics in their vector-quantized representations. In this paper, we propose to leverage Discrete Distribution Discrepancy-aware Quantization Error (D$^3$QE) for autoregressive-generated image detection that exploits the distinctive patterns and the frequency distribution bias of the codebook existing in real and fake images. We introduce a discrete distribution discrepancy-aware transformer that integrates dynamic codebook frequency statistics into its attention mechanism, fusing semantic features and quantization error latent. To evaluate our method, we construct a comprehensive dataset termed ARForensics covering 7 mainstream visual AR models. Experiments demonstrate superior detection accuracy and strong generalization of D$^3$QE across different AR models, with robustness to real-world perturbations. Code is available at \href{https://github.com/Zhangyr2022/D3QE}{https://github.com/Zhangyr2022/D3QE}.

</details>


### [13] [Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis](https://arxiv.org/abs/2510.05976)
*Eashan Adhikarla,Yixin Liu,Brian D. Davison*

Main category: cs.CV

TL;DR: 这篇综述对基于扩散模型的低光照图像增强方法进行了全面分析，提出了多视角分类法，并与GAN和Transformer方法进行了性能比较，同时讨论了实际部署挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 低光照图像增强在安全关键应用中至关重要，扩散模型因其通过迭代去噪建模复杂图像分布的能力而成为有前景的LLIE方法，需要系统性的分析和评估。

Method: 提出了包含六个类别的多视角分类法：内在分解、谱与潜在、加速、引导、多模态和自主；基于模型机制和条件信号的混合视角进行分类，并进行定性失败模式分析和基准测试比较。

Result: 评估了扩散模型在LLIE中的性能，发现了基准不一致性，分析了可解释性、泛化性和推理效率之间的权衡，识别了实际部署约束和伦理考虑。

Conclusion: 该综述旨在通过突出趋势和提出开放研究问题来指导下一代基于扩散的LLIE研究，包括新颖条件、实时适应和基础模型的潜力。

Abstract: Low-light image enhancement (LLIE) is vital for safety-critical applications such as surveillance, autonomous navigation, and medical imaging, where visibility degradation can impair downstream task performance. Recently, diffusion models have emerged as a promising generative paradigm for LLIE due to their capacity to model complex image distributions via iterative denoising. This survey provides an up-to-date critical analysis of diffusion models for LLIE, distinctively featuring an in-depth comparative performance evaluation against Generative Adversarial Network and Transformer-based state-of-the-art methods, a thorough examination of practical deployment challenges, and a forward-looking perspective on the role of emerging paradigms like foundation models. We propose a multi-perspective taxonomy encompassing six categories: Intrinsic Decomposition, Spectral & Latent, Accelerated, Guided, Multimodal, and Autonomous; that map enhancement methods across physical priors, conditioning schemes, and computational efficiency. Our taxonomy is grounded in a hybrid view of both the model mechanism and the conditioning signals. We evaluate qualitative failure modes, benchmark inconsistencies, and trade-offs between interpretability, generalization, and inference efficiency. We also discuss real-world deployment constraints (e.g., memory, energy use) and ethical considerations. This survey aims to guide the next generation of diffusion-based LLIE research by highlighting trends and surfacing open research questions, including novel conditioning, real-time adaptation, and the potential of foundation models.

</details>


### [14] [A Dynamic Mode Decomposition Approach to Morphological Component Analysis](https://arxiv.org/abs/2510.05977)
*Owen T. Huber,Raghu G. Raj,Tianyu Chen,Zacharie I. Idriss*

Main category: cs.CV

TL;DR: 提出动态形态成分分析(DMCA)方法，通过特征值聚类学习自适应视频表示，用于分离视频中结构不同的形态成分


<details>
  <summary>Details</summary>
Motivation: 传统形态成分分析(MCA)使用预定义的不相干字典，需要为不同视频手动调整参数，缺乏自适应能力

Method: 基于动态模式分解特征值聚类，学习数据驱动的MCA字典，扩展传统MCA算法

Result: 在Adobe 240fps数据集去噪、海面微弱目标信噪比增强、逆合成孔径雷达图像中自行车与风杂波分离等任务中表现有效

Conclusion: DMCA能够自适应地分离视频中的不同形态成分，在多个应用场景中展现出良好性能

Abstract: This paper introduces a novel methodology of adapting the representation of videos based on the dynamics of their scene content variation. In particular, we demonstrate how the clustering of dynamic mode decomposition eigenvalues can be leveraged to learn an adaptive video representation for separating structurally distinct morphologies of a video. We extend the morphological component analysis (MCA) algorithm, which uses multiple predefined incoherent dictionaries and a sparsity prior to separate distinct sources in signals, by introducing our novel eigenspace clustering technique to obtain data-driven MCA dictionaries, which we call dynamic morphological component analysis (DMCA). After deriving our novel algorithm, we offer a motivational example of DMCA applied to a still image, then demonstrate DMCA's effectiveness in denoising applications on videos from the Adobe 240fps dataset. Afterwards, we provide an example of DMCA enhancing the signal-to-noise ratio of a faint target summed with a sea state, and conclude the paper by applying DMCA to separate a bicycle from wind clutter in inverse synthetic aperture radar images.

</details>


### [15] [Diffusion-Based Image Editing for Breaking Robust Watermarks](https://arxiv.org/abs/2510.05978)
*Yunyi Ni,Finn Carter,Ze Niu,Emily Davis,Bo Zhang*

Main category: cs.CV

TL;DR: 扩散模型能够有效破坏原本设计用于抵抗常规扰动的鲁棒图像水印方案，通过图像再生过程消除水印同时保持视觉内容，理论上证明了水印与图像间的互信息会消失。


<details>
  <summary>Details</summary>
Motivation: 强大的基于扩散的图像生成和编辑技术对现有的鲁棒水印方案构成了新的威胁，需要研究扩散模型如何破坏这些水印。

Method: 提出了扩散驱动的图像再生过程，以及一种新颖的引导扩散攻击，在生成过程中明确针对水印信号，显著降低水印可检测性。

Result: 在多个最先进的水印方案上评估，攻击后水印恢复率接近零，同时再生图像保持高视觉保真度。

Conclusion: 当前鲁棒水印技术在面对基于生成模型的攻击时存在根本性漏洞，在生成AI时代需要新的水印策略。

Abstract: Robust invisible watermarking aims to embed hidden information into images such that the watermark can survive various image manipulations. However, the rise of powerful diffusion-based image generation and editing techniques poses a new threat to these watermarking schemes. In this paper, we present a theoretical study and method demonstrating that diffusion models can effectively break robust image watermarks that were designed to resist conventional perturbations. We show that a diffusion-driven ``image regeneration'' process can erase embedded watermarks while preserving perceptual image content. We further introduce a novel guided diffusion attack that explicitly targets the watermark signal during generation, significantly degrading watermark detectability. Theoretically, we prove that as an image undergoes sufficient diffusion-based transformation, the mutual information between the watermarked image and the embedded watermark payload vanishes, resulting in decoding failure. Experimentally, we evaluate our approach on multiple state-of-the-art watermarking schemes (including the deep learning-based methods StegaStamp, TrustMark, and VINE) and demonstrate near-zero watermark recovery rates after attack, while maintaining high visual fidelity of the regenerated images. Our findings highlight a fundamental vulnerability in current robust watermarking techniques against generative model-based attacks, underscoring the need for new watermarking strategies in the era of generative AI.

</details>


### [16] [A public cardiac CT dataset featuring the left atrial appendage](https://arxiv.org/abs/2510.06090)
*Bjoern Hansen,Jonas Pedersen,Klaus F. Kofoed,Oscar Camara,Rasmus R. Paulsen,Kristine Soerensen*

Main category: cs.CV

TL;DR: 提出了首个开源、解剖学一致的高分辨率左心耳、冠状动脉和肺静脉分割数据集，基于ImageCAS的1000个心脏CTA扫描，旨在促进左心耳形态分析的新方法。


<details>
  <summary>Details</summary>
Motivation: 尽管有TotalSegmentator等先进分割框架，但左心耳、冠状动脉和肺静脉的准确分割在医学成像中仍具挑战性，需要高质量的分割数据集来推动相关研究。

Method: 使用专门为高分辨率左心耳分割开发的最先进分割框架，在大型私有数据集上训练网络，然后将模型迁移到ImageCAS数据；冠状动脉标签从原始ImageCAS注释改进，肺静脉分割从TS输出细化。

Result: 创建了包含1000个心脏CTA扫描的解剖学一致分割数据集，提供了左心耳、冠状动脉和肺静脉的高质量分割标签，并识别了包含常见数据缺陷的扫描列表。

Conclusion: 该数据集填补了左心耳、冠状动脉和肺静脉高质量分割数据的空白，为医学图像分析研究提供了宝贵资源，特别有助于左心耳形态分析的新方法开发。

Abstract: Despite the success of advanced segmentation frameworks such as TotalSegmentator (TS), accurate segmentations of the left atrial appendage (LAA), coronary arteries (CAs), and pulmonary veins (PVs) remain a significant challenge in medical imaging. In this work, we present the first open-source, anatomically coherent dataset of curated, high-resolution segmentations for these structures, supplemented with whole-heart labels produced by TS on the publicly available ImageCAS dataset consisting of 1000 cardiac computed tomography angiography (CCTA) scans. One purpose of the data set is to foster novel approaches to the analysis of LAA morphology.   LAA segmentations on ImageCAS were generated using a state-of-the-art segmentation framework developed specifically for high resolution LAA segmentation. We trained the network on a large private dataset with manual annotations provided by medical readers guided by a trained cardiologist and transferred the model to ImageCAS data. CA labels were improved from the original ImageCAS annotations, while PV segmentations were refined from TS outputs. In addition, we provide a list of scans from ImageCAS that contains common data flaws such as step artefacts, LAAs extending beyond the scanner's field of view, and other types of data defects.

</details>


### [17] [Compact Multi-level-prior Tensor Representation for Hyperspectral Image Super-resolution](https://arxiv.org/abs/2510.06098)
*Yinjian Wang,Wei Li,Yuanyuan Gui,Gemine Vivone*

Main category: cs.CV

TL;DR: 提出了一种新的高光谱图像超分辨率模型，通过块项分解将光谱低秩性和空间先验解耦，并引入非凸模式混洗张量相关总变分来共同建模高阶空间低秩性和平滑性先验。


<details>
  <summary>Details</summary>
Motivation: 现有基于张量的高光谱图像超分辨率方法只能有效利用一个或两个级别的先验，同时融入多级先验会增加模型复杂度，难以平衡不同先验的权重和优化多块结构。

Method: 使用块项分解将潜在高空间-光谱分辨率图像分解为光谱子空间和空间映射，将空间映射堆叠为空间张量，通过非凸模式混洗张量相关总变分共同建模高阶空间低秩性和平滑性先验，并设计了基于线性化交替方向乘子法的高效优化算法。

Result: 在多个数据集上的实验证明了所提算法的有效性。

Conclusion: 该模型紧凑地刻画了高光谱图像的多级先验，能够有效解决高光谱图像超分辨率问题，并提供了理论收敛保证。

Abstract: Fusing a hyperspectral image with a multispectral image acquired over the same scene, \textit{i.e.}, hyperspectral image super-resolution, has become a popular computational way to access the latent high-spatial-spectral-resolution image. To date, a variety of fusion methods have been proposed, among which the tensor-based ones have testified that multiple priors, such as multidimensional low-rankness and spatial total variation at multiple levels, effectively drive the fusion process. However, existing tensor-based models can only effectively leverage one or two priors at one or two levels, since simultaneously incorporating multi-level priors inevitably increases model complexity. This introduces challenges in both balancing the weights of different priors and optimizing multi-block structures. Concerning this, we present a novel hyperspectral super-resolution model compactly characterizing these multi-level priors of hyperspectral images within the tensor framework. Firstly, the proposed model decouples the spectral low-rankness and spatial priors by casting the latent high-spatial-spectral-resolution image into spectral subspace and spatial maps via block term decomposition. Secondly, these spatial maps are stacked as the spatial tensor encoding the high-order spatial low-rankness and smoothness priors, which are co-modeled via the proposed non-convex mode-shuffled tensor correlated total variation. Finally, we draw inspiration from the linearized alternating direction method of multipliers to design an efficient algorithm to optimize the resulting model, theoretically proving its Karush-Kuhn-Tucker convergence under mild conditions. Experiments on multiple datasets demonstrate the effectiveness of the proposed algorithm. The code implementation will be available from https://github.com/WongYinJ.

</details>


### [18] [Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation](https://arxiv.org/abs/2510.06131)
*Jiawei Mao,Yuhan Wang,Lifeng Chen,Can Zhao,Yucheng Tang,Dong Yang,Liangqiong Qu,Daguang Xu,Yuyin Zhou*

Main category: cs.CV

TL;DR: MeDiM是首个医学离散扩散模型，通过共享概率空间统一多模态生成任务，无需特定模态组件，实现了跨模态的高质量医学图像和报告生成。


<details>
  <summary>Details</summary>
Motivation: 现有生成医学模型受限于特定模态场景，无法整合影像、病理和临床记录等互补证据，阻碍了基础模型的发展。

Method: 基于离散扩散框架，使用多模态大语言模型作为扩散主干，移除因果注意力掩码实现双向上下文，注入连续时间步嵌入实现扩散感知。

Result: 在MIMIC-CXR上FID为16.60，PathGen上FID为24.19；报告生成METEOR分别为0.2650和0.2580；联合生成的图像-报告对显著提升下游性能。

Conclusion: MeDiM支持连贯且临床基础的多模态输出，为跨模态医学生成提供了有效解决方案。

Abstract: Recent advances in generative medical models are constrained by modality-specific scenarios that hinder the integration of complementary evidence from imaging, pathology, and clinical notes. This fragmentation limits their evolution into foundation models that can learn and reason across the full spectrum of biomedical data. We propose MeDiM, the first medical discrete diffusion model that learns shared distributions across modalities without modality-specific components. MeDiM unifies multiple generative tasks: translating between images and text, and jointly producing image-report pairs across domains in response to prompts. Built on a discrete diffusion framework, MeDiM bridges vision and language representations through a shared probabilistic space. To enable unified and flexible medical generation, we employ a multimodal large language model (MLLM) as the diffusion backbone, leveraging its prior knowledge and cross-modal reasoning. Two key designs are introduced: (1) removing the causal attention mask for bidirectional context, and (2) injecting continuous timestep embeddings for diffusion awareness. Experiments demonstrate high-fidelity medical generation (FID 16.60 on MIMIC-CXR and FID 24.19 on PathGen) and accurate report generation (METEOR 0.2650 and 0.2580). Jointly generated image-report pairs further enhance downstream performance (plus6.43 percent BLEU-1, plus18.57 percent BLEU-2, plus31.58 percent BLEU-3, plus4.80 percent METEOR), showing that MeDiM supports coherent and clinically grounded multimodal outputs.

</details>


### [19] [Deforming Videos to Masks: Flow Matching for Referring Video Segmentation](https://arxiv.org/abs/2510.06139)
*Zanyi Wang,Dengyang Jiang,Liuzhuozheng Li,Sizhe Dang,Chengzu Li,Harry Yang,Guang Dai,Mengmeng Wang,Jingdong Wang*

Main category: cs.CV

TL;DR: FlowRVS将参考视频对象分割重新定义为条件连续流问题，利用预训练T2V模型的优势，通过语言引导的变形从视频整体表示直接生成目标掩码，在多个基准测试中达到新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 解决传统'定位-分割'流水线存在的信息瓶颈和时序一致性问题，传统方法将语义简化为粗粒度几何提示且分割过程与初始语言定位解耦。

Method: 提出FlowRVS框架，将RVOS重新定义为条件连续流问题，学习从视频整体表示到目标掩码的语言引导直接变形，采用单阶段生成方法。

Result: 在所有主要RVOS基准测试中达到新的最先进结果：MeViS上J&F为51.1（比之前SOTA提高1.6），零样本Ref-DAVIS17上为73.3（提高2.7）。

Conclusion: 将视频理解任务建模为连续变形过程具有显著潜力，FlowRVS展示了这种方法的有效性。

Abstract: Referring Video Object Segmentation (RVOS) requires segmenting specific objects in a video guided by a natural language description. The core challenge of RVOS is to anchor abstract linguistic concepts onto a specific set of pixels and continuously segment them through the complex dynamics of a video. Faced with this difficulty, prior work has often decomposed the task into a pragmatic `locate-then-segment' pipeline. However, this cascaded design creates an information bottleneck by simplifying semantics into coarse geometric prompts (e.g, point), and struggles to maintain temporal consistency as the segmenting process is often decoupled from the initial language grounding. To overcome these fundamental limitations, we propose FlowRVS, a novel framework that reconceptualizes RVOS as a conditional continuous flow problem. This allows us to harness the inherent strengths of pretrained T2V models, fine-grained pixel control, text-video semantic alignment, and temporal coherence. Instead of conventional generating from noise to mask or directly predicting mask, we reformulate the task by learning a direct, language-guided deformation from a video's holistic representation to its target mask. Our one-stage, generative approach achieves new state-of-the-art results across all major RVOS benchmarks. Specifically, achieving a $\mathcal{J}\&\mathcal{F}$ of 51.1 in MeViS (+1.6 over prior SOTA) and 73.3 in the zero shot Ref-DAVIS17 (+2.7), demonstrating the significant potential of modeling video understanding tasks as continuous deformation processes.

</details>


### [20] [Bimanual 3D Hand Motion and Articulation Forecasting in Everyday Images](https://arxiv.org/abs/2510.06145)
*Aditya Prakash,David Forsyth,Saurabh Gupta*

Main category: cs.CV

TL;DR: 该论文提出了一种从单张图像预测日常场景中双手3D运动与关节的方法，通过扩散模型将2D手部关键点序列提升为4D手部运动，并使用扩散损失处理手部运动分布的多模态性。


<details>
  <summary>Details</summary>
Motivation: 解决在多样化场景中缺乏3D手部标注数据的问题，实现从单张图像准确预测双手3D运动与关节。

Method: 设计了一个标注流程，使用扩散模型将2D手部关键点序列提升为4D手部运动；预测模型采用扩散损失来处理手部运动分布的多模态性。

Result: 在6个数据集上的广泛实验显示，使用推算标签的多样化数据训练带来了14%的改进，提升模型性能提升42%，预测模型性能提升16.4%，特别是在日常图像的零样本泛化方面表现优异。

Conclusion: 提出的方法在双手3D运动预测方面显著优于现有基线方法，特别是在零样本泛化到日常图像场景中表现出色。

Abstract: We tackle the problem of forecasting bimanual 3D hand motion & articulation from a single image in everyday settings. To address the lack of 3D hand annotations in diverse settings, we design an annotation pipeline consisting of a diffusion model to lift 2D hand keypoint sequences to 4D hand motion. For the forecasting model, we adopt a diffusion loss to account for the multimodality in hand motion distribution. Extensive experiments across 6 datasets show the benefits of training on diverse data with imputed labels (14% improvement) and effectiveness of our lifting (42% better) & forecasting (16.4% gain) models, over the best baselines, especially in zero-shot generalization to everyday images.

</details>


### [21] [Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models](https://arxiv.org/abs/2510.06209)
*Jiahao Wang,Zhenpei Yang,Yijing Bai,Yingwei Li,Yuliang Zou,Bo Sun,Abhijit Kundu,Jose Lezama,Luna Yue Huang,Zehao Zhu,Jyh-Jing Hwang,Dragomir Anguelov,Mingxing Tan,Chiyu Max Jiang*

Main category: cs.CV

TL;DR: 该论文提出Drive&Gen框架，将端到端驾驶模型与生成式世界模型结合，用于评估生成视频的真实性、分析驾驶模型性能差距，并利用合成数据提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决生成视频能否真实反映指定条件以评估端到端自动驾驶规划器，以及如何深入理解端到端规划器的偏见并提升其在分布外场景的泛化能力。

Method: 提出基于端到端驾驶模型的统计度量来评估生成视频的真实性；利用视频生成模型的可控性进行针对性实验；使用合成数据改进端到端模型的泛化能力。

Result: 生成视频能够有效评估端到端规划器；合成数据可作为真实数据收集的低成本替代方案，有效提升模型在超出设计域范围的泛化性能。

Conclusion: Drive&Gen框架成功弥合了驾驶模型与生成式世界模型之间的差距，为自动驾驶系统的评估和改进提供了有效工具，促进了自动驾驶服务向新运营环境的扩展。

Abstract: Recent advances in generative models have sparked exciting new possibilities in the field of autonomous vehicles. Specifically, video generation models are now being explored as controllable virtual testing environments. Simultaneously, end-to-end (E2E) driving models have emerged as a streamlined alternative to conventional modular autonomous driving systems, gaining popularity for their simplicity and scalability. However, the application of these techniques to simulation and planning raises important questions. First, while video generation models can generate increasingly realistic videos, can these videos faithfully adhere to the specified conditions and be realistic enough for E2E autonomous planner evaluation? Second, given that data is crucial for understanding and controlling E2E planners, how can we gain deeper insights into their biases and improve their ability to generalize to out-of-distribution scenarios? In this work, we bridge the gap between the driving models and generative world models (Drive&Gen) to address these questions. We propose novel statistical measures leveraging E2E drivers to evaluate the realism of generated videos. By exploiting the controllability of the video generation model, we conduct targeted experiments to investigate distribution gaps affecting E2E planner performance. Finally, we show that synthetic data produced by the video generation model offers a cost-effective alternative to real-world data collection. This synthetic data effectively improves E2E model generalization beyond existing Operational Design Domains, facilitating the expansion of autonomous vehicle services into new operational contexts.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [22] [SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models](https://arxiv.org/abs/2510.05173)
*Peigui Qi,Kunsheng Tang,Wenbo Zhou,Weiming Zhang,Nenghai Yu,Tianwei Zhang,Qing Guo,Jie Zhang*

Main category: cs.CR

TL;DR: SafeGuider是一个针对文本到图像模型的安全防护框架，通过分析[EOS]令牌的语义聚合特性，结合嵌入级识别模型和安全感知特征擦除波束搜索算法，在保持高质量图像生成的同时有效防御对抗性提示攻击。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型容易受到对抗性提示攻击，绕过安全措施生成有害内容。现有防御策略在保持实用性和鲁棒性方面存在挑战，需要一种既能防御攻击又不影响良性提示生成质量的解决方案。

Method: 首先对Stable Diffusion模型的文本编码器进行实证研究，发现[EOS]令牌在良性提示和对抗性提示的嵌入空间中具有不同的分布模式。基于此开发SafeGuider框架，包含嵌入级识别模型和安全感知特征擦除波束搜索算法。

Result: SafeGuider在各种攻击场景下的最大攻击成功率仅为5.48%，能有效防御域内和域外攻击。对于不安全提示，不是拒绝生成或生成黑图，而是生成安全且有意义的图像，增强了实用性。该框架还可应用于其他文本到图像模型如Flux。

Conclusion: SafeGuider为安全文本到图像系统的实际部署提供了有效解决方案，在保持高质量图像生成的同时实现了强大的安全防护，具有很好的通用性和适应性。

Abstract: Text-to-image models have shown remarkable capabilities in generating high-quality images from natural language descriptions. However, these models are highly vulnerable to adversarial prompts, which can bypass safety measures and produce harmful content. Despite various defensive strategies, achieving robustness against attacks while maintaining practical utility in real-world applications remains a significant challenge. To address this issue, we first conduct an empirical study of the text encoder in the Stable Diffusion (SD) model, which is a widely used and representative text-to-image model. Our findings reveal that the [EOS] token acts as a semantic aggregator, exhibiting distinct distributional patterns between benign and adversarial prompts in its embedding space. Building on this insight, we introduce \textbf{SafeGuider}, a two-step framework designed for robust safety control without compromising generation quality. SafeGuider combines an embedding-level recognition model with a safety-aware feature erasure beam search algorithm. This integration enables the framework to maintain high-quality image generation for benign prompts while ensuring robust defense against both in-domain and out-of-domain attacks. SafeGuider demonstrates exceptional effectiveness in minimizing attack success rates, achieving a maximum rate of only 5.48\% across various attack scenarios. Moreover, instead of refusing to generate or producing black images for unsafe prompts, \textbf{SafeGuider} generates safe and meaningful images, enhancing its practical utility. In addition, SafeGuider is not limited to the SD model and can be effectively applied to other text-to-image models, such as the Flux model, demonstrating its versatility and adaptability across different architectures. We hope that SafeGuider can shed some light on the practical deployment of secure text-to-image systems.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [23] [Controllable Audio-Visual Viewpoint Generation from 360° Spatial Information](https://arxiv.org/abs/2510.06060)
*Christian Marinoni,Riccardo Fosco Gramaccioni,Eleonora Grassucci,Danilo Comminiello*

Main category: cs.MM

TL;DR: 提出了首个可控音频-视频生成框架，通过全景显著性图、边界框感知距离图和场景描述等条件信号，从360度环境中生成视角特定的视频和音频。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏从大型沉浸式360度环境生成视角特定内容的细粒度控制能力，限制了创建能够感知镜头外事件的音频-视觉体验。

Method: 使用扩散模型，引入全景显著性图识别感兴趣区域、边界框感知有符号距离图定义目标视角、以及整个场景的描述性标题作为条件信号。

Result: 模型能够生成空间感知的视角视频和音频，这些内容与更广泛的、不可见的环境上下文连贯地相互影响。

Conclusion: 该框架为逼真和沉浸式的音频-视觉生成引入了关键的可控性，通过音频-视觉示例证明了其有效性。

Abstract: The generation of sounding videos has seen significant advancements with the advent of diffusion models. However, existing methods often lack the fine-grained control needed to generate viewpoint-specific content from larger, immersive 360-degree environments. This limitation restricts the creation of audio-visual experiences that are aware of off-camera events. To the best of our knowledge, this is the first work to introduce a framework for controllable audio-visual generation, addressing this unexplored gap. Specifically, we propose a diffusion model by introducing a set of powerful conditioning signals derived from the full 360-degree space: a panoramic saliency map to identify regions of interest, a bounding-box-aware signed distance map to define the target viewpoint, and a descriptive caption of the entire scene. By integrating these controls, our model generates spatially-aware viewpoint videos and audios that are coherently influenced by the broader, unseen environmental context, introducing a strong controllability that is essential for realistic and immersive audio-visual generation. We show audiovisual examples proving the effectiveness of our framework.

</details>
