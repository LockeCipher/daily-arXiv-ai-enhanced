{"id": "2506.10035", "pdf": "https://arxiv.org/pdf/2506.10035", "abs": "https://arxiv.org/abs/2506.10035", "authors": ["Fuhan Cai", "Yong Guo", "Jie Li", "Wenbo Li", "Xiangzhong Fang", "Jian Chen"], "title": "FastFLUX: Pruning FLUX with Block-wise Replacement and Sandwich Training", "categories": ["cs.GR", "cs.AI"], "comment": "14 pages", "summary": "Recent advancements in text-to-image (T2I) generation have led to the emergence of highly expressive models such as diffusion transformers (DiTs), exemplified by FLUX. However, their massive parameter sizes lead to slow inference, high memory usage, and poor deployability. Existing acceleration methods (e.g., single-step distillation and attention pruning) often suffer from significant performance degradation and incur substantial training costs. To address these limitations, we propose FastFLUX, an architecture-level pruning framework designed to enhance the inference efficiency of FLUX. At its core is the Block-wise Replacement with Linear Layers (BRLL) method, which replaces structurally complex residual branches in ResBlocks with lightweight linear layers while preserving the original shortcut connections for stability. Furthermore, we introduce Sandwich Training (ST), a localized fine-tuning strategy that leverages LoRA to supervise neighboring blocks, mitigating performance drops caused by structural replacement. Experiments show that our FastFLUX maintains high image quality under both qualitative and quantitative evaluations, while significantly improving inference speed, even with 20\\% of the hierarchy pruned. Our code will be available soon.", "AI": {"tldr": "FastFLUX\u901a\u8fc7\u67b6\u6784\u7ea7\u526a\u679d\u6846\u67b6\u63d0\u5347FLUX\u7684\u63a8\u7406\u6548\u7387\uff0c\u91c7\u7528BRLL\u65b9\u6cd5\u548cST\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u52a0\u901f\u63a8\u7406\u4e14\u4fdd\u6301\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u73b0\u6709T2I\u751f\u6210\u6a21\u578b\uff08\u5982FLUX\uff09\u53c2\u6570\u89c4\u6a21\u5927\uff0c\u63a8\u7406\u901f\u5ea6\u6162\u4e14\u90e8\u7f72\u56f0\u96be\uff0c\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u6027\u80fd\u4e0b\u964d\u660e\u663e\u4e14\u8bad\u7ec3\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51faFastFLUX\u6846\u67b6\uff0c\u6838\u5fc3\u662fBRLL\u65b9\u6cd5\uff08\u7528\u7ebf\u6027\u5c42\u66ff\u6362\u590d\u6742\u6b8b\u5dee\u5206\u652f\uff09\u548cST\u8bad\u7ec3\u7b56\u7565\uff08\u5c40\u90e8\u5fae\u8c03\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFastFLUX\u5728\u526a\u679d20%\u5c42\u7ea7\u540e\u4ecd\u4fdd\u6301\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "FastFLUX\u6709\u6548\u89e3\u51b3\u4e86FLUX\u7684\u63a8\u7406\u6548\u7387\u95ee\u9898\uff0c\u4e3aT2I\u6a21\u578b\u52a0\u901f\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.10036", "pdf": "https://arxiv.org/pdf/2506.10036", "abs": "https://arxiv.org/abs/2506.10036", "authors": ["Javad Rajabi", "Soroush Mehraban", "Seyedmorteza Sadat", "Babak Taati"], "title": "Token Perturbation Guidance for Diffusion Models", "categories": ["cs.GR", "cs.CL"], "comment": "18 pages, 14 figures", "summary": "Classifier-free guidance (CFG) has become an essential component of modern diffusion models to enhance both generation quality and alignment with input conditions. However, CFG requires specific training procedures and is limited to conditional generation. To address these limitations, we propose Token Perturbation Guidance (TPG), a novel method that applies perturbation matrices directly to intermediate token representations within the diffusion network. TPG employs a norm-preserving shuffling operation to provide effective and stable guidance signals that improve generation quality without architectural changes. As a result, TPG is training-free and agnostic to input conditions, making it readily applicable to both conditional and unconditional generation. We further analyze the guidance term provided by TPG and show that its effect on sampling more closely resembles CFG compared to existing training-free guidance techniques. Extensive experiments on SDXL and Stable Diffusion 2.1 show that TPG achieves nearly a 2$\\times$ improvement in FID for unconditional generation over the SDXL baseline, while closely matching CFG in prompt alignment. These results establish TPG as a general, condition-agnostic guidance method that brings CFG-like benefits to a broader class of diffusion models. The code is available at https://github.com/TaatiTeam/Token-Perturbation-Guidance", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aToken Perturbation Guidance (TPG)\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6270\u52a8\u6269\u6563\u7f51\u7edc\u4e2d\u7684\u4e2d\u95f4\u4ee4\u724c\u8868\u793a\uff0c\u65e0\u9700\u7279\u5b9a\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3Classifier-free guidance (CFG)\u9700\u8981\u7279\u5b9a\u8bad\u7ec3\u4e14\u4ec5\u9002\u7528\u4e8e\u6761\u4ef6\u751f\u6210\u7684\u5c40\u9650\u6027\u3002", "method": "TPG\u901a\u8fc7\u89c4\u8303\u4fdd\u6301\u7684\u6d17\u724c\u64cd\u4f5c\u76f4\u63a5\u6270\u52a8\u4e2d\u95f4\u4ee4\u724c\u8868\u793a\uff0c\u63d0\u4f9b\u7a33\u5b9a\u4e14\u6709\u6548\u7684\u6307\u5bfc\u4fe1\u53f7\u3002", "result": "\u5728SDXL\u548cStable Diffusion 2.1\u4e0a\uff0cTPG\u5728\u65e0\u6761\u4ef6\u751f\u6210\u4e2dFID\u63d0\u5347\u8fd12\u500d\uff0c\u540c\u65f6\u5728\u63d0\u793a\u5bf9\u9f50\u4e0a\u4e0eCFG\u8868\u73b0\u63a5\u8fd1\u3002", "conclusion": "TPG\u662f\u4e00\u79cd\u901a\u7528\u4e14\u6761\u4ef6\u65e0\u5173\u7684\u6307\u5bfc\u65b9\u6cd5\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u7684\u6269\u6563\u6a21\u578b\u5e26\u6765\u7c7b\u4f3cCFG\u7684\u4f18\u52bf\u3002"}}
{"id": "2506.10038", "pdf": "https://arxiv.org/pdf/2506.10038", "abs": "https://arxiv.org/abs/2506.10038", "authors": ["Giannis Daras", "Adrian Rodriguez-Munoz", "Adam Klivans", "Antonio Torralba", "Constantinos Daskalakis"], "title": "Ambient Diffusion Omni: Training Good Models with Bad Data", "categories": ["cs.GR", "cs.AI", "cs.LG"], "comment": "Preprint, work in progress", "summary": "We show how to use low-quality, synthetic, and out-of-distribution images to improve the quality of a diffusion model. Typically, diffusion models are trained on curated datasets that emerge from highly filtered data pools from the Web and other sources. We show that there is immense value in the lower-quality images that are often discarded. We present Ambient Diffusion Omni, a simple, principled framework to train diffusion models that can extract signal from all available images during training. Our framework exploits two properties of natural images -- spectral power law decay and locality. We first validate our framework by successfully training diffusion models with images synthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We then use our framework to achieve state-of-the-art ImageNet FID, and we show significant improvements in both image quality and diversity for text-to-image generative modeling. The core insight is that noise dampens the initial skew between the desired high-quality distribution and the mixed distribution we actually observe. We provide rigorous theoretical justification for our approach by analyzing the trade-off between learning from biased data versus limited unbiased data across diffusion times.", "AI": {"tldr": "\u901a\u8fc7\u5229\u7528\u4f4e\u8d28\u91cf\u3001\u5408\u6210\u548c\u5206\u5e03\u5916\u56fe\u50cf\uff0cAmbient Diffusion Omni\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u56fe\u50cf\u8d28\u91cf\u548c\u591a\u6837\u6027\u7684\u6539\u8fdb\u3002", "motivation": "\u4f20\u7edf\u6269\u6563\u6a21\u578b\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u4f46\u4f4e\u8d28\u91cf\u56fe\u50cf\u4e2d\u4ecd\u8574\u542b\u4ef7\u503c\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5229\u7528\u8fd9\u4e9b\u88ab\u4e22\u5f03\u7684\u56fe\u50cf\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51faAmbient Diffusion Omni\u6846\u67b6\uff0c\u5229\u7528\u81ea\u7136\u56fe\u50cf\u7684\u5149\u8c31\u5e42\u5f8b\u8870\u51cf\u548c\u5c40\u90e8\u6027\u7279\u6027\uff0c\u4ece\u6240\u6709\u53ef\u7528\u56fe\u50cf\u4e2d\u63d0\u53d6\u4fe1\u53f7\u3002", "result": "\u5728\u5408\u6210\u635f\u574f\u56fe\u50cf\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\uff0c\u5e76\u5728ImageNet\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f73FID\uff0c\u540c\u65f6\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002", "conclusion": "\u566a\u58f0\u53ef\u4ee5\u7f13\u89e3\u9ad8\u8d28\u91cf\u5206\u5e03\u4e0e\u6df7\u5408\u5206\u5e03\u4e4b\u95f4\u7684\u521d\u59cb\u504f\u5dee\uff0c\u7406\u8bba\u5206\u6790\u652f\u6301\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.10005", "pdf": "https://arxiv.org/pdf/2506.10005", "abs": "https://arxiv.org/abs/2506.10005", "authors": ["Sridhar S", "Nithin A", "Shakeel Rifath", "Vasantha Raj"], "title": "Multimodal Cinematic Video Synthesis Using Text-to-Image and Audio Generation Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.GR", "cs.MM"], "comment": "10 pages, seven figures about Multimodal Cinematic Video Synthesis   Using Text-to-Image and Audio Generation Models", "summary": "Advances in generative artificial intelligence have altered multimedia creation, allowing for automatic cinematic video synthesis from text inputs. This work describes a method for creating 60-second cinematic movies incorporating Stable Diffusion for high-fidelity image synthesis, GPT-2 for narrative structuring, and a hybrid audio pipeline using gTTS and YouTube-sourced music. It uses a five-scene framework, which is augmented by linear frame interpolation, cinematic post-processing (e.g., sharpening), and audio-video synchronization to provide professional-quality results. It was created in a GPU-accelerated Google Colab environment using Python 3.11. It has a dual-mode Gradio interface (Simple and Advanced), which supports resolutions of up to 1024x768 and frame rates of 15-30 FPS. Optimizations such as CUDA memory management and error handling ensure reliability. The experiments demonstrate outstanding visual quality, narrative coherence, and efficiency, furthering text-to-video synthesis for creative, educational, and industrial applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Stable Diffusion\u3001GPT-2\u548c\u6df7\u5408\u97f3\u9891\u7ba1\u9053\u7684\u81ea\u52a8\u751f\u621060\u79d2\u7535\u5f71\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e94\u573a\u666f\u6846\u67b6\u3001\u5e27\u63d2\u503c\u548c\u97f3\u89c6\u9891\u540c\u6b65\u5b9e\u73b0\u4e13\u4e1a\u8d28\u91cf\u3002", "motivation": "\u5229\u7528\u751f\u6210\u5f0fAI\u6280\u672f\u6539\u8fdb\u591a\u5a92\u4f53\u521b\u4f5c\uff0c\u5b9e\u73b0\u4ece\u6587\u672c\u8f93\u5165\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u7535\u5f71\u3002", "method": "\u7ed3\u5408Stable Diffusion\u751f\u6210\u56fe\u50cf\u3001GPT-2\u6784\u5efa\u53d9\u4e8b\uff0c\u6df7\u5408\u97f3\u9891\u7ba1\u9053\uff08gTTS\u548cYouTube\u97f3\u4e50\uff09\uff0c\u5e76\u91c7\u7528\u4e94\u573a\u666f\u6846\u67b6\u3001\u5e27\u63d2\u503c\u548c\u97f3\u89c6\u9891\u540c\u6b65\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u51fa\u8272\u7684\u89c6\u89c9\u8d28\u91cf\u3001\u53d9\u4e8b\u8fde\u8d2f\u6027\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u521b\u610f\u3001\u6559\u80b2\u548c\u5de5\u4e1a\u5e94\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8fdb\u4e00\u6b65\u63a8\u52a8\u4e86\u6587\u672c\u5230\u89c6\u9891\u5408\u6210\u7684\u5e94\u7528\uff0c\u5c55\u793a\u4e86\u751f\u6210\u5f0fAI\u5728\u591a\u5a92\u4f53\u521b\u4f5c\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.10468", "pdf": "https://arxiv.org/pdf/2506.10468", "abs": "https://arxiv.org/abs/2506.10468", "authors": ["Zaiqiang Wu", "Yechen Li", "Jingyuan Liu", "Yuki Shibata", "Takayuki Hori", "I-Chao Shen", "Takeo Igarashi"], "title": "Low-Barrier Dataset Collection with Real Human Body for Interactive Per-Garment Virtual Try-On", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Existing image-based virtual try-on methods are often limited to the front view and lack real-time performance. While per-garment virtual try-on methods have tackled these issues by capturing per-garment datasets and training per-garment neural networks, they still encounter practical limitations: (1) the robotic mannequin used to capture per-garment datasets is prohibitively expensive for widespread adoption and fails to accurately replicate natural human body deformation; (2) the synthesized garments often misalign with the human body. To address these challenges, we propose a low-barrier approach for collecting per-garment datasets using real human bodies, eliminating the necessity for a customized robotic mannequin. We also introduce a hybrid person representation that enhances the existing intermediate representation with a simplified DensePose map. This ensures accurate alignment of synthesized garment images with the human body and enables human-garment interaction without the need for customized wearable devices. We performed qualitative and quantitative evaluations against other state-of-the-art image-based virtual try-on methods and conducted ablation studies to demonstrate the superiority of our method regarding image quality and temporal consistency. Finally, our user study results indicated that most participants found our virtual try-on system helpful for making garment purchasing decisions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u57fa\u4e8e\u771f\u4eba\u6570\u636e\u7684\u865a\u62df\u8bd5\u8863\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u673a\u5668\u4eba\u6a21\u578b\u548c\u670d\u88c5\u5bf9\u9f50\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u865a\u62df\u8bd5\u8863\u65b9\u6cd5\u5c40\u9650\u4e8e\u6b63\u9762\u89c6\u89d2\u4e14\u7f3a\u4e4f\u5b9e\u65f6\u6027\uff0c\u800c\u57fa\u4e8e\u5355\u4ef6\u670d\u88c5\u7684\u65b9\u6cd5\u867d\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u4ecd\u5b58\u5728\u6570\u636e\u96c6\u91c7\u96c6\u6210\u672c\u9ad8\u548c\u670d\u88c5\u4e0e\u4eba\u4f53\u5bf9\u9f50\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u771f\u4eba\u8eab\u4f53\u91c7\u96c6\u6570\u636e\u96c6\uff0c\u907f\u514d\u4f7f\u7528\u6602\u8d35\u7684\u673a\u5668\u4eba\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u6df7\u5408\u4eba\u7269\u8868\u793a\u65b9\u6cd5\uff08\u7ed3\u5408\u7b80\u5316\u7684DensePose\u56fe\uff09\u4ee5\u63d0\u5347\u670d\u88c5\u5bf9\u9f50\u7cbe\u5ea6\u3002", "result": "\u901a\u8fc7\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4e0a\u7684\u4f18\u8d8a\u6027\u3002\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u6709\u52a9\u4e8e\u670d\u88c5\u8d2d\u4e70\u51b3\u7b56\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u964d\u4f4e\u4e86\u865a\u62df\u8bd5\u8863\u7684\u95e8\u69db\uff0c\u63d0\u5347\u4e86\u670d\u88c5\u5bf9\u9f50\u7684\u51c6\u786e\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.10082", "pdf": "https://arxiv.org/pdf/2506.10082", "abs": "https://arxiv.org/abs/2506.10082", "authors": ["Chenjian Gao", "Lihe Ding", "Xin Cai", "Zhanpeng Huang", "Zibin Wang", "Tianfan Xue"], "title": "LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware LoRA Fine-Tuning", "categories": ["cs.CV"], "comment": "12 pages", "summary": "Video editing using diffusion models has achieved remarkable results in generating high-quality edits for videos. However, current methods often rely on large-scale pretraining, limiting flexibility for specific edits. First-frame-guided editing provides control over the first frame, but lacks flexibility over subsequent frames. To address this, we propose a mask-based LoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video (I2V) models for flexible video editing. Our approach preserves background regions while enabling controllable edits propagation. This solution offers efficient and adaptable video editing without altering the model architecture. To better steer this process, we incorporate additional references, such as alternate viewpoints or representative scene states, which serve as visual anchors for how content should unfold. We address the control challenge using a mask-driven LoRA tuning strategy that adapts a pre-trained image-to-video model to the editing context. The model must learn from two distinct sources: the input video provides spatial structure and motion cues, while reference images offer appearance guidance. A spatial mask enables region-specific learning by dynamically modulating what the model attends to, ensuring that each area draws from the appropriate source. Experimental results show our method achieves superior video editing performance compared to state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a9\u7801\u7684LoRA\u8c03\u4f18\u65b9\u6cd5\uff0c\u7528\u4e8e\u7075\u6d3b\u7684\u89c6\u9891\u7f16\u8f91\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u67b6\u6784\uff0c\u540c\u65f6\u4fdd\u7559\u80cc\u666f\u533a\u57df\u5e76\u5b9e\u73b0\u53ef\u63a7\u7f16\u8f91\u4f20\u64ad\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u4e14\u9996\u5e27\u5f15\u5bfc\u7f16\u8f91\u5bf9\u540e\u7eed\u5e27\u63a7\u5236\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u63a9\u7801\u9a71\u52a8\u7684LoRA\u8c03\u4f18\u7b56\u7565\uff0c\u7ed3\u5408\u8f93\u5165\u89c6\u9891\u7684\u7a7a\u95f4\u7ed3\u6784\u548c\u53c2\u8003\u56fe\u50cf\u7684\u5916\u89c2\u6307\u5bfc\uff0c\u52a8\u6001\u8c03\u8282\u6a21\u578b\u6ce8\u610f\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u9891\u7f16\u8f91\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u89c6\u9891\u7f16\u8f91\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u7279\u5b9a\u7f16\u8f91\u9700\u6c42\u3002"}}
{"id": "2506.10507", "pdf": "https://arxiv.org/pdf/2506.10507", "abs": "https://arxiv.org/abs/2506.10507", "authors": ["Junchao Huang", "Xinting Hu", "Zhuotao Tian", "Shaoshuai Shi", "Li Jiang"], "title": "Edit360: 2D Image Edits to 3D Assets from Any Angle", "categories": ["cs.GR", "cs.CV"], "comment": "11 pages, 9 figures", "summary": "Recent advances in diffusion models have significantly improved image generation and editing, but extending these capabilities to 3D assets remains challenging, especially for fine-grained edits that require multi-view consistency. Existing methods typically restrict editing to predetermined viewing angles, severely limiting their flexibility and practical applications. We introduce Edit360, a tuning-free framework that extends 2D modifications to multi-view consistent 3D editing. Built upon video diffusion models, Edit360 enables user-specific editing from arbitrary viewpoints while ensuring structural coherence across all views. The framework selects anchor views for 2D modifications and propagates edits across the entire 360-degree range. To achieve this, Edit360 introduces a novel Anchor-View Editing Propagation mechanism, which effectively aligns and merges multi-view information within the latent and attention spaces of diffusion models. The resulting edited multi-view sequences facilitate the reconstruction of high-quality 3D assets, enabling customizable 3D content creation.", "AI": {"tldr": "Edit360\u662f\u4e00\u4e2a\u65e0\u9700\u8c03\u6574\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u9891\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u591a\u89c6\u89d2\u4e00\u81f4\u76843D\u7f16\u8f91\uff0c\u652f\u6301\u4efb\u610f\u89c6\u89d2\u7684\u7528\u6237\u5b9a\u5236\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u76843D\u7f16\u8f91\u65b9\u6cd5\u901a\u5e38\u5c40\u9650\u4e8e\u9884\u5b9a\u7684\u89c6\u89d2\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u5b9e\u9645\u5e94\u7528\u3002", "method": "Edit360\u57fa\u4e8e\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u5f15\u5165\u951a\u70b9\u89c6\u89d2\u7f16\u8f91\u4f20\u64ad\u673a\u5236\uff0c\u5728\u6f5c\u5728\u548c\u6ce8\u610f\u529b\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u591a\u89c6\u89d2\u4fe1\u606f\u3002", "result": "\u6846\u67b6\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u591a\u89c6\u89d2\u5e8f\u5217\uff0c\u652f\u6301\u53ef\u5b9a\u5236\u76843D\u5185\u5bb9\u521b\u5efa\u3002", "conclusion": "Edit360\u4e3a3D\u8d44\u4ea7\u7684\u591a\u89c6\u89d2\u4e00\u81f4\u7f16\u8f91\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.10173", "pdf": "https://arxiv.org/pdf/2506.10173", "abs": "https://arxiv.org/abs/2506.10173", "authors": ["Mohammad Jalali", "Haoyu Lei", "Amin Gohari", "Farzan Farnia"], "title": "SPARKE: Scalable Prompt-Aware Diversity Guidance in Diffusion Models via RKE Score", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Diffusion models have demonstrated remarkable success in high-fidelity image synthesis and prompt-guided generative modeling. However, ensuring adequate diversity in generated samples of prompt-guided diffusion models remains a challenge, particularly when the prompts span a broad semantic spectrum and the diversity of generated data needs to be evaluated in a prompt-aware fashion across semantically similar prompts. Recent methods have introduced guidance via diversity measures to encourage more varied generations. In this work, we extend the diversity measure-based approaches by proposing the Scalable Prompt-Aware R\\'eny Kernel Entropy Diversity Guidance (SPARKE) method for prompt-aware diversity guidance. SPARKE utilizes conditional entropy for diversity guidance, which dynamically conditions diversity measurement on similar prompts and enables prompt-aware diversity control. While the entropy-based guidance approach enhances prompt-aware diversity, its reliance on the matrix-based entropy scores poses computational challenges in large-scale generation settings. To address this, we focus on the special case of Conditional latent RKE Score Guidance, reducing entropy computation and gradient-based optimization complexity from the $O(n^3)$ of general entropy measures to $O(n)$. The reduced computational complexity allows for diversity-guided sampling over potentially thousands of generation rounds on different prompts. We numerically test the SPARKE method on several text-to-image diffusion models, demonstrating that the proposed method improves the prompt-aware diversity of the generated data without incurring significant computational costs. We release our code on the project page: https://mjalali.github.io/SPARKE", "AI": {"tldr": "SPARKE\u65b9\u6cd5\u901a\u8fc7\u6761\u4ef6\u71b5\u5b9e\u73b0\u63d0\u793a\u611f\u77e5\u591a\u6837\u6027\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u6837\u672c\u7684\u591a\u6837\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u89e3\u51b3\u63d0\u793a\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u6837\u672c\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8bed\u4e49\u5e7f\u6cdb\u7684\u63d0\u793a\u4e0b\u3002", "method": "\u63d0\u51faSPARKE\u65b9\u6cd5\uff0c\u5229\u7528\u6761\u4ef6\u71b5\u52a8\u6001\u6d4b\u91cf\u591a\u6837\u6027\uff0c\u5e76\u901a\u8fc7\u7b80\u5316\u8ba1\u7b97\u590d\u6742\u5ea6\uff08\u4eceO(n^3)\u5230O(n)\uff09\u5b9e\u73b0\u5927\u89c4\u6a21\u751f\u6210\u3002", "result": "\u5728\u591a\u4e2a\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0cSPARKE\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6570\u636e\u7684\u63d0\u793a\u611f\u77e5\u591a\u6837\u6027\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u3002", "conclusion": "SPARKE\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u63d0\u793a\u611f\u77e5\u591a\u6837\u6027\u5f15\u5bfc\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u751f\u6210\u4efb\u52a1\u3002"}}
{"id": "2506.10182", "pdf": "https://arxiv.org/pdf/2506.10182", "abs": "https://arxiv.org/abs/2506.10182", "authors": ["Fiona Ryan", "Josef Sivic", "Fabian Caba Heilbron", "Judy Hoffman", "James M. Rehg", "Bryan Russell"], "title": "Improving Personalized Search with Regularized Low-Rank Parameter Updates", "categories": ["cs.CV"], "comment": "CVPR 2025 Highlight. Code: http://github.com/adobe-research/polar-vl", "summary": "Personalized vision-language retrieval seeks to recognize new concepts (e.g. \"my dog Fido\") from only a few examples. This task is challenging because it requires not only learning a new concept from a few images, but also integrating the personal and general knowledge together to recognize the concept in different contexts. In this paper, we show how to effectively adapt the internal representation of a vision-language dual encoder model for personalized vision-language retrieval. We find that regularized low-rank adaption of a small set of parameters in the language encoder's final layer serves as a highly effective alternative to textual inversion for recognizing the personal concept while preserving general knowledge. Additionally, we explore strategies for combining parameters of multiple learned personal concepts, finding that parameter addition is effective. To evaluate how well general knowledge is preserved in a finetuned representation, we introduce a metric that measures image retrieval accuracy based on captions generated by a vision language model (VLM). Our approach achieves state-of-the-art accuracy on two benchmarks for personalized image retrieval with natural language queries - DeepFashion2 and ConCon-Chi - outperforming the prior art by 4%-22% on personal retrievals.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\u65b9\u6cd5\u6539\u8fdb\u89c6\u89c9-\u8bed\u8a00\u53cc\u7f16\u7801\u5668\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u89c6\u89c9-\u8bed\u8a00\u68c0\u7d22\u4efb\u52a1\uff0c\u5e76\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u4e2a\u6027\u5316\u89c6\u89c9-\u8bed\u8a00\u68c0\u7d22\u4efb\u52a1\u9700\u8981\u4ece\u5c11\u91cf\u793a\u4f8b\u4e2d\u5b66\u4e60\u65b0\u6982\u5ff5\uff0c\u5e76\u6574\u5408\u4e2a\u4eba\u4e0e\u901a\u7528\u77e5\u8bc6\u4ee5\u8bc6\u522b\u4e0d\u540c\u4e0a\u4e0b\u6587\u4e2d\u7684\u6982\u5ff5\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u91c7\u7528\u8bed\u8a00\u7f16\u7801\u5668\u6700\u540e\u4e00\u5c42\u7684\u6b63\u5219\u5316\u4f4e\u79e9\u9002\u5e94\u65b9\u6cd5\uff0c\u66ff\u4ee3\u6587\u672c\u53cd\u8f6c\uff0c\u4ee5\u4fdd\u7559\u901a\u7528\u77e5\u8bc6\u7684\u540c\u65f6\u8bc6\u522b\u4e2a\u4eba\u6982\u5ff5\u3002\u540c\u65f6\u63a2\u7d22\u4e86\u591a\u4e2a\u4eba\u6982\u5ff5\u53c2\u6570\u7684\u7ed3\u5408\u7b56\u7565\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08DeepFashion2\u548cConCon-Chi\uff09\u4e2d\uff0c\u4e2a\u6027\u5316\u68c0\u7d22\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd54%-22%\u3002", "conclusion": "\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\u548c\u53c2\u6570\u7ed3\u5408\u7b56\u7565\uff0c\u672c\u6587\u65b9\u6cd5\u5728\u4e2a\u6027\u5316\u89c6\u89c9-\u8bed\u8a00\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u901a\u7528\u77e5\u8bc6\u3002"}}
{"id": "2506.10335", "pdf": "https://arxiv.org/pdf/2506.10335", "abs": "https://arxiv.org/abs/2506.10335", "authors": ["Lintao Xiang", "Hongpei Zheng", "Yating Huang", "Qijun Yang", "Hujun Yin"], "title": "PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian splatting (3DGS) is an innovative rendering technique that surpasses the neural radiance field (NeRF) in both rendering speed and visual quality by leveraging an explicit 3D scene representation. Existing 3DGS approaches require a large number of calibrated views to generate a consistent and complete scene representation. When input views are limited, 3DGS tends to overfit the training views, leading to noticeable degradation in rendering quality. To address this limitation, we propose a Point-wise Feature-Aware Gaussian Splatting framework that enables real-time, high-quality rendering from sparse training views. Specifically, we first employ the latest stereo foundation model to estimate accurate camera poses and reconstruct a dense point cloud for Gaussian initialization. We then encode the colour attributes of each 3D Gaussian by sampling and aggregating multiscale 2D appearance features from sparse inputs. To enhance point-wise appearance representation, we design a point interaction network based on a self-attention mechanism, allowing each Gaussian point to interact with its nearest neighbors. These enriched features are subsequently decoded into Gaussian parameters through two lightweight multi-layer perceptrons (MLPs) for final rendering. Extensive experiments on diverse benchmarks demonstrate that our method significantly outperforms NeRF-based approaches and achieves competitive performance under few-shot settings compared to the state-of-the-art 3DGS methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u70b9\u7279\u5f81\u611f\u77e5\u7684\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u8bad\u7ec3\u89c6\u56fe\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u8d28\u91cf\u6e32\u67d3\u3002", "motivation": "\u73b0\u67093D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6821\u51c6\u89c6\u56fe\uff0c\u800c\u5728\u8f93\u5165\u89c6\u56fe\u6709\u9650\u65f6\u4f1a\u8fc7\u62df\u5408\u8bad\u7ec3\u89c6\u56fe\uff0c\u5bfc\u81f4\u6e32\u67d3\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u5229\u7528\u7acb\u4f53\u57fa\u7840\u6a21\u578b\u4f30\u8ba1\u76f8\u673a\u59ff\u6001\u548c\u91cd\u5efa\u5bc6\u96c6\u70b9\u4e91\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea62D\u5916\u89c2\u7279\u5f81\u7f16\u7801\u989c\u8272\u5c5e\u6027\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u70b9\u4ea4\u4e92\u7f51\u7edc\u589e\u5f3a\u8868\u793a\uff0c\u6700\u540e\u901a\u8fc7MLP\u89e3\u7801\u4e3a\u9ad8\u65af\u53c2\u6570\u3002", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u4e8eNeRF\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u4e0e\u6700\u5148\u8fdb\u76843DGS\u65b9\u6cd5\u7ade\u4e89\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u89c6\u56fe\u4e0b\u7684\u6e32\u67d3\u8d28\u91cf\u95ee\u9898\uff0c\u63d0\u5347\u4e863D\u9ad8\u65af\u6cfc\u6e85\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.10371", "pdf": "https://arxiv.org/pdf/2506.10371", "abs": "https://arxiv.org/abs/2506.10371", "authors": ["Laziz U. Abdullaev", "Maksim Tkachenko", "Tan M. Nguyen"], "title": "Revisiting Transformers with Insights from Image Filtering", "categories": ["cs.CV", "cs.LG"], "comment": "12 pages, 6 figures", "summary": "The self-attention mechanism, a cornerstone of Transformer-based state-of-the-art deep learning architectures, is largely heuristic-driven and fundamentally challenging to interpret. Establishing a robust theoretical foundation to explain its remarkable success and limitations has therefore become an increasingly prominent focus in recent research. Some notable directions have explored understanding self-attention through the lens of image denoising and nonparametric regression. While promising, existing frameworks still lack a deeper mechanistic interpretation of various architectural components that enhance self-attention, both in its original formulation and subsequent variants. In this work, we aim to advance this understanding by developing a unifying image processing framework, capable of explaining not only the self-attention computation itself but also the role of components such as positional encoding and residual connections, including numerous later variants. We also pinpoint potential distinctions between the two concepts building upon our framework, and make effort to close this gap. We introduce two independent architectural modifications within transformers. While our primary objective is interpretability, we empirically observe that image processing-inspired modifications can also lead to notably improved accuracy and robustness against data contamination and adversaries across language and vision tasks as well as better long sequence understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u5904\u7406\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u91ca\u81ea\u6ce8\u610f\u529b\u673a\u5236\u53ca\u5176\u7ec4\u4ef6\uff08\u5982\u4f4d\u7f6e\u7f16\u7801\u548c\u6b8b\u5dee\u8fde\u63a5\uff09\u7684\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u6539\u8fdb\u67b6\u6784\u3002", "motivation": "\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u6210\u529f\u7f3a\u4e4f\u7406\u8bba\u89e3\u91ca\uff0c\u73b0\u6709\u6846\u67b6\u672a\u80fd\u6df1\u5165\u7406\u89e3\u5176\u7ec4\u4ef6\u7684\u4f5c\u7528\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u56fe\u50cf\u5904\u7406\u6846\u67b6\uff0c\u89e3\u91ca\u81ea\u6ce8\u610f\u529b\u53ca\u5176\u7ec4\u4ef6\uff0c\u5e76\u5f15\u5165\u4e24\u79cd\u6539\u8fdb\u7684Transformer\u67b6\u6784\u3002", "result": "\u6539\u8fdb\u540e\u7684\u67b6\u6784\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u8fd8\u5728\u8bed\u8a00\u548c\u89c6\u89c9\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u957f\u5e8f\u5217\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u56fe\u50cf\u5904\u7406\u6846\u67b6\u4e3a\u81ea\u6ce8\u610f\u529b\u673a\u5236\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u67b6\u6784\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.10391", "pdf": "https://arxiv.org/pdf/2506.10391", "abs": "https://arxiv.org/abs/2506.10391", "authors": ["Yuanyi Song", "Pumeng Lyu", "Ben Fei", "Fenghua Ling", "Wanli Ouyang", "Lei Bai"], "title": "ReconMOST: Multi-Layer Sea Temperature Reconstruction with Observations-Guided Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Accurate reconstruction of ocean is essential for reflecting global climate dynamics and supporting marine meteorological research. Conventional methods face challenges due to sparse data, algorithmic complexity, and high computational costs, while increasing usage of machine learning (ML) method remains limited to reconstruction problems at the sea surface and local regions, struggling with issues like cloud occlusion. To address these limitations, this paper proposes ReconMOST, a data-driven guided diffusion model framework for multi-layer sea temperature reconstruction. Specifically, we first pre-train an unconditional diffusion model using a large collection of historical numerical simulation data, enabling the model to attain physically consistent distribution patterns of ocean temperature fields. During the generation phase, sparse yet high-accuracy in-situ observational data are utilized as guidance points for the reverse diffusion process, generating accurate reconstruction results. Importantly, in regions lacking direct observational data, the physically consistent spatial distribution patterns learned during pre-training enable implicitly guided and physically plausible reconstructions. Our method extends ML-based SST reconstruction to a global, multi-layer setting, handling over 92.5% missing data while maintaining reconstruction accuracy, spatial resolution, and superior generalization capability. We pre-train our model on CMIP6 numerical simulation data and conduct guided reconstruction experiments on CMIP6 and EN4 analysis data. The results of mean squared error (MSE) values achieve 0.049 on guidance, 0.680 on reconstruction, and 0.633 on total, respectively, demonstrating the effectiveness and robustness of the proposed framework. Our source code is available at https://github.com/norsheep/ReconMOST.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faReconMOST\u6846\u67b6\uff0c\u5229\u7528\u6570\u636e\u9a71\u52a8\u7684\u6269\u6563\u6a21\u578b\u8fdb\u884c\u591a\u5c42\u6d77\u6c34\u6e29\u5ea6\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u6570\u636e\u7a00\u758f\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u95ee\u9898\uff0c\u5e76\u6269\u5c55\u4e86\u673a\u5668\u5b66\u4e60\u5728\u6d77\u6d0b\u6e29\u5ea6\u91cd\u5efa\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u51c6\u786e\u91cd\u5efa\u6d77\u6d0b\u6e29\u5ea6\u5bf9\u5168\u7403\u6c14\u5019\u52a8\u6001\u548c\u6d77\u6d0b\u6c14\u8c61\u7814\u7a76\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u53d7\u9650\u4e8e\u6570\u636e\u7a00\u758f\u548c\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4e5f\u4ec5\u9002\u7528\u4e8e\u8868\u5c42\u6216\u5c40\u90e8\u533a\u57df\u3002", "method": "\u901a\u8fc7\u9884\u8bad\u7ec3\u65e0\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5b66\u4e60\u5386\u53f2\u6570\u503c\u6a21\u62df\u6570\u636e\uff0c\u751f\u6210\u9636\u6bb5\u5229\u7528\u7a00\u758f\u4f46\u9ad8\u7cbe\u5ea6\u7684\u73b0\u573a\u89c2\u6d4b\u6570\u636e\u4f5c\u4e3a\u53cd\u5411\u6269\u6563\u8fc7\u7a0b\u7684\u5f15\u5bfc\u70b9\uff0c\u5b9e\u73b0\u591a\u5c42\u6e29\u5ea6\u91cd\u5efa\u3002", "result": "\u5728CMIP6\u548cEN4\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cMSE\u503c\u5206\u522b\u4e3a0.049\uff08\u5f15\u5bfc\uff09\u30010.680\uff08\u91cd\u5efa\uff09\u548c0.633\uff08\u603b\u4f53\uff09\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "ReconMOST\u6846\u67b6\u6210\u529f\u6269\u5c55\u4e86\u673a\u5668\u5b66\u4e60\u5728\u6d77\u6d0b\u6e29\u5ea6\u91cd\u5efa\u4e2d\u7684\u5e94\u7528\uff0c\u80fd\u591f\u5904\u740692.5%\u7684\u7f3a\u5931\u6570\u636e\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u548c\u7a7a\u95f4\u5206\u8fa8\u7387\u3002"}}
{"id": "2506.10395", "pdf": "https://arxiv.org/pdf/2506.10395", "abs": "https://arxiv.org/abs/2506.10395", "authors": ["Zhiyang Xu", "Jiuhai Chen", "Zhaojiang Lin", "Xichen Pan", "Lifu Huang", "Tianyi Zhou", "Madian Khabsa", "Qifan Wang", "Di Jin", "Michihiro Yasunaga", "Lili Yu", "Xi Victoria Lin", "Shaoliang Nie"], "title": "Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Unified image understanding and generation model", "summary": "Recent advances in large language models (LLMs) have enabled multimodal foundation models to tackle both image understanding and generation within a unified framework. Despite these gains, unified models often underperform compared to specialized models in either task. A key challenge in developing unified models lies in the inherent differences between the visual features needed for image understanding versus generation, as well as the distinct training processes required for each modality. In this work, we introduce Pisces, an auto-regressive multimodal foundation model that addresses this challenge through a novel decoupled visual encoding architecture and tailored training techniques optimized for multimodal generation. Combined with meticulous data curation, pretraining, and finetuning, Pisces achieves competitive performance in both image understanding and image generation. We evaluate Pisces on over 20 public benchmarks for image understanding, where it demonstrates strong performance across a wide range of tasks. Additionally, on GenEval, a widely adopted benchmark for image generation, Pisces exhibits robust generative capabilities. Our extensive analysis reveals the synergistic relationship between image understanding and generation, and the benefits of using separate visual encoders, advancing the field of unified multimodal models.", "AI": {"tldr": "Pisces\u662f\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u89e3\u8026\u89c6\u89c9\u7f16\u7801\u67b6\u6784\u548c\u5b9a\u5236\u8bad\u7ec3\u6280\u672f\uff0c\u5728\u56fe\u50cf\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u56fe\u50cf\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5176\u6027\u80fd\u4ecd\u4e0d\u53ca\u4e13\u7528\u6a21\u578b\u3002Pisces\u65e8\u5728\u89e3\u51b3\u89c6\u89c9\u7279\u5f81\u548c\u8bad\u7ec3\u8fc7\u7a0b\u5dee\u5f02\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u89e3\u8026\u89c6\u89c9\u7f16\u7801\u67b6\u6784\u548c\u5b9a\u5236\u8bad\u7ec3\u6280\u672f\uff0c\u7ed3\u5408\u7cbe\u7ec6\u6570\u636e\u6574\u7406\u3001\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u3002", "result": "\u572820\u591a\u4e2a\u56fe\u50cf\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u548cGenEval\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Pisces\u5c55\u793a\u4e86\u56fe\u50cf\u7406\u89e3\u4e0e\u751f\u6210\u7684\u534f\u540c\u5173\u7cfb\uff0c\u5e76\u8bc1\u660e\u89e3\u8026\u89c6\u89c9\u7f16\u7801\u5668\u7684\u4f18\u52bf\uff0c\u63a8\u52a8\u4e86\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.10568", "pdf": "https://arxiv.org/pdf/2506.10568", "abs": "https://arxiv.org/abs/2506.10568", "authors": ["Lizhen Wang", "Zhurong Xia", "Tianshu Hu", "Pengrui Wang", "Pengfei Wang", "Zerong Zheng", "Ming Zhou"], "title": "DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In e-commerce and digital marketing, generating high-fidelity human-product demonstration videos is important for effective product presentation. However, most existing frameworks either fail to preserve the identities of both humans and products or lack an understanding of human-product spatial relationships, leading to unrealistic representations and unnatural interactions. To address these challenges, we propose a Diffusion Transformer (DiT)-based framework. Our method simultaneously preserves human identities and product-specific details, such as logos and textures, by injecting paired human-product reference information and utilizing an additional masked cross-attention mechanism. We employ a 3D body mesh template and product bounding boxes to provide precise motion guidance, enabling intuitive alignment of hand gestures with product placements. Additionally, structured text encoding is used to incorporate category-level semantics, enhancing 3D consistency during small rotational changes across frames. Trained on a hybrid dataset with extensive data augmentation strategies, our approach outperforms state-of-the-art techniques in maintaining the identity integrity of both humans and products and generating realistic demonstration motions. Project page: https://submit2025-dream.github.io/DreamActor-H1/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4eba-\u4ea7\u54c1\u6f14\u793a\u89c6\u9891\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u6301\u8eab\u4efd\u548c\u7a7a\u95f4\u5173\u7cfb\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u4fdd\u6301\u4eba\u548c\u4ea7\u54c1\u7684\u8eab\u4efd\u7279\u5f81\u6216\u7406\u89e3\u5176\u7a7a\u95f4\u5173\u7cfb\uff0c\u5bfc\u81f4\u4e0d\u771f\u5b9e\u7684\u6f14\u793a\u6548\u679c\u3002", "method": "\u91c7\u7528DiT\u6846\u67b6\uff0c\u6ce8\u5165\u914d\u5bf9\u7684\u4eba-\u4ea7\u54c1\u53c2\u8003\u4fe1\u606f\uff0c\u5e76\u4f7f\u7528\u63a9\u7801\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff1b\u5229\u75283D\u8eab\u4f53\u7f51\u683c\u6a21\u677f\u548c\u4ea7\u54c1\u8fb9\u754c\u6846\u63d0\u4f9b\u7cbe\u786e\u8fd0\u52a8\u6307\u5bfc\uff1b\u7ed3\u5408\u7ed3\u6784\u5316\u6587\u672c\u7f16\u7801\u589e\u5f3a3D\u4e00\u81f4\u6027\u3002", "result": "\u5728\u6df7\u5408\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u540e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u8eab\u4efd\u5b8c\u6574\u6027\u548c\u751f\u6210\u771f\u5b9e\u8fd0\u52a8\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4eba-\u4ea7\u54c1\u6f14\u793a\u89c6\u9891\u751f\u6210\u4e2d\u7684\u8eab\u4efd\u548c\u7a7a\u95f4\u5173\u7cfb\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.10575", "pdf": "https://arxiv.org/pdf/2506.10575", "abs": "https://arxiv.org/abs/2506.10575", "authors": ["Chun-Mei Feng", "Kai Yu", "Xinxing Xu", "Salman Khan", "Rick Siow Mong Goh", "Wangmeng Zuo", "Yong Liu"], "title": "Text to Image for Multi-Label Image Recognition with Joint Prompt-Adapter Learning", "categories": ["cs.CV"], "comment": null, "summary": "Benefited from image-text contrastive learning, pre-trained vision-language models, e.g., CLIP, allow to direct leverage texts as images (TaI) for parameter-efficient fine-tuning (PEFT). While CLIP is capable of making image features to be similar to the corresponding text features, the modality gap remains a nontrivial issue and limits image recognition performance of TaI. Using multi-label image recognition (MLR) as an example, we present a novel method, called T2I-PAL to tackle the modality gap issue when using only text captions for PEFT. The core design of T2I-PAL is to leverage pre-trained text-to-image generation models to generate photo-realistic and diverse images from text captions, thereby reducing the modality gap. To further enhance MLR, T2I-PAL incorporates a class-wise heatmap and learnable prototypes. This aggregates local similarities, making the representation of local visual features more robust and informative for multi-label recognition. For better PEFT, we further combine both prompt tuning and adapter learning to enhance classification performance. T2I-PAL offers significant advantages: it eliminates the need for fully semantically annotated training images, thereby reducing the manual annotation workload, and it preserves the intrinsic mode of the CLIP model, allowing for seamless integration with any existing CLIP framework. Extensive experiments on multiple benchmarks, including MS-COCO, VOC2007, and NUS-WIDE, show that our T2I-PAL can boost recognition performance by 3.47% in average above the top-ranked state-of-the-art methods.", "AI": {"tldr": "T2I-PAL\u5229\u7528\u6587\u672c\u751f\u6210\u56fe\u50cf\u51cf\u5c11\u6a21\u6001\u5dee\u5f02\uff0c\u7ed3\u5408\u70ed\u56fe\u548c\u539f\u578b\u5b66\u4e60\u63d0\u5347\u591a\u6807\u7b7e\u8bc6\u522b\u6027\u80fd\uff0c\u65e0\u9700\u5168\u6807\u6ce8\u56fe\u50cf\uff0c\u5b9e\u9a8c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3CLIP\u6a21\u578b\u4e2d\u56fe\u50cf\u4e0e\u6587\u672c\u6a21\u6001\u5dee\u5f02\u95ee\u9898\uff0c\u63d0\u5347\u4ec5\u7528\u6587\u672c\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65f6\u7684\u56fe\u50cf\u8bc6\u522b\u6027\u80fd\u3002", "method": "\u5229\u7528\u6587\u672c\u751f\u6210\u56fe\u50cf\u6a21\u578b\u751f\u6210\u591a\u6837\u56fe\u50cf\uff0c\u7ed3\u5408\u7c7b\u70ed\u56fe\u548c\u53ef\u5b66\u4e60\u539f\u578b\u589e\u5f3a\u5c40\u90e8\u7279\u5f81\u8868\u793a\uff0c\u5e76\u878d\u5408\u63d0\u793a\u8c03\u4f18\u548c\u9002\u914d\u5668\u5b66\u4e60\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u6027\u80fd\u63d0\u53473.47%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "T2I-PAL\u6709\u6548\u51cf\u5c11\u6a21\u6001\u5dee\u5f02\uff0c\u63d0\u5347\u6027\u80fd\u4e14\u65e0\u9700\u5168\u6807\u6ce8\u56fe\u50cf\uff0c\u517c\u5bb9\u73b0\u6709CLIP\u6846\u67b6\u3002"}}
{"id": "2506.10605", "pdf": "https://arxiv.org/pdf/2506.10605", "abs": "https://arxiv.org/abs/2506.10605", "authors": ["Eshan Ramesh", "Nishio Takayuki"], "title": "High-resolution efficient image generation from WiFi CSI using a pretrained latent diffusion model", "categories": ["cs.CV"], "comment": "6 pages, 4 figures", "summary": "We present LatentCSI, a novel method for generating images of the physical environment from WiFi CSI measurements that leverages a pretrained latent diffusion model (LDM). Unlike prior approaches that rely on complex and computationally intensive techniques such as GANs, our method employs a lightweight neural network to map CSI amplitudes directly into the latent space of an LDM. We then apply the LDM's denoising diffusion model to the latent representation with text-based guidance before decoding using the LDM's pretrained decoder to obtain a high-resolution image. This design bypasses the challenges of pixel-space image generation and avoids the explicit image encoding stage typically required in conventional image-to-image pipelines, enabling efficient and high-quality image synthesis. We validate our approach on two datasets: a wide-band CSI dataset we collected with off-the-shelf WiFi devices and cameras; and a subset of the publicly available MM-Fi dataset. The results demonstrate that LatentCSI outperforms baselines of comparable complexity trained directly on ground-truth images in both computational efficiency and perceptual quality, while additionally providing practical advantages through its unique capacity for text-guided controllability.", "AI": {"tldr": "LatentCSI\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\u4eceWiFi CSI\u6d4b\u91cf\u751f\u6210\u7269\u7406\u73af\u5883\u56fe\u50cf\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u5c06CSI\u5e45\u5ea6\u76f4\u63a5\u6620\u5c04\u5230LDM\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u5b9e\u73b0\u9ad8\u6548\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u5408\u6210\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u4e14\u8ba1\u7b97\u5bc6\u96c6\u7684\u6280\u672f\uff08\u5982GANs\uff09\uff0c\u800cLatentCSI\u65e8\u5728\u901a\u8fc7\u7b80\u5316\u6d41\u7a0b\u548c\u5229\u7528LDM\u7684\u4f18\u52bf\uff0c\u63d0\u9ad8\u56fe\u50cf\u751f\u6210\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u5c06CSI\u5e45\u5ea6\u6620\u5c04\u5230LDM\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u7ed3\u5408\u6587\u672c\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\u8fdb\u884c\u53bb\u566a\uff0c\u6700\u540e\u901a\u8fc7LDM\u7684\u89e3\u7801\u5668\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u81ea\u6536\u96c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cLatentCSI\u5728\u8ba1\u7b97\u6548\u7387\u548c\u611f\u77e5\u8d28\u91cf\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5177\u5907\u6587\u672c\u5f15\u5bfc\u7684\u7075\u6d3b\u6027\u3002", "conclusion": "LatentCSI\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u6620\u5c04\u548cLDM\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u751f\u6210\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u6587\u672c\u5f15\u5bfc\u7684\u5b9e\u7528\u4f18\u52bf\u3002"}}
{"id": "2506.10612", "pdf": "https://arxiv.org/pdf/2506.10612", "abs": "https://arxiv.org/abs/2506.10612", "authors": ["Suin Lee", "Dae-Shik Kim"], "title": "TexTailor: Customized Text-aligned Texturing via Effective Resampling", "categories": ["cs.CV", "cs.AI", "I.2.10"], "comment": "Submitted to ICLR 2025", "summary": "We present TexTailor, a novel method for generating consistent object textures from textual descriptions. Existing text-to-texture synthesis approaches utilize depth-aware diffusion models to progressively generate images and synthesize textures across predefined multiple viewpoints. However, these approaches lead to a gradual shift in texture properties across viewpoints due to (1) insufficient integration of previously synthesized textures at each viewpoint during the diffusion process and (2) the autoregressive nature of the texture synthesis process. Moreover, the predefined selection of camera positions, which does not account for the object's geometry, limits the effective use of texture information synthesized from different viewpoints, ultimately degrading overall texture consistency. In TexTailor, we address these issues by (1) applying a resampling scheme that repeatedly integrates information from previously synthesized textures within the diffusion process, and (2) fine-tuning a depth-aware diffusion model on these resampled textures. During this process, we observed that using only a few training images restricts the model's original ability to generate high-fidelity images aligned with the conditioning, and therefore propose an performance preservation loss to mitigate this issue. Additionally, we improve the synthesis of view-consistent textures by adaptively adjusting camera positions based on the object's geometry. Experiments on a subset of the Objaverse dataset and the ShapeNet car dataset demonstrate that TexTailor outperforms state-of-the-art methods in synthesizing view-consistent textures. The source code for TexTailor is available at https://github.com/Adios42/Textailor", "AI": {"tldr": "TexTailor\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6587\u672c\u63cf\u8ff0\u751f\u6210\u4e00\u81f4\u7684\u5bf9\u8c61\u7eb9\u7406\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u89c6\u89d2\u4e0b\u7eb9\u7406\u5c5e\u6027\u9010\u6e10\u504f\u79fb\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u7eb9\u7406\u5408\u6210\u65b9\u6cd5\u5728\u591a\u89c6\u89d2\u4e0b\u7eb9\u7406\u4e00\u81f4\u6027\u5dee\uff0c\u4e14\u76f8\u673a\u4f4d\u7f6e\u9009\u62e9\u56fa\u5b9a\uff0c\u65e0\u6cd5\u9002\u5e94\u5bf9\u8c61\u51e0\u4f55\u5f62\u72b6\u3002", "method": "\u91c7\u7528\u91cd\u91c7\u6837\u65b9\u6848\u6574\u5408\u5148\u524d\u5408\u6210\u7eb9\u7406\u4fe1\u606f\uff0c\u5fae\u8c03\u6df1\u5ea6\u611f\u77e5\u6269\u6563\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u6027\u80fd\u4fdd\u7559\u635f\u5931\u548c\u81ea\u9002\u5e94\u76f8\u673a\u4f4d\u7f6e\u8c03\u6574\u3002", "result": "\u5728Objaverse\u548cShapeNet\u6570\u636e\u96c6\u4e0a\uff0cTexTailor\u5728\u5408\u6210\u89c6\u89d2\u4e00\u81f4\u7eb9\u7406\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TexTailor\u901a\u8fc7\u6539\u8fdb\u7eb9\u7406\u6574\u5408\u548c\u76f8\u673a\u4f4d\u7f6e\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7eb9\u7406\u4e00\u81f4\u6027\u3002"}}
{"id": "2506.10633", "pdf": "https://arxiv.org/pdf/2506.10633", "abs": "https://arxiv.org/abs/2506.10633", "authors": ["Konstantinos Vilouras", "Ilias Stogiannidis", "Junyu Yan", "Alison Q. O'Neil", "Sotirios A. Tsaftaris"], "title": "Anatomy-Grounded Weakly Supervised Prompt Tuning for Chest X-ray Latent Diffusion Models", "categories": ["cs.CV"], "comment": "14 pages, 6 figures", "summary": "Latent Diffusion Models have shown remarkable results in text-guided image synthesis in recent years. In the domain of natural (RGB) images, recent works have shown that such models can be adapted to various vision-language downstream tasks with little to no supervision involved. On the contrary, text-to-image Latent Diffusion Models remain relatively underexplored in the field of medical imaging, primarily due to limited data availability (e.g., due to privacy concerns). In this work, focusing on the chest X-ray modality, we first demonstrate that a standard text-conditioned Latent Diffusion Model has not learned to align clinically relevant information in free-text radiology reports with the corresponding areas of the given scan. Then, to alleviate this issue, we propose a fine-tuning framework to improve multi-modal alignment in a pre-trained model such that it can be efficiently repurposed for downstream tasks such as phrase grounding. Our method sets a new state-of-the-art on a standard benchmark dataset (MS-CXR), while also exhibiting robust performance on out-of-distribution data (VinDr-CXR). Our code will be made publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u533b\u5b66\u5f71\u50cf\uff08\u80f8\u90e8X\u5149\uff09\u7684\u6587\u672c\u5230\u56fe\u50cf\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5fae\u8c03\u6846\u67b6\uff0c\u4ee5\u6539\u5584\u591a\u6a21\u6001\u5bf9\u9f50\uff0c\u5e76\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u9886\u57df\u7f3a\u4e4f\u6587\u672c\u5230\u56fe\u50cf\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u7814\u7a76\uff0c\u4e14\u73b0\u6709\u6a21\u578b\u672a\u80fd\u6709\u6548\u5bf9\u9f50\u4e34\u5e8a\u76f8\u5173\u6587\u672c\u4e0e\u5f71\u50cf\u533a\u57df\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5fae\u8c03\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u591a\u6a21\u6001\u5bf9\u9f50\u80fd\u529b\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u77ed\u8bed\u5b9a\u4f4d\uff09\u3002", "result": "\u5728\u6807\u51c6\u6570\u636e\u96c6\uff08MS-CXR\uff09\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u4f73\u6027\u80fd\uff0c\u5e76\u5728\u5916\u90e8\u6570\u636e\uff08VinDr-CXR\uff09\u4e0a\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u533b\u5b66\u5f71\u50cf\u9886\u57df\u7684\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.10634", "pdf": "https://arxiv.org/pdf/2506.10634", "abs": "https://arxiv.org/abs/2506.10634", "authors": ["Francisco Caetano", "Christiaan Viviers", "Peter H. N. De With", "Fons van der Sommen"], "title": "Symmetrical Flow Matching: Unified Image Generation, Segmentation, and Classification with Score-Based Generative Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Flow Matching has emerged as a powerful framework for learning continuous transformations between distributions, enabling high-fidelity generative modeling. This work introduces Symmetrical Flow Matching (SymmFlow), a new formulation that unifies semantic segmentation, classification, and image generation within a single model. Using a symmetric learning objective, SymmFlow models forward and reverse transformations jointly, ensuring bi-directional consistency, while preserving sufficient entropy for generative diversity. A new training objective is introduced to explicitly retain semantic information across flows, featuring efficient sampling while preserving semantic structure, allowing for one-step segmentation and classification without iterative refinement. Unlike previous approaches that impose strict one-to-one mapping between masks and images, SymmFlow generalizes to flexible conditioning, supporting both pixel-level and image-level class labels. Experimental results on various benchmarks demonstrate that SymmFlow achieves state-of-the-art performance on semantic image synthesis, obtaining FID scores of 11.9 on CelebAMask-HQ and 7.0 on COCO-Stuff with only 25 inference steps. Additionally, it delivers competitive results on semantic segmentation and shows promising capabilities in classification tasks. The code will be publicly available.", "AI": {"tldr": "SymmFlow\u662f\u4e00\u79cd\u5bf9\u79f0\u6d41\u5339\u914d\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u8bed\u4e49\u5206\u5272\u3001\u5206\u7c7b\u548c\u56fe\u50cf\u751f\u6210\uff0c\u901a\u8fc7\u53cc\u5411\u4e00\u81f4\u6027\u5b66\u4e60\u548c\u4fdd\u7559\u8bed\u4e49\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u91c7\u6837\u548c\u591a\u6837\u5316\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u56fe\u50cf\u751f\u6210\u548c\u8bed\u4e49\u4efb\u52a1\u4e4b\u95f4\u5b58\u5728\u5206\u79bb\uff0c\u4e14\u901a\u5e38\u9700\u8981\u4e25\u683c\u7684\u4e00\u5bf9\u4e00\u6620\u5c04\u3002SymmFlow\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u652f\u6301\u7075\u6d3b\u7684\u50cf\u7d20\u7ea7\u548c\u56fe\u50cf\u7ea7\u6761\u4ef6\u3002", "method": "\u63d0\u51fa\u5bf9\u79f0\u5b66\u4e60\u76ee\u6807\uff0c\u8054\u5408\u5efa\u6a21\u6b63\u5411\u548c\u53cd\u5411\u53d8\u6362\uff0c\u786e\u4fdd\u53cc\u5411\u4e00\u81f4\u6027\uff0c\u5e76\u5f15\u5165\u65b0\u8bad\u7ec3\u76ee\u6807\u4ee5\u4fdd\u7559\u8bed\u4e49\u4fe1\u606f\uff0c\u652f\u6301\u4e00\u6b65\u5206\u5272\u548c\u5206\u7c7b\u3002", "result": "\u5728CelebAMask-HQ\u548cCOCO-Stuff\u4e0a\u5206\u522b\u53d6\u5f97FID 11.9\u548c7.0\u7684SOTA\u6027\u80fd\uff0c\u540c\u65f6\u5728\u8bed\u4e49\u5206\u5272\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "SymmFlow\u5c55\u793a\u4e86\u7edf\u4e00\u751f\u6210\u548c\u8bed\u4e49\u4efb\u52a1\u7684\u6f5c\u529b\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2506.10639", "pdf": "https://arxiv.org/pdf/2506.10639", "abs": "https://arxiv.org/abs/2506.10639", "authors": ["Xiaoyi Bao", "Jindi Lv", "Xiaofeng Wang", "Zheng Zhu", "Xinze Chen", "YuKun Zhou", "Jiancheng Lv", "Xingang Wang", "Guan Huang"], "title": "GigaVideo-1: Advancing Video Generation via Automatic Feedback with 4 GPU-Hours Fine-Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Recent progress in diffusion models has greatly enhanced video generation quality, yet these models still require fine-tuning to improve specific dimensions like instance preservation, motion rationality, composition, and physical plausibility. Existing fine-tuning approaches often rely on human annotations and large-scale computational resources, limiting their practicality. In this work, we propose GigaVideo-1, an efficient fine-tuning framework that advances video generation without additional human supervision. Rather than injecting large volumes of high-quality data from external sources, GigaVideo-1 unlocks the latent potential of pre-trained video diffusion models through automatic feedback. Specifically, we focus on two key aspects of the fine-tuning process: data and optimization. To improve fine-tuning data, we design a prompt-driven data engine that constructs diverse, weakness-oriented training samples. On the optimization side, we introduce a reward-guided training strategy, which adaptively weights samples using feedback from pre-trained vision-language models with a realism constraint. We evaluate GigaVideo-1 on the VBench-2.0 benchmark using Wan2.1 as the baseline across 17 evaluation dimensions. Experiments show that GigaVideo-1 consistently improves performance on almost all the dimensions with an average gain of about 4% using only 4 GPU-hours. Requiring no manual annotations and minimal real data, GigaVideo-1 demonstrates both effectiveness and efficiency. Code, model, and data will be publicly available.", "AI": {"tldr": "GigaVideo-1\u662f\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u9ad8\u6548\u89c6\u9891\u751f\u6210\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u53cd\u9988\u63d0\u5347\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5fae\u8c03\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u548c\u5927\u89c4\u6a21\u8ba1\u7b97\u8d44\u6e90\uff0c\u9650\u5236\u4e86\u5b9e\u7528\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u63d0\u793a\u9a71\u52a8\u7684\u6570\u636e\u5f15\u64ce\u548c\u5956\u52b1\u5f15\u5bfc\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53cd\u9988\u4f18\u5316\u6837\u672c\u6743\u91cd\u3002", "result": "\u5728VBench-2.0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u5347\u7ea64%\uff0c\u4ec5\u97004 GPU\u5c0f\u65f6\u3002", "conclusion": "GigaVideo-1\u5728\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u548c\u5c11\u91cf\u771f\u5b9e\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u6709\u6548\u7684\u89c6\u9891\u751f\u6210\u4f18\u5316\u3002"}}
{"id": "2506.10683", "pdf": "https://arxiv.org/pdf/2506.10683", "abs": "https://arxiv.org/abs/2506.10683", "authors": ["Subhram Dasgupta", "Janelle Mason", "Xiaohong Yuan", "Olusola Odeyomi", "Kaushik Roy"], "title": "Enhancing Deepfake Detection using SE Block Attention with CNN", "categories": ["cs.CV"], "comment": null, "summary": "In the digital age, Deepfake present a formidable challenge by using advanced artificial intelligence to create highly convincing manipulated content, undermining information authenticity and security. These sophisticated fabrications surpass traditional detection methods in complexity and realism. To address this issue, we aim to harness cutting-edge deep learning methodologies to engineer an innovative deepfake detection model. However, most of the models designed for deepfake detection are large, causing heavy storage and memory consumption. In this research, we propose a lightweight convolution neural network (CNN) with squeeze and excitation block attention (SE) for Deepfake detection. The SE block module is designed to perform dynamic channel-wise feature recalibration. The SE block allows the network to emphasize informative features and suppress less useful ones, which leads to a more efficient and effective learning module. This module is integrated with a simple sequential model to perform Deepfake detection. The model is smaller in size and it achieves competing accuracy with the existing models for deepfake detection tasks. The model achieved an overall classification accuracy of 94.14% and AUC-ROC score of 0.985 on the Style GAN dataset from the Diverse Fake Face Dataset. Our proposed approach presents a promising avenue for combating the Deepfake challenge with minimal computational resources, developing efficient and scalable solutions for digital content verification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7ed3\u5408Squeeze and Excitation\uff08SE\uff09\u6ce8\u610f\u529b\u6a21\u5757\u7684Deepfake\u68c0\u6d4b\u65b9\u6cd5\uff0c\u6a21\u578b\u4f53\u79ef\u5c0f\u4f46\u6027\u80fd\u4f18\u5f02\u3002", "motivation": "Deepfake\u6280\u672f\u5bf9\u4fe1\u606f\u771f\u5b9e\u6027\u548c\u5b89\u5168\u6027\u6784\u6210\u5a01\u80c1\uff0c\u73b0\u6709\u68c0\u6d4b\u6a21\u578b\u4f53\u79ef\u5927\u3001\u8d44\u6e90\u6d88\u8017\u9ad8\uff0c\u9700\u5f00\u53d1\u9ad8\u6548\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7CNN\u7ed3\u5408SE\u6a21\u5757\uff0c\u52a8\u6001\u8c03\u6574\u901a\u9053\u7279\u5f81\u6743\u91cd\uff0c\u63d0\u5347\u6a21\u578b\u6548\u7387\u3002", "result": "\u5728Style GAN\u6570\u636e\u96c6\u4e0a\u5206\u7c7b\u51c6\u786e\u7387\u8fbe94.14%\uff0cAUC-ROC\u5f97\u5206\u4e3a0.985\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aDeepfake\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u4f4e\u8d44\u6e90\u6d88\u8017\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.10685", "pdf": "https://arxiv.org/pdf/2506.10685", "abs": "https://arxiv.org/abs/2506.10685", "authors": ["Xia Du", "Xiaoyuan Liu", "Jizhe Zhou", "Zheng Lin", "Chi-man Pun", "Zhe Chen", "Wei Ni", "Jun Luo"], "title": "Unsourced Adversarial CAPTCHA: A Bi-Phase Adversarial CAPTCHA Framework", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "With the rapid advancements in deep learning, traditional CAPTCHA schemes are increasingly vulnerable to automated attacks powered by deep neural networks (DNNs). Existing adversarial attack methods often rely on original image characteristics, resulting in distortions that hinder human interpretation and limit applicability in scenarios lacking initial input images. To address these challenges, we propose the Unsourced Adversarial CAPTCHA (UAC), a novel framework generating high-fidelity adversarial examples guided by attacker-specified text prompts. Leveraging a Large Language Model (LLM), UAC enhances CAPTCHA diversity and supports both targeted and untargeted attacks. For targeted attacks, the EDICT method optimizes dual latent variables in a diffusion model for superior image quality. In untargeted attacks, especially for black-box scenarios, we introduce bi-path unsourced adversarial CAPTCHA (BP-UAC), a two-step optimization strategy employing multimodal gradients and bi-path optimization for efficient misclassification. Experiments show BP-UAC achieves high attack success rates across diverse systems, generating natural CAPTCHAs indistinguishable to humans and DNNs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUAC\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u63d0\u793a\u751f\u6210\u9ad8\u4fdd\u771f\u5bf9\u6297\u6837\u672c\uff0c\u63d0\u5347CAPTCHA\u591a\u6837\u6027\uff0c\u652f\u6301\u5b9a\u5411\u548c\u975e\u5b9a\u5411\u653b\u51fb\u3002", "motivation": "\u4f20\u7edfCAPTCHA\u65b9\u6848\u56e0\u6df1\u5ea6\u5b66\u4e60\u7684\u5feb\u901f\u53d1\u5c55\u800c\u5bb9\u6613\u88ab\u81ea\u52a8\u5316\u653b\u51fb\u653b\u7834\uff0c\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u4f9d\u8d56\u539f\u59cb\u56fe\u50cf\u7279\u5f81\uff0c\u5bfc\u81f4\u5931\u771f\u4e14\u9002\u7528\u6027\u53d7\u9650\u3002", "method": "UAC\u6846\u67b6\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u5bf9\u6297\u6837\u672c\uff1b\u5b9a\u5411\u653b\u51fb\u91c7\u7528EDICT\u65b9\u6cd5\u4f18\u5316\u6269\u6563\u6a21\u578b\u7684\u53cc\u6f5c\u5728\u53d8\u91cf\uff1b\u975e\u5b9a\u5411\u653b\u51fb\u5f15\u5165BP-UAC\u7b56\u7565\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u68af\u5ea6\u548c\u53cc\u8def\u5f84\u4f18\u5316\u5b9e\u73b0\u9ad8\u6548\u8bef\u5206\u7c7b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eBP-UAC\u5728\u591a\u79cd\u7cfb\u7edf\u4e2d\u653b\u51fb\u6210\u529f\u7387\u9ad8\uff0c\u751f\u6210\u7684CAPTCHA\u5bf9\u4eba\u7c7b\u548cDNN\u5747\u96be\u4ee5\u533a\u5206\u3002", "conclusion": "UAC\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfCAPTCHA\u7684\u8106\u5f31\u6027\u95ee\u9898\uff0c\u4e3a\u5bf9\u6297\u653b\u51fb\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.10774", "pdf": "https://arxiv.org/pdf/2506.10774", "abs": "https://arxiv.org/abs/2506.10774", "authors": ["Wenhao Guo", "Peng Lu", "Xujun Peng", "Zhaoran Zhao", "Sheng Li"], "title": "Stroke-based Cyclic Amplifier: Image Super-Resolution at Arbitrary Ultra-Large Scales", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Prior Arbitrary-Scale Image Super-Resolution (ASISR) methods often experience a significant performance decline when the upsampling factor exceeds the range covered by the training data, introducing substantial blurring. To address this issue, we propose a unified model, Stroke-based Cyclic Amplifier (SbCA), for ultra-large upsampling tasks. The key of SbCA is the stroke vector amplifier, which decomposes the image into a series of strokes represented as vector graphics for magnification. Then, the detail completion module also restores missing details, ensuring high-fidelity image reconstruction. Our cyclic strategy achieves ultra-large upsampling by iteratively refining details with this unified SbCA model, trained only once for all, while keeping sub-scales within the training range. Our approach effectively addresses the distribution drift issue and eliminates artifacts, noise and blurring, producing high-quality, high-resolution super-resolved images. Experimental validations on both synthetic and real-world datasets demonstrate that our approach significantly outperforms existing methods in ultra-large upsampling tasks (e.g. $\\times100$), delivering visual quality far superior to state-of-the-art techniques.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b14\u753b\u7684\u5faa\u73af\u653e\u5927\u5668\uff08SbCA\uff09\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u8d85\u5927\u89c4\u6a21\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4efb\u610f\u5c3a\u5ea6\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5728\u8d85\u51fa\u8bad\u7ec3\u6570\u636e\u8303\u56f4\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u5bfc\u81f4\u6a21\u7cca\u95ee\u9898\u3002", "method": "SbCA\u901a\u8fc7\u7b14\u753b\u5411\u91cf\u653e\u5927\u5668\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u77e2\u91cf\u56fe\u5f62\u8fdb\u884c\u653e\u5927\uff0c\u5e76\u901a\u8fc7\u7ec6\u8282\u8865\u5168\u6a21\u5757\u6062\u590d\u7f3a\u5931\u7ec6\u8282\uff0c\u91c7\u7528\u5faa\u73af\u7b56\u7565\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSbCA\u5728\u8d85\u5927\u89c4\u6a21\u4e0a\u91c7\u6837\u4efb\u52a1\uff08\u5982\u00d7100\uff09\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002", "conclusion": "SbCA\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5e03\u6f02\u79fb\u95ee\u9898\uff0c\u6d88\u9664\u4e86\u4f2a\u5f71\u548c\u6a21\u7cca\uff0c\u4e3a\u8d85\u5927\u89c4\u6a21\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.10840", "pdf": "https://arxiv.org/pdf/2506.10840", "abs": "https://arxiv.org/abs/2506.10840", "authors": ["Tianrui Zhu", "Houyuan Chen", "Ruihao Gong", "Michele Magno", "Haotong Qin", "Kai Zhang"], "title": "Post-Training Quantization for Video Matting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video matting is crucial for applications such as film production and virtual reality, yet deploying its computationally intensive models on resource-constrained devices presents challenges. Quantization is a key technique for model compression and acceleration. As an efficient approach, Post-Training Quantization (PTQ) is still in its nascent stages for video matting, facing significant hurdles in maintaining accuracy and temporal coherence. To address these challenges, this paper proposes a novel and general PTQ framework specifically designed for video matting models, marking, to the best of our knowledge, the first systematic attempt in this domain. Our contributions include: (1) A two-stage PTQ strategy that combines block-reconstruction-based optimization for fast, stable initial quantization and local dependency capture, followed by a global calibration of quantization parameters to minimize accuracy loss. (2) A Statistically-Driven Global Affine Calibration (GAC) method that enables the network to compensate for cumulative statistical distortions arising from factors such as neglected BN layer effects, even reducing the error of existing PTQ methods on video matting tasks up to 20%. (3) An Optical Flow Assistance (OFA) component that leverages temporal and semantic priors from frames to guide the PTQ process, enhancing the model's ability to distinguish moving foregrounds in complex scenes and ultimately achieving near full-precision performance even under ultra-low-bit quantization. Comprehensive quantitative and visual results show that our PTQ4VM achieves the state-of-the-art accuracy performance across different bit-widths compared to the existing quantization methods. We highlight that the 4-bit PTQ4VM even achieves performance close to the full-precision counterpart while enjoying 8x FLOP savings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u4e3a\u89c6\u9891\u62a0\u56fe\u6a21\u578b\u8bbe\u8ba1\u7684\u540e\u8bad\u7ec3\u91cf\u5316\uff08PTQ\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u7b56\u7565\u3001\u5168\u5c40\u4eff\u5c04\u6821\u51c6\u548c\u5149\u6d41\u8f85\u52a9\u7ec4\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cf\u5316\u6a21\u578b\u7684\u7cbe\u5ea6\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\u3002", "motivation": "\u89c6\u9891\u62a0\u56fe\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u65f6\u9762\u4e34\u8ba1\u7b97\u5bc6\u96c6\u578b\u6a21\u578b\u7684\u6311\u6218\uff0c\u73b0\u6709PTQ\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5PTQ\u7b56\u7565\uff08\u5757\u91cd\u5efa\u4f18\u5316\u548c\u5168\u5c40\u53c2\u6570\u6821\u51c6\uff09\u3001\u7edf\u8ba1\u9a71\u52a8\u7684\u5168\u5c40\u4eff\u5c04\u6821\u51c6\uff08GAC\uff09\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5149\u6d41\u8f85\u52a9\uff08OFA\uff09\u7ec4\u4ef6\u3002", "result": "PTQ4VM\u5728\u4e0d\u540c\u6bd4\u7279\u5bbd\u5ea6\u4e0b\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u7cbe\u5ea6\uff0c4\u4f4d\u91cf\u5316\u6a21\u578b\u6027\u80fd\u63a5\u8fd1\u5168\u7cbe\u5ea6\u6a21\u578b\uff0c\u8ba1\u7b97\u91cf\u51cf\u5c118\u500d\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u89c6\u9891\u62a0\u56fe\u91cf\u5316\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u6bd4\u7279\u91cf\u5316\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2506.10915", "pdf": "https://arxiv.org/pdf/2506.10915", "abs": "https://arxiv.org/abs/2506.10915", "authors": ["Jiancheng Huang", "Gengwei Zhang", "Zequn Jie", "Siyu Jiao", "Yinlong Qian", "Ling Chen", "Yunchao Wei", "Lin Ma"], "title": "M4V: Multi-Modal Mamba for Text-to-Video Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Text-to-video generation has significantly enriched content creation and holds the potential to evolve into powerful world simulators. However, modeling the vast spatiotemporal space remains computationally demanding, particularly when employing Transformers, which incur quadratic complexity in sequence processing and thus limit practical applications. Recent advancements in linear-time sequence modeling, particularly the Mamba architecture, offer a more efficient alternative. Nevertheless, its plain design limits its direct applicability to multi-modal and spatiotemporal video generation tasks. To address these challenges, we introduce M4V, a Multi-Modal Mamba framework for text-to-video generation. Specifically, we propose a multi-modal diffusion Mamba (MM-DiM) block that enables seamless integration of multi-modal information and spatiotemporal modeling through a multi-modal token re-composition design. As a result, the Mamba blocks in M4V reduce FLOPs by 45% compared to the attention-based alternative when generating videos at 768$\\times$1280 resolution. Additionally, to mitigate the visual quality degradation in long-context autoregressive generation processes, we introduce a reward learning strategy that further enhances per-frame visual realism. Extensive experiments on text-to-video benchmarks demonstrate M4V's ability to produce high-quality videos while significantly lowering computational costs. Code and models will be publicly available at https://huangjch526.github.io/M4V_project.", "AI": {"tldr": "M4V\u662f\u4e00\u4e2a\u57fa\u4e8eMamba\u67b6\u6784\u7684\u591a\u6a21\u6001\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6269\u6563Mamba\u5757\uff08MM-DiM\uff09\u548c\u5956\u52b1\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u5347\u4e86\u89c6\u9891\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfTransformer\u5728\u89c6\u9891\u751f\u6210\u4e2d\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u591a\u6a21\u6001\u548c\u65f6\u7a7a\u5efa\u6a21\u7684\u6548\u7387\u3002", "method": "\u63d0\u51faMM-DiM\u5757\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u4fe1\u606f\u91cd\u7ec4\u548c\u65f6\u7a7a\u5efa\u6a21\uff1b\u5f15\u5165\u5956\u52b1\u5b66\u4e60\u7b56\u7565\u4f18\u5316\u957f\u4e0a\u4e0b\u6587\u751f\u6210\u8d28\u91cf\u3002", "result": "\u5728768\u00d71280\u5206\u8fa8\u7387\u4e0b\uff0cM4V\u6bd4\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u65b9\u6cd5\u51cf\u5c1145%\u7684FLOPs\uff0c\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002", "conclusion": "M4V\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2506.10941", "pdf": "https://arxiv.org/pdf/2506.10941", "abs": "https://arxiv.org/abs/2506.10941", "authors": ["Leigang Qu", "Feng Cheng", "Ziyan Yang", "Qi Zhao", "Shanchuan Lin", "Yichun Shi", "Yicong Li", "Wenjie Wang", "Tat-Seng Chua", "Lu Jiang"], "title": "VINCIE: Unlocking In-context Image Editing from Video", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Project page: https://vincie2025.github.io/", "summary": "In-context image editing aims to modify images based on a contextual sequence comprising text and previously generated images. Existing methods typically depend on task-specific pipelines and expert models (e.g., segmentation and inpainting) to curate training data. In this work, we explore whether an in-context image editing model can be learned directly from videos. We introduce a scalable approach to annotate videos as interleaved multimodal sequences. To effectively learn from this data, we design a block-causal diffusion transformer trained on three proxy tasks: next-image prediction, current segmentation prediction, and next-segmentation prediction. Additionally, we propose a novel multi-turn image editing benchmark to advance research in this area. Extensive experiments demonstrate that our model exhibits strong in-context image editing capabilities and achieves state-of-the-art results on two multi-turn image editing benchmarks. Despite being trained exclusively on videos, our model also shows promising abilities in multi-concept composition, story generation, and chain-of-editing applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a5\u4ece\u89c6\u9891\u4e2d\u5b66\u4e60\u4e0a\u4e0b\u6587\u56fe\u50cf\u7f16\u8f91\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5757\u56e0\u679c\u6269\u6563\u53d8\u6362\u5668\u548c\u591a\u4efb\u52a1\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u56fe\u50cf\u7f16\u8f91\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u89c6\u9891\u76f4\u63a5\u5b66\u4e60\u4e0a\u4e0b\u6587\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\uff0c\u51cf\u5c11\u5bf9\u4efb\u52a1\u7279\u5b9a\u6d41\u6c34\u7ebf\u548c\u4e13\u5bb6\u6a21\u578b\u7684\u4f9d\u8d56\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89c6\u9891\u6807\u6ce8\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u5757\u56e0\u679c\u6269\u6563\u53d8\u6362\u5668\uff0c\u8bad\u7ec3\u4e8e\u4e09\u4e2a\u4ee3\u7406\u4efb\u52a1\uff1a\u4e0b\u4e00\u56fe\u50cf\u9884\u6d4b\u3001\u5f53\u524d\u5206\u5272\u9884\u6d4b\u548c\u4e0b\u4e00\u5206\u5272\u9884\u6d4b\u3002", "result": "\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u591a\u8f6e\u56fe\u50cf\u7f16\u8f91\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u5c3d\u7ba1\u4ec5\u901a\u8fc7\u89c6\u9891\u8bad\u7ec3\uff0c\u6a21\u578b\u5728\u591a\u6982\u5ff5\u7ec4\u5408\u3001\u6545\u4e8b\u751f\u6210\u548c\u94fe\u5f0f\u7f16\u8f91\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\u3002"}}
{"id": "2506.10963", "pdf": "https://arxiv.org/pdf/2506.10963", "abs": "https://arxiv.org/abs/2506.10963", "authors": ["Yuxuan Luo", "Yuhui Yuan", "Junwen Chen", "Haonan Cai", "Ziyi Yue", "Yuwei Yang", "Fatima Zohra Daha", "Ji Li", "Zhouhui Lian"], "title": "MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for Text-to-Image Reasoning", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "In this paper, we introduce knowledge image generation as a new task, alongside the Massive Multi-Discipline Multi-Tier Knowledge-Image Generation Benchmark (MMMG) to probe the reasoning capability of image generation models. Knowledge images have been central to human civilization and to the mechanisms of human learning--a fact underscored by dual-coding theory and the picture-superiority effect. Generating such images is challenging, demanding multimodal reasoning that fuses world knowledge with pixel-level grounding into clear explanatory visuals. To enable comprehensive evaluation, MMMG offers 4,456 expert-validated (knowledge) image-prompt pairs spanning 10 disciplines, 6 educational levels, and diverse knowledge formats such as charts, diagrams, and mind maps. To eliminate confounding complexity during evaluation, we adopt a unified Knowledge Graph (KG) representation. Each KG explicitly delineates a target image's core entities and their dependencies. We further introduce MMMG-Score to evaluate generated knowledge images. This metric combines factual fidelity, measured by graph-edit distance between KGs, with visual clarity assessment. Comprehensive evaluations of 16 state-of-the-art text-to-image generation models expose serious reasoning deficits--low entity fidelity, weak relations, and clutter--with GPT-4o achieving an MMMG-Score of only 50.20, underscoring the benchmark's difficulty. To spur further progress, we release FLUX-Reason (MMMG-Score of 34.45), an effective and open baseline that combines a reasoning LLM with diffusion models and is trained on 16,000 curated knowledge image-prompt pairs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u77e5\u8bc6\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u53caMMMG\u57fa\u51c6\uff0c\u8bc4\u4f30\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u53d1\u5e03\u5f00\u6e90\u57fa\u7ebfFLUX-Reason\u3002", "motivation": "\u77e5\u8bc6\u56fe\u50cf\u5bf9\u4eba\u7c7b\u6587\u660e\u548c\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u751f\u6210\u6b64\u7c7b\u56fe\u50cf\u9700\u8981\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u73b0\u6709\u6a21\u578b\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u65b0\u57fa\u51c6\u63a8\u52a8\u7814\u7a76\u3002", "method": "\u63d0\u51faMMMG\u57fa\u51c6\uff0c\u5305\u542b4,456\u4e2a\u4e13\u5bb6\u9a8c\u8bc1\u7684\u77e5\u8bc6\u56fe\u50cf-\u63d0\u793a\u5bf9\uff0c\u91c7\u7528\u7edf\u4e00\u77e5\u8bc6\u56fe\u8c31\u8868\u793a\uff0c\u5e76\u8bbe\u8ba1MMMG-Score\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u8bc4\u4f3016\u4e2a\u5148\u8fdb\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u53d1\u73b0\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\uff0cGPT-4o\u4ec5\u5f9750.20\u5206\u3002\u53d1\u5e03\u57fa\u7ebfFLUX-Reason\uff0834.45\u5206\uff09\u3002", "conclusion": "MMMG\u57fa\u51c6\u63ed\u793a\u4e86\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u63a8\u7406\u7f3a\u9677\uff0cFLUX-Reason\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u57fa\u7ebf\u3002"}}
{"id": "2506.10975", "pdf": "https://arxiv.org/pdf/2506.10975", "abs": "https://arxiv.org/abs/2506.10975", "authors": ["Weiliang Chen", "Wenzhao Zheng", "Yu Zheng", "Lei Chen", "Jie Zhou", "Jiwen Lu", "Yueqi Duan"], "title": "GenWorld: Towards Detecting AI-generated Real-world Simulation Videos", "categories": ["cs.CV"], "comment": null, "summary": "The flourishing of video generation technologies has endangered the credibility of real-world information and intensified the demand for AI-generated video detectors. Despite some progress, the lack of high-quality real-world datasets hinders the development of trustworthy detectors. In this paper, we propose GenWorld, a large-scale, high-quality, and real-world simulation dataset for AI-generated video detection. GenWorld features the following characteristics: (1) Real-world Simulation: GenWorld focuses on videos that replicate real-world scenarios, which have a significant impact due to their realism and potential influence; (2) High Quality: GenWorld employs multiple state-of-the-art video generation models to provide realistic and high-quality forged videos; (3) Cross-prompt Diversity: GenWorld includes videos generated from diverse generators and various prompt modalities (e.g., text, image, video), offering the potential to learn more generalizable forensic features. We analyze existing methods and find they fail to detect high-quality videos generated by world models (i.e., Cosmos), revealing potential drawbacks of ignoring real-world clues. To address this, we propose a simple yet effective model, SpannDetector, to leverage multi-view consistency as a strong criterion for real-world AI-generated video detection. Experiments show that our method achieves superior results, highlighting a promising direction for explainable AI-generated video detection based on physical plausibility. We believe that GenWorld will advance the field of AI-generated video detection. Project Page: https://chen-wl20.github.io/GenWorld", "AI": {"tldr": "GenWorld\u662f\u4e00\u4e2a\u9ad8\u8d28\u91cf\u3001\u771f\u5b9e\u6a21\u62df\u7684AI\u751f\u6210\u89c6\u9891\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u8d28\u91cf\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u89c6\u9891\u751f\u6210\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u5a01\u80c1\u4e86\u771f\u5b9e\u4e16\u754c\u4fe1\u606f\u7684\u53ef\u4fe1\u5ea6\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684AI\u751f\u6210\u89c6\u9891\u68c0\u6d4b\u5de5\u5177\u3002", "method": "\u63d0\u51faGenWorld\u6570\u636e\u96c6\uff0c\u5177\u6709\u771f\u5b9e\u6a21\u62df\u3001\u9ad8\u8d28\u91cf\u548c\u8de8\u63d0\u793a\u591a\u6837\u6027\u7279\u70b9\uff0c\u5e76\u8bbe\u8ba1SpannDetector\u6a21\u578b\uff0c\u5229\u7528\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u68c0\u6d4bAI\u751f\u6210\u89c6\u9891\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSpannDetector\u5728\u68c0\u6d4b\u9ad8\u8d28\u91cfAI\u751f\u6210\u89c6\u9891\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "GenWorld\u548cSpannDetector\u4e3a\u57fa\u4e8e\u7269\u7406\u5408\u7406\u6027\u7684\u53ef\u89e3\u91caAI\u751f\u6210\u89c6\u9891\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.10978", "pdf": "https://arxiv.org/pdf/2506.10978", "abs": "https://arxiv.org/abs/2506.10978", "authors": ["Donghoon Ahn", "Jiwon Kang", "Sanghyun Lee", "Minjae Kim", "Jaewon Min", "Wooseok Jang", "Saungwu Lee", "Sayak Paul", "Susung Hong", "Seungryong Kim"], "title": "Fine-Grained Perturbation Guidance via Attention Head Selection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project page: https://cvlab-kaist.github.io/HeadHunter/", "summary": "Recent guidance methods in diffusion models steer reverse sampling by perturbing the model to construct an implicit weak model and guide generation away from it. Among these approaches, attention perturbation has demonstrated strong empirical performance in unconditional scenarios where classifier-free guidance is not applicable. However, existing attention perturbation methods lack principled approaches for determining where perturbations should be applied, particularly in Diffusion Transformer (DiT) architectures where quality-relevant computations are distributed across layers. In this paper, we investigate the granularity of attention perturbations, ranging from the layer level down to individual attention heads, and discover that specific heads govern distinct visual concepts such as structure, style, and texture quality. Building on this insight, we propose \"HeadHunter\", a systematic framework for iteratively selecting attention heads that align with user-centric objectives, enabling fine-grained control over generation quality and visual attributes. In addition, we introduce SoftPAG, which linearly interpolates each selected head's attention map toward an identity matrix, providing a continuous knob to tune perturbation strength and suppress artifacts. Our approach not only mitigates the oversmoothing issues of existing layer-level perturbation but also enables targeted manipulation of specific visual styles through compositional head selection. We validate our method on modern large-scale DiT-based text-to-image models including Stable Diffusion 3 and FLUX.1, demonstrating superior performance in both general quality enhancement and style-specific guidance. Our work provides the first head-level analysis of attention perturbation in diffusion models, uncovering interpretable specialization within attention layers and enabling practical design of effective perturbation strategies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHeadHunter\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u9009\u62e9\u6ce8\u610f\u529b\u5934\u5b9e\u73b0\u751f\u6210\u8d28\u91cf\u548c\u89c6\u89c9\u5c5e\u6027\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u5e76\u5f15\u5165SoftPAG\u65b9\u6cd5\u8c03\u8282\u6270\u52a8\u5f3a\u5ea6\u3002", "motivation": "\u73b0\u6709\u6ce8\u610f\u529b\u6270\u52a8\u65b9\u6cd5\u7f3a\u4e4f\u786e\u5b9a\u6270\u52a8\u4f4d\u7f6e\u7684\u539f\u7406\u6027\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728DiT\u67b6\u6784\u4e2d\uff0c\u8d28\u91cf\u76f8\u5173\u8ba1\u7b97\u5206\u5e03\u5728\u591a\u5c42\u4e2d\u3002", "method": "\u7814\u7a76\u6ce8\u610f\u529b\u6270\u52a8\u7684\u7c92\u5ea6\uff0c\u4ece\u5c42\u7ea7\u5230\u5355\u4e2a\u6ce8\u610f\u529b\u5934\uff0c\u63d0\u51faHeadHunter\u6846\u67b6\u548cSoftPAG\u65b9\u6cd5\u3002", "result": "\u5728Stable Diffusion 3\u548cFLUX.1\u7b49\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u751f\u6210\u8d28\u91cf\u63d0\u5347\u548c\u98ce\u683c\u7279\u5b9a\u5f15\u5bfc\u3002", "conclusion": "\u9996\u6b21\u5728\u6269\u6563\u6a21\u578b\u4e2d\u8fdb\u884c\u4e86\u5934\u7ea7\u6ce8\u610f\u529b\u6270\u52a8\u5206\u6790\uff0c\u63ed\u793a\u4e86\u6ce8\u610f\u529b\u5c42\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u6709\u6548\u6270\u52a8\u7b56\u7565\u7684\u8bbe\u8ba1\u65b9\u6cd5\u3002"}}
{"id": "2506.10980", "pdf": "https://arxiv.org/pdf/2506.10980", "abs": "https://arxiv.org/abs/2506.10980", "authors": ["Junqi You", "Chieh Hubert Lin", "Weijie Lyu", "Zhengbo Zhang", "Ming-Hsuan Yang"], "title": "InstaInpaint: Instant 3D-Scene Inpainting with Masked Large Reconstruction Model", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in 3D scene reconstruction enable real-time viewing in virtual and augmented reality. To support interactive operations for better immersiveness, such as moving or editing objects, 3D scene inpainting methods are proposed to repair or complete the altered geometry. However, current approaches rely on lengthy and computationally intensive optimization, making them impractical for real-time or online applications. We propose InstaInpaint, a reference-based feed-forward framework that produces 3D-scene inpainting from a 2D inpainting proposal within 0.4 seconds. We develop a self-supervised masked-finetuning strategy to enable training of our custom large reconstruction model (LRM) on the large-scale dataset. Through extensive experiments, we analyze and identify several key designs that improve generalization, textural consistency, and geometric correctness. InstaInpaint achieves a 1000x speed-up from prior methods while maintaining a state-of-the-art performance across two standard benchmarks. Moreover, we show that InstaInpaint generalizes well to flexible downstream applications such as object insertion and multi-region inpainting. More video results are available at our project page: https://dhmbb2.github.io/InstaInpaint_page/.", "AI": {"tldr": "InstaInpaint\u662f\u4e00\u79cd\u57fa\u4e8e\u53c2\u8003\u7684\u524d\u9988\u6846\u67b6\uff0c\u80fd\u57280.4\u79d2\u5185\u5b8c\u62103D\u573a\u666f\u4fee\u590d\uff0c\u901f\u5ea6\u63d0\u53471000\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "motivation": "\u5f53\u524d3D\u573a\u666f\u4fee\u590d\u65b9\u6cd5\u8ba1\u7b97\u8017\u65f6\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u6216\u5728\u7ebf\u5e94\u7528\u9700\u6c42\u3002", "method": "\u63d0\u51faInstaInpaint\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u63a9\u7801\u5fae\u8c03\u7b56\u7565\u8bad\u7ec3\u5927\u578b\u91cd\u5efa\u6a21\u578b\uff08LRM\uff09\uff0c\u4ece2D\u4fee\u590d\u63d0\u6848\u751f\u62103D\u4fee\u590d\u7ed3\u679c\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u901f\u5ea6\u63d0\u53471000\u500d\uff0c\u5e76\u80fd\u7075\u6d3b\u5e94\u7528\u4e8e\u5bf9\u8c61\u63d2\u5165\u548c\u591a\u533a\u57df\u4fee\u590d\u3002", "conclusion": "InstaInpaint\u5728\u901f\u5ea6\u548c\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e0b\u6e38\u5e94\u7528\u3002"}}
{"id": "2506.10981", "pdf": "https://arxiv.org/pdf/2506.10981", "abs": "https://arxiv.org/abs/2506.10981", "authors": ["Weiliang Chen", "Jiayi Bi", "Yuanhui Huang", "Wenzhao Zheng", "Yueqi Duan"], "title": "SceneCompleter: Dense 3D Scene Completion for Generative Novel View Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Generative models have gained significant attention in novel view synthesis (NVS) by alleviating the reliance on dense multi-view captures. However, existing methods typically fall into a conventional paradigm, where generative models first complete missing areas in 2D, followed by 3D recovery techniques to reconstruct the scene, which often results in overly smooth surfaces and distorted geometry, as generative models struggle to infer 3D structure solely from RGB data. In this paper, we propose SceneCompleter, a novel framework that achieves 3D-consistent generative novel view synthesis through dense 3D scene completion. SceneCompleter achieves both visual coherence and 3D-consistent generative scene completion through two key components: (1) a geometry-appearance dual-stream diffusion model that jointly synthesizes novel views in RGBD space; (2) a scene embedder that encodes a more holistic scene understanding from the reference image. By effectively fusing structural and textural information, our method demonstrates superior coherence and plausibility in generative novel view synthesis across diverse datasets. Project Page: https://chen-wl20.github.io/SceneCompleter", "AI": {"tldr": "SceneCompleter\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5bc6\u96c63D\u573a\u666f\u8865\u5168\u5b9e\u73b03D\u4e00\u81f4\u751f\u6210\u65b0\u89c6\u89d2\u5408\u6210\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u56e0\u4f9d\u8d562D\u8865\u5168\u548c3D\u6062\u590d\u6280\u672f\u5bfc\u81f4\u7684\u5e73\u6ed1\u8868\u9762\u548c\u51e0\u4f55\u5931\u771f\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u65b0\u89c6\u89d2\u5408\u6210\u4e2d\u4f9d\u8d562D\u8865\u5168\u548c3D\u6062\u590d\u6280\u672f\uff0c\u5bfc\u81f4\u7ed3\u679c\u8fc7\u4e8e\u5e73\u6ed1\u4e14\u51e0\u4f55\u5931\u771f\uff0c\u65e0\u6cd5\u4ec5\u4eceRGB\u6570\u636e\u63a8\u65ad3D\u7ed3\u6784\u3002", "method": "SceneCompleter\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a(1) \u51e0\u4f55-\u5916\u89c2\u53cc\u6d41\u6269\u6563\u6a21\u578b\uff0c\u8054\u5408\u5728RGBD\u7a7a\u95f4\u5408\u6210\u65b0\u89c6\u89d2\uff1b(2) \u573a\u666f\u7f16\u7801\u5668\uff0c\u4ece\u53c2\u8003\u56fe\u50cf\u4e2d\u7f16\u7801\u66f4\u5168\u9762\u7684\u573a\u666f\u7406\u89e3\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u751f\u6210\u65b0\u89c6\u89d2\u5408\u6210\u4e2d\u7684\u4f18\u8d8a\u4e00\u81f4\u6027\u548c\u5408\u7406\u6027\u3002", "conclusion": "SceneCompleter\u901a\u8fc7\u878d\u5408\u7ed3\u6784\u548c\u7eb9\u7406\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u89c6\u89c9\u4e00\u81f4\u4e143D\u4e00\u81f4\u7684\u751f\u6210\u573a\u666f\u8865\u5168\u3002"}}
{"id": "2506.10002", "pdf": "https://arxiv.org/pdf/2506.10002", "abs": "https://arxiv.org/abs/2506.10002", "authors": ["Jianwu Fang", "Lei-Lei Li", "Zhedong Zheng", "Hongkai Yu", "Jianru Xue", "Zhengguo Li", "Tat-Seng Chua"], "title": "EQ-TAA: Equivariant Traffic Accident Anticipation via Diffusion-Based Accident Video Synthesis", "categories": ["cs.MM", "cs.AI", "cs.CV", "cs.RO"], "comment": "Accepted by IEEE-TMM", "summary": "Traffic Accident Anticipation (TAA) in traffic scenes is a challenging problem for achieving zero fatalities in the future. Current approaches typically treat TAA as a supervised learning task needing the laborious annotation of accident occurrence duration. However, the inherent long-tailed, uncertain, and fast-evolving nature of traffic scenes has the problem that real causal parts of accidents are difficult to identify and are easily dominated by data bias, resulting in a background confounding issue. Thus, we propose an Attentive Video Diffusion (AVD) model that synthesizes additional accident video clips by generating the causal part in dashcam videos, i.e., from normal clips to accident clips. AVD aims to generate causal video frames based on accident or accident-free text prompts while preserving the style and content of frames for TAA after video generation. This approach can be trained using datasets collected from various driving scenes without any extra annotations. Additionally, AVD facilitates an Equivariant TAA (EQ-TAA) with an equivariant triple loss for an anchor accident-free video clip, along with the generated pair of contrastive pseudo-normal and pseudo-accident clips. Extensive experiments have been conducted to evaluate the performance of AVD and EQ-TAA, and competitive performance compared to state-of-the-art methods has been obtained.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u89c6\u9891\u6269\u6563\uff08AVD\uff09\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u4ea4\u901a\u4e8b\u6545\u89c6\u9891\u7247\u6bb5\uff0c\u5e76\u901a\u8fc7\u7b49\u53d8\u4e09\u5143\u635f\u5931\uff08EQ-TAA\uff09\u63d0\u5347\u4ea4\u901a\u4e8b\u6545\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u4ea4\u901a\u4e8b\u6545\u9884\u6d4b\uff08TAA\uff09\u7684\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4e14\u6613\u53d7\u6570\u636e\u504f\u5dee\u5f71\u54cd\uff0c\u96be\u4ee5\u6355\u6349\u771f\u5b9e\u7684\u56e0\u679c\u5173\u7cfb\u3002", "method": "\u63d0\u51faAVD\u6a21\u578b\uff0c\u901a\u8fc7\u6587\u672c\u63d0\u793a\u751f\u6210\u56e0\u679c\u89c6\u9891\u5e27\uff0c\u5e76\u7ed3\u5408EQ-TAA\u65b9\u6cd5\uff0c\u4f7f\u7528\u7b49\u53d8\u4e09\u5143\u635f\u5931\u4f18\u5316\u9884\u6d4b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAVD\u548cEQ-TAA\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AVD\u548cEQ-TAA\u4e3a\u4ea4\u901a\u4e8b\u6545\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u6807\u6ce8\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.10006", "pdf": "https://arxiv.org/pdf/2506.10006", "abs": "https://arxiv.org/abs/2506.10006", "authors": ["Jie Qin", "Wei Yang", "Yan Su", "Yiran Zhu", "Weizhen Li", "Yunyue Pan", "Chengchang Pan", "Honggang Qi"], "title": "HER2 Expression Prediction with Flexible Multi-Modal Inputs via Dynamic Bidirectional Reconstruction", "categories": ["cs.MM", "cs.AI", "cs.CV", "cs.LG"], "comment": "7 pages,5 figures,3 tables,submitted to the 33rd ACM International   Conference on Multimedia(ACM MM 2025)", "summary": "Current HER2 assessment models for breast cancer predominantly analyze H&E or IHC images in isolation,despite clinical reliance on their synergistic interpretation. However, concurrent acquisition of both modalities is often hindered by workflow complexity and cost constraints. We propose an adaptive bimodal framework enabling flexible single-/dual-modality HER2 prediction through three innovations: 1) A dynamic branch selector that activates either single-modality reconstruction or dual-modality joint inference based on input completeness; 2) A bidirectional cross-modal GAN performing context-aware feature-space reconstruction of missing modalities; 3) A hybrid training protocol integrating adversarial learning and multi-task optimization. This architecture elevates single-modality H&E prediction accuracy from 71.44% to 94.25% while achieving 95.09% dual-modality accuracy, maintaining 90.28% reliability with sole IHC inputs. The framework's \"dual-preferred, single-compatible\" design delivers near-bimodal performance without requiring synchronized acquisition, particularly benefiting resource-limited settings through IHC infrastructure cost reduction. Experimental validation confirms 22.81%/12.90% accuracy improvements over H&E/IHC baselines respectively, with cross-modal reconstruction enhancing F1-scores to 0.9609 (HE to IHC) and 0.9251 (IHC to HE). By dynamically routing inputs through reconstruction-enhanced or native fusion pathways, the system mitigates performance degradation from missing data while preserving computational efficiency (78.55% parameter reduction in lightweight variant). This elastic architecture demonstrates significant potential for democratizing precise HER2 assessment across diverse healthcare settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u53cc\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u652f\u9009\u62e9\u3001\u53cc\u5411\u8de8\u6a21\u6001GAN\u548c\u6df7\u5408\u8bad\u7ec3\u534f\u8bae\uff0c\u7075\u6d3b\u652f\u6301\u5355/\u53cc\u6a21\u6001HER2\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u548c\u8d44\u6e90\u6548\u7387\u3002", "motivation": "\u73b0\u6709HER2\u8bc4\u4f30\u6a21\u578b\u901a\u5e38\u5355\u72ec\u5206\u6790H&E\u6216IHC\u56fe\u50cf\uff0c\u800c\u4e34\u5e8a\u4f9d\u8d56\u4e24\u8005\u7684\u534f\u540c\u89e3\u8bfb\uff0c\u4f46\u53cc\u6a21\u6001\u83b7\u53d6\u5e38\u53d7\u9650\u4e8e\u5de5\u4f5c\u6d41\u590d\u6742\u6027\u548c\u6210\u672c\u3002", "method": "1) \u52a8\u6001\u5206\u652f\u9009\u62e9\u5668\uff1b2) \u53cc\u5411\u8de8\u6a21\u6001GAN\uff1b3) \u6df7\u5408\u8bad\u7ec3\u534f\u8bae\u3002", "result": "\u5355\u6a21\u6001H&E\u9884\u6d4b\u51c6\u786e\u7387\u4ece71.44%\u63d0\u5347\u81f394.25%\uff0c\u53cc\u6a21\u6001\u51c6\u786e\u7387\u8fbe95.09%\uff0cIHC\u5355\u6a21\u6001\u53ef\u9760\u6027\u4e3a90.28%\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5f39\u6027\u8bbe\u8ba1\u663e\u8457\u63d0\u5347HER2\u8bc4\u4f30\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u573a\u666f\uff0c\u5177\u6709\u5e7f\u6cdb\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.10007", "pdf": "https://arxiv.org/pdf/2506.10007", "abs": "https://arxiv.org/abs/2506.10007", "authors": ["Kangwei Liu", "Junwu Liu", "Xiaowei Yi", "Jinlin Guo", "Yun Cao"], "title": "Controllable Expressive 3D Facial Animation via Diffusion in a Unified Multimodal Space", "categories": ["cs.MM", "cs.AI", "cs.CV"], "comment": "Accepted by ICME2025", "summary": "Audio-driven emotional 3D facial animation encounters two significant challenges: (1) reliance on single-modal control signals (videos, text, or emotion labels) without leveraging their complementary strengths for comprehensive emotion manipulation, and (2) deterministic regression-based mapping that constrains the stochastic nature of emotional expressions and non-verbal behaviors, limiting the expressiveness of synthesized animations. To address these challenges, we present a diffusion-based framework for controllable expressive 3D facial animation. Our approach introduces two key innovations: (1) a FLAME-centered multimodal emotion binding strategy that aligns diverse modalities (text, audio, and emotion labels) through contrastive learning, enabling flexible emotion control from multiple signal sources, and (2) an attention-based latent diffusion model with content-aware attention and emotion-guided layers, which enriches motion diversity while maintaining temporal coherence and natural facial dynamics. Extensive experiments demonstrate that our method outperforms existing approaches across most metrics, achieving a 21.6\\% improvement in emotion similarity while preserving physiologically plausible facial dynamics. Project Page: https://kangweiiliu.github.io/Control_3D_Animation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u591a\u6a21\u6001\u60c5\u611f3D\u9762\u90e8\u52a8\u753b\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5355\u6a21\u6001\u4fe1\u53f7\u548c\u786e\u5b9a\u6027\u6620\u5c04\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5355\u6a21\u6001\u63a7\u5236\u4fe1\u53f7\u4e14\u7f3a\u4e4f\u968f\u673a\u6027\u8868\u8fbe\uff0c\u9650\u5236\u4e86\u52a8\u753b\u7684\u8868\u73b0\u529b\u3002", "method": "\u91c7\u7528FLAME\u591a\u6a21\u6001\u60c5\u611f\u7ed1\u5b9a\u7b56\u7565\u548c\u6ce8\u610f\u529b\u6f5c\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u591a\u4fe1\u53f7\u6e90\u60c5\u611f\u63a7\u5236\u548c\u591a\u6837\u5316\u8fd0\u52a8\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u60c5\u611f\u76f8\u4f3c\u5ea6\u4e0a\u63d0\u534721.6%\uff0c\u540c\u65f6\u4fdd\u6301\u81ea\u7136\u7684\u9762\u90e8\u52a8\u6001\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u548c\u6269\u6563\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e863D\u9762\u90e8\u52a8\u753b\u7684\u8868\u73b0\u529b\u548c\u53ef\u63a7\u6027\u3002"}}
{"id": "2506.10172", "pdf": "https://arxiv.org/pdf/2506.10172", "abs": "https://arxiv.org/abs/2506.10172", "authors": ["Yicheng Duan", "Kaiyu tang"], "title": "A Navigation Framework Utilizing Vision-Language Models", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Vision-and-Language Navigation (VLN) presents a complex challenge in embodied AI, requiring agents to interpret natural language instructions and navigate through visually rich, unfamiliar environments. Recent advances in large vision-language models (LVLMs), such as CLIP and Flamingo, have significantly improved multimodal understanding but introduced new challenges related to computational cost and real-time deployment. In this project, we propose a modular, plug-and-play navigation framework that decouples vision-language understanding from action planning. By integrating a frozen vision-language model, Qwen2.5-VL-7B-Instruct, with lightweight planning logic, we aim to achieve flexible, fast, and adaptable navigation without extensive model fine-tuning. Our framework leverages prompt engineering, structured history management, and a two-frame visual input strategy to enhance decision-making continuity across navigation steps. We evaluate our system on the Room-to-Room benchmark within the VLN-CE setting using the Matterport3D dataset and Habitat-Lab simulation environment. Although our initial results reveal challenges in generalizing to unseen environments under strict evaluation settings, our modular approach lays a foundation for scalable and efficient navigation systems, highlighting promising directions for future improvement through enhanced environmental priors and expanded multimodal input integration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u4e0e\u884c\u52a8\u89c4\u5212\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u89c4\u5212\u903b\u8f91\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u7075\u6d3b\u7684\u5bfc\u822a\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u5b9e\u65f6\u90e8\u7f72\u96be\u7684\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u5bfc\u822a\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u548c\u6548\u7387\u3002", "method": "\u4f7f\u7528\u51bb\u7ed3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578bQwen2.5-VL-7B-Instruct\uff0c\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u3001\u7ed3\u6784\u5316\u5386\u53f2\u7ba1\u7406\u548c\u53cc\u5e27\u89c6\u89c9\u8f93\u5165\u7b56\u7565\u3002", "result": "\u5728Room-to-Room\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u521d\u6b65\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u4f46\u5728\u672a\u89c1\u73af\u5883\u4e2d\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "conclusion": "\u6a21\u5757\u5316\u65b9\u6cd5\u4e3a\u9ad8\u6548\u5bfc\u822a\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u672a\u6765\u53ef\u901a\u8fc7\u589e\u5f3a\u73af\u5883\u5148\u9a8c\u548c\u6269\u5c55\u591a\u6a21\u6001\u8f93\u5165\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2506.10177", "pdf": "https://arxiv.org/pdf/2506.10177", "abs": "https://arxiv.org/abs/2506.10177", "authors": ["Defang Chen", "Zhenyu Zhou", "Can Wang", "Siwei Lyu"], "title": "Geometric Regularity in Deterministic Sampling of Diffusion-based Generative Models", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": "50 pages. The short version appeared in ICML 2024. arXiv admin note:   substantial text overlap with arXiv:2405.11326", "summary": "Diffusion-based generative models employ stochastic differential equations (SDEs) and their equivalent probability flow ordinary differential equations (ODEs) to establish a smooth transformation between complex high-dimensional data distributions and tractable prior distributions. In this paper, we reveal a striking geometric regularity in the deterministic sampling dynamics: each simulated sampling trajectory lies within an extremely low-dimensional subspace, and all trajectories exhibit an almost identical ''boomerang'' shape, regardless of the model architecture, applied conditions, or generated content. We characterize several intriguing properties of these trajectories, particularly under closed-form solutions based on kernel-estimated data modeling. We also demonstrate a practical application of the discovered trajectory regularity by proposing a dynamic programming-based scheme to better align the sampling time schedule with the underlying trajectory structure. This simple strategy requires minimal modification to existing ODE-based numerical solvers, incurs negligible computational overhead, and achieves superior image generation performance, especially in regions with only $5 \\sim 10$ function evaluations.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u786e\u5b9a\u6027\u91c7\u6837\u52a8\u6001\u4e2d\u7684\u51e0\u4f55\u89c4\u5f8b\u6027\uff0c\u53d1\u73b0\u91c7\u6837\u8f68\u8ff9\u96c6\u4e2d\u5728\u6781\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u4e14\u5f62\u72b6\u76f8\u4f3c\uff0c\u5e76\u63d0\u51fa\u4e86\u52a8\u6001\u89c4\u5212\u65b9\u6848\u4f18\u5316\u91c7\u6837\u65f6\u95f4\u8868\u3002", "motivation": "\u63a2\u7d22\u6269\u6563\u751f\u6210\u6a21\u578b\u4e2d\u786e\u5b9a\u6027\u91c7\u6837\u8f68\u8ff9\u7684\u51e0\u4f55\u7279\u6027\uff0c\u4ee5\u63d0\u5347\u91c7\u6837\u6548\u7387\u548c\u751f\u6210\u8d28\u91cf\u3002", "method": "\u5206\u6790\u91c7\u6837\u8f68\u8ff9\u7684\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u7279\u6027\uff0c\u63d0\u51fa\u57fa\u4e8e\u52a8\u6001\u89c4\u5212\u7684\u91c7\u6837\u65f6\u95f4\u8868\u4f18\u5316\u65b9\u6848\u3002", "result": "\u91c7\u6837\u8f68\u8ff9\u5177\u6709\u4f4e\u7ef4\u6027\u548c\u5f62\u72b6\u76f8\u4f3c\u6027\uff0c\u4f18\u5316\u65b9\u6848\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u751f\u6210\u6027\u80fd\u3002", "conclusion": "\u53d1\u73b0\u91c7\u6837\u8f68\u8ff9\u7684\u51e0\u4f55\u89c4\u5f8b\u6027\u4e3a\u4f18\u5316\u6269\u6563\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u52a8\u6001\u89c4\u5212\u65b9\u6848\u7b80\u5355\u9ad8\u6548\u3002"}}
{"id": "2506.10230", "pdf": "https://arxiv.org/pdf/2506.10230", "abs": "https://arxiv.org/abs/2506.10230", "authors": ["Emerson P. Grabke", "Masoom A. Haider", "Babak Taati"], "title": "Prompt-Guided Latent Diffusion with Predictive Class Conditioning for 3D Prostate MRI Generation", "categories": ["eess.IV", "cs.CV"], "comment": "MAH and BT are co-senior authors on the work. This work has been   submitted to the IEEE for possible publication", "summary": "Latent diffusion models (LDM) could alleviate data scarcity challenges affecting machine learning development for medical imaging. However, medical LDM training typically relies on performance- or scientific accessibility-limiting strategies including a reliance on short-prompt text encoders, the reuse of non-medical LDMs, or a requirement for fine-tuning with large data volumes. We propose a Class-Conditioned Efficient Large Language model Adapter (CCELLA) to address these limitations. CCELLA is a novel dual-head conditioning approach that simultaneously conditions the LDM U-Net with non-medical large language model-encoded text features through cross-attention and with pathology classification through the timestep embedding. We also propose a joint loss function and a data-efficient LDM training framework. In combination, these strategies enable pathology-conditioned LDM training for high-quality medical image synthesis given limited data volume and human data annotation, improving LDM performance and scientific accessibility. Our method achieves a 3D FID score of 0.025 on a size-limited prostate MRI dataset, significantly outperforming a recent foundation model with FID 0.071. When training a classifier for prostate cancer prediction, adding synthetic images generated by our method to the training dataset improves classifier accuracy from 69% to 74%. Training a classifier solely on our method's synthetic images achieved comparable performance to training on real images alone.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCCELLA\u7684\u53cc\u5934\u6761\u4ef6\u65b9\u6cd5\uff0c\u7ed3\u5408\u975e\u533b\u5b66\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u75c5\u7406\u5206\u7c7b\uff0c\u89e3\u51b3\u4e86\u533b\u5b66LDM\u8bad\u7ec3\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5408\u6210\u56fe\u50cf\u8d28\u91cf\u548c\u5206\u7c7b\u5668\u6027\u80fd\u3002", "motivation": "\u533b\u5b66LDM\u8bad\u7ec3\u5e38\u53d7\u9650\u4e8e\u6570\u636e\u7a00\u7f3a\u3001\u77ed\u63d0\u793a\u6587\u672c\u7f16\u7801\u5668\u6216\u975e\u533b\u5b66LDMs\u7684\u590d\u7528\uff0c\u5bfc\u81f4\u6027\u80fd\u6216\u79d1\u5b66\u53ef\u8bbf\u95ee\u6027\u53d7\u9650\u3002", "method": "\u63d0\u51faCCELLA\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u65f6\u95f4\u6b65\u5d4c\u5165\u540c\u65f6\u6761\u4ef6\u5316LDM U-Net\uff0c\u5e76\u8bbe\u8ba1\u8054\u5408\u635f\u5931\u51fd\u6570\u548c\u6570\u636e\u9ad8\u6548\u8bad\u7ec3\u6846\u67b6\u3002", "result": "\u5728\u6709\u9650\u6570\u636e\u4e0b\uff0c\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff08FID 0.025 vs 0.071\uff09\uff0c\u5408\u6210\u56fe\u50cf\u63d0\u5347\u4e86\u5206\u7c7b\u5668\u51c6\u786e\u7387\uff0869%\u523074%\uff09\u3002", "conclusion": "CCELLA\u65b9\u6cd5\u5728\u6570\u636e\u6709\u9650\u6761\u4ef6\u4e0b\u9ad8\u6548\u8bad\u7ec3\u533b\u5b66LDM\uff0c\u63d0\u5347\u4e86\u5408\u6210\u56fe\u50cf\u8d28\u91cf\u548c\u79d1\u5b66\u53ef\u8bbf\u95ee\u6027\u3002"}}
{"id": "2506.10233", "pdf": "https://arxiv.org/pdf/2506.10233", "abs": "https://arxiv.org/abs/2506.10233", "authors": ["Ana Lawry Aguila", "Peirong Liu", "Oula Puonti", "Juan Eugenio Iglesias"], "title": "Conditional diffusion models for guided anomaly detection in brain images using fluid-driven anomaly randomization", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Supervised machine learning has enabled accurate pathology detection in brain MRI, but requires training data from diseased subjects that may not be readily available in some scenarios, for example, in the case of rare diseases. Reconstruction-based unsupervised anomaly detection, in particular using diffusion models, has gained popularity in the medical field as it allows for training on healthy images alone, eliminating the need for large disease-specific cohorts. These methods assume that a model trained on normal data cannot accurately represent or reconstruct anomalies. However, this assumption often fails with models failing to reconstruct healthy tissue or accurately reconstruct abnormal regions i.e., failing to remove anomalies. In this work, we introduce a novel conditional diffusion model framework for anomaly detection and healthy image reconstruction in brain MRI. Our weakly supervised approach integrates synthetically generated pseudo-pathology images into the modeling process to better guide the reconstruction of healthy images. To generate these pseudo-pathologies, we apply fluid-driven anomaly randomization to augment real pathology segmentation maps from an auxiliary dataset, ensuring that the synthetic anomalies are both realistic and anatomically coherent. We evaluate our model's ability to detect pathology, using both synthetic anomaly datasets and real pathology from the ATLAS dataset. In our extensive experiments, our model: (i) consistently outperforms variational autoencoders, and conditional and unconditional latent diffusion; and (ii) surpasses on most datasets, the performance of supervised inpainting methods with access to paired diseased/healthy images.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u8111MRI\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u548c\u5065\u5eb7\u56fe\u50cf\u91cd\u5efa\uff0c\u901a\u8fc7\u5f15\u5165\u5408\u6210\u4f2a\u75c5\u7406\u56fe\u50cf\u6539\u8fdb\u91cd\u5efa\u6548\u679c\u3002", "motivation": "\u76d1\u7763\u5b66\u4e60\u9700\u8981\u5927\u91cf\u60a3\u75c5\u6570\u636e\uff0c\u800c\u7f55\u89c1\u75be\u75c5\u6570\u636e\u7a00\u7f3a\uff1b\u73b0\u6709\u65e0\u76d1\u7763\u65b9\u6cd5\u5047\u8bbe\u6a21\u578b\u65e0\u6cd5\u91cd\u5efa\u5f02\u5e38\uff0c\u4f46\u5b9e\u9645\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\u6846\u67b6\uff0c\u7ed3\u5408\u5408\u6210\u4f2a\u75c5\u7406\u56fe\u50cf\uff08\u901a\u8fc7\u6d41\u4f53\u9a71\u52a8\u5f02\u5e38\u968f\u673a\u5316\u751f\u6210\uff09\u6307\u5bfc\u5065\u5eb7\u56fe\u50cf\u91cd\u5efa\u3002", "result": "\u6a21\u578b\u5728\u5408\u6210\u548c\u771f\u5b9e\u75c5\u7406\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668\u3001\u6761\u4ef6/\u65e0\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u751a\u81f3\u90e8\u5206\u8d85\u8d8a\u6709\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u5f31\u76d1\u7763\u65b9\u6cd5\u5728\u5f02\u5e38\u68c0\u6d4b\u548c\u5065\u5eb7\u56fe\u50cf\u91cd\u5efa\u4e2d\u6548\u679c\u663e\u8457\uff0c\u4e3a\u7f55\u89c1\u75be\u75c5\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.10325", "pdf": "https://arxiv.org/pdf/2506.10325", "abs": "https://arxiv.org/abs/2506.10325", "authors": ["Cheng Wang", "Siqi Chen", "Donghua Mi", "Yang Chen", "Yudong Zhang", "Yinsheng Li"], "title": "SWDL: Stratum-Wise Difference Learning with Deep Laplacian Pyramid for Semi-Supervised 3D Intracranial Hemorrhage Segmentation", "categories": ["eess.IV", "cs.CV", "cs.LG", "92C55, 68T45", "I.4.6; I.4.9; I.2.6; J.3"], "comment": "11 pages, 4 figures, 6 Tables", "summary": "Recent advances in medical imaging have established deep learning-based segmentation as the predominant approach, though it typically requires large amounts of manually annotated data. However, obtaining annotations for intracranial hemorrhage (ICH) remains particularly challenging due to the tedious and costly labeling process. Semi-supervised learning (SSL) has emerged as a promising solution to address the scarcity of labeled data, especially in volumetric medical image segmentation. Unlike conventional SSL methods that primarily focus on high-confidence pseudo-labels or consistency regularization, we propose SWDL-Net, a novel SSL framework that exploits the complementary advantages of Laplacian pyramid and deep convolutional upsampling. The Laplacian pyramid excels at edge sharpening, while deep convolutions enhance detail precision through flexible feature mapping. Our framework achieves superior segmentation of lesion details and boundaries through a difference learning mechanism that effectively integrates these complementary approaches. Extensive experiments on a 271-case ICH dataset and public benchmarks demonstrate that SWDL-Net outperforms current state-of-the-art methods in scenarios with only 2% labeled data. Additional evaluations on the publicly available Brain Hemorrhage Segmentation Dataset (BHSD) with 5% labeled data further confirm the superiority of our approach. Code and data have been released at https://github.com/SIAT-CT-LAB/SWDL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6SWDL-Net\uff0c\u7ed3\u5408\u62c9\u666e\u62c9\u65af\u91d1\u5b57\u5854\u548c\u6df1\u5ea6\u5377\u79ef\u4e0a\u91c7\u6837\u7684\u4f18\u52bf\uff0c\u7528\u4e8e\u9885\u5185\u51fa\u8840\uff08ICH\uff09\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u5728\u4ec5\u6709\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u9885\u5185\u51fa\u8840\uff08ICH\uff09\u6807\u6ce8\u6570\u636e\u7684\u83b7\u53d6\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u5c40\u9650\u6027\u51f8\u663e\u3002\u534a\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u6210\u4e3a\u89e3\u51b3\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u6f5c\u5728\u65b9\u6848\u3002", "method": "\u63d0\u51faSWDL-Net\u6846\u67b6\uff0c\u7ed3\u5408\u62c9\u666e\u62c9\u65af\u91d1\u5b57\u5854\uff08\u64c5\u957f\u8fb9\u7f18\u9510\u5316\uff09\u548c\u6df1\u5ea6\u5377\u79ef\u4e0a\u91c7\u6837\uff08\u63d0\u5347\u7ec6\u8282\u7cbe\u5ea6\uff09\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u901a\u8fc7\u5dee\u5f02\u5b66\u4e60\u673a\u5236\u5b9e\u73b0\u66f4\u597d\u7684\u5206\u5272\u6548\u679c\u3002", "result": "\u5728271\u4f8bICH\u6570\u636e\u96c6\u548c\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ec5\u4f7f\u75282%\u6807\u6ce8\u6570\u636e\u65f6\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff1b\u5728BHSD\u6570\u636e\u96c6\uff085%\u6807\u6ce8\u6570\u636e\uff09\u4e0a\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "SWDL-Net\u901a\u8fc7\u521b\u65b0\u6027\u5730\u6574\u5408\u4e92\u8865\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u534a\u76d1\u7763\u5b66\u4e60\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.10540", "pdf": "https://arxiv.org/pdf/2506.10540", "abs": "https://arxiv.org/abs/2506.10540", "authors": ["Haoyuan Shi", "Yunxin Li", "Xinyu Chen", "Longyue Wang", "Baotian Hu", "Min Zhang"], "title": "AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation", "categories": ["cs.MA", "cs.CV"], "comment": null, "summary": "Despite rapid advancements in video generation models, generating coherent storytelling videos that span multiple scenes and characters remains challenging. Current methods often rigidly convert pre-generated keyframes into fixed-length clips, resulting in disjointed narratives and pacing issues. Furthermore, the inherent instability of video generation models means that even a single low-quality clip can significantly degrade the entire output animation's logical coherence and visual continuity. To overcome these obstacles, we introduce AniMaker, a multi-agent framework enabling efficient multi-candidate clip generation and storytelling-aware clip selection, thus creating globally consistent and story-coherent animation solely from text input. The framework is structured around specialized agents, including the Director Agent for storyboard generation, the Photography Agent for video clip generation, the Reviewer Agent for evaluation, and the Post-Production Agent for editing and voiceover. Central to AniMaker's approach are two key technical components: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search (MCTS)-inspired strategy that intelligently navigates the candidate space to generate high-potential clips while optimizing resource usage; and AniEval in Reviewer Agent, the first framework specifically designed for multi-shot animation evaluation, which assesses critical aspects such as story-level consistency, action completion, and animation-specific features by considering each clip in the context of its preceding and succeeding clips. Experiments demonstrate that AniMaker achieves superior quality as measured by popular metrics including VBench and our proposed AniEval framework, while significantly improving the efficiency of multi-candidate generation, pushing AI-generated storytelling animation closer to production standards.", "AI": {"tldr": "AniMaker\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u6548\u751f\u6210\u591a\u5019\u9009\u89c6\u9891\u7247\u6bb5\u548c\u6545\u4e8b\u611f\u77e5\u7684\u7247\u6bb5\u9009\u62e9\uff0c\u89e3\u51b3\u4e86\u591a\u573a\u666f\u548c\u89d2\u8272\u89c6\u9891\u751f\u6210\u7684\u8fde\u8d2f\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u591a\u573a\u666f\u548c\u89d2\u8272\u53d9\u4e8b\u89c6\u9891\u751f\u6210\u4e2d\u5b58\u5728\u53d9\u4e8b\u4e0d\u8fde\u8d2f\u548c\u8282\u594f\u95ee\u9898\uff0cAniMaker\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "AniMaker\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u62ec\u5bfc\u6f14\u3001\u6444\u5f71\u3001\u8bc4\u5ba1\u548c\u540e\u671f\u5236\u4f5c\u4ee3\u7406\uff0c\u7ed3\u5408MCTS-Gen\u548cAniEval\u6280\u672f\u4f18\u5316\u751f\u6210\u548c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAniMaker\u5728VBench\u548cAniEval\u7b49\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u5019\u9009\u751f\u6210\u7684\u6548\u7387\u548c\u53d9\u4e8b\u8fde\u8d2f\u6027\u3002", "conclusion": "AniMaker\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u4e13\u7528\u8bc4\u4f30\u6846\u67b6\uff0c\u63a8\u52a8\u4e86AI\u751f\u6210\u53d9\u4e8b\u52a8\u753b\u5411\u751f\u4ea7\u6807\u51c6\u8fc8\u8fdb\u3002"}}
{"id": "2506.10632", "pdf": "https://arxiv.org/pdf/2506.10632", "abs": "https://arxiv.org/abs/2506.10632", "authors": ["Alexander Lobashev", "Dmitry Guskov", "Maria Larchenko", "Mikhail Tamm"], "title": "Hessian Geometry of Latent Space in Generative Models", "categories": ["cs.LG", "cond-mat.stat-mech", "cs.CV", "math.DG", "math.ST", "stat.TH"], "comment": "ICML 2025", "summary": "This paper presents a novel method for analyzing the latent space geometry of generative models, including statistical physics models and diffusion models, by reconstructing the Fisher information metric. The method approximates the posterior distribution of latent variables given generated samples and uses this to learn the log-partition function, which defines the Fisher metric for exponential families. Theoretical convergence guarantees are provided, and the method is validated on the Ising and TASEP models, outperforming existing baselines in reconstructing thermodynamic quantities. Applied to diffusion models, the method reveals a fractal structure of phase transitions in the latent space, characterized by abrupt changes in the Fisher metric. We demonstrate that while geodesic interpolations are approximately linear within individual phases, this linearity breaks down at phase boundaries, where the diffusion model exhibits a divergent Lipschitz constant with respect to the latent space. These findings provide new insights into the complex structure of diffusion model latent spaces and their connection to phenomena like phase transitions. Our source code is available at https://github.com/alobashev/hessian-geometry-of-diffusion-models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u6784Fisher\u4fe1\u606f\u5ea6\u91cf\u6765\u5206\u6790\u751f\u6210\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u51e0\u4f55\u7ed3\u6784\uff0c\u5305\u62ec\u7edf\u8ba1\u7269\u7406\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u3002", "motivation": "\u7814\u7a76\u751f\u6210\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u7684\u51e0\u4f55\u7ed3\u6784\u53ca\u5176\u4e0e\u7269\u7406\u73b0\u8c61\uff08\u5982\u76f8\u53d8\uff09\u7684\u8054\u7cfb\u3002", "method": "\u901a\u8fc7\u8fd1\u4f3c\u6f5c\u5728\u53d8\u91cf\u7684\u540e\u9a8c\u5206\u5e03\u5b66\u4e60\u5bf9\u6570\u914d\u5206\u51fd\u6570\uff0c\u4ece\u800c\u5b9a\u4e49Fisher\u5ea6\u91cf\uff0c\u5e76\u63d0\u4f9b\u7406\u8bba\u6536\u655b\u4fdd\u8bc1\u3002", "result": "\u5728Ising\u548cTASEP\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002\u6269\u6563\u6a21\u578b\u4e2d\u63ed\u793a\u4e86\u6f5c\u5728\u7a7a\u95f4\u7684\u76f8\u53d8\u5206\u5f62\u7ed3\u6784\u3002", "conclusion": "\u65b9\u6cd5\u63ed\u793a\u4e86\u6269\u6563\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u7684\u590d\u6742\u7ed3\u6784\u53ca\u5176\u4e0e\u76f8\u53d8\u7684\u8054\u7cfb\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2506.10955", "pdf": "https://arxiv.org/pdf/2506.10955", "abs": "https://arxiv.org/abs/2506.10955", "authors": ["Aayush Karan", "Kulin Shah", "Sitan Chen"], "title": "ReGuidance: A Simple Diffusion Wrapper for Boosting Sample Quality on Hard Inverse Problems", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "38 pages, 14 figures", "summary": "There has been a flurry of activity around using pretrained diffusion models as informed data priors for solving inverse problems, and more generally around steering these models using reward models. Training-free methods like diffusion posterior sampling (DPS) and its many variants have offered flexible heuristic algorithms for these tasks, but when the reward is not informative enough, e.g., in hard inverse problems with low signal-to-noise ratio, these techniques veer off the data manifold, failing to produce realistic outputs. In this work, we devise a simple wrapper, ReGuidance, for boosting both the sample realism and reward achieved by these methods. Given a candidate solution $\\hat{x}$ produced by an algorithm of the user's choice, we propose inverting the solution by running the unconditional probability flow ODE in reverse starting from $\\hat{x}$, and then using the resulting latent as an initialization for DPS. We evaluate our wrapper on hard inverse problems like large box in-painting and super-resolution with high upscaling. Whereas state-of-the-art baselines visibly fail, we find that applying our wrapper on top of these baselines significantly boosts sample quality and measurement consistency. We complement these findings with theory proving that on certain multimodal data distributions, ReGuidance simultaneously boosts the reward and brings the candidate solution closer to the data manifold. To our knowledge, this constitutes the first rigorous algorithmic guarantee for DPS.", "AI": {"tldr": "ReGuidance\u662f\u4e00\u79cd\u7b80\u5355\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u6269\u6563\u6a21\u578b\u5728\u89e3\u51b3\u9006\u95ee\u9898\u65f6\u7684\u6837\u672c\u771f\u5b9e\u6027\u548c\u5956\u52b1\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4f4e\u4fe1\u566a\u6bd4\u7684\u9006\u95ee\u9898\u4e2d\u5bb9\u6613\u504f\u79bb\u6570\u636e\u6d41\u5f62\uff0c\u5bfc\u81f4\u8f93\u51fa\u4e0d\u771f\u5b9e\u3002", "method": "\u901a\u8fc7\u53cd\u8f6c\u5019\u9009\u89e3\u5e76\u91cd\u65b0\u521d\u59cb\u5316DPS\uff0c\u63d0\u5347\u6837\u672c\u8d28\u91cf\u548c\u6d4b\u91cf\u4e00\u81f4\u6027\u3002", "result": "\u5728\u56f0\u96be\u7684\u9006\u95ee\u9898\u4e2d\uff0cReGuidance\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "ReGuidance\u662f\u9996\u4e2a\u4e3aDPS\u63d0\u4f9b\u4e25\u683c\u7b97\u6cd5\u4fdd\u8bc1\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.10968", "pdf": "https://arxiv.org/pdf/2506.10968", "abs": "https://arxiv.org/abs/2506.10968", "authors": ["Justin Kerr", "Kush Hari", "Ethan Weber", "Chung Min Kim", "Brent Yi", "Tyler Bonnen", "Ken Goldberg", "Angjoo Kanazawa"], "title": "Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop", "categories": ["cs.RO", "cs.CV"], "comment": "Project page: https://www.eyerobot.net/", "summary": "Humans do not passively observe the visual world -- we actively look in order to act. Motivated by this principle, we introduce EyeRobot, a robotic system with gaze behavior that emerges from the need to complete real-world tasks. We develop a mechanical eyeball that can freely rotate to observe its surroundings and train a gaze policy to control it using reinforcement learning. We accomplish this by first collecting teleoperated demonstrations paired with a 360 camera. This data is imported into a simulation environment that supports rendering arbitrary eyeball viewpoints, allowing episode rollouts of eye gaze on top of robot demonstrations. We then introduce a BC-RL loop to train the hand and eye jointly: the hand (BC) agent is trained from rendered eye observations, and the eye (RL) agent is rewarded when the hand produces correct action predictions. In this way, hand-eye coordination emerges as the eye looks towards regions which allow the hand to complete the task. EyeRobot implements a foveal-inspired policy architecture allowing high resolution with a small compute budget, which we find also leads to the emergence of more stable fixation as well as improved ability to track objects and ignore distractors. We evaluate EyeRobot on five panoramic workspace manipulation tasks requiring manipulation in an arc surrounding the robot arm. Our experiments suggest EyeRobot exhibits hand-eye coordination behaviors which effectively facilitate manipulation over large workspaces with a single camera. See project site for videos: https://www.eyerobot.net/", "AI": {"tldr": "EyeRobot\u662f\u4e00\u4e2a\u6a21\u4eff\u4eba\u7c7b\u4e3b\u52a8\u89c2\u5bdf\u884c\u4e3a\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u773c\u7403\u8fd0\u52a8\u7b56\u7565\uff0c\u5b9e\u73b0\u624b\u773c\u534f\u8c03\u5b8c\u6210\u4efb\u52a1\u3002", "motivation": "\u4eba\u7c7b\u901a\u8fc7\u4e3b\u52a8\u89c2\u5bdf\u6765\u5b8c\u6210\u4efb\u52a1\uff0c\u53d7\u6b64\u542f\u53d1\uff0c\u5f00\u53d1\u4e86\u80fd\u591f\u81ea\u4e3b\u63a7\u5236\u773c\u7403\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528360\u5ea6\u6444\u50cf\u5934\u6536\u96c6\u6570\u636e\uff0c\u5728\u4eff\u771f\u73af\u5883\u4e2d\u8bad\u7ec3\u773c\u7403\u8fd0\u52a8\u7b56\u7565\uff0c\u5e76\u901a\u8fc7BC-RL\u5faa\u73af\u8054\u5408\u8bad\u7ec3\u624b\u548c\u773c\u3002", "result": "EyeRobot\u5728\u4e94\u4e2a\u5168\u666f\u5de5\u4f5c\u7a7a\u95f4\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u7684\u624b\u773c\u534f\u8c03\u80fd\u529b\uff0c\u80fd\u591f\u5728\u5927\u8303\u56f4\u5185\u5b8c\u6210\u4efb\u52a1\u3002", "conclusion": "EyeRobot\u5c55\u793a\u4e86\u901a\u8fc7\u5355\u4e00\u6444\u50cf\u5934\u5b9e\u73b0\u9ad8\u6548\u624b\u773c\u534f\u8c03\u7684\u6f5c\u529b\uff0c\u4e3a\u673a\u5668\u4eba\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
