{"id": "2510.05532", "pdf": "https://arxiv.org/pdf/2510.05532", "abs": "https://arxiv.org/abs/2510.05532", "authors": ["Sam Sartor", "Pieter Peers"], "title": "Teamwork: Collaborative Diffusion with Low-rank Coordination and Adaptation", "categories": ["cs.CV", "cs.GR", "cs.LG"], "comment": null, "summary": "Large pretrained diffusion models can provide strong priors beneficial for many graphics applications. However, generative applications such as neural rendering and inverse methods such as SVBRDF estimation and intrinsic image decomposition require additional input or output channels. Current solutions for channel expansion are often application specific and these solutions can be difficult to adapt to different diffusion models or new tasks. This paper introduces Teamwork: a flexible and efficient unified solution for jointly increasing the number of input and output channels as well as adapting a pretrained diffusion model to new tasks. Teamwork achieves channel expansion without altering the pretrained diffusion model architecture by coordinating and adapting multiple instances of the base diffusion model (\\ie, teammates). We employ a novel variation of Low Rank-Adaptation (LoRA) to jointly address both adaptation and coordination between the different teammates. Furthermore Teamwork supports dynamic (de)activation of teammates. We demonstrate the flexibility and efficiency of Teamwork on a variety of generative and inverse graphics tasks such as inpainting, single image SVBRDF estimation, intrinsic decomposition, neural shading, and intrinsic image synthesis.", "AI": {"tldr": "Teamwork\u662f\u4e00\u4e2a\u7075\u6d3b\u9ad8\u6548\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u534f\u8c03\u591a\u4e2a\u57fa\u7840\u6269\u6563\u6a21\u578b\u5b9e\u4f8b\u6765\u6269\u5c55\u8f93\u5165\u8f93\u51fa\u901a\u9053\uff0c\u65e0\u9700\u4fee\u6539\u9884\u8bad\u7ec3\u6a21\u578b\u67b6\u6784\uff0c\u652f\u6301\u591a\u79cd\u56fe\u5f62\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u901a\u9053\u6269\u5c55\u65b9\u6cd5\u901a\u5e38\u662f\u7279\u5b9a\u4e8e\u5e94\u7528\u7684\uff0c\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u6269\u6563\u6a21\u578b\u6216\u65b0\u4efb\u52a1\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u591a\u4e2a\u57fa\u7840\u6269\u6563\u6a21\u578b\u5b9e\u4f8b\uff08\u961f\u53cb\uff09\u8fdb\u884c\u534f\u8c03\uff0c\u91c7\u7528\u6539\u8fdb\u7684\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u65b9\u6cd5\u540c\u65f6\u5904\u7406\u9002\u5e94\u548c\u534f\u8c03\u95ee\u9898\uff0c\u652f\u6301\u961f\u53cb\u7684\u52a8\u6001\u6fc0\u6d3b/\u505c\u7528\u3002", "result": "\u5728\u591a\u79cd\u751f\u6210\u548c\u9006\u5411\u56fe\u5f62\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ec\u4fee\u590d\u3001\u5355\u56fe\u50cfSVBRDF\u4f30\u8ba1\u3001\u672c\u5f81\u5206\u89e3\u3001\u795e\u7ecf\u7740\u8272\u548c\u672c\u5f81\u56fe\u50cf\u5408\u6210\u3002", "conclusion": "Teamwork\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u4e14\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u80fd\u591f\u7075\u6d3b\u6269\u5c55\u6269\u6563\u6a21\u578b\u7684\u8f93\u5165\u8f93\u51fa\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u56fe\u5f62\u5e94\u7528\u3002"}}
{"id": "2510.05356", "pdf": "https://arxiv.org/pdf/2510.05356", "abs": "https://arxiv.org/abs/2510.05356", "authors": ["Kostas Triaridis", "Alexandros Graikos", "Aggelina Chatziagapi", "Grigorios G. Chrysos", "Dimitris Samaras"], "title": "Mitigating Diffusion Model Hallucinations with Dynamic Guidance", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Diffusion models, despite their impressive demos, often produce hallucinatory samples with structural inconsistencies that lie outside of the support of the true data distribution. Such hallucinations can be attributed to excessive smoothing between modes of the data distribution. However, semantic interpolations are often desirable and can lead to generation diversity, thus we believe a more nuanced solution is required. In this work, we introduce Dynamic Guidance, which tackles this issue. Dynamic Guidance mitigates hallucinations by selectively sharpening the score function only along the pre-determined directions known to cause artifacts, while preserving valid semantic variations. To our knowledge, this is the first approach that addresses hallucinations at generation time rather than through post-hoc filtering. Dynamic Guidance substantially reduces hallucinations on both controlled and natural image datasets, significantly outperforming baselines.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u5f15\u5bfc\u65b9\u6cd5\uff0c\u5728\u751f\u6210\u65f6\u9009\u62e9\u6027\u9510\u5316\u5bfc\u81f4\u4f2a\u5f71\u7684\u5206\u6570\u51fd\u6570\u65b9\u5411\uff0c\u51cf\u5c11\u6269\u6563\u6a21\u578b\u7684\u5e7b\u89c9\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u591a\u6837\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u867d\u7136\u751f\u6210\u6548\u679c\u60ca\u8273\uff0c\u4f46\u5e38\u5e38\u4ea7\u751f\u7ed3\u6784\u4e0d\u4e00\u81f4\u7684\u5e7b\u89c9\u6837\u672c\uff0c\u8fd9\u4e9b\u6837\u672c\u4f4d\u4e8e\u771f\u5b9e\u6570\u636e\u5206\u5e03\u7684\u652f\u6301\u4e4b\u5916\u3002\u8fd9\u4e9b\u5e7b\u89c9\u53ef\u5f52\u56e0\u4e8e\u6570\u636e\u5206\u5e03\u6a21\u5f0f\u95f4\u7684\u8fc7\u5ea6\u5e73\u6ed1\u3002", "method": "\u5f15\u5165\u52a8\u6001\u5f15\u5bfc\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ec5\u6cbf\u5df2\u77e5\u5bfc\u81f4\u4f2a\u5f71\u7684\u9884\u5b9a\u65b9\u5411\u9009\u62e9\u6027\u9510\u5316\u5206\u6570\u51fd\u6570\uff0c\u6765\u51cf\u8f7b\u5e7b\u89c9\uff0c\u540c\u65f6\u4fdd\u7559\u6709\u6548\u7684\u8bed\u4e49\u53d8\u5316\u3002", "result": "\u52a8\u6001\u5f15\u5bfc\u5728\u53d7\u63a7\u548c\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\uff0c\u660e\u663e\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5728\u751f\u6210\u65f6\u800c\u975e\u901a\u8fc7\u540e\u5904\u7406\u8fc7\u6ee4\u6765\u89e3\u51b3\u5e7b\u89c9\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u52a8\u6001\u5f15\u5bfc\u80fd\u6709\u6548\u51cf\u5c11\u6269\u6563\u6a21\u578b\u7684\u5e7b\u89c9\u751f\u6210\u3002"}}
{"id": "2510.05367", "pdf": "https://arxiv.org/pdf/2510.05367", "abs": "https://arxiv.org/abs/2510.05367", "authors": ["Yang Xiao", "Gen Li", "Kaiyuan Deng", "Yushu Wu", "Zheng Zhan", "Yanzhi Wang", "Xiaolong Ma", "Bo Hui"], "title": "LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Training-free acceleration has emerged as an advanced research area in video generation based on diffusion models. The redundancy of latents in diffusion model inference provides a natural entry point for acceleration. In this paper, we decompose the inference process into the encoding, denoising, and decoding stages, and observe that cache-based acceleration methods often lead to substantial memory surges in the latter two stages. To address this problem, we analyze the characteristics of inference across different stages and propose stage-specific strategies for reducing memory consumption: 1) Asynchronous Cache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same time, we ensure that the time overhead introduced by these three strategies remains lower than the acceleration gains themselves. Compared with the baseline, our approach achieves faster inference speed and lower memory usage, while maintaining quality degradation within an acceptable range. The Code is available at https://github.com/NKUShaw/LightCache .", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u514d\u8d39\u7684\u89c6\u9891\u751f\u6210\u52a0\u901f\u65b9\u6cd5LightCache\uff0c\u901a\u8fc7\u5206\u6790\u6269\u6563\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u7f16\u7801\u3001\u53bb\u566a\u548c\u89e3\u7801\u4e09\u4e2a\u9636\u6bb5\uff0c\u8bbe\u8ba1\u4e86\u5f02\u6b65\u7f13\u5b58\u4ea4\u6362\u3001\u7279\u5f81\u5206\u5757\u548c\u5207\u7247\u89e3\u7801\u7b49\u7b56\u7565\u6765\u964d\u4f4e\u5185\u5b58\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u52a0\u901f\u6548\u679c\u3002", "motivation": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c6\u9891\u751f\u6210\u4e2d\uff0c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u6f5c\u5728\u53d8\u91cf\u5197\u4f59\u4e3a\u52a0\u901f\u63d0\u4f9b\u4e86\u5207\u5165\u70b9\u3002\u73b0\u6709\u57fa\u4e8e\u7f13\u5b58\u7684\u52a0\u901f\u65b9\u6cd5\u5728\u540e\u4e24\u4e2a\u9636\u6bb5\uff08\u53bb\u566a\u548c\u89e3\u7801\uff09\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u5185\u5b58\u6fc0\u589e\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u5185\u5b58\u6d88\u8017\u95ee\u9898\u3002", "method": "1) \u5f02\u6b65\u7f13\u5b58\u4ea4\u6362\uff1a\u4f18\u5316\u7f13\u5b58\u7ba1\u7406\u7b56\u7565\uff1b2) \u7279\u5f81\u5206\u5757\uff1a\u5c06\u7279\u5f81\u5206\u5272\u5904\u7406\uff1b3) \u5207\u7247\u89e3\u7801\uff1a\u5bf9\u6f5c\u5728\u53d8\u91cf\u8fdb\u884c\u5207\u7247\u89e3\u7801\u3002\u8fd9\u4e9b\u7b56\u7565\u786e\u4fdd\u5f15\u5165\u7684\u65f6\u95f4\u5f00\u9500\u4f4e\u4e8e\u52a0\u901f\u6536\u76ca\u3002", "result": "\u4e0e\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u548c\u66f4\u4f4e\u7684\u5185\u5b58\u4f7f\u7528\uff0c\u540c\u65f6\u5c06\u8d28\u91cf\u4e0b\u964d\u63a7\u5236\u5728\u53ef\u63a5\u53d7\u8303\u56f4\u5185\u3002", "conclusion": "LightCache\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u751f\u6210\u6269\u6563\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u6d88\u8017\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u52a0\u901f\u6548\u679c\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u9700\u6c42\uff0c\u4e3a\u8bad\u7ec3\u514d\u8d39\u52a0\u901f\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.05488", "pdf": "https://arxiv.org/pdf/2510.05488", "abs": "https://arxiv.org/abs/2510.05488", "authors": ["Peizhi Yan", "Rabab Ward", "Qiang Tang", "Shan Du"], "title": "ArchitectHead: Continuous Level of Detail Control for 3D Gaussian Head Avatars", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has enabled photorealistic and real-time rendering of 3D head avatars. Existing 3DGS-based avatars typically rely on tens of thousands of 3D Gaussian points (Gaussians), with the number of Gaussians fixed after training. However, many practical applications require adjustable levels of detail (LOD) to balance rendering efficiency and visual quality. In this work, we propose \"ArchitectHead\", the first framework for creating 3D Gaussian head avatars that support continuous control over LOD. Our key idea is to parameterize the Gaussians in a 2D UV feature space and propose a UV feature field composed of multi-level learnable feature maps to encode their latent features. A lightweight neural network-based decoder then transforms these latent features into 3D Gaussian attributes for rendering. ArchitectHead controls the number of Gaussians by dynamically resampling feature maps from the UV feature field at the desired resolutions. This method enables efficient and continuous control of LOD without retraining. Experimental results show that ArchitectHead achieves state-of-the-art (SOTA) quality in self and cross-identity reenactment tasks at the highest LOD, while maintaining near SOTA performance at lower LODs. At the lowest LOD, our method uses only 6.2\\% of the Gaussians while the quality degrades moderately (L1 Loss +7.9\\%, PSNR --0.97\\%, SSIM --0.6\\%, LPIPS Loss +24.1\\%), and the rendering speed nearly doubles.", "AI": {"tldr": "ArchitectHead\u662f\u9996\u4e2a\u652f\u6301\u8fde\u7eed\u63a7\u5236\u7ec6\u8282\u7ea7\u522b\u76843D\u9ad8\u65af\u5934\u90e8\u5316\u8eab\u6846\u67b6\uff0c\u901a\u8fc7UV\u7279\u5f81\u573a\u548c\u52a8\u6001\u91cd\u91c7\u6837\u5b9e\u73b0\u6e32\u67d3\u6548\u7387\u4e0e\u89c6\u89c9\u8d28\u91cf\u7684\u5e73\u8861\u3002", "motivation": "\u73b0\u67093DGS\u5316\u8eab\u7684\u9ad8\u65af\u70b9\u6570\u91cf\u56fa\u5b9a\uff0c\u65e0\u6cd5\u6839\u636e\u5e94\u7528\u9700\u6c42\u8c03\u6574\u7ec6\u8282\u7ea7\u522b\u6765\u5e73\u8861\u6e32\u67d3\u6548\u7387\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "method": "\u57282D UV\u7279\u5f81\u7a7a\u95f4\u4e2d\u53c2\u6570\u5316\u9ad8\u65af\u70b9\uff0c\u6784\u5efa\u591a\u7ea7\u53ef\u5b66\u4e60\u7279\u5f81\u56fe\u7ec4\u6210\u7684UV\u7279\u5f81\u573a\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u89e3\u7801\u5668\u5c06\u6f5c\u5728\u7279\u5f81\u8f6c\u6362\u4e3a3D\u9ad8\u65af\u5c5e\u6027\uff0c\u901a\u8fc7\u52a8\u6001\u91cd\u91c7\u6837\u7279\u5f81\u56fe\u63a7\u5236\u9ad8\u65af\u70b9\u6570\u91cf\u3002", "result": "\u5728\u6700\u9ad8\u7ec6\u8282\u7ea7\u522b\u5b9e\u73b0SOTA\u8d28\u91cf\uff0c\u6700\u4f4e\u7ec6\u8282\u7ea7\u522b\u4ec5\u4f7f\u75286.2%\u9ad8\u65af\u70b9\uff0c\u8d28\u91cf\u9002\u5ea6\u4e0b\u964d\uff08L1\u635f\u5931+7.9%\uff0cPSNR -0.97%\uff0cSSIM -0.6%\uff0cLPIPS\u635f\u5931+24.1%\uff09\uff0c\u6e32\u67d3\u901f\u5ea6\u51e0\u4e4e\u7ffb\u500d\u3002", "conclusion": "ArchitectHead\u5b9e\u73b0\u4e86\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u8fde\u7eed\u7ec6\u8282\u7ea7\u522b\u63a7\u5236\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6e32\u67d3\u6548\u7387\u3002"}}
{"id": "2510.05593", "pdf": "https://arxiv.org/pdf/2510.05593", "abs": "https://arxiv.org/abs/2510.05593", "authors": ["Zeqi Gu", "Markos Georgopoulos", "Xiaoliang Dai", "Marjan Ghazvininejad", "Chu Wang", "Felix Juefei-Xu", "Kunpeng Li", "Yujun Shi", "Zecheng He", "Zijian He", "Jiawei Zhou", "Abe Davis", "Jialiang Wang"], "title": "Improving Chain-of-Thought Efficiency for Autoregressive Image Generation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Autoregressive multimodal large language models have recently gained popularity for image generation, driven by advances in foundation models. To enhance alignment and detail, newer approaches employ chain-of-thought (CoT) reasoning, expanding user inputs into elaborated prompts prior to image synthesis. However, this strategy can introduce unnecessary redundancy -- a phenomenon we call visual overthinking -- which increases computational costs and can introduce details that contradict the original prompt. In this work, we explore how to generate more concise CoT sequences for more efficient image generation. We introduce ShortCoTI, a lightweight optimization framework that encourages more concise CoT while preserving output image quality. ShortCoTI rewards more concise prompts with an adaptive function that scales according to an estimated difficulty for each task. Incorporating this reward into a reinforcement learning paradigm reduces prompt reasoning length by 54% while maintaining or slightly improving quality metrics across multiple benchmarks (T2I-CompBench, GenEval). Qualitative analysis shows that our method eliminates verbose explanations and repetitive refinements, producing reasoning prompts that are both concise and semantically rich. As a result, ShortCoTI improves computational efficiency without compromising the fidelity or visual appeal of generated images.", "AI": {"tldr": "ShortCoTI\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u51cf\u5c11\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u601d\u7ef4\u94fe\u5197\u4f59\uff0c\u5728\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\u5c06\u63d0\u793a\u63a8\u7406\u957f\u5ea6\u51cf\u5c1154%\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u601d\u7ef4\u94fe\u63a8\u7406\u6765\u589e\u5f3a\u56fe\u50cf\u751f\u6210\u7684\u5bf9\u9f50\u6027\u548c\u7ec6\u8282\uff0c\u4f46\u4f1a\u5bfc\u81f4\u89c6\u89c9\u8fc7\u5ea6\u601d\u8003\u73b0\u8c61\uff0c\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u5e76\u53ef\u80fd\u5f15\u5165\u4e0e\u539f\u59cb\u63d0\u793a\u77db\u76fe\u7684\u7ec6\u8282\u3002", "method": "\u63d0\u51faShortCoTI\u6846\u67b6\uff0c\u4f7f\u7528\u81ea\u9002\u5e94\u5956\u52b1\u51fd\u6570\u9f13\u52b1\u66f4\u7b80\u6d01\u7684\u601d\u7ef4\u94fe\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\u4f18\u5316\u63d0\u793a\u63a8\u7406\u957f\u5ea6\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08T2I-CompBench\u3001GenEval\uff09\u4e0a\uff0c\u63a8\u7406\u957f\u5ea6\u51cf\u5c1154%\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u7565\u5fae\u63d0\u9ad8\u8d28\u91cf\u6307\u6807\uff0c\u6d88\u9664\u4e86\u5197\u4f59\u89e3\u91ca\u548c\u91cd\u590d\u4f18\u5316\u3002", "conclusion": "ShortCoTI\u5728\u4e0d\u5f71\u54cd\u751f\u6210\u56fe\u50cf\u4fdd\u771f\u5ea6\u6216\u89c6\u89c9\u5438\u5f15\u529b\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2510.05610", "pdf": "https://arxiv.org/pdf/2510.05610", "abs": "https://arxiv.org/abs/2510.05610", "authors": ["Jiaqi Liu", "Tao Huang", "Chang Xu"], "title": "Efficient Conditional Generation on Scale-based Visual Autoregressive Models", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in autoregressive (AR) models have demonstrated their potential to rival diffusion models in image synthesis. However, for complex spatially-conditioned generation, current AR approaches rely on fine-tuning the pre-trained model, leading to significant training costs. In this paper, we propose the Efficient Control Model (ECM), a plug-and-play framework featuring a lightweight control module that introduces control signals via a distributed architecture. This architecture consists of context-aware attention layers that refine conditional features using real-time generated tokens, and a shared gated feed-forward network (FFN) designed to maximize the utilization of its limited capacity and ensure coherent control feature learning. Furthermore, recognizing the critical role of early-stage generation in determining semantic structure, we introduce an early-centric sampling strategy that prioritizes learning early control sequences. This approach reduces computational cost by lowering the number of training tokens per iteration, while a complementary temperature scheduling during inference compensates for the resulting insufficient training of late-stage tokens. Extensive experiments on scale-based AR models validate that our method achieves high-fidelity and diverse control over image generation, surpassing existing baselines while significantly improving both training and inference efficiency.", "AI": {"tldr": "\u63d0\u51faECM\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u63a7\u5236\u6a21\u5757\u5b9e\u73b0\u9ad8\u6548\u7684\u7a7a\u95f4\u6761\u4ef6\u56fe\u50cf\u751f\u6210\uff0c\u65e0\u9700\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u81ea\u56de\u5f52\u6a21\u578b\u5728\u590d\u6742\u7a7a\u95f4\u6761\u4ef6\u751f\u6210\u65f6\u9700\u8981\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5bfc\u81f4\u9ad8\u6602\u8bad\u7ec3\u6210\u672c\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u5206\u5e03\u5f0f\u67b6\u6784\u7684\u8f7b\u91cf\u63a7\u5236\u6a21\u5757\uff0c\u5305\u542b\u4e0a\u4e0b\u6587\u611f\u77e5\u6ce8\u610f\u529b\u5c42\u548c\u5171\u4eab\u95e8\u63a7FFN\uff0c\u914d\u5408\u65e9\u671f\u4e2d\u5fc3\u91c7\u6837\u7b56\u7565\u548c\u6e29\u5ea6\u8c03\u5ea6\u3002", "result": "\u5728\u57fa\u4e8e\u5c3a\u5ea6\u7684\u81ea\u56de\u5f52\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u548c\u591a\u6837\u6027\u7684\u56fe\u50cf\u751f\u6210\u63a7\u5236\uff0c\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ECM\u6846\u67b6\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6761\u4ef6\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.05633", "pdf": "https://arxiv.org/pdf/2510.05633", "abs": "https://arxiv.org/abs/2510.05633", "authors": ["Sara Mandelli", "Diego Vila-Portela", "David V\u00e1zquez-Pad\u00edn", "Paolo Bestagini", "Fernando P\u00e9rez-Gonz\u00e1lez"], "title": "Beyond Spectral Peaks: Interpreting the Cues Behind Synthetic Image Detection", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "Over the years, the forensics community has proposed several deep learning-based detectors to mitigate the risks of generative AI. Recently, frequency-domain artifacts (particularly periodic peaks in the magnitude spectrum), have received significant attention, as they have been often considered a strong indicator of synthetic image generation. However, state-of-the-art detectors are typically used as black-boxes, and it still remains unclear whether they truly rely on these peaks. This limits their interpretability and trust. In this work, we conduct a systematic study to address this question. We propose a strategy to remove spectral peaks from images and analyze the impact of this operation on several detectors. In addition, we introduce a simple linear detector that relies exclusively on frequency peaks, providing a fully interpretable baseline free from the confounding influence of deep learning. Our findings reveal that most detectors are not fundamentally dependent on spectral peaks, challenging a widespread assumption in the field and paving the way for more transparent and reliable forensic tools.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u5206\u6790\u4e86\u57fa\u4e8e\u9891\u7387\u57df\u4f2a\u5f71\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\u662f\u5426\u771f\u6b63\u4f9d\u8d56\u9891\u8c31\u5cf0\u503c\uff0c\u53d1\u73b0\u5927\u591a\u6570\u68c0\u6d4b\u5668\u5e76\u4e0d\u4ece\u6839\u672c\u4e0a\u4f9d\u8d56\u8fd9\u4e9b\u5cf0\u503c\uff0c\u6311\u6218\u4e86\u8be5\u9886\u57df\u7684\u666e\u904d\u5047\u8bbe\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4f2a\u9020\u68c0\u6d4b\u5668\u901a\u5e38\u88ab\u89c6\u4e3a\u9ed1\u76d2\uff0c\u4e0d\u6e05\u695a\u5b83\u4eec\u662f\u5426\u771f\u6b63\u4f9d\u8d56\u9891\u8c31\u5cf0\u503c\u4f5c\u4e3a\u5408\u6210\u56fe\u50cf\u751f\u6210\u7684\u5f3a\u6307\u6807\uff0c\u8fd9\u9650\u5236\u4e86\u68c0\u6d4b\u5668\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u56fe\u50cf\u4e2d\u79fb\u9664\u9891\u8c31\u5cf0\u503c\u7684\u7b56\u7565\uff0c\u5e76\u5206\u6790\u8be5\u64cd\u4f5c\u5bf9\u591a\u4e2a\u68c0\u6d4b\u5668\u7684\u5f71\u54cd\uff1b\u540c\u65f6\u5f15\u5165\u4e86\u4e00\u4e2a\u4ec5\u4f9d\u8d56\u9891\u7387\u5cf0\u503c\u7684\u7b80\u5355\u7ebf\u6027\u68c0\u6d4b\u5668\u4f5c\u4e3a\u5b8c\u5168\u53ef\u89e3\u91ca\u7684\u57fa\u7ebf\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5927\u591a\u6570\u68c0\u6d4b\u5668\u5e76\u4e0d\u4ece\u6839\u672c\u4e0a\u4f9d\u8d56\u9891\u8c31\u5cf0\u503c\uff0c\u8fd9\u4e0e\u8be5\u9886\u57df\u7684\u666e\u904d\u5047\u8bbe\u76f8\u77db\u76fe\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u6311\u6218\u4e86\u5173\u4e8e\u9891\u8c31\u5cf0\u503c\u5728\u4f2a\u9020\u68c0\u6d4b\u4e2d\u91cd\u8981\u6027\u7684\u666e\u904d\u5047\u8bbe\uff0c\u4e3a\u5f00\u53d1\u66f4\u900f\u660e\u53ef\u9760\u7684\u53d6\u8bc1\u5de5\u5177\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.05661", "pdf": "https://arxiv.org/pdf/2510.05661", "abs": "https://arxiv.org/abs/2510.05661", "authors": ["Daniel Gonz\u00e1lbez-Biosca", "Josep Cabacas-Maso", "Carles Ventura", "Ismael Benito-Altamirano"], "title": "When and How to Cut Classical Concerts? A Multimodal Automated Video Editing Approach", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Automated video editing remains an underexplored task in the computer vision and multimedia domains, especially when contrasted with the growing interest in video generation and scene understanding. In this work, we address the specific challenge of editing multicamera recordings of classical music concerts by decomposing the problem into two key sub-tasks: when to cut and how to cut. Building on recent literature, we propose a novel multimodal architecture for the temporal segmentation task (when to cut), which integrates log-mel spectrograms from the audio signals, plus an optional image embedding, and scalar temporal features through a lightweight convolutional-transformer pipeline. For the spatial selection task (how to cut), we improve the literature by updating from old backbones, e.g. ResNet, with a CLIP-based encoder and constraining distractor selection to segments from the same concert. Our dataset was constructed following a pseudo-labeling approach, in which raw video data was automatically clustered into coherent shot segments. We show that our models outperformed previous baselines in detecting cut points and provide competitive visual shot selection, advancing the state of the art in multimodal automated video editing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u53e4\u5178\u97f3\u4e50\u4f1a\u591a\u6444\u50cf\u673a\u5f55\u50cf\u81ea\u52a8\u7f16\u8f91\u7684\u591a\u6a21\u6001\u67b6\u6784\uff0c\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u65f6\u95f4\u5206\u5272\uff08\u4f55\u65f6\u526a\u5207\uff09\u548c\u7a7a\u95f4\u9009\u62e9\uff08\u5982\u4f55\u526a\u5207\uff09\u4e24\u4e2a\u5b50\u4efb\u52a1\u3002", "motivation": "\u81ea\u52a8\u89c6\u9891\u7f16\u8f91\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u591a\u5a92\u4f53\u9886\u57df\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u7279\u522b\u662f\u4e0e\u89c6\u9891\u751f\u6210\u548c\u573a\u666f\u7406\u89e3\u76f8\u6bd4\u3002\u672c\u6587\u4e13\u6ce8\u4e8e\u7f16\u8f91\u53e4\u5178\u97f3\u4e50\u4f1a\u7684\u591a\u6444\u50cf\u673a\u5f55\u50cf\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u5377\u79ef-Transformer\u7ba1\u9053\u6574\u5408\u97f3\u9891\u5bf9\u6570\u6885\u5c14\u9891\u8c31\u56fe\u3001\u53ef\u9009\u56fe\u50cf\u5d4c\u5165\u548c\u6807\u91cf\u65f6\u95f4\u7279\u5f81\u8fdb\u884c\u65f6\u95f4\u5206\u5272\uff1b\u4f7f\u7528CLIP\u7f16\u7801\u5668\u6539\u8fdb\u7a7a\u95f4\u9009\u62e9\uff0c\u5e76\u5c06\u5e72\u6270\u9879\u9009\u62e9\u9650\u5236\u5728\u540c\u4e00\u97f3\u4e50\u4f1a\u7247\u6bb5\u5185\u3002", "result": "\u6a21\u578b\u5728\u68c0\u6d4b\u526a\u5207\u70b9\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u57fa\u7ebf\uff0c\u5e76\u5728\u89c6\u89c9\u955c\u5934\u9009\u62e9\u65b9\u9762\u5177\u6709\u7ade\u4e89\u529b\uff0c\u63a8\u8fdb\u4e86\u591a\u6a21\u6001\u81ea\u52a8\u89c6\u9891\u7f16\u8f91\u7684\u6280\u672f\u6c34\u5e73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5206\u89e3\u7f16\u8f91\u4efb\u52a1\u548c\u91c7\u7528\u591a\u6a21\u6001\u67b6\u6784\uff0c\u5728\u53e4\u5178\u97f3\u4e50\u4f1a\u89c6\u9891\u81ea\u52a8\u7f16\u8f91\u65b9\u9762\u53d6\u5f97\u4e86\u5148\u8fdb\u6210\u679c\uff0c\u4e3a\u81ea\u52a8\u89c6\u9891\u7f16\u8f91\u9886\u57df\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2510.05715", "pdf": "https://arxiv.org/pdf/2510.05715", "abs": "https://arxiv.org/abs/2510.05715", "authors": ["Shihao Zhu", "Bohan Cao", "Ziheng Ouyang", "Zhen Li", "Peng-Tao Jiang", "Qibin Hou"], "title": "AgeBooth: Controllable Facial Aging and Rejuvenation via Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Recent diffusion model research focuses on generating identity-consistent images from a reference photo, but they struggle to accurately control age while preserving identity, and fine-tuning such models often requires costly paired images across ages. In this paper, we propose AgeBooth, a novel age-specific finetuning approach that can effectively enhance the age control capability of adapterbased identity personalization models without the need for expensive age-varied datasets. To reduce dependence on a large amount of age-labeled data, we exploit the linear nature of aging by introducing age-conditioned prompt blending and an age-specific LoRA fusion strategy that leverages SVDMix, a matrix fusion technique. These techniques enable high-quality generation of intermediate-age portraits. Our AgeBooth produces realistic and identity-consistent face images across different ages from a single reference image. Experiments show that AgeBooth achieves superior age control and visual quality compared to previous state-of-the-art editing-based methods.", "AI": {"tldr": "AgeBooth\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5e74\u9f84\u7279\u5b9a\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e74\u9f84\u6761\u4ef6\u63d0\u793a\u6df7\u5408\u548c\u5e74\u9f84\u7279\u5b9aLoRA\u878d\u5408\u7b56\u7565\uff0c\u65e0\u9700\u5927\u91cf\u5e74\u9f84\u6807\u6ce8\u6570\u636e\u5373\u53ef\u589e\u5f3a\u57fa\u4e8e\u9002\u914d\u5668\u7684\u8eab\u4efd\u4e2a\u6027\u5316\u6a21\u578b\u7684\u5e74\u9f84\u63a7\u5236\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u8eab\u4efd\u4e00\u81f4\u56fe\u50cf\u65f6\u96be\u4ee5\u51c6\u786e\u63a7\u5236\u5e74\u9f84\uff0c\u4e14\u5fae\u8c03\u9700\u8981\u6602\u8d35\u7684\u8de8\u5e74\u9f84\u914d\u5bf9\u56fe\u50cf\u3002", "method": "\u5229\u7528\u5e74\u9f84\u7684\u7ebf\u6027\u7279\u6027\uff0c\u5f15\u5165\u5e74\u9f84\u6761\u4ef6\u63d0\u793a\u6df7\u5408\u548c\u57fa\u4e8eSVDMix\u77e9\u9635\u878d\u5408\u7684\u5e74\u9f84\u7279\u5b9aLoRA\u878d\u5408\u7b56\u7565\uff0c\u5b9e\u73b0\u4e2d\u95f4\u5e74\u9f84\u8096\u50cf\u7684\u9ad8\u8d28\u91cf\u751f\u6210\u3002", "result": "AgeBooth\u4ece\u5355\u5f20\u53c2\u8003\u56fe\u50cf\u751f\u6210\u4e0d\u540c\u5e74\u9f84\u6bb5\u7684\u771f\u5b9e\u4e14\u8eab\u4efd\u4e00\u81f4\u7684\u4eba\u8138\u56fe\u50cf\uff0c\u5728\u5e74\u9f84\u63a7\u5236\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u7f16\u8f91\u65b9\u6cd5\u3002", "conclusion": "AgeBooth\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u5927\u91cf\u5e74\u9f84\u6807\u6ce8\u6570\u636e\u7684\u6709\u6548\u5e74\u9f84\u63a7\u5236\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8eab\u4efd\u4e2a\u6027\u5316\u6a21\u578b\u7684\u5e74\u9f84\u7f16\u8f91\u80fd\u529b\u3002"}}
{"id": "2510.05722", "pdf": "https://arxiv.org/pdf/2510.05722", "abs": "https://arxiv.org/abs/2510.05722", "authors": ["Jiaojiao Ye", "Jiaxing Zhong", "Qian Xie", "Yuzhou Zhou", "Niki Trigoni", "Andrew Markham"], "title": "Data Factory with Minimal Human Effort Using VLMs", "categories": ["cs.CV"], "comment": "Tech report", "summary": "Generating enough and diverse data through augmentation offers an efficient solution to the time-consuming and labour-intensive process of collecting and annotating pixel-wise images. Traditional data augmentation techniques often face challenges in manipulating high-level semantic attributes, such as materials and textures. In contrast, diffusion models offer a robust alternative, by effectively utilizing text-to-image or image-to-image transformation. However, existing diffusion-based methods are either computationally expensive or compromise on performance. To address this issue, we introduce a novel training-free pipeline that integrates pretrained ControlNet and Vision-Language Models (VLMs) to generate synthetic images paired with pixel-level labels. This approach eliminates the need for manual annotations and significantly improves downstream tasks. To improve the fidelity and diversity, we add a Multi-way Prompt Generator, Mask Generator and High-quality Image Selection module. Our results on PASCAL-5i and COCO-20i present promising performance and outperform concurrent work for one-shot semantic segmentation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u7ba1\u9053\uff0c\u96c6\u6210\u9884\u8bad\u7ec3\u7684ControlNet\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u751f\u6210\u5e26\u6709\u50cf\u7d20\u7ea7\u6807\u7b7e\u7684\u5408\u6210\u56fe\u50cf\uff0c\u7528\u4e8e\u63d0\u5347\u5c11\u6837\u672c\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u96be\u4ee5\u64cd\u4f5c\u9ad8\u7ea7\u8bed\u4e49\u5c5e\u6027\uff08\u5982\u6750\u8d28\u548c\u7eb9\u7406\uff09\uff0c\u800c\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\u8981\u4e48\u8ba1\u7b97\u6602\u8d35\uff0c\u8981\u4e48\u6027\u80fd\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684ControlNet\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u8bad\u7ec3\u514d\u8d39\u7ba1\u9053\uff0c\u5305\u542b\u591a\u8def\u63d0\u793a\u751f\u6210\u5668\u3001\u63a9\u7801\u751f\u6210\u5668\u548c\u9ad8\u8d28\u91cf\u56fe\u50cf\u9009\u62e9\u6a21\u5757\uff0c\u751f\u6210\u5e26\u6709\u50cf\u7d20\u7ea7\u6807\u7b7e\u7684\u5408\u6210\u56fe\u50cf\u3002", "result": "\u5728PASCAL-5i\u548cCOCO-20i\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u6709\u5e0c\u671b\u7684\u6027\u80fd\uff0c\u5e76\u5728\u4e00\u6b21\u6027\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u4f18\u4e8e\u540c\u671f\u5de5\u4f5c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u624b\u52a8\u6807\u6ce8\uff0c\u663e\u8457\u6539\u5584\u4e0b\u6e38\u4efb\u52a1\uff0c\u901a\u8fc7\u63d0\u9ad8\u4fdd\u771f\u5ea6\u548c\u591a\u6837\u6027\u6765\u589e\u5f3a\u5c11\u6837\u672c\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2510.05814", "pdf": "https://arxiv.org/pdf/2510.05814", "abs": "https://arxiv.org/abs/2510.05814", "authors": ["Yi-Hsin Li", "Thomas Sikora", "Sebastian Knorr", "M\u00e5rten Sj\u00f6str\u00f6m"], "title": "Rasterized Steered Mixture of Experts for Efficient 2D Image Regression", "categories": ["cs.CV"], "comment": null, "summary": "The Steered Mixture of Experts regression framework has demonstrated strong performance in image reconstruction, compression, denoising, and super-resolution. However, its high computational cost limits practical applications. This work introduces a rasterization-based optimization strategy that combines the efficiency of rasterized Gaussian kernel rendering with the edge-aware gating mechanism of the Steered Mixture of Experts. The proposed method is designed to accelerate two-dimensional image regression while maintaining the model's inherent sparsity and reconstruction quality. By replacing global iterative optimization with a rasterized formulation, the method achieves significantly faster parameter updates and more memory-efficient model representations. In addition, the proposed framework supports applications such as native super-resolution and image denoising, which are not directly achievable with standard rasterized Gaussian kernel approaches. The combination of fast rasterized optimization with the edge-aware structure of the Steered Mixture of Experts provides a new balance between computational efficiency and reconstruction fidelity for two-dimensional image processing tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5149\u6805\u5316\u7684\u4f18\u5316\u7b56\u7565\uff0c\u7ed3\u5408\u5149\u6805\u5316\u9ad8\u65af\u6838\u6e32\u67d3\u6548\u7387\u548cSteered Mixture of Experts\u7684\u8fb9\u7f18\u611f\u77e5\u95e8\u63a7\u673a\u5236\uff0c\u52a0\u901f\u4e8c\u7ef4\u56fe\u50cf\u56de\u5f52\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7a00\u758f\u6027\u548c\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "Steered Mixture of Experts\u56de\u5f52\u6846\u67b6\u5728\u56fe\u50cf\u91cd\u5efa\u3001\u538b\u7f29\u3001\u53bb\u566a\u548c\u8d85\u5206\u8fa8\u7387\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9ad8\u8ba1\u7b97\u6210\u672c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5149\u6805\u5316\u9ad8\u65af\u6838\u6e32\u67d3\u66ff\u4ee3\u5168\u5c40\u8fed\u4ee3\u4f18\u5316\uff0c\u7ed3\u5408Steered Mixture of Experts\u7684\u8fb9\u7f18\u611f\u77e5\u95e8\u63a7\u673a\u5236\uff0c\u5b9e\u73b0\u5feb\u901f\u53c2\u6570\u66f4\u65b0\u548c\u5185\u5b58\u9ad8\u6548\u7684\u6a21\u578b\u8868\u793a\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u52a0\u901f\u4e86\u53c2\u6570\u66f4\u65b0\uff0c\u652f\u6301\u539f\u751f\u8d85\u5206\u8fa8\u7387\u548c\u56fe\u50cf\u53bb\u566a\u7b49\u5e94\u7528\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u91cd\u5efa\u4fdd\u771f\u5ea6\u4e4b\u95f4\u8fbe\u5230\u65b0\u7684\u5e73\u8861\u3002", "conclusion": "\u5149\u6805\u5316\u4f18\u5316\u4e0e\u8fb9\u7f18\u611f\u77e5\u7ed3\u6784\u7684\u7ed3\u5408\u4e3a\u4e8c\u7ef4\u56fe\u50cf\u5904\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u91cd\u5efa\u8d28\u91cf\u4e4b\u95f4\u7684\u826f\u597d\u6743\u8861\u3002"}}
{"id": "2510.05891", "pdf": "https://arxiv.org/pdf/2510.05891", "abs": "https://arxiv.org/abs/2510.05891", "authors": ["Yanran Zhang", "Bingyao Yu", "Yu Zheng", "Wenzhao Zheng", "Yueqi Duan", "Lei Chen", "Jie Zhou", "Jiwen Lu"], "title": "$\\bf{D^3}$QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 5 figures, published to ICCV2025", "summary": "The emergence of visual autoregressive (AR) models has revolutionized image generation while presenting new challenges for synthetic image detection. Unlike previous GAN or diffusion-based methods, AR models generate images through discrete token prediction, exhibiting both marked improvements in image synthesis quality and unique characteristics in their vector-quantized representations. In this paper, we propose to leverage Discrete Distribution Discrepancy-aware Quantization Error (D$^3$QE) for autoregressive-generated image detection that exploits the distinctive patterns and the frequency distribution bias of the codebook existing in real and fake images. We introduce a discrete distribution discrepancy-aware transformer that integrates dynamic codebook frequency statistics into its attention mechanism, fusing semantic features and quantization error latent. To evaluate our method, we construct a comprehensive dataset termed ARForensics covering 7 mainstream visual AR models. Experiments demonstrate superior detection accuracy and strong generalization of D$^3$QE across different AR models, with robustness to real-world perturbations. Code is available at \\href{https://github.com/Zhangyr2022/D3QE}{https://github.com/Zhangyr2022/D3QE}.", "AI": {"tldr": "\u63d0\u51faD3QE\u65b9\u6cd5\uff0c\u5229\u7528\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u751f\u6210\u56fe\u50cf\u65f6\u5b58\u5728\u7684\u79bb\u6563\u5206\u5e03\u5dee\u5f02\u548c\u91cf\u5316\u8bef\u5dee\uff0c\u901a\u8fc7\u79bb\u6563\u5206\u5e03\u611f\u77e5\u53d8\u6362\u5668\u68c0\u6d4b\u81ea\u56de\u5f52\u751f\u6210\u7684\u4f2a\u9020\u56fe\u50cf\u3002", "motivation": "\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u901a\u8fc7\u79bb\u6563token\u9884\u6d4b\u751f\u6210\u56fe\u50cf\uff0c\u4e0e\u4f20\u7edfGAN\u6216\u6269\u6563\u6a21\u578b\u4e0d\u540c\uff0c\u5728\u56fe\u50cf\u5408\u6210\u8d28\u91cf\u548c\u5411\u91cf\u91cf\u5316\u8868\u793a\u65b9\u9762\u5177\u6709\u72ec\u7279\u7279\u5f81\uff0c\u8fd9\u4e3a\u4f2a\u9020\u56fe\u50cf\u68c0\u6d4b\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\u548c\u673a\u4f1a\u3002", "method": "\u63d0\u51fa\u79bb\u6563\u5206\u5e03\u5dee\u5f02\u611f\u77e5\u91cf\u5316\u8bef\u5dee(D3QE)\u65b9\u6cd5\uff0c\u5f15\u5165\u52a8\u6001\u7801\u672c\u9891\u7387\u7edf\u8ba1\u5230\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u53d8\u6362\u5668\uff0c\u878d\u5408\u8bed\u4e49\u7279\u5f81\u548c\u91cf\u5316\u8bef\u5dee\u6f5c\u5728\u8868\u793a\u3002", "result": "\u5728\u5305\u542b7\u79cd\u4e3b\u6d41\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u7684ARForensics\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cD3QE\u5177\u6709\u4f18\u8d8a\u7684\u68c0\u6d4b\u7cbe\u5ea6\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5bf9\u771f\u5b9e\u4e16\u754c\u6270\u52a8\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "D3QE\u65b9\u6cd5\u6709\u6548\u5229\u7528\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u751f\u6210\u56fe\u50cf\u7684\u72ec\u7279\u7279\u5f81\uff0c\u5728\u81ea\u56de\u5f52\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5e94\u5bf9\u65b0\u5174\u89c6\u89c9\u751f\u6210\u6280\u672f\u7684\u4f2a\u9020\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.05976", "pdf": "https://arxiv.org/pdf/2510.05976", "abs": "https://arxiv.org/abs/2510.05976", "authors": ["Eashan Adhikarla", "Yixin Liu", "Brian D. Davison"], "title": "Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Low-light image enhancement (LLIE) is vital for safety-critical applications such as surveillance, autonomous navigation, and medical imaging, where visibility degradation can impair downstream task performance. Recently, diffusion models have emerged as a promising generative paradigm for LLIE due to their capacity to model complex image distributions via iterative denoising. This survey provides an up-to-date critical analysis of diffusion models for LLIE, distinctively featuring an in-depth comparative performance evaluation against Generative Adversarial Network and Transformer-based state-of-the-art methods, a thorough examination of practical deployment challenges, and a forward-looking perspective on the role of emerging paradigms like foundation models. We propose a multi-perspective taxonomy encompassing six categories: Intrinsic Decomposition, Spectral & Latent, Accelerated, Guided, Multimodal, and Autonomous; that map enhancement methods across physical priors, conditioning schemes, and computational efficiency. Our taxonomy is grounded in a hybrid view of both the model mechanism and the conditioning signals. We evaluate qualitative failure modes, benchmark inconsistencies, and trade-offs between interpretability, generalization, and inference efficiency. We also discuss real-world deployment constraints (e.g., memory, energy use) and ethical considerations. This survey aims to guide the next generation of diffusion-based LLIE research by highlighting trends and surfacing open research questions, including novel conditioning, real-time adaptation, and the potential of foundation models.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u5bf9\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u591a\u89c6\u89d2\u5206\u7c7b\u6cd5\uff0c\u5e76\u4e0eGAN\u548cTransformer\u65b9\u6cd5\u8fdb\u884c\u4e86\u6027\u80fd\u6bd4\u8f83\uff0c\u540c\u65f6\u8ba8\u8bba\u4e86\u5b9e\u9645\u90e8\u7f72\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u6269\u6563\u6a21\u578b\u56e0\u5176\u901a\u8fc7\u8fed\u4ee3\u53bb\u566a\u5efa\u6a21\u590d\u6742\u56fe\u50cf\u5206\u5e03\u7684\u80fd\u529b\u800c\u6210\u4e3a\u6709\u524d\u666f\u7684LLIE\u65b9\u6cd5\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u5206\u6790\u548c\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4e86\u5305\u542b\u516d\u4e2a\u7c7b\u522b\u7684\u591a\u89c6\u89d2\u5206\u7c7b\u6cd5\uff1a\u5185\u5728\u5206\u89e3\u3001\u8c31\u4e0e\u6f5c\u5728\u3001\u52a0\u901f\u3001\u5f15\u5bfc\u3001\u591a\u6a21\u6001\u548c\u81ea\u4e3b\uff1b\u57fa\u4e8e\u6a21\u578b\u673a\u5236\u548c\u6761\u4ef6\u4fe1\u53f7\u7684\u6df7\u5408\u89c6\u89d2\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u8fdb\u884c\u5b9a\u6027\u5931\u8d25\u6a21\u5f0f\u5206\u6790\u548c\u57fa\u51c6\u6d4b\u8bd5\u6bd4\u8f83\u3002", "result": "\u8bc4\u4f30\u4e86\u6269\u6563\u6a21\u578b\u5728LLIE\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u4e86\u57fa\u51c6\u4e0d\u4e00\u81f4\u6027\uff0c\u5206\u6790\u4e86\u53ef\u89e3\u91ca\u6027\u3001\u6cdb\u5316\u6027\u548c\u63a8\u7406\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u8bc6\u522b\u4e86\u5b9e\u9645\u90e8\u7f72\u7ea6\u675f\u548c\u4f26\u7406\u8003\u8651\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u65e8\u5728\u901a\u8fc7\u7a81\u51fa\u8d8b\u52bf\u548c\u63d0\u51fa\u5f00\u653e\u7814\u7a76\u95ee\u9898\u6765\u6307\u5bfc\u4e0b\u4e00\u4ee3\u57fa\u4e8e\u6269\u6563\u7684LLIE\u7814\u7a76\uff0c\u5305\u62ec\u65b0\u9896\u6761\u4ef6\u3001\u5b9e\u65f6\u9002\u5e94\u548c\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.05977", "pdf": "https://arxiv.org/pdf/2510.05977", "abs": "https://arxiv.org/abs/2510.05977", "authors": ["Owen T. Huber", "Raghu G. Raj", "Tianyu Chen", "Zacharie I. Idriss"], "title": "A Dynamic Mode Decomposition Approach to Morphological Component Analysis", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "This paper introduces a novel methodology of adapting the representation of videos based on the dynamics of their scene content variation. In particular, we demonstrate how the clustering of dynamic mode decomposition eigenvalues can be leveraged to learn an adaptive video representation for separating structurally distinct morphologies of a video. We extend the morphological component analysis (MCA) algorithm, which uses multiple predefined incoherent dictionaries and a sparsity prior to separate distinct sources in signals, by introducing our novel eigenspace clustering technique to obtain data-driven MCA dictionaries, which we call dynamic morphological component analysis (DMCA). After deriving our novel algorithm, we offer a motivational example of DMCA applied to a still image, then demonstrate DMCA's effectiveness in denoising applications on videos from the Adobe 240fps dataset. Afterwards, we provide an example of DMCA enhancing the signal-to-noise ratio of a faint target summed with a sea state, and conclude the paper by applying DMCA to separate a bicycle from wind clutter in inverse synthetic aperture radar images.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u5f62\u6001\u6210\u5206\u5206\u6790(DMCA)\u65b9\u6cd5\uff0c\u901a\u8fc7\u7279\u5f81\u503c\u805a\u7c7b\u5b66\u4e60\u81ea\u9002\u5e94\u89c6\u9891\u8868\u793a\uff0c\u7528\u4e8e\u5206\u79bb\u89c6\u9891\u4e2d\u7ed3\u6784\u4e0d\u540c\u7684\u5f62\u6001\u6210\u5206", "motivation": "\u4f20\u7edf\u5f62\u6001\u6210\u5206\u5206\u6790(MCA)\u4f7f\u7528\u9884\u5b9a\u4e49\u7684\u4e0d\u76f8\u5e72\u5b57\u5178\uff0c\u9700\u8981\u4e3a\u4e0d\u540c\u89c6\u9891\u624b\u52a8\u8c03\u6574\u53c2\u6570\uff0c\u7f3a\u4e4f\u81ea\u9002\u5e94\u80fd\u529b", "method": "\u57fa\u4e8e\u52a8\u6001\u6a21\u5f0f\u5206\u89e3\u7279\u5f81\u503c\u805a\u7c7b\uff0c\u5b66\u4e60\u6570\u636e\u9a71\u52a8\u7684MCA\u5b57\u5178\uff0c\u6269\u5c55\u4f20\u7edfMCA\u7b97\u6cd5", "result": "\u5728Adobe 240fps\u6570\u636e\u96c6\u53bb\u566a\u3001\u6d77\u9762\u5fae\u5f31\u76ee\u6807\u4fe1\u566a\u6bd4\u589e\u5f3a\u3001\u9006\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u56fe\u50cf\u4e2d\u81ea\u884c\u8f66\u4e0e\u98ce\u6742\u6ce2\u5206\u79bb\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u6548", "conclusion": "DMCA\u80fd\u591f\u81ea\u9002\u5e94\u5730\u5206\u79bb\u89c6\u9891\u4e2d\u7684\u4e0d\u540c\u5f62\u6001\u6210\u5206\uff0c\u5728\u591a\u4e2a\u5e94\u7528\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u6027\u80fd"}}
{"id": "2510.05978", "pdf": "https://arxiv.org/pdf/2510.05978", "abs": "https://arxiv.org/abs/2510.05978", "authors": ["Yunyi Ni", "Finn Carter", "Ze Niu", "Emily Davis", "Bo Zhang"], "title": "Diffusion-Based Image Editing for Breaking Robust Watermarks", "categories": ["cs.CV"], "comment": "Preprint", "summary": "Robust invisible watermarking aims to embed hidden information into images such that the watermark can survive various image manipulations. However, the rise of powerful diffusion-based image generation and editing techniques poses a new threat to these watermarking schemes. In this paper, we present a theoretical study and method demonstrating that diffusion models can effectively break robust image watermarks that were designed to resist conventional perturbations. We show that a diffusion-driven ``image regeneration'' process can erase embedded watermarks while preserving perceptual image content. We further introduce a novel guided diffusion attack that explicitly targets the watermark signal during generation, significantly degrading watermark detectability. Theoretically, we prove that as an image undergoes sufficient diffusion-based transformation, the mutual information between the watermarked image and the embedded watermark payload vanishes, resulting in decoding failure. Experimentally, we evaluate our approach on multiple state-of-the-art watermarking schemes (including the deep learning-based methods StegaStamp, TrustMark, and VINE) and demonstrate near-zero watermark recovery rates after attack, while maintaining high visual fidelity of the regenerated images. Our findings highlight a fundamental vulnerability in current robust watermarking techniques against generative model-based attacks, underscoring the need for new watermarking strategies in the era of generative AI.", "AI": {"tldr": "\u6269\u6563\u6a21\u578b\u80fd\u591f\u6709\u6548\u7834\u574f\u539f\u672c\u8bbe\u8ba1\u7528\u4e8e\u62b5\u6297\u5e38\u89c4\u6270\u52a8\u7684\u9c81\u68d2\u56fe\u50cf\u6c34\u5370\u65b9\u6848\uff0c\u901a\u8fc7\u56fe\u50cf\u518d\u751f\u8fc7\u7a0b\u6d88\u9664\u6c34\u5370\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u5185\u5bb9\uff0c\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u6c34\u5370\u4e0e\u56fe\u50cf\u95f4\u7684\u4e92\u4fe1\u606f\u4f1a\u6d88\u5931\u3002", "motivation": "\u5f3a\u5927\u7684\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u6280\u672f\u5bf9\u73b0\u6709\u7684\u9c81\u68d2\u6c34\u5370\u65b9\u6848\u6784\u6210\u4e86\u65b0\u7684\u5a01\u80c1\uff0c\u9700\u8981\u7814\u7a76\u6269\u6563\u6a21\u578b\u5982\u4f55\u7834\u574f\u8fd9\u4e9b\u6c34\u5370\u3002", "method": "\u63d0\u51fa\u4e86\u6269\u6563\u9a71\u52a8\u7684\u56fe\u50cf\u518d\u751f\u8fc7\u7a0b\uff0c\u4ee5\u53ca\u4e00\u79cd\u65b0\u9896\u7684\u5f15\u5bfc\u6269\u6563\u653b\u51fb\uff0c\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u660e\u786e\u9488\u5bf9\u6c34\u5370\u4fe1\u53f7\uff0c\u663e\u8457\u964d\u4f4e\u6c34\u5370\u53ef\u68c0\u6d4b\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u6c34\u5370\u65b9\u6848\u4e0a\u8bc4\u4f30\uff0c\u653b\u51fb\u540e\u6c34\u5370\u6062\u590d\u7387\u63a5\u8fd1\u96f6\uff0c\u540c\u65f6\u518d\u751f\u56fe\u50cf\u4fdd\u6301\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u5f53\u524d\u9c81\u68d2\u6c34\u5370\u6280\u672f\u5728\u9762\u5bf9\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u653b\u51fb\u65f6\u5b58\u5728\u6839\u672c\u6027\u6f0f\u6d1e\uff0c\u5728\u751f\u6210AI\u65f6\u4ee3\u9700\u8981\u65b0\u7684\u6c34\u5370\u7b56\u7565\u3002"}}
{"id": "2510.06090", "pdf": "https://arxiv.org/pdf/2510.06090", "abs": "https://arxiv.org/abs/2510.06090", "authors": ["Bjoern Hansen", "Jonas Pedersen", "Klaus F. Kofoed", "Oscar Camara", "Rasmus R. Paulsen", "Kristine Soerensen"], "title": "A public cardiac CT dataset featuring the left atrial appendage", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 5 figures, published at STACOM2025", "summary": "Despite the success of advanced segmentation frameworks such as TotalSegmentator (TS), accurate segmentations of the left atrial appendage (LAA), coronary arteries (CAs), and pulmonary veins (PVs) remain a significant challenge in medical imaging. In this work, we present the first open-source, anatomically coherent dataset of curated, high-resolution segmentations for these structures, supplemented with whole-heart labels produced by TS on the publicly available ImageCAS dataset consisting of 1000 cardiac computed tomography angiography (CCTA) scans. One purpose of the data set is to foster novel approaches to the analysis of LAA morphology.   LAA segmentations on ImageCAS were generated using a state-of-the-art segmentation framework developed specifically for high resolution LAA segmentation. We trained the network on a large private dataset with manual annotations provided by medical readers guided by a trained cardiologist and transferred the model to ImageCAS data. CA labels were improved from the original ImageCAS annotations, while PV segmentations were refined from TS outputs. In addition, we provide a list of scans from ImageCAS that contains common data flaws such as step artefacts, LAAs extending beyond the scanner's field of view, and other types of data defects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u5f00\u6e90\u3001\u89e3\u5256\u5b66\u4e00\u81f4\u7684\u9ad8\u5206\u8fa8\u7387\u5de6\u5fc3\u8033\u3001\u51a0\u72b6\u52a8\u8109\u548c\u80ba\u9759\u8109\u5206\u5272\u6570\u636e\u96c6\uff0c\u57fa\u4e8eImageCAS\u76841000\u4e2a\u5fc3\u810fCTA\u626b\u63cf\uff0c\u65e8\u5728\u4fc3\u8fdb\u5de6\u5fc3\u8033\u5f62\u6001\u5206\u6790\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u6709TotalSegmentator\u7b49\u5148\u8fdb\u5206\u5272\u6846\u67b6\uff0c\u4f46\u5de6\u5fc3\u8033\u3001\u51a0\u72b6\u52a8\u8109\u548c\u80ba\u9759\u8109\u7684\u51c6\u786e\u5206\u5272\u5728\u533b\u5b66\u6210\u50cf\u4e2d\u4ecd\u5177\u6311\u6218\u6027\uff0c\u9700\u8981\u9ad8\u8d28\u91cf\u7684\u5206\u5272\u6570\u636e\u96c6\u6765\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u4e13\u95e8\u4e3a\u9ad8\u5206\u8fa8\u7387\u5de6\u5fc3\u8033\u5206\u5272\u5f00\u53d1\u7684\u6700\u5148\u8fdb\u5206\u5272\u6846\u67b6\uff0c\u5728\u5927\u578b\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7f51\u7edc\uff0c\u7136\u540e\u5c06\u6a21\u578b\u8fc1\u79fb\u5230ImageCAS\u6570\u636e\uff1b\u51a0\u72b6\u52a8\u8109\u6807\u7b7e\u4ece\u539f\u59cbImageCAS\u6ce8\u91ca\u6539\u8fdb\uff0c\u80ba\u9759\u8109\u5206\u5272\u4eceTS\u8f93\u51fa\u7ec6\u5316\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b1000\u4e2a\u5fc3\u810fCTA\u626b\u63cf\u7684\u89e3\u5256\u5b66\u4e00\u81f4\u5206\u5272\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u4e86\u5de6\u5fc3\u8033\u3001\u51a0\u72b6\u52a8\u8109\u548c\u80ba\u9759\u8109\u7684\u9ad8\u8d28\u91cf\u5206\u5272\u6807\u7b7e\uff0c\u5e76\u8bc6\u522b\u4e86\u5305\u542b\u5e38\u89c1\u6570\u636e\u7f3a\u9677\u7684\u626b\u63cf\u5217\u8868\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u586b\u8865\u4e86\u5de6\u5fc3\u8033\u3001\u51a0\u72b6\u52a8\u8109\u548c\u80ba\u9759\u8109\u9ad8\u8d28\u91cf\u5206\u5272\u6570\u636e\u7684\u7a7a\u767d\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u7279\u522b\u6709\u52a9\u4e8e\u5de6\u5fc3\u8033\u5f62\u6001\u5206\u6790\u7684\u65b0\u65b9\u6cd5\u5f00\u53d1\u3002"}}
{"id": "2510.06098", "pdf": "https://arxiv.org/pdf/2510.06098", "abs": "https://arxiv.org/abs/2510.06098", "authors": ["Yinjian Wang", "Wei Li", "Yuanyuan Gui", "Gemine Vivone"], "title": "Compact Multi-level-prior Tensor Representation for Hyperspectral Image Super-resolution", "categories": ["cs.CV"], "comment": null, "summary": "Fusing a hyperspectral image with a multispectral image acquired over the same scene, \\textit{i.e.}, hyperspectral image super-resolution, has become a popular computational way to access the latent high-spatial-spectral-resolution image. To date, a variety of fusion methods have been proposed, among which the tensor-based ones have testified that multiple priors, such as multidimensional low-rankness and spatial total variation at multiple levels, effectively drive the fusion process. However, existing tensor-based models can only effectively leverage one or two priors at one or two levels, since simultaneously incorporating multi-level priors inevitably increases model complexity. This introduces challenges in both balancing the weights of different priors and optimizing multi-block structures. Concerning this, we present a novel hyperspectral super-resolution model compactly characterizing these multi-level priors of hyperspectral images within the tensor framework. Firstly, the proposed model decouples the spectral low-rankness and spatial priors by casting the latent high-spatial-spectral-resolution image into spectral subspace and spatial maps via block term decomposition. Secondly, these spatial maps are stacked as the spatial tensor encoding the high-order spatial low-rankness and smoothness priors, which are co-modeled via the proposed non-convex mode-shuffled tensor correlated total variation. Finally, we draw inspiration from the linearized alternating direction method of multipliers to design an efficient algorithm to optimize the resulting model, theoretically proving its Karush-Kuhn-Tucker convergence under mild conditions. Experiments on multiple datasets demonstrate the effectiveness of the proposed algorithm. The code implementation will be available from https://github.com/WongYinJ.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6a21\u578b\uff0c\u901a\u8fc7\u5757\u9879\u5206\u89e3\u5c06\u5149\u8c31\u4f4e\u79e9\u6027\u548c\u7a7a\u95f4\u5148\u9a8c\u89e3\u8026\uff0c\u5e76\u5f15\u5165\u975e\u51f8\u6a21\u5f0f\u6df7\u6d17\u5f20\u91cf\u76f8\u5173\u603b\u53d8\u5206\u6765\u5171\u540c\u5efa\u6a21\u9ad8\u9636\u7a7a\u95f4\u4f4e\u79e9\u6027\u548c\u5e73\u6ed1\u6027\u5148\u9a8c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f20\u91cf\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u53ea\u80fd\u6709\u6548\u5229\u7528\u4e00\u4e2a\u6216\u4e24\u4e2a\u7ea7\u522b\u7684\u5148\u9a8c\uff0c\u540c\u65f6\u878d\u5165\u591a\u7ea7\u5148\u9a8c\u4f1a\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u96be\u4ee5\u5e73\u8861\u4e0d\u540c\u5148\u9a8c\u7684\u6743\u91cd\u548c\u4f18\u5316\u591a\u5757\u7ed3\u6784\u3002", "method": "\u4f7f\u7528\u5757\u9879\u5206\u89e3\u5c06\u6f5c\u5728\u9ad8\u7a7a\u95f4-\u5149\u8c31\u5206\u8fa8\u7387\u56fe\u50cf\u5206\u89e3\u4e3a\u5149\u8c31\u5b50\u7a7a\u95f4\u548c\u7a7a\u95f4\u6620\u5c04\uff0c\u5c06\u7a7a\u95f4\u6620\u5c04\u5806\u53e0\u4e3a\u7a7a\u95f4\u5f20\u91cf\uff0c\u901a\u8fc7\u975e\u51f8\u6a21\u5f0f\u6df7\u6d17\u5f20\u91cf\u76f8\u5173\u603b\u53d8\u5206\u5171\u540c\u5efa\u6a21\u9ad8\u9636\u7a7a\u95f4\u4f4e\u79e9\u6027\u548c\u5e73\u6ed1\u6027\u5148\u9a8c\uff0c\u5e76\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u7ebf\u6027\u5316\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5\u7684\u9ad8\u6548\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u7d27\u51d1\u5730\u523b\u753b\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\u7684\u591a\u7ea7\u5148\u9a8c\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u9ad8\u5149\u8c31\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u6536\u655b\u4fdd\u8bc1\u3002"}}
{"id": "2510.06131", "pdf": "https://arxiv.org/pdf/2510.06131", "abs": "https://arxiv.org/abs/2510.06131", "authors": ["Jiawei Mao", "Yuhan Wang", "Lifeng Chen", "Can Zhao", "Yucheng Tang", "Dong Yang", "Liangqiong Qu", "Daguang Xu", "Yuyin Zhou"], "title": "Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages,6 figures", "summary": "Recent advances in generative medical models are constrained by modality-specific scenarios that hinder the integration of complementary evidence from imaging, pathology, and clinical notes. This fragmentation limits their evolution into foundation models that can learn and reason across the full spectrum of biomedical data. We propose MeDiM, the first medical discrete diffusion model that learns shared distributions across modalities without modality-specific components. MeDiM unifies multiple generative tasks: translating between images and text, and jointly producing image-report pairs across domains in response to prompts. Built on a discrete diffusion framework, MeDiM bridges vision and language representations through a shared probabilistic space. To enable unified and flexible medical generation, we employ a multimodal large language model (MLLM) as the diffusion backbone, leveraging its prior knowledge and cross-modal reasoning. Two key designs are introduced: (1) removing the causal attention mask for bidirectional context, and (2) injecting continuous timestep embeddings for diffusion awareness. Experiments demonstrate high-fidelity medical generation (FID 16.60 on MIMIC-CXR and FID 24.19 on PathGen) and accurate report generation (METEOR 0.2650 and 0.2580). Jointly generated image-report pairs further enhance downstream performance (plus6.43 percent BLEU-1, plus18.57 percent BLEU-2, plus31.58 percent BLEU-3, plus4.80 percent METEOR), showing that MeDiM supports coherent and clinically grounded multimodal outputs.", "AI": {"tldr": "MeDiM\u662f\u9996\u4e2a\u533b\u5b66\u79bb\u6563\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5171\u4eab\u6982\u7387\u7a7a\u95f4\u7edf\u4e00\u591a\u6a21\u6001\u751f\u6210\u4efb\u52a1\uff0c\u65e0\u9700\u7279\u5b9a\u6a21\u6001\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u4e86\u8de8\u6a21\u6001\u7684\u9ad8\u8d28\u91cf\u533b\u5b66\u56fe\u50cf\u548c\u62a5\u544a\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u533b\u5b66\u6a21\u578b\u53d7\u9650\u4e8e\u7279\u5b9a\u6a21\u6001\u573a\u666f\uff0c\u65e0\u6cd5\u6574\u5408\u5f71\u50cf\u3001\u75c5\u7406\u548c\u4e34\u5e8a\u8bb0\u5f55\u7b49\u4e92\u8865\u8bc1\u636e\uff0c\u963b\u788d\u4e86\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u57fa\u4e8e\u79bb\u6563\u6269\u6563\u6846\u67b6\uff0c\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u6269\u6563\u4e3b\u5e72\uff0c\u79fb\u9664\u56e0\u679c\u6ce8\u610f\u529b\u63a9\u7801\u5b9e\u73b0\u53cc\u5411\u4e0a\u4e0b\u6587\uff0c\u6ce8\u5165\u8fde\u7eed\u65f6\u95f4\u6b65\u5d4c\u5165\u5b9e\u73b0\u6269\u6563\u611f\u77e5\u3002", "result": "\u5728MIMIC-CXR\u4e0aFID\u4e3a16.60\uff0cPathGen\u4e0aFID\u4e3a24.19\uff1b\u62a5\u544a\u751f\u6210METEOR\u5206\u522b\u4e3a0.2650\u548c0.2580\uff1b\u8054\u5408\u751f\u6210\u7684\u56fe\u50cf-\u62a5\u544a\u5bf9\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u6027\u80fd\u3002", "conclusion": "MeDiM\u652f\u6301\u8fde\u8d2f\u4e14\u4e34\u5e8a\u57fa\u7840\u7684\u591a\u6a21\u6001\u8f93\u51fa\uff0c\u4e3a\u8de8\u6a21\u6001\u533b\u5b66\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.06139", "pdf": "https://arxiv.org/pdf/2510.06139", "abs": "https://arxiv.org/abs/2510.06139", "authors": ["Zanyi Wang", "Dengyang Jiang", "Liuzhuozheng Li", "Sizhe Dang", "Chengzu Li", "Harry Yang", "Guang Dai", "Mengmeng Wang", "Jingdong Wang"], "title": "Deforming Videos to Masks: Flow Matching for Referring Video Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Referring Video Object Segmentation (RVOS) requires segmenting specific objects in a video guided by a natural language description. The core challenge of RVOS is to anchor abstract linguistic concepts onto a specific set of pixels and continuously segment them through the complex dynamics of a video. Faced with this difficulty, prior work has often decomposed the task into a pragmatic `locate-then-segment' pipeline. However, this cascaded design creates an information bottleneck by simplifying semantics into coarse geometric prompts (e.g, point), and struggles to maintain temporal consistency as the segmenting process is often decoupled from the initial language grounding. To overcome these fundamental limitations, we propose FlowRVS, a novel framework that reconceptualizes RVOS as a conditional continuous flow problem. This allows us to harness the inherent strengths of pretrained T2V models, fine-grained pixel control, text-video semantic alignment, and temporal coherence. Instead of conventional generating from noise to mask or directly predicting mask, we reformulate the task by learning a direct, language-guided deformation from a video's holistic representation to its target mask. Our one-stage, generative approach achieves new state-of-the-art results across all major RVOS benchmarks. Specifically, achieving a $\\mathcal{J}\\&\\mathcal{F}$ of 51.1 in MeViS (+1.6 over prior SOTA) and 73.3 in the zero shot Ref-DAVIS17 (+2.7), demonstrating the significant potential of modeling video understanding tasks as continuous deformation processes.", "AI": {"tldr": "FlowRVS\u5c06\u53c2\u8003\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6761\u4ef6\u8fde\u7eed\u6d41\u95ee\u9898\uff0c\u5229\u7528\u9884\u8bad\u7ec3T2V\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u8bed\u8a00\u5f15\u5bfc\u7684\u53d8\u5f62\u4ece\u89c6\u9891\u6574\u4f53\u8868\u793a\u76f4\u63a5\u751f\u6210\u76ee\u6807\u63a9\u7801\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf'\u5b9a\u4f4d-\u5206\u5272'\u6d41\u6c34\u7ebf\u5b58\u5728\u7684\u4fe1\u606f\u74f6\u9888\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5c06\u8bed\u4e49\u7b80\u5316\u4e3a\u7c97\u7c92\u5ea6\u51e0\u4f55\u63d0\u793a\u4e14\u5206\u5272\u8fc7\u7a0b\u4e0e\u521d\u59cb\u8bed\u8a00\u5b9a\u4f4d\u89e3\u8026\u3002", "method": "\u63d0\u51faFlowRVS\u6846\u67b6\uff0c\u5c06RVOS\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6761\u4ef6\u8fde\u7eed\u6d41\u95ee\u9898\uff0c\u5b66\u4e60\u4ece\u89c6\u9891\u6574\u4f53\u8868\u793a\u5230\u76ee\u6807\u63a9\u7801\u7684\u8bed\u8a00\u5f15\u5bfc\u76f4\u63a5\u53d8\u5f62\uff0c\u91c7\u7528\u5355\u9636\u6bb5\u751f\u6210\u65b9\u6cd5\u3002", "result": "\u5728\u6240\u6709\u4e3b\u8981RVOS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c\uff1aMeViS\u4e0aJ&F\u4e3a51.1\uff08\u6bd4\u4e4b\u524dSOTA\u63d0\u9ad81.6\uff09\uff0c\u96f6\u6837\u672cRef-DAVIS17\u4e0a\u4e3a73.3\uff08\u63d0\u9ad82.7\uff09\u3002", "conclusion": "\u5c06\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u5efa\u6a21\u4e3a\u8fde\u7eed\u53d8\u5f62\u8fc7\u7a0b\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0cFlowRVS\u5c55\u793a\u4e86\u8fd9\u79cd\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.06145", "pdf": "https://arxiv.org/pdf/2510.06145", "abs": "https://arxiv.org/abs/2510.06145", "authors": ["Aditya Prakash", "David Forsyth", "Saurabh Gupta"], "title": "Bimanual 3D Hand Motion and Articulation Forecasting in Everyday Images", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project page: https://ap229997.github.io/projects/forehand4d", "summary": "We tackle the problem of forecasting bimanual 3D hand motion & articulation from a single image in everyday settings. To address the lack of 3D hand annotations in diverse settings, we design an annotation pipeline consisting of a diffusion model to lift 2D hand keypoint sequences to 4D hand motion. For the forecasting model, we adopt a diffusion loss to account for the multimodality in hand motion distribution. Extensive experiments across 6 datasets show the benefits of training on diverse data with imputed labels (14% improvement) and effectiveness of our lifting (42% better) & forecasting (16.4% gain) models, over the best baselines, especially in zero-shot generalization to everyday images.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5355\u5f20\u56fe\u50cf\u9884\u6d4b\u65e5\u5e38\u573a\u666f\u4e2d\u53cc\u624b3D\u8fd0\u52a8\u4e0e\u5173\u8282\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5c062D\u624b\u90e8\u5173\u952e\u70b9\u5e8f\u5217\u63d0\u5347\u4e3a4D\u624b\u90e8\u8fd0\u52a8\uff0c\u5e76\u4f7f\u7528\u6269\u6563\u635f\u5931\u5904\u7406\u624b\u90e8\u8fd0\u52a8\u5206\u5e03\u7684\u591a\u6a21\u6001\u6027\u3002", "motivation": "\u89e3\u51b3\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u7f3a\u4e4f3D\u624b\u90e8\u6807\u6ce8\u6570\u636e\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4ece\u5355\u5f20\u56fe\u50cf\u51c6\u786e\u9884\u6d4b\u53cc\u624b3D\u8fd0\u52a8\u4e0e\u5173\u8282\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6807\u6ce8\u6d41\u7a0b\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u5c062D\u624b\u90e8\u5173\u952e\u70b9\u5e8f\u5217\u63d0\u5347\u4e3a4D\u624b\u90e8\u8fd0\u52a8\uff1b\u9884\u6d4b\u6a21\u578b\u91c7\u7528\u6269\u6563\u635f\u5931\u6765\u5904\u7406\u624b\u90e8\u8fd0\u52a8\u5206\u5e03\u7684\u591a\u6a21\u6001\u6027\u3002", "result": "\u57286\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u663e\u793a\uff0c\u4f7f\u7528\u63a8\u7b97\u6807\u7b7e\u7684\u591a\u6837\u5316\u6570\u636e\u8bad\u7ec3\u5e26\u6765\u4e8614%\u7684\u6539\u8fdb\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u63d0\u534742%\uff0c\u9884\u6d4b\u6a21\u578b\u6027\u80fd\u63d0\u534716.4%\uff0c\u7279\u522b\u662f\u5728\u65e5\u5e38\u56fe\u50cf\u7684\u96f6\u6837\u672c\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u53cc\u624b3D\u8fd0\u52a8\u9884\u6d4b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u96f6\u6837\u672c\u6cdb\u5316\u5230\u65e5\u5e38\u56fe\u50cf\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.06209", "pdf": "https://arxiv.org/pdf/2510.06209", "abs": "https://arxiv.org/abs/2510.06209", "authors": ["Jiahao Wang", "Zhenpei Yang", "Yijing Bai", "Yingwei Li", "Yuliang Zou", "Bo Sun", "Abhijit Kundu", "Jose Lezama", "Luna Yue Huang", "Zehao Zhu", "Jyh-Jing Hwang", "Dragomir Anguelov", "Mingxing Tan", "Chiyu Max Jiang"], "title": "Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models", "categories": ["cs.CV"], "comment": "Accepted by IROS 2025", "summary": "Recent advances in generative models have sparked exciting new possibilities in the field of autonomous vehicles. Specifically, video generation models are now being explored as controllable virtual testing environments. Simultaneously, end-to-end (E2E) driving models have emerged as a streamlined alternative to conventional modular autonomous driving systems, gaining popularity for their simplicity and scalability. However, the application of these techniques to simulation and planning raises important questions. First, while video generation models can generate increasingly realistic videos, can these videos faithfully adhere to the specified conditions and be realistic enough for E2E autonomous planner evaluation? Second, given that data is crucial for understanding and controlling E2E planners, how can we gain deeper insights into their biases and improve their ability to generalize to out-of-distribution scenarios? In this work, we bridge the gap between the driving models and generative world models (Drive&Gen) to address these questions. We propose novel statistical measures leveraging E2E drivers to evaluate the realism of generated videos. By exploiting the controllability of the video generation model, we conduct targeted experiments to investigate distribution gaps affecting E2E planner performance. Finally, we show that synthetic data produced by the video generation model offers a cost-effective alternative to real-world data collection. This synthetic data effectively improves E2E model generalization beyond existing Operational Design Domains, facilitating the expansion of autonomous vehicle services into new operational contexts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faDrive&Gen\u6846\u67b6\uff0c\u5c06\u7aef\u5230\u7aef\u9a7e\u9a76\u6a21\u578b\u4e0e\u751f\u6210\u5f0f\u4e16\u754c\u6a21\u578b\u7ed3\u5408\uff0c\u7528\u4e8e\u8bc4\u4f30\u751f\u6210\u89c6\u9891\u7684\u771f\u5b9e\u6027\u3001\u5206\u6790\u9a7e\u9a76\u6a21\u578b\u6027\u80fd\u5dee\u8ddd\uff0c\u5e76\u5229\u7528\u5408\u6210\u6570\u636e\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u751f\u6210\u89c6\u9891\u80fd\u5426\u771f\u5b9e\u53cd\u6620\u6307\u5b9a\u6761\u4ef6\u4ee5\u8bc4\u4f30\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u5668\uff0c\u4ee5\u53ca\u5982\u4f55\u6df1\u5165\u7406\u89e3\u7aef\u5230\u7aef\u89c4\u5212\u5668\u7684\u504f\u89c1\u5e76\u63d0\u5347\u5176\u5728\u5206\u5e03\u5916\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7aef\u5230\u7aef\u9a7e\u9a76\u6a21\u578b\u7684\u7edf\u8ba1\u5ea6\u91cf\u6765\u8bc4\u4f30\u751f\u6210\u89c6\u9891\u7684\u771f\u5b9e\u6027\uff1b\u5229\u7528\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u53ef\u63a7\u6027\u8fdb\u884c\u9488\u5bf9\u6027\u5b9e\u9a8c\uff1b\u4f7f\u7528\u5408\u6210\u6570\u636e\u6539\u8fdb\u7aef\u5230\u7aef\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u751f\u6210\u89c6\u9891\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u7aef\u5230\u7aef\u89c4\u5212\u5668\uff1b\u5408\u6210\u6570\u636e\u53ef\u4f5c\u4e3a\u771f\u5b9e\u6570\u636e\u6536\u96c6\u7684\u4f4e\u6210\u672c\u66ff\u4ee3\u65b9\u6848\uff0c\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u8d85\u51fa\u8bbe\u8ba1\u57df\u8303\u56f4\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "Drive&Gen\u6846\u67b6\u6210\u529f\u5f25\u5408\u4e86\u9a7e\u9a76\u6a21\u578b\u4e0e\u751f\u6210\u5f0f\u4e16\u754c\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u8bc4\u4f30\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u4fc3\u8fdb\u4e86\u81ea\u52a8\u9a7e\u9a76\u670d\u52a1\u5411\u65b0\u8fd0\u8425\u73af\u5883\u7684\u6269\u5c55\u3002"}}
{"id": "2510.05173", "pdf": "https://arxiv.org/pdf/2510.05173", "abs": "https://arxiv.org/abs/2510.05173", "authors": ["Peigui Qi", "Kunsheng Tang", "Wenbo Zhou", "Weiming Zhang", "Nenghai Yu", "Tianwei Zhang", "Qing Guo", "Jie Zhang"], "title": "SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models", "categories": ["cs.CR", "cs.AI", "cs.CV", "I.2"], "comment": "Accepted by ACM CCS 2025", "summary": "Text-to-image models have shown remarkable capabilities in generating high-quality images from natural language descriptions. However, these models are highly vulnerable to adversarial prompts, which can bypass safety measures and produce harmful content. Despite various defensive strategies, achieving robustness against attacks while maintaining practical utility in real-world applications remains a significant challenge. To address this issue, we first conduct an empirical study of the text encoder in the Stable Diffusion (SD) model, which is a widely used and representative text-to-image model. Our findings reveal that the [EOS] token acts as a semantic aggregator, exhibiting distinct distributional patterns between benign and adversarial prompts in its embedding space. Building on this insight, we introduce \\textbf{SafeGuider}, a two-step framework designed for robust safety control without compromising generation quality. SafeGuider combines an embedding-level recognition model with a safety-aware feature erasure beam search algorithm. This integration enables the framework to maintain high-quality image generation for benign prompts while ensuring robust defense against both in-domain and out-of-domain attacks. SafeGuider demonstrates exceptional effectiveness in minimizing attack success rates, achieving a maximum rate of only 5.48\\% across various attack scenarios. Moreover, instead of refusing to generate or producing black images for unsafe prompts, \\textbf{SafeGuider} generates safe and meaningful images, enhancing its practical utility. In addition, SafeGuider is not limited to the SD model and can be effectively applied to other text-to-image models, such as the Flux model, demonstrating its versatility and adaptability across different architectures. We hope that SafeGuider can shed some light on the practical deployment of secure text-to-image systems.", "AI": {"tldr": "SafeGuider\u662f\u4e00\u4e2a\u9488\u5bf9\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5b89\u5168\u9632\u62a4\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790[EOS]\u4ee4\u724c\u7684\u8bed\u4e49\u805a\u5408\u7279\u6027\uff0c\u7ed3\u5408\u5d4c\u5165\u7ea7\u8bc6\u522b\u6a21\u578b\u548c\u5b89\u5168\u611f\u77e5\u7279\u5f81\u64e6\u9664\u6ce2\u675f\u641c\u7d22\u7b97\u6cd5\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u7684\u540c\u65f6\u6709\u6548\u9632\u5fa1\u5bf9\u6297\u6027\u63d0\u793a\u653b\u51fb\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u63d0\u793a\u653b\u51fb\uff0c\u7ed5\u8fc7\u5b89\u5168\u63aa\u65bd\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u3002\u73b0\u6709\u9632\u5fa1\u7b56\u7565\u5728\u4fdd\u6301\u5b9e\u7528\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u9632\u5fa1\u653b\u51fb\u53c8\u4e0d\u5f71\u54cd\u826f\u6027\u63d0\u793a\u751f\u6210\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u9996\u5148\u5bf9Stable Diffusion\u6a21\u578b\u7684\u6587\u672c\u7f16\u7801\u5668\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u53d1\u73b0[EOS]\u4ee4\u724c\u5728\u826f\u6027\u63d0\u793a\u548c\u5bf9\u6297\u6027\u63d0\u793a\u7684\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5177\u6709\u4e0d\u540c\u7684\u5206\u5e03\u6a21\u5f0f\u3002\u57fa\u4e8e\u6b64\u5f00\u53d1SafeGuider\u6846\u67b6\uff0c\u5305\u542b\u5d4c\u5165\u7ea7\u8bc6\u522b\u6a21\u578b\u548c\u5b89\u5168\u611f\u77e5\u7279\u5f81\u64e6\u9664\u6ce2\u675f\u641c\u7d22\u7b97\u6cd5\u3002", "result": "SafeGuider\u5728\u5404\u79cd\u653b\u51fb\u573a\u666f\u4e0b\u7684\u6700\u5927\u653b\u51fb\u6210\u529f\u7387\u4ec5\u4e3a5.48%\uff0c\u80fd\u6709\u6548\u9632\u5fa1\u57df\u5185\u548c\u57df\u5916\u653b\u51fb\u3002\u5bf9\u4e8e\u4e0d\u5b89\u5168\u63d0\u793a\uff0c\u4e0d\u662f\u62d2\u7edd\u751f\u6210\u6216\u751f\u6210\u9ed1\u56fe\uff0c\u800c\u662f\u751f\u6210\u5b89\u5168\u4e14\u6709\u610f\u4e49\u7684\u56fe\u50cf\uff0c\u589e\u5f3a\u4e86\u5b9e\u7528\u6027\u3002\u8be5\u6846\u67b6\u8fd8\u53ef\u5e94\u7528\u4e8e\u5176\u4ed6\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5982Flux\u3002", "conclusion": "SafeGuider\u4e3a\u5b89\u5168\u6587\u672c\u5230\u56fe\u50cf\u7cfb\u7edf\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u5b89\u5168\u9632\u62a4\uff0c\u5177\u6709\u5f88\u597d\u7684\u901a\u7528\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2510.06060", "pdf": "https://arxiv.org/pdf/2510.06060", "abs": "https://arxiv.org/abs/2510.06060", "authors": ["Christian Marinoni", "Riccardo Fosco Gramaccioni", "Eleonora Grassucci", "Danilo Comminiello"], "title": "Controllable Audio-Visual Viewpoint Generation from 360\u00b0 Spatial Information", "categories": ["cs.MM", "cs.AI", "cs.CV"], "comment": null, "summary": "The generation of sounding videos has seen significant advancements with the advent of diffusion models. However, existing methods often lack the fine-grained control needed to generate viewpoint-specific content from larger, immersive 360-degree environments. This limitation restricts the creation of audio-visual experiences that are aware of off-camera events. To the best of our knowledge, this is the first work to introduce a framework for controllable audio-visual generation, addressing this unexplored gap. Specifically, we propose a diffusion model by introducing a set of powerful conditioning signals derived from the full 360-degree space: a panoramic saliency map to identify regions of interest, a bounding-box-aware signed distance map to define the target viewpoint, and a descriptive caption of the entire scene. By integrating these controls, our model generates spatially-aware viewpoint videos and audios that are coherently influenced by the broader, unseen environmental context, introducing a strong controllability that is essential for realistic and immersive audio-visual generation. We show audiovisual examples proving the effectiveness of our framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u53ef\u63a7\u97f3\u9891-\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u666f\u663e\u8457\u6027\u56fe\u3001\u8fb9\u754c\u6846\u611f\u77e5\u8ddd\u79bb\u56fe\u548c\u573a\u666f\u63cf\u8ff0\u7b49\u6761\u4ef6\u4fe1\u53f7\uff0c\u4ece360\u5ea6\u73af\u5883\u4e2d\u751f\u6210\u89c6\u89d2\u7279\u5b9a\u7684\u89c6\u9891\u548c\u97f3\u9891\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u4ece\u5927\u578b\u6c89\u6d78\u5f0f360\u5ea6\u73af\u5883\u751f\u6210\u89c6\u89d2\u7279\u5b9a\u5185\u5bb9\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\u80fd\u529b\uff0c\u9650\u5236\u4e86\u521b\u5efa\u80fd\u591f\u611f\u77e5\u955c\u5934\u5916\u4e8b\u4ef6\u7684\u97f3\u9891-\u89c6\u89c9\u4f53\u9a8c\u3002", "method": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\uff0c\u5f15\u5165\u5168\u666f\u663e\u8457\u6027\u56fe\u8bc6\u522b\u611f\u5174\u8da3\u533a\u57df\u3001\u8fb9\u754c\u6846\u611f\u77e5\u6709\u7b26\u53f7\u8ddd\u79bb\u56fe\u5b9a\u4e49\u76ee\u6807\u89c6\u89d2\u3001\u4ee5\u53ca\u6574\u4e2a\u573a\u666f\u7684\u63cf\u8ff0\u6027\u6807\u9898\u4f5c\u4e3a\u6761\u4ef6\u4fe1\u53f7\u3002", "result": "\u6a21\u578b\u80fd\u591f\u751f\u6210\u7a7a\u95f4\u611f\u77e5\u7684\u89c6\u89d2\u89c6\u9891\u548c\u97f3\u9891\uff0c\u8fd9\u4e9b\u5185\u5bb9\u4e0e\u66f4\u5e7f\u6cdb\u7684\u3001\u4e0d\u53ef\u89c1\u7684\u73af\u5883\u4e0a\u4e0b\u6587\u8fde\u8d2f\u5730\u76f8\u4e92\u5f71\u54cd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u903c\u771f\u548c\u6c89\u6d78\u5f0f\u7684\u97f3\u9891-\u89c6\u89c9\u751f\u6210\u5f15\u5165\u4e86\u5173\u952e\u7684\u53ef\u63a7\u6027\uff0c\u901a\u8fc7\u97f3\u9891-\u89c6\u89c9\u793a\u4f8b\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
