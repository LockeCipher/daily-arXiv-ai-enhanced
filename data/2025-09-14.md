<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 18]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [CameraVDP: Perceptual Display Assessment with Uncertainty Estimation via Camera and Visual Difference Prediction](https://arxiv.org/abs/2509.08947)
*Yancheng Cai,Robert Wanat,Rafal Mantiuk*

Main category: cs.GR

TL;DR: 相机+VDP框架用于显示器评测，结合了高精度重建和视觉差异预测，能够评估屏幕缺陷和非均匀性


<details>
  <summary>Details</summary>
Motivation: 传统显示器测量方法无法捕捉空间变化差异，而相机测量又引入了光学和光度异常，需要结合视觉系统模型进行感知评估

Method: 提出CameraVDP框架，结合HDR堆栈、MTF逆变换、暗角校正、几何去异变、同胚变换和颜色校正的重建流水线，以及Visual Difference Predictor视觉差异预测器

Result: 通过缺陷像素检测、颜色缘故识别和显示非均匀性评估三个应用验证框架，不确定性分析提供了理论性能上限和质量分数的置信区间

Conclusion: CameraVDP框架能够使普通相机作为精确的显示器测量仪器，通过结合重建技术和视觉模型，实现了对显示器缺陷的感知评估

Abstract: Accurate measurement of images produced by electronic displays is critical for the evaluation of both traditional and computational displays. Traditional display measurement methods based on sparse radiometric sampling and fitting a model are inadequate for capturing spatially varying display artifacts, as they fail to capture high-frequency and pixel-level distortions. While cameras offer sufficient spatial resolution, they introduce optical, sampling, and photometric distortions. Furthermore, the physical measurement must be combined with a model of a visual system to assess whether the distortions are going to be visible. To enable perceptual assessment of displays, we propose a combination of a camera-based reconstruction pipeline with a visual difference predictor, which account for both the inaccuracy of camera measurements and visual difference prediction. The reconstruction pipeline combines HDR image stacking, MTF inversion, vignetting correction, geometric undistortion, homography transformation, and color correction, enabling cameras to function as precise display measurement instruments. By incorporating a Visual Difference Predictor (VDP), our system models the visibility of various stimuli under different viewing conditions for the human visual system. We validate the proposed CameraVDP framework through three applications: defective pixel detection, color fringing awareness, and display non-uniformity evaluation. Our uncertainty analysis framework enables the estimation of the theoretical upper bound for defect pixel detection performance and provides confidence intervals for VDP quality scores.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [Diffusion-Based Action Recognition Generalizes to Untrained Domains](https://arxiv.org/abs/2509.08908)
*Rogerio Guimaraes,Frank Xiao,Pietro Perona,Markus Marks*

Main category: cs.CV

TL;DR: 使用视觉扩散模型特征和transformer聚合，实现跨物种、视角和上下文的人类级动作识别泛化能力


<details>
  <summary>Details</summary>
Motivation: 人类能够识别不同物种、视角和上下文中的相同动作，但当前深度学习模型在此类泛化方面存在困难

Method: 利用视觉扩散模型生成特征，通过transformer进行聚合，特别使用扩散过程早期时间步的条件模型来突出语义信息而非像素细节

Result: 在跨物种、跨视角和跨上下文三个泛化基准测试中都达到了新的最先进水平

Conclusion: 该方法使机器动作识别更接近人类级别的鲁棒性，为跨域动作识别提供了有效解决方案

Abstract: Humans can recognize the same actions despite large context and viewpoint variations, such as differences between species (walking in spiders vs. horses), viewpoints (egocentric vs. third-person), and contexts (real life vs movies). Current deep learning models struggle with such generalization. We propose using features generated by a Vision Diffusion Model (VDM), aggregated via a transformer, to achieve human-like action recognition across these challenging conditions. We find that generalization is enhanced by the use of a model conditioned on earlier timesteps of the diffusion process to highlight semantic information over pixel level details in the extracted features. We experimentally explore the generalization properties of our approach in classifying actions across animal species, across different viewing angles, and different recording contexts. Our model sets a new state-of-the-art across all three generalization benchmarks, bringing machine action recognition closer to human-like robustness. Project page: $\href{https://www.vision.caltech.edu/actiondiff/}{\texttt{vision.caltech.edu/actiondiff}}$ Code: $\href{https://github.com/frankyaoxiao/ActionDiff}{\texttt{github.com/frankyaoxiao/ActionDiff}}$

</details>


### [3] [Discovering Divergent Representations between Text-to-Image Models](https://arxiv.org/abs/2509.08940)
*Lisa Dunlap,Joseph E. Gonzalez,Trevor Darrell,Fabian Caba Heilbron,Josef Sivic,Bryan Russell*

Main category: cs.CV

TL;DR: CompCon算法用于比较不同文本到图像生成模型的视觉表示差异，通过进化搜索发现模型间差异化的视觉属性和触发这些差异的提示概念


<details>
  <summary>Details</summary>
Motivation: 研究不同生成模型在相同文本提示下产生不同视觉表示的原因，探索模型间的概念差异和触发机制

Method: 提出CompCon进化搜索算法，自动发现一个模型比另一个模型更常出现的视觉属性，并找出与这些视觉差异相关的提示概念。建立了自动化数据生成管道创建ID2数据集进行验证

Result: 成功比较了流行文本到图像模型（如PixArt和Stable Diffusion 3.5），发现了模型间的表示差异，例如PixArt在表达孤独的提示中倾向于描绘湿漉街道，而Stable Diffusion 3.5在描绘非裔美国人时更倾向于媒体职业形象

Conclusion: CompCon能够有效识别和比较不同生成模型的视觉表示差异，为理解模型间的概念偏差和生成特性提供了有力工具

Abstract: In this paper, we investigate when and how visual representations learned by two different generative models diverge. Given two text-to-image models, our goal is to discover visual attributes that appear in images generated by one model but not the other, along with the types of prompts that trigger these attribute differences. For example, "flames" might appear in one model's outputs when given prompts expressing strong emotions, while the other model does not produce this attribute given the same prompts. We introduce CompCon (Comparing Concepts), an evolutionary search algorithm that discovers visual attributes more prevalent in one model's output than the other, and uncovers the prompt concepts linked to these visual differences. To evaluate CompCon's ability to find diverging representations, we create an automated data generation pipeline to produce ID2, a dataset of 60 input-dependent differences, and compare our approach to several LLM- and VLM-powered baselines. Finally, we use CompCon to compare popular text-to-image models, finding divergent representations such as how PixArt depicts prompts mentioning loneliness with wet streets and Stable Diffusion 3.5 depicts African American people in media professions. Code at: https://github.com/adobe-research/CompCon

</details>


### [4] [Integrating Anatomical Priors into a Causal Diffusion Model](https://arxiv.org/abs/2509.09054)
*Binxu Li,Wei Peng,Mingjie Li,Ehsan Adeli,Kilian M. Pohl*

Main category: cs.CV

TL;DR: PCGM是一种新的3D脑MRI反事实生成方法，通过概率因果图模型整合解剖约束，生成高质量且解剖学合理的脑部MRI图像，能够保留细微的医学相关局部变化。


<details>
  <summary>Details</summary>
Motivation: 现有反事实模型难以生成解剖学合理的脑MRI图像，因为它们缺乏明确的归纳偏置来保留细粒度的解剖细节，而优化整体外观的训练目标无法保留医学相关的局部细微变化。

Method: 提出概率因果图模型(PCGM)，在生成扩散框架中显式整合体素级解剖约束作为先验。通过概率图模块捕获解剖约束并转换为空间二进制掩码，使用3D ControlNet编码掩码，约束新型反事实去噪UNet，最后通过3D扩散解码器生成高质量脑MRI。

Result: 在多个数据集上的实验表明，PCGM生成的脑MRI质量优于多个基线方法。首次证明从PCGM生成的反事实图像中提取的脑测量结果能够复现神经科学文献中报告的疾病对皮层脑区的细微影响。

Conclusion: PCGM在合成MRI用于研究细微形态差异方面取得了重要里程碑，能够生成高质量且解剖学合理的脑部MRI图像，成功保留了疾病相关的细微形态变化。

Abstract: 3D brain MRI studies often examine subtle morphometric differences between cohorts that are hard to detect visually. Given the high cost of MRI acquisition, these studies could greatly benefit from image syntheses, particularly counterfactual image generation, as seen in other domains, such as computer vision. However, counterfactual models struggle to produce anatomically plausible MRIs due to the lack of explicit inductive biases to preserve fine-grained anatomical details. This shortcoming arises from the training of the models aiming to optimize for the overall appearance of the images (e.g., via cross-entropy) rather than preserving subtle, yet medically relevant, local variations across subjects. To preserve subtle variations, we propose to explicitly integrate anatomical constraints on a voxel-level as prior into a generative diffusion framework. Called Probabilistic Causal Graph Model (PCGM), the approach captures anatomical constraints via a probabilistic graph module and translates those constraints into spatial binary masks of regions where subtle variations occur. The masks (encoded by a 3D extension of ControlNet) constrain a novel counterfactual denoising UNet, whose encodings are then transferred into high-quality brain MRIs via our 3D diffusion decoder. Extensive experiments on multiple datasets demonstrate that PCGM generates structural brain MRIs of higher quality than several baseline approaches. Furthermore, we show for the first time that brain measurements extracted from counterfactuals (generated by PCGM) replicate the subtle effects of a disease on cortical brain regions previously reported in the neuroscience literature. This achievement is an important milestone in the use of synthetic MRIs in studies investigating subtle morphological differences.

</details>


### [5] [ALL-PET: A Low-resource and Low-shot PET Foundation Model in the Projection Domain](https://arxiv.org/abs/2509.09130)
*Bin Huang,Kang Chen,Bingxuan Li,Huafeng Liu,Qiegen Liu*

Main category: cs.CV

TL;DR: ALL-PET是一个低资源、低样本的PET投影域基础模型，通过潜在扩散模型和三项关键创新技术，仅用500个样本就能实现高质量的投影数据生成，并在多种PET任务中展现出色性能。


<details>
  <summary>Details</summary>
Motivation: PET成像领域构建大规模基础模型面临标注数据有限和计算资源不足的挑战，需要解决数据稀缺和效率限制的问题。

Method: 采用潜在扩散模型(LDM)，包含三项创新：1)Radon掩码增强策略(RMAS)生成20万+多样化训练样本；2)正负掩码约束嵌入几何一致性；3)透明医学注意力(TMA)机制增强病灶相关区域。

Result: 仅使用500个样本就实现了高质量的投影数据生成，性能可与大数据集训练模型相媲美，内存使用低于24GB，在低剂量重建、衰减校正、延迟帧预测和示踪剂分离等任务中均表现良好。

Conclusion: ALL-PET成功解决了PET成像中的数据稀缺问题，提供了一个高效、灵活且可解释的基础模型解决方案，在多种临床应用场景中展现出强大的泛化能力。

Abstract: Building large-scale foundation model for PET imaging is hindered by limited access to labeled data and insufficient computational resources. To overcome data scarcity and efficiency limitations, we propose ALL-PET, a low-resource, low-shot PET foundation model operating directly in the projection domain. ALL-PET leverages a latent diffusion model (LDM) with three key innovations. First, we design a Radon mask augmentation strategy (RMAS) that generates over 200,000 structurally diverse training samples by projecting randomized image-domain masks into sinogram space, significantly improving generalization with minimal data. This is extended by a dynamic multi-mask (DMM) mechanism that varies mask quantity and distribution, enhancing data diversity without added model complexity. Second, we implement positive/negative mask constraints to embed strict geometric consistency, reducing parameter burden while preserving generation quality. Third, we introduce transparent medical attention (TMA), a parameter-free, geometry-driven mechanism that enhances lesion-related regions in raw projection data. Lesion-focused attention maps are derived from coarse segmentation, covering both hypermetabolic and hypometabolic areas, and projected into sinogram space for physically consistent guidance. The system supports clinician-defined ROI adjustments, ensuring flexible, interpretable, and task-adaptive emphasis aligned with PET acquisition physics. Experimental results show ALL-PET achieves high-quality sinogram generation using only 500 samples, with performance comparable to models trained on larger datasets. ALL-PET generalizes across tasks including low-dose reconstruction, attenuation correction, delayed-frame prediction, and tracer separation, operating efficiently with memory use under 24GB.

</details>


### [6] [RT-DETR++ for UAV Object Detection](https://arxiv.org/abs/2509.09157)
*Yuan Shufang*

Main category: cs.CV

TL;DR: RT-DETR++通过改进RT-DETR模型的编码器，引入通道门控注意力上采样/下采样机制和CSP-PAC特征融合技术，在保持实时检测速度的同时，显著提升了无人机图像中小目标和密集目标的检测性能。


<details>
  <summary>Details</summary>
Motivation: 无人机图像中的目标检测面临小目标密集、尺度变化大、遮挡严重等挑战，需要开发既能保持实时性又能提升检测精度的解决方案。

Method: 1. 引入基于通道门控注意力的上采样/下采样(AU/AD)双路径机制，减少特征层传播误差并保留细节；2. 在特征融合中采用CSP-PAC技术，使用并行空洞卷积在同一层处理局部和上下文信息，促进多尺度特征融合。

Result: 新型neck设计在小目标和密集目标检测方面表现出优越性能，模型在保持实时检测速度的同时不增加计算复杂度。

Conclusion: 该研究为实时检测系统中的特征编码设计提供了有效方法，解决了无人机图像目标检测的关键挑战。

Abstract: Object detection in unmanned aerial vehicle (UAV) imagery presents significant challenges. Issues such as densely packed small objects, scale variations, and occlusion are commonplace. This paper introduces RT-DETR++, which enhances the encoder component of the RT-DETR model. Our improvements focus on two key aspects. First, we introduce a channel-gated attention-based upsampling/downsampling (AU/AD) mechanism. This dual-path system minimizes errors and preserves details during feature layer propagation. Second, we incorporate CSP-PAC during feature fusion. This technique employs parallel hollow convolutions to process local and contextual information within the same layer, facilitating the integration of multi-scale features. Evaluation demonstrates that our novel neck design achieves superior performance in detecting small and densely packed objects. The model maintains sufficient speed for real-time detection without increasing computational complexity. This study provides an effective approach for feature encoding design in real-time detection systems.

</details>


### [7] [Bridging the Gap Between Ideal and Real-world Evaluation: Benchmarking AI-Generated Image Detection in Challenging Scenarios](https://arxiv.org/abs/2509.09172)
*Chunxiao Li,Xiaoxiao Wang,Meiling Li,Boming Miao,Peng Sun,Yunjian Zhang,Xiangyang Ji,Yao Zhu*

Main category: cs.CV

TL;DR: RRDataset是一个用于评估AI生成图像检测模型在真实世界复杂条件下的综合数据集，包含场景泛化、网络传输鲁棒性和重数字化鲁棒性三个维度，基准测试揭示了当前检测方法的局限性


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的快速发展，高度逼真的图像合成对数字安全和媒体可信度提出了新挑战。现有AI生成图像检测方法在复杂真实世界条件下的性能评估存在研究空白

Method: 构建包含7个主要场景的高质量图像的RRDataset，评估17个检测器和10个视觉语言模型，并进行192名参与者的大规模人类研究，探索人类在检测AI生成图像方面的少样本学习能力

Result: 基准测试结果揭示了当前AI检测方法在真实世界条件下的局限性，强调了借鉴人类适应性开发更鲁棒检测算法的重要性

Conclusion: 该研究填补了AI生成图像检测在真实世界评估方面的空白，RRDataset为全面评估检测模型提供了重要基准，人类研究结果为开发更鲁棒的检测算法提供了新思路

Abstract: With the rapid advancement of generative models, highly realistic image synthesis has posed new challenges to digital security and media credibility. Although AI-generated image detection methods have partially addressed these concerns, a substantial research gap remains in evaluating their performance under complex real-world conditions. This paper introduces the Real-World Robustness Dataset (RRDataset) for comprehensive evaluation of detection models across three dimensions: 1) Scenario Generalization: RRDataset encompasses high-quality images from seven major scenarios (War and Conflict, Disasters and Accidents, Political and Social Events, Medical and Public Health, Culture and Religion, Labor and Production, and everyday life), addressing existing dataset gaps from a content perspective. 2) Internet Transmission Robustness: examining detector performance on images that have undergone multiple rounds of sharing across various social media platforms. 3) Re-digitization Robustness: assessing model effectiveness on images altered through four distinct re-digitization methods. We benchmarked 17 detectors and 10 vision-language models (VLMs) on RRDataset and conducted a large-scale human study involving 192 participants to investigate human few-shot learning capabilities in detecting AI-generated images. The benchmarking results reveal the limitations of current AI detection methods under real-world conditions and underscore the importance of drawing on human adaptability to develop more robust detection algorithms.

</details>


### [8] [Dark-ISP: Enhancing RAW Image Processing for Low-Light Object Detection](https://arxiv.org/abs/2509.09183)
*Jiasheng Guo,Xin Gao,Yuxiang Yan,Guanghao Li,Jian Pu*

Main category: cs.CV

TL;DR: 提出了Dark-ISP轻量级自适应ISP插件，直接在暗环境下处理Bayer RAW图像，通过解构传统ISP流水线为可微分组件，实现端到端的低光目标检测优化。


<details>
  <summary>Details</summary>
Motivation: 低光目标检测因图像质量退化而具有挑战性，现有方法要么使用有信息损失的RAW-RGB图像，要么采用复杂框架，需要更轻量高效的解决方案。

Method: 将传统ISP流水线分解为顺序线性（传感器校准）和非线性（色调映射）子模块，作为可微分组件通过任务驱动损失优化；每个模块具有内容感知适应性和物理先验；利用ISP级联结构设计自增强机制促进子模块协作。

Result: 在三个RAW图像数据集上的广泛实验表明，该方法在低光环境下以最少的参数超越了最先进的RGB和RAW检测方法，取得了优越结果。

Conclusion: Dark-ISP提供了一种轻量级、自适应的端到端低光目标检测解决方案，通过优化ISP流水线实现更好的检测性能。

Abstract: Low-light Object detection is crucial for many real-world applications but remains challenging due to degraded image quality. While recent studies have shown that RAW images offer superior potential over RGB images, existing approaches either use RAW-RGB images with information loss or employ complex frameworks. To address these, we propose a lightweight and self-adaptive Image Signal Processing (ISP) plugin, Dark-ISP, which directly processes Bayer RAW images in dark environments, enabling seamless end-to-end training for object detection. Our key innovations are: (1) We deconstruct conventional ISP pipelines into sequential linear (sensor calibration) and nonlinear (tone mapping) sub-modules, recasting them as differentiable components optimized through task-driven losses. Each module is equipped with content-aware adaptability and physics-informed priors, enabling automatic RAW-to-RGB conversion aligned with detection objectives. (2) By exploiting the ISP pipeline's intrinsic cascade structure, we devise a Self-Boost mechanism that facilitates cooperation between sub-modules. Through extensive experiments on three RAW image datasets, we demonstrate that our method outperforms state-of-the-art RGB- and RAW-based detection approaches, achieving superior results with minimal parameters in challenging low-light environments.

</details>


### [9] [Plug-and-play Diffusion Models for Image Compressive Sensing with Data Consistency Projection](https://arxiv.org/abs/2509.09365)
*Xiaodong Wang,Ping Wang,Zhangyuan Li,Xin Yuan*

Main category: cs.CV

TL;DR: 本文提出了一个统一框架，将PnP方法与DDIM扩散模型结合用于解决单像素成像等病态逆问题，通过解耦扩散过程并引入混合数据一致性模块来提高重建质量。


<details>
  <summary>Details</summary>
Motivation: 探索PnP方法与DDIM扩散模型之间的联系，特别是在解决病态逆问题方面的应用，旨在整合学习先验与物理前向模型。

Method: 将扩散过程解耦为三个可解释阶段：去噪、数据一致性强制和采样；提出混合数据一致性模块，线性组合多个PnP式保真度项，直接在去噪估计上应用混合校正。

Result: 在单像素成像任务上的实验结果表明，该方法获得了更好的重建质量。

Conclusion: 提出的统一框架成功整合了学习先验与物理模型，混合数据一致性模块在不破坏扩散采样轨迹的情况下改善了测量一致性。

Abstract: We explore the connection between Plug-and-Play (PnP) methods and Denoising Diffusion Implicit Models (DDIM) for solving ill-posed inverse problems, with a focus on single-pixel imaging. We begin by identifying key distinctions between PnP and diffusion models-particularly in their denoising mechanisms and sampling procedures. By decoupling the diffusion process into three interpretable stages: denoising, data consistency enforcement, and sampling, we provide a unified framework that integrates learned priors with physical forward models in a principled manner. Building upon this insight, we propose a hybrid data-consistency module that linearly combines multiple PnP-style fidelity terms. This hybrid correction is applied directly to the denoised estimate, improving measurement consistency without disrupting the diffusion sampling trajectory. Experimental results on single-pixel imaging tasks demonstrate that our method achieves better reconstruction quality.

</details>


### [10] [Decoupling Clinical and Class-Agnostic Features for Reliable Few-Shot Adaptation under Shift](https://arxiv.org/abs/2509.09397)
*Umaima Rahman,Raza Imam,Mohammad Yaqub,Dwarikanath Mahapatra*

Main category: cs.CV

TL;DR: DRiFt是一个医疗视觉语言模型的特征解耦框架，通过分离临床相关信号和任务无关噪声，提升模型在分布偏移下的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医疗视觉语言模型在分布偏移下可靠性不足，容易学习任务无关的相关性，限制了在真实临床环境中的安全部署。

Method: 使用参数高效调优(LoRA)和可学习提示令牌，显式分离临床相关信号和任务无关噪声；通过生成高质量临床基础图像-文本对来增强跨模态对齐。

Result: 在分布内性能提升11.4% Top-1准确率和3.3% Macro-F1，在未见数据集上保持强鲁棒性。

Conclusion: 特征解耦和精心对齐显著提升模型泛化能力，减少领域偏移下的不可预测行为，有助于构建更安全可信的临床VLM。

Abstract: Medical vision-language models (VLMs) offer promise for clinical decision support, yet their reliability under distribution shifts remains a major concern for safe deployment. These models often learn task-agnostic correlations due to variability in imaging protocols and free-text reports, limiting their generalizability and increasing the risk of failure in real-world settings. We propose DRiFt, a structured feature decoupling framework that explicitly separates clinically relevant signals from task-agnostic noise using parameter-efficient tuning (LoRA) and learnable prompt tokens. To enhance cross-modal alignment and reduce uncertainty, we curate high-quality, clinically grounded image-text pairs by generating captions for a diverse medical dataset. Our approach improves in-distribution performance by +11.4% Top-1 accuracy and +3.3% Macro-F1 over prior prompt-based methods, while maintaining strong robustness across unseen datasets. Ablation studies reveal that disentangling task-relevant features and careful alignment significantly enhance model generalization and reduce unpredictable behavior under domain shift. These insights contribute toward building safer, more trustworthy VLMs for clinical use. The code is available at https://github.com/rumaima/DRiFt.

</details>


### [11] [FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolution](https://arxiv.org/abs/2509.09427)
*Yuchan Jie,Yushen Xu,Xiaosong Li,Fuqiang Zhou,Jianming Lv,Huafeng Li*

Main category: cs.CV

TL;DR: FS-Diff是一种基于扩散模型的联合图像融合和超分辨率方法，通过语义引导和清晰度感知机制，在低分辨率感知和跨模态特征提取方面表现出色，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在军事侦察和远程检测等实际应用中，多模态图像中的目标和背景结构容易损坏，分辨率低且语义信息弱，导致现有融合技术效果不佳，需要同时解决图像融合和超分辨率问题。

Method: 将图像融合和超分辨率统一为条件生成问题，利用清晰度感知机制提供语义引导，采用双向特征Mamba提取全局特征，通过改进的U-Net网络实现随机迭代去噪过程，在多个噪声级别进行训练。

Result: 在六个公共数据集和自建的AVMS数据集上的实验表明，FS-Diff在多个放大倍数下优于最先进方法，能够恢复更丰富的细节和语义信息。

Conclusion: FS-Diff通过语义引导和清晰度感知机制有效解决了多模态图像融合和超分辨率的联合问题，在真实场景应用中具有重要价值。

Abstract: As an influential information fusion and low-level vision technique, image fusion integrates complementary information from source images to yield an informative fused image. A few attempts have been made in recent years to jointly realize image fusion and super-resolution. However, in real-world applications such as military reconnaissance and long-range detection missions, the target and background structures in multimodal images are easily corrupted, with low resolution and weak semantic information, which leads to suboptimal results in current fusion techniques. In response, we propose FS-Diff, a semantic guidance and clarity-aware joint image fusion and super-resolution method. FS-Diff unifies image fusion and super-resolution as a conditional generation problem. It leverages semantic guidance from the proposed clarity sensing mechanism for adaptive low-resolution perception and cross-modal feature extraction. Specifically, we initialize the desired fused result as pure Gaussian noise and introduce the bidirectional feature Mamba to extract the global features of the multimodal images. Moreover, utilizing the source images and semantics as conditions, we implement a random iterative denoising process via a modified U-Net network. This network istrained for denoising at multiple noise levels to produce high-resolution fusion results with cross-modal features and abundant semantic information. We also construct a powerful aerial view multiscene (AVMS) benchmark covering 600 pairs of images. Extensive joint image fusion and super-resolution experiments on six public and our AVMS datasets demonstrated that FS-Diff outperforms the state-of-the-art methods at multiple magnifications and can recover richer details and semantics in the fused images. The code is available at https://github.com/XylonXu01/FS-Diff.

</details>


### [12] [FlexiD-Fuse: Flexible number of inputs multi-modal medical image fusion based on diffusion model](https://arxiv.org/abs/2509.09456)
*Yushen Xu,Xiaosong Li,Yuchun Wang,Xiaoqi Cheng,Huafeng Li,Haishu Tan*

Main category: cs.CV

TL;DR: FlexiD-Fuse是一个基于扩散模型的医学图像融合网络，能够处理任意数量的输入模态，解决了现有方法只能处理固定数量模态输入的限制。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像融合方法只能处理固定数量的模态输入（如双模态或三模态），无法直接处理变化数量的输入，这限制了其在临床环境中的应用。

Method: 提出FlexiD-Fuse扩散融合网络，将扩散融合问题转化为基于扩散过程和分层贝叶斯建模的最大似然估计问题，通过将期望最大化算法融入扩散采样迭代过程，实现任意数量输入模态的高质量融合。

Result: 在哈佛数据集上的实验表明，该方法在可变输入数量的医学图像融合中取得了最佳性能，同时在红外-可见光、多曝光和多焦点图像融合任务中也表现出优越性。

Conclusion: FlexiD-Fuse方法有效解决了医学图像融合中可变输入数量的问题，在多个融合任务中均表现出优异的性能，具有很好的临床应用前景。

Abstract: Different modalities of medical images provide unique physiological and anatomical information for diseases. Multi-modal medical image fusion integrates useful information from different complementary medical images with different modalities, producing a fused image that comprehensively and objectively reflects lesion characteristics to assist doctors in clinical diagnosis. However, existing fusion methods can only handle a fixed number of modality inputs, such as accepting only two-modal or tri-modal inputs, and cannot directly process varying input quantities, which hinders their application in clinical settings. To tackle this issue, we introduce FlexiD-Fuse, a diffusion-based image fusion network designed to accommodate flexible quantities of input modalities. It can end-to-end process two-modal and tri-modal medical image fusion under the same weight. FlexiD-Fuse transforms the diffusion fusion problem, which supports only fixed-condition inputs, into a maximum likelihood estimation problem based on the diffusion process and hierarchical Bayesian modeling. By incorporating the Expectation-Maximization algorithm into the diffusion sampling iteration process, FlexiD-Fuse can generate high-quality fused images with cross-modal information from source images, independently of the number of input images. We compared the latest two and tri-modal medical image fusion methods, tested them on Harvard datasets, and evaluated them using nine popular metrics. The experimental results show that our method achieves the best performance in medical image fusion with varying inputs. Meanwhile, we conducted extensive extension experiments on infrared-visible, multi-exposure, and multi-focus image fusion tasks with arbitrary numbers, and compared them with the perspective SOTA methods. The results of the extension experiments consistently demonstrate the effectiveness and superiority of our method.

</details>


### [13] [Improving Video Diffusion Transformer Training by Multi-Feature Fusion and Alignment from Self-Supervised Vision Encoders](https://arxiv.org/abs/2509.09547)
*Dohun Lee,Hyeonho Jeong,Jiwook Kim,Duygu Ceylan,Jong Chul Ye*

Main category: cs.CV

TL;DR: 提出了Align4Gen方法，通过在视频扩散模型训练中引入预训练视觉编码器的特征对齐，提升视频生成质量


<details>
  <summary>Details</summary>
Motivation: 当前视频扩散模型主要关注架构创新和训练目标改进，而忽视了特征表示能力的提升。研究发现中间特征与预训练视觉编码器的对齐可以改善视频生成效果

Method: 提出多特征融合和对齐方法Align4Gen，集成到视频扩散模型训练中。首先分析评估不同视觉编码器的判别性和时序一致性，然后选择合适编码器进行特征对齐

Result: 在无条件和类条件视频生成任务上都取得了改进，多种量化指标显示视频生成质量得到提升

Conclusion: 特征对齐是提升视频扩散模型性能的有效方法，Align4Gen通过多特征融合和对齐机制显著改善了视频生成效果

Abstract: Video diffusion models have advanced rapidly in the recent years as a result of series of architectural innovations (e.g., diffusion transformers) and use of novel training objectives (e.g., flow matching). In contrast, less attention has been paid to improving the feature representation power of such models. In this work, we show that training video diffusion models can benefit from aligning the intermediate features of the video generator with feature representations of pre-trained vision encoders. We propose a new metric and conduct an in-depth analysis of various vision encoders to evaluate their discriminability and temporal consistency, thereby assessing their suitability for video feature alignment. Based on the analysis, we present Align4Gen which provides a novel multi-feature fusion and alignment method integrated into video diffusion model training. We evaluate Align4Gen both for unconditional and class-conditional video generation tasks and show that it results in improved video generation as quantified by various metrics. Full video results are available on our project page: https://align4gen.github.io/align4gen/

</details>


### [14] [PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient Fine-Tuning for Remote Sensing Change Detection](https://arxiv.org/abs/2509.09572)
*Sijun Dong,Yuxuan Hu,LiBo Wang,Geng Chen,Xiaoliang Meng*

Main category: cs.CV

TL;DR: PeftCD是一个基于视觉基础模型(VFMs)和参数高效微调(PEFT)的变化检测框架，通过LoRA和Adapter模块实现高效任务适应，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决多时相多源遥感影像中伪变化普遍、标记样本稀缺和跨域泛化困难的问题

Method: 使用权重共享的Siamese编码器，集成LoRA和Adapter模块，采用SAM2和DINOv3作为骨干网络，配合轻量级解码器

Result: 在SYSU-CD(IoU 73.81%)、WHUCD(92.05%)、MSRSCD(64.07%)、MLCD(76.89%)、CDD(97.01%)、S2Looking(52.25%)和LEVIR-CD(85.62%)等多个数据集上达到最先进性能

Conclusion: PeftCD在准确性、效率和泛化性之间实现了最佳平衡，为将大规模VFM适配到实际遥感变化检测应用提供了强大且可扩展的范式

Abstract: To tackle the prevalence of pseudo changes, the scarcity of labeled samples, and the difficulty of cross-domain generalization in multi-temporal and multi-source remote sensing imagery, we propose PeftCD, a change detection framework built upon Vision Foundation Models (VFMs) with Parameter-Efficient Fine-Tuning (PEFT). At its core, PeftCD employs a weight-sharing Siamese encoder derived from a VFM, into which LoRA and Adapter modules are seamlessly integrated. This design enables highly efficient task adaptation by training only a minimal set of additional parameters. To fully unlock the potential of VFMs, we investigate two leading backbones: the Segment Anything Model v2 (SAM2), renowned for its strong segmentation priors, and DINOv3, a state-of-the-art self-supervised representation learner. The framework is complemented by a deliberately lightweight decoder, ensuring the focus remains on the powerful feature representations from the backbones. Extensive experiments demonstrate that PeftCD achieves state-of-the-art performance across multiple public datasets, including SYSU-CD (IoU 73.81%), WHUCD (92.05%), MSRSCD (64.07%), MLCD (76.89%), CDD (97.01%), S2Looking (52.25%) and LEVIR-CD (85.62%), with notably precise boundary delineation and strong suppression of pseudo-changes. In summary, PeftCD presents an optimal balance of accuracy, efficiency, and generalization. It offers a powerful and scalable paradigm for adapting large-scale VFMs to real-world remote sensing change detection applications. The code and pretrained models will be released at https://github.com/dyzy41/PeftCD.

</details>


### [15] [Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis](https://arxiv.org/abs/2509.09595)
*Yikang Ding,Jiwen Liu,Wenyuan Zhang,Zekun Wang,Wentao Hu,Liyuan Cui,Mingming Lao,Yingchao Shao,Hui Liu,Xiaohan Li,Ming Chen,Xiaoqiang Liu,Yu-Shen Liu,Pengfei Wan*

Main category: cs.CV

TL;DR: Kling-Avatar是一个新颖的级联框架，通过多模态指令理解和照片级真实感人像生成，解决了现有音频驱动虚拟人视频生成方法在叙事连贯性和角色表现力方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅将指令条件视为声学或视觉线索驱动的低级跟踪，没有建模指令传达的交流目的，这影响了叙事连贯性和角色表现力。

Method: 采用两阶段流水线：第一阶段使用多模态大语言模型导演生成蓝图视频，控制高级语义如角色动作和情感；第二阶段在蓝图关键帧指导下，使用首尾帧策略并行生成多个子片段。

Result: 能够生成生动、流畅、长时长的1080p 48fps视频，在唇同步准确性、情感和动态表现力、指令可控性、身份保持和跨域泛化方面表现优异。

Conclusion: Kling-Avatar为基于语义的高保真音频驱动虚拟人合成建立了新的基准，适用于数字人直播和视频博客等实际应用。

Abstract: Recent advances in audio-driven avatar video generation have significantly enhanced audio-visual realism. However, existing methods treat instruction conditioning merely as low-level tracking driven by acoustic or visual cues, without modeling the communicative purpose conveyed by the instructions. This limitation compromises their narrative coherence and character expressiveness. To bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that unifies multimodal instruction understanding with photorealistic portrait generation. Our approach adopts a two-stage pipeline. In the first stage, we design a multimodal large language model (MLLM) director that produces a blueprint video conditioned on diverse instruction signals, thereby governing high-level semantics such as character motion and emotions. In the second stage, guided by blueprint keyframes, we generate multiple sub-clips in parallel using a first-last frame strategy. This global-to-local framework preserves fine-grained details while faithfully encoding the high-level intent behind multimodal instructions. Our parallel architecture also enables fast and stable generation of long-duration videos, making it suitable for real-world applications such as digital human livestreaming and vlogging. To comprehensively evaluate our method, we construct a benchmark of 375 curated samples covering diverse instructions and challenging scenarios. Extensive experiments demonstrate that Kling-Avatar is capable of generating vivid, fluent, long-duration videos at up to 1080p and 48 fps, achieving superior performance in lip synchronization accuracy, emotion and dynamic expressiveness, instruction controllability, identity preservation, and cross-domain generalization. These results establish Kling-Avatar as a new benchmark for semantically grounded, high-fidelity audio-driven avatar synthesis.

</details>


### [16] [Mechanistic Learning with Guided Diffusion Models to Predict Spatio-Temporal Brain Tumor Growth](https://arxiv.org/abs/2509.09610)
*Daria Laslo,Efthymios Georgiou,Marius George Linguraru,Andreas Rauschecker,Sabine Muller,Catherine R. Jutzeler,Sarah Bruningk*

Main category: cs.CV

TL;DR: 一种结合机制模型和导向去噪模型的混合框架，用于预测脑结构的时空进展，生成解剖学可行的未来MRI图像。


<details>
  <summary>Details</summary>
Motivation: 预测脑结构的时空进展对神经脱外科临床决策至关重要，需要方法在数据有限情况下进行生物学信息的图像生成。

Method: 采用数学脑结构增长模型（常微分方程）捕捉时间动态，结合检查影响估计，然后用梯度导向去噪模型（DDIM）进行图像合成，确保与预测增长和病人解剖结构对齐。

Result: 在BraTS成人和儿童绪杂胞瘤数据集训练，在60个纵向儿童DMG病例上评估。模型生成了现实的随访扫描，通过空间相似性指标验证，并提供了投影增长概率图，显示了临床相关的结构范围和增长方向性。

Conclusion: 该方法能够在数据有限情况下进行生物学信息的图像生成，提供考虑机制前知的生成式时空预测。

Abstract: Predicting the spatio-temporal progression of brain tumors is essential for guiding clinical decisions in neuro-oncology. We propose a hybrid mechanistic learning framework that combines a mathematical tumor growth model with a guided denoising diffusion implicit model (DDIM) to synthesize anatomically feasible future MRIs from preceding scans. The mechanistic model, formulated as a system of ordinary differential equations, captures temporal tumor dynamics including radiotherapy effects and estimates future tumor burden. These estimates condition a gradient-guided DDIM, enabling image synthesis that aligns with both predicted growth and patient anatomy. We train our model on the BraTS adult and pediatric glioma datasets and evaluate on 60 axial slices of in-house longitudinal pediatric diffuse midline glioma (DMG) cases. Our framework generates realistic follow-up scans based on spatial similarity metrics. It also introduces tumor growth probability maps, which capture both clinically relevant extent and directionality of tumor growth as shown by 95th percentile Hausdorff Distance. The method enables biologically informed image generation in data-limited scenarios, offering generative-space-time predictions that account for mechanistic priors.

</details>


### [17] [Geometric Neural Distance Fields for Learning Human Motion Priors](https://arxiv.org/abs/2509.09667)
*Zhengdi Yu,Simone Foti,Linguang Zhang,Amy Zhao,Cem Keskin,Stefanos Zafeiriou,Tolga Birdal*

Main category: cs.CV

TL;DR: NRMF是一种新颖的3D生成式人体运动先验模型，通过神经距离场在关节旋转、角速度和角加速度的乘积空间中显式建模人体运动，实现了鲁棒、时间一致且物理合理的3D运动恢复。


<details>
  <summary>Details</summary>
Motivation: 现有VAE或扩散模型方法在3D人体运动恢复中存在局限性，需要一种能够显式建模运动动力学并尊重底层关节几何结构的新方法。

Method: 在关节旋转、角速度和角加速度的乘积空间上构建神经距离场，提出自适应步长混合算法进行投影，以及几何积分器来展开运动轨迹。

Result: 在AMASS数据集上训练后，NRMF在多种输入模态和任务中表现出显著且一致的性能提升，包括去噪、运动插值和部分2D/3D观测拟合。

Conclusion: NRMF通过几何感知的运动建模方法，为3D人体运动恢复提供了更鲁棒和物理合理的解决方案，具有很好的泛化能力。

Abstract: We introduce Neural Riemannian Motion Fields (NRMF), a novel 3D generative human motion prior that enables robust, temporally consistent, and physically plausible 3D motion recovery. Unlike existing VAE or diffusion-based methods, our higher-order motion prior explicitly models the human motion in the zero level set of a collection of neural distance fields (NDFs) corresponding to pose, transition (velocity), and acceleration dynamics. Our framework is rigorous in the sense that our NDFs are constructed on the product space of joint rotations, their angular velocities, and angular accelerations, respecting the geometry of the underlying articulations. We further introduce: (i) a novel adaptive-step hybrid algorithm for projecting onto the set of plausible motions, and (ii) a novel geometric integrator to "roll out" realistic motion trajectories during test-time-optimization and generation. Our experiments show significant and consistent gains: trained on the AMASS dataset, NRMF remarkably generalizes across multiple input modalities and to diverse tasks ranging from denoising to motion in-betweening and fitting to partial 2D / 3D observations.

</details>


### [18] [Locality in Image Diffusion Models Emerges from Data Statistics](https://arxiv.org/abs/2509.09672)
*Artem Lukoianov,Chenyang Yuan,Justin Solomon,Vincent Sitzmann*

Main category: cs.CV

TL;DR: 本文研究发现扩散模型中局部性特征源于图像数据集的统计特性，而非卷积神经网络的归纳偏置，并基于此构建了更匹配深度扩散模型的解析去噪器


<details>
  <summary>Details</summary>
Motivation: 现有研究认为深度扩散模型与最优去噪器之间的性能差距源于卷积神经网络的平移等变性和局部性归纳偏置，但本文旨在证明这种局部性实际上是图像数据集本身的统计特性

Method: 通过理论分析和实验验证，证明最优参数化线性去噪器表现出与深度神经去噪器相似的局部性特征，并利用自然图像数据集中像素相关性的洞察构建新的解析去噪器

Result: 研究发现局部性直接源于自然图像数据集中的像素相关性，构建的解析去噪器比先前专家设计的替代方案更能匹配深度扩散模型预测的分数

Conclusion: 扩散模型中的局部性特征本质上是图像数据集的统计属性，这一发现为理解深度扩散模型行为提供了新视角，并有助于构建更准确的解析模型

Abstract: Among generative models, diffusion models are uniquely intriguing due to the existence of a closed-form optimal minimizer of their training objective, often referred to as the optimal denoiser. However, diffusion using this optimal denoiser merely reproduces images in the training set and hence fails to capture the behavior of deep diffusion models. Recent work has attempted to characterize this gap between the optimal denoiser and deep diffusion models, proposing analytical, training-free models that can generate images that resemble those generated by a trained UNet. The best-performing method hypothesizes that shift equivariance and locality inductive biases of convolutional neural networks are the cause of the performance gap, hence incorporating these assumptions into its analytical model. In this work, we present evidence that the locality in deep diffusion models emerges as a statistical property of the image dataset, not due to the inductive bias of convolutional neural networks. Specifically, we demonstrate that an optimal parametric linear denoiser exhibits similar locality properties to the deep neural denoisers. We further show, both theoretically and experimentally, that this locality arises directly from the pixel correlations present in natural image datasets. Finally, we use these insights to craft an analytical denoiser that better matches scores predicted by a deep diffusion model than the prior expert-crafted alternative.

</details>


### [19] [FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark](https://arxiv.org/abs/2509.09680)
*Rongyao Fang,Aldrich Yu,Chengqi Duan,Linjiang Huang,Shuai Bai,Yuxuan Cai,Kun Wang,Si Liu,Xihui Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: FLUX-Reason-6M是一个包含600万高质量图像和2000万双语描述的推理数据集，PRISM-Bench提供7个评估赛道的新基准，旨在提升开源文本生成图像模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 开源文本生成图像模型因缺乏大规模推理数据集和全面评估基准而落后于闭源系统，需要解决这一性能差距。

Method: 构建FLUX-Reason-6M数据集（6M图像+20M双语描述），采用六维特征组织和生成思维链；创建PRISM-Bench评估基准（7个赛道）；使用15000 A100 GPU天进行数据整理。

Result: 对19个领先模型的评估揭示了关键性能差距和改进需求，为社区提供了工业级资源。

Conclusion: 该数据集和基准将推动下一代面向推理的文本生成图像技术的发展，所有资源已开源发布。

Abstract: The advancement of open-source text-to-image (T2I) models has been hindered by the absence of large-scale, reasoning-focused datasets and comprehensive evaluation benchmarks, resulting in a performance gap compared to leading closed-source systems. To address this challenge, We introduce FLUX-Reason-6M and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark). FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality FLUX-generated images and 20 million bilingual (English and Chinese) descriptions specifically designed to teach complex reasoning. The image are organized according to six key characteristics: Imagination, Entity, Text rendering, Style, Affection, and Composition, and design explicit Generation Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation steps. The whole data curation takes 15,000 A100 GPU days, providing the community with a resource previously unattainable outside of large industrial labs. PRISM-Bench offers a novel evaluation standard with seven distinct tracks, including a formidable Long Text challenge using GCoT. Through carefully designed prompts, it utilizes advanced vision-language models for nuanced human-aligned assessment of prompt-image alignment and image aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench reveals critical performance gaps and highlights specific areas requiring improvement. Our dataset, benchmark, and evaluation code are released to catalyze the next wave of reasoning-oriented T2I generation. Project page: https://flux-reason-6m.github.io/ .

</details>
