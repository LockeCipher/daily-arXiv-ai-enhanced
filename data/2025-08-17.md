<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 29]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MANGO: Multimodal Attention-based Normalizing Flow Approach to Fusion Learning](https://arxiv.org/abs/2508.10133)
*Thanh-Dat Truong,Christophe Bobda,Nitin Agarwal,Khoa Luu*

Main category: cs.CV

TL;DR: 论文提出了一种新的多模态注意力归一化流（MANGO）方法，通过可逆交叉注意力层（ICA）和三种新的交叉注意力机制，显式、可解释地学习多模态数据的复杂相关性，并在多个任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态融合方法依赖Transformer的注意力机制隐式学习特征相关性，难以捕捉模态的本质特征和复杂结构，因此需要一种显式、可解释且可扩展的多模态融合方法。

Method: 提出MANGO方法，包括可逆交叉注意力层（ICA）和三种新机制（MMCA、IMCA、LICA），结合归一化流模型处理高维多模态数据。

Result: 在语义分割、图像到图像翻译和电影类型分类三个任务中，MANGO方法实现了SOTA性能。

Conclusion: MANGO方法通过显式建模多模态数据的相关性，提高了模型的解释性和性能，适用于高维数据。

Abstract: Multimodal learning has gained much success in recent years. However, current multimodal fusion methods adopt the attention mechanism of Transformers to implicitly learn the underlying correlation of multimodal features. As a result, the multimodal model cannot capture the essential features of each modality, making it difficult to comprehend complex structures and correlations of multimodal inputs. This paper introduces a novel Multimodal Attention-based Normalizing Flow (MANGO) approach\footnote{The source code of this work will be publicly available.} to developing explicit, interpretable, and tractable multimodal fusion learning. In particular, we propose a new Invertible Cross-Attention (ICA) layer to develop the Normalizing Flow-based Model for multimodal data. To efficiently capture the complex, underlying correlations in multimodal data in our proposed invertible cross-attention layer, we propose three new cross-attention mechanisms: Modality-to-Modality Cross-Attention (MMCA), Inter-Modality Cross-Attention (IMCA), and Learnable Inter-Modality Cross-Attention (LICA). Finally, we introduce a new Multimodal Attention-based Normalizing Flow to enable the scalability of our proposed method to high-dimensional multimodal data. Our experimental results on three different multimodal learning tasks, i.e., semantic segmentation, image-to-image translation, and movie genre classification, have illustrated the state-of-the-art (SoTA) performance of the proposed approach.

</details>


### [2] [EntropyGS: An Efficient Entropy Coding on 3D Gaussian Splatting](https://arxiv.org/abs/2508.10227)
*Yuning Huang,Jiahao Pang,Fengqing Zhu,Dong Tian*

Main category: cs.CV

TL;DR: 3D高斯泼溅（3DGS）是一种新兴的视图合成方法，具有快速训练/渲染和高质量视觉效果的优点。本文提出了一种名为EntropyGS的因子化和参数化熵编码方法，用于压缩3DGS高斯属性，实现了30倍的码率降低，同时保持相似的渲染质量。


<details>
  <summary>Details</summary>
Motivation: 3DGS的高斯创建和视图渲染任务通常在不同时间或设备上分离，因此需要存储/传输和压缩3DGS高斯属性。本文旨在通过分析高斯属性的统计特性，提出高效的压缩方法。

Method: 通过相关性和统计分析发现，球谐AC属性遵循拉普拉斯分布，而旋转、缩放和不透明度可以用高斯混合模型近似。基于此，提出了因子化和参数化的熵编码方法EntropyGS，自适应地对高斯属性进行量化和熵编码。

Result: EntropyGS在基准数据集上实现了约30倍的码率降低，同时保持相似的渲染质量，且编码和解码时间快速。

Conclusion: EntropyGS是一种高效的3DGS高斯属性压缩方法，适用于实际应用中的存储和传输需求。

Abstract: As an emerging novel view synthesis approach, 3D Gaussian Splatting (3DGS) demonstrates fast training/rendering with superior visual quality. The two tasks of 3DGS, Gaussian creation and view rendering, are typically separated over time or devices, and thus storage/transmission and finally compression of 3DGS Gaussians become necessary. We begin with a correlation and statistical analysis of 3DGS Gaussian attributes. An inspiring finding in this work reveals that spherical harmonic AC attributes precisely follow Laplace distributions, while mixtures of Gaussian distributions can approximate rotation, scaling, and opacity. Additionally, harmonic AC attributes manifest weak correlations with other attributes except for inherited correlations from a color space. A factorized and parameterized entropy coding method, EntropyGS, is hereinafter proposed. During encoding, distribution parameters of each Gaussian attribute are estimated to assist their entropy coding. The quantization for entropy coding is adaptively performed according to Gaussian attribute types. EntropyGS demonstrates about 30x rate reduction on benchmark datasets while maintaining similar rendering quality compared to input 3DGS data, with a fast encoding and decoding time.

</details>


### [3] [High Fidelity Text to Image Generation with Contrastive Alignment and Structural Guidance](https://arxiv.org/abs/2508.10280)
*Danyi Gao*

Main category: cs.CV

TL;DR: 提出了一种结合文本-图像对比约束与结构引导机制的高保真图像生成方法，解决了现有方法在语义对齐和结构一致性上的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动图像生成方法在语义对齐精度和结构一致性上存在性能瓶颈，需改进。

Method: 通过对比学习模块增强跨模态对齐约束，结合结构先验（如语义布局图或边缘草图）引导生成器进行空间级结构建模，并联合优化多种损失函数。

Result: 在COCO-2014数据集上的实验表明，该方法在CLIP Score、FID和SSIM指标上表现优异，有效平衡语义对齐与结构保真度。

Conclusion: 该方法能生成语义清晰且结构完整的图像，为联合文本-图像建模与图像生成提供了可行技术路径。

Abstract: This paper addresses the performance bottlenecks of existing text-driven image generation methods in terms of semantic alignment accuracy and structural consistency. A high-fidelity image generation method is proposed by integrating text-image contrastive constraints with structural guidance mechanisms. The approach introduces a contrastive learning module that builds strong cross-modal alignment constraints to improve semantic matching between text and image. At the same time, structural priors such as semantic layout maps or edge sketches are used to guide the generator in spatial-level structural modeling. This enhances the layout completeness and detail fidelity of the generated images. Within the overall framework, the model jointly optimizes contrastive loss, structural consistency loss, and semantic preservation loss. A multi-objective supervision mechanism is adopted to improve the semantic consistency and controllability of the generated content. Systematic experiments are conducted on the COCO-2014 dataset. Sensitivity analyses are performed on embedding dimensions, text length, and structural guidance strength. Quantitative metrics confirm the superior performance of the proposed method in terms of CLIP Score, FID, and SSIM. The results show that the method effectively bridges the gap between semantic alignment and structural fidelity without increasing computational complexity. It demonstrates a strong ability to generate semantically clear and structurally complete images, offering a viable technical path for joint text-image modeling and image generation.

</details>


### [4] [AtomDiffuser: Time-Aware Degradation Modeling for Drift and Beam Damage in STEM Imaging](https://arxiv.org/abs/2508.10359)
*Hao Wang,Hongkui Zheng,Kai He,Abolfazl Razi*

Main category: cs.CV

TL;DR: AtomDiffuser是一个时间感知的降解建模框架，用于分离STEM数据中的样本漂移和辐射衰减，通过预测仿射变换和空间变化的衰减图来解析原子结构演化。


<details>
  <summary>Details</summary>
Motivation: 解释时间分辨STEM数据时，空间漂移和辐射损伤导致的信号损失会复杂地扭曲几何和强度信息，现有方法难以明确分离这些效应或建模原子分辨率下的材料动力学。

Method: 提出AtomDiffuser框架，通过预测仿射变换和空间衰减图来解耦漂移和辐射衰减，利用降解作为物理启发的时间条件过程。

Result: AtomDiffuser在合成降解过程上训练，能很好地泛化到真实世界的cryo-STEM数据，支持高分辨率降解推断和漂移对齐。

Conclusion: AtomDiffuser提供了解释性结构演化分析工具，可用于可视化和量化与辐射诱导原子不稳定性相关的降解模式。

Abstract: Scanning transmission electron microscopy (STEM) plays a critical role in modern materials science, enabling direct imaging of atomic structures and their evolution under external interferences. However, interpreting time-resolved STEM data remains challenging due to two entangled degradation effects: spatial drift caused by mechanical and thermal instabilities, and beam-induced signal loss resulting from radiation damage. These factors distort both geometry and intensity in complex, temporally correlated ways, making it difficult for existing methods to explicitly separate their effects or model material dynamics at atomic resolution. In this work, we present AtomDiffuser, a time-aware degradation modeling framework that disentangles sample drift and radiometric attenuation by predicting an affine transformation and a spatially varying decay map between any two STEM frames. Unlike traditional denoising or registration pipelines, our method leverages degradation as a physically heuristic, temporally conditioned process, enabling interpretable structural evolutions across time. Trained on synthetic degradation processes, AtomDiffuser also generalizes well to real-world cryo-STEM data. It further supports high-resolution degradation inference and drift alignment, offering tools for visualizing and quantifying degradation patterns that correlate with radiation-induced atomic instabilities.

</details>


### [5] [Towards Spatially Consistent Image Generation: On Incorporating Intrinsic Scene Properties into Diffusion Models](https://arxiv.org/abs/2508.10382)
*Hyundo Lee,Suhyung Choi,Byoung-Tak Zhang,Inwoo Hwang*

Main category: cs.CV

TL;DR: 论文提出了一种通过共同生成图像及其内在场景属性（如深度、分割图）的方法，以解决现有图像生成模型在空间一致性和布局上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型因缺乏底层结构和空间布局信息，常生成空间不一致和扭曲的图像。

Method: 利用预训练估计器从大型图像数据集中提取内在场景属性，并通过自编码器聚合为单一潜在变量；基于预训练的潜在扩散模型（LDMs），同时去噪图像和内在域，共享信息以保持一致性。

Result: 实验表明，该方法能纠正空间不一致性，生成更自然的场景布局，同时保持基础模型（如Stable Diffusion）的保真度和文本对齐性。

Conclusion: 通过共同生成图像和内在属性，模型能隐式捕捉场景结构，生成更一致和真实的图像。

Abstract: Image generation models trained on large datasets can synthesize high-quality images but often produce spatially inconsistent and distorted images due to limited information about the underlying structures and spatial layouts. In this work, we leverage intrinsic scene properties (e.g., depth, segmentation maps) that provide rich information about the underlying scene, unlike prior approaches that solely rely on image-text pairs or use intrinsics as conditional inputs. Our approach aims to co-generate both images and their corresponding intrinsics, enabling the model to implicitly capture the underlying scene structure and generate more spatially consistent and realistic images. Specifically, we first extract rich intrinsic scene properties from a large image dataset with pre-trained estimators, eliminating the need for additional scene information or explicit 3D representations. We then aggregate various intrinsic scene properties into a single latent variable using an autoencoder. Building upon pre-trained large-scale Latent Diffusion Models (LDMs), our method simultaneously denoises the image and intrinsic domains by carefully sharing mutual information so that the image and intrinsic reflect each other without degrading image quality. Experimental results demonstrate that our method corrects spatial inconsistencies and produces a more natural layout of scenes while maintaining the fidelity and textual alignment of the base model (e.g., Stable Diffusion).

</details>


### [6] [PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce Driver Distraction Detection](https://arxiv.org/abs/2508.10397)
*Haibin Sun,Xinghui Song*

Main category: cs.CV

TL;DR: 论文提出了一种基于姿态驱动的质量控制数据增强框架（PQ-DAF），通过扩散模型和视觉语言模型提升少样本驾驶员分心检测的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型在真实场景中泛化能力不足，主要由于数据标注成本高和训练与部署环境的域偏移问题。

Method: 采用渐进条件扩散模型（PCDMs）生成多样训练样本，并利用CogVLM视觉语言模型过滤低质量样本。

Result: 实验表明PQ-DAF显著提升了少样本条件下的模型性能。

Conclusion: PQ-DAF通过数据增强和质量控制有效解决了驾驶员分心检测中的泛化问题。

Abstract: Driver distraction detection is essential for improving traffic safety and reducing road accidents. However, existing models often suffer from degraded generalization when deployed in real-world scenarios. This limitation primarily arises from the few-shot learning challenge caused by the high cost of data annotation in practical environments, as well as the substantial domain shift between training datasets and target deployment conditions. To address these issues, we propose a Pose-driven Quality-controlled Data Augmentation Framework (PQ-DAF) that leverages a vision-language model for sample filtering to cost-effectively expand training data and enhance cross-domain robustness. Specifically, we employ a Progressive Conditional Diffusion Model (PCDMs) to accurately capture key driver pose features and synthesize diverse training examples. A sample quality assessment module, built upon the CogVLM vision-language model, is then introduced to filter out low-quality synthetic samples based on a confidence threshold, ensuring the reliability of the augmented dataset. Extensive experiments demonstrate that PQ-DAF substantially improves performance in few-shot driver distraction detection, achieving significant gains in model generalization under data-scarce conditions.

</details>


### [7] [NanoControl: A Lightweight Framework for Precise and Efficient Control in Diffusion Transformer](https://arxiv.org/abs/2508.10424)
*Shanyuan Liu,Jian Zhu,Junda Lu,Yue Gong,Liuzhuozheng Li,Bo Cheng,Yuhang Ma,Liebucha Wu,Xiaoyu Wu,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: NanoControl提出了一种高效的DiT可控文本到图像生成方法，显著减少了参数和计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于ControlNet的方法在DiT中引入过多参数和计算成本，需要更高效的解决方案。

Method: 采用Flux作为主干网络，设计LoRA风格控制模块和KV-Context增强机制。

Result: 仅增加0.024%参数和0.029% GFLOPs，实现了高效且高质量的可控生成。

Conclusion: NanoControl在减少计算开销的同时保持了生成质量和可控性。

Abstract: Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in text-to-image synthesis. However, in the domain of controllable text-to-image generation using DiTs, most existing methods still rely on the ControlNet paradigm originally designed for UNet-based diffusion models. This paradigm introduces significant parameter overhead and increased computational costs. To address these challenges, we propose the Nano Control Diffusion Transformer (NanoControl), which employs Flux as the backbone network. Our model achieves state-of-the-art controllable text-to-image generation performance while incurring only a 0.024\% increase in parameter count and a 0.029\% increase in GFLOPs, thus enabling highly efficient controllable generation. Specifically, rather than duplicating the DiT backbone for control, we design a LoRA-style (low-rank adaptation) control module that directly learns control signals from raw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation mechanism that integrates condition-specific key-value information into the backbone in a simple yet highly effective manner, facilitating deep fusion of conditional features. Extensive benchmark experiments demonstrate that NanoControl significantly reduces computational overhead compared to conventional control approaches, while maintaining superior generation quality and achieving improved controllability.

</details>


### [8] [CRISP: Contrastive Residual Injection and Semantic Prompting for Continual Video Instance Segmentation](https://arxiv.org/abs/2508.10432)
*Baichen Liu,Qi Lyu,Xudong Wang,Jiahua Dong,Lianqing Liu,Zhi Han*

Main category: cs.CV

TL;DR: CRISP方法通过对比残差注入和语义提示，解决了持续视频实例分割中的实例级、类别级和任务级混淆问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决持续视频实例分割中因新类别引入导致的实例、类别和任务混淆问题，同时保持时间一致性。

Method: 提出实例相关损失、自适应残差语义提示框架和对比学习语义一致性损失，以及任务级初始化策略。

Result: 在YouTube-VIS数据集上显著优于现有方法，避免了灾难性遗忘，提升了分割和分类性能。

Conclusion: CRISP是解决持续视频实例分割问题的有效方法，代码已开源。

Abstract: Continual video instance segmentation demands both the plasticity to absorb new object categories and the stability to retain previously learned ones, all while preserving temporal consistency across frames. In this work, we introduce Contrastive Residual Injection and Semantic Prompting (CRISP), an earlier attempt tailored to address the instance-wise, category-wise, and task-wise confusion in continual video instance segmentation. For instance-wise learning, we model instance tracking and construct instance correlation loss, which emphasizes the correlation with the prior query space while strengthening the specificity of the current task query. For category-wise learning, we build an adaptive residual semantic prompt (ARSP) learning framework, which constructs a learnable semantic residual prompt pool generated by category text and uses an adjustive query-prompt matching mechanism to build a mapping relationship between the query of the current task and the semantic residual prompt. Meanwhile, a semantic consistency loss based on the contrastive learning is introduced to maintain semantic coherence between object queries and residual prompts during incremental training. For task-wise learning, to ensure the correlation at the inter-task level within the query space, we introduce a concise yet powerful initialization strategy for incremental prompts. Extensive experiments on YouTube-VIS-2019 and YouTube-VIS-2021 datasets demonstrate that CRISP significantly outperforms existing continual segmentation methods in the long-term continual video instance segmentation task, avoiding catastrophic forgetting and effectively improving segmentation and classification performance. The code is available at https://github.com/01upup10/CRISP.

</details>


### [9] [From Images to Perception: Emergence of Perceptual Properties by Reconstructing Images](https://arxiv.org/abs/2508.10450)
*Pablo Hernández-Cámara,Jesus Malo,Valero Laparra*

Main category: cs.CV

TL;DR: 论文提出了一种生物启发的架构PerceptNet，通过端到端优化完成图像重建任务，发现其编码器阶段（类似V1皮层）与人类感知判断高度相关，表明视觉系统可能针对特定失真水平进行优化。


<details>
  <summary>Details</summary>
Motivation: 探索人类视觉感知是否源于图像统计，以及生物启发模型能否无需人类监督学习感知指标。

Method: 设计并优化PerceptNet架构，用于图像重建任务（如自动编码、去噪、去模糊和稀疏正则化）。

Result: 编码器阶段与人类感知判断高度相关，且在中等噪声、模糊和稀疏水平下表现最佳。

Conclusion: 视觉系统可能针对特定失真水平优化，生物启发模型可无监督学习感知指标。

Abstract: A number of scientists suggested that human visual perception may emerge from image statistics, shaping efficient neural representations in early vision. In this work, a bio-inspired architecture that can accommodate several known facts in the retina-V1 cortex, the PerceptNet, has been end-to-end optimized for different tasks related to image reconstruction: autoencoding, denoising, deblurring, and sparsity regularization. Our results show that the encoder stage (V1-like layer) consistently exhibits the highest correlation with human perceptual judgments on image distortion despite not using perceptual information in the initialization or training. This alignment exhibits an optimum for moderate noise, blur and sparsity. These findings suggest that the visual system may be tuned to remove those particular levels of distortion with that level of sparsity and that biologically inspired models can learn perceptual metrics without human supervision.

</details>


### [10] [Trajectory-aware Shifted State Space Models for Online Video Super-Resolution](https://arxiv.org/abs/2508.10453)
*Qiang Zhu,Xiandong Meng,Yuxian Jiang,Fan Zhang,David Bull,Shuyuan Zhu,Bing Zeng*

Main category: cs.CV

TL;DR: 本文提出了一种基于轨迹感知的移位状态空间模型（TS-Mamba）的在线视频超分辨率方法，通过长时轨迹建模和低复杂度Mamba实现高效时空信息聚合。


<details>
  <summary>Details</summary>
Motivation: 现有在线视频超分辨率方法仅使用相邻前一帧进行时间对齐，限制了长时建模能力。状态空间模型（SSMs）因其线性计算复杂度和全局感受野而展现出潜力。

Method: TS-Mamba通过构建视频轨迹选择相似token，并利用轨迹感知的移位Mamba聚合模块（TSMA）进行信息聚合。移位SSMs块基于Hilbert扫描设计以补偿扫描损失。

Result: 在三个广泛使用的VSR测试数据集上，TS-Mamba在大多数情况下优于六个基准模型，计算复杂度降低22.7%。

Conclusion: TS-Mamba通过轨迹建模和移位SSMs实现了高效且高性能的在线视频超分辨率。

Abstract: Online video super-resolution (VSR) is an important technique for many real-world video processing applications, which aims to restore the current high-resolution video frame based on temporally previous frames. Most of the existing online VSR methods solely employ one neighboring previous frame to achieve temporal alignment, which limits long-range temporal modeling of videos. Recently, state space models (SSMs) have been proposed with linear computational complexity and a global receptive field, which significantly improve computational efficiency and performance. In this context, this paper presents a novel online VSR method based on Trajectory-aware Shifted SSMs (TS-Mamba), leveraging both long-term trajectory modeling and low-complexity Mamba to achieve efficient spatio-temporal information aggregation. Specifically, TS-Mamba first constructs the trajectories within a video to select the most similar tokens from the previous frames. Then, a Trajectory-aware Shifted Mamba Aggregation (TSMA) module consisting of proposed shifted SSMs blocks is employed to aggregate the selected tokens. The shifted SSMs blocks are designed based on Hilbert scannings and corresponding shift operations to compensate for scanning losses and strengthen the spatial continuity of Mamba. Additionally, we propose a trajectory-aware loss function to supervise the trajectory generation, ensuring the accuracy of token selection when training our model. Extensive experiments on three widely used VSR test datasets demonstrate that compared with six online VSR benchmark models, our TS-Mamba achieves state-of-the-art performance in most cases and over 22.7\% complexity reduction (in MACs). The source code for TS-Mamba will be available at https://github.com.

</details>


### [11] [TweezeEdit: Consistent and Efficient Image Editing with Path Regularization](https://arxiv.org/abs/2508.10498)
*Jianda Mao,Kaibo Wang,Yang Xiang,Kani Chen*

Main category: cs.CV

TL;DR: TweezeEdit是一种无需调优和反转的图像编辑框架，通过正则化整个去噪路径而非依赖反转锚点，实现高效的语义保留和目标对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法在编辑图像时过度依赖目标提示，导致源图像语义保留不足且编辑路径冗长。

Method: TweezeEdit通过梯度驱动的正则化，在一致性模型的引导下，直接注入目标提示语义，缩短编辑路径。

Result: 实验表明，TweezeEdit在语义保留和目标对齐上优于现有方法，仅需12步（1.6秒/次编辑）。

Conclusion: TweezeEdit为实时图像编辑提供了高效且语义保留的解决方案。

Abstract: Large-scale pre-trained diffusion models empower users to edit images through text guidance. However, existing methods often over-align with target prompts while inadequately preserving source image semantics. Such approaches generate target images explicitly or implicitly from the inversion noise of the source images, termed the inversion anchors. We identify this strategy as suboptimal for semantic preservation and inefficient due to elongated editing paths. We propose TweezeEdit, a tuning- and inversion-free framework for consistent and efficient image editing. Our method addresses these limitations by regularizing the entire denoising path rather than relying solely on the inversion anchors, ensuring source semantic retention and shortening editing paths. Guided by gradient-driven regularization, we efficiently inject target prompt semantics along a direct path using a consistency model. Extensive experiments demonstrate TweezeEdit's superior performance in semantic preservation and target alignment, outperforming existing methods. Remarkably, it requires only 12 steps (1.6 seconds per edit), underscoring its potential for real-time applications.

</details>


### [12] [Multi-Sample Anti-Aliasing and Constrained Optimization for 3D Gaussian Splatting](https://arxiv.org/abs/2508.10507)
*Zheng Zhou,Jia-Chen Zhang,Yu-Jie Xiong,Chun-Ming Xia*

Main category: cs.CV

TL;DR: 提出了一种结合多重采样抗锯齿（MSAA）和双重几何约束的优化框架，显著提升了3D高斯泼溅在细节重建和实时渲染中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅技术在优化过程中缺乏足够的几何约束，导致高频纹理和锐利不连续区域的细节模糊。

Method: 通过自适应混合四重子样本计算像素颜色，引入动态梯度分析的权重策略和梯度微分约束，优先优化关键区域。

Result: 在多个基准测试中，该方法在细节保留和实时渲染效率上达到最优性能，SSIM和LPIPS指标显著优于基线方法。

Conclusion: 提出的框架有效解决了高频纹理和锐利不连续区域的细节模糊问题，同时保持了实时渲染效率。

Abstract: Recent advances in 3D Gaussian splatting have significantly improved real-time novel view synthesis, yet insufficient geometric constraints during scene optimization often result in blurred reconstructions of fine-grained details, particularly in regions with high-frequency textures and sharp discontinuities. To address this, we propose a comprehensive optimization framework integrating multisample anti-aliasing (MSAA) with dual geometric constraints. Our system computes pixel colors through adaptive blending of quadruple subsamples, effectively reducing aliasing artifacts in high-frequency components. The framework introduces two constraints: (a) an adaptive weighting strategy that prioritizes under-reconstructed regions through dynamic gradient analysis, and (b) gradient differential constraints enforcing geometric regularization at object boundaries. This targeted optimization enables the model to allocate computational resources preferentially to critical regions requiring refinement while maintaining global consistency. Extensive experimental evaluations across multiple benchmarks demonstrate that our method achieves state-of-the-art performance in detail preservation, particularly in preserving high-frequency textures and sharp discontinuities, while maintaining real-time rendering efficiency. Quantitative metrics and perceptual studies confirm statistically significant improvements over baseline approaches in both structural similarity (SSIM) and perceptual quality (LPIPS).

</details>


### [13] [A Segmentation-driven Editing Method for Bolt Defect Augmentation and Detection](https://arxiv.org/abs/2508.10509)
*Yangjie Xiao,Ke Zhang,Jiacun Wang,Xin Sheng,Yurong Guo,Meijuan Chen,Zehua Ren,Zhaoye Zheng,Zhenbing Zhao*

Main category: cs.CV

TL;DR: 提出了一种基于分割的螺栓缺陷编辑方法（SBDE），通过增强数据集解决缺陷图像稀缺和数据不平衡问题，显著提升了螺栓缺陷检测性能。


<details>
  <summary>Details</summary>
Motivation: 螺栓缺陷检测对输电线路安全至关重要，但缺陷图像稀缺和数据分布不平衡限制了检测性能。

Method: 1. 提出Bolt-SAM模型，通过CLAHE-FFT适配器和多部分感知掩码解码器生成高质量掩码；2. 设计掩码优化模块（MOD）与图像修复模型（LaMa）结合，构建MOD-LaMa模型进行缺陷属性编辑；3. 提出编辑恢复增强（ERA）策略扩展数据集。

Result: 实验表明，SBDE生成的缺陷图像优于现有编辑模型，并显著提升缺陷检测性能。

Conclusion: SBDE方法有效解决了数据稀缺问题，具有实际应用潜力。

Abstract: Bolt defect detection is critical to ensure the safety of transmission lines. However, the scarcity of defect images and imbalanced data distributions significantly limit detection performance. To address this problem, we propose a segmentationdriven bolt defect editing method (SBDE) to augment the dataset. First, a bolt attribute segmentation model (Bolt-SAM) is proposed, which enhances the segmentation of complex bolt attributes through the CLAHE-FFT Adapter (CFA) and Multipart- Aware Mask Decoder (MAMD), generating high-quality masks for subsequent editing tasks. Second, a mask optimization module (MOD) is designed and integrated with the image inpainting model (LaMa) to construct the bolt defect attribute editing model (MOD-LaMa), which converts normal bolts into defective ones through attribute editing. Finally, an editing recovery augmentation (ERA) strategy is proposed to recover and put the edited defect bolts back into the original inspection scenes and expand the defect detection dataset. We constructed multiple bolt datasets and conducted extensive experiments. Experimental results demonstrate that the bolt defect images generated by SBDE significantly outperform state-of-the-art image editing models, and effectively improve the performance of bolt defect detection, which fully verifies the effectiveness and application potential of the proposed method. The code of the project is available at https://github.com/Jay-xyj/SBDE.

</details>


### [14] [HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head Synthesis](https://arxiv.org/abs/2508.10566)
*Shiyu Liu,Kui Jiang,Xianming Liu,Hongxun Yao,Xiaocheng Feng*

Main category: cs.CV

TL;DR: HM-Talker提出了一种结合隐式和显式运动特征的混合运动表示方法，通过跨模态解耦模块和混合运动建模模块，显著提升了说话头部视频的生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前音频驱动的说话头部视频生成方法存在运动模糊和唇部抖动问题，主要由于缺乏显式发音先验。

Method: HM-Talker结合隐式和显式运动特征，使用跨模态解耦模块提取互补特征，并通过混合运动建模模块增强跨主体泛化能力。

Result: 实验表明，HM-Talker在视觉质量和唇同步准确性上优于现有方法。

Conclusion: HM-Talker通过混合运动表示和身份无关学习，显著提升了说话头部视频的生成效果。

Abstract: Audio-driven talking head video generation enhances user engagement in human-computer interaction. However, current methods frequently produce videos with motion blur and lip jitter, primarily due to their reliance on implicit modeling of audio-facial motion correlations--an approach lacking explicit articulatory priors (i.e., anatomical guidance for speech-related facial movements). To overcome this limitation, we propose HM-Talker, a novel framework for generating high-fidelity, temporally coherent talking heads. HM-Talker leverages a hybrid motion representation combining both implicit and explicit motion cues. Explicit cues use Action Units (AUs), anatomically defined facial muscle movements, alongside implicit features to minimize phoneme-viseme misalignment. Specifically, our Cross-Modal Disentanglement Module (CMDM) extracts complementary implicit/explicit motion features while predicting AUs directly from audio input aligned to visual cues. To mitigate identity-dependent biases in explicit features and enhance cross-subject generalization, we introduce the Hybrid Motion Modeling Module (HMMM). This module dynamically merges randomly paired implicit/explicit features, enforcing identity-agnostic learning. Together, these components enable robust lip synchronization across diverse identities, advancing personalized talking head synthesis. Extensive experiments demonstrate HM-Talker's superiority over state-of-the-art methods in visual quality and lip-sync accuracy.

</details>


### [15] [SpaRC-AD: A Baseline for Radar-Camera Fusion in End-to-End Autonomous Driving](https://arxiv.org/abs/2508.10567)
*Philipp Wolters,Johannes Gilg,Torben Teepe,Gerhard Rigoll*

Main category: cs.CV

TL;DR: SpaRC-AD是一种基于查询的端到端相机-雷达融合框架，用于规划导向的自动驾驶，解决了视觉方法在恶劣天气、遮挡和速度估计中的局限性。


<details>
  <summary>Details</summary>
Motivation: 视觉方法在恶劣天气、部分遮挡和精确速度估计方面存在局限性，而自动驾驶在安全敏感场景中需要准确的运动理解和长时程轨迹预测。

Method: 通过稀疏3D特征对齐和基于多普勒的速度估计，优化了代理锚点、地图多段线和运动建模的3D场景表示。

Result: 在3D检测、多目标跟踪、在线地图、运动预测和轨迹规划等多个任务上优于现有视觉基线。

Conclusion: SpaRC-AD展示了雷达融合在安全关键场景中的有效性，特别是在运动理解和长时程轨迹预测方面。

Abstract: End-to-end autonomous driving systems promise stronger performance through unified optimization of perception, motion forecasting, and planning. However, vision-based approaches face fundamental limitations in adverse weather conditions, partial occlusions, and precise velocity estimation - critical challenges in safety-sensitive scenarios where accurate motion understanding and long-horizon trajectory prediction are essential for collision avoidance. To address these limitations, we propose SpaRC-AD, a query-based end-to-end camera-radar fusion framework for planning-oriented autonomous driving. Through sparse 3D feature alignment, and doppler-based velocity estimation, we achieve strong 3D scene representations for refinement of agent anchors, map polylines and motion modelling. Our method achieves strong improvements over the state-of-the-art vision-only baselines across multiple autonomous driving tasks, including 3D detection (+4.8% mAP), multi-object tracking (+8.3% AMOTA), online mapping (+1.8% mAP), motion prediction (-4.0% mADE), and trajectory planning (-0.1m L2 and -9% TPC). We achieve both spatial coherence and temporal consistency on multiple challenging benchmarks, including real-world open-loop nuScenes, long-horizon T-nuScenes, and closed-loop simulator Bench2Drive. We show the effectiveness of radar-based fusion in safety-critical scenarios where accurate motion understanding and long-horizon trajectory prediction are essential for collision avoidance. The source code of all experiments is available at https://phi-wol.github.io/sparcad/

</details>


### [16] [Fourier-Guided Attention Upsampling for Image Super-Resolution](https://arxiv.org/abs/2508.10616)
*Daejune Choi,Youchan No,Jinhyung Lee,Duksu Kim*

Main category: cs.CV

TL;DR: Frequency-Guided Attention (FGA) 是一种轻量级上采样模块，通过傅里叶特征和频率域损失提升超分辨率重建的高频细节和减少伪影。


<details>
  <summary>Details</summary>
Motivation: 传统上采样方法（如子像素卷积）在重建高频细节时表现不佳且易引入伪影，FGA旨在解决这些问题。

Method: FGA结合了傅里叶特征MLP、跨分辨率相关注意力层和频率域L1损失，以提升频谱保真度和空间对齐。

Result: 实验显示FGA在五种超分辨率骨干网络上平均PSNR提升0.12~0.14 dB，频率域一致性提升29%。

Conclusion: FGA是一种高效且可扩展的上采样替代方案，能有效减少伪影并保留细节。

Abstract: We propose Frequency-Guided Attention (FGA), a lightweight upsampling module for single image super-resolution. Conventional upsamplers, such as Sub-Pixel Convolution, are efficient but frequently fail to reconstruct high-frequency details and introduce aliasing artifacts. FGA addresses these issues by integrating (1) a Fourier feature-based Multi-Layer Perceptron (MLP) for positional frequency encoding, (2) a cross-resolution Correlation Attention Layer for adaptive spatial alignment, and (3) a frequency-domain L1 loss for spectral fidelity supervision. Adding merely 0.3M parameters, FGA consistently enhances performance across five diverse super-resolution backbones in both lightweight and full-capacity scenarios. Experimental results demonstrate average PSNR gains of 0.12~0.14 dB and improved frequency-domain consistency by up to 29%, particularly evident on texture-rich datasets. Visual and spectral evaluations confirm FGA's effectiveness in reducing aliasing and preserving fine details, establishing it as a practical, scalable alternative to traditional upsampling methods.

</details>


### [17] [ChatENV: An Interactive Vision-Language Model for Sensor-Guided Environmental Monitoring and Scenario Simulation](https://arxiv.org/abs/2508.10635)
*Hosam Elgendy,Ahmed Sharshar,Ahmed Aboeitta,Mohsen Guizani*

Main category: cs.CV

TL;DR: ChatENV是一个交互式视觉语言模型，结合卫星图像和传感器数据，用于环境监测和情景分析。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型忽视环境传感器的因果信号，依赖单一来源的标题，缺乏交互式情景推理。

Method: 创建包含177k图像的数据集，使用GPT-4和Gemini 2.0标注，并通过LoRA适配器微调Qwen-2.5-VL模型。

Result: 在时间推理和假设推理中表现优异（如BERT-F1 0.903），优于现有时间模型。

Conclusion: ChatENV是一个强大的工具，支持基于传感器的环境监测和交互式分析。

Abstract: Understanding environmental changes from aerial imagery is vital for climate resilience, urban planning, and ecosystem monitoring. Yet, current vision language models (VLMs) overlook causal signals from environmental sensors, rely on single-source captions prone to stylistic bias, and lack interactive scenario-based reasoning. We present ChatENV, the first interactive VLM that jointly reasons over satellite image pairs and real-world sensor data. Our framework: (i) creates a 177k-image dataset forming 152k temporal pairs across 62 land-use classes in 197 countries with rich sensor metadata (e.g., temperature, PM10, CO); (ii) annotates data using GPT- 4o and Gemini 2.0 for stylistic and semantic diversity; and (iii) fine-tunes Qwen-2.5-VL using efficient Low-Rank Adaptation (LoRA) adapters for chat purposes. ChatENV achieves strong performance in temporal and "what-if" reasoning (e.g., BERT-F1 0.903) and rivals or outperforms state-of-the-art temporal models, while supporting interactive scenario-based analysis. This positions ChatENV as a powerful tool for grounded, sensor-aware environmental monitoring.

</details>


### [18] [Hybrid Generative Fusion for Efficient and Privacy-Preserving Face Recognition Dataset Generation](https://arxiv.org/abs/2508.10672)
*Feiran Li,Qianqian Xu,Shilong Bao,Boyu Han,Zhiyong Yang,Qingming Huang*

Main category: cs.CV

TL;DR: 论文提出了一种构建高质量人脸数据集的方法，用于训练人脸识别模型，通过混合专家策略、数据增强和合成身份生成，最终在竞赛中获得第一名。


<details>
  <summary>Details</summary>
Motivation: 解决现有公共人脸数据集中身份重叠的问题，构建一个高质量且无身份泄漏的数据集。

Method: 结合Mixture-of-Experts策略清理数据，使用Stable Diffusion生成合成身份，并通过Vec2Face扩展，采用课程学习策略优化训练。

Result: 构建的数据集在10K、20K和100K身份规模上均提升了模型性能，并在竞赛中获得第一名。

Conclusion: 混合真实与合成数据的策略有效构建了高质量数据集，显著提升了人脸识别模型的性能。

Abstract: In this paper, we present our approach to the DataCV ICCV Challenge, which centers on building a high-quality face dataset to train a face recognition model. The constructed dataset must not contain identities overlapping with any existing public face datasets. To handle this challenge, we begin with a thorough cleaning of the baseline HSFace dataset, identifying and removing mislabeled or inconsistent identities through a Mixture-of-Experts (MoE) strategy combining face embedding clustering and GPT-4o-assisted verification. We retain the largest consistent identity cluster and apply data augmentation up to a fixed number of images per identity. To further diversify the dataset, we generate synthetic identities using Stable Diffusion with prompt engineering. As diffusion models are computationally intensive, we generate only one reference image per identity and efficiently expand it using Vec2Face, which rapidly produces 49 identity-consistent variants. This hybrid approach fuses GAN-based and diffusion-based samples, enabling efficient construction of a diverse and high-quality dataset. To address the high visual similarity among synthetic identities, we adopt a curriculum learning strategy by placing them early in the training schedule, allowing the model to progress from easier to harder samples. Our final dataset contains 50 images per identity, and all newly generated identities are checked with mainstream face datasets to ensure no identity leakage. Our method achieves \textbf{1st place} in the competition, and experimental results show that our dataset improves model performance across 10K, 20K, and 100K identity scales. Code is available at https://github.com/Ferry-Li/datacv_fr.

</details>


### [19] [Physics-Informed Joint Multi-TE Super-Resolution with Implicit Neural Representation for Robust Fetal T2 Mapping](https://arxiv.org/abs/2508.10680)
*Busra Bulut,Maik Dannecker,Thomas Sanchez,Sara Neves Silva,Vladyslav Zalevskyi,Steven Jia,Jean-Baptiste Ledoux,Guillaume Auzias,François Rousseau,Jana Hutter,Daniel Rueckert,Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: 该论文提出了一种联合重建跨TE数据的方法，用于胎儿脑MRI中的T2映射，以减少运动伪影和扫描时间。


<details>
  <summary>Details</summary>
Motivation: 胎儿脑MRI中的T2映射在0.55T中场强下具有潜力，但现有方法因运动伪影和长扫描时间而受限。

Method: 结合隐式神经表示和物理知识正则化，联合重建跨TE数据，以共享信息并保持T2定量精度。

Result: 在模拟胎儿脑和成人数据中表现优异，并首次在0.55T下实现活体胎儿T2映射。

Conclusion: 该方法通过利用解剖冗余，有望减少每个TE所需的堆栈数量，提高效率。

Abstract: T2 mapping in fetal brain MRI has the potential to improve characterization of the developing brain, especially at mid-field (0.55T), where T2 decay is slower. However, this is challenging as fetal MRI acquisition relies on multiple motion-corrupted stacks of thick slices, requiring slice-to-volume reconstruction (SVR) to estimate a high-resolution (HR) 3D volume. Currently, T2 mapping involves repeated acquisitions of these stacks at each echo time (TE), leading to long scan times and high sensitivity to motion. We tackle this challenge with a method that jointly reconstructs data across TEs, addressing severe motion. Our approach combines implicit neural representations with a physics-informed regularization that models T2 decay, enabling information sharing across TEs while preserving anatomical and quantitative T2 fidelity. We demonstrate state-of-the-art performance on simulated fetal brain and in vivo adult datasets with fetal-like motion. We also present the first in vivo fetal T2 mapping results at 0.55T. Our study shows potential for reducing the number of stacks per TE in T2 mapping by leveraging anatomical redundancy.

</details>


### [20] [Novel View Synthesis using DDIM Inversion](https://arxiv.org/abs/2508.10688)
*Sehajdeep SIngh,A V Subramanyam*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级的视图转换框架TUNet，利用预训练扩散模型的高保真生成能力，通过融合策略提升单图像新视图合成的清晰度和细节。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要微调大型扩散模型或从头训练，成本高且效果不佳（模糊重建和泛化能力差），因此探索一种轻量级框架以直接利用预训练模型的能力。

Method: 使用DDIM反演潜在空间，通过相机姿态条件转换U-Net（TUNet）预测目标视图的潜在表示，并提出融合策略以保留细节。

Result: 在MVImgNet上的实验表明，该方法优于现有方法，能够生成更清晰的合成视图。

Conclusion: 提出的轻量级框架和融合策略有效提升了单图像新视图合成的质量和效率。

Abstract: Synthesizing novel views from a single input image is a challenging task. It requires extrapolating the 3D structure of a scene while inferring details in occluded regions, and maintaining geometric consistency across viewpoints. Many existing methods must fine-tune large diffusion backbones using multiple views or train a diffusion model from scratch, which is extremely expensive. Additionally, they suffer from blurry reconstruction and poor generalization. This gap presents the opportunity to explore an explicit lightweight view translation framework that can directly utilize the high-fidelity generative capabilities of a pretrained diffusion model while reconstructing a scene from a novel view. Given the DDIM-inverted latent of a single input image, we employ a camera pose-conditioned translation U-Net, TUNet, to predict the inverted latent corresponding to the desired target view. However, the image sampled using the predicted latent may result in a blurry reconstruction. To this end, we propose a novel fusion strategy that exploits the inherent noise correlation structure observed in DDIM inversion. The proposed fusion strategy helps preserve the texture and fine-grained details. To synthesize the novel view, we use the fused latent as the initial condition for DDIM sampling, leveraging the generative prior of the pretrained diffusion model. Extensive experiments on MVImgNet demonstrate that our method outperforms existing methods.

</details>


### [21] [Beyond conventional vision: RGB-event fusion for robust object detection in dynamic traffic scenarios](https://arxiv.org/abs/2508.10704)
*Zhanwen Liu,Yujing Sun,Yang Wang,Nan Yang,Shengbo Eben Li,Xiangmo Zhao*

Main category: cs.CV

TL;DR: 论文提出了一种结合事件相机和RGB相机的方法（MCFNet），通过动态范围增强和跨模态特征融合，显著提升了复杂交通场景下的目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统RGB相机在动态范围上的限制导致复杂交通环境（如夜间驾驶、隧道）中全局对比度降低和高频细节丢失，影响目标检测性能。

Method: 提出MCFNet，包括事件校正模块（ECM）进行时间对齐，事件动态上采样模块（EDUM）提升空间分辨率，以及跨模态Mamba融合模块（CMM）实现自适应特征融合。

Result: 在DSEC-Det和PKU-DAVIS-SOD数据集上，MCFNet在低光照和快速移动场景中表现优异，mAP50和mAP分别提升7.4%和1.7%。

Conclusion: MCFNet通过结合事件相机和RGB相机，有效解决了动态范围限制问题，显著提升了复杂场景下的目标检测性能。

Abstract: The dynamic range limitation of conventional RGB cameras reduces global contrast and causes loss of high-frequency details such as textures and edges in complex traffic environments (e.g., nighttime driving, tunnels), hindering discriminative feature extraction and degrading frame-based object detection. To address this, we integrate a bio-inspired event camera with an RGB camera to provide high dynamic range information and propose a motion cue fusion network (MCFNet), which achieves optimal spatiotemporal alignment and adaptive cross-modal feature fusion under challenging lighting. Specifically, an event correction module (ECM) temporally aligns asynchronous event streams with image frames via optical-flow-based warping, jointly optimized with the detection network to learn task-aware event representations. The event dynamic upsampling module (EDUM) enhances spatial resolution of event frames to match image structures, ensuring precise spatiotemporal alignment. The cross-modal mamba fusion module (CMM) uses adaptive feature fusion with a novel interlaced scanning mechanism, effectively integrating complementary information for robust detection. Experiments conducted on the DSEC-Det and PKU-DAVIS-SOD datasets demonstrate that MCFNet significantly outperforms existing methods in various poor lighting and fast moving traffic scenarios. Notably, on the DSEC-Det dataset, MCFNet achieves a remarkable improvement, surpassing the best existing methods by 7.4% in mAP50 and 1.7% in mAP metrics, respectively. The code is available at https://github.com/Charm11492/MCFNet.

</details>


### [22] [CountCluster: Training-Free Object Quantity Guidance with Cross-Attention Map Clustering for Text-to-Image Generation](https://arxiv.org/abs/2508.10710)
*Joohyeon Lee,Jin-Seop Lee,Jee-Hyong Lee*

Main category: cs.CV

TL;DR: 论文提出了一种名为CountCluster的方法，通过优化对象交叉注意力图，在生成图像时更准确地反映输入提示中指定的对象数量。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在生成图像时难以准确反映输入提示中的对象数量，且现有方法依赖外部模块或忽略早期去噪步骤的关键作用。

Method: CountCluster通过聚类对象交叉注意力图，定义理想分布并优化潜在变量，无需外部工具或额外训练。

Result: 方法在对象数量准确性上平均提升了18.5%，并在多种提示下表现出优越的数量控制性能。

Conclusion: CountCluster通过早期去噪步骤的优化，显著提升了生成图像中对象数量的准确性。

Abstract: Diffusion-based text-to-image generation models have demonstrated strong performance in terms of image quality and diversity. However, they still struggle to generate images that accurately reflect the number of objects specified in the input prompt. Several approaches have been proposed that rely on either external counting modules for iterative refinement or quantity representations derived from learned tokens or latent features. However, they still have limitations in accurately reflecting the specified number of objects and overlook an important structural characteristic--The number of object instances in the generated image is largely determined in the early timesteps of the denoising process. To correctly reflect the object quantity for image generation, the highly activated regions in the object cross-attention map at the early timesteps should match the input object quantity, while each region should be clearly separated. To address this issue, we propose \textit{CountCluster}, a method that guides the object cross-attention map to be clustered according to the specified object count in the input, without relying on any external tools or additional training. The proposed method partitions the object cross-attention map into $k$ clusters at inference time based on attention scores, defines an ideal distribution in which each cluster is spatially well-separated, and optimizes the latent to align with this target distribution. Our method achieves an average improvement of 18.5\%p in object count accuracy compared to existing methods, and demonstrates superior quantity control performance across a variety of prompts. Code will be released at: https://github.com/JoohyeonL22/CountCluster .

</details>


### [23] [NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale](https://arxiv.org/abs/2508.10711)
*NextStep Team,Chunrui Han,Guopeng Li,Jingwei Wu,Quan Sun,Yan Cai,Yuang Peng,Zheng Ge,Deyu Zhou,Haomiao Tang,Hongyu Zhou,Kenkun Liu,Ailin Huang,Bin Wang,Changxin Miao,Deshan Sun,En Yu,Fukun Yin,Gang Yu,Hao Nie,Haoran Lv,Hanpeng Hu,Jia Wang,Jian Zhou,Jianjian Sun,Kaijun Tan,Kang An,Kangheng Lin,Liang Zhao,Mei Chen,Peng Xing,Rui Wang,Shiyu Liu,Shutao Xia,Tianhao You,Wei Ji,Xianfang Zeng,Xin Han,Xuelin Zhang,Yana Wei,Yanming Xu,Yimin Jiang,Yingming Wang,Yu Zhou,Yucheng Han,Ziyang Meng,Binxing Jiao,Daxin Jiang,Xiangyu Zhang,Yibo Zhu*

Main category: cs.CV

TL;DR: NextStep-1是一种14B自回归模型，结合157M流匹配头，用于文本到图像生成任务，性能优异且支持图像编辑。


<details>
  <summary>Details</summary>
Motivation: 解决现有自回归模型在文本到图像生成中依赖高计算量的扩散模型或量化损失的问题。

Method: 使用离散文本标记和连续图像标记，通过下一标记预测目标训练模型。

Result: 在文本到图像生成任务中达到最先进性能，支持高保真图像合成和编辑。

Conclusion: NextStep-1展示了统一方法的强大和多功能性，代码和模型将开源。

Abstract: Prevailing autoregressive (AR) models for text-to-image generation either rely on heavy, computationally-intensive diffusion models to process continuous image tokens, or employ vector quantization (VQ) to obtain discrete tokens with quantization loss. In this paper, we push the autoregressive paradigm forward with NextStep-1, a 14B autoregressive model paired with a 157M flow matching head, training on discrete text tokens and continuous image tokens with next-token prediction objectives. NextStep-1 achieves state-of-the-art performance for autoregressive models in text-to-image generation tasks, exhibiting strong capabilities in high-fidelity image synthesis. Furthermore, our method shows strong performance in image editing, highlighting the power and versatility of our unified approach. To facilitate open research, we will release our code and models to the community.

</details>


### [24] [Exploiting Discriminative Codebook Prior for Autoregressive Image Generation](https://arxiv.org/abs/2508.10719)
*Longxiang Tang,Ruihang Chu,Xiang Wang,Yujin Han,Pingyu Wu,Chunming He,Yingya Zhang,Shiwei Zhang,Jiaya Jia*

Main category: cs.CV

TL;DR: 论文提出了一种名为DCPE的方法，用于替代k-means聚类，更有效地利用代码本中的令牌相似性信息，提升自回归模型的训练效率和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归生成模型仅利用索引值训练，忽略了代码本中丰富的令牌相似性信息。k-means聚类在代码本特征空间中表现不佳，存在令牌空间差异和质心距离不准确等问题。

Method: 提出DCPE方法，采用基于实例的距离替代质心距离，并通过凝聚合并技术解决令牌空间差异问题。

Result: DCPE在LlamaGen-B上加速训练42%，并提高了FID和IS性能。

Conclusion: DCPE是一种即插即用的方法，能有效利用代码本先验，提升自回归模型的训练效率和生成质量。

Abstract: Advanced discrete token-based autoregressive image generation systems first tokenize images into sequences of token indices with a codebook, and then model these sequences in an autoregressive paradigm. While autoregressive generative models are trained only on index values, the prior encoded in the codebook, which contains rich token similarity information, is not exploited. Recent studies have attempted to incorporate this prior by performing naive k-means clustering on the tokens, helping to facilitate the training of generative models with a reduced codebook. However, we reveal that k-means clustering performs poorly in the codebook feature space due to inherent issues, including token space disparity and centroid distance inaccuracy. In this work, we propose the Discriminative Codebook Prior Extractor (DCPE) as an alternative to k-means clustering for more effectively mining and utilizing the token similarity information embedded in the codebook. DCPE replaces the commonly used centroid-based distance, which is found to be unsuitable and inaccurate for the token feature space, with a more reasonable instance-based distance. Using an agglomerative merging technique, it further addresses the token space disparity issue by avoiding splitting high-density regions and aggregating low-density ones. Extensive experiments demonstrate that DCPE is plug-and-play and integrates seamlessly with existing codebook prior-based paradigms. With the discriminative prior extracted, DCPE accelerates the training of autoregressive models by 42% on LlamaGen-B and improves final FID and IS performance.

</details>


### [25] [Video-BLADE: Block-Sparse Attention Meets Step Distillation for Efficient Video Generation](https://arxiv.org/abs/2508.10774)
*Youping Gu,Xiaolong Li,Yuhao Hu,Bohan Zhuang*

Main category: cs.CV

TL;DR: BLADE提出了一种数据无关的联合训练框架，结合自适应块稀疏注意力机制和稀疏感知的步蒸馏方法，显著加速视频生成模型的推理速度，同时保持或提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前扩散变换器在高质量视频生成中表现优异，但其迭代去噪过程缓慢且长序列的二次注意力成本高，导致推理瓶颈。

Method: BLADE引入自适应块稀疏注意力机制（ASA）动态生成内容感知的稀疏掩码，并结合基于轨迹分布匹配（TDM）的稀疏感知步蒸馏方法。

Result: 在CogVideoX-5B和Wan2.1-1.3B模型上，BLADE分别实现了8.89x和14.10x的推理加速，并在VBench-2.0基准测试中提升了生成质量。

Conclusion: BLADE通过创新框架有效解决了视频生成模型的推理效率问题，同时提升了生成质量，具有广泛的应用潜力。

Abstract: Diffusion transformers currently lead the field in high-quality video generation, but their slow iterative denoising process and prohibitive quadratic attention costs for long sequences create significant inference bottlenecks. While both step distillation and sparse attention mechanisms have shown promise as independent acceleration strategies, effectively combining these approaches presents critical challenges -- training-free integration yields suboptimal results, while separately training sparse attention after step distillation requires prohibitively expensive high-quality video data. To overcome these limitations, we propose BLADE, an innovative data-free joint training framework that introduces: (1) an Adaptive Block-Sparse Attention (ASA) mechanism for dynamically generating content-aware sparsity masks to focus computation on salient spatiotemporal features, and (2) a sparsity-aware step distillation paradigm built upon Trajectory Distribution Matching (TDM) that directly incorporates sparsity into the distillation process rather than treating it as a separate compression step, with fast convergence. We validate BLADE on text-to-video models like CogVideoX-5B and Wan2.1-1.3B. Our framework demonstrates remarkable efficiency gains across different scales. On Wan2.1-1.3B, BLADE achieves a 14.10x end-to-end inference acceleration over a 50-step baseline. Moreover, on models such as CogVideoX-5B with short video sequence lengths, our framework delivers a robust 8.89x speedup. Crucially, the acceleration is accompanied by a consistent quality improvement. On the VBench-2.0 benchmark, BLADE boosts the score of CogVideoX-5B to 0.569 (from 0.534) and Wan2.1-1.3B to 0.570 (from 0.563), results that are further corroborated by superior ratings in human evaluations. Our code and model weights are publicly available at: http://ziplab.co/BLADE-Homepage/.

</details>


### [26] [Ultra-High-Definition Reference-Based Landmark Image Super-Resolution with Generative Diffusion Prior](https://arxiv.org/abs/2508.10779)
*Zhenning Shi,Zizheng Yan,Yuhang Yu,Clara Xue,Jingyu Zhuang,Qi Zhang,Jinwei Chen,Tao Li,Qingnan Fan*

Main category: cs.CV

TL;DR: 提出TriFlowSR框架，通过显式模式匹配解决LR图像与参考HR图像对齐问题，并引入首个UHD地标场景RefSR数据集Landmark-4K。


<details>
  <summary>Details</summary>
Motivation: 现有基于ControlNet的RefSR方法难以有效对齐LR与参考HR图像信息，且现有数据集分辨率低、质量差，无法支持高质量恢复。

Method: 设计TriFlowSR框架，采用参考匹配策略，实现LR与参考HR图像的有效匹配。

Result: 实验表明，TriFlowSR能更好地利用参考HR图像的语义和纹理信息，优于现有方法。

Conclusion: TriFlowSR是首个针对UHD地标场景的基于扩散的RefSR方法，解决了真实世界退化问题。

Abstract: Reference-based Image Super-Resolution (RefSR) aims to restore a low-resolution (LR) image by utilizing the semantic and texture information from an additional reference high-resolution (reference HR) image. Existing diffusion-based RefSR methods are typically built upon ControlNet, which struggles to effectively align the information between the LR image and the reference HR image. Moreover, current RefSR datasets suffer from limited resolution and poor image quality, resulting in the reference images lacking sufficient fine-grained details to support high-quality restoration. To overcome the limitations above, we propose TriFlowSR, a novel framework that explicitly achieves pattern matching between the LR image and the reference HR image. Meanwhile, we introduce Landmark-4K, the first RefSR dataset for Ultra-High-Definition (UHD) landmark scenarios. Considering the UHD scenarios with real-world degradation, in TriFlowSR, we design a Reference Matching Strategy to effectively match the LR image with the reference HR image. Experimental results show that our approach can better utilize the semantic and texture information of the reference HR image compared to previous methods. To the best of our knowledge, we propose the first diffusion-based RefSR pipeline for ultra-high definition landmark scenarios under real-world degradation. Our code and model will be available at https://github.com/nkicsl/TriFlowSR.

</details>


### [27] [Object Fidelity Diffusion for Remote Sensing Image Generation](https://arxiv.org/abs/2508.10801)
*Ziqi Ye,Shuran Ma,Jie Yang,Xiaoyi Yang,Ziyang Gong,Xue Yang,Haipeng Wang*

Main category: cs.CV

TL;DR: OF-Diff通过提取对象先验形状和双分支扩散模型，显著提升了遥感图像生成的保真度，并在多项指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在遥感图像生成中保真度不足，影响目标检测模型的鲁棒性和可靠性。

Method: 提出OF-Diff，首次基于布局提取对象先验形状，引入双分支扩散模型和扩散一致性损失，无需真实图像即可生成高保真图像，并通过DDPO优化扩散过程。

Result: 实验表明OF-Diff在多项关键指标上优于现有方法，例如飞机、船只和车辆的mAP分别提高了8.3%、7.7%和4.0%。

Conclusion: OF-Diff有效提升了遥感图像生成的保真度和多样性，尤其在小目标和多形态对象上表现突出。

Abstract: High-precision controllable remote sensing image generation is both meaningful and challenging. Existing diffusion models often produce low-fidelity images due to their inability to adequately capture morphological details, which may affect the robustness and reliability of object detection models. To enhance the accuracy and fidelity of generated objects in remote sensing, this paper proposes Object Fidelity Diffusion (OF-Diff), which effectively improves the fidelity of generated objects. Specifically, we are the first to extract the prior shapes of objects based on the layout for diffusion models in remote sensing. Then, we introduce a dual-branch diffusion model with diffusion consistency loss, which can generate high-fidelity remote sensing images without providing real images during the sampling phase. Furthermore, we introduce DDPO to fine-tune the diffusion process, making the generated remote sensing images more diverse and semantically consistent. Comprehensive experiments demonstrate that OF-Diff outperforms state-of-the-art methods in the remote sensing across key quality metrics. Notably, the performance of several polymorphic and small object classes shows significant improvement. For instance, the mAP increases by 8.3%, 7.7%, and 4.0% for airplanes, ships, and vehicles, respectively.

</details>


### [28] [Hierarchical Fine-grained Preference Optimization for Physically Plausible Video Generation](https://arxiv.org/abs/2508.10858)
*Harold Haodong Chen,Haojian Huang,Qifeng Chen,Harry Yang,Ser-Nam Lim*

Main category: cs.CV

TL;DR: PhysHPO提出了一种分层跨模态直接偏好优化框架，通过四个层次的对齐提升视频生成的物理合理性。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成技术难以满足物理规律的要求，限制了真实感和准确性。

Method: PhysHPO通过实例、状态、运动和语义四个层次的对齐优化视频生成，并引入自动化数据选择流程。

Result: 实验表明，PhysHPO显著提升了视频生成的物理合理性和整体质量。

Conclusion: PhysHPO为更真实和符合人类偏好的视频生成提供了新思路。

Abstract: Recent advancements in video generation have enabled the creation of high-quality, visually compelling videos. However, generating videos that adhere to the laws of physics remains a critical challenge for applications requiring realism and accuracy. In this work, we propose PhysHPO, a novel framework for Hierarchical Cross-Modal Direct Preference Optimization, to tackle this challenge by enabling fine-grained preference alignment for physically plausible video generation. PhysHPO optimizes video alignment across four hierarchical granularities: a) Instance Level, aligning the overall video content with the input prompt; b) State Level, ensuring temporal consistency using boundary frames as anchors; c) Motion Level, modeling motion trajectories for realistic dynamics; and d) Semantic Level, maintaining logical consistency between narrative and visuals. Recognizing that real-world videos are the best reflections of physical phenomena, we further introduce an automated data selection pipeline to efficiently identify and utilize "good data" from existing large-scale text-video datasets, thereby eliminating the need for costly and time-intensive dataset construction. Extensive experiments on both physics-focused and general capability benchmarks demonstrate that PhysHPO significantly improves physical plausibility and overall video generation quality of advanced models. To the best of our knowledge, this is the first work to explore fine-grained preference alignment and data selection for video generation, paving the way for more realistic and human-preferred video generation paradigms.

</details>


### [29] [TexVerse: A Universe of 3D Objects with High-Resolution Textures](https://arxiv.org/abs/2508.10868)
*Yibo Zhang,Li Zhang,Rui Ma,Nan Cao*

Main category: cs.CV

TL;DR: TexVerse是一个大规模高分辨率纹理的3D数据集，填补了现有数据集中高分辨率纹理生成的空白。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模3D数据集在高分辨率几何生成方面有所进展，但高分辨率纹理的端到端生成仍缺乏合适的数据集支持。

Method: TexVerse从Sketchfab收集了超过858K个独特的高分辨率3D模型，包括158K个支持PBR材质的模型，总计1.6M实例，并提供详细的模型注释。

Result: 数据集包含TexVerse-Skeleton（69K绑定模型）和TexVerse-Animation（54K动画模型），支持纹理合成、PBR材质开发等应用。

Conclusion: TexVerse为纹理合成、3D视觉和图形任务提供了高质量的数据资源。

Abstract: We introduce TexVerse, a large-scale 3D dataset featuring high-resolution textures. While recent advances in large-scale 3D datasets have enhanced high-resolution geometry generation, creating high-resolution textures end-to-end remains underexplored due to the lack of suitable datasets. TexVerse fills this gap with a curated collection of over 858K unique high-resolution 3D models sourced from Sketchfab, including more than 158K models with physically based rendering (PBR) materials. Each model encompasses all of its high-resolution variants, bringing the total to 1.6M 3D instances. TexVerse also includes specialized subsets: TexVerse-Skeleton, with 69K rigged models, and TexVerse-Animation, with 54K animated models, both preserving original skeleton and animation data uploaded by the user. We also provide detailed model annotations describing overall characteristics, structural components, and intricate features. TexVerse offers a high-quality data resource with wide-ranging potential applications in texture synthesis, PBR material development, animation, and various 3D vision and graphics tasks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [30] [Invisible Watermarks, Visible Gains: Steering Machine Unlearning with Bi-Level Watermarking Design](https://arxiv.org/abs/2508.10065)
*Yuhao Sun,Yihua Zhang,Gaowen Liu,Hongtao Xie,Sijia Liu*

Main category: cs.CR

TL;DR: 论文提出了一种基于数字水印的机器遗忘（MU）新方法Water4MU，通过数据级调整提升遗忘效果，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着‘被遗忘权’需求增加，机器遗忘成为提升信任和合规性的重要工具，但现有方法多依赖模型权重调整，数据级调整潜力未被充分探索。

Method: 提出Water4MU框架，结合数字水印和双层优化（BLO），上层优化水印网络以降低遗忘难度，下层独立训练模型。

Result: 实验证明Water4MU在图像分类和生成任务中均有效，尤其在‘挑战性遗忘’场景中表现优异。

Conclusion: Water4MU通过数据级调整显著提升机器遗忘效果，为敏感数据移除提供了新思路。

Abstract: With the increasing demand for the right to be forgotten, machine unlearning (MU) has emerged as a vital tool for enhancing trust and regulatory compliance by enabling the removal of sensitive data influences from machine learning (ML) models. However, most MU algorithms primarily rely on in-training methods to adjust model weights, with limited exploration of the benefits that data-level adjustments could bring to the unlearning process. To address this gap, we propose a novel approach that leverages digital watermarking to facilitate MU by strategically modifying data content. By integrating watermarking, we establish a controlled unlearning mechanism that enables precise removal of specified data while maintaining model utility for unrelated tasks. We first examine the impact of watermarked data on MU, finding that MU effectively generalizes to watermarked data. Building on this, we introduce an unlearning-friendly watermarking framework, termed Water4MU, to enhance unlearning effectiveness. The core of Water4MU is a bi-level optimization (BLO) framework: at the upper level, the watermarking network is optimized to minimize unlearning difficulty, while at the lower level, the model itself is trained independently of watermarking. Experimental results demonstrate that Water4MU is effective in MU across both image classification and image generation tasks. Notably, it outperforms existing methods in challenging MU scenarios, known as "challenging forgets".

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [31] [Geospatial Diffusion for Land Cover Imperviousness Change Forecasting](https://arxiv.org/abs/2508.10649)
*Debvrat Varshney,Vibhas Vats,Bhartendu Pandey,Christa Brelsford,Philipe Dias*

Main category: cs.LG

TL;DR: 论文提出了一种利用生成式AI（GenAI）预测土地覆盖变化的新方法，通过将土地覆盖变化预测视为基于历史和辅助数据的数据合成问题。实验表明，该方法在预测不透水表面变化方面优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 土地覆盖变化对地球系统过程有重要影响，但现有模型在预测土地覆盖变化方面能力不足。

Method: 将土地覆盖变化预测视为数据合成问题，利用扩散模型进行十年尺度的不透水表面预测。

Result: 在12个大都市区的实验中，模型在分辨率≥0.7×0.7km²时，MAE低于基线模型。

Conclusion: 生成式模型能有效捕捉历史数据中的时空模式，未来研究将整合更多地球物理属性和驱动变量。

Abstract: Land cover, both present and future, has a significant effect on several important Earth system processes. For example, impervious surfaces heat up and speed up surface water runoff and reduce groundwater infiltration, with concomitant effects on regional hydrology and flood risk. While regional Earth System models have increasing skill at forecasting hydrologic and atmospheric processes at high resolution in future climate scenarios, our ability to forecast land-use and land-cover change (LULC), a critical input to risk and consequences assessment for these scenarios, has lagged behind. In this paper, we propose a new paradigm exploiting Generative AI (GenAI) for land cover change forecasting by framing LULC forecasting as a data synthesis problem conditioned on historical and auxiliary data-sources. We discuss desirable properties of generative models that fundament our research premise, and demonstrate the feasibility of our methodology through experiments on imperviousness forecasting using historical data covering the entire conterminous United States. Specifically, we train a diffusion model for decadal forecasting of imperviousness and compare its performance to a baseline that assumes no change at all. Evaluation across 12 metropolitan areas for a year held-out during training indicate that for average resolutions $\geq 0.7\times0.7km^2$ our model yields MAE lower than such a baseline. This finding corroborates that such a generative model can capture spatiotemporal patterns from historical data that are significant for projecting future change. Finally, we discuss future research to incorporate auxiliary information on physical properties about the Earth, as well as supporting simulation of different scenarios by means of driver variables.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [32] [DINOMotion: advanced robust tissue motion tracking with DINOv2 in 2D-Cine MRI-guided radiotherapy](https://arxiv.org/abs/2508.10260)
*Soorena Salari,Catherine Spino,Laurie-Anne Pharand,Fabienne Lathuiliere,Hassan Rivaz,Silvain Beriault,Yiming Xiao*

Main category: eess.IV

TL;DR: DINOMotion是一种基于DINOv2和LoRA层的新型深度学习框架，用于2D-Cine MRI引导放疗中的运动跟踪，具有高效、鲁棒和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在大偏差和可解释性方面存在不足，需要一种更鲁棒且直观的运动跟踪解决方案。

Method: 结合DINOv2的特征表示和LoRA层的参数优化，通过自动检测标志点实现图像配准。

Result: 在肾脏、肝脏和肺部的实验中，Dice分数分别为92.07%、90.90%和95.23%，处理时间约为30ms/扫描。

Conclusion: DINOMotion在实时运动跟踪中表现出色，尤其在处理大偏差时优于现有方法。

Abstract: Accurate tissue motion tracking is critical to ensure treatment outcome and safety in 2D-Cine MRI-guided radiotherapy. This is typically achieved by registration of sequential images, but existing methods often face challenges with large misalignments and lack of interpretability. In this paper, we introduce DINOMotion, a novel deep learning framework based on DINOv2 with Low-Rank Adaptation (LoRA) layers for robust, efficient, and interpretable motion tracking. DINOMotion automatically detects corresponding landmarks to derive optimal image registration, enhancing interpretability by providing explicit visual correspondences between sequential images. The integration of LoRA layers reduces trainable parameters, improving training efficiency, while DINOv2's powerful feature representations offer robustness against large misalignments. Unlike iterative optimization-based methods, DINOMotion directly computes image registration at test time. Our experiments on volunteer and patient datasets demonstrate its effectiveness in estimating both linear and nonlinear transformations, achieving Dice scores of 92.07% for the kidney, 90.90% for the liver, and 95.23% for the lung, with corresponding Hausdorff distances of 5.47 mm, 8.31 mm, and 6.72 mm, respectively. DINOMotion processes each scan in approximately 30ms and consistently outperforms state-of-the-art methods, particularly in handling large misalignments. These results highlight its potential as a robust and interpretable solution for real-time motion tracking in 2D-Cine MRI-guided radiotherapy.

</details>


### [33] [Efficient Image Denoising Using Global and Local Circulant Representation](https://arxiv.org/abs/2508.10307)
*Zhaoming Kong,Jiahuan Zhang,Xiaowei Yang*

Main category: eess.IV

TL;DR: 提出了一种基于Haar变换和t-SVD的图像去噪算法Haar-tSVD，通过统一投影捕捉全局和局部相关性，实现高效并行化去噪。


<details>
  <summary>Details</summary>
Motivation: 随着成像设备的发展和海量图像数据的生成，高效且有效的图像去噪需求日益增长。

Method: 结合Haar变换和t-SVD，利用非局部自相似性先验，通过统一投影捕捉全局和局部相关性，无需学习局部基。引入基于CNN的自适应噪声估计方案。

Result: 实验验证了Haar-tSVD在去噪和细节保留方面的高效性和有效性。

Conclusion: Haar-tSVD在去噪速度和性能之间取得了平衡，适用于实际去噪任务。

Abstract: The advancement of imaging devices and countless image data generated everyday impose an increasingly high demand on efficient and effective image denoising. In this paper, we present a computationally simple denoising algorithm, termed Haar-tSVD, aiming to explore the nonlocal self-similarity prior and leverage the connection between principal component analysis (PCA) and the Haar transform under circulant representation. We show that global and local patch correlations can be effectively captured through a unified tensor-singular value decomposition (t-SVD) projection with the Haar transform. This results in a one-step, highly parallelizable filtering method that eliminates the need for learning local bases to represent image patches, striking a balance between denoising speed and performance. Furthermore, we introduce an adaptive noise estimation scheme based on a CNN estimator and eigenvalue analysis to enhance the robustness and adaptability of the proposed method. Experiments on different real-world denoising tasks validate the efficiency and effectiveness of Haar-tSVD for noise removal and detail preservation. Datasets, code and results are publicly available at https://github.com/ZhaomingKong/Haar-tSVD.

</details>
