{"id": "2508.07240", "pdf": "https://arxiv.org/pdf/2508.07240", "abs": "https://arxiv.org/abs/2508.07240", "authors": ["Zixuan Li", "Zixiong Wang", "Jian Yang", "Milos Hasan", "Beibei Wang"], "title": "PureSample: Neural Materials Learned by Sampling Microgeometry", "categories": ["cs.GR"], "comment": null, "summary": "Traditional physically-based material models rely on analytically derived bidirectional reflectance distribution functions (BRDFs), typically by considering statistics of micro-primitives such as facets, flakes, or spheres, sometimes combined with multi-bounce interactions such as layering and multiple scattering. These derivations are often complex and model-specific, and typically consider a statistical aggregate of a large surface area, ignoring spatial variation. Once an analytic BRDF's evaluation is defined, one still needs to design an importance sampling method for it, and a way to evaluate the pdf of that sampling distribution, requiring further model-specific derivations.   We present PureSample: a novel neural BRDF representation that allows learning a material's behavior purely by sampling forward random walks on the microgeometry, which is usually straightforward to implement. Our representation allows for efficient importance sampling, pdf evaluation, and BRDF evaluation, for homogeneous as well as spatially varying materials.   We achieve this by two learnable components: first, the sampling distribution is modeled using a flow matching neural network, which allows both importance sampling and pdf evaluation; second, we introduce a view-dependent albedo term, captured by a lightweight neural network, which allows for converting a scalar pdf value to a colored BRDF value for any pair of view and light directions.   We demonstrate PureSample on challenging materials, including multi-layered materials, multiple-scattering microfacet materials, and various other microstructures.", "AI": {"tldr": "PureSample\u662f\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684BRDF\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u968f\u673a\u884c\u8d70\u91c7\u6837\u5b66\u4e60\u6750\u6599\u884c\u4e3a\uff0c\u7b80\u5316\u4e86\u4f20\u7edfBRDF\u6a21\u578b\u7684\u590d\u6742\u6027\u3002", "motivation": "\u4f20\u7edfBRDF\u6a21\u578b\u4f9d\u8d56\u590d\u6742\u7684\u89e3\u6790\u63a8\u5bfc\uff0c\u4e14\u96be\u4ee5\u5904\u7406\u7a7a\u95f4\u53d8\u5316\u6750\u6599\u3002PureSample\u65e8\u5728\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "PureSample\u5305\u542b\u4e24\u4e2a\u53ef\u5b66\u4e60\u7ec4\u4ef6\uff1a\u6d41\u5339\u914d\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\u91c7\u6837\u5206\u5e03\uff0c\u4ee5\u53ca\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u89c6\u89d2\u76f8\u5173\u53cd\u7167\u7387\u3002", "result": "PureSample\u5728\u591a\u5c42\u6750\u6599\u3001\u591a\u6b21\u6563\u5c04\u5fae\u8868\u9762\u6750\u6599\u7b49\u591a\u79cd\u590d\u6742\u6750\u6599\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "PureSample\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u7684BRDF\u8868\u793a\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5747\u8d28\u548c\u975e\u5747\u8d28\u6750\u6599\u3002"}}
{"id": "2508.07852", "pdf": "https://arxiv.org/pdf/2508.07852", "abs": "https://arxiv.org/abs/2508.07852", "authors": ["Rui Su", "Honghao Dong", "Haojie Jin", "Yisong Chen", "Guoping Wang", "Sheng Li"], "title": "Vertex Features for Neural Global Illumination", "categories": ["cs.GR", "cs.AI"], "comment": "Accepted by ACM SIGGRAPH Asia'2025", "summary": "Recent research on learnable neural representations has been widely adopted in the field of 3D scene reconstruction and neural rendering applications. However, traditional feature grid representations often suffer from substantial memory footprint, posing a significant bottleneck for modern parallel computing hardware. In this paper, we present neural vertex features, a generalized formulation of learnable representation for neural rendering tasks involving explicit mesh surfaces. Instead of uniformly distributing neural features throughout 3D space, our method stores learnable features directly at mesh vertices, leveraging the underlying geometry as a compact and structured representation for neural processing. This not only optimizes memory efficiency, but also improves feature representation by aligning compactly with the surface using task-specific geometric priors. We validate our neural representation across diverse neural rendering tasks, with a specific emphasis on neural radiosity. Experimental results demonstrate that our method reduces memory consumption to only one-fifth (or even less) of grid-based representations, while maintaining comparable rendering quality and lowering inference overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f51\u683c\u9876\u70b9\u7684\u53ef\u5b66\u4e60\u795e\u7ecf\u8868\u793a\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u5360\u7528\u5e76\u4fdd\u6301\u4e86\u6e32\u67d3\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u7279\u5f81\u7f51\u683c\u8868\u793a\u57283D\u573a\u666f\u91cd\u5efa\u548c\u795e\u7ecf\u6e32\u67d3\u4e2d\u5185\u5b58\u5360\u7528\u9ad8\uff0c\u9650\u5236\u4e86\u5e76\u884c\u8ba1\u7b97\u786c\u4ef6\u7684\u6548\u7387\u3002", "method": "\u5c06\u53ef\u5b66\u4e60\u7279\u5f81\u76f4\u63a5\u5b58\u50a8\u5728\u7f51\u683c\u9876\u70b9\u4e0a\uff0c\u5229\u7528\u51e0\u4f55\u7ed3\u6784\u4f5c\u4e3a\u7d27\u51d1\u8868\u793a\uff0c\u4f18\u5316\u5185\u5b58\u6548\u7387\u5e76\u63d0\u5347\u7279\u5f81\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5185\u5b58\u6d88\u8017\u4ec5\u4e3a\u57fa\u4e8e\u7f51\u683c\u8868\u793a\u7684\u4e94\u5206\u4e4b\u4e00\u6216\u66f4\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\u5e76\u964d\u4f4e\u63a8\u7406\u5f00\u9500\u3002", "conclusion": "\u795e\u7ecf\u9876\u70b9\u7279\u5f81\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7d27\u51d1\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u795e\u7ecf\u6e32\u67d3\u4efb\u52a1\u3002"}}
{"id": "2508.06496", "pdf": "https://arxiv.org/pdf/2508.06496", "abs": "https://arxiv.org/abs/2508.06496", "authors": ["Rakesh Raj Madavan", "Akshat Kaimal", "Hashim Faisal", "Chandrakala S"], "title": "Med-GRIM: Enhanced Zero-Shot Medical VQA using prompt-embedded Multimodal Graph RAG", "categories": ["cs.CV", "cs.MA"], "comment": null, "summary": "An ensemble of trained multimodal encoders and vision-language models (VLMs) has become a standard approach for visual question answering (VQA) tasks. However, such models often fail to produce responses with the detailed precision necessary for complex, domain-specific applications such as medical VQA. Our representation model, BIND: BLIVA Integrated with Dense Encoding, extends prior multimodal work by refining the joint embedding space through dense, query-token-based encodings inspired by contrastive pretraining techniques. This refined encoder powers Med-GRIM, a model designed for medical VQA tasks that leverages graph-based retrieval and prompt engineering to integrate domain-specific knowledge. Rather than relying on compute-heavy fine-tuning of vision and language models on specific datasets, Med-GRIM applies a low-compute, modular workflow with small language models (SLMs) for efficiency. Med-GRIM employs prompt-based retrieval to dynamically inject relevant knowledge, ensuring both accuracy and robustness in its responses. By assigning distinct roles to each agent within the VQA system, Med-GRIM achieves large language model performance at a fraction of the computational cost. Additionally, to support scalable research in zero-shot multimodal medical applications, we introduce DermaGraph, a novel Graph-RAG dataset comprising diverse dermatological conditions. This dataset facilitates both multimodal and unimodal querying. The code and dataset are available at: https://github.com/Rakesh-123-cryp/Med-GRIM.git", "AI": {"tldr": "BIND\u548cMed-GRIM\u901a\u8fc7\u5bc6\u96c6\u7f16\u7801\u548c\u56fe\u68c0\u7d22\u63d0\u5347\u533b\u7597VQA\u4efb\u52a1\u6027\u80fd\uff0c\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709VQA\u6a21\u578b\u5728\u590d\u6742\u9886\u57df\uff08\u5982\u533b\u7597\uff09\u4e2d\u7f3a\u4e4f\u7cbe\u786e\u6027\uff0c\u9700\u8981\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "BIND\u6539\u8fdb\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\uff0cMed-GRIM\u7ed3\u5408\u56fe\u68c0\u7d22\u548c\u63d0\u793a\u5de5\u7a0b\uff0c\u4f7f\u7528\u5c0f\u8bed\u8a00\u6a21\u578b\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "result": "Med-GRIM\u4ee5\u4f4e\u6210\u672c\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u5e76\u53d1\u5e03DermaGraph\u6570\u636e\u96c6\u652f\u6301\u7814\u7a76\u3002", "conclusion": "BIND\u548cMed-GRIM\u4e3a\u533b\u7597VQA\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06768", "pdf": "https://arxiv.org/pdf/2508.06768", "abs": "https://arxiv.org/abs/2508.06768", "authors": ["Noe Bertramo", "Gabriel Duguey", "Vivek Gopalakrishnan"], "title": "DiffUS: Differentiable Ultrasound Rendering from Volumetric Imaging", "categories": ["cs.CV", "cs.GR"], "comment": "10 pages, accepted to MICCAI ASMUS 25", "summary": "Intraoperative ultrasound imaging provides real-time guidance during numerous surgical procedures, but its interpretation is complicated by noise, artifacts, and poor alignment with high-resolution preoperative MRI/CT scans. To bridge the gap between reoperative planning and intraoperative guidance, we present DiffUS, a physics-based, differentiable ultrasound renderer that synthesizes realistic B-mode images from volumetric imaging. DiffUS first converts MRI 3D scans into acoustic impedance volumes using a machine learning approach. Next, we simulate ultrasound beam propagation using ray tracing with coupled reflection-transmission equations. DiffUS formulates wave propagation as a sparse linear system that captures multiple internal reflections. Finally, we reconstruct B-mode images via depth-resolved echo extraction across fan-shaped acquisition geometry, incorporating realistic artifacts including speckle noise and depth-dependent degradation. DiffUS is entirely implemented as differentiable tensor operations in PyTorch, enabling gradient-based optimization for downstream applications such as slice-to-volume registration and volumetric reconstruction. Evaluation on the ReMIND dataset demonstrates DiffUS's ability to generate anatomically accurate ultrasound images from brain MRI data.", "AI": {"tldr": "DiffUS\u662f\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u7684\u53ef\u5fae\u5206\u8d85\u58f0\u6e32\u67d3\u5668\uff0c\u80fd\u591f\u4eceMRI/CT\u6570\u636e\u751f\u6210\u903c\u771f\u7684B\u6a21\u5f0f\u8d85\u58f0\u56fe\u50cf\uff0c\u7528\u4e8e\u672f\u4e2d\u5f15\u5bfc\u3002", "motivation": "\u89e3\u51b3\u672f\u4e2d\u8d85\u58f0\u56fe\u50cf\u56e0\u566a\u58f0\u3001\u4f2a\u5f71\u548c\u4e0e\u672f\u524dMRI/CT\u5bf9\u9f50\u56f0\u96be\u5bfc\u81f4\u7684\u89e3\u8bfb\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u5c06MRI 3D\u626b\u63cf\u8f6c\u6362\u4e3a\u58f0\u963b\u6297\u4f53\u79ef\uff0c\u5229\u7528\u5c04\u7ebf\u8ffd\u8e2a\u548c\u53cd\u5c04-\u900f\u5c04\u65b9\u7a0b\u6a21\u62df\u8d85\u58f0\u675f\u4f20\u64ad\uff0c\u5e76\u901a\u8fc7\u7a00\u758f\u7ebf\u6027\u7cfb\u7edf\u6355\u6349\u591a\u6b21\u5185\u90e8\u53cd\u5c04\u3002", "result": "\u5728ReMIND\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86DiffUS\u80fd\u591f\u4ece\u8111MRI\u6570\u636e\u751f\u6210\u89e3\u5256\u5b66\u51c6\u786e\u7684\u8d85\u58f0\u56fe\u50cf\u3002", "conclusion": "DiffUS\u4e3a\u672f\u4e2d\u5f15\u5bfc\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u8d85\u58f0\u56fe\u50cf\u5408\u6210\u5de5\u5177\uff0c\u652f\u6301\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u5e94\u7528\u3002"}}
{"id": "2508.06968", "pdf": "https://arxiv.org/pdf/2508.06968", "abs": "https://arxiv.org/abs/2508.06968", "authors": ["Ulas Gunes", "Matias Turkulainen", "Juho Kannala", "Esa Rahtu"], "title": "Evaluating Fisheye-Compatible 3D Gaussian Splatting Methods on Real Images Beyond 180 Degree Field of View", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "We present the first evaluation of fisheye-based 3D Gaussian Splatting methods, Fisheye-GS and 3DGUT, on real images with fields of view exceeding 180 degree. Our study covers both indoor and outdoor scenes captured with 200 degree fisheye cameras and analyzes how each method handles extreme distortion in real world settings. We evaluate performance under varying fields of view (200 degree, 160 degree, and 120 degree) to study the tradeoff between peripheral distortion and spatial coverage. Fisheye-GS benefits from field of view (FoV) reduction, particularly at 160 degree, while 3DGUT remains stable across all settings and maintains high perceptual quality at the full 200 degree view. To address the limitations of SfM-based initialization, which often fails under strong distortion, we also propose a depth-based strategy using UniK3D predictions from only 2-3 fisheye images per scene. Although UniK3D is not trained on real fisheye data, it produces dense point clouds that enable reconstruction quality on par with SfM, even in difficult scenes with fog, glare, or sky. Our results highlight the practical viability of fisheye-based 3DGS methods for wide-angle 3D reconstruction from sparse and distortion-heavy image inputs.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86Fisheye-GS\u548c3DGUT\u4e24\u79cd\u57fa\u4e8e\u9c7c\u773c\u955c\u5934\u76843D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u5728\u8d85\u8fc7180\u5ea6\u89c6\u573a\u7684\u771f\u5b9e\u56fe\u50cf\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u6df1\u5ea6\u7684\u521d\u59cb\u5316\u7b56\u7565\u4ee5\u89e3\u51b3\u5f3a\u5931\u771f\u4e0bSfM\u521d\u59cb\u5316\u5931\u8d25\u7684\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u9c7c\u773c\u955c\u5934\u5728\u6781\u7aef\u5931\u771f\u6761\u4ef6\u4e0b\u76843D\u91cd\u5efa\u6027\u80fd\uff0c\u63a2\u7d22\u4e0d\u540c\u89c6\u573a\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u521d\u59cb\u5316\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5ba4\u5185\u5916\u573a\u666f\u7684200\u5ea6\u9c7c\u773c\u56fe\u50cf\uff0c\u5bf9\u6bd4Fisheye-GS\u548c3DGUT\u5728\u4e0d\u540c\u89c6\u573a\uff08200\u5ea6\u3001160\u5ea6\u3001120\u5ea6\uff09\u4e0b\u7684\u8868\u73b0\uff1b\u63d0\u51fa\u57fa\u4e8eUniK3D\u9884\u6d4b\u7684\u6df1\u5ea6\u521d\u59cb\u5316\u7b56\u7565\u3002", "result": "Fisheye-GS\u5728160\u5ea6\u89c6\u573a\u8868\u73b0\u6700\u4f73\uff0c\u800c3DGUT\u5728200\u5ea6\u89c6\u573a\u4e0b\u4fdd\u6301\u7a33\u5b9a\uff1bUniK3D\u751f\u6210\u7684\u5bc6\u96c6\u70b9\u4e91\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u9c7c\u773c3DGS\u65b9\u6cd5\u5728\u7a00\u758f\u4e14\u9ad8\u5931\u771f\u56fe\u50cf\u8f93\u5165\u4e0b\u5177\u6709\u5b9e\u9645\u53ef\u884c\u6027\uff0cUniK3D\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u521d\u59cb\u5316\u6548\u679c\u3002"}}
{"id": "2508.07011", "pdf": "https://arxiv.org/pdf/2508.07011", "abs": "https://arxiv.org/abs/2508.07011", "authors": ["Zixiong Wang", "Jian Yang", "Yiwei Hu", "Milos Hasan", "Beibei Wang"], "title": "HiMat: DiT-based Ultra-High Resolution SVBRDF Generation", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Creating highly detailed SVBRDFs is essential for 3D content creation. The rise of high-resolution text-to-image generative models, based on diffusion transformers (DiT), suggests an opportunity to finetune them for this task. However, retargeting the models to produce multiple aligned SVBRDF maps instead of just RGB images, while achieving high efficiency and ensuring consistency across different maps, remains a challenge. In this paper, we introduce HiMat: a memory- and computation-efficient diffusion-based framework capable of generating native 4K-resolution SVBRDFs. A key challenge we address is maintaining consistency across different maps in a lightweight manner, without relying on training new VAEs or significantly altering the DiT backbone (which would damage its prior capabilities). To tackle this, we introduce the CrossStitch module, a lightweight convolutional module that captures inter-map dependencies through localized operations. Its weights are initialized such that the DiT backbone operation is unchanged before finetuning starts. HiMat enables generation with strong structural coherence and high-frequency details. Results with a large set of text prompts demonstrate the effectiveness of our approach for 4K SVBRDF generation. Further experiments suggest generalization to tasks such as intrinsic decomposition.", "AI": {"tldr": "HiMat\u662f\u4e00\u4e2a\u9ad8\u6548\u751f\u62104K\u5206\u8fa8\u7387SVBRDF\u7684\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7CrossStitch\u6a21\u5757\u89e3\u51b3\u591a\u56fe\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u5174\u8d77\u4e3aSVBRDF\u751f\u6210\u63d0\u4f9b\u4e86\u673a\u4f1a\uff0c\u4f46\u5982\u4f55\u9ad8\u6548\u751f\u6210\u591a\u56fe\u5bf9\u9f50\u7684SVBRDF\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faHiMat\u6846\u67b6\uff0c\u5f15\u5165\u8f7b\u91cf\u7ea7CrossStitch\u6a21\u5757\u6355\u6349\u56fe\u95f4\u4f9d\u8d56\uff0c\u4fdd\u6301DiT\u4e3b\u5e72\u4e0d\u53d8\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eHiMat\u80fd\u751f\u6210\u7ed3\u6784\u4e00\u81f4\u4e14\u9ad8\u9891\u7ec6\u8282\u4e30\u5bcc\u76844K SVBRDF\uff0c\u5e76\u9002\u7528\u4e8e\u5176\u4ed6\u4efb\u52a1\u5982\u672c\u5f81\u5206\u89e3\u3002", "conclusion": "HiMat\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cfSVBRDF\uff0c\u5c55\u73b0\u4e86\u6269\u6563\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.07760", "pdf": "https://arxiv.org/pdf/2508.07760", "abs": "https://arxiv.org/abs/2508.07760", "authors": ["Maximilian Kromer", "Panagiotis Agrafiotis", "Beg\u00fcm Demir"], "title": "Sea-Undistort: A Dataset for Through-Water Image Restoration in High Resolution Airborne Bathymetric Mapping", "categories": ["eess.IV", "cs.CV", "cs.GR"], "comment": "Under review in IEEE Geoscience and Remote Sensing Letters", "summary": "Accurate image-based bathymetric mapping in shallow waters remains challenging due to the complex optical distortions such as wave induced patterns, scattering and sunglint, introduced by the dynamic water surface, the water column properties, and solar illumination. In this work, we introduce Sea-Undistort, a comprehensive synthetic dataset of 1200 paired 512x512 through-water scenes rendered in Blender. Each pair comprises a distortion-free and a distorted view, featuring realistic water effects such as sun glint, waves, and scattering over diverse seabeds. Accompanied by per-image metadata such as camera parameters, sun position, and average depth, Sea-Undistort enables supervised training that is otherwise infeasible in real environments. We use Sea-Undistort to benchmark two state-of-the-art image restoration methods alongside an enhanced lightweight diffusion-based framework with an early-fusion sun-glint mask. When applied to real aerial data, the enhanced diffusion model delivers more complete Digital Surface Models (DSMs) of the seabed, especially in deeper areas, reduces bathymetric errors, suppresses glint and scattering, and crisply restores fine seabed details. Dataset, weights, and code are publicly available at https://www.magicbathy.eu/Sea-Undistort.html.", "AI": {"tldr": "Sea-Undistort\u662f\u4e00\u4e2a\u5408\u6210\u7684\u6d45\u6c34\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3\u56fe\u50cf\u6062\u590d\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u771f\u5b9e\u73af\u5883\u4e2d\u96be\u4ee5\u83b7\u53d6\u65e0\u5931\u771f\u6570\u636e\u7684\u95ee\u9898\u3002", "motivation": "\u6d45\u6c34\u533a\u57fa\u4e8e\u56fe\u50cf\u7684\u6d4b\u6df1\u6620\u5c04\u56e0\u6c34\u9762\u52a8\u6001\u3001\u6c34\u8d28\u548c\u5149\u7167\u5f15\u8d77\u7684\u590d\u6742\u5149\u5b66\u5931\u771f\uff08\u5982\u6ce2\u6d6a\u3001\u6563\u5c04\u548c\u592a\u9633\u8000\u6591\uff09\u800c\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528Blender\u6e32\u67d31200\u5bf9512x512\u7684\u65e0\u5931\u771f\u548c\u5931\u771f\u56fe\u50cf\uff0c\u5305\u542b\u592a\u9633\u8000\u6591\u3001\u6ce2\u6d6a\u548c\u6563\u5c04\u7b49\u6548\u679c\uff0c\u5e76\u63d0\u4f9b\u5143\u6570\u636e\u3002\u57fa\u4e8e\u6b64\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u4e24\u79cd\u5148\u8fdb\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u548c\u4e00\u79cd\u6539\u8fdb\u7684\u8f7b\u91cf\u7ea7\u6269\u6563\u6a21\u578b\u3002", "result": "\u6539\u8fdb\u7684\u6269\u6563\u6a21\u578b\u5728\u771f\u5b9e\u822a\u7a7a\u6570\u636e\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u751f\u6210\u66f4\u5b8c\u6574\u7684\u6d77\u5e95\u6570\u5b57\u8868\u9762\u6a21\u578b\uff0c\u51cf\u5c11\u6d4b\u6df1\u8bef\u5dee\uff0c\u6291\u5236\u8000\u6591\u548c\u6563\u5c04\uff0c\u5e76\u6062\u590d\u7cbe\u7ec6\u6d77\u5e95\u7ec6\u8282\u3002", "conclusion": "Sea-Undistort\u6570\u636e\u96c6\u4e3a\u6d45\u6c34\u56fe\u50cf\u6062\u590d\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bad\u7ec3\u8d44\u6e90\uff0c\u6539\u8fdb\u7684\u6269\u6563\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.08086", "pdf": "https://arxiv.org/pdf/2508.08086", "abs": "https://arxiv.org/abs/2508.08086", "authors": ["Zhongqi Yang", "Wenhang Ge", "Yuqi Li", "Jiaqi Chen", "Haoyuan Li", "Mengyin An", "Fei Kang", "Hua Xue", "Baixin Xu", "Yuyang Yin", "Eric Li", "Yang Liu", "Yikai Wang", "Hao-Xiang Guo", "Yahui Zhou"], "title": "Matrix-3D: Omnidirectional Explorable 3D World Generation", "categories": ["cs.CV", "cs.GR"], "comment": "Technical Report", "summary": "Explorable 3D world generation from a single image or text prompt forms a cornerstone of spatial intelligence. Recent works utilize video model to achieve wide-scope and generalizable 3D world generation. However, existing approaches often suffer from a limited scope in the generated scenes. In this work, we propose Matrix-3D, a framework that utilize panoramic representation for wide-coverage omnidirectional explorable 3D world generation that combines conditional video generation and panoramic 3D reconstruction. We first train a trajectory-guided panoramic video diffusion model that employs scene mesh renders as condition, to enable high-quality and geometrically consistent scene video generation. To lift the panorama scene video to 3D world, we propose two separate methods: (1) a feed-forward large panorama reconstruction model for rapid 3D scene reconstruction and (2) an optimization-based pipeline for accurate and detailed 3D scene reconstruction. To facilitate effective training, we also introduce the Matrix-Pano dataset, the first large-scale synthetic collection comprising 116K high-quality static panoramic video sequences with depth and trajectory annotations. Extensive experiments demonstrate that our proposed framework achieves state-of-the-art performance in panoramic video generation and 3D world generation. See more in https://matrix-3d.github.io.", "AI": {"tldr": "Matrix-3D\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5168\u666f\u8868\u793a\u548c\u6761\u4ef6\u89c6\u9891\u751f\u6210\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u5e7f\u6cdb\u8986\u76d6\u7684\u53ef\u63a2\u7d223D\u4e16\u754c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u573a\u666f\u65f6\u8303\u56f4\u6709\u9650\uff0cMatrix-3D\u65e8\u5728\u901a\u8fc7\u5168\u666f\u8868\u793a\u548c\u89c6\u9891\u6a21\u578b\u7ed3\u5408\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bad\u7ec3\u8f68\u8ff9\u5f15\u5bfc\u7684\u5168\u666f\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd3D\u91cd\u5efa\u65b9\u6cd5\uff1a\u524d\u9988\u5f0f\u5927\u578b\u5168\u666f\u91cd\u5efa\u6a21\u578b\u548c\u57fa\u4e8e\u4f18\u5316\u7684\u7ba1\u9053\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMatrix-3D\u5728\u5168\u666f\u89c6\u9891\u751f\u6210\u548c3D\u4e16\u754c\u751f\u6210\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "Matrix-3D\u6846\u67b6\u901a\u8fc7\u5168\u666f\u8868\u793a\u548c\u9ad8\u6548\u91cd\u5efa\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u4e16\u754c\u751f\u6210\u7684\u8986\u76d6\u8303\u56f4\u548c\u8d28\u91cf\u3002"}}
{"id": "2508.06543", "pdf": "https://arxiv.org/pdf/2508.06543", "abs": "https://arxiv.org/abs/2508.06543", "authors": ["Jinghan Yu", "Zhiyuan Ma", "Yue Ma", "Kaiqi Liu", "Yuhan Wang", "Jianjun Li"], "title": "MILD: Multi-Layer Diffusion Strategy for Complex and Precise Multi-IP Aware Human Erasing", "categories": ["cs.CV"], "comment": null, "summary": "Recent years have witnessed the success of diffusion models in image-customized tasks. Prior works have achieved notable progress on human-oriented erasing using explicit mask guidance and semantic-aware inpainting. However, they struggle under complex multi-IP scenarios involving human-human occlusions, human-object entanglements, and background interferences. These challenges are mainly due to: 1) Dataset limitations, as existing datasets rarely cover dense occlusions, camouflaged backgrounds, and diverse interactions; 2) Lack of spatial decoupling, where foreground instances cannot be effectively disentangled, limiting clean background restoration. In this work, we introduce a high-quality multi-IP human erasing dataset with diverse pose variations and complex backgrounds. We then propose Multi-Layer Diffusion (MILD), a novel strategy that decomposes generation into semantically separated pathways for each instance and the background. To enhance human-centric understanding, we introduce Human Morphology Guidance, integrating pose, parsing, and spatial relations. We further present Spatially-Modulated Attention to better guide attention flow. Extensive experiments show that MILD outperforms state-of-the-art methods on challenging human erasing benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMILD\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u590d\u6742\u591a\u5b9e\u4f8b\u573a\u666f\u4e0b\u7684\u4eba\u50cf\u64e6\u9664\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u5c42\u6269\u6563\u548c\u7a7a\u95f4\u89e3\u8026\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u591a\u5b9e\u4f8b\u573a\u666f\uff08\u5982\u4eba\u7269\u906e\u6321\u3001\u80cc\u666f\u5e72\u6270\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u7531\u4e8e\u6570\u636e\u96c6\u9650\u5236\u548c\u7f3a\u4e4f\u7a7a\u95f4\u89e3\u8026\u80fd\u529b\u3002", "method": "\u63d0\u51faMulti-Layer Diffusion\uff08MILD\uff09\u7b56\u7565\uff0c\u5c06\u751f\u6210\u8fc7\u7a0b\u5206\u89e3\u4e3a\u8bed\u4e49\u5206\u79bb\u7684\u8def\u5f84\uff0c\u5e76\u5f15\u5165Human Morphology Guidance\u548cSpatially-Modulated Attention\u3002", "result": "MILD\u5728\u6311\u6218\u6027\u7684\u4eba\u50cf\u64e6\u9664\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MILD\u901a\u8fc7\u591a\u5c42\u6269\u6563\u548c\u7a7a\u95f4\u89e3\u8026\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u573a\u666f\u4e0b\u7684\u4eba\u50cf\u64e6\u9664\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2508.06551", "pdf": "https://arxiv.org/pdf/2508.06551", "abs": "https://arxiv.org/abs/2508.06551", "authors": ["Ye Tao"], "title": "Slice or the Whole Pie? Utility Control for AI Models", "categories": ["cs.CV"], "comment": null, "summary": "Training deep neural networks (DNNs) has become an increasingly resource-intensive task, requiring large volumes of labeled data, substantial computational power, and considerable fine-tuning efforts to achieve optimal performance across diverse use cases. Although pre-trained models offer a useful starting point, adapting them to meet specific user needs often demands extensive customization, and infrastructure overhead. This challenge grows when a single model must support diverse appli-cations with differing requirements for performance. Traditional solutions often involve training multiple model versions to meet varying requirements, which can be inefficient and difficult to maintain. In order to overcome this challenge, we propose NNObfuscator, a novel utility control mechanism that enables AI models to dynamically modify their performance according to predefined conditions. It is different from traditional methods that need separate models for each user. Instead, NNObfuscator allows a single model to be adapted in real time, giving you controlled access to multiple levels of performance. This mechanism enables model owners set up tiered access, ensuring that free-tier users receive a baseline level of performance while premium users benefit from enhanced capabilities. The approach improves resource allocation, reduces unnecessary computation, and supports sustainable business models in AI deployment. To validate our approach, we conducted experiments on multiple tasks, including image classification, semantic segmentation, and text to image generation, using well-established models such as ResNet, DeepLab, VGG16, FCN and Stable Diffusion. Experimental results show that NNObfuscator successfully makes model more adaptable, so that a single trained model can handle a broad range of tasks without requiring a lot of changes.", "AI": {"tldr": "NNObfuscator\u662f\u4e00\u79cd\u65b0\u578b\u5de5\u5177\uff0c\u5141\u8bb8\u5355\u4e2aAI\u6a21\u578b\u6839\u636e\u9884\u8bbe\u6761\u4ef6\u52a8\u6001\u8c03\u6574\u6027\u80fd\uff0c\u65e0\u9700\u4e3a\u4e0d\u540c\u9700\u6c42\u8bad\u7ec3\u591a\u4e2a\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u4e3a\u4e0d\u540c\u6027\u80fd\u9700\u6c42\u8bad\u7ec3\u548c\u7ef4\u62a4\u591a\u4e2a\u6a21\u578b\u7684\u4f4e\u6548\u95ee\u9898\u3002", "method": "\u63d0\u51faNNObfuscator\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u80fd\u5b9e\u65f6\u8c03\u6574\u6027\u80fd\uff0c\u652f\u6301\u5206\u5c42\u8bbf\u95ee\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86NNObfuscator\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5355\u4e2a\u6a21\u578b\u53ef\u9002\u5e94\u5e7f\u6cdb\u9700\u6c42\u3002", "conclusion": "NNObfuscator\u63d0\u9ad8\u4e86\u8d44\u6e90\u5229\u7528\u7387\uff0c\u652f\u6301\u53ef\u6301\u7eed\u7684AI\u90e8\u7f72\u5546\u4e1a\u6a21\u5f0f\u3002"}}
{"id": "2508.06624", "pdf": "https://arxiv.org/pdf/2508.06624", "abs": "https://arxiv.org/abs/2508.06624", "authors": ["Kexin Yu", "Zihan Xu", "Jialei Xie", "Carter Adams"], "title": "VL-MedGuide: A Visual-Linguistic Large Model for Intelligent and Explainable Skin Disease Auxiliary Diagnosis", "categories": ["cs.CV"], "comment": null, "summary": "Accurate diagnosis of skin diseases remains a significant challenge due to the complex and diverse visual features present in dermatoscopic images, often compounded by a lack of interpretability in existing purely visual diagnostic models. To address these limitations, this study introduces VL-MedGuide (Visual-Linguistic Medical Guide), a novel framework leveraging the powerful multi-modal understanding and reasoning capabilities of Visual-Language Large Models (LVLMs) for intelligent and inherently interpretable auxiliary diagnosis of skin conditions. VL-MedGuide operates in two interconnected stages: a Multi-modal Concept Perception Module, which identifies and linguistically describes dermatologically relevant visual features through sophisticated prompt engineering, and an Explainable Disease Reasoning Module, which integrates these concepts with raw visual information via Chain-of-Thought prompting to provide precise disease diagnoses alongside transparent rationales. Comprehensive experiments on the Derm7pt dataset demonstrate that VL-MedGuide achieves state-of-the-art performance in both disease diagnosis (83.55% BACC, 80.12% F1) and concept detection (76.10% BACC, 67.45% F1), surpassing existing baselines. Furthermore, human evaluations confirm the high clarity, completeness, and trustworthiness of its generated explanations, bridging the gap between AI performance and clinical utility by offering actionable, explainable insights for dermatological practice.", "AI": {"tldr": "VL-MedGuide\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u5927\u6a21\u578b\u7684\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u76ae\u80a4\u75c5\u7684\u667a\u80fd\u8f85\u52a9\u8bca\u65ad\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6982\u5ff5\u611f\u77e5\u548c\u53ef\u89e3\u91ca\u7684\u75be\u75c5\u63a8\u7406\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u900f\u660e\u5316\u7684\u8bca\u65ad\u3002", "motivation": "\u89e3\u51b3\u76ae\u80a4\u75c5\u8bca\u65ad\u4e2d\u89c6\u89c9\u7279\u5f81\u590d\u6742\u591a\u6837\u4ee5\u53ca\u73b0\u6709\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u591a\u6a21\u6001\u6982\u5ff5\u611f\u77e5\u6a21\u5757\u8bc6\u522b\u5e76\u63cf\u8ff0\u76ae\u80a4\u75c5\u76f8\u5173\u7279\u5f81\uff1b\u53ef\u89e3\u91ca\u75be\u75c5\u63a8\u7406\u6a21\u5757\u7ed3\u5408\u89c6\u89c9\u4fe1\u606f\u548c\u6982\u5ff5\u8fdb\u884c\u8bca\u65ad\u5e76\u63d0\u4f9b\u900f\u660e\u89e3\u91ca\u3002", "result": "\u5728Derm7pt\u6570\u636e\u96c6\u4e0a\uff0cVL-MedGuide\u5728\u75be\u75c5\u8bca\u65ad\uff0883.55% BACC, 80.12% F1\uff09\u548c\u6982\u5ff5\u68c0\u6d4b\uff0876.10% BACC, 67.45% F1\uff09\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "VL-MedGuide\u901a\u8fc7\u63d0\u4f9b\u6e05\u6670\u3001\u5b8c\u6574\u7684\u89e3\u91ca\uff0c\u5f25\u5408\u4e86AI\u6027\u80fd\u4e0e\u4e34\u5e8a\u5b9e\u7528\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2508.06625", "pdf": "https://arxiv.org/pdf/2508.06625", "abs": "https://arxiv.org/abs/2508.06625", "authors": ["Shilong Zou", "Yuhang Huang", "Renjiao Yi", "Chenyang Zhu", "Kai Xu"], "title": "CycleDiff: Cycle Diffusion Models for Unpaired Image-to-image Translation", "categories": ["cs.CV"], "comment": null, "summary": "We introduce a diffusion-based cross-domain image translator in the absence of paired training data. Unlike GAN-based methods, our approach integrates diffusion models to learn the image translation process, allowing for more coverable modeling of the data distribution and performance improvement of the cross-domain translation. However, incorporating the translation process within the diffusion process is still challenging since the two processes are not aligned exactly, i.e., the diffusion process is applied to the noisy signal while the translation process is conducted on the clean signal. As a result, recent diffusion-based studies employ separate training or shallow integration to learn the two processes, yet this may cause the local minimal of the translation optimization, constraining the effectiveness of diffusion models. To address the problem, we propose a novel joint learning framework that aligns the diffusion and the translation process, thereby improving the global optimality. Specifically, we propose to extract the image components with diffusion models to represent the clean signal and employ the translation process with the image components, enabling an end-to-end joint learning manner. On the other hand, we introduce a time-dependent translation network to learn the complex translation mapping, resulting in effective translation learning and significant performance improvement. Benefiting from the design of joint learning, our method enables global optimization of both processes, enhancing the optimality and achieving improved fidelity and structural consistency. We have conducted extensive experiments on RGB$\\leftrightarrow$RGB and diverse cross-modality translation tasks including RGB$\\leftrightarrow$Edge, RGB$\\leftrightarrow$Semantics and RGB$\\leftrightarrow$Depth, showcasing better generative performances than the state of the arts.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.06632", "pdf": "https://arxiv.org/pdf/2508.06632", "abs": "https://arxiv.org/abs/2508.06632", "authors": ["Wenpeng Xing", "Jie Chen", "Zaifeng Yang", "Tiancheng Zhao", "Gaolei Li", "Changting Lin", "Yike Guo", "Meng Han"], "title": "CoDe-NeRF: Neural Rendering via Dynamic Coefficient Decomposition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Neural Radiance Fields (NeRF) have shown impressive performance in novel view synthesis, but challenges remain in rendering scenes with complex specular reflections and highlights. Existing approaches may produce blurry reflections due to entanglement between lighting and material properties, or encounter optimization instability when relying on physically-based inverse rendering. In this work, we present a neural rendering framework based on dynamic coefficient decomposition, aiming to improve the modeling of view-dependent appearance. Our approach decomposes complex appearance into a shared, static neural basis that encodes intrinsic material properties, and a set of dynamic coefficients generated by a Coefficient Network conditioned on view and illumination. A Dynamic Radiance Integrator then combines these components to synthesize the final radiance. Experimental results on several challenging benchmarks suggest that our method can produce sharper and more realistic specular highlights compared to existing techniques. We hope that this decomposition paradigm can provide a flexible and effective direction for modeling complex appearance in neural scene representations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u7cfb\u6570\u5206\u89e3\u7684\u795e\u7ecf\u6e32\u67d3\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u590d\u6742\u89c6\u70b9\u4f9d\u8d56\u5916\u89c2\u7684\u5efa\u6a21\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u955c\u9762\u53cd\u5c04\u548c\u9ad8\u5149\u65f6\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u955c\u9762\u53cd\u5c04\u548c\u9ad8\u5149\u65f6\uff0c\u5e38\u56e0\u5149\u7167\u4e0e\u6750\u8d28\u5c5e\u6027\u7ea0\u7f20\u6216\u4f18\u5316\u4e0d\u7a33\u5b9a\u800c\u4ea7\u751f\u6a21\u7cca\u53cd\u5c04\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u52a8\u6001\u7cfb\u6570\u5206\u89e3\uff0c\u5c06\u590d\u6742\u5916\u89c2\u5206\u89e3\u4e3a\u5171\u4eab\u7684\u9759\u6001\u795e\u7ecf\u57fa\u548c\u7531\u89c6\u70b9\u4e0e\u5149\u7167\u6761\u4ef6\u751f\u6210\u7684\u52a8\u6001\u7cfb\u6570\uff0c\u518d\u901a\u8fc7\u52a8\u6001\u8f90\u5c04\u79ef\u5206\u5668\u5408\u6210\u6700\u7ec8\u8f90\u5c04\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u80fd\u751f\u6210\u66f4\u9510\u5229\u548c\u771f\u5b9e\u7684\u955c\u9762\u9ad8\u5149\u3002", "conclusion": "\u8fd9\u79cd\u5206\u89e3\u8303\u5f0f\u4e3a\u795e\u7ecf\u573a\u666f\u8868\u793a\u4e2d\u590d\u6742\u5916\u89c2\u7684\u5efa\u6a21\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u6709\u6548\u7684\u65b9\u5411\u3002"}}
{"id": "2508.06656", "pdf": "https://arxiv.org/pdf/2508.06656", "abs": "https://arxiv.org/abs/2508.06656", "authors": ["Denis Lukovnikov", "Andreas M\u00fcller", "Erwin Quiring", "Asja Fischer"], "title": "Towards Robust Red-Green Watermarking for Autoregressive Image Generators", "categories": ["cs.CV"], "comment": null, "summary": "In-generation watermarking for detecting and attributing generated content has recently been explored for latent diffusion models (LDMs), demonstrating high robustness. However, the use of in-generation watermarks in autoregressive (AR) image models has not been explored yet. AR models generate images by autoregressively predicting a sequence of visual tokens that are then decoded into pixels using a vector-quantized decoder. Inspired by red-green watermarks for large language models, we examine token-level watermarking schemes that bias the next-token prediction based on prior tokens. We find that a direct transfer of these schemes works in principle, but the detectability of the watermarks decreases considerably under common image perturbations. As a remedy, we propose two novel watermarking methods that rely on visual token clustering to assign similar tokens to the same set. Firstly, we investigate a training-free approach that relies on a cluster lookup table, and secondly, we finetune VAE encoders to predict token clusters directly from perturbed images. Overall, our experiments show that cluster-level watermarks improve robustness against perturbations and regeneration attacks while preserving image quality. Cluster classification further boosts watermark detectability, outperforming a set of baselines. Moreover, our methods offer fast verification runtime, comparable to lightweight post-hoc watermarking methods.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u81ea\u56de\u5f52\uff08AR\uff09\u56fe\u50cf\u6a21\u578b\u4e2d\u5e94\u7528\u751f\u6210\u5185\u6c34\u5370\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u89c6\u89c9\u4ee4\u724c\u805a\u7c7b\u7684\u6c34\u5370\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u5370\u7684\u9c81\u68d2\u6027\u548c\u68c0\u6d4b\u6027\u3002", "motivation": "\u5c3d\u7ba1\u751f\u6210\u5185\u6c34\u5370\u5728\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDMs\uff09\u4e2d\u8868\u73b0\u51fa\u9ad8\u9c81\u68d2\u6027\uff0c\u4f46\u5728\u81ea\u56de\u5f52\u56fe\u50cf\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u88ab\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u4e86\u57fa\u4e8e\u4ee4\u724c\u7ea7\u6c34\u5370\u7684\u65b9\u6848\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u65b0\u65b9\u6cd5\uff1a1\uff09\u57fa\u4e8e\u805a\u7c7b\u67e5\u627e\u8868\u7684\u65e0\u8bad\u7ec3\u65b9\u6cd5\uff1b2\uff09\u901a\u8fc7\u5fae\u8c03VAE\u7f16\u7801\u5668\u76f4\u63a5\u4ece\u6270\u52a8\u56fe\u50cf\u9884\u6d4b\u4ee4\u724c\u805a\u7c7b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u805a\u7c7b\u7ea7\u6c34\u5370\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u6270\u52a8\u548c\u518d\u751f\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002\u805a\u7c7b\u5206\u7c7b\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u6c34\u5370\u68c0\u6d4b\u6027\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u6c34\u5370\u7684\u9c81\u68d2\u6027\u548c\u68c0\u6d4b\u6027\uff0c\u8fd8\u5177\u6709\u5feb\u901f\u7684\u9a8c\u8bc1\u8fd0\u884c\u65f6\uff0c\u9002\u7528\u4e8e\u8f7b\u91cf\u7ea7\u540e\u5904\u7406\u6c34\u5370\u573a\u666f\u3002"}}
{"id": "2508.06805", "pdf": "https://arxiv.org/pdf/2508.06805", "abs": "https://arxiv.org/abs/2508.06805", "authors": ["Aarav Mehta", "Priya Deshmukh", "Vikram Singh", "Siddharth Malhotra", "Krishnan Menon Iyer", "Tanvi Iyer"], "title": "Edge Detection for Organ Boundaries via Top Down Refinement and SubPixel Upsampling", "categories": ["cs.CV"], "comment": "MICCAIA Workshop", "summary": "Accurate localization of organ boundaries is critical in medical imaging for segmentation, registration, surgical planning, and radiotherapy. While deep convolutional networks (ConvNets) have advanced general-purpose edge detection to near-human performance on natural images, their outputs often lack precise localization, a limitation that is particularly harmful in medical applications where millimeter-level accuracy is required. Building on a systematic analysis of ConvNet edge outputs, we propose a medically focused crisp edge detector that adapts a novel top-down backward refinement architecture to medical images (2D and volumetric). Our method progressively upsamples and fuses high-level semantic features with fine-grained low-level cues through a backward refinement pathway, producing high-resolution, well-localized organ boundaries. We further extend the design to handle anisotropic volumes by combining 2D slice-wise refinement with light 3D context aggregation to retain computational efficiency. Evaluations on several CT and MRI organ datasets demonstrate substantially improved boundary localization under strict criteria (boundary F-measure, Hausdorff distance) compared to baseline ConvNet detectors and contemporary medical edge/contour methods. Importantly, integrating our crisp edge maps into downstream pipelines yields consistent gains in organ segmentation (higher Dice scores, lower boundary errors), more accurate image registration, and improved delineation of lesions near organ interfaces. The proposed approach produces clinically valuable, crisp organ edges that materially enhance common medical-imaging tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u533b\u5b66\u56fe\u50cf\u7684\u7cbe\u786e\u8fb9\u7f18\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cd\u5411\u7ec6\u5316\u67b6\u6784\u63d0\u5347\u8fb9\u754c\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u663e\u8457\u6539\u5584\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u4e2d\u5668\u5b98\u8fb9\u754c\u7684\u6beb\u7c73\u7ea7\u7cbe\u786e\u5b9a\u4f4d\u5bf9\u5206\u5272\u3001\u624b\u672f\u89c4\u5212\u7b49\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u5377\u79ef\u7f51\u7edc\u5728\u81ea\u7136\u56fe\u50cf\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u533b\u5b66\u56fe\u50cf\u4e2d\u5b9a\u4f4d\u7cbe\u5ea6\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u81ea\u4e0a\u800c\u4e0b\u7684\u53cd\u5411\u7ec6\u5316\u67b6\u6784\uff0c\u9010\u6b65\u878d\u5408\u9ad8\u5c42\u8bed\u4e49\u7279\u5f81\u4e0e\u4f4e\u5c42\u7ec6\u8282\uff0c\u652f\u63012D\u548c\u4f53\u79ef\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf3D\u4e0a\u4e0b\u6587\u805a\u5408\u5904\u7406\u5404\u5411\u5f02\u6027\u4f53\u79ef\u3002", "result": "\u5728CT\u548cMRI\u6570\u636e\u96c6\u4e0a\uff0c\u8fb9\u754c\u5b9a\u4f4d\u7cbe\u5ea6\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u5206\u5272\u3001\u914d\u51c6\uff09\u6027\u80fd\u4e5f\u5f97\u5230\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u7cbe\u786e\u5668\u5b98\u8fb9\u754c\u5bf9\u4e34\u5e8a\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2508.06831", "pdf": "https://arxiv.org/pdf/2508.06831", "abs": "https://arxiv.org/abs/2508.06831", "authors": ["Taha Mustapha Nehdi", "Nairouz Mrabah", "Atif Belal", "Marco Pedersoli", "Eric Granger"], "title": "Low-Rank Expert Merging for Multi-Source Domain Adaptation in Person Re-Identification", "categories": ["cs.CV"], "comment": null, "summary": "Adapting person re-identification (reID) models to new target environments remains a challenging problem that is typically addressed using unsupervised domain adaptation (UDA) methods. Recent works show that when labeled data originates from several distinct sources (e.g., datasets and cameras), considering each source separately and applying multi-source domain adaptation (MSDA) typically yields higher accuracy and robustness compared to blending the sources and performing conventional UDA. However, state-of-the-art MSDA methods learn domain-specific backbone models or require access to source domain data during adaptation, resulting in significant growth in training parameters and computational cost. In this paper, a Source-free Adaptive Gated Experts (SAGE-reID) method is introduced for person reID. Our SAGE-reID is a cost-effective, source-free MSDA method that first trains individual source-specific low-rank adapters (LoRA) through source-free UDA. Next, a lightweight gating network is introduced and trained to dynamically assign optimal merging weights for fusion of LoRA experts, enabling effective cross-domain knowledge transfer. While the number of backbone parameters remains constant across source domains, LoRA experts scale linearly but remain negligible in size (<= 2% of the backbone), reducing both the memory consumption and risk of overfitting. Extensive experiments conducted on three challenging benchmarks: Market-1501, DukeMTMC-reID, and MSMT17 indicate that SAGE-reID outperforms state-of-the-art methods while being computationally efficient.", "AI": {"tldr": "SAGE-reID\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u6e90\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u6e90\u7279\u5b9a\u7684\u4f4e\u79e9\u9002\u914d\u5668\u548c\u8f7b\u91cf\u7ea7\u95e8\u63a7\u7f51\u7edc\u5b9e\u73b0\u8de8\u57df\u77e5\u8bc6\u8fc1\u79fb\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u6d88\u8017\u3002", "motivation": "\u89e3\u51b3\u591a\u6e90\u57df\u81ea\u9002\u5e94\uff08MSDA\uff09\u65b9\u6cd5\u4e2d\u8bad\u7ec3\u53c2\u6570\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528\u6e90\u7279\u5b9a\u7684\u4f4e\u79e9\u9002\u914d\u5668\uff08LoRA\uff09\u548c\u8f7b\u91cf\u7ea7\u95e8\u63a7\u7f51\u7edc\u52a8\u6001\u5408\u5e76\u9002\u914d\u5668\uff0c\u5b9e\u73b0\u8de8\u57df\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "\u5728Market-1501\u3001DukeMTMC-reID\u548cMSMT17\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u8ba1\u7b97\u9ad8\u6548\u3002", "conclusion": "SAGE-reID\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684MSDA\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u884c\u4eba\u91cd\u8bc6\u522b\u4efb\u52a1\u3002"}}
{"id": "2508.06905", "pdf": "https://arxiv.org/pdf/2508.06905", "abs": "https://arxiv.org/abs/2508.06905", "authors": ["Ruoxi Chen", "Dongping Chen", "Siyuan Wu", "Sinan Wang", "Shiyun Lang", "Petr Sushko", "Gaoyang Jiang", "Yao Wan", "Ranjay Krishna"], "title": "MultiRef: Controllable Image Generation with Multiple Visual References", "categories": ["cs.CV"], "comment": "Accepted to ACM MM 2025 Datasets", "summary": "Visual designers naturally draw inspiration from multiple visual references, combining diverse elements and aesthetic principles to create artwork. However, current image generative frameworks predominantly rely on single-source inputs -- either text prompts or individual reference images. In this paper, we focus on the task of controllable image generation using multiple visual references. We introduce MultiRef-bench, a rigorous evaluation framework comprising 990 synthetic and 1,000 real-world samples that require incorporating visual content from multiple reference images. The synthetic samples are synthetically generated through our data engine RefBlend, with 10 reference types and 33 reference combinations. Based on RefBlend, we further construct a dataset MultiRef containing 38k high-quality images to facilitate further research. Our experiments across three interleaved image-text models (i.e., OmniGen, ACE, and Show-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that even state-of-the-art systems struggle with multi-reference conditioning, with the best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in real-world cases on average compared to the golden answer. These findings provide valuable directions for developing more flexible and human-like creative tools that can effectively integrate multiple sources of visual inspiration. The dataset is publicly available at: https://multiref.github.io/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u53c2\u8003\u56fe\u50cf\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u4e86MultiRef-bench\u8bc4\u4f30\u6846\u67b6\u548cMultiRef\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u8868\u660e\u73b0\u6709\u6a21\u578b\u5728\u591a\u53c2\u8003\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u5f53\u524d\u56fe\u50cf\u751f\u6210\u6846\u67b6\u4e3b\u8981\u4f9d\u8d56\u5355\u6e90\u8f93\u5165\uff0c\u800c\u8bbe\u8ba1\u5e08\u901a\u5e38\u4ece\u591a\u4e2a\u89c6\u89c9\u53c2\u8003\u4e2d\u83b7\u53d6\u7075\u611f\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u6574\u5408\u591a\u53c2\u8003\u7684\u751f\u6210\u5de5\u5177\u3002", "method": "\u901a\u8fc7RefBlend\u6570\u636e\u5f15\u64ce\u751f\u6210\u5408\u6210\u6837\u672c\uff0c\u6784\u5efaMultiRef-bench\u8bc4\u4f30\u6846\u67b6\u548cMultiRef\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u591a\u4e2a\u56fe\u50cf-\u6587\u672c\u6a21\u578b\u548c\u4ee3\u7406\u6846\u67b6\u3002", "result": "\u6700\u4f73\u6a21\u578bOmniGen\u5728\u5408\u6210\u6837\u672c\u548c\u771f\u5b9e\u6837\u672c\u4e2d\u5206\u522b\u4ec5\u8fbe\u523066.6%\u548c79.0%\u7684\u51c6\u786e\u7387\uff0c\u8868\u660e\u73b0\u6709\u7cfb\u7edf\u5728\u591a\u53c2\u8003\u6761\u4ef6\u4e0b\u4ecd\u6709\u6311\u6218\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u66f4\u7075\u6d3b\u3001\u7c7b\u4eba\u7684\u521b\u610f\u5de5\u5177\u63d0\u4f9b\u4e86\u65b9\u5411\uff0cMultiRef\u6570\u636e\u96c6\u5c06\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2508.06916", "pdf": "https://arxiv.org/pdf/2508.06916", "abs": "https://arxiv.org/abs/2508.06916", "authors": ["Shichao Ma", "Yunhe Guo", "Jiahao Su", "Qihe Huang", "Zhengyang Zhou", "Yang Wang"], "title": "Talk2Image: A Multi-Agent System for Multi-Turn Image Generation and Editing", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image generation tasks have driven remarkable advances in diverse media applications, yet most focus on single-turn scenarios and struggle with iterative, multi-turn creative tasks. Recent dialogue-based systems attempt to bridge this gap, but their single-agent, sequential paradigm often causes intention drift and incoherent edits. To address these limitations, we present Talk2Image, a novel multi-agent system for interactive image generation and editing in multi-turn dialogue scenarios. Our approach integrates three key components: intention parsing from dialogue history, task decomposition and collaborative execution across specialized agents, and feedback-driven refinement based on a multi-view evaluation mechanism. Talk2Image enables step-by-step alignment with user intention and consistent image editing. Experiments demonstrate that Talk2Image outperforms existing baselines in controllability, coherence, and user satisfaction across iterative image generation and editing tasks.", "AI": {"tldr": "Talk2Image\u662f\u4e00\u4e2a\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u4ea4\u4e92\u5f0f\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5355\u4ee3\u7406\u7cfb\u7edf\u7684\u610f\u56fe\u6f02\u79fb\u548c\u4e0d\u8fde\u8d2f\u7f16\u8f91\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u591a\u5173\u6ce8\u5355\u8f6e\u573a\u666f\uff0c\u96be\u4ee5\u5904\u7406\u591a\u8f6e\u8fed\u4ee3\u521b\u4f5c\u4efb\u52a1\uff0c\u4e14\u5355\u4ee3\u7406\u7cfb\u7edf\u6613\u5bfc\u81f4\u610f\u56fe\u6f02\u79fb\u548c\u7f16\u8f91\u4e0d\u8fde\u8d2f\u3002", "method": "Talk2Image\u901a\u8fc7\u610f\u56fe\u89e3\u6790\u3001\u4efb\u52a1\u5206\u89e3\u4e0e\u534f\u4f5c\u6267\u884c\u3001\u591a\u89c6\u89d2\u53cd\u9988\u9a71\u52a8\u4f18\u5316\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\u5b9e\u73b0\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u56fe\u50cf\u751f\u6210\u4e0e\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTalk2Image\u5728\u53ef\u63a7\u6027\u3001\u8fde\u8d2f\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "Talk2Image\u901a\u8fc7\u591a\u4ee3\u7406\u534f\u4f5c\u548c\u53cd\u9988\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8f6e\u56fe\u50cf\u751f\u6210\u4e0e\u7f16\u8f91\u7684\u6548\u679c\u3002"}}
{"id": "2508.06924", "pdf": "https://arxiv.org/pdf/2508.06924", "abs": "https://arxiv.org/abs/2508.06924", "authors": ["Shihao Yuan", "Yahui Liu", "Yang Yue", "Jingyuan Zhang", "Wangmeng Zuo", "Qi Wang", "Fuzheng Zhang", "Guorui Zhou"], "title": "AR-GRPO: Training Autoregressive Image Generation Models via Reinforcement Learning", "categories": ["cs.CV"], "comment": "27 pages, 15 figures", "summary": "Inspired by the success of reinforcement learning (RL) in refining large language models (LLMs), we propose AR-GRPO, an approach to integrate online RL training into autoregressive (AR) image generation models. We adapt the Group Relative Policy Optimization (GRPO) algorithm to refine the vanilla autoregressive models' outputs by carefully designed reward functions that evaluate generated images across multiple quality dimensions, including perceptual quality, realism, and semantic fidelity. We conduct comprehensive experiments on both class-conditional (i.e., class-to-image) and text-conditional (i.e., text-to-image) image generation tasks, demonstrating that our RL-enhanced framework significantly improves both the image quality and human preference of generated images compared to the standard AR baselines. Our results show consistent improvements across various evaluation metrics, establishing the viability of RL-based optimization for AR image generation and opening new avenues for controllable and high-quality image synthesis. The source codes and models are available at: https://github.com/Kwai-Klear/AR-GRPO.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAR-GRPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u548c\u4eba\u7c7b\u504f\u597d\u3002", "motivation": "\u53d7\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6210\u529f\u542f\u53d1\uff0c\u63a2\u7d22\u5176\u5728\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u63d0\u5347\u751f\u6210\u56fe\u50cf\u7684\u591a\u7ef4\u8d28\u91cf\u3002", "method": "\u91c7\u7528Group Relative Policy Optimization (GRPO)\u7b97\u6cd5\uff0c\u7ed3\u5408\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\uff0c\u8bc4\u4f30\u751f\u6210\u56fe\u50cf\u7684\u611f\u77e5\u8d28\u91cf\u3001\u771f\u5b9e\u6027\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002", "result": "\u5728\u7c7b\u6761\u4ef6\u548c\u6587\u672c\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u5b9e\u9a8c\u8868\u660eRL\u589e\u5f3a\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u81ea\u56de\u5f52\u57fa\u7ebf\uff0c\u5404\u9879\u8bc4\u4f30\u6307\u6807\u5747\u6709\u63d0\u5347\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4f18\u5316\u5728\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u53ef\u63a7\u9ad8\u8d28\u91cf\u56fe\u50cf\u5408\u6210\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.06937", "pdf": "https://arxiv.org/pdf/2508.06937", "abs": "https://arxiv.org/abs/2508.06937", "authors": ["Weiyan Xie", "Han Gao", "Didan Deng", "Kaican Li", "April Hua Liu", "Yongxiang Huang", "Nevin L. Zhang"], "title": "CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-Free Image Editing", "categories": ["cs.CV", "cs.AI"], "comment": "Project Page: vaynexie.github.io/CannyEdit/", "summary": "Recent advances in text-to-image (T2I) models have enabled training-free regional image editing by leveraging the generative priors of foundation models. However, existing methods struggle to balance text adherence in edited regions, context fidelity in unedited areas, and seamless integration of edits. We introduce CannyEdit, a novel training-free framework that addresses these challenges through two key innovations: (1) Selective Canny Control, which masks the structural guidance of Canny ControlNet in user-specified editable regions while strictly preserving details of the source images in unedited areas via inversion-phase ControlNet information retention. This enables precise, text-driven edits without compromising contextual integrity. (2) Dual-Prompt Guidance, which combines local prompts for object-specific edits with a global target prompt to maintain coherent scene interactions. On real-world image editing tasks (addition, replacement, removal), CannyEdit outperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent improvement in the balance of text adherence and context fidelity. In terms of editing seamlessness, user studies reveal only 49.2 percent of general users and 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited when paired with real images without edits, versus 76.08 to 89.09 percent for competitor methods.", "AI": {"tldr": "CannyEdit\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027Canny\u63a7\u5236\u548c\u53cc\u63d0\u793a\u5f15\u5bfc\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6587\u672c\u9075\u4ece\u6027\u3001\u4e0a\u4e0b\u6587\u4fdd\u771f\u5ea6\u548c\u7f16\u8f91\u65e0\u7f1d\u6027\u4e0a\u7684\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u533a\u57df\u7f16\u8f91\u4e2d\u96be\u4ee5\u5e73\u8861\u6587\u672c\u9075\u4ece\u6027\u3001\u4e0a\u4e0b\u6587\u4fdd\u771f\u5ea6\u548c\u7f16\u8f91\u65e0\u7f1d\u6027\u3002", "method": "CannyEdit\u91c7\u7528\u9009\u62e9\u6027Canny\u63a7\u5236\uff08\u4fdd\u7559\u672a\u7f16\u8f91\u533a\u57df\u7ec6\u8282\uff09\u548c\u53cc\u63d0\u793a\u5f15\u5bfc\uff08\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u63d0\u793a\uff09\u5b9e\u73b0\u7cbe\u786e\u7f16\u8f91\u3002", "result": "CannyEdit\u5728\u771f\u5b9e\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6587\u672c\u9075\u4ece\u6027\u548c\u4e0a\u4e0b\u6587\u4fdd\u771f\u5ea6\u5e73\u8861\u63d0\u53472.93%\u81f310.49%\uff0c\u4e14\u7f16\u8f91\u65e0\u7f1d\u6027\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "CannyEdit\u901a\u8fc7\u521b\u65b0\u6027\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u7f16\u8f91\u7684\u8d28\u91cf\u548c\u81ea\u7136\u5ea6\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2508.06982", "pdf": "https://arxiv.org/pdf/2508.06982", "abs": "https://arxiv.org/abs/2508.06982", "authors": ["Yixin Zhu", "Zuoliang Zhu", "Milo\u0161 Ha\u0161an", "Jian Yang", "Jin Xie", "Beibei Wang"], "title": "WeatherDiffusion: Weather-Guided Diffusion Model for Forward and Inverse Rendering", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Forward and inverse rendering have emerged as key techniques for enabling understanding and reconstruction in the context of autonomous driving (AD). However, complex weather and illumination pose great challenges to this task. The emergence of large diffusion models has shown promise in achieving reasonable results through learning from 2D priors, but these models are difficult to control and lack robustness. In this paper, we introduce WeatherDiffusion, a diffusion-based framework for forward and inverse rendering on AD scenes with various weather and lighting conditions. Our method enables authentic estimation of material properties, scene geometry, and lighting, and further supports controllable weather and illumination editing through the use of predicted intrinsic maps guided by text descriptions. We observe that different intrinsic maps should correspond to different regions of the original image. Based on this observation, we propose Intrinsic map-aware attention (MAA) to enable high-quality inverse rendering. Additionally, we introduce a synthetic dataset (\\ie WeatherSynthetic) and a real-world dataset (\\ie WeatherReal) for forward and inverse rendering on AD scenes with diverse weather and lighting. Extensive experiments show that our WeatherDiffusion outperforms state-of-the-art methods on several benchmarks. Moreover, our method demonstrates significant value in downstream tasks for AD, enhancing the robustness of object detection and image segmentation in challenging weather scenarios.", "AI": {"tldr": "WeatherDiffusion\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u6b63\u5411\u548c\u9006\u5411\u6e32\u67d3\uff0c\u652f\u6301\u5929\u6c14\u548c\u5149\u7167\u7f16\u8f91\uff0c\u5e76\u901a\u8fc7\u63d0\u51fa\u7684MAA\u673a\u5236\u63d0\u5347\u6e32\u67d3\u8d28\u91cf\u3002", "motivation": "\u590d\u6742\u5929\u6c14\u548c\u5149\u7167\u6761\u4ef6\u5bf9\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u7684\u7406\u89e3\u548c\u91cd\u5efa\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u73b0\u6709\u6269\u6563\u6a21\u578b\u96be\u4ee5\u63a7\u5236\u4e14\u7f3a\u4e4f\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faWeatherDiffusion\u6846\u67b6\uff0c\u7ed3\u5408\u6587\u672c\u5f15\u5bfc\u7684\u9884\u6d4b\u672c\u5f81\u56fe\uff0c\u5b9e\u73b0\u53ef\u63a7\u7684\u5929\u6c14\u548c\u5149\u7167\u7f16\u8f91\uff0c\u5e76\u5f15\u5165MAA\u673a\u5236\u63d0\u5347\u9006\u5411\u6e32\u67d3\u8d28\u91cf\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u76ee\u6807\u68c0\u6d4b\u548c\u56fe\u50cf\u5206\u5272\uff09\u4e2d\u663e\u8457\u63d0\u5347\u9c81\u68d2\u6027\u3002", "conclusion": "WeatherDiffusion\u4e3a\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u7684\u6e32\u67d3\u548c\u7f16\u8f91\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.06988", "pdf": "https://arxiv.org/pdf/2508.06988", "abs": "https://arxiv.org/abs/2508.06988", "authors": ["Fangmin Zhao", "Weichao Zeng", "Zhenhang Li", "Dongbao Yang", "Yu Zhou"], "title": "TADoc: Robust Time-Aware Document Image Dewarping", "categories": ["cs.CV"], "comment": "8 pages, 8 figures", "summary": "Flattening curved, wrinkled, and rotated document images captured by portable photographing devices, termed document image dewarping, has become an increasingly important task with the rise of digital economy and online working. Although many methods have been proposed recently, they often struggle to achieve satisfactory results when confronted with intricate document structures and higher degrees of deformation in real-world scenarios. Our main insight is that, unlike other document restoration tasks (e.g., deblurring), dewarping in real physical scenes is a progressive motion rather than a one-step transformation. Based on this, we have undertaken two key initiatives. Firstly, we reformulate this task, modeling it for the first time as a dynamic process that encompasses a series of intermediate states. Secondly, we design a lightweight framework called TADoc (Time-Aware Document Dewarping Network) to address the geometric distortion of document images. In addition, due to the inadequacy of OCR metrics for document images containing sparse text, the comprehensiveness of evaluation is insufficient. To address this shortcoming, we propose a new metric -- DLS (Document Layout Similarity) -- to evaluate the effectiveness of document dewarping in downstream tasks. Extensive experiments and in-depth evaluations have been conducted and the results indicate that our model possesses strong robustness, achieving superiority on several benchmarks with different document types and degrees of distortion.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u5efa\u6a21\u6587\u6863\u53bb\u626d\u66f2\u4efb\u52a1\u7684\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u8f7b\u91cf\u7ea7\u6846\u67b6TADoc\uff0c\u540c\u65f6\u5f15\u5165\u65b0\u8bc4\u4ef7\u6307\u6807DLS\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u6587\u6863\u7ed3\u6784\u548c\u9ad8\u53d8\u5f62\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u4f5c\u8005\u8ba4\u4e3a\u53bb\u626d\u66f2\u662f\u4e00\u4e2a\u6e10\u8fdb\u8fc7\u7a0b\u800c\u975e\u4e00\u6b65\u8f6c\u6362\u3002", "method": "\u5c06\u4efb\u52a1\u91cd\u65b0\u5efa\u6a21\u4e3a\u52a8\u6001\u8fc7\u7a0b\uff0c\u8bbe\u8ba1TADoc\u6846\u67b6\uff0c\u5e76\u63d0\u51faDLS\u8bc4\u4ef7\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u6a21\u578b\u5177\u6709\u5f3a\u9c81\u68d2\u6027\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u52a8\u6001\u5efa\u6a21\u548cTADoc\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u6587\u6863\u53bb\u626d\u66f2\u6027\u80fd\uff0cDLS\u6307\u6807\u8865\u5145\u4e86\u73b0\u6709\u8bc4\u4ef7\u4e0d\u8db3\u3002"}}
{"id": "2508.07006", "pdf": "https://arxiv.org/pdf/2508.07006", "abs": "https://arxiv.org/abs/2508.07006", "authors": ["Gian Mario Favero", "Ge Ya Luo", "Nima Fathi", "Justin Szeto", "Douglas L. Arnold", "Brennan Nichyporuk", "Chris Pal", "Tal Arbel"], "title": "Spatio-Temporal Conditional Diffusion Models for Forecasting Future Multiple Sclerosis Lesion Masks Conditioned on Treatments", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted to MICCAI 2025 (LMID Workshop)", "summary": "Image-based personalized medicine has the potential to transform healthcare, particularly for diseases that exhibit heterogeneous progression such as Multiple Sclerosis (MS). In this work, we introduce the first treatment-aware spatio-temporal diffusion model that is able to generate future masks demonstrating lesion evolution in MS. Our voxel-space approach incorporates multi-modal patient data, including MRI and treatment information, to forecast new and enlarging T2 (NET2) lesion masks at a future time point. Extensive experiments on a multi-centre dataset of 2131 patient 3D MRIs from randomized clinical trials for relapsing-remitting MS demonstrate that our generative model is able to accurately predict NET2 lesion masks for patients across six different treatments. Moreover, we demonstrate our model has the potential for real-world clinical applications through downstream tasks such as future lesion count and location estimation, binary lesion activity classification, and generating counterfactual future NET2 masks for several treatments with different efficacies. This work highlights the potential of causal, image-based generative models as powerful tools for advancing data-driven prognostics in MS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u7684\u6cbb\u7597\u611f\u77e5\u65f6\u7a7a\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u591a\u53d1\u6027\u786c\u5316\u75c7\uff08MS\uff09\u60a3\u8005\u7684\u672a\u6765\u75c5\u53d8\u6f14\u53d8\u3002", "motivation": "\u591a\u53d1\u6027\u786c\u5316\u75c7\uff08MS\uff09\u7684\u5f02\u8d28\u6027\u8fdb\u5c55\u9700\u8981\u4e2a\u6027\u5316\u533b\u7597\uff0c\u800c\u57fa\u4e8e\u56fe\u50cf\u7684\u9884\u6d4b\u6a21\u578b\u53ef\u4ee5\u4e3a\u6b64\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u91c7\u7528\u4f53\u7d20\u7a7a\u95f4\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u60a3\u8005\u6570\u636e\uff08\u5982MRI\u548c\u6cbb\u7597\u4fe1\u606f\uff09\uff0c\u9884\u6d4b\u672a\u6765\u65f6\u95f4\u70b9\u7684T2\u75c5\u53d8\uff08NET2\uff09\u6f14\u53d8\u3002", "result": "\u57282131\u540d\u60a3\u8005\u76843D MRI\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6a21\u578b\u80fd\u51c6\u786e\u9884\u6d4b\u516d\u79cd\u4e0d\u540c\u6cbb\u7597\u65b9\u6848\u7684NET2\u75c5\u53d8\uff0c\u5e76\u5c55\u793a\u51fa\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u57fa\u4e8e\u56e0\u679c\u5173\u7cfb\u7684\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728MS\u6570\u636e\u9a71\u52a8\u9884\u540e\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.07031", "pdf": "https://arxiv.org/pdf/2508.07031", "abs": "https://arxiv.org/abs/2508.07031", "authors": ["Anindya Bijoy Das", "Shahnewaz Karim Sakib", "Shibbir Ahmed"], "title": "Trustworthy Medical Imaging with Large Language Models: A Study of Hallucinations Across Modalities", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly applied to medical imaging tasks, including image interpretation and synthetic image generation. However, these models often produce hallucinations, which are confident but incorrect outputs that can mislead clinical decisions. This study examines hallucinations in two directions: image to text, where LLMs generate reports from X-ray, CT, or MRI scans, and text to image, where models create medical images from clinical prompts. We analyze errors such as factual inconsistencies and anatomical inaccuracies, evaluating outputs using expert informed criteria across imaging modalities. Our findings reveal common patterns of hallucination in both interpretive and generative tasks, with implications for clinical reliability. We also discuss factors contributing to these failures, including model architecture and training data. By systematically studying both image understanding and generation, this work provides insights into improving the safety and trustworthiness of LLM driven medical imaging systems.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5206\u6790\u4e86\u56fe\u50cf\u5230\u6587\u672c\u548c\u6587\u672c\u5230\u56fe\u50cf\u4e24\u79cd\u65b9\u5411\u4e2d\u7684\u9519\u8bef\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "LLMs\u5728\u533b\u5b66\u5f71\u50cf\u5e94\u7528\u4e2d\u5e38\u4ea7\u751f\u8bef\u5bfc\u6027\u5e7b\u89c9\uff0c\u5f71\u54cd\u4e34\u5e8a\u51b3\u7b56\uff0c\u9700\u7cfb\u7edf\u6027\u7814\u7a76\u5176\u9519\u8bef\u6a21\u5f0f\u548c\u539f\u56e0\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u4e13\u5bb6\u6807\u51c6\u8bc4\u4f30LLMs\u5728\u56fe\u50cf\u5230\u6587\u672c\uff08\u751f\u6210\u62a5\u544a\uff09\u548c\u6587\u672c\u5230\u56fe\u50cf\uff08\u751f\u6210\u5f71\u50cf\uff09\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\uff0c\u5206\u6790\u9519\u8bef\u7c7b\u578b\u3002", "result": "\u53d1\u73b0\u4e24\u79cd\u4efb\u52a1\u4e2d\u5747\u5b58\u5728\u5e38\u89c1\u5e7b\u89c9\u6a21\u5f0f\uff0c\u5982\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u548c\u89e3\u5256\u5b66\u9519\u8bef\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u6570\u636e\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u63d0\u5347LLM\u9a71\u52a8\u7684\u533b\u5b66\u5f71\u50cf\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u4fe1\u5ea6\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2508.07038", "pdf": "https://arxiv.org/pdf/2508.07038", "abs": "https://arxiv.org/abs/2508.07038", "authors": ["Yuke Xing", "William Gordon", "Qi Yang", "Kaifa Yang", "Jiarui Wang", "Yiling Xu"], "title": "3DGS-VBench: A Comprehensive Video Quality Evaluation Benchmark for 3DGS Compression", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) enables real-time novel view synthesis with high visual fidelity, but its substantial storage requirements hinder practical deployment, prompting state-of-the-art (SOTA) 3DGS methods to incorporate compression modules. However, these 3DGS generative compression techniques introduce unique distortions lacking systematic quality assessment research. To this end, we establish 3DGS-VBench, a large-scale Video Quality Assessment (VQA) Dataset and Benchmark with 660 compressed 3DGS models and video sequences generated from 11 scenes across 6 SOTA 3DGS compression algorithms with systematically designed parameter levels. With annotations from 50 participants, we obtained MOS scores with outlier removal and validated dataset reliability. We benchmark 6 3DGS compression algorithms on storage efficiency and visual quality, and evaluate 15 quality assessment metrics across multiple paradigms. Our work enables specialized VQA model training for 3DGS, serving as a catalyst for compression and quality assessment research. The dataset is available at https://github.com/YukeXing/3DGS-VBench.", "AI": {"tldr": "3DGS-VBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f303D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u538b\u7f29\u7b97\u6cd5\u89c6\u89c9\u8d28\u91cf\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b660\u4e2a\u538b\u7f29\u6a21\u578b\u548c\u89c6\u9891\u5e8f\u5217\uff0c\u5e76\u63d0\u4f9b\u4e86MOS\u8bc4\u5206\u548c\u591a\u79cd\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u7684\u5bf9\u6bd4\u3002", "motivation": "3DGS\u7684\u9ad8\u5b58\u50a8\u9700\u6c42\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\uff0c\u800c\u73b0\u6709\u7684\u538b\u7f29\u6280\u672f\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u89c6\u89c9\u8d28\u91cf\u8bc4\u4f30\u7814\u7a76\u3002", "method": "\u5efa\u7acb\u4e863DGS-VBench\u6570\u636e\u96c6\uff0c\u5305\u542b11\u4e2a\u573a\u666f\u7684660\u4e2a\u538b\u7f293DGS\u6a21\u578b\u548c\u89c6\u9891\u5e8f\u5217\uff0c\u753150\u540d\u53c2\u4e0e\u8005\u6807\u6ce8MOS\u8bc4\u5206\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u7684\u53ef\u9760\u6027\u3002", "result": "\u8bc4\u4f30\u4e866\u79cdSOTA 3DGS\u538b\u7f29\u7b97\u6cd5\u7684\u5b58\u50a8\u6548\u7387\u548c\u89c6\u89c9\u8d28\u91cf\uff0c\u5e76\u5bf9\u6bd4\u4e8615\u79cd\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a3DGS\u7684\u538b\u7f29\u548c\u8d28\u91cf\u8bc4\u4f30\u7814\u7a76\u63d0\u4f9b\u4e86\u4e13\u7528\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u76f8\u5173\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.07089", "pdf": "https://arxiv.org/pdf/2508.07089", "abs": "https://arxiv.org/abs/2508.07089", "authors": ["Sandro Papais", "Letian Wang", "Brian Cheong", "Steven L. Waslander"], "title": "ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted to ICCV 2025", "summary": "We introduce ForeSight, a novel joint detection and forecasting framework for vision-based 3D perception in autonomous vehicles. Traditional approaches treat detection and forecasting as separate sequential tasks, limiting their ability to leverage temporal cues. ForeSight addresses this limitation with a multi-task streaming and bidirectional learning approach, allowing detection and forecasting to share query memory and propagate information seamlessly. The forecast-aware detection transformer enhances spatial reasoning by integrating trajectory predictions from a multiple hypothesis forecast memory queue, while the streaming forecast transformer improves temporal consistency using past forecasts and refined detections. Unlike tracking-based methods, ForeSight eliminates the need for explicit object association, reducing error propagation with a tracking-free model that efficiently scales across multi-frame sequences. Experiments on the nuScenes dataset show that ForeSight achieves state-of-the-art performance, achieving an EPA of 54.9%, surpassing previous methods by 9.3%, while also attaining the best mAP and minADE among multi-view detection and forecasting models.", "AI": {"tldr": "ForeSight\u662f\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u89c6\u89c93D\u611f\u77e5\u7684\u8054\u5408\u68c0\u6d4b\u4e0e\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u6d41\u5f0f\u5b66\u4e60\u548c\u53cc\u5411\u5b66\u4e60\u63d0\u5347\u6027\u80fd\uff0c\u65e0\u9700\u663e\u5f0f\u76ee\u6807\u5173\u8054\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u68c0\u6d4b\u4e0e\u9884\u6d4b\u4f5c\u4e3a\u72ec\u7acb\u4efb\u52a1\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u65f6\u95f4\u7ebf\u7d22\uff0cForeSight\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u4efb\u52a1\u6d41\u5f0f\u5b66\u4e60\u548c\u53cc\u5411\u5b66\u4e60\uff0c\u7ed3\u5408\u68c0\u6d4b\u4e0e\u9884\u6d4b\u7684\u67e5\u8be2\u8bb0\u5fc6\uff0c\u5229\u7528\u8f68\u8ff9\u9884\u6d4b\u589e\u5f3a\u7a7a\u95f4\u63a8\u7406\uff0c\u5e76\u901a\u8fc7\u6d41\u5f0f\u9884\u6d4b\u53d8\u6362\u5668\u63d0\u5347\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0cForeSight\u4ee554.9%\u7684EPA\u9886\u5148\u5148\u524d\u65b9\u6cd59.3%\uff0c\u5e76\u5728mAP\u548cminADE\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "ForeSight\u901a\u8fc7\u8054\u5408\u68c0\u6d4b\u4e0e\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a763D\u611f\u77e5\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2508.07146", "pdf": "https://arxiv.org/pdf/2508.07146", "abs": "https://arxiv.org/abs/2508.07146", "authors": ["Yu Liu", "Zhijie Liu", "Xiao Ren", "You-Fu Li", "He Kong"], "title": "Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Predicting pedestrian motion trajectories is critical for the path planning and motion control of autonomous vehicles. Recent diffusion-based models have shown promising results in capturing the inherent stochasticity of pedestrian behavior for trajectory prediction. However, the absence of explicit semantic modelling of pedestrian intent in many diffusion-based methods may result in misinterpreted behaviors and reduced prediction accuracy. To address the above challenges, we propose a diffusion-based pedestrian trajectory prediction framework that incorporates both short-term and long-term motion intentions. Short-term intent is modelled using a residual polar representation, which decouples direction and magnitude to capture fine-grained local motion patterns. Long-term intent is estimated through a learnable, token-based endpoint predictor that generates multiple candidate goals with associated probabilities, enabling multimodal and context-aware intention modelling. Furthermore, we enhance the diffusion process by incorporating adaptive guidance and a residual noise predictor that dynamically refines denoising accuracy. The proposed framework is evaluated on the widely used ETH, UCY, and SDD benchmarks, demonstrating competitive results against state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u77ed\u671f\u548c\u957f\u671f\u8fd0\u52a8\u610f\u56fe\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6307\u5bfc\u548c\u6b8b\u5dee\u566a\u58f0\u9884\u6d4b\u5668\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u7f3a\u4e4f\u5bf9\u884c\u4eba\u610f\u56fe\u7684\u663e\u5f0f\u8bed\u4e49\u5efa\u6a21\uff0c\u53ef\u80fd\u5bfc\u81f4\u884c\u4e3a\u8bef\u89e3\u548c\u9884\u6d4b\u7cbe\u5ea6\u4e0b\u964d\u3002", "method": "\u77ed\u671f\u610f\u56fe\u901a\u8fc7\u6b8b\u5dee\u6781\u5750\u6807\u8868\u793a\u5efa\u6a21\uff0c\u957f\u671f\u610f\u56fe\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u57fa\u4e8e\u4ee4\u724c\u7684\u7ec8\u70b9\u9884\u6d4b\u5668\u4f30\u8ba1\uff0c\u5e76\u7ed3\u5408\u81ea\u9002\u5e94\u6307\u5bfc\u548c\u6b8b\u5dee\u566a\u58f0\u9884\u6d4b\u5668\u4f18\u5316\u6269\u6563\u8fc7\u7a0b\u3002", "result": "\u5728ETH\u3001UCY\u548cSDD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408\u610f\u56fe\u5efa\u6a21\u7684\u81ea\u9002\u5e94\u6269\u6563\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.07149", "pdf": "https://arxiv.org/pdf/2508.07149", "abs": "https://arxiv.org/abs/2508.07149", "authors": ["Ruolin Yang", "Da Li", "Honggang Zhang", "Yi-Zhe Song"], "title": "SketchAnimator: Animate Sketch via Motion Customization of Text-to-Video Diffusion Models", "categories": ["cs.CV"], "comment": "2024 IEEE International Conference on Visual Communications and Image   Processing (VCIP); Oral", "summary": "Sketching is a uniquely human tool for expressing ideas and creativity. The animation of sketches infuses life into these static drawings, opening a new dimension for designers. Animating sketches is a time-consuming process that demands professional skills and extensive experience, often proving daunting for amateurs. In this paper, we propose a novel sketch animation model SketchAnimator, which enables adding creative motion to a given sketch, like \"a jumping car''. Namely, given an input sketch and a reference video, we divide the sketch animation into three stages: Appearance Learning, Motion Learning and Video Prior Distillation. In stages 1 and 2, we utilize LoRA to integrate sketch appearance information and motion dynamics from the reference video into the pre-trained T2V model. In the third stage, we utilize Score Distillation Sampling (SDS) to update the parameters of the Bezier curves in each sketch frame according to the acquired motion information. Consequently, our model produces a sketch video that not only retains the original appearance of the sketch but also mirrors the dynamic movements of the reference video. We compare our method with alternative approaches and demonstrate that it generates the desired sketch video under the challenge of one-shot motion customization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSketchAnimator\u7684\u8349\u56fe\u52a8\u753b\u6a21\u578b\uff0c\u901a\u8fc7\u4e09\u4e2a\u9636\u6bb5\uff08\u5916\u89c2\u5b66\u4e60\u3001\u8fd0\u52a8\u5b66\u4e60\u548c\u89c6\u9891\u5148\u9a8c\u84b8\u998f\uff09\u5c06\u9759\u6001\u8349\u56fe\u8f6c\u5316\u4e3a\u52a8\u6001\u89c6\u9891\uff0c\u4fdd\u7559\u8349\u56fe\u5916\u89c2\u7684\u540c\u65f6\u6a21\u4eff\u53c2\u8003\u89c6\u9891\u7684\u8fd0\u52a8\u3002", "motivation": "\u8349\u56fe\u52a8\u753b\u901a\u5e38\u9700\u8981\u4e13\u4e1a\u6280\u80fd\u4e14\u8017\u65f6\uff0c\u4e1a\u4f59\u7528\u6237\u96be\u4ee5\u5b8c\u6210\u3002\u672c\u6587\u65e8\u5728\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u4f7f\u975e\u4e13\u4e1a\u4eba\u58eb\u4e5f\u80fd\u4e3a\u8349\u56fe\u6dfb\u52a0\u521b\u610f\u8fd0\u52a8\u3002", "method": "\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a1\uff09\u5916\u89c2\u5b66\u4e60\uff0c2\uff09\u8fd0\u52a8\u5b66\u4e60\uff08\u5229\u7528LoRA\u6574\u5408\u8349\u56fe\u5916\u89c2\u548c\u53c2\u8003\u89c6\u9891\u7684\u8fd0\u52a8\u4fe1\u606f\uff09\uff0c3\uff09\u89c6\u9891\u5148\u9a8c\u84b8\u998f\uff08\u4f7f\u7528SDS\u66f4\u65b0Bezier\u66f2\u7ebf\u53c2\u6570\uff09\u3002", "result": "\u6a21\u578b\u80fd\u591f\u751f\u6210\u4fdd\u7559\u8349\u56fe\u5916\u89c2\u5e76\u6a21\u4eff\u53c2\u8003\u89c6\u9891\u8fd0\u52a8\u7684\u52a8\u753b\uff0c\u4e14\u5728\u5355\u6b21\u8fd0\u52a8\u5b9a\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "SketchAnimator\u4e3a\u8349\u56fe\u52a8\u753b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7528\u6237\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u975e\u4e13\u4e1a\u4eba\u58eb\u3002"}}
{"id": "2508.07211", "pdf": "https://arxiv.org/pdf/2508.07211", "abs": "https://arxiv.org/abs/2508.07211", "authors": ["Junyi He", "Liuling Chen", "Hongyang Zhou", "Zhang xiaoxing", "Xiaobin Zhu", "Shengxiang Yu", "Jingyan Qin", "Xu-Cheng Yin"], "title": "Similarity Matters: A Novel Depth-guided Network for Image Restoration and A New Dataset", "categories": ["cs.CV"], "comment": "12 pages, 10 figures", "summary": "Image restoration has seen substantial progress in recent years. However, existing methods often neglect depth information, which hurts similarity matching, results in attention distractions in shallow depth-of-field (DoF) scenarios, and excessive enhancement of background content in deep DoF settings. To overcome these limitations, we propose a novel Depth-Guided Network (DGN) for image restoration, together with a novel large-scale high-resolution dataset. Specifically, the network consists of two interactive branches: a depth estimation branch that provides structural guidance, and an image restoration branch that performs the core restoration task. In addition, the image restoration branch exploits intra-object similarity through progressive window-based self-attention and captures inter-object similarity via sparse non-local attention. Through joint training, depth features contribute to improved restoration quality, while the enhanced visual features from the restoration branch in turn help refine depth estimation. Notably, we also introduce a new dataset for training and evaluation, consisting of 9,205 high-resolution images from 403 plant species, with diverse depth and texture variations. Extensive experiments show that our method achieves state-of-the-art performance on several standard benchmarks and generalizes well to unseen plant images, demonstrating its effectiveness and robustness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u5f15\u5bfc\u7f51\u7edc\uff08DGN\uff09\u7528\u4e8e\u56fe\u50cf\u6062\u590d\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u6df1\u5ea6\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u5927\u89c4\u6a21\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u5e38\u5ffd\u7565\u6df1\u5ea6\u4fe1\u606f\uff0c\u5bfc\u81f4\u76f8\u4f3c\u6027\u5339\u914d\u4e0d\u4f73\u3001\u6d45\u666f\u6df1\u573a\u666f\u4e2d\u6ce8\u610f\u529b\u5206\u6563\u4ee5\u53ca\u6df1\u666f\u6df1\u80cc\u666f\u4e0b\u5185\u5bb9\u8fc7\u5ea6\u589e\u5f3a\u3002", "method": "DGN\u5305\u542b\u4e24\u4e2a\u4ea4\u4e92\u5206\u652f\uff1a\u6df1\u5ea6\u4f30\u8ba1\u5206\u652f\u63d0\u4f9b\u7ed3\u6784\u5f15\u5bfc\uff0c\u56fe\u50cf\u6062\u590d\u5206\u652f\u6267\u884c\u6838\u5fc3\u4efb\u52a1\u3002\u6062\u590d\u5206\u652f\u901a\u8fc7\u6e10\u8fdb\u7a97\u53e3\u81ea\u6ce8\u610f\u529b\u548c\u7a00\u758f\u975e\u5c40\u90e8\u6ce8\u610f\u529b\u6355\u6349\u5bf9\u8c61\u5185\u548c\u5bf9\u8c61\u95f4\u76f8\u4f3c\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6807\u51c6\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u690d\u7269\u56fe\u50cf\u3002", "conclusion": "\u6df1\u5ea6\u5f15\u5bfc\u7f51\u7edc\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u63d0\u5347\u4e86\u6062\u590d\u8d28\u91cf\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u6df1\u5ea6\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.07214", "pdf": "https://arxiv.org/pdf/2508.07214", "abs": "https://arxiv.org/abs/2508.07214", "authors": ["Hongyang Zhou", "Xiaobin Zhu", "Liuling Chen", "Junyi He", "Jingyan Qin", "Xu-Cheng Yin", "Zhang xiaoxing"], "title": "Unsupervised Real-World Super-Resolution via Rectified Flow Degradation Modelling", "categories": ["cs.CV", "eess.IV"], "comment": "10 pages, 9 figures", "summary": "Unsupervised real-world super-resolution (SR) faces critical challenges due to the complex, unknown degradation distributions in practical scenarios. Existing methods struggle to generalize from synthetic low-resolution (LR) and high-resolution (HR) image pairs to real-world data due to a significant domain gap. In this paper, we propose an unsupervised real-world SR method based on rectified flow to effectively capture and model real-world degradation, synthesizing LR-HR training pairs with realistic degradation. Specifically, given unpaired LR and HR images, we propose a novel Rectified Flow Degradation Module (RFDM) that introduces degradation-transformed LR (DT-LR) images as intermediaries. By modeling the degradation trajectory in a continuous and invertible manner, RFDM better captures real-world degradation and enhances the realism of generated LR images. Additionally, we propose a Fourier Prior Guided Degradation Module (FGDM) that leverages structural information embedded in Fourier phase components to ensure more precise modeling of real-world degradation. Finally, the LR images are processed by both FGDM and RFDM, producing final synthetic LR images with real-world degradation. The synthetic LR images are paired with the given HR images to train the off-the-shelf SR networks. Extensive experiments on real-world datasets demonstrate that our method significantly enhances the performance of existing SR approaches in real-world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6821\u6b63\u6d41\u7684\u65e0\u76d1\u7763\u771f\u5b9e\u4e16\u754c\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u901a\u8fc7\u65b0\u6a21\u5757RFDM\u548cFGDM\u6355\u6349\u771f\u5b9e\u9000\u5316\uff0c\u63d0\u5347SR\u7f51\u7edc\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u56e0\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u57df\u5dee\u8ddd\u5927\u800c\u96be\u4ee5\u6cdb\u5316\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528RFDM\u548cFGDM\u6a21\u5757\uff0c\u5206\u522b\u901a\u8fc7\u8fde\u7eed\u53ef\u9006\u9000\u5316\u8f68\u8ff9\u548c\u5085\u91cc\u53f6\u76f8\u4f4d\u4fe1\u606f\u5efa\u6a21\u771f\u5b9e\u9000\u5316\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709SR\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u771f\u5b9e\u4e16\u754cSR\u4e2d\u7684\u9000\u5316\u5efa\u6a21\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.07246", "pdf": "https://arxiv.org/pdf/2508.07246", "abs": "https://arxiv.org/abs/2508.07246", "authors": ["Xin Ma", "Yaohui Wang", "Genyun Jia", "Xinyuan Chen", "Tien-Tsin Wong", "Cunjian Chen"], "title": "Consistent and Controllable Image Animation with Motion Linear Diffusion Transformers", "categories": ["cs.CV"], "comment": "Project Page: https://maxin-cn.github.io/miramo_project", "summary": "Image animation has seen significant progress, driven by the powerful generative capabilities of diffusion models. However, maintaining appearance consistency with static input images and mitigating abrupt motion transitions in generated animations remain persistent challenges. While text-to-video (T2V) generation has demonstrated impressive performance with diffusion transformer models, the image animation field still largely relies on U-Net-based diffusion models, which lag behind the latest T2V approaches. Moreover, the quadratic complexity of vanilla self-attention mechanisms in Transformers imposes heavy computational demands, making image animation particularly resource-intensive. To address these issues, we propose MiraMo, a framework designed to enhance efficiency, appearance consistency, and motion smoothness in image animation. Specifically, MiraMo introduces three key elements: (1) A foundational text-to-video architecture replacing vanilla self-attention with efficient linear attention to reduce computational overhead while preserving generation quality; (2) A novel motion residual learning paradigm that focuses on modeling motion dynamics rather than directly predicting frames, improving temporal consistency; and (3) A DCT-based noise refinement strategy during inference to suppress sudden motion artifacts, complemented by a dynamics control module to balance motion smoothness and expressiveness. Extensive experiments against state-of-the-art methods validate the superiority of MiraMo in generating consistent, smooth, and controllable animations with accelerated inference speed. Additionally, we demonstrate the versatility of MiraMo through applications in motion transfer and video editing tasks.", "AI": {"tldr": "MiraMo\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u4e00\u81f4\u4e14\u5e73\u6ed1\u7684\u56fe\u50cf\u52a8\u753b\u6846\u67b6\uff0c\u901a\u8fc7\u7ebf\u6027\u6ce8\u610f\u529b\u3001\u8fd0\u52a8\u6b8b\u5dee\u5b66\u4e60\u548cDCT\u566a\u58f0\u4f18\u5316\u89e3\u51b3\u73b0\u6709\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u56fe\u50cf\u52a8\u753b\u4e2d\u5916\u89c2\u4e00\u81f4\u6027\u548c\u8fd0\u52a8\u5e73\u6ed1\u6027\u7684\u6311\u6218\uff0c\u540c\u65f6\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u91c7\u7528\u7ebf\u6027\u6ce8\u610f\u529b\u66ff\u4ee3\u4f20\u7edf\u81ea\u6ce8\u610f\u529b\uff0c\u5f15\u5165\u8fd0\u52a8\u6b8b\u5dee\u5b66\u4e60\u548cDCT\u566a\u58f0\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMiraMo\u5728\u751f\u6210\u4e00\u81f4\u6027\u3001\u5e73\u6ed1\u6027\u548c\u53ef\u63a7\u6027\u52a8\u753b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MiraMo\u5728\u56fe\u50cf\u52a8\u753b\u9886\u57df\u5c55\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u591a\u529f\u80fd\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u3002"}}
{"id": "2508.07298", "pdf": "https://arxiv.org/pdf/2508.07298", "abs": "https://arxiv.org/abs/2508.07298", "authors": ["Zhiqiang Shen", "Peng Cao", "Xiaoli Liu", "Jinzhu Yang", "Osmar R. Zaiane"], "title": "SynMatch: Rethinking Consistency in Medical Image Segmentation with Sparse Annotations", "categories": ["cs.CV"], "comment": null, "summary": "Label scarcity remains a major challenge in deep learning-based medical image segmentation. Recent studies use strong-weak pseudo supervision to leverage unlabeled data. However, performance is often hindered by inconsistencies between pseudo labels and their corresponding unlabeled images. In this work, we propose \\textbf{SynMatch}, a novel framework that sidesteps the need for improving pseudo labels by synthesizing images to match them instead. Specifically, SynMatch synthesizes images using texture and shape features extracted from the same segmentation model that generates the corresponding pseudo labels for unlabeled images. This design enables the generation of highly consistent synthesized-image-pseudo-label pairs without requiring any training parameters for image synthesis. We extensively evaluate SynMatch across diverse medical image segmentation tasks under semi-supervised learning (SSL), weakly-supervised learning (WSL), and barely-supervised learning (BSL) settings with increasingly limited annotations. The results demonstrate that SynMatch achieves superior performance, especially in the most challenging BSL setting. For example, it outperforms the recent strong-weak pseudo supervision-based method by 29.71\\% and 10.05\\% on the polyp segmentation task with 5\\% and 10\\% scribble annotations, respectively. The code will be released at https://github.com/Senyh/SynMatch.", "AI": {"tldr": "SynMatch\u901a\u8fc7\u5408\u6210\u56fe\u50cf\u5339\u914d\u4f2a\u6807\u7b7e\uff0c\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u6807\u7b7e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u65e0\u9700\u6539\u8fdb\u4f2a\u6807\u7b7e\u5373\u53ef\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u6807\u7b7e\u7a00\u7f3a\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u4f2a\u6807\u7b7e\u4e0e\u672a\u6807\u8bb0\u56fe\u50cf\u4e0d\u4e00\u81f4\u800c\u6027\u80fd\u53d7\u9650\u3002", "method": "SynMatch\u5229\u7528\u5206\u5272\u6a21\u578b\u63d0\u53d6\u7684\u7eb9\u7406\u548c\u5f62\u72b6\u7279\u5f81\u5408\u6210\u56fe\u50cf\uff0c\u751f\u6210\u9ad8\u5ea6\u4e00\u81f4\u7684\u5408\u6210\u56fe\u50cf-\u4f2a\u6807\u7b7e\u5bf9\u3002", "result": "\u5728\u591a\u79cd\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728BSL\u8bbe\u7f6e\u4e0b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SynMatch\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u53c2\u6570\u7684\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6807\u7b7e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u7684\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2508.07341", "pdf": "https://arxiv.org/pdf/2508.07341", "abs": "https://arxiv.org/abs/2508.07341", "authors": ["Fangtai Wu", "Mushui Liu", "Weijie He", "Wanggui He", "Hao Jiang", "Zhao Wang", "Yunlong Yu"], "title": "CoAR: Concept Injection into Autoregressive Models for Personalized Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "The unified autoregressive (AR) model excels at multimodal understanding and generation, but its potential for customized image generation remains underexplored. Existing customized generation methods rely on full fine-tuning or adapters, making them costly and prone to overfitting or catastrophic forgetting. In this paper, we propose \\textbf{CoAR}, a novel framework for injecting subject concepts into the unified AR models while keeping all pre-trained parameters completely frozen. CoAR learns effective, specific subject representations with only a minimal number of parameters using a Layerwise Multimodal Context Learning strategy. To address overfitting and language drift, we further introduce regularization that preserves the pre-trained distribution and anchors context tokens to improve subject fidelity and re-contextualization. Additionally, CoAR supports training-free subject customization in a user-provided style. Experiments demonstrate that CoAR achieves superior performance on both subject-driven personalization and style personalization, while delivering significant gains in computational and memory efficiency. Notably, CoAR tunes less than \\textbf{0.05\\%} of the parameters while achieving competitive performance compared to recent Proxy-Tuning. Code: https://github.com/KZF-kzf/CoAR", "AI": {"tldr": "CoAR\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7edf\u4e00\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u6ce8\u5165\u4e3b\u9898\u6982\u5ff5\uff0c\u540c\u65f6\u4fdd\u6301\u6240\u6709\u9884\u8bad\u7ec3\u53c2\u6570\u51bb\u7ed3\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u548c\u5185\u5b58\u6548\u7387\u3002", "motivation": "\u63a2\u7d22\u7edf\u4e00\u81ea\u56de\u5f52\u6a21\u578b\u5728\u5b9a\u5236\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u6f5c\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u56e0\u5168\u5fae\u8c03\u6216\u9002\u914d\u5668\u5bfc\u81f4\u7684\u6210\u672c\u9ad8\u3001\u8fc7\u62df\u5408\u6216\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "method": "\u91c7\u7528Layerwise Multimodal Context Learning\u7b56\u7565\uff0c\u4ec5\u7528\u6781\u5c11\u91cf\u53c2\u6570\u5b66\u4e60\u6709\u6548\u7684\u4e3b\u9898\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u6b63\u5219\u5316\u9632\u6b62\u8fc7\u62df\u5408\u548c\u8bed\u8a00\u6f02\u79fb\u3002", "result": "CoAR\u5728\u4e3b\u9898\u9a71\u52a8\u548c\u98ce\u683c\u4e2a\u6027\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u548c\u5185\u5b58\u6548\u7387\uff0c\u53c2\u6570\u8c03\u6574\u91cf\u5c11\u4e8e0.05%\u3002", "conclusion": "CoAR\u4e3a\u5b9a\u5236\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u7f3a\u9677\u3002"}}
{"id": "2508.07346", "pdf": "https://arxiv.org/pdf/2508.07346", "abs": "https://arxiv.org/abs/2508.07346", "authors": ["Tingyu Yang", "Jue Gong", "Jinpei Guo", "Wenbo Li", "Yong Guo", "Yulun Zhang"], "title": "SODiff: Semantic-Oriented Diffusion Model for JPEG Compression Artifacts Removal", "categories": ["cs.CV"], "comment": "7 pages, 5 figures. The code will be available at   \\url{https://github.com/frakenation/SODiff}", "summary": "JPEG, as a widely used image compression standard, often introduces severe visual artifacts when achieving high compression ratios. Although existing deep learning-based restoration methods have made considerable progress, they often struggle to recover complex texture details, resulting in over-smoothed outputs. To overcome these limitations, we propose SODiff, a novel and efficient semantic-oriented one-step diffusion model for JPEG artifacts removal. Our core idea is that effective restoration hinges on providing semantic-oriented guidance to the pre-trained diffusion model, thereby fully leveraging its powerful generative prior. To this end, SODiff incorporates a semantic-aligned image prompt extractor (SAIPE). SAIPE extracts rich features from low-quality (LQ) images and projects them into an embedding space semantically aligned with that of the text encoder. Simultaneously, it preserves crucial information for faithful reconstruction. Furthermore, we propose a quality factor-aware time predictor that implicitly learns the compression quality factor (QF) of the LQ image and adaptively selects the optimal denoising start timestep for the diffusion process. Extensive experimental results show that our SODiff outperforms recent leading methods in both visual quality and quantitative metrics. Code is available at: https://github.com/frakenation/SODiff", "AI": {"tldr": "SODiff\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8bed\u4e49\u5bfc\u5411\u4e00\u6b65\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8eJPEG\u4f2a\u5f71\u53bb\u9664\uff0c\u901a\u8fc7\u8bed\u4e49\u5bf9\u9f50\u7684\u56fe\u50cf\u63d0\u793a\u63d0\u53d6\u5668\u548c\u8d28\u91cf\u56e0\u5b50\u611f\u77e5\u65f6\u95f4\u9884\u6d4b\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6062\u590d\u6548\u679c\u3002", "motivation": "JPEG\u9ad8\u538b\u7f29\u6bd4\u5e38\u5bfc\u81f4\u4e25\u91cd\u89c6\u89c9\u4f2a\u5f71\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u6062\u590d\u590d\u6742\u7eb9\u7406\u7ec6\u8282\uff0c\u5bfc\u81f4\u8f93\u51fa\u8fc7\u5ea6\u5e73\u6ed1\u3002", "method": "\u63d0\u51faSODiff\u6a21\u578b\uff0c\u7ed3\u5408\u8bed\u4e49\u5bf9\u9f50\u56fe\u50cf\u63d0\u793a\u63d0\u53d6\u5668\uff08SAIPE\uff09\u548c\u8d28\u91cf\u56e0\u5b50\u611f\u77e5\u65f6\u95f4\u9884\u6d4b\u5668\uff0c\u4f18\u5316\u6269\u6563\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSODiff\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u5b9a\u91cf\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u9886\u5148\u65b9\u6cd5\u3002", "conclusion": "SODiff\u901a\u8fc7\u8bed\u4e49\u5bfc\u5411\u548c\u81ea\u9002\u5e94\u65f6\u95f4\u9884\u6d4b\uff0c\u6709\u6548\u63d0\u5347\u4e86JPEG\u4f2a\u5f71\u53bb\u9664\u7684\u6027\u80fd\u3002"}}
{"id": "2508.07355", "pdf": "https://arxiv.org/pdf/2508.07355", "abs": "https://arxiv.org/abs/2508.07355", "authors": ["Qilin Zhang", "Olaf Wysocki", "Boris Jutzi"], "title": "GS4Buildings: Prior-Guided Gaussian Splatting for 3D Building Reconstruction", "categories": ["cs.CV"], "comment": "Accepted for presentation at ISPRS 3D GeoInfo & Smart Data, Smart   Cities 2025, Kashiwa, Japan. To appear in the ISPRS Annals of the   Photogrammetry, Remote Sensing and Spatial Information Sciences", "summary": "Recent advances in Gaussian Splatting (GS) have demonstrated its effectiveness in photo-realistic rendering and 3D reconstruction. Among these, 2D Gaussian Splatting (2DGS) is particularly suitable for surface reconstruction due to its flattened Gaussian representation and integrated normal regularization. However, its performance often degrades in large-scale and complex urban scenes with frequent occlusions, leading to incomplete building reconstructions. We propose GS4Buildings, a novel prior-guided Gaussian Splatting method leveraging the ubiquity of semantic 3D building models for robust and scalable building surface reconstruction. Instead of relying on traditional Structure-from-Motion (SfM) pipelines, GS4Buildings initializes Gaussians directly from low-level Level of Detail (LoD)2 semantic 3D building models. Moreover, we generate prior depth and normal maps from the planar building geometry and incorporate them into the optimization process, providing strong geometric guidance for surface consistency and structural accuracy. We also introduce an optional building-focused mode that limits reconstruction to building regions, achieving a 71.8% reduction in Gaussian primitives and enabling a more efficient and compact representation. Experiments on urban datasets demonstrate that GS4Buildings improves reconstruction completeness by 20.5% and geometric accuracy by 32.8%. These results highlight the potential of semantic building model integration to advance GS-based reconstruction toward real-world urban applications such as smart cities and digital twins. Our project is available: https://github.com/zqlin0521/GS4Buildings.", "AI": {"tldr": "GS4Buildings\u5229\u7528\u8bed\u4e493D\u5efa\u7b51\u6a21\u578b\u6539\u8fdb2D\u9ad8\u65af\u6e85\u5c04\uff0c\u63d0\u5347\u5927\u89c4\u6a21\u57ce\u5e02\u5efa\u7b51\u91cd\u5efa\u7684\u5b8c\u6574\u6027\u548c\u51e0\u4f55\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b32D\u9ad8\u65af\u6e85\u5c04\u5728\u590d\u6742\u57ce\u5e02\u573a\u666f\u4e2d\u56e0\u906e\u6321\u5bfc\u81f4\u91cd\u5efa\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\u3002", "method": "\u76f4\u63a5\u4eceLoD2\u8bed\u4e493D\u5efa\u7b51\u6a21\u578b\u521d\u59cb\u5316\u9ad8\u65af\uff0c\u7ed3\u5408\u5148\u9a8c\u6df1\u5ea6\u548c\u6cd5\u7ebf\u56fe\u4f18\u5316\u91cd\u5efa\u8fc7\u7a0b\u3002", "result": "\u91cd\u5efa\u5b8c\u6574\u6027\u63d0\u534720.5%\uff0c\u51e0\u4f55\u7cbe\u5ea6\u63d0\u534732.8%\uff0c\u9ad8\u65af\u57fa\u5143\u51cf\u5c1171.8%\u3002", "conclusion": "\u8bed\u4e49\u5efa\u7b51\u6a21\u578b\u96c6\u6210\u53ef\u63a8\u52a8\u9ad8\u65af\u6e85\u5c04\u6280\u672f\u5728\u57ce\u5e02\u5e94\u7528\u4e2d\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.07372", "pdf": "https://arxiv.org/pdf/2508.07372", "abs": "https://arxiv.org/abs/2508.07372", "authors": ["Rajaei Khatib", "Raja Giryes"], "title": "DIP-GS: Deep Image Prior For Gaussian Splatting Sparse View Recovery", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) is a leading 3D scene reconstruction method, obtaining high-quality reconstruction with real-time rendering runtime performance. The main idea behind 3DGS is to represent the scene as a collection of 3D gaussians, while learning their parameters to fit the given views of the scene. While achieving superior performance in the presence of many views, 3DGS struggles with sparse view reconstruction, where the input views are sparse and do not fully cover the scene and have low overlaps. In this paper, we propose DIP-GS, a Deep Image Prior (DIP) 3DGS representation. By using the DIP prior, which utilizes internal structure and patterns, with coarse-to-fine manner, DIP-based 3DGS can operate in scenarios where vanilla 3DGS fails, such as sparse view recovery. Note that our approach does not use any pre-trained models such as generative models and depth estimation, but rather relies only on the input frames. Among such methods, DIP-GS obtains state-of-the-art (SOTA) competitive results on various sparse-view reconstruction tasks, demonstrating its capabilities.", "AI": {"tldr": "DIP-GS\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u56fe\u50cf\u5148\u9a8c\uff08DIP\uff09\u76843D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u6539\u8fdb\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e863DGS\u5728\u7a00\u758f\u89c6\u56fe\u91cd\u5efa\u4e2d\u7684\u4e0d\u8db3\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\u6a21\u578b\u5373\u53ef\u5b9e\u73b0\u7ade\u4e89\u6027\u7ed3\u679c\u3002", "motivation": "3DGS\u5728\u591a\u89c6\u56fe\u91cd\u5efa\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u7a00\u758f\u89c6\u56fe\u573a\u666f\u4e0b\u6548\u679c\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7DIP\u5148\u9a8c\u63d0\u53473DGS\u5728\u7a00\u758f\u89c6\u56fe\u91cd\u5efa\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faDIP-GS\uff0c\u5229\u7528DIP\u5148\u9a8c\u7684\u5185\u90e8\u7ed3\u6784\u548c\u6a21\u5f0f\uff0c\u4ee5\u7c97\u5230\u7ec6\u7684\u65b9\u5f0f\u4f18\u53163DGS\u53c2\u6570\uff0c\u4ec5\u4f9d\u8d56\u8f93\u5165\u5e27\u800c\u4e0d\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "result": "DIP-GS\u5728\u7a00\u758f\u89c6\u56fe\u91cd\u5efa\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u7ade\u4e89\u6027\u7684SOTA\u7ed3\u679c\u3002", "conclusion": "DIP-GS\u901a\u8fc7DIP\u5148\u9a8c\u6709\u6548\u89e3\u51b3\u4e863DGS\u5728\u7a00\u758f\u89c6\u56fe\u91cd\u5efa\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5c55\u793a\u4e86\u5176\u6f5c\u529b\u3002"}}
{"id": "2508.07409", "pdf": "https://arxiv.org/pdf/2508.07409", "abs": "https://arxiv.org/abs/2508.07409", "authors": ["Junyao Gao", "Jiaxing Li", "Wenran Liu", "Yanhong Zeng", "Fei Shen", "Kai Chen", "Yanan Sun", "Cairong Zhao"], "title": "CharacterShot: Controllable and Consistent 4D Character Animation", "categories": ["cs.CV"], "comment": "13 pages, 10 figures. Code at https://github.com/Jeoyal/CharacterShot", "summary": "In this paper, we propose \\textbf{CharacterShot}, a controllable and consistent 4D character animation framework that enables any individual designer to create dynamic 3D characters (i.e., 4D character animation) from a single reference character image and a 2D pose sequence. We begin by pretraining a powerful 2D character animation model based on a cutting-edge DiT-based image-to-video model, which allows for any 2D pose sequnce as controllable signal. We then lift the animation model from 2D to 3D through introducing dual-attention module together with camera prior to generate multi-view videos with spatial-temporal and spatial-view consistency. Finally, we employ a novel neighbor-constrained 4D gaussian splatting optimization on these multi-view videos, resulting in continuous and stable 4D character representations. Moreover, to improve character-centric performance, we construct a large-scale dataset Character4D, containing 13,115 unique characters with diverse appearances and motions, rendered from multiple viewpoints. Extensive experiments on our newly constructed benchmark, CharacterBench, demonstrate that our approach outperforms current state-of-the-art methods. Code, models, and datasets will be publicly available at https://github.com/Jeoyal/CharacterShot.", "AI": {"tldr": "CharacterShot\u662f\u4e00\u4e2a\u53ef\u63a7\u4e14\u4e00\u81f4\u76844D\u89d2\u8272\u52a8\u753b\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u5f20\u53c2\u8003\u56fe\u50cf\u548c2D\u59ff\u52bf\u5e8f\u5217\u751f\u6210\u52a8\u60013D\u89d2\u8272\u52a8\u753b\u3002", "motivation": "\u4e3a\u8bbe\u8ba1\u5e08\u63d0\u4f9b\u4e00\u79cd\u7b80\u5355\u7684\u65b9\u6cd5\uff0c\u4ece\u5355\u5f20\u56fe\u50cf\u548c2D\u59ff\u52bf\u5e8f\u5217\u751f\u6210\u9ad8\u8d28\u91cf\u76844D\u89d2\u8272\u52a8\u753b\u3002", "method": "1. \u9884\u8bad\u7ec3\u57fa\u4e8eDiT\u76842D\u89d2\u8272\u52a8\u753b\u6a21\u578b\uff1b2. \u901a\u8fc7\u53cc\u6ce8\u610f\u529b\u6a21\u5757\u548c\u76f8\u673a\u5148\u9a8c\u5c06\u52a8\u753b\u4ece2D\u63d0\u5347\u52303D\uff1b3. \u4f7f\u75284D\u9ad8\u65af\u6e85\u5c04\u4f18\u5316\u751f\u6210\u8fde\u7eed\u7a33\u5b9a\u76844D\u89d2\u8272\u8868\u793a\u3002", "result": "\u5728\u65b0\u5efa\u7684CharacterBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CharacterShot\u6846\u67b6\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u3002"}}
{"id": "2508.07413", "pdf": "https://arxiv.org/pdf/2508.07413", "abs": "https://arxiv.org/abs/2508.07413", "authors": ["Youqi Wang", "Shunquan Tan", "Rongxuan Peng", "Bin Li", "Jiwu Huang"], "title": "CLUE: Leveraging Low-Rank Adaptation to Capture Latent Uncovered Evidence for Image Forgery Localization", "categories": ["cs.CV"], "comment": null, "summary": "The increasing accessibility of image editing tools and generative AI has led to a proliferation of visually convincing forgeries, compromising the authenticity of digital media. In this paper, in addition to leveraging distortions from conventional forgeries, we repurpose the mechanism of a state-of-the-art (SOTA) text-to-image synthesis model by exploiting its internal generative process, turning it into a high-fidelity forgery localization tool. To this end, we propose CLUE (Capture Latent Uncovered Evidence), a framework that employs Low- Rank Adaptation (LoRA) to parameter-efficiently reconfigure Stable Diffusion 3 (SD3) as a forensic feature extractor. Our approach begins with the strategic use of SD3's Rectified Flow (RF) mechanism to inject noise at varying intensities into the latent representation, thereby steering the LoRAtuned denoising process to amplify subtle statistical inconsistencies indicative of a forgery. To complement the latent analysis with high-level semantic context and precise spatial details, our method incorporates contextual features from the image encoder of the Segment Anything Model (SAM), which is parameter-efficiently adapted to better trace the boundaries of forged regions. Extensive evaluations demonstrate CLUE's SOTA generalization performance, significantly outperforming prior methods. Furthermore, CLUE shows superior robustness against common post-processing attacks and Online Social Networks (OSNs). Code is publicly available at https://github.com/SZAISEC/CLUE.", "AI": {"tldr": "CLUE\u5229\u7528Stable Diffusion 3\u548cSegment Anything Model\uff0c\u901a\u8fc7\u566a\u58f0\u6ce8\u5165\u548c\u53c2\u6570\u9ad8\u6548\u8c03\u6574\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u4f2a\u9020\u5b9a\u4f4d\u3002", "motivation": "\u56fe\u50cf\u7f16\u8f91\u5de5\u5177\u548c\u751f\u6210AI\u7684\u666e\u53ca\u5bfc\u81f4\u4f2a\u9020\u56fe\u50cf\u6cdb\u6ee5\uff0c\u5a01\u80c1\u6570\u5b57\u5a92\u4f53\u7684\u771f\u5b9e\u6027\u3002", "method": "\u7ed3\u5408SD3\u7684Rectified Flow\u673a\u5236\u548cLoRA\u8c03\u6574\uff0c\u4ee5\u53caSAM\u7684\u56fe\u50cf\u7f16\u7801\u5668\uff0c\u63d0\u53d6\u4f2a\u9020\u7279\u5f81\u3002", "result": "CLUE\u5728\u6cdb\u5316\u6027\u80fd\u548c\u9c81\u68d2\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CLUE\u4e3a\u4f2a\u9020\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07483", "pdf": "https://arxiv.org/pdf/2508.07483", "abs": "https://arxiv.org/abs/2508.07483", "authors": ["Pranav Chougule"], "title": "Novel View Synthesis with Gaussian Splatting: Impact on Photogrammetry Model Accuracy and Resolution", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "In this paper, I present a comprehensive study comparing Photogrammetry and Gaussian Splatting techniques for 3D model reconstruction and view synthesis. I created a dataset of images from a real-world scene and constructed 3D models using both methods. To evaluate the performance, I compared the models using structural similarity index (SSIM), peak signal-to-noise ratio (PSNR), learned perceptual image patch similarity (LPIPS), and lp/mm resolution based on the USAF resolution chart. A significant contribution of this work is the development of a modified Gaussian Splatting repository, which I forked and enhanced to enable rendering images from novel camera poses generated in the Blender environment. This innovation allows for the synthesis of high-quality novel views, showcasing the flexibility and potential of Gaussian Splatting. My investigation extends to an augmented dataset that includes both original ground images and novel views synthesized via Gaussian Splatting. This augmented dataset was employed to generate a new photogrammetry model, which was then compared against the original photogrammetry model created using only the original images. The results demonstrate the efficacy of using Gaussian Splatting to generate novel high-quality views and its potential to improve photogrammetry-based 3D reconstructions. The comparative analysis highlights the strengths and limitations of both approaches, providing valuable information for applications in extended reality (XR), photogrammetry, and autonomous vehicle simulations. Code is available at https://github.com/pranavc2255/gaussian-splatting-novel-view-render.git.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u6444\u5f71\u6d4b\u91cf\u4e0e\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u57283D\u6a21\u578b\u91cd\u5efa\u548c\u89c6\u56fe\u5408\u6210\u4e2d\u7684\u8868\u73b0\uff0c\u5f00\u53d1\u4e86\u6539\u8fdb\u7684\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u63d0\u5347\u6444\u5f71\u6d4b\u91cf\u91cd\u5efa\u8d28\u91cf\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u7814\u7a76\u6444\u5f71\u6d4b\u91cf\u4e0e\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u57283D\u91cd\u5efa\u548c\u89c6\u56fe\u5408\u6210\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u63a2\u7d22\u9ad8\u65af\u6cfc\u6e85\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u65b0\u89c6\u56fe\u548c\u6539\u8fdb\u6444\u5f71\u6d4b\u91cf\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u521b\u5efa\u771f\u5b9e\u573a\u666f\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5206\u522b\u7528\u6444\u5f71\u6d4b\u91cf\u548c\u9ad8\u65af\u6cfc\u6e85\u6784\u5efa3D\u6a21\u578b\uff0c\u901a\u8fc7SSIM\u3001PSNR\u3001LPIPS\u548c\u5206\u8fa8\u7387\u6307\u6807\u8bc4\u4f30\u6027\u80fd\uff0c\u6539\u8fdb\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u4ee5\u652f\u6301Blender\u73af\u5883\u4e2d\u7684\u65b0\u89c6\u56fe\u6e32\u67d3\u3002", "result": "\u9ad8\u65af\u6cfc\u6e85\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u65b0\u89c6\u56fe\uff0c\u5e76\u63d0\u5347\u6444\u5f71\u6d4b\u91cf\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e24\u79cd\u65b9\u6cd5\u5404\u6709\u4f18\u52a3\u3002", "conclusion": "\u9ad8\u65af\u6cfc\u6e85\u57283D\u91cd\u5efa\u548c\u89c6\u56fe\u5408\u6210\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u6269\u5c55\u73b0\u5b9e\u3001\u6444\u5f71\u6d4b\u91cf\u548c\u81ea\u52a8\u9a7e\u9a76\u6a21\u62df\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u4fe1\u606f\u3002"}}
{"id": "2508.07519", "pdf": "https://arxiv.org/pdf/2508.07519", "abs": "https://arxiv.org/abs/2508.07519", "authors": ["Joonghyuk Shin", "Alchan Hwang", "Yujin Kim", "Daneul Kim", "Jaesik Park"], "title": "Exploring Multimodal Diffusion Transformers for Enhanced Prompt-based Image Editing", "categories": ["cs.CV"], "comment": "ICCV 2025. Project webpage:   https://joonghyuk.com/exploring-mmdit-web/", "summary": "Transformer-based diffusion models have recently superseded traditional U-Net architectures, with multimodal diffusion transformers (MM-DiT) emerging as the dominant approach in state-of-the-art models like Stable Diffusion 3 and Flux.1. Previous approaches have relied on unidirectional cross-attention mechanisms, with information flowing from text embeddings to image latents. In contrast, MMDiT introduces a unified attention mechanism that concatenates input projections from both modalities and performs a single full attention operation, allowing bidirectional information flow between text and image branches. This architectural shift presents significant challenges for existing editing techniques. In this paper, we systematically analyze MM-DiT's attention mechanism by decomposing attention matrices into four distinct blocks, revealing their inherent characteristics. Through these analyses, we propose a robust, prompt-based image editing method for MM-DiT that supports global to local edits across various MM-DiT variants, including few-step models. We believe our findings bridge the gap between existing U-Net-based methods and emerging architectures, offering deeper insights into MMDiT's behavioral patterns.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\uff08MM-DiT\uff09\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4e0d\u540cMM-DiT\u53d8\u4f53\u3002", "motivation": "\u4f20\u7edfU-Net\u67b6\u6784\u5df2\u88ab\u57fa\u4e8eTransformer\u7684\u6269\u6563\u6a21\u578b\u53d6\u4ee3\uff0c\u4f46MM-DiT\u7684\u53cc\u5411\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u73b0\u6709\u7f16\u8f91\u6280\u672f\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5c06\u6ce8\u610f\u529b\u77e9\u9635\u5206\u89e3\u4e3a\u56db\u4e2a\u5757\uff0c\u5206\u6790\u5176\u7279\u6027\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u7684\u7f16\u8f91\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u652f\u6301\u4ece\u5168\u5c40\u5230\u5c40\u90e8\u7f16\u8f91\u7684\u9c81\u68d2\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5305\u62ec\u5c11\u6b65\u6a21\u578b\u5728\u5185\u7684\u591a\u79cdMM-DiT\u53d8\u4f53\u3002", "conclusion": "\u7814\u7a76\u586b\u8865\u4e86U-Net\u65b9\u6cd5\u4e0e\u65b0\u5174\u67b6\u6784\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u5bf9MM-DiT\u884c\u4e3a\u6a21\u5f0f\u7684\u6df1\u5165\u7406\u89e3\u3002"}}
{"id": "2508.07537", "pdf": "https://arxiv.org/pdf/2508.07537", "abs": "https://arxiv.org/abs/2508.07537", "authors": ["Xiaoming Li", "Wangmeng Zuo", "Chen Change Loy"], "title": "Enhanced Generative Structure Prior for Chinese Text Image Super-resolution", "categories": ["cs.CV"], "comment": "TPAMI", "summary": "Faithful text image super-resolution (SR) is challenging because each character has a unique structure and usually exhibits diverse font styles and layouts. While existing methods primarily focus on English text, less attention has been paid to more complex scripts like Chinese. In this paper, we introduce a high-quality text image SR framework designed to restore the precise strokes of low-resolution (LR) Chinese characters. Unlike methods that rely on character recognition priors to regularize the SR task, we propose a novel structure prior that offers structure-level guidance to enhance visual quality. Our framework incorporates this structure prior within a StyleGAN model, leveraging its generative capabilities for restoration. To maintain the integrity of character structures while accommodating various font styles and layouts, we implement a codebook-based mechanism that restricts the generative space of StyleGAN. Each code in the codebook represents the structure of a specific character, while the vector $w$ in StyleGAN controls the character's style, including typeface, orientation, and location. Through the collaborative interaction between the codebook and style, we generate a high-resolution structure prior that aligns with LR characters both spatially and structurally. Experiments demonstrate that this structure prior provides robust, character-specific guidance, enabling the accurate restoration of clear strokes in degraded characters, even for real-world LR Chinese text with irregular layouts. Our code and pre-trained models will be available at https://github.com/csxmli2016/MARCONetPlusPlus", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u4e2d\u6587\u6587\u672c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5148\u9a8c\u548cStyleGAN\u7ed3\u5408\uff0c\u6062\u590d\u4f4e\u5206\u8fa8\u7387\u5b57\u7b26\u7684\u7cbe\u786e\u7b14\u753b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u82f1\u6587\u6587\u672c\uff0c\u5bf9\u590d\u6742\u811a\u672c\uff08\u5982\u4e2d\u6587\uff09\u7684\u8d85\u5206\u8fa8\u7387\u7814\u7a76\u8f83\u5c11\uff0c\u4e14\u7f3a\u4e4f\u7ed3\u6784\u7ea7\u6307\u5bfc\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u5148\u9a8c\uff0c\u7ed3\u5408StyleGAN\u751f\u6210\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u4ee3\u7801\u672c\u673a\u5236\u9650\u5236\u751f\u6210\u7a7a\u95f4\uff0c\u786e\u4fdd\u5b57\u7b26\u7ed3\u6784\u7684\u5b8c\u6574\u6027\u548c\u591a\u6837\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u51c6\u786e\u6062\u590d\u4f4e\u5206\u8fa8\u7387\u4e2d\u6587\u5b57\u7b26\u7684\u6e05\u6670\u7b14\u753b\uff0c\u9002\u7528\u4e8e\u4e0d\u89c4\u5219\u5e03\u5c40\u7684\u771f\u5b9e\u573a\u666f\u3002", "conclusion": "\u7ed3\u6784\u5148\u9a8c\u4e0eStyleGAN\u7684\u534f\u540c\u4f5c\u7528\u4e3a\u4e2d\u6587\u6587\u672c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07557", "pdf": "https://arxiv.org/pdf/2508.07557", "abs": "https://arxiv.org/abs/2508.07557", "authors": ["Minghao Yin", "Yukang Cao", "Songyou Peng", "Kai Han"], "title": "Splat4D: Diffusion-Enhanced 4D Gaussian Splatting for Temporally and Spatially Consistent Content Creation", "categories": ["cs.CV"], "comment": null, "summary": "Generating high-quality 4D content from monocular videos for applications such as digital humans and AR/VR poses challenges in ensuring temporal and spatial consistency, preserving intricate details, and incorporating user guidance effectively. To overcome these challenges, we introduce Splat4D, a novel framework enabling high-fidelity 4D content generation from a monocular video. Splat4D achieves superior performance while maintaining faithful spatial-temporal coherence by leveraging multi-view rendering, inconsistency identification, a video diffusion model, and an asymmetric U-Net for refinement. Through extensive evaluations on public benchmarks, Splat4D consistently demonstrates state-of-the-art performance across various metrics, underscoring the efficacy of our approach. Additionally, the versatility of Splat4D is validated in various applications such as text/image conditioned 4D generation, 4D human generation, and text-guided content editing, producing coherent outcomes following user instructions.", "AI": {"tldr": "Splat4D\u662f\u4e00\u4e2a\u4ece\u5355\u76ee\u89c6\u9891\u751f\u6210\u9ad8\u8d28\u91cf4D\u5185\u5bb9\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u6e32\u67d3\u3001\u4e0d\u4e00\u81f4\u6027\u8bc6\u522b\u3001\u89c6\u9891\u6269\u6563\u6a21\u578b\u548c\u975e\u5bf9\u79f0U-Net\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u65f6\u7a7a\u4e00\u81f4\u6027\u548c\u9ad8\u4fdd\u771f\u5ea6\u3002", "motivation": "\u89e3\u51b3\u4ece\u5355\u76ee\u89c6\u9891\u751f\u62104D\u5185\u5bb9\u65f6\u9762\u4e34\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\u3001\u7ec6\u8282\u4fdd\u7559\u548c\u7528\u6237\u6307\u5bfc\u95ee\u9898\u3002", "method": "\u5229\u7528\u591a\u89c6\u89d2\u6e32\u67d3\u3001\u4e0d\u4e00\u81f4\u6027\u8bc6\u522b\u3001\u89c6\u9891\u6269\u6563\u6a21\u578b\u548c\u975e\u5bf9\u79f0U-Net\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u591a\u79cd\u5e94\u7528\u5982\u6587\u672c/\u56fe\u50cf\u6761\u4ef64D\u751f\u6210\u548c\u5185\u5bb9\u7f16\u8f91\u3002", "conclusion": "Splat4D\u57284D\u5185\u5bb9\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.07597", "pdf": "https://arxiv.org/pdf/2508.07597", "abs": "https://arxiv.org/abs/2508.07597", "authors": ["Yuang Zhang", "Junqi Cheng", "Haoyu Zhao", "Jiaxi Gu", "Fangyuan Zou", "Zenghui Lu", "Peng Shu"], "title": "ShoulderShot: Generating Over-the-Shoulder Dialogue Videos", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Over-the-shoulder dialogue videos are essential in films, short dramas, and advertisements, providing visual variety and enhancing viewers' emotional connection. Despite their importance, such dialogue scenes remain largely underexplored in video generation research. The main challenges include maintaining character consistency across different shots, creating a sense of spatial continuity, and generating long, multi-turn dialogues within limited computational budgets. Here, we present ShoulderShot, a framework that combines dual-shot generation with looping video, enabling extended dialogues while preserving character consistency. Our results demonstrate capabilities that surpass existing methods in terms of shot-reverse-shot layout, spatial continuity, and flexibility in dialogue length, thereby opening up new possibilities for practical dialogue video generation. Videos and comparisons are available at https://shouldershot.github.io.", "AI": {"tldr": "ShoulderShot\u6846\u67b6\u901a\u8fc7\u53cc\u955c\u5934\u751f\u6210\u548c\u5faa\u73af\u89c6\u9891\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u5bf9\u8bdd\u89c6\u9891\u751f\u6210\u4e2d\u7684\u89d2\u8272\u4e00\u81f4\u6027\u548c\u7a7a\u95f4\u8fde\u7eed\u6027\u95ee\u9898\uff0c\u5e76\u5728\u591a\u8f6e\u5bf9\u8bdd\u957f\u5ea6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u8fc7\u80a9\u5bf9\u8bdd\u89c6\u9891\u5728\u5f71\u89c6\u4f5c\u54c1\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5bf9\u5176\u751f\u6210\u6280\u672f\u63a2\u7d22\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u89d2\u8272\u4e00\u81f4\u6027\u3001\u7a7a\u95f4\u8fde\u7eed\u6027\u548c\u957f\u5bf9\u8bdd\u751f\u6210\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51faShoulderShot\u6846\u67b6\uff0c\u7ed3\u5408\u53cc\u955c\u5934\u751f\u6210\u548c\u5faa\u73af\u89c6\u9891\u6280\u672f\uff0c\u4ee5\u4fdd\u6301\u89d2\u8272\u4e00\u81f4\u6027\u5e76\u652f\u6301\u957f\u5bf9\u8bdd\u751f\u6210\u3002", "result": "ShoulderShot\u5728\u955c\u5934\u5207\u6362\u5e03\u5c40\u3001\u7a7a\u95f4\u8fde\u7eed\u6027\u548c\u5bf9\u8bdd\u957f\u5ea6\u7075\u6d3b\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5b9e\u9645\u5bf9\u8bdd\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.07607", "pdf": "https://arxiv.org/pdf/2508.07607", "abs": "https://arxiv.org/abs/2508.07607", "authors": ["Jian Ma", "Xujie Zhu", "Zihao Pan", "Qirong Peng", "Xu Guo", "Chen Chen", "Haonan Lu"], "title": "X2Edit: Revisiting Arbitrary-Instruction Image Editing through Self-Constructed Data and Task-Aware Representation Learning", "categories": ["cs.CV"], "comment": "https://github.com/OPPO-Mente-Lab/X2Edit", "summary": "Existing open-source datasets for arbitrary-instruction image editing remain suboptimal, while a plug-and-play editing module compatible with community-prevalent generative models is notably absent. In this paper, we first introduce the X2Edit Dataset, a comprehensive dataset covering 14 diverse editing tasks, including subject-driven generation. We utilize the industry-leading unified image generation models and expert models to construct the data. Meanwhile, we design reasonable editing instructions with the VLM and implement various scoring mechanisms to filter the data. As a result, we construct 3.7 million high-quality data with balanced categories. Second, to better integrate seamlessly with community image generation models, we design task-aware MoE-LoRA training based on FLUX.1, with only 8\\% of the parameters of the full model. To further improve the final performance, we utilize the internal representations of the diffusion model and define positive/negative samples based on image editing types to introduce contrastive learning. Extensive experiments demonstrate that the model's editing performance is competitive among many excellent models. Additionally, the constructed dataset exhibits substantial advantages over existing open-source datasets. The open-source code, checkpoints, and datasets for X2Edit can be found at the following link: https://github.com/OPPO-Mente-Lab/X2Edit.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86X2Edit\u6570\u636e\u96c6\u548c\u4efb\u52a1\u611f\u77e5\u7684MoE-LoRA\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u6570\u636e\u96c6\u548c\u7f16\u8f91\u6a21\u5757\u5728\u591a\u6837\u6027\u548c\u517c\u5bb9\u6027\u4e0a\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u6784\u5efaX2Edit\u6570\u636e\u96c6\uff0814\u7c7b\u4efb\u52a1\uff0c370\u4e07\u9ad8\u8d28\u91cf\u6570\u636e\uff09\uff0c\u8bbe\u8ba1MoE-LoRA\u8bad\u7ec3\u65b9\u6cd5\uff08\u53c2\u6570\u4ec58%\uff09\uff0c\u5e76\u5f15\u5165\u5bf9\u6bd4\u5b66\u4e60\u3002", "result": "\u6a21\u578b\u6027\u80fd\u4f18\u79c0\uff0c\u6570\u636e\u96c6\u4f18\u52bf\u663e\u8457\u3002", "conclusion": "X2Edit\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u65b9\u6cd5\u4e3a\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07647", "pdf": "https://arxiv.org/pdf/2508.07647", "abs": "https://arxiv.org/abs/2508.07647", "authors": ["Xiaohang Zhan", "Dingming Liu"], "title": "LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025 (oral). Project page:   https://xiaohangzhan.github.io/projects/larender/", "summary": "We propose a novel training-free image generation algorithm that precisely controls the occlusion relationships between objects in an image. Existing image generation methods typically rely on prompts to influence occlusion, which often lack precision. While layout-to-image methods provide control over object locations, they fail to address occlusion relationships explicitly. Given a pre-trained image diffusion model, our method leverages volume rendering principles to \"render\" the scene in latent space, guided by occlusion relationships and the estimated transmittance of objects. This approach does not require retraining or fine-tuning the image diffusion model, yet it enables accurate occlusion control due to its physics-grounded foundation. In extensive experiments, our method significantly outperforms existing approaches in terms of occlusion accuracy. Furthermore, we demonstrate that by adjusting the opacities of objects or concepts during rendering, our method can achieve a variety of effects, such as altering the transparency of objects, the density of mass (e.g., forests), the concentration of particles (e.g., rain, fog), the intensity of light, and the strength of lens effects, etc.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u56fe\u50cf\u751f\u6210\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f53\u79ef\u6e32\u67d3\u539f\u7406\u7cbe\u786e\u63a7\u5236\u56fe\u50cf\u4e2d\u7269\u4f53\u7684\u906e\u6321\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u63d0\u793a\u6216\u5e03\u5c40\u63a7\u5236\u906e\u6321\uff0c\u4f46\u7f3a\u4e4f\u7cbe\u786e\u6027\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u65b9\u6cd5\u5b9e\u73b0\u7cbe\u51c6\u906e\u6321\u63a7\u5236\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u4f53\u79ef\u6e32\u67d3\u539f\u7406\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u201c\u6e32\u67d3\u201d\u573a\u666f\uff0c\u901a\u8fc7\u906e\u6321\u5173\u7cfb\u548c\u7269\u4f53\u900f\u5c04\u7387\u6307\u5bfc\u751f\u6210\u3002", "result": "\u5728\u906e\u6321\u51c6\u786e\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u901a\u8fc7\u8c03\u6574\u7269\u4f53\u900f\u660e\u5ea6\u7b49\u53c2\u6570\u5b9e\u73b0\u591a\u79cd\u89c6\u89c9\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u57fa\u4e8e\u7269\u7406\u539f\u7406\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u906e\u6321\u63a7\u5236\uff0c\u6269\u5c55\u4e86\u56fe\u50cf\u751f\u6210\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2508.07682", "pdf": "https://arxiv.org/pdf/2508.07682", "abs": "https://arxiv.org/abs/2508.07682", "authors": ["Wenzhuo Ma", "Zhenzhong Chen"], "title": "DiffVC-OSD: One-Step Diffusion-based Perceptual Neural Video Compression Framework", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "In this work, we first propose DiffVC-OSD, a One-Step Diffusion-based Perceptual Neural Video Compression framework. Unlike conventional multi-step diffusion-based methods, DiffVC-OSD feeds the reconstructed latent representation directly into a One-Step Diffusion Model, enhancing perceptual quality through a single diffusion step guided by both temporal context and the latent itself. To better leverage temporal dependencies, we design a Temporal Context Adapter that encodes conditional inputs into multi-level features, offering more fine-grained guidance for the Denoising Unet. Additionally, we employ an End-to-End Finetuning strategy to improve overall compression performance. Extensive experiments demonstrate that DiffVC-OSD achieves state-of-the-art perceptual compression performance, offers about 20$\\times$ faster decoding and a 86.92\\% bitrate reduction compared to the corresponding multi-step diffusion-based variant.", "AI": {"tldr": "DiffVC-OSD\u662f\u4e00\u79cd\u57fa\u4e8e\u5355\u6b65\u6269\u6563\u7684\u611f\u77e5\u795e\u7ecf\u89c6\u9891\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u8f93\u5165\u91cd\u5efa\u7684\u6f5c\u5728\u8868\u793a\u5230\u5355\u6b65\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u611f\u77e5\u8d28\u91cf\u548c\u89e3\u7801\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u591a\u6b65\u6269\u6563\u65b9\u6cd5\u6548\u7387\u8f83\u4f4e\uff0cDiffVC-OSD\u65e8\u5728\u901a\u8fc7\u5355\u6b65\u6269\u6563\u6a21\u578b\u63d0\u5347\u89c6\u9891\u538b\u7f29\u7684\u611f\u77e5\u8d28\u91cf\u548c\u6548\u7387\u3002", "method": "\u8bbe\u8ba1\u4e86Temporal Context Adapter\u4ee5\u5229\u7528\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u5e76\u901a\u8fc7End-to-End Finetuning\u7b56\u7565\u4f18\u5316\u538b\u7f29\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDiffVC-OSD\u5728\u611f\u77e5\u538b\u7f29\u6027\u80fd\u4e0a\u8fbe\u5230\u6700\u4f18\uff0c\u89e3\u7801\u901f\u5ea6\u5feb20\u500d\uff0c\u6bd4\u7279\u7387\u964d\u4f4e86.92%\u3002", "conclusion": "DiffVC-OSD\u901a\u8fc7\u5355\u6b65\u6269\u6563\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u538b\u7f29\u7684\u6548\u7387\u548c\u8d28\u91cf\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.07701", "pdf": "https://arxiv.org/pdf/2508.07701", "abs": "https://arxiv.org/abs/2508.07701", "authors": ["Bo Jia", "Yanan Guo", "Ying Chang", "Benkui Zhang", "Ying Xie", "Kangning Du", "Lin Cao"], "title": "Multi-view Normal and Distance Guidance Gaussian Splatting for Surface Reconstruction", "categories": ["cs.CV", "cs.RO"], "comment": "This paper has been accepted by IROS 2025", "summary": "3D Gaussian Splatting (3DGS) achieves remarkable results in the field of surface reconstruction. However, when Gaussian normal vectors are aligned within the single-view projection plane, while the geometry appears reasonable in the current view, biases may emerge upon switching to nearby views. To address the distance and global matching challenges in multi-view scenes, we design multi-view normal and distance-guided Gaussian splatting. This method achieves geometric depth unification and high-accuracy reconstruction by constraining nearby depth maps and aligning 3D normals. Specifically, for the reconstruction of small indoor and outdoor scenes, we propose a multi-view distance reprojection regularization module that achieves multi-view Gaussian alignment by computing the distance loss between two nearby views and the same Gaussian surface. Additionally, we develop a multi-view normal enhancement module, which ensures consistency across views by matching the normals of pixel points in nearby views and calculating the loss. Extensive experimental results demonstrate that our method outperforms the baseline in both quantitative and qualitative evaluations, significantly enhancing the surface reconstruction capability of 3DGS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u89c6\u89d2\u8ddd\u79bb\u548c\u6cd5\u7ebf\u5f15\u5bfc\u7684\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e863D\u9ad8\u65af\u6cfc\u6e85\u5728\u591a\u89c6\u89d2\u573a\u666f\u4e2d\u7684\u51e0\u4f55\u504f\u5dee\u95ee\u9898\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u5728\u5355\u89c6\u89d2\u6295\u5f71\u5e73\u9762\u5185\u51e0\u4f55\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591a\u89c6\u89d2\u5207\u6362\u65f6\u53ef\u80fd\u51fa\u73b0\u504f\u5dee\uff0c\u9700\u89e3\u51b3\u591a\u89c6\u89d2\u573a\u666f\u7684\u8ddd\u79bb\u548c\u5168\u5c40\u5339\u914d\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u591a\u89c6\u89d2\u8ddd\u79bb\u91cd\u6295\u5f71\u6b63\u5219\u5316\u6a21\u5757\u548c\u591a\u89c6\u89d2\u6cd5\u7ebf\u589e\u5f3a\u6a21\u5757\uff0c\u901a\u8fc7\u7ea6\u675f\u90bb\u8fd1\u6df1\u5ea6\u56fe\u548c\u5339\u914d3D\u6cd5\u7ebf\u5b9e\u73b0\u51e0\u4f55\u6df1\u5ea6\u7edf\u4e00\u548c\u9ad8\u7cbe\u5ea6\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u9ad8\u65af\u6cfc\u6e85\u7684\u8868\u9762\u91cd\u5efa\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u89c6\u89d2\u51e0\u4f55\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u5347\u4e863D\u9ad8\u65af\u6cfc\u6e85\u7684\u91cd\u5efa\u7cbe\u5ea6\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.07747", "pdf": "https://arxiv.org/pdf/2508.07747", "abs": "https://arxiv.org/abs/2508.07747", "authors": ["Junhyuk So", "Juncheol Shin", "Hyunho Kook", "Eunhyeok Park"], "title": "Grouped Speculative Decoding for Autoregressive Image Generation", "categories": ["cs.CV"], "comment": "Accepted to the ICCV 2025", "summary": "Recently, autoregressive (AR) image models have demonstrated remarkable generative capabilities, positioning themselves as a compelling alternative to diffusion models. However, their sequential nature leads to long inference times, limiting their practical scalability. In this work, we introduce Grouped Speculative Decoding (GSD), a novel, training-free acceleration method for AR image models. While recent studies have explored Speculative Decoding (SD) as a means to speed up AR image generation, existing approaches either provide only modest acceleration or require additional training. Our in-depth analysis reveals a fundamental difference between language and image tokens: image tokens exhibit inherent redundancy and diversity, meaning multiple tokens can convey valid semantics. However, traditional SD methods are designed to accept only a single most-likely token, which fails to leverage this difference, leading to excessive false-negative rejections. To address this, we propose a new SD strategy that evaluates clusters of visually valid tokens rather than relying on a single target token. Additionally, we observe that static clustering based on embedding distance is ineffective, which motivates our dynamic GSD approach. Extensive experiments show that GSD accelerates AR image models by an average of 3.7x while preserving image quality-all without requiring any additional training. The source code is available at https://github.com/junhyukso/GSD", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGrouped Speculative Decoding (GSD)\u7684\u65e0\u8bad\u7ec3\u52a0\u901f\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u56de\u5f52\u56fe\u50cf\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u56e0\u4ec5\u63a5\u53d7\u5355\u4e00\u6700\u53ef\u80fd\u6807\u8bb0\u800c\u5bfc\u81f4\u7684\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "motivation": "\u81ea\u56de\u5f52\u56fe\u50cf\u6a21\u578b\u56e0\u5176\u5e8f\u5217\u6027\u8d28\u5bfc\u81f4\u63a8\u7406\u65f6\u95f4\u957f\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u8981\u4e48\u6548\u679c\u6709\u9650\uff0c\u8981\u4e48\u9700\u8981\u989d\u5916\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa\u52a8\u6001GSD\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc4\u4f30\u89c6\u89c9\u6709\u6548\u6807\u8bb0\u7684\u7c07\u800c\u975e\u5355\u4e00\u76ee\u6807\u6807\u8bb0\uff0c\u5229\u7528\u56fe\u50cf\u6807\u8bb0\u7684\u5197\u4f59\u6027\u548c\u591a\u6837\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGSD\u5e73\u5747\u52a0\u901f3.7\u500d\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "GSD\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u81ea\u56de\u5f52\u56fe\u50cf\u6a21\u578b\u52a0\u901f\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2508.07755", "pdf": "https://arxiv.org/pdf/2508.07755", "abs": "https://arxiv.org/abs/2508.07755", "authors": ["Minseo Kim", "Minchan Kwon", "Dongyeun Lee", "Yunho Jeon", "Junmo Kim"], "title": "Comparison Reveals Commonality: Customized Image Generation through Contrastive Inversion", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025 workshop (AI4CC)", "summary": "The recent demand for customized image generation raises a need for techniques that effectively extract the common concept from small sets of images. Existing methods typically rely on additional guidance, such as text prompts or spatial masks, to capture the common target concept. Unfortunately, relying on manually provided guidance can lead to incomplete separation of auxiliary features, which degrades generation quality.In this paper, we propose Contrastive Inversion, a novel approach that identifies the common concept by comparing the input images without relying on additional information. We train the target token along with the image-wise auxiliary text tokens via contrastive learning, which extracts the well-disentangled true semantics of the target. Then we apply disentangled cross-attention fine-tuning to improve concept fidelity without overfitting. Experimental results and analysis demonstrate that our method achieves a balanced, high-level performance in both concept representation and editing, outperforming existing techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u4fe1\u606f\u7684\u5bf9\u6bd4\u53cd\u8f6c\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u8f93\u5165\u56fe\u50cf\u63d0\u53d6\u5171\u540c\u6982\u5ff5\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u5b9a\u5236\u5316\u56fe\u50cf\u751f\u6210\u9700\u6c42\u589e\u52a0\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u989d\u5916\u6307\u5bfc\uff08\u5982\u6587\u672c\u63d0\u793a\u6216\u7a7a\u95f4\u63a9\u7801\uff09\uff0c\u53ef\u80fd\u5bfc\u81f4\u8f85\u52a9\u7279\u5f81\u5206\u79bb\u4e0d\u5b8c\u6574\uff0c\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u76ee\u6807\u6807\u8bb0\u548c\u56fe\u50cf\u8f85\u52a9\u6587\u672c\u6807\u8bb0\uff0c\u901a\u8fc7\u89e3\u8026\u4ea4\u53c9\u6ce8\u610f\u529b\u5fae\u8c03\u63d0\u9ad8\u6982\u5ff5\u4fdd\u771f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6982\u5ff5\u8868\u793a\u548c\u7f16\u8f91\u65b9\u9762\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u5bf9\u6bd4\u53cd\u8f6c\u65b9\u6cd5\u5728\u65e0\u9700\u989d\u5916\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5171\u540c\u6982\u5ff5\u63d0\u53d6\u548c\u7f16\u8f91\u3002"}}
{"id": "2508.07759", "pdf": "https://arxiv.org/pdf/2508.07759", "abs": "https://arxiv.org/abs/2508.07759", "authors": ["Haoran Wang", "Zekun Li", "Jian Zhang", "Lei Qi", "Yinghuan Shi"], "title": "Correspondence as Video: Test-Time Adaption on SAM2 for Reference Segmentation in the Wild", "categories": ["cs.CV"], "comment": null, "summary": "Large vision models like the Segment Anything Model (SAM) exhibit significant limitations when applied to downstream tasks in the wild. Consequently, reference segmentation, which leverages reference images and their corresponding masks to impart novel knowledge to the model, emerges as a promising new direction for adapting vision models. However, existing reference segmentation approaches predominantly rely on meta-learning, which still necessitates an extensive meta-training process and brings massive data and computational cost. In this study, we propose a novel approach by representing the inherent correspondence between reference-target image pairs as a pseudo video. This perspective allows the latest version of SAM, known as SAM2, which is equipped with interactive video object segmentation (iVOS) capabilities, to be adapted to downstream tasks in a lightweight manner. We term this approach Correspondence As Video for SAM (CAV-SAM). CAV-SAM comprises two key modules: the Diffusion-Based Semantic Transition (DBST) module employs a diffusion model to construct a semantic transformation sequence, while the Test-Time Geometric Alignment (TTGA) module aligns the geometric changes within this sequence through test-time fine-tuning. We evaluated CAVSAM on widely-used datasets, achieving segmentation performance improvements exceeding 5% over SOTA methods. Implementation is provided in the supplementary materials.", "AI": {"tldr": "CAV-SAM\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u53c2\u8003-\u76ee\u6807\u56fe\u50cf\u5bf9\u7684\u5bf9\u5e94\u5173\u7cfb\u8868\u793a\u4e3a\u4f2a\u89c6\u9891\uff0c\u5229\u7528SAM2\u7684iVOS\u80fd\u529b\u9002\u5e94\u4e0b\u6e38\u4efb\u52a1\uff0c\u6027\u80fd\u63d0\u5347\u8d85\u8fc75%\u3002", "motivation": "\u73b0\u6709\u53c2\u8003\u5206\u5272\u65b9\u6cd5\u4f9d\u8d56\u5143\u5b66\u4e60\uff0c\u6570\u636e\u4e0e\u8ba1\u7b97\u6210\u672c\u9ad8\uff0cCAV-SAM\u65e8\u5728\u4ee5\u66f4\u8f7b\u91cf\u7684\u65b9\u5f0f\u9002\u5e94\u4e0b\u6e38\u4efb\u52a1\u3002", "method": "\u5c06\u53c2\u8003-\u76ee\u6807\u56fe\u50cf\u5bf9\u7684\u5bf9\u5e94\u5173\u7cfb\u8868\u793a\u4e3a\u4f2a\u89c6\u9891\uff0c\u5229\u7528SAM2\u7684iVOS\u80fd\u529b\uff0c\u7ed3\u5408DBST\u548cTTGA\u6a21\u5757\u3002", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u5206\u5272\u6027\u80fd\u63d0\u5347\u8d85\u8fc75%\u3002", "conclusion": "CAV-SAM\u4e3a\u8f7b\u91cf\u7ea7\u9002\u5e94\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2508.07769", "pdf": "https://arxiv.org/pdf/2508.07769", "abs": "https://arxiv.org/abs/2508.07769", "authors": ["Xiaoyan Liu", "Kangrui Li", "Jiaxin Liu"], "title": "Dream4D: Lifting Camera-Controlled I2V towards Spatiotemporally Consistent 4D Generation", "categories": ["cs.CV"], "comment": "Project Page: https://wanderer7-sk.github.io/Dream4D.github.io/", "summary": "The synthesis of spatiotemporally coherent 4D content presents fundamental challenges in computer vision, requiring simultaneous modeling of high-fidelity spatial representations and physically plausible temporal dynamics. Current approaches often struggle to maintain view consistency while handling complex scene dynamics, particularly in large-scale environments with multiple interacting elements. This work introduces Dream4D, a novel framework that bridges this gap through a synergy of controllable video generation and neural 4D reconstruction. Our approach seamlessly combines a two-stage architecture: it first predicts optimal camera trajectories from a single image using few-shot learning, then generates geometrically consistent multi-view sequences via a specialized pose-conditioned diffusion process, which are finally converted into a persistent 4D representation. This framework is the first to leverage both rich temporal priors from video diffusion models and geometric awareness of the reconstruction models, which significantly facilitates 4D generation and shows higher quality (e.g., mPSNR, mSSIM) over existing methods.", "AI": {"tldr": "Dream4D\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u53ef\u63a7\u89c6\u9891\u751f\u6210\u548c\u795e\u7ecf4D\u91cd\u5efa\uff0c\u89e3\u51b3\u4e864D\u5185\u5bb9\u5408\u6210\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728\u4fdd\u6301\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u5904\u7406\u590d\u6742\u573a\u666f\u52a8\u6001\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u591a\u5143\u7d20\u4ea4\u4e92\u73af\u5883\u4e2d\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u67b6\u6784\uff1a\u5148\u901a\u8fc7\u5c11\u6837\u672c\u5b66\u4e60\u4ece\u5355\u56fe\u50cf\u9884\u6d4b\u6700\u4f73\u76f8\u673a\u8f68\u8ff9\uff0c\u518d\u901a\u8fc7\u59ff\u6001\u6761\u4ef6\u6269\u6563\u8fc7\u7a0b\u751f\u6210\u51e0\u4f55\u4e00\u81f4\u7684\u591a\u89c6\u89d2\u5e8f\u5217\uff0c\u6700\u7ec8\u8f6c\u6362\u4e3a\u6301\u4e454D\u8868\u793a\u3002", "result": "\u6846\u67b6\u9996\u6b21\u7ed3\u5408\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u65f6\u95f4\u5148\u9a8c\u548c\u91cd\u5efa\u6a21\u578b\u7684\u51e0\u4f55\u611f\u77e5\uff0c\u751f\u6210\u8d28\u91cf\uff08\u5982mPSNR\u3001mSSIM\uff09\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Dream4D\u901a\u8fc7\u534f\u540c\u53ef\u63a7\u89c6\u9891\u751f\u6210\u4e0e\u795e\u7ecf4D\u91cd\u5efa\uff0c\u663e\u8457\u63d0\u5347\u4e864D\u5185\u5bb9\u5408\u6210\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\u548c\u8d28\u91cf\u3002"}}
{"id": "2508.07788", "pdf": "https://arxiv.org/pdf/2508.07788", "abs": "https://arxiv.org/abs/2508.07788", "authors": ["Runze Wang", "Zeli Chen", "Zhiyun Song", "Wei Fang", "Jiajin Zhang", "Danyang Tu", "Yuxing Tang", "Minfeng Xu", "Xianghua Ye", "Le Lu", "Dakai Jin"], "title": "Anatomy-Aware Low-Dose CT Denoising via Pretrained Vision Models and Semantic-Guided Contrastive Learning", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "To reduce radiation exposure and improve the diagnostic efficacy of low-dose computed tomography (LDCT), numerous deep learning-based denoising methods have been developed to mitigate noise and artifacts. However, most of these approaches ignore the anatomical semantics of human tissues, which may potentially result in suboptimal denoising outcomes. To address this problem, we propose ALDEN, an anatomy-aware LDCT denoising method that integrates semantic features of pretrained vision models (PVMs) with adversarial and contrastive learning. Specifically, we introduce an anatomy-aware discriminator that dynamically fuses hierarchical semantic features from reference normal-dose CT (NDCT) via cross-attention mechanisms, enabling tissue-specific realism evaluation in the discriminator. In addition, we propose a semantic-guided contrastive learning module that enforces anatomical consistency by contrasting PVM-derived features from LDCT, denoised CT and NDCT, preserving tissue-specific patterns through positive pairs and suppressing artifacts via dual negative pairs. Extensive experiments conducted on two LDCT denoising datasets reveal that ALDEN achieves the state-of-the-art performance, offering superior anatomy preservation and substantially reducing over-smoothing issue of previous work. Further validation on a downstream multi-organ segmentation task (encompassing 117 anatomical structures) affirms the model's ability to maintain anatomical awareness.", "AI": {"tldr": "ALDEN\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4f4e\u5242\u91cfCT\u53bb\u566a\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u7684\u8bed\u4e49\u7279\u5f81\u4e0e\u5bf9\u6297\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53bb\u566a\u6548\u679c\u5e76\u4fdd\u7559\u4e86\u7ec4\u7ec7\u89e3\u5256\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u4f4e\u5242\u91cfCT\u53bb\u566a\u65b9\u6cd5\u5ffd\u89c6\u4e86\u7ec4\u7ec7\u89e3\u5256\u8bed\u4e49\uff0c\u53ef\u80fd\u5bfc\u81f4\u53bb\u566a\u6548\u679c\u4e0d\u4f73\u3002ALDEN\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u89e3\u5256\u611f\u77e5\u673a\u5236\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "ALDEN\u7ed3\u5408\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u7684\u8bed\u4e49\u7279\u5f81\uff0c\u4f7f\u7528\u89e3\u5256\u611f\u77e5\u5224\u522b\u5668\u548c\u8bed\u4e49\u5f15\u5bfc\u7684\u5bf9\u6bd4\u5b66\u4e60\u6a21\u5757\uff0c\u52a8\u6001\u8bc4\u4f30\u7ec4\u7ec7\u7279\u5f02\u6027\u771f\u5b9e\u6027\u5e76\u4fdd\u6301\u89e3\u5256\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u4f4e\u5242\u91cfCT\u53bb\u566a\u6570\u636e\u96c6\u4e0a\uff0cALDEN\u5b9e\u73b0\u4e86\u6700\u4f18\u6027\u80fd\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8fc7\u5e73\u6ed1\u95ee\u9898\uff0c\u5e76\u5728\u591a\u5668\u5b98\u5206\u5272\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u89e3\u5256\u611f\u77e5\u80fd\u529b\u3002", "conclusion": "ALDEN\u901a\u8fc7\u89e3\u5256\u611f\u77e5\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u5242\u91cfCT\u53bb\u566a\u6548\u679c\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07811", "pdf": "https://arxiv.org/pdf/2508.07811", "abs": "https://arxiv.org/abs/2508.07811", "authors": ["Sicheng Gao", "Nancy Mehta", "Zongwei Wu", "Radu Timofte"], "title": "DiTVR: Zero-Shot Diffusion Transformer for Video Restoration", "categories": ["cs.CV"], "comment": "7 pages, 6 figures", "summary": "Video restoration aims to reconstruct high quality video sequences from low quality inputs, addressing tasks such as super resolution, denoising, and deblurring. Traditional regression based methods often produce unrealistic details and require extensive paired datasets, while recent generative diffusion models face challenges in ensuring temporal consistency. We introduce DiTVR, a zero shot video restoration framework that couples a diffusion transformer with trajectory aware attention and a wavelet guided, flow consistent sampler. Unlike prior 3D convolutional or frame wise diffusion approaches, our attention mechanism aligns tokens along optical flow trajectories, with particular emphasis on vital layers that exhibit the highest sensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically selects relevant tokens based on motion correspondences across frames. The flow guided sampler injects data consistency only into low-frequency bands, preserving high frequency priors while accelerating convergence. DiTVR establishes a new zero shot state of the art on video restoration benchmarks, demonstrating superior temporal consistency and detail preservation while remaining robust to flow noise and occlusions.", "AI": {"tldr": "DiTVR\u662f\u4e00\u79cd\u96f6\u6837\u672c\u89c6\u9891\u4fee\u590d\u6846\u67b6\uff0c\u7ed3\u5408\u6269\u6563\u53d8\u6362\u5668\u548c\u8f68\u8ff9\u611f\u77e5\u6ce8\u610f\u529b\uff0c\u901a\u8fc7\u5149\u6d41\u8f68\u8ff9\u5bf9\u9f50\u4ee4\u724c\uff0c\u63d0\u5347\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edf\u56de\u5f52\u65b9\u6cd5\u751f\u6210\u4e0d\u771f\u5b9e\u7ec6\u8282\u4e14\u9700\u5927\u91cf\u914d\u5bf9\u6570\u636e\uff0c\u800c\u751f\u6210\u6269\u6563\u6a21\u578b\u96be\u4ee5\u4fdd\u8bc1\u65f6\u95f4\u4e00\u81f4\u6027\u3002DiTVR\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6269\u6563\u53d8\u6362\u5668\u4e0e\u8f68\u8ff9\u611f\u77e5\u6ce8\u610f\u529b\u7ed3\u5408\uff0c\u901a\u8fc7\u5149\u6d41\u8f68\u8ff9\u5bf9\u9f50\u4ee4\u724c\uff0c\u5e76\u5f15\u5165\u5c0f\u6ce2\u5f15\u5bfc\u7684\u6d41\u4e00\u81f4\u6027\u91c7\u6837\u5668\u3002", "result": "\u5728\u89c6\u9891\u4fee\u590d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u96f6\u6837\u672c\u6700\u4f18\u6027\u80fd\uff0c\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u4fdd\u7559\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5bf9\u5149\u6d41\u566a\u58f0\u548c\u906e\u6321\u9c81\u68d2\u3002", "conclusion": "DiTVR\u901a\u8fc7\u521b\u65b0\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u91c7\u6837\u5668\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u4fee\u590d\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u8d28\u91cf\u3002"}}
{"id": "2508.07819", "pdf": "https://arxiv.org/pdf/2508.07819", "abs": "https://arxiv.org/abs/2508.07819", "authors": ["Ke Ma", "Jun Long", "Hongxiao Fei", "Liujie Hua", "Yueyi Luo"], "title": "Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "4 pages, 1 reference, 3 figures, icassp 2026", "summary": "Pre-trained Vision-Language Models (VLMs) face a significant adaptation gap when applied to Zero-Shot Anomaly Detection (ZSAD), stemming from their lack of local inductive biases for dense prediction and their reliance on inflexible feature fusion paradigms. We address these limitations through an Architectural Co-Design framework that jointly refines feature representation and cross-modal fusion. Our method integrates a parameter-efficient Convolutional Low-Rank Adaptation (Conv-LoRA) adapter to inject local inductive biases for fine-grained representation, and introduces a Dynamic Fusion Gateway (DFG) that leverages visual context to adaptively modulate text prompts, enabling a powerful bidirectional fusion. Extensive experiments on diverse industrial and medical benchmarks demonstrate superior accuracy and robustness, validating that this synergistic co-design is critical for robustly adapting foundation models to dense perception tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u67b6\u6784\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u7279\u5f81\u8868\u793a\u548c\u8de8\u6a21\u6001\u878d\u5408\uff0c\u89e3\u51b3\u4e86\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u9002\u5e94\u6027\u95ee\u9898\u3002", "motivation": "\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5b58\u5728\u9002\u5e94\u6027\u5dee\u8ddd\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u5c40\u90e8\u5f52\u7eb3\u504f\u7f6e\u548c\u7279\u5f81\u878d\u5408\u8303\u5f0f\u4e0d\u7075\u6d3b\u3002", "method": "\u91c7\u7528\u53c2\u6570\u9ad8\u6548\u7684\u5377\u79ef\u4f4e\u79e9\u9002\u5e94\uff08Conv-LoRA\uff09\u9002\u914d\u5668\u6ce8\u5165\u5c40\u90e8\u5f52\u7eb3\u504f\u7f6e\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u878d\u5408\u7f51\u5173\uff08DFG\uff09\u81ea\u9002\u5e94\u8c03\u5236\u6587\u672c\u63d0\u793a\u3002", "result": "\u5728\u5de5\u4e1a\u548c\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\u5bf9\u4e8e\u5c06\u57fa\u7840\u6a21\u578b\u9002\u5e94\u4e8e\u5bc6\u96c6\u611f\u77e5\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.07878", "pdf": "https://arxiv.org/pdf/2508.07878", "abs": "https://arxiv.org/abs/2508.07878", "authors": ["Hanting Wang", "Shengpeng Ji", "Shulei Wang", "Hai Huang", "Xiao Jin", "Qifei Zhang", "Tao Jin"], "title": "TAP: Parameter-efficient Task-Aware Prompting for Adverse Weather Removal", "categories": ["cs.CV"], "comment": null, "summary": "Image restoration under adverse weather conditions has been extensively explored, leading to numerous high-performance methods. In particular, recent advances in All-in-One approaches have shown impressive results by training on multi-task image restoration datasets. However, most of these methods rely on dedicated network modules or parameters for each specific degradation type, resulting in a significant parameter overhead. Moreover, the relatedness across different restoration tasks is often overlooked. In light of these issues, we propose a parameter-efficient All-in-One image restoration framework that leverages task-aware enhanced prompts to tackle various adverse weather degradations.Specifically, we adopt a two-stage training paradigm consisting of a pretraining phase and a prompt-tuning phase to mitigate parameter conflicts across tasks. We first employ supervised learning to acquire general restoration knowledge, and then adapt the model to handle specific degradation via trainable soft prompts. Crucially, we enhance these task-specific prompts in a task-aware manner. We apply low-rank decomposition to these prompts to capture both task-general and task-specific characteristics, and impose contrastive constraints to better align them with the actual inter-task relatedness. These enhanced prompts not only improve the parameter efficiency of the restoration model but also enable more accurate task modeling, as evidenced by t-SNE analysis. Experimental results on different restoration tasks demonstrate that the proposed method achieves superior performance with only 2.75M parameters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684All-in-One\u56fe\u50cf\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u589e\u5f3a\u63d0\u793a\u5904\u7406\u591a\u79cd\u6076\u52a3\u5929\u6c14\u9000\u5316\u95ee\u9898\uff0c\u53c2\u6570\u4ec52.75M\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e13\u7528\u7f51\u7edc\u6a21\u5757\u6216\u53c2\u6570\uff0c\u5bfc\u81f4\u53c2\u6570\u5f00\u9500\u5927\uff0c\u4e14\u5ffd\u89c6\u4efb\u52a1\u95f4\u76f8\u5173\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u9884\u8bad\u7ec3\u548c\u63d0\u793a\u8c03\u4f18\uff09\uff0c\u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u589e\u5f3a\u63d0\u793a\u548c\u4f4e\u79e9\u5206\u89e3\u6355\u6349\u4efb\u52a1\u901a\u7528\u4e0e\u7279\u5b9a\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u4fee\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u53c2\u6570\u6548\u7387\u9ad8\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\uff0c\u4e3a\u591a\u4efb\u52a1\u56fe\u50cf\u4fee\u590d\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.07897", "pdf": "https://arxiv.org/pdf/2508.07897", "abs": "https://arxiv.org/abs/2508.07897", "authors": ["Tianle Zeng", "Junlei Hu", "Gerardo Loza Galindo", "Sharib Ali", "Duygu Sarikaya", "Pietro Valdastri", "Dominic Jones"], "title": "NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and Deformable 3D Gaussian Reconstruction", "categories": ["cs.CV", "cs.AI", "I.3.3"], "comment": "13 pages, 9 figures", "summary": "Computer vision-based technologies significantly enhance surgical automation by advancing tool tracking, detection, and localization. However, Current data-driven approaches are data-voracious, requiring large, high-quality labeled image datasets, which limits their application in surgical data science. Our Work introduces a novel dynamic Gaussian Splatting technique to address the data scarcity in surgical image datasets. We propose a dynamic Gaussian model to represent dynamic surgical scenes, enabling the rendering of surgical instruments from unseen viewpoints and deformations with real tissue backgrounds. We utilize a dynamic training adjustment strategy to address challenges posed by poorly calibrated camera poses from real-world scenarios. Additionally, we propose a method based on dynamic Gaussians for automatically generating annotations for our synthetic data. For evaluation, we constructed a new dataset featuring seven scenes with 14,000 frames of tool and camera motion and tool jaw articulation, with a background of an ex-vivo porcine model. Using this dataset, we synthetically replicate the scene deformation from the ground truth data, allowing direct comparisons of synthetic image quality. Experimental results illustrate that our method generates photo-realistic labeled image datasets with the highest values in Peak-Signal-to-Noise Ratio (29.87). We further evaluate the performance of medical-specific neural networks trained on real and synthetic images using an unseen real-world image dataset. Our results show that the performance of models trained on synthetic images generated by the proposed method outperforms those trained with state-of-the-art standard data augmentation by 10%, leading to an overall improvement in model performances by nearly 15%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u9ad8\u65afSplatting\u6280\u672f\uff0c\u7528\u4e8e\u89e3\u51b3\u624b\u672f\u56fe\u50cf\u6570\u636e\u96c6\u7a00\u7f3a\u95ee\u9898\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u9650\u5236\u4e86\u624b\u672f\u6570\u636e\u79d1\u5b66\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u52a8\u6001\u9ad8\u65af\u6a21\u578b\u8868\u793a\u52a8\u6001\u624b\u672f\u573a\u666f\uff0c\u7ed3\u5408\u52a8\u6001\u8bad\u7ec3\u8c03\u6574\u7b56\u7565\u548c\u81ea\u52a8\u6807\u6ce8\u65b9\u6cd5\u3002", "result": "\u751f\u6210\u7684\u7167\u7247\u7ea7\u5408\u6210\u6570\u636e\u5728PSNR\u4e0a\u8868\u73b0\u6700\u4f73\uff0829.87\uff09\uff0c\u6a21\u578b\u6027\u80fd\u63d0\u534715%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u624b\u672f\u81ea\u52a8\u5316\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2508.07901", "pdf": "https://arxiv.org/pdf/2508.07901", "abs": "https://arxiv.org/abs/2508.07901", "authors": ["Bowen Xue", "Qixin Yan", "Wenjing Wang", "Hao Liu", "Chen Li"], "title": "Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Generating high-fidelity human videos that match user-specified identities is important yet challenging in the field of generative AI. Existing methods often rely on an excessive number of training parameters and lack compatibility with other AIGC tools. In this paper, we propose Stand-In, a lightweight and plug-and-play framework for identity preservation in video generation. Specifically, we introduce a conditional image branch into the pre-trained video generation model. Identity control is achieved through restricted self-attentions with conditional position mapping, and can be learned quickly with only 2000 pairs. Despite incorporating and training just $\\sim$1\\% additional parameters, our framework achieves excellent results in video quality and identity preservation, outperforming other full-parameter training methods. Moreover, our framework can be seamlessly integrated for other tasks, such as subject-driven video generation, pose-referenced video generation, stylization, and face swapping.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684\u89c6\u9891\u751f\u6210\u6846\u67b6Stand-In\uff0c\u901a\u8fc7\u6761\u4ef6\u56fe\u50cf\u5206\u652f\u548c\u53d7\u9650\u81ea\u6ce8\u610f\u529b\u5b9e\u73b0\u8eab\u4efd\u4fdd\u7559\uff0c\u4ec5\u9700\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u548c\u53c2\u6570\u5373\u53ef\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8fc7\u591a\u8bad\u7ec3\u53c2\u6570\u4e14\u4e0e\u5176\u4ed6AIGC\u5de5\u5177\u4e0d\u517c\u5bb9\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8eab\u4efd\u4fdd\u7559\u89c6\u9891\u751f\u6210\u65b9\u6848\u3002", "method": "\u5728\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e2d\u5f15\u5165\u6761\u4ef6\u56fe\u50cf\u5206\u652f\uff0c\u901a\u8fc7\u53d7\u9650\u81ea\u6ce8\u610f\u529b\u548c\u6761\u4ef6\u4f4d\u7f6e\u6620\u5c04\u5b9e\u73b0\u8eab\u4efd\u63a7\u5236\uff0c\u4ec5\u97002000\u5bf9\u6570\u636e\u5feb\u901f\u5b66\u4e60\u3002", "result": "\u4ec5\u589e\u52a0\u7ea61%\u53c2\u6570\uff0c\u5373\u53ef\u5728\u89c6\u9891\u8d28\u91cf\u548c\u8eab\u4efd\u4fdd\u7559\u4e0a\u4f18\u4e8e\u5176\u4ed6\u5168\u53c2\u6570\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u652f\u6301\u591a\u79cd\u4efb\u52a1\u96c6\u6210\u3002", "conclusion": "Stand-In\u6846\u67b6\u9ad8\u6548\u3001\u8f7b\u91cf\u4e14\u591a\u529f\u80fd\uff0c\u4e3a\u8eab\u4efd\u4fdd\u7559\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07903", "pdf": "https://arxiv.org/pdf/2508.07903", "abs": "https://arxiv.org/abs/2508.07903", "authors": ["Johanna P. M\u00fcller", "Anika Knupfer", "Pedro Bl\u00f6ss", "Edoardo Berardi Vittur", "Bernhard Kainz", "Jana Hutter"], "title": "Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Accepted at MICCAI CAPI 2025", "summary": "Despite significant progress in generative modelling, existing diffusion models often struggle to produce anatomically precise female pelvic images, limiting their application in gynaecological imaging, where data scarcity and patient privacy concerns are critical. To overcome these barriers, we introduce a novel diffusion-based framework for uterine MRI synthesis, integrating both unconditional and conditioned Denoising Diffusion Probabilistic Models (DDPMs) and Latent Diffusion Models (LDMs) in 2D and 3D. Our approach generates anatomically coherent, high fidelity synthetic images that closely mimic real scans and provide valuable resources for training robust diagnostic models. We evaluate generative quality using advanced perceptual and distributional metrics, benchmarking against standard reconstruction methods, and demonstrate substantial gains in diagnostic accuracy on a key classification task. A blinded expert evaluation further validates the clinical realism of our synthetic images. We release our models with privacy safeguards and a comprehensive synthetic uterine MRI dataset to support reproducible research and advance equitable AI in gynaecology.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6269\u6563\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u5973\u6027\u76c6\u8154MRI\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u89e3\u5256\u5b66\u7cbe\u786e\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u5973\u6027\u76c6\u8154\u56fe\u50cf\u65f6\u89e3\u5256\u5b66\u7cbe\u786e\u6027\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5728\u5987\u79d1\u5f71\u50cf\u4e2d\u7684\u5e94\u7528\u3002\u6570\u636e\u7a00\u7f3a\u548c\u60a3\u8005\u9690\u79c1\u95ee\u9898\u4e5f\u662f\u5173\u952e\u6311\u6218\u3002", "method": "\u7ed3\u5408\u65e0\u6761\u4ef6\u4e0e\u6761\u4ef6\u5316Denoising Diffusion Probabilistic Models (DDPMs)\u548cLatent Diffusion Models (LDMs)\u76842D\u548c3D\u6846\u67b6\uff0c\u751f\u6210\u89e3\u5256\u5b66\u4e00\u81f4\u7684\u9ad8\u4fdd\u771f\u5408\u6210\u56fe\u50cf\u3002", "result": "\u751f\u6210\u7684\u56fe\u50cf\u5728\u611f\u77e5\u548c\u5206\u5e03\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002\u4e13\u5bb6\u76f2\u8bc4\u9a8c\u8bc1\u4e86\u5176\u4e34\u5e8a\u771f\u5b9e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5987\u79d1\u5f71\u50cf\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u6570\u636e\u8d44\u6e90\uff0c\u652f\u6301\u53ef\u91cd\u590d\u7814\u7a76\u548c\u516c\u5e73AI\u53d1\u5c55\u3002"}}
{"id": "2508.07905", "pdf": "https://arxiv.org/pdf/2508.07905", "abs": "https://arxiv.org/abs/2508.07905", "authors": ["Yongtao Ge", "Kangyang Xie", "Guangkai Xu", "Mingyu Liu", "Li Ke", "Longtao Huang", "Hui Xue", "Hao Chen", "Chunhua Shen"], "title": "Generative Video Matting", "categories": ["cs.CV"], "comment": null, "summary": "Video matting has traditionally been limited by the lack of high-quality ground-truth data. Most existing video matting datasets provide only human-annotated imperfect alpha and foreground annotations, which must be composited to background images or videos during the training stage. Thus, the generalization capability of previous methods in real-world scenarios is typically poor. In this work, we propose to solve the problem from two perspectives. First, we emphasize the importance of large-scale pre-training by pursuing diverse synthetic and pseudo-labeled segmentation datasets. We also develop a scalable synthetic data generation pipeline that can render diverse human bodies and fine-grained hairs, yielding around 200 video clips with a 3-second duration for fine-tuning. Second, we introduce a novel video matting approach that can effectively leverage the rich priors from pre-trained video diffusion models. This architecture offers two key advantages. First, strong priors play a critical role in bridging the domain gap between synthetic and real-world scenes. Second, unlike most existing methods that process video matting frame-by-frame and use an independent decoder to aggregate temporal information, our model is inherently designed for video, ensuring strong temporal consistency. We provide a comprehensive quantitative evaluation across three benchmark datasets, demonstrating our approach's superior performance, and present comprehensive qualitative results in diverse real-world scenes, illustrating the strong generalization capability of our method. The code is available at https://github.com/aim-uofa/GVM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u62a0\u56fe\u65b9\u6cd5\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u5408\u6210\u6570\u636e\u751f\u6210\uff0c\u7ed3\u5408\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u62a0\u56fe\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u771f\u5b9e\u6570\u636e\u800c\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "1. \u5229\u7528\u591a\u6837\u5316\u7684\u5408\u6210\u548c\u4f2a\u6807\u7b7e\u5206\u5272\u6570\u636e\u96c6\u8fdb\u884c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff1b2. \u5f00\u53d1\u53ef\u6269\u5c55\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff1b3. \u63d0\u51fa\u57fa\u4e8e\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u65b0\u67b6\u6784\uff0c\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u62a0\u56fe\u7684\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.07981", "pdf": "https://arxiv.org/pdf/2508.07981", "abs": "https://arxiv.org/abs/2508.07981", "authors": ["Fangyuan Mao", "Aiming Hao", "Jintao Chen", "Dongxia Liu", "Xiaokun Feng", "Jiashu Zhu", "Meiqi Wu", "Chubin Chen", "Jiahong Wu", "Xiangxiang Chu"], "title": "Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual effects (VFX) are essential visual enhancements fundamental to modern cinematic production. Although video generation models offer cost-efficient solutions for VFX production, current methods are constrained by per-effect LoRA training, which limits generation to single effects. This fundamental limitation impedes applications that require spatially controllable composite effects, i.e., the concurrent generation of multiple effects at designated locations. However, integrating diverse effects into a unified framework faces major challenges: interference from effect variations and spatial uncontrollability during multi-VFX joint training. To tackle these challenges, we propose Omni-Effects, a first unified framework capable of generating prompt-guided effects and spatially controllable composite effects. The core of our framework comprises two key innovations: (1) LoRA-based Mixture of Experts (LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects within a unified model while effectively mitigating cross-task interference. (2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the text token, enabling precise spatial control. Furthermore, we introduce an Independent-Information Flow (IIF) module integrated within the SAP, isolating the control signals corresponding to individual effects to prevent any unwanted blending. To facilitate this research, we construct a comprehensive VFX dataset Omni-VFX via a novel data collection pipeline combining image editing and First-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX evaluation framework for validating model performance. Extensive experiments demonstrate that Omni-Effects achieves precise spatial control and diverse effect generation, enabling users to specify both the category and location of desired effects.", "AI": {"tldr": "Omni-Effects\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u652f\u6301\u63d0\u793a\u5f15\u5bfc\u7684\u6548\u679c\u751f\u6210\u548c\u7a7a\u95f4\u53ef\u63a7\u7684\u590d\u5408\u6548\u679c\uff0c\u89e3\u51b3\u4e86\u591aVFX\u8054\u5408\u8bad\u7ec3\u4e2d\u7684\u5e72\u6270\u548c\u7a7a\u95f4\u4e0d\u53ef\u63a7\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u751f\u6210\u6a21\u578b\u53d7\u9650\u4e8e\u5355\u6548\u679c\u8bad\u7ec3\uff0c\u65e0\u6cd5\u5b9e\u73b0\u7a7a\u95f4\u53ef\u63a7\u7684\u590d\u5408\u6548\u679c\uff0c\u9650\u5236\u4e86\u5e94\u7528\u3002", "method": "\u63d0\u51faLoRA-MoE\u548cSAP\u4e24\u9879\u521b\u65b0\uff0c\u7ed3\u5408IIF\u6a21\u5757\uff0c\u5b9e\u73b0\u591a\u6548\u679c\u96c6\u6210\u548c\u7cbe\u786e\u7a7a\u95f4\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eOmni-Effects\u80fd\u7cbe\u786e\u63a7\u5236\u6548\u679c\u7c7b\u522b\u548c\u4f4d\u7f6e\uff0c\u751f\u6210\u591a\u6837\u5316\u6548\u679c\u3002", "conclusion": "Omni-Effects\u4e3aVFX\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u591a\u6548\u679c\u96c6\u6210\u7684\u7814\u7a76\u3002"}}
{"id": "2508.08048", "pdf": "https://arxiv.org/pdf/2508.08048", "abs": "https://arxiv.org/abs/2508.08048", "authors": ["Peng Dai", "Feitong Tan", "Qiangeng Xu", "Yihua Huang", "David Futschik", "Ruofei Du", "Sean Fanello", "Yinda Zhang", "Xiaojuan Qi"], "title": "S^2VG: 3D Stereoscopic and Spatial Video Generation via Denoising Frame Matrix", "categories": ["cs.CV"], "comment": "immsersive video generation", "summary": "While video generation models excel at producing high-quality monocular videos, generating 3D stereoscopic and spatial videos for immersive applications remains an underexplored challenge. We present a pose-free and training-free method that leverages an off-the-shelf monocular video generation model to produce immersive 3D videos. Our approach first warps the generated monocular video into pre-defined camera viewpoints using estimated depth information, then applies a novel \\textit{frame matrix} inpainting framework. This framework utilizes the original video generation model to synthesize missing content across different viewpoints and timestamps, ensuring spatial and temporal consistency without requiring additional model fine-tuning. Moreover, we develop a \\dualupdate~scheme that further improves the quality of video inpainting by alleviating the negative effects propagated from disoccluded areas in the latent space. The resulting multi-view videos are then adapted into stereoscopic pairs or optimized into 4D Gaussians for spatial video synthesis. We validate the efficacy of our proposed method by conducting experiments on videos from various generative models, such as Sora, Lumiere, WALT, and Zeroscope. The experiments demonstrate that our method has a significant improvement over previous methods. Project page at: https://daipengwa.github.io/S-2VG_ProjectPage/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u59ff\u6001\u4f30\u8ba1\u548c\u989d\u5916\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u73b0\u6210\u7684\u5355\u76ee\u89c6\u9891\u751f\u6210\u6a21\u578b\u751f\u6210\u6c89\u6d78\u5f0f3D\u89c6\u9891\u3002", "motivation": "\u5c3d\u7ba1\u5355\u76ee\u89c6\u9891\u751f\u6210\u6a21\u578b\u5df2\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u4f46\u751f\u62103D\u7acb\u4f53\u548c\u7a7a\u95f4\u89c6\u9891\u4ecd\u662f\u4e00\u4e2a\u672a\u5145\u5206\u63a2\u7d22\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u6df1\u5ea6\u4fe1\u606f\u5c06\u5355\u76ee\u89c6\u9891\u53d8\u5f62\u5230\u9884\u5b9a\u4e49\u89c6\u89d2\uff0c\u5e76\u91c7\u7528\u65b0\u578b\u5e27\u77e9\u9635\u4fee\u590d\u6846\u67b6\uff0c\u7ed3\u5408\u53cc\u66f4\u65b0\u65b9\u6848\u63d0\u5347\u4fee\u590d\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u751f\u6210\u6a21\u578b\u4e0a\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a3D\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.08098", "pdf": "https://arxiv.org/pdf/2508.08098", "abs": "https://arxiv.org/abs/2508.08098", "authors": ["Junzhe Xu", "Yuyang Yin", "Xi Chen"], "title": "TBAC-UniImage: Unified Understanding and Generation by Ladder-Side Diffusion Tuning", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces TBAC-UniImage, a novel unified model for multimodal understanding and generation. We achieve this by deeply integrating a pre-trained Diffusion Model, acting as a generative ladder, with a Multimodal Large Language Model (MLLM). Previous diffusion-based unified models face two primary limitations. One approach uses only the MLLM's final hidden state as the generative condition. This creates a shallow connection, as the generator is isolated from the rich, hierarchical representations within the MLLM's intermediate layers. The other approach, pretraining a unified generative architecture from scratch, is computationally expensive and prohibitive for many researchers. To overcome these issues, our work explores a new paradigm. Instead of relying on a single output, we use representations from multiple, diverse layers of the MLLM as generative conditions for the diffusion model. This method treats the pre-trained generator as a ladder, receiving guidance from various depths of the MLLM's understanding process. Consequently, TBAC-UniImage achieves a much deeper and more fine-grained unification of understanding and generation.", "AI": {"tldr": "TBAC-UniImage\u662f\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u7406\u89e3\u4e0e\u751f\u6210\u7edf\u4e00\u6a21\u578b\uff0c\u901a\u8fc7\u6df1\u5ea6\u96c6\u6210\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6d45\u5c42\u8fde\u63a5\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u7edf\u4e00\u6a21\u578b\u5b58\u5728\u4e24\u79cd\u5c40\u9650\uff1a\u4e00\u662f\u4ec5\u5229\u7528MLLM\u7684\u6700\u7ec8\u9690\u85cf\u72b6\u6001\u4f5c\u4e3a\u751f\u6210\u6761\u4ef6\uff0c\u5bfc\u81f4\u6d45\u5c42\u8fde\u63a5\uff1b\u4e8c\u662f\u4ece\u5934\u8bad\u7ec3\u7edf\u4e00\u751f\u6210\u67b6\u6784\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u5229\u7528MLLM\u591a\u4e2a\u4e0d\u540c\u5c42\u7684\u8868\u793a\u4f5c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u6761\u4ef6\uff0c\u5c06\u9884\u8bad\u7ec3\u751f\u6210\u5668\u89c6\u4e3a\u201c\u68af\u5b50\u201d\uff0c\u63a5\u6536MLLM\u7406\u89e3\u8fc7\u7a0b\u4e2d\u591a\u5c42\u6b21\u7684\u6307\u5bfc\u3002", "result": "TBAC-UniImage\u5b9e\u73b0\u4e86\u66f4\u6df1\u5c42\u6b21\u3001\u66f4\u7ec6\u7c92\u5ea6\u7684\u7406\u89e3\u4e0e\u751f\u6210\u7edf\u4e00\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u6a21\u6001\u7406\u89e3\u4e0e\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6df1\u5165\u7684\u7edf\u4e00\u8303\u5f0f\u3002"}}
{"id": "2508.08123", "pdf": "https://arxiv.org/pdf/2508.08123", "abs": "https://arxiv.org/abs/2508.08123", "authors": ["Lingjing Chen", "Chengxiu Zhang", "Yinqiao Yi", "Yida Wang", "Yang Song", "Xu Yan", "Shengfang Xu", "Dalin Zhu", "Mengqiu Cao", "Yan Zhou", "Chenglong Wang", "Guang Yang"], "title": "A Physics-Driven Neural Network with Parameter Embedding for Generating Quantitative MR Maps from Weighted Images", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "We propose a deep learning-based approach that integrates MRI sequence parameters to improve the accuracy and generalizability of quantitative image synthesis from clinical weighted MRI. Our physics-driven neural network embeds MRI sequence parameters -- repetition time (TR), echo time (TE), and inversion time (TI) -- directly into the model via parameter embedding, enabling the network to learn the underlying physical principles of MRI signal formation. The model takes conventional T1-weighted, T2-weighted, and T2-FLAIR images as input and synthesizes T1, T2, and proton density (PD) quantitative maps. Trained on healthy brain MR images, it was evaluated on both internal and external test datasets. The proposed method achieved high performance with PSNR values exceeding 34 dB and SSIM values above 0.92 for all synthesized parameter maps. It outperformed conventional deep learning models in accuracy and robustness, including data with previously unseen brain structures and lesions. Notably, our model accurately synthesized quantitative maps for these unseen pathological regions, highlighting its superior generalization capability. Incorporating MRI sequence parameters via parameter embedding allows the neural network to better learn the physical characteristics of MR signals, significantly enhancing the performance and reliability of quantitative MRI synthesis. This method shows great potential for accelerating qMRI and improving its clinical utility.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684MRI\u5e8f\u5217\u53c2\u6570\u6574\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u6570\u5d4c\u5165\u63d0\u5347\u5b9a\u91cf\u56fe\u50cf\u5408\u6210\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u63d0\u9ad8\u4e34\u5e8a\u52a0\u6743MRI\u5b9a\u91cf\u56fe\u50cf\u5408\u6210\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u6027\uff0c\u4ee5\u52a0\u901fqMRI\u5e76\u63d0\u5347\u5176\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u91c7\u7528\u7269\u7406\u9a71\u52a8\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u5c06MRI\u5e8f\u5217\u53c2\u6570\uff08TR\u3001TE\u3001TI\uff09\u901a\u8fc7\u53c2\u6570\u5d4c\u5165\u76f4\u63a5\u6574\u5408\u5230\u6a21\u578b\u4e2d\uff0c\u8f93\u5165T1\u52a0\u6743\u3001T2\u52a0\u6743\u548cT2-FLAIR\u56fe\u50cf\uff0c\u5408\u6210T1\u3001T2\u548cPD\u5b9a\u91cf\u56fe\u3002", "result": "\u5728\u5185\u90e8\u548c\u5916\u90e8\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cPSNR\u8d85\u8fc734 dB\uff0cSSIM\u9ad8\u4e8e0.92\uff0c\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4e14\u5bf9\u672a\u89c1\u8fc7\u7684\u8111\u7ed3\u6784\u548c\u75c5\u53d8\u533a\u57df\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u53c2\u6570\u5d4c\u5165\u6574\u5408MRI\u5e8f\u5217\u53c2\u6570\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u91cfMRI\u5408\u6210\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\uff0c\u5c55\u793a\u4e86\u52a0\u901fqMRI\u548c\u63d0\u5347\u4e34\u5e8a\u5b9e\u7528\u6027\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.08134", "pdf": "https://arxiv.org/pdf/2508.08134", "abs": "https://arxiv.org/abs/2508.08134", "authors": ["Zeqian Long", "Mingzhe Zheng", "Kunyu Feng", "Xinhua Zhang", "Hongyu Liu", "Harry Yang", "Linfeng Zhang", "Qifeng Chen", "Yue Ma"], "title": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control", "categories": ["cs.CV"], "comment": "Project webpage is available at https://follow-your-shape.github.io/", "summary": "While recent flow-based image editing models demonstrate general-purpose capabilities across diverse tasks, they often struggle to specialize in challenging scenarios -- particularly those involving large-scale shape transformations. When performing such structural edits, these methods either fail to achieve the intended shape change or inadvertently alter non-target regions, resulting in degraded background quality. We propose Follow-Your-Shape, a training-free and mask-free framework that supports precise and controllable editing of object shapes while strictly preserving non-target content. Motivated by the divergence between inversion and editing trajectories, we compute a Trajectory Divergence Map (TDM) by comparing token-wise velocity differences between the inversion and denoising paths. The TDM enables precise localization of editable regions and guides a Scheduled KV Injection mechanism that ensures stable and faithful editing. To facilitate a rigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120 new images and enriched prompt pairs specifically curated for shape-aware editing. Experiments demonstrate that our method achieves superior editability and visual fidelity, particularly in tasks requiring large-scale shape replacement.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u548c\u63a9\u7801\u7684\u6846\u67b6Follow-Your-Shape\uff0c\u901a\u8fc7\u8f68\u8ff9\u5dee\u5f02\u56fe\uff08TDM\uff09\u548cKV\u6ce8\u5165\u673a\u5236\uff0c\u5b9e\u73b0\u7cbe\u786e\u53ef\u63a7\u7684\u5f62\u72b6\u7f16\u8f91\uff0c\u5e76\u4fdd\u6301\u975e\u76ee\u6807\u5185\u5bb9\u4e0d\u53d8\u3002", "motivation": "\u73b0\u6709\u6d41\u5f0f\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u5728\u5927\u89c4\u6a21\u5f62\u72b6\u53d8\u6362\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5bb9\u6613\u5f71\u54cd\u975e\u76ee\u6807\u533a\u57df\u3002", "method": "\u63d0\u51faTDM\u5b9a\u4f4d\u53ef\u7f16\u8f91\u533a\u57df\uff0c\u7ed3\u5408KV\u6ce8\u5165\u673a\u5236\u5b9e\u73b0\u7a33\u5b9a\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5f62\u72b6\u66ff\u6362\u4efb\u52a1\u4e2d\u5177\u6709\u4f18\u8d8a\u7684\u7f16\u8f91\u80fd\u529b\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "conclusion": "Follow-Your-Shape\u6846\u67b6\u5728\u5f62\u72b6\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.08136", "pdf": "https://arxiv.org/pdf/2508.08136", "abs": "https://arxiv.org/abs/2508.08136", "authors": ["Yitong Yang", "Yinglin Wang", "Changshuo Wang", "Huajie Wang", "Shuting He"], "title": "FantasyStyle: Controllable Stylized Distillation for 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "The success of 3DGS in generative and editing applications has sparked growing interest in 3DGS-based style transfer. However, current methods still face two major challenges: (1) multi-view inconsistency often leads to style conflicts, resulting in appearance smoothing and distortion; and (2) heavy reliance on VGG features, which struggle to disentangle style and content from style images, often causing content leakage and excessive stylization. To tackle these issues, we introduce \\textbf{FantasyStyle}, a 3DGS-based style transfer framework, and the first to rely entirely on diffusion model distillation. It comprises two key components: (1) \\textbf{Multi-View Frequency Consistency}. We enhance cross-view consistency by applying a 3D filter to multi-view noisy latent, selectively reducing low-frequency components to mitigate stylized prior conflicts. (2) \\textbf{Controllable Stylized Distillation}. To suppress content leakage from style images, we introduce negative guidance to exclude undesired content. In addition, we identify the limitations of Score Distillation Sampling and Delta Denoising Score in 3D style transfer and remove the reconstruction term accordingly. Building on these insights, we propose a controllable stylized distillation that leverages negative guidance to more effectively optimize the 3D Gaussians. Extensive experiments demonstrate that our method consistently outperforms state-of-the-art approaches, achieving higher stylization quality and visual realism across various scenes and styles.", "AI": {"tldr": "FantasyStyle\u662f\u4e00\u4e2a\u57fa\u4e8e3DGS\u7684\u98ce\u683c\u8fc1\u79fb\u6846\u67b6\uff0c\u9996\u6b21\u5b8c\u5168\u4f9d\u8d56\u6269\u6563\u6a21\u578b\u84b8\u998f\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u89d2\u4e0d\u4e00\u81f4\u6027\u548cVGG\u7279\u5f81\u4f9d\u8d56\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u98ce\u683c\u8fc1\u79fb\u8d28\u91cf\u548c\u89c6\u89c9\u771f\u5b9e\u611f\u3002", "motivation": "\u5f53\u524d3DGS\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\u5b58\u5728\u591a\u89c6\u89d2\u4e0d\u4e00\u81f4\u5bfc\u81f4\u98ce\u683c\u51b2\u7a81\uff0c\u4ee5\u53caVGG\u7279\u5f81\u96be\u4ee5\u5206\u79bb\u98ce\u683c\u4e0e\u5185\u5bb9\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faMulti-View Frequency Consistency\u589e\u5f3a\u8de8\u89c6\u89d2\u4e00\u81f4\u6027\uff0cControllable Stylized Distillation\u6291\u5236\u5185\u5bb9\u6cc4\u6f0f\uff0c\u5e76\u4f18\u5316\u4e863D\u9ad8\u65af\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u573a\u666f\u548c\u98ce\u683c\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u98ce\u683c\u8fc1\u79fb\u8d28\u91cf\u548c\u89c6\u89c9\u771f\u5b9e\u611f\u66f4\u9ad8\u3002", "conclusion": "FantasyStyle\u901a\u8fc7\u6269\u6563\u6a21\u578b\u84b8\u998f\u548c\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u98ce\u683c\u8fc1\u79fb\u7684\u6548\u679c\u3002"}}
{"id": "2508.08173", "pdf": "https://arxiv.org/pdf/2508.08173", "abs": "https://arxiv.org/abs/2508.08173", "authors": ["Chongke Bi", "Xin Gao", "Jiangkang Deng", "Guan"], "title": "CD-TVD: Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data", "categories": ["cs.CV", "eess.IV"], "comment": "Time-varying data visualization, deep learning, super-resolution,   diffusion model", "summary": "Large-scale scientific simulations require significant resources to generate high-resolution time-varying data (TVD). While super-resolution is an efficient post-processing strategy to reduce costs, existing methods rely on a large amount of HR training data, limiting their applicability to diverse simulation scenarios. To address this constraint, we proposed CD-TVD, a novel framework that combines contrastive learning and an improved diffusion-based super-resolution model to achieve accurate 3D super-resolution from limited time-step high-resolution data. During pre-training on historical simulation data, the contrastive encoder and diffusion superresolution modules learn degradation patterns and detailed features of high-resolution and low-resolution samples. In the training phase, the improved diffusion model with a local attention mechanism is fine-tuned using only one newly generated high-resolution timestep, leveraging the degradation knowledge learned by the encoder. This design minimizes the reliance on large-scale high-resolution datasets while maintaining the capability to recover fine-grained details. Experimental results on fluid and atmospheric simulation datasets confirm that CD-TVD delivers accurate and resource-efficient 3D super-resolution, marking a significant advancement in data augmentation for large-scale scientific simulations. The code is available at https://github.com/Xin-Gao-private/CD-TVD.", "AI": {"tldr": "CD-TVD\u662f\u4e00\u4e2a\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u6539\u8fdb\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u6709\u9650\u7684\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u4e2d\u5b9e\u73b03D\u8d85\u5206\u8fa8\u7387\uff0c\u51cf\u5c11\u5bf9\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u4f9d\u8d56\u3002", "motivation": "\u5927\u89c4\u6a21\u79d1\u5b66\u6a21\u62df\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u65f6\u53d8\u6570\u636e\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u9ad8\u5206\u8fa8\u7387\u8bad\u7ec3\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u9002\u7528\u6027\u3002", "method": "\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u6539\u8fdb\u7684\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u5b66\u4e60\u9000\u5316\u6a21\u5f0f\u548c\u7ec6\u8282\u7279\u5f81\uff0c\u4ec5\u9700\u4e00\u4e2a\u65b0\u751f\u6210\u7684\u9ad8\u5206\u8fa8\u7387\u65f6\u95f4\u6b65\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u6d41\u4f53\u548c\u5927\u6c14\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cCD-TVD\u5b9e\u73b0\u4e86\u51c6\u786e\u4e14\u8d44\u6e90\u9ad8\u6548\u76843D\u8d85\u5206\u8fa8\u7387\u3002", "conclusion": "CD-TVD\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u79d1\u5b66\u6a21\u62df\u7684\u6570\u636e\u589e\u5f3a\u80fd\u529b\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.08219", "pdf": "https://arxiv.org/pdf/2508.08219", "abs": "https://arxiv.org/abs/2508.08219", "authors": ["Wentao Sun", "Quanyun Wu", "Hanqing Xu", "Kyle Gao", "Zhengsen Xu", "Yiping Chen", "Dedong Zhang", "Lingfei Ma", "John S. Zelek", "Jonathan Li"], "title": "SAGOnline: Segment Any Gaussians Online", "categories": ["cs.CV"], "comment": "19 pages, 10 figures", "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for explicit 3D scene representation, yet achieving efficient and consistent 3D segmentation remains challenging. Current methods suffer from prohibitive computational costs, limited 3D spatial reasoning, and an inability to track multiple objects simultaneously. We present Segment Any Gaussians Online (SAGOnline), a lightweight and zero-shot framework for real-time 3D segmentation in Gaussian scenes that addresses these limitations through two key innovations: (1) a decoupled strategy that integrates video foundation models (e.g., SAM2) for view-consistent 2D mask propagation across synthesized views; and (2) a GPU-accelerated 3D mask generation and Gaussian-level instance labeling algorithm that assigns unique identifiers to 3D primitives, enabling lossless multi-object tracking and segmentation across views. SAGOnline achieves state-of-the-art performance on NVOS (92.7% mIoU) and Spin-NeRF (95.2% mIoU) benchmarks, outperforming Feature3DGS, OmniSeg3D-gs, and SA3D by 15--1500 times in inference speed (27 ms/frame). Qualitative results demonstrate robust multi-object segmentation and tracking in complex scenes. Our contributions include: (i) a lightweight and zero-shot framework for 3D segmentation in Gaussian scenes, (ii) explicit labeling of Gaussian primitives enabling simultaneous segmentation and tracking, and (iii) the effective adaptation of 2D video foundation models to the 3D domain. This work allows real-time rendering and 3D scene understanding, paving the way for practical AR/VR and robotic applications.", "AI": {"tldr": "SAGOnline\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u96f6\u6837\u672c\u7684\u5b9e\u65f63D\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u7b56\u7565\u548cGPU\u52a0\u901f\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e863D\u9ad8\u65af\u573a\u666f\u5206\u5272\u4e2d\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u7a7a\u95f4\u63a8\u7406\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u5f53\u524d3D\u9ad8\u65af\u573a\u666f\u5206\u5272\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u6709\u9650\u4ee5\u53ca\u65e0\u6cd5\u540c\u65f6\u8ddf\u8e2a\u591a\u5bf9\u8c61\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u4e00\u81f4\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "SAGOnline\u7ed3\u5408\u89c6\u9891\u57fa\u7840\u6a21\u578b\uff08\u5982SAM2\uff09\u5b9e\u73b0\u89c6\u56fe\u4e00\u81f4\u76842D\u63a9\u7801\u4f20\u64ad\uff0c\u5e76\u901a\u8fc7GPU\u52a0\u901f\u76843D\u63a9\u7801\u751f\u6210\u548c\u9ad8\u65af\u7ea7\u5b9e\u4f8b\u6807\u6ce8\u7b97\u6cd5\uff0c\u4e3a3D\u57fa\u5143\u5206\u914d\u552f\u4e00\u6807\u8bc6\u7b26\u3002", "result": "\u5728NVOS\u548cSpin-NeRF\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSAGOnline\u5206\u522b\u8fbe\u523092.7%\u548c95.2%\u7684mIoU\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb15-1500\u500d\uff0827\u6beb\u79d2/\u5e27\uff09\u3002", "conclusion": "SAGOnline\u4e3a\u9ad8\u65af\u573a\u666f\u63d0\u4f9b\u4e86\u5b9e\u65f63D\u5206\u5272\u548c\u8ddf\u8e2a\u80fd\u529b\uff0c\u63a8\u52a8\u4e86AR/VR\u548c\u673a\u5668\u4eba\u5e94\u7528\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.08220", "pdf": "https://arxiv.org/pdf/2508.08220", "abs": "https://arxiv.org/abs/2508.08220", "authors": ["Wenyi Mo", "Ying Ba", "Tianyu Zhang", "Yalong Bai", "Biye Li"], "title": "Learning User Preferences for Image Generation Model", "categories": ["cs.CV"], "comment": null, "summary": "User preference prediction requires a comprehensive and accurate understanding of individual tastes. This includes both surface-level attributes, such as color and style, and deeper content-related aspects, such as themes and composition. However, existing methods typically rely on general human preferences or assume static user profiles, often neglecting individual variability and the dynamic, multifaceted nature of personal taste. To address these limitations, we propose an approach built upon Multimodal Large Language Models, introducing contrastive preference loss and preference tokens to learn personalized user preferences from historical interactions. The contrastive preference loss is designed to effectively distinguish between user ''likes'' and ''dislikes'', while the learnable preference tokens capture shared interest representations among existing users, enabling the model to activate group-specific preferences and enhance consistency across similar users. Extensive experiments demonstrate our model outperforms other methods in preference prediction accuracy, effectively identifying users with similar aesthetic inclinations and providing more precise guidance for generating images that align with individual tastes. The project page is \\texttt{https://learn-user-pref.github.io/}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u504f\u597d\u635f\u5931\u548c\u504f\u597d\u6807\u8bb0\u6765\u5b66\u4e60\u4e2a\u6027\u5316\u7528\u6237\u504f\u597d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u504f\u597d\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u901a\u7528\u4eba\u7c7b\u504f\u597d\u6216\u9759\u6001\u7528\u6237\u753b\u50cf\uff0c\u5ffd\u7565\u4e86\u7528\u6237\u504f\u597d\u7684\u52a8\u6001\u6027\u548c\u591a\u6837\u6027\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5f15\u5165\u5bf9\u6bd4\u504f\u597d\u635f\u5931\u548c\u53ef\u5b66\u4e60\u7684\u504f\u597d\u6807\u8bb0\uff0c\u4ee5\u533a\u5206\u7528\u6237\u559c\u597d\u5e76\u6355\u6349\u5171\u4eab\u5174\u8da3\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u504f\u597d\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5e76\u80fd\u6709\u6548\u8bc6\u522b\u5177\u6709\u76f8\u4f3c\u5ba1\u7f8e\u503e\u5411\u7684\u7528\u6237\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u66f4\u7cbe\u786e\u5730\u751f\u6210\u7b26\u5408\u4e2a\u4eba\u53e3\u5473\u7684\u56fe\u50cf\uff0c\u4e3a\u4e2a\u6027\u5316\u63a8\u8350\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u6307\u5bfc\u3002"}}
{"id": "2508.08227", "pdf": "https://arxiv.org/pdf/2508.08227", "abs": "https://arxiv.org/abs/2508.08227", "authors": ["Zhiqiang Wu", "Zhaomang Sun", "Tong Zhou", "Bingtao Fu", "Ji Cong", "Yitong Dong", "Huaqi Zhang", "Xuan Tang", "Mingsong Chen", "Xian Wei"], "title": "OMGSR: You Only Need One Mid-timestep Guidance for Real-World Image Super-Resolution", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Denoising Diffusion Probabilistic Models (DDPM) and Flow Matching (FM) generative models show promising potential for one-step Real-World Image Super-Resolution (Real-ISR). Recent one-step Real-ISR models typically inject a Low-Quality (LQ) image latent distribution at the initial timestep. However, a fundamental gap exists between the LQ image latent distribution and the Gaussian noisy latent distribution, limiting the effective utilization of generative priors. We observe that the noisy latent distribution at DDPM/FM mid-timesteps aligns more closely with the LQ image latent distribution. Based on this insight, we present One Mid-timestep Guidance Real-ISR (OMGSR), a universal framework applicable to DDPM/FM-based generative models. OMGSR injects the LQ image latent distribution at a pre-computed mid-timestep, incorporating the proposed Latent Distribution Refinement loss to alleviate the latent distribution gap. We also design the Overlap-Chunked LPIPS/GAN loss to eliminate checkerboard artifacts in image generation. Within this framework, we instantiate OMGSR for DDPM/FM-based generative models with two variants: OMGSR-S (SD-Turbo) and OMGSR-F (FLUX.1-dev). Experimental results demonstrate that OMGSR-S/F achieves balanced/excellent performance across quantitative and qualitative metrics at 512-resolution. Notably, OMGSR-F establishes overwhelming dominance in all reference metrics. We further train a 1k-resolution OMGSR-F to match the default resolution of FLUX.1-dev, which yields excellent results, especially in the details of the image generation. We also generate 2k-resolution images by the 1k-resolution OMGSR-F using our two-stage Tiled VAE & Diffusion.", "AI": {"tldr": "OMGSR\u662f\u4e00\u79cd\u57fa\u4e8eDDPM/FM\u751f\u6210\u6a21\u578b\u7684\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u4e2d\u65f6\u95f4\u6b65\u6ce8\u5165\u4f4e\u8d28\u91cf\u56fe\u50cf\u6f5c\u5728\u5206\u5e03\uff0c\u5e76\u5f15\u5165\u6f5c\u5728\u5206\u5e03\u7ec6\u5316\u635f\u5931\u548c\u91cd\u53e0\u5206\u5757LPIPS/GAN\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08Real-ISR\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u4e00\u6b65Real-ISR\u6a21\u578b\u5728\u521d\u59cb\u65f6\u95f4\u6b65\u6ce8\u5165\u4f4e\u8d28\u91cf\u56fe\u50cf\u6f5c\u5728\u5206\u5e03\uff0c\u4f46\u5176\u4e0e\u9ad8\u65af\u566a\u58f0\u6f5c\u5728\u5206\u5e03\u5b58\u5728\u5dee\u8ddd\uff0c\u9650\u5236\u4e86\u751f\u6210\u5148\u9a8c\u7684\u6709\u6548\u5229\u7528\u3002\u7814\u7a76\u53d1\u73b0\u4e2d\u65f6\u95f4\u6b65\u7684\u566a\u58f0\u6f5c\u5728\u5206\u5e03\u4e0e\u4f4e\u8d28\u91cf\u56fe\u50cf\u6f5c\u5728\u5206\u5e03\u66f4\u63a5\u8fd1\u3002", "method": "\u63d0\u51faOMGSR\u6846\u67b6\uff0c\u5728\u4e2d\u65f6\u95f4\u6b65\u6ce8\u5165\u4f4e\u8d28\u91cf\u56fe\u50cf\u6f5c\u5728\u5206\u5e03\uff0c\u5f15\u5165\u6f5c\u5728\u5206\u5e03\u7ec6\u5316\u635f\u5931\u4ee5\u51cf\u5c11\u5206\u5e03\u5dee\u8ddd\uff0c\u5e76\u8bbe\u8ba1\u91cd\u53e0\u5206\u5757LPIPS/GAN\u635f\u5931\u6d88\u9664\u68cb\u76d8\u4f2a\u5f71\u3002\u57fa\u4e8eDDPM/FM\u6a21\u578b\u5b9e\u73b0\u4e24\u79cd\u53d8\u4f53\uff1aOMGSR-S\u548cOMGSR-F\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOMGSR-S/F\u5728512\u5206\u8fa8\u7387\u4e0b\u8868\u73b0\u5747\u8861\u4e14\u4f18\u79c0\uff0cOMGSR-F\u5728\u6240\u6709\u53c2\u8003\u6307\u6807\u4e2d\u5360\u636e\u7edd\u5bf9\u4f18\u52bf\u30021k\u5206\u8fa8\u7387\u7684OMGSR-F\u5728\u7ec6\u8282\u751f\u6210\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u901a\u8fc7\u4e24\u9636\u6bb5Tiled VAE & Diffusion\u751f\u62102k\u5206\u8fa8\u7387\u56fe\u50cf\u3002", "conclusion": "OMGSR\u6846\u67b6\u901a\u8fc7\u4f18\u5316\u6f5c\u5728\u5206\u5e03\u6ce8\u5165\u65f6\u95f4\u548c\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e00\u6b65Real-ISR\u7684\u6027\u80fd\uff0c\u4e3a\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.08248", "pdf": "https://arxiv.org/pdf/2508.08248", "abs": "https://arxiv.org/abs/2508.08248", "authors": ["Shuyuan Tu", "Yueming Pan", "Yinming Huang", "Xintong Han", "Zhen Xing", "Qi Dai", "Chong Luo", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Current diffusion models for audio-driven avatar video generation struggle to synthesize long videos with natural audio synchronization and identity consistency. This paper presents StableAvatar, the first end-to-end video diffusion transformer that synthesizes infinite-length high-quality videos without post-processing. Conditioned on a reference image and audio, StableAvatar integrates tailored training and inference modules to enable infinite-length video generation. We observe that the main reason preventing existing models from generating long videos lies in their audio modeling. They typically rely on third-party off-the-shelf extractors to obtain audio embeddings, which are then directly injected into the diffusion model via cross-attention. Since current diffusion backbones lack any audio-related priors, this approach causes severe latent distribution error accumulation across video clips, leading the latent distribution of subsequent segments to drift away from the optimal distribution gradually. To address this, StableAvatar introduces a novel Time-step-aware Audio Adapter that prevents error accumulation via time-step-aware modulation. During inference, we propose a novel Audio Native Guidance Mechanism to further enhance the audio synchronization by leveraging the diffusion's own evolving joint audio-latent prediction as a dynamic guidance signal. To enhance the smoothness of the infinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy that fuses latent over time. Experiments on benchmarks show the effectiveness of StableAvatar both qualitatively and quantitatively.", "AI": {"tldr": "StableAvatar\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u89c6\u9891\u6269\u6563\u53d8\u6362\u5668\uff0c\u80fd\u591f\u751f\u6210\u65e0\u9650\u957f\u5ea6\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u97f3\u9891\u540c\u6b65\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u4e0a\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u97f3\u9891\u9a71\u52a8\u7684\u865a\u62df\u5316\u8eab\u89c6\u9891\u751f\u6210\u6a21\u578b\u96be\u4ee5\u751f\u6210\u957f\u89c6\u9891\uff0c\u4e14\u97f3\u9891\u540c\u6b65\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u8868\u73b0\u4e0d\u4f73\u3002", "method": "StableAvatar\u901a\u8fc7\u65f6\u95f4\u6b65\u611f\u77e5\u97f3\u9891\u9002\u914d\u5668\u548c\u97f3\u9891\u539f\u751f\u5f15\u5bfc\u673a\u5236\u4f18\u5316\u97f3\u9891\u5efa\u6a21\uff0c\u5e76\u91c7\u7528\u52a8\u6001\u52a0\u6743\u6ed1\u52a8\u7a97\u53e3\u7b56\u7565\u589e\u5f3a\u89c6\u9891\u5e73\u6ed1\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eStableAvatar\u5728\u8d28\u91cf\u548c\u6570\u91cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "StableAvatar\u4e3a\u65e0\u9650\u957f\u5ea6\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97f3\u9891\u540c\u6b65\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.08252", "pdf": "https://arxiv.org/pdf/2508.08252", "abs": "https://arxiv.org/abs/2508.08252", "authors": ["Shuting He", "Guangquan Jie", "Changshuo Wang", "Yun Zhou", "Shuming Hu", "Guanbin Li", "Henghui Ding"], "title": "ReferSplat: Referring Segmentation in 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "ICML 2025 Oral, Code: https://github.com/heshuting555/ReferSplat", "summary": "We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task that aims to segment target objects in a 3D Gaussian scene based on natural language descriptions, which often contain spatial relationships or object attributes. This task requires the model to identify newly described objects that may be occluded or not directly visible in a novel view, posing a significant challenge for 3D multi-modal understanding. Developing this capability is crucial for advancing embodied AI. To support research in this area, we construct the first R3DGS dataset, Ref-LERF. Our analysis reveals that 3D multi-modal understanding and spatial relationship modeling are key challenges for R3DGS. To address these challenges, we propose ReferSplat, a framework that explicitly models 3D Gaussian points with natural language expressions in a spatially aware paradigm. ReferSplat achieves state-of-the-art performance on both the newly proposed R3DGS task and 3D open-vocabulary segmentation benchmarks. Dataset and code are available at https://github.com/heshuting555/ReferSplat.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u4efb\u52a1R3DGS\uff0c\u65e8\u5728\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u57283D\u9ad8\u65af\u573a\u666f\u4e2d\u5206\u5272\u76ee\u6807\u5bf9\u8c61\uff0c\u5e76\u6784\u5efa\u4e86\u9996\u4e2a\u6570\u636e\u96c6Ref-LERF\u3002\u63d0\u51fa\u7684ReferSplat\u6846\u67b6\u5728R3DGS\u4efb\u52a1\u548c3D\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63a8\u52a8\u5177\u8eabAI\u7684\u53d1\u5c55\uff0c\u89e3\u51b33D\u591a\u6a21\u6001\u7406\u89e3\u548c\u7a7a\u95f4\u5173\u7cfb\u5efa\u6a21\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faReferSplat\u6846\u67b6\uff0c\u663e\u5f0f\u5efa\u6a213D\u9ad8\u65af\u70b9\u4e0e\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\u7684\u7a7a\u95f4\u611f\u77e5\u8303\u5f0f\u3002", "result": "ReferSplat\u5728R3DGS\u4efb\u52a1\u548c3D\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "R3DGS\u4efb\u52a1\u548cReferSplat\u6846\u67b6\u4e3a3D\u591a\u6a21\u6001\u7406\u89e3\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2402.16868", "pdf": "https://arxiv.org/pdf/2402.16868", "abs": "https://arxiv.org/abs/2402.16868", "authors": ["Peigen Ye", "Yaping Sun", "Shumin Yao", "Hao Chen", "Xiaodong Xu", "Shuguang Cui"], "title": "Codebook-enabled Generative End-to-end Semantic Communication Powered by Transformer", "categories": ["cs.IT", "cs.AI", "cs.CV", "math.IT"], "comment": "IEEE INFOCOM PerAI6G 2024(accepted)", "summary": "Codebook-based generative semantic communication attracts increasing attention, since only indices are required to be transmitted when the codebook is shared between transmitter and receiver. However, due to the fact that the semantic relations among code vectors are not necessarily related to the distance of the corresponding code indices, the performance of the codebook-enabled semantic communication system is susceptible to the channel noise. Thus, how to improve the system robustness against the noise requires careful design. This paper proposes a robust codebook-assisted image semantic communication system, where semantic codec and codebook are first jointly constructed, and then vector-to-index transformer is designed guided by the codebook to eliminate the effects of channel noise, and achieve image generation. Thanks to the assistance of the high-quality codebook to the Transformer, the generated images at the receiver outperform those of the compared methods in terms of visual perception. In the end, numerical results and generated images demonstrate the advantages of the generative semantic communication method over JPEG+LDPC and traditional joint source channel coding (JSCC) methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u8d28\u91cf\u7801\u672c\u7684\u9c81\u68d2\u56fe\u50cf\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u5408\u6784\u5efa\u8bed\u4e49\u7f16\u89e3\u7801\u5668\u548c\u7801\u672c\uff0c\u8bbe\u8ba1\u5411\u91cf\u5230\u7d22\u5f15\u7684\u8f6c\u6362\u5668\u4ee5\u51cf\u5c11\u4fe1\u9053\u566a\u58f0\u5f71\u54cd\uff0c\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u4f18\u4e8e\u5bf9\u6bd4\u65b9\u6cd5\u3002", "motivation": "\u7801\u672c\u751f\u6210\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\u4e2d\uff0c\u7801\u5411\u91cf\u8bed\u4e49\u5173\u7cfb\u4e0e\u7d22\u5f15\u8ddd\u79bb\u65e0\u5173\uff0c\u5bfc\u81f4\u7cfb\u7edf\u6027\u80fd\u6613\u53d7\u4fe1\u9053\u566a\u58f0\u5f71\u54cd\uff0c\u9700\u63d0\u5347\u7cfb\u7edf\u9c81\u68d2\u6027\u3002", "method": "\u8054\u5408\u6784\u5efa\u8bed\u4e49\u7f16\u89e3\u7801\u5668\u548c\u7801\u672c\uff0c\u8bbe\u8ba1\u5411\u91cf\u5230\u7d22\u5f15\u8f6c\u6362\u5668\u4ee5\u51cf\u5c11\u566a\u58f0\u5f71\u54cd\uff0c\u5b9e\u73b0\u56fe\u50cf\u751f\u6210\u3002", "result": "\u63a5\u6536\u7aef\u751f\u6210\u7684\u56fe\u50cf\u5728\u89c6\u89c9\u611f\u77e5\u4e0a\u4f18\u4e8e\u5bf9\u6bd4\u65b9\u6cd5\uff08JPEG+LDPC\u548c\u4f20\u7edfJSCC\uff09\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u751f\u6210\u8bed\u4e49\u901a\u4fe1\u65b9\u6cd5\u5728\u6570\u503c\u7ed3\u679c\u548c\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2508.07263", "pdf": "https://arxiv.org/pdf/2508.07263", "abs": "https://arxiv.org/abs/2508.07263", "authors": ["Qingyuan Zeng", "Shu Jiang", "Jiajing Lin", "Zhenzhong Wang", "Kay Chen Tan", "Min Jiang"], "title": "Fading the Digital Ink: A Universal Black-Box Attack Framework for 3DGS Watermarking Systems", "categories": ["cs.CR", "cs.CV"], "comment": null, "summary": "With the rise of 3D Gaussian Splatting (3DGS), a variety of digital watermarking techniques, embedding either 1D bitstreams or 2D images, are used for copyright protection. However, the robustness of these watermarking techniques against potential attacks remains underexplored. This paper introduces the first universal black-box attack framework, the Group-based Multi-objective Evolutionary Attack (GMEA), designed to challenge these watermarking systems. We formulate the attack as a large-scale multi-objective optimization problem, balancing watermark removal with visual quality. In a black-box setting, we introduce an indirect objective function that blinds the watermark detector by minimizing the standard deviation of features extracted by a convolutional network, thus rendering the feature maps uninformative. To manage the vast search space of 3DGS models, we employ a group-based optimization strategy to partition the model into multiple, independent sub-optimization problems. Experiments demonstrate that our framework effectively removes both 1D and 2D watermarks from mainstream 3DGS watermarking methods while maintaining high visual fidelity. This work reveals critical vulnerabilities in existing 3DGS copyright protection schemes and calls for the development of more robust watermarking systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGMEA\u7684\u9ed1\u76d2\u653b\u51fb\u6846\u67b6\uff0c\u7528\u4e8e\u6311\u62183D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u4e2d\u7684\u6570\u5b57\u6c34\u5370\u6280\u672f\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u5e73\u8861\u6c34\u5370\u79fb\u9664\u4e0e\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u968f\u77403DGS\u7684\u5174\u8d77\uff0c\u6570\u5b57\u6c34\u5370\u6280\u672f\u88ab\u7528\u4e8e\u7248\u6743\u4fdd\u62a4\uff0c\u4f46\u5176\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51faGMEA\u6846\u67b6\uff0c\u5c06\u653b\u51fb\u5efa\u6a21\u4e3a\u5927\u89c4\u6a21\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u91c7\u7528\u95f4\u63a5\u76ee\u6807\u51fd\u6570\u548c\u5206\u7ec4\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGMEA\u80fd\u6709\u6548\u79fb\u9664\u4e3b\u6d413DGS\u6c34\u5370\u65b9\u6cd5\u4e2d\u76841D\u548c2D\u6c34\u5370\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u63ed\u793a\u4e86\u73b0\u67093DGS\u7248\u6743\u4fdd\u62a4\u65b9\u6848\u7684\u8106\u5f31\u6027\uff0c\u547c\u5401\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u6c34\u5370\u7cfb\u7edf\u3002"}}
{"id": "2508.07406", "pdf": "https://arxiv.org/pdf/2508.07406", "abs": "https://arxiv.org/abs/2508.07406", "authors": ["Xiaobei Zhao", "Xingqi Lyu", "Xiang Li"], "title": "AgriVLN: Vision-and-Language Navigation for Agricultural Robots", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Agricultural robots have emerged as powerful members in agricultural tasks, nevertheless, still heavily rely on manual operation or untransportable railway for movement, resulting in limited mobility and poor adaptability. Vision-and-Language Navigation (VLN) enables robots to navigate to the target destinations following natural language instructions, demonstrating strong performance on several domains. However, none of the existing benchmarks or methods is specifically designed for agricultural scenes. To bridge this gap, we propose Agriculture to Agriculture (A2A) benchmark, containing 1,560 episodes across six diverse agricultural scenes, in which all realistic RGB videos are captured by front-facing camera on a quadruped robot at a height of 0.38 meters, aligning with the practical deployment conditions. Meanwhile, we propose Vision-and-Language Navigation for Agricultural Robots (AgriVLN) baseline based on Vision-Language Model (VLM) prompted with carefully crafted templates, which can understand both given instructions and agricultural environments to generate appropriate low-level actions for robot control. When evaluated on A2A, AgriVLN performs well on short instructions but struggles with long instructions, because it often fails to track which part of the instruction is currently being executed. To address this, we further propose Subtask List (STL) instruction decomposition module and integrate it into AgriVLN, improving Success Rate (SR) from 0.33 to 0.47. We additionally compare AgriVLN with several existing VLN methods, demonstrating the state-of-the-art performance in the agricultural domain.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u519c\u4e1a\u573a\u666f\u7684\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bfc\u822a\u57fa\u51c6A2A\u548c\u57fa\u7ebf\u65b9\u6cd5AgriVLN\uff0c\u901a\u8fc7\u5b50\u4efb\u52a1\u5206\u89e3\u6a21\u5757\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u519c\u4e1a\u673a\u5668\u4eba\u5bfc\u822a\u4f9d\u8d56\u4eba\u5de5\u6216\u56fa\u5b9a\u8f68\u9053\u7684\u95ee\u9898\uff0c\u586b\u8865\u519c\u4e1a\u573a\u666fVLN\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u5f00\u53d1A2A\u57fa\u51c6\u548cAgriVLN\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7ed3\u5408VLM\u548c\u5b50\u4efb\u52a1\u5206\u89e3\u6a21\u5757STL\u3002", "result": "AgriVLN\u5728\u77ed\u6307\u4ee4\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u957f\u6307\u4ee4\u901a\u8fc7STL\u63d0\u5347SR\u81f30.47\u3002", "conclusion": "AgriVLN\u5728\u519c\u4e1a\u9886\u57df\u8868\u73b0\u9886\u5148\uff0cSTL\u663e\u8457\u63d0\u5347\u957f\u6307\u4ee4\u5bfc\u822a\u6210\u529f\u7387\u3002"}}
{"id": "2508.07817", "pdf": "https://arxiv.org/pdf/2508.07817", "abs": "https://arxiv.org/abs/2508.07817", "authors": ["Tao Tang", "Chengxu Yang"], "title": "MIND: A Noise-Adaptive Denoising Framework for Medical Images Integrating Multi-Scale Transformer", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "6 pages, 6 figures", "summary": "The core role of medical images in disease diagnosis makes their quality directly affect the accuracy of clinical judgment. However, due to factors such as low-dose scanning, equipment limitations and imaging artifacts, medical images are often accompanied by non-uniform noise interference, which seriously affects structure recognition and lesion detection. This paper proposes a medical image adaptive denoising model (MI-ND) that integrates multi-scale convolutional and Transformer architecture, introduces a noise level estimator (NLE) and a noise adaptive attention module (NAAB), and realizes channel-spatial attention regulation and cross-modal feature fusion driven by noise perception. Systematic testing is carried out on multimodal public datasets. Experiments show that this method significantly outperforms the comparative methods in image quality indicators such as PSNR, SSIM, and LPIPS, and improves the F1 score and ROC-AUC in downstream diagnostic tasks, showing strong prac-tical value and promotional potential. The model has outstanding benefits in structural recovery, diagnostic sensitivity, and cross-modal robustness, and provides an effective solution for medical image enhancement and AI-assisted diagnosis and treatment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u5c3a\u5ea6\u5377\u79ef\u548cTransformer\u67b6\u6784\u7684\u533b\u5b66\u56fe\u50cf\u81ea\u9002\u5e94\u53bb\u566a\u6a21\u578b\uff08MI-ND\uff09\uff0c\u901a\u8fc7\u566a\u58f0\u6c34\u5e73\u4f30\u8ba1\u5668\u548c\u566a\u58f0\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\u548c\u4e0b\u6e38\u8bca\u65ad\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u56e0\u566a\u58f0\u5e72\u6270\u5f71\u54cd\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u89e3\u51b3\u975e\u5747\u5300\u566a\u58f0\u95ee\u9898\u3002", "method": "\u63d0\u51faMI-ND\u6a21\u578b\uff0c\u96c6\u6210\u591a\u5c3a\u5ea6\u5377\u79ef\u548cTransformer\u67b6\u6784\uff0c\u5f15\u5165\u566a\u58f0\u6c34\u5e73\u4f30\u8ba1\u5668\uff08NLE\uff09\u548c\u566a\u58f0\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u6a21\u5757\uff08NAAB\uff09\uff0c\u5b9e\u73b0\u566a\u58f0\u611f\u77e5\u9a71\u52a8\u7684\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728PSNR\u3001SSIM\u7b49\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u5bf9\u6bd4\u65b9\u6cd5\uff0c\u4e0b\u6e38\u8bca\u65ad\u4efb\u52a1\u7684F1\u5206\u6570\u548cROC-AUC\u4e5f\u6709\u6240\u63d0\u5347\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u7ed3\u6784\u6062\u590d\u3001\u8bca\u65ad\u654f\u611f\u6027\u548c\u8de8\u6a21\u6001\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u589e\u5f3a\u548cAI\u8f85\u52a9\u8bca\u7597\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
