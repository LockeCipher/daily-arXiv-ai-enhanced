{"id": "2509.15249", "pdf": "https://arxiv.org/pdf/2509.15249", "abs": "https://arxiv.org/abs/2509.15249", "authors": ["Shen Chen", "Ruiyu Zhao", "Jiale Zhou", "Zongkai Wu", "Jenq-Neng Hwang", "Lei Li"], "title": "Causal Reasoning Elicits Controllable 3D Scene Generation", "categories": ["cs.GR", "cs.AI"], "comment": null, "summary": "Existing 3D scene generation methods often struggle to model the complex logical dependencies and physical constraints between objects, limiting their ability to adapt to dynamic and realistic environments. We propose CausalStruct, a novel framework that embeds causal reasoning into 3D scene generation. Utilizing large language models (LLMs), We construct causal graphs where nodes represent objects and attributes, while edges encode causal dependencies and physical constraints. CausalStruct iteratively refines the scene layout by enforcing causal order to determine the placement order of objects and applies causal intervention to adjust the spatial configuration according to physics-driven constraints, ensuring consistency with textual descriptions and real-world dynamics. The refined scene causal graph informs subsequent optimization steps, employing a Proportional-Integral-Derivative(PID) controller to iteratively tune object scales and positions. Our method uses text or images to guide object placement and layout in 3D scenes, with 3D Gaussian Splatting and Score Distillation Sampling improving shape accuracy and rendering stability. Extensive experiments show that CausalStruct generates 3D scenes with enhanced logical coherence, realistic spatial interactions, and robust adaptability.", "AI": {"tldr": "CausalStruct\u662f\u4e00\u4e2a\u5c06\u56e0\u679c\u63a8\u7406\u5d4c\u51653D\u573a\u666f\u751f\u6210\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u56e0\u679c\u56fe\u6765\u5efa\u6a21\u5bf9\u8c61\u95f4\u7684\u903b\u8f91\u4f9d\u8d56\u548c\u7269\u7406\u7ea6\u675f\uff0c\u63d0\u5347\u573a\u666f\u7684\u903b\u8f91\u4e00\u81f4\u6027\u548c\u771f\u5b9e\u611f\u3002", "motivation": "\u73b0\u67093D\u573a\u666f\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u5efa\u6a21\u5bf9\u8c61\u95f4\u590d\u6742\u7684\u903b\u8f91\u4f9d\u8d56\u548c\u7269\u7406\u7ea6\u675f\uff0c\u9650\u5236\u4e86\u5728\u52a8\u6001\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u56e0\u679c\u56fe\uff0c\u8282\u70b9\u8868\u793a\u5bf9\u8c61\u548c\u5c5e\u6027\uff0c\u8fb9\u7f16\u7801\u56e0\u679c\u4f9d\u8d56\u548c\u7269\u7406\u7ea6\u675f\uff1b\u901a\u8fc7\u56e0\u679c\u987a\u5e8f\u786e\u5b9a\u5bf9\u8c61\u653e\u7f6e\u987a\u5e8f\uff0c\u56e0\u679c\u5e72\u9884\u8c03\u6574\u7a7a\u95f4\u914d\u7f6e\uff1b\u4f7f\u7528PID\u63a7\u5236\u5668\u4f18\u5316\u5bf9\u8c61\u5c3a\u5ea6\u548c\u4f4d\u7f6e\uff0c\u7ed3\u54083D\u9ad8\u65af\u6cfc\u6e85\u548c\u5206\u6570\u84b8\u998f\u91c7\u6837\u63d0\u5347\u5f62\u72b6\u7cbe\u5ea6\u548c\u6e32\u67d3\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCausalStruct\u751f\u6210\u76843D\u573a\u666f\u5177\u6709\u589e\u5f3a\u7684\u903b\u8f91\u4e00\u81f4\u6027\u3001\u771f\u5b9e\u7684\u7a7a\u95f4\u4ea4\u4e92\u548c\u9c81\u68d2\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5c06\u56e0\u679c\u63a8\u7406\u878d\u51653D\u573a\u666f\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u573a\u666f\u7684\u903b\u8f91\u8fde\u8d2f\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u3002"}}
{"id": "2509.15242", "pdf": "https://arxiv.org/pdf/2509.15242", "abs": "https://arxiv.org/abs/2509.15242", "authors": ["Jaydeep Rade", "Md Hasibul Hasan Hasib", "Meric Ozturk", "Baboucarr Faal", "Sheng Yang", "Dipali G. Sashital", "Vincenzo Venditti", "Baoyu Chen", "Soumik Sarkar", "Adarsh Krishnamurthy", "Anwesha Sarkar"], "title": "ProFusion: 3D Reconstruction of Protein Complex Structures from Multi-view AFM Images", "categories": ["cs.CV"], "comment": null, "summary": "AI-based in silico methods have improved protein structure prediction but often struggle with large protein complexes (PCs) involving multiple interacting proteins due to missing 3D spatial cues. Experimental techniques like Cryo-EM are accurate but costly and time-consuming. We present ProFusion, a hybrid framework that integrates a deep learning model with Atomic Force Microscopy (AFM), which provides high-resolution height maps from random orientations, naturally yielding multi-view data for 3D reconstruction. However, generating a large-scale AFM imaging data set sufficient to train deep learning models is impractical. Therefore, we developed a virtual AFM framework that simulates the imaging process and generated a dataset of ~542,000 proteins with multi-view synthetic AFM images. We train a conditional diffusion model to synthesize novel views from unposed inputs and an instance-specific Neural Radiance Field (NeRF) model to reconstruct 3D structures. Our reconstructed 3D protein structures achieve an average Chamfer Distance within the AFM imaging resolution, reflecting high structural fidelity. Our method is extensively validated on experimental AFM images of various PCs, demonstrating strong potential for accurate, cost-effective protein complex structure prediction and rapid iterative validation using AFM experiments.", "AI": {"tldr": "ProFusion\u662f\u4e00\u4e2a\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u539f\u5b50\u529b\u663e\u5fae\u955c\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u5927\u578b\u86cb\u767d\u8d28\u590d\u5408\u7269\u76843D\u7ed3\u6784\u3002\u5b83\u901a\u8fc7\u865a\u62dfAFM\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u6761\u4ef6\u6269\u6563\u6a21\u578b\u548cNeRF\u6a21\u578b\u8fdb\u884c3D\u91cd\u5efa\u3002", "motivation": "AI\u65b9\u6cd5\u5728\u9884\u6d4b\u5927\u578b\u86cb\u767d\u8d28\u590d\u5408\u7269\u7ed3\u6784\u65f6\u56e0\u7f3a\u4e4f3D\u7a7a\u95f4\u7ebf\u7d22\u800c\u56f0\u96be\uff0c\u5b9e\u9a8c\u65b9\u6cd5\u5982Cryo-EM\u51c6\u786e\u4f46\u6210\u672c\u9ad8\u3001\u8017\u65f6\u3002\u9700\u8981\u5f00\u53d1\u51c6\u786e\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u86cb\u767d\u8d28\u590d\u5408\u7269\u7ed3\u6784\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u865a\u62dfAFM\u6846\u67b6\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5408\u6210\u65b0\u89c6\u89d2\u56fe\u50cf\uff0c\u4f7f\u7528\u5b9e\u4f8b\u7279\u5b9aNeRF\u6a21\u578b\u8fdb\u884c3D\u91cd\u5efa\u3002", "result": "\u91cd\u5efa\u76843D\u86cb\u767d\u8d28\u7ed3\u6784\u5728AFM\u6210\u50cf\u5206\u8fa8\u7387\u5185\u8fbe\u5230\u5e73\u5747Chamfer\u8ddd\u79bb\uff0c\u663e\u793a\u9ad8\u7ed3\u6784\u4fdd\u771f\u5ea6\u3002\u5728\u5b9e\u9a8cAFM\u56fe\u50cf\u4e0a\u5e7f\u6cdb\u9a8c\u8bc1\uff0c\u8bc1\u660e\u51c6\u786e\u9884\u6d4b\u86cb\u767d\u8d28\u590d\u5408\u7269\u7ed3\u6784\u7684\u6f5c\u529b\u3002", "conclusion": "ProFusion\u5c55\u793a\u4e86\u4f7f\u7528AFM\u5b9e\u9a8c\u8fdb\u884c\u51c6\u786e\u3001\u6210\u672c\u6548\u76ca\u9ad8\u7684\u86cb\u767d\u8d28\u590d\u5408\u7269\u7ed3\u6784\u9884\u6d4b\u548c\u5feb\u901f\u8fed\u4ee3\u9a8c\u8bc1\u7684\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.16064", "pdf": "https://arxiv.org/pdf/2509.16064", "abs": "https://arxiv.org/abs/2509.16064", "authors": ["Purvi Goel", "Guy Tevet", "C. K. Liu", "Kayvon Fatahalian"], "title": "Generating Detailed Character Motion from Blocking Poses", "categories": ["cs.GR"], "comment": null, "summary": "We focus on the problem of using generative diffusion models for the task of motion detailing: converting a rough version of a character animation, represented by a sparse set of coarsely posed, and imprecisely timed blocking poses, into a detailed, natural looking character animation. Current diffusion models can address the problem of correcting the timing of imprecisely timed poses, but we find that no good solution exists for leveraging the diffusion prior to enhance a sparse set of blocking poses with additional pose detail. We overcome this challenge using a simple inference-time trick. At certain diffusion steps, we blend the outputs of an unconditioned diffusion model with input blocking pose constraints using per-blocking-pose tolerance weights, and pass this result in as the input condition to an pre-existing motion retiming model. We find this approach works significantly better than existing approaches that attempt to add detail by blending model outputs or via expressing blocking pose constraints as guidance. The result is the first diffusion model that can robustly convert blocking-level poses into plausible detailed character animations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u751f\u6210\u6269\u6563\u6a21\u578b\u8fdb\u884c\u8fd0\u52a8\u7ec6\u8282\u5316\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5c06\u7c97\u7565\u7684\u52a8\u753b\u59ff\u52bf\u8f6c\u6362\u4e3a\u8be6\u7ec6\u81ea\u7136\u7684\u89d2\u8272\u52a8\u753b\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u89e3\u51b3\u4e0d\u7cbe\u786e\u65f6\u95f4\u59ff\u52bf\u7684\u4fee\u6b63\u95ee\u9898\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u65b9\u6cd5\u5229\u7528\u6269\u6563\u5148\u9a8c\u6765\u589e\u5f3a\u7a00\u758f\u7684\u963b\u585e\u59ff\u52bf\u7ec6\u8282\u3002", "method": "\u91c7\u7528\u63a8\u7406\u65f6\u6280\u5de7\uff1a\u5728\u67d0\u4e9b\u6269\u6563\u6b65\u9aa4\u4e2d\uff0c\u5c06\u65e0\u6761\u4ef6\u6269\u6563\u6a21\u578b\u8f93\u51fa\u4e0e\u8f93\u5165\u963b\u585e\u59ff\u52bf\u7ea6\u675f\u6309\u6bcf\u4e2a\u59ff\u52bf\u7684\u5bb9\u5fcd\u6743\u91cd\u6df7\u5408\uff0c\u7136\u540e\u5c06\u7ed3\u679c\u4f5c\u4e3a\u8f93\u5165\u6761\u4ef6\u4f20\u9012\u7ed9\u73b0\u6709\u7684\u8fd0\u52a8\u91cd\u5b9a\u65f6\u6a21\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u901a\u8fc7\u6df7\u5408\u6a21\u578b\u8f93\u51fa\u6216\u901a\u8fc7\u8868\u8fbe\u963b\u585e\u59ff\u52bf\u7ea6\u675f\u4f5c\u4e3a\u6307\u5bfc\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u7a33\u5065\u5730\u5c06\u963b\u585e\u7ea7\u59ff\u52bf\u8f6c\u6362\u4e3a\u5408\u7406\u7684\u8be6\u7ec6\u89d2\u8272\u52a8\u753b\u3002", "conclusion": "\u8fd9\u662f\u7b2c\u4e00\u4e2a\u80fd\u591f\u7a33\u5065\u5730\u5c06\u963b\u585e\u7ea7\u59ff\u52bf\u8f6c\u6362\u4e3a\u5408\u7406\u8be6\u7ec6\u89d2\u8272\u52a8\u753b\u7684\u6269\u6563\u6a21\u578b\uff0c\u4e3a\u52a8\u753b\u5236\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u5316\u5de5\u5177\u3002"}}
{"id": "2509.15257", "pdf": "https://arxiv.org/pdf/2509.15257", "abs": "https://arxiv.org/abs/2509.15257", "authors": ["Silpa Vadakkeeveetil Sreelatha", "Sauradip Nag", "Muhammad Awais", "Serge Belongie", "Anjan Dutta"], "title": "RespoDiff: Dual-Module Bottleneck Transformation for Responsible & Faithful T2I Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The rapid advancement of diffusion models has enabled high-fidelity and semantically rich text-to-image generation; however, ensuring fairness and safety remains an open challenge. Existing methods typically improve fairness and safety at the expense of semantic fidelity and image quality. In this work, we propose RespoDiff, a novel framework for responsible text-to-image generation that incorporates a dual-module transformation on the intermediate bottleneck representations of diffusion models. Our approach introduces two distinct learnable modules: one focused on capturing and enforcing responsible concepts, such as fairness and safety, and the other dedicated to maintaining semantic alignment with neutral prompts. To facilitate the dual learning process, we introduce a novel score-matching objective that enables effective coordination between the modules. Our method outperforms state-of-the-art methods in responsible generation by ensuring semantic alignment while optimizing both objectives without compromising image fidelity. Our approach improves responsible and semantically coherent generation by 20% across diverse, unseen prompts. Moreover, it integrates seamlessly into large-scale models like SDXL, enhancing fairness and safety. Code will be released upon acceptance.", "AI": {"tldr": "RespoDiff\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u6269\u6563\u6a21\u578b\u7684\u4e2d\u95f4\u74f6\u9888\u8868\u793a\u4e0a\u5f15\u5165\u53cc\u6a21\u5757\u8f6c\u6362\uff0c\u540c\u65f6\u786e\u4fdd\u516c\u5e73\u6027\u3001\u5b89\u5168\u6027\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u63d0\u9ad8\u516c\u5e73\u6027\u548c\u5b89\u5168\u6027\u65f6\u5f80\u5f80\u727a\u7272\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u56fe\u50cf\u8d28\u91cf\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u4f18\u5316\u8fd9\u4e24\u4e2a\u76ee\u6807\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u53cc\u6a21\u5757\u5b66\u4e60\u65b9\u6cd5\uff1a\u4e00\u4e2a\u6a21\u5757\u4e13\u6ce8\u4e8e\u6355\u83b7\u548c\u6267\u884c\u8d1f\u8d23\u4efb\u6982\u5ff5\uff08\u516c\u5e73\u6027\u548c\u5b89\u5168\u6027\uff09\uff0c\u53e6\u4e00\u4e2a\u6a21\u5757\u81f4\u529b\u4e8e\u4fdd\u6301\u4e0e\u4e2d\u6027\u63d0\u793a\u7684\u8bed\u4e49\u5bf9\u9f50\u3002\u5f15\u5165\u65b0\u7684\u5206\u6570\u5339\u914d\u76ee\u6807\u6765\u534f\u8c03\u4e24\u4e2a\u6a21\u5757\u3002", "result": "\u5728\u8d1f\u8d23\u4efb\u751f\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u786e\u4fdd\u8bed\u4e49\u5bf9\u9f50\u7684\u540c\u65f6\u4e0d\u727a\u7272\u56fe\u50cf\u4fdd\u771f\u5ea6\u3002\u5728\u591a\u6837\u5316\u672a\u89c1\u63d0\u793a\u4e0a\uff0c\u8d1f\u8d23\u4efb\u548c\u8bed\u4e49\u4e00\u81f4\u7684\u751f\u6210\u63d0\u9ad8\u4e8620%\u3002\u53ef\u65e0\u7f1d\u96c6\u6210\u5230SDXL\u7b49\u5927\u89c4\u6a21\u6a21\u578b\u4e2d\u3002", "conclusion": "RespoDiff\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u516c\u5e73\u6027\u548c\u5b89\u5168\u6027\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u56fe\u50cf\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2509.15267", "pdf": "https://arxiv.org/pdf/2509.15267", "abs": "https://arxiv.org/abs/2509.15267", "authors": ["Valeria Pais", "Luis Oala", "Daniele Faccio", "Marco Aversa"], "title": "Autoguided Online Data Curation for Diffusion Model Training", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted non-archival paper at ICCV 2025 Workshop on Curated Data for   Efficient Learning (CDEL)", "summary": "The costs of generative model compute rekindled promises and hopes for efficient data curation. In this work, we investigate whether recently developed autoguidance and online data selection methods can improve the time and sample efficiency of training generative diffusion models. We integrate joint example selection (JEST) and autoguidance into a unified code base for fast ablation and benchmarking. We evaluate combinations of data curation on a controlled 2-D synthetic data generation task as well as (3x64x64)-D image generation. Our comparisons are made at equal wall-clock time and equal number of samples, explicitly accounting for the overhead of selection. Across experiments, autoguidance consistently improves sample quality and diversity. Early AJEST (applying selection only at the beginning of training) can match or modestly exceed autoguidance alone in data efficiency on both tasks. However, its time overhead and added complexity make autoguidance or uniform random data selection preferable in most situations. These findings suggest that while targeted online selection can yield efficiency gains in early training, robust sample quality improvements are primarily driven by autoguidance. We discuss limitations and scope, and outline when data selection may be beneficial.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u81ea\u52a8\u5f15\u5bfc\u548c\u5728\u7ebf\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u662f\u5426\u80fd\u63d0\u9ad8\u751f\u6210\u6269\u6563\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\uff0c\u901a\u8fc7\u6574\u5408JEST\u548c\u81ea\u52a8\u5f15\u5bfc\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u53d1\u73b0\u81ea\u52a8\u5f15\u5bfc\u80fd\u6301\u7eed\u63d0\u5347\u6837\u672c\u8d28\u91cf\u548c\u591a\u6837\u6027\uff0c\u800c\u65e9\u671f\u6570\u636e\u9009\u62e9\u4ec5\u5728\u8bad\u7ec3\u521d\u671f\u6709\u4f18\u52bf\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u7684\u8ba1\u7b97\u6210\u672c\u91cd\u65b0\u6fc0\u53d1\u4e86\u9ad8\u6548\u6570\u636e\u7b5b\u9009\u7684\u6f5c\u529b\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u6700\u65b0\u7684\u81ea\u52a8\u5f15\u5bfc\u548c\u5728\u7ebf\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u662f\u5426\u80fd\u6539\u5584\u751f\u6210\u6269\u6563\u6a21\u578b\u7684\u65f6\u95f4\u548c\u6837\u672c\u6548\u7387\u3002", "method": "\u5c06\u8054\u5408\u793a\u4f8b\u9009\u62e9\uff08JEST\uff09\u548c\u81ea\u52a8\u5f15\u5bfc\u6574\u5408\u5230\u7edf\u4e00\u4ee3\u7801\u5e93\u4e2d\uff0c\u57282D\u5408\u6210\u6570\u636e\u751f\u6210\u548c3x64x64\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e0a\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u8003\u8651\u9009\u62e9\u5f00\u9500\u7684\u5f71\u54cd\u3002", "result": "\u81ea\u52a8\u5f15\u5bfc\u5728\u6240\u6709\u5b9e\u9a8c\u4e2d\u90fd\u80fd\u6301\u7eed\u63d0\u5347\u6837\u672c\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002\u65e9\u671fAJEST\u5728\u6570\u636e\u6548\u7387\u4e0a\u80fd\u5339\u914d\u6216\u7565\u5fae\u8d85\u8fc7\u5355\u72ec\u4f7f\u7528\u81ea\u52a8\u5f15\u5bfc\uff0c\u4f46\u5176\u65f6\u95f4\u5f00\u9500\u548c\u590d\u6742\u6027\u4f7f\u5f97\u81ea\u52a8\u5f15\u5bfc\u6216\u5747\u5300\u968f\u673a\u6570\u636e\u9009\u62e9\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u66f4\u4f18\u3002", "conclusion": "\u867d\u7136\u6709\u9488\u5bf9\u6027\u7684\u5728\u7ebf\u9009\u62e9\u5728\u65e9\u671f\u8bad\u7ec3\u4e2d\u80fd\u5e26\u6765\u6548\u7387\u63d0\u5347\uff0c\u4f46\u7a33\u5065\u7684\u6837\u672c\u8d28\u91cf\u6539\u8fdb\u4e3b\u8981\u7531\u81ea\u52a8\u5f15\u5bfc\u9a71\u52a8\u3002\u6570\u636e\u9009\u62e9\u4ec5\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u6709\u76ca\u3002"}}
{"id": "2509.15270", "pdf": "https://arxiv.org/pdf/2509.15270", "abs": "https://arxiv.org/abs/2509.15270", "authors": ["Emanuele Ricco", "Elia Onofri", "Lorenzo Cima", "Stefano Cresci", "Roberto Di Pietro"], "title": "PRISM: Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting AI-generated images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "A critical need has emerged for generative AI: attribution methods. That is, solutions that can identify the model originating AI-generated content. This feature, generally relevant in multimodal applications, is especially sensitive in commercial settings where users subscribe to paid proprietary services and expect guarantees about the source of the content they receive. To address these issues, we introduce PRISM, a scalable Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting AI-generated images. PRISM is based on a radial reduction of the discrete Fourier transform that leverages amplitude and phase information to capture model-specific signatures. The output of the above process is subsequently clustered via linear discriminant analysis to achieve reliable model attribution in diverse settings, even if the model's internal details are inaccessible. To support our work, we construct PRISM-36K, a novel dataset of 36,000 images generated by six text-to-image GAN- and diffusion-based models. On this dataset, PRISM achieves an attribution accuracy of 92.04%. We additionally evaluate our method on four benchmarks from the literature, reaching an average accuracy of 81.60%. Finally, we evaluate our methodology also in the binary task of detecting real vs fake images, achieving an average accuracy of 88.41%. We obtain our best result on GenImage with an accuracy of 95.06%, whereas the original benchmark achieved 82.20%. Our results demonstrate the effectiveness of frequency-domain fingerprinting for cross-architecture and cross-dataset model attribution, offering a viable solution for enforcing accountability and trust in generative AI systems.", "AI": {"tldr": "PRISM\u662f\u4e00\u4e2a\u57fa\u4e8e\u5085\u91cc\u53f6\u53d8\u6362\u7684\u56fe\u50cf\u6307\u7eb9\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522bAI\u751f\u6210\u56fe\u50cf\u7684\u6765\u6e90\u6a21\u578b\uff0c\u5728\u5546\u4e1a\u73af\u5883\u4e2d\u786e\u4fdd\u5185\u5bb9\u6765\u6e90\u7684\u53ef\u8ffd\u6eaf\u6027\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u7684\u666e\u53ca\uff0c\u5546\u4e1a\u73af\u5883\u4e2d\u9700\u8981\u80fd\u591f\u8bc6\u522bAI\u751f\u6210\u5185\u5bb9\u6765\u6e90\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u4ed8\u8d39\u7528\u6237\u671f\u671b\u83b7\u5f97\u5185\u5bb9\u6765\u6e90\u7684\u4fdd\u8bc1\u3002", "method": "PRISM\u91c7\u7528\u76f8\u4f4d\u589e\u5f3a\u7684\u5f84\u5411\u56fe\u50cf\u7b7e\u540d\u6620\u5c04\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\u7684\u5f84\u5411\u7f29\u51cf\uff0c\u5229\u7528\u632f\u5e45\u548c\u76f8\u4f4d\u4fe1\u606f\u6355\u6349\u6a21\u578b\u7279\u5b9a\u7b7e\u540d\uff0c\u7136\u540e\u901a\u8fc7\u7ebf\u6027\u5224\u522b\u5206\u6790\u8fdb\u884c\u805a\u7c7b\u3002", "result": "\u5728\u81ea\u5efa\u6570\u636e\u96c6PRISM-36K\u4e0a\u8fbe\u523092.04%\u7684\u5f52\u56e0\u51c6\u786e\u7387\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u51c6\u786e\u7387\u4e3a81.60%\uff0c\u5728\u771f\u5047\u56fe\u50cf\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5e73\u5747\u51c6\u786e\u7387\u4e3a88.41%\uff0c\u5728GenImage\u4e0a\u8fbe\u523095.06%\u3002", "conclusion": "\u9891\u57df\u6307\u7eb9\u8bc6\u522b\u5728\u8de8\u67b6\u6784\u548c\u8de8\u6570\u636e\u96c6\u6a21\u578b\u5f52\u56e0\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u4e3a\u751f\u6210\u5f0fAI\u7cfb\u7edf\u7684\u95ee\u8d23\u548c\u4fe1\u4efb\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.15342", "pdf": "https://arxiv.org/pdf/2509.15342", "abs": "https://arxiv.org/abs/2509.15342", "authors": ["Jiuyi Xu", "Qing Jin", "Meida Chen", "Andrew Feng", "Yang Sui", "Yangming Shi"], "title": "LowDiff: Efficient Diffusion Sampling with Low-Resolution Condition", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have achieved remarkable success in image generation but their practical application is often hindered by the slow sampling speed. Prior efforts of improving efficiency primarily focus on compressing models or reducing the total number of denoising steps, largely neglecting the possibility to leverage multiple input resolutions in the generation process. In this work, we propose LowDiff, a novel and efficient diffusion framework based on a cascaded approach by generating increasingly higher resolution outputs. Besides, LowDiff employs a unified model to progressively refine images from low resolution to the desired resolution. With the proposed architecture design and generation techniques, we achieve comparable or even superior performance with much fewer high-resolution sampling steps. LowDiff is applicable to diffusion models in both pixel space and latent space. Extensive experiments on both conditional and unconditional generation tasks across CIFAR-10, FFHQ and ImageNet demonstrate the effectiveness and generality of our method. Results show over 50% throughput improvement across all datasets and settings while maintaining comparable or better quality. On unconditional CIFAR-10, LowDiff achieves an FID of 2.11 and IS of 9.87, while on conditional CIFAR-10, an FID of 1.94 and IS of 10.03. On FFHQ 64x64, LowDiff achieves an FID of 2.43, and on ImageNet 256x256, LowDiff built on LightningDiT-B/1 produces high-quality samples with a FID of 4.00 and an IS of 195.06, together with substantial efficiency gains.", "AI": {"tldr": "LowDiff\u662f\u4e00\u79cd\u57fa\u4e8e\u7ea7\u8054\u65b9\u6cd5\u7684\u6269\u6563\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u4f4e\u5206\u8fa8\u7387\u5230\u9ad8\u5206\u8fa8\u7387\u7684\u6e10\u8fdb\u5f0f\u751f\u6210\u6765\u63d0\u9ad8\u91c7\u6837\u6548\u7387\uff0c\u5728\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u9ad8\u5206\u8fa8\u7387\u91c7\u6837\u6b65\u9aa4\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u53d7\u5230\u91c7\u6837\u901f\u5ea6\u6162\u7684\u9650\u5236\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u538b\u7f29\u6216\u51cf\u5c11\u53bb\u566a\u6b65\u9aa4\uff0c\u800c\u5ffd\u7565\u4e86\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5229\u7528\u591a\u5206\u8fa8\u7387\u8f93\u5165\u7684\u53ef\u80fd\u6027\u3002", "method": "\u63d0\u51faLowDiff\u6846\u67b6\uff0c\u91c7\u7528\u7ea7\u8054\u65b9\u6cd5\u9010\u6b65\u751f\u6210\u66f4\u9ad8\u5206\u8fa8\u7387\u7684\u8f93\u51fa\uff0c\u4f7f\u7528\u7edf\u4e00\u6a21\u578b\u4ece\u4f4e\u5206\u8fa8\u7387\u9010\u6b65\u7ec6\u5316\u5230\u76ee\u6807\u5206\u8fa8\u7387\uff0c\u9002\u7528\u4e8e\u50cf\u7d20\u7a7a\u95f4\u548c\u6f5c\u5728\u7a7a\u95f4\u7684\u6269\u6563\u6a21\u578b\u3002", "result": "\u5728CIFAR-10\u3001FFHQ\u548cImageNet\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u6570\u636e\u96c6\u548c\u8bbe\u7f6e\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc750%\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u6216\u66f4\u597d\u7684\u8d28\u91cf\u3002\u5177\u4f53\u6307\u6807\u5305\u62ec\uff1a\u65e0\u6761\u4ef6CIFAR-10 FID 2.11\uff0cIS 9.87\uff1b\u6761\u4ef6CIFAR-10 FID 1.94\uff0cIS 10.03\uff1bFFHQ 64x64 FID 2.43\uff1bImageNet 256x256 FID 4.00\uff0cIS 195.06\u3002", "conclusion": "LowDiff\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u8bbe\u8ba1\u548c\u751f\u6210\u6280\u672f\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u6548\u7387\uff0c\u8bc1\u660e\u4e86\u5229\u7528\u591a\u5206\u8fa8\u7387\u751f\u6210\u7b56\u7565\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2509.15357", "pdf": "https://arxiv.org/pdf/2509.15357", "abs": "https://arxiv.org/abs/2509.15357", "authors": ["Yu Chang", "Jiahao Chen", "Anzhe Cheng", "Paul Bogdan"], "title": "MaskAttn-SDXL: Controllable Region-Level Text-To-Image Generation", "categories": ["cs.CV", "cs.LG"], "comment": "Submitted to ICASSP 2026", "summary": "Text-to-image diffusion models achieve impressive realism but often suffer from compositional failures on prompts with multiple objects, attributes, and spatial relations, resulting in cross-token interference where entities entangle, attributes mix across objects, and spatial cues are violated. To address these failures, we propose MaskAttn-SDXL,a region-level gating mechanism applied to the cross-attention logits of Stable Diffusion XL(SDXL)'s UNet. MaskAttn-SDXL learns a binary mask per layer, injecting it into each cross-attention logit map before softmax to sparsify token-to-latent interactions so that only semantically relevant connections remain active. The method requires no positional encodings, auxiliary tokens, or external region masks, and preserves the original inference path with negligible overhead. In practice, our model improves spatial compliance and attribute binding in multi-object prompts while preserving overall image quality and diversity. These findings demonstrate that logit-level maksed cross-attention is an data-efficient primitve for enforcing compositional control, and our method thus serves as a practical extension for spatial control in text-to-image generation.", "AI": {"tldr": "\u63d0\u51faMaskAttn-SDXL\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728SDXL\u7684\u4ea4\u53c9\u6ce8\u610f\u529blogits\u4e2d\u6ce8\u5165\u4e8c\u8fdb\u5236\u63a9\u7801\u6765\u7a00\u758f\u5316token\u4e0e\u6f5c\u5728\u7a7a\u95f4\u4ea4\u4e92\uff0c\u89e3\u51b3\u591a\u5bf9\u8c61\u63d0\u793a\u4e2d\u7684\u7ec4\u5408\u5931\u8d25\u95ee\u9898", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u591a\u5bf9\u8c61\u3001\u5c5e\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\u7684\u63d0\u793a\u4e0a\u7ecf\u5e38\u51fa\u73b0\u7ec4\u5408\u5931\u8d25\uff0c\u5bfc\u81f4\u5b9e\u4f53\u7ea0\u7f20\u3001\u5c5e\u6027\u6df7\u6dc6\u548c\u7a7a\u95f4\u7ebf\u7d22\u8fdd\u53cd", "method": "\u5728SDXL UNet\u7684\u4ea4\u53c9\u6ce8\u610f\u529blogits\u4e0a\u5e94\u7528\u533a\u57df\u7ea7\u95e8\u63a7\u673a\u5236\uff0c\u4e3a\u6bcf\u5c42\u5b66\u4e60\u4e8c\u8fdb\u5236\u63a9\u7801\uff0c\u5728softmax\u524d\u6ce8\u5165\u5230\u4ea4\u53c9\u6ce8\u610f\u529blogit\u56fe\u4e2d\uff0c\u4ec5\u4fdd\u7559\u8bed\u4e49\u76f8\u5173\u7684\u8fde\u63a5", "result": "\u6a21\u578b\u63d0\u9ad8\u4e86\u591a\u5bf9\u8c61\u63d0\u793a\u4e2d\u7684\u7a7a\u95f4\u5408\u89c4\u6027\u548c\u5c5e\u6027\u7ed1\u5b9a\uff0c\u540c\u65f6\u4fdd\u6301\u6574\u4f53\u56fe\u50cf\u8d28\u91cf\u548c\u591a\u6837\u6027", "conclusion": "logit\u7ea7\u522b\u7684\u63a9\u7801\u4ea4\u53c9\u6ce8\u610f\u529b\u662f\u5b9e\u65bd\u7ec4\u5408\u63a7\u5236\u7684\u6570\u636e\u9ad8\u6548\u539f\u8bed\uff0c\u8be5\u65b9\u6cd5\u53ef\u4f5c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7a7a\u95f4\u63a7\u5236\u7684\u5b9e\u7528\u6269\u5c55"}}
{"id": "2509.15391", "pdf": "https://arxiv.org/pdf/2509.15391", "abs": "https://arxiv.org/abs/2509.15391", "authors": ["Mst Tasnim Pervin", "George Bebis", "Fang Jiang", "Alireza Tavakkoli"], "title": "RaceGAN: A Framework for Preserving Individuality while Converting Racial Information for Image-to-Image Translation", "categories": ["cs.CV"], "comment": null, "summary": "Generative adversarial networks (GANs) have demonstrated significant progress in unpaired image-to-image translation in recent years for several applications. CycleGAN was the first to lead the way, although it was restricted to a pair of domains. StarGAN overcame this constraint by tackling image-to-image translation across various domains, although it was not able to map in-depth low-level style changes for these domains. Style mapping via reference-guided image synthesis has been made possible by the innovations of StarGANv2 and StyleGAN. However, these models do not maintain individuality and need an extra reference image in addition to the input. Our study aims to translate racial traits by means of multi-domain image-to-image translation. We present RaceGAN, a novel framework capable of mapping style codes over several domains during racial attribute translation while maintaining individuality and high level semantics without relying on a reference image. RaceGAN outperforms other models in translating racial features (i.e., Asian, White, and Black) when tested on Chicago Face Dataset. We also give quantitative findings utilizing InceptionReNetv2-based classification to demonstrate the effectiveness of our racial translation. Moreover, we investigate how well the model partitions the latent space into distinct clusters of faces for each ethnic group.", "AI": {"tldr": "RaceGAN\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u57df\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u79cd\u65cf\u7279\u5f81\u8f6c\u6362\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u4e2a\u4f53\u7279\u5f81\u548c\u9ad8\u5c42\u8bed\u4e49\u7684\u540c\u65f6\u6620\u5c04\u591a\u4e2a\u57df\u7684\u6837\u5f0f\u4ee3\u7801\uff0c\u65e0\u9700\u53c2\u8003\u56fe\u50cf\u3002", "motivation": "\u73b0\u6709\u7684GAN\u6a21\u578b\u5982CycleGAN\u3001StarGAN\u7b49\u5728\u79cd\u65cf\u7279\u5f81\u8f6c\u6362\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u8981\u4e48\u65e0\u6cd5\u5904\u7406\u591a\u57df\u8f6c\u6362\uff0c\u8981\u4e48\u65e0\u6cd5\u4fdd\u6301\u4e2a\u4f53\u7279\u5f81\uff0c\u6216\u8005\u9700\u8981\u989d\u5916\u7684\u53c2\u8003\u56fe\u50cf\u3002", "method": "RaceGAN\u901a\u8fc7\u591a\u57df\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\u6280\u672f\uff0c\u5728\u79cd\u65cf\u5c5e\u6027\u8f6c\u6362\u8fc7\u7a0b\u4e2d\u6620\u5c04\u6837\u5f0f\u4ee3\u7801\uff0c\u540c\u65f6\u4fdd\u6301\u4e2a\u4f53\u7279\u5f81\u548c\u9ad8\u5c42\u8bed\u4e49\uff0c\u65e0\u9700\u4f9d\u8d56\u53c2\u8003\u56fe\u50cf\u3002", "result": "\u5728\u829d\u52a0\u54e5\u4eba\u8138\u6570\u636e\u96c6\u4e0a\u7684\u6d4b\u8bd5\u8868\u660e\uff0cRaceGAN\u5728\u79cd\u65cf\u7279\u5f81\u8f6c\u6362\uff08\u4e9a\u6d32\u4eba\u3001\u767d\u4eba\u548c\u9ed1\u4eba\uff09\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u5e76\u901a\u8fc7InceptionReNetv2\u5206\u7c7b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "RaceGAN\u6210\u529f\u5b9e\u73b0\u4e86\u79cd\u65cf\u7279\u5f81\u7684\u51c6\u786e\u8f6c\u6362\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e2a\u4f53\u7279\u5f81\uff0c\u5e76\u4e14\u80fd\u591f\u5c06\u6f5c\u5728\u7a7a\u95f4\u5212\u5206\u4e3a\u4e0d\u540c\u79cd\u65cf\u7fa4\u4f53\u7684\u6e05\u6670\u805a\u7c7b\u3002"}}
{"id": "2509.15459", "pdf": "https://arxiv.org/pdf/2509.15459", "abs": "https://arxiv.org/abs/2509.15459", "authors": ["Yiyi Liu", "Chunyang Liu", "Weiqin Jiao", "Bojian Wu", "Fashuai Li", "Biao Xiong"], "title": "CAGE: Continuity-Aware edGE Network Unlocks Robust Floorplan Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present \\textbf{CAGE} (\\textit{Continuity-Aware edGE}) network, a \\textcolor{red}{robust} framework for reconstructing vector floorplans directly from point-cloud density maps. Traditional corner-based polygon representations are highly sensitive to noise and incomplete observations, often resulting in fragmented or implausible layouts. Recent line grouping methods leverage structural cues to improve robustness but still struggle to recover fine geometric details. To address these limitations, we propose a \\textit{native} edge-centric formulation, modeling each wall segment as a directed, geometrically continuous edge. This representation enables inference of coherent floorplan structures, ensuring watertight, topologically valid room boundaries while improving robustness and reducing artifacts. Towards this design, we develop a dual-query transformer decoder that integrates perturbed and latent queries within a denoising framework, which not only stabilizes optimization but also accelerates convergence. Extensive experiments on Structured3D and SceneCAD show that \\textbf{CAGE} achieves state-of-the-art performance, with F1 scores of 99.1\\% (rooms), 91.7\\% (corners), and 89.3\\% (angles). The method also demonstrates strong cross-dataset generalization, underscoring the efficacy of our architectural innovations. Code and pretrained models will be released upon acceptance.", "AI": {"tldr": "CAGE\u662f\u4e00\u79cd\u4ece\u70b9\u4e91\u5bc6\u5ea6\u56fe\u76f4\u63a5\u91cd\u5efa\u77e2\u91cf\u5e73\u9762\u56fe\u7684\u9c81\u68d2\u6846\u67b6\uff0c\u901a\u8fc7\u8fb9\u7f18\u4e2d\u5fc3\u8868\u793a\u548c\u53cc\u67e5\u8be2\u53d8\u6362\u5668\u89e3\u7801\u5668\u5b9e\u73b0\u8fde\u7eed\u6027\u548c\u62d3\u6251\u6709\u6548\u6027", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89d2\u70b9\u7684\u591a\u8fb9\u5f62\u8868\u793a\u5bf9\u566a\u58f0\u548c\u89c2\u6d4b\u4e0d\u5b8c\u6574\u9ad8\u5ea6\u654f\u611f\uff0c\u5bfc\u81f4\u5e03\u5c40\u788e\u7247\u5316\uff1b\u73b0\u6709\u7ebf\u5206\u7ec4\u65b9\u6cd5\u96be\u4ee5\u6062\u590d\u7cbe\u7ec6\u51e0\u4f55\u7ec6\u8282", "method": "\u63d0\u51fa\u539f\u751f\u8fb9\u7f18\u4e2d\u5fc3\u8868\u793a\u6cd5\uff0c\u5c06\u5899\u6bb5\u5efa\u6a21\u4e3a\u5b9a\u5411\u51e0\u4f55\u8fde\u7eed\u8fb9\uff1b\u5f00\u53d1\u53cc\u67e5\u8be2\u53d8\u6362\u5668\u89e3\u7801\u5668\uff0c\u5728\u53bb\u566a\u6846\u67b6\u4e2d\u96c6\u6210\u6270\u52a8\u548c\u6f5c\u5728\u67e5\u8be2", "result": "\u5728Structured3D\u548cSceneCAD\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff1a\u623f\u95f4F1 99.1%\uff0c\u89d2\u70b9F1 91.7%\uff0c\u89d2\u5ea6F1 89.3%\uff1b\u5c55\u793a\u5f3a\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b", "conclusion": "CAGE\u6846\u67b6\u901a\u8fc7\u8fb9\u7f18\u4e2d\u5fc3\u8868\u793a\u548c\u53cc\u67e5\u8be2\u89e3\u7801\u5668\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5e73\u9762\u56fe\u91cd\u5efa\u7684\u9c81\u68d2\u6027\u548c\u51e0\u4f55\u8fde\u7eed\u6027"}}
{"id": "2509.15479", "pdf": "https://arxiv.org/pdf/2509.15479", "abs": "https://arxiv.org/abs/2509.15479", "authors": ["Bj\u00f6rn M\u00f6ller", "Zhengyang Li", "Malte Stelzer", "Thomas Graave", "Fabian Bettels", "Muaaz Ataya", "Tim Fingscheidt"], "title": "OpenViGA: Video Generation for Automotive Driving Scenes by Streamlining and Fine-Tuning Open Source Models with Public Data", "categories": ["cs.CV"], "comment": null, "summary": "Recent successful video generation systems that predict and create realistic automotive driving scenes from short video inputs assign tokenization, future state prediction (world model), and video decoding to dedicated models. These approaches often utilize large models that require significant training resources, offer limited insight into design choices, and lack publicly available code and datasets. In this work, we address these deficiencies and present OpenViGA, an open video generation system for automotive driving scenes. Our contributions are: Unlike several earlier works for video generation, such as GAIA-1, we provide a deep analysis of the three components of our system by separate quantitative and qualitative evaluation: Image tokenizer, world model, video decoder. Second, we purely build upon powerful pre-trained open source models from various domains, which we fine-tune by publicly available automotive data (BDD100K) on GPU hardware at academic scale. Third, we build a coherent video generation system by streamlining interfaces of our components. Fourth, due to public availability of the underlying models and data, we allow full reproducibility. Finally, we also publish our code and models on Github. For an image size of 256x256 at 4 fps we are able to predict realistic driving scene videos frame-by-frame with only one frame of algorithmic latency.", "AI": {"tldr": "OpenViGA\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u89c6\u9891\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u6790\u56fe\u50cf\u5206\u8bcd\u5668\u3001\u4e16\u754c\u6a21\u578b\u548c\u89c6\u9891\u89e3\u7801\u5668\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u5f00\u6e90\u6a21\u578b\u548c\u516c\u5f00\u6570\u636e\u96c6\u6784\u5efa\uff0c\u5b9e\u73b0\u4e86\u53ef\u590d\u73b0\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u7cfb\u7edf\u901a\u5e38\u4f7f\u7528\u5927\u578b\u6a21\u578b\uff0c\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u8d44\u6e90\uff0c\u8bbe\u8ba1\u9009\u62e9\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u4e14\u7f3a\u4e4f\u516c\u5f00\u7684\u4ee3\u7801\u548c\u6570\u636e\u96c6\u3002OpenViGA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u5f00\u6e90\u6a21\u578b\uff0c\u4f7f\u7528BDD100K\u6570\u636e\u96c6\u5728\u5b66\u672f\u89c4\u6a21GPU\u786c\u4ef6\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u901a\u8fc7\u56fe\u50cf\u5206\u8bcd\u5668\u3001\u4e16\u754c\u6a21\u578b\u548c\u89c6\u9891\u89e3\u7801\u5668\u4e09\u4e2a\u7ec4\u4ef6\u6784\u5efa\u8fde\u8d2f\u7684\u89c6\u9891\u751f\u6210\u7cfb\u7edf\u3002", "result": "\u5728256x256\u5206\u8fa8\u7387\u30014fps\u6761\u4ef6\u4e0b\uff0c\u4ec5\u4f7f\u7528\u4e00\u5e27\u7b97\u6cd5\u5ef6\u8fdf\u5c31\u80fd\u9010\u5e27\u9884\u6d4b\u771f\u5b9e\u7684\u9a7e\u9a76\u573a\u666f\u89c6\u9891\u3002", "conclusion": "OpenViGA\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b8c\u5168\u53ef\u590d\u73b0\u7684\u5f00\u6e90\u89c6\u9891\u751f\u6210\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u5728\u8d44\u6e90\u9700\u6c42\u3001\u900f\u660e\u5ea6\u548c\u53ef\u7528\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2509.15496", "pdf": "https://arxiv.org/pdf/2509.15496", "abs": "https://arxiv.org/abs/2509.15496", "authors": ["Shen Sang", "Tiancheng Zhi", "Tianpei Gu", "Jing Liu", "Linjie Luo"], "title": "Lynx: Towards High-Fidelity Personalized Video Generation", "categories": ["cs.CV"], "comment": "Lynx Technical Report", "summary": "We present Lynx, a high-fidelity model for personalized video synthesis from a single input image. Built on an open-source Diffusion Transformer (DiT) foundation model, Lynx introduces two lightweight adapters to ensure identity fidelity. The ID-adapter employs a Perceiver Resampler to convert ArcFace-derived facial embeddings into compact identity tokens for conditioning, while the Ref-adapter integrates dense VAE features from a frozen reference pathway, injecting fine-grained details across all transformer layers through cross-attention. These modules collectively enable robust identity preservation while maintaining temporal coherence and visual realism. Through evaluation on a curated benchmark of 40 subjects and 20 unbiased prompts, which yielded 800 test cases, Lynx has demonstrated superior face resemblance, competitive prompt following, and strong video quality, thereby advancing the state of personalized video generation.", "AI": {"tldr": "Lynx\u662f\u4e00\u4e2a\u57fa\u4e8eDiT\u7684\u9ad8\u4fdd\u771f\u4e2a\u6027\u5316\u89c6\u9891\u5408\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u4e2a\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u786e\u4fdd\u8eab\u4efd\u4fdd\u771f\u5ea6\uff0c\u5728\u8eab\u4efd\u4fdd\u6301\u548c\u89c6\u9891\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u4f18\u5f02", "motivation": "\u89e3\u51b3\u4e2a\u6027\u5316\u89c6\u9891\u751f\u6210\u4e2d\u7684\u8eab\u4efd\u4fdd\u771f\u5ea6\u95ee\u9898\uff0c\u786e\u4fdd\u4ece\u5355\u5f20\u8f93\u5165\u56fe\u50cf\u751f\u6210\u7684\u89c6\u9891\u80fd\u591f\u51c6\u786e\u4fdd\u6301\u4eba\u7269\u8eab\u4efd\u7279\u5f81", "method": "\u57fa\u4e8e\u5f00\u6e90DiT\u57fa\u7840\u6a21\u578b\uff0c\u5f15\u5165ID-adapter\u4f7f\u7528Perceiver Resampler\u5c06ArcFace\u9762\u90e8\u5d4c\u5165\u8f6c\u6362\u4e3a\u7d27\u51d1\u8eab\u4efd\u4ee4\u724c\uff0cRef-adapter\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u5728\u6240\u6709transformer\u5c42\u6ce8\u5165\u51bb\u7ed3\u53c2\u8003\u8def\u5f84\u7684\u5bc6\u96c6VAE\u7279\u5f81", "result": "\u5728\u5305\u542b40\u4e2a\u4e3b\u4f53\u548c20\u4e2a\u65e0\u504f\u63d0\u793a\u7684800\u4e2a\u6d4b\u8bd5\u6848\u4f8b\u8bc4\u4f30\u4e2d\uff0cLynx\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9762\u90e8\u76f8\u4f3c\u5ea6\u3001\u7ade\u4e89\u529b\u7684\u63d0\u793a\u8ddf\u968f\u80fd\u529b\u548c\u5f3a\u5927\u7684\u89c6\u9891\u8d28\u91cf", "conclusion": "Lynx\u901a\u8fc7\u521b\u65b0\u7684\u9002\u914d\u5668\u8bbe\u8ba1\u5728\u4e2a\u6027\u5316\u89c6\u9891\u751f\u6210\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5b9e\u73b0\u4e86\u8eab\u4efd\u4fdd\u771f\u5ea6\u548c\u89c6\u89c9\u771f\u5b9e\u6027\u7684\u5e73\u8861"}}
{"id": "2509.15536", "pdf": "https://arxiv.org/pdf/2509.15536", "abs": "https://arxiv.org/abs/2509.15536", "authors": ["Sen Wang", "Jingyi Tian", "Le Wang", "Zhimin Liao", "Jiayi Li", "Huaiyi Dong", "Kun Xia", "Sanping Zhou", "Wei Tang", "Hua Gang"], "title": "SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models", "categories": ["cs.CV", "cs.RO"], "comment": "22 pages,15 figures", "summary": "World models allow agents to simulate the consequences of actions in imagined environments for planning, control, and long-horizon decision-making. However, existing autoregressive world models struggle with visually coherent predictions due to disrupted spatial structure, inefficient decoding, and inadequate motion modeling. In response, we propose \\textbf{S}cale-wise \\textbf{A}utoregression with \\textbf{M}otion \\textbf{P}r\\textbf{O}mpt (\\textbf{SAMPO}), a hybrid framework that combines visual autoregressive modeling for intra-frame generation with causal modeling for next-frame generation. Specifically, SAMPO integrates temporal causal decoding with bidirectional spatial attention, which preserves spatial locality and supports parallel decoding within each scale. This design significantly enhances both temporal consistency and rollout efficiency. To further improve dynamic scene understanding, we devise an asymmetric multi-scale tokenizer that preserves spatial details in observed frames and extracts compact dynamic representations for future frames, optimizing both memory usage and model performance. Additionally, we introduce a trajectory-aware motion prompt module that injects spatiotemporal cues about object and robot trajectories, focusing attention on dynamic regions and improving temporal consistency and physical realism. Extensive experiments show that SAMPO achieves competitive performance in action-conditioned video prediction and model-based control, improving generation quality with 4.4$\\times$ faster inference. We also evaluate SAMPO's zero-shot generalization and scaling behavior, demonstrating its ability to generalize to unseen tasks and benefit from larger model sizes.", "AI": {"tldr": "SAMPO\u662f\u4e00\u4e2a\u6df7\u5408\u4e16\u754c\u6a21\u578b\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u89c6\u89c9\u81ea\u56de\u5f52\u5efa\u6a21\u548c\u56e0\u679c\u5efa\u6a21\uff0c\u901a\u8fc7\u5c3a\u5ea6\u7ea7\u81ea\u56de\u5f52\u548c\u8fd0\u52a8\u63d0\u793a\u6765\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u7a7a\u95f4\u7ed3\u6784\u3001\u89e3\u7801\u6548\u7387\u548c\u8fd0\u52a8\u5efa\u6a21\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u56de\u5f52\u4e16\u754c\u6a21\u578b\u5728\u89c6\u89c9\u4e00\u81f4\u6027\u9884\u6d4b\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u95ee\u9898\u5305\u62ec\u7a7a\u95f4\u7ed3\u6784\u7834\u574f\u3001\u89e3\u7801\u6548\u7387\u4f4e\u4e0b\u548c\u8fd0\u52a8\u5efa\u6a21\u4e0d\u8db3\u3002", "method": "SAMPO\u6574\u5408\u4e86\u65f6\u95f4\u56e0\u679c\u89e3\u7801\u548c\u53cc\u5411\u7a7a\u95f4\u6ce8\u610f\u529b\uff0c\u91c7\u7528\u975e\u5bf9\u79f0\u591a\u5c3a\u5ea6\u5206\u8bcd\u5668\u4fdd\u7559\u7a7a\u95f4\u7ec6\u8282\uff0c\u5e76\u5f15\u5165\u8f68\u8ff9\u611f\u77e5\u8fd0\u52a8\u63d0\u793a\u6a21\u5757\u6ce8\u5165\u65f6\u7a7a\u7ebf\u7d22\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSAMPO\u5728\u52a8\u4f5c\u6761\u4ef6\u89c6\u9891\u9884\u6d4b\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u63a7\u5236\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751f\u6210\u8d28\u91cf\u63d0\u5347\u4e14\u63a8\u7406\u901f\u5ea6\u5feb4.4\u500d\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u548c\u826f\u597d\u7684\u6269\u5c55\u6027\u3002", "conclusion": "SAMPO\u901a\u8fc7\u521b\u65b0\u7684\u6df7\u5408\u6846\u67b6\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e16\u754c\u6a21\u578b\u4e2d\u7684\u89c6\u89c9\u4e00\u81f4\u6027\u548c\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u957f\u671f\u51b3\u7b56\u89c4\u5212\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u6a21\u62df\u73af\u5883\u3002"}}
{"id": "2509.15540", "pdf": "https://arxiv.org/pdf/2509.15540", "abs": "https://arxiv.org/abs/2509.15540", "authors": ["Wei Chen", "Tongguan Wang", "Feiyue Xue", "Junkai Li", "Hui Liu", "Ying Sha"], "title": "Beyond Words: Enhancing Desire, Emotion, and Sentiment Recognition with Non-Verbal Cues", "categories": ["cs.CV", "cs.CL"], "comment": "13 page, 5 figures, uploaded by Wei Chen", "summary": "Desire, as an intention that drives human behavior, is closely related to both emotion and sentiment. Multimodal learning has advanced sentiment and emotion recognition, but multimodal approaches specially targeting human desire understanding remain underexplored. And existing methods in sentiment analysis predominantly emphasize verbal cues and overlook images as complementary non-verbal cues. To address these gaps, we propose a Symmetrical Bidirectional Multimodal Learning Framework for Desire, Emotion, and Sentiment Recognition, which enforces mutual guidance between text and image modalities to effectively capture intention-related representations in the image. Specifically, low-resolution images are used to obtain global visual representations for cross-modal alignment, while high resolution images are partitioned into sub-images and modeled with masked image modeling to enhance the ability to capture fine-grained local features. A text-guided image decoder and an image-guided text decoder are introduced to facilitate deep cross-modal interaction at both local and global representations of image information. Additionally, to balance perceptual gains with computation cost, a mixed-scale image strategy is adopted, where high-resolution images are cropped into sub-images for masked modeling. The proposed approach is evaluated on MSED, a multimodal dataset that includes a desire understanding benchmark, as well as emotion and sentiment recognition. Experimental results indicate consistent improvements over other state-of-the-art methods, validating the effectiveness of our proposed method. Specifically, our method outperforms existing approaches, achieving F1-score improvements of 1.1% in desire understanding, 0.6% in emotion recognition, and 0.9% in sentiment analysis. Our code is available at: https://github.com/especiallyW/SyDES.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u79f0\u53cc\u5411\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u6b32\u671b\u3001\u60c5\u611f\u548c\u60c5\u7eea\u8bc6\u522b\uff0c\u901a\u8fc7\u6587\u672c\u548c\u56fe\u50cf\u6a21\u6001\u7684\u76f8\u4e92\u5f15\u5bfc\u6765\u6355\u6349\u610f\u56fe\u76f8\u5173\u8868\u793a\uff0c\u5728MSED\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8a00\u8bed\u7ebf\u7d22\u800c\u5ffd\u89c6\u56fe\u50cf\u4f5c\u4e3a\u8865\u5145\u7684\u975e\u8a00\u8bed\u7ebf\u7d22\uff0c\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u4eba\u7c7b\u6b32\u671b\u7406\u89e3\u65b9\u9762\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u91c7\u7528\u5bf9\u79f0\u53cc\u5411\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u83b7\u53d6\u5168\u5c40\u89c6\u89c9\u8868\u793a\u8fdb\u884c\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5206\u5272\u4e3a\u5b50\u56fe\u50cf\u5e76\u901a\u8fc7\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u6765\u589e\u5f3a\u7ec6\u7c92\u5ea6\u5c40\u90e8\u7279\u5f81\u6355\u6349\u80fd\u529b\u3002\u5f15\u5165\u6587\u672c\u5f15\u5bfc\u7684\u56fe\u50cf\u89e3\u7801\u5668\u548c\u56fe\u50cf\u5f15\u5bfc\u7684\u6587\u672c\u89e3\u7801\u5668\u4fc3\u8fdb\u6df1\u5ea6\u8de8\u6a21\u6001\u4ea4\u4e92\u3002", "result": "\u5728MSED\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u6b32\u671b\u7406\u89e3\u65b9\u9762F1\u5206\u6570\u63d0\u53471.1%\uff0c\u60c5\u611f\u8bc6\u522b\u63d0\u53470.6%\uff0c\u60c5\u7eea\u5206\u6790\u63d0\u53470.9%\uff0c\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u5bf9\u79f0\u53cc\u5411\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u6b32\u671b\u3001\u60c5\u611f\u548c\u60c5\u7eea\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.15548", "pdf": "https://arxiv.org/pdf/2509.15548", "abs": "https://arxiv.org/abs/2509.15548", "authors": ["Deming Li", "Kaiwen Jiang", "Yutao Tang", "Ravi Ramamoorthi", "Rama Chellappa", "Cheng Peng"], "title": "MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild", "categories": ["cs.CV"], "comment": null, "summary": "In-the-wild photo collections often contain limited volumes of imagery and exhibit multiple appearances, e.g., taken at different times of day or seasons, posing significant challenges to scene reconstruction and novel view synthesis. Although recent adaptations of Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) have improved in these areas, they tend to oversmooth and are prone to overfitting. In this paper, we present MS-GS, a novel framework designed with Multi-appearance capabilities in Sparse-view scenarios using 3DGS. To address the lack of support due to sparse initializations, our approach is built on the geometric priors elicited from monocular depth estimations. The key lies in extracting and utilizing local semantic regions with a Structure-from-Motion (SfM) points anchored algorithm for reliable alignment and geometry cues. Then, to introduce multi-view constraints, we propose a series of geometry-guided supervision at virtual views in a fine-grained and coarse scheme to encourage 3D consistency and reduce overfitting. We also introduce a dataset and an in-the-wild experiment setting to set up more realistic benchmarks. We demonstrate that MS-GS achieves photorealistic renderings under various challenging sparse-view and multi-appearance conditions and outperforms existing approaches significantly across different datasets.", "AI": {"tldr": "MS-GS\u662f\u4e00\u4e2a\u9488\u5bf9\u7a00\u758f\u89c6\u56fe\u548c\u591a\u5916\u89c2\u573a\u666f\u76843D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u5229\u7528\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684\u51e0\u4f55\u5148\u9a8c\u548cSfM\u70b9\u951a\u5b9a\u7b97\u6cd5\uff0c\u901a\u8fc7\u51e0\u4f55\u5f15\u5bfc\u7684\u865a\u62df\u89c6\u56fe\u76d1\u7763\u5b9e\u73b0\u591a\u89c6\u56fe\u7ea6\u675f\uff0c\u5728\u7a00\u758f\u89c6\u56fe\u548c\u591a\u5916\u89c2\u6761\u4ef6\u4e0b\u5b9e\u73b0\u903c\u771f\u6e32\u67d3\u3002", "motivation": "\u89e3\u51b3\u91ce\u5916\u7167\u7247\u96c6\u5408\u4e2d\u56fe\u50cf\u6570\u91cf\u6709\u9650\u4e14\u5b58\u5728\u591a\u79cd\u5916\u89c2\uff08\u5982\u4e0d\u540c\u65f6\u95f4\u3001\u5b63\u8282\uff09\u65f6\uff0c\u573a\u666f\u91cd\u5efa\u548c\u65b0\u89c6\u56fe\u5408\u6210\u7684\u6311\u6218\u3002\u73b0\u6709NeRF\u548c3DGS\u65b9\u6cd5\u5bb9\u6613\u8fc7\u5ea6\u5e73\u6ed1\u548c\u8fc7\u62df\u5408\u3002", "method": "\u57fa\u4e8e\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u63d0\u53d6\u51e0\u4f55\u5148\u9a8c\uff0c\u4f7f\u7528SfM\u70b9\u951a\u5b9a\u7b97\u6cd5\u63d0\u53d6\u5c40\u90e8\u8bed\u4e49\u533a\u57df\u8fdb\u884c\u53ef\u9760\u5bf9\u9f50\u548c\u51e0\u4f55\u63d0\u793a\uff0c\u63d0\u51fa\u7ec6\u7c92\u5ea6\u548c\u7c97\u7c92\u5ea6\u65b9\u6848\u7684\u51e0\u4f55\u5f15\u5bfc\u865a\u62df\u89c6\u56fe\u76d1\u7763\u6765\u9f13\u52b13D\u4e00\u81f4\u6027\u548c\u51cf\u5c11\u8fc7\u62df\u5408\u3002", "result": "MS-GS\u5728\u5404\u79cd\u6311\u6218\u6027\u7684\u7a00\u758f\u89c6\u56fe\u548c\u591a\u5916\u89c2\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u903c\u771f\u6e32\u67d3\uff0c\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MS-GS\u6846\u67b6\u901a\u8fc7\u51e0\u4f55\u5148\u9a8c\u548c\u865a\u62df\u89c6\u56fe\u76d1\u7763\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u89c6\u56fe\u591a\u5916\u89c2\u573a\u666f\u7684\u91cd\u5efa\u95ee\u9898\uff0c\u4e3a\u66f4\u73b0\u5b9e\u7684\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u548c\u5b9e\u9a8c\u8bbe\u7f6e\u3002"}}
{"id": "2509.15642", "pdf": "https://arxiv.org/pdf/2509.15642", "abs": "https://arxiv.org/abs/2509.15642", "authors": ["Fangyuan Mao", "Shuo Wang", "Jilin Mei", "Chen Min", "Shun Lu", "Fuyang Liu", "Yu Hu"], "title": "UNIV: Unified Foundation Model for Infrared and Visible Modalities", "categories": ["cs.CV"], "comment": null, "summary": "The demand for joint RGB-visible and infrared perception is growing rapidly, particularly to achieve robust performance under diverse weather conditions. Although pre-trained models for RGB-visible and infrared data excel in their respective domains, they often underperform in multimodal scenarios, such as autonomous vehicles equipped with both sensors. To address this challenge, we propose a biologically inspired UNified foundation model for Infrared and Visible modalities (UNIV), featuring two key innovations. First, we introduce Patch-wise Cross-modality Contrastive Learning (PCCL), an attention-guided distillation framework that mimics retinal horizontal cells' lateral inhibition, which enables effective cross-modal feature alignment while remaining compatible with any transformer-based architecture. Second, our dual-knowledge preservation mechanism emulates the retina's bipolar cell signal routing - combining LoRA adapters (2% added parameters) with synchronous distillation to prevent catastrophic forgetting, thereby replicating the retina's photopic (cone-driven) and scotopic (rod-driven) functionality. To support cross-modal learning, we introduce the MVIP dataset, the most comprehensive visible-infrared benchmark to date. It contains 98,992 precisely aligned image pairs spanning diverse scenarios. Extensive experiments demonstrate UNIV's superior performance on infrared tasks (+1.7 mIoU in semantic segmentation and +0.7 mAP in object detection) while maintaining 99%+ of the baseline performance on visible RGB tasks. Our code is available at https://github.com/fangyuanmao/UNIV.", "AI": {"tldr": "UNIV\u662f\u4e00\u4e2a\u53d7\u751f\u7269\u5b66\u542f\u53d1\u7684\u7edf\u4e00\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u7ea2\u5916\u548c\u53ef\u89c1\u5149\u6a21\u6001\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u5bf9\u6bd4\u5b66\u4e60\u548c\u53cc\u77e5\u8bc6\u4fdd\u7559\u673a\u5236\uff0c\u5728\u8de8\u6a21\u6001\u4efb\u52a1\u4e2d\u5b9e\u73b0\u5353\u8d8a\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3RGB-\u53ef\u89c1\u5149\u548c\u7ea2\u5916\u6570\u636e\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u591a\u6a21\u6001\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u5168\u5929\u5019\u9c81\u68d2\u611f\u77e5\u3002", "method": "\u63d0\u51faPatch-wise Cross-modality Contrastive Learning\uff08PCCL\uff09\u6a21\u62df\u89c6\u7f51\u819c\u6c34\u5e73\u7ec6\u80de\u7684\u4fa7\u5411\u6291\u5236\uff0c\u4ee5\u53ca\u53cc\u77e5\u8bc6\u4fdd\u7559\u673a\u5236\u7ed3\u5408LoRA\u9002\u914d\u5668\u548c\u540c\u6b65\u84b8\u998f\u6765\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "\u5728\u7ea2\u5916\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff08\u8bed\u4e49\u5206\u5272+1.7 mIoU\uff0c\u76ee\u6807\u68c0\u6d4b+0.7 mAP\uff09\uff0c\u540c\u65f6\u5728\u53ef\u89c1\u5149RGB\u4efb\u52a1\u4e0a\u4fdd\u630199%\u4ee5\u4e0a\u7684\u57fa\u7ebf\u6027\u80fd\u3002", "conclusion": "UNIV\u6a21\u578b\u6210\u529f\u5b9e\u73b0\u4e86\u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\u548c\u77e5\u8bc6\u4fdd\u7559\uff0c\u4e3a\u5168\u5929\u5019\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.15645", "pdf": "https://arxiv.org/pdf/2509.15645", "abs": "https://arxiv.org/abs/2509.15645", "authors": ["Donghyun Lee", "Dawoon Jeong", "Jae W. Lee", "Hongil Yoon"], "title": "GS-Scale: Unlocking Large-Scale 3D Gaussian Splatting Training via Host Offloading", "categories": ["cs.CV"], "comment": null, "summary": "The advent of 3D Gaussian Splatting has revolutionized graphics rendering by delivering high visual quality and fast rendering speeds. However, training large-scale scenes at high quality remains challenging due to the substantial memory demands required to store parameters, gradients, and optimizer states, which can quickly overwhelm GPU memory. To address these limitations, we propose GS-Scale, a fast and memory-efficient training system for 3D Gaussian Splatting. GS-Scale stores all Gaussians in host memory, transferring only a subset to the GPU on demand for each forward and backward pass. While this dramatically reduces GPU memory usage, it requires frustum culling and optimizer updates to be executed on the CPU, introducing slowdowns due to CPU's limited compute and memory bandwidth. To mitigate this, GS-Scale employs three system-level optimizations: (1) selective offloading of geometric parameters for fast frustum culling, (2) parameter forwarding to pipeline CPU optimizer updates with GPU computation, and (3) deferred optimizer update to minimize unnecessary memory accesses for Gaussians with zero gradients. Our extensive evaluations on large-scale datasets demonstrate that GS-Scale significantly lowers GPU memory demands by 3.3-5.6x, while achieving training speeds comparable to GPU without host offloading. This enables large-scale 3D Gaussian Splatting training on consumer-grade GPUs; for instance, GS-Scale can scale the number of Gaussians from 4 million to 18 million on an RTX 4070 Mobile GPU, leading to 23-35% LPIPS (learned perceptual image patch similarity) improvement.", "AI": {"tldr": "GS-Scale\u662f\u4e00\u4e2a\u9488\u5bf93D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u7684\u5185\u5b58\u9ad8\u6548\u8bad\u7ec3\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u9ad8\u65af\u6570\u636e\u5b58\u50a8\u5728\u4e3b\u673a\u5185\u5b58\u4e2d\uff0c\u4ec5\u6309\u9700\u4f20\u8f93\u5b50\u96c6\u5230GPU\uff0c\u663e\u8457\u964d\u4f4eGPU\u5185\u5b58\u9700\u6c423.3-5.6\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u8bad\u7ec3\u901f\u5ea6\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u867d\u7136\u63d0\u4f9b\u9ad8\u8d28\u91cf\u6e32\u67d3\u548c\u5feb\u901f\u6e32\u67d3\u901f\u5ea6\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u573a\u666f\u8bad\u7ec3\u65f6\u9762\u4e34GPU\u5185\u5b58\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u9700\u8981\u5b58\u50a8\u53c2\u6570\u3001\u68af\u5ea6\u548c\u4f18\u5316\u5668\u72b6\u6001\u3002", "method": "GS-Scale\u91c7\u7528\u4e09\u79cd\u7cfb\u7edf\u7ea7\u4f18\u5316\uff1a(1)\u9009\u62e9\u6027\u5378\u8f7d\u51e0\u4f55\u53c2\u6570\u4ee5\u52a0\u901f\u89c6\u9525\u4f53\u5254\u9664\uff1b(2)\u53c2\u6570\u8f6c\u53d1\u5c06CPU\u4f18\u5316\u5668\u66f4\u65b0\u4e0eGPU\u8ba1\u7b97\u6d41\u6c34\u7ebf\u5316\uff1b(3)\u5ef6\u8fdf\u4f18\u5316\u5668\u66f4\u65b0\u4ee5\u51cf\u5c11\u5bf9\u96f6\u68af\u5ea6\u9ad8\u65af\u7684\u4e0d\u5fc5\u8981\u5185\u5b58\u8bbf\u95ee\u3002", "result": "\u5728\u5927\u578b\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cGS-Scale\u5c06GPU\u5185\u5b58\u9700\u6c42\u964d\u4f4e3.3-5.6\u500d\uff0c\u5728RTX 4070\u79fb\u52a8GPU\u4e0a\u53ef\u5c06\u9ad8\u65af\u6570\u91cf\u4ece400\u4e07\u6269\u5c55\u52301800\u4e07\uff0cLPIPS\u6307\u6807\u63d0\u534723-35%\u3002", "conclusion": "GS-Scale\u4f7f\u5927\u89c4\u6a213D\u9ad8\u65af\u6cfc\u6e85\u8bad\u7ec3\u80fd\u591f\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u5b9e\u73b0\uff0c\u89e3\u51b3\u4e86\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2509.15648", "pdf": "https://arxiv.org/pdf/2509.15648", "abs": "https://arxiv.org/abs/2509.15648", "authors": ["Yuwei Jia", "Yutang Lu", "Zhe Cui", "Fei Su"], "title": "FingerSplat: Contactless Fingerprint 3D Reconstruction and Generation based on 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Researchers have conducted many pioneer researches on contactless fingerprints, yet the performance of contactless fingerprint recognition still lags behind contact-based methods primary due to the insufficient contactless fingerprint data with pose variations and lack of the usage of implicit 3D fingerprint representations. In this paper, we introduce a novel contactless fingerprint 3D registration, reconstruction and generation framework by integrating 3D Gaussian Splatting, with the goal of offering a new paradigm for contactless fingerprint recognition that integrates 3D fingerprint reconstruction and generation. To our knowledge, this is the first work to apply 3D Gaussian Splatting to the field of fingerprint recognition, and the first to achieve effective 3D registration and complete reconstruction of contactless fingerprints with sparse input images and without requiring camera parameters information. Experiments on 3D fingerprint registration, reconstruction, and generation prove that our method can accurately align and reconstruct 3D fingerprints from 2D images, and sequentially generates high-quality contactless fingerprints from 3D model, thus increasing the performances for contactless fingerprint recognition.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u65e0\u63a5\u89e6\u6307\u7eb93D\u914d\u51c6\u3001\u91cd\u5efa\u548c\u751f\u6210\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u8be5\u6280\u672f\u5e94\u7528\u4e8e\u6307\u7eb9\u8bc6\u522b\u9886\u57df\uff0c\u80fd\u591f\u5728\u65e0\u9700\u76f8\u673a\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u4ece\u7a00\u758f\u76842D\u56fe\u50cf\u4e2d\u5b9e\u73b0\u6709\u6548\u76843D\u914d\u51c6\u548c\u5b8c\u6574\u91cd\u5efa\u3002", "motivation": "\u5f53\u524d\u65e0\u63a5\u89e6\u6307\u7eb9\u8bc6\u522b\u6027\u80fd\u843d\u540e\u4e8e\u63a5\u89e6\u5f0f\u65b9\u6cd5\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u59ff\u6001\u53d8\u5316\u7684\u65e0\u63a5\u89e6\u6307\u7eb9\u6570\u636e\u4ee5\u53ca\u672a\u5145\u5206\u5229\u7528\u9690\u5f0f3D\u6307\u7eb9\u8868\u793a\u3002", "method": "\u901a\u8fc7\u96c6\u62103D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u65e0\u63a5\u89e6\u6307\u7eb9\u8bc6\u522b\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e863D\u6307\u7eb9\u7684\u91cd\u5efa\u548c\u751f\u6210\uff0c\u65e0\u9700\u76f8\u673a\u53c2\u6570\u4fe1\u606f\u5373\u53ef\u4ece\u7a00\u758f\u8f93\u5165\u56fe\u50cf\u5b8c\u62103D\u914d\u51c6\u548c\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece2D\u56fe\u50cf\u51c6\u786e\u5bf9\u9f50\u548c\u91cd\u5efa3D\u6307\u7eb9\uff0c\u5e76\u4ece3D\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u65e0\u63a5\u89e6\u6307\u7eb9\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u65e0\u63a5\u89e6\u6307\u7eb9\u8bc6\u522b\u7684\u6027\u80fd\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5c063D\u9ad8\u65af\u6cfc\u6e85\u5e94\u7528\u4e8e\u6307\u7eb9\u8bc6\u522b\u7684\u5de5\u4f5c\uff0c\u4e3a\u65e0\u63a5\u89e6\u6307\u7eb9\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bc6\u522b\u6027\u80fd\u3002"}}
{"id": "2509.15677", "pdf": "https://arxiv.org/pdf/2509.15677", "abs": "https://arxiv.org/abs/2509.15677", "authors": ["Gahye Lee", "Hyomin Kim", "Gwangjin Ju", "Jooeun Son", "Hyejeong Yoon", "Seungyong Lee"], "title": "Camera Splatting for Continuous View Optimization", "categories": ["cs.CV"], "comment": null, "summary": "We propose Camera Splatting, a novel view optimization framework for novel view synthesis. Each camera is modeled as a 3D Gaussian, referred to as a camera splat, and virtual cameras, termed point cameras, are placed at 3D points sampled near the surface to observe the distribution of camera splats. View optimization is achieved by continuously and differentiably refining the camera splats so that desirable target distributions are observed from the point cameras, in a manner similar to the original 3D Gaussian splatting. Compared to the Farthest View Sampling (FVS) approach, our optimized views demonstrate superior performance in capturing complex view-dependent phenomena, including intense metallic reflections and intricate textures such as text.", "AI": {"tldr": "Camera Splatting\u662f\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u56fe\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u76f8\u673a\u5efa\u6a21\u4e3a3D\u9ad8\u65af\u5206\u5e03\uff08\u76f8\u673asplat\uff09\uff0c\u5728\u8868\u9762\u9644\u8fd1\u653e\u7f6e\u865a\u62df\u70b9\u76f8\u673a\u6765\u4f18\u5316\u89c6\u56fe\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u7684Farthest View Sampling (FVS)\u65b9\u6cd5\u5728\u6355\u6349\u590d\u6742\u89c6\u89d2\u76f8\u5173\u73b0\u8c61\uff08\u5982\u5f3a\u70c8\u91d1\u5c5e\u53cd\u5c04\u548c\u590d\u6742\u7eb9\u7406\uff09\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89c6\u56fe\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u5c06\u6bcf\u4e2a\u76f8\u673a\u5efa\u6a21\u4e3a3D\u9ad8\u65af\u5206\u5e03\uff08\u76f8\u673asplat\uff09\uff0c\u5728\u8868\u9762\u9644\u8fd1\u91c7\u68373D\u70b9\u653e\u7f6e\u865a\u62df\u70b9\u76f8\u673a\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u8fde\u7eed\u4f18\u5316\u76f8\u673asplat\u6765\u5b9e\u73b0\u89c6\u56fe\u4f18\u5316\uff0c\u7c7b\u4f3c\u4e8e\u539f\u59cb3D\u9ad8\u65afsplatting\u7684\u65b9\u6cd5\u3002", "result": "\u4e0eFVS\u65b9\u6cd5\u76f8\u6bd4\uff0cCamera Splatting\u5728\u6355\u6349\u590d\u6742\u89c6\u89d2\u76f8\u5173\u73b0\u8c61\u65b9\u9762\u8868\u73b0\u51fa\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u5f3a\u70c8\u91d1\u5c5e\u53cd\u5c04\u548c\u590d\u6742\u7eb9\u7406\uff08\u5982\u6587\u5b57\uff09\u3002", "conclusion": "Camera Splatting\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89c6\u56fe\u4f18\u5316\u6846\u67b6\uff0c\u5728\u590d\u6742\u573a\u666f\u7684\u89c6\u56fe\u5408\u6210\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edf\u7684FVS\u65b9\u6cd5\u3002"}}
{"id": "2509.15678", "pdf": "https://arxiv.org/pdf/2509.15678", "abs": "https://arxiv.org/abs/2509.15678", "authors": ["Sidra Hanif", "Longin Jan Latecki"], "title": "Layout Stroke Imitation: A Layout Guided Handwriting Stroke Generation for Style Imitation with Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "Handwriting stroke generation is crucial for improving the performance of tasks such as handwriting recognition and writers order recovery. In handwriting stroke generation, it is significantly important to imitate the sample calligraphic style. The previous studies have suggested utilizing the calligraphic features of the handwriting. However, they had not considered word spacing (word layout) as an explicit handwriting feature, which results in inconsistent word spacing for style imitation. Firstly, this work proposes multi-scale attention features for calligraphic style imitation. These multi-scale feature embeddings highlight the local and global style features. Secondly, we propose to include the words layout, which facilitates word spacing for handwriting stroke generation. Moreover, we propose a conditional diffusion model to predict strokes in contrast to previous work, which directly generated style images. Stroke generation provides additional temporal coordinate information, which is lacking in image generation. Hence, our proposed conditional diffusion model for stroke generation is guided by calligraphic style and word layout for better handwriting imitation and stroke generation in a calligraphic style. Our experimentation shows that the proposed diffusion model outperforms the current state-of-the-art stroke generation and is competitive with recent image generation networks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u624b\u5199\u7b14\u753b\u751f\u6210\u7684\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u7279\u5f81\u548c\u8bcd\u95f4\u8ddd\u5e03\u5c40\u6765\u6a21\u4eff\u4e66\u6cd5\u98ce\u683c\uff0c\u5728\u7b14\u753b\u751f\u6210\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u624b\u5199\u7b14\u753b\u751f\u6210\u5bf9\u4e8e\u63d0\u5347\u624b\u5199\u8bc6\u522b\u548c\u4e66\u5199\u987a\u5e8f\u6062\u590d\u7b49\u4efb\u52a1\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7814\u7a76\u672a\u80fd\u5c06\u8bcd\u95f4\u8ddd\u4f5c\u4e3a\u663e\u5f0f\u7684\u624b\u5199\u7279\u5f81\u8003\u8651\uff0c\u5bfc\u81f4\u98ce\u683c\u6a21\u4eff\u65f6\u8bcd\u95f4\u8ddd\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51fa\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u7279\u5f81\u7528\u4e8e\u4e66\u6cd5\u98ce\u683c\u6a21\u4eff\uff0c\u5f15\u5165\u8bcd\u5e03\u5c40\u4fe1\u606f\u4ee5\u4fc3\u8fdb\u8bcd\u95f4\u8ddd\u63a7\u5236\uff0c\u5e76\u8bbe\u8ba1\u6761\u4ef6\u6269\u6563\u6a21\u578b\u8fdb\u884c\u7b14\u753b\u751f\u6210\u800c\u975e\u76f4\u63a5\u751f\u6210\u98ce\u683c\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u6269\u6563\u6a21\u578b\u5728\u7b14\u753b\u751f\u6210\u4efb\u52a1\u4e0a\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u4e0e\u6700\u8fd1\u7684\u56fe\u50cf\u751f\u6210\u7f51\u7edc\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u4e66\u6cd5\u98ce\u683c\u548c\u8bcd\u5e03\u5c40\u6307\u5bfc\u7684\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u597d\u7684\u624b\u5199\u6a21\u4eff\u548c\u4e66\u6cd5\u98ce\u683c\u7684\u7b14\u753b\u751f\u6210\u3002"}}
{"id": "2509.15750", "pdf": "https://arxiv.org/pdf/2509.15750", "abs": "https://arxiv.org/abs/2509.15750", "authors": ["Han Ye", "Haofu Wang", "Yunchi Zhang", "Jiangjian Xiao", "Yuqiang Jin", "Jinyuan Liu", "Wen-An Zhang", "Uladzislau Sychou", "Alexander Tuzikov", "Vladislav Sobolevskii", "Valerii Zakharov", "Boris Sokolov", "Minglei Fu"], "title": "FloorSAM: SAM-Guided Floorplan Reconstruction with Semantic-Geometric Fusion", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 15 figures,", "summary": "Reconstructing building floor plans from point cloud data is key for indoor navigation, BIM, and precise measurements. Traditional methods like geometric algorithms and Mask R-CNN-based deep learning often face issues with noise, limited generalization, and loss of geometric details. We propose FloorSAM, a framework that integrates point cloud density maps with the Segment Anything Model (SAM) for accurate floor plan reconstruction from LiDAR data. Using grid-based filtering, adaptive resolution projection, and image enhancement, we create robust top-down density maps. FloorSAM uses SAM's zero-shot learning for precise room segmentation, improving reconstruction across diverse layouts. Room masks are generated via adaptive prompt points and multistage filtering, followed by joint mask and point cloud analysis for contour extraction and regularization. This produces accurate floor plans and recovers room topological relationships. Tests on Giblayout and ISPRS datasets show better accuracy, recall, and robustness than traditional methods, especially in noisy and complex settings. Code and materials: github.com/Silentbarber/FloorSAM.", "AI": {"tldr": "FloorSAM\u662f\u4e00\u4e2a\u96c6\u6210\u70b9\u4e91\u5bc6\u5ea6\u56fe\u548cSegment Anything Model\uff08SAM\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4eceLiDAR\u6570\u636e\u4e2d\u51c6\u786e\u91cd\u5efa\u5efa\u7b51\u5e73\u9762\u56fe\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u566a\u58f0\u3001\u6cdb\u5316\u80fd\u529b\u548c\u51e0\u4f55\u7ec6\u8282\u635f\u5931\u65b9\u9762\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5982\u51e0\u4f55\u7b97\u6cd5\u548c\u57fa\u4e8eMask R-CNN\u7684\u6df1\u5ea6\u5b66\u4e60\u5728\u70b9\u4e91\u6570\u636e\u91cd\u5efa\u5efa\u7b51\u5e73\u9762\u56fe\u65f6\u9762\u4e34\u566a\u58f0\u5e72\u6270\u3001\u6cdb\u5316\u80fd\u529b\u6709\u9650\u548c\u51e0\u4f55\u7ec6\u8282\u4e22\u5931\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u7f51\u683c\u8fc7\u6ee4\u3001\u81ea\u9002\u5e94\u5206\u8fa8\u7387\u6295\u5f71\u548c\u56fe\u50cf\u589e\u5f3a\u521b\u5efa\u9c81\u68d2\u7684\u4fef\u89c6\u5bc6\u5ea6\u56fe\uff0c\u7ed3\u5408SAM\u7684\u96f6\u6837\u672c\u5b66\u4e60\u8fdb\u884c\u7cbe\u786e\u623f\u95f4\u5206\u5272\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u63d0\u793a\u70b9\u548c\u591a\u9636\u6bb5\u8fc7\u6ee4\u751f\u6210\u623f\u95f4\u63a9\u7801\uff0c\u6700\u540e\u8fdb\u884c\u63a9\u7801\u548c\u70b9\u4e91\u8054\u5408\u5206\u6790\u4ee5\u63d0\u53d6\u548c\u6b63\u5219\u5316\u8f6e\u5ed3\u3002", "result": "\u5728Giblayout\u548cISPRS\u6570\u636e\u96c6\u4e0a\u7684\u6d4b\u8bd5\u663e\u793a\uff0cFloorSAM\u5728\u51c6\u786e\u6027\u3001\u53ec\u56de\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u566a\u58f0\u548c\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "FloorSAM\u901a\u8fc7\u7ed3\u5408\u70b9\u4e91\u5bc6\u5ea6\u56fe\u548cSAM\u7684\u96f6\u6837\u672c\u5b66\u4e60\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u91cd\u5efa\u51c6\u786e\u7684\u5efa\u7b51\u5e73\u9762\u56fe\u5e76\u6062\u590d\u623f\u95f4\u62d3\u6251\u5173\u7cfb\uff0c\u4e3a\u5ba4\u5185\u5bfc\u822a\u3001BIM\u548c\u7cbe\u786e\u6d4b\u91cf\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2509.15772", "pdf": "https://arxiv.org/pdf/2509.15772", "abs": "https://arxiv.org/abs/2509.15772", "authors": ["Weimin Bai", "Yubo Li", "Weijian Luo", "Wenzheng Chen", "He Sun"], "title": "Vision-Language Models as Differentiable Semantic and Spatial Rewards for Text-to-3D Generation", "categories": ["cs.CV"], "comment": null, "summary": "Score Distillation Sampling (SDS) enables high-quality text-to-3D generation by supervising 3D models through the denoising of multi-view 2D renderings, using a pretrained text-to-image diffusion model to align with the input prompt and ensure 3D consistency. However, existing SDS-based methods face two fundamental limitations: (1) their reliance on CLIP-style text encoders leads to coarse semantic alignment and struggles with fine-grained prompts; and (2) 2D diffusion priors lack explicit 3D spatial constraints, resulting in geometric inconsistencies and inaccurate object relationships in multi-object scenes. To address these challenges, we propose VLM3D, a novel text-to-3D generation framework that integrates large vision-language models (VLMs) into the SDS pipeline as differentiable semantic and spatial priors. Unlike standard text-to-image diffusion priors, VLMs leverage rich language-grounded supervision that enables fine-grained prompt alignment. Moreover, their inherent vision language modeling provides strong spatial understanding, which significantly enhances 3D consistency for single-object generation and improves relational reasoning in multi-object scenes. We instantiate VLM3D based on the open-source Qwen2.5-VL model and evaluate it on the GPTeval3D benchmark. Experiments across diverse objects and complex scenes show that VLM3D significantly outperforms prior SDS-based methods in semantic fidelity, geometric coherence, and spatial correctness.", "AI": {"tldr": "VLM3D\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6587\u672c\u52303D\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u5230SDS\u6d41\u7a0b\u4e2d\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u4e49\u5bf9\u9f50\u548c3D\u7a7a\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709SDS\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u6839\u672c\u9650\u5236\uff1a(1)\u4f9d\u8d56CLIP\u5f0f\u6587\u672c\u7f16\u7801\u5668\u5bfc\u81f4\u7c97\u7c92\u5ea6\u8bed\u4e49\u5bf9\u9f50\uff0c\u96be\u4ee5\u5904\u7406\u7ec6\u7c92\u5ea6\u63d0\u793a\uff1b(2)2D\u6269\u6563\u5148\u9a8c\u7f3a\u4e4f\u663e\u5f0f3D\u7a7a\u95f4\u7ea6\u675f\uff0c\u5bfc\u81f4\u591a\u5bf9\u8c61\u573a\u666f\u4e2d\u7684\u51e0\u4f55\u4e0d\u4e00\u81f4\u548c\u5bf9\u8c61\u5173\u7cfb\u4e0d\u51c6\u786e\u3002", "method": "\u63d0\u51faVLM3D\u6846\u67b6\uff0c\u5c06\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u53ef\u5fae\u5206\u8bed\u4e49\u548c\u7a7a\u95f4\u5148\u9a8c\u96c6\u6210\u5230SDS\u6d41\u7a0b\u4e2d\u3002\u57fa\u4e8e\u5f00\u6e90Qwen2.5-VL\u6a21\u578b\u5b9e\u73b0\uff0c\u5229\u7528\u5176\u4e30\u5bcc\u7684\u8bed\u8a00\u63a5\u5730\u76d1\u7763\u548c\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u3002", "result": "\u5728GPTeval3D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVLM3D\u5728\u591a\u6837\u5316\u5bf9\u8c61\u548c\u590d\u6742\u573a\u666f\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709SDS\u65b9\u6cd5\uff0c\u5728\u8bed\u4e49\u4fdd\u771f\u5ea6\u3001\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u7a7a\u95f4\u6b63\u786e\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "VLM3D\u901a\u8fc7\u96c6\u6210VLM\u5148\u9a8c\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6587\u672c\u52303D\u751f\u6210\u7684\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5bf9\u9f50\u548c3D\u7a7a\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2509.15871", "pdf": "https://arxiv.org/pdf/2509.15871", "abs": "https://arxiv.org/abs/2509.15871", "authors": ["Liwei Liao", "Xufeng Li", "Xiaoyun Zheng", "Boning Liu", "Feng Gao", "Ronggang Wang"], "title": "Zero-Shot Visual Grounding in 3D Gaussians via View Retrieval", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "3D Visual Grounding (3DVG) aims to locate objects in 3D scenes based on text prompts, which is essential for applications such as robotics. However, existing 3DVG methods encounter two main challenges: first, they struggle to handle the implicit representation of spatial textures in 3D Gaussian Splatting (3DGS), making per-scene training indispensable; second, they typically require larges amounts of labeled data for effective training. To this end, we propose \\underline{G}rounding via \\underline{V}iew \\underline{R}etrieval (GVR), a novel zero-shot visual grounding framework for 3DGS to transform 3DVG as a 2D retrieval task that leverages object-level view retrieval to collect grounding clues from multiple views, which not only avoids the costly process of 3D annotation, but also eliminates the need for per-scene training. Extensive experiments demonstrate that our method achieves state-of-the-art visual grounding performance while avoiding per-scene training, providing a solid foundation for zero-shot 3DVG research. Video demos can be found in https://github.com/leviome/GVR_demos.", "AI": {"tldr": "GVR\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u96f6\u6837\u672c3D\u89c6\u89c9\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u5c063D\u89c6\u89c9\u5b9a\u4f4d\u8f6c\u5316\u4e3a2D\u68c0\u7d22\u4efb\u52a1\uff0c\u907f\u514d\u4e86\u6602\u8d35\u76843D\u6807\u6ce8\u548c\u9010\u573a\u666f\u8bad\u7ec3\u7684\u9700\u6c42\u3002", "motivation": "\u73b0\u67093D\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u96be\u4ee5\u5904\u74063D\u9ad8\u65af\u6e85\u5c04\u4e2d\u7684\u9690\u5f0f\u7a7a\u95f4\u7eb9\u7406\u8868\u793a\uff0c\u9700\u8981\u9010\u573a\u666f\u8bad\u7ec3\uff1b\u901a\u5e38\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u6709\u6548\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u89c6\u56fe\u68c0\u7d22\u7684\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u5229\u7528\u5bf9\u8c61\u7ea7\u89c6\u56fe\u68c0\u7d22\u4ece\u591a\u4e2a\u89c6\u89d2\u6536\u96c6\u5b9a\u4f4d\u7ebf\u7d22\uff0c\u5c063D\u89c6\u89c9\u5b9a\u4f4d\u8f6c\u5316\u4e3a2D\u68c0\u7d22\u4efb\u52a1\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u907f\u514d\u9010\u573a\u666f\u8bad\u7ec3\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u5b9a\u4f4d\u6027\u80fd\u3002", "conclusion": "GVR\u4e3a\u96f6\u6837\u672c3D\u89c6\u89c9\u5b9a\u4f4d\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u6d88\u9664\u4e86\u5bf93D\u6807\u6ce8\u548c\u9010\u573a\u666f\u8bad\u7ec3\u7684\u4f9d\u8d56\u3002"}}
{"id": "2509.16017", "pdf": "https://arxiv.org/pdf/2509.16017", "abs": "https://arxiv.org/abs/2509.16017", "authors": ["Meng Yang", "Fan Fan", "Zizhuo Li", "Songchu Deng", "Yong Ma", "Jiayi Ma"], "title": "DistillMatch: Leveraging Knowledge Distillation from Vision Foundation Model for Multimodal Image Matching", "categories": ["cs.CV", "I.4.3; I.5.2"], "comment": "10 pages, 4 figures, 3 tables", "summary": "Multimodal image matching seeks pixel-level correspondences between images of different modalities, crucial for cross-modal perception, fusion and analysis. However, the significant appearance differences between modalities make this task challenging. Due to the scarcity of high-quality annotated datasets, existing deep learning methods that extract modality-common features for matching perform poorly and lack adaptability to diverse scenarios. Vision Foundation Model (VFM), trained on large-scale data, yields generalizable and robust feature representations adapted to data and tasks of various modalities, including multimodal matching. Thus, we propose DistillMatch, a multimodal image matching method using knowledge distillation from VFM. DistillMatch employs knowledge distillation to build a lightweight student model that extracts high-level semantic features from VFM (including DINOv2 and DINOv3) to assist matching across modalities. To retain modality-specific information, it extracts and injects modality category information into the other modality's features, which enhances the model's understanding of cross-modal correlations. Furthermore, we design V2I-GAN to boost the model's generalization by translating visible to pseudo-infrared images for data augmentation. Experiments show that DistillMatch outperforms existing algorithms on public datasets.", "AI": {"tldr": "DistillMatch\u662f\u4e00\u79cd\u591a\u6a21\u6001\u56fe\u50cf\u5339\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u6765\u63d0\u53d6\u9ad8\u5c42\u6b21\u8bed\u4e49\u7279\u5f81\uff0c\u5e76\u6ce8\u5165\u6a21\u6001\u7c7b\u522b\u4fe1\u606f\u6765\u589e\u5f3a\u8de8\u6a21\u6001\u76f8\u5173\u6027\u7406\u89e3\u3002", "motivation": "\u591a\u6a21\u6001\u56fe\u50cf\u5339\u914d\u9762\u4e34\u663e\u8457\u7684\u5916\u89c2\u5dee\u5f02\u6311\u6218\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7531\u4e8e\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u800c\u6027\u80fd\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u5bf9\u591a\u6837\u5316\u573a\u666f\u7684\u9002\u5e94\u6027\u3002\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5177\u6709\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u7279\u5f81\u8868\u793a\uff0c\u53ef\u7528\u4e8e\u591a\u6a21\u6001\u5339\u914d\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u84b8\u998f\u6784\u5efa\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578b\uff0c\u4eceDINOv2\u548cDINOv3\u7b49VFM\u63d0\u53d6\u9ad8\u5c42\u6b21\u8bed\u4e49\u7279\u5f81\uff1b\u6ce8\u5165\u6a21\u6001\u7c7b\u522b\u4fe1\u606f\u4fdd\u7559\u6a21\u6001\u7279\u5b9a\u4fe1\u606f\uff1b\u8bbe\u8ba1V2I-GAN\u8fdb\u884c\u53ef\u89c1\u5149\u5230\u4f2a\u7ea2\u5916\u56fe\u50cf\u8f6c\u6362\u4ee5\u589e\u5f3a\u6570\u636e\u589e\u5f3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDistillMatch\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u6a21\u6001\u4fe1\u606f\u6ce8\u5165\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u56fe\u50cf\u5339\u914d\u7684\u6311\u6218\uff0c\u5c55\u793a\u4e86VFM\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.16031", "pdf": "https://arxiv.org/pdf/2509.16031", "abs": "https://arxiv.org/abs/2509.16031", "authors": ["Tianyue Wang", "Shuang Yang", "Shiguang Shan", "Xilin Chen"], "title": "GLip: A Global-Local Integrated Progressive Framework for Robust Visual Speech Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Visual speech recognition (VSR), also known as lip reading, is the task of recognizing speech from silent video. Despite significant advancements in VSR over recent decades, most existing methods pay limited attention to real-world visual challenges such as illumination variations, occlusions, blurring, and pose changes. To address these challenges, we propose GLip, a Global-Local Integrated Progressive framework designed for robust VSR. GLip is built upon two key insights: (i) learning an initial \\textit{coarse} alignment between visual features across varying conditions and corresponding speech content facilitates the subsequent learning of \\textit{precise} visual-to-speech mappings in challenging environments; (ii) under adverse conditions, certain local regions (e.g., non-occluded areas) often exhibit more discriminative cues for lip reading than global features. To this end, GLip introduces a dual-path feature extraction architecture that integrates both global and local features within a two-stage progressive learning framework. In the first stage, the model learns to align both global and local visual features with corresponding acoustic speech units using easily accessible audio-visual data, establishing a coarse yet semantically robust foundation. In the second stage, we introduce a Contextual Enhancement Module (CEM) to dynamically integrate local features with relevant global context across both spatial and temporal dimensions, refining the coarse representations into precise visual-speech mappings. Our framework uniquely exploits discriminative local regions through a progressive learning strategy, demonstrating enhanced robustness against various visual challenges and consistently outperforming existing methods on the LRS2 and LRS3 benchmarks. We further validate its effectiveness on a newly introduced challenging Mandarin dataset.", "AI": {"tldr": "GLip\u662f\u4e00\u4e2a\u7528\u4e8e\u89c6\u89c9\u8bed\u97f3\u8bc6\u522b\u7684\u5168\u5c40-\u5c40\u90e8\u96c6\u6210\u6e10\u8fdb\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u8def\u5f84\u7279\u5f81\u63d0\u53d6\u548c\u4e24\u9636\u6bb5\u5b66\u4e60\u7b56\u7565\uff0c\u6709\u6548\u5e94\u5bf9\u5149\u7167\u53d8\u5316\u3001\u906e\u6321\u7b49\u73b0\u5b9e\u89c6\u89c9\u6311\u6218\u3002", "motivation": "\u73b0\u6709VSR\u65b9\u6cd5\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u89c6\u89c9\u6311\u6218\uff08\u5982\u5149\u7167\u53d8\u5316\u3001\u906e\u6321\u3001\u6a21\u7cca\u548c\u59ff\u6001\u53d8\u5316\uff09\u5173\u6ce8\u6709\u9650\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faGLip\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u8def\u5f84\u7279\u5f81\u63d0\u53d6\u67b6\u6784\uff0c\u96c6\u6210\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u6e10\u8fdb\u5b66\u4e60\uff1a\u7b2c\u4e00\u9636\u6bb5\u5b66\u4e60\u89c6\u89c9\u7279\u5f81\u4e0e\u8bed\u97f3\u5355\u5143\u7684\u7c97\u7565\u5bf9\u9f50\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u4e0a\u4e0b\u6587\u589e\u5f3a\u6a21\u5757\u52a8\u6001\u6574\u5408\u5c40\u90e8\u7279\u5f81\u4e0e\u5168\u5c40\u4e0a\u4e0b\u6587\u3002", "result": "\u5728LRS2\u548cLRS3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u65b0\u5f15\u5165\u7684\u6311\u6218\u6027\u4e2d\u6587\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "GLip\u901a\u8fc7\u6e10\u8fdb\u5f0f\u5b66\u4e60\u7b56\u7565\u6709\u6548\u5229\u7528\u5224\u522b\u6027\u5c40\u90e8\u533a\u57df\uff0c\u5728\u5404\u79cd\u89c6\u89c9\u6311\u6218\u4e0b\u8868\u73b0\u51fa\u589e\u5f3a\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.16091", "pdf": "https://arxiv.org/pdf/2509.16091", "abs": "https://arxiv.org/abs/2509.16091", "authors": ["Shen Cheng", "Haipeng Li", "Haibin Huang", "Xiaohong Liu", "Shuaicheng Liu"], "title": "Blind-Spot Guided Diffusion for Self-supervised Real-World Denoising", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we present Blind-Spot Guided Diffusion, a novel self-supervised framework for real-world image denoising. Our approach addresses two major challenges: the limitations of blind-spot networks (BSNs), which often sacrifice local detail and introduce pixel discontinuities due to spatial independence assumptions, and the difficulty of adapting diffusion models to self-supervised denoising. We propose a dual-branch diffusion framework that combines a BSN-based diffusion branch, generating semi-clean images, with a conventional diffusion branch that captures underlying noise distributions. To enable effective training without paired data, we use the BSN-based branch to guide the sampling process, capturing noise structure while preserving local details. Extensive experiments on the SIDD and DND datasets demonstrate state-of-the-art performance, establishing our method as a highly effective self-supervised solution for real-world denoising. Code and pre-trained models are released at: https://github.com/Sumching/BSGD.", "AI": {"tldr": "\u63d0\u51faBlind-Spot Guided Diffusion\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u6269\u6563\u6a21\u578b\u89e3\u51b3\u76f2\u70b9\u7f51\u7edc\u5728\u81ea\u76d1\u7763\u56fe\u50cf\u53bb\u566a\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5728SIDD\u548cDND\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u76f2\u70b9\u7f51\u7edc\u5728\u81ea\u76d1\u7763\u53bb\u566a\u4e2d\u727a\u7272\u5c40\u90e8\u7ec6\u8282\u548c\u5f15\u5165\u50cf\u7d20\u4e0d\u8fde\u7eed\u6027\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u6269\u6563\u6a21\u578b\u5728\u81ea\u76d1\u7763\u53bb\u566a\u4e2d\u96be\u4ee5\u9002\u5e94\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u6269\u6563\u6846\u67b6\uff1aBSN\u6269\u6563\u5206\u652f\u751f\u6210\u534a\u6e05\u6d01\u56fe\u50cf\uff0c\u5e38\u89c4\u6269\u6563\u5206\u652f\u6355\u6349\u566a\u58f0\u5206\u5e03\uff1b\u4f7f\u7528BSN\u5206\u652f\u6307\u5bfc\u91c7\u6837\u8fc7\u7a0b\uff0c\u5728\u65e0\u914d\u5bf9\u6570\u636e\u4e0b\u6709\u6548\u8bad\u7ec3\u3002", "result": "\u5728SIDD\u548cDND\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u5353\u8d8a\u6027\u80fd\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u53bb\u566a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u81ea\u76d1\u7763\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.16119", "pdf": "https://arxiv.org/pdf/2509.16119", "abs": "https://arxiv.org/abs/2509.16119", "authors": ["Weiyi Xiong", "Bing Zhu", "Tao Huang", "Zewei Zheng"], "title": "RadarGaussianDet3D: An Efficient and Effective Gaussian-based 3D Detector with 4D Automotive Radars", "categories": ["cs.CV"], "comment": null, "summary": "4D automotive radars have gained increasing attention for autonomous driving due to their low cost, robustness, and inherent velocity measurement capability. However, existing 4D radar-based 3D detectors rely heavily on pillar encoders for BEV feature extraction, where each point contributes to only a single BEV grid, resulting in sparse feature maps and degraded representation quality. In addition, they also optimize bounding box attributes independently, leading to sub-optimal detection accuracy. Moreover, their inference speed, while sufficient for high-end GPUs, may fail to meet the real-time requirement on vehicle-mounted embedded devices. To overcome these limitations, an efficient and effective Gaussian-based 3D detector, namely RadarGaussianDet3D is introduced, leveraging Gaussian primitives and distributions as intermediate representations for radar points and bounding boxes. In RadarGaussianDet3D, a novel Point Gaussian Encoder (PGE) is designed to transform each point into a Gaussian primitive after feature aggregation and employs the 3D Gaussian Splatting (3DGS) technique for BEV rasterization, yielding denser feature maps. PGE exhibits exceptionally low latency, owing to the optimized algorithm for point feature aggregation and fast rendering of 3DGS. In addition, a new Box Gaussian Loss (BGL) is proposed, which converts bounding boxes into 3D Gaussian distributions and measures their distance to enable more comprehensive and consistent optimization. Extensive experiments on TJ4DRadSet and View-of-Delft demonstrate that RadarGaussianDet3D achieves state-of-the-art detection accuracy while delivering substantially faster inference, highlighting its potential for real-time deployment in autonomous driving.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u5206\u5e03\u76843D\u96f7\u8fbe\u68c0\u6d4b\u5668RadarGaussianDet3D\uff0c\u901a\u8fc7\u9ad8\u65af\u539f\u8bed\u548c\u5206\u5e03\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u73b0\u67094D\u96f7\u8fbe\u68c0\u6d4b\u5668\u7279\u5f81\u7a00\u758f\u3001\u68c0\u6d4b\u7cbe\u5ea6\u4e0d\u8db3\u548c\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u67094D\u96f7\u8fbe3D\u68c0\u6d4b\u5668\u4f9d\u8d56pillar\u7f16\u7801\u5668\u8fdb\u884cBEV\u7279\u5f81\u63d0\u53d6\uff0c\u5bfc\u81f4\u7279\u5f81\u56fe\u7a00\u758f\u3001\u8868\u793a\u8d28\u91cf\u4e0b\u964d\uff1b\u8fb9\u754c\u6846\u5c5e\u6027\u72ec\u7acb\u4f18\u5316\u5bfc\u81f4\u68c0\u6d4b\u7cbe\u5ea6\u4e0d\u8db3\uff1b\u5728\u9ad8\u6027\u80fdGPU\u4e0a\u63a8\u7406\u901f\u5ea6\u5c1a\u53ef\uff0c\u4f46\u5728\u8f66\u8f7d\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u6027\u8981\u6c42\u3002", "method": "\u8bbe\u8ba1\u4e86Point Gaussian Encoder (PGE)\u5c06\u6bcf\u4e2a\u70b9\u8f6c\u6362\u4e3a\u9ad8\u65af\u539f\u8bed\uff0c\u91c7\u75283D Gaussian Splatting\u6280\u672f\u8fdb\u884cBEV\u6805\u683c\u5316\uff0c\u751f\u6210\u66f4\u5bc6\u96c6\u7684\u7279\u5f81\u56fe\uff1b\u63d0\u51faBox Gaussian Loss (BGL)\u5c06\u8fb9\u754c\u6846\u8f6c\u6362\u4e3a3D\u9ad8\u65af\u5206\u5e03\u5e76\u6d4b\u91cf\u8ddd\u79bb\uff0c\u5b9e\u73b0\u66f4\u5168\u9762\u4e00\u81f4\u7684\u4f18\u5316\u3002", "result": "\u5728TJ4DRadSet\u548cView-of-Delft\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRadarGaussianDet3D\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "RadarGaussianDet3D\u5728\u68c0\u6d4b\u7cbe\u5ea6\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5b9e\u65f6\u90e8\u7f72\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.16141", "pdf": "https://arxiv.org/pdf/2509.16141", "abs": "https://arxiv.org/abs/2509.16141", "authors": ["Vatsal Malaviya", "Agneet Chatterjee", "Maitreya Patel", "Yezhou Yang", "Chitta Baral"], "title": "AcT2I: Evaluating and Improving Action Depiction in Text-to-Image Models", "categories": ["cs.CV"], "comment": "Project Page : https://vatsal-malaviya.github.io/AcT2I/", "summary": "Text-to-Image (T2I) models have recently achieved remarkable success in generating images from textual descriptions. However, challenges still persist in accurately rendering complex scenes where actions and interactions form the primary semantic focus. Our key observation in this work is that T2I models frequently struggle to capture nuanced and often implicit attributes inherent in action depiction, leading to generating images that lack key contextual details. To enable systematic evaluation, we introduce AcT2I, a benchmark designed to evaluate the performance of T2I models in generating images from action-centric prompts. We experimentally validate that leading T2I models do not fare well on AcT2I. We further hypothesize that this shortcoming arises from the incomplete representation of the inherent attributes and contextual dependencies in the training corpora of existing T2I models. We build upon this by developing a training-free, knowledge distillation technique utilizing Large Language Models to address this limitation. Specifically, we enhance prompts by incorporating dense information across three dimensions, observing that injecting prompts with temporal details significantly improves image generation accuracy, with our best model achieving an increase of 72%. Our findings highlight the limitations of current T2I methods in generating images that require complex reasoning and demonstrate that integrating linguistic knowledge in a systematic way can notably advance the generation of nuanced and contextually accurate images.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86AcT2I\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u751f\u6210\u52a8\u4f5c\u4e2d\u5fc3\u63d0\u793a\u56fe\u50cf\u65f6\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u6765\u589e\u5f3a\u63d0\u793a\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u56fe\u50cf\u751f\u6210\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u52a8\u4f5c\u548c\u4ea4\u4e92\u573a\u666f\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u52a8\u4f5c\u63cf\u7ed8\u4e2d\u7684\u7ec6\u5fae\u548c\u9690\u542b\u5c5e\u6027\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u56fe\u50cf\u7f3a\u4e4f\u5173\u952e\u4e0a\u4e0b\u6587\u7ec6\u8282\u3002", "method": "\u5f00\u53d1\u4e86\u8bad\u7ec3\u514d\u8d39\u7684\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e09\u4e2a\u7ef4\u5ea6\u4e0a\u589e\u5f3a\u63d0\u793a\u4fe1\u606f\uff0c\u7279\u522b\u662f\u6ce8\u5165\u65f6\u95f4\u7ec6\u8282\uff0c\u4ee5\u6539\u5584\u56fe\u50cf\u751f\u6210\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\uff0c\u9886\u5148\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728AcT2I\u57fa\u51c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u901a\u8fc7\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u6700\u4f73\u6a21\u578b\u5b9e\u73b0\u4e8672%\u7684\u51c6\u786e\u6027\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u65b9\u6cd5\u5728\u9700\u8981\u590d\u6742\u63a8\u7406\u7684\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u8bc1\u660e\u7cfb\u7edf\u6574\u5408\u8bed\u8a00\u77e5\u8bc6\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u751f\u6210\u7ec6\u5fae\u548c\u4e0a\u4e0b\u6587\u51c6\u786e\u56fe\u50cf\u7684\u80fd\u529b\u3002"}}
{"id": "2509.16197", "pdf": "https://arxiv.org/pdf/2509.16197", "abs": "https://arxiv.org/abs/2509.16197", "authors": ["Yanghao Li", "Rui Qian", "Bowen Pan", "Haotian Zhang", "Haoshuo Huang", "Bowen Zhang", "Jialing Tong", "Haoxuan You", "Xianzhi Du", "Zhe Gan", "Hyunjik Kim", "Chao Jia", "Zhenbang Wang", "Yinfei Yang", "Mingfei Gao", "Zi-Yi Dou", "Wenze Hu", "Chang Gao", "Dongxu Li", "Philipp Dufter", "Zirui Wang", "Guoli Yin", "Zhengdong Zhang", "Chen Chen", "Yang Zhao", "Ruoming Pang", "Zhifeng Chen"], "title": "MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Unified multimodal Large Language Models (LLMs) that can both understand and generate visual content hold immense potential. However, existing open-source models often suffer from a performance trade-off between these capabilities. We present Manzano, a simple and scalable unified framework that substantially reduces this tension by coupling a hybrid image tokenizer with a well-curated training recipe. A single shared vision encoder feeds two lightweight adapters that produce continuous embeddings for image-to-text understanding and discrete tokens for text-to-image generation within a common semantic space. A unified autoregressive LLM predicts high-level semantics in the form of text and image tokens, with an auxiliary diffusion decoder subsequently translating the image tokens into pixels. The architecture, together with a unified training recipe over understanding and generation data, enables scalable joint learning of both capabilities. Manzano achieves state-of-the-art results among unified models, and is competitive with specialist models, particularly on text-rich evaluation. Our studies show minimal task conflicts and consistent gains from scaling model size, validating our design choice of a hybrid tokenizer.", "AI": {"tldr": "Manzano\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u56fe\u50cf\u5206\u8bcd\u5668\u548c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u56fe\u50cf\u7406\u89e3\u548c\u56fe\u50cf\u751f\u6210\u80fd\u529b\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u7684\u5f00\u6e90\u591a\u6a21\u6001\u6a21\u578b\u5728\u7406\u89e3\u80fd\u529b\u548c\u751f\u6210\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u6027\u80fd\u6743\u8861\uff0c\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u540c\u65f6\u4f18\u5316\u8fd9\u4e24\u79cd\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u5171\u4eab\u7684\u89c6\u89c9\u7f16\u7801\u5668\u914d\u5408\u4e24\u4e2a\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\uff0c\u5206\u522b\u751f\u6210\u7528\u4e8e\u56fe\u50cf\u7406\u89e3\u7684\u8fde\u7eed\u5d4c\u5165\u548c\u7528\u4e8e\u56fe\u50cf\u751f\u6210\u7684\u79bb\u6563\u6807\u8bb0\u3002\u91c7\u7528\u7edf\u4e00\u7684\u81ea\u52a8\u56de\u5f52LLM\u9884\u6d4b\u6587\u672c\u548c\u56fe\u50cf\u6807\u8bb0\uff0c\u914d\u5408\u8f85\u52a9\u6269\u6563\u89e3\u7801\u5668\u5c06\u56fe\u50cf\u6807\u8bb0\u8f6c\u6362\u4e3a\u50cf\u7d20\u3002", "result": "Manzano\u5728\u7edf\u4e00\u6a21\u578b\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u6587\u672c\u4e30\u5bcc\u7684\u8bc4\u4f30\u4e2d\u4e0e\u4e13\u4e1a\u6a21\u578b\u7ade\u4e89\u529b\u76f8\u5f53\u3002\u7814\u7a76\u8868\u660e\u4efb\u52a1\u51b2\u7a81\u6700\u5c0f\uff0c\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u5e26\u6765\u4e00\u81f4\u6536\u76ca\u3002", "conclusion": "\u6df7\u5408\u5206\u8bcd\u5668\u7684\u8bbe\u8ba1\u9009\u62e9\u5f97\u5230\u9a8c\u8bc1\uff0c\u8be5\u6846\u67b6\u4e3a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u7406\u89e3\u4e0e\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.15328", "pdf": "https://arxiv.org/pdf/2509.15328", "abs": "https://arxiv.org/abs/2509.15328", "authors": ["Yue Song", "T. Anderson Keller", "Sevan Brodjian", "Takeru Miyato", "Yisong Yue", "Pietro Perona", "Max Welling"], "title": "Kuramoto Orientation Diffusion Models", "categories": ["cs.LG", "cs.CV", "q-bio.NC"], "comment": "NeurIPS 2025", "summary": "Orientation-rich images, such as fingerprints and textures, often exhibit coherent angular directional patterns that are challenging to model using standard generative approaches based on isotropic Euclidean diffusion. Motivated by the role of phase synchronization in biological systems, we propose a score-based generative model built on periodic domains by leveraging stochastic Kuramoto dynamics in the diffusion process. In neural and physical systems, Kuramoto models capture synchronization phenomena across coupled oscillators -- a behavior that we re-purpose here as an inductive bias for structured image generation. In our framework, the forward process performs \\textit{synchronization} among phase variables through globally or locally coupled oscillator interactions and attraction to a global reference phase, gradually collapsing the data into a low-entropy von Mises distribution. The reverse process then performs \\textit{desynchronization}, generating diverse patterns by reversing the dynamics with a learned score function. This approach enables structured destruction during forward diffusion and a hierarchical generation process that progressively refines global coherence into fine-scale details. We implement wrapped Gaussian transition kernels and periodicity-aware networks to account for the circular geometry. Our method achieves competitive results on general image benchmarks and significantly improves generation quality on orientation-dense datasets like fingerprints and textures. Ultimately, this work demonstrates the promise of biologically inspired synchronization dynamics as structured priors in generative modeling.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eKuramoto\u540c\u6b65\u52a8\u529b\u5b66\u7684\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u5904\u7406\u5177\u6709\u65b9\u5411\u6027\u6a21\u5f0f\u7684\u56fe\u50cf\uff08\u5982\u6307\u7eb9\u548c\u7eb9\u7406\uff09\uff0c\u901a\u8fc7\u540c\u6b65-\u53bb\u540c\u6b65\u8fc7\u7a0b\u5b9e\u73b0\u7ed3\u6784\u5316\u751f\u6210\u3002", "motivation": "\u4f20\u7edf\u5404\u5411\u540c\u6027\u6b27\u51e0\u91cc\u5f97\u6269\u6563\u96be\u4ee5\u6709\u6548\u5efa\u6a21\u5177\u6709\u76f8\u5e72\u89d2\u5ea6\u65b9\u5411\u6a21\u5f0f\u7684\u56fe\u50cf\uff0c\u53d7\u751f\u7269\u7cfb\u7edf\u4e2d\u76f8\u4f4d\u540c\u6b65\u73b0\u8c61\u542f\u53d1\uff0c\u5229\u7528Kuramoto\u52a8\u529b\u5b66\u4f5c\u4e3a\u7ed3\u6784\u5316\u5148\u9a8c\u3002", "method": "\u4f7f\u7528\u968f\u673aKuramoto\u52a8\u529b\u5b66\u6784\u5efa\u5206\u6570\u751f\u6210\u6a21\u578b\uff0c\u524d\u5411\u8fc7\u7a0b\u901a\u8fc7\u5168\u5c40/\u5c40\u90e8\u8026\u5408\u632f\u8361\u5668\u76f8\u4e92\u4f5c\u7528\u5b9e\u73b0\u76f8\u4f4d\u540c\u6b65\uff0c\u53cd\u5411\u8fc7\u7a0b\u901a\u8fc7\u5b66\u4e60\u7684\u5206\u6570\u51fd\u6570\u8fdb\u884c\u53bb\u540c\u6b65\u751f\u6210\u3002", "result": "\u5728\u901a\u7528\u56fe\u50cf\u57fa\u51c6\u4e0a\u53d6\u5f97\u7ade\u4e89\u6027\u7ed3\u679c\uff0c\u5728\u6307\u7eb9\u548c\u7eb9\u7406\u7b49\u65b9\u5411\u5bc6\u96c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "\u751f\u7269\u542f\u53d1\u7684\u540c\u6b65\u52a8\u529b\u5b66\u4f5c\u4e3a\u7ed3\u6784\u5316\u5148\u9a8c\u5728\u751f\u6210\u5efa\u6a21\u4e2d\u5177\u6709\u5e7f\u9614\u524d\u666f\u3002"}}
{"id": "2509.15363", "pdf": "https://arxiv.org/pdf/2509.15363", "abs": "https://arxiv.org/abs/2509.15363", "authors": ["Debasish Dutta", "Neeharika Sonowal", "Risheraj Barauh", "Deepjyoti Chetia", "Sanjib Kr Kalita"], "title": "Recent Advancements in Microscopy Image Enhancement using Deep Learning: A Survey", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "7 pages, 3 figures and 1 table. 2024 IEEE International Conference on   Computer Vision and Machine Intelligence (CVMI). IEEE, 2024", "summary": "Microscopy image enhancement plays a pivotal role in understanding the details of biological cells and materials at microscopic scales. In recent years, there has been a significant rise in the advancement of microscopy image enhancement, specifically with the help of deep learning methods. This survey paper aims to provide a snapshot of this rapidly growing state-of-the-art method, focusing on its evolution, applications, challenges, and future directions. The core discussions take place around the key domains of microscopy image enhancement of super-resolution, reconstruction, and denoising, with each domain explored in terms of its current trends and their practical utility of deep learning.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u6982\u8ff0\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u663e\u5fae\u955c\u56fe\u50cf\u589e\u5f3a\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u91cd\u70b9\u5173\u6ce8\u8d85\u5206\u8fa8\u7387\u3001\u91cd\u5efa\u548c\u53bb\u566a\u4e09\u4e2a\u5173\u952e\u9886\u57df\u7684\u53d1\u5c55\u3001\u5e94\u7528\u3001\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u663e\u5fae\u955c\u56fe\u50cf\u589e\u5f3a\u5bf9\u4e8e\u7406\u89e3\u751f\u7269\u7ec6\u80de\u548c\u6750\u6599\u7684\u5fae\u89c2\u7ec6\u8282\u81f3\u5173\u91cd\u8981\u3002\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u5feb\u901f\u53d1\u5c55\u663e\u8457\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u6b65\uff0c\u9700\u8981\u5bf9\u8fd9\u4e9b\u6700\u65b0\u6280\u672f\u8fdb\u884c\u7cfb\u7edf\u6027\u603b\u7ed3\u3002", "method": "\u91c7\u7528\u7efc\u8ff0\u7814\u7a76\u65b9\u6cd5\uff0c\u5bf9\u663e\u5fae\u955c\u56fe\u50cf\u589e\u5f3a\u9886\u57df\uff08\u7279\u522b\u662f\u8d85\u5206\u8fa8\u7387\u3001\u91cd\u5efa\u548c\u53bb\u566a\u4e09\u4e2a\u6838\u5fc3\u9886\u57df\uff09\u7684\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u8fdb\u884c\u7cfb\u7edf\u6027\u68b3\u7406\u548c\u5206\u6790\u3002", "result": "\u63d0\u4f9b\u4e86\u8be5\u9886\u57df\u5feb\u901f\u53d1\u5c55\u73b0\u72b6\u7684\u5feb\u7167\uff0c\u603b\u7ed3\u4e86\u5f53\u524d\u8d8b\u52bf\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u5728\u663e\u5fae\u955c\u56fe\u50cf\u589e\u5f3a\u9886\u57df\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u53d1\u5c55\u3002"}}
{"id": "2509.15422", "pdf": "https://arxiv.org/pdf/2509.15422", "abs": "https://arxiv.org/abs/2509.15422", "authors": ["Edward P. Chandler", "Shirin Shoushtari", "Brendt Wohlberg", "Ulugbek S. Kamilov"], "title": "Analysis Plug-and-Play Methods for Imaging Inverse Problems", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Plug-and-Play Priors (PnP) is a popular framework for solving imaging inverse problems by integrating learned priors in the form of denoisers trained to remove Gaussian noise from images. In standard PnP methods, the denoiser is applied directly in the image domain, serving as an implicit prior on natural images. This paper considers an alternative analysis formulation of PnP, in which the prior is imposed on a transformed representation of the image, such as its gradient. Specifically, we train a Gaussian denoiser to operate in the gradient domain, rather than on the image itself. Conceptually, this is an extension of total variation (TV) regularization to learned TV regularization. To incorporate this gradient-domain prior in image reconstruction algorithms, we develop two analysis PnP algorithms based on half-quadratic splitting (APnP-HQS) and the alternating direction method of multipliers (APnP-ADMM). We evaluate our approach on image deblurring and super-resolution, demonstrating that the analysis formulation achieves performance comparable to image-domain PnP algorithms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u57df\u7684PnP\uff08Plug-and-Play Priors\uff09\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u68af\u5ea6\u57df\u800c\u975e\u56fe\u50cf\u57df\u8bad\u7ec3\u53bb\u566a\u5668\uff0c\u5c06\u603b\u53d8\u5dee\u6b63\u5219\u5316\u6269\u5c55\u4e3a\u5b66\u4e60\u578bTV\u6b63\u5219\u5316\uff0c\u5e76\u5f00\u53d1\u4e86\u4e24\u79cd\u5206\u6790PnP\u7b97\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684PnP\u65b9\u6cd5\u76f4\u63a5\u5728\u56fe\u50cf\u57df\u5e94\u7528\u53bb\u566a\u5668\u4f5c\u4e3a\u9690\u5f0f\u5148\u9a8c\uff0c\u672c\u6587\u63a2\u7d22\u5728\u56fe\u50cf\u7684\u53d8\u6362\u8868\u793a\uff08\u5982\u68af\u5ea6\u57df\uff09\u4e0a\u65bd\u52a0\u5148\u9a8c\u7684\u66ff\u4ee3\u5206\u6790\u516c\u5f0f\uff0c\u8fd9\u53ef\u4ee5\u770b\u4f5c\u662f\u603b\u53d8\u5dee\u6b63\u5219\u5316\u7684\u5b66\u4e60\u578b\u6269\u5c55\u3002", "method": "\u8bad\u7ec3\u5728\u68af\u5ea6\u57df\u64cd\u4f5c\u7684\u9ad8\u65af\u53bb\u566a\u5668\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8e\u534a\u4e8c\u6b21\u5206\u88c2\uff08APnP-HQS\uff09\u548c\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5\uff08APnP-ADMM\uff09\u7684\u4e24\u79cd\u5206\u6790PnP\u7b97\u6cd5\u3002", "result": "\u5728\u56fe\u50cf\u53bb\u6a21\u7cca\u548c\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u5206\u6790\u516c\u5f0f\u7684\u6027\u80fd\u4e0e\u56fe\u50cf\u57dfPnP\u7b97\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u68af\u5ea6\u57dfPnP\u65b9\u6cd5\u4e3a\u56fe\u50cf\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5c06\u7ecf\u5178TV\u6b63\u5219\u5316\u6210\u529f\u6269\u5c55\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u5b66\u4e60\u578b\u6b63\u5219\u5316\u65b9\u6cd5\u3002"}}
{"id": "2509.15591", "pdf": "https://arxiv.org/pdf/2509.15591", "abs": "https://arxiv.org/abs/2509.15591", "authors": ["Zinan Lin", "Enshu Liu", "Xuefei Ning", "Junyi Zhu", "Wenyu Wang", "Sergey Yekhanin"], "title": "Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Published in NeurIPS 2025", "summary": "Generative modeling, representation learning, and classification are three core problems in machine learning (ML), yet their state-of-the-art (SoTA) solutions remain largely disjoint. In this paper, we ask: Can a unified principle address all three? Such unification could simplify ML pipelines and foster greater synergy across tasks. We introduce Latent Zoning Network (LZN) as a step toward this goal. At its core, LZN creates a shared Gaussian latent space that encodes information across all tasks. Each data type (e.g., images, text, labels) is equipped with an encoder that maps samples to disjoint latent zones, and a decoder that maps latents back to data. ML tasks are expressed as compositions of these encoders and decoders: for example, label-conditional image generation uses a label encoder and image decoder; image embedding uses an image encoder; classification uses an image encoder and label decoder. We demonstrate the promise of LZN in three increasingly complex scenarios: (1) LZN can enhance existing models (image generation): When combined with the SoTA Rectified Flow model, LZN improves FID on CIFAR10 from 2.76 to 2.59-without modifying the training objective. (2) LZN can solve tasks independently (representation learning): LZN can implement unsupervised representation learning without auxiliary loss functions, outperforming the seminal MoCo and SimCLR methods by 9.3% and 0.2%, respectively, on downstream linear classification on ImageNet. (3) LZN can solve multiple tasks simultaneously (joint generation and classification): With image and label encoders/decoders, LZN performs both tasks jointly by design, improving FID and achieving SoTA classification accuracy on CIFAR10. The code and trained models are available at https://github.com/microsoft/latent-zoning-networks. The project website is at https://zinanlin.me/blogs/latent_zoning_networks.html.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLatent Zoning Network (LZN)\uff0c\u901a\u8fc7\u521b\u5efa\u5171\u4eab\u9ad8\u65af\u6f5c\u7a7a\u95f4\u6765\u7edf\u4e00\u751f\u6210\u5efa\u6a21\u3001\u8868\u793a\u5b66\u4e60\u548c\u5206\u7c7b\u4e09\u5927\u673a\u5668\u5b66\u4e60\u4efb\u52a1\uff0c\u7b80\u5316ML\u6d41\u7a0b\u5e76\u4fc3\u8fdb\u4efb\u52a1\u95f4\u534f\u540c\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u5b66\u4e60\u4e2d\u751f\u6210\u5efa\u6a21\u3001\u8868\u793a\u5b66\u4e60\u548c\u5206\u7c7b\u7684\u6700\u5148\u8fdb\u89e3\u51b3\u65b9\u6848\u76f8\u4e92\u72ec\u7acb\uff0c\u7f3a\u4e4f\u7edf\u4e00\u539f\u5219\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u80fd\u5426\u7528\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u540c\u65f6\u89e3\u51b3\u8fd9\u4e09\u4e2a\u6838\u5fc3\u95ee\u9898\u3002", "method": "LZN\u6784\u5efa\u5171\u4eab\u9ad8\u65af\u6f5c\u7a7a\u95f4\uff0c\u4e3a\u6bcf\u79cd\u6570\u636e\u7c7b\u578b\uff08\u5982\u56fe\u50cf\u3001\u6587\u672c\u3001\u6807\u7b7e\uff09\u914d\u5907\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u3002ML\u4efb\u52a1\u901a\u8fc7\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u7ec4\u5408\u5b9e\u73b0\uff1a\u6761\u4ef6\u751f\u6210\u4f7f\u7528\u6807\u7b7e\u7f16\u7801\u5668\u548c\u56fe\u50cf\u89e3\u7801\u5668\uff0c\u8868\u793a\u5b66\u4e60\u4f7f\u7528\u56fe\u50cf\u7f16\u7801\u5668\uff0c\u5206\u7c7b\u4f7f\u7528\u56fe\u50cf\u7f16\u7801\u5668\u548c\u6807\u7b7e\u89e3\u7801\u5668\u3002", "result": "\u5728\u4e09\u4e2a\u573a\u666f\u4e2d\u9a8c\u8bc1LZN\uff1a1\uff09\u4e0eRectified Flow\u7ed3\u5408\u5c06CIFAR10\u7684FID\u4ece2.76\u63d0\u5347\u52302.59\uff1b2\uff09\u65e0\u76d1\u7763\u8868\u793a\u5b66\u4e60\u5728\u7ebf\u6027\u5206\u7c7b\u4e0a\u4f18\u4e8eMoCo\u548cSimCLR\uff1b3\uff09\u5728CIFAR10\u4e0a\u540c\u65f6\u5b9e\u73b0\u751f\u6210\u548c\u5206\u7c7b\u4efb\u52a1\uff0c\u83b7\u5f97SOTA\u5206\u7c7b\u7cbe\u5ea6\u3002", "conclusion": "LZN\u5c55\u793a\u4e86\u7edf\u4e00\u6846\u67b6\u89e3\u51b3\u591a\u4efb\u52a1ML\u95ee\u9898\u7684\u6f5c\u529b\uff0c\u4e3a\u7b80\u5316\u673a\u5668\u5b66\u4e60\u6d41\u7a0b\u548c\u4fc3\u8fdb\u4efb\u52a1\u534f\u540c\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2509.15814", "pdf": "https://arxiv.org/pdf/2509.15814", "abs": "https://arxiv.org/abs/2509.15814", "authors": ["Qijun Yang", "Yating Huang", "Lintao Xiang", "Hujun Yin"], "title": "QWD-GAN: Quality-aware Wavelet-driven GAN for Unsupervised Medical Microscopy Images Denoising", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Image denoising plays a critical role in biomedical and microscopy imaging, especially when acquiring wide-field fluorescence-stained images. This task faces challenges in multiple fronts, including limitations in image acquisition conditions, complex noise types, algorithm adaptability, and clinical application demands. Although many deep learning-based denoising techniques have demonstrated promising results, further improvements are needed in preserving image details, enhancing algorithmic efficiency, and increasing clinical interpretability. We propose an unsupervised image denoising method based on a Generative Adversarial Network (GAN) architecture. The approach introduces a multi-scale adaptive generator based on the Wavelet Transform and a dual-branch discriminator that integrates difference perception feature maps with original features. Experimental results on multiple biomedical microscopy image datasets show that the proposed model achieves state-of-the-art denoising performance, particularly excelling in the preservation of high-frequency information. Furthermore, the dual-branch discriminator is seamlessly compatible with various GAN frameworks. The proposed quality-aware, wavelet-driven GAN denoising model is termed as QWD-GAN.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGAN\u7684\u65e0\u76d1\u7763\u56fe\u50cf\u53bb\u566a\u65b9\u6cd5QWD-GAN\uff0c\u901a\u8fc7\u5c0f\u6ce2\u53d8\u6362\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u751f\u6210\u5668\u548c\u53cc\u5206\u652f\u5224\u522b\u5668\uff0c\u5728\u751f\u7269\u533b\u5b66\u663e\u5fae\u955c\u56fe\u50cf\u53bb\u566a\u4e2d\u53d6\u5f97\u4f18\u5f02\u6548\u679c\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u663e\u5fae\u955c\u56fe\u50cf\u53bb\u566a\u9762\u4e34\u91c7\u96c6\u6761\u4ef6\u9650\u5236\u3001\u590d\u6742\u566a\u58f0\u7c7b\u578b\u3001\u7b97\u6cd5\u9002\u5e94\u6027\u548c\u4e34\u5e8a\u9700\u6c42\u7b49\u591a\u91cd\u6311\u6218\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u56fe\u50cf\u7ec6\u8282\u4fdd\u7559\u3001\u7b97\u6cd5\u6548\u7387\u548c\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4ecd\u9700\u6539\u8fdb\u3002", "method": "\u91c7\u7528GAN\u67b6\u6784\uff0c\u5f15\u5165\u57fa\u4e8e\u5c0f\u6ce2\u53d8\u6362\u7684\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u751f\u6210\u5668\uff0c\u4ee5\u53ca\u96c6\u6210\u5dee\u5f02\u611f\u77e5\u7279\u5f81\u56fe\u4e0e\u539f\u59cb\u7279\u5f81\u7684\u53cc\u5206\u652f\u5224\u522b\u5668\u3002", "result": "\u5728\u591a\u4e2a\u751f\u7269\u533b\u5b66\u663e\u5fae\u955c\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u53bb\u566a\u6027\u80fd\uff0c\u7279\u522b\u64c5\u957f\u4fdd\u7559\u9ad8\u9891\u4fe1\u606f\uff0c\u4e14\u53cc\u5206\u652f\u5224\u522b\u5668\u4e0e\u591a\u79cdGAN\u6846\u67b6\u517c\u5bb9\u3002", "conclusion": "\u63d0\u51fa\u7684QWD-GAN\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u56fe\u50cf\u53bb\u566a\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u8d28\u91cf\u611f\u77e5\u548c\u5c0f\u6ce2\u9a71\u52a8\u7684\u7279\u70b9\u3002"}}
{"id": "2509.15968", "pdf": "https://arxiv.org/pdf/2509.15968", "abs": "https://arxiv.org/abs/2509.15968", "authors": ["Shiyu Fang", "Yiming Cui", "Haoyang Liang", "Chen Lv", "Peng Hang", "Jian Sun"], "title": "CoReVLA: A Dual-Stage End-to-End Autonomous Driving Framework for Long-Tail Scenarios via Collect-and-Refine", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Autonomous Driving (AD) systems have made notable progress, but their performance in long-tail, safety-critical scenarios remains limited. These rare cases contribute a disproportionate number of accidents. Vision-Language Action (VLA) models have strong reasoning abilities and offer a potential solution, but their effectiveness is limited by the lack of high-quality data and inefficient learning in such conditions. To address these challenges, we propose CoReVLA, a continual learning end-to-end autonomous driving framework that improves the performance in long-tail scenarios through a dual-stage process of data Collection and behavior Refinement. First, the model is jointly fine-tuned on a mixture of open-source driving QA datasets, allowing it to acquire a foundational understanding of driving scenarios. Next, CoReVLA is deployed within the Cave Automatic Virtual Environment (CAVE) simulation platform, where driver takeover data is collected from real-time interactions. Each takeover indicates a long-tail scenario that CoReVLA fails to handle reliably. Finally, the model is refined via Direct Preference Optimization (DPO), allowing it to learn directly from human preferences and thereby avoid reward hacking caused by manually designed rewards. Extensive open-loop and closed-loop experiments demonstrate that the proposed CoReVLA model can accurately perceive driving scenarios and make appropriate decisions. On the Bench2Drive benchmark, CoReVLA achieves a Driving Score (DS) of 72.18 and a Success Rate (SR) of 50%, outperforming state-of-the-art methods by 7.96 DS and 15% SR under long-tail, safety-critical scenarios. Furthermore, case studies demonstrate the model's ability to continually improve its performance in similar failure-prone scenarios by leveraging past takeover experiences. All codea and preprocessed datasets are available at: https://github.com/FanGShiYuu/CoReVLA", "AI": {"tldr": "CoReVLA\u662f\u4e00\u4e2a\u6301\u7eed\u5b66\u4e60\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u6536\u96c6\u548c\u884c\u4e3a\u7cbe\u70bc\u7684\u53cc\u9636\u6bb5\u8fc7\u7a0b\uff0c\u63d0\u5347\u5728\u957f\u5c3e\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u957f\u5c3e\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u8fd9\u4e9b\u7f55\u89c1\u60c5\u51b5\u5bfc\u81f4\u4e86\u4e0d\u6210\u6bd4\u4f8b\u7684\u4e8b\u6545\u6570\u91cf\u3002\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u5177\u6709\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u53d7\u9650\u4e8e\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u7f3a\u4e4f\u548c\u4f4e\u6548\u5b66\u4e60\u3002", "method": "\u91c7\u7528\u53cc\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u5728\u5f00\u6e90\u9a7e\u9a76QA\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8054\u5408\u5fae\u8c03\uff0c\u83b7\u5f97\u57fa\u7840\u9a7e\u9a76\u573a\u666f\u7406\u89e3\uff1b2\uff09\u5728CAVE\u4eff\u771f\u5e73\u53f0\u4e2d\u6536\u96c6\u9a7e\u9a76\u5458\u63a5\u7ba1\u6570\u636e\uff0c\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316\u8fdb\u884c\u6a21\u578b\u7cbe\u70bc\uff0c\u907f\u514d\u4eba\u5de5\u8bbe\u8ba1\u5956\u52b1\u5bfc\u81f4\u7684\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\u3002", "result": "\u5728Bench2Drive\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoReVLA\u83b7\u5f9772.18\u7684\u9a7e\u9a76\u5206\u6570\u548c50%\u7684\u6210\u529f\u7387\uff0c\u5728\u957f\u5c3e\u5b89\u5168\u5173\u952e\u573a\u666f\u4e0b\u5206\u522b\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u9ad8\u51fa7.96\u5206\u548c15%\u3002\u6848\u4f8b\u7814\u7a76\u663e\u793a\u6a21\u578b\u80fd\u591f\u5229\u7528\u8fc7\u53bb\u7684\u63a5\u7ba1\u7ecf\u9a8c\u6301\u7eed\u6539\u8fdb\u6027\u80fd\u3002", "conclusion": "CoReVLA\u6846\u67b6\u80fd\u591f\u51c6\u786e\u611f\u77e5\u9a7e\u9a76\u573a\u666f\u5e76\u505a\u51fa\u9002\u5f53\u51b3\u7b56\uff0c\u901a\u8fc7\u6301\u7eed\u5b66\u4e60\u673a\u5236\u6709\u6548\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u957f\u5c3e\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2509.16106", "pdf": "https://arxiv.org/pdf/2509.16106", "abs": "https://arxiv.org/abs/2509.16106", "authors": ["Yuanyun Hu", "Evan Bell", "Guijin Wang", "Yu Sun"], "title": "PRISM: Probabilistic and Robust Inverse Solver with Measurement-Conditioned Diffusion Prior for Blind Inverse Problems", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Diffusion models are now commonly used to solve inverse problems in computational imaging. However, most diffusion-based inverse solvers require complete knowledge of the forward operator to be used. In this work, we introduce a novel probabilistic and robust inverse solver with measurement-conditioned diffusion prior (PRISM) to effectively address blind inverse problems. PRISM offers a technical advancement over current methods by incorporating a powerful measurement-conditioned diffusion model into a theoretically principled posterior sampling scheme. Experiments on blind image deblurring validate the effectiveness of the proposed method, demonstrating the superior performance of PRISM over state-of-the-art baselines in both image and blur kernel recovery.", "AI": {"tldr": "PRISM\u662f\u4e00\u79cd\u65b0\u7684\u6982\u7387\u9c81\u68d2\u9006\u6c42\u89e3\u5668\uff0c\u4f7f\u7528\u6d4b\u91cf\u6761\u4ef6\u6269\u6563\u5148\u9a8c\u6765\u89e3\u51b3\u76f2\u9006\u95ee\u9898\uff0c\u65e0\u9700\u5b8c\u6574\u524d\u5411\u7b97\u5b50\u77e5\u8bc6\uff0c\u5728\u76f2\u56fe\u50cf\u53bb\u6a21\u7cca\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9006\u6c42\u89e3\u5668\u9700\u8981\u5b8c\u6574\u7684\u524d\u5411\u7b97\u5b50\u77e5\u8bc6\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u76f2\u9006\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5c06\u5f3a\u5927\u7684\u6d4b\u91cf\u6761\u4ef6\u6269\u6563\u6a21\u578b\u878d\u5165\u7406\u8bba\u4e0a\u6709\u539f\u5219\u7684\u540e\u9a8c\u91c7\u6837\u65b9\u6848\u4e2d\uff0c\u5f00\u53d1\u4e86PRISM\u65b9\u6cd5\u3002", "result": "\u5728\u76f2\u56fe\u50cf\u53bb\u6a21\u7cca\u5b9e\u9a8c\u4e2d\uff0cPRISM\u5728\u56fe\u50cf\u548c\u6a21\u7cca\u6838\u6062\u590d\u65b9\u9762\u90fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "PRISM\u4e3a\u76f2\u9006\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u8ba1\u7b97\u6210\u50cf\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
