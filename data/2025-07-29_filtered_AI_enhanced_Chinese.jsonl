{"id": "2507.19718", "pdf": "https://arxiv.org/pdf/2507.19718", "abs": "https://arxiv.org/abs/2507.19718", "authors": ["David Bauer", "Qi Wu", "Hamid Gadirov", "Kwan-Liu Ma"], "title": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D Gaussian Splatting", "categories": ["cs.GR", "cs.LG"], "comment": null, "summary": "Real-time path tracing is rapidly becoming the standard for rendering in entertainment and professional applications. In scientific visualization, volume rendering plays a crucial role in helping researchers analyze and interpret complex 3D data. Recently, photorealistic rendering techniques have gained popularity in scientific visualization, yet they face significant challenges. One of the most prominent issues is slow rendering performance and high pixel variance caused by Monte Carlo integration. In this work, we introduce a novel radiance caching approach for path-traced volume rendering. Our method leverages advances in volumetric scene representation and adapts 3D Gaussian splatting to function as a multi-level, path-space radiance cache. This cache is designed to be trainable on the fly, dynamically adapting to changes in scene parameters such as lighting configurations and transfer functions. By incorporating our cache, we achieve less noisy, higher-quality images without increasing rendering costs. To evaluate our approach, we compare it against a baseline path tracer that supports uniform sampling and next-event estimation and the state-of-the-art for neural radiance caching. Through both quantitative and qualitative analyses, we demonstrate that our path-space radiance cache is a robust solution that is easy to integrate and significantly enhances the rendering quality of volumetric visualization applications while maintaining comparable computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8def\u5f84\u8ffd\u8e2a\u7684\u4f53\u79ef\u6e32\u67d3\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8bad\u7ec3\u7684\u591a\u7ea7\u8def\u5f84\u7a7a\u95f4\u8f90\u5c04\u7f13\u5b58\uff0c\u663e\u8457\u63d0\u5347\u6e32\u67d3\u8d28\u91cf\u5e76\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u79d1\u5b66\u53ef\u89c6\u5316\u4e2d\uff0c\u771f\u5b9e\u611f\u6e32\u67d3\u6280\u672f\u56e0\u8499\u7279\u5361\u6d1b\u79ef\u5206\u5bfc\u81f4\u7684\u9ad8\u50cf\u7d20\u65b9\u5dee\u548c\u4f4e\u6027\u80fd\u95ee\u9898\u9762\u4e34\u6311\u6218\u3002", "method": "\u91c7\u75283D\u9ad8\u65af\u6563\u5c04\u4f5c\u4e3a\u53ef\u52a8\u6001\u8bad\u7ec3\u7684\u591a\u7ea7\u8def\u5f84\u7a7a\u95f4\u8f90\u5c04\u7f13\u5b58\uff0c\u9002\u5e94\u573a\u666f\u53c2\u6570\u53d8\u5316\u3002", "result": "\u76f8\u6bd4\u57fa\u7ebf\u8def\u5f84\u8ffd\u8e2a\u5668\u548c\u795e\u7ecf\u8f90\u5c04\u7f13\u5b58\uff0c\u65b0\u65b9\u6cd5\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u566a\u58f0\u5e76\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "\u8def\u5f84\u7a7a\u95f4\u8f90\u5c04\u7f13\u5b58\u662f\u4e00\u79cd\u6613\u4e8e\u96c6\u6210\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f53\u79ef\u53ef\u89c6\u5316\u5e94\u7528\u7684\u6e32\u67d3\u8d28\u91cf\u3002"}}
{"id": "2507.19830", "pdf": "https://arxiv.org/pdf/2507.19830", "abs": "https://arxiv.org/abs/2507.19830", "authors": ["Yuze Wang", "Yue Qi"], "title": "Taking Language Embedded 3D Gaussian Splatting into the Wild", "categories": ["cs.GR", "cs.CV"], "comment": "Visit our project page at   https://yuzewang1998.github.io/takinglangsplatw/", "summary": "Recent advances in leveraging large-scale Internet photo collections for 3D reconstruction have enabled immersive virtual exploration of landmarks and historic sites worldwide. However, little attention has been given to the immersive understanding of architectural styles and structural knowledge, which remains largely confined to browsing static text-image pairs. Therefore, can we draw inspiration from 3D in-the-wild reconstruction techniques and use unconstrained photo collections to create an immersive approach for understanding the 3D structure of architectural components? To this end, we extend language embedded 3D Gaussian splatting (3DGS) and propose a novel framework for open-vocabulary scene understanding from unconstrained photo collections. Specifically, we first render multiple appearance images from the same viewpoint as the unconstrained image with the reconstructed radiance field, then extract multi-appearance CLIP features and two types of language feature uncertainty maps-transient and appearance uncertainty-derived from the multi-appearance features to guide the subsequent optimization process. Next, we propose a transient uncertainty-aware autoencoder, a multi-appearance language field 3DGS representation, and a post-ensemble strategy to effectively compress, learn, and fuse language features from multiple appearances. Finally, to quantitatively evaluate our method, we introduce PT-OVS, a new benchmark dataset for assessing open-vocabulary segmentation performance on unconstrained photo collections. Experimental results show that our method outperforms existing methods, delivering accurate open-vocabulary segmentation and enabling applications such as interactive roaming with open-vocabulary queries, architectural style pattern recognition, and 3D scene editing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u7ea6\u675f\u7167\u7247\u96c6\u5408\u7684\u5f00\u653e\u8bcd\u6c47\u573a\u666f\u7406\u89e3\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u8a00\u5d4c\u5165\u76843D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5efa\u7b51\u7ec4\u4ef6\u7684\u6c89\u6d78\u5f0f\u7406\u89e3\u3002", "motivation": "\u5f53\u524d\u5229\u7528\u4e92\u8054\u7f51\u7167\u7247\u8fdb\u884c3D\u91cd\u5efa\u7684\u6280\u672f\u867d\u80fd\u5b9e\u73b0\u5730\u6807\u548c\u53e4\u8ff9\u7684\u865a\u62df\u63a2\u7d22\uff0c\u4f46\u5bf9\u5efa\u7b51\u98ce\u683c\u548c\u7ed3\u6784\u77e5\u8bc6\u7684\u6c89\u6d78\u5f0f\u7406\u89e3\u4ecd\u5c40\u9650\u4e8e\u9759\u6001\u56fe\u6587\u6d4f\u89c8\u3002", "method": "\u6269\u5c55\u8bed\u8a00\u5d4c\u5165\u76843D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u63d0\u51fa\u591a\u5916\u89c2CLIP\u7279\u5f81\u63d0\u53d6\u3001\u8bed\u8a00\u7279\u5f81\u4e0d\u786e\u5b9a\u6027\u6620\u5c04\u3001\u77ac\u6001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u81ea\u7f16\u7801\u5668\u7b49\u521b\u65b0\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u6f2b\u6e38\u3001\u5efa\u7b51\u98ce\u683c\u8bc6\u522b\u548c3D\u573a\u666f\u7f16\u8f91\u7b49\u5e94\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65e0\u7ea6\u675f\u7167\u7247\u96c6\u5408\u7684\u5f00\u653e\u8bcd\u6c47\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6269\u5c55\u4e863D\u91cd\u5efa\u6280\u672f\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.19836", "pdf": "https://arxiv.org/pdf/2507.19836", "abs": "https://arxiv.org/abs/2507.19836", "authors": ["Xuanchen Wang", "Heng Wang", "Weidong Cai"], "title": "ChoreoMuse: Robust Music-to-Dance Video Generation with Style Transfer and Beat-Adherent Motion", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.MM", "cs.SD"], "comment": "10 pages, 5 figures, accepted by the 33rd ACM International   Conference on Multimedia (ACM MM 2025), demo page:   https://choreomuse.github.io", "summary": "Modern artistic productions increasingly demand automated choreography generation that adapts to diverse musical styles and individual dancer characteristics. Existing approaches often fail to produce high-quality dance videos that harmonize with both musical rhythm and user-defined choreography styles, limiting their applicability in real-world creative contexts. To address this gap, we introduce ChoreoMuse, a diffusion-based framework that uses SMPL format parameters and their variation version as intermediaries between music and video generation, thereby overcoming the usual constraints imposed by video resolution. Critically, ChoreoMuse supports style-controllable, high-fidelity dance video generation across diverse musical genres and individual dancer characteristics, including the flexibility to handle any reference individual at any resolution. Our method employs a novel music encoder MotionTune to capture motion cues from audio, ensuring that the generated choreography closely follows the beat and expressive qualities of the input music. To quantitatively evaluate how well the generated dances match both musical and choreographic styles, we introduce two new metrics that measure alignment with the intended stylistic cues. Extensive experiments confirm that ChoreoMuse achieves state-of-the-art performance across multiple dimensions, including video quality, beat alignment, dance diversity, and style adherence, demonstrating its potential as a robust solution for a wide range of creative applications. Video results can be found on our project page: https://choreomuse.github.io.", "AI": {"tldr": "ChoreoMuse\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7SMPL\u53c2\u6570\u4f5c\u4e3a\u97f3\u4e50\u4e0e\u89c6\u9891\u751f\u6210\u7684\u4e2d\u95f4\u5a92\u4ecb\uff0c\u652f\u6301\u98ce\u683c\u53ef\u63a7\u7684\u9ad8\u8d28\u91cf\u821e\u8e48\u89c6\u9891\u751f\u6210\uff0c\u9002\u5e94\u591a\u6837\u97f3\u4e50\u98ce\u683c\u548c\u821e\u8005\u7279\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u821e\u8e48\u89c6\u9891\u65f6\u96be\u4ee5\u540c\u65f6\u9002\u5e94\u97f3\u4e50\u8282\u594f\u548c\u7528\u6237\u5b9a\u4e49\u7f16\u821e\u98ce\u683c\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528SMPL\u53c2\u6570\u4f5c\u4e3a\u4e2d\u4ecb\uff0c\u7ed3\u5408\u65b0\u9896\u7684\u97f3\u4e50\u7f16\u7801\u5668MotionTune\u4ece\u97f3\u9891\u4e2d\u63d0\u53d6\u8fd0\u52a8\u7ebf\u7d22\uff0c\u786e\u4fdd\u751f\u6210\u7684\u7f16\u821e\u4e0e\u97f3\u4e50\u8282\u62cd\u548c\u8868\u8fbe\u7279\u6027\u4e00\u81f4\u3002", "result": "ChoreoMuse\u5728\u89c6\u9891\u8d28\u91cf\u3001\u8282\u62cd\u5bf9\u9f50\u3001\u821e\u8e48\u591a\u6837\u6027\u548c\u98ce\u683c\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "ChoreoMuse\u4e3a\u5e7f\u6cdb\u521b\u610f\u5e94\u7528\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.20200", "pdf": "https://arxiv.org/pdf/2507.20200", "abs": "https://arxiv.org/abs/2507.20200", "authors": ["Xin Zhang", "Anpei Chen", "Jincheng Xiong", "Pinxuan Dai", "Yujun Shen", "Weiwei Xu"], "title": "Neural Shell Texture Splatting: More Details and Fewer Primitives", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Gaussian splatting techniques have shown promising results in novel view synthesis, achieving high fidelity and efficiency. However, their high reconstruction quality comes at the cost of requiring a large number of primitives. We identify this issue as stemming from the entanglement of geometry and appearance in Gaussian Splatting. To address this, we introduce a neural shell texture, a global representation that encodes texture information around the surface. We use Gaussian primitives as both a geometric representation and texture field samplers, efficiently splatting texture features into image space. Our evaluation demonstrates that this disentanglement enables high parameter efficiency, fine texture detail reconstruction, and easy textured mesh extraction, all while using significantly fewer primitives.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u5916\u58f3\u7eb9\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u51e0\u4f55\u4e0e\u5916\u89c2\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u9ad8\u65af\u6e85\u5c04\u6280\u672f\u6240\u9700\u7684\u57fa\u5143\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u7684\u91cd\u5efa\u6548\u679c\u3002", "motivation": "\u9ad8\u65af\u6e85\u5c04\u6280\u672f\u5728\u89c6\u89d2\u5408\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9700\u8981\u5927\u91cf\u57fa\u5143\uff0c\u95ee\u9898\u6e90\u4e8e\u51e0\u4f55\u4e0e\u5916\u89c2\u7684\u8026\u5408\u3002", "method": "\u5f15\u5165\u795e\u7ecf\u5916\u58f3\u7eb9\u7406\u4f5c\u4e3a\u5168\u5c40\u8868\u793a\uff0c\u7528\u9ad8\u65af\u57fa\u5143\u540c\u65f6\u4f5c\u4e3a\u51e0\u4f55\u8868\u793a\u548c\u7eb9\u7406\u91c7\u6837\u5668\uff0c\u9ad8\u6548\u5730\u5c06\u7eb9\u7406\u7279\u5f81\u6e85\u5c04\u5230\u56fe\u50cf\u7a7a\u95f4\u3002", "result": "\u89e3\u8026\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u53c2\u6570\u6548\u7387\u3001\u7cbe\u7ec6\u7eb9\u7406\u7ec6\u8282\u91cd\u5efa\u548c\u6613\u4e8e\u63d0\u53d6\u7eb9\u7406\u7f51\u683c\uff0c\u4e14\u57fa\u5143\u6570\u91cf\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u65af\u6e85\u5c04\u4e2d\u57fa\u5143\u6570\u91cf\u8fc7\u591a\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u7684\u91cd\u5efa\u6548\u679c\u3002"}}
{"id": "2507.19770", "pdf": "https://arxiv.org/pdf/2507.19770", "abs": "https://arxiv.org/abs/2507.19770", "authors": ["Jiaxin Liu", "Qichao Ying", "Zhenxing Qian", "Sheng Li", "Runqi Zhang", "Jian Liu", "Xinpeng Zhang"], "title": "MoFRR: Mixture of Diffusion Models for Face Retouching Restoration", "categories": ["cs.CV"], "comment": null, "summary": "The widespread use of face retouching on social media platforms raises concerns about the authenticity of face images. While existing methods focus on detecting face retouching, how to accurately recover the original faces from the retouched ones has yet to be answered. This paper introduces Face Retouching Restoration (FRR), a novel computer vision task aimed at restoring original faces from their retouched counterparts. FRR differs from traditional image restoration tasks by addressing the complex retouching operations with various types and degrees, which focuses more on the restoration of the low-frequency information of the faces. To tackle this challenge, we propose MoFRR, Mixture of Diffusion Models for FRR. Inspired by DeepSeek's expert isolation strategy, the MoFRR uses sparse activation of specialized experts handling distinct retouching types and the engagement of a shared expert dealing with universal retouching traces. Each specialized expert follows a dual-branch structure with a DDIM-based low-frequency branch guided by an Iterative Distortion Evaluation Module (IDEM) and a Cross-Attention-based High-Frequency branch (HFCAM) for detail refinement. Extensive experiments on a newly constructed face retouching dataset, RetouchingFFHQ++, demonstrate the effectiveness of MoFRR for FRR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMoFRR\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u4fee\u9970\u8fc7\u7684\u9762\u90e8\u56fe\u50cf\u4e2d\u6062\u590d\u539f\u59cb\u9762\u90e8\uff0c\u901a\u8fc7\u6df7\u5408\u6269\u6563\u6a21\u578b\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u7684\u4fee\u9970\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e0a\u5e7f\u6cdb\u4f7f\u7528\u7684\u9762\u90e8\u4fee\u9970\u6280\u672f\u5f15\u53d1\u4e86\u5bf9\u9762\u90e8\u56fe\u50cf\u771f\u5b9e\u6027\u7684\u62c5\u5fe7\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u68c0\u6d4b\u4fee\u9970\uff0c\u800c\u5982\u4f55\u4ece\u4fee\u9970\u56fe\u50cf\u4e2d\u51c6\u786e\u6062\u590d\u539f\u59cb\u9762\u90e8\u5c1a\u672a\u89e3\u51b3\u3002", "method": "\u63d0\u51faMoFRR\u65b9\u6cd5\uff0c\u91c7\u7528\u6df7\u5408\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u7a00\u758f\u6fc0\u6d3b\u7684\u4e13\u5bb6\u5904\u7406\u7279\u5b9a\u4fee\u9970\u7c7b\u578b\u548c\u5171\u4eab\u4e13\u5bb6\u5904\u7406\u901a\u7528\u4fee\u9970\u75d5\u8ff9\uff0c\u6bcf\u4e2a\u4e13\u5bb6\u91c7\u7528\u53cc\u5206\u652f\u7ed3\u6784\uff08\u4f4e\u9891\u5206\u652f\u548c\u9ad8\u9891\u5206\u652f\uff09\u3002", "result": "\u5728\u65b0\u5efa\u7684\u6570\u636e\u96c6RetouchingFFHQ++\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86MoFRR\u7684\u6709\u6548\u6027\u3002", "conclusion": "MoFRR\u80fd\u591f\u6709\u6548\u6062\u590d\u4fee\u9970\u8fc7\u7684\u9762\u90e8\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u672a\u6d89\u53ca\u7684\u6062\u590d\u95ee\u9898\u3002"}}
{"id": "2507.19795", "pdf": "https://arxiv.org/pdf/2507.19795", "abs": "https://arxiv.org/abs/2507.19795", "authors": ["Steven Walton"], "title": "Smaller, Faster, Cheaper: Architectural Designs for Efficient Machine Learning", "categories": ["cs.CV", "cs.AR", "cs.LG"], "comment": "Ph.D. Thesis", "summary": "Major advancements in the capabilities of computer vision models have been primarily fueled by rapid expansion of datasets, model parameters, and computational budgets, leading to ever-increasing demands on computational infrastructure. However, as these models are deployed in increasingly diverse and resource-constrained environments, there is a pressing need for architectures that can deliver high performance while requiring fewer computational resources.   This dissertation focuses on architectural principles through which models can achieve increased performance while reducing their computational demands. We discuss strides towards this goal through three directions. First, we focus on data ingress and egress, investigating how information may be passed into and retrieved from our core neural processing units. This ensures that our models make the most of available data, allowing smaller architectures to become more performant. Second, we investigate modifications to the core neural architecture, applied to restricted attention in vision transformers. This section explores how removing uniform context windows in restricted attention increases the expressivity of the underlying neural architecture. Third, we explore the natural structures of Normalizing Flows and how we can leverage these properties to better distill model knowledge.   These contributions demonstrate that careful design of neural architectures can increase the efficiency of machine learning algorithms, allowing them to become smaller, faster, and cheaper.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5728\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u7684\u540c\u65f6\u63d0\u5347\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u7684\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u4e09\u4e2a\u65b9\u5411\u7684\u7814\u7a76\uff1a\u6570\u636e\u8f93\u5165\u8f93\u51fa\u4f18\u5316\u3001\u6838\u5fc3\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u6539\u8fdb\u4ee5\u53ca\u5229\u7528Normalizing Flows\u7684\u7279\u6027\u3002", "motivation": "\u968f\u7740\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u5728\u591a\u6837\u5316\u4e14\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u90e8\u7f72\uff0c\u9700\u8981\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u67b6\u6784\u4ee5\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u4f18\u5316\u6570\u636e\u8f93\u5165\u8f93\u51fa\u3001\u6539\u8fdb\u6838\u5fc3\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff08\u5982\u53d7\u9650\u6ce8\u610f\u529b\u673a\u5236\uff09\u4ee5\u53ca\u5229\u7528Normalizing Flows\u7684\u7279\u6027\uff0c\u63d0\u5347\u6a21\u578b\u6548\u7387\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u53ef\u4ee5\u63d0\u9ad8\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u6548\u7387\uff0c\u4f7f\u5176\u66f4\u5c0f\u3001\u66f4\u5feb\u3001\u66f4\u7ecf\u6d4e\u3002", "conclusion": "\u901a\u8fc7\u67b6\u6784\u4f18\u5316\uff0c\u53ef\u4ee5\u5728\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u3002"}}
{"id": "2507.19808", "pdf": "https://arxiv.org/pdf/2507.19808", "abs": "https://arxiv.org/abs/2507.19808", "authors": ["Joon Hyun Park", "Kumju Jo", "Sungyong Baik"], "title": "SeeDiff: Off-the-Shelf Seeded Mask Generation from Diffusion Models", "categories": ["cs.CV"], "comment": "AAAI 2025", "summary": "Entrusted with the goal of pixel-level object classification, the semantic segmentation networks entail the laborious preparation of pixel-level annotation masks. To obtain pixel-level annotation masks for a given class without human efforts, recent few works have proposed to generate pairs of images and annotation masks by employing image and text relationships modeled by text-to-image generative models, especially Stable Diffusion. However, these works do not fully exploit the capability of text-guided Diffusion models and thus require a pre-trained segmentation network, careful text prompt tuning, or the training of a segmentation network to generate final annotation masks. In this work, we take a closer look at attention mechanisms of Stable Diffusion, from which we draw connections with classical seeded segmentation approaches. In particular, we show that cross-attention alone provides very coarse object localization, which however can provide initial seeds. Then, akin to region expansion in seeded segmentation, we utilize the semantic-correspondence-modeling capability of self-attention to iteratively spread the attention to the whole class from the seeds using multi-scale self-attention maps. We also observe that a simple-text-guided synthetic image often has a uniform background, which is easier to find correspondences, compared to complex-structured objects. Thus, we further refine a mask using a more accurate background mask. Our proposed method, dubbed SeeDiff, generates high-quality masks off-the-shelf from Stable Diffusion, without additional training procedure, prompt tuning, or a pre-trained segmentation network.", "AI": {"tldr": "SeeDiff\u5229\u7528Stable Diffusion\u7684\u6ce8\u610f\u529b\u673a\u5236\u751f\u6210\u9ad8\u8d28\u91cf\u50cf\u7d20\u7ea7\u6807\u6ce8\u63a9\u7801\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u63d0\u793a\u8c03\u4f18\u3002", "motivation": "\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u7684\u8d1f\u62c5\uff0c\u5229\u7528\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u6f5c\u529b\u81ea\u52a8\u751f\u6210\u50cf\u7d20\u7ea7\u6807\u6ce8\u63a9\u7801\u3002", "method": "\u901a\u8fc7\u5206\u6790Stable Diffusion\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u81ea\u6ce8\u610f\u529b\uff0c\u8fed\u4ee3\u6269\u5c55\u79cd\u5b50\u533a\u57df\u4ee5\u8986\u76d6\u6574\u4e2a\u76ee\u6807\u7c7b\u522b\uff0c\u5e76\u5229\u7528\u80cc\u666f\u63a9\u7801\u8fdb\u884c\u7ec6\u5316\u3002", "result": "SeeDiff\u80fd\u591f\u76f4\u63a5\u4eceStable Diffusion\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6807\u6ce8\u63a9\u7801\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u63d0\u793a\u8c03\u4f18\u3002", "conclusion": "SeeDiff\u5c55\u793a\u4e86\u5229\u7528\u751f\u6210\u6a21\u578b\u7684\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u9ad8\u6548\u50cf\u7d20\u7ea7\u6807\u6ce8\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.19847", "pdf": "https://arxiv.org/pdf/2507.19847", "abs": "https://arxiv.org/abs/2507.19847", "authors": ["Wenjie Zhu", "Yabin Zhang", "Xin Jin", "Wenjun Zeng", "Lei Zhang"], "title": "Knowledge Regularized Negative Feature Tuning for Out-of-Distribution Detection with Vision-Language Models", "categories": ["cs.CV"], "comment": "accepted by ACMMM 2025", "summary": "Out-of-distribution (OOD) detection is crucial for building reliable machine learning models. Although negative prompt tuning has enhanced the OOD detection capabilities of vision-language models, these tuned models often suffer from reduced generalization performance on unseen classes and styles. To address this challenge, we propose a novel method called Knowledge Regularized Negative Feature Tuning (KR-NFT), which integrates an innovative adaptation architecture termed Negative Feature Tuning (NFT) and a corresponding knowledge-regularization (KR) optimization strategy. Specifically, NFT applies distribution-aware transformations to pre-trained text features, effectively separating positive and negative features into distinct spaces. This separation maximizes the distinction between in-distribution (ID) and OOD images. Additionally, we introduce image-conditional learnable factors through a lightweight meta-network, enabling dynamic adaptation to individual images and mitigating sensitivity to class and style shifts. Compared to traditional negative prompt tuning, NFT demonstrates superior efficiency and scalability. To optimize this adaptation architecture, the KR optimization strategy is designed to enhance the discrimination between ID and OOD sets while mitigating pre-trained knowledge forgetting. This enhances OOD detection performance on trained ID classes while simultaneously improving OOD detection on unseen ID datasets. Notably, when trained with few-shot samples from ImageNet dataset, KR-NFT not only improves ID classification accuracy and OOD detection but also significantly reduces the FPR95 by 5.44\\% under an unexplored generalization setting with unseen ID categories. Codes can be found at \\href{https://github.com/ZhuWenjie98/KRNFT}{https://github.com/ZhuWenjie98/KRNFT}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKR-NFT\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d1f\u7279\u5f81\u8c03\u4f18\u548c\u77e5\u8bc6\u6b63\u5219\u5316\u7b56\u7565\uff0c\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684OOD\u68c0\u6d4b\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u672a\u89c1\u7c7b\u548c\u98ce\u683c\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u8d1f\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\u5728\u63d0\u5347OOD\u68c0\u6d4b\u80fd\u529b\u65f6\uff0c\u5f80\u5f80\u727a\u7272\u4e86\u5bf9\u672a\u89c1\u7c7b\u548c\u98ce\u683c\u7684\u6cdb\u5316\u6027\u80fd\uff0cKR-NFT\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "KR-NFT\u7ed3\u5408\u4e86\u8d1f\u7279\u5f81\u8c03\u4f18\uff08NFT\uff09\u548c\u77e5\u8bc6\u6b63\u5219\u5316\uff08KR\uff09\u7b56\u7565\u3002NFT\u901a\u8fc7\u5206\u5e03\u611f\u77e5\u53d8\u6362\u5206\u79bb\u6b63\u8d1f\u7279\u5f81\uff0cKR\u4f18\u5316\u7b56\u7565\u589e\u5f3aID\u4e0eOOD\u7684\u533a\u5206\u5e76\u51cf\u5c11\u9884\u8bad\u7ec3\u77e5\u8bc6\u9057\u5fd8\u3002", "result": "\u5728ImageNet\u6570\u636e\u96c6\u4e0a\uff0cKR-NFT\u663e\u8457\u63d0\u5347\u4e86ID\u5206\u7c7b\u51c6\u786e\u6027\u548cOOD\u68c0\u6d4b\u6027\u80fd\uff0cFPR95\u964d\u4f4e\u4e865.44%\u3002", "conclusion": "KR-NFT\u5728\u63d0\u5347OOD\u68c0\u6d4b\u80fd\u529b\u7684\u540c\u65f6\uff0c\u6709\u6548\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u672a\u89c1ID\u7c7b\u522b\u7684\u573a\u666f\u3002"}}
{"id": "2507.19856", "pdf": "https://arxiv.org/pdf/2507.19856", "abs": "https://arxiv.org/abs/2507.19856", "authors": ["Xiaokai Bai", "Chenxu Zhou", "Lianqing Zheng", "Si-Yuan Cao", "Jianan Liu", "Xiaohan Zhang", "Zhengzhuang Zhang", "Hui-liang Shen"], "title": "RaGS: Unleashing 3D Gaussian Splatting from 4D Radar and Monocular Cues for 3D Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 6 figures, conference", "summary": "4D millimeter-wave radar has emerged as a promising sensor for autonomous driving, but effective 3D object detection from both 4D radar and monocular images remains a challenge. Existing fusion approaches typically rely on either instance-based proposals or dense BEV grids, which either lack holistic scene understanding or are limited by rigid grid structures. To address these, we propose RaGS, the first framework to leverage 3D Gaussian Splatting (GS) as representation for fusing 4D radar and monocular cues in 3D object detection. 3D GS naturally suits 3D object detection by modeling the scene as a field of Gaussians, dynamically allocating resources on foreground objects and providing a flexible, resource-efficient solution. RaGS uses a cascaded pipeline to construct and refine the Gaussian field. It starts with the Frustum-based Localization Initiation (FLI), which unprojects foreground pixels to initialize coarse 3D Gaussians positions. Then, the Iterative Multimodal Aggregation (IMA) fuses semantics and geometry, refining the limited Gaussians to the regions of interest. Finally, the Multi-level Gaussian Fusion (MGF) renders the Gaussians into multi-level BEV features for 3D object detection. By dynamically focusing on sparse objects within scenes, RaGS enable object concentrating while offering comprehensive scene perception. Extensive experiments on View-of-Delft, TJ4DRadSet, and OmniHD-Scenes benchmarks demonstrate its state-of-the-art performance. Code will be released.", "AI": {"tldr": "RaGS\u6846\u67b6\u5229\u75283D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u878d\u54084D\u96f7\u8fbe\u548c\u5355\u76ee\u56fe\u50cf\uff0c\u5b9e\u73b0\u9ad8\u6548\u76843D\u7269\u4f53\u68c0\u6d4b\uff0c\u901a\u8fc7\u52a8\u6001\u8d44\u6e90\u5206\u914d\u548c\u7075\u6d3b\u8868\u793a\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u878d\u5408\u65b9\u6cd5\u7f3a\u4e4f\u5168\u5c40\u573a\u666f\u7406\u89e3\u6216\u53d7\u9650\u4e8e\u56fa\u5b9a\u7f51\u683c\u7ed3\u6784\uff0cRaGS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u75283D\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u573a\u666f\uff0c\u901a\u8fc7FLI\u3001IMA\u548cMGF\u4e09\u9636\u6bb5\u6d41\u7a0b\u52a8\u6001\u4f18\u5316\u9ad8\u65af\u573a\u5e76\u751f\u6210BEV\u7279\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "RaGS\u901a\u8fc7\u52a8\u6001\u805a\u7126\u548c\u7075\u6d3b\u8868\u793a\uff0c\u4e3a3D\u7269\u4f53\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19874", "pdf": "https://arxiv.org/pdf/2507.19874", "abs": "https://arxiv.org/abs/2507.19874", "authors": ["Haowei Chen", "Zhiwen Yang", "Haotian Hou", "Hui Zhang", "Bingzheng Wei", "Gang Zhou", "Yan Xu"], "title": "All-in-One Medical Image Restoration with Latent Diffusion-Enhanced Vector-Quantized Codebook Prior", "categories": ["cs.CV"], "comment": "11pages, 3figures, MICCAI 2025", "summary": "All-in-one medical image restoration (MedIR) aims to address multiple MedIR tasks using a unified model, concurrently recovering various high-quality (HQ) medical images (e.g., MRI, CT, and PET) from low-quality (LQ) counterparts. However, all-in-one MedIR presents significant challenges due to the heterogeneity across different tasks. Each task involves distinct degradations, leading to diverse information losses in LQ images. Existing methods struggle to handle these diverse information losses associated with different tasks. To address these challenges, we propose a latent diffusion-enhanced vector-quantized codebook prior and develop \\textbf{DiffCode}, a novel framework leveraging this prior for all-in-one MedIR. Specifically, to compensate for diverse information losses associated with different tasks, DiffCode constructs a task-adaptive codebook bank to integrate task-specific HQ prior features across tasks, capturing a comprehensive prior. Furthermore, to enhance prior retrieval from the codebook bank, DiffCode introduces a latent diffusion strategy that utilizes the diffusion model's powerful mapping capabilities to iteratively refine the latent feature distribution, estimating more accurate HQ prior features during restoration. With the help of the task-adaptive codebook bank and latent diffusion strategy, DiffCode achieves superior performance in both quantitative metrics and visual quality across three MedIR tasks: MRI super-resolution, CT denoising, and PET synthesis.", "AI": {"tldr": "DiffCode\u662f\u4e00\u4e2a\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u6f5c\u5728\u6269\u6563\u589e\u5f3a\u7684\u5411\u91cf\u91cf\u5316\u7801\u672c\u5148\u9a8c\uff0c\u89e3\u51b3\u591a\u4efb\u52a1\u533b\u5b66\u56fe\u50cf\u6062\u590d\uff08MedIR\uff09\u4e2d\u7684\u5f02\u8d28\u6027\u95ee\u9898\u3002", "motivation": "\u591a\u4efb\u52a1MedIR\u9762\u4e34\u4e0d\u540c\u4efb\u52a1\u95f4\u4fe1\u606f\u635f\u5931\u7684\u591a\u6837\u6027\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u3002", "method": "DiffCode\u6784\u5efa\u4efb\u52a1\u81ea\u9002\u5e94\u7801\u672c\u5e93\u6574\u5408\u4efb\u52a1\u7279\u5b9a\u9ad8\u8d28\u91cf\u5148\u9a8c\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u6f5c\u5728\u6269\u6563\u7b56\u7565\u4f18\u5316\u7279\u5f81\u5206\u5e03\u3002", "result": "DiffCode\u5728MRI\u8d85\u5206\u8fa8\u7387\u3001CT\u53bb\u566a\u548cPET\u5408\u6210\u4e09\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "DiffCode\u901a\u8fc7\u4efb\u52a1\u81ea\u9002\u5e94\u7801\u672c\u548c\u6f5c\u5728\u6269\u6563\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4efb\u52a1MedIR\u7684\u6027\u80fd\u3002"}}
{"id": "2507.19924", "pdf": "https://arxiv.org/pdf/2507.19924", "abs": "https://arxiv.org/abs/2507.19924", "authors": ["Chang Liu", "Yunfan Ye", "Fan Zhang", "Qingyang Zhou", "Yuchuan Luo", "Zhiping Cai"], "title": "HumanSAM: Classifying Human-centric Forgery Videos in Human Spatial, Appearance, and Motion Anomaly", "categories": ["cs.CV"], "comment": "ICCV 2025. Project page: https://dejian-lc.github.io/humansam/", "summary": "Numerous synthesized videos from generative models, especially human-centric ones that simulate realistic human actions, pose significant threats to human information security and authenticity. While progress has been made in binary forgery video detection, the lack of fine-grained understanding of forgery types raises concerns regarding both reliability and interpretability, which are critical for real-world applications. To address this limitation, we propose HumanSAM, a new framework that builds upon the fundamental challenges of video generation models. Specifically, HumanSAM aims to classify human-centric forgeries into three distinct types of artifacts commonly observed in generated content: spatial, appearance, and motion anomaly.To better capture the features of geometry, semantics and spatiotemporal consistency, we propose to generate the human forgery representation by fusing two branches of video understanding and spatial depth. We also adopt a rank-based confidence enhancement strategy during the training process to learn more robust representation by introducing three prior scores. For training and evaluation, we construct the first public benchmark, the Human-centric Forgery Video (HFV) dataset, with all types of forgeries carefully annotated semi-automatically. In our experiments, HumanSAM yields promising results in comparison with state-of-the-art methods, both in binary and multi-class forgery classification.", "AI": {"tldr": "HumanSAM\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u65e8\u5728\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4eba\u7c7b\u4e2d\u5fc3\u4f2a\u9020\u89c6\u9891\u7684\u4e09\u79cd\u5f02\u5e38\u7c7b\u578b\uff08\u7a7a\u95f4\u3001\u5916\u89c2\u548c\u8fd0\u52a8\uff09\uff0c\u901a\u8fc7\u878d\u5408\u89c6\u9891\u7406\u89e3\u548c\u7a7a\u95f4\u6df1\u5ea6\u7279\u5f81\uff0c\u63d0\u5347\u68c0\u6d4b\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u5408\u6210\u7684\u89c6\u9891\u5bf9\u4eba\u7c7b\u4fe1\u606f\u5b89\u5168\u548c\u771f\u5b9e\u6027\u6784\u6210\u5a01\u80c1\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u4f2a\u9020\u7c7b\u578b\u7684\u7ec6\u7c92\u5ea6\u7406\u89e3\uff0c\u5f71\u54cd\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51faHumanSAM\u6846\u67b6\uff0c\u878d\u5408\u89c6\u9891\u7406\u89e3\u548c\u7a7a\u95f4\u6df1\u5ea6\u7279\u5f81\u751f\u6210\u4f2a\u9020\u8868\u793a\uff0c\u91c7\u7528\u57fa\u4e8e\u6392\u540d\u7684\u7f6e\u4fe1\u589e\u5f3a\u7b56\u7565\uff0c\u5e76\u6784\u5efa\u9996\u4e2a\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6HFV\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHumanSAM\u5728\u4e8c\u5206\u7c7b\u548c\u591a\u5206\u7c7b\u4f2a\u9020\u68c0\u6d4b\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HumanSAM\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5206\u7c7b\u548c\u7279\u5f81\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f2a\u9020\u89c6\u9891\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2507.19939", "pdf": "https://arxiv.org/pdf/2507.19939", "abs": "https://arxiv.org/abs/2507.19939", "authors": ["Jiaze Wang", "Rui Chen", "Haowang Cui"], "title": "LLMControl: Grounded Control of Text-to-Image Diffusion-based Synthesis with Multimodal LLMs", "categories": ["cs.CV"], "comment": null, "summary": "Recent spatial control methods for text-to-image (T2I) diffusion models have shown compelling results. However, these methods still fail to precisely follow the control conditions and generate the corresponding images, especially when encountering the textual prompts that contain multiple objects or have complex spatial compositions. In this work, we present a LLM-guided framework called LLM\\_Control to address the challenges of the controllable T2I generation task. By improving grounding capabilities, LLM\\_Control is introduced to accurately modulate the pre-trained diffusion models, where visual conditions and textual prompts influence the structures and appearance generation in a complementary way. We utilize the multimodal LLM as a global controller to arrange spatial layouts, augment semantic descriptions and bind object attributes. The obtained control signals are injected into the denoising network to refocus and enhance attention maps according to novel sampling constraints. Extensive qualitative and quantitative experiments have demonstrated that LLM\\_Control achieves competitive synthesis quality compared to other state-of-the-art methods across various pre-trained T2I models. It is noteworthy that LLM\\_Control allows the challenging input conditions on which most of the existing methods", "AI": {"tldr": "LLM_Control\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u7a7a\u95f4\u63a7\u5236\uff0c\u89e3\u51b3\u591a\u5bf9\u8c61\u548c\u590d\u6742\u7a7a\u95f4\u7ec4\u5408\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u6587\u672c\u63d0\u793a\u4e0b\u96be\u4ee5\u7cbe\u786e\u63a7\u5236\u56fe\u50cf\u751f\u6210\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528\u591a\u6a21\u6001LLM\u4f5c\u4e3a\u5168\u5c40\u63a7\u5236\u5668\uff0c\u4f18\u5316\u7a7a\u95f4\u5e03\u5c40\u548c\u8bed\u4e49\u63cf\u8ff0\uff0c\u5e76\u5c06\u63a7\u5236\u4fe1\u53f7\u6ce8\u5165\u53bb\u566a\u7f51\u7edc\u4ee5\u589e\u5f3a\u6ce8\u610f\u529b\u56fe\u3002", "result": "LLM_Control\u5728\u591a\u79cd\u9884\u8bad\u7ec3T2I\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "LLM_Control\u4e3a\u590d\u6742\u6761\u4ef6\u4e0b\u7684\u53ef\u63a7\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19946", "pdf": "https://arxiv.org/pdf/2507.19946", "abs": "https://arxiv.org/abs/2507.19946", "authors": ["Ryan Xu", "Dongyang Jin", "Yancheng Bai", "Rui Lan", "Xu Duan", "Lei Sun", "Xiangxiang Chu"], "title": "SCALAR: Scale-wise Controllable Visual Autoregressive Learning", "categories": ["cs.CV"], "comment": null, "summary": "Controllable image synthesis, which enables fine-grained control over generated outputs, has emerged as a key focus in visual generative modeling. However, controllable generation remains challenging for Visual Autoregressive (VAR) models due to their hierarchical, next-scale prediction style. Existing VAR-based methods often suffer from inefficient control encoding and disruptive injection mechanisms that compromise both fidelity and efficiency. In this work, we present SCALAR, a controllable generation method based on VAR, incorporating a novel Scale-wise Conditional Decoding mechanism. SCALAR leverages a", "AI": {"tldr": "SCALAR\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eVAR\u7684\u53ef\u63a7\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c3a\u5ea6\u6761\u4ef6\u89e3\u7801\u673a\u5236\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u63a7\u5236\u7f16\u7801\u548c\u6ce8\u5165\u673a\u5236\u4e0a\u7684\u4f4e\u6548\u95ee\u9898\u3002", "motivation": "\u53ef\u63a7\u56fe\u50cf\u5408\u6210\u662f\u89c6\u89c9\u751f\u6210\u5efa\u6a21\u7684\u5173\u952e\u65b9\u5411\uff0c\u4f46VAR\u6a21\u578b\u7531\u4e8e\u5176\u5c42\u6b21\u5316\u7684\u9884\u6d4b\u65b9\u5f0f\u96be\u4ee5\u5b9e\u73b0\u9ad8\u6548\u63a7\u5236\u3002", "method": "SCALAR\u91c7\u7528\u5c3a\u5ea6\u6761\u4ef6\u89e3\u7801\u673a\u5236\uff0c\u4f18\u5316\u63a7\u5236\u7f16\u7801\u548c\u6ce8\u5165\u8fc7\u7a0b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u63a7\u5236\u6548\u7387\u3002", "conclusion": "SCALAR\u4e3aVAR\u6a21\u578b\u7684\u53ef\u63a7\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.20083", "pdf": "https://arxiv.org/pdf/2507.20083", "abs": "https://arxiv.org/abs/2507.20083", "authors": ["Shibang Liu", "Xuemei Xie", "Guangming Shi"], "title": "KB-DMGen: Knowledge-Based Global Guidance and Dynamic Pose Masking for Human Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent methods using diffusion models have made significant progress in human image generation with various control signals such as pose priors. In portrait generation, both the accuracy of human pose and the overall visual quality are crucial for realistic synthesis. Most existing methods focus on controlling the accuracy of generated poses, but ignore the quality assurance of the entire image. In order to ensure the global image quality and pose accuracy, we propose Knowledge-Based Global Guidance and Dynamic pose Masking for human image Generation (KB-DMGen). The Knowledge Base (KB) is designed not only to enhance pose accuracy but also to leverage image feature information to maintain overall image quality. Dynamic Masking (DM) dynamically adjusts the importance of pose-related regions. Experiments demonstrate the effectiveness of our model, achieving new state-of-the-art results in terms of AP and CAP on the HumanArt dataset. The code will be made publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u77e5\u8bc6\u5e93\u548c\u52a8\u6001\u63a9\u7801\u7684\u6269\u6563\u6a21\u578b\uff08KB-DMGen\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u4eba\u50cf\u751f\u6210\u7684\u5168\u5c40\u8d28\u91cf\u548c\u59ff\u6001\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u5173\u6ce8\u59ff\u6001\u51c6\u786e\u6027\uff0c\u4f46\u5ffd\u7565\u4e86\u5168\u5c40\u56fe\u50cf\u8d28\u91cf\uff0cKB-DMGen\u65e8\u5728\u540c\u65f6\u4f18\u5316\u4e24\u8005\u3002", "method": "\u901a\u8fc7\u77e5\u8bc6\u5e93\uff08KB\uff09\u589e\u5f3a\u59ff\u6001\u51c6\u786e\u6027\u5e76\u5229\u7528\u56fe\u50cf\u7279\u5f81\u4fdd\u6301\u5168\u5c40\u8d28\u91cf\uff0c\u52a8\u6001\u63a9\u7801\uff08DM\uff09\u8c03\u6574\u59ff\u6001\u76f8\u5173\u533a\u57df\u7684\u91cd\u8981\u6027\u3002", "result": "\u5728HumanArt\u6570\u636e\u96c6\u4e0a\u53d6\u5f97AP\u548cCAP\u7684\u65b0\u6700\u4f18\u7ed3\u679c\u3002", "conclusion": "KB-DMGen\u5728\u59ff\u6001\u51c6\u786e\u6027\u548c\u5168\u5c40\u8d28\u91cf\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2507.20094", "pdf": "https://arxiv.org/pdf/2507.20094", "abs": "https://arxiv.org/abs/2507.20094", "authors": ["Ankit Sanjyal"], "title": "Local Prompt Adaptation for Style-Consistent Multi-Object Generation in Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.MA"], "comment": "10 Pages, 8 figures, pre-print", "summary": "Diffusion models have become a powerful backbone for text-to-image generation, enabling users to synthesize high-quality visuals from natural language prompts. However, they often struggle with complex prompts involving multiple objects and global or local style specifications. In such cases, the generated scenes tend to lack style uniformity and spatial coherence, limiting their utility in creative and controllable content generation. In this paper, we propose a simple, training-free architectural method called Local Prompt Adaptation (LPA). Our method decomposes the prompt into content and style tokens, and injects them selectively into the U-Net's attention layers at different stages. By conditioning object tokens early and style tokens later in the generation process, LPA enhances both layout control and stylistic consistency. We evaluate our method on a custom benchmark of 50 style-rich prompts across five categories and compare against strong baselines including Composer, MultiDiffusion, Attend-and-Excite, LoRA, and SDXL. Our approach outperforms prior work on both CLIP score and style consistency metrics, offering a new direction for controllable, expressive diffusion-based generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5c40\u90e8\u63d0\u793a\u9002\u5e94\u65b9\u6cd5\uff08LPA\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u6269\u6563\u6a21\u578b\u5728\u590d\u6742\u63d0\u793a\u4e0b\u7684\u98ce\u683c\u4e00\u81f4\u6027\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u591a\u5bf9\u8c61\u548c\u98ce\u683c\u590d\u6742\u7684\u63d0\u793a\u4e0b\u751f\u6210\u6548\u679c\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u98ce\u683c\u7edf\u4e00\u548c\u7a7a\u95f4\u8fde\u8d2f\u6027\u3002", "method": "\u5c06\u63d0\u793a\u5206\u89e3\u4e3a\u5185\u5bb9\u548c\u98ce\u683c\u6807\u8bb0\uff0c\u9009\u62e9\u6027\u6ce8\u5165U-Net\u6ce8\u610f\u529b\u5c42\uff0c\u65e9\u671f\u6ce8\u5165\u5bf9\u8c61\u6807\u8bb0\uff0c\u540e\u671f\u6ce8\u5165\u98ce\u683c\u6807\u8bb0\u3002", "result": "\u572850\u4e2a\u98ce\u683c\u4e30\u5bcc\u7684\u63d0\u793a\u4e0a\u8bc4\u4f30\uff0cLPA\u5728CLIP\u5206\u6570\u548c\u98ce\u683c\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "LPA\u4e3a\u53ef\u63a7\u3001\u8868\u8fbe\u6027\u5f3a\u7684\u6269\u6563\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.20099", "pdf": "https://arxiv.org/pdf/2507.20099", "abs": "https://arxiv.org/abs/2507.20099", "authors": ["Haoyue Li", "Di Wu"], "title": "Hybrid-Domain Synergistic Transformer for Hyperspectral Image Denoising", "categories": ["cs.CV"], "comment": "10 pages, 4 figures, 4 tables", "summary": "Hyperspectral image denoising faces the challenge of multi-dimensional coupling of spatially non-uniform noise and spectral correlation interference. Existing deep learning methods mostly focus on RGB images and struggle to effectively handle the unique spatial-spectral characteristics and complex noise distributions of hyperspectral images (HSI). This paper proposes an HSI denoising framework, Hybrid-Domain Synergistic Transformer Network (HDST), based on frequency domain enhancement and multiscale modeling, achieving three-dimensional collaborative processing of spatial, frequency and channel domains. The method innovatively integrates three key mechanisms: (1) introducing an FFT preprocessing module with multi-band convolution to extract cross-band correlations and decouple spectral noise components; (2) designing a dynamic cross-domain attention module that adaptively fuses spatial domain texture features and frequency domain noise priors through a learnable gating mechanism; (3) building a hierarchical architecture where shallow layers capture global noise statistics using multiscale atrous convolution, and deep layers achieve detail recovery through frequency domain postprocessing. Experiments on both real and synthetic datasets demonstrate that HDST significantly improves denoising performance while maintaining computational efficiency, validating the effectiveness of the proposed method. This research provides new insights and a universal framework for addressing complex noise coupling issues in HSI and other high-dimensional visual data. The code is available at https://github.com/lhy-cn/HDST-HSIDenoise.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9891\u57df\u589e\u5f3a\u548c\u591a\u5c3a\u5ea6\u5efa\u6a21\u7684HSI\u53bb\u566a\u6846\u67b6HDST\uff0c\u901a\u8fc7\u7a7a\u95f4\u3001\u9891\u7387\u548c\u901a\u9053\u57df\u7684\u4e09\u7ef4\u534f\u540c\u5904\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86HSI\u53bb\u566a\u4e2d\u7684\u591a\u7ef4\u8026\u5408\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9RGB\u56fe\u50cf\uff0c\u96be\u4ee5\u6709\u6548\u5904\u7406HSI\u72ec\u7279\u7684\u7a7a\u95f4-\u5149\u8c31\u7279\u6027\u548c\u590d\u6742\u566a\u58f0\u5206\u5e03\u3002", "method": "HDST\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u673a\u5236\uff1aFFT\u9884\u5904\u7406\u6a21\u5757\u63d0\u53d6\u8de8\u6ce2\u6bb5\u76f8\u5173\u6027\uff0c\u52a8\u6001\u8de8\u57df\u6ce8\u610f\u529b\u6a21\u5757\u878d\u5408\u7a7a\u95f4\u548c\u9891\u57df\u7279\u5f81\uff0c\u4ee5\u53ca\u5206\u5c42\u67b6\u6784\u5b9e\u73b0\u5168\u5c40\u566a\u58f0\u7edf\u8ba1\u548c\u7ec6\u8282\u6062\u590d\u3002", "result": "\u5b9e\u9a8c\u8868\u660eHDST\u5728\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u53bb\u566a\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aHSI\u548c\u5176\u4ed6\u9ad8\u7ef4\u89c6\u89c9\u6570\u636e\u4e2d\u7684\u590d\u6742\u566a\u58f0\u8026\u5408\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u548c\u901a\u7528\u6846\u67b6\u3002"}}
{"id": "2507.20110", "pdf": "https://arxiv.org/pdf/2507.20110", "abs": "https://arxiv.org/abs/2507.20110", "authors": ["Shiyu Liu", "Lianlei Shan"], "title": "NeuroVoxel-LM: Language-Aligned 3D Perception via Dynamic Voxelization and Meta-Embedding", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.4; I.5"], "comment": "**14 pages, 3 figures, 2 tables", "summary": "Recent breakthroughs in Visual Language Models (VLMs) and Multimodal Large Language Models (MLLMs) have significantly advanced 3D scene perception towards language-driven cognition. However, existing 3D language models struggle with sparse, large-scale point clouds due to slow feature extraction and limited representation accuracy. To address these challenges, we propose NeuroVoxel-LM, a novel framework that integrates Neural Radiance Fields (NeRF) with dynamic resolution voxelization and lightweight meta-embedding. Specifically, we introduce a Dynamic Resolution Multiscale Voxelization (DR-MSV) technique that adaptively adjusts voxel granularity based on geometric and structural complexity, reducing computational cost while preserving reconstruction fidelity. In addition, we propose the Token-level Adaptive Pooling for Lightweight Meta-Embedding (TAP-LME) mechanism, which enhances semantic representation through attention-based weighting and residual fusion. Experimental results demonstrate that DR-MSV significantly improves point cloud feature extraction efficiency and accuracy, while TAP-LME outperforms conventional max-pooling in capturing fine-grained semantics from NeRF weights.", "AI": {"tldr": "NeuroVoxel-LM\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408NeRF\u7684\u52a8\u6001\u5206\u8fa8\u7387\u4f53\u7d20\u5316\u548c\u8f7b\u91cf\u7ea7\u5143\u5d4c\u5165\u7684\u65b0\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u67093D\u8bed\u8a00\u6a21\u578b\u5728\u5927\u89c4\u6a21\u70b9\u4e91\u4e2d\u7279\u5f81\u63d0\u53d6\u6162\u548c\u8868\u793a\u7cbe\u5ea6\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u67093D\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u7a00\u758f\u3001\u5927\u89c4\u6a21\u70b9\u4e91\u65f6\u5b58\u5728\u7279\u5f81\u63d0\u53d6\u901f\u5ea6\u6162\u548c\u8868\u793a\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u8bed\u8a00\u9a71\u52a8\u8ba4\u77e5\u7684\u8fdb\u5c55\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u5206\u8fa8\u7387\u591a\u5c3a\u5ea6\u4f53\u7d20\u5316\uff08DR-MSV\uff09\u6280\u672f\uff0c\u6839\u636e\u51e0\u4f55\u548c\u7ed3\u6784\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\u4f53\u7d20\u7c92\u5ea6\uff1b\u5f15\u5165\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u8f7b\u91cf\u7ea7\u5143\u5d4c\u5165\u673a\u5236\uff08TAP-LME\uff09\u589e\u5f3a\u8bed\u4e49\u8868\u793a\u3002", "result": "DR-MSV\u663e\u8457\u63d0\u9ad8\u4e86\u70b9\u4e91\u7279\u5f81\u63d0\u53d6\u7684\u6548\u7387\u548c\u7cbe\u5ea6\uff0cTAP-LME\u5728\u6355\u6349NeRF\u6743\u91cd\u7684\u7ec6\u7c92\u5ea6\u8bed\u4e49\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u6700\u5927\u6c60\u5316\u3002", "conclusion": "NeuroVoxel-LM\u901a\u8fc7\u52a8\u6001\u4f53\u7d20\u5316\u548c\u5143\u5d4c\u5165\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e863D\u573a\u666f\u611f\u77e5\u7684\u8bed\u8a00\u9a71\u52a8\u8ba4\u77e5\u80fd\u529b\u3002"}}
{"id": "2507.20120", "pdf": "https://arxiv.org/pdf/2507.20120", "abs": "https://arxiv.org/abs/2507.20120", "authors": ["Rajat Koner", "Zhipeng Wang", "Srinivas Parthasarathy", "Chinghang Chen"], "title": "Local2Global query Alignment for Video Instance Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Online video segmentation methods excel at handling long sequences and capturing gradual changes, making them ideal for real-world applications. However, achieving temporally consistent predictions remains a challenge, especially with gradual accumulation of noise or drift in on-line propagation, abrupt occlusions and scene transitions. This paper introduces Local2Global, an online framework, for video instance segmentation, exhibiting state-of-the-art performance with simple baseline and training purely in online fashion. Leveraging the DETR-based query propagation framework, we introduce two novel sets of queries:(1) local queries that capture initial object-specific spatial features from each frame and (2) global queries containing past spatio-temporal representations. We propose the L2G-aligner, a novel lightweight transformer decoder, to facilitate an early alignment between local and global queries. This alignment allows our model to effectively utilize current frame information while maintaining temporal consistency, producing a smooth transition between frames. Furthermore, L2G-aligner is integrated within the segmentation model, without relying on additional complex heuristics, or memory mechanisms. Extensive experiments across various challenging VIS and VPS datasets showcase the superiority of our method with simple online training, surpassing current benchmarks without bells and rings. For instance, we achieve 54.3 and 49.4 AP on Youtube-VIS-19/-21 datasets and 37.0 AP on OVIS dataset respectively withthe ResNet-50 backbone.", "AI": {"tldr": "Local2Global\u6846\u67b6\u901a\u8fc7\u5c40\u90e8\u548c\u5168\u5c40\u67e5\u8be2\u7684\u65e9\u671f\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u5728\u7ebf\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\u7684\u9ad8\u6027\u80fd\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\u3002", "motivation": "\u89e3\u51b3\u5728\u7ebf\u89c6\u9891\u5206\u5272\u4e2d\u65f6\u5e8f\u4e00\u81f4\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u566a\u58f0\u79ef\u7d2f\u3001\u906e\u6321\u548c\u573a\u666f\u8f6c\u6362\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165\u5c40\u90e8\u548c\u5168\u5c40\u67e5\u8be2\uff0c\u5229\u7528L2G-aligner\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\u5b9e\u73b0\u65e9\u671f\u5bf9\u9f50\uff0c\u65e0\u9700\u590d\u6742\u542f\u53d1\u5f0f\u6216\u5185\u5b58\u673a\u5236\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5982Youtube-VIS-19/-21\u548cOVIS\uff0cAP\u5206\u522b\u8fbe\u523054.3\u300149.4\u548c37.0\u3002", "conclusion": "Local2Global\u6846\u67b6\u901a\u8fc7\u7b80\u5355\u5728\u7ebf\u8bad\u7ec3\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u51c6\u3002"}}
{"id": "2507.20148", "pdf": "https://arxiv.org/pdf/2507.20148", "abs": "https://arxiv.org/abs/2507.20148", "authors": ["Jingxi Liao", "Shijie Hao", "Richang Hong", "Meng Wang"], "title": "GT-Mean Loss: A Simple Yet Effective Solution for Brightness Mismatch in Low-Light Image Enhancement", "categories": ["cs.CV"], "comment": "Accepted to ICCV2025. GitHub repository:   https://github.com/jingxiLiao/GT-mean-loss", "summary": "Low-light image enhancement (LLIE) aims to improve the visual quality of images captured under poor lighting conditions. In supervised LLIE research, there exists a significant yet often overlooked inconsistency between the overall brightness of an enhanced image and its ground truth counterpart, referred to as brightness mismatch in this study. Brightness mismatch negatively impact supervised LLIE models by misleading model training. However, this issue is largely neglected in current research. In this context, we propose the GT-mean loss, a simple yet effective loss function directly modeling the mean values of images from a probabilistic perspective. The GT-mean loss is flexible, as it extends existing supervised LLIE loss functions into the GT-mean form with minimal additional computational costs. Extensive experiments demonstrate that the incorporation of the GT-mean loss results in consistent performance improvements across various methods and datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGT-mean loss\u7684\u635f\u5931\u51fd\u6570\uff0c\u7528\u4e8e\u89e3\u51b3\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4e2d\u4eae\u5ea6\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u76d1\u7763\u5f0f\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u7814\u7a76\u4e2d\uff0c\u589e\u5f3a\u56fe\u50cf\u4e0e\u771f\u5b9e\u56fe\u50cf\u4e4b\u95f4\u7684\u4eae\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\u88ab\u5ffd\u89c6\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u8bad\u7ec3\u6548\u679c\u3002", "method": "\u63d0\u51faGT-mean loss\uff0c\u4ece\u6982\u7387\u89d2\u5ea6\u76f4\u63a5\u5efa\u6a21\u56fe\u50cf\u5747\u503c\uff0c\u7075\u6d3b\u6269\u5c55\u73b0\u6709\u635f\u5931\u51fd\u6570\uff0c\u8ba1\u7b97\u6210\u672c\u4f4e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGT-mean loss\u80fd\u4e00\u81f4\u63d0\u5347\u4e0d\u540c\u65b9\u6cd5\u548c\u6570\u636e\u96c6\u7684\u6027\u80fd\u3002", "conclusion": "GT-mean loss\u7b80\u5355\u6709\u6548\uff0c\u89e3\u51b3\u4e86\u4eae\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4e3a\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.20158", "pdf": "https://arxiv.org/pdf/2507.20158", "abs": "https://arxiv.org/abs/2507.20158", "authors": ["Yuhong Zhang", "Liyao Wang", "Han Wang", "Danni Wu", "Zuzeng Lin", "Feng Wang", "Li Song"], "title": "AnimeColor: Reference-based Animation Colorization with Diffusion Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Animation colorization plays a vital role in animation production, yet existing methods struggle to achieve color accuracy and temporal consistency. To address these challenges, we propose \\textbf{AnimeColor}, a novel reference-based animation colorization framework leveraging Diffusion Transformers (DiT). Our approach integrates sketch sequences into a DiT-based video diffusion model, enabling sketch-controlled animation generation. We introduce two key components: a High-level Color Extractor (HCE) to capture semantic color information and a Low-level Color Guider (LCG) to extract fine-grained color details from reference images. These components work synergistically to guide the video diffusion process. Additionally, we employ a multi-stage training strategy to maximize the utilization of reference image color information. Extensive experiments demonstrate that AnimeColor outperforms existing methods in color accuracy, sketch alignment, temporal consistency, and visual quality. Our framework not only advances the state of the art in animation colorization but also provides a practical solution for industrial applications. The code will be made publicly available at \\href{https://github.com/IamCreateAI/AnimeColor}{https://github.com/IamCreateAI/AnimeColor}.", "AI": {"tldr": "AnimeColor\u662f\u4e00\u79cd\u57fa\u4e8e\u53c2\u8003\u7684\u52a8\u753b\u7740\u8272\u6846\u67b6\uff0c\u5229\u7528\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u989c\u8272\u51c6\u786e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u52a8\u753b\u7740\u8272\u5728\u52a8\u753b\u5236\u4f5c\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u989c\u8272\u51c6\u786e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faAnimeColor\u6846\u67b6\uff0c\u7ed3\u5408DiT\u548c\u8349\u56fe\u5e8f\u5217\uff0c\u5f15\u5165\u9ad8\u7ea7\u989c\u8272\u63d0\u53d6\u5668\uff08HCE\uff09\u548c\u4f4e\u7ea7\u989c\u8272\u5f15\u5bfc\u5668\uff08LCG\uff09\uff0c\u5e76\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAnimeColor\u5728\u989c\u8272\u51c6\u786e\u6027\u3001\u8349\u56fe\u5bf9\u9f50\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AnimeColor\u4e0d\u4ec5\u63a8\u52a8\u4e86\u52a8\u753b\u7740\u8272\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u8fd8\u4e3a\u5de5\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.20170", "pdf": "https://arxiv.org/pdf/2507.20170", "abs": "https://arxiv.org/abs/2507.20170", "authors": ["Clinton Ansun Mo", "Kun Hu", "Chengjiang Long", "Dong Yuan", "Wan-Chi Siu", "Zhiyong Wang"], "title": "PUMPS: Skeleton-Agnostic Point-based Universal Motion Pre-Training for Synthesis in Human Motion Tasks", "categories": ["cs.CV"], "comment": "Accepted for publication in ICCV 2025", "summary": "Motion skeletons drive 3D character animation by transforming bone hierarchies, but differences in proportions or structure make motion data hard to transfer across skeletons, posing challenges for data-driven motion synthesis. Temporal Point Clouds (TPCs) offer an unstructured, cross-compatible motion representation. Though reversible with skeletons, TPCs mainly serve for compatibility, not for direct motion task learning. Doing so would require data synthesis capabilities for the TPC format, which presents unexplored challenges regarding its unique temporal consistency and point identifiability. Therefore, we propose PUMPS, the primordial autoencoder architecture for TPC data. PUMPS independently reduces frame-wise point clouds into sampleable feature vectors, from which a decoder extracts distinct temporal points using latent Gaussian noise vectors as sampling identifiers. We introduce linear assignment-based point pairing to optimise the TPC reconstruction process, and negate the use of expensive point-wise attention mechanisms in the architecture. Using these latent features, we pre-train a motion synthesis model capable of performing motion prediction, transition generation, and keyframe interpolation. For these pre-training tasks, PUMPS performs remarkably well even without native dataset supervision, matching state-of-the-art performance. When fine-tuned for motion denoising or estimation, PUMPS outperforms many respective methods without deviating from its generalist architecture.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPUMPS\uff0c\u4e00\u79cd\u9488\u5bf9TPC\u6570\u636e\u7684\u81ea\u7f16\u7801\u5668\u67b6\u6784\uff0c\u89e3\u51b3\u4e86TPC\u683c\u5f0f\u5728\u8fd0\u52a8\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u4e0d\u540c\u9aa8\u9abc\u7ed3\u6784\u95f4\u8fd0\u52a8\u6570\u636e\u96be\u4ee5\u8fc1\u79fb\u7684\u95ee\u9898\uff0cTPC\u4f5c\u4e3a\u4e00\u79cd\u8de8\u517c\u5bb9\u7684\u8fd0\u52a8\u8868\u793a\u5f62\u5f0f\uff0c\u4f46\u5176\u5728\u76f4\u63a5\u8fd0\u52a8\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u63a2\u7d22\u3002", "method": "\u63d0\u51faPUMPS\u67b6\u6784\uff0c\u901a\u8fc7\u72ec\u7acb\u964d\u7ef4\u5e27\u7ea7\u70b9\u4e91\u4e3a\u53ef\u91c7\u6837\u7279\u5f81\u5411\u91cf\uff0c\u89e3\u7801\u5668\u5229\u7528\u6f5c\u5728\u9ad8\u65af\u566a\u58f0\u5411\u91cf\u63d0\u53d6\u65f6\u5e8f\u70b9\uff0c\u5e76\u5f15\u5165\u7ebf\u6027\u5206\u914d\u4f18\u5316\u91cd\u5efa\u8fc7\u7a0b\u3002", "result": "PUMPS\u5728\u8fd0\u52a8\u9884\u6d4b\u3001\u8fc7\u6e21\u751f\u6210\u548c\u5173\u952e\u5e27\u63d2\u503c\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u65e0\u9700\u539f\u751f\u6570\u636e\u96c6\u76d1\u7763\u5373\u53ef\u8fbe\u5230\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "PUMPS\u662f\u4e00\u79cd\u901a\u7528\u67b6\u6784\uff0c\u5728\u591a\u9879\u8fd0\u52a8\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c55\u793a\u4e86TPC\u6570\u636e\u5728\u8fd0\u52a8\u5408\u6210\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.20224", "pdf": "https://arxiv.org/pdf/2507.20224", "abs": "https://arxiv.org/abs/2507.20224", "authors": ["Ruizi Yang", "Xiaolu Liu", "Junbo Chen", "Jianke Zhu"], "title": "MambaMap: Online Vectorized HD Map Construction using State Space Model", "categories": ["cs.CV"], "comment": null, "summary": "High-definition (HD) maps are essential for autonomous driving, as they provide precise road information for downstream tasks. Recent advances highlight the potential of temporal modeling in addressing challenges like occlusions and extended perception range. However, existing methods either fail to fully exploit temporal information or incur substantial computational overhead in handling extended sequences. To tackle these challenges, we propose MambaMap, a novel framework that efficiently fuses long-range temporal features in the state space to construct online vectorized HD maps. Specifically, MambaMap incorporates a memory bank to store and utilize information from historical frames, dynamically updating BEV features and instance queries to improve robustness against noise and occlusions. Moreover, we introduce a gating mechanism in the state space, selectively integrating dependencies of map elements in high computational efficiency. In addition, we design innovative multi-directional and spatial-temporal scanning strategies to enhance feature extraction at both BEV and instance levels. These strategies significantly boost the prediction accuracy of our approach while ensuring robust temporal consistency. Extensive experiments on the nuScenes and Argoverse2 datasets demonstrate that our proposed MambaMap approach outperforms state-of-the-art methods across various splits and perception ranges. Source code will be available at https://github.com/ZiziAmy/MambaMap.", "AI": {"tldr": "MambaMap\u662f\u4e00\u4e2a\u9ad8\u6548\u878d\u5408\u957f\u65f6\u7a0b\u65f6\u7a7a\u7279\u5f81\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7ebf\u6784\u5efa\u5411\u91cf\u5316\u9ad8\u6e05\u5730\u56fe\uff0c\u663e\u8457\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5730\u56fe\u9884\u6d4b\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u9ad8\u6e05\u5730\u56fe\u5bf9\u81ea\u52a8\u9a7e\u9a76\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u65f6\u7a7a\u4fe1\u606f\u6216\u8ba1\u7b97\u5f00\u9500\u8fc7\u5927\u3002", "method": "\u63d0\u51faMambaMap\u6846\u67b6\uff0c\u7ed3\u5408\u8bb0\u5fc6\u5e93\u52a8\u6001\u66f4\u65b0BEV\u7279\u5f81\u548c\u5b9e\u4f8b\u67e5\u8be2\uff0c\u5f15\u5165\u95e8\u63a7\u673a\u5236\u548c\u591a\u65b9\u5411\u65f6\u7a7a\u626b\u63cf\u7b56\u7565\u3002", "result": "\u5728nuScenes\u548cArgoverse2\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MambaMap\u901a\u8fc7\u9ad8\u6548\u65f6\u7a7a\u5efa\u6a21\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u6e05\u5730\u56fe\u6784\u5efa\u7684\u6027\u80fd\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2507.20239", "pdf": "https://arxiv.org/pdf/2507.20239", "abs": "https://arxiv.org/abs/2507.20239", "authors": ["Binxiao Huang", "Zhengwu Liu", "Ngai Wong"], "title": "Decomposing Densification in Gaussian Splatting for Faster 3D Scene Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (GS) has emerged as a powerful representation for high-quality scene reconstruction, offering compelling rendering quality. However, the training process of GS often suffers from slow convergence due to inefficient densification and suboptimal spatial distribution of Gaussian primitives. In this work, we present a comprehensive analysis of the split and clone operations during the densification phase, revealing their distinct roles in balancing detail preservation and computational efficiency. Building upon this analysis, we propose a global-to-local densification strategy, which facilitates more efficient growth of Gaussians across the scene space, promoting both global coverage and local refinement. To cooperate with the proposed densification strategy and promote sufficient diffusion of Gaussian primitives in space, we introduce an energy-guided coarse-to-fine multi-resolution training framework, which gradually increases resolution based on energy density in 2D images. Additionally, we dynamically prune unnecessary Gaussian primitives to speed up the training. Extensive experiments on MipNeRF-360, Deep Blending, and Tanks & Temples datasets demonstrate that our approach significantly accelerates training,achieving over 2x speedup with fewer Gaussian primitives and superior reconstruction performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u5c40\u5230\u5c40\u90e8\u7684\u9ad8\u65af\u5206\u5e03\u7b56\u7565\u548c\u80fd\u91cf\u5f15\u5bfc\u7684\u591a\u5206\u8fa8\u7387\u8bad\u7ec3\u6846\u67b6\uff0c\u663e\u8457\u52a0\u901f\u4e863D\u9ad8\u65af\u6cfc\u6e85\u7684\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5e76\u63d0\u5347\u4e86\u91cd\u5efa\u6027\u80fd\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\uff08GS\uff09\u5728\u573a\u666f\u91cd\u5efa\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u8bad\u7ec3\u8fc7\u7a0b\u56e0\u9ad8\u65af\u539f\u8bed\u7684\u7a7a\u95f4\u5206\u5e03\u4e0d\u4f18\u548c\u4f4e\u6548\u7684\u5bc6\u96c6\u5316\u800c\u6536\u655b\u7f13\u6162\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5bc6\u96c6\u5316\u9636\u6bb5\u7684\u62c6\u5206\u548c\u514b\u9686\u64cd\u4f5c\uff0c\u63d0\u51fa\u5168\u5c40\u5230\u5c40\u90e8\u5bc6\u96c6\u5316\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u80fd\u91cf\u5f15\u5bfc\u7684\u591a\u5206\u8fa8\u7387\u8bad\u7ec3\u6846\u67b6\u548c\u52a8\u6001\u526a\u679d\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8d85\u8fc72\u500d\u7684\u8bad\u7ec3\u52a0\u901f\uff0c\u4f7f\u7528\u66f4\u5c11\u7684\u9ad8\u65af\u539f\u8bed\u4e14\u91cd\u5efa\u6027\u80fd\u66f4\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u9ad8\u65af\u539f\u8bed\u7684\u5206\u5e03\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u9ad8\u65af\u6cfc\u6e85\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2507.20291", "pdf": "https://arxiv.org/pdf/2507.20291", "abs": "https://arxiv.org/abs/2507.20291", "authors": ["Qiaosi Yi", "Shuai Li", "Rongyuan Wu", "Lingchen Sun", "Yuhui Wu", "Lei Zhang"], "title": "Fine-structure Preserved Real-world Image Super-resolution via Transfer VAE Training", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "Impressive results on real-world image super-resolution (Real-ISR) have been achieved by employing pre-trained stable diffusion (SD) models. However, one critical issue of such methods lies in their poor reconstruction of image fine structures, such as small characters and textures, due to the aggressive resolution reduction of the VAE (eg., 8$\\times$ downsampling) in the SD model. One solution is to employ a VAE with a lower downsampling rate for diffusion; however, adapting its latent features with the pre-trained UNet while mitigating the increased computational cost poses new challenges. To address these issues, we propose a Transfer VAE Training (TVT) strategy to transfer the 8$\\times$ downsampled VAE into a 4$\\times$ one while adapting to the pre-trained UNet. Specifically, we first train a 4$\\times$ decoder based on the output features of the original VAE encoder, then train a 4$\\times$ encoder while keeping the newly trained decoder fixed. Such a TVT strategy aligns the new encoder-decoder pair with the original VAE latent space while enhancing image fine details. Additionally, we introduce a compact VAE and compute-efficient UNet by optimizing their network architectures, reducing the computational cost while capturing high-resolution fine-scale features. Experimental results demonstrate that our TVT method significantly improves fine-structure preservation, which is often compromised by other SD-based methods, while requiring fewer FLOPs than state-of-the-art one-step diffusion models. The official code can be found at https://github.com/Joyies/TVT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cdTransfer VAE Training (TVT)\u7b56\u7565\uff0c\u901a\u8fc7\u5c068\u500d\u4e0b\u91c7\u6837\u7684VAE\u8f6c\u6362\u4e3a4\u500d\u4e0b\u91c7\u6837\uff0c\u540c\u65f6\u9002\u5e94\u9884\u8bad\u7ec3\u7684UNet\uff0c\u4ee5\u6539\u5584\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4e2d\u7cbe\u7ec6\u7ed3\u6784\u7684\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7a33\u5b9a\u6269\u6563(SD)\u6a21\u578b\u7684\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5728\u91cd\u5efa\u56fe\u50cf\u7cbe\u7ec6\u7ed3\u6784\uff08\u5982\u5c0f\u5b57\u7b26\u548c\u7eb9\u7406\uff09\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662fSD\u6a21\u578b\u4e2dVAE\u7684\u6fc0\u8fdb\u5206\u8fa8\u7387\u964d\u4f4e\uff088\u500d\u4e0b\u91c7\u6837\uff09\u3002", "method": "\u63d0\u51faTVT\u7b56\u7565\uff1a\u5148\u57fa\u4e8e\u539f\u59cbVAE\u7f16\u7801\u5668\u7684\u8f93\u51fa\u7279\u5f81\u8bad\u7ec34\u500d\u89e3\u7801\u5668\uff0c\u518d\u8bad\u7ec34\u500d\u7f16\u7801\u5668\u5e76\u56fa\u5b9a\u89e3\u7801\u5668\u3002\u540c\u65f6\u4f18\u5316VAE\u548cUNet\u7684\u7f51\u7edc\u7ed3\u6784\u4ee5\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTVT\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u7cbe\u7ec6\u7ed3\u6784\u7684\u4fdd\u7559\uff0c\u4e14\u8ba1\u7b97\u91cf\u4f4e\u4e8e\u5f53\u524d\u6700\u4f18\u7684\u4e00\u6b65\u6269\u6563\u6a21\u578b\u3002", "conclusion": "TVT\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86SD\u6a21\u578b\u4e2dVAE\u4e0b\u91c7\u6837\u7387\u8fc7\u9ad8\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2507.20331", "pdf": "https://arxiv.org/pdf/2507.20331", "abs": "https://arxiv.org/abs/2507.20331", "authors": ["Chenjian Gao", "Lihe Ding", "Rui Han", "Zhanpeng Huang", "Zibin Wang", "Tianfan Xue"], "title": "From Gallery to Wrist: Realistic 3D Bracelet Insertion in Videos", "categories": ["cs.CV"], "comment": "12 pages", "summary": "Inserting 3D objects into videos is a longstanding challenge in computer graphics with applications in augmented reality, virtual try-on, and video composition. Achieving both temporal consistency, or realistic lighting remains difficult, particularly in dynamic scenarios with complex object motion, perspective changes, and varying illumination. While 2D diffusion models have shown promise for producing photorealistic edits, they often struggle with maintaining temporal coherence across frames. Conversely, traditional 3D rendering methods excel in spatial and temporal consistency but fall short in achieving photorealistic lighting. In this work, we propose a hybrid object insertion pipeline that combines the strengths of both paradigms. Specifically, we focus on inserting bracelets into dynamic wrist scenes, leveraging the high temporal consistency of 3D Gaussian Splatting (3DGS) for initial rendering and refining the results using a 2D diffusion-based enhancement model to ensure realistic lighting interactions. Our method introduces a shading-driven pipeline that separates intrinsic object properties (albedo, shading, reflectance) and refines both shading and sRGB images for photorealism. To maintain temporal coherence, we optimize the 3DGS model with multi-frame weighted adjustments. This is the first approach to synergize 3D rendering and 2D diffusion for video object insertion, offering a robust solution for realistic and consistent video editing. Project Page: https://cjeen.github.io/BraceletPaper/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u54083D\u6e32\u67d3\u548c2D\u6269\u6563\u6a21\u578b\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u52a8\u6001\u89c6\u9891\u4e2d\u63d2\u51653D\u5bf9\u8c61\uff0c\u4ee5\u5b9e\u73b0\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u771f\u5b9e\u5149\u7167\u3002", "motivation": "\u89e3\u51b3\u5728\u52a8\u6001\u573a\u666f\u4e2d\u63d2\u51653D\u5bf9\u8c61\u65f6\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u771f\u5b9e\u5149\u7167\u95ee\u9898\uff0c\u7ed3\u54082D\u6269\u6563\u6a21\u578b\u548c3D\u6e32\u67d3\u7684\u4f18\u52bf\u3002", "method": "\u4f7f\u75283D\u9ad8\u65af\u6563\u5c04\uff083DGS\uff09\u8fdb\u884c\u521d\u59cb\u6e32\u67d3\uff0c\u5e76\u901a\u8fc72D\u6269\u6563\u6a21\u578b\u589e\u5f3a\u5149\u7167\u6548\u679c\uff0c\u540c\u65f6\u4f18\u5316\u591a\u5e27\u52a0\u6743\u8c03\u6574\u4ee5\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u73b0\u4e86\u5728\u52a8\u6001\u89c6\u9891\u4e2d\u63d2\u51653D\u5bf9\u8c61\u65f6\u7684\u9ad8\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u771f\u5b9e\u5149\u7167\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9996\u6b21\u7ed3\u5408\u4e863D\u6e32\u67d3\u548c2D\u6269\u6563\u6a21\u578b\uff0c\u4e3a\u89c6\u9891\u7f16\u8f91\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u548c\u4e00\u81f4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.20363", "pdf": "https://arxiv.org/pdf/2507.20363", "abs": "https://arxiv.org/abs/2507.20363", "authors": ["Djamel Eddine Boukhari", "Ali chemsa"], "title": "Generative Pre-training for Subjective Tasks: A Diffusion Transformer-Based Framework for Facial Beauty Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Facial Beauty Prediction (FBP) is a challenging computer vision task due to its subjective nature and the subtle, holistic features that influence human perception. Prevailing methods, often based on deep convolutional networks or standard Vision Transformers pre-trained on generic object classification (e.g., ImageNet), struggle to learn feature representations that are truly aligned with high-level aesthetic assessment. In this paper, we propose a novel two-stage framework that leverages the power of generative models to create a superior, domain-specific feature extractor. In the first stage, we pre-train a Diffusion Transformer on a large-scale, unlabeled facial dataset (FFHQ) through a self-supervised denoising task. This process forces the model to learn the fundamental data distribution of human faces, capturing nuanced details and structural priors essential for aesthetic evaluation. In the second stage, the pre-trained and frozen encoder of our Diffusion Transformer is used as a backbone feature extractor, with only a lightweight regression head being fine-tuned on the target FBP dataset (FBP5500). Our method, termed Diff-FBP, sets a new state-of-the-art on the FBP5500 benchmark, achieving a Pearson Correlation Coefficient (PCC) of 0.932, significantly outperforming prior art based on general-purpose pre-training. Extensive ablation studies validate that our generative pre-training strategy is the key contributor to this performance leap, creating feature representations that are more semantically potent for subjective visual tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDiff-FBP\u6846\u67b6\uff0c\u5229\u7528\u751f\u6210\u6a21\u578b\u63d0\u5347\u9762\u90e8\u7f8e\u611f\u9884\u6d4b\u4efb\u52a1\u6027\u80fd\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5b9e\u73b0SOTA\u7ed3\u679c\u3002", "motivation": "\u9762\u90e8\u7f8e\u611f\u9884\u6d4b\uff08FBP\uff09\u56e0\u4e3b\u89c2\u6027\u548c\u590d\u6742\u7279\u5f81\u800c\u5177\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5b66\u4e60\u4e0e\u7f8e\u5b66\u8bc4\u4f30\u5bf9\u9f50\u7684\u7279\u5f81\u8868\u793a\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u81ea\u76d1\u7763\u53bb\u566a\u4efb\u52a1\u9884\u8bad\u7ec3Diffusion Transformer\uff1b2\uff09\u51bb\u7ed3\u7f16\u7801\u5668\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u5fae\u8c03\u8f7b\u91cf\u56de\u5f52\u5934\u3002", "result": "\u5728FBP5500\u6570\u636e\u96c6\u4e0a\u8fbe\u5230PCC 0.932\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u751f\u6210\u5f0f\u9884\u8bad\u7ec3\u7b56\u7565\u662f\u5173\u952e\uff0c\u80fd\u5b66\u4e60\u66f4\u5177\u8bed\u4e49\u6f5c\u529b\u7684\u7279\u5f81\u8868\u793a\u3002"}}
{"id": "2507.20368", "pdf": "https://arxiv.org/pdf/2507.20368", "abs": "https://arxiv.org/abs/2507.20368", "authors": ["Shuolin Xu", "Bingyuan Wang", "Zeyu Cai", "Fangteng Fu", "Yue Ma", "Tongyi Lee", "Hongchuan Yu", "Zeyu Wang"], "title": "MagicAnime: A Hierarchically Annotated, Multimodal and Multitasking Dataset with Benchmarks for Cartoon Animation Generation", "categories": ["cs.CV", "cs.MM"], "comment": "8 pages,6 figures", "summary": "Generating high-quality cartoon animations multimodal control is challenging due to the complexity of non-human characters, stylistically diverse motions and fine-grained emotions. There is a huge domain gap between real-world videos and cartoon animation, as cartoon animation is usually abstract and has exaggerated motion. Meanwhile, public multimodal cartoon data are extremely scarce due to the difficulty of large-scale automatic annotation processes compared with real-life scenarios. To bridge this gap, We propose the MagicAnime dataset, a large-scale, hierarchically annotated, and multimodal dataset designed to support multiple video generation tasks, along with the benchmarks it includes. Containing 400k video clips for image-to-video generation, 50k pairs of video clips and keypoints for whole-body annotation, 12k pairs of video clips for video-to-video face animation, and 2.9k pairs of video and audio clips for audio-driven face animation. Meanwhile, we also build a set of multi-modal cartoon animation benchmarks, called MagicAnime-Bench, to support the comparisons of different methods in the tasks above. Comprehensive experiments on four tasks, including video-driven face animation, audio-driven face animation, image-to-video animation, and pose-driven character animation, validate its effectiveness in supporting high-fidelity, fine-grained, and controllable generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MagicAnime\u6570\u636e\u96c6\u548cMagicAnime-Bench\u57fa\u51c6\uff0c\u7528\u4e8e\u652f\u6301\u591a\u6a21\u6001\u5361\u901a\u52a8\u753b\u751f\u6210\u4efb\u52a1\uff0c\u586b\u8865\u4e86\u771f\u5b9e\u89c6\u9891\u4e0e\u5361\u901a\u52a8\u753b\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\u3002", "motivation": "\u5361\u901a\u52a8\u753b\u751f\u6210\u9762\u4e34\u975e\u4eba\u7c7b\u89d2\u8272\u590d\u6742\u6027\u3001\u591a\u6837\u5316\u52a8\u4f5c\u548c\u7cbe\u7ec6\u60c5\u611f\u7684\u6311\u6218\uff0c\u4e14\u7f3a\u4e4f\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6807\u6ce8\u6570\u636e\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b40\u4e07\u89c6\u9891\u7247\u6bb5\u7684\u591a\u5c42\u6b21\u6807\u6ce8\u6570\u636e\u96c6MagicAnime\uff0c\u5e76\u8bbe\u8ba1\u4e86\u591a\u4efb\u52a1\u57fa\u51c6MagicAnime-Bench\u3002", "result": "\u5728\u56db\u9879\u4efb\u52a1\uff08\u89c6\u9891\u9a71\u52a8\u9762\u90e8\u52a8\u753b\u3001\u97f3\u9891\u9a71\u52a8\u9762\u90e8\u52a8\u753b\u3001\u56fe\u50cf\u5230\u89c6\u9891\u52a8\u753b\u3001\u59ff\u6001\u9a71\u52a8\u89d2\u8272\u52a8\u753b\uff09\u4e2d\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u3002", "conclusion": "MagicAnime\u6570\u636e\u96c6\u548c\u57fa\u51c6\u652f\u6301\u9ad8\u4fdd\u771f\u3001\u7ec6\u7c92\u5ea6\u4e14\u53ef\u63a7\u7684\u5361\u901a\u52a8\u753b\u751f\u6210\u3002"}}
{"id": "2507.20388", "pdf": "https://arxiv.org/pdf/2507.20388", "abs": "https://arxiv.org/abs/2507.20388", "authors": ["Alexandru Brateanu", "Raul Balmez", "Ciprian Orhei", "Codruta Ancuti", "Cosmin Ancuti"], "title": "ModalFormer: Multimodal Transformer for Low-Light Image Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Low-light image enhancement (LLIE) is a fundamental yet challenging task due to the presence of noise, loss of detail, and poor contrast in images captured under insufficient lighting conditions. Recent methods often rely solely on pixel-level transformations of RGB images, neglecting the rich contextual information available from multiple visual modalities. In this paper, we present ModalFormer, the first large-scale multimodal framework for LLIE that fully exploits nine auxiliary modalities to achieve state-of-the-art performance. Our model comprises two main components: a Cross-modal Transformer (CM-T) designed to restore corrupted images while seamlessly integrating multimodal information, and multiple auxiliary subnetworks dedicated to multimodal feature reconstruction. Central to the CM-T is our novel Cross-modal Multi-headed Self-Attention mechanism (CM-MSA), which effectively fuses RGB data with modality-specific features--including deep feature embeddings, segmentation information, geometric cues, and color information--to generate information-rich hybrid attention maps. Extensive experiments on multiple benchmark datasets demonstrate ModalFormer's state-of-the-art performance in LLIE. Pre-trained models and results are made available at https://github.com/albrateanu/ModalFormer.", "AI": {"tldr": "ModalFormer\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u7684\u5927\u89c4\u6a21\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u4e5d\u79cd\u8f85\u52a9\u6a21\u6001\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\u4efb\u52a1\u9762\u4e34\u566a\u58f0\u3001\u7ec6\u8282\u4e22\u5931\u548c\u5bf9\u6bd4\u5ea6\u5dee\u7b49\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f9d\u8d56RGB\u56fe\u50cf\u7684\u50cf\u7d20\u7ea7\u53d8\u6362\uff0c\u5ffd\u7565\u4e86\u591a\u6a21\u6001\u7684\u4e30\u5bcc\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "method": "\u63d0\u51faModalFormer\u6846\u67b6\uff0c\u5305\u542b\u8de8\u6a21\u6001Transformer\uff08CM-T\uff09\u548c\u591a\u4e2a\u8f85\u52a9\u5b50\u7f51\u7edc\uff0c\u5229\u7528\u65b0\u9896\u7684\u8de8\u6a21\u6001\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff08CM-MSA\uff09\u878d\u5408RGB\u6570\u636e\u4e0e\u591a\u6a21\u6001\u7279\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cModalFormer\u5728\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "ModalFormer\u901a\u8fc7\u5145\u5206\u5229\u7528\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\u7684\u6548\u679c\u3002"}}
{"id": "2507.20454", "pdf": "https://arxiv.org/pdf/2507.20454", "abs": "https://arxiv.org/abs/2507.20454", "authors": ["Zhuokun Chen", "Jugang Fan", "Zhuowei Yu", "Bohan Zhuang", "Mingkui Tan"], "title": "Frequency-Aware Autoregressive Modeling for Efficient High-Resolution Image Synthesis", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Visual autoregressive modeling, based on the next-scale prediction paradigm, exhibits notable advantages in image quality and model scalability over traditional autoregressive and diffusion models. It generates images by progressively refining resolution across multiple stages. However, the computational overhead in high-resolution stages remains a critical challenge due to the substantial number of tokens involved. In this paper, we introduce SparseVAR, a plug-and-play acceleration framework for next-scale prediction that dynamically excludes low-frequency tokens during inference without requiring additional training. Our approach is motivated by the observation that tokens in low-frequency regions have a negligible impact on image quality in high-resolution stages and exhibit strong similarity with neighboring tokens. Additionally, we observe that different blocks in the next-scale prediction model focus on distinct regions, with some concentrating on high-frequency areas. SparseVAR leverages these insights by employing lightweight MSE-based metrics to identify low-frequency tokens while preserving the fidelity of excluded regions through a small set of uniformly sampled anchor tokens. By significantly reducing the computational cost while maintaining high image generation quality, SparseVAR achieves notable acceleration in both HART and Infinity. Specifically, SparseVAR achieves up to a 2 times speedup with minimal quality degradation in Infinity-2B.", "AI": {"tldr": "SparseVAR\u662f\u4e00\u79cd\u7528\u4e8e\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u7684\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6392\u9664\u4f4e\u9891\u6807\u8bb0\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u5728\u9ad8\u5206\u8fa8\u7387\u9636\u6bb5\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u4f4e\u9891\u6807\u8bb0\u5bf9\u56fe\u50cf\u8d28\u91cf\u5f71\u54cd\u5c0f\u4e14\u76f8\u4f3c\u5ea6\u9ad8\uff0c\u56e0\u6b64\u63d0\u51fa\u52a8\u6001\u6392\u9664\u4f4e\u9891\u6807\u8bb0\u7684\u52a0\u901f\u65b9\u6cd5\u3002", "method": "SparseVAR\u5229\u7528\u8f7b\u91cf\u7ea7MSE\u6307\u6807\u8bc6\u522b\u4f4e\u9891\u6807\u8bb0\uff0c\u5e76\u901a\u8fc7\u951a\u70b9\u6807\u8bb0\u4fdd\u7559\u533a\u57df\u4fdd\u771f\u5ea6\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5728Infinity-2B\u6a21\u578b\u4e2d\uff0cSparseVAR\u5b9e\u73b0\u4e862\u500d\u52a0\u901f\u4e14\u56fe\u50cf\u8d28\u91cf\u635f\u5931\u6781\u5c0f\u3002", "conclusion": "SparseVAR\u901a\u8fc7\u52a8\u6001\u6392\u9664\u4f4e\u9891\u6807\u8bb0\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u3002"}}
{"id": "2507.20480", "pdf": "https://arxiv.org/pdf/2507.20480", "abs": "https://arxiv.org/abs/2507.20480", "authors": ["Shiyang Liu", "Dianyi Yang", "Yu Gao", "Bohan Ren", "Yi Yang", "Mengyin Fu"], "title": "Automated 3D-GS Registration and Fusion via Skeleton Alignment and Gaussian-Adaptive Features", "categories": ["cs.CV"], "comment": "Accepted to IROS 2025", "summary": "In recent years, 3D Gaussian Splatting (3D-GS)-based scene representation demonstrates significant potential in real-time rendering and training efficiency. However, most existing methods primarily focus on single-map reconstruction, while the registration and fusion of multiple 3D-GS sub-maps remain underexplored. Existing methods typically rely on manual intervention to select a reference sub-map as a template and use point cloud matching for registration. Moreover, hard-threshold filtering of 3D-GS primitives often degrades rendering quality after fusion. In this paper, we present a novel approach for automated 3D-GS sub-map alignment and fusion, eliminating the need for manual intervention while enhancing registration accuracy and fusion quality. First, we extract geometric skeletons across multiple scenes and leverage ellipsoid-aware convolution to capture 3D-GS attributes, facilitating robust scene registration. Second, we introduce a multi-factor Gaussian fusion strategy to mitigate the scene element loss caused by rigid thresholding. Experiments on the ScanNet-GSReg and our Coord datasets demonstrate the effectiveness of the proposed method in registration and fusion. For registration, it achieves a 41.9\\% reduction in RRE on complex scenes, ensuring more precise pose estimation. For fusion, it improves PSNR by 10.11 dB, highlighting superior structural preservation. These results confirm its ability to enhance scene alignment and reconstruction fidelity, ensuring more consistent and accurate 3D scene representation for robotic perception and autonomous navigation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u76843D-GS\u5b50\u5730\u56fe\u5bf9\u9f50\u4e0e\u878d\u5408\u65b9\u6cd5\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\uff0c\u63d0\u5347\u4e86\u914d\u51c6\u7cbe\u5ea6\u548c\u878d\u5408\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u4eba\u5de5\u5e72\u9884\u4e14\u878d\u5408\u540e\u6e32\u67d3\u8d28\u91cf\u4e0b\u964d\uff0c\u9700\u89e3\u51b3\u591a\u5b50\u5730\u56fe\u81ea\u52a8\u914d\u51c6\u4e0e\u9ad8\u8d28\u91cf\u878d\u5408\u95ee\u9898\u3002", "method": "\u63d0\u53d6\u51e0\u4f55\u9aa8\u67b6\u5e76\u5229\u7528\u692d\u7403\u611f\u77e5\u5377\u79ef\u6355\u6349\u5c5e\u6027\uff0c\u63d0\u51fa\u591a\u56e0\u7d20\u9ad8\u65af\u878d\u5408\u7b56\u7565\u3002", "result": "\u914d\u51c6\u8bef\u5dee\u964d\u4f4e41.9%\uff0c\u878d\u5408PSNR\u63d0\u534710.11dB\u3002", "conclusion": "\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u573a\u666f\u5bf9\u9f50\u4e0e\u91cd\u5efa\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u611f\u77e5\u4e0e\u81ea\u4e3b\u5bfc\u822a\u3002"}}
{"id": "2507.20512", "pdf": "https://arxiv.org/pdf/2507.20512", "abs": "https://arxiv.org/abs/2507.20512", "authors": ["Haiyang Bai", "Jiaqi Zhu", "Songru Jiang", "Wei Huang", "Tao Lu", "Yuanqi Li", "Jie Guo", "Runze Fu", "Yanwen Guo", "Lijun Chen"], "title": "GaRe: Relightable 3D Gaussian Splatting for Outdoor Scenes from Unconstrained Photo Collections", "categories": ["cs.CV"], "comment": null, "summary": "We propose a 3D Gaussian splatting-based framework for outdoor relighting that leverages intrinsic image decomposition to precisely integrate sunlight, sky radiance, and indirect lighting from unconstrained photo collections. Unlike prior methods that compress the per-image global illumination into a single latent vector, our approach enables simultaneously diverse shading manipulation and the generation of dynamic shadow effects. This is achieved through three key innovations: (1) a residual-based sun visibility extraction method to accurately separate direct sunlight effects, (2) a region-based supervision framework with a structural consistency loss for physically interpretable and coherent illumination decomposition, and (3) a ray-tracing-based technique for realistic shadow simulation. Extensive experiments demonstrate that our framework synthesizes novel views with competitive fidelity against state-of-the-art relighting solutions and produces more natural and multifaceted illumination and shadow effects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u6237\u5916\u91cd\u5149\u7167\u6846\u67b6\uff0c\u901a\u8fc7\u672c\u5f81\u56fe\u50cf\u5206\u89e3\u7cbe\u786e\u6574\u5408\u9633\u5149\u3001\u5929\u7a7a\u8f90\u5c04\u548c\u95f4\u63a5\u5149\u7167\uff0c\u652f\u6301\u591a\u6837\u7684\u9634\u5f71\u64cd\u4f5c\u548c\u52a8\u6001\u9634\u5f71\u6548\u679c\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u5168\u5c40\u5149\u7167\u538b\u7f29\u4e3a\u5355\u4e00\u6f5c\u5728\u5411\u91cf\uff0c\u9650\u5236\u4e86\u5149\u7167\u5206\u89e3\u7684\u591a\u6837\u6027\u548c\u9634\u5f71\u6548\u679c\u7684\u771f\u5b9e\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "1. \u57fa\u4e8e\u6b8b\u5dee\u7684\u9633\u5149\u53ef\u89c1\u6027\u63d0\u53d6\u65b9\u6cd5\uff1b2. \u533a\u57df\u76d1\u7763\u6846\u67b6\u4e0e\u7ed3\u6784\u4e00\u81f4\u6027\u635f\u5931\uff1b3. \u57fa\u4e8e\u5149\u7ebf\u8ffd\u8e2a\u7684\u9634\u5f71\u6a21\u62df\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u65b0\u89c6\u56fe\u65f6\u5177\u6709\u9ad8\u4fdd\u771f\u5ea6\uff0c\u5e76\u80fd\u4ea7\u751f\u66f4\u81ea\u7136\u3001\u591a\u9762\u7684\u5149\u7167\u548c\u9634\u5f71\u6548\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u6237\u5916\u91cd\u5149\u7167\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.20536", "pdf": "https://arxiv.org/pdf/2507.20536", "abs": "https://arxiv.org/abs/2507.20536", "authors": ["Chieh-Yun Chen", "Min Shi", "Gong Zhang", "Humphrey Shi"], "title": "T2I-Copilot: A Training-Free Multi-Agent Text-to-Image System for Enhanced Prompt Interpretation and Interactive Generation", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "ICCV 2025", "summary": "Text-to-Image (T2I) generative models have revolutionized content creation but remain highly sensitive to prompt phrasing, often requiring users to repeatedly refine prompts multiple times without clear feedback. While techniques such as automatic prompt engineering, controlled text embeddings, denoising, and multi-turn generation mitigate these issues, they offer limited controllability, or often necessitate additional training, restricting the generalization abilities. Thus, we introduce T2I-Copilot, a training-free multi-agent system that leverages collaboration between (Multimodal) Large Language Models to automate prompt phrasing, model selection, and iterative refinement. This approach significantly simplifies prompt engineering while enhancing generation quality and text-image alignment compared to direct generation. Specifically, T2I-Copilot consists of three agents: (1) Input Interpreter, which parses the input prompt, resolves ambiguities, and generates a standardized report; (2) Generation Engine, which selects the appropriate model from different types of T2I models and organizes visual and textual prompts to initiate generation; and (3) Quality Evaluator, which assesses aesthetic quality and text-image alignment, providing scores and feedback for potential regeneration. T2I-Copilot can operate fully autonomously while also supporting human-in-the-loop intervention for fine-grained control. On GenAI-Bench, using open-source generation models, T2I-Copilot achieves a VQA score comparable to commercial models RecraftV3 and Imagen 3, surpasses FLUX1.1-pro by 6.17% at only 16.59% of its cost, and outperforms FLUX.1-dev and SD 3.5 Large by 9.11% and 6.36%. Code will be released at: https://github.com/SHI-Labs/T2I-Copilot.", "AI": {"tldr": "T2I-Copilot\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u534f\u4f5c\u4f18\u5316\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u8d28\u91cf\u548c\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5bf9\u63d0\u793a\u8bcd\u654f\u611f\uff0c\u9700\u8981\u53cd\u590d\u4f18\u5316\u4e14\u7f3a\u4e4f\u660e\u786e\u53cd\u9988\uff0c\u73b0\u6709\u6280\u672f\u53ef\u63a7\u6027\u6709\u9650\u6216\u9700\u989d\u5916\u8bad\u7ec3\u3002", "method": "T2I-Copilot\u7531\u4e09\u4e2a\u667a\u80fd\u4f53\u7ec4\u6210\uff1a\u8f93\u5165\u89e3\u6790\u5668\u3001\u751f\u6210\u5f15\u64ce\u548c\u8d28\u91cf\u8bc4\u4f30\u5668\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u63d0\u793a\u8bcd\u4f18\u5316\u548c\u6a21\u578b\u9009\u62e9\u3002", "result": "\u5728\u5f00\u6e90\u6a21\u578b\u4e0a\uff0cT2I-Copilot\u6027\u80fd\u63a5\u8fd1\u5546\u4e1a\u6a21\u578b\uff0c\u6210\u672c\u4ec5\u4e3aFLUX1.1-pro\u768416.59%\uff0c\u4e14\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u5f00\u6e90\u6a21\u578b\u3002", "conclusion": "T2I-Copilot\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u7b80\u5316\u4e86\u63d0\u793a\u5de5\u7a0b\uff0c\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u652f\u6301\u5168\u81ea\u52a8\u6216\u4eba\u5de5\u5e72\u9884\u6a21\u5f0f\u3002"}}
{"id": "2507.20590", "pdf": "https://arxiv.org/pdf/2507.20590", "abs": "https://arxiv.org/abs/2507.20590", "authors": ["Xinqi Lin", "Fanghua Yu", "Jinfan Hu", "Zhiyuan You", "Wu Shi", "Jimmy S. Ren", "Jinjin Gu", "Chao Dong"], "title": "Harnessing Diffusion-Yielded Score Priors for Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Deep image restoration models aim to learn a mapping from degraded image space to natural image space. However, they face several critical challenges: removing degradation, generating realistic details, and ensuring pixel-level consistency. Over time, three major classes of methods have emerged, including MSE-based, GAN-based, and diffusion-based methods. However, they fail to achieve a good balance between restoration quality, fidelity, and speed. We propose a novel method, HYPIR, to address these challenges. Our solution pipeline is straightforward: it involves initializing the image restoration model with a pre-trained diffusion model and then fine-tuning it with adversarial training. This approach does not rely on diffusion loss, iterative sampling, or additional adapters. We theoretically demonstrate that initializing adversarial training from a pre-trained diffusion model positions the initial restoration model very close to the natural image distribution. Consequently, this initialization improves numerical stability, avoids mode collapse, and substantially accelerates the convergence of adversarial training. Moreover, HYPIR inherits the capabilities of diffusion models with rich user control, enabling text-guided restoration and adjustable texture richness. Requiring only a single forward pass, it achieves faster convergence and inference speed than diffusion-based methods. Extensive experiments show that HYPIR outperforms previous state-of-the-art methods, achieving efficient and high-quality image restoration.", "AI": {"tldr": "HYPIR\u662f\u4e00\u79cd\u65b0\u578b\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u548c\u5bf9\u6297\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4fee\u590d\u8d28\u91cf\u3001\u4fdd\u771f\u5ea6\u548c\u901f\u5ea6\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\uff08\u5982MSE\u3001GAN\u548c\u6269\u6563\u6a21\u578b\uff09\u96be\u4ee5\u5e73\u8861\u4fee\u590d\u8d28\u91cf\u3001\u4fdd\u771f\u5ea6\u548c\u901f\u5ea6\uff0cHYPIR\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "HYPIR\u901a\u8fc7\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u521d\u59cb\u5316\u4fee\u590d\u6a21\u578b\uff0c\u518d\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u5fae\u8c03\uff0c\u65e0\u9700\u6269\u6563\u635f\u5931\u6216\u8fed\u4ee3\u91c7\u6837\u3002", "result": "HYPIR\u5728\u4fee\u590d\u8d28\u91cf\u3001\u901f\u5ea6\u548c\u7528\u6237\u63a7\u5236\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u3002", "conclusion": "HYPIR\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u5bf9\u6297\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u4fee\u590d\uff0c\u5e76\u5177\u5907\u4e30\u5bcc\u7684\u7528\u6237\u63a7\u5236\u529f\u80fd\u3002"}}
{"id": "2507.20680", "pdf": "https://arxiv.org/pdf/2507.20680", "abs": "https://arxiv.org/abs/2507.20680", "authors": ["Deepak Joshi", "Mayukha Pal"], "title": "Lightweight Transformer-Driven Segmentation of Hotspots and Snail Trails in Solar PV Thermal Imagery", "categories": ["cs.CV"], "comment": "31 pages, 6 figures", "summary": "Accurate detection of defects such as hotspots and snail trails in photovoltaic modules is essential for maintaining energy efficiency and system reliablility. This work presents a supervised deep learning framework for segmenting thermal infrared images of PV panels, using a dataset of 277 aerial thermographic images captured by zenmuse XT infrared camera mounted on a DJI Matrice 100 drone. The preprocessing pipeline includes image resizing, CLAHE based contrast enhancement, denoising, and normalisation. A lightweight semantic segmentation model based on SegFormer is developed, featuring a customised Transformwer encoder and streamlined decoder, and fine-tuned on annotated images with manually labeled defect regions. To evaluate performance, we benchmark our model against U-Net, DeepLabV3, PSPNet, and Mask2Former using consistent preprocessing and augmentation. Evaluation metrices includes per-class Dice score, F1-score, Cohen's kappa, mean IoU, and pixel accuracy. The SegFormer-based model outperforms baselines in accuracy and efficiency, particularly for segmenting small and irregular defects. Its lightweight design real-time deployment on edge devices and seamless integration with drone-based systems for automated inspection of large-scale solar farms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSegFormer\u7684\u8f7b\u91cf\u7ea7\u8bed\u4e49\u5206\u5272\u6a21\u578b\uff0c\u7528\u4e8e\u5149\u4f0f\u6a21\u5757\u70ed\u7ea2\u5916\u56fe\u50cf\u4e2d\u7684\u7f3a\u9677\u68c0\u6d4b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5149\u4f0f\u6a21\u5757\u4e2d\u7684\u70ed\u70b9\u548c\u8717\u725b\u7eb9\u7b49\u7f3a\u9677\u5f71\u54cd\u80fd\u6e90\u6548\u7387\u548c\u7cfb\u7edf\u53ef\u9760\u6027\uff0c\u9700\u8981\u9ad8\u6548\u51c6\u786e\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528277\u5f20\u65e0\u4eba\u673a\u62cd\u6444\u7684\u70ed\u7ea2\u5916\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u9884\u5904\u7406\u5305\u62ec\u56fe\u50cf\u8c03\u6574\u3001CLAHE\u5bf9\u6bd4\u589e\u5f3a\u3001\u53bb\u566a\u548c\u5f52\u4e00\u5316\uff1b\u5f00\u53d1\u57fa\u4e8eSegFormer\u7684\u8f7b\u91cf\u6a21\u578b\uff0c\u5b9a\u5236Transformer\u7f16\u7801\u5668\u548c\u7b80\u5316\u89e3\u7801\u5668\u3002", "result": "SegFormer\u6a21\u578b\u5728Dice\u5206\u6570\u3001F1\u5206\u6570\u3001Cohen's kappa\u3001\u5e73\u5747IoU\u548c\u50cf\u7d20\u7cbe\u5ea6\u7b49\u6307\u6807\u4e0a\u4f18\u4e8eU-Net\u3001DeepLabV3\u3001PSPNet\u548cMask2Former\u3002", "conclusion": "\u8be5\u6a21\u578b\u8f7b\u91cf\u9ad8\u6548\uff0c\u9002\u5408\u5b9e\u65f6\u90e8\u7f72\u4e8e\u8fb9\u7f18\u8bbe\u5907\uff0c\u53ef\u96c6\u6210\u81f3\u65e0\u4eba\u673a\u7cfb\u7edf\u7528\u4e8e\u5927\u89c4\u6a21\u592a\u9633\u80fd\u519c\u573a\u81ea\u52a8\u5316\u68c0\u6d4b\u3002"}}
{"id": "2507.20721", "pdf": "https://arxiv.org/pdf/2507.20721", "abs": "https://arxiv.org/abs/2507.20721", "authors": ["Haowen Li", "Zhenfeng Fan", "Zhang Wen", "Zhengzhou Zhu", "Yunjin Li"], "title": "AIComposer: Any Style and Content Image Composition via Feature Integration", "categories": ["cs.CV"], "comment": null, "summary": "Image composition has advanced significantly with large-scale pre-trained T2I diffusion models. Despite progress in same-domain composition, cross-domain composition remains under-explored. The main challenges are the stochastic nature of diffusion models and the style gap between input images, leading to failures and artifacts. Additionally, heavy reliance on text prompts limits practical applications. This paper presents the first cross-domain image composition method that does not require text prompts, allowing natural stylization and seamless compositions. Our method is efficient and robust, preserving the diffusion prior, as it involves minor steps for backward inversion and forward denoising without training the diffuser. Our method also uses a simple multilayer perceptron network to integrate CLIP features from foreground and background, manipulating diffusion with a local cross-attention strategy. It effectively preserves foreground content while enabling stable stylization without a pre-stylization network. Finally, we create a benchmark dataset with diverse contents and styles for fair evaluation, addressing the lack of testing datasets for cross-domain image composition. Our method outperforms state-of-the-art techniques in both qualitative and quantitative evaluations, significantly improving the LPIPS score by 30.5% and the CSD metric by 18.1%. We believe our method will advance future research and applications. Code and benchmark at https://github.com/sherlhw/AIComposer.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6587\u672c\u63d0\u793a\u7684\u8de8\u57df\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b80\u5355\u9ad8\u6548\u7684\u6b65\u9aa4\u5b9e\u73b0\u81ea\u7136\u98ce\u683c\u5316\u548c\u65e0\u7f1d\u5408\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u6307\u6807\u3002", "motivation": "\u8de8\u57df\u56fe\u50cf\u5408\u6210\u56e0\u6269\u6563\u6a21\u578b\u7684\u968f\u673a\u6027\u548c\u8f93\u5165\u56fe\u50cf\u98ce\u683c\u5dee\u5f02\u800c\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6587\u672c\u63d0\u793a\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u53cd\u5411\u53cd\u8f6c\u548c\u6b63\u5411\u53bb\u566a\u7684\u8f7b\u91cf\u6b65\u9aa4\uff0c\u7ed3\u5408\u591a\u5c42\u611f\u77e5\u673a\u6574\u5408CLIP\u7279\u5f81\uff0c\u5229\u7528\u5c40\u90e8\u4ea4\u53c9\u6ce8\u610f\u529b\u7b56\u7565\u64cd\u7eb5\u6269\u6563\u8fc7\u7a0b\u3002", "result": "\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0cLPIPS\u5206\u6570\u63d0\u534730.5%\uff0cCSD\u6307\u6807\u63d0\u534718.1%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8de8\u57df\u56fe\u50cf\u5408\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u672a\u6765\u7814\u7a76\u548c\u5e94\u7528\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.20745", "pdf": "https://arxiv.org/pdf/2507.20745", "abs": "https://arxiv.org/abs/2507.20745", "authors": ["Yue Zhu", "Haiwen Diao", "Shang Gao", "Jiazuo Yu", "Jiawen Zhu", "Yunzhi Zhuge", "Shuai Hao", "Xu Jia", "Lu Zhang", "Ying Zhang", "Huchuan Lu"], "title": "Regularizing Subspace Redundancy of Low-Rank Adaptation", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "10 pages, 4 figures, Accepted by ACMMM2025", "summary": "Low-Rank Adaptation (LoRA) and its variants have delivered strong capability in Parameter-Efficient Transfer Learning (PETL) by minimizing trainable parameters and benefiting from reparameterization. However, their projection matrices remain unrestricted during training, causing high representation redundancy and diminishing the effectiveness of feature adaptation in the resulting subspaces. While existing methods mitigate this by manually adjusting the rank or implicitly applying channel-wise masks, they lack flexibility and generalize poorly across various datasets and architectures. Hence, we propose ReSoRA, a method that explicitly models redundancy between mapping subspaces and adaptively Regularizes Subspace redundancy of Low-Rank Adaptation. Specifically, it theoretically decomposes the low-rank submatrices into multiple equivalent subspaces and systematically applies de-redundancy constraints to the feature distributions across different projections. Extensive experiments validate that our proposed method consistently facilitates existing state-of-the-art PETL methods across various backbones and datasets in vision-language retrieval and standard visual classification benchmarks. Besides, as a training supervision, ReSoRA can be seamlessly integrated into existing approaches in a plug-and-play manner, with no additional inference costs. Code is publicly available at: https://github.com/Lucenova/ReSoRA.", "AI": {"tldr": "ReSoRA\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u6b63\u5219\u5316\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u5b50\u7a7a\u95f4\u5197\u4f59\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u5b50\u77e9\u9635\u5e76\u5e94\u7528\u53bb\u5197\u4f59\u7ea6\u675f\uff0c\u63d0\u5347\u4e86\u53c2\u6570\u9ad8\u6548\u8fc1\u79fb\u5b66\u4e60\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LoRA\u53ca\u5176\u53d8\u4f53\u5728\u8bad\u7ec3\u4e2d\u6295\u5f71\u77e9\u9635\u4e0d\u53d7\u9650\u5236\uff0c\u5bfc\u81f4\u8868\u793a\u5197\u4f59\u548c\u7279\u5f81\u9002\u5e94\u6548\u679c\u4e0b\u964d\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "ReSoRA\u5c06\u4f4e\u79e9\u5b50\u77e9\u9635\u5206\u89e3\u4e3a\u591a\u4e2a\u7b49\u4ef7\u5b50\u7a7a\u95f4\uff0c\u5e76\u7cfb\u7edf\u5730\u5bf9\u4e0d\u540c\u6295\u5f71\u7684\u7279\u5f81\u5206\u5e03\u5e94\u7528\u53bb\u5197\u4f59\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cReSoRA\u5728\u591a\u79cd\u9aa8\u5e72\u7f51\u7edc\u548c\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u4e86\u73b0\u6709PETL\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u989d\u5916\u63a8\u7406\u6210\u672c\u3002", "conclusion": "ReSoRA\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u548c\u6b63\u5219\u5316\u5b50\u7a7a\u95f4\u5197\u4f59\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53c2\u6570\u9ad8\u6548\u8fc1\u79fb\u5b66\u4e60\u7684\u7075\u6d3b\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2507.20782", "pdf": "https://arxiv.org/pdf/2507.20782", "abs": "https://arxiv.org/abs/2507.20782", "authors": ["Pavel Korshunov", "Ketan Kotwal", "Christophe Ecabert", "Vidit Vidit", "Amir Mohammadi", "Sebastien Marcel"], "title": "Investigation of Accuracy and Bias in Face Recognition Trained with Synthetic Data", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted for publication in IEEE International Joint Conference on   Biometrics (IJCB), 2025", "summary": "Synthetic data has emerged as a promising alternative for training face recognition (FR) models, offering advantages in scalability, privacy compliance, and potential for bias mitigation. However, critical questions remain on whether both high accuracy and fairness can be achieved with synthetic data. In this work, we evaluate the impact of synthetic data on bias and performance of FR systems. We generate balanced face dataset, FairFaceGen, using two state of the art text-to-image generators, Flux.1-dev and Stable Diffusion v3.5 (SD35), and combine them with several identity augmentation methods, including Arc2Face and four IP-Adapters. By maintaining equal identity count across synthetic and real datasets, we ensure fair comparisons when evaluating FR performance on standard (LFW, AgeDB-30, etc.) and challenging IJB-B/C benchmarks and FR bias on Racial Faces in-the-Wild (RFW) dataset. Our results demonstrate that although synthetic data still lags behind the real datasets in the generalization on IJB-B/C, demographically balanced synthetic datasets, especially those generated with SD35, show potential for bias mitigation. We also observe that the number and quality of intra-class augmentations significantly affect FR accuracy and fairness. These findings provide practical guidelines for constructing fairer FR systems using synthetic data.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5408\u6210\u6570\u636e\u5728\u8bad\u7ec3\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u4e2d\u7684\u6f5c\u529b\uff0c\u8bc4\u4f30\u4e86\u5176\u5bf9\u6027\u80fd\u548c\u504f\u89c1\u7684\u53cc\u91cd\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u6784\u5efa\u66f4\u516c\u5e73\u7cfb\u7edf\u7684\u5b9e\u7528\u6307\u5357\u3002", "motivation": "\u7814\u7a76\u5408\u6210\u6570\u636e\u662f\u5426\u80fd\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u51cf\u5c11\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u7684\u504f\u89c1\u3002", "method": "\u4f7f\u7528Flux.1-dev\u548cStable Diffusion v3.5\u751f\u6210\u5e73\u8861\u6570\u636e\u96c6FairFaceGen\uff0c\u7ed3\u5408\u591a\u79cd\u8eab\u4efd\u589e\u5f3a\u65b9\u6cd5\uff0c\u5e76\u4e0e\u771f\u5b9e\u6570\u636e\u96c6\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5408\u6210\u6570\u636e\u5728IJB-B/C\u4e0a\u6cdb\u5316\u80fd\u529b\u7a0d\u900a\uff0c\u4f46SD35\u751f\u6210\u7684\u5e73\u8861\u6570\u636e\u96c6\u5728\u51cf\u5c11\u504f\u89c1\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u5728\u6784\u5efa\u516c\u5e73\u7684\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u4e2d\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u5173\u6ce8\u589e\u5f3a\u7684\u6570\u91cf\u548c\u8d28\u91cf\u3002"}}
{"id": "2507.20808", "pdf": "https://arxiv.org/pdf/2507.20808", "abs": "https://arxiv.org/abs/2507.20808", "authors": ["Pavel Korshunov", "Amir Mohammadi", "Vidit Vidit", "Christophe Ecabert", "S\u00e9bastien Marcel"], "title": "FantasyID: A dataset for detecting digital manipulations of ID-documents", "categories": ["cs.CV"], "comment": "Accepted to IJCB 2025; for project page, see   https://www.idiap.ch/paper/fantasyid", "summary": "Advancements in image generation led to the availability of easy-to-use tools for malicious actors to create forged images. These tools pose a serious threat to the widespread Know Your Customer (KYC) applications, requiring robust systems for detection of the forged Identity Documents (IDs). To facilitate the development of the detection algorithms, in this paper, we propose a novel publicly available (including commercial use) dataset, FantasyID, which mimics real-world IDs but without tampering with legal documents and, compared to previous public datasets, it does not contain generated faces or specimen watermarks. FantasyID contains ID cards with diverse design styles, languages, and faces of real people. To simulate a realistic KYC scenario, the cards from FantasyID were printed and captured with three different devices, constituting the bonafide class. We have emulated digital forgery/injection attacks that could be performed by a malicious actor to tamper the IDs using the existing generative tools. The current state-of-the-art forgery detection algorithms, such as TruFor, MMFusion, UniFD, and FatFormer, are challenged by FantasyID dataset. It especially evident, in the evaluation conditions close to practical, with the operational threshold set on validation set so that false positive rate is at 10%, leading to false negative rates close to 50% across the board on the test set. The evaluation experiments demonstrate that FantasyID dataset is complex enough to be used as an evaluation benchmark for detection algorithms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aFantasyID\u7684\u65b0\u516c\u5f00\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u68c0\u6d4b\u4f2a\u9020\u8eab\u4efd\u8bc1\u4ef6\uff0c\u6a21\u62df\u771f\u5b9eKYC\u573a\u666f\uff0c\u6311\u6218\u73b0\u6709\u68c0\u6d4b\u7b97\u6cd5\u3002", "motivation": "\u968f\u7740\u56fe\u50cf\u751f\u6210\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u4f2a\u9020\u8eab\u4efd\u8bc1\u4ef6\u53d8\u5f97\u5bb9\u6613\uff0c\u8fd9\u5bf9KYC\u5e94\u7528\u6784\u6210\u5a01\u80c1\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u68c0\u6d4b\u7cfb\u7edf\u3002", "method": "\u521b\u5efaFantasyID\u6570\u636e\u96c6\uff0c\u6a21\u62df\u771f\u5b9eID\u8bbe\u8ba1\u98ce\u683c\u548c\u8bed\u8a00\uff0c\u5e76\u6253\u5370\u540e\u901a\u8fc7\u4e0d\u540c\u8bbe\u5907\u62cd\u6444\uff0c\u540c\u65f6\u6a21\u62df\u6570\u5b57\u4f2a\u9020\u653b\u51fb\u3002", "result": "\u73b0\u6709\u68c0\u6d4b\u7b97\u6cd5\uff08\u5982TruFor\u3001MMFusion\u7b49\uff09\u5728FantasyID\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5047\u9634\u6027\u7387\u63a5\u8fd150%\u3002", "conclusion": "FantasyID\u6570\u636e\u96c6\u590d\u6742\u5ea6\u9ad8\uff0c\u9002\u5408\u4f5c\u4e3a\u68c0\u6d4b\u7b97\u6cd5\u7684\u8bc4\u4f30\u57fa\u51c6\u3002"}}
{"id": "2507.20855", "pdf": "https://arxiv.org/pdf/2507.20855", "abs": "https://arxiv.org/abs/2507.20855", "authors": ["Adil Kaan Akan", "Yucel Yemez"], "title": "Compositional Video Synthesis by Temporal Object-Centric Learning", "categories": ["cs.CV"], "comment": "12+21 pages, submitted to IEEE Transactions on Pattern Analysis and   Machine Intelligence (TPAMI), currently under review", "summary": "We present a novel framework for compositional video synthesis that leverages temporally consistent object-centric representations, extending our previous work, SlotAdapt, from images to video. While existing object-centric approaches either lack generative capabilities entirely or treat video sequences holistically, thus neglecting explicit object-level structure, our approach explicitly captures temporal dynamics by learning pose invariant object-centric slots and conditioning them on pretrained diffusion models. This design enables high-quality, pixel-level video synthesis with superior temporal coherence, and offers intuitive compositional editing capabilities such as object insertion, deletion, or replacement, maintaining consistent object identities across frames. Extensive experiments demonstrate that our method sets new benchmarks in video generation quality and temporal consistency, outperforming previous object-centric generative methods. Although our segmentation performance closely matches state-of-the-art methods, our approach uniquely integrates this capability with robust generative performance, significantly advancing interactive and controllable video generation and opening new possibilities for advanced content creation, semantic editing, and dynamic scene understanding.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a\u7684\u89c6\u9891\u5408\u6210\u6846\u67b6\uff0c\u6269\u5c55\u4e86\u4e4b\u524d\u7684SlotAdapt\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u65f6\u5e8f\u4e00\u81f4\u7684\u89c6\u9891\u751f\u6210\u548c\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u751f\u6210\u80fd\u529b\u6216\u5ffd\u7565\u5bf9\u8c61\u7ea7\u7ed3\u6784\uff0c\u65e0\u6cd5\u5b9e\u73b0\u65f6\u5e8f\u4e00\u81f4\u7684\u5bf9\u8c61\u7f16\u8f91\u3002", "method": "\u901a\u8fc7\u5b66\u4e60\u59ff\u6001\u4e0d\u53d8\u7684\u5bf9\u8c61\u4e2d\u5fc3\u69fd\uff0c\u5e76\u7ed3\u5408\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u89c6\u9891\u5408\u6210\u548c\u7f16\u8f91\u3002", "result": "\u5728\u89c6\u9891\u751f\u6210\u8d28\u91cf\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u5bf9\u8c61\u63d2\u5165\u3001\u5220\u9664\u548c\u66ff\u6362\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4ea4\u4e92\u5f0f\u89c6\u9891\u751f\u6210\u548c\u8bed\u4e49\u7f16\u8f91\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u63a8\u52a8\u4e86\u52a8\u6001\u573a\u666f\u7406\u89e3\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.20934", "pdf": "https://arxiv.org/pdf/2507.20934", "abs": "https://arxiv.org/abs/2507.20934", "authors": ["Melissa Cote", "Alexandra Branzan Albu"], "title": "Exploring text-to-image generation for historical document image retrieval", "categories": ["cs.CV"], "comment": "Accepted and presented as an extended abstract (double-blind review   process) at the 2025 Scandinavian Conference on Image Analysis (SCIA). 4   pages", "summary": "Attribute-based document image retrieval (ABDIR) was recently proposed as an alternative to query-by-example (QBE) searches, the dominant document image retrieval (DIR) paradigm. One drawback of QBE searches is that they require sample query documents on hand that may not be available. ABDIR aims to offer users a flexible way to retrieve document images based on memorable visual features of document contents, describing document images with combinations of visual attributes determined via convolutional neural network (CNN)-based binary classifiers. We present an exploratory study of the use of generative AI to bridge the gap between QBE and ABDIR, focusing on historical documents as a use case for their diversity and uniqueness in visual features. We hypothesize that text-to-image (T2I) generation can be leveraged to create query document images using text prompts based on ABDIR-like attributes. We propose T2I-QBE, which uses Leonardo.Ai as the T2I generator with prompts that include a rough description of the desired document type and a list of the desired ABDIR-style attributes. This creates query images that are then used within the traditional QBE paradigm, which compares CNN-extracted query features to those of the document images in the dataset to retrieve the most relevant documents. Experiments on the HisIR19 dataset of historical documents confirm our hypothesis and suggest that T2I-QBE is a viable option for historical document image retrieval. To the authors' knowledge, this is the first attempt at utilizing T2I generation for DIR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u751f\u6210\u5f0fAI\u7684\u65b9\u6cd5T2I-QBE\uff0c\u901a\u8fc7\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6280\u672f\u521b\u5efa\u67e5\u8be2\u56fe\u50cf\uff0c\u7528\u4e8e\u5386\u53f2\u6587\u6863\u56fe\u50cf\u68c0\u7d22\uff0c\u5f25\u8865\u4e86\u4f20\u7edfQBE\u548cABDIR\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edfQBE\u641c\u7d22\u9700\u8981\u5b9e\u9645\u67e5\u8be2\u6587\u6863\u6837\u672c\uff0c\u800cABDIR\u867d\u7136\u7075\u6d3b\u4f46\u4f9d\u8d56\u89c6\u89c9\u5c5e\u6027\u63cf\u8ff0\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u751f\u6210\u5f0fAI\uff08\u5982\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff09\u586b\u8865\u4e24\u8005\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u5c24\u5176\u9488\u5bf9\u5386\u53f2\u6587\u6863\u7684\u591a\u6837\u6027\u3002", "method": "\u63d0\u51faT2I-QBE\u65b9\u6cd5\uff0c\u4f7f\u7528Leonardo.Ai\u751f\u6210\u57fa\u4e8eABDIR\u5c5e\u6027\u63cf\u8ff0\u7684\u67e5\u8be2\u56fe\u50cf\uff0c\u518d\u901a\u8fc7\u4f20\u7edfQBE\u8303\u5f0f\u8fdb\u884c\u68c0\u7d22\u3002", "result": "\u5728HisIR19\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86T2I-QBE\u7684\u53ef\u884c\u6027\uff0c\u8868\u660e\u5176\u53ef\u7528\u4e8e\u5386\u53f2\u6587\u6863\u56fe\u50cf\u68c0\u7d22\u3002", "conclusion": "T2I-QBE\u662f\u9996\u6b21\u5c06\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6280\u672f\u5e94\u7528\u4e8e\u6587\u6863\u56fe\u50cf\u68c0\u7d22\u7684\u521b\u65b0\u5c1d\u8bd5\uff0c\u4e3a\u5386\u53f2\u6587\u6863\u68c0\u7d22\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.20953", "pdf": "https://arxiv.org/pdf/2507.20953", "abs": "https://arxiv.org/abs/2507.20953", "authors": ["Dogucan Yaman", "Fevziye Irem Eyiokur", "Leonard B\u00e4rmann", "Haz\u0131m Kemal Ekenel", "Alexander Waibel"], "title": "Mask-Free Audio-driven Talking Face Generation for Enhanced Visual Quality and Identity Preservation", "categories": ["cs.CV"], "comment": null, "summary": "Audio-Driven Talking Face Generation aims at generating realistic videos of talking faces, focusing on accurate audio-lip synchronization without deteriorating any identity-related visual details. Recent state-of-the-art methods are based on inpainting, meaning that the lower half of the input face is masked, and the model fills the masked region by generating lips aligned with the given audio. Hence, to preserve identity-related visual details from the lower half, these approaches additionally require an unmasked identity reference image randomly selected from the same video. However, this common masking strategy suffers from (1) information loss in the input faces, significantly affecting the networks' ability to preserve visual quality and identity details, (2) variation between identity reference and input image degrading reconstruction performance, and (3) the identity reference negatively impacting the model, causing unintended copying of elements unaligned with the audio. To address these issues, we propose a mask-free talking face generation approach while maintaining the 2D-based face editing task. Instead of masking the lower half, we transform the input images to have closed mouths, using a two-step landmark-based approach trained in an unpaired manner. Subsequently, we provide these edited but unmasked faces to a lip adaptation model alongside the audio to generate appropriate lip movements. Thus, our approach needs neither masked input images nor identity reference images. We conduct experiments on the benchmark LRS2 and HDTF datasets and perform various ablation studies to validate our contributions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u63a9\u7801\u7684\u8bf4\u8bdd\u4eba\u8138\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u6b65\u5730\u6807\u8f6c\u6362\u751f\u6210\u95ed\u53e3\u56fe\u50cf\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u63a9\u7801\u65b9\u6cd5\u7684\u4fe1\u606f\u4e22\u5931\u548c\u8eab\u4efd\u53c2\u8003\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u63a9\u7801\u7684\u8bf4\u8bdd\u4eba\u8138\u751f\u6210\u65b9\u6cd5\u4e2d\u7684\u4fe1\u606f\u4e22\u5931\u3001\u8eab\u4efd\u53c2\u8003\u4e0d\u4e00\u81f4\u53ca\u8d1f\u9762\u5e72\u6270\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u5730\u6807\u8f6c\u6362\u5c06\u8f93\u5165\u56fe\u50cf\u8f6c\u6362\u4e3a\u95ed\u53e3\u72b6\u6001\uff0c\u518d\u901a\u8fc7\u5507\u90e8\u9002\u914d\u6a21\u578b\u751f\u6210\u4e0e\u97f3\u9891\u540c\u6b65\u7684\u5507\u90e8\u52a8\u4f5c\u3002", "result": "\u5728LRS2\u548cHDTF\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u65e0\u9700\u63a9\u7801\u6216\u8eab\u4efd\u53c2\u8003\u56fe\u50cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u65e0\u63a9\u7801\u65b9\u6cd5\u5728\u4fdd\u6301\u8eab\u4efd\u7ec6\u8282\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u63a9\u7801\u7b56\u7565\u3002"}}
{"id": "2507.20963", "pdf": "https://arxiv.org/pdf/2507.20963", "abs": "https://arxiv.org/abs/2507.20963", "authors": ["Tianhao Li", "Yang Li", "Mengtian Li", "Yisheng Deng", "Weifeng Ge"], "title": "GTAD: Global Temporal Aggregation Denoising Learning for 3D Semantic Occupancy Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Accurately perceiving dynamic environments is a fundamental task for autonomous driving and robotic systems. Existing methods inadequately utilize temporal information, relying mainly on local temporal interactions between adjacent frames and failing to leverage global sequence information effectively. To address this limitation, we investigate how to effectively aggregate global temporal features from temporal sequences, aiming to achieve occupancy representations that efficiently utilize global temporal information from historical observations. For this purpose, we propose a global temporal aggregation denoising network named GTAD, introducing a global temporal information aggregation framework as a new paradigm for holistic 3D scene understanding. Our method employs an in-model latent denoising network to aggregate local temporal features from the current moment and global temporal features from historical sequences. This approach enables the effective perception of both fine-grained temporal information from adjacent frames and global temporal patterns from historical observations. As a result, it provides a more coherent and comprehensive understanding of the environment. Extensive experiments on the nuScenes and Occ3D-nuScenes benchmark and ablation studies demonstrate the superiority of our method.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGTAD\u7684\u5168\u5c40\u65f6\u95f4\u805a\u5408\u53bb\u566a\u7f51\u7edc\uff0c\u7528\u4e8e\u9ad8\u6548\u5229\u7528\u5386\u53f2\u89c2\u6d4b\u4e2d\u7684\u5168\u5c40\u65f6\u95f4\u4fe1\u606f\uff0c\u63d0\u5347\u52a8\u6001\u73af\u5883\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u5168\u5c40\u65f6\u95f4\u5e8f\u5217\u4fe1\u606f\uff0c\u4e3b\u8981\u4f9d\u8d56\u76f8\u90bb\u5e27\u7684\u5c40\u90e8\u65f6\u95f4\u4ea4\u4e92\uff0c\u9650\u5236\u4e86\u52a8\u6001\u73af\u5883\u611f\u77e5\u7684\u51c6\u786e\u6027\u548c\u5168\u9762\u6027\u3002", "method": "\u63d0\u51faGTAD\u7f51\u7edc\uff0c\u7ed3\u5408\u5c40\u90e8\u65f6\u95f4\u7279\u5f81\uff08\u5f53\u524d\u5e27\uff09\u548c\u5168\u5c40\u65f6\u95f4\u7279\u5f81\uff08\u5386\u53f2\u5e8f\u5217\uff09\uff0c\u901a\u8fc7\u53bb\u566a\u7f51\u7edc\u5b9e\u73b0\u5168\u5c40\u65f6\u95f4\u4fe1\u606f\u7684\u9ad8\u6548\u805a\u5408\u3002", "result": "\u5728nuScenes\u548cOcc3D-nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u8fde\u8d2f\u548c\u5168\u9762\u7684\u73af\u5883\u7406\u89e3\u3002", "conclusion": "GTAD\u901a\u8fc7\u5168\u5c40\u65f6\u95f4\u4fe1\u606f\u805a\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u73af\u5883\u611f\u77e5\u7684\u51c6\u786e\u6027\u548c\u5168\u9762\u6027\uff0c\u4e3a3D\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.20987", "pdf": "https://arxiv.org/pdf/2507.20987", "abs": "https://arxiv.org/abs/2507.20987", "authors": ["Xinhan Di", "Kristin Qi", "Pengqian Yu"], "title": "JWB-DH-V1: Benchmark for Joint Whole-Body Talking Avatar and Speech Generation Version 1", "categories": ["cs.CV", "cs.AI"], "comment": "WiCV @ ICCV 2025", "summary": "Recent advances in diffusion-based video generation have enabled photo-realistic short clips, but current methods still struggle to achieve multi-modal consistency when jointly generating whole-body motion and natural speech. Current approaches lack comprehensive evaluation frameworks that assess both visual and audio quality, and there are insufficient benchmarks for region-specific performance analysis. To address these gaps, we introduce the Joint Whole-Body Talking Avatar and Speech Generation Version I(JWB-DH-V1), comprising a large-scale multi-modal dataset with 10,000 unique identities across 2 million video samples, and an evaluation protocol for assessing joint audio-video generation of whole-body animatable avatars. Our evaluation of SOTA models reveals consistent performance disparities between face/hand-centric and whole-body performance, which incidates essential areas for future research. The dataset and evaluation tools are publicly available at https://github.com/deepreasonings/WholeBodyBenchmark.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86JWB-DH-V1\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u7528\u4e8e\u89e3\u51b3\u5f53\u524d\u6269\u6563\u89c6\u9891\u751f\u6210\u4e2d\u5168\u8eab\u52a8\u4f5c\u4e0e\u8bed\u97f3\u8054\u5408\u751f\u6210\u7684\u591a\u6a21\u6001\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728\u8054\u5408\u751f\u6210\u5168\u8eab\u52a8\u4f5c\u548c\u81ea\u7136\u8bed\u97f3\u65f6\u7f3a\u4e4f\u591a\u6a21\u6001\u4e00\u81f4\u6027\uff0c\u4e14\u7f3a\u4e4f\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u533a\u57df\u6027\u80fd\u5206\u6790\u57fa\u51c6\u3002", "method": "\u63d0\u51fa\u4e86JWB-DH-V1\uff0c\u5305\u542b\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u7528\u4e8e\u8bc4\u4f30\u5168\u8eab\u53ef\u52a8\u753b\u5316\u865a\u62df\u5f62\u8c61\u7684\u8054\u5408\u97f3\u89c6\u9891\u751f\u6210\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u73b0\u6709\u6a21\u578b\u5728\u9762\u90e8/\u624b\u90e8\u4e0e\u5168\u8eab\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002", "conclusion": "JWB-DH-V1\u4e3a\u5168\u8eab\u52a8\u4f5c\u4e0e\u8bed\u97f3\u8054\u5408\u751f\u6210\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u5de5\u5177\uff0c\u586b\u8865\u4e86\u7814\u7a76\u7a7a\u767d\u3002"}}
{"id": "2507.19551", "pdf": "https://arxiv.org/pdf/2507.19551", "abs": "https://arxiv.org/abs/2507.19551", "authors": ["Ran Tong", "Songtao Wei", "Jiaqi Liu", "Lanruo Wang"], "title": "Rainbow Noise: Stress-Testing Multimodal Harmful-Meme Detectors on LGBTQ Content", "categories": ["cs.CY", "cs.AI", "cs.CV"], "comment": "9 pages, 1 figure", "summary": "Hateful memes aimed at LGBTQ\\,+ communities often evade detection by tweaking either the caption, the image, or both. We build the first robustness benchmark for this setting, pairing four realistic caption attacks with three canonical image corruptions and testing all combinations on the PrideMM dataset. Two state-of-the-art detectors, MemeCLIP and MemeBLIP2, serve as case studies, and we introduce a lightweight \\textbf{Text Denoising Adapter (TDA)} to enhance the latter's resilience. Across the grid, MemeCLIP degrades more gently, while MemeBLIP2 is particularly sensitive to the caption edits that disrupt its language processing. However, the addition of the TDA not only remedies this weakness but makes MemeBLIP2 the most robust model overall. Ablations reveal that all systems lean heavily on text, but architectural choices and pre-training data significantly impact robustness. Our benchmark exposes where current multimodal safety models crack and demonstrates that targeted, lightweight modules like the TDA offer a powerful path towards stronger defences.", "AI": {"tldr": "\u8bba\u6587\u6784\u5efa\u4e86\u9996\u4e2a\u9488\u5bf9LGBTQ+\u793e\u533a\u4ec7\u6068\u6a21\u56e0\u7684\u9c81\u68d2\u6027\u57fa\u51c6\uff0c\u6d4b\u8bd5\u4e86\u4e24\u79cd\u5148\u8fdb\u68c0\u6d4b\u5668\uff08MemeCLIP\u548cMemeBLIP2\uff09\u5728\u591a\u79cd\u653b\u51fb\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u8f7b\u91cf\u7ea7\u6587\u672c\u53bb\u566a\u9002\u914d\u5668\uff08TDA\uff09\u4ee5\u63d0\u5347\u9c81\u68d2\u6027\u3002", "motivation": "\u4ec7\u6068\u6a21\u56e0\u901a\u8fc7\u4fee\u6539\u6587\u672c\u6216\u56fe\u50cf\u9003\u907f\u68c0\u6d4b\uff0c\u73b0\u6709\u6a21\u578b\u5728\u6b64\u7c7b\u653b\u51fb\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4e9f\u9700\u63d0\u5347\u9c81\u68d2\u6027\u3002", "method": "\u7ed3\u5408\u56db\u79cd\u6587\u672c\u653b\u51fb\u548c\u4e09\u79cd\u56fe\u50cf\u6270\u52a8\uff0c\u6d4b\u8bd5\u4e24\u79cd\u68c0\u6d4b\u5668\uff08MemeCLIP\u548cMemeBLIP2\uff09\u7684\u6027\u80fd\uff0c\u5e76\u5f15\u5165TDA\u589e\u5f3aMemeBLIP2\u3002", "result": "MemeCLIP\u8868\u73b0\u66f4\u7a33\u5b9a\uff0cMemeBLIP2\u5bf9\u6587\u672c\u653b\u51fb\u654f\u611f\uff0c\u4f46TDA\u4f7f\u5176\u6210\u4e3a\u6700\u9c81\u68d2\u7684\u6a21\u578b\u3002", "conclusion": "\u5f53\u524d\u591a\u6a21\u6001\u5b89\u5168\u6a21\u578b\u5b58\u5728\u6f0f\u6d1e\uff0c\u8f7b\u91cf\u7ea7\u6a21\u5757\uff08\u5982TDA\uff09\u662f\u63d0\u5347\u9632\u5fa1\u80fd\u529b\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2507.19565", "pdf": "https://arxiv.org/pdf/2507.19565", "abs": "https://arxiv.org/abs/2507.19565", "authors": ["Brady K. Zhou", "Jason J. Hu", "Jane K. J. Lee", "Z. Hong Zhou", "Demetri Terzopoulos"], "title": "Review of Deep Learning Applications to Structural Proteomics Enabled by Cryogenic Electron Microscopy and Tomography", "categories": ["q-bio.QM", "cs.CV", "cs.LG"], "comment": "16 pages", "summary": "The past decade's \"cryoEM revolution\" has produced exponential growth in high-resolution structural data through advances in cryogenic electron microscopy (cryoEM) and tomography (cryoET). Deep learning integration into structural proteomics workflows addresses longstanding challenges including low signal-to-noise ratios, preferred orientation artifacts, and missing-wedge problems that historically limited efficiency and scalability. This review examines AI applications across the entire cryoEM pipeline, from automated particle picking using convolutional neural networks (Topaz, crYOLO, CryoSegNet) to computational solutions for preferred orientation bias (spIsoNet, cryoPROS) and advanced denoising algorithms (Topaz-Denoise). In cryoET, tools like IsoNet employ U-Net architectures for simultaneous missing-wedge correction and noise reduction, while TomoNet streamlines subtomogram averaging through AI-driven particle detection. The workflow culminates with automated atomic model building using sophisticated tools like ModelAngelo, DeepTracer, and CryoREAD that translate density maps into interpretable biological structures. These AI-enhanced approaches have achieved near-atomic resolution reconstructions with minimal manual intervention, resolved previously intractable datasets suffering from severe orientation bias, and enabled successful application to diverse biological systems from HIV virus-like particles to in situ ribosomal complexes. As deep learning evolves, particularly with large language models and vision transformers, the future promises sophisticated automation and accessibility in structural biology, potentially revolutionizing our understanding of macromolecular architecture and function.", "AI": {"tldr": "\u7efc\u8ff0\u63a2\u8ba8\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u51b7\u51bb\u7535\u955c\uff08cryoEM\uff09\u548c\u51b7\u51bb\u7535\u5b50\u65ad\u5c42\u626b\u63cf\uff08cryoET\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u89e3\u51b3\u4e86\u4fe1\u53f7\u566a\u58f0\u3001\u53d6\u5411\u504f\u5dee\u7b49\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u539f\u5b50\u6a21\u578b\u6784\u5efa\u3002", "motivation": "\u89e3\u51b3\u51b7\u51bb\u7535\u955c\u6280\u672f\u4e2d\u7684\u4f4e\u4fe1\u566a\u6bd4\u3001\u53d6\u5411\u504f\u5dee\u548c\u7f3a\u5931\u6954\u5f62\u95ee\u9898\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u5229\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08\u5982Topaz\u3001crYOLO\uff09\u8fdb\u884c\u81ea\u52a8\u9897\u7c92\u6311\u9009\uff0c\u91c7\u7528U-Net\u67b6\u6784\uff08\u5982IsoNet\uff09\u8fdb\u884c\u7f3a\u5931\u6954\u5f62\u6821\u6b63\u548c\u964d\u566a\uff0c\u4ee5\u53ca\u81ea\u52a8\u5316\u539f\u5b50\u6a21\u578b\u6784\u5efa\u5de5\u5177\uff08\u5982ModelAngelo\uff09\u3002", "result": "\u5b9e\u73b0\u4e86\u8fd1\u539f\u5b50\u5206\u8fa8\u7387\u7684\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u4e25\u91cd\u53d6\u5411\u504f\u5dee\u7684\u6570\u636e\u96c6\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u591a\u79cd\u751f\u7269\u7cfb\u7edf\u3002", "conclusion": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u7684\u8fdb\u6b65\uff0c\u672a\u6765\u5c06\u5b9e\u73b0\u66f4\u9ad8\u7ea7\u7684\u81ea\u52a8\u5316\u548c\u53ef\u8bbf\u95ee\u6027\uff0c\u63a8\u52a8\u5bf9\u751f\u7269\u5927\u5206\u5b50\u7ed3\u6784\u548c\u529f\u80fd\u7684\u7406\u89e3\u3002"}}
{"id": "2507.19887", "pdf": "https://arxiv.org/pdf/2507.19887", "abs": "https://arxiv.org/abs/2507.19887", "authors": ["Shishir Muralidhara", "Didier Stricker", "Ren\u00e9 Schuster"], "title": "CLoRA: Parameter-Efficient Continual Learning with Low-Rank Adaptation", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted at CoLLAs 2025", "summary": "In the past, continual learning (CL) was mostly concerned with the problem of catastrophic forgetting in neural networks, that arises when incrementally learning a sequence of tasks. Current CL methods function within the confines of limited data access, without any restrictions imposed on computational resources. However, in real-world scenarios, the latter takes precedence as deployed systems are often computationally constrained. A major drawback of most CL methods is the need to retrain the entire model for each new task. The computational demands of retraining large models can be prohibitive, limiting the applicability of CL in environments with limited resources. Through CLoRA, we explore the applicability of Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method for class-incremental semantic segmentation. CLoRA leverages a small set of parameters of the model and uses the same set for learning across all tasks. Results demonstrate the efficacy of CLoRA, achieving performance on par with and exceeding the baseline methods. We further evaluate CLoRA using NetScore, underscoring the need to factor in resource efficiency and evaluate CL methods beyond task performance. CLoRA significantly reduces the hardware requirements for training, making it well-suited for CL in resource-constrained environments after deployment.", "AI": {"tldr": "CLoRA\u5229\u7528\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u6240\u9700\u7684\u786c\u4ef6\u8d44\u6e90\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "method": "CLoRA\u91c7\u7528\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u5c11\u91cf\u53c2\u6570\u8fdb\u884c\u4efb\u52a1\u5b66\u4e60\uff0c\u907f\u514d\u4e86\u5168\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "CLoRA\u5728\u6027\u80fd\u4e0a\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u786c\u4ef6\u9700\u6c42\u3002", "conclusion": "CLoRA\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19905", "pdf": "https://arxiv.org/pdf/2507.19905", "abs": "https://arxiv.org/abs/2507.19905", "authors": ["Bilal Hussain Abbasi", "Zirui Gong", "Yanjun Zhang", "Shang Gao", "Antonio Robles-Kelly", "Leo Zhang"], "title": "ConSeg: Contextual Backdoor Attack Against Semantic Segmentation", "categories": ["cs.CR", "cs.CV"], "comment": null, "summary": "Despite significant advancements in computer vision, semantic segmentation models may be susceptible to backdoor attacks. These attacks, involving hidden triggers, aim to cause the models to misclassify instances of the victim class as the target class when triggers are present, posing serious threats to the reliability of these models. To further explore the field of backdoor attacks against semantic segmentation, in this paper, we propose a simple yet effective backdoor attack called Contextual Segmentation Backdoor Attack (ConSeg). ConSeg leverages the contextual information inherent in semantic segmentation models to enhance backdoor performance. Our method is motivated by an intriguing observation, i.e., when the target class is set as the `co-occurring' class of the victim class, the victim class can be more easily `mis-segmented'. Building upon this insight, ConSeg mimics the contextual information of the target class and rebuilds it in the victim region to establish the contextual relationship between the target class and the victim class, making the attack easier. Our experiments reveal that ConSeg achieves improvements in Attack Success Rate (ASR) with increases of 15.55\\%, compared to existing methods, while exhibiting resilience against state-of-the-art backdoor defenses.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aConSeg\u7684\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u5229\u7528\u8bed\u4e49\u5206\u5272\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u63d0\u5347\u653b\u51fb\u6548\u679c\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u653b\u51fb\u6210\u529f\u7387\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad815.55%\uff0c\u4e14\u80fd\u62b5\u6297\u5148\u8fdb\u9632\u5fa1\u3002", "motivation": "\u8bed\u4e49\u5206\u5272\u6a21\u578b\u6613\u53d7\u540e\u95e8\u653b\u51fb\u5a01\u80c1\uff0c\u73b0\u6709\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff0c\u9700\u63a2\u7d22\u66f4\u6709\u6548\u7684\u653b\u51fb\u65b9\u5f0f\u3002", "method": "\u63d0\u51faConSeg\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u76ee\u6807\u7c7b\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u5e76\u5728\u53d7\u5bb3\u533a\u57df\u91cd\u5efa\uff0c\u589e\u5f3a\u540e\u95e8\u653b\u51fb\u6548\u679c\u3002", "result": "ConSeg\u653b\u51fb\u6210\u529f\u7387\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad815.55%\uff0c\u4e14\u5bf9\u5148\u8fdb\u9632\u5fa1\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "ConSeg\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u5229\u7528\u4e0a\u4e0b\u6587\u4fe1\u606f\u663e\u8457\u63d0\u5347\u653b\u51fb\u6027\u80fd\u3002"}}
{"id": "2507.19970", "pdf": "https://arxiv.org/pdf/2507.19970", "abs": "https://arxiv.org/abs/2507.19970", "authors": ["Zhaobin Xu"], "title": "SkinDualGen: Prompt-Driven Diffusion for Simultaneous Image-Mask Generation in Skin Lesions", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Medical image analysis plays a pivotal role in the early diagnosis of diseases such as skin lesions. However, the scarcity of data and the class imbalance significantly hinder the performance of deep learning models. We propose a novel method that leverages the pretrained Stable Diffusion-2.0 model to generate high-quality synthetic skin lesion images and corresponding segmentation masks. This approach augments training datasets for classification and segmentation tasks. We adapt Stable Diffusion-2.0 through domain-specific Low-Rank Adaptation (LoRA) fine-tuning and joint optimization of multi-objective loss functions, enabling the model to simultaneously generate clinically relevant images and segmentation masks conditioned on textual descriptions in a single step. Experimental results show that the generated images, validated by FID scores, closely resemble real images in quality. A hybrid dataset combining real and synthetic data markedly enhances the performance of classification and segmentation models, achieving substantial improvements in accuracy and F1-score of 8% to 15%, with additional positive gains in other key metrics such as the Dice coefficient and IoU. Our approach offers a scalable solution to address the challenges of medical imaging data, contributing to improved accuracy and reliability in diagnosing rare diseases.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eStable Diffusion-2.0\u7684\u65b9\u6cd5\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u76ae\u80a4\u75c5\u53d8\u56fe\u50cf\u548c\u5206\u5272\u63a9\u7801\uff0c\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5206\u7c7b\u548c\u5206\u5272\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u6790\u5728\u75be\u75c5\u65e9\u671f\u8bca\u65ad\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6570\u636e\u7a00\u7f3a\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u9650\u5236\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u7684LoRA\u5fae\u8c03\u548c\u591a\u76ee\u6807\u635f\u5931\u51fd\u6570\u8054\u5408\u4f18\u5316\uff0c\u4f7fStable Diffusion-2.0\u80fd\u4e00\u6b65\u751f\u6210\u4e34\u5e8a\u76f8\u5173\u56fe\u50cf\u548c\u5206\u5272\u63a9\u7801\u3002", "result": "\u751f\u6210\u7684\u56fe\u50cf\u8d28\u91cf\u63a5\u8fd1\u771f\u5b9e\u56fe\u50cf\uff08FID\u9a8c\u8bc1\uff09\uff0c\u6df7\u5408\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff08\u51c6\u786e\u7387\u548cF1-score\u63d0\u9ad88%-15%\uff0c\u5176\u4ed6\u5173\u952e\u6307\u6807\u5982Dice\u7cfb\u6570\u548cIoU\u4e5f\u6709\u63d0\u5347\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u533b\u5b66\u5f71\u50cf\u6570\u636e\u6311\u6218\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u7f55\u89c1\u75be\u75c5\u8bca\u65ad\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.20447", "pdf": "https://arxiv.org/pdf/2507.20447", "abs": "https://arxiv.org/abs/2507.20447", "authors": ["Takanobu Furuhashi", "Hidekata Hontani", "Tatsuya Yokota"], "title": "WEEP: A Differentiable Nonconvex Sparse Regularizer via Weakly-Convex Envelope", "categories": ["cs.LG", "cs.CV"], "comment": "8 pages, 4 figures", "summary": "Sparse regularization is fundamental in signal processing for efficient signal recovery and feature extraction. However, it faces a fundamental dilemma: the most powerful sparsity-inducing penalties are often non-differentiable, conflicting with gradient-based optimizers that dominate the field. We introduce WEEP (Weakly-convex Envelope of Piecewise Penalty), a novel, fully differentiable sparse regularizer derived from the weakly-convex envelope framework. WEEP provides strong, unbiased sparsity while maintaining full differentiability and L-smoothness, making it natively compatible with any gradient-based optimizer. This resolves the conflict between statistical performance and computational tractability. We demonstrate superior performance compared to the L1-norm and other established non-convex sparse regularizers on challenging signal and image denoising tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWEEP\u7684\u65b0\u578b\u7a00\u758f\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7a00\u758f\u6b63\u5219\u5316\u65b9\u6cd5\u5728\u975e\u53ef\u5fae\u6027\u4e0e\u68af\u5ea6\u4f18\u5316\u5668\u517c\u5bb9\u6027\u4e4b\u95f4\u7684\u51b2\u7a81\u3002", "motivation": "\u7a00\u758f\u6b63\u5219\u5316\u5728\u4fe1\u53f7\u5904\u7406\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6700\u5f3a\u7684\u7a00\u758f\u8bf1\u5bfc\u60e9\u7f5a\u901a\u5e38\u4e0d\u53ef\u5fae\uff0c\u4e0e\u4e3b\u6d41\u7684\u68af\u5ea6\u4f18\u5316\u5668\u4e0d\u517c\u5bb9\u3002", "method": "\u901a\u8fc7\u5f31\u51f8\u5305\u7edc\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u5b8c\u5168\u53ef\u5fae\u7684\u7a00\u758f\u6b63\u5219\u5316\u65b9\u6cd5WEEP\uff0c\u517c\u5177\u5f3a\u7a00\u758f\u6027\u548cL-\u5e73\u6ed1\u6027\u3002", "result": "\u5728\u4fe1\u53f7\u548c\u56fe\u50cf\u53bb\u566a\u4efb\u52a1\u4e2d\uff0cWEEP\u8868\u73b0\u4f18\u4e8eL1\u8303\u6570\u548c\u5176\u4ed6\u975e\u51f8\u7a00\u758f\u6b63\u5219\u5316\u65b9\u6cd5\u3002", "conclusion": "WEEP\u89e3\u51b3\u4e86\u7edf\u8ba1\u6027\u80fd\u4e0e\u8ba1\u7b97\u53ef\u884c\u6027\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u4e3a\u7a00\u758f\u6b63\u5219\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.20765", "pdf": "https://arxiv.org/pdf/2507.20765", "abs": "https://arxiv.org/abs/2507.20765", "authors": ["Davide Piccinini", "Diego Valsesia", "Enrico Magli"], "title": "Onboard Hyperspectral Super-Resolution with Deep Pushbroom Neural Network", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Hyperspectral imagers on satellites obtain the fine spectral signatures essential for distinguishing one material from another at the expense of limited spatial resolution. Enhancing the latter is thus a desirable preprocessing step in order to further improve the detection capabilities offered by hyperspectral images on downstream tasks. At the same time, there is a growing interest towards deploying inference methods directly onboard of satellites, which calls for lightweight image super-resolution methods that can be run on the payload in real time. In this paper, we present a novel neural network design, called Deep Pushbroom Super-Resolution (DPSR) that matches the pushbroom acquisition of hyperspectral sensors by processing an image line by line in the along-track direction with a causal memory mechanism to exploit previously acquired lines. This design greatly limits memory requirements and computational complexity, achieving onboard real-time performance, i.e., the ability to super-resolve a line in the time it takes to acquire the next one, on low-power hardware. Experiments show that the quality of the super-resolved images is competitive or even outperforms state-of-the-art methods that are significantly more complex.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1DPSR\uff0c\u7528\u4e8e\u5b9e\u65f6\u63d0\u5347\u536b\u661f\u4e0a\u9ad8\u5149\u8c31\u56fe\u50cf\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\uff0c\u6ee1\u8db3\u661f\u8f7d\u5b9e\u65f6\u5904\u7406\u9700\u6c42\u3002", "motivation": "\u9ad8\u5149\u8c31\u56fe\u50cf\u5177\u6709\u7cbe\u7ec6\u7684\u5149\u8c31\u7279\u5f81\uff0c\u4f46\u7a7a\u95f4\u5206\u8fa8\u7387\u6709\u9650\u3002\u63d0\u5347\u5206\u8fa8\u7387\u6709\u52a9\u4e8e\u4e0b\u6e38\u4efb\u52a1\uff0c\u540c\u65f6\u661f\u8f7d\u5b9e\u65f6\u5904\u7406\u9700\u6c42\u63a8\u52a8\u4e86\u8f7b\u91cf\u7ea7\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u7684\u53d1\u5c55\u3002", "method": "\u8bbe\u8ba1\u4e86DPSR\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u9010\u884c\u5904\u7406\u56fe\u50cf\u5e76\u5229\u7528\u56e0\u679c\u8bb0\u5fc6\u673a\u5236\uff0c\u964d\u4f4e\u5185\u5b58\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5b9e\u73b0\u661f\u8f7d\u5b9e\u65f6\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDPSR\u5728\u8d85\u5206\u8fa8\u7387\u56fe\u50cf\u8d28\u91cf\u4e0a\u4f18\u4e8e\u6216\u5ab2\u7f8e\u66f4\u590d\u6742\u7684\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DPSR\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u8f7b\u91cf\u7ea7\u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u661f\u8f7d\u5b9e\u65f6\u5904\u7406\u9700\u6c42\u3002"}}
{"id": "2507.20973", "pdf": "https://arxiv.org/pdf/2507.20973", "abs": "https://arxiv.org/abs/2507.20973", "authors": ["Chao Wu", "Zhenyi Wang", "Kangxian Xie", "Naresh Kumar Devulapally", "Vishnu Suresh Lokhande", "Mingchen Gao"], "title": "Model-Agnostic Gender Bias Control for Text-to-Image Generation via Sparse Autoencoder", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Text-to-image (T2I) diffusion models often exhibit gender bias, particularly by generating stereotypical associations between professions and gendered subjects. This paper presents SAE Debias, a lightweight and model-agnostic framework for mitigating such bias in T2I generation. Unlike prior approaches that rely on CLIP-based filtering or prompt engineering, which often require model-specific adjustments and offer limited control, SAE Debias operates directly within the feature space without retraining or architectural modifications. By leveraging a k-sparse autoencoder pre-trained on a gender bias dataset, the method identifies gender-relevant directions within the sparse latent space, capturing professional stereotypes. Specifically, a biased direction per profession is constructed from sparse latents and suppressed during inference to steer generations toward more gender-balanced outputs. Trained only once, the sparse autoencoder provides a reusable debiasing direction, offering effective control and interpretable insight into biased subspaces. Extensive evaluations across multiple T2I models, including Stable Diffusion 1.4, 1.5, 2.1, and SDXL, demonstrate that SAE Debias substantially reduces gender bias while preserving generation quality. To the best of our knowledge, this is the first work to apply sparse autoencoders for identifying and intervening in gender bias within T2I models. These findings contribute toward building socially responsible generative AI, providing an interpretable and model-agnostic tool to support fairness in text-to-image generation.", "AI": {"tldr": "SAE Debias\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u51cf\u5c11\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6027\u522b\u504f\u89c1\uff0c\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u8bc6\u522b\u548c\u6291\u5236\u6027\u522b\u76f8\u5173\u65b9\u5411\u3002", "motivation": "T2I\u6269\u6563\u6a21\u578b\u5e38\u8868\u73b0\u51fa\u6027\u522b\u504f\u89c1\uff0c\u4f8b\u5982\u5c06\u804c\u4e1a\u4e0e\u7279\u5b9a\u6027\u522b\u523b\u677f\u5173\u8054\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56CLIP\u8fc7\u6ee4\u6216\u63d0\u793a\u5de5\u7a0b\uff0c\u6548\u679c\u6709\u9650\u4e14\u9700\u6a21\u578b\u7279\u5b9a\u8c03\u6574\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684k\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5728\u7a00\u758f\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8bc6\u522b\u6027\u522b\u76f8\u5173\u65b9\u5411\uff0c\u6784\u5efa\u804c\u4e1a\u76f8\u5173\u7684\u504f\u89c1\u65b9\u5411\u5e76\u5728\u63a8\u7406\u65f6\u6291\u5236\uff0c\u4ee5\u751f\u6210\u66f4\u6027\u522b\u5e73\u8861\u7684\u56fe\u50cf\u3002", "result": "\u5728\u591a\u4e2aT2I\u6a21\u578b\uff08\u5982Stable Diffusion\u7cfb\u5217\uff09\u4e0a\u8bc4\u4f30\uff0cSAE Debias\u663e\u8457\u51cf\u5c11\u6027\u522b\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "SAE Debias\u662f\u9996\u4e2a\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u8bc6\u522b\u548c\u5e72\u9884T2I\u6a21\u578b\u4e2d\u6027\u522b\u504f\u89c1\u7684\u5de5\u4f5c\uff0c\u4e3a\u6784\u5efa\u793e\u4f1a\u8d23\u4efb\u7684\u751f\u6210AI\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u4e14\u6a21\u578b\u65e0\u5173\u7684\u5de5\u5177\u3002"}}
