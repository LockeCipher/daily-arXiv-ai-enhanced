<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 3]
- [cs.CV](#cs.CV) [Total: 35]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [T2Bs: Text-to-Character Blendshapes via Video Generation](https://arxiv.org/abs/2509.10678)
*Jiahao Luo,Chaoyang Wang,Michael Vasilkovsky,Vladislav Shakhrai,Di Liu,Peiye Zhuang,Sergey Tulyakov,Peter Wonka,Hsin-Ying Lee,James Davis,Jian Wang*

Main category: cs.GR

TL;DR: T2Bs是一个从文本生成高质量可动画角色头部形变模型的框架，结合静态文本到3D生成和视频扩散技术，通过可变形3D高斯泼溅对齐静态3D资产与视频输出。


<details>
  <summary>Details</summary>
Motivation: 解决文本到3D模型缺乏运动合成能力，以及视频扩散模型存在时间和多视角几何不一致的问题，实现高质量可动画角色头部的生成。

Method: 利用可变形3D高斯泼溅技术，通过静态几何约束运动，并使用视角依赖变形MLP来对齐静态3D资产与视频输出。

Result: 在准确性和表现力方面优于现有4D生成方法，减少视频伪影和视角不一致，重建平滑、连贯、完全配准的3D几何体。

Conclusion: 能够合成超越当前4D生成技术的富有表现力的可动画角色头部，为构建具有多样化逼真面部运动的形变模型提供了可扩展的解决方案。

Abstract: We present T2Bs, a framework for generating high-quality, animatable character head morphable models from text by combining static text-to-3D generation with video diffusion. Text-to-3D models produce detailed static geometry but lack motion synthesis, while video diffusion models generate motion with temporal and multi-view geometric inconsistencies. T2Bs bridges this gap by leveraging deformable 3D Gaussian splatting to align static 3D assets with video outputs. By constraining motion with static geometry and employing a view-dependent deformation MLP, T2Bs (i) outperforms existing 4D generation methods in accuracy and expressiveness while reducing video artifacts and view inconsistencies, and (ii) reconstructs smooth, coherent, fully registered 3D geometries designed to scale for building morphable models with diverse, realistic facial motions. This enables synthesizing expressive, animatable character heads that surpass current 4D generation techniques.

</details>


### [2] [AD-GS: Alternating Densification for Sparse-Input 3D Gaussian Splatting](https://arxiv.org/abs/2509.11003)
*Gurutva Patle,Nilay Girgaonkar,Nagabhushan Somraj,Rajiv Soundararajan*

Main category: cs.GR

TL;DR: AD-GS是一种交替密度化框架，通过高低密度化阶段交替来控制3D高斯分布的密度增长，解决稀疏视角下的伪影和过拟合问题


<details>
  <summary>Details</summary>
Motivation: 3D高斯分布技术在稀疏视角设置下容易产生浮动物体、几何不准确和过拟合等伪影，主要原因是无控制的密度化过程

Method: 提出交替密度化框架：高密度化阶段激进添加高斯基元并基于光度损失训练；低密度化阶段激进修剪不透明度并通过伪视角一致性和边缘感知深度平滑正则化几何

Result: 在挑战性数据集上的实验表明，AD-GS相比现有方法显著提升了渲染质量和几何一致性

Conclusion: 交替密度化方法通过精心控制模型容量增长，有效减少过拟合并逐步优化场景表示

Abstract: 3D Gaussian Splatting (3DGS) has shown impressive results in real-time novel view synthesis. However, it often struggles under sparse-view settings, producing undesirable artifacts such as floaters, inaccurate geometry, and overfitting due to limited observations. We find that a key contributing factor is uncontrolled densification, where adding Gaussian primitives rapidly without guidance can harm geometry and cause artifacts. We propose AD-GS, a novel alternating densification framework that interleaves high and low densification phases. During high densification, the model densifies aggressively, followed by photometric loss based training to capture fine-grained scene details. Low densification then primarily involves aggressive opacity pruning of Gaussians followed by regularizing their geometry through pseudo-view consistency and edge-aware depth smoothness. This alternating approach helps reduce overfitting by carefully controlling model capacity growth while progressively refining the scene representation. Extensive experiments on challenging datasets demonstrate that AD-GS significantly improves rendering quality and geometric consistency compared to existing methods.

</details>


### [3] [3D Gaussian Modeling and Ray Marching of OpenVDB datasets for Scientific Visualization](https://arxiv.org/abs/2509.11377)
*Isha Sharma,Dieter Schmalstieg*

Main category: cs.GR

TL;DR: 这篇论文探索了使用OpenVDB稀疏体积数据格式作为3D高斯粒子的前置模型框架，并提出了基于光线积分的渲染算法，以实现更高效的科学可视化数据压缩和统一模型表示。


<details>
  <summary>Details</summary>
Motivation: 科学可视化领域中常见的密集格点数据格式内存效率低，而OpenVDB稀疏体积数据格式能够避免存储空单元格，为转换到3D高斯粒子提供了压缩优势。

Method: 使用OpenVDB作为前置模型框架，开发了基于光线积分的渲染算法，在OptiX8.1中实现了计3D高斯粒子沿光线贡献的计算。同时实现了基于NanoVDB HDDA的对照渲染器。

Result: 该方法能够有效地将稀疏体积数据转换为3D高斯粒子表示，实现数据压缩。渲染算法能够准确计算高斯粒子的光学深度积累效果。

Conclusion: 这种方法为科学可视化提供了一种统一的模型表示方式，能够处理不同类型的体积数据（包括正则格点、AMR体积和点云），具有良好的压缩效果和渲染性能。

Abstract: 3D Gaussians are currently being heavily investigated for their scene modeling and compression abilities. In 3D volumes, their use is being explored for representing dense volumes as sparsely as possible. However, most of these methods begin with a memory inefficient data format. Specially in Scientific Visualization(SciVis), where most popular formats are dense-grid data structures that store every grid cell, irrespective of its contribution. OpenVDB library and data format were introduced for representing sparse volumetric data specifically for visual effects use cases such as clouds, fire, fluids etc. It avoids storing empty cells by masking them during storage. It presents an opportunity for use in SciVis, specifically as a modeling framework for conversion to 3D Gaussian particles for further compression and for a unified modeling approach for different scientific volume types. This compression head-start is non-trivial and this paper would like to present this with a rendering algorithm based on line integration implemented in OptiX8.1 for calculating 3D Gaussians contribution along a ray for optical-depth accumulation. For comparing the rendering results of our ray marching Gaussians renderer, we also implement a SciVis style primary-ray only NanoVDB HDDA based ray marcher for OpenVDB voxel grids. Finally, this paper also explores application of this Gaussian model to formats of volumes other than regular grids, such as AMR volumes and point clouds, using internal representation of OpenVDB grid class types for data hierarchy and subdivision structure.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [4] [A Real-Time Diminished Reality Approach to Privacy in MR Collaboration](https://arxiv.org/abs/2509.10466)
*Christian Fane*

Main category: cs.CV

TL;DR: 这篇论文提出了一种基于图像恢复的实时虚减现实系统，通过语义分割和实时恢复技术在混合现实会议中实现隐私保护，可以在20fps的速度移除敏感物品。


<details>
  <summary>Details</summary>
Motivation: 为了解决混合现实会议中的隐私泄漏问题，允许主要用户选择性移除环境中的个人或敏感物品，确保其他参与者无法看到这些物体。

Method: 系统采用YOLOv11进行物体检测，通过语义分割和精确物体选择，使用改进的Decoupled Spatial-Temporal Transformer (DSTT)模型进行高质量视频恢复，基于mobile ZED 2i深度摄像头实现实时处理。

Result: 在720p分辨率下，系统能够维持超过20fps的桌面率，证明了实时虚减现实在实际隐私保护应用中的可行性。

Conclusion: 该系统提供了一种符合实际应用需求的可移动、高效的隐私保护方案，无需固定观察点或环境3D扫描，为MR会议隐私控制开启了新方向。

Abstract: Diminished reality (DR) refers to the digital removal of real-world objects by compositing background content in their place. This thesis presents a real-time, inpainting-based DR system designed to enable privacy control in shared-space mixed reality (MR) meetings. The system allows a primary headset user to selectively remove personal or sensitive items from their environment, ensuring that those objects are no longer visible to other participants. Removal is achieved through semantic segmentation and precise object selection, followed by real-time inpainting from the viewpoint of a secondary observer, implemented using a mobile ZED 2i depth camera. The solution is designed to be portable and robust, requiring neither a fixed secondary viewpoint nor prior 3D scanning of the environment. The system utilises YOLOv11 for object detection and a modified Decoupled Spatial-Temporal Transformer (DSTT) model for high-quality video inpainting. At 720p resolution, the pipeline sustains frame rates exceeding 20 fps, demonstrating the feasibility of real-time diminished reality for practical privacy-preserving MR applications.

</details>


### [5] [On the Skinning of Gaussian Avatars](https://arxiv.org/abs/2509.11411)
*Nikolaos Zioulis,Nikolaos Kotarelas,Georgios Albanis,Spyridon Thermos,Anargyros Chatzitofis*

Main category: cs.CV

TL;DR: 基于向量权重旋转混合的新方法，解决伪梵散点在人体动画中的旋转问题，无需网格或预测模型


<details>
  <summary>Details</summary>
Motivation: 伪梵散点技术虽然提高了训练和渲染速度，但线性混合皮肤方法无法正确处理高斯分布的非线性旋转特性，导致动画效果不理想

Method: 提出基于四元数平均的权重旋转混合方法，只需修改线性混合皮肤技术即可实现

Result: 实现了更简单的顶点基伪梵散点模型，能够高效动画并集成到任何渲染引擎中

Conclusion: 该方法有效解决了伪梵散点在人体动画中的旋转问题，为高效人体虚拟化身创建提供了更简单易用的解决方案

Abstract: Radiance field-based methods have recently been used to reconstruct human avatars, showing that we can significantly downscale the systems needed for creating animated human avatars. Although this progress has been initiated by neural radiance fields, their slow rendering and backward mapping from the observation space to the canonical space have been the main challenges. With Gaussian splatting overcoming both challenges, a new family of approaches has emerged that are faster to train and render, while also straightforward to implement using forward skinning from the canonical to the observation space. However, the linear blend skinning required for the deformation of the Gaussians does not provide valid results for their non-linear rotation properties. To address such artifacts, recent works use mesh properties to rotate the non-linear Gaussian properties or train models to predict corrective offsets. Instead, we propose a weighted rotation blending approach that leverages quaternion averaging. This leads to simpler vertex-based Gaussians that can be efficiently animated and integrated in any engine by only modifying the linear blend skinning technique, and using any Gaussian rasterizer.

</details>


### [6] [Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation](https://arxiv.org/abs/2509.10687)
*Hao Zhang,Chun-Han Yao,Simon Donné,Narendra Ahuja,Varun Jampani*

Main category: cs.CV

TL;DR: SP4D是一个从单目输入生成配对RGB和运动学部件视频的框架，通过双分支扩散模型联合合成RGB帧和部件分割图，采用空间颜色编码简化架构，并通过双向扩散融合模块增强跨分支一致性。


<details>
  <summary>Details</summary>
Motivation: 传统部件分割方法依赖基于外观的语义线索，而SP4D旨在学习生成与物体关节对齐、在视角和时间上保持一致的动态运动学部件，为下游动画和运动相关任务提供支持。

Method: 采用双分支扩散模型联合合成RGB和部件分割；引入空间颜色编码将部件掩码映射为连续RGB图像；使用双向扩散融合模块增强一致性；采用对比部件一致性损失促进时空对齐。

Result: SP4D能够泛化到多样化场景，包括真实世界视频、新生成物体和罕见关节姿态，生成的2D部件图可提升到3D以推导骨骼结构和皮肤权重。

Conclusion: SP4D框架能够生成运动学感知的输出，适用于动画和运动相关任务，并通过KinematicParts20K数据集进行训练和评估，表现出强大的泛化能力。

Abstract: We present Stable Part Diffusion 4D (SP4D), a framework for generating paired RGB and kinematic part videos from monocular inputs. Unlike conventional part segmentation methods that rely on appearance-based semantic cues, SP4D learns to produce kinematic parts - structural components aligned with object articulation and consistent across views and time. SP4D adopts a dual-branch diffusion model that jointly synthesizes RGB frames and corresponding part segmentation maps. To simplify the architecture and flexibly enable different part counts, we introduce a spatial color encoding scheme that maps part masks to continuous RGB-like images. This encoding allows the segmentation branch to share the latent VAE from the RGB branch, while enabling part segmentation to be recovered via straightforward post-processing. A Bidirectional Diffusion Fusion (BiDiFuse) module enhances cross-branch consistency, supported by a contrastive part consistency loss to promote spatial and temporal alignment of part predictions. We demonstrate that the generated 2D part maps can be lifted to 3D to derive skeletal structures and harmonic skinning weights with few manual adjustments. To train and evaluate SP4D, we construct KinematicParts20K, a curated dataset of over 20K rigged objects selected and processed from Objaverse XL (Deitke et al., 2023), each paired with multi-view RGB and part video sequences. Experiments show that SP4D generalizes strongly to diverse scenarios, including real-world videos, novel generated objects, and rare articulated poses, producing kinematic-aware outputs suitable for downstream animation and motion-related tasks.

</details>


### [7] [Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing for Physics-based Camera Effect Data Generation](https://arxiv.org/abs/2509.10759)
*Yi-Ruei Liu,You-Zhe Xie,Yu-Hsiang Hsu,I-Sheng Fang,Yu-Lun Liu,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 4D-GRT是一个两阶段流水线，结合4D高斯溅射和基于物理的光线追踪，用于模拟相机效果，解决现有方法成本高、仿真到真实差距大或无法准确建模相机效果的问题。


<details>
  <summary>Details</summary>
Motivation: 传统计算机视觉系统假设理想针孔相机，但在面对鱼眼畸变、滚动快门等真实相机效果时表现不佳，主要原因是缺乏包含相机效果的训练数据。现有数据生成方法要么成本高，要么存在仿真到真实差距，或无法准确建模相机效果。

Method: 提出4D高斯光线追踪(4D-GRT)两阶段流水线：第一阶段使用4D高斯溅射重建动态场景，第二阶段应用基于物理的光线追踪生成具有可控、物理准确的相机效果的视频。

Result: 4D-GRT实现了最快的渲染速度，同时在渲染质量上优于或与现有基线方法相当。构建了包含8个合成动态场景的基准测试集，涵盖4种相机效果。

Conclusion: 4D-GRT能够有效生成具有物理准确相机效果的视频数据，解决了现有方法的瓶颈问题，为计算机视觉系统在真实相机效果下的性能提升提供了有效的数据生成方案。

Abstract: Common computer vision systems typically assume ideal pinhole cameras but fail when facing real-world camera effects such as fisheye distortion and rolling shutter, mainly due to the lack of learning from training data with camera effects. Existing data generation approaches suffer from either high costs, sim-to-real gaps or fail to accurately model camera effects. To address this bottleneck, we propose 4D Gaussian Ray Tracing (4D-GRT), a novel two-stage pipeline that combines 4D Gaussian Splatting with physically-based ray tracing for camera effect simulation. Given multi-view videos, 4D-GRT first reconstructs dynamic scenes, then applies ray tracing to generate videos with controllable, physically accurate camera effects. 4D-GRT achieves the fastest rendering speed while performing better or comparable rendering quality compared to existing baselines. Additionally, we construct eight synthetic dynamic scenes in indoor environments across four camera effects as a benchmark to evaluate generated videos with camera effects.

</details>


### [8] [TrueSkin: Towards Fair and Accurate Skin Tone Recognition and Generation](https://arxiv.org/abs/2509.10980)
*Haoming Lu*

Main category: cs.CV

TL;DR: TrueSkin是一个包含7299张图像的数据集，系统地将皮肤色调分为6类，用于评估和改进皮肤色调识别与生成模型的公平性和准确性。


<details>
  <summary>Details</summary>
Motivation: 皮肤色调识别和生成在模型公平性、医疗健康和生成式AI中很重要，但由于缺乏全面数据集和稳健方法而面临挑战。现有大型多模态模型和图像生成模型在准确识别和合成皮肤色调方面存在困难。

Method: 引入TrueSkin数据集，包含7299张在不同光照条件、相机角度和拍摄设置下收集的图像，系统分为6个类别。使用该数据集对现有识别和生成方法进行基准测试，并训练专门的识别模型和微调生成模型。

Result: 发现现有模型存在显著偏见：LMMs倾向于将中间皮肤色调误分类为较浅色调，生成模型在受提示中无关属性（如发型或环境背景）影响时难以准确生成指定皮肤色调。在TrueSkin上训练的识别模型比LMMs和传统方法准确率提高20%以上，微调后生成模型的皮肤色调保真度显著改善。

Conclusion: TrueSkin数据集不仅可作为评估现有模型的基准，还能作为有价值的训练资源，提高皮肤色调识别和生成任务的公平性和准确性，强调了此类全面数据集的重要性。

Abstract: Skin tone recognition and generation play important roles in model fairness, healthcare, and generative AI, yet they remain challenging due to the lack of comprehensive datasets and robust methodologies. Compared to other human image analysis tasks, state-of-the-art large multimodal models (LMMs) and image generation models struggle to recognize and synthesize skin tones accurately. To address this, we introduce TrueSkin, a dataset with 7299 images systematically categorized into 6 classes, collected under diverse lighting conditions, camera angles, and capture settings. Using TrueSkin, we benchmark existing recognition and generation approaches, revealing substantial biases: LMMs tend to misclassify intermediate skin tones as lighter ones, whereas generative models struggle to accurately produce specified skin tones when influenced by inherent biases from unrelated attributes in the prompts, such as hairstyle or environmental context. We further demonstrate that training a recognition model on TrueSkin improves classification accuracy by more than 20\% compared to LMMs and conventional approaches, and fine-tuning with TrueSkin significantly improves skin tone fidelity in image generation models. Our findings highlight the need for comprehensive datasets like TrueSkin, which not only serves as a benchmark for evaluating existing models but also provides a valuable training resource to enhance fairness and accuracy in skin tone recognition and generation tasks.

</details>


### [9] [The System Description of CPS Team for Track on Driving with Language of CVPR 2024 Autonomous Grand Challenge](https://arxiv.org/abs/2509.11071)
*Jinghan Peng,Jingwen Wang,Xing Yu,Dehui Du*

Main category: cs.CV

TL;DR: 基于LLaVA模型的视觉语言系统，通过LoRA和DoRA微调方法，集成深度信息，采用Chain-of-Thought推理，在CVPR 2024自动驾驶挑战赛中取得第一名


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶场景中的语言理解和视觉推理问题，提升模型在复杂驾驶环境中的问答能力

Method: 使用LLaVA模型为基础，采用LoRA和DoRA进行微调，集成开源深度估计模型的深度信息，在推理时使用Chain-of-Thought方法处理选择题和是非题

Result: 在验证集排行榜上获得0.7799的最高分，排名第一

Conclusion: 通过综合的视觉语言模型增强方法，包括深度信息集成和推理策略优化，显著提升了自动驾驶场景下的语言理解性能

Abstract: This report outlines our approach using vision language model systems for the Driving with Language track of the CVPR 2024 Autonomous Grand Challenge. We have exclusively utilized the DriveLM-nuScenes dataset for training our models. Our systems are built on the LLaVA models, which we enhanced through fine-tuning with the LoRA and DoRA methods. Additionally, we have integrated depth information from open-source depth estimation models to enrich the training and inference processes. For inference, particularly with multiple-choice and yes/no questions, we adopted a Chain-of-Thought reasoning approach to improve the accuracy of the results. This comprehensive methodology enabled us to achieve a top score of 0.7799 on the validation set leaderboard, ranking 1st on the leaderboard.

</details>


### [10] [PanoLora: Bridging Perspective and Panoramic Video Generation with LoRA Adaptation](https://arxiv.org/abs/2509.11092)
*Zeyu Dong,Yuyang Yin,Yuqi Li,Eric Li,Hao-Xiang Guo,Yikai Wang*

Main category: cs.CV

TL;DR: 通过LoRA适配技术，将现有视频生成模型轻量级调整为高质量360度全景视频生成模型，仅需约1,000个视频进行训练即可达到独特效果。


<details>
  <summary>Details</summary>
Motivation: 解决360度全景视频生成的挑战，现有方案通常需要复杂架构或大规模训练，导致效率低下和效果不佳。

Method: 将全景视频生成视为从透视视图到全景投影的适配问题，采用Low-Rank Adaptation (LoRA)技术对预训练视频模型进行精细调整。

Result: 实验结果显示，该方法能够维持正确的投影几何结构，在视觉质量、左右一致性和运动多样性方面超越了之前的最先进方法。

Conclusion: LoRA技术可以高效地模型不同投影之间的变换关系，通过轻量级调整方案实现了高质量的360度全景视频生成。

Abstract: Generating high-quality 360{\deg} panoramic videos remains a significant challenge due to the fundamental differences between panoramic and traditional perspective-view projections. While perspective videos rely on a single viewpoint with a limited field of view, panoramic content requires rendering the full surrounding environment, making it difficult for standard video generation models to adapt. Existing solutions often introduce complex architectures or large-scale training, leading to inefficiency and suboptimal results. Motivated by the success of Low-Rank Adaptation (LoRA) in style transfer tasks, we propose treating panoramic video generation as an adaptation problem from perspective views. Through theoretical analysis, we demonstrate that LoRA can effectively model the transformation between these projections when its rank exceeds the degrees of freedom in the task. Our approach efficiently fine-tunes a pretrained video diffusion model using only approximately 1,000 videos while achieving high-quality panoramic generation. Experimental results demonstrate that our method maintains proper projection geometry and surpasses previous state-of-the-art approaches in visual quality, left-right consistency, and motion diversity.

</details>


### [11] [SMILE: A Super-resolution Guided Multi-task Learning Method for Hyperspectral Unmixing](https://arxiv.org/abs/2509.11093)
*Ruiying Li,Bin Pan,Qiaoying Qu,Xia Xu,Zhenwei Shi*

Main category: cs.CV

TL;DR: SMILE方法通过理论分析和多任务学习框架，将超分辨率与高光谱解混结合，验证任务亲和性并保证解混收敛性


<details>
  <summary>Details</summary>
Motivation: 高光谱解混性能受限于低空间分辨率，直接整合超分辨率和解混存在任务亲和性未验证和解混收敛性无法保证的问题

Method: 提出超分辨率引导的多任务学习框架SMILE，通过理论分析验证任务亲和性，学习共享和特定表示来传递正向信息，并提供可达性定理保证解混收敛

Result: 在合成和真实数据集上的实验验证了方法的有效性

Conclusion: SMILE提供了渐进式理论支持，设计了超分辨率引导下的解混新框架，解决了多任务学习中的关键挑战

Abstract: The performance of hyperspectral unmixing may be constrained by low spatial resolution, which can be enhanced using super-resolution in a multitask learning way. However, integrating super-resolution and unmixing directly may suffer two challenges: Task affinity is not verified, and the convergence of unmixing is not guaranteed. To address the above issues, in this paper, we provide theoretical analysis and propose super-resolution guided multi-task learning method for hyperspectral unmixing (SMILE). The provided theoretical analysis validates feasibility of multitask learning way and verifies task affinity, which consists of relationship and existence theorems by proving the positive guidance of super-resolution. The proposed framework generalizes positive information from super-resolution to unmixing by learning both shared and specific representations. Moreover, to guarantee the convergence, we provide the accessibility theorem by proving the optimal solution of unmixing. The major contributions of SMILE include providing progressive theoretical support, and designing a new framework for unmixing under the guidance of super-resolution. Our experiments on both synthetic and real datasets have substantiate the usefulness of our work.

</details>


### [12] [Filling the Gaps: A Multitask Hybrid Multiscale Generative Framework for Missing Modality in Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2509.11102)
*Nhi Kieu,Kien Nguyen,Arnold Wiliem,Clinton Fookes,Sridha Sridharan*

Main category: cs.CV

TL;DR: 提出GEMMNet网络解决多模态遥感数据缺失问题，通过混合特征提取、多尺度融合和互补损失函数，在语义分割任务上超越现有生成式和非生成式方法


<details>
  <summary>Details</summary>
Motivation: 现实场景中多模态信号容易因传感器故障和恶劣天气而缺失，传统生成模型（AE、GAN）在处理多模态遥感数据异质性和语义上下文捕获方面存在不足，且容易过度依赖主导模态

Method: 提出GEMMNet网络，包含三个核心组件：1) HyFEx混合特征提取器学习模态特定表示；2) HyFMA多尺度感知混合融合捕获跨尺度模态协同语义上下文；3) CoLoss互补损失方案通过鼓励跨模态和任务一致性来减轻固有偏差

Result: 在两个具有挑战性的遥感语义分割数据集（Vaihingen和Potsdam）上，GEMMNet超越了生成式基线方法（AE、cGAN）和最先进的非生成式方法（mmformer、shaspec）

Conclusion: GEMMNet有效解决了多模态遥感数据缺失问题，通过创新的网络设计和损失函数，显著提升了在复杂场景下的语义分割性能，为多模态学习提供了新的解决方案

Abstract: Multimodal learning has shown significant performance boost compared to ordinary unimodal models across various domains. However, in real-world scenarios, multimodal signals are susceptible to missing because of sensor failures and adverse weather conditions, which drastically deteriorates models' operation and performance. Generative models such as AutoEncoder (AE) and Generative Adversarial Network (GAN) are intuitive solutions aiming to reconstruct missing modality from available ones. Yet, their efficacy in remote sensing semantic segmentation remains underexplored. In this paper, we first examine the limitations of existing generative approaches in handling the heterogeneity of multimodal remote sensing data. They inadequately capture semantic context in complex scenes with large intra-class and small inter-class variation. In addition, traditional generative models are susceptible to heavy dependence on the dominant modality, introducing bias that affects model robustness under missing modality conditions. To tackle these limitations, we propose a novel Generative-Enhanced MultiModal learning Network (GEMMNet) with three key components: (1) Hybrid Feature Extractor (HyFEx) to effectively learn modality-specific representations, (2) Hybrid Fusion with Multiscale Awareness (HyFMA) to capture modality-synergistic semantic context across scales and (3) Complementary Loss (CoLoss) scheme to alleviate the inherent bias by encouraging consistency across modalities and tasks. Our method, GEMMNet, outperforms both generative baselines AE, cGAN (conditional GAN), and state-of-the-art non-generative approaches - mmformer and shaspec - on two challenging semantic segmentation remote sensing datasets (Vaihingen and Potsdam). Source code is made available.

</details>


### [13] [SVR-GS: Spatially Variant Regularization for Probabilistic Masks in 3D Gaussian Splatting](https://arxiv.org/abs/2509.11116)
*Ashkan Taghipour,Vahid Naghshin,Benjamin Southwell,Farid Boussaid,Hamid Laga,Mohammed Bennamoun*

Main category: cs.CV

TL;DR: SVR-GS是一种空间变体正则化方法，通过基于每个高斯沿光线有效贡献的逐像素空间掩码，显著减少3D高斯数量同时保持图像质量


<details>
  <summary>Details</summary>
Motivation: 现有基于掩码的剪枝方法（如MaskGS）使用全局均值正则化，这与决定图像质量的逐像素重建损失不匹配，需要更精细的空间感知正则化

Method: 提出空间变体正则化器，渲染每个高斯的逐像素空间掩码，探索三种空间掩码聚合策略，并在CUDA中实现，通过梯度分析确定最终设计

Result: 在三个数据集上平均减少高斯数量：相比MaskGS减少1.79倍，相比3DGS减少5.63倍，PSNR仅下降0.50dB和0.40dB

Conclusion: SVR-GS产生更小、更快、内存效率更高的模型，适用于机器人、AR/VR和移动感知等实时应用

Abstract: 3D Gaussian Splatting (3DGS) enables fast, high-quality novel view synthesis but typically relies on densification followed by pruning to optimize the number of Gaussians. Existing mask-based pruning, such as MaskGS, regularizes the global mean of the mask, which is misaligned with the local per-pixel (per-ray) reconstruction loss that determines image quality along individual camera rays. This paper introduces SVR-GS, a spatially variant regularizer that renders a per-pixel spatial mask from each Gaussian's effective contribution along the ray, thereby applying sparsity pressure where it matters: on low-importance Gaussians. We explore three spatial-mask aggregation strategies, implement them in CUDA, and conduct a gradient analysis to motivate our final design. Extensive experiments on Tanks\&Temples, Deep Blending, and Mip-NeRF360 datasets demonstrate that, on average across the three datasets, the proposed SVR-GS reduces the number of Gaussians by 1.79\(\times\) compared to MaskGS and 5.63\(\times\) compared to 3DGS, while incurring only 0.50 dB and 0.40 dB PSNR drops, respectively. These gains translate into significantly smaller, faster, and more memory-efficient models, making them well-suited for real-time applications such as robotics, AR/VR, and mobile perception.

</details>


### [14] [Traffic-MLLM: A Spatio-Temporal MLLM with Retrieval-Augmented Generation for Causal Inference in Traffic](https://arxiv.org/abs/2509.11165)
*Waikit Xiu,Qiang Lu,Xiying Li,Chen Hu,Shengbo Sun*

Main category: cs.CV

TL;DR: Traffic-MLLM是基于Qwen2.5-VL的多模态大语言模型，通过LoRA微调和知识提示模块，在交通视频分析中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有交通视频理解方法在建模时空因果关系和整合领域知识方面存在不足，限制了在复杂场景中的有效性。

Method: 基于Qwen2.5-VL架构，使用高质量交通多模态数据集和LoRA轻量级微调，引入结合CoT推理和RAG的知识提示模块。

Result: 在TrafficQA和DriveQA基准测试中达到最先进性能，展现出优秀的零样本推理和跨场景泛化能力。

Conclusion: Traffic-MLLM通过创新的知识融合和微调策略，显著提升了交通视频分析的时空建模和逻辑推理能力。

Abstract: As intelligent transportation systems advance, traffic video understanding plays an increasingly pivotal role in comprehensive scene perception and causal analysis. Yet, existing approaches face notable challenges in accurately modeling spatiotemporal causality and integrating domain-specific knowledge, limiting their effectiveness in complex scenarios. To address these limitations, we propose Traffic-MLLM, a multimodal large language model tailored for fine-grained traffic analysis. Built on the Qwen2.5-VL backbone, our model leverages high-quality traffic-specific multimodal datasets and uses Low-Rank Adaptation (LoRA) for lightweight fine-tuning, significantly enhancing its capacity to model continuous spatiotemporal features in video sequences. Furthermore, we introduce an innovative knowledge prompting module fusing Chain-of-Thought (CoT) reasoning with Retrieval-Augmented Generation (RAG), enabling precise injection of detailed traffic regulations and domain knowledge into the inference process. This design markedly boosts the model's logical reasoning and knowledge adaptation capabilities. Experimental results on TrafficQA and DriveQA benchmarks show Traffic-MLLM achieves state-of-the-art performance, validating its superior ability to process multimodal traffic data. It also exhibits remarkable zero-shot reasoning and cross-scenario generalization capabilities.

</details>


### [15] [Multispectral-NeRF:a multispectral modeling approach based on neural radiance fields](https://arxiv.org/abs/2509.11169)
*Hong Zhang,Fei Guo,Zihan Xie,Dizhao Yao*

Main category: cs.CV

TL;DR: 提出了Multispectral-NeRF，一种基于NeRF的改进神经网络架构，能够有效处理多光谱信息进行3D重建，解决了传统方法无法利用多波段信息的问题。


<details>
  <summary>Details</summary>
Motivation: 传统3D重建技术主要依赖RGB光谱信息，而现有的多光谱3D重建方法存在成本高、精度低和几何特征差的问题。NeRF及其改进模型只能处理三波段数据，无法充分利用多波段信息。

Method: 通过三个技术改进：扩展隐藏层维度以容纳6波段光谱输入；重新设计残差函数以优化重建图像与参考图像之间的光谱差异计算；调整数据压缩模块以适应多光谱图像增加的位深度需求。

Result: 实验结果表明Multispectral-NeRF成功处理了多波段光谱特征，同时准确保留了原始场景的光谱特性。

Conclusion: Multispectral-NeRF有效解决了当前多光谱3D重建方法的局限性，能够产生高精度、高质量的3D重建结果，为多光谱3D重建提供了新的解决方案。

Abstract: 3D reconstruction technology generates three-dimensional representations of real-world objects, scenes, or environments using sensor data such as 2D images, with extensive applications in robotics, autonomous vehicles, and virtual reality systems. Traditional 3D reconstruction techniques based on 2D images typically relies on RGB spectral information. With advances in sensor technology, additional spectral bands beyond RGB have been increasingly incorporated into 3D reconstruction workflows. Existing methods that integrate these expanded spectral data often suffer from expensive scheme prices, low accuracy and poor geometric features. Three - dimensional reconstruction based on NeRF can effectively address the various issues in current multispectral 3D reconstruction methods, producing high - precision and high - quality reconstruction results. However, currently, NeRF and some improved models such as NeRFacto are trained on three - band data and cannot take into account the multi - band information. To address this problem, we propose Multispectral-NeRF, an enhanced neural architecture derived from NeRF that can effectively integrates multispectral information. Our technical contributions comprise threefold modifications: Expanding hidden layer dimensionality to accommodate 6-band spectral inputs; Redesigning residual functions to optimize spectral discrepancy calculations between reconstructed and reference images; Adapting data compression modules to address the increased bit-depth requirements of multispectral imagery. Experimental results confirm that Multispectral-NeRF successfully processes multi-band spectral features while accurately preserving the original scenes' spectral characteristics.

</details>


### [16] [SPHERE: Semantic-PHysical Engaged REpresentation for 3D Semantic Scene Completion](https://arxiv.org/abs/2509.11171)
*Zhiwen Yang,Yuxin Peng*

Main category: cs.CV

TL;DR: SPHERE是一种新颖的相机3D语义场景补全方法，结合体素和高斯表示，通过语义引导的高斯初始化和物理感知谐波增强模块，在保持语义准确性的同时实现逼真的几何细节重建。


<details>
  <summary>Details</summary>
Motivation: 现有体素和平面方法难以捕捉物理规律实现逼真几何细节，而神经重建方法如NeRF和3DGS计算成本高、收敛慢，语义准确性差，需要一种能同时处理语义和物理信息的方法。

Method: 提出SPHERE框架：1）语义引导高斯初始化模块利用双分支3D场景表示定位焦点体素作为锚点；2）物理感知谐波增强模块通过语义球谐函数建模物理感知上下文细节，通过焦点分布对齐促进语义-几何一致性。

Result: 在SemanticKITTI和SSCBench-KITTI-360基准测试上进行了广泛实验验证，证明方法有效。

Conclusion: SPHERE成功整合了体素和高斯表示的优势，在保持语义准确性的同时实现了逼真的几何细节重建，为自动驾驶场景感知提供了有效的解决方案。

Abstract: Camera-based 3D Semantic Scene Completion (SSC) is a critical task in autonomous driving systems, assessing voxel-level geometry and semantics for holistic scene perception. While existing voxel-based and plane-based SSC methods have achieved considerable progress, they struggle to capture physical regularities for realistic geometric details. On the other hand, neural reconstruction methods like NeRF and 3DGS demonstrate superior physical awareness, but suffer from high computational cost and slow convergence when handling large-scale, complex autonomous driving scenes, leading to inferior semantic accuracy. To address these issues, we propose the Semantic-PHysical Engaged REpresentation (SPHERE) for camera-based SSC, which integrates voxel and Gaussian representations for joint exploitation of semantic and physical information. First, the Semantic-guided Gaussian Initialization (SGI) module leverages dual-branch 3D scene representations to locate focal voxels as anchors to guide efficient Gaussian initialization. Then, the Physical-aware Harmonics Enhancement (PHE) module incorporates semantic spherical harmonics to model physical-aware contextual details and promote semantic-geometry consistency through focal distribution alignment, generating SSC results with realistic details. Extensive experiments and analyses on the popular SemanticKITTI and SSCBench-KITTI-360 benchmarks validate the effectiveness of SPHERE. The code is available at https://github.com/PKU-ICST-MIPL/SPHERE_ACMMM2025.

</details>


### [17] [Beyond Sliders: Mastering the Art of Diffusion-based Image Manipulation](https://arxiv.org/abs/2509.11213)
*Yufei Tang,Daiheng Gao,Pingyu Wu,Wenbo Zhou,Bang Zhang,Weiming Zhang*

Main category: cs.CV

TL;DR: Beyond Sliders是一个创新框架，结合GAN和扩散模型，在概念滑块基础上改进，通过文本和视觉的细粒度对抗性指导来增强图像质量和真实感，特别针对非AIGC图像处理。


<details>
  <summary>Details</summary>
Motivation: 现有方法如概念滑块在处理非AIGC图像（特别是真实世界拍摄的图像）时表现不佳，需要开发能够跨越不同图像类别进行复杂图像操作的新方法。

Method: 集成GAN和扩散模型，通过文本和视觉的细粒度对抗性指导来精炼图像，改进概念滑块方法。

Result: 广泛的实验验证证实了Beyond Sliders在各种应用中的鲁棒性和多功能性，图像质量和真实感显著提升。

Conclusion: Beyond Sliders成功填补了现有方法在处理非AIGC图像方面的不足，为跨类别图像操作提供了有效的解决方案。

Abstract: In the realm of image generation, the quest for realism and customization has never been more pressing. While existing methods like concept sliders have made strides, they often falter when it comes to no-AIGC images, particularly images captured in real world settings. To bridge this gap, we introduce Beyond Sliders, an innovative framework that integrates GANs and diffusion models to facilitate sophisticated image manipulation across diverse image categories. Improved upon concept sliders, our method refines the image through fine grained guidance both textual and visual in an adversarial manner, leading to a marked enhancement in image quality and realism. Extensive experimental validation confirms the robustness and versatility of Beyond Sliders across a spectrum of applications.

</details>


### [18] [ANROT-HELANet: Adverserially and Naturally Robust Attention-Based Aggregation Network via The Hellinger Distance for Few-Shot Classification](https://arxiv.org/abs/2509.11220)
*Gao Yu Lee,Tanmoy Dam,Md Meftahul Ferdaus,Daniel Puiu Poenar,Vu N. Duong*

Main category: cs.CV

TL;DR: ANROT-HELANet是一个基于Hellinger距离的对抗性和自然鲁棒的小样本学习网络，在对抗扰动和噪声环境下显著提升性能，在多个基准数据集上达到新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有的基于KL散度的贝叶斯估计方法虽然在小样本学习中表现优于普通CNN方法，但仍然容易受到对抗攻击和自然噪声的影响，需要开发更鲁棒的解决方案。

Method: 提出了ANROT-HELANet网络，采用基于Hellinger距离的特征类别聚合方案，引入新颖的Hellinger相似性对比损失函数，结合注意力机制，实现对对抗扰动和自然噪声的鲁棒性。

Result: 在对抗扰动ε=0.30和高斯噪声σ=0.30下保持鲁棒性；在miniImageNet上1-shot和5-shot场景分别提升1.20%和1.40%；图像重建质量FID得分2.75，优于传统VAE和WAE方法。

Conclusion: ANROT-HELANet通过Hellinger距离特征聚合、注意力机制和新颖损失函数的组合，在保持对抗性和自然鲁棒性的同时，在小样本学习任务中确立了新的最先进性能。

Abstract: Few-Shot Learning (FSL), which involves learning to generalize using only a few data samples, has demonstrated promising and superior performances to ordinary CNN methods. While Bayesian based estimation approaches using Kullback-Leibler (KL) divergence have shown improvements, they remain vulnerable to adversarial attacks and natural noises. We introduce ANROT-HELANet, an Adversarially and Naturally RObusT Hellinger Aggregation Network that significantly advances the state-of-the-art in FSL robustness and performance. Our approach implements an adversarially and naturally robust Hellinger distance-based feature class aggregation scheme, demonstrating resilience to adversarial perturbations up to $\epsilon=0.30$ and Gaussian noise up to $\sigma=0.30$. The network achieves substantial improvements across benchmark datasets, including gains of 1.20\% and 1.40\% for 1-shot and 5-shot scenarios on miniImageNet respectively. We introduce a novel Hellinger Similarity contrastive loss function that generalizes cosine similarity contrastive loss for variational few-shot inference scenarios. Our approach also achieves superior image reconstruction quality with a FID score of 2.75, outperforming traditional VAE (3.43) and WAE (3.38) approaches. Extensive experiments conducted on four few-shot benchmarked datasets verify that ANROT-HELANet's combination of Hellinger distance-based feature aggregation, attention mechanisms, and our novel loss function establishes new state-of-the-art performance while maintaining robustness against both adversarial and natural perturbations. Our code repository will be available at https://github.com/GreedYLearner1146/ANROT-HELANet/tree/main.

</details>


### [19] [ROSGS: Relightable Outdoor Scenes With Gaussian Splatting](https://arxiv.org/abs/2509.11275)
*Lianjun Liao,Chunhui Zhang,Tong Wu,Henglei Lv,Bailin Deng,Lin Gao*

Main category: cs.CV

TL;DR: ROSGS是一个两阶段的高效室外场景重光照重建方法，使用高斯泼溅表示，结合单目法线先验和混合光照模型，在重光照精度和渲染效率方面达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 室外图像数据通常包含无界场景和变化的照明条件，现有方法如NeRF和3DGS存在计算开销大和低频光照表示导致渲染效率低、重光照精度差的问题

Method: 两阶段流程：1) 使用单目法线先验和2D高斯泼溅重建场景几何；2) 通过混合光照模型分解纹理和光照，使用球面高斯函数捕捉高频阳光分量，球谐系数学习低频天光

Result: 定量指标和定性比较表明，ROSGS在室外场景重光照方面达到最先进性能，具有优越的重光照精度和渲染效率

Conclusion: ROSGS通过创新的两阶段方法和混合光照表示，成功解决了室外场景重光照的挑战，在精度和效率方面均表现出色

Abstract: Image data captured outdoors often exhibit unbounded scenes and unconstrained, varying lighting conditions, making it challenging to decompose them into geometry, reflectance, and illumination. Recent works have focused on achieving this decomposition using Neural Radiance Fields (NeRF) or the 3D Gaussian Splatting (3DGS) representation but remain hindered by two key limitations: the high computational overhead associated with neural networks of NeRF and the use of low-frequency lighting representations, which often result in inefficient rendering and suboptimal relighting accuracy. We propose ROSGS, a two-stage pipeline designed to efficiently reconstruct relightable outdoor scenes using the Gaussian Splatting representation. By leveraging monocular normal priors, ROSGS first reconstructs the scene's geometry with the compact 2D Gaussian Splatting (2DGS) representation, providing an efficient and accurate geometric foundation. Building upon this reconstructed geometry, ROSGS then decomposes the scene's texture and lighting through a hybrid lighting model. This model effectively represents typical outdoor lighting by employing a spherical Gaussian function to capture the directional, high-frequency components of sunlight, while learning a radiance transfer function via Spherical Harmonic coefficients to model the remaining low-frequency skylight comprehensively. Both quantitative metrics and qualitative comparisons demonstrate that ROSGS achieves state-of-the-art performance in relighting outdoor scenes and highlight its ability to deliver superior relighting accuracy and rendering efficiency.

</details>


### [20] [In-Vivo Skin 3-D Surface Reconstruction and Wrinkle Depth Estimation using Handheld High Resolution Tactile Sensing](https://arxiv.org/abs/2509.11385)
*Akhil Padmanabha,Arpit Agarwal,Catherine Li,Austin Williams,Dinesh K. Patel,Sankalp Chopkar,Achu Wilson,Ahmet Ozkan,Wenzhen Yuan,Sonal Choudhary,Arash Mostaghimi,Zackory Erickson,Carmel Majidi*

Main category: cs.CV

TL;DR: 一种基于GelSight触觉成像技术的手持式3D皮肤重建探头，通过学习算法实现微米级细纹深度估计，在多个身体区域提供第一个经验证的细纹深度评估工具。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏精确、可供实际应用的三维皮肤表面重建设备，需要一种可以在各种身体位置进行高分辨率深度重建的符合交付要求的手持式设备。

Method: 基于GelSight触觉成像技术，使用自定制弹性凝胶和基于学习的重建算法，集成到具有力感测量功能的手持式探头中，以保证一致的接触条件。

Result: 在细纹模拟测试物体上实现了12.55微米的平均绝对误差，在15名健康参与者的多个身体区域提供了第一个经验证的细纹深度指标，并证明了使用过订护肤霜后三个位置的细纹高度减少具有统计学显著性。

Conclusion: 该研究提供了一种经过验证的工具，可用于临床和化妆皮肤分析，在诊断、治疗监测和护肤效果评估方面具有潜在应用价值。

Abstract: Three-dimensional (3-D) skin surface reconstruction offers promise for objective and quantitative dermatological assessment, but no portable, high-resolution device exists that has been validated and used for depth reconstruction across various body locations. We present a compact 3-D skin reconstruction probe based on GelSight tactile imaging with a custom elastic gel and a learning-based reconstruction algorithm for micron-level wrinkle height estimation. Our probe, integrated into a handheld probe with force sensing for consistent contact, achieves a mean absolute error of 12.55 micron on wrinkle-like test objects. In a study with 15 participants without skin disorders, we provide the first validated wrinkle depth metrics across multiple body regions. We further demonstrate statistically significant reductions in wrinkle height at three locations following over-the-counter moisturizer application. Our work offers a validated tool for clinical and cosmetic skin analysis, with potential applications in diagnosis, treatment monitoring, and skincare efficacy evaluation.

</details>


### [21] [MVQA-68K: A Multi-dimensional and Causally-annotated Dataset with Quality Interpretability for Video Assessment](https://arxiv.org/abs/2509.11589)
*Yanyun Pu,Kehan Li,Zeyi Huang,Zhijie Zhong,Kaixiang Yang*

Main category: cs.CV

TL;DR: 作者提出MVQA-68K多维视频质量评估数据集，包含68,000个视频和7个质量维度的详细注释，通过链式思维推理提高了多模态大语言模型的VQA性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统视频质量评估方法产生的单一数值分数缺乏全面性和可解释性，无法满足Sora等视频生成模型对大规模高质量视频数据集的需求。

Method: 构建MVQA-68K数据集，包含68,000个详细注释的视频，覆盖7个关键质量维度：整体美学、摄像机运动、动态程度、纹理细节、构图、视觉质量和事实一致性。每个注释包含链式思维推理过程。

Result: 实验结果显示MVQA-68K显著提升了多模态大语言模型在VQA任务上的性能，在内部测试集和公开测试集（LSVQ-test、LSVQ-1080p、LIVE-VQC）上达到最高水平。同时，在VQA训练中融入显式推理过程显著提高了零样本泛化能力。

Conclusion: MVQA-68K数据集通过多维度质量注释和链式思维推理，有效解决了传统VQA方法的缺陷，为视频质量评估提供了更全面、可解释性强的解决方案，对视频生成模型的训练数据选择具有重要价值。

Abstract: With the rapid advancement of video generation models such as Sora, video quality assessment (VQA) is becoming increasingly crucial for selecting high-quality videos from large-scale datasets used in pre-training. Traditional VQA methods, typically producing single numerical scores, often lack comprehensiveness and interpretability. To address these challenges, we introduce MVQA-68K, a novel multi-dimensional VQA dataset comprising over 68,000 carefully annotated videos, covering seven essential quality dimensions: overall aesthetics, camera movement, dynamic degree, texture detail, composition, visual quality, and factual consistency. Each annotation includes detailed chain-of-thought reasoning to facilitate interpretability and comprehensive understanding. Extensive experiments demonstrate that MVQA-68K significantly enhances the performance of various multimodal large language models (MLLMs) on the VQA task, achieving state-of-the-art results not only on our internal test set (Fig.1) but also on public benchmarks including LSVQ-test, LSVQ-1080p, and LIVE-VQC. Meantime, incorporating explicit reasoning process during VQA training substantially boosts the zero-shot generalization. Code and dataset will be available at github: https://github.com/Controller01-ai/MVQA-68K

</details>


### [22] [A Controllable 3D Deepfake Generation Framework with Gaussian Splatting](https://arxiv.org/abs/2509.11624)
*Wending Liu,Siyun Liang,Huy H. Nguyen,Isao Echizen*

Main category: cs.CV

TL;DR: 基于3D高斯拖尾的新题3D深度伪造生成框架，支持多视角一致的面部换脸和重现，充分利甩3D高斯表示的优势


<details>
  <summary>Details</summary>
Motivation: 解决传统2D深度伪造技术在几何不一致性、新视角沿生和控制性方面的限制，尝试将3D建模与深度伪造合成相结合

Method: 结合参数化头部模型与动态高斯表示，明确分离头部和背景高斯，使用预训练2D指导进行多视角优化，并主动修复极端姿势和表情下的视觉一致性

Result: 在NeRSemble数据集上识别保持、姿势表情一致性方面与最佳2D方法相当，在多视角渲染质量和3D一致性方面显著优于对2D方法

Conclusion: 该方法为场景感知、可控制和沉浸式视觉伪造开启了新方向，同时也告诉了3D高斯拖尾技术可能被用于操纷攻击的风险

Abstract: We propose a novel 3D deepfake generation framework based on 3D Gaussian Splatting that enables realistic, identity-preserving face swapping and reenactment in a fully controllable 3D space. Compared to conventional 2D deepfake approaches that suffer from geometric inconsistencies and limited generalization to novel view, our method combines a parametric head model with dynamic Gaussian representations to support multi-view consistent rendering, precise expression control, and seamless background integration. To address editing challenges in point-based representations, we explicitly separate the head and background Gaussians and use pre-trained 2D guidance to optimize the facial region across views. We further introduce a repair module to enhance visual consistency under extreme poses and expressions. Experiments on NeRSemble and additional evaluation videos demonstrate that our method achieves comparable performance to state-of-the-art 2D approaches in identity preservation, as well as pose and expression consistency, while significantly outperforming them in multi-view rendering quality and 3D consistency. Our approach bridges the gap between 3D modeling and deepfake synthesis, enabling new directions for scene-aware, controllable, and immersive visual forgeries, revealing the threat that emerging 3D Gaussian Splatting technique could be used for manipulation attacks.

</details>


### [23] [IS-Diff: Improving Diffusion-Based Inpainting with Better Initial Seed](https://arxiv.org/abs/2509.11638)
*Yongzhe Lyu,Yu Wu,Yutian Lin,Bo Du*

Main category: cs.CV

TL;DR: IS-Diff是一种无需训练的扩散模型修复方法，通过从非掩码区域采样初始化种子来生成语义一致的修复结果，避免传统随机初始化导致的语义不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型在修复任务中使用随机初始化种子，可能导致掩码区域与非掩码区域的语义信息不匹配，产生低一致性和低连贯性的修复结果。

Method: 提出IS-Diff方法：1）使用从非掩码区域采样的分布和谐种子作为初始化；2）设计动态选择性精炼机制，检测中间潜在空间中的不和谐修复并动态调整初始化先验强度。

Result: 在CelebA-HQ、ImageNet和Places2数据集的标准和大掩码修复任务中，IS-Diff在所有指标上都优于最先进的修复方法。

Conclusion: IS-Diff通过分布和谐的初始化种子和动态精炼机制，有效提升了扩散模型修复的语义一致性和连贯性，且无需额外训练。

Abstract: Diffusion models have shown promising results in free-form inpainting. Recent studies based on refined diffusion samplers or novel architectural designs led to realistic results and high data consistency. However, random initialization seed (noise) adopted in vanilla diffusion process may introduce mismatched semantic information in masked regions, leading to biased inpainting results, e.g., low consistency and low coherence with the other unmasked area. To address this issue, we propose the Initial Seed refined Diffusion Model (IS-Diff), a completely training-free approach incorporating distributional harmonious seeds to produce harmonious results. Specifically, IS-Diff employs initial seeds sampled from unmasked areas to imitate the masked data distribution, thereby setting a promising direction for the diffusion procedure. Moreover, a dynamic selective refinement mechanism is proposed to detect severe unharmonious inpaintings in intermediate latent and adjust the strength of our initialization prior dynamically. We validate our method on both standard and large-mask inpainting tasks using the CelebA-HQ, ImageNet, and Places2 datasets, demonstrating its effectiveness across all metrics compared to state-of-the-art inpainting methods.

</details>


### [24] [WeatherBench: A Real-World Benchmark Dataset for All-in-One Adverse Weather Image Restoration](https://arxiv.org/abs/2509.11642)
*Qiyuan Guan,Qianfeng Yang,Xiang Chen,Tianyu Song,Guiyue Jin,Jiyu Jin*

Main category: cs.CV

TL;DR: 这篇论文提出了一个真实世界的全能天气图像恢复数据集WeatherBench，解决了当前预训练模型使用合成数据集存在的领域差异问题，为全能天气恢复预训练模型的发展提供了基础。


<details>
  <summary>Details</summary>
Motivation: 现有的全能天气图像恢复方法主要使用合成数据集进行训练和评估，但这些数据集在分辨率、风格和领域特征上存在显著差异，导致了大的领域间隔，阻碍了统一模型的发展和公平评估。同时，缺乏大规模真实世界的全能天气恢复数据集是该领域发展的关键瓶颈。

Method: 研究人员构建了一个真实世界的全能天气恶劣天气图像恢复数据集基准，包含在多种天气条件（如雨、雪、霜）下捕获的图像对，以及多样的户外场景和照明设置。该数据集提供了精确对齐的退化和清澄图像，支持监督学习和严格的评估。

Result: 研究人员在新数据集上对各种任务特定、任务通用和全能天气恢复方法进行了全面的实验对比评测。该数据集为推动真实世界场景下健壮和实用的全能天气图像恢复技术提供了价值较高的基础。

Conclusion: 该研究通过构建WeatherBench数据集，有效解决了当前全能天气图像恢复预训练模型面临的数据集问题。该数据集已经公开发布，为该领域的研究和应用提供了重要的资源支持。

Abstract: Existing all-in-one image restoration approaches, which aim to handle multiple weather degradations within a single framework, are predominantly trained and evaluated using mixed single-weather synthetic datasets. However, these datasets often differ significantly in resolution, style, and domain characteristics, leading to substantial domain gaps that hinder the development and fair evaluation of unified models. Furthermore, the lack of a large-scale, real-world all-in-one weather restoration dataset remains a critical bottleneck in advancing this field. To address these limitations, we present a real-world all-in-one adverse weather image restoration benchmark dataset, which contains image pairs captured under various weather conditions, including rain, snow, and haze, as well as diverse outdoor scenes and illumination settings. The resulting dataset provides precisely aligned degraded and clean images, enabling supervised learning and rigorous evaluation. We conduct comprehensive experiments by benchmarking a variety of task-specific, task-general, and all-in-one restoration methods on our dataset. Our dataset offers a valuable foundation for advancing robust and practical all-in-one image restoration in real-world scenarios. The dataset has been publicly released and is available at https://github.com/guanqiyuan/WeatherBench.

</details>


### [25] [DTGen: Generative Diffusion-Based Few-Shot Data Augmentation for Fine-Grained Dirty Tableware Recognition](https://arxiv.org/abs/2509.11661)
*Lifei Hao,Yue Cheng,Baoqi Huang,Bing Jia,Xuandong Zhao*

Main category: cs.CV

TL;DR: DTGen是一种基于扩散模型的少样本数据增强方案，专门用于细粒度脏餐具识别，通过LoRA实现高效领域专业化，生成多样化脏污图像，并通过CLIP跨模态过滤确保数据质量。


<details>
  <summary>Details</summary>
Motivation: 智能餐具清洁在食品安全和智能家居中至关重要，但现有方法受限于粗粒度分类和少样本数据稀缺，难以满足工业化需求。

Method: 基于生成扩散模型的少样本数据增强方案，使用LoRA实现高效领域专业化，通过结构化提示生成多样化脏污图像，采用CLIP跨模态过滤确保数据质量。

Result: 在极有限的真实少样本条件下，DTGen能合成几乎无限的高质量样本，显著提升分类器性能，支持细粒度脏餐具识别。

Conclusion: DTGen不仅验证了生成式AI在少样本工业视觉中的价值，还为自动化餐具清洁和食品安全监控提供了可行的部署路径。

Abstract: Intelligent tableware cleaning is a critical application in food safety and smart homes, but existing methods are limited by coarse-grained classification and scarcity of few-shot data, making it difficult to meet industrialization requirements. We propose DTGen, a few-shot data augmentation scheme based on generative diffusion models, specifically designed for fine-grained dirty tableware recognition. DTGen achieves efficient domain specialization through LoRA, generates diverse dirty images via structured prompts, and ensures data quality through CLIP-based cross-modal filtering. Under extremely limited real few-shot conditions, DTGen can synthesize virtually unlimited high-quality samples, significantly improving classifier performance and supporting fine-grained dirty tableware recognition. We further elaborate on lightweight deployment strategies, promising to transfer DTGen's benefits to embedded dishwashers and integrate with cleaning programs to intelligently regulate energy consumption and detergent usage. Research results demonstrate that DTGen not only validates the value of generative AI in few-shot industrial vision but also provides a feasible deployment path for automated tableware cleaning and food safety monitoring.

</details>


### [26] [Segmentation-Driven Initialization for Sparse-view 3D Gaussian Splatting](https://arxiv.org/abs/2509.11853)
*Yi-Hsin Li,Thomas Sikora,Sebastian Knorr,Måarten Sjöström*

Main category: cs.CV

TL;DR: SDI-GS通过区域分割驱动初始化，在稀疏视图合成中减少50%高斯点数量，保持渲染质量的同时降低内存和训练时间


<details>
  <summary>Details</summary>
Motivation: 解决稀疏视图合成中现有方法依赖SfM相机姿态估计困难，以及基于MVS的方法生成过多高斯点导致内存成本高的问题

Method: 利用区域分割识别结构重要区域，对密集点云进行选择性下采样，保留场景保真度同时大幅减少高斯点数量

Result: 在多个基准测试中减少高达50%的高斯点数量，PSNR和SSIM指标相当或更优，LPIPS仅有轻微下降，训练更快且内存占用更低

Conclusion: SDI-GS提高了3D高斯溅射在受限视图场景中的实用性，为稀疏视图合成提供了高效解决方案

Abstract: Sparse-view synthesis remains a challenging problem due to the difficulty of recovering accurate geometry and appearance from limited observations. While recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time rendering with competitive quality, existing pipelines often rely on Structure-from-Motion (SfM) for camera pose estimation, an approach that struggles in genuinely sparse-view settings. Moreover, several SfM-free methods replace SfM with multi-view stereo (MVS) models, but generate massive numbers of 3D Gaussians by back-projecting every pixel into 3D space, leading to high memory costs. We propose Segmentation-Driven Initialization for Gaussian Splatting (SDI-GS), a method that mitigates inefficiency by leveraging region-based segmentation to identify and retain only structurally significant regions. This enables selective downsampling of the dense point cloud, preserving scene fidelity while substantially reducing Gaussian count. Experiments across diverse benchmarks show that SDI-GS reduces Gaussian count by up to 50% and achieves comparable or superior rendering quality in PSNR and SSIM, with only marginal degradation in LPIPS. It further enables faster training and lower memory footprint, advancing the practicality of 3DGS for constrained-view scenarios.

</details>


### [27] [Do It Yourself (DIY): Modifying Images for Poems in a Zero-Shot Setting Using Weighted Prompt Manipulation](https://arxiv.org/abs/2509.11878)
*Sofia Jamil,Kotla Sai Charan,Sriparna Saha,Koustava Goswami,K J Joseph*

Main category: cs.CV

TL;DR: 基于权重提示操控(WPM)技术的零样本诗歌图像生成方法，通过动态调整注意力权重和文本嵌入来改善诗歌的视觉化效果


<details>
  <summary>Details</summary>
Motivation: 诗歌作为一种表达性艺术形式，幻请多重解释，需要为诗歌生成图像并支持阅读者根据自身需求修改图像

Method: 提出权重提示操控(WPM)技术，系统性修改模型内的注意力权重和文本嵌入，动态调整具体单词的重要性，利用双向模型和大语言模型结合现有诗歌数据集

Result: 实现了语义更丰富、上下文更准确的视觉化效果，支持阅读者根据个人需求修改生成的图像

Conclusion: 这是首次将权重提示操控技术应用于提升诗歌语言图像生成的研究，为文学领域的图像生成提供了结构化方法

Abstract: Poetry is an expressive form of art that invites multiple interpretations, as readers often bring their own emotions, experiences, and cultural backgrounds into their understanding of a poem. Recognizing this, we aim to generate images for poems and improve these images in a zero-shot setting, enabling audiences to modify images as per their requirements. To achieve this, we introduce a novel Weighted Prompt Manipulation (WPM) technique, which systematically modifies attention weights and text embeddings within diffusion models. By dynamically adjusting the importance of specific words, WPM enhances or suppresses their influence in the final generated image, leading to semantically richer and more contextually accurate visualizations. Our approach exploits diffusion models and large language models (LLMs) such as GPT in conjunction with existing poetry datasets, ensuring a comprehensive and structured methodology for improved image generation in the literary domain. To the best of our knowledge, this is the first attempt at integrating weighted prompt manipulation for enhancing imagery in poetic language.

</details>


### [28] [Sphere-GAN: a GAN-based Approach for Saliency Estimation in 360° Videos](https://arxiv.org/abs/2509.11948)
*Mahmoud Z. A. Wahba,Sara Baldoni,Federica Battisti*

Main category: cs.CV

TL;DR: Sphere-GAN是一个基于生成对抗网络和球面卷积的360度视频显著性检测模型，在公开数据集上表现优于现有最佳模型


<details>
  <summary>Details</summary>
Motivation: 随着沉浸式应用的成功，需要新的方法来处理360度图像视频并优化传输。显著性估计可以识别视觉相关区域，但现有算法大多针对2D内容，360度显著性估计研究较少

Method: 使用生成对抗网络（GAN）结合球面卷积来构建360度视频显著性检测模型

Result: 在公开的360度视频显著性数据集上进行广泛实验，结果显示Sphere-GAN在准确预测显著性图方面优于最先进的模型

Conclusion: Sphere-GAN为360度视频显著性检测提供了有效的解决方案，在性能上超越了现有方法

Abstract: The recent success of immersive applications is pushing the research community to define new approaches to process 360{\deg} images and videos and optimize their transmission. Among these, saliency estimation provides a powerful tool that can be used to identify visually relevant areas and, consequently, adapt processing algorithms. Although saliency estimation has been widely investigated for 2D content, very few algorithms have been proposed for 360{\deg} saliency estimation. Towards this goal, we introduce Sphere-GAN, a saliency detection model for 360{\deg} videos that leverages a Generative Adversarial Network with spherical convolutions. Extensive experiments were conducted using a public 360{\deg} video saliency dataset, and the results demonstrate that Sphere-GAN outperforms state-of-the-art models in accurately predicting saliency maps.

</details>


### [29] [Learning to Generate 4D LiDAR Sequences](https://arxiv.org/abs/2509.11959)
*Ao Liang,Youquan Liu,Yu Yang,Dongyue Lu,Linfeng Li,Lingdong Kong,Huaici Zhao,Wei Tsang Ooi*

Main category: cs.CV

TL;DR: LiDARCrafter是一个统一的框架，可以将自由形式的语言转换为可编辑的LiDAR序列，解决了4D LiDAR数据生成在可控性、时间稳定性和评估方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式世界模型在视频和基于占用的数据合成方面取得了进展，但LiDAR生成仍然未被充分探索，尽管它对于准确的3D感知至关重要。将生成扩展到4D LiDAR数据带来了可控性、时间稳定性和评估方面的挑战。

Method: 提出LiDARCrafter框架：1）将指令解析为以自我为中心的场景图；2）使用三分支扩散模型转换为对象布局、轨迹和形状；3）范围图像扩散模型生成初始扫描；4）自回归模块扩展为时间相干序列。支持对象级编辑如插入或重定位。

Result: 在nuScenes数据集上，LiDARCrafter实现了最先进的保真度、可控性和时间一致性。提供了EvalSuite基准测试套件，涵盖场景、对象和序列级别的度量标准。

Conclusion: LiDARCrafter为基于LiDAR的仿真和数据增强提供了基础，在LiDAR序列生成方面取得了突破性进展，解决了4D LiDAR数据生成的关键挑战。

Abstract: While generative world models have advanced video and occupancy-based data synthesis, LiDAR generation remains underexplored despite its importance for accurate 3D perception. Extending generation to 4D LiDAR data introduces challenges in controllability, temporal stability, and evaluation. We present LiDARCrafter, a unified framework that converts free-form language into editable LiDAR sequences. Instructions are parsed into ego-centric scene graphs, which a tri-branch diffusion model transforms into object layouts, trajectories, and shapes. A range-image diffusion model generates the initial scan, and an autoregressive module extends it into a temporally coherent sequence. The explicit layout design further supports object-level editing, such as insertion or relocation. To enable fair assessment, we provide EvalSuite, a benchmark spanning scene-, object-, and sequence-level metrics. On nuScenes, LiDARCrafter achieves state-of-the-art fidelity, controllability, and temporal consistency, offering a foundation for LiDAR-based simulation and data augmentation.

</details>


### [30] [Robust Concept Erasure in Diffusion Models: A Theoretical Perspective on Security and Robustness](https://arxiv.org/abs/2509.12024)
*Zixuan Fu,Yan Ren,Finn Carter,Chenyue Wen,Le Ku,Daheng Yu,Emily Davis,Bo Zhang*

Main category: cs.CV

TL;DR: SCORE是一个用于扩散模型概念擦除的新框架，通过对抗性独立性理论保证模型输出与擦除概念的统计独立性，在多个基准测试中表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成方面取得了巨大成功，但在隐私、公平性和安全性方面存在风险，需要能够擦除敏感或有害概念（如NSFW内容、私人信息、艺术风格）同时保持整体生成能力

Method: SCORE将概念擦除表述为对抗性独立性问题，最小化目标概念与生成输出之间的互信息，结合对抗优化、轨迹一致性和显著性驱动的微调

Result: 在Stable Diffusion和FLUX上的四个基准测试中，SCORE比现有方法（EraseAnything、ANT、MACE、ESD、UCE）擦除效果提升高达12.5%，同时保持相当或更好的图像质量

Conclusion: SCORE通过理论保证和实证验证，为扩散模型的安全和鲁棒概念擦除设立了新标准

Abstract: Diffusion models have achieved unprecedented success in image generation but pose increasing risks in terms of privacy, fairness, and security. A growing demand exists to \emph{erase} sensitive or harmful concepts (e.g., NSFW content, private individuals, artistic styles) from these models while preserving their overall generative capabilities. We introduce \textbf{SCORE} (Secure and Concept-Oriented Robust Erasure), a novel framework for robust concept removal in diffusion models. SCORE formulates concept erasure as an \emph{adversarial independence} problem, theoretically guaranteeing that the model's outputs become statistically independent of the erased concept. Unlike prior heuristic methods, SCORE minimizes the mutual information between a target concept and generated outputs, yielding provable erasure guarantees. We provide formal proofs establishing convergence properties and derive upper bounds on residual concept leakage. Empirically, we evaluate SCORE on Stable Diffusion and FLUX across four challenging benchmarks: object erasure, NSFW removal, celebrity face suppression, and artistic style unlearning. SCORE consistently outperforms state-of-the-art methods including EraseAnything, ANT, MACE, ESD, and UCE, achieving up to \textbf{12.5\%} higher erasure efficacy while maintaining comparable or superior image quality. By integrating adversarial optimization, trajectory consistency, and saliency-driven fine-tuning, SCORE sets a new standard for secure and robust concept erasure in diffusion models.

</details>


### [31] [RAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restoration](https://arxiv.org/abs/2509.12039)
*Zilong Zhang,Chujie Qin,Chunle Guo,Yong Zhang,Chao Xue,Ming-Ming Cheng,Chongyi Li*

Main category: cs.CV

TL;DR: RAM++是一个两阶段图像修复框架，通过自适应语义感知掩码、掩码属性传导和鲁棒特征正则化三个关键设计，实现了对可见、不可见、极端和混合退化的鲁棒、平衡和最先进的修复性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有退化导向方法在极端场景下的局限性（如退化与图像结构强耦合），以及跨任务性能不平衡、对已见退化过拟合、对未见退化泛化能力弱等常见挑战。

Method: 1) 自适应语义感知掩码(AdaSAM)：在语义丰富和纹理区域应用像素级掩码的预训练策略；2) 掩码属性传导(MAC)：选择性微调策略，调整贡献度高的层；3) 鲁棒特征正则化(RFR)：利用DINOv2的语义一致性和退化不变性表示。

Result: RAM++在可见、不可见、极端和混合退化情况下都实现了鲁棒、平衡和最先进的修复性能。

Conclusion: RAM++通过集成高级语义理解和低级纹理生成，实现了内容导向的鲁棒图像修复，解决了现有方法的多个关键局限性。

Abstract: This work presents Robust Representation Learning via Adaptive Mask (RAM++), a two-stage framework for all-in-one image restoration. RAM++ integrates high-level semantic understanding with low-level texture generation to achieve content-oriented robust restoration. It addresses the limitations of existing degradation-oriented methods in extreme scenarios (e.g., degradations strongly coupled with image structures). RAM++ also mitigates common challenges such as unbalanced performance across tasks, overfitting to seen degradations, and weak generalization to unseen ones through three key designs: 1) Adaptive Semantic-Aware Mask (AdaSAM): a pretraining strategy that applies pixel-level masks to semantically rich and textured regions. This design enables the network to learn both generative priors and image content priors from various degradations. 2) Mask Attribute Conductance (MAC): a selective fine-tuning strategy that adjusts the layers with higher contributions to bridge the integrity gap between masked pretraining and full-image fine-tuning while retaining learned priors. 3) Robust Feature Regularization (RFR): a strategy that leverages DINOv2's semantically consistent and degradation-invariant representations, together with efficient feature fusion, to achieve faithful and semantically coherent restoration. With these designs, RAM++ achieves robust, well-balanced, and state-of-the-art performance across seen, unseen, extreme, and mixed degradations. Our code and model will be released at https://github.com/DragonisCV/RAM

</details>


### [32] [Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing](https://arxiv.org/abs/2509.12040)
*Bingyu Li,Haocheng Dong,Da Zhang,Zhiyuan Zhao,Junyu Gao,Xuelong Li*

Main category: cs.CV

TL;DR: 这篇论文提出了RSKT-Seg框架，专门解决远程感知图像的开放词汇分割问题，通过新的评测基准和三个核心模块，在性能和速度上都超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 远程感知图像开放词汇分割任务(OVRSIS)缺乏统一评测基准，且自然图像与远感图像存在域间距，需要专门的解决方案。

Method: 首先建立OVRSISBench评测基准，然后提出RSKT-Seg框架，包含：1）多方向成本地图聚合模块(RS-CMA)捐捕旋转不变特征；2）高效成本地图融合变换器(RS-Fusion)；3）远感知识转移模块(RS-Transfer)注入领域知识。

Result: 在基准上评测显示，RSKT-Seg比强基线提升+3.8 mIoU和+5.9 mACC，同时推理速度提升2倍。

Conclusion: 该方法有效解决了远感图像开放词汇分割的挑战，为该领域提供了可靠的评测标准和高效的解决方案。

Abstract: Open-Vocabulary Remote Sensing Image Segmentation (OVRSIS), an emerging task that adapts Open-Vocabulary Segmentation (OVS) to the remote sensing (RS) domain, remains underexplored due to the absence of a unified evaluation benchmark and the domain gap between natural and RS images. To bridge these gaps, we first establish a standardized OVRSIS benchmark (\textbf{OVRSISBench}) based on widely-used RS segmentation datasets, enabling consistent evaluation across methods. Using this benchmark, we comprehensively evaluate several representative OVS/OVRSIS models and reveal their limitations when directly applied to remote sensing scenarios. Building on these insights, we propose \textbf{RSKT-Seg}, a novel open-vocabulary segmentation framework tailored for remote sensing. RSKT-Seg integrates three key components: (1) a Multi-Directional Cost Map Aggregation (RS-CMA) module that captures rotation-invariant visual cues by computing vision-language cosine similarities across multiple directions; (2) an Efficient Cost Map Fusion (RS-Fusion) transformer, which jointly models spatial and semantic dependencies with a lightweight dimensionality reduction strategy; and (3) a Remote Sensing Knowledge Transfer (RS-Transfer) module that injects pre-trained knowledge and facilitates domain adaptation via enhanced upsampling. Extensive experiments on the benchmark show that RSKT-Seg consistently outperforms strong OVS baselines by +3.8 mIoU and +5.9 mACC, while achieving 2x faster inference through efficient aggregation. Our code is \href{https://github.com/LiBingyu01/RSKT-Seg}{\textcolor{blue}{here}}.

</details>


### [33] [Layout-Conditioned Autoregressive Text-to-Image Generation via Structured Masking](https://arxiv.org/abs/2509.12046)
*Zirui Zheng,Takashi Isobe,Tong Shen,Xu Jia,Jianbin Zhao,Xiaomin Li,Mengmeng Ge,Baolu Li,Qinghe Wang,Dong Li,Dong Zhou,Yunzhi Zhuge,Huchuan Lu,Emad Barsoum*

Main category: cs.CV

TL;DR: SMARLI是一个新颖的布局到图像生成框架，通过结构化掩码策略将空间布局约束有效整合到自回归图像生成中，解决了布局条件稀疏和特征纠缠的问题。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在图像生成方面取得了显著成功，但扩展到布局条件生成仍然具有挑战性，主要原因是布局条件的稀疏性和特征纠缠风险。

Method: 采用结构化掩码策略来控制全局提示、布局和图像token之间的交互，防止不同区域与其描述之间的错误关联；同时使用基于组相对策略优化的后训练方案和专门设计的布局奖励函数。

Result: 实验结果表明SMARLI能够无缝整合布局token与文本和图像token，在不影响生成质量的情况下实现卓越的布局感知控制，同时保持自回归模型的结构简单性和生成效率。

Conclusion: SMARLI框架成功解决了布局条件图像生成的挑战，为自回归模型提供了有效的布局控制能力，在保持模型简洁性的同时实现了高质量的布局感知图像生成。

Abstract: While autoregressive (AR) models have demonstrated remarkable success in image generation, extending them to layout-conditioned generation remains challenging due to the sparse nature of layout conditions and the risk of feature entanglement. We present Structured Masking for AR-based Layout-to-Image (SMARLI), a novel framework for layoutto-image generation that effectively integrates spatial layout constraints into AR-based image generation. To equip AR model with layout control, a specially designed structured masking strategy is applied to attention computation to govern the interaction among the global prompt, layout, and image tokens. This design prevents mis-association between different regions and their descriptions while enabling sufficient injection of layout constraints into the generation process. To further enhance generation quality and layout accuracy, we incorporate Group Relative Policy Optimization (GRPO) based post-training scheme with specially designed layout reward functions for next-set-based AR models. Experimental results demonstrate that SMARLI is able to seamlessly integrate layout tokens with text and image tokens without compromising generation quality. It achieves superior layoutaware control while maintaining the structural simplicity and generation efficiency of AR models.

</details>


### [34] [AvatarSync: Rethinking Talking-Head Animation through Autoregressive Perspective](https://arxiv.org/abs/2509.12052)
*Yuchen Deng,Xiuyang Wu,Hai-Tao Zheng,Suiyang Zhang,Yi He,Yuxing Han*

Main category: cs.CV

TL;DR: AvatarSync是一个基于音素表示的自回归框架，通过两阶段生成策略（关键帧生成和帧间插值）从单张参考图像生成高质量、可控的说话头部动画，解决了现有方法的闪烁、身份漂移和推理速度慢的问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于GAN和扩散模型的说话头部动画方法存在的帧间闪烁、身份漂移和推理速度慢等限制，提供更适用于实际应用的高质量动画生成方案。

Method: 采用两阶段生成策略：1）面部关键帧生成阶段，通过音素到视觉映射和文本-帧因果注意力掩码生成语义关键帧；2）帧间插值阶段，使用时序感知自适应策略和选择性状态空间模型确保时序一致性和视觉平滑度。

Result: 在视觉保真度、时序一致性和计算效率方面优于现有说话头部动画方法，提供了可扩展和可控的解决方案。

Conclusion: AvatarSync通过创新的自回归框架和两阶段生成策略，成功解决了说话头部动画中的关键问题，为实际应用提供了高效可靠的生成方案。

Abstract: Existing talking-head animation approaches based on Generative Adversarial Networks (GANs) or diffusion models often suffer from inter-frame flicker, identity drift, and slow inference. These limitations inherent to their video generation pipelines restrict their suitability for applications. To address this, we introduce AvatarSync, an autoregressive framework on phoneme representations that generates realistic and controllable talking-head animations from a single reference image, driven directly text or audio input. In addition, AvatarSync adopts a two-stage generation strategy, decoupling semantic modeling from visual dynamics, which is a deliberate "Divide and Conquer" design. The first stage, Facial Keyframe Generation (FKG), focuses on phoneme-level semantic representation by leveraging the many-to-one mapping from text or audio to phonemes. A Phoneme-to-Visual Mapping is constructed to anchor abstract phonemes to character-level units. Combined with a customized Text-Frame Causal Attention Mask, the keyframes are generated. The second stage, inter-frame interpolation, emphasizes temporal coherence and visual smoothness. We introduce a timestamp-aware adaptive strategy based on a selective state space model, enabling efficient bidirectional context reasoning. To support deployment, we optimize the inference pipeline to reduce latency without compromising visual fidelity. Extensive experiments show that AvatarSync outperforms existing talking-head animation methods in visual fidelity, temporal consistency, and computational efficiency, providing a scalable and controllable solution.

</details>


### [35] [FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptation](https://arxiv.org/abs/2509.12105)
*Bernardo Forni,Gabriele Lombardi,Federico Pozzi,Mirco Planamente*

Main category: cs.CV

TL;DR: FS-SAM2是一个基于Segment Anything Model 2的少样本语义分割方法，通过LoRA微调和重用途视频能力，在多个数据集上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有的少样本分割方法需要在大规模数据集上从头训练额外模块，计算成本高。SAM2作为基础分割模型具有强大的零样本能力，但需要适应少样本任务。

Method: 将SAM2的视频能力直接重用于少样本任务，应用低秩适应（LoRA）处理标准数据集的多样性图像，仅微调少量参数进行元训练。

Result: 在PASCAL-5i、COCO-20i和FSS-1000数据集上取得了显著结果，推理时具有优秀的计算效率。

Conclusion: FS-SAM2有效利用了SAM2的强大分割能力，通过少量参数微调实现了优异的少样本分割性能，支持任意K-shot配置。

Abstract: Few-shot semantic segmentation has recently attracted great attention. The goal is to develop a model capable of segmenting unseen classes using only a few annotated samples. Most existing approaches adapt a pre-trained model by training from scratch an additional module. Achieving optimal performance with these approaches requires extensive training on large-scale datasets. The Segment Anything Model 2 (SAM2) is a foundational model for zero-shot image and video segmentation with a modular design. In this paper, we propose a Few-Shot segmentation method based on SAM2 (FS-SAM2), where SAM2's video capabilities are directly repurposed for the few-shot task. Moreover, we apply a Low-Rank Adaptation (LoRA) to the original modules in order to handle the diverse images typically found in standard datasets, unlike the temporally connected frames used in SAM2's pre-training. With this approach, only a small number of parameters is meta-trained, which effectively adapts SAM2 while benefiting from its impressive segmentation performance. Our method supports any K-shot configuration. We evaluate FS-SAM2 on the PASCAL-5$^i$, COCO-20$^i$ and FSS-1000 datasets, achieving remarkable results and demonstrating excellent computational efficiency during inference. Code is available at https://github.com/fornib/FS-SAM2

</details>


### [36] [LoRA-fine-tuned Large Vision Models for Automated Assessment of Post-SBRT Lung Injury](https://arxiv.org/abs/2509.12155)
*M. Bolhassani,B. Veasey,E. Daugherty,S. Keltner,N. Kumar,N. Dunlap,A. Amini*

Main category: cs.CV

TL;DR: LoRA微调方法在DinoV2和SwinV2视觉模型上诊断放射性肺损伤，相比全微调和无微调方法，在计算效率和性能方面表现更优


<details>
  <summary>Details</summary>
Motivation: 研究低秩适应(LoRA)方法在大型视觉模型微调中的有效性，特别是在诊断SBRT治疗后放射性肺损伤的医学影像分析任务中

Method: 使用DinoV2和SwinV2模型，比较LoRA、传统全微调和仅推理三种方法。采用50mm³和75mm³两种尺寸的裁剪图像，并探索2D模型适应3D数据的不同技术

Result: LoRA方法在达到与传统微调相当或更优性能的同时，显著减少了计算成本和训练时间，所需可训练参数更少

Conclusion: LoRA是一种高效且鲁棒的微调方法，特别适用于医学影像分析任务，能够在保持性能的同时大幅降低计算资源需求

Abstract: This study investigates the efficacy of Low-Rank Adaptation (LoRA) for fine-tuning large Vision Models, DinoV2 and SwinV2, to diagnose Radiation-Induced Lung Injury (RILI) from X-ray CT scans following Stereotactic Body Radiation Therapy (SBRT). To evaluate the robustness and efficiency of this approach, we compare LoRA with traditional full fine-tuning and inference-only (no fine-tuning) methods. Cropped images of two sizes (50 mm3 and 75 mm3), centered at the treatment isocenter, in addition to different adaptation techniques for adapting the 2D LVMs for 3D data were used to determine the sensitivity of the models to spatial context. Experimental results show that LoRA achieves comparable or superior performance to traditional fine-tuning while significantly reducing computational costs and training times by requiring fewer trainable parameters.

</details>


### [37] [OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling](https://arxiv.org/abs/2509.12201)
*Yang Zhou,Yifan Wang,Jianjun Zhou,Wenzheng Chang,Haoyu Guo,Zizun Li,Kaijing Ma,Xinyue Li,Yating Wang,Haoyi Zhu,Mingyu Liu,Dingning Liu,Jiange Yang,Zhoujie Fu,Junyi Chen,Chunhua Shen,Jiangmiao Pang,Kaipeng Zhang,Tong He*

Main category: cs.CV

TL;DR: OmniWorld是一个大规模多模态4D世界建模数据集，包含新收集的OmniWorld-Game数据和多个公共数据集，旨在解决现有数据在动态复杂性、多领域多样性和时空标注方面的不足，为4D重建和视频生成等任务提供训练和评估资源。


<details>
  <summary>Details</summary>
Motivation: 现有4D世界建模领域缺乏高质量数据，现有数据集在动态复杂性、多领域多样性和时空标注方面存在不足，限制了通用4D世界模型的发展。

Method: 引入OmniWorld数据集，包括新收集的OmniWorld-Game数据集（提供更丰富的模态覆盖、更大规模和更真实的动态交互）和多个精选的公共数据集，并基于此建立基准测试。

Result: 基准测试暴露了当前最先进方法在建模复杂4D环境方面的局限性，在OmniWorld上微调现有方法显著提升了4D重建和视频生成任务的性能。

Conclusion: OmniWorld作为强大的训练和评估资源，将加速通用4D世界模型的发展，推动机器对物理世界的整体理解。

Abstract: The field of 4D world modeling - aiming to jointly capture spatial geometry and temporal dynamics - has witnessed remarkable progress in recent years, driven by advances in large-scale generative models and multimodal learning. However, the development of truly general 4D world models remains fundamentally constrained by the availability of high-quality data. Existing datasets and benchmarks often lack the dynamic complexity, multi-domain diversity, and spatial-temporal annotations required to support key tasks such as 4D geometric reconstruction, future prediction, and camera-control video generation. To address this gap, we introduce OmniWorld, a large-scale, multi-domain, multi-modal dataset specifically designed for 4D world modeling. OmniWorld consists of a newly collected OmniWorld-Game dataset and several curated public datasets spanning diverse domains. Compared with existing synthetic datasets, OmniWorld-Game provides richer modality coverage, larger scale, and more realistic dynamic interactions. Based on this dataset, we establish a challenging benchmark that exposes the limitations of current state-of-the-art (SOTA) approaches in modeling complex 4D environments. Moreover, fine-tuning existing SOTA methods on OmniWorld leads to significant performance gains across 4D reconstruction and video generation tasks, strongly validating OmniWorld as a powerful resource for training and evaluation. We envision OmniWorld as a catalyst for accelerating the development of general-purpose 4D world models, ultimately advancing machines' holistic understanding of the physical world.

</details>


### [38] [LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence](https://arxiv.org/abs/2509.12203)
*Zixin Yin,Xili Dai,Duomin Wang,Xianfang Zeng,Lionel M. Ni,Gang Yu,Heung-Yeung Shum*

Main category: cs.CV

TL;DR: LazyDrag是首个基于拖拽的多模态扩散变换器图像编辑方法，通过显式对应映射替代隐式点匹配，实现了稳定全强度反演，无需测试时优化，统一了几何控制与文本引导。


<details>
  <summary>Details</summary>
Motivation: 当前基于拖拽的编辑方法依赖注意力机制的隐式点匹配，导致反演强度减弱和昂贵的测试时优化，限制了扩散模型的高保真修复和文本引导创作能力。

Method: 从用户拖拽输入生成显式对应映射作为可靠参考来增强注意力控制，实现稳定的全强度反演过程，消除测试时优化需求。

Result: 在DragBench评估中，LazyDrag在拖拽精度和感知质量上优于基线方法，支持多轮工作流和同时移动缩放操作，能够实现复杂编辑如开口修复、生成新物体等。

Conclusion: LazyDrag不仅建立了新的最先进性能，还为编辑范式开辟了新途径，统一了几何精确控制与文本引导能力。

Abstract: The reliance on implicit point matching via attention has become a core bottleneck in drag-based editing, resulting in a fundamental compromise on weakened inversion strength and costly test-time optimization (TTO). This compromise severely limits the generative capabilities of diffusion models, suppressing high-fidelity inpainting and text-guided creation. In this paper, we introduce LazyDrag, the first drag-based image editing method for Multi-Modal Diffusion Transformers, which directly eliminates the reliance on implicit point matching. In concrete terms, our method generates an explicit correspondence map from user drag inputs as a reliable reference to boost the attention control. This reliable reference opens the potential for a stable full-strength inversion process, which is the first in the drag-based editing task. It obviates the necessity for TTO and unlocks the generative capability of models. Therefore, LazyDrag naturally unifies precise geometric control with text guidance, enabling complex edits that were previously out of reach: opening the mouth of a dog and inpainting its interior, generating new objects like a ``tennis ball'', or for ambiguous drags, making context-aware changes like moving a hand into a pocket. Additionally, LazyDrag supports multi-round workflows with simultaneous move and scale operations. Evaluated on the DragBench, our method outperforms baselines in drag accuracy and perceptual quality, as validated by VIEScore and human evaluation. LazyDrag not only establishes new state-of-the-art performance, but also paves a new way to editing paradigms.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [39] [Spectral Bottleneck in Deep Neural Networks: Noise is All You Need](https://arxiv.org/abs/2509.09719)
*Hemanth Chandravamsi,Dhanush V. Shenoy,Itay Zinn,Shimon Pisnoy,Steven H. Frankel*

Main category: eess.AS

TL;DR: 提出了WINNER权重扰动初始化方法，通过自适应高斯噪声扰动解决SIREN网络在处理高频主导信号时的频谱瓶颈问题


<details>
  <summary>Details</summary>
Motivation: 深度神经网络存在频谱学习偏差，低频成分先被学习，高频成分学习缓慢。当目标信号缺乏低频成分且以高频为主时，会出现频谱瓶颈，导致模型无法有效重建信号

Method: 提出WINNER权重扰动方案，在均匀初始化权重基础上添加高斯噪声，噪声尺度根据目标信号的频谱质心自适应确定，从而控制网络激活谱和神经正切核特征基

Result: 该方法不仅解决了频谱瓶颈问题，还实现了更快的收敛速度和更高的表示精度，在音频拟合任务中超越现有方法，在图像拟合和去噪任务中也有显著提升

Conclusion: WINNER方法为计算机视觉和科学机器学习中的自适应权重初始化策略开辟了新方向，能够有效处理任意频率内容的目标信号

Abstract: Deep neural networks are known to exhibit a spectral learning bias, wherein low-frequency components are learned early in training, while high-frequency modes emerge more gradually in later epochs. However, when the target signal lacks low-frequency components and is dominated by broadband high frequencies, training suffers from a 'spectral bottleneck', and the model fails to reconstruct the entire signal, including the frequency components that lie within the network's representational capacity. We examine such a scenario in the context of implicit neural representations (INRs) with sinusoidal representation networks (SIRENs), focusing on the challenge of fitting high-frequency-dominant signals that are susceptible to spectral bottleneck. To effectively fit any target signal irrespective of it's frequency content, we propose a generalized target-aware 'weight perturbation scheme' (WINNER - weight initialization with noise for neural representations) for network initialization. The scheme perturbs uniformly initialized weights with Gaussian noise, where the noise scales are adaptively determined by the spectral centroid of the target signal. We show that the noise scales can provide control over the spectra of network activations and the eigenbasis of the empirical neural tangent kernel. This method not only addresses the spectral bottleneck but also yields faster convergence and with improved representation accuracy, outperforming state-of-the-art approaches in audio fitting and achieving notable gains in image fitting and denoising tasks. Beyond signal reconstruction, our approach opens new directions for adaptive weight initialization strategies in computer vision and scientific machine learning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [40] [Robustifying Diffusion-Denoised Smoothing Against Covariate Shift](https://arxiv.org/abs/2509.10913)
*Ali Hedayatnia,Mostafa Tavassolipour,Babak Nadjar Araabi,Abdol-Hossein Vahabie*

Main category: cs.LG

TL;DR: 本文提出了一种新方法来解决扩散去噪平滑中的协变量偏移问题，通过在去噪扩散模型的添加噪声上使用对抗目标函数来训练基础分类器，显著提高了认证精度并在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法中使用预训练去噪扩散模型作为去噪器会引入协变量偏移，这是由于对添加噪声的错误估计导致的，这会降低平滑分类器的性能。

Method: 提出了一种新颖的对抗目标函数，专注于去噪扩散模型的添加噪声，训练基础分类器使其对去噪器引入的协变量偏移具有鲁棒性。

Result: 在MNIST、CIFAR-10和ImageNet三个标准分类基准测试中显著提高了认证精度，实现了l2对抗扰动的最先进性能。

Conclusion: 通过针对去噪扩散模型添加噪声的对抗训练方法，有效解决了协变量偏移问题，为随机平滑方法提供了更鲁棒的解决方案。

Abstract: Randomized smoothing is a well-established method for achieving certified robustness against l2-adversarial perturbations. By incorporating a denoiser before the base classifier, pretrained classifiers can be seamlessly integrated into randomized smoothing without significant performance degradation. Among existing methods, Diffusion Denoised Smoothing - where a pretrained denoising diffusion model serves as the denoiser - has produced state-of-the-art results. However, we show that employing a denoising diffusion model introduces a covariate shift via misestimation of the added noise, ultimately degrading the smoothed classifier's performance. To address this issue, we propose a novel adversarial objective function focused on the added noise of the denoising diffusion model. This approach is inspired by our understanding of the origin of the covariate shift. Our goal is to train the base classifier to ensure it is robust against the covariate shift introduced by the denoiser. Our method significantly improves certified accuracy across three standard classification benchmarks - MNIST, CIFAR-10, and ImageNet - achieving new state-of-the-art performance in l2-adversarial perturbations. Our implementation is publicly available at https://github.com/ahedayat/Robustifying-DDS-Against-Covariate-Shift

</details>


### [41] [SpeCa: Accelerating Diffusion Transformers with Speculative Feature Caching](https://arxiv.org/abs/2509.11628)
*Jiacheng Liu,Chang Zou,Yuanhuiyi Lyu,Fei Ren,Shaobo Wang,Kaixin Li,Linfeng Zhang*

Main category: cs.LG

TL;DR: SpeCa是一个基于推测采样的扩散模型加速框架，通过预测中间特征和验证机制实现6-7倍加速，同时保持生成质量


<details>
  <summary>Details</summary>
Motivation: 扩散模型计算需求大，存在严格的时间依赖性和计算密集型前向传递问题，限制了实时应用

Method: 引入推测采样预测后续时间步特征，采用参数无关验证机制评估预测可靠性，并实现样本自适应计算分配

Result: 在FLUX上实现6.34倍加速（质量下降5.5%），DiT上7.3倍加速保持保真度，HunyuanVideo上6.1倍加速获得79.84% VBench分数

Conclusion: SpeCa为扩散模型推理建立了新的高效范式，在激进加速比下仍能保持生成质量，验证机制开销极小

Abstract: Diffusion models have revolutionized high-fidelity image and video synthesis, yet their computational demands remain prohibitive for real-time applications. These models face two fundamental challenges: strict temporal dependencies preventing parallelization, and computationally intensive forward passes required at each denoising step. Drawing inspiration from speculative decoding in large language models, we present SpeCa, a novel 'Forecast-then-verify' acceleration framework that effectively addresses both limitations. SpeCa's core innovation lies in introducing Speculative Sampling to diffusion models, predicting intermediate features for subsequent timesteps based on fully computed reference timesteps. Our approach implements a parameter-free verification mechanism that efficiently evaluates prediction reliability, enabling real-time decisions to accept or reject each prediction while incurring negligible computational overhead. Furthermore, SpeCa introduces sample-adaptive computation allocation that dynamically modulates resources based on generation complexity, allocating reduced computation for simpler samples while preserving intensive processing for complex instances. Experiments demonstrate 6.34x acceleration on FLUX with minimal quality degradation (5.5% drop), 7.3x speedup on DiT while preserving generation fidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The verification mechanism incurs minimal overhead (1.67%-3.5% of full inference costs), establishing a new paradigm for efficient diffusion model inference while maintaining generation quality even at aggressive acceleration ratios. Our codes have been released in Github: \textbf{https://github.com/Shenyi-Z/Cache4Diffusion}

</details>


### [42] [DRAG: Data Reconstruction Attack using Guided Diffusion](https://arxiv.org/abs/2509.11724)
*Wa-Kin Lei,Jun-Cheng Chen,Shang-Tse Chen*

Main category: cs.LG

TL;DR: 基于导向扩散模型的数据重构攻击方法，利用领先知识从视觉基础模型的中间表征重构高保真图像，显示分开推理中的严重隐私风险


<details>
  <summary>Details</summary>
Motivation: 随着大型基础模型的兴起，分开推理成为普遍计算范式，但现有的数据重构攻击主要集中在小型CNN分类模型上，大型基础模型在分开推理场景中的隐私风险很少被探索

Method: 提出了一种基于导向扩散模型的新题数据重构攻击方法，利用在大规模数据集上领先训练的潜在扩散模型(LDM)中嵌入的丰富领先知识，在LDM学习到的图像领先上进行迭代重构

Result: 实验结果显示，该方法在从视觉基础模型的深层中间表征重构数据时，在质量和数量上都显著超过了现有最先进方法，能够生成高保真度的图像

Conclusion: 该研究结果强调了在分开推理场景中为大型模型开发更稳健的隐私保护机制的紧迫性

Abstract: With the rise of large foundation models, split inference (SI) has emerged as a popular computational paradigm for deploying models across lightweight edge devices and cloud servers, addressing data privacy and computational cost concerns. However, most existing data reconstruction attacks have focused on smaller CNN classification models, leaving the privacy risks of foundation models in SI settings largely unexplored. To address this gap, we propose a novel data reconstruction attack based on guided diffusion, which leverages the rich prior knowledge embedded in a latent diffusion model (LDM) pre-trained on a large-scale dataset. Our method performs iterative reconstruction on the LDM's learned image prior, effectively generating high-fidelity images resembling the original data from their intermediate representations (IR). Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art methods, both qualitatively and quantitatively, in reconstructing data from deep-layer IRs of the vision foundation model. The results highlight the urgent need for more robust privacy protection mechanisms for large models in SI scenarios. Code is available at: https://github.com/ntuaislab/DRAG.

</details>


### [43] [Early Detection of Branched Broomrape (Phelipanche ramosa) Infestation in Tomato Crops Using Leaf Spectral Analysis and Machine Learning](https://arxiv.org/abs/2509.12074)
*Mohammadreza Narimani,Alireza Pourreza,Ali Moghimi,Parastoo Farajpoor,Hamid Jafarbiglu,Mohsen B. Mesgaran*

Main category: cs.LG

TL;DR: 通过叶面光谱反射和集成机器学习方法，在585增温度积温时实现了早期检测樱桃伯呼藏莫藏的高准确率，为针对性干预提供支持


<details>
  <summary>Details</summary>
Motivation: 分支怡手棕是一种严重威胁番茄生产的寄生杂草，需要在可见症状出现前实现早期检测，以减少产量损失

Method: 采用叶面光谱反射技术(400-2500nm)和集成机器学习算法(Random Forest、XGBoost、SVM、Naive Bayes)，通过增温度积温定义植案生长阶段

Result: 在585增温度积温时达到89%的检测准确率，感染和非感染植案的召回率分别为0.86和0.93，后期准确率下降至69%

Conclusion: 近端感知技术结合集成学习能够在可见症状出现前及时检测分支怡手棕，为针对性干预提供了有效手段

Abstract: Branched broomrape (Phelipanche ramosa) is a chlorophyll-deficient parasitic weed that threatens tomato production by extracting nutrients from the host. We investigate early detection using leaf-level spectral reflectance (400-2500 nm) and ensemble machine learning. In a field experiment in Woodland, California, we tracked 300 tomato plants across growth stages defined by growing degree days (GDD). Leaf reflectance was acquired with a portable spectrometer and preprocessed (band denoising, 1 nm interpolation, Savitzky-Golay smoothing, correlation-based band reduction). Clear class differences were observed near 1500 nm and 2000 nm water absorption features, consistent with reduced leaf water content in infected plants at early stages. An ensemble combining Random Forest, XGBoost, SVM with RBF kernel, and Naive Bayes achieved 89% accuracy at 585 GDD, with recalls of 0.86 (infected) and 0.93 (noninfected). Accuracy declined at later stages (e.g., 69% at 1568 GDD), likely due to senescence and weed interference. Despite the small number of infected plants and environmental confounders, results show that proximal sensing with ensemble learning enables timely detection of broomrape before canopy symptoms are visible, supporting targeted interventions and reduced yield losses.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [44] [Maestro: Self-Improving Text-to-Image Generation via Agent Orchestration](https://arxiv.org/abs/2509.10704)
*Xingchen Wan,Han Zhou,Ruoxi Sun,Hootan Nakhost,Ke Jiang,Rajarishi Sinha,Sercan Ö. Arık*

Main category: cs.AI

TL;DR: Maestro是一个自演化的文本到图像生成系统，通过多模态LLM代理进行自我批判和演化，无需人工干预即可自动改进生成图像质量


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像模型高度依赖人工干预和迭代提示工程，存在可用性挑战，需要手动处理不明确的提示

Method: 采用两个关键创新：1) 自我批判 - 多模态LLM代理作为评论家识别图像弱点并提供可解释的编辑信号；2) 自我演化 - 使用MLLM作为评判者进行迭代图像比较，演化创意提示

Result: 在复杂文本到图像任务上的广泛实验表明，Maestro显著提高了图像质量，优于初始提示和最先进的自动化方法，且效果随MLLM组件进步而提升

Conclusion: 该工作提供了一个强大、可解释且有效的自改进文本到图像生成途径

Abstract: Text-to-image (T2I) models, while offering immense creative potential, are highly reliant on human intervention, posing significant usability challenges that often necessitate manual, iterative prompt engineering over often underspecified prompts. This paper introduces Maestro, a novel self-evolving image generation system that enables T2I models to autonomously self-improve generated images through iterative evolution of prompts, using only an initial prompt. Maestro incorporates two key innovations: 1) self-critique, where specialized multimodal LLM (MLLM) agents act as 'critics' to identify weaknesses in generated images, correct for under-specification, and provide interpretable edit signals, which are then integrated by a 'verifier' agent while preserving user intent; and 2) self-evolution, utilizing MLLM-as-a-judge for head-to-head comparisons between iteratively generated images, eschewing problematic images, and evolving creative prompt candidates that align with user intents. Extensive experiments on complex T2I tasks using black-box models demonstrate that Maestro significantly improves image quality over initial prompts and state-of-the-art automated methods, with effectiveness scaling with more advanced MLLM components. This work presents a robust, interpretable, and effective pathway towards self-improving T2I generation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [45] [DreamNav: A Trajectory-Based Imaginative Framework for Zero-Shot Vision-and-Language Navigation](https://arxiv.org/abs/2509.11197)
*Yunheng Wang,Yuetong Fang,Taowen Wang,Yixiao Feng,Yawen Tan,Shuning Zhang,Peiran Liu,Yiding Ji,Renjing Xu*

Main category: cs.RO

TL;DR: DreamNav是一个零样本视觉语言导航方法，通过视角校正、轨迹级规划和主动想象来解决现有方法的高成本、动作语义不对齐和短视规划问题，在VLN-CE和真实世界测试中取得了新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本VLN方法依赖昂贵的感知和被动场景理解，将控制简化为点级选择，导致部署成本高、动作语义不对齐和规划短视。

Method: 提出DreamNav方法，包含三个核心组件：(1)EgoView Corrector对齐视角并稳定自我中心感知；(2)Trajectory Predictor进行全局轨迹级规划；(3)Imagination Predictor赋予智能体主动思考能力。

Result: 在VLN-CE和真实世界测试中，DreamNav在SR和SPL指标上分别比最强的自我中心基线高出7.49%和18.15%，建立了新的零样本SOTA。

Conclusion: 这是第一个统一轨迹级规划和主动想象的零样本VLN方法，仅使用自我中心输入，为解决真实世界导航任务提供了有效方案。

Abstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE), which links language instructions to perception and control in the real world, is a core capability of embodied robots. Recently, large-scale pretrained foundation models have been leveraged as shared priors for perception, reasoning, and action, enabling zero-shot VLN without task-specific training. However, existing zero-shot VLN methods depend on costly perception and passive scene understanding, collapsing control to point-level choices. As a result, they are expensive to deploy, misaligned in action semantics, and short-sighted in planning. To address these issues, we present DreamNav that focuses on the following three aspects: (1) for reducing sensory cost, our EgoView Corrector aligns viewpoints and stabilizes egocentric perception; (2) instead of point-level actions, our Trajectory Predictor favors global trajectory-level planning to better align with instruction semantics; and (3) to enable anticipatory and long-horizon planning, we propose an Imagination Predictor to endow the agent with proactive thinking capability. On VLN-CE and real-world tests, DreamNav sets a new zero-shot state-of-the-art (SOTA), outperforming the strongest egocentric baseline with extra information by up to 7.49\% and 18.15\% in terms of SR and SPL metrics. To our knowledge, this is the first zero-shot VLN method to unify trajectory-level planning and active imagination while using only egocentric inputs.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [46] [Data-driven Smile Design: Personalized Dental Aesthetics Outcomes Using Deep Learning](https://arxiv.org/abs/2509.12001)
*Marcus Lin,Jennifer Lai*

Main category: eess.IV

TL;DR: 这篇论文提出了一种整合AI、大数据和识别技术的系统，用于自动化笑容设计过程，解决传统手工方法对医生专业知识的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 传统笑容设计依赖牙医专业知识和手工操作，存在效果不稳定问题，而现有数字技术仍受到实施者偏见和训练数据限制的影响。

Method: 研究提出了一个综合系统，包含面部特征提取模块和图像生成模块，通过AI、大数据和识别技术自动化笑容设计过程。

Result: 该系统能够让经验丰富和经验不足的牙医都能轻松生成美观的笑容设计，最大限度减少了主观偏见的影响。

Conclusion: 这种数字化笑容设计系统有望提高牙科实践中的美学效果，未来可通过虚拟现实/增强现实技术进行实时预览，所收集的数据也可用于美学偏好分析。

Abstract: A healthy smile plays a significant role in functional as well as esthetic considerations, improving confidence. It is difficult for dental professionals to strike a balance between esthetic requirements and functional requirements. Traditional smile design has had heavy reliance on dentist expertise and used plaster models and hand drawings, raising questions about the outcome for patients. Digital technology, led by Dr. Christian Coachman in 2007, allows photographic and videographic assessments, enabling improved intercommunication among specialists and patients. Advances in artificial intelligence (AI) and big data have supported analysis of facial features and development of personalized smile designs in the last few years. Outputs are, however, susceptible to practitioner bias or limitations of training data, and may be suboptimal for individual users. The study presented here suggests a comprehensive system integrating AI, big data, and recognition technologies to automate the smile design process so that both experienced and inexperienced dentists can generate pleasing aesthetics with ease. The system has a Facial Feature Extraction Module and an Image Generation Module, serving diverse practitioner and patient needs. User data can be incorporated in future research for design optimization and testing of virtual and augmented reality for real-time previewing. Data gathered can also be employed in aesthetic preference analyses, which can enhance our knowledge of smile design in dental practice.

</details>
