{"id": "2507.15979", "pdf": "https://arxiv.org/pdf/2507.15979", "abs": "https://arxiv.org/abs/2507.15979", "authors": ["Marcel C. B\u00fchler", "Ye Yuan", "Xueting Li", "Yangyi Huang", "Koki Nagano", "Umar Iqbal"], "title": "Dream, Lift, Animate: From Single Images to Animatable Gaussian Avatars", "categories": ["cs.GR", "cs.AI"], "comment": null, "summary": "We introduce Dream, Lift, Animate (DLA), a novel framework that reconstructs animatable 3D human avatars from a single image. This is achieved by leveraging multi-view generation, 3D Gaussian lifting, and pose-aware UV-space mapping of 3D Gaussians. Given an image, we first dream plausible multi-views using a video diffusion model, capturing rich geometric and appearance details. These views are then lifted into unstructured 3D Gaussians. To enable animation, we propose a transformer-based encoder that models global spatial relationships and projects these Gaussians into a structured latent representation aligned with the UV space of a parametric body model. This latent code is decoded into UV-space Gaussians that can be animated via body-driven deformation and rendered conditioned on pose and viewpoint. By anchoring Gaussians to the UV manifold, our method ensures consistency during animation while preserving fine visual details. DLA enables real-time rendering and intuitive editing without requiring post-processing. Our method outperforms state-of-the-art approaches on ActorsHQ and 4D-Dress datasets in both perceptual quality and photometric accuracy. By combining the generative strengths of video diffusion models with a pose-aware UV-space Gaussian mapping, DLA bridges the gap between unstructured 3D representations and high-fidelity, animation-ready avatars.", "AI": {"tldr": "DLA\u6846\u67b6\u901a\u8fc7\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u591a\u89c6\u89d2\u30013D\u9ad8\u65af\u63d0\u5347\u548c\u59ff\u6001\u611f\u77e5UV\u7a7a\u95f4\u6620\u5c04\uff0c\u4ece\u5355\u5f20\u56fe\u50cf\u91cd\u5efa\u53ef\u52a8\u753b\u76843D\u4eba\u4f53\u5316\u8eab\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6e32\u67d3\u548c\u76f4\u89c2\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u4ece\u5355\u5f20\u56fe\u50cf\u91cd\u5efa\u65e2\u4fdd\u6301\u7cbe\u7ec6\u89c6\u89c9\u7ec6\u8282\u53c8\u652f\u6301\u52a8\u753b\u76843D\u4eba\u4f53\u5316\u8eab\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6865\u63a5\u975e\u7ed3\u6784\u53163D\u8868\u793a\u4e0e\u9ad8\u4fdd\u771f\u5ea6\u52a8\u753b\u5c31\u7eea\u5316\u8eab\u4e4b\u95f4\u5dee\u8ddd\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDLA\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u5408\u7406\u7684\u591a\u89c6\u89d2\u56fe\u50cf\uff1b2\uff09\u5c06\u591a\u89c6\u89d2\u63d0\u5347\u4e3a\u975e\u7ed3\u6784\u53163D\u9ad8\u65af\uff1b3\uff09\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u7f16\u7801\u5668\u5efa\u6a21\u5168\u5c40\u7a7a\u95f4\u5173\u7cfb\uff0c\u5c06\u9ad8\u65af\u6295\u5f71\u5230\u4e0e\u53c2\u6570\u5316\u8eab\u4f53\u6a21\u578bUV\u7a7a\u95f4\u5bf9\u9f50\u7684\u7ed3\u6784\u5316\u6f5c\u5728\u8868\u793a\uff1b4\uff09\u5c06\u6f5c\u5728\u4ee3\u7801\u89e3\u7801\u4e3a\u53ef\u901a\u8fc7\u8eab\u4f53\u9a71\u52a8\u53d8\u5f62\u8fdb\u884c\u52a8\u753b\u7684UV\u7a7a\u95f4\u9ad8\u65af\u3002", "result": "\u5728ActorsHQ\u548c4D-Dress\u6570\u636e\u96c6\u4e0a\uff0cDLA\u5728\u611f\u77e5\u8d28\u91cf\u548c\u5149\u5ea6\u6d4b\u91cf\u7cbe\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u5b9e\u65f6\u6e32\u67d3\u548c\u76f4\u89c2\u7f16\u8f91\uff0c\u65e0\u9700\u540e\u5904\u7406\u3002", "conclusion": "\u901a\u8fc7\u5c06\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u4f18\u52bf\u4e0e\u59ff\u6001\u611f\u77e5UV\u7a7a\u95f4\u9ad8\u65af\u6620\u5c04\u76f8\u7ed3\u5408\uff0cDLA\u6210\u529f\u6865\u63a5\u4e86\u975e\u7ed3\u6784\u53163D\u8868\u793a\u4e0e\u9ad8\u4fdd\u771f\u5ea6\u52a8\u753b\u5c31\u7eea\u5316\u8eab\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u901a\u8fc7\u5c06\u9ad8\u65af\u951a\u5b9a\u5230UV\u6d41\u5f62\u786e\u4fdd\u52a8\u753b\u8fc7\u7a0b\u4e2d\u7684\u4e00\u81f4\u6027\u5e76\u4fdd\u6301\u7cbe\u7ec6\u89c6\u89c9\u7ec6\u8282\u3002"}}
{"id": "2507.16165", "pdf": "https://arxiv.org/pdf/2507.16165", "abs": "https://arxiv.org/abs/2507.16165", "authors": ["Liam Naddell", "Marcelo Ponce"], "title": "Parallel Ray Tracing of Black Hole Images Using the Schwarzschild Metric", "categories": ["cs.DC", "cs.GR", "gr-qc"], "comment": "Published and presented at PEARC '25: Practice and Experience in   Advanced Research Computing 2025: \"The Power of Collaboration\"", "summary": "Rendering images of black holes by utilizing ray tracing techniques is a common methodology employed in many aspects of scientific and astrophysical visualizations. Similarly, general ray tracing techniques are widely used in areas related to computer graphics. In this work we describe the implementation of a parallel open-source program that can ray trace images in the presence of a black hole geometry. We do this by combining a couple of different techniques usually present in parallel scientific computing, such as, mathematical approximations, utilization of scientific libraries, shared-memory and distributed-memory parallelism.", "AI": {"tldr": "\u672c\u6587\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5e76\u884c\u5f00\u6e90\u7a0b\u5e8f\uff0c\u7528\u4e8e\u5728\u9ed1\u6d1e\u51e0\u4f55\u73af\u5883\u4e2d\u8fdb\u884c\u5149\u7ebf\u8ffd\u8e2a\u6e32\u67d3\uff0c\u7ed3\u5408\u4e86\u6570\u5b66\u8fd1\u4f3c\u3001\u79d1\u5b66\u5e93\u548c\u5e76\u884c\u8ba1\u7b97\u6280\u672f\u3002", "motivation": "\u9ed1\u6d1e\u56fe\u50cf\u6e32\u67d3\u5728\u79d1\u5b66\u548c\u5929\u4f53\u7269\u7406\u53ef\u89c6\u5316\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u540c\u65f6\u5149\u7ebf\u8ffd\u8e2a\u6280\u672f\u5728\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u9886\u57df\u4e5f\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u9700\u8981\u4e00\u4e2a\u9ad8\u6548\u7684\u5e76\u884c\u8ba1\u7b97\u89e3\u51b3\u65b9\u6848\u6765\u5904\u7406\u9ed1\u6d1e\u51e0\u4f55\u73af\u5883\u4e0b\u7684\u590d\u6742\u5149\u7ebf\u8ffd\u8e2a\u8ba1\u7b97\u3002", "method": "\u7ed3\u5408\u591a\u79cd\u5e76\u884c\u79d1\u5b66\u8ba1\u7b97\u6280\u672f\uff0c\u5305\u62ec\u6570\u5b66\u8fd1\u4f3c\u65b9\u6cd5\u3001\u79d1\u5b66\u8ba1\u7b97\u5e93\u7684\u4f7f\u7528\u3001\u5171\u4eab\u5185\u5b58\u5e76\u884c\u548c\u5206\u5e03\u5f0f\u5185\u5b58\u5e76\u884c\u6280\u672f\uff0c\u5f00\u53d1\u5e76\u884c\u5f00\u6e90\u7a0b\u5e8f\u6765\u5b9e\u73b0\u9ed1\u6d1e\u51e0\u4f55\u73af\u5883\u4e0b\u7684\u5149\u7ebf\u8ffd\u8e2a\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u4e00\u4e2a\u80fd\u591f\u5728\u9ed1\u6d1e\u51e0\u4f55\u6761\u4ef6\u4e0b\u8fdb\u884c\u5149\u7ebf\u8ffd\u8e2a\u7684\u5e76\u884c\u5f00\u6e90\u7a0b\u5e8f\uff0c\u8be5\u7a0b\u5e8f\u6574\u5408\u4e86\u591a\u79cd\u5e76\u884c\u8ba1\u7b97\u6280\u672f\u548c\u6570\u5b66\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u6570\u5b66\u8fd1\u4f3c\u3001\u79d1\u5b66\u5e93\u548c\u591a\u79cd\u5e76\u884c\u8ba1\u7b97\u6280\u672f\uff0c\u6210\u529f\u5f00\u53d1\u4e86\u7528\u4e8e\u9ed1\u6d1e\u51e0\u4f55\u73af\u5883\u5149\u7ebf\u8ffd\u8e2a\u7684\u5e76\u884c\u5f00\u6e90\u7a0b\u5e8f\uff0c\u4e3a\u79d1\u5b66\u53ef\u89c6\u5316\u548c\u5929\u4f53\u7269\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2507.15888", "pdf": "https://arxiv.org/pdf/2507.15888", "abs": "https://arxiv.org/abs/2507.15888", "authors": ["Leonardo Santiago Benitez Pereira", "Arathy Jeevan"], "title": "PAT++: a cautionary tale about generative visual augmentation for Object Re-identification", "categories": ["cs.CV"], "comment": null, "summary": "Generative data augmentation has demonstrated gains in several vision tasks, but its impact on object re-identification - where preserving fine-grained visual details is essential - remains largely unexplored. In this work, we assess the effectiveness of identity-preserving image generation for object re-identification. Our novel pipeline, named PAT++, incorporates Diffusion Self-Distillation into the well-established Part-Aware Transformer. Using the Urban Elements ReID Challenge dataset, we conduct extensive experiments with generated images used for both model training and query expansion. Our results show consistent performance degradation, driven by domain shifts and failure to retain identity-defining features. These findings challenge assumptions about the transferability of generative models to fine-grained recognition tasks and expose key limitations in current approaches to visual augmentation for identity-preserving applications.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u751f\u6210\u5f0f\u6570\u636e\u589e\u5f3a\u5728\u76ee\u6807\u91cd\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u63d0\u51fa\u4e86PAT++\u7ba1\u9053\uff0c\u7ed3\u5408\u6269\u6563\u81ea\u84b8\u998f\u548c\u90e8\u5206\u611f\u77e5\u53d8\u6362\u5668\uff0c\u4f46\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u751f\u6210\u56fe\u50cf\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u6311\u6218\u4e86\u751f\u6210\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u5047\u8bbe\u3002", "motivation": "\u867d\u7136\u751f\u6210\u5f0f\u6570\u636e\u589e\u5f3a\u5728\u591a\u4e2a\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9700\u8981\u4fdd\u6301\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7ec6\u8282\u7684\u76ee\u6807\u91cd\u8bc6\u522b\u4efb\u52a1\u4e2d\uff0c\u5176\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u4f5c\u8005\u5e0c\u671b\u8bc4\u4f30\u8eab\u4efd\u4fdd\u6301\u56fe\u50cf\u751f\u6210\u5728\u76ee\u6807\u91cd\u8bc6\u522b\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u540d\u4e3aPAT++\u7684\u65b0\u578b\u7ba1\u9053\uff0c\u5c06\u6269\u6563\u81ea\u84b8\u998f\uff08Diffusion Self-Distillation\uff09\u878d\u5165\u5230\u5df2\u5efa\u7acb\u7684\u90e8\u5206\u611f\u77e5\u53d8\u6362\u5668\uff08Part-Aware Transformer\uff09\u4e2d\u3002\u4f7f\u7528Urban Elements ReID Challenge\u6570\u636e\u96c6\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5c06\u751f\u6210\u7684\u56fe\u50cf\u7528\u4e8e\u6a21\u578b\u8bad\u7ec3\u548c\u67e5\u8be2\u6269\u5c55\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6027\u80fd\u6301\u7eed\u4e0b\u964d\uff0c\u4e3b\u8981\u7531\u9886\u57df\u504f\u79fb\u548c\u65e0\u6cd5\u4fdd\u7559\u8eab\u4efd\u5b9a\u4e49\u7279\u5f81\u6240\u9a71\u52a8\u3002\u751f\u6210\u7684\u56fe\u50cf\u5728\u76ee\u6807\u91cd\u8bc6\u522b\u4efb\u52a1\u4e2d\u5e76\u672a\u5e26\u6765\u9884\u671f\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6311\u6218\u4e86\u751f\u6210\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u8bc6\u522b\u4efb\u52a1\u4e2d\u53ef\u8f6c\u79fb\u6027\u7684\u5047\u8bbe\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u89c6\u89c9\u589e\u5f3a\u65b9\u6cd5\u5728\u8eab\u4efd\u4fdd\u6301\u5e94\u7528\u4e2d\u7684\u5173\u952e\u5c40\u9650\u6027\u3002\u8fd9\u8868\u660e\u751f\u6210\u5f0f\u6570\u636e\u589e\u5f3a\u53ef\u80fd\u4e0d\u9002\u7528\u4e8e\u6240\u6709\u89c6\u89c9\u4efb\u52a1\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u9700\u8981\u7cbe\u786e\u4fdd\u6301\u7ec6\u7c92\u5ea6\u7279\u5f81\u7684\u4efb\u52a1\u3002"}}
{"id": "2507.16095", "pdf": "https://arxiv.org/pdf/2507.16095", "abs": "https://arxiv.org/abs/2507.16095", "authors": ["Parul Gupta", "Abhinav Dhall", "Thanh-Toan Do"], "title": "Improving Personalized Image Generation through Social Context Feedback", "categories": ["cs.CV"], "comment": null, "summary": "Personalized image generation, where reference images of one or more subjects are used to generate their image according to a scene description, has gathered significant interest in the community. However, such generated images suffer from three major limitations -- complex activities, such as $<$man, pushing, motorcycle$>$ are not generated properly with incorrect human poses, reference human identities are not preserved, and generated human gaze patterns are unnatural/inconsistent with the scene description. In this work, we propose to overcome these shortcomings through feedback-based fine-tuning of existing personalized generation methods, wherein, state-of-art detectors of pose, human-object-interaction, human facial recognition and human gaze-point estimation are used to refine the diffusion model. We also propose timestep-based inculcation of different feedback modules, depending upon whether the signal is low-level (such as human pose), or high-level (such as gaze point). The images generated in this manner show an improvement in the generated interactions, facial identities and image quality over three benchmark datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u9988\u7684\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u59ff\u6001\u3001\u4eba\u7269-\u7269\u4f53\u4ea4\u4e92\u3001\u9762\u90e8\u8bc6\u522b\u548c\u89c6\u7ebf\u4f30\u8ba1\u68c0\u6d4b\u5668\u6765\u6539\u8fdb\u6269\u6563\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u6d3b\u52a8\u751f\u6210\u4e0d\u5f53\u3001\u8eab\u4efd\u4fdd\u6301\u4e0d\u4f73\u548c\u89c6\u7ebf\u6a21\u5f0f\u4e0d\u81ea\u7136\u7b49\u95ee\u9898", "motivation": "\u73b0\u6709\u7684\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u9650\u5236\uff1a1\uff09\u590d\u6742\u6d3b\u52a8\uff08\u5982\"\u7537\u4eba\u63a8\u6469\u6258\u8f66\"\uff09\u751f\u6210\u4e0d\u5f53\uff0c\u4eba\u4f53\u59ff\u6001\u9519\u8bef\uff1b2\uff09\u53c2\u8003\u4eba\u7269\u8eab\u4efd\u65e0\u6cd5\u4fdd\u6301\uff1b3\uff09\u751f\u6210\u7684\u4eba\u7269\u89c6\u7ebf\u6a21\u5f0f\u4e0d\u81ea\u7136\u6216\u4e0e\u573a\u666f\u63cf\u8ff0\u4e0d\u4e00\u81f4", "method": "\u63d0\u51fa\u57fa\u4e8e\u53cd\u9988\u7684\u5fae\u8c03\u65b9\u6cd5\u6765\u6539\u8fdb\u73b0\u6709\u4e2a\u6027\u5316\u751f\u6210\u65b9\u6cd5\uff0c\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u59ff\u6001\u68c0\u6d4b\u5668\u3001\u4eba\u7269-\u7269\u4f53\u4ea4\u4e92\u68c0\u6d4b\u5668\u3001\u4eba\u8138\u8bc6\u522b\u5668\u548c\u4eba\u773c\u89c6\u7ebf\u4f30\u8ba1\u5668\u6765\u4f18\u5316\u6269\u6563\u6a21\u578b\u3002\u6839\u636e\u4fe1\u53f7\u662f\u4f4e\u7ea7\uff08\u5982\u4eba\u4f53\u59ff\u6001\uff09\u8fd8\u662f\u9ad8\u7ea7\uff08\u5982\u89c6\u7ebf\u70b9\uff09\uff0c\u63d0\u51fa\u57fa\u4e8e\u65f6\u95f4\u6b65\u7684\u4e0d\u540c\u53cd\u9988\u6a21\u5757\u96c6\u6210\u7b56\u7565", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u751f\u6210\u7684\u56fe\u50cf\u5728\u4ea4\u4e92\u8d28\u91cf\u3001\u9762\u90e8\u8eab\u4efd\u4fdd\u6301\u548c\u6574\u4f53\u56fe\u50cf\u8d28\u91cf\u65b9\u9762\u90fd\u6709\u663e\u8457\u6539\u5584", "conclusion": "\u901a\u8fc7\u96c6\u6210\u591a\u79cd\u4e13\u95e8\u7684\u68c0\u6d4b\u5668\u4f5c\u4e3a\u53cd\u9988\u4fe1\u53f7\uff0c\u5e76\u91c7\u7528\u65f6\u95f4\u6b65\u76f8\u5173\u7684\u53cd\u9988\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u548c\u51c6\u786e\u6027"}}
{"id": "2507.16116", "pdf": "https://arxiv.org/pdf/2507.16116", "abs": "https://arxiv.org/abs/2507.16116", "authors": ["Yaofang Liu", "Yumeng Ren", "Aitor Artola", "Yuxuan Hu", "Xiaodong Cun", "Xiaotong Zhao", "Alan Zhao", "Raymond H. Chan", "Suiyun Zhang", "Rui Liu", "Dandan Tu", "Jean-Michel Morel"], "title": "PUSA V1.0: Surpassing Wan-I2V with $500 Training Cost by Vectorized Timestep Adaptation", "categories": ["cs.CV"], "comment": "Code is open-sourced at https://github.com/Yaofang-Liu/Pusa-VidGen", "summary": "The rapid advancement of video diffusion models has been hindered by fundamental limitations in temporal modeling, particularly the rigid synchronization of frame evolution imposed by conventional scalar timestep variables. While task-specific adaptations and autoregressive models have sought to address these challenges, they remain constrained by computational inefficiency, catastrophic forgetting, or narrow applicability. In this work, we present Pusa, a groundbreaking paradigm that leverages vectorized timestep adaptation (VTA) to enable fine-grained temporal control within a unified video diffusion framework. Besides, VTA is a non-destructive adaptation, which means it fully preserves the capabilities of the base model. By finetuning the SOTA Wan2.1-T2V-14B model with VTA, we achieve unprecedented efficiency -- surpassing the performance of Wan-I2V-14B with $\\leq$ 1/200 of the training cost (\\$500 vs. $\\geq$ \\$100,000) and $\\leq$ 1/2500 of the dataset size (4K vs. $\\geq$ 10M samples). Pusa not only sets a new standard for image-to-video (I2V) generation, achieving a VBench-I2V total score of 87.32\\% (vs. 86.86\\% of Wan-I2V-14B), but also unlocks many zero-shot multi-task capabilities such as start-end frames and video extension -- all without task-specific training. Meanwhile, Pusa can still perform text-to-video generation. Mechanistic analyses reveal that our approach preserves the foundation model's generative priors while surgically injecting temporal dynamics, avoiding the combinatorial explosion inherent to vectorized timesteps. This work establishes a scalable, efficient, and versatile paradigm for next-generation video synthesis, democratizing high-fidelity video generation for research and industry alike. Code is open-sourced at https://github.com/Yaofang-Liu/Pusa-VidGen", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Pusa\uff0c\u4e00\u4e2a\u4f7f\u7528\u5411\u91cf\u5316\u65f6\u95f4\u6b65\u9002\u5e94(VTA)\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\uff0c\u4ee5\u6781\u4f4e\u7684\u8bad\u7ec3\u6210\u672c(\u4ec5\u97001/200\u7684\u8d39\u7528\u548c1/2500\u7684\u6570\u636e\u91cf)\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u65f6\u95f4\u5efa\u6a21\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\uff0c\u7279\u522b\u662f\u4f20\u7edf\u6807\u91cf\u65f6\u95f4\u6b65\u53d8\u91cf\u5bfc\u81f4\u7684\u5e27\u6f14\u5316\u521a\u6027\u540c\u6b65\u95ee\u9898\u3002\u867d\u7136\u6709\u4efb\u52a1\u7279\u5b9a\u9002\u5e94\u548c\u81ea\u56de\u5f52\u6a21\u578b\u8bd5\u56fe\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u4f46\u5b83\u4eec\u4ecd\u53d7\u5230\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u3001\u707e\u96be\u6027\u9057\u5fd8\u6216\u9002\u7528\u6027\u72ed\u7a84\u7684\u9650\u5236\u3002", "method": "\u63d0\u51faPusa\u8303\u5f0f\uff0c\u91c7\u7528\u5411\u91cf\u5316\u65f6\u95f4\u6b65\u9002\u5e94(VTA)\u6280\u672f\uff0c\u5728\u7edf\u4e00\u7684\u89c6\u9891\u6269\u6563\u6846\u67b6\u5185\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u65f6\u95f4\u63a7\u5236\u3002VTA\u662f\u4e00\u79cd\u975e\u7834\u574f\u6027\u9002\u5e94\u65b9\u6cd5\uff0c\u5b8c\u5168\u4fdd\u7559\u57fa\u7840\u6a21\u578b\u7684\u80fd\u529b\u3002\u901a\u8fc7\u5bf9\u6700\u5148\u8fdb\u7684Wan2.1-T2V-14B\u6a21\u578b\u8fdb\u884cVTA\u5fae\u8c03\u6765\u5b9e\u73b0\u3002", "result": "\u5728\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86VBench-I2V\u603b\u520687.32%\u7684\u6210\u7ee9(\u8d85\u8fc7Wan-I2V-14B\u768486.86%)\uff0c\u540c\u65f6\u8bad\u7ec3\u6210\u672c\u4ec5\u4e3a\u539f\u6765\u76841/200($500 vs. \u2265$100,000)\uff0c\u6570\u636e\u96c6\u5927\u5c0f\u4ec5\u4e3a1/2500(4K vs. \u226510M\u6837\u672c)\u3002\u6b64\u5916\u8fd8\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u591a\u4efb\u52a1\u80fd\u529b\uff0c\u5982\u8d77\u59cb-\u7ed3\u675f\u5e27\u548c\u89c6\u9891\u6269\u5c55\u7b49\u529f\u80fd\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u4e14\u591a\u529f\u80fd\u7684\u4e0b\u4e00\u4ee3\u89c6\u9891\u5408\u6210\u8303\u5f0f\uff0c\u4e3a\u7814\u7a76\u548c\u5de5\u4e1a\u754c\u6c11\u4e3b\u5316\u9ad8\u4fdd\u771f\u89c6\u9891\u751f\u6210\u3002\u673a\u5236\u5206\u6790\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4fdd\u7559\u57fa\u7840\u6a21\u578b\u751f\u6210\u5148\u9a8c\u7684\u540c\u65f6\u7cbe\u786e\u6ce8\u5165\u65f6\u95f4\u52a8\u6001\uff0c\u907f\u514d\u4e86\u5411\u91cf\u5316\u65f6\u95f4\u6b65\u56fa\u6709\u7684\u7ec4\u5408\u7206\u70b8\u95ee\u9898\u3002"}}
{"id": "2507.16144", "pdf": "https://arxiv.org/pdf/2507.16144", "abs": "https://arxiv.org/abs/2507.16144", "authors": ["Guichen Huang", "Ruoyu Wang", "Xiangjun Gao", "Che Sun", "Yuwei Wu", "Shenghua Gao", "Yunde Jia"], "title": "LongSplat: Online Generalizable 3D Gaussian Splatting from Long Sequence Images", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting achieves high-fidelity novel view synthesis, but its application to online long-sequence scenarios is still limited. Existing methods either rely on slow per-scene optimization or fail to provide efficient incremental updates, hindering continuous performance. In this paper, we propose LongSplat, an online real-time 3D Gaussian reconstruction framework designed for long-sequence image input. The core idea is a streaming update mechanism that incrementally integrates current-view observations while selectively compressing redundant historical Gaussians. Crucial to this mechanism is our Gaussian-Image Representation (GIR), a representation that encodes 3D Gaussian parameters into a structured, image-like 2D format. GIR simultaneously enables efficient fusion of current-view and historical Gaussians and identity-aware redundancy compression. These functions enable online reconstruction and adapt the model to long sequences without overwhelming memory or computational costs. Furthermore, we leverage an existing image compression method to guide the generation of more compact and higher-quality 3D Gaussians. Extensive evaluations demonstrate that LongSplat achieves state-of-the-art efficiency-quality trade-offs in real-time novel view synthesis, delivering real-time reconstruction while reducing Gaussian counts by 44\\% compared to existing per-pixel Gaussian prediction methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86LongSplat\uff0c\u4e00\u4e2a\u9488\u5bf9\u957f\u5e8f\u5217\u56fe\u50cf\u8f93\u5165\u7684\u5728\u7ebf\u5b9e\u65f63D\u9ad8\u65af\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u6d41\u5f0f\u66f4\u65b0\u673a\u5236\u548c\u9ad8\u65af\u56fe\u50cf\u8868\u793a(GIR)\u5b9e\u73b0\u9ad8\u6548\u7684\u589e\u91cf\u66f4\u65b0\u548c\u5197\u4f59\u538b\u7f29\uff0c\u5728\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c1144%\u7684\u9ad8\u65af\u6570\u91cf\u3002", "motivation": "\u73b0\u6709\u76843D\u9ad8\u65af\u55b7\u5c04\u65b9\u6cd5\u5728\u5728\u7ebf\u957f\u5e8f\u5217\u573a\u666f\u4e2d\u5e94\u7528\u53d7\u9650\uff0c\u8981\u4e48\u4f9d\u8d56\u7f13\u6162\u7684\u9010\u573a\u666f\u4f18\u5316\uff0c\u8981\u4e48\u65e0\u6cd5\u63d0\u4f9b\u9ad8\u6548\u7684\u589e\u91cf\u66f4\u65b0\uff0c\u963b\u788d\u4e86\u8fde\u7eed\u6027\u80fd\u8868\u73b0\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u957f\u5e8f\u5217\u8f93\u5165\u7684\u5b9e\u65f63D\u91cd\u5efa\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u6d41\u5f0f\u66f4\u65b0\u673a\u5236\uff0c\u9010\u6b65\u6574\u5408\u5f53\u524d\u89c6\u56fe\u89c2\u6d4b\u5e76\u9009\u62e9\u6027\u538b\u7f29\u5197\u4f59\u7684\u5386\u53f2\u9ad8\u65af\uff1b\u8bbe\u8ba1\u9ad8\u65af\u56fe\u50cf\u8868\u793a(GIR)\uff0c\u5c063D\u9ad8\u65af\u53c2\u6570\u7f16\u7801\u4e3a\u7ed3\u6784\u5316\u7684\u7c7b\u56fe\u50cf2D\u683c\u5f0f\uff1b\u5229\u7528\u73b0\u6709\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\u6307\u5bfc\u751f\u6210\u66f4\u7d27\u51d1\u3001\u66f4\u9ad8\u8d28\u91cf\u76843D\u9ad8\u65af\u3002", "result": "\u5728\u5b9e\u65f6\u65b0\u89c6\u56fe\u5408\u6210\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6548\u7387-\u8d28\u91cf\u6743\u8861\uff0c\u63d0\u4f9b\u5b9e\u65f6\u91cd\u5efa\u80fd\u529b\uff0c\u540c\u65f6\u76f8\u6bd4\u73b0\u6709\u7684\u9010\u50cf\u7d20\u9ad8\u65af\u9884\u6d4b\u65b9\u6cd5\u51cf\u5c11\u4e8644%\u7684\u9ad8\u65af\u6570\u91cf\uff0c\u6709\u6548\u63a7\u5236\u4e86\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "LongSplat\u6210\u529f\u89e3\u51b3\u4e863D\u9ad8\u65af\u55b7\u5c04\u5728\u957f\u5e8f\u5217\u573a\u666f\u4e2d\u7684\u5e94\u7528\u9650\u5236\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6d41\u5f0f\u66f4\u65b0\u673a\u5236\u548c\u9ad8\u65af\u56fe\u50cf\u8868\u793a\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5728\u7ebf3D\u91cd\u5efa\uff0c\u4e3a\u5b9e\u65f6\u65b0\u89c6\u56fe\u5408\u6210\u63d0\u4f9b\u4e86\u4f18\u79c0\u7684\u6548\u7387-\u8d28\u91cf\u5e73\u8861\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16154", "pdf": "https://arxiv.org/pdf/2507.16154", "abs": "https://arxiv.org/abs/2507.16154", "authors": ["Jyun-Ze Tang", "Chih-Fan Hsu", "Jeng-Lin Li", "Ming-Ching Chang", "Wei-Chao Chen"], "title": "LSSGen: Leveraging Latent Space Scaling in Flow and Diffusion for Efficient Text to Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV AIGENS 2025", "summary": "Flow matching and diffusion models have shown impressive results in text-to-image generation, producing photorealistic images through an iterative denoising process. A common strategy to speed up synthesis is to perform early denoising at lower resolutions. However, traditional methods that downscale and upscale in pixel space often introduce artifacts and distortions. These issues arise when the upscaled images are re-encoded into the latent space, leading to degraded final image quality. To address this, we propose {\\bf Latent Space Scaling Generation (LSSGen)}, a framework that performs resolution scaling directly in the latent space using a lightweight latent upsampler. Without altering the Transformer or U-Net architecture, LSSGen improves both efficiency and visual quality while supporting flexible multi-resolution generation. Our comprehensive evaluation covering text-image alignment and perceptual quality shows that LSSGen significantly outperforms conventional scaling approaches. When generating $1024^2$ images at similar speeds, it achieves up to 246\\% TOPIQ score improvement.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u6f5c\u5728\u7a7a\u95f4\u7f29\u653e\u751f\u6210(LSSGen)\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u6f5c\u5728\u7a7a\u95f4\u800c\u975e\u50cf\u7d20\u7a7a\u95f4\u8fdb\u884c\u5206\u8fa8\u7387\u7f29\u653e\u6765\u6539\u8fdb\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u6548\u7387\u548c\u8d28\u91cf\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u4e0a\u4e0b\u91c7\u6837\u65b9\u6cd5\u5f15\u5165\u7684\u4f2a\u5f71\u548c\u5931\u771f\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u867d\u7136\u6548\u679c\u51fa\u8272\uff0c\u4f46\u4e3a\u4e86\u52a0\u901f\u5408\u6210\u901a\u5e38\u5728\u4f4e\u5206\u8fa8\u7387\u4e0b\u8fdb\u884c\u65e9\u671f\u53bb\u566a\uff0c\u7136\u540e\u5728\u50cf\u7d20\u7a7a\u95f4\u8fdb\u884c\u4e0b\u91c7\u6837\u548c\u4e0a\u91c7\u6837\u3002\u8fd9\u79cd\u65b9\u6cd5\u4f1a\u5f15\u5165\u4f2a\u5f71\u548c\u5931\u771f\uff0c\u5f53\u4e0a\u91c7\u6837\u56fe\u50cf\u91cd\u65b0\u7f16\u7801\u5230\u6f5c\u5728\u7a7a\u95f4\u65f6\u4f1a\u5bfc\u81f4\u6700\u7ec8\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u6f5c\u5728\u7a7a\u95f4\u7f29\u653e\u751f\u6210(LSSGen)\u6846\u67b6\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6f5c\u5728\u4e0a\u91c7\u6837\u5668\u76f4\u63a5\u5728\u6f5c\u5728\u7a7a\u95f4\u6267\u884c\u5206\u8fa8\u7387\u7f29\u653e\uff0c\u800c\u65e0\u9700\u4fee\u6539Transformer\u6216U-Net\u67b6\u6784\u3002\u8be5\u65b9\u6cd5\u907f\u514d\u4e86\u50cf\u7d20\u7a7a\u95f4\u7f29\u653e\u7684\u95ee\u9898\uff0c\u652f\u6301\u7075\u6d3b\u7684\u591a\u5206\u8fa8\u7387\u751f\u6210\u3002", "result": "\u5728\u751f\u62101024\u00b2\u56fe\u50cf\u65f6\u8fbe\u5230\u76f8\u4f3c\u901f\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0cLSSGen\u76f8\u6bd4\u4f20\u7edf\u7f29\u653e\u65b9\u6cd5\u5728TOPIQ\u8bc4\u5206\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe246%\u7684\u6539\u8fdb\u3002\u7efc\u5408\u8bc4\u4f30\u663e\u793a\u5728\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u548c\u611f\u77e5\u8d28\u91cf\u65b9\u9762\u90fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7f29\u653e\u65b9\u6cd5\u3002", "conclusion": "LSSGen\u6846\u67b6\u901a\u8fc7\u5728\u6f5c\u5728\u7a7a\u95f4\u76f4\u63a5\u8fdb\u884c\u5206\u8fa8\u7387\u7f29\u653e\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u50cf\u7d20\u7a7a\u95f4\u7f29\u653e\u65b9\u6cd5\u7684\u4f2a\u5f71\u548c\u5931\u771f\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u751f\u6210\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8d28\u91cf\uff0c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16191", "pdf": "https://arxiv.org/pdf/2507.16191", "abs": "https://arxiv.org/abs/2507.16191", "authors": ["Fansheng Zeng", "Bineng Zhong", "Haiying Xia", "Yufei Tan", "Xiantao Hu", "Liangtao Shi", "Shuxiang Song"], "title": "Explicit Context Reasoning with Supervision for Visual Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Contextual reasoning with constraints is crucial for enhancing temporal consistency in cross-frame modeling for visual tracking. However, mainstream tracking algorithms typically associate context by merely stacking historical information without explicitly supervising the association process, making it difficult to effectively model the target's evolving dynamics. To alleviate this problem, we propose RSTrack, which explicitly models and supervises context reasoning via three core mechanisms. \\textit{1) Context Reasoning Mechanism}: Constructs a target state reasoning pipeline, converting unconstrained contextual associations into a temporal reasoning process that predicts the current representation based on historical target states, thereby enhancing temporal consistency. \\textit{2) Forward Supervision Strategy}: Utilizes true target features as anchors to constrain the reasoning pipeline, guiding the predicted output toward the true target distribution and suppressing drift in the context reasoning process. \\textit{3) Efficient State Modeling}: Employs a compression-reconstruction mechanism to extract the core features of the target, removing redundant information across frames and preventing ineffective contextual associations. These three mechanisms collaborate to effectively alleviate the issue of contextual association divergence in traditional temporal modeling. Experimental results show that RSTrack achieves state-of-the-art performance on multiple benchmark datasets while maintaining real-time running speeds. Our code is available at https://github.com/GXNU-ZhongLab/RSTrack.", "AI": {"tldr": "RSTrack\u662f\u4e00\u4e2a\u89c6\u89c9\u8ddf\u8e2a\u7b97\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u548c\u76d1\u7763\u4e0a\u4e0b\u6587\u63a8\u7406\u6765\u589e\u5f3a\u8de8\u5e27\u5efa\u6a21\u4e2d\u7684\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u5e76\u4fdd\u6301\u5b9e\u65f6\u8fd0\u884c\u901f\u5ea6\u3002", "motivation": "\u4e3b\u6d41\u8ddf\u8e2a\u7b97\u6cd5\u901a\u5e38\u4ec5\u901a\u8fc7\u5806\u53e0\u5386\u53f2\u4fe1\u606f\u6765\u5173\u8054\u4e0a\u4e0b\u6587\uff0c\u800c\u6ca1\u6709\u663e\u5f0f\u76d1\u7763\u5173\u8054\u8fc7\u7a0b\uff0c\u8fd9\u4f7f\u5f97\u96be\u4ee5\u6709\u6548\u5efa\u6a21\u76ee\u6807\u7684\u6f14\u5316\u52a8\u6001\uff0c\u5bfc\u81f4\u4e0a\u4e0b\u6587\u5173\u8054\u53d1\u6563\u95ee\u9898\u3002", "method": "\u63d0\u51faRSTrack\u7b97\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u673a\u5236\uff1a1\uff09\u4e0a\u4e0b\u6587\u63a8\u7406\u673a\u5236\uff1a\u6784\u5efa\u76ee\u6807\u72b6\u6001\u63a8\u7406\u7ba1\u9053\uff0c\u5c06\u65e0\u7ea6\u675f\u7684\u4e0a\u4e0b\u6587\u5173\u8054\u8f6c\u6362\u4e3a\u57fa\u4e8e\u5386\u53f2\u76ee\u6807\u72b6\u6001\u9884\u6d4b\u5f53\u524d\u8868\u793a\u7684\u65f6\u95f4\u63a8\u7406\u8fc7\u7a0b\uff1b2\uff09\u524d\u5411\u76d1\u7763\u7b56\u7565\uff1a\u5229\u7528\u771f\u5b9e\u76ee\u6807\u7279\u5f81\u4f5c\u4e3a\u951a\u70b9\u7ea6\u675f\u63a8\u7406\u7ba1\u9053\uff0c\u5f15\u5bfc\u9884\u6d4b\u8f93\u51fa\u8d8b\u5411\u771f\u5b9e\u76ee\u6807\u5206\u5e03\uff1b3\uff09\u9ad8\u6548\u72b6\u6001\u5efa\u6a21\uff1a\u91c7\u7528\u538b\u7f29-\u91cd\u6784\u673a\u5236\u63d0\u53d6\u76ee\u6807\u6838\u5fc3\u7279\u5f81\uff0c\u53bb\u9664\u8de8\u5e27\u5197\u4f59\u4fe1\u606f\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u8fd0\u884c\u901f\u5ea6\u3002\u4e09\u4e2a\u673a\u5236\u534f\u540c\u5de5\u4f5c\u6709\u6548\u7f13\u89e3\u4e86\u4f20\u7edf\u65f6\u95f4\u5efa\u6a21\u4e2d\u7684\u4e0a\u4e0b\u6587\u5173\u8054\u53d1\u6563\u95ee\u9898\u3002", "conclusion": "RSTrack\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u548c\u76d1\u7763\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u8ddf\u8e2a\u4e2d\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u4e3a\u89c6\u89c9\u8ddf\u8e2a\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16240", "pdf": "https://arxiv.org/pdf/2507.16240", "abs": "https://arxiv.org/abs/2507.16240", "authors": ["Chao Zhou", "Tianyi Wei", "Nenghai Yu"], "title": "Scale Your Instructions: Enhance the Instruction-Following Fidelity of Unified Image Generation Model by Self-Adaptive Attention Scaling", "categories": ["cs.CV"], "comment": "Accept by ICCV2025", "summary": "Recent advancements in unified image generation models, such as OmniGen, have enabled the handling of diverse image generation and editing tasks within a single framework, accepting multimodal, interleaved texts and images in free form. This unified architecture eliminates the need for text encoders, greatly reducing model complexity and standardizing various image generation and editing tasks, making it more user-friendly. However, we found that it suffers from text instruction neglect, especially when the text instruction contains multiple sub-instructions. To explore this issue, we performed a perturbation analysis on the input to identify critical steps and layers. By examining the cross-attention maps of these key steps, we observed significant conflicts between neglected sub-instructions and the activations of the input image. In response, we propose Self-Adaptive Attention Scaling (SaaS), a method that leverages the consistency of cross-attention between adjacent timesteps to dynamically scale the attention activation for each sub-instruction. Our SaaS enhances instruction-following fidelity without requiring additional training or test-time optimization. Experimental results on instruction-based image editing and visual conditional image generation validate the effectiveness of our SaaS, showing superior instruction-following fidelity over existing methods. The code is available https://github.com/zhouchao-ops/SaaS.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u7edf\u4e00\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff08\u5982OmniGen\uff09\u5728\u5904\u7406\u591a\u5b50\u6307\u4ee4\u65f6\u5b58\u5728\u7684\u6587\u672c\u6307\u4ee4\u5ffd\u7565\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u7f29\u653e\uff08SaaS\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6ce8\u610f\u529b\u6fc0\u6d3b\u6765\u63d0\u9ad8\u6307\u4ee4\u9075\u5faa\u4fdd\u771f\u5ea6\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u6709\u6548\u6539\u5584\u591a\u5b50\u6307\u4ee4\u7684\u6267\u884c\u6548\u679c\u3002", "motivation": "\u7edf\u4e00\u56fe\u50cf\u751f\u6210\u6a21\u578b\u867d\u7136\u80fd\u5728\u5355\u4e00\u6846\u67b6\u5185\u5904\u7406\u591a\u6837\u5316\u7684\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\uff0c\u4f46\u5b58\u5728\u6587\u672c\u6307\u4ee4\u5ffd\u7565\u95ee\u9898\uff0c\u7279\u522b\u662f\u5f53\u6587\u672c\u6307\u4ee4\u5305\u542b\u591a\u4e2a\u5b50\u6307\u4ee4\u65f6\uff0c\u6a21\u578b\u65e0\u6cd5\u51c6\u786e\u9075\u5faa\u6240\u6709\u6307\u4ee4\u5185\u5bb9\uff0c\u5f71\u54cd\u4e86\u7528\u6237\u4f53\u9a8c\u548c\u6a21\u578b\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u7f29\u653e\uff08SaaS\uff09\u65b9\u6cd5\uff0c\u9996\u5148\u901a\u8fc7\u6270\u52a8\u5206\u6790\u8bc6\u522b\u5173\u952e\u6b65\u9aa4\u548c\u5c42\uff0c\u7136\u540e\u68c0\u67e5\u4ea4\u53c9\u6ce8\u610f\u529b\u56fe\u53d1\u73b0\u88ab\u5ffd\u7565\u7684\u5b50\u6307\u4ee4\u4e0e\u8f93\u5165\u56fe\u50cf\u6fc0\u6d3b\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u6700\u540e\u5229\u7528\u76f8\u90bb\u65f6\u95f4\u6b65\u4e4b\u95f4\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u4e00\u81f4\u6027\u6765\u52a8\u6001\u7f29\u653e\u6bcf\u4e2a\u5b50\u6307\u4ee4\u7684\u6ce8\u610f\u529b\u6fc0\u6d3b\u3002", "result": "\u5728\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u548c\u89c6\u89c9\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSaaS\u65b9\u6cd5\u5728\u6307\u4ee4\u9075\u5faa\u4fdd\u771f\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u5b50\u6307\u4ee4\u5904\u7406\u4e2d\u7684\u6587\u672c\u6307\u4ee4\u5ffd\u7565\u95ee\u9898\u3002", "conclusion": "SaaS\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u7edf\u4e00\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u7684\u6587\u672c\u6307\u4ee4\u5ffd\u7565\u95ee\u9898\uff0c\u7279\u522b\u662f\u591a\u5b50\u6307\u4ee4\u573a\u666f\u4e0b\u7684\u5904\u7406\u80fd\u529b\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u6d4b\u8bd5\u65f6\u4f18\u5316\uff0c\u4e3a\u7edf\u4e00\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6539\u8fdb\u65b9\u6848\u3002"}}
{"id": "2507.16310", "pdf": "https://arxiv.org/pdf/2507.16310", "abs": "https://arxiv.org/abs/2507.16310", "authors": ["Yanchen Liu", "Yanan Sun", "Zhening Xing", "Junyao Gao", "Kai Chen", "Wenjie Pei"], "title": "MotionShot: Adaptive Motion Transfer across Arbitrary Objects for Text-to-Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Existing text-to-video methods struggle to transfer motion smoothly from a reference object to a target object with significant differences in appearance or structure between them. To address this challenge, we introduce MotionShot, a training-free framework capable of parsing reference-target correspondences in a fine-grained manner, thereby achieving high-fidelity motion transfer while preserving coherence in appearance. To be specific, MotionShot first performs semantic feature matching to ensure high-level alignments between the reference and target objects. It then further establishes low-level morphological alignments through reference-to-target shape retargeting. By encoding motion with temporal attention, our MotionShot can coherently transfer motion across objects, even in the presence of significant appearance and structure disparities, demonstrated by extensive experiments. The project page is available at: https://motionshot.github.io/.", "AI": {"tldr": "MotionShot\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5728\u5916\u89c2\u6216\u7ed3\u6784\u5dee\u5f02\u663e\u8457\u7684\u7269\u4f53\u95f4\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u7684\u8fd0\u52a8\u8fc1\u79fb\uff0c\u901a\u8fc7\u8bed\u4e49\u7279\u5f81\u5339\u914d\u548c\u5f62\u6001\u5b66\u5bf9\u9f50\u6765\u89e3\u51b3\u73b0\u6709\u6587\u672c\u5230\u89c6\u9891\u65b9\u6cd5\u5728\u8fd0\u52a8\u8fc1\u79fb\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u89c6\u9891\u65b9\u6cd5\u5728\u5904\u7406\u5916\u89c2\u6216\u7ed3\u6784\u5dee\u5f02\u663e\u8457\u7684\u53c2\u8003\u7269\u4f53\u548c\u76ee\u6807\u7269\u4f53\u4e4b\u95f4\u7684\u8fd0\u52a8\u8fc1\u79fb\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u65e0\u6cd5\u5b9e\u73b0\u5e73\u6ed1\u7684\u8fd0\u52a8\u4f20\u9012\uff0c\u8fd9\u9650\u5236\u4e86\u89c6\u9891\u751f\u6210\u7684\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002", "method": "MotionShot\u91c7\u7528\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u9996\u5148\u901a\u8fc7\u8bed\u4e49\u7279\u5f81\u5339\u914d\u786e\u4fdd\u53c2\u8003\u7269\u4f53\u548c\u76ee\u6807\u7269\u4f53\u4e4b\u95f4\u7684\u9ad8\u7ea7\u5bf9\u9f50\uff0c\u7136\u540e\u901a\u8fc7\u53c2\u8003\u5230\u76ee\u6807\u7684\u5f62\u72b6\u91cd\u5b9a\u5411\u5efa\u7acb\u4f4e\u7ea7\u5f62\u6001\u5b66\u5bf9\u9f50\uff0c\u6700\u540e\u4f7f\u7528\u65f6\u95f4\u6ce8\u610f\u529b\u673a\u5236\u7f16\u7801\u8fd0\u52a8\u4fe1\u606f\uff0c\u5b9e\u73b0\u7cbe\u7ec6\u5316\u7684\u53c2\u8003-\u76ee\u6807\u5bf9\u5e94\u5173\u7cfb\u89e3\u6790\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0cMotionShot\u80fd\u591f\u5728\u5b58\u5728\u663e\u8457\u5916\u89c2\u548c\u7ed3\u6784\u5dee\u5f02\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u4e0d\u540c\u7269\u4f53\u95f4\u8fde\u8d2f\u5730\u4f20\u9012\u8fd0\u52a8\uff0c\u540c\u65f6\u4fdd\u6301\u5916\u89c2\u7684\u4e00\u81f4\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u8fd0\u52a8\u8fc1\u79fb\u6548\u679c\u3002", "conclusion": "MotionShot\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u6587\u672c\u5230\u89c6\u9891\u65b9\u6cd5\u5728\u8de8\u7269\u4f53\u8fd0\u52a8\u8fc1\u79fb\u65b9\u9762\u7684\u6311\u6218\uff0c\u901a\u8fc7\u591a\u5c42\u6b21\u7684\u5bf9\u9f50\u7b56\u7565\u548c\u65f6\u95f4\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4e3a\u89c6\u9891\u751f\u6210\u9886\u57df\u7684\u8fd0\u52a8\u4f20\u9012\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16330", "pdf": "https://arxiv.org/pdf/2507.16330", "abs": "https://arxiv.org/abs/2507.16330", "authors": ["Joseph De Mathia", "Carlos Francisco Moreno-Garc\u00eda"], "title": "Scene Text Detection and Recognition \"in light of\" Challenging Environmental Conditions using Aria Glasses Egocentric Vision Cameras", "categories": ["cs.CV"], "comment": "15 pages, 8 figures", "summary": "In an era where wearable technology is reshaping applications, Scene Text Detection and Recognition (STDR) becomes a straightforward choice through the lens of egocentric vision. Leveraging Meta's Project Aria smart glasses, this paper investigates how environmental variables, such as lighting, distance, and resolution, affect the performance of state-of-the-art STDR algorithms in real-world scenarios. We introduce a novel, custom-built dataset captured under controlled conditions and evaluate two OCR pipelines: EAST with CRNN, and EAST with PyTesseract. Our findings reveal that resolution and distance significantly influence recognition accuracy, while lighting plays a less predictable role. Notably, image upscaling emerged as a key pre-processing technique, reducing Character Error Rate (CER) from 0.65 to 0.48. We further demonstrate the potential of integrating eye-gaze tracking to optimise processing efficiency by focusing on user attention zones. This work not only benchmarks STDR performance under realistic conditions but also lays the groundwork for adaptive, user-aware AR systems. Our contributions aim to inspire future research in robust, context-sensitive text recognition for assistive and research-oriented applications, such as asset inspection and nutrition analysis. The code is available at https://github.com/josepDe/Project_Aria_STR.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528Meta\u7684Project Aria\u667a\u80fd\u773c\u955c\u63a2\u7d22\u73af\u5883\u53d8\u91cf\u5bf9\u573a\u666f\u6587\u672c\u68c0\u6d4b\u8bc6\u522b(STDR)\u7b97\u6cd5\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5206\u8fa8\u7387\u548c\u8ddd\u79bb\u662f\u5173\u952e\u56e0\u7d20\uff0c\u56fe\u50cf\u4e0a\u91c7\u6837\u80fd\u663e\u8457\u63d0\u5347\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u5e76\u5c55\u793a\u4e86\u773c\u52a8\u8ffd\u8e2a\u4f18\u5316\u5904\u7406\u6548\u7387\u7684\u6f5c\u529b\u3002", "motivation": "\u968f\u7740\u53ef\u7a7f\u6234\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4ece\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u8fdb\u884c\u573a\u666f\u6587\u672c\u68c0\u6d4b\u8bc6\u522b\u6210\u4e3a\u91cd\u8981\u5e94\u7528\u65b9\u5411\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u771f\u5b9e\u73af\u5883\u6761\u4ef6\u4e0bSTDR\u7b97\u6cd5\u6027\u80fd\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0c\u7279\u522b\u662f\u5149\u7167\u3001\u8ddd\u79bb\u3001\u5206\u8fa8\u7387\u7b49\u73af\u5883\u53d8\u91cf\u5bf9\u7b97\u6cd5\u6027\u80fd\u7684\u5177\u4f53\u5f71\u54cd\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u4f7f\u7528Meta\u7684Project Aria\u667a\u80fd\u773c\u955c\u5728\u53d7\u63a7\u6761\u4ef6\u4e0b\u6784\u5efa\u5b9a\u5236\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e24\u79cdOCR\u6d41\u6c34\u7ebf(EAST+CRNN\u548cEAST+PyTesseract)\u7684\u6027\u80fd\u3002\u7cfb\u7edf\u5206\u6790\u5149\u7167\u3001\u8ddd\u79bb\u548c\u5206\u8fa8\u7387\u7b49\u73af\u5883\u53d8\u91cf\u5bf9STDR\u7b97\u6cd5\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u7d22\u56fe\u50cf\u4e0a\u91c7\u6837\u7b49\u9884\u5904\u7406\u6280\u672f\u7684\u6548\u679c\u3002\u540c\u65f6\u96c6\u6210\u773c\u52a8\u8ffd\u8e2a\u6280\u672f\u6765\u4f18\u5316\u5904\u7406\u6548\u7387\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5206\u8fa8\u7387\u548c\u8ddd\u79bb\u5bf9\u8bc6\u522b\u51c6\u786e\u7387\u6709\u663e\u8457\u5f71\u54cd\uff0c\u800c\u5149\u7167\u7684\u4f5c\u7528\u8f83\u96be\u9884\u6d4b\u3002\u56fe\u50cf\u4e0a\u91c7\u6837\u4f5c\u4e3a\u5173\u952e\u9884\u5904\u7406\u6280\u672f\uff0c\u80fd\u5c06\u5b57\u7b26\u9519\u8bef\u7387(CER)\u4ece0.65\u964d\u4f4e\u52300.48\u3002\u773c\u52a8\u8ffd\u8e2a\u6280\u672f\u901a\u8fc7\u805a\u7126\u7528\u6237\u6ce8\u610f\u533a\u57df\u5c55\u73b0\u51fa\u4f18\u5316\u5904\u7406\u6548\u7387\u7684\u6f5c\u529b\u3002", "conclusion": "\u672c\u5de5\u4f5c\u5728\u771f\u5b9e\u6761\u4ef6\u4e0b\u5bf9STDR\u6027\u80fd\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u5f00\u53d1\u81ea\u9002\u5e94\u3001\u7528\u6237\u611f\u77e5\u7684AR\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u7814\u7a76\u6210\u679c\u65e8\u5728\u63a8\u52a8\u9762\u5411\u8f85\u52a9\u5e94\u7528\u548c\u7814\u7a76\u5bfc\u5411\u5e94\u7528(\u5982\u8d44\u4ea7\u68c0\u67e5\u548c\u8425\u517b\u5206\u6790)\u7684\u9c81\u68d2\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u6587\u672c\u8bc6\u522b\u6280\u672f\u7684\u672a\u6765\u53d1\u5c55\u3002"}}
{"id": "2507.16341", "pdf": "https://arxiv.org/pdf/2507.16341", "abs": "https://arxiv.org/abs/2507.16341", "authors": ["Mingtao Guo", "Guanyu Xing", "Yanci Zhang", "Yanli Liu"], "title": "Navigating Large-Pose Challenge for High-Fidelity Face Reenactment with Video Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "Face reenactment aims to generate realistic talking head videos by transferring motion from a driving video to a static source image while preserving the source identity. Although existing methods based on either implicit or explicit keypoints have shown promise, they struggle with large pose variations due to warping artifacts or the limitations of coarse facial landmarks. In this paper, we present the Face Reenactment Video Diffusion model (FRVD), a novel framework for high-fidelity face reenactment under large pose changes. Our method first employs a motion extractor to extract implicit facial keypoints from the source and driving images to represent fine-grained motion and to perform motion alignment through a warping module. To address the degradation introduced by warping, we introduce a Warping Feature Mapper (WFM) that maps the warped source image into the motion-aware latent space of a pretrained image-to-video (I2V) model. This latent space encodes rich priors of facial dynamics learned from large-scale video data, enabling effective warping correction and enhancing temporal coherence. Extensive experiments show that FRVD achieves superior performance over existing methods in terms of pose accuracy, identity preservation, and visual quality, especially in challenging scenarios with extreme pose variations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9762\u90e8\u91cd\u6f14\u89c6\u9891\u6269\u6563\u6a21\u578b(FRVD)\uff0c\u901a\u8fc7\u8fd0\u52a8\u63d0\u53d6\u5668\u548c\u53d8\u5f62\u7279\u5f81\u6620\u5c04\u5668\u89e3\u51b3\u5927\u59ff\u6001\u53d8\u5316\u4e0b\u7684\u9762\u90e8\u91cd\u6f14\u95ee\u9898\uff0c\u5728\u59ff\u6001\u51c6\u786e\u6027\u3001\u8eab\u4efd\u4fdd\u6301\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u9762\u90e8\u91cd\u6f14\u65b9\u6cd5\u5728\u5904\u7406\u5927\u59ff\u6001\u53d8\u5316\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u57fa\u4e8e\u9690\u5f0f\u6216\u663e\u5f0f\u5173\u952e\u70b9\u7684\u65b9\u6cd5\u4f1a\u4ea7\u751f\u53d8\u5f62\u4f2a\u5f71\u6216\u53d7\u5230\u7c97\u7cd9\u9762\u90e8\u6807\u5fd7\u70b9\u7684\u9650\u5236\uff0c\u96be\u4ee5\u5728\u6781\u7aef\u59ff\u6001\u53d8\u5316\u4e0b\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u7684\u9762\u90e8\u91cd\u6f14\u6548\u679c\u3002", "method": "\u63d0\u51faFRVD\u6846\u67b6\uff0c\u5305\u542b\uff1a1)\u8fd0\u52a8\u63d0\u53d6\u5668\u4ece\u6e90\u56fe\u50cf\u548c\u9a71\u52a8\u56fe\u50cf\u4e2d\u63d0\u53d6\u9690\u5f0f\u9762\u90e8\u5173\u952e\u70b9\u6765\u8868\u793a\u7ec6\u7c92\u5ea6\u8fd0\u52a8\uff1b2)\u901a\u8fc7\u53d8\u5f62\u6a21\u5757\u8fdb\u884c\u8fd0\u52a8\u5bf9\u9f50\uff1b3)\u5f15\u5165\u53d8\u5f62\u7279\u5f81\u6620\u5c04\u5668(WFM)\u5c06\u53d8\u5f62\u540e\u7684\u6e90\u56fe\u50cf\u6620\u5c04\u5230\u9884\u8bad\u7ec3\u56fe\u50cf\u5230\u89c6\u9891\u6a21\u578b\u7684\u8fd0\u52a8\u611f\u77e5\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u5229\u7528\u5927\u89c4\u6a21\u89c6\u9891\u6570\u636e\u5b66\u4e60\u7684\u9762\u90e8\u52a8\u6001\u5148\u9a8c\u77e5\u8bc6\u3002", "result": "\u5728\u59ff\u6001\u51c6\u786e\u6027\u3001\u8eab\u4efd\u4fdd\u6301\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u5728\u6781\u7aef\u59ff\u6001\u53d8\u5316\u7684\u6311\u6218\u6027\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u7ea0\u6b63\u53d8\u5f62\u4f2a\u5f71\u5e76\u589e\u5f3a\u65f6\u95f4\u8fde\u8d2f\u6027\u3002", "conclusion": "FRVD\u901a\u8fc7\u7ed3\u5408\u8fd0\u52a8\u63d0\u53d6\u3001\u53d8\u5f62\u5bf9\u9f50\u548c\u7279\u5f81\u6620\u5c04\u6280\u672f\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5927\u59ff\u6001\u53d8\u5316\u4e0b\u7684\u9ad8\u4fdd\u771f\u9762\u90e8\u91cd\u6f14\u95ee\u9898\uff0c\u4e3a\u9762\u90e8\u91cd\u6f14\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16385", "pdf": "https://arxiv.org/pdf/2507.16385", "abs": "https://arxiv.org/abs/2507.16385", "authors": ["Kuo-Cheng Wu", "Guohang Zhuang", "Jinyang Huang", "Xiang Zhang", "Wanli Ouyang", "Yan Lu"], "title": "STAR: A Benchmark for Astronomical Star Fields Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Super-resolution (SR) advances astronomical imaging by enabling cost-effective high-resolution capture, crucial for detecting faraway celestial objects and precise structural analysis. However, existing datasets for astronomical SR (ASR) exhibit three critical limitations: flux inconsistency, object-crop setting, and insufficient data diversity, significantly impeding ASR development. We propose STAR, a large-scale astronomical SR dataset containing 54,738 flux-consistent star field image pairs covering wide celestial regions. These pairs combine Hubble Space Telescope high-resolution observations with physically faithful low-resolution counterparts generated through a flux-preserving data generation pipeline, enabling systematic development of field-level ASR models. To further empower the ASR community, STAR provides a novel Flux Error (FE) to evaluate SR models in physical view. Leveraging this benchmark, we propose a Flux-Invariant Super Resolution (FISR) model that could accurately infer the flux-consistent high-resolution images from input photometry, suppressing several SR state-of-the-art methods by 24.84% on a novel designed flux consistency metric, showing the priority of our method for astrophysics. Extensive experiments demonstrate the effectiveness of our proposed method and the value of our dataset. Code and models are available at https://github.com/GuoCheng12/STAR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86STAR\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b54,738\u5bf9\u901a\u91cf\u4e00\u81f4\u7684\u661f\u573a\u56fe\u50cf\u5bf9\u7684\u5927\u89c4\u6a21\u5929\u6587\u8d85\u5206\u8fa8\u7387\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u901a\u91cf\u4e0d\u53d8\u8d85\u5206\u8fa8\u7387(FISR)\u6a21\u578b\uff0c\u5728\u901a\u91cf\u4e00\u81f4\u6027\u6307\u6807\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534724.84%", "motivation": "\u73b0\u6709\u5929\u6587\u8d85\u5206\u8fa8\u7387\u6570\u636e\u96c6\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u9650\u5236\uff1a\u901a\u91cf\u4e0d\u4e00\u81f4\u3001\u5bf9\u8c61\u88c1\u526a\u8bbe\u7f6e\u548c\u6570\u636e\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u8fd9\u4e9b\u95ee\u9898\u4e25\u91cd\u963b\u788d\u4e86\u5929\u6587\u8d85\u5206\u8fa8\u7387\u6280\u672f\u7684\u53d1\u5c55", "method": "\u63d0\u51faSTAR\u6570\u636e\u96c6\uff0c\u5305\u542b\u54c8\u52c3\u7a7a\u95f4\u671b\u8fdc\u955c\u9ad8\u5206\u8fa8\u7387\u89c2\u6d4b\u6570\u636e\u548c\u901a\u8fc7\u901a\u91cf\u4fdd\u6301\u6570\u636e\u751f\u6210\u7ba1\u9053\u751f\u6210\u7684\u7269\u7406\u771f\u5b9e\u4f4e\u5206\u8fa8\u7387\u5bf9\u5e94\u56fe\u50cf\uff1b\u5f15\u5165\u901a\u91cf\u8bef\u5dee(FE)\u8bc4\u4f30\u6307\u6807\uff1b\u5f00\u53d1\u901a\u91cf\u4e0d\u53d8\u8d85\u5206\u8fa8\u7387(FISR)\u6a21\u578b", "result": "STAR\u6570\u636e\u96c6\u5305\u542b54,738\u5bf9\u901a\u91cf\u4e00\u81f4\u7684\u661f\u573a\u56fe\u50cf\u5bf9\uff0c\u8986\u76d6\u5e7f\u6cdb\u5929\u4f53\u533a\u57df\uff1bFISR\u6a21\u578b\u5728\u901a\u91cf\u4e00\u81f4\u6027\u6307\u6807\u4e0a\u6bd4\u6700\u5148\u8fdb\u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u63d0\u534724.84%\uff1b\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6570\u636e\u96c6\u7684\u4ef7\u503c", "conclusion": "\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u901a\u91cf\u4e00\u81f4\u7684\u5929\u6587\u8d85\u5206\u8fa8\u7387\u6570\u636e\u96c6STAR\u548c\u5f00\u53d1FISR\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5929\u6587\u8d85\u5206\u8fa8\u7387\u7684\u6027\u80fd\uff0c\u4e3a\u5929\u4f53\u7269\u7406\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u4f18\u5148\u7ea7\u66f4\u9ad8\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.16406", "pdf": "https://arxiv.org/pdf/2507.16406", "abs": "https://arxiv.org/abs/2507.16406", "authors": ["Tanveer Younis", "Zhanglin Cheng"], "title": "Sparse-View 3D Reconstruction: Recent Advances and Open Challenges", "categories": ["cs.CV"], "comment": "30 pages, 6 figures", "summary": "Sparse-view 3D reconstruction is essential for applications in which dense image acquisition is impractical, such as robotics, augmented/virtual reality (AR/VR), and autonomous systems. In these settings, minimal image overlap prevents reliable correspondence matching, causing traditional methods, such as structure-from-motion (SfM) and multiview stereo (MVS), to fail. This survey reviews the latest advances in neural implicit models (e.g., NeRF and its regularized versions), explicit point-cloud-based approaches (e.g., 3D Gaussian Splatting), and hybrid frameworks that leverage priors from diffusion and vision foundation models (VFMs).We analyze how geometric regularization, explicit shape modeling, and generative inference are used to mitigate artifacts such as floaters and pose ambiguities in sparse-view settings. Comparative results on standard benchmarks reveal key trade-offs between the reconstruction accuracy, efficiency, and generalization. Unlike previous reviews, our survey provides a unified perspective on geometry-based, neural implicit, and generative (diffusion-based) methods. We highlight the persistent challenges in domain generalization and pose-free reconstruction and outline future directions for developing 3D-native generative priors and achieving real-time, unconstrained sparse-view reconstruction.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u7cfb\u7edf\u56de\u987e\u4e86\u7a00\u758f\u89c6\u89d23D\u91cd\u5efa\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5305\u62ec\u795e\u7ecf\u9690\u5f0f\u6a21\u578b(\u5982NeRF)\u3001\u663e\u5f0f\u70b9\u4e91\u65b9\u6cd5(\u59823DGS)\u548c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u7684\u6df7\u5408\u6846\u67b6\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u5728\u89e3\u51b3\u7a00\u758f\u89c6\u89d2\u91cd\u5efa\u6311\u6218\u65b9\u9762\u7684\u4f18\u7f3a\u70b9\u548c\u6743\u8861\u5173\u7cfb\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u6280\u672f\u3001AR/VR\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u5e94\u7528\u4e2d\uff0c\u5bc6\u96c6\u56fe\u50cf\u91c7\u96c6\u5f80\u5f80\u4e0d\u73b0\u5b9e\uff0c\u7a00\u758f\u89c6\u89d2\u4e0b\u56fe\u50cf\u91cd\u53e0\u5ea6\u6781\u5c0f\u5bfc\u81f4\u4f20\u7edfSfM\u548cMVS\u65b9\u6cd5\u5931\u6548\uff0c\u56e0\u6b64\u9700\u8981\u7efc\u8ff0\u548c\u5206\u6790\u80fd\u591f\u5904\u7406\u7a00\u758f\u89c6\u89d23D\u91cd\u5efa\u95ee\u9898\u7684\u65b0\u5174\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u7efc\u8ff0\u7814\u7a76\u65b9\u6cd5\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u4e09\u7c7b\u4e3b\u8981\u65b9\u6cd5\uff1a1)\u795e\u7ecf\u9690\u5f0f\u6a21\u578b(NeRF\u53ca\u5176\u6b63\u5219\u5316\u7248\u672c)\uff1b2)\u663e\u5f0f\u70b9\u4e91\u65b9\u6cd5(\u59823D\u9ad8\u65af\u6e85\u5c04)\uff1b3)\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5148\u9a8c\u7684\u6df7\u5408\u6846\u67b6\u3002\u91cd\u70b9\u5206\u6790\u51e0\u4f55\u6b63\u5219\u5316\u3001\u663e\u5f0f\u5f62\u72b6\u5efa\u6a21\u548c\u751f\u6210\u63a8\u7406\u5982\u4f55\u7f13\u89e3\u7a00\u758f\u89c6\u89d2\u8bbe\u7f6e\u4e2d\u7684\u6f02\u6d6e\u7269\u548c\u59ff\u6001\u6b67\u4e49\u7b49\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u7684\u5bf9\u6bd4\u7ed3\u679c\uff0c\u63ed\u793a\u4e86\u91cd\u5efa\u7cbe\u5ea6\u3001\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u4e4b\u95f4\u7684\u5173\u952e\u6743\u8861\u5173\u7cfb\u3002\u4e0e\u4ee5\u5f80\u7efc\u8ff0\u4e0d\u540c\uff0c\u672c\u6587\u63d0\u4f9b\u4e86\u57fa\u4e8e\u51e0\u4f55\u3001\u795e\u7ecf\u9690\u5f0f\u548c\u751f\u6210(\u57fa\u4e8e\u6269\u6563)\u65b9\u6cd5\u7684\u7edf\u4e00\u89c6\u89d2\uff0c\u8bc6\u522b\u4e86\u9886\u57df\u6cdb\u5316\u548c\u65e0\u59ff\u6001\u91cd\u5efa\u7b49\u6301\u7eed\u6311\u6218\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u5f00\u53d13D\u539f\u751f\u751f\u6210\u5148\u9a8c\u548c\u5b9e\u73b0\u5b9e\u65f6\u3001\u65e0\u7ea6\u675f\u7a00\u758f\u89c6\u89d2\u91cd\u5efa\u7684\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u8def\u7ebf\u56fe\u548c\u6307\u5bfc\u3002"}}
{"id": "2507.16429", "pdf": "https://arxiv.org/pdf/2507.16429", "abs": "https://arxiv.org/abs/2507.16429", "authors": ["Lin Xi", "Yingliang Ma", "Cheng Wang", "Sandra Howell", "Aldo Rinaldi", "Kawal S. Rhode"], "title": "Robust Noisy Pseudo-label Learning for Semi-supervised Medical Image Segmentation Using Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "Obtaining pixel-level annotations in the medical domain is both expensive and time-consuming, often requiring close collaboration between clinical experts and developers. Semi-supervised medical image segmentation aims to leverage limited annotated data alongside abundant unlabeled data to achieve accurate segmentation. However, existing semi-supervised methods often struggle to structure semantic distributions in the latent space due to noise introduced by pseudo-labels. In this paper, we propose a novel diffusion-based framework for semi-supervised medical image segmentation. Our method introduces a constraint into the latent structure of semantic labels during the denoising diffusion process by enforcing prototype-based contrastive consistency. Rather than explicitly delineating semantic boundaries, the model leverages class prototypes centralized semantic representations in the latent space as anchors. This strategy improves the robustness of dense predictions, particularly in the presence of noisy pseudo-labels. We also introduce a new publicly available benchmark: Multi-Object Segmentation in X-ray Angiography Videos (MOSXAV), which provides detailed, manually annotated segmentation ground truth for multiple anatomical structures in X-ray angiography videos. Extensive experiments on the EndoScapes2023 and MOSXAV datasets demonstrate that our method outperforms state-of-the-art medical image segmentation approaches under the semi-supervised learning setting. This work presents a robust and data-efficient diffusion model that offers enhanced flexibility and strong potential for a wide range of clinical applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u539f\u578b\u5bf9\u6bd4\u4e00\u81f4\u6027\u7ea6\u675f\u6539\u5584\u8bed\u4e49\u5206\u5e03\u7ed3\u6784\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684X\u5c04\u7ebf\u8840\u7ba1\u9020\u5f71\u89c6\u9891\u591a\u76ee\u6807\u5206\u5272\u57fa\u51c6\u6570\u636e\u96c6MOSXAV\u3002", "motivation": "\u533b\u5b66\u9886\u57df\u50cf\u7d20\u7ea7\u6807\u6ce8\u65e2\u6602\u8d35\u53c8\u8017\u65f6\uff0c\u9700\u8981\u4e34\u5e8a\u4e13\u5bb6\u4e0e\u5f00\u53d1\u8005\u5bc6\u5207\u5408\u4f5c\u3002\u73b0\u6709\u534a\u76d1\u7763\u65b9\u6cd5\u7531\u4e8e\u4f2a\u6807\u7b7e\u566a\u58f0\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u96be\u4ee5\u6784\u5efa\u826f\u597d\u7684\u8bed\u4e49\u5206\u5e03\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6846\u67b6\uff0c\u5728\u53bb\u566a\u6269\u6563\u8fc7\u7a0b\u4e2d\u901a\u8fc7\u5f3a\u5236\u6267\u884c\u57fa\u4e8e\u539f\u578b\u7684\u5bf9\u6bd4\u4e00\u81f4\u6027\u6765\u7ea6\u675f\u8bed\u4e49\u6807\u7b7e\u7684\u6f5c\u5728\u7ed3\u6784\u3002\u4f7f\u7528\u7c7b\u539f\u578b\u4f5c\u4e3a\u6f5c\u5728\u7a7a\u95f4\u4e2d\u96c6\u4e2d\u8bed\u4e49\u8868\u793a\u7684\u951a\u70b9\uff0c\u800c\u975e\u663e\u5f0f\u5212\u5b9a\u8bed\u4e49\u8fb9\u754c\u3002", "result": "\u5728EndoScapes2023\u548cMOSXAV\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u534a\u76d1\u7763\u5b66\u4e60\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u3002\u540c\u65f6\u5f15\u5165\u4e86\u65b0\u7684\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6MOSXAV\uff0c\u4e3aX\u5c04\u7ebf\u8840\u7ba1\u9020\u5f71\u89c6\u9891\u4e2d\u7684\u591a\u4e2a\u89e3\u5256\u7ed3\u6784\u63d0\u4f9b\u8be6\u7ec6\u7684\u624b\u52a8\u6807\u6ce8\u5206\u5272\u771f\u503c\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9c81\u68d2\u4e14\u6570\u636e\u9ad8\u6548\u7684\u6269\u6563\u6a21\u578b\uff0c\u63d0\u4f9b\u4e86\u589e\u5f3a\u7684\u7075\u6d3b\u6027\uff0c\u5728\u5e7f\u6cdb\u7684\u4e34\u5e8a\u5e94\u7528\u4e2d\u5177\u6709\u5f3a\u5927\u6f5c\u529b\u3002\u8be5\u65b9\u6cd5\u7279\u522b\u5728\u5b58\u5728\u566a\u58f0\u4f2a\u6807\u7b7e\u65f6\u6539\u5584\u4e86\u5bc6\u96c6\u9884\u6d4b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.16472", "pdf": "https://arxiv.org/pdf/2507.16472", "abs": "https://arxiv.org/abs/2507.16472", "authors": ["Yu-Fan Lin", "Chia-Ming Lee", "Chih-Chung Hsu"], "title": "DenseSR: Image Shadow Removal as Dense Prediction", "categories": ["cs.CV"], "comment": "Paper accepted to ACMMM 2025", "summary": "Shadows are a common factor degrading image quality. Single-image shadow removal (SR), particularly under challenging indirect illumination, is hampered by non-uniform content degradation and inherent ambiguity. Consequently, traditional methods often fail to simultaneously recover intra-shadow details and maintain sharp boundaries, resulting in inconsistent restoration and blurring that negatively affect both downstream applications and the overall viewing experience. To overcome these limitations, we propose the DenseSR, approaching the problem from a dense prediction perspective to emphasize restoration quality. This framework uniquely synergizes two key strategies: (1) deep scene understanding guided by geometric-semantic priors to resolve ambiguity and implicitly localize shadows, and (2) high-fidelity restoration via a novel Dense Fusion Block (DFB) in the decoder. The DFB employs adaptive component processing-using an Adaptive Content Smoothing Module (ACSM) for consistent appearance and a Texture-Boundary Recuperation Module (TBRM) for fine textures and sharp boundaries-thereby directly tackling the inconsistent restoration and blurring issues. These purposefully processed components are effectively fused, yielding an optimized feature representation preserving both consistency and fidelity. Extensive experimental results demonstrate the merits of our approach over existing methods. Our code can be available on https://github$.$com/VanLinLin/DenseSR", "AI": {"tldr": "\u63d0\u51fa\u4e86DenseSR\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u8bed\u4e49\u5148\u9a8c\u548cDense Fusion Block\u89e3\u51b3\u5355\u56fe\u50cf\u9634\u5f71\u53bb\u9664\u4e2d\u7684\u5185\u5bb9\u964d\u89e3\u4e0d\u5747\u5300\u548c\u56fa\u6709\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u9634\u5f71\u53bb\u9664", "motivation": "\u4f20\u7edf\u5355\u56fe\u50cf\u9634\u5f71\u53bb\u9664\u65b9\u6cd5\u5728\u95f4\u63a5\u5149\u7167\u7b49\u6311\u6218\u6027\u6761\u4ef6\u4e0b\uff0c\u7531\u4e8e\u975e\u5747\u5300\u5185\u5bb9\u964d\u89e3\u548c\u56fa\u6709\u6a21\u7cca\u6027\uff0c\u65e0\u6cd5\u540c\u65f6\u6062\u590d\u9634\u5f71\u5185\u90e8\u7ec6\u8282\u548c\u4fdd\u6301\u6e05\u6670\u8fb9\u754c\uff0c\u5bfc\u81f4\u6062\u590d\u4e0d\u4e00\u81f4\u548c\u6a21\u7cca\u95ee\u9898\uff0c\u5f71\u54cd\u4e0b\u6e38\u5e94\u7528\u548c\u89c6\u89c9\u4f53\u9a8c", "method": "\u63d0\u51faDenseSR\u6846\u67b6\uff0c\u91c7\u7528\u5bc6\u96c6\u9884\u6d4b\u89c6\u89d2\uff0c\u7ed3\u5408\u4e24\u4e2a\u5173\u952e\u7b56\u7565\uff1a(1)\u5229\u7528\u51e0\u4f55\u8bed\u4e49\u5148\u9a8c\u8fdb\u884c\u6df1\u5ea6\u573a\u666f\u7406\u89e3\u6765\u89e3\u51b3\u6a21\u7cca\u6027\u5e76\u9690\u5f0f\u5b9a\u4f4d\u9634\u5f71\uff1b(2)\u901a\u8fc7\u89e3\u7801\u5668\u4e2d\u7684Dense Fusion Block\u5b9e\u73b0\u9ad8\u4fdd\u771f\u6062\u590d\uff0cDFB\u5305\u542b\u81ea\u9002\u5e94\u5185\u5bb9\u5e73\u6ed1\u6a21\u5757(ACSM)\u548c\u7eb9\u7406\u8fb9\u754c\u6062\u590d\u6a21\u5757(TBRM)\uff0c\u5206\u522b\u5904\u7406\u5916\u89c2\u4e00\u81f4\u6027\u548c\u7cbe\u7ec6\u7eb9\u7406\u8fb9\u754c", "result": "\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u4f18\u52bf\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4e0d\u4e00\u81f4\u6062\u590d\u548c\u6a21\u7cca\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u4e00\u81f4\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u6062\u590d\u4fdd\u771f\u5ea6", "conclusion": "DenseSR\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u8bed\u4e49\u5148\u9a8c\u7684\u6df1\u5ea6\u573a\u666f\u7406\u89e3\u548cDense Fusion Block\u7684\u81ea\u9002\u5e94\u7ec4\u4ef6\u5904\u7406\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5355\u56fe\u50cf\u9634\u5f71\u53bb\u9664\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u9634\u5f71\u53bb\u9664\u6548\u679c"}}
{"id": "2507.16608", "pdf": "https://arxiv.org/pdf/2507.16608", "abs": "https://arxiv.org/abs/2507.16608", "authors": ["Xueming Fu", "Pei Wu", "Yingtai Li", "Xin Luo", "Zihang Jiang", "Junhao Mei", "Jian Lu", "Gao-Jun Teng", "S. Kevin Zhou"], "title": "Dyna3DGR: 4D Cardiac Motion Tracking with Dynamic 3D Gaussian Representation", "categories": ["cs.CV"], "comment": "Accepted to MICCAI 2025", "summary": "Accurate analysis of cardiac motion is crucial for evaluating cardiac function. While dynamic cardiac magnetic resonance imaging (CMR) can capture detailed tissue motion throughout the cardiac cycle, the fine-grained 4D cardiac motion tracking remains challenging due to the homogeneous nature of myocardial tissue and the lack of distinctive features. Existing approaches can be broadly categorized into image based and representation-based, each with its limitations. Image-based methods, including both raditional and deep learning-based registration approaches, either struggle with topological consistency or rely heavily on extensive training data. Representation-based methods, while promising, often suffer from loss of image-level details. To address these limitations, we propose Dynamic 3D Gaussian Representation (Dyna3DGR), a novel framework that combines explicit 3D Gaussian representation with implicit neural motion field modeling. Our method simultaneously optimizes cardiac structure and motion in a self-supervised manner, eliminating the need for extensive training data or point-to-point correspondences. Through differentiable volumetric rendering, Dyna3DGR efficiently bridges continuous motion representation with image-space alignment while preserving both topological and temporal consistency. Comprehensive evaluations on the ACDC dataset demonstrate that our approach surpasses state-of-the-art deep learning-based diffeomorphic registration methods in tracking accuracy. The code will be available in https://github.com/windrise/Dyna3DGR.", "AI": {"tldr": "\u63d0\u51fa\u4e86Dynamic 3D Gaussian Representation (Dyna3DGR)\u6846\u67b6\uff0c\u7ed3\u5408\u663e\u5f0f3D\u9ad8\u65af\u8868\u793a\u548c\u9690\u5f0f\u795e\u7ecf\u8fd0\u52a8\u573a\u5efa\u6a21\uff0c\u7528\u4e8e\u7cbe\u786e\u7684\u5fc3\u810f\u8fd0\u52a8\u5206\u6790\uff0c\u5728ACDC\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u5fc3\u810f\u8fd0\u52a8\u5206\u6790\u5bf9\u8bc4\u4f30\u5fc3\u810f\u529f\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5fc3\u808c\u7ec4\u7ec7\u7684\u540c\u8d28\u6027\u548c\u7f3a\u4e4f\u7279\u5f81\u6027\u6807\u8bb0\uff0c\u73b0\u6709\u7684\u57fa\u4e8e\u56fe\u50cf\u548c\u57fa\u4e8e\u8868\u793a\u7684\u65b9\u6cd5\u90fd\u5b58\u5728\u5c40\u9650\u6027\uff1a\u56fe\u50cf\u65b9\u6cd5\u5728\u62d3\u6251\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\u6216\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u8868\u793a\u65b9\u6cd5\u5bb9\u6613\u4e22\u5931\u56fe\u50cf\u7ea7\u7ec6\u8282\u3002", "method": "\u63d0\u51faDyna3DGR\u6846\u67b6\uff0c\u7ed3\u5408\u663e\u5f0f3D\u9ad8\u65af\u8868\u793a\u548c\u9690\u5f0f\u795e\u7ecf\u8fd0\u52a8\u573a\u5efa\u6a21\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u76d1\u7763\u65b9\u5f0f\u540c\u65f6\u4f18\u5316\u5fc3\u810f\u7ed3\u6784\u548c\u8fd0\u52a8\uff0c\u91c7\u7528\u53ef\u5fae\u5206\u4f53\u79ef\u6e32\u67d3\u6280\u672f\uff0c\u5728\u4fdd\u6301\u62d3\u6251\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u540c\u65f6\uff0c\u6709\u6548\u8fde\u63a5\u8fde\u7eed\u8fd0\u52a8\u8868\u793a\u4e0e\u56fe\u50cf\u7a7a\u95f4\u5bf9\u9f50\u3002", "result": "\u5728ACDC\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u8ddf\u8e2a\u7cbe\u5ea6\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5fae\u5206\u540c\u80da\u914d\u51c6\u65b9\u6cd5\uff0c\u65e0\u9700\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u6216\u70b9\u5bf9\u70b9\u5bf9\u5e94\u5173\u7cfb\u3002", "conclusion": "Dyna3DGR\u6210\u529f\u89e3\u51b3\u4e86\u5fc3\u810f\u8fd0\u52a8\u8ddf\u8e2a\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u7ed3\u5408\u663e\u5f0f\u548c\u9690\u5f0f\u8868\u793a\u7684\u4f18\u52bf\uff0c\u5728\u4fdd\u6301\u62d3\u6251\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u4e3a\u5fc3\u810f\u529f\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2507.16732", "pdf": "https://arxiv.org/pdf/2507.16732", "abs": "https://arxiv.org/abs/2507.16732", "authors": ["Ying Li", "Xinzhe Li", "Yong Du", "Yangyang Xu", "Junyu Dong", "Shengfeng He"], "title": "HarmonPaint: Harmonized Training-Free Diffusion Inpainting", "categories": ["cs.CV"], "comment": null, "summary": "Existing inpainting methods often require extensive retraining or fine-tuning to integrate new content seamlessly, yet they struggle to maintain coherence in both structure and style between inpainted regions and the surrounding background. Motivated by these limitations, we introduce HarmonPaint, a training-free inpainting framework that seamlessly integrates with the attention mechanisms of diffusion models to achieve high-quality, harmonized image inpainting without any form of training. By leveraging masking strategies within self-attention, HarmonPaint ensures structural fidelity without model retraining or fine-tuning. Additionally, we exploit intrinsic diffusion model properties to transfer style information from unmasked to masked regions, achieving a harmonious integration of styles. Extensive experiments demonstrate the effectiveness of HarmonPaint across diverse scenes and styles, validating its versatility and performance.", "AI": {"tldr": "HarmonPaint\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u56fe\u50cf\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u4e0d\u8fdb\u884c\u4efb\u4f55\u8bad\u7ec3\u6216\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u548c\u8c10\u7684\u56fe\u50cf\u4fee\u590d\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\u624d\u80fd\u65e0\u7f1d\u96c6\u6210\u65b0\u5185\u5bb9\uff0c\u4f46\u5728\u4fdd\u6301\u4fee\u590d\u533a\u57df\u4e0e\u5468\u56f4\u80cc\u666f\u5728\u7ed3\u6784\u548c\u98ce\u683c\u4e0a\u7684\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "HarmonPaint\u901a\u8fc7\u5728\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u4f7f\u7528\u63a9\u7801\u7b56\u7565\u6765\u786e\u4fdd\u7ed3\u6784\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u5185\u5728\u5c5e\u6027\u5c06\u98ce\u683c\u4fe1\u606f\u4ece\u672a\u63a9\u7801\u533a\u57df\u4f20\u9012\u5230\u63a9\u7801\u533a\u57df\uff0c\u5b9e\u73b0\u98ce\u683c\u7684\u548c\u8c10\u878d\u5408\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86HarmonPaint\u5728\u4e0d\u540c\u573a\u666f\u548c\u98ce\u683c\u4e0b\u7684\u6709\u6548\u6027\uff0c\u9a8c\u8bc1\u4e86\u5176\u591a\u529f\u80fd\u6027\u548c\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "HarmonPaint\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u4fee\u590d\uff0c\u5728\u7ed3\u6784\u4fdd\u771f\u5ea6\u548c\u98ce\u683c\u548c\u8c10\u6027\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u56fe\u50cf\u4fee\u590d\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16743", "pdf": "https://arxiv.org/pdf/2507.16743", "abs": "https://arxiv.org/abs/2507.16743", "authors": ["Keneni W. Tesema", "Lyndon Hill", "Mark W. Jones", "Gary K. L. Tam"], "title": "Denoising-While-Completing Network (DWCNet): Robust Point Cloud Completion Under Corruption", "categories": ["cs.CV"], "comment": "Accepted for Computers and Graphics and EG Symposium on 3D Object   Retrieval 2025 (3DOR'25)", "summary": "Point cloud completion is crucial for 3D computer vision tasks in autonomous driving, augmented reality, and robotics. However, obtaining clean and complete point clouds from real-world environments is challenging due to noise and occlusions. Consequently, most existing completion networks -- trained on synthetic data -- struggle with real-world degradations. In this work, we tackle the problem of completing and denoising highly corrupted partial point clouds affected by multiple simultaneous degradations. To benchmark robustness, we introduce the Corrupted Point Cloud Completion Dataset (CPCCD), which highlights the limitations of current methods under diverse corruptions. Building on these insights, we propose DWCNet (Denoising-While-Completing Network), a completion framework enhanced with a Noise Management Module (NMM) that leverages contrastive learning and self-attention to suppress noise and model structural relationships. DWCNet achieves state-of-the-art performance on both clean and corrupted, synthetic and real-world datasets. The dataset and code will be publicly available at https://github.com/keneniwt/DWCNET-Robust-Point-Cloud-Completion-against-Corruptions", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DWCNet\u7f51\u7edc\u6765\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u4e2d\u566a\u58f0\u548c\u906e\u6321\u5bfc\u81f4\u7684\u70b9\u4e91\u8865\u5168\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e86CPCCD\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u65b9\u6cd5\u7684\u9c81\u68d2\u6027", "motivation": "\u73b0\u6709\u7684\u70b9\u4e91\u8865\u5168\u7f51\u7edc\u4e3b\u8981\u5728\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5728\u9762\u5bf9\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u566a\u58f0\u548c\u906e\u6321\u7b49\u591a\u79cd\u9000\u5316\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u5904\u7406\u70b9\u4e91\u8865\u5168\u548c\u53bb\u566a\u7684\u9c81\u68d2\u65b9\u6cd5", "method": "\u63d0\u51faDWCNet\uff08\u8fb9\u53bb\u566a\u8fb9\u8865\u5168\u7f51\u7edc\uff09\uff0c\u5305\u542b\u566a\u58f0\u7ba1\u7406\u6a21\u5757\uff08NMM\uff09\uff0c\u8be5\u6a21\u5757\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u6291\u5236\u566a\u58f0\u5e76\u5efa\u6a21\u7ed3\u6784\u5173\u7cfb\uff1b\u540c\u65f6\u5f15\u5165CPCCD\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u4e0d\u540c\u9000\u5316\u6761\u4ef6\u4e0b\u7684\u65b9\u6cd5\u6027\u80fd", "result": "DWCNet\u5728\u5e72\u51c0\u548c\u635f\u574f\u7684\u5408\u6210\u6570\u636e\u96c6\u4ee5\u53ca\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5904\u7406\u591a\u79cd\u540c\u65f6\u9000\u5316\u7684\u70b9\u4e91\u8865\u5168\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027", "conclusion": "DWCNet\u6210\u529f\u89e3\u51b3\u4e86\u771f\u5b9e\u4e16\u754c\u70b9\u4e91\u8865\u5168\u4e2d\u7684\u566a\u58f0\u548c\u906e\u6321\u95ee\u9898\uff0c\u901a\u8fc7\u566a\u58f0\u7ba1\u7406\u6a21\u5757\u6709\u6548\u63d0\u5347\u4e86\u8865\u5168\u7f51\u7edc\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u3001\u589e\u5f3a\u73b0\u5b9e\u548c\u673a\u5668\u4eba\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u76843D\u89c6\u89c9\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.15894", "pdf": "https://arxiv.org/pdf/2507.15894", "abs": "https://arxiv.org/abs/2507.15894", "authors": ["Shahar Zuler", "Gal Lifshitz", "Hadar Averbuch-Elor", "Dan Raviv"], "title": "Systole-Conditioned Generative Cardiac Motion", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Accurate motion estimation in cardiac computed tomography (CT) imaging is critical for assessing cardiac function and surgical planning. Data-driven methods have become the standard approach for dense motion estimation, but they rely on vast amounts of labeled data with dense ground-truth (GT) motion annotations, which are often unfeasible to obtain. To address this limitation, we present a novel approach that synthesizes realistically looking pairs of cardiac CT frames enriched with dense 3D flow field annotations.   Our method leverages a conditional Variational Autoencoder (CVAE), which incorporates a novel multi-scale feature conditioning mechanism and is trained to generate 3D flow fields conditioned on a single CT frame. By applying the generated flow field to warp the given frame, we create pairs of frames that simulate realistic myocardium deformations across the cardiac cycle. These pairs serve as fully annotated data samples, providing optical flow GT annotations. Our data generation pipeline could enable the training and validation of more complex and accurate myocardium motion models, allowing for substantially reducing reliance on manual annotations.   Our code, along with animated generated samples and additional material, is available on our project page: https://shaharzuler.github.io/GenerativeCardiacMotion_Page.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668(CVAE)\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5408\u6210\u5e26\u6709\u5bc6\u96c63D\u6d41\u573a\u6807\u6ce8\u7684\u5fc3\u810fCT\u56fe\u50cf\u5bf9\uff0c\u4ee5\u89e3\u51b3\u5fc3\u810f\u8fd0\u52a8\u4f30\u8ba1\u4e2d\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898", "motivation": "\u5fc3\u810fCT\u6210\u50cf\u4e2d\u51c6\u786e\u7684\u8fd0\u52a8\u4f30\u8ba1\u5bf9\u4e8e\u8bc4\u4f30\u5fc3\u810f\u529f\u80fd\u548c\u624b\u672f\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6570\u636e\u9a71\u52a8\u7684\u5bc6\u96c6\u8fd0\u52a8\u4f30\u8ba1\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u5e26\u6709\u5bc6\u96c6\u771f\u503c\u8fd0\u52a8\u6807\u6ce8\u7684\u6807\u8bb0\u6570\u636e\uff0c\u800c\u8fd9\u4e9b\u6570\u636e\u5f80\u5f80\u96be\u4ee5\u83b7\u5f97", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668(CVAE)\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u65b0\u9896\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u6761\u4ef6\u673a\u5236\uff0c\u8bad\u7ec3\u751f\u6210\u4ee5\u5355\u5e27CT\u56fe\u50cf\u4e3a\u6761\u4ef6\u76843D\u6d41\u573a\u3002\u901a\u8fc7\u5c06\u751f\u6210\u7684\u6d41\u573a\u5e94\u7528\u4e8e\u626d\u66f2\u7ed9\u5b9a\u5e27\uff0c\u521b\u5efa\u6a21\u62df\u5fc3\u808c\u5728\u5fc3\u52a8\u5468\u671f\u4e2d\u771f\u5b9e\u53d8\u5f62\u7684\u56fe\u50cf\u5bf9", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u5177\u6709\u771f\u5b9e\u5916\u89c2\u7684\u5fc3\u810fCT\u56fe\u50cf\u5bf9\uff0c\u5e76\u914d\u6709\u5bc6\u96c6\u76843D\u6d41\u573a\u6807\u6ce8\uff0c\u8fd9\u4e9b\u56fe\u50cf\u5bf9\u53ef\u4f5c\u4e3a\u5b8c\u5168\u6807\u6ce8\u7684\u6570\u636e\u6837\u672c\uff0c\u63d0\u4f9b\u5149\u6d41\u771f\u503c\u6807\u6ce8", "conclusion": "\u8be5\u6570\u636e\u751f\u6210\u7ba1\u9053\u80fd\u591f\u652f\u6301\u8bad\u7ec3\u548c\u9a8c\u8bc1\u66f4\u590d\u6742\u3001\u66f4\u51c6\u786e\u7684\u5fc3\u808c\u8fd0\u52a8\u6a21\u578b\uff0c\u663e\u8457\u51cf\u5c11\u5bf9\u4eba\u5de5\u6807\u6ce8\u7684\u4f9d\u8d56"}}
{"id": "2507.16302", "pdf": "https://arxiv.org/pdf/2507.16302", "abs": "https://arxiv.org/abs/2507.16302", "authors": ["Boheng Li", "Renjie Gu", "Junjie Wang", "Leyi Qi", "Yiming Li", "Run Wang", "Zhan Qin", "Tianwei Zhang"], "title": "Towards Resilient Safety-driven Unlearning for Diffusion Models against Downstream Fine-tuning", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV"], "comment": "Preprint version. Under review", "summary": "Text-to-image (T2I) diffusion models have achieved impressive image generation quality and are increasingly fine-tuned for personalized applications. However, these models often inherit unsafe behaviors from toxic pretraining data, raising growing safety concerns. While recent safety-driven unlearning methods have made promising progress in suppressing model toxicity, they are identified to be fragile to downstream fine-tuning, where we reveal that state-of-the-art methods largely fail to retain their effectiveness even when fine-tuned on entirely benign datasets. To mitigate this problem, in this paper, we propose ResAlign, a safety-driven unlearning framework with enhanced resilience against downstream fine-tuning. By modeling downstream fine-tuning as an implicit optimization problem with a Moreau Envelope-based reformulation, ResAlign enables efficient gradient estimation to minimize the recovery of harmful behaviors. Additionally, a meta-learning strategy is proposed to simulate a diverse distribution of fine-tuning scenarios to improve generalization. Extensive experiments across a wide range of datasets, fine-tuning methods, and configurations demonstrate that ResAlign consistently outperforms prior unlearning approaches in retaining safety after downstream fine-tuning while preserving benign generation capability well.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ResAlign\u6846\u67b6\uff0c\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u4e2a\u6027\u5316\u5fae\u8c03\u540e\u5b89\u5168\u6027\u5931\u6548\u7684\u95ee\u9898\uff0c\u901a\u8fc7Moreau\u5305\u7edc\u4f18\u5316\u548c\u5143\u5b66\u4e60\u7b56\u7565\u589e\u5f3a\u5b89\u5168\u9057\u5fd8\u65b9\u6cd5\u5bf9\u4e0b\u6e38\u5fae\u8c03\u7684\u6297\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u867d\u7136\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u5f88\u9ad8\uff0c\u4f46\u4ece\u6709\u6bd2\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u7ee7\u627f\u4e86\u4e0d\u5b89\u5168\u884c\u4e3a\u3002\u867d\u7136\u5b89\u5168\u9a71\u52a8\u7684\u9057\u5fd8\u65b9\u6cd5\u5728\u6291\u5236\u6a21\u578b\u6bd2\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5728\u4e0b\u6e38\u5fae\u8c03\u65f6\u8868\u73b0\u8106\u5f31\uff0c\u5373\u4f7f\u5728\u5b8c\u5168\u826f\u6027\u7684\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u4e5f\u4f1a\u5931\u53bb\u6709\u6548\u6027\u3002", "method": "\u63d0\u51faResAlign\u6846\u67b6\uff0c\u5c06\u4e0b\u6e38\u5fae\u8c03\u5efa\u6a21\u4e3a\u5e26Moreau\u5305\u7edc\u91cd\u65b0\u8868\u8ff0\u7684\u9690\u5f0f\u4f18\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u68af\u5ea6\u4f30\u8ba1\u4ee5\u6700\u5c0f\u5316\u6709\u5bb3\u884c\u4e3a\u7684\u6062\u590d\u3002\u540c\u65f6\u91c7\u7528\u5143\u5b66\u4e60\u7b56\u7565\u6a21\u62df\u591a\u6837\u5316\u7684\u5fae\u8c03\u573a\u666f\u5206\u5e03\u4ee5\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u6570\u636e\u96c6\u3001\u5fae\u8c03\u65b9\u6cd5\u548c\u914d\u7f6e\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cResAlign\u5728\u4fdd\u6301\u4e0b\u6e38\u5fae\u8c03\u540e\u7684\u5b89\u5168\u6027\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u5148\u524d\u7684\u9057\u5fd8\u65b9\u6cd5\uff0c\u540c\u65f6\u5f88\u597d\u5730\u4fdd\u6301\u4e86\u826f\u6027\u751f\u6210\u80fd\u529b\u3002", "conclusion": "ResAlign\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5b89\u5168\u9057\u5fd8\u65b9\u6cd5\u5728\u4e0b\u6e38\u5fae\u8c03\u4e2d\u7684\u8106\u5f31\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u4f18\u5316\u7b56\u7565\u548c\u5143\u5b66\u4e60\u65b9\u6cd5\uff0c\u6210\u529f\u589e\u5f3a\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5b89\u5168\u6027\u7684\u6301\u4e45\u6027\uff0c\u4e3a\u4e2a\u6027\u5316AI\u5e94\u7528\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u4fdd\u969c\u3002"}}
{"id": "2507.16360", "pdf": "https://arxiv.org/pdf/2507.16360", "abs": "https://arxiv.org/abs/2507.16360", "authors": ["Jinquan Guan", "Junhong Guo", "Qi Chen", "Jian Chen", "Yongkang Cai", "Yilin He", "Zhiquan Huang", "Yan Wang", "Yutong Xie"], "title": "A High Magnifications Histopathology Image Dataset for Oral Squamous Cell Carcinoma Diagnosis and Prognosis", "categories": ["eess.IV", "cs.CV"], "comment": "12 pages, 11 tables, 4 figures", "summary": "Oral Squamous Cell Carcinoma (OSCC) is a prevalent and aggressive malignancy where deep learning-based computer-aided diagnosis and prognosis can enhance clinical assessments.However, existing publicly available OSCC datasets often suffer from limited patient cohorts and a restricted focus on either diagnostic or prognostic tasks, limiting the development of comprehensive and generalizable models. To bridge this gap, we introduce Multi-OSCC, a new histopathology image dataset comprising 1,325 OSCC patients, integrating both diagnostic and prognostic information to expand existing public resources. Each patient is represented by six high resolution histopathology images captured at x200, x400, and x1000 magnifications-two per magnification-covering both the core and edge tumor regions.The Multi-OSCC dataset is richly annotated for six critical clinical tasks: recurrence prediction (REC), lymph node metastasis (LNM), tumor differentiation (TD), tumor invasion (TI), cancer embolus (CE), and perineural invasion (PI). To benchmark this dataset, we systematically evaluate the impact of different visual encoders, multi-image fusion techniques, stain normalization, and multi-task learning frameworks. Our analysis yields several key insights: (1) The top-performing models achieve excellent results, with an Area Under the Curve (AUC) of 94.72% for REC and 81.23% for TD, while all tasks surpass 70% AUC; (2) Stain normalization benefits diagnostic tasks but negatively affects recurrence prediction; (3) Multi-task learning incurs a 3.34% average AUC degradation compared to single-task models in our multi-task benchmark, underscoring the challenge of balancing multiple tasks in our dataset. To accelerate future research, we publicly release the Multi-OSCC dataset and baseline models at https://github.com/guanjinquan/OSCC-PathologyImageDataset.", "AI": {"tldr": "\u7814\u7a76\u8005\u6784\u5efa\u4e86\u5305\u542b1,325\u540d\u53e3\u8154\u9cde\u72b6\u7ec6\u80de\u764c\u60a3\u8005\u7684Multi-OSCC\u6570\u636e\u96c6\uff0c\u6bcf\u4f4d\u60a3\u8005\u5305\u542b6\u5f20\u4e0d\u540c\u653e\u5927\u500d\u6570\u7684\u75c5\u7406\u56fe\u50cf\uff0c\u6db5\u76d66\u4e2a\u4e34\u5e8a\u4efb\u52a1\u7684\u6807\u6ce8\uff0c\u5e76\u901a\u8fc7\u7cfb\u7edf\u6027\u8bc4\u4f30\u53d1\u73b0\u6700\u4f73\u6a21\u578b\u5728\u590d\u53d1\u9884\u6d4b\u4e0a\u8fbe\u523094.72%\u7684AUC\uff0c\u4f46\u591a\u4efb\u52a1\u5b66\u4e60\u76f8\u6bd4\u5355\u4efb\u52a1\u6a21\u578b\u5e73\u5747\u6027\u80fd\u4e0b\u964d3.34%\u3002", "motivation": "\u73b0\u6709\u516c\u5f00\u7684\u53e3\u8154\u9cde\u72b6\u7ec6\u80de\u764c\u6570\u636e\u96c6\u5b58\u5728\u60a3\u8005\u961f\u5217\u6709\u9650\u3001\u4ec5\u5173\u6ce8\u8bca\u65ad\u6216\u9884\u540e\u5355\u4e00\u4efb\u52a1\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u7efc\u5408\u6027\u548c\u53ef\u6cdb\u5316\u6a21\u578b\u7684\u5f00\u53d1\uff0c\u9700\u8981\u6784\u5efa\u66f4\u5168\u9762\u7684\u6570\u636e\u96c6\u6765\u5f25\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u4e86Multi-OSCC\u75c5\u7406\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5305\u542b1,325\u540d\u60a3\u8005\uff0c\u6bcf\u4f4d\u60a3\u8005\u67096\u5f20\u9ad8\u5206\u8fa8\u7387\u75c5\u7406\u56fe\u50cf\uff08x200\u3001x400\u3001x1000\u4e09\u79cd\u653e\u5927\u500d\u6570\uff0c\u6bcf\u79cd\u500d\u65702\u5f20\uff0c\u8986\u76d6\u80bf\u7624\u6838\u5fc3\u548c\u8fb9\u7f18\u533a\u57df\uff09\uff0c\u6807\u6ce8\u4e866\u4e2a\u5173\u952e\u4e34\u5e8a\u4efb\u52a1\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e0d\u540c\u89c6\u89c9\u7f16\u7801\u5668\u3001\u591a\u56fe\u50cf\u878d\u5408\u6280\u672f\u3001\u67d3\u8272\u6807\u51c6\u5316\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u7684\u5f71\u54cd\u3002", "result": "\u6700\u4f73\u6a21\u578b\u5728\u590d\u53d1\u9884\u6d4b(REC)\u4efb\u52a1\u4e0a\u8fbe\u523094.72%\u7684AUC\uff0c\u80bf\u7624\u5206\u5316(TD)\u4efb\u52a1\u8fbe\u523081.23%\u7684AUC\uff0c\u6240\u6709\u4efb\u52a1\u5747\u8d85\u8fc770%\u7684AUC\uff1b\u67d3\u8272\u6807\u51c6\u5316\u5bf9\u8bca\u65ad\u4efb\u52a1\u6709\u76ca\u4f46\u5bf9\u590d\u53d1\u9884\u6d4b\u6709\u8d1f\u9762\u5f71\u54cd\uff1b\u591a\u4efb\u52a1\u5b66\u4e60\u76f8\u6bd4\u5355\u4efb\u52a1\u6a21\u578b\u5e73\u5747AUC\u4e0b\u964d3.34%\u3002", "conclusion": "Multi-OSCC\u6570\u636e\u96c6\u4e3a\u53e3\u8154\u9cde\u72b6\u7ec6\u80de\u764c\u7684\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u548c\u9884\u540e\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u4e0d\u540c\u6280\u672f\u65b9\u6cd5\u7684\u6548\u679c\u5dee\u5f02\uff0c\u7279\u522b\u662f\u591a\u4efb\u52a1\u5b66\u4e60\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u9762\u4e34\u7684\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u548c\u516c\u5f00\u8d44\u6e90\u3002"}}
{"id": "2507.16579", "pdf": "https://arxiv.org/pdf/2507.16579", "abs": "https://arxiv.org/abs/2507.16579", "authors": ["Xiaojiao Xiao", "Qinmin Vivian Hu", "Guanghui Wang"], "title": "Pyramid Hierarchical Masked Diffusion Model for Imaging Synthesis", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Medical image synthesis plays a crucial role in clinical workflows, addressing the common issue of missing imaging modalities due to factors such as extended scan times, scan corruption, artifacts, patient motion, and intolerance to contrast agents. The paper presents a novel image synthesis network, the Pyramid Hierarchical Masked Diffusion Model (PHMDiff), which employs a multi-scale hierarchical approach for more detailed control over synthesizing high-quality images across different resolutions and layers. Specifically, this model utilizes randomly multi-scale high-proportion masks to speed up diffusion model training, and balances detail fidelity and overall structure. The integration of a Transformer-based Diffusion model process incorporates cross-granularity regularization, modeling the mutual information consistency across each granularity's latent spaces, thereby enhancing pixel-level perceptual accuracy. Comprehensive experiments on two challenging datasets demonstrate that PHMDiff achieves superior performance in both the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM), highlighting its capability to produce high-quality synthesized images with excellent structural integrity. Ablation studies further confirm the contributions of each component. Furthermore, the PHMDiff model, a multi-scale image synthesis framework across and within medical imaging modalities, shows significant advantages over other methods. The source code is available at https://github.com/xiaojiao929/PHMDiff", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PHMDiff\uff08\u91d1\u5b57\u5854\u5206\u5c42\u63a9\u7801\u6269\u6563\u6a21\u578b\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u533b\u5b66\u56fe\u50cf\u5408\u6210\u7f51\u7edc\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u5206\u5c42\u65b9\u6cd5\u548c\u968f\u673a\u591a\u5c3a\u5ea6\u9ad8\u6bd4\u4f8b\u63a9\u7801\u6765\u52a0\u901f\u8bad\u7ec3\uff0c\u5728\u533b\u5b66\u56fe\u50cf\u5408\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5408\u6210\u5728\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u4e3b\u8981\u89e3\u51b3\u7531\u4e8e\u626b\u63cf\u65f6\u95f4\u5ef6\u957f\u3001\u626b\u63cf\u635f\u574f\u3001\u4f2a\u5f71\u3001\u60a3\u8005\u8fd0\u52a8\u548c\u5bf9\u6bd4\u5242\u4e0d\u8010\u53d7\u7b49\u56e0\u7d20\u5bfc\u81f4\u7684\u6210\u50cf\u6a21\u6001\u7f3a\u5931\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u7ed3\u6784\u5b8c\u6574\u7684\u533b\u5b66\u56fe\u50cf\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u91d1\u5b57\u5854\u5206\u5c42\u63a9\u7801\u6269\u6563\u6a21\u578b\uff08PHMDiff\uff09\uff0c\u91c7\u7528\u591a\u5c3a\u5ea6\u5206\u5c42\u65b9\u6cd5\u5bf9\u4e0d\u540c\u5206\u8fa8\u7387\u548c\u5c42\u6b21\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u5408\u6210\u8fdb\u884c\u66f4\u7cbe\u7ec6\u7684\u63a7\u5236\u3002\u6a21\u578b\u5229\u7528\u968f\u673a\u591a\u5c3a\u5ea6\u9ad8\u6bd4\u4f8b\u63a9\u7801\u52a0\u901f\u6269\u6563\u6a21\u578b\u8bad\u7ec3\uff0c\u5e73\u8861\u7ec6\u8282\u4fdd\u771f\u5ea6\u548c\u6574\u4f53\u7ed3\u6784\u3002\u96c6\u6210\u57fa\u4e8eTransformer\u7684\u6269\u6563\u6a21\u578b\u8fc7\u7a0b\uff0c\u7ed3\u5408\u8de8\u7c92\u5ea6\u6b63\u5219\u5316\uff0c\u5efa\u6a21\u5404\u7c92\u5ea6\u6f5c\u5728\u7a7a\u95f4\u7684\u4e92\u4fe1\u606f\u4e00\u81f4\u6027\uff0c\u63d0\u5347\u50cf\u7d20\u7ea7\u611f\u77e5\u7cbe\u5ea6\u3002", "result": "\u5728\u4e24\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cPHMDiff\u5728\u5cf0\u503c\u4fe1\u566a\u6bd4\uff08PSNR\uff09\u548c\u7ed3\u6784\u76f8\u4f3c\u6027\u6307\u6570\u6d4b\u91cf\uff08SSIM\uff09\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\uff0c\u5c55\u73b0\u4e86\u5176\u751f\u6210\u5177\u6709\u51fa\u8272\u7ed3\u6784\u5b8c\u6574\u6027\u7684\u9ad8\u8d28\u91cf\u5408\u6210\u56fe\u50cf\u7684\u80fd\u529b\u3002\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u5404\u7ec4\u4ef6\u7684\u8d21\u732e\u3002", "conclusion": "PHMDiff\u6a21\u578b\u4f5c\u4e3a\u4e00\u4e2a\u8de8\u533b\u5b66\u6210\u50cf\u6a21\u6001\u548c\u6a21\u6001\u5185\u7684\u591a\u5c3a\u5ea6\u56fe\u50cf\u5408\u6210\u6846\u67b6\uff0c\u76f8\u6bd4\u5176\u4ed6\u65b9\u6cd5\u663e\u793a\u51fa\u663e\u8457\u4f18\u52bf\u3002\u8be5\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5408\u6210\u9886\u57df\u5177\u6709\u91cd\u8981\u7684\u5e94\u7528\u4ef7\u503c\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u6210\u50cf\u6a21\u6001\u7f3a\u5931\u95ee\u9898\u3002"}}
