{"id": "2507.17963", "pdf": "https://arxiv.org/pdf/2507.17963", "abs": "https://arxiv.org/abs/2507.17963", "authors": ["Rameen Abdal", "Or Patashnik", "Ekaterina Deyneka", "Hao Chen", "Aliaksandr Siarohin", "Sergey Tulyakov", "Daniel Cohen-Or", "Kfir Aberman"], "title": "Zero-Shot Dynamic Concept Personalization with Grid-Based LoRA", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Project Page and Video :   https://snap-research.github.io/zero-shot-dynamic-concepts/", "summary": "Recent advances in text-to-video generation have enabled high-quality synthesis from text and image prompts. While the personalization of dynamic concepts, which capture subject-specific appearance and motion from a single video, is now feasible, most existing methods require per-instance fine-tuning, limiting scalability. We introduce a fully zero-shot framework for dynamic concept personalization in text-to-video models. Our method leverages structured 2x2 video grids that spatially organize input and output pairs, enabling the training of lightweight Grid-LoRA adapters for editing and composition within these grids. At inference, a dedicated Grid Fill module completes partially observed layouts, producing temporally coherent and identity preserving outputs. Once trained, the entire system operates in a single forward pass, generalizing to previously unseen dynamic concepts without any test-time optimization. Extensive experiments demonstrate high-quality and consistent results across a wide range of subjects beyond trained concepts and editing scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u6846\u67b6\uff0c\u7528\u4e8e\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u4e2d\u7684\u52a8\u6001\u6982\u5ff5\u4e2a\u6027\u5316\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u63a8\u5e7f\u5230\u65b0\u6982\u5ff5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u9488\u5bf9\u6bcf\u4e2a\u5b9e\u4f8b\u8fdb\u884c\u5fae\u8c03\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u96f6\u6837\u672c\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u7ed3\u6784\u53162x2\u89c6\u9891\u7f51\u683c\u8bad\u7ec3\u8f7b\u91cf\u7ea7Grid-LoRA\u9002\u914d\u5668\uff0c\u5e76\u901a\u8fc7Grid Fill\u6a21\u5757\u5b8c\u6210\u90e8\u5206\u89c2\u5bdf\u7684\u5e03\u5c40\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u672a\u89c1\u8fc7\u7684\u52a8\u6001\u6982\u5ff5\u548c\u7f16\u8f91\u573a\u666f\u4e2d\u5747\u80fd\u4ea7\u751f\u9ad8\u8d28\u91cf\u4e14\u4e00\u81f4\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u5b9e\u73b0\u52a8\u6001\u6982\u5ff5\u4e2a\u6027\u5316\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.18155", "pdf": "https://arxiv.org/pdf/2507.18155", "abs": "https://arxiv.org/abs/2507.18155", "authors": ["SeungJun Moon", "Hah Min Lew", "Seungeun Lee", "Ji-Su Kang", "Gyeong-Moon Park"], "title": "GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "ICCV 2025, Project page: https://hahminlew.github.io/geoavatar/", "summary": "Despite recent progress in 3D head avatar generation, balancing identity preservation, i.e., reconstruction, with novel poses and expressions, i.e., animation, remains a challenge. Existing methods struggle to adapt Gaussians to varying geometrical deviations across facial regions, resulting in suboptimal quality. To address this, we propose GeoAvatar, a framework for adaptive geometrical Gaussian Splatting. GeoAvatar leverages Adaptive Pre-allocation Stage (APS), an unsupervised method that segments Gaussians into rigid and flexible sets for adaptive offset regularization. Then, based on mouth anatomy and dynamics, we introduce a novel mouth structure and the part-wise deformation strategy to enhance the animation fidelity of the mouth. Finally, we propose a regularization loss for precise rigging between Gaussians and 3DMM faces. Moreover, we release DynamicFace, a video dataset with highly expressive facial motions. Extensive experiments show the superiority of GeoAvatar compared to state-of-the-art methods in reconstruction and novel animation scenarios.", "AI": {"tldr": "GeoAvatar\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u51e0\u4f55\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u901a\u8fc7APS\u5206\u5272\u9ad8\u65af\u4e3a\u521a\u6027\u548c\u67d4\u6027\u96c6\uff0c\u7ed3\u5408\u5634\u90e8\u7ed3\u6784\u548c\u90e8\u5206\u53d8\u5f62\u7b56\u7565\uff0c\u63d0\u5347\u52a8\u753b\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b33D\u5934\u90e8\u5934\u50cf\u751f\u6210\u4e2d\u8eab\u4efd\u4fdd\u6301\u4e0e\u65b0\u9896\u59ff\u6001\u548c\u8868\u60c5\u52a8\u753b\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u9884\u5206\u914d\u9636\u6bb5\uff08APS\uff09\u5206\u5272\u9ad8\u65af\uff0c\u63d0\u51fa\u5634\u90e8\u7ed3\u6784\u548c\u90e8\u5206\u53d8\u5f62\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u6b63\u5219\u5316\u635f\u5931\u3002", "result": "GeoAvatar\u5728\u91cd\u5efa\u548c\u65b0\u52a8\u753b\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GeoAvatar\u901a\u8fc7\u81ea\u9002\u5e94\u51e0\u4f55\u9ad8\u65af\u6cfc\u6e85\u548c\u5634\u90e8\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5934\u50cf\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2507.18231", "pdf": "https://arxiv.org/pdf/2507.18231", "abs": "https://arxiv.org/abs/2507.18231", "authors": ["Yixiao Chen", "Bin Liang", "Hanzhi Guo", "Yongqing Cheng", "Jiayi Zhao", "Dongdong Weng"], "title": "PS-GS: Gaussian Splatting for Multi-View Photometric Stereo", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Integrating inverse rendering with multi-view photometric stereo (MVPS) yields more accurate 3D reconstructions than the inverse rendering approaches that rely on fixed environment illumination. However, efficient inverse rendering with MVPS remains challenging. To fill this gap, we introduce the Gaussian Splatting for Multi-view Photometric Stereo (PS-GS), which efficiently and jointly estimates the geometry, materials, and lighting of the object that is illuminated by diverse directional lights (multi-light). Our method first reconstructs a standard 2D Gaussian splatting model as the initial geometry. Based on the initialization model, it then proceeds with the deferred inverse rendering by the full rendering equation containing a lighting-computing multi-layer perceptron. During the whole optimization, we regularize the rendered normal maps by the uncalibrated photometric stereo estimated normals. We also propose the 2D Gaussian ray-tracing for single directional light to refine the incident lighting. The regularizations and the use of multi-view and multi-light images mitigate the ill-posed problem of inverse rendering. After optimization, the reconstructed object can be used for novel-view synthesis, relighting, and material and shape editing. Experiments on both synthetic and real datasets demonstrate that our method outperforms prior works in terms of reconstruction accuracy and computational efficiency.", "AI": {"tldr": "PS-GS\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u9ad8\u65af\u70b9\u4e91\u548c\u9006\u6e32\u67d3\u6280\u672f\uff0c\u5728\u591a\u5149\u6e90\u6761\u4ef6\u4e0b\u9ad8\u6548\u4f30\u8ba1\u51e0\u4f55\u3001\u6750\u8d28\u548c\u5149\u7167\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u91cd\u5efa\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u9006\u6e32\u67d3\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u73af\u5883\u5149\u7167\uff0c\u5bfc\u81f43D\u91cd\u5efa\u7cbe\u5ea6\u4e0d\u8db3\u3002PS-GS\u65e8\u5728\u901a\u8fc7\u591a\u5149\u6e90\u548c\u591a\u89c6\u89d2\u6570\u636e\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u9996\u5148\u6784\u5efa2D\u9ad8\u65af\u70b9\u4e91\u6a21\u578b\u4f5c\u4e3a\u521d\u59cb\u51e0\u4f55\uff0c\u968f\u540e\u901a\u8fc7\u5305\u542b\u5149\u7167\u8ba1\u7b97\u7684\u591a\u5c42\u611f\u77e5\u673a\u8fdb\u884c\u9006\u6e32\u67d3\uff0c\u5e76\u7ed3\u5408\u672a\u6807\u5b9a\u7684\u5149\u5ea6\u7acb\u4f53\u6cd5\u4f30\u8ba1\u6cd5\u7ebf\u8fdb\u884c\u6b63\u5219\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPS-GS\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u91cd\u5efa\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "PS-GS\u4e3a\u591a\u5149\u6e90\u6761\u4ef6\u4e0b\u76843D\u91cd\u5efa\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u65b0\u89c6\u89d2\u5408\u6210\u3001\u91cd\u5149\u7167\u53ca\u6750\u8d28\u7f16\u8f91\u7b49\u5e94\u7528\u3002"}}
{"id": "2507.17801", "pdf": "https://arxiv.org/pdf/2507.17801", "abs": "https://arxiv.org/abs/2507.17801", "authors": ["Yi Xin", "Juncheng Yan", "Qi Qin", "Zhen Li", "Dongyang Liu", "Shicheng Li", "Victor Shea-Jay Huang", "Yupeng Zhou", "Renrui Zhang", "Le Zhuo", "Tiancheng Han", "Xiaoqing Sun", "Siqi Luo", "Mengmeng Wang", "Bin Fu", "Yuewen Cao", "Hongsheng Li", "Guangtao Zhai", "Xiaohong Liu", "Yu Qiao", "Peng Gao"], "title": "Lumina-mGPT 2.0: Stand-Alone AutoRegressive Image Modeling", "categories": ["cs.CV"], "comment": "Tech Report, 23 pages, 11 figures, 7 tables", "summary": "We present Lumina-mGPT 2.0, a stand-alone, decoder-only autoregressive model that revisits and revitalizes the autoregressive paradigm for high-quality image generation and beyond. Unlike existing approaches that rely on pretrained components or hybrid architectures, Lumina-mGPT 2.0 is trained entirely from scratch, enabling unrestricted architectural design and licensing freedom. It achieves generation quality on par with state-of-the-art diffusion models such as DALL-E 3 and SANA, while preserving the inherent flexibility and compositionality of autoregressive modeling. Our unified tokenization scheme allows the model to seamlessly handle a wide spectrum of tasks-including subject-driven generation, image editing, controllable synthesis, and dense prediction-within a single generative framework. To further boost usability, we incorporate efficient decoding strategies like inference-time scaling and speculative Jacobi sampling to improve quality and speed, respectively. Extensive evaluations on standard text-to-image benchmarks (e.g., GenEval, DPG) demonstrate that Lumina-mGPT 2.0 not only matches but in some cases surpasses diffusion-based models. Moreover, we confirm its multi-task capabilities on the Graph200K benchmark, with the native Lumina-mGPT 2.0 performing exceptionally well. These results position Lumina-mGPT 2.0 as a strong, flexible foundation model for unified multimodal generation. We have released our training details, code, and models at https://github.com/Alpha-VLLM/Lumina-mGPT-2.0.", "AI": {"tldr": "Lumina-mGPT 2.0\u662f\u4e00\u4e2a\u72ec\u7acb\u7684\u89e3\u7801\u5668\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\uff0c\u65e0\u9700\u4f9d\u8d56\u9884\u8bad\u7ec3\u7ec4\u4ef6\u6216\u6df7\u5408\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u4e0e\u6269\u6563\u6a21\u578b\u76f8\u5ab2\u7f8e\u7684\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u652f\u6301\u591a\u4efb\u52a1\u5904\u7406\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bad\u7ec3\u7ec4\u4ef6\u6216\u6df7\u5408\u67b6\u6784\uff0c\u9650\u5236\u4e86\u8bbe\u8ba1\u81ea\u7531\u548c\u8bb8\u53ef\u7075\u6d3b\u6027\u3002Lumina-mGPT 2.0\u65e8\u5728\u901a\u8fc7\u5b8c\u5168\u4ece\u5934\u8bad\u7ec3\uff0c\u63d0\u4f9b\u66f4\u7075\u6d3b\u3001\u53ef\u7ec4\u5408\u7684\u81ea\u56de\u5f52\u6a21\u578b\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u7684\u5206\u8bcd\u65b9\u6848\uff0c\u652f\u6301\u591a\u4efb\u52a1\u5904\u7406\uff08\u5982\u751f\u6210\u3001\u7f16\u8f91\u3001\u53ef\u63a7\u5408\u6210\u7b49\uff09\uff0c\u5e76\u7ed3\u5408\u9ad8\u6548\u89e3\u7801\u7b56\u7565\uff08\u5982\u63a8\u7406\u65f6\u95f4\u7f29\u653e\u548c\u63a8\u6d4b\u6027Jacobi\u91c7\u6837\uff09\u63d0\u5347\u8d28\u91cf\u548c\u901f\u5ea6\u3002", "result": "\u5728\u6807\u51c6\u6587\u672c\u5230\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8d8a\u6269\u6563\u6a21\u578b\uff1b\u5728Graph200K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u591a\u4efb\u52a1\u80fd\u529b\u3002", "conclusion": "Lumina-mGPT 2.0\u662f\u4e00\u4e2a\u5f3a\u5927\u3001\u7075\u6d3b\u7684\u7edf\u4e00\u591a\u6a21\u6001\u751f\u6210\u57fa\u7840\u6a21\u578b\uff0c\u5df2\u5f00\u6e90\u8bad\u7ec3\u7ec6\u8282\u3001\u4ee3\u7801\u548c\u6a21\u578b\u3002"}}
{"id": "2507.17844", "pdf": "https://arxiv.org/pdf/2507.17844", "abs": "https://arxiv.org/abs/2507.17844", "authors": ["Sai Varun Kodathala", "Yashwanth Reddy Vutukoori", "Rakesh Vunnam"], "title": "SV3.3B: A Sports Video Understanding Model for Action Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 6 figures, 4 tables. Submitted to AIxSET 2025", "summary": "This paper addresses the challenge of automated sports video analysis, which has traditionally been limited by computationally intensive models requiring server-side processing and lacking fine-grained understanding of athletic movements. Current approaches struggle to capture the nuanced biomechanical transitions essential for meaningful sports analysis, often missing critical phases like preparation, execution, and follow-through that occur within seconds. To address these limitations, we introduce SV3.3B, a lightweight 3.3B parameter video understanding model that combines novel temporal motion difference sampling with self-supervised learning for efficient on-device deployment. Our approach employs a DWT-VGG16-LDA based keyframe extraction mechanism that intelligently identifies the 16 most representative frames from sports sequences, followed by a V-DWT-JEPA2 encoder pretrained through mask-denoising objectives and an LLM decoder fine-tuned for sports action description generation. Evaluated on a subset of the NSVA basketball dataset, SV3.3B achieves superior performance across both traditional text generation metrics and sports-specific evaluation criteria, outperforming larger closed-source models including GPT-4o variants while maintaining significantly lower computational requirements. Our model demonstrates exceptional capability in generating technically detailed and analytically rich sports descriptions, achieving 29.2% improvement over GPT-4o in ground truth validation metrics, with substantial improvements in information density, action complexity, and measurement precision metrics essential for comprehensive athletic analysis. Model Available at https://huggingface.co/sportsvision/SV3.3B.", "AI": {"tldr": "SV3.3B\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u76843.3B\u53c2\u6570\u89c6\u9891\u7406\u89e3\u6a21\u578b\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u4f53\u80b2\u89c6\u9891\u5206\u6790\uff0c\u7ed3\u5408\u4e86\u65f6\u95f4\u8fd0\u52a8\u5dee\u5f02\u91c7\u6837\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u52a8\u4f5c\u7684\u7ec6\u7c92\u5ea6\u7406\u89e3\u3002", "motivation": "\u4f20\u7edf\u4f53\u80b2\u89c6\u9891\u5206\u6790\u6a21\u578b\u8ba1\u7b97\u91cf\u5927\u4e14\u7f3a\u4e4f\u5bf9\u8fd0\u52a8\u52a8\u4f5c\u7684\u7ec6\u7c92\u5ea6\u7406\u89e3\uff0c\u65e0\u6cd5\u6355\u6349\u5173\u952e\u7684\u751f\u7269\u529b\u5b66\u8fc7\u6e21\u9636\u6bb5\u3002", "method": "\u91c7\u7528DWT-VGG16-LDA\u5173\u952e\u5e27\u63d0\u53d6\u673a\u5236\u548cV-DWT-JEPA2\u7f16\u7801\u5668\uff0c\u7ed3\u5408LLM\u89e3\u7801\u5668\uff0c\u5b9e\u73b0\u9ad8\u6548\u8bbe\u5907\u7aef\u90e8\u7f72\u3002", "result": "\u5728NSVA\u7bee\u7403\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u751f\u6210\u6280\u672f\u4e0a\u8be6\u7ec6\u4e14\u5206\u6790\u4e30\u5bcc\u7684\u4f53\u80b2\u63cf\u8ff0\uff0c\u6027\u80fd\u8d85\u8fc7GPT-4o\u53d8\u4f53\u3002", "conclusion": "SV3.3B\u5728\u8ba1\u7b97\u9700\u6c42\u4f4e\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f53\u80b2\u89c6\u9891\u5206\u6790\u7684\u6027\u80fd\u548c\u4fe1\u606f\u5bc6\u5ea6\u3002"}}
{"id": "2507.17860", "pdf": "https://arxiv.org/pdf/2507.17860", "abs": "https://arxiv.org/abs/2507.17860", "authors": ["Ko Watanabe. Stanislav Frolov. Adriano Lucieri. Andreas Dengel"], "title": "Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advancements in Deep Learning and its application on the edge hold great potential for the revolution of routine screenings for skin cancers like Melanoma. Along with the anticipated benefits of this technology, potential dangers arise from unforseen and inherent biases. Thus, assessing and improving the fairness of such systems is of utmost importance. A key challenge in fairness assessment is to ensure that the evaluation dataset is sufficiently representative of different Personal Identifiable Information (PII) (sex, age, and race) and other minority groups. Against the backdrop of this challenge, this study leverages the state-of-the-art Generative AI (GenAI) LightningDiT model to assess the fairness of publicly available melanoma classifiers. The results suggest that fairness assessment using highly realistic synthetic data is a promising direction. Yet, our findings indicate that verifying fairness becomes difficult when the melanoma-detection model used for evaluation is trained on data that differ from the dataset underpinning the synthetic images. Nonetheless, we propose that our approach offers a valuable new avenue for employing synthetic data to gauge and enhance fairness in medical-imaging GenAI systems.", "AI": {"tldr": "\u5229\u7528\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u6a21\u578b\u8bc4\u4f30\u76ae\u80a4\u764c\u5206\u7c7b\u5668\u7684\u516c\u5e73\u6027\uff0c\u53d1\u73b0\u5408\u6210\u6570\u636e\u5728\u516c\u5e73\u6027\u8bc4\u4f30\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u6ce8\u610f\u6570\u636e\u4e00\u81f4\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u76ae\u80a4\u764c\u7b5b\u67e5\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5b58\u5728\u6f5c\u5728\u504f\u89c1\uff0c\u9700\u8bc4\u4f30\u548c\u6539\u8fdb\u516c\u5e73\u6027\u3002", "method": "\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u751f\u6210\u5f0fAI\u6a21\u578b\uff08LightningDiT\uff09\u8bc4\u4f30\u516c\u5f00\u9ed1\u8272\u7d20\u7624\u5206\u7c7b\u5668\u7684\u516c\u5e73\u6027\u3002", "result": "\u5408\u6210\u6570\u636e\u5728\u516c\u5e73\u6027\u8bc4\u4f30\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u82e5\u8bc4\u4f30\u6a21\u578b\u4e0e\u5408\u6210\u6570\u636e\u8bad\u7ec3\u96c6\u4e0d\u4e00\u81f4\uff0c\u516c\u5e73\u6027\u9a8c\u8bc1\u4f1a\u53d8\u5f97\u56f0\u96be\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u4e3a\u533b\u5b66\u5f71\u50cfAI\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u4f46\u9700\u786e\u4fdd\u6570\u636e\u4e00\u81f4\u6027\u3002"}}
{"id": "2507.17892", "pdf": "https://arxiv.org/pdf/2507.17892", "abs": "https://arxiv.org/abs/2507.17892", "authors": ["Hanzhou Liu", "Binghan Li", "Chengkai Liu", "Mi Lu"], "title": "DiNAT-IR: Exploring Dilated Neighborhood Attention for High-Quality Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Transformers, with their self-attention mechanisms for modeling long-range dependencies, have become a dominant paradigm in image restoration tasks. However, the high computational cost of self-attention limits scalability to high-resolution images, making efficiency-quality trade-offs a key research focus. To address this, Restormer employs channel-wise self-attention, which computes attention across channels instead of spatial dimensions. While effective, this approach may overlook localized artifacts that are crucial for high-quality image restoration. To bridge this gap, we explore Dilated Neighborhood Attention (DiNA) as a promising alternative, inspired by its success in high-level vision tasks. DiNA balances global context and local precision by integrating sliding-window attention with mixed dilation factors, effectively expanding the receptive field without excessive overhead. However, our preliminary experiments indicate that directly applying this global-local design to the classic deblurring task hinders accurate visual restoration, primarily due to the constrained global context understanding within local attention. To address this, we introduce a channel-aware module that complements local attention, effectively integrating global context without sacrificing pixel-level precision. The proposed DiNAT-IR, a Transformer-based architecture specifically designed for image restoration, achieves competitive results across multiple benchmarks, offering a high-quality solution for diverse low-level computer vision problems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5DiNAT-IR\uff0c\u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u6ce8\u610f\u529b\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u9ad8\u6548\u6027\u548c\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709Transformer\u65b9\u6cd5\u5728\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u4e2d\u56e0\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u800c\u96be\u4ee5\u6269\u5c55\u5230\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u4e14\u53ef\u80fd\u5ffd\u7565\u5c40\u90e8\u7ec6\u8282\u3002", "method": "\u5f15\u5165Dilated Neighborhood Attention (DiNA)\u7ed3\u5408\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u548c\u6df7\u5408\u81a8\u80c0\u56e0\u5b50\uff0c\u5e76\u52a0\u5165\u901a\u9053\u611f\u77e5\u6a21\u5757\u4ee5\u6574\u5408\u5168\u5c40\u4e0a\u4e0b\u6587\u3002", "result": "DiNAT-IR\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u7ade\u4e89\u6027\u7ed3\u679c\uff0c\u4e3a\u4f4e\u5c42\u8ba1\u7b97\u673a\u89c6\u89c9\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "DiNAT-IR\u901a\u8fc7\u5e73\u8861\u5168\u5c40\u548c\u5c40\u90e8\u6ce8\u610f\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u4fee\u590d\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2507.17957", "pdf": "https://arxiv.org/pdf/2507.17957", "abs": "https://arxiv.org/abs/2507.17957", "authors": ["Md. Al-Masrur Khan", "Durgakant Pushp", "Lantao Liu"], "title": "AFRDA: Attentive Feature Refinement for Domain Adaptive Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "In Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS), a model is trained on labeled source domain data (e.g., synthetic images) and adapted to an unlabeled target domain (e.g., real-world images) without access to target annotations. Existing UDA-SS methods often struggle to balance fine-grained local details with global contextual information, leading to segmentation errors in complex regions. To address this, we introduce the Adaptive Feature Refinement (AFR) module, which enhances segmentation accuracy by refining highresolution features using semantic priors from low-resolution logits. AFR also integrates high-frequency components, which capture fine-grained structures and provide crucial boundary information, improving object delineation. Additionally, AFR adaptively balances local and global information through uncertaintydriven attention, reducing misclassifications. Its lightweight design allows seamless integration into HRDA-based UDA methods, leading to state-of-the-art segmentation performance. Our approach improves existing UDA-SS methods by 1.05% mIoU on GTA V --> Cityscapes and 1.04% mIoU on Synthia-->Cityscapes. The implementation of our framework is available at: https://github.com/Masrur02/AFRDA", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u7279\u5f81\u7ec6\u5316\uff08AFR\uff09\u6a21\u5757\uff0c\u901a\u8fc7\u7ed3\u5408\u9ad8\u4f4e\u5206\u8fa8\u7387\u7279\u5f81\u548c\u9ad8\u9891\u7ec4\u4ef6\uff0c\u63d0\u5347\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u8bed\u4e49\u5206\u5272\uff08UDA-SS\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709UDA-SS\u65b9\u6cd5\u5728\u5e73\u8861\u5c40\u90e8\u7ec6\u8282\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u590d\u6742\u533a\u57df\u5206\u5272\u9519\u8bef\u3002", "method": "\u5f15\u5165AFR\u6a21\u5757\uff0c\u5229\u7528\u4f4e\u5206\u8fa8\u7387\u8bed\u4e49\u5148\u9a8c\u7ec6\u5316\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\uff0c\u5e76\u6574\u5408\u9ad8\u9891\u7ec4\u4ef6\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u7ed3\u6784\u3002\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u6ce8\u610f\u529b\u5e73\u8861\u5c40\u90e8\u4e0e\u5168\u5c40\u4fe1\u606f\u3002", "result": "\u5728GTA V\u2192Cityscapes\u548cSynthia\u2192Cityscapes\u4e0a\u5206\u522b\u63d0\u53471.05%\u548c1.04%\u7684mIoU\u3002", "conclusion": "AFR\u6a21\u5757\u8f7b\u91cf\u9ad8\u6548\uff0c\u663e\u8457\u63d0\u5347\u4e86UDA-SS\u7684\u5206\u5272\u7cbe\u5ea6\u3002"}}
{"id": "2507.18023", "pdf": "https://arxiv.org/pdf/2507.18023", "abs": "https://arxiv.org/abs/2507.18023", "authors": ["Jun Zhou", "Dinghao Li", "Nannan Li", "Mingjie Wang"], "title": "High-fidelity 3D Gaussian Inpainting: preserving multi-view consistency and photorealistic details", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in multi-view 3D reconstruction and novel-view synthesis, particularly through Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have greatly enhanced the fidelity and efficiency of 3D content creation. However, inpainting 3D scenes remains a challenging task due to the inherent irregularity of 3D structures and the critical need for maintaining multi-view consistency. In this work, we propose a novel 3D Gaussian inpainting framework that reconstructs complete 3D scenes by leveraging sparse inpainted views. Our framework incorporates an automatic Mask Refinement Process and region-wise Uncertainty-guided Optimization. Specifically, we refine the inpainting mask using a series of operations, including Gaussian scene filtering and back-projection, enabling more accurate localization of occluded regions and realistic boundary restoration. Furthermore, our Uncertainty-guided Fine-grained Optimization strategy, which estimates the importance of each region across multi-view images during training, alleviates multi-view inconsistencies and enhances the fidelity of fine details in the inpainted results. Comprehensive experiments conducted on diverse datasets demonstrate that our approach outperforms existing state-of-the-art methods in both visual quality and view consistency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u5206\u5e03\u7684\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u4fee\u590d\u89c6\u56fe\u91cd\u5efa\u5b8c\u65743D\u573a\u666f\uff0c\u7ed3\u5408\u81ea\u52a8\u63a9\u6a21\u4f18\u5316\u548c\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4fee\u590d\u6548\u679c\u3002", "motivation": "3D\u573a\u666f\u4fee\u590d\u56e0\u7ed3\u6784\u4e0d\u89c4\u5219\u548c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u9700\u6c42\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u9ad8\u4fdd\u771f\u9700\u6c42\u3002", "method": "\u91c7\u7528\u81ea\u52a8\u63a9\u6a21\u4f18\u5316\uff08\u9ad8\u65af\u573a\u666f\u8fc7\u6ee4\u548c\u53cd\u6295\u5f71\uff09\u548c\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u7ec6\u7c92\u5ea6\u4f18\u5316\u7b56\u7565\uff0c\u63d0\u5347\u4fee\u590d\u7cbe\u5ea6\u548c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u89c6\u89d2\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e863D\u573a\u666f\u4fee\u590d\u4e2d\u7684\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u4fdd\u771f\u95ee\u9898\uff0c\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2507.18031", "pdf": "https://arxiv.org/pdf/2507.18031", "abs": "https://arxiv.org/abs/2507.18031", "authors": ["Ahmad ALBarqawi", "Mahmoud Nazzal", "Issa Khalil", "Abdallah Khreishah", "NhatHai Phan"], "title": "ViGText: Deepfake Image Detection with Vision-Language Model Explanations and Graph Neural Networks", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid rise of deepfake technology, which produces realistic but fraudulent digital content, threatens the authenticity of media. Traditional deepfake detection approaches often struggle with sophisticated, customized deepfakes, especially in terms of generalization and robustness against malicious attacks. This paper introduces ViGText, a novel approach that integrates images with Vision Large Language Model (VLLM) Text explanations within a Graph-based framework to improve deepfake detection. The novelty of ViGText lies in its integration of detailed explanations with visual data, as it provides a more context-aware analysis than captions, which often lack specificity and fail to reveal subtle inconsistencies. ViGText systematically divides images into patches, constructs image and text graphs, and integrates them for analysis using Graph Neural Networks (GNNs) to identify deepfakes. Through the use of multi-level feature extraction across spatial and frequency domains, ViGText captures details that enhance its robustness and accuracy to detect sophisticated deepfakes. Extensive experiments demonstrate that ViGText significantly enhances generalization and achieves a notable performance boost when it detects user-customized deepfakes. Specifically, average F1 scores rise from 72.45% to 98.32% under generalization evaluation, and reflects the model's superior ability to generalize to unseen, fine-tuned variations of stable diffusion models. As for robustness, ViGText achieves an increase of 11.1% in recall compared to other deepfake detection approaches. When facing targeted attacks that exploit its graph-based architecture, ViGText limits classification performance degradation to less than 4%. ViGText uses detailed visual and textual analysis to set a new standard for detecting deepfakes, helping ensure media authenticity and information integrity.", "AI": {"tldr": "ViGText\u662f\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\uff08VLLM\uff09\u6587\u672c\u89e3\u91ca\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u5a01\u80c1\u5a92\u4f53\u771f\u5b9e\u6027\uff0c\u4f20\u7edf\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u3001\u5b9a\u5236\u5316\u7684\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\u3002", "method": "ViGText\u901a\u8fc7\u5c06\u56fe\u50cf\u5206\u5757\u3001\u6784\u5efa\u56fe\u50cf\u548c\u6587\u672c\u56fe\uff0c\u5e76\u5229\u7528GNNs\u6574\u5408\u5206\u6790\uff0c\u7ed3\u5408\u7a7a\u95f4\u548c\u9891\u57df\u7684\u591a\u5c42\u6b21\u7279\u5f81\u63d0\u53d6\u3002", "result": "\u5b9e\u9a8c\u663e\u793aViGText\u663e\u8457\u63d0\u5347\u6cdb\u5316\u6027\uff08F1\u5206\u6570\u4ece72.45%\u5347\u81f398.32%\uff09\u548c\u9c81\u68d2\u6027\uff08\u53ec\u56de\u7387\u63d0\u9ad811.1%\uff09\uff0c\u5bf9\u6297\u653b\u51fb\u6027\u80fd\u4e0b\u964d\u5c0f\u4e8e4%\u3002", "conclusion": "ViGText\u901a\u8fc7\u8be6\u7ec6\u7684\u89c6\u89c9\u548c\u6587\u672c\u5206\u6790\uff0c\u4e3a\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u6709\u52a9\u4e8e\u4fdd\u969c\u5a92\u4f53\u771f\u5b9e\u6027\u548c\u4fe1\u606f\u5b8c\u6574\u6027\u3002"}}
{"id": "2507.18046", "pdf": "https://arxiv.org/pdf/2507.18046", "abs": "https://arxiv.org/abs/2507.18046", "authors": ["Hanwen Shen", "Jiajie Lu", "Yupeng Cao", "Xiaonan Yang"], "title": "Enhancing Scene Transition Awareness in Video Generation via Post-Training", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in AI-generated video have shown strong performance on \\emph{text-to-video} tasks, particularly for short clips depicting a single scene. However, current models struggle to generate longer videos with coherent scene transitions, primarily because they cannot infer when a transition is needed from the prompt. Most open-source models are trained on datasets consisting of single-scene video clips, which limits their capacity to learn and respond to prompts requiring multiple scenes. Developing scene transition awareness is essential for multi-scene generation, as it allows models to identify and segment videos into distinct clips by accurately detecting transitions.   To address this, we propose the \\textbf{Transition-Aware Video} (TAV) dataset, which consists of preprocessed video clips with multiple scene transitions. Our experiment shows that post-training on the \\textbf{TAV} dataset improves prompt-based scene transition understanding, narrows the gap between required and generated scenes, and maintains image quality.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTAV\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u63d0\u5347AI\u751f\u6210\u89c6\u9891\u4e2d\u573a\u666f\u8f6c\u6362\u7684\u8fde\u8d2f\u6027\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u6a21\u578b\u5728\u751f\u6210\u591a\u573a\u666f\u89c6\u9891\u65f6\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524dAI\u751f\u6210\u7684\u89c6\u9891\u5728\u5355\u573a\u666f\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591a\u573a\u666f\u89c6\u9891\u751f\u6210\u4e2d\u7f3a\u4e4f\u8fde\u8d2f\u7684\u573a\u666f\u8f6c\u6362\u80fd\u529b\uff0c\u4e3b\u8981\u56e0\u4e3a\u6a21\u578b\u65e0\u6cd5\u4ece\u63d0\u793a\u4e2d\u63a8\u65ad\u4f55\u65f6\u9700\u8981\u8f6c\u6362\u3002", "method": "\u63d0\u51fa\u4e86Transition-Aware Video (TAV)\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u573a\u666f\u8f6c\u6362\u7684\u89c6\u9891\u7247\u6bb5\uff0c\u5e76\u901a\u8fc7\u540e\u8bad\u7ec3\u63d0\u5347\u6a21\u578b\u5bf9\u573a\u666f\u8f6c\u6362\u7684\u7406\u89e3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528TAV\u6570\u636e\u96c6\u540e\u8bad\u7ec3\u80fd\u591f\u6539\u5584\u573a\u666f\u8f6c\u6362\u7406\u89e3\uff0c\u7f29\u5c0f\u6240\u9700\u4e0e\u751f\u6210\u573a\u666f\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "TAV\u6570\u636e\u96c6\u4e3a\u591a\u573a\u666f\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u573a\u666f\u8f6c\u6362\u80fd\u529b\u3002"}}
{"id": "2507.18060", "pdf": "https://arxiv.org/pdf/2507.18060", "abs": "https://arxiv.org/abs/2507.18060", "authors": ["Chengxuan Zhu", "Qingnan Fan", "Qi Zhang", "Jinwei Chen", "Huaqi Zhang", "Chao Xu", "Boxin Shi"], "title": "BokehDiff: Neural Lens Blur with One-Step Diffusion", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "We introduce BokehDiff, a novel lens blur rendering method that achieves physically accurate and visually appealing outcomes, with the help of generative diffusion prior. Previous methods are bounded by the accuracy of depth estimation, generating artifacts in depth discontinuities. Our method employs a physics-inspired self-attention module that aligns with the image formation process, incorporating depth-dependent circle of confusion constraint and self-occlusion effects. We adapt the diffusion model to the one-step inference scheme without introducing additional noise, and achieve results of high quality and fidelity. To address the lack of scalable paired data, we propose to synthesize photorealistic foregrounds with transparency with diffusion models, balancing authenticity and scene diversity.", "AI": {"tldr": "BokehDiff\u662f\u4e00\u79cd\u65b0\u9896\u7684\u955c\u5934\u6a21\u7cca\u6e32\u67d3\u65b9\u6cd5\uff0c\u7ed3\u5408\u751f\u6210\u6269\u6563\u5148\u9a8c\uff0c\u5b9e\u73b0\u7269\u7406\u51c6\u786e\u4e14\u89c6\u89c9\u5438\u5f15\u4eba\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u6df1\u5ea6\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u5728\u6df1\u5ea6\u4e0d\u8fde\u7eed\u5904\u4ea7\u751f\u4f2a\u5f71\u3002", "method": "\u91c7\u7528\u7269\u7406\u542f\u53d1\u7684\u81ea\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u7ed3\u5408\u6df1\u5ea6\u4f9d\u8d56\u7684\u6a21\u7cca\u5708\u7ea6\u675f\u548c\u81ea\u906e\u6321\u6548\u5e94\uff0c\u5e76\u9002\u914d\u6269\u6563\u6a21\u578b\u5230\u4e00\u6b65\u63a8\u7406\u65b9\u6848\u3002", "result": "\u751f\u6210\u9ad8\u8d28\u91cf\u548c\u9ad8\u4fdd\u771f\u7684\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5408\u6210\u903c\u771f\u524d\u666f\uff0c\u89e3\u51b3\u53ef\u6269\u5c55\u914d\u5bf9\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e73\u8861\u771f\u5b9e\u6027\u4e0e\u573a\u666f\u591a\u6837\u6027\u3002"}}
{"id": "2507.18064", "pdf": "https://arxiv.org/pdf/2507.18064", "abs": "https://arxiv.org/abs/2507.18064", "authors": ["Xiaoran Sun", "Liyan Wang", "Cong Wang", "Yeying Jin", "Kin-man Lam", "Zhixun Su", "Yang Yang", "Jinshan Pan"], "title": "Adapting Large VLMs with Iterative and Manual Instructions for Generative Low-light Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Most existing low-light image enhancement (LLIE) methods rely on pre-trained model priors, low-light inputs, or both, while neglecting the semantic guidance available from normal-light images. This limitation hinders their effectiveness in complex lighting conditions. In this paper, we propose VLM-IMI, a novel framework that leverages large vision-language models (VLMs) with iterative and manual instructions (IMIs) for LLIE. VLM-IMI incorporates textual descriptions of the desired normal-light content as enhancement cues, enabling semantically informed restoration. To effectively integrate cross-modal priors, we introduce an instruction prior fusion module, which dynamically aligns and fuses image and text features, promoting the generation of detailed and semantically coherent outputs. During inference, we adopt an iterative and manual instruction strategy to refine textual instructions, progressively improving visual quality. This refinement enhances structural fidelity, semantic alignment, and the recovery of fine details under extremely low-light conditions. Extensive experiments across diverse scenarios demonstrate that VLM-IMI outperforms state-of-the-art methods in both quantitative metrics and perceptual quality. The source code is available at https://github.com/sunxiaoran01/VLM-IMI.", "AI": {"tldr": "VLM-IMI\u662f\u4e00\u79cd\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u8fed\u4ee3\u624b\u52a8\u6307\u4ee4\u7684\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u6307\u5bfc\u63d0\u5347\u590d\u6742\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u5ffd\u89c6\u6b63\u5e38\u5149\u56fe\u50cf\u7684\u8bed\u4e49\u6307\u5bfc\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u6587\u672c\u63cf\u8ff0\uff0c\u901a\u8fc7\u6307\u4ee4\u5148\u9a8c\u878d\u5408\u6a21\u5757\u52a8\u6001\u5bf9\u9f50\u56fe\u50cf\u4e0e\u6587\u672c\u7279\u5f81\uff0c\u91c7\u7528\u8fed\u4ee3\u624b\u52a8\u6307\u4ee4\u7b56\u7565\u4f18\u5316\u7ed3\u679c\u3002", "result": "\u5728\u591a\u6837\u573a\u666f\u4e2d\uff0cVLM-IMI\u5728\u5b9a\u91cf\u6307\u6807\u548c\u611f\u77e5\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "VLM-IMI\u901a\u8fc7\u8bed\u4e49\u6307\u5bfc\u548c\u52a8\u6001\u878d\u5408\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u7684\u6548\u679c\u3002"}}
{"id": "2507.18082", "pdf": "https://arxiv.org/pdf/2507.18082", "abs": "https://arxiv.org/abs/2507.18082", "authors": ["Pascal Spiegler", "Taha Koleilat", "Arash Harirpoush", "Corey S. Miller", "Hassan Rivaz", "Marta Kersten-Oertel", "Yiming Xiao"], "title": "TextSAM-EUS: Text Prompt Learning for SAM to Accurately Segment Pancreatic Tumor in Endoscopic Ultrasound", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to ICCV 2025 Workshop CVAMD", "summary": "Pancreatic cancer carries a poor prognosis and relies on endoscopic ultrasound (EUS) for targeted biopsy and radiotherapy. However, the speckle noise, low contrast, and unintuitive appearance of EUS make segmentation of pancreatic tumors with fully supervised deep learning (DL) models both error-prone and dependent on large, expert-curated annotation datasets. To address these challenges, we present TextSAM-EUS, a novel, lightweight, text-driven adaptation of the Segment Anything Model (SAM) that requires no manual geometric prompts at inference. Our approach leverages text prompt learning (context optimization) through the BiomedCLIP text encoder in conjunction with a LoRA-based adaptation of SAM's architecture to enable automatic pancreatic tumor segmentation in EUS, tuning only 0.86% of the total parameters. On the public Endoscopic Ultrasound Database of the Pancreas, TextSAM-EUS with automatic prompts attains 82.69% Dice and 85.28% normalized surface distance (NSD), and with manual geometric prompts reaches 83.10% Dice and 85.70% NSD, outperforming both existing state-of-the-art (SOTA) supervised DL models and foundation models (e.g., SAM and its variants). As the first attempt to incorporate prompt learning in SAM-based medical image segmentation, TextSAM-EUS offers a practical option for efficient and robust automatic EUS segmentation. Our code will be publicly available upon acceptance.", "AI": {"tldr": "TextSAM-EUS\u662f\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u9a71\u52a8\u7684\u8f7b\u91cf\u7ea7Segment Anything Model\uff08SAM\uff09\u6539\u8fdb\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5206\u5272\u80f0\u817a\u80bf\u7624\uff0c\u65e0\u9700\u624b\u52a8\u51e0\u4f55\u63d0\u793a\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u80f0\u817a\u764c\u9884\u540e\u5dee\uff0c\u4f9d\u8d56\u5185\u955c\u8d85\u58f0\uff08EUS\uff09\u8fdb\u884c\u6d3b\u68c0\u548c\u653e\u7597\uff0c\u4f46EUS\u56fe\u50cf\u566a\u58f0\u591a\u3001\u5bf9\u6bd4\u5ea6\u4f4e\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5206\u5272\u6548\u679c\u5dee\u4e14\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u3002", "method": "\u901a\u8fc7BiomedCLIP\u6587\u672c\u7f16\u7801\u5668\u548cLoRA\u67b6\u6784\u6539\u8fdbSAM\uff0c\u5b9e\u73b0\u81ea\u52a8\u80f0\u817a\u80bf\u7624\u5206\u5272\uff0c\u4ec5\u9700\u8c03\u65740.86%\u7684\u53c2\u6570\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cTextSAM-EUS\u7684Dice\u548cNSD\u5206\u522b\u8fbe\u523082.69%\u548c85.28%\uff08\u81ea\u52a8\u63d0\u793a\uff09\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TextSAM-EUS\u662f\u9996\u4e2a\u5c06\u63d0\u793a\u5b66\u4e60\u5f15\u5165SAM\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u65b9\u6cd5\uff0c\u4e3aEUS\u5206\u5272\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.18107", "pdf": "https://arxiv.org/pdf/2507.18107", "abs": "https://arxiv.org/abs/2507.18107", "authors": ["Yubin Chen", "Xuyang Guo", "Zhenmei Shi", "Zhao Song", "Jiahao Zhang"], "title": "T2VWorldBench: A Benchmark for Evaluating World Knowledge in Text-to-Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-video (T2V) models have shown remarkable performance in generating visually reasonable scenes, while their capability to leverage world knowledge for ensuring semantic consistency and factual accuracy remains largely understudied. In response to this challenge, we propose T2VWorldBench, the first systematic evaluation framework for evaluating the world knowledge generation abilities of text-to-video models, covering 6 major categories, 60 subcategories, and 1,200 prompts across a wide range of domains, including physics, nature, activity, culture, causality, and object. To address both human preference and scalable evaluation, our benchmark incorporates both human evaluation and automated evaluation using vision-language models (VLMs). We evaluated the 10 most advanced text-to-video models currently available, ranging from open source to commercial models, and found that most models are unable to understand world knowledge and generate truly correct videos. These findings point out a critical gap in the capability of current text-to-video models to leverage world knowledge, providing valuable research opportunities and entry points for constructing models with robust capabilities for commonsense reasoning and factual generation.", "AI": {"tldr": "T2VWorldBench\u662f\u4e00\u4e2a\u8bc4\u4f30\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u4e16\u754c\u77e5\u8bc6\u751f\u6210\u80fd\u529b\u7684\u6846\u67b6\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u4e8b\u5b9e\u51c6\u786e\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "motivation": "\u7814\u7a76\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u5728\u5229\u7528\u4e16\u754c\u77e5\u8bc6\u65b9\u9762\u7684\u80fd\u529b\u4e0d\u8db3\uff0c\u63d0\u51fa\u7cfb\u7edf\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faT2VWorldBench\u6846\u67b6\uff0c\u6db5\u76d66\u5927\u7c7b\u300160\u5b50\u7c7b\u548c1200\u4e2a\u63d0\u793a\uff0c\u7ed3\u5408\u4eba\u5de5\u548c\u81ea\u52a8\u5316\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f3010\u4e2a\u5148\u8fdb\u6a21\u578b\uff0c\u53d1\u73b0\u591a\u6570\u65e0\u6cd5\u6b63\u786e\u751f\u6210\u7b26\u5408\u4e16\u754c\u77e5\u8bc6\u7684\u89c6\u9891\u3002", "conclusion": "\u5f53\u524d\u6a21\u578b\u5728\u4e16\u754c\u77e5\u8bc6\u5229\u7528\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.18144", "pdf": "https://arxiv.org/pdf/2507.18144", "abs": "https://arxiv.org/abs/2507.18144", "authors": ["Jinhong He", "Minglong Xue", "Zhipu Liu", "Mingliang Zhou", "Aoxiang Ning", "Palaiahnakote Shivakumara"], "title": "Degradation-Consistent Learning via Bidirectional Diffusion for Low-Light Image Enhancement", "categories": ["cs.CV", "eess.IV"], "comment": "10page", "summary": "Low-light image enhancement aims to improve the visibility of degraded images to better align with human visual perception. While diffusion-based methods have shown promising performance due to their strong generative capabilities. However, their unidirectional modelling of degradation often struggles to capture the complexity of real-world degradation patterns, leading to structural inconsistencies and pixel misalignments. To address these challenges, we propose a bidirectional diffusion optimization mechanism that jointly models the degradation processes of both low-light and normal-light images, enabling more precise degradation parameter matching and enhancing generation quality. Specifically, we perform bidirectional diffusion-from low-to-normal light and from normal-to-low light during training and introduce an adaptive feature interaction block (AFI) to refine feature representation. By leveraging the complementarity between these two paths, our approach imposes an implicit symmetry constraint on illumination attenuation and noise distribution, facilitating consistent degradation learning and improving the models ability to perceive illumination and detail degradation. Additionally, we design a reflection-aware correction module (RACM) to guide color restoration post-denoising and suppress overexposed regions, ensuring content consistency and generating high-quality images that align with human visual perception. Extensive experiments on multiple benchmark datasets demonstrate that our method outperforms state-of-the-art methods in both quantitative and qualitative evaluations while generalizing effectively to diverse degradation scenarios. Code at https://github.com/hejh8/BidDiff", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5411\u6269\u6563\u4f18\u5316\u673a\u5236\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u4f4e\u5149\u4e0e\u6b63\u5e38\u5149\u56fe\u50cf\u7684\u9000\u5316\u8fc7\u7a0b\uff0c\u63d0\u5347\u56fe\u50cf\u589e\u5f3a\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6269\u6563\u65b9\u6cd5\u56e0\u5355\u5411\u5efa\u6a21\u9000\u5316\u800c\u96be\u4ee5\u6355\u6349\u771f\u5b9e\u4e16\u754c\u590d\u6742\u9000\u5316\u6a21\u5f0f\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53cc\u5411\u6269\u6563\uff08\u4f4e\u5149\u5230\u6b63\u5e38\u5149\u53ca\u6b63\u5e38\u5149\u5230\u4f4e\u5149\uff09\u8bad\u7ec3\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u7279\u5f81\u4ea4\u4e92\u5757\uff08AFI\uff09\u548c\u53cd\u5c04\u611f\u77e5\u6821\u6b63\u6a21\u5757\uff08RACM\uff09\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u80fd\u6cdb\u5316\u5230\u591a\u6837\u5316\u9000\u5316\u573a\u666f\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u53cc\u5411\u5efa\u6a21\u548c\u7279\u5f81\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u7684\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2507.18192", "pdf": "https://arxiv.org/pdf/2507.18192", "abs": "https://arxiv.org/abs/2507.18192", "authors": ["Minghao Fu", "Guo-Hua Wang", "Xiaohao Chen", "Qing-Guo Chen", "Zhao Xu", "Weihua Luo", "Kaifu Zhang"], "title": "TeEFusion: Blending Text Embeddings to Distill Classifier-Free Guidance", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025. The code is publicly available at   https://github.com/AIDC-AI/TeEFusion", "summary": "Recent advances in text-to-image synthesis largely benefit from sophisticated sampling strategies and classifier-free guidance (CFG) to ensure high-quality generation. However, CFG's reliance on two forward passes, especially when combined with intricate sampling algorithms, results in prohibitively high inference costs. To address this, we introduce TeEFusion (\\textbf{Te}xt \\textbf{E}mbeddings \\textbf{Fusion}), a novel and efficient distillation method that directly incorporates the guidance magnitude into the text embeddings and distills the teacher model's complex sampling strategy. By simply fusing conditional and unconditional text embeddings using linear operations, TeEFusion reconstructs the desired guidance without adding extra parameters, simultaneously enabling the student model to learn from the teacher's output produced via its sophisticated sampling approach. Extensive experiments on state-of-the-art models such as SD3 demonstrate that our method allows the student to closely mimic the teacher's performance with a far simpler and more efficient sampling strategy. Consequently, the student model achieves inference speeds up to 6$\\times$ faster than the teacher model, while maintaining image quality at levels comparable to those obtained through the teacher's complex sampling approach. The code is publicly available at \\href{https://github.com/AIDC-AI/TeEFusion}{github.com/AIDC-AI/TeEFusion}.", "AI": {"tldr": "TeEFusion\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5f15\u5bfc\u5e45\u5ea6\u76f4\u63a5\u878d\u5165\u6587\u672c\u5d4c\u5165\u5e76\u84b8\u998f\u590d\u6742\u91c7\u6837\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u89e3\u51b3CFG\u4f9d\u8d56\u4e24\u6b21\u524d\u5411\u4f20\u64ad\u548c\u590d\u6742\u91c7\u6837\u7b56\u7565\u5bfc\u81f4\u7684\u9ad8\u63a8\u7406\u6210\u672c\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7ebf\u6027\u64cd\u4f5c\u878d\u5408\u6761\u4ef6\u548c\u975e\u6761\u4ef6\u6587\u672c\u5d4c\u5165\uff0c\u65e0\u9700\u989d\u5916\u53c2\u6570\uff0c\u5b66\u751f\u6a21\u578b\u5b66\u4e60\u6559\u5e08\u6a21\u578b\u7684\u8f93\u51fa\u3002", "result": "\u5b66\u751f\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u63d0\u53476\u500d\uff0c\u56fe\u50cf\u8d28\u91cf\u63a5\u8fd1\u6559\u5e08\u6a21\u578b\u3002", "conclusion": "TeEFusion\u5728\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u3002"}}
{"id": "2507.18214", "pdf": "https://arxiv.org/pdf/2507.18214", "abs": "https://arxiv.org/abs/2507.18214", "authors": ["Qilin Huang", "Tianyu Lin", "Zhiguang Chen", "Fudan Zheng"], "title": "LEAF: Latent Diffusion with Efficient Encoder Distillation for Aligned Features in Medical Image Segmentation", "categories": ["cs.CV"], "comment": "Accepted at MICCAI 2025", "summary": "Leveraging the powerful capabilities of diffusion models has yielded quite effective results in medical image segmentation tasks. However, existing methods typically transfer the original training process directly without specific adjustments for segmentation tasks. Furthermore, the commonly used pre-trained diffusion models still have deficiencies in feature extraction. Based on these considerations, we propose LEAF, a medical image segmentation model grounded in latent diffusion models. During the fine-tuning process, we replace the original noise prediction pattern with a direct prediction of the segmentation map, thereby reducing the variance of segmentation results. We also employ a feature distillation method to align the hidden states of the convolutional layers with the features from a transformer-based vision encoder. Experimental results demonstrate that our method enhances the performance of the original diffusion model across multiple segmentation datasets for different disease types. Notably, our approach does not alter the model architecture, nor does it increase the number of parameters or computation during the inference phase, making it highly efficient.", "AI": {"tldr": "LEAF\u662f\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u9884\u6d4b\u5206\u5272\u56fe\u548c\u4f7f\u7528\u7279\u5f81\u84b8\u998f\u6280\u672f\uff0c\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u9ad8\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\u672a\u9488\u5bf9\u4efb\u52a1\u7279\u6027\u8fdb\u884c\u8c03\u6574\uff0c\u4e14\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\uff0c\u5c06\u566a\u58f0\u9884\u6d4b\u6a21\u5f0f\u66ff\u6362\u4e3a\u76f4\u63a5\u9884\u6d4b\u5206\u5272\u56fe\uff0c\u5e76\u91c7\u7528\u7279\u5f81\u84b8\u998f\u65b9\u6cd5\u5bf9\u9f50\u5377\u79ef\u5c42\u548cTransformer\u7f16\u7801\u5668\u7684\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLEAF\u5728\u591a\u79cd\u75be\u75c5\u7c7b\u578b\u7684\u5206\u5272\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u4e86\u539f\u59cb\u6269\u6563\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e14\u672a\u589e\u52a0\u63a8\u7406\u65f6\u7684\u53c2\u6570\u6216\u8ba1\u7b97\u91cf\u3002", "conclusion": "LEAF\u5728\u4e0d\u6539\u53d8\u6a21\u578b\u67b6\u6784\u6216\u589e\u52a0\u8ba1\u7b97\u8d1f\u62c5\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6548\u679c\u3002"}}
{"id": "2507.18371", "pdf": "https://arxiv.org/pdf/2507.18371", "abs": "https://arxiv.org/abs/2507.18371", "authors": ["Xiaotian Chen", "DongFu Yin", "Fei Richard Yu", "Xuanchen Li", "Xinhao Zhang"], "title": "MVG4D: Image Matrix-Based Multi-View and Motion Generation for 4D Content Creation from a Single Image", "categories": ["cs.CV"], "comment": null, "summary": "Advances in generative modeling have significantly enhanced digital content creation, extending from 2D images to complex 3D and 4D scenes. Despite substantial progress, producing high-fidelity and temporally consistent dynamic 4D content remains a challenge. In this paper, we propose MVG4D, a novel framework that generates dynamic 4D content from a single still image by combining multi-view synthesis with 4D Gaussian Splatting (4D GS). At its core, MVG4D employs an image matrix module that synthesizes temporally coherent and spatially diverse multi-view images, providing rich supervisory signals for downstream 3D and 4D reconstruction. These multi-view images are used to optimize a 3D Gaussian point cloud, which is further extended into the temporal domain via a lightweight deformation network. Our method effectively enhances temporal consistency, geometric fidelity, and visual realism, addressing key challenges in motion discontinuity and background degradation that affect prior 4D GS-based methods. Extensive experiments on the Objaverse dataset demonstrate that MVG4D outperforms state-of-the-art baselines in CLIP-I, PSNR, FVD, and time efficiency. Notably, it reduces flickering artifacts and sharpens structural details across views and time, enabling more immersive AR/VR experiences. MVG4D sets a new direction for efficient and controllable 4D generation from minimal inputs.", "AI": {"tldr": "MVG4D\u662f\u4e00\u4e2a\u4ece\u5355\u5f20\u9759\u6001\u56fe\u50cf\u751f\u6210\u52a8\u60014D\u5185\u5bb9\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u89c6\u56fe\u5408\u6210\u548c4D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u771f\u5b9e\u611f\u3002", "motivation": "\u5c3d\u7ba1\u751f\u6210\u6a21\u578b\u5728\u6570\u5b57\u5185\u5bb9\u521b\u4f5c\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u751f\u6210\u9ad8\u4fdd\u771f\u4e14\u65f6\u95f4\u4e00\u81f4\u7684\u52a8\u60014D\u5185\u5bb9\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "MVG4D\u901a\u8fc7\u56fe\u50cf\u77e9\u9635\u6a21\u5757\u5408\u6210\u65f6\u95f4\u76f8\u5e72\u4e14\u7a7a\u95f4\u591a\u6837\u5316\u7684\u591a\u89c6\u56fe\u56fe\u50cf\uff0c\u5e76\u5229\u7528\u8f7b\u91cf\u7ea7\u53d8\u5f62\u7f51\u7edc\u5c063D\u9ad8\u65af\u70b9\u4e91\u6269\u5c55\u5230\u65f6\u95f4\u57df\u3002", "result": "\u5728Objaverse\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMVG4D\u5728CLIP-I\u3001PSNR\u3001FVD\u548c\u65f6\u95f4\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e86\u95ea\u70c1\u4f2a\u5f71\u5e76\u589e\u5f3a\u4e86\u7ed3\u6784\u7ec6\u8282\u3002", "conclusion": "MVG4D\u4e3a\u4ece\u6700\u5c0f\u8f93\u5165\u9ad8\u6548\u53ef\u63a7\u5730\u751f\u62104D\u5185\u5bb9\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u63d0\u5347\u4e86AR/VR\u4f53\u9a8c\u3002"}}
{"id": "2507.18385", "pdf": "https://arxiv.org/pdf/2507.18385", "abs": "https://arxiv.org/abs/2507.18385", "authors": ["Yu Jiang", "Jiahao Xia", "Jiongming Qin", "Yusen Wang", "Tuo Cao", "Chunxia Xiao"], "title": "HumanMaterial: Human Material Estimation from a Single Image via Progressive Training", "categories": ["cs.CV"], "comment": "14", "summary": "Full-body Human inverse rendering based on physically-based rendering aims to acquire high-quality materials, which helps achieve photo-realistic rendering under arbitrary illuminations. This task requires estimating multiple material maps and usually relies on the constraint of rendering result. The absence of constraints on the material maps makes inverse rendering an ill-posed task. Previous works alleviated this problem by building material dataset for training, but their simplified material data and rendering equation lead to rendering results with limited realism, especially that of skin. To further alleviate this problem, we construct a higher-quality dataset (OpenHumanBRDF) based on scanned real data and statistical material data. In addition to the normal, diffuse albedo, roughness, specular albedo, we produce displacement and subsurface scattering to enhance the realism of rendering results, especially for the skin. With the increase in prediction tasks for more materials, using an end-to-end model as in the previous work struggles to balance the importance among various material maps, and leads to model underfitting. Therefore, we design a model (HumanMaterial) with progressive training strategy to make full use of the supervision information of the material maps and improve the performance of material estimation. HumanMaterial first obtain the initial material results via three prior models, and then refine the results by a finetuning model. Prior models estimate different material maps, and each map has different significance for rendering results. Thus, we design a Controlled PBR Rendering (CPR) loss, which enhances the importance of the materials to be optimized during the training of prior models. Extensive experiments on OpenHumanBRDF dataset and real data demonstrate that our method achieves state-of-the-art performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u6e32\u67d3\u7684\u5168\u4eba\u4f53\u9006\u5411\u6e32\u67d3\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u8bbe\u8ba1\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u6a21\u578b\uff0c\u63d0\u5347\u6750\u8d28\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u76ae\u80a4\u7684\u771f\u5b9e\u611f\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u56e0\u6750\u8d28\u6570\u636e\u7b80\u5316\u548c\u6e32\u67d3\u65b9\u7a0b\u9650\u5236\u5bfc\u81f4\u7684\u6e32\u67d3\u7ed3\u679c\u771f\u5b9e\u611f\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u76ae\u80a4\u6e32\u67d3\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efaOpenHumanBRDF\u6570\u636e\u96c6\uff0c\u5305\u542b\u4f4d\u79fb\u548c\u6b21\u8868\u9762\u6563\u5c04\u7b49\u6750\u8d28\uff1b\u8bbe\u8ba1HumanMaterial\u6a21\u578b\uff0c\u91c7\u7528\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\u548cCPR\u635f\u5931\u51fd\u6570\u4f18\u5316\u6750\u8d28\u4f30\u8ba1\u3002", "result": "\u5728OpenHumanBRDF\u6570\u636e\u96c6\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6750\u8d28\u4f30\u8ba1\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5168\u4eba\u4f53\u9006\u5411\u6e32\u67d3\u7684\u771f\u5b9e\u611f\uff0c\u5c24\u5176\u662f\u76ae\u80a4\u6e32\u67d3\u6548\u679c\u3002"}}
{"id": "2507.18405", "pdf": "https://arxiv.org/pdf/2507.18405", "abs": "https://arxiv.org/abs/2507.18405", "authors": ["Simin Huo", "Ning Li"], "title": "Iwin Transformer: Hierarchical Vision Transformer using Interleaved Windows", "categories": ["cs.CV", "cs.LG"], "comment": "14 pages, 10 figures, Submitted to IEEE Transactions on Pattern   Analysis and Machine Intelligence", "summary": "We introduce Iwin Transformer, a novel position-embedding-free hierarchical vision transformer, which can be fine-tuned directly from low to high resolution, through the collaboration of innovative interleaved window attention and depthwise separable convolution. This approach uses attention to connect distant tokens and applies convolution to link neighboring tokens, enabling global information exchange within a single module, overcoming Swin Transformer's limitation of requiring two consecutive blocks to approximate global attention. Extensive experiments on visual benchmarks demonstrate that Iwin Transformer exhibits strong competitiveness in tasks such as image classification (87.4 top-1 accuracy on ImageNet-1K), semantic segmentation and video action recognition. We also validate the effectiveness of the core component in Iwin as a standalone module that can seamlessly replace the self-attention module in class-conditional image generation. The concepts and methods introduced by the Iwin Transformer have the potential to inspire future research, like Iwin 3D Attention in video generation. The code and models are available at https://github.com/cominder/Iwin-Transformer.", "AI": {"tldr": "Iwin Transformer\u662f\u4e00\u79cd\u65e0\u9700\u4f4d\u7f6e\u5d4c\u5165\u7684\u5206\u5c42\u89c6\u89c9Transformer\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u4ea4\u9519\u7a97\u53e3\u6ce8\u610f\u529b\u548c\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\uff0c\u53ef\u76f4\u63a5\u4ece\u4f4e\u5206\u8fa8\u7387\u5230\u9ad8\u5206\u8fa8\u7387\u8fdb\u884c\u5fae\u8c03\u3002", "motivation": "\u89e3\u51b3Swin Transformer\u9700\u8981\u4e24\u4e2a\u8fde\u7eed\u5757\u624d\u80fd\u8fd1\u4f3c\u5168\u5c40\u6ce8\u610f\u529b\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u5355\u6a21\u5757\u5185\u7684\u5168\u5c40\u4fe1\u606f\u4ea4\u6362\u3002", "method": "\u7ed3\u5408\u4ea4\u9519\u7a97\u53e3\u6ce8\u610f\u529b\uff08\u8fde\u63a5\u8fdc\u8ddd\u79bb\u6807\u8bb0\uff09\u548c\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\uff08\u8fde\u63a5\u90bb\u8fd1\u6807\u8bb0\uff09\uff0c\u5b9e\u73b0\u5168\u5c40\u4fe1\u606f\u4ea4\u6362\u3002", "result": "\u5728ImageNet-1K\u4e0a\u8fbe\u523087.4%\u7684top-1\u51c6\u786e\u7387\uff0c\u5728\u8bed\u4e49\u5206\u5272\u548c\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Iwin Transformer\u7684\u6838\u5fc3\u7ec4\u4ef6\u53ef\u4f5c\u4e3a\u72ec\u7acb\u6a21\u5757\u66ff\u6362\u81ea\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5176\u65b9\u6cd5\u6709\u671b\u542f\u53d1\u672a\u6765\u7814\u7a76\uff0c\u5982\u89c6\u9891\u751f\u6210\u4e2d\u7684Iwin 3D\u6ce8\u610f\u529b\u3002"}}
{"id": "2507.18473", "pdf": "https://arxiv.org/pdf/2507.18473", "abs": "https://arxiv.org/abs/2507.18473", "authors": ["Haoran Xu", "Saining Zhang", "Peishuo Li", "Baijun Ye", "Xiaoxue Chen", "Huan-ang Gao", "Jv Zheng", "Xiaowei Song", "Ziqiao Peng", "Run Miao", "Jinrang Jia", "Yifeng Shi", "Guangqi Yi", "Hang Zhao", "Hao Tang", "Hongyang Li", "Kaicheng Yu", "Hao Zhao"], "title": "CRUISE: Cooperative Reconstruction and Editing in V2X Scenarios using Gaussian Splatting", "categories": ["cs.CV"], "comment": "IROS 2025, Code: https://github.com/SainingZhang/CRUISE", "summary": "Vehicle-to-everything (V2X) communication plays a crucial role in autonomous driving, enabling cooperation between vehicles and infrastructure. While simulation has significantly contributed to various autonomous driving tasks, its potential for data generation and augmentation in V2X scenarios remains underexplored. In this paper, we introduce CRUISE, a comprehensive reconstruction-and-synthesis framework designed for V2X driving environments. CRUISE employs decomposed Gaussian Splatting to accurately reconstruct real-world scenes while supporting flexible editing. By decomposing dynamic traffic participants into editable Gaussian representations, CRUISE allows for seamless modification and augmentation of driving scenes. Furthermore, the framework renders images from both ego-vehicle and infrastructure views, enabling large-scale V2X dataset augmentation for training and evaluation. Our experimental results demonstrate that: 1) CRUISE reconstructs real-world V2X driving scenes with high fidelity; 2) using CRUISE improves 3D detection across ego-vehicle, infrastructure, and cooperative views, as well as cooperative 3D tracking on the V2X-Seq benchmark; and 3) CRUISE effectively generates challenging corner cases.", "AI": {"tldr": "CRUISE\u662f\u4e00\u4e2a\u7528\u4e8eV2X\u9a7e\u9a76\u73af\u5883\u7684\u7efc\u5408\u91cd\u5efa\u4e0e\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u9ad8\u65af\u6563\u5c04\u6280\u672f\u5b9e\u73b0\u9ad8\u4fdd\u771f\u91cd\u5efa\u548c\u7075\u6d3b\u7f16\u8f91\uff0c\u63d0\u53473D\u68c0\u6d4b\u548c\u8ddf\u8e2a\u6027\u80fd\uff0c\u5e76\u751f\u6210\u5177\u6709\u6311\u6218\u6027\u7684\u6781\u7aef\u6848\u4f8b\u3002", "motivation": "\u63a2\u7d22\u4eff\u771f\u5728V2X\u573a\u666f\u4e2d\u6570\u636e\u751f\u6210\u548c\u589e\u5f3a\u7684\u6f5c\u529b\uff0c\u4ee5\u652f\u6301\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8f66\u8f86\u4e0e\u57fa\u7840\u8bbe\u65bd\u534f\u4f5c\u3002", "method": "\u91c7\u7528\u5206\u89e3\u9ad8\u65af\u6563\u5c04\u6280\u672f\u91cd\u5efa\u771f\u5b9e\u573a\u666f\uff0c\u652f\u6301\u52a8\u6001\u4ea4\u901a\u53c2\u4e0e\u8005\u7684\u7f16\u8f91\u548c\u56fe\u50cf\u6e32\u67d3\uff0c\u7528\u4e8e\u6570\u636e\u96c6\u589e\u5f3a\u3002", "result": "CRUISE\u5b9e\u73b0\u9ad8\u4fdd\u771f\u91cd\u5efa\uff0c\u63d0\u53473D\u68c0\u6d4b\u548c\u8ddf\u8e2a\u6027\u80fd\uff0c\u5e76\u80fd\u751f\u6210\u6781\u7aef\u6848\u4f8b\u3002", "conclusion": "CRUISE\u4e3aV2X\u573a\u666f\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u751f\u6210\u548c\u589e\u5f3a\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.18532", "pdf": "https://arxiv.org/pdf/2507.18532", "abs": "https://arxiv.org/abs/2507.18532", "authors": ["Akbar Ali", "Mahek Vyas", "Soumyaratna Debnath", "Chanda Grover Kamra", "Jaidev Sanjay Khalane", "Reuben Shibu Devanesan", "Indra Deep Mastan", "Subramanian Sankaranarayanan", "Pankaj Khanna", "Shanmuganathan Raman"], "title": "COT-AD: Cotton Analysis Dataset", "categories": ["cs.CV", "I.4.9; I.5.4; H.2.8"], "comment": "Dataset publicly available at:   https://ieee-dataport.org/documents/cot-adcotton-analysis-dataset. Accepted   to IEEE International Conference on Image Processing (ICIP) 2025", "summary": "This paper presents COT-AD, a comprehensive Dataset designed to enhance cotton crop analysis through computer vision. Comprising over 25,000 images captured throughout the cotton growth cycle, with 5,000 annotated images, COT-AD includes aerial imagery for field-scale detection and segmentation and high-resolution DSLR images documenting key diseases. The annotations cover pest and disease recognition, vegetation, and weed analysis, addressing a critical gap in cotton-specific agricultural datasets. COT-AD supports tasks such as classification, segmentation, image restoration, enhancement, deep generative model-based cotton crop synthesis, and early disease management, advancing data-driven crop management", "AI": {"tldr": "COT-AD\u662f\u4e00\u4e2a\u7528\u4e8e\u68c9\u82b1\u4f5c\u7269\u5206\u6790\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u6570\u636e\u96c6\uff0c\u5305\u542b25,000\u591a\u5f20\u56fe\u50cf\u548c5,000\u5f20\u6807\u6ce8\u56fe\u50cf\uff0c\u652f\u6301\u5206\u7c7b\u3001\u5206\u5272\u3001\u56fe\u50cf\u4fee\u590d\u7b49\u591a\u79cd\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u68c9\u82b1\u519c\u4e1a\u6570\u636e\u96c6\u7684\u4e0d\u8db3\uff0c\u63d0\u4f9b\u5168\u9762\u7684\u68c9\u82b1\u751f\u957f\u5468\u671f\u56fe\u50cf\u548c\u6807\u6ce8\u6570\u636e\u3002", "method": "\u901a\u8fc7\u822a\u62cd\u548c\u9ad8\u5206\u8fa8\u7387DSLR\u56fe\u50cf\u6536\u96c6\u6570\u636e\uff0c\u6807\u6ce8\u5305\u62ec\u75c5\u866b\u5bb3\u8bc6\u522b\u3001\u690d\u88ab\u548c\u6742\u8349\u5206\u6790\u3002", "result": "COT-AD\u6570\u636e\u96c6\u652f\u6301\u591a\u79cd\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff0c\u63a8\u52a8\u4e86\u6570\u636e\u9a71\u52a8\u7684\u4f5c\u7269\u7ba1\u7406\u3002", "conclusion": "COT-AD\u586b\u8865\u4e86\u68c9\u82b1\u519c\u4e1a\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4e3a\u4f5c\u7269\u7ba1\u7406\u548c\u75be\u75c5\u65e9\u671f\u9632\u6cbb\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2507.18534", "pdf": "https://arxiv.org/pdf/2507.18534", "abs": "https://arxiv.org/abs/2507.18534", "authors": ["Xingyu Qiu", "Mengying Yang", "Xinghua Ma", "Dong Liang", "Yuzhen Li", "Fanding Li", "Gongning Luo", "Wei Wang", "Kuanquan Wang", "Shuo Li"], "title": "Elucidating the Design Space of Arbitrary-Noise-Based Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": "21 pages, 4 figures", "summary": "EDM elucidates the unified design space of diffusion models, yet its fixed noise patterns restricted to pure Gaussian noise, limit advancements in image restoration. Our study indicates that forcibly injecting Gaussian noise corrupts the degraded images, overextends the image transformation distance, and increases restoration complexity. To address this problem, our proposed EDA Elucidates the Design space of Arbitrary-noise-based diffusion models. Theoretically, EDA expands the freedom of noise pattern while preserving the original module flexibility of EDM, with rigorous proof that increased noise complexity incurs no additional computational overhead during restoration. EDA is validated on three typical tasks: MRI bias field correction (global smooth noise), CT metal artifact reduction (global sharp noise), and natural image shadow removal (local boundary-aware noise). With only 5 sampling steps, EDA outperforms most task-specific methods and achieves state-of-the-art performance in bias field correction and shadow removal.", "AI": {"tldr": "EDA\u6269\u5c55\u4e86\u57fa\u4e8e\u4efb\u610f\u566a\u58f0\u7684\u6269\u6563\u6a21\u578b\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u89e3\u51b3\u4e86EDM\u56fa\u5b9a\u9ad8\u65af\u566a\u58f0\u9650\u5236\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u56fe\u50cf\u6062\u590d\u6027\u80fd\u3002", "motivation": "EDM\u7684\u56fa\u5b9a\u9ad8\u65af\u566a\u58f0\u6a21\u5f0f\u9650\u5236\u4e86\u56fe\u50cf\u6062\u590d\u7684\u8fdb\u5c55\uff0c\u5f3a\u5236\u6ce8\u5165\u9ad8\u65af\u566a\u58f0\u4f1a\u7834\u574f\u9000\u5316\u56fe\u50cf\u5e76\u589e\u52a0\u6062\u590d\u590d\u6742\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86EDA\uff0c\u6269\u5c55\u566a\u58f0\u6a21\u5f0f\u7684\u81ea\u7531\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301EDM\u6a21\u5757\u7684\u7075\u6d3b\u6027\uff0c\u5e76\u8bc1\u660e\u566a\u58f0\u590d\u6742\u5ea6\u589e\u52a0\u4e0d\u4f1a\u5e26\u6765\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728MRI\u504f\u7f6e\u573a\u6821\u6b63\u3001CT\u91d1\u5c5e\u4f2a\u5f71\u51cf\u5c11\u548c\u81ea\u7136\u56fe\u50cf\u9634\u5f71\u53bb\u9664\u4efb\u52a1\u4e2d\uff0c\u4ec5\u97005\u6b21\u91c7\u6837\u6b65\u9aa4\uff0cEDA\u8868\u73b0\u4f18\u4e8e\u5927\u591a\u6570\u4efb\u52a1\u4e13\u7528\u65b9\u6cd5\uff0c\u5e76\u5728\u504f\u7f6e\u573a\u6821\u6b63\u548c\u9634\u5f71\u53bb\u9664\u4e2d\u8fbe\u5230SOTA\u3002", "conclusion": "EDA\u901a\u8fc7\u5f15\u5165\u4efb\u610f\u566a\u58f0\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u3002"}}
{"id": "2507.18541", "pdf": "https://arxiv.org/pdf/2507.18541", "abs": "https://arxiv.org/abs/2507.18541", "authors": ["Chong Cheng", "Zijian Wang", "Sicheng Yu", "Yu Hu", "Nanjie Yao", "Hao Wang"], "title": "Unposed 3DGS Reconstruction with Probabilistic Procrustes Mapping", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has emerged as a core technique for 3D representation. Its effectiveness largely depends on precise camera poses and accurate point cloud initialization, which are often derived from pretrained Multi-View Stereo (MVS) models. However, in unposed reconstruction task from hundreds of outdoor images, existing MVS models may struggle with memory limits and lose accuracy as the number of input images grows. To address this limitation, we propose a novel unposed 3DGS reconstruction framework that integrates pretrained MVS priors with the probabilistic Procrustes mapping strategy. The method partitions input images into subsets, maps submaps into a global space, and jointly optimizes geometry and poses with 3DGS. Technically, we formulate the mapping of tens of millions of point clouds as a probabilistic Procrustes problem and solve a closed-form alignment. By employing probabilistic coupling along with a soft dustbin mechanism to reject uncertain correspondences, our method globally aligns point clouds and poses within minutes across hundreds of images. Moreover, we propose a joint optimization framework for 3DGS and camera poses. It constructs Gaussians from confidence-aware anchor points and integrates 3DGS differentiable rendering with an analytical Jacobian to jointly refine scene and poses, enabling accurate reconstruction and pose estimation. Experiments on Waymo and KITTI datasets show that our method achieves accurate reconstruction from unposed image sequences, setting a new state of the art for unposed 3DGS reconstruction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u59ff\u60013D\u9ad8\u65af\u5206\u5e03\u91cd\u5efa\u6846\u67b6\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3MVS\u5148\u9a8c\u548c\u6982\u7387Procrustes\u6620\u5c04\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u6237\u5916\u56fe\u50cf\u91cd\u5efa\u4e2d\u7684\u5185\u5b58\u548c\u7cbe\u5ea6\u95ee\u9898\u3002", "motivation": "\u73b0\u6709MVS\u6a21\u578b\u5728\u5927\u89c4\u6a21\u65e0\u59ff\u6001\u56fe\u50cf\u91cd\u5efa\u4e2d\u9762\u4e34\u5185\u5b58\u9650\u5236\u548c\u7cbe\u5ea6\u4e0b\u964d\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u8f93\u5165\u56fe\u50cf\u5212\u5206\u4e3a\u5b50\u96c6\uff0c\u901a\u8fc7\u6982\u7387Procrustes\u6620\u5c04\u5168\u5c40\u5bf9\u9f50\u70b9\u4e91\u548c\u59ff\u6001\uff0c\u5e76\u8054\u5408\u4f18\u53163D\u9ad8\u65af\u5206\u5e03\u548c\u76f8\u673a\u59ff\u6001\u3002", "result": "\u5728Waymo\u548cKITTI\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u91cd\u5efa\uff0c\u6210\u4e3a\u65e0\u59ff\u60013D\u9ad8\u65af\u5206\u5e03\u91cd\u5efa\u7684\u65b0\u6807\u6746\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u65e0\u59ff\u6001\u56fe\u50cf\u91cd\u5efa\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u91cd\u5efa\u7cbe\u5ea6\u548c\u6548\u7387\u3002"}}
{"id": "2507.18566", "pdf": "https://arxiv.org/pdf/2507.18566", "abs": "https://arxiv.org/abs/2507.18566", "authors": ["Nitish Shukla", "Arun Ross"], "title": "Facial Demorphing from a Single Morph Using a Latent Conditional GAN", "categories": ["cs.CV"], "comment": null, "summary": "A morph is created by combining two (or more) face images from two (or more) identities to create a composite image that is highly similar to both constituent identities, allowing the forged morph to be biometrically associated with more than one individual. Morph Attack Detection (MAD) can be used to detect a morph, but does not reveal the constituent images. Demorphing - the process of deducing the constituent images - is thus vital to provide additional evidence about a morph. Existing demorphing methods suffer from the morph replication problem, where the outputs tend to look very similar to the morph itself, or assume that train and test morphs are generated using the same morph technique. The proposed method overcomes these issues. The method decomposes a morph in latent space allowing it to demorph images created from unseen morph techniques and face styles. We train our method on morphs created from synthetic faces and test on morphs created from real faces using arbitrary morph techniques. Our method outperforms existing methods by a considerable margin and produces high fidelity demorphed face images.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53bb\u53d8\u5f62\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u53bb\u53d8\u5f62\u8fc7\u7a0b\u4e2d\u5bb9\u6613\u590d\u5236\u53d8\u5f62\u56fe\u50cf\u6216\u4f9d\u8d56\u76f8\u540c\u53d8\u5f62\u6280\u672f\u7684\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5728\u6f5c\u5728\u7a7a\u95f4\u5206\u89e3\u53d8\u5f62\u56fe\u50cf\uff0c\u9002\u7528\u4e8e\u672a\u89c1\u8fc7\u7684\u53d8\u5f62\u6280\u672f\u548c\u4eba\u8138\u98ce\u683c\u3002", "motivation": "\u73b0\u6709\u7684\u53bb\u53d8\u5f62\u65b9\u6cd5\u5b58\u5728\u53d8\u5f62\u590d\u5236\u95ee\u9898\uff0c\u4e14\u901a\u5e38\u5047\u8bbe\u8bad\u7ec3\u548c\u6d4b\u8bd5\u4f7f\u7528\u76f8\u540c\u7684\u53d8\u5f62\u6280\u672f\u3002\u672c\u6587\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u901a\u7528\u7684\u53bb\u53d8\u5f62\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5728\u6f5c\u5728\u7a7a\u95f4\u5206\u89e3\u53d8\u5f62\u56fe\u50cf\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u672a\u89c1\u8fc7\u7684\u53d8\u5f62\u6280\u672f\u548c\u4eba\u8138\u98ce\u683c\u3002\u8bad\u7ec3\u4f7f\u7528\u5408\u6210\u4eba\u8138\u751f\u6210\u7684\u53d8\u5f62\u56fe\u50cf\uff0c\u6d4b\u8bd5\u65f6\u5e94\u7528\u4e8e\u771f\u5b9e\u4eba\u8138\u548c\u4efb\u610f\u53d8\u5f62\u6280\u672f\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u53bb\u53d8\u5f62\u4eba\u8138\u56fe\u50cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u53bb\u53d8\u5f62\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u53d8\u5f62\u6280\u672f\u548c\u4eba\u8138\u98ce\u683c\u3002"}}
{"id": "2507.18569", "pdf": "https://arxiv.org/pdf/2507.18569", "abs": "https://arxiv.org/abs/2507.18569", "authors": ["Yanzuo Lu", "Yuxi Ren", "Xin Xia", "Shanchuan Lin", "Xing Wang", "Xuefeng Xiao", "Andy J. Ma", "Xiaohua Xie", "Jian-Huang Lai"], "title": "Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025 (Highlight)", "summary": "Distribution Matching Distillation (DMD) is a promising score distillation technique that compresses pre-trained teacher diffusion models into efficient one-step or multi-step student generators. Nevertheless, its reliance on the reverse Kullback-Leibler (KL) divergence minimization potentially induces mode collapse (or mode-seeking) in certain applications. To circumvent this inherent drawback, we propose Adversarial Distribution Matching (ADM), a novel framework that leverages diffusion-based discriminators to align the latent predictions between real and fake score estimators for score distillation in an adversarial manner. In the context of extremely challenging one-step distillation, we further improve the pre-trained generator by adversarial distillation with hybrid discriminators in both latent and pixel spaces. Different from the mean squared error used in DMD2 pre-training, our method incorporates the distributional loss on ODE pairs collected from the teacher model, and thus providing a better initialization for score distillation fine-tuning in the next stage. By combining the adversarial distillation pre-training with ADM fine-tuning into a unified pipeline termed DMDX, our proposed method achieves superior one-step performance on SDXL compared to DMD2 while consuming less GPU time. Additional experiments that apply multi-step ADM distillation on SD3-Medium, SD3.5-Large, and CogVideoX set a new benchmark towards efficient image and video synthesis.", "AI": {"tldr": "DMDX\u662f\u4e00\u79cd\u901a\u8fc7\u5bf9\u6297\u6027\u84b8\u998f\u9884\u8bad\u7ec3\u548cADM\u5fae\u8c03\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e00\u6b65\u751f\u6210\u6027\u80fd\uff0c\u5e76\u5728\u56fe\u50cf\u548c\u89c6\u9891\u5408\u6210\u4e2d\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\u3002", "motivation": "\u89e3\u51b3DMD\u4e2d\u53cd\u5411KL\u6563\u5ea6\u6700\u5c0f\u5316\u5bfc\u81f4\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u63d0\u51fa\u66f4\u6709\u6548\u7684\u5206\u6570\u84b8\u998f\u65b9\u6cd5\u3002", "method": "\u63d0\u51faADM\u6846\u67b6\uff0c\u5229\u7528\u6269\u6563\u5224\u522b\u5668\u5728\u6f5c\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u771f\u5b9e\u548c\u865a\u5047\u5206\u6570\u4f30\u8ba1\u5668\uff1b\u7ed3\u5408\u5bf9\u6297\u6027\u84b8\u998f\u9884\u8bad\u7ec3\u548cADM\u5fae\u8c03\u4e3aDMDX\u3002", "result": "\u5728SDXL\u4e0a\u7684\u4e00\u6b65\u751f\u6210\u6027\u80fd\u4f18\u4e8eDMD2\uff0cGPU\u65f6\u95f4\u66f4\u5c11\uff1b\u5728SD3-Medium\u3001SD3.5-Large\u548cCogVideoX\u4e0a\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\u3002", "conclusion": "DMDX\u901a\u8fc7\u5bf9\u6297\u6027\u84b8\u998f\u548cADM\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u6570\u84b8\u998f\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002"}}
{"id": "2507.18594", "pdf": "https://arxiv.org/pdf/2507.18594", "abs": "https://arxiv.org/abs/2507.18594", "authors": ["Xuecheng Bai", "Yuxiang Wang", "Boyu Hu", "Qinyuan Jie", "Chuanzhi Xu", "Hongru Xiao", "Kechen Li", "Vera Chung"], "title": "DRWKV: Focusing on Object Edges for Low-Light Image Enhancement", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Low-light image enhancement remains a challenging task, particularly in preserving object edge continuity and fine structural details under extreme illumination degradation. In this paper, we propose a novel model, DRWKV (Detailed Receptance Weighted Key Value), which integrates our proposed Global Edge Retinex (GER) theory, enabling effective decoupling of illumination and edge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV Attention, a spiral-scanning mechanism that captures spatial edge continuity and models irregular structures more effectively. Thirdly, we design the Bilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align luminance and chrominance features, improving visual naturalness and mitigating artifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV achieves leading performance in PSNR, SSIM, and NIQE while maintaining low computational complexity. Furthermore, DRWKV enhances downstream performance in low-light multi-object tracking tasks, validating its generalization capabilities.", "AI": {"tldr": "DRWKV\u6a21\u578b\u901a\u8fc7GER\u7406\u8bba\u3001Evolving WKV Attention\u548cBi-SAB\u673a\u5236\uff0c\u63d0\u5347\u4e86\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u7684\u8fb9\u7f18\u4fdd\u771f\u5ea6\u548c\u89c6\u89c9\u6548\u679c\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4e2d\u8fb9\u7f18\u8fde\u7eed\u6027\u548c\u7ec6\u8282\u4fdd\u7559\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faDRWKV\u6a21\u578b\uff0c\u7ed3\u5408GER\u7406\u8bba\u3001Evolving WKV Attention\u548cBi-SAB\u673a\u5236\uff0c\u4f18\u5316\u8fb9\u7f18\u548c\u7ed3\u6784\u5206\u79bb\u3002", "result": "\u5728PSNR\u3001SSIM\u548cNIQE\u6307\u6807\u4e0a\u8868\u73b0\u9886\u5148\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u4f4e\uff0c\u4e14\u80fd\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "DRWKV\u5728\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.18632", "pdf": "https://arxiv.org/pdf/2507.18632", "abs": "https://arxiv.org/abs/2507.18632", "authors": ["Ye-Chan Kim", "SeungJu Cha", "Si-Woo Kim", "Taewhan Kim", "Dong-Jin Kim"], "title": "SIDA: Synthetic Image Driven Zero-shot Domain Adaptation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": "Accepted to ACM MM 2025", "summary": "Zero-shot domain adaptation is a method for adapting a model to a target domain without utilizing target domain image data. To enable adaptation without target images, existing studies utilize CLIP's embedding space and text description to simulate target-like style features. Despite the previous achievements in zero-shot domain adaptation, we observe that these text-driven methods struggle to capture complex real-world variations and significantly increase adaptation time due to their alignment process. Instead of relying on text descriptions, we explore solutions leveraging image data, which provides diverse and more fine-grained style cues. In this work, we propose SIDA, a novel and efficient zero-shot domain adaptation method leveraging synthetic images. To generate synthetic images, we first create detailed, source-like images and apply image translation to reflect the style of the target domain. We then utilize the style features of these synthetic images as a proxy for the target domain. Based on these features, we introduce Domain Mix and Patch Style Transfer modules, which enable effective modeling of real-world variations. In particular, Domain Mix blends multiple styles to expand the intra-domain representations, and Patch Style Transfer assigns different styles to individual patches. We demonstrate the effectiveness of our method by showing state-of-the-art performance in diverse zero-shot adaptation scenarios, particularly in challenging domains. Moreover, our approach achieves high efficiency by significantly reducing the overall adaptation time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5408\u6210\u56fe\u50cf\u7684\u96f6\u6837\u672c\u57df\u9002\u5e94\u65b9\u6cd5SIDA\uff0c\u901a\u8fc7\u751f\u6210\u76ee\u6807\u57df\u98ce\u683c\u7684\u5408\u6210\u56fe\u50cf\u6765\u66ff\u4ee3\u6587\u672c\u63cf\u8ff0\uff0c\u63d0\u9ad8\u4e86\u9002\u5e94\u6548\u7387\u548c\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6587\u672c\u63cf\u8ff0\u7684\u96f6\u6837\u672c\u57df\u9002\u5e94\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u590d\u6742\u73b0\u5b9e\u53d8\u5316\u4e14\u8017\u65f6\u8f83\u957f\uff0c\u56e0\u6b64\u63a2\u7d22\u5229\u7528\u56fe\u50cf\u6570\u636e\u63d0\u4f9b\u66f4\u7ec6\u7c92\u5ea6\u7684\u98ce\u683c\u7ebf\u7d22\u3002", "method": "\u901a\u8fc7\u751f\u6210\u6e90\u57df\u98ce\u683c\u7684\u8be6\u7ec6\u56fe\u50cf\u5e76\u5e94\u7528\u56fe\u50cf\u7ffb\u8bd1\u751f\u6210\u76ee\u6807\u57df\u98ce\u683c\u7684\u5408\u6210\u56fe\u50cf\uff0c\u5229\u7528\u5176\u98ce\u683c\u7279\u5f81\u4f5c\u4e3a\u76ee\u6807\u57df\u4ee3\u7406\uff0c\u5e76\u5f15\u5165Domain Mix\u548cPatch Style Transfer\u6a21\u5757\u3002", "result": "\u5728\u591a\u79cd\u96f6\u6837\u672c\u9002\u5e94\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u6311\u6218\u6027\u9886\u57df\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u9002\u5e94\u65f6\u95f4\u3002", "conclusion": "SIDA\u65b9\u6cd5\u901a\u8fc7\u5408\u6210\u56fe\u50cf\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u9a71\u52a8\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u96f6\u6837\u672c\u57df\u9002\u5e94\u3002"}}
{"id": "2507.18634", "pdf": "https://arxiv.org/pdf/2507.18634", "abs": "https://arxiv.org/abs/2507.18634", "authors": ["Junfei Xiao", "Ceyuan Yang", "Lvmin Zhang", "Shengqu Cai", "Yang Zhao", "Yuwei Guo", "Gordon Wetzstein", "Maneesh Agrawala", "Alan Yuille", "Lu Jiang"], "title": "Captain Cinema: Towards Short Movie Generation", "categories": ["cs.CV"], "comment": "Under review. Project page: https://thecinema.ai", "summary": "We present Captain Cinema, a generation framework for short movie generation. Given a detailed textual description of a movie storyline, our approach firstly generates a sequence of keyframes that outline the entire narrative, which ensures long-range coherence in both the storyline and visual appearance (e.g., scenes and characters). We refer to this step as top-down keyframe planning. These keyframes then serve as conditioning signals for a video synthesis model, which supports long context learning, to produce the spatio-temporal dynamics between them. This step is referred to as bottom-up video synthesis. To support stable and efficient generation of multi-scene long narrative cinematic works, we introduce an interleaved training strategy for Multimodal Diffusion Transformers (MM-DiT), specifically adapted for long-context video data. Our model is trained on a specially curated cinematic dataset consisting of interleaved data pairs. Our experiments demonstrate that Captain Cinema performs favorably in the automated creation of visually coherent and narrative consistent short movies in high quality and efficiency. Project page: https://thecinema.ai", "AI": {"tldr": "Captain Cinema\u662f\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u77ed\u7535\u5f71\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u81ea\u4e0a\u800c\u4e0b\u7684\u5173\u952e\u5e27\u89c4\u5212\u548c\u81ea\u4e0b\u800c\u4e0a\u7684\u89c6\u9891\u5408\u6210\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u9ad8\u6548\u7387\u7684\u81ea\u52a8\u5316\u7535\u5f71\u521b\u4f5c\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u5316\u751f\u6210\u89c6\u89c9\u8fde\u8d2f\u4e14\u53d9\u4e8b\u4e00\u81f4\u7684\u77ed\u7535\u5f71\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\uff08MM-DiT\uff09\u548c\u4ea4\u9519\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u5173\u952e\u5e27\u89c4\u5212\u548c\u89c6\u9891\u5408\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCaptain Cinema\u80fd\u591f\u9ad8\u6548\u751f\u6210\u89c6\u89c9\u8fde\u8d2f\u4e14\u53d9\u4e8b\u4e00\u81f4\u7684\u77ed\u7535\u5f71\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u52a8\u5316\u7535\u5f71\u521b\u4f5c\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u548c\u9ad8\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17800", "pdf": "https://arxiv.org/pdf/2507.17800", "abs": "https://arxiv.org/abs/2507.17800", "authors": ["Christian K. Belardi", "Chia-Hao Lee", "Yingheng Wang", "Justin Lovelace", "Kilian Q. Weinberger", "David A. Muller", "Carla P. Gomes"], "title": "Improving Multislice Electron Ptychography with a Generative Prior", "categories": ["eess.IV", "cond-mat.mtrl-sci", "cs.CV", "physics.optics"], "comment": "16 pages, 10 figures, 5 tables", "summary": "Multislice electron ptychography (MEP) is an inverse imaging technique that computationally reconstructs the highest-resolution images of atomic crystal structures from diffraction patterns. Available algorithms often solve this inverse problem iteratively but are both time consuming and produce suboptimal solutions due to their ill-posed nature. We develop MEP-Diffusion, a diffusion model trained on a large database of crystal structures specifically for MEP to augment existing iterative solvers. MEP-Diffusion is easily integrated as a generative prior into existing reconstruction methods via Diffusion Posterior Sampling (DPS). We find that this hybrid approach greatly enhances the quality of the reconstructed 3D volumes, achieving a 90.50% improvement in SSIM over existing methods.", "AI": {"tldr": "MEP-Diffusion\u7ed3\u5408\u6269\u6563\u6a21\u578b\u4e0e\u8fed\u4ee3\u6c42\u89e3\u5668\uff0c\u663e\u8457\u63d0\u5347\u591a\u5207\u7247\u7535\u5b50\u884d\u5c04\u6210\u50cf\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u8fed\u4ee3\u7b97\u6cd5\u8017\u65f6\u957f\u4e14\u89e3\u4e0d\u7406\u60f3\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u5f00\u53d1MEP-Diffusion\u6269\u6563\u6a21\u578b\uff0c\u4f5c\u4e3a\u751f\u6210\u5148\u9a8c\u96c6\u6210\u5230\u73b0\u6709\u65b9\u6cd5\u4e2d\u3002", "result": "SSIM\u63d0\u534790.50%\uff0c\u91cd\u5efa3D\u4f53\u79ef\u8d28\u91cf\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u6df7\u5408\u65b9\u6cd5\u6709\u6548\u4f18\u5316\u4e86\u591a\u5207\u7247\u7535\u5b50\u884d\u5c04\u6210\u50cf\u7684\u91cd\u5efa\u6548\u679c\u3002"}}
{"id": "2507.17911", "pdf": "https://arxiv.org/pdf/2507.17911", "abs": "https://arxiv.org/abs/2507.17911", "authors": ["Dou Hoon Kwark", "Shirui Luo", "Xiyue Zhu", "Yudu Li", "Zhi-Pei Liang", "Volodymyr Kindratenko"], "title": "Hierarchical Diffusion Framework for Pseudo-Healthy Brain MRI Inpainting with Enhanced 3D Consistency", "categories": ["eess.IV", "cs.CV"], "comment": "11 pages, 2 figures", "summary": "Pseudo-healthy image inpainting is an essential preprocessing step for analyzing pathological brain MRI scans. Most current inpainting methods favor slice-wise 2D models for their high in-plane fidelity, but their independence across slices produces discontinuities in the volume. Fully 3D models alleviate this issue, but their high model capacity demands extensive training data for reliable, high-fidelity synthesis -- often impractical in medical settings. We address these limitations with a hierarchical diffusion framework by replacing direct 3D modeling with two perpendicular coarse-to-fine 2D stages. An axial diffusion model first yields a coarse, globally consistent inpainting; a coronal diffusion model then refines anatomical details. By combining perpendicular spatial views with adaptive resampling, our method balances data efficiency and volumetric consistency. Our experiments show our approach outperforms state-of-the-art baselines in both realism and volumetric consistency, making it a promising solution for pseudo-healthy image inpainting. Code is available at https://github.com/dou0000/3dMRI-Consistent-Inpaint.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u4e2a\u5782\u76f4\u76842D\u9636\u6bb5\uff08\u8f74\u5411\u548c\u51a0\u72b6\uff09\u5b9e\u73b0\u4f2a\u5065\u5eb7MRI\u56fe\u50cf\u4fee\u590d\uff0c\u5e73\u8861\u6570\u636e\u6548\u7387\u548c\u4f53\u79ef\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e2d\uff0c2D\u6a21\u578b\u5207\u7247\u95f4\u4e0d\u8fde\u7eed\uff0c3D\u6a21\u578b\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8f74\u5411\u6269\u6563\u6a21\u578b\u751f\u6210\u5168\u5c40\u4e00\u81f4\u7684\u7c97\u4fee\u590d\uff0c\u518d\u7528\u51a0\u72b6\u6269\u6563\u6a21\u578b\u7ec6\u5316\u89e3\u5256\u7ec6\u8282\u3002\u7ed3\u5408\u5782\u76f4\u7a7a\u95f4\u89c6\u56fe\u548c\u81ea\u9002\u5e94\u91cd\u91c7\u6837\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u6027\u548c\u4f53\u79ef\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4f2a\u5065\u5eb7\u56fe\u50cf\u4fee\u590d\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u4e00\u81f4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.18012", "pdf": "https://arxiv.org/pdf/2507.18012", "abs": "https://arxiv.org/abs/2507.18012", "authors": ["Hang Xu", "Alexandre Bousse", "Alessandro Perelli"], "title": "Direct Dual-Energy CT Material Decomposition using Model-based Denoising Diffusion Model", "categories": ["eess.IV", "cs.CV", "physics.med-ph", "92C55, 94A08", "I.4.5; J.3"], "comment": "13 pages, 10 figures, 2 tables", "summary": "Dual-energy X-ray Computed Tomography (DECT) constitutes an advanced technology which enables automatic decomposition of materials in clinical images without manual segmentation using the dependency of the X-ray linear attenuation with energy. However, most methods perform material decomposition in the image domain as a post-processing step after reconstruction but this procedure does not account for the beam-hardening effect and it results in sub-optimal results. In this work, we propose a deep learning procedure called Dual-Energy Decomposition Model-based Diffusion (DEcomp-MoD) for quantitative material decomposition which directly converts the DECT projection data into material images. The algorithm is based on incorporating the knowledge of the spectral DECT model into the deep learning training loss and combining a score-based denoising diffusion learned prior in the material image domain. Importantly the inference optimization loss takes as inputs directly the sinogram and converts to material images through a model-based conditional diffusion model which guarantees consistency of the results. We evaluate the performance with both quantitative and qualitative estimation of the proposed DEcomp-MoD method on synthetic DECT sinograms from the low-dose AAPM dataset. Finally, we show that DEcomp-MoD outperform state-of-the-art unsupervised score-based model and supervised deep learning networks, with the potential to be deployed for clinical diagnosis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDEcomp-MoD\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u76f4\u63a5\u4eceDECT\u6295\u5f71\u6570\u636e\u751f\u6210\u6750\u6599\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u56fe\u50cf\u57df\u5206\u89e3\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edfDECT\u6750\u6599\u5206\u89e3\u65b9\u6cd5\u5728\u56fe\u50cf\u57df\u8fdb\u884c\u540e\u5904\u7406\uff0c\u672a\u8003\u8651\u5c04\u675f\u786c\u5316\u6548\u5e94\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u7406\u60f3\u3002", "method": "\u7ed3\u5408\u8c31DECT\u6a21\u578b\u77e5\u8bc6\u5230\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u635f\u5931\u4e2d\uff0c\u5e76\u5728\u6750\u6599\u56fe\u50cf\u57df\u5f15\u5165\u57fa\u4e8e\u8bc4\u5206\u7684\u53bb\u566a\u6269\u6563\u5148\u9a8c\u3002", "result": "\u5728\u4f4e\u5242\u91cfAAPM\u6570\u636e\u96c6\u4e0a\uff0cDEcomp-MoD\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DEcomp-MoD\u5177\u6709\u4e34\u5e8a\u8bca\u65ad\u6f5c\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u65e0\u76d1\u7763\u548c\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2507.18112", "pdf": "https://arxiv.org/pdf/2507.18112", "abs": "https://arxiv.org/abs/2507.18112", "authors": ["Binghua Li", "Ziqing Chang", "Tong Liang", "Chao Li", "Toshihisa Tanaka", "Shigeki Aoki", "Qibin Zhao", "Zhe Sun"], "title": "Parameter-Efficient Fine-Tuning of 3D DDPM for MRI Image Generation Using Tensor Networks", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "We address the challenge of parameter-efficient fine-tuning (PEFT) for three-dimensional (3D) U-Net-based denoising diffusion probabilistic models (DDPMs) in magnetic resonance imaging (MRI) image generation. Despite its practical significance, research on parameter-efficient representations of 3D convolution operations remains limited. To bridge this gap, we propose Tensor Volumetric Operator (TenVOO), a novel PEFT method specifically designed for fine-tuning DDPMs with 3D convolutional backbones. Leveraging tensor network modeling, TenVOO represents 3D convolution kernels with lower-dimensional tensors, effectively capturing complex spatial dependencies during fine-tuning with few parameters. We evaluate TenVOO on three downstream brain MRI datasets-ADNI, PPMI, and BraTS2021-by fine-tuning a DDPM pretrained on 59,830 T1-weighted brain MRI scans from the UK Biobank. Our results demonstrate that TenVOO achieves state-of-the-art performance in multi-scale structural similarity index measure (MS-SSIM), outperforming existing approaches in capturing spatial dependencies while requiring only 0.3% of the trainable parameters of the original model. Our code is available at: https://github.com/xiaovhua/tenvoo", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTenVOO\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u7528\u4e8e3D U-Net\u57fa\u7840\u7684DDPM\u5728MRI\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5e94\u7528\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u53c2\u6570\u9700\u6c42\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b33D\u5377\u79ef\u64cd\u4f5c\u53c2\u6570\u9ad8\u6548\u8868\u793a\u7684\u7814\u7a76\u4e0d\u8db3\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728MRI\u56fe\u50cf\u751f\u6210\u4e2d\u3002", "method": "\u5229\u7528\u5f20\u91cf\u7f51\u7edc\u5efa\u6a21\uff0c\u5c063D\u5377\u79ef\u6838\u8868\u793a\u4e3a\u4f4e\u7ef4\u5f20\u91cf\uff0c\u51cf\u5c11\u53c2\u6570\u9700\u6c42\u3002", "result": "\u5728\u4e09\u4e2a\u8111MRI\u6570\u636e\u96c6\u4e0a\uff0cTenVOO\u4ec5\u9700\u539f\u59cb\u6a21\u578b0.3%\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u5373\u8fbe\u5230\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "TenVOO\u662f\u4e00\u79cd\u9ad8\u6548\u7684PEFT\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e3D\u5377\u79ef\u6a21\u578b\u7684\u5fae\u8c03\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.18126", "pdf": "https://arxiv.org/pdf/2507.18126", "abs": "https://arxiv.org/abs/2507.18126", "authors": ["Juexin Zhang", "Ying Weng", "Ke Chen"], "title": "U-Net Based Healthy 3D Brain Tissue Inpainting", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Accepted by the International Brain Tumor Segmentation (BraTS)   challenge organized at MICCAI 2024 conference. Included 7 pages, 2 figures", "summary": "This paper introduces a novel approach to synthesize healthy 3D brain tissue from masked input images, specifically focusing on the task of 'ASNR-MICCAI BraTS Local Synthesis of Tissue via Inpainting'. Our proposed method employs a U-Net-based architecture, which is designed to effectively reconstruct the missing or corrupted regions of brain MRI scans. To enhance our model's generalization capabilities and robustness, we implement a comprehensive data augmentation strategy that involves randomly masking healthy images during training. Our model is trained on the BraTS-Local-Inpainting dataset and demonstrates the exceptional performance in recovering healthy brain tissue. The evaluation metrics employed, including Structural Similarity Index (SSIM), Peak Signal-to-Noise Ratio (PSNR), and Mean Squared Error (MSE), consistently yields impressive results. On the BraTS-Local-Inpainting validation set, our model achieved an SSIM score of 0.841, a PSNR score of 23.257, and an MSE score of 0.007. Notably, these evaluation metrics exhibit relatively low standard deviations, i.e., 0.103 for SSIM score, 4.213 for PSNR score and 0.007 for MSE score, which indicates that our model's reliability and consistency across various input scenarios. Our method also secured first place in the challenge.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eU-Net\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u63a9\u7801\u8f93\u5165\u56fe\u50cf\u5408\u6210\u5065\u5eb7\u76843D\u8111\u7ec4\u7ec7\uff0c\u5e76\u5728ASNR-MICCAI BraTS\u6311\u6218\u8d5b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u8111MRI\u626b\u63cf\u4e2d\u7f3a\u5931\u6216\u635f\u574f\u533a\u57df\u7684\u91cd\u5efa\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528U-Net\u67b6\u6784\uff0c\u7ed3\u5408\u968f\u673a\u63a9\u7801\u5065\u5eb7\u56fe\u50cf\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u8bad\u7ec3\u4e8eBraTS-Local-Inpainting\u6570\u636e\u96c6\u3002", "result": "\u5728\u9a8c\u8bc1\u96c6\u4e0a\u53d6\u5f97SSIM 0.841\u3001PSNR 23.257\u3001MSE 0.007\u7684\u4f18\u5f02\u8868\u73b0\uff0c\u4e14\u6807\u51c6\u5dee\u8f83\u4f4e\uff0c\u6a21\u578b\u53ef\u9760\u6027\u548c\u4e00\u81f4\u6027\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6311\u6218\u8d5b\u4e2d\u593a\u51a0\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u8111\u7ec4\u7ec7\u5408\u6210\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2507.18362", "pdf": "https://arxiv.org/pdf/2507.18362", "abs": "https://arxiv.org/abs/2507.18362", "authors": ["Yilong Hu", "Shijie Chang", "Lihe Zhang", "Feng Tian", "Weibing Sun", "Huchuan Lu"], "title": "UniSegDiff: Boosting Unified Lesion Segmentation via a Staged Diffusion Model", "categories": ["eess.IV", "cs.CV"], "comment": "MICCAI2025", "summary": "The Diffusion Probabilistic Model (DPM) has demonstrated remarkable performance across a variety of generative tasks. The inherent randomness in diffusion models helps address issues such as blurring at the edges of medical images and labels, positioning Diffusion Probabilistic Models (DPMs) as a promising approach for lesion segmentation. However, we find that the current training and inference strategies of diffusion models result in an uneven distribution of attention across different timesteps, leading to longer training times and suboptimal solutions. To this end, we propose UniSegDiff, a novel diffusion model framework designed to address lesion segmentation in a unified manner across multiple modalities and organs. This framework introduces a staged training and inference approach, dynamically adjusting the prediction targets at different stages, forcing the model to maintain high attention across all timesteps, and achieves unified lesion segmentation through pre-training the feature extraction network for segmentation. We evaluate performance on six different organs across various imaging modalities. Comprehensive experimental results demonstrate that UniSegDiff significantly outperforms previous state-of-the-art (SOTA) approaches. The code is available at https://github.com/HUYILONG-Z/UniSegDiff.", "AI": {"tldr": "UniSegDiff\u662f\u4e00\u79cd\u65b0\u578b\u6269\u6563\u6982\u7387\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u8bad\u7ec3\u548c\u63a8\u7406\u52a8\u6001\u8c03\u6574\u76ee\u6807\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6269\u6563\u6a21\u578b\u5728\u75c5\u7076\u5206\u5272\u4e2d\u6ce8\u610f\u529b\u5206\u5e03\u4e0d\u5747\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u6a21\u6001\u548c\u591a\u5668\u5b98\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6269\u6563\u6982\u7387\u6a21\u578b\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u75c5\u7076\u5206\u5272\u4e2d\u5b58\u5728\u6ce8\u610f\u529b\u5206\u5e03\u4e0d\u5747\u3001\u8bad\u7ec3\u65f6\u95f4\u957f\u548c\u6548\u679c\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faUniSegDiff\u6846\u67b6\uff0c\u91c7\u7528\u5206\u9636\u6bb5\u8bad\u7ec3\u548c\u63a8\u7406\u7b56\u7565\uff0c\u52a8\u6001\u8c03\u6574\u9884\u6d4b\u76ee\u6807\uff0c\u5e76\u901a\u8fc7\u9884\u8bad\u7ec3\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\u5b9e\u73b0\u7edf\u4e00\u7684\u75c5\u7076\u5206\u5272\u3002", "result": "\u5728\u516d\u79cd\u4e0d\u540c\u5668\u5b98\u548c\u591a\u79cd\u6210\u50cf\u6a21\u6001\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUniSegDiff\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "UniSegDiff\u901a\u8fc7\u6539\u8fdb\u8bad\u7ec3\u548c\u63a8\u7406\u7b56\u7565\uff0c\u5728\u591a\u6a21\u6001\u548c\u591a\u5668\u5b98\u75c5\u7076\u5206\u5272\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u4f18\u52bf\u3002"}}
