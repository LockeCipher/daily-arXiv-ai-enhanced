<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 14]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Sparse Transformer for Ultra-sparse Sampled Video Compressive Sensing](https://arxiv.org/abs/2509.08228)
*Miao Cao,Siming Zheng,Lishun Wang,Ziyang Chen,David Brady,Xin Yuan*

Main category: cs.CV

TL;DR: 本文提出超稀疏采样(USS)策略，通过在每个空间位置仅设置一个子帧为1的方式，大幅降低视频摄像的电力消耗，并设计BSTFormer模型来处理采样数据。


<details>
  <summary>Details</summary>
Motivation: 传统数码相机在高分辨率高帧率摄像时电力消耗过高，无法支持未来的吉像素摄像需求，需要开发更节能的采样技术。

Method: 提出USS稀疏采样策略，并建立DMD编码系统验证。设计BSTFormer模型，结合局部坐标注意力、全局稀疏注意力和全局时间注意力来处理USS采样数据。

Result: 在模拟和实际数据上，方法表现显著超过以往最优算法。USS策略还具有比随机采样更高的动态范围和固定曝光时间优势。

Conclusion: USS策略通过极稀疏采样实现了高效能的视频采集，为未来高速高分辨率摄像系统提供了可行的解决方案，特别适合隧道式芯片集成。

Abstract: Digital cameras consume ~0.1 microjoule per pixel to capture and encode video, resulting in a power usage of ~20W for a 4K sensor operating at 30 fps. Imagining gigapixel cameras operating at 100-1000 fps, the current processing model is unsustainable. To address this, physical layer compressive measurement has been proposed to reduce power consumption per pixel by 10-100X. Video Snapshot Compressive Imaging (SCI) introduces high frequency modulation in the optical sensor layer to increase effective frame rate. A commonly used sampling strategy of video SCI is Random Sampling (RS) where each mask element value is randomly set to be 0 or 1. Similarly, image inpainting (I2P) has demonstrated that images can be recovered from a fraction of the image pixels. Inspired by I2P, we propose Ultra-Sparse Sampling (USS) regime, where at each spatial location, only one sub-frame is set to 1 and all others are set to 0. We then build a Digital Micro-mirror Device (DMD) encoding system to verify the effectiveness of our USS strategy. Ideally, we can decompose the USS measurement into sub-measurements for which we can utilize I2P algorithms to recover high-speed frames. However, due to the mismatch between the DMD and CCD, the USS measurement cannot be perfectly decomposed. To this end, we propose BSTFormer, a sparse TransFormer that utilizes local Block attention, global Sparse attention, and global Temporal attention to exploit the sparsity of the USS measurement. Extensive results on both simulated and real-world data show that our method significantly outperforms all previous state-of-the-art algorithms. Additionally, an essential advantage of the USS strategy is its higher dynamic range than that of the RS strategy. Finally, from the application perspective, the USS strategy is a good choice to implement a complete video SCI system on chip due to its fixed exposure time.

</details>


### [2] [EVDI++: Event-based Video Deblurring and Interpolation via Self-Supervised Learning](https://arxiv.org/abs/2509.08260)
*Chi Zhang,Xiang Zhang,Chenxu Jiang,Gui-Song Xia,Lei Yu*

Main category: cs.CV

TL;DR: EVDI++是一个基于事件相机的自监督视频去模糊和插值框架，利用事件相机的高时间分辨率来解决传统相机长曝光导致的运动模糊问题，并在合成和真实数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统帧式相机在长曝光时会产生明显的视觉模糊和帧间信息丢失，严重影响视频质量。事件相机的高时间分辨率特性为解决这一问题提供了新的可能性。

Method: 提出Learnable Double Integral (LDI)网络估计参考帧与清晰潜在图像的映射关系；引入基于学习的除法重建模块优化结果和训练效率；设计自适应无参数融合策略利用LDI输出的置信度；构建自监督学习框架利用模糊帧、潜在图像和事件流之间的相互约束。

Result: 在合成和真实数据集上的大量实验表明，该方法在视频去模糊和插值任务中达到了最先进的性能，并在使用DAVIS346c相机构建的真实数据集上展示了良好的泛化能力。

Conclusion: EVDI++成功地将事件相机的高时间分辨率优势应用于视频去模糊和插值任务，通过统一的自我监督框架有效解决了传统相机长曝光导致的运动模糊问题，为实际应用提供了可行的解决方案。

Abstract: Frame-based cameras with extended exposure times often produce perceptible visual blurring and information loss between frames, significantly degrading video quality. To address this challenge, we introduce EVDI++, a unified self-supervised framework for Event-based Video Deblurring and Interpolation that leverages the high temporal resolution of event cameras to mitigate motion blur and enable intermediate frame prediction. Specifically, the Learnable Double Integral (LDI) network is designed to estimate the mapping relation between reference frames and sharp latent images. Then, we refine the coarse results and optimize overall training efficiency by introducing a learning-based division reconstruction module, enabling images to be converted with varying exposure intervals. We devise an adaptive parameter-free fusion strategy to obtain the final results, utilizing the confidence embedded in the LDI outputs of concurrent events. A self-supervised learning framework is proposed to enable network training with real-world blurry videos and events by exploring the mutual constraints among blurry frames, latent images, and event streams. We further construct a dataset with real-world blurry images and events using a DAVIS346c camera, demonstrating the generalizability of the proposed EVDI++ in real-world scenarios. Extensive experiments on both synthetic and real-world datasets show that our method achieves state-of-the-art performance in video deblurring and interpolation tasks.

</details>


### [3] [Bitrate-Controlled Diffusion for Disentangling Motion and Content in Video](https://arxiv.org/abs/2509.08376)
*Xiao Li,Qi Chen,Xiulian Peng,Kai Yu,Xie Chen,Yan Lu*

Main category: cs.CV

TL;DR: 通过变据器架构和低码率向量量化，提出了一种自监督的视频动态运动与静态内容解耦表征学习框架


<details>
  <summary>Details</summary>
Motivation: 解决视频数据中动态运动与静态内容的混合问题，提供一种偏轻偏差更少的自监督解耦方法

Method: 使用变据器架构联合生成帧间运动和剧情内容隐式特征，通过低码率向量量化作为信息瓶颈促进解耦，并用于降噪扩散模型的条件输入

Result: 在说话头像视频的运动转移和自回归运动生成任务上验证有效性，同时能够泛化到2D卡通角色等其他类型视频数据

Conclusion: 为自监督解耦视频表征学习提供了新视角，对视频分析和生成领域做出了贡献

Abstract: We propose a novel and general framework to disentangle video data into its dynamic motion and static content components. Our proposed method is a self-supervised pipeline with less assumptions and inductive biases than previous works: it utilizes a transformer-based architecture to jointly generate flexible implicit features for frame-wise motion and clip-wise content, and incorporates a low-bitrate vector quantization as an information bottleneck to promote disentanglement and form a meaningful discrete motion space. The bitrate-controlled latent motion and content are used as conditional inputs to a denoising diffusion model to facilitate self-supervised representation learning. We validate our disentangled representation learning framework on real-world talking head videos with motion transfer and auto-regressive motion generation tasks. Furthermore, we also show that our method can generalize to other types of video data, such as pixel sprites of 2D cartoon characters. Our work presents a new perspective on self-supervised learning of disentangled video representations, contributing to the broader field of video analysis and generation.

</details>


### [4] [VRAE: Vertical Residual Autoencoder for License Plate Denoising and Deblurring](https://arxiv.org/abs/2509.08392)
*Cuong Nguyen,Dung T. Tran,Hong Nguyen,Xuan-Vu Phan,Nam-Phong Nguyen*

Main category: cs.CV

TL;DR: 提出垂直残差自编码器(VRAE)用于交通监控图像增强，通过辅助块注入输入感知特征来指导表示学习，在保持参数少量增加的同时显著提升图像质量指标


<details>
  <summary>Details</summary>
Motivation: 现实交通监控中，恶劣天气、光照不足或高速运动导致车辆图像严重噪声和模糊，特别是车牌区域较小的情况会显著降低识别准确率，需要实时图像增强作为预处理步骤

Method: 垂直残差自编码器架构，在每个编码阶段使用辅助块注入输入感知特征来指导表示学习过程，比传统自编码器能更好地保持通用信息

Result: 在车牌可见的车辆图像数据集上，方法 consistently 优于自编码器、生成对抗网络和基于流的方法。与同深度自编码器相比，PSNR提升约20%，NMSE降低约50%，SSIM提升1%，参数仅增加约1%

Conclusion: VRAE架构通过输入感知特征注入机制有效提升了交通监控图像增强性能，在保持计算效率的同时显著改善了图像质量指标

Abstract: In real-world traffic surveillance, vehicle images captured under adverse weather, poor lighting, or high-speed motion often suffer from severe noise and blur. Such degradations significantly reduce the accuracy of license plate recognition systems, especially when the plate occupies only a small region within the full vehicle image. Restoring these degraded images a fast realtime manner is thus a crucial pre-processing step to enhance recognition performance. In this work, we propose a Vertical Residual Autoencoder (VRAE) architecture designed for the image enhancement task in traffic surveillance. The method incorporates an enhancement strategy that employs an auxiliary block, which injects input-aware features at each encoding stage to guide the representation learning process, enabling better general information preservation throughout the network compared to conventional autoencoders. Experiments on a vehicle image dataset with visible license plates demonstrate that our method consistently outperforms Autoencoder (AE), Generative Adversarial Network (GAN), and Flow-Based (FB) approaches. Compared with AE at the same depth, it improves PSNR by about 20\%, reduces NMSE by around 50\%, and enhances SSIM by 1\%, while requiring only a marginal increase of roughly 1\% in parameters.

</details>


### [5] [LD-ViCE: Latent Diffusion Model for Video Counterfactual Explanations](https://arxiv.org/abs/2509.08422)
*Payal Varshney,Adriano Lucieri,Christoph Balada,Sheraz Ahmed,Andreas Dengel*

Main category: cs.CV

TL;DR: LD-ViCE是一个基于潜在扩散模型的视频反事实解释框架，通过潜在空间操作降低计算成本，生成现实且可解释的反事实视频解释，在多个视频数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 视频AI系统在安全关键领域应用广泛，但现有解释技术存在时间连贯性不足、鲁棒性差、缺乏因果洞察等问题，且当前反事实解释方法通常未结合目标模型指导，降低了语义保真度和实用价值。

Method: 使用最先进的扩散模型在潜在空间中操作，通过额外的精炼步骤生成现实且可解释的反事实解释，显著降低了生成解释的计算成本。

Result: 在三个不同视频数据集（EchoNet-Dynamic、FERV39k、Something-Something V2）上的实验显示，LD-ViCE优于现有最先进方法，R2分数提升高达68%，同时推理时间减少一半。定性分析确认生成的解释具有语义意义和时间连贯性。

Conclusion: LD-ViCE代表了在安全关键领域实现可信AI部署的重要进展，能够为视频AI模型行为提供有价值的洞察。

Abstract: Video-based AI systems are increasingly adopted in safety-critical domains such as autonomous driving and healthcare. However, interpreting their decisions remains challenging due to the inherent spatiotemporal complexity of video data and the opacity of deep learning models. Existing explanation techniques often suffer from limited temporal coherence, insufficient robustness, and a lack of actionable causal insights. Current counterfactual explanation methods typically do not incorporate guidance from the target model, reducing semantic fidelity and practical utility. We introduce Latent Diffusion for Video Counterfactual Explanations (LD-ViCE), a novel framework designed to explain the behavior of video-based AI models. Compared to previous approaches, LD-ViCE reduces the computational costs of generating explanations by operating in latent space using a state-of-the-art diffusion model, while producing realistic and interpretable counterfactuals through an additional refinement step. Our experiments demonstrate the effectiveness of LD-ViCE across three diverse video datasets, including EchoNet-Dynamic (cardiac ultrasound), FERV39k (facial expression), and Something-Something V2 (action recognition). LD-ViCE outperforms a recent state-of-the-art method, achieving an increase in R2 score of up to 68% while reducing inference time by half. Qualitative analysis confirms that LD-ViCE generates semantically meaningful and temporally coherent explanations, offering valuable insights into the target model behavior. LD-ViCE represents a valuable step toward the trustworthy deployment of AI in safety-critical domains.

</details>


### [6] [Spherical Brownian Bridge Diffusion Models for Conditional Cortical Thickness Forecasting](https://arxiv.org/abs/2509.08442)
*Ivan Stoyanov,Fabian Bongratz,Christian Wachinger*

Main category: cs.CV

TL;DR: 提出SBDM模型，通过球形布朗桥扩散过程预测个体化皮层厚度轨迹，在ADNI和OASIS数据集上显著降低预测误差


<details>
  <summary>Details</summary>
Motivation: 准确预测个体化高分辨率皮层厚度轨迹对检测神经退行性病变至关重要，但面临皮层复杂几何结构和多模态数据整合的挑战

Method: 使用双向条件布朗桥扩散过程，结合条件球形U-Net（CoS-UNet）进行去噪，整合球形卷积和密集交叉注意力机制

Result: 相比现有方法显著降低预测误差，能够生成个体事实和反事实皮层厚度轨迹

Conclusion: SBDM为探索皮层发育假设场景提供了新框架，在神经影像分析中具有重要应用价值

Abstract: Accurate forecasting of individualized, high-resolution cortical thickness (CTh) trajectories is essential for detecting subtle cortical changes, providing invaluable insights into neurodegenerative processes and facilitating earlier and more precise intervention strategies. However, CTh forecasting is a challenging task due to the intricate non-Euclidean geometry of the cerebral cortex and the need to integrate multi-modal data for subject-specific predictions. To address these challenges, we introduce the Spherical Brownian Bridge Diffusion Model (SBDM). Specifically, we propose a bidirectional conditional Brownian bridge diffusion process to forecast CTh trajectories at the vertex level of registered cortical surfaces. Our technical contribution includes a new denoising model, the conditional spherical U-Net (CoS-UNet), which combines spherical convolutions and dense cross-attention to integrate cortical surfaces and tabular conditions seamlessly. Compared to previous approaches, SBDM achieves significantly reduced prediction errors, as demonstrated by our experiments based on longitudinal datasets from the ADNI and OASIS. Additionally, we demonstrate SBDM's ability to generate individual factual and counterfactual CTh trajectories, offering a novel framework for exploring hypothetical scenarios of cortical development.

</details>


### [7] [First-order State Space Model for Lightweight Image Super-resolution](https://arxiv.org/abs/2509.08458)
*Yujie Zhu,Xinyi Zhang,Yekai Lu,Guang Yang,Faming Fang,Guixu Zhang*

Main category: cs.CV

TL;DR: 提出了First-order State Space Model (FSSM)来改进Mamba模块，通过一阶保持条件和新的离散化形式提升轻量级超分辨率任务的性能，在不增加参数的情况下超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的Mamba视觉模型主要关注网络架构和扫描路径，而很少关注SSM模块本身。为了探索SSM的潜力，需要改进SSM计算过程来提升轻量级超分辨率任务的性能。

Method: 引入一阶保持条件到SSM中，推导出新的离散化形式，并分析累积误差。提出了First-order State Space Model (FSSM)来改进原始Mamba模块，通过融入token相关性来增强性能。

Result: 在五个基准数据集上，FSSM在不增加额外参数的情况下提升了MambaIR的性能，超越了当前的轻量级超分辨率方法，达到了最先进的成果。

Conclusion: FSSM通过改进SSM模块的计算过程，有效提升了轻量级超分辨率任务的性能，证明了在保持参数效率的同时进一步提升模型性能的可行性。

Abstract: State space models (SSMs), particularly Mamba, have shown promise in NLP tasks and are increasingly applied to vision tasks. However, most Mamba-based vision models focus on network architecture and scan paths, with little attention to the SSM module. In order to explore the potential of SSMs, we modified the calculation process of SSM without increasing the number of parameters to improve the performance on lightweight super-resolution tasks. In this paper, we introduce the First-order State Space Model (FSSM) to improve the original Mamba module, enhancing performance by incorporating token correlations. We apply a first-order hold condition in SSMs, derive the new discretized form, and analyzed cumulative error. Extensive experimental results demonstrate that FSSM improves the performance of MambaIR on five benchmark datasets without additionally increasing the number of parameters, and surpasses current lightweight SR methods, achieving state-of-the-art results.

</details>


### [8] [Prompt-Driven Image Analysis with Multimodal Generative AI: Detection, Segmentation, Inpainting, and Interpretation](https://arxiv.org/abs/2509.08489)
*Kaleem Ahmad*

Main category: cs.CV

TL;DR: 基于单个自然语言提示的统一图像分析管道，结合开放词汇检测、提示分割、文本条件塞充和视觉语言描述功能


<details>
  <summary>Details</summary>
Motivation: 建立一个从单个提示到多步操作的统一工作流，提高图像分析的可靠性和可重现性

Method: 组合开放词汇检测、提示分割、文本条件塞充和视觉语言描述的统一管道，包括阈值调整、面具检查、资源认知默认值等集成选择

Result: 在单词提示分割中，检测和分割在90%以上情况下生成可用面具，准确率超15%；在高端GPU上，塞充占总运行时间的60-75%

Conclusion: 提供了一个透明、可靠的模式，通过明确的护栏和运营实践提高对象替换、场景增强和删除的可靠性

Abstract: Prompt-driven image analysis converts a single natural-language instruction into multiple steps: locate, segment, edit, and describe. We present a practical case study of a unified pipeline that combines open-vocabulary detection, promptable segmentation, text-conditioned inpainting, and vision-language description into a single workflow. The system works end to end from a single prompt, retains intermediate artifacts for transparent debugging (such as detections, masks, overlays, edited images, and before and after composites), and provides the same functionality through an interactive UI and a scriptable CLI for consistent, repeatable runs. We highlight integration choices that reduce brittleness, including threshold adjustments, mask inspection with light morphology, and resource-aware defaults. In a small, single-word prompt segment, detection and segmentation produced usable masks in over 90% of cases with an accuracy above 85% based on our criteria. On a high-end GPU, inpainting makes up 60 to 75% of total runtime under typical guidance and sampling settings, which highlights the need for careful tuning. The study offers implementation-guided advice on thresholds, mask tightness, and diffusion parameters, and details version pinning, artifact logging, and seed control to support replay. Our contribution is a transparent, reliable pattern for assembling modern vision and multimodal models behind a single prompt, with clear guardrails and operational practices that improve reliability in object replacement, scene augmentation, and removal.

</details>


### [9] [HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning](https://arxiv.org/abs/2509.08519)
*Liyang Chen,Tianxiang Ma,Jiawei Liu,Bingchuan Li,Zhuowei Chen,Lijie Liu,Xu He,Gen Li,Qian He,Zhiyong Wu*

Main category: cs.CV

TL;DR: HuMo是一个统一的人类中心视频生成框架，通过两阶段训练范式和任务特定策略，有效协调文本、图像和音频等多模态输入，解决了现有方法在主体保持和音视频同步方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的人类中心视频生成方法在处理异构多模态输入时面临两个主要挑战：配对三元组训练数据的稀缺性，以及主体保持和音视频同步子任务在多模态输入下的协作困难。

Method: 提出两阶段渐进式多模态训练范式：1) 构建高质量配对数据集；2) 采用最小侵入式图像注入策略保持主体；3) 提出焦点预测策略关联音频与面部区域；4) 设计时间自适应分类器自由引导策略进行细粒度控制。

Result: 实验结果表明，HuMo在各项子任务上超越了专门的先进方法，建立了统一的多模态条件人类中心视频生成框架。

Conclusion: HuMo通过创新的训练策略和架构设计，成功实现了多模态输入的协同控制，为人类中心视频生成提供了有效的统一解决方案。

Abstract: Human-Centric Video Generation (HCVG) methods seek to synthesize human videos from multimodal inputs, including text, image, and audio. Existing methods struggle to effectively coordinate these heterogeneous modalities due to two challenges: the scarcity of training data with paired triplet conditions and the difficulty of collaborating the sub-tasks of subject preservation and audio-visual sync with multimodal inputs. In this work, we present HuMo, a unified HCVG framework for collaborative multimodal control. For the first challenge, we construct a high-quality dataset with diverse and paired text, reference images, and audio. For the second challenge, we propose a two-stage progressive multimodal training paradigm with task-specific strategies. For the subject preservation task, to maintain the prompt following and visual generation abilities of the foundation model, we adopt the minimal-invasive image injection strategy. For the audio-visual sync task, besides the commonly adopted audio cross-attention layer, we propose a focus-by-predicting strategy that implicitly guides the model to associate audio with facial regions. For joint learning of controllabilities across multimodal inputs, building on previously acquired capabilities, we progressively incorporate the audio-visual sync task. During inference, for flexible and fine-grained multimodal control, we design a time-adaptive Classifier-Free Guidance strategy that dynamically adjusts guidance weights across denoising steps. Extensive experimental results demonstrate that HuMo surpasses specialized state-of-the-art methods in sub-tasks, establishing a unified framework for collaborative multimodal-conditioned HCVG. Project Page: https://phantom-video.github.io/HuMo.

</details>


### [10] [LADB: Latent Aligned Diffusion Bridges for Semi-Supervised Domain Translation](https://arxiv.org/abs/2509.08628)
*Xuqin Wang,Tao Wu,Yanfeng Zhang,Lu Liu,Dong Wang,Mingwei Sun,Yongliang Wang,Niclas Zeller,Daniel Cremers*

Main category: cs.CV

TL;DR: LADB是一个半监督的样本到样本转换框架，利用部分配对数据在共享潜在空间中对齐源域和目标域分布，无需完全监督即可实现确定性域映射。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在数据稀缺领域面临挑战，需要大量重新训练或昂贵的配对数据。LADB旨在解决这些限制，特别是在数据标注成本高或不完整的实际场景中。

Method: 通过在共享潜在空间中对齐源域和目标域分布，将预训练的源域扩散模型与目标域潜在对齐扩散模型（LADM）集成，利用部分配对的潜在表示进行训练。

Result: 实验结果显示在部分监督下的深度到图像转换任务中表现优异，并能扩展到多源转换（深度图和分割掩码）和多目标转换的类条件风格迁移任务。

Conclusion: LADB为实际域转换提供了一个可扩展且通用的解决方案，在保真度和多样性之间取得了良好平衡，特别适用于数据标注成本高或不完整的场景。

Abstract: Diffusion models excel at generating high-quality outputs but face challenges in data-scarce domains, where exhaustive retraining or costly paired data are often required. To address these limitations, we propose Latent Aligned Diffusion Bridges (LADB), a semi-supervised framework for sample-to-sample translation that effectively bridges domain gaps using partially paired data. By aligning source and target distributions within a shared latent space, LADB seamlessly integrates pretrained source-domain diffusion models with a target-domain Latent Aligned Diffusion Model (LADM), trained on partially paired latent representations. This approach enables deterministic domain mapping without the need for full supervision. Compared to unpaired methods, which often lack controllability, and fully paired approaches that require large, domain-specific datasets, LADB strikes a balance between fidelity and diversity by leveraging a mixture of paired and unpaired latent-target couplings. Our experimental results demonstrate superior performance in depth-to-image translation under partial supervision. Furthermore, we extend LADB to handle multi-source translation (from depth maps and segmentation masks) and multi-target translation in a class-conditioned style transfer task, showcasing its versatility in handling diverse and heterogeneous use cases. Ultimately, we present LADB as a scalable and versatile solution for real-world domain translation, particularly in scenarios where data annotation is costly or incomplete.

</details>


### [11] [Computational Imaging for Enhanced Computer Vision](https://arxiv.org/abs/2509.08712)
*Humera Shaikh,Kaur Jashanpreet*

Main category: cs.CV

TL;DR: 本文对计算成像技术及其在计算机视觉应用中的变革性影响进行了全面综述，重点分析了各种CI方法如何提升CV系统在挑战性环境下的性能


<details>
  <summary>Details</summary>
Motivation: 传统成像方法在低光照、运动模糊、高动态范围等挑战性条件下往往无法提供高质量视觉数据，限制了先进计算机视觉系统的性能表现

Method: 系统性地调研了光场成像、高动态范围成像、去模糊、高速成像和眩光抑制等多种计算成像技术，并分析它们与计算机视觉核心任务的协同关系

Result: 揭示了计算成像技术与目标检测、深度估计、光流、人脸识别和关键点检测等CV任务之间的关联性及其实际贡献

Conclusion: 强调了任务特异性自适应成像管道的潜力，能够提高自动驾驶、监控、增强现实和机器人等现实场景中的鲁棒性、准确性和效率，指出了未来的研究方向和挑战

Abstract: This paper presents a comprehensive survey of computational imaging (CI) techniques and their transformative impact on computer vision (CV) applications. Conventional imaging methods often fail to deliver high-fidelity visual data in challenging conditions, such as low light, motion blur, or high dynamic range scenes, thereby limiting the performance of state-of-the-art CV systems. Computational imaging techniques, including light field imaging, high dynamic range (HDR) imaging, deblurring, high-speed imaging, and glare mitigation, address these limitations by enhancing image acquisition and reconstruction processes. This survey systematically explores the synergies between CI techniques and core CV tasks, including object detection, depth estimation, optical flow, face recognition, and keypoint detection. By analyzing the relationships between CI methods and their practical contributions to CV applications, this work highlights emerging opportunities, challenges, and future research directions. We emphasize the potential for task-specific, adaptive imaging pipelines that improve robustness, accuracy, and efficiency in real-world scenarios, such as autonomous navigation, surveillance, augmented reality, and robotics.

</details>


### [12] [GeneVA: A Dataset of Human Annotations for Generative Text to Video Artifacts](https://arxiv.org/abs/2509.08818)
*Jenna Kang,Maria Silva,Patsorn Sangkloy,Kenneth Chen,Niall Williams,Qi Sun*

Main category: cs.CV

TL;DR: GeneVA是一个大规模人工标注的视频生成伪影数据集，专注于文本驱动视频生成中的时空伪影问题


<details>
  <summary>Details</summary>
Motivation: 现有的概率生成模型在文本驱动视频生成中存在随机性导致的物理不可能性和时间不一致性等伪影问题，但缺乏专门针对视频时空复杂性的系统基准数据集

Method: 构建大规模人工标注数据集GeneVA，包含自然文本提示生成的视频中的丰富时空伪影标注

Result: 成功创建了专注于视频生成时空伪影的大规模标注数据集

Conclusion: GeneVA数据集能够支持和辅助关键应用，如基准测试模型性能和提升生成视频质量

Abstract: Recent advances in probabilistic generative models have extended capabilities from static image synthesis to text-driven video generation. However, the inherent randomness of their generation process can lead to unpredictable artifacts, such as impossible physics and temporal inconsistency. Progress in addressing these challenges requires systematic benchmarks, yet existing datasets primarily focus on generative images due to the unique spatio-temporal complexities of videos. To bridge this gap, we introduce GeneVA, a large-scale artifact dataset with rich human annotations that focuses on spatio-temporal artifacts in videos generated from natural text prompts. We hope GeneVA can enable and assist critical applications, such as benchmarking model performance and improving generative video quality.

</details>


### [13] [RewardDance: Reward Scaling in Visual Generation](https://arxiv.org/abs/2509.08826)
*Jie Wu,Yu Gao,Zilyu Ye,Ming Li,Liang Li,Hanzhong Guo,Jie Liu,Zeyue Xue,Xiaoxia Hou,Wei Liu,Yan Zeng,Weilin Huang*

Main category: cs.CV

TL;DR: RewardDance是一个可扩展的奖励建模框架，通过生成式奖励范式解决视觉生成中奖励模型扩展的挑战，支持260亿参数规模，有效防止奖励破解问题。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型在视觉生成中存在架构限制：CLIP-based模型受输入模态约束，Bradley-Terry损失与视觉语言模型的下一个token预测机制不匹配，且RLHF优化过程存在奖励破解问题。

Method: 提出RewardDance框架，将奖励分数重新定义为模型预测"yes"token的概率，表示生成图像在特定标准下优于参考图像，使奖励目标与VLM架构内在对齐。

Result: 在文本到图像、文本到视频和图像到视频生成任务中显著超越最先进方法，大规模奖励模型在RL微调期间保持高奖励方差，证明其抗破解能力。

Conclusion: RewardDance解决了奖励模型扩展的根本限制，通过生成式奖励范式实现了模型规模和上下文规模的扩展，有效缓解了模式崩溃问题。

Abstract: Reward Models (RMs) are critical for improving generation models via Reinforcement Learning (RL), yet the RM scaling paradigm in visual generation remains largely unexplored. It primarily due to fundamental limitations in existing approaches: CLIP-based RMs suffer from architectural and input modality constraints, while prevalent Bradley-Terry losses are fundamentally misaligned with the next-token prediction mechanism of Vision-Language Models (VLMs), hindering effective scaling. More critically, the RLHF optimization process is plagued by Reward Hacking issue, where models exploit flaws in the reward signal without improving true quality. To address these challenges, we introduce RewardDance, a scalable reward modeling framework that overcomes these barriers through a novel generative reward paradigm. By reformulating the reward score as the model's probability of predicting a "yes" token, indicating that the generated image outperforms a reference image according to specific criteria, RewardDance intrinsically aligns reward objectives with VLM architectures. This alignment unlocks scaling across two dimensions: (1) Model Scaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context Scaling: Integration of task-specific instructions, reference examples, and chain-of-thought (CoT) reasoning. Extensive experiments demonstrate that RewardDance significantly surpasses state-of-the-art methods in text-to-image, text-to-video, and image-to-video generation. Crucially, we resolve the persistent challenge of "reward hacking": Our large-scale RMs exhibit and maintain high reward variance during RL fine-tuning, proving their resistance to hacking and ability to produce diverse, high-quality outputs. It greatly relieves the mode collapse problem that plagues smaller models.

</details>


### [14] [SAFT: Shape and Appearance of Fabrics from Template via Differentiable Physical Simulations from Monocular Video](https://arxiv.org/abs/2509.08828)
*David Stotko,Reinhard Klein*

Main category: cs.CV

TL;DR: 通过物理模拟和可微渲染技术，从单目RGB视频重建纺物的3D动态场景和外观特征，提出两个新的正则化项解决单目深度歧义问题。


<details>
  <summary>Details</summary>
Motivation: 解决从单目RGB视频重建3D动态纺物场景的挑战，特别是深度歧义问题，并同时完成几何重建和外观估计任务。

Method: 结合物理模拟和可微渲染技术，提出两个新的正则化项来改善单目视频的3D重建质量，解决深度歧义问题。

Result: 与最新方法相比，3D重建错误减少3.64倍，每个场景运行时间为30分钟，能够从单目RGB视频中恢复出清晰的细节。

Conclusion: 该方法能够从单目RGB视频高质量地重建纺物的3D动态场景和外观特征，显著提升了重建精度和效率。

Abstract: The reconstruction of three-dimensional dynamic scenes is a well-established yet challenging task within the domain of computer vision. In this paper, we propose a novel approach that combines the domains of 3D geometry reconstruction and appearance estimation for physically based rendering and present a system that is able to perform both tasks for fabrics, utilizing only a single monocular RGB video sequence as input. In order to obtain realistic and high-quality deformations and renderings, a physical simulation of the cloth geometry and differentiable rendering are employed. In this paper, we introduce two novel regularization terms for the 3D reconstruction task that improve the plausibility of the reconstruction by addressing the depth ambiguity problem in monocular video. In comparison with the most recent methods in the field, we have reduced the error in the 3D reconstruction by a factor of 2.64 while requiring a medium runtime of 30 min per scene. Furthermore, the optimized motion achieves sufficient quality to perform an appearance estimation of the deforming object, recovering sharp details from this single monocular RGB video.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [15] [Physics-Guided Rectified Flow for Low-light RAW Image Enhancement](https://arxiv.org/abs/2509.08330)
*Juntai Zeng*

Main category: eess.IV

TL;DR: 提出PGRF框架，结合物理噪声模型和整流流生成方法，通过像素级噪声校准提升低光照RAW图像增强效果


<details>
  <summary>Details</summary>
Motivation: 现有合成数据集方法主要考虑加性噪声，忽略乘性分量和像素级制造差异，难以准确模拟真实传感器噪声

Method: 从物理噪声机制推导噪声模型，提出结合加性乘性噪声的复合模型，采用基于物理的像素级噪声模拟校准方案，结合整流流生成框架

Result: 在自建的LLID数据集上实验证明，该框架在低光照RAW图像增强方面取得显著改进

Conclusion: PGRF框架通过物理引导的整流流方法有效提升了低光照图像增强性能，解决了传统全局校准的局限性

Abstract: Enhancing RAW images captured under low light conditions is a challenging task. Recent deep learning based RAW enhancement methods have shifted from using real paired data to relying on synthetic datasets. These synthetic datasets are typically generated by physically modeling sensor noise, but existing approaches often consider only additive noise, ignore multiplicative components, and rely on global calibration that overlooks pixel level manufacturing variations. As a result, such methods struggle to accurately reproduce real sensor noise. To address these limitations, this paper derives a noise model from the physical noise generation mechanisms that occur under low illumination and proposes a novel composite model that integrates both additive and multiplicative noise. To solve the model, we introduce a physics based per pixel noise simulation and calibration scheme that estimates and synthesizes noise for each individual pixel, thereby overcoming the restrictions of traditional global calibration and capturing spatial noise variations induced by microscopic CMOS manufacturing differences. Motivated by the strong performance of rectified flow methods in image generation and processing, we further combine the physics-based noise synthesis with a rectified flow generative framework and present PGRF a physics-guided rectified flow framework for low light image enhancement. PGRF leverages the ability of rectified flows to model complex data distributions and uses physical guidance to steer the generation toward the desired clean image. To validate the effectiveness of the proposed model, we established the LLID dataset, an indoor low light benchmark captured with the Sony A7S II camera. Experimental results demonstrate that the proposed framework achieves significant improvements in low light RAW image enhancement.

</details>
