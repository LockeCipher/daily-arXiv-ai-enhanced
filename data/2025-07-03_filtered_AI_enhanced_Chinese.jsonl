{"id": "2507.01099", "pdf": "https://arxiv.org/pdf/2507.01099", "abs": "https://arxiv.org/abs/2507.01099", "authors": ["Zeyi Liu", "Shuang Li", "Eric Cousineau", "Siyuan Feng", "Benjamin Burchfiel", "Shuran Song"], "title": "Geometry-aware 4D Video Generation for Robot Manipulation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Project website: https://robot4dgen.github.io", "summary": "Understanding and predicting the dynamics of the physical world can enhance a robot's ability to plan and interact effectively in complex environments. While recent video generation models have shown strong potential in modeling dynamic scenes, generating videos that are both temporally coherent and geometrically consistent across camera views remains a significant challenge. To address this, we propose a 4D video generation model that enforces multi-view 3D consistency of videos by supervising the model with cross-view pointmap alignment during training. This geometric supervision enables the model to learn a shared 3D representation of the scene, allowing it to predict future video sequences from novel viewpoints based solely on the given RGB-D observations, without requiring camera poses as inputs. Compared to existing baselines, our method produces more visually stable and spatially aligned predictions across multiple simulated and real-world robotic datasets. We further show that the predicted 4D videos can be used to recover robot end-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting robust robot manipulation and generalization to novel camera viewpoints.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd4D\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u8de8\u89c6\u89d2\u70b9\u56fe\u5bf9\u9f50\u76d1\u7763\uff0c\u5b9e\u73b0\u591a\u89c6\u89d23D\u4e00\u81f4\u6027\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5bf9\u590d\u6742\u73af\u5883\u7684\u52a8\u6001\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u589e\u5f3a\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u89c4\u5212\u548c\u4ea4\u4e92\u7684\u80fd\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u591a\u89c6\u89d2\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u65f6\u95f4\u8fde\u8d2f\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa4D\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u5229\u7528\u8de8\u89c6\u89d2\u70b9\u56fe\u5bf9\u9f50\u76d1\u7763\u8bad\u7ec3\uff0c\u5b66\u4e60\u5171\u4eab3D\u573a\u666f\u8868\u793a\uff0c\u65e0\u9700\u76f8\u673a\u59ff\u6001\u8f93\u5165\u5373\u53ef\u9884\u6d4b\u65b0\u89c6\u89d2\u7684\u672a\u6765\u89c6\u9891\u5e8f\u5217\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u751f\u6210\u7684\u89c6\u9891\u5728\u89c6\u89c9\u7a33\u5b9a\u6027\u548c\u7a7a\u95f4\u5bf9\u9f50\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u652f\u6301\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u6cdb\u5316\u5230\u65b0\u89c6\u89d2\uff0c\u901a\u8fc7\u9884\u6d4b\u76844D\u89c6\u9891\u53ef\u6062\u590d\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u8f68\u8ff9\u3002"}}
{"id": "2507.01110", "pdf": "https://arxiv.org/pdf/2507.01110", "abs": "https://arxiv.org/abs/2507.01110", "authors": ["Felix Windisch", "Lukas Radl", "Thomas K\u00f6hler", "Michael Steiner", "Dieter Schmalstieg", "Markus Steinberger"], "title": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory", "categories": ["cs.GR", "cs.LG"], "comment": null, "summary": "Gaussian Splatting has emerged as a high-performance technique for novel view synthesis, enabling real-time rendering and high-quality reconstruction of small scenes. However, scaling to larger environments has so far relied on partitioning the scene into chunks -- a strategy that introduces artifacts at chunk boundaries, complicates training across varying scales, and is poorly suited to unstructured scenarios such as city-scale flyovers combined with street-level views. Moreover, rendering remains fundamentally limited by GPU memory, as all visible chunks must reside in VRAM simultaneously. We introduce A LoD of Gaussians, a framework for training and rendering ultra-large-scale Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our method stores the full scene out-of-core (e.g., in CPU memory) and trains a Level-of-Detail (LoD) representation directly, dynamically streaming only the relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with Sequential Point Trees enables efficient, view-dependent LoD selection, while a lightweight caching and view scheduling system exploits temporal coherence to support real-time streaming and rendering. Together, these innovations enable seamless multi-scale reconstruction and interactive visualization of complex scenes -- from broad aerial views to fine-grained ground-level details.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5206\u533a\u7684\u5355GPU\u8bad\u7ec3\u548c\u6e32\u67d3\u8d85\u5927\u89c4\u6a21\u9ad8\u65af\u573a\u666f\u7684\u6846\u67b6\uff0c\u652f\u6301\u591a\u5c3a\u5ea6\u91cd\u5efa\u548c\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u3002", "motivation": "\u89e3\u51b3\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u56e0\u5206\u533a\u5bfc\u81f4\u7684\u8fb9\u754c\u4f2a\u5f71\u3001\u8bad\u7ec3\u590d\u6742\u6027\u548cGPU\u5185\u5b58\u9650\u5236\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5c42\u6b21\u5316\u9ad8\u65af\u4e0e\u987a\u5e8f\u70b9\u6811\u7ed3\u5408\u7684\u6df7\u5408\u6570\u636e\u7ed3\u6784\uff0c\u52a8\u6001\u6d41\u5f0f\u4f20\u8f93\u76f8\u5173\u9ad8\u65af\uff0c\u5e76\u5229\u7528\u8f7b\u91cf\u7ea7\u7f13\u5b58\u548c\u89c6\u56fe\u8c03\u5ea6\u7cfb\u7edf\u3002", "result": "\u5b9e\u73b0\u4e86\u65e0\u7f1d\u591a\u5c3a\u5ea6\u91cd\u5efa\u548c\u590d\u6742\u573a\u666f\u7684\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\uff0c\u4ece\u7a7a\u4e2d\u4fef\u77b0\u5230\u5730\u9762\u7ec6\u8282\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5355\u6d88\u8d39\u7ea7GPU\u4e0a\u6210\u529f\u8bad\u7ec3\u548c\u6e32\u67d3\u8d85\u5927\u89c4\u6a21\u9ad8\u65af\u573a\u666f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.01255", "pdf": "https://arxiv.org/pdf/2507.01255", "abs": "https://arxiv.org/abs/2507.01255", "authors": ["Xiao Liu", "Jiawei Zhang"], "title": "AIGVE-MACS: Unified Multi-Aspect Commenting and Scoring Model for AI-Generated Video Evaluation", "categories": ["cs.CV"], "comment": "Working in Progress", "summary": "The rapid advancement of AI-generated video models has created a pressing need for robust and interpretable evaluation frameworks. Existing metrics are limited to producing numerical scores without explanatory comments, resulting in low interpretability and human evaluation alignment. To address those challenges, we introduce AIGVE-MACS, a unified model for AI-Generated Video Evaluation(AIGVE), which can provide not only numerical scores but also multi-aspect language comment feedback in evaluating these generated videos. Central to our approach is AIGVE-BENCH 2, a large-scale benchmark comprising 2,500 AI-generated videos and 22,500 human-annotated detailed comments and numerical scores across nine critical evaluation aspects. Leveraging AIGVE-BENCH 2, AIGVE-MACS incorporates recent Vision-Language Models with a novel token-wise weighted loss and a dynamic frame sampling strategy to better align with human evaluators. Comprehensive experiments across supervised and zero-shot benchmarks demonstrate that AIGVE-MACS achieves state-of-the-art performance in both scoring correlation and comment quality, significantly outperforming prior baselines including GPT-4o and VideoScore. In addition, we further showcase a multi-agent refinement framework where feedback from AIGVE-MACS drives iterative improvements in video generation, leading to 53.5% quality enhancement. This work establishes a new paradigm for comprehensive, human-aligned evaluation of AI-generated videos. We release the AIGVE-BENCH 2 and AIGVE-MACS at https://huggingface.co/xiaoliux/AIGVE-MACS.", "AI": {"tldr": "\u63d0\u51faAIGVE-MACS\u6a21\u578b\uff0c\u7528\u4e8eAI\u751f\u6210\u89c6\u9891\u7684\u591a\u65b9\u9762\u8bc4\u4f30\uff0c\u63d0\u4f9b\u5206\u6570\u548c\u8bed\u8a00\u53cd\u9988\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709AI\u751f\u6210\u89c6\u9891\u8bc4\u4f30\u6307\u6807\u7f3a\u4e4f\u89e3\u91ca\u6027\uff0c\u96be\u4ee5\u4e0e\u4eba\u7c7b\u8bc4\u4ef7\u5bf9\u9f50\u3002", "method": "\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u52a0\u6743\u635f\u5931\u548c\u52a8\u6001\u5e27\u91c7\u6837\u7b56\u7565\uff0c\u6784\u5efaAIGVE-BENCH 2\u57fa\u51c6\u3002", "result": "\u5728\u8bc4\u5206\u76f8\u5173\u6027\u548c\u8bc4\u8bba\u8d28\u91cf\u4e0a\u8fbe\u5230\u6700\u4f18\uff0c\u89c6\u9891\u751f\u6210\u8d28\u91cf\u63d0\u534753.5%\u3002", "conclusion": "AIGVE-MACS\u4e3aAI\u751f\u6210\u89c6\u9891\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5168\u9762\u3001\u4eba\u7c7b\u5bf9\u9f50\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.01305", "pdf": "https://arxiv.org/pdf/2507.01305", "abs": "https://arxiv.org/abs/2507.01305", "authors": ["Worameth Chinchuthakun", "Pakkapon Phongthawee", "Amit Raj", "Varun Jampani", "Pramook Khungurn", "Supasorn Suwajanakorn"], "title": "DiffusionLight-Turbo: Accelerated Light Probes for Free via Single-Pass Chrome Ball Inpainting", "categories": ["cs.CV", "cs.GR", "cs.LG", "I.3.3; I.4.8"], "comment": "arXiv admin note: substantial text overlap with arXiv:2312.09168", "summary": "We introduce a simple yet effective technique for estimating lighting from a single low-dynamic-range (LDR) image by reframing the task as a chrome ball inpainting problem. This approach leverages a pre-trained diffusion model, Stable Diffusion XL, to overcome the generalization failures of existing methods that rely on limited HDR panorama datasets. While conceptually simple, the task remains challenging because diffusion models often insert incorrect or inconsistent content and cannot readily generate chrome balls in HDR format. Our analysis reveals that the inpainting process is highly sensitive to the initial noise in the diffusion process, occasionally resulting in unrealistic outputs. To address this, we first introduce DiffusionLight, which uses iterative inpainting to compute a median chrome ball from multiple outputs to serve as a stable, low-frequency lighting prior that guides the generation of a high-quality final result. To generate high-dynamic-range (HDR) light probes, an Exposure LoRA is fine-tuned to create LDR images at multiple exposure values, which are then merged. While effective, DiffusionLight is time-intensive, requiring approximately 30 minutes per estimation. To reduce this overhead, we introduce DiffusionLight-Turbo, which reduces the runtime to about 30 seconds with minimal quality loss. This 60x speedup is achieved by training a Turbo LoRA to directly predict the averaged chrome balls from the iterative process. Inference is further streamlined into a single denoising pass using a LoRA swapping technique. Experimental results that show our method produces convincing light estimates across diverse settings and demonstrates superior generalization to in-the-wild scenarios. Our code is available at https://diffusionlight.github.io/turbo", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5c06\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u9540\u94ec\u7403\u4fee\u590d\u95ee\u9898\uff0c\u4ece\u5355\u5f20\u4f4e\u52a8\u6001\u8303\u56f4\uff08LDR\uff09\u56fe\u50cf\u4f30\u8ba1\u5149\u7167\u7684\u7b80\u5355\u6709\u6548\u6280\u672f\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578bStable Diffusion XL\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u4fee\u590d\u751f\u6210\u9ad8\u8d28\u91cf\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6709\u9650\u7684HDR\u5168\u666f\u6570\u636e\u96c6\uff0c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u800c\u6269\u6563\u6a21\u578b\u5728\u751f\u6210HDR\u683c\u5f0f\u7684\u9540\u94ec\u7403\u65f6\u5b58\u5728\u5185\u5bb9\u4e0d\u4e00\u81f4\u6216\u9519\u8bef\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faDiffusionLight\uff0c\u901a\u8fc7\u8fed\u4ee3\u4fee\u590d\u8ba1\u7b97\u591a\u4e2a\u8f93\u51fa\u7684\u4e2d\u503c\u9540\u94ec\u7403\u4f5c\u4e3a\u7a33\u5b9a\u7684\u4f4e\u9891\u5149\u7167\u5148\u9a8c\uff0c\u5e76\u8bad\u7ec3Exposure LoRA\u751f\u6210\u591a\u66dd\u5149LDR\u56fe\u50cf\u4ee5\u5408\u5e76\u4e3aHDR\u5149\u63a2\u9488\u3002\u8fdb\u4e00\u6b65\u63d0\u51faDiffusionLight-Turbo\uff0c\u901a\u8fc7Turbo LoRA\u76f4\u63a5\u9884\u6d4b\u8fed\u4ee3\u8fc7\u7a0b\u7684\u5e73\u5747\u9540\u94ec\u7403\uff0c\u663e\u8457\u52a0\u901f\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u751f\u6210\u903c\u771f\u7684\u5149\u7167\u4f30\u8ba1\uff0c\u5e76\u5728\u91ce\u5916\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DiffusionLight\u53ca\u5176\u52a0\u901f\u7248\u672cDiffusionLight-Turbo\u901a\u8fc7\u521b\u65b0\u7684\u8fed\u4ee3\u4fee\u590d\u548cLoRA\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5149\u7167\u4f30\u8ba1\u4efb\u52a1\u4e2d\u7684\u6311\u6218\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u3002"}}
{"id": "2507.01275", "pdf": "https://arxiv.org/pdf/2507.01275", "abs": "https://arxiv.org/abs/2507.01275", "authors": ["Chengxu Liu", "Lu Qi", "Jinshan Pan", "Xueming Qian", "Ming-Hsuan Yang"], "title": "Frequency Domain-Based Diffusion Model for Unpaired Image Dehazing", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Unpaired image dehazing has attracted increasing attention due to its flexible data requirements during model training. Dominant methods based on contrastive learning not only introduce haze-unrelated content information, but also ignore haze-specific properties in the frequency domain (\\ie,~haze-related degradation is mainly manifested in the amplitude spectrum). To address these issues, we propose a novel frequency domain-based diffusion model, named \\ours, for fully exploiting the beneficial knowledge in unpaired clear data. In particular, inspired by the strong generative ability shown by Diffusion Models (DMs), we tackle the dehazing task from the perspective of frequency domain reconstruction and perform the DMs to yield the amplitude spectrum consistent with the distribution of clear images. To implement it, we propose an Amplitude Residual Encoder (ARE) to extract the amplitude residuals, which effectively compensates for the amplitude gap from the hazy to clear domains, as well as provide supervision for the DMs training. In addition, we propose a Phase Correction Module (PCM) to eliminate artifacts by further refining the phase spectrum during dehazing with a simple attention mechanism. Experimental results demonstrate that our \\ours outperforms other state-of-the-art methods on both synthetic and real-world datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9891\u57df\u7684\u6269\u6563\u6a21\u578b\uff08OURS\uff09\uff0c\u7528\u4e8e\u65e0\u914d\u5bf9\u56fe\u50cf\u53bb\u96fe\uff0c\u901a\u8fc7\u632f\u5e45\u6b8b\u5dee\u7f16\u7801\u5668\u548c\u76f8\u4f4d\u6821\u6b63\u6a21\u5757\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u65b9\u6cd5\u5f15\u5165\u4e86\u4e0e\u96fe\u65e0\u5173\u7684\u5185\u5bb9\u4fe1\u606f\uff0c\u4e14\u5ffd\u7565\u4e86\u9891\u57df\u4e2d\u96fe\u76f8\u5173\u7684\u7279\u6027\uff0c\u5982\u632f\u5e45\u8c31\u4e2d\u7684\u9000\u5316\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u9891\u57df\u6269\u6563\u6a21\u578b\uff0c\u5229\u7528\u632f\u5e45\u6b8b\u5dee\u7f16\u7801\u5668\uff08ARE\uff09\u8865\u507f\u632f\u5e45\u5dee\u8ddd\uff0c\u5e76\u901a\u8fc7\u76f8\u4f4d\u6821\u6b63\u6a21\u5757\uff08PCM\uff09\u6d88\u9664\u4f2a\u5f71\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cOURS\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u9891\u57df\u6269\u6563\u6a21\u578b\u7ed3\u5408\u632f\u5e45\u548c\u76f8\u4f4d\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u914d\u5bf9\u56fe\u50cf\u53bb\u96fe\u7684\u6548\u679c\u3002"}}
{"id": "2507.01631", "pdf": "https://arxiv.org/pdf/2507.01631", "abs": "https://arxiv.org/abs/2507.01631", "authors": ["Camille Billouard", "Dawa Derksen", "Alexandre Constantin", "Bruno Vallet"], "title": "Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "comment": "Accepted at ICCV 2025 Workshop 3D-VAST (From street to space: 3D   Vision Across Altitudes). Version before camera ready. Our code will be made   public after the conference", "summary": "Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D reconstruction from multiview satellite imagery. However, state-of-the-art NeRF methods are typically constrained to small scenes due to the memory footprint during training, which we study in this paper. Previous work on large-scale NeRFs palliate this by dividing the scene into NeRFs. This paper introduces Snake-NeRF, a framework that scales to large scenes. Our out-of-core method eliminates the need to load all images and networks simultaneously, and operates on a single device. We achieve this by dividing the region of interest into NeRFs that 3D tile without overlap. Importantly, we crop the images with overlap to ensure each NeRFs is trained with all the necessary pixels. We introduce a novel $2\\times 2$ 3D tile progression strategy and segmented sampler, which together prevent 3D reconstruction errors along the tile edges. Our experiments conclude that large satellite images can effectively be processed with linear time complexity, on a single GPU, and without compromise in quality.", "AI": {"tldr": "Snake-NeRF\u662f\u4e00\u79cd\u7528\u4e8e\u5927\u89c4\u6a21\u573a\u666f3D\u91cd\u5efa\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5757\u5904\u7406\u548c\u4f18\u5316\u91c7\u6837\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfNeRF\u65b9\u6cd5\u5185\u5b58\u5360\u7528\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709NeRF\u65b9\u6cd5\u56e0\u5185\u5b58\u9650\u5236\u4ec5\u9002\u7528\u4e8e\u5c0f\u573a\u666f\uff0c\u800c\u5927\u89c4\u6a21\u573a\u666f\u9700\u8981\u5206\u5757\u5904\u7406\uff0c\u4f46\u4f20\u7edf\u5206\u5757\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u8fb9\u7f18\u91cd\u5efa\u9519\u8bef\u3002", "method": "\u63d0\u51faSnake-NeRF\u6846\u67b6\uff0c\u91c7\u7528\u5916\u5b58\u65b9\u6cd5\u907f\u514d\u540c\u65f6\u52a0\u8f7d\u6240\u6709\u6570\u636e\u548c\u7f51\u7edc\uff0c\u5c06\u573a\u666f\u5212\u5206\u4e3a\u65e0\u91cd\u53e0\u76843D\u5757\uff0c\u5e76\u5f15\u51652\u00d72\u5757\u7ea7\u8fdb\u7b56\u7565\u548c\u5206\u6bb5\u91c7\u6837\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSnake-NeRF\u80fd\u5728\u5355GPU\u4e0a\u7ebf\u6027\u65f6\u95f4\u5904\u7406\u5927\u89c4\u6a21\u536b\u661f\u56fe\u50cf\uff0c\u4e14\u4e0d\u635f\u5931\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "Snake-NeRF\u6210\u529f\u6269\u5c55\u4e86NeRF\u7684\u5e94\u7528\u8303\u56f4\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u573a\u666f\u76843D\u91cd\u5efa\u3002"}}
{"id": "2507.01342", "pdf": "https://arxiv.org/pdf/2507.01342", "abs": "https://arxiv.org/abs/2507.01342", "authors": ["Luxi Zhao", "Mahmoud Afifi", "Michael S. Brown"], "title": "Learning Camera-Agnostic White-Balance Preferences", "categories": ["cs.CV"], "comment": null, "summary": "The image signal processor (ISP) pipeline in modern cameras consists of several modules that transform raw sensor data into visually pleasing images in a display color space. Among these, the auto white balance (AWB) module is essential for compensating for scene illumination. However, commercial AWB systems often strive to compute aesthetic white-balance preferences rather than accurate neutral color correction. While learning-based methods have improved AWB accuracy, they typically struggle to generalize across different camera sensors -- an issue for smartphones with multiple cameras. Recent work has explored cross-camera AWB, but most methods remain focused on achieving neutral white balance. In contrast, this paper is the first to address aesthetic consistency by learning a post-illuminant-estimation mapping that transforms neutral illuminant corrections into aesthetically preferred corrections in a camera-agnostic space. Once trained, our mapping can be applied after any neutral AWB module to enable consistent and stylized color rendering across unseen cameras. Our proposed model is lightweight -- containing only $\\sim$500 parameters -- and runs in just 0.024 milliseconds on a typical flagship mobile CPU. Evaluated on a dataset of 771 smartphone images from three different cameras, our method achieves state-of-the-art performance while remaining fully compatible with existing cross-camera AWB techniques, introducing minimal computational and memory overhead.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u540e\u5149\u7167\u4f30\u8ba1\u6620\u5c04\uff0c\u5c06\u4e2d\u6027\u767d\u5e73\u8861\u6821\u6b63\u8f6c\u6362\u4e3a\u7f8e\u5b66\u504f\u597d\u7684\u6821\u6b63\uff0c\u5b9e\u73b0\u8de8\u76f8\u673a\u7f8e\u5b66\u4e00\u81f4\u6027\u3002", "motivation": "\u5546\u4e1a\u81ea\u52a8\u767d\u5e73\u8861\uff08AWB\uff09\u7cfb\u7edf\u901a\u5e38\u8ffd\u6c42\u7f8e\u5b66\u504f\u597d\u800c\u975e\u51c6\u786e\u4e2d\u6027\u6821\u6b63\uff0c\u4e14\u73b0\u6709\u5b66\u4e60\u578b\u65b9\u6cd5\u96be\u4ee5\u8de8\u76f8\u673a\u4f20\u611f\u5668\u6cdb\u5316\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u540e\u5149\u7167\u4f30\u8ba1\u6620\u5c04\uff0c\u5c06\u4e2d\u6027AWB\u6821\u6b63\u8f6c\u6362\u4e3a\u76f8\u673a\u65e0\u5173\u7a7a\u95f4\u7684\u7f8e\u5b66\u504f\u597d\u6821\u6b63\uff0c\u6a21\u578b\u4ec5\u542b\u7ea6500\u53c2\u6570\u3002", "result": "\u5728771\u5f20\u667a\u80fd\u624b\u673a\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u8ba1\u7b97\u65f6\u95f4\u4ec50.024\u6beb\u79d2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u8f7b\u91cf\u7ea7\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u8de8\u76f8\u673a\u7f8e\u5b66\u4e00\u81f4\u6027\uff0c\u517c\u5bb9\u73b0\u6709\u6280\u672f\u4e14\u8ba1\u7b97\u5f00\u9500\u4f4e\u3002"}}
{"id": "2507.01367", "pdf": "https://arxiv.org/pdf/2507.01367", "abs": "https://arxiv.org/abs/2507.01367", "authors": ["Tianrui Lou", "Xiaojun Jia", "Siyuan Liang", "Jiawei Liang", "Ming Zhang", "Yanjun Xiao", "Xiaochun Cao"], "title": "3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Physical adversarial attack methods expose the vulnerabilities of deep neural networks and pose a significant threat to safety-critical scenarios such as autonomous driving. Camouflage-based physical attack is a more promising approach compared to the patch-based attack, offering stronger adversarial effectiveness in complex physical environments. However, most prior work relies on mesh priors of the target object and virtual environments constructed by simulators, which are time-consuming to obtain and inevitably differ from the real world. Moreover, due to the limitations of the backgrounds in training images, previous methods often fail to produce multi-view robust adversarial camouflage and tend to fall into sub-optimal solutions. Due to these reasons, prior work lacks adversarial effectiveness and robustness across diverse viewpoints and physical environments. We propose a physical attack framework based on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and precise reconstruction with few images, along with photo-realistic rendering capabilities. Our framework further enhances cross-view robustness and adversarial effectiveness by preventing mutual and self-occlusion among Gaussians and employing a min-max optimization approach that adjusts the imaging background of each viewpoint, helping the algorithm filter out non-robust adversarial features. Extensive experiments validate the effectiveness and superiority of PGA. Our code is available at:https://github.com/TRLou/PGA.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u7684\u7269\u7406\u653b\u51fb\u6846\u67b6PGA\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u6709\u4f2a\u88c5\u653b\u51fb\u65b9\u6cd5\u5728\u591a\u89c6\u89d2\u548c\u590d\u6742\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7269\u7406\u653b\u51fb\u65b9\u6cd5\u4f9d\u8d56\u76ee\u6807\u5bf9\u8c61\u7684\u7f51\u683c\u5148\u9a8c\u548c\u6a21\u62df\u73af\u5883\uff0c\u8017\u65f6\u957f\u4e14\u4e0e\u73b0\u5b9e\u5dee\u5f02\u5927\uff0c\u5bfc\u81f4\u5bf9\u6297\u6548\u679c\u548c\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "PGA\u5229\u75283DGS\u5b9e\u73b0\u5feb\u901f\u7cbe\u786e\u7684\u91cd\u5efa\u548c\u903c\u771f\u6e32\u67d3\uff0c\u901a\u8fc7\u9632\u6b62\u9ad8\u65af\u95f4\u7684\u76f8\u4e92\u906e\u6321\u548c\u81ea\u906e\u6321\uff0c\u7ed3\u5408min-max\u4f18\u5316\u8c03\u6574\u80cc\u666f\uff0c\u63d0\u5347\u591a\u89c6\u89d2\u9c81\u68d2\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PGA\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "PGA\u663e\u8457\u63d0\u5347\u4e86\u7269\u7406\u653b\u51fb\u5728\u591a\u89c6\u89d2\u548c\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bf9\u6297\u6548\u679c\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.01397", "pdf": "https://arxiv.org/pdf/2507.01397", "abs": "https://arxiv.org/abs/2507.01397", "authors": ["Khanh Son Pham", "Christian Witte", "Jens Behley", "Johannes Betz", "Cyrill Stachniss"], "title": "Coherent Online Road Topology Estimation and Reasoning with Standard-Definition Maps", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at IROS 2025", "summary": "Most autonomous cars rely on the availability of high-definition (HD) maps. Current research aims to address this constraint by directly predicting HD map elements from onboard sensors and reasoning about the relationships between the predicted map and traffic elements. Despite recent advancements, the coherent online construction of HD maps remains a challenging endeavor, as it necessitates modeling the high complexity of road topologies in a unified and consistent manner. To address this challenge, we propose a coherent approach to predict lane segments and their corresponding topology, as well as road boundaries, all by leveraging prior map information represented by commonly available standard-definition (SD) maps. We propose a network architecture, which leverages hybrid lane segment encodings comprising prior information and denoising techniques to enhance training stability and performance. Furthermore, we facilitate past frames for temporal consistency. Our experimental evaluation demonstrates that our approach outperforms previous methods by a large margin, highlighting the benefits of our modeling scheme.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6807\u51c6\u5730\u56fe\uff08SD\uff09\u7684\u81ea\u52a8\u9a7e\u9a76\u9ad8\u7cbe\u5730\u56fe\uff08HD\uff09\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5148\u9a8c\u4fe1\u606f\u548c\u53bb\u566a\u6280\u672f\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u9ad8\u7cbe\u5730\u56fe\u5728\u7ebf\u6784\u5efa\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u9053\u8def\u62d3\u6251\u7684\u9ad8\u590d\u6742\u6027\u5efa\u6a21\u95ee\u9898\u3002", "method": "\u5229\u7528\u5148\u9a8c\u5730\u56fe\u4fe1\u606f\uff08SD\u5730\u56fe\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7f51\u7edc\u67b6\u6784\uff0c\u7ed3\u5408\u6df7\u5408\u8f66\u9053\u6bb5\u7f16\u7801\u548c\u53bb\u566a\u6280\u672f\uff0c\u5e76\u5f15\u5165\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5efa\u6a21\u65b9\u6848\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u5148\u9a8c\u4fe1\u606f\u548c\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u9ad8\u7cbe\u5730\u56fe\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2507.01422", "pdf": "https://arxiv.org/pdf/2507.01422", "abs": "https://arxiv.org/abs/2507.01422", "authors": ["Wenjie Liu", "Bingshu Wang", "Ze Wang", "C. L. Philip Chen"], "title": "DocShaDiffusion: Diffusion Model in Latent Space for Document Image Shadow Removal", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Document shadow removal is a crucial task in the field of document image enhancement. However, existing methods tend to remove shadows with constant color background and ignore color shadows. In this paper, we first design a diffusion model in latent space for document image shadow removal, called DocShaDiffusion. It translates shadow images from pixel space to latent space, enabling the model to more easily capture essential features. To address the issue of color shadows, we design a shadow soft-mask generation module (SSGM). It is able to produce accurate shadow mask and add noise into shadow regions specially. Guided by the shadow mask, a shadow mask-aware guided diffusion module (SMGDM) is proposed to remove shadows from document images by supervising the diffusion and denoising process. We also propose a shadow-robust perceptual feature loss to preserve details and structures in document images. Moreover, we develop a large-scale synthetic document color shadow removal dataset (SDCSRD). It simulates the distribution of realistic color shadows and provides powerful supports for the training of models. Experiments on three public datasets validate the proposed method's superiority over state-of-the-art. Our code and dataset will be publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDocShaDiffusion\u7684\u6f5c\u5728\u7a7a\u95f4\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u6587\u6863\u56fe\u50cf\u9634\u5f71\u53bb\u9664\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9634\u5f71\u8f6f\u63a9\u6a21\u751f\u6210\u6a21\u5757\uff08SSGM\uff09\u548c\u9634\u5f71\u63a9\u6a21\u5f15\u5bfc\u6269\u6563\u6a21\u5757\uff08SMGDM\uff09\u4ee5\u89e3\u51b3\u5f69\u8272\u9634\u5f71\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4ec5\u5904\u7406\u6052\u5b9a\u989c\u8272\u80cc\u666f\u7684\u9634\u5f71\uff0c\u800c\u5ffd\u7565\u4e86\u5f69\u8272\u9634\u5f71\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u53bb\u9664\u6587\u6863\u56fe\u50cf\u4e2d\u7684\u5f69\u8272\u9634\u5f71\u3002", "method": "1. \u8bbe\u8ba1\u6f5c\u5728\u7a7a\u95f4\u6269\u6563\u6a21\u578bDocShaDiffusion\uff1b2. \u63d0\u51faSSGM\u751f\u6210\u9634\u5f71\u63a9\u6a21\u5e76\u6dfb\u52a0\u566a\u58f0\uff1b3. \u63d0\u51faSMGDM\u5f15\u5bfc\u6269\u6563\u548c\u53bb\u566a\u8fc7\u7a0b\uff1b4. \u5f15\u5165\u9634\u5f71\u9c81\u68d2\u611f\u77e5\u7279\u5f81\u635f\u5931\uff1b5. \u6784\u5efa\u5408\u6210\u6570\u636e\u96c6SDCSRD\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "DocShaDiffusion\u5728\u6587\u6863\u56fe\u50cf\u9634\u5f71\u53bb\u9664\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u3002"}}
{"id": "2507.01428", "pdf": "https://arxiv.org/pdf/2507.01428", "abs": "https://arxiv.org/abs/2507.01428", "authors": ["Chen Sun", "Haiyang Sun", "Zhiqing Guo", "Yunfeng Diao", "Liejun Wang", "Dan Ma", "Gaobo Yang", "Keqin Li"], "title": "DiffMark: Diffusion-based Robust Watermark Against Deepfakes", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Deepfakes pose significant security and privacy threats through malicious facial manipulations. While robust watermarking can aid in authenticity verification and source tracking, existing methods often lack the sufficient robustness against Deepfake manipulations. Diffusion models have demonstrated remarkable performance in image generation, enabling the seamless fusion of watermark with image during generation. In this study, we propose a novel robust watermarking framework based on diffusion model, called DiffMark. By modifying the training and sampling scheme, we take the facial image and watermark as conditions to guide the diffusion model to progressively denoise and generate corresponding watermarked image. In the construction of facial condition, we weight the facial image by a timestep-dependent factor that gradually reduces the guidance intensity with the decrease of noise, thus better adapting to the sampling process of diffusion model. To achieve the fusion of watermark condition, we introduce a cross information fusion (CIF) module that leverages a learnable embedding table to adaptively extract watermark features and integrates them with image features via cross-attention. To enhance the robustness of the watermark against Deepfake manipulations, we integrate a frozen autoencoder during training phase to simulate Deepfake manipulations. Additionally, we introduce Deepfake-resistant guidance that employs specific Deepfake model to adversarially guide the diffusion sampling process to generate more robust watermarked images. Experimental results demonstrate the effectiveness of the proposed DiffMark on typical Deepfakes. Our code will be available at https://github.com/vpsg-research/DiffMark.", "AI": {"tldr": "DiffMark\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9c81\u68d2\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u8bad\u7ec3\u548c\u91c7\u6837\u65b9\u6848\uff0c\u7ed3\u5408\u9762\u90e8\u56fe\u50cf\u548c\u6c34\u5370\u6761\u4ef6\u751f\u6210\u6c34\u5370\u56fe\u50cf\uff0c\u5e76\u5229\u7528\u4ea4\u53c9\u4fe1\u606f\u878d\u5408\u6a21\u5757\u548c\u5bf9\u6297\u6027\u6307\u5bfc\u589e\u5f3a\u6c34\u5370\u5bf9Deepfake\u64cd\u4f5c\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u5bf9Deepfake\u64cd\u4f5c\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u4f18\u5f02\u8868\u73b0\u63d0\u4f9b\u4e86\u6539\u8fdb\u6c34\u5370\u6280\u672f\u7684\u673a\u4f1a\u3002", "method": "\u901a\u8fc7\u4fee\u6539\u6269\u6563\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u91c7\u6837\u65b9\u6848\uff0c\u7ed3\u5408\u9762\u90e8\u56fe\u50cf\u548c\u6c34\u5370\u6761\u4ef6\uff0c\u4f7f\u7528\u4ea4\u53c9\u4fe1\u606f\u878d\u5408\u6a21\u5757\u548c\u5bf9\u6297\u6027\u6307\u5bfc\u751f\u6210\u6c34\u5370\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eDiffMark\u5728\u5178\u578bDeepfake\u64cd\u4f5c\u4e2d\u8868\u73b0\u6709\u6548\u3002", "conclusion": "DiffMark\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u5bf9\u6297\u6027\u6307\u5bfc\u5b9e\u73b0\u4e86\u5bf9Deepfake\u64cd\u4f5c\u7684\u9c81\u68d2\u6c34\u5370\u751f\u6210\u3002"}}
{"id": "2507.01467", "pdf": "https://arxiv.org/pdf/2507.01467", "abs": "https://arxiv.org/abs/2507.01467", "authors": ["Ge Wu", "Shen Zhang", "Ruijing Shi", "Shanghua Gao", "Zhenyuan Chen", "Lei Wang", "Zhaowei Chen", "Hongcheng Gao", "Yao Tang", "Jian Yang", "Ming-Ming Cheng", "Xiang Li"], "title": "Representation Entanglement for Generation:Training Diffusion Transformers Is Much Easier Than You Think", "categories": ["cs.CV"], "comment": null, "summary": "REPA and its variants effectively mitigate training challenges in diffusion models by incorporating external visual representations from pretrained models, through alignment between the noisy hidden projections of denoising networks and foundational clean image representations. We argue that the external alignment, which is absent during the entire denoising inference process, falls short of fully harnessing the potential of discriminative representations. In this work, we propose a straightforward method called Representation Entanglement for Generation (REG), which entangles low-level image latents with a single high-level class token from pretrained foundation models for denoising. REG acquires the capability to produce coherent image-class pairs directly from pure noise, substantially improving both generation quality and training efficiency. This is accomplished with negligible additional inference overhead, requiring only one single additional token for denoising (<0.5\\% increase in FLOPs and latency). The inference process concurrently reconstructs both image latents and their corresponding global semantics, where the acquired semantic knowledge actively guides and enhances the image generation process. On ImageNet 256$\\times$256, SiT-XL/2 + REG demonstrates remarkable convergence acceleration, achieving $\\textbf{63}\\times$ and $\\textbf{23}\\times$ faster training than SiT-XL/2 and SiT-XL/2 + REPA, respectively. More impressively, SiT-L/2 + REG trained for merely 400K iterations outperforms SiT-XL/2 + REPA trained for 4M iterations ($\\textbf{10}\\times$ longer). Code is available at: https://github.com/Martinser/REG.", "AI": {"tldr": "REG\u901a\u8fc7\u5c06\u4f4e\u5c42\u56fe\u50cf\u6f5c\u5728\u8868\u793a\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u9ad8\u5c42\u7c7b\u522b\u6807\u8bb0\u7ea0\u7f20\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\u548c\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982REPA\uff09\u5728\u53bb\u566a\u63a8\u7406\u8fc7\u7a0b\u4e2d\u672a\u80fd\u5145\u5206\u5229\u7528\u5224\u522b\u6027\u8868\u793a\uff0c\u9650\u5236\u4e86\u751f\u6210\u6a21\u578b\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faREG\u65b9\u6cd5\uff0c\u5c06\u56fe\u50cf\u6f5c\u5728\u8868\u793a\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5355\u4e00\u9ad8\u5c42\u7c7b\u522b\u6807\u8bb0\u7ea0\u7f20\uff0c\u5b9e\u73b0\u56fe\u50cf\u548c\u7c7b\u522b\u7684\u8054\u5408\u751f\u6210\u3002", "result": "\u5728ImageNet 256\u00d7256\u4e0a\uff0cREG\u663e\u8457\u52a0\u901f\u8bad\u7ec3\uff0863\u500d\u548c23\u500d\uff09\uff0c\u5e76\u5728\u66f4\u77ed\u8bad\u7ec3\u65f6\u95f4\u5185\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "REG\u901a\u8fc7\u8bed\u4e49\u5f15\u5bfc\u751f\u6210\uff0c\u9ad8\u6548\u4e14\u8f7b\u91cf\u5730\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.01573", "pdf": "https://arxiv.org/pdf/2507.01573", "abs": "https://arxiv.org/abs/2507.01573", "authors": ["Hao Wang", "Keyan Hu", "Xin Guo", "Haifeng Li", "Chao Tao"], "title": "A Gift from the Integration of Discriminative and Diffusion-based Generative Learning: Boundary Refinement Remote Sensing Semantic Segmentation", "categories": ["cs.CV"], "comment": "20 pages, 14 figures", "summary": "Remote sensing semantic segmentation must address both what the ground objects are within an image and where they are located. Consequently, segmentation models must ensure not only the semantic correctness of large-scale patches (low-frequency information) but also the precise localization of boundaries between patches (high-frequency information). However, most existing approaches rely heavily on discriminative learning, which excels at capturing low-frequency features, while overlooking its inherent limitations in learning high-frequency features for semantic segmentation. Recent studies have revealed that diffusion generative models excel at generating high-frequency details. Our theoretical analysis confirms that the diffusion denoising process significantly enhances the model's ability to learn high-frequency features; however, we also observe that these models exhibit insufficient semantic inference for low-frequency features when guided solely by the original image. Therefore, we integrate the strengths of both discriminative and generative learning, proposing the Integration of Discriminative and diffusion-based Generative learning for Boundary Refinement (IDGBR) framework. The framework first generates a coarse segmentation map using a discriminative backbone model. This map and the original image are fed into a conditioning guidance network to jointly learn a guidance representation subsequently leveraged by an iterative denoising diffusion process refining the coarse segmentation. Extensive experiments across five remote sensing semantic segmentation datasets (binary and multi-class segmentation) confirm our framework's capability of consistent boundary refinement for coarse results from diverse discriminative architectures. The source code will be available at https://github.com/KeyanHu-git/IDGBR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5224\u522b\u5f0f\u5b66\u4e60\u548c\u6269\u6563\u751f\u6210\u5b66\u4e60\u7684\u6846\u67b6IDGBR\uff0c\u7528\u4e8e\u4f18\u5316\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u7684\u8fb9\u754c\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5224\u522b\u5f0f\u5b66\u4e60\uff0c\u64c5\u957f\u6355\u6349\u4f4e\u9891\u7279\u5f81\u4f46\u5ffd\u89c6\u9ad8\u9891\u8fb9\u754c\u7ec6\u8282\uff0c\u800c\u6269\u6563\u751f\u6210\u6a21\u578b\u64c5\u957f\u751f\u6210\u9ad8\u9891\u7ec6\u8282\u4f46\u4f4e\u9891\u8bed\u4e49\u63a8\u65ad\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u5224\u522b\u5f0f\u6a21\u578b\u751f\u6210\u7c97\u5206\u5272\u56fe\uff0c\u5e76\u901a\u8fc7\u6269\u6563\u53bb\u566a\u8fc7\u7a0b\u7ec6\u5316\u8fb9\u754c\u3002", "result": "\u5728\u4e94\u4e2a\u9065\u611f\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86IDGBR\u80fd\u4e00\u81f4\u4f18\u5316\u4e0d\u540c\u5224\u522b\u5f0f\u67b6\u6784\u7684\u7c97\u5206\u5272\u7ed3\u679c\u3002", "conclusion": "IDGBR\u6846\u67b6\u6709\u6548\u6574\u5408\u4e86\u5224\u522b\u5f0f\u548c\u751f\u6210\u5f0f\u5b66\u4e60\u7684\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fb9\u754c\u5206\u5272\u7cbe\u5ea6\u3002"}}
{"id": "2507.01586", "pdf": "https://arxiv.org/pdf/2507.01586", "abs": "https://arxiv.org/abs/2507.01586", "authors": ["Bryan Constantine Sadihin", "Michael Hua Wang", "Shei Pern Chua", "Hang Su"], "title": "SketchColour: Channel Concat Guided DiT-based Sketch-to-Colour Pipeline for 2D Animation", "categories": ["cs.CV"], "comment": "Project page and code: https://bconstantine.github.io/SketchColour", "summary": "The production of high-quality 2D animation is highly labor-intensive process, as animators are currently required to draw and color a large number of frames by hand. We present SketchColour, the first sketch-to-colour pipeline for 2D animation built on a diffusion transformer (DiT) backbone. By replacing the conventional U-Net denoiser with a DiT-style architecture and injecting sketch information via lightweight channel-concatenation adapters accompanied with LoRA finetuning, our method natively integrates conditioning without the parameter and memory bloat of a duplicated ControlNet, greatly reducing parameter count and GPU memory usage. Evaluated on the SAKUGA dataset, SketchColour outperforms previous state-of-the-art video colourization methods across all metrics, despite using only half the training data of competing models. Our approach produces temporally coherent animations with minimal artifacts such as colour bleeding or object deformation. Our code is available at: https://bconstantine.github.io/SketchColour .", "AI": {"tldr": "SketchColour\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u76842D\u52a8\u753b\u8349\u56fe\u7740\u8272\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u548cLoRA\u5fae\u8c03\uff0c\u663e\u8457\u51cf\u5c11\u53c2\u6570\u548cGPU\u5185\u5b58\u4f7f\u7528\uff0c\u5e76\u5728SAKUGA\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf2D\u52a8\u753b\u5236\u4f5c\u9700\u8981\u5927\u91cf\u624b\u5de5\u7ed8\u5236\u548c\u7740\u8272\uff0c\u8017\u65f6\u8017\u529b\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u6280\u672f\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u91c7\u7528\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u66ff\u4ee3\u4f20\u7edfU-Net\u53bb\u566a\u5668\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u901a\u9053\u8fde\u63a5\u9002\u914d\u5668\u548cLoRA\u5fae\u8c03\uff0c\u907f\u514d\u53c2\u6570\u81a8\u80c0\u3002", "result": "\u5728SAKUGA\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bad\u7ec3\u6570\u636e\u4ec5\u4e3a\u7ade\u4e89\u6a21\u578b\u7684\u4e00\u534a\uff0c\u4e14\u751f\u6210\u52a8\u753b\u65f6\u95f4\u4e00\u81f4\u3001\u4f2a\u5f71\u5c11\u3002", "conclusion": "SketchColour\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u76842D\u52a8\u753b\u7740\u8272\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u751f\u4ea7\u6548\u7387\u3002"}}
{"id": "2507.01587", "pdf": "https://arxiv.org/pdf/2507.01587", "abs": "https://arxiv.org/abs/2507.01587", "authors": ["Youngjin Oh", "Junhyeong Kwon", "Keuntek Lee", "Nam Ik Cho"], "title": "Towards Controllable Real Image Denoising with Camera Parameters", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted for publication in ICIP 2025, IEEE International Conference   on Image Processing", "summary": "Recent deep learning-based image denoising methods have shown impressive performance; however, many lack the flexibility to adjust the denoising strength based on the noise levels, camera settings, and user preferences. In this paper, we introduce a new controllable denoising framework that adaptively removes noise from images by utilizing information from camera parameters. Specifically, we focus on ISO, shutter speed, and F-number, which are closely related to noise levels. We convert these selected parameters into a vector to control and enhance the performance of the denoising network. Experimental results show that our method seamlessly adds controllability to standard denoising neural networks and improves their performance. Code is available at https://github.com/OBAKSA/CPADNet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f8\u673a\u53c2\u6570\u7684\u53ef\u63a7\u53bb\u566a\u6846\u67b6\uff0c\u901a\u8fc7ISO\u3001\u5feb\u95e8\u901f\u5ea6\u548cF\u503c\u8c03\u6574\u53bb\u566a\u5f3a\u5ea6\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u6839\u636e\u566a\u58f0\u6c34\u5e73\u3001\u76f8\u673a\u8bbe\u7f6e\u548c\u7528\u6237\u504f\u597d\u8c03\u6574\u53bb\u566a\u5f3a\u5ea6\u7684\u7075\u6d3b\u6027\u3002", "method": "\u5c06ISO\u3001\u5feb\u95e8\u901f\u5ea6\u548cF\u503c\u8f6c\u6362\u4e3a\u5411\u91cf\uff0c\u7528\u4e8e\u63a7\u5236\u53bb\u566a\u7f51\u7edc\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e3a\u53bb\u566a\u7f51\u7edc\u589e\u52a0\u4e86\u53ef\u63a7\u6027\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u57fa\u4e8e\u76f8\u673a\u53c2\u6570\u7684\u81ea\u9002\u5e94\u53bb\u566a\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.01603", "pdf": "https://arxiv.org/pdf/2507.01603", "abs": "https://arxiv.org/abs/2507.01603", "authors": ["Yue-Jiang Dong", "Wang Zhao", "Jiale Xu", "Ying Shan", "Song-Hai Zhang"], "title": "DepthSync: Diffusion Guidance-Based Depth Synchronization for Scale- and Geometry-Consistent Video Depth Estimation", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Diffusion-based video depth estimation methods have achieved remarkable success with strong generalization ability. However, predicting depth for long videos remains challenging. Existing methods typically split videos into overlapping sliding windows, leading to accumulated scale discrepancies across different windows, particularly as the number of windows increases. Additionally, these methods rely solely on 2D diffusion priors, overlooking the inherent 3D geometric structure of video depths, which results in geometrically inconsistent predictions. In this paper, we propose DepthSync, a novel, training-free framework using diffusion guidance to achieve scale- and geometry-consistent depth predictions for long videos. Specifically, we introduce scale guidance to synchronize the depth scale across windows and geometry guidance to enforce geometric alignment within windows based on the inherent 3D constraints in video depths. These two terms work synergistically, steering the denoising process toward consistent depth predictions. Experiments on various datasets validate the effectiveness of our method in producing depth estimates with improved scale and geometry consistency, particularly for long videos.", "AI": {"tldr": "DepthSync\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u5f15\u5bfc\u5b9e\u73b0\u957f\u89c6\u9891\u6df1\u5ea6\u9884\u6d4b\u7684\u5c3a\u5ea6\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u5b58\u5728\u5c3a\u5ea6\u4e0d\u4e00\u81f4\u548c\u51e0\u4f55\u9884\u6d4b\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u7531\u4e8e\u6ed1\u52a8\u7a97\u53e3\u5206\u5272\u548c\u4ec5\u4f9d\u8d562D\u6269\u6563\u5148\u9a8c\u3002", "method": "\u5f15\u5165\u5c3a\u5ea6\u5f15\u5bfc\u548c\u51e0\u4f55\u5f15\u5bfc\uff0c\u5206\u522b\u540c\u6b65\u7a97\u53e3\u95f4\u7684\u6df1\u5ea6\u5c3a\u5ea6\u548c\u7a97\u53e3\u5185\u7684\u51e0\u4f55\u5bf9\u9f50\uff0c\u534f\u540c\u6307\u5bfc\u53bb\u566a\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDepthSync\u5728\u591a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u9884\u6d4b\u7684\u5c3a\u5ea6\u548c\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u957f\u89c6\u9891\u3002", "conclusion": "DepthSync\u901a\u8fc7\u5c3a\u5ea6\u548c\u51e0\u4f55\u5f15\u5bfc\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u6df1\u5ea6\u9884\u6d4b\u7684\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002"}}
{"id": "2507.01652", "pdf": "https://arxiv.org/pdf/2507.01652", "abs": "https://arxiv.org/abs/2507.01652", "authors": ["Yuxin Mao", "Zhen Qin", "Jinxing Zhou", "Hui Deng", "Xuyang Shen", "Bin Fan", "Jing Zhang", "Yiran Zhong", "Yuchao Dai"], "title": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware Decay Perspective", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "Autoregressive (AR) models have garnered significant attention in image generation for their ability to effectively capture both local and global structures within visual data. However, prevalent AR models predominantly rely on the transformer architectures, which are beset by quadratic computational complexity concerning input sequence length and substantial memory overhead due to the necessity of maintaining key-value caches. Although linear attention mechanisms have successfully reduced this burden in language models, our initial experiments reveal that they significantly degrade image generation quality because of their inability to capture critical long-range dependencies in visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a novel attention mechanism that explicitly preserves genuine 2D spatial relationships within the flattened image sequences by computing position-dependent decay factors based on true 2D spatial location rather than 1D sequence positions. Based on this mechanism, we present LASADGen, an autoregressive image generator that enables selective attention to relevant spatial contexts with linear complexity. Experiments on ImageNet show LASADGen achieves state-of-the-art image generation performance and computational efficiency, bridging the gap between linear attention's efficiency and spatial understanding needed for high-quality generation.", "AI": {"tldr": "LASADGen\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236LASAD\uff0c\u901a\u8fc7\u4fdd\u75592D\u7a7a\u95f4\u5173\u7cfb\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7ebf\u6027\u6ce8\u610f\u529b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u957f\u8ddd\u79bb\u4f9d\u8d56\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u4f20\u7edf\u81ea\u56de\u5f52\u6a21\u578b\u4f9d\u8d56Transformer\u67b6\u6784\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u4e14\u5185\u5b58\u5f00\u9500\u5927\u3002\u7ebf\u6027\u6ce8\u610f\u529b\u867d\u80fd\u964d\u4f4e\u8ba1\u7b97\u8d1f\u62c5\uff0c\u4f46\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u56e0\u65e0\u6cd5\u6355\u6349\u957f\u8ddd\u79bb\u4f9d\u8d56\u800c\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u63d0\u51faLASAD\u673a\u5236\uff0c\u57fa\u4e8e\u771f\u5b9e2D\u7a7a\u95f4\u4f4d\u7f6e\u8ba1\u7b97\u4f4d\u7f6e\u4f9d\u8d56\u7684\u8870\u51cf\u56e0\u5b50\uff0c\u4fdd\u7559\u7a7a\u95f4\u5173\u7cfb\u3002\u57fa\u4e8e\u6b64\u6784\u5efaLASADGen\u6a21\u578b\uff0c\u5b9e\u73b0\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u9009\u62e9\u6027\u7a7a\u95f4\u4e0a\u4e0b\u6587\u6ce8\u610f\u529b\u3002", "result": "\u5728ImageNet\u4e0a\uff0cLASADGen\u5728\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u8fbe\u5230\u6700\u4f18\u6c34\u5e73\u3002", "conclusion": "LASADGen\u901a\u8fc7\u7ed3\u5408\u7ebf\u6027\u6ce8\u610f\u529b\u7684\u9ad8\u6548\u6027\u548c\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.01653", "pdf": "https://arxiv.org/pdf/2507.01653", "abs": "https://arxiv.org/abs/2507.01653", "authors": ["Yuran Wang", "Yingping Liang", "Yutao Hu", "Ying Fu"], "title": "RobuSTereo: Robust Zero-Shot Stereo Matching under Adverse Weather", "categories": ["cs.CV"], "comment": "accepted by ICCV25", "summary": "Learning-based stereo matching models struggle in adverse weather conditions due to the scarcity of corresponding training data and the challenges in extracting discriminative features from degraded images. These limitations significantly hinder zero-shot generalization to out-of-distribution weather conditions. In this paper, we propose \\textbf{RobuSTereo}, a novel framework that enhances the zero-shot generalization of stereo matching models under adverse weather by addressing both data scarcity and feature extraction challenges. First, we introduce a diffusion-based simulation pipeline with a stereo consistency module, which generates high-quality stereo data tailored for adverse conditions. By training stereo matching models on our synthetic datasets, we reduce the domain gap between clean and degraded images, significantly improving the models' robustness to unseen weather conditions. The stereo consistency module ensures structural alignment across synthesized image pairs, preserving geometric integrity and enhancing depth estimation accuracy. Second, we design a robust feature encoder that combines a specialized ConvNet with a denoising transformer to extract stable and reliable features from degraded images. The ConvNet captures fine-grained local structures, while the denoising transformer refines global representations, effectively mitigating the impact of noise, low visibility, and weather-induced distortions. This enables more accurate disparity estimation even under challenging visual conditions. Extensive experiments demonstrate that \\textbf{RobuSTereo} significantly improves the robustness and generalization of stereo matching models across diverse adverse weather scenarios.", "AI": {"tldr": "RobuSTereo\u6846\u67b6\u901a\u8fc7\u6269\u6563\u6a21\u62df\u548c\u7a33\u5065\u7279\u5f81\u7f16\u7801\uff0c\u63d0\u5347\u7acb\u4f53\u5339\u914d\u6a21\u578b\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7acb\u4f53\u5339\u914d\u6a21\u578b\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u6570\u636e\u7a00\u7f3a\u548c\u7279\u5f81\u63d0\u53d6\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u6269\u6563\u6a21\u62df\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5e76\u8bbe\u8ba1\u7a33\u5065\u7279\u5f81\u7f16\u7801\u5668\uff08ConvNet+\u53bb\u566aTransformer\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRobuSTereo\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RobuSTereo\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u7279\u5f81\u63d0\u53d6\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7acb\u4f53\u5339\u914d\u6a21\u578b\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.01712", "pdf": "https://arxiv.org/pdf/2507.01712", "abs": "https://arxiv.org/abs/2507.01712", "authors": ["Xinle Tian", "Matthew Nunes", "Emiko Dupont", "Shaunagh Downing", "Freddie Lichtenstein", "Matt Burns"], "title": "Using Wavelet Domain Fingerprints to Improve Source Camera Identification", "categories": ["cs.CV", "eess.IV", "stat.AP"], "comment": null, "summary": "Camera fingerprint detection plays a crucial role in source identification and image forensics, with wavelet denoising approaches proving to be particularly effective in extracting sensor pattern noise (SPN). In this article, we propose a modification to wavelet-based SPN extraction. Rather than constructing the fingerprint as an image, we introduce the notion of a wavelet domain fingerprint. This avoids the final inversion step of the denoising algorithm and allows fingerprint comparisons to be made directly in the wavelet domain. As such, our modification streamlines the extraction and comparison process. Experimental results on real-world datasets demonstrate that our method not only achieves higher detection accuracy but can also significantly improve processing speed.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u57df\u7684\u6307\u7eb9\u63d0\u53d6\u65b9\u6cd5\uff0c\u7b80\u5316\u4e86SPN\u63d0\u53d6\u548c\u6bd4\u8f83\u6d41\u7a0b\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u548c\u5904\u7406\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u5c0f\u6ce2\u53bb\u566a\u65b9\u6cd5\u5728\u63d0\u53d6\u4f20\u611f\u5668\u6a21\u5f0f\u566a\u58f0\uff08SPN\uff09\u65f6\u9700\u8981\u56fe\u50cf\u91cd\u5efa\u6b65\u9aa4\uff0c\u6548\u7387\u8f83\u4f4e\u3002", "method": "\u63d0\u51fa\u5c0f\u6ce2\u57df\u6307\u7eb9\u6982\u5ff5\uff0c\u76f4\u63a5\u5728\u9891\u57df\u8fdb\u884c\u6307\u7eb9\u63d0\u53d6\u548c\u6bd4\u8f83\uff0c\u7701\u53bb\u53cd\u6f14\u6b65\u9aa4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u68c0\u6d4b\u7cbe\u5ea6\u66f4\u9ad8\uff0c\u5904\u7406\u901f\u5ea6\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u5c0f\u6ce2\u57df\u6307\u7eb9\u65b9\u6cd5\u4f18\u5316\u4e86SPN\u63d0\u53d6\u6d41\u7a0b\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2507.01744", "pdf": "https://arxiv.org/pdf/2507.01744", "abs": "https://arxiv.org/abs/2507.01744", "authors": ["Benjamin Jin", "Grant Mair", "Joanna M. Wardlaw", "Maria del C. Vald\u00e9s Hern\u00e1ndez"], "title": "Calibrated Self-supervised Vision Transformers Improve Intracranial Arterial Calcification Segmentation from Clinical CT Head Scans", "categories": ["cs.CV"], "comment": null, "summary": "Vision Transformers (ViTs) have gained significant popularity in the natural image domain but have been less successful in 3D medical image segmentation. Nevertheless, 3D ViTs are particularly interesting for large medical imaging volumes due to their efficient self-supervised training within the masked autoencoder (MAE) framework, which enables the use of imaging data without the need for expensive manual annotations. intracranial arterial calcification (IAC) is an imaging biomarker visible on routinely acquired CT scans linked to neurovascular diseases such as stroke and dementia, and automated IAC quantification could enable their large-scale risk assessment. We pre-train ViTs with MAE and fine-tune them for IAC segmentation for the first time. To develop our models, we use highly heterogeneous data from a large clinical trial, the third International Stroke Trial (IST-3). We evaluate key aspects of MAE pre-trained ViTs in IAC segmentation, and analyse the clinical implications. We show: 1) our calibrated self-supervised ViT beats a strong supervised nnU-Net baseline by 3.2 Dice points, 2) low patch sizes are crucial for ViTs for IAC segmentation and interpolation upsampling with regular convolutions is preferable to transposed convolutions for ViT-based models, and 3) our ViTs increase robustness to higher slice thicknesses and improve risk group classification in a clinical scenario by 46%. Our code is available online.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57283D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u4f7f\u7528Vision Transformers\uff08ViTs\uff09\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u9488\u5bf9\u9885\u5185\u52a8\u8109\u9499\u5316\uff08IAC\uff09\u7684\u81ea\u52a8\u91cf\u5316\u3002\u901a\u8fc7\u81ea\u76d1\u7763\u7684MAE\u6846\u67b6\u9884\u8bad\u7ec3ViTs\uff0c\u5e76\u5728IST-3\u4e34\u5e8a\u8bd5\u9a8c\u6570\u636e\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u53d6\u5f97\u4e86\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1ViTs\u5728\u81ea\u7136\u56fe\u50cf\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u57283D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u5e94\u7528\u8f83\u5c11\u3002IAC\u662f\u4e00\u79cd\u4e0e\u4e2d\u98ce\u548c\u75f4\u5446\u76f8\u5173\u7684\u751f\u7269\u6807\u5fd7\u7269\uff0c\u81ea\u52a8\u91cf\u5316IAC\u6709\u52a9\u4e8e\u5927\u89c4\u6a21\u98ce\u9669\u8bc4\u4f30\u3002", "method": "\u91c7\u7528MAE\u6846\u67b6\u5bf9ViTs\u8fdb\u884c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u5e76\u5728IST-3\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u91cd\u70b9\u7814\u7a76\u4e86ViTs\u5728IAC\u5206\u5272\u4e2d\u7684\u5173\u952e\u56e0\u7d20\uff08\u5982patch\u5927\u5c0f\u548c\u4e0a\u91c7\u6837\u65b9\u6cd5\uff09\u3002", "result": "1\uff09\u81ea\u76d1\u7763ViT\u6bd4\u76d1\u7763nnU-Net\u57fa\u7ebf\u9ad83.2 Dice\u5206\u6570\uff1b2\uff09\u5c0fpatch\u5c3a\u5bf8\u5bf9IAC\u5206\u5272\u81f3\u5173\u91cd\u8981\uff0c\u63d2\u503c\u4e0a\u91c7\u6837\u4f18\u4e8e\u8f6c\u7f6e\u5377\u79ef\uff1b3\uff09ViT\u5bf9\u9ad8\u5207\u7247\u539a\u5ea6\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u4e34\u5e8a\u98ce\u9669\u5206\u7c7b\u63d0\u534746%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u81ea\u76d1\u7763ViTs\u57283D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5c24\u5176\u662f\u5bf9IAC\u7684\u81ea\u52a8\u91cf\u5316\uff0c\u4e3a\u4e34\u5e8a\u98ce\u9669\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2507.01756", "pdf": "https://arxiv.org/pdf/2507.01756", "abs": "https://arxiv.org/abs/2507.01756", "authors": ["Peng Zheng", "Junke Wang", "Yi Chang", "Yizhou Yu", "Rui Ma", "Zuxuan Wu"], "title": "Rethinking Discrete Tokens: Treating Them as Conditions for Continuous Autoregressive Image Synthesis", "categories": ["cs.CV"], "comment": "accepted by iccv 2025", "summary": "Recent advances in large language models (LLMs) have spurred interests in encoding images as discrete tokens and leveraging autoregressive (AR) frameworks for visual generation. However, the quantization process in AR-based visual generation models inherently introduces information loss that degrades image fidelity. To mitigate this limitation, recent studies have explored to autoregressively predict continuous tokens. Unlike discrete tokens that reside in a structured and bounded space, continuous representations exist in an unbounded, high-dimensional space, making density estimation more challenging and increasing the risk of generating out-of-distribution artifacts. Based on the above findings, this work introduces DisCon (Discrete-Conditioned Continuous Autoregressive Model), a novel framework that reinterprets discrete tokens as conditional signals rather than generation targets. By modeling the conditional probability of continuous representations conditioned on discrete tokens, DisCon circumvents the optimization challenges of continuous token modeling while avoiding the information loss caused by quantization. DisCon achieves a gFID score of 1.38 on ImageNet 256$\\times$256 generation, outperforming state-of-the-art autoregressive approaches by a clear margin.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDisCon\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u79bb\u6563\u6807\u8bb0\u4f5c\u4e3a\u6761\u4ef6\u4fe1\u53f7\u800c\u975e\u751f\u6210\u76ee\u6807\uff0c\u89e3\u51b3\u4e86\u8fde\u7eed\u6807\u8bb0\u5efa\u6a21\u7684\u4f18\u5316\u6311\u6218\uff0c\u540c\u65f6\u907f\u514d\u4e86\u91cf\u5316\u5e26\u6765\u7684\u4fe1\u606f\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u81ea\u56de\u5f52\u7684\u89c6\u89c9\u751f\u6210\u6a21\u578b\u56e0\u91cf\u5316\u8fc7\u7a0b\u5bfc\u81f4\u4fe1\u606f\u635f\u5931\uff0c\u5f71\u54cd\u56fe\u50cf\u4fdd\u771f\u5ea6\u3002\u8fde\u7eed\u6807\u8bb0\u867d\u80fd\u907f\u514d\u91cf\u5316\u95ee\u9898\uff0c\u4f46\u56e0\u5176\u9ad8\u7ef4\u65e0\u754c\u7279\u6027\uff0c\u589e\u52a0\u4e86\u5bc6\u5ea6\u4f30\u8ba1\u96be\u5ea6\u548c\u751f\u6210\u5f02\u5e38\u7684\u98ce\u9669\u3002", "method": "\u63d0\u51faDisCon\u6846\u67b6\uff0c\u5c06\u79bb\u6563\u6807\u8bb0\u4f5c\u4e3a\u6761\u4ef6\u4fe1\u53f7\uff0c\u5efa\u6a21\u8fde\u7eed\u8868\u793a\u7684\u6761\u4ef6\u6982\u7387\uff0c\u907f\u514d\u76f4\u63a5\u5efa\u6a21\u8fde\u7eed\u6807\u8bb0\u7684\u6311\u6218\u3002", "result": "\u5728ImageNet 256\u00d7256\u751f\u6210\u4efb\u52a1\u4e2d\uff0cDisCon\u7684gFID\u5f97\u5206\u4e3a1.38\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u81ea\u56de\u5f52\u65b9\u6cd5\u3002", "conclusion": "DisCon\u901a\u8fc7\u7ed3\u5408\u79bb\u6563\u548c\u8fde\u7eed\u6807\u8bb0\u7684\u4f18\u52bf\uff0c\u6709\u6548\u63d0\u5347\u4e86\u56fe\u50cf\u751f\u6210\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2507.01792", "pdf": "https://arxiv.org/pdf/2507.01792", "abs": "https://arxiv.org/abs/2507.01792", "authors": ["Peng Zheng", "Ye Wang", "Rui Ma", "Zuxuan Wu"], "title": "FreeLoRA: Enabling Training-Free LoRA Fusion for Autoregressive Multi-Subject Personalization", "categories": ["cs.CV"], "comment": null, "summary": "Subject-driven image generation plays a crucial role in applications such as virtual try-on and poster design. Existing approaches typically fine-tune pretrained generative models or apply LoRA-based adaptations for individual subjects. However, these methods struggle with multi-subject personalization, as combining independently adapted modules often requires complex re-tuning or joint optimization. We present FreeLoRA, a simple and generalizable framework that enables training-free fusion of subject-specific LoRA modules for multi-subject personalization. Each LoRA module is adapted on a few images of a specific subject using a Full Token Tuning strategy, where it is applied across all tokens in the prompt to encourage weakly supervised token-content alignment. At inference, we adopt Subject-Aware Inference, activating each module only on its corresponding subject tokens. This enables training-free fusion of multiple personalized subjects within a single image, while mitigating overfitting and mutual interference between subjects. Extensive experiments show that FreeLoRA achieves strong performance in both subject fidelity and prompt consistency.", "AI": {"tldr": "FreeLoRA\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u591a\u4e2a\u7279\u5b9a\u4e3b\u9898\u7684LoRA\u6a21\u5757\u5b9e\u73b0\u591a\u4e3b\u9898\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u4e3b\u9898\u4e2a\u6027\u5316\u751f\u6210\u65f6\u9700\u590d\u6742\u8c03\u6574\u6216\u8054\u5408\u4f18\u5316\uff0cFreeLoRA\u65e8\u5728\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528Full Token Tuning\u7b56\u7565\u8bad\u7ec3\u6bcf\u4e2a\u4e3b\u9898\u7684LoRA\u6a21\u5757\uff0c\u5e76\u901a\u8fc7Subject-Aware Inference\u5728\u63a8\u7406\u65f6\u6fc0\u6d3b\u5bf9\u5e94\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFreeLoRA\u5728\u4e3b\u9898\u4fdd\u771f\u5ea6\u548c\u63d0\u793a\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "FreeLoRA\u4e3a\u591a\u4e3b\u9898\u4e2a\u6027\u5316\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01838", "pdf": "https://arxiv.org/pdf/2507.01838", "abs": "https://arxiv.org/abs/2507.01838", "authors": ["Hailong Yan", "Ao Li", "Xiangtao Zhang", "Zhe Liu", "Zenglin Shi", "Ce Zhu", "Le Zhang"], "title": "MobileIE: An Extremely Lightweight and Effective ConvNet for Real-Time Image Enhancement on Mobile Devices", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Recent advancements in deep neural networks have driven significant progress in image enhancement (IE). However, deploying deep learning models on resource-constrained platforms, such as mobile devices, remains challenging due to high computation and memory demands. To address these challenges and facilitate real-time IE on mobile, we introduce an extremely lightweight Convolutional Neural Network (CNN) framework with around 4K parameters. Our approach integrates reparameterization with an Incremental Weight Optimization strategy to ensure efficiency. Additionally, we enhance performance with a Feature Self-Transform module and a Hierarchical Dual-Path Attention mechanism, optimized with a Local Variance-Weighted loss. With this efficient framework, we are the first to achieve real-time IE inference at up to 1,100 frames per second (FPS) while delivering competitive image quality, achieving the best trade-off between speed and performance across multiple IE tasks. The code will be available at https://github.com/AVC2-UESTC/MobileIE.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6781\u8f7b\u91cf\u7ea7\u7684CNN\u6846\u67b6\uff0c\u7528\u4e8e\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u56fe\u50cf\u589e\u5f3a\uff0c\u901a\u8fc7\u91cd\u53c2\u6570\u5316\u548c\u589e\u91cf\u6743\u91cd\u4f18\u5316\u7b56\u7565\u5b9e\u73b0\u9ad8\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u90e8\u7f72\u7684\u9ad8\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u91cd\u53c2\u6570\u5316\u548c\u589e\u91cf\u6743\u91cd\u4f18\u5316\u7b56\u7565\uff0c\u5f15\u5165\u7279\u5f81\u81ea\u53d8\u6362\u6a21\u5757\u548c\u5206\u5c42\u53cc\u8def\u5f84\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u4f7f\u7528\u5c40\u90e8\u65b9\u5dee\u52a0\u6743\u635f\u5931\u4f18\u5316\u6027\u80fd\u3002", "result": "\u9996\u6b21\u5b9e\u73b0\u9ad8\u8fbe1,100 FPS\u7684\u5b9e\u65f6\u56fe\u50cf\u589e\u5f3a\u63a8\u7406\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u53d6\u5f97\u901f\u5ea6\u548c\u6027\u80fd\u7684\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u56fe\u50cf\u589e\u5f3a\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.01908", "pdf": "https://arxiv.org/pdf/2507.01908", "abs": "https://arxiv.org/abs/2507.01908", "authors": ["Qingdong He", "Xueqin Chen", "Chaoyi Wang", "Yanjie Pan", "Xiaobin Hu", "Zhenye Gan", "Yabiao Wang", "Chengjie Wang", "Xiangtai Li", "Jiangning Zhang"], "title": "Reasoning to Edit: Hypothetical Instruction-Based Image Editing with Visual Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Instruction-based image editing (IIE) has advanced rapidly with the success of diffusion models. However, existing efforts primarily focus on simple and explicit instructions to execute editing operations such as adding, deleting, moving, or swapping objects. They struggle to handle more complex implicit hypothetical instructions that require deeper reasoning to infer plausible visual changes and user intent. Additionally, current datasets provide limited support for training and evaluating reasoning-aware editing capabilities. Architecturally, these methods also lack mechanisms for fine-grained detail extraction that support such reasoning. To address these limitations, we propose Reason50K, a large-scale dataset specifically curated for training and evaluating hypothetical instruction reasoning image editing, along with ReasonBrain, a novel framework designed to reason over and execute implicit hypothetical instructions across diverse scenarios. Reason50K includes over 50K samples spanning four key reasoning scenarios: Physical, Temporal, Causal, and Story reasoning. ReasonBrain leverages Multimodal Large Language Models (MLLMs) for editing guidance generation and a diffusion model for image synthesis, incorporating a Fine-grained Reasoning Cue Extraction (FRCE) module to capture detailed visual and textual semantics essential for supporting instruction reasoning. To mitigate the semantic loss, we further introduce a Cross-Modal Enhancer (CME) that enables rich interactions between the fine-grained cues and MLLM-derived features. Extensive experiments demonstrate that ReasonBrain consistently outperforms state-of-the-art baselines on reasoning scenarios while exhibiting strong zero-shot generalization to conventional IIE tasks. Our dataset and code will be released publicly.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faReason50K\u6570\u636e\u96c6\u548cReasonBrain\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u590d\u6742\u7684\u9690\u542b\u5047\u8bbe\u6307\u4ee4\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u63a8\u7406\u548c\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u7684\u9690\u542b\u5047\u8bbe\u6307\u4ee4\uff0c\u4e14\u7f3a\u4e4f\u652f\u6301\u63a8\u7406\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51faReason50K\u6570\u636e\u96c6\u548cReasonBrain\u6846\u67b6\uff0c\u7ed3\u5408MLLM\u548c\u6269\u6563\u6a21\u578b\uff0c\u5e76\u5f15\u5165FRCE\u6a21\u5757\u548cCME\u589e\u5f3a\u5668\u3002", "result": "ReasonBrain\u5728\u63a8\u7406\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5177\u6709\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Reason50K\u548cReasonBrain\u4e3a\u590d\u6742\u6307\u4ee4\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2507.01945", "pdf": "https://arxiv.org/pdf/2507.01945", "abs": "https://arxiv.org/abs/2507.01945", "authors": ["Nan Chen", "Mengqi Huang", "Yihao Meng", "Zhendong Mao"], "title": "LongAnimation: Long Animation Generation with Dynamic Global-Local Memory", "categories": ["cs.CV"], "comment": null, "summary": "Animation colorization is a crucial part of real animation industry production. Long animation colorization has high labor costs. Therefore, automated long animation colorization based on the video generation model has significant research value. Existing studies are limited to short-term colorization. These studies adopt a local paradigm, fusing overlapping features to achieve smooth transitions between local segments. However, the local paradigm neglects global information, failing to maintain long-term color consistency. In this study, we argue that ideal long-term color consistency can be achieved through a dynamic global-local paradigm, i.e., dynamically extracting global color-consistent features relevant to the current generation. Specifically, we propose LongAnimation, a novel framework, which mainly includes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color Consistency Reward. The SketchDiT captures hybrid reference features to support the DGLM module. The DGLM module employs a long video understanding model to dynamically compress global historical features and adaptively fuse them with the current generation features. To refine the color consistency, we introduce a Color Consistency Reward. During inference, we propose a color consistency fusion to smooth the video segment transition. Extensive experiments on both short-term (14 frames) and long-term (average 500 frames) animations show the effectiveness of LongAnimation in maintaining short-term and long-term color consistency for open-domain animation colorization task. The code can be found at https://cn-makers.github.io/long_animation_web/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLongAnimation\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u957f\u52a8\u753b\u7740\u8272\u4e2d\u7684\u989c\u8272\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u7ed3\u5408\u52a8\u6001\u5168\u5c40-\u5c40\u90e8\u8bb0\u5fc6\u6a21\u5757\u548c\u989c\u8272\u4e00\u81f4\u6027\u5956\u52b1\u3002", "motivation": "\u957f\u52a8\u753b\u7740\u8272\u5728\u52a8\u753b\u4ea7\u4e1a\u4e2d\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u9002\u7528\u4e8e\u77ed\u7247\u6bb5\u4e14\u5ffd\u89c6\u5168\u5c40\u4fe1\u606f\uff0c\u5bfc\u81f4\u957f\u671f\u989c\u8272\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51faLongAnimation\u6846\u67b6\uff0c\u5305\u62ecSketchDiT\u3001\u52a8\u6001\u5168\u5c40-\u5c40\u90e8\u8bb0\u5fc6\u6a21\u5757\uff08DGLM\uff09\u548c\u989c\u8272\u4e00\u81f4\u6027\u5956\u52b1\uff0c\u52a8\u6001\u63d0\u53d6\u5168\u5c40\u7279\u5f81\u5e76\u878d\u5408\u5c40\u90e8\u4fe1\u606f\u3002", "result": "\u5728\u77ed\u7247\u6bb5\uff0814\u5e27\uff09\u548c\u957f\u7247\u6bb5\uff08\u5e73\u5747500\u5e27\uff09\u52a8\u753b\u4e0a\u9a8c\u8bc1\u4e86LongAnimation\u5728\u4fdd\u6301\u989c\u8272\u4e00\u81f4\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "LongAnimation\u901a\u8fc7\u52a8\u6001\u5168\u5c40-\u5c40\u90e8\u8303\u5f0f\u663e\u8457\u63d0\u5347\u4e86\u957f\u52a8\u753b\u7740\u8272\u7684\u989c\u8272\u4e00\u81f4\u6027\uff0c\u9002\u7528\u4e8e\u5f00\u653e\u9886\u57df\u4efb\u52a1\u3002"}}
{"id": "2507.01953", "pdf": "https://arxiv.org/pdf/2507.01953", "abs": "https://arxiv.org/abs/2507.01953", "authors": ["Yukang Cao", "Chenyang Si", "Jinghao Wang", "Ziwei Liu"], "title": "FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model", "categories": ["cs.CV"], "comment": "ICCV 2025. Project page: https://yukangcao.github.io/FreeMorph/", "summary": "We present FreeMorph, the first tuning-free method for image morphing that accommodates inputs with different semantics or layouts. Unlike existing methods that rely on finetuning pre-trained diffusion models and are limited by time constraints and semantic/layout discrepancies, FreeMorph delivers high-fidelity image morphing without requiring per-instance training. Despite their efficiency and potential, tuning-free methods face challenges in maintaining high-quality results due to the non-linear nature of the multi-step denoising process and biases inherited from the pre-trained diffusion model. In this paper, we introduce FreeMorph to address these challenges by integrating two key innovations. 1) We first propose a guidance-aware spherical interpolation design that incorporates explicit guidance from the input images by modifying the self-attention modules, thereby addressing identity loss and ensuring directional transitions throughout the generated sequence. 2) We further introduce a step-oriented variation trend that blends self-attention modules derived from each input image to achieve controlled and consistent transitions that respect both inputs. Our extensive evaluations demonstrate that FreeMorph outperforms existing methods, being 10x ~ 50x faster and establishing a new state-of-the-art for image morphing.", "AI": {"tldr": "FreeMorph\u662f\u4e00\u79cd\u65e0\u9700\u8c03\u4f18\u7684\u56fe\u50cf\u53d8\u5f62\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u8bed\u4e49\u6216\u5e03\u5c40\u7684\u8f93\u5165\uff0c\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u5fae\u8c03\uff0c\u53d7\u9650\u4e8e\u65f6\u95f4\u548c\u8bed\u4e49/\u5e03\u5c40\u5dee\u5f02\uff0cFreeMorph\u65e8\u5728\u65e0\u9700\u5b9e\u4f8b\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u56fe\u50cf\u53d8\u5f62\u3002", "method": "1) \u63d0\u51fa\u57fa\u4e8e\u5f15\u5bfc\u7684\u7403\u5f62\u63d2\u503c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u4fee\u6539\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u89e3\u51b3\u8eab\u4efd\u4e22\u5931\u95ee\u9898\uff1b2) \u5f15\u5165\u6b65\u957f\u5bfc\u5411\u7684\u53d8\u5206\u8d8b\u52bf\uff0c\u6df7\u5408\u8f93\u5165\u56fe\u50cf\u7684\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u4ee5\u5b9e\u73b0\u53ef\u63a7\u8fc7\u6e21\u3002", "result": "FreeMorph\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u901f\u5ea6\u5feb10~50\u500d\uff0c\u6210\u4e3a\u56fe\u50cf\u53d8\u5f62\u7684\u65b0\u6807\u6746\u3002", "conclusion": "FreeMorph\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u89e3\u51b3\u4e86\u8c03\u4f18\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u53d8\u5f62\u3002"}}
{"id": "2507.01955", "pdf": "https://arxiv.org/pdf/2507.01955", "abs": "https://arxiv.org/abs/2507.01955", "authors": ["Rahul Ramachandran", "Ali Garjani", "Roman Bachmann", "Andrei Atanov", "O\u011fuzhan Fatih Kar", "Amir Zamir"], "title": "How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project page at https://fm-vision-evals.epfl.ch/", "summary": "Multimodal foundation models, such as GPT-4o, have recently made remarkable progress, but it is not clear where exactly these models stand in terms of understanding vision. In this paper, we benchmark the performance of popular multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0 Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision tasks (semantic segmentation, object detection, image classification, depth and surface normal prediction) using established datasets (e.g., COCO, ImageNet and its variants, etc).   The main challenges to performing this are: 1) most models are trained to output text and cannot natively express versatile domains, such as segments or 3D geometry, and 2) many leading models are proprietary and accessible only at an API level, i.e., there is no weight access to adapt them. We address these challenges by translating standard vision tasks into equivalent text-promptable and API-compatible tasks via prompt chaining to create a standardized benchmarking framework.   We observe that 1) the models are not close to the state-of-the-art specialist models at any task. However, 2) they are respectable generalists; this is remarkable as they are presumably trained on primarily image-text-based tasks. 3) They perform semantic tasks notably better than geometric ones. 4) While the prompt-chaining techniques affect performance, better models exhibit less sensitivity to prompt variations. 5) GPT-4o performs the best among non-reasoning models, securing the top position in 4 out of 6 tasks, 6) reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a preliminary analysis of models with native image generation, like the latest GPT-4o, shows they exhibit quirks like hallucinations and spatial misalignments.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u6807\u51c6\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5b83\u4eec\u867d\u4e0d\u53ca\u4e13\u4e1a\u6a21\u578b\uff0c\u4f46\u4f5c\u4e3a\u901a\u7528\u6a21\u578b\u8868\u73b0\u5c1a\u53ef\uff0c\u4e14\u5728\u8bed\u4e49\u4efb\u52a1\u4e0a\u4f18\u4e8e\u51e0\u4f55\u4efb\u52a1\u3002GPT-4o\u5728\u975e\u63a8\u7406\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u63a2\u8ba8\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u7684\u5b9e\u9645\u80fd\u529b\uff0c\u5e76\u89e3\u51b3\u5176\u65e0\u6cd5\u76f4\u63a5\u8868\u8fbe\u590d\u6742\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u5206\u5272\u62163D\u51e0\u4f55\uff09\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u63d0\u793a\u94fe\u5c06\u6807\u51c6\u89c6\u89c9\u4efb\u52a1\u8f6c\u5316\u4e3a\u6587\u672c\u53ef\u63d0\u793a\u548cAPI\u517c\u5bb9\u7684\u4efb\u52a1\uff0c\u5efa\u7acb\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u6a21\u578b\u5728\u8bed\u4e49\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u51e0\u4f55\u4efb\u52a1\uff0cGPT-4o\u5728\u975e\u63a8\u7406\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u63a8\u7406\u6a21\u578b\u5728\u51e0\u4f55\u4efb\u52a1\u4e0a\u6709\u6539\u8fdb\u3002", "conclusion": "\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u5c24\u5176\u662f\u5728\u51e0\u4f55\u4efb\u52a1\u548c\u51cf\u5c11\u63d0\u793a\u654f\u611f\u6027\u65b9\u9762\u3002"}}
{"id": "2507.01957", "pdf": "https://arxiv.org/pdf/2507.01957", "abs": "https://arxiv.org/abs/2507.01957", "authors": ["Zhuoyang Zhang", "Luke J. Huang", "Chengyue Wu", "Shang Yang", "Kelly Peng", "Yao Lu", "Song Han"], "title": "Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": "The first two authors contributed equally to this work", "summary": "We present Locality-aware Parallel Decoding (LPD) to accelerate autoregressive image generation. Traditional autoregressive image generation relies on next-patch prediction, a memory-bound process that leads to high latency. Existing works have tried to parallelize next-patch prediction by shifting to multi-patch prediction to accelerate the process, but only achieved limited parallelization. To achieve high parallelization while maintaining generation quality, we introduce two key techniques: (1) Flexible Parallelized Autoregressive Modeling, a novel architecture that enables arbitrary generation ordering and degrees of parallelization. It uses learnable position query tokens to guide generation at target positions while ensuring mutual visibility among concurrently generated tokens for consistent parallel decoding. (2) Locality-aware Generation Ordering, a novel schedule that forms groups to minimize intra-group dependencies and maximize contextual support, enhancing generation quality. With these designs, we reduce the generation steps from 256 to 20 (256$\\times$256 res.) and 1024 to 48 (512$\\times$512 res.) without compromising quality on the ImageNet class-conditional generation, and achieving at least 3.4$\\times$ lower latency than previous parallelized autoregressive models.", "AI": {"tldr": "Locality-aware Parallel Decoding (LPD) \u901a\u8fc7\u7075\u6d3b\u5e76\u884c\u81ea\u56de\u5f52\u5efa\u6a21\u548c\u5c40\u90e8\u611f\u77e5\u751f\u6210\u987a\u5e8f\uff0c\u663e\u8457\u52a0\u901f\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\uff0c\u51cf\u5c11\u751f\u6210\u6b65\u9aa4\u5e76\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u4f20\u7edf\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u4f9d\u8d56\u9010\u5757\u9884\u6d4b\uff0c\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u3002\u73b0\u6709\u65b9\u6cd5\u5c1d\u8bd5\u591a\u5757\u9884\u6d4b\u5e76\u884c\u5316\uff0c\u4f46\u6548\u679c\u6709\u9650\u3002", "method": "\u5f15\u5165\u7075\u6d3b\u5e76\u884c\u81ea\u56de\u5f52\u5efa\u6a21\uff08\u652f\u6301\u4efb\u610f\u751f\u6210\u987a\u5e8f\u548c\u5e76\u884c\u5ea6\uff09\u548c\u5c40\u90e8\u611f\u77e5\u751f\u6210\u987a\u5e8f\uff08\u6700\u5c0f\u5316\u7ec4\u5185\u4f9d\u8d56\uff0c\u6700\u5927\u5316\u4e0a\u4e0b\u6587\u652f\u6301\uff09\u3002", "result": "\u5728 ImageNet \u7c7b\u6761\u4ef6\u751f\u6210\u4e2d\uff0c\u751f\u6210\u6b65\u9aa4\u4ece 256 \u51cf\u5c11\u5230 20\uff08256\u00d7256 \u5206\u8fa8\u7387\uff09\uff0c1024 \u51cf\u5c11\u5230 48\uff08512\u00d7512 \u5206\u8fa8\u7387\uff09\uff0c\u5ef6\u8fdf\u964d\u4f4e\u81f3\u5c11 3.4 \u500d\u3002", "conclusion": "LPD \u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e76\u884c\u5316\u548c\u751f\u6210\u6548\u7387\u3002"}}
{"id": "2507.01055", "pdf": "https://arxiv.org/pdf/2507.01055", "abs": "https://arxiv.org/abs/2507.01055", "authors": ["Hao Yang", "Xinlong Liang", "Zhang Li", "Yue Sun", "Zheyu Hu", "Xinghe Xie", "Behdad Dashtbozorg", "Jincheng Huang", "Shiwei Zhu", "Luyi Han", "Jiong Zhang", "Shanshan Wang", "Ritse Mann", "Qifeng Yu", "Tao Tan"], "title": "Prompt Mechanisms in Medical Imaging: A Comprehensive Survey", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Deep learning offers transformative potential in medical imaging, yet its clinical adoption is frequently hampered by challenges such as data scarcity, distribution shifts, and the need for robust task generalization. Prompt-based methodologies have emerged as a pivotal strategy to guide deep learning models, providing flexible, domain-specific adaptations that significantly enhance model performance and adaptability without extensive retraining. This systematic review critically examines the burgeoning landscape of prompt engineering in medical imaging. We dissect diverse prompt modalities, including textual instructions, visual prompts, and learnable embeddings, and analyze their integration for core tasks such as image generation, segmentation, and classification. Our synthesis reveals how these mechanisms improve task-specific outcomes by enhancing accuracy, robustness, and data efficiency and reducing reliance on manual feature engineering while fostering greater model interpretability by making the model's guidance explicit. Despite substantial advancements, we identify persistent challenges, particularly in prompt design optimization, data heterogeneity, and ensuring scalability for clinical deployment. Finally, this review outlines promising future trajectories, including advanced multimodal prompting and robust clinical integration, underscoring the critical role of prompt-driven AI in accelerating the revolution of diagnostics and personalized treatment planning in medicine.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u63d0\u793a\u5de5\u7a0b\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u5e94\u7528\uff0c\u63a2\u8ba8\u4e86\u5176\u5982\u4f55\u901a\u8fc7\u591a\u6a21\u6001\u63d0\u793a\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u3001\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u4f18\u5316\u8bbe\u8ba1\u548c\u4e34\u5e8a\u90e8\u7f72\u7684\u6311\u6218\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u3001\u5206\u5e03\u504f\u79fb\u548c\u4efb\u52a1\u6cdb\u5316\u7b49\u6311\u6218\u3002\u63d0\u793a\u65b9\u6cd5\u4e3a\u6a21\u578b\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u9886\u57df\u7279\u5b9a\u7684\u6307\u5bfc\uff0c\u6709\u671b\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u4e86\u6587\u672c\u6307\u4ee4\u3001\u89c6\u89c9\u63d0\u793a\u548c\u53ef\u5b66\u4e60\u5d4c\u5165\u7b49\u591a\u6a21\u6001\u63d0\u793a\u65b9\u6cd5\uff0c\u53ca\u5176\u5728\u56fe\u50cf\u751f\u6210\u3001\u5206\u5272\u548c\u5206\u7c7b\u7b49\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u63d0\u793a\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387\uff0c\u51cf\u5c11\u4e86\u624b\u52a8\u7279\u5f81\u5de5\u7a0b\u9700\u6c42\uff0c\u5e76\u589e\u5f3a\u4e86\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u5c3d\u7ba1\u8fdb\u5c55\u663e\u8457\uff0c\u63d0\u793a\u8bbe\u8ba1\u4f18\u5316\u3001\u6570\u636e\u5f02\u8d28\u6027\u548c\u4e34\u5e8a\u90e8\u7f72\u7684\u6269\u5c55\u6027\u4ecd\u662f\u6311\u6218\u3002\u672a\u6765\u65b9\u5411\u5305\u62ec\u591a\u6a21\u6001\u63d0\u793a\u548c\u4e34\u5e8a\u6574\u5408\uff0c\u4ee5\u63a8\u52a8\u533b\u5b66\u8bca\u65ad\u548c\u4e2a\u6027\u5316\u6cbb\u7597\u7684\u9769\u547d\u3002"}}
{"id": "2507.01387", "pdf": "https://arxiv.org/pdf/2507.01387", "abs": "https://arxiv.org/abs/2507.01387", "authors": ["Ahmad Soliman", "Ron Keuth", "Marian Himstedt"], "title": "BronchoGAN: Anatomically consistent and domain-agnostic image-to-image translation for video bronchoscopy", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The limited availability of bronchoscopy images makes image synthesis particularly interesting for training deep learning models. Robust image translation across different domains -- virtual bronchoscopy, phantom as well as in-vivo and ex-vivo image data -- is pivotal for clinical applications. This paper proposes BronchoGAN introducing anatomical constraints for image-to-image translation being integrated into a conditional GAN. In particular, we force bronchial orifices to match across input and output images. We further propose to use foundation model-generated depth images as intermediate representation ensuring robustness across a variety of input domains establishing models with substantially less reliance on individual training datasets. Moreover our intermediate depth image representation allows to easily construct paired image data for training. Our experiments showed that input images from different domains (e.g. virtual bronchoscopy, phantoms) can be successfully translated to images mimicking realistic human airway appearance. We demonstrated that anatomical settings (i.e. bronchial orifices) can be robustly preserved with our approach which is shown qualitatively and quantitatively by means of improved FID, SSIM and dice coefficients scores. Our anatomical constraints enabled an improvement in the Dice coefficient of up to 0.43 for synthetic images. Through foundation models for intermediate depth representations, bronchial orifice segmentation integrated as anatomical constraints into conditional GANs we are able to robustly translate images from different bronchoscopy input domains. BronchoGAN allows to incorporate public CT scan data (virtual bronchoscopy) in order to generate large-scale bronchoscopy image datasets with realistic appearance. BronchoGAN enables to bridge the gap of missing public bronchoscopy images.", "AI": {"tldr": "BronchoGAN\u5229\u7528\u89e3\u5256\u5b66\u7ea6\u675f\u548c\u4e2d\u95f4\u6df1\u5ea6\u56fe\u50cf\u8868\u793a\uff0c\u5b9e\u73b0\u8de8\u57df\u652f\u6c14\u7ba1\u955c\u56fe\u50cf\u5408\u6210\uff0c\u63d0\u5347\u751f\u6210\u56fe\u50cf\u7684\u89e3\u5256\u5b66\u51c6\u786e\u6027\u548c\u6570\u636e\u96c6\u89c4\u6a21\u3002", "motivation": "\u652f\u6c14\u7ba1\u955c\u56fe\u50cf\u6570\u636e\u6709\u9650\uff0c\u8de8\u57df\u56fe\u50cf\u8f6c\u6362\u5bf9\u4e34\u5e8a\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7ed3\u5408\u6761\u4ef6GAN\u548c\u89e3\u5256\u5b66\u7ea6\u675f\uff08\u652f\u6c14\u7ba1\u5b54\u5339\u914d\uff09\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u751f\u6210\u6df1\u5ea6\u56fe\u50cf\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u51cf\u5c11\u5bf9\u5355\u4e00\u6570\u636e\u96c6\u7684\u4f9d\u8d56\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cBronchoGAN\u80fd\u6210\u529f\u5c06\u865a\u62df\u652f\u6c14\u7ba1\u955c\u7b49\u56fe\u50cf\u8f6c\u6362\u4e3a\u903c\u771f\u7684\u4eba\u4f53\u6c14\u9053\u56fe\u50cf\uff0c\u89e3\u5256\u5b66\u7ed3\u6784\uff08\u5982\u652f\u6c14\u7ba1\u5b54\uff09\u4fdd\u7559\u826f\u597d\uff0cFID\u3001SSIM\u548cDice\u7cfb\u6570\u663e\u8457\u63d0\u5347\u3002", "conclusion": "BronchoGAN\u901a\u8fc7\u89e3\u5256\u5b66\u7ea6\u675f\u548c\u6df1\u5ea6\u8868\u793a\uff0c\u586b\u8865\u4e86\u516c\u5171\u652f\u6c14\u7ba1\u955c\u56fe\u50cf\u6570\u636e\u7684\u7a7a\u767d\uff0c\u4e3a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u751f\u6210\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01808", "pdf": "https://arxiv.org/pdf/2507.01808", "abs": "https://arxiv.org/abs/2507.01808", "authors": ["Xiaoyu Ji", "Jessica Shorland", "Joshua Shank", "Pascal Delpe-Brice", "Latanya Sweeney", "Jan Allebach", "Ali Shakouri"], "title": "Empowering Manufacturers with Privacy-Preserving AI Tools: A Case Study in Privacy-Preserving Machine Learning to Solve Real-World Problems", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.ET", "68T01, 68T05, 68T45, 94A60"], "comment": "20 pages, 11 figures, 30 references", "summary": "Small- and medium-sized manufacturers need innovative data tools but, because of competition and privacy concerns, often do not want to share their proprietary data with researchers who might be interested in helping. This paper introduces a privacy-preserving platform by which manufacturers may safely share their data with researchers through secure methods, so that those researchers then create innovative tools to solve the manufacturers' real-world problems, and then provide tools that execute solutions back onto the platform for others to use with privacy and confidentiality guarantees. We illustrate this problem through a particular use case which addresses an important problem in the large-scale manufacturing of food crystals, which is that quality control relies on image analysis tools. Previous to our research, food crystals in the images were manually counted, which required substantial and time-consuming human efforts, but we have developed and deployed a crystal analysis tool which makes this process both more rapid and accurate. The tool enables automatic characterization of the crystal size distribution and numbers from microscope images while the natural imperfections from the sample preparation are automatically removed; a machine learning model to count high resolution translucent crystals and agglomeration of crystals was also developed to aid in these efforts. The resulting algorithm was then packaged for real-world use on the factory floor via a web-based app secured through the originating privacy-preserving platform, allowing manufacturers to use it while keeping their proprietary data secure. After demonstrating this full process, future directions are also explored.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u5e73\u53f0\uff0c\u5e2e\u52a9\u4e2d\u5c0f\u578b\u5236\u9020\u5546\u5b89\u5168\u5171\u4eab\u6570\u636e\uff0c\u7814\u7a76\u8005\u5f00\u53d1\u5de5\u5177\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5e73\u53f0\u56de\u4f20\u5de5\u5177\u4f9b\u4ed6\u4eba\u4f7f\u7528\u3002\u4ee5\u98df\u54c1\u6676\u4f53\u8d28\u91cf\u63a7\u5236\u4e3a\u4f8b\uff0c\u5f00\u53d1\u4e86\u81ea\u52a8\u5206\u6790\u5de5\u5177\u3002", "motivation": "\u4e2d\u5c0f\u578b\u5236\u9020\u5546\u56e0\u7ade\u4e89\u548c\u9690\u79c1\u95ee\u9898\u4e0d\u613f\u5171\u4eab\u6570\u636e\uff0c\u4f46\u9700\u8981\u521b\u65b0\u5de5\u5177\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u9690\u79c1\u4fdd\u62a4\u5e73\u53f0\uff0c\u5b89\u5168\u5171\u4eab\u6570\u636e\uff1b\u5f00\u53d1\u81ea\u52a8\u6676\u4f53\u5206\u6790\u5de5\u5177\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u5f00\u53d1\u4e86\u5feb\u901f\u51c6\u786e\u7684\u6676\u4f53\u5206\u6790\u5de5\u5177\uff0c\u5e76\u901a\u8fc7\u5e73\u53f0\u90e8\u7f72\u5230\u5de5\u5382\u3002", "conclusion": "\u9690\u79c1\u4fdd\u62a4\u5e73\u53f0\u548c\u5de5\u5177\u6210\u529f\u89e3\u51b3\u4e86\u5236\u9020\u5546\u7684\u5b9e\u9645\u95ee\u9898\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u6269\u5c55\u3002"}}
