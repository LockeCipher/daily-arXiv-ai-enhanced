<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 21]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Domain Adaptive SAR Wake Detection: Leveraging Similarity Filtering and Memory Guidance](https://arxiv.org/abs/2509.12279)
*He Gao,Baoxiang Huang,Milena Radenkovic,Borui Li,Ge Chen*

Main category: cs.CV

TL;DR: 提出SimMemDA框架解决SAR与光学图像间的跨模态域适应问题，通过相似性引导和记忆引导机制提升无监督域自适应船舶尾迹检测性能


<details>
  <summary>Details</summary>
Motivation: SAR图像中的尾迹特征抽象且噪声多，标注困难；光学图像视觉线索清晰但存在域偏移问题，直接迁移会导致性能下降

Method: 使用WakeGAN进行风格迁移生成SAR风格伪图像，设计实例级特征相似性过滤机制筛选目标域样分布样本，引入特征-置信度记忆库和K近邻置信度加权融合策略动态校准伪标签，结合区域混合训练提升泛化能力

Result: 实验结果表明SimMemDA方法能够提高跨模态船舶尾迹检测任务的准确性和鲁棒性

Conclusion: 所提方法有效解决了跨模态域适应挑战，验证了方法的有效性和可行性

Abstract: Synthetic Aperture Radar (SAR), with its all-weather and wide-area observation capabilities, serves as a crucial tool for wake detection. However, due to its complex imaging mechanism, wake features in SAR images often appear abstract and noisy, posing challenges for accurate annotation. In contrast, optical images provide more distinct visual cues, but models trained on optical data suffer from performance degradation when applied to SAR images due to domain shift. To address this cross-modal domain adaptation challenge, we propose a Similarity-Guided and Memory-Guided Domain Adaptation (termed SimMemDA) framework for unsupervised domain adaptive ship wake detection via instance-level feature similarity filtering and feature memory guidance. Specifically, to alleviate the visual discrepancy between optical and SAR images, we first utilize WakeGAN to perform style transfer on optical images, generating pseudo-images close to the SAR style. Then, instance-level feature similarity filtering mechanism is designed to identify and prioritize source samples with target-like distributions, minimizing negative transfer. Meanwhile, a Feature-Confidence Memory Bank combined with a K-nearest neighbor confidence-weighted fusion strategy is introduced to dynamically calibrate pseudo-labels in the target domain, improving the reliability and stability of pseudo-labels. Finally, the framework further enhances generalization through region-mixed training, strategically combining source annotations with calibrated target pseudo-labels. Experimental results demonstrate that the proposed SimMemDA method can improve the accuracy and robustness of cross-modal ship wake detection tasks, validating the effectiveness and feasibility of the proposed method.

</details>


### [2] [Artist-Created Mesh Generation from Raw Observation](https://arxiv.org/abs/2509.12501)
*Yao He,Youngjoong Kwon,Wenxiao Cai,Ehsan Adeli*

Main category: cs.CV

TL;DR: 一种从噪声或不完整点云生成艺术家风格网格的端到端框架，通过将3D点云精细化重构为2D图像修复任务，利用生成模型生成高质量网格。


<details>
  <summary>Details</summary>
Motivation: 艺术家创建的网格对商业图形流水线至关重要，但现有方法偏向于假设清洁完整输入或依赖复杂的多阶段流程，限制了在实际应用中的可用性。

Method: 提出端到端方法，将3D点云精细化重新形式化为2D图像修复任务，利用强大的生成模型直接产生高质量艺术家风格网格。

Result: 在ShapeNet数据集上的初步结果显示，该框架能够生成清洁、完整的网格。

Conclusion: 该方法为从实际传感器获取的噪声或不完整点云数据生成商业级艺术网格提供了有前景的解决方案。

Abstract: We present an end-to-end framework for generating artist-style meshes from noisy or incomplete point clouds, such as those captured by real-world sensors like LiDAR or mobile RGB-D cameras. Artist-created meshes are crucial for commercial graphics pipelines due to their compatibility with animation and texturing tools and their efficiency in rendering. However, existing approaches often assume clean, complete inputs or rely on complex multi-stage pipelines, limiting their applicability in real-world scenarios. To address this, we propose an end-to-end method that refines the input point cloud and directly produces high-quality, artist-style meshes. At the core of our approach is a novel reformulation of 3D point cloud refinement as a 2D inpainting task, enabling the use of powerful generative models. Preliminary results on the ShapeNet dataset demonstrate the promise of our framework in producing clean, complete meshes.

</details>


### [3] [Adaptive Sampling Scheduler](https://arxiv.org/abs/2509.12569)
*Qi Wang,Shuliang Zhu,Jinjia Zhou*

Main category: cs.CV

TL;DR: 提出自适应采样调度器，适用于各种一致性蒸馏框架，通过动态时间步选择、优化交替采样和稳定化技术，显著提升生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有蒸馏方法的时间步选择主要依赖确定性或随机策略，需要为不同蒸馏过程专门设计采样调度器，限制了扩散模型在实际应用中的采样灵活性。

Method: 提出自适应采样调度器，包含三个创新策略：动态目标时间步选择、基于时间步重要性的优化交替采样、平滑裁剪和颜色平衡技术。

Result: 在各种一致性蒸馏方法上验证了有效性，实验结果表明生成性能显著提升，方法具有很强的适应性。

Conclusion: 自适应采样调度器解决了现有方法的灵活性限制，扩展了一致性蒸馏模型在复杂生成场景中的适用性。

Abstract: Consistent distillation methods have evolved into effective techniques that significantly accelerate the sampling process of diffusion models. Although existing methods have achieved remarkable results, the selection of target timesteps during distillation mainly relies on deterministic or stochastic strategies, which often require sampling schedulers to be designed specifically for different distillation processes. Moreover, this pattern severely limits flexibility, thereby restricting the full sampling potential of diffusion models in practical applications. To overcome these limitations, this paper proposes an adaptive sampling scheduler that is applicable to various consistency distillation frameworks. The scheduler introduces three innovative strategies: (i) dynamic target timestep selection, which adapts to different consistency distillation frameworks by selecting timesteps based on their computed importance; (ii) Optimized alternating sampling along the solution trajectory by guiding forward denoising and backward noise addition based on the proposed time step importance, enabling more effective exploration of the solution space to enhance generation performance; and (iii) Utilization of smoothing clipping and color balancing techniques to achieve stable and high-quality generation results at high guidance scales, thereby expanding the applicability of consistency distillation models in complex generation scenarios. We validated the effectiveness and flexibility of the adaptive sampling scheduler across various consistency distillation methods through comprehensive experimental evaluations. Experimental results consistently demonstrated significant improvements in generative performance, highlighting the strong adaptability achieved by our method.

</details>


### [4] [Exploring Spectral Characteristics for Single Image Reflection Removal](https://arxiv.org/abs/2509.12627)
*Pengbo Guo,Chengxu Liu,Guoshuai Zhao,Xingsong Hou,Jialie Shen,Xueming Qian*

Main category: cs.CV

TL;DR: 提出基于光谱学习的反射去除方法，通过光谱码本重建反射图像的光谱，利用波长差异区分反射，结合光谱先验精炼模块和光谱感知Transformer在光谱和像素域联合恢复透射内容。


<details>
  <summary>Details</summary>
Motivation: 现有反射去除方法仅在图像域处理，忽略了反射光的光谱特性变化，无法有效区分反射。反射和透射分量在捕获图像中重叠，使得准确区分和恢复干净背景成为挑战。

Method: 1. 提出光谱码本重建反射图像的光谱；2. 设计两个光谱先验精炼模块在空间维度重新分布像素，在波长维度自适应增强光谱差异；3. 提出光谱感知Transformer在光谱和像素域联合恢复透射内容。

Result: 在三个不同的反射基准测试上的实验结果表明，该方法相比最先进模型具有优越性和泛化能力。

Conclusion: 通过光谱学习的新视角，提出的光谱码本和光谱感知Transformer能够有效区分反射并恢复透射内容，在反射去除任务上表现出色。

Abstract: Eliminating reflections caused by incident light interacting with reflective medium remains an ill-posed problem in the image restoration area. The primary challenge arises from the overlapping of reflection and transmission components in the captured images, which complicates the task of accurately distinguishing and recovering the clean background. Existing approaches typically address reflection removal solely in the image domain, ignoring the spectral property variations of reflected light, which hinders their ability to effectively discern reflections. In this paper, we start with a new perspective on spectral learning, and propose the Spectral Codebook to reconstruct the optical spectrum of the reflection image. The reflections can be effectively distinguished by perceiving the wavelength differences between different light sources in the spectrum. To leverage the reconstructed spectrum, we design two spectral prior refinement modules to re-distribute pixels in the spatial dimension and adaptively enhance the spectral differences along the wavelength dimension. Furthermore, we present the Spectrum-Aware Transformer to jointly recover the transmitted content in spectral and pixel domains. Experimental results on three different reflection benchmarks demonstrate the superiority and generalization ability of our method compared to state-of-the-art models.

</details>


### [5] [Effective Gaussian Management for High-fidelity Object Reconstruction](https://arxiv.org/abs/2509.12742)
*Jiateng Liu,Hao Gao,Jiu-Cheng Xie,Chi-Man Pun,Jian Xiong,Haolun Li,Feng Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的高斯管理方法，通过动态激活球象调和法向量、调整SH阶数和任务解耦剪枝，在保持重建质量的同时大幅减少模型参数数量。


<details>
  <summary>Details</summary>
Motivation: 解决传统高斯拟合方法中属性分配没有区别导致的梯度冲突问题，以及表示效率不高的问题。

Method: 使用动态激活策略在表面重建监督下选择性激活SH或法向量，根据梯度大小自适应调整SH阶数，并采用任务解耦剪枝方法。

Result: 在保持优秀重建质量的同时，大幅减少了模型参数数量，在重建质量和效率方面都超过了现有最优方法。

Conclusion: 该高斯管理方法模型无关且可以无缝集成到其他框架中，在提升性能的同时减少模型大小。

Abstract: This paper proposes an effective Gaussian management approach for high-fidelity object reconstruction. Departing from recent Gaussian Splatting (GS) methods that employ indiscriminate attribute assignment, our approach introduces a novel densification strategy that dynamically activates spherical harmonics (SHs) or normals under the supervision of a surface reconstruction module, which effectively mitigates the gradient conflicts caused by dual supervision and achieves superior reconstruction results. To further improve representation efficiency, we develop a lightweight Gaussian representation that adaptively adjusts the SH orders of each Gaussian based on gradient magnitudes and performs task-decoupled pruning to remove Gaussian with minimal impact on a reconstruction task without sacrificing others, which balances the representational capacity with parameter quantity. Notably, our management approach is model-agnostic and can be seamlessly integrated into other frameworks, enhancing performance while reducing model size. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art approaches in both reconstruction quality and efficiency, achieving superior performance with significantly fewer parameters.

</details>


### [6] [DyGLNet: Hybrid Global-Local Feature Fusion with Dynamic Upsampling for Medical Image Segmentation](https://arxiv.org/abs/2509.12763)
*Yican Zhao,Ce Wang,You Hao,Lei Li,Tianli Liao*

Main category: cs.CV

TL;DR: DyGLNet通过融合全局和局部特征与动态上采样机制，实现了高效准确的医学图像分割，在边界精度和小目标分割方面表现优异，计算复杂度低。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分割中的多尺度病变变异性、边界模糊和计算密集型处理需求等挑战。

Method: 提出混合特征提取模块(SHDCBlock)结合单头自注意力和多尺度扩张卷积，以及动态自适应上采样模块(DyFusionUp)实现特征图高保真重建，采用轻量化设计降低计算开销。

Result: 在七个公共数据集上的实验表明，DyGLNet优于现有方法，特别是在边界精度和小目标分割方面表现突出，同时具有较低的计算复杂度。

Conclusion: DyGLNet为临床医学图像分析提供了一个高效可靠的解决方案，代码即将开源。

Abstract: Medical image segmentation grapples with challenges including multi-scale lesion variability, ill-defined tissue boundaries, and computationally intensive processing demands. This paper proposes the DyGLNet, which achieves efficient and accurate segmentation by fusing global and local features with a dynamic upsampling mechanism. The model innovatively designs a hybrid feature extraction module (SHDCBlock), combining single-head self-attention and multi-scale dilated convolutions to model local details and global context collaboratively. We further introduce a dynamic adaptive upsampling module (DyFusionUp) to realize high-fidelity reconstruction of feature maps based on learnable offsets. Then, a lightweight design is adopted to reduce computational overhead. Experiments on seven public datasets demonstrate that DyGLNet outperforms existing methods, particularly excelling in boundary accuracy and small-object segmentation. Meanwhile, it exhibits lower computation complexity, enabling an efficient and reliable solution for clinical medical image analysis. The code will be made available soon.

</details>


### [7] [Double Helix Diffusion for Cross-Domain Anomaly Image Generation](https://arxiv.org/abs/2509.12787)
*Linchun Wu,Qin Zou,Xianbiao Qi,Bo Du,Zhongyuan Wang,Qingquan Li*

Main category: cs.CV

TL;DR: DH-Diff是一个双螺旋扩散框架，通过跨域生成同时合成高质量异常图像和像素级标注掩码，解决了现有方法中结构不一致和特征纠缠的问题。


<details>
  <summary>Details</summary>
Motivation: 制造业视觉异常检测面临真实异常样本稀缺的问题，现有合成方法存在结构不一致和特征纠缠的局限性，影响检测器训练效果。

Method: 采用双螺旋架构，包含特征分离、连接和融合模块，使用域解耦注意力机制独立增强图像和标注特征，语义评分图对齐模块确保结构真实性。

Result: 在多样性和真实性方面显著优于最先进方法，下游异常检测性能得到显著提升。

Conclusion: DH-Diff框架有效解决了异常合成中的结构一致性和特征纠缠问题，为制造业视觉异常检测提供了高质量的训练数据生成方案。

Abstract: Visual anomaly inspection is critical in manufacturing, yet hampered by the scarcity of real anomaly samples for training robust detectors. Synthetic data generation presents a viable strategy for data augmentation; however, current methods remain constrained by two principal limitations: 1) the generation of anomalies that are structurally inconsistent with the normal background, and 2) the presence of undesirable feature entanglement between synthesized images and their corresponding annotation masks, which undermines the perceptual realism of the output. This paper introduces Double Helix Diffusion (DH-Diff), a novel cross-domain generative framework designed to simultaneously synthesize high-fidelity anomaly images and their pixel-level annotation masks, explicitly addressing these challenges. DH-Diff employs a unique architecture inspired by a double helix, cycling through distinct modules for feature separation, connection, and merging. Specifically, a domain-decoupled attention mechanism mitigates feature entanglement by enhancing image and annotation features independently, and meanwhile a semantic score map alignment module ensures structural authenticity by coherently integrating anomaly foregrounds. DH-Diff offers flexible control via text prompts and optional graphical guidance. Extensive experiments demonstrate that DH-Diff significantly outperforms state-of-the-art methods in diversity and authenticity, leading to significant improvements in downstream anomaly detection performance.

</details>


### [8] [Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation](https://arxiv.org/abs/2509.12815)
*Biwen Lei,Yang Li,Xinhai Liu,Shuhui Yang,Lixin Xu,Jingwei Huang,Ruining Tang,Haohan Weng,Jian Liu,Jing Xu,Zhen Zhou,Yiling Zhu,Jiankai Xing,Jiachen Xu,Changfeng Ma,Xinhao Yan,Yunhan Yang,Chunshi Wang,Duoteng Xu,Xueqi Ma,Yuguang Chen,Jing Li,Mingxin Yang,Sheng Zhang,Yifei Feng,Xin Huang,Di Luo,Zebin He,Puhua Jiang,Changrong Hu,Zihan Qin,Shiwei Miao,Haolin Liu,Yunfei Zhao,Zeqiang Lai,Qingxiang Lin,Zibo Zhao,Kunhong Li,Xianghui Yang,Huiwen Shi,Xin Yang,Yuxuan Wang,Zebin Yao,Yihang Lian,Sicong Liu,Xintong Han,Wangchen Qin,Caisheng Ouyang,Jianyin Liu,Tianwen Yuan,Shuai Jiang,Hong Duan,Yanqi Niu,Wencong Lin,Yifu Sun,Shirui Huang,Lin Niu,Gu Gong,Guojian Xiao,Bojian Zheng,Xiang Yuan,Qi Chen,Jie Xiao,Dongyang Zheng,Xiaofeng Yang,Kai Liu,Jianchen Zhu,Lifu Wang,Qinglin Lu,Jie Liu,Liang Dong,Fan Jiang,Ruibin Chen,Lei Wang,Chao Zhang,Jiaxin Lin,Hao Zhang,Zheng Ye,Peng He,Runzhou Wu,Yinhe Wu,Jiayao Du,Jupeng Chen,Xinyue Mao,Dongyuan Guo,Yixuan Tang,Yulin Tsai,Yonghao Tan,Jiaao Yu,Junlin Yu,Keren Zhang,Yifan Li,Peng Chen,Tian Liu,Di Wang,Yuhong Liu,Linus,Jie Jiang,Zhuo Chen,Chunchao Guo*

Main category: cs.CV

TL;DR: Hunyuan3D Studio是一个端到端的AI驱动3D内容创作平台，能够从概念图像或文本描述快速生成游戏就绪的3D资产，显著简化游戏开发流程。


<details>
  <summary>Details</summary>
Motivation: 传统3D资产创建流程劳动密集且专业化，需要简化和自动化游戏生产管线。

Method: 集成多个先进神经模块（部件级3D生成、多边形生成、语义UV等）的统一框架，将概念图像或文本转换为完整3D模型。

Result: 生成的资产不仅视觉吸引力强，而且符合现代游戏引擎的严格技术要求，显著减少迭代时间。

Conclusion: Hunyuan3D Studio代表了游戏开发和交互媒体中AI辅助工作流程的重大进步，为创意意图到技术资产提供了无缝桥梁。

Abstract: The creation of high-quality 3D assets, a cornerstone of modern game development, has long been characterized by labor-intensive and specialized workflows. This paper presents Hunyuan3D Studio, an end-to-end AI-powered content creation platform designed to revolutionize the game production pipeline by automating and streamlining the generation of game-ready 3D assets. At its core, Hunyuan3D Studio integrates a suite of advanced neural modules (such as Part-level 3D Generation, Polygon Generation, Semantic UV, etc.) into a cohesive and user-friendly system. This unified framework allows for the rapid transformation of a single concept image or textual description into a fully-realized, production-quality 3D model complete with optimized geometry and high-fidelity PBR textures. We demonstrate that assets generated by Hunyuan3D Studio are not only visually compelling but also adhere to the stringent technical requirements of contemporary game engines, significantly reducing iteration time and lowering the barrier to entry for 3D content creation. By providing a seamless bridge from creative intent to technical asset, Hunyuan3D Studio represents a significant leap forward for AI-assisted workflows in game development and interactive media.

</details>


### [9] [Exploring Metric Fusion for Evaluation of NeRFs](https://arxiv.org/abs/2509.12836)
*Shreyas Shivakumara,Gabriel Eilertsen,Karljohan Lundin Palmerius*

Main category: cs.CV

TL;DR: 结合DISTS和VMAF两种感知质量评估指标，通过不同的归一化和融合策略，提出了一种改进的NeRF生成图像质量评估方法，在两个数据集上验证了其与主观评分的高相关性。


<details>
  <summary>Details</summary>
Motivation: NeRF生成的新视角图像存在独特伪影，现有单一指标在不同数据集上表现不一致，需要结合不同感知方法的指标来克服个体局限性。

Method: 采用DISTS（深度图像结构和纹理相似性）和VMAF（视频多方法评估融合）两种指标，实验了两种归一化策略和两种融合策略，在两个数据集（合成和室外）上测试了三种配置。

Result: 融合指标相比单一指标与主观质量评分具有更高的相关性，证明了融合方法的鲁棒性和泛化能力。

Conclusion: 结合不同感知方法的多个指标可以有效改进NeRF生成图像的质量评估，融合策略能够提供更可靠的质量预测。

Abstract: Neural Radiance Fields (NeRFs) have demonstrated significant potential in synthesizing novel viewpoints. Evaluating the NeRF-generated outputs, however, remains a challenge due to the unique artifacts they exhibit, and no individual metric performs well across all datasets. We hypothesize that combining two successful metrics, Deep Image Structure and Texture Similarity (DISTS) and Video Multi-Method Assessment Fusion (VMAF), based on different perceptual methods, can overcome the limitations of individual metrics and achieve improved correlation with subjective quality scores. We experiment with two normalization strategies for the individual metrics and two fusion strategies to evaluate their impact on the resulting correlation with the subjective scores. The proposed pipeline is tested on two distinct datasets, Synthetic and Outdoor, and its performance is evaluated across three different configurations. We present a detailed analysis comparing the correlation coefficients of fusion methods and individual scores with subjective scores to demonstrate the robustness and generalizability of the fusion metrics.

</details>


### [10] [Few to Big: Prototype Expansion Network via Diffusion Learner for Point Cloud Few-shot Semantic Segmentation](https://arxiv.org/abs/2509.12878)
*Qianguang Zhao,Dongli Wang,Yan Zhou,Jianxun Li,Richard Irampa*

Main category: cs.CV

TL;DR: PENet是一个用于少样本3D点云语义分割的新框架，通过扩散模型生成通用特征来扩展原型表示能力，解决类内多样性和集合间不一致性问题


<details>
  <summary>Details</summary>
Motivation: 现有基于原型的方法存在两个关键挑战：(1)类内多样性问题 - 原型的有限表示能力无法覆盖类的全部变化；(2)集合间不一致性问题 - 支持集生成的原型与查询特征空间不对齐

Method: 提出原型扩展网络(PENet)，采用双流学习器架构：保持传统监督学习的内在学习器(IL)，同时引入扩散学习器(DL)提供丰富的通用特征。通过原型同化模块(PAM)和原型校准机制(PCM)处理双原型

Result: 在S3DIS和ScanNet数据集上的大量实验表明，PENet在各种少样本设置下显著优于最先进的方法

Conclusion: 通过利用扩散模型的生成能力扩展原型表示范围，PENet有效解决了少样本3D点云分割中的类内多样性和集合间不一致性问题

Abstract: Few-shot 3D point cloud semantic segmentation aims to segment novel categories using a minimal number of annotated support samples. While existing prototype-based methods have shown promise, they are constrained by two critical challenges: (1) Intra-class Diversity, where a prototype's limited representational capacity fails to cover a class's full variations, and (2) Inter-set Inconsistency, where prototypes derived from the support set are misaligned with the query feature space. Motivated by the powerful generative capability of diffusion model, we re-purpose its pre-trained conditional encoder to provide a novel source of generalizable features for expanding the prototype's representational range. Under this setup, we introduce the Prototype Expansion Network (PENet), a framework that constructs big-capacity prototypes from two complementary feature sources. PENet employs a dual-stream learner architecture: it retains a conventional fully supervised Intrinsic Learner (IL) to distill representative features, while introducing a novel Diffusion Learner (DL) to provide rich generalizable features. The resulting dual prototypes are then processed by a Prototype Assimilation Module (PAM), which adopts a novel push-pull cross-guidance attention block to iteratively align the prototypes with the query space. Furthermore, a Prototype Calibration Mechanism (PCM) regularizes the final big capacity prototype to prevent semantic drift. Extensive experiments on the S3DIS and ScanNet datasets demonstrate that PENet significantly outperforms state-of-the-art methods across various few-shot settings.

</details>


### [11] [AREPAS: Anomaly Detection in Fine-Grained Anatomy with Reconstruction-Based Semantic Patch-Scoring](https://arxiv.org/abs/2509.12905)
*Branko Mitic,Philipp Seeböck,Helmut Prosch,Georg Langs*

Main category: cs.CV

TL;DR: 新的生成式异常检测方法，通过图像到图像翻译和补丁相似性评分，在胸部CT和脑部MRI中实现了更精确的异常分割效果


<details>
  <summary>Details</summary>
Motivation: 医学领域异常检测和无监督分割应用广泛，但现有生成式方法在处理正常细粒度组织变异(如肺部解剖结构)时遇到挑战

Method: 组合图像到图像翻译进行无异常重建，然后通过观察图像与生成图像之间的补丁相似性评分来定位异常

Result: 在胸部CT感染性病变检测和脑部MRI缺血性脑梗塌分割任务中，相对DICE分数分别提高+1.9%和+4.4%，超越其他重建基方法

Conclusion: 该方法能够有效处理正常细粒度组织变异挑战，在不同医学模态下都显示出优秀的异常检测和分割性能

Abstract: Early detection of newly emerging diseases, lesion severity assessment, differentiation of medical conditions and automated screening are examples for the wide applicability and importance of anomaly detection (AD) and unsupervised segmentation in medicine. Normal fine-grained tissue variability such as present in pulmonary anatomy is a major challenge for existing generative AD methods. Here, we propose a novel generative AD approach addressing this issue. It consists of an image-to-image translation for anomaly-free reconstruction and a subsequent patch similarity scoring between observed and generated image-pairs for precise anomaly localization. We validate the new method on chest computed tomography (CT) scans for the detection and segmentation of infectious disease lesions. To assess generalizability, we evaluate the method on an ischemic stroke lesion segmentation task in T1-weighted brain MRI. Results show improved pixel-level anomaly segmentation in both chest CTs and brain MRIs, with relative DICE score improvements of +1.9% and +4.4%, respectively, compared to other state-of-the-art reconstruction-based methods.

</details>


### [12] [T-SiamTPN: Temporal Siamese Transformer Pyramid Networks for Robust and Efficient UAV Tracking](https://arxiv.org/abs/2509.12913)
*Hojat Ardi,Amir Jahanshahi,Ali Diba*

Main category: cs.CV

TL;DR: T-SiamTPN是一个时间感知的Siamese跟踪框架，通过在SiamTPN架构中加入显式时间建模，解决了空中目标跟踪中的时间依赖性问题，在保持计算效率的同时显著提升了跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 现有跟踪器大多关注空间线索而忽视时间依赖性，导致长期跟踪和遮挡情况下鲁棒性不足。同时，基于相关的Siamese跟踪器受限于线性相关操作，难以处理复杂的非线性外观变化。

Method: 扩展SiamTPN架构，引入时间特征融合和基于注意力的交互机制，加强时间一致性并实现更丰富的特征表示。

Result: 相比基线方法，T-SiamTPN成功率和精确度分别提升13.7%和14.7%，在Jetson Nano上实现7.1 FPS的实时性能，计算效率得到保持。

Conclusion: 时间建模在Siamese跟踪框架中至关重要，T-SiamTPN为空中目标跟踪提供了一个强大且高效的解决方案，适用于实际嵌入式应用。

Abstract: Aerial object tracking remains a challenging task due to scale variations, dynamic backgrounds, clutter, and frequent occlusions. While most existing trackers emphasize spatial cues, they often overlook temporal dependencies, resulting in limited robustness in long-term tracking and under occlusion. Furthermore, correlation-based Siamese trackers are inherently constrained by the linear nature of correlation operations, making them ineffective against complex, non-linear appearance changes. To address these limitations, we introduce T-SiamTPN, a temporal-aware Siamese tracking framework that extends the SiamTPN architecture with explicit temporal modeling. Our approach incorporates temporal feature fusion and attention-based interactions, strengthening temporal consistency and enabling richer feature representations. These enhancements yield significant improvements over the baseline and achieve performance competitive with state-of-the-art trackers. Crucially, despite the added temporal modules, T-SiamTPN preserves computational efficiency. Deployed on the resource-constrained Jetson Nano, the tracker runs in real time at 7.1 FPS, demonstrating its suitability for real-world embedded applications without notable runtime overhead. Experimental results highlight substantial gains: compared to the baseline, T-SiamTPN improves success rate by 13.7% and precision by 14.7%. These findings underscore the importance of temporal modeling in Siamese tracking frameworks and establish T-SiamTPN as a strong and efficient solution for aerial object tracking. Code is available at: https://github.com/to/be/released

</details>


### [13] [4DRadar-GS: Self-Supervised Dynamic Driving Scene Reconstruction with 4D Radar](https://arxiv.org/abs/2509.12931)
*Xiao Tang,Guirong Zhuo,Cong Wang,Boyuan Zheng,Minqing Huang,Lianqing Zheng,Long Chen,Shouyi Lu*

Main category: cs.CV

TL;DR: 4DRadar-GS是一个基于4D雷达增强的自监督3D重建框架，专门针对动态驾驶场景，通过雷达辅助的高斯初始化和速度引导的点跟踪模型，解决了现有方法在动态物体重建中的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督3D重建方法在动态物体重建方面存在困难，主要由于运动估计不准确和时间一致性弱，导致动态场景元素的重建不完整或失真。

Method: 提出4D雷达辅助的高斯初始化方案，利用4D雷达的速度和空间信息分割动态物体并恢复单目深度尺度；开发速度引导的点跟踪模型（VGPT），在场景流监督下联合训练，跟踪细粒度动态轨迹并构建时间一致的表示。

Result: 在OmniHD-Scenes数据集上评估，4DRadar-GS在动态驾驶场景3D重建方面达到了最先进的性能。

Conclusion: 4DRadar-GS通过整合4D雷达信息，有效解决了动态驾驶场景中的3D重建挑战，为自动驾驶系统的验证和感知模型训练提供了更准确的动态场景重建能力。

Abstract: 3D reconstruction and novel view synthesis are critical for validating autonomous driving systems and training advanced perception models. Recent self-supervised methods have gained significant attention due to their cost-effectiveness and enhanced generalization in scenarios where annotated bounding boxes are unavailable. However, existing approaches, which often rely on frequency-domain decoupling or optical flow, struggle to accurately reconstruct dynamic objects due to imprecise motion estimation and weak temporal consistency, resulting in incomplete or distorted representations of dynamic scene elements. To address these challenges, we propose 4DRadar-GS, a 4D Radar-augmented self-supervised 3D reconstruction framework tailored for dynamic driving scenes. Specifically, we first present a 4D Radar-assisted Gaussian initialization scheme that leverages 4D Radar's velocity and spatial information to segment dynamic objects and recover monocular depth scale, generating accurate Gaussian point representations. In addition, we propose a Velocity-guided PointTrack (VGPT) model, which is jointly trained with the reconstruction pipeline under scene flow supervision, to track fine-grained dynamic trajectories and construct temporally consistent representations. Evaluated on the OmniHD-Scenes dataset, 4DRadar-GS achieves state-of-the-art performance in dynamic driving scene 3D reconstruction.

</details>


### [14] [Beyond Averages: Open-Vocabulary 3D Scene Understanding with Gaussian Splatting and Bag of Embeddings](https://arxiv.org/abs/2509.12938)
*Abdalla Arafa,Didier Stricker*

Main category: cs.CV

TL;DR: 提出了一种基于预分解对象级高斯和CLIP特征聚合的新方法，解决了3D高斯溅射在语义理解中的模糊性问题，实现了准确的开放词汇对象检索和任务适应


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射虽然能够实现实时逼真渲染，但其固有的模糊性限制了在AR/VR和机器人等领域的3D场景理解应用。现有方法通过2D基础模型蒸馏学习语义，但存在alpha混合平均语义的根本局限性

Method: 利用预分解的对象级高斯，通过多视图CLIP特征聚合为每个对象创建全面的嵌入包，完全绕过可微分渲染进行语义处理

Result: 实验表明该方法有效克服了3D开放词汇对象提取的挑战，同时在2D开放词汇分割方面与最先进方法性能相当

Conclusion: 该方法通过对象级语义表示范式，为3D高斯溅射的语义理解提供了新的解决方案，具有广泛的应用前景

Abstract: Novel view synthesis has seen significant advancements with 3D Gaussian Splatting (3DGS), enabling real-time photorealistic rendering. However, the inherent fuzziness of Gaussian Splatting presents challenges for 3D scene understanding, restricting its broader applications in AR/VR and robotics. While recent works attempt to learn semantics via 2D foundation model distillation, they inherit fundamental limitations: alpha blending averages semantics across objects, making 3D-level understanding impossible. We propose a paradigm-shifting alternative that bypasses differentiable rendering for semantics entirely. Our key insight is to leverage predecomposed object-level Gaussians and represent each object through multiview CLIP feature aggregation, creating comprehensive "bags of embeddings" that holistically describe objects. This allows: (1) accurate open-vocabulary object retrieval by comparing text queries to object-level (not Gaussian-level) embeddings, and (2) seamless task adaptation: propagating object IDs to pixels for 2D segmentation or to Gaussians for 3D extraction. Experiments demonstrate that our method effectively overcomes the challenges of 3D open-vocabulary object extraction while remaining comparable to state-of-the-art performance in 2D open-vocabulary segmentation, ensuring minimal compromise.

</details>


### [15] [Dual-Stage Reweighted MoE for Long-Tailed Egocentric Mistake Detection](https://arxiv.org/abs/2509.12990)
*Boyu Han,Qianqian Xu,Shilong Bao,Zhiyong Yang,Sicong Li,Qingming Huang*

Main category: cs.CV

TL;DR: 通过双阶段重加权专家混合框架(DR-MoE)，从自我视角视频中识别用户动作错误，特别关注细微和稀罕的错误情况


<details>
  <summary>Details</summary>
Motivation: 解决在自我视角视频中检测用户动作错误的挑战，特别是细微、不常出现的错误实例

Method: 双阶段DR-MoE框架：第一阶段使用冻结ViViT和LoRA调整ViViT提取特征，通过特征级专家模块结合；第二阶段训练三个不同目标的分类器（重加权交叉瑣、AUC损失、标签意识损失），通过分类级专家模块融合预测

Result: 方法表现出艰固的性能，在识别稀罕和模糊的错误实例方面特别有效

Conclusion: DR-MoE框枵能够有效处理自我视角视频中的动作错误检测问题，通过多重损失函数和专家混合机制提升了对细微、稀罕错误的识别能力

Abstract: In this report, we address the problem of determining whether a user performs an action incorrectly from egocentric video data. To handle the challenges posed by subtle and infrequent mistakes, we propose a Dual-Stage Reweighted Mixture-of-Experts (DR-MoE) framework. In the first stage, features are extracted using a frozen ViViT model and a LoRA-tuned ViViT model, which are combined through a feature-level expert module. In the second stage, three classifiers are trained with different objectives: reweighted cross-entropy to mitigate class imbalance, AUC loss to improve ranking under skewed distributions, and label-aware loss with sharpness-aware minimization to enhance calibration and generalization. Their predictions are fused using a classification-level expert module. The proposed method achieves strong performance, particularly in identifying rare and ambiguous mistake instances. The code is available at https://github.com/boyuh/DR-MoE.

</details>


### [16] [Using KL-Divergence to Focus Frequency Information in Low-Light Image Enhancement](https://arxiv.org/abs/2509.13083)
*Yan Xingyang,Huang Xiaohong,Zhang Zhao,You Tian,Xu Ziheng*

Main category: cs.CV

TL;DR: LLFDisc是一个U形深度增强网络，通过交叉注意力和门控机制进行频率感知增强，提出分布感知损失函数直接拟合傅里叶域信息，在多个基准测试中达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 传统傅里叶频率信息拟合使用像素级损失函数，过度关注局部信息可能导致全局信息丢失，需要更有效的方法来对齐傅里叶域信息

Method: 提出LLFDisc U形深度增强网络，集成交叉注意力和门控机制；设计分布感知损失函数，使用闭式KL散度目标最小化傅里叶域信息差异；基于VGG增强感知损失，在提取的深度特征上嵌入KL散度

Result: 在多个基准测试的定性和定量评估中均达到最先进性能

Conclusion: LLFDisc通过频率感知增强和分布感知损失函数，能够比传统MSE损失更鲁棒地对齐傅里叶域信息，实现更好的结构保真度

Abstract: In the Fourier domain, luminance information is primarily encoded in the amplitude spectrum, while spatial structures are captured in the phase components. The traditional Fourier Frequency information fitting employs pixel-wise loss functions, which tend to focus excessively on local information and may lead to global information loss. In this paper, we present LLFDisc, a U-shaped deep enhancement network that integrates cross-attention and gating mechanisms tailored for frequency-aware enhancement. We propose a novel distribution-aware loss that directly fits the Fourier-domain information and minimizes their divergence using a closed-form KL-Divergence objective. This enables the model to align Fourier-domain information more robustly than with conventional MSE-based losses. Furthermore, we enhance the perceptual loss based on VGG by embedding KL-Divergence on extracted deep features, enabling better structural fidelity. Extensive experiments across multiple benchmarks demonstrate that LLFDisc achieves state-of-the-art performance in both qualitative and quantitative evaluations. Our code will be released at: https://github.com/YanXY000/LLFDisc

</details>


### [17] [MSDNet: Efficient 4D Radar Super-Resolution via Multi-Stage Distillation](https://arxiv.org/abs/2509.13149)
*Minqing Huang,Shouyi Lu,Boyuan Zheng,Ziyao Li,Xiao Tang,Guirong Zhuo*

Main category: cs.CV

TL;DR: MSDNet是一个多阶段蒸馏框架，通过特征重建和轻量级扩散网络将LiDAR先验知识转移到4D雷达特征中，实现高保真重建和低延迟推理


<details>
  <summary>Details</summary>
Motivation: 现有4D雷达超分辨率方法存在训练成本高、推理延迟大、泛化能力差的问题，难以平衡精度和效率

Method: 提出两阶段蒸馏框架：第一阶段进行重建引导的特征蒸馏，第二阶段采用扩散引导的特征蒸馏，并引入噪声适配器自适应对齐噪声水平

Result: 在VoD和内部数据集上的实验表明，MSDNet实现了高保真重建和低延迟推理，并在下游任务上持续提升性能

Conclusion: MSDNet通过有效的知识蒸馏策略成功解决了4D雷达超分辨率中精度与效率的平衡问题，为自动驾驶感知提供了实用解决方案

Abstract: 4D radar super-resolution, which aims to reconstruct sparse and noisy point clouds into dense and geometrically consistent representations, is a foundational problem in autonomous perception. However, existing methods often suffer from high training cost or rely on complex diffusion-based sampling, resulting in high inference latency and poor generalization, making it difficult to balance accuracy and efficiency. To address these limitations, we propose MSDNet, a multi-stage distillation framework that efficiently transfers dense LiDAR priors to 4D radar features to achieve both high reconstruction quality and computational efficiency. The first stage performs reconstruction-guided feature distillation, aligning and densifying the student's features through feature reconstruction. In the second stage, we propose diffusion-guided feature distillation, which treats the stage-one distilled features as a noisy version of the teacher's representations and refines them via a lightweight diffusion network. Furthermore, we introduce a noise adapter that adaptively aligns the noise level of the feature with a predefined diffusion timestep, enabling a more precise denoising. Extensive experiments on the VoD and in-house datasets demonstrate that MSDNet achieves both high-fidelity reconstruction and low-latency inference in the task of 4D radar point cloud super-resolution, and consistently improves performance on downstream tasks. The code will be publicly available upon publication.

</details>


### [18] [More performant and scalable: Rethinking contrastive vision-language pre-training of radiology in the LLM era](https://arxiv.org/abs/2509.13175)
*Yingtai Li,Haoran Lai,Xiaoqian Zhou,Shuai Ming,Wenxin Ma,Wei Wei,Shaohua Kevin Zhou*

Main category: cs.CV

TL;DR: 利用LLMs自动从放射报告中提取诊断标签，创建大规模银标准数据集，实现低成本监督预训练，显著提升医学视觉-语言对比学习性能


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的出现为医学对比视觉-语言预训练提供了革命性机会，但如何利用LLMs实现大规模监督预训练并提升视觉-语言对齐仍待探索

Method: 使用LLMs自动提取放射报告诊断标签（AUC>96%），构建50k CT图像-报告对的银标准数据集（成本仅约3美元），基于3D ResNet-18和标准CLIP训练进行监督预训练

Result: 在CT-RATE上零样本诊断达到83.8% AUC，RAD-ChestCT上77.3% AUC，跨模态检索显著提升（图像-图像MAP@50=53.7%，报告-图像Recall@100=52.2%）

Conclusion: LLMs能够实现更高效和可扩展的医学AI系统，监督预训练从根本上改善了视觉-语言对齐，为医学AI民主化提供了新途径

Abstract: The emergence of Large Language Models (LLMs) presents unprecedented opportunities to revolutionize medical contrastive vision-language pre-training. In this paper, we show how LLMs can facilitate large-scale supervised pre-training, thereby advancing vision-language alignment. We begin by demonstrate that modern LLMs can automatically extract diagnostic labels from radiology reports with remarkable precision (>96\% AUC in our experiments) without complex prompt engineering, enabling the creation of large-scale "silver-standard" datasets at a minimal cost (~\$3 for 50k CT image-report pairs). Further, we find that vision encoder trained on this "silver-standard" dataset achieves performance comparable to those trained on labels extracted by specialized BERT-based models, thereby democratizing the access to large-scale supervised pre-training. Building on this foundation, we proceed to reveal that supervised pre-training fundamentally improves contrastive vision-language alignment. Our approach achieves state-of-the-art performance using only a 3D ResNet-18 with vanilla CLIP training, including 83.8\% AUC for zero-shot diagnosis on CT-RATE, 77.3\% AUC on RAD-ChestCT, and substantial improvements in cross-modal retrieval (MAP@50=53.7\% for image-image, Recall@100=52.2\% for report-image). These results demonstrate the potential of utilizing LLMs to facilitate {\bf more performant and scalable} medical AI systems. Our code is avaiable at https://github.com/SadVoxel/More-performant-and-scalable.

</details>


### [19] [End4: End-to-end Denoising Diffusion for Diffusion-Based Inpainting Detection](https://arxiv.org/abs/2509.13214)
*Fei Wang,Xuecheng Wu,Zheng Zhang,Danlei Huang,Yuheng Huang,BoWang*

Main category: cs.CV

TL;DR: 提出End4方法检测扩散修复图像，通过去噪重建模型和尺度感知金字塔融合模块提高检测性能，建立了包含五种掩码区域的综合基准测试


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成和修复方面表现出强大能力，但也可能被恶意滥用。现有方法难以检测基于扩散的修复图像，即使训练数据中包含类似修复图像

Method: End4方法设计去噪重建模型改善重建和检测过程的潜在空间对齐，利用尺度感知金字塔融合模块(SPFM)在不同尺度注意力金字塔层指导下细化局部图像特征

Result: 大量实验证明End4能有效泛化到未见过的掩码模式，并在各种扰动下保持鲁棒性

Conclusion: End4方法在检测扩散修复图像方面表现出色，具有良好的泛化能力和鲁棒性，代码和数据集将很快发布

Abstract: The powerful generative capabilities of diffusion models have significantly advanced the field of image synthesis, enhancing both full image generation and inpainting-based image editing. Despite their remarkable advancements, diffusion models also raise concerns about potential misuse for malicious purposes. However, existing approaches struggle to identify images generated by diffusion-based inpainting models, even when similar inpainted images are included in their training data. To address this challenge, we propose a novel detection method based on End-to-end denoising diffusion (End4). Specifically, End4 designs a denoising reconstruction model to improve the alignment degree between the latent spaces of the reconstruction and detection processes, thus reconstructing features that are more conducive to detection. Meanwhile, it leverages a Scale-aware Pyramid-like Fusion Module (SPFM) that refines local image features under the guidance of attention pyramid layers at different scales, enhancing feature discriminability. Additionally, to evaluate detection performance on inpainted images, we establish a comprehensive benchmark comprising images generated from five distinct masked regions. Extensive experiments demonstrate that our End4 effectively generalizes to unseen masking patterns and remains robust under various perturbations. Our code and dataset will be released soon.

</details>


### [20] [ResidualViT for Efficient Temporally Dense Video Encoding](https://arxiv.org/abs/2509.13255)
*Mattia Soldan,Fabian Caba Heilbron,Bernard Ghanem,Josef Sivic,Bryan Russell*

Main category: cs.CV

TL;DR: 提出ResidualViT架构，通过可学习残差连接和token缩减模块，在保持精度的同时显著降低视频时序密集任务的计算成本


<details>
  <summary>Details</summary>
Motivation: 视频理解任务需要高时间分辨率处理，但计算帧级特征成本高昂，需要减少时序密集任务的特征计算开销

Method: ResidualViT架构：1）可学习残差连接确保时序一致性；2）token缩减模块选择性丢弃冗余信息并重用预训练权重；3）轻量级蒸馏策略近似原始模型特征

Result: 在4个任务5个数据集上验证，计算成本降低60%，推理速度提升2.5倍，精度接近原始基础模型

Conclusion: ResidualViT能有效处理视频时序密集任务，在显著降低计算成本的同时保持模型性能

Abstract: Several video understanding tasks, such as natural language temporal video grounding, temporal activity localization, and audio description generation, require "temporally dense" reasoning over frames sampled at high temporal resolution. However, computing frame-level features for these tasks is computationally expensive given the temporal resolution requirements. In this paper, we make three contributions to reduce the cost of computing features for temporally dense tasks. First, we introduce a vision transformer (ViT) architecture, dubbed ResidualViT, that leverages the large temporal redundancy in videos to efficiently compute temporally dense frame-level features. Our architecture incorporates (i) learnable residual connections that ensure temporal consistency across consecutive frames and (ii) a token reduction module that enhances processing speed by selectively discarding temporally redundant information while reusing weights of a pretrained foundation model. Second, we propose a lightweight distillation strategy to approximate the frame-level features of the original foundation model. Finally, we evaluate our approach across four tasks and five datasets, in both zero-shot and fully supervised settings, demonstrating significant reductions in computational cost (up to 60%) and improvements in inference speed (up to 2.5x faster), all while closely approximating the accuracy of the original foundation model.

</details>


### [21] [3D Aware Region Prompted Vision Language Model](https://arxiv.org/abs/2509.13317)
*An-Chieh Cheng,Yang Fu,Yukang Chen,Zhijian Liu,Xiaolong Li,Subhashree Radhakrishnan,Song Han,Yao Lu,Jan Kautz,Pavlo Molchanov,Hongxu Yin,Xiaolong Wang,Sifei Liu*

Main category: cs.CV

TL;DR: SR-3D是一个将单视图2D图像和多视图3D数据通过共享视觉标记空间连接起来的视觉语言模型，支持灵活的区域提示，无需多帧标注即可实现3D空间推理。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统方法需要大量多帧标注的问题，以及实现2D和3D表示空间的统一，提高场景理解中空间推理的准确性。

Method: 通过用3D位置嵌入丰富2D视觉特征，使3D模型能够利用强大的2D先验知识，在不同帧之间进行准确的空间推理，即使目标对象不在同一视图中出现。

Result: 在通用2D视觉语言和专门3D空间基准测试中达到最先进性能，能够准确推断空间关系和度量测量，适用于无传感器3D输入或真实3D标注的野外视频。

Conclusion: SR-3D有效统一了2D和3D表示空间，在场景理解方面表现出色，具有广泛的应用前景。

Abstract: We present Spatial Region 3D (SR-3D) aware vision-language model that connects single-view 2D images and multi-view 3D data through a shared visual token space. SR-3D supports flexible region prompting, allowing users to annotate regions with bounding boxes, segmentation masks on any frame, or directly in 3D, without the need for exhaustive multi-frame labeling. We achieve this by enriching 2D visual features with 3D positional embeddings, which allows the 3D model to draw upon strong 2D priors for more accurate spatial reasoning across frames, even when objects of interest do not co-occur within the same view. Extensive experiments on both general 2D vision language and specialized 3D spatial benchmarks demonstrate that SR-3D achieves state-of-the-art performance, underscoring its effectiveness for unifying 2D and 3D representation space on scene understanding. Moreover, we observe applicability to in-the-wild videos without sensory 3D inputs or ground-truth 3D annotations, where SR-3D accurately infers spatial relationships and metric measurements.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [22] [Human + AI for Accelerating Ad Localization Evaluation](https://arxiv.org/abs/2509.12543)
*Harshit Rajgarhia,Shivali Dalmia,Mengyang Zhao,Mukherji Abhishek,Kiran Ganesh*

Main category: cs.AI

TL;DR: 首个结构化框架，结合自动化组件与人工监督，通过场景文本检测、图像修复、机器翻译和文本重新排版技术，加速多语言广告本地化评估流程


<details>
  <summary>Details</summary>
Motivation: 多语言广告本地化不仅需要文本翻译，还需保持视觉一致性、空间对齐和风格完整性，以满足不同语言和格式的需求

Method: 结合自动化组件与人工监督的结构化框架，集成场景文本检测、图像修复(inpainting)、机器翻译(MT)和文本重新排版(text reimposition)技术

Result: 在六个地区语言环境中的定性结果显示，该方法能够产生语义准确且视觉一致的本地化广告，适合实际工作流程部署

Conclusion: 该研究提出了一种有效的广告本地化解决方案，通过技术集成和人机协作，实现了高质量的多语言广告生产，为广告行业提供了可部署的工作流程

Abstract: Adapting advertisements for multilingual audiences requires more than simple text translation; it demands preservation of visual consistency, spatial alignment, and stylistic integrity across diverse languages and formats. We introduce a structured framework that combines automated components with human oversight to address the complexities of advertisement localization. To the best of our knowledge, this is the first work to integrate scene text detection, inpainting, machine translation (MT), and text reimposition specifically for accelerating ad localization evaluation workflows. Qualitative results across six locales demonstrate that our approach produces semantically accurate and visually coherent localized advertisements, suitable for deployment in real-world workflows.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [InJecteD: Analyzing Trajectories and Drift Dynamics in Denoising Diffusion Probabilistic Models for 2D Point Cloud Generation](https://arxiv.org/abs/2509.12239)
*Sanyam Jain,Khuram Naveed,Illia Oleksiienko,Alexandros Iosifidis,Ruben Pauwels*

Main category: cs.LG

TL;DR: InJecteD是一个用于分析DDPM去噪过程中样本轨迹的框架，通过量化位移、速度、聚类等轨迹特性来增强模型透明度，支持人类-AI协作调试生成模型。


<details>
  <summary>Details</summary>
Motivation: 提高DDPM模型的可解释性，通过分析去噪过程中的样本轨迹来增强模型透明度，支持实践者调试和优化生成模型。

Method: 使用简化的DDPM架构，分析2D点云生成过程中的样本轨迹，量化位移、速度、聚类和漂移场动态等特性，采用Wasserstein距离和余弦相似度等统计指标。

Result: 实验揭示了三个明显的去噪阶段：初始噪声探索、快速形状形成和最终细化，不同数据集表现出特定行为（如牛眼图的同心收敛vs恐龙图的复杂轮廓形成）。傅里叶基嵌入提高了轨迹稳定性和重建质量。

Conclusion: InJecteD框架有效提升了DDPM模型的可解释性，通过轨迹分析揭示了去噪过程的动态特性，为模型调试和优化提供了有力工具，傅里叶嵌入方法显示出更好的性能。

Abstract: This work introduces InJecteD, a framework for interpreting Denoising Diffusion Probabilistic Models (DDPMs) by analyzing sample trajectories during the denoising process of 2D point cloud generation. We apply this framework to three datasets from the Datasaurus Dozen bullseye, dino, and circle using a simplified DDPM architecture with customizable input and time embeddings. Our approach quantifies trajectory properties, including displacement, velocity, clustering, and drift field dynamics, using statistical metrics such as Wasserstein distance and cosine similarity. By enhancing model transparency, InJecteD supports human AI collaboration by enabling practitioners to debug and refine generative models. Experiments reveal distinct denoising phases: initial noise exploration, rapid shape formation, and final refinement, with dataset-specific behaviors example, bullseyes concentric convergence vs. dinos complex contour formation. We evaluate four model configurations, varying embeddings and noise schedules, demonstrating that Fourier based embeddings improve trajectory stability and reconstruction quality

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [24] [Neural 3D Object Reconstruction with Small-Scale Unmanned Aerial Vehicles](https://arxiv.org/abs/2509.12458)
*Àlmos Veres-Vitàlyos,Genis Castillo Gomez-Raya,Filip Lemic,Daniel Johannes Bugelnig,Bernhard Rinner,Sergi Abadal,Xavier Costa-Pérez*

Main category: cs.RO

TL;DR: 这篇论文提出了一种用于超小型无人机的自主高精度3D扫描系统，通过双重重建流水线和动态路径调整来提升扫描质量


<details>
  <summary>Details</summary>
Motivation: 解决超小型无人机在负载和自主性方面的限制，以支持在室内和难以达到区域进行高质量3D重建

Method: 采用双重建流水线：近实时SfM点云生成用于动态路径调整，非实时NeRF基于N3DR方法结合UWB位置数据进行精确重建

Result: 在Crazyflie 2.1无人机上验证，动态路径调整在单机和多机配置中均能持续提升重建质量，超越静态飞行路径

Conclusion: 该系统为小型化无人机提供了可扩展的自主解决方案，开启了在受限环境中进行细粒度3D重建的新可能性

Abstract: Small Unmanned Aerial Vehicles (UAVs) exhibit immense potential for navigating indoor and hard-to-reach areas, yet their significant constraints in payload and autonomy have largely prevented their use for complex tasks like high-quality 3-Dimensional (3D) reconstruction. To overcome this challenge, we introduce a novel system architecture that enables fully autonomous, high-fidelity 3D scanning of static objects using UAVs weighing under 100 grams. Our core innovation lies in a dual-reconstruction pipeline that creates a real-time feedback loop between data capture and flight control. A near-real-time (near-RT) process uses Structure from Motion (SfM) to generate an instantaneous pointcloud of the object. The system analyzes the model quality on the fly and dynamically adapts the UAV's trajectory to intelligently capture new images of poorly covered areas. This ensures comprehensive data acquisition. For the final, detailed output, a non-real-time (non-RT) pipeline employs a Neural Radiance Fields (NeRF)-based Neural 3D Reconstruction (N3DR) approach, fusing SfM-derived camera poses with precise Ultra Wide-Band (UWB) location data to achieve superior accuracy. We implemented and validated this architecture using Crazyflie 2.1 UAVs. Our experiments, conducted in both single- and multi-UAV configurations, conclusively show that dynamic trajectory adaptation consistently improves reconstruction quality over static flight paths. This work demonstrates a scalable and autonomous solution that unlocks the potential of miniaturized UAVs for fine-grained 3D reconstruction in constrained environments, a capability previously limited to much larger platforms.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [25] [Generalizable Holographic Reconstruction via Amplitude-Only Diffusion Priors](https://arxiv.org/abs/2509.12728)
*Jeongsol Kim,Chanseok Lee,Jong Chul Ye,Mooseok Jang*

Main category: physics.optics

TL;DR: 基于只需要幅值数据训练的滑散模型，通过预测-纠正采样框架实现光学同时形式的幅值和相位恢复，解决内联全息成像中的非线性逆向问题


<details>
  <summary>Details</summary>
Motivation: 内联全息成像中的相位恢复是一个基础但痴罪的逆向问题，因为幅值和相位在相干成像中存在非线性耦合关系，传统方法需要真实相位数据进行训练

Method: 采用滑散模型训练仅需对象幅值数据，通过预测-纠正采样框架，使用分离的幅值和相位可能性梯度来恢复复杂场

Result: 通过广泛的模拟和实验验证，方法在多种对象形状、成像系统配置和模态下都展现了稳健的沿用性，包括无镜头设置，使用简单幅值数据训练的滑散前矢量成功恢复了复杂的生物组织结构

Conclusion: 该框架为计算成像中的非线性逆向问题提供了一个成本效益高、通用性强的解决方案，并为超越全息成像的更广泛相干成像应用奠定了基础

Abstract: Phase retrieval in inline holography is a fundamental yet ill-posed inverse problem due to the nonlinear coupling between amplitude and phase in coherent imaging. We present a novel off-the-shelf solution that leverages a diffusion model trained solely on object amplitude to recover both amplitude and phase from diffraction intensities. Using a predictor-corrector sampling framework with separate likelihood gradients for amplitude and phase, our method enables complex field reconstruction without requiring ground-truth phase data for training. We validate the proposed approach through extensive simulations and experiments, demonstrating robust generalization across diverse object shapes, imaging system configurations, and modalities, including lensless setups. Notably, a diffusion prior trained on simple amplitude data (e.g., polystyrene beads) successfully reconstructs complex biological tissue structures, highlighting the method's adaptability. This framework provides a cost-effective, generalizable solution for nonlinear inverse problems in computational imaging, and establishes a foundation for broader coherent imaging applications beyond holography.

</details>
