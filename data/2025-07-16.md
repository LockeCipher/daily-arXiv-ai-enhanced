<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 13]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.RO](#cs.RO) [Total: 2]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [CWNet: Causal Wavelet Network for Low-Light Image Enhancement](https://arxiv.org/abs/2507.10689)
*Tongshun Zhang,Pingping Liu,Yubing Lu,Mengen Cai,Zijian Zhang,Zhe Zhang,Qiuzhan Zhou*

Main category: cs.CV

TL;DR: CWNet提出了一种基于小波变换和因果推理的低光图像增强方法，通过全局和局部策略优化语义和频率信息恢复，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统低光图像增强方法忽略实例级语义信息和特征特性，CWNet旨在通过因果推理和小波变换解决这些问题。

Method: 采用因果推理视角和小波变换，包括全局度量学习和局部CLIP语义损失，优化频率信息恢复。

Result: CWNet在多个数据集上显著优于现有方法，表现出强大的泛化能力。

Conclusion: CWNet通过因果推理和小波变换，实现了更精确的低光图像增强，代码已开源。

Abstract: Traditional Low-Light Image Enhancement (LLIE) methods primarily focus on uniform brightness adjustment, often neglecting instance-level semantic information and the inherent characteristics of different features. To address these limitations, we propose CWNet (Causal Wavelet Network), a novel architecture that leverages wavelet transforms for causal reasoning. Specifically, our approach comprises two key components: 1) Inspired by the concept of intervention in causality, we adopt a causal reasoning perspective to reveal the underlying causal relationships in low-light enhancement. From a global perspective, we employ a metric learning strategy to ensure causal embeddings adhere to causal principles, separating them from non-causal confounding factors while focusing on the invariance of causal factors. At the local level, we introduce an instance-level CLIP semantic loss to precisely maintain causal factor consistency. 2) Based on our causal analysis, we present a wavelet transform-based backbone network that effectively optimizes the recovery of frequency information, ensuring precise enhancement tailored to the specific attributes of wavelet transforms. Extensive experiments demonstrate that CWNet significantly outperforms current state-of-the-art methods across multiple datasets, showcasing its robust performance across diverse scenes. Code is available at https://github.com/bywlzts/CWNet-Causal-Wavelet-Network.

</details>


### [2] [Robust ID-Specific Face Restoration via Alignment Learning](https://arxiv.org/abs/2507.10943)
*Yushun Fang,Lu Liu,Xiang Gao,Qiang Hu,Ning Cao,Jianghe Cui,Gang Chen,Xiaoyun Zhang*

Main category: cs.CV

TL;DR: RIDFR是一种基于扩散模型的新型ID特定人脸修复框架，通过内容注入和身份注入模块，结合对齐学习，显著提升了修复结果的ID保真度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前人脸修复方法在视觉质量上取得了进展，但身份模糊输入和随机生成过程导致的不确定性仍未解决。

Method: RIDFR利用预训练扩散模型，结合内容注入模块和身份注入模块，并通过对齐学习抑制ID无关语义的干扰。

Result: 实验表明，RIDFR优于现有方法，能够重建高质量且ID保真度高的结果，并表现出强鲁棒性。

Conclusion: RIDFR为解决身份不确定性提供了一种有效方法，显著提升了人脸修复的ID保真度和鲁棒性。

Abstract: The latest developments in Face Restoration have yielded significant advancements in visual quality through the utilization of diverse diffusion priors. Nevertheless, the uncertainty of face identity introduced by identity-obscure inputs and stochastic generative processes remains unresolved. To address this challenge, we present Robust ID-Specific Face Restoration (RIDFR), a novel ID-specific face restoration framework based on diffusion models. Specifically, RIDFR leverages a pre-trained diffusion model in conjunction with two parallel conditioning modules. The Content Injection Module inputs the severely degraded image, while the Identity Injection Module integrates the specific identity from a given image. Subsequently, RIDFR incorporates Alignment Learning, which aligns the restoration results from multiple references with the same identity in order to suppress the interference of ID-irrelevant face semantics (e.g. pose, expression, make-up, hair style). Experiments demonstrate that our framework outperforms the state-of-the-art methods, reconstructing high-quality ID-specific results with high identity fidelity and demonstrating strong robustness.

</details>


### [3] [Human-Guided Shade Artifact Suppression in CBCT-to-MDCT Translation via Schrödinger Bridge with Conditional Diffusion](https://arxiv.org/abs/2507.11025)
*Sung Ho Kang,Hyun-Cheol Park*

Main category: cs.CV

TL;DR: 提出了一种基于Schrodinger Bridge框架的CBCT-to-MDCT转换方法，结合GAN先验和人类引导的条件扩散，通过边界一致性和人类反馈优化生成结果。


<details>
  <summary>Details</summary>
Motivation: 传统GAN或扩散模型在医学图像转换中难以同时保证解剖学保真度和感知可控性，需要一种更高效且符合临床偏好的方法。

Method: 结合Schrodinger Bridge框架，利用GAN先验和人类反馈（通过CFG）优化生成过程，通过迭代和偏好选择学习人类偏好。

Result: 在临床数据集上，该方法在RMSE、SSIM、LPIPS和Dice指标上表现优异，仅需10次采样步骤。

Conclusion: 该方法高效且符合临床偏好，适用于实时医学图像转换。

Abstract: We present a novel framework for CBCT-to-MDCT translation, grounded in the Schrodinger Bridge (SB) formulation, which integrates GAN-derived priors with human-guided conditional diffusion. Unlike conventional GANs or diffusion models, our approach explicitly enforces boundary consistency between CBCT inputs and pseudo targets, ensuring both anatomical fidelity and perceptual controllability. Binary human feedback is incorporated via classifier-free guidance (CFG), effectively steering the generative process toward clinically preferred outcomes. Through iterative refinement and tournament-based preference selection, the model internalizes human preferences without relying on a reward model. Subtraction image visualizations reveal that the proposed method selectively attenuates shade artifacts in key anatomical regions while preserving fine structural detail. Quantitative evaluations further demonstrate superior performance across RMSE, SSIM, LPIPS, and Dice metrics on clinical datasets -- outperforming prior GAN- and fine-tuning-based feedback methods -- while requiring only 10 sampling steps. These findings underscore the effectiveness and efficiency of our framework for real-time, preference-aligned medical image translation.

</details>


### [4] [Efficient Dual-domain Image Dehazing with Haze Prior Perception](https://arxiv.org/abs/2507.11035)
*Lirong Zheng,Yanshan Li,Rui Yu,Kaihao Zhang*

Main category: cs.CV

TL;DR: 提出了一种双域去雾网络DGFDNet，结合空间域和频域特征，通过物理引导的退化对齐提升性能。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在单图像去雾中表现优异但计算成本高，现有方法依赖空间域特征或频域特征耦合不足。

Method: DGFDNet包含HAFM模块（基于暗通道先验生成雾霾置信图）和MGAM模块（多尺度特征融合），并引入PCGB分支进行迭代优化。

Result: 在四个基准数据集上达到最优性能，兼具鲁棒性和实时性。

Conclusion: DGFDNet通过双域协同和物理引导，显著提升了去雾效果和效率。

Abstract: Transformer-based models exhibit strong global modeling capabilities in single-image dehazing, but their high computational cost limits real-time applicability. Existing methods predominantly rely on spatial-domain features to capture long-range dependencies, which are computationally expensive and often inadequate under complex haze conditions. While some approaches introduce frequency-domain cues, the weak coupling between spatial and frequency branches limits the overall performance. To overcome these limitations, we propose the Dark Channel Guided Frequency-aware Dehazing Network (DGFDNet), a novel dual-domain framework that performs physically guided degradation alignment across spatial and frequency domains. At its core, the DGFDBlock comprises two key modules: 1) the Haze-Aware Frequency Modulator (HAFM), which generates a pixel-level haze confidence map from dark channel priors to adaptively enhance haze-relevant frequency components, thereby achieving global degradation-aware spectral modulation; 2) the Multi-level Gating Aggregation Module (MGAM), which fuses multi-scale features through diverse convolutional kernels and hybrid gating mechanisms to recover fine structural details. Additionally, a Prior Correction Guidance Branch (PCGB) incorporates a closed-loop feedback mechanism, enabling iterative refinement of the prior by intermediate dehazed features and significantly improving haze localization accuracy, especially in challenging outdoor scenes. Extensive experiments on four benchmark haze datasets demonstrate that DGFDNet achieves state-of-the-art performance with superior robustness and real-time efficiency. Code is available at: https://github.com/Dilizlr/DGFDNet.

</details>


### [5] [Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery](https://arxiv.org/abs/2507.11040)
*Nicolas Drapier,Aladine Chetouani,Aurélien Chateigner*

Main category: cs.CV

TL;DR: GLOD是一种基于Transformer的架构，用于高分辨率卫星图像中的目标检测，通过Swin Transformer和新型模块实现高效特征提取，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决高分辨率卫星图像中目标检测的挑战，提升性能并保持计算效率。

Method: 采用Swin Transformer替代CNN主干，结合UpConvMixer块和Fusion Blocks进行多尺度特征整合，创新性地使用CBAM注意力机制和多路径头设计。

Result: 在xView数据集上达到32.95%的准确率，比现有最佳方法高出11.46%。

Conclusion: GLOD通过创新的架构设计，显著提升了卫星图像目标检测的性能和效率。

Abstract: We present GLOD, a transformer-first architecture for object detection in high-resolution satellite imagery. GLOD replaces CNN backbones with a Swin Transformer for end-to-end feature extraction, combined with novel UpConvMixer blocks for robust upsampling and Fusion Blocks for multi-scale feature integration. Our approach achieves 32.95\% on xView, outperforming SOTA methods by 11.46\%. Key innovations include asymmetric fusion with CBAM attention and a multi-path head design capturing objects across scales. The architecture is optimized for satellite imagery challenges, leveraging spatial priors while maintaining computational efficiency.

</details>


### [6] [Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling](https://arxiv.org/abs/2507.11061)
*Hayeon Kim,Ji Ha Jang,Se Young Chun*

Main category: cs.CV

TL;DR: RoMaP是一种新型的局部3D高斯编辑框架，通过3D-GALP模块和正则化SDS损失，实现了精确且大幅度的部分级别3D编辑。


<details>
  <summary>Details</summary>
Motivation: 尽管3D神经表示和实例级编辑模型取得了进展，但实现精确的局部3D编辑仍具挑战性，尤其是在高斯泼溅中。

Method: RoMaP结合了3D-GALP模块（利用球谐系数建模视图依赖标签）和正则化SDS损失（包括L1锚定损失和其他正则化器）。

Result: 实验表明，RoMaP在重建和生成的高斯场景中实现了最先进的局部3D编辑效果。

Conclusion: RoMaP为部分级别3D高斯编辑提供了更强大和灵活的解决方案。

Abstract: Recent advances in 3D neural representations and instance-level editing models have enabled the efficient creation of high-quality 3D content. However, achieving precise local 3D edits remains challenging, especially for Gaussian Splatting, due to inconsistent multi-view 2D part segmentations and inherently ambiguous nature of Score Distillation Sampling (SDS) loss. To address these limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that enables precise and drastic part-level modifications. First, we introduce a robust 3D mask generation module with our 3D-Geometry Aware Label Prediction (3D-GALP), which uses spherical harmonics (SH) coefficients to model view-dependent label variations and soft-label property, yielding accurate and consistent part segmentations across viewpoints. Second, we propose a regularized SDS loss that combines the standard SDS loss with additional regularizers. In particular, an L1 anchor loss is introduced via our Scheduled Latent Mixing and Part (SLaMP) editing method, which generates high-quality part-edited 2D images and confines modifications only to the target region while preserving contextual coherence. Additional regularizers, such as Gaussian prior removal, further improve flexibility by allowing changes beyond the existing context, and robust 3D masking prevents unintended edits. Experimental results demonstrate that our RoMaP achieves state-of-the-art local 3D editing on both reconstructed and generated Gaussian scenes and objects qualitatively and quantitatively, making it possible for more robust and flexible part-level 3D Gaussian editing.

</details>


### [7] [NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models](https://arxiv.org/abs/2507.11245)
*X. Feng,H. Yu,M. Wu,S. Hu,J. Chen,C. Zhu,J. Wu,X. Chu,K. Huang*

Main category: cs.CV

TL;DR: 提出了首个用于评估长视频生成模型叙事表达能力的基准NarrLV，通过引入时间叙事原子（TNA）和基于MLLM的评估指标，实验表明其与人类判断高度一致。


<details>
  <summary>Details</summary>
Motivation: 现有长视频生成模型缺乏专门评估叙事表达能力的基准，导致评估主要依赖简单叙事提示的基准（如VBench）。

Method: 1. 引入时间叙事原子（TNA）作为基本叙事单元；2. 设计自动提示生成管道；3. 基于MLLM框架设计评估指标。

Result: 实验结果表明，NarrLV的评估指标与人类判断高度一致，揭示了当前视频生成模型在叙事表达上的能力边界。

Conclusion: NarrLV是首个全面评估长视频生成模型叙事表达能力的基准，为未来研究提供了重要工具。

Abstract: With the rapid development of foundation video generation technologies, long video generation models have exhibited promising research potential thanks to expanded content creation space. Recent studies reveal that the goal of long video generation tasks is not only to extend video duration but also to accurately express richer narrative content within longer videos. However, due to the lack of evaluation benchmarks specifically designed for long video generation models, the current assessment of these models primarily relies on benchmarks with simple narrative prompts (e.g., VBench). To the best of our knowledge, our proposed NarrLV is the first benchmark to comprehensively evaluate the Narrative expression capabilities of Long Video generation models. Inspired by film narrative theory, (i) we first introduce the basic narrative unit maintaining continuous visual presentation in videos as Temporal Narrative Atom (TNA), and use its count to quantitatively measure narrative richness. Guided by three key film narrative elements influencing TNA changes, we construct an automatic prompt generation pipeline capable of producing evaluation prompts with a flexibly expandable number of TNAs. (ii) Then, based on the three progressive levels of narrative content expression, we design an effective evaluation metric using the MLLM-based question generation and answering framework. (iii) Finally, we conduct extensive evaluations on existing long video generation models and the foundation generation models. Experimental results demonstrate that our metric aligns closely with human judgments. The derived evaluation outcomes reveal the detailed capability boundaries of current video generation models in narrative content expression.

</details>


### [8] [MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection](https://arxiv.org/abs/2507.11252)
*Guanghao Wu,Chen Xu,Hai Song,Chong Wang,Qixing Zhang*

Main category: cs.CV

TL;DR: 提出了一种生成森林火灾烟雾图像的框架，通过改进修复模型和引入新损失函数，生成高质量烟雾图像，提升烟雾检测性能。


<details>
  <summary>Details</summary>
Motivation: 森林火灾烟雾图像数据稀缺，现有修复模型生成烟雾与背景不一致，限制了烟雾检测效果。

Method: 结合预训练分割模型和多模态模型获取烟雾掩码和图像描述，设计掩码和掩码图像特征引导的网络架构，提出掩码随机差异损失函数，利用多模态大语言模型筛选图像。

Result: 生成的烟雾图像真实多样，显著提升了烟雾检测模型的性能。

Conclusion: 提出的框架有效解决了烟雾图像生成问题，为烟雾检测任务提供了高质量数据集。

Abstract: Smoke is the first visible indicator of a wildfire.With the advancement of deep learning, image-based smoke detection has become a crucial method for detecting and preventing forest fires. However, the scarcity of smoke image data from forest fires is one of the significant factors hindering the detection of forest fire smoke. Image generation models offer a promising solution for synthesizing realistic smoke images. However, current inpainting models exhibit limitations in generating high-quality smoke representations, particularly manifesting as inconsistencies between synthesized smoke and background contexts. To solve these problems, we proposed a comprehensive framework for generating forest fire smoke images. Firstly, we employed the pre-trained segmentation model and the multimodal model to obtain smoke masks and image captions.Then, to address the insufficient utilization of masks and masked images by inpainting models, we introduced a network architecture guided by mask and masked image features. We also proposed a new loss function, the mask random difference loss, which enhances the consistency of the generated effects around the mask by randomly expanding and eroding the mask edges.Finally, to generate a smoke image dataset using random masks for subsequent detection tasks, we incorporated smoke characteristics and use a multimodal large language model as a filtering tool to select diverse and reasonable smoke images, thereby improving the quality of the synthetic dataset. Experiments showed that our generated smoke images are realistic and diverse, and effectively enhance the performance of forest fire smoke detection models. Code is available at https://github.com/wghr123/MFGDiffusion.

</details>


### [9] [A Mixed-Primitive-based Gaussian Splatting Method for Surface Reconstruction](https://arxiv.org/abs/2507.11321)
*Haoxuan Qu,Yujun Cai,Hossein Rahmani,Ajay Kumar,Junsong Yuan,Jun Liu*

Main category: cs.CV

TL;DR: 论文提出了一种新的高斯泼溅框架，首次在表面重建过程中引入多种几何基元，提高了重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有高斯泼溅方法仅使用单一类型的基元（高斯椭圆或椭球）表示复杂多样的物体表面，导致重建质量不足。

Method: 提出组合泼溅策略、混合基元初始化策略和顶点修剪机制，支持多种基元的高斯泼溅流程。

Result: 实验证明框架有效，实现了精确的表面重建。

Conclusion: 多基元高斯泼溅框架显著提升了表面重建质量，为复杂物体表示提供了新思路。

Abstract: Recently, Gaussian Splatting (GS) has received a lot of attention in surface reconstruction. However, while 3D objects can be of complex and diverse shapes in the real world, existing GS-based methods only limitedly use a single type of splatting primitive (Gaussian ellipse or Gaussian ellipsoid) to represent object surfaces during their reconstruction. In this paper, we highlight that this can be insufficient for object surfaces to be represented in high quality. Thus, we propose a novel framework that, for the first time, enables Gaussian Splatting to incorporate multiple types of (geometrical) primitives during its surface reconstruction process. Specifically, in our framework, we first propose a compositional splatting strategy, enabling the splatting and rendering of different types of primitives in the Gaussian Splatting pipeline. In addition, we also design our framework with a mixed-primitive-based initialization strategy and a vertex pruning mechanism to further promote its surface representation learning process to be well executed leveraging different types of primitives. Extensive experiments show the efficacy of our framework and its accurate surface reconstruction performance.

</details>


### [10] [Implementing Adaptations for Vision AutoRegressive Model](https://arxiv.org/abs/2507.11441)
*Kaif Shaikh,Antoni Kowalczuk,Franziska Boenisch,Adam Dziedzic*

Main category: cs.CV

TL;DR: 本文研究了Vision AutoRegressive模型（VAR）在图像生成任务中的适应性，特别是与Diffusion Models（DMs）的比较，并探讨了差分隐私（DP）适应性在VAR中的表现。


<details>
  <summary>Details</summary>
Motivation: VAR作为DMs的替代方案在图像生成领域表现优异，但其适应性和隐私保护技术尚未充分研究。

Method: 实现并比较了VAR的多种适应性策略，包括差分隐私适应性，并与DMs的现有策略进行对比。

Result: VAR在非差分隐私适应性任务中优于DMs，但在差分隐私适应性中表现不佳。

Conclusion: VAR在适应性任务中具有潜力，但差分隐私适应性仍需进一步研究。

Abstract: Vision AutoRegressive model (VAR) was recently introduced as an alternative to Diffusion Models (DMs) in image generation domain. In this work we focus on its adaptations, which aim to fine-tune pre-trained models to perform specific downstream tasks, like medical data generation. While for DMs there exist many techniques, adaptations for VAR remain underexplored. Similarly, differentially private (DP) adaptations-ones that aim to preserve privacy of the adaptation data-have been extensively studied for DMs, while VAR lacks such solutions. In our work, we implement and benchmark many strategies for VAR, and compare them to state-of-the-art DM adaptation strategies. We observe that VAR outperforms DMs for non-DP adaptations, however, the performance of DP suffers, which necessitates further research in private adaptations for VAR. Code is available at https://github.com/sprintml/finetuning_var_dp.

</details>


### [11] [HUG-VAS: A Hierarchical NURBS-Based Generative Model for Aortic Geometry Synthesis and Controllable Editing](https://arxiv.org/abs/2507.11474)
*Pan Du,Mingqi Xu,Xiaozhi Zhu,Jian-xun Wang*

Main category: cs.CV

TL;DR: HUG-VAS是一种基于NURBS和扩散模型的层次化生成模型，用于合成真实、细粒度的主动脉几何结构，解决了传统线性SSM方法在复杂拓扑结构中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统统计形状建模（SSM）方法依赖线性假设，难以表达复杂的多分支血管结构。HUG-VAS旨在通过结合NURBS参数化和扩散模型，生成更真实的血管几何。

Method: HUG-VAS采用层次化架构，包括生成中心线的去噪扩散模型和基于中心线生成径向剖面的引导扩散模型，支持零样本条件生成。

Result: 模型在21个患者样本上训练，生成的主动脉结构具有解剖学保真度，生物标志物分布与原始数据集高度匹配。

Conclusion: HUG-VAS首次通过NURBS参数化和层次化扩散过程，将图像先验与生成形状建模统一起来，具有实际应用潜力。

Abstract: Accurate characterization of vascular geometry is essential for cardiovascular diagnosis and treatment planning. Traditional statistical shape modeling (SSM) methods rely on linear assumptions, limiting their expressivity and scalability to complex topologies such as multi-branch vascular structures. We introduce HUG-VAS, a Hierarchical NURBS Generative model for Vascular geometry Synthesis, which integrates NURBS surface parameterization with diffusion-based generative modeling to synthesize realistic, fine-grained aortic geometries. Trained with 21 patient-specific samples, HUG-VAS generates anatomically faithful aortas with supra-aortic branches, yielding biomarker distributions that closely match those of the original dataset. HUG-VAS adopts a hierarchical architecture comprising a denoising diffusion model that generates centerlines and a guided diffusion model that synthesizes radial profiles conditioned on those centerlines, thereby capturing two layers of anatomical variability. Critically, the framework supports zero-shot conditional generation from image-derived priors, enabling practical applications such as interactive semi-automatic segmentation, robust reconstruction under degraded imaging conditions, and implantable device optimization. To our knowledge, HUG-VAS is the first SSM framework to bridge image-derived priors with generative shape modeling via a unified integration of NURBS parameterization and hierarchical diffusion processes.

</details>


### [12] [CATVis: Context-Aware Thought Visualization](https://arxiv.org/abs/2507.11522)
*Tariq Mehmood,Hamza Ahmad,Muhammad Haroon Shakeel,Murtaza Taj*

Main category: cs.CV

TL;DR: 提出一种5阶段框架，通过EEG信号解码视觉表征，结合跨模态对齐和图像生成技术，显著提升图像质量和语义对齐。


<details>
  <summary>Details</summary>
Motivation: EEG信号解码视觉表征的复杂性和噪声问题，需要一种更有效的方法来生成高质量且语义对齐的图像。

Method: 1) EEG编码器分类概念；2) EEG与文本嵌入在CLIP特征空间对齐；3) 通过重排优化标题；4) 加权插值概念与标题嵌入；5) 使用Stable Diffusion生成图像。

Result: 方法在分类准确率、生成准确率和图像质量上显著优于现有技术，分别提升13.43%、15.21%和降低36.61%的FID。

Conclusion: 提出的框架在EEG信号解码视觉表征任务中表现出色，为脑机接口应用提供了新的可能性。

Abstract: EEG-based brain-computer interfaces (BCIs) have shown promise in various applications, such as motor imagery and cognitive state monitoring. However, decoding visual representations from EEG signals remains a significant challenge due to their complex and noisy nature. We thus propose a novel 5-stage framework for decoding visual representations from EEG signals: (1) an EEG encoder for concept classification, (2) cross-modal alignment of EEG and text embeddings in CLIP feature space, (3) caption refinement via re-ranking, (4) weighted interpolation of concept and caption embeddings for richer semantics, and (5) image generation using a pre-trained Stable Diffusion model. We enable context-aware EEG-to-image generation through cross-modal alignment and re-ranking. Experimental results demonstrate that our method generates high-quality images aligned with visual stimuli, outperforming SOTA approaches by 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and reducing Fr\'echet Inception Distance by 36.61%, indicating superior semantic alignment and image quality.

</details>


### [13] [CharaConsist: Fine-Grained Consistent Character Generation](https://arxiv.org/abs/2507.11533)
*Mengyu Wang,Henghui Ding,Jianing Peng,Yao Zhao,Yunpeng Chen,Yunchao Wei*

Main category: cs.CV

TL;DR: CharaConsist提出了一种新方法，通过点跟踪注意力和自适应令牌合并，解决了文本到图像生成中前后景一致性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成一致内容时无法保持背景细节和身份一致性，限制了实际应用。

Method: 采用点跟踪注意力和自适应令牌合并，解耦前后景控制。

Result: 实现了前后景的细粒度一致性，支持连续或离散场景中的角色生成。

Conclusion: CharaConsist是首个针对DiT模型的一致性生成方法，扩展了实际应用范围。

Abstract: In text-to-image generation, producing a series of consistent contents that preserve the same identity is highly valuable for real-world applications. Although a few works have explored training-free methods to enhance the consistency of generated subjects, we observe that they suffer from the following problems. First, they fail to maintain consistent background details, which limits their applicability. Furthermore, when the foreground character undergoes large motion variations, inconsistencies in identity and clothing details become evident. To address these problems, we propose CharaConsist, which employs point-tracking attention and adaptive token merge along with decoupled control of the foreground and background. CharaConsist enables fine-grained consistency for both foreground and background, supporting the generation of one character in continuous shots within a fixed scene or in discrete shots across different scenes. Moreover, CharaConsist is the first consistent generation method tailored for text-to-image DiT model. Its ability to maintain fine-grained consistency, combined with the larger capacity of latest base model, enables it to produce high-quality visual outputs, broadening its applicability to a wider range of real-world scenarios. The source code has been released at https://github.com/Murray-Wang/CharaConsist

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [14] [Flows and Diffusions on the Neural Manifold](https://arxiv.org/abs/2507.10623)
*Daniel Saragih,Deyu Cao,Tejas Balaji*

Main category: cs.LG

TL;DR: 论文提出了一种基于扩散和流模型的权重空间学习方法，通过优化动态的结构先验，统一了轨迹推断技术，并在实验中展示了优越性能。


<details>
  <summary>Details</summary>
Motivation: 将扩散和流生成模型的成功扩展到权重空间学习，利用优化动态的结构先验提升生成和初始化效果。

Method: 将梯度下降轨迹建模为轨迹推断问题，提出梯度流匹配框架，结合自编码器、任务上下文条件和Kaiming均匀分布等技术。

Result: 方法在生成分布内权重、下游训练初始化和性能微调方面优于基线，并在安全关键系统中检测有害协变量偏移时表现突出。

Conclusion: 论文为权重空间学习提供了理论框架和实用方法，展示了在生成和初始化任务中的潜力。

Abstract: Diffusion and flow-based generative models have achieved remarkable success in domains such as image synthesis, video generation, and natural language modeling. In this work, we extend these advances to weight space learning by leveraging recent techniques to incorporate structural priors derived from optimization dynamics. Central to our approach is modeling the trajectory induced by gradient descent as a trajectory inference problem. We unify several trajectory inference techniques under the framework of gradient flow matching, providing a theoretical framework for treating optimization paths as inductive bias. We further explore architectural and algorithmic choices, including reward fine-tuning by adjoint matching, the use of autoencoders for latent weight representation, conditioning on task-specific context data, and adopting informative source distributions such as Kaiming uniform. Experiments demonstrate that our method matches or surpasses baselines in generating in-distribution weights, improves initialization for downstream training, and supports fine-tuning to enhance performance. Finally, we illustrate a practical application in safety-critical systems: detecting harmful covariate shifts, where our method outperforms the closest comparable baseline.

</details>


### [15] [Spatial Reasoners for Continuous Variables in Any Domain](https://arxiv.org/abs/2507.10768)
*Bart Pogodzinski,Christopher Wewer,Bernt Schiele,Jan Eric Lenssen*

Main category: cs.LG

TL;DR: Spatial Reasoners是一个用于处理连续变量空间推理的软件框架，基于生成去噪模型。


<details>
  <summary>Details</summary>
Motivation: 由于生成去噪模型在复杂高维分布采样中的有效性，已成为图像生成的标准方法，但其在多连续变量推理中的应用研究基础设施需求高。

Method: 框架提供易用接口，支持变量映射、生成模型范式和推理策略的控制。

Result: Spatial Reasoners框架已开源，旨在促进该领域研究。

Conclusion: 该框架简化了生成推理研究，支持多样化应用。

Abstract: We present Spatial Reasoners, a software framework to perform spatial reasoning over continuous variables with generative denoising models. Denoising generative models have become the de-facto standard for image generation, due to their effectiveness in sampling from complex, high-dimensional distributions. Recently, they have started being explored in the context of reasoning over multiple continuous variables. Providing infrastructure for generative reasoning with such models requires a high effort, due to a wide range of different denoising formulations, samplers, and inference strategies. Our presented framework aims to facilitate research in this area, providing easy-to-use interfaces to control variable mapping from arbitrary data domains, generative model paradigms, and inference strategies. Spatial Reasoners are openly available at https://spatialreasoners.github.io/

</details>


### [16] [LogTinyLLM: Tiny Large Language Models Based Contextual Log Anomaly Detection](https://arxiv.org/abs/2507.11071)
*Isaiah Thompson Ocansey,Ritwik Bhattacharya,Tanmay Sen*

Main category: cs.LG

TL;DR: 论文提出了一种基于LoRA和适配器的高效参数微调方法，用于检测大规模日志数据中的异常序列，性能显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则或深度学习的日志异常检测方法在处理大规模复杂日志序列时效果不佳，因此需要更高效的方法。

Method: 采用参数高效微调技术（如LoRA和适配器），在Thunderbird数据集上比较不同小型大语言模型（LLMs）。

Result: LoRA微调比LogBert全微调性能提升18-19%，准确率达到97.76%-98.83%，而后者仅为79.37%。

Conclusion: LoRA微调方法在日志异常检测中表现优异，显著提升了检测性能。

Abstract: Log anomaly detection using traditional rule based or deep learning based methods is often challenging due to the large volume and highly complex nature of log sequence. So effective way of detection of anomalous sequence of logs is crucial for system maintenance and development. This paper proposes parameter efficient finetuning specifically low rank adaptation (LoRA) and adapter based approaches for finding contextual anomalies in sequence of logs in large log data set. It compares different tiny large language models (LLMs) on the Thunderbird dataset. The results show that LoRA based finetuning provides substantial performance improvements of 18 to 19 percentage over LogBert based full finetuning approach, achieving accuracy scores between 97.76% and 98.83% compared to 79.37%.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [17] [Learning to Tune Like an Expert: Interpretable and Scene-Aware Navigation via MLLM Reasoning and CVAE-Based Adaptation](https://arxiv.org/abs/2507.11001)
*Yanbo Wang,Zipeng Fang,Lei Zhao,Weidong Chen*

Main category: cs.RO

TL;DR: LE-Nav是一种基于多模态大语言模型和条件变分自编码器的导航框架，通过自适应调整规划器超参数，提升机器人在动态环境中的导航性能。


<details>
  <summary>Details</summary>
Motivation: 传统导航系统在动态和非结构化环境中表现不佳，现有强化学习方法因泛化能力差和模拟多样性不足而难以实际应用。

Method: 结合多模态大语言模型推理和条件变分自编码器，利用单次示例和思维链提示策略实现零样本场景理解，自适应调整超参数。

Result: 实验表明，LE-Nav生成的超参数在多样场景中达到人类水平，实际导航测试和用户研究显示其在成功率、效率、安全性和舒适性上优于现有方法。

Conclusion: LE-Nav通过自适应超参数调整，显著提升了导航性能和社会接受度，适用于动态和非结构化环境。

Abstract: Service robots are increasingly deployed in diverse and dynamic environments, where both physical layouts and social contexts change over time and across locations. In these unstructured settings, conventional navigation systems that rely on fixed parameters often fail to generalize across scenarios, resulting in degraded performance and reduced social acceptance. Although recent approaches have leveraged reinforcement learning to enhance traditional planners, these methods often fail in real-world deployments due to poor generalization and limited simulation diversity, which hampers effective sim-to-real transfer. To tackle these issues, we present LE-Nav, an interpretable and scene-aware navigation framework that leverages multi-modal large language model reasoning and conditional variational autoencoders to adaptively tune planner hyperparameters. To achieve zero-shot scene understanding, we utilize one-shot exemplars and chain-of-thought prompting strategies. Additionally, a conditional variational autoencoder captures the mapping between natural language instructions and navigation hyperparameters, enabling expert-level tuning. Experiments show that LE-Nav can generate hyperparameters achieving human-level tuning across diverse planners and scenarios. Real-world navigation trials and a user study on a smart wheelchair platform demonstrate that it outperforms state-of-the-art methods on quantitative metrics such as success rate, efficiency, safety, and comfort, while receiving higher subjective scores for perceived safety and social acceptance. Code is available at https://github.com/Cavendish518/LE-Nav.

</details>


### [18] [TRAN-D: 2D Gaussian Splatting-based Sparse-view Transparent Object Depth Reconstruction via Physics Simulation for Scene Update](https://arxiv.org/abs/2507.11069)
*Jeongyun Kim,Seunghoon Jeong,Giseop Kim,Myung-Hwan Jeon,Eunji Jun,Ayoung Kim*

Main category: cs.RO

TL;DR: TRAN-D是一种基于2D高斯泼溅的透明物体深度重建方法，通过分离透明物体与背景并优化高斯分布，结合物理模拟，显著提升了稀疏视图和动态环境下的重建效果。


<details>
  <summary>Details</summary>
Motivation: 透明物体的3D几何重建因反射和折射等物理特性而极具挑战性，尤其是在稀疏视图和动态环境中。

Method: TRAN-D通过分离透明物体与背景，优化对应高斯分布，并引入物体感知损失和物理模拟，减少伪影并提升重建精度。

Result: 在合成和真实场景中，TRAN-D比现有方法平均绝对误差降低39%，单图像更新时精度提升1.5倍。

Conclusion: TRAN-D在透明物体深度重建中表现出色，显著优于现有方法，适用于动态和稀疏视图场景。

Abstract: Understanding the 3D geometry of transparent objects from RGB images is challenging due to their inherent physical properties, such as reflection and refraction. To address these difficulties, especially in scenarios with sparse views and dynamic environments, we introduce TRAN-D, a novel 2D Gaussian Splatting-based depth reconstruction method for transparent objects. Our key insight lies in separating transparent objects from the background, enabling focused optimization of Gaussians corresponding to the object. We mitigate artifacts with an object-aware loss that places Gaussians in obscured regions, ensuring coverage of invisible surfaces while reducing overfitting. Furthermore, we incorporate a physics-based simulation that refines the reconstruction in just a few seconds, effectively handling object removal and chain-reaction movement of remaining objects without the need for rescanning. TRAN-D is evaluated on both synthetic and real-world sequences, and it consistently demonstrated robust improvements over existing GS-based state-of-the-art methods. In comparison with baselines, TRAN-D reduces the mean absolute error by over 39% for the synthetic TRansPose sequences. Furthermore, despite being updated using only one image, TRAN-D reaches a {\delta} < 2.5 cm accuracy of 48.46%, over 1.5 times that of baselines, which uses six images. Code and more results are available at https://jeongyun0609.github.io/TRAN-D/.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [19] [Latent Space Consistency for Sparse-View CT Reconstruction](https://arxiv.org/abs/2507.11152)
*Duoyou Chen,Yunqing Chen,Can Zhang,Zhou Wang,Cheng Chen,Ruoxiu Xiao*

Main category: eess.IV

TL;DR: 提出了一种名为CLS-DM的模型，通过跨模态特征对比学习，解决2D X射线图像与3D CT图像潜在空间对齐问题，提升稀疏X射线重建CT的效果。


<details>
  <summary>Details</summary>
Motivation: 稀疏X射线图像重建CT可减少时间和辐射风险，但现有方法在潜在空间对齐上存在不足。

Method: 提出CLS-DM模型，利用跨模态特征对比学习，从2D X射线图像中提取3D信息并实现潜在空间对齐。

Result: 在LIDC-IDRI和CTSpine1K数据集上，CLS-DM在PSNR和SSIM指标上优于现有生成模型。

Conclusion: CLS-DM不仅提升了稀疏X射线重建CT的效果，还可推广到其他跨模态任务，如文本到图像合成。

Abstract: Computed Tomography (CT) is a widely utilized imaging modality in clinical settings. Using densely acquired rotational X-ray arrays, CT can capture 3D spatial features. However, it is confronted with challenged such as significant time consumption and high radiation exposure. CT reconstruction methods based on sparse-view X-ray images have garnered substantial attention from researchers as they present a means to mitigate costs and risks. In recent years, diffusion models, particularly the Latent Diffusion Model (LDM), have demonstrated promising potential in the domain of 3D CT reconstruction. Nonetheless, due to the substantial differences between the 2D latent representation of X-ray modalities and the 3D latent representation of CT modalities, the vanilla LDM is incapable of achieving effective alignment within the latent space. To address this issue, we propose the Consistent Latent Space Diffusion Model (CLS-DM), which incorporates cross-modal feature contrastive learning to efficiently extract latent 3D information from 2D X-ray images and achieve latent space alignment between modalities. Experimental results indicate that CLS-DM outperforms classical and state-of-the-art generative models in terms of standard voxel-level metrics (PSNR, SSIM) on the LIDC-IDRI and CTSpine1K datasets. This methodology not only aids in enhancing the effectiveness and economic viability of sparse X-ray reconstructed CT but can also be generalized to other cross-modal transformation tasks, such as text-to-image synthesis. We have made our code publicly available at https://anonymous.4open.science/r/CLS-DM-50D6/ to facilitate further research and applications in other domains.

</details>
