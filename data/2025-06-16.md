<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 4]
- [cs.CV](#cs.CV) [Total: 12]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [eess.IV](#eess.IV) [Total: 7]
- [cs.CR](#cs.CR) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Anti-Aliased 2D Gaussian Splatting](https://arxiv.org/abs/2506.11252)
*Mae Younes,Adnane Boukhayma*

Main category: cs.GR

TL;DR: AA-2DGS提出了一种抗锯齿的2D高斯泼溅方法，解决了2DGS在不同采样率下的渲染问题。


<details>
  <summary>Details</summary>
Motivation: 2DGS在新视角合成和表面重建中表现优异，但在不同采样率下会出现严重的锯齿问题，限制了其实际应用。

Method: 引入世界空间平滑核和对象空间Mip滤波器，约束高斯基元的频率内容并优化抗锯齿效果。

Result: AA-2DGS显著提升了不同尺度下的渲染质量，消除了高频锯齿。

Conclusion: AA-2DGS在保持几何优势的同时，有效解决了2DGS的锯齿问题，扩展了其应用场景。

Abstract: 2D Gaussian Splatting (2DGS) has recently emerged as a promising method for novel view synthesis and surface reconstruction, offering better view-consistency and geometric accuracy than volumetric 3DGS. However, 2DGS suffers from severe aliasing artifacts when rendering at different sampling rates than those used during training, limiting its practical applications in scenarios requiring camera zoom or varying fields of view. We identify that these artifacts stem from two key limitations: the lack of frequency constraints in the representation and an ineffective screen-space clamping approach. To address these issues, we present AA-2DGS, an antialiased formulation of 2D Gaussian Splatting that maintains its geometric benefits while significantly enhancing rendering quality across different scales. Our method introduces a world space flat smoothing kernel that constrains the frequency content of 2D Gaussian primitives based on the maximal sampling frequency from training views, effectively eliminating high-frequency artifacts when zooming in. Additionally, we derive a novel object space Mip filter by leveraging an affine approximation of the ray-splat intersection mapping, which allows us to efficiently apply proper anti-aliasing directly in the local space of each splat.

</details>


### [2] [On Ray Reordering Techniques for Faster GPU Ray Tracing](https://arxiv.org/abs/2506.11273)
*Daniel Meister,Jakub Bokšanský,Michael Guthe,Jiří Bittner*

Main category: cs.GR

TL;DR: 研究射线重排序以提高GPU射线追踪性能，提出一种基于终止点估计的新方法，并在RTX内核中验证其效果。


<details>
  <summary>Details</summary>
Motivation: 提高现有GPU射线追踪实现的性能，尤其是针对二次射线的追踪。

Method: 提出一种基于终止点估计的射线重排序方法，适用于波前路径追踪和RTX内核。

Result: 射线重排序显著提升追踪速度（1.3-2.0倍），但硬件加速阶段的排序开销难以完全抵消。

Conclusion: 射线重排序在提升性能方面有效，但需进一步优化以平衡排序开销。

Abstract: We study ray reordering as a tool for increasing the performance of existing GPU ray tracing implementations. We focus on ray reordering that is fully agnostic to the particular trace kernel. We summarize the existing methods for computing the ray sorting keys and discuss their properties. We propose a novel modification of a previously proposed method using the termination point estimation that is well-suited to tracing secondary rays. We evaluate the ray reordering techniques in the context of the wavefront path tracing using the RTX trace kernels. We show that ray reordering yields significantly higher trace speed on recent GPUs (1.3-2.0x), but to recover the reordering overhead in the hardware-accelerated trace phase is problematic.

</details>


### [3] [Adaptive Tetrahedral Grids for Volumetric Path-Tracing](https://arxiv.org/abs/2506.11510)
*Anis Benyoub,Jonathan Dupuy*

Main category: cs.GR

TL;DR: 论文提出使用最长边二分算法构建的四面体网格进行体积数据路径追踪渲染，其优势在于高适应性空间分区和低内存占用，GPU实现性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 传统网格在体积数据渲染中存在内存占用高和适应性不足的问题，四面体网格通过其独特结构提供更高效的解决方案。

Method: 采用最长边二分算法构建四面体网格，并设计优化的GPU算法和数据结构，支持路径追踪。

Result: GPU实现性能比常规网格提升高达30倍，支持实时渲染32样本/像素的生产级资产。

Conclusion: 四面体网格在体积数据渲染中具有显著优势，适合高性能GPU实现。

Abstract: We advertise the use of tetrahedral grids constructed via the longest edge bisection algorithm for rendering volumetric data with path tracing. The key benefits of such grids is two-fold. First, they provide a highly adaptive space-partitioning representation that limits the memory footprint of volumetric assets. Second, each (tetrahedral) cell has exactly 4 neighbors within the volume (one per face of each tetrahedron) or less at boundaries. We leverage these properties to devise optimized algorithms and data-structures to compute and path-trace adaptive tetrahedral grids on the GPU. In practice, our GPU implementation outperforms regular grids by up to x30 and renders production assets in real time at 32 samples per pixel.

</details>


### [4] [CGVQM+D: Computer Graphics Video Quality Metric and Dataset](https://arxiv.org/abs/2506.11546)
*Akshay Jindal,Nabil Sadaka,Manu Mathew Thomas,Anton Sochenov,Anton Kaplanyan*

Main category: cs.GR

TL;DR: 论文提出了一个专注于高级渲染技术引入的失真的视频质量数据集，并开发了新的质量评估指标CGVQM。


<details>
  <summary>Details</summary>
Motivation: 现有视频和图像质量数据集主要研究自然视频和传统失真，对合成内容和现代渲染失真的感知研究不足。

Method: 提出了一个新型视频质量数据集，评估了神经超采样、新视角合成等高级渲染技术的失真，并开发了CGVQM指标。

Result: 现有全参考质量指标在这些失真上表现不佳（最大Pearson相关系数为0.78），CGVQM显著优于现有指标。

Conclusion: CGVQM在评估渲染失真时表现优异，数据集和指标实现已开源。

Abstract: While existing video and image quality datasets have extensively studied natural videos and traditional distortions, the perception of synthetic content and modern rendering artifacts remains underexplored. We present a novel video quality dataset focused on distortions introduced by advanced rendering techniques, including neural supersampling, novel-view synthesis, path tracing, neural denoising, frame interpolation, and variable rate shading. Our evaluations show that existing full-reference quality metrics perform sub-optimally on these distortions, with a maximum Pearson correlation of 0.78. Additionally, we find that the feature space of pre-trained 3D CNNs aligns strongly with human perception of visual quality. We propose CGVQM, a full-reference video quality metric that significantly outperforms existing metrics while generating both per-pixel error maps and global quality scores. Our dataset and metric implementation is available at https://github.com/IntelLabs/CGVQM.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [5] [Adaptive Object Detection with ESRGAN-Enhanced Resolution & Faster R-CNN](https://arxiv.org/abs/2506.11122)
*Divya Swetha K,Ziaul Haque Choudhury,Hemanta Kumar Bhuyan,Biswajit Brahma,Nilayam Kumar Kamila*

Main category: cs.CV

TL;DR: 提出了一种结合ESRGAN和Faster R-CNN的方法，用于从低分辨率图像中提升物体检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决低分辨率图像中物体检测性能不佳的问题，适用于图像质量不一致的应用场景。

Method: 使用ESRGAN作为预处理步骤增强图像质量，再通过Faster R-CNN进行物体检测。

Result: 实验表明，该方法在低分辨率图像上的检测性能优于传统方法。

Conclusion: 该框架在图像质量受限的场景中提供了更鲁棒和可靠的物体检测解决方案。

Abstract: In this study, proposes a method for improved object detection from the low-resolution images by integrating Enhanced Super-Resolution Generative Adversarial Networks (ESRGAN) and Faster Region-Convolutional Neural Network (Faster R-CNN). ESRGAN enhances low-quality images, restoring details and improving clarity, while Faster R-CNN performs accurate object detection on the enhanced images. The combination of these techniques ensures better detection performance, even with poor-quality inputs, offering an effective solution for applications where image resolution is in consistent. ESRGAN is employed as a pre-processing step to enhance the low-resolution input image, effectively restoring lost details and improving overall image quality. Subsequently, the enhanced image is fed into the Faster R-CNN model for accurate object detection and localization. Experimental results demonstrate that this integrated approach yields superior performance compared to traditional methods applied directly to low-resolution images. The proposed framework provides a promising solution for applications where image quality is variable or limited, enabling more robust and reliable object detection in challenging scenarios. It achieves a balance between improved image quality and efficient object detection

</details>


### [6] [Technical Report for Argoverse2 Scenario Mining Challenges on Iterative Error Correction and Spatially-Aware Prompting](https://arxiv.org/abs/2506.11124)
*Yifei Chen,Ross Greer*

Main category: cs.CV

TL;DR: RefAV框架通过LLM将自然语言查询转化为可执行代码以挖掘自动驾驶场景，但面临代码错误和参数解释不准确的问题。本文提出两种改进方法：容错迭代代码生成和专用提示工程，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成代码的运行时错误和复杂空间关系参数解释不准确的问题，提升自动驾驶场景挖掘的可靠性。

Method: 1. 容错迭代代码生成机制，通过错误反馈重新提示LLM优化代码；2. 专用提示工程，提高LLM对空间关系函数的理解和应用准确性。

Result: 在Argoverse 2验证集上，使用多种LLM（Qwen2.5-VL-7B、Gemini 2.5 Flash和Gemini 2.5 Pro）均取得一致提升，Gemini 2.5 Pro的HOTA-Temporal得分达52.37。

Conclusion: 提出的方法显著提高了自动驾驶场景挖掘的可靠性和精度，验证了其有效性。

Abstract: Scenario mining from extensive autonomous driving datasets, such as Argoverse 2, is crucial for the development and validation of self-driving systems. The RefAV framework represents a promising approach by employing Large Language Models (LLMs) to translate natural-language queries into executable code for identifying relevant scenarios. However, this method faces challenges, including runtime errors stemming from LLM-generated code and inaccuracies in interpreting parameters for functions that describe complex multi-object spatial relationships. This technical report introduces two key enhancements to address these limitations: (1) a fault-tolerant iterative code-generation mechanism that refines code by re-prompting the LLM with error feedback, and (2) specialized prompt engineering that improves the LLM's comprehension and correct application of spatial-relationship functions. Experiments on the Argoverse 2 validation set with diverse LLMs-Qwen2.5-VL-7B, Gemini 2.5 Flash, and Gemini 2.5 Pro-show consistent gains across multiple metrics; most notably, the proposed system achieves a HOTA-Temporal score of 52.37 on the official test set using Gemini 2.5 Pro. These results underline the efficacy of the proposed techniques for reliable, high-precision scenario mining.

</details>


### [7] [JAFAR: Jack up Any Feature at Any Resolution](https://arxiv.org/abs/2506.11136)
*Paul Couairon,Loick Chambon,Louis Serrano,Jean-Emmanuel Haugeard,Matthieu Cord,Nicolas Thome*

Main category: cs.CV

TL;DR: JAFAR是一种轻量级、灵活的特征上采样器，用于提升基础视觉编码器的空间分辨率，无需高分辨率监督即可泛化到更高输出尺度。


<details>
  <summary>Details</summary>
Motivation: 基础视觉编码器的低分辨率空间特征输出需要上采样以满足下游任务的高分辨率需求。

Method: JAFAR采用基于注意力的模块，通过空间特征变换（SFT）调制，促进高分辨率查询与语义丰富的低分辨率键之间的对齐。

Result: 实验表明，JAFAR能有效恢复细粒度空间细节，并在多种下游任务中优于现有上采样方法。

Conclusion: JAFAR是一种高效的特征上采样解决方案，适用于广泛的下游任务。

Abstract: Foundation Vision Encoders have become essential for a wide range of dense vision tasks. However, their low-resolution spatial feature outputs necessitate feature upsampling to produce the high-resolution modalities required for downstream tasks. In this work, we introduce JAFAR, a lightweight and flexible feature upsampler that enhances the spatial resolution of visual features from any Foundation Vision Encoder to an arbitrary target resolution. JAFAR employs an attention-based module designed to promote semantic alignment between high-resolution queries, derived from low-level image features, and semantically enriched low-resolution keys, using Spatial Feature Transform (SFT) modulation. Notably, despite the absence of high-resolution supervision, we demonstrate that learning at low upsampling ratios and resolutions generalizes remarkably well to significantly higher output scales. Extensive experiments show that JAFAR effectively recovers fine-grained spatial details and consistently outperforms existing feature upsampling methods across a diverse set of downstream tasks. Project page at https://jafar-upsampler.github.io

</details>


### [8] [AlignHuman: Improving Motion and Fidelity via Timestep-Segment Preference Optimization for Audio-Driven Human Animation](https://arxiv.org/abs/2506.11144)
*Chao Liang,Jianwen Jiang,Wang Liao,Jiaqi Yang,Zerong zheng,Weihong Zeng,Han Liang*

Main category: cs.CV

TL;DR: AlignHuman通过偏好优化和分治训练策略，优化了人类视频生成中运动自然性和视觉保真度的权衡，显著提升了生成质量和效率。


<details>
  <summary>Details</summary>
Motivation: 解决人类视频生成中运动自然性与视觉保真度之间的权衡问题。

Method: 提出AlignHuman框架，结合偏好优化和分治训练策略，利用时间步分段偏好优化（TPO）和两个专用LoRA模块分别优化运动动态和保真度。

Result: 实验表明AlignHuman显著提升了基线性能，推理速度提升3.3倍（从100 NFEs降至30 NFEs），且生成质量影响极小。

Conclusion: AlignHuman通过分治策略和偏好优化，有效解决了人类视频生成中的关键挑战，实现了高质量和高效生成。

Abstract: Recent advancements in human video generation and animation tasks, driven by diffusion models, have achieved significant progress. However, expressive and realistic human animation remains challenging due to the trade-off between motion naturalness and visual fidelity. To address this, we propose \textbf{AlignHuman}, a framework that combines Preference Optimization as a post-training technique with a divide-and-conquer training strategy to jointly optimize these competing objectives. Our key insight stems from an analysis of the denoising process across timesteps: (1) early denoising timesteps primarily control motion dynamics, while (2) fidelity and human structure can be effectively managed by later timesteps, even if early steps are skipped. Building on this observation, we propose timestep-segment preference optimization (TPO) and introduce two specialized LoRAs as expert alignment modules, each targeting a specific dimension in its corresponding timestep interval. The LoRAs are trained using their respective preference data and activated in the corresponding intervals during inference to enhance motion naturalness and fidelity. Extensive experiments demonstrate that AlignHuman improves strong baselines and reduces NFEs during inference, achieving a 3.3$\times$ speedup (from 100 NFEs to 30 NFEs) with minimal impact on generation quality. Homepage: \href{https://alignhuman.github.io/}{https://alignhuman.github.io/}

</details>


### [9] [TARDIS STRIDE: A Spatio-Temporal Road Image Dataset for Exploration and Autonomy](https://arxiv.org/abs/2506.11302)
*Héctor Carrión,Yutong Bai,Víctor A. Hernández Castro,Kishan Panaganti,Ayush Zenith,Matthew Trang,Tony Zhang,Pietro Perona,Jitendra Malik*

Main category: cs.CV

TL;DR: 论文提出了一种名为STRIDE的数据集和TARDIS模型，用于模拟动态时空环境中的智能体行为。


<details>
  <summary>Details</summary>
Motivation: 现实世界环境在时空上动态变化，传统方法难以捕捉这种复杂性，因此需要新的数据集和模型。

Method: 通过STRIDE数据集将360度全景图像转化为观测、状态和动作节点，并利用TARDIS模型（基于Transformer）统一建模时空动态。

Result: 在可控图像合成、指令跟随、自主控制和地理定位等任务中表现出色。

Conclusion: 该方法为构建能够理解和操纵时空环境的通用智能体提供了新方向。

Abstract: World models aim to simulate environments and enable effective agent behavior. However, modeling real-world environments presents unique challenges as they dynamically change across both space and, crucially, time. To capture these composed dynamics, we introduce a Spatio-Temporal Road Image Dataset for Exploration (STRIDE) permuting 360-degree panoramic imagery into rich interconnected observation, state and action nodes. Leveraging this structure, we can simultaneously model the relationship between egocentric views, positional coordinates, and movement commands across both space and time. We benchmark this dataset via TARDIS, a transformer-based generative world model that integrates spatial and temporal dynamics through a unified autoregressive framework trained on STRIDE. We demonstrate robust performance across a range of agentic tasks such as controllable photorealistic image synthesis, instruction following, autonomous self-control, and state-of-the-art georeferencing. These results suggest a promising direction towards sophisticated generalist agents--capable of understanding and manipulating the spatial and temporal aspects of their material environments--with enhanced embodied reasoning capabilities. Training code, datasets, and model checkpoints are made available at https://huggingface.co/datasets/Tera-AI/STRIDE.

</details>


### [10] [A Watermark for Auto-Regressive Image Generation Models](https://arxiv.org/abs/2506.11371)
*Yihan Wu,Xuehao Cui,Ruibo Chen,Georgios Milis,Heng Huang*

Main category: cs.CV

TL;DR: 本文提出了一种名为C-reweight的无失真水印方法，用于解决图像生成模型中的重标记不匹配问题，同时保持图像质量并提高可检测性。


<details>
  <summary>Details</summary>
Motivation: 图像生成模型的快速发展带来了潜在的滥用风险（如深度伪造、钓鱼攻击等），因此需要有效的真实性验证机制。传统水印技术因重标记不匹配问题难以直接应用于图像生成模型。

Method: 提出C-reweight方法，采用基于聚类的策略，将同一聚类内的标记视为等效，以解决重标记不匹配问题。

Result: 在主流图像生成平台上的评估表明，C-reweight在保持图像质量的同时，显著优于现有无失真水印技术。

Conclusion: C-reweight为安全可靠的图像合成设定了新标准，解决了重标记不匹配问题并提高了水印的可检测性。

Abstract: The rapid evolution of image generation models has revolutionized visual content creation, enabling the synthesis of highly realistic and contextually accurate images for diverse applications. However, the potential for misuse, such as deepfake generation, image based phishing attacks, and fabrication of misleading visual evidence, underscores the need for robust authenticity verification mechanisms. While traditional statistical watermarking techniques have proven effective for autoregressive language models, their direct adaptation to image generation models encounters significant challenges due to a phenomenon we term retokenization mismatch, a disparity between original and retokenized sequences during the image generation process. To overcome this limitation, we propose C-reweight, a novel, distortion-free watermarking method explicitly designed for image generation models. By leveraging a clustering-based strategy that treats tokens within the same cluster equivalently, C-reweight mitigates retokenization mismatch while preserving image fidelity. Extensive evaluations on leading image generation platforms reveal that C-reweight not only maintains the visual quality of generated images but also improves detectability over existing distortion-free watermarking techniques, setting a new standard for secure and trustworthy image synthesis.

</details>


### [11] [Auditing Data Provenance in Real-world Text-to-Image Diffusion Models for Privacy and Copyright Protection](https://arxiv.org/abs/2506.11434)
*Jie Zhu,Leye Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为FSCA的黑盒审计框架，用于解决文本到图像扩散模型中的数据来源审计问题，无需访问模型内部知识，并在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型依赖于大规模文本-图像数据集，但数据来源可能涉及版权和隐私问题。现有审计方法依赖模型内部知识或评估不可靠，因此需要一种更实用的解决方案。

Method: FSCA利用文本到图像扩散模型中的两种语义连接进行审计，无需内部知识。实验在LAION-mi和COCO数据集上进行，并与八种基线方法对比。

Result: FSCA在多种指标和数据分布下优于基线方法，用户级准确率达到90%（仅需10个样本/用户）。

Conclusion: FSCA展示了在现实审计场景中的强大潜力，代码已开源。

Abstract: Text-to-image diffusion model since its propose has significantly influenced the content creation due to its impressive generation capability. However, this capability depends on large-scale text-image datasets gathered from web platforms like social media, posing substantial challenges in copyright compliance and personal privacy leakage. Though there are some efforts devoted to explore approaches for auditing data provenance in text-to-image diffusion models, existing work has unrealistic assumptions that can obtain model internal knowledge, e.g., intermediate results, or the evaluation is not reliable. To fill this gap, we propose a completely black-box auditing framework called Feature Semantic Consistency-based Auditing (FSCA). It utilizes two types of semantic connections within the text-to-image diffusion model for auditing, eliminating the need for access to internal knowledge. To demonstrate the effectiveness of our FSCA framework, we perform extensive experiments on LAION-mi dataset and COCO dataset, and compare with eight state-of-the-art baseline approaches. The results show that FSCA surpasses previous baseline approaches across various metrics and different data distributions, showcasing the superiority of our FSCA. Moreover, we introduce a recall balance strategy and a threshold adjustment strategy, which collectively allows FSCA to reach up a user-level accuracy of 90% in a real-world auditing scenario with only 10 samples/user, highlighting its strong auditing potential in real-world applications. Our code is made available at https://github.com/JiePKU/FSCA.

</details>


### [12] [SignAligner: Harmonizing Complementary Pose Modalities for Coherent Sign Language Generation](https://arxiv.org/abs/2506.11621)
*Xu Wang,Shengeng Tang,Lechao Cheng,Feng Li,Shuo Wang,Richang Hong*

Main category: cs.CV

TL;DR: 本文提出了一种名为SignAligner的新方法，用于生成逼真的手语视频，通过三个阶段实现：文本驱动的姿势模态共生成、多模态在线协作校正和逼真手语视频合成。


<details>
  <summary>Details</summary>
Motivation: 由于手语的复杂性（包括手势、面部表情和身体动作），实现逼真和自然的手语生成仍是一个重大挑战。

Method: SignAligner方法分为三个阶段：1）基于文本语义的联合手语生成器；2）在线协作校正多模态；3）利用预训练视频生成网络合成高质量手语视频。

Result: 实验表明，SignAligner显著提高了生成手语视频的准确性和表现力。

Conclusion: SignAligner通过多模态协作和动态校正，成功实现了逼真手语视频的生成。

Abstract: Sign language generation aims to produce diverse sign representations based on spoken language. However, achieving realistic and naturalistic generation remains a significant challenge due to the complexity of sign language, which encompasses intricate hand gestures, facial expressions, and body movements. In this work, we introduce PHOENIX14T+, an extended version of the widely-used RWTH-PHOENIX-Weather 2014T dataset, featuring three new sign representations: Pose, Hamer and Smplerx. We also propose a novel method, SignAligner, for realistic sign language generation, consisting of three stages: text-driven pose modalities co-generation, online collaborative correction of multimodality, and realistic sign video synthesis. First, by incorporating text semantics, we design a joint sign language generator to simultaneously produce posture coordinates, gesture actions, and body movements. The text encoder, based on a Transformer architecture, extracts semantic features, while a cross-modal attention mechanism integrates these features to generate diverse sign language representations, ensuring accurate mapping and controlling the diversity of modal features. Next, online collaborative correction is introduced to refine the generated pose modalities using a dynamic loss weighting strategy and cross-modal attention, facilitating the complementarity of information across modalities, eliminating spatiotemporal conflicts, and ensuring semantic coherence and action consistency. Finally, the corrected pose modalities are fed into a pre-trained video generation network to produce high-fidelity sign language videos. Extensive experiments demonstrate that SignAligner significantly improves both the accuracy and expressiveness of the generated sign videos.

</details>


### [13] [Dynamic Mixture of Curriculum LoRA Experts for Continual Multimodal Instruction Tuning](https://arxiv.org/abs/2506.11672)
*Chendi Ge,Xin Wang,Zeyang Zhang,Hong Chen,Jiapei Fan,Longtao Huang,Hui Xue,Wenwu Zhu*

Main category: cs.CV

TL;DR: 论文提出了一种动态混合课程LoRA专家（D-MoLE）方法，通过动态调整多模态大语言模型（MLLM）的架构，解决任务架构冲突和模态不平衡问题，显著提升了持续学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法采用固定架构，难以适应新任务且存在模态不平衡问题，因此需要一种动态调整架构的方法。

Method: 提出D-MoLE方法，包括动态分层专家分配器和基于梯度的跨模态持续课程，以优化架构和模态更新。

Result: 实验表明D-MoLE显著优于现有基线，平均提升15%。

Conclusion: D-MoLE首次从架构角度研究了MLLM的持续学习，为解决任务冲突和模态不平衡提供了有效方案。

Abstract: Continual multimodal instruction tuning is crucial for adapting Multimodal Large Language Models (MLLMs) to evolving tasks. However, most existing methods adopt a fixed architecture, struggling with adapting to new tasks due to static model capacity. We propose to evolve the architecture under parameter budgets for dynamic task adaptation, which remains unexplored and imposes two challenges: 1) task architecture conflict, where different tasks require varying layer-wise adaptations, and 2) modality imbalance, where different tasks rely unevenly on modalities, leading to unbalanced updates. To address these challenges, we propose a novel Dynamic Mixture of Curriculum LoRA Experts (D-MoLE) method, which automatically evolves MLLM's architecture with controlled parameter budgets to continually adapt to new tasks while retaining previously learned knowledge. Specifically, we propose a dynamic layer-wise expert allocator, which automatically allocates LoRA experts across layers to resolve architecture conflicts, and routes instructions layer-wisely to facilitate knowledge sharing among experts. Then, we propose a gradient-based inter-modal continual curriculum, which adjusts the update ratio of each module in MLLM based on the difficulty of each modality within the task to alleviate the modality imbalance problem. Extensive experiments show that D-MoLE significantly outperforms state-of-the-art baselines, achieving a 15% average improvement over the best baseline. To the best of our knowledge, this is the first study of continual learning for MLLMs from an architectural perspective.

</details>


### [14] [DiffFuSR: Super-Resolution of all Sentinel-2 Multispectral Bands using Diffusion Models](https://arxiv.org/abs/2506.11764)
*Muhammad Sarmad,Arnt-Børre Salberg,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: DiffFuSR是一个模块化管道，用于将Sentinel-2 Level-2A影像的12个光谱波段超分辨率到统一的2.5米地面采样距离（GSD）。


<details>
  <summary>Details</summary>
Motivation: 解决Sentinel-2影像多光谱波段分辨率不一致的问题，提升影像质量。

Method: 采用两阶段方法：1）基于扩散模型的RGB超分辨率；2）利用超分辨率RGB图像作为空间先验，通过融合网络上采样其他多光谱波段。

Result: 在OpenSR基准测试中表现优于现有方法，反射率保真度、光谱一致性、空间对齐和幻觉抑制方面均有提升。

Conclusion: 通过生成先验和融合策略，实现了模块化的Sentinel-2超分辨率框架。

Abstract: This paper presents DiffFuSR, a modular pipeline for super-resolving all 12 spectral bands of Sentinel-2 Level-2A imagery to a unified ground sampling distance (GSD) of 2.5 meters. The pipeline comprises two stages: (i) a diffusion-based super-resolution (SR) model trained on high-resolution RGB imagery from the NAIP and WorldStrat datasets, harmonized to simulate Sentinel-2 characteristics; and (ii) a learned fusion network that upscales the remaining multispectral bands using the super-resolved RGB image as a spatial prior. We introduce a robust degradation model and contrastive degradation encoder to support blind SR. Extensive evaluations of the proposed SR pipeline on the OpenSR benchmark demonstrate that the proposed method outperforms current SOTA baselines in terms of reflectance fidelity, spectral consistency, spatial alignment, and hallucination suppression. Furthermore, the fusion network significantly outperforms classical pansharpening approaches, enabling accurate enhancement of Sentinel-2's 20 m and 60 m bands. This study underscores the power of harmonized learning with generative priors and fusion strategies to create a modular framework for Sentinel-2 SR. Our code and models can be found at https://github.com/NorskRegnesentral/DiffFuSR.

</details>


### [15] [MambaVSR: Content-Aware Scanning State Space Model for Video Super-Resolution](https://arxiv.org/abs/2506.11768)
*Linfeng He,Meiqin Liu,Qi Tang,Chao Yao,Yao Zhao*

Main category: cs.CV

TL;DR: MambaVSR是一种基于状态空间模型的视频超分辨率方法，通过动态时空交互和内容感知机制，解决了传统方法在大运动位移和长视频序列中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有视频超分辨率方法在处理非局部依赖性和计算效率方面存在不足，尤其是在大运动位移和长视频序列中表现不佳。

Method: 提出MambaVSR框架，包含共享罗盘构建（SCC）和内容感知序列化（CAS）模块，以及全局-局部状态空间块（GLSSB），实现动态时空交互和高效特征传播。

Result: 在REDS数据集上，MambaVSR比基于Transformer的方法PSNR提高了0.58 dB，且参数减少了55%。

Conclusion: MambaVSR通过创新的状态空间模型和内容感知机制，显著提升了视频超分辨率的性能，同时保持了计算效率。

Abstract: Video super-resolution (VSR) faces critical challenges in effectively modeling non-local dependencies across misaligned frames while preserving computational efficiency. Existing VSR methods typically rely on optical flow strategies or transformer architectures, which struggle with large motion displacements and long video sequences. To address this, we propose MambaVSR, the first state-space model framework for VSR that incorporates an innovative content-aware scanning mechanism. Unlike rigid 1D sequential processing in conventional vision Mamba methods, our MambaVSR enables dynamic spatiotemporal interactions through the Shared Compass Construction (SCC) and the Content-Aware Sequentialization (CAS). Specifically, the SCC module constructs intra-frame semantic connectivity graphs via efficient sparse attention and generates adaptive spatial scanning sequences through spectral clustering. Building upon SCC, the CAS module effectively aligns and aggregates non-local similar content across multiple frames by interleaving temporal features along the learned spatial order. To bridge global dependencies with local details, the Global-Local State Space Block (GLSSB) synergistically integrates window self-attention operations with SSM-based feature propagation, enabling high-frequency detail recovery under global dependency guidance. Extensive experiments validate MambaVSR's superiority, outperforming the Transformer-based method by 0.58 dB PSNR on the REDS dataset with 55% fewer parameters.

</details>


### [16] [Aligned Novel View Image and Geometry Synthesis via Cross-modal Attention Instillation](https://arxiv.org/abs/2506.11924)
*Min-Seop Kwak,Junho Kim,Sangdoo Yun,Dongyoon Han,Taekyoung Kim,Seungryong Kim,Jin-Hwa Kim*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散的框架，通过扭曲和修复方法实现对齐的新视角图像和几何生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要密集的姿势图像或仅限于域内视角的姿势嵌入生成模型，而本文方法利用现成的几何预测器预测部分几何，并将新视角合成视为图像和几何的修复任务。

Method: 提出跨模态注意力蒸馏，将图像扩散分支的注意力图注入并行几何扩散分支，并引入基于邻近的网格条件以整合深度和法线线索。

Result: 方法在未见场景中实现了高保真外推视角合成，在插值设置下提供有竞争力的重建质量，并生成几何对齐的彩色点云。

Conclusion: 该方法在多任务协同作用下，实现了几何稳健的图像合成和明确的几何预测。

Abstract: We introduce a diffusion-based framework that performs aligned novel view image and geometry generation via a warping-and-inpainting methodology. Unlike prior methods that require dense posed images or pose-embedded generative models limited to in-domain views, our method leverages off-the-shelf geometry predictors to predict partial geometries viewed from reference images, and formulates novel-view synthesis as an inpainting task for both image and geometry. To ensure accurate alignment between generated images and geometry, we propose cross-modal attention distillation, where attention maps from the image diffusion branch are injected into a parallel geometry diffusion branch during both training and inference. This multi-task approach achieves synergistic effects, facilitating geometrically robust image synthesis as well as well-defined geometry prediction. We further introduce proximity-based mesh conditioning to integrate depth and normal cues, interpolating between point cloud and filtering erroneously predicted geometry from influencing the generation process. Empirically, our method achieves high-fidelity extrapolative view synthesis on both image and geometry across a range of unseen scenes, delivers competitive reconstruction quality under interpolation settings, and produces geometrically aligned colored point clouds for comprehensive 3D completion. Project page is available at https://cvlab-kaist.github.io/MoAI.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [17] [Voxel-Level Brain States Prediction Using Swin Transformer](https://arxiv.org/abs/2506.11455)
*Yifei Sun,Daniel Chahine,Qinghao Wen,Tianming Liu,Xiang Li,Yixuan Yuan,Fernando Calamante,Jinglei Lv*

Main category: q-bio.NC

TL;DR: 该研究提出了一种基于4D Swin Transformer的模型，用于预测人类静息态脑活动，展示了高精度和潜在应用价值。


<details>
  <summary>Details</summary>
Motivation: 理解脑动力学对神经科学和心理健康至关重要，fMRI数据的高分辨率预测有助于减少扫描时间和开发脑机接口。

Method: 使用4D Swin Transformer编码器学习时空信息，卷积解码器预测脑状态，数据来自HCP项目的100名受试者。

Result: 模型能高精度预测7.2秒的静息态脑活动，预测结果与BOLD信号高度相似。

Conclusion: Swin Transformer模型能高效学习人脑时空组织，为减少fMRI扫描时间和脑机接口开发提供了潜力。

Abstract: Understanding brain dynamics is important for neuroscience and mental health. Functional magnetic resonance imaging (fMRI) enables the measurement of neural activities through blood-oxygen-level-dependent (BOLD) signals, which represent brain states. In this study, we aim to predict future human resting brain states with fMRI. Due to the 3D voxel-wise spatial organization and temporal dependencies of the fMRI data, we propose a novel architecture which employs a 4D Shifted Window (Swin) Transformer as encoder to efficiently learn spatio-temporal information and a convolutional decoder to enable brain state prediction at the same spatial and temporal resolution as the input fMRI data. We used 100 unrelated subjects from the Human Connectome Project (HCP) for model training and testing. Our novel model has shown high accuracy when predicting 7.2s resting-state brain activities based on the prior 23.04s fMRI time series. The predicted brain states highly resemble BOLD contrast and dynamics. This work shows promising evidence that the spatiotemporal organization of the human brain can be learned by a Swin Transformer model, at high resolution, which provides a potential for reducing the fMRI scan time and the development of brain-computer interfaces in the future.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [18] [EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction](https://arxiv.org/abs/2506.12015)
*Hsi-Che Lin,Yu-Chu Yu,Kai-Po Chang,Yu-Chiang Frank Wang*

Main category: cs.LG

TL;DR: EMLoC是一种基于模拟器的内存高效微调框架，通过LoRA修正，使模型微调能在与推理相同的内存预算内完成。


<details>
  <summary>Details</summary>
Motivation: 开源基础模型虽强大，但针对特定领域或个性化任务的微调因内存开销大而昂贵，限制了用户使用。

Method: EMLoC利用激活感知SVD构建轻量级模拟器，通过LoRA微调，并提出补偿算法修正LoRA模块以对齐原始模型。

Result: 实验表明，EMLoC在多个数据集和模态上优于基线方法，且能在24GB消费级GPU上微调38B模型。

Conclusion: EMLoC为个体用户提供了高效且实用的模型适配方案。

Abstract: Open-source foundation models have seen rapid adoption and development, enabling powerful general-purpose capabilities across diverse domains. However, fine-tuning large foundation models for domain-specific or personalized tasks remains prohibitively expensive for most users due to the significant memory overhead beyond that of inference. We introduce EMLoC, an Emulator-based Memory-efficient fine-tuning framework with LoRA Correction, which enables model fine-tuning within the same memory budget required for inference. EMLoC constructs a task-specific light-weight emulator using activation-aware singular value decomposition (SVD) on a small downstream calibration set. Fine-tuning then is performed on this lightweight emulator via LoRA. To tackle the misalignment between the original model and the compressed emulator, we propose a novel compensation algorithm to correct the fine-tuned LoRA module, which thus can be merged into the original model for inference. EMLoC supports flexible compression ratios and standard training pipelines, making it adaptable to a wide range of applications. Extensive experiments demonstrate that EMLoC outperforms other baselines across multiple datasets and modalities. Moreover, without quantization, EMLoC enables fine-tuning of a 38B model on a single 24GB consumer GPU-bringing efficient and practical model adaptation to individual users.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [19] [Grids Often Outperform Implicit Neural Representations](https://arxiv.org/abs/2506.11139)
*Namhoon Kim,Sara Fridovich-Keil*

Main category: eess.IV

TL;DR: 研究了隐式神经表示（INRs）的性能，发现简单正则化网格在多数任务中表现优于INRs，仅在某些低维结构信号中INRs更优。


<details>
  <summary>Details</summary>
Motivation: 理解INRs的基本能力、隐式偏差和扩展行为，填补当前研究的空白。

Method: 通过多样化的INRs在2D和3D信号（包括真实和合成信号）上进行实验，分析模型大小、信号类型和带宽对性能的影响。

Result: 多数任务中，正则化网格比INRs更快且质量更高；INRs仅在低维结构信号（如形状轮廓）中表现更优。

Conclusion: INRs在特定低维结构信号中具有优势，未来应针对这些应用场景优化使用。

Abstract: Implicit Neural Representations (INRs) have recently shown impressive results, but their fundamental capacity, implicit biases, and scaling behavior remain poorly understood. We investigate the performance of diverse INRs across a suite of 2D and 3D real and synthetic signals with varying effective bandwidth, as well as both overfitting and generalization tasks including tomography, super-resolution, and denoising. By stratifying performance according to model size as well as signal type and bandwidth, our results shed light on how different INR and grid representations allocate their capacity. We find that, for most tasks and signals, a simple regularized grid with interpolation trains faster and to higher quality than any INR with the same number of parameters. We also find limited settings where INRs outperform grids -- namely fitting signals with underlying lower-dimensional structure such as shape contours -- to guide future use of INRs towards the most advantageous applications. Code and synthetic signals used in our analysis are available at https://github.com/voilalab/INR-benchmark.

</details>


### [20] [DiffPR: Diffusion-Based Phase Reconstruction via Frequency-Decoupled Learning](https://arxiv.org/abs/2506.11183)
*Yi Zhang*

Main category: eess.IV

TL;DR: 论文提出DiffPR框架，通过取消高频跳跃连接和引入扩散模型，解决了QPI中的过平滑问题，显著提升了图像质量。


<details>
  <summary>Details</summary>
Motivation: 深度学习中U-Net的频谱偏差导致QPI图像细节丢失，过平滑问题严重，影响诊断效果。

Method: 提出两阶段DiffPR框架：第一阶段用取消高频跳跃的非对称U-Net预测低分辨率相位图；第二阶段用扩散模型恢复高频细节。

Result: 在四个QPI数据集上，DiffPR比U-Net基线PSNR提升1.1 dB，MAE降低11%，图像更清晰。

Conclusion: 取消高频跳跃连接并用扩散模型补充细节，是解决频谱偏差的有效方法。

Abstract: Oversmoothing remains a persistent problem when applying deep learning to off-axis quantitative phase imaging (QPI). End-to-end U-Nets favour low-frequency content and under-represent fine, diagnostic detail. We trace this issue to spectral bias and show that the bias is reinforced by high-level skip connections that feed high-frequency features directly into the decoder. Removing those deepest skips thus supervising the network only at a low resolution significantly improves generalisation and fidelity. Building on this insight, we introduce DiffPR, a two-stage frequency-decoupled framework. Stage 1: an asymmetric U-Net with cancelled high-frequency skips predicts a quarter-scale phase map from the interferogram, capturing reliable low-frequency structure while avoiding spectral bias. Stage 2: the upsampled prediction, lightly perturbed with Gaussian noise, is refined by an unconditional diffusion model that iteratively recovers the missing high-frequency residuals through reverse denoising. Experiments on four QPI datasets (B-Cell, WBC, HeLa, 3T3) show that DiffPR outperforms strong U-Net baselines, boosting PSNR by up to 1.1 dB and reducing MAE by 11 percent, while delivering markedly sharper membrane ridges and speckle patterns. The results demonstrate that cancelling high-level skips and delegating detail synthesis to a diffusion prior is an effective remedy for the spectral bias that limits conventional phase-retrieval networks.

</details>


### [21] [Joint Denoising of Cryo-EM Projection Images using Polar Transformers](https://arxiv.org/abs/2506.11283)
*Joakim Andén,Justus Sagemüller*

Main category: eess.IV

TL;DR: 提出一种基于Transformer的神经网络架构，用于同时聚类、对齐和去噪冷冻电镜图像，显著降低噪声。


<details>
  <summary>Details</summary>
Motivation: 传统DNN在高噪声环境下（如冷冻电镜图像）效果有限，而数据冗余性可用于改进去噪方法。

Method: 基于Transformer的架构，同时实现图像聚类、对齐和去噪。

Result: 在合成数据上，相对MSE降低45%（SNR=0.03）。

Conclusion: 该方法在高噪声环境下优于单图像DNN，提升了冷冻电镜图像的去噪效果。

Abstract: Deep neural networks~(DNNs) have proven powerful for denoising, but they are ultimately of limited use in high-noise settings, such as for cryogenic electron microscopy~(cryo-EM) projection images. In this setting, however, datasets contain a large number of projections of the same molecule, each taken from a different viewing direction. This redundancy of information is useful in traditional denoising techniques known as class averaging methods, where images are clustered, aligned, and then averaged to reduce the noise level. We present a neural network architecture based on transformers that extends these class averaging methods by simultaneously clustering, aligning, and denoising cryo-EM images. Results on synthetic data show accurate denoising performance using this architecture, reducing the relative mean squared error (MSE) single-image DNNs by $45\%$ at a signal-to-noise (SNR) of $0.03$.

</details>


### [22] [Taming Stable Diffusion for Computed Tomography Blind Super-Resolution](https://arxiv.org/abs/2506.11496)
*Chunlei Li,Yilei Shi,Haoxi Hu,Jingliang Hu,Xiao Xiang Zhu,Lichao Mou*

Main category: eess.IV

TL;DR: 本文提出了一种基于Stable Diffusion的CT盲超分辨率框架，通过合成低质量图像和生成文本描述，结合预训练模型，显著提升了CT图像质量，同时降低辐射剂量。


<details>
  <summary>Details</summary>
Motivation: 高分辨率CT成像对医疗诊断至关重要，但辐射剂量与图像质量之间存在矛盾。现有深度学习方法在复杂退化和有限医学数据下表现不佳，而预训练扩散模型（如Stable Diffusion）在细节合成方面表现出色。

Method: 提出了一种新框架，利用实际退化模型合成低质量CT图像，结合预训练视觉语言模型生成文本描述，并通过Stable Diffusion进行超分辨率重建，同时控制输入条件和文本描述。

Result: 实验表明，该方法优于现有方法，能够在降低辐射剂量的同时实现高质量的CT成像。

Conclusion: 该方法为高分辨率CT成像提供了一种有效解决方案，具有实际应用潜力。代码将公开。

Abstract: High-resolution computed tomography (CT) imaging is essential for medical diagnosis but requires increased radiation exposure, creating a critical trade-off between image quality and patient safety. While deep learning methods have shown promise in CT super-resolution, they face challenges with complex degradations and limited medical training data. Meanwhile, large-scale pre-trained diffusion models, particularly Stable Diffusion, have demonstrated remarkable capabilities in synthesizing fine details across various vision tasks. Motivated by this, we propose a novel framework that adapts Stable Diffusion for CT blind super-resolution. We employ a practical degradation model to synthesize realistic low-quality images and leverage a pre-trained vision-language model to generate corresponding descriptions. Subsequently, we perform super-resolution using Stable Diffusion with a specialized controlling strategy, conditioned on both low-resolution inputs and the generated text descriptions. Extensive experiments show that our method outperforms existing approaches, demonstrating its potential for achieving high-quality CT imaging at reduced radiation doses. Our code will be made publicly available.

</details>


### [23] [FCA2: Frame Compression-Aware Autoencoder for Modular and Fast Compressed Video Super-Resolution](https://arxiv.org/abs/2506.11545)
*Zhaoyang Wang,Jie Li,Wen Lu,Lihuo He,Maoguo Gong,Xinbo Gao*

Main category: eess.IV

TL;DR: 提出了一种基于高光谱图像与视频数据相似性的高效压缩视频超分辨率方法，显著降低计算复杂度和推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有压缩视频超分辨率模型存在推理时间长、训练流程复杂、依赖辅助信息等问题，且传统帧间信息利用方法难以满足需求。

Method: 采用压缩驱动的降维策略，设计模块化架构，提升时间信息提取效率，并与现有框架无缝集成。

Result: 实验表明，该方法性能达到或超越当前SOTA模型，同时显著减少推理时间。

Conclusion: 通过解决关键瓶颈，为视频超分辨率技术提供了高效实用的解决方案。

Abstract: State-of-the-art (SOTA) compressed video super-resolution (CVSR) models face persistent challenges, including prolonged inference time, complex training pipelines, and reliance on auxiliary information. As video frame rates continue to increase, the diminishing inter-frame differences further expose the limitations of traditional frame-to-frame information exploitation methods, which are inadequate for addressing current video super-resolution (VSR) demands. To overcome these challenges, we propose an efficient and scalable solution inspired by the structural and statistical similarities between hyperspectral images (HSI) and video data. Our approach introduces a compression-driven dimensionality reduction strategy that reduces computational complexity, accelerates inference, and enhances the extraction of temporal information across frames. The proposed modular architecture is designed for seamless integration with existing VSR frameworks, ensuring strong adaptability and transferability across diverse applications. Experimental results demonstrate that our method achieves performance on par with, or surpassing, the current SOTA models, while significantly reducing inference time. By addressing key bottlenecks in CVSR, our work offers a practical and efficient pathway for advancing VSR technology. Our code will be publicly available at https://github.com/handsomewzy/FCA2.

</details>


### [24] [Exploring the Effectiveness of Deep Features from Domain-Specific Foundation Models in Retinal Image Synthesis](https://arxiv.org/abs/2506.11753)
*Zuzanna Skorniewska,Bartlomiej W. Papiez*

Main category: eess.IV

TL;DR: 研究探讨了在医学影像生成中，基于领域特定深度特征的距离损失函数与感知损失和边缘检测损失函数的比较，发现传统边缘检测滤波器在提升血管结构清晰度上更有效。


<details>
  <summary>Details</summary>
Motivation: 医学影像生成面临隐私、数据稀缺和公平性问题，深度生成模型提供解决方案，但需验证形态和临床准确性。

Method: 比较了基于领域特定深度特征的距离损失函数与感知损失和边缘检测损失函数的效果。

Result: 领域特定深度特征未提升生成质量，传统边缘检测滤波器在血管结构清晰度上表现更好。

Conclusion: 传统边缘检测滤波器在医学影像生成中更有效，领域特定深度特征未带来显著优势。

Abstract: The adoption of neural network models in medical imaging has been constrained by strict privacy regulations, limited data availability, high acquisition costs, and demographic biases. Deep generative models offer a promising solution by generating synthetic data that bypasses privacy concerns and addresses fairness by producing samples for under-represented groups. However, unlike natural images, medical imaging requires validation not only for fidelity (e.g., Fr\'echet Inception Score) but also for morphological and clinical accuracy. This is particularly true for colour fundus retinal imaging, which requires precise replication of the retinal vascular network, including vessel topology, continuity, and thickness. In this study, we in-vestigated whether a distance-based loss function based on deep activation layers of a large foundational model trained on large corpus of domain data, colour fundus imaging, offers advantages over a perceptual loss and edge-detection based loss functions. Our extensive validation pipeline, based on both domain-free and domain specific tasks, suggests that domain-specific deep features do not improve autoen-coder image generation. Conversely, our findings highlight the effectiveness of con-ventional edge detection filters in improving the sharpness of vascular structures in synthetic samples.

</details>


### [25] [Structural Similarity-Inspired Unfolding for Lightweight Image Super-Resolution](https://arxiv.org/abs/2506.11823)
*Zhangkai Ni,Yang Zhang,Wenhan Yang,Hanli Wang,Shiqi Wang,Sam Kwong*

Main category: eess.IV

TL;DR: 提出了一种基于结构相似性启发的展开方法（SSIU），用于高效图像超分辨率，结合数据驱动和模型驱动的优势，性能优于现有方法且参数更少。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的超分辨率方法通常通过增加模型深度或使用注意力机制来扩展感受野，但会增加模型复杂度。模型驱动方法通过展开范式在保持紧凑性的同时提升性能，因此提出结合两者优势的方法。

Method: 通过展开一个受结构相似性约束的超分辨率优化函数设计SSIU方法，包含混合尺度门控模块（MSGM）和高效稀疏注意力模块（ESAM），并设计了基于专家混合的特征选择器（MoE-FS）以利用多级特征信息。

Result: 实验表明，SSIU在性能上优于当前最先进模型，同时参数更少、内存消耗更低。

Conclusion: SSIU方法通过结合数据驱动和模型驱动的优势，实现了高效且高性能的图像超分辨率。

Abstract: Major efforts in data-driven image super-resolution (SR) primarily focus on expanding the receptive field of the model to better capture contextual information. However, these methods are typically implemented by stacking deeper networks or leveraging transformer-based attention mechanisms, which consequently increases model complexity. In contrast, model-driven methods based on the unfolding paradigm show promise in improving performance while effectively maintaining model compactness through sophisticated module design. Based on these insights, we propose a Structural Similarity-Inspired Unfolding (SSIU) method for efficient image SR. This method is designed through unfolding an SR optimization function constrained by structural similarity, aiming to combine the strengths of both data-driven and model-driven approaches. Our model operates progressively following the unfolding paradigm. Each iteration consists of multiple Mixed-Scale Gating Modules (MSGM) and an Efficient Sparse Attention Module (ESAM). The former implements comprehensive constraints on features, including a structural similarity constraint, while the latter aims to achieve sparse activation. In addition, we design a Mixture-of-Experts-based Feature Selector (MoE-FS) that fully utilizes multi-level feature information by combining features from different steps. Extensive experiments validate the efficacy and efficiency of our unfolding-inspired network. Our model outperforms current state-of-the-art models, boasting lower parameter counts and reduced memory consumption. Our code will be available at: https://github.com/eezkni/SSIU

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [26] [GaussMarker: Robust Dual-Domain Watermark for Diffusion Models](https://arxiv.org/abs/2506.11444)
*Kecen Li,Zhicong Huang,Xinwen Hou,Cheng Hong*

Main category: cs.CR

TL;DR: 论文提出了一种双域扩散模型水印方法，通过空间和频率域嵌入水印，结合高斯噪声恢复器提升鲁棒性，在多种攻击下表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型生成图像越来越逼真，版权和滥用问题日益突出，现有单域水印方法鲁棒性不足。

Method: 采用双域水印嵌入方法（空间和频率域），并引入模型无关的高斯噪声恢复器（GNR）增强鲁棒性。

Result: 在多种图像失真和高级攻击下，GaussMarker在三个Stable Diffusion版本中表现最优，召回率高且误报率低。

Conclusion: GaussMarker通过双域水印和GNR实现了高效且鲁棒的水印嵌入与检测，适用于实际应用。

Abstract: As Diffusion Models (DM) generate increasingly realistic images, related issues such as copyright and misuse have become a growing concern. Watermarking is one of the promising solutions. Existing methods inject the watermark into the single-domain of initial Gaussian noise for generation, which suffers from unsatisfactory robustness. This paper presents the first dual-domain DM watermarking approach using a pipelined injector to consistently embed watermarks in both the spatial and frequency domains. To further boost robustness against certain image manipulations and advanced attacks, we introduce a model-independent learnable Gaussian Noise Restorer (GNR) to refine Gaussian noise extracted from manipulated images and enhance detection robustness by integrating the detection scores of both watermarks. GaussMarker efficiently achieves state-of-the-art performance under eight image distortions and four advanced attacks across three versions of Stable Diffusion with better recall and lower false positive rates, as preferred in real applications.

</details>
