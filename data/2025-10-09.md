<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 27]
- [cs.LG](#cs.LG) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Capture and Interact: Rapid 3D Object Acquisition and Rendering with Gaussian Splatting in Unity](https://arxiv.org/abs/2510.06802)
*Islomjon Shukhratov,Sergey Gorinsky*

Main category: cs.GR

TL;DR: 基于3D高斯泼溅的端到端实时3D对象采集与渲染系统，使用手机视频扫描、云端处理和本地Unity渲染，实现150fps的交互式可视化。


<details>
  <summary>Details</summary>
Motivation: 解决实时3D对象捕获和渲染的挑战，为增强现实、数字孪生、远程协作等应用提供潜力。

Method: 集成移动设备捕获、云端3D高斯泼溅重建和Unity渲染的端到端流水线，用户通过手机视频扫描对象并上传云端处理。

Result: 系统在GPU上约10分钟完成扫描处理，在笔记本电脑上实现平均150fps的实时渲染性能。

Conclusion: 该流水线成功实现了从移动捕捉到交互式渲染的实时3D对象处理，支持实时远程呈现应用。

Abstract: Capturing and rendering three-dimensional (3D) objects in real time remain a significant challenge, yet hold substantial potential for applications in augmented reality, digital twin systems, remote collaboration and prototyping. We present an end-to-end pipeline that leverages 3D Gaussian Splatting (3D GS) to enable rapid acquisition and interactive rendering of real-world objects using a mobile device, cloud processing and a local computer. Users scan an object with a smartphone video, upload it for automated 3D reconstruction, and visualize it interactively in Unity at an average of 150 frames per second (fps) on a laptop. The system integrates mobile capture, cloud-based 3D GS and Unity rendering to support real-time telepresence. Our experiments show that the pipeline processes scans in approximately 10 minutes on a graphics processing unit (GPU) achieving real-time rendering on the laptop.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [Improving the Spatial Resolution of GONG Solar Images to GST Quality Using Deep Learning](https://arxiv.org/abs/2510.06281)
*Chenyang Li,Qin Li,Haimin Wang,Bo Shen*

Main category: cs.CV

TL;DR: 提出基于GAN的超分辨率方法，将GONG的低分辨率Hα太阳图像增强至接近BBSO/GST的高分辨率质量，有效恢复太阳黑子半影和细丝结构的细节。


<details>
  <summary>Details</summary>
Motivation: 高分辨率太阳成像对捕捉细丝和纤维等精细动态特征至关重要，但全盘Hα图像的空间分辨率有限，无法解析这些微小结构。

Method: 使用Real-ESRGAN模型，包含残差中的残差密集块和相对判别器，对GONG-GST图像对进行精心对齐处理。

Result: 模型有效恢复了太阳黑子半影内的精细细节，并解析了细丝和纤维的细微结构，平均MSE为467.15，RMSE为21.59，交叉相关性为0.7794。

Conclusion: 图像对的轻微不对齐限制了定量性能，未来计划解决对齐问题并扩展数据集以进一步提高重建质量。

Abstract: High-resolution (HR) solar imaging is crucial for capturing fine-scale dynamic features such as filaments and fibrils. However, the spatial resolution of the full-disk H$\alpha$ images is limited and insufficient to resolve these small-scale structures. To address this, we propose a GAN-based superresolution approach to enhance low-resolution (LR) full-disk H$\alpha$ images from the Global Oscillation Network Group (GONG) to a quality comparable with HR observations from the Big Bear Solar Observatory/Goode Solar Telescope (BBSO/GST). We employ Real-ESRGAN with Residual-in-Residual Dense Blocks and a relativistic discriminator. We carefully aligned GONG-GST pairs. The model effectively recovers fine details within sunspot penumbrae and resolves fine details in filaments and fibrils, achieving an average mean squared error (MSE) of 467.15, root mean squared error (RMSE) of 21.59, and cross-correlation (CC) of 0.7794. Slight misalignments between image pairs limit quantitative performance, which we plan to address in future work alongside dataset expansion to further improve reconstruction quality.

</details>


### [3] [Efficient High-Resolution Image Editing with Hallucination-Aware Loss and Adaptive Tiling](https://arxiv.org/abs/2510.06295)
*Young D. Kwon,Abhinav Mehrotra,Malcolm Chadwick,Alberto Gil Ramos,Sourav Bhattacharya*

Main category: cs.CV

TL;DR: MobilePicasso是一个高效的高分辨率图像编辑系统，通过三阶段方法在移动设备上实现4K图像编辑，显著降低计算成本和内存使用。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在资源受限设备上进行高分辨率图像编辑时面临内存和图像质量的挑战，需要开发更高效的解决方案。

Method: 采用三阶段方法：标准分辨率编辑（含幻觉感知损失）、潜在投影避免像素空间转换、自适应上下文保持分块上采样。

Result: 用户研究表明图像质量提升18-48%，幻觉减少14-51%，延迟降低高达55.8倍，内存仅增加9%，设备端运行速度甚至超过A100 GPU上的服务器模型。

Conclusion: MobilePicasso成功实现了在移动设备上高效的高分辨率图像编辑，在保持低内存占用的同时显著提升了图像质量和处理速度。

Abstract: High-resolution (4K) image-to-image synthesis has become increasingly important for mobile applications. Existing diffusion models for image editing face significant challenges, in terms of memory and image quality, when deployed on resource-constrained devices. In this paper, we present MobilePicasso, a novel system that enables efficient image editing at high resolutions, while minimising computational cost and memory usage. MobilePicasso comprises three stages: (i) performing image editing at a standard resolution with hallucination-aware loss, (ii) applying latent projection to overcome going to the pixel space, and (iii) upscaling the edited image latent to a higher resolution with adaptive context-preserving tiling. Our user study with 46 participants reveals that MobilePicasso not only improves image quality by 18-48% but reduces hallucinations by 14-51% over existing methods. MobilePicasso demonstrates significantly lower latency, e.g., up to 55.8$\times$ speed-up, yet with a small increase in runtime memory, e.g., a mere 9% increase over prior work. Surprisingly, the on-device runtime of MobilePicasso is observed to be faster than a server-based high-resolution image editing model running on an A100 GPU.

</details>


### [4] [RGBD Gaze Tracking Using Transformer for Feature Fusion](https://arxiv.org/abs/2510.06298)
*Tobias J. Bauer*

Main category: cs.CV

TL;DR: 基于RGBD图像的AI视线追踪系统实现，使用Transformer融合特征，在多个数据集上评估性能，发现不使用预训练GAN模块和用MLP替代Transformer能获得更好结果。


<details>
  <summary>Details</summary>
Motivation: 现有数据集要么缺少深度信息，要么只适合视线点估计而不适合视线角度估计，因此需要创建新数据集并研究RGBD图像与Transformer结合的新方法。

Method: 使用RGBD图像输入，基于Transformer架构的特征融合模块，采用GAN去除深度图伪影并提取头部姿态特征，在三个不同数据集上训练和评估多种模型配置。

Result: 在ShanghaiTechGaze+数据集上，带Transformer模块的模型平均欧几里得误差为55.3mm，不使用预训练GAN模块降至30.1mm，用MLP替代Transformer进一步降至26.9mm。在ETH-XGaze数据集上，带Transformer模块的平均角度误差为3.59度，不带为3.26度。

Conclusion: RGBD图像与Transformer结合的方法可行，但使用更简单的MLP替代Transformer能获得更好的性能，且不使用预训练GAN模块也能改善结果。

Abstract: Subject of this thesis is the implementation of an AI-based Gaze Tracking system using RGBD images that contain both color (RGB) and depth (D) information. To fuse the features extracted from the images, a module based on the Transformer architecture is used. The combination of RGBD input images and Transformers was chosen because it has not yet been investigated. Furthermore, a new dataset is created for training the AI models as existing datasets either do not contain depth information or only contain labels for Gaze Point Estimation that are not suitable for the task of Gaze Angle Estimation. Various model configurations are trained, validated and evaluated on a total of three different datasets. The trained models are then to be used in a real-time pipeline to estimate the gaze direction and thus the gaze point of a person in front of a computer screen. The AI model architecture used in this thesis is based on an earlier work by Lian et al. It uses a Generative Adversarial Network (GAN) to simultaneously remove depth map artifacts and extract head pose features. Lian et al. achieve a mean Euclidean error of 38.7mm on their own dataset ShanghaiTechGaze+. In this thesis, a model architecture with a Transformer module for feature fusion achieves a mean Euclidean error of 55.3mm on the same dataset, but we show that using no pre-trained GAN module leads to a mean Euclidean error of 30.1mm. Replacing the Transformer module with a Multilayer Perceptron (MLP) improves the error to 26.9mm. These results are coherent with the ones on the other two datasets. On the ETH-XGaze dataset, the model with Transformer module achieves a mean angular error of 3.59{\deg} and without Transformer module 3.26{\deg}, whereas the fundamentally different model architecture used by the dataset authors Zhang et al. achieves a mean angular error of 2.04{\deg}. On the OTH-Gaze-Estimation dataset created for...

</details>


### [5] [Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding](https://arxiv.org/abs/2510.06308)
*Yi Xin,Qi Qin,Siqi Luo,Kaiwen Zhu,Juncheng Yan,Yan Tai,Jiayi Lei,Yuewen Cao,Keqi Wang,Yibin Wang,Jinbin Bai,Qian Yu,Dengyang Jiang,Yuandong Pu,Haoxing Chen,Le Zhuo,Junjun He,Gen Luo,Tianbin Li,Ming Hu,Jin Ye,Shenglong Ye,Bo Zhang,Chang Xu,Wenhai Wang,Hongsheng Li,Guangtao Zhai,Tianfan Xue,Bin Fu,Xiaohong Liu,Yu Qiao,Yihao Liu*

Main category: cs.CV

TL;DR: Lumina-DiMOO是一个开源的多模态基础模型，采用完全离散扩散建模处理多模态输入输出，在采样效率和任务支持范围上优于之前的自回归或混合方法。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型在采样效率和任务支持范围上存在局限，需要更高效的方法来处理文本、图像等多种模态的生成和理解任务。

Method: 使用完全离散扩散建模来处理多模态输入输出，相比自回归或混合自回归-扩散方法具有更高的采样效率。

Result: 在多个基准测试中达到最先进性能，超越了现有的开源统一多模态模型。

Conclusion: Lumina-DiMOO通过离散扩散建模实现了高效的多模态生成和理解，为多模态和离散扩散模型研究提供了有价值的开源工具。

Abstract: We introduce Lumina-DiMOO, an open-source foundational model for seamless multi-modal generation and understanding. Lumina-DiMOO sets itself apart from prior unified models by utilizing a fully discrete diffusion modeling to handle inputs and outputs across various modalities. This innovative approach allows Lumina-DiMOO to achieve higher sampling efficiency compared to previous autoregressive (AR) or hybrid AR-Diffusion paradigms and adeptly support a broad spectrum of multi-modal tasks, including text-to-image generation, image-to-image generation (e.g., image editing, subject-driven generation, and image inpainting, etc.), as well as image understanding. Lumina-DiMOO achieves state-of-the-art performance on multiple benchmarks, surpassing existing open-source unified multi-modal models. To foster further advancements in multi-modal and discrete diffusion model research, we release our code and checkpoints to the community. Project Page: https://synbol.github.io/Lumina-DiMOO.

</details>


### [6] [TDiff: Thermal Plug-And-Play Prior with Patch-Based Diffusion](https://arxiv.org/abs/2510.06460)
*Piyush Dashpute,Niki Nezakati,Wolfgang Heidrich,Vishwanath Saragadam*

Main category: cs.CV

TL;DR: 提出了基于patch的扩散框架TDiff，通过在小热图像patch上训练来解决低分辨率热图像的恢复问题，在去噪、超分辨率和去模糊任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 低成本热成像相机存在低分辨率、固定模式噪声等局部退化问题，且现有热成像数据集在规模和多样性方面都有限。

Method: 基于patch的扩散框架，通过在小热图像patch上训练学习先验知识，使用重叠patch去噪和平滑空间窗口融合来恢复全分辨率图像。

Result: 在模拟和真实热数据上的去噪、超分辨率和去模糊实验都取得了强劲结果。

Conclusion: 该方法成为热图像恢复的统一流程，是首个基于patch的扩散框架，能够跨多个任务建模学习先验。

Abstract: Thermal images from low-cost cameras often suffer from low resolution, fixed pattern noise, and other localized degradations. Available datasets for thermal imaging are also limited in both size and diversity. To address these challenges, we propose a patch-based diffusion framework (TDiff) that leverages the local nature of these distortions by training on small thermal patches. In this approach, full-resolution images are restored by denoising overlapping patches and blending them using smooth spatial windowing. To our knowledge, this is the first patch-based diffusion framework that models a learned prior for thermal image restoration across multiple tasks. Experiments on denoising, super-resolution, and deblurring demonstrate strong results on both simulated and real thermal data, establishing our method as a unified restoration pipeline.

</details>


### [7] [SIGMA-GEN: Structure and Identity Guided Multi-subject Assembly for Image Generation](https://arxiv.org/abs/2510.06469)
*Oindrila Saha,Vojtech Krs,Radomir Mech,Subhransu Maji,Kevin Blackburn-Matzen,Matheus Gadelha*

Main category: cs.CV

TL;DR: SIGMA-GEN是一个统一的多身份保持图像生成框架，首次实现了基于结构和空间约束的单次多主体身份保持生成，支持从粗粒度到像素级精度的用户引导。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在多主体身份保持生成方面的局限性，提供更灵活的用户引导能力。

Method: 引入SIGMA-SET27K合成数据集，包含10万+独特主体和2.7万张图像的身份、结构和空间信息；开发统一框架支持多种精度级别的用户引导。

Result: 在身份保持、图像生成质量和速度方面达到最先进性能。

Conclusion: SIGMA-GEN为多身份保持图像生成提供了有效的解决方案，具有优异的性能和灵活性。

Abstract: We present SIGMA-GEN, a unified framework for multi-identity preserving image generation. Unlike prior approaches, SIGMA-GEN is the first to enable single-pass multi-subject identity-preserved generation guided by both structural and spatial constraints. A key strength of our method is its ability to support user guidance at various levels of precision -- from coarse 2D or 3D boxes to pixel-level segmentations and depth -- with a single model. To enable this, we introduce SIGMA-SET27K, a novel synthetic dataset that provides identity, structure, and spatial information for over 100k unique subjects across 27k images. Through extensive evaluation we demonstrate that SIGMA-GEN achieves state-of-the-art performance in identity preservation, image generation quality, and speed. Code and visualizations at https://oindrilasaha.github.io/SIGMA-Gen/

</details>


### [8] [VUGEN: Visual Understanding priors for GENeration](https://arxiv.org/abs/2510.06529)
*Xiangyi Chen,Théophane Vallaeys,Maha Elbayad,John Nguyen,Jakob Verbeek*

Main category: cs.CV

TL;DR: VUGEN是一个新颖的视觉语言模型框架，通过利用预训练的视觉理解先验来实现高效高质量的图像生成，无需复杂的自编码器或桥接机制。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在图像生成方面存在挑战，通常依赖于重建导向的自编码器或复杂的桥接机制，导致理解与生成表示不对齐或架构复杂。

Method: 首先将VLM视觉编码器的高维潜在空间转换为低维可处理分布，然后训练VLM在该降维潜在空间中采样，最后使用专门的像素解码器将生成的潜在映射回图像空间。

Result: VUGEN在图像生成性能上表现优异，将DPG Bench从71.17提升到74.32，COCO上的FID从11.86改善到9.06，同时完全保留了VLM原有的理解能力。

Conclusion: VUGEN框架成功地将视觉语言模型的视觉理解能力与图像生成能力对齐，实现了高效高质量的图像生成，且无需复杂的潜在扩散解码器。

Abstract: Recent advances in Vision-Language Models (VLMs) have enabled unified understanding across text and images, yet equipping these models with robust image generation capabilities remains challenging. Existing approaches often rely on reconstruction-oriented autoencoders or complex bridging mechanisms, leading to misalignment between understanding and generation representations, or architectural complexity. In this work, we propose VUGEN, a novel framework that explicitly leverages VLM's pretrained visual understanding priors for efficient and high-quality image generation. Our approach first transforms the high-dimensional latent space of the VLM's native vision encoder into a lower-dimensional, tractable distribution that maximally preserves visual information. The VLM is then trained to sample within this reduced latent space, ensuring alignment with its visual understanding capabilities. Finally, a dedicated pixel decoder maps these generated latents back to the image space. We find that a VAE-free pixel diffusion decoder to be on par or better than commonly used complex latent diffusion decoders that internally rely on VAE latents. Extensive experiments demonstrate that VUGEN achieves superior image generation performance, improving DPG Bench from 71.17 to 74.32 and FID from 11.86 to 9.06 on COCO, while fully preserving the VLM's original understanding capabilities.

</details>


### [9] [HSNet: Heterogeneous Subgraph Network for Single Image Super-resolution](https://arxiv.org/abs/2510.06564)
*Qiongyang Hu,Wenyang Liu,Wenbin Zou,Yuejiao Su,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: 提出HSNet框架，通过分解全局图为可管理的子图组件来解决图像超分辨率中结构不灵活和计算复杂度过高的问题


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN和注意力机制的深度学习方法在图像超分辨率中存在结构不灵活问题，而基于图的方法虽然表示能力更强但计算复杂度过高

Method: 使用异构子图网络(HSNet)，包含构造性子图集块(CSSB)生成互补子图，子图聚合块(SAB)集成多图特征，以及节点采样策略(NSS)选择重要特征

Result: 大量实验表明HSNet实现了最先进的性能，在重建质量和计算效率之间取得了良好平衡

Conclusion: HSNet通过有效的图建模方法成功解决了图像超分辨率中的结构不灵活和计算复杂度过高问题

Abstract: Existing deep learning approaches for image super-resolution, particularly those based on CNNs and attention mechanisms, often suffer from structural inflexibility. Although graph-based methods offer greater representational adaptability, they are frequently impeded by excessive computational complexity. To overcome these limitations, this paper proposes the Heterogeneous Subgraph Network (HSNet), a novel framework that efficiently leverages graph modeling while maintaining computational feasibility. The core idea of HSNet is to decompose the global graph into manageable sub-components. First, we introduce the Constructive Subgraph Set Block (CSSB), which generates a diverse set of complementary subgraphs. Rather than relying on a single monolithic graph, CSSB captures heterogeneous characteristics of the image by modeling different relational patterns and feature interactions, producing a rich ensemble of both local and global graph structures. Subsequently, the Subgraph Aggregation Block (SAB) integrates the representations embedded across these subgraphs. Through adaptive weighting and fusion of multi-graph features, SAB constructs a comprehensive and discriminative representation that captures intricate interdependencies. Furthermore, a Node Sampling Strategy (NSS) is designed to selectively retain the most salient features, thereby enhancing accuracy while reducing computational overhead. Extensive experiments demonstrate that HSNet achieves state-of-the-art performance, effectively balancing reconstruction quality with computational efficiency. The code will be made publicly available.

</details>


### [10] [AIM 2025 Challenge on Real-World RAW Image Denoising](https://arxiv.org/abs/2510.06601)
*Feiran Li,Jiacheng Li,Marcos V. Conde,Beril Besbinar,Vlad Hosu,Daisuke Iso,Radu Timofte*

Main category: cs.CV

TL;DR: AIM 2025真实世界RAW图像去噪挑战赛，旨在通过数据合成推进高效去噪技术，使用五种不同DSLR相机在真实低光环境下拍摄的图像作为评测基准。


<details>
  <summary>Details</summary>
Motivation: 推动基于合成数据的相机无关低光RAW图像去噪技术的发展，促进与数字摄影快速进步相一致的鲁棒实用模型开发。

Method: 参与者需要开发新颖的噪声合成流程、网络架构和训练方法，在不同相机型号上实现高性能。

Result: 优胜者基于全参考指标（PSNR、SSIM、LPIPS）和无参考指标（ARNIQA、TOPIQ）的综合表现确定。

Conclusion: 该竞赛成果预计将影响从图像恢复到夜间自动驾驶等多个领域。

Abstract: We introduce the AIM 2025 Real-World RAW Image Denoising Challenge, aiming to advance efficient and effective denoising techniques grounded in data synthesis. The competition is built upon a newly established evaluation benchmark featuring challenging low-light noisy images captured in the wild using five different DSLR cameras. Participants are tasked with developing novel noise synthesis pipelines, network architectures, and training methodologies to achieve high performance across different camera models. Winners are determined based on a combination of performance metrics, including full-reference measures (PSNR, SSIM, LPIPS), and non-reference ones (ARNIQA, TOPIQ). By pushing the boundaries of camera-agnostic low-light RAW image denoising trained on synthetic data, the competition promotes the development of robust and practical models aligned with the rapid progress in digital photography. We expect the competition outcomes to influence multiple domains, from image restoration to night-time autonomous driving.

</details>


### [11] [SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis](https://arxiv.org/abs/2510.06694)
*Jipeng Lyu,Jiahua Dong,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: SCas4D是一个级联优化框架，利用3D高斯泼溅中的结构模式来建模动态场景，通过从粗到细的变形优化实现高效训练和高质量结果。


<details>
  <summary>Details</summary>
Motivation: 解决动态场景建模中准确捕捉变形同时保持计算效率的挑战，利用现实世界变形通常呈现层次化模式的特点。

Method: 采用级联优化框架，从粗粒度的部件级变形逐步细化到细粒度的点级变形，利用高斯群组共享相似变换的特性。

Result: 每个时间帧在100次迭代内收敛，仅需现有方法二十分之一的训练迭代次数即可达到可比结果，在自监督关节物体分割、新视角合成和密集点跟踪任务中表现有效。

Conclusion: SCas4D通过层次化变形建模实现了动态场景的高效高质量建模，在多个任务中展现出优越性能。

Abstract: Persistent dynamic scene modeling for tracking and novel-view synthesis remains challenging due to the difficulty of capturing accurate deformations while maintaining computational efficiency. We propose SCas4D, a cascaded optimization framework that leverages structural patterns in 3D Gaussian Splatting for dynamic scenes. The key idea is that real-world deformations often exhibit hierarchical patterns, where groups of Gaussians share similar transformations. By progressively refining deformations from coarse part-level to fine point-level, SCas4D achieves convergence within 100 iterations per time frame and produces results comparable to existing methods with only one-twentieth of the training iterations. The approach also demonstrates effectiveness in self-supervised articulated object segmentation, novel view synthesis, and dense point tracking tasks.

</details>


### [12] [DeRainMamba: A Frequency-Aware State Space Model with Detail Enhancement for Image Deraining](https://arxiv.org/abs/2510.06746)
*Zhiliang Zhu,Tao Zeng,Tao Yang,Guoliang Luo,Jiyong Zeng*

Main category: cs.CV

TL;DR: 提出DeRainMamba模型，结合频率感知状态空间模块和多向感知卷积，在图像去雨任务中实现更好的细节保留和雨纹去除效果。


<details>
  <summary>Details</summary>
Motivation: 现有Mamba模型在捕捉细粒度细节和频率域感知方面能力有限，限制了图像去雨性能的进一步提升。

Method: 集成频率感知状态空间模块(FASSM)和多向感知卷积(MDPConv)，FASSM利用傅里叶变换区分雨纹和图像高频细节，MDPConv通过捕获各向异性梯度特征和多分支融合来恢复局部结构。

Result: 在四个公开基准测试中，DeRainMamba在PSNR和SSIM指标上持续优于最先进方法，同时需要更少的参数和计算成本。

Conclusion: 在状态空间框架中结合频率域建模和空间细节增强对于单图像去雨是有效的。

Abstract: Image deraining is crucial for improving visual quality and supporting reliable downstream vision tasks. Although Mamba-based models provide efficient sequence modeling, their limited ability to capture fine-grained details and lack of frequency-domain awareness restrict further improvements. To address these issues, we propose DeRainMamba, which integrates a Frequency-Aware State-Space Module (FASSM) and Multi-Directional Perception Convolution (MDPConv). FASSM leverages Fourier transform to distinguish rain streaks from high-frequency image details, balancing rain removal and detail preservation. MDPConv further restores local structures by capturing anisotropic gradient features and efficiently fusing multiple convolution branches. Extensive experiments on four public benchmarks demonstrate that DeRainMamba consistently outperforms state-of-the-art methods in PSNR and SSIM, while requiring fewer parameters and lower computational costs. These results validate the effectiveness of combining frequency-domain modeling and spatial detail enhancement within a state-space framework for single image deraining.

</details>


### [13] [OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot](https://arxiv.org/abs/2510.06751)
*Junhan Zhu,Hesong Wang,Mingluo Su,Zefang Wang,Huan Wang*

Main category: cs.CV

TL;DR: OBS-Diff是一种新颖的一次性剪枝框架，用于准确且无需训练地压缩大规模文生图扩散模型，通过改进的OBS算法、时间步感知Hessian构建和分组顺序剪枝策略实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: 大规模文生图扩散模型计算成本过高，现有的一次性网络剪枝方法由于扩散模型的迭代去噪特性而难以直接应用。

Method: 改进经典OBS算法适应现代扩散模型架构；提出时间步感知Hessian构建，采用对数递减加权方案；设计计算高效的分组顺序剪枝策略。

Result: OBS-Diff在扩散模型的一次性剪枝中达到最先进水平，在视觉质量最小退化的情况下实现推理加速。

Conclusion: OBS-Diff成功解决了大规模扩散模型的高效压缩问题，为实际应用提供了可行的解决方案。

Abstract: Large-scale text-to-image diffusion models, while powerful, suffer from prohibitive computational cost. Existing one-shot network pruning methods can hardly be directly applied to them due to the iterative denoising nature of diffusion models. To bridge the gap, this paper presents OBS-Diff, a novel one-shot pruning framework that enables accurate and training-free compression of large-scale text-to-image diffusion models. Specifically, (i) OBS-Diff revitalizes the classic Optimal Brain Surgeon (OBS), adapting it to the complex architectures of modern diffusion models and supporting diverse pruning granularity, including unstructured, N:M semi-structured, and structured (MHA heads and FFN neurons) sparsity; (ii) To align the pruning criteria with the iterative dynamics of the diffusion process, by examining the problem from an error-accumulation perspective, we propose a novel timestep-aware Hessian construction that incorporates a logarithmic-decrease weighting scheme, assigning greater importance to earlier timesteps to mitigate potential error accumulation; (iii) Furthermore, a computationally efficient group-wise sequential pruning strategy is proposed to amortize the expensive calibration process. Extensive experiments show that OBS-Diff achieves state-of-the-art one-shot pruning for diffusion models, delivering inference acceleration with minimal degradation in visual quality.

</details>


### [14] [Transforming Noise Distributions with Histogram Matching: Towards a Single Denoiser for All](https://arxiv.org/abs/2510.06757)
*Sheng Fu,Junchao Zhang,Kailun Yang*

Main category: cs.CV

TL;DR: 提出一种直方图匹配方法，将任意噪声转换为目标高斯分布，通过噪声转换与去噪的相互增强循环，使单个高斯去噪器能够处理各种分布外噪声。


<details>
  <summary>Details</summary>
Motivation: 监督高斯去噪器在处理分布外噪声时泛化能力有限，因为不同噪声类型具有不同的分布特性。

Method: 使用直方图匹配将任意噪声转换为已知强度的目标高斯分布，建立噪声转换与去噪的相互增强循环，并针对特定噪声复杂性采用局部直方图匹配、片内置换和频域直方图匹配等方法。

Result: 单个高斯去噪器获得了显著处理各种分布外噪声的能力，包括泊松噪声、椒盐噪声、重复模式噪声以及复杂的真实世界噪声。

Conclusion: 该方法在广泛实验中展示了优越的泛化能力和有效性。

Abstract: Supervised Gaussian denoisers exhibit limited generalization when confronted with out-of-distribution noise, due to the diverse distributional characteristics of different noise types. To bridge this gap, we propose a histogram matching approach that transforms arbitrary noise towards a target Gaussian distribution with known intensity. Moreover, a mutually reinforcing cycle is established between noise transformation and subsequent denoising. This cycle progressively refines the noise to be converted, making it approximate the real noise, thereby enhancing the noise transformation effect and further improving the denoising performance. We tackle specific noise complexities: local histogram matching handles signal-dependent noise, intrapatch permutation processes channel-related noise, and frequency-domain histogram matching coupled with pixel-shuffle down-sampling breaks spatial correlation. By applying these transformations, a single Gaussian denoiser gains remarkable capability to handle various out-of-distribution noises, including synthetic noises such as Poisson, salt-and-pepper and repeating pattern noises, as well as complex real-world noises. Extensive experiments demonstrate the superior generalization and effectiveness of our method.

</details>


### [15] [StyleKeeper: Prevent Content Leakage using Negative Visual Query Guidance](https://arxiv.org/abs/2510.06827)
*Jaeseok Jeong,Junho Kim,Gayoung Lee,Yunjey Choi,Youngjung Uh*

Main category: cs.CV

TL;DR: 提出了一种名为负视觉查询引导（NVQG）的新方法，通过扩展无分类器引导并交换自注意力层的查询来减少文本到图像生成中的内容泄漏问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉提示方法在控制风格时经常出现内容泄漏问题，即视觉风格提示中不需要的内容元素会与预期风格一起被转移。

Method: 1) 扩展无分类器引导（CFG）以利用交换自注意力；2) 提出负视觉查询引导（NVQG），通过故意模拟内容泄漏场景来交换自注意力层的查询而非键和值。

Result: 该方法在各种风格和文本提示的广泛评估中表现出优于现有方法的性能，能准确反映参考图像的风格，同时确保生成图像与文本提示匹配。

Conclusion: NVQG是一种简单而有效的方法，能显著减少内容泄漏问题，并为使用真实图像作为视觉风格提示提供了解决方案。

Abstract: In the domain of text-to-image generation, diffusion models have emerged as powerful tools. Recently, studies on visual prompting, where images are used as prompts, have enabled more precise control over style and content. However, existing methods often suffer from content leakage, where undesired elements of the visual style prompt are transferred along with the intended style. To address this issue, we 1) extend classifier-free guidance (CFG) to utilize swapping self-attention and propose 2) negative visual query guidance (NVQG) to reduce the transfer of unwanted contents. NVQG employs negative score by intentionally simulating content leakage scenarios that swap queries instead of key and values of self-attention layers from visual style prompts. This simple yet effective method significantly reduces content leakage. Furthermore, we provide careful solutions for using a real image as visual style prompts. Through extensive evaluation across various styles and text prompts, our method demonstrates superiority over existing approaches, reflecting the style of the references, and ensuring that resulting images match the text prompts. Our code is available \href{https://github.com/naver-ai/StyleKeeper}{here}.

</details>


### [16] [IAR2: Improving Autoregressive Visual Generation with Semantic-Detail Associated Token Prediction](https://arxiv.org/abs/2510.06928)
*Ran Yi,Teng Hu,Zihan Su,Lizhuang Ma*

Main category: cs.CV

TL;DR: IAR2提出了一种分层语义-细节合成的自回归框架，通过语义-细节关联双码本将图像表示解耦为语义码本和细节码本，显著提升了生成质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有自回归模型忽略了视觉数据的内在结构特性，且受限于预训练码本的刚性和硬聚类的局限性，需要更灵活的分层表示方法。

Method: 提出语义-细节关联双码本、语义-细节自回归预测方案、局部上下文增强自回归头，以及渐进注意力引导自适应CFG机制。

Result: 在ImageNet上达到FID 1.50的新SOTA，不仅性能超越先前方法，还展现出优越的计算效率。

Conclusion: IAR2通过结构化、从粗到细的生成策略，证明了分层语义-细节合成在自回归图像生成中的有效性。

Abstract: Autoregressive models have emerged as a powerful paradigm for visual content creation, but often overlook the intrinsic structural properties of visual data. Our prior work, IAR, initiated a direction to address this by reorganizing the visual codebook based on embedding similarity, thereby improving generation robustness. However, it is constrained by the rigidity of pre-trained codebooks and the inaccuracies of hard, uniform clustering. To overcome these limitations, we propose IAR2, an advanced autoregressive framework that enables a hierarchical semantic-detail synthesis process. At the core of IAR2 is a novel Semantic-Detail Associated Dual Codebook, which decouples image representations into a semantic codebook for global semantic information and a detail codebook for fine-grained refinements. It expands the quantization capacity from a linear to a polynomial scale, significantly enhancing expressiveness. To accommodate this dual representation, we propose a Semantic-Detail Autoregressive Prediction scheme coupled with a Local-Context Enhanced Autoregressive Head, which performs hierarchical prediction-first the semantic token, then the detail token-while leveraging a local context window to enhance spatial coherence. Furthermore, for conditional generation, we introduce a Progressive Attention-Guided Adaptive CFG mechanism that dynamically modulates the guidance scale for each token based on its relevance to the condition and its temporal position in the generation sequence, improving conditional alignment without sacrificing realism. Extensive experiments demonstrate that IAR2 sets a new state-of-the-art for autoregressive image generation, achieving a FID of 1.50 on ImageNet. Our model not only surpasses previous methods in performance but also demonstrates superior computational efficiency, highlighting the effectiveness of our structured, coarse-to-fine generation strategy.

</details>


### [17] [Generating Surface for Text-to-3D using 2D Gaussian Splatting](https://arxiv.org/abs/2510.06967)
*Huanning Dong,Fan Li,Ping Kuang,Jianwen Min*

Main category: cs.CV

TL;DR: 提出DirectGaussian方法，通过条件文本生成模型和2D高斯泼溅，结合多视角法线和纹理先验，生成基于surfels的3D物体表面，并在优化过程中加入曲率约束解决几何一致性问题。


<details>
  <summary>Details</summary>
Motivation: 自然世界中物体几何形状复杂，现有方法要么使用2D扩散先验恢复3D几何，要么基于特定3D表示直接训练模型，3D内容生成仍具挑战性。

Method: 使用条件文本生成模型，通过2D高斯泼溅渲染3D物体表面，结合多视角法线和纹理先验，在优化过程中加入曲率约束确保多视角几何一致性。

Result: 通过大量实验证明，该框架能够实现多样化和高保真度的3D内容创建。

Conclusion: DirectGaussian方法在3D内容生成方面表现出色，能够处理复杂几何形状并保持高质量输出。

Abstract: Recent advancements in Text-to-3D modeling have shown significant potential for the creation of 3D content. However, due to the complex geometric shapes of objects in the natural world, generating 3D content remains a challenging task. Current methods either leverage 2D diffusion priors to recover 3D geometry, or train the model directly based on specific 3D representations. In this paper, we propose a novel method named DirectGaussian, which focuses on generating the surfaces of 3D objects represented by surfels. In DirectGaussian, we utilize conditional text generation models and the surface of a 3D object is rendered by 2D Gaussian splatting with multi-view normal and texture priors. For multi-view geometric consistency problems, DirectGaussian incorporates curvature constraints on the generated surface during optimization process. Through extensive experiments, we demonstrate that our framework is capable of achieving diverse and high-fidelity 3D content creation.

</details>


### [18] [Addressing the ID-Matching Challenge in Long Video Captioning](https://arxiv.org/abs/2510.06973)
*Zhantao Yang,Huangji Wang,Ruili Feng,Han Zhang,Yuting Hu,Shangwen Zhu,Junyan Li,Yu Liu,Fan Cheng*

Main category: cs.CV

TL;DR: 该论文提出了RICE方法，利用大型视觉语言模型的内在ID匹配能力来解决长视频字幕中的身份识别问题，显著提升了ID匹配的准确率和召回率。


<details>
  <summary>Details</summary>
Motivation: 长视频字幕生成中的关键挑战是准确识别不同帧中出现的同一人物（ID匹配问题），现有方法泛化能力有限且依赖点对点匹配，效果不佳。

Method: 基于LVLMs构建RICE方法，通过增强图像信息利用和增加个体描述信息量两种方式来提升ID匹配性能，并建立了新的评估基准。

Result: 在GPT-4o上实现，RICE将ID匹配的精确度从50%提升到90%，召回率从15%提升到80%，显著优于基线方法。

Conclusion: RICE方法能够有效跟踪长视频字幕中不同个体的连续出现，为文本到视频生成和多模态理解领域提供了重要解决方案。

Abstract: Generating captions for long and complex videos is both critical and challenging, with significant implications for the growing fields of text-to-video generation and multi-modal understanding. One key challenge in long video captioning is accurately recognizing the same individuals who appear in different frames, which we refer to as the ID-Matching problem. Few prior works have focused on this important issue. Those that have, usually suffer from limited generalization and depend on point-wise matching, which limits their overall effectiveness. In this paper, unlike previous approaches, we build upon LVLMs to leverage their powerful priors. We aim to unlock the inherent ID-Matching capabilities within LVLMs themselves to enhance the ID-Matching performance of captions. Specifically, we first introduce a new benchmark for assessing the ID-Matching capabilities of video captions. Using this benchmark, we investigate LVLMs containing GPT-4o, revealing key insights that the performance of ID-Matching can be improved through two methods: 1) enhancing the usage of image information and 2) increasing the quantity of information of individual descriptions. Based on these insights, we propose a novel video captioning method called Recognizing Identities for Captioning Effectively (RICE). Extensive experiments including assessments of caption quality and ID-Matching performance, demonstrate the superiority of our approach. Notably, when implemented on GPT-4o, our RICE improves the precision of ID-Matching from 50% to 90% and improves the recall of ID-Matching from 15% to 80% compared to baseline. RICE makes it possible to continuously track different individuals in the captions of long videos.

</details>


### [19] [No MoCap Needed: Post-Training Motion Diffusion Models with Reinforcement Learning using Only Textual Prompts](https://arxiv.org/abs/2510.06988)
*Girolamo Macaluso,Lorenzo Mandelli,Mirko Bicchierai,Stefano Berretti,Andrew D. Bagdanov*

Main category: cs.CV

TL;DR: 提出基于强化学习的后训练框架，仅使用文本提示微调预训练运动扩散模型，无需运动真值数据，实现跨数据集和留一运动适应。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在人体运动生成方面取得进展，但适应未见动作或风格通常需要额外运动捕捉数据和完整重训练，成本高且难以扩展。

Method: 使用预训练文本-运动检索网络作为奖励信号，通过Denoising Diffusion Policy Optimization优化扩散策略，无需配对运动数据即可将生成分布转向目标域。

Result: 在HumanML3D和KIT-ML数据集上的评估显示，该方法在定量指标和用户研究中一致提高了生成运动的质量和多样性，同时保持原始分布性能。

Conclusion: 该方法为运动适应提供了灵活、数据高效且保护隐私的解决方案。

Abstract: Diffusion models have recently advanced human motion generation, producing realistic and diverse animations from textual prompts. However, adapting these models to unseen actions or styles typically requires additional motion capture data and full retraining, which is costly and difficult to scale. We propose a post-training framework based on Reinforcement Learning that fine-tunes pretrained motion diffusion models using only textual prompts, without requiring any motion ground truth. Our approach employs a pretrained text-motion retrieval network as a reward signal and optimizes the diffusion policy with Denoising Diffusion Policy Optimization, effectively shifting the model's generative distribution toward the target domain without relying on paired motion data. We evaluate our method on cross-dataset adaptation and leave-one-out motion experiments using the HumanML3D and KIT-ML datasets across both latent- and joint-space diffusion architectures. Results from quantitative metrics and user studies show that our approach consistently improves the quality and diversity of generated motions, while preserving performance on the original distribution. Our approach is a flexible, data-efficient, and privacy-preserving solution for motion adaptation.

</details>


### [20] [Bayesian Modelling of Multi-Year Crop Type Classification Using Deep Neural Networks and Hidden Markov Models](https://arxiv.org/abs/2510.07008)
*Gianmarco Perantoni,Giulio Weikmann,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 提出了一种结合深度学习和贝叶斯建模的新方法，使用隐马尔可夫模型与Transformer编码器进行年度卫星图像时间序列分类，旨在捕捉复杂的时间相关性和多年作物类型序列模式。


<details>
  <summary>Details</summary>
Motivation: 年度土地覆盖地图的时间一致性对于建模土地覆盖的演变和变化至关重要，需要解决年度卫星图像时间序列分类中的时间一致性问题。

Method: 将隐马尔可夫模型层构建在Transformer编码器之上，利用HMM进行级联分类，识别一致的年度作物类型序列。

Result: 在包含47种作物类型和6年Sentinel-2采集数据的多年作物类型分类数据集上验证，HMM提高了整体性能和F1分数。

Conclusion: 建模预测标签中的时间一致性非常重要，所提出的方法在增强时间一致性方面表现出有效性。

Abstract: The temporal consistency of yearly land-cover maps is of great importance to model the evolution and change of the land cover over the years. In this paper, we focus the attention on a novel approach to classification of yearly satellite image time series (SITS) that combines deep learning with Bayesian modelling, using Hidden Markov Models (HMMs) integrated with Transformer Encoder (TE) based DNNs. The proposed approach aims to capture both i) intricate temporal correlations in yearly SITS and ii) specific patterns in multiyear crop type sequences. It leverages the cascade classification of an HMM layer built on top of the TE, discerning consistent yearly crop-type sequences. Validation on a multiyear crop type classification dataset spanning 47 crop types and six years of Sentinel-2 acquisitions demonstrates the importance of modelling temporal consistency in the predicted labels. HMMs enhance the overall performance and F1 scores, emphasising the effectiveness of the proposed approach.

</details>


### [21] [Graph Conditioned Diffusion for Controllable Histopathology Image Generation](https://arxiv.org/abs/2510.07129)
*Sarah Cechnicka,Matthew Baugh,Weitong Zhang,Mischa Dombrowski,Zhe Li,Johannes C. Paetzold,Candice Roufosse,Bernhard Kainz*

Main category: cs.CV

TL;DR: 提出基于图的对象级表示方法Graph-Conditioned-Diffusion，用于医学图像生成，通过图节点表示主要结构特征和关系，实现细粒度控制生成。


<details>
  <summary>Details</summary>
Motivation: 现有扩散概率模型在噪声潜在空间中缺乏语义结构和强先验，难以在医学图像等敏感领域实现有意义的控制生成。医学图像具有固有的结构特征，对诊断至关重要。

Method: 生成对应图像中每个主要结构的图节点，封装个体特征和关系；使用transformer模块处理图表示，通过文本条件机制集成到扩散模型中。

Result: 在真实组织病理学用例中评估，生成的数据可以可靠地替代标注的患者数据用于下游分割任务。

Conclusion: 图条件扩散方法能够实现医学图像的细粒度控制生成，生成的合成数据可有效替代真实标注数据。

Abstract: Recent advances in Diffusion Probabilistic Models (DPMs) have set new standards in high-quality image synthesis. Yet, controlled generation remains challenging, particularly in sensitive areas such as medical imaging. Medical images feature inherent structure such as consistent spatial arrangement, shape or texture, all of which are critical for diagnosis. However, existing DPMs operate in noisy latent spaces that lack semantic structure and strong priors, making it difficult to ensure meaningful control over generated content. To address this, we propose graph-based object-level representations for Graph-Conditioned-Diffusion. Our approach generates graph nodes corresponding to each major structure in the image, encapsulating their individual features and relationships. These graph representations are processed by a transformer module and integrated into a diffusion model via the text-conditioning mechanism, enabling fine-grained control over generation. We evaluate this approach using a real-world histopathology use case, demonstrating that our generated data can reliably substitute for annotated patient data in downstream segmentation tasks. The code is available here.

</details>


### [22] [MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis](https://arxiv.org/abs/2510.07190)
*Yihao Zhi,Chenghong Li,Hongjie Liao,Xihe Yang,Zhengwentai Sun,Jiahao Chang,Xiaodong Cun,Wensen Feng,Xiaoguang Han*

Main category: cs.CV

TL;DR: MV-Performer是一个用于从单目全身捕捉生成同步多视角视频的创新框架，专注于人体中心的新视角合成，能够实现360度视角变化。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成方法主要局限于前视角的相机轨迹重定向，难以生成360度视角变化，特别是在人体中心领域。

Method: 使用MVHumanNet数据集，结合相机相关法线图作为条件信号，提出多视角人体中心视频扩散模型，融合参考视频、部分渲染和不同视角的信息。

Result: 在三个数据集上的广泛实验表明，MV-Performer在人体中心4D新视角合成方面具有最先进的有效性和鲁棒性。

Conclusion: MV-Performer为人体中心4D新视角合成建立了一个强大的模型基准。

Abstract: Recent breakthroughs in video generation, powered by large-scale datasets and diffusion techniques, have shown that video diffusion models can function as implicit 4D novel view synthesizers. Nevertheless, current methods primarily concentrate on redirecting camera trajectory within the front view while struggling to generate 360-degree viewpoint changes. In this paper, we focus on human-centric subdomain and present MV-Performer, an innovative framework for creating synchronized novel view videos from monocular full-body captures. To achieve a 360-degree synthesis, we extensively leverage the MVHumanNet dataset and incorporate an informative condition signal. Specifically, we use the camera-dependent normal maps rendered from oriented partial point clouds, which effectively alleviate the ambiguity between seen and unseen observations. To maintain synchronization in the generated videos, we propose a multi-view human-centric video diffusion model that fuses information from the reference video, partial rendering, and different viewpoints. Additionally, we provide a robust inference procedure for in-the-wild video cases, which greatly mitigates the artifacts induced by imperfect monocular depth estimation. Extensive experiments on three datasets demonstrate our MV-Performer's state-of-the-art effectiveness and robustness, setting a strong model for human-centric 4D novel view synthesis.

</details>


### [23] [EigenScore: OOD Detection using Covariance in Diffusion Models](https://arxiv.org/abs/2510.07206)
*Shirin Shoushtari,Yi Wang,Xiao Shi,M. Salman Asif,Ulugbek S. Kamilov*

Main category: cs.CV

TL;DR: EigenScore是一种新的OOD检测方法，利用扩散模型诱导的后验协方差矩阵的特征值谱来检测分布外数据，在保持计算可行性的同时实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: OOD检测对于机器学习系统在安全敏感领域的部署至关重要。扩散模型作为强大的生成模型，能够捕捉复杂的数据分布，但现有的基于扩散的OOD检测方法在近OOD场景中表现不佳。

Method: 提出EigenScore方法，利用扩散模型后验协方差矩阵的特征值谱作为分布偏移的指标。采用无雅可比矩阵的子空间迭代方法仅通过去噪器的前向评估来估计主导特征值，确保计算可行性。

Result: EigenScore实现了最先进的性能，AUROC比最佳基线提高了5%。在CIFAR-10 vs CIFAR-100等近OOD场景中保持鲁棒性，而现有基于扩散的方法往往失败。

Conclusion: 后验协方差提供了分布偏移的一致信号，EigenScore通过利用这一信号实现了有效的OOD检测，特别是在具有挑战性的近OOD场景中表现出色。

Abstract: Out-of-distribution (OOD) detection is critical for the safe deployment of machine learning systems in safety-sensitive domains. Diffusion models have recently emerged as powerful generative models, capable of capturing complex data distributions through iterative denoising. Building on this progress, recent work has explored their potential for OOD detection. We propose EigenScore, a new OOD detection method that leverages the eigenvalue spectrum of the posterior covariance induced by a diffusion model. We argue that posterior covariance provides a consistent signal of distribution shift, leading to larger trace and leading eigenvalues on OOD inputs, yielding a clear spectral signature. We further provide analysis explicitly linking posterior covariance to distribution mismatch, establishing it as a reliable signal for OOD detection. To ensure tractability, we adopt a Jacobian-free subspace iteration method to estimate the leading eigenvalues using only forward evaluations of the denoiser. Empirically, EigenScore achieves SOTA performance, with up to 5% AUROC improvement over the best baseline. Notably, it remains robust in near-OOD settings such as CIFAR-10 vs CIFAR-100, where existing diffusion-based methods often fail.

</details>


### [24] [GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in Image Generation](https://arxiv.org/abs/2510.07217)
*Wen Ye,Zhaocheng Liu,Yuwei Gui,Tingyu Yuan,Yunyue Su,Bowen Fang,Chaoyang Zhao,Qiang Liu,Liang Wang*

Main category: cs.CV

TL;DR: 提出GenPilot，一种即插即用的多代理系统，用于测试时提示优化，通过错误分析、聚类自适应探索、细粒度验证和内存模块迭代优化，提升文本到图像生成的一致性和结构连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像合成在处理复杂长提示时存在语义不一致和细节缺失问题，现有解决方案如微调需要训练且模型特定，自动提示优化方法缺乏系统错误分析和优化策略，测试时缩放方法在固定提示上操作，缺乏可解释性和适应性。

Method: 提出GenPilot多代理系统，包含错误分析、聚类自适应探索、细粒度验证和内存模块，直接在输入文本上进行测试时提示优化，无需训练且模型无关。

Result: 在DPG-bench和Geneval数据集上分别提升16.9%和5.7%，显著增强生成图像的文本一致性和结构连贯性。

Conclusion: GenPilot提供了一种灵活高效的测试时提示优化策略，能够有效处理复杂长提示，总结错误模式和优化策略，为后续研究提供经验。

Abstract: Text-to-image synthesis has made remarkable progress, yet accurately interpreting complex and lengthy prompts remains challenging, often resulting in semantic inconsistencies and missing details. Existing solutions, such as fine-tuning, are model-specific and require training, while prior automatic prompt optimization (APO) approaches typically lack systematic error analysis and refinement strategies, resulting in limited reliability and effectiveness. Meanwhile, test-time scaling methods operate on fixed prompts and on noise or sample numbers, limiting their interpretability and adaptability. To solve these, we introduce a flexible and efficient test-time prompt optimization strategy that operates directly on the input text. We propose a plug-and-play multi-agent system called GenPilot, integrating error analysis, clustering-based adaptive exploration, fine-grained verification, and a memory module for iterative optimization. Our approach is model-agnostic, interpretable, and well-suited for handling long and complex prompts. Simultaneously, we summarize the common patterns of errors and the refinement strategy, offering more experience and encouraging further exploration. Experiments on DPG-bench and Geneval with improvements of up to 16.9% and 5.7% demonstrate the strong capability of our methods in enhancing the text and image consistency and structural coherence of generated images, revealing the effectiveness of our test-time prompt optimization strategy. The code is available at https://github.com/27yw/GenPilot.

</details>


### [25] [TalkCuts: A Large-Scale Dataset for Multi-Shot Human Speech Video Generation](https://arxiv.org/abs/2510.07249)
*Jiaben Chen,Zixin Wang,Ailing Zeng,Yang Fu,Xueyang Yu,Siyuan Cen,Julian Tanke,Yihang Chen,Koichi Saito,Yuki Mitsufuji,Chuang Gan*

Main category: cs.CV

TL;DR: TalkCuts是一个大规模多镜头人类语音视频数据集，包含164k个片段、500+小时高质量视频，涵盖多种拍摄角度和详细标注。作者还提出了Orator框架作为基线，展示该数据集在多镜头语音视频生成中的价值。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要关注单镜头静态视角，缺乏多镜头、动态视角的语音视频数据，限制了多镜头语音视频生成的研究。

Method: 构建TalkCuts大规模数据集，并提出Orator框架——一个LLM引导的多模态生成框架，其中语言模型作为导演协调摄像机转换、手势和语音调制。

Result: 在姿态引导和音频驱动设置下的广泛实验表明，在TalkCuts上训练显著提升了生成多镜头语音视频的电影连贯性和视觉吸引力。

Conclusion: TalkCuts为可控多镜头语音视频生成和更广泛的多模态学习提供了坚实基础。

Abstract: In this work, we present TalkCuts, a large-scale dataset designed to facilitate the study of multi-shot human speech video generation. Unlike existing datasets that focus on single-shot, static viewpoints, TalkCuts offers 164k clips totaling over 500 hours of high-quality human speech videos with diverse camera shots, including close-up, half-body, and full-body views. The dataset includes detailed textual descriptions, 2D keypoints and 3D SMPL-X motion annotations, covering over 10k identities, enabling multimodal learning and evaluation. As a first attempt to showcase the value of the dataset, we present Orator, an LLM-guided multi-modal generation framework as a simple baseline, where the language model functions as a multi-faceted director, orchestrating detailed specifications for camera transitions, speaker gesticulations, and vocal modulation. This architecture enables the synthesis of coherent long-form videos through our integrated multi-modal video generation module. Extensive experiments in both pose-guided and audio-driven settings show that training on TalkCuts significantly enhances the cinematographic coherence and visual appeal of generated multi-shot speech videos. We believe TalkCuts provides a strong foundation for future work in controllable, multi-shot speech video generation and broader multimodal learning.

</details>


### [26] [MATRIX: Mask Track Alignment for Interaction-aware Video Generation](https://arxiv.org/abs/2510.07310)
*Siyoon Jin,Seongchan Kim,Dahyun Chung,Jaeho Lee,Hyunwook Choi,Jisu Nam,Jiyoung Kim,Seungryong Kim*

Main category: cs.CV

TL;DR: 论文分析了视频DiTs在建模多实例和主体-客体交互方面的不足，提出了MATRIX-11K数据集和MATRIX正则化方法，通过注意力对齐增强交互保真度和语义对齐。


<details>
  <summary>Details</summary>
Motivation: 当前视频DiTs在多实例或主体-客体交互建模方面存在困难，需要理解这些模型内部如何表示交互关系。

Method: 构建MATRIX-11K数据集，系统分析视频DiTs的语义基础和语义传播特性，提出MATRIX正则化方法对齐注意力与多实例掩码轨迹。

Result: MATRIX方法提高了交互保真度和语义对齐，减少了漂移和幻觉现象。

Conclusion: 通过注意力对齐的简单正则化方法能有效提升视频DiTs的交互建模能力，为交互感知视频生成提供了新的评估协议。

Abstract: Video DiTs have advanced video generation, yet they still struggle to model multi-instance or subject-object interactions. This raises a key question: How do these models internally represent interactions? To answer this, we curate MATRIX-11K, a video dataset with interaction-aware captions and multi-instance mask tracks. Using this dataset, we conduct a systematic analysis that formalizes two perspectives of video DiTs: semantic grounding, via video-to-text attention, which evaluates whether noun and verb tokens capture instances and their relations; and semantic propagation, via video-to-video attention, which assesses whether instance bindings persist across frames. We find both effects concentrate in a small subset of interaction-dominant layers. Motivated by this, we introduce MATRIX, a simple and effective regularization that aligns attention in specific layers of video DiTs with multi-instance mask tracks from the MATRIX-11K dataset, enhancing both grounding and propagation. We further propose InterGenEval, an evaluation protocol for interaction-aware video generation. In experiments, MATRIX improves both interaction fidelity and semantic alignment while reducing drift and hallucination. Extensive ablations validate our design choices. Codes and weights will be released.

</details>


### [27] [WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation](https://arxiv.org/abs/2510.07313)
*Zezhong Qian,Xiaowei Chi,Yuming Li,Shizun Wang,Zhiyuan Qin,Xiaozhu Ju,Sirui Han,Shanghang Zhang*

Main category: cs.CV

TL;DR: 提出了WristWorld，首个仅从锚点视角生成腕部视角视频的4D世界模型，通过几何重建和视频生成两阶段方法解决锚点-腕部视角差距问题。


<details>
  <summary>Details</summary>
Motivation: 腕部视角观察对VLA模型至关重要，但大规模数据集很少包含此类记录，导致锚点视角丰富而腕部视角稀缺的显著差距。

Method: 两阶段方法：重建阶段扩展VGGT并引入空间投影一致性损失来估计几何一致的腕部视角姿态和4D点云；生成阶段使用视频生成模型从重建视角合成时序连贯的腕部视角视频。

Result: 在Droid、Calvin和Franka Panda数据集上实现了最先进的视频生成效果，具有优越的空间一致性，同时提高了VLA性能，在Calvin上平均任务完成长度提升3.81%，填补了42.4%的锚点-腕部视角差距。

Conclusion: WristWorld成功解决了锚点视角到腕部视角的转换问题，为VLA模型提供了关键的腕部视角观察，显著提升了操作性能。

Abstract: Wrist-view observations are crucial for VLA models as they capture fine-grained hand-object interactions that directly enhance manipulation performance. Yet large-scale datasets rarely include such recordings, resulting in a substantial gap between abundant anchor views and scarce wrist views. Existing world models cannot bridge this gap, as they require a wrist-view first frame and thus fail to generate wrist-view videos from anchor views alone. Amid this gap, recent visual geometry models such as VGGT emerge with geometric and cross-view priors that make it possible to address extreme viewpoint shifts. Inspired by these insights, we propose WristWorld, the first 4D world model that generates wrist-view videos solely from anchor views. WristWorld operates in two stages: (i) Reconstruction, which extends VGGT and incorporates our Spatial Projection Consistency (SPC) Loss to estimate geometrically consistent wrist-view poses and 4D point clouds; (ii) Generation, which employs our video generation model to synthesize temporally coherent wrist-view videos from the reconstructed perspective. Experiments on Droid, Calvin, and Franka Panda demonstrate state-of-the-art video generation with superior spatial consistency, while also improving VLA performance, raising the average task completion length on Calvin by 3.81% and closing 42.4% of the anchor-wrist view gap.

</details>


### [28] [Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers](https://arxiv.org/abs/2510.07316)
*Gangwei Xu,Haotong Lin,Hongcheng Luo,Xianqi Wang,Jingfeng Yao,Lianghui Zhu,Yuechuan Pu,Cheng Chi,Haiyang Sun,Bing Wang,Guang Chen,Hangjun Ye,Sida Peng,Xin Yang*

Main category: cs.CV

TL;DR: 提出Pixel-Perfect Depth模型，通过像素空间扩散生成实现高质量、无飞行像素的单目深度估计，在多个基准测试中达到最佳性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于Stable Diffusion的生成式深度估计模型需要VAE将深度图压缩到潜在空间，这会在边缘和细节处引入飞行像素伪影。

Method: 1) 直接在像素空间进行扩散生成以避免VAE伪影；2) 引入语义提示扩散变换器(SP-DiT)将视觉基础模型的语义表示融入DiT；3) 采用级联DiT设计逐步增加token数量。

Result: 在五个基准测试中达到所有已发布生成模型的最佳性能，在边缘感知点云评估中显著优于其他所有模型。

Conclusion: 像素空间扩散生成能够有效解决VAE引入的飞行像素问题，结合语义提示和级联设计可以实现高质量、无伪影的深度估计。

Abstract: This paper presents Pixel-Perfect Depth, a monocular depth estimation model based on pixel-space diffusion generation that produces high-quality, flying-pixel-free point clouds from estimated depth maps. Current generative depth estimation models fine-tune Stable Diffusion and achieve impressive performance. However, they require a VAE to compress depth maps into latent space, which inevitably introduces \textit{flying pixels} at edges and details. Our model addresses this challenge by directly performing diffusion generation in the pixel space, avoiding VAE-induced artifacts. To overcome the high complexity associated with pixel-space generation, we introduce two novel designs: 1) Semantics-Prompted Diffusion Transformers (SP-DiT), which incorporate semantic representations from vision foundation models into DiT to prompt the diffusion process, thereby preserving global semantic consistency while enhancing fine-grained visual details; and 2) Cascade DiT Design that progressively increases the number of tokens to further enhance efficiency and accuracy. Our model achieves the best performance among all published generative models across five benchmarks, and significantly outperforms all other models in edge-aware point cloud evaluation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [29] [The False Promise of Zero-Shot Super-Resolution in Machine-Learned Operators](https://arxiv.org/abs/2510.06646)
*Mansi Sakarvadia,Kareem Hegazy,Amin Totounferoush,Kyle Chard,Yaoqing Yang,Ian Foster,Michael W. Mahoney*

Main category: cs.LG

TL;DR: 论文评估了机器学习算子(MLOs)在零样本超分辨率任务中的表现，发现现有MLOs无法在未训练的分辨率上准确推理，存在混叠问题，并提出了一种简单的多分辨率训练协议来解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习中一个核心挑战是建模连续现象，机器学习算子(MLOs)被引入作为实现这一目标的手段，但需要评估其是否能够实现零样本超分辨率推理。

Method: 将多分辨率推理解耦为两个关键行为：1)对不同频率信息的推断；2)在不同分辨率间的插值。通过实证评估MLOs的零样本多分辨率推理能力，并提出了数据驱动的多分辨率训练协议。

Result: 实证研究表明MLOs无法以零样本方式完成频率推断和分辨率插值任务，在未训练的分辨率上推理不准确，容易受到混叠影响。提出的多分辨率训练协议能够克服混叠问题并提供稳健的多分辨率泛化。

Conclusion: 单纯的MLOs架构创新不足以实现零样本超分辨率，需要专门的多分辨率训练策略来确保模型在不同分辨率上的稳健性能。

Abstract: A core challenge in scientific machine learning, and scientific computing more generally, is modeling continuous phenomena which (in practice) are represented discretely. Machine-learned operators (MLOs) have been introduced as a means to achieve this modeling goal, as this class of architecture can perform inference at arbitrary resolution. In this work, we evaluate whether this architectural innovation is sufficient to perform "zero-shot super-resolution," namely to enable a model to serve inference on higher-resolution data than that on which it was originally trained. We comprehensively evaluate both zero-shot sub-resolution and super-resolution (i.e., multi-resolution) inference in MLOs. We decouple multi-resolution inference into two key behaviors: 1) extrapolation to varying frequency information; and 2) interpolating across varying resolutions. We empirically demonstrate that MLOs fail to do both of these tasks in a zero-shot manner. Consequently, we find MLOs are not able to perform accurate inference at resolutions different from those on which they were trained, and instead they are brittle and susceptible to aliasing. To address these failure modes, we propose a simple, computationally-efficient, and data-driven multi-resolution training protocol that overcomes aliasing and that provides robust multi-resolution generalization.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [30] [SER-Diff: Synthetic Error Replay Diffusion for Incremental Brain Tumor Segmentation](https://arxiv.org/abs/2510.06283)
*Sashank Makanaboyina*

Main category: eess.IV

TL;DR: SER-Diff是一个将扩散模型与增量学习统一的新框架，通过生成合成误差图来缓解灾难性遗忘，在脑肿瘤分割任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤分割模型需要适应不断演化的临床数据，但灾难性遗忘问题阻碍了增量学习。现有方法依赖生成重放或辅助存储，而扩散模型在分割细化中有效但未在增量学习中被探索。

Method: 提出SER-Diff框架，利用冻结的教师扩散模型生成过去任务的合成误差图，在新任务训练时重放这些误差图。采用结合Dice损失和知识蒸馏损失的双损失函数，确保适应性和知识保留。

Result: 在BraTS2020、BraTS2021和BraTS2023数据集上的实验表明，SER-Diff始终优于先前方法，分别获得95.8%、94.9%和94.6%的最高Dice分数，以及4.4mm、4.7mm和4.9mm的最低HD95值。

Conclusion: SER-Diff不仅有效缓解了灾难性遗忘，还在演化数据集上提供了更准确和解剖学上更一致的分割结果。

Abstract: Incremental brain tumor segmentation is critical for models that must adapt to evolving clinical datasets without retraining on all prior data. However, catastrophic forgetting, where models lose previously acquired knowledge, remains a major obstacle. Recent incremental learning frameworks with knowledge distillation partially mitigate forgetting but rely heavily on generative replay or auxiliary storage. Meanwhile, diffusion models have proven effective for refining tumor segmentations, but have not been explored in incremental learning contexts. We propose Synthetic Error Replay Diffusion (SER-Diff), the first framework that unifies diffusion-based refinement with incremental learning. SER-Diff leverages a frozen teacher diffusion model to generate synthetic error maps from past tasks, which are replayed during training on new tasks. A dual-loss formulation combining Dice loss for new data and knowledge distillation loss for replayed errors ensures both adaptability and retention. Experiments on BraTS2020, BraTS2021, and BraTS2023 demonstrate that SER-Diff consistently outperforms prior methods. It achieves the highest Dice scores of 95.8\%, 94.9\%, and 94.6\%, along with the lowest HD95 values of 4.4 mm, 4.7 mm, and 4.9 mm, respectively. These results indicate that SER-Diff not only mitigates catastrophic forgetting but also delivers more accurate and anatomically coherent segmentations across evolving datasets.

</details>


### [31] [Conditional Denoising Diffusion Model-Based Robust MR Image Reconstruction from Highly Undersampled Data](https://arxiv.org/abs/2510.06335)
*Mohammed Alsubaie,Wenxi Liu,Linxia Gu,Ovidiu C. Andronesi,Sirani M. Perera,Xianqi Li*

Main category: eess.IV

TL;DR: 提出了一种结合条件去噪扩散模型和迭代数据一致性校正的MRI重建方法，在每次反向扩散步骤中嵌入测量模型，显著提升了重建图像的质量和感知真实性。


<details>
  <summary>Details</summary>
Motivation: MRI采集时间过长是临床诊断的主要限制，现有欠采样方法会导致图像伪影和质量下降。虽然扩散模型在重建高保真图像方面有潜力，但现有方法要么缺乏配对监督，要么仅在后期处理中应用数据一致性。

Method: 开发了条件去噪扩散框架，在每个反向扩散步骤中直接嵌入测量模型，并在配对的欠采样-真实数据上进行训练，将生成灵活性与MRI物理约束相结合。

Result: 在fastMRI数据集上的实验表明，该方法在SSIM、PSNR和LPIPS指标上均优于最新的深度学习和基于扩散的方法，LPIPS更能准确捕捉感知改进。

Conclusion: 将条件监督与迭代一致性更新相结合，在像素级保真度和感知真实性方面都带来了显著改进，为稳健的加速MRI重建提供了原则性和实用的进展。

Abstract: Magnetic Resonance Imaging (MRI) is a critical tool in modern medical diagnostics, yet its prolonged acquisition time remains a critical limitation, especially in time-sensitive clinical scenarios. While undersampling strategies can accelerate image acquisition, they often result in image artifacts and degraded quality. Recent diffusion models have shown promise for reconstructing high-fidelity images from undersampled data by learning powerful image priors; however, most existing approaches either (i) rely on unsupervised score functions without paired supervision or (ii) apply data consistency only as a post-processing step. In this work, we introduce a conditional denoising diffusion framework with iterative data-consistency correction, which differs from prior methods by embedding the measurement model directly into every reverse diffusion step and training the model on paired undersampled-ground truth data. This hybrid design bridges generative flexibility with explicit enforcement of MRI physics. Experiments on the fastMRI dataset demonstrate that our framework consistently outperforms recent state-of-the-art deep learning and diffusion-based methods in SSIM, PSNR, and LPIPS, with LPIPS capturing perceptual improvements more faithfully. These results demonstrate that integrating conditional supervision with iterative consistency updates yields substantial improvements in both pixel-level fidelity and perceptual realism, establishing a principled and practical advance toward robust, accelerated MRI reconstruction.

</details>
