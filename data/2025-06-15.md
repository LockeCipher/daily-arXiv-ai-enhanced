<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 5]
- [cs.CV](#cs.CV) [Total: 26]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.MM](#cs.MM) [Total: 3]
- [cs.RO](#cs.RO) [Total: 2]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [FastFLUX: Pruning FLUX with Block-wise Replacement and Sandwich Training](https://arxiv.org/abs/2506.10035)
*Fuhan Cai,Yong Guo,Jie Li,Wenbo Li,Xiangzhong Fang,Jian Chen*

Main category: cs.GR

TL;DR: FastFLUX通过架构级剪枝框架提升FLUX的推理效率，采用BRLL方法和ST训练策略，显著加速推理且保持高质量图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有T2I生成模型（如FLUX）参数规模大，推理速度慢且部署困难，现有加速方法性能下降明显且训练成本高。

Method: 提出FastFLUX框架，核心是BRLL方法（用线性层替换复杂残差分支）和ST训练策略（局部微调）。

Result: 实验表明FastFLUX在剪枝20%层级后仍保持高质量图像生成，同时显著提升推理速度。

Conclusion: FastFLUX有效解决了FLUX的推理效率问题，为T2I模型加速提供了新思路。

Abstract: Recent advancements in text-to-image (T2I) generation have led to the emergence of highly expressive models such as diffusion transformers (DiTs), exemplified by FLUX. However, their massive parameter sizes lead to slow inference, high memory usage, and poor deployability. Existing acceleration methods (e.g., single-step distillation and attention pruning) often suffer from significant performance degradation and incur substantial training costs. To address these limitations, we propose FastFLUX, an architecture-level pruning framework designed to enhance the inference efficiency of FLUX. At its core is the Block-wise Replacement with Linear Layers (BRLL) method, which replaces structurally complex residual branches in ResBlocks with lightweight linear layers while preserving the original shortcut connections for stability. Furthermore, we introduce Sandwich Training (ST), a localized fine-tuning strategy that leverages LoRA to supervise neighboring blocks, mitigating performance drops caused by structural replacement. Experiments show that our FastFLUX maintains high image quality under both qualitative and quantitative evaluations, while significantly improving inference speed, even with 20\% of the hierarchy pruned. Our code will be available soon.

</details>


### [2] [Token Perturbation Guidance for Diffusion Models](https://arxiv.org/abs/2506.10036)
*Javad Rajabi,Soroush Mehraban,Seyedmorteza Sadat,Babak Taati*

Main category: cs.GR

TL;DR: 提出了一种名为Token Perturbation Guidance (TPG)的新方法，通过扰动扩散网络中的中间令牌表示，无需特定训练即可提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决Classifier-free guidance (CFG)需要特定训练且仅适用于条件生成的局限性。

Method: TPG通过规范保持的洗牌操作直接扰动中间令牌表示，提供稳定且有效的指导信号。

Result: 在SDXL和Stable Diffusion 2.1上，TPG在无条件生成中FID提升近2倍，同时在提示对齐上与CFG表现接近。

Conclusion: TPG是一种通用且条件无关的指导方法，为更广泛的扩散模型带来类似CFG的优势。

Abstract: Classifier-free guidance (CFG) has become an essential component of modern diffusion models to enhance both generation quality and alignment with input conditions. However, CFG requires specific training procedures and is limited to conditional generation. To address these limitations, we propose Token Perturbation Guidance (TPG), a novel method that applies perturbation matrices directly to intermediate token representations within the diffusion network. TPG employs a norm-preserving shuffling operation to provide effective and stable guidance signals that improve generation quality without architectural changes. As a result, TPG is training-free and agnostic to input conditions, making it readily applicable to both conditional and unconditional generation. We further analyze the guidance term provided by TPG and show that its effect on sampling more closely resembles CFG compared to existing training-free guidance techniques. Extensive experiments on SDXL and Stable Diffusion 2.1 show that TPG achieves nearly a 2$\times$ improvement in FID for unconditional generation over the SDXL baseline, while closely matching CFG in prompt alignment. These results establish TPG as a general, condition-agnostic guidance method that brings CFG-like benefits to a broader class of diffusion models. The code is available at https://github.com/TaatiTeam/Token-Perturbation-Guidance

</details>


### [3] [Ambient Diffusion Omni: Training Good Models with Bad Data](https://arxiv.org/abs/2506.10038)
*Giannis Daras,Adrian Rodriguez-Munoz,Adam Klivans,Antonio Torralba,Constantinos Daskalakis*

Main category: cs.GR

TL;DR: 通过利用低质量、合成和分布外图像，Ambient Diffusion Omni框架显著提升了扩散模型的性能，实现了图像质量和多样性的改进。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型依赖高质量数据集，但低质量图像中仍蕴含价值。本文旨在探索如何利用这些被丢弃的图像提升模型性能。

Method: 提出Ambient Diffusion Omni框架，利用自然图像的光谱幂律衰减和局部性特性，从所有可用图像中提取信号。

Result: 在合成损坏图像上验证了框架有效性，并在ImageNet上实现了最佳FID，同时在文本到图像生成中显著提升了质量和多样性。

Conclusion: 噪声可以缓解高质量分布与混合分布之间的初始偏差，理论分析支持了该方法的有效性。

Abstract: We show how to use low-quality, synthetic, and out-of-distribution images to improve the quality of a diffusion model. Typically, diffusion models are trained on curated datasets that emerge from highly filtered data pools from the Web and other sources. We show that there is immense value in the lower-quality images that are often discarded. We present Ambient Diffusion Omni, a simple, principled framework to train diffusion models that can extract signal from all available images during training. Our framework exploits two properties of natural images -- spectral power law decay and locality. We first validate our framework by successfully training diffusion models with images synthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We then use our framework to achieve state-of-the-art ImageNet FID, and we show significant improvements in both image quality and diversity for text-to-image generative modeling. The core insight is that noise dampens the initial skew between the desired high-quality distribution and the mixed distribution we actually observe. We provide rigorous theoretical justification for our approach by analyzing the trade-off between learning from biased data versus limited unbiased data across diffusion times.

</details>


### [4] [Low-Barrier Dataset Collection with Real Human Body for Interactive Per-Garment Virtual Try-On](https://arxiv.org/abs/2506.10468)
*Zaiqiang Wu,Yechen Li,Jingyuan Liu,Yuki Shibata,Takayuki Hori,I-Chao Shen,Takeo Igarashi*

Main category: cs.GR

TL;DR: 提出了一种低成本、基于真人数据的虚拟试衣方法，解决了现有方法依赖昂贵机器人模型和服装对齐不准确的问题。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试衣方法局限于正面视角且缺乏实时性，而基于单件服装的方法虽有所改进，但仍存在数据集采集成本高和服装与人体对齐不准确的问题。

Method: 采用真人身体采集数据集，避免使用昂贵的机器人模型，并引入混合人物表示方法（结合简化的DensePose图）以提升服装对齐精度。

Result: 通过定性和定量评估，证明了该方法在图像质量和时间一致性上的优越性。用户研究表明，该系统有助于服装购买决策。

Conclusion: 该方法降低了虚拟试衣的门槛，提升了服装对齐的准确性，具有实际应用价值。

Abstract: Existing image-based virtual try-on methods are often limited to the front view and lack real-time performance. While per-garment virtual try-on methods have tackled these issues by capturing per-garment datasets and training per-garment neural networks, they still encounter practical limitations: (1) the robotic mannequin used to capture per-garment datasets is prohibitively expensive for widespread adoption and fails to accurately replicate natural human body deformation; (2) the synthesized garments often misalign with the human body. To address these challenges, we propose a low-barrier approach for collecting per-garment datasets using real human bodies, eliminating the necessity for a customized robotic mannequin. We also introduce a hybrid person representation that enhances the existing intermediate representation with a simplified DensePose map. This ensures accurate alignment of synthesized garment images with the human body and enables human-garment interaction without the need for customized wearable devices. We performed qualitative and quantitative evaluations against other state-of-the-art image-based virtual try-on methods and conducted ablation studies to demonstrate the superiority of our method regarding image quality and temporal consistency. Finally, our user study results indicated that most participants found our virtual try-on system helpful for making garment purchasing decisions.

</details>


### [5] [Edit360: 2D Image Edits to 3D Assets from Any Angle](https://arxiv.org/abs/2506.10507)
*Junchao Huang,Xinting Hu,Zhuotao Tian,Shaoshuai Shi,Li Jiang*

Main category: cs.GR

TL;DR: Edit360是一个无需调整的框架，通过视频扩散模型实现多视角一致的3D编辑，支持任意视角的用户定制编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的3D编辑方法通常局限于预定的视角，限制了灵活性和实际应用。

Method: Edit360基于视频扩散模型，引入锚点视角编辑传播机制，在潜在和注意力空间中对齐多视角信息。

Result: 框架能够生成高质量的多视角序列，支持可定制的3D内容创建。

Conclusion: Edit360为3D资产的多视角一致编辑提供了灵活且实用的解决方案。

Abstract: Recent advances in diffusion models have significantly improved image generation and editing, but extending these capabilities to 3D assets remains challenging, especially for fine-grained edits that require multi-view consistency. Existing methods typically restrict editing to predetermined viewing angles, severely limiting their flexibility and practical applications. We introduce Edit360, a tuning-free framework that extends 2D modifications to multi-view consistent 3D editing. Built upon video diffusion models, Edit360 enables user-specific editing from arbitrary viewpoints while ensuring structural coherence across all views. The framework selects anchor views for 2D modifications and propagates edits across the entire 360-degree range. To achieve this, Edit360 introduces a novel Anchor-View Editing Propagation mechanism, which effectively aligns and merges multi-view information within the latent and attention spaces of diffusion models. The resulting edited multi-view sequences facilitate the reconstruction of high-quality 3D assets, enabling customizable 3D content creation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [6] [Multimodal Cinematic Video Synthesis Using Text-to-Image and Audio Generation Models](https://arxiv.org/abs/2506.10005)
*Sridhar S,Nithin A,Shakeel Rifath,Vasantha Raj*

Main category: cs.CV

TL;DR: 本文提出了一种结合Stable Diffusion、GPT-2和混合音频管道的自动生成60秒电影的方法，通过五场景框架、帧插值和音视频同步实现专业质量。


<details>
  <summary>Details</summary>
Motivation: 利用生成式AI技术改进多媒体创作，实现从文本输入自动生成高质量电影。

Method: 结合Stable Diffusion生成图像、GPT-2构建叙事，混合音频管道（gTTS和YouTube音乐），并采用五场景框架、帧插值和音视频同步。

Result: 实验显示出色的视觉质量、叙事连贯性和效率，适用于创意、教育和工业应用。

Conclusion: 该方法进一步推动了文本到视频合成的应用，展示了生成式AI在多媒体创作中的潜力。

Abstract: Advances in generative artificial intelligence have altered multimedia creation, allowing for automatic cinematic video synthesis from text inputs. This work describes a method for creating 60-second cinematic movies incorporating Stable Diffusion for high-fidelity image synthesis, GPT-2 for narrative structuring, and a hybrid audio pipeline using gTTS and YouTube-sourced music. It uses a five-scene framework, which is augmented by linear frame interpolation, cinematic post-processing (e.g., sharpening), and audio-video synchronization to provide professional-quality results. It was created in a GPU-accelerated Google Colab environment using Python 3.11. It has a dual-mode Gradio interface (Simple and Advanced), which supports resolutions of up to 1024x768 and frame rates of 15-30 FPS. Optimizations such as CUDA memory management and error handling ensure reliability. The experiments demonstrate outstanding visual quality, narrative coherence, and efficiency, furthering text-to-video synthesis for creative, educational, and industrial applications.

</details>


### [7] [LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware LoRA Fine-Tuning](https://arxiv.org/abs/2506.10082)
*Chenjian Gao,Lihe Ding,Xin Cai,Zhanpeng Huang,Zibin Wang,Tianfan Xue*

Main category: cs.CV

TL;DR: 提出了一种基于掩码的LoRA调优方法，用于灵活的视频编辑，无需修改模型架构，同时保留背景区域并实现可控编辑传播。


<details>
  <summary>Details</summary>
Motivation: 当前视频编辑方法依赖大规模预训练，缺乏灵活性，且首帧引导编辑对后续帧控制不足。

Method: 采用掩码驱动的LoRA调优策略，结合输入视频的空间结构和参考图像的外观指导，动态调节模型注意力。

Result: 实验结果表明，该方法在视频编辑性能上优于现有先进方法。

Conclusion: 该方法提供了一种高效且灵活的视频编辑解决方案，适用于特定编辑需求。

Abstract: Video editing using diffusion models has achieved remarkable results in generating high-quality edits for videos. However, current methods often rely on large-scale pretraining, limiting flexibility for specific edits. First-frame-guided editing provides control over the first frame, but lacks flexibility over subsequent frames. To address this, we propose a mask-based LoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video (I2V) models for flexible video editing. Our approach preserves background regions while enabling controllable edits propagation. This solution offers efficient and adaptable video editing without altering the model architecture. To better steer this process, we incorporate additional references, such as alternate viewpoints or representative scene states, which serve as visual anchors for how content should unfold. We address the control challenge using a mask-driven LoRA tuning strategy that adapts a pre-trained image-to-video model to the editing context. The model must learn from two distinct sources: the input video provides spatial structure and motion cues, while reference images offer appearance guidance. A spatial mask enables region-specific learning by dynamically modulating what the model attends to, ensuring that each area draws from the appropriate source. Experimental results show our method achieves superior video editing performance compared to state-of-the-art methods.

</details>


### [8] [SPARKE: Scalable Prompt-Aware Diversity Guidance in Diffusion Models via RKE Score](https://arxiv.org/abs/2506.10173)
*Mohammad Jalali,Haoyu Lei,Amin Gohari,Farzan Farnia*

Main category: cs.CV

TL;DR: SPARKE方法通过条件熵实现提示感知多样性控制，显著提升生成样本的多样性，同时降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决提示引导扩散模型在生成样本多样性不足的问题，特别是在语义广泛的提示下。

Method: 提出SPARKE方法，利用条件熵动态测量多样性，并通过简化计算复杂度（从O(n^3)到O(n)）实现大规模生成。

Result: 在多个文本到图像扩散模型上验证，SPARKE显著提升了生成数据的提示感知多样性，且计算成本低。

Conclusion: SPARKE是一种高效且可扩展的提示感知多样性引导方法，适用于大规模生成任务。

Abstract: Diffusion models have demonstrated remarkable success in high-fidelity image synthesis and prompt-guided generative modeling. However, ensuring adequate diversity in generated samples of prompt-guided diffusion models remains a challenge, particularly when the prompts span a broad semantic spectrum and the diversity of generated data needs to be evaluated in a prompt-aware fashion across semantically similar prompts. Recent methods have introduced guidance via diversity measures to encourage more varied generations. In this work, we extend the diversity measure-based approaches by proposing the Scalable Prompt-Aware R\'eny Kernel Entropy Diversity Guidance (SPARKE) method for prompt-aware diversity guidance. SPARKE utilizes conditional entropy for diversity guidance, which dynamically conditions diversity measurement on similar prompts and enables prompt-aware diversity control. While the entropy-based guidance approach enhances prompt-aware diversity, its reliance on the matrix-based entropy scores poses computational challenges in large-scale generation settings. To address this, we focus on the special case of Conditional latent RKE Score Guidance, reducing entropy computation and gradient-based optimization complexity from the $O(n^3)$ of general entropy measures to $O(n)$. The reduced computational complexity allows for diversity-guided sampling over potentially thousands of generation rounds on different prompts. We numerically test the SPARKE method on several text-to-image diffusion models, demonstrating that the proposed method improves the prompt-aware diversity of the generated data without incurring significant computational costs. We release our code on the project page: https://mjalali.github.io/SPARKE

</details>


### [9] [Improving Personalized Search with Regularized Low-Rank Parameter Updates](https://arxiv.org/abs/2506.10182)
*Fiona Ryan,Josef Sivic,Fabian Caba Heilbron,Judy Hoffman,James M. Rehg,Bryan Russell*

Main category: cs.CV

TL;DR: 本文提出了一种通过低秩适应方法改进视觉-语言双编码器模型的方法，用于个性化视觉-语言检索任务，并在两个基准测试中取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 个性化视觉-语言检索任务需要从少量示例中学习新概念，并整合个人与通用知识以识别不同上下文中的概念。本文旨在解决这一挑战。

Method: 采用语言编码器最后一层的正则化低秩适应方法，替代文本反转，以保留通用知识的同时识别个人概念。同时探索了多个人概念参数的结合策略。

Result: 在两个基准测试（DeepFashion2和ConCon-Chi）中，个性化检索性能优于现有方法4%-22%。

Conclusion: 通过低秩适应和参数结合策略，本文方法在个性化视觉-语言检索任务中表现出色，同时保留了通用知识。

Abstract: Personalized vision-language retrieval seeks to recognize new concepts (e.g. "my dog Fido") from only a few examples. This task is challenging because it requires not only learning a new concept from a few images, but also integrating the personal and general knowledge together to recognize the concept in different contexts. In this paper, we show how to effectively adapt the internal representation of a vision-language dual encoder model for personalized vision-language retrieval. We find that regularized low-rank adaption of a small set of parameters in the language encoder's final layer serves as a highly effective alternative to textual inversion for recognizing the personal concept while preserving general knowledge. Additionally, we explore strategies for combining parameters of multiple learned personal concepts, finding that parameter addition is effective. To evaluate how well general knowledge is preserved in a finetuned representation, we introduce a metric that measures image retrieval accuracy based on captions generated by a vision language model (VLM). Our approach achieves state-of-the-art accuracy on two benchmarks for personalized image retrieval with natural language queries - DeepFashion2 and ConCon-Chi - outperforming the prior art by 4%-22% on personal retrievals.

</details>


### [10] [PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian Splatting](https://arxiv.org/abs/2506.10335)
*Lintao Xiang,Hongpei Zheng,Yating Huang,Qijun Yang,Hujun Yin*

Main category: cs.CV

TL;DR: 提出了一种基于点特征感知的高斯泼溅框架，用于从稀疏训练视图中实现实时高质量渲染。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法需要大量校准视图，而在输入视图有限时会过拟合训练视图，导致渲染质量下降。

Method: 利用立体基础模型估计相机姿态和重建密集点云，通过多尺度2D外观特征编码颜色属性，设计基于自注意力机制的点交互网络增强表示，最后通过MLP解码为高斯参数。

Result: 在多样化基准测试中显著优于基于NeRF的方法，并在少样本设置下与最先进的3DGS方法竞争。

Conclusion: 该方法有效解决了稀疏视图下的渲染质量问题，提升了3D高斯泼溅的实用性。

Abstract: 3D Gaussian splatting (3DGS) is an innovative rendering technique that surpasses the neural radiance field (NeRF) in both rendering speed and visual quality by leveraging an explicit 3D scene representation. Existing 3DGS approaches require a large number of calibrated views to generate a consistent and complete scene representation. When input views are limited, 3DGS tends to overfit the training views, leading to noticeable degradation in rendering quality. To address this limitation, we propose a Point-wise Feature-Aware Gaussian Splatting framework that enables real-time, high-quality rendering from sparse training views. Specifically, we first employ the latest stereo foundation model to estimate accurate camera poses and reconstruct a dense point cloud for Gaussian initialization. We then encode the colour attributes of each 3D Gaussian by sampling and aggregating multiscale 2D appearance features from sparse inputs. To enhance point-wise appearance representation, we design a point interaction network based on a self-attention mechanism, allowing each Gaussian point to interact with its nearest neighbors. These enriched features are subsequently decoded into Gaussian parameters through two lightweight multi-layer perceptrons (MLPs) for final rendering. Extensive experiments on diverse benchmarks demonstrate that our method significantly outperforms NeRF-based approaches and achieves competitive performance under few-shot settings compared to the state-of-the-art 3DGS methods.

</details>


### [11] [Revisiting Transformers with Insights from Image Filtering](https://arxiv.org/abs/2506.10371)
*Laziz U. Abdullaev,Maksim Tkachenko,Tan M. Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种基于图像处理的统一框架，用于解释自注意力机制及其组件（如位置编码和残差连接）的作用，并提出了两种改进架构。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制的成功缺乏理论解释，现有框架未能深入理解其组件的作用，本文旨在填补这一空白。

Method: 开发了一个统一的图像处理框架，解释自注意力及其组件，并引入两种改进的Transformer架构。

Result: 改进后的架构不仅提高了可解释性，还在语言和视觉任务中提升了准确性和鲁棒性，同时增强了长序列理解能力。

Conclusion: 本文通过图像处理框架为自注意力机制提供了理论支持，并通过改进架构展示了其实际应用潜力。

Abstract: The self-attention mechanism, a cornerstone of Transformer-based state-of-the-art deep learning architectures, is largely heuristic-driven and fundamentally challenging to interpret. Establishing a robust theoretical foundation to explain its remarkable success and limitations has therefore become an increasingly prominent focus in recent research. Some notable directions have explored understanding self-attention through the lens of image denoising and nonparametric regression. While promising, existing frameworks still lack a deeper mechanistic interpretation of various architectural components that enhance self-attention, both in its original formulation and subsequent variants. In this work, we aim to advance this understanding by developing a unifying image processing framework, capable of explaining not only the self-attention computation itself but also the role of components such as positional encoding and residual connections, including numerous later variants. We also pinpoint potential distinctions between the two concepts building upon our framework, and make effort to close this gap. We introduce two independent architectural modifications within transformers. While our primary objective is interpretability, we empirically observe that image processing-inspired modifications can also lead to notably improved accuracy and robustness against data contamination and adversaries across language and vision tasks as well as better long sequence understanding.

</details>


### [12] [ReconMOST: Multi-Layer Sea Temperature Reconstruction with Observations-Guided Diffusion](https://arxiv.org/abs/2506.10391)
*Yuanyi Song,Pumeng Lyu,Ben Fei,Fenghua Ling,Wanli Ouyang,Lei Bai*

Main category: cs.CV

TL;DR: 论文提出ReconMOST框架，利用数据驱动的扩散模型进行多层海水温度重建，解决了传统方法的数据稀疏和计算成本高问题，并扩展了机器学习在海洋温度重建中的应用。


<details>
  <summary>Details</summary>
Motivation: 准确重建海洋温度对全球气候动态和海洋气象研究至关重要，但传统方法受限于数据稀疏和计算复杂性，机器学习方法也仅适用于表层或局部区域。

Method: 通过预训练无条件扩散模型学习历史数值模拟数据，生成阶段利用稀疏但高精度的现场观测数据作为反向扩散过程的引导点，实现多层温度重建。

Result: 在CMIP6和EN4数据上的实验显示，MSE值分别为0.049（引导）、0.680（重建）和0.633（总体），验证了框架的有效性和鲁棒性。

Conclusion: ReconMOST框架成功扩展了机器学习在海洋温度重建中的应用，能够处理92.5%的缺失数据，同时保持高精度和空间分辨率。

Abstract: Accurate reconstruction of ocean is essential for reflecting global climate dynamics and supporting marine meteorological research. Conventional methods face challenges due to sparse data, algorithmic complexity, and high computational costs, while increasing usage of machine learning (ML) method remains limited to reconstruction problems at the sea surface and local regions, struggling with issues like cloud occlusion. To address these limitations, this paper proposes ReconMOST, a data-driven guided diffusion model framework for multi-layer sea temperature reconstruction. Specifically, we first pre-train an unconditional diffusion model using a large collection of historical numerical simulation data, enabling the model to attain physically consistent distribution patterns of ocean temperature fields. During the generation phase, sparse yet high-accuracy in-situ observational data are utilized as guidance points for the reverse diffusion process, generating accurate reconstruction results. Importantly, in regions lacking direct observational data, the physically consistent spatial distribution patterns learned during pre-training enable implicitly guided and physically plausible reconstructions. Our method extends ML-based SST reconstruction to a global, multi-layer setting, handling over 92.5% missing data while maintaining reconstruction accuracy, spatial resolution, and superior generalization capability. We pre-train our model on CMIP6 numerical simulation data and conduct guided reconstruction experiments on CMIP6 and EN4 analysis data. The results of mean squared error (MSE) values achieve 0.049 on guidance, 0.680 on reconstruction, and 0.633 on total, respectively, demonstrating the effectiveness and robustness of the proposed framework. Our source code is available at https://github.com/norsheep/ReconMOST.

</details>


### [13] [Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation](https://arxiv.org/abs/2506.10395)
*Zhiyang Xu,Jiuhai Chen,Zhaojiang Lin,Xichen Pan,Lifu Huang,Tianyi Zhou,Madian Khabsa,Qifan Wang,Di Jin,Michihiro Yasunaga,Lili Yu,Xi Victoria Lin,Shaoliang Nie*

Main category: cs.CV

TL;DR: Pisces是一种新型多模态基础模型，通过解耦视觉编码架构和定制训练技术，在图像理解和生成任务中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态基础模型在图像理解和生成方面取得进展，但其性能仍不及专用模型。Pisces旨在解决视觉特征和训练过程差异带来的挑战。

Method: 采用解耦视觉编码架构和定制训练技术，结合精细数据整理、预训练和微调。

Result: 在20多个图像理解基准测试和GenEval生成基准测试中表现优异。

Conclusion: Pisces展示了图像理解与生成的协同关系，并证明解耦视觉编码器的优势，推动了统一多模态模型的发展。

Abstract: Recent advances in large language models (LLMs) have enabled multimodal foundation models to tackle both image understanding and generation within a unified framework. Despite these gains, unified models often underperform compared to specialized models in either task. A key challenge in developing unified models lies in the inherent differences between the visual features needed for image understanding versus generation, as well as the distinct training processes required for each modality. In this work, we introduce Pisces, an auto-regressive multimodal foundation model that addresses this challenge through a novel decoupled visual encoding architecture and tailored training techniques optimized for multimodal generation. Combined with meticulous data curation, pretraining, and finetuning, Pisces achieves competitive performance in both image understanding and image generation. We evaluate Pisces on over 20 public benchmarks for image understanding, where it demonstrates strong performance across a wide range of tasks. Additionally, on GenEval, a widely adopted benchmark for image generation, Pisces exhibits robust generative capabilities. Our extensive analysis reveals the synergistic relationship between image understanding and generation, and the benefits of using separate visual encoders, advancing the field of unified multimodal models.

</details>


### [14] [DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers](https://arxiv.org/abs/2506.10568)
*Lizhen Wang,Zhurong Xia,Tianshu Hu,Pengrui Wang,Pengfei Wang,Zerong Zheng,Ming Zhou*

Main category: cs.CV

TL;DR: 提出了一种基于扩散变换器（DiT）的框架，用于生成高质量的人-产品演示视频，解决了现有方法在保持身份和空间关系上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法同时保持人和产品的身份特征或理解其空间关系，导致不真实的演示效果。

Method: 采用DiT框架，注入配对的人-产品参考信息，并使用掩码交叉注意力机制；利用3D身体网格模板和产品边界框提供精确运动指导；结合结构化文本编码增强3D一致性。

Result: 在混合数据集上训练后，该方法在保持身份完整性和生成真实运动方面优于现有技术。

Conclusion: 提出的框架有效解决了人-产品演示视频生成中的身份和空间关系问题，具有实际应用价值。

Abstract: In e-commerce and digital marketing, generating high-fidelity human-product demonstration videos is important for effective product presentation. However, most existing frameworks either fail to preserve the identities of both humans and products or lack an understanding of human-product spatial relationships, leading to unrealistic representations and unnatural interactions. To address these challenges, we propose a Diffusion Transformer (DiT)-based framework. Our method simultaneously preserves human identities and product-specific details, such as logos and textures, by injecting paired human-product reference information and utilizing an additional masked cross-attention mechanism. We employ a 3D body mesh template and product bounding boxes to provide precise motion guidance, enabling intuitive alignment of hand gestures with product placements. Additionally, structured text encoding is used to incorporate category-level semantics, enhancing 3D consistency during small rotational changes across frames. Trained on a hybrid dataset with extensive data augmentation strategies, our approach outperforms state-of-the-art techniques in maintaining the identity integrity of both humans and products and generating realistic demonstration motions. Project page: https://submit2025-dream.github.io/DreamActor-H1/.

</details>


### [15] [Text to Image for Multi-Label Image Recognition with Joint Prompt-Adapter Learning](https://arxiv.org/abs/2506.10575)
*Chun-Mei Feng,Kai Yu,Xinxing Xu,Salman Khan,Rick Siow Mong Goh,Wangmeng Zuo,Yong Liu*

Main category: cs.CV

TL;DR: T2I-PAL利用文本生成图像减少模态差异，结合热图和原型学习提升多标签识别性能，无需全标注图像，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决CLIP模型中图像与文本模态差异问题，提升仅用文本进行参数高效微调时的图像识别性能。

Method: 利用文本生成图像模型生成多样图像，结合类热图和可学习原型增强局部特征表示，并融合提示调优和适配器学习。

Result: 在多个基准测试中平均性能提升3.47%，优于现有方法。

Conclusion: T2I-PAL有效减少模态差异，提升性能且无需全标注图像，兼容现有CLIP框架。

Abstract: Benefited from image-text contrastive learning, pre-trained vision-language models, e.g., CLIP, allow to direct leverage texts as images (TaI) for parameter-efficient fine-tuning (PEFT). While CLIP is capable of making image features to be similar to the corresponding text features, the modality gap remains a nontrivial issue and limits image recognition performance of TaI. Using multi-label image recognition (MLR) as an example, we present a novel method, called T2I-PAL to tackle the modality gap issue when using only text captions for PEFT. The core design of T2I-PAL is to leverage pre-trained text-to-image generation models to generate photo-realistic and diverse images from text captions, thereby reducing the modality gap. To further enhance MLR, T2I-PAL incorporates a class-wise heatmap and learnable prototypes. This aggregates local similarities, making the representation of local visual features more robust and informative for multi-label recognition. For better PEFT, we further combine both prompt tuning and adapter learning to enhance classification performance. T2I-PAL offers significant advantages: it eliminates the need for fully semantically annotated training images, thereby reducing the manual annotation workload, and it preserves the intrinsic mode of the CLIP model, allowing for seamless integration with any existing CLIP framework. Extensive experiments on multiple benchmarks, including MS-COCO, VOC2007, and NUS-WIDE, show that our T2I-PAL can boost recognition performance by 3.47% in average above the top-ranked state-of-the-art methods.

</details>


### [16] [High-resolution efficient image generation from WiFi CSI using a pretrained latent diffusion model](https://arxiv.org/abs/2506.10605)
*Eshan Ramesh,Nishio Takayuki*

Main category: cs.CV

TL;DR: LatentCSI利用预训练的潜在扩散模型（LDM）从WiFi CSI测量生成物理环境图像，通过轻量级神经网络将CSI幅度直接映射到LDM的潜在空间，实现高效高质量的图像合成。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖复杂且计算密集的技术（如GANs），而LatentCSI旨在通过简化流程和利用LDM的优势，提高图像生成的效率和质量。

Method: 使用轻量级神经网络将CSI幅度映射到LDM的潜在空间，结合文本引导的扩散模型进行去噪，最后通过LDM的解码器生成高分辨率图像。

Result: 在公开数据集和自收集数据集上验证，LatentCSI在计算效率和感知质量上优于基线方法，并具备文本引导的灵活性。

Conclusion: LatentCSI通过潜在空间映射和LDM的结合，实现了高效、高质量的图像生成，同时提供了文本引导的实用优势。

Abstract: We present LatentCSI, a novel method for generating images of the physical environment from WiFi CSI measurements that leverages a pretrained latent diffusion model (LDM). Unlike prior approaches that rely on complex and computationally intensive techniques such as GANs, our method employs a lightweight neural network to map CSI amplitudes directly into the latent space of an LDM. We then apply the LDM's denoising diffusion model to the latent representation with text-based guidance before decoding using the LDM's pretrained decoder to obtain a high-resolution image. This design bypasses the challenges of pixel-space image generation and avoids the explicit image encoding stage typically required in conventional image-to-image pipelines, enabling efficient and high-quality image synthesis. We validate our approach on two datasets: a wide-band CSI dataset we collected with off-the-shelf WiFi devices and cameras; and a subset of the publicly available MM-Fi dataset. The results demonstrate that LatentCSI outperforms baselines of comparable complexity trained directly on ground-truth images in both computational efficiency and perceptual quality, while additionally providing practical advantages through its unique capacity for text-guided controllability.

</details>


### [17] [TexTailor: Customized Text-aligned Texturing via Effective Resampling](https://arxiv.org/abs/2506.10612)
*Suin Lee,Dae-Shik Kim*

Main category: cs.CV

TL;DR: TexTailor是一种新方法，通过文本描述生成一致的对象纹理，解决了现有方法在多视角下纹理属性逐渐偏移的问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到纹理合成方法在多视角下纹理一致性差，且相机位置选择固定，无法适应对象几何形状。

Method: 采用重采样方案整合先前合成纹理信息，微调深度感知扩散模型，并提出性能保留损失和自适应相机位置调整。

Result: 在Objaverse和ShapeNet数据集上，TexTailor在合成视角一致纹理方面优于现有方法。

Conclusion: TexTailor通过改进纹理整合和相机位置选择，显著提升了纹理一致性。

Abstract: We present TexTailor, a novel method for generating consistent object textures from textual descriptions. Existing text-to-texture synthesis approaches utilize depth-aware diffusion models to progressively generate images and synthesize textures across predefined multiple viewpoints. However, these approaches lead to a gradual shift in texture properties across viewpoints due to (1) insufficient integration of previously synthesized textures at each viewpoint during the diffusion process and (2) the autoregressive nature of the texture synthesis process. Moreover, the predefined selection of camera positions, which does not account for the object's geometry, limits the effective use of texture information synthesized from different viewpoints, ultimately degrading overall texture consistency. In TexTailor, we address these issues by (1) applying a resampling scheme that repeatedly integrates information from previously synthesized textures within the diffusion process, and (2) fine-tuning a depth-aware diffusion model on these resampled textures. During this process, we observed that using only a few training images restricts the model's original ability to generate high-fidelity images aligned with the conditioning, and therefore propose an performance preservation loss to mitigate this issue. Additionally, we improve the synthesis of view-consistent textures by adaptively adjusting camera positions based on the object's geometry. Experiments on a subset of the Objaverse dataset and the ShapeNet car dataset demonstrate that TexTailor outperforms state-of-the-art methods in synthesizing view-consistent textures. The source code for TexTailor is available at https://github.com/Adios42/Textailor

</details>


### [18] [Anatomy-Grounded Weakly Supervised Prompt Tuning for Chest X-ray Latent Diffusion Models](https://arxiv.org/abs/2506.10633)
*Konstantinos Vilouras,Ilias Stogiannidis,Junyu Yan,Alison Q. O'Neil,Sotirios A. Tsaftaris*

Main category: cs.CV

TL;DR: 本文提出了一种针对医学影像（胸部X光）的文本到图像潜在扩散模型微调框架，以改善多模态对齐，并在标准数据集上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像领域缺乏文本到图像潜在扩散模型的研究，且现有模型未能有效对齐临床相关文本与影像区域。

Method: 提出了一种微调框架，用于改进预训练模型的多模态对齐能力，使其适用于下游任务（如短语定位）。

Result: 在标准数据集（MS-CXR）上取得了新的最佳性能，并在外部数据（VinDr-CXR）上表现出鲁棒性。

Conclusion: 该方法为医学影像领域的文本到图像任务提供了有效的解决方案，并具有广泛的应用潜力。

Abstract: Latent Diffusion Models have shown remarkable results in text-guided image synthesis in recent years. In the domain of natural (RGB) images, recent works have shown that such models can be adapted to various vision-language downstream tasks with little to no supervision involved. On the contrary, text-to-image Latent Diffusion Models remain relatively underexplored in the field of medical imaging, primarily due to limited data availability (e.g., due to privacy concerns). In this work, focusing on the chest X-ray modality, we first demonstrate that a standard text-conditioned Latent Diffusion Model has not learned to align clinically relevant information in free-text radiology reports with the corresponding areas of the given scan. Then, to alleviate this issue, we propose a fine-tuning framework to improve multi-modal alignment in a pre-trained model such that it can be efficiently repurposed for downstream tasks such as phrase grounding. Our method sets a new state-of-the-art on a standard benchmark dataset (MS-CXR), while also exhibiting robust performance on out-of-distribution data (VinDr-CXR). Our code will be made publicly available.

</details>


### [19] [Symmetrical Flow Matching: Unified Image Generation, Segmentation, and Classification with Score-Based Generative Models](https://arxiv.org/abs/2506.10634)
*Francisco Caetano,Christiaan Viviers,Peter H. N. De With,Fons van der Sommen*

Main category: cs.CV

TL;DR: SymmFlow是一种对称流匹配框架，统一了语义分割、分类和图像生成，通过双向一致性学习和保留语义信息，实现了高效采样和多样化生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在图像生成和语义任务之间存在分离，且通常需要严格的一对一映射。SymmFlow旨在通过统一框架解决这些问题，支持灵活的像素级和图像级条件。

Method: 提出对称学习目标，联合建模正向和反向变换，确保双向一致性，并引入新训练目标以保留语义信息，支持一步分割和分类。

Result: 在CelebAMask-HQ和COCO-Stuff上分别取得FID 11.9和7.0的SOTA性能，同时在语义分割和分类任务中表现优异。

Conclusion: SymmFlow展示了统一生成和语义任务的潜力，代码将开源。

Abstract: Flow Matching has emerged as a powerful framework for learning continuous transformations between distributions, enabling high-fidelity generative modeling. This work introduces Symmetrical Flow Matching (SymmFlow), a new formulation that unifies semantic segmentation, classification, and image generation within a single model. Using a symmetric learning objective, SymmFlow models forward and reverse transformations jointly, ensuring bi-directional consistency, while preserving sufficient entropy for generative diversity. A new training objective is introduced to explicitly retain semantic information across flows, featuring efficient sampling while preserving semantic structure, allowing for one-step segmentation and classification without iterative refinement. Unlike previous approaches that impose strict one-to-one mapping between masks and images, SymmFlow generalizes to flexible conditioning, supporting both pixel-level and image-level class labels. Experimental results on various benchmarks demonstrate that SymmFlow achieves state-of-the-art performance on semantic image synthesis, obtaining FID scores of 11.9 on CelebAMask-HQ and 7.0 on COCO-Stuff with only 25 inference steps. Additionally, it delivers competitive results on semantic segmentation and shows promising capabilities in classification tasks. The code will be publicly available.

</details>


### [20] [GigaVideo-1: Advancing Video Generation via Automatic Feedback with 4 GPU-Hours Fine-Tuning](https://arxiv.org/abs/2506.10639)
*Xiaoyi Bao,Jindi Lv,Xiaofeng Wang,Zheng Zhu,Xinze Chen,YuKun Zhou,Jiancheng Lv,Xingang Wang,Guan Huang*

Main category: cs.CV

TL;DR: GigaVideo-1是一种无需人工标注的高效视频生成微调框架，通过自动反馈提升预训练扩散模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法依赖人工标注和大规模计算资源，限制了实用性。

Method: 设计了提示驱动的数据引擎和奖励引导的训练策略，利用预训练视觉语言模型的反馈优化样本权重。

Result: 在VBench-2.0基准测试中，平均性能提升约4%，仅需4 GPU小时。

Conclusion: GigaVideo-1在无需人工标注和少量真实数据的情况下，实现了高效且有效的视频生成优化。

Abstract: Recent progress in diffusion models has greatly enhanced video generation quality, yet these models still require fine-tuning to improve specific dimensions like instance preservation, motion rationality, composition, and physical plausibility. Existing fine-tuning approaches often rely on human annotations and large-scale computational resources, limiting their practicality. In this work, we propose GigaVideo-1, an efficient fine-tuning framework that advances video generation without additional human supervision. Rather than injecting large volumes of high-quality data from external sources, GigaVideo-1 unlocks the latent potential of pre-trained video diffusion models through automatic feedback. Specifically, we focus on two key aspects of the fine-tuning process: data and optimization. To improve fine-tuning data, we design a prompt-driven data engine that constructs diverse, weakness-oriented training samples. On the optimization side, we introduce a reward-guided training strategy, which adaptively weights samples using feedback from pre-trained vision-language models with a realism constraint. We evaluate GigaVideo-1 on the VBench-2.0 benchmark using Wan2.1 as the baseline across 17 evaluation dimensions. Experiments show that GigaVideo-1 consistently improves performance on almost all the dimensions with an average gain of about 4% using only 4 GPU-hours. Requiring no manual annotations and minimal real data, GigaVideo-1 demonstrates both effectiveness and efficiency. Code, model, and data will be publicly available.

</details>


### [21] [Enhancing Deepfake Detection using SE Block Attention with CNN](https://arxiv.org/abs/2506.10683)
*Subhram Dasgupta,Janelle Mason,Xiaohong Yuan,Olusola Odeyomi,Kaushik Roy*

Main category: cs.CV

TL;DR: 提出了一种轻量级卷积神经网络（CNN）结合Squeeze and Excitation（SE）注意力模块的Deepfake检测方法，模型体积小但性能优异。


<details>
  <summary>Details</summary>
Motivation: Deepfake技术对信息真实性和安全性构成威胁，现有检测模型体积大、资源消耗高，需开发高效轻量级解决方案。

Method: 采用轻量级CNN结合SE模块，动态调整通道特征权重，提升模型效率。

Result: 在Style GAN数据集上分类准确率达94.14%，AUC-ROC得分为0.985。

Conclusion: 该方法为Deepfake检测提供了高效、低资源消耗的解决方案，具有实际应用潜力。

Abstract: In the digital age, Deepfake present a formidable challenge by using advanced artificial intelligence to create highly convincing manipulated content, undermining information authenticity and security. These sophisticated fabrications surpass traditional detection methods in complexity and realism. To address this issue, we aim to harness cutting-edge deep learning methodologies to engineer an innovative deepfake detection model. However, most of the models designed for deepfake detection are large, causing heavy storage and memory consumption. In this research, we propose a lightweight convolution neural network (CNN) with squeeze and excitation block attention (SE) for Deepfake detection. The SE block module is designed to perform dynamic channel-wise feature recalibration. The SE block allows the network to emphasize informative features and suppress less useful ones, which leads to a more efficient and effective learning module. This module is integrated with a simple sequential model to perform Deepfake detection. The model is smaller in size and it achieves competing accuracy with the existing models for deepfake detection tasks. The model achieved an overall classification accuracy of 94.14% and AUC-ROC score of 0.985 on the Style GAN dataset from the Diverse Fake Face Dataset. Our proposed approach presents a promising avenue for combating the Deepfake challenge with minimal computational resources, developing efficient and scalable solutions for digital content verification.

</details>


### [22] [Unsourced Adversarial CAPTCHA: A Bi-Phase Adversarial CAPTCHA Framework](https://arxiv.org/abs/2506.10685)
*Xia Du,Xiaoyuan Liu,Jizhe Zhou,Zheng Lin,Chi-man Pun,Zhe Chen,Wei Ni,Jun Luo*

Main category: cs.CV

TL;DR: 提出了一种名为UAC的新框架，通过文本提示生成高保真对抗样本，提升CAPTCHA多样性，支持定向和非定向攻击。


<details>
  <summary>Details</summary>
Motivation: 传统CAPTCHA方案因深度学习的快速发展而容易被自动化攻击攻破，现有对抗攻击方法依赖原始图像特征，导致失真且适用性受限。

Method: UAC框架利用大语言模型（LLM）生成对抗样本；定向攻击采用EDICT方法优化扩散模型的双潜在变量；非定向攻击引入BP-UAC策略，通过多模态梯度和双路径优化实现高效误分类。

Result: 实验表明BP-UAC在多种系统中攻击成功率高，生成的CAPTCHA对人类和DNN均难以区分。

Conclusion: UAC框架有效解决了传统CAPTCHA的脆弱性问题，为对抗攻击提供了新思路。

Abstract: With the rapid advancements in deep learning, traditional CAPTCHA schemes are increasingly vulnerable to automated attacks powered by deep neural networks (DNNs). Existing adversarial attack methods often rely on original image characteristics, resulting in distortions that hinder human interpretation and limit applicability in scenarios lacking initial input images. To address these challenges, we propose the Unsourced Adversarial CAPTCHA (UAC), a novel framework generating high-fidelity adversarial examples guided by attacker-specified text prompts. Leveraging a Large Language Model (LLM), UAC enhances CAPTCHA diversity and supports both targeted and untargeted attacks. For targeted attacks, the EDICT method optimizes dual latent variables in a diffusion model for superior image quality. In untargeted attacks, especially for black-box scenarios, we introduce bi-path unsourced adversarial CAPTCHA (BP-UAC), a two-step optimization strategy employing multimodal gradients and bi-path optimization for efficient misclassification. Experiments show BP-UAC achieves high attack success rates across diverse systems, generating natural CAPTCHAs indistinguishable to humans and DNNs.

</details>


### [23] [Stroke-based Cyclic Amplifier: Image Super-Resolution at Arbitrary Ultra-Large Scales](https://arxiv.org/abs/2506.10774)
*Wenhao Guo,Peng Lu,Xujun Peng,Zhaoran Zhao,Sheng Li*

Main category: cs.CV

TL;DR: 论文提出了一种基于笔画的循环放大器（SbCA）模型，用于解决超大规模图像超分辨率任务中的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有任意尺度图像超分辨率方法在超出训练数据范围时性能显著下降，导致模糊问题。

Method: SbCA通过笔画向量放大器将图像分解为矢量图形进行放大，并通过细节补全模块恢复缺失细节，采用循环策略迭代优化。

Result: 实验表明，SbCA在超大规模上采样任务（如×100）中显著优于现有方法，生成高质量图像。

Conclusion: SbCA有效解决了分布漂移问题，消除了伪影和模糊，为超大规模图像超分辨率提供了高效解决方案。

Abstract: Prior Arbitrary-Scale Image Super-Resolution (ASISR) methods often experience a significant performance decline when the upsampling factor exceeds the range covered by the training data, introducing substantial blurring. To address this issue, we propose a unified model, Stroke-based Cyclic Amplifier (SbCA), for ultra-large upsampling tasks. The key of SbCA is the stroke vector amplifier, which decomposes the image into a series of strokes represented as vector graphics for magnification. Then, the detail completion module also restores missing details, ensuring high-fidelity image reconstruction. Our cyclic strategy achieves ultra-large upsampling by iteratively refining details with this unified SbCA model, trained only once for all, while keeping sub-scales within the training range. Our approach effectively addresses the distribution drift issue and eliminates artifacts, noise and blurring, producing high-quality, high-resolution super-resolved images. Experimental validations on both synthetic and real-world datasets demonstrate that our approach significantly outperforms existing methods in ultra-large upsampling tasks (e.g. $\times100$), delivering visual quality far superior to state-of-the-art techniques.

</details>


### [24] [Post-Training Quantization for Video Matting](https://arxiv.org/abs/2506.10840)
*Tianrui Zhu,Houyuan Chen,Ruihao Gong,Michele Magno,Haotong Qin,Kai Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种专为视频抠图模型设计的后训练量化（PTQ）框架，通过两阶段策略、全局仿射校准和光流辅助组件，显著提升了量化模型的精度和时序一致性。


<details>
  <summary>Details</summary>
Motivation: 视频抠图在资源受限设备上部署时面临计算密集型模型的挑战，现有PTQ方法在精度和时序一致性上存在不足。

Method: 提出两阶段PTQ策略（块重建优化和全局参数校准）、统计驱动的全局仿射校准（GAC）方法，以及光流辅助（OFA）组件。

Result: PTQ4VM在不同比特宽度下均达到最先进精度，4位量化模型性能接近全精度模型，计算量减少8倍。

Conclusion: 该框架为视频抠图量化提供了系统化解决方案，显著提升了低比特量化下的性能。

Abstract: Video matting is crucial for applications such as film production and virtual reality, yet deploying its computationally intensive models on resource-constrained devices presents challenges. Quantization is a key technique for model compression and acceleration. As an efficient approach, Post-Training Quantization (PTQ) is still in its nascent stages for video matting, facing significant hurdles in maintaining accuracy and temporal coherence. To address these challenges, this paper proposes a novel and general PTQ framework specifically designed for video matting models, marking, to the best of our knowledge, the first systematic attempt in this domain. Our contributions include: (1) A two-stage PTQ strategy that combines block-reconstruction-based optimization for fast, stable initial quantization and local dependency capture, followed by a global calibration of quantization parameters to minimize accuracy loss. (2) A Statistically-Driven Global Affine Calibration (GAC) method that enables the network to compensate for cumulative statistical distortions arising from factors such as neglected BN layer effects, even reducing the error of existing PTQ methods on video matting tasks up to 20%. (3) An Optical Flow Assistance (OFA) component that leverages temporal and semantic priors from frames to guide the PTQ process, enhancing the model's ability to distinguish moving foregrounds in complex scenes and ultimately achieving near full-precision performance even under ultra-low-bit quantization. Comprehensive quantitative and visual results show that our PTQ4VM achieves the state-of-the-art accuracy performance across different bit-widths compared to the existing quantization methods. We highlight that the 4-bit PTQ4VM even achieves performance close to the full-precision counterpart while enjoying 8x FLOP savings.

</details>


### [25] [M4V: Multi-Modal Mamba for Text-to-Video Generation](https://arxiv.org/abs/2506.10915)
*Jiancheng Huang,Gengwei Zhang,Zequn Jie,Siyu Jiao,Yinlong Qian,Ling Chen,Yunchao Wei,Lin Ma*

Main category: cs.CV

TL;DR: M4V是一个基于Mamba架构的多模态文本到视频生成框架，通过多模态扩散Mamba块（MM-DiM）和奖励学习策略，显著降低了计算成本并提升了视频质量。


<details>
  <summary>Details</summary>
Motivation: 解决传统Transformer在视频生成中计算复杂度高的问题，同时提升多模态和时空建模的效率。

Method: 提出MM-DiM块，结合多模态信息重组和时空建模；引入奖励学习策略优化长上下文生成质量。

Result: 在768×1280分辨率下，M4V比基于注意力的方法减少45%的FLOPs，并生成高质量视频。

Conclusion: M4V在降低计算成本的同时，有效提升了文本到视频生成的质量和效率。

Abstract: Text-to-video generation has significantly enriched content creation and holds the potential to evolve into powerful world simulators. However, modeling the vast spatiotemporal space remains computationally demanding, particularly when employing Transformers, which incur quadratic complexity in sequence processing and thus limit practical applications. Recent advancements in linear-time sequence modeling, particularly the Mamba architecture, offer a more efficient alternative. Nevertheless, its plain design limits its direct applicability to multi-modal and spatiotemporal video generation tasks. To address these challenges, we introduce M4V, a Multi-Modal Mamba framework for text-to-video generation. Specifically, we propose a multi-modal diffusion Mamba (MM-DiM) block that enables seamless integration of multi-modal information and spatiotemporal modeling through a multi-modal token re-composition design. As a result, the Mamba blocks in M4V reduce FLOPs by 45% compared to the attention-based alternative when generating videos at 768$\times$1280 resolution. Additionally, to mitigate the visual quality degradation in long-context autoregressive generation processes, we introduce a reward learning strategy that further enhances per-frame visual realism. Extensive experiments on text-to-video benchmarks demonstrate M4V's ability to produce high-quality videos while significantly lowering computational costs. Code and models will be publicly available at https://huangjch526.github.io/M4V_project.

</details>


### [26] [VINCIE: Unlocking In-context Image Editing from Video](https://arxiv.org/abs/2506.10941)
*Leigang Qu,Feng Cheng,Ziyan Yang,Qi Zhao,Shanchuan Lin,Yichun Shi,Yicong Li,Wenjie Wang,Tat-Seng Chua,Lu Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种直接从视频中学习上下文图像编辑的方法，通过设计块因果扩散变换器和多任务训练，实现了强大的图像编辑能力。


<details>
  <summary>Details</summary>
Motivation: 探索是否可以通过视频直接学习上下文图像编辑模型，减少对任务特定流水线和专家模型的依赖。

Method: 提出了一种可扩展的视频标注方法，设计块因果扩散变换器，训练于三个代理任务：下一图像预测、当前分割预测和下一分割预测。

Result: 模型在上下文图像编辑任务中表现优异，并在多轮图像编辑基准测试中达到最先进水平。

Conclusion: 尽管仅通过视频训练，模型在多概念组合、故事生成和链式编辑应用中展现出潜力。

Abstract: In-context image editing aims to modify images based on a contextual sequence comprising text and previously generated images. Existing methods typically depend on task-specific pipelines and expert models (e.g., segmentation and inpainting) to curate training data. In this work, we explore whether an in-context image editing model can be learned directly from videos. We introduce a scalable approach to annotate videos as interleaved multimodal sequences. To effectively learn from this data, we design a block-causal diffusion transformer trained on three proxy tasks: next-image prediction, current segmentation prediction, and next-segmentation prediction. Additionally, we propose a novel multi-turn image editing benchmark to advance research in this area. Extensive experiments demonstrate that our model exhibits strong in-context image editing capabilities and achieves state-of-the-art results on two multi-turn image editing benchmarks. Despite being trained exclusively on videos, our model also shows promising abilities in multi-concept composition, story generation, and chain-of-editing applications.

</details>


### [27] [MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for Text-to-Image Reasoning](https://arxiv.org/abs/2506.10963)
*Yuxuan Luo,Yuhui Yuan,Junwen Chen,Haonan Cai,Ziyi Yue,Yuwei Yang,Fatima Zohra Daha,Ji Li,Zhouhui Lian*

Main category: cs.CV

TL;DR: 论文提出知识图像生成任务及MMMG基准，评估图像生成模型的推理能力，发现现有模型表现不佳，并发布开源基线FLUX-Reason。


<details>
  <summary>Details</summary>
Motivation: 知识图像对人类文明和学习至关重要，但生成此类图像需要多模态推理能力，现有模型表现不足，需新基准推动研究。

Method: 提出MMMG基准，包含4,456个专家验证的知识图像-提示对，采用统一知识图谱表示，并设计MMMG-Score评估指标。

Result: 评估16个先进文本到图像生成模型，发现推理能力不足，GPT-4o仅得50.20分。发布基线FLUX-Reason（34.45分）。

Conclusion: MMMG基准揭示了图像生成模型的推理缺陷，FLUX-Reason为未来研究提供了有效基线。

Abstract: In this paper, we introduce knowledge image generation as a new task, alongside the Massive Multi-Discipline Multi-Tier Knowledge-Image Generation Benchmark (MMMG) to probe the reasoning capability of image generation models. Knowledge images have been central to human civilization and to the mechanisms of human learning--a fact underscored by dual-coding theory and the picture-superiority effect. Generating such images is challenging, demanding multimodal reasoning that fuses world knowledge with pixel-level grounding into clear explanatory visuals. To enable comprehensive evaluation, MMMG offers 4,456 expert-validated (knowledge) image-prompt pairs spanning 10 disciplines, 6 educational levels, and diverse knowledge formats such as charts, diagrams, and mind maps. To eliminate confounding complexity during evaluation, we adopt a unified Knowledge Graph (KG) representation. Each KG explicitly delineates a target image's core entities and their dependencies. We further introduce MMMG-Score to evaluate generated knowledge images. This metric combines factual fidelity, measured by graph-edit distance between KGs, with visual clarity assessment. Comprehensive evaluations of 16 state-of-the-art text-to-image generation models expose serious reasoning deficits--low entity fidelity, weak relations, and clutter--with GPT-4o achieving an MMMG-Score of only 50.20, underscoring the benchmark's difficulty. To spur further progress, we release FLUX-Reason (MMMG-Score of 34.45), an effective and open baseline that combines a reasoning LLM with diffusion models and is trained on 16,000 curated knowledge image-prompt pairs.

</details>


### [28] [GenWorld: Towards Detecting AI-generated Real-world Simulation Videos](https://arxiv.org/abs/2506.10975)
*Weiliang Chen,Wenzhao Zheng,Yu Zheng,Lei Chen,Jie Zhou,Jiwen Lu,Yueqi Duan*

Main category: cs.CV

TL;DR: GenWorld是一个高质量、真实模拟的AI生成视频检测数据集，旨在解决现有数据集质量不足的问题。


<details>
  <summary>Details</summary>
Motivation: 视频生成技术的快速发展威胁了真实世界信息的可信度，需要更可靠的AI生成视频检测工具。

Method: 提出GenWorld数据集，具有真实模拟、高质量和跨提示多样性特点，并设计SpannDetector模型，利用多视角一致性检测AI生成视频。

Result: 实验表明，SpannDetector在检测高质量AI生成视频方面表现优异。

Conclusion: GenWorld和SpannDetector为基于物理合理性的可解释AI生成视频检测提供了新方向。

Abstract: The flourishing of video generation technologies has endangered the credibility of real-world information and intensified the demand for AI-generated video detectors. Despite some progress, the lack of high-quality real-world datasets hinders the development of trustworthy detectors. In this paper, we propose GenWorld, a large-scale, high-quality, and real-world simulation dataset for AI-generated video detection. GenWorld features the following characteristics: (1) Real-world Simulation: GenWorld focuses on videos that replicate real-world scenarios, which have a significant impact due to their realism and potential influence; (2) High Quality: GenWorld employs multiple state-of-the-art video generation models to provide realistic and high-quality forged videos; (3) Cross-prompt Diversity: GenWorld includes videos generated from diverse generators and various prompt modalities (e.g., text, image, video), offering the potential to learn more generalizable forensic features. We analyze existing methods and find they fail to detect high-quality videos generated by world models (i.e., Cosmos), revealing potential drawbacks of ignoring real-world clues. To address this, we propose a simple yet effective model, SpannDetector, to leverage multi-view consistency as a strong criterion for real-world AI-generated video detection. Experiments show that our method achieves superior results, highlighting a promising direction for explainable AI-generated video detection based on physical plausibility. We believe that GenWorld will advance the field of AI-generated video detection. Project Page: https://chen-wl20.github.io/GenWorld

</details>


### [29] [Fine-Grained Perturbation Guidance via Attention Head Selection](https://arxiv.org/abs/2506.10978)
*Donghoon Ahn,Jiwon Kang,Sanghyun Lee,Minjae Kim,Jaewon Min,Wooseok Jang,Saungwu Lee,Sayak Paul,Susung Hong,Seungryong Kim*

Main category: cs.CV

TL;DR: 论文提出了一种名为HeadHunter的系统框架，通过细粒度选择注意力头实现生成质量和视觉属性的精确控制，并引入SoftPAG方法调节扰动强度。


<details>
  <summary>Details</summary>
Motivation: 现有注意力扰动方法缺乏确定扰动位置的原理性方法，特别是在DiT架构中，质量相关计算分布在多层中。

Method: 研究注意力扰动的粒度，从层级到单个注意力头，提出HeadHunter框架和SoftPAG方法。

Result: 在Stable Diffusion 3和FLUX.1等模型上验证，实现了生成质量提升和风格特定引导。

Conclusion: 首次在扩散模型中进行了头级注意力扰动分析，揭示了注意力层的可解释性，并提供了有效扰动策略的设计方法。

Abstract: Recent guidance methods in diffusion models steer reverse sampling by perturbing the model to construct an implicit weak model and guide generation away from it. Among these approaches, attention perturbation has demonstrated strong empirical performance in unconditional scenarios where classifier-free guidance is not applicable. However, existing attention perturbation methods lack principled approaches for determining where perturbations should be applied, particularly in Diffusion Transformer (DiT) architectures where quality-relevant computations are distributed across layers. In this paper, we investigate the granularity of attention perturbations, ranging from the layer level down to individual attention heads, and discover that specific heads govern distinct visual concepts such as structure, style, and texture quality. Building on this insight, we propose "HeadHunter", a systematic framework for iteratively selecting attention heads that align with user-centric objectives, enabling fine-grained control over generation quality and visual attributes. In addition, we introduce SoftPAG, which linearly interpolates each selected head's attention map toward an identity matrix, providing a continuous knob to tune perturbation strength and suppress artifacts. Our approach not only mitigates the oversmoothing issues of existing layer-level perturbation but also enables targeted manipulation of specific visual styles through compositional head selection. We validate our method on modern large-scale DiT-based text-to-image models including Stable Diffusion 3 and FLUX.1, demonstrating superior performance in both general quality enhancement and style-specific guidance. Our work provides the first head-level analysis of attention perturbation in diffusion models, uncovering interpretable specialization within attention layers and enabling practical design of effective perturbation strategies.

</details>


### [30] [InstaInpaint: Instant 3D-Scene Inpainting with Masked Large Reconstruction Model](https://arxiv.org/abs/2506.10980)
*Junqi You,Chieh Hubert Lin,Weijie Lyu,Zhengbo Zhang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: InstaInpaint是一种基于参考的前馈框架，能在0.4秒内完成3D场景修复，速度提升1000倍，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 当前3D场景修复方法计算耗时，无法满足实时或在线应用需求。

Method: 提出InstaInpaint框架，结合自监督掩码微调策略训练大型重建模型（LRM），从2D修复提案生成3D修复结果。

Result: 在标准基准测试中表现优异，速度提升1000倍，并能灵活应用于对象插入和多区域修复。

Conclusion: InstaInpaint在速度和性能上均优于现有方法，适用于多种下游应用。

Abstract: Recent advances in 3D scene reconstruction enable real-time viewing in virtual and augmented reality. To support interactive operations for better immersiveness, such as moving or editing objects, 3D scene inpainting methods are proposed to repair or complete the altered geometry. However, current approaches rely on lengthy and computationally intensive optimization, making them impractical for real-time or online applications. We propose InstaInpaint, a reference-based feed-forward framework that produces 3D-scene inpainting from a 2D inpainting proposal within 0.4 seconds. We develop a self-supervised masked-finetuning strategy to enable training of our custom large reconstruction model (LRM) on the large-scale dataset. Through extensive experiments, we analyze and identify several key designs that improve generalization, textural consistency, and geometric correctness. InstaInpaint achieves a 1000x speed-up from prior methods while maintaining a state-of-the-art performance across two standard benchmarks. Moreover, we show that InstaInpaint generalizes well to flexible downstream applications such as object insertion and multi-region inpainting. More video results are available at our project page: https://dhmbb2.github.io/InstaInpaint_page/.

</details>


### [31] [SceneCompleter: Dense 3D Scene Completion for Generative Novel View Synthesis](https://arxiv.org/abs/2506.10981)
*Weiliang Chen,Jiayi Bi,Yuanhui Huang,Wenzhao Zheng,Yueqi Duan*

Main category: cs.CV

TL;DR: SceneCompleter提出了一种通过密集3D场景补全实现3D一致生成新视角合成的框架，解决了现有方法因依赖2D补全和3D恢复技术导致的平滑表面和几何失真问题。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在新视角合成中依赖2D补全和3D恢复技术，导致结果过于平滑且几何失真，无法仅从RGB数据推断3D结构。

Method: SceneCompleter包含两个关键组件：(1) 几何-外观双流扩散模型，联合在RGBD空间合成新视角；(2) 场景编码器，从参考图像中编码更全面的场景理解。

Result: 该方法在多个数据集上展示了生成新视角合成中的优越一致性和合理性。

Conclusion: SceneCompleter通过融合结构和纹理信息，实现了视觉一致且3D一致的生成场景补全。

Abstract: Generative models have gained significant attention in novel view synthesis (NVS) by alleviating the reliance on dense multi-view captures. However, existing methods typically fall into a conventional paradigm, where generative models first complete missing areas in 2D, followed by 3D recovery techniques to reconstruct the scene, which often results in overly smooth surfaces and distorted geometry, as generative models struggle to infer 3D structure solely from RGB data. In this paper, we propose SceneCompleter, a novel framework that achieves 3D-consistent generative novel view synthesis through dense 3D scene completion. SceneCompleter achieves both visual coherence and 3D-consistent generative scene completion through two key components: (1) a geometry-appearance dual-stream diffusion model that jointly synthesizes novel views in RGBD space; (2) a scene embedder that encodes a more holistic scene understanding from the reference image. By effectively fusing structural and textural information, our method demonstrates superior coherence and plausibility in generative novel view synthesis across diverse datasets. Project Page: https://chen-wl20.github.io/SceneCompleter

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [32] [Geometric Regularity in Deterministic Sampling of Diffusion-based Generative Models](https://arxiv.org/abs/2506.10177)
*Defang Chen,Zhenyu Zhou,Can Wang,Siwei Lyu*

Main category: cs.LG

TL;DR: 论文揭示了确定性采样动态中的几何规律性，发现采样轨迹集中在极低维子空间且形状相似，并提出了动态规划方案优化采样时间表。


<details>
  <summary>Details</summary>
Motivation: 探索扩散生成模型中确定性采样轨迹的几何特性，以提升采样效率和生成质量。

Method: 分析采样轨迹的低维子空间特性，提出基于动态规划的采样时间表优化方案。

Result: 采样轨迹具有低维性和形状相似性，优化方案显著提升了图像生成性能。

Conclusion: 发现采样轨迹的几何规律性为优化扩散生成模型提供了新思路，动态规划方案简单高效。

Abstract: Diffusion-based generative models employ stochastic differential equations (SDEs) and their equivalent probability flow ordinary differential equations (ODEs) to establish a smooth transformation between complex high-dimensional data distributions and tractable prior distributions. In this paper, we reveal a striking geometric regularity in the deterministic sampling dynamics: each simulated sampling trajectory lies within an extremely low-dimensional subspace, and all trajectories exhibit an almost identical ''boomerang'' shape, regardless of the model architecture, applied conditions, or generated content. We characterize several intriguing properties of these trajectories, particularly under closed-form solutions based on kernel-estimated data modeling. We also demonstrate a practical application of the discovered trajectory regularity by proposing a dynamic programming-based scheme to better align the sampling time schedule with the underlying trajectory structure. This simple strategy requires minimal modification to existing ODE-based numerical solvers, incurs negligible computational overhead, and achieves superior image generation performance, especially in regions with only $5 \sim 10$ function evaluations.

</details>


### [33] [Hessian Geometry of Latent Space in Generative Models](https://arxiv.org/abs/2506.10632)
*Alexander Lobashev,Dmitry Guskov,Maria Larchenko,Mikhail Tamm*

Main category: cs.LG

TL;DR: 本文提出了一种新方法，通过重构Fisher信息度量来分析生成模型的潜在空间几何结构，包括统计物理模型和扩散模型。


<details>
  <summary>Details</summary>
Motivation: 研究生成模型潜在空间的几何结构及其与物理现象（如相变）的联系。

Method: 通过近似潜在变量的后验分布学习对数配分函数，从而定义Fisher度量，并提供理论收敛保证。

Result: 在Ising和TASEP模型上验证了方法的有效性，优于现有基线。扩散模型中揭示了潜在空间的相变分形结构。

Conclusion: 方法揭示了扩散模型潜在空间的复杂结构及其与相变的联系，为相关研究提供了新视角。

Abstract: This paper presents a novel method for analyzing the latent space geometry of generative models, including statistical physics models and diffusion models, by reconstructing the Fisher information metric. The method approximates the posterior distribution of latent variables given generated samples and uses this to learn the log-partition function, which defines the Fisher metric for exponential families. Theoretical convergence guarantees are provided, and the method is validated on the Ising and TASEP models, outperforming existing baselines in reconstructing thermodynamic quantities. Applied to diffusion models, the method reveals a fractal structure of phase transitions in the latent space, characterized by abrupt changes in the Fisher metric. We demonstrate that while geodesic interpolations are approximately linear within individual phases, this linearity breaks down at phase boundaries, where the diffusion model exhibits a divergent Lipschitz constant with respect to the latent space. These findings provide new insights into the complex structure of diffusion model latent spaces and their connection to phenomena like phase transitions. Our source code is available at https://github.com/alobashev/hessian-geometry-of-diffusion-models.

</details>


### [34] [ReGuidance: A Simple Diffusion Wrapper for Boosting Sample Quality on Hard Inverse Problems](https://arxiv.org/abs/2506.10955)
*Aayush Karan,Kulin Shah,Sitan Chen*

Main category: cs.LG

TL;DR: ReGuidance是一种简单的方法，用于提升扩散模型在解决逆问题时的样本真实性和奖励效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在低信噪比的逆问题中容易偏离数据流形，导致输出不真实。

Method: 通过反转候选解并重新初始化DPS，提升样本质量和测量一致性。

Result: 在困难的逆问题中，ReGuidance显著提升了样本质量和一致性。

Conclusion: ReGuidance是首个为DPS提供严格算法保证的方法。

Abstract: There has been a flurry of activity around using pretrained diffusion models as informed data priors for solving inverse problems, and more generally around steering these models using reward models. Training-free methods like diffusion posterior sampling (DPS) and its many variants have offered flexible heuristic algorithms for these tasks, but when the reward is not informative enough, e.g., in hard inverse problems with low signal-to-noise ratio, these techniques veer off the data manifold, failing to produce realistic outputs. In this work, we devise a simple wrapper, ReGuidance, for boosting both the sample realism and reward achieved by these methods. Given a candidate solution $\hat{x}$ produced by an algorithm of the user's choice, we propose inverting the solution by running the unconditional probability flow ODE in reverse starting from $\hat{x}$, and then using the resulting latent as an initialization for DPS. We evaluate our wrapper on hard inverse problems like large box in-painting and super-resolution with high upscaling. Whereas state-of-the-art baselines visibly fail, we find that applying our wrapper on top of these baselines significantly boosts sample quality and measurement consistency. We complement these findings with theory proving that on certain multimodal data distributions, ReGuidance simultaneously boosts the reward and brings the candidate solution closer to the data manifold. To our knowledge, this constitutes the first rigorous algorithmic guarantee for DPS.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [35] [EQ-TAA: Equivariant Traffic Accident Anticipation via Diffusion-Based Accident Video Synthesis](https://arxiv.org/abs/2506.10002)
*Jianwu Fang,Lei-Lei Li,Zhedong Zheng,Hongkai Yu,Jianru Xue,Zhengguo Li,Tat-Seng Chua*

Main category: cs.MM

TL;DR: 论文提出了一种基于注意力视频扩散（AVD）的模型，用于生成交通事故视频片段，并通过等变三元损失（EQ-TAA）提升交通事故预测性能。


<details>
  <summary>Details</summary>
Motivation: 交通事故预测（TAA）的现有方法依赖大量标注数据，且易受数据偏差影响，难以捕捉真实的因果关系。

Method: 提出AVD模型，通过文本提示生成因果视频帧，并结合EQ-TAA方法，使用等变三元损失优化预测性能。

Result: 实验表明，AVD和EQ-TAA在性能上优于现有方法。

Conclusion: AVD和EQ-TAA为交通事故预测提供了一种无需额外标注的高效解决方案。

Abstract: Traffic Accident Anticipation (TAA) in traffic scenes is a challenging problem for achieving zero fatalities in the future. Current approaches typically treat TAA as a supervised learning task needing the laborious annotation of accident occurrence duration. However, the inherent long-tailed, uncertain, and fast-evolving nature of traffic scenes has the problem that real causal parts of accidents are difficult to identify and are easily dominated by data bias, resulting in a background confounding issue. Thus, we propose an Attentive Video Diffusion (AVD) model that synthesizes additional accident video clips by generating the causal part in dashcam videos, i.e., from normal clips to accident clips. AVD aims to generate causal video frames based on accident or accident-free text prompts while preserving the style and content of frames for TAA after video generation. This approach can be trained using datasets collected from various driving scenes without any extra annotations. Additionally, AVD facilitates an Equivariant TAA (EQ-TAA) with an equivariant triple loss for an anchor accident-free video clip, along with the generated pair of contrastive pseudo-normal and pseudo-accident clips. Extensive experiments have been conducted to evaluate the performance of AVD and EQ-TAA, and competitive performance compared to state-of-the-art methods has been obtained.

</details>


### [36] [HER2 Expression Prediction with Flexible Multi-Modal Inputs via Dynamic Bidirectional Reconstruction](https://arxiv.org/abs/2506.10006)
*Jie Qin,Wei Yang,Yan Su,Yiran Zhu,Weizhen Li,Yunyue Pan,Chengchang Pan,Honggang Qi*

Main category: cs.MM

TL;DR: 提出了一种自适应双模态框架，通过动态分支选择、双向跨模态GAN和混合训练协议，灵活支持单/双模态HER2预测，显著提升准确性和资源效率。


<details>
  <summary>Details</summary>
Motivation: 现有HER2评估模型通常单独分析H&E或IHC图像，而临床依赖两者的协同解读，但双模态获取常受限于工作流复杂性和成本。

Method: 1) 动态分支选择器；2) 双向跨模态GAN；3) 混合训练协议。

Result: 单模态H&E预测准确率从71.44%提升至94.25%，双模态准确率达95.09%，IHC单模态可靠性为90.28%。

Conclusion: 该框架通过弹性设计显著提升HER2评估性能，适用于资源有限场景，具有广泛临床应用潜力。

Abstract: Current HER2 assessment models for breast cancer predominantly analyze H&E or IHC images in isolation,despite clinical reliance on their synergistic interpretation. However, concurrent acquisition of both modalities is often hindered by workflow complexity and cost constraints. We propose an adaptive bimodal framework enabling flexible single-/dual-modality HER2 prediction through three innovations: 1) A dynamic branch selector that activates either single-modality reconstruction or dual-modality joint inference based on input completeness; 2) A bidirectional cross-modal GAN performing context-aware feature-space reconstruction of missing modalities; 3) A hybrid training protocol integrating adversarial learning and multi-task optimization. This architecture elevates single-modality H&E prediction accuracy from 71.44% to 94.25% while achieving 95.09% dual-modality accuracy, maintaining 90.28% reliability with sole IHC inputs. The framework's "dual-preferred, single-compatible" design delivers near-bimodal performance without requiring synchronized acquisition, particularly benefiting resource-limited settings through IHC infrastructure cost reduction. Experimental validation confirms 22.81%/12.90% accuracy improvements over H&E/IHC baselines respectively, with cross-modal reconstruction enhancing F1-scores to 0.9609 (HE to IHC) and 0.9251 (IHC to HE). By dynamically routing inputs through reconstruction-enhanced or native fusion pathways, the system mitigates performance degradation from missing data while preserving computational efficiency (78.55% parameter reduction in lightweight variant). This elastic architecture demonstrates significant potential for democratizing precise HER2 assessment across diverse healthcare settings.

</details>


### [37] [Controllable Expressive 3D Facial Animation via Diffusion in a Unified Multimodal Space](https://arxiv.org/abs/2506.10007)
*Kangwei Liu,Junwu Liu,Xiaowei Yi,Jinlin Guo,Yun Cao*

Main category: cs.MM

TL;DR: 提出了一种基于扩散模型的多模态情感3D面部动画框架，解决了现有方法依赖单模态信号和确定性映射的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单模态控制信号且缺乏随机性表达，限制了动画的表现力。

Method: 采用FLAME多模态情感绑定策略和注意力潜扩散模型，实现多信号源情感控制和多样化运动生成。

Result: 实验表明，该方法在情感相似度上提升21.6%，同时保持自然的面部动态。

Conclusion: 该框架通过多模态融合和扩散模型显著提升了3D面部动画的表现力和可控性。

Abstract: Audio-driven emotional 3D facial animation encounters two significant challenges: (1) reliance on single-modal control signals (videos, text, or emotion labels) without leveraging their complementary strengths for comprehensive emotion manipulation, and (2) deterministic regression-based mapping that constrains the stochastic nature of emotional expressions and non-verbal behaviors, limiting the expressiveness of synthesized animations. To address these challenges, we present a diffusion-based framework for controllable expressive 3D facial animation. Our approach introduces two key innovations: (1) a FLAME-centered multimodal emotion binding strategy that aligns diverse modalities (text, audio, and emotion labels) through contrastive learning, enabling flexible emotion control from multiple signal sources, and (2) an attention-based latent diffusion model with content-aware attention and emotion-guided layers, which enriches motion diversity while maintaining temporal coherence and natural facial dynamics. Extensive experiments demonstrate that our method outperforms existing approaches across most metrics, achieving a 21.6\% improvement in emotion similarity while preserving physiologically plausible facial dynamics. Project Page: https://kangweiiliu.github.io/Control_3D_Animation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [38] [A Navigation Framework Utilizing Vision-Language Models](https://arxiv.org/abs/2506.10172)
*Yicheng Duan,Kaiyu tang*

Main category: cs.RO

TL;DR: 提出了一种模块化的视觉语言导航框架，通过解耦视觉语言理解与行动规划，结合轻量级规划逻辑，实现了快速、灵活的导航。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言导航中计算成本高、实时部署难的问题，同时提升导航系统的适应性和效率。

Method: 使用冻结的视觉语言模型Qwen2.5-VL-7B-Instruct，结合提示工程、结构化历史管理和双帧视觉输入策略。

Result: 在Room-to-Room基准测试中初步验证了框架的有效性，但在未见环境中泛化能力有限。

Conclusion: 模块化方法为高效导航系统奠定了基础，未来可通过增强环境先验和扩展多模态输入进一步优化。

Abstract: Vision-and-Language Navigation (VLN) presents a complex challenge in embodied AI, requiring agents to interpret natural language instructions and navigate through visually rich, unfamiliar environments. Recent advances in large vision-language models (LVLMs), such as CLIP and Flamingo, have significantly improved multimodal understanding but introduced new challenges related to computational cost and real-time deployment. In this project, we propose a modular, plug-and-play navigation framework that decouples vision-language understanding from action planning. By integrating a frozen vision-language model, Qwen2.5-VL-7B-Instruct, with lightweight planning logic, we aim to achieve flexible, fast, and adaptable navigation without extensive model fine-tuning. Our framework leverages prompt engineering, structured history management, and a two-frame visual input strategy to enhance decision-making continuity across navigation steps. We evaluate our system on the Room-to-Room benchmark within the VLN-CE setting using the Matterport3D dataset and Habitat-Lab simulation environment. Although our initial results reveal challenges in generalizing to unseen environments under strict evaluation settings, our modular approach lays a foundation for scalable and efficient navigation systems, highlighting promising directions for future improvement through enhanced environmental priors and expanded multimodal input integration.

</details>


### [39] [Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop](https://arxiv.org/abs/2506.10968)
*Justin Kerr,Kush Hari,Ethan Weber,Chung Min Kim,Brent Yi,Tyler Bonnen,Ken Goldberg,Angjoo Kanazawa*

Main category: cs.RO

TL;DR: EyeRobot是一个模仿人类主动观察行为的机器人系统，通过强化学习训练眼球运动策略，实现手眼协调完成任务。


<details>
  <summary>Details</summary>
Motivation: 人类通过主动观察来完成任务，受此启发，开发了能够自主控制眼球的机器人系统。

Method: 使用360度摄像头收集数据，在仿真环境中训练眼球运动策略，并通过BC-RL循环联合训练手和眼。

Result: EyeRobot在五个全景工作空间任务中表现出有效的手眼协调能力，能够在大范围内完成任务。

Conclusion: EyeRobot展示了通过单一摄像头实现高效手眼协调的潜力，为机器人视觉任务提供了新思路。

Abstract: Humans do not passively observe the visual world -- we actively look in order to act. Motivated by this principle, we introduce EyeRobot, a robotic system with gaze behavior that emerges from the need to complete real-world tasks. We develop a mechanical eyeball that can freely rotate to observe its surroundings and train a gaze policy to control it using reinforcement learning. We accomplish this by first collecting teleoperated demonstrations paired with a 360 camera. This data is imported into a simulation environment that supports rendering arbitrary eyeball viewpoints, allowing episode rollouts of eye gaze on top of robot demonstrations. We then introduce a BC-RL loop to train the hand and eye jointly: the hand (BC) agent is trained from rendered eye observations, and the eye (RL) agent is rewarded when the hand produces correct action predictions. In this way, hand-eye coordination emerges as the eye looks towards regions which allow the hand to complete the task. EyeRobot implements a foveal-inspired policy architecture allowing high resolution with a small compute budget, which we find also leads to the emergence of more stable fixation as well as improved ability to track objects and ignore distractors. We evaluate EyeRobot on five panoramic workspace manipulation tasks requiring manipulation in an arc surrounding the robot arm. Our experiments suggest EyeRobot exhibits hand-eye coordination behaviors which effectively facilitate manipulation over large workspaces with a single camera. See project site for videos: https://www.eyerobot.net/

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [40] [Prompt-Guided Latent Diffusion with Predictive Class Conditioning for 3D Prostate MRI Generation](https://arxiv.org/abs/2506.10230)
*Emerson P. Grabke,Masoom A. Haider,Babak Taati*

Main category: eess.IV

TL;DR: 论文提出了一种名为CCELLA的双头条件方法，结合非医学大型语言模型和病理分类，解决了医学LDM训练中的数据稀缺问题，显著提升了合成图像质量和分类器性能。


<details>
  <summary>Details</summary>
Motivation: 医学LDM训练常受限于数据稀缺、短提示文本编码器或非医学LDMs的复用，导致性能或科学可访问性受限。

Method: 提出CCELLA方法，通过交叉注意力和时间步嵌入同时条件化LDM U-Net，并设计联合损失函数和数据高效训练框架。

Result: 在有限数据下，方法显著优于现有模型（FID 0.025 vs 0.071），合成图像提升了分类器准确率（69%到74%）。

Conclusion: CCELLA方法在数据有限条件下高效训练医学LDM，提升了合成图像质量和科学可访问性。

Abstract: Latent diffusion models (LDM) could alleviate data scarcity challenges affecting machine learning development for medical imaging. However, medical LDM training typically relies on performance- or scientific accessibility-limiting strategies including a reliance on short-prompt text encoders, the reuse of non-medical LDMs, or a requirement for fine-tuning with large data volumes. We propose a Class-Conditioned Efficient Large Language model Adapter (CCELLA) to address these limitations. CCELLA is a novel dual-head conditioning approach that simultaneously conditions the LDM U-Net with non-medical large language model-encoded text features through cross-attention and with pathology classification through the timestep embedding. We also propose a joint loss function and a data-efficient LDM training framework. In combination, these strategies enable pathology-conditioned LDM training for high-quality medical image synthesis given limited data volume and human data annotation, improving LDM performance and scientific accessibility. Our method achieves a 3D FID score of 0.025 on a size-limited prostate MRI dataset, significantly outperforming a recent foundation model with FID 0.071. When training a classifier for prostate cancer prediction, adding synthetic images generated by our method to the training dataset improves classifier accuracy from 69% to 74%. Training a classifier solely on our method's synthetic images achieved comparable performance to training on real images alone.

</details>


### [41] [Conditional diffusion models for guided anomaly detection in brain images using fluid-driven anomaly randomization](https://arxiv.org/abs/2506.10233)
*Ana Lawry Aguila,Peirong Liu,Oula Puonti,Juan Eugenio Iglesias*

Main category: eess.IV

TL;DR: 论文提出了一种基于条件扩散模型的新框架，用于脑MRI中的异常检测和健康图像重建，通过引入合成伪病理图像改进重建效果。


<details>
  <summary>Details</summary>
Motivation: 监督学习需要大量患病数据，而罕见疾病数据稀缺；现有无监督方法假设模型无法重建异常，但实际效果不佳。

Method: 采用条件扩散模型框架，结合合成伪病理图像（通过流体驱动异常随机化生成）指导健康图像重建。

Result: 模型在合成和真实病理数据集上表现优异，优于变分自编码器、条件/无条件潜在扩散模型，甚至部分超越有监督方法。

Conclusion: 提出的弱监督方法在异常检测和健康图像重建中效果显著，为罕见疾病诊断提供了新思路。

Abstract: Supervised machine learning has enabled accurate pathology detection in brain MRI, but requires training data from diseased subjects that may not be readily available in some scenarios, for example, in the case of rare diseases. Reconstruction-based unsupervised anomaly detection, in particular using diffusion models, has gained popularity in the medical field as it allows for training on healthy images alone, eliminating the need for large disease-specific cohorts. These methods assume that a model trained on normal data cannot accurately represent or reconstruct anomalies. However, this assumption often fails with models failing to reconstruct healthy tissue or accurately reconstruct abnormal regions i.e., failing to remove anomalies. In this work, we introduce a novel conditional diffusion model framework for anomaly detection and healthy image reconstruction in brain MRI. Our weakly supervised approach integrates synthetically generated pseudo-pathology images into the modeling process to better guide the reconstruction of healthy images. To generate these pseudo-pathologies, we apply fluid-driven anomaly randomization to augment real pathology segmentation maps from an auxiliary dataset, ensuring that the synthetic anomalies are both realistic and anatomically coherent. We evaluate our model's ability to detect pathology, using both synthetic anomaly datasets and real pathology from the ATLAS dataset. In our extensive experiments, our model: (i) consistently outperforms variational autoencoders, and conditional and unconditional latent diffusion; and (ii) surpasses on most datasets, the performance of supervised inpainting methods with access to paired diseased/healthy images.

</details>


### [42] [SWDL: Stratum-Wise Difference Learning with Deep Laplacian Pyramid for Semi-Supervised 3D Intracranial Hemorrhage Segmentation](https://arxiv.org/abs/2506.10325)
*Cheng Wang,Siqi Chen,Donghua Mi,Yang Chen,Yudong Zhang,Yinsheng Li*

Main category: eess.IV

TL;DR: 论文提出了一种新的半监督学习框架SWDL-Net，结合拉普拉斯金字塔和深度卷积上采样的优势，用于颅内出血（ICH）的医学图像分割，在仅有少量标注数据的情况下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于颅内出血（ICH）标注数据的获取成本高且耗时，传统深度学习方法依赖大量标注数据的局限性凸显。半监督学习（SSL）成为解决标注数据稀缺的潜在方案。

Method: 提出SWDL-Net框架，结合拉普拉斯金字塔（擅长边缘锐化）和深度卷积上采样（提升细节精度）的互补优势，通过差异学习机制实现更好的分割效果。

Result: 在271例ICH数据集和公开基准测试中，仅使用2%标注数据时表现优于现有方法；在BHSD数据集（5%标注数据）上进一步验证了其优越性。

Conclusion: SWDL-Net通过创新性地整合互补技术，显著提升了半监督学习在医学图像分割中的性能，为标注数据稀缺问题提供了有效解决方案。

Abstract: Recent advances in medical imaging have established deep learning-based segmentation as the predominant approach, though it typically requires large amounts of manually annotated data. However, obtaining annotations for intracranial hemorrhage (ICH) remains particularly challenging due to the tedious and costly labeling process. Semi-supervised learning (SSL) has emerged as a promising solution to address the scarcity of labeled data, especially in volumetric medical image segmentation. Unlike conventional SSL methods that primarily focus on high-confidence pseudo-labels or consistency regularization, we propose SWDL-Net, a novel SSL framework that exploits the complementary advantages of Laplacian pyramid and deep convolutional upsampling. The Laplacian pyramid excels at edge sharpening, while deep convolutions enhance detail precision through flexible feature mapping. Our framework achieves superior segmentation of lesion details and boundaries through a difference learning mechanism that effectively integrates these complementary approaches. Extensive experiments on a 271-case ICH dataset and public benchmarks demonstrate that SWDL-Net outperforms current state-of-the-art methods in scenarios with only 2% labeled data. Additional evaluations on the publicly available Brain Hemorrhage Segmentation Dataset (BHSD) with 5% labeled data further confirm the superiority of our approach. Code and data have been released at https://github.com/SIAT-CT-LAB/SWDL.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [43] [AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation](https://arxiv.org/abs/2506.10540)
*Haoyuan Shi,Yunxin Li,Xinyu Chen,Longyue Wang,Baotian Hu,Min Zhang*

Main category: cs.MA

TL;DR: AniMaker是一个多智能体框架，通过高效生成多候选视频片段和故事感知的片段选择，解决了多场景和角色视频生成的连贯性问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在多场景和角色叙事视频生成中存在叙事不连贯和节奏问题，AniMaker旨在解决这些问题。

Method: AniMaker采用多智能体框架，包括导演、摄影、评审和后期制作代理，结合MCTS-Gen和AniEval技术优化生成和评估。

Result: 实验表明，AniMaker在VBench和AniEval等指标上表现优异，显著提升了多候选生成的效率和叙事连贯性。

Conclusion: AniMaker通过多智能体协作和专用评估框架，推动了AI生成叙事动画向生产标准迈进。

Abstract: Despite rapid advancements in video generation models, generating coherent storytelling videos that span multiple scenes and characters remains challenging. Current methods often rigidly convert pre-generated keyframes into fixed-length clips, resulting in disjointed narratives and pacing issues. Furthermore, the inherent instability of video generation models means that even a single low-quality clip can significantly degrade the entire output animation's logical coherence and visual continuity. To overcome these obstacles, we introduce AniMaker, a multi-agent framework enabling efficient multi-candidate clip generation and storytelling-aware clip selection, thus creating globally consistent and story-coherent animation solely from text input. The framework is structured around specialized agents, including the Director Agent for storyboard generation, the Photography Agent for video clip generation, the Reviewer Agent for evaluation, and the Post-Production Agent for editing and voiceover. Central to AniMaker's approach are two key technical components: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search (MCTS)-inspired strategy that intelligently navigates the candidate space to generate high-potential clips while optimizing resource usage; and AniEval in Reviewer Agent, the first framework specifically designed for multi-shot animation evaluation, which assesses critical aspects such as story-level consistency, action completion, and animation-specific features by considering each clip in the context of its preceding and succeeding clips. Experiments demonstrate that AniMaker achieves superior quality as measured by popular metrics including VBench and our proposed AniEval framework, while significantly improving the efficiency of multi-candidate generation, pushing AI-generated storytelling animation closer to production standards.

</details>
