<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 24]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Assessing Learned Models for Phase-only Hologram Compression](https://arxiv.org/abs/2507.06646)
*Zicong Peng,Yicheng Zhan,Josef Spjut,Kaan Akşit*

Main category: cs.GR

TL;DR: 评估了四种常见学习模型在压缩相位全息图中的性能，发现INR模型（如SIREN）表现优于VAE模型（如TAESD）。


<details>
  <summary>Details</summary>
Motivation: 研究不同模型在相位全息图压缩任务中的表现，以确定最适合的模型结构。

Method: 比较了四种模型（MLP、SIREN、FilmSIREN和TAESD）在压缩相位全息图时的性能。

Result: SIREN（4.9k参数）实现了40%压缩率且重建质量高（PSNR=34.54 dB），而TAESD（2.2M参数）表现不佳。

Conclusion: INR模型在相位全息图压缩中更有效，预训练VAE模型需任务特定适配。

Abstract: We evaluate the performance of four common learned models utilizing INR and VAE structures for compressing phase-only holograms in holographic displays. The evaluated models include a vanilla MLP, SIREN, and FilmSIREN, with TAESD as the representative VAE model. Our experiments reveal that a pretrained image VAE, TAESD, with 2.2M parameters struggles with phase-only hologram compression, revealing the need for task-specific adaptations. Among the INRs, SIREN with 4.9k parameters achieves %40 compression with high quality in the reconstructed 3D images (PSNR = 34.54 dB). These results emphasize the effectiveness of INRs and identify the limitations of pretrained image compression VAEs for hologram compression task.

</details>


### [2] [Enhancing non-Rigid 3D Model Deformations Using Mesh-based Gaussian Splatting](https://arxiv.org/abs/2507.07000)
*Wijayathunga W. M. R. D. B*

Main category: cs.GR

TL;DR: 提出了一种新框架，通过将网格表示与3D高斯泼溅结合，增强非刚性3D模型变形能力。


<details>
  <summary>Details</summary>
Motivation: 传统高斯泼溅虽能实现快速实时辐射场渲染，但其后编辑功能和对大规模非刚性变形的支持有限。

Method: 将高斯核直接嵌入显式网格表面，利用网格的拓扑和几何先验指导编辑操作（如移动、缩放、旋转）并支持复杂变形（如弯曲和拉伸）。

Result: 实现了更灵活的3D内容创作流程。

Conclusion: 为虚拟现实、角色动画和交互设计等应用提供了更灵活的3D内容创作工具。

Abstract: We propose a novel framework that enhances non-rigid 3D model deformations by bridging mesh representations with 3D Gaussian splatting. While traditional Gaussian splatting delivers fast, real-time radiance-field rendering, its post-editing capabilities and support for large-scale, non-rigid deformations remain limited. Our method addresses these challenges by embedding Gaussian kernels directly onto explicit mesh surfaces. This allows the mesh's inherent topological and geometric priors to guide intuitive editing operations -- such as moving, scaling, and rotating individual 3D components -- and enables complex deformations like bending and stretching. This work paves the way for more flexible 3D content-creation workflows in applications spanning virtual reality, character animation, and interactive design.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [3] [Unveiling the Underwater World: CLIP Perception Model-Guided Underwater Image Enhancement](https://arxiv.org/abs/2507.06234)
*Jiangzhong Cao,Zekai Zeng,Xu Zhang,Huan Zhang,Chunling Fan,Gangyi Jiang,Weisi Lin*

Main category: cs.CV

TL;DR: 提出了一种结合CLIP感知损失模块和课程对比正则化的水下图像增强方法，以提升感知质量和内容恢复。


<details>
  <summary>Details</summary>
Motivation: 水下图像质量受光吸收和散射影响，现有深度学习方法忽视人类感知且约束不足，导致增强图像质量下降或内容恢复不佳。

Method: 利用CLIP模型的视觉语义特征提取能力，设计感知损失模块和课程对比正则化，优化增强网络的约束和感知质量。

Result: 实验表明，该方法在视觉质量和泛化能力上优于现有方法。

Conclusion: 结合CLIP感知和课程对比正则化有效提升了水下图像增强的感知质量和内容恢复效果。

Abstract: High-quality underwater images are essential for both machine vision tasks and viewers with their aesthetic appeal.However, the quality of underwater images is severely affected by light absorption and scattering. Deep learning-based methods for Underwater Image Enhancement (UIE) have achieved good performance. However, these methods often overlook considering human perception and lack sufficient constraints within the solution space. Consequently, the enhanced images often suffer from diminished perceptual quality or poor content restoration.To address these issues, we propose a UIE method with a Contrastive Language-Image Pre-Training (CLIP) perception loss module and curriculum contrastive regularization. Above all, to develop a perception model for underwater images that more aligns with human visual perception, the visual semantic feature extraction capability of the CLIP model is leveraged to learn an appropriate prompt pair to map and evaluate the quality of underwater images. This CLIP perception model is then incorporated as a perception loss module into the enhancement network to improve the perceptual quality of enhanced images. Furthermore, the CLIP perception model is integrated with the curriculum contrastive regularization to enhance the constraints imposed on the enhanced images within the CLIP perceptual space, mitigating the risk of both under-enhancement and over-enhancement. Specifically, the CLIP perception model is employed to assess and categorize the learning difficulty level of negatives in the regularization process, ensuring comprehensive and nuanced utilization of distorted images and negatives with varied quality levels. Extensive experiments demonstrate that our method outperforms state-of-the-art methods in terms of visual quality and generalization ability.

</details>


### [4] [FIFA: Unified Faithfulness Evaluation Framework for Text-to-Video and Video-to-Text Generation](https://arxiv.org/abs/2507.06523)
*Liqiang Jing,Viet Lai,Seunghyun Yoon,Trung Bui,Xinya Du*

Main category: cs.CV

TL;DR: FIFA是一个统一的视频多模态大语言模型（VideoMLLMs）忠实性评估框架，通过提取描述性事实、建模语义依赖关系并验证，解决了现有评估方法在开放自由响应中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法局限于单一任务且无法评估开放自由响应中的幻觉问题，因此需要一种更全面的评估框架。

Method: 提出FIFA框架，提取描述性事实，构建时空语义依赖图，并通过VideoQA模型验证；同时引入基于工具的后校正框架Post-Correction。

Result: FIFA比现有评估方法更接近人类判断，Post-Correction有效提升了文本和视频生成的事实一致性。

Conclusion: FIFA和Post-Correction为VideoMLLMs的忠实性评估和幻觉修正提供了有效解决方案。

Abstract: Video Multimodal Large Language Models (VideoMLLMs) have achieved remarkable progress in both Video-to-Text and Text-to-Video tasks. However, they often suffer fro hallucinations, generating content that contradicts the visual input. Existing evaluation methods are limited to one task (e.g., V2T) and also fail to assess hallucinations in open-ended, free-form responses. To address this gap, we propose FIFA, a unified FaIthFulness evAluation framework that extracts comprehensive descriptive facts, models their semantic dependencies via a Spatio-Temporal Semantic Dependency Graph, and verifies them using VideoQA models. We further introduce Post-Correction, a tool-based correction framework that revises hallucinated content. Extensive experiments demonstrate that FIFA aligns more closely with human judgment than existing evaluation methods, and that Post-Correction effectively improves factual consistency in both text and video generation.

</details>


### [5] [A Probabilistic Approach to Uncertainty Quantification Leveraging 3D Geometry](https://arxiv.org/abs/2507.06269)
*Rushil Desai,Frederik Warburg,Trevor Darrell,Marissa Ramirez de Chanlatte*

Main category: cs.CV

TL;DR: BayesSDF提出了一种新的概率框架，用于量化神经隐式SDF模型中的不确定性，解决了现有方法在几何一致性和校准性上的不足。


<details>
  <summary>Details</summary>
Motivation: 科学模拟应用（如森林中的流体流动）需要精确的表面几何和对几何不确定性的感知，而现有方法缺乏直接几何集成，导致不确定性映射不准确。

Method: BayesSDF利用拉普拉斯近似，通过基于Hessian的指标量化局部表面不稳定性，实现高效计算和表面感知的不确定性估计。

Result: 实验表明，BayesSDF在合成和真实数据集上优于现有方法，提供了与重建几何密切相关的可操作置信度。

Conclusion: BayesSDF为不确定性感知的3D场景重建、模拟和机器人决策奠定了坚实基础。

Abstract: Quantifying uncertainty in neural implicit 3D representations, particularly those utilizing Signed Distance Functions (SDFs), remains a substantial challenge due to computational inefficiencies, scalability issues, and geometric inconsistencies. Existing methods typically neglect direct geometric integration, leading to poorly calibrated uncertainty maps. We introduce BayesSDF, a novel probabilistic framework for uncertainty quantification in neural implicit SDF models, motivated by scientific simulation applications with 3D environments (e.g., forests) such as modeling fluid flow through forests, where precise surface geometry and awareness of fidelity surface geometric uncertainty are essential. Unlike radiance-based models such as NeRF or 3D Gaussian splatting, which lack explicit surface formulations, SDFs define continuous and differentiable geometry, making them better suited for physical modeling and analysis. BayesSDF leverages a Laplace approximation to quantify local surface instability via Hessian-based metrics, enabling computationally efficient, surface-aware uncertainty estimation. Our method shows that uncertainty predictions correspond closely with poorly reconstructed geometry, providing actionable confidence measures for downstream use. Extensive evaluations on synthetic and real-world datasets demonstrate that BayesSDF outperforms existing methods in both calibration and geometric consistency, establishing a strong foundation for uncertainty-aware 3D scene reconstruction, simulation, and robotic decision-making.

</details>


### [6] [Concept Unlearning by Modeling Key Steps of Diffusion Process](https://arxiv.org/abs/2507.06526)
*Chaoshuo Zhang,Chenhao Lin,Zhengyu Zhao,Le Yang,Qian Wang,Chao Shen*

Main category: cs.CV

TL;DR: 论文提出了一种名为KSCU的新方法，通过针对扩散模型的关键步骤进行概念遗忘，以平衡遗忘效果与生成能力的保留。


<details>
  <summary>Details</summary>
Motivation: 现有概念遗忘方法难以平衡遗忘效果与生成能力的保留，导致扩散模型的安全风险无法有效解决。

Method: KSCU方法利用扩散模型的逐步采样特性，专注于对最终结果影响最大的关键步骤，仅在这些步骤上微调模型。

Result: 实验表明，KSCU能有效防止生成不良图像，同时更好地保留模型的生成能力。

Conclusion: KSCU通过关键步骤的针对性遗忘，实现了安全性与生成能力的平衡。

Abstract: Text-to-image diffusion models (T2I DMs), represented by Stable Diffusion, which generate highly realistic images based on textual input, have been widely used. However, their misuse poses serious security risks. While existing concept unlearning methods aim to mitigate these risks, they struggle to balance unlearning effectiveness with generative retainability.To overcome this limitation, we innovatively propose the Key Step Concept Unlearning (KSCU) method, which ingeniously capitalizes on the unique stepwise sampling characteristic inherent in diffusion models during the image generation process. Unlike conventional approaches that treat all denoising steps equally, KSCU strategically focuses on pivotal steps with the most influence over the final outcome by dividing key steps for different concept unlearning tasks and fine-tuning the model only at those steps. This targeted approach reduces the number of parameter updates needed for effective unlearning, while maximizing the retention of the model's generative capabilities.Through extensive benchmark experiments, we demonstrate that KSCU effectively prevents T2I DMs from generating undesirable images while better retaining the model's generative capabilities.Our code will be released.

</details>


### [7] [Concept-TRAK: Understanding how diffusion models learn concepts through concept-level attribution](https://arxiv.org/abs/2507.06547)
*Yonghyun Park,Chieh-Hsin Lai,Satoshi Hayakawa,Yuhta Takida,Naoki Murata,Wei-Hsiang Liao,Woosung Choi,Kin Wai Cheuk,Junghyun Koo,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 论文提出了一种名为Concept-TRAK的新方法，用于解决扩散模型在图像生成中的版权和透明度问题，通过概念级归因提供更精细的分析。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中表现优异，但其广泛使用引发了版权和透明度问题。现有方法无法精确归因到具体元素（如风格或对象），因此需要更精细的解决方案。

Method: 提出Concept-TRAK方法，扩展影响函数，包括基于扩散后验采样的训练损失和强调语义相关性的概念感知奖励函数。

Result: 在AbC基准测试中，Concept-TRAK显著优于现有方法，并通过案例研究展示了其在版权保护、内容安全和模型分析中的实用性。

Conclusion: Concept-TRAK为生成式AI的负责任开发和治理提供了可操作的见解，解决了概念级归因的需求。

Abstract: While diffusion models excel at image generation, their growing adoption raises critical concerns around copyright issues and model transparency. Existing attribution methods identify training examples influencing an entire image, but fall short in isolating contributions to specific elements, such as styles or objects, that matter most to stakeholders. To bridge this gap, we introduce \emph{concept-level attribution} via a novel method called \emph{Concept-TRAK}. Concept-TRAK extends influence functions with two key innovations: (1) a reformulated diffusion training loss based on diffusion posterior sampling, enabling robust, sample-specific attribution; and (2) a concept-aware reward function that emphasizes semantic relevance. We evaluate Concept-TRAK on the AbC benchmark, showing substantial improvements over prior methods. Through diverse case studies--ranging from identifying IP-protected and unsafe content to analyzing prompt engineering and compositional learning--we demonstrate how concept-level attribution yields actionable insights for responsible generative AI development and governance.

</details>


### [8] [MOST: Motion Diffusion Model for Rare Text via Temporal Clip Banzhaf Interaction](https://arxiv.org/abs/2507.06590)
*Yin Wang,Mu li,Zhiying Leng,Frederick W. B. Li,Xiaohui Liang*

Main category: cs.CV

TL;DR: MOST是一种新颖的运动扩散模型，通过时间片段Banzhaf交互解决从罕见语言提示生成人类运动的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在粗粒度匹配和忽略语义线索方面存在不足，MOST通过细粒度片段关系解决这些问题。

Method: 提出时间片段Banzhaf交互量化文本-运动一致性，结合运动提示模块生成语义一致的运动。

Result: MOST在文本到运动检索和生成方面达到最先进水平，尤其在罕见提示上表现突出。

Conclusion: MOST通过细粒度匹配和消除冗余，显著提升了文本到运动生成的性能。

Abstract: We introduce MOST, a novel motion diffusion model via temporal clip Banzhaf interaction, aimed at addressing the persistent challenge of generating human motion from rare language prompts. While previous approaches struggle with coarse-grained matching and overlook important semantic cues due to motion redundancy, our key insight lies in leveraging fine-grained clip relationships to mitigate these issues. MOST's retrieval stage presents the first formulation of its kind - temporal clip Banzhaf interaction - which precisely quantifies textual-motion coherence at the clip level. This facilitates direct, fine-grained text-to-motion clip matching and eliminates prevalent redundancy. In the generation stage, a motion prompt module effectively utilizes retrieved motion clips to produce semantically consistent movements. Extensive evaluations confirm that MOST achieves state-of-the-art text-to-motion retrieval and generation performance by comprehensively addressing previous challenges, as demonstrated through quantitative and qualitative results highlighting its effectiveness, especially for rare prompts.

</details>


### [9] [ClipGS: Clippable Gaussian Splatting for Interactive Cinematic Visualization of Volumetric Medical Data](https://arxiv.org/abs/2507.06647)
*Chengkun Li,Yuqi Tong,Kai Chen,Zhenya Yang,Ruiyang Li,Shi Qiu,Jason Ying-Kuen Chan,Pheng-Ann Heng,Qi Dou*

Main category: cs.CV

TL;DR: ClipGS是一种支持裁剪平面的高斯泼溅框架，用于医学体积数据的交互式电影化渲染，通过可学习的截断方案和自适应调整模型，显著提升了渲染质量和效率。


<details>
  <summary>Details</summary>
Motivation: 医学体积数据的可视化对诊断和手术规划至关重要，但现有方法因高计算成本和低渲染速度难以满足交互需求。

Method: 提出ClipGS框架，结合可学习的截断方案和自适应调整模型，动态调整高斯基元的可见性和变形。

Result: 在五种医学数据上测试，平均PSNR为36.635，帧率为156 FPS，模型大小为16.1 MB，优于现有方法。

Conclusion: ClipGS在渲染质量和效率上表现优异，为医学数据交互式可视化提供了有效解决方案。

Abstract: The visualization of volumetric medical data is crucial for enhancing diagnostic accuracy and improving surgical planning and education. Cinematic rendering techniques significantly enrich this process by providing high-quality visualizations that convey intricate anatomical details, thereby facilitating better understanding and decision-making in medical contexts. However, the high computing cost and low rendering speed limit the requirement of interactive visualization in practical applications. In this paper, we introduce ClipGS, an innovative Gaussian splatting framework with the clipping plane supported, for interactive cinematic visualization of volumetric medical data. To address the challenges posed by dynamic interactions, we propose a learnable truncation scheme that automatically adjusts the visibility of Gaussian primitives in response to the clipping plane. Besides, we also design an adaptive adjustment model to dynamically adjust the deformation of Gaussians and refine the rendering performance. We validate our method on five volumetric medical data (including CT and anatomical slice data), and reach an average 36.635 PSNR rendering quality with 156 FPS and 16.1 MB model size, outperforming state-of-the-art methods in rendering quality and efficiency.

</details>


### [10] [Diff$^2$I2P: Differentiable Image-to-Point Cloud Registration with Diffusion Prior](https://arxiv.org/abs/2507.06651)
*Juncheng Mu,Chengwei Ren,Weixiang Zhang,Liang Pan,Xiao-Ping Zhang,Yue Gao*

Main category: cs.CV

TL;DR: Diff$^2$I2P提出了一种基于扩散先验的全微分I2P配准框架，通过Control-Side Score Distillation和Deformable Correspondence Tuning模块解决模态差距问题，显著提升了配准性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过度量学习实现跨模态特征对齐，但忽略了图像与点云数据之间的固有模态差距，导致跨模态对应关系不准确。

Method: 提出Diff$^2$I2P框架，利用深度条件扩散模型的Control-Side Score Distillation技术优化变换预测，并通过Deformable Correspondence Tuning模块实现可微对应关系估计和变换求解。

Result: 在7-Scenes基准测试中，Diff$^2$I2P显著优于现有方法，配准召回率提升超过7%。

Conclusion: 扩散模型作为强先验，有效指导跨模态特征学习，形成鲁棒的对应关系，提升了I2P配准性能。

Abstract: Learning cross-modal correspondences is essential for image-to-point cloud (I2P) registration. Existing methods achieve this mostly by utilizing metric learning to enforce feature alignment across modalities, disregarding the inherent modality gap between image and point data. Consequently, this paradigm struggles to ensure accurate cross-modal correspondences. To this end, inspired by the cross-modal generation success of recent large diffusion models, we propose Diff$^2$I2P, a fully Differentiable I2P registration framework, leveraging a novel and effective Diffusion prior for bridging the modality gap. Specifically, we propose a Control-Side Score Distillation (CSD) technique to distill knowledge from a depth-conditioned diffusion model to directly optimize the predicted transformation. However, the gradients on the transformation fail to backpropagate onto the cross-modal features due to the non-differentiability of correspondence retrieval and PnP solver. To this end, we further propose a Deformable Correspondence Tuning (DCT) module to estimate the correspondences in a differentiable way, followed by the transformation estimation using a differentiable PnP solver. With these two designs, the Diffusion model serves as a strong prior to guide the cross-modal feature learning of image and point cloud for forming robust correspondences, which significantly improves the registration. Extensive experimental results demonstrate that Diff$^2$I2P consistently outperforms SoTA I2P registration methods, achieving over 7% improvement in registration recall on the 7-Scenes benchmark.

</details>


### [11] [MS-DPPs: Multi-Source Determinantal Point Processes for Contextual Diversity Refinement of Composite Attributes in Text to Image Retrieval](https://arxiv.org/abs/2507.06654)
*Naoya Sogi,Takashi Shibata,Makoto Terao,Masanori Suganuma,Takayuki Okatani*

Main category: cs.CV

TL;DR: 论文提出了一种名为CDR-CA的新任务，通过多源DPP方法优化多属性多样性，以适应不同应用场景。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅关注图像外观多样性，但多样性指标及其期望值因应用而异，限制了其应用范围。

Method: 提出Multi-Source DPPs，扩展DPP至多源，基于流形表示的统一相似矩阵建模，并引入切线归一化以反映上下文。

Result: 大量实验证明所提方法的有效性。

Conclusion: CDR-CA任务及Multi-Source DPPs方法能有效优化多属性多样性，适应不同应用场景。

Abstract: Result diversification (RD) is a crucial technique in Text-to-Image Retrieval for enhancing the efficiency of a practical application. Conventional methods focus solely on increasing the diversity metric of image appearances. However, the diversity metric and its desired value vary depending on the application, which limits the applications of RD. This paper proposes a novel task called CDR-CA (Contextual Diversity Refinement of Composite Attributes). CDR-CA aims to refine the diversities of multiple attributes, according to the application's context. To address this task, we propose Multi-Source DPPs, a simple yet strong baseline that extends the Determinantal Point Process (DPP) to multi-sources. We model MS-DPP as a single DPP model with a unified similarity matrix based on a manifold representation. We also introduce Tangent Normalization to reflect contexts. Extensive experiments demonstrate the effectiveness of the proposed method. Our code is publicly available at https://github.com/NEC-N-SOGI/msdpp.

</details>


### [12] [Enhancing Diffusion Model Stability for Image Restoration via Gradient Management](https://arxiv.org/abs/2507.06656)
*Hongjie Wu,Mingqin Zhang,Linchao He,Ji-Zhe Zhou,Jiancheng Lv*

Main category: cs.CV

TL;DR: 论文提出了一种新的梯度管理技术SPGD，通过渐进式似然预热和自适应方向动量平滑，解决了扩散模型中先验与似然梯度冲突及不稳定的问题，显著提升了图像恢复性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像恢复中表现出色，但先验与似然梯度之间的交互及其不稳定性尚未充分研究，影响了生成过程的稳定性与恢复效果。

Method: 提出SPGD技术，包含渐进式似然预热策略和自适应方向动量平滑，分别用于缓解梯度冲突和减少梯度波动。

Result: 实验表明，SPGD显著提升了生成稳定性，在多种恢复任务中实现了最优的定量指标和视觉结果。

Conclusion: SPGD通过有效管理梯度冲突与波动，显著提升了扩散模型在图像恢复中的性能。

Abstract: Diffusion models have shown remarkable promise for image restoration by leveraging powerful priors. Prominent methods typically frame the restoration problem within a Bayesian inference framework, which iteratively combines a denoising step with a likelihood guidance step. However, the interactions between these two components in the generation process remain underexplored. In this paper, we analyze the underlying gradient dynamics of these components and identify significant instabilities. Specifically, we demonstrate conflicts between the prior and likelihood gradient directions, alongside temporal fluctuations in the likelihood gradient itself. We show that these instabilities disrupt the generative process and compromise restoration performance. To address these issues, we propose Stabilized Progressive Gradient Diffusion (SPGD), a novel gradient management technique. SPGD integrates two synergistic components: (1) a progressive likelihood warm-up strategy to mitigate gradient conflicts; and (2) adaptive directional momentum (ADM) smoothing to reduce fluctuations in the likelihood gradient. Extensive experiments across diverse restoration tasks demonstrate that SPGD significantly enhances generation stability, leading to state-of-the-art performance in quantitative metrics and visually superior results. Code is available at \href{https://github.com/74587887/SPGD}{here}.

</details>


### [13] [FlexGaussian: Flexible and Cost-Effective Training-Free Compression for 3D Gaussian Splatting](https://arxiv.org/abs/2507.06671)
*Boyuan Tian,Qizhe Gao,Siran Xianyu,Xiaotong Cui,Minjia Zhang*

Main category: cs.CV

TL;DR: FlexGaussian是一种无需训练的3D高斯压缩方法，结合混合精度量化和属性判别剪枝，实现高压缩比（96.4%）且保持高质量渲染（PSNR下降<1 dB），适用于移动设备。


<details>
  <summary>Details</summary>
Motivation: 大规模3D模型的需求增长需要高效压缩以减少内存和计算成本，现有方法缺乏灵活性且需重新训练。

Method: FlexGaussian结合混合精度量化和属性判别剪枝，无需重新训练，适应不同压缩目标。

Result: FlexGaussian实现96.4%压缩比，PSNR下降<1 dB，速度比现有方法快1.7-2.1倍。

Conclusion: FlexGaussian是一种高效、灵活的3D高斯压缩方法，适用于资源受限设备。

Abstract: 3D Gaussian splatting has become a prominent technique for representing and rendering complex 3D scenes, due to its high fidelity and speed advantages. However, the growing demand for large-scale models calls for effective compression to reduce memory and computation costs, especially on mobile and edge devices with limited resources. Existing compression methods effectively reduce 3D Gaussian parameters but often require extensive retraining or fine-tuning, lacking flexibility under varying compression constraints.   In this paper, we introduce FlexGaussian, a flexible and cost-effective method that combines mixed-precision quantization with attribute-discriminative pruning for training-free 3D Gaussian compression. FlexGaussian eliminates the need for retraining and adapts easily to diverse compression targets. Evaluation results show that FlexGaussian achieves up to 96.4% compression while maintaining high rendering quality (<1 dB drop in PSNR), and is deployable on mobile devices. FlexGaussian delivers high compression ratios within seconds, being 1.7-2.1x faster than state-of-the-art training-free methods and 10-100x faster than training-involved approaches. The code is being prepared and will be released soon at: https://github.com/Supercomputing-System-AI-Lab/FlexGaussian

</details>


### [14] [Spatial-Temporal Graph Mamba for Music-Guided Dance Video Synthesis](https://arxiv.org/abs/2507.06689)
*Hao Tang,Ling Shao,Zhenyu Zhang,Luc Van Gool,Nicu Sebe*

Main category: cs.CV

TL;DR: 提出了一种名为STG-Mamba的新方法，用于音乐引导的舞蹈视频合成任务，通过音乐到骨架和骨架到视频的两步映射实现。


<details>
  <summary>Details</summary>
Motivation: 解决音乐到舞蹈视频的合成问题，捕捉空间和时间维度的依赖关系。

Method: 使用STGM块进行音乐到骨架的转换，并提出自监督正则化网络进行骨架到视频的转换。

Result: 在实验中表现优于现有方法。

Conclusion: STG-Mamba在音乐引导舞蹈视频合成任务中具有显著优势。

Abstract: We propose a novel spatial-temporal graph Mamba (STG-Mamba) for the music-guided dance video synthesis task, i.e., to translate the input music to a dance video. STG-Mamba consists of two translation mappings: music-to-skeleton translation and skeleton-to-video translation. In the music-to-skeleton translation, we introduce a novel spatial-temporal graph Mamba (STGM) block to effectively construct skeleton sequences from the input music, capturing dependencies between joints in both the spatial and temporal dimensions. For the skeleton-to-video translation, we propose a novel self-supervised regularization network to translate the generated skeletons, along with a conditional image, into a dance video. Lastly, we collect a new skeleton-to-video translation dataset from the Internet, containing 54,944 video clips. Extensive experiments demonstrate that STG-Mamba achieves significantly better results than existing methods.

</details>


### [15] [DIFFUMA: High-Fidelity Spatio-Temporal Video Prediction via Dual-Path Mamba and Diffusion Enhancement](https://arxiv.org/abs/2507.06738)
*Xinyu Xie,Weifeng Cao,Jun Shi,Yangyang Hu,Hui Liang,Wanyong Liang,Xiaoliang Qian*

Main category: cs.CV

TL;DR: 论文提出了首个半导体晶圆切割过程的公开数据集CHDL，并设计了一种双路径预测架构DIFFUMA，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决高精度工业场景（如半导体制造）中缺乏专用数据集的问题，推动复杂过程建模和预测的研究。

Method: 构建CHDL数据集，并提出DIFFUMA模型，结合Mamba模块和扩散模块，分别捕捉全局时序上下文和增强空间细节。

Result: DIFFUMA在CHDL数据集上表现优异，MSE降低39%，SSIM提升至0.988，且在自然现象数据集上也有良好泛化能力。

Conclusion: 论文不仅提出了新的SOTA模型，还为工业AI研究提供了宝贵的数据资源。

Abstract: Spatio-temporal video prediction plays a pivotal role in critical domains, ranging from weather forecasting to industrial automation. However, in high-precision industrial scenarios such as semiconductor manufacturing, the absence of specialized benchmark datasets severely hampers research on modeling and predicting complex processes. To address this challenge, we make a twofold contribution.First, we construct and release the Chip Dicing Lane Dataset (CHDL), the first public temporal image dataset dedicated to the semiconductor wafer dicing process. Captured via an industrial-grade vision system, CHDL provides a much-needed and challenging benchmark for high-fidelity process modeling, defect detection, and digital twin development.Second, we propose DIFFUMA, an innovative dual-path prediction architecture specifically designed for such fine-grained dynamics. The model captures global long-range temporal context through a parallel Mamba module, while simultaneously leveraging a diffusion module, guided by temporal features, to restore and enhance fine-grained spatial details, effectively combating feature degradation. Experiments demonstrate that on our CHDL benchmark, DIFFUMA significantly outperforms existing methods, reducing the Mean Squared Error (MSE) by 39% and improving the Structural Similarity (SSIM) from 0.926 to a near-perfect 0.988. This superior performance also generalizes to natural phenomena datasets. Our work not only delivers a new state-of-the-art (SOTA) model but, more importantly, provides the community with an invaluable data resource to drive future research in industrial AI.

</details>


### [16] [PromptTea: Let Prompts Tell TeaCache the Optimal Threshold](https://arxiv.org/abs/2507.06739)
*Zishen Huang,Chunyu Yang,Mengyuan Ren*

Main category: cs.CV

TL;DR: 论文提出了一种基于提示复杂度的自适应缓存方法（PCA缓存），通过动态调整重用阈值和优化输入输出关系建模，显著提升了视频生成的推理速度，同时保持高质量输出。


<details>
  <summary>Details</summary>
Motivation: 视频生成中的推理速度是主要瓶颈，固定频率的缓存机制在复杂场景中质量下降明显，手动调整阈值效率低且缺乏鲁棒性。

Method: 提出PCA缓存方法，根据输入提示估计场景复杂度动态调整重用阈值；改进TeaCache的输入输出关系建模；引入DynCFGCache动态选择重用CFG输出。

Result: 实验表明，该方法在Wan2.1模型上实现了2.79倍的加速，同时保持高视觉保真度。

Conclusion: PCA缓存和动态机制的结合显著提升了视频生成的效率和质量，适用于复杂场景。

Abstract: Despite recent progress in video generation, inference speed remains a major bottleneck. A common acceleration strategy involves reusing model outputs via caching mechanisms at fixed intervals. However, we find that such fixed-frequency reuse significantly degrades quality in complex scenes, while manually tuning reuse thresholds is inefficient and lacks robustness. To address this, we propose Prompt-Complexity-Aware (PCA) caching, a method that automatically adjusts reuse thresholds based on scene complexity estimated directly from the input prompt. By incorporating prompt-derived semantic cues, PCA enables more adaptive and informed reuse decisions than conventional caching methods. We also revisit the assumptions behind TeaCache and identify a key limitation: it suffers from poor input-output relationship modeling due to an oversimplified prior. To overcome this, we decouple the noisy input, enhance the contribution of meaningful textual information, and improve the model's predictive accuracy through multivariate polynomial feature expansion. To further reduce computational cost, we replace the static CFGCache with DynCFGCache, a dynamic mechanism that selectively reuses classifier-free guidance (CFG) outputs based on estimated output variations. This allows for more flexible reuse without compromising output quality. Extensive experiments demonstrate that our approach achieves significant acceleration-for example, 2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across a range of scenes.

</details>


### [17] [Democratizing High-Fidelity Co-Speech Gesture Video Generation](https://arxiv.org/abs/2507.06812)
*Xu Yang,Shaoli Huang,Shenbo Xie,Xuelin Chen,Yifei Liu,Changxing Ding*

Main category: cs.CV

TL;DR: 提出了一种轻量级框架，利用2D全身骨架作为辅助条件，通过扩散模型生成与音频同步的说话者视频，并发布了首个公开数据集CSG-405。


<details>
  <summary>Details</summary>
Motivation: 解决语音-视觉内容的一对多映射问题，以及缺乏大规模公开数据集和高计算需求带来的挑战。

Method: 使用2D骨架作为条件，结合细粒度音频片段和参考图像，通过扩散模型预测骨骼运动，再输入现有人体视频生成模型合成视频。

Result: 在视觉质量和同步性上优于现有方法，并能泛化到不同说话者和场景。

Conclusion: 提出的框架和数据集为语音-手势视频生成研究提供了高效且可扩展的解决方案。

Abstract: Co-speech gesture video generation aims to synthesize realistic, audio-aligned videos of speakers, complete with synchronized facial expressions and body gestures. This task presents challenges due to the significant one-to-many mapping between audio and visual content, further complicated by the scarcity of large-scale public datasets and high computational demands. We propose a lightweight framework that utilizes 2D full-body skeletons as an efficient auxiliary condition to bridge audio signals with visual outputs. Our approach introduces a diffusion model conditioned on fine-grained audio segments and a skeleton extracted from the speaker's reference image, predicting skeletal motions through skeleton-audio feature fusion to ensure strict audio coordination and body shape consistency. The generated skeletons are then fed into an off-the-shelf human video generation model with the speaker's reference image to synthesize high-fidelity videos. To democratize research, we present CSG-405-the first public dataset with 405 hours of high-resolution videos across 71 speech types, annotated with 2D skeletons and diverse speaker demographics. Experiments show that our method exceeds state-of-the-art approaches in visual quality and synchronization while generalizing across speakers and contexts.

</details>


### [18] [HVI-CIDNet+: Beyond Extreme Darkness for Low-Light Image Enhancement](https://arxiv.org/abs/2507.06814)
*Qingsen Yan,Kangbiao Shi,Yixu Feng,Tao Hu,Peng Wu,Guansong Pang,Yanning Zhang*

Main category: cs.CV

TL;DR: 提出了一种新的HVI颜色空间和HVI-CIDNet+网络，用于低光图像增强，解决了颜色偏差和亮度伪影问题，并在10个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于sRGB和HSV颜色空间的低光图像增强方法存在颜色偏差和亮度伪影问题，尤其是红色和黑色噪声伪影。

Method: 提出HVI颜色空间，结合HV颜色图和可学习强度；设计HVI-CIDNet+网络，利用预训练视觉语言模型提取上下文知识，并通过Prior-guided Attention Block（PAB）和Region Refinement Block实现内容恢复和颜色校正。

Result: 在10个基准数据集上，HVI-CIDNet+优于现有最先进方法。

Conclusion: HVI颜色空间和HVI-CIDNet+网络有效解决了低光图像增强中的颜色和亮度问题，显著提升了性能。

Abstract: Low-Light Image Enhancement (LLIE) aims to restore vivid content and details from corrupted low-light images. However, existing standard RGB (sRGB) color space-based LLIE methods often produce color bias and brightness artifacts due to the inherent high color sensitivity. While Hue, Saturation, and Value (HSV) color space can decouple brightness and color, it introduces significant red and black noise artifacts. To address this problem, we propose a new color space for LLIE, namely Horizontal/Vertical-Intensity (HVI), defined by the HV color map and learnable intensity. The HV color map enforces small distances for the red coordinates to remove red noise artifacts, while the learnable intensity compresses the low-light regions to remove black noise artifacts. Additionally, we introduce the Color and Intensity Decoupling Network+ (HVI-CIDNet+), built upon the HVI color space, to restore damaged content and mitigate color distortion in extremely dark regions. Specifically, HVI-CIDNet+ leverages abundant contextual and degraded knowledge extracted from low-light images using pre-trained vision-language models, integrated via a novel Prior-guided Attention Block (PAB). Within the PAB, latent semantic priors can promote content restoration, while degraded representations guide precise color correction, both particularly in extremely dark regions through the meticulously designed cross-attention fusion mechanism. Furthermore, we construct a Region Refinement Block that employs convolution for information-rich regions and self-attention for information-scarce regions, ensuring accurate brightness adjustments. Comprehensive results from benchmark experiments demonstrate that the proposed HVI-CIDNet+ outperforms the state-of-the-art methods on 10 datasets.

</details>


### [19] [Physics-Grounded Motion Forecasting via Equation Discovery for Trajectory-Guided Image-to-Video Generation](https://arxiv.org/abs/2507.06830)
*Tao Feng,Xianbing Zhao,Zhenhua Chen,Tien Tsin Wong,Hamid Rezatofighi,Gholamreza Haffari,Lizhen Qu*

Main category: cs.CV

TL;DR: 论文提出了一种结合符号回归和轨迹引导的视频生成框架，以解决现有模型物理对齐不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有扩散和自回归视频生成模型虽视觉逼真，但缺乏物理准确性，无法模拟真实世界动态。

Method: 通过提取运动轨迹、符号回归和检索预训练机制，发现运动方程以生成物理准确的未来轨迹，并引导视频生成。

Result: 在经典力学场景（如弹簧-质量、摆锤和抛体运动）中成功恢复真实运动方程，并提升生成视频的物理对齐性。

Conclusion: 该框架无需微调现有模型即可显著提升视频生成的物理准确性。

Abstract: Recent advances in diffusion-based and autoregressive video generation models have achieved remarkable visual realism. However, these models typically lack accurate physical alignment, failing to replicate real-world dynamics in object motion. This limitation arises primarily from their reliance on learned statistical correlations rather than capturing mechanisms adhering to physical laws. To address this issue, we introduce a novel framework that integrates symbolic regression (SR) and trajectory-guided image-to-video (I2V) models for physics-grounded video forecasting. Our approach extracts motion trajectories from input videos, uses a retrieval-based pre-training mechanism to enhance symbolic regression, and discovers equations of motion to forecast physically accurate future trajectories. These trajectories then guide video generation without requiring fine-tuning of existing models. Evaluated on scenarios in Classical Mechanics, including spring-mass, pendulums, and projectile motions, our method successfully recovers ground-truth analytical equations and improves the physical alignment of generated videos over baseline methods.

</details>


### [20] [Hallucinating 360°: Panoramic Street-View Generation via Local Scenes Diffusion and Probabilistic Prompting](https://arxiv.org/abs/2507.06971)
*Fei Teng,Kai Luo,Sheng Wu,Siyu Li,Pujun Guo,Jiale Wei,Kunyu Peng,Jiaming Zhang,Kailun Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Percep360的全景生成方法，用于自动驾驶，通过局部场景扩散方法和概率提示方法实现高质量、可控的全景数据生成。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要全景感知，但现有数据采集和标注过程复杂且耗时，且现有生成模型无法实现高质量可控的全景生成。

Method: 提出局部场景扩散方法（LSDM）解决信息丢失问题，并提出概率提示方法（PPM）实现可控生成。

Result: 生成图像在无参考质量指标上优于原始拼接图像，并提升了下游感知模型的性能。

Conclusion: Percep360为自动驾驶提供了一种高效、可控的全景数据生成方法，具有实际应用潜力。

Abstract: Panoramic perception holds significant potential for autonomous driving, enabling vehicles to acquire a comprehensive 360{\deg} surround view in a single shot. However, autonomous driving is a data-driven task. Complete panoramic data acquisition requires complex sampling systems and annotation pipelines, which are time-consuming and labor-intensive. Although existing street view generation models have demonstrated strong data regeneration capabilities, they can only learn from the fixed data distribution of existing datasets and cannot achieve high-quality, controllable panoramic generation. In this paper, we propose the first panoramic generation method Percep360 for autonomous driving. Percep360 enables coherent generation of panoramic data with control signals based on the stitched panoramic data. Percep360 focuses on two key aspects: coherence and controllability. Specifically, to overcome the inherent information loss caused by the pinhole sampling process, we propose the Local Scenes Diffusion Method (LSDM). LSDM reformulates the panorama generation as a spatially continuous diffusion process, bridging the gaps between different data distributions. Additionally, to achieve the controllable generation of panoramic images, we propose a Probabilistic Prompting Method (PPM). PPM dynamically selects the most relevant control cues, enabling controllable panoramic image generation. We evaluate the effectiveness of the generated images from three perspectives: image quality assessment (i.e., no-reference and with reference), controllability, and their utility in real-world Bird's Eye View (BEV) segmentation. Notably, the generated data consistently outperforms the original stitched images in no-reference quality metrics and enhances downstream perception models. The source code will be publicly available at https://github.com/Bryant-Teng/Percep360.

</details>


### [21] [DenoiseCP-Net: Efficient Collective Perception in Adverse Weather via Joint LiDAR-Based 3D Object Detection and Denoising](https://arxiv.org/abs/2507.06976)
*Sven Teufel,Dominique Mayer,Jörg Gamerdinger,Oliver Bringmann*

Main category: cs.CV

TL;DR: 论文提出了一种名为DenoiseCP-Net的多任务架构，用于恶劣天气下基于LiDAR的集体感知，通过去噪和对象检测的集成，显著降低了通信开销和计算延迟。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆的感知系统在恶劣天气和遮挡环境下容易失效，集体感知虽能弥补这一缺陷，但恶劣天气下的研究尚不充分。

Method: 提出DenoiseCP-Net，集成体素级去噪和对象检测，采用稀疏卷积骨干网络，避免冗余计算。

Result: 在模拟的雨雪雾天气中，DenoiseCP-Net实现了近乎完美的去噪效果，带宽需求降低23.6%，同时保持检测精度和减少延迟。

Conclusion: DenoiseCP-Net为恶劣天气下的集体感知提供了一种高效解决方案，显著提升了系统性能。

Abstract: While automated vehicles hold the potential to significantly reduce traffic accidents, their perception systems remain vulnerable to sensor degradation caused by adverse weather and environmental occlusions. Collective perception, which enables vehicles to share information, offers a promising approach to overcoming these limitations. However, to this date collective perception in adverse weather is mostly unstudied. Therefore, we conduct the first study of LiDAR-based collective perception under diverse weather conditions and present a novel multi-task architecture for LiDAR-based collective perception under adverse weather. Adverse weather conditions can not only degrade perception capabilities, but also negatively affect bandwidth requirements and latency due to the introduced noise that is also transmitted and processed. Denoising prior to communication can effectively mitigate these issues. Therefore, we propose DenoiseCP-Net, a novel multi-task architecture for LiDAR-based collective perception under adverse weather conditions. DenoiseCP-Net integrates voxel-level noise filtering and object detection into a unified sparse convolution backbone, eliminating redundant computations associated with two-stage pipelines. This design not only reduces inference latency and computational cost but also minimizes communication overhead by removing non-informative noise. We extended the well-known OPV2V dataset by simulating rain, snow, and fog using our realistic weather simulation models. We demonstrate that DenoiseCP-Net achieves near-perfect denoising accuracy in adverse weather, reduces the bandwidth requirements by up to 23.6% while maintaining the same detection accuracy and reducing the inference latency for cooperative vehicles.

</details>


### [22] [Reading a Ruler in the Wild](https://arxiv.org/abs/2507.07077)
*Yimu Pan,Manas Mehta,Gwen Sincerbeaux,Jeffery A. Goldstein,Alison D. Gernand,James Z. Wang*

Main category: cs.CV

TL;DR: RulerNet是一个深度学习框架，通过将标尺读数统一为关键点检测问题，并利用几何级数参数表示标尺，实现了在复杂环境中准确估计真实世界尺寸。


<details>
  <summary>Details</summary>
Motivation: 解决计算机视觉中将像素测量转换为真实世界尺寸的挑战，推动生物医学、法医学、营养分析和电子商务等关键应用的发展。

Method: 提出RulerNet框架，将标尺读数重新定义为关键点检测问题，使用几何级数参数表示标尺，并采用抗畸变标注和训练策略。还提出合成数据管道和轻量级网络DeepGP。

Result: RulerNet在多样化标尺类型和成像条件下表现出色，实现了准确、一致且高效的尺寸估计。

Conclusion: RulerNet作为一种通用测量工具，具有广泛的应用潜力，并可与其他视觉组件集成，实现自动化、尺度感知的分析。

Abstract: Accurately converting pixel measurements into absolute real-world dimensions remains a fundamental challenge in computer vision and limits progress in key applications such as biomedicine, forensics, nutritional analysis, and e-commerce. We introduce RulerNet, a deep learning framework that robustly infers scale "in the wild" by reformulating ruler reading as a unified keypoint-detection problem and by representing the ruler with geometric-progression parameters that are invariant to perspective transformations. Unlike traditional methods that rely on handcrafted thresholds or rigid, ruler-specific pipelines, RulerNet directly localizes centimeter marks using a distortion-invariant annotation and training strategy, enabling strong generalization across diverse ruler types and imaging conditions while mitigating data scarcity. We also present a scalable synthetic-data pipeline that combines graphics-based ruler generation with ControlNet to add photorealistic context, greatly increasing training diversity and improving performance. To further enhance robustness and efficiency, we propose DeepGP, a lightweight feed-forward network that regresses geometric-progression parameters from noisy marks and eliminates iterative optimization, enabling real-time scale estimation on mobile or edge devices. Experiments show that RulerNet delivers accurate, consistent, and efficient scale estimates under challenging real-world conditions. These results underscore its utility as a generalizable measurement tool and its potential for integration with other vision components for automated, scale-aware analysis in high-impact domains. A live demo is available at https://huggingface.co/spaces/ymp5078/RulerNet-Demo.

</details>


### [23] [Evaluating Attribute Confusion in Fashion Text-to-Image Generation](https://arxiv.org/abs/2507.07079)
*Ziyue Liu,Federico Girella,Yiming Wang,Davide Talon*

Main category: cs.CV

TL;DR: 论文提出了一种新的自动评估方法L-VQAScore，用于解决T2I生成模型在复杂组合生成（如时尚领域）中评估的局限性，特别是在实体-属性语义关联方面。


<details>
  <summary>Details</summary>
Motivation: 现有的T2I评估方法在评估复杂实体-属性语义时存在局限性，尤其是属性混淆问题（属性正确但关联到错误实体）。

Method: 基于视觉问答（VQA）定位策略，提出L-VQAScore，结合视觉定位和VQA探测，评估正确和错误定位的属性生成。

Result: 在新数据集上，L-VQAScore在与人评估的相关性上优于现有方法，能有效捕捉细粒度实体-属性关联。

Conclusion: L-VQAScore是一种可靠且可扩展的替代主观评估的方法。

Abstract: Despite the rapid advances in Text-to-Image (T2I) generation models, their evaluation remains challenging in domains like fashion, involving complex compositional generation. Recent automated T2I evaluation methods leverage pre-trained vision-language models to measure cross-modal alignment. However, our preliminary study reveals that they are still limited in assessing rich entity-attribute semantics, facing challenges in attribute confusion, i.e., when attributes are correctly depicted but associated to the wrong entities. To address this, we build on a Visual Question Answering (VQA) localization strategy targeting one single entity at a time across both visual and textual modalities. We propose a localized human evaluation protocol and introduce a novel automatic metric, Localized VQAScore (L-VQAScore), that combines visual localization with VQA probing both correct (reflection) and miss-localized (leakage) attribute generation. On a newly curated dataset featuring challenging compositional alignment scenarios, L-VQAScore outperforms state-of-the-art T2I evaluation methods in terms of correlation with human judgments, demonstrating its strength in capturing fine-grained entity-attribute associations. We believe L-VQAScore can be a reliable and scalable alternative to subjective evaluations.

</details>


### [24] [Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation from Diffusion Models](https://arxiv.org/abs/2507.07104)
*Tiezheng Zhang,Yitong Li,Yu-cheng Chou,Jieneng Chen,Alan Yuille,Chen Wei,Junfei Xiao*

Main category: cs.CV

TL;DR: 提出了一种名为VLV的自动编码器框架，通过利用预训练组件（视觉编码器、T2I扩散模型解码器和LLM）显著降低了训练成本，同时实现了与GPT-4o等领先模型相当的图像描述能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统视觉语言模型需要大量高质量图像-文本对和昂贵计算资源的问题。

Method: 通过冻结预训练的T2I扩散解码器建立信息瓶颈，利用连续嵌入从扩散模型中提取知识，并微调LLM生成详细描述。

Result: 构建了一个成本效益极高的图像描述模型，训练成本低于1000美元，性能媲美GPT-4o等领先模型。

Conclusion: VLV框架通过巧妙利用预训练模型，显著降低了数据需求和计算成本，同时保持了高性能。

Abstract: Building state-of-the-art Vision-Language Models (VLMs) with strong captioning capabilities typically necessitates training on billions of high-quality image-text pairs, requiring millions of GPU hours. This paper introduces the Vision-Language-Vision (VLV) auto-encoder framework, which strategically leverages key pretrained components: a vision encoder, the decoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large Language Model (LLM). Specifically, we establish an information bottleneck by regularizing the language representation space, achieved through freezing the pretrained T2I diffusion decoder. Our VLV pipeline effectively distills knowledge from the text-conditioned diffusion model using continuous embeddings, demonstrating comprehensive semantic understanding via high-quality reconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the intermediate language representations into detailed descriptions, we construct a state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o and Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and significantly reduces data requirements; by primarily utilizing single-modal images for training and maximizing the utility of existing pretrained models (image encoder, T2I diffusion model, and LLM), it circumvents the need for massive paired image-text datasets, keeping the total training expenditure under $1,000 USD.

</details>


### [25] [4KAgent: Agentic Any Image to 4K Super-Resolution](https://arxiv.org/abs/2507.07105)
*Yushen Zuo,Qi Zheng,Mingyang Wu,Xinrui Jiang,Renjie Li,Jian Wang,Yide Zhang,Gengchen Mai,Lihong V. Wang,James Zou,Xiaoyu Wang,Ming-Hsuan Yang,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 4KAgent是一个统一的代理超分辨率通用系统，可将任何图像提升至4K分辨率，甚至更高。它通过三个核心模块实现高质量图像恢复，并在多个领域和基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决低分辨率图像恢复问题，尤其是严重退化的输入，通过代理范式推动低层次视觉任务的创新。

Method: 系统包含三个模块：Profiling（定制化流程）、Perception Agent（分析输入并制定恢复计划）、Restoration Agent（执行计划并优化输出）。

Result: 在11个任务类别和26个基准测试中表现优异，涵盖自然图像、肖像照片、AI生成内容等多个领域。

Conclusion: 4KAgent为低层次视觉任务提供了新的代理范式，推动了视觉自主代理的研究和创新。

Abstract: We present 4KAgent, a unified agentic super-resolution generalist system designed to universally upscale any image to 4K resolution (and even higher, if applied iteratively). Our system can transform images from extremely low resolutions with severe degradations, for example, highly distorted inputs at 256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three core components: (1) Profiling, a module that customizes the 4KAgent pipeline based on bespoke use cases; (2) A Perception Agent, which leverages vision-language models alongside image quality assessment experts to analyze the input image and make a tailored restoration plan; and (3) A Restoration Agent, which executes the plan, following a recursive execution-reflection paradigm, guided by a quality-driven mixture-of-expert policy to select the optimal output for each step. Additionally, 4KAgent embeds a specialized face restoration pipeline, significantly enhancing facial details in portrait and selfie photos. We rigorously evaluate our 4KAgent across 11 distinct task categories encompassing a total of 26 diverse benchmarks, setting new state-of-the-art on a broad spectrum of imaging domains. Our evaluations cover natural images, portrait photos, AI-generated content, satellite imagery, fluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and X-ray, demonstrating superior performance in terms of both perceptual (e.g., NIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic paradigm for low-level vision tasks, we aim to catalyze broader interest and innovation within vision-centric autonomous agents across diverse research communities. We will release all the code, models, and results at: https://4kagent.github.io.

</details>


### [26] [Towards Multimodal Understanding via Stable Diffusion as a Task-Aware Feature Extractor](https://arxiv.org/abs/2507.07106)
*Vatsal Agarwal,Matthew Gwilliam,Gefen Kohavi,Eshan Verma,Daniel Ulbricht,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: 本文探讨了预训练的文本到图像扩散模型是否可以作为指令感知的视觉编码器，以解决CLIP在捕捉细粒度细节上的不足。研究发现扩散特征语义丰富且能编码强图像-文本对齐，并提出了一种融合策略来结合CLIP和扩散特征。


<details>
  <summary>Details</summary>
Motivation: 解决CLIP视觉编码器在捕捉细粒度细节上的局限性，探索扩散模型在视觉理解中的潜力。

Method: 分析扩散模型的内部表示，利用文本条件聚焦相关区域，研究如何与大型语言模型对齐，并提出泄漏现象的缓解策略。

Result: 在通用VQA和专用MLLM基准测试中验证了扩散模型在视觉理解中的优势，特别是在需要空间和组合推理的任务中。

Conclusion: 扩散模型在视觉理解中表现出色，尤其在需要细粒度分析的场景中，融合CLIP和扩散特征是一种有效策略。

Abstract: Recent advances in multimodal large language models (MLLMs) have enabled image-based question-answering capabilities. However, a key limitation is the use of CLIP as the visual encoder; while it can capture coarse global information, it often can miss fine-grained details that are relevant to the input query. To address these shortcomings, this work studies whether pre-trained text-to-image diffusion models can serve as instruction-aware visual encoders. Through an analysis of their internal representations, we find diffusion features are both rich in semantics and can encode strong image-text alignment. Moreover, we find that we can leverage text conditioning to focus the model on regions relevant to the input question. We then investigate how to align these features with large language models and uncover a leakage phenomenon, where the LLM can inadvertently recover information from the original diffusion prompt. We analyze the causes of this leakage and propose a mitigation strategy. Based on these insights, we explore a simple fusion strategy that utilizes both CLIP and conditional diffusion features. We evaluate our approach on both general VQA and specialized MLLM benchmarks, demonstrating the promise of diffusion models for visual understanding, particularly in vision-centric tasks that require spatial and compositional reasoning. Our project page can be found https://vatsalag99.github.io/mustafar/.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [27] [Speckle2Self: Self-Supervised Ultrasound Speckle Reduction Without Clean Data](https://arxiv.org/abs/2507.06828)
*Xuesong Li,Nassir Navab,Zhongliang Jiang*

Main category: eess.IV

TL;DR: Speckle2Self是一种自监督算法，用于仅使用单次噪声观测减少医学超声图像中的斑点噪声。


<details>
  <summary>Details</summary>
Motivation: 医学超声图像中的斑点噪声具有组织依赖性，传统去噪方法无法直接应用，因此需要一种新的自监督方法。

Method: 通过多尺度扰动（MSP）操作引入组织依赖的斑点噪声变化，同时保留共享的解剖结构，将干净图像建模为低秩信号并分离稀疏噪声。

Result: Speckle2Self在模拟和真实超声图像上表现优于传统滤波器和最先进的学习方法，并展示了跨设备的泛化能力。

Conclusion: Speckle2Self为医学超声图像去噪提供了一种有效的自监督解决方案，具有广泛的应用潜力。

Abstract: Image denoising is a fundamental task in computer vision, particularly in medical ultrasound (US) imaging, where speckle noise significantly degrades image quality. Although recent advancements in deep neural networks have led to substantial improvements in denoising for natural images, these methods cannot be directly applied to US speckle noise, as it is not purely random. Instead, US speckle arises from complex wave interference within the body microstructure, making it tissue-dependent. This dependency means that obtaining two independent noisy observations of the same scene, as required by pioneering Noise2Noise, is not feasible. Additionally, blind-spot networks also cannot handle US speckle noise due to its high spatial dependency. To address this challenge, we introduce Speckle2Self, a novel self-supervised algorithm for speckle reduction using only single noisy observations. The key insight is that applying a multi-scale perturbation (MSP) operation introduces tissue-dependent variations in the speckle pattern across different scales, while preserving the shared anatomical structure. This enables effective speckle suppression by modeling the clean image as a low-rank signal and isolating the sparse noise component. To demonstrate its effectiveness, Speckle2Self is comprehensively compared with conventional filter-based denoising algorithms and SOTA learning-based methods, using both realistic simulated US images and human carotid US images. Additionally, data from multiple US machines are employed to evaluate model generalization and adaptability to images from unseen domains. \textit{Code and datasets will be released upon acceptance.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [28] [Denoising Multi-Beta VAE: Representation Learning for Disentanglement and Generation](https://arxiv.org/abs/2507.06613)
*Anshuk Uppal,Yuhta Takida,Chieh-Hsin Lai,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 论文提出了一种新的生成模型框架，通过使用不同的β值学习多个潜在表示，解决了生成质量和解耦表示之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统的β-VAE框架在解耦表示和生成质量之间存在权衡，高β值偏向解耦但牺牲重建质量。本文旨在解决这一问题。

Method: 提出了一种新的损失函数，训练单个VAE以学习多个潜在表示，并结合非线性扩散模型平滑过渡不同β值对应的表示。

Result: 模型在解耦和生成质量上均表现良好，支持无输入图像的样本生成，并实现了潜在空间的平滑过渡。

Conclusion: 该框架成功平衡了解耦表示和生成质量，同时支持灵活的潜在空间操作。

Abstract: Disentangled and interpretable latent representations in generative models typically come at the cost of generation quality. The $\beta$-VAE framework introduces a hyperparameter $\beta$ to balance disentanglement and reconstruction quality, where setting $\beta > 1$ introduces an information bottleneck that favors disentanglement over sharp, accurate reconstructions. To address this trade-off, we propose a novel generative modeling framework that leverages a range of $\beta$ values to learn multiple corresponding latent representations. First, we obtain a slew of representations by training a single variational autoencoder (VAE), with a new loss function that controls the information retained in each latent representation such that the higher $\beta$ value prioritize disentanglement over reconstruction fidelity. We then, introduce a non-linear diffusion model that smoothly transitions latent representations corresponding to different $\beta$ values. This model denoises towards less disentangled and more informative representations, ultimately leading to (almost) lossless representations, enabling sharp reconstructions. Furthermore, our model supports sample generation without input images, functioning as a standalone generative model. We evaluate our framework in terms of both disentanglement and generation quality. Additionally, we observe smooth transitions in the latent spaces with respect to changes in $\beta$, facilitating consistent manipulation of generated outputs.

</details>
