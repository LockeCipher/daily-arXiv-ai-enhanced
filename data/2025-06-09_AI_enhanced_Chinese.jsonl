{"id": "2506.05358", "pdf": "https://arxiv.org/pdf/2506.05358", "abs": "https://arxiv.org/abs/2506.05358", "authors": ["Souradip Nath"], "title": "Can ChatGPT Perform Image Splicing Detection? A Preliminary Study", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) like GPT-4V are capable of reasoning\nacross text and image modalities, showing promise in a variety of complex\nvision-language tasks. In this preliminary study, we investigate the\nout-of-the-box capabilities of GPT-4V in the domain of image forensics,\nspecifically, in detecting image splicing manipulations. Without any\ntask-specific fine-tuning, we evaluate GPT-4V using three prompting strategies:\nZero-Shot (ZS), Few-Shot (FS), and Chain-of-Thought (CoT), applied over a\ncurated subset of the CASIA v2.0 splicing dataset.\n  Our results show that GPT-4V achieves competitive detection performance in\nzero-shot settings (more than 85% accuracy), with CoT prompting yielding the\nmost balanced trade-off across authentic and spliced images. Qualitative\nanalysis further reveals that the model not only detects low-level visual\nartifacts but also draws upon real-world contextual knowledge such as object\nscale, semantic consistency, and architectural facts, to identify implausible\ncomposites. While GPT-4V lags behind specialized state-of-the-art splicing\ndetection models, its generalizability, interpretability, and encyclopedic\nreasoning highlight its potential as a flexible tool in image forensics.", "AI": {"tldr": "GPT-4V\u5728\u56fe\u50cf\u53d6\u8bc1\u9886\u57df\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u5c24\u5176\u5728\u68c0\u6d4b\u56fe\u50cf\u62fc\u63a5\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u8d85\u8fc785%\u3002", "motivation": "\u7814\u7a76GPT-4V\u5728\u56fe\u50cf\u53d6\u8bc1\u4e2d\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u63a2\u7d22\u5176\u5728\u68c0\u6d4b\u56fe\u50cf\u62fc\u63a5\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u63d0\u793a\u7b56\u7565\uff08\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u601d\u7ef4\u94fe\uff09\u5728CASIA v2.0\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30GPT-4V\u3002", "result": "GPT-4V\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u601d\u7ef4\u94fe\u63d0\u793a\u5728\u771f\u5b9e\u548c\u62fc\u63a5\u56fe\u50cf\u95f4\u53d6\u5f97\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "GPT-4V\u867d\u4e0d\u53ca\u4e13\u7528\u6a21\u578b\uff0c\u4f46\u5176\u901a\u7528\u6027\u548c\u63a8\u7406\u80fd\u529b\u4f7f\u5176\u6210\u4e3a\u56fe\u50cf\u53d6\u8bc1\u4e2d\u7684\u7075\u6d3b\u5de5\u5177\u3002"}}
{"id": "2506.05360", "pdf": "https://arxiv.org/pdf/2506.05360", "abs": "https://arxiv.org/abs/2506.05360", "authors": ["Taminul Islam", "Toqi Tahamid Sarker", "Mohamed G Embaby", "Khaled R Ahmed", "Amer AbuGhazaleh"], "title": "CarboNeXT and CarboFormer: Dual Semantic Segmentation Architectures for Detecting and Quantifying Carbon Dioxide Emissions Using Optical Gas Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Carbon dioxide (CO$_2$) emissions are critical indicators of both\nenvironmental impact and various industrial processes, including livestock\nmanagement. We introduce CarboNeXT, a semantic segmentation framework for\nOptical Gas Imaging (OGI), designed to detect and quantify CO$_2$ emissions\nacross diverse applications. Our approach integrates a multi-scale context\naggregation network with UPerHead and auxiliary FCN components to effectively\nmodel both local details and global relationships in gas plume imagery. We\ncontribute two novel datasets: (1) the Controlled Carbon Dioxide Release (CCR)\ndataset, which simulates gas leaks with systematically varied flow rates\n(10-100 SCCM), and (2) the Real Time Ankom (RTA) dataset, focusing on emissions\nfrom dairy cow rumen fluid in vitro experiments. Extensive evaluations\ndemonstrate that CarboNeXT outperforms state-of-the-art methods, achieving\n88.46% mIoU on CCR and 92.95% mIoU on RTA, with particular effectiveness in\nchallenging low-flow scenarios. The model operates at 60.95 FPS, enabling\nreal-time monitoring applications. Additionally, we propose CarboFormer, a\nlightweight variant with only 5.07M parameters that achieves 84.68 FPS, with\ncompetitive performance of 84.88% mIoU on CCR and 92.98% on RTA, making it\nsuitable for resource-constrained platforms such as programmable drones. Our\nwork advances both environmental sensing and precision livestock management by\nproviding robust tools for CO$_2$ emission analysis, with a specific focus on\nlivestock applications.", "AI": {"tldr": "CarboNeXT\u662f\u4e00\u4e2a\u7528\u4e8e\u5149\u5b66\u6c14\u4f53\u6210\u50cf\u7684\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u91cf\u5316CO\u2082\u6392\u653e\uff0c\u5728\u4f4e\u6d41\u91cf\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u652f\u6301\u5b9e\u65f6\u76d1\u6d4b\u3002", "motivation": "CO\u2082\u6392\u653e\u662f\u73af\u5883\u548c\u5de5\u4e1a\u8fc7\u7a0b\u7684\u91cd\u8981\u6307\u6807\uff0c\u7279\u522b\u662f\u5728\u755c\u7267\u4e1a\u7ba1\u7406\u4e2d\uff0c\u9700\u8981\u9ad8\u6548\u7684\u5de5\u5177\u8fdb\u884c\u76d1\u6d4b\u548c\u5206\u6790\u3002", "method": "\u7ed3\u5408\u591a\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u805a\u5408\u7f51\u7edc\u3001UPerHead\u548c\u8f85\u52a9FCN\u7ec4\u4ef6\uff0c\u63d0\u51faCarboNeXT\u6846\u67b6\uff0c\u5e76\u8d21\u732e\u4e86\u4e24\u4e2a\u65b0\u6570\u636e\u96c6\uff08CCR\u548cRTA\uff09\u3002", "result": "CarboNeXT\u5728CCR\u548cRTA\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523088.46%\u548c92.95%\u7684mIoU\uff0c\u5b9e\u65f6\u6027\u80fd\u4e3a60.95 FPS\uff1b\u8f7b\u91cf\u7248CarboFormer\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aCO\u2082\u6392\u653e\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\uff0c\u7279\u522b\u9002\u7528\u4e8e\u755c\u7267\u4e1a\u548c\u73af\u5883\u76d1\u6d4b\uff0c\u63a8\u52a8\u4e86\u7cbe\u51c6\u755c\u7267\u4e1a\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.05361", "pdf": "https://arxiv.org/pdf/2506.05361", "abs": "https://arxiv.org/abs/2506.05361", "authors": ["Tinglin Huang", "Tianyu Liu", "Mehrtash Babadi", "Wengong Jin", "Rex Ying"], "title": "Scalable Generation of Spatial Transcriptomics from Histology Images via Whole-Slide Flow Matching", "categories": ["cs.CV", "q-bio.GN"], "comment": "Accepted at ICML 2025", "summary": "Spatial transcriptomics (ST) has emerged as a powerful technology for\nbridging histology imaging with gene expression profiling. However, its\napplication has been limited by low throughput and the need for specialized\nexperimental facilities. Prior works sought to predict ST from whole-slide\nhistology images to accelerate this process, but they suffer from two major\nlimitations. First, they do not explicitly model cell-cell interaction as they\nfactorize the joint distribution of whole-slide ST data and predict the gene\nexpression of each spot independently. Second, their encoders struggle with\nmemory constraints due to the large number of spots (often exceeding 10,000) in\ntypical ST datasets. Herein, we propose STFlow, a flow matching generative\nmodel that considers cell-cell interaction by modeling the joint distribution\nof gene expression of an entire slide. It also employs an efficient slide-level\nencoder with local spatial attention, enabling whole-slide processing without\nexcessive memory overhead. On the recently curated HEST-1k and STImage-1K4M\nbenchmarks, STFlow substantially outperforms state-of-the-art baselines and\nachieves over 18% relative improvements over the pathology foundation models.", "AI": {"tldr": "STFlow\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u5efa\u6a21\u6574\u4e2a\u5207\u7247\u7684\u57fa\u56e0\u8868\u8fbe\u8054\u5408\u5206\u5e03\u6765\u8003\u8651\u7ec6\u80de\u95f4\u76f8\u4e92\u4f5c\u7528\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5185\u5b58\u548c\u72ec\u7acb\u6027\u9884\u6d4b\u4e0a\u7684\u9650\u5236\u3002", "motivation": "\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\uff08ST\uff09\u6280\u672f\u867d\u5f3a\u5927\uff0c\u4f46\u53d7\u9650\u4e8e\u4f4e\u901a\u91cf\u548c\u5b9e\u9a8c\u8bbe\u65bd\u9700\u6c42\u3002\u73b0\u6709\u9884\u6d4b\u65b9\u6cd5\u56e0\u72ec\u7acb\u9884\u6d4b\u548c\u5185\u5b58\u95ee\u9898\u8868\u73b0\u4e0d\u4f73\u3002", "method": "STFlow\u91c7\u7528\u6d41\u5339\u914d\u751f\u6210\u6a21\u578b\uff0c\u5efa\u6a21\u6574\u4e2a\u5207\u7247\u7684\u57fa\u56e0\u8868\u8fbe\u8054\u5408\u5206\u5e03\uff0c\u5e76\u5f15\u5165\u9ad8\u6548\u7684\u5207\u7247\u7ea7\u7f16\u7801\u5668\u4e0e\u5c40\u90e8\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5728HEST-1k\u548cSTImage-1K4M\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSTFlow\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u76f8\u5bf9\u75c5\u7406\u57fa\u7840\u6a21\u578b\u63d0\u5347\u8d85\u8fc718%\u3002", "conclusion": "STFlow\u901a\u8fc7\u5efa\u6a21\u7ec6\u80de\u95f4\u76f8\u4e92\u4f5c\u7528\u548c\u4f18\u5316\u5185\u5b58\u4f7f\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u9884\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2506.05358", "pdf": "https://arxiv.org/pdf/2506.05358", "abs": "https://arxiv.org/abs/2506.05358", "authors": ["Souradip Nath"], "title": "Can ChatGPT Perform Image Splicing Detection? A Preliminary Study", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) like GPT-4V are capable of reasoning\nacross text and image modalities, showing promise in a variety of complex\nvision-language tasks. In this preliminary study, we investigate the\nout-of-the-box capabilities of GPT-4V in the domain of image forensics,\nspecifically, in detecting image splicing manipulations. Without any\ntask-specific fine-tuning, we evaluate GPT-4V using three prompting strategies:\nZero-Shot (ZS), Few-Shot (FS), and Chain-of-Thought (CoT), applied over a\ncurated subset of the CASIA v2.0 splicing dataset.\n  Our results show that GPT-4V achieves competitive detection performance in\nzero-shot settings (more than 85% accuracy), with CoT prompting yielding the\nmost balanced trade-off across authentic and spliced images. Qualitative\nanalysis further reveals that the model not only detects low-level visual\nartifacts but also draws upon real-world contextual knowledge such as object\nscale, semantic consistency, and architectural facts, to identify implausible\ncomposites. While GPT-4V lags behind specialized state-of-the-art splicing\ndetection models, its generalizability, interpretability, and encyclopedic\nreasoning highlight its potential as a flexible tool in image forensics.", "AI": {"tldr": "GPT-4V\u5728\u56fe\u50cf\u53d6\u8bc1\u9886\u57df\uff08\u7279\u522b\u662f\u56fe\u50cf\u62fc\u63a5\u68c0\u6d4b\uff09\u4e2d\u7684\u96f6\u6837\u672c\u80fd\u529b\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u8d85\u8fc785%\uff0c\u4e14\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u63d0\u793a\u7b56\u7565\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u7814\u7a76GPT-4V\u5728\u56fe\u50cf\u53d6\u8bc1\u4efb\u52a1\u4e2d\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u63a2\u7d22\u5176\u5728\u65e0\u9700\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u68c0\u6d4b\u56fe\u50cf\u62fc\u63a5\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u63d0\u793a\u7b56\u7565\uff08\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u3001\u94fe\u5f0f\u601d\u7ef4\uff09\u5728CASIA v2.0\u6570\u636e\u96c6\u7684\u5b50\u96c6\u4e0a\u8bc4\u4f30GPT-4V\u3002", "result": "GPT-4V\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4f18\u5f02\uff0cCoT\u63d0\u793a\u7b56\u7565\u5728\u771f\u5b9e\u4e0e\u62fc\u63a5\u56fe\u50cf\u95f4\u53d6\u5f97\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "\u5c3d\u7ba1\u4e0d\u53ca\u4e13\u7528\u6a21\u578b\uff0cGPT-4V\u7684\u901a\u7528\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u767e\u79d1\u5168\u4e66\u5f0f\u63a8\u7406\u80fd\u529b\u4f7f\u5176\u6210\u4e3a\u56fe\u50cf\u53d6\u8bc1\u4e2d\u7684\u7075\u6d3b\u5de5\u5177\u3002"}}
{"id": "2506.05360", "pdf": "https://arxiv.org/pdf/2506.05360", "abs": "https://arxiv.org/abs/2506.05360", "authors": ["Taminul Islam", "Toqi Tahamid Sarker", "Mohamed G Embaby", "Khaled R Ahmed", "Amer AbuGhazaleh"], "title": "CarboNeXT and CarboFormer: Dual Semantic Segmentation Architectures for Detecting and Quantifying Carbon Dioxide Emissions Using Optical Gas Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Carbon dioxide (CO$_2$) emissions are critical indicators of both\nenvironmental impact and various industrial processes, including livestock\nmanagement. We introduce CarboNeXT, a semantic segmentation framework for\nOptical Gas Imaging (OGI), designed to detect and quantify CO$_2$ emissions\nacross diverse applications. Our approach integrates a multi-scale context\naggregation network with UPerHead and auxiliary FCN components to effectively\nmodel both local details and global relationships in gas plume imagery. We\ncontribute two novel datasets: (1) the Controlled Carbon Dioxide Release (CCR)\ndataset, which simulates gas leaks with systematically varied flow rates\n(10-100 SCCM), and (2) the Real Time Ankom (RTA) dataset, focusing on emissions\nfrom dairy cow rumen fluid in vitro experiments. Extensive evaluations\ndemonstrate that CarboNeXT outperforms state-of-the-art methods, achieving\n88.46% mIoU on CCR and 92.95% mIoU on RTA, with particular effectiveness in\nchallenging low-flow scenarios. The model operates at 60.95 FPS, enabling\nreal-time monitoring applications. Additionally, we propose CarboFormer, a\nlightweight variant with only 5.07M parameters that achieves 84.68 FPS, with\ncompetitive performance of 84.88% mIoU on CCR and 92.98% on RTA, making it\nsuitable for resource-constrained platforms such as programmable drones. Our\nwork advances both environmental sensing and precision livestock management by\nproviding robust tools for CO$_2$ emission analysis, with a specific focus on\nlivestock applications.", "AI": {"tldr": "CarboNeXT\u662f\u4e00\u4e2a\u7528\u4e8e\u5149\u5b66\u6c14\u4f53\u6210\u50cf\uff08OGI\uff09\u7684\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u91cf\u5316CO$_2$\u6392\u653e\uff0c\u5728\u4f4e\u6d41\u91cf\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u652f\u6301\u5b9e\u65f6\u76d1\u6d4b\u3002", "motivation": "CO$_2$\u6392\u653e\u662f\u73af\u5883\u548c\u5de5\u4e1a\u8fc7\u7a0b\u7684\u91cd\u8981\u6307\u6807\uff0c\u5c24\u5176\u662f\u5728\u755c\u7267\u4e1a\u7ba1\u7406\u4e2d\u3002\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u548c\u91cf\u5316\u8fd9\u4e9b\u6392\u653e\u3002", "method": "\u7ed3\u5408\u591a\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u805a\u5408\u7f51\u7edc\u3001UPerHead\u548c\u8f85\u52a9FCN\u7ec4\u4ef6\uff0c\u6784\u5efaCarboNeXT\u6846\u67b6\uff0c\u5e76\u5f00\u53d1\u4e86\u4e24\u4e2a\u65b0\u6570\u636e\u96c6\uff08CCR\u548cRTA\uff09\u3002", "result": "CarboNeXT\u5728CCR\u548cRTA\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523088.46%\u548c92.95%\u7684mIoU\uff0c\u652f\u630160.95 FPS\u7684\u5b9e\u65f6\u76d1\u6d4b\u3002\u8f7b\u91cf\u7ea7\u53d8\u4f53CarboFormer\u8868\u73b0\u540c\u6837\u51fa\u8272\u3002", "conclusion": "CarboNeXT\u548cCarboFormer\u4e3aCO$_2$\u6392\u653e\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\uff0c\u7279\u522b\u9002\u7528\u4e8e\u755c\u7267\u4e1a\u548c\u73af\u5883\u76d1\u6d4b\u3002"}}
{"id": "2506.05361", "pdf": "https://arxiv.org/pdf/2506.05361", "abs": "https://arxiv.org/abs/2506.05361", "authors": ["Tinglin Huang", "Tianyu Liu", "Mehrtash Babadi", "Wengong Jin", "Rex Ying"], "title": "Scalable Generation of Spatial Transcriptomics from Histology Images via Whole-Slide Flow Matching", "categories": ["cs.CV", "q-bio.GN"], "comment": "Accepted at ICML 2025", "summary": "Spatial transcriptomics (ST) has emerged as a powerful technology for\nbridging histology imaging with gene expression profiling. However, its\napplication has been limited by low throughput and the need for specialized\nexperimental facilities. Prior works sought to predict ST from whole-slide\nhistology images to accelerate this process, but they suffer from two major\nlimitations. First, they do not explicitly model cell-cell interaction as they\nfactorize the joint distribution of whole-slide ST data and predict the gene\nexpression of each spot independently. Second, their encoders struggle with\nmemory constraints due to the large number of spots (often exceeding 10,000) in\ntypical ST datasets. Herein, we propose STFlow, a flow matching generative\nmodel that considers cell-cell interaction by modeling the joint distribution\nof gene expression of an entire slide. It also employs an efficient slide-level\nencoder with local spatial attention, enabling whole-slide processing without\nexcessive memory overhead. On the recently curated HEST-1k and STImage-1K4M\nbenchmarks, STFlow substantially outperforms state-of-the-art baselines and\nachieves over 18% relative improvements over the pathology foundation models.", "AI": {"tldr": "STFlow\u662f\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u5efa\u6a21\u6574\u4e2a\u5207\u7247\u7684\u57fa\u56e0\u8868\u8fbe\u8054\u5408\u5206\u5e03\u6765\u8003\u8651\u7ec6\u80de\u95f4\u76f8\u4e92\u4f5c\u7528\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5185\u5b58\u548c\u72ec\u7acb\u6027\u9884\u6d4b\u4e0a\u7684\u9650\u5236\u3002", "motivation": "\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\uff08ST\uff09\u6280\u672f\u7ed3\u5408\u7ec4\u7ec7\u5b66\u6210\u50cf\u548c\u57fa\u56e0\u8868\u8fbe\u5206\u6790\uff0c\u4f46\u53d7\u9650\u4e8e\u4f4e\u901a\u91cf\u548c\u5b9e\u9a8c\u8bbe\u65bd\u9700\u6c42\u3002\u73b0\u6709\u9884\u6d4b\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5efa\u6a21\u7ec6\u80de\u95f4\u76f8\u4e92\u4f5c\u7528\u4e14\u5185\u5b58\u6548\u7387\u4f4e\u3002", "method": "\u63d0\u51faSTFlow\uff0c\u91c7\u7528\u6d41\u5339\u914d\u751f\u6210\u6a21\u578b\u5efa\u6a21\u5207\u7247\u7ea7\u57fa\u56e0\u8868\u8fbe\u8054\u5408\u5206\u5e03\uff0c\u5e76\u7ed3\u5408\u5c40\u90e8\u7a7a\u95f4\u6ce8\u610f\u529b\u7684\u9ad8\u6548\u7f16\u7801\u5668\u3002", "result": "\u5728HEST-1k\u548cSTImage-1K4M\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSTFlow\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u76f8\u5bf9\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u63d0\u534718%\u3002", "conclusion": "STFlow\u901a\u8fc7\u5efa\u6a21\u7ec6\u80de\u95f4\u76f8\u4e92\u4f5c\u7528\u548c\u9ad8\u6548\u7f16\u7801\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u9884\u6d4b\u7684\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.05370", "pdf": "https://arxiv.org/pdf/2506.05370", "abs": "https://arxiv.org/abs/2506.05370", "authors": ["Kristy Wedel"], "title": "Contextual Memory Intelligence -- A Foundational Paradigm for Human-AI Collaboration and Reflective Generative AI Systems", "categories": ["cs.AI", "cs.ET"], "comment": "32 pages, 9 tables, 1 figure", "summary": "A critical challenge remains unresolved as generative AI systems are quickly\nimplemented in various organizational settings. Despite significant advances in\nmemory components such as RAG, vector stores, and LLM agents, these systems\nstill have substantial memory limitations. Gen AI workflows rarely store or\nreflect on the full context in which decisions are made. This leads to repeated\nerrors and a general lack of clarity. This paper introduces Contextual Memory\nIntelligence (CMI) as a new foundational paradigm for building intelligent\nsystems. It repositions memory as an adaptive infrastructure necessary for\nlongitudinal coherence, explainability, and responsible decision-making rather\nthan passive data. Drawing on cognitive science, organizational theory,\nhuman-computer interaction, and AI governance, CMI formalizes the structured\ncapture, inference, and regeneration of context as a fundamental system\ncapability. The Insight Layer is presented in this paper to operationalize this\nvision. This modular architecture uses human-in-the-loop reflection, drift\ndetection, and rationale preservation to incorporate contextual memory into\nsystems. The paper argues that CMI allows systems to reason with data, history,\njudgment, and changing context, thereby addressing a foundational blind spot in\ncurrent AI architectures and governance efforts. A framework for creating\nintelligent systems that are effective, reflective, auditable, and socially\nresponsible is presented through CMI. This enhances human-AI collaboration,\ngenerative AI design, and the resilience of the institutions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u667a\u80fd\uff08CMI\uff09\u7684\u65b0\u8303\u5f0f\uff0c\u65e8\u5728\u89e3\u51b3\u751f\u6210\u5f0fAI\u7cfb\u7edf\u4e2d\u7684\u8bb0\u5fc6\u9650\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6355\u6349\u548c\u63a8\u7406\u4e0a\u4e0b\u6587\uff0c\u63d0\u5347\u7cfb\u7edf\u7684\u957f\u671f\u4e00\u81f4\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u751f\u6210\u5f0fAI\u7cfb\u7edf\u5728\u7ec4\u7ec7\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u8bb0\u5fc6\u7ec4\u4ef6\uff08\u5982RAG\u3001\u5411\u91cf\u5b58\u50a8\u7b49\uff09\u4ecd\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5bfc\u81f4\u91cd\u590d\u9519\u8bef\u548c\u51b3\u7b56\u4e0d\u900f\u660e\u3002CMI\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "CMI\u5c06\u8bb0\u5fc6\u91cd\u65b0\u5b9a\u4f4d\u4e3a\u81ea\u9002\u5e94\u57fa\u7840\u8bbe\u65bd\uff0c\u7ed3\u5408\u8ba4\u77e5\u79d1\u5b66\u3001\u7ec4\u7ec7\u7406\u8bba\u548cAI\u6cbb\u7406\uff0c\u63d0\u51faInsight Layer\u67b6\u6784\uff0c\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u3001\u6f02\u79fb\u68c0\u6d4b\u548c\u7406\u7531\u4fdd\u5b58\u5b9e\u73b0\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u3002", "result": "CMI\u80fd\u591f\u4f7f\u7cfb\u7edf\u57fa\u4e8e\u6570\u636e\u3001\u5386\u53f2\u3001\u5224\u65ad\u548c\u52a8\u6001\u4e0a\u4e0b\u6587\u8fdb\u884c\u63a8\u7406\uff0c\u586b\u8865\u5f53\u524dAI\u67b6\u6784\u7684\u76f2\u70b9\uff0c\u63d0\u5347\u7cfb\u7edf\u6548\u80fd\u548c\u793e\u4f1a\u8d23\u4efb\u3002", "conclusion": "CMI\u4e3a\u6784\u5efa\u9ad8\u6548\u3001\u53ef\u5ba1\u8ba1\u4e14\u793e\u4f1a\u8d23\u4efb\u7684\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u589e\u5f3a\u4eba\u673a\u534f\u4f5c\u548c\u751f\u6210\u5f0fAI\u8bbe\u8ba1\u7684\u97e7\u6027\u3002"}}
{"id": "2506.05588", "pdf": "https://arxiv.org/pdf/2506.05588", "abs": "https://arxiv.org/abs/2506.05588", "authors": ["Rishona Daniels", "Duna Wattad", "Ronny Ronen", "David Saad", "Shahar Kvatinsky"], "title": "Preprocessing Methods for Memristive Reservoir Computing for Image Recognition", "categories": ["cs.NE", "cs.AR", "cs.ET"], "comment": "6 pages, 8 figures, submitted for review in IEEE MetroXRAINE 2025\n  conference", "summary": "Reservoir computing (RC) has attracted attention as an efficient recurrent\nneural network architecture due to its simplified training, requiring only its\nlast perceptron readout layer to be trained. When implemented with memristors,\nRC systems benefit from their dynamic properties, which make them ideal for\nreservoir construction. However, achieving high performance in memristor-based\nRC remains challenging, as it critically depends on the input preprocessing\nmethod and reservoir size. Despite growing interest, a comprehensive evaluation\nthat quantifies the impact of these factors is still lacking. This paper\nsystematically compares various preprocessing methods for memristive RC\nsystems, assessing their effects on accuracy and energy consumption. We also\npropose a parity-based preprocessing method that improves accuracy by 2-6%\nwhile requiring only a modest increase in device count compared to other\nmethods. Our findings highlight the importance of informed preprocessing\nstrategies to improve the efficiency and scalability of memristive RC systems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6bd4\u8f83\u4e86\u5fc6\u963b\u5668\u50a8\u5c42\u8ba1\u7b97\uff08RC\uff09\u7cfb\u7edf\u7684\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5947\u5076\u6821\u9a8c\u7684\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u9884\u5904\u7406\u7b56\u7565\u5bf9\u7cfb\u7edf\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5fc6\u963b\u5668\u50a8\u5c42\u8ba1\u7b97\u7cfb\u7edf\u56e0\u5176\u7b80\u5316\u8bad\u7ec3\u548c\u52a8\u6001\u7279\u6027\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u9ad8\u6027\u80fd\u7684\u5b9e\u73b0\u4ecd\u53d7\u9650\u4e8e\u9884\u5904\u7406\u65b9\u6cd5\u548c\u50a8\u5c42\u89c4\u6a21\uff0c\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u56e0\u7d20\u5f71\u54cd\u7684\u5168\u9762\u8bc4\u4f30\u3002", "method": "\u7cfb\u7edf\u6bd4\u8f83\u4e86\u591a\u79cd\u9884\u5904\u7406\u65b9\u6cd5\u5bf9\u5fc6\u963bRC\u7cfb\u7edf\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5947\u5076\u6821\u9a8c\u7684\u9884\u5904\u7406\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u7684\u9884\u5904\u7406\u65b9\u6cd5\u5c06\u51c6\u786e\u6027\u63d0\u9ad8\u4e862-6%\uff0c\u540c\u65f6\u8bbe\u5907\u6570\u91cf\u4ec5\u9002\u5ea6\u589e\u52a0\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u9884\u5904\u7406\u7b56\u7565\u5bf9\u63d0\u5347\u5fc6\u963bRC\u7cfb\u7edf\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.05653", "pdf": "https://arxiv.org/pdf/2506.05653", "abs": "https://arxiv.org/abs/2506.05653", "authors": ["Thien Hoang Nguyen", "Erik Muller", "Michael Rubin", "Xiaofei Wang", "Fiorella Sibona", "Salah Sukkarieh"], "title": "Towards Autonomous In-situ Soil Sampling and Mapping in Large-Scale Agricultural Environments", "categories": ["cs.RO", "cs.ET"], "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics", "summary": "Traditional soil sampling and analysis methods are labor-intensive,\ntime-consuming, and limited in spatial resolution, making them unsuitable for\nlarge-scale precision agriculture. To address these limitations, we present a\nrobotic solution for real-time sampling, analysis and mapping of key soil\nproperties. Our system consists of two main sub-systems: a Sample Acquisition\nSystem (SAS) for precise, automated in-field soil sampling; and a Sample\nAnalysis Lab (Lab) for real-time soil property analysis. The system's\nperformance was validated through extensive field trials at a large-scale\nAustralian farm. Experimental results show that the SAS can consistently\nacquire soil samples with a mass of 50g at a depth of 200mm, while the Lab can\nprocess each sample within 10 minutes to accurately measure pH and\nmacronutrients. These results demonstrate the potential of the system to\nprovide farmers with timely, data-driven insights for more efficient and\nsustainable soil management and fertilizer application.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8e\u5b9e\u65f6\u571f\u58e4\u91c7\u6837\u3001\u5206\u6790\u548c\u7ed8\u56fe\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u571f\u58e4\u91c7\u6837\u65b9\u6cd5\u8017\u65f6\u3001\u8d39\u529b\u4e14\u7a7a\u95f4\u5206\u8fa8\u7387\u4f4e\uff0c\u4e0d\u9002\u5408\u5927\u89c4\u6a21\u7cbe\u51c6\u519c\u4e1a\u3002", "method": "\u7cfb\u7edf\u5305\u62ec\u571f\u58e4\u91c7\u6837\u5b50\u7cfb\u7edf\uff08SAS\uff09\u548c\u5b9e\u65f6\u5206\u6790\u5b9e\u9a8c\u5ba4\uff08Lab\uff09\uff0c\u901a\u8fc7\u5b9e\u5730\u8bd5\u9a8c\u9a8c\u8bc1\u6027\u80fd\u3002", "result": "SAS\u80fd\u7a33\u5b9a\u91c7\u96c6200mm\u6df1\u300150g\u91cd\u7684\u571f\u58e4\u6837\u672c\uff0cLab\u80fd\u572810\u5206\u949f\u5185\u5206\u6790pH\u548c\u5b8f\u91cf\u8425\u517b\u7d20\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u4e3a\u519c\u6c11\u63d0\u4f9b\u53ca\u65f6\u6570\u636e\uff0c\u652f\u6301\u9ad8\u6548\u3001\u53ef\u6301\u7eed\u7684\u571f\u58e4\u7ba1\u7406\u548c\u65bd\u80a5\u3002"}}
{"id": "2506.05814", "pdf": "https://arxiv.org/pdf/2506.05814", "abs": "https://arxiv.org/abs/2506.05814", "authors": ["Yogesh Verma", "Amauri H. Souza", "Vikas Garg"], "title": "Positional Encoding meets Persistent Homology on Graphs", "categories": ["cs.LG", "cs.AI", "cs.ET", "cs.SI"], "comment": "Accepted at ICML 2025", "summary": "The local inductive bias of message-passing graph neural networks (GNNs)\nhampers their ability to exploit key structural information (e.g., connectivity\nand cycles). Positional encoding (PE) and Persistent Homology (PH) have emerged\nas two promising approaches to mitigate this issue. PE schemes endow GNNs with\nlocation-aware features, while PH methods enhance GNNs with multiresolution\ntopological features. However, a rigorous theoretical characterization of the\nrelative merits and shortcomings of PE and PH has remained elusive. We bridge\nthis gap by establishing that neither paradigm is more expressive than the\nother, providing novel constructions where one approach fails but the other\nsucceeds. Our insights inform the design of a novel learnable method, PiPE\n(Persistence-informed Positional Encoding), which is provably more expressive\nthan both PH and PE. PiPE demonstrates strong performance across a variety of\ntasks (e.g., molecule property prediction, graph classification, and\nout-of-distribution generalization), thereby advancing the frontiers of graph\nrepresentation learning. Code is available at\nhttps://github.com/Aalto-QuML/PIPE.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4f4d\u7f6e\u7f16\u7801\uff08PE\uff09\u548c\u6301\u4e45\u540c\u8c03\uff08PH\uff09\u7684\u65b0\u65b9\u6cd5PiPE\uff0c\u89e3\u51b3\u4e86GNN\u5728\u7ed3\u6784\u4fe1\u606f\u5229\u7528\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6d88\u606f\u4f20\u9012\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u7684\u5c40\u90e8\u5f52\u7eb3\u504f\u5dee\u9650\u5236\u4e86\u5176\u5bf9\u5173\u952e\u7ed3\u6784\u4fe1\u606f\uff08\u5982\u8fde\u901a\u6027\u548c\u5faa\u73af\uff09\u7684\u5229\u7528\uff0cPE\u548cPH\u662f\u4e24\u79cd\u6f5c\u5728\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u6bd4\u8f83\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u6bd4\u8f83PE\u548cPH\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u5b66\u4e60\u65b9\u6cd5PiPE\uff0c\u7ed3\u5408\u4e86\u4e24\u8005\u7684\u4f18\u52bf\u3002", "result": "PiPE\u5728\u8868\u8fbe\u80fd\u529b\u4e0a\u4f18\u4e8ePE\u548cPH\uff0c\u5e76\u5728\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\u3001\u56fe\u5206\u7c7b\u548c\u5206\u5e03\u5916\u6cdb\u5316\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "PiPE\u4e3a\u56fe\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.05994", "pdf": "https://arxiv.org/pdf/2506.05994", "abs": "https://arxiv.org/abs/2506.05994", "authors": ["Yi-Chun Liao", "Chieh-Lin Tsai", "Yuan-Hao Chang", "Cam\u00e9lia Slimani", "Jalil Boukhobza", "Tei-Wei Kuo"], "title": "RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory", "categories": ["cs.LG", "cs.AR", "cs.ET"], "comment": null, "summary": "Although deep learning has demonstrated remarkable capabilities in learning\nfrom unstructured data, modern tree-based ensemble models remain superior in\nextracting relevant information and learning from structured datasets. While\nseveral efforts have been made to accelerate tree-based models, the inherent\ncharacteristics of the models pose significant challenges for conventional\naccelerators. Recent research leveraging content-addressable memory (CAM)\noffers a promising solution for accelerating tree-based models, yet existing\ndesigns suffer from excessive memory consumption and low utilization. This work\naddresses these challenges by introducing RETENTION, an end-to-end framework\nthat significantly reduces CAM capacity requirement for tree-based model\ninference. We propose an iterative pruning algorithm with a novel pruning\ncriterion tailored for bagging-based models (e.g., Random Forest), which\nminimizes model complexity while ensuring controlled accuracy degradation.\nAdditionally, we present a tree mapping scheme that incorporates two innovative\ndata placement strategies to alleviate the memory redundancy caused by the\nwidespread use of don't care states in CAM. Experimental results show that\nimplementing the tree mapping scheme alone achieves $1.46\\times$ to $21.30\n\\times$ better space efficiency, while the full RETENTION framework yields\n$4.35\\times$ to $207.12\\times$ improvement with less than 3% accuracy loss.\nThese results demonstrate that RETENTION is highly effective in reducing CAM\ncapacity requirement, providing a resource-efficient direction for tree-based\nmodel acceleration.", "AI": {"tldr": "RETENTION\u6846\u67b6\u901a\u8fc7\u8fed\u4ee3\u526a\u679d\u548c\u6811\u6620\u5c04\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u6811\u6a21\u578b\u63a8\u7406\u4e2dCAM\u7684\u5bb9\u91cf\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u7cbe\u5ea6\u3002", "motivation": "\u6811\u6a21\u578b\u5728\u7ed3\u6784\u5316\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4f20\u7edf\u52a0\u901f\u5668\u96be\u4ee5\u9ad8\u6548\u652f\u6301\uff0c\u73b0\u6709CAM\u8bbe\u8ba1\u5b58\u5728\u5185\u5b58\u6d88\u8017\u9ad8\u548c\u5229\u7528\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u8fed\u4ee3\u526a\u679d\u7b97\u6cd5\u548c\u6811\u6620\u5c04\u65b9\u6848\uff0c\u4f18\u5316CAM\u4f7f\u7528\u5e76\u51cf\u5c11\u5185\u5b58\u5197\u4f59\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cRETENTION\u6846\u67b6\u5c06\u7a7a\u95f4\u6548\u7387\u63d0\u53474.35\u00d7\u81f3207.12\u00d7\uff0c\u7cbe\u5ea6\u635f\u5931\u4f4e\u4e8e3%\u3002", "conclusion": "RETENTION\u4e3a\u6811\u6a21\u578b\u52a0\u901f\u63d0\u4f9b\u4e86\u9ad8\u6548\u8d44\u6e90\u5229\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
