{"id": "2506.05358", "pdf": "https://arxiv.org/pdf/2506.05358", "abs": "https://arxiv.org/abs/2506.05358", "authors": ["Souradip Nath"], "title": "Can ChatGPT Perform Image Splicing Detection? A Preliminary Study", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) like GPT-4V are capable of reasoning\nacross text and image modalities, showing promise in a variety of complex\nvision-language tasks. In this preliminary study, we investigate the\nout-of-the-box capabilities of GPT-4V in the domain of image forensics,\nspecifically, in detecting image splicing manipulations. Without any\ntask-specific fine-tuning, we evaluate GPT-4V using three prompting strategies:\nZero-Shot (ZS), Few-Shot (FS), and Chain-of-Thought (CoT), applied over a\ncurated subset of the CASIA v2.0 splicing dataset.\n  Our results show that GPT-4V achieves competitive detection performance in\nzero-shot settings (more than 85% accuracy), with CoT prompting yielding the\nmost balanced trade-off across authentic and spliced images. Qualitative\nanalysis further reveals that the model not only detects low-level visual\nartifacts but also draws upon real-world contextual knowledge such as object\nscale, semantic consistency, and architectural facts, to identify implausible\ncomposites. While GPT-4V lags behind specialized state-of-the-art splicing\ndetection models, its generalizability, interpretability, and encyclopedic\nreasoning highlight its potential as a flexible tool in image forensics.", "AI": {"tldr": "GPT-4V\u5728\u56fe\u50cf\u53d6\u8bc1\u9886\u57df\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u5c24\u5176\u5728\u68c0\u6d4b\u56fe\u50cf\u62fc\u63a5\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u8d85\u8fc785%\u3002", "motivation": "\u7814\u7a76GPT-4V\u5728\u56fe\u50cf\u53d6\u8bc1\u4e2d\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u63a2\u7d22\u5176\u5728\u68c0\u6d4b\u56fe\u50cf\u62fc\u63a5\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u63d0\u793a\u7b56\u7565\uff08\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u601d\u7ef4\u94fe\uff09\u5728CASIA v2.0\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30GPT-4V\u3002", "result": "GPT-4V\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u601d\u7ef4\u94fe\u63d0\u793a\u5728\u771f\u5b9e\u548c\u62fc\u63a5\u56fe\u50cf\u95f4\u53d6\u5f97\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "GPT-4V\u867d\u4e0d\u53ca\u4e13\u7528\u6a21\u578b\uff0c\u4f46\u5176\u901a\u7528\u6027\u548c\u63a8\u7406\u80fd\u529b\u4f7f\u5176\u6210\u4e3a\u56fe\u50cf\u53d6\u8bc1\u4e2d\u7684\u7075\u6d3b\u5de5\u5177\u3002"}}
{"id": "2506.05360", "pdf": "https://arxiv.org/pdf/2506.05360", "abs": "https://arxiv.org/abs/2506.05360", "authors": ["Taminul Islam", "Toqi Tahamid Sarker", "Mohamed G Embaby", "Khaled R Ahmed", "Amer AbuGhazaleh"], "title": "CarboNeXT and CarboFormer: Dual Semantic Segmentation Architectures for Detecting and Quantifying Carbon Dioxide Emissions Using Optical Gas Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Carbon dioxide (CO$_2$) emissions are critical indicators of both\nenvironmental impact and various industrial processes, including livestock\nmanagement. We introduce CarboNeXT, a semantic segmentation framework for\nOptical Gas Imaging (OGI), designed to detect and quantify CO$_2$ emissions\nacross diverse applications. Our approach integrates a multi-scale context\naggregation network with UPerHead and auxiliary FCN components to effectively\nmodel both local details and global relationships in gas plume imagery. We\ncontribute two novel datasets: (1) the Controlled Carbon Dioxide Release (CCR)\ndataset, which simulates gas leaks with systematically varied flow rates\n(10-100 SCCM), and (2) the Real Time Ankom (RTA) dataset, focusing on emissions\nfrom dairy cow rumen fluid in vitro experiments. Extensive evaluations\ndemonstrate that CarboNeXT outperforms state-of-the-art methods, achieving\n88.46% mIoU on CCR and 92.95% mIoU on RTA, with particular effectiveness in\nchallenging low-flow scenarios. The model operates at 60.95 FPS, enabling\nreal-time monitoring applications. Additionally, we propose CarboFormer, a\nlightweight variant with only 5.07M parameters that achieves 84.68 FPS, with\ncompetitive performance of 84.88% mIoU on CCR and 92.98% on RTA, making it\nsuitable for resource-constrained platforms such as programmable drones. Our\nwork advances both environmental sensing and precision livestock management by\nproviding robust tools for CO$_2$ emission analysis, with a specific focus on\nlivestock applications.", "AI": {"tldr": "CarboNeXT\u662f\u4e00\u4e2a\u7528\u4e8e\u5149\u5b66\u6c14\u4f53\u6210\u50cf\u7684\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u91cf\u5316CO\u2082\u6392\u653e\uff0c\u5728\u4f4e\u6d41\u91cf\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u652f\u6301\u5b9e\u65f6\u76d1\u6d4b\u3002", "motivation": "CO\u2082\u6392\u653e\u662f\u73af\u5883\u548c\u5de5\u4e1a\u8fc7\u7a0b\u7684\u91cd\u8981\u6307\u6807\uff0c\u7279\u522b\u662f\u5728\u755c\u7267\u4e1a\u7ba1\u7406\u4e2d\uff0c\u9700\u8981\u9ad8\u6548\u7684\u5de5\u5177\u8fdb\u884c\u76d1\u6d4b\u548c\u5206\u6790\u3002", "method": "\u7ed3\u5408\u591a\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u805a\u5408\u7f51\u7edc\u3001UPerHead\u548c\u8f85\u52a9FCN\u7ec4\u4ef6\uff0c\u63d0\u51faCarboNeXT\u6846\u67b6\uff0c\u5e76\u8d21\u732e\u4e86\u4e24\u4e2a\u65b0\u6570\u636e\u96c6\uff08CCR\u548cRTA\uff09\u3002", "result": "CarboNeXT\u5728CCR\u548cRTA\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523088.46%\u548c92.95%\u7684mIoU\uff0c\u5b9e\u65f6\u6027\u80fd\u4e3a60.95 FPS\uff1b\u8f7b\u91cf\u7248CarboFormer\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aCO\u2082\u6392\u653e\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\uff0c\u7279\u522b\u9002\u7528\u4e8e\u755c\u7267\u4e1a\u548c\u73af\u5883\u76d1\u6d4b\uff0c\u63a8\u52a8\u4e86\u7cbe\u51c6\u755c\u7267\u4e1a\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.05361", "pdf": "https://arxiv.org/pdf/2506.05361", "abs": "https://arxiv.org/abs/2506.05361", "authors": ["Tinglin Huang", "Tianyu Liu", "Mehrtash Babadi", "Wengong Jin", "Rex Ying"], "title": "Scalable Generation of Spatial Transcriptomics from Histology Images via Whole-Slide Flow Matching", "categories": ["cs.CV", "q-bio.GN"], "comment": "Accepted at ICML 2025", "summary": "Spatial transcriptomics (ST) has emerged as a powerful technology for\nbridging histology imaging with gene expression profiling. However, its\napplication has been limited by low throughput and the need for specialized\nexperimental facilities. Prior works sought to predict ST from whole-slide\nhistology images to accelerate this process, but they suffer from two major\nlimitations. First, they do not explicitly model cell-cell interaction as they\nfactorize the joint distribution of whole-slide ST data and predict the gene\nexpression of each spot independently. Second, their encoders struggle with\nmemory constraints due to the large number of spots (often exceeding 10,000) in\ntypical ST datasets. Herein, we propose STFlow, a flow matching generative\nmodel that considers cell-cell interaction by modeling the joint distribution\nof gene expression of an entire slide. It also employs an efficient slide-level\nencoder with local spatial attention, enabling whole-slide processing without\nexcessive memory overhead. On the recently curated HEST-1k and STImage-1K4M\nbenchmarks, STFlow substantially outperforms state-of-the-art baselines and\nachieves over 18% relative improvements over the pathology foundation models.", "AI": {"tldr": "STFlow\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u5efa\u6a21\u6574\u4e2a\u5207\u7247\u7684\u57fa\u56e0\u8868\u8fbe\u8054\u5408\u5206\u5e03\u6765\u8003\u8651\u7ec6\u80de\u95f4\u76f8\u4e92\u4f5c\u7528\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5185\u5b58\u548c\u72ec\u7acb\u6027\u9884\u6d4b\u4e0a\u7684\u9650\u5236\u3002", "motivation": "\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\uff08ST\uff09\u6280\u672f\u867d\u5f3a\u5927\uff0c\u4f46\u53d7\u9650\u4e8e\u4f4e\u901a\u91cf\u548c\u5b9e\u9a8c\u8bbe\u65bd\u9700\u6c42\u3002\u73b0\u6709\u9884\u6d4b\u65b9\u6cd5\u56e0\u72ec\u7acb\u9884\u6d4b\u548c\u5185\u5b58\u95ee\u9898\u8868\u73b0\u4e0d\u4f73\u3002", "method": "STFlow\u91c7\u7528\u6d41\u5339\u914d\u751f\u6210\u6a21\u578b\uff0c\u5efa\u6a21\u6574\u4e2a\u5207\u7247\u7684\u57fa\u56e0\u8868\u8fbe\u8054\u5408\u5206\u5e03\uff0c\u5e76\u5f15\u5165\u9ad8\u6548\u7684\u5207\u7247\u7ea7\u7f16\u7801\u5668\u4e0e\u5c40\u90e8\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5728HEST-1k\u548cSTImage-1K4M\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSTFlow\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u76f8\u5bf9\u75c5\u7406\u57fa\u7840\u6a21\u578b\u63d0\u5347\u8d85\u8fc718%\u3002", "conclusion": "STFlow\u901a\u8fc7\u5efa\u6a21\u7ec6\u80de\u95f4\u76f8\u4e92\u4f5c\u7528\u548c\u4f18\u5316\u5185\u5b58\u4f7f\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u9884\u6d4b\u7684\u6027\u80fd\u3002"}}
