<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 34]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [ReSWD: ReSTIR'd, not shaken. Combining Reservoir Sampling and Sliced Wasserstein Distance for Variance Reduction](https://arxiv.org/abs/2510.01061)
*Mark Boss,Andreas Engelhardt,Simon Donné,Varun Jampani*

Main category: cs.GR

TL;DR: 提出了Reservoir SWD(ReSWD)方法，通过加权储层采样技术改进切片Wasserstein距离，在保持无偏性的同时减少方差，提供更稳定的梯度。


<details>
  <summary>Details</summary>
Motivation: 高维分布计算中Wasserstein距离成本过高，而切片Wasserstein距离的蒙特卡洛估计器方差大，导致梯度噪声大、收敛慢。

Method: 将加权储层采样整合到SWD中，在优化步骤中自适应保留信息丰富的投影方向。

Result: 在合成基准测试和实际任务（如色彩校正和扩散引导）中，ReSWD始终优于标准SWD和其他方差减少基线方法。

Conclusion: ReSWD通过减少方差提供稳定梯度，同时保持无偏性，在多种视觉和图形任务中表现优异。

Abstract: Distribution matching is central to many vision and graphics tasks, where the widely used Wasserstein distance is too costly to compute for high dimensional distributions. The Sliced Wasserstein Distance (SWD) offers a scalable alternative, yet its Monte Carlo estimator suffers from high variance, resulting in noisy gradients and slow convergence. We introduce Reservoir SWD (ReSWD), which integrates Weighted Reservoir Sampling into SWD to adaptively retain informative projection directions in optimization steps, resulting in stable gradients while remaining unbiased. Experiments on synthetic benchmarks and real-world tasks such as color correction and diffusion guidance show that ReSWD consistently outperforms standard SWD and other variance reduction baselines. Project page: https://reservoirswd.github.io/

</details>


### [2] [Audio Driven Real-Time Facial Animation for Social Telepresence](https://arxiv.org/abs/2510.01176)
*Jiye Lee,Chenghui Li,Linh Tran,Shih-En Wei,Jason Saragih,Alexander Richard,Hanbyul Joo,Shaojie Bai*

Main category: cs.GR

TL;DR: 提出了一种音频驱动的实时3D面部动画系统，通过扩散模型实现低延迟（<15ms）的光真实感面部表情生成，适用于VR社交互动。


<details>
  <summary>Details</summary>
Motivation: 为虚拟现实社交互动开发实时、低延迟的光真实感面部动画系统，解决现有方法在实时性能和动画质量方面的不足。

Method: 使用编码器将音频信号转换为潜在面部表情序列，通过扩散模型解码为3D面部动画。采用在线transformer消除对未来输入的依赖，并通过蒸馏流程将迭代去噪加速为单步处理。

Result: 相比现有离线方法，面部动画精度显著提升，推理速度提高100-1000倍，在GPU上处理时间低于15ms。支持多语言语音和VR头戴设备的多模态应用。

Conclusion: 该系统成功实现了实时、低延迟的光真实感面部动画，为VR社交互动提供了有效的解决方案，并在多语言和多模态场景中验证了其有效性。

Abstract: We present an audio-driven real-time system for animating photorealistic 3D facial avatars with minimal latency, designed for social interactions in virtual reality for anyone. Central to our approach is an encoder model that transforms audio signals into latent facial expression sequences in real time, which are then decoded as photorealistic 3D facial avatars. Leveraging the generative capabilities of diffusion models, we capture the rich spectrum of facial expressions necessary for natural communication while achieving real-time performance (<15ms GPU time). Our novel architecture minimizes latency through two key innovations: an online transformer that eliminates dependency on future inputs and a distillation pipeline that accelerates iterative denoising into a single step. We further address critical design challenges in live scenarios for processing continuous audio signals frame-by-frame while maintaining consistent animation quality. The versatility of our framework extends to multimodal applications, including semantic modalities such as emotion conditions and multimodal sensors with head-mounted eye cameras on VR headsets. Experimental results demonstrate significant improvements in facial animation accuracy over existing offline state-of-the-art baselines, achieving 100 to 1000 times faster inference speed. We validate our approach through live VR demonstrations and across various scenarios such as multilingual speeches.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [3] [Hybrid Deep Learning for Hyperspectral Single Image Super-Resolution](https://arxiv.org/abs/2510.00033)
*Usman Muhammad,Jorma Laaksonen*

Main category: cs.CV

TL;DR: 提出SSUF模块和空间-光谱梯度损失函数，用于提升高光谱单图像超分辨率的空间细节恢复和光谱保真度


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在高光谱超分辨率任务中难以同时恢复精细空间细节和保持宽波长范围内的光谱保真度

Method: 提出光谱-空间解混融合(SSUF)模块，结合光谱解混和光谱-空间特征提取，指导基于ResNet的CNN进行重建；并设计空间-光谱梯度损失函数

Result: 在三个公共遥感高光谱数据集上的实验表明，该混合深度学习模型在降低模型复杂度的同时实现了有竞争力的性能

Conclusion: SSUF模块和空间-光谱梯度损失函数能有效提升高光谱超分辨率的性能

Abstract: Hyperspectral single image super-resolution (SISR) is a challenging task due to the difficulty of restoring fine spatial details while preserving spectral fidelity across a wide range of wavelengths, which limits the performance of conventional deep learning models. To address this challenge, we introduce Spectral-Spatial Unmixing Fusion (SSUF), a novel module that can be seamlessly integrated into standard 2D convolutional architectures to enhance both spatial resolution and spectral integrity. The SSUF combines spectral unmixing with spectral--spatial feature extraction and guides a ResNet-based convolutional neural network for improved reconstruction. In addition, we propose a custom Spatial-Spectral Gradient Loss function that integrates mean squared error with spatial and spectral gradient components, encouraging accurate reconstruction of both spatial and spectral features. Experiments on three public remote sensing hyperspectral datasets demonstrate that the proposed hybrid deep learning model achieves competitive performance while reducing model complexity.

</details>


### [4] [Explanation-Driven Counterfactual Testing for Faithfulness in Vision-Language Model Explanations](https://arxiv.org/abs/2510.00047)
*Sihao Ding,Santosh Vasa,Aditi Ramadwar*

Main category: cs.CV

TL;DR: 提出了EDCT方法，通过自动生成反事实图像来验证视觉语言模型解释的忠实性，发现模型解释与真实因果因素之间存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型生成的解释听起来合理但可能不反映真实预测原因，这种可信性与忠实性不匹配存在技术和治理风险。

Method: EDCT方法：获取模型答案和解释→解析为可测试视觉概念→通过生成修复生成针对性反事实编辑→使用LLM辅助分析计算反事实一致性分数。

Result: 在120个OK-VQA示例和多个VLM上，EDCT揭示了显著的忠实性差距，并提供了监管对齐的审计证据。

Conclusion: EDCT能够自动验证VLM解释的忠实性，识别被引用概念在因果测试中失败的情况，为模型治理提供重要工具。

Abstract: Vision-Language Models (VLMs) often produce fluent Natural Language Explanations (NLEs) that sound convincing but may not reflect the causal factors driving predictions. This mismatch of plausibility and faithfulness poses technical and governance risks. We introduce Explanation-Driven Counterfactual Testing (EDCT), a fully automated verification procedure for a target VLM that treats the model's own explanation as a falsifiable hypothesis. Given an image-question pair, EDCT: (1) obtains the model's answer and NLE, (2) parses the NLE into testable visual concepts, (3) generates targeted counterfactual edits via generative inpainting, and (4) computes a Counterfactual Consistency Score (CCS) using LLM-assisted analysis of changes in both answers and explanations. Across 120 curated OK-VQA examples and multiple VLMs, EDCT uncovers substantial faithfulness gaps and provides regulator-aligned audit artifacts indicating when cited concepts fail causal tests.

</details>


### [5] [HiDe: Rethinking The Zoom-IN method in High Resolution MLLMs via Hierarchical Decoupling](https://arxiv.org/abs/2510.00054)
*Xianjie Liu,Yiman Hu,Yixiong Zou,Liang Wu,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: 提出了HiDe框架，通过Token-wise Attention Decoupling和Layout-Preserving Decoupling解决多模态大语言模型在高分辨率图像中的背景干扰问题，无需训练即可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法认为MLLMs在高分辨率图像上表现不佳是因为感知限制和小物体识别困难，但本文分析发现主要问题其实是复杂背景干扰而非物体大小。

Method: 提出分层解耦框架HiDe：1) Token-wise Attention Decoupling解耦问题token识别关键信息token；2) Layout-Preserving Decoupling从背景中解耦目标区域并重建紧凑表示。

Result: 在V*Bench、HRBench4K和HRBench8K上达到新SOTA，将Qwen2.5-VL 7B和InternVL3 8B分别提升至92.1%和91.6%，超越RL方法，内存使用减少75%。

Conclusion: HiDe框架有效解决了高分辨率图像中的背景干扰问题，无需训练即可显著提升MLLMs性能，为视觉理解任务提供了高效解决方案。

Abstract: Multimodal Large Language Models (MLLMs) have made significant strides in visual understanding tasks. However, their performance on high-resolution images remains suboptimal. While existing approaches often attribute this limitation to perceptual constraints and argue that MLLMs struggle to recognize small objects, leading them to use "zoom in" strategies for better detail, our analysis reveals a different cause: the main issue is not object size, but rather caused by complex background interference. We systematically analyze this "zoom in" operation through a series of decoupling experiments and propose the Hierarchical Decoupling Framework (HiDe), a training-free framework that uses Token-wise Attention Decoupling (TAD) to decouple the question tokens and identify the key information tokens, then leverages their attention weights to achieve precise alignment with the target visual regions. Subsequently, it employs Layout-Preserving Decoupling (LPD) to decouple these regions from the background and reconstructs a compact representation that preserves essential spatial layouts while eliminating background interference. HiDe sets a new SOTA on V*Bench, HRBench4K, and HRBench8K, boosting Qwen2.5-VL 7B and InternVL3 8B to SOTA (92.1% and 91.6% on V*Bench), even surpassing RL methods. After optimization, HiDe uses 75% less memory than the previous training-free approach. Code is provided in https://github.com/Tennine2077/HiDe.

</details>


### [6] [MOLM: Mixture of LoRA Markers](https://arxiv.org/abs/2510.00293)
*Samar Fares,Nurbek Tastan,Noor Hussein,Karthik Nandakumar*

Main category: cs.CV

TL;DR: 提出MOLM水印框架，通过LoRA适配器实现生成模型的参数扰动，在保持图像质量的同时提供鲁棒的水印验证能力。


<details>
  <summary>Details</summary>
Motivation: 生成模型能大规模生成逼真图像，引发对检测和溯源合成图像的担忧。现有水印方法对真实失真脆弱、易被自适应移除，且更新成本高。

Method: 提出基于路由的MOLM框架，使用二进制密钥激活轻量级LoRA适配器，避免密钥特定重训练，实现不可感知性、保真度、可验证性和鲁棒性。

Result: 在Stable Diffusion和FLUX上的实验表明，MOLM在保持图像质量的同时，能有效抵抗失真、压缩、再生、平均攻击和黑盒对抗攻击。

Conclusion: MOLM框架为生成模型提供了一种有效的水印解决方案，平衡了图像质量与水印鲁棒性。

Abstract: Generative models can generate photorealistic images at scale. This raises urgent concerns about the ability to detect synthetically generated images and attribute these images to specific sources. While watermarking has emerged as a possible solution, existing methods remain fragile to realistic distortions, susceptible to adaptive removal, and expensive to update when the underlying watermarking key changes. We propose a general watermarking framework that formulates the encoding problem as key-dependent perturbation of the parameters of a generative model. Within this framework, we introduce Mixture of LoRA Markers (MOLM), a routing-based instantiation in which binary keys activate lightweight LoRA adapters inside residual and attention blocks. This design avoids key-specific re-training and achieves the desired properties such as imperceptibility, fidelity, verifiability, and robustness. Experiments on Stable Diffusion and FLUX show that MOLM preserves image quality while achieving robust key recovery against distortions, compression and regeneration, averaging attacks, and black-box adversarial attacks on the extractor.

</details>


### [7] [Discrete Wavelet Transform as a Facilitator for Expressive Latent Space Representation in Variational Autoencoders in Satellite Imagery](https://arxiv.org/abs/2510.00376)
*Arpan Mahara,Md Rezaul Karim Khan,Naphtali Rishe,Wenjia Wang,Seyed Masoud Sadjadi*

Main category: cs.CV

TL;DR: 提出ExpDWT-VAE方法，通过离散小波变换增强VAE的潜在空间表示，专门针对卫星图像设计，在遥感应用中提升潜在扩散模型的性能。


<details>
  <summary>Details</summary>
Motivation: 虽然已有许多研究改进潜在扩散模型，但针对内在潜在空间改进的研究仍然稀缺。本文旨在通过增强VAE的潜在空间表示来提升遥感应用中的性能。

Method: 提出ExpDWT-VAE方法，采用双分支结构：一个分支通过卷积处理空间域输入，另一个分支通过2D Haar小波分解、卷积操作和逆DWT重构提取和处理频域特征。两个分支融合形成集成空间-频率表示，再通过卷积和对角高斯映射精炼为鲁棒的潜在表示。

Result: 在TerraFly映射系统的新卫星图像数据集上进行实验，多个性能指标的实验结果突显了所提方法在增强潜在空间表示方面的有效性。

Conclusion: ExpDWT-VAE方法通过集成空间和频域特征，成功增强了VAE的潜在空间表示，为遥感应用中的潜在扩散模型提供了改进方案。

Abstract: Latent Diffusion Models (LDM), a subclass of diffusion models, mitigate the computational complexity of pixel-space diffusion by operating within a compressed latent space constructed by Variational Autoencoders (VAEs), demonstrating significant advantages in Remote Sensing (RS) applications. Though numerous studies enhancing LDMs have been conducted, investigations explicitly targeting improvements within the intrinsic latent space remain scarce. This paper proposes an innovative perspective, utilizing the Discrete Wavelet Transform (DWT) to enhance the VAE's latent space representation, designed for satellite imagery. The proposed method, ExpDWT-VAE, introduces dual branches: one processes spatial domain input through convolutional operations, while the other extracts and processes frequency-domain features via 2D Haar wavelet decomposition, convolutional operation, and inverse DWT reconstruction. These branches merge to create an integrated spatial-frequency representation, further refined through convolutional and diagonal Gaussian mapping into a robust latent representation. We utilize a new satellite imagery dataset housed by the TerraFly mapping system to validate our method. Experimental results across several performance metrics highlight the efficacy of the proposed method at enhancing latent space representation.

</details>


### [8] [BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration](https://arxiv.org/abs/2510.00438)
*Zhaoyang Li,Dongjun Qian,Kai Su,Qishuai Diao,Xiangyang Xia,Chang Liu,Wenfei Yang,Tianzhu Zhang,Zehuan Yuan*

Main category: cs.CV

TL;DR: BindWeave是一个统一的视频生成框架，通过MLLM-DiT架构解决多主体一致性视频生成的难题，在复杂空间关系和时序逻辑下实现高质量的主体一致视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在处理复杂空间关系、时序逻辑和多主体交互时难以保持主体一致性，无法准确解析包含多个主体及其交互的提示词。

Method: 提出BindWeave框架，采用预训练多模态大语言模型进行深度跨模态推理，识别实体并解耦角色、属性和交互关系，生成主体感知的隐藏状态来条件化扩散变换器。

Result: 在OpenS2V基准测试中，该方法在主体一致性、自然度和文本相关性方面均优于现有开源和商业模型。

Conclusion: BindWeave通过统一的MLLM-DiT框架有效解决了复杂多主体视频生成中的主体一致性问题，为高质量视频生成提供了新思路。

Abstract: Diffusion Transformer has shown remarkable abilities in generating high-fidelity videos, delivering visually coherent frames and rich details over extended durations. However, existing video generation models still fall short in subject-consistent video generation due to an inherent difficulty in parsing prompts that specify complex spatial relationships, temporal logic, and interactions among multiple subjects. To address this issue, we propose BindWeave, a unified framework that handles a broad range of subject-to-video scenarios from single-subject cases to complex multi-subject scenes with heterogeneous entities. To bind complex prompt semantics to concrete visual subjects, we introduce an MLLM-DiT framework in which a pretrained multimodal large language model performs deep cross-modal reasoning to ground entities and disentangle roles, attributes, and interactions, yielding subject-aware hidden states that condition the diffusion transformer for high-fidelity subject-consistent video generation. Experiments on the OpenS2V benchmark demonstrate that our method achieves superior performance across subject consistency, naturalness, and text relevance in generated videos, outperforming existing open-source and commercial models.

</details>


### [9] [Measuring and Controlling the Spectral Bias for Self-Supervised Image Denoising](https://arxiv.org/abs/2510.00454)
*Wang Zhang,Huaqiu Li,Xiaowan Hu,Tao Jiang,Zikang Chen,Haoqian Wang*

Main category: cs.CV

TL;DR: 提出SCNet网络优化自监督图像去噪，通过频率控制解决高频细节丢失和噪声学习问题


<details>
  <summary>Details</summary>
Motivation: 现有自监督去噪方法存在两个问题：高频结构细节保留不足，以及网络在学习高频时从噪声图像中学习到高频噪声

Method: 1. 频率带选择策略加速训练收敛；2. 基于Lipschitz常数的参数优化方法限制卷积核对高频噪声的学习能力；3. 频谱分离和低秩重建模块(SSR)通过频域分离和低秩空间重建分离噪声与高频细节

Result: 在合成和真实数据集上的实验验证了SCNet的有效性

Conclusion: SCNet通过频谱控制成功解决了自监督去噪中的高频细节保留和噪声学习问题

Abstract: Current self-supervised denoising methods for paired noisy images typically involve mapping one noisy image through the network to the other noisy image. However, after measuring the spectral bias of such methods using our proposed Image Pair Frequency-Band Similarity, it suffers from two practical limitations. Firstly, the high-frequency structural details in images are not preserved well enough. Secondly, during the process of fitting high frequencies, the network learns high-frequency noise from the mapped noisy images. To address these challenges, we introduce a Spectral Controlling network (SCNet) to optimize self-supervised denoising of paired noisy images. First, we propose a selection strategy to choose frequency band components for noisy images, to accelerate the convergence speed of training. Next, we present a parameter optimization method that restricts the learning ability of convolutional kernels to high-frequency noise using the Lipschitz constant, without changing the network structure. Finally, we introduce the Spectral Separation and low-rank Reconstruction module (SSR module), which separates noise and high-frequency details through frequency domain separation and low-rank space reconstruction, to retain the high-frequency structural details of images. Experiments performed on synthetic and real-world datasets verify the effectiveness of SCNet.

</details>


### [10] [Cascaded Diffusion Framework for Probabilistic Coarse-to-Fine Hand Pose Estimation](https://arxiv.org/abs/2510.00527)
*Taeyun Woo,Jinah Park,Tae-Kyun Kim*

Main category: cs.CV

TL;DR: 提出了一种从粗到精的级联扩散框架，结合概率建模与级联优化，用于3D手部姿态重建，解决姿态模糊性和不确定性建模问题。


<details>
  <summary>Details</summary>
Motivation: 现有的确定性模型难以处理自遮挡和复杂手部关节带来的姿态模糊性，而现有的概率方法仅限于单阶段估计，无法进行细化。

Method: 使用两阶段级联扩散框架：第一阶段是联合扩散模型生成多样化的3D关节假设，第二阶段是基于网格潜在扩散模型，在关节样本条件下重建3D手部网格。

Result: 在FreiHAND和HO3Dv2数据集上的实验表明，该方法实现了最先进的性能，同时有效建模了姿态分布。

Conclusion: 该框架成功结合了概率建模与级联优化，解决了3D手部姿态重建中的模糊性和不确定性挑战。

Abstract: Deterministic models for 3D hand pose reconstruction, whether single-staged or cascaded, struggle with pose ambiguities caused by self-occlusions and complex hand articulations. Existing cascaded approaches refine predictions in a coarse-to-fine manner but remain deterministic and cannot capture pose uncertainties. Recent probabilistic methods model pose distributions yet are restricted to single-stage estimation, which often fails to produce accurate 3D reconstructions without refinement. To address these limitations, we propose a coarse-to-fine cascaded diffusion framework that combines probabilistic modeling with cascaded refinement. The first stage is a joint diffusion model that samples diverse 3D joint hypotheses, and the second stage is a Mesh Latent Diffusion Model (Mesh LDM) that reconstructs a 3D hand mesh conditioned on a joint sample. By training Mesh LDM with diverse joint hypotheses in a learned latent space, our framework learns distribution-aware joint-mesh relationships and robust hand priors. Furthermore, the cascaded design mitigates the difficulty of directly mapping 2D images to dense 3D poses, enhancing accuracy through sequential refinement. Experiments on FreiHAND and HO3Dv2 demonstrate that our method achieves state-of-the-art performance while effectively modeling pose distributions.

</details>


### [11] [Adaptive Shared Experts with LoRA-Based Mixture of Experts for Multi-Task Learning](https://arxiv.org/abs/2510.00570)
*Minghao Yang,Ren Togo,Guang Li,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

TL;DR: 提出自适应共享专家(ASE)框架，在基于LoRA的MoE中引入共享专家，通过路由器计算的门控权重实现从单任务到多任务学习的平滑过渡，提升专家专业化和协作能力。


<details>
  <summary>Details</summary>
Motivation: 现有MoE-MTL方法依赖单任务预训练骨干网络，在从单任务到多任务学习转换过程中存在冗余适配和知识共享效率低的问题。

Method: 在LoRA-based MoE中引入自适应共享专家，共享专家与稀疏专家联合归一化门控权重；通过增加LoRA专家数量同时按比例降低其秩，实现细粒度专家设计。

Result: 在PASCAL-Context基准测试中，ASE在不同配置下均能持续提升性能，验证了细粒度设计对MTL的有效性。

Conclusion: ASE框架有效解决了STL到MTL转换中的问题，通过自适应共享专家和细粒度专家设计实现了更好的知识共享和性能提升。

Abstract: Mixture-of-Experts (MoE) has emerged as a powerful framework for multi-task learning (MTL). However, existing MoE-MTL methods often rely on single-task pretrained backbones and suffer from redundant adaptation and inefficient knowledge sharing during the transition from single-task to multi-task learning (STL to MTL). To address these limitations, we propose adaptive shared experts (ASE) within a low-rank adaptation (LoRA) based MoE, where shared experts are assigned router-computed gating weights jointly normalized with sparse experts. This design facilitates STL to MTL transition, enhances expert specialization, and cooperation. Furthermore, we incorporate fine-grained experts by increasing the number of LoRA experts while proportionally reducing their rank, enabling more effective knowledge sharing under a comparable parameter budget. Extensive experiments on the PASCAL-Context benchmark, under unified training settings, demonstrate that ASE consistently improves performance across diverse configurations and validates the effectiveness of fine-grained designs for MTL.

</details>


### [12] [Arbitrary Generative Video Interpolation](https://arxiv.org/abs/2510.00578)
*Guozhen Zhang,Haiguang Wang,Chunyu Wang,Yuan Zhou,Qinglin Lu,Limin Wang*

Main category: cs.CV

TL;DR: ArbInterp是一个创新的视频帧插值框架，支持任意时间戳和任意长度的插值，解决了现有方法只能生成固定数量中间帧的限制。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式视频帧插值方法只能合成固定数量的中间帧，缺乏调整生成帧率或总序列时长的灵活性。

Method: 提出Timestamp-aware Rotary Position Embedding (TaRoPE)来支持任意时间戳插值，以及将长序列生成分解为分段帧合成的任意长度插值方法，采用外观-运动解耦条件策略确保跨段时空连续性。

Result: 在2x到32x多尺度帧插值基准测试中，ArbInterp在所有场景下都优于先前方法，具有更高的保真度和更无缝的时空连续性。

Conclusion: ArbInterp通过创新的时间戳感知位置编码和分段生成策略，实现了灵活高效的任意时间戳和任意长度视频帧插值。

Abstract: Video frame interpolation (VFI), which generates intermediate frames from given start and end frames, has become a fundamental function in video generation applications. However, existing generative VFI methods are constrained to synthesize a fixed number of intermediate frames, lacking the flexibility to adjust generated frame rates or total sequence duration. In this work, we present ArbInterp, a novel generative VFI framework that enables efficient interpolation at any timestamp and of any length. Specifically, to support interpolation at any timestamp, we propose the Timestamp-aware Rotary Position Embedding (TaRoPE), which modulates positions in temporal RoPE to align generated frames with target normalized timestamps. This design enables fine-grained control over frame timestamps, addressing the inflexibility of fixed-position paradigms in prior work. For any-length interpolation, we decompose long-sequence generation into segment-wise frame synthesis. We further design a novel appearance-motion decoupled conditioning strategy: it leverages prior segment endpoints to enforce appearance consistency and temporal semantics to maintain motion coherence, ensuring seamless spatiotemporal transitions across segments. Experimentally, we build comprehensive benchmarks for multi-scale frame interpolation (2x to 32x) to assess generalizability across arbitrary interpolation factors. Results show that ArbInterp outperforms prior methods across all scenarios with higher fidelity and more seamless spatiotemporal continuity. Project website: https://mcg-nju.github.io/ArbInterp-Web/.

</details>


### [13] [Multi-level Dynamic Style Transfer for NeRFs](https://arxiv.org/abs/2510.00592)
*Zesheng Li,Shuaibo Li,Wei Ma,Jianwei Guo,Hongbin Zha*

Main category: cs.CV

TL;DR: MDS-NeRF是一种针对神经辐射场（NeRF）的3D风格迁移方法，通过多级特征适配器和动态风格注入模块，在保持场景多尺度空间结构的同时实现高质量风格化。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF风格迁移方法通常将风格统计信息直接集成到原始NeRF流程中，导致内容保持和艺术风格化效果都不理想。

Method: 提出多级动态风格迁移框架，包括：多级特征适配器生成多级特征网格表示；动态风格注入模块学习提取相关风格特征并自适应集成；多级级联解码器将风格化特征转换为最终风格化视图。

Result: 大量实验证明MDS-NeRF在3D风格迁移方面表现优异，能够保持多尺度空间结构同时有效传递风格特征。

Conclusion: 该方法成功重构了NeRF流程以适应风格化需求，并支持使用3D风格参考进行全视角风格迁移。

Abstract: As the application of neural radiance fields (NeRFs) in various 3D vision tasks continues to expand, numerous NeRF-based style transfer techniques have been developed. However, existing methods typically integrate style statistics into the original NeRF pipeline, often leading to suboptimal results in both content preservation and artistic stylization. In this paper, we present multi-level dynamic style transfer for NeRFs (MDS-NeRF), a novel approach that reengineers the NeRF pipeline specifically for stylization and incorporates an innovative dynamic style injection module. Particularly, we propose a multi-level feature adaptor that helps generate a multi-level feature grid representation from the content radiance field, effectively capturing the multi-scale spatial structure of the scene. In addition, we present a dynamic style injection module that learns to extract relevant style features and adaptively integrates them into the content patterns. The stylized multi-level features are then transformed into the final stylized view through our proposed multi-level cascade decoder. Furthermore, we extend our 3D style transfer method to support omni-view style transfer using 3D style references. Extensive experiments demonstrate that MDS-NeRF achieves outstanding performance for 3D style transfer, preserving multi-scale spatial structures while effectively transferring stylistic characteristics.

</details>


### [14] [UCD: Unconditional Discriminator Promotes Nash Equilibrium in GANs](https://arxiv.org/abs/2510.00624)
*Mengfei Xia,Nan Xue,Jiapeng Zhu,Yujun Shen*

Main category: cs.CV

TL;DR: 本文提出使用无条件判别器(UCD)来解决GAN训练中的模式崩溃问题，通过强制判别器提取更全面的特征来促进纳什均衡，在ImageNet-64上达到了1.47 FID的优异性能。


<details>
  <summary>Details</summary>
Motivation: 对抗训练在一步生成中很关键，但GAN训练难以正确收敛且容易陷入模式崩溃。研究发现输入条件到判别器会产生冗余捷径，阻碍有意义的特征提取。

Method: 提出无条件判别器(UCD)，强制判别器在不注入条件的情况下提取更全面和鲁棒的特征，从而为生成器提供更好的监督，促进纳什均衡。

Result: 在ImageNet-64数据集上达到1.47 FID，超越了StyleGAN-XL和多个最先进的一步扩散模型，表现出显著的性能提升和高效率。

Conclusion: 无条件判别器能够有效解决GAN训练中的模式崩溃问题，与原始GAN理论兼容，可以作为插件方式实现，实验证明其显著提升了生成质量。

Abstract: Adversarial training turns out to be the key to one-step generation, especially for Generative Adversarial Network (GAN) and diffusion model distillation. Yet in practice, GAN training hardly converges properly and struggles in mode collapse. In this work, we quantitatively analyze the extent of Nash equilibrium in GAN training, and conclude that redundant shortcuts by inputting condition in $D$ disables meaningful knowledge extraction. We thereby propose to employ an unconditional discriminator (UCD), in which $D$ is enforced to extract more comprehensive and robust features with no condition injection. In this way, $D$ is able to leverage better knowledge to supervise $G$, which promotes Nash equilibrium in GAN literature. Theoretical guarantee on compatibility with vanilla GAN theory indicates that UCD can be implemented in a plug-in manner. Extensive experiments confirm the significant performance improvements with high efficiency. For instance, we achieved \textbf{1.47 FID} on the ImageNet-64 dataset, surpassing StyleGAN-XL and several state-of-the-art one-step diffusion models. The code will be made publicly available.

</details>


### [15] [Virtual Fashion Photo-Shoots: Building a Large-Scale Garment-Lookbook Dataset](https://arxiv.org/abs/2510.00633)
*Yannick Hauri,Luca A. Lanzendörfer,Till Aczel*

Main category: cs.CV

TL;DR: 该论文提出了虚拟时尚摄影任务，旨在将标准化的服装图像转化为具有情境背景的编辑图像，并构建了首个大规模服装-画册配对数据集。


<details>
  <summary>Details</summary>
Motivation: 现有时尚图像生成主要关注虚拟试穿等狭窄任务，而编辑时尚通过动态姿势、多样地点和精心设计的视觉叙事来展示服装。本文旨在捕捉这种丰富性，弥合电商和时尚媒体之间的差距。

Method: 设计自动化检索管道，结合视觉语言推理和对象级定位，跨领域对齐服装。构建包含三个质量级别的数据集：高质量（10,000对）、中等质量（50,000对）和低质量（300,000对）。

Result: 成功构建了大规模服装-画册配对数据集，为超越目录式生成、迈向反映创意、氛围和故事叙述的时尚图像模型奠定了基础。

Conclusion: 该研究为时尚图像生成开辟了新方向，使模型能够生成更具创造力和叙事性的编辑图像，而不仅仅是简单的产品展示。

Abstract: Fashion image generation has so far focused on narrow tasks such as virtual try-on, where garments appear in clean studio environments. In contrast, editorial fashion presents garments through dynamic poses, diverse locations, and carefully crafted visual narratives. We introduce the task of virtual fashion photo-shoot, which seeks to capture this richness by transforming standardized garment images into contextually grounded editorial imagery. To enable this new direction, we construct the first large-scale dataset of garment-lookbook pairs, bridging the gap between e-commerce and fashion media. Because such pairs are not readily available, we design an automated retrieval pipeline that aligns garments across domains, combining visual-language reasoning with object-level localization. We construct a dataset with three garment-lookbook pair accuracy levels: high quality (10,000 pairs), medium quality (50,000 pairs), and low quality (300,000 pairs). This dataset offers a foundation for models that move beyond catalog-style generation and toward fashion imagery that reflects creativity, atmosphere, and storytelling.

</details>


### [16] [Erased, But Not Forgotten: Erased Rectified Flow Transformers Still Remain Unsafe Under Concept Attack](https://arxiv.org/abs/2510.00635)
*Nanxiang Jiang,Zhaoxin Fan,Enhan Kang,Daiheng Gao,Yun Zhou,Yanxia Chang,Zheng Zhu,Yeying Jin,Wenjun Wu*

Main category: cs.CV

TL;DR: 提出了ReFlux，第一个专门针对最新整流流T2I框架的概念攻击方法，通过反向注意力优化和速度引导动态来评估概念擦除的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法在应用于Flux等下一代整流流变换器时效果有限，主要依赖于注意力定位现象，需要专门的方法来评估其鲁棒性。

Method: 采用反向注意力优化策略重新激活被抑制信号并稳定注意力，结合速度引导动态增强概念重新激活的鲁棒性，以及一致性保持目标维持全局布局。

Result: 大量实验一致证明了所提攻击方法的有效性和效率，为评估整流流变换器中概念擦除策略的鲁棒性建立了可靠基准。

Conclusion: ReFlux方法成功评估了整流流T2I框架中概念擦除的脆弱性，为未来安全研究提供了重要参考。

Abstract: Recent advances in text-to-image (T2I) diffusion models have enabled impressive generative capabilities, but they also raise significant safety concerns due to the potential to produce harmful or undesirable content. While concept erasure has been explored as a mitigation strategy, most existing approaches and corresponding attack evaluations are tailored to Stable Diffusion (SD) and exhibit limited effectiveness when transferred to next-generation rectified flow transformers such as Flux. In this work, we present ReFlux, the first concept attack method specifically designed to assess the robustness of concept erasure in the latest rectified flow-based T2I framework. Our approach is motivated by the observation that existing concept erasure techniques, when applied to Flux, fundamentally rely on a phenomenon known as attention localization. Building on this insight, we propose a simple yet effective attack strategy that specifically targets this property. At its core, a reverse-attention optimization strategy is introduced to effectively reactivate suppressed signals while stabilizing attention. This is further reinforced by a velocity-guided dynamic that enhances the robustness of concept reactivation by steering the flow matching process, and a consistency-preserving objective that maintains the global layout and preserves unrelated content. Extensive experiments consistently demonstrate the effectiveness and efficiency of the proposed attack method, establishing a reliable benchmark for evaluating the robustness of concept erasure strategies in rectified flow transformers.

</details>


### [17] [Multi-Domain Brain Vessel Segmentation Through Feature Disentanglement](https://arxiv.org/abs/2510.00665)
*Francesco Galati,Daniele Falcetta,Rosa Cortese,Ferran Prados,Ninon Burgos,Maria A. Zuluaga*

Main category: cs.CV

TL;DR: 该论文提出了一种用于多模态脑血管分割的域适应框架，通过图像到图像翻译技术，能够在保持血管空间信息的同时适应不同域的外观差异，无需特定领域模型设计或数据协调。


<details>
  <summary>Details</summary>
Motivation: 脑血管复杂形态对自动分割模型构成挑战，而准确治疗脑部疾病需要全面理解脑血管树，不受特定采集程序限制。现有方法通常只关注单一成像模态，无法适应多中心、多模态的实际情况。

Method: 采用解缠技术独立操纵不同图像属性，在域适应过程中保持标签信息。重点是在适应过程中操纵血管外观，同时保留对正确分割至关重要的空间信息（如形状和位置）。

Result: 评估显示该框架能有效跨越医疗中心、图像模态和血管类型的大域差距。消融研究确定了所需标注的最佳数量和其他架构选择，证明了框架的鲁棒性和多功能性。

Conclusion: 该研究展示了域适应方法在多场景下准确执行脑血管图像分割的潜力，为多模态脑血管分析提供了有效解决方案。

Abstract: The intricate morphology of brain vessels poses significant challenges for automatic segmentation models, which usually focus on a single imaging modality. However, accurately treating brain-related conditions requires a comprehensive understanding of the cerebrovascular tree, regardless of the specific acquisition procedure. Our framework effectively segments brain arteries and veins in various datasets through image-to-image translation while avoiding domain-specific model design and data harmonization between the source and the target domain. This is accomplished by employing disentanglement techniques to independently manipulate different image properties, allowing them to move from one domain to another in a label-preserving manner. Specifically, we focus on manipulating vessel appearances during adaptation while preserving spatial information, such as shapes and locations, which are crucial for correct segmentation. Our evaluation effectively bridges large and varied domain gaps across medical centers, image modalities, and vessel types. Additionally, we conduct ablation studies on the optimal number of required annotations and other architectural choices. The results highlight our framework's robustness and versatility, demonstrating the potential of domain adaptation methodologies to perform cerebrovascular image segmentation in multiple scenarios accurately. Our code is available at https://github.com/i-vesseg/MultiVesSeg.

</details>


### [18] [A Geometric Unification of Generative AI with Manifold-Probabilistic Projection Models](https://arxiv.org/abs/2510.00666)
*Leah Bar,Liron Mor Yosef,Shai Zucker,Neta Shoham,Inbar Seroussi,Nir Sochen*

Main category: cs.CV

TL;DR: 本文提出了一个统一几何和概率视角的框架，将扩散模型解释为向“好图像”流形的投影机制，并构建了新的确定性模型MPPM，在潜在空间中优于潜在扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型忽视了数据的几何结构，仅关注概率方法，且将潜在空间的概率分布视为无趣或预定义的。本文旨在统一几何和概率视角。

Method: 提出了几何框架和基于核的概率方法，将扩散模型解释为向图像流形的投影，构建了确定性模型MPPM，在表示空间和潜在空间运行。

Result: 潜在MPPM（LMPPM）在多个数据集上优于潜在扩散模型（LDM），在图像恢复和生成方面取得更优结果。

Conclusion: 该框架为生成模型提供了新的几何解释，MPPM模型在性能上超越了现有扩散模型。

Abstract: The foundational premise of generative AI for images is the assumption that images are inherently low-dimensional objects embedded within a high-dimensional space. Additionally, it is often implicitly assumed that thematic image datasets form smooth or piecewise smooth manifolds. Common approaches overlook the geometric structure and focus solely on probabilistic methods, approximating the probability distribution through universal approximation techniques such as the kernel method. In some generative models, the low dimensional nature of the data manifest itself by the introduction of a lower dimensional latent space. Yet, the probability distribution in the latent or the manifold coordinate space is considered uninteresting and is predefined or considered uniform. This study unifies the geometric and probabilistic perspectives by providing a geometric framework and a kernel-based probabilistic method simultaneously. The resulting framework demystifies diffusion models by interpreting them as a projection mechanism onto the manifold of ``good images''. This interpretation leads to the construction of a new deterministic model, the Manifold-Probabilistic Projection Model (MPPM), which operates in both the representation (pixel) space and the latent space. We demonstrate that the Latent MPPM (LMPPM) outperforms the Latent Diffusion Model (LDM) across various datasets, achieving superior results in terms of image restoration and generation.

</details>


### [19] [Extreme Blind Image Restoration via Prompt-Conditioned Information Bottleneck](https://arxiv.org/abs/2510.00728)
*Hongeun Kim,Bryan Sangwoo Kim,Jong Chul Ye*

Main category: cs.CV

TL;DR: 提出了一种解决极端盲图像恢复(EBIR)问题的新框架，通过将极度低质量(ELQ)到高质量(HQ)的恢复过程分解为两个步骤：首先将ELQ图像投影到中间低质量(LQ)流形，然后使用现成的BIR模型恢复为HQ。


<details>
  <summary>Details</summary>
Motivation: 现有的盲图像恢复方法在处理极端盲图像恢复(EBIR)时表现不佳，当输入图像遭受严重复合退化时，直接学习从ELQ到HQ的映射由于巨大的领域差距而困难，往往导致不自然的伪影和细节丢失。

Method: 基于信息论视角，将图像恢复视为信息瓶颈问题。首先学习一个投影器将ELQ图像映射到中间LQ流形，然后使用冻结的现成BIR模型将中间图像恢复为HQ。设计了理论驱动的目标函数来训练投影器。

Result: 在严重退化机制下的广泛实验证明了该方法的有效性，支持推理时提示优化和即插即用地增强现有图像恢复模型，无需微调。

Conclusion: 该框架通过分解恢复过程有效解决了EBIR问题，提供了理论支撑的稳定训练方法，能够增强现有模型性能且无需额外训练。

Abstract: Blind Image Restoration (BIR) methods have achieved remarkable success but falter when faced with Extreme Blind Image Restoration (EBIR), where inputs suffer from severe, compounded degradations beyond their training scope. Directly learning a mapping from extremely low-quality (ELQ) to high-quality (HQ) images is challenging due to the massive domain gap, often leading to unnatural artifacts and loss of detail. To address this, we propose a novel framework that decomposes the intractable ELQ-to-HQ restoration process. We first learn a projector that maps an ELQ image onto an intermediate, less-degraded LQ manifold. This intermediate image is then restored to HQ using a frozen, off-the-shelf BIR model. Our approach is grounded in information theory; we provide a novel perspective of image restoration as an Information Bottleneck problem and derive a theoretically-driven objective to train our projector. This loss function effectively stabilizes training by balancing a low-quality reconstruction term with a high-quality prior-matching term. Our framework enables Look Forward Once (LFO) for inference-time prompt refinement, and supports plug-and-play strengthening of existing image restoration models without need for finetuning. Extensive experiments under severe degradation regimes provide a thorough analysis of the effectiveness of our work.

</details>


### [20] [From Seeing to Predicting: A Vision-Language Framework for Trajectory Forecasting and Controlled Video Generation](https://arxiv.org/abs/2510.00806)
*Fan Yang,Zhiyang Chen,Yousong Zhu,Xin Li,Jinqiao Wang*

Main category: cs.CV

TL;DR: TrajVLM-Gen是一个两阶段物理感知图像到视频生成框架，通过预测符合物理规律的运动轨迹来生成更真实的视频


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型产生的运动存在物理不一致性，违反了真实世界动力学规律

Method: 两阶段框架：第一阶段使用视觉语言模型预测粗粒度运动轨迹；第二阶段通过基于注意力的机制进行细粒度运动精化

Result: 在UCF-101和MSR-VTT数据集上表现优异，分别达到545和539的FVD分数

Conclusion: TrajVLM-Gen能够生成物理一致的运动，在视频生成任务中优于现有方法

Abstract: Current video generation models produce physically inconsistent motion that violates real-world dynamics. We propose TrajVLM-Gen, a two-stage framework for physics-aware image-to-video generation. First, we employ a Vision Language Model to predict coarse-grained motion trajectories that maintain consistency with real-world physics. Second, these trajectories guide video generation through attention-based mechanisms for fine-grained motion refinement. We build a trajectory prediction dataset based on video tracking data with realistic motion patterns. Experiments on UCF-101 and MSR-VTT demonstrate that TrajVLM-Gen outperforms existing methods, achieving competitive FVD scores of 545 on UCF-101 and 539 on MSR-VTT.

</details>


### [21] [NSARM: Next-Scale Autoregressive Modeling for Robust Real-World Image Super-Resolution](https://arxiv.org/abs/2510.00820)
*Xiangtao Kong,Rongyuan Wu,Shuaizheng Liu,Lingchen Sun,Lei Zhang*

Main category: cs.CV

TL;DR: 提出NSARM框架，基于视觉自回归模型和下一尺度预测策略，通过两阶段训练实现高效且鲁棒的真实图像超分辨率。


<details>
  <summary>Details</summary>
Motivation: 现有Real-ISR方法存在速度与质量权衡问题：基于扩散模型的方法要么速度慢，要么质量低，且对输入图像退化程度敏感，容易产生过度增强伪影。

Method: 采用两阶段训练：首先训练转换网络将低质量图像映射到初步尺度，然后进行端到端全模型微调，基于视觉自回归模型的下一尺度预测策略。

Result: NSARM在定量和定性评估中均优于现有Real-ISR方法，保持快速推理速度，对输入图像质量具有更高鲁棒性和泛化性能。

Conclusion: NSARM作为纯自回归模型，在真实图像超分辨率任务中实现了优越的视觉结果和鲁棒性，展示了自回归模型在该领域的潜力。

Abstract: Most recent real-world image super-resolution (Real-ISR) methods employ pre-trained text-to-image (T2I) diffusion models to synthesize the high-quality image either from random Gaussian noise, which yields realistic results but is slow due to iterative denoising, or directly from the input low-quality image, which is efficient but at the price of lower output quality. These approaches train ControlNet or LoRA modules while keeping the pre-trained model fixed, which often introduces over-enhanced artifacts and hallucinations, suffering from the robustness to inputs of varying degradations. Recent visual autoregressive (AR) models, such as pre-trained Infinity, can provide strong T2I generation capabilities while offering superior efficiency by using the bitwise next-scale prediction strategy. Building upon next-scale prediction, we introduce a robust Real-ISR framework, namely Next-Scale Autoregressive Modeling (NSARM). Specifically, we train NSARM in two stages: a transformation network is first trained to map the input low-quality image to preliminary scales, followed by an end-to-end full-model fine-tuning. Such a comprehensive fine-tuning enhances the robustness of NSARM in Real-ISR tasks without compromising its generative capability. Extensive quantitative and qualitative evaluations demonstrate that as a pure AR model, NSARM achieves superior visual results over existing Real-ISR methods while maintaining a fast inference speed. Most importantly, it demonstrates much higher robustness to the quality of input images, showing stronger generalization performance. Project page: https://github.com/Xiangtaokong/NSARM

</details>


### [22] [Can World Models Benefit VLMs for World Dynamics?](https://arxiv.org/abs/2510.00855)
*Kevin Zhang,Kuangzhi Ge,Xiaowei Chi,Renrui Zhang,Shaojun Shi,Zhen Dong,Sirui Han,Shanghang Zhang*

Main category: cs.CV

TL;DR: 该研究探索将视频生成世界模型作为视觉编码器用于多模态理解任务，提出了World-Language Models (WorldLMs)框架，其中最佳变体DyVA在空间推理和多帧推理方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着生成式世界模型在视频数据上的训练日益强大，研究者希望探索它们是否能替代传统视觉编码器范式，用于通用多模态理解任务。

Method: 将视频扩散模型重新用作生成编码器，执行单步去噪并将生成的潜在表示作为视觉嵌入，构建World-Language Models (WorldLMs)。

Result: WorldLMs能够捕获对下游理解有用的潜在特征，在空间推理任务上显著超越开源和专有基线模型，达到最先进或相当的性能水平。

Conclusion: 从视频预训练中继承的运动一致性内部化使WorldLMs具有优势，这项研究为利用世界模型先验的新一代视觉语言模型开辟了道路。

Abstract: Trained on internet-scale video data, generative world models are increasingly recognized as powerful world simulators that can generate consistent and plausible dynamics over structure, motion, and physics. This raises a natural question: with the advent of strong video foundational models, might they supplant conventional vision encoder paradigms for general-purpose multimodal understanding? While recent studies have begun to explore the potential of world models on common vision tasks, these explorations typically lack a systematic investigation of generic, multimodal tasks. In this work, we strive to investigate the capabilities when world model priors are transferred into Vision-Language Models: we re-purpose a video diffusion model as a generative encoder to perform a single denoising step and treat the resulting latents as a set of visual embedding. We empirically investigate this class of models, which we refer to as World-Language Models (WorldLMs), and we find that generative encoders can capture latents useful for downstream understanding that show distinctions from conventional encoders. Naming our best-performing variant Dynamic Vision Aligner (DyVA), we further discover that this method significantly enhances spatial reasoning abilities and enables single-image models to perform multi-frame reasoning. Through the curation of a suite of visual reasoning tasks, we find DyVA to surpass both open-source and proprietary baselines, achieving state-of-the-art or comparable performance. We attribute these gains to WorldLM's inherited motion-consistency internalization from video pre-training. Finally, we systematically explore extensive model designs to highlight promising directions for future work. We hope our study can pave the way for a new family of VLMs that leverage priors from world models and are on a promising path towards generalist vision learners.

</details>


### [23] [Gather-Scatter Mamba: Accelerating Propagation with Efficient State Space Model](https://arxiv.org/abs/2510.00862)
*Hyun-kyu Ko,Youbin Kim,Jihyeon Park,Dongheok Park,Gyeongjin Kang,Wonjun Cho,Hyung Yi,Eunbyung Park*

Main category: cs.CV

TL;DR: 提出了GSMamba，一种结合Mamba选择性状态空间模型和移位窗口自注意力的混合架构，用于视频超分辨率，通过Gather-Scatter Mamba机制解决遮挡问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于RNN的VSR方法存在梯度消失、缺乏并行性和推理速度慢的问题，而Transformer在长序列上存在二次复杂度问题。Mamba虽然提供了线性时间复杂度的选择性状态转移，但难以捕捉细粒度空间依赖。

Method: 结合移位窗口自注意力进行空间上下文聚合，使用Mamba选择性扫描进行高效时间传播，并引入Gather-Scatter Mamba机制在Mamba传播前后对特征进行扭曲和重分布。

Result: GSMamba能够有效减少遮挡伪影，确保聚合信息在所有帧间的有效重分布，同时保持高效的时间传播能力。

Conclusion: 提出的混合架构成功结合了Mamba的高效时间建模能力和自注意力的空间上下文聚合能力，为视频超分辨率提供了有效的解决方案。

Abstract: State Space Models (SSMs)-most notably RNNs-have historically played a central role in sequential modeling. Although attention mechanisms such as Transformers have since dominated due to their ability to model global context, their quadratic complexity and limited scalability make them less suited for long sequences. Video super-resolution (VSR) methods have traditionally relied on recurrent architectures to propagate features across frames. However, such approaches suffer from well-known issues including vanishing gradients, lack of parallelism, and slow inference speed. Recent advances in selective SSMs like Mamba offer a compelling alternative: by enabling input-dependent state transitions with linear-time complexity, Mamba mitigates these issues while maintaining strong long-range modeling capabilities. Despite this potential, Mamba alone struggles to capture fine-grained spatial dependencies due to its causal nature and lack of explicit context aggregation. To address this, we propose a hybrid architecture that combines shifted window self-attention for spatial context aggregation with Mamba-based selective scanning for efficient temporal propagation. Furthermore, we introduce Gather-Scatter Mamba (GSM), an alignment-aware mechanism that warps features toward a center anchor frame within the temporal window before Mamba propagation and scatters them back afterward, effectively reducing occlusion artifacts and ensuring effective redistribution of aggregated information across all frames. The official implementation is provided at: https://github.com/Ko-Lani/GSMamba.

</details>


### [24] [Equivariant Splitting: Self-supervised learning from incomplete data](https://arxiv.org/abs/2510.00929)
*Victor Sechaud,Jérémy Scanvic,Quentin Barthélemy,Patrice Abry,Julián Tachella*

Main category: cs.CV

TL;DR: 提出了一种新的自监督学习策略，用于在测量仅通过单一不完整观测模型获得的情况下训练重建网络，通过结合自监督分裂损失和等变重建网络实现无偏监督损失估计。


<details>
  <summary>Details</summary>
Motivation: 解决在无法获取真实参考数据或获取成本高昂的情况下，基于学习的逆问题解决方案的需求，特别是在测量仅通过单一不完整观测模型获得的挑战性场景。

Method: 引入了重建网络等变性的新定义，将自监督分裂损失与等变重建网络相结合，从而获得监督损失的无偏估计。

Result: 在图像修复、加速磁共振成像和压缩感知等实验中，该方法在高度秩不足前向模型设置下达到了最先进的性能。

Conclusion: 所提出的自监督学习策略在具有挑战性的单观测模型设置下有效，为无法获取真实参考数据的逆问题提供了可行的学习解决方案。

Abstract: Self-supervised learning for inverse problems allows to train a reconstruction network from noise and/or incomplete data alone. These methods have the potential of enabling learning-based solutions when obtaining ground-truth references for training is expensive or even impossible. In this paper, we propose a new self-supervised learning strategy devised for the challenging setting where measurements are observed via a single incomplete observation model. We introduce a new definition of equivariance in the context of reconstruction networks, and show that the combination of self-supervised splitting losses and equivariant reconstruction networks results in unbiased estimates of the supervised loss. Through a series of experiments on image inpainting, accelerated magnetic resonance imaging, and compressive sensing, we demonstrate that the proposed loss achieves state-of-the-art performance in settings with highly rank-deficient forward models.

</details>


### [25] [Looking Alike From Far to Near: Enhancing Cross-Resolution Re-Identification via Feature Vector Panning](https://arxiv.org/abs/2510.00936)
*Zanwu Liu,Chao Yuan,Bo Li,Xiaowei Zhang,Guanglin Niu*

Main category: cs.CV

TL;DR: 提出了一种轻量级的向量平移特征对齐框架，从建模分辨率特定特征差异的新视角解决跨分辨率行人重识别问题，显著优于现有方法且效率更高。


<details>
  <summary>Details</summary>
Motivation: 监控场景中相机距离变化导致行人图像分辨率差异显著，使得低分辨率图像难以与高分辨率图像匹配，限制了行人重识别性能。现有方法依赖超分辨率或联合学习，增加了复杂度且性能已达瓶颈。

Method: 基于语义方向在特征空间中的发现，提出向量平移特征对齐框架，通过建模分辨率特定特征差异来进行跨分辨率行人重识别。

Result: 在多个跨分辨率行人重识别基准测试中显著优于现有最先进基线模型，同时获得更高的效率。

Conclusion: 基于分辨率特定特征差异建模的新视角是解决跨分辨率行人重识别问题的有效方法，提出的轻量级框架在性能和效率方面都表现出优越性。

Abstract: In surveillance scenarios, varying camera distances cause significant differences among pedestrian image resolutions, making it hard to match low-resolution (LR) images with high-resolution (HR) counterparts, limiting the performance of Re-Identification (ReID) tasks. Most existing Cross-Resolution ReID (CR-ReID) methods rely on super-resolution (SR) or joint learning for feature compensation, which increases training and inference complexity and has reached a performance bottleneck in recent studies. Inspired by semantic directions in the word embedding space, we empirically discover that semantic directions implying resolution differences also emerge in the feature space of ReID, and we substantiate this finding from a statistical perspective using Canonical Correlation Analysis and Pearson Correlation Analysis. Based on this interesting finding, we propose a lightweight and effective Vector Panning Feature Alignment (VPFA) framework, which conducts CR-ReID from a novel perspective of modeling the resolution-specific feature discrepancy. Extensive experimental results on multiple CR-ReID benchmarks show that our method significantly outperforms previous state-of-the-art baseline models while obtaining higher efficiency, demonstrating the effectiveness and superiority of our model based on the new finding in this paper.

</details>


### [26] [InfVSR: Breaking Length Limits of Generic Video Super-Resolution](https://arxiv.org/abs/2510.00948)
*Ziqing Zhang,Kai Liu,Zheng Chen,Xi Li,Yucong Chen,Bingnan Duan,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: 提出了InfVSR方法，将视频超分辨率重新定义为自回归单步扩散范式，解决了长视频处理中的效率和可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频超分辨率方法在处理长序列时面临效率低下和可扩展性差的问题，需要突破这些限制。

Method: 将预训练的DiT调整为因果结构，通过滚动KV缓存和联合视觉指导保持局部和全局一致性；将扩散过程蒸馏为单步，使用补丁级像素监督和跨块分布匹配。

Result: 实现了无界长度视频的高效可扩展VSR，在质量上达到最先进水平，语义一致性增强，比现有方法如MGLD-VSR快58倍。

Conclusion: InfVSR推动了长格式视频超分辨率的前沿发展，提供了高效且可扩展的解决方案。

Abstract: Real-world videos often extend over thousands of frames. Existing video super-resolution (VSR) approaches, however, face two persistent challenges when processing long sequences: (1) inefficiency due to the heavy cost of multi-step denoising for full-length sequences; and (2) poor scalability hindered by temporal decomposition that causes artifacts and discontinuities. To break these limits, we propose InfVSR, which novelly reformulates VSR as an autoregressive-one-step-diffusion paradigm. This enables streaming inference while fully leveraging pre-trained video diffusion priors. First, we adapt the pre-trained DiT into a causal structure, maintaining both local and global coherence via rolling KV-cache and joint visual guidance. Second, we distill the diffusion process into a single step efficiently, with patch-wise pixel supervision and cross-chunk distribution matching. Together, these designs enable efficient and scalable VSR for unbounded-length videos. To fill the gap in long-form video evaluation, we build a new benchmark tailored for extended sequences and further introduce semantic-level metrics to comprehensively assess temporal consistency. Our method pushes the frontier of long-form VSR, achieves state-of-the-art quality with enhanced semantic consistency, and delivers up to 58x speed-up over existing methods such as MGLD-VSR. Code will be available at https://github.com/Kai-Liu001/InfVSR.

</details>


### [27] [JEPA-T: Joint-Embedding Predictive Architecture with Text Fusion for Image Generation](https://arxiv.org/abs/2510.00974)
*Siheng Wan,Zhengtao Yao,Zhengdao Li,Junhao Dong,Yanshu Li,Yikai Li,Linshan Li,Haoyan Xu,Yijiang Li,Zhikang Dong,Huacan Wang,Jifeng Shen*

Main category: cs.CV

TL;DR: JEPA-T是一个统一的多模态框架，通过联合嵌入预测Transformer处理图像和文本的离散标记，结合交叉注意力和原始文本嵌入注入，在ImageNet-1K上展现出强大的数据效率和开放词汇泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现代文本到图像生成中基于标记的架构在融合文本与视觉标记方面的挑战，寻求在条件强度与骨干网络通用性之间找到有效平衡。

Method: 将图像和标题编码为离散的视觉和文本标记，使用联合嵌入预测Transformer处理，在特征预测器后加入交叉注意力进行条件去噪，并在训练时注入原始文本嵌入以改善对齐。

Result: 在ImageNet-1K评估中，JEPA-T实现了强大的数据效率和开放词汇泛化能力，始终优于非融合和后期融合基线方法。

Conclusion: 晚期架构融合与目标级对齐相结合，为基于标记的文本到图像生成提供了条件强度与骨干通用性之间的有效平衡。

Abstract: Modern Text-to-Image (T2I) generation increasingly relies on token-centric architectures that are trained with self-supervision, yet effectively fusing text with visual tokens remains a challenge. We propose \textbf{JEPA-T}, a unified multimodal framework that encodes images and captions into discrete visual and textual tokens, processed by a joint-embedding predictive Transformer. To enhance fusion, we incorporate cross-attention after the feature predictor for conditional denoising while maintaining a task-agnostic backbone. Additionally, raw texts embeddings are injected prior to the flow matching loss to improve alignment during training. During inference, the same network performs both class-conditional and free-text image generation by iteratively denoising visual tokens conditioned on text. Evaluations on ImageNet-1K demonstrate that JEPA-T achieves strong data efficiency, open-vocabulary generalization, and consistently outperforms non-fusion and late-fusion baselines. Our approach shows that late architectural fusion combined with objective-level alignment offers an effective balance between conditioning strength and backbone generality in token-based T2I.The code is now available: https://github.com/justin-herry/JEPA-T.git

</details>


### [28] [SoftCFG: Uncertainty-guided Stable Guidance for Visual autoregressive Model](https://arxiv.org/abs/2510.00996)
*Dongli Xu,Aleksei Tiulpin,Matthew B. Blaschko*

Main category: cs.CV

TL;DR: 提出了SoftCFG方法，解决自回归模型中分类器自由引导存在的引导衰减和过度引导问题，通过不确定性加权引导和步长归一化来提升图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在图像生成中应用分类器自由引导时面临两个关键问题：引导衰减（条件-无条件差距随解码进展快速消失）和过度引导（强条件破坏视觉连贯性）。

Method: 提出了SoftCFG方法，通过不确定性引导的推理机制，在所有序列标记上分布自适应扰动，让每个生成的标记贡献确定性加权的引导；同时引入步长归一化来限制SoftCFG的累积扰动。

Result: 实验表明SoftCFG显著优于标准CFG，在ImageNet 256上实现了自回归模型中最先进的FID分数。

Conclusion: SoftCFG是一种无需训练、模型无关的方法，能无缝集成到现有自回归流程中，有效解决了CFG在AR模型中的引导问题。

Abstract: Autoregressive (AR) models have emerged as powerful tools for image generation by modeling images as sequences of discrete tokens. While Classifier-Free Guidance (CFG) has been adopted to improve conditional generation, its application in AR models faces two key issues: guidance diminishing, where the conditional-unconditional gap quickly vanishes as decoding progresses, and over-guidance, where strong conditions distort visual coherence. To address these challenges, we propose SoftCFG, an uncertainty-guided inference method that distributes adaptive perturbations across all tokens in the sequence. The key idea behind SoftCFG is to let each generated token contribute certainty-weighted guidance, ensuring that the signal persists across steps while resolving conflicts between text guidance and visual context. To further stabilize long-sequence generation, we introduce Step Normalization, which bounds cumulative perturbations of SoftCFG. Our method is training-free, model-agnostic, and seamlessly integrates with existing AR pipelines. Experiments show that SoftCFG significantly improves image quality over standard CFG and achieves state-of-the-art FID on ImageNet 256 among autoregressive models.

</details>


### [29] [ImageDoctor: Diagnosing Text-to-Image Generation via Grounded Image Reasoning](https://arxiv.org/abs/2510.01010)
*Yuxiang Guo,Jiang Liu,Ze Wang,Hao Chen,Ximeng Sun,Yang Zhao,Jialian Wu,Xiaodong Yu,Zicheng Liu,Emad Barsoum*

Main category: cs.CV

TL;DR: ImageDoctor是一个统一的多方面文本到图像模型评估框架，通过四个维度评估图像质量并提供像素级缺陷热图，可作为密集奖励用于模型偏好对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常使用单一标量量化生成图像质量，无法提供全面且可解释的反馈，限制了文本到图像模型评估的全面性。

Method: 引入"观察-思考-预测"范式：先定位潜在缺陷，生成推理，最后给出定量评分。基于视觉语言模型，结合监督微调和强化学习训练。

Result: 在多个数据集上与人类偏好高度一致，作为奖励模型用于偏好调优时，比基于标量的奖励模型提升10%的生成质量。

Conclusion: ImageDoctor作为评估指标有效，作为奖励模型能显著提升生成质量，为文本到图像模型提供了全面且可解释的评估框架。

Abstract: The rapid advancement of text-to-image (T2I) models has increased the need for reliable human preference modeling, a demand further amplified by recent progress in reinforcement learning for preference alignment. However, existing approaches typically quantify the quality of a generated image using a single scalar, limiting their ability to provide comprehensive and interpretable feedback on image quality. To address this, we introduce ImageDoctor, a unified multi-aspect T2I model evaluation framework that assesses image quality across four complementary dimensions: plausibility, semantic alignment, aesthetics, and overall quality. ImageDoctor also provides pixel-level flaw indicators in the form of heatmaps, which highlight misaligned or implausible regions, and can be used as a dense reward for T2I model preference alignment. Inspired by the diagnostic process, we improve the detail sensitivity and reasoning capability of ImageDoctor by introducing a "look-think-predict" paradigm, where the model first localizes potential flaws, then generates reasoning, and finally concludes the evaluation with quantitative scores. Built on top of a vision-language model and trained through a combination of supervised fine-tuning and reinforcement learning, ImageDoctor demonstrates strong alignment with human preference across multiple datasets, establishing its effectiveness as an evaluation metric. Furthermore, when used as a reward model for preference tuning, ImageDoctor significantly improves generation quality -- achieving an improvement of 10% over scalar-based reward models.

</details>


### [30] [Secure and reversible face anonymization with diffusion models](https://arxiv.org/abs/2510.01031)
*Pol Labarbarie,Vincent Itier,William Puech*

Main category: cs.CV

TL;DR: 提出首个基于扩散模型的安全、高质量可逆人脸匿名化方法，通过结合密钥与潜在人脸表示，在保持图像质量的同时实现可逆恢复


<details>
  <summary>Details</summary>
Motivation: 当前人脸匿名化方法难以在安全方案、高质量图像生成和可逆性之间取得良好平衡，扩散模型缺乏密钥机制确保只有授权方才能逆转过程

Method: 将密钥与扩散模型的潜在人脸表示相结合，使用面部掩码约束生成以保留身份无关特征，通过确定性前向和后向扩散过程确保原始人脸可恢复

Result: 该方法生成的匿名化人脸与原始人脸的视觉相似度低于其他先前工作，同时保持高质量图像

Conclusion: 该方法在安全性和图像质量之间实现了良好平衡，为可逆人脸匿名化提供了有效解决方案

Abstract: Face images processed by computer vision algorithms contain sensitive personal information that malicious actors can capture without consent. These privacy and security risks highlight the need for effective face anonymization methods. Current methods struggle to propose a good trade-off between a secure scheme with high-quality image generation and reversibility for later person authentication. Diffusion-based approaches produce high-quality anonymized images but lack the secret key mechanism to ensure that only authorized parties can reverse the process. In this paper, we introduce, to our knowledge, the first secure, high-quality reversible anonymization method based on a diffusion model. We propose to combine the secret key with the latent faces representation of the diffusion model. To preserve identity-irrelevant features, generation is constrained by a facial mask, maintaining high-quality images. By using a deterministic forward and backward diffusion process, our approach enforces that the original face can be recovered with the correct secret key. We also show that the proposed method produces anonymized faces that are less visually similar to the original faces, compared to other previous work.

</details>


### [31] [Authentic Discrete Diffusion Model](https://arxiv.org/abs/2510.01047)
*Xiao Li,Jiaqi Zhang,Shuxiang Zhang,Tianshui Chen,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: 提出Authentic Discrete Diffusion (ADD)框架，在one-hot空间中保持核心扩散特性，通过协调机制重新定义伪离散扩散方法


<details>
  <summary>Details</summary>
Motivation: 传统伪离散扩散方法依赖连续潜在空间扩散或掩码策略，ADD旨在直接在one-hot空间实现真正的离散扩散

Method: 使用浮点编码的one-hot类别数据作为扩散输入，引入时间步条件交叉熵损失，在扩散模型输出和原始one-hot标签之间建立桥梁

Result: 在分类任务上优于基线方法，在图像描述生成中展现出优秀的文本生成能力

Conclusion: ADD通过协调机制在one-hot空间中实现了真正的离散扩散，连接了判别性和生成性学习

Abstract: We propose an Authentic Discrete Diffusion (ADD) framework that fundamentally redefines prior pseudo-discrete approaches by preserving core diffusion characteristics directly in the one-hot space through a suite of coordinated mechanisms. Unlike conventional "pseudo" discrete diffusion (PDD) methods, ADD reformulates the diffusion input by directly using float-encoded one-hot class data, without relying on diffusing in the continuous latent spaces or masking policies. At its core, a timestep-conditioned cross-entropy loss is introduced between the diffusion model's outputs and the original one-hot labels. This synergistic design establishes a bridge between discriminative and generative learning. Our experiments demonstrate that ADD not only achieves superior performance on classification tasks compared to the baseline, but also exhibits excellent text generation capabilities on Image captioning. Extensive ablations validate the measurable gains of each component.

</details>


### [32] [Instant4D: 4D Gaussian Splatting in Minutes](https://arxiv.org/abs/2510.01119)
*Zhanpeng Luo,Haoxi Ran,Li Lu*

Main category: cs.CV

TL;DR: Instant4D是一个单目重建系统，使用原生4D表示从非标定视频中快速重建动态场景，在几分钟内完成处理，无需标定相机或深度传感器。


<details>
  <summary>Details</summary>
Motivation: 动态视图合成虽有进展，但从非标定、随意拍摄的视频重建场景仍具挑战，主要问题包括优化速度慢和参数估计复杂。

Method: 首先通过深度视觉SLAM进行几何恢复，然后通过网格剪枝优化场景表示，引入简化的4D高斯表示高效处理时间动态。

Result: 显著减少冗余同时保持几何完整性，模型大小降至原始10%以下，实现30倍加速，训练时间控制在2分钟内，在多个基准测试中保持竞争力。

Conclusion: 该方法在Dycheck数据集上10分钟内重建单个视频，适用于200帧典型视频，并展示了在野外视频中的泛化能力。

Abstract: Dynamic view synthesis has seen significant advances, yet reconstructing scenes from uncalibrated, casual video remains challenging due to slow optimization and complex parameter estimation. In this work, we present Instant4D, a monocular reconstruction system that leverages native 4D representation to efficiently process casual video sequences within minutes, without calibrated cameras or depth sensors. Our method begins with geometric recovery through deep visual SLAM, followed by grid pruning to optimize scene representation. Our design significantly reduces redundancy while maintaining geometric integrity, cutting model size to under 10% of its original footprint. To handle temporal dynamics efficiently, we introduce a streamlined 4D Gaussian representation, achieving a 30x speed-up and reducing training time to within two minutes, while maintaining competitive performance across several benchmarks. Our method reconstruct a single video within 10 minutes on the Dycheck dataset or for a typical 200-frame video. We further apply our model to in-the-wild videos, showcasing its generalizability. Our project website is published at https://instant4d.github.io/.

</details>


### [33] [Strategic Fusion of Vision Language Models: Shapley-Credited Context-Aware Dawid-Skene for Multi-Label Tasks in Autonomous Driving](https://arxiv.org/abs/2510.01126)
*Yuxiang Feng,Keyang Zhang,Hassane Ouchouid,Ashwil Kaniamparambil,Ioannis Souflas,Panagiotis Angeloudis*

Main category: cs.CV

TL;DR: 提出Shapley-credited Context-Aware Dawid-Skene with Agreement方法，用于自动驾驶车辆中多标签视觉语言模型的融合，显著降低幻觉并提高可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在自动驾驶系统中应用日益广泛，但幻觉问题限制了其在安全关键管道中的可靠性，需要开发能够校准和解释的融合方法。

Method: 使用博弈论融合方法，学习每个模型、每个标签、上下文条件下的可靠性，在推理时将模型报告转换为协议保护的对数似然比，结合上下文先验和基于Shapley的团队信誉状态更新。

Result: 与最佳单模型相比，Hamming距离减少23%，Macro-F1提高55%，Micro-F1提高47%，支持VLM融合作为自动驾驶管道的校准、可解释和鲁棒决策支持组件。

Conclusion: 该方法能够放大可靠模型间的一致性，保留独特正确的单模型信号，并适应漂移，为自动驾驶系统提供可靠的视觉语言模型融合解决方案。

Abstract: Large vision-language models (VLMs) are increasingly used in autonomous-vehicle (AV) stacks, but hallucination limits their reliability in safety-critical pipelines. We present Shapley-credited Context-Aware Dawid-Skene with Agreement, a game-theoretic fusion method for multi-label understanding of ego-view dashcam video. It learns per-model, per-label, context-conditioned reliabilities from labelled history and, at inference, converts each model's report into an agreement-guardrailed log-likelihood ratio that is combined with a contextual prior and a public reputation state updated via Shapley-based team credit. The result is calibrated, thresholdable posteriors that (i) amplify agreement among reliable models, (ii) preserve uniquely correct single-model signals, and (iii) adapt to drift. To specialise general VLMs, we curate 1,000 real-world dashcam clips with structured annotations (scene description, manoeuvre recommendation, rationale) via an automatic pipeline that fuses HDD ground truth, vehicle kinematics, and YOLOv11 + BoT-SORT tracking, guided by a three-step chain-of-thought prompt; three heterogeneous VLMs are then fine-tuned with LoRA. We evaluate with Hamming distance, Micro-Macro-F1, and average per-video latency. Empirically, the proposed method achieves a 23% reduction in Hamming distance, 55% improvement in Macro-F1, and 47% improvement in Micro-F1 when comparing with the best single model, supporting VLM fusion as a calibrated, interpretable, and robust decision-support component for AV pipelines.

</details>


### [34] [Code2Video: A Code-centric Paradigm for Educational Video Generation](https://arxiv.org/abs/2510.01174)
*Yanzhe Chen,Kevin Qinghong Lin,Mike Zheng Shou*

Main category: cs.CV

TL;DR: Code2Video是一个基于代码的智能体框架，通过可执行的Python代码生成教育视频，解决了现有生成模型在专业教育视频制作中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的像素空间视频生成模型难以制作专业教育视频，因为这类视频需要学科知识、精确的视觉结构和连贯的过渡。通过可渲染环境的代码控制能更好地满足这些需求。

Method: 框架包含三个协作智能体：Planner（规划内容流程和视觉资源）、Coder（将指令转换为可执行代码，包含范围引导的自动修复）、Critic（使用视觉语言模型优化空间布局和清晰度）。

Result: 在MMMC基准测试中，Code2Video相比直接代码生成提升了40%的效果，生成的视频质量可与人工制作的教程相媲美。

Conclusion: Code2Video展示了作为可扩展、可解释和可控方法的潜力，为教育视频生成提供了新的解决方案。

Abstract: While recent generative models advance pixel-space video synthesis, they remain limited in producing professional educational videos, which demand disciplinary knowledge, precise visual structures, and coherent transitions, limiting their applicability in educational scenarios. Intuitively, such requirements are better addressed through the manipulation of a renderable environment, which can be explicitly controlled via logical commands (e.g., code). In this work, we propose Code2Video, a code-centric agent framework for generating educational videos via executable Python code. The framework comprises three collaborative agents: (i) Planner, which structures lecture content into temporally coherent flows and prepares corresponding visual assets; (ii) Coder, which converts structured instructions into executable Python codes while incorporating scope-guided auto-fix to enhance efficiency; and (iii) Critic, which leverages vision-language models (VLM) with visual anchor prompts to refine spatial layout and ensure clarity. To support systematic evaluation, we build MMMC, a benchmark of professionally produced, discipline-specific educational videos. We evaluate MMMC across diverse dimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and particularly, TeachQuiz, a novel end-to-end metric that quantifies how well a VLM, after unlearning, can recover knowledge by watching the generated videos. Our results demonstrate the potential of Code2Video as a scalable, interpretable, and controllable approach, achieving 40% improvement over direct code generation and producing videos comparable to human-crafted tutorials. The code and datasets are available at https://github.com/showlab/Code2Video.

</details>


### [35] [EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory](https://arxiv.org/abs/2510.01183)
*Jiahao Wang,Luoxin Ye,TaiMing Lu,Junfei Xiao,Jiahan Zhang,Yuxiang Guo,Xijun Liu,Rama Chellappa,Cheng Peng,Alan Yuille,Jieneng Chen*

Main category: cs.CV

TL;DR: EvoWorld是一个结合全景视频生成与演化3D记忆的世界模型，通过利用演化的3D重建作为显式空间引导，显著提升视觉真实感和几何一致性。


<details>
  <summary>Details</summary>
Motivation: 受人类能够在脑海中探索和重放3D环境的能力启发，旨在实现空间一致的长时程探索。

Method: 首先通过具有细粒度视图控制的视频生成器生成未来视频帧，然后使用前馈即插即用transformer演化场景的3D重建，最后基于这个演化显式3D记忆的几何重投影合成未来帧。

Result: 在合成户外环境、Habitat室内场景和真实场景的基准测试中，演化3D记忆显著提高了视觉保真度并保持了空间场景一致性。

Conclusion: EvoWorld代表了向长时程空间一致世界建模的重要进展，演化3D记忆相比现有方法在视觉真实性和空间一致性方面有显著提升。

Abstract: Humans possess a remarkable ability to mentally explore and replay 3D environments they have previously experienced. Inspired by this mental process, we present EvoWorld: a world model that bridges panoramic video generation with evolving 3D memory to enable spatially consistent long-horizon exploration. Given a single panoramic image as input, EvoWorld first generates future video frames by leveraging a video generator with fine-grained view control, then evolves the scene's 3D reconstruction using a feedforward plug-and-play transformer, and finally synthesizes futures by conditioning on geometric reprojections from this evolving explicit 3D memory. Unlike prior state-of-the-arts that synthesize videos only, our key insight lies in exploiting this evolving 3D reconstruction as explicit spatial guidance for the video generation process, projecting the reconstructed geometry onto target viewpoints to provide rich spatial cues that significantly enhance both visual realism and geometric consistency. To evaluate long-range exploration capabilities, we introduce the first comprehensive benchmark spanning synthetic outdoor environments, Habitat indoor scenes, and challenging real-world scenarios, with particular emphasis on loop-closure detection and spatial coherence over extended trajectories. Extensive experiments demonstrate that our evolving 3D memory substantially improves visual fidelity and maintains spatial scene coherence compared to existing approaches, representing a significant advance toward long-horizon spatially consistent world modeling.

</details>


### [36] [IMAGEdit: Let Any Subject Transform](https://arxiv.org/abs/2510.01186)
*Fei Shen,Weihao Xu,Rui Yan,Dong Zhang,Xiangbo Shu,Jinhui Tang*

Main category: cs.CV

TL;DR: IMAGEdit是一个无需训练的视频多主体编辑框架，能够同时编辑任意数量的视频主体外观，同时保留非目标区域，无需微调或重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法在多主体编辑时存在多模态条件不足和掩码边界纠缠的问题，限制了其适用性。

Method: 通过提示引导的多模态对齐模块和基于先验的掩码重定向模块，提供鲁棒的多模态条件和精确的掩码序列，然后利用预训练的掩码驱动视频生成模型合成编辑后的视频。

Result: 在新建的多主体基准MSVBench上的大量实验表明，IMAGEdit持续超越最先进的方法。

Conclusion: IMAGEdit具有强大的泛化能力，显著扩展了视频编辑的适用性，且与任何掩码驱动视频生成模型兼容，显著提升了整体性能。

Abstract: In this paper, we present IMAGEdit, a training-free framework for any number of video subject editing that manipulates the appearances of multiple designated subjects while preserving non-target regions, without finetuning or retraining. We achieve this by providing robust multimodal conditioning and precise mask sequences through a prompt-guided multimodal alignment module and a prior-based mask retargeting module. We first leverage large models' understanding and generation capabilities to produce multimodal information and mask motion sequences for multiple subjects across various types. Then, the obtained prior mask sequences are fed into a pretrained mask-driven video generation model to synthesize the edited video. With strong generalization capability, IMAGEdit remedies insufficient prompt-side multimodal conditioning and overcomes mask boundary entanglement in videos with any number of subjects, thereby significantly expanding the applicability of video editing. More importantly, IMAGEdit is compatible with any mask-driven video generation model, significantly improving overall performance. Extensive experiments on our newly constructed multi-subject benchmark MSVBench verify that IMAGEdit consistently surpasses state-of-the-art methods. Code, models, and datasets are publicly available at https://github.com/XWH-A/IMAGEdit.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [37] [Learning Energy-based Variational Latent Prior for VAEs](https://arxiv.org/abs/2510.00260)
*Debottam Dutta,Chaitanya Amballa,Zhongweiyang Xu,Yu-Lin Wei,Romit Roy Choudhury*

Main category: cs.LG

TL;DR: 提出EVaLP方法，使用能量基模型作为VAE先验，通过变分方法解决EBM归一化常数问题，实现高效采样和改善生成质量


<details>
  <summary>Details</summary>
Motivation: 解决VAE中'先验空洞'问题，即先验高概率区域在后验中概率低，导致生成样本模糊和不一致

Method: 将先验建模为能量基模型，引入变分方法处理EBM归一化常数，使用采样器网络近似变分形式，采用交替优化训练

Result: 相比SOTA基线，在图像生成质量、减少先验空洞和采样效率方面均有改进

Conclusion: EVaLP方法成功解决了VAE先验空洞问题，实现了灵活先验匹配和高效采样

Abstract: Variational Auto-Encoders (VAEs) are known to generate blurry and inconsistent samples. One reason for this is the "prior hole" problem. A prior hole refers to regions that have high probability under the VAE's prior but low probability under the VAE's posterior. This means that during data generation, high probability samples from the prior could have low probability under the posterior, resulting in poor quality data. Ideally, a prior needs to be flexible enough to match the posterior while retaining the ability to generate samples fast. Generative models continue to address this tradeoff. This paper proposes to model the prior as an energy-based model (EBM). While EBMs are known to offer the flexibility to match posteriors (and also improving the ELBO), they are traditionally slow in sample generation due to their dependency on MCMC methods. Our key idea is to bring a variational approach to tackle the normalization constant in EBMs, thus bypassing the expensive MCMC approaches. The variational form can be approximated with a sampler network, and we show that such an approach to training priors can be formulated as an alternating optimization problem. Moreover, the same sampler reduces to an implicit variational prior during generation, providing efficient and fast sampling. We compare our Energy-based Variational Latent Prior (EVaLP) method to multiple SOTA baselines and show improvements in image generation quality, reduced prior holes, and better sampling efficiency.

</details>


### [38] [Plug-and-Play Prompt Refinement via Latent Feedback for Diffusion Model Alignment](https://arxiv.org/abs/2510.00430)
*Suhyeon Lee,Jong Chul Ye*

Main category: cs.LG

TL;DR: PromptLoop是一个基于强化学习的插件式框架，通过多模态大语言模型在扩散模型采样过程中迭代更新提示，实现有效的奖励优化和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于强化学习的扩散模型微调方法在泛化性、组合性和抗奖励攻击方面存在不足，而现有的提示优化方法大多采用前馈方式，未能充分利用强化学习的序列特性。

Method: 训练多模态大语言模型使用强化学习，基于扩散模型的中间潜状态迭代更新提示，而不是修改扩散模型权重，实现结构上类似于Diffusion RL但保持提示对齐的灵活性。

Result: 在各种奖励函数和扩散骨干网络上的广泛实验表明，PromptLoop能有效优化奖励、无缝泛化到未见模型、与现有对齐方法正交组合，并减轻过优化和奖励攻击问题。

Conclusion: PromptLoop提供了一个有效的插件式RL框架，通过迭代提示优化实现了扩散模型的奖励对齐，同时保持了泛化性和组合性。

Abstract: Despite the recent progress, reinforcement learning (RL)-based fine-tuning of diffusion models often struggles with generalization, composability, and robustness against reward hacking. Recent studies have explored prompt refinement as a modular alternative, but most adopt a feed-forward approach that applies a single refined prompt throughout the entire sampling trajectory, thereby failing to fully leverage the sequential nature of reinforcement learning. To address this, here we introduce PromptLoop, a plug-and-play RL framework that incorporates latent feedback into step-wise prompt refinement. Rather than modifying diffusion model weights, a multimodal large language model (MLLM) is trained with RL to iteratively update prompts based on intermediate latent states of diffusion models. This design achieves a structural analogy to the Diffusion RL approach, while retaining the flexibility and generality of prompt-based alignment. Extensive experiments across diverse reward functions and diffusion backbones demonstrate that PromptLoop (i) achieves effective reward optimization, (ii) generalizes seamlessly to unseen models, (iii) composes orthogonally with existing alignment methods, and (iv) mitigates over-optimization and reward hacking.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [39] [Object-AVEdit: An Object-level Audio-Visual Editing Model](https://arxiv.org/abs/2510.00050)
*Youquan Fu,Ruiyang Si,Hongfa Wang,Dongzhan Zhou,Jiacheng Sun,Ping Luo,Di Hu,Hongyuan Zhang,Xuelong Li*

Main category: cs.MM

TL;DR: Object-AVEdit是一个基于反演-再生范式的对象级音视频编辑方法，通过开发词-发声对象对齐的音频生成模型和整体优化的编辑算法，实现了跨模态的对象添加、替换和删除操作。


<details>
  <summary>Details</summary>
Motivation: 当前音视频编辑模型难以处理对象级的跨模态操作，需要能够在编辑过程中同时处理音频和视觉模态的对象操作，并保持源实例的结构信息。

Method: 基于反演-再生范式，开发词-发声对象对齐的音频生成模型来弥合音频和视频生成模型之间的对象可控性差距，并提出整体优化的编辑算法来保证信息保留和更好的再生效果。

Result: 实验表明该方法在音视频对象级编辑任务中取得了先进成果，具有良好的音视频语义对齐效果，音频生成模型也表现出先进性能。

Conclusion: Object-AVEdit成功实现了对象级音视频编辑，通过创新的音频生成模型和编辑算法解决了跨模态对象操作的挑战。

Abstract: There is a high demand for audio-visual editing in video post-production and the film making field. While numerous models have explored audio and video editing, they struggle with object-level audio-visual operations. Specifically, object-level audio-visual editing requires the ability to perform object addition, replacement, and removal across both audio and visual modalities, while preserving the structural information of the source instances during the editing process. In this paper, we present \textbf{Object-AVEdit}, achieving the object-level audio-visual editing based on the inversion-regeneration paradigm. To achieve the object-level controllability during editing, we develop a word-to-sounding-object well-aligned audio generation model, bridging the gap in object-controllability between audio and current video generation models. Meanwhile, to achieve the better structural information preservation and object-level editing effect, we propose an inversion-regeneration holistically-optimized editing algorithm, ensuring both information retention during the inversion and better regeneration effect. Extensive experiments demonstrate that our editing model achieved advanced results in both audio-video object-level editing tasks with fine audio-visual semantic alignment. In addition, our developed audio generation model also achieved advanced performance. More results on our project page: https://gewu-lab.github.io/Object_AVEdit-website/.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [40] [Latent Representation Learning from 3D Brain MRI for Interpretable Prediction in Multiple Sclerosis](https://arxiv.org/abs/2510.00051)
*Trinh Ngoc Huynh,Nguyen Duc Kien,Nguyen Hai Anh,Dinh Tran Hiep,Manuela Vaneckova,Tomas Uher,Jeroen Van Schependom,Stijn Denissen,Tran Quoc Long,Nguyen Linh Trung,Guy Nagels*

Main category: eess.IV

TL;DR: InfoVAE-Med3D是一种用于3D脑部MRI的潜在表示学习方法，旨在发现认知衰退的可解释生物标志物。该方法扩展了InfoVAE，通过最大化图像与潜在变量之间的互信息，生成紧凑、结构化的嵌入，保留临床意义内容。在两个队列上的评估显示，该方法在脑龄和SDMT回归任务中表现优异，优于其他VAE变体。


<details>
  <summary>Details</summary>
Motivation: 标准统计模型和浅层机器学习方法缺乏效力，而大多数深度学习方法表现为黑盒。需要一种既能保持预测性能又具有可解释性的方法，用于分析神经疾病中的认知衰退。

Method: 扩展InfoVAE方法，明确最大化图像与潜在变量之间的互信息，生成紧凑且结构化的嵌入。在两个数据集上评估：大型健康对照组（n=6527）和临床多发性硬化数据集（n=904）。

Result: 学习的潜在变量支持准确的脑龄和SDMT回归，保留关键医学属性，并形成直观的聚类以帮助解释。在重建和下游预测任务中，InfoVAE-Med3D始终优于其他VAE变体，表明在嵌入空间中具有更强的信息捕获能力。

Conclusion: InfoVAE-Med3D通过结合预测性能和可解释性，为基于MRI的生物标志物和神经疾病认知衰退的透明分析提供了实用路径。

Abstract: We present InfoVAE-Med3D, a latent-representation learning approach for 3D brain MRI that targets interpretable biomarkers of cognitive decline. Standard statistical models and shallow machine learning often lack power, while most deep learning methods behave as black boxes. Our method extends InfoVAE to explicitly maximize mutual information between images and latent variables, producing compact, structured embeddings that retain clinically meaningful content. We evaluate on two cohorts: a large healthy-control dataset (n=6527) with chronological age, and a clinical multiple sclerosis dataset from Charles University in Prague (n=904) with age and Symbol Digit Modalities Test (SDMT) scores. The learned latents support accurate brain-age and SDMT regression, preserve key medical attributes, and form intuitive clusters that aid interpretation. Across reconstruction and downstream prediction tasks, InfoVAE-Med3D consistently outperforms other VAE variants, indicating stronger information capture in the embedding space. By uniting predictive performance with interpretability, InfoVAE-Med3D offers a practical path toward MRI-based biomarkers and more transparent analysis of cognitive deterioration in neurological disease.

</details>
