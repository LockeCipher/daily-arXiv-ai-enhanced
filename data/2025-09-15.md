<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 15]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Chord: Chain of Rendering Decomposition for PBR Material Estimation from Generated Texture Images](https://arxiv.org/abs/2509.09952)
*Zhi Ying,Boxiang Rong,Jingyu Wang,Maoyuan Xu*

Main category: cs.GR

TL;DR: 一种新题的两阶段生成-估计框架，通过细调扩散模型生成平铺纹理图像，然后采用链式分解方案预测SVBRDF通道，实现高质量、灵活的PBR材质生成与重建


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在PBR材质合成中存在的质量不高、灵活性差和用户控制不足等问题

Method: 两阶段框架：生成阶段使用细调扩散模型生成平铺纹理图像，估计阶段采用链式分解方案通过单步图像条件扩散模型序列预测SVBRDF通道

Result: 方法在材质生成和估计方面表现優异，在生成纹理和实际照片上都显示出强劲的稳健性

Conclusion: 该框架高效、高质量且支持灵活的用户控制，能够应用于文本到材质、图像到材质、结构指导生成和材质编辑等多种场景

Abstract: Material creation and reconstruction are crucial for appearance modeling but traditionally require significant time and expertise from artists. While recent methods leverage visual foundation models to synthesize PBR materials from user-provided inputs, they often fall short in quality, flexibility, and user control. We propose a novel two-stage generate-and-estimate framework for PBR material generation. In the generation stage, a fine-tuned diffusion model synthesizes shaded, tileable texture images aligned with user input. In the estimation stage, we introduce a chained decomposition scheme that sequentially predicts SVBRDF channels by passing previously extracted representation as input into a single-step image-conditional diffusion model. Our method is efficient, high quality, and enables flexible user control. We evaluate our approach against existing material generation and estimation methods, demonstrating superior performance. Our material estimation method shows strong robustness on both generated textures and in-the-wild photographs. Furthermore, we highlight the flexibility of our framework across diverse applications, including text-to-material, image-to-material, structure-guided generation, and material editing.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [World Modeling with Probabilistic Structure Integration](https://arxiv.org/abs/2509.09737)
*Klemen Kotar,Wanhee Lee,Rahul Venkatesh,Honglin Chen,Daniel Bear,Jared Watrous,Simon Kim,Khai Loong Aw,Lilian Naing Chen,Stefan Stojanov,Kevin Feigelis,Imran Thobani,Alex Durango,Khaled Jedoui,Atlas Kazemian,Dan Yamins*

Main category: cs.CV

TL;DR: PSI是一个从数据中学习可控制、可提示世界模型的系统，通过概率预测、结构提取和集成三个步骤循环提升模型能力，在视频数据上实现了先进的视频预测和理解能力。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够从数据中自动学习丰富控制结构和灵活提示功能的世界模型系统，实现对视频数据的高效理解和预测。

Method: 三步骤循环：1)构建概率图模型Psi；2)通过因果推断零样本提取底层低维结构；3)将结构转换为新token类型并集成回训练中。

Result: 在1.4万亿视频token上训练，实现了最先进的光流、自监督深度和对象分割，支持完整的预测改进循环。

Conclusion: PSI系统通过循环结构集成有效提升了世界模型的预测能力和控制灵活性，为视频理解和生成提供了强大的基础模型。

Abstract: We present Probabilistic Structure Integration (PSI), a system for learning richly controllable and flexibly promptable world models from data. PSI consists of a three-step cycle. The first step, Probabilistic prediction, involves building a probabilistic graphical model Psi of the data, in the form of a random-access autoregressive sequence model. Psi supports a complete set of learned conditional distributions describing the dependence of any variables in the data on any other set of variables. In step 2, Structure extraction, we show how to extract underlying low-dimensional properties in the data, corresponding to a diverse set of meaningful "intermediate structures", in a zero-shot fashion via causal inference on Psi. Step 3, Integration, completes the cycle by converting these structures into new token types that are then continually mixed back into the training diet as conditioning signals and prediction targets. Each such cycle augments the capabilities of Psi, both allowing it to model the underlying data better, and creating new control handles -- akin to an LLM-like universal prompting language. We train an instance of Psi on 1.4 trillion tokens of internet video data; we use it to perform a variety of useful video prediction and understanding inferences; we extract state-of-the-art optical flow, self-supervised depth and object segmentation; and we use these structures to support a full cycle of predictive improvements.

</details>


### [3] [Images in Motion?: A First Look into Video Leakage in Collaborative Deep Learning](https://arxiv.org/abs/2509.09742)
*Md Fazle Rasul,Alanood Alqobaisi,Bruhadeshwar Bezawada,Indrakshi Ray*

Main category: cs.CV

TL;DR: 本文首次分析了联邦学习中视频数据的梯度反演攻击风险，发现使用特征提取器能提供更好保护，但攻击者仍可通过超分辨率技术重建高质量视频，证明了视频数据泄露是真实威胁。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的核心隐私保护原则是通过交换模型更新而非原始数据来保护隐私，但梯度反演攻击能够从共享梯度中重建原始训练数据。虽然这种攻击对图像、文本和表格数据的影响已知，但对视频数据的影响尚未研究。

Method: 评估两种视频分类方法：使用预训练特征提取器的方法和处理原始视频帧的简单变换方法。测试了攻击者在拥有零个、一个或多个参考帧的情况下进行梯度反演攻击，并使用图像超分辨率技术提升重建视频质量。

Result: 特征提取器对梯度反演攻击具有更强的抵抗力，但攻击者仍可通过超分辨率技术重建高质量视频。实验验证了在不同参考帧访问场景下攻击的可行性，发现分类器复杂度不足时仍可能发生数据泄露。

Conclusion: 视频数据在联邦学习中存在梯度反演攻击的泄露风险，使用特征提取器能提供一定保护但并非绝对安全，需要进一步研究泄露发生的具体条件和防护措施。

Abstract: Federated learning (FL) allows multiple entities to train a shared model collaboratively. Its core, privacy-preserving principle is that participants only exchange model updates, such as gradients, and never their raw, sensitive data. This approach is fundamental for applications in domains where privacy and confidentiality are important. However, the security of this very mechanism is threatened by gradient inversion attacks, which can reverse-engineer private training data directly from the shared gradients, defeating the purpose of FL. While the impact of these attacks is known for image, text, and tabular data, their effect on video data remains an unexamined area of research. This paper presents the first analysis of video data leakage in FL using gradient inversion attacks. We evaluate two common video classification approaches: one employing pre-trained feature extractors and another that processes raw video frames with simple transformations. Our initial results indicate that the use of feature extractors offers greater resilience against gradient inversion attacks. We also demonstrate that image super-resolution techniques can enhance the frames extracted through gradient inversion attacks, enabling attackers to reconstruct higher-quality videos. Our experiments validate this across scenarios where the attacker has access to zero, one, or more reference frames from the target environment. We find that although feature extractors make attacks more challenging, leakage is still possible if the classifier lacks sufficient complexity. We, therefore, conclude that video data leakage in FL is a viable threat, and the conditions under which it occurs warrant further investigation.

</details>


### [4] [Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey](https://arxiv.org/abs/2509.09971)
*Aupendu Kar,Vishnu Raj,Guan-Ming Su*

Main category: cs.CV

TL;DR: 这篇调研论文系统述了事件相机与传统框率相机融合的发展，重点关注在视频恢复和3D重建任务中的深度学习方法。包括时间增强、空间增强和开放数据集等多个方面。


<details>
  <summary>Details</summary>
Motivation: 事件相机作为一种新兴传感器，具有低延迟、低功耗和超高损失率等优势，但需要与传统框率相机融合才能充分发挥其潜力。调研的动机是系统整理这种融合技术的发展进展，为进一步研究提供基础。

Method: 论文采用系统性调研方法，从两个维度全面评估深度学习在图像/视频增强和恢复中的贡献：时间增强（包括帧间插值、运动去模糊）和空间增强（包括超分辨率、低光增强、HDR增强和伪影减少）。同时讨论事件驱动融合对3D重建领域的影响。

Result: 调研系统整理了最新进展和见解，编写了完整的开放数据集清单，支持可复现研究和测试。论文展示了事件相机与传统框率相机融合在挑战性条件下改善视觉质量的潜力。

Conclusion: 这份调研通过汇总最新进展，期望能够激发更多研究者利用事件相机系统，特别是与深度学习结合，从而推动高级视觉媒体恢复和增强技术的发展。

Abstract: Event camera sensors are bio-inspired sensors which asynchronously capture per-pixel brightness changes and output a stream of events encoding the polarity, location and time of these changes. These systems are witnessing rapid advancements as an emerging field, driven by their low latency, reduced power consumption, and ultra-high capture rates. This survey explores the evolution of fusing event-stream captured with traditional frame-based capture, highlighting how this synergy significantly benefits various video restoration and 3D reconstruction tasks. The paper systematically reviews major deep learning contributions to image/video enhancement and restoration, focusing on two dimensions: temporal enhancement (such as frame interpolation and motion deblurring) and spatial enhancement (including super-resolution, low-light and HDR enhancement, and artifact reduction). This paper also explores how the 3D reconstruction domain evolves with the advancement of event driven fusion. Diverse topics are covered, with in-depth discussions on recent works for improving visual quality under challenging conditions. Additionally, the survey compiles a comprehensive list of openly available datasets, enabling reproducible research and benchmarking. By consolidating recent progress and insights, this survey aims to inspire further research into leveraging event camera systems, especially in combination with deep learning, for advanced visual media restoration and enhancement.

</details>


### [5] [Realism Control One-step Diffusion for Real-World Image Super-Resolution](https://arxiv.org/abs/2509.10122)
*Zongliang Wu,Siming Zheng,Peng-Tao Jiang,Xin Yuan*

Main category: cs.CV

TL;DR: RCOD框架通过潜在域分组策略和退化感知采样，在单步扩散模型中实现了保真度与真实感的灵活控制，在保持计算效率的同时提升了超分辨率性能


<details>
  <summary>Details</summary>
Motivation: 传统单步扩散方法在真实图像超分辨率任务中缺乏灵活的保真度-真实感权衡控制机制，无法像多步方法那样通过调整采样步骤来适应不同场景需求

Method: 提出RCOD框架：1）潜在域分组策略实现噪声预测阶段的显式控制；2）退化感知采样策略对齐蒸馏正则化；3）视觉提示注入模块替换文本提示，使用退化感知视觉token

Result: 在定量指标和视觉质量上均优于现有单步扩散方法，实现了推理阶段的灵活真实感控制，同时保持计算效率

Conclusion: RCOD成功解决了单步扩散模型在保真度-真实感权衡方面的局限性，为真实图像超分辨率提供了高效且可控的解决方案

Abstract: Pre-trained diffusion models have shown great potential in real-world image super-resolution (Real-ISR) tasks by enabling high-resolution reconstructions. While one-step diffusion (OSD) methods significantly improve efficiency compared to traditional multi-step approaches, they still have limitations in balancing fidelity and realism across diverse scenarios. Since the OSDs for SR are usually trained or distilled by a single timestep, they lack flexible control mechanisms to adaptively prioritize these competing objectives, which are inherently manageable in multi-step methods through adjusting sampling steps. To address this challenge, we propose a Realism Controlled One-step Diffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping strategy that enables explicit control over fidelity-realism trade-offs during the noise prediction phase with minimal training paradigm modifications and original training data. A degradation-aware sampling strategy is also introduced to align distillation regularization with the grouping strategy and enhance the controlling of trade-offs. Moreover, a visual prompt injection module is used to replace conventional text prompts with degradation-aware visual tokens, enhancing both restoration accuracy and semantic consistency. Our method achieves superior fidelity and perceptual quality while maintaining computational efficiency. Extensive experiments demonstrate that RCOD outperforms state-of-the-art OSD methods in both quantitative metrics and visual qualities, with flexible realism control capabilities in the inference stage. The code will be released.

</details>


### [6] [Scalable Training for Vector-Quantized Networks with 100% Codebook Utilization](https://arxiv.org/abs/2509.10140)
*Yifan Chang,Jie Qin,Limeng Qiao,Xiaofeng Wang,Zheng Zhu,Lin Ma,Xingang Wang*

Main category: cs.CV

TL;DR: 提出了VQBridge方法解决VQ训练不稳定性问题，通过compress-process-recover管道实现100%码本使用率，显著提升图像重建和生成性能


<details>
  <summary>Details</summary>
Motivation: 解决向量量化(VQ)训练中的不稳定性问题，包括直通估计偏差、一步滞后更新和稀疏码本梯度，这些导致重建性能不佳和码本使用率低

Method: 提出VQBridge投影器，基于映射函数方法，通过compress-process-recover管道优化码向量，结合学习退火实现稳定码本训练

Result: 实现100%码本使用率（即使262k大码本），达到最先进重建性能，与LlamaGen集成后图像生成性能超越VAR模型0.5 rFID和DiT模型0.2 rFID

Conclusion: 高质量分词器对强自回归图像生成至关重要，FVQ方法有效、可扩展且通用，在不同VQ变体中都保持有效性

Abstract: Vector quantization (VQ) is a key component in discrete tokenizers for image generation, but its training is often unstable due to straight-through estimation bias, one-step-behind updates, and sparse codebook gradients, which lead to suboptimal reconstruction performance and low codebook usage. In this work, we analyze these fundamental challenges and provide a simple yet effective solution. To maintain high codebook usage in VQ networks (VQN) during learning annealing and codebook size expansion, we propose VQBridge, a robust, scalable, and efficient projector based on the map function method. VQBridge optimizes code vectors through a compress-process-recover pipeline, enabling stable and effective codebook training. By combining VQBridge with learning annealing, our VQN achieves full (100%) codebook usage across diverse codebook configurations, which we refer to as FVQ (FullVQ). Through extensive experiments, we demonstrate that FVQ is effective, scalable, and generalizable: it attains 100% codebook usage even with a 262k-codebook, achieves state-of-the-art reconstruction performance, consistently improves with larger codebooks, higher vector channels, or longer training, and remains effective across different VQ variants. Moreover, when integrated with LlamaGen, FVQ significantly enhances image generation performance, surpassing visual autoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID, highlighting the importance of high-quality tokenizers for strong autoregressive image generation.

</details>


### [7] [On the Geometric Accuracy of Implicit and Primitive-based Representations Derived from View Rendering Constraints](https://arxiv.org/abs/2509.10241)
*Elias De Smijter,Renaud Detry,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: 本文系统比较了隐式和显式新视角合成方法在空间3D物体重建中的表现，重点评估了外观嵌入的作用。研究发现嵌入虽然能提升光度保真度，但不会显著改善几何精度，而凸样条比高斯样条能提供更紧凑、无杂乱的表示。


<details>
  <summary>Details</summary>
Motivation: 研究空间机器人应用中3D重建方法的几何精度需求，评估外观嵌入在提升重建质量方面的实际效果，为安全关键应用提供更优的表示方法。

Method: 使用SPEED+数据集，比较K-Planes、高斯样条和凸样条三种方法，分析外观嵌入对几何精度和表示效率的影响。

Result: 外观嵌入主要减少显式方法所需的基本元素数量，而非提升几何保真度；凸样条相比高斯样条能产生更紧凑且无杂乱的表示。

Conclusion: 外观嵌入在几何中心任务中存在局限性，空间场景中需要在重建质量和表示效率之间进行权衡，凸样条在安全关键应用中具有优势。

Abstract: We present the first systematic comparison of implicit and explicit Novel View Synthesis methods for space-based 3D object reconstruction, evaluating the role of appearance embeddings. While embeddings improve photometric fidelity by modeling lighting variation, we show they do not translate into meaningful gains in geometric accuracy - a critical requirement for space robotics applications. Using the SPEED+ dataset, we compare K-Planes, Gaussian Splatting, and Convex Splatting, and demonstrate that embeddings primarily reduce the number of primitives needed for explicit methods rather than enhancing geometric fidelity. Moreover, convex splatting achieves more compact and clutter-free representations than Gaussian splatting, offering advantages for safety-critical applications such as interaction and collision avoidance. Our findings clarify the limits of appearance embeddings for geometry-centric tasks and highlight trade-offs between reconstruction quality and representation efficiency in space scenarios.

</details>


### [8] [GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection](https://arxiv.org/abs/2509.10250)
*Haozhen Yan,Yan Hong,Suning Lang,Jiahui Zhan,Yikun Ji,Yujie Gao,Jun Lan,Huijia Zhu,Weiqiang Wang,Jianfu Zhang*

Main category: cs.CV

TL;DR: GAMMA是一个新的训练框架，通过减少领域偏差和增强语义对齐来提升AI生成图像检测的泛化能力，在GenImage基准上实现了5.8%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有的AI生成图像检测器虽然在分布内生成图像上表现良好，但对未见过的生成模型泛化能力有限，主要因为它们依赖生成特定的伪影（如风格先验和压缩模式）。

Method: 提出GAMMA框架，引入多样化操作策略（如基于修复的操作和语义保持扰动）确保操作内容与真实内容的一致性；采用多任务监督，使用双重分割头和分类头实现像素级来源归因；引入反向交叉注意力机制让分割头指导和纠正分类分支中的偏差表示。

Result: 在GenImage基准上实现了最先进的泛化性能，准确率提升5.8%，同时在新发布的生成模型（如GPT-4o）上保持强大的鲁棒性。

Conclusion: GAMMA通过减少领域偏差和增强语义对齐，有效解决了AI生成图像检测器的泛化问题，为检测多样化生成模型提供了有效解决方案。

Abstract: With generative models becoming increasingly sophisticated and diverse, detecting AI-generated images has become increasingly challenging. While existing AI-genereted Image detectors achieve promising performance on in-distribution generated images, their generalization to unseen generative models remains limited. This limitation is largely attributed to their reliance on generation-specific artifacts, such as stylistic priors and compression patterns. To address these limitations, we propose GAMMA, a novel training framework designed to reduce domain bias and enhance semantic alignment. GAMMA introduces diverse manipulation strategies, such as inpainting-based manipulation and semantics-preserving perturbations, to ensure consistency between manipulated and authentic content. We employ multi-task supervision with dual segmentation heads and a classification head, enabling pixel-level source attribution across diverse generative domains. In addition, a reverse cross-attention mechanism is introduced to allow the segmentation heads to guide and correct biased representations in the classification branch. Our method achieves state-of-the-art generalization performance on the GenImage benchmark, imporving accuracy by 5.8%, but also maintains strong robustness on newly released generative model such as GPT-4o.

</details>


### [9] [Robustness and Diagnostic Performance of Super-Resolution Fetal Brain MRI](https://arxiv.org/abs/2509.10257)
*Ema Masterl,Tina Vipotnik Vesnaver,Žiga Špiclin*

Main category: cs.CV

TL;DR: 这篇论文对比了三种胎儿脑部MRI超分辨率重建方法，发现NeSVoR方法具有最高的重建成功率，虽然不同方法导致体积测量差异，但诊断性能受影响较小。


<details>
  <summary>Details</summary>
Motivation: 胎儿脑部MRI采集存在分辨率低、运动位移问题，需要评估不同超分辨率重建方法在病理情况下的表现及对下游分析的影响。

Method: 对140例胎儿脑部MRI扫描（包括健康和脑室扩大病例）应用三种SRR方法（NiftyMIC、SVRTK、NeSVoR），通过BoUNTi算法进行脑结构分割，评估视觉质量、重建成功率、体积测量一致性和诊断分类性能。

Result: NeSVoR在健康和病理组都呈现最高重建成功率（>90%）。虽然不同SRR方法导致体积测量显著差异，但脑室扩大的诊断分类性能受SRR方法选择影响较小。

Conclusion: NeSVoR方法具有更好的稳健性，虽然不同SRR方法导致体积测量差异，但诊断性能很少受到影响，这为临床应用提供了重要参考。

Abstract: Fetal brain MRI relies on rapid multi-view 2D slice acquisitions to reduce motion artifacts caused by fetal movement. However, these stacks are typically low resolution, may suffer from motion corruption, and do not adequately capture 3D anatomy. Super-resolution reconstruction (SRR) methods aim to address these limitations by combining slice-to-volume registration and super-resolution techniques to generate high-resolution (HR) 3D volumes. While several SRR methods have been proposed, their comparative performance - particularly in pathological cases - and their influence on downstream volumetric analysis and diagnostic tasks remain underexplored. In this study, we applied three state-of-the-art SRR method - NiftyMIC, SVRTK, and NeSVoR - to 140 fetal brain MRI scans, including both healthy controls (HC) and pathological cases (PC) with ventriculomegaly (VM). Each HR reconstruction was segmented using the BoUNTi algorithm to extract volumes of nine principal brain structures. We evaluated visual quality, SRR success rates, volumetric measurement agreement, and diagnostic classification performance. NeSVoR demonstrated the highest and most consistent reconstruction success rate (>90%) across both HC and PC groups. Although significant differences in volumetric estimates were observed between SRR methods, classification performance for VM was not affected by the choice of SRR method. These findings highlight NeSVoR's robustness and the resilience of diagnostic performance despite SRR-induced volumetric variability.

</details>


### [10] [Mask Consistency Regularization in Object Removal](https://arxiv.org/abs/2509.10259)
*Hua Yuan,Jin Yuan,Yicheng Jiang,Yao Zhang,Xin Geng,Yong Rui*

Main category: cs.CV

TL;DR: 通过提出面具一致性正则化(MCR)训练策略，解决图像填充中的面具幻觉和面具形状偏辛问题，提升物体移除效果


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在物体移除任务中存在两个关键问题：面具幻觉（在遮罩区域生成无关内容）和面具形状偏辛（按照遮罩形状生成物体而非环境内容）

Method: 提出Mask Consistency Regularization (MCR)训练策略，在训练中引入两种面具批动：扩张和重形。扩张面具帮助对齐模型输出与周围内容，重形面具鼓励模型突破面具形状偏辛

Result: 实验表明MCR显著减少了幻觉现象和面具形状偏辛，提升了物体移除的性能

Conclusion: MCR能够生成更稳健和上下文一致的填充结果，有效解决了当前图像填充模型在物体移除任务中的关键挑战

Abstract: Object removal, a challenging task within image inpainting, involves seamlessly filling the removed region with content that matches the surrounding context. Despite advancements in diffusion models, current methods still face two critical challenges. The first is mask hallucination, where the model generates irrelevant or spurious content inside the masked region, and the second is mask-shape bias, where the model fills the masked area with an object that mimics the mask's shape rather than surrounding content. To address these issues, we propose Mask Consistency Regularization (MCR), a novel training strategy designed specifically for object removal tasks. During training, our approach introduces two mask perturbations: dilation and reshape, enforcing consistency between the outputs of these perturbed branches and the original mask. The dilated masks help align the model's output with the surrounding content, while reshaped masks encourage the model to break the mask-shape bias. This combination of strategies enables MCR to produce more robust and contextually coherent inpainting results. Our experiments demonstrate that MCR significantly reduces hallucinations and mask-shape bias, leading to improved performance in object removal.

</details>


### [11] [MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation](https://arxiv.org/abs/2509.10260)
*Jia Wang,Jie Hu,Xiaoqi Ma,Hanghang Ma,Yanbing Zeng,Xiaoming Wei*

Main category: cs.CV

TL;DR: MagicMirror是一个全面的文本到图像生成伪影评估框架，包含首个大规模人工标注数据集MagicData340K、基于VLM的评估模型MagicAssessor，以及自动化基准测试MagicBench，揭示了当前顶级T2I模型仍存在严重伪影问题。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成虽然在指令跟随和美学方面取得显著进展，但普遍存在解剖学和结构缺陷等物理伪影，严重影响了感知质量并限制了应用。现有基准缺乏系统性和细粒度的评估框架。

Method: 1) 建立详细的生成图像伪影分类法；2) 人工标注340K图像数据集MagicData340K；3) 训练视觉语言模型MagicAssessor进行详细评估；4) 设计新颖的数据采样策略和多级奖励系统用于GRPO训练；5) 构建自动化基准测试MagicBench。

Result: 评估发现即使像GPT-image-1这样的顶级模型也持续存在显著伪影，表明伪影减少是未来T2I发展的关键前沿。

Conclusion: MagicMirror框架填补了T2I生成伪影评估的空白，为系统评估和改善图像生成质量提供了重要工具，强调了解决伪影问题的重要性。

Abstract: Text-to-image (T2I) generation has achieved remarkable progress in instruction following and aesthetics. However, a persistent challenge is the prevalence of physical artifacts, such as anatomical and structural flaws, which severely degrade perceptual quality and limit application. Given the diversity and complexity of these artifacts, a systematic and fine-grained evaluation framework is required, which is lacking in current benchmarks. To fill this gap, we introduce MagicMirror, a comprehensive framework for artifacts assessment. We first establish a detailed taxonomy of generated image artifacts. Guided by this taxonomy, we manually annotate MagicData340K, the first human-annotated large-scale dataset of 340K generated images with fine-grained artifact labels. Building on this dataset, we train MagicAssessor, a Vision-Language Model (VLM) that provides detailed assessments and corresponding labels. To overcome challenges like class imbalance and reward hacking, we design a novel data sampling strategy and a multi-level reward system for Group Relative Policy Optimization (GRPO). Finally, we leverage MagicAssessor to construct MagicBench, an automated benchmark for evaluating the image artifacts of current T2I models. Our evaluation with MagicBench reveals that despite their widespread adoption, even top-tier models like GPT-image-1 are consistently plagued by significant artifacts, highlighting artifact reduction as a critical frontier for future T2I development. Project page: https://wj-inf.github.io/MagicMirror-page/.

</details>


### [12] [Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching](https://arxiv.org/abs/2509.10312)
*Zhixin Zheng,Xinyu Wang,Chang Zou,Shaobo Wang,Linfeng Zhang*

Main category: cs.CV

TL;DR: ClusCa通过空间聚类减少扩散变换器的token数量，实现4.96倍加速，保持图像质量


<details>
  <summary>Details</summary>
Motivation: 现有特征缓存方法只利用时间维度相似性，忽略了空间维度的相似性，计算成本仍然很高

Method: 在每个时间步对token进行空间聚类，每个聚类只计算一个token并将其信息传播给其他token，减少90%以上token数量

Result: 在DiT、FLUX和HunyuanVideo上验证有效，FLUX加速4.96倍，ImageReward达到99.49%，比原始模型提升0.51%

Conclusion: ClusCa是现有特征缓存方法的正交补充，无需训练即可直接应用于任何扩散变换器，显著加速推理过程

Abstract: Diffusion transformers have gained significant attention in recent years for their ability to generate high-quality images and videos, yet still suffer from a huge computational cost due to their iterative denoising process. Recently, feature caching has been introduced to accelerate diffusion transformers by caching the feature computation in previous timesteps and reusing it in the following timesteps, which leverage the temporal similarity of diffusion models while ignoring the similarity in the spatial dimension. In this paper, we introduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and complementary perspective for previous feature caching. Specifically, ClusCa performs spatial clustering on tokens in each timestep, computes only one token in each cluster and propagates their information to all the other tokens, which is able to reduce the number of tokens by over 90%. Extensive experiments on DiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image and text-to-video generation. Besides, it can be directly applied to any diffusion transformer without requirements for training. For instance, ClusCa achieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing the original model by 0.51%. The code is available at https://github.com/Shenyi-Z/Cache4Diffusion.

</details>


### [13] [I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic Segmentation](https://arxiv.org/abs/2509.10334)
*Jordan Sassoon,Michal Szczepanski,Martyna Poreba*

Main category: cs.CV

TL;DR: I-Segmenter是首个完全整数化的ViT语义分割框架，通过系统替换浮点运算、提出λ-ShiftGELU激活函数等技术，在保持精度的同时显著提升效率。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在语义分割中表现优异，但在资源受限设备上部署受限，量化技术虽然能提高效率，但ViT分割模型在低精度下表现脆弱。

Method: 基于Segmenter架构，系统替换浮点运算为整数运算；提出λ-ShiftGELU激活函数处理长尾分布；移除L2归一化层；用最近邻上采样替换双线性插值。

Result: 在精度损失平均5.1%的情况下，模型大小减少3.8倍，推理速度提升1.2倍；单图像PTQ也能保持竞争力。

Conclusion: I-Segmenter为ViT分割模型的资源受限部署提供了实用解决方案，在效率和精度间取得了良好平衡。

Abstract: Vision Transformers (ViTs) have recently achieved strong results in semantic segmentation, yet their deployment on resource-constrained devices remains limited due to their high memory footprint and computational cost. Quantization offers an effective strategy to improve efficiency, but ViT-based segmentation models are notoriously fragile under low precision, as quantization errors accumulate across deep encoder-decoder pipelines. We introduce I-Segmenter, the first fully integer-only ViT segmentation framework. Building on the Segmenter architecture, I-Segmenter systematically replaces floating-point operations with integer-only counterparts. To further stabilize both training and inference, we propose $\lambda$-ShiftGELU, a novel activation function that mitigates the limitations of uniform quantization in handling long-tailed activation distributions. In addition, we remove the L2 normalization layer and replace bilinear interpolation in the decoder with nearest neighbor upsampling, ensuring integer-only execution throughout the computational graph. Extensive experiments show that I-Segmenter achieves accuracy within a reasonable margin of its FP32 baseline (5.1 % on average), while reducing model size by up to 3.8x and enabling up to 1.2x faster inference with optimized runtimes. Notably, even in one-shot PTQ with a single calibration image, I-Segmenter delivers competitive accuracy, underscoring its practicality for real-world deployment.

</details>


### [14] [GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT](https://arxiv.org/abs/2509.10341)
*Botond Fazekas,Thomas Pinetz,Guilherme Aresta,Taha Emre,Hrvoje Bogunovic*

Main category: cs.CV

TL;DR: GARD是一种基于伽马扩散模型的OCT图像去噪方法，通过噪声降低保真项和加速推理框架，在保持解剖结构细节的同时有效去除散斑噪声


<details>
  <summary>Details</summary>
Motivation: OCT图像受散斑噪声影响严重，现有去噪方法难以在噪声去除和结构保持之间取得平衡，需要更准确的噪声统计模型和更好的去噪指导机制

Method: 提出GARD方法：1) 使用去噪扩散伽马模型替代传统高斯模型，更准确反映散斑噪声统计特性；2) 引入噪声降低保真项，利用预处理低噪声图像指导去噪过程；3) 采用去噪扩散隐式模型框架加速推理

Result: 在配对噪声-低噪声OCT B扫描数据集上，GARD在PSNR、SSIM和MSE指标上显著优于传统方法和最先进的深度学习模型，定性结果显示边缘更清晰、解剖细节保持更好

Conclusion: GARD通过伽马扩散模型和噪声降低保真项，成功解决了OCT图像去噪中噪声去除与结构保持的平衡问题，为医学图像处理提供了有效解决方案

Abstract: Optical Coherence Tomography (OCT) is a vital imaging modality for diagnosing and monitoring retinal diseases. However, OCT images are inherently degraded by speckle noise, which obscures fine details and hinders accurate interpretation. While numerous denoising methods exist, many struggle to balance noise reduction with the preservation of crucial anatomical structures. This paper introduces GARD (Gamma-based Anatomical Restoration and Denoising), a novel deep learning approach for OCT image despeckling that leverages the strengths of diffusion probabilistic models. Unlike conventional diffusion models that assume Gaussian noise, GARD employs a Denoising Diffusion Gamma Model to more accurately reflect the statistical properties of speckle. Furthermore, we introduce a Noise-Reduced Fidelity Term that utilizes a pre-processed, less-noisy image to guide the denoising process. This crucial addition prevents the reintroduction of high-frequency noise. We accelerate the inference process by adapting the Denoising Diffusion Implicit Model framework to our Gamma-based model. Experiments on a dataset with paired noisy and less-noisy OCT B-scans demonstrate that GARD significantly outperforms traditional denoising methods and state-of-the-art deep learning models in terms of PSNR, SSIM, and MSE. Qualitative results confirm that GARD produces sharper edges and better preserves fine anatomical details.

</details>


### [15] [Immunizing Images from Text to Image Editing via Adversarial Cross-Attention](https://arxiv.org/abs/2509.10359)
*Matteo Trippodo,Federico Becattini,Lorenzo Seidenari*

Main category: cs.CV

TL;DR: 通过自动生成的图片描述作为代理提示，Attention Attack攻击突破文本提示与图像表征之间的跨注意力机制，干扰文本图像编辑方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的文本图像编辑方法容易受到对冲攻击的影响，需要一种新的攻击方法来测试并提高这些系统的安全性。

Method: 使用自动生成的源图片描述作为代理提示，突破文本提示与图像表征之间的跨注意力机制，无需知道编辑方法或编辑提示。

Result: 在TEDBench++基准测试中，该攻击显著降低了编辑性能但保持了不可察觉性。

Conclusion: Attention Attack提供了一种有效的对冲攻击方法，并提出了新的评估策略来量化攻击效果。

Abstract: Recent advances in text-based image editing have enabled fine-grained manipulation of visual content guided by natural language. However, such methods are susceptible to adversarial attacks. In this work, we propose a novel attack that targets the visual component of editing methods. We introduce Attention Attack, which disrupts the cross-attention between a textual prompt and the visual representation of the image by using an automatically generated caption of the source image as a proxy for the edit prompt. This breaks the alignment between the contents of the image and their textual description, without requiring knowledge of the editing method or the editing prompt. Reflecting on the reliability of existing metrics for immunization success, we propose two novel evaluation strategies: Caption Similarity, which quantifies semantic consistency between original and adversarial edits, and semantic Intersection over Union (IoU), which measures spatial layout disruption via segmentation masks. Experiments conducted on the TEDBench++ benchmark demonstrate that our attack significantly degrades editing performance while remaining imperceptible.

</details>


### [16] [InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis](https://arxiv.org/abs/2509.10441)
*Tao Han,Wanghan Xu,Junchao Gong,Xiaoyu Yue,Song Guo,Luping Zhou,Lei Bai*

Main category: cs.CV

TL;DR: InfGen是一种基于潜在扩散模型的第二代方法，通过用一步生成器替换VAE解码器，实现从固定大小潜在表示生成任意分辨率图像，大幅降低计算复杂度


<details>
  <summary>Details</summary>
Motivation: 解决当前扩散模型随分辨率增加计算需求呈二次增长的问题，实现4K图像生成的快速高效处理

Method: 将扩散模型生成的固定潜在表示作为内容表示，使用紧凑的一步生成器解码任意分辨率图像，替换原有的VAE解码器

Result: 将4K图像生成时间从100多秒减少到10秒以内，能够将多种模型升级到任意高分辨率时代

Conclusion: InfGen提供了一种简化流程、降低计算复杂度的解决方案，可应用于使用相同潜在空间的任何模型，实现高效任意分辨率图像生成

Abstract: Arbitrary resolution image generation provides a consistent visual experience across devices, having extensive applications for producers and consumers. Current diffusion models increase computational demand quadratically with resolution, causing 4K image generation delays over 100 seconds. To solve this, we explore the second generation upon the latent diffusion models, where the fixed latent generated by diffusion models is regarded as the content representation and we propose to decode arbitrary resolution images with a compact generated latent using a one-step generator. Thus, we present the \textbf{InfGen}, replacing the VAE decoder with the new generator, for generating images at any resolution from a fixed-size latent without retraining the diffusion models, which simplifies the process, reducing computational complexity and can be applied to any model using the same latent space. Experiments show InfGen is capable of improving many models into the arbitrary high-resolution era while cutting 4K image generation time to under 10 seconds.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [17] [Automated Tuning for Diffusion Inverse Problem Solvers without Generative Prior Retraining](https://arxiv.org/abs/2509.09880)
*Yaşar Utku Alçalar,Junno Yun,Mehmet Akçakaya*

Main category: eess.IV

TL;DR: ZADS是一种零样本自适应扩散采样方法，通过测试时优化自适应调整保真度权重，无需重新训练扩散先验，在加速MRI重建中优于传统压缩感知和现有扩散方法。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在解决逆问题时严重依赖精心调整的数据保真度权重，特别是在快速采样计划中，现有方法往往依赖启发式或固定权重，无法适应不同的测量条件和不规则时间步计划。

Method: 提出Zero-shot Adaptive Diffusion Sampling (ZADS)，将去噪过程视为固定的展开采样器，仅使用欠采样测量以自监督方式优化保真度权重。

Result: 在fastMRI膝关节数据集上的实验表明，ZADS始终优于传统压缩感知和最近的扩散方法，能够在不同噪声计划和采集设置下提供高保真重建。

Conclusion: ZADS提供了一种有效的方法来自适应调整扩散采样中的保真度权重，无需重新训练，在各种条件下都能实现高质量的重建性能。

Abstract: Diffusion/score-based models have recently emerged as powerful generative priors for solving inverse problems, including accelerated MRI reconstruction. While their flexibility allows decoupling the measurement model from the learned prior, their performance heavily depends on carefully tuned data fidelity weights, especially under fast sampling schedules with few denoising steps. Existing approaches often rely on heuristics or fixed weights, which fail to generalize across varying measurement conditions and irregular timestep schedules. In this work, we propose Zero-shot Adaptive Diffusion Sampling (ZADS), a test-time optimization method that adaptively tunes fidelity weights across arbitrary noise schedules without requiring retraining of the diffusion prior. ZADS treats the denoising process as a fixed unrolled sampler and optimizes fidelity weights in a self-supervised manner using only undersampled measurements. Experiments on the fastMRI knee dataset demonstrate that ZADS consistently outperforms both traditional compressed sensing and recent diffusion-based methods, showcasing its ability to deliver high-fidelity reconstructions across varying noise schedules and acquisition settings.

</details>


### [18] [Polarization Denoising and Demosaicking: Dataset and Baseline Method](https://arxiv.org/abs/2509.10098)
*Muhamad Daniel Ariff Bin Abdul Rahman,Yusuke Monno,Masayuki Tanaka,Masatoshi Okutomi*

Main category: eess.IV

TL;DR: 本文提出了一种用于偏振光器的噪声除去和马赛克重构的新方法和数据集，通过先除噪后重构的方式实现了更高的图像恢复性能。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏适当的评估数据集和坚实的基准方法，对于偏振光器的噪声除去和马赛克重构联合任务的研究较少。

Method: 采用先除噪后重构的方法，基于广泛接受的信号处理组件来提供可复现的方法。

Result: 实验结果显示，该方法在图像重构性能方面显示出比其他替代方法更高的表现，为该领域提供了坚实的基准。

Conclusion: 本文提供的新数据集和方法有效解决了偏振光器噪声除去和马赛克重构的关键问题，为该领域的研究提供了重要支撑。

Abstract: A division-of-focal-plane (DoFP) polarimeter enables us to acquire images with multiple polarization orientations in one shot and thus it is valuable for many applications using polarimetric information. The image processing pipeline for a DoFP polarimeter entails two crucial tasks: denoising and demosaicking. While polarization demosaicking for a noise-free case has increasingly been studied, the research for the joint task of polarization denoising and demosaicking is scarce due to the lack of a suitable evaluation dataset and a solid baseline method. In this paper, we propose a novel dataset and method for polarization denoising and demosaicking. Our dataset contains 40 real-world scenes and three noise-level conditions, consisting of pairs of noisy mosaic inputs and noise-free full images. Our method takes a denoising-then-demosaicking approach based on well-accepted signal processing components to offer a reproducible method. Experimental results demonstrate that our method exhibits higher image reconstruction performance than other alternative methods, offering a solid baseline.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [19] [HHI-Assist: A Dataset and Benchmark of Human-Human Interaction in Physical Assistance Scenario](https://arxiv.org/abs/2509.10096)
*Saeed Saadatnejad,Reyhaneh Hosseininejad,Jose Barreiros,Katherine M. Tsui,Alexandre Alahi*

Main category: cs.RO

TL;DR: 提出了HHI-Assist数据集和基于条件Transformer的扩散模型，用于预测物理交互场景中的人体运动，提升辅助机器人的安全性和响应能力


<details>
  <summary>Details</summary>
Motivation: 劳动力短缺和人口老龄化需要辅助机器人，但机器人需要准确的人体运动预测来确保安全交互，这在物理交互场景中由于环境多变性和耦合动力学复杂性而具有挑战性

Method: 开发了HHI-Assist数据集（包含人-人辅助交互的运动捕捉数据）和基于条件Transformer的去噪扩散模型，用于预测交互智能体的姿态

Result: 模型有效捕捉了护理者和被护理者之间的耦合动力学，相比基线方法有改进，并在未见场景中表现出强泛化能力

Conclusion: 通过推进交互感知的运动预测和引入新数据集，这项工作有潜力显著增强机器人辅助策略，数据集和代码已开源

Abstract: The increasing labor shortage and aging population underline the need for assistive robots to support human care recipients. To enable safe and responsive assistance, robots require accurate human motion prediction in physical interaction scenarios. However, this remains a challenging task due to the variability of assistive settings and the complexity of coupled dynamics in physical interactions. In this work, we address these challenges through two key contributions: (1) HHI-Assist, a dataset comprising motion capture clips of human-human interactions in assistive tasks; and (2) a conditional Transformer-based denoising diffusion model for predicting the poses of interacting agents. Our model effectively captures the coupled dynamics between caregivers and care receivers, demonstrating improvements over baselines and strong generalization to unseen scenarios. By advancing interaction-aware motion prediction and introducing a new dataset, our work has the potential to significantly enhance robotic assistance policies. The dataset and code are available at: https://sites.google.com/view/hhi-assist/home

</details>
