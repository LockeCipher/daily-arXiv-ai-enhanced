<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 19]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Improving 3D Gaussian Splatting Compression by Scene-Adaptive Lattice Vector Quantization](https://arxiv.org/abs/2509.13482)
*Hao Xu,Xiaolin Wu,Xi Zhang*

Main category: cs.CV

TL;DR: 提出SALVQ方法，用场景自适应的格点矢量量化替代传统均匀标量量化，提升3DGS压缩的率失真性能，同时保持低复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS压缩方法都使用简单的均匀标量量化，但更复杂的量化器可能以极小开销显著提升压缩性能。

Method: 采用格点矢量量化(LVQ)并针对每个场景优化格点基，实现场景自适应LVQ(SALVQ)，可无缝集成到现有3DGS压缩架构中。

Result: SALVQ在率失真效率和复杂度之间取得平衡，通过缩放格点基向量可动态调整压缩率，无需为不同压缩级别训练单独模型。

Conclusion: SALVQ方法以最小修改和计算开销显著提升现有3DGS压缩方法的性能，同时提供多码率灵活性。

Abstract: 3D Gaussian Splatting (3DGS) is rapidly gaining popularity for its photorealistic rendering quality and real-time performance, but it generates massive amounts of data. Hence compressing 3DGS data is necessary for the cost effectiveness of 3DGS models. Recently, several anchor-based neural compression methods have been proposed, achieving good 3DGS compression performance. However, they all rely on uniform scalar quantization (USQ) due to its simplicity. A tantalizing question is whether more sophisticated quantizers can improve the current 3DGS compression methods with very little extra overhead and minimal change to the system. The answer is yes by replacing USQ with lattice vector quantization (LVQ). To better capture scene-specific characteristics, we optimize the lattice basis for each scene, improving LVQ's adaptability and R-D efficiency. This scene-adaptive LVQ (SALVQ) strikes a balance between the R-D efficiency of vector quantization and the low complexity of USQ. SALVQ can be seamlessly integrated into existing 3DGS compression architectures, enhancing their R-D performance with minimal modifications and computational overhead. Moreover, by scaling the lattice basis vectors, SALVQ can dynamically adjust lattice density, enabling a single model to accommodate multiple bit rate targets. This flexibility eliminates the need to train separate models for different compression levels, significantly reducing training time and memory consumption.

</details>


### [2] [BiasMap: Leveraging Cross-Attentions to Discover and Mitigate Hidden Social Biases in Text-to-Image Generation](https://arxiv.org/abs/2509.13496)
*Rajatsubhra Chakraborty,Xujun Che,Depeng Xu,Cori Faklaris,Xi Niu,Shuhan Yuan*

Main category: cs.CV

TL;DR: BiasMap是一个模型无关的框架，用于发现稳定扩散模型中的潜在概念级表征偏见，通过交叉注意力归因图揭示人口统计特征与语义概念的结构性纠缠，并提出基于能量引导采样的偏见缓解方法。


<details>
  <summary>Details</summary>
Motivation: 现有偏见发现方法主要关注输出层面的人口统计分布，无法保证偏见缓解后概念表征能够解耦。需要更深入地探索生成过程中的表征偏见。

Method: 利用交叉注意力归因图量化人口统计特征与语义概念的空间纠缠（通过IoU指标），并采用能量引导扩散采样在去噪过程中直接修改潜在噪声空间以最小化SoftIoU。

Result: 研究发现现有公平性干预措施可能减少输出分布差距，但往往无法解耦概念级耦合，而BiasMap方法能够在图像生成中缓解概念纠缠，同时补充分布偏见缓解。

Conclusion: BiasMap提供了一个新的视角来发现和缓解TTI模型中的概念级表征偏见，揭示了现有方法无法检测到的隐藏偏见，并通过直接操作潜在空间实现了更有效的偏见缓解。

Abstract: Bias discovery is critical for black-box generative models, especiall text-to-image (TTI) models. Existing works predominantly focus on output-level demographic distributions, which do not necessarily guarantee concept representations to be disentangled post-mitigation. We propose BiasMap, a model-agnostic framework for uncovering latent concept-level representational biases in stable diffusion models. BiasMap leverages cross-attention attribution maps to reveal structural entanglements between demographics (e.g., gender, race) and semantics (e.g., professions), going deeper into representational bias during the image generation. Using attribution maps of these concepts, we quantify the spatial demographics-semantics concept entanglement via Intersection over Union (IoU), offering a lens into bias that remains hidden in existing fairness discovery approaches. In addition, we further utilize BiasMap for bias mitigation through energy-guided diffusion sampling that directly modifies latent noise space and minimizes the expected SoftIoU during the denoising process. Our findings show that existing fairness interventions may reduce the output distributional gap but often fail to disentangle concept-level coupling, whereas our mitigation method can mitigate concept entanglement in image generation while complementing distributional bias mitigation.

</details>


### [3] [DEFT-VTON: Efficient Virtual Try-On with Consistent Generalised H-Transform](https://arxiv.org/abs/2509.13506)
*Xingzi Xu,Qi Li,Shuwen Qiu,Julien Han,Karim Bouyarmane*

Main category: cs.CV

TL;DR: 提出了DEFT-VTON方法，通过Doob's h-transform高效微调和自适应一致性损失，在仅训练1.42%参数的情况下实现高质量虚拟试穿，推理步骤减少到15步。


<details>
  <summary>Details</summary>
Motivation: 解决现有虚拟试穿方法需要大量端到端训练和推理资源的问题，满足实际应用中有限的训练和部署预算需求。

Method: 使用DEFT冻结预训练模型参数，训练小型h-transform网络学习条件变换；结合自适应一致性损失和去噪分数匹配损失进行低成本微调。

Result: 在虚拟试穿任务上达到最先进性能，仅需15步去噪步骤，参数量仅为传统PEFT方法的1.42%（相比5.52%）。

Conclusion: DEFT-VTON方法在保持竞争力的同时显著降低了训练和推理成本，为实际部署提供了高效解决方案。

Abstract: Diffusion models enable high-quality virtual try-on (VTO) with their established image synthesis abilities. Despite the extensive end-to-end training of large pre-trained models involved in current VTO methods, real-world applications often prioritize limited training and inference, serving, and deployment budgets for VTO. To solve this obstacle, we apply Doob's h-transform efficient fine-tuning (DEFT) for adapting large pre-trained unconditional models for downstream image-conditioned VTO abilities. DEFT freezes the pre-trained model's parameters and trains a small h-transform network to learn a conditional h-transform. The h-transform network allows training only 1.42 percent of the frozen parameters, compared to a baseline of 5.52 percent in traditional parameter-efficient fine-tuning (PEFT).   To further improve DEFT's performance and decrease existing models' inference time, we additionally propose an adaptive consistency loss. Consistency training distills slow but high-performing diffusion models into a fast one while retaining performance by enforcing consistencies along the inference path. Inspired by constrained optimization, instead of distillation, we combine the consistency loss and the denoising score matching loss in a data-adaptive manner for fine-tuning existing VTO models at a low cost. Empirical results show the proposed DEFT-VTON method achieves state-of-the-art performance on VTO tasks, with as few as 15 denoising steps, while maintaining competitive results.

</details>


### [4] [FunKAN: Functional Kolmogorov-Arnold Network for Medical Image Enhancement and Segmentation](https://arxiv.org/abs/2509.13508)
*Maksim Penkin,Andrey Krylov*

Main category: cs.CV

TL;DR: 提出了FunKAN网络，将Kolmogorov-Arnold定理推广到函数空间，使用Hermite函数进行傅里叶分解，在医学图像增强和分割任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法架构复杂且可解释性有限，而KAN网络在处理图像时会破坏空间结构信息，需要一种既能保持可解释性又能处理图像空间结构的新方法。

Method: 提出Functional Kolmogorov-Arnold Network (FunKAN)，将Kolmogorov-Arnold表示定理推广到函数空间，使用Hermite函数进行傅里叶分解来学习内部函数。还提出了U-FunKAN用于医学图像分割。

Result: 在IXI数据集上的MRI吉布斯伪影抑制任务中表现优异（PSNR、TV指标），在BUSI（超声图像）、GlaS（组织学结构）和CVC-ClinicDB（结肠镜视频）三个医学数据集上的分割任务中（IoU、F1指标）均优于其他KAN基模型。

Conclusion: FunKAN成功弥合了理论函数逼近与医学图像分析之间的差距，为临床应用提供了强大且可解释的解决方案，在多种医学图像处理任务中表现出色。

Abstract: Medical image enhancement and segmentation are critical yet challenging tasks in modern clinical practice, constrained by artifacts and complex anatomical variations. Traditional deep learning approaches often rely on complex architectures with limited interpretability. While Kolmogorov-Arnold networks offer interpretable solutions, their reliance on flattened feature representations fundamentally disrupts the intrinsic spatial structure of imaging data. To address this issue we propose a Functional Kolmogorov-Arnold Network (FunKAN) -- a novel interpretable neural framework, designed specifically for image processing, that formally generalizes the Kolmogorov-Arnold representation theorem onto functional spaces and learns inner functions using Fourier decomposition over the basis Hermite functions. We explore FunKAN on several medical image processing tasks, including Gibbs ringing suppression in magnetic resonance images, benchmarking on IXI dataset. We also propose U-FunKAN as state-of-the-art binary medical segmentation model with benchmarks on three medical datasets: BUSI (ultrasound images), GlaS (histological structures) and CVC-ClinicDB (colonoscopy videos), detecting breast cancer, glands and polyps, respectively. Experiments on those diverse datasets demonstrate that our approach outperforms other KAN-based backbones in both medical image enhancement (PSNR, TV) and segmentation (IoU, F1). Our work bridges the gap between theoretical function approximation and medical image analysis, offering a robust, interpretable solution for clinical applications.

</details>


### [5] [ColonCrafter: A Depth Estimation Model for Colonoscopy Videos Using Diffusion Priors](https://arxiv.org/abs/2509.13525)
*Romain Hardy,Tyler Berzin,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: ColonCrafter是一个基于扩散模型的深度估计方法，能够从单目结肠镜视频生成时间一致的深度图，在C3VD数据集上实现了最先进的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 结肠镜中的三维场景理解面临重大挑战，现有深度估计模型在视频序列中缺乏时间一致性，限制了其在3D重建中的应用。

Method: 使用基于扩散的深度估计模型，从合成结肠镜序列学习稳健的几何先验来生成时间一致的深度图，并引入风格迁移技术将真实临床视频适配到合成训练域。

Result: 在C3VD数据集上实现了最先进的零样本性能，优于通用和结肠镜专用方法，能够生成3D点云和进行表面覆盖评估。

Conclusion: 虽然完整的轨迹3D重建仍然具有挑战性，但ColonCrafter展示了在临床相关应用中的潜力，包括3D点云生成和表面覆盖评估。

Abstract: Three-dimensional (3D) scene understanding in colonoscopy presents significant challenges that necessitate automated methods for accurate depth estimation. However, existing depth estimation models for endoscopy struggle with temporal consistency across video sequences, limiting their applicability for 3D reconstruction. We present ColonCrafter, a diffusion-based depth estimation model that generates temporally consistent depth maps from monocular colonoscopy videos. Our approach learns robust geometric priors from synthetic colonoscopy sequences to generate temporally consistent depth maps. We also introduce a style transfer technique that preserves geometric structure while adapting real clinical videos to match our synthetic training domain. ColonCrafter achieves state-of-the-art zero-shot performance on the C3VD dataset, outperforming both general-purpose and endoscopy-specific approaches. Although full trajectory 3D reconstruction remains a challenge, we demonstrate clinically relevant applications of ColonCrafter, including 3D point cloud generation and surface coverage assessment.

</details>


### [6] [MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM](https://arxiv.org/abs/2509.13536)
*Yinlong Bai,Hongxin Zhang,Sheng Zhong,Junkai Niu,Hai Li,Yijia He,Yi Zhou*

Main category: cs.CV

TL;DR: 通过基于几何相似性的汇署空间合并算法和Patch-Grid点采样初始化，在不影响运行性能的前提下降低了GPU内存使用并提升了3D高斯涂涂的渲染质量


<details>
  <summary>Details</summary>
Motivation: 解决嵌入式平台（如微空中运输工具）在3D高斯涂涂技术中遇到的计算资源和内存限制问题，当前研究主要集中在高性能桌面GPU上

Method: 1. 在汇署空间中基于几何相似性合并SLAM中的冗余高斯原语 2. 使用Patch-Grid点采样方法初始化高斯原语，更准确地建模整个场景

Result: 在公开数据集上的定量和定性评估证明了方法的有效性，在不影响系统运行时间性能的前提下降低了GPU内存使用并提升了渲染质量

Conclusion: 该研究为嵌入式平台提供了更高效的3D高斯涂涂方案，通过合并冗余原语和改进初始化方法，实现了内存使用和渲染质量的双重优化

Abstract: Recent advancements in 3D Gaussian Splatting (3DGS) have made a significant impact on rendering and reconstruction techniques. Current research predominantly focuses on improving rendering performance and reconstruction quality using high-performance desktop GPUs, largely overlooking applications for embedded platforms like micro air vehicles (MAVs). These devices, with their limited computational resources and memory, often face a trade-off between system performance and reconstruction quality. In this paper, we improve existing methods in terms of GPU memory usage while enhancing rendering quality. Specifically, to address redundant 3D Gaussian primitives in SLAM, we propose merging them in voxel space based on geometric similarity. This reduces GPU memory usage without impacting system runtime performance. Furthermore, rendering quality is improved by initializing 3D Gaussian primitives via Patch-Grid (PG) point sampling, enabling more accurate modeling of the entire scene. Quantitative and qualitative evaluations on publicly available datasets demonstrate the effectiveness of our improvements.

</details>


### [7] [Intelligent Healthcare Imaging Platform An VLM-Based Framework for Automated Medical Image Analysis and Clinical Report Generation](https://arxiv.org/abs/2509.13590)
*Samer Al-Hamadani*

Main category: cs.CV

TL;DR: 基于Google Gemini 2.5 Flash的多模态医学图像分析框架，通过视觉-语言模型实现自动肿瘤检测和临床报告生成，支持CT、MRI、X光片和超声等多种成像模态。


<details>
  <summary>Details</summary>
Motivation: 利用AI提升医疗形象诊断效率和临床决策过程，解决传统医学图像分析对大规模数据集的依赖问题。

Method: 整合视觉特征提取与自然语言处理，采用坐标验证机制和概率高斯模型进行异常分布分析，多层可视化技术生成详细医学图解。

Result: 异常检测在多模态下表现高性能，位置测量平均偏差仅80像素，具备零样本学习能力减少对大数据集的依赖。

Conclusion: 该框架代表了自动诊断支持和放射工作流效率的重要进步，但需要临床验证和多中心评估才能广泛推广。

Abstract: The rapid advancement of artificial intelligence (AI) in healthcare imaging has revolutionized diagnostic medicine and clinical decision-making processes. This work presents an intelligent multimodal framework for medical image analysis that leverages Vision-Language Models (VLMs) in healthcare diagnostics. The framework integrates Google Gemini 2.5 Flash for automated tumor detection and clinical report generation across multiple imaging modalities including CT, MRI, X-ray, and Ultrasound. The system combines visual feature extraction with natural language processing to enable contextual image interpretation, incorporating coordinate verification mechanisms and probabilistic Gaussian modeling for anomaly distribution. Multi-layered visualization techniques generate detailed medical illustrations, overlay comparisons, and statistical representations to enhance clinical confidence, with location measurement achieving 80 pixels average deviation. Result processing utilizes precise prompt engineering and textual analysis to extract structured clinical information while maintaining interpretability. Experimental evaluations demonstrated high performance in anomaly detection across multiple modalities. The system features a user-friendly Gradio interface for clinical workflow integration and demonstrates zero-shot learning capabilities to reduce dependence on large datasets. This framework represents a significant advancement in automated diagnostic support and radiological workflow efficiency, though clinical validation and multi-center evaluation are necessary prior to widespread adoption.

</details>


### [8] [Deep Lookup Network](https://arxiv.org/abs/2509.13662)
*Yulan Guo,Longguang Wang,Wendong Mao,Xiaoyu Dong,Yingqian Wang,Li Liu,Wei An*

Main category: cs.CV

TL;DR: 提出一种通用的查找表操作替代神经网络中的乘法运算，通过可微查找表实现端到端优化，在图像分类、超分辨率和点云分类任务中实现更高能效和推理速度，同时保持竞争力性能。


<details>
  <summary>Details</summary>
Motivation: 卷积神经网络中的乘法运算计算复杂度高、能耗大，阻碍在移动设备上的部署。受资源受限边缘设备使用查找表简化计算的启发，希望用查找操作替代乘法运算来提升效率。

Method: 引入通用的可微查找表操作替代权重和激活值的乘法计算，提出多种训练策略促进收敛，构建查找网络应用于多个视觉任务。

Result: 查找网络在能耗和推理速度方面效率更高，同时在图像分类、超分辨率和点云分类任务中保持与原始卷积网络相当的性能，在不同任务和数据类型上都达到先进水平。

Conclusion: 查找表操作是构建高效神经网络的有效基础操作，能够显著降低计算成本并加速推理，同时维持模型性能，适用于资源受限的边缘设备部署。

Abstract: Convolutional neural networks are constructed with massive operations with different types and are highly computationally intensive. Among these operations, multiplication operation is higher in computational complexity and usually requires {more} energy consumption with longer inference time than other operations, which hinders the deployment of convolutional neural networks on mobile devices. In many resource-limited edge devices, complicated operations can be calculated via lookup tables to reduce computational cost. Motivated by this, in this paper, we introduce a generic and efficient lookup operation which can be used as a basic operation for the construction of neural networks. Instead of calculating the multiplication of weights and activation values, simple yet efficient lookup operations are adopted to compute their responses. To enable end-to-end optimization of the lookup operation, we construct the lookup tables in a differentiable manner and propose several training strategies to promote their convergence. By replacing computationally expensive multiplication operations with our lookup operations, we develop lookup networks for the image classification, image super-resolution, and point cloud classification tasks. It is demonstrated that our lookup networks can benefit from the lookup operations to achieve higher efficiency in terms of energy consumption and inference speed while maintaining competitive performance to vanilla convolutional networks. Extensive experiments show that our lookup networks produce state-of-the-art performance on different tasks (both classification and regression tasks) and different data types (both images and point clouds).

</details>


### [9] [Controllable-Continuous Color Editing in Diffusion Model via Color Mapping](https://arxiv.org/abs/2509.13756)
*Yuqi Yang,Dongliang Chang,Yuanchen Fang,Yi-Zhe SonG,Zhanyu Ma,Jun Guo*

Main category: cs.CV

TL;DR: 提出颜色映射模块解决文本驱动图像编辑中颜色控制不精确的问题，通过建立文本嵌入空间与RGB值的对应关系实现连续可控的颜色编辑


<details>
  <summary>Details</summary>
Motivation: 自然语言的模糊性和离散性导致颜色编辑精度不足且难以实现连续控制，现有线性插值方法缺乏对颜色变化范围的精确控制

Method: 引入颜色映射模块，显式建模文本嵌入空间与图像RGB值的对应关系，根据给定RGB值预测对应的嵌入向量

Result: 实验结果表明该方法在颜色连续性和可控性方面表现良好

Conclusion: 该方法能够实现更细粒度、连续且可控的颜色编辑，用户可指定目标RGB范围生成所需范围内的连续颜色变化图像

Abstract: In recent years, text-driven image editing has made significant progress. However, due to the inherent ambiguity and discreteness of natural language, color editing still faces challenges such as insufficient precision and difficulty in achieving continuous control. Although linearly interpolating the embedding vectors of different textual descriptions can guide the model to generate a sequence of images with varying colors, this approach lacks precise control over the range of color changes in the output images. Moreover, the relationship between the interpolation coefficient and the resulting image color is unknown and uncontrollable. To address these issues, we introduce a color mapping module that explicitly models the correspondence between the text embedding space and image RGB values. This module predicts the corresponding embedding vector based on a given RGB value, enabling precise color control of the generated images while maintaining semantic consistency. Users can specify a target RGB range to generate images with continuous color variations within the desired range, thereby achieving finer-grained, continuous, and controllable color editing. Experimental results demonstrate that our method performs well in terms of color continuity and controllability.

</details>


### [10] [Iterative Prompt Refinement for Safer Text-to-Image Generation](https://arxiv.org/abs/2509.13760)
*Jinwoo Jeon,JunHyeok Oh,Hayeong Lee,Byung-Jun Lee*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉语言模型的迭代提示词优化算法，通过分析文本提示和生成图像来提升文本到图像生成的安全性，同时保持用户意图。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的文本到图像安全方法只关注文本提示，忽略了生成图像本身，可能导致不安全输出或对安全提示的不必要修改。

Method: 使用视觉语言模型分析输入提示和生成图像，通过视觉反馈迭代优化提示词，并构建了包含文本和视觉安全信号的新数据集进行监督微调。

Result: 实验结果表明该方法能生成更安全的输出，同时保持与用户意图的一致性，性能与现有基于大语言模型的方法相当。

Conclusion: 该方法为生成更安全的文本到图像内容提供了实用解决方案，通过视觉反馈有效提升安全性而不损害用户意图。

Abstract: Text-to-Image (T2I) models have made remarkable progress in generating images from text prompts, but their output quality and safety still depend heavily on how prompts are phrased. Existing safety methods typically refine prompts using large language models (LLMs), but they overlook the images produced, which can result in unsafe outputs or unnecessary changes to already safe prompts. To address this, we propose an iterative prompt refinement algorithm that uses Vision Language Models (VLMs) to analyze both the input prompts and the generated images. By leveraging visual feedback, our method refines prompts more effectively, improving safety while maintaining user intent and reliability comparable to existing LLM-based approaches. Additionally, we introduce a new dataset labeled with both textual and visual safety signals using off-the-shelf multi-modal LLM, enabling supervised fine-tuning. Experimental results demonstrate that our approach produces safer outputs without compromising alignment with user intent, offering a practical solution for generating safer T2I content. Our code is available at https://github.com/ku-dmlab/IPR. \textbf{\textcolor{red}WARNING: This paper contains examples of harmful or inappropriate images generated by models.

</details>


### [11] [NDLPNet: A Location-Aware Nighttime Deraining Network and a Real-World Benchmark Dataset](https://arxiv.org/abs/2509.13766)
*Huichun Liu,Xiaosong Li,Yang Liu,Xiaoqi Cheng,Haishu Tan*

Main category: cs.CV

TL;DR: 提出NDLPNet网络用于夜间图像去雨，通过位置感知模块捕获雨纹空间位置信息，在低光照条件下有效去除雨纹并保留背景细节，同时构建了真实夜间雨景数据集NSR。


<details>
  <summary>Details</summary>
Motivation: 现有去雨技术主要针对白天条件，在夜间低光照环境下性能不佳，因为雨纹分布空间异质性和光照依赖性导致可见度变化，需要专门针对夜间条件的去雨方法。

Method: 提出NDLPNet网络，包含位置感知模块(PPM)来捕获空间上下文信息，增强模型识别和重新校准不同特征通道重要性的能力，专门处理夜间雨纹的空间位置和密度分布。

Result: 在现有数据集和新建的NSR数据集上的大量实验表明，该方法在夜间去雨任务中优于现有最先进方法，既能有效去除雨纹又能保留关键背景信息。

Conclusion: NDLPNet通过位置感知机制有效解决了夜间图像去雨问题，构建的NSR数据集为夜间去雨研究提供了新基准，该方法在真实夜间场景中表现出优越性能。

Abstract: Visual degradation caused by rain streak artifacts in low-light conditions significantly hampers the performance of nighttime surveillance and autonomous navigation. Existing image deraining techniques are primarily designed for daytime conditions and perform poorly under nighttime illumination due to the spatial heterogeneity of rain distribution and the impact of light-dependent stripe visibility. In this paper, we propose a novel Nighttime Deraining Location-enhanced Perceptual Network(NDLPNet) that effectively captures the spatial positional information and density distribution of rain streaks in low-light environments. Specifically, we introduce a Position Perception Module (PPM) to capture and leverage spatial contextual information from input data, enhancing the model's capability to identify and recalibrate the importance of different feature channels. The proposed nighttime deraining network can effectively remove the rain streaks as well as preserve the crucial background information. Furthermore, We construct a night scene rainy (NSR) dataset comprising 900 image pairs, all based on real-world nighttime scenes, providing a new benchmark for nighttime deraining task research. Extensive qualitative and quantitative experimental evaluations on both existing datasets and the NSR dataset consistently demonstrate our method outperform the state-of-the-art (SOTA) methods in nighttime deraining tasks. The source code and dataset is available at https://github.com/Feecuin/NDLPNet.

</details>


### [12] [BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching](https://arxiv.org/abs/2509.13789)
*Hanshuai Cui,Zhiqing Tang,Zhifei Xu,Zhi Yao,Wenyi Zeng,Weijia Jia*

Main category: cs.CV

TL;DR: BWCache是一种无需训练的方法，通过动态缓存和重用DiT块特征来加速基于DiT的视频生成，在保持视觉质量的同时实现最高2.24倍加速


<details>
  <summary>Details</summary>
Motivation: 现有DiT视频生成方法的顺序去噪过程导致不可避免的延迟，限制了实际应用。现有加速方法要么因架构修改而牺牲视觉质量，要么无法在适当粒度上重用中间特征

Method: 提出Block-Wise Caching (BWCache)方法，动态缓存和重用跨扩散时间步的DiT块特征，并引入相似性指示器在相邻时间步块特征差异低于阈值时触发特征重用

Result: 在多个视频扩散模型上的广泛实验表明，BWCache实现了最高2.24倍的加速，同时保持可比的视觉质量

Conclusion: BWCache通过有效利用DiT块特征的时间冗余性，为DiT视频生成提供了一种高效且保持质量的加速解决方案

Abstract: Recent advancements in Diffusion Transformers (DiTs) have established them as the state-of-the-art method for video generation. However, their inherently sequential denoising process results in inevitable latency, limiting real-world applicability. Existing acceleration methods either compromise visual quality due to architectural modifications or fail to reuse intermediate features at proper granularity. Our analysis reveals that DiT blocks are the primary contributors to inference latency. Across diffusion timesteps, the feature variations of DiT blocks exhibit a U-shaped pattern with high similarity during intermediate timesteps, which suggests substantial computational redundancy. In this paper, we propose Block-Wise Caching (BWCache), a training-free method to accelerate DiT-based video generation. BWCache dynamically caches and reuses features from DiT blocks across diffusion timesteps. Furthermore, we introduce a similarity indicator that triggers feature reuse only when the differences between block features at adjacent timesteps fall below a threshold, thereby minimizing redundant computations while maintaining visual fidelity. Extensive experiments on several video diffusion models demonstrate that BWCache achieves up to 2.24$\times$ speedup with comparable visual quality.

</details>


### [13] [SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation](https://arxiv.org/abs/2509.13848)
*Jiayi Pan,Jiaming Xu,Yongkang Zhou,Guohao Dai*

Main category: cs.CV

TL;DR: SpecDiff是一种基于自推测信息的训练免费多级特征缓存策略，通过结合历史信息和未来信息来加速扩散模型推理，在保持质量的同时实现显著加速


<details>
  <summary>Details</summary>
Motivation: 现有特征缓存方法仅依赖历史信息，导致精度和速度性能受限，需要引入未来信息来突破速度-精度权衡瓶颈

Method: 提出自推测范式，基于不同迭代次数间相同时间步的信息相似性引入未来信息；包含基于自推测信息的缓存特征选择算法和基于特征重要性分数的多级特征分类算法

Result: 在Stable Diffusion 3、3.5和FLUX上分别实现平均2.80×、2.74×和3.17×的加速，质量损失可忽略

Conclusion: 通过融合推测信息和历史信息，SpecDiff突破了速度-精度权衡瓶颈，推动了高效扩散模型推理的帕累托前沿

Abstract: Feature caching has recently emerged as a promising method for diffusion model acceleration. It effectively alleviates the inefficiency problem caused by high computational requirements by caching similar features in the inference process of the diffusion model. In this paper, we analyze existing feature caching methods from the perspective of information utilization, and point out that relying solely on historical information will lead to constrained accuracy and speed performance. And we propose a novel paradigm that introduces future information via self-speculation based on the information similarity at the same time step across different iteration times. Based on this paradigm, we present \textit{SpecDiff}, a training-free multi-level feature caching strategy including a cached feature selection algorithm and a multi-level feature classification algorithm. (1) Feature selection algorithm based on self-speculative information. \textit{SpecDiff} determines a dynamic importance score for each token based on self-speculative information and historical information, and performs cached feature selection through the importance score. (2) Multi-level feature classification algorithm based on feature importance scores. \textit{SpecDiff} classifies tokens by leveraging the differences in feature importance scores and introduces a multi-level feature calculation strategy. Extensive experiments show that \textit{SpecDiff} achieves average 2.80 \times, 2.74 \times , and 3.17\times speedup with negligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow on NVIDIA A800-80GB GPU. By merging speculative and historical information, \textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing the Pareto frontier of speedup and accuracy in the efficient diffusion model inference.

</details>


### [14] [EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics](https://arxiv.org/abs/2509.13858)
*Qianxin Xia,Jiawei Du,Guoming Lu,Zhiyong Shu,Jielei Wang*

Main category: cs.CV

TL;DR: EDITS是一个新的数据集蒸馏框架，利用视觉语言模型提取图像中的文本语义信息，通过全局语义查询和局部语义感知构建图像和文本原型，最终使用扩散模型生成合成数据集。


<details>
  <summary>Details</summary>
Motivation: 传统数据集蒸馏方法主要捕获低级视觉特征，忽略了图像中的高级语义和结构信息，导致蒸馏效果有限。

Method: 1. 使用视觉语言模型生成外部文本并与图像特征融合
2. 通过全局语义查询模块形成先验聚类缓冲区
3. 局部语义感知选择代表性样本构建图像和文本原型
4. 使用精心设计的提示词引导大型语言模型生成文本原型
5. 采用双原型指导策略通过扩散模型生成最终合成数据集

Result: 大量实验证实了该方法的有效性，在保持竞争力的模型性能的同时实现了高效学习。

Conclusion: EDITS框架通过利用图像中的隐含文本语义信息，显著提升了数据集蒸馏的效果，为高效机器学习提供了新的解决方案。

Abstract: Dataset distillation aims to synthesize a compact dataset from the original large-scale one, enabling highly efficient learning while preserving competitive model performance. However, traditional techniques primarily capture low-level visual features, neglecting the high-level semantic and structural information inherent in images. In this paper, we propose EDITS, a novel framework that exploits the implicit textual semantics within the image data to achieve enhanced distillation. First, external texts generated by a Vision Language Model (VLM) are fused with image features through a Global Semantic Query module, forming the prior clustered buffer. Local Semantic Awareness then selects representative samples from the buffer to construct image and text prototypes, with the latter produced by guiding a Large Language Model (LLM) with meticulously crafted prompt. Ultimately, Dual Prototype Guidance strategy generates the final synthetic dataset through a diffusion model. Extensive experiments confirm the effectiveness of our method.Source code is available in: https://github.com/einsteinxia/EDITS.

</details>


### [15] [LamiGauss: Pitching Radiative Gaussian for Sparse-View X-ray Laminography Reconstruction](https://arxiv.org/abs/2509.13863)
*Chu Chen,Ander Biguri,Jean-Michel Morel,Raymond H. Chan,Carola-Bibiane Schönlieb,Jizhou Li*

Main category: cs.CV

TL;DR: LamiGauss是一种结合高斯溅射辐射光栅化和专用探测器到世界变换模型的CL重建算法，能够在极稀疏投影条件下实现高质量重建，仅需3%的全视图即可超越传统方法。


<details>
  <summary>Details</summary>
Motivation: X射线计算机层析成像(CL)对于板状结构的无损检测至关重要，但在高度稀疏视图采集条件下，从层析投影重建高质量体积仍然具有挑战性。传统CT由于几何约束难以处理此类问题。

Method: 提出LamiGauss算法，结合高斯溅射辐射光栅化和包含层析倾斜角的专用探测器到世界变换模型。采用初始化策略显式过滤初步重建中的常见层析伪影，防止冗余高斯分配到错误结构，集中模型能力表示真实物体。

Result: 在合成和真实数据集上的广泛实验证明了该方法的有效性和优越性。LamiGauss仅使用3%的全视图就能在完整数据集上优化的迭代方法上实现更优性能。

Conclusion: 该方法能够直接从稀疏投影进行有效优化，在有限数据条件下实现准确高效的重建，为板状结构的无损检测提供了有效的解决方案。

Abstract: X-ray Computed Laminography (CL) is essential for non-destructive inspection of plate-like structures in applications such as microchips and composite battery materials, where traditional computed tomography (CT) struggles due to geometric constraints. However, reconstructing high-quality volumes from laminographic projections remains challenging, particularly under highly sparse-view acquisition conditions. In this paper, we propose a reconstruction algorithm, namely LamiGauss, that combines Gaussian Splatting radiative rasterization with a dedicated detector-to-world transformation model incorporating the laminographic tilt angle. LamiGauss leverages an initialization strategy that explicitly filters out common laminographic artifacts from the preliminary reconstruction, preventing redundant Gaussians from being allocated to false structures and thereby concentrating model capacity on representing the genuine object. Our approach effectively optimizes directly from sparse projections, enabling accurate and efficient reconstruction with limited data. Extensive experiments on both synthetic and real datasets demonstrate the effectiveness and superiority of the proposed method over existing techniques. LamiGauss uses only 3$\%$ of full views to achieve superior performance over the iterative method optimized on a full dataset.

</details>


### [16] [Towards Robust Defense against Customization via Protective Perturbation Resistant to Diffusion-based Purification](https://arxiv.org/abs/2509.13922)
*Wenkui Yang,Jie Cao,Junxian Duan,Ran He*

Main category: cs.CV

TL;DR: 提出了AntiPure方法，通过在扩散模型中引入两种引导机制来抵抗净化攻击，保护图像免受恶意伪造


<details>
  <summary>Details</summary>
Motivation: 扩散模型如Stable Diffusion的强大定制能力带来了严重的安全风险，现有的保护性扰动方法容易被净化技术移除，导致图像再次面临恶意伪造风险

Method: 提出了AntiPure方法，包含两种引导机制：1) Patch-wise Frequency Guidance减少模型对高频分量的影响；2) Erroneous Timestep Guidance扰乱不同时间步的去噪策略

Result: 实验表明AntiPure在净化-定制工作流中实现了最小的感知差异和最大的失真效果，优于其他保护性扰动方法

Conclusion: AntiPure作为净化的压力测试，能够有效嵌入不可感知的扰动并在代表性净化设置下持续存在，实现有效的后定制失真

Abstract: Diffusion models like Stable Diffusion have become prominent in visual synthesis tasks due to their powerful customization capabilities, which also introduce significant security risks, including deepfakes and copyright infringement. In response, a class of methods known as protective perturbation emerged, which mitigates image misuse by injecting imperceptible adversarial noise. However, purification can remove protective perturbations, thereby exposing images again to the risk of malicious forgery. In this work, we formalize the anti-purification task, highlighting challenges that hinder existing approaches, and propose a simple diagnostic protective perturbation named AntiPure. AntiPure exposes vulnerabilities of purification within the "purification-customization" workflow, owing to two guidance mechanisms: 1) Patch-wise Frequency Guidance, which reduces the model's influence over high-frequency components in the purified image, and 2) Erroneous Timestep Guidance, which disrupts the model's denoising strategy across different timesteps. With additional guidance, AntiPure embeds imperceptible perturbations that persist under representative purification settings, achieving effective post-customization distortion. Experiments show that, as a stress test for purification, AntiPure achieves minimal perceptual discrepancy and maximal distortion, outperforming other protective perturbation methods within the purification-customization workflow.

</details>


### [17] [Noise-Level Diffusion Guidance: Well Begun is Half Done](https://arxiv.org/abs/2509.13936)
*Harvey Mannering,Zhiwu Huang,Adam Prugel-Bennett*

Main category: cs.CV

TL;DR: 这篇论文提出了噪声级别指导（NLG）方法，通过优化初始噪声来提高漫散模型的图像生成质量和提示遵循性，无需额外训练数据、辅助网络或反向传播优化。


<details>
  <summary>Details</summary>
Motivation: 当前漫散模型中的随机高斯噪声会影响最终输出质量和提示遵循性，而现有噪声优化方法需要额外数据集、网络或优化过程，实用性受限。

Method: 提出Noise Level Guidance（NLG）方法，通过增加初始噪声与一般指导对齐的可能性来精炼噪声，无需额外训练数据、辅助网络或反向传播优化。

Result: 在五个标准测试集上的实验结果显示，NLG方法能够提高输出生成质量和输入条件遵循性。

Conclusion: NLG作为一种简单、高效且通用的噪声优化方法，可以与现有指导方法无缝集成同时保持计算效率，是漫散模型的实用可扩展性增强。

Abstract: Diffusion models have achieved state-of-the-art image generation. However, the random Gaussian noise used to start the diffusion process influences the final output, causing variations in image quality and prompt adherence. Existing noise-level optimization approaches generally rely on extra dataset construction, additional networks, or backpropagation-based optimization, limiting their practicality. In this paper, we propose Noise Level Guidance (NLG), a simple, efficient, and general noise-level optimization approach that refines initial noise by increasing the likelihood of its alignment with general guidance - requiring no additional training data, auxiliary networks, or backpropagation. The proposed NLG approach provides a unified framework generalizable to both conditional and unconditional diffusion models, accommodating various forms of diffusion-level guidance. Extensive experiments on five standard benchmarks demonstrate that our approach enhances output generation quality and input condition adherence. By seamlessly integrating with existing guidance methods while maintaining computational efficiency, our method establishes NLG as a practical and scalable enhancement to diffusion models. Code can be found at https://github.com/harveymannering/NoiseLevelGuidance.

</details>


### [18] [Wan-Animate: Unified Character Animation and Replacement with Holistic Replication](https://arxiv.org/abs/2509.14055)
*Gang Cheng,Xin Gao,Li Hu,Siqi Hu,Mingyang Huang,Chaonan Ji,Ju Li,Dechao Meng,Jinwei Qi,Penchong Qiao,Zhen Shen,Yafei Song,Ke Sun,Linrui Tian,Feng Wang,Guangyuan Wang,Qi Wang,Zhongjian Wang,Jiayu Xiao,Sheng Xu,Bang Zhang,Peng Zhang,Xindi Zhang,Zhe Zhang,Jingren Zhou,Lian Zhuo*

Main category: cs.CV

TL;DR: Wan-Animate是一个统一的角色动画和替换框架，能够根据参考视频精确复制角色的表情和动作来生成高质量角色视频，或将动画角色无缝集成到参考视频中替换原始角色。


<details>
  <summary>Details</summary>
Motivation: 为了解决角色动画和替换任务中需要同时处理动作复制、表情重现和环境光照融合的挑战，开发一个统一的框架来实现高保真度的角色动画生成和场景集成。

Method: 基于Wan模型构建，采用改进的输入范式区分参考条件和生成区域；使用空间对齐的骨架信号复制身体运动，从源图像提取隐式面部特征重现表情；开发辅助的Relighting LoRA模块来增强环境光照融合。

Result: 实验结果表明Wan-Animate达到了最先进的性能水平，能够生成具有高可控性和表现力的角色视频，并实现无缝的环境集成效果。

Conclusion: Wan-Animate成功地将多个任务统一到共同的符号表示中，为角色动画和替换提供了有效的解决方案，并承诺开源模型权重和源代码。

Abstract: We introduce Wan-Animate, a unified framework for character animation and replacement. Given a character image and a reference video, Wan-Animate can animate the character by precisely replicating the expressions and movements of the character in the video to generate high-fidelity character videos. Alternatively, it can integrate the animated character into the reference video to replace the original character, replicating the scene's lighting and color tone to achieve seamless environmental integration. Wan-Animate is built upon the Wan model. To adapt it for character animation tasks, we employ a modified input paradigm to differentiate between reference conditions and regions for generation. This design unifies multiple tasks into a common symbolic representation. We use spatially-aligned skeleton signals to replicate body motion and implicit facial features extracted from source images to reenact expressions, enabling the generation of character videos with high controllability and expressiveness. Furthermore, to enhance environmental integration during character replacement, we develop an auxiliary Relighting LoRA. This module preserves the character's appearance consistency while applying the appropriate environmental lighting and color tone. Experimental results demonstrate that Wan-Animate achieves state-of-the-art performance. We are committed to open-sourcing the model weights and its source code.

</details>


### [19] [GenExam: A Multidisciplinary Text-to-Image Exam](https://arxiv.org/abs/2509.14232)
*Zhaokai Wang,Penghao Yin,Xiangyu Zhao,Changyao Tian,Yu Qiao,Wenhai Wang,Jifeng Dai,Gen Luo*

Main category: cs.CV

TL;DR: GenExam是首个多学科文本到图像考试基准，包含10个学科的1000个样本，采用四级分类的考试风格提示，用于精确评估语义正确性和视觉合理性。


<details>
  <summary>Details</summary>
Motivation: 现有考试风格基准主要关注理解和推理任务，而生成基准强调世界知识和视觉概念的展示，忽视了严格绘图考试的评估。

Method: 构建包含1000个样本的多学科文本到图像考试基准，每个问题配备真实图像和细粒度评分点，采用四级分类法组织考试风格提示。

Result: 实验显示即使最先进的模型如GPT-Image-1和Gemini-2.5-Flash-Image也只能获得低于15%的严格分数，大多数模型几乎得0分，表明该基准具有巨大挑战性。

Conclusion: 通过将图像生成框架为考试，GenExam提供了对模型整合知识、推理和生成能力的严格评估，为通往通用AGI的道路提供了见解。

Abstract: Exams are a fundamental test of expert-level intelligence and require integrated understanding, reasoning, and generation. Existing exam-style benchmarks mainly focus on understanding and reasoning tasks, and current generation benchmarks emphasize the illustration of world knowledge and visual concepts, neglecting the evaluation of rigorous drawing exams. We introduce GenExam, the first benchmark for multidisciplinary text-to-image exams, featuring 1,000 samples across 10 subjects with exam-style prompts organized under a four-level taxonomy. Each problem is equipped with ground-truth images and fine-grained scoring points to enable a precise evaluation of semantic correctness and visual plausibility. Experiments show that even state-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve less than 15% strict scores, and most models yield almost 0%, suggesting the great challenge of our benchmark. By framing image generation as an exam, GenExam offers a rigorous assessment of models' ability to integrate knowledge, reasoning, and generation, providing insights on the path to general AGI.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [20] [Cross-Distribution Diffusion Priors-Driven Iterative Reconstruction for Sparse-View CT](https://arxiv.org/abs/2509.13576)
*Haodong Li,Shuo Han,Haiyang Mao,Yu Shi,Changsheng Fang,Jianjia Zhang,Weiwen Wu,Hengyong Yu*

Main category: eess.IV

TL;DR: 提出CDPIR框架解决稀疏视图CT重建中的分布外问题，通过跨分布扩散先验和基于模型的迭代重建方法，在OOD场景下实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 稀疏视图CT重建存在视图减少导致的伪影问题，且由于扫描仪、协议或解剖结构变化导致的域偏移，在分布外场景中性能下降，阻碍临床使用

Method: 提出CDPIR框架，集成基于可扩展插值变换器(SiT)的跨分布扩散先验与基于模型的迭代重建方法。使用无分类器引导在多个数据集上训练，学习域特定和域不变先验，通过数据保真度和采样更新交替进行重建

Result: 在稀疏视图CT重建中实现最先进性能，特别是在分布外条件下显著优于现有方法，具有优越的细节保持能力和鲁棒性

Conclusion: CDPIR框架通过跨分布扩散先验有效解决了稀疏视图CT重建中的分布外问题，展现出强大的临床潜力

Abstract: Sparse-View CT (SVCT) reconstruction enhances temporal resolution and reduces radiation dose, yet its clinical use is hindered by artifacts due to view reduction and domain shifts from scanner, protocol, or anatomical variations, leading to performance degradation in out-of-distribution (OOD) scenarios. In this work, we propose a Cross-Distribution Diffusion Priors-Driven Iterative Reconstruction (CDPIR) framework to tackle the OOD problem in SVCT. CDPIR integrates cross-distribution diffusion priors, derived from a Scalable Interpolant Transformer (SiT), with model-based iterative reconstruction methods. Specifically, we train a SiT backbone, an extension of the Diffusion Transformer (DiT) architecture, to establish a unified stochastic interpolant framework, leveraging Classifier-Free Guidance (CFG) across multiple datasets. By randomly dropping the conditioning with a null embedding during training, the model learns both domain-specific and domain-invariant priors, enhancing generalizability. During sampling, the globally sensitive transformer-based diffusion model exploits the cross-distribution prior within the unified stochastic interpolant framework, enabling flexible and stable control over multi-distribution-to-noise interpolation paths and decoupled sampling strategies, thereby improving adaptation to OOD reconstruction. By alternating between data fidelity and sampling updates, our model achieves state-of-the-art performance with superior detail preservation in SVCT reconstructions. Extensive experiments demonstrate that CDPIR significantly outperforms existing approaches, particularly under OOD conditions, highlighting its robustness and potential clinical value in challenging imaging scenarios.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [21] [MCGS-SLAM: A Multi-Camera SLAM Framework Using Gaussian Splatting for High-Fidelity Mapping](https://arxiv.org/abs/2509.14191)
*Zhihao Cao,Hanyu Wu,Li Wa Tang,Zizhou Luo,Zihan Zhu,Wei Zhang,Marc Pollefeys,Martin R. Oswald*

Main category: cs.RO

TL;DR: MCGS-SLAM是首个基于纯RGB的多相机SLAM系统，使用3D高斯泼溅技术，通过多视角融合实现实时高精度建图和轨迹估计


<details>
  <summary>Details</summary>
Motivation: 现有的密集SLAM方法主要针对单目设置，在鲁棒性和几何覆盖方面存在不足，需要开发能够利用多相机输入提升性能的SLAM系统

Method: 采用多相机束调整(MCBA)联合优化位姿和深度，使用密集光度与几何残差，并通过尺度一致性模块利用低秩先验实现跨视角的度量对齐

Result: 在合成和真实数据集上实验表明，MCGS-SLAM能够产生准确的轨迹和逼真的重建效果，通常优于单目基线方法，并能重建单目系统遗漏的侧视区域

Conclusion: 多相机高斯泼溅SLAM在机器人和自动驾驶的高保真建图中具有巨大潜力，宽视场角输入对于安全自主操作至关重要

Abstract: Recent progress in dense SLAM has primarily targeted monocular setups, often at the expense of robustness and geometric coverage. We present MCGS-SLAM, the first purely RGB-based multi-camera SLAM system built on 3D Gaussian Splatting (3DGS). Unlike prior methods relying on sparse maps or inertial data, MCGS-SLAM fuses dense RGB inputs from multiple viewpoints into a unified, continuously optimized Gaussian map. A multi-camera bundle adjustment (MCBA) jointly refines poses and depths via dense photometric and geometric residuals, while a scale consistency module enforces metric alignment across views using low-rank priors. The system supports RGB input and maintains real-time performance at large scale. Experiments on synthetic and real-world datasets show that MCGS-SLAM consistently yields accurate trajectories and photorealistic reconstructions, usually outperforming monocular baselines. Notably, the wide field of view from multi-camera input enables reconstruction of side-view regions that monocular setups miss, critical for safe autonomous operation. These results highlight the promise of multi-camera Gaussian Splatting SLAM for high-fidelity mapping in robotics and autonomous driving.

</details>
