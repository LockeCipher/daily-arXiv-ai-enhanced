<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 49]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [RefAdGen: High-Fidelity Advertising Image Generation](https://arxiv.org/abs/2508.11695)
*Yiyun Chen,Weikai Yang*

Main category: cs.GR

TL;DR: 提出了RefAdGen框架，通过解耦设计和注意力融合模块解决AIGC广告图像生成中的保真度-效率困境，在大型数据集AdProd-100K上实现高保真度和高效生成。


<details>
  <summary>Details</summary>
Motivation: 现有AIGC技术在广告图像生成中要么需要对每个参考图像进行大量微调，要么难以在不同产品上保持保真度，无法满足电商和营销行业的实际需求。

Method: 构建AdProd-100K大规模数据集，采用双重数据增强策略；提出RefAdGen框架，通过产品掩码注入实现精确空间控制，使用注意力融合模块整合产品特征。

Result: RefAdGen在广泛实验中达到最先进性能，对未见产品和真实世界图像都能保持高保真度和优异视觉效果。

Conclusion: 该方法为传统工作流程提供了可扩展且成本效益高的替代方案，解决了现有方法的保真度-效率困境。

Abstract: The rapid advancement of Artificial Intelligence Generated Content (AIGC) techniques has unlocked opportunities in generating diverse and compelling advertising images based on referenced product images and textual scene descriptions. This capability substantially reduces human labor and production costs in traditional marketing workflows. However, existing AIGC techniques either demand extensive fine-tuning for each referenced image to achieve high fidelity, or they struggle to maintain fidelity across diverse products, making them impractical for e-commerce and marketing industries. To tackle this limitation, we first construct AdProd-100K, a large-scale advertising image generation dataset. A key innovation in its construction is our dual data augmentation strategy, which fosters robust, 3D-aware representations crucial for realistic and high-fidelity image synthesis. Leveraging this dataset, we propose RefAdGen, a generation framework that achieves high fidelity through a decoupled design. The framework enforces precise spatial control by injecting a product mask at the U-Net input, and employs an efficient Attention Fusion Module (AFM) to integrate product features. This design effectively resolves the fidelity-efficiency dilemma present in existing methods. Extensive experiments demonstrate that RefAdGen achieves state-of-the-art performance, showcasing robust generalization by maintaining high fidelity and remarkable visual results for both unseen products and challenging real-world, in-the-wild images. This offers a scalable and cost-effective alternative to traditional workflows. Code and datasets are publicly available at https://github.com/Anonymous-Name-139/RefAdgen.

</details>


### [2] [MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration](https://arxiv.org/abs/2508.12691)
*Yuanxin Wei,Lansong Diao,Bujiao Chen,Shenggan Cheng,Zhengping Qian,Wenyuan Yu,Nong Xiao,Wei Lin,Jiangsu Du*

Main category: cs.GR

TL;DR: MixCache是一个无需训练的高效视频DiT推理框架，通过多粒度缓存策略和自适应决策机制，在保持生成质量的同时显著加速视频生成


<details>
  <summary>Details</summary>
Motivation: 现有的视频DiT模型虽然能生成高质量视频，但多步迭代去噪过程计算成本高、推理延迟大。现有缓存方法仅限于单粒度策略，难以灵活平衡生成质量和推理速度

Method: 提出MixCache框架：1）区分不同缓存策略的干扰和边界；2）引入上下文感知缓存触发策略决定何时启用缓存；3）自适应混合缓存决策策略动态选择最优缓存粒度

Result: 在多种模型上的实验表明，MixCache能显著加速视频生成（Wan 14B上1.94倍加速，HunyuanVideo上1.97倍加速），同时提供优于基线方法的生成质量和推理效率

Conclusion: MixCache通过创新的多粒度缓存策略有效解决了视频DiT模型推理效率问题，在保持高质量生成的同时实现了显著的加速效果

Abstract: Leveraging the Transformer architecture and the diffusion process, video DiT models have emerged as a dominant approach for high-quality video generation. However, their multi-step iterative denoising process incurs high computational cost and inference latency. Caching, a widely adopted optimization method in DiT models, leverages the redundancy in the diffusion process to skip computations in different granularities (e.g., step, cfg, block). Nevertheless, existing caching methods are limited to single-granularity strategies, struggling to balance generation quality and inference speed in a flexible manner. In this work, we propose MixCache, a training-free caching-based framework for efficient video DiT inference. It first distinguishes the interference and boundary between different caching strategies, and then introduces a context-aware cache triggering strategy to determine when caching should be enabled, along with an adaptive hybrid cache decision strategy for dynamically selecting the optimal caching granularity. Extensive experiments on diverse models demonstrate that, MixCache can significantly accelerate video generation (e.g., 1.94$\times$ speedup on Wan 14B, 1.97$\times$ speedup on HunyuanVideo) while delivering both superior generation quality and inference efficiency compared to baseline methods.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [3] [UniDCF: A Foundation Model for Comprehensive Dentocraniofacial Hard Tissue Reconstruction](https://arxiv.org/abs/2508.11728)
*Chunxia Ren,Ning Zhu,Yue Lai,Gui Chen,Ruijie Wang,Yangyi Hu,Suyao Liu,Shuwen Mao,Hong Su,Yu Zhang,Li Xiao*

Main category: cs.CV

TL;DR: UniDCF是一个统一的多模态深度学习框架，能够通过点云和多视角图像的融合编码，重建多种牙颌面硬组织，解决了现有单模态方法的局限性，在几何精度、结构完整性和空间准确性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 牙颌面硬组织缺损严重影响患者的生理功能、面部美观和心理健康，当前深度学习模型仅限于单组织场景和特定模态成像输入，导致泛化性差，需要在解剖保真度、计算效率和跨组织适应性之间做出权衡。

Method: 提出UniDCF框架，通过点云和多视角图像的多模态融合编码，利用各模态的互补优势，并引入基于分数的去噪模块来优化表面平滑度。构建了包含6,609名患者的口内扫描、CBCT和CT数据的大型多模态数据集。

Result: UniDCF在几何精度、结构完整性和空间准确性方面优于现有最先进方法。临床模拟显示重建设计时间减少99%，临床医生接受度超过94%。

Conclusion: UniDCF实现了快速、自动化、高保真度的重建，支持个性化和精确的修复治疗，简化临床工作流程，改善患者治疗效果。

Abstract: Dentocraniofacial hard tissue defects profoundly affect patients' physiological functions, facial aesthetics, and psychological well-being, posing significant challenges for precise reconstruction. Current deep learning models are limited to single-tissue scenarios and modality-specific imaging inputs, resulting in poor generalizability and trade-offs between anatomical fidelity, computational efficiency, and cross-tissue adaptability. Here we introduce UniDCF, a unified framework capable of reconstructing multiple dentocraniofacial hard tissues through multimodal fusion encoding of point clouds and multi-view images. By leveraging the complementary strengths of each modality and incorporating a score-based denoising module to refine surface smoothness, UniDCF overcomes the limitations of prior single-modality approaches. We curated the largest multimodal dataset, comprising intraoral scans, CBCT, and CT from 6,609 patients, resulting in 54,555 annotated instances. Evaluations demonstrate that UniDCF outperforms existing state-of-the-art methods in terms of geometric precision, structural completeness, and spatial accuracy. Clinical simulations indicate UniDCF reduces reconstruction design time by 99% and achieves clinician-rated acceptability exceeding 94%. Overall, UniDCF enables rapid, automated, and high-fidelity reconstruction, supporting personalized and precise restorative treatments, streamlining clinical workflows, and enhancing patient outcomes.

</details>


### [4] [ComplicitSplat: Downstream Models are Vulnerable to Blackbox Attacks by 3D Gaussian Splat Camouflages](https://arxiv.org/abs/2508.11854)
*Matthew Hull,Haoyang Yang,Pratham Mehta,Mansi Phute,Aeree Cho,Haorang Wang,Matthew Lau,Wenke Lee,Wilian Lunardi,Martin Andreoni,Polo Chau*

Main category: cs.CV

TL;DR: ComplicitSplat是一种针对3D高斯泼溅技术的黑盒攻击方法，通过在特定视角下嵌入对抗性内容来欺骗目标检测器，无需访问模型架构或权重。


<details>
  <summary>Details</summary>
Motivation: 随着3D高斯泼溅技术在安全关键任务中的快速应用，需要研究攻击者如何通过篡改图像来造成危害，特别是针对自主导航等关键机器人系统的安全风险。

Method: 利用标准3DGS着色方法创建视角特定的伪装（随视角变化的颜色和纹理），在场景对象中嵌入仅从特定视角可见的对抗性内容，实现黑盒攻击。

Result: 实验表明ComplicitSplat能成功攻击各种流行的检测器（单阶段、多阶段和基于transformer的模型），在真实物理对象捕获和合成场景中均有效。

Conclusion: 这是首个针对下游目标检测器的3DGS黑盒攻击，揭示了自主导航等关键应用的新型安全风险。

Abstract: As 3D Gaussian Splatting (3DGS) gains rapid adoption in safety-critical tasks for efficient novel-view synthesis from static images, how might an adversary tamper images to cause harm? We introduce ComplicitSplat, the first attack that exploits standard 3DGS shading methods to create viewpoint-specific camouflage - colors and textures that change with viewing angle - to embed adversarial content in scene objects that are visible only from specific viewpoints and without requiring access to model architecture or weights. Our extensive experiments show that ComplicitSplat generalizes to successfully attack a variety of popular detector - both single-stage, multi-stage, and transformer-based models on both real-world capture of physical objects and synthetic scenes. To our knowledge, this is the first black-box attack on downstream object detectors using 3DGS, exposing a novel safety risk for applications like autonomous navigation and other mission-critical robotic systems.

</details>


### [5] [Large Kernel Modulation Network for Efficient Image Super-Resolution](https://arxiv.org/abs/2508.11893)
*Quanwei Hu,Yinggan Tang,Xuguang Zhang*

Main category: cs.CV

TL;DR: LKMN是一个纯CNN的轻量级超分辨率模型，通过大核调制块和交叉门前馈网络，在保持低延迟的同时实现了非局部特征提取，在性能和效率之间取得了良好平衡。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限场景下图像超分辨率的需求，传统CNN模型缺乏非局部特征捕获能力，而Transformer虽然擅长非局部建模但推理速度慢，需要在性能与延迟之间找到平衡。

Method: 提出Large Kernel Modulation Network (LKMN)，包含两个核心组件：Enhanced Partial Large Kernel Block (EPLKB) 使用通道混洗增强通道间交互，结合通道注意力关注关键信息，在部分通道上应用大核条带卷积进行非局部特征提取；Cross-Gate Feed-Forward Network (CGFN) 通过可学习缩放因子动态调整输入、局部和非局部特征之间的差异，采用交叉门策略调制和融合这些特征。

Result: 在Manga109数据集上4倍放大时，LKMN-L比DAT-light提升0.23 dB PSNR，推理速度快约4.8倍，超越了现有的轻量级超分辨率SOTA模型。

Conclusion: LKMN证明了纯CNN架构可以在保持低延迟的同时实现有效的非局部特征建模，为轻量级超分辨率任务提供了性能与效率平衡的优秀解决方案。

Abstract: Image super-resolution (SR) in resource-constrained scenarios demands lightweight models balancing performance and latency. Convolutional neural networks (CNNs) offer low latency but lack non-local feature capture, while Transformers excel at non-local modeling yet suffer slow inference. To address this trade-off, we propose the Large Kernel Modulation Network (LKMN), a pure CNN-based model. LKMN has two core components: Enhanced Partial Large Kernel Block (EPLKB) and Cross-Gate Feed-Forward Network (CGFN). The EPLKB utilizes channel shuffle to boost inter-channel interaction, incorporates channel attention to focus on key information, and applies large kernel strip convolutions on partial channels for non-local feature extraction with reduced complexity. The CGFN dynamically adjusts discrepancies between input, local, and non-local features via a learnable scaling factor, then employs a cross-gate strategy to modulate and fuse these features, enhancing their complementarity. Extensive experiments demonstrate that our method outperforms existing state-of-the-art (SOTA) lightweight SR models while balancing quality and efficiency. Specifically, LKMN-L achieves 0.23 dB PSNR improvement over DAT-light on the Manga109 dataset at $\times$4 upscale, with nearly $\times$4.8 times faster. Codes are in the supplementary materials. The code is available at https://github.com/Supereeeee/LKMN.

</details>


### [6] [Assessment of Using Synthetic Data in Brain Tumor Segmentation](https://arxiv.org/abs/2508.11922)
*Aditi Jahagirdar,Sameer Joshi*

Main category: cs.CV

TL;DR: 使用GAN生成的合成MRI数据来补充真实数据进行脑部脾瘤分割，在整体性能相似的情况下，6:4的合成真实混合比例在脾瘤边界分割方面显示出改善效果


<details>
  <summary>Details</summary>
Motivation: 解决脑脾瘤手动分割面临的挑战：脾瘤异质性、标注数据稀缺、类别不平衡。通过生成式模型产生合成数据来提升数据集多样性

Method: 使用预训练GAN模型(medigan库)生成合成MRI数据，将其与BraTS 2020真实数据按不同比例混合，训练U-Net分割网络进行对比实验

Result: 整体量化指标(Dice系数、IoU等)在真实数据和混合数据之间相似，但定性分析显示40%真实+60%合成的混合比例在整体脾瘤边界分割方面有改善，但脾瘤核心和增强区域的准确性仍较低

Conclusion: 合成数据作为数据增强策略在脑脾瘤分割中具有可行性，但需要进一步解决类别不平衡问题，并进行更大规模的实验验证

Abstract: Manual brain tumor segmentation from MRI scans is challenging due to tumor heterogeneity, scarcity of annotated data, and class imbalance in medical imaging datasets. Synthetic data generated by generative models has the potential to mitigate these issues by improving dataset diversity. This study investigates, as a proof of concept, the impact of incorporating synthetic MRI data, generated using a pre-trained GAN model, into training a U-Net segmentation network. Experiments were conducted using real data from the BraTS 2020 dataset, synthetic data generated with the medigan library, and hybrid datasets combining real and synthetic samples in varying proportions. While overall quantitative performance (Dice coefficient, IoU, precision, recall, accuracy) was comparable between real-only and hybrid-trained models, qualitative inspection suggested that hybrid datasets, particularly with 40% real and 60% synthetic data, improved whole tumor boundary delineation. However, region-wise accuracy for the tumor core and the enhancing tumor remained lower, indicating a persistent class imbalance. The findings support the feasibility of synthetic data as an augmentation strategy for brain tumor segmentation, while highlighting the need for larger-scale experiments, volumetric data consistency, and mitigating class imbalance in future work.

</details>


### [7] [Deep Learning For Point Cloud Denoising: A Survey](https://arxiv.org/abs/2508.11932)
*Chengwei Zhang,Xueyi Zhang,Mingrui Lao,Tao Jiang,Xinhao Xu,Wenjie Li,Fubo Zhang,Longyong Chen*

Main category: cs.CV

TL;DR: 这是一篇关于深度学习基于点云去噪的综述性论文，系统总结了该领域的发展状况、挖掘关键挑战、提出分类体系并展望未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 实际环境中的点云数据存在各种噪声，去噪是下游任务的关键预处理步骤。深度学习方法已超越传统方法，但缺乏系统的研究综述。本文旨在填补这一空白。

Method: 将点云去噪模型化为两步过程：离群点移除和表面噪声恢复。通过这种分析框架，对现有方法进行系统分类和比较分析，提出专门适用于去噪任务的分类体系。

Result: 论文成功构建了一个全面的分析框架，识别了深度学习基于点云去噪的关键挑战，系统总结了各种方法的主要贡献和特点，并完整地比较了不同方法的优势和差异。

Conclusion: 论文为深度学习基于点云去噪领域提供了一个结构化的研究框架，不仅系统总结了现有技术状况，还提出了研究限制和未来发展方向，对该领域的进一步发展具有重要指导意义。

Abstract: Real-world environment-derived point clouds invariably exhibit noise across varying modalities and intensities. Hence, point cloud denoising (PCD) is essential as a preprocessing step to improve downstream task performance. Deep learning (DL)-based PCD models, known for their strong representation capabilities and flexible architectures, have surpassed traditional methods in denoising performance. To our best knowledge, despite recent advances in performance, no comprehensive survey systematically summarizes the developments of DL-based PCD. To fill the gap, this paper seeks to identify key challenges in DL-based PCD, summarizes the main contributions of existing methods, and proposes a taxonomy tailored to denoising tasks. To achieve this goal, we formulate PCD as a two-step process: outlier removal and surface noise restoration, encompassing most scenarios and requirements of PCD. Additionally, we compare methods in terms of similarities, differences, and respective advantages. Finally, we discuss research limitations and future directions, offering insights for further advancements in PCD.

</details>


### [8] [UniUGG: Unified 3D Understanding and Generation via Geometric-Semantic Encoding](https://arxiv.org/abs/2508.11952)
*Yueming Xu,Jiahui Zhang,Ze Huang,Yurui Chen,Yanpeng Zhou,Zhenyu Chen,Yu-Jie Yuan,Pengxiang Xia,Guowei Huang,Xinyue Cai,Zhongang Qi,Xingyue Quan,Jianye Hao,Hang Xu,Li Zhang*

Main category: cs.CV

TL;DR: UniUGG是首个统一理解和生成3D模态的框架，使用LLM理解句子和3D表示，通过空间解码器和潜在扩散模型生成高质量3D内容，支持空间VQA任务和基于参考图像的3D场景生成。


<details>
  <summary>Details</summary>
Motivation: 尽管统一架构在图像理解和生成方面取得了显著进展，但3D任务的整合仍然具有挑战性且研究不足，需要开发能够同时处理3D理解和生成的统一框架。

Method: 提出UniUGG框架，使用LLM进行句子和3D表示的解码，核心是采用潜在扩散模型的空间解码器生成3D表示，并提出几何语义学习策略预训练视觉编码器，联合捕获输入的语义和几何线索。

Result: 大量实验结果表明该方法在视觉表示、空间理解和3D生成方面具有优越性。

Conclusion: UniUGG成功实现了3D模态的统一理解和生成，通过创新的空间解码器和几何语义学习策略，在多个3D任务上表现出色，为3D多模态学习提供了有效解决方案。

Abstract: Despite the impressive progress on understanding and generating images shown by the recent unified architectures, the integration of 3D tasks remains challenging and largely unexplored. In this paper, we introduce UniUGG, the first unified understanding and generation framework for 3D modalities. Our unified framework employs an LLM to comprehend and decode sentences and 3D representations. At its core, we propose a spatial decoder leveraging a latent diffusion model to generate high-quality 3D representations. This allows for the generation and imagination of 3D scenes based on a reference image and an arbitrary view transformation, while remaining supports for spatial visual question answering (VQA) tasks. Additionally, we propose a geometric-semantic learning strategy to pretrain the vision encoder. This design jointly captures the input's semantic and geometric cues, enhancing both spatial understanding and generation. Extensive experimental results demonstrate the superiority of our method in visual representation, spatial understanding, and 3D generation. The source code will be released upon paper acceptance.

</details>


### [9] [InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes](https://arxiv.org/abs/2508.12015)
*Hongyuan Liu,Haochen Yu,Jianfei Jiang,Qiankun Liu,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: InstDrive是一个针对动态驾驶场景的实例感知3D高斯泼溅框架，首次实现了开放世界动态驾驶场景的3D实例分割。


<details>
  <summary>Details</summary>
Motivation: 现有方法将背景元素统一为单一表示，阻碍了实例级理解和灵活场景编辑；现有室内场景方法不适用于户外驾驶场景；需要解决2D分割到3D空间的映射问题。

Method: 使用SAM生成的掩码作为伪真值，通过对比损失和伪监督目标指导2D特征学习；在3D层面引入正则化隐式编码实例身份，通过体素损失强制一致性；使用轻量级静态码本桥接连续特征和离散身份。

Result: 定量和定性实验证明了InstDrive的有效性，首次在动态开放世界驾驶场景中实现3D实例分割。

Conclusion: InstDrive为动态驾驶场景的交互式重建提供了有效的实例感知解决方案，无需数据预处理或复杂优化即可实现3D实例分割。

Abstract: Reconstructing dynamic driving scenes from dashcam videos has attracted increasing attention due to its significance in autonomous driving and scene understanding. While recent advances have made impressive progress, most methods still unify all background elements into a single representation, hindering both instance-level understanding and flexible scene editing. Some approaches attempt to lift 2D segmentation into 3D space, but often rely on pre-processed instance IDs or complex pipelines to map continuous features to discrete identities. Moreover, these methods are typically designed for indoor scenes with rich viewpoints, making them less applicable to outdoor driving scenarios. In this paper, we present InstDrive, an instance-aware 3D Gaussian Splatting framework tailored for the interactive reconstruction of dynamic driving scene. We use masks generated by SAM as pseudo ground-truth to guide 2D feature learning via contrastive loss and pseudo-supervised objectives. At the 3D level, we introduce regularization to implicitly encode instance identities and enforce consistency through a voxel-based loss. A lightweight static codebook further bridges continuous features and discrete identities without requiring data pre-processing or complex optimization. Quantitative and qualitative experiments demonstrate the effectiveness of InstDrive, and to the best of our knowledge, it is the first framework to achieve 3D instance segmentation in dynamic, open-world driving scenes.More visualizations are available at our project page.

</details>


### [10] [Generic Event Boundary Detection via Denoising Diffusion](https://arxiv.org/abs/2508.12084)
*Jaejun Hwang,Dayoung Gong,Manjin Kim,Minsu Cho*

Main category: cs.CV

TL;DR: DiffGEBD是一个基于扩散模型的通用事件边界检测方法，通过生成式视角解决事件边界检测问题，能够产生多样化的边界预测结果。


<details>
  <summary>Details</summary>
Motivation: 传统的事件边界检测方法主要关注确定性预测，忽略了事件边界的主观性和解决方案的多样性。本文旨在从生成式角度解决这个问题，提供多种合理的边界预测。

Method: 提出DiffGEBD模型：1）通过时间自相似性编码相邻帧之间的相关变化；2）使用扩散模型迭代地将随机噪声解码为合理的事件边界；3）采用分类器自由引导来控制去噪过程中的多样性程度。

Result: 在Kinetics-GEBD和TAPOS两个标准基准测试中取得了强劲性能，能够生成多样且合理的事件边界。

Conclusion: DiffGEBD成功地将扩散模型应用于事件边界检测任务，提供了一种能够产生多样化预测的新方法，同时引入了考虑多样性和保真度的新评估指标。

Abstract: Generic event boundary detection (GEBD) aims to identify natural boundaries in a video, segmenting it into distinct and meaningful chunks. Despite the inherent subjectivity of event boundaries, previous methods have focused on deterministic predictions, overlooking the diversity of plausible solutions. In this paper, we introduce a novel diffusion-based boundary detection model, dubbed DiffGEBD, that tackles the problem of GEBD from a generative perspective. The proposed model encodes relevant changes across adjacent frames via temporal self-similarity and then iteratively decodes random noise into plausible event boundaries being conditioned on the encoded features. Classifier-free guidance allows the degree of diversity to be controlled in denoising diffusion. In addition, we introduce a new evaluation metric to assess the quality of predictions considering both diversity and fidelity. Experiments show that our method achieves strong performance on two standard benchmarks, Kinetics-GEBD and TAPOS, generating diverse and plausible event boundaries.

</details>


### [11] [Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion](https://arxiv.org/abs/2508.12094)
*Songwei Liu,Hong Liu,Fangmin Chen,Xurui Peng,Chenqian Yan,Lean Fu,Xing Mei*

Main category: cs.CV

TL;DR: 通过理论分析推导变换模型中量化错误传播方程，提出时间步感知的累计错误补偿策略，显著提升低精度变换模型的性能


<details>
  <summary>Details</summary>
Motivation: 变换模型在图像合成领域达到了突破性质量，但迭代去噪过程导致计算开销较大，后训练量化方法在加速采样时存在步进式量化错误累积问题，影响输出保真度

Method: 建立理论框架数学形式化变换模型中的错误传播机制，推导每步量化错误传播方程，并得出累计错误的闭式解。基于理论基础提出时间步感知的累计错误补偿策略

Result: 在多个图像数据集上进行的广泛实验表明，该补偿策略能够有效减少错误传播，显著提升现有PTQ方法的性能，在低精度变换模型上达到了最先进的性能

Conclusion: 通过理论分析和实验验证，时间步感知的累计错误补偿策略能够有效解决变换模型量化中的错误累积问题，为大规模部署提供了有效的加速方案

Abstract: Diffusion models have transformed image synthesis by establishing unprecedented quality and creativity benchmarks. Nevertheless, their large-scale deployment faces challenges due to computationally intensive iterative denoising processes. Although post-training quantization(PTQ) provides an effective pathway for accelerating sampling, the iterative nature of diffusion models causes stepwise quantization errors to accumulate progressively during generation, inevitably compromising output fidelity. To address this challenge, we develop a theoretical framework that mathematically formulates error propagation in Diffusion Models (DMs), deriving per-step quantization error propagation equations and establishing the first closed-form solution for cumulative error. Building on this theoretical foundation, we propose a timestep-aware cumulative error compensation scheme. Extensive experiments across multiple image datasets demonstrate that our compensation strategy effectively mitigates error propagation, significantly enhancing existing PTQ methods to achieve state-of-the-art(SOTA) performance on low-precision diffusion models.

</details>


### [12] [DualFit: A Two-Stage Virtual Try-On via Warping and Synthesis](https://arxiv.org/abs/2508.12131)
*Minh Tran,Johnmark Clements,Annie Prasanna,Tri Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: DualFit是一个两阶段的虚拟试穿方法，通过变形和保真度保持模块，在保持服装细节的同时实现无缝试穿效果


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散式虚拟试穿方法无法保持服装精细细节（如logo和印刷文字）的问题，这对品牌完整性和客户信任至关重要

Method: 两阶段混合管道：第一阶段使用学习流场变形目标服装与人像对齐；第二阶段通过保真度保持试穿模块，结合保留区域输入和修复掩码，合成最终输出

Result: 广泛的定性结果显示DualFit实现了视觉无缝的试穿效果，同时忠实地保持了高频服装细节，在重建准确性和感知真实性之间取得了有效平衡

Conclusion: DualFil通过混合方法成功解决了虚拟试穿中细节保持的挑战，为在线时尚零售提供了更可靠的虚拟试穿解决方案

Abstract: Virtual Try-On technology has garnered significant attention for its potential to transform the online fashion retail experience by allowing users to visualize how garments would look on them without physical trials. While recent advances in diffusion-based warping-free methods have improved perceptual quality, they often fail to preserve fine-grained garment details such as logos and printed text elements that are critical for brand integrity and customer trust. In this work, we propose DualFit, a hybrid VTON pipeline that addresses this limitation by two-stage approach. In the first stage, DualFit warps the target garment to align with the person image using a learned flow field, ensuring high-fidelity preservation. In the second stage, a fidelity-preserving try-on module synthesizes the final output by blending the warped garment with preserved human regions. Particularly, to guide this process, we introduce a preserved-region input and an inpainting mask, enabling the model to retain key areas and regenerate only where necessary, particularly around garment seams. Extensive qualitative results show that DualFit achieves visually seamless try-on results while faithfully maintaining high-frequency garment details, striking an effective balance between reconstruction accuracy and perceptual realism.

</details>


### [13] [TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks](https://arxiv.org/abs/2508.12132)
*Amira Guesmi,Bassem Ouni,Muhammad Shafique*

Main category: cs.CV

TL;DR: TriQDef是一个针对量化神经网络的三级防御框架，通过特征不对齐惩罚和梯度感知失谐惩罚来破坏跨比特宽度的补丁式对抗攻击的可迁移性，在保持高准确率的同时显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 量化神经网络虽然提高了计算和内存效率，但对基于补丁的对抗攻击的鲁棒性有限，现有防御方法要么过拟合于固定量化设置，要么无法解决跨比特泛化漏洞。

Method: TriQDef包含三个核心组件：特征不对齐惩罚(FDP)通过惩罚中间表示的感知相似性来强制语义不一致；梯度感知失谐惩罚(GPDP)通过边缘IoU和HOG余弦度量最小化结构性和方向性一致性来显式错位输入梯度；联合量化感知训练协议在多个量化级别上统一这些惩罚。

Result: 在CIFAR-10和ImageNet上的广泛实验表明，TriQDef在未见过的补丁和量化组合上将攻击成功率(ASR)降低了40%以上，同时保持了高清洁准确率。

Conclusion: 研究强调了破坏语义和感知梯度对齐对于减轻量化神经网络中补丁可迁移性的重要性，TriQDef框架有效解决了跨比特泛化漏洞问题。

Abstract: Quantized Neural Networks (QNNs) are increasingly deployed in edge and resource-constrained environments due to their efficiency in computation and memory usage. While shown to distort the gradient landscape and weaken conventional pixel-level attacks, it provides limited robustness against patch-based adversarial attacks-localized, high-saliency perturbations that remain surprisingly transferable across bit-widths. Existing defenses either overfit to fixed quantization settings or fail to address this cross-bit generalization vulnerability. We introduce \textbf{TriQDef}, a tri-level quantization-aware defense framework designed to disrupt the transferability of patch-based adversarial attacks across QNNs. TriQDef consists of: (1) a Feature Disalignment Penalty (FDP) that enforces semantic inconsistency by penalizing perceptual similarity in intermediate representations; (2) a Gradient Perceptual Dissonance Penalty (GPDP) that explicitly misaligns input gradients across bit-widths by minimizing structural and directional agreement via Edge IoU and HOG Cosine metrics; and (3) a Joint Quantization-Aware Training Protocol that unifies these penalties within a shared-weight training scheme across multiple quantization levels. Extensive experiments on CIFAR-10 and ImageNet demonstrate that TriQDef reduces Attack Success Rates (ASR) by over 40\% on unseen patch and quantization combinations, while preserving high clean accuracy. Our findings underscore the importance of disrupting both semantic and perceptual gradient alignment to mitigate patch transferability in QNNs.

</details>


### [14] [RealTalk: Realistic Emotion-Aware Lifelike Talking-Head Synthesis](https://arxiv.org/abs/2508.12163)
*Wenqing Wang,Yun Fu*

Main category: cs.CV

TL;DR: RealTalk是一个新颖的情感说话头合成框架，通过VAE生成3D面部标志点，结合情感标签嵌入和ResNet-based LDM产生情感标志点，再通过tri-plane attention NeRF合成高质量情感说话头，在情感准确性、可控性和身份保持方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前方法虽然在唇形同步和图像质量方面表现出色，但在生成准确可控的情感表达同时保持主体身份方面存在不足，这限制了人工智能社交智能的发展。

Method: 使用变分自编码器(VAE)从驱动音频生成3D面部标志点，通过ResNet-based地标变形模型(LDM)将情感标签嵌入与标志点连接，产生情感标志点。这些标志点和面部混合形状系数共同条件化新颖的三平面注意力神经辐射场(NeRF)来合成情感说话头。

Result: 大量实验表明，RealTalk在情感准确性、可控性和身份保持方面优于现有方法，推动了社交智能AI系统的发展。

Conclusion: RealTalk框架成功解决了情感说话头合成中的关键挑战，在保持高质量图像生成的同时实现了准确的情感表达和身份保持，为人工智能社交智能提供了重要技术支撑。

Abstract: Emotion is a critical component of artificial social intelligence. However, while current methods excel in lip synchronization and image quality, they often fail to generate accurate and controllable emotional expressions while preserving the subject's identity. To address this challenge, we introduce RealTalk, a novel framework for synthesizing emotional talking heads with high emotion accuracy, enhanced emotion controllability, and robust identity preservation. RealTalk employs a variational autoencoder (VAE) to generate 3D facial landmarks from driving audio, which are concatenated with emotion-label embeddings using a ResNet-based landmark deformation model (LDM) to produce emotional landmarks. These landmarks and facial blendshape coefficients jointly condition a novel tri-plane attention Neural Radiance Field (NeRF) to synthesize highly realistic emotional talking heads. Extensive experiments demonstrate that RealTalk outperforms existing methods in emotion accuracy, controllability, and identity preservation, advancing the development of socially intelligent AI systems.

</details>


### [15] [Scalable RF Simulation in Generative 4D Worlds](https://arxiv.org/abs/2508.12176)
*Zhiwei Zheng,Dongyin Hu,Mingmin Zhao*

Main category: cs.CV

TL;DR: WaveVerse是一个基于提示的RF信号仿真框架，通过语言引导生成室内场景和人体运动，使用相位相干射线追踪模拟真实RF信号，解决了RF数据采集难题。


<details>
  <summary>Details</summary>
Motivation: RF传感作为隐私保护的室内感知替代方案面临高质量数据采集困难，特别是在动态多样的室内环境中难以获取足够的训练数据。

Method: 采用语言引导的4D世界生成器，包括状态感知因果变换器用于基于空间约束和文本的人体运动生成，以及相位相干射线追踪模拟器来产生准确的RF信号。

Result: 实验证明该方法在条件人体运动生成方面有效，相位相干性成功应用于波束成形和呼吸监测。在RF成像和人体活动识别案例中，WaveVerse首次实现了RF成像数据生成，并在数据有限和充足场景下均获得性能提升。

Conclusion: WaveVerse提供了一个可扩展的RF信号仿真框架，能够生成高质量的RF训练数据，解决了实际数据采集难题，为RF感知应用提供了有效的数据生成解决方案。

Abstract: Radio Frequency (RF) sensing has emerged as a powerful, privacy-preserving alternative to vision-based methods for indoor perception tasks. However, collecting high-quality RF data in dynamic and diverse indoor environments remains a major challenge. To address this, we introduce WaveVerse, a prompt-based, scalable framework that simulates realistic RF signals from generated indoor scenes with human motions. WaveVerse introduces a language-guided 4D world generator, which includes a state-aware causal transformer for human motion generation conditioned on spatial constraints and texts, and a phase-coherent ray tracing simulator that enables the simulation of accurate and coherent RF signals. Experiments demonstrate the effectiveness of our approach in conditioned human motion generation and highlight how phase coherence is applied to beamforming and respiration monitoring. We further present two case studies in ML-based high-resolution imaging and human activity recognition, demonstrating that WaveVerse not only enables data generation for RF imaging for the first time, but also consistently achieves performance gain in both data-limited and data-adequate scenarios.

</details>


### [16] [SNNSIR: A Simple Spiking Neural Network for Stereo Image Restoration](https://arxiv.org/abs/2508.12271)
*Ronghua Xu,Jin Xie,Jing Nie,Jiale Cao,Yanwei Pang*

Main category: cs.CV

TL;DR: 提出SNNSIR，一种完全脉冲驱动的脉冲神经网络，用于立体图像恢复，在保持竞争性恢复性能的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的混合SNN-ANN模型仍依赖浮点矩阵除法或指数运算，与SNN的二进制和事件驱动特性不兼容。需要开发完全脉冲驱动的架构来实现低功耗和硬件友好的计算。

Method: 1) 引入轻量级脉冲残差基本块(SRBB)通过脉冲兼容的残差学习增强信息流；2) 脉冲立体卷积调制(SSCM)模块通过逐元素乘法引入简化非线性，并通过跨视图感知调制突出噪声敏感区域；3) 脉冲立体交叉注意力(SSCA)模块在脉冲兼容框架内实现高效的跨视图双向特征交互。

Result: 在多种立体图像恢复任务（雨纹去除、雨滴去除、低光增强和超分辨率）上的广泛实验表明，该模型在保持竞争性恢复性能的同时显著降低了计算开销。

Conclusion: 该方法展示了实时、低功耗立体视觉应用的潜力，为完全脉冲驱动的立体图像恢复提供了有效解决方案。

Abstract: Spiking Neural Networks (SNNs), characterized by discrete binary activations, offer high computational efficiency and low energy consumption, making them well-suited for computation-intensive tasks such as stereo image restoration. In this work, we propose SNNSIR, a simple yet effective Spiking Neural Network for Stereo Image Restoration, specifically designed under the spike-driven paradigm where neurons transmit information through sparse, event-based binary spikes. In contrast to existing hybrid SNN-ANN models that still rely on operations such as floating-point matrix division or exponentiation, which are incompatible with the binary and event-driven nature of SNNs, our proposed SNNSIR adopts a fully spike-driven architecture to achieve low-power and hardware-friendly computation. To address the expressiveness limitations of binary spiking neurons, we first introduce a lightweight Spike Residual Basic Block (SRBB) to enhance information flow via spike-compatible residual learning. Building on this, the Spike Stereo Convolutional Modulation (SSCM) module introduces simplified nonlinearity through element-wise multiplication and highlights noise-sensitive regions via cross-view-aware modulation. Complementing this, the Spike Stereo Cross-Attention (SSCA) module further improves stereo correspondence by enabling efficient bidirectional feature interaction across views within a spike-compatible framework. Extensive experiments on diverse stereo image restoration tasks, including rain streak removal, raindrop removal, low-light enhancement, and super-resolution demonstrate that our model achieves competitive restoration performance while significantly reducing computational overhead. These results highlight the potential for real-time, low-power stereo vision applications. The code will be available after the article is accepted.

</details>


### [17] [Improving Densification in 3D Gaussian Splatting for High-Fidelity Rendering](https://arxiv.org/abs/2508.12313)
*Xiaobin Deng,Changyu Diao,Min Li,Ruohan Yu,Duanqing Xu*

Main category: cs.CV

TL;DR: 基于3D高斯拓扑的密化策略全面改进，通过边缘感知选择、长轴分割策略和抑制过拟合技术，在不增加计算开销的情况下提升渲染质量


<details>
  <summary>Details</summary>
Motivation: 3D高斯拓扑技术虽然实现了实时渲染，但其密化策略导致重建质量不佳，需要从密化时机、方式和抑制过拟合等多个角度进行全面改进

Method: 提出边缘感知评分选择分割候选高斯函数，采用长轴分割策略减少克隆和分割操作引入的几何异常，设计恢复感知剪枝、多步更新和增长控制技术来应对过拟合问题

Result: 方法在不增加训练或推理开销的情况下显著提升了渲染保真度，以更少的高斯函数数量达到了最先进的性能水平

Conclusion: 通过系统性的密化管道改进，该研究为3D高斯拓扑技术提供了一套有效的重建质量优化方案，在保持实时渲染优势的同时显著提升了视觉效果

Abstract: Although 3D Gaussian Splatting (3DGS) has achieved impressive performance in real-time rendering, its densification strategy often results in suboptimal reconstruction quality. In this work, we present a comprehensive improvement to the densification pipeline of 3DGS from three perspectives: when to densify, how to densify, and how to mitigate overfitting. Specifically, we propose an Edge-Aware Score to effectively select candidate Gaussians for splitting. We further introduce a Long-Axis Split strategy that reduces geometric distortions introduced by clone and split operations. To address overfitting, we design a set of techniques, including Recovery-Aware Pruning, Multi-step Update, and Growth Control. Our method enhances rendering fidelity without introducing additional training or inference overhead, achieving state-of-the-art performance with fewer Gaussians.

</details>


### [18] [Geometry-Aware Video Inpainting for Joint Headset Occlusion Removal and Face Reconstruction in Social XR](https://arxiv.org/abs/2508.12336)
*Fatemeh Ghorbani Lohesara,Karen Eguiazarian,Sebastian Knorr*

Main category: cs.CV

TL;DR: 一种基于深度学习的几何感知框架，能够从单视角RGB视频中同时去除头显设备遮挡并重建完整的3D面部几何，为社交XR应用提供涉及面部表情的洞察体验。


<details>
  <summary>Details</summary>
Motivation: 头显设备(HMDs)遮挡了用户面部上部分，影响外部视频录制和社交XR应用（如远程会议）的氛围创造，因为面部表情和眼神细节对于氛围洞察至关重要。

Method: 集成GAN基础的视频修复网络，在密集面部关键点和单张无遮挡参照帧的指导下恢复缺失面部区域保持身份识别；继而使用SynergyNet基础模块从修复后的帧中回归3D可变形模型(3DMM)参数，实现准确的3D面部重建；通过密集关键点优化提升修复质量和几何重建保真度。

Result: 实验结果显示该框架能成功从RGB面部视频中去除HMD遮挡，保持面部身份识别和真实性，生成以照片为真实的3D面部几何输出。分离研究进一步证明框架在不同关键点密度下均保持稳健，在稀疏关键点配置下仅有轻微质量下降。

Conclusion: 该研究提出的几何感知学习框架能够有效解决HMD遮挡问题，为社交XR应用提供了可靠的面部表情恢复和3D重建方案，在保持高质量输出的同时具有良好的稳健性。

Abstract: Head-mounted displays (HMDs) are essential for experiencing extended reality (XR) environments and observing virtual content. However, they obscure the upper part of the user's face, complicating external video recording and significantly impacting social XR applications such as teleconferencing, where facial expressions and eye gaze details are crucial for creating an immersive experience. This study introduces a geometry-aware learning-based framework to jointly remove HMD occlusions and reconstruct complete 3D facial geometry from RGB frames captured from a single viewpoint. The method integrates a GAN-based video inpainting network, guided by dense facial landmarks and a single occlusion-free reference frame, to restore missing facial regions while preserving identity. Subsequently, a SynergyNet-based module regresses 3D Morphable Model (3DMM) parameters from the inpainted frames, enabling accurate 3D face reconstruction. Dense landmark optimization is incorporated throughout the pipeline to improve both the inpainting quality and the fidelity of the recovered geometry. Experimental results demonstrate that the proposed framework can successfully remove HMDs from RGB facial videos while maintaining facial identity and realism, producing photorealistic 3D face geometry outputs. Ablation studies further show that the framework remains robust across different landmark densities, with only minor quality degradation under sparse landmark configurations.

</details>


### [19] [Semantic Discrepancy-aware Detector for Image Forgery Identification](https://arxiv.org/abs/2508.12341)
*Ziye Wang,Minghang Yu,Chunyan Xu,Zhen Cui*

Main category: cs.CV

TL;DR: 提出SDD检测器，通过重建学习在细粒度视觉层面对齐伪造痕迹和语义概念空间，利用预训练视觉语言模型的概念知识来提升伪造图像检测性能


<details>
  <summary>Details</summary>
Motivation: 随着图像生成技术的快速发展，确保数字媒体可信度需要强大的伪造检测。预训练模型学习的语义概念对识别假图像很关键，但伪造空间与语义概念空间的不对齐阻碍了检测性能

Method: 提出语义差异感知检测器(SDD)：1)语义标记采样模块缓解与伪造痕迹和语义概念无关的特征造成的空间偏移；2)基于视觉重建范式的概念级伪造差异学习模块；3)低级伪造特征增强器整合学习到的概念级差异

Result: 在两个标准图像伪造数据集上的实验证明SDD的有效性，相比现有方法取得了更优越的结果

Conclusion: SDD通过重建学习有效对齐伪造和语义概念空间，利用预训练视觉语言模型的概念知识，显著提升了伪造图像检测性能

Abstract: With the rapid advancement of image generation techniques, robust forgery detection has become increasingly imperative to ensure the trustworthiness of digital media. Recent research indicates that the learned semantic concepts of pre-trained models are critical for identifying fake images. However, the misalignment between the forgery and semantic concept spaces hinders the model's forgery detection performance. To address this problem, we propose a novel Semantic Discrepancy-aware Detector (SDD) that leverages reconstruction learning to align the two spaces at a fine-grained visual level. By exploiting the conceptual knowledge embedded in the pre-trained vision language model, we specifically design a semantic token sampling module to mitigate the space shifts caused by features irrelevant to both forgery traces and semantic concepts. A concept-level forgery discrepancy learning module, built upon a visual reconstruction paradigm, is proposed to strengthen the interaction between visual semantic concepts and forgery traces, effectively capturing discrepancies under the concepts' guidance. Finally, the low-level forgery feature enhancemer integrates the learned concept level forgery discrepancies to minimize redundant forgery information. Experiments conducted on two standard image forgery datasets demonstrate the efficacy of the proposed SDD, which achieves superior results compared to existing methods. The code is available at https://github.com/wzy1111111/SSD.

</details>


### [20] [AquaFeat: A Features-Based Image Enhancement Model for Underwater Object Detection](https://arxiv.org/abs/2508.12343)
*Emanuel C. Silva,Tatiana T. Schein,Stephanie L. Brião,Guilherme L. M. Costa,Felipe G. Oliveira,Gustavo P. Almeida,Eduardo L. Silva,Sam S. Devincenzi,Karina S. Machado,Paulo L. J. Drews-Jr*

Main category: cs.CV

TL;DR: AquaFeat是一个即插即用的任务驱动特征增强模块，专门针对水下目标检测任务设计，通过端到端训练实现多尺度特征增强，在YOLOv8m上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 水下环境的严重图像退化会损害目标检测模型的性能，传统的图像增强方法通常没有针对下游任务进行优化。

Method: 提出AquaFeat模块，集成多尺度特征增强网络，通过端到端训练与检测器损失函数结合，确保增强过程明确指导优化与检测任务最相关的特征。

Result: 在挑战性水下数据集上，AquaFeat达到最先进的精度(0.877)和召回率(0.624)，以及竞争力的mAP分数(mAP@0.5为0.677，mAP@[0.5:0.95]为0.421)，处理速度为46.5 FPS。

Conclusion: 该模型在保持实用处理速度的同时提供准确性提升，为海洋生态系统监测和基础设施检查等实际应用提供了有效且计算效率高的解决方案。

Abstract: The severe image degradation in underwater environments impairs object detection models, as traditional image enhancement methods are often not optimized for such downstream tasks. To address this, we propose AquaFeat, a novel, plug-and-play module that performs task-driven feature enhancement. Our approach integrates a multi-scale feature enhancement network trained end-to-end with the detector's loss function, ensuring the enhancement process is explicitly guided to refine features most relevant to the detection task. When integrated with YOLOv8m on challenging underwater datasets, AquaFeat achieves state-of-the-art Precision (0.877) and Recall (0.624), along with competitive mAP scores (mAP@0.5 of 0.677 and mAP@[0.5:0.95] of 0.421). By delivering these accuracy gains while maintaining a practical processing speed of 46.5 FPS, our model provides an effective and computationally efficient solution for real-world applications, such as marine ecosystem monitoring and infrastructure inspection.

</details>


### [21] [MBMamba: When Memory Buffer Meets Mamba for Structure-Aware Image Deblurring](https://arxiv.org/abs/2508.12346)
*Hu Gao,Depeng Dang*

Main category: cs.CV

TL;DR: MBMamba：一种基于Mamba架构的图像去模糊网络，通过内存缓冲机制和Ising正则化损失解决局部像素遗忘和通道冗余问题，在不改变原始架构的情况下提升性能


<details>
  <summary>Details</summary>
Motivation: Mamba架构在图像去模糊中表现出潜力，但其flatten-and-scan策略导致局部像素遗忘和通道冗余，现有改进方法增加了计算复杂度影响实时性能

Method: 提出内存缓冲机制保存历史信息用于后续融合，引入Ising启发的正则化损失模拟像素间"相互吸引"的能量最小化，保持图像结构和连贯性

Result: 在广泛使用的基准测试中优于最先进方法

Conclusion: MBMamba在不改变Mamba原始架构的情况下有效解决了局部信息丢失问题，实现了更好的图像去模糊性能

Abstract: The Mamba architecture has emerged as a promising alternative to CNNs and Transformers for image deblurring. However, its flatten-and-scan strategy often results in local pixel forgetting and channel redundancy, limiting its ability to effectively aggregate 2D spatial information. Although existing methods mitigate this by modifying the scan strategy or incorporating local feature modules, it increase computational complexity and hinder real-time performance. In this paper, we propose a structure-aware image deblurring network without changing the original Mamba architecture. Specifically, we design a memory buffer mechanism to preserve historical information for later fusion, enabling reliable modeling of relevance between adjacent features. Additionally, we introduce an Ising-inspired regularization loss that simulates the energy minimization of the physical system's "mutual attraction" between pixels, helping to maintain image structure and coherence. Building on this, we develop MBMamba. Experimental results show that our method outperforms state-of-the-art approaches on widely used benchmarks.

</details>


### [22] [Synthetic Data is Sufficient for Zero-Shot Visual Generalization from Offline Data](https://arxiv.org/abs/2508.12356)
*Ahmet H. Güzel,Ilija Bogunovic,Jack Parker-Holder*

Main category: cs.CV

TL;DR: 通过两步步骤：首先对离线数据进行增帆提升零梭法通用性，然后使用死元模型在潜空间生成合成数据，从而提高视觉基离线强化学习的演策通用性性能。


<details>
  <summary>Details</summary>
Motivation: 解决离线强化学习中视觉数据的噪声、干扰和偏相关联问题，充分利用视觉基离线数据训练更稳健且具有良好通用性的演策。

Method: 首先对原始离线数据进行增帆提升多样性，然后使用死元模型在潜空间生成额外的合成训练数据，无需改变现有模型免费离线RL算法。

Result: 在连续动作空间(Visual D4RL)和离散动作空间(Procgen)上都显示出显著的通用性提升，同时增加了训练数据多样性并减小了测试时的通用性差距。

Conclusion: 该方法为使用合成数据训练具有更强通用性的演策提供了有前景的方向，且保持了计算效率。

Abstract: Offline reinforcement learning (RL) offers a promising framework for training agents using pre-collected datasets without the need for further environment interaction. However, policies trained on offline data often struggle to generalise due to limited exposure to diverse states. The complexity of visual data introduces additional challenges such as noise, distractions, and spurious correlations, which can misguide the policy and increase the risk of overfitting if the training data is not sufficiently diverse. Indeed, this makes it challenging to leverage vision-based offline data in training robust agents that can generalize to unseen environments. To solve this problem, we propose a simple approach generating additional synthetic training data. We propose a two-step process, first augmenting the originally collected offline data to improve zero-shot generalization by introducing diversity, then using a diffusion model to generate additional data in latent space. We test our method across both continuous action spaces (Visual D4RL) and discrete action spaces (Procgen), demonstrating that it significantly improves generalization without requiring any algorithmic changes to existing model-free offline RL methods. We show that our method not only increases the diversity of the training data but also significantly reduces the generalization gap at test time while maintaining computational efficiency. We believe this approach could fuel additional progress in generating synthetic data to train more general agents in the future.

</details>


### [23] [DeCoT: Decomposing Complex Instructions for Enhanced Text-to-Image Generation with Large Language Models](https://arxiv.org/abs/2508.12396)
*Xiaochuan Lin,Xiangyong Chen,Xuan Li,Yichen Su*

Main category: cs.CV

TL;DR: DeCoT是一个通过大语言模型分解复杂文本指令来提升文生图模型性能的新框架，在LongBench-T2I基准测试中显著改善了文本和构图等挑战性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 当前文生图模型在处理复杂长文本指令时存在困难，经常无法准确渲染细节、空间关系和特定约束，需要更好的方法来理解和执行复杂指令。

Method: DeCoT采用两阶段框架：1）复杂指令分解和语义增强，用LLM将原始指令分解为结构化语义单元；2）多阶段提示集成和自适应生成，将这些单元转换为适合现有T2I模型的层次化或优化提示。

Result: 在LongBench-T2I数据集上，DeCoT显著提升了主流T2I模型的性能，特别是"文本"和"构图"方面。与Infinity-8B集成时平均得分3.52，优于基线3.44。人工评估也证实了感知质量和指令保真度的提升。

Conclusion: DeCoT有效弥合了高级用户意图与T2I模型需求之间的差距，实现了更忠实和准确的图像生成，每个组件都发挥了关键作用，复杂的LLM提示策略也很重要。

Abstract: Despite remarkable advancements, current Text-to-Image (T2I) models struggle with complex, long-form textual instructions, frequently failing to accurately render intricate details, spatial relationships, or specific constraints. This limitation is highlighted by benchmarks such as LongBench-T2I, which reveal deficiencies in handling composition, specific text, and fine textures. To address this, we propose DeCoT (Decomposition-CoT), a novel framework that leverages Large Language Models (LLMs) to significantly enhance T2I models' understanding and execution of complex instructions. DeCoT operates in two core stages: first, Complex Instruction Decomposition and Semantic Enhancement, where an LLM breaks down raw instructions into structured, actionable semantic units and clarifies ambiguities; second, Multi-Stage Prompt Integration and Adaptive Generation, which transforms these units into a hierarchical or optimized single prompt tailored for existing T2I models. Extensive experiments on the LongBench-T2I dataset demonstrate that DeCoT consistently and substantially improves the performance of leading T2I models across all evaluated dimensions, particularly in challenging aspects like "Text" and "Composition". Quantitative results, validated by multiple MLLM evaluators (Gemini-2.0-Flash and InternVL3-78B), show that DeCoT, when integrated with Infinity-8B, achieves an average score of 3.52, outperforming the baseline Infinity-8B (3.44). Ablation studies confirm the critical contribution of each DeCoT component and the importance of sophisticated LLM prompting. Furthermore, human evaluations corroborate these findings, indicating superior perceptual quality and instruction fidelity. DeCoT effectively bridges the gap between high-level user intent and T2I model requirements, leading to more faithful and accurate image generation.

</details>


### [24] [TiP4GEN: Text to Immersive Panorama 4D Scene Generation](https://arxiv.org/abs/2508.12415)
*Ke Xing,Hanwen Liang,Dejia Xu,Yuyang Yin,Konstantinos N. Plataniotis,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: TiP4GEN是一个文本到动态全景场景生成框架，通过双分支生成模型和几何对齐重建模型，实现了高质量360度沉浸式动态场景的生成。


<details>
  <summary>Details</summary>
Motivation: 现有生成工作主要集中于静态场景或窄视角动态场景，无法提供真正的360度沉浸式体验，需要开发能够从任意视角生成动态全景场景的技术。

Method: 采用双分支生成模型（全景分支和透视分支）进行视频生成，通过双向交叉注意力机制实现信息交换；基于3D高斯泼溅的几何对齐重建模型，利用度量深度图对齐时空点云并初始化场景相机位姿。

Result: 实验证明所提设计的有效性，TiP4GEN在生成视觉吸引人且运动连贯的动态全景场景方面具有优越性。

Conclusion: TiP4GEN框架成功解决了动态全景场景生成的挑战，为创建高质量360度沉浸式虚拟环境提供了有效解决方案。

Abstract: With the rapid advancement and widespread adoption of VR/AR technologies, there is a growing demand for the creation of high-quality, immersive dynamic scenes. However, existing generation works predominantly concentrate on the creation of static scenes or narrow perspective-view dynamic scenes, falling short of delivering a truly 360-degree immersive experience from any viewpoint. In this paper, we introduce \textbf{TiP4GEN}, an advanced text-to-dynamic panorama scene generation framework that enables fine-grained content control and synthesizes motion-rich, geometry-consistent panoramic 4D scenes. TiP4GEN integrates panorama video generation and dynamic scene reconstruction to create 360-degree immersive virtual environments. For video generation, we introduce a \textbf{Dual-branch Generation Model} consisting of a panorama branch and a perspective branch, responsible for global and local view generation, respectively. A bidirectional cross-attention mechanism facilitates comprehensive information exchange between the branches. For scene reconstruction, we propose a \textbf{Geometry-aligned Reconstruction Model} based on 3D Gaussian Splatting. By aligning spatial-temporal point clouds using metric depth maps and initializing scene cameras with estimated poses, our method ensures geometric consistency and temporal coherence for the reconstructed scenes. Extensive experiments demonstrate the effectiveness of our proposed designs and the superiority of TiP4GEN in generating visually compelling and motion-coherent dynamic panoramic scenes. Our project page is at https://ke-xing.github.io/TiP4GEN/.

</details>


### [25] [Standardization of Neuromuscular Reflex Analysis -- Role of Fine-Tuned Vision-Language Model Consortium and OpenAI gpt-oss Reasoning LLM Enabled Decision Support System](https://arxiv.org/abs/2508.12473)
*Eranga Bandara,Ross Gore,Sachin Shetty,Ravi Mukkamala,Christopher Rhea,Atmaram Yarlagadda,Shaifali Kaushik,L. H. M. P. De Silva,Andriy Maznychenko,Inna Sokolowska,Amin Hass,Kasun De Zoysa*

Main category: cs.CV

TL;DR: 这篇论文提出了一种基于精调视觉-语言模型联盟和理解大语言模型的决策支持系统，用于自动化H-反射电机图形分析和诊断。


<details>
  <summary>Details</summary>
Motivation: 传统H-反射EMG波形分析存在变异性大、解释偏差和可靠性低的问题，需要提高神经肌肉诊断的标准化和自动化水平。

Method: 使用多个细调的视觉-语言模型组成联盟，每个模型基于清单标注的H-反射EMG图像数据进行训练，然后通过共识机制聚合输出，再由专门的理解LLM进行精炼。

Result: 实验结果显示该混合系统能够提供高精度、一致性和可解释性的H-反射评估，显著提升了神经肌肉诊断的自动化和标准化水平。

Conclusion: 该研究首次将精调VLM聚合与理解LLM集成用于图像基H-反射分析，为下一代AI辅助神经肌肉评估和运动员监测平台奠定了基础。

Abstract: Accurate assessment of neuromuscular reflexes, such as the H-reflex, plays a critical role in sports science, rehabilitation, and clinical neurology. Traditional analysis of H-reflex EMG waveforms is subject to variability and interpretation bias among clinicians and researchers, limiting reliability and standardization. To address these challenges, we propose a Fine-Tuned Vision-Language Model (VLM) Consortium and a reasoning Large-Language Model (LLM)-enabled Decision Support System for automated H-reflex waveform interpretation and diagnosis. Our approach leverages multiple VLMs, each fine-tuned on curated datasets of H-reflex EMG waveform images annotated with clinical observations, recovery timelines, and athlete metadata. These models are capable of extracting key electrophysiological features and predicting neuromuscular states, including fatigue, injury, and recovery, directly from EMG images and contextual metadata. Diagnostic outputs from the VLM consortium are aggregated using a consensus-based method and refined by a specialized reasoning LLM, which ensures robust, transparent, and explainable decision support for clinicians and sports scientists. The end-to-end platform orchestrates seamless communication between the VLM ensemble and the reasoning LLM, integrating prompt engineering strategies and automated reasoning workflows using LLM Agents. Experimental results demonstrate that this hybrid system delivers highly accurate, consistent, and interpretable H-reflex assessments, significantly advancing the automation and standardization of neuromuscular diagnostics. To our knowledge, this work represents the first integration of a fine-tuned VLM consortium with a reasoning LLM for image-based H-reflex analysis, laying the foundation for next-generation AI-assisted neuromuscular assessment and athlete monitoring platforms.

</details>


### [26] [LangVision-LoRA-NAS: Neural Architecture Search for Variable LoRA Rank in Vision Language Models](https://arxiv.org/abs/2508.12512)
*Krishna Teja Chitty-Venkata,Murali Emani,Venkatram Vishwanath*

Main category: cs.CV

TL;DR: 本文提出了LangVision-LoRA-NAS框架，通过神经架构搜索为视觉语言模型动态优化LoRA秩配置，在提升性能的同时降低微调成本。


<details>
  <summary>Details</summary>
Motivation: 现有的LoRA方法使用固定秩进行微调，限制了在不同多模态任务中的灵活性和效率，需要一种能够动态调整秩配置的方法。

Method: 将神经架构搜索(NAS)与LoRA结合，动态搜索最优的LoRA秩配置，针对特定多模态任务平衡性能和计算效率。

Result: 在LLaMA-3.2-11B模型上的实验表明，该方法显著提升了模型性能，同时降低了微调成本。

Conclusion: LangVision-LoRA-NAS框架为视觉语言模型提供了一种高效的变秩适应方法，在性能和效率之间取得了良好平衡。

Abstract: Vision Language Models (VLMs) integrate visual and text modalities to enable multimodal understanding and generation. These models typically combine a Vision Transformer (ViT) as an image encoder and a Large Language Model (LLM) for text generation. LoRA (Low-Rank Adaptation) is an efficient fine-tuning method to adapt pre-trained models to new tasks by introducing low-rank updates to their weights. While LoRA has emerged as a powerful technique for fine-tuning large models by introducing low-rank updates, current implementations assume a fixed rank, potentially limiting flexibility and efficiency across diverse tasks. This paper introduces \textit{LangVision-LoRA-NAS}, a novel framework that integrates Neural Architecture Search (NAS) with LoRA to optimize VLMs for variable-rank adaptation. Our approach leverages NAS to dynamically search for the optimal LoRA rank configuration tailored to specific multimodal tasks, balancing performance and computational efficiency. Through extensive experiments using the LLaMA-3.2-11B model on several datasets, LangVision-LoRA-NAS demonstrates notable improvement in model performance while reducing fine-tuning costs. Our Base and searched fine-tuned models on LLaMA-3.2-11B-Vision-Instruct can be found \href{https://huggingface.co/collections/krishnateja95/llama-32-11b-vision-instruct-langvision-lora-nas-6786cac480357a6a6fcc59ee}{\textcolor{blue}{here}} and the code for LangVision-LoRA-NAS can be found \href{https://github.com/krishnateja95/LangVision-NAS}{\textcolor{blue}{here}}.

</details>


### [27] [ViLaD: A Large Vision Language Diffusion Framework for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.12603)
*Can Cui,Yupeng Zhou,Juntong Peng,Sung-Yeon Park,Zichong Yang,Prashanth Sankaranarayanan,Jiaru Zhang,Ruqi Zhang,Ziran Wang*

Main category: cs.CV

TL;DR: ViLaD是一个基于扩散模型的新型端到端自动驾驶框架，通过并行生成驾驶决策序列显著降低延迟，支持双向推理，在nuScenes数据集上表现优于现有自回归VLM方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视觉语言模型的自动驾驶系统依赖自回归架构，存在推理延迟高、无法进行双向推理的问题，不适合动态的安全关键环境。

Method: 采用掩码扩散模型实现驾驶决策序列的并行生成，支持双向推理和渐进式简单优先生成策略。

Result: 在nuScenes数据集上，ViLaD在规划精度和推理速度方面均优于最先进的自回归VLM基线，接近零失败率，并在真实自动驾驶车辆上成功部署验证。

Conclusion: ViLaD框架代表了自动驾驶领域的范式转变，通过扩散模型解决了自回归架构的局限性，为实际应用提供了高效可靠的解决方案。

Abstract: End-to-end autonomous driving systems built on Vision Language Models (VLMs) have shown significant promise, yet their reliance on autoregressive architectures introduces some limitations for real-world applications. The sequential, token-by-token generation process of these models results in high inference latency and cannot perform bidirectional reasoning, making them unsuitable for dynamic, safety-critical environments. To overcome these challenges, we introduce ViLaD, a novel Large Vision Language Diffusion (LVLD) framework for end-to-end autonomous driving that represents a paradigm shift. ViLaD leverages a masked diffusion model that enables parallel generation of entire driving decision sequences, significantly reducing computational latency. Moreover, its architecture supports bidirectional reasoning, allowing the model to consider both past and future simultaneously, and supports progressive easy-first generation to iteratively improve decision quality. We conduct comprehensive experiments on the nuScenes dataset, where ViLaD outperforms state-of-the-art autoregressive VLM baselines in both planning accuracy and inference speed, while achieving a near-zero failure rate. Furthermore, we demonstrate the framework's practical viability through a real-world deployment on an autonomous vehicle for an interactive parking task, confirming its effectiveness and soundness for practical applications.

</details>


### [28] [ViDA-UGC: Detailed Image Quality Analysis via Visual Distortion Assessment for UGC Images](https://arxiv.org/abs/2508.12605)
*Wenjie Liao,Jieyu Yuan,Yifang Xu,Chunle Guo,Zilong Zhang,Jihong Li,Jiachen Fu,Haotian Fan,Tao Li,Junhui Cui,Chongyi Li*

Main category: cs.CV

TL;DR: 该研究构建了首个大规模视觉失真评估指令调优数据集ViDA-UGC，专门针对用户生成内容图像，包含细粒度质量标注和详细质量描述，通过CoT框架提升多模态大语言模型的图像质量评估能力。


<details>
  <summary>Details</summary>
Motivation: 当前可解释图像质量评估方法存在两个主要问题：一是对用户生成内容和AI生成内容使用相同的失真标准进行评估不够合理；二是缺乏详细的图像质量分析来监控质量和指导图像恢复。

Method: 通过失真导向的流程构建ViDA-UGC数据集，包含11K图像，采用人类标注和Chain-of-Thought评估框架引导GPT-4o生成质量描述，并精选476图像构建ViDA-UGC-Bench基准。

Result: 实验结果表明，ViDA-UGC和CoT框架能持续增强多种基础MLLM在图像质量分析方面的能力，在ViDA-UGC-Bench和Q-Bench基准测试中甚至超越了GPT-4o的表现。

Conclusion: 该研究为UGC图像质量评估提供了首个大规模数据集和基准测试，通过CoT框架有效提升了MLLM的图像质量分析能力，为图像质量监控和恢复指导提供了重要工具。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have introduced a paradigm shift for Image Quality Assessment (IQA) from unexplainable image quality scoring to explainable IQA, demonstrating practical applications like quality control and optimization guidance. However, current explainable IQA methods not only inadequately use the same distortion criteria to evaluate both User-Generated Content (UGC) and AI-Generated Content (AIGC) images, but also lack detailed quality analysis for monitoring image quality and guiding image restoration. In this study, we establish the first large-scale Visual Distortion Assessment Instruction Tuning Dataset for UGC images, termed ViDA-UGC, which comprises 11K images with fine-grained quality grounding, detailed quality perception, and reasoning quality description data. This dataset is constructed through a distortion-oriented pipeline, which involves human subject annotation and a Chain-of-Thought (CoT) assessment framework. This framework guides GPT-4o to generate quality descriptions by identifying and analyzing UGC distortions, which helps capturing rich low-level visual features that inherently correlate with distortion patterns. Moreover, we carefully select 476 images with corresponding 6,149 question answer pairs from ViDA-UGC and invite a professional team to ensure the accuracy and quality of GPT-generated information. The selected and revised data further contribute to the first UGC distortion assessment benchmark, termed ViDA-UGC-Bench. Experimental results demonstrate the effectiveness of the ViDA-UGC and CoT framework for consistently enhancing various image quality analysis abilities across multiple base MLLMs on ViDA-UGC-Bench and Q-Bench, even surpassing GPT-4o.

</details>


### [29] [OpenMoCap: Rethinking Optical Motion Capture under Real-world Occlusion](https://arxiv.org/abs/2508.12610)
*Chen Qian,Danyang Li,Xinran Yu,Zheng Yang,Qiang Ma*

Main category: cs.CV

TL;DR: 这篇论文解决了光学动作抓取中标记点遮挡问题，通过新的CMU-Occlu数据集和OpenMoCap模型提升了遮挡情况下的动作解析精度


<details>
  <summary>Details</summary>
Motivation: 现有光学动作抓取系统在大规模标记点遮挡情况下性能严重下降，主要问题是缺乏反映实际遮挡模式的训练数据集和无法抓取标记点间长程依赖关系

Method: 提出CMU-Occlu数据集，利用光线追踪技术模拟实际遮挡模式；设计OpenMoCap模型，通过标记-关节链推理机制实现标记与关节间深度约束的同时优化

Result: 对比实验显示OpenMoCap在多种场景下都明显超过竞争方法，CMU-Occlu数据集为稳健动作解析研究打开了新方向

Conclusion: 该研究成功解决了光学动作抓取中遮挡问题，通过实际的数据模拟和创新的模型设计，为虚拟现实和电影制作等领域提供了更稳健的动作抓取解决方案

Abstract: Optical motion capture is a foundational technology driving advancements in cutting-edge fields such as virtual reality and film production. However, system performance suffers severely under large-scale marker occlusions common in real-world applications. An in-depth analysis identifies two primary limitations of current models: (i) the lack of training datasets accurately reflecting realistic marker occlusion patterns, and (ii) the absence of training strategies designed to capture long-range dependencies among markers. To tackle these challenges, we introduce the CMU-Occlu dataset, which incorporates ray tracing techniques to realistically simulate practical marker occlusion patterns. Furthermore, we propose OpenMoCap, a novel motion-solving model designed specifically for robust motion capture in environments with significant occlusions. Leveraging a marker-joint chain inference mechanism, OpenMoCap enables simultaneous optimization and construction of deep constraints between markers and joints. Extensive comparative experiments demonstrate that OpenMoCap consistently outperforms competing methods across diverse scenarios, while the CMU-Occlu dataset opens the door for future studies in robust motion solving. The proposed OpenMoCap is integrated into the MoSen MoCap system for practical deployment. The code is released at: https://github.com/qianchen214/OpenMoCap.

</details>


### [30] [DyCrowd: Towards Dynamic Crowd Reconstruction from a Large-scene Video](https://arxiv.org/abs/2508.12644)
*Hao Wen,Hongbo Kang,Jian Ma,Jing Huang,Yuanwang Yang,Haozhe Lin,Yu-Kun Lai,Kun Li*

Main category: cs.CV

TL;DR: DyCrowd是首个从大场景视频中进行时空一致3D人群重建的框架，通过粗到细的群体引导运动优化策略和VAE运动先验，有效解决遮挡问题，并贡献了VirtualCrowd虚拟数据集。


<details>
  <summary>Details</summary>
Motivation: 当前方法从静态图像重建3D人群缺乏时间一致性，无法解决遮挡问题，需要开发能够处理大场景视频中动态人群重建的方法。

Method: 提出粗到细的群体引导运动优化策略，结合VAE人类运动先验和分段级群体引导优化，利用异步运动一致性损失(AMC)实现遮挡恢复。

Result: 实验结果表明该方法在大场景动态人群重建任务中达到state-of-the-art性能。

Conclusion: DyCrowd框架能够有效处理大场景视频中的动态人群3D重建，解决了时间一致性和遮挡问题，为相关研究提供了新的解决方案和数据集。

Abstract: 3D reconstruction of dynamic crowds in large scenes has become increasingly important for applications such as city surveillance and crowd analysis. However, current works attempt to reconstruct 3D crowds from a static image, causing a lack of temporal consistency and inability to alleviate the typical impact caused by occlusions. In this paper, we propose DyCrowd, the first framework for spatio-temporally consistent 3D reconstruction of hundreds of individuals' poses, positions and shapes from a large-scene video. We design a coarse-to-fine group-guided motion optimization strategy for occlusion-robust crowd reconstruction in large scenes. To address temporal instability and severe occlusions, we further incorporate a VAE (Variational Autoencoder)-based human motion prior along with a segment-level group-guided optimization. The core of our strategy leverages collective crowd behavior to address long-term dynamic occlusions. By jointly optimizing the motion sequences of individuals with similar motion segments and combining this with the proposed Asynchronous Motion Consistency (AMC) loss, we enable high-quality unoccluded motion segments to guide the motion recovery of occluded ones, ensuring robust and plausible motion recovery even in the presence of temporal desynchronization and rhythmic inconsistencies. Additionally, in order to fill the gap of no existing well-annotated large-scene video dataset, we contribute a virtual benchmark dataset, VirtualCrowd, for evaluating dynamic crowd reconstruction from large-scene videos. Experimental results demonstrate that the proposed method achieves state-of-the-art performance in the large-scene dynamic crowd reconstruction task. The code and dataset will be available for research purposes.

</details>


### [31] [Stable Diffusion-Based Approach for Human De-Occlusion](https://arxiv.org/abs/2508.12663)
*Seung Young Noh,Ju Yong Chang*

Main category: cs.CV

TL;DR: 本文提出了一种两阶段深度学习方法，通过掩码完成和RGB完成来恢复遮挡人体的结构和外观。方法结合了双向模型、关节热力图和人体特征描述，在严重遮挡情况下也能准确重建人体外观，并能提升下游任务的性能。


<details>
  <summary>Details</summary>
Motivation: 人类能够利用先验知识和可见线索推断遮挡物体的缺失部分，但让深度学习模型准确预测遮挡区域仍是一项挑战性任务。特别是在人体去遮挡领域，需要恢复遮挡的身体结构和外观。

Method: 方法将任务分解为两个阶段：掩码完成和RGB完成。第一阶段利用基于反向模型的人体先验知识，结合遮挡关节热力图提供缺失区域的显式空间线索。重建的无模态掩码作为第二阶段的条件输入，指导RGB重建。还结合了通过VQA模型提取的人体特征文本描述，并使用Stable Diffusion进行RGB完成，通过解码器微调减少可见区域的像素级退化。

Result: 该方法在严重遮挡情况下也能有效地重建人体外观，并在掩码完成和RGB完成上都持续超越现有方法。甚至，通过该方法生成的去遮挡图像还能提高下游任务的性能，如2D姿势估计和3D人体重建。

Conclusion: 该研究提出了一种有效的人体去遮挡方法，通过两阶段分解、结合多种提示信息（包括掩码、关节热力图、文本特征）和优化的反向模型实现，成功解决了严重遮挡下人体外观重建的挑战，并为下游任务提供了更好的输入数据。

Abstract: Humans can infer the missing parts of an occluded object by leveraging prior knowledge and visible cues. However, enabling deep learning models to accurately predict such occluded regions remains a challenging task. De-occlusion addresses this problem by reconstructing both the mask and RGB appearance. In this work, we focus on human de-occlusion, specifically targeting the recovery of occluded body structures and appearances. Our approach decomposes the task into two stages: mask completion and RGB completion. The first stage leverages a diffusion-based human body prior to provide a comprehensive representation of body structure, combined with occluded joint heatmaps that offer explicit spatial cues about missing regions. The reconstructed amodal mask then serves as a conditioning input for the second stage, guiding the model on which areas require RGB reconstruction. To further enhance RGB generation, we incorporate human-specific textual features derived using a visual question answering (VQA) model and encoded via a CLIP encoder. RGB completion is performed using Stable Diffusion, with decoder fine-tuning applied to mitigate pixel-level degradation in visible regions -- a known limitation of prior diffusion-based de-occlusion methods caused by latent space transformations. Our method effectively reconstructs human appearances even under severe occlusions and consistently outperforms existing methods in both mask and RGB completion. Moreover, the de-occluded images generated by our approach can improve the performance of downstream human-centric tasks, such as 2D pose estimation and 3D human reconstruction. The code will be made publicly available.

</details>


### [32] [WP-CLIP: Leveraging CLIP to Predict Wölfflin's Principles in Visual Art](https://arxiv.org/abs/2508.12668)
*Abhijay Ghildyal,Li-Yun Wang,Feng Liu*

Main category: cs.CV

TL;DR: 这篇论文研究了如何使用CLIP模型预测Wölfflin五大艺术原则，通过细调在绘画数据集上的方法提升了对艺术风格的理解能力。


<details>
  <summary>Details</summary>
Motivation: Wölfflin五大原则提供了结构化的艺术分析方法，但现有计量方法无法有效预测所有原则。视觉-语言模型在抽象图像属性评估方面的进展使得它们成为潜在的解决方案。

Method: 首先评估预训练的CLIP模型是否能理解Wölfflin原则，然后在注释过的真实艺术图像数据集上对CLIP进行细调，以预测每个原则的得分。开发了WP-CLIP模型。

Result: 研究发现预训练的CLIP并不能内在理解这些细致的艺术元素。细调后的WP-CLIP模型能够在GAN生成的绘画和Pandora-18K艺术数据集上进行良好的泛化，适用于多样的艺术风格。

Conclusion: 这项研究高亮了视视-语言模型在自动化艺术分析方面的潜力，通过有目标的细调可以让模型掌握更深入的艺术风格特征。

Abstract: W\"olfflin's five principles offer a structured approach to analyzing stylistic variations for formal analysis. However, no existing metric effectively predicts all five principles in visual art. Computationally evaluating the visual aspects of a painting requires a metric that can interpret key elements such as color, composition, and thematic choices. Recent advancements in vision-language models (VLMs) have demonstrated their ability to evaluate abstract image attributes, making them promising candidates for this task. In this work, we investigate whether CLIP, pre-trained on large-scale data, can understand and predict W\"olfflin's principles. Our findings indicate that it does not inherently capture such nuanced stylistic elements. To address this, we fine-tune CLIP on annotated datasets of real art images to predict a score for each principle. We evaluate our model, WP-CLIP, on GAN-generated paintings and the Pandora-18K art dataset, demonstrating its ability to generalize across diverse artistic styles. Our results highlight the potential of VLMs for automated art analysis.

</details>


### [33] [Single-Reference Text-to-Image Manipulation with Dual Contrastive Denoising Score](https://arxiv.org/abs/2508.12718)
*Syed Muhmmad Israr,Feng Zhao*

Main category: cs.CV

TL;DR: 基于对比学习的双对比去噪框架，利用潜在潜散模型的自注意力层中间表征，实现真实图像编辑中的灵活内容修改和结构保持


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像生成模型在真实图像编辑中的两大挑战：用户难以准确描述所有视觉细节的提示词，以及现有方法在修改特定区域时容易造成意外变化

Method: 提出双对比去噪得分框架，在潜在潜散模型的自注意力层中间表征中引入简单的双对比损失，无需附加网络

Result: 方法在真实图像编辑上超过现有方法，同时维持了直接使用预训练文本到图像潜散模型的能力，实现了灵活内容修改和结构保持

Conclusion: 该框架通过利用潜在潜散模型的中间表征和对比学习机制，有效解决了真实图像编辑的关键挑战，具有零样本图像到图像转换能力

Abstract: Large-scale text-to-image generative models have shown remarkable ability to synthesize diverse and high-quality images. However, it is still challenging to directly apply these models for editing real images for two reasons. First, it is difficult for users to come up with a perfect text prompt that accurately describes every visual detail in the input image. Second, while existing models can introduce desirable changes in certain regions, they often dramatically alter the input content and introduce unexpected changes in unwanted regions. To address these challenges, we present Dual Contrastive Denoising Score, a simple yet powerful framework that leverages the rich generative prior of text-to-image diffusion models. Inspired by contrastive learning approaches for unpaired image-to-image translation, we introduce a straightforward dual contrastive loss within the proposed framework. Our approach utilizes the extensive spatial information from the intermediate representations of the self-attention layers in latent diffusion models without depending on auxiliary networks. Our method achieves both flexible content modification and structure preservation between input and output images, as well as zero-shot image-to-image translation. Through extensive experiments, we show that our approach outperforms existing methods in real image editing while maintaining the capability to directly utilize pretrained text-to-image diffusion models without further training.

</details>


### [34] [Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian Splatting](https://arxiv.org/abs/2508.12720)
*Kangjie Chen,Yingji Zhong,Zhihao Li,Jiaqi Lin,Youyu Chen,Minghan Qin,Haoqian Wang*

Main category: cs.CV

TL;DR: 本文分析了稀疏视图下3D高斯泼溅(3DGS)的外观伪影问题，提出了量化高斯纠缠程度的共适应评分(CA)，并提出了两种轻量级策略来缓解共适应效应。


<details>
  <summary>Details</summary>
Motivation: 3DGS在密集视图下表现优异，但在稀疏视图场景中，尽管训练视图渲染效果真实，但在新视图中会出现外观伪影。本文旨在探究这些伪影的根本原因。

Method: 提出了共适应评分(CA)来量化高斯之间的纠缠程度，并提出了两种策略：(1)随机高斯丢弃；(2)对不透明度注入乘性噪声。这两种策略都是即插即用的。

Result: 分析发现高斯之间的共适应程度随着训练视图数量的增加而自然缓解。提出的两种策略在各种方法和基准测试中都证明了有效性。

Conclusion: 对共适应效应的深入理解将有助于社区更全面地理解稀疏视图3DGS，提出的轻量级策略能有效缓解外观伪影问题。

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated impressive performance in novel view synthesis under dense-view settings. However, in sparse-view scenarios, despite the realistic renderings in training views, 3DGS occasionally manifests appearance artifacts in novel views. This paper investigates the appearance artifacts in sparse-view 3DGS and uncovers a core limitation of current approaches: the optimized Gaussians are overly-entangled with one another to aggressively fit the training views, which leads to a neglect of the real appearance distribution of the underlying scene and results in appearance artifacts in novel views. The analysis is based on a proposed metric, termed Co-Adaptation Score (CA), which quantifies the entanglement among Gaussians, i.e., co-adaptation, by computing the pixel-wise variance across multiple renderings of the same viewpoint, with different random subsets of Gaussians. The analysis reveals that the degree of co-adaptation is naturally alleviated as the number of training views increases. Based on the analysis, we propose two lightweight strategies to explicitly mitigate the co-adaptation in sparse-view 3DGS: (1) random gaussian dropout; (2) multiplicative noise injection to the opacity. Both strategies are designed to be plug-and-play, and their effectiveness is validated across various methods and benchmarks. We hope that our insights into the co-adaptation effect will inspire the community to achieve a more comprehensive understanding of sparse-view 3DGS.

</details>


### [35] [Frequency-Driven Inverse Kernel Prediction for Single Image Defocus Deblurring](https://arxiv.org/abs/2508.12736)
*Ying Zhang,Xiongxin Tang,Chongyi Li,Qiao Chen,Yuquan Wu*

Main category: cs.CV

TL;DR: 频域驱动的逆核预测网络(FDIKP)，通过双分支逆核预测策略和位置适配卷积，提升了单图散焦去模糊的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依靠空间特征估计核，但在严重模糊区域性能会降级，因为局部高频细节缺失。频域基础表示能够提供更好的结构可识别性。

Method: 设计了双分支逆核预测策略(DIKP)来提高核估计精度和稳定性；使用位置适配卷积(PAC)增强反卷积过程的适应性；提出双域尺度递归模块(DSRM)来融合反卷积结果并渐进改善去模糊质量。

Result: 大量实验证明该方法在单图散焦去模糊任务上超过现有方法。

Conclusion: 通过利用频域基础表示和创新的模块设计，本文方法能够有效地解决严重模糊区域的核估计问题，实现了更优秀的去模糊效果。

Abstract: Single image defocus deblurring aims to recover an all-in-focus image from a defocus counterpart, where accurately modeling spatially varying blur kernels remains a key challenge. Most existing methods rely on spatial features for kernel estimation, but their performance degrades in severely blurry regions where local high-frequency details are missing. To address this, we propose a Frequency-Driven Inverse Kernel Prediction network (FDIKP) that incorporates frequency-domain representations to enhance structural identifiability in kernel modeling. Given the superior discriminative capability of the frequency domain for blur modeling, we design a Dual-Branch Inverse Kernel Prediction (DIKP) strategy that improves the accuracy of kernel estimation while maintaining stability. Moreover, considering the limited number of predicted inverse kernels, we introduce a Position Adaptive Convolution (PAC) to enhance the adaptability of the deconvolution process. Finally, we propose a Dual-Domain Scale Recurrent Module (DSRM) to fuse deconvolution results and progressively improve deblurring quality from coarse to fine. Extensive experiments demonstrate that our method outperforms existing approaches. Code will be made publicly available.

</details>


### [36] [Leveraging Diffusion Models for Stylization using Multiple Style Images](https://arxiv.org/abs/2508.12784)
*Dan Ruta,Abdelaziz Djelouah,Raphael Ortiz,Christopher Schroers*

Main category: cs.CV

TL;DR: 基于多样式图片的潜在扩散模型风格转换方法，通过图片提示适配器和统计对齐技术，在去噪UNet的交叉注意力和自注意力层进行干预，实现了独特的风格转换效果


<details>
  <summary>Details</summary>
Motivation: 现有风格转换方法在准确匹配风格、可使用样式图片数量限制以及内容与风格恐合问题方面仍遇到挑战

Method: 利用多样式图片表征风格特征并防止内容泄漏，结合图片提示适配器和去噪过程中的特征统计对齐，在UNet的交叉注意力和自注意力层进行干预，通过聚类技术从大量样式样本中精炼小规模代表性注意力特征集

Result: 该方法在风格化任务上达到了最先进水平

Conclusion: 通过多样式图片结合图像提示适配器和统计对齐技术，有效解决了现有风格转换方法的限制，实现了更准确的风格匹配和更好的内容-风格分离

Abstract: Recent advances in latent diffusion models have enabled exciting progress in image style transfer. However, several key issues remain. For example, existing methods still struggle to accurately match styles. They are often limited in the number of style images that can be used. Furthermore, they tend to entangle content and style in undesired ways. To address this, we propose leveraging multiple style images which helps better represent style features and prevent content leaking from the style images. We design a method that leverages both image prompt adapters and statistical alignment of the features during the denoising process. With this, our approach is designed such that it can intervene both at the cross-attention and the self-attention layers of the denoising UNet. For the statistical alignment, we employ clustering to distill a small representative set of attention features from the large number of attention values extracted from the style samples. As demonstrated in our experimental section, the resulting method achieves state-of-the-art results for stylization.

</details>


### [37] [Next Visual Granularity Generation](https://arxiv.org/abs/2508.12811)
*Yikai Wang,Zhouxia Wang,Zhonghua Wu,Qingyi Tao,Kang Liao,Chen Change Loy*

Main category: cs.CV

TL;DR: 提出了一种新的图像生成方法NVG，通过将图像分解为结构化序列，从全局布局到细节逐步细化生成，在ImageNet上取得了优于VAR系列的FID分数。


<details>
  <summary>Details</summary>
Motivation: 为了解决图像生成过程中对多粒度层次控制的不足，提出了一种能够从粗到细逐步细化的结构化生成框架。

Method: 将图像分解为具有相同空间分辨率但不同视觉粒度的结构化序列，使用Next Visual Granularity (NVG)框架从空图像开始逐步生成，实现从全局布局到细节的层次化生成过程。

Result: 在ImageNet数据集上的类条件图像生成任务中，NVG模型相比VAR系列在FID分数上持续提升（3.30→3.03，2.57→2.44，2.09→2.06），并显示出清晰的缩放行为。

Conclusion: NVG框架通过结构化序列分解和渐进式细化生成，实现了对图像生成过程的多粒度层次控制，在性能上超越了现有方法，展现了强大的能力和潜力。

Abstract: We propose a novel approach to image generation by decomposing an image into a structured sequence, where each element in the sequence shares the same spatial resolution but differs in the number of unique tokens used, capturing different level of visual granularity. Image generation is carried out through our newly introduced Next Visual Granularity (NVG) generation framework, which generates a visual granularity sequence beginning from an empty image and progressively refines it, from global layout to fine details, in a structured manner. This iterative process encodes a hierarchical, layered representation that offers fine-grained control over the generation process across multiple granularity levels. We train a series of NVG models for class-conditional image generation on the ImageNet dataset and observe clear scaling behavior. Compared to the VAR series, NVG consistently outperforms it in terms of FID scores (3.30 -> 3.03, 2.57 ->2.44, 2.09 -> 2.06). We also conduct extensive analysis to showcase the capability and potential of the NVG framework. Our code and models will be released.

</details>


### [38] [DEEP-SEA: Deep-Learning Enhancement for Environmental Perception in Submerged Aquatics](https://arxiv.org/abs/2508.12824)
*Shuang Chen,Ronald Thenius,Farshad Arvin,Amir Atapour-Abarghouei*

Main category: cs.CV

TL;DR: DEEP-SEA是一个基于深度学习的海底图像恢复模型，通过双频增强自注意力机制同时优化空间和频率信息，有效解决水下图像退化问题


<details>
  <summary>Details</summary>
Motivation: 水下环境存在光散射、吸收和浑浊等问题，导致图像清晰度下降和颜色失真，影响海洋生物多样性监测和生态评估的准确性

Method: 提出双频增强自注意力空间和频率调制器，自适应地在频域中优化特征表示，同时保持空间信息以更好地保护结构

Result: 在EUVP和LSUI数据集上的实验表明，该模型在恢复精细图像细节和结构一致性方面优于现有最先进方法

Conclusion: DEEP-SEA通过有效缓解水下视觉退化问题，有潜力提高水下监测平台的可靠性，实现更准确的生态观测、物种识别和自主导航

Abstract: Continuous and reliable underwater monitoring is essential for assessing marine biodiversity, detecting ecological changes and supporting autonomous exploration in aquatic environments. Underwater monitoring platforms rely on mainly visual data for marine biodiversity analysis, ecological assessment and autonomous exploration. However, underwater environments present significant challenges due to light scattering, absorption and turbidity, which degrade image clarity and distort colour information, which makes accurate observation difficult. To address these challenges, we propose DEEP-SEA, a novel deep learning-based underwater image restoration model to enhance both low- and high-frequency information while preserving spatial structures. The proposed Dual-Frequency Enhanced Self-Attention Spatial and Frequency Modulator aims to adaptively refine feature representations in frequency domains and simultaneously spatial information for better structural preservation. Our comprehensive experiments on EUVP and LSUI datasets demonstrate the superiority over the state of the art in restoring fine-grained image detail and structural consistency. By effectively mitigating underwater visual degradation, DEEP-SEA has the potential to improve the reliability of underwater monitoring platforms for more accurate ecological observation, species identification and autonomous navigation.

</details>


### [39] [S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models](https://arxiv.org/abs/2508.12880)
*Chubin Chen,Jiashu Zhu,Xiaokun Feng,Nisha Huang,Meiqi Wu,Fangyuan Mao,Jiahong Wu,Xiangxiang Chu,Xiu Li*

Main category: cs.CV

TL;DR: S²-Guidance是一种新的导向方法，通过随机块投弃构建子网络，改善了CFG在模糊模型中的次优预测问题，在文本到图像/视频生成任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 经验分析发现CFG在高斯混合模型中产生与真实值的偏差，模型过度依赖次优预测导致语义不一致和低质量输出。

Method: 通过在前向过程中使用随机块投弃构建随机子网络，用于精炼模型的次优预测，引导模型避免低质量预测并向高质量输出发展。

Result: 在文本到图像和文本到视频生成任务中，S²-Guidance在定性和定量实验中都显示出更优的性能，一贵超过CFG和其他先进导向策略。

Conclusion: S²-Guidance通过利用模型自身的子网络精炼预测，有效解决了CFG的限制，为模糊模型提供了更有效的导向方法。

Abstract: Classifier-free Guidance (CFG) is a widely used technique in modern diffusion models for enhancing sample quality and prompt adherence. However, through an empirical analysis on Gaussian mixture modeling with a closed-form solution, we observe a discrepancy between the suboptimal results produced by CFG and the ground truth. The model's excessive reliance on these suboptimal predictions often leads to semantic incoherence and low-quality outputs. To address this issue, we first empirically demonstrate that the model's suboptimal predictions can be effectively refined using sub-networks of the model itself. Building on this insight, we propose S^2-Guidance, a novel method that leverages stochastic block-dropping during the forward process to construct stochastic sub-networks, effectively guiding the model away from potential low-quality predictions and toward high-quality outputs. Extensive qualitative and quantitative experiments on text-to-image and text-to-video generation tasks demonstrate that S^2-Guidance delivers superior performance, consistently surpassing CFG and other advanced guidance strategies. Our code will be released.

</details>


### [40] [CTFlow: Video-Inspired Latent Flow Matching for 3D CT Synthesis](https://arxiv.org/abs/2508.12900)
*Jiayi Wang,Hadrien Reynaud,Franciskus Xaverius Erick,Bernhard Kainz*

Main category: cs.CV

TL;DR: CTFlow是一个0.5B参数的潜在流匹配变换器模型，通过临床报告文本条件生成完整的3D CT体积，在时间一致性、图像多样性和文本-图像对齐方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 基于临床报告生成完整CT体积可以加速医学研究，通过数据增强、隐私保护合成和减少患者数据监管约束，同时保留诊断信号。CT-RATE数据集的发布使得训练大型文本条件CT生成模型成为可能。

Method: 使用FLUX的A-VAE定义潜在空间，CT-Clip文本编码器编码临床报告。采用自定义自回归方法：模型首先从纯文本预测第一个切片序列，然后基于先前生成的切片序列和文本预测后续序列，以保持整个CT体积的一致性并控制内存约束。

Result: 与最先进的CT生成模型相比，CTFlow在时间一致性、图像多样性和文本-图像对齐方面表现出优越性，通过FID、FVD、IS分数和CLIP分数进行评估。

Conclusion: CTFlow成功实现了基于临床报告的完整3D CT体积生成，为医学影像研究提供了有效的生成模型解决方案，在多个评估指标上超越了现有方法。

Abstract: Generative modelling of entire CT volumes conditioned on clinical reports has the potential to accelerate research through data augmentation, privacy-preserving synthesis and reducing regulator-constraints on patient data while preserving diagnostic signals. With the recent release of CT-RATE, a large-scale collection of 3D CT volumes paired with their respective clinical reports, training large text-conditioned CT volume generation models has become achievable. In this work, we introduce CTFlow, a 0.5B latent flow matching transformer model, conditioned on clinical reports. We leverage the A-VAE from FLUX to define our latent space, and rely on the CT-Clip text encoder to encode the clinical reports. To generate consistent whole CT volumes while keeping the memory constraints tractable, we rely on a custom autoregressive approach, where the model predicts the first sequence of slices of the volume from text-only, and then relies on the previously generated sequence of slices and the text, to predict the following sequence. We evaluate our results against state-of-the-art generative CT model, and demonstrate the superiority of our approach in terms of temporal coherence, image diversity and text-image alignment, with FID, FVD, IS scores and CLIP score.

</details>


### [41] [7Bench: a Comprehensive Benchmark for Layout-guided Text-to-image Models](https://arxiv.org/abs/2508.12919)
*Elena Izzo,Luca Parolari,Davide Vezzaro,Lamberto Ballan*

Main category: cs.CV

TL;DR: 7Bench是首个评估布局引导文本到图像生成中语义和空间对齐的基准测试，包含七个挑战性场景，用于评估对象生成、颜色保真度、属性识别、对象间关系和空间控制。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试主要评估文本对齐，而忽略了布局对齐，无法全面评估模型的空间保真度，这在合成数据生成等应用中至关重要。

Method: 提出了一个评估协议，在现有框架基础上加入布局对齐分数来评估空间准确性，并构建了包含七个挑战性场景的文本-布局对数据集。

Result: 使用7Bench评估了多个最先进的扩散模型，揭示了它们在不同对齐任务中的优势和局限性。

Conclusion: 7Bench填补了布局引导文本到图像生成评估的空白，为模型的空间保真度评估提供了重要工具，有助于提升合成数据质量。

Abstract: Layout-guided text-to-image models offer greater control over the generation process by explicitly conditioning image synthesis on the spatial arrangement of elements. As a result, their adoption has increased in many computer vision applications, ranging from content creation to synthetic data generation. A critical challenge is achieving precise alignment between the image, textual prompt, and layout, ensuring semantic fidelity and spatial accuracy. Although recent benchmarks assess text alignment, layout alignment remains overlooked, and no existing benchmark jointly evaluates both. This gap limits the ability to evaluate a model's spatial fidelity, which is crucial when using layout-guided generation for synthetic data, as errors can introduce noise and degrade data quality. In this work, we introduce 7Bench, the first benchmark to assess both semantic and spatial alignment in layout-guided text-to-image generation. It features text-and-layout pairs spanning seven challenging scenarios, investigating object generation, color fidelity, attribute recognition, inter-object relationships, and spatial control. We propose an evaluation protocol that builds on existing frameworks by incorporating the layout alignment score to assess spatial accuracy. Using 7Bench, we evaluate several state-of-the-art diffusion models, uncovering their respective strengths and limitations across diverse alignment tasks. The benchmark is available at https://github.com/Elizzo/7Bench.

</details>


### [42] [Compact Attention: Exploiting Structured Spatio-Temporal Sparsity for Fast Video Generation](https://arxiv.org/abs/2508.12969)
*Qirui Li,Guangcong Zheng,Qi Zhao,Jie Li,Bin Dong,Yiwu Yao,Xi Li*

Main category: cs.CV

TL;DR: 这篇论文提出了Compact Attention框架，通过认别和利用视频数据中的结构化稀疏模式，在保持认知质量的同时实现了注意力计算的1.6~2.5倍加速。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制的计算要求给基于transformer的视频生成带来了重大挑战，特别是在合成超长序列时。现有的因子化注意力和固定稀疏模式方法无法充分利用视频数据中内在的时空冗余性。

Method: 计算机确知的加速框架Compact Attention，包含三项创新：1)通过动态制图分组近似多样化空间交互模式的适应性制图策略；2)根据帧距离调整稀疏程度的时变窗口；3)在保留关键注意力途径的同时优化稀疏模式的自动化配置搜索算法。

Result: 在单GPU环境下，方法实现了注意力计算1.6~2.5倍的加速，同时保持了与全注意力基线相当的视觉质量。

Conclusion: 这项工作通过结构化稀疏利用提供了一种有理论基础的方法，用于开启高效的长形式视频生成。

Abstract: The computational demands of self-attention mechanisms pose a critical challenge for transformer-based video generation, particularly in synthesizing ultra-long sequences. Current approaches, such as factorized attention and fixed sparse patterns, fail to fully exploit the inherent spatio-temporal redundancies in video data. Through systematic analysis of video diffusion transformers (DiT), we uncover a key insight: Attention matrices exhibit structured, yet heterogeneous sparsity patterns, where specialized heads dynamically attend to distinct spatiotemporal regions (e.g., local pattern, cross-shaped pattern, or global pattern). Existing sparse attention methods either impose rigid constraints or introduce significant overhead, limiting their effectiveness. To address this, we propose Compact Attention, a hardware-aware acceleration framework featuring three innovations: 1) Adaptive tiling strategies that approximate diverse spatial interaction patterns via dynamic tile grouping, 2) Temporally varying windows that adjust sparsity levels based on frame proximity, and 3) An automated configuration search algorithm that optimizes sparse patterns while preserving critical attention pathways. Our method achieves 1.6~2.5x acceleration in attention computation on single-GPU setups while maintaining comparable visual quality with full-attention baselines. This work provides a principled approach to unlocking efficient long-form video generation through structured sparsity exploitation. Project Page: https://yo-ava.github.io/Compact-Attention.github.io/

</details>


### [43] [Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model](https://arxiv.org/abs/2508.13009)
*Xianglong He,Chunli Peng,Zexiang Liu,Boyang Wang,Yifan Zhang,Qi Cui,Fei Kang,Biao Jiang,Mengyin An,Yangyang Ren,Baixin Xu,Hao-Xiang Guo,Kaixiong Gong,Cyrus Wu,Wei Li,Xuchen Song,Yang Liu,Eric Li,Yahui Zhou*

Main category: cs.CV

TL;DR: Matrix-Game 2.0是一个实时交互式世界模型，通过少步自回归扩散生成高质量长视频，速度达到25FPS


<details>
  <summary>Details</summary>
Motivation: 现有交互式世界模型依赖双向注意力和冗长推理步骤，严重限制实时性能，难以模拟需要即时更新的真实世界动态

Method: 包含三个关键组件：1)可扩展的数据生产流水线；2)动作注入模块；3)基于因果架构的少步蒸馏技术

Result: 能够以超快速度25FPS生成分钟级高质量视频，涵盖多样化场景

Conclusion: 该模型在交互式世界建模方面取得重要进展，开源模型权重和代码库以推动相关研究

Abstract: Recent advances in interactive video generations have demonstrated diffusion model's potential as world models by capturing complex physical dynamics and interactive behaviors. However, existing interactive world models depend on bidirectional attention and lengthy inference steps, severely limiting real-time performance. Consequently, they are hard to simulate real-world dynamics, where outcomes must update instantaneously based on historical context and current actions. To address this, we present Matrix-Game 2.0, an interactive world model generates long videos on-the-fly via few-step auto-regressive diffusion. Our framework consists of three key components: (1) A scalable data production pipeline for Unreal Engine and GTA5 environments to effectively produce massive amounts (about 1200 hours) of video data with diverse interaction annotations; (2) An action injection module that enables frame-level mouse and keyboard inputs as interactive conditions; (3) A few-step distillation based on the casual architecture for real-time and streaming video generation. Matrix Game 2.0 can generate high-quality minute-level videos across diverse scenes at an ultra-fast speed of 25 FPS. We open-source our model weights and codebase to advance research in interactive world modeling.

</details>


### [44] [EgoTwin: Dreaming Body and View in First Person](https://arxiv.org/abs/2508.13013)
*Jingqiao Xiu,Fangzhou Hong,Yicong Li,Mengze Li,Wentao Wang,Sirui Han,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: 提出了EgoTwin框架，用于联合生成第一人称视角视频和人体运动，解决了视角对齐和因果交互两个关键挑战。


<details>
  <summary>Details</summary>
Motivation: 虽然外中心视频合成取得了很大进展，但第一人称视角视频生成仍然很少被探索，需要同时建模第一人称视角内容和穿戴者身体运动引起的相机运动模式。

Method: 基于扩散变换器架构构建EgoTwin框架，引入以头部为中心的运动表示，并采用控制论启发的交互机制在注意力操作中显式捕捉视频与运动之间的因果交互。

Result: 构建了大规模真实世界同步文本-视频-运动三元组数据集，设计了新的评估指标，大量实验证明了EgoTwin框架的有效性。

Conclusion: EgoTwin成功解决了第一人称视频与人体运动联合生成的任务，通过创新的表示和交互机制实现了视频与运动之间的精确对齐和因果一致性。

Abstract: While exocentric video synthesis has achieved great progress, egocentric video generation remains largely underexplored, which requires modeling first-person view content along with camera motion patterns induced by the wearer's body movements. To bridge this gap, we introduce a novel task of joint egocentric video and human motion generation, characterized by two key challenges: 1) Viewpoint Alignment: the camera trajectory in the generated video must accurately align with the head trajectory derived from human motion; 2) Causal Interplay: the synthesized human motion must causally align with the observed visual dynamics across adjacent video frames. To address these challenges, we propose EgoTwin, a joint video-motion generation framework built on the diffusion transformer architecture. Specifically, EgoTwin introduces a head-centric motion representation that anchors the human motion to the head joint and incorporates a cybernetics-inspired interaction mechanism that explicitly captures the causal interplay between video and motion within attention operations. For comprehensive evaluation, we curate a large-scale real-world dataset of synchronized text-video-motion triplets and design novel metrics to assess video-motion consistency. Extensive experiments demonstrate the effectiveness of the EgoTwin framework.

</details>


### [45] [IntelliCap: Intelligent Guidance for Consistent View Sampling](https://arxiv.org/abs/2508.13043)
*Ayaka Yasunaga,Hideo Saito,Dieter Schmalstieg,Shohei Mori*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的手机扫描导航技术，通过识别重要物体并生成球面代理来指导用户采集更包容视角依赖外观的图像，提升新视角合成质量。


<details>
  <summary>Details</summary>
Motivation: 虽然3D向量散射等新视角合成技术取得了重大进展，但在图像采集过程中导航人类用户仍缺乏有效工具。人类拍摄者常因急忙、耐心不足或不理解场景结构，无法完成均匀密集的视角采样，影响合成质量。

Method: 提出一种多尺度扫描的惯位可视化技术。在扫描过程中，通过语义分割和类别识别检测重要物体，使用视觉-语言模型进行排名。为排名较高的物体生成球面代理，用于在扫描时导航用户要求更充分的图像覆盖。

Result: 在真实场景中的实验结果显示，该方法在新视角合成质量方面表现出色，显著优于传统的视角采样策略。

Conclusion: 该研究成功开发了一种能够在图像采集过程中有效导航用户的惯位可视化系统，通过自动识别重要物体并提供有针对性的扫描建议，显著提升了新视角合成的质量和效果。

Abstract: Novel view synthesis from images, for example, with 3D Gaussian splatting, has made great progress. Rendering fidelity and speed are now ready even for demanding virtual reality applications. However, the problem of assisting humans in collecting the input images for these rendering algorithms has received much less attention. High-quality view synthesis requires uniform and dense view sampling. Unfortunately, these requirements are not easily addressed by human camera operators, who are in a hurry, impatient, or lack understanding of the scene structure and the photographic process. Existing approaches to guide humans during image acquisition concentrate on single objects or neglect view-dependent material characteristics. We propose a novel situated visualization technique for scanning at multiple scales. During the scanning of a scene, our method identifies important objects that need extended image coverage to properly represent view-dependent appearance. To this end, we leverage semantic segmentation and category identification, ranked by a vision-language model. Spherical proxies are generated around highly ranked objects to guide the user during scanning. Our results show superior performance in real scenes compared to conventional view sampling strategies.

</details>


### [46] [Odo: Depth-Guided Diffusion for Identity-Preserving Body Reshaping](https://arxiv.org/abs/2508.13065)
*Siddharth Khandelwal,Sridhar Kamath,Arjun Jain*

Main category: cs.CV

TL;DR: 首个大规模人体形状编辑数据集，提出Odo方法，通过混合冻结UNet和ControlNet实现实时人体形状编辑，重建误差从13.6mm降低到7.5mm


<details>
  <summary>Details</summary>
Motivation: 人体形状编辑领域缺乏大规模公开数据集，现有方法存在体比不协调、纹理扭曲和背景不一致等问题

Method: 构建18,573张图片的大规模数据集，提出Odo端到端滴渏方法，结合冻结UNet保保外观细节和ControlNet通过SMPL深度地图导航形状变换

Result: 在重建误差方面显著优于基线方法，从13.6mm降低到7.5mm，生成的结果更加实际且准确匹配目标形状

Conclusion: 该方法为人体形状编辑领域提供了有效的解决方案，通过大规模数据集和创新的模型结构实现了更加实际的形状编辑效果

Abstract: Human shape editing enables controllable transformation of a person's body shape, such as thin, muscular, or overweight, while preserving pose, identity, clothing, and background. Unlike human pose editing, which has advanced rapidly, shape editing remains relatively underexplored. Current approaches typically rely on 3D morphable models or image warping, often introducing unrealistic body proportions, texture distortions, and background inconsistencies due to alignment errors and deformations. A key limitation is the lack of large-scale, publicly available datasets for training and evaluating body shape manipulation methods. In this work, we introduce the first large-scale dataset of 18,573 images across 1523 subjects, specifically designed for controlled human shape editing. It features diverse variations in body shape, including fat, muscular and thin, captured under consistent identity, clothing, and background conditions. Using this dataset, we propose Odo, an end-to-end diffusion-based method that enables realistic and intuitive body reshaping guided by simple semantic attributes. Our approach combines a frozen UNet that preserves fine-grained appearance and background details from the input image with a ControlNet that guides shape transformation using target SMPL depth maps. Extensive experiments demonstrate that our method outperforms prior approaches, achieving per-vertex reconstruction errors as low as 7.5mm, significantly lower than the 13.6mm observed in baseline methods, while producing realistic results that accurately match the desired target shapes.

</details>


### [47] [ID-Card Synthetic Generation: Toward a Simulated Bona fide Dataset](https://arxiv.org/abs/2508.13078)
*Qingwen Zeng,Juan E. Tapia,Izan Garcia,Juan M. Espin,Christoph Busch*

Main category: cs.CV

TL;DR: 使用Stable Diffusion生成合成真实ID卡图像来解决实际真实样本缺乏问题，提升阻止攻击检测系统的性能


<details>
  <summary>Details</summary>
Motivation: ID卡阻止攻击检测(PAD)系统面临真实样本数量有限和攻击手段多样化的挑战，当前算法多关注攻击样本生成而忽视了真实图像的缺乏问题

Method: 提出使用Stable Diffusion生成合成真实ID卡图像的方法，并在从头训练的系统和商业解决方案中评估这些新图像

Result: 生成的合成图像被PAD系统识别为真实图像，对检测性能和数据限制问题产生了积极影响

Conclusion: 这是首次提出使用生成式AI技术来模仿真实ID卡图像的方法，有助于提高PAD系统的通用性能力

Abstract: Nowadays, the development of a Presentation Attack Detection (PAD) system for ID cards presents a challenge due to the lack of images available to train a robust PAD system and the increase in diversity of possible attack instrument species. Today, most algorithms focus on generating attack samples and do not take into account the limited number of bona fide images. This work is one of the first to propose a method for mimicking bona fide images by generating synthetic versions of them using Stable Diffusion, which may help improve the generalisation capabilities of the detector. Furthermore, the new images generated are evaluated in a system trained from scratch and in a commercial solution. The PAD system yields an interesting result, as it identifies our images as bona fide, which has a positive impact on detection performance and data restrictions.

</details>


### [48] [DMS:Diffusion-Based Multi-Baseline Stereo Generation for Improving Self-Supervised Depth Estimation](https://arxiv.org/abs/2508.13091)
*Zihua Liu,Yizhou Li,Songyan Zhang,Masatoshi Okutomi*

Main category: cs.CV

TL;DR: DMS是一个模型无关的方法，利用扩散模型的几何先验合成沿极线方向的新视角，通过方向提示来补充遮挡像素，提升自监督立体匹配和单目深度估计性能。


<details>
  <summary>Details</summary>
Motivation: 自监督方法使用立体图像作为监督信号的研究相对较少，主要挑战来自光度重建中的模糊性，特别是在目标视角的遮挡区域和出框区域中对应像素缺失的问题。

Method: 微调Stable Diffusion模型来模拟关键位置的视角：从左相机偏移的左-左视角、从右相机偏移的右-右视角，以及左右相机之间的额外新视角。这些合成视角补充遮挡像素，实现显式光度重建。

Result: 大量实验证明方法的有效性，异常值减少高达35%，在多个基准数据集上达到最先进性能。

Conclusion: DMS是一个无需成本的即插即用方法，仅使用未标记的立体图像对进行训练和合成，能无缝增强自监督立体匹配和单目深度估计。

Abstract: While supervised stereo matching and monocular depth estimation have advanced significantly with learning-based algorithms, self-supervised methods using stereo images as supervision signals have received relatively less focus and require further investigation. A primary challenge arises from ambiguity introduced during photometric reconstruction, particularly due to missing corresponding pixels in ill-posed regions of the target view, such as occlusions and out-of-frame areas. To address this and establish explicit photometric correspondences, we propose DMS, a model-agnostic approach that utilizes geometric priors from diffusion models to synthesize novel views along the epipolar direction, guided by directional prompts. Specifically, we finetune a Stable Diffusion model to simulate perspectives at key positions: left-left view shifted from the left camera, right-right view shifted from the right camera, along with an additional novel view between the left and right cameras. These synthesized views supplement occluded pixels, enabling explicit photometric reconstruction. Our proposed DMS is a cost-free, ''plug-and-play'' method that seamlessly enhances self-supervised stereo matching and monocular depth estimation, and relies solely on unlabeled stereo image pairs for both training and synthesizing. Extensive experiments demonstrate the effectiveness of our approach, with up to 35% outlier reduction and state-of-the-art performance across multiple benchmark datasets.

</details>


### [49] [Precise Action-to-Video Generation Through Visual Action Prompts](https://arxiv.org/abs/2508.13104)
*Yuang Wang,Chao Wen,Haoyu Guo,Sida Peng,Minghan Qin,Hujun Bao,Xiaowei Zhou,Ruizhen Hu*

Main category: cs.CV

TL;DR: 提出了视觉动作提示（visual action prompts），一种统一的动作表示方法，用于复杂高自由度交互的动作到视频生成，同时保持跨领域的可转移视觉动态。


<details>
  <summary>Details</summary>
Motivation: 现有的动作驱动视频生成方法面临精度与通用性的权衡：文本、原始动作或粗糙掩码方法具有通用性但缺乏精度，而智能体中心动作信号提供精度但缺乏跨领域可转移性。

Method: 将动作"渲染"为精确的视觉提示作为领域无关表示，选择视觉骨架作为通用且易获取的表示形式。构建从人-物交互和灵巧机器人操作数据中提取骨架的鲁棒流程，通过轻量级微调将视觉骨架集成到预训练视频生成模型中。

Result: 在EgoVid、RT-1和DROID数据集上的实验证明了所提出方法的有效性。

Conclusion: 视觉动作提示能够在保持复杂交互精确动作控制的同时，保留跨领域动态的学习能力，平衡了动作精度和动态可转移性。

Abstract: We present visual action prompts, a unified action representation for action-to-video generation of complex high-DoF interactions while maintaining transferable visual dynamics across domains. Action-driven video generation faces a precision-generality trade-off: existing methods using text, primitive actions, or coarse masks offer generality but lack precision, while agent-centric action signals provide precision at the cost of cross-domain transferability. To balance action precision and dynamic transferability, we propose to "render" actions into precise visual prompts as domain-agnostic representations that preserve both geometric precision and cross-domain adaptability for complex actions; specifically, we choose visual skeletons for their generality and accessibility. We propose robust pipelines to construct skeletons from two interaction-rich data sources - human-object interactions (HOI) and dexterous robotic manipulation - enabling cross-domain training of action-driven generative models. By integrating visual skeletons into pretrained video generation models via lightweight fine-tuning, we enable precise action control of complex interaction while preserving the learning of cross-domain dynamics. Experiments on EgoVid, RT-1 and DROID demonstrate the effectiveness of our proposed approach. Project page: https://zju3dv.github.io/VAP/.

</details>


### [50] [IGFuse: Interactive 3D Gaussian Scene Reconstruction via Multi-Scans Fusion](https://arxiv.org/abs/2508.13153)
*Wenhao Hu,Zesheng Li,Haonan Zhou,Liu Liu,Xuexiang Wen,Zhizhong Su,Xi Li,Gaoang Wang*

Main category: cs.CV

TL;DR: IGFuse是一个新颖的3D场景重建框架，通过融合多视角扫描数据来重建交互式高斯场景，解决了传统方法因遮挡和传感器限制导致的完整性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的3D场景重建方法面临物体遮挡和传感器覆盖有限的问题，多阶段处理流程容易出错且难以扩展，需要一种能够有效处理多扫描数据并揭示被遮挡区域的新方法。

Method: 提出IGFuse框架，构建分割感知的高斯场，通过双向光度一致性和语义一致性约束多扫描数据，引入伪中间场景状态处理空间错位，采用协作共剪枝策略优化几何结构。

Result: 实验验证表明该方法对新场景配置具有强泛化能力，能够实现高保真渲染和对象级场景操作，无需密集观测或复杂处理流程。

Conclusion: IGFuse为真实世界3D重建和真实到仿真的转换提供了有效的解决方案，通过多扫描融合成功解决了遮挡问题，实现了完整的交互式3D场景重建。

Abstract: Reconstructing complete and interactive 3D scenes remains a fundamental challenge in computer vision and robotics, particularly due to persistent object occlusions and limited sensor coverage. Multiview observations from a single scene scan often fail to capture the full structural details. Existing approaches typically rely on multi stage pipelines, such as segmentation, background completion, and inpainting or require per-object dense scanning, both of which are error-prone, and not easily scalable. We propose IGFuse, a novel framework that reconstructs interactive Gaussian scene by fusing observations from multiple scans, where natural object rearrangement between captures reveal previously occluded regions. Our method constructs segmentation aware Gaussian fields and enforces bi-directional photometric and semantic consistency across scans. To handle spatial misalignments, we introduce a pseudo-intermediate scene state for unified alignment, alongside collaborative co-pruning strategies to refine geometry. IGFuse enables high fidelity rendering and object level scene manipulation without dense observations or complex pipelines. Extensive experiments validate the framework's strong generalization to novel scene configurations, demonstrating its effectiveness for real world 3D reconstruction and real-to-simulation transfer. Our project page is available online.

</details>


### [51] [4DNeX: Feed-Forward 4D Generative Modeling Made Easy](https://arxiv.org/abs/2508.13154)
*Zhaoxi Chen,Tianqi Liu,Long Zhuo,Jiawei Ren,Zeng Tao,He Zhu,Fangzhou Hong,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: 4DNeX是首个从单张图像生成4D（动态3D）场景表示的端到端前馈框架，通过微调预训练视频扩散模型实现高效图像到4D生成


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖计算密集型优化或需要多帧视频输入的问题，为生成式4D世界模型提供可扩展解决方案

Method: 1)构建大规模4D数据集4DNeX-10M；2)引入统一6D视频表示联合建模RGB和XYZ序列；3)提出适配策略将预训练视频扩散模型用于4D建模

Result: 生成高质量动态点云，支持新颖视角视频合成，在效率和泛化性方面优于现有4D生成方法

Conclusion: 4DNeX为图像到4D建模提供了可扩展解决方案，为模拟动态场景演化的生成式4D世界模型奠定了基础

Abstract: We present 4DNeX, the first feed-forward framework for generating 4D (i.e., dynamic 3D) scene representations from a single image. In contrast to existing methods that rely on computationally intensive optimization or require multi-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D generation by fine-tuning a pretrained video diffusion model. Specifically, 1) to alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale dataset with high-quality 4D annotations generated using advanced reconstruction approaches. 2) we introduce a unified 6D video representation that jointly models RGB and XYZ sequences, facilitating structured learning of both appearance and geometry. 3) we propose a set of simple yet effective adaptation strategies to repurpose pretrained video diffusion models for 4D modeling. 4DNeX produces high-quality dynamic point clouds that enable novel-view video synthesis. Extensive experiments demonstrate that 4DNeX outperforms existing 4D generation methods in efficiency and generalizability, offering a scalable solution for image-to-4D modeling and laying the foundation for generative 4D world models that simulate dynamic scene evolution.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [52] [Point upsampling networks for single-photon sensing](https://arxiv.org/abs/2508.12986)
*Jinyi Liu,Guoyang Zhao,Lijun Liu,Yiguang Hong,Weiping Zhang,Shuming Cheng*

Main category: physics.optics

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Single-photon sensing has generated great interest as a prominent technique of long-distance and ultra-sensitive imaging, however, it tends to yield sparse and spatially biased point clouds, thus limiting its practical utility. In this work, we propose using point upsampling networks to increase point density and reduce spatial distortion in single-photon point cloud. Particularly, our network is built on the state space model which integrates a multi-path scanning mechanism to enrich spatial context, a bidirectional Mamba backbone to capture global geometry and local details, and an adaptive upsample shift module to correct offset-induced distortions. Extensive experiments are implemented on commonly-used datasets to confirm its high reconstruction accuracy and strong robustness to the distortion noise, and also on real-world data to demonstrate that our model is able to generate visually consistent, detail-preserving, and noise suppressed point clouds. Our work is the first to establish the upsampling framework for single-photon sensing, and hence opens a new avenue for single-photon sensing and its practical applications in the downstreaming tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [53] [Contrastive Regularization over LoRA for Multimodal Biomedical Image Incremental Learning](https://arxiv.org/abs/2508.11673)
*Haojie Zhang,Yixiong Liang,Hulin Kuang,Lihui Cen,Zhe Qu,Yigang Cen,Min Zeng,Shichao Kan*

Main category: cs.LG

TL;DR: 提出MSLoRA-CR方法解决多模态生物医学图像增量学习问题，通过模态特定LoRA模块和对比正则化实现知识保持与共享，在计算效率下性能提升1.88%


<details>
  <summary>Details</summary>
Motivation: 生物医学领域需要处理多种模态和任务，为每个模态单独训练模型会显著增加推理成本，而现有增量学习方法主要关注单模态任务扩展

Method: 基于大型视觉语言模型，冻结预训练参数，为每个模态增量适配特定LoRA模块，并引入对比正则化促进模态内知识共享和模态间知识区分

Result: 在生物医学图像增量学习实验中，MSLoRA-CR优于为每个模态单独训练模型的方法和通用增量学习方法，整体性能提升1.88%

Conclusion: MSLoRA-CR有效解决了多模态生物医学图像增量学习的两个核心挑战，在保持计算效率的同时显著提升了性能

Abstract: Multimodal Biomedical Image Incremental Learning (MBIIL) is essential for handling diverse tasks and modalities in the biomedical domain, as training separate models for each modality or task significantly increases inference costs. Existing incremental learning methods focus on task expansion within a single modality, whereas MBIIL seeks to train a unified model incrementally across modalities. The MBIIL faces two challenges: I) How to preserve previously learned knowledge during incremental updates? II) How to effectively leverage knowledge acquired from existing modalities to support new modalities? To address these challenges, we propose MSLoRA-CR, a method that fine-tunes Modality-Specific LoRA modules while incorporating Contrastive Regularization to enhance intra-modality knowledge sharing and promote inter-modality knowledge differentiation. Our approach builds upon a large vision-language model (LVLM), keeping the pretrained model frozen while incrementally adapting new LoRA modules for each modality or task. Experiments on the incremental learning of biomedical images demonstrate that MSLoRA-CR outperforms both the state-of-the-art (SOTA) approach of training separate models for each modality and the general incremental learning method (incrementally fine-tuning LoRA). Specifically, MSLoRA-CR achieves a 1.88% improvement in overall performance compared to unconstrained incremental learning methods while maintaining computational efficiency. Our code is publicly available at https://github.com/VentusAislant/MSLoRA_CR.

</details>
