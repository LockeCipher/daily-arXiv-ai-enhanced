{"id": "2507.09140", "pdf": "https://arxiv.org/pdf/2507.09140", "abs": "https://arxiv.org/abs/2507.09140", "authors": ["Chuang Chen", "Xiaoxuan Xie", "Yongming Zhang", "Tianyu Zhang", "Haoran Xie"], "title": "Interactive Drawing Guidance for Anime Illustrations with Diffusion Model", "categories": ["cs.GR"], "comment": "9 pages, 7 figures. In proceedings of NICOGRAPH International 2025", "summary": "Creating high-quality anime illustrations presents notable challenges, particularly for beginners, due to the intricate styles and fine details inherent in anime art. We present an interactive drawing guidance system specifically designed for anime illustrations to address this issue. It offers real-time guidance to help users refine their work and streamline the creative process. Our system is built upon the StreamDiffusion pipeline to deliver real-time drawing assistance. We fine-tune Stable Diffusion with LoRA to synthesize anime style RGB images from user-provided hand-drawn sketches and prompts. Leveraging the Informative Drawings model, we transform these RGB images into rough sketches, which are further refined into structured guidance sketches using a custom-designed optimizer. The proposed system offers precise, real-time guidance aligned with the creative intent of the user, significantly enhancing both the efficiency and accuracy of the drawing process. To assess the effectiveness of our approach, we conducted a user study, gathering empirical feedback on both system performance and interface usability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eStreamDiffusion\u548cLoRA\u7684\u4ea4\u4e92\u5f0f\u52a8\u6f2b\u7ed8\u56fe\u5f15\u5bfc\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b9e\u65f6\u6307\u5bfc\u548c\u4f18\u5316\u8349\u56fe\u63d0\u5347\u7ed8\u56fe\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u52a8\u6f2b\u7ed8\u56fe\u5bf9\u521d\u5b66\u8005\u96be\u5ea6\u9ad8\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u5b9e\u65f6\u6307\u5bfc\u4ee5\u7b80\u5316\u521b\u4f5c\u6d41\u7a0b\u3002", "method": "\u7ed3\u5408StreamDiffusion\u548cLoRA\u5fae\u8c03Stable Diffusion\uff0c\u5c06\u624b\u7ed8\u8349\u56fe\u8f6c\u5316\u4e3a\u52a8\u6f2b\u98ce\u683cRGB\u56fe\u50cf\uff0c\u518d\u901a\u8fc7Informative Drawings\u6a21\u578b\u548c\u4f18\u5316\u5668\u751f\u6210\u7ed3\u6784\u5316\u5f15\u5bfc\u8349\u56fe\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u7ed8\u56fe\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u52a8\u6f2b\u7ed8\u56fe\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b9e\u65f6\u6307\u5bfc\u5de5\u5177\uff0c\u9002\u5408\u521d\u5b66\u8005\u4f7f\u7528\u3002"}}
{"id": "2507.09146", "pdf": "https://arxiv.org/pdf/2507.09146", "abs": "https://arxiv.org/abs/2507.09146", "authors": ["Ryuichi Miyauchi", "Hengyuan Chang", "Tsukasa Fukusato", "Kazunori Miyata", "Haoran Xie"], "title": "Physics-Aware Fluid Field Generation from User Sketches Using Helmholtz-Hodge Decomposition", "categories": ["cs.GR"], "comment": "8 pages, 12 figures. In proceedings of NICOGRAPH International 2025", "summary": "Fluid simulation techniques are widely used in various fields such as film production, but controlling complex fluid behaviors remains challenging. While recent generative models enable intuitive generation of vector fields from user sketches, they struggle to maintain physical properties such as incompressibility. To address these issues, this paper proposes a method for interactively designing 2D vector fields. Conventional generative models can intuitively generate vector fields from user sketches, but remain difficult to consider physical properties. Therefore, we add a simple editing process after generating the vector field. In the first stage, we use a latent diffusion model~(LDM) to automatically generate initial 2D vector fields from user sketches. In the second stage, we apply the Helmholtz-Hodge decomposition to locally extract physical properties such as incompressibility from the results generated by LDM and recompose them according to user intentions. Through multiple experiments, we demonstrate the effectiveness of our proposed method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ea4\u4e92\u5f0f\u8bbe\u8ba12D\u77e2\u91cf\u573a\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u6a21\u578b\u548c\u7269\u7406\u5c5e\u6027\u7f16\u8f91\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u4fdd\u6301\u7269\u7406\u7279\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u63a7\u5236\u590d\u6742\u6d41\u4f53\u884c\u4e3a\u7684\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u751f\u6210\u6a21\u578b\u5728\u4fdd\u6301\u7269\u7406\u5c5e\u6027\uff08\u5982\u4e0d\u53ef\u538b\u7f29\u6027\uff09\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\u4ece\u7528\u6237\u8349\u56fe\u4e2d\u751f\u6210\u521d\u59cb\u77e2\u91cf\u573a\uff0c\u518d\u901a\u8fc7Helmholtz-Hodge\u5206\u89e3\u5c40\u90e8\u63d0\u53d6\u7269\u7406\u5c5e\u6027\u5e76\u91cd\u65b0\u7ec4\u5408\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u7269\u7406\u5c5e\u6027\u7684\u540c\u65f6\uff0c\u80fd\u591f\u6ee1\u8db3\u7528\u6237\u610f\u56fe\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u751f\u6210\u6a21\u578b\u5728\u7269\u7406\u5c5e\u6027\u4fdd\u6301\u4e0a\u7684\u95ee\u9898\uff0c\u4e3a\u6d41\u4f53\u6a21\u62df\u63d0\u4f9b\u4e86\u66f4\u76f4\u89c2\u7684\u8bbe\u8ba1\u5de5\u5177\u3002"}}
{"id": "2507.09441", "pdf": "https://arxiv.org/pdf/2507.09441", "abs": "https://arxiv.org/abs/2507.09441", "authors": ["Ankit Sanjyal"], "title": "RectifiedHR: High-Resolution Diffusion via Energy Profiling and Adaptive Guidance Scheduling", "categories": ["cs.GR", "cs.CV"], "comment": "8 Pages, 10 Figures, Pre-Print Version, Code Available at:   https://github.com/ANKITSANJYAL/RectifiedHR", "summary": "High-resolution image synthesis with diffusion models often suffers from energy instabilities and guidance artifacts that degrade visual quality. We analyze the latent energy landscape during sampling and propose adaptive classifier-free guidance (CFG) schedules that maintain stable energy trajectories. Our approach introduces energy-aware scheduling strategies that modulate guidance strength over time, achieving superior stability scores (0.9998) and consistency metrics (0.9873) compared to fixed-guidance approaches. We demonstrate that DPM++ 2M with linear-decreasing CFG scheduling yields optimal performance, providing sharper, more faithful images while reducing artifacts. Our energy profiling framework serves as a powerful diagnostic tool for understanding and improving diffusion model behavior.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\uff08CFG\uff09\u8c03\u5ea6\u7b56\u7565\uff0c\u89e3\u51b3\u6269\u6563\u6a21\u578b\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5408\u6210\u4e2d\u7684\u80fd\u91cf\u4e0d\u7a33\u5b9a\u548c\u5f15\u5bfc\u4f2a\u5f71\u95ee\u9898\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5408\u6210\u4e2d\u6269\u6563\u6a21\u578b\u7684\u80fd\u91cf\u4e0d\u7a33\u5b9a\u548c\u5f15\u5bfc\u4f2a\u5f71\u95ee\u9898\u5f71\u54cd\u89c6\u89c9\u8d28\u91cf\u3002", "method": "\u5206\u6790\u91c7\u6837\u8fc7\u7a0b\u4e2d\u7684\u6f5c\u5728\u80fd\u91cf\u666f\u89c2\uff0c\u63d0\u51fa\u80fd\u91cf\u611f\u77e5\u8c03\u5ea6\u7b56\u7565\uff0c\u52a8\u6001\u8c03\u6574\u5f15\u5bfc\u5f3a\u5ea6\u3002", "result": "\u91c7\u7528\u7ebf\u6027\u9012\u51cfCFG\u8c03\u5ea6\u7684DPM++ 2M\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u7a33\u5b9a\u6027\u5f97\u5206\uff080.9998\uff09\u548c\u4e00\u81f4\u6027\u6307\u6807\uff080.9873\uff09\u4f18\u4e8e\u56fa\u5b9a\u5f15\u5bfc\u65b9\u6cd5\u3002", "conclusion": "\u80fd\u91cf\u5206\u6790\u6846\u67b6\u4e3a\u7406\u89e3\u548c\u6539\u8fdb\u6269\u6563\u6a21\u578b\u884c\u4e3a\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2507.08917", "pdf": "https://arxiv.org/pdf/2507.08917", "abs": "https://arxiv.org/abs/2507.08917", "authors": ["Justin D. Norman", "Hany Farid"], "title": "Detecting Deepfake Talking Heads from Facial Biometric Anomalies", "categories": ["cs.CV"], "comment": "10 pages, 3 figures, 3 tables", "summary": "The combination of highly realistic voice cloning, along with visually compelling avatar, face-swap, or lip-sync deepfake video generation, makes it relatively easy to create a video of anyone saying anything. Today, such deepfake impersonations are often used to power frauds, scams, and political disinformation. We propose a novel forensic machine learning technique for the detection of deepfake video impersonations that leverages unnatural patterns in facial biometrics. We evaluate this technique across a large dataset of deepfake techniques and impersonations, as well as assess its reliability to video laundering and its generalization to previously unseen video deepfake generators.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9762\u90e8\u751f\u7269\u7279\u5f81\u5f02\u5e38\u6a21\u5f0f\u7684\u6df1\u5ea6\u5b66\u4e60\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u3002", "motivation": "\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u5bb9\u6613\u88ab\u7528\u4e8e\u6b3a\u8bc8\u3001\u8bc8\u9a97\u548c\u653f\u6cbb\u865a\u5047\u4fe1\u606f\uff0c\u4e9f\u9700\u6709\u6548\u7684\u68c0\u6d4b\u624b\u6bb5\u3002", "method": "\u5229\u7528\u9762\u90e8\u751f\u7269\u7279\u5f81\u4e2d\u7684\u975e\u81ea\u7136\u6a21\u5f0f\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u578b\u6cd5\u533b\u673a\u5668\u5b66\u4e60\u6280\u672f\u3002", "result": "\u5728\u5927\u89c4\u6a21\u6df1\u5ea6\u4f2a\u9020\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u8be5\u6280\u672f\u7684\u6709\u6548\u6027\uff0c\u5e76\u6d4b\u8bd5\u4e86\u5176\u5bf9\u89c6\u9891\u7be1\u6539\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\uff0c\u5e76\u5bf9\u672a\u77e5\u751f\u6210\u5668\u5177\u6709\u4e00\u5b9a\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.10542", "pdf": "https://arxiv.org/pdf/2507.10542", "abs": "https://arxiv.org/abs/2507.10542", "authors": ["Shivangi Aneja", "Sebastian Weiss", "Irene Baeza", "Prashanth Chandran", "Gaspard Zoss", "Matthias Nie\u00dfner", "Derek Bradley"], "title": "ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "(SIGGRAPH 2025) Paper Video: https://youtu.be/VyWkgsGdbkk Project   Page: https://shivangi-aneja.github.io/projects/scaffoldavatar/", "summary": "Generating high-fidelity real-time animated sequences of photorealistic 3D head avatars is important for many graphics applications, including immersive telepresence and movies. This is a challenging problem particularly when rendering digital avatar close-ups for showing character's facial microfeatures and expressions. To capture the expressive, detailed nature of human heads, including skin furrowing and finer-scale facial movements, we propose to couple locally-defined facial expressions with 3D Gaussian splatting to enable creating ultra-high fidelity, expressive and photorealistic 3D head avatars. In contrast to previous works that operate on a global expression space, we condition our avatar's dynamics on patch-based local expression features and synthesize 3D Gaussians at a patch level. In particular, we leverage a patch-based geometric 3D face model to extract patch expressions and learn how to translate these into local dynamic skin appearance and motion by coupling the patches with anchor points of Scaffold-GS, a recent hierarchical scene representation. These anchors are then used to synthesize 3D Gaussians on-the-fly, conditioned by patch-expressions and viewing direction. We employ color-based densification and progressive training to obtain high-quality results and faster convergence for high resolution 3K training images. By leveraging patch-level expressions, ScaffoldAvatar consistently achieves state-of-the-art performance with visually natural motion, while encompassing diverse facial expressions and styles in real time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5c40\u90e8\u9762\u90e8\u8868\u60c5\u4e0e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u5b9e\u65f6\u76843D\u5934\u90e8\u865a\u62df\u5f62\u8c61\u3002", "motivation": "\u89e3\u51b3\u5728\u8fd1\u8ddd\u79bb\u6e32\u67d3\u6570\u5b57\u865a\u62df\u5f62\u8c61\u65f6\uff0c\u6355\u6349\u9762\u90e8\u5fae\u7279\u5f81\u548c\u8868\u60c5\u7684\u6311\u6218\uff0c\u4ee5\u5b9e\u73b0\u66f4\u771f\u5b9e\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5c40\u90e8\u8865\u4e01\u7684\u8868\u60c5\u7279\u5f81\uff0c\u7ed3\u54083D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u901a\u8fc7\u8865\u4e01\u7ea7\u51e0\u4f55\u6a21\u578b\u63d0\u53d6\u8868\u60c5\u5e76\u52a8\u6001\u5408\u62103D\u9ad8\u65af\u3002", "result": "ScaffoldAvatar\u5728\u5b9e\u65f6\u6027\u3001\u591a\u6837\u6027\u548c\u89c6\u89c9\u81ea\u7136\u6027\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u901a\u8fc7\u5c40\u90e8\u8865\u4e01\u7ea7\u8868\u60c5\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u3001\u5b9e\u65f6\u76843D\u5934\u90e8\u865a\u62df\u5f62\u8c61\u751f\u6210\u3002"}}
{"id": "2507.09005", "pdf": "https://arxiv.org/pdf/2507.09005", "abs": "https://arxiv.org/abs/2507.09005", "authors": ["Cheng-Hsi Hsiao", "Krishna Kumar"], "title": "From images to properties: a NeRF-driven framework for granular material parameter inversion", "categories": ["cs.CV", "physics.geo-ph"], "comment": null, "summary": "We introduce a novel framework that integrates Neural Radiance Fields (NeRF) with Material Point Method (MPM) simulation to infer granular material properties from visual observations. Our approach begins by generating synthetic experimental data, simulating an plow interacting with sand. The experiment is rendered into realistic images as the photographic observations. These observations include multi-view images of the experiment's initial state and time-sequenced images from two fixed cameras. Using NeRF, we reconstruct the 3D geometry from the initial multi-view images, leveraging its capability to synthesize novel viewpoints and capture intricate surface details. The reconstructed geometry is then used to initialize material point positions for the MPM simulation, where the friction angle remains unknown. We render images of the simulation under the same camera setup and compare them to the observed images. By employing Bayesian optimization, we minimize the image loss to estimate the best-fitting friction angle. Our results demonstrate that friction angle can be estimated with an error within 2 degrees, highlighting the effectiveness of inverse analysis through purely visual observations. This approach offers a promising solution for characterizing granular materials in real-world scenarios where direct measurement is impractical or impossible.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408NeRF\u548cMPM\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u89c2\u6d4b\u63a8\u65ad\u9897\u7c92\u6750\u6599\u7279\u6027\uff0c\u6469\u64e6\u89d2\u4f30\u8ba1\u8bef\u5dee\u57282\u5ea6\u4ee5\u5185\u3002", "motivation": "\u5728\u65e0\u6cd5\u76f4\u63a5\u6d4b\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u89c6\u89c9\u89c2\u6d4b\u8868\u5f81\u9897\u7c92\u6750\u6599\u7684\u7279\u6027\u3002", "method": "\u751f\u6210\u5408\u6210\u5b9e\u9a8c\u6570\u636e\uff0c\u7528NeRF\u91cd\u5efa3D\u51e0\u4f55\uff0cMPM\u6a21\u62df\u6750\u6599\u884c\u4e3a\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u4f30\u8ba1\u6469\u64e6\u89d2\u3002", "result": "\u6469\u64e6\u89d2\u4f30\u8ba1\u8bef\u5dee\u57282\u5ea6\u4ee5\u5185\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9897\u7c92\u6750\u6599\u7684\u89c6\u89c9\u8868\u5f81\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09052", "pdf": "https://arxiv.org/pdf/2507.09052", "abs": "https://arxiv.org/abs/2507.09052", "authors": ["Fang Chen", "Alex Villa", "Gongbo Liang", "Xiaoyi Lu", "Meng Tang"], "title": "Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?", "categories": ["cs.CV", "cs.LG"], "comment": "20 pages, 11 figures", "summary": "Training data for class-conditional image synthesis often exhibit a long-tailed distribution with limited images for tail classes. Such an imbalance causes mode collapse and reduces the diversity of synthesized images for tail classes. For class-conditional diffusion models trained on imbalanced data, we aim to improve the diversity of tail class images without compromising the fidelity and diversity of head class images. We achieve this by introducing two deceptively simple but highly effective contrastive loss functions. Firstly, we employ an unsupervised InfoNCE loss utilizing negative samples to increase the distance/dissimilarity among synthetic images, particularly for tail classes. To further enhance the diversity of tail classes, our second loss is an MSE loss that contrasts class-conditional generation with unconditional generation at large timesteps. This second loss makes the denoising process insensitive to class conditions for the initial steps, which enriches tail classes through knowledge sharing from head classes. Conditional-unconditional alignment has been shown to enhance the performance of long-tailed GAN. We are the first to adapt such alignment to diffusion models. We successfully leveraged contrastive learning for class-imbalanced diffusion models. Our contrastive learning framework is easy to implement and outperforms standard DDPM and alternative methods for class-imbalanced diffusion models across various datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and ImageNetLT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u5bf9\u6bd4\u635f\u5931\u51fd\u6570\uff0c\u7528\u4e8e\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u6269\u6563\u6a21\u578b\u4e2d\u5c3e\u90e8\u7c7b\u522b\u56fe\u50cf\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u5934\u90e8\u7c7b\u522b\u7684\u4fdd\u771f\u5ea6\u548c\u591a\u6837\u6027\u3002", "motivation": "\u7c7b\u522b\u4e0d\u5e73\u8861\u6570\u636e\u5bfc\u81f4\u5c3e\u90e8\u7c7b\u522b\u56fe\u50cf\u5408\u6210\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u5728\u4e0d\u5f71\u54cd\u5934\u90e8\u7c7b\u522b\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u5c3e\u90e8\u7c7b\u522b\u7684\u591a\u6837\u6027\u3002", "method": "\u5f15\u5165\u4e24\u79cd\u5bf9\u6bd4\u635f\u5931\u51fd\u6570\uff1a\u65e0\u76d1\u7763InfoNCE\u635f\u5931\u548cMSE\u635f\u5931\uff0c\u901a\u8fc7\u5bf9\u6bd4\u6761\u4ef6\u4e0e\u65e0\u6761\u4ef6\u751f\u6210\u589e\u5f3a\u5c3e\u90e8\u7c7b\u522b\u591a\u6837\u6027\u3002", "result": "\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\uff08\u5982CIFAR10/100-LT\u7b49\uff09\u4e0a\u4f18\u4e8e\u6807\u51c6DDPM\u548c\u5176\u4ed6\u66ff\u4ee3\u65b9\u6cd5\u3002", "conclusion": "\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u7b80\u5355\u6709\u6548\uff0c\u6210\u529f\u63d0\u5347\u4e86\u7c7b\u522b\u4e0d\u5e73\u8861\u6269\u6563\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.09102", "pdf": "https://arxiv.org/pdf/2507.09102", "abs": "https://arxiv.org/abs/2507.09102", "authors": ["Yiyang Chen", "Shanshan Zhao", "Lunhao Duan", "Changxing Ding", "Dacheng Tao"], "title": "Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Diffusion-based models, widely used in text-to-image generation, have proven effective in 2D representation learning. Recently, this framework has been extended to 3D self-supervised learning by constructing a conditional point generator for enhancing 3D representations. However, its performance remains constrained by the 3D diffusion model, which is trained on the available 3D datasets with limited size. We hypothesize that the robust capabilities of text-to-image diffusion models, particularly Stable Diffusion (SD), which is trained on large-scale datasets, can help overcome these limitations. To investigate this hypothesis, we propose PointSD, a framework that leverages the SD model for 3D self-supervised learning. By replacing the SD model's text encoder with a 3D encoder, we train a point-to-image diffusion model that allows point clouds to guide the denoising of rendered noisy images. With the trained point-to-image diffusion model, we use noise-free images as the input and point clouds as the condition to extract SD features. Next, we train a 3D backbone by aligning its features with these SD features, thereby facilitating direct semantic learning. Comprehensive experiments on downstream point cloud tasks and ablation studies demonstrate that the SD model can enhance point cloud self-supervised learning. Code is publicly available at https://github.com/wdttt/PointSD.", "AI": {"tldr": "PointSD\u5229\u7528\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff08\u5982Stable Diffusion\uff09\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u901a\u8fc73D\u7f16\u7801\u5668\u66ff\u6362\u6587\u672c\u7f16\u7801\u5668\uff0c\u6784\u5efa\u70b9\u4e91\u5230\u56fe\u50cf\u7684\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u589e\u5f3a3D\u81ea\u76d1\u7763\u5b66\u4e60\u3002", "motivation": "\u73b0\u67093D\u6269\u6563\u6a21\u578b\u53d7\u9650\u4e8e\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u800c\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff08\u5982Stable Diffusion\uff09\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u53ef\u80fd\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\u3002", "method": "\u63d0\u51faPointSD\u6846\u67b6\uff0c\u901a\u8fc7\u70b9\u4e91\u5f15\u5bfc\u53bb\u566a\u6e32\u67d3\u56fe\u50cf\uff0c\u63d0\u53d6SD\u7279\u5f81\uff0c\u5e76\u8bad\u7ec33D\u9aa8\u5e72\u7f51\u7edc\u5bf9\u9f50\u8fd9\u4e9b\u7279\u5f81\u4ee5\u5b9e\u73b0\u8bed\u4e49\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660ePointSD\u80fd\u6709\u6548\u63d0\u5347\u70b9\u4e91\u81ea\u76d1\u7763\u5b66\u4e60\u6027\u80fd\u3002", "conclusion": "PointSD\u901a\u8fc7\u7ed3\u5408SD\u6a21\u578b\u7684\u80fd\u529b\uff0c\u6210\u529f\u514b\u670d\u4e863D\u6570\u636e\u96c6\u7684\u9650\u5236\uff0c\u63d0\u5347\u4e863D\u8868\u793a\u5b66\u4e60\u6548\u679c\u3002"}}
{"id": "2507.09105", "pdf": "https://arxiv.org/pdf/2507.09105", "abs": "https://arxiv.org/abs/2507.09105", "authors": ["Maoxiao Ye", "Xinfeng Ye", "Mano Manoharan"], "title": "Hybrid Autoregressive-Diffusion Model for Real-Time Streaming Sign Language Production", "categories": ["cs.CV"], "comment": null, "summary": "Earlier Sign Language Production (SLP) models typically relied on autoregressive methods that generate output tokens one by one, which inherently provide temporal alignment. Although techniques like Teacher Forcing can prevent model collapse during training, they still cannot solve the problem of error accumulation during inference, since ground truth is unavailable at that stage. In contrast, more recent approaches based on diffusion models leverage step-by-step denoising to enable high-quality generation. However, the iterative nature of these models and the requirement to denoise entire sequences limit their applicability in real-time tasks like SLP. To address it, we apply a hybrid approach combining autoregressive and diffusion models to SLP for the first time, leveraging the strengths of both models in sequential dependency modeling and output refinement. To capture fine-grained body movements, we design a Multi-Scale Pose Representation module that separately extracts detailed features from distinct articulators and integrates them via a Multi-Scale Fusion module. Furthermore, we introduce a Confidence-Aware Causal Attention mechanism that utilizes joint-level confidence scores to dynamically guide the pose generation process, improving accuracy and robustness. Extensive experiments on the PHOENIX14T and How2Sign datasets demonstrate the effectiveness of our method in both generation quality and real-time streaming efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u56de\u5f52\u548c\u6269\u6563\u6a21\u578b\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u624b\u8bed\u751f\u6210\uff08SLP\uff09\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u63a8\u7406\u9636\u6bb5\u7684\u9519\u8bef\u7d2f\u79ef\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u5b9e\u65f6\u6027\u3002", "motivation": "\u4f20\u7edf\u81ea\u56de\u5f52\u65b9\u6cd5\u5728\u63a8\u7406\u65f6\u65e0\u6cd5\u907f\u514d\u9519\u8bef\u7d2f\u79ef\uff0c\u800c\u6269\u6563\u6a21\u578b\u867d\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u7ed3\u679c\uff0c\u4f46\u8fed\u4ee3\u7279\u6027\u9650\u5236\u4e86\u5b9e\u65f6\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\uff0c\u63d0\u5347SLP\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u6df7\u5408\u81ea\u56de\u5f52\u548c\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e86\u591a\u5c3a\u5ea6\u59ff\u6001\u8868\u793a\u6a21\u5757\u548c\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u56e0\u679c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u6355\u6349\u7cbe\u7ec6\u52a8\u4f5c\u5e76\u52a8\u6001\u5f15\u5bfc\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5728PHOENIX14T\u548cHow2Sign\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u8d28\u91cf\u548c\u5b9e\u65f6\u6d41\u5f0f\u6548\u7387\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u6df7\u5408\u65b9\u6cd5\u6709\u6548\u7ed3\u5408\u4e86\u81ea\u56de\u5f52\u548c\u6269\u6563\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u624b\u8bed\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6027\u3002"}}
{"id": "2507.09144", "pdf": "https://arxiv.org/pdf/2507.09144", "abs": "https://arxiv.org/abs/2507.09144", "authors": ["Zhimin Liao", "Ping Wei", "Ruijie Zhang", "Shuaijia Chen", "Haoxuan Wang", "Ziyang Ren"], "title": "$I^{2}$-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting", "categories": ["cs.CV"], "comment": null, "summary": "Forecasting the evolution of 3D scenes and generating unseen scenarios via occupancy-based world models offers substantial potential for addressing corner cases in autonomous driving systems. While tokenization has revolutionized image and video generation, efficiently tokenizing complex 3D scenes remains a critical challenge for 3D world models. To address this, we propose $I^{2}$-World, an efficient framework for 4D occupancy forecasting. Our method decouples scene tokenization into intra-scene and inter-scene tokenizers. The intra-scene tokenizer employs a multi-scale residual quantization strategy to hierarchically compress 3D scenes while preserving spatial details. The inter-scene tokenizer residually aggregates temporal dependencies across timesteps. This dual design preserves the compactness of 3D tokenizers while retaining the dynamic expressiveness of 4D tokenizers. Unlike decoder-only GPT-style autoregressive models, $I^{2}$-World adopts an encoder-decoder architecture. The encoder aggregates spatial context from the current scene and predicts a transformation matrix to enable high-level control over scene generation. The decoder, conditioned on this matrix and historical tokens, ensures temporal consistency during generation. Experiments demonstrate that $I^{2}$-World achieves state-of-the-art performance, outperforming existing methods by 25.1\\% in mIoU and 36.9\\% in IoU for 4D occupancy forecasting while exhibiting exceptional computational efficiency: it requires merely 2.9 GB of training memory and achieves real-time inference at 37.0 FPS. Our code is available on https://github.com/lzzzzzm/II-World.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a$I^{2}$-World\u7684\u9ad8\u65484D\u5360\u7528\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u573a\u666f\u6807\u8bb0\u5316\u5b9e\u73b0\u52a8\u60013D\u573a\u666f\u5efa\u6a21\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u590d\u67423D\u573a\u666f\u7684\u9ad8\u6548\u6807\u8bb0\u5316\u95ee\u9898\uff0c\u4ee5\u9884\u6d4b\u573a\u666f\u6f14\u5316\u548c\u751f\u6210\u672a\u89c1\u573a\u666f\u3002", "method": "\u91c7\u7528\u53cc\u6807\u8bb0\u5668\u8bbe\u8ba1\uff08\u573a\u666f\u5185\u548c\u573a\u666f\u95f4\uff09\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u6b8b\u5dee\u91cf\u5316\u548c\u6b8b\u5dee\u805a\u5408\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u4f7f\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u5b9e\u73b0\u9ad8\u6548\u63a7\u5236\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u57284D\u5360\u7528\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cmIoU\u548cIoU\u5206\u522b\u63d0\u534725.1%\u548c36.9%\uff0c\u8bad\u7ec3\u5185\u5b58\u4ec5\u97002.9GB\uff0c\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u4e3a37.0FPS\u3002", "conclusion": "$I^{2}$-World\u6846\u67b6\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a3D\u573a\u666f\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09168", "pdf": "https://arxiv.org/pdf/2507.09168", "abs": "https://arxiv.org/abs/2507.09168", "authors": ["Haiming Zhu", "Yangyang Xu", "Chenshu Xu", "Tingrui Shen", "Wenxi Liu", "Yong Du", "Jun Yu", "Shengfeng He"], "title": "Stable Score Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Text-guided image and 3D editing have advanced with diffusion-based models, yet methods like Delta Denoising Score often struggle with stability, spatial control, and editing strength. These limitations stem from reliance on complex auxiliary structures, which introduce conflicting optimization signals and restrict precise, localized edits. We introduce Stable Score Distillation (SSD), a streamlined framework that enhances stability and alignment in the editing process by anchoring a single classifier to the source prompt. Specifically, SSD utilizes Classifier-Free Guidance (CFG) equation to achieves cross-prompt alignment, and introduces a constant term null-text branch to stabilize the optimization process. This approach preserves the original content's structure and ensures that editing trajectories are closely aligned with the source prompt, enabling smooth, prompt-specific modifications while maintaining coherence in surrounding regions. Additionally, SSD incorporates a prompt enhancement branch to boost editing strength, particularly for style transformations. Our method achieves state-of-the-art results in 2D and 3D editing tasks, including NeRF and text-driven style edits, with faster convergence and reduced complexity, providing a robust and efficient solution for text-guided editing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u7a33\u5b9a\u5206\u6570\u84b8\u998f\uff08SSD\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u6587\u672c\u5f15\u5bfc\u7684\u56fe\u50cf\u548c3D\u7f16\u8f91\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7a33\u5b9a\u6027\u3001\u7a7a\u95f4\u63a7\u5236\u548c\u7f16\u8f91\u5f3a\u5ea6\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982Delta Denoising Score\u4f9d\u8d56\u590d\u6742\u7684\u8f85\u52a9\u7ed3\u6784\uff0c\u5bfc\u81f4\u4f18\u5316\u4fe1\u53f7\u51b2\u7a81\u548c\u5c40\u90e8\u7f16\u8f91\u4e0d\u7cbe\u786e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7a33\u5b9a\u3001\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "SSD\u901a\u8fc7\u951a\u5b9a\u5355\u4e00\u5206\u7c7b\u5668\u5230\u6e90\u63d0\u793a\uff0c\u5229\u7528Classifier-Free Guidance\u65b9\u7a0b\u5b9e\u73b0\u8de8\u63d0\u793a\u5bf9\u9f50\uff0c\u5e76\u5f15\u5165\u5e38\u6570\u9879\u7a7a\u6587\u672c\u5206\u652f\u4ee5\u7a33\u5b9a\u4f18\u5316\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u8fd8\u52a0\u5165\u63d0\u793a\u589e\u5f3a\u5206\u652f\u4ee5\u63d0\u9ad8\u7f16\u8f91\u5f3a\u5ea6\u3002", "result": "SSD\u57282D\u548c3D\u7f16\u8f91\u4efb\u52a1\uff08\u5982NeRF\u548c\u6587\u672c\u9a71\u52a8\u98ce\u683c\u7f16\u8f91\uff09\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u6536\u655b\u66f4\u5feb\u4e14\u590d\u6742\u5ea6\u66f4\u4f4e\u3002", "conclusion": "SSD\u4e3a\u6587\u672c\u5f15\u5bfc\u7f16\u8f91\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5065\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u4fdd\u6301\u539f\u59cb\u5185\u5bb9\u7ed3\u6784\u5e76\u5b9e\u73b0\u5e73\u6ed1\u3001\u63d0\u793a\u7279\u5b9a\u7684\u4fee\u6539\u3002"}}
{"id": "2507.09200", "pdf": "https://arxiv.org/pdf/2507.09200", "abs": "https://arxiv.org/abs/2507.09200", "authors": ["Trong-Thuan Nguyen", "Pha Nguyen", "Jackson Cothren", "Alper Yilmaz", "Minh-Triet Tran", "Khoa Luu"], "title": "THYME: Temporal Hierarchical-Cyclic Interactivity Modeling for Video Scene Graphs in Aerial Footage", "categories": ["cs.CV"], "comment": null, "summary": "The rapid proliferation of video in applications such as autonomous driving, surveillance, and sports analytics necessitates robust methods for dynamic scene understanding. Despite advances in static scene graph generation and early attempts at video scene graph generation, previous methods often suffer from fragmented representations, failing to capture fine-grained spatial details and long-range temporal dependencies simultaneously. To address these limitations, we introduce the Temporal Hierarchical Cyclic Scene Graph (THYME) approach, which synergistically integrates hierarchical feature aggregation with cyclic temporal refinement to address these limitations. In particular, THYME effectively models multi-scale spatial context and enforces temporal consistency across frames, yielding more accurate and coherent scene graphs. In addition, we present AeroEye-v1.0, a novel aerial video dataset enriched with five types of interactivity that overcome the constraints of existing datasets and provide a comprehensive benchmark for dynamic scene graph generation. Empirically, extensive experiments on ASPIRe and AeroEye-v1.0 demonstrate that the proposed THYME approach outperforms state-of-the-art methods, offering improved scene understanding in ground-view and aerial scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTHYME\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u6b21\u7279\u5f81\u805a\u5408\u548c\u5faa\u73af\u65f6\u95f4\u7ec6\u5316\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u573a\u666f\u56fe\u751f\u6210\u4e2d\u7684\u7a7a\u95f4\u7ec6\u8282\u548c\u65f6\u95f4\u4f9d\u8d56\u95ee\u9898\uff0c\u5e76\u5728\u65b0\u6570\u636e\u96c6AeroEye-v1.0\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u52a8\u6001\u573a\u666f\u7406\u89e3\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u76d1\u63a7\u548c\u4f53\u80b2\u5206\u6790\u7b49\u5e94\u7528\u4e2d\u9700\u6c42\u8feb\u5207\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u7a7a\u95f4\u7ec6\u8282\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u63d0\u51faTHYME\u65b9\u6cd5\uff0c\u7ed3\u5408\u5c42\u6b21\u7279\u5f81\u805a\u5408\u548c\u5faa\u73af\u65f6\u95f4\u7ec6\u5316\uff0c\u540c\u65f6\u5efa\u6a21\u591a\u5c3a\u5ea6\u7a7a\u95f4\u4e0a\u4e0b\u6587\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5728ASPIRe\u548cAeroEye-v1.0\u6570\u636e\u96c6\u4e0a\uff0cTHYME\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u573a\u666f\u7406\u89e3\u7684\u51c6\u786e\u6027\u3002", "conclusion": "THYME\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u573a\u666f\u56fe\u751f\u6210\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.09230", "pdf": "https://arxiv.org/pdf/2507.09230", "abs": "https://arxiv.org/abs/2507.09230", "authors": ["G. Kutay T\u00fcrkoglu", "Julian Tanke", "Iheb Belgacem", "Lev Markhasin"], "title": "EgoAnimate: Generating Human Animations from Egocentric top-down Views", "categories": ["cs.CV"], "comment": "10 pages, 5 figures", "summary": "An ideal digital telepresence experience requires accurate replication of a person's body, clothing, and movements. To capture and transfer these movements into virtual reality, the egocentric (first-person) perspective can be adopted, which enables the use of a portable and cost-effective device without front-view cameras. However, this viewpoint introduces challenges such as occlusions and distorted body proportions.   There are few works reconstructing human appearance from egocentric views, and none use a generative prior-based approach. Some methods create avatars from a single egocentric image during inference, but still rely on multi-view datasets during training. To our knowledge, this is the first study using a generative backbone to reconstruct animatable avatars from egocentric inputs. Based on Stable Diffusion, our method reduces training burden and improves generalizability.   Inspired by methods such as SiTH and MagicMan, which perform 360-degree reconstruction from a frontal image, we introduce a pipeline that generates realistic frontal views from occluded top-down images using ControlNet and a Stable Diffusion backbone.   Our goal is to convert a single top-down egocentric image into a realistic frontal representation and feed it into an image-to-motion model. This enables generation of avatar motions from minimal input, paving the way for more accessible and generalizable telepresence systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5148\u9a8c\u7684\u65b9\u6cd5\uff0c\u4ece\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u91cd\u5efa\u53ef\u52a8\u753b\u5316\u865a\u62df\u5f62\u8c61\uff0c\u5229\u7528Stable Diffusion\u51cf\u5c11\u8bad\u7ec3\u8d1f\u62c5\u5e76\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7406\u60f3\u6570\u5b57\u8fdc\u7a0b\u5448\u73b0\u9700\u8981\u51c6\u786e\u590d\u5236\u4eba\u7684\u8eab\u4f53\u3001\u670d\u88c5\u548c\u52a8\u4f5c\uff0c\u4f46\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u5b58\u5728\u906e\u6321\u548c\u8eab\u4f53\u6bd4\u4f8b\u5931\u771f\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u591a\u89c6\u89d2\u6570\u636e\u96c6\u8bad\u7ec3\u3002", "method": "\u57fa\u4e8eStable Diffusion\u548cControlNet\uff0c\u63d0\u51fa\u4ece\u906e\u6321\u7684\u4fef\u89c6\u56fe\u50cf\u751f\u6210\u771f\u5b9e\u6b63\u9762\u89c6\u56fe\u7684\u6d41\u7a0b\uff0c\u5e76\u5c06\u5176\u8f93\u5165\u56fe\u50cf\u5230\u52a8\u4f5c\u6a21\u578b\u3002", "result": "\u5b9e\u73b0\u4e86\u4ece\u5355\u5f20\u4fef\u89c6\u56fe\u50cf\u751f\u6210\u53ef\u52a8\u753b\u5316\u865a\u62df\u5f62\u8c61\uff0c\u51cf\u5c11\u4e86\u8bad\u7ec3\u4f9d\u8d56\u5e76\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u66f4\u6613\u7528\u548c\u901a\u7528\u7684\u8fdc\u7a0b\u5448\u73b0\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.09279", "pdf": "https://arxiv.org/pdf/2507.09279", "abs": "https://arxiv.org/abs/2507.09279", "authors": ["Anita Kriz", "Elizabeth Laura Janes", "Xing Shen", "Tal Arbel"], "title": "Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Preprint version. The peer-reviewed version of this paper has been   accepted to ICCV 2025 Workshop CVAMD", "summary": "Multimodal large language models (MLLMs) hold considerable promise for applications in healthcare. However, their deployment in safety-critical settings is hindered by two key limitations: (i) sensitivity to prompt design, and (ii) a tendency to generate incorrect responses with high confidence. As clinicians may rely on a model's stated confidence to gauge the reliability of its predictions, it is especially important that when a model expresses high confidence, it is also highly accurate. We introduce Prompt4Trust, the first reinforcement learning (RL) framework for prompt augmentation targeting confidence calibration in MLLMs. A lightweight LLM is trained to produce context-aware auxiliary prompts that guide a downstream task MLLM to generate responses in which the expressed confidence more accurately reflects predictive accuracy. Unlike conventional calibration techniques, Prompt4Trust specifically prioritizes aspects of calibration most critical for safe and trustworthy clinical decision-making. Beyond improvements driven by this clinically motivated calibration objective, our proposed method also improves task accuracy, achieving state-of-the-art medical visual question answering (VQA) performance on the PMC-VQA benchmark, which is composed of multiple-choice questions spanning diverse medical imaging modalities. Moreover, our framework trained with a small downstream task MLLM showed promising zero-shot generalization to larger MLLMs in our experiments, suggesting the potential for scalable calibration without the associated computational costs. This work demonstrates the potential of automated yet human-aligned prompt engineering for improving the the trustworthiness of MLLMs in safety critical settings. Our codebase can be found at https://github.com/xingbpshen/vccrl-llm.", "AI": {"tldr": "Prompt4Trust\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u533b\u7597\u9886\u57df\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\uff0c\u540c\u65f6\u63d0\u9ad8\u4efb\u52a1\u51c6\u786e\u6027\u3002", "motivation": "MLLMs\u5728\u533b\u7597\u5e94\u7528\u4e2d\u5b58\u5728\u5bf9\u63d0\u793a\u8bbe\u8ba1\u654f\u611f\u548c\u9519\u8bef\u9ad8\u7f6e\u4fe1\u5ea6\u54cd\u5e94\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e34\u5e8a\u51b3\u7b56\u7684\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u8f7b\u91cf\u7ea7LLM\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8f85\u52a9\u63d0\u793a\uff0c\u6307\u5bfc\u4e0b\u6e38MLLM\u751f\u6210\u7f6e\u4fe1\u5ea6\u66f4\u51c6\u786e\u7684\u54cd\u5e94\u3002", "result": "\u5728PMC-VQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u7684\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5bf9\u66f4\u5927MLLMs\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Prompt4Trust\u5c55\u793a\u4e86\u81ea\u52a8\u5316\u63d0\u793a\u5de5\u7a0b\u5728\u63d0\u5347MLLMs\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u53ef\u4fe1\u5ea6\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.09285", "pdf": "https://arxiv.org/pdf/2507.09285", "abs": "https://arxiv.org/abs/2507.09285", "authors": ["Chenhao Ding", "Jiangtao Zhang", "Zongsheng Yue", "Hui Wang", "Qian Zhao", "Deyu Meng"], "title": "Generative Latent Kernel Modeling for Blind Motion Deblurring", "categories": ["cs.CV"], "comment": null, "summary": "Deep prior-based approaches have demonstrated remarkable success in blind motion deblurring (BMD) recently. These methods, however, are often limited by the high non-convexity of the underlying optimization process in BMD, which leads to extreme sensitivity to the initial blur kernel. To address this issue, we propose a novel framework for BMD that leverages a deep generative model to encode the kernel prior and induce a better initialization for the blur kernel. Specifically, we pre-train a kernel generator based on a generative adversarial network (GAN) to aptly characterize the kernel's prior distribution, as well as a kernel initializer to provide a well-informed and high-quality starting point for kernel estimation. By combining these two components, we constrain the BMD solution within a compact latent kernel manifold, thus alleviating the aforementioned sensitivity for kernel initialization. Notably, the kernel generator and initializer are designed to be easily integrated with existing BMD methods in a plug-and-play manner, enhancing their overall performance. Furthermore, we extend our approach to tackle blind non-uniform motion deblurring without the need for additional priors, achieving state-of-the-art performance on challenging benchmark datasets. The source code is available at https://github.com/dch0319/GLKM-Deblur.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u7684\u76f2\u8fd0\u52a8\u53bb\u6a21\u7cca\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7684GAN\u751f\u6210\u6a21\u7cca\u6838\u5148\u9a8c\u5206\u5e03\u548c\u521d\u59cb\u5316\u5668\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u521d\u59cb\u6a21\u7cca\u6838\u9ad8\u5ea6\u654f\u611f\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5148\u9a8c\u65b9\u6cd5\u5728\u76f2\u8fd0\u52a8\u53bb\u6a21\u7cca\u4e2d\u56e0\u4f18\u5316\u8fc7\u7a0b\u9ad8\u5ea6\u975e\u51f8\u800c\u5bf9\u521d\u59cb\u6a21\u7cca\u6838\u6781\u5ea6\u654f\u611f\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002", "method": "\u9884\u8bad\u7ec3\u57fa\u4e8eGAN\u7684\u6a21\u7cca\u6838\u751f\u6210\u5668\u548c\u521d\u59cb\u5316\u5668\uff0c\u7ea6\u675f\u89e3\u5728\u7d27\u51d1\u7684\u6f5c\u5728\u6838\u6d41\u5f62\u4e2d\uff0c\u63d0\u5347\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "result": "\u5728\u6311\u6218\u6027\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u989d\u5916\u5148\u9a8c\u5373\u53ef\u5904\u7406\u975e\u5747\u5300\u8fd0\u52a8\u6a21\u7cca\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u7cca\u6838\u521d\u59cb\u5316\u654f\u611f\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u76f2\u8fd0\u52a8\u53bb\u6a21\u7cca\u7684\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2507.09308", "pdf": "https://arxiv.org/pdf/2507.09308", "abs": "https://arxiv.org/abs/2507.09308", "authors": ["Zile Wang", "Hao Yu", "Jiabo Zhan", "Chun Yuan"], "title": "AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in latent diffusion models have achieved remarkable results in high-fidelity RGB image synthesis by leveraging pretrained VAEs to compress and reconstruct pixel data at low computational cost. However, the generation of transparent or layered content (RGBA image) remains largely unexplored, due to the lack of large-scale benchmarks. In this work, we propose ALPHA, the first comprehensive RGBA benchmark that adapts standard RGB metrics to four-channel images via alpha blending over canonical backgrounds. We further introduce ALPHAVAE, a unified end-to-end RGBA VAE that extends a pretrained RGB VAE by incorporating a dedicated alpha channel. The model is trained with a composite objective that combines alpha-blended pixel reconstruction, patch-level fidelity, perceptual consistency, and dual KL divergence constraints to ensure latent fidelity across both RGB and alpha representations. Our RGBA VAE, trained on only 8K images in contrast to 1M used by prior methods, achieves a +4.9 dB improvement in PSNR and a +3.2% increase in SSIM over LayerDiffuse in reconstruction. It also enables superior transparent image generation when fine-tuned within a latent diffusion framework. Our code, data, and models are released on https://github.com/o0o0o00o0/AlphaVAE for reproducibility.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86ALPHA\u57fa\u51c6\u548cALPHAVAE\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u900f\u660e\u6216\u5206\u5c42\u7684RGBA\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5728RGB\u56fe\u50cf\u5408\u6210\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46RGBA\u56fe\u50cf\u751f\u6210\u56e0\u7f3a\u4e4f\u5927\u89c4\u6a21\u57fa\u51c6\u800c\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faALPHA\u57fa\u51c6\uff0c\u5e76\u5f00\u53d1ALPHAVAE\u6a21\u578b\uff0c\u901a\u8fc7\u6269\u5c55\u9884\u8bad\u7ec3\u7684RGB VAE\uff0c\u7ed3\u5408\u591a\u79cd\u76ee\u6807\u8bad\u7ec3\u3002", "result": "ALPHAVAE\u5728\u4ec58K\u56fe\u50cf\u4e0a\u8bad\u7ec3\uff0cPSNR\u63d0\u53474.9 dB\uff0cSSIM\u63d0\u53473.2%\uff0c\u4e14\u900f\u660e\u56fe\u50cf\u751f\u6210\u6548\u679c\u66f4\u4f18\u3002", "conclusion": "ALPHAVAE\u4e3aRGBA\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.09514", "pdf": "https://arxiv.org/pdf/2507.09514", "abs": "https://arxiv.org/abs/2507.09514", "authors": ["Tien-Yu Chi", "Hung-Yueh Chiang", "Diana Marculescu", "Kai-Chiang Wu"], "title": "QuarterMap: Efficient Post-Training Token Pruning for Visual State Space Models", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by Efficient Systems for Foundation Models Workshop at the   International Conference on Machine Learning (ICML) 2025", "summary": "State space models (SSMs) reduce the quadratic complexity of transformers by leveraging linear recurrence. Recently, VMamba has emerged as a strong SSM-based vision backbone, yet remains bottlenecked by spatial redundancy in its four-directional scan. We propose QuarterMap, a post-training activation pruning method that removes redundant spatial activations before scanning and restores dimensions via nearest-neighbor upsampling. Our method improves throughput without retraining. On ImageNet-1K, QuarterMap achieves up to 11% speedup on VMamba with less than 0.9% accuracy drop, and yields similar gains on ADE20K segmentation. Beyond VMamba, we validate QuarterMap on MedMamba, a domain-specific model that shares the same four-directional scanning structure, where it consistently improves throughput while preserving accuracy across multiple medical imaging tasks. Compared to token merging methods like ToMe, QuarterMap is tailored for SSMs and avoids costly merge-unmerge operations. Our method offers a plug-and-play tool for deployment-time efficiency without compromising transferability.", "AI": {"tldr": "QuarterMap\u662f\u4e00\u79cd\u540e\u8bad\u7ec3\u6fc0\u6d3b\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u79fb\u9664\u5197\u4f59\u7a7a\u95f4\u6fc0\u6d3b\u5e76\u6062\u590d\u7ef4\u5ea6\uff0c\u63d0\u5347VMamba\u7b49SSM\u6a21\u578b\u7684\u6548\u7387\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u89e3\u51b3VMamba\u7b49\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u7684\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u5728\u7a7a\u95f4\u626b\u63cf\u4e2d\u7684\u5197\u4f59\u95ee\u9898\uff0c\u63d0\u5347\u8fd0\u884c\u6548\u7387\u3002", "method": "\u63d0\u51faQuarterMap\u65b9\u6cd5\uff0c\u5728\u626b\u63cf\u524d\u526a\u679d\u5197\u4f59\u7a7a\u95f4\u6fc0\u6d3b\uff0c\u5e76\u901a\u8fc7\u6700\u8fd1\u90bb\u4e0a\u91c7\u6837\u6062\u590d\u7ef4\u5ea6\u3002", "result": "\u5728ImageNet-1K\u4e0a\u5b9e\u73b011%\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u7cbe\u5ea6\u4e0b\u964d\u5c0f\u4e8e0.9%\uff1b\u5728ADE20K\u548cMedMamba\u4e0a\u540c\u6837\u6709\u6548\u3002", "conclusion": "QuarterMap\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u90e8\u7f72\u6548\u7387\u5de5\u5177\uff0c\u9002\u7528\u4e8eSSM\u6a21\u578b\uff0c\u4e14\u4e0d\u5f71\u54cd\u8fc1\u79fb\u6027\u3002"}}
{"id": "2507.09524", "pdf": "https://arxiv.org/pdf/2507.09524", "abs": "https://arxiv.org/abs/2507.09524", "authors": ["Yunwei Lan", "Zhigao Cui", "Xin Luo", "Chang Liu", "Nian Wang", "Menglin Zhang", "Yanzhao Su", "Dong Liu"], "title": "When Schr\u00f6dinger Bridge Meets Real-World Image Dehazing with Unpaired Training", "categories": ["cs.CV"], "comment": "Accepted by ICCV2025", "summary": "Recent advancements in unpaired dehazing, particularly those using GANs, show promising performance in processing real-world hazy images. However, these methods tend to face limitations due to the generator's limited transport mapping capability, which hinders the full exploitation of their effectiveness in unpaired training paradigms. To address these challenges, we propose DehazeSB, a novel unpaired dehazing framework based on the Schr\\\"odinger Bridge. By leveraging optimal transport (OT) theory, DehazeSB directly bridges the distributions between hazy and clear images. This enables optimal transport mappings from hazy to clear images in fewer steps, thereby generating high-quality results. To ensure the consistency of structural information and details in the restored images, we introduce detail-preserving regularization, which enforces pixel-level alignment between hazy inputs and dehazed outputs. Furthermore, we propose a novel prompt learning to leverage pre-trained CLIP models in distinguishing hazy images and clear ones, by learning a haze-aware vision-language alignment. Extensive experiments on multiple real-world datasets demonstrate our method's superiority. Code: https://github.com/ywxjm/DehazeSB.", "AI": {"tldr": "DehazeSB\u662f\u4e00\u79cd\u57fa\u4e8eSchr\u00f6dinger Bridge\u7684\u65b0\u578b\u65e0\u914d\u5bf9\u53bb\u96fe\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u76f4\u63a5\u8fde\u63a5\u96fe\u56fe\u548c\u6e05\u6670\u56fe\u7684\u5206\u5e03\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eGAN\u7684\u65e0\u914d\u5bf9\u53bb\u96fe\u65b9\u6cd5\u56e0\u751f\u6210\u5668\u7684\u4f20\u8f93\u6620\u5c04\u80fd\u529b\u6709\u9650\u800c\u6548\u679c\u53d7\u9650\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u5229\u7528\u6700\u4f18\u4f20\u8f93\u7406\u8bba\uff0c\u7ed3\u5408\u7ec6\u8282\u4fdd\u7559\u6b63\u5219\u5316\u548c\u57fa\u4e8eCLIP\u7684\u63d0\u793a\u5b66\u4e60\uff0c\u5b9e\u73b0\u9ad8\u6548\u53bb\u96fe\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "DehazeSB\u901a\u8fc7Schr\u00f6dinger Bridge\u548c\u7ec6\u8282\u4fdd\u7559\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u914d\u5bf9\u53bb\u96fe\u7684\u6548\u679c\u3002"}}
{"id": "2507.09562", "pdf": "https://arxiv.org/pdf/2507.09562", "abs": "https://arxiv.org/abs/2507.09562", "authors": ["Yidong Jiang"], "title": "Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The Segment Anything Model (SAM) has revolutionized image segmentation through its innovative prompt-based approach, yet the critical role of prompt engineering in its success remains underexplored. This paper presents the first comprehensive survey focusing specifically on prompt engineering techniques for SAM and its variants. We systematically organize and analyze the rapidly growing body of work in this emerging field, covering fundamental methodologies, practical applications, and key challenges. Our review reveals how prompt engineering has evolved from simple geometric inputs to sophisticated multimodal approaches, enabling SAM's adaptation across diverse domains including medical imaging and remote sensing. We identify unique challenges in prompt optimization and discuss promising research directions. This survey fills an important gap in the literature by providing a structured framework for understanding and advancing prompt engineering in foundation models for segmentation.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5168\u9762\u8c03\u67e5\u4e86SAM\u53ca\u5176\u53d8\u4f53\u7684\u63d0\u793a\u5de5\u7a0b\u6280\u672f\uff0c\u7cfb\u7edf\u6574\u7406\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u63ed\u793a\u4e86\u63d0\u793a\u5de5\u7a0b\u4ece\u7b80\u5355\u51e0\u4f55\u8f93\u5165\u5230\u591a\u6a21\u6001\u65b9\u6cd5\u7684\u53d1\u5c55\uff0c\u5e76\u8ba8\u8bba\u4e86\u4f18\u5316\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1SAM\u901a\u8fc7\u63d0\u793a\u65b9\u6cd5\u9769\u65b0\u4e86\u56fe\u50cf\u5206\u5272\uff0c\u4f46\u63d0\u793a\u5de5\u7a0b\u7684\u5173\u952e\u4f5c\u7528\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7cfb\u7edf\u6574\u7406\u548c\u5206\u6790SAM\u53ca\u5176\u53d8\u4f53\u7684\u63d0\u793a\u5de5\u7a0b\u6280\u672f\uff0c\u5305\u62ec\u65b9\u6cd5\u3001\u5e94\u7528\u548c\u6311\u6218\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u63d0\u793a\u5de5\u7a0b\u5df2\u4ece\u7b80\u5355\u51e0\u4f55\u8f93\u5165\u53d1\u5c55\u4e3a\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u652f\u6301SAM\u5728\u533b\u7597\u5f71\u50cf\u548c\u9065\u611f\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002", "conclusion": "\u672c\u6587\u4e3a\u5206\u5272\u57fa\u7840\u6a21\u578b\u4e2d\u7684\u63d0\u793a\u5de5\u7a0b\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.09574", "pdf": "https://arxiv.org/pdf/2507.09574", "abs": "https://arxiv.org/abs/2507.09574", "authors": ["Haozhe Zhao", "Zefan Cai", "Shuzheng Si", "Liang Chen", "Jiuxiang Gu", "Wen Xiao", "Junjie Hu"], "title": "MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "24 pages,12 figures", "summary": "Recent text-to-image models produce high-quality results but still struggle with precise visual control, balancing multimodal inputs, and requiring extensive training for complex multimodal image generation. To address these limitations, we propose MENTOR, a novel autoregressive (AR) framework for efficient Multimodal-conditioned Tuning for Autoregressive multimodal image generation. MENTOR combines an AR image generator with a two-stage training paradigm, enabling fine-grained, token-level alignment between multimodal inputs and image outputs without relying on auxiliary adapters or cross-attention modules. The two-stage training consists of: (1) a multimodal alignment stage that establishes robust pixel- and semantic-level alignment, followed by (2) a multimodal instruction tuning stage that balances the integration of multimodal inputs and enhances generation controllability. Despite modest model size, suboptimal base components, and limited training resources, MENTOR achieves strong performance on the DreamBench++ benchmark, outperforming competitive baselines in concept preservation and prompt following. Additionally, our method delivers superior image reconstruction fidelity, broad task adaptability, and improved training efficiency compared to diffusion-based methods. Dataset, code, and models are available at: https://github.com/HaozheZhao/MENTOR", "AI": {"tldr": "MENTOR\u662f\u4e00\u4e2a\u65b0\u578b\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5b9e\u73b0\u591a\u6a21\u6001\u8f93\u5165\u4e0e\u56fe\u50cf\u8f93\u51fa\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\uff0c\u63d0\u5347\u4e86\u751f\u6210\u63a7\u5236\u6027\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u7cbe\u786e\u89c6\u89c9\u63a7\u5236\u3001\u591a\u6a21\u6001\u8f93\u5165\u5e73\u8861\u548c\u590d\u6742\u591a\u6a21\u6001\u56fe\u50cf\u751f\u6210\u8bad\u7ec3\u9700\u6c42\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u7ed3\u5408\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u5668\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff1a\u591a\u6a21\u6001\u5bf9\u9f50\u9636\u6bb5\u548c\u6307\u4ee4\u8c03\u4f18\u9636\u6bb5\u3002", "result": "\u5728DreamBench++\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u5177\u6709\u66f4\u9ad8\u7684\u56fe\u50cf\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u8bad\u7ec3\u6548\u7387\u3002", "conclusion": "MENTOR\u901a\u8fc7\u9ad8\u6548\u7684\u591a\u6a21\u6001\u6761\u4ef6\u8c03\u4f18\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u751f\u6210\u7684\u7cbe\u786e\u6027\u548c\u63a7\u5236\u6027\u3002"}}
{"id": "2507.09595", "pdf": "https://arxiv.org/pdf/2507.09595", "abs": "https://arxiv.org/abs/2507.09595", "authors": ["Or Greenberg"], "title": "Demystifying Flux Architecture", "categories": ["cs.CV"], "comment": null, "summary": "FLUX.1 is a diffusion-based text-to-image generation model developed by Black Forest Labs, designed to achieve faithful text-image alignment while maintaining high image quality and diversity. FLUX is considered state-of-the-art in text-to-image generation, outperforming popular models such as Midjourney, DALL-E 3, Stable Diffusion 3 (SD3), and SDXL. Although publicly available as open source, the authors have not released official technical documentation detailing the model's architecture or training setup. This report summarizes an extensive reverse-engineering effort aimed at demystifying FLUX's architecture directly from its source code, to support its adoption as a backbone for future research and development. This document is an unofficial technical report and is not published or endorsed by the original developers or their affiliated institutions.", "AI": {"tldr": "FLUX.1\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u65e8\u5728\u5b9e\u73b0\u9ad8\u4fdd\u771f\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002", "motivation": "\u5c3d\u7ba1FLUX.1\u662f\u5f00\u6e90\u7684\uff0c\u4f46\u7f3a\u4e4f\u5b98\u65b9\u6280\u672f\u6587\u6863\uff0c\u56e0\u6b64\u9700\u8981\u9006\u5411\u5de5\u7a0b\u4ee5\u652f\u6301\u5176\u4f5c\u4e3a\u672a\u6765\u7814\u7a76\u548c\u5f00\u53d1\u7684\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u9006\u5411\u5de5\u7a0b\u4ece\u6e90\u4ee3\u7801\u4e2d\u89e3\u6790FLUX.1\u7684\u67b6\u6784\u548c\u8bad\u7ec3\u8bbe\u7f6e\u3002", "result": "\u6210\u529f\u89e3\u6790\u4e86FLUX.1\u7684\u67b6\u6784\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u652f\u6301\u3002", "conclusion": "\u672c\u62a5\u544a\u4e3aFLUX.1\u7684\u9006\u5411\u5de5\u7a0b\u63d0\u4f9b\u4e86\u6280\u672f\u7ec6\u8282\uff0c\u6709\u52a9\u4e8e\u5176\u5728\u672a\u6765\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.09612", "pdf": "https://arxiv.org/pdf/2507.09612", "abs": "https://arxiv.org/abs/2507.09612", "authors": ["You Huang", "Lichao Chen", "Jiayi Ji", "Liujuan Cao", "Shengchuan Zhang", "Rongrong Ji"], "title": "Inter2Former: Dynamic Hybrid Attention for Efficient High-Precision Interactive", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Interactive segmentation (IS) improves annotation efficiency by segmenting target regions from user prompts, with widespread applications in real-world scenarios. Current approaches face a critical trade-off: dense-token methods achieve superior accuracy and detail preservation but suffer from prohibitively slow processing on CPU devices, while the Segment Anything Model (SAM) advances the field with sparse prompt tokens for fast inference but compromises segmentation quality. In this paper, we propose Inter2Former to address this challenge by optimizing computation allocation in dense-token processing, which introduces four key enhancements. First, we propose Dynamic Prompt Embedding (DPE) that adaptively processes only regions of interest while avoiding additional overhead from background tokens. Second, we introduce Dynamic Hybrid Attention (DHA), which leverages previous segmentation masks to route tokens through either full attention (O(N2)) for boundary regions or our proposed efficient BSQ attention (O(N)) for non-boundary regions. Third, we develop Hybrid Mixture of Experts (HMoE), which applies similar adaptive computation strategies in FFN modules with CPU-optimized parallel processing. Finally, we present Dynamic Local Upsampling (DLU), a reverse operation of DPE, which localizes objects with a lightweight MLP and performs fine-grained upsampling only in detected regions. Experimental results on high-precision IS benchmarks demonstrate that Inter2Former achieves SOTA performance with high efficiency on CPU devices.", "AI": {"tldr": "Inter2Former\u901a\u8fc7\u4f18\u5316\u5bc6\u96c6\u4ee4\u724c\u5904\u7406\u7684\u8ba1\u7b97\u5206\u914d\uff0c\u63d0\u51fa\u56db\u79cd\u5173\u952e\u6539\u8fdb\uff0c\u89e3\u51b3\u4e86\u4ea4\u4e92\u5f0f\u5206\u5272\u4e2d\u901f\u5ea6\u4e0e\u8d28\u91cf\u7684\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86CPU\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548SOTA\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u4ea4\u4e92\u5f0f\u5206\u5272\u65b9\u6cd5\u5728\u5bc6\u96c6\u4ee4\u724c\u5904\u7406\u4e0a\u5b58\u5728\u901f\u5ea6\u4e0e\u8d28\u91cf\u7684\u77db\u76fe\uff1a\u5bc6\u96c6\u4ee4\u724c\u65b9\u6cd5\u7cbe\u5ea6\u9ad8\u4f46\u901f\u5ea6\u6162\uff0cSAM\u901f\u5ea6\u5feb\u4f46\u8d28\u91cf\u4f4e\u3002Inter2Former\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51fa\u56db\u79cd\u6539\u8fdb\uff1a1) \u52a8\u6001\u63d0\u793a\u5d4c\u5165(DPE)\uff0c\u81ea\u9002\u5e94\u5904\u7406\u611f\u5174\u8da3\u533a\u57df\uff1b2) \u52a8\u6001\u6df7\u5408\u6ce8\u610f\u529b(DHA)\uff0c\u6839\u636e\u533a\u57df\u7c7b\u578b\u9009\u62e9\u6ce8\u610f\u529b\u673a\u5236\uff1b3) \u6df7\u5408\u4e13\u5bb6\u7cfb\u7edf(HMoE)\uff0c\u5728FFN\u6a21\u5757\u4e2d\u4f18\u5316\u8ba1\u7b97\uff1b4) \u52a8\u6001\u5c40\u90e8\u4e0a\u91c7\u6837(DLU)\uff0c\u8f7b\u91cf\u7ea7MLP\u5b9a\u4f4d\u5bf9\u8c61\u5e76\u7cbe\u7ec6\u4e0a\u91c7\u6837\u3002", "result": "\u5728\u9ad8\u7cbe\u5ea6\u4ea4\u4e92\u5f0f\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cInter2Former\u5728CPU\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u4e0e\u9ad8\u6548\u7387\u3002", "conclusion": "Inter2Former\u901a\u8fc7\u8ba1\u7b97\u4f18\u5316\uff0c\u6210\u529f\u5e73\u8861\u4e86\u4ea4\u4e92\u5f0f\u5206\u5272\u7684\u901f\u5ea6\u4e0e\u8d28\u91cf\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09619", "pdf": "https://arxiv.org/pdf/2507.09619", "abs": "https://arxiv.org/abs/2507.09619", "authors": ["Yilin Lu", "Jianghang Lin", "Linhuang Xie", "Kai Zhao", "Yansong Qu", "Shengchuan Zhang", "Liujuan Cao", "Rongrong Ji"], "title": "Generate Aligned Anomaly: Region-Guided Few-Shot Anomaly Image-Mask Pair Synthesis for Industrial Inspection", "categories": ["cs.CV"], "comment": null, "summary": "Anomaly inspection plays a vital role in industrial manufacturing, but the scarcity of anomaly samples significantly limits the effectiveness of existing methods in tasks such as localization and classification. While several anomaly synthesis approaches have been introduced for data augmentation, they often struggle with low realism, inaccurate mask alignment, and poor generalization. To overcome these limitations, we propose Generate Aligned Anomaly (GAA), a region-guided, few-shot anomaly image-mask pair generation framework. GAA leverages the strong priors of a pretrained latent diffusion model to generate realistic, diverse, and semantically aligned anomalies using only a small number of samples. The framework first employs Localized Concept Decomposition to jointly model the semantic features and spatial information of anomalies, enabling flexible control over the type and location of anomalies. It then utilizes Adaptive Multi-Round Anomaly Clustering to perform fine-grained semantic clustering of anomaly concepts, thereby enhancing the consistency of anomaly representations. Subsequently, a region-guided mask generation strategy ensures precise alignment between anomalies and their corresponding masks, while a low-quality sample filtering module is introduced to further improve the overall quality of the generated samples. Extensive experiments on the MVTec AD and LOCO datasets demonstrate that GAA achieves superior performance in both anomaly synthesis quality and downstream tasks such as localization and classification.", "AI": {"tldr": "GAA\u662f\u4e00\u4e2a\u57fa\u4e8e\u533a\u57df\u5f15\u5bfc\u7684\u5c11\u6837\u672c\u5f02\u5e38\u56fe\u50cf-\u63a9\u7801\u5bf9\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u751f\u6210\u771f\u5b9e\u3001\u591a\u6837\u4e14\u8bed\u4e49\u5bf9\u9f50\u7684\u5f02\u5e38\u6837\u672c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5f02\u5e38\u5408\u6210\u4e2d\u7684\u4f4e\u771f\u5b9e\u6027\u548c\u5bf9\u9f50\u95ee\u9898\u3002", "motivation": "\u5de5\u4e1a\u5236\u9020\u4e2d\u5f02\u5e38\u6837\u672c\u7a00\u7f3a\uff0c\u73b0\u6709\u5f02\u5e38\u5408\u6210\u65b9\u6cd5\u5b58\u5728\u4f4e\u771f\u5b9e\u6027\u3001\u63a9\u7801\u5bf9\u9f50\u4e0d\u51c6\u786e\u548c\u6cdb\u5316\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "GAA\u91c7\u7528\u5c40\u90e8\u6982\u5ff5\u5206\u89e3\u5efa\u6a21\u5f02\u5e38\u8bed\u4e49\u548c\u7a7a\u95f4\u4fe1\u606f\uff0c\u81ea\u9002\u5e94\u591a\u8f6e\u5f02\u5e38\u805a\u7c7b\u589e\u5f3a\u8868\u793a\u4e00\u81f4\u6027\uff0c\u533a\u57df\u5f15\u5bfc\u63a9\u7801\u751f\u6210\u786e\u4fdd\u5bf9\u9f50\uff0c\u5e76\u5f15\u5165\u4f4e\u8d28\u91cf\u6837\u672c\u8fc7\u6ee4\u6a21\u5757\u3002", "result": "\u5728MVTec AD\u548cLOCO\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGAA\u5728\u5f02\u5e38\u5408\u6210\u8d28\u91cf\u548c\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u5b9a\u4f4d\u548c\u5206\u7c7b\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "GAA\u901a\u8fc7\u521b\u65b0\u7684\u533a\u57df\u5f15\u5bfc\u548c\u5c11\u6837\u672c\u751f\u6210\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u5408\u6210\u7684\u771f\u5b9e\u6027\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2507.09630", "pdf": "https://arxiv.org/pdf/2507.09630", "abs": "https://arxiv.org/abs/2507.09630", "authors": ["Shomukh Qari", "Maha A. Thafar"], "title": "Brain Stroke Detection and Classification Using CT Imaging with Transformer Models and Explainable AI", "categories": ["cs.CV", "cs.AI"], "comment": "5 figures", "summary": "Stroke is one of the leading causes of death globally, making early and accurate diagnosis essential for improving patient outcomes, particularly in emergency settings where timely intervention is critical. CT scans are the key imaging modality because of their speed, accessibility, and cost-effectiveness. This study proposed an artificial intelligence framework for multiclass stroke classification (ischemic, hemorrhagic, and no stroke) using CT scan images from a dataset provided by the Republic of Turkey's Ministry of Health. The proposed method adopted MaxViT, a state-of-the-art Vision Transformer, as the primary deep learning model for image-based stroke classification, with additional transformer variants (vision transformer, transformer-in-transformer, and ConvNext). To enhance model generalization and address class imbalance, we applied data augmentation techniques, including synthetic image generation. The MaxViT model trained with augmentation achieved the best performance, reaching an accuracy and F1-score of 98.00%, outperforming all other evaluated models and the baseline methods. The primary goal of this study was to distinguish between stroke types with high accuracy while addressing crucial issues of transparency and trust in artificial intelligence models. To achieve this, Explainable Artificial Intelligence (XAI) was integrated into the framework, particularly Grad-CAM++. It provides visual explanations of the model's decisions by highlighting relevant stroke regions in the CT scans and establishing an accurate, interpretable, and clinically applicable solution for early stroke detection. This research contributed to the development of a trustworthy AI-assisted diagnostic tool for stroke, facilitating its integration into clinical practice and enhancing access to timely and optimal stroke diagnosis in emergency departments, thereby saving more lives.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMaxViT\u7684\u591a\u7c7b\u522b\u4e2d\u98ce\u5206\u7c7bAI\u6846\u67b6\uff0c\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u548cXAI\u6280\u672f\uff0c\u5b9e\u73b0\u4e8698%\u7684\u51c6\u786e\u7387\u548cF1\u5206\u6570\uff0c\u65e8\u5728\u63d0\u5347\u4e2d\u98ce\u65e9\u671f\u8bca\u65ad\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4e2d\u98ce\u662f\u5168\u7403\u4e3b\u8981\u6b7b\u56e0\u4e4b\u4e00\uff0c\u65e9\u671f\u51c6\u786e\u8bca\u65ad\u5bf9\u6539\u5584\u60a3\u8005\u9884\u540e\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u5728\u6025\u8bca\u73af\u5883\u4e2d\u3002CT\u626b\u63cf\u56e0\u5176\u5feb\u901f\u3001\u53ef\u53ca\u6027\u548c\u6210\u672c\u6548\u76ca\u6210\u4e3a\u5173\u952e\u5f71\u50cf\u5b66\u624b\u6bb5\u3002", "method": "\u91c7\u7528MaxViT\u4f5c\u4e3a\u4e3b\u8981\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7ed3\u5408\u5176\u4ed6Transformer\u53d8\u4f53\uff08\u5982Vision Transformer\u548cConvNext\uff09\uff0c\u5e76\u5e94\u7528\u6570\u636e\u589e\u5f3a\u6280\u672f\uff08\u5305\u62ec\u5408\u6210\u56fe\u50cf\u751f\u6210\uff09\u4ee5\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "result": "MaxViT\u6a21\u578b\u5728\u589e\u5f3a\u6570\u636e\u8bad\u7ec3\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u548cF1\u5206\u6570\u8fbe98.00%\uff0c\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u548c\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u53ef\u4fe1\u8d56\u7684AI\u8f85\u52a9\u8bca\u65ad\u5de5\u5177\uff0c\u7ed3\u5408XAI\u6280\u672f\uff08\u5982Grad-CAM++\uff09\uff0c\u4e3a\u4e34\u5e8a\u5b9e\u8df5\u63d0\u4f9b\u4e86\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u4e2d\u98ce\u65e9\u671f\u68c0\u6d4b\u65b9\u6848\u3002"}}
{"id": "2507.09681", "pdf": "https://arxiv.org/pdf/2507.09681", "abs": "https://arxiv.org/abs/2507.09681", "authors": ["Osher Rafaeli", "Tal Svoray", "Ariel Nahlieli"], "title": "Prompt2DEM: High-Resolution DEMs for Urban and Open Environments from Global Prompts Using a Monocular Foundation Model", "categories": ["cs.CV", "eess.IV"], "comment": "18 pages", "summary": "High-resolution elevation estimations are essential to understand catchment and hillslope hydrology, study urban morphology and dynamics, and monitor the growth, decline, and mortality of terrestrial ecosystems. Various deep learning approaches (e.g., super-resolution techniques, monocular depth estimation) have been developed to create high-resolution Digital Elevation Models (DEMs). However, super-resolution techniques are limited by the upscaling factor, and monocular depth estimation lacks global elevation context, making its conversion to a seamless DEM restricted. The recently introduced technique of prompt-based monocular depth estimation has opened new opportunities to extract estimates of absolute elevation in a global context. We present here a framework for the estimation of high-resolution DEMs as a new paradigm for absolute global elevation mapping. It is exemplified using low-resolution Shuttle Radar Topography Mission (SRTM) elevation data as prompts and high-resolution RGB imagery from the National Agriculture Imagery Program (NAIP). The approach fine-tunes a vision transformer encoder with LiDAR-derived DEMs and employs a versatile prompting strategy, enabling tasks such as DEM estimation, void filling, and updating. Our framework achieves a 100x resolution gain (from 30-m to 30-cm), surpassing prior methods by an order of magnitude. Evaluations across three diverse U.S. landscapes show robust generalization, capturing urban structures and fine-scale terrain features with < 5 m MAE relative to LiDAR, improving over SRTM by up to 18%. Hydrological analysis confirms suitability for hazard and environmental studies. We demonstrate scalability by applying the framework to large regions in the U.S. and Israel. All code and pretrained models are publicly available at: https://osherr1996.github.io/prompt2dem_propage/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u6570\u5b57\u9ad8\u7a0b\u6a21\u578b\uff08DEM\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u8fa8\u7387\uff08100\u500d\uff09\uff0c\u5e76\u5728\u591a\u6837\u5316\u7684\u666f\u89c2\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u9ad8\u7a0b\u4f30\u8ba1\u5bf9\u6c34\u6587\u3001\u57ce\u5e02\u5f62\u6001\u548c\u751f\u6001\u7cfb\u7edf\u7814\u7a76\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u8d85\u5206\u8fa8\u7387\u6280\u672f\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff09\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u4f4e\u5206\u8fa8\u7387SRTM\u9ad8\u7a0b\u6570\u636e\u4f5c\u4e3a\u63d0\u793a\uff0c\u7ed3\u5408\u9ad8\u5206\u8fa8\u7387RGB\u56fe\u50cf\uff0c\u901a\u8fc7\u5fae\u8c03\u89c6\u89c9\u53d8\u6362\u5668\u7f16\u7801\u5668\u5b9e\u73b0DEM\u4f30\u8ba1\u3001\u586b\u8865\u548c\u66f4\u65b0\u3002", "result": "\u6846\u67b6\u5b9e\u73b0\u4e86\u4ece30\u7c73\u523030\u5398\u7c73\u7684100\u500d\u5206\u8fa8\u7387\u63d0\u5347\uff0cMAE\u4f4e\u4e8e5\u7c73\uff0c\u4f18\u4e8eSRTM 18%\uff0c\u9002\u7528\u4e8e\u6c34\u6587\u548c\u73af\u5883\u7814\u7a76\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5168\u7403\u9ad8\u7a0b\u6d4b\u7ed8\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u516c\u5f00\u3002"}}
{"id": "2507.09748", "pdf": "https://arxiv.org/pdf/2507.09748", "abs": "https://arxiv.org/abs/2507.09748", "authors": ["Yu Lei", "Bingde Liu", "Qingsong Xie", "Haonan Lu", "Zhijie Deng"], "title": "Advancing Text-to-3D Generation with Linearized Lookahead Variational Score Distillation", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Text-to-3D generation based on score distillation of pre-trained 2D diffusion models has gained increasing interest, with variational score distillation (VSD) as a remarkable example. VSD proves that vanilla score distillation can be improved by introducing an extra score-based model, which characterizes the distribution of images rendered from 3D models, to correct the distillation gradient. Despite the theoretical foundations, VSD, in practice, is likely to suffer from slow and sometimes ill-posed convergence. In this paper, we perform an in-depth investigation of the interplay between the introduced score model and the 3D model, and find that there exists a mismatching problem between LoRA and 3D distributions in practical implementation. We can simply adjust their optimization order to improve the generation quality. By doing so, the score model looks ahead to the current 3D state and hence yields more reasonable corrections. Nevertheless, naive lookahead VSD may suffer from unstable training in practice due to the potential over-fitting. To address this, we propose to use a linearized variant of the model for score distillation, giving rise to the Linearized Lookahead Variational Score Distillation ($L^2$-VSD). $L^2$-VSD can be realized efficiently with forward-mode autodiff functionalities of existing deep learning libraries. Extensive experiments validate the efficacy of $L^2$-VSD, revealing its clear superiority over prior score distillation-based methods. We also show that our method can be seamlessly incorporated into any other VSD-based text-to-3D framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u53d8\u5206\u5206\u6570\u84b8\u998f\u65b9\u6cd5\uff08$L^2$-VSD\uff09\uff0c\u901a\u8fc7\u8c03\u6574\u4f18\u5316\u987a\u5e8f\u548c\u7ebf\u6027\u5316\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfVSD\u65b9\u6cd5\u7684\u6536\u655b\u6162\u548c\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfVSD\u65b9\u6cd5\u5728\u5b9e\u8df5\u4e2d\u5b58\u5728\u6536\u655b\u6162\u548c\u4f18\u5316\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u7814\u7a76\u53d1\u73b0\u8fd9\u662f\u7531\u4e8eLoRA\u4e0e3D\u5206\u5e03\u4e0d\u5339\u914d\u5bfc\u81f4\u7684\u3002", "method": "\u63d0\u51fa\u7ebf\u6027\u5316\u524d\u77bb\u53d8\u5206\u5206\u6570\u84b8\u998f\uff08$L^2$-VSD\uff09\uff0c\u901a\u8fc7\u8c03\u6574\u4f18\u5316\u987a\u5e8f\u548c\u7ebf\u6027\u5316\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e$L^2$-VSD\u5728\u751f\u6210\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u65e0\u7f1d\u96c6\u6210\u5230\u5176\u4ed6VSD\u6846\u67b6\u4e2d\u3002", "conclusion": "$L^2$-VSD\u901a\u8fc7\u7b80\u5355\u800c\u6709\u6548\u7684\u6539\u8fdb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u52303D\u751f\u6210\u7684\u6027\u80fd\u3002"}}
{"id": "2507.09881", "pdf": "https://arxiv.org/pdf/2507.09881", "abs": "https://arxiv.org/abs/2507.09881", "authors": ["Yiran Qiao", "Disheng Liu", "Yiren Lu", "Yu Yin", "Mengnan Du", "Jing Ma"], "title": "Counterfactual Visual Explanation via Causally-Guided Adversarial Steering", "categories": ["cs.CV"], "comment": null, "summary": "Recent work on counterfactual visual explanations has contributed to making artificial intelligence models more explainable by providing visual perturbation to flip the prediction. However, these approaches neglect the causal relationships and the spurious correlations behind the image generation process, which often leads to unintended alterations in the counterfactual images and renders the explanations with limited quality. To address this challenge, we introduce a novel framework CECAS, which first leverages a causally-guided adversarial method to generate counterfactual explanations. It innovatively integrates a causal perspective to avoid unwanted perturbations on spurious factors in the counterfactuals. Extensive experiments demonstrate that our method outperforms existing state-of-the-art approaches across multiple benchmark datasets and ultimately achieves a balanced trade-off among various aspects of validity, sparsity, proximity, and realism.", "AI": {"tldr": "CECAS\u6846\u67b6\u901a\u8fc7\u56e0\u679c\u5f15\u5bfc\u7684\u5bf9\u6297\u65b9\u6cd5\u751f\u6210\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u907f\u514d\u865a\u5047\u56e0\u7d20\u5e72\u6270\uff0c\u63d0\u5347\u89e3\u91ca\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u53cd\u4e8b\u5b9e\u89c6\u89c9\u89e3\u91ca\u65b9\u6cd5\u5ffd\u89c6\u56e0\u679c\u5173\u7cfb\u548c\u865a\u5047\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u89e3\u91ca\u8d28\u91cf\u53d7\u9650\u3002", "method": "\u63d0\u51faCECAS\u6846\u67b6\uff0c\u7ed3\u5408\u56e0\u679c\u89c6\u89d2\u548c\u5bf9\u6297\u65b9\u6cd5\u751f\u6210\u53cd\u4e8b\u5b9e\u89e3\u91ca\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u8861\u4e86\u6709\u6548\u6027\u3001\u7a00\u758f\u6027\u3001\u63a5\u8fd1\u6027\u548c\u771f\u5b9e\u6027\u3002", "conclusion": "CECAS\u901a\u8fc7\u56e0\u679c\u6574\u5408\u663e\u8457\u63d0\u5347\u53cd\u4e8b\u5b9e\u89e3\u91ca\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.09885", "pdf": "https://arxiv.org/pdf/2507.09885", "abs": "https://arxiv.org/abs/2507.09885", "authors": ["Zhanjiang Yang", "Lijun Sun", "Jiawei Dong", "Xiaoxin An", "Yang Liu", "Meng Li"], "title": "MCGA: Mixture of Codebooks Hyperspectral Reconstruction via Grayscale-Aware Attention", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing hyperspectral images (HSI) from RGB images is a cost-effective solution for various vision-based applications. However, most existing learning-based hyperspectral reconstruction methods directly learn the RGB-to-HSI mapping using complex attention mechanisms, neglecting the inherent challenge of transitioning from low-dimensional to high-dimensional information. To address this limitation, we propose a two-stage approach, MCGA, which first learns spectral patterns before estimating the mapping. In the first stage, a multi-scale VQ-VAE learns representations from heterogeneous HSI datasets, extracting a Mixture of Codebooks (MoC). In the second stage, the RGB-to-HSI mapping is refined by querying features from the MoC to replace latent HSI representations, incorporating prior knowledge rather than forcing a direct high-dimensional transformation. To further enhance reconstruction quality, we introduce Grayscale-Aware Attention and Quantized Self-Attention, which adaptively adjust feature map intensities to meet hyperspectral reconstruction requirements. This physically motivated attention mechanism ensures lightweight and efficient HSI recovery. Moreover, we propose an entropy-based Test-Time Adaptation strategy to improve robustness in real-world scenarios. Extensive experiments demonstrate that our method, MCGA, achieves state-of-the-art performance. The code and models will be released at https://github.com/Fibonaccirabbit/MCGA", "AI": {"tldr": "MCGA\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5148\u5b66\u4e60\u5149\u8c31\u6a21\u5f0f\u518d\u4f30\u8ba1\u6620\u5c04\uff0c\u89e3\u51b3\u4e86RGB\u5230HSI\u91cd\u5efa\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u76f4\u63a5\u5b66\u4e60RGB\u5230HSI\u7684\u6620\u5c04\uff0c\u5ffd\u7565\u4e86\u4ece\u4f4e\u7ef4\u5230\u9ad8\u7ef4\u4fe1\u606f\u8f6c\u6362\u7684\u56fa\u6709\u6311\u6218\u3002", "method": "MCGA\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u591a\u5c3a\u5ea6VQ-VAE\u5b66\u4e60\u5149\u8c31\u6a21\u5f0f\uff0c\u63d0\u53d6\u6df7\u5408\u7801\u672c\uff08MoC\uff09\uff1b2\uff09\u901a\u8fc7\u67e5\u8be2MoC\u7279\u5f81\u4f18\u5316RGB\u5230HSI\u6620\u5c04\u3002\u5f15\u5165\u7070\u5ea6\u611f\u77e5\u6ce8\u610f\u529b\u548c\u91cf\u5316\u81ea\u6ce8\u610f\u529b\u4ee5\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMCGA\u5728HSI\u91cd\u5efa\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "MCGA\u901a\u8fc7\u7ed3\u5408\u5148\u9a8c\u77e5\u8bc6\u548c\u7269\u7406\u9a71\u52a8\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u8f7b\u91cf\u9ad8\u6548\u7684HSI\u91cd\u5efa\u3002"}}
{"id": "2507.09910", "pdf": "https://arxiv.org/pdf/2507.09910", "abs": "https://arxiv.org/abs/2507.09910", "authors": ["Yadong Qu", "Shancheng Fang", "Yuxin Wang", "Xiaorui Wang", "Zhineng Chen", "Hongtao Xie", "Yongdong Zhang"], "title": "IGD: Instructional Graphic Design with Multimodal Layer Generation", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "Graphic design visually conveys information and data by creating and combining text, images and graphics. Two-stage methods that rely primarily on layout generation lack creativity and intelligence, making graphic design still labor-intensive. Existing diffusion-based methods generate non-editable graphic design files at image level with poor legibility in visual text rendering, which prevents them from achieving satisfactory and practical automated graphic design. In this paper, we propose Instructional Graphic Designer (IGD) to swiftly generate multimodal layers with editable flexibility with only natural language instructions. IGD adopts a new paradigm that leverages parametric rendering and image asset generation. First, we develop a design platform and establish a standardized format for multi-scenario design files, thus laying the foundation for scaling up data. Second, IGD utilizes the multimodal understanding and reasoning capabilities of MLLM to accomplish attribute prediction, sequencing and layout of layers. It also employs a diffusion model to generate image content for assets. By enabling end-to-end training, IGD architecturally supports scalability and extensibility in complex graphic design tasks. The superior experimental results demonstrate that IGD offers a new solution for graphic design.", "AI": {"tldr": "IGD\u662f\u4e00\u79cd\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5feb\u901f\u751f\u6210\u53ef\u7f16\u8f91\u591a\u6a21\u6001\u5c42\u7684\u56fe\u5f62\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u521b\u9020\u529b\u548c\u975e\u53ef\u7f16\u8f91\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u56fe\u5f62\u8bbe\u8ba1\u65b9\u6cd5\u4f9d\u8d56\u5e03\u5c40\u751f\u6210\u6216\u751f\u6210\u4e0d\u53ef\u7f16\u8f91\u7684\u56fe\u50cf\u6587\u4ef6\uff0c\u7f3a\u4e4f\u667a\u80fd\u5316\u548c\u5b9e\u7528\u6027\u3002", "method": "IGD\u7ed3\u5408\u53c2\u6570\u5316\u6e32\u67d3\u548c\u56fe\u50cf\u8d44\u4ea7\u751f\u6210\uff0c\u5229\u7528MLLM\u7684\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\u9884\u6d4b\u5c5e\u6027\u3001\u6392\u5e8f\u548c\u5e03\u5c40\uff0c\u5e76\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u56fe\u50cf\u5185\u5bb9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cIGD\u5728\u590d\u6742\u56fe\u5f62\u8bbe\u8ba1\u4efb\u52a1\u4e2d\u5177\u6709\u4f18\u8d8a\u6027\u3002", "conclusion": "IGD\u4e3a\u56fe\u5f62\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u53ef\u6269\u5c55\u6027\u548c\u53ef\u7f16\u8f91\u6027\u3002"}}
{"id": "2507.09915", "pdf": "https://arxiv.org/pdf/2507.09915", "abs": "https://arxiv.org/abs/2507.09915", "authors": ["Siyue Yao", "Mingjie Sun", "Eng Gee Lim", "Ran Yi", "Baojiang Zhong", "Moncef Gabbouj"], "title": "Crucial-Diff: A Unified Diffusion Model for Crucial Image and Annotation Synthesis in Data-scarce Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "The scarcity of data in various scenarios, such as medical, industry and autonomous driving, leads to model overfitting and dataset imbalance, thus hindering effective detection and segmentation performance. Existing studies employ the generative models to synthesize more training samples to mitigate data scarcity. However, these synthetic samples are repetitive or simplistic and fail to provide \"crucial information\" that targets the downstream model's weaknesses. Additionally, these methods typically require separate training for different objects, leading to computational inefficiencies. To address these issues, we propose Crucial-Diff, a domain-agnostic framework designed to synthesize crucial samples. Our method integrates two key modules. The Scene Agnostic Feature Extractor (SAFE) utilizes a unified feature extractor to capture target information. The Weakness Aware Sample Miner (WASM) generates hard-to-detect samples using feedback from the detection results of downstream model, which is then fused with the output of SAFE module. Together, our Crucial-Diff framework generates diverse, high-quality training data, achieving a pixel-level AP of 83.63% and an F1-MAX of 78.12% on MVTec. On polyp dataset, Crucial-Diff reaches an mIoU of 81.64% and an mDice of 87.69%. Code will be released after acceptance.", "AI": {"tldr": "Crucial-Diff\u662f\u4e00\u4e2a\u9886\u57df\u65e0\u5173\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5173\u952e\u6837\u672c\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u5347\u68c0\u6d4b\u548c\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u6570\u636e\u7a00\u7f3a\u5bfc\u81f4\u6a21\u578b\u8fc7\u62df\u5408\u548c\u6570\u636e\u96c6\u4e0d\u5e73\u8861\uff0c\u73b0\u6709\u751f\u6210\u65b9\u6cd5\u751f\u6210\u7684\u6837\u672c\u91cd\u590d\u6216\u7b80\u5355\uff0c\u65e0\u6cd5\u9488\u5bf9\u4e0b\u6e38\u6a21\u578b\u7684\u5f31\u70b9\u63d0\u4f9b\u5173\u952e\u4fe1\u606f\u3002", "method": "\u63d0\u51faCrucial-Diff\u6846\u67b6\uff0c\u5305\u542bSAFE\u6a21\u5757\uff08\u7edf\u4e00\u7279\u5f81\u63d0\u53d6\uff09\u548cWASM\u6a21\u5757\uff08\u57fa\u4e8e\u4e0b\u6e38\u6a21\u578b\u53cd\u9988\u751f\u6210\u96be\u68c0\u6d4b\u6837\u672c\uff09\u3002", "result": "\u5728MVTec\u4e0a\u8fbe\u523083.63%\u7684AP\u548c78.12%\u7684F1-MAX\uff1b\u5728\u606f\u8089\u6570\u636e\u96c6\u4e0a\u8fbe\u523081.64%\u7684mIoU\u548c87.69%\u7684mDice\u3002", "conclusion": "Crucial-Diff\u80fd\u751f\u6210\u591a\u6837\u4e14\u9ad8\u8d28\u91cf\u7684\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2507.09953", "pdf": "https://arxiv.org/pdf/2507.09953", "abs": "https://arxiv.org/abs/2507.09953", "authors": ["Zifei Wang", "Zian Mao", "Xiaoya He", "Xi Huang", "Haoran Zhang", "Chun Cheng", "Shufen Chu", "Tingzheng Hou", "Xiaoqin Zeng", "Yujun Xie"], "title": "4D-MISR: A unified model for low-dose super-resolution imaging via feature fusion", "categories": ["cs.CV"], "comment": null, "summary": "While electron microscopy offers crucial atomic-resolution insights into structure-property relationships, radiation damage severely limits its use on beam-sensitive materials like proteins and 2D materials. To overcome this challenge, we push beyond the electron dose limits of conventional electron microscopy by adapting principles from multi-image super-resolution (MISR) that have been widely used in remote sensing. Our method fuses multiple low-resolution, sub-pixel-shifted views and enhances the reconstruction with a convolutional neural network (CNN) that integrates features from synthetic, multi-angle observations. We developed a dual-path, attention-guided network for 4D-STEM that achieves atomic-scale super-resolution from ultra-low-dose data. This provides robust atomic-scale visualization across amorphous, semi-crystalline, and crystalline beam-sensitive specimens. Systematic evaluations on representative materials demonstrate comparable spatial resolution to conventional ptychography under ultra-low-dose conditions. Our work expands the capabilities of 4D-STEM, offering a new and generalizable method for the structural analysis of radiation-vulnerable materials.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08MISR\uff09\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u8d85\u4f4e\u5242\u91cf\u6761\u4ef6\u4e0b\u5b9e\u73b0\u539f\u5b50\u7ea7\u5206\u8fa8\u7387\uff0c\u9002\u7528\u4e8e\u8f90\u5c04\u654f\u611f\u6750\u6599\u7684\u7535\u5b50\u663e\u5fae\u955c\u6210\u50cf\u3002", "motivation": "\u7535\u5b50\u663e\u5fae\u955c\u5728\u8f90\u5c04\u654f\u611f\u6750\u6599\uff08\u5982\u86cb\u767d\u8d28\u548c\u4e8c\u7ef4\u6750\u6599\uff09\u4e2d\u7684\u5e94\u7528\u53d7\u5230\u8f90\u5c04\u635f\u4f24\u7684\u9650\u5236\uff0c\u9700\u8981\u7a81\u7834\u4f20\u7edf\u7535\u5b50\u663e\u5fae\u955c\u7684\u5242\u91cf\u9650\u5236\u3002", "method": "\u7ed3\u5408\u591a\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08MISR\uff09\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u53cc\u8def\u5f84\u3001\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u7f51\u7edc\uff0c\u7528\u4e8e4D-STEM\uff0c\u5b9e\u73b0\u8d85\u4f4e\u5242\u91cf\u4e0b\u7684\u539f\u5b50\u7ea7\u8d85\u5206\u8fa8\u7387\u6210\u50cf\u3002", "result": "\u5728\u8d85\u4f4e\u5242\u91cf\u6761\u4ef6\u4e0b\uff0c\u8be5\u65b9\u6cd5\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\u4e0e\u4f20\u7edfptychography\u76f8\u5f53\uff0c\u9002\u7528\u4e8e\u975e\u6676\u3001\u534a\u6676\u548c\u6676\u6001\u8f90\u5c04\u654f\u611f\u6750\u6599\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6269\u5c55\u4e864D-STEM\u7684\u80fd\u529b\uff0c\u4e3a\u8f90\u5c04\u654f\u611f\u6750\u6599\u7684\u7ed3\u6784\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2507.09984", "pdf": "https://arxiv.org/pdf/2507.09984", "abs": "https://arxiv.org/abs/2507.09984", "authors": ["Junho Lee", "Jeongwoo Shin", "Hyungwook Choi", "Joonseok Lee"], "title": "Latent Diffusion Models with Masked AutoEncoders", "categories": ["cs.CV"], "comment": null, "summary": "In spite of remarkable potential of the Latent Diffusion Models (LDMs) in image generation, the desired properties and optimal design of the autoencoders have been underexplored. In this work, we analyze the role of autoencoders in LDMs and identify three key properties: latent smoothness, perceptual compression quality, and reconstruction quality. We demonstrate that existing autoencoders fail to simultaneously satisfy all three properties, and propose Variational Masked AutoEncoders (VMAEs), taking advantage of the hierarchical features maintained by Masked AutoEncoder. We integrate VMAEs into the LDM framework, introducing Latent Diffusion Models with Masked AutoEncoders (LDMAEs). Through comprehensive experiments, we demonstrate significantly enhanced image generation quality and computational efficiency.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDMs\uff09\u4e2d\u81ea\u7f16\u7801\u5668\u7684\u5173\u952e\u5c5e\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53d8\u5206\u63a9\u7801\u81ea\u7f16\u7801\u5668\uff08VMAEs\uff09\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u7684\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5c3d\u7ba1\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u81ea\u7f16\u7801\u5668\u7684\u7406\u60f3\u5c5e\u6027\u548c\u6700\u4f18\u8bbe\u8ba1\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u5206\u6790\u81ea\u7f16\u7801\u5668\u5728LDMs\u4e2d\u7684\u4f5c\u7528\uff0c\u8bc6\u522b\u4e86\u4e09\u4e2a\u5173\u952e\u5c5e\u6027\uff08\u6f5c\u5728\u5e73\u6ed1\u6027\u3001\u611f\u77e5\u538b\u7f29\u8d28\u91cf\u548c\u91cd\u5efa\u8d28\u91cf\uff09\uff0c\u5e76\u63d0\u51faVMAEs\u4ee5\u6ee1\u8db3\u8fd9\u4e9b\u5c5e\u6027\u3002\u5c06VMAEs\u96c6\u6210\u5230LDM\u6846\u67b6\u4e2d\uff0c\u5f62\u6210LDMAEs\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLDMAEs\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "VMAEs\u662f\u4e00\u79cd\u6709\u6548\u7684\u81ea\u7f16\u7801\u5668\u8bbe\u8ba1\uff0c\u80fd\u591f\u540c\u65f6\u6ee1\u8db3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u5173\u952e\u9700\u6c42\u3002"}}
{"id": "2507.09993", "pdf": "https://arxiv.org/pdf/2507.09993", "abs": "https://arxiv.org/abs/2507.09993", "authors": ["Yixun Zhang", "Lizhi Wang", "Junjun Zhao", "Wending Zhao", "Feng Zhou", "Yonghao Dang", "Jianqin Yin"], "title": "3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for Autonomous Driving", "categories": ["cs.CV"], "comment": "Submitted to WACV 2026", "summary": "Camera-based object detection systems play a vital role in autonomous driving, yet they remain vulnerable to adversarial threats in real-world environments. While existing 2D and 3D physical attacks typically optimize texture, they often struggle to balance physical realism and attack robustness. In this work, we propose 3D Gaussian-based Adversarial Attack (3DGAA), a novel adversarial object generation framework that leverages the full 14-dimensional parameterization of 3D Gaussian Splatting (3DGS) to jointly optimize geometry and appearance in physically realizable ways. Unlike prior works that rely on patches or texture, 3DGAA jointly perturbs both geometric attributes (shape, scale, rotation) and appearance attributes (color, opacity) to produce physically realistic and transferable adversarial objects. We further introduce a physical filtering module to preserve geometric fidelity, and a physical augmentation module to simulate complex physical scenarios, thus enhancing attack generalization under real-world conditions. We evaluate 3DGAA on both virtual benchmarks and physical-world setups using miniature vehicle models. Experimental results show that 3DGAA achieves to reduce the detection mAP from 87.21% to 7.38%, significantly outperforming existing 3D physical attacks. Moreover, our method maintains high transferability across different physical conditions, demonstrating a new state-of-the-art in physically realizable adversarial attacks. These results validate 3DGAA as a practical attack framework for evaluating the safety of perception systems in autonomous driving.", "AI": {"tldr": "3DGAA\u662f\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u5206\u5e03\u7684\u5bf9\u6297\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u51e0\u4f55\u548c\u5916\u89c2\u5c5e\u6027\uff0c\u751f\u6210\u7269\u7406\u4e0a\u53ef\u5b9e\u73b0\u4e14\u9c81\u68d2\u7684\u5bf9\u6297\u5bf9\u8c61\uff0c\u663e\u8457\u964d\u4f4e\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u67092D\u548c3D\u7269\u7406\u653b\u51fb\u65b9\u6cd5\u5728\u5e73\u8861\u7269\u7406\u771f\u5b9e\u6027\u548c\u653b\u51fb\u9c81\u68d2\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u7684\u5bf9\u6297\u653b\u51fb\u6846\u67b6\u3002", "method": "\u5229\u75283D\u9ad8\u65af\u5206\u5e03\u768414\u7ef4\u53c2\u6570\u5316\uff0c\u8054\u5408\u4f18\u5316\u51e0\u4f55\uff08\u5f62\u72b6\u3001\u5c3a\u5ea6\u3001\u65cb\u8f6c\uff09\u548c\u5916\u89c2\uff08\u989c\u8272\u3001\u4e0d\u900f\u660e\u5ea6\uff09\u5c5e\u6027\uff0c\u5e76\u5f15\u5165\u7269\u7406\u8fc7\u6ee4\u548c\u589e\u5f3a\u6a21\u5757\u4ee5\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u865a\u62df\u548c\u7269\u7406\u5b9e\u9a8c\u4e2d\uff0c3DGAA\u5c06\u68c0\u6d4bmAP\u4ece87.21%\u964d\u81f37.38%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u9ad8\u8fc1\u79fb\u6027\u3002", "conclusion": "3DGAA\u4e3a\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u5bf9\u6297\u653b\u51fb\u6846\u67b6\u3002"}}
{"id": "2507.10217", "pdf": "https://arxiv.org/pdf/2507.10217", "abs": "https://arxiv.org/abs/2507.10217", "authors": ["Jeongho Kim", "Sunghyun Park", "Hyoungwoo Park", "Sungrack Yun", "Jaegul Choo", "Seokeon Cho"], "title": "From Wardrobe to Canvas: Wardrobe Polyptych LoRA for Part-level Controllable Human Image Generation", "categories": ["cs.CV"], "comment": "10 pages, 8 figures", "summary": "Recent diffusion models achieve personalization by learning specific subjects, allowing learned attributes to be integrated into generated images. However, personalized human image generation remains challenging due to the need for precise and consistent attribute preservation (e.g., identity, clothing details). Existing subject-driven image generation methods often require either (1) inference-time fine-tuning with few images for each new subject or (2) large-scale dataset training for generalization. Both approaches are computationally expensive and impractical for real-time applications. To address these limitations, we present Wardrobe Polyptych LoRA, a novel part-level controllable model for personalized human image generation. By training only LoRA layers, our method removes the computational burden at inference while ensuring high-fidelity synthesis of unseen subjects. Our key idea is to condition the generation on the subject's wardrobe and leverage spatial references to reduce information loss, thereby improving fidelity and consistency. Additionally, we introduce a selective subject region loss, which encourages the model to disregard some of reference images during training. Our loss ensures that generated images better align with text prompts while maintaining subject integrity. Notably, our Wardrobe Polyptych LoRA requires no additional parameters at the inference stage and performs generation using a single model trained on a few training samples. We construct a new dataset and benchmark tailored for personalized human image generation. Extensive experiments show that our approach significantly outperforms existing techniques in fidelity and consistency, enabling realistic and identity-preserving full-body synthesis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWardrobe Polyptych LoRA\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u90e8\u5206\u7ea7\u53ef\u63a7\u6a21\u578b\u5b9e\u73b0\u4e2a\u6027\u5316\u4eba\u7c7b\u56fe\u50cf\u751f\u6210\uff0c\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d1f\u62c5\u5e76\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4e2a\u6027\u5316\u4eba\u7c7b\u56fe\u50cf\u751f\u6210\u4e2d\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u5b9e\u65f6\u6027\u5dee\u7684\u95ee\u9898\uff0c\u9700\u8981\u89e3\u51b3\u7cbe\u786e\u5c5e\u6027\u4fdd\u7559\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3LoRA\u5c42\uff0c\u7ed3\u5408\u7a7a\u95f4\u53c2\u8003\u548c\u9009\u62e9\u6027\u4e3b\u9898\u533a\u57df\u635f\u5931\uff0c\u51cf\u5c11\u4fe1\u606f\u4e22\u5931\u5e76\u63d0\u9ad8\u751f\u6210\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u771f\u5ea6\u548c\u4e00\u81f4\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u652f\u6301\u5c11\u6837\u672c\u8bad\u7ec3\u548c\u5355\u6a21\u578b\u63a8\u7406\u3002", "conclusion": "Wardrobe Polyptych LoRA\u4e3a\u4e2a\u6027\u5316\u4eba\u7c7b\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10239", "pdf": "https://arxiv.org/pdf/2507.10239", "abs": "https://arxiv.org/abs/2507.10239", "authors": ["Ben Hamscher", "Edgar Heinert", "Annika M\u00fctze", "Kira Maag", "Matthias Rottmann"], "title": "Transferring Styles for Reduced Texture Bias and Improved Robustness in Semantic Segmentation Networks", "categories": ["cs.CV"], "comment": "accepted at ECAI 2025", "summary": "Recent research has investigated the shape and texture biases of deep neural networks (DNNs) in image classification which influence their generalization capabilities and robustness. It has been shown that, in comparison to regular DNN training, training with stylized images reduces texture biases in image classification and improves robustness with respect to image corruptions. In an effort to advance this line of research, we examine whether style transfer can likewise deliver these two effects in semantic segmentation. To this end, we perform style transfer with style varying across artificial image areas. Those random areas are formed by a chosen number of Voronoi cells. The resulting style-transferred data is then used to train semantic segmentation DNNs with the objective of reducing their dependence on texture cues while enhancing their reliance on shape-based features. In our experiments, it turns out that in semantic segmentation, style transfer augmentation reduces texture bias and strongly increases robustness with respect to common image corruptions as well as adversarial attacks. These observations hold for convolutional neural networks and transformer architectures on the Cityscapes dataset as well as on PASCAL Context, showing the generality of the proposed method.", "AI": {"tldr": "\u901a\u8fc7\u98ce\u683c\u8fc1\u79fb\u589e\u5f3a\u8bed\u4e49\u5206\u5272\u4efb\u52a1\uff0c\u51cf\u5c11\u7eb9\u7406\u504f\u89c1\u5e76\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "motivation": "\u7814\u7a76\u98ce\u683c\u8fc1\u79fb\u662f\u5426\u80fd\u51cf\u5c11\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u7eb9\u7406\u504f\u89c1\u5e76\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eVoronoi\u7ec6\u80de\u7684\u968f\u673a\u533a\u57df\u98ce\u683c\u8fc1\u79fb\u6570\u636e\u8bad\u7ec3\u8bed\u4e49\u5206\u5272\u6a21\u578b\u3002", "result": "\u98ce\u683c\u8fc1\u79fb\u51cf\u5c11\u4e86\u7eb9\u7406\u504f\u89c1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u56fe\u50cf\u635f\u574f\u548c\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u98ce\u683c\u8fc1\u79fb\u5728\u8bed\u4e49\u5206\u5272\u4e2d\u6709\u6548\u51cf\u5c11\u7eb9\u7406\u504f\u89c1\u5e76\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u67b6\u6784\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2507.10293", "pdf": "https://arxiv.org/pdf/2507.10293", "abs": "https://arxiv.org/abs/2507.10293", "authors": ["Wenkang Han", "Wang Lin", "Yiyun Zhou", "Qi Liu", "Shulei Wang", "Chang Yao", "Jingyuan Chen"], "title": "Show and Polish: Reference-Guided Identity Preservation in Face Video Restoration", "categories": ["cs.CV"], "comment": "Accepted by MM 2025", "summary": "Face Video Restoration (FVR) aims to recover high-quality face videos from degraded versions. Traditional methods struggle to preserve fine-grained, identity-specific features when degradation is severe, often producing average-looking faces that lack individual characteristics. To address these challenges, we introduce IP-FVR, a novel method that leverages a high-quality reference face image as a visual prompt to provide identity conditioning during the denoising process. IP-FVR incorporates semantically rich identity information from the reference image using decoupled cross-attention mechanisms, ensuring detailed and identity consistent results. For intra-clip identity drift (within 24 frames), we introduce an identity-preserving feedback learning method that combines cosine similarity-based reward signals with suffix-weighted temporal aggregation. This approach effectively minimizes drift within sequences of frames. For inter-clip identity drift, we develop an exponential blending strategy that aligns identities across clips by iteratively blending frames from previous clips during the denoising process. This method ensures consistent identity representation across different clips. Additionally, we enhance the restoration process with a multi-stream negative prompt, guiding the model's attention to relevant facial attributes and minimizing the generation of low-quality or incorrect features. Extensive experiments on both synthetic and real-world datasets demonstrate that IP-FVR outperforms existing methods in both quality and identity preservation, showcasing its substantial potential for practical applications in face video restoration.", "AI": {"tldr": "IP-FVR\u662f\u4e00\u79cd\u65b0\u9896\u7684\u4eba\u8138\u89c6\u9891\u4fee\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u9ad8\u8d28\u91cf\u53c2\u8003\u56fe\u50cf\u548c\u8eab\u4efd\u4fdd\u6301\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u4e25\u91cd\u9000\u5316\u60c5\u51b5\u4e0b\u96be\u4ee5\u4fdd\u7559\u8eab\u4efd\u7279\u5f81\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u4eba\u8138\u89c6\u9891\u4fee\u590d\u65b9\u6cd5\u5728\u4e25\u91cd\u9000\u5316\u65f6\u96be\u4ee5\u4fdd\u7559\u7ec6\u7c92\u5ea6\u7684\u8eab\u4efd\u7279\u5f81\uff0c\u5bfc\u81f4\u7ed3\u679c\u7f3a\u4e4f\u4e2a\u4f53\u7279\u6027\u3002", "method": "IP-FVR\u5229\u7528\u53c2\u8003\u56fe\u50cf\u63d0\u4f9b\u8eab\u4efd\u6761\u4ef6\uff0c\u91c7\u7528\u89e3\u8026\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u548c\u53cd\u9988\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5956\u52b1\u4fe1\u53f7\u548c\u65f6\u95f4\u805a\u5408\uff0c\u51cf\u5c11\u5e27\u5185\u548c\u5e27\u95f4\u8eab\u4efd\u6f02\u79fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cIP-FVR\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4fee\u590d\u8d28\u91cf\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u3002", "conclusion": "IP-FVR\u5728\u8eab\u4efd\u4fdd\u6301\u548c\u89c6\u9891\u4fee\u590d\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.10318", "pdf": "https://arxiv.org/pdf/2507.10318", "abs": "https://arxiv.org/abs/2507.10318", "authors": ["Yuhan Liu", "Jingwen Fu", "Yang Wu", "Kangyi Wu", "Pengna Li", "Jiayi Wu", "Sanping Zhou", "Jingmin Xin"], "title": "Mind the Gap: Aligning Vision Foundation Models to Image Feature Matching", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Leveraging the vision foundation models has emerged as a mainstream paradigm that improves the performance of image feature matching. However, previous works have ignored the misalignment when introducing the foundation models into feature matching. The misalignment arises from the discrepancy between the foundation models focusing on single-image understanding and the cross-image understanding requirement of feature matching. Specifically, 1) the embeddings derived from commonly used foundation models exhibit discrepancies with the optimal embeddings required for feature matching; 2) lacking an effective mechanism to leverage the single-image understanding ability into cross-image understanding. A significant consequence of the misalignment is they struggle when addressing multi-instance feature matching problems. To address this, we introduce a simple but effective framework, called IMD (Image feature Matching with a pre-trained Diffusion model) with two parts: 1) Unlike the dominant solutions employing contrastive-learning based foundation models that emphasize global semantics, we integrate the generative-based diffusion models to effectively capture instance-level details. 2) We leverage the prompt mechanism in generative model as a natural tunnel, propose a novel cross-image interaction prompting module to facilitate bidirectional information interaction between image pairs. To more accurately measure the misalignment, we propose a new benchmark called IMIM, which focuses on multi-instance scenarios. Our proposed IMD establishes a new state-of-the-art in commonly evaluated benchmarks, and the superior improvement 12% in IMIM indicates our method efficiently mitigates the misalignment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6IMD\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u89e3\u51b3\u56fe\u50cf\u7279\u5f81\u5339\u914d\u4e2d\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u5b9e\u4f8b\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5f15\u5165\u57fa\u7840\u6a21\u578b\u65f6\u5ffd\u89c6\u4e86\u5355\u56fe\u50cf\u7406\u89e3\u4e0e\u8de8\u56fe\u50cf\u7406\u89e3\u9700\u6c42\u4e4b\u95f4\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u5bfc\u81f4\u591a\u5b9e\u4f8b\u7279\u5f81\u5339\u914d\u6548\u679c\u4e0d\u4f73\u3002", "method": "IMD\u6846\u67b6\u5305\u542b\u4e24\u90e8\u5206\uff1a1) \u4f7f\u7528\u751f\u6210\u5f0f\u6269\u6563\u6a21\u578b\u6355\u6349\u5b9e\u4f8b\u7ea7\u7ec6\u8282\uff1b2) \u63d0\u51fa\u8de8\u56fe\u50cf\u4ea4\u4e92\u63d0\u793a\u6a21\u5757\u4fc3\u8fdb\u56fe\u50cf\u5bf9\u95f4\u7684\u53cc\u5411\u4fe1\u606f\u4ea4\u4e92\u3002", "result": "IMD\u5728\u5e38\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0SOTA\uff0c\u5e76\u5728\u591a\u5b9e\u4f8b\u57fa\u51c6IMIM\u4e0a\u63d0\u534712%\u3002", "conclusion": "IMD\u6709\u6548\u89e3\u51b3\u4e86\u57fa\u7840\u6a21\u578b\u5728\u7279\u5f81\u5339\u914d\u4e2d\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u5b9e\u4f8b\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.10340", "pdf": "https://arxiv.org/pdf/2507.10340", "abs": "https://arxiv.org/abs/2507.10340", "authors": ["Hongjae Lee", "Myungjun Son", "Dongjea Kang", "Seung-Won Jung"], "title": "Text Embedding Knows How to Quantize Text-Guided Diffusion Models", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "Despite the success of diffusion models in image generation tasks such as text-to-image, the enormous computational complexity of diffusion models limits their use in resource-constrained environments. To address this, network quantization has emerged as a promising solution for designing efficient diffusion models. However, existing diffusion model quantization methods do not consider input conditions, such as text prompts, as an essential source of information for quantization. In this paper, we propose a novel quantization method dubbed Quantization of Language-to-Image diffusion models using text Prompts (QLIP). QLIP leverages text prompts to guide the selection of bit precision for every layer at each time step. In addition, QLIP can be seamlessly integrated into existing quantization methods to enhance quantization efficiency. Our extensive experiments demonstrate the effectiveness of QLIP in reducing computational complexity and improving the quality of the generated images across various datasets.", "AI": {"tldr": "QLIP\u662f\u4e00\u79cd\u65b0\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u6587\u672c\u63d0\u793a\u6307\u5bfc\u6269\u6563\u6a21\u578b\u7684\u9010\u5c42\u6bd4\u7279\u7cbe\u5ea6\u9009\u62e9\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u63d0\u5347\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u8f93\u5165\u6761\u4ef6\uff08\u5982\u6587\u672c\u63d0\u793a\uff09\u7684\u4fe1\u606f\u3002", "method": "\u63d0\u51faQLIP\u65b9\u6cd5\uff0c\u901a\u8fc7\u6587\u672c\u63d0\u793a\u52a8\u6001\u9009\u62e9\u6bcf\u5c42\u548c\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u6bd4\u7279\u7cbe\u5ea6\uff0c\u5e76\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cQLIP\u80fd\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "QLIP\u4e3a\u6269\u6563\u6a21\u578b\u91cf\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002"}}
{"id": "2507.10461", "pdf": "https://arxiv.org/pdf/2507.10461", "abs": "https://arxiv.org/abs/2507.10461", "authors": ["Tao Tang", "Chengxu Yang"], "title": "RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "eess.IV"], "comment": "To appear in the proceedings of the 6th International Conference on   Artificial Intelligence and Electromechanical Automation (AIEA 2025). 5   pages, 6 figures", "summary": "Pansharpening refers to the process of integrating a high resolution panchromatic (PAN) image with a lower resolution multispectral (MS) image to generate a fused product, which is pivotal in remote sensing. Despite the effectiveness of CNNs in addressing this challenge, they are inherently constrained by the uniform application of convolutional kernels across all spatial positions, overlooking local content variations. To overcome this issue, we introduce RAPNet, a new architecture that leverages content-adaptive convolution. At its core, RAPNet employs the Receptive-field Adaptive Pansharpening Convolution (RAPConv), designed to produce spatially adaptive kernels responsive to local feature context, thereby enhancing the precision of spatial detail extraction. Additionally, the network integrates the Pansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an attention mechanism to achieve an optimal balance between spatial detail enhancement and spectral fidelity. Comprehensive evaluations on publicly available datasets confirm that RAPNet delivers superior performance compared to existing approaches, as demonstrated by both quantitative metrics and qualitative assessments. Ablation analyses further substantiate the effectiveness of the proposed adaptive components.", "AI": {"tldr": "RAPNet\u662f\u4e00\u79cd\u65b0\u7684\u9065\u611f\u56fe\u50cf\u878d\u5408\u67b6\u6784\uff0c\u901a\u8fc7\u5185\u5bb9\u81ea\u9002\u5e94\u5377\u79ef\u548c\u52a8\u6001\u7279\u5f81\u878d\u5408\u6a21\u5757\u63d0\u5347\u7a7a\u95f4\u7ec6\u8282\u63d0\u53d6\u548c\u5149\u8c31\u4fdd\u771f\u5ea6\u3002", "motivation": "\u4f20\u7edfCNN\u5728\u9065\u611f\u56fe\u50cf\u878d\u5408\u4e2d\u56e0\u5377\u79ef\u6838\u7684\u5747\u5300\u5e94\u7528\u800c\u5ffd\u7565\u5c40\u90e8\u5185\u5bb9\u53d8\u5316\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002", "method": "\u63d0\u51faRAPNet\uff0c\u91c7\u7528Receptive-field Adaptive Pansharpening Convolution (RAPConv)\u548cPansharpening Dynamic Feature Fusion (PAN-DFF)\u6a21\u5757\uff0c\u5b9e\u73b0\u7a7a\u95f4\u81ea\u9002\u5e94\u5377\u79ef\u548c\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cRAPNet\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RAPNet\u901a\u8fc7\u81ea\u9002\u5e94\u7ec4\u4ef6\u663e\u8457\u63d0\u5347\u4e86\u9065\u611f\u56fe\u50cf\u878d\u5408\u7684\u6027\u80fd\u3002"}}
{"id": "2507.10470", "pdf": "https://arxiv.org/pdf/2507.10470", "abs": "https://arxiv.org/abs/2507.10470", "authors": ["Zhicun Yin", "Junjie Chen", "Ming Liu", "Zhixin Wang", "Fan Li", "Renjing Pei", "Xiaoming Li", "Rynson W. H. Lau", "Wangmeng Zuo"], "title": "RefSTAR: Blind Facial Image Restoration with Reference Selection, Transfer, and Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Blind facial image restoration is highly challenging due to unknown complex degradations and the sensitivity of humans to faces. Although existing methods introduce auxiliary information from generative priors or high-quality reference images, they still struggle with identity preservation problems, mainly due to improper feature introduction on detailed textures. In this paper, we focus on effectively incorporating appropriate features from high-quality reference images, presenting a novel blind facial image restoration method that considers reference selection, transfer, and reconstruction (RefSTAR). In terms of selection, we construct a reference selection (RefSel) module. For training the RefSel module, we construct a RefSel-HQ dataset through a mask generation pipeline, which contains annotating masks for 10,000 ground truth-reference pairs. As for the transfer, due to the trivial solution in vanilla cross-attention operations, a feature fusion paradigm is designed to force the features from the reference to be integrated. Finally, we propose a reference image reconstruction mechanism that further ensures the presence of reference image features in the output image. The cycle consistency loss is also redesigned in conjunction with the mask. Extensive experiments on various backbone models demonstrate superior performance, showing better identity preservation ability and reference feature transfer quality. Source code, dataset, and pre-trained models are available at https://github.com/yinzhicun/RefSTAR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRefSTAR\u7684\u76f2\u4eba\u8138\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u8003\u56fe\u50cf\u7684\u9009\u62e9\u3001\u7279\u5f81\u8f6c\u79fb\u548c\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u8eab\u4efd\u4fdd\u7559\u95ee\u9898\u3002", "motivation": "\u76f2\u4eba\u8138\u56fe\u50cf\u4fee\u590d\u56e0\u672a\u77e5\u7684\u590d\u6742\u9000\u5316\u548c\u4eba\u7c7b\u5bf9\u8138\u90e8\u7684\u654f\u611f\u6027\u800c\u6781\u5177\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8eab\u4efd\u4fdd\u7559\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faRefSTAR\u65b9\u6cd5\uff0c\u5305\u62ec\u53c2\u8003\u9009\u62e9\u6a21\u5757\uff08RefSel\uff09\u3001\u7279\u5f81\u878d\u5408\u8303\u5f0f\u53ca\u91cd\u5efa\u673a\u5236\uff0c\u5e76\u7ed3\u5408\u5faa\u73af\u4e00\u81f4\u6027\u635f\u5931\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRefSTAR\u5728\u8eab\u4efd\u4fdd\u7559\u548c\u53c2\u8003\u7279\u5f81\u8f6c\u79fb\u8d28\u91cf\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "RefSTAR\u901a\u8fc7\u6709\u6548\u5f15\u5165\u53c2\u8003\u56fe\u50cf\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76f2\u4eba\u8138\u56fe\u50cf\u4fee\u590d\u7684\u6548\u679c\u3002"}}
{"id": "2507.10547", "pdf": "https://arxiv.org/pdf/2507.10547", "abs": "https://arxiv.org/abs/2507.10547", "authors": ["Borui Zhang", "Qihang Rao", "Wenzhao Zheng", "Jie Zhou", "Jiwen Lu"], "title": "Quantize-then-Rectify: Efficient VQ-VAE Training", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Visual tokenizers are pivotal in multimodal large models, acting as bridges between continuous inputs and discrete tokens. Nevertheless, training high-compression-rate VQ-VAEs remains computationally demanding, often necessitating thousands of GPU hours. This work demonstrates that a pre-trained VAE can be efficiently transformed into a VQ-VAE by controlling quantization noise within the VAE's tolerance threshold. We present \\textbf{Quantize-then-Rectify (ReVQ)}, a framework leveraging pre-trained VAEs to enable rapid VQ-VAE training with minimal computational overhead. By integrating \\textbf{channel multi-group quantization} to enlarge codebook capacity and a \\textbf{post rectifier} to mitigate quantization errors, ReVQ compresses ImageNet images into at most 512 tokens while sustaining competitive reconstruction quality (rFID = 1.06). Significantly, ReVQ reduces training costs by over two orders of magnitude relative to state-of-the-art approaches: ReVQ finishes full training on a single NVIDIA 4090 in approximately 22 hours, whereas comparable methods require 4.5 days on 32 A100 GPUs. Experimental results show that ReVQ achieves superior efficiency-reconstruction trade-offs.", "AI": {"tldr": "ReVQ\u6846\u67b6\u901a\u8fc7\u9884\u8bad\u7ec3\u7684VAE\u5feb\u901f\u8bad\u7ec3VQ-VAE\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "motivation": "\u89e3\u51b3\u9ad8\u538b\u7f29\u7387VQ-VAE\u8bad\u7ec3\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faQuantize-then-Rectify (ReVQ)\u6846\u67b6\uff0c\u7ed3\u5408\u901a\u9053\u591a\u7ec4\u91cf\u5316\u548c\u540e\u77eb\u6b63\u5668\u3002", "result": "\u5728ImageNet\u4e0a\u538b\u7f29\u81f3512\u4e2atoken\uff0crFID=1.06\uff0c\u8bad\u7ec3\u6210\u672c\u964d\u4f4e\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "ReVQ\u5728\u6548\u7387\u548c\u91cd\u5efa\u8d28\u91cf\u95f4\u53d6\u5f97\u4f18\u8d8a\u5e73\u8861\u3002"}}
{"id": "2507.08980", "pdf": "https://arxiv.org/pdf/2507.08980", "abs": "https://arxiv.org/abs/2507.08980", "authors": ["Chenyu Wang", "Cai Zhou", "Sharut Gupta", "Zongyu Lin", "Stefanie Jegelka", "Stephen Bates", "Tommi Jaakkola"], "title": "Learning Diffusion Models with Flexible Representation Guidance", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Diffusion models can be improved with additional guidance towards more effective representations of input. Indeed, prior empirical work has already shown that aligning internal representations of the diffusion model with those of pre-trained models improves generation quality. In this paper, we present a systematic framework for incorporating representation guidance into diffusion models. We provide alternative decompositions of denoising models along with their associated training criteria, where the decompositions determine when and how the auxiliary representations are incorporated. Guided by our theoretical insights, we introduce two new strategies for enhancing representation alignment in diffusion models. First, we pair examples with target representations either derived from themselves or arisen from different synthetic modalities, and subsequently learn a joint model over the multimodal pairs. Second, we design an optimal training curriculum that balances representation learning and data generation. Our experiments across image, protein sequence, and molecule generation tasks demonstrate superior performance as well as accelerated training. In particular, on the class-conditional ImageNet $256\\times 256$ benchmark, our guidance results in $23.3$ times faster training than the original SiT-XL as well as four times speedup over the state-of-the-art method REPA. The code is available at https://github.com/ChenyuWang-Monica/REED.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u8868\u793a\u5f15\u5bfc\u6539\u8fdb\u6269\u6563\u6a21\u578b\uff0c\u5f15\u5165\u4e24\u79cd\u65b0\u7b56\u7565\u5e76\u5c55\u793a\u4e86\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u989d\u5916\u7684\u8868\u793a\u5f15\u5bfc\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u6027\u6846\u67b6\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u7b56\u7565\uff1a1\uff09\u5c06\u76ee\u6807\u8868\u793a\u4e0e\u6837\u672c\u914d\u5bf9\u5b66\u4e60\u8054\u5408\u6a21\u578b\uff1b2\uff09\u8bbe\u8ba1\u5e73\u8861\u8868\u793a\u5b66\u4e60\u548c\u6570\u636e\u751f\u6210\u7684\u6700\u4f18\u8bad\u7ec3\u8bfe\u7a0b\u3002", "result": "\u5728\u56fe\u50cf\u3001\u86cb\u767d\u8d28\u5e8f\u5217\u548c\u5206\u5b50\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0cImageNet 256\u00d7256\u4efb\u52a1\u4e0a\u8bad\u7ec3\u901f\u5ea6\u63d0\u534723.3\u500d\u3002", "conclusion": "\u8868\u793a\u5f15\u5bfc\u663e\u8457\u63d0\u5347\u6269\u6563\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\u548c\u751f\u6210\u8d28\u91cf\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.09212", "pdf": "https://arxiv.org/pdf/2507.09212", "abs": "https://arxiv.org/abs/2507.09212", "authors": ["Jonas Scholz", "Richard E. Turner"], "title": "Warm Starts Accelerate Generative Modelling", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": "10 pages, 6 figures", "summary": "Iterative generative models, like diffusion and flow-matching, create high-fidelity samples by progressively refining a noise vector into data. However, this process is notoriously slow, often requiring hundreds of function evaluations. We introduce the warm-start model, a simple, deterministic model that dramatically accelerates conditional generation by providing a better starting point. Instead of starting generation from an uninformed N(0, I) prior, our warm-start model predicts an informed prior N(mu, sigma), whose moments are conditioned on the input context. This \"warm start\" substantially reduces the distance the generative process must traverse, particularly when the conditioning information is strongly informative. On tasks like image inpainting, our method achieves results competitive with a 1000-step DDPM baseline using only 11 total function evaluations (1 for the warm start, 10 for generation). A simple conditional normalization trick makes our method compatible with any standard generative model and sampler without modification, allowing it to be combined with other efficient sampling techniques for further acceleration. Our implementation is available at https://github.com/jonas-scholz123/warm-start-model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cwarm-start model\u201d\u7684\u786e\u5b9a\u6027\u6a21\u578b\uff0c\u901a\u8fc7\u63d0\u4f9b\u66f4\u597d\u7684\u8d77\u59cb\u70b9\u52a0\u901f\u6761\u4ef6\u751f\u6210\uff0c\u663e\u8457\u51cf\u5c11\u751f\u6210\u8fc7\u7a0b\u6240\u9700\u7684\u6b65\u9aa4\u3002", "motivation": "\u4f20\u7edf\u8fed\u4ee3\u751f\u6210\u6a21\u578b\uff08\u5982\u6269\u6563\u6a21\u578b\u548c\u6d41\u5339\u914d\uff09\u751f\u6210\u9ad8\u4fdd\u771f\u6837\u672c\u9700\u8981\u6570\u767e\u6b21\u51fd\u6570\u8bc4\u4f30\uff0c\u901f\u5ea6\u6162\u3002", "method": "\u5f15\u5165warm-start\u6a21\u578b\uff0c\u9884\u6d4b\u4e00\u4e2a\u6761\u4ef6\u5316\u7684\u5148\u9a8c\u5206\u5e03N(mu, sigma)\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684N(0, I)\u5148\u9a8c\uff0c\u51cf\u5c11\u751f\u6210\u8fc7\u7a0b\u7684\u8ddd\u79bb\u3002", "result": "\u5728\u56fe\u50cf\u4fee\u590d\u7b49\u4efb\u52a1\u4e2d\uff0c\u4ec5\u970011\u6b21\u51fd\u6570\u8bc4\u4f30\u5373\u53ef\u8fbe\u5230\u4e0e1000\u6b65DDPM\u57fa\u7ebf\u7ade\u4e89\u7684\u7ed3\u679c\u3002", "conclusion": "warm-start\u6a21\u578b\u7b80\u5355\u9ad8\u6548\uff0c\u53ef\u4e0e\u4efb\u4f55\u6807\u51c6\u751f\u6210\u6a21\u578b\u7ed3\u5408\uff0c\u65e0\u9700\u4fee\u6539\uff0c\u8fdb\u4e00\u6b65\u52a0\u901f\u751f\u6210\u8fc7\u7a0b\u3002"}}
{"id": "2507.09227", "pdf": "https://arxiv.org/pdf/2507.09227", "abs": "https://arxiv.org/abs/2507.09227", "authors": ["Sanyam Jain", "Bruna Neves de Freitas", "Andreas Basse-OConnor", "Alexandros Iosifidis", "Ruben Pauwels"], "title": "PanoDiff-SR: Synthesizing Dental Panoramic Radiographs using Diffusion and Super-resolution", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "There has been increasing interest in the generation of high-quality, realistic synthetic medical images in recent years. Such synthetic datasets can mitigate the scarcity of public datasets for artificial intelligence research, and can also be used for educational purposes. In this paper, we propose a combination of diffusion-based generation (PanoDiff) and Super-Resolution (SR) for generating synthetic dental panoramic radiographs (PRs). The former generates a low-resolution (LR) seed of a PR (256 X 128) which is then processed by the SR model to yield a high-resolution (HR) PR of size 1024 X 512. For SR, we propose a state-of-the-art transformer that learns local-global relationships, resulting in sharper edges and textures. Experimental results demonstrate a Frechet inception distance score of 40.69 between 7243 real and synthetic images (in HR). Inception scores were 2.55, 2.30, 2.90 and 2.98 for real HR, synthetic HR, real LR and synthetic LR images, respectively. Among a diverse group of six clinical experts, all evaluating a mixture of 100 synthetic and 100 real PRs in a time-limited observation, the average accuracy in distinguishing real from synthetic images was 68.5% (with 50% corresponding to random guessing).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6269\u6563\u751f\u6210\uff08PanoDiff\uff09\u548c\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u7259\u79d1\u5168\u666fX\u5149\u7247\uff08PRs\uff09\uff0c\u4ee5\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u5bf9\u9ad8\u8d28\u91cf\u5408\u6210\u533b\u5b66\u56fe\u50cf\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4ee5\u7f13\u89e3\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u4e2d\u516c\u5f00\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u7528\u4e8e\u6559\u80b2\u76ee\u7684\u3002", "method": "\u91c7\u7528\u6269\u6563\u751f\u6210\u6a21\u578b\u751f\u6210\u4f4e\u5206\u8fa8\u7387\uff08256 X 128\uff09PRs\u79cd\u5b50\uff0c\u518d\u901a\u8fc7\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u63d0\u5347\u81f3\u9ad8\u5206\u8fa8\u7387\uff081024 X 512\uff09\u3002SR\u6a21\u578b\u91c7\u7528\u5148\u8fdb\u7684transformer\uff0c\u5b66\u4e60\u5c40\u90e8-\u5168\u5c40\u5173\u7cfb\uff0c\u4ee5\u751f\u6210\u66f4\u6e05\u6670\u7684\u8fb9\u7f18\u548c\u7eb9\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u9ad8\u5206\u8fa8\u7387\u5408\u6210\u56fe\u50cf\u4e0e\u771f\u5b9e\u56fe\u50cf\u7684Frechet inception\u8ddd\u79bb\u4e3a40.69\uff0cInception\u5206\u6570\u5206\u522b\u4e3a2.55\uff08\u771f\u5b9eHR\uff09\u30012.30\uff08\u5408\u6210HR\uff09\u30012.90\uff08\u771f\u5b9eLR\uff09\u548c2.98\uff08\u5408\u6210LR\uff09\u3002\u4e34\u5e8a\u4e13\u5bb6\u533a\u5206\u771f\u5b9e\u4e0e\u5408\u6210\u56fe\u50cf\u7684\u5e73\u5747\u51c6\u786e\u7387\u4e3a68.5%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u751f\u6210\u4e86\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u7259\u79d1\u5168\u666fX\u5149\u7247\uff0c\u4e14\u5728\u4e13\u5bb6\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u63a5\u8fd1\u771f\u5b9e\u56fe\u50cf\u7684\u8d28\u91cf\u3002"}}
{"id": "2507.09608", "pdf": "https://arxiv.org/pdf/2507.09608", "abs": "https://arxiv.org/abs/2507.09608", "authors": ["Mehmet Onurcan Kaya", "Figen S. Oktem"], "title": "prNet: Data-Driven Phase Retrieval via Stochastic Refinement", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "We propose a novel framework for phase retrieval that leverages Langevin dynamics to enable efficient posterior sampling, yielding reconstructions that explicitly balance distortion and perceptual quality. Unlike conventional approaches that prioritize pixel-wise accuracy, our method navigates the perception-distortion tradeoff through a principled combination of stochastic sampling, learned denoising, and model-based updates. The framework comprises three variants of increasing complexity, integrating theoretically grounded Langevin inference, adaptive noise schedule learning, parallel reconstruction sampling, and warm-start initialization from classical solvers. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple benchmarks, both in terms of fidelity and perceptual quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLangevin\u52a8\u529b\u5b66\u7684\u65b0\u578b\u76f8\u4f4d\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u540e\u9a8c\u91c7\u6837\u5e73\u8861\u5931\u771f\u4e0e\u611f\u77e5\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u8fc7\u4e8e\u5173\u6ce8\u50cf\u7d20\u7ea7\u7cbe\u5ea6\uff0c\u800c\u5ffd\u89c6\u4e86\u611f\u77e5\u8d28\u91cf\u4e0e\u5931\u771f\u7684\u6743\u8861\u3002", "method": "\u7ed3\u5408\u968f\u673a\u91c7\u6837\u3001\u5b66\u4e60\u53bb\u566a\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u66f4\u65b0\uff0c\u8bbe\u8ba1\u4e86\u4e09\u79cd\u590d\u6742\u5ea6\u9012\u589e\u7684\u53d8\u4f53\uff0c\u5305\u62ecLangevin\u63a8\u65ad\u3001\u81ea\u9002\u5e94\u566a\u58f0\u8c03\u5ea6\u5b66\u4e60\u548c\u5e76\u884c\u91c7\u6837\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u517c\u987e\u4e86\u4fdd\u771f\u5ea6\u548c\u611f\u77e5\u8d28\u91cf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u76f8\u4f4d\u68c0\u7d22\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u6548\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u611f\u77e5\u8d28\u91cf\u4e0e\u5931\u771f\u7684\u5e73\u8861\u3002"}}
{"id": "2507.09759", "pdf": "https://arxiv.org/pdf/2507.09759", "abs": "https://arxiv.org/abs/2507.09759", "authors": ["Abdul Manaf", "Nimra Mughal"], "title": "AI-Enhanced Pediatric Pneumonia Detection: A CNN-Based Approach Using Data Augmentation and Generative Adversarial Networks (GANs)", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Pneumonia is a leading cause of mortality in children under five, requiring accurate chest X-ray diagnosis. This study presents a machine learning-based Pediatric Chest Pneumonia Classification System to assist healthcare professionals in diagnosing pneumonia from chest X-ray images. The CNN-based model was trained on 5,863 labeled chest X-ray images from children aged 0-5 years from the Guangzhou Women and Children's Medical Center. To address limited data, we applied augmentation techniques (rotation, zooming, shear, horizontal flipping) and employed GANs to generate synthetic images, addressing class imbalance. The system achieved optimal performance using combined original, augmented, and GAN-generated data, evaluated through accuracy and F1 score metrics. The final model was deployed via a Flask web application, enabling real-time classification with probability estimates. Results demonstrate the potential of deep learning and GANs in improving diagnostic accuracy and efficiency for pediatric pneumonia classification, particularly valuable in resource-limited clinical settings https://github.com/AbdulManaf12/Pediatric-Chest-Pneumonia-Classification", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u513f\u79d1\u80f8\u90e8\u80ba\u708e\u5206\u7c7b\u7cfb\u7edf\uff0c\u5229\u7528CNN\u6a21\u578b\u548cGAN\u751f\u6210\u5408\u6210\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u4e0d\u8db3\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u6700\u7ec8\u901a\u8fc7Flask\u5e94\u7528\u5b9e\u73b0\u5b9e\u65f6\u5206\u7c7b\u3002", "motivation": "\u80ba\u708e\u662f\u4e94\u5c81\u4ee5\u4e0b\u513f\u7ae5\u6b7b\u4ea1\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u9700\u8981\u51c6\u786e\u7684\u80f8\u90e8X\u5149\u8bca\u65ad\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u63d0\u9ad8\u8bca\u65ad\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u6709\u9650\u7684\u4e34\u5e8a\u73af\u5883\u4e2d\u3002", "method": "\u4f7f\u75285,863\u5f20\u6807\u8bb0\u7684\u513f\u7ae5\u80f8\u90e8X\u5149\u56fe\u50cf\u8bad\u7ec3CNN\u6a21\u578b\uff0c\u91c7\u7528\u6570\u636e\u589e\u5f3a\uff08\u65cb\u8f6c\u3001\u7f29\u653e\u3001\u526a\u5207\u3001\u6c34\u5e73\u7ffb\u8f6c\uff09\u548cGAN\u751f\u6210\u5408\u6210\u56fe\u50cf\u89e3\u51b3\u6570\u636e\u4e0d\u8db3\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u7ed3\u5408\u539f\u59cb\u3001\u589e\u5f3a\u548cGAN\u751f\u6210\u6570\u636e\uff0c\u7cfb\u7edf\u5728\u51c6\u786e\u7387\u548cF1\u5206\u6570\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u901a\u8fc7Flask\u5e94\u7528\u5b9e\u73b0\u5b9e\u65f6\u5206\u7c7b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u6df1\u5ea6\u5b66\u4e60\u548cGAN\u5728\u513f\u79d1\u80ba\u708e\u5206\u7c7b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u663e\u8457\u63d0\u9ad8\u8bca\u65ad\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u7684\u4e34\u5e8a\u73af\u5883\u3002"}}
{"id": "2507.09923", "pdf": "https://arxiv.org/pdf/2507.09923", "abs": "https://arxiv.org/abs/2507.09923", "authors": ["Sejin Park", "Sangmin Lee", "Kyong Hwan Jin", "Seung-Won Jung"], "title": "IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution", "categories": ["eess.IV", "cs.CV"], "comment": "ICCV 2025", "summary": "Super-resolution (SR) has been a pivotal task in image processing, aimed at enhancing image resolution across various applications. Recently, look-up table (LUT)-based approaches have attracted interest due to their efficiency and performance. However, these methods are typically designed for fixed scale factors, making them unsuitable for arbitrary-scale image SR (ASISR). Existing ASISR techniques often employ implicit neural representations, which come with considerable computational cost and memory demands. To address these limitations, we propose Interpolation Mixing LUT (IM-LUT), a novel framework that operates ASISR by learning to blend multiple interpolation functions to maximize their representational capacity. Specifically, we introduce IM-Net, a network trained to predict mixing weights for interpolation functions based on local image patterns and the target scale factor. To enhance efficiency of interpolation-based methods, IM-Net is transformed into IM-LUT, where LUTs are employed to replace computationally expensive operations, enabling lightweight and fast inference on CPUs while preserving reconstruction quality. Experimental results on several benchmark datasets demonstrate that IM-LUT consistently achieves a superior balance between image quality and efficiency compared to existing methods, highlighting its potential as a promising solution for resource-constrained applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d2\u503c\u6df7\u5408\u67e5\u627e\u8868\uff08IM-LUT\uff09\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u4efb\u610f\u5c3a\u5ea6\u8d85\u5206\u8fa8\u7387\uff08ASISR\uff09\uff0c\u901a\u8fc7\u6df7\u5408\u591a\u4e2a\u63d2\u503c\u51fd\u6570\u63d0\u5347\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u67e5\u627e\u8868\uff08LUT\uff09\u7684\u65b9\u6cd5\u4ec5\u9002\u7528\u4e8e\u56fa\u5b9a\u5c3a\u5ea6\uff0c\u800c\u73b0\u6709ASISR\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5185\u5b58\u9700\u6c42\u5927\u3002", "method": "\u63d0\u51faIM-LUT\u6846\u67b6\uff0c\u901a\u8fc7IM-Net\u9884\u6d4b\u63d2\u503c\u51fd\u6570\u7684\u6df7\u5408\u6743\u91cd\uff0c\u5e76\u5229\u7528LUT\u66ff\u6362\u9ad8\u8ba1\u7b97\u91cf\u64cd\u4f5c\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cIM-LUT\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "IM-LUT\u662f\u4e00\u79cd\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u5e94\u7528\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09995", "pdf": "https://arxiv.org/pdf/2507.09995", "abs": "https://arxiv.org/abs/2507.09995", "authors": ["Guohao Huo", "Ruiting Dai", "Hao Tang"], "title": "Graph-based Multi-Modal Interaction Lightweight Network for Brain Tumor Segmentation (GMLN-BTS) in Edge Iterative MRI Lesion Localization System (EdgeIMLocSys)", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Brain tumor segmentation plays a critical role in clinical diagnosis and treatment planning, yet the variability in imaging quality across different MRI scanners presents significant challenges to model generalization. To address this, we propose the Edge Iterative MRI Lesion Localization System (EdgeIMLocSys), which integrates Continuous Learning from Human Feedback to adaptively fine-tune segmentation models based on clinician feedback, thereby enhancing robustness to scanner-specific imaging characteristics. Central to this system is the Graph-based Multi-Modal Interaction Lightweight Network for Brain Tumor Segmentation (GMLN-BTS), which employs a Modality-Aware Adaptive Encoder (M2AE) to extract multi-scale semantic features efficiently, and a Graph-based Multi-Modal Collaborative Interaction Module (G2MCIM) to model complementary cross-modal relationships via graph structures. Additionally, we introduce a novel Voxel Refinement UpSampling Module (VRUM) that synergistically combines linear interpolation and multi-scale transposed convolutions to suppress artifacts while preserving high-frequency details, improving segmentation boundary accuracy. Our proposed GMLN-BTS model achieves a Dice score of 85.1% on the BraTS2017 dataset with only 4.58 million parameters, representing a 98% reduction compared to mainstream 3D Transformer models, and significantly outperforms existing lightweight approaches. This work demonstrates a synergistic breakthrough in achieving high-accuracy, resource-efficient brain tumor segmentation suitable for deployment in resource-constrained clinical environments.", "AI": {"tldr": "\u63d0\u51faEdgeIMLocSys\u7cfb\u7edf\uff0c\u7ed3\u5408GMLN-BTS\u7f51\u7edc\u548cVRUM\u6a21\u5757\uff0c\u901a\u8fc7\u4eba\u7c7b\u53cd\u9988\u6301\u7eed\u5b66\u4e60\uff0c\u63d0\u5347\u8111\u80bf\u7624\u5206\u5272\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3MRI\u626b\u63cf\u4eea\u6210\u50cf\u8d28\u91cf\u5dee\u5f02\u5bfc\u81f4\u7684\u6a21\u578b\u6cdb\u5316\u95ee\u9898\uff0c\u63d0\u5347\u8111\u80bf\u7624\u5206\u5272\u7684\u4e34\u5e8a\u9002\u7528\u6027\u3002", "method": "\u91c7\u7528GMLN-BTS\u7f51\u7edc\uff0c\u5305\u542bM2AE\u7f16\u7801\u5668\u548cG2MCIM\u6a21\u5757\uff0c\u7ed3\u5408VRUM\u6a21\u5757\u4f18\u5316\u5206\u5272\u8fb9\u754c\u3002", "result": "\u5728BraTS2017\u6570\u636e\u96c6\u4e0aDice\u5f97\u5206\u4e3a85.1%\uff0c\u53c2\u6570\u91cf\u4ec54.58M\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u8f7b\u91cf\u65b9\u6cd5\u3002", "conclusion": "\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u8d44\u6e90\u6d88\u8017\u7684\u8111\u80bf\u7624\u5206\u5272\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u4e34\u5e8a\u73af\u5883\u3002"}}
