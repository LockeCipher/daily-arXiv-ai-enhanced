<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 19]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Inference Time Debiasing Concepts in Diffusion Models](https://arxiv.org/abs/2508.14933)
*Lucas S. Kupssinskü,Marco N. Bochernitsan,Jordan Kopper,Otávio Parraga,Rodrigo C. Barros*

Main category: cs.GR

TL;DR: DeCoDi是一种针对文本到图像扩散模型的去偏方法，通过改变推理过程避免潜在维度中的偏见概念区域，无需复杂干预或显著计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习去偏方法通常需要复杂或计算密集的干预，而DeCoDi旨在仅改变推理过程，使其对更广泛的实践者更加可访问。

Method: DeCoDi改变扩散过程以避免潜在维度中的偏见概念区域，该方法可以应用于任何基于扩散的图像生成模型。

Result: 通过对护士、消防员和CEO等概念进行性别、种族和年龄的去偏，人工评估1200张生成图像显示该方法有效缓解了基于性别、种族和年龄的偏见。GPT4o的自动偏见评估与人工评估无显著统计差异。

Conclusion: DeCoDi方法有潜力显著改善基于扩散的文本到图像生成模型生成图像的多样性，评估结果显示评估者之间具有可靠的共识水平和对受保护属性的更好覆盖。

Abstract: We propose DeCoDi, a debiasing procedure for text-to-image diffusion-based models that changes the inference procedure, does not significantly change image quality, has negligible compute overhead, and can be applied in any diffusion-based image generation model. DeCoDi changes the diffusion process to avoid latent dimension regions of biased concepts. While most deep learning debiasing methods require complex or compute-intensive interventions, our method is designed to change only the inference procedure. Therefore, it is more accessible to a wide range of practitioners. We show the effectiveness of the method by debiasing for gender, ethnicity, and age for the concepts of nurse, firefighter, and CEO. Two distinct human evaluators manually inspect 1,200 generated images. Their evaluation results provide evidence that our method is effective in mitigating biases based on gender, ethnicity, and age. We also show that an automatic bias evaluation performed by the GPT4o is not significantly statistically distinct from a human evaluation. Our evaluation shows promising results, with reliable levels of agreement between evaluators and more coverage of protected attributes. Our method has the potential to significantly improve the diversity of images it generates by diffusion-based text-to-image generative models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [TAIGen: Training-Free Adversarial Image Generation via Diffusion Models](https://arxiv.org/abs/2508.15020)
*Susim Roy,Anubhooti Jain,Mayank Vatsa,Richa Singh*

Main category: cs.CV

TL;DR: TAIGen是一种无需训练的黑盒对抗图像生成方法，通过仅需3-20个采样步骤从无条件扩散模型中高效生成高质量对抗样本，攻击成功率高且速度快10倍


<details>
  <summary>Details</summary>
Motivation: 现有生成模型的对抗攻击往往产生低质量图像且计算资源消耗大，扩散模型虽然能生成高质量图像但通常需要数百个采样步骤，需要开发更高效的对抗生成方法

Method: 在混合步骤区间注入扰动，开发选择性RGB通道策略：对红色通道应用注意力图，对绿色和蓝色通道使用GradCAM引导的扰动，在保持图像结构的同时最大化目标模型的误分类

Result: 在ImageNet数据集上，以VGGNet为源模型，对ResNet攻击成功率达70.6%，对MNASNet达80.8%，对ShuffleNet达97.8%；PSNR超过30dB，生成速度比现有扩散攻击快10倍

Conclusion: TAIGen实现了高效高质量的对抗图像生成，具有最低的鲁棒准确率，表明防御机制最难净化其生成的图像，是最具影响力的攻击方法

Abstract: Adversarial attacks from generative models often produce low-quality images and require substantial computational resources. Diffusion models, though capable of high-quality generation, typically need hundreds of sampling steps for adversarial generation. This paper introduces TAIGen, a training-free black-box method for efficient adversarial image generation. TAIGen produces adversarial examples using only 3-20 sampling steps from unconditional diffusion models. Our key finding is that perturbations injected during the mixing step interval achieve comparable attack effectiveness without processing all timesteps. We develop a selective RGB channel strategy that applies attention maps to the red channel while using GradCAM-guided perturbations on green and blue channels. This design preserves image structure while maximizing misclassification in target models. TAIGen maintains visual quality with PSNR above 30 dB across all tested datasets. On ImageNet with VGGNet as source, TAIGen achieves 70.6% success against ResNet, 80.8% against MNASNet, and 97.8% against ShuffleNet. The method generates adversarial examples 10x faster than existing diffusion-based attacks. Our method achieves the lowest robust accuracy, indicating it is the most impactful attack as the defense mechanism is least successful in purifying the images generated by TAIGen.

</details>


### [3] [Reversible Unfolding Network for Concealed Visual Perception with Generative Refinement](https://arxiv.org/abs/2508.15027)
*Chunming He,Fengyang Xiao,Rihan Zhang,Chengyu Fang,Deng-Ping Fan,Sina Farsiu*

Main category: cs.CV

TL;DR: RUN++是一个可逆展开网络，通过生成式细化解决隐蔽视觉感知任务，在掩码和RGB域同时应用可逆建模，并利用扩散模型处理不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有隐蔽视觉感知方法主要局限于掩码域，未能充分利用RGB域的潜力，且在处理不确定性方面存在局限。

Method: 将CVP任务建模为数学优化问题，展开为多阶段深度网络，包含CORE（掩码域可逆建模）、CARE（RGB域可逆建模）和FINE（基于噪声的细化）三个模块，其中FINE模块使用定向伯努利扩散模型仅细化不确定区域。

Result: 该方法通过展开网络为扩散模型提供强不确定性先验，有效减少误报和漏报，在保持计算效率的同时实现精细细节恢复。

Conclusion: RUN++提出了构建鲁棒CVP系统的新范式，通过双级优化框架在真实世界退化条件下保持有效性，实现了掩码和RGB域的可逆建模协同。

Abstract: Existing methods for concealed visual perception (CVP) often leverage reversible strategies to decrease uncertainty, yet these are typically confined to the mask domain, leaving the potential of the RGB domain underexplored. To address this, we propose a reversible unfolding network with generative refinement, termed RUN++. Specifically, RUN++ first formulates the CVP task as a mathematical optimization problem and unfolds the iterative solution into a multi-stage deep network. This approach provides a principled way to apply reversible modeling across both mask and RGB domains while leveraging a diffusion model to resolve the resulting uncertainty. Each stage of the network integrates three purpose-driven modules: a Concealed Object Region Extraction (CORE) module applies reversible modeling to the mask domain to identify core object regions; a Context-Aware Region Enhancement (CARE) module extends this principle to the RGB domain to foster better foreground-background separation; and a Finetuning Iteration via Noise-based Enhancement (FINE) module provides a final refinement. The FINE module introduces a targeted Bernoulli diffusion model that refines only the uncertain regions of the segmentation mask, harnessing the generative power of diffusion for fine-detail restoration without the prohibitive computational cost of a full-image process. This unique synergy, where the unfolding network provides a strong uncertainty prior for the diffusion model, allows RUN++ to efficiently direct its focus toward ambiguous areas, significantly mitigating false positives and negatives. Furthermore, we introduce a new paradigm for building robust CVP systems that remain effective under real-world degradations and extend this concept into a broader bi-level optimization framework.

</details>


### [4] [CurveFlow: Curvature-Guided Flow Matching for Image Generation](https://arxiv.org/abs/2508.15093)
*Yan Luo,Drake Du,Hao Huang,Yi Fang,Mengyu Wang*

Main category: cs.CV

TL;DR: CurveFlow通过引入曲率正则化来学习非线性轨迹，解决了传统线性整流流在文本到图像生成中语义对齐不足的问题，在多个指标上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的整流流模型基于数据和噪声分布之间的线性轨迹，这种线性性强制零曲率，可能迫使图像生成过程穿过数据流形的低概率区域，影响生成图像与文本描述的语义对齐。

Method: 提出CurveFlow框架，通过将曲率指导直接整合到流路径中来学习平滑的非线性轨迹，采用鲁棒的曲率正则化技术惩罚轨迹内在动力学的突变。

Result: 在MS COCO 2014和2017数据集上的实验表明，CurveFlow在文本到图像生成中达到最先进性能，显著优于标准整流流变体和其他非线性基线，在BLEU、METEOR、ROUGE和CLAIR等语义一致性指标上改进尤为明显。

Conclusion: 曲率感知建模显著增强了模型忠实遵循复杂指令的能力，同时保持高图像质量，证明了曲率与语义对齐之间的重要关联。

Abstract: Existing rectified flow models are based on linear trajectories between data and noise distributions. This linearity enforces zero curvature, which can inadvertently force the image generation process through low-probability regions of the data manifold. A key question remains underexplored: how does the curvature of these trajectories correlate with the semantic alignment between generated images and their corresponding captions, i.e., instructional compliance? To address this, we introduce CurveFlow, a novel flow matching framework designed to learn smooth, non-linear trajectories by directly incorporating curvature guidance into the flow path. Our method features a robust curvature regularization technique that penalizes abrupt changes in the trajectory's intrinsic dynamics.Extensive experiments on MS COCO 2014 and 2017 demonstrate that CurveFlow achieves state-of-the-art performance in text-to-image generation, significantly outperforming both standard rectified flow variants and other non-linear baselines like Rectified Diffusion. The improvements are especially evident in semantic consistency metrics such as BLEU, METEOR, ROUGE, and CLAIR. This confirms that our curvature-aware modeling substantially enhances the model's ability to faithfully follow complex instructions while simultaneously maintaining high image quality. The code is made publicly available at https://github.com/Harvard-AI-and-Robotics-Lab/CurveFlow.

</details>


### [5] [Image-Conditioned 3D Gaussian Splat Quantization](https://arxiv.org/abs/2508.15372)
*Xinshuang Liu,Runfa Blark Li,Keito Suzuki,Truong Nguyen*

Main category: cs.CV

TL;DR: 提出ICGS-Quantizer方法，通过联合利用高斯间和属性间相关性，使用跨场景共享码本，将3DGS压缩到KB级别，并支持基于图像的场景更新


<details>
  <summary>Details</summary>
Motivation: 现有3DGS压缩方法只能压缩到MB级别，不适用于大规模场景，且缺乏对存档后场景变化的适应机制

Method: 联合利用高斯间和属性间相关性，使用共享码本，编码-量化-解码联合训练，支持基于图像的场景条件解码

Result: 在压缩效率和场景变化适应性方面均优于现有方法，将存储需求降至KB级别同时保持视觉保真度

Conclusion: ICGS-Quantizer显著提升了3DGS的压缩效率和存档后场景更新的适应性，为大规模场景存档提供了实用解决方案

Abstract: 3D Gaussian Splatting (3DGS) has attracted considerable attention for enabling high-quality real-time rendering. Although 3DGS compression methods have been proposed for deployment on storage-constrained devices, two limitations hinder archival use: (1) they compress medium-scale scenes only to the megabyte range, which remains impractical for large-scale scenes or extensive scene collections; and (2) they lack mechanisms to accommodate scene changes after long-term archival. To address these limitations, we propose an Image-Conditioned Gaussian Splat Quantizer (ICGS-Quantizer) that substantially enhances compression efficiency and provides adaptability to scene changes after archiving. ICGS-Quantizer improves quantization efficiency by jointly exploiting inter-Gaussian and inter-attribute correlations and by using shared codebooks across all training scenes, which are then fixed and applied to previously unseen test scenes, eliminating the overhead of per-scene codebooks. This approach effectively reduces the storage requirements for 3DGS to the kilobyte range while preserving visual fidelity. To enable adaptability to post-archival scene changes, ICGS-Quantizer conditions scene decoding on images captured at decoding time. The encoding, quantization, and decoding processes are trained jointly, ensuring that the codes, which are quantized representations of the scene, are effective for conditional decoding. We evaluate ICGS-Quantizer on 3D scene compression and 3D scene updating. Experimental results show that ICGS-Quantizer consistently outperforms state-of-the-art methods in compression efficiency and adaptability to scene changes. Our code, model, and data will be publicly available on GitHub.

</details>


### [6] [Scaling Group Inference for Diverse and High-Quality Generation](https://arxiv.org/abs/2508.15773)
*Gaurav Parmar,Or Patashnik,Daniil Ostashev,Kuan-Chieh Wang,Kfir Aberman,Srinivasa Narasimhan,Jun-Yan Zhu*

Main category: cs.CV

TL;DR: 提出了一种可扩展的组推理方法，通过将组推理建模为二次整数分配问题，在提升样本质量的同时最大化组多样性，解决了独立采样导致的冗余问题。


<details>
  <summary>Details</summary>
Motivation: 在真实应用中，用户通常需要获得一组多样化的输出（如4-8个图像），但传统的独立采样方法往往产生冗余结果，限制了用户选择和创意探索。

Method: 将组推理建模为二次整数分配问题：候选输出作为图节点，通过优化样本质量（一元项）和最大化组多样性（二元项）来选择子集；采用渐进式剪枝策略提高运行效率。

Result: 实验表明该方法相比独立采样基线和最新推理算法，显著提升了组多样性和质量，适用于文本到图像、图像到图像、图像提示和视频生成等多种任务。

Conclusion: 该框架使生成模型能够将多个输出视为有凝聚力的组而非独立样本，为实际应用提供了更丰富的输出选择。

Abstract: Generative models typically sample outputs independently, and recent inference-time guidance and scaling algorithms focus on improving the quality of individual samples. However, in real-world applications, users are often presented with a set of multiple images (e.g., 4-8) for each prompt, where independent sampling tends to lead to redundant results, limiting user choices and hindering idea exploration. In this work, we introduce a scalable group inference method that improves both the diversity and quality of a group of samples. We formulate group inference as a quadratic integer assignment problem: candidate outputs are modeled as graph nodes, and a subset is selected to optimize sample quality (unary term) while maximizing group diversity (binary term). To substantially improve runtime efficiency, we progressively prune the candidate set using intermediate predictions, allowing our method to scale up to large candidate sets. Extensive experiments show that our method significantly improves group diversity and quality compared to independent sampling baselines and recent inference algorithms. Our framework generalizes across a wide range of tasks, including text-to-image, image-to-image, image prompting, and video generation, enabling generative models to treat multiple outputs as cohesive groups rather than independent samples.

</details>


### [7] [XDR-LVLM: An Explainable Vision-Language Large Model for Diabetic Retinopathy Diagnosis](https://arxiv.org/abs/2508.15168)
*Masato Ito,Kaito Tanaka,Keisuke Matsuda,Aya Nakayama*

Main category: cs.CV

TL;DR: XDR-LVLM是一个基于视觉语言大模型的可解释性糖尿病视网膜病变诊断框架，通过自然语言解释实现高精度诊断和透明化报告生成


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在糖尿病视网膜病变诊断中缺乏透明度和可解释性的问题，促进临床采用

Method: 集成专业医学视觉编码器、LVLM核心，采用多任务提示工程和多阶段微调，从眼底图像中理解病理特征并生成包含DR分级、关键病理概念识别和详细解释的诊断报告

Result: 在DDR数据集上达到84.55%的平衡准确率和79.92%的F1分数，概念检测达到77.95% BACC和66.88% F1，人类评估确认生成解释的高流畅性、准确性和临床实用性

Conclusion: XDR-LVLM通过提供稳健且可解释的洞察，成功弥合了自动化诊断与临床需求之间的差距

Abstract: Diabetic Retinopathy (DR) is a major cause of global blindness, necessitating early and accurate diagnosis. While deep learning models have shown promise in DR detection, their black-box nature often hinders clinical adoption due to a lack of transparency and interpretability. To address this, we propose XDR-LVLM (eXplainable Diabetic Retinopathy Diagnosis with LVLM), a novel framework that leverages Vision-Language Large Models (LVLMs) for high-precision DR diagnosis coupled with natural language-based explanations. XDR-LVLM integrates a specialized Medical Vision Encoder, an LVLM Core, and employs Multi-task Prompt Engineering and Multi-stage Fine-tuning to deeply understand pathological features within fundus images and generate comprehensive diagnostic reports. These reports explicitly include DR severity grading, identification of key pathological concepts (e.g., hemorrhages, exudates, microaneurysms), and detailed explanations linking observed features to the diagnosis. Extensive experiments on the Diabetic Retinopathy (DDR) dataset demonstrate that XDR-LVLM achieves state-of-the-art performance, with a Balanced Accuracy of 84.55% and an F1 Score of 79.92% for disease diagnosis, and superior results for concept detection (77.95% BACC, 66.88% F1). Furthermore, human evaluations confirm the high fluency, accuracy, and clinical utility of the generated explanations, showcasing XDR-LVLM's ability to bridge the gap between automated diagnosis and clinical needs by providing robust and interpretable insights.

</details>


### [8] [MeSS: City Mesh-Guided Outdoor Scene Generation with Cross-View Consistent Diffusion](https://arxiv.org/abs/2508.15169)
*Xuyang Chen,Zhijun Zhai,Kaixuan Zhou,Zengmao Wang,Jianan He,Dong Wang,Yanfeng Zhang,mingwei Sun,Rüdiger Westermann,Konrad Schindler,Liqiu Meng*

Main category: cs.CV

TL;DR: 提出MeSS方法，基于城市网格模型生成高质量、风格一致的室外场景纹理，通过改进图像扩散模型实现跨视角一致性，并重建3D高斯溅射场景


<details>
  <summary>Details</summary>
Motivation: 城市网格模型缺乏真实纹理，限制了在虚拟城市导航和自动驾驶中的应用，需要生成高质量且视角一致的场景纹理

Method: 三阶段流程：1)使用级联外绘ControlNet生成几何一致的稀疏视图；2)通过AGInpaint传播密集中间视图；3)用GCAlign模块消除视觉不一致性。同时基于网格表面初始化高斯球重建3DGS场景

Result: 在几何对齐和生成质量方面优于现有方法，生成场景可通过重照明和风格转换技术呈现多样化风格

Conclusion: MeSS方法成功解决了城市网格模型的纹理生成问题，实现了高质量、风格一致的室外场景合成，为虚拟城市应用提供了有效解决方案

Abstract: Mesh models have become increasingly accessible for numerous cities; however, the lack of realistic textures restricts their application in virtual urban navigation and autonomous driving. To address this, this paper proposes MeSS (Meshbased Scene Synthesis) for generating high-quality, styleconsistent outdoor scenes with city mesh models serving as the geometric prior. While image and video diffusion models can leverage spatial layouts (such as depth maps or HD maps) as control conditions to generate street-level perspective views, they are not directly applicable to 3D scene generation. Video diffusion models excel at synthesizing consistent view sequences that depict scenes but often struggle to adhere to predefined camera paths or align accurately with rendered control videos. In contrast, image diffusion models, though unable to guarantee cross-view visual consistency, can produce more geometry-aligned results when combined with ControlNet. Building on this insight, our approach enhances image diffusion models by improving cross-view consistency. The pipeline comprises three key stages: first, we generate geometrically consistent sparse views using Cascaded Outpainting ControlNets; second, we propagate denser intermediate views via a component dubbed AGInpaint; and third, we globally eliminate visual inconsistencies (e.g., varying exposure) using the GCAlign module. Concurrently with generation, a 3D Gaussian Splatting (3DGS) scene is reconstructed by initializing Gaussian balls on the mesh surface. Our method outperforms existing approaches in both geometric alignment and generation quality. Once synthesized, the scene can be rendered in diverse styles through relighting and style transfer techniques.

</details>


### [9] [Collaborative Multi-Modal Coding for High-Quality 3D Generation](https://arxiv.org/abs/2508.15228)
*Ziang Cao,Zhaoxi Chen,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: TriMM是首个前馈式3D原生生成模型，通过多模态协作编码和triplane潜在扩散模型，有效利用RGB、RGBD和点云等多模态数据进行3D资产生成。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成架构大多局限于单模态或纯3D结构，无法充分利用多模态数据的互补优势，限制了训练数据集的范围和模型性能。

Method: 1) 引入协作多模态编码整合模态特定特征；2) 使用辅助2D和3D监督提升鲁棒性；3) 基于triplane潜在扩散模型生成高质量3D资产

Result: 在多个知名数据集上的实验表明，TriMM仅使用少量训练数据就能达到与大规模数据集训练模型相竞争的性能，并在RGB-D数据集上验证了多模态集成的可行性

Conclusion: TriMM通过有效利用多模态数据，在3D生成任务中实现了优异的纹理和几何细节表现，为多模态3D建模提供了新的解决方案

Abstract: 3D content inherently encompasses multi-modal characteristics and can be projected into different modalities (e.g., RGB images, RGBD, and point clouds). Each modality exhibits distinct advantages in 3D asset modeling: RGB images contain vivid 3D textures, whereas point clouds define fine-grained 3D geometries. However, most existing 3D-native generative architectures either operate predominantly within single-modality paradigms-thus overlooking the complementary benefits of multi-modality data-or restrict themselves to 3D structures, thereby limiting the scope of available training datasets. To holistically harness multi-modalities for 3D modeling, we present TriMM, the first feed-forward 3D-native generative model that learns from basic multi-modalities (e.g., RGB, RGBD, and point cloud). Specifically, 1) TriMM first introduces collaborative multi-modal coding, which integrates modality-specific features while preserving their unique representational strengths. 2) Furthermore, auxiliary 2D and 3D supervision are introduced to raise the robustness and performance of multi-modal coding. 3) Based on the embedded multi-modal code, TriMM employs a triplane latent diffusion model to generate 3D assets of superior quality, enhancing both the texture and the geometric detail. Extensive experiments on multiple well-known datasets demonstrate that TriMM, by effectively leveraging multi-modality, achieves competitive performance with models trained on large-scale datasets, despite utilizing a small amount of training data. Furthermore, we conduct additional experiments on recent RGB-D datasets, verifying the feasibility of incorporating other multi-modal datasets into 3D generation.

</details>


### [10] [Pretrained Diffusion Models Are Inherently Skipped-Step Samplers](https://arxiv.org/abs/2508.15233)
*Wenju Xu*

Main category: cs.CV

TL;DR: 这篇论文提出了跳跃步采样机制，通过跳过多个中间去噪步骤来加速传播模型生成，而无需依赖非马尔可夫过程。


<details>
  <summary>Details</summary>
Motivation: 传播模型的迭代生成过程需要较长序列步骤，现有方法如DDIM尝试减少采样步骤，但仍不明确原始传播过程是否能在保持马尔可夫性的情况下实现同样的效率。

Method: 提出跳跃步采样机制，直接跳过多个中间去噪步骤，这种方法来自于与标准传播模型相同的训练目标。同时结合DDIM提出了增强版生成方法。

Result: 在OpenAI ADM、Stable Diffusion和Open Sora等领先传播模型上进行了大量实验，结果显示该方法能够在显著减少采样步骤的同时实现高质量生成。

Conclusion: 跳跃步采样通过马尔可夫方式加速采样是预训练传播模型的内在特性，无需依赖非马尔可夫过程，为传播模型的高效生成提供了新的视角。

Abstract: Diffusion models have been achieving state-of-the-art results across various generation tasks. However, a notable drawback is their sequential generation process, requiring long-sequence step-by-step generation. Existing methods, such as DDIM, attempt to reduce sampling steps by constructing a class of non-Markovian diffusion processes that maintain the same training objective. However, there remains a gap in understanding whether the original diffusion process can achieve the same efficiency without resorting to non-Markovian processes. In this paper, we provide a confirmative answer and introduce skipped-step sampling, a mechanism that bypasses multiple intermediate denoising steps in the iterative generation process, in contrast with the traditional step-by-step refinement of standard diffusion inference. Crucially, we demonstrate that this skipped-step sampling mechanism is derived from the same training objective as the standard diffusion model, indicating that accelerated sampling via skipped-step sampling via a Markovian way is an intrinsic property of pretrained diffusion models. Additionally, we propose an enhanced generation method by integrating our accelerated sampling technique with DDIM. Extensive experiments on popular pretrained diffusion models, including the OpenAI ADM, Stable Diffusion, and Open Sora models, show that our method achieves high-quality generation with significantly reduced sampling steps.

</details>


### [11] [TPA: Temporal Prompt Alignment for Fetal Congenital Heart Defect Classification](https://arxiv.org/abs/2508.15298)
*Darya Taratynova,Alya Almsouti,Beknur Kalmakhanbet,Numan Saeed,Mohammad Yaqub*

Main category: cs.CV

TL;DR: TPA是一种基于图像-文本基础模型和提示感知对比学习的胎儿先天性心脏病超声视频分类方法，通过时间特征聚合和文本提示对齐，结合条件变分自编码器进行不确定性量化，在CHD检测和心功能评估任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前超声视频CHD检测方法存在三个主要问题：忽略时间信息、仅限于二分类、缺乏预测校准。这些限制了自动化方法在临床中的可靠应用。

Method: TPA方法包含：1）图像编码器提取视频帧特征；2）可训练时间提取器聚合心脏运动信息；3）通过边界铰链对比损失将视频表示与类别特定文本提示对齐；4）CVAESM模块学习潜在风格向量调制嵌入并量化分类不确定性。

Result: 在CHD检测任务上达到85.40%的macro F1分数，预期校准误差降低5.38%，自适应ECE降低6.8%。在EchoNet-Dynamic三分类任务上，macro F1提升4.73%（从53.89%到58.62%）。

Conclusion: TPA框架成功整合了时间建模、提示感知对比学习和不确定性量化，为胎儿先天性心脏病超声视频分类提供了有效的解决方案，显著提升了分类性能和临床可靠性。

Abstract: Congenital heart defect (CHD) detection in ultrasound videos is hindered by image noise and probe positioning variability. While automated methods can reduce operator dependence, current machine learning approaches often neglect temporal information, limit themselves to binary classification, and do not account for prediction calibration. We propose Temporal Prompt Alignment (TPA), a method leveraging foundation image-text model and prompt-aware contrastive learning to classify fetal CHD on cardiac ultrasound videos. TPA extracts features from each frame of video subclips using an image encoder, aggregates them with a trainable temporal extractor to capture heart motion, and aligns the video representation with class-specific text prompts via a margin-hinge contrastive loss. To enhance calibration for clinical reliability, we introduce a Conditional Variational Autoencoder Style Modulation (CVAESM) module, which learns a latent style vector to modulate embeddings and quantifies classification uncertainty. Evaluated on a private dataset for CHD detection and on a large public dataset, EchoNet-Dynamic, for systolic dysfunction, TPA achieves state-of-the-art macro F1 scores of 85.40% for CHD diagnosis, while also reducing expected calibration error by 5.38% and adaptive ECE by 6.8%. On EchoNet-Dynamic's three-class task, it boosts macro F1 by 4.73% (from 53.89% to 58.62%). Temporal Prompt Alignment (TPA) is a framework for fetal congenital heart defect (CHD) classification in ultrasound videos that integrates temporal modeling, prompt-aware contrastive learning, and uncertainty quantification.

</details>


### [12] [Transfer learning optimization based on evolutionary selective fine tuning](https://arxiv.org/abs/2508.15367)
*Jacinto Colan,Ana Davila,Yasuhisa Hasegawa*

Main category: cs.CV

TL;DR: 这篇论文提出了BioTune，一种通过进化算法选择性微调关键层的迅速迁移学习方法，在保持精度的同时降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统的全参数微调方法存在过拟合风险和计算成本高的问题，需要一种更高效的选择性微调技术来提升迁移学习效率。

Method: 使用进化算法来识别和选择最佳的微调层集合，只对关键层进行微调，减少可训练参数数量。

Result: 在9个不同领域的图像分类数据集上评估，BioTune达到了与AutoRGN和LoRA等现有方法相当或更优的精度和效率。

Conclusion: BioTune通过集中微调关键层的方式，能够在保持模型性能的同时显著降低计算成本，为过拟合问题提供了解决方案，推进了迁移学习在多样化数据上的应用。

Abstract: Deep learning has shown substantial progress in image analysis. However, the computational demands of large, fully trained models remain a consideration. Transfer learning offers a strategy for adapting pre-trained models to new tasks. Traditional fine-tuning often involves updating all model parameters, which can potentially lead to overfitting and higher computational costs. This paper introduces BioTune, an evolutionary adaptive fine-tuning technique that selectively fine-tunes layers to enhance transfer learning efficiency. BioTune employs an evolutionary algorithm to identify a focused set of layers for fine-tuning, aiming to optimize model performance on a given target task. Evaluation across nine image classification datasets from various domains indicates that BioTune achieves competitive or improved accuracy and efficiency compared to existing fine-tuning methods such as AutoRGN and LoRA. By concentrating the fine-tuning process on a subset of relevant layers, BioTune reduces the number of trainable parameters, potentially leading to decreased computational cost and facilitating more efficient transfer learning across diverse data characteristics and distributions.

</details>


### [13] [DriveSplat: Decoupled Driving Scene Reconstruction with Geometry-enhanced Partitioned Neural Gaussians](https://arxiv.org/abs/2508.15376)
*Cong Wang,Xianda Guo,Wenbo Xu,Wei Tian,Ruiqi Song,Chenming Zhang,Lingxi Li,Long Chen*

Main category: cs.CV

TL;DR: DriveSplat是一种基于神经高斯表示的动态-静态解耦方法，专门针对驾驶场景的高质量3D重建，通过区域化体素初始化、可变形神经高斯和深度/法线先验监督，在Waymo和KITTI数据集上实现了最先进的新视角合成性能。


<details>
  <summary>Details</summary>
Motivation: 驾驶场景中存在快速移动的车辆、行人运动和大规模静态背景，给3D场景重建带来巨大挑战。现有基于3D高斯泼溅的方法通过解耦动态和静态组件来解决运动模糊问题，但这些方法忽视了具有充分几何关系的背景优化，仅通过添加高斯来拟合每个训练视图，导致新视角渲染鲁棒性有限且缺乏准确的几何表示。

Method: 1) 采用区域化体素初始化方案，将场景划分为近、中、远区域以增强近距离细节表示；2) 引入可变形神经高斯来建模非刚性动态对象，其参数通过可学习变形网络进行时间调整；3) 整个框架通过预训练模型的深度和法线先验进行监督，提高几何结构的准确性。

Result: 在Waymo和KITTI数据集上的严格评估表明，该方法在驾驶场景的新视角合成方面达到了最先进的性能。

Conclusion: DriveSplat通过创新的动态-静态解耦策略、区域化初始化和几何先验监督，有效解决了驾驶场景3D重建的挑战，为高质量的新视角合成提供了强有力的解决方案。

Abstract: In the realm of driving scenarios, the presence of rapidly moving vehicles, pedestrians in motion, and large-scale static backgrounds poses significant challenges for 3D scene reconstruction. Recent methods based on 3D Gaussian Splatting address the motion blur problem by decoupling dynamic and static components within the scene. However, these decoupling strategies overlook background optimization with adequate geometry relationships and rely solely on fitting each training view by adding Gaussians. Therefore, these models exhibit limited robustness in rendering novel views and lack an accurate geometric representation. To address the above issues, we introduce DriveSplat, a high-quality reconstruction method for driving scenarios based on neural Gaussian representations with dynamic-static decoupling. To better accommodate the predominantly linear motion patterns of driving viewpoints, a region-wise voxel initialization scheme is employed, which partitions the scene into near, middle, and far regions to enhance close-range detail representation. Deformable neural Gaussians are introduced to model non-rigid dynamic actors, whose parameters are temporally adjusted by a learnable deformation network. The entire framework is further supervised by depth and normal priors from pre-trained models, improving the accuracy of geometric structures. Our method has been rigorously evaluated on the Waymo and KITTI datasets, demonstrating state-of-the-art performance in novel-view synthesis for driving scenarios.

</details>


### [14] [Enhancing Novel View Synthesis from extremely sparse views with SfM-free 3D Gaussian Splatting Framework](https://arxiv.org/abs/2508.15457)
*Zongqi He,Hanmin Li,Kin-Chung Chan,Yushen Zuo,Hao Xie,Zhe Xiao,Jun Xiao,Kin-Man Lam*

Main category: cs.CV

TL;DR: 提出了一种无需SfM的3D高斯泼溅方法，能够在极稀疏视角输入下联合估计相机位姿和重建3D场景，显著提升了渲染质量


<details>
  <summary>Details</summary>
Motivation: 传统3DGS方法严重依赖密集多视角输入和精确相机位姿，但在实际应用中极稀疏视角输入时，SfM初始化方法无法准确重建3D几何结构，导致渲染质量下降

Method: 提出稠密立体模块逐步估计相机位姿并重建全局稠密点云；设计一致性视角插值模块生成额外监督信号；引入多尺度拉普拉斯一致性正则化和自适应空间感知多尺度几何正则化

Result: 在极稀疏视角条件下（仅使用2个训练视角），PSNR指标显著提升2.75dB，合成图像失真最小且保留丰富高频细节

Conclusion: 该方法有效解决了极稀疏视角下的3D场景重建问题，在视觉质量和几何结构方面均优于现有技术

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated remarkable real-time performance in novel view synthesis, yet its effectiveness relies heavily on dense multi-view inputs with precisely known camera poses, which are rarely available in real-world scenarios. When input views become extremely sparse, the Structure-from-Motion (SfM) method that 3DGS depends on for initialization fails to accurately reconstruct the 3D geometric structures of scenes, resulting in degraded rendering quality. In this paper, we propose a novel SfM-free 3DGS-based method that jointly estimates camera poses and reconstructs 3D scenes from extremely sparse-view inputs. Specifically, instead of SfM, we propose a dense stereo module to progressively estimates camera pose information and reconstructs a global dense point cloud for initialization. To address the inherent problem of information scarcity in extremely sparse-view settings, we propose a coherent view interpolation module that interpolates camera poses based on training view pairs and generates viewpoint-consistent content as additional supervision signals for training. Furthermore, we introduce multi-scale Laplacian consistent regularization and adaptive spatial-aware multi-scale geometry regularization to enhance the quality of geometrical structures and rendered content. Experiments show that our method significantly outperforms other state-of-the-art 3DGS-based approaches, achieving a remarkable 2.75dB improvement in PSNR under extremely sparse-view conditions (using only 2 training views). The images synthesized by our method exhibit minimal distortion while preserving rich high-frequency details, resulting in superior visual quality compared to existing techniques.

</details>


### [15] [Multi-Object Sketch Animation with Grouping and Motion Trajectory Priors](https://arxiv.org/abs/2508.15535)
*Guotao Liang,Juncheng Hu,Ximing Xing,Jing Zhang,Qian Yu*

Main category: cs.CV

TL;DR: GroupSketch是一种新颖的矢量草图动画方法，通过两阶段流程处理多对象交互和复杂运动，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多对象交互和复杂运动时存在局限性，要么仅限于单对象情况，要么存在时间不一致性和泛化能力差的问题

Method: 采用两阶段流程：运动初始化和运动细化。第一阶段交互式分割语义组并定义关键帧，通过插值生成粗略动画；第二阶段提出基于组的位移网络(GDN)，利用文本到视频模型的先验预测组特定位移场，并包含上下文条件特征增强(CCFE)等专门模块

Result: 大量实验表明，该方法在生成复杂多对象草图的高质量、时间一致动画方面显著优于现有方法

Conclusion: GroupSketch扩展了草图动画的实际应用范围，能够有效处理多对象交互和复杂运动场景

Abstract: We introduce GroupSketch, a novel method for vector sketch animation that effectively handles multi-object interactions and complex motions. Existing approaches struggle with these scenarios, either being limited to single-object cases or suffering from temporal inconsistency and poor generalization. To address these limitations, our method adopts a two-stage pipeline comprising Motion Initialization and Motion Refinement. In the first stage, the input sketch is interactively divided into semantic groups and key frames are defined, enabling the generation of a coarse animation via interpolation. In the second stage, we propose a Group-based Displacement Network (GDN), which refines the coarse animation by predicting group-specific displacement fields, leveraging priors from a text-to-video model. GDN further incorporates specialized modules, such as Context-conditioned Feature Enhancement (CCFE), to improve temporal consistency. Extensive experiments demonstrate that our approach significantly outperforms existing methods in generating high-quality, temporally consistent animations for complex, multi-object sketches, thus expanding the practical applications of sketch animation.

</details>


### [16] [When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding](https://arxiv.org/abs/2508.15641)
*Pengcheng Fang,Yuxia Chen,Rui Guo*

Main category: cs.CV

TL;DR: Grounded VideoDiT是一个视频LLM，通过扩散时间潜在编码器、对象接地表示和混合令牌方案，显著提升了视频时间感知和实体交互理解能力，在多个基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有视频LLM在时间感知方面存在局限：时间戳编码隐式、帧级特征连续性差、语言视觉对齐漂移，需要更精细的时间推理和实体交互理解能力。

Method: 1. 扩散时间潜在(DTL)编码器增强边界敏感性和时间一致性；2. 对象接地表示将查询实体显式绑定到局部视觉证据；3. 混合令牌方案提供显式时间戳建模

Result: 在Charades STA、NExT GQA和多个VideoQA基准测试中取得了最先进的性能，验证了强大的接地能力

Conclusion: Grounded VideoDiT通过三项关键创新成功解决了视频LLM的时间感知局限，为视频理解提供了更精细的时间推理和实体交互能力

Abstract: Understanding videos requires more than answering open ended questions, it demands the ability to pinpoint when events occur and how entities interact across time. While recent Video LLMs have achieved remarkable progress in holistic reasoning, they remain coarse in temporal perception: timestamps are encoded only implicitly, frame level features are weak in capturing continuity, and language vision alignment often drifts from the entities of interest. In this paper, we present Grounded VideoDiT, a Video LLM designed to overcome these limitations by introducing three key innovations. First, a Diffusion Temporal Latent (DTL) encoder enhances boundary sensitivity and maintains temporal consistency. Second, object grounded representations explicitly bind query entities to localized visual evidence, strengthening alignment. Third, a mixed token scheme with discrete temporal tokens provides explicit timestamp modeling, enabling fine grained temporal reasoning. Together, these designs equip Grounded VideoDiT with robust grounding capabilities, as validated by state of the art results on Charades STA, NExT GQA, and multiple VideoQA benchmarks.

</details>


### [17] [WorldWeaver: Generating Long-Horizon Video Worlds via Rich Perception](https://arxiv.org/abs/2508.15720)
*Zhiheng Liu,Xueqing Deng,Shoufa Chen,Angtian Wang,Qiushan Guo,Mingfei Han,Zeyue Xue,Mengzhao Chen,Ping Luo,Linjie Yang*

Main category: cs.CV

TL;DR: WorldWeaver是一个用于生成长视频的框架，通过联合建模RGB帧和感知条件，使用深度线索构建记忆库，并采用分段噪声调度来提升时间一致性和减少漂移问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成方法主要依赖RGB信号，导致在长序列中对象结构和运动出现累积错误，需要解决时间一致性和结构保持的问题。

Method: 提出统一的长时域建模方案，联合预测感知条件和颜色信息；利用深度线索构建记忆库保持上下文信息；采用分段噪声调度训练预测组。

Result: 在扩散模型和整流流模型上的大量实验表明，WorldWeaver能有效减少时间漂移并提高生成视频的保真度。

Conclusion: 该框架通过多模态联合建模和创新的记忆机制，显著提升了长视频生成的质量和时间一致性。

Abstract: Generative video modeling has made significant strides, yet ensuring structural and temporal consistency over long sequences remains a challenge. Current methods predominantly rely on RGB signals, leading to accumulated errors in object structure and motion over extended durations. To address these issues, we introduce WorldWeaver, a robust framework for long video generation that jointly models RGB frames and perceptual conditions within a unified long-horizon modeling scheme. Our training framework offers three key advantages. First, by jointly predicting perceptual conditions and color information from a unified representation, it significantly enhances temporal consistency and motion dynamics. Second, by leveraging depth cues, which we observe to be more resistant to drift than RGB, we construct a memory bank that preserves clearer contextual information, improving quality in long-horizon video generation. Third, we employ segmented noise scheduling for training prediction groups, which further mitigates drift and reduces computational cost. Extensive experiments on both diffusion- and rectified flow-based models demonstrate the effectiveness of WorldWeaver in reducing temporal drift and improving the fidelity of generated videos.

</details>


### [18] [Waver: Wave Your Way to Lifelike Video Generation](https://arxiv.org/abs/2508.15761)
*Yifu Zhang,Hao Yang,Yuqi Zhang,Yifei Hu,Fengda Zhu,Chuang Lin,Xiaofeng Mei,Yi Jiang,Zehuan Yuan,Bingyue Peng*

Main category: cs.CV

TL;DR: Waver是一个高性能的图像和视频生成基础模型，支持文本到视频、图像到视频和文本到图像生成，在多个基准测试中排名前三，性能优于开源模型并媲美商业解决方案。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有视频生成模型在运动捕捉、时间一致性和分辨率方面的限制，开发一个统一的框架来同时支持多种生成任务。

Method: 采用混合流DiT架构增强模态对齐和训练收敛速度，建立全面的数据筛选流程，使用MLLM视频质量模型过滤高质量样本，并提供详细的训练和推理方案。

Result: 能够生成5-10秒720p原生分辨率视频（可升级至1080p），在T2V和I2V排行榜中排名前三，在复杂运动捕捉和时序一致性方面表现优异。

Conclusion: Waver为高质量视频生成模型的训练提供了有效方法，有望加速视频生成技术的发展，相关技术细节已开源供社区使用。

Abstract: We present Waver, a high-performance foundation model for unified image and video generation. Waver can directly generate videos with durations ranging from 5 to 10 seconds at a native resolution of 720p, which are subsequently upscaled to 1080p. The model simultaneously supports text-to-video (T2V), image-to-video (I2V), and text-to-image (T2I) generation within a single, integrated framework. We introduce a Hybrid Stream DiT architecture to enhance modality alignment and accelerate training convergence. To ensure training data quality, we establish a comprehensive data curation pipeline and manually annotate and train an MLLM-based video quality model to filter for the highest-quality samples. Furthermore, we provide detailed training and inference recipes to facilitate the generation of high-quality videos. Building on these contributions, Waver excels at capturing complex motion, achieving superior motion amplitude and temporal consistency in video synthesis. Notably, it ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial Analysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming existing open-source models and matching or surpassing state-of-the-art commercial solutions. We hope this technical report will help the community more efficiently train high-quality video generation models and accelerate progress in video generation technologies. Official page: https://github.com/FoundationVision/Waver.

</details>


### [19] [Visual Autoregressive Modeling for Instruction-Guided Image Editing](https://arxiv.org/abs/2508.15772)
*Qingyang Mao,Qi Cai,Yehao Li,Yingwei Pan,Mingyue Cheng,Ting Yao,Qi Liu,Tao Mei*

Main category: cs.CV

TL;DR: VAREdit是一个基于视觉自回归模型的新型图像编辑框架，通过将图像编辑重构为多尺度特征预测问题，解决了扩散模型在指令引导编辑中的全局纠缠问题，实现了更高的编辑精度和效率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在指令引导图像编辑中存在全局去噪过程导致编辑区域与整个图像上下文纠缠的问题，会产生意外的伪修改并影响对编辑指令的遵循。自回归模型通过序列化合成过程提供了不同的解决方案。

Method: 提出VAREdit框架，将图像编辑重新定义为多尺度特征预测问题。核心创新是尺度对齐参考(SAR)模块，通过在第一自注意力层注入尺度匹配的条件信息，解决了细粒度源特征无法有效指导粗粒度目标特征预测的挑战。

Result: 在标准基准测试中，VAREdit比领先的基于扩散的方法高出30%+的GPT-Balance分数。完成512×512图像编辑仅需1.2秒，比同等规模的UltraEdit快2.2倍。

Conclusion: VAREdit通过自回归范式和多尺度特征预测机制，在编辑精度和效率方面都取得了显著进步，为指令引导图像编辑提供了新的有效解决方案。

Abstract: Recent advances in diffusion models have brought remarkable visual fidelity to instruction-guided image editing. However, their global denoising process inherently entangles the edited region with the entire image context, leading to unintended spurious modifications and compromised adherence to editing instructions. In contrast, autoregressive models offer a distinct paradigm by formulating image synthesis as a sequential process over discrete visual tokens. Their causal and compositional mechanism naturally circumvents the adherence challenges of diffusion-based methods. In this paper, we present VAREdit, a visual autoregressive (VAR) framework that reframes image editing as a next-scale prediction problem. Conditioned on source image features and text instructions, VAREdit generates multi-scale target features to achieve precise edits. A core challenge in this paradigm is how to effectively condition the source image tokens. We observe that finest-scale source features cannot effectively guide the prediction of coarser target features. To bridge this gap, we introduce a Scale-Aligned Reference (SAR) module, which injects scale-matched conditioning information into the first self-attention layer. VAREdit demonstrates significant advancements in both editing adherence and efficiency. On standard benchmarks, it outperforms leading diffusion-based methods by 30\%+ higher GPT-Balance score. Moreover, it completes a $512\times512$ editing in 1.2 seconds, making it 2.2$\times$ faster than the similarly sized UltraEdit. The models are available at https://github.com/HiDream-ai/VAREdit.

</details>


### [20] [CineScale: Free Lunch in High-Resolution Cinematic Visual Generation](https://arxiv.org/abs/2508.15774)
*Haonan Qiu,Ning Yu,Ziqi Huang,Paul Debevec,Ziwei Liu*

Main category: cs.CV

TL;DR: CineScale是一种新的推理范式，能够实现更高分辨率的视觉生成，无需微调即可生成8K图像，仅需少量LoRA微调即可生成4K视频


<details>
  <summary>Details</summary>
Motivation: 现有的视觉扩散模型由于缺乏高分辨率数据和计算资源限制，通常在有限分辨率下训练，难以生成高质量的高分辨率图像或视频，且现有方法容易产生重复模式的低质量内容

Method: 提出了CineScale推理范式，针对不同类型的视频生成架构设计了专门的变体，扩展了高分辨率I2V和V2V合成能力，基于最先进的开源视频生成框架构建

Result: 实验验证了该范式在扩展图像和视频模型高分辨率生成能力方面的优越性，能够实现8K图像生成（无需微调）和4K视频生成（仅需最小LoRA微调）

Conclusion: CineScale成功解决了高分辨率视觉生成中的高频信息增加和重复模式问题，显著提升了预训练模型的高分辨率生成能力

Abstract: Visual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of high-resolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained models. However, these methods are still prone to producing low-quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. In this work, we propose CineScale, a novel inference paradigm to enable higher-resolution visual generation. To tackle the various issues introduced by the two types of video generation architectures, we propose dedicated variants tailored to each. Unlike existing baseline methods that are confined to high-resolution T2I and T2V generation, CineScale broadens the scope by enabling high-resolution I2V and V2V synthesis, built atop state-of-the-art open-source video generation frameworks. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Remarkably, our approach enables 8k image generation without any fine-tuning, and achieves 4k video generation with only minimal LoRA fine-tuning. Generated video samples are available at our website: https://eyeline-labs.github.io/CineScale/.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [21] [Pixels Under Pressure: Exploring Fine-Tuning Paradigms for Foundation Models in High-Resolution Medical Imaging](https://arxiv.org/abs/2508.14931)
*Zahra TehraniNasab,Amar Kumar,Tal Arbel*

Main category: eess.IV

TL;DR: 本文系统研究了不同微调技术对高分辨率（512x512）图像生成质量的影响，包括全微调和参数高效微调方法，评估了生成质量和下游分类任务的实用性。


<details>
  <summary>Details</summary>
Motivation: 随着高分辨率图像合成在医疗成像等领域的重要性日益增加，需要研究如何通过微调技术将预训练扩散模型适配到特定任务和数据分布。

Method: 采用系统研究方法，对多种微调技术进行基准测试，包括全微调策略和参数高效微调（PEFT），评估FID、Vendi分数和提示-图像对齐等关键质量指标。

Result: 研究发现特定微调策略能够同时提高生成保真度和下游性能，在数据稀缺条件下使用合成图像进行分类器训练和真实图像评估时表现更佳。

Conclusion: 微调是适应预训练模型到高分辨率图像生成任务的关键机制，特定微调方法能有效提升生成质量和下游任务性能。

Abstract: Advancements in diffusion-based foundation models have improved text-to-image generation, yet most efforts have been limited to low-resolution settings. As high-resolution image synthesis becomes increasingly essential for various applications, particularly in medical imaging domains, fine-tuning emerges as a crucial mechanism for adapting these powerful pre-trained models to task-specific requirements and data distributions. In this work, we present a systematic study, examining the impact of various fine-tuning techniques on image generation quality when scaling to high resolution 512x512 pixels. We benchmark a diverse set of fine-tuning methods, including full fine-tuning strategies and parameter-efficient fine-tuning (PEFT). We dissect how different fine-tuning methods influence key quality metrics, including Fr\'echet Inception Distance (FID), Vendi score, and prompt-image alignment. We also evaluate the utility of generated images in a downstream classification task under data-scarce conditions, demonstrating that specific fine-tuning strategies improve both generation fidelity and downstream performance when synthetic images are used for classifier training and evaluation on real images. Our code is accessible through the project website - https://tehraninasab.github.io/PixelUPressure/.

</details>


### [22] [Zero-shot Volumetric CT Super-Resolution using 3D Gaussian Splatting with Upsampled 2D X-ray Projection Priors](https://arxiv.org/abs/2508.15151)
*Jeonghyun Noh,Hyun-Jic Oh,Byungju Chae,Won-Ki Jeong*

Main category: eess.IV

TL;DR: 一种新的无监督3D CT超分辨率方法，利用激光行为模型生成高分辨率2D X光投影作为外部先验知识，通过3D高斯拟合和负值混合技术完成3D CT重建


<details>
  <summary>Details</summary>
Motivation: 解决传统监督方法需要大量成对LR-HR数据集且难以获得，而现有无监督方法因仅依赖单个LR输入而无法恢复细微解剖结构的问题

Method: 训练激光行为模型生成高分辨率2D X光投影作为外部先验，采用每张投影适配采样策略，使用3D高斯拟合重建3D CT体积，并提出支持负值的负值混合技术(NAB-GS)以促进高频结构恢复

Result: 在两个数据集上的实验显示，该方法在3D CT超分辨率任务中获得了优秀的定量和定性结果

Conclusion: 该方法成功结合了外部先验知识和无监督学习的优势，为解决缺乏成对数据的3D CT超分辨率问题提供了有效方案

Abstract: Computed tomography (CT) is widely used in clinical diagnosis, but acquiring high-resolution (HR) CT is limited by radiation exposure risks. Deep learning-based super-resolution (SR) methods have been studied to reconstruct HR from low-resolution (LR) inputs. While supervised SR approaches have shown promising results, they require large-scale paired LR-HR volume datasets that are often unavailable. In contrast, zero-shot methods alleviate the need for paired data by using only a single LR input, but typically struggle to recover fine anatomical details due to limited internal information. To overcome these, we propose a novel zero-shot 3D CT SR framework that leverages upsampled 2D X-ray projection priors generated by a diffusion model. Exploiting the abundance of HR 2D X-ray data, we train a diffusion model on large-scale 2D X-ray projection and introduce a per-projection adaptive sampling strategy. It selects the generative process for each projection, thus providing HR projections as strong external priors for 3D CT reconstruction. These projections serve as inputs to 3D Gaussian splatting for reconstructing a 3D CT volume. Furthermore, we propose negative alpha blending (NAB-GS) that allows negative values in Gaussian density representation. NAB-GS enables residual learning between LR and diffusion-based projections, thereby enhancing high-frequency structure reconstruction. Experiments on two datasets show that our method achieves superior quantitative and qualitative results for 3D CT SR.

</details>


### [23] [Pathology-Informed Latent Diffusion Model for Anomaly Detection in Lymph Node Metastasis](https://arxiv.org/abs/2508.15236)
*Jiamu Wang,Keunho Byeon,Jinsol Song,Anh Nguyen,Sangjeong Ahn,Sung Hak Lee,Jin Tae Kwak*

Main category: eess.IV

TL;DR: 结合视觉语言模型和扩散模型的无监督异常检测方法，利用组织病理学提示进行重建，在数字病理学中有效区分正常和异常组织


<details>
  <summary>Details</summary>
Motivation: 数字病理学中监督学习方法需要大量标注数据，但标注数据稀缺。无监督异常检测能够在不依赖详尽标注的情况下识别组织异常，扩散模型在该领域展现出良好性能

Method: 将视觉语言模型与扩散模型结合，使用一组与正常组织相关的病理学关键词作为提示来指导重建过程，帮助区分正常和异常组织

Result: 在本地医院胃淋巴结数据集和公共乳腺淋巴结数据集上进行实验，验证了方法的有效性和在领域转移下的泛化能力

Conclusion: 该方法在数字病理学多个器官的无监督异常检测中具有潜力，为减少对标注数据的依赖提供了可行方案

Abstract: Anomaly detection is an emerging approach in digital pathology for its ability to efficiently and effectively utilize data for disease diagnosis. While supervised learning approaches deliver high accuracy, they rely on extensively annotated datasets, suffering from data scarcity in digital pathology. Unsupervised anomaly detection, however, offers a viable alternative by identifying deviations from normal tissue distributions without requiring exhaustive annotations. Recently, denoising diffusion probabilistic models have gained popularity in unsupervised anomaly detection, achieving promising performance in both natural and medical imaging datasets. Building on this, we incorporate a vision-language model with a diffusion model for unsupervised anomaly detection in digital pathology, utilizing histopathology prompts during reconstruction. Our approach employs a set of pathology-related keywords associated with normal tissues to guide the reconstruction process, facilitating the differentiation between normal and abnormal tissues. To evaluate the effectiveness of the proposed method, we conduct experiments on a gastric lymph node dataset from a local hospital and assess its generalization ability under domain shift using a public breast lymph node dataset. The experimental results highlight the potential of the proposed method for unsupervised anomaly detection across various organs in digital pathology. Code: https://github.com/QuIIL/AnoPILaD.

</details>


### [24] [Deep Equilibrium Convolutional Sparse Coding for Hyperspectral Image Denoising](https://arxiv.org/abs/2508.15553)
*Jin Ye,Jingran Wang,Fengchao Xiong,Jingzhou Chen,Yuntao Qian*

Main category: eess.IV

TL;DR: 基于深度均衡模型的卷积稀疏编码框架DECSC，通过统一局部空间-光谱相关性、非局部空间自相似性和全局空间一致性，实现高光谱图像去噪


<details>
  <summary>Details</summary>
Motivation: 高光谱图像常受复杂噪声影响，现有深度展开方法缺乏收敛保证。深度均衡模型将网络视为无限深度，与优化过程自然一致，适合解决这一问题

Method: 在卷积稀疏编码框架中结合2D和3D卷积稀疏表示：2D CSC确保波段间全局空间一致性，3D CSC捕捉局部空间-光谱细节，嵌入Transformer块利用非局部自相似性，加入细节增强模块

Result: 实验结果表明DECSC方法在去噪性能上优于现有最先进方法

Conclusion: 提出的DECSC框架成功统一了多种特征表示，深度均衡模型为高光谱图像去噪提供了有效的收敛保证和优异性能

Abstract: Hyperspectral images (HSIs) play a crucial role in remote sensing but are often degraded by complex noise patterns. Ensuring the physical property of the denoised HSIs is vital for robust HSI denoising, giving the rise of deep unfolding-based methods. However, these methods map the optimization of a physical model to a learnable network with a predefined depth, which lacks convergence guarantees. In contrast, Deep Equilibrium (DEQ) models treat the hidden layers of deep networks as the solution to a fixed-point problem and models them as infinite-depth networks, naturally consistent with the optimization. Under the framework of DEQ, we propose a Deep Equilibrium Convolutional Sparse Coding (DECSC) framework that unifies local spatial-spectral correlations, nonlocal spatial self-similarities, and global spatial consistency for robust HSI denoising. Within the convolutional sparse coding (CSC) framework, we enforce shared 2D convolutional sparse representation to ensure global spatial consistency across bands, while unshared 3D convolutional sparse representation captures local spatial-spectral details. To further exploit nonlocal self-similarities, a transformer block is embedded after the 2D CSC. Additionally, a detail enhancement module is integrated with the 3D CSC to promote image detail preservation. We formulate the proximal gradient descent of the CSC model as a fixed-point problem and transform the iterative updates into a learnable network architecture within the framework of DEQ. Experimental results demonstrate that our DECSC method achieves superior denoising performance compared to state-of-the-art methods.

</details>


### [25] [Are Virtual DES Images a Valid Alternative to the Real Ones?](https://arxiv.org/abs/2508.15594)
*Ana C. Perre,Luís A. Alexandre,Luís C. Freire*

Main category: eess.IV

TL;DR: 本研究探索使用三种模型从低能量乳腺图像生成虚拟双能量减影图像，并评估其对乳腺病变分类的影响。预训练U-Net模型表现最佳，虚拟DES图像分类F1分数达85.59%，接近真实DES图像的90.35%。


<details>
  <summary>Details</summary>
Motivation: 通过图像到图像转换技术从低能量(LE)图像生成双能量减影(DES)图像，可减少患者在高能量图像采集时的辐射暴露，具有重要的临床价值。

Method: 研究比较了三种模型：预训练U-Net模型、端到端训练的U-Net模型和CycleGAN模型，用于从LE图像生成虚拟DES图像，并评估这些虚拟图像对恶性/非恶性乳腺病变分类任务的影响。

Result: 预训练U-Net模型表现最佳，使用虚拟DES图像获得85.59%的F1分数，与真实DES图像的90.35%相比存在一定差距，但显示出良好的潜力。

Conclusion: 虚拟DES图像生成具有显著潜力，虽然目前与真实图像存在性能差距，但未来技术进步有望缩小这一差距，使完全依赖虚拟DES图像在临床上成为可行选择。

Abstract: Contrast-enhanced spectral mammography (CESM) is an imaging modality that provides two types of images, commonly known as low-energy (LE) and dual-energy subtracted (DES) images. In many domains, particularly in medicine, the emergence of image-to-image translation techniques has enabled the artificial generation of images using other images as input. Within CESM, applying such techniques to generate DES images from LE images could be highly beneficial, potentially reducing patient exposure to radiation associated with high-energy image acquisition. In this study, we investigated three models for the artificial generation of DES images (virtual DES): a pre-trained U-Net model, a U-Net trained end-to-end model, and a CycleGAN model. We also performed a series of experiments to assess the impact of using virtual DES images on the classification of CESM examinations into malignant and non-malignant categories. To our knowledge, this is the first study to evaluate the impact of virtual DES images on CESM lesion classification. The results demonstrate that the best performance was achieved with the pre-trained U-Net model, yielding an F1 score of 85.59% when using the virtual DES images, compared to 90.35% with the real DES images. This discrepancy likely results from the additional diagnostic information in real DES images, which contributes to a higher classification accuracy. Nevertheless, the potential for virtual DES image generation is considerable and future advancements may narrow this performance gap to a level where exclusive reliance on virtual DES images becomes clinically viable.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [26] [Probability Density from Latent Diffusion Models for Out-of-Distribution Detection](https://arxiv.org/abs/2508.15737)
*Joonas Järve,Karl Kaspar Haavel,Meelis Kull*

Main category: cs.LG

TL;DR: 本文探讨了生成模型中基于似然的OOD检测方法，发现在表示空间中该方法性能优于像素空间，并与最先进方法相媲美。


<details>
  <summary>Details</summary>
Motivation: 尽管AI快速发展，安全仍是机器学习系统部署的主要瓶颈。OOD检测是关键安全组件，但先前工作报告似然方法在实践中经常失败，需要探索这是表示空间的问题还是像素空间的问题。

Method: 训练变分扩散模型在预训练ResNet-18的表示空间上，而非图像像素空间，使用基于似然的OOD检测器并与OpenOOD套件中的最先进方法进行比较。

Result: 在表示空间中，基于似然的OOD检测器性能优于像素空间中的表现，并且与最先进方法性能相当。

Conclusion: 似然方法在合适的表示空间中可以作为有效的OOD检测器，解决了先前在像素空间中观察到的性能问题。

Abstract: Despite rapid advances in AI, safety remains the main bottleneck to deploying machine-learning systems. A critical safety component is out-of-distribution detection: given an input, decide whether it comes from the same distribution as the training data. In generative models, the most natural OOD score is the data likelihood. Actually, under the assumption of uniformly distributed OOD data, the likelihood is even the optimal OOD detector, as we show in this work. However, earlier work reported that likelihood often fails in practice, raising doubts about its usefulness. We explore whether, in practice, the representation space also suffers from the inability to learn good density estimation for OOD detection, or if it is merely a problem of the pixel space typically used in generative models. To test this, we trained a Variational Diffusion Model not on images, but on the representation space of a pre-trained ResNet-18 to assess the performance of our likelihood-based detector in comparison to state-of-the-art methods from the OpenOOD suite.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [27] [See it. Say it. Sorted: Agentic System for Compositional Diagram Generation](https://arxiv.org/abs/2508.15222)
*Hantao Zhang,Jingyang Liu,Ed Li*

Main category: cs.AI

TL;DR: 基于VLM和LLM的迭代系统，将手绘草图转换为精确的编程式SVG图表，解决传统模型在空间精度和结构化表现上的不足


<details>
  <summary>Details</summary>
Motivation: 流程图等符号化图表需要高空间精度、对齐和结构保持，传统渗透模型在这些方面表现不佳，需要新方法来生成可编辑的程序化SVG输出

Method: 设计了一个迭代循环系统：Critic VLM提出定性关系编辑建议，多个LLM候选生成不同策略的SVG更新，Judge VLM选择最佳方案，确保稳定改进

Result: 在10个发表论文流程图草图上，比GPT-5和Gemini-2.5-Pro更准确地重建布局和结构，准确组合多头箭头等原语，不会插入不应有的文本

Conclusion: 该方法通过定性推理优于脆弱的数值估计，保持全局约束，支持人工干预编辑，且SVG程序化输出方便扩展到PPT等工具

Abstract: We study sketch-to-diagram generation: converting rough hand sketches into precise, compositional diagrams. Diffusion models excel at photorealism but struggle with the spatial precision, alignment, and symbolic structure required for flowcharts. We introduce See it. Say it. Sorted., a training-free agentic system that couples a Vision-Language Model (VLM) with Large Language Models (LLMs) to produce editable Scalable Vector Graphics (SVG) programs. The system runs an iterative loop in which a Critic VLM proposes a small set of qualitative, relational edits; multiple candidate LLMs synthesize SVG updates with diverse strategies (conservative->aggressive, alternative, focused); and a Judge VLM selects the best candidate, ensuring stable improvement. This design prioritizes qualitative reasoning over brittle numerical estimates, preserves global constraints (e.g., alignment, connectivity), and naturally supports human-in-the-loop corrections. On 10 sketches derived from flowcharts in published papers, our method more faithfully reconstructs layout and structure than two frontier closed-source image generation LLMs (GPT-5 and Gemini-2.5-Pro), accurately composing primitives (e.g., multi-headed arrows) without inserting unwanted text. Because outputs are programmatic SVGs, the approach is readily extensible to presentation tools (e.g., PowerPoint) via APIs and can be specialized with improved prompts and task-specific tools. The codebase is open-sourced at https://github.com/hantaoZhangrichard/see_it_say_it_sorted.git.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [28] [Scalable FPGA Framework for Real-Time Denoising in High-Throughput Imaging: A DRAM-Optimized Pipeline using High-Level Synthesis](https://arxiv.org/abs/2508.14917)
*Weichien Liao*

Main category: cs.AR

TL;DR: FPGA实时预处理流水线，用于高通量成像数据的去噪和降维


<details>
  <summary>Details</summary>
Motivation: 高通量成像工作流（如PRISM）的数据生成速率超过传统实时处理能力，需要低延迟的实时预处理解决方案

Method: 采用基于FPGA的可扩展预处理流水线，通过高级综合（HLS）实现，优化DRAM缓冲，使用突发模式AXI4接口进行帧减法和平均操作

Result: 内核运行时间低于帧间间隔，实现内联去噪并减少下游CPU/GPU分析的数据集大小，在PRISM规模采集下验证有效

Conclusion: 该模块化FPGA框架为光谱学和显微镜学中的延迟敏感成像工作流提供了实用解决方案

Abstract: High-throughput imaging workflows, such as Parallel Rapid Imaging with Spectroscopic Mapping (PRISM), generate data at rates that exceed conventional real-time processing capabilities. We present a scalable FPGA-based preprocessing pipeline for real-time denoising, implemented via High-Level Synthesis (HLS) and optimized for DRAM-backed buffering. Our architecture performs frame subtraction and averaging directly on streamed image data, minimizing latency through burst-mode AXI4 interfaces. The resulting kernel operates below the inter-frame interval, enabling inline denoising and reducing dataset size for downstream CPU/GPU analysis. Validated under PRISM-scale acquisition, this modular FPGA framework offers a practical solution for latency-sensitive imaging workflows in spectroscopy and microscopy.

</details>
