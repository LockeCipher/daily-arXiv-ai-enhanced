{"id": "2509.23336", "pdf": "https://arxiv.org/pdf/2509.23336", "abs": "https://arxiv.org/abs/2509.23336", "authors": ["Weidan Xiong", "Yongli Wu", "Bochuan Zeng", "Jianwei Guo", "Dani Lischinski", "Daniel Cohen-Or", "Hui Huang"], "title": "DiffTex: Differentiable Texturing for Architectural Proxy Models", "categories": ["cs.GR", "cs.CV"], "comment": "ACM TOG and SIGGRAPH Asia 2025 (Patent Protected); Project page:   https://vcc.tech/research/2025/DiffTex", "summary": "Simplified proxy models are commonly used to represent architectural structures, reducing storage requirements and enabling real-time rendering. However, the geometric simplifications inherent in proxies result in a loss of fine color and geometric details, making it essential for textures to compensate for the loss. Preserving the rich texture information from the original dense architectural reconstructions remains a daunting task, particularly when working with unordered RGB photographs. We propose an automated method for generating realistic texture maps for architectural proxy models at the texel level from an unordered collection of registered photographs. Our approach establishes correspondences between texels on a UV map and pixels in the input images, with each texel's color computed as a weighted blend of associated pixel values. Using differentiable rendering, we optimize blending parameters to ensure photometric and perspective consistency, while maintaining seamless texture coherence. Experimental results demonstrate the effectiveness and robustness of our method across diverse architectural models and varying photographic conditions, enabling the creation of high-quality textures that preserve visual fidelity and structural detail.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ece\u65e0\u5e8fRGB\u7167\u7247\u81ea\u52a8\u751f\u6210\u5efa\u7b51\u4ee3\u7406\u6a21\u578b\u7eb9\u7406\u56fe\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u6e32\u67d3\u4f18\u5316\u6df7\u5408\u53c2\u6570\uff0c\u786e\u4fdd\u7eb9\u7406\u7684\u5149\u5ea6\u4e00\u81f4\u6027\u548c\u900f\u89c6\u4e00\u81f4\u6027\u3002", "motivation": "\u5efa\u7b51\u4ee3\u7406\u6a21\u578b\u7684\u51e0\u4f55\u7b80\u5316\u5bfc\u81f4\u989c\u8272\u548c\u51e0\u4f55\u7ec6\u8282\u4e22\u5931\uff0c\u9700\u8981\u7eb9\u7406\u6765\u8865\u507f\u8fd9\u4e9b\u635f\u5931\uff0c\u4f46\u4ece\u65e0\u5e8f\u7167\u7247\u4e2d\u4fdd\u7559\u539f\u59cb\u5bc6\u96c6\u91cd\u5efa\u7684\u4e30\u5bcc\u7eb9\u7406\u4fe1\u606f\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5efa\u7acbUV\u56fe\u4e0a\u7eb9\u7406\u4e0e\u8f93\u5165\u56fe\u50cf\u50cf\u7d20\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u6bcf\u4e2a\u7eb9\u7406\u7684\u989c\u8272\u8ba1\u7b97\u4e3a\u76f8\u5173\u50cf\u7d20\u503c\u7684\u52a0\u6743\u6df7\u5408\uff0c\u4f7f\u7528\u53ef\u5fae\u5206\u6e32\u67d3\u4f18\u5316\u6df7\u5408\u53c2\u6570\u4ee5\u786e\u4fdd\u5149\u5ea6\u4e00\u81f4\u6027\u548c\u900f\u89c6\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u5efa\u7b51\u6a21\u578b\u548c\u4e0d\u540c\u6444\u5f71\u6761\u4ef6\u4e0b\u90fd\u5177\u6709\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u521b\u5efa\u4fdd\u6301\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u7ed3\u6784\u7ec6\u8282\u7684\u9ad8\u8d28\u91cf\u7eb9\u7406\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5efa\u7b51\u7eb9\u7406\uff0c\u6709\u6548\u8865\u507f\u51e0\u4f55\u7b80\u5316\u5e26\u6765\u7684\u7ec6\u8282\u635f\u5931\uff0c\u4e3a\u5efa\u7b51\u4ee3\u7406\u6a21\u578b\u63d0\u4f9b\u903c\u771f\u7684\u7eb9\u7406\u6620\u5c04\u3002"}}
{"id": "2509.23607", "pdf": "https://arxiv.org/pdf/2509.23607", "abs": "https://arxiv.org/abs/2509.23607", "authors": ["Xiang Tang", "Ruotong Li", "Xiaopeng Fan"], "title": "ZeroScene: A Zero-Shot Framework for 3D Scene Generation from a Single Image and Controllable Texture Editing", "categories": ["cs.GR", "cs.CV"], "comment": "16 pages, 15 figures, Project page:   https://xdlbw.github.io/ZeroScene/", "summary": "In the field of 3D content generation, single image scene reconstruction methods still struggle to simultaneously ensure the quality of individual assets and the coherence of the overall scene in complex environments, while texture editing techniques often fail to maintain both local continuity and multi-view consistency. In this paper, we propose a novel system ZeroScene, which leverages the prior knowledge of large vision models to accomplish both single image-to-3D scene reconstruction and texture editing in a zero-shot manner. ZeroScene extracts object-level 2D segmentation and depth information from input images to infer spatial relationships within the scene. It then jointly optimizes 3D and 2D projection losses of the point cloud to update object poses for precise scene alignment, ultimately constructing a coherent and complete 3D scene that encompasses both foreground and background. Moreover, ZeroScene supports texture editing of objects in the scene. By imposing constraints on the diffusion model and introducing a mask-guided progressive image generation strategy, we effectively maintain texture consistency across multiple viewpoints and further enhance the realism of rendered results through Physically Based Rendering (PBR) material estimation. Experimental results demonstrate that our framework not only ensures the geometric and appearance accuracy of generated assets, but also faithfully reconstructs scene layouts and produces highly detailed textures that closely align with text prompts.", "AI": {"tldr": "ZeroScene\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u76843D\u573a\u666f\u751f\u6210\u7cfb\u7edf\uff0c\u80fd\u591f\u4ece\u5355\u5f20\u56fe\u50cf\u91cd\u5efa3D\u573a\u666f\u5e76\u652f\u6301\u7eb9\u7406\u7f16\u8f91\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u89c6\u89c9\u6a21\u578b\u5148\u9a8c\u77e5\u8bc6\u5b9e\u73b0\u573a\u666f\u4e00\u81f4\u6027\u548c\u591a\u89c6\u89d2\u7eb9\u7406\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u5355\u56fe\u50cf3D\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u5355\u4e2a\u8d44\u4ea7\u8d28\u91cf\u548c\u6574\u4f53\u573a\u666f\u8fde\u8d2f\u6027\uff0c\u7eb9\u7406\u7f16\u8f91\u6280\u672f\u4e5f\u96be\u4ee5\u4fdd\u6301\u5c40\u90e8\u8fde\u7eed\u6027\u548c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u53d6\u5bf9\u8c61\u7ea72D\u5206\u5272\u548c\u6df1\u5ea6\u4fe1\u606f\u63a8\u65ad\u7a7a\u95f4\u5173\u7cfb\uff0c\u8054\u5408\u4f18\u5316\u70b9\u4e91\u76843D\u548c2D\u6295\u5f71\u635f\u5931\u6765\u66f4\u65b0\u5bf9\u8c61\u59ff\u6001\uff1b\u901a\u8fc7\u6269\u6563\u6a21\u578b\u7ea6\u675f\u548c\u63a9\u7801\u5f15\u5bfc\u6e10\u8fdb\u56fe\u50cf\u751f\u6210\u7b56\u7565\u4fdd\u6301\u7eb9\u7406\u4e00\u81f4\u6027\uff0c\u5e76\u4f7f\u7528PBR\u6750\u8d28\u4f30\u8ba1\u589e\u5f3a\u771f\u5b9e\u611f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u4e0d\u4ec5\u786e\u4fdd\u751f\u6210\u8d44\u4ea7\u7684\u51e0\u4f55\u548c\u5916\u89c2\u51c6\u786e\u6027\uff0c\u8fd8\u80fd\u5fe0\u5b9e\u91cd\u5efa\u573a\u666f\u5e03\u5c40\u5e76\u751f\u6210\u4e0e\u6587\u672c\u63d0\u793a\u9ad8\u5ea6\u4e00\u81f4\u7684\u8be6\u7ec6\u7eb9\u7406\u3002", "conclusion": "ZeroScene\u6210\u529f\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u7684\u5355\u56fe\u50cf\u52303D\u573a\u666f\u91cd\u5efa\u548c\u7eb9\u7406\u7f16\u8f91\uff0c\u5728\u4fdd\u6301\u573a\u666f\u8fde\u8d2f\u6027\u548c\u591a\u89c6\u89d2\u7eb9\u7406\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.23718", "pdf": "https://arxiv.org/pdf/2509.23718", "abs": "https://arxiv.org/abs/2509.23718", "authors": ["Zhenyu Shu", "Jiawei Wen", "Shiyang Li", "Shiqing Xin", "Ligang Liu"], "title": "Diff-3DCap: Shape Captioning with Diffusion Models", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": null, "summary": "The task of 3D shape captioning occupies a significant place within the domain of computer graphics and has garnered considerable interest in recent years. Traditional approaches to this challenge frequently depend on the utilization of costly voxel representations or object detection techniques, yet often fail to deliver satisfactory outcomes. To address the above challenges, in this paper, we introduce Diff-3DCap, which employs a sequence of projected views to represent a 3D object and a continuous diffusion model to facilitate the captioning process. More precisely, our approach utilizes the continuous diffusion model to perturb the embedded captions during the forward phase by introducing Gaussian noise and then predicts the reconstructed annotation during the reverse phase. Embedded within the diffusion framework is a commitment to leveraging a visual embedding obtained from a pre-trained visual-language model, which naturally allows the embedding to serve as a guiding signal, eliminating the need for an additional classifier. Extensive results of our experiments indicate that Diff-3DCap can achieve performance comparable to that of the current state-of-the-art methods.", "AI": {"tldr": "Diff-3DCap\u4f7f\u7528\u6295\u5f71\u89c6\u56fe\u8868\u793a3D\u5bf9\u8c61\uff0c\u5e76\u91c7\u7528\u8fde\u7eed\u6269\u6563\u6a21\u578b\u8fdb\u884c3D\u5f62\u72b6\u63cf\u8ff0\u751f\u6210\uff0c\u65e0\u9700\u989d\u5916\u5206\u7c7b\u5668\u5373\u53ef\u8fbe\u5230\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf3D\u5f62\u72b6\u63cf\u8ff0\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u4f53\u7d20\u8868\u793a\u6216\u7269\u4f53\u68c0\u6d4b\u6280\u672f\uff0c\u4f46\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u6295\u5f71\u89c6\u56fe\u5e8f\u5217\u8868\u793a3D\u5bf9\u8c61\uff0c\u901a\u8fc7\u8fde\u7eed\u6269\u6563\u6a21\u578b\u5728\u6b63\u5411\u9636\u6bb5\u7528\u9ad8\u65af\u566a\u58f0\u6270\u52a8\u5d4c\u5165\u63cf\u8ff0\uff0c\u5728\u53cd\u5411\u9636\u6bb5\u9884\u6d4b\u91cd\u6784\u6ce8\u91ca\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u5d4c\u5165\u4f5c\u4e3a\u5f15\u5bfc\u4fe1\u53f7\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eDiff-3DCap\u80fd\u591f\u8fbe\u5230\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u53ef\u6bd4\u6027\u80fd\u3002", "conclusion": "Diff-3DCap\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u76843D\u5f62\u72b6\u63cf\u8ff0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u5408\uff0c\u65e0\u9700\u989d\u5916\u5206\u7c7b\u5668\u5373\u53ef\u83b7\u5f97\u826f\u597d\u6027\u80fd\u3002"}}
{"id": "2509.22692", "pdf": "https://arxiv.org/pdf/2509.22692", "abs": "https://arxiv.org/abs/2509.22692", "authors": ["Le Zhang", "Ao Li", "Qibin Hou", "Ce Zhu", "Yonina C. Eldar"], "title": "Deep Learning Empowered Super-Resolution: A Comprehensive Survey and Future Prospects", "categories": ["cs.CV"], "comment": "Accepted by Proceedings of the IEEE", "summary": "Super-resolution (SR) has garnered significant attention within the computer vision community, driven by advances in deep learning (DL) techniques and the growing demand for high-quality visual applications. With the expansion of this field, numerous surveys have emerged. Most existing surveys focus on specific domains, lacking a comprehensive overview of this field. Here, we present an in-depth review of diverse SR methods, encompassing single image super-resolution (SISR), video super-resolution (VSR), stereo super-resolution (SSR), and light field super-resolution (LFSR). We extensively cover over 150 SISR methods, nearly 70 VSR approaches, and approximately 30 techniques for SSR and LFSR. We analyze methodologies, datasets, evaluation protocols, empirical results, and complexity. In addition, we conducted a taxonomy based on each backbone structure according to the diverse purposes. We also explore valuable yet under-studied open issues in the field. We believe that this work will serve as a valuable resource and offer guidance to researchers in this domain. To facilitate access to related work, we created a dedicated repository available at https://github.com/AVC2-UESTC/Holistic-Super-Resolution-Review.", "AI": {"tldr": "\u8fd9\u662f\u4e00\u7bc7\u5173\u4e8e\u8d85\u5206\u8fa8\u7387\u6280\u672f\u7684\u5168\u9762\u7efc\u8ff0\u8bba\u6587\uff0c\u6db5\u76d6\u4e86\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u3001\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u3001\u7acb\u4f53\u8d85\u5206\u8fa8\u7387\u548c\u5149\u573a\u8d85\u5206\u8fa8\u7387\u7b49\u591a\u4e2a\u9886\u57df\uff0c\u5206\u6790\u4e86\u8d85\u8fc7150\u79cdSISR\u65b9\u6cd5\u3001\u8fd170\u79cdVSR\u65b9\u6cd5\u4ee5\u53ca\u7ea630\u79cdSSR\u548cLFSR\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u7684\u8d85\u5206\u8fa8\u7387\u7efc\u8ff0\u5927\u591a\u4e13\u6ce8\u4e8e\u7279\u5b9a\u9886\u57df\uff0c\u7f3a\u4e4f\u5bf9\u8be5\u9886\u57df\u7684\u5168\u9762\u6982\u8ff0\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u6df1\u5165\u5168\u9762\u7684\u7efc\u8ff0\uff0c\u6db5\u76d6\u8d85\u5206\u8fa8\u7387\u6280\u672f\u7684\u5404\u4e2a\u5206\u652f\u3002", "method": "\u5bf9\u4e0d\u540c\u7c7b\u578b\u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u5206\u7c7b\u548c\u5206\u6790\uff0c\u5305\u62ec\u65b9\u6cd5\u8bba\u3001\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u534f\u8bae\u3001\u5b9e\u8bc1\u7ed3\u679c\u548c\u590d\u6742\u5ea6\u5206\u6790\uff0c\u5e76\u6839\u636e\u4e0d\u540c\u7684\u9aa8\u5e72\u7ed3\u6784\u8fdb\u884c\u4e86\u5206\u7c7b\u3002", "result": "\u63d0\u4f9b\u4e86\u8d85\u8fc7150\u79cdSISR\u65b9\u6cd5\u3001\u8fd170\u79cdVSR\u65b9\u6cd5\u4ee5\u53ca\u7ea630\u79cdSSR\u548cLFSR\u6280\u672f\u7684\u8be6\u7ec6\u5206\u6790\uff0c\u5e76\u521b\u5efa\u4e86\u4e00\u4e2a\u4e13\u95e8\u7684GitHub\u4ed3\u5e93\u6765\u4fc3\u8fdb\u76f8\u5173\u5de5\u4f5c\u7684\u8bbf\u95ee\u3002", "conclusion": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u5c06\u6210\u4e3a\u8be5\u9886\u57df\u7814\u7a76\u4eba\u5458\u7684\u5b9d\u8d35\u8d44\u6e90\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u6307\u5bfc\uff0c\u540c\u65f6\u63a2\u8ba8\u4e86\u8be5\u9886\u57df\u4e2d\u5c1a\u672a\u5145\u5206\u7814\u7a76\u7684\u5f00\u653e\u6027\u95ee\u9898\u3002"}}
{"id": "2509.22720", "pdf": "https://arxiv.org/pdf/2509.22720", "abs": "https://arxiv.org/abs/2509.22720", "authors": ["Zezhong Fan", "Xiaohan Li", "Luyi Ma", "Kai Zhao", "Liang Peng", "Topojoy Biswas", "Evren Korpeoglu", "Kaushiki Nag", "Kannan Achan"], "title": "LayoutAgent: A Vision-Language Agent Guided Compositional Diffusion for Spatial Layout Planning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "NeurIPS 2025 Workshop on SPACE in Vision, Language, and Embodied AI", "summary": "Designing realistic multi-object scenes requires not only generating images, but also planning spatial layouts that respect semantic relations and physical plausibility. On one hand, while recent advances in diffusion models have enabled high-quality image generation, they lack explicit spatial reasoning, leading to unrealistic object layouts. On the other hand, traditional spatial planning methods in robotics emphasize geometric and relational consistency, but they struggle to capture semantic richness in visual scenes. To bridge this gap, in this paper, we propose LayoutAgent, an agentic framework that unifies vision-language reasoning with compositional diffusion for layout generation. Given multiple input images with target objects in them, our method first employs visual-language model to preprocess the inputs through segmentation, object size estimation, scene graph construction, and prompt rewriting. Then we leverage compositional diffusion-a method traditionally used in robotics-to synthesize bounding boxes that respect object relations encoded in the scene graph for spatial layouts. In the end, a foreground-conditioned image generator composes the complete scene by rendering the objects into the planned layout guided by designed prompts. Experiments demonstrate that LayoutAgent outperforms other state-of-the-art layout generation models in layout coherence, spatial realism and aesthetic alignment.", "AI": {"tldr": "LayoutAgent\u662f\u4e00\u4e2a\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u548c\u7ec4\u5408\u6269\u6563\u7684\u667a\u80fd\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u8bed\u4e49\u5173\u7cfb\u548c\u7269\u7406\u5408\u7406\u6027\u7684\u591a\u7269\u4f53\u573a\u666f\u5e03\u5c40\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u7f3a\u4e4f\u663e\u5f0f\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5bfc\u81f4\u7269\u4f53\u5e03\u5c40\u4e0d\u771f\u5b9e\uff1b\u800c\u4f20\u7edf\u673a\u5668\u4eba\u7a7a\u95f4\u89c4\u5212\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u89c6\u89c9\u573a\u666f\u7684\u8bed\u4e49\u4e30\u5bcc\u6027\u3002\u9700\u8981\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u9996\u5148\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8f93\u5165\u9884\u5904\u7406\uff08\u5206\u5272\u3001\u7269\u4f53\u5c3a\u5bf8\u4f30\u8ba1\u3001\u573a\u666f\u56fe\u6784\u5efa\u3001\u63d0\u793a\u91cd\u5199\uff09\uff0c\u7136\u540e\u5229\u7528\u7ec4\u5408\u6269\u6563\u65b9\u6cd5\u5408\u6210\u7b26\u5408\u573a\u666f\u56fe\u5173\u7cfb\u7684\u8fb9\u754c\u6846\uff0c\u6700\u540e\u901a\u8fc7\u524d\u666f\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u5668\u6e32\u67d3\u5b8c\u6574\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLayoutAgent\u5728\u5e03\u5c40\u8fde\u8d2f\u6027\u3001\u7a7a\u95f4\u771f\u5b9e\u6027\u548c\u7f8e\u5b66\u5bf9\u9f50\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u5e03\u5c40\u751f\u6210\u6a21\u578b\u3002", "conclusion": "LayoutAgent\u6210\u529f\u5730\u5c06\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u4e0e\u7ec4\u5408\u6269\u6563\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u66f4\u771f\u5b9e\u7684\u591a\u7269\u4f53\u573a\u666f\u5e03\u5c40\u751f\u6210\u3002"}}
{"id": "2509.22761", "pdf": "https://arxiv.org/pdf/2509.22761", "abs": "https://arxiv.org/abs/2509.22761", "authors": ["Yapeng Mi", "Hengli Li", "Yanpeng Zhao", "Chenxi Li", "Huimin Wu", "Xiaojian Ma", "Song-Chun Zhu", "Ying Nian Wu", "Qing Li"], "title": "MILR: Improving Multimodal Image Generation via Test-Time Latent Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "21 pages,13 figures,7 tables", "summary": "Reasoning-augmented machine learning systems have shown improved performance in various domains, including image generation. However, existing reasoning-based methods for image generation either restrict reasoning to a single modality (image or text) or rely on high-quality reasoning data for fine-tuning. To tackle these limitations, we propose MILR, a test-time method that jointly reasons over image and text in a unified latent vector space. Reasoning in MILR is performed by searching through vector representations of discrete image and text tokens. Practically, this is implemented via the policy gradient method, guided by an image quality critic. We instantiate MILR within the unified multimodal understanding and generation (MUG) framework that natively supports language reasoning before image synthesis and thus facilitates cross-modal reasoning. The intermediate model outputs, which are to be optimized, serve as the unified latent space, enabling MILR to operate entirely at test time. We evaluate MILR on GenEval, T2I-CompBench, and WISE, achieving state-of-the-art results on all benchmarks. Notably, on knowledge-intensive WISE, MILR attains an overall score of 0.63, improving over the baseline by 80%. Our further analysis indicates that joint reasoning in the unified latent space is the key to its strong performance. Moreover, our qualitative studies reveal MILR's non-trivial ability in temporal and cultural reasoning, highlighting the efficacy of our reasoning method.", "AI": {"tldr": "MILR\u662f\u4e00\u79cd\u5728\u6d4b\u8bd5\u65f6\u8054\u5408\u63a8\u7406\u56fe\u50cf\u548c\u6587\u672c\u7684\u65b9\u6cd5\uff0c\u5728\u7edf\u4e00\u6f5c\u5728\u5411\u91cf\u7a7a\u95f4\u4e2d\u901a\u8fc7\u7b56\u7565\u68af\u5ea6\u641c\u7d22\u5b9e\u73b0\u8de8\u6a21\u6001\u63a8\u7406\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63a8\u7406\u7684\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u8981\u4e48\u5c40\u9650\u4e8e\u5355\u4e00\u6a21\u6001\uff08\u56fe\u50cf\u6216\u6587\u672c\uff09\uff0c\u8981\u4e48\u4f9d\u8d56\u9ad8\u8d28\u91cf\u63a8\u7406\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u5728\u7edf\u4e00\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\uff08MUG\uff09\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u5728\u79bb\u6563\u56fe\u50cf\u548c\u6587\u672c\u6807\u8bb0\u7684\u5411\u91cf\u8868\u793a\u4e2d\u8fdb\u884c\u641c\u7d22\uff0c\u7531\u56fe\u50cf\u8d28\u91cf\u8bc4\u5224\u5668\u6307\u5bfc\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5728GenEval\u3001T2I-CompBench\u548cWISE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u5728\u77e5\u8bc6\u5bc6\u96c6\u578bWISE\u4e0a\u603b\u4f53\u5f97\u52060.63\uff0c\u6bd4\u57fa\u7ebf\u63d0\u9ad880%\u3002", "conclusion": "\u5728\u7edf\u4e00\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u8054\u5408\u63a8\u7406\u662fMILR\u5f3a\u6027\u80fd\u7684\u5173\u952e\uff0c\u8be5\u65b9\u6cd5\u5728\u65f6\u95f4\u548c\u6587\u5316\u63a8\u7406\u65b9\u9762\u5c55\u73b0\u51fa\u663e\u8457\u80fd\u529b\u3002"}}
{"id": "2509.25079", "pdf": "https://arxiv.org/pdf/2509.25079", "abs": "https://arxiv.org/abs/2509.25079", "authors": ["Guanjun Wu", "Jiemin Fang", "Chen Yang", "Sikuang Li", "Taoran Yi", "Jia Lu", "Zanwei Zhou", "Jiazhong Cen", "Lingxi Xie", "Xiaopeng Zhang", "Wei Wei", "Wenyu Liu", "Xinggang Wang", "Qi Tian"], "title": "UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "Project page: https://unilat3d.github.io/", "summary": "High-fidelity 3D asset generation is crucial for various industries. While recent 3D pretrained models show strong capability in producing realistic content, most are built upon diffusion models and follow a two-stage pipeline that first generates geometry and then synthesizes appearance. Such a decoupled design tends to produce geometry-texture misalignment and non-negligible cost. In this paper, we propose UniLat3D, a unified framework that encodes geometry and appearance in a single latent space, enabling direct single-stage generation. Our key contribution is a geometry-appearance Unified VAE, which compresses high-resolution sparse features into a compact latent representation -- UniLat. UniLat integrates structural and visual information into a dense low-resolution latent, which can be efficiently decoded into diverse 3D formats, e.g., 3D Gaussians and meshes. Based on this unified representation, we train a single flow-matching model to map Gaussian noise directly into UniLat, eliminating redundant stages. Trained solely on public datasets, UniLat3D produces high-quality 3D assets in seconds from a single image, achieving superior appearance fidelity and geometric quality. More demos \\& code are available at https://unilat3d.github.io/", "AI": {"tldr": "UniLat3D\u662f\u4e00\u4e2a\u7edf\u4e00\u76843D\u8d44\u4ea7\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55-\u5916\u89c2\u7edf\u4e00VAE\u5c06\u9ad8\u5206\u8fa8\u7387\u7a00\u758f\u7279\u5f81\u538b\u7f29\u5230\u7d27\u51d1\u7684UniLat\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u5b9e\u73b0\u5355\u9636\u6bb5\u76f4\u63a5\u751f\u6210\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u4e24\u9636\u6bb5\u65b9\u6cd5\u4e2d\u7684\u51e0\u4f55-\u7eb9\u7406\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf3D\u751f\u6210\u65b9\u6cd5\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff08\u5148\u51e0\u4f55\u540e\u5916\u89c2\uff09\uff0c\u5bfc\u81f4\u51e0\u4f55-\u7eb9\u7406\u4e0d\u5bf9\u9f50\u548c\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7edf\u4e00\u7684\u751f\u6210\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u51e0\u4f55-\u5916\u89c2\u7edf\u4e00VAE\uff0c\u5c06\u9ad8\u5206\u8fa8\u7387\u7a00\u758f\u7279\u5f81\u538b\u7f29\u4e3a\u7d27\u51d1\u7684UniLat\u6f5c\u5728\u8868\u793a\uff1b\u57fa\u4e8e\u6b64\u8bad\u7ec3\u5355\u4e00\u6d41\u5339\u914d\u6a21\u578b\uff0c\u76f4\u63a5\u4ece\u9ad8\u65af\u566a\u58f0\u6620\u5c04\u5230UniLat\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0cUniLat3D\u80fd\u5728\u6570\u79d2\u5185\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf3D\u8d44\u4ea7\uff0c\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u51e0\u4f55\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "UniLat3D\u901a\u8fc7\u7edf\u4e00\u7684\u6f5c\u5728\u8868\u793a\u548c\u5355\u9636\u6bb5\u751f\u6210\u6d41\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u9ad8\u8d28\u91cf\u76843D\u8d44\u4ea7\u751f\u6210\u3002"}}
{"id": "2509.22793", "pdf": "https://arxiv.org/pdf/2509.22793", "abs": "https://arxiv.org/abs/2509.22793", "authors": ["Komal Kumar", "Rao Muhammad Anwer", "Fahad Shahbaz Khan", "Salman Khan", "Ivan Laptev", "Hisham Cholakkal"], "title": "DEFT: Decompositional Efficient Fine-Tuning for Text-to-Image Models", "categories": ["cs.CV"], "comment": "13 Figures, 21 pages, accepted in NeurIPS 2025", "summary": "Efficient fine-tuning of pre-trained Text-to-Image (T2I) models involves adjusting the model to suit a particular task or dataset while minimizing computational resources and limiting the number of trainable parameters. However, it often faces challenges in striking a trade-off between aligning with the target distribution: learning a novel concept from a limited image for personalization and retaining the instruction ability needed for unifying multiple tasks, all while maintaining editability (aligning with a variety of prompts or in-context generation). In this work, we introduce DEFT, Decompositional Efficient Fine-Tuning, an efficient fine-tuning framework that adapts a pre-trained weight matrix by decomposing its update into two components with two trainable matrices: (1) a projection onto the complement of a low-rank subspace spanned by a low-rank matrix, and (2) a low-rank update. The single trainable low-rank matrix defines the subspace, while the other trainable low-rank matrix enables flexible parameter adaptation within that subspace. We conducted extensive experiments on the Dreambooth and Dreambench Plus datasets for personalization, the InsDet dataset for object and scene adaptation, and the VisualCloze dataset for a universal image generation framework through visual in-context learning with both Stable Diffusion and a unified model. Our results demonstrated state-of-the-art performance, highlighting the emergent properties of efficient fine-tuning. Our code is available on \\href{https://github.com/MAXNORM8650/DEFT}{DEFTBase}.", "AI": {"tldr": "DEFT\u662f\u4e00\u79cd\u5206\u89e3\u5f0f\u9ad8\u6548\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9884\u8bad\u7ec3\u6743\u91cd\u77e9\u9635\u7684\u66f4\u65b0\u5206\u89e3\u4e3a\u4e24\u4e2a\u53ef\u8bad\u7ec3\u77e9\u9635\uff1a\u4e00\u4e2a\u5728\u4f4e\u79e9\u5b50\u7a7a\u95f4\u8865\u96c6\u4e0a\u7684\u6295\u5f71\u548c\u4e00\u4e2a\u4f4e\u79e9\u66f4\u65b0\uff0c\u5b9e\u73b0\u4e86\u5728\u4e2a\u6027\u5316\u3001\u591a\u4efb\u52a1\u7edf\u4e00\u548c\u53ef\u7f16\u8f91\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u9ad8\u6548\u5fae\u8c03\u4e2d\u9762\u4e34\u7684\u6311\u6218\uff1a\u5728\u6709\u9650\u56fe\u50cf\u4e2d\u5b66\u4e60\u65b0\u6982\u5ff5\u3001\u4fdd\u6301\u591a\u4efb\u52a1\u6307\u4ee4\u80fd\u529b\u3001\u7ef4\u6301\u53ef\u7f16\u8f91\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u8ba1\u7b97\u8d44\u6e90\u548c\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "method": "DEFT\u6846\u67b6\u5c06\u9884\u8bad\u7ec3\u6743\u91cd\u77e9\u9635\u66f4\u65b0\u5206\u89e3\u4e3a\u4e24\u4e2a\u7ec4\u4ef6\uff1a1) \u5728\u7531\u4f4e\u79e9\u77e9\u9635\u5f20\u6210\u7684\u4f4e\u79e9\u5b50\u7a7a\u95f4\u8865\u96c6\u4e0a\u7684\u6295\u5f71\uff1b2) \u4f4e\u79e9\u66f4\u65b0\u3002\u5355\u4e2a\u53ef\u8bad\u7ec3\u4f4e\u79e9\u77e9\u9635\u5b9a\u4e49\u5b50\u7a7a\u95f4\uff0c\u53e6\u4e00\u4e2a\u53ef\u8bad\u7ec3\u4f4e\u79e9\u77e9\u9635\u5728\u8be5\u5b50\u7a7a\u95f4\u5185\u5b9e\u73b0\u7075\u6d3b\u53c2\u6570\u9002\u5e94\u3002", "result": "\u5728Dreambooth\u3001Dreambench Plus\u3001InsDet\u548cVisualCloze\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDEFT\u5728\u4e2a\u6027\u5316\u3001\u5bf9\u8c61\u573a\u666f\u9002\u5e94\u548c\u89c6\u89c9\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u901a\u7528\u56fe\u50cf\u751f\u6210\u6846\u67b6\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "DEFT\u6846\u67b6\u5c55\u793a\u4e86\u9ad8\u6548\u5fae\u8c03\u7684\u65b0\u5174\u7279\u6027\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u53ef\u7f16\u8f91\u6027\u7684\u540c\u65f6\uff0c\u6709\u6548\u5e73\u8861\u4e86\u76ee\u6807\u5206\u5e03\u5bf9\u9f50\u3001\u65b0\u6982\u5ff5\u5b66\u4e60\u548c\u591a\u4efb\u52a1\u7edf\u4e00\u80fd\u529b\u3002"}}
{"id": "2509.22799", "pdf": "https://arxiv.org/pdf/2509.22799", "abs": "https://arxiv.org/abs/2509.22799", "authors": ["Xuan He", "Dongfu Jiang", "Ping Nie", "Minghao Liu", "Zhengxuan Jiang", "Mingyi Su", "Wentao Ma", "Junru Lin", "Chun Ye", "Yi Lu", "Keming Wu", "Benjamin Schneider", "Quy Duc Do", "Zhuofeng Li", "Yiming Jia", "Yuxuan Zhang", "Guo Cheng", "Haozhe Wang", "Wangchunshu Zhou", "Qunshu Lin", "Yuanxing Zhang", "Ge Zhang", "Wenhao Huang", "Wenhu Chen"], "title": "VideoScore2: Think before You Score in Generative Video Evaluation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in text-to-video generation have produced increasingly realistic and diverse content, yet evaluating such videos remains a fundamental challenge due to their multi-faceted nature encompassing visual quality, semantic alignment, and physical consistency. Existing evaluators and reward models are limited to single opaque scores, lack interpretability, or provide only coarse analysis, making them insufficient for capturing the comprehensive nature of video quality assessment. We present VideoScore2, a multi-dimensional, interpretable, and human-aligned framework that explicitly evaluates visual quality, text-to-video alignment, and physical/common-sense consistency while producing detailed chain-of-thought rationales. Our model is trained on a large-scale dataset VideoFeedback2 containing 27,168 human-annotated videos with both scores and reasoning traces across three dimensions, using a two-stage pipeline of supervised fine-tuning followed by reinforcement learning with Group Relative Policy Optimization (GRPO) to enhance analytical robustness. Extensive experiments demonstrate that VideoScore2 achieves superior performance with 44.35 (+5.94) accuracy on our in-domain benchmark VideoScore-Bench-v2 and 50.37 (+4.32) average performance across four out-of-domain benchmarks (VideoGenReward-Bench, VideoPhy2, etc), while providing interpretable assessments that bridge the gap between evaluation and controllable generation through effective reward modeling for Best-of-N sampling. Project Page: https://tiger-ai-lab.github.io/VideoScore2/", "AI": {"tldr": "VideoScore2\u662f\u4e00\u4e2a\u591a\u7ef4\u5ea6\u3001\u53ef\u89e3\u91ca\u7684\u89c6\u9891\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u8bc4\u4f30\u89c6\u89c9\u8d28\u91cf\u3001\u6587\u672c-\u89c6\u9891\u5bf9\u9f50\u548c\u7269\u7406/\u5e38\u8bc6\u4e00\u81f4\u6027\uff0c\u63d0\u4f9b\u8be6\u7ec6\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u8bc4\u4f30\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u4e00\u4e0d\u900f\u660e\u5206\u6570\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u6216\u4ec5\u63d0\u4f9b\u7c97\u7565\u5206\u6790\uff0c\u65e0\u6cd5\u5168\u9762\u6355\u6349\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u7684\u591a\u65b9\u9762\u7279\u6027\u3002", "method": "\u4f7f\u7528\u5305\u542b27,168\u4e2a\u4eba\u5de5\u6807\u6ce8\u89c6\u9891\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6VideoFeedback2\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1a\u76d1\u7763\u5fae\u8c03\u540e\u4f7f\u7528\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316(GRPO)\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\uff0c\u4ee5\u589e\u5f3a\u5206\u6790\u9c81\u68d2\u6027\u3002", "result": "VideoScore2\u5728\u5185\u90e8\u57fa\u51c6VideoScore-Bench-v2\u4e0a\u8fbe\u523044.35\u7684\u51c6\u786e\u7387\uff08\u63d0\u53475.94\uff09\uff0c\u5728\u56db\u4e2a\u5916\u90e8\u57fa\u51c6\u4e0a\u5e73\u5747\u6027\u80fd\u8fbe\u523050.37\uff08\u63d0\u53474.32\uff09\u3002", "conclusion": "VideoScore2\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\uff0c\u901a\u8fc7\u6709\u6548\u7684\u5956\u52b1\u5efa\u6a21\u4e3a\u6700\u4f73N\u91c7\u6837\u642d\u5efa\u4e86\u8bc4\u4f30\u4e0e\u53ef\u63a7\u751f\u6210\u4e4b\u95f4\u7684\u6865\u6881\u3002"}}
{"id": "2509.22836", "pdf": "https://arxiv.org/pdf/2509.22836", "abs": "https://arxiv.org/abs/2509.22836", "authors": ["Roie Kazoom", "Alon Goldberg", "Hodaya Cohen", "Ofer Hadar"], "title": "Seeing Isn't Believing: Context-Aware Adversarial Patch Synthesis via Conditional GAN", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Adversarial patch attacks pose a severe threat to deep neural networks, yet most existing approaches rely on unrealistic white-box assumptions, untargeted objectives, or produce visually conspicuous patches that limit real-world applicability. In this work, we introduce a novel framework for fully controllable adversarial patch generation, where the attacker can freely choose both the input image x and the target class y target, thereby dictating the exact misclassification outcome. Our method combines a generative U-Net design with Grad-CAM-guided patch placement, enabling semantic-aware localization that maximizes attack effectiveness while preserving visual realism. Extensive experiments across convolutional networks (DenseNet-121, ResNet-50) and vision transformers (ViT-B/16, Swin-B/16, among others) demonstrate that our approach achieves state-of-the-art performance across all settings, with attack success rates (ASR) and target-class success (TCS) consistently exceeding 99%.   Importantly, we show that our method not only outperforms prior white-box attacks and untargeted baselines, but also surpasses existing non-realistic approaches that produce detectable artifacts. By simultaneously ensuring realism, targeted control, and black-box applicability-the three most challenging dimensions of patch-based attacks-our framework establishes a new benchmark for adversarial robustness research, bridging the gap between theoretical attack strength and practical stealthiness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u53ef\u63a7\u7684\u5bf9\u6297\u6027\u8865\u4e01\u751f\u6210\u6846\u67b6\uff0c\u653b\u51fb\u8005\u53ef\u4ee5\u81ea\u7531\u9009\u62e9\u8f93\u5165\u56fe\u50cf\u548c\u76ee\u6807\u7c7b\u522b\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u9519\u8bef\u5206\u7c7b\u7ed3\u679c\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u751f\u6210\u5f0fU-Net\u8bbe\u8ba1\u548cGrad-CAM\u5f15\u5bfc\u7684\u8865\u4e01\u653e\u7f6e\uff0c\u5728\u4fdd\u6301\u89c6\u89c9\u771f\u5b9e\u6027\u7684\u540c\u65f6\u6700\u5927\u5316\u653b\u51fb\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u6027\u8865\u4e01\u653b\u51fb\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u4e0d\u5207\u5b9e\u9645\u7684\u767d\u76d2\u5047\u8bbe\u3001\u65e0\u76ee\u6807\u653b\u51fb\u76ee\u6807\uff0c\u6216\u4ea7\u751f\u89c6\u89c9\u4e0a\u660e\u663e\u7684\u8865\u4e01\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u786e\u4fdd\u771f\u5b9e\u6027\u3001\u76ee\u6807\u63a7\u5236\uff0c\u53c8\u80fd\u5728\u9ed1\u76d2\u73af\u5883\u4e2d\u9002\u7528\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u751f\u6210\u5f0fU-Net\u67b6\u6784\u548cGrad-CAM\u5f15\u5bfc\u7684\u8865\u4e01\u653e\u7f6e\uff0c\u5b9e\u73b0\u8bed\u4e49\u611f\u77e5\u7684\u5b9a\u4f4d\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u89c6\u89c9\u771f\u5b9e\u6027\u7684\u540c\u65f6\u4f18\u5316\u653b\u51fb\u6548\u679c\u3002", "result": "\u5728\u5377\u79ef\u7f51\u7edc\u548c\u89c6\u89c9\u53d8\u6362\u5668\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u8bbe\u7f6e\u4e0b\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u653b\u51fb\u6210\u529f\u7387\u548c\u76ee\u6807\u7c7b\u522b\u6210\u529f\u7387\u5747\u8d85\u8fc799%\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u767d\u76d2\u653b\u51fb\u3001\u65e0\u76ee\u6807\u57fa\u7ebf\u548c\u4ea7\u751f\u53ef\u68c0\u6d4b\u4f2a\u5f71\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u540c\u65f6\u786e\u4fdd\u771f\u5b9e\u6027\u3001\u76ee\u6807\u63a7\u5236\u548c\u9ed1\u76d2\u9002\u7528\u6027\uff0c\u4e3a\u5bf9\u6297\u6027\u9c81\u68d2\u6027\u7814\u7a76\u5efa\u7acb\u4e86\u65b0\u57fa\u51c6\uff0c\u5f25\u5408\u4e86\u7406\u8bba\u653b\u51fb\u5f3a\u5ea6\u4e0e\u5b9e\u9645\u9690\u853d\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2509.22864", "pdf": "https://arxiv.org/pdf/2509.22864", "abs": "https://arxiv.org/abs/2509.22864", "authors": ["Yixuan Hu", "Yuxuan Xue", "Simon Klenk", "Daniel Cremers", "Gerard Pons-Moll"], "title": "ControlEvents: Controllable Synthesis of Event Camera Datawith Foundational Prior from Image Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, event cameras have gained significant attention due to their bio-inspired properties, such as high temporal resolution and high dynamic range. However, obtaining large-scale labeled ground-truth data for event-based vision tasks remains challenging and costly. In this paper, we present ControlEvents, a diffusion-based generative model designed to synthesize high-quality event data guided by diverse control signals such as class text labels, 2D skeletons, and 3D body poses. Our key insight is to leverage the diffusion prior from foundation models, such as Stable Diffusion, enabling high-quality event data generation with minimal fine-tuning and limited labeled data. Our method streamlines the data generation process and significantly reduces the cost of producing labeled event datasets. We demonstrate the effectiveness of our approach by synthesizing event data for visual recognition, 2D skeleton estimation, and 3D body pose estimation. Our experiments show that the synthesized labeled event data enhances model performance in all tasks. Additionally, our approach can generate events based on unseen text labels during training, illustrating the powerful text-based generation capabilities inherited from foundation models.", "AI": {"tldr": "ControlEvents\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u80fd\u591f\u901a\u8fc7\u6587\u672c\u6807\u7b7e\u30012D\u9aa8\u67b6\u548c3D\u8eab\u4f53\u59ff\u6001\u7b49\u63a7\u5236\u4fe1\u53f7\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u4e8b\u4ef6\u6570\u636e\uff0c\u663e\u8457\u964d\u4f4e\u4e8b\u4ef6\u6570\u636e\u6807\u6ce8\u6210\u672c\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u9ad8\u52a8\u6001\u8303\u56f4\u7b49\u751f\u7269\u542f\u53d1\u7279\u6027\uff0c\u4f46\u83b7\u53d6\u5927\u89c4\u6a21\u6807\u6ce8\u7684\u4e8b\u4ef6\u6570\u636e\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u4e14\u6210\u672c\u9ad8\u6602\u3002", "method": "\u5229\u7528Stable Diffusion\u7b49\u57fa\u7840\u6a21\u578b\u7684\u6269\u6563\u5148\u9a8c\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u5fae\u8c03\u548c\u6709\u9650\u6807\u6ce8\u6570\u636e\u5b9e\u73b0\u9ad8\u8d28\u91cf\u4e8b\u4ef6\u6570\u636e\u751f\u6210\uff0c\u652f\u6301\u6587\u672c\u6807\u7b7e\u30012D\u9aa8\u67b6\u548c3D\u59ff\u6001\u7b49\u591a\u79cd\u63a7\u5236\u4fe1\u53f7\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5408\u6210\u7684\u6807\u6ce8\u4e8b\u4ef6\u6570\u636e\u5728\u89c6\u89c9\u8bc6\u522b\u30012D\u9aa8\u67b6\u4f30\u8ba1\u548c3D\u59ff\u6001\u4f30\u8ba1\u7b49\u4efb\u52a1\u4e2d\u90fd\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u4e14\u80fd\u591f\u57fa\u4e8e\u8bad\u7ec3\u4e2d\u672a\u89c1\u8fc7\u7684\u6587\u672c\u6807\u7b7e\u751f\u6210\u4e8b\u4ef6\u6570\u636e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5316\u4e86\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6807\u6ce8\u4e8b\u4ef6\u6570\u636e\u96c6\u7684\u6210\u672c\uff0c\u5e76\u7ee7\u627f\u4e86\u57fa\u7840\u6a21\u578b\u5f3a\u5927\u7684\u57fa\u4e8e\u6587\u672c\u7684\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2509.22917", "pdf": "https://arxiv.org/pdf/2509.22917", "abs": "https://arxiv.org/abs/2509.22917", "authors": ["Yuelin Xin", "Yuheng Liu", "Xiaohui Xie", "Xinke Li"], "title": "Learning Unified Representation of 3D Gaussian Splatting", "categories": ["cs.CV", "I.4.10"], "comment": "18 pages, 9 figures, 2 tables", "summary": "A well-designed vectorized representation is crucial for the learning systems natively based on 3D Gaussian Splatting. While 3DGS enables efficient and explicit 3D reconstruction, its parameter-based representation remains hard to learn as features, especially for neural-network-based models. Directly feeding raw Gaussian parameters into learning frameworks fails to address the non-unique and heterogeneous nature of the Gaussian parameterization, yielding highly data-dependent models. This challenge motivates us to explore a more principled approach to represent 3D Gaussian Splatting in neural networks that preserves the underlying color and geometric structure while enforcing unique mapping and channel homogeneity. In this paper, we propose an embedding representation of 3DGS based on continuous submanifold fields that encapsulate the intrinsic information of Gaussian primitives, thereby benefiting the learning of 3DGS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fde\u7eed\u5b50\u6d41\u5f62\u573a\u76843D\u9ad8\u65af\u6cfc\u6e85\u5d4c\u5165\u8868\u793a\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u539f\u59cb\u9ad8\u65af\u53c2\u6570\u4f5c\u4e3a\u795e\u7ecf\u7f51\u7edc\u7279\u5f81\u65f6\u7684\u4e0d\u552f\u4e00\u6027\u548c\u5f02\u6784\u6027\u95ee\u9898\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u7684\u53c2\u6570\u5316\u8868\u793a\u96be\u4ee5\u4f5c\u4e3a\u795e\u7ecf\u7f51\u7edc\u7279\u5f81\u5b66\u4e60\uff0c\u56e0\u4e3a\u9ad8\u65af\u53c2\u6570\u5177\u6709\u975e\u552f\u4e00\u6027\u548c\u5f02\u6784\u6027\uff0c\u76f4\u63a5\u4f7f\u7528\u4f1a\u5bfc\u81f4\u6a21\u578b\u9ad8\u5ea6\u4f9d\u8d56\u6570\u636e\u3002", "method": "\u57fa\u4e8e\u8fde\u7eed\u5b50\u6d41\u5f62\u573a\u6784\u5efa3D\u9ad8\u65af\u6cfc\u6e85\u7684\u5d4c\u5165\u8868\u793a\uff0c\u5c01\u88c5\u9ad8\u65af\u57fa\u5143\u7684\u672c\u8d28\u4fe1\u606f\uff0c\u4fdd\u6301\u989c\u8272\u548c\u51e0\u4f55\u7ed3\u6784\u7684\u540c\u65f6\u786e\u4fdd\u552f\u4e00\u6620\u5c04\u548c\u901a\u9053\u540c\u8d28\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u4e3a3D\u9ad8\u65af\u6cfc\u6e85\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u539f\u5219\u6027\u7684\u8868\u793a\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u5d4c\u5165\u8868\u793a\u65b9\u6cd5\u80fd\u591f\u66f4\u597d\u5730\u652f\u6301\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u5b66\u4e60\u7cfb\u7edf\u3002"}}
{"id": "2509.22925", "pdf": "https://arxiv.org/pdf/2509.22925", "abs": "https://arxiv.org/abs/2509.22925", "authors": ["Yuanzhi Zhu", "Xi Wang", "St\u00e9phane Lathuili\u00e8re", "Vicky Kalogeiton"], "title": "Soft-Di[M]O: Improving One-Step Discrete Image Generation with Soft Embeddings", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "One-step generators distilled from Masked Diffusion Models (MDMs) compress multiple sampling steps into a single forward pass, enabling efficient text and image synthesis. However, they suffer two key limitations: they inherit modeling bias from the teacher, and their discrete token outputs block gradient flow, preventing post-distillation refinements such as adversarial training, reward-based fine-tuning, and Test-Time Embedding Optimization (TTEO). In this work, we introduce soft embeddings, a simple relaxation that replaces discrete tokens with the expected embeddings under the generator's output distribution. Soft embeddings preserve representation fidelity for one-step discrete generator while providing a fully differentiable continuous surrogate that is compatible with teacher backbones and tokenizer decoders. Integrating soft embeddings into the Di[M]O distillation framework (denoted Soft-Di[M]O) makes one-step generators end-to-end trainable and enables straightforward application of GAN-based refinement, differentiable reward fine-tuning, and TTEO. Empirically, across multiple MDM teachers (e.g., MaskBit, MaskGen), Soft-Di[M]O achieves state-of-the-art one-step results: improved class-to-image performance, a one-step FID of 1.56 on ImageNet-256 with GAN-based refinement, along with higher GenEval and HPS scores on text-to-image with reward fine-tuning, and further gains from TTEO.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u8f6f\u5d4c\u5165\u65b9\u6cd5\uff0c\u5c06\u4e00\u6b65\u751f\u6210\u5668\u4e2d\u7684\u79bb\u6563\u6807\u8bb0\u66ff\u6362\u4e3a\u751f\u6210\u5668\u8f93\u51fa\u5206\u5e03\u7684\u671f\u671b\u5d4c\u5165\uff0c\u89e3\u51b3\u4e86\u79bb\u6563\u6807\u8bb0\u963b\u788d\u68af\u5ea6\u6d41\u52a8\u7684\u95ee\u9898\uff0c\u4f7f\u4e00\u6b65\u751f\u6210\u5668\u53ef\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\u548c\u540e\u84b8\u998f\u4f18\u5316\u3002", "motivation": "\u89e3\u51b3\u4e00\u6b65\u751f\u6210\u5668\u5b58\u5728\u7684\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a\u7ee7\u627f\u6559\u5e08\u6a21\u578b\u7684\u5efa\u6a21\u504f\u5dee\uff0c\u4ee5\u53ca\u79bb\u6563\u6807\u8bb0\u8f93\u51fa\u963b\u788d\u68af\u5ea6\u6d41\u52a8\uff0c\u65e0\u6cd5\u8fdb\u884c\u5bf9\u6297\u8bad\u7ec3\u3001\u57fa\u4e8e\u5956\u52b1\u7684\u5fae\u8c03\u548c\u6d4b\u8bd5\u65f6\u5d4c\u5165\u4f18\u5316\u7b49\u540e\u84b8\u998f\u4f18\u5316\u3002", "method": "\u5f15\u5165\u8f6f\u5d4c\u5165\u65b9\u6cd5\uff0c\u7528\u751f\u6210\u5668\u8f93\u51fa\u5206\u5e03\u7684\u671f\u671b\u5d4c\u5165\u66ff\u6362\u79bb\u6563\u6807\u8bb0\uff0c\u4fdd\u6301\u8868\u793a\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u63d0\u4f9b\u5b8c\u5168\u53ef\u5fae\u7684\u8fde\u7eed\u66ff\u4ee3\u65b9\u6848\uff0c\u4e0e\u6559\u5e08\u9aa8\u5e72\u7f51\u7edc\u548c\u6807\u8bb0\u5668\u89e3\u7801\u5668\u517c\u5bb9\u3002", "result": "\u5728\u591a\u4e2aMDM\u6559\u5e08\u6a21\u578b\u4e0a\uff0cSoft-Di[M]O\u5b9e\u73b0\u4e86\u4e00\u6d41\u7684\u4e00\u6b65\u751f\u6210\u7ed3\u679c\uff1a\u6539\u8fdb\u7684\u7c7b\u5230\u56fe\u50cf\u6027\u80fd\uff0c\u5728ImageNet-256\u4e0a\u901a\u8fc7GAN\u4f18\u5316\u8fbe\u52301.56\u7684\u4e00\u6b65FID\uff0c\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u4e0a\u83b7\u5f97\u66f4\u9ad8\u7684GenEval\u548cHPS\u5206\u6570\uff0c\u5e76\u901a\u8fc7TTEO\u83b7\u5f97\u8fdb\u4e00\u6b65\u589e\u76ca\u3002", "conclusion": "\u8f6f\u5d4c\u5165\u65b9\u6cd5\u4f7f\u4e00\u6b65\u751f\u6210\u5668\u53ef\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u652f\u6301GAN\u4f18\u5316\u3001\u53ef\u5fae\u5956\u52b1\u5fae\u8c03\u548c\u6d4b\u8bd5\u65f6\u5d4c\u5165\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e00\u6b65\u751f\u6210\u5668\u7684\u6027\u80fd\u3002"}}
{"id": "2509.22930", "pdf": "https://arxiv.org/pdf/2509.22930", "abs": "https://arxiv.org/abs/2509.22930", "authors": ["Chenghan Yang", "Peng Zhou", "Dong-Sheng Zhang", "Yueyun Wang", "Hong-Bin Shen", "Xiaoyong Pan"], "title": "FishAI 2.0: Marine Fish Image Classification with Multi-modal Few-shot Learning", "categories": ["cs.CV"], "comment": null, "summary": "Traditional marine biological image recognition faces challenges of incomplete datasets and unsatisfactory model accuracy, particularly for few-shot conditions of rare species where data scarcity significantly hampers the performance. To address these issues, this study proposes an intelligent marine fish recognition framework, FishAI 2.0, integrating multimodal few-shot deep learning techniques with image generation for data augmentation. First, a hierarchical marine fish benchmark dataset, which provides a comprehensive data foundation for subsequent model training, is utilized to train the FishAI 2.0 model. To address the data scarcity of rare classes, the large language model DeepSeek was employed to generate high-quality textual descriptions, which are input into Stable Diffusion 2 for image augmentation through a hierarchical diffusion strategy that extracts latent encoding to construct a multimodal feature space. The enhanced visual-textual datasets were then fed into a Contrastive Language-Image Pre-Training (CLIP) based model, enabling robust few-shot image recognition. Experimental results demonstrate that FishAI 2.0 achieves a Top-1 accuracy of 91.67 percent and Top-5 accuracy of 97.97 percent at the family level, outperforming baseline CLIP and ViT models with a substantial margin for the minority classes with fewer than 10 training samples. To better apply FishAI 2.0 to real-world scenarios, at the genus and species level, FishAI 2.0 respectively achieves a Top-1 accuracy of 87.58 percent and 85.42 percent, demonstrating practical utility. In summary, FishAI 2.0 improves the efficiency and accuracy of marine fish identification and provides a scalable technical solution for marine ecological monitoring and conservation, highlighting its scientific value and practical applicability.", "AI": {"tldr": "FishAI 2.0\u662f\u4e00\u4e2a\u667a\u80fd\u6d77\u6d0b\u9c7c\u7c7b\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5c0f\u6837\u672c\u6df1\u5ea6\u5b66\u4e60\u548c\u56fe\u50cf\u751f\u6210\u6280\u672f\u89e3\u51b3\u7a00\u6709\u7269\u79cd\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u6d77\u6d0b\u751f\u7269\u56fe\u50cf\u8bc6\u522b\u9762\u4e34\u6570\u636e\u96c6\u4e0d\u5b8c\u6574\u548c\u6a21\u578b\u7cbe\u5ea6\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u7a00\u6709\u7269\u79cd\u7684\u5c0f\u6837\u672c\u6761\u4ef6\u4e0b\uff0c\u6570\u636e\u7a00\u7f3a\u4e25\u91cd\u5f71\u54cd\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u5206\u5c42\u6d77\u6d0b\u9c7c\u7c7b\u57fa\u51c6\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\uff0c\u5229\u7528DeepSeek\u751f\u6210\u9ad8\u8d28\u91cf\u6587\u672c\u63cf\u8ff0\uff0c\u901a\u8fc7Stable Diffusion 2\u8fdb\u884c\u56fe\u50cf\u589e\u5f3a\uff0c\u6784\u5efa\u591a\u6a21\u6001\u7279\u5f81\u7a7a\u95f4\uff0c\u5e76\u91c7\u7528\u57fa\u4e8eCLIP\u7684\u6a21\u578b\u8fdb\u884c\u5c0f\u6837\u672c\u56fe\u50cf\u8bc6\u522b\u3002", "result": "FishAI 2.0\u5728\u79d1\u7ea7\u522b\u8fbe\u5230Top-1\u51c6\u786e\u738791.67%\u548cTop-5\u51c6\u786e\u738797.97%\uff0c\u5728\u5c5e\u548c\u79cd\u7ea7\u522b\u5206\u522b\u8fbe\u523087.58%\u548c85.42%\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "FishAI 2.0\u63d0\u9ad8\u4e86\u6d77\u6d0b\u9c7c\u7c7b\u8bc6\u522b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u6d77\u6d0b\u751f\u6001\u76d1\u6d4b\u548c\u4fdd\u62a4\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u79d1\u5b66\u4ef7\u503c\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.23022", "pdf": "https://arxiv.org/pdf/2509.23022", "abs": "https://arxiv.org/abs/2509.23022", "authors": ["Xiafeng Man", "Zhipeng Wei", "Jingjing Chen"], "title": "Copyright Infringement Detection in Text-to-Image Diffusion Models via Differential Privacy", "categories": ["cs.CV"], "comment": null, "summary": "The widespread deployment of large vision models such as Stable Diffusion raises significant legal and ethical concerns, as these models can memorize and reproduce copyrighted content without authorization. Existing detection approaches often lack robustness and fail to provide rigorous theoretical underpinnings. To address these gaps, we formalize the concept of copyright infringement and its detection from the perspective of Differential Privacy (DP), and introduce the conditional sensitivity metric, a concept analogous to sensitivity in DP, that quantifies the deviation in a diffusion model's output caused by the inclusion or exclusion of a specific training data point. To operationalize this metric, we propose D-Plus-Minus (DPM), a novel post-hoc detection framework that identifies copyright infringement in text-to-image diffusion models. Specifically, DPM simulates inclusion and exclusion processes by fine-tuning models in two opposing directions: learning or unlearning. Besides, to disentangle concept-specific influence from the global parameter shifts induced by fine-tuning, DPM computes confidence scores over orthogonal prompt distributions using statistical metrics. Moreover, to facilitate standardized benchmarking, we also construct the Copyright Infringement Detection Dataset (CIDD), a comprehensive resource for evaluating detection across diverse categories. Our results demonstrate that DPM reliably detects infringement content without requiring access to the original training dataset or text prompts, offering an interpretable and practical solution for safeguarding intellectual property in the era of generative AI.", "AI": {"tldr": "\u63d0\u51fa\u4e86D-Plus-Minus (DPM)\u6846\u67b6\uff0c\u57fa\u4e8e\u5dee\u5206\u9690\u79c1\u7406\u8bba\u68c0\u6d4b\u6269\u6563\u6a21\u578b\u4e2d\u7684\u7248\u6743\u4fb5\u6743\u5185\u5bb9\uff0c\u65e0\u9700\u8bbf\u95ee\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u6216\u6587\u672c\u63d0\u793a\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u6a21\u578b\u5982Stable Diffusion\u53ef\u80fd\u8bb0\u5fc6\u5e76\u590d\u5236\u53d7\u7248\u6743\u4fdd\u62a4\u7684\u5185\u5bb9\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u9c81\u68d2\u6027\u548c\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5f15\u5165\u6761\u4ef6\u654f\u611f\u6027\u6307\u6807\uff0c\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u5728\u4e24\u4e2a\u76f8\u53cd\u65b9\u5411\uff08\u5b66\u4e60/\u9057\u5fd8\uff09\u6765\u6a21\u62df\u5305\u542b\u548c\u6392\u9664\u8fc7\u7a0b\uff0c\u5e76\u4f7f\u7528\u7edf\u8ba1\u6307\u6807\u5728\u6b63\u4ea4\u63d0\u793a\u5206\u5e03\u4e0a\u8ba1\u7b97\u7f6e\u4fe1\u5ea6\u5206\u6570\u3002", "result": "DPM\u80fd\u591f\u53ef\u9760\u5730\u68c0\u6d4b\u4fb5\u6743\u5185\u5bb9\uff0c\u5e76\u6784\u5efa\u4e86CIDD\u6570\u636e\u96c6\u7528\u4e8e\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "DPM\u4e3a\u751f\u6210AI\u65f6\u4ee3\u7684\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23025", "pdf": "https://arxiv.org/pdf/2509.23025", "abs": "https://arxiv.org/abs/2509.23025", "authors": ["Gabriel A. Viana", "Luis F. Alves Pereira", "Tsang Ing Ren", "George D. C. Cavalcanti", "Jan Sijbers"], "title": "Perceptual Influence: Improving the Perceptual Loss Design for Low-Dose CT Enhancement", "categories": ["cs.CV", "I.5.1; I.5.4; I.4.3; J.3"], "comment": null, "summary": "Perceptual losses have emerged as powerful tools for training networks to enhance Low-Dose Computed Tomography (LDCT) images, offering an alternative to traditional pixel-wise losses such as Mean Squared Error, which often lead to over-smoothed reconstructions and loss of clinically relevant details in LDCT images. The perceptual losses operate in a latent feature space defined by a pretrained encoder and aim to preserve semantic content by comparing high-level features rather than raw pixel values. However, the design of perceptual losses involves critical yet underexplored decisions, including the feature representation level, the dataset used to pretrain the encoder, and the relative importance assigned to the perceptual component during optimization. In this work, we introduce the concept of perceptual influence (a metric that quantifies the relative contribution of the perceptual loss term to the total loss) and propose a principled framework to assess the impact of the loss design choices on the model training performance. Through systematic experimentation, we show that the widely used configurations in the literature to set up a perceptual loss underperform compared to better-designed alternatives. Our findings show that better perceptual loss designs lead to significant improvements in noise reduction and structural fidelity of reconstructed CT images, without requiring any changes to the network architecture. We also provide objective guidelines, supported by statistical analysis, to inform the effective use of perceptual losses in LDCT denoising. Our source code is available at https://github.com/vngabriel/perceptual-influence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u611f\u77e5\u5f71\u54cd\u5ea6\u6982\u5ff5\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u611f\u77e5\u635f\u5931\u5728\u4f4e\u5242\u91cfCT\u56fe\u50cf\u589e\u5f3a\u4e2d\u7684\u8bbe\u8ba1\u9009\u62e9\uff0c\u53d1\u73b0\u6587\u732e\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u914d\u7f6e\u8868\u73b0\u4e0d\u4f73\uff0c\u4f18\u5316\u540e\u7684\u611f\u77e5\u635f\u5931\u8bbe\u8ba1\u80fd\u663e\u8457\u6539\u5584\u91cd\u5efaCT\u56fe\u50cf\u7684\u8d28\u91cf\u3002", "motivation": "\u611f\u77e5\u635f\u5931\u5df2\u6210\u4e3a\u8bad\u7ec3\u7f51\u7edc\u589e\u5f3a\u4f4e\u5242\u91cfCT\u56fe\u50cf\u7684\u6709\u529b\u5de5\u5177\uff0c\u4f46\u611f\u77e5\u635f\u5931\u7684\u8bbe\u8ba1\u6d89\u53ca\u5173\u952e\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u51b3\u7b56\uff0c\u5305\u62ec\u7279\u5f81\u8868\u793a\u5c42\u7ea7\u3001\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u6570\u636e\u96c6\u4ee5\u53ca\u611f\u77e5\u5206\u91cf\u7684\u76f8\u5bf9\u91cd\u8981\u6027\u3002", "method": "\u5f15\u5165\u611f\u77e5\u5f71\u54cd\u5ea6\u6982\u5ff5\uff08\u91cf\u5316\u611f\u77e5\u635f\u5931\u9879\u5bf9\u603b\u635f\u5931\u7684\u76f8\u5bf9\u8d21\u732e\uff09\uff0c\u63d0\u51fa\u539f\u5219\u6027\u6846\u67b6\u8bc4\u4f30\u635f\u5931\u8bbe\u8ba1\u9009\u62e9\u5bf9\u6a21\u578b\u8bad\u7ec3\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u6bd4\u8f83\u4e0d\u540c\u914d\u7f6e\u3002", "result": "\u6587\u732e\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u611f\u77e5\u635f\u5931\u914d\u7f6e\u8868\u73b0\u4e0d\u5982\u4f18\u5316\u8bbe\u8ba1\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u66f4\u597d\u7684\u611f\u77e5\u635f\u5931\u8bbe\u8ba1\u80fd\u663e\u8457\u6539\u5584\u91cd\u5efaCT\u56fe\u50cf\u7684\u566a\u58f0\u964d\u4f4e\u548c\u7ed3\u6784\u4fdd\u771f\u5ea6\uff0c\u65e0\u9700\u6539\u53d8\u7f51\u7edc\u67b6\u6784\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u57fa\u4e8e\u7edf\u8ba1\u5206\u6790\u7684\u76ee\u6807\u6307\u5bfc\u539f\u5219\uff0c\u652f\u6301\u611f\u77e5\u635f\u5931\u5728\u4f4e\u5242\u91cfCT\u53bb\u566a\u4e2d\u7684\u6709\u6548\u4f7f\u7528\uff0c\u8bc1\u660e\u4e86\u4f18\u5316\u611f\u77e5\u635f\u5931\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.23082", "pdf": "https://arxiv.org/pdf/2509.23082", "abs": "https://arxiv.org/abs/2509.23082", "authors": ["Yutao Shen", "Junkun Yuan", "Toru Aonishi", "Hideki Nakayama", "Yue Ma"], "title": "Follow-Your-Preference: Towards Preference-Aligned Image Inpainting", "categories": ["cs.CV"], "comment": "16 pages,9 figures, 4 tables", "summary": "This paper investigates image inpainting with preference alignment. Instead of introducing a novel method, we go back to basics and revisit fundamental problems in achieving such alignment. We leverage the prominent direct preference optimization approach for alignment training and employ public reward models to construct preference training datasets. Experiments are conducted across nine reward models, two benchmarks, and two baseline models with varying structures and generative algorithms. Our key findings are as follows: (1) Most reward models deliver valid reward scores for constructing preference data, even if some of them are not reliable evaluators. (2) Preference data demonstrates robust trends in both candidate scaling and sample scaling across models and benchmarks. (3) Observable biases in reward models, particularly in brightness, composition, and color scheme, render them susceptible to cause reward hacking. (4) A simple ensemble of these models yields robust and generalizable results by mitigating such biases. Built upon these observations, our alignment models significantly outperform prior models across standard metrics, GPT-4 assessments, and human evaluations, without any changes to model structures or the use of new datasets. We hope our work can set a simple yet solid baseline, pushing this promising frontier. Our code is open-sourced at: https://github.com/shenytzzz/Follow-Your-Preference.", "AI": {"tldr": "\u8be5\u8bba\u6587\u91cd\u65b0\u5ba1\u89c6\u56fe\u50cf\u4fee\u590d\u4e2d\u7684\u504f\u597d\u5bf9\u9f50\u95ee\u9898\uff0c\u4f7f\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u8fdb\u884c\u5bf9\u9f50\u8bad\u7ec3\uff0c\u5e76\u5229\u7528\u516c\u5171\u5956\u52b1\u6a21\u578b\u6784\u5efa\u504f\u597d\u8bad\u7ec3\u6570\u636e\u96c6\u3002\u7814\u7a76\u53d1\u73b0\u5927\u591a\u6570\u5956\u52b1\u6a21\u578b\u80fd\u63d0\u4f9b\u6709\u6548\u5956\u52b1\u5206\u6570\uff0c\u504f\u597d\u6570\u636e\u5728\u4e0d\u540c\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u8d8b\u52bf\uff0c\u4f46\u5956\u52b1\u6a21\u578b\u5b58\u5728\u53ef\u89c2\u5bdf\u7684\u504f\u89c1\u3002\u901a\u8fc7\u7b80\u5355\u96c6\u6210\u8fd9\u4e9b\u6a21\u578b\u53ef\u4ee5\u7f13\u89e3\u504f\u89c1\uff0c\u83b7\u5f97\u7a33\u5065\u548c\u6cdb\u5316\u6027\u5f3a\u7684\u7ed3\u679c\u3002", "motivation": "\u91cd\u65b0\u5ba1\u89c6\u56fe\u50cf\u4fee\u590d\u4e2d\u504f\u597d\u5bf9\u9f50\u7684\u57fa\u672c\u95ee\u9898\uff0c\u800c\u4e0d\u662f\u5f15\u5165\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u5efa\u7acb\u4e00\u4e2a\u7b80\u5355\u800c\u575a\u5b9e\u7684\u57fa\u51c6\u7ebf\u3002", "method": "\u5229\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u8fdb\u884c\u5bf9\u9f50\u8bad\u7ec3\uff0c\u4f7f\u7528\u516c\u5171\u5956\u52b1\u6a21\u578b\u6784\u5efa\u504f\u597d\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5728\u4e5d\u4e2a\u5956\u52b1\u6a21\u578b\u3001\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e24\u4e2a\u57fa\u7ebf\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u91c7\u7528\u7b80\u5355\u96c6\u6210\u7b56\u7565\u7f13\u89e3\u5956\u52b1\u6a21\u578b\u504f\u89c1\u3002", "result": "\u5bf9\u9f50\u6a21\u578b\u5728\u6807\u51c6\u6307\u6807\u3001GPT-4\u8bc4\u4f30\u548c\u4eba\u5de5\u8bc4\u4f30\u4e2d\u663e\u8457\u4f18\u4e8e\u5148\u524d\u6a21\u578b\uff0c\u65e0\u9700\u6539\u53d8\u6a21\u578b\u7ed3\u6784\u6216\u4f7f\u7528\u65b0\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u56fe\u50cf\u4fee\u590d\u4e2d\u7684\u504f\u597d\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684\u57fa\u51c6\u7ebf\uff0c\u901a\u8fc7\u7f13\u89e3\u5956\u52b1\u6a21\u578b\u504f\u89c1\u83b7\u5f97\u4e86\u7a33\u5065\u548c\u6cdb\u5316\u6027\u5f3a\u7684\u7ed3\u679c\u3002"}}
{"id": "2509.23122", "pdf": "https://arxiv.org/pdf/2509.23122", "abs": "https://arxiv.org/abs/2509.23122", "authors": ["Chenrui Ma", "Xi Xiao", "Tianyang Wang", "Xiao Wang", "Yanning Shen"], "title": "Stochastic Interpolants via Conditional Dependent Coupling", "categories": ["cs.CV"], "comment": null, "summary": "Existing image generation models face critical challenges regarding the trade-off between computation and fidelity. Specifically, models relying on a pretrained Variational Autoencoder (VAE) suffer from information loss, limited detail, and the inability to support end-to-end training. In contrast, models operating directly in the pixel space incur prohibitive computational cost. Although cascade models can mitigate computational cost, stage-wise separation prevents effective end-to-end optimization, hampers knowledge sharing, and often results in inaccurate distribution learning within each stage. To address these challenges, we introduce a unified multistage generative framework based on our proposed Conditional Dependent Coupling strategy. It decomposes the generative process into interpolant trajectories at multiple stages, ensuring accurate distribution learning while enabling end-to-end optimization. Importantly, the entire process is modeled as a single unified Diffusion Transformer, eliminating the need for disjoint modules and also enabling knowledge sharing. Extensive experiments demonstrate that our method achieves both high fidelity and efficiency across multiple resolutions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6761\u4ef6\u4f9d\u8d56\u8026\u5408\u7b56\u7565\u7684\u7edf\u4e00\u591a\u9636\u6bb5\u751f\u6210\u6846\u67b6\uff0c\u89e3\u51b3\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u8ba1\u7b97\u6210\u672c\u4e0e\u4fdd\u771f\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u63d2\u503c\u8f68\u8ff9\u5206\u89e3\u751f\u6210\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u4f18\u5316\u548c\u77e5\u8bc6\u5171\u4eab\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u751f\u6210\u6a21\u578b\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u4e0e\u4fdd\u771f\u5ea6\u7684\u6743\u8861\u6311\u6218\uff1a\u57fa\u4e8e\u9884\u8bad\u7ec3VAE\u7684\u6a21\u578b\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u548c\u7ec6\u8282\u6709\u9650\u95ee\u9898\uff0c\u50cf\u7d20\u7a7a\u95f4\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u7ea7\u8054\u6a21\u578b\u65e0\u6cd5\u7aef\u5230\u7aef\u4f18\u5316\u4e14\u5404\u9636\u6bb5\u5206\u5e03\u5b66\u4e60\u4e0d\u51c6\u786e\u3002", "method": "\u63d0\u51fa\u6761\u4ef6\u4f9d\u8d56\u8026\u5408\u7b56\u7565\uff0c\u5c06\u751f\u6210\u8fc7\u7a0b\u5206\u89e3\u4e3a\u591a\u9636\u6bb5\u63d2\u503c\u8f68\u8ff9\uff0c\u4f7f\u7528\u7edf\u4e00\u7684\u6269\u6563Transformer\u5efa\u6a21\u6574\u4e2a\u8fc7\u7a0b\uff0c\u907f\u514d\u5206\u79bb\u6a21\u5757\u5e76\u5b9e\u73b0\u77e5\u8bc6\u5171\u4eab\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u5206\u8fa8\u7387\u4e0b\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u548c\u9ad8\u6548\u7387\u3002", "conclusion": "\u8be5\u7edf\u4e00\u591a\u9636\u6bb5\u751f\u6210\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u56fe\u50cf\u751f\u6210\u4e2d\u8ba1\u7b97\u6548\u7387\u4e0e\u4fdd\u771f\u5ea6\u7684\u6743\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u4f18\u5316\u548c\u77e5\u8bc6\u5171\u4eab\u5b9e\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2509.23150", "pdf": "https://arxiv.org/pdf/2509.23150", "abs": "https://arxiv.org/abs/2509.23150", "authors": ["Wenxuan Fang", "Jiangwei Weng", "Jianjun Qian", "Jian Yang", "Jun Li"], "title": "WeatherCycle: Unpaired Multi-Weather Restoration via Color Space Decoupled Cycle Learning", "categories": ["cs.CV"], "comment": null, "summary": "Unsupervised image restoration under multi-weather conditions remains a fundamental yet underexplored challenge. While existing methods often rely on task-specific physical priors, their narrow focus limits scalability and generalization to diverse real-world weather scenarios. In this work, we propose \\textbf{WeatherCycle}, a unified unpaired framework that reformulates weather restoration as a bidirectional degradation-content translation cycle, guided by degradation-aware curriculum regularization. At its core, WeatherCycle employs a \\textit{lumina-chroma decomposition} strategy to decouple degradation from content without modeling complex weather, enabling domain conversion between degraded and clean images. To model diverse and complex degradations, we propose a \\textit{Lumina Degradation Guidance Module} (LDGM), which learns luminance degradation priors from a degraded image pool and injects them into clean images via frequency-domain amplitude modulation, enabling controllable and realistic degradation modeling. Additionally, we incorporate a \\textit{Difficulty-Aware Contrastive Regularization (DACR)} module that identifies hard samples via a CLIP-based classifier and enforces contrastive alignment between hard samples and restored features to enhance semantic consistency and robustness. Extensive experiments across serve multi-weather datasets, demonstrate that our method achieves state-of-the-art performance among unsupervised approaches, with strong generalization to complex weather degradations.", "AI": {"tldr": "WeatherCycle\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u975e\u914d\u5bf9\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5411\u964d\u89e3-\u5185\u5bb9\u8f6c\u6362\u5faa\u73af\u548c\u964d\u89e3\u611f\u77e5\u8bfe\u7a0b\u6b63\u5219\u5316\u6765\u89e3\u51b3\u591a\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u56fe\u50cf\u6062\u590d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u7279\u5b9a\u4efb\u52a1\u7684\u7269\u7406\u5148\u9a8c\uff0c\u5176\u72ed\u7a84\u7684\u7126\u70b9\u9650\u5236\u4e86\u5728\u591a\u6837\u5316\u771f\u5b9e\u5929\u6c14\u573a\u666f\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u4eae\u5ea6-\u8272\u5ea6\u5206\u89e3\u7b56\u7565\u89e3\u8026\u964d\u89e3\u4e0e\u5185\u5bb9\uff0c\u63d0\u51fa\u4eae\u5ea6\u964d\u89e3\u5f15\u5bfc\u6a21\u5757(LDGM)\u5b66\u4e60\u4eae\u5ea6\u964d\u89e3\u5148\u9a8c\uff0c\u5e76\u5f15\u5165\u96be\u5ea6\u611f\u77e5\u5bf9\u6bd4\u6b63\u5219\u5316(DACR)\u6a21\u5757\u589e\u5f3a\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u591a\u5929\u6c14\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u65e0\u76d1\u7763\u65b9\u6cd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5bf9\u590d\u6742\u5929\u6c14\u964d\u89e3\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "WeatherCycle\u901a\u8fc7\u7edf\u4e00\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u56fe\u50cf\u6062\u590d\u95ee\u9898\uff0c\u65e0\u9700\u590d\u6742\u5929\u6c14\u5efa\u6a21\u5373\u53ef\u5b9e\u73b0\u964d\u89e3\u4e0e\u5185\u5bb9\u7684\u89e3\u8026\u3002"}}
{"id": "2509.23169", "pdf": "https://arxiv.org/pdf/2509.23169", "abs": "https://arxiv.org/abs/2509.23169", "authors": ["Bolin Chen", "Ru-Ling Liao", "Yan Ye", "Jie Chen", "Shanzhi Yin", "Xinrui Ju", "Shiqi Wang", "Yibo Fan"], "title": "Sparse2Dense: A Keypoint-driven Generative Framework for Human Video Compression and Vertex Prediction", "categories": ["cs.CV"], "comment": null, "summary": "For bandwidth-constrained multimedia applications, simultaneously achieving ultra-low bitrate human video compression and accurate vertex prediction remains a critical challenge, as it demands the harmonization of dynamic motion modeling, detailed appearance synthesis, and geometric consistency. To address this challenge, we propose Sparse2Dense, a keypoint-driven generative framework that leverages extremely sparse 3D keypoints as compact transmitted symbols to enable ultra-low bitrate human video compression and precise human vertex prediction. The key innovation is the multi-task learning-based and keypoint-aware deep generative model, which could encode complex human motion via compact 3D keypoints and leverage these sparse keypoints to estimate dense motion for video synthesis with temporal coherence and realistic textures. Additionally, a vertex predictor is integrated to learn human vertex geometry through joint optimization with video generation, ensuring alignment between visual content and geometric structure. Extensive experiments demonstrate that the proposed Sparse2Dense framework achieves competitive compression performance for human video over traditional/generative video codecs, whilst enabling precise human vertex prediction for downstream geometry applications. As such, Sparse2Dense is expected to facilitate bandwidth-efficient human-centric media transmission, such as real-time motion analysis, virtual human animation, and immersive entertainment.", "AI": {"tldr": "Sparse2Dense\u662f\u4e00\u4e2a\u57fa\u4e8e\u5173\u952e\u70b9\u7684\u751f\u6210\u6846\u67b6\uff0c\u5229\u7528\u6781\u7a00\u758f\u76843D\u5173\u952e\u70b9\u4f5c\u4e3a\u4f20\u8f93\u7b26\u53f7\uff0c\u5b9e\u73b0\u8d85\u4f4e\u7801\u7387\u4eba\u4f53\u89c6\u9891\u538b\u7f29\u548c\u7cbe\u786e\u4eba\u4f53\u9876\u70b9\u9884\u6d4b\u3002", "motivation": "\u89e3\u51b3\u5e26\u5bbd\u53d7\u9650\u591a\u5a92\u4f53\u5e94\u7528\u4e2d\u540c\u65f6\u5b9e\u73b0\u8d85\u4f4e\u7801\u7387\u4eba\u4f53\u89c6\u9891\u538b\u7f29\u548c\u51c6\u786e\u9876\u70b9\u9884\u6d4b\u7684\u6311\u6218\uff0c\u9700\u8981\u534f\u8c03\u52a8\u6001\u8fd0\u52a8\u5efa\u6a21\u3001\u7ec6\u8282\u5916\u89c2\u5408\u6210\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u5173\u952e\u70b9\u611f\u77e5\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u7d27\u51d13D\u5173\u952e\u70b9\u7f16\u7801\u590d\u6742\u4eba\u4f53\u8fd0\u52a8\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u7a00\u758f\u5173\u952e\u70b9\u4f30\u8ba1\u5bc6\u96c6\u8fd0\u52a8\u4ee5\u5b9e\u73b0\u65f6\u95f4\u4e00\u81f4\u548c\u771f\u5b9e\u7eb9\u7406\u7684\u89c6\u9891\u5408\u6210\u3002\u96c6\u6210\u9876\u70b9\u9884\u6d4b\u5668\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5b66\u4e60\u4eba\u4f53\u9876\u70b9\u51e0\u4f55\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSparse2Dense\u5728\u4eba\u4f53\u89c6\u9891\u538b\u7f29\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf/\u751f\u6210\u89c6\u9891\u7f16\u89e3\u7801\u5668\uff0c\u540c\u65f6\u80fd\u591f\u5b9e\u73b0\u7cbe\u786e\u7684\u4eba\u4f53\u9876\u70b9\u9884\u6d4b\u3002", "conclusion": "Sparse2Dense\u6709\u671b\u4fc3\u8fdb\u5e26\u5bbd\u9ad8\u6548\u7684\u4eba\u4f53\u4e2d\u5fc3\u5a92\u4f53\u4f20\u8f93\uff0c\u5982\u5b9e\u65f6\u8fd0\u52a8\u5206\u6790\u3001\u865a\u62df\u4eba\u4f53\u52a8\u753b\u548c\u6c89\u6d78\u5f0f\u5a31\u4e50\u3002"}}
{"id": "2509.23225", "pdf": "https://arxiv.org/pdf/2509.23225", "abs": "https://arxiv.org/abs/2509.23225", "authors": ["Alisher Myrgyyassov", "Zhen Song", "Yu Sun", "Bruce Xiao Wang", "Min Ney Wong", "Yongping Zheng"], "title": "UltraUNet: Real-Time Ultrasound Tongue Segmentation for Diverse Linguistic and Imaging Conditions", "categories": ["cs.CV", "cs.LG"], "comment": "16 pages, 8 figures", "summary": "Ultrasound tongue imaging (UTI) is a non-invasive and cost-effective tool for studying speech articulation, motor control, and related disorders. However, real-time tongue contour segmentation remains challenging due to low signal-to-noise ratios, imaging variability, and computational demands. We propose UltraUNet, a lightweight encoder-decoder architecture optimized for real-time segmentation of tongue contours in ultrasound images. UltraUNet incorporates domain-specific innovations such as lightweight Squeeze-and-Excitation blocks, Group Normalization for small-batch stability, and summation-based skip connections to reduce memory and computational overhead. It achieves 250 frames per second and integrates ultrasound-specific augmentations like denoising and blur simulation. Evaluations on 8 datasets demonstrate high accuracy and robustness, with single-dataset Dice = 0.855 and MSD = 0.993px, and cross-dataset Dice averaging 0.734 and 0.761. UltraUNet provides a fast, accurate solution for speech research, clinical diagnostics, and analysis of speech motor disorders.", "AI": {"tldr": "\u63d0\u51faUltraUNet\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u7528\u4e8e\u5b9e\u65f6\u8d85\u58f0\u820c\u8f6e\u5ed3\u5206\u5272\uff0c\u8fbe\u5230250\u5e27/\u79d2\u901f\u5ea6\uff0c\u57288\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u8d85\u58f0\u820c\u6210\u50cf\u662f\u4e00\u79cd\u975e\u4fb5\u5165\u6027\u3001\u6210\u672c\u6548\u76ca\u9ad8\u7684\u5de5\u5177\uff0c\u4f46\u5b9e\u65f6\u820c\u8f6e\u5ed3\u5206\u5272\u9762\u4e34\u4fe1\u566a\u6bd4\u4f4e\u3001\u6210\u50cf\u53d8\u5f02\u6027\u548c\u8ba1\u7b97\u9700\u6c42\u7b49\u6311\u6218\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5305\u542b\u9886\u57df\u7279\u5b9a\u521b\u65b0\uff1a\u8f7b\u91cf\u7ea7Squeeze-and-Excitation\u5757\u3001\u5c0f\u6279\u91cf\u7a33\u5b9a\u6027\u7684Group Normalization\u3001\u57fa\u4e8e\u6c42\u548c\u7684\u8df3\u8dc3\u8fde\u63a5\u4ee5\u51cf\u5c11\u5185\u5b58\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u96c6\u6210\u8d85\u58f0\u7279\u5b9a\u589e\u5f3a\u5982\u53bb\u566a\u548c\u6a21\u7cca\u6a21\u62df\u3002", "result": "\u57288\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u663e\u793a\u9ad8\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5355\u6570\u636e\u96c6Dice=0.855\u3001MSD=0.993px\uff0c\u8de8\u6570\u636e\u96c6Dice\u5e73\u5747\u4e3a0.734\u548c0.761\u3002", "conclusion": "UltraUNet\u4e3a\u8bed\u97f3\u7814\u7a76\u3001\u4e34\u5e8a\u8bca\u65ad\u548c\u8a00\u8bed\u8fd0\u52a8\u969c\u788d\u5206\u6790\u63d0\u4f9b\u4e86\u5feb\u901f\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23258", "pdf": "https://arxiv.org/pdf/2509.23258", "abs": "https://arxiv.org/abs/2509.23258", "authors": ["Atakan Topaloglu", "Kunyi Li", "Michael Niemeyer", "Nassir Navab", "A. Murat Tekalp", "Federico Tombari"], "title": "OracleGS: Grounding Generative Priors for Sparse-View Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Sparse-view novel view synthesis is fundamentally ill-posed due to severe geometric ambiguity. Current methods are caught in a trade-off: regressive models are geometrically faithful but incomplete, whereas generative models can complete scenes but often introduce structural inconsistencies. We propose OracleGS, a novel framework that reconciles generative completeness with regressive fidelity for sparse view Gaussian Splatting. Instead of using generative models to patch incomplete reconstructions, our \"propose-and-validate\" framework first leverages a pre-trained 3D-aware diffusion model to synthesize novel views to propose a complete scene. We then repurpose a multi-view stereo (MVS) model as a 3D-aware oracle to validate the 3D uncertainties of generated views, using its attention maps to reveal regions where the generated views are well-supported by multi-view evidence versus where they fall into regions of high uncertainty due to occlusion, lack of texture, or direct inconsistency. This uncertainty signal directly guides the optimization of a 3D Gaussian Splatting model via an uncertainty-weighted loss. Our approach conditions the powerful generative prior on multi-view geometric evidence, filtering hallucinatory artifacts while preserving plausible completions in under-constrained regions, outperforming state-of-the-art methods on datasets including Mip-NeRF 360 and NeRF Synthetic.", "AI": {"tldr": "OracleGS\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u6a21\u578b\u7684\u5b8c\u6574\u6027\u548c\u56de\u5f52\u6a21\u578b\u7684\u51e0\u4f55\u4fdd\u771f\u5ea6\uff0c\u89e3\u51b3\u4e86\u7a00\u758f\u89c6\u56fe\u65b0\u89c6\u89d2\u5408\u6210\u4e2d\u7684\u51e0\u4f55\u6a21\u7cca\u95ee\u9898\u3002\u5b83\u4f7f\u7528\u9884\u8bad\u7ec3\u76843D\u611f\u77e5\u6269\u6563\u6a21\u578b\u751f\u6210\u5b8c\u6574\u573a\u666f\uff0c\u7136\u540e\u5229\u7528\u591a\u89c6\u89d2\u7acb\u4f53\u6a21\u578b\u4f5c\u4e3a3D\u611f\u77e5oracle\u9a8c\u8bc1\u751f\u6210\u89c6\u56fe\u76843D\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u635f\u5931\u6307\u5bfc3D\u9ad8\u65af\u6e85\u5c04\u4f18\u5316\u3002", "motivation": "\u7a00\u758f\u89c6\u56fe\u65b0\u89c6\u89d2\u5408\u6210\u5b58\u5728\u4e25\u91cd\u7684\u51e0\u4f55\u6a21\u7cca\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u9677\u5165\u4e24\u96be\uff1a\u56de\u5f52\u6a21\u578b\u51e0\u4f55\u4fdd\u771f\u4f46\u4e0d\u5b8c\u6574\uff0c\u751f\u6210\u6a21\u578b\u80fd\u8865\u5168\u573a\u666f\u4f46\u5e38\u5f15\u5165\u7ed3\u6784\u4e0d\u4e00\u81f4\u3002\u9700\u8981\u4e00\u79cd\u80fd\u517c\u987e\u751f\u6210\u5b8c\u6574\u6027\u548c\u56de\u5f52\u4fdd\u771f\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\"\u63d0\u8bae-\u9a8c\u8bc1\"\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u9884\u8bad\u7ec33D\u611f\u77e5\u6269\u6563\u6a21\u578b\u5408\u6210\u65b0\u89c6\u89d2\u4ee5\u751f\u6210\u5b8c\u6574\u573a\u666f\uff1b2\uff09\u5c06\u591a\u89c6\u89d2\u7acb\u4f53\u6a21\u578b\u91cd\u65b0\u7528\u4f5c3D\u611f\u77e5oracle\uff0c\u901a\u8fc7\u5176\u6ce8\u610f\u529b\u56fe\u9a8c\u8bc1\u751f\u6210\u89c6\u56fe\u76843D\u4e0d\u786e\u5b9a\u6027\uff1b3\uff09\u4f7f\u7528\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\u901a\u8fc7\u52a0\u6743\u635f\u5931\u6307\u5bfc3D\u9ad8\u65af\u6e85\u5c04\u4f18\u5316\u3002", "result": "\u5728Mip-NeRF 360\u548cNeRF Synthetic\u7b49\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5c06\u5f3a\u5927\u7684\u751f\u6210\u5148\u9a8c\u6761\u4ef6\u5316\u4e8e\u591a\u89c6\u89d2\u51e0\u4f55\u8bc1\u636e\uff0c\u8fc7\u6ee4\u5e7b\u89c9\u4f2a\u5f71\uff0c\u540c\u65f6\u5728\u7ea6\u675f\u4e0d\u8db3\u533a\u57df\u4fdd\u7559\u5408\u7406\u8865\u5168\u3002", "conclusion": "OracleGS\u6210\u529f\u8c03\u548c\u4e86\u751f\u6210\u5b8c\u6574\u6027\u4e0e\u56de\u5f52\u4fdd\u771f\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u901a\u8fc7\u6761\u4ef6\u5316\u751f\u6210\u5148\u9a8c\u4e8e\u591a\u89c6\u89d2\u51e0\u4f55\u8bc1\u636e\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u53ef\u9760\u7684\u7a00\u758f\u89c6\u56fe\u65b0\u89c6\u89d2\u5408\u6210\u3002"}}
{"id": "2509.23279", "pdf": "https://arxiv.org/pdf/2509.23279", "abs": "https://arxiv.org/abs/2509.23279", "authors": ["Rohit Chowdhury", "Aniruddha Bala", "Rohan Jaiswal", "Siddharth Roheda"], "title": "Vid-Freeze: Protecting Images from Malicious Image-to-Video Generation via Temporal Freezing", "categories": ["cs.CV", "cs.AI"], "comment": "Under Review at ICASSP 26 4 pages, 4 figures, 3 tables", "summary": "The rapid progress of image-to-video (I2V) generation models has introduced significant risks, enabling video synthesis from static images and facilitating deceptive or malicious content creation. While prior defenses such as I2VGuard attempt to immunize images, effective and principled protection to block motion remains underexplored. In this work, we introduce Vid-Freeze - a novel attention-suppressing adversarial attack that adds carefully crafted adversarial perturbations to images. Our method explicitly targets the attention mechanism of I2V models, completely disrupting motion synthesis while preserving semantic fidelity of the input image. The resulting immunized images generate stand-still or near-static videos, effectively blocking malicious content creation. Our experiments demonstrate the impressive protection provided by the proposed approach, highlighting the importance of attention attacks as a promising direction for robust and proactive defenses against misuse of I2V generation models.", "AI": {"tldr": "Vid-Freeze\u662f\u4e00\u79cd\u9488\u5bf9\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6dfb\u52a0\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5bf9\u6297\u6270\u52a8\u6765\u6291\u5236\u6ce8\u610f\u529b\u673a\u5236\uff0c\u963b\u6b62\u8fd0\u52a8\u5408\u6210\uff0c\u4ece\u800c\u4fdd\u62a4\u56fe\u50cf\u4e0d\u88ab\u6076\u610f\u4f7f\u7528\u3002", "motivation": "\u968f\u7740\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4ece\u9759\u6001\u56fe\u50cf\u5408\u6210\u89c6\u9891\u5e26\u6765\u4e86\u4e25\u91cd\u98ce\u9669\uff0c\u53ef\u80fd\u88ab\u7528\u4e8e\u521b\u5efa\u6b3a\u9a97\u6027\u6216\u6076\u610f\u5185\u5bb9\u3002\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5982I2VGuard\u5c1d\u8bd5\u514d\u75ab\u56fe\u50cf\uff0c\u4f46\u6709\u6548\u963b\u6b62\u8fd0\u52a8\u7684\u65b9\u6cd5\u4ecd\u6709\u5f85\u63a2\u7d22\u3002", "method": "\u63d0\u51faVid-Freeze\u65b9\u6cd5\uff0c\u901a\u8fc7\u6dfb\u52a0\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5bf9\u6297\u6270\u52a8\u6765\u663e\u5f0f\u653b\u51fbI2V\u6a21\u578b\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b8c\u5168\u7834\u574f\u8fd0\u52a8\u5408\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u8f93\u5165\u56fe\u50cf\u7684\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u51fa\u8272\u7684\u4fdd\u62a4\u6548\u679c\uff0c\u514d\u75ab\u540e\u7684\u56fe\u50cf\u751f\u6210\u9759\u6b62\u6216\u63a5\u8fd1\u9759\u6b62\u7684\u89c6\u9891\uff0c\u6709\u6548\u963b\u6b62\u6076\u610f\u5185\u5bb9\u521b\u5efa\u3002", "conclusion": "\u6ce8\u610f\u529b\u653b\u51fb\u662f\u9632\u6b62I2V\u751f\u6210\u6a21\u578b\u88ab\u6ee5\u7528\u7684\u4e00\u4e2a\u6709\u6548\u4e14\u6709\u524d\u666f\u7684\u9632\u5fa1\u65b9\u5411\uff0c\u4e3a\u9c81\u68d2\u548c\u4e3b\u52a8\u7684\u9632\u5fa1\u63d0\u4f9b\u4e86\u91cd\u8981\u601d\u8def\u3002"}}
{"id": "2509.23304", "pdf": "https://arxiv.org/pdf/2509.23304", "abs": "https://arxiv.org/abs/2509.23304", "authors": ["Liwen Hu", "Yang Li", "Mianzhi Liu", "Yijia Guo", "Shenghao Xie", "Ziluo Ding", "Tiejun Huang", "Lei Ma"], "title": "Seeing the Unseen in Low-light Spike Streams", "categories": ["cs.CV"], "comment": null, "summary": "Spike camera, a type of neuromorphic sensor with high-temporal resolution, shows great promise for high-speed visual tasks. Unlike traditional cameras, spike camera continuously accumulates photons and fires asynchronous spike streams. Due to unique data modality, spike streams require reconstruction methods to become perceptible to the human eye.   However, lots of methods struggle to handle spike streams in low-light high-speed scenarios due to severe noise and sparse information. In this work, we propose Diff-SPK, the first diffusion-based reconstruction method for spike camera. Diff-SPK effectively leverages generative priors to supplement texture information in low-light conditions. Specifically, it first employs an \\textbf{E}nhanced \\textbf{T}exture \\textbf{f}rom Inter-spike \\textbf{I}nterval (ETFI) to aggregate sparse information from low-light spike streams. Then, ETFI serves as a conditioning input for ControlNet to generate the high-speed scenes. To improve the quality of results, we introduce an ETFI-based feature fusion module during the generation process.   Moreover, we establish the first bona fide benchmark for the low-light spike stream reconstruction task. It significantly surpasses existing reconstruction datasets in scale and provides quantitative illumination information. The performance on real low-light spike streams demonstrates the superiority of Diff-SPK.", "AI": {"tldr": "Diff-SPK\u662f\u9996\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8109\u51b2\u76f8\u673a\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7ETFI\u6280\u672f\u805a\u5408\u4f4e\u5149\u6761\u4ef6\u4e0b\u7a00\u758f\u7684\u8109\u51b2\u6d41\u4fe1\u606f\uff0c\u5229\u7528ControlNet\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u9ad8\u901f\u573a\u666f\u56fe\u50cf\uff0c\u5728\u4f4e\u5149\u9ad8\u901f\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u8109\u51b2\u76f8\u673a\u91cd\u5efa\u65b9\u6cd5\u5728\u4f4e\u5149\u9ad8\u901f\u573a\u666f\u4e0b\u96be\u4ee5\u5904\u7406\u4e25\u91cd\u7684\u566a\u58f0\u548c\u7a00\u758f\u4fe1\u606f\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u8865\u5145\u7eb9\u7406\u4fe1\u606f\u3002", "method": "\u63d0\u51faETFI\uff08\u589e\u5f3a\u7eb9\u7406\u4ece\u8109\u51b2\u95f4\u9694\uff09\u6280\u672f\u805a\u5408\u4f4e\u5149\u8109\u51b2\u6d41\u4fe1\u606f\uff0c\u4f5c\u4e3aControlNet\u7684\u6761\u4ef6\u8f93\u5165\u751f\u6210\u9ad8\u901f\u573a\u666f\uff0c\u5e76\u5f15\u5165ETFI\u7279\u5f81\u878d\u5408\u6a21\u5757\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "result": "\u5728\u771f\u5b9e\u4f4e\u5149\u8109\u51b2\u6d41\u4e0a\u7684\u5b9e\u9a8c\u8868\u660eDiff-SPK\u5177\u6709\u4f18\u8d8a\u6027\u80fd\uff0c\u540c\u65f6\u5efa\u7acb\u4e86\u9996\u4e2a\u4f4e\u5149\u8109\u51b2\u6d41\u91cd\u5efa\u57fa\u51c6\u6570\u636e\u96c6\u3002", "conclusion": "Diff-SPK\u6210\u529f\u5229\u7528\u751f\u6210\u5148\u9a8c\u8865\u5145\u4f4e\u5149\u6761\u4ef6\u4e0b\u7684\u7eb9\u7406\u4fe1\u606f\uff0c\u4e3a\u8109\u51b2\u76f8\u673a\u5728\u4f4e\u5149\u9ad8\u901f\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23310", "pdf": "https://arxiv.org/pdf/2509.23310", "abs": "https://arxiv.org/abs/2509.23310", "authors": ["Hao Liu", "Yongjie Zheng", "Yuhan Kang", "Mingyang Zhang", "Maoguo Gong", "Lorenzo Bruzzone"], "title": "Balanced Diffusion-Guided Fusion for Multimodal Remote Sensing Classification", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning-based techniques for the analysis of multimodal remote sensing data have become popular due to their ability to effectively integrate complementary spatial, spectral, and structural information from different sensors. Recently, denoising diffusion probabilistic models (DDPMs) have attracted attention in the remote sensing community due to their powerful ability to capture robust and complex spatial-spectral distributions. However, pre-training multimodal DDPMs may result in modality imbalance, and effectively leveraging diffusion features to guide complementary diversity feature extraction remains an open question. To address these issues, this paper proposes a balanced diffusion-guided fusion (BDGF) framework that leverages multimodal diffusion features to guide a multi-branch network for land-cover classification. Specifically, we propose an adaptive modality masking strategy to encourage the DDPMs to obtain a modality-balanced rather than spectral image-dominated data distribution. Subsequently, these diffusion features hierarchically guide feature extraction among CNN, Mamba, and transformer networks by integrating feature fusion, group channel attention, and cross-attention mechanisms. Finally, a mutual learning strategy is developed to enhance inter-branch collaboration by aligning the probability entropy and feature similarity of individual subnetworks. Extensive experiments on four multimodal remote sensing datasets demonstrate that the proposed method achieves superior classification performance. The code is available at https://github.com/HaoLiu-XDU/BDGF.", "AI": {"tldr": "\u63d0\u51fa\u5e73\u8861\u6269\u6563\u5f15\u5bfc\u878d\u5408\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u6269\u6563\u7279\u5f81\u6307\u5bfc\u591a\u5206\u652f\u7f51\u7edc\u8fdb\u884c\u571f\u5730\u8986\u76d6\u5206\u7c7b\uff0c\u89e3\u51b3\u6a21\u6001\u4e0d\u5e73\u8861\u95ee\u9898\u5e76\u63d0\u5347\u5206\u7c7b\u6027\u80fd", "motivation": "\u591a\u6a21\u6001DDPM\u9884\u8bad\u7ec3\u5b58\u5728\u6a21\u6001\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5982\u4f55\u6709\u6548\u5229\u7528\u6269\u6563\u7279\u5f81\u6307\u5bfc\u4e92\u8865\u591a\u6837\u6027\u7279\u5f81\u63d0\u53d6\u4ecd\u5f85\u89e3\u51b3", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u6a21\u6001\u63a9\u7801\u7b56\u7565\u4f7fDDPM\u83b7\u5f97\u5e73\u8861\u6570\u636e\u5206\u5e03\uff0c\u901a\u8fc7\u7279\u5f81\u878d\u5408\u3001\u7ec4\u901a\u9053\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5206\u5c42\u6307\u5bfcCNN\u3001Mamba\u548cTransformer\u7f51\u7edc\u7684\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u5f00\u53d1\u4e92\u5b66\u4e60\u7b56\u7565\u589e\u5f3a\u5206\u652f\u95f4\u534f\u4f5c", "result": "\u5728\u56db\u4e2a\u591a\u6a21\u6001\u9065\u611f\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u5206\u7c7b\u6027\u80fd", "conclusion": "BDGF\u6846\u67b6\u901a\u8fc7\u5e73\u8861\u6269\u6563\u7279\u5f81\u5f15\u5bfc\u548c\u591a\u5206\u652f\u534f\u4f5c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u9065\u611f\u6570\u636e\u5206\u7c7b\u4e2d\u7684\u6a21\u6001\u4e0d\u5e73\u8861\u95ee\u9898"}}
{"id": "2509.23402", "pdf": "https://arxiv.org/pdf/2509.23402", "abs": "https://arxiv.org/abs/2509.23402", "authors": ["Ziyue Zhu", "Zhanqian Wu", "Zhenxin Zhu", "Lijun Zhou", "Haiyang Sun", "Bing Wan", "Kun Ma", "Guang Chen", "Hangjun Ye", "Jin Xie", "jian Yang"], "title": "WorldSplat: Gaussian-Centric Feed-Forward 4D Scene Generation for Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in driving-scene generation and reconstruction have demonstrated significant potential for enhancing autonomous driving systems by producing scalable and controllable training data. Existing generation methods primarily focus on synthesizing diverse and high-fidelity driving videos; however, due to limited 3D consistency and sparse viewpoint coverage, they struggle to support convenient and high-quality novel-view synthesis (NVS). Conversely, recent 3D/4D reconstruction approaches have significantly improved NVS for real-world driving scenes, yet inherently lack generative capabilities. To overcome this dilemma between scene generation and reconstruction, we propose \\textbf{WorldSplat}, a novel feed-forward framework for 4D driving-scene generation. Our approach effectively generates consistent multi-track videos through two key steps: ((i)) We introduce a 4D-aware latent diffusion model integrating multi-modal information to produce pixel-aligned 4D Gaussians in a feed-forward manner. ((ii)) Subsequently, we refine the novel view videos rendered from these Gaussians using a enhanced video diffusion model. Extensive experiments conducted on benchmark datasets demonstrate that \\textbf{WorldSplat} effectively generates high-fidelity, temporally and spatially consistent multi-track novel view driving videos.", "AI": {"tldr": "WorldSplat\u662f\u4e00\u4e2a\u7528\u4e8e4D\u9a7e\u9a76\u573a\u666f\u751f\u6210\u7684\u521b\u65b0\u6846\u67b6\uff0c\u901a\u8fc74D\u611f\u77e5\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u589e\u5f3a\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u65f6\u7a7a\u4e00\u81f4\u7684\u591a\u89c6\u89d2\u9a7e\u9a76\u89c6\u9891\u3002", "motivation": "\u73b0\u6709\u7684\u9a7e\u9a76\u573a\u666f\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5408\u6210\u591a\u6837\u5316\u548c\u9ad8\u4fdd\u771f\u5ea6\u7684\u9a7e\u9a76\u89c6\u9891\uff0c\u4f46\u7f3a\u4e4f3D\u4e00\u81f4\u6027\u548c\u7a00\u758f\u89c6\u89d2\u8986\u76d6\uff0c\u96be\u4ee5\u652f\u6301\u9ad8\u8d28\u91cf\u7684\u65b0\u89c6\u89d2\u5408\u6210\u3002\u800c3D/4D\u91cd\u5efa\u65b9\u6cd5\u867d\u7136\u6539\u8fdb\u4e86\u771f\u5b9e\u9a7e\u9a76\u573a\u666f\u7684\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u4f46\u7f3a\u4e4f\u751f\u6210\u80fd\u529b\u3002", "method": "\u91c7\u7528\u524d\u9988\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6b65\u9aa4\uff1a(i) \u5f15\u51654D\u611f\u77e5\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\u4ee5\u524d\u9988\u65b9\u5f0f\u751f\u6210\u50cf\u7d20\u5bf9\u9f50\u76844D\u9ad8\u65af\uff1b(ii) \u4f7f\u7528\u589e\u5f3a\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u5bf9\u8fd9\u4e9b\u9ad8\u65af\u6e32\u67d3\u7684\u65b0\u89c6\u89d2\u89c6\u9891\u8fdb\u884c\u7ec6\u5316\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cWorldSplat\u80fd\u591f\u6709\u6548\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u3001\u65f6\u7a7a\u4e00\u81f4\u7684\u591a\u89c6\u89d2\u9a7e\u9a76\u89c6\u9891\u3002", "conclusion": "WorldSplat\u6210\u529f\u514b\u670d\u4e86\u573a\u666f\u751f\u6210\u4e0e\u91cd\u5efa\u4e4b\u95f4\u7684\u56f0\u5883\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u53ef\u63a7\u7684\u8bad\u7ec3\u6570\u636e\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23438", "pdf": "https://arxiv.org/pdf/2509.23438", "abs": "https://arxiv.org/abs/2509.23438", "authors": ["Mohammed Alsakabi", "Wael Mobeirek", "John M. Dolan", "Ozan K. Tonguz"], "title": "FM-SIREN & FM-FINER: Nyquist-Informed Frequency Multiplier for Implicit Neural Representation with Periodic Activation", "categories": ["cs.CV"], "comment": null, "summary": "Existing periodic activation-based implicit neural representation (INR) networks, such as SIREN and FINER, suffer from hidden feature redundancy, where neurons within a layer capture overlapping frequency components due to the use of a fixed frequency multiplier. This redundancy limits the expressive capacity of multilayer perceptrons (MLPs). Drawing inspiration from classical signal processing methods such as the Discrete Sine Transform (DST), we propose FM-SIREN and FM-FINER, which assign Nyquist-informed, neuron-specific frequency multipliers to periodic activations. Unlike existing approaches, our design introduces frequency diversity without requiring hyperparameter tuning or additional network depth. This simple yet principled modification reduces the redundancy of features by nearly 50% and consistently improves signal reconstruction across diverse INR tasks, including fitting 1D audio, 2D image and 3D shape, and synthesis of neural radiance fields (NeRF), outperforming their baseline counterparts while maintaining efficiency.", "AI": {"tldr": "\u63d0\u51faFM-SIREN\u548cFM-FINER\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3a\u5468\u671f\u6027\u6fc0\u6d3b\u51fd\u6570\u5206\u914d\u795e\u7ecf\u5143\u7279\u5b9a\u7684\u9891\u7387\u4e58\u5b50\uff0c\u51cf\u5c11\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u4e2d\u7684\u7279\u5f81\u5197\u4f59\uff0c\u63d0\u5347\u4fe1\u53f7\u91cd\u5efa\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5468\u671f\u6027\u6fc0\u6d3b\u7684\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u7f51\u7edc\u5b58\u5728\u9690\u85cf\u7279\u5f81\u5197\u4f59\u95ee\u9898\uff0c\u540c\u4e00\u5c42\u4e2d\u7684\u795e\u7ecf\u5143\u7531\u4e8e\u4f7f\u7528\u56fa\u5b9a\u9891\u7387\u4e58\u5b50\u800c\u6355\u83b7\u91cd\u53e0\u7684\u9891\u7387\u5206\u91cf\uff0c\u9650\u5236\u4e86\u591a\u5c42\u611f\u77e5\u673a\u7684\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u53d7\u79bb\u6563\u6b63\u5f26\u53d8\u6362\u7b49\u7ecf\u5178\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5\u542f\u53d1\uff0c\u4e3a\u5468\u671f\u6027\u6fc0\u6d3b\u51fd\u6570\u5206\u914d\u57fa\u4e8e\u5948\u594e\u65af\u7279\u9891\u7387\u7684\u795e\u7ecf\u5143\u7279\u5b9a\u9891\u7387\u4e58\u5b50\uff0c\u5f15\u5165\u9891\u7387\u591a\u6837\u6027\u800c\u4e0d\u9700\u8981\u8d85\u53c2\u6570\u8c03\u4f18\u6216\u589e\u52a0\u7f51\u7edc\u6df1\u5ea6\u3002", "result": "\u8be5\u65b9\u6cd5\u5c06\u7279\u5f81\u5197\u4f59\u51cf\u5c11\u8fd150%\uff0c\u57281D\u97f3\u9891\u30012D\u56fe\u50cf\u30013D\u5f62\u72b6\u62df\u5408\u4ee5\u53ca\u795e\u7ecf\u8f90\u5c04\u573a\u5408\u6210\u7b49\u591a\u79cdINR\u4efb\u52a1\u4e2d\u6301\u7eed\u6539\u5584\u4fe1\u53f7\u91cd\u5efa\u6027\u80fd\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u540c\u65f6\u4fdd\u6301\u6548\u7387\u3002", "conclusion": "\u8fd9\u79cd\u7b80\u5355\u800c\u539f\u5219\u6027\u7684\u4fee\u6539\u901a\u8fc7\u51cf\u5c11\u7279\u5f81\u5197\u4f59\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5468\u671f\u6027\u6fc0\u6d3b\u57fa\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2509.23457", "pdf": "https://arxiv.org/pdf/2509.23457", "abs": "https://arxiv.org/abs/2509.23457", "authors": ["Mohammad Hossein Sameti", "Amir M. Mansourian", "Arash Marioriyad", "Soheil Fadaee Oshyani", "Mohammad Hossein Rohban", "Mahdieh Soleymani Baghshah"], "title": "No Concept Left Behind: Test-Time Optimization for Compositional Text-to-Image Generation", "categories": ["cs.CV"], "comment": "8 pages, 8 figures, 1 table", "summary": "Despite recent advances in text-to-image (T2I) models, they often fail to faithfully render all elements of complex prompts, frequently omitting or misrepresenting specific objects and attributes. Test-time optimization has emerged as a promising approach to address this limitation by refining generation without the need for retraining. In this paper, we propose a fine-grained test-time optimization framework that enhances compositional faithfulness in T2I generation. Unlike most of prior approaches that rely solely on a global image/text similarity score, our method decomposes the input prompt into semantic concepts and evaluates alignment at both the global and concept levels. A fine-grained variant of CLIP is used to compute concept-level correspondence, producing detailed feedback on missing or inaccurate concepts. This feedback is fed into an iterative prompt refinement loop, enabling the large language model to propose improved prompts. Experiments on DrawBench and CompBench prompts demonstrate that our method significantly improves concept coverage and human-judged faithfulness over both standard test-time optimization and the base T2I model. Code is available at: https://github.com/AmirMansurian/NoConceptLeftBehind", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec6\u7c92\u5ea6\u7684\u6d4b\u8bd5\u65f6\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u63d0\u793a\u5206\u89e3\u4e3a\u8bed\u4e49\u6982\u5ff5\u5e76\u5728\u5168\u5c40\u548c\u6982\u5ff5\u7ea7\u522b\u8bc4\u4f30\u5bf9\u9f50\uff0c\u4f7f\u7528\u7ec6\u7c92\u5ea6CLIP\u8ba1\u7b97\u6982\u5ff5\u7ea7\u5bf9\u5e94\u5173\u7cfb\uff0c\u901a\u8fc7\u8fed\u4ee3\u63d0\u793a\u4f18\u5316\u5faa\u73af\u63d0\u9ad8\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u7ec4\u5408\u5fe0\u5b9e\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u63d0\u793a\u65f6\u7ecf\u5e38\u65e0\u6cd5\u5fe0\u5b9e\u6e32\u67d3\u6240\u6709\u5143\u7d20\uff0c\u4f1a\u9057\u6f0f\u6216\u9519\u8bef\u8868\u793a\u7279\u5b9a\u5bf9\u8c61\u548c\u5c5e\u6027\uff0c\u9700\u8981\u6539\u8fdb\u7ec4\u5408\u5fe0\u5b9e\u5ea6\u3002", "method": "\u5c06\u8f93\u5165\u63d0\u793a\u5206\u89e3\u4e3a\u8bed\u4e49\u6982\u5ff5\uff0c\u5728\u5168\u5c40\u548c\u6982\u5ff5\u7ea7\u522b\u8bc4\u4f30\u5bf9\u9f50\uff0c\u4f7f\u7528\u7ec6\u7c92\u5ea6CLIP\u8ba1\u7b97\u6982\u5ff5\u7ea7\u5bf9\u5e94\u5173\u7cfb\uff0c\u901a\u8fc7\u8fed\u4ee3\u63d0\u793a\u4f18\u5316\u5faa\u73af\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u51fa\u6539\u8fdb\u7684\u63d0\u793a\u3002", "result": "\u5728DrawBench\u548cCompBench\u63d0\u793a\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6982\u5ff5\u8986\u76d6\u7387\u548c\u4eba\u7c7b\u8bc4\u5224\u7684\u5fe0\u5b9e\u5ea6\uff0c\u4f18\u4e8e\u6807\u51c6\u6d4b\u8bd5\u65f6\u4f18\u5316\u548c\u57fa\u7840\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u7ec6\u7c92\u5ea6\u6d4b\u8bd5\u65f6\u4f18\u5316\u6846\u67b6\u6709\u6548\u63d0\u9ad8\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u7ec4\u5408\u5fe0\u5b9e\u5ea6\uff0c\u901a\u8fc7\u6982\u5ff5\u7ea7\u8bc4\u4f30\u548c\u8fed\u4ee3\u63d0\u793a\u4f18\u5316\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6982\u5ff5\u8986\u76d6\u548c\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2509.23480", "pdf": "https://arxiv.org/pdf/2509.23480", "abs": "https://arxiv.org/abs/2509.23480", "authors": ["Shourya Verma", "Mengbo Wang", "Nadia Atallah Lanman", "Ananth Grama"], "title": "RestoRect: Degraded Image Restoration via Latent Rectified Flow & Feature Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Current approaches for restoration of degraded images face a critical trade-off: high-performance models are too slow for practical use, while fast models produce poor results. Knowledge distillation transfers teacher knowledge to students, but existing static feature matching methods cannot capture how modern transformer architectures dynamically generate features. We propose 'RestoRect', a novel Latent Rectified Flow Feature Distillation method for restoring degraded images. We apply rectified flow to reformulate feature distillation as a generative process where students learn to synthesize teacher-quality features through learnable trajectories in latent space. Our framework combines Retinex theory for physics-based decomposition with learnable anisotropic diffusion constraints, and trigonometric color space polarization. We introduce a Feature Layer Extraction loss for robust knowledge transfer between different network architectures through cross-normalized transformer feature alignment with percentile-based outlier detection. RestoRect achieves better training stability, and faster convergence and inference while preserving restoration quality. We demonstrate superior results across 15 image restoration datasets, covering 4 tasks, on 8 metrics.", "AI": {"tldr": "\u63d0\u51faRestoRect\u65b9\u6cd5\uff0c\u901a\u8fc7\u6f5c\u5728\u6574\u6d41\u6d41\u7279\u5f81\u84b8\u998f\u89e3\u51b3\u56fe\u50cf\u6062\u590d\u4e2d\u901f\u5ea6\u4e0e\u6027\u80fd\u7684\u6743\u8861\u95ee\u9898\uff0c\u7ed3\u5408Retinex\u7406\u8bba\u3001\u53ef\u5b66\u4e60\u5404\u5411\u5f02\u6027\u6269\u6563\u548c\u4e09\u89d2\u8272\u5f69\u7a7a\u95f4\u6781\u5316\uff0c\u5b9e\u73b0\u5feb\u901f\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u6062\u590d\u3002", "motivation": "\u5f53\u524d\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u9762\u4e34\u5173\u952e\u6743\u8861\uff1a\u9ad8\u6027\u80fd\u6a21\u578b\u901f\u5ea6\u592a\u6162\uff0c\u5feb\u901f\u6a21\u578b\u6548\u679c\u5dee\u3002\u73b0\u6709\u9759\u6001\u7279\u5f81\u5339\u914d\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u73b0\u4ee3transformer\u67b6\u6784\u7684\u52a8\u6001\u7279\u5f81\u751f\u6210\u8fc7\u7a0b\u3002", "method": "\u5e94\u7528\u6574\u6d41\u6d41\u5c06\u7279\u5f81\u84b8\u998f\u91cd\u65b0\u8868\u8ff0\u4e3a\u751f\u6210\u8fc7\u7a0b\uff0c\u5b66\u751f\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u53ef\u5b66\u4e60\u8f68\u8ff9\u5b66\u4e60\u5408\u6210\u6559\u5e08\u8d28\u91cf\u7279\u5f81\u3002\u7ed3\u5408Retinex\u7406\u8bba\u8fdb\u884c\u7269\u7406\u5206\u89e3\u3001\u53ef\u5b66\u4e60\u5404\u5411\u5f02\u6027\u6269\u6563\u7ea6\u675f\u548c\u4e09\u89d2\u8272\u5f69\u7a7a\u95f4\u6781\u5316\uff0c\u5f15\u5165\u7279\u5f81\u5c42\u63d0\u53d6\u635f\u5931\u5b9e\u73b0\u4e0d\u540c\u7f51\u7edc\u67b6\u6784\u95f4\u7684\u9c81\u68d2\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "\u572815\u4e2a\u56fe\u50cf\u6062\u590d\u6570\u636e\u96c6\u30014\u4e2a\u4efb\u52a1\u30018\u4e2a\u6307\u6807\u4e0a\u5c55\u793a\u4e86\u4f18\u8d8a\u7ed3\u679c\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u3001\u66f4\u5feb\u7684\u6536\u655b\u548c\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6062\u590d\u8d28\u91cf\u3002", "conclusion": "RestoRect\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u56fe\u50cf\u6062\u590d\u4e2d\u901f\u5ea6\u4e0e\u6027\u80fd\u7684\u6743\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u6f5c\u5728\u6574\u6d41\u6d41\u7279\u5f81\u84b8\u998f\u5b9e\u73b0\u4e86\u5feb\u901f\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u6062\u590d\u3002"}}
{"id": "2509.23492", "pdf": "https://arxiv.org/pdf/2509.23492", "abs": "https://arxiv.org/abs/2509.23492", "authors": ["Junyi Wu", "Jiachen Tao", "Haoxuan Wang", "Gaowen Liu", "Ramana Rao Kompella", "Yan Yan"], "title": "Orientation-anchored Hyper-Gaussian for 4D Reconstruction from Casual Videos", "categories": ["cs.CV"], "comment": "NeurIPS 2025. Code: \\href{https://github.com/adreamwu/OriGS}{OriGS}", "summary": "We present Orientation-anchored Gaussian Splatting (OriGS), a novel framework for high-quality 4D reconstruction from casually captured monocular videos. While recent advances extend 3D Gaussian Splatting to dynamic scenes via various motion anchors, such as graph nodes or spline control points, they often rely on low-rank assumptions and fall short in modeling complex, region-specific deformations inherent to unconstrained dynamics. OriGS addresses this by introducing a hyperdimensional representation grounded in scene orientation. We first estimate a Global Orientation Field that propagates principal forward directions across space and time, serving as stable structural guidance for dynamic modeling. Built upon this, we propose Orientation-aware Hyper-Gaussian, a unified formulation that embeds time, space, geometry, and orientation into a coherent probabilistic state. This enables inferring region-specific deformation through principled conditioned slicing, adaptively capturing diverse local dynamics in alignment with global motion intent. Experiments demonstrate the superior reconstruction fidelity of OriGS over mainstream methods in challenging real-world dynamic scenes.", "AI": {"tldr": "OriGS\u662f\u4e00\u79cd\u57fa\u4e8e\u573a\u666f\u65b9\u5411\u76844D\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u65b9\u5411\u573a\u548c\u65b9\u5411\u611f\u77e5\u8d85\u9ad8\u65af\u8868\u793a\uff0c\u5728\u5355\u76ee\u89c6\u9891\u4e2d\u5b9e\u73b0\u9ad8\u8d28\u91cf\u52a8\u6001\u573a\u666f\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u52a8\u6001\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\u4f9d\u8d56\u4f4e\u79e9\u5047\u8bbe\uff0c\u96be\u4ee5\u5efa\u6a21\u65e0\u7ea6\u675f\u52a8\u6001\u4e2d\u590d\u6742\u7684\u533a\u57df\u7279\u5b9a\u53d8\u5f62\u3002", "method": "\u9996\u5148\u4f30\u8ba1\u5168\u5c40\u65b9\u5411\u573a\u4f5c\u4e3a\u7ed3\u6784\u6307\u5bfc\uff0c\u7136\u540e\u63d0\u51fa\u65b9\u5411\u611f\u77e5\u8d85\u9ad8\u65af\u8868\u793a\uff0c\u5c06\u65f6\u95f4\u3001\u7a7a\u95f4\u3001\u51e0\u4f55\u548c\u65b9\u5411\u5d4c\u5165\u7edf\u4e00\u6982\u7387\u72b6\u6001\uff0c\u901a\u8fc7\u6761\u4ef6\u5207\u7247\u63a8\u65ad\u533a\u57df\u7279\u5b9a\u53d8\u5f62\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u771f\u5b9e\u4e16\u754c\u52a8\u6001\u573a\u666f\u4e2d\uff0cOriGS\u76f8\u6bd4\u4e3b\u6d41\u65b9\u6cd5\u5177\u6709\u66f4\u4f18\u7684\u91cd\u5efa\u4fdd\u771f\u5ea6\u3002", "conclusion": "OriGS\u901a\u8fc7\u57fa\u4e8e\u65b9\u5411\u7684\u8868\u793a\u6210\u529f\u89e3\u51b3\u4e86\u590d\u6742\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e2d\u7684\u533a\u57df\u7279\u5b9a\u53d8\u5f62\u5efa\u6a21\u95ee\u9898\u3002"}}
{"id": "2509.23535", "pdf": "https://arxiv.org/pdf/2509.23535", "abs": "https://arxiv.org/abs/2509.23535", "authors": ["Ibne Farabi Shihab", "Weiheng Chai", "Jiyang Wang", "Sanjeda Akter", "Senem Velipasalar Gursoy", "Anuj Sharma"], "title": "Calibrated and Resource-Aware Super-Resolution for Reliable Driver Behavior Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Driver monitoring systems require not just high accuracy but reliable, well-calibrated confidence scores for safety-critical deployment. While direct low-resolution training yields high overall accuracy, it produces poorly calibrated predictions that can be dangerous in safety-critical scenarios. We propose a resource-aware adaptive super-resolution framework that optimizes for model calibration and high precision-recall on critical events. Our approach achieves state-of-the-art performance on safety-centric metrics: best calibration (ECE of 5.8\\% vs 6.2\\% for LR-trained baselines), highest AUPR for drowsiness detection (0.78 vs 0.74), and superior precision-recall for phone use detection (0.74 vs 0.71). A lightweight artifact detector (0.3M parameters, 5.2ms overhead) provides additional safety by filtering SR-induced hallucinations. While LR-trained video models serve as strong general-purpose baselines, our adaptive framework represents the state-of-the-art solution for safety-critical applications where reliability is paramount.", "AI": {"tldr": "\u63d0\u51fa\u8d44\u6e90\u611f\u77e5\u81ea\u9002\u5e94\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u4f18\u5316\u6a21\u578b\u6821\u51c6\u548c\u5173\u952e\u4e8b\u4ef6\u7684\u9ad8\u7cbe\u5ea6\u53ec\u56de\uff0c\u5728\u5b89\u5168\u5173\u952e\u6307\u6807\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u9a7e\u9a76\u5458\u76d1\u63a7\u7cfb\u7edf\u4e0d\u4ec5\u9700\u8981\u9ad8\u7cbe\u5ea6\uff0c\u8fd8\u9700\u8981\u53ef\u9760\u7684\u7f6e\u4fe1\u5ea6\u5206\u6570\u4ee5\u786e\u4fdd\u5b89\u5168\u5173\u952e\u90e8\u7f72\u3002\u76f4\u63a5\u4f4e\u5206\u8fa8\u7387\u8bad\u7ec3\u867d\u7136\u6574\u4f53\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u9884\u6d4b\u6821\u51c6\u5dee\uff0c\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u53ef\u80fd\u5371\u9669\u3002", "method": "\u8d44\u6e90\u611f\u77e5\u81ea\u9002\u5e94\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4f2a\u5f71\u68c0\u6d4b\u5668\uff080.3M\u53c2\u6570\uff0c5.2ms\u5f00\u9500\uff09\u8fc7\u6ee4\u8d85\u5206\u8fa8\u7387\u5f15\u8d77\u7684\u5e7b\u89c9\uff0c\u4f18\u5316\u6a21\u578b\u6821\u51c6\u548c\u5173\u952e\u4e8b\u4ef6\u7684\u7cbe\u786e\u5ea6-\u53ec\u56de\u7387\u3002", "result": "\u5728\u5b89\u5168\u4e2d\u5fc3\u6307\u6807\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff1a\u6700\u4f73\u6821\u51c6\uff08ECE 5.8% vs \u57fa\u7ebf6.2%\uff09\u3001\u6700\u9ad8AUPR\u7528\u4e8e\u56f0\u5026\u68c0\u6d4b\uff080.78 vs 0.74\uff09\u3001\u624b\u673a\u4f7f\u7528\u68c0\u6d4b\u7684\u4f18\u8d8a\u7cbe\u786e\u5ea6-\u53ec\u56de\u7387\uff080.74 vs 0.71\uff09\u3002", "conclusion": "\u867d\u7136\u4f4e\u5206\u8fa8\u7387\u8bad\u7ec3\u7684\u89c6\u9891\u6a21\u578b\u4f5c\u4e3a\u5f3a\u901a\u7528\u57fa\u7ebf\uff0c\u4f46\u81ea\u9002\u5e94\u6846\u67b6\u4ee3\u8868\u4e86\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u7684\u6700\u5148\u8fdb\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23555", "pdf": "https://arxiv.org/pdf/2509.23555", "abs": "https://arxiv.org/abs/2509.23555", "authors": ["Javed Ahmad", "Penggang Gao", "Donatien Delehelle", "Mennuti Canio", "Nikhil Deshpande", "Jes\u00fas Ortiz", "Darwin G. Caldwell", "Yonas Teodros Tefera"], "title": "From Fields to Splats: A Cross-Domain Survey of Real-Time Neural Scene Representations", "categories": ["cs.CV"], "comment": "18 pages", "summary": "Neural scene representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have transformed how 3D environments are modeled, rendered, and interpreted. NeRF introduced view-consistent photorealism via volumetric rendering; 3DGS has rapidly emerged as an explicit, efficient alternative that supports high-quality rendering, faster optimization, and integration into hybrid pipelines for enhanced photorealism and task-driven scene understanding. This survey examines how 3DGS is being adopted across SLAM, telepresence and teleoperation, robotic manipulation, and 3D content generation. Despite their differences, these domains share common goals: photorealistic rendering, meaningful 3D structure, and accurate downstream tasks. We organize the review around unified research questions that explain why 3DGS is increasingly displacing NeRF-based approaches: What technical advantages drive its adoption? How does it adapt to different input modalities and domain-specific constraints? What limitations remain? By systematically comparing domain-specific pipelines, we show that 3DGS balances photorealism, geometric fidelity, and computational efficiency. The survey offers a roadmap for leveraging neural rendering not only for image synthesis but also for perception, interaction, and content creation across real and virtual environments.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e863D\u9ad8\u65af\u6cfc\u6e85(3DGS)\u5982\u4f55\u53d6\u4ee3NeRF\u6210\u4e3a\u795e\u7ecf\u573a\u666f\u8868\u793a\u7684\u4e3b\u6d41\u6280\u672f\uff0c\u5206\u6790\u4e86\u5176\u5728SLAM\u3001\u8fdc\u7a0b\u5448\u73b0\u3001\u673a\u5668\u4eba\u64cd\u4f5c\u548c3D\u5185\u5bb9\u751f\u6210\u7b49\u9886\u57df\u7684\u5e94\u7528\u4f18\u52bf\u3002", "motivation": "\u968f\u7740\u795e\u7ecf\u573a\u666f\u8868\u793a\u6280\u672f\u7684\u53d1\u5c55\uff0c3DGS\u56e0\u5176\u663e\u5f0f\u8868\u793a\u3001\u9ad8\u6548\u4f18\u5316\u548c\u9ad8\u8d28\u91cf\u6e32\u67d3\u80fd\u529b\uff0c\u6b63\u5728\u5404\u4e2a\u9886\u57df\u8fc5\u901f\u53d6\u4ee3\u57fa\u4e8eNeRF\u7684\u65b9\u6cd5\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u5206\u67903DGS\u7684\u6280\u672f\u4f18\u52bf\u3001\u9002\u5e94\u6027\u548c\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6bd4\u8f83\u4e0d\u540c\u9886\u57df\u7684\u5177\u4f53\u6280\u672f\u6d41\u7a0b\uff0c\u56f4\u7ed5\u7edf\u4e00\u7684\u7814\u7a76\u95ee\u9898\u7ec4\u7ec7\u7efc\u8ff0\uff1a3DGS\u7684\u6280\u672f\u4f18\u52bf\u3001\u5bf9\u4e0d\u540c\u8f93\u5165\u6a21\u6001\u7684\u9002\u5e94\u6027\u4ee5\u53ca\u73b0\u6709\u5c40\u9650\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e3DGS\u5728\u771f\u5b9e\u611f\u6e32\u67d3\u3001\u51e0\u4f55\u4fdd\u771f\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u826f\u597d\u5e73\u8861\uff0c\u80fd\u591f\u6ee1\u8db3\u591a\u79cd\u5e94\u7528\u573a\u666f\u7684\u9700\u6c42\u3002", "conclusion": "3DGS\u4e3a\u795e\u7ecf\u6e32\u67d3\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u7684\u53d1\u5c55\u8def\u7ebf\u56fe\uff0c\u4e0d\u4ec5\u53ef\u7528\u4e8e\u56fe\u50cf\u5408\u6210\uff0c\u8fd8\u80fd\u5728\u771f\u5b9e\u548c\u865a\u62df\u73af\u5883\u4e2d\u652f\u6301\u611f\u77e5\u3001\u4ea4\u4e92\u548c\u5185\u5bb9\u521b\u5efa\u7b49\u591a\u79cd\u4efb\u52a1\u3002"}}
{"id": "2509.23566", "pdf": "https://arxiv.org/pdf/2509.23566", "abs": "https://arxiv.org/abs/2509.23566", "authors": ["Pinyuan Feng", "Hossein Adeli", "Wenxuan Guo", "Fan Cheng", "Ethan Hwang", "Nikolaus Kriegeskorte"], "title": "Towards Interpretable Visual Decoding with Attention to Brain Representations", "categories": ["cs.CV", "I.2.0; I.4.9"], "comment": "10 pages, 7 figures, under review", "summary": "Recent work has demonstrated that complex visual stimuli can be decoded from human brain activity using deep generative models, helping brain science researchers interpret how the brain represents real-world scenes. However, most current approaches leverage mapping brain signals into intermediate image or text feature spaces before guiding the generative process, masking the effect of contributions from different brain areas on the final reconstruction output. In this work, we propose NeuroAdapter, a visual decoding framework that directly conditions a latent diffusion model on brain representations, bypassing the need for intermediate feature spaces. Our method demonstrates competitive visual reconstruction quality on public fMRI datasets compared to prior work, while providing greater transparency into how brain signals shape the generation process. To this end, we contribute an Image-Brain BI-directional interpretability framework (IBBI) which investigates cross-attention mechanisms across diffusion denoising steps to reveal how different cortical areas influence the unfolding generative trajectory. Our results highlight the potential of end-to-end brain-to-image decoding and establish a path toward interpreting diffusion models through the lens of visual neuroscience.", "AI": {"tldr": "\u63d0\u51faNeuroAdapter\u6846\u67b6\uff0c\u76f4\u63a5\u57fa\u4e8e\u5927\u8111\u8868\u5f81\u6761\u4ef6\u5316\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u65e0\u9700\u4e2d\u95f4\u7279\u5f81\u7a7a\u95f4\uff0c\u5728\u4fdd\u6301\u91cd\u5efa\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u4f9b\u66f4\u597d\u7684\u751f\u6210\u8fc7\u7a0b\u900f\u660e\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u5c06\u5927\u8111\u4fe1\u53f7\u6620\u5c04\u5230\u4e2d\u95f4\u56fe\u50cf\u6216\u6587\u672c\u7279\u5f81\u7a7a\u95f4\u6765\u6307\u5bfc\u751f\u6210\u8fc7\u7a0b\uff0c\u8fd9\u63a9\u76d6\u4e86\u4e0d\u540c\u5927\u8111\u533a\u57df\u5bf9\u6700\u7ec8\u91cd\u5efa\u8f93\u51fa\u7684\u8d21\u732e\u6548\u679c\u3002", "method": "\u5f00\u53d1NeuroAdapter\u6846\u67b6\uff0c\u76f4\u63a5\u6761\u4ef6\u5316\u6f5c\u5728\u6269\u6563\u6a21\u578b\u4e8e\u5927\u8111\u8868\u5f81\uff1b\u63d0\u51faIBBI\u53cc\u5411\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u6269\u6563\u53bb\u566a\u6b65\u9aa4\u4e2d\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u6765\u63ed\u793a\u4e0d\u540c\u76ae\u5c42\u533a\u57df\u5982\u4f55\u5f71\u54cd\u751f\u6210\u8f68\u8ff9\u3002", "result": "\u5728\u516c\u5171fMRI\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u4e0e\u5148\u524d\u5de5\u4f5c\u76f8\u5f53\u7684\u89c6\u89c9\u91cd\u5efa\u8d28\u91cf\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u5927\u8111\u4fe1\u53f7\u5982\u4f55\u5851\u9020\u751f\u6210\u8fc7\u7a0b\u7684\u66f4\u5927\u900f\u660e\u5ea6\u3002", "conclusion": "\u7aef\u5230\u7aef\u8111\u5230\u56fe\u50cf\u89e3\u7801\u5177\u6709\u6f5c\u529b\uff0c\u5e76\u5efa\u7acb\u4e86\u901a\u8fc7\u89c6\u89c9\u795e\u7ecf\u79d1\u5b66\u89c6\u89d2\u89e3\u91ca\u6269\u6563\u6a21\u578b\u7684\u8def\u5f84\u3002"}}
{"id": "2509.23582", "pdf": "https://arxiv.org/pdf/2509.23582", "abs": "https://arxiv.org/abs/2509.23582", "authors": ["Kaicheng Yang", "Xun Zhang", "Haotong Qin", "Yucheng Lin", "Kaisen Yang", "Xianglong Yan", "Yulun Zhang"], "title": "RobuQ: Pushing DiTs to W1.58A2 via Robust Activation Quantization", "categories": ["cs.CV"], "comment": "The code and models will be available at   https://github.com/racoonykc/RobuQ", "summary": "Diffusion Transformers (DiTs) have recently emerged as a powerful backbone for image generation, demonstrating superior scalability and performance over U-Net architectures. However, their practical deployment is hindered by substantial computational and memory costs. While Quantization-Aware Training (QAT) has shown promise for U-Nets, its application to DiTs faces unique challenges, primarily due to the sensitivity and distributional complexity of activations. In this work, we identify activation quantization as the primary bottleneck for pushing DiTs to extremely low-bit settings. To address this, we propose a systematic QAT framework for DiTs, named RobuQ. We start by establishing a strong ternary weight (W1.58A4) DiT baseline. Building upon this, we propose RobustQuantizer to achieve robust activation quantization. Our theoretical analyses show that the Hadamard transform can convert unknown per-token distributions into per-token normal distributions, providing a strong foundation for this method. Furthermore, we propose AMPN, the first Activation-only Mixed-Precision Network pipeline for DiTs. This method applies ternary weights across the entire network while allocating different activation precisions to each layer to eliminate information bottlenecks. Through extensive experiments on unconditional and conditional image generation, our RobuQ framework achieves state-of-the-art performance for DiT quantization in sub-4-bit quantization configuration. To the best of our knowledge, RobuQ is the first achieving stable and competitive image generation on large datasets like ImageNet-1K with activations quantized to average 2 bits. The code and models will be available at https://github.com/racoonykc/RobuQ .", "AI": {"tldr": "\u63d0\u51fa\u4e86RobuQ\u6846\u67b6\uff0c\u901a\u8fc7\u9c81\u68d2\u6fc0\u6d3b\u91cf\u5316\u548c\u6fc0\u6d3b\u6df7\u5408\u7cbe\u5ea6\u7f51\u7edc\uff0c\u9996\u6b21\u5728ImageNet-1K\u7b49\u5927\u578b\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5e73\u57472\u4f4d\u6fc0\u6d3b\u91cf\u5316\u7684\u7a33\u5b9a\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u6269\u6563\u53d8\u6362\u5668(DiTs)\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u9ad8\uff0c\u963b\u788d\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u6fc0\u6d3b\u91cf\u5316\u662fDiTs\u6781\u4f4e\u4f4d\u91cf\u5316\u7684\u4e3b\u8981\u74f6\u9888\u3002", "method": "\u5efa\u7acb\u5f3a\u4e09\u5143\u6743\u91cd\u57fa\u7ebf\uff0c\u63d0\u51faRobustQuantizer\u8fdb\u884c\u9c81\u68d2\u6fc0\u6d3b\u91cf\u5316\uff0c\u5229\u7528Hadamard\u53d8\u6362\u5c06\u672a\u77e5\u5206\u5e03\u8f6c\u6362\u4e3a\u6b63\u6001\u5206\u5e03\uff0c\u5e76\u63d0\u51fa\u9996\u4e2a\u4ec5\u6fc0\u6d3b\u6df7\u5408\u7cbe\u5ea6\u7f51\u7edc(AMPN)\u7ba1\u9053\u3002", "result": "\u5728\u65e0\u6761\u4ef6/\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u5b9e\u9a8c\u4e2d\uff0cRobuQ\u5728\u4e9a4\u4f4d\u91cf\u5316\u914d\u7f6e\u4e0b\u5b9e\u73b0\u4e86DiT\u91cf\u5316\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u9996\u6b21\u5728ImageNet-1K\u7b49\u5927\u578b\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u5e73\u57472\u4f4d\u6fc0\u6d3b\u91cf\u5316\u7684\u7a33\u5b9a\u7ade\u4e89\u6027\u56fe\u50cf\u751f\u6210\u3002", "conclusion": "RobuQ\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86DiTs\u7684\u6fc0\u6d3b\u91cf\u5316\u6311\u6218\uff0c\u4e3a\u6781\u4f4e\u4f4dDiT\u91cf\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u3002"}}
{"id": "2509.23584", "pdf": "https://arxiv.org/pdf/2509.23584", "abs": "https://arxiv.org/abs/2509.23584", "authors": ["Shulian Zhang", "Yong Guo", "Long Peng", "Ziyang Wang", "Ye Chen", "Wenbo Li", "Xiao Zhang", "Yulun Zhang", "Jian Chen"], "title": "VividFace: High-Quality and Efficient One-Step Diffusion For Video Face Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Video Face Enhancement (VFE) seeks to reconstruct high-quality facial regions from degraded video sequences, a capability that underpins numerous applications including video conferencing, film restoration, and surveillance. Despite substantial progress in the field, current methods that primarily rely on video super-resolution and generative frameworks continue to face three fundamental challenges: (1) faithfully modeling intricate facial textures while preserving temporal consistency; (2) restricted model generalization due to the lack of high-quality face video training data; and (3) low efficiency caused by repeated denoising steps during inference. To address these challenges, we propose VividFace, a novel and efficient one-step diffusion framework for video face enhancement. Built upon the pretrained WANX video generation model, our method leverages powerful spatiotemporal priors through a single-step flow matching paradigm, enabling direct mapping from degraded inputs to high-quality outputs with significantly reduced inference time. To further boost efficiency, we propose a Joint Latent-Pixel Face-Focused Training strategy that employs stochastic switching between facial region optimization and global reconstruction, providing explicit supervision in both latent and pixel spaces through a progressive two-stage training process. Additionally, we introduce an MLLM-driven data curation pipeline for automated selection of high-quality video face datasets, enhancing model generalization. Extensive experiments demonstrate that VividFace achieves state-of-the-art results in perceptual quality, identity preservation, and temporal stability, while offering practical resources for the research community.", "AI": {"tldr": "VividFace\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u4e00\u6b65\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u89c6\u9891\u4eba\u8138\u589e\u5f3a\uff0c\u901a\u8fc7\u5355\u6b65\u6d41\u5339\u914d\u8303\u5f0f\u76f4\u63a5\u4ece\u9000\u5316\u8f93\u5165\u6620\u5c04\u5230\u9ad8\u8d28\u91cf\u8f93\u51fa\uff0c\u663e\u8457\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u89c6\u9891\u4eba\u8138\u589e\u5f3a\u65b9\u6cd5\u9762\u4e34\u7684\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u590d\u6742\u9762\u90e8\u7eb9\u7406\u5efa\u6a21\u4e0e\u65f6\u95f4\u4e00\u81f4\u6027\u95ee\u9898\u3001\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7f3a\u4e4f\u5bfc\u81f4\u7684\u6a21\u578b\u6cdb\u5316\u9650\u5236\u3001\u4ee5\u53ca\u63a8\u7406\u8fc7\u7a0b\u4e2d\u91cd\u590d\u53bb\u566a\u6b65\u9aa4\u5bfc\u81f4\u7684\u4f4e\u6548\u7387\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684WANX\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u91c7\u7528\u5355\u6b65\u6d41\u5339\u914d\u8303\u5f0f\uff1b\u63d0\u51fa\u8054\u5408\u6f5c\u5728-\u50cf\u7d20\u4eba\u8138\u805a\u7126\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u968f\u673a\u5207\u6362\u9762\u90e8\u533a\u57df\u4f18\u5316\u548c\u5168\u5c40\u91cd\u5efa\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u548c\u50cf\u7d20\u7a7a\u95f4\u63d0\u4f9b\u663e\u5f0f\u76d1\u7763\uff1b\u5f15\u5165MLLM\u9a71\u52a8\u7684\u6570\u636e\u7b5b\u9009\u6d41\u7a0b\u81ea\u52a8\u9009\u62e9\u9ad8\u8d28\u91cf\u89c6\u9891\u4eba\u8138\u6570\u636e\u96c6\u3002", "result": "\u5728\u611f\u77e5\u8d28\u91cf\u3001\u8eab\u4efd\u4fdd\u6301\u548c\u65f6\u95f4\u7a33\u5b9a\u6027\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u7387\u3002", "conclusion": "VividFace\u5728\u89c6\u9891\u4eba\u8138\u589e\u5f3a\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8d44\u6e90\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6548\u7387\u548c\u6cdb\u5316\u95ee\u9898\u3002"}}
{"id": "2509.23601", "pdf": "https://arxiv.org/pdf/2509.23601", "abs": "https://arxiv.org/abs/2509.23601", "authors": ["Han Hu", "Zhuoran Zheng", "Liang Li", "Chen Lyu"], "title": "VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Recent Mamba-based image restoration methods have achieved promising results but remain   limited by fixed scanning patterns and inefficient feature utilization. Conventional Mamba   architectures rely on predetermined paths that cannot adapt to diverse degradations, constraining   both restoration performance and computational efficiency. To overcome these limitations, we   propose VAMamba, a Visual Adaptive Mamba framework with two key innovations. First,   QCLAM(Queue-basedCacheLow-rankAdaptiveMemory)enhancesfeaturelearningthrougha   FIFO cache that stores historical representations. Similarity between current LoRA-adapted and   cached features guides intelligent fusion, enabling dynamic reuse while effectively controlling   memorygrowth.Second, GPS-SS2D(GreedyPathScanSS2D)introducesadaptive scanning. A   Vision Transformer generates score maps to estimate pixel importance, and a greedy strategy de termines optimal forward and backward scanning paths. These learned trajectories replace rigid   patterns, enabling SS2D to perform targeted feature extraction. The integration of QCLAM and   GPS-SS2D allows VAMamba to adaptively focus on degraded regions while maintaining high   computational efficiency. Extensive experiments across diverse restoration tasks demonstrate   that VAMamba consistently outperforms existing approaches in both restoration quality and   efficiency, establishing new benchmarks for adaptive image restoration. Our code is available   at https://github.com/WaterHQH/VAMamba.", "AI": {"tldr": "VAMamba\u662f\u4e00\u4e2a\u89c6\u89c9\u81ea\u9002\u5e94Mamba\u6846\u67b6\uff0c\u901a\u8fc7QCLAM\u548cGPS-SS2D\u4e24\u4e2a\u521b\u65b0\u7ec4\u4ef6\u89e3\u51b3\u4e86\u4f20\u7edfMamba\u65b9\u6cd5\u4e2d\u56fa\u5b9a\u626b\u63cf\u6a21\u5f0f\u548c\u4f4e\u6548\u7279\u5f81\u5229\u7528\u7684\u95ee\u9898\uff0c\u5728\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u4e0e\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eMamba\u7684\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u53d7\u9650\u4e8e\u56fa\u5b9a\u7684\u626b\u63cf\u6a21\u5f0f\u548c\u4f4e\u6548\u7684\u7279\u5f81\u5229\u7528\uff0c\u65e0\u6cd5\u9002\u5e94\u591a\u6837\u7684\u56fe\u50cf\u9000\u5316\u60c5\u51b5\uff0c\u9650\u5236\u4e86\u6062\u590d\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u63d0\u51faVAMamba\u6846\u67b6\uff1a1) QCLAM\u4f7f\u7528FIFO\u7f13\u5b58\u5b58\u50a8\u5386\u53f2\u8868\u793a\uff0c\u901a\u8fc7\u76f8\u4f3c\u6027\u6307\u5bfc\u667a\u80fd\u7279\u5f81\u878d\u5408\uff1b2) GPS-SS2D\u4f7f\u7528Vision Transformer\u751f\u6210\u5206\u6570\u56fe\u4f30\u8ba1\u50cf\u7d20\u91cd\u8981\u6027\uff0c\u91c7\u7528\u8d2a\u5fc3\u7b56\u7565\u786e\u5b9a\u6700\u4f18\u626b\u63cf\u8def\u5f84\u3002", "result": "\u5728\u591a\u79cd\u6062\u590d\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cVAMamba\u5728\u6062\u590d\u8d28\u91cf\u548c\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e86\u81ea\u9002\u5e94\u56fe\u50cf\u6062\u590d\u7684\u65b0\u57fa\u51c6\u3002", "conclusion": "VAMamba\u901a\u8fc7\u81ea\u9002\u5e94\u626b\u63cf\u548c\u667a\u80fd\u7279\u5f81\u878d\u5408\uff0c\u80fd\u591f\u6709\u9488\u5bf9\u6027\u5730\u5173\u6ce8\u9000\u5316\u533a\u57df\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23603", "pdf": "https://arxiv.org/pdf/2509.23603", "abs": "https://arxiv.org/abs/2509.23603", "authors": ["Tangtangfang Fang", "Jingxi Hu", "Xiangjian He", "Jiaqi Yang"], "title": "MAN: Latent Diffusion Enhanced Multistage Anti-Noise Network for Efficient and High-Quality Low-Dose CT Image Denoising", "categories": ["cs.CV"], "comment": "Submitted to ICASSP 2026", "summary": "While diffusion models have set a new benchmark for quality in Low-Dose Computed Tomography (LDCT) denoising, their clinical adoption is critically hindered by extreme computational costs, with inference times often exceeding thousands of seconds per scan. To overcome this barrier, we introduce MAN, a Latent Diffusion Enhanced Multistage Anti-Noise Network for Efficient and High-Quality Low-Dose CT Image Denoising task. Our method operates in a compressed latent space via a perceptually-optimized autoencoder, enabling an attention-based conditional U-Net to perform the fast, deterministic conditional denoising diffusion process with drastically reduced overhead. On the LDCT and Projection dataset, our model achieves superior perceptual quality, surpassing CNN/GAN-based methods while rivaling the reconstruction fidelity of computationally heavy diffusion models like DDPM and Dn-Dp. Most critically, in the inference stage, our model is over 60x faster than representative pixel space diffusion denoisers, while remaining competitive on PSNR/SSIM scores. By bridging the gap between high fidelity and clinical viability, our work demonstrates a practical path forward for advanced generative models in medical imaging.", "AI": {"tldr": "\u63d0\u51faMAN\u6a21\u578b\uff0c\u4e00\u79cd\u6f5c\u5728\u6269\u6563\u589e\u5f3a\u7684\u591a\u9636\u6bb5\u6297\u566a\u58f0\u7f51\u7edc\uff0c\u7528\u4e8e\u9ad8\u6548\u9ad8\u8d28\u91cf\u7684\u4f4e\u5242\u91cfCT\u56fe\u50cf\u53bb\u566a\uff0c\u76f8\u6bd4\u4f20\u7edf\u6269\u6563\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u63d0\u534760\u500d\u4ee5\u4e0a\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u4f4e\u5242\u91cfCT\u53bb\u566a\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u6781\u9ad8\uff0c\u63a8\u7406\u65f6\u95f4\u8fc7\u957f\uff0c\u963b\u788d\u4e86\u4e34\u5e8a\u91c7\u7528\u3002\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5728\u538b\u7f29\u6f5c\u5728\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u4f7f\u7528\u611f\u77e5\u4f18\u5316\u7684\u81ea\u52a8\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6761\u4ef6U-Net\u8fdb\u884c\u5feb\u901f\u786e\u5b9a\u6027\u6761\u4ef6\u53bb\u566a\u6269\u6563\u8fc7\u7a0b\uff0c\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728LDCT\u548c\u6295\u5f71\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u611f\u77e5\u8d28\u91cf\uff0c\u8d85\u8d8aCNN/GAN\u65b9\u6cd5\uff0c\u540c\u65f6\u4e0e\u8ba1\u7b97\u91cf\u5927\u7684\u6269\u6563\u6a21\u578b\u5982DDPM\u548cDn-Dp\u76f8\u5ab2\u7f8e\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u50cf\u7d20\u7a7a\u95f4\u6269\u6563\u53bb\u566a\u5668\u5feb60\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u901a\u8fc7\u5f25\u5408\u9ad8\u4fdd\u771f\u5ea6\u548c\u4e34\u5e8a\u53ef\u884c\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u533b\u5b66\u6210\u50cf\u4e2d\u7684\u5148\u8fdb\u751f\u6210\u6a21\u578b\u5c55\u793a\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2509.23605", "pdf": "https://arxiv.org/pdf/2509.23605", "abs": "https://arxiv.org/abs/2509.23605", "authors": ["Zeren Xiong", "Yue Yu", "Zedong Zhang", "Shuo Chen", "Jian Yang", "Jun Li"], "title": "VMDiff: Visual Mixing Diffusion for Limitless Cross-Object Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Creating novel images by fusing visual cues from multiple sources is a fundamental yet underexplored problem in image-to-image generation, with broad applications in artistic creation, virtual reality and visual media. Existing methods often face two key challenges: coexistent generation, where multiple objects are simply juxtaposed without true integration, and bias generation, where one object dominates the output due to semantic imbalance. To address these issues, we propose Visual Mixing Diffusion (VMDiff), a simple yet effective diffusion-based framework that synthesizes a single, coherent object by integrating two input images at both noise and latent levels. Our approach comprises: (1) a hybrid sampling process that combines guided denoising, inversion, and spherical interpolation with adjustable parameters to achieve structure-aware fusion, mitigating coexistent generation; and (2) an efficient adaptive adjustment module, which introduces a novel similarity-based score to automatically and adaptively search for optimal parameters, countering semantic bias. Experiments on a curated benchmark of 780 concept pairs demonstrate that our method outperforms strong baselines in visual quality, semantic consistency, and human-rated creativity.", "AI": {"tldr": "\u63d0\u51fa\u4e86Visual Mixing Diffusion (VMDiff)\u6846\u67b6\uff0c\u901a\u8fc7\u566a\u58f0\u548c\u6f5c\u5728\u5c42\u9762\u7684\u878d\u5408\u89e3\u51b3\u591a\u6e90\u56fe\u50cf\u878d\u5408\u4e2d\u7684\u5171\u5b58\u751f\u6210\u548c\u504f\u5dee\u751f\u6210\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u56fe\u50cf\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u591a\u6e90\u89c6\u89c9\u7ebf\u7d22\u878d\u5408\u65f6\u5b58\u5728\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u5171\u5b58\u751f\u6210\uff08\u591a\u4e2a\u5bf9\u8c61\u7b80\u5355\u5e76\u5217\u800c\u65e0\u771f\u6b63\u878d\u5408\uff09\u548c\u504f\u5dee\u751f\u6210\uff08\u4e00\u4e2a\u5bf9\u8c61\u56e0\u8bed\u4e49\u4e0d\u5e73\u8861\u4e3b\u5bfc\u8f93\u51fa\uff09\u3002", "method": "VMDiff\u6846\u67b6\u5305\u542b\uff1a(1) \u6df7\u5408\u91c7\u6837\u8fc7\u7a0b\uff0c\u7ed3\u5408\u5f15\u5bfc\u53bb\u566a\u3001\u53cd\u6f14\u548c\u7403\u9762\u63d2\u503c\u5b9e\u73b0\u7ed3\u6784\u611f\u77e5\u878d\u5408\uff1b(2) \u9ad8\u6548\u81ea\u9002\u5e94\u8c03\u6574\u6a21\u5757\uff0c\u5f15\u5165\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u8bc4\u5206\u81ea\u52a8\u641c\u7d22\u6700\u4f18\u53c2\u6570\u3002", "result": "\u5728780\u4e2a\u6982\u5ff5\u5bf9\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u4eba\u7c7b\u8bc4\u4ef7\u7684\u521b\u9020\u6027\u65b9\u9762\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "VMDiff\u662f\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u6269\u6563\u6846\u67b6\uff0c\u80fd\u591f\u6210\u529f\u5408\u6210\u5355\u4e00\u8fde\u8d2f\u5bf9\u8c61\uff0c\u89e3\u51b3\u591a\u6e90\u56fe\u50cf\u878d\u5408\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2509.23608", "pdf": "https://arxiv.org/pdf/2509.23608", "abs": "https://arxiv.org/abs/2509.23608", "authors": ["Liubing Hu", "Chen Wu", "Anrui Wang", "Dianjie Lu", "Guijuan Zhang", "Zhuoran Zheng"], "title": "FlowLUT: Efficient Image Enhancement via Differentiable LUTs and Iterative Flow Matching", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning-based image enhancement methods face a fundamental trade-off between computational efficiency and representational capacity. For example, although a conventional three-dimensional Look-Up Table (3D LUT) can process a degraded image in real time, it lacks representational flexibility and depends solely on a fixed prior. To address this problem, we introduce FlowLUT, a novel end-to-end model that integrates the efficiency of LUTs, multiple priors, and the parameter-independent characteristic of flow-matched reconstructed images. Specifically, firstly, the input image is transformed in color space by a collection of differentiable 3D LUTs (containing a large number of 3D LUTs with different priors). Subsequently, a lightweight content-aware dynamically predicts fusion weights, enabling scene-adaptive color correction with $\\mathcal{O}(1)$ complexity. Next, a lightweight fusion prediction network runs on multiple 3D LUTs, with $\\mathcal{O}(1)$ complexity for scene-adaptive color correction.Furthermore, to address the inherent representation limitations of LUTs, we design an innovative iterative flow matching method to restore local structural details and eliminate artifacts. Finally, the entire model is jointly optimized under a composite loss function enforcing perceptual and structural fidelity. Extensive experimental results demonstrate the effectiveness of our method on three benchmarks.", "AI": {"tldr": "FlowLUT\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u56fe\u50cf\u589e\u5f3a\u6a21\u578b\uff0c\u7ed3\u5408\u4e863D LUT\u7684\u9ad8\u6548\u6027\u3001\u591a\u5148\u9a8c\u77e5\u8bc6\u548c\u6d41\u5339\u914d\u91cd\u5efa\u7684\u53c2\u6570\u65e0\u5173\u7279\u6027\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u5904\u7406\u4e0e\u9ad8\u8868\u73b0\u529b\u7684\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u4e0e\u8868\u793a\u80fd\u529b\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u4f20\u7edf3D LUT\u867d\u7136\u5b9e\u65f6\u4f46\u7f3a\u4e4f\u8868\u793a\u7075\u6d3b\u6027\u4e14\u4f9d\u8d56\u56fa\u5b9a\u5148\u9a8c\u3002", "method": "\u4f7f\u7528\u53ef\u5fae\u52063D LUT\u96c6\u5408\u8fdb\u884c\u8272\u5f69\u7a7a\u95f4\u53d8\u6362\uff0c\u8f7b\u91cf\u7ea7\u5185\u5bb9\u611f\u77e5\u7f51\u7edc\u9884\u6d4b\u878d\u5408\u6743\u91cd\uff0c\u521b\u65b0\u8fed\u4ee3\u6d41\u5339\u914d\u65b9\u6cd5\u6062\u590d\u5c40\u90e8\u7ed3\u6784\u7ec6\u8282\uff0c\u6574\u4f53\u6a21\u578b\u5728\u590d\u5408\u635f\u5931\u51fd\u6570\u4e0b\u8054\u5408\u4f18\u5316\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u6709\u6548\u6027\u3002", "conclusion": "FlowLUT\u6210\u529f\u89e3\u51b3\u4e86\u56fe\u50cf\u589e\u5f3a\u4e2d\u6548\u7387\u4e0e\u8868\u73b0\u529b\u7684\u6743\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u79cd\u6280\u672f\u5b9e\u73b0\u4e86\u573a\u666f\u81ea\u9002\u5e94\u7684\u5b9e\u65f6\u9ad8\u8d28\u91cf\u589e\u5f3a\u3002"}}
{"id": "2509.23624", "pdf": "https://arxiv.org/pdf/2509.23624", "abs": "https://arxiv.org/abs/2509.23624", "authors": ["Wei Pan", "Huiguo He", "Hiuyi Cheng", "Yilin Shi", "Lianwen Jin"], "title": "DiffInk: Glyph- and Style-Aware Latent Diffusion Transformer for Text to Online Handwriting Generation", "categories": ["cs.CV"], "comment": "24 pages, 16 figures", "summary": "Deep generative models have advanced text-to-online handwriting generation (TOHG), which aims to synthesize realistic pen trajectories conditioned on textual input and style references. However, most existing methods still primarily focus on character- or word-level generation, resulting in inefficiency and a lack of holistic structural modeling when applied to full text lines. To address these issues, we propose DiffInk, the first latent diffusion Transformer framework for full-line handwriting generation. We first introduce InkVAE, a novel sequential variational autoencoder enhanced with two complementary latent-space regularization losses: (1) an OCR-based loss enforcing glyph-level accuracy, and (2) a style-classification loss preserving writing style. This dual regularization yields a semantically structured latent space where character content and writer styles are effectively disentangled. We then introduce InkDiT, a novel latent diffusion Transformer that integrates target text and reference styles to generate coherent pen trajectories. Experimental results demonstrate that DiffInk outperforms existing state-of-the-art methods in both glyph accuracy and style fidelity, while significantly improving generation efficiency. Code will be made publicly available.", "AI": {"tldr": "DiffInk\u662f\u9996\u4e2a\u7528\u4e8e\u5168\u884c\u624b\u5199\u751f\u6210\u7684\u6f5c\u5728\u6269\u6563Transformer\u6846\u67b6\uff0c\u901a\u8fc7InkVAE\u548cInkDiT\u7ec4\u4ef6\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u5b57\u5f62\u548c\u98ce\u683c\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u624b\u5199\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5b57\u7b26\u6216\u5355\u8bcd\u7ea7\u522b\uff0c\u5bfc\u81f4\u5168\u6587\u672c\u884c\u751f\u6210\u6548\u7387\u4f4e\u4e0b\u4e14\u7f3a\u4e4f\u6574\u4f53\u7ed3\u6784\u5efa\u6a21\u3002", "method": "\u63d0\u51faInkVAE\u5e8f\u5217\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u4f7f\u7528OCR\u635f\u5931\u548c\u98ce\u683c\u5206\u7c7b\u635f\u5931\u8fdb\u884c\u6f5c\u5728\u7a7a\u95f4\u6b63\u5219\u5316\uff1b\u7136\u540e\u63d0\u51faInkDiT\u6f5c\u5728\u6269\u6563Transformer\uff0c\u6574\u5408\u76ee\u6807\u6587\u672c\u548c\u53c2\u8003\u98ce\u683c\u751f\u6210\u8fde\u8d2f\u7b14\u8ff9\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aDiffInk\u5728\u5b57\u5f62\u51c6\u786e\u6027\u548c\u98ce\u683c\u4fdd\u771f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u6548\u7387\u3002", "conclusion": "DiffInk\u901a\u8fc7\u53cc\u6b63\u5219\u5316\u6f5c\u5728\u7a7a\u95f4\u548c\u6269\u6563Transformer\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u5168\u884c\u624b\u5199\u751f\u6210\u3002"}}
{"id": "2509.23639", "pdf": "https://arxiv.org/pdf/2509.23639", "abs": "https://arxiv.org/abs/2509.23639", "authors": ["Boyu Han", "Qianqian Xu", "Shilong Bao", "Zhiyong Yang", "Kangli Zi", "Qingming Huang"], "title": "LightFair: Towards an Efficient Alternative for Fair T2I Diffusion via Debiasing Pre-trained Text Encoders", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper explores a novel lightweight approach LightFair to achieve fair text-to-image diffusion models (T2I DMs) by addressing the adverse effects of the text encoder. Most existing methods either couple different parts of the diffusion model for full-parameter training or rely on auxiliary networks for correction. They incur heavy training or sampling burden and unsatisfactory performance. Since T2I DMs consist of multiple components, with the text encoder being the most fine-tunable and front-end module, this paper focuses on mitigating bias by fine-tuning text embeddings. To validate feasibility, we observe that the text encoder's neutral embedding output shows substantial skewness across image embeddings of various attributes in the CLIP space. More importantly, the noise prediction network further amplifies this imbalance. To finetune the text embedding, we propose a collaborative distance-constrained debiasing strategy that balances embedding distances to improve fairness without auxiliary references. However, mitigating bias can compromise the original generation quality. To address this, we introduce a two-stage text-guided sampling strategy to limit when the debiased text encoder intervenes. Extensive experiments demonstrate that LightFair is effective and efficient. Notably, on Stable Diffusion v1.5, our method achieves SOTA debiasing at just $1/4$ of the training burden, with virtually no increase in sampling burden. The code is available at https://github.com/boyuh/LightFair.", "AI": {"tldr": "LightFair\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03\u6587\u672c\u7f16\u7801\u5668\u7684\u5d4c\u5165\u6765\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u516c\u5e73\u6027\uff0c\u91c7\u7528\u8ddd\u79bb\u7ea6\u675f\u53bb\u504f\u7b56\u7565\u548c\u4e24\u9636\u6bb5\u91c7\u6837\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u8d1f\u62c5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u5168\u53c2\u6570\u8bad\u7ec3\uff0c\u8981\u4e48\u4f9d\u8d56\u8f85\u52a9\u7f51\u7edc\u8fdb\u884c\u6821\u6b63\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6216\u91c7\u6837\u8d1f\u62c5\u91cd\u4e14\u6027\u80fd\u4e0d\u4f73\u3002\u6587\u672c\u7f16\u7801\u5668\u4f5c\u4e3a\u6700\u53ef\u5fae\u8c03\u548c\u524d\u7aef\u6a21\u5757\uff0c\u5176\u8f93\u51fa\u7684\u5d4c\u5165\u5728\u4e0d\u540c\u5c5e\u6027\u56fe\u50cf\u4e0a\u5b58\u5728\u663e\u8457\u504f\u5dee\uff0c\u4e14\u566a\u58f0\u9884\u6d4b\u7f51\u7edc\u4f1a\u8fdb\u4e00\u6b65\u653e\u5927\u8fd9\u79cd\u4e0d\u5e73\u8861\u3002", "method": "\u63d0\u51fa\u534f\u4f5c\u8ddd\u79bb\u7ea6\u675f\u53bb\u504f\u7b56\u7565\uff0c\u901a\u8fc7\u5e73\u8861\u5d4c\u5165\u8ddd\u79bb\u6765\u6539\u5584\u516c\u5e73\u6027\uff0c\u65e0\u9700\u8f85\u52a9\u53c2\u8003\uff1b\u5f15\u5165\u4e24\u9636\u6bb5\u6587\u672c\u5f15\u5bfc\u91c7\u6837\u7b56\u7565\uff0c\u9650\u5236\u53bb\u504f\u6587\u672c\u7f16\u7801\u5668\u7684\u5e72\u9884\u65f6\u673a\u4ee5\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "result": "\u5728Stable Diffusion v1.5\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4ec5\u97001/4\u7684\u8bad\u7ec3\u8d1f\u62c5\u5373\u53ef\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u53bb\u504f\u6548\u679c\uff0c\u91c7\u6837\u8d1f\u62c5\u51e0\u4e4e\u6ca1\u6709\u589e\u52a0\u3002", "conclusion": "LightFair\u662f\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684\u516c\u5e73\u6027\u63d0\u5347\u65b9\u6cd5\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u5904\u7406\u6587\u672c\u7f16\u7801\u5668\u504f\u5dee\uff0c\u5728\u4f4e\u8d1f\u62c5\u4e0b\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u53bb\u504f\u6027\u80fd\u3002"}}
{"id": "2509.23646", "pdf": "https://arxiv.org/pdf/2509.23646", "abs": "https://arxiv.org/abs/2509.23646", "authors": ["Lu Xiao", "Jiale Zhang", "Yang Liu", "Taicheng Huang", "Xin Tian"], "title": "Sparse-Up: Learnable Sparse Upsampling for 3D Generation with High-Fidelity Textures", "categories": ["cs.CV"], "comment": null, "summary": "The creation of high-fidelity 3D assets is often hindered by a 'pixel-level pain point': the loss of high-frequency details. Existing methods often trade off one aspect for another: either sacrificing cross-view consistency, resulting in torn or drifting textures, or remaining trapped by the resolution ceiling of explicit voxels, forfeiting fine texture detail. In this work, we propose Sparse-Up, a memory-efficient, high-fidelity texture modeling framework that effectively preserves high-frequency details. We use sparse voxels to guide texture reconstruction and ensure multi-view consistency, while leveraging surface anchoring and view-domain partitioning to break through resolution constraints. Surface anchoring employs a learnable upsampling strategy to constrain voxels to the mesh surface, eliminating over 70% of redundant voxels present in traditional voxel upsampling. View-domain partitioning introduces an image patch-guided voxel partitioning scheme, supervising and back-propagating gradients only on visible local patches. Through these two strategies, we can significantly reduce memory consumption during high-resolution voxel training without sacrificing geometric consistency, while preserving high-frequency details in textures.", "AI": {"tldr": "\u63d0\u51fa\u4e86Sparse-Up\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u4f53\u7d20\u5f15\u5bfc\u7eb9\u7406\u91cd\u5efa\uff0c\u4f7f\u7528\u8868\u9762\u951a\u5b9a\u548c\u89c6\u57df\u5206\u533a\u6280\u672f\u7a81\u7834\u5206\u8fa8\u7387\u9650\u5236\uff0c\u5728\u4fdd\u6301\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u7684\u540c\u65f6\u6709\u6548\u4fdd\u7559\u9ad8\u9891\u7eb9\u7406\u7ec6\u8282\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u521b\u5efa\u9ad8\u4fdd\u771f3D\u8d44\u4ea7\u65f6\u9762\u4e34'\u50cf\u7d20\u7ea7\u75db\u70b9'\uff1a\u9ad8\u9891\u7ec6\u8282\u4e22\u5931\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u727a\u7272\u8de8\u89c6\u89d2\u4e00\u81f4\u6027\u5bfc\u81f4\u7eb9\u7406\u6495\u88c2\uff0c\u8981\u4e48\u53d7\u9650\u4e8e\u663e\u5f0f\u4f53\u7d20\u7684\u5206\u8fa8\u7387\u4e0a\u9650\u800c\u4e22\u5931\u7cbe\u7ec6\u7eb9\u7406\u7ec6\u8282\u3002", "method": "1. \u4f7f\u7528\u7a00\u758f\u4f53\u7d20\u6307\u5bfc\u7eb9\u7406\u91cd\u5efa\u786e\u4fdd\u591a\u89c6\u89d2\u4e00\u81f4\u6027\uff1b2. \u8868\u9762\u951a\u5b9a\u91c7\u7528\u53ef\u5b66\u4e60\u4e0a\u91c7\u6837\u7b56\u7565\u5c06\u4f53\u7d20\u7ea6\u675f\u5728\u7f51\u683c\u8868\u9762\uff0c\u6d88\u966470%\u4ee5\u4e0a\u5197\u4f59\u4f53\u7d20\uff1b3. \u89c6\u57df\u5206\u533a\u5f15\u5165\u56fe\u50cf\u5757\u5f15\u5bfc\u7684\u4f53\u7d20\u5206\u533a\u65b9\u6848\uff0c\u4ec5\u5728\u53ef\u89c1\u5c40\u90e8\u5757\u4e0a\u8fdb\u884c\u68af\u5ea6\u76d1\u7763\u548c\u53cd\u5411\u4f20\u64ad\u3002", "result": "\u663e\u8457\u51cf\u5c11\u9ad8\u5206\u8fa8\u7387\u4f53\u7d20\u8bad\u7ec3\u65f6\u7684\u5185\u5b58\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u51e0\u4f55\u4e00\u81f4\u6027\u5e76\u4fdd\u7559\u7eb9\u7406\u4e2d\u7684\u9ad8\u9891\u7ec6\u8282\u3002", "conclusion": "Sparse-Up\u662f\u4e00\u4e2a\u5185\u5b58\u9ad8\u6548\u7684\u9ad8\u4fdd\u771f\u7eb9\u7406\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u7a00\u758f\u4f53\u7d20\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e863D\u8d44\u4ea7\u521b\u5efa\u4e2d\u7684\u9ad8\u9891\u7ec6\u8282\u4fdd\u7559\u95ee\u9898\u3002"}}
{"id": "2509.23681", "pdf": "https://arxiv.org/pdf/2509.23681", "abs": "https://arxiv.org/abs/2509.23681", "authors": ["Weilun Feng", "Chuanguang Yang", "Haotong Qin", "Mingqiang Wu", "Yuqi Li", "Xiangqi Li", "Zhulin An", "Libo Huang", "Yulun Zhang", "Michele Magno", "Yongjun Xu"], "title": "QuantSparse: Comprehensively Compressing Video Diffusion Transformer with Model Quantization and Attention Sparsification", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion transformers exhibit remarkable video generation capability, yet their prohibitive computational and memory costs hinder practical deployment. Model quantization and attention sparsification are two promising directions for compression, but each alone suffers severe performance degradation under aggressive compression. Combining them promises compounded efficiency gains, but naive integration is ineffective. The sparsity-induced information loss exacerbates quantization noise, leading to amplified attention shifts. To address this, we propose \\textbf{QuantSparse}, a unified framework that integrates model quantization with attention sparsification. Specifically, we introduce \\textit{Multi-Scale Salient Attention Distillation}, which leverages both global structural guidance and local salient supervision to mitigate quantization-induced bias. In addition, we develop \\textit{Second-Order Sparse Attention Reparameterization}, which exploits the temporal stability of second-order residuals to efficiently recover information lost under sparsity. Experiments on HunyuanVideo-13B demonstrate that QuantSparse achieves 20.88 PSNR, substantially outperforming the state-of-the-art quantization baseline Q-VDiT (16.85 PSNR), while simultaneously delivering a \\textbf{3.68$\\times$} reduction in storage and \\textbf{1.88$\\times$} acceleration in end-to-end inference. Our code will be released in https://github.com/wlfeng0509/QuantSparse.", "AI": {"tldr": "\u63d0\u51fa\u4e86QuantSparse\u6846\u67b6\uff0c\u5c06\u6a21\u578b\u91cf\u5316\u4e0e\u6ce8\u610f\u529b\u7a00\u758f\u5316\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u663e\u8457\u6ce8\u610f\u529b\u84b8\u998f\u548c\u4e8c\u9636\u7a00\u758f\u6ce8\u610f\u529b\u91cd\u53c2\u6570\u5316\uff0c\u5728\u663e\u8457\u964d\u4f4e\u5b58\u50a8\u548c\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u89c6\u9891\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u53d8\u6362\u5668\u5728\u89c6\u9891\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u8fc7\u9ad8\u963b\u788d\u5b9e\u9645\u90e8\u7f72\u3002\u5355\u72ec\u4f7f\u7528\u91cf\u5316\u6216\u7a00\u758f\u5316\u5728\u6fc0\u8fdb\u538b\u7f29\u4e0b\u4f1a\u5bfc\u81f4\u4e25\u91cd\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u538b\u7f29\u65b9\u6cd5\u3002", "method": "1. \u591a\u5c3a\u5ea6\u663e\u8457\u6ce8\u610f\u529b\u84b8\u998f\uff1a\u5229\u7528\u5168\u5c40\u7ed3\u6784\u6307\u5bfc\u548c\u5c40\u90e8\u663e\u8457\u76d1\u7763\u7f13\u89e3\u91cf\u5316\u504f\u5dee\n2. \u4e8c\u9636\u7a00\u758f\u6ce8\u610f\u529b\u91cd\u53c2\u6570\u5316\uff1a\u5229\u7528\u4e8c\u9636\u6b8b\u5dee\u7684\u65f6\u95f4\u7a33\u5b9a\u6027\u9ad8\u6548\u6062\u590d\u7a00\u758f\u5316\u4e22\u5931\u7684\u4fe1\u606f", "result": "\u5728HunyuanVideo-13B\u4e0a\uff0cQuantSparse\u8fbe\u523020.88 PSNR\uff0c\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u91cf\u5316\u57fa\u7ebfQ-VDiT\uff0816.85 PSNR\uff09\uff0c\u540c\u65f6\u5b58\u50a8\u51cf\u5c113.68\u500d\uff0c\u7aef\u5230\u7aef\u63a8\u7406\u52a0\u901f1.88\u500d\u3002", "conclusion": "QuantSparse\u6210\u529f\u5c06\u91cf\u5316\u4e0e\u7a00\u758f\u5316\u76f8\u7ed3\u5408\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u6548\u7387\uff0c\u4e3a\u6269\u6563\u53d8\u6362\u5668\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23708", "pdf": "https://arxiv.org/pdf/2509.23708", "abs": "https://arxiv.org/abs/2509.23708", "authors": ["Boseong Jeon", "Junghyuk Lee", "Jimin Park", "Kwanyoung Kim", "Jingi Jung", "Sangwon Lee", "Hyunbo Shim"], "title": "CrimEdit: Controllable Editing for Counterfactual Object Removal, Insertion, and Movement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent works on object removal and insertion have enhanced their performance by handling object effects such as shadows and reflections, using diffusion models trained on counterfactual datasets. However, the performance impact of applying classifier-free guidance to handle object effects across removal and insertion tasks within a unified model remains largely unexplored. To address this gap and improve efficiency in composite editing, we propose CrimEdit, which jointly trains the task embeddings for removal and insertion within a single model and leverages them in a classifier-free guidance scheme -- enhancing the removal of both objects and their effects, and enabling controllable synthesis of object effects during insertion. CrimEdit also extends these two task prompts to be applied to spatially distinct regions, enabling object movement (repositioning) within a single denoising step. By employing both guidance techniques, extensive experiments show that CrimEdit achieves superior object removal, controllable effect insertion, and efficient object movement without requiring additional training or separate removal and insertion stages.", "AI": {"tldr": "CrimEdit\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u79fb\u9664\u548c\u63d2\u5165\u4efb\u52a1\u5d4c\u5165\uff0c\u5229\u7528\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u6280\u672f\uff0c\u5728\u5355\u4e00\u6a21\u578b\u4e2d\u5b9e\u73b0\u5bf9\u8c61\u79fb\u9664\u3001\u53ef\u63a7\u6548\u679c\u63d2\u5165\u548c\u9ad8\u6548\u5bf9\u8c61\u79fb\u52a8\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5bf9\u8c61\u6548\u679c\uff08\u5982\u9634\u5f71\u548c\u53cd\u5c04\uff09\u65f6\u6027\u80fd\u6709\u9650\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u5728\u7edf\u4e00\u6a21\u578b\u4e2d\u5904\u7406\u5bf9\u8c61\u79fb\u9664\u548c\u63d2\u5165\u4efb\u52a1\u5f71\u54cd\u7684\u7814\u7a76\u3002", "method": "\u5728\u5355\u4e00\u6a21\u578b\u4e2d\u8054\u5408\u8bad\u7ec3\u79fb\u9664\u548c\u63d2\u5165\u4efb\u52a1\u5d4c\u5165\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u65b9\u6848\uff0c\u540c\u65f6\u6269\u5c55\u4efb\u52a1\u63d0\u793a\u4ee5\u652f\u6301\u7a7a\u95f4\u4e0d\u540c\u533a\u57df\u7684\u5bf9\u8c61\u79fb\u52a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCrimEdit\u5728\u5bf9\u8c61\u79fb\u9664\u3001\u53ef\u63a7\u6548\u679c\u63d2\u5165\u548c\u5bf9\u8c61\u79fb\u52a8\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u5206\u79bb\u7684\u79fb\u9664\u548c\u63d2\u5165\u9636\u6bb5\u3002", "conclusion": "CrimEdit\u901a\u8fc7\u7edf\u4e00\u6a21\u578b\u548c\u5f15\u5bfc\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5bf9\u8c61\u7f16\u8f91\uff0c\u5305\u62ec\u79fb\u9664\u3001\u63d2\u5165\u548c\u79fb\u52a8\uff0c\u5c55\u793a\u4e86\u5728\u590d\u5408\u7f16\u8f91\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2509.23723", "pdf": "https://arxiv.org/pdf/2509.23723", "abs": "https://arxiv.org/abs/2509.23723", "authors": ["Zijun Li", "Hongyu Yan", "Shijie Li", "Kunming Luo", "Li Lu", "Xulei Yang", "Weisi Lin"], "title": "DiffPCN: Latent Diffusion Model Based on Multi-view Depth Images for Point Cloud Completion", "categories": ["cs.CV"], "comment": null, "summary": "Latent diffusion models (LDMs) have demonstrated remarkable generative capabilities across various low-level vision tasks. However, their potential for point cloud completion remains underexplored due to the unstructured and irregular nature of point clouds. In this work, we propose DiffPCN, a novel diffusion-based coarse-to-fine framework for point cloud completion. Our approach comprises two stages: an initial stage for generating coarse point clouds, and a refinement stage that improves their quality through point denoising and upsampling. Specifically, we first project the unordered and irregular partial point cloud into structured depth images, which serve as conditions for a well-designed DepthLDM to synthesize completed multi-view depth images that are used to form coarse point clouds. In this way, our DiffPCN can yield high-quality and high-completeness coarse point clouds by leveraging LDM' s powerful generation and comprehension capabilities. Then, since LDMs inevitably introduce outliers into the generated depth maps, we design a Point Denoising Network to remove artifacts from the coarse point cloud by predicting a per-point distance score. Finally, we devise an Association-Aware Point Upsampler, which guides the upsampling process by leveraging local association features between the input point cloud and the corresponding coarse points, further yielding a dense and high-fidelity output. Experimental results demonstrate that our DiffPCN achieves state-of-the-art performance in geometric accuracy and shape completeness, significantly improving the robustness and consistency of point cloud completion.", "AI": {"tldr": "DiffPCN\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u70b9\u4e91\u8865\u5168\u6846\u67b6\uff0c\u91c7\u7528\u4ece\u7c97\u5230\u7cbe\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u9996\u5148\u751f\u6210\u7c97\u7cd9\u70b9\u4e91\uff0c\u7136\u540e\u901a\u8fc7\u53bb\u566a\u548c\u4e0a\u91c7\u6837\u8fdb\u884c\u7ec6\u5316\u3002", "motivation": "\u7531\u4e8e\u70b9\u4e91\u7684\u65e0\u7ed3\u6784\u548c\u4e0d\u89c4\u5219\u7279\u6027\uff0c\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5728\u70b9\u4e91\u8865\u5168\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5c06\u65e0\u5e8f\u70b9\u4e91\u6295\u5f71\u4e3a\u7ed3\u6784\u5316\u6df1\u5ea6\u56fe\u50cf\uff0c\u4f7f\u7528DepthLDM\u5408\u6210\u591a\u89c6\u89d2\u6df1\u5ea6\u56fe\u50cf\u5f62\u6210\u7c97\u7cd9\u70b9\u4e91\uff0c\u7136\u540e\u901a\u8fc7\u70b9\u53bb\u566a\u7f51\u7edc\u53bb\u9664\u5f02\u5e38\u503c\uff0c\u6700\u540e\u4f7f\u7528\u5173\u8054\u611f\u77e5\u70b9\u4e0a\u91c7\u6837\u5668\u751f\u6210\u5bc6\u96c6\u9ad8\u4fdd\u771f\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eDiffPCN\u5728\u51e0\u4f55\u7cbe\u5ea6\u548c\u5f62\u72b6\u5b8c\u6574\u6027\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u70b9\u4e91\u8865\u5168\u7684\u9c81\u68d2\u6027\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "DiffPCN\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u6a21\u578b\u7684\u5f3a\u5927\u751f\u6210\u80fd\u529b\u548c\u70b9\u4e91\u5904\u7406\u6280\u672f\uff0c\u4e3a\u70b9\u4e91\u8865\u5168\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23728", "pdf": "https://arxiv.org/pdf/2509.23728", "abs": "https://arxiv.org/abs/2509.23728", "authors": ["Yiheng Zhang", "Zhuojiang Cai", "Mingdao Wang", "Meitong Guo", "Tianxiao Li", "Li Lin", "Yuwang Wang"], "title": "M3DLayout: A Multi-Source Dataset of 3D Indoor Layouts and Structured Descriptions for 3D Generation", "categories": ["cs.CV", "cs.AI"], "comment": "https://graphic-kiliani.github.io/M3DLayout/", "summary": "In text-driven 3D scene generation, object layout serves as a crucial intermediate representation that bridges high-level language instructions with detailed geometric output. It not only provides a structural blueprint for ensuring physical plausibility but also supports semantic controllability and interactive editing. However, the learning capabilities of current 3D indoor layout generation models are constrained by the limited scale, diversity, and annotation quality of existing datasets. To address this, we introduce M3DLayout, a large-scale, multi-source dataset for 3D indoor layout generation. M3DLayout comprises 15,080 layouts and over 258k object instances, integrating three distinct sources: real-world scans, professional CAD designs, and procedurally generated scenes. Each layout is paired with detailed structured text describing global scene summaries, relational placements of large furniture, and fine-grained arrangements of smaller items. This diverse and richly annotated resource enables models to learn complex spatial and semantic patterns across a wide variety of indoor environments. To assess the potential of M3DLayout, we establish a benchmark using a text-conditioned diffusion model. Experimental results demonstrate that our dataset provides a solid foundation for training layout generation models. Its multi-source composition enhances diversity, notably through the Inf3DLayout subset which provides rich small-object information, enabling the generation of more complex and detailed scenes. We hope that M3DLayout can serve as a valuable resource for advancing research in text-driven 3D scene synthesis.", "AI": {"tldr": "\u63d0\u51fa\u4e86M3DLayout\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e3D\u5ba4\u5185\u5e03\u5c40\u751f\u6210\u7684\u5927\u89c4\u6a21\u591a\u6e90\u6570\u636e\u96c6\uff0c\u5305\u542b15,080\u4e2a\u5e03\u5c40\u548c258k+\u7269\u4f53\u5b9e\u4f8b\uff0c\u6574\u5408\u4e86\u771f\u5b9e\u626b\u63cf\u3001CAD\u8bbe\u8ba1\u548c\u7a0b\u5e8f\u751f\u6210\u573a\u666f\u4e09\u79cd\u6765\u6e90\u3002", "motivation": "\u5f53\u524d3D\u5ba4\u5185\u5e03\u5c40\u751f\u6210\u6a21\u578b\u53d7\u9650\u4e8e\u73b0\u6709\u6570\u636e\u96c6\u7684\u89c4\u6a21\u3001\u591a\u6837\u6027\u548c\u6807\u6ce8\u8d28\u91cf\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u4e30\u5bcc\u7684\u6570\u636e\u8d44\u6e90\u6765\u63d0\u5347\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u6784\u5efa\u5305\u542b\u771f\u5b9e\u626b\u63cf\u3001\u4e13\u4e1aCAD\u8bbe\u8ba1\u548c\u7a0b\u5e8f\u751f\u6210\u573a\u666f\u7684\u591a\u6e90\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u5e03\u5c40\u90fd\u914d\u6709\u7ed3\u6784\u5316\u6587\u672c\u63cf\u8ff0\uff0c\u5305\u62ec\u5168\u5c40\u573a\u666f\u6458\u8981\u3001\u5927\u578b\u5bb6\u5177\u5173\u7cfb\u5e03\u5c40\u548c\u5c0f\u7269\u54c1\u7ec6\u7c92\u5ea6\u6392\u5217\u3002", "result": "\u5b9e\u9a8c\u8868\u660eM3DLayout\u4e3a\u5e03\u5c40\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u591a\u6e90\u7ec4\u6210\u589e\u5f3a\u4e86\u591a\u6837\u6027\uff0c\u7279\u522b\u662fInf3DLayout\u5b50\u96c6\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u5c0f\u7269\u4f53\u4fe1\u606f\uff0c\u80fd\u591f\u751f\u6210\u66f4\u590d\u6742\u8be6\u7ec6\u7684\u573a\u666f\u3002", "conclusion": "M3DLayout\u53ef\u4f5c\u4e3a\u63a8\u8fdb\u6587\u672c\u9a71\u52a83D\u573a\u666f\u5408\u6210\u7814\u7a76\u7684\u6709\u4ef7\u503c\u8d44\u6e90\uff0c\u5176\u591a\u6837\u6027\u548c\u4e30\u5bcc\u6807\u6ce8\u6709\u52a9\u4e8e\u6a21\u578b\u5b66\u4e60\u590d\u6742\u7684\u7a7a\u95f4\u548c\u8bed\u4e49\u6a21\u5f0f\u3002"}}
{"id": "2509.23736", "pdf": "https://arxiv.org/pdf/2509.23736", "abs": "https://arxiv.org/abs/2509.23736", "authors": ["Cong Chen", "Ziyuan Huang", "Cheng Zou", "Muzhi Zhu", "Kaixiang Ji", "Jiajia Liu", "Jingdong Chen", "Hao Chen", "Chunhua Shen"], "title": "HieraTok: Multi-Scale Visual Tokenizer Improves Image Reconstruction and Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this work, we present HieraTok, a novel multi-scale Vision Transformer (ViT)-based tokenizer that overcomes the inherent limitation of modeling single-scale representations. This is realized through two key designs: (1) multi-scale downsampling applied to the token map generated by the tokenizer encoder, producing a sequence of multi-scale tokens, and (2) a scale-causal attention mechanism that enables the progressive flow of information from low-resolution global semantic features to high-resolution structural details. Coupling these designs, HieraTok achieves significant improvements in both image reconstruction and generation tasks. Under identical settings, the multi-scale visual tokenizer outperforms its single-scale counterpart by a 27.2\\% improvement in rFID ($1.47 \\rightarrow 1.07$). When integrated into downstream generation frameworks, it achieves a $1.38\\times$ faster convergence rate and an 18.9\\% boost in gFID ($16.4 \\rightarrow 13.3$), which may be attributed to the smoother and more uniformly distributed latent space. Furthermore, by scaling up the tokenizer's training, we demonstrate its potential by a sota rFID of 0.45 and a gFID of 1.82 among ViT tokenizers. To the best of our knowledge, we are the first to introduce multi-scale ViT-based tokenizer in image reconstruction and image generation. We hope our findings and designs advance the ViT-based tokenizers in visual generation tasks.", "AI": {"tldr": "HieraTok\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u5c3a\u5ea6Vision Transformer tokenizer\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u4e0b\u91c7\u6837\u548c\u5c3a\u5ea6\u56e0\u679c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u56fe\u50cf\u91cd\u5efa\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u5355\u5c3a\u5ea6tokenizer\u3002", "motivation": "\u514b\u670d\u5355\u5c3a\u5ea6\u8868\u793a\u5efa\u6a21\u7684\u56fa\u6709\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4ece\u4f4e\u5206\u8fa8\u7387\u5168\u5c40\u8bed\u4e49\u7279\u5f81\u5230\u9ad8\u5206\u8fa8\u7387\u7ed3\u6784\u7ec6\u8282\u7684\u4fe1\u606f\u6e10\u8fdb\u6d41\u52a8\u3002", "method": "\u4f7f\u7528\u591a\u5c3a\u5ea6\u4e0b\u91c7\u6837\u751f\u6210\u591a\u5c3a\u5ea6token\u5e8f\u5217\uff0c\u7ed3\u5408\u5c3a\u5ea6\u56e0\u679c\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u4fe1\u606f\u4ece\u7c97\u5230\u7ec6\u7684\u6d41\u52a8\u3002", "result": "\u76f8\u6bd4\u5355\u5c3a\u5ea6tokenizer\uff0crFID\u63d0\u534727.2%(1.47\u21921.07)\uff0c\u4e0b\u6e38\u751f\u6210\u4efb\u52a1\u6536\u655b\u901f\u5ea6\u63d0\u53471.38\u500d\uff0cgFID\u63d0\u534718.9%(16.4\u219213.3)\uff0c\u6700\u4f73rFID\u8fbe\u52300.45\uff0cgFID\u8fbe\u52301.82\u3002", "conclusion": "HieraTok\u662f\u9996\u4e2a\u591a\u5c3a\u5ea6ViT tokenizer\uff0c\u5176\u5e73\u6ed1\u4e14\u5747\u5300\u5206\u5e03\u7684\u6f5c\u5728\u7a7a\u95f4\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u751f\u6210\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2509.23760", "pdf": "https://arxiv.org/pdf/2509.23760", "abs": "https://arxiv.org/abs/2509.23760", "authors": ["Xinyang Song", "Libin Wang", "Weining Wang", "Shaozhen Liu", "Dandan Zheng", "Jingdong Chen", "Qi Li", "Zhenan Sun"], "title": "UniAlignment: Semantic Alignment for Unified Image Generation, Understanding, Manipulation and Perception", "categories": ["cs.CV"], "comment": null, "summary": "The remarkable success of diffusion models in text-to-image generation has sparked growing interest in expanding their capabilities to a variety of multi-modal tasks, including image understanding, manipulation, and perception. These tasks require advanced semantic comprehension across both visual and textual modalities, especially in scenarios involving complex semantic instructions. However, existing approaches often rely heavily on vision-language models (VLMs) or modular designs for semantic guidance, leading to fragmented architectures and computational inefficiency. To address these challenges, we propose UniAlignment, a unified multimodal generation framework within a single diffusion transformer. UniAlignment introduces a dual-stream diffusion training strategy that incorporates both intrinsic-modal semantic alignment and cross-modal semantic alignment, thereby enhancing the model's cross-modal consistency and instruction-following robustness. Additionally, we present SemGen-Bench, a new benchmark specifically designed to evaluate multimodal semantic consistency under complex textual instructions. Extensive experiments across multiple tasks and benchmarks demonstrate that UniAlignment outperforms existing baselines, underscoring the significant potential of diffusion models in unified multimodal generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86UniAlignment\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5355\u4e00\u6269\u6563\u53d8\u6362\u5668\u7684\u7edf\u4e00\u591a\u6a21\u6001\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u6d41\u6269\u6563\u8bad\u7ec3\u7b56\u7565\u589e\u5f3a\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u548c\u6307\u4ee4\u8ddf\u968f\u9c81\u68d2\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u6210\u529f\u6fc0\u53d1\u4e86\u5c06\u5176\u6269\u5c55\u5230\u591a\u6a21\u6001\u4efb\u52a1\u7684\u9700\u6c42\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6216\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u5bfc\u81f4\u67b6\u6784\u788e\u7247\u5316\u548c\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u91c7\u7528\u5355\u4e00\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\uff0c\u5f15\u5165\u53cc\u6d41\u6269\u6563\u8bad\u7ec3\u7b56\u7565\uff0c\u5305\u542b\u5185\u5728\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\u548c\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\uff0c\u5e76\u521b\u5efaSemGen-Bench\u57fa\u51c6\u6765\u8bc4\u4f30\u590d\u6742\u6587\u672c\u6307\u4ee4\u4e0b\u7684\u591a\u6a21\u6001\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cUniAlignment\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u7edf\u4e00\u591a\u6a21\u6001\u751f\u6210\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.23774", "pdf": "https://arxiv.org/pdf/2509.23774", "abs": "https://arxiv.org/abs/2509.23774", "authors": ["Qifan Li", "Jiale Zou", "Jinhua Zhang", "Wei Long", "Xinyu Zhou", "Shuhang Gu"], "title": "Texture Vector-Quantization and Reconstruction Aware Prediction for Generative Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Vector-quantized based models have recently demonstrated strong potential for visual prior modeling. However, existing VQ-based methods simply encode visual features with nearest codebook items and train index predictor with code-level supervision. Due to the richness of visual signal, VQ encoding often leads to large quantization error. Furthermore, training predictor with code-level supervision can not take the final reconstruction errors into consideration, result in sub-optimal prior modeling accuracy. In this paper we address the above two issues and propose a Texture Vector-Quantization and a Reconstruction Aware Prediction strategy. The texture vector-quantization strategy leverages the task character of super-resolution and only introduce codebook to model the prior of missing textures. While the reconstruction aware prediction strategy makes use of the straight-through estimator to directly train index predictor with image-level supervision. Our proposed generative SR model (TVQ&RAP) is able to deliver photo-realistic SR results with small computational cost.", "AI": {"tldr": "\u63d0\u51fa\u7eb9\u7406\u5411\u91cf\u91cf\u5316\u548c\u91cd\u5efa\u611f\u77e5\u9884\u6d4b\u7b56\u7565\uff0c\u89e3\u51b3VQ\u65b9\u6cd5\u5728\u89c6\u89c9\u5148\u9a8c\u5efa\u6a21\u4e2d\u7684\u91cf\u5316\u8bef\u5dee\u548c\u6b21\u4f18\u9884\u6d4b\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u8d85\u5206\u8fa8\u7387\u91cd\u5efa", "motivation": "\u73b0\u6709VQ\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a1\uff09\u89c6\u89c9\u7279\u5f81\u4e30\u5bcc\u5bfc\u81f4VQ\u7f16\u7801\u4ea7\u751f\u8f83\u5927\u91cf\u5316\u8bef\u5dee\uff1b2\uff09\u4f7f\u7528\u4ee3\u7801\u7ea7\u76d1\u7763\u8bad\u7ec3\u9884\u6d4b\u5668\u65e0\u6cd5\u8003\u8651\u6700\u7ec8\u91cd\u5efa\u8bef\u5dee\uff0c\u5bfc\u81f4\u5148\u9a8c\u5efa\u6a21\u7cbe\u5ea6\u6b21\u4f18", "method": "1. \u7eb9\u7406\u5411\u91cf\u91cf\u5316\uff1a\u5229\u7528\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u7279\u6027\uff0c\u4ec5\u5bf9\u7f3a\u5931\u7eb9\u7406\u5f15\u5165\u7801\u672c\u5efa\u6a21\u5148\u9a8c\uff1b2. \u91cd\u5efa\u611f\u77e5\u9884\u6d4b\uff1a\u4f7f\u7528\u76f4\u901a\u4f30\u8ba1\u5668\u76f4\u63a5\u4ee5\u56fe\u50cf\u7ea7\u76d1\u7763\u8bad\u7ec3\u7d22\u5f15\u9884\u6d4b\u5668", "result": "\u63d0\u51fa\u7684TVQ&RAP\u751f\u6210\u5f0fSR\u6a21\u578b\u80fd\u591f\u4ee5\u8f83\u5c0f\u8ba1\u7b97\u6210\u672c\u5b9e\u73b0\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u7684\u8d85\u5206\u8fa8\u7387\u7ed3\u679c", "conclusion": "\u901a\u8fc7\u7eb9\u7406\u5411\u91cf\u91cf\u5316\u548c\u91cd\u5efa\u611f\u77e5\u9884\u6d4b\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86VQ\u65b9\u6cd5\u5728\u89c6\u89c9\u5148\u9a8c\u5efa\u6a21\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u9ad8\u8d28\u91cf\u7684\u8d85\u5206\u8fa8\u7387\u91cd\u5efa"}}
{"id": "2509.23828", "pdf": "https://arxiv.org/pdf/2509.23828", "abs": "https://arxiv.org/abs/2509.23828", "authors": ["Hanyu Zhou", "Gim Hee Lee"], "title": "Uni4D-LLM: A Unified SpatioTemporal-Aware VLM for 4D Understanding and Generation", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) have demonstrated strong performance in 2D scene understanding and generation, but extending this unification to the physical world remains an open challenge. Existing 3D and 4D approaches typically embed scene geometry into autoregressive model for semantic understanding and diffusion model for content generation. This paradigm gap prevents a single model from jointly handling both tasks, especially in dynamic 4D settings where spatiotemporal modeling is critical. We propose Uni4D-LLM, the first unified VLM framework with spatiotemporal awareness for 4D scene understanding and generation. Our design is guided by two key insights: 1) Unification requires a shared representation. We extract semantic features for understanding and noisy-injected appearance features for generation, incorporate 4D geometric cues, and fuse them into a spatiotemporal-aware visual representation through adaptive cross-attention. 2) Unification requires a shared architecture. Both autoregression and diffusion are built on Transformer backbones, and this enables integration into a single LLM with task-specific heads. By aligning visual and linguistic representations, our Uni4D-LLM produces predictions for both understanding and generation within one Transformer-based framework. We further apply instruction fine-tuning on diverse 4D vision-language datasets to improve generalization across tasks. Extensive experiments on multiple benchmarks demonstrate that Uni4D-LLM achieves competitive or superior results compared to state-of-the-art models and offers the first true unification of 4D scene understanding and generation.", "AI": {"tldr": "Uni4D-LLM\u662f\u9996\u4e2a\u7edf\u4e004D\u573a\u666f\u7406\u89e3\u548c\u751f\u6210\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u8868\u793a\u548c\u67b6\u6784\u5b9e\u73b0\u65f6\u7a7a\u611f\u77e5\u7684\u7edf\u4e00\u5904\u7406\u3002", "motivation": "\u73b0\u67093D\u548c4D\u65b9\u6cd5\u901a\u5e38\u5728\u8bed\u4e49\u7406\u89e3\u4e0a\u4f7f\u7528\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u5728\u5185\u5bb9\u751f\u6210\u4e0a\u4f7f\u7528\u6269\u6563\u6a21\u578b\uff0c\u8fd9\u79cd\u8303\u5f0f\u5dee\u8ddd\u963b\u788d\u4e86\u5355\u4e00\u6a21\u578b\u540c\u65f6\u5904\u7406\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u65f6\u7a7a\u5efa\u6a21\u7684\u52a8\u60014D\u573a\u666f\u4e2d\u3002", "method": "1) \u63d0\u53d6\u8bed\u4e49\u7279\u5f81\u7528\u4e8e\u7406\u89e3\uff0c\u6ce8\u5165\u566a\u58f0\u7684\u5916\u89c2\u7279\u5f81\u7528\u4e8e\u751f\u6210\uff0c\u7ed3\u54084D\u51e0\u4f55\u7ebf\u7d22\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u6210\u65f6\u7a7a\u611f\u77e5\u7684\u89c6\u89c9\u8868\u793a\uff1b2) \u5c06\u81ea\u56de\u5f52\u548c\u6269\u6563\u96c6\u6210\u5230\u5355\u4e00LLM\u4e2d\uff0c\u4f7f\u7528\u4efb\u52a1\u7279\u5b9a\u7684\u5934\u90e8\uff1b3) \u5728\u591a\u68374D\u89c6\u89c9\u8bed\u8a00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6307\u4ee4\u5fae\u8c03\u4ee5\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cUni4D-LLM\u76f8\u6bd4\u6700\u5148\u8fdb\u6a21\u578b\u53d6\u5f97\u4e86\u7ade\u4e89\u6027\u6216\u66f4\u4f18\u7684\u7ed3\u679c\uff0c\u5b9e\u73b0\u4e864D\u573a\u666f\u7406\u89e3\u548c\u751f\u6210\u7684\u771f\u6b63\u7edf\u4e00\u3002", "conclusion": "Uni4D-LLM\u901a\u8fc7\u5171\u4eab\u8868\u793a\u548c\u67b6\u6784\u7684\u8bbe\u8ba1\uff0c\u6210\u529f\u5b9e\u73b0\u4e864D\u573a\u666f\u7406\u89e3\u548c\u751f\u6210\u7684\u7edf\u4e00\uff0c\u4e3a\u7269\u7406\u4e16\u754c\u7684\u89c6\u89c9\u8bed\u8a00\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23859", "pdf": "https://arxiv.org/pdf/2509.23859", "abs": "https://arxiv.org/abs/2509.23859", "authors": ["Djamel Eddine Boukhari"], "title": "FairViT-GAN: A Hybrid Vision Transformer with Adversarial Debiasing for Fair and Explainable Facial Beauty Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Facial Beauty Prediction (FBP) has made significant strides with the application of deep learning, yet state-of-the-art models often exhibit critical limitations, including architectural constraints, inherent demographic biases, and a lack of transparency. Existing methods, primarily based on Convolutional Neural Networks (CNNs), excel at capturing local texture but struggle with global facial harmony, while Vision Transformers (ViTs) effectively model long-range dependencies but can miss fine-grained details. Furthermore, models trained on benchmark datasets can inadvertently learn and perpetuate societal biases related to protected attributes like ethnicity. To address these interconnected challenges, we propose \\textbf{FairViT-GAN}, a novel hybrid framework that synergistically integrates a CNN branch for local feature extraction and a ViT branch for global context modeling. More significantly, we introduce an adversarial debiasing mechanism where the feature extractor is explicitly trained to produce representations that are invariant to protected attributes, thereby actively mitigating algorithmic bias. Our framework's transparency is enhanced by visualizing the distinct focus of each architectural branch. Extensive experiments on the SCUT-FBP5500 benchmark demonstrate that FairViT-GAN not only sets a new state-of-the-art in predictive accuracy, achieving a Pearson Correlation of \\textbf{0.9230} and reducing RMSE to \\textbf{0.2650}, but also excels in fairness. Our analysis reveals a remarkable \\textbf{82.9\\% reduction in the performance gap} between ethnic subgroups, with the adversary's classification accuracy dropping to near-random chance (52.1\\%). We believe FairViT-GAN provides a robust, transparent, and significantly fairer blueprint for developing responsible AI systems for subjective visual assessment.", "AI": {"tldr": "\u63d0\u51faFairViT-GAN\u6846\u67b6\uff0c\u7ed3\u5408CNN\u548cViT\u4f18\u52bf\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u53bb\u504f\u673a\u5236\u89e3\u51b3\u9762\u90e8\u7f8e\u9884\u6d4b\u4e2d\u7684\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u9762\u90e8\u7f8e\u9884\u6d4b\u6a21\u578b\u5b58\u5728\u67b6\u6784\u9650\u5236\u3001\u4eba\u53e3\u7edf\u8ba1\u504f\u89c1\u548c\u7f3a\u4e4f\u900f\u660e\u5ea6\u7b49\u95ee\u9898\uff0c\u9700\u8981\u540c\u65f6\u89e3\u51b3\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u6311\u6218\u3002", "method": "\u4f7f\u7528CNN\u5206\u652f\u63d0\u53d6\u5c40\u90e8\u7279\u5f81\uff0cViT\u5206\u652f\u5efa\u6a21\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u5e76\u5f15\u5165\u5bf9\u6297\u6027\u53bb\u504f\u673a\u5236\u4f7f\u7279\u5f81\u8868\u793a\u5bf9\u53d7\u4fdd\u62a4\u5c5e\u6027\u4e0d\u53d8\u3002", "result": "\u5728SCUT-FBP5500\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0SOTA\uff0c\u76ae\u5c14\u900a\u76f8\u51730.9230\uff0cRMSE 0.2650\uff0c\u79cd\u65cf\u5b50\u7ec4\u95f4\u6027\u80fd\u5dee\u8ddd\u51cf\u5c1182.9%\uff0c\u5bf9\u6297\u8005\u5206\u7c7b\u51c6\u786e\u7387\u964d\u81f352.1%\u3002", "conclusion": "FairViT-GAN\u4e3a\u5f00\u53d1\u8d1f\u8d23\u4efb\u7684\u4e3b\u89c2\u89c6\u89c9\u8bc4\u4f30AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7a33\u5065\u3001\u900f\u660e\u4e14\u66f4\u516c\u5e73\u7684\u84dd\u56fe\u3002"}}
{"id": "2509.23876", "pdf": "https://arxiv.org/pdf/2509.23876", "abs": "https://arxiv.org/abs/2509.23876", "authors": ["Ky Dan Nguyen", "Hoang Lam Tran", "Anh-Dung Dinh", "Daochang Liu", "Weidong Cai", "Xiuying Wang", "Chang Xu"], "title": "Not All Tokens are Guided Equal: Improving Guidance in Visual Autoregressive Models", "categories": ["cs.CV", "cs.AI"], "comment": "17 pages, 7 figures", "summary": "Autoregressive (AR) models based on next-scale prediction are rapidly emerging as a powerful tool for image generation, but they face a critical weakness: information inconsistencies between patches across timesteps introduced by progressive resolution scaling. These inconsistencies scatter guidance signals, causing them to drift away from conditioning information and leaving behind ambiguous, unfaithful features. We tackle this challenge with Information-Grounding Guidance (IGG), a novel mechanism that anchors guidance to semantically important regions through attention. By adaptively reinforcing informative patches during sampling, IGG ensures that guidance and content remain tightly aligned. Across both class-conditioned and text-to-image generation tasks, IGG delivers sharper, more coherent, and semantically grounded images, setting a new benchmark for AR-based methods.", "AI": {"tldr": "\u63d0\u51fa\u4fe1\u606f\u63a5\u5730\u5f15\u5bfc(IGG)\u673a\u5236\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5c06\u5f15\u5bfc\u4fe1\u53f7\u951a\u5b9a\u5230\u8bed\u4e49\u91cd\u8981\u533a\u57df\uff0c\u89e3\u51b3\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u56e0\u6e10\u8fdb\u5206\u8fa8\u7387\u7f29\u653e\u5bfc\u81f4\u7684\u4fe1\u606f\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6a21\u578b\u9762\u4e34\u5173\u952e\u5f31\u70b9\uff1a\u6e10\u8fdb\u5206\u8fa8\u7387\u7f29\u653e\u5bfc\u81f4\u8de8\u65f6\u95f4\u6b65\u7684\u8865\u4e01\u95f4\u4fe1\u606f\u4e0d\u4e00\u81f4\uff0c\u8fd9\u4e9b\u4e0d\u4e00\u81f4\u4f1a\u5206\u6563\u5f15\u5bfc\u4fe1\u53f7\uff0c\u4f7f\u5176\u504f\u79bb\u6761\u4ef6\u4fe1\u606f\uff0c\u7559\u4e0b\u6a21\u7cca\u4e0d\u5fe0\u5b9e\u7684\u7279\u5f81\u3002", "method": "\u5f00\u53d1\u4fe1\u606f\u63a5\u5730\u5f15\u5bfc(IGG)\u673a\u5236\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u81ea\u9002\u5e94\u5730\u589e\u5f3a\u91c7\u6837\u8fc7\u7a0b\u4e2d\u7684\u4fe1\u606f\u4e30\u5bcc\u8865\u4e01\uff0c\u786e\u4fdd\u5f15\u5bfc\u548c\u5185\u5bb9\u4fdd\u6301\u7d27\u5bc6\u5bf9\u9f50\u3002", "result": "\u5728\u7c7b\u522b\u6761\u4ef6\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0cIGG\u751f\u6210\u66f4\u6e05\u6670\u3001\u66f4\u8fde\u8d2f\u4e14\u8bed\u4e49\u63a5\u5730\u7684\u56fe\u50cf\uff0c\u4e3a\u57fa\u4e8e\u81ea\u56de\u5f52\u7684\u65b9\u6cd5\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\u3002", "conclusion": "IGG\u901a\u8fc7\u5c06\u5f15\u5bfc\u951a\u5b9a\u5230\u8bed\u4e49\u91cd\u8981\u533a\u57df\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u4fe1\u606f\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2509.23885", "pdf": "https://arxiv.org/pdf/2509.23885", "abs": "https://arxiv.org/abs/2509.23885", "authors": ["Guoquan Wei", "Zekun Zhou", "Liu Shi", "Wenzhe Shan", "Qiegen Liu"], "title": "Tunable-Generalization Diffusion Powered by Self-Supervised Contextual Sub-Data for Low-Dose CT Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Current models based on deep learning for low-dose CT denoising rely heavily on paired data and generalize poorly. Even the more concerned diffusion models need to learn the distribution of clean data for reconstruction, which is difficult to satisfy in medical clinical applications. At the same time, self-supervised-based methods face the challenge of significant degradation of generalizability of models pre-trained for the current dose to expand to other doses. To address these issues, this paper proposes a novel method of tunable-generalization diffusion powered by self-supervised contextual sub-data for low-dose CT reconstruction, named SuperDiff. Firstly, a contextual subdata similarity adaptive sensing strategy is designed for denoising centered on the LDCT projection domain, which provides an initial prior for the subsequent progress. Subsequently, the initial prior is used to combine knowledge distillation with a deep combination of latent diffusion models for optimizing image details. The pre-trained model is used for inference reconstruction, and the pixel-level self-correcting fusion technique is proposed for fine-grained reconstruction of the image domain to enhance the image fidelity, using the initial prior and the LDCT image as a guide. In addition, the technique is flexibly applied to the generalization of upper and lower doses or even unseen doses. Dual-domain strategy cascade for self-supervised LDCT denoising, SuperDiff requires only LDCT projection domain data for training and testing. Full qualitative and quantitative evaluations on both datasets and real data show that SuperDiff consistently outperforms existing state-of-the-art methods in terms of reconstruction and generalization performance.", "AI": {"tldr": "\u63d0\u51faSuperDiff\u65b9\u6cd5\uff0c\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u4e0a\u4e0b\u6587\u5b50\u6570\u636e\u7684\u53ef\u8c03\u6cdb\u5316\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u4f4e\u5242\u91cfCT\u91cd\u5efa\uff0c\u65e0\u9700\u914d\u5bf9\u6570\u636e\u4e14\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4f4e\u5242\u91cfCT\u53bb\u566a\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u914d\u5bf9\u6570\u636e\u4e14\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u6269\u6563\u6a21\u578b\u9700\u8981\u5b66\u4e60\u5e72\u51c0\u6570\u636e\u5206\u5e03\uff0c\u8fd9\u5728\u533b\u7597\u4e34\u5e8a\u5e94\u7528\u4e2d\u96be\u4ee5\u6ee1\u8db3\u3002\u81ea\u76d1\u7763\u65b9\u6cd5\u9762\u4e34\u4ece\u5f53\u524d\u5242\u91cf\u9884\u8bad\u7ec3\u6a21\u578b\u6269\u5c55\u5230\u5176\u4ed6\u5242\u91cf\u65f6\u6cdb\u5316\u80fd\u529b\u663e\u8457\u4e0b\u964d\u7684\u6311\u6218\u3002", "method": "1. \u8bbe\u8ba1\u4e0a\u4e0b\u6587\u5b50\u6570\u636e\u76f8\u4f3c\u6027\u81ea\u9002\u5e94\u611f\u77e5\u7b56\u7565\u7528\u4e8e\u6295\u5f71\u57df\u53bb\u566a\uff1b2. \u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u4e0e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u4f18\u5316\u56fe\u50cf\u7ec6\u8282\uff1b3. \u4f7f\u7528\u50cf\u7d20\u7ea7\u81ea\u6821\u6b63\u878d\u5408\u6280\u672f\u589e\u5f3a\u56fe\u50cf\u4fdd\u771f\u5ea6\uff1b4. \u7075\u6d3b\u5e94\u7528\u4e8e\u4e0a\u4e0b\u5242\u91cf\u751a\u81f3\u672a\u89c1\u5242\u91cf\u7684\u6cdb\u5316\u3002", "result": "\u5728\u6570\u636e\u96c6\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5168\u9762\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u8868\u660e\uff0cSuperDiff\u5728\u91cd\u5efa\u548c\u6cdb\u5316\u6027\u80fd\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "SuperDiff\u901a\u8fc7\u53cc\u57df\u7b56\u7565\u7ea7\u8054\u5b9e\u73b0\u81ea\u76d1\u7763\u4f4e\u5242\u91cfCT\u53bb\u566a\uff0c\u4ec5\u9700\u4f4e\u5242\u91cfCT\u6295\u5f71\u57df\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\uff0c\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.23919", "pdf": "https://arxiv.org/pdf/2509.23919", "abs": "https://arxiv.org/abs/2509.23919", "authors": ["Longtao Jiang", "Mingfei Han", "Lei Chen", "Yongqiang Yu", "Feng Zhao", "Xiaojun Chang", "Zhihui Li"], "title": "Token Painter: Training-Free Text-Guided Image Inpainting via Mask Autoregressive Models", "categories": ["cs.CV"], "comment": null, "summary": "Text-guided image inpainting aims to inpaint masked image regions based on a textual prompt while preserving the background. Although diffusion-based methods have become dominant, their property of modeling the entire image in latent space makes it challenging for the results to align well with prompt details and maintain a consistent background. To address these issues, we explore Mask AutoRegressive (MAR) models for this task. MAR naturally supports image inpainting by generating latent tokens corresponding to mask regions, enabling better local controllability without altering the background. However, directly applying MAR to this task makes the inpainting content either ignore the prompts or be disharmonious with the background context. Through analysis of the attention maps from the inpainting images, we identify the impact of background tokens on text tokens during the MAR generation, and leverage this to design \\textbf{Token Painter}, a training-free text-guided image inpainting method based on MAR. Our approach introduces two key components: (1) Dual-Stream Encoder Information Fusion (DEIF), which fuses the semantic and context information from text and background in frequency domain to produce novel guidance tokens, allowing MAR to generate text-faithful inpainting content while keeping harmonious with background context. (2) Adaptive Decoder Attention Score Enhancing (ADAE), which adaptively enhances attention scores on guidance tokens and inpainting tokens to further enhance the alignment of prompt details and the content visual quality. Extensive experiments demonstrate that our training-free method outperforms prior state-of-the-art methods across almost all metrics and delivers superior visual results. Codes will be released.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a9\u7801\u81ea\u56de\u5f52\u6a21\u578b\u7684\u514d\u8bad\u7ec3\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5Token Painter\uff0c\u901a\u8fc7\u53cc\u6d41\u7f16\u7801\u5668\u4fe1\u606f\u878d\u5408\u548c\u81ea\u9002\u5e94\u89e3\u7801\u5668\u6ce8\u610f\u529b\u589e\u5f3a\uff0c\u5728\u4fdd\u6301\u80cc\u666f\u4e00\u81f4\u6027\u7684\u540c\u65f6\u751f\u6210\u4e0e\u6587\u672c\u63d0\u793a\u5bf9\u9f50\u7684\u4fee\u590d\u5185\u5bb9\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u4fee\u590d\u4e2d\u9762\u4e34\u6311\u6218\uff1a\u5728\u6f5c\u5728\u7a7a\u95f4\u5efa\u6a21\u6574\u4e2a\u56fe\u50cf\u5bfc\u81f4\u7ed3\u679c\u96be\u4ee5\u4e0e\u63d0\u793a\u7ec6\u8282\u5bf9\u9f50\u4e14\u96be\u4ee5\u4fdd\u6301\u80cc\u666f\u4e00\u81f4\u6027\u3002\u63a9\u7801\u81ea\u56de\u5f52\u6a21\u578b\u867d\u7136\u652f\u6301\u56fe\u50cf\u4fee\u590d\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u4f1a\u5ffd\u7565\u63d0\u793a\u6216\u4e0e\u80cc\u666f\u4e0d\u534f\u8c03\u3002", "method": "1. \u53cc\u6d41\u7f16\u7801\u5668\u4fe1\u606f\u878d\u5408(DEIF)\uff1a\u5728\u9891\u57df\u878d\u5408\u6587\u672c\u548c\u80cc\u666f\u7684\u8bed\u4e49\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u751f\u6210\u5f15\u5bfc\u6807\u8bb0\uff1b2. \u81ea\u9002\u5e94\u89e3\u7801\u5668\u6ce8\u610f\u529b\u589e\u5f3a(ADAE)\uff1a\u81ea\u9002\u5e94\u589e\u5f3a\u5bf9\u5f15\u5bfc\u6807\u8bb0\u548c\u4fee\u590d\u6807\u8bb0\u7684\u6ce8\u610f\u529b\u5206\u6570\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u514d\u8bad\u7ec3\u65b9\u6cd5\u5728\u51e0\u4e4e\u6240\u6709\u6307\u6807\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u66f4\u4f18\u8d8a\u7684\u89c6\u89c9\u7ed3\u679c\u3002", "conclusion": "Token Painter\u901a\u8fc7\u5206\u6790\u6ce8\u610f\u529b\u56fe\u5e76\u5229\u7528\u80cc\u666f\u6807\u8bb0\u5bf9\u6587\u672c\u6807\u8bb0\u7684\u5f71\u54cd\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6587\u672c\u5bf9\u9f50\u548c\u80cc\u666f\u534f\u8c03\u6027\uff0c\u4e3a\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u4fee\u590d\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23947", "pdf": "https://arxiv.org/pdf/2509.23947", "abs": "https://arxiv.org/abs/2509.23947", "authors": ["Drago\u015f-Andrei Chileban", "Andrei-\u015etefan Bulzan", "Cosmin Cern\u01cezanu-Gl\u01cevan"], "title": "CrashSplat: 2D to 3D Vehicle Damage Segmentation in Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Automatic car damage detection has been a topic of significant interest for the auto insurance industry as it promises faster, accurate, and cost-effective damage assessments. However, few works have gone beyond 2D image analysis to leverage 3D reconstruction methods, which have the potential to provide a more comprehensive and geometrically accurate representation of the damage. Moreover, recent methods employing 3D representations for novel view synthesis, particularly 3D Gaussian Splatting (3D-GS), have demonstrated the ability to generate accurate and coherent 3D reconstructions from a limited number of views. In this work we introduce an automatic car damage detection pipeline that performs 3D damage segmentation by up-lifting 2D masks. Additionally, we propose a simple yet effective learning-free approach for single-view 3D-GS segmentation. Specifically, Gaussians are projected onto the image plane using camera parameters obtained via Structure from Motion (SfM). They are then filtered through an algorithm that utilizes Z-buffering along with a normal distribution model of depth and opacities. Through experiments we found that this method is particularly effective for challenging scenarios like car damage detection, where target objects (e.g., scratches, small dents) may only be clearly visible in a single view, making multi-view consistency approaches impractical or impossible. The code is publicly available at: https://github.com/DragosChileban/CrashSplat.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u6c7d\u8f66\u635f\u4f24\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u89c6\u56fe\u5206\u5272\u5b9e\u73b03D\u635f\u4f24\u5206\u5272\uff0c\u7279\u522b\u9002\u7528\u4e8e\u53ea\u5728\u5355\u89c6\u89d2\u53ef\u89c1\u7684\u5c0f\u635f\u4f24\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u57fa\u4e8e2D\u56fe\u50cf\u5206\u6790\uff0c\u800c3D\u91cd\u5efa\u65b9\u6cd5\u80fd\u63d0\u4f9b\u66f4\u5168\u9762\u548c\u51e0\u4f55\u7cbe\u786e\u7684\u635f\u4f24\u8868\u793a\u30023D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u80fd\u4ece\u6709\u9650\u89c6\u89d2\u751f\u6210\u51c6\u786e\u76843D\u91cd\u5efa\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u591a\u89c6\u89d2\u4e00\u81f4\u6027\uff0c\u4e0d\u9002\u7528\u4e8e\u53ea\u5728\u5355\u89c6\u89d2\u53ef\u89c1\u7684\u635f\u4f24\u3002", "method": "\u63d0\u51fa\u81ea\u52a8\u6c7d\u8f66\u635f\u4f24\u68c0\u6d4b\u6d41\u7a0b\uff0c\u901a\u8fc72D\u63a9\u7801\u63d0\u5347\u5b9e\u73b03D\u635f\u4f24\u5206\u5272\u3002\u91c7\u7528\u65e0\u9700\u5b66\u4e60\u7684\u65b9\u6cd5\u8fdb\u884c\u5355\u89c6\u56fe3D\u9ad8\u65af\u6cfc\u6e85\u5206\u5272\uff1a\u5c06\u9ad8\u65af\u6295\u5f71\u5230\u56fe\u50cf\u5e73\u9762\uff0c\u4f7f\u7528SfM\u83b7\u53d6\u76f8\u673a\u53c2\u6570\uff0c\u901a\u8fc7Z\u7f13\u51b2\u548c\u6df1\u5ea6/\u4e0d\u900f\u660e\u5ea6\u7684\u6b63\u6001\u5206\u5e03\u6a21\u578b\u8fdb\u884c\u8fc7\u6ee4\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6c7d\u8f66\u635f\u4f24\u68c0\u6d4b\u573a\u666f\u4e2d\u7279\u522b\u6709\u6548\uff0c\u5982\u5212\u75d5\u548c\u5c0f\u51f9\u9677\u7b49\u76ee\u6807\u7269\u4f53\u53ef\u80fd\u53ea\u5728\u5355\u89c6\u89d2\u6e05\u6670\u53ef\u89c1\u7684\u60c5\u51b5\u3002", "conclusion": "\u63d0\u51fa\u7684\u5355\u89c6\u56fe3D\u9ad8\u65af\u6cfc\u6e85\u5206\u5272\u65b9\u6cd5\u4e3a\u6c7d\u8f66\u635f\u4f24\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u65b9\u6cd5\u4e0d\u9002\u7528\u6216\u4e0d\u53ef\u884c\u7684\u573a\u666f\u3002"}}
{"id": "2509.23951", "pdf": "https://arxiv.org/pdf/2509.23951", "abs": "https://arxiv.org/abs/2509.23951", "authors": ["Siyu Cao", "Hangting Chen", "Peng Chen", "Yiji Cheng", "Yutao Cui", "Xinchi Deng", "Ying Dong", "Kipper Gong", "Tianpeng Gu", "Xiusen Gu", "Tiankai Hang", "Duojun Huang", "Jie Jiang", "Zhengkai Jiang", "Weijie Kong", "Changlin Li", "Donghao Li", "Junzhe Li", "Xin Li", "Yang Li", "Zhenxi Li", "Zhimin Li", "Jiaxin Lin", "Linus", "Lucaz Liu", "Shu Liu", "Songtao Liu", "Yu Liu", "Yuhong Liu", "Yanxin Long", "Fanbin Lu", "Qinglin Lu", "Yuyang Peng", "Yuanbo Peng", "Xiangwei Shen", "Yixuan Shi", "Jiale Tao", "Yangyu Tao", "Qi Tian", "Pengfei Wan", "Chunyu Wang", "Kai Wang", "Lei Wang", "Linqing Wang", "Lucas Wang", "Qixun Wang", "Weiyan Wang", "Hao Wen", "Bing Wu", "Jianbing Wu", "Yue Wu", "Senhao Xie", "Fang Yang", "Miles Yang", "Xiaofeng Yang", "Xuan Yang", "Zhantao Yang", "Jingmiao Yu", "Zheng Yuan", "Chao Zhang", "Jian-Wei Zhang", "Peizhen Zhang", "Shi-Xue Zhang", "Tao Zhang", "Weigang Zhang", "Yepeng Zhang", "Yingfang Zhang", "Zihao Zhang", "Zijian Zhang", "Penghao Zhao", "Zhiyuan Zhao", "Xuefei Zhe", "Jianchen Zhu", "Zhao Zhong"], "title": "HunyuanImage 3.0 Technical Report", "categories": ["cs.CV"], "comment": null, "summary": "We present HunyuanImage 3.0, a native multimodal model that unifies multimodal understanding and generation within an autoregressive framework, with its image generation module publicly available. The achievement of HunyuanImage 3.0 relies on several key components, including meticulous data curation, advanced architecture design, a native Chain-of-Thoughts schema, progressive model pre-training, aggressive model post-training, and an efficient infrastructure that enables large-scale training and inference. With these advancements, we successfully trained a Mixture-of-Experts (MoE) model comprising over 80 billion parameters in total, with 13 billion parameters activated per token during inference, making it the largest and most powerful open-source image generative model to date. We conducted extensive experiments and the results of automatic and human evaluation of text-image alignment and visual quality demonstrate that HunyuanImage 3.0 rivals previous state-of-the-art models. By releasing the code and weights of HunyuanImage 3.0, we aim to enable the community to explore new ideas with a state-of-the-art foundation model, fostering a dynamic and vibrant multimodal ecosystem. All open source assets are publicly available at https://github.com/Tencent-Hunyuan/HunyuanImage-3.0", "AI": {"tldr": "HunyuanImage 3.0\u662f\u4e00\u4e2a\u7edf\u4e00\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u7684\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u901a\u8fc7\u7cbe\u5fc3\u6570\u636e\u51c6\u5907\u3001\u5148\u8fdb\u67b6\u6784\u8bbe\u8ba1\u3001\u601d\u7ef4\u94fe\u673a\u5236\u7b49\u5173\u952e\u6280\u672f\uff0c\u8bad\u7ec3\u4e86\u8d85\u8fc7800\u4ebf\u53c2\u6570\u7684MoE\u6a21\u578b\uff0c\u5728\u6587\u672c\u56fe\u50cf\u5bf9\u9f50\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u7edf\u4e00\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u7684\u5f3a\u5927\u57fa\u7840\u6a21\u578b\uff0c\u63a8\u52a8\u591a\u6a21\u6001\u751f\u6001\u7cfb\u7edf\u53d1\u5c55\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u6700\u5148\u8fdb\u7684\u56fe\u50cf\u751f\u6210\u80fd\u529b\u3002", "method": "\u91c7\u7528\u81ea\u56de\u5f52\u6846\u67b6\u7edf\u4e00\u591a\u6a21\u6001\u4efb\u52a1\uff0c\u5305\u542b\u6570\u636e\u51c6\u5907\u3001\u67b6\u6784\u8bbe\u8ba1\u3001\u539f\u751f\u601d\u7ef4\u94fe\u3001\u6e10\u8fdb\u9884\u8bad\u7ec3\u3001\u79ef\u6781\u540e\u8bad\u7ec3\u7b49\u5173\u952e\u6280\u672f\uff0c\u8bad\u7ec3\u4e86800\u4ebf\u53c2\u6570\u7684MoE\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u6587\u672c\u56fe\u50cf\u5bf9\u9f50\u548c\u89c6\u89c9\u8d28\u91cf\u8bc4\u4f30\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u662f\u5f53\u524d\u6700\u5927\u6700\u5f3a\u7684\u5f00\u6e90\u56fe\u50cf\u751f\u6210\u6a21\u578b\u3002", "conclusion": "HunyuanImage 3.0\u6210\u529f\u5c55\u793a\u4e86\u7edf\u4e00\u591a\u6a21\u6001\u6846\u67b6\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u5f00\u6e90\u4ee3\u7801\u548c\u6743\u91cd\u4fc3\u8fdb\u591a\u6a21\u6001\u7814\u7a76\u793e\u533a\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.23958", "pdf": "https://arxiv.org/pdf/2509.23958", "abs": "https://arxiv.org/abs/2509.23958", "authors": ["Yang Ye", "Tianyu He", "Shuo Yang", "Jiang Bian"], "title": "Reinforcement Learning with Inverse Rewards for World Model Post-training", "categories": ["cs.CV"], "comment": null, "summary": "World models simulate dynamic environments, enabling agents to interact with diverse input modalities. Although recent advances have improved the visual quality and temporal consistency of video world models, their ability of accurately modeling human-specified actions remains under-explored. Reinforcement learning presents a promising approach for directly improving the suboptimal action-following capability of pre-trained models, assuming that an appropriate reward function can be defined. However, transferring reinforcement learning post-training methods to world model is impractical due to the prohibitive cost of large-scale preference annotations and the infeasibility of constructing rule-based video verifiers. To address this gap, we propose Reinforcement Learning with Inverse Rewards (RLIR), a post-training framework that derives verifiable reward signals by recovering input actions from generated videos using an Inverse Dynamics Model. By mapping high-dimensional video modality to a low-dimensional action space, RLIR provides an objective and verifiable reward for optimization via Group Relative Policy Optimization. Experiments across autoregressive and diffusion paradigms demonstrate 5-10% gains in action-following, up to 10% improvements in visual quality, and higher human preference scores, establishing RLIR as the first post-training method specifically designed to enhance action-following in video world models.", "AI": {"tldr": "\u63d0\u51fa\u4e86RLIR\u6846\u67b6\uff0c\u901a\u8fc7\u9006\u5411\u52a8\u529b\u5b66\u6a21\u578b\u4ece\u751f\u6210\u89c6\u9891\u4e2d\u6062\u590d\u8f93\u5165\u52a8\u4f5c\uff0c\u4e3a\u89c6\u9891\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u63d0\u5347\u52a8\u4f5c\u8ddf\u968f\u80fd\u529b", "motivation": "\u73b0\u6709\u89c6\u9891\u4e16\u754c\u6a21\u578b\u5728\u52a8\u4f5c\u8ddf\u968f\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u65b9\u6cd5\u4e0d\u73b0\u5b9e\uff0c\u56e0\u4e3a\u5927\u89c4\u6a21\u504f\u597d\u6807\u6ce8\u6210\u672c\u8fc7\u9ad8\u4e14\u96be\u4ee5\u6784\u5efa\u57fa\u4e8e\u89c4\u5219\u7684\u89c6\u9891\u9a8c\u8bc1\u5668", "method": "\u4f7f\u7528\u9006\u5411\u52a8\u529b\u5b66\u6a21\u578b\u5c06\u9ad8\u7ef4\u89c6\u9891\u6a21\u6001\u6620\u5c04\u5230\u4f4e\u7ef4\u52a8\u4f5c\u7a7a\u95f4\uff0c\u901a\u8fc7\u6062\u590d\u8f93\u5165\u52a8\u4f5c\u6765\u83b7\u5f97\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u91c7\u7528\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u8fdb\u884c\u4f18\u5316", "result": "\u5728\u81ea\u56de\u5f52\u548c\u6269\u6563\u8303\u5f0f\u4e0b\uff0c\u52a8\u4f5c\u8ddf\u968f\u80fd\u529b\u63d0\u53475-10%\uff0c\u89c6\u89c9\u8d28\u91cf\u63d0\u5347\u9ad8\u8fbe10%\uff0c\u83b7\u5f97\u66f4\u9ad8\u7684\u4eba\u7c7b\u504f\u597d\u8bc4\u5206", "conclusion": "RLIR\u662f\u9996\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u589e\u5f3a\u89c6\u9891\u4e16\u754c\u6a21\u578b\u52a8\u4f5c\u8ddf\u968f\u80fd\u529b\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5956\u52b1\u4fe1\u53f7\u83b7\u53d6\u7684\u96be\u9898"}}
{"id": "2509.23971", "pdf": "https://arxiv.org/pdf/2509.23971", "abs": "https://arxiv.org/abs/2509.23971", "authors": ["Kargi Chauhan", "Leilani H. Gilpin"], "title": "VFSI: Validity First Spatial Intelligence for Constraint-Guided Traffic Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Modern diffusion models generate realistic traffic simulations but systematically violate physical constraints. In a large-scale evaluation of SceneDiffuser++, a state-of-the-art traffic simulator, we find that 50% of generated trajectories violate basic physical laws - vehicles collide, drive off roads, and spawn inside buildings. This reveals a fundamental limitation: current models treat physical validity as an emergent property rather than an architectural requirement. We propose Validity-First Spatial Intelligence (VFSI), which enforces constraints through energy-based guidance during diffusion sampling, without model retraining. By incorporating collision avoidance and kinematic constraints as energy functions, we guide the denoising process toward physically valid trajectories. Across 200 urban scenarios from the Waymo Open Motion Dataset, VFSI reduces collision rates by 67% (24.6% to 8.1%) and improves overall validity by 87% (50.3% to 94.2%), while simultaneously improving realism metrics (ADE: 1.34m to 1.21m). Our model-agnostic approach demonstrates that explicit constraint enforcement during inference is both necessary and sufficient for physically valid traffic simulation.", "AI": {"tldr": "\u63d0\u51faVFSI\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u80fd\u91cf\u7684\u5f15\u5bfc\u5728\u6269\u6563\u91c7\u6837\u8fc7\u7a0b\u4e2d\u5f3a\u5236\u6267\u884c\u7269\u7406\u7ea6\u675f\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4ea4\u901a\u4eff\u771f\u7684\u7269\u7406\u6709\u6548\u6027", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u4ea4\u901a\u4eff\u771f\u867d\u7136\u771f\u5b9e\u4f46\u7cfb\u7edf\u6027\u5730\u8fdd\u53cd\u7269\u7406\u7ea6\u675f\uff0c50%\u7684\u8f68\u8ff9\u8fdd\u53cd\u57fa\u672c\u7269\u7406\u5b9a\u5f8b\uff0c\u5982\u8f66\u8f86\u78b0\u649e\u3001\u9a76\u79bb\u9053\u8def\u3001\u5728\u5efa\u7b51\u7269\u5185\u751f\u6210", "method": "VFSI\u65b9\u6cd5\u5728\u6269\u6563\u91c7\u6837\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u57fa\u4e8e\u80fd\u91cf\u7684\u5f15\u5bfc\uff0c\u5c06\u78b0\u649e\u907f\u514d\u548c\u8fd0\u52a8\u5b66\u7ea6\u675f\u4f5c\u4e3a\u80fd\u91cf\u51fd\u6570\uff0c\u5f15\u5bfc\u53bb\u566a\u8fc7\u7a0b\u751f\u6210\u7269\u7406\u6709\u6548\u7684\u8f68\u8ff9", "result": "\u5728Waymo\u5f00\u653e\u8fd0\u52a8\u6570\u636e\u96c6\u7684200\u4e2a\u57ce\u5e02\u573a\u666f\u4e2d\uff0cVFSI\u5c06\u78b0\u649e\u7387\u964d\u4f4e67%\uff0824.6%\u52308.1%\uff09\uff0c\u6574\u4f53\u6709\u6548\u6027\u63d0\u9ad887%\uff0850.3%\u523094.2%\uff09\uff0c\u540c\u65f6\u6539\u8fdb\u771f\u5b9e\u611f\u6307\u6807\uff08ADE\uff1a1.34m\u52301.21m\uff09", "conclusion": "\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u663e\u5f0f\u5f3a\u5236\u6267\u884c\u7ea6\u675f\u5bf9\u4e8e\u7269\u7406\u6709\u6548\u7684\u4ea4\u901a\u4eff\u771f\u662f\u5fc5\u8981\u4e14\u5145\u5206\u7684\uff0c\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\u8bc1\u660e\u4e86\u8fd9\u4e00\u70b9"}}
{"id": "2509.23980", "pdf": "https://arxiv.org/pdf/2509.23980", "abs": "https://arxiv.org/abs/2509.23980", "authors": ["Jinpei Guo", "Yifei Ji", "Zheng Chen", "Yufei Wang", "Sizhuo Ma", "Yong Guo", "Yulun Zhang", "Jian Wang"], "title": "Towards Redundancy Reduction in Diffusion Models for Efficient Video Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have recently shown promising results for video super-resolution (VSR). However, directly adapting generative diffusion models to VSR can result in redundancy, since low-quality videos already preserve substantial content information. Such redundancy leads to increased computational overhead and learning burden, as the model performs superfluous operations and must learn to filter out irrelevant information. To address this problem, we propose OASIS, an efficient $\\textbf{o}$ne-step diffusion model with $\\textbf{a}$ttention $\\textbf{s}$pecialization for real-world v$\\textbf{i}$deo $\\textbf{s}$uper-resolution. OASIS incorporates an attention specialization routing that assigns attention heads to different patterns according to their intrinsic behaviors. This routing mitigates redundancy while effectively preserving pretrained knowledge, allowing diffusion models to better adapt to VSR and achieve stronger performance. Moreover, we propose a simple yet effective progressive training strategy, which starts with temporally consistent degradations and then shifts to inconsistent settings. This strategy facilitates learning under complex degradations. Extensive experiments demonstrate that OASIS achieves state-of-the-art performance on both synthetic and real-world datasets. OASIS also provides superior inference speed, offering a $\\textbf{6.2$\\times$}$ speedup over one-step diffusion baselines such as SeedVR2. The code will be available at \\href{https://github.com/jp-guo/OASIS}{https://github.com/jp-guo/OASIS}.", "AI": {"tldr": "OASIS\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5355\u6b65\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u4e13\u4e1a\u5316\u8def\u7531\u548c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u548c6.2\u500d\u52a0\u901f\u3002", "motivation": "\u76f4\u63a5\u5e94\u7528\u751f\u6210\u5f0f\u6269\u6563\u6a21\u578b\u5230\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4f1a\u4ea7\u751f\u5197\u4f59\uff0c\u56e0\u4e3a\u4f4e\u8d28\u91cf\u89c6\u9891\u5df2\u5305\u542b\u5927\u91cf\u5185\u5bb9\u4fe1\u606f\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u589e\u52a0\u548c\u5b66\u4e60\u8d1f\u62c5\u52a0\u91cd\u3002", "method": "\u63d0\u51fa\u6ce8\u610f\u529b\u4e13\u4e1a\u5316\u8def\u7531\uff0c\u6839\u636e\u6ce8\u610f\u529b\u5934\u5185\u5728\u884c\u4e3a\u5206\u914d\u4e0d\u540c\u6a21\u5f0f\uff1b\u91c7\u7528\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\uff0c\u4ece\u65f6\u95f4\u4e00\u81f4\u9000\u5316\u5230\u4e0d\u4e00\u81f4\u9000\u5316\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u76f8\u6bd4SeedVR2\u7b49\u5355\u6b65\u6269\u6563\u57fa\u7ebf\u63d0\u4f9b6.2\u500d\u52a0\u901f\u3002", "conclusion": "OASIS\u901a\u8fc7\u51cf\u5c11\u5197\u4f59\u548c\u6709\u6548\u4fdd\u7559\u9884\u8bad\u7ec3\u77e5\u8bc6\uff0c\u4f7f\u6269\u6563\u6a21\u578b\u66f4\u597d\u5730\u9002\u5e94\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u3002"}}
{"id": "2509.24022", "pdf": "https://arxiv.org/pdf/2509.24022", "abs": "https://arxiv.org/abs/2509.24022", "authors": ["Nate Rothschild", "Moshe Kimhi", "Avi Mendelson", "Chaim Baskin"], "title": "$\\mathbf{R}^3$: Reconstruction, Raw, and Rain: Deraining Directly in the Bayer Domain", "categories": ["cs.CV"], "comment": "9 pages", "summary": "Image reconstruction from corrupted images is crucial across many domains. Most reconstruction networks are trained on post-ISP sRGB images, even though the image-signal-processing pipeline irreversibly mixes colors, clips dynamic range, and blurs fine detail. This paper uses the rain degradation problem as a use case to show that these losses are avoidable, and demonstrates that learning directly on raw Bayer mosaics yields superior reconstructions. To substantiate the claim, we (i) evaluate post-ISP and Bayer reconstruction pipelines, (ii) curate Raw-Rain, the first public benchmark of real rainy scenes captured in both 12-bit Bayer and bit-depth-matched sRGB, and (iii) introduce Information Conservation Score (ICS), a color-invariant metric that aligns more closely with human opinion than PSNR or SSIM. On the test split, our raw-domain model improves sRGB results by up to +0.99 dB PSNR and +1.2% ICS, while running faster with half of the GFLOPs. The results advocate an ISP-last paradigm for low-level vision and open the door to end-to-end learnable camera pipelines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u76f4\u63a5\u5728\u539f\u59cbBayer\u683c\u5f0f\u4e0a\u5b66\u4e60\u56fe\u50cf\u91cd\u5efa\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684sRGB\u540e\u5904\u7406\u6d41\u7a0b\u80fd\u83b7\u5f97\u66f4\u597d\u7684\u53bb\u96e8\u6548\u679c\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807ICS\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u91cd\u5efa\u7f51\u7edc\u5728sRGB\u56fe\u50cf\u4e0a\u8bad\u7ec3\uff0c\u4f46\u56fe\u50cf\u4fe1\u53f7\u5904\u7406(ISP)\u6d41\u7a0b\u4f1a\u4e0d\u53ef\u9006\u5730\u6df7\u5408\u989c\u8272\u3001\u88c1\u526a\u52a8\u6001\u8303\u56f4\u548c\u6a21\u7cca\u7ec6\u8282\u3002\u672c\u6587\u65e8\u5728\u8bc1\u660e\u8fd9\u4e9b\u635f\u5931\u662f\u53ef\u4ee5\u907f\u514d\u7684\u3002", "method": "\u4f7f\u7528\u539f\u59cbBayer\u9a6c\u8d5b\u514b\u76f4\u63a5\u5b66\u4e60\u91cd\u5efa\uff1b\u521b\u5efaRaw-Rain\u57fa\u51c6\u6570\u636e\u96c6\uff1b\u5f15\u5165\u4fe1\u606f\u5b88\u6052\u5206\u6570(ICS)\u8bc4\u4f30\u6307\u6807\uff1b\u6bd4\u8f83\u540eISP\u548cBayer\u91cd\u5efa\u6d41\u7a0b\u3002", "result": "\u5728\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u539f\u59cb\u57df\u6a21\u578b\u5c06sRGB\u7ed3\u679c\u63d0\u5347\u4e86+0.99 dB PSNR\u548c+1.2% ICS\uff0c\u540c\u65f6\u8fd0\u884c\u901f\u5ea6\u66f4\u5feb\u4e14\u8ba1\u7b97\u91cf\u51cf\u534a\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u652f\u6301\u4f4e\u5c42\u89c6\u89c9\u91c7\u7528ISP\u540e\u5904\u7406\u8303\u5f0f\uff0c\u5e76\u4e3a\u7aef\u5230\u7aef\u53ef\u5b66\u4e60\u76f8\u673a\u7ba1\u9053\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2509.24081", "pdf": "https://arxiv.org/pdf/2509.24081", "abs": "https://arxiv.org/abs/2509.24081", "authors": ["Sucheng Ren", "Chen Chen", "Zhenbang Wang", "Liangchen Song", "Xiangxin Zhu", "Alan Yuille", "Yinfei Yang", "Jiasen Lu"], "title": "Autoregressive Video Generation beyond Next Frames Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Autoregressive models for video generation typically operate frame-by-frame, extending next-token prediction from language to video's temporal dimension. We question that unlike word as token is universally agreed in language if frame is a appropriate prediction unit? To address this, we present VideoAR, a unified framework that supports a spectrum of prediction units including full frames, key-detail frames, multiscale refinements, and spatiotemporal cubes. Among these designs, we find model video generation using \\textit{spatiotemporal} cubes as prediction units, which allows autoregressive models to operate across both spatial and temporal dimensions simultaneously. This approach eliminates the assumption that frames are the natural atomic units for video autoregression. We evaluate VideoAR across diverse prediction strategies, finding that cube-based prediction consistently delivers superior quality, speed, and temporal coherence. By removing the frame-by-frame constraint, our video generator surpasses state-of-the-art baselines on VBench while achieving faster inference and enabling seamless scaling to minute-long sequences. We hope this work will motivate rethinking sequence decomposition in video and other spatiotemporal domains.", "AI": {"tldr": "VideoAR\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u652f\u6301\u591a\u79cd\u9884\u6d4b\u5355\u5143\uff08\u5b8c\u6574\u5e27\u3001\u5173\u952e\u7ec6\u8282\u5e27\u3001\u591a\u5c3a\u5ea6\u7ec6\u5316\u3001\u65f6\u7a7a\u7acb\u65b9\u4f53\uff09\uff0c\u5176\u4e2d\u65f6\u7a7a\u7acb\u65b9\u4f53\u4f5c\u4e3a\u9884\u6d4b\u5355\u5143\u5728\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u8d28\u7591\u4f20\u7edf\u9010\u5e27\u9884\u6d4b\u7684\u5408\u7406\u6027\uff0c\u8ba4\u4e3a\u5e27\u4e0d\u4e00\u5b9a\u662f\u89c6\u9891\u81ea\u56de\u5f52\u7684\u81ea\u7136\u539f\u5b50\u5355\u5143\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u5408\u9002\u7684\u9884\u6d4b\u5355\u5143\u3002", "method": "\u5f00\u53d1\u4e86VideoAR\u6846\u67b6\uff0c\u652f\u6301\u591a\u79cd\u9884\u6d4b\u5355\u5143\u9009\u62e9\uff0c\u7279\u522b\u662f\u5f15\u5165\u65f6\u7a7a\u7acb\u65b9\u4f53\u4f5c\u4e3a\u9884\u6d4b\u5355\u5143\uff0c\u8ba9\u81ea\u56de\u5f52\u6a21\u578b\u80fd\u540c\u65f6\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u64cd\u4f5c\u3002", "result": "\u57fa\u4e8e\u7acb\u65b9\u4f53\u7684\u9884\u6d4b\u5728VBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u5e76\u80fd\u65e0\u7f1d\u6269\u5c55\u5230\u5206\u949f\u7ea7\u5e8f\u5217\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u9f13\u52b1\u91cd\u65b0\u601d\u8003\u89c6\u9891\u548c\u5176\u4ed6\u65f6\u7a7a\u57df\u4e2d\u7684\u5e8f\u5217\u5206\u89e3\u65b9\u5f0f\uff0c\u6253\u7834\u4e86\u9010\u5e27\u9884\u6d4b\u7684\u9650\u5236\u3002"}}
{"id": "2509.24128", "pdf": "https://arxiv.org/pdf/2509.24128", "abs": "https://arxiv.org/abs/2509.24128", "authors": ["Chandon Hamel", "Mike Busch"], "title": "GANji: A Framework for Introductory AI Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "The comparative study of generative models often requires significant computational resources, creating a barrier for researchers and practitioners. This paper introduces GANji, a lightweight framework for benchmarking foundational AI image generation techniques using a dataset of 10,314 Japanese Kanji characters. It systematically compares the performance of a Variational Autoencoder (VAE), a Generative Adversarial Network (GAN), and a Denoising Diffusion Probabilistic Model (DDPM). The results demonstrate that while the DDPM achieves the highest image fidelity, with a Fr\\'echet Inception Distance (FID) score of 26.2, its sampling time is over 2,000 times slower than the other models. The GANji framework is an effective and accessible tool for revealing the fundamental trade-offs between model architecture, computational cost, and visual quality, making it ideal for both educational and research purposes.", "AI": {"tldr": "GANji\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u7528\u4e8e\u6bd4\u8f83VAE\u3001GAN\u548cDDPM\u5728\u65e5\u6587\u6c49\u5b57\u751f\u6210\u4e0a\u7684\u6027\u80fd\uff0c\u53d1\u73b0DDPM\u56fe\u50cf\u8d28\u91cf\u6700\u597d\u4f46\u91c7\u6837\u901f\u5ea6\u6700\u6162\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u6bd4\u8f83\u7814\u7a76\u901a\u5e38\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u8fd9\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u8bbe\u7f6e\u4e86\u969c\u788d\u3002", "method": "\u4f7f\u752810,314\u4e2a\u65e5\u6587\u6c49\u5b57\u5b57\u7b26\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u6bd4\u8f83VAE\u3001GAN\u548cDDPM\u4e09\u79cd\u57fa\u7840AI\u56fe\u50cf\u751f\u6210\u6280\u672f\u7684\u6027\u80fd\u3002", "result": "DDPM\u83b7\u5f97\u6700\u9ad8\u56fe\u50cf\u4fdd\u771f\u5ea6\uff08FID\u5f97\u520626.2\uff09\uff0c\u4f46\u5176\u91c7\u6837\u65f6\u95f4\u6bd4\u5176\u4ed6\u6a21\u578b\u61622,000\u591a\u500d\u3002", "conclusion": "GANji\u6846\u67b6\u662f\u63ed\u793a\u6a21\u578b\u67b6\u6784\u3001\u8ba1\u7b97\u6210\u672c\u548c\u89c6\u89c9\u8d28\u91cf\u4e4b\u95f4\u57fa\u672c\u6743\u8861\u7684\u6709\u6548\u4e14\u6613\u7528\u7684\u5de5\u5177\uff0c\u9002\u5408\u6559\u80b2\u548c\u7814\u7a76\u7528\u9014\u3002"}}
{"id": "2509.24142", "pdf": "https://arxiv.org/pdf/2509.24142", "abs": "https://arxiv.org/abs/2509.24142", "authors": ["Jianze Li", "Yong Guo", "Yulun Zhang", "Xiaokang Yang"], "title": "Asymmetric VAE for One-Step Video Super-Resolution Acceleration", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have significant advantages in the field of real-world video super-resolution and have demonstrated strong performance in past research. In recent diffusion-based video super-resolution (VSR) models, the number of sampling steps has been reduced to just one, yet there remains significant room for further optimization in inference efficiency. In this paper, we propose FastVSR, which achieves substantial reductions in computational cost by implementing a high compression VAE (spatial compression ratio of 16, denoted as f16). We design the structure of the f16 VAE and introduce a stable training framework. We employ pixel shuffle and channel replication to achieve additional upsampling. Furthermore, we propose a lower-bound-guided training strategy, which introduces a simpler training objective as a lower bound for the VAE's performance. It makes the training process more stable and easier to converge. Experimental results show that FastVSR achieves speedups of 111.9 times compared to multi-step models and 3.92 times compared to existing one-step models. We will release code and models at https://github.com/JianzeLi-114/FastVSR.", "AI": {"tldr": "FastVSR\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u6a21\u578b\uff0c\u901a\u8fc7\u9ad8\u538b\u7f29VAE\uff08f16\uff09\u548c\u7a33\u5b9a\u8bad\u7ec3\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u63a8\u7406\u901f\u5ea6\uff0c\u76f8\u6bd4\u591a\u6b65\u6a21\u578b\u52a0\u901f111.9\u500d\uff0c\u76f8\u6bd4\u73b0\u6709\u5355\u6b65\u6a21\u578b\u52a0\u901f3.92\u500d\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u867d\u7136\u5c06\u91c7\u6837\u6b65\u9aa4\u51cf\u5c11\u5230\u4e00\u6b65\uff0c\u4f46\u5728\u63a8\u7406\u6548\u7387\u65b9\u9762\u4ecd\u6709\u5f88\u5927\u4f18\u5316\u7a7a\u95f4\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u91c7\u7528\u9ad8\u538b\u7f29VAE\uff08\u7a7a\u95f4\u538b\u7f29\u6bd416\uff0cf16\uff09\uff0c\u8bbe\u8ba1f16 VAE\u7ed3\u6784\u5e76\u5f15\u5165\u7a33\u5b9a\u8bad\u7ec3\u6846\u67b6\uff0c\u4f7f\u7528\u50cf\u7d20\u91cd\u6392\u548c\u901a\u9053\u590d\u5236\u5b9e\u73b0\u989d\u5916\u4e0a\u91c7\u6837\uff0c\u63d0\u51fa\u4e0b\u754c\u5f15\u5bfc\u8bad\u7ec3\u7b56\u7565\u7b80\u5316\u8bad\u7ec3\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aFastVSR\u76f8\u6bd4\u591a\u6b65\u6a21\u578b\u52a0\u901f111.9\u500d\uff0c\u76f8\u6bd4\u73b0\u6709\u5355\u6b65\u6a21\u578b\u52a0\u901f3.92\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u6027\u80fd\u3002", "conclusion": "FastVSR\u901a\u8fc7\u9ad8\u538b\u7f29VAE\u548c\u4f18\u5316\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u4e3a\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24165", "pdf": "https://arxiv.org/pdf/2509.24165", "abs": "https://arxiv.org/abs/2509.24165", "authors": ["Moxin Zhao", "Nan Meng", "Jason Pui Yin Cheung", "Chris Yuk Kwan Tang", "Chenxi Yu", "Wenting Zhong", "Pengyu Lu", "Chang Shi", "Yipeng Zhuang", "Teng Zhang"], "title": "LatXGen: Towards Radiation-Free and Accurate Quantitative Analysis of Sagittal Spinal Alignment Via Cross-Modal Radiographic View Synthesis", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 6 figures", "summary": "Adolescent Idiopathic Scoliosis (AIS) is a complex three-dimensional spinal deformity, and accurate morphological assessment requires evaluating both coronal and sagittal alignment. While previous research has made significant progress in developing radiation-free methods for coronal plane assessment, reliable and accurate evaluation of sagittal alignment without ionizing radiation remains largely underexplored. To address this gap, we propose LatXGen, a novel generative framework that synthesizes realistic lateral spinal radiographs from posterior Red-Green-Blue and Depth (RGBD) images of unclothed backs. This enables accurate, radiation-free estimation of sagittal spinal alignment. LatXGen tackles two core challenges: (1) inferring sagittal spinal morphology changes from a lateral perspective based on posteroanterior surface geometry, and (2) performing cross-modality translation from RGBD input to the radiographic domain. The framework adopts a dual-stage architecture that progressively estimates lateral spinal structure and synthesizes corresponding radiographs. To enhance anatomical consistency, we introduce an attention-based Fast Fourier Convolution (FFC) module for integrating anatomical features from RGBD images and 3D landmarks, and a Spatial Deformation Network (SDN) to model morphological variations in the lateral view. Additionally, we construct the first large-scale paired dataset for this task, comprising 3,264 RGBD and lateral radiograph pairs. Experimental results demonstrate that LatXGen produces anatomically accurate radiographs and outperforms existing GAN-based methods in both visual fidelity and quantitative metrics. This study offers a promising, radiation-free solution for sagittal spine assessment and advances comprehensive AIS evaluation.", "AI": {"tldr": "LatXGen\u662f\u4e00\u4e2a\u751f\u6210\u5f0f\u6846\u67b6\uff0c\u53ef\u4ece\u80cc\u90e8RGBD\u56fe\u50cf\u5408\u6210\u4fa7\u4f4d\u810a\u67f1X\u5149\u7247\uff0c\u5b9e\u73b0\u65e0\u8f90\u5c04\u7684\u77e2\u72b6\u9762\u810a\u67f1\u5bf9\u9f50\u8bc4\u4f30\u3002", "motivation": "\u9752\u5c11\u5e74\u7279\u53d1\u6027\u810a\u67f1\u4fa7\u51f8\u9700\u8981\u51a0\u72b6\u9762\u548c\u77e2\u72b6\u9762\u7684\u4e09\u7ef4\u8bc4\u4f30\uff0c\u4f46\u73b0\u6709\u65e0\u8f90\u5c04\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u51a0\u72b6\u9762\uff0c\u77e2\u72b6\u9762\u8bc4\u4f30\u4ecd\u7f3a\u4e4f\u53ef\u9760\u7684\u65e0\u8f90\u5c04\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u53cc\u9636\u6bb5\u67b6\u6784\uff1a\u9996\u5148\u4f30\u8ba1\u4fa7\u4f4d\u810a\u67f1\u7ed3\u6784\uff0c\u7136\u540e\u5408\u6210\u76f8\u5e94X\u5149\u7247\u3002\u5f15\u5165\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5feb\u901f\u5085\u91cc\u53f6\u5377\u79ef\u6a21\u5757\u6574\u5408\u89e3\u5256\u7279\u5f81\uff0c\u4ee5\u53ca\u7a7a\u95f4\u53d8\u5f62\u7f51\u7edc\u5efa\u6a21\u4fa7\u4f4d\u5f62\u6001\u53d8\u5316\u3002", "result": "LatXGen\u751f\u6210\u89e3\u5256\u5b66\u51c6\u786e\u7684X\u5149\u7247\uff0c\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u5b9a\u91cf\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709GAN\u65b9\u6cd5\u3002\u6784\u5efa\u4e86\u5305\u542b3,264\u5bf9RGBD\u548c\u4fa7\u4f4dX\u5149\u7247\u7684\u5927\u89c4\u6a21\u914d\u5bf9\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u77e2\u72b6\u9762\u810a\u67f1\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65e0\u8f90\u5c04\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u9752\u5c11\u5e74\u7279\u53d1\u6027\u810a\u67f1\u4fa7\u51f8\u7684\u5168\u9762\u8bc4\u4f30\u3002"}}
{"id": "2509.24182", "pdf": "https://arxiv.org/pdf/2509.24182", "abs": "https://arxiv.org/abs/2509.24182", "authors": ["Jonghun Kim", "Inye Na", "Eun Sook Ko", "Hyunjin Park"], "title": "Tumor Synthesis conditioned on Radiomics", "categories": ["cs.CV"], "comment": "WACV'25", "summary": "Due to privacy concerns, obtaining large datasets is challenging in medical image analysis, especially with 3D modalities like Computed Tomography (CT) and Magnetic Resonance Imaging (MRI). Existing generative models, developed to address this issue, often face limitations in output diversity and thus cannot accurately represent 3D medical images. We propose a tumor-generation model that utilizes radiomics features as generative conditions. Radiomics features are high-dimensional handcrafted semantic features that are biologically well-grounded and thus are good candidates for conditioning. Our model employs a GAN-based model to generate tumor masks and a diffusion-based approach to generate tumor texture conditioned on radiomics features. Our method allows the user to generate tumor images according to user-specified radiomics features such as size, shape, and texture at an arbitrary location. This enables the physicians to easily visualize tumor images to better understand tumors according to changing radiomics features. Our approach allows for the removal, manipulation, and repositioning of tumors, generating various tumor types in different scenarios. The model has been tested on tumors in four different organs (kidney, lung, breast, and brain) across CT and MRI. The synthesized images are shown to effectively aid in training for downstream tasks and their authenticity was also evaluated through expert evaluations. Our method has potential usage in treatment planning with diverse synthesized tumors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u7684\u80bf\u7624\u751f\u6210\u6a21\u578b\uff0c\u4f7f\u7528GAN\u751f\u6210\u80bf\u7624\u63a9\u819c\uff0c\u6269\u6563\u6a21\u578b\u751f\u6210\u80bf\u7624\u7eb9\u7406\uff0c\u53ef\u6839\u636e\u7528\u6237\u6307\u5b9a\u7684\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u5728\u4efb\u610f\u4f4d\u7f6e\u751f\u6210\u80bf\u7624\u56fe\u50cf\u3002", "motivation": "\u7531\u4e8e\u9690\u79c1\u95ee\u9898\uff0c\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u83b7\u53d6\u5927\u578b3D\u6570\u636e\u96c6\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u8f93\u51fa\u591a\u6837\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u51c6\u786e\u8868\u793a3D\u533b\u5b66\u56fe\u50cf\u3002", "method": "\u7ed3\u5408GAN\u548c\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff1aGAN\u751f\u6210\u80bf\u7624\u63a9\u819c\uff0c\u6269\u6563\u6a21\u578b\u751f\u6210\u80bf\u7624\u7eb9\u7406\uff0c\u4e24\u8005\u90fd\u4ee5\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u4e3a\u751f\u6210\u6761\u4ef6\u3002", "result": "\u5728\u56db\u4e2a\u4e0d\u540c\u5668\u5b98\uff08\u80be\u810f\u3001\u80ba\u3001\u4e73\u817a\u3001\u8111\uff09\u7684CT\u548cMRI\u4e0a\u6d4b\u8bd5\uff0c\u5408\u6210\u56fe\u50cf\u6709\u6548\u8f85\u52a9\u4e0b\u6e38\u4efb\u52a1\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u8bc4\u4f30\u9a8c\u8bc1\u771f\u5b9e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u7528\u4e8e\u6cbb\u7597\u89c4\u5212\uff0c\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u80bf\u7624\u56fe\u50cf\u5e2e\u52a9\u533b\u751f\u66f4\u597d\u5730\u7406\u89e3\u80bf\u7624\u7279\u5f81\u53d8\u5316\u3002"}}
{"id": "2509.24185", "pdf": "https://arxiv.org/pdf/2509.24185", "abs": "https://arxiv.org/abs/2509.24185", "authors": ["Jonghun Kim", "Hyunjin Park"], "title": "Simulating Post-Neoadjuvant Chemotherapy Breast Cancer MRI via Diffusion Model with Prompt Tuning", "categories": ["cs.CV"], "comment": "ISBI'25, 5 pages, 4 figures", "summary": "Neoadjuvant chemotherapy (NAC) is a common therapy option before the main surgery for breast cancer. Response to NAC is monitored using follow-up dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI). Accurate prediction of NAC response helps with treatment planning. Here, we adopt maximum intensity projection images from DCE-MRI to generate post-treatment images (i.e., 3 or 12 weeks after NAC) from pre-treatment images leveraging the emerging diffusion model. We introduce prompt tuning to account for the known clinical factors affecting response to NAC. Our model performed better than other generative models in image quality metrics. Our model was better at generating images that reflected changes in tumor size according to pCR compared to other models. Ablation study confirmed the design choices of our method. Our study has the potential to help with precision medicine.", "AI": {"tldr": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\u4ece\u6cbb\u7597\u524d\u7684DCE-MRI\u56fe\u50cf\u751f\u6210\u4e73\u817a\u764c\u65b0\u8f85\u52a9\u5316\u7597\u540e\u7684\u56fe\u50cf\uff0c\u7ed3\u5408\u4e34\u5e8a\u56e0\u7d20\u63d0\u793a\u8c03\u4f18\uff0c\u80fd\u51c6\u786e\u9884\u6d4b\u80bf\u7624\u5927\u5c0f\u53d8\u5316\u3002", "motivation": "\u65b0\u8f85\u52a9\u5316\u7597\u662f\u4e73\u817a\u764c\u5e38\u89c1\u672f\u524d\u7597\u6cd5\uff0c\u51c6\u786e\u9884\u6d4b\u5176\u53cd\u5e94\u6709\u52a9\u4e8e\u6cbb\u7597\u89c4\u5212\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u968f\u8bbfDCE-MRI\u76d1\u6d4b\u53cd\u5e94\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u524d\u9884\u6d4b\u6cbb\u7597\u6548\u679c\u3002", "method": "\u91c7\u7528\u6269\u6563\u6a21\u578b\u4ece\u6cbb\u7597\u524dDCE-MRI\u7684\u6700\u5927\u5f3a\u5ea6\u6295\u5f71\u56fe\u50cf\u751f\u6210\u6cbb\u7597\u540e\u56fe\u50cf\uff0c\u5f15\u5165\u63d0\u793a\u8c03\u4f18\u673a\u5236\u6574\u5408\u4e34\u5e8a\u56e0\u7d20\u3002", "result": "\u6a21\u578b\u5728\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u4e0a\u4f18\u4e8e\u5176\u4ed6\u751f\u6210\u6a21\u578b\uff0c\u80fd\u66f4\u597d\u5730\u751f\u6210\u53cd\u6620pCR\u76f8\u5173\u80bf\u7624\u5927\u5c0f\u53d8\u5316\u7684\u56fe\u50cf\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u8bbe\u8ba1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u52a9\u4e8e\u5b9e\u73b0\u7cbe\u51c6\u533b\u7597\uff0c\u4e3a\u65b0\u8f85\u52a9\u5316\u7597\u54cd\u5e94\u9884\u6d4b\u63d0\u4f9b\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2509.24194", "pdf": "https://arxiv.org/pdf/2509.24194", "abs": "https://arxiv.org/abs/2509.24194", "authors": ["Zach Eidex", "Mojtaba Safari", "Jie Ding", "Richard Qiu", "Justin Roper", "David Yu", "Hui-Kuo Shu", "Zhen Tian", "Hui Mao", "Xiaofeng Yang"], "title": "An Efficient 3D Latent Diffusion Model for T1-contrast Enhanced MRI Generation", "categories": ["cs.CV"], "comment": null, "summary": "Objective: Gadolinium-based contrast agents (GBCAs) are commonly employed with T1w MRI to enhance lesion visualization but are restricted in patients at risk of nephrogenic systemic fibrosis and variations in GBCA administration can introduce imaging inconsistencies. This study develops an efficient 3D deep-learning framework to generate T1-contrast enhanced images (T1C) from pre-contrast multiparametric MRI. Approach: We propose the 3D latent rectified flow (T1C-RFlow) model for generating high-quality T1C images. First, T1w and T2-FLAIR images are input into a pretrained autoencoder to acquire an efficient latent space representation. A rectified flow diffusion model is then trained in this latent space representation. The T1C-RFlow model was trained on a curated dataset comprised of the BraTS 2024 glioma (GLI; 1480 patients), meningioma (MEN; 1141 patients), and metastases (MET; 1475 patients) datasets. Selected patients were split into train (N=2860), validation (N=612), and test (N=614) sets. Results: Both qualitative and quantitative results demonstrate that the T1C-RFlow model outperforms benchmark 3D models (pix2pix, DDPM, Diffusion Transformers (DiT-3D)) trained in the same latent space. T1C-RFlow achieved the following metrics - GLI: NMSE 0.044 +/- 0.047, SSIM 0.935 +/- 0.025; MEN: NMSE 0.046 +/- 0.029, SSIM 0.937 +/- 0.021; MET: NMSE 0.098 +/- 0.088, SSIM 0.905 +/- 0.082. T1C-RFlow had the best tumor reconstruction performance and significantly faster denoising times (6.9 s/volume, 200 steps) than conventional DDPM models in both latent space (37.7s, 1000 steps) and patch-based in image space (4.3 hr/volume). Significance: Our proposed method generates synthetic T1C images that closely resemble ground truth T1C in much less time than previous diffusion models. Further development may permit a practical method for contrast-agent-free MRI for brain tumors.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u9ad8\u6548\u76843D\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6T1C-RFlow\uff0c\u7528\u4e8e\u4ece\u591a\u53c2\u6570MRI\u751f\u6210T1\u5bf9\u6bd4\u589e\u5f3a\u56fe\u50cf\uff0c\u907f\u514d\u4f7f\u7528\u9486\u57fa\u5bf9\u6bd4\u5242\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u751f\u6210\u901f\u5ea6\u3002", "motivation": "\u9486\u57fa\u5bf9\u6bd4\u5242\u5728T1w MRI\u4e2d\u5e38\u7528\u4e8e\u589e\u5f3a\u75c5\u7076\u53ef\u89c6\u5316\uff0c\u4f46\u5bf9\u6709\u80be\u6e90\u6027\u7cfb\u7edf\u6027\u7ea4\u7ef4\u5316\u98ce\u9669\u7684\u60a3\u8005\u53d7\u9650\uff0c\u4e14\u5bf9\u6bd4\u5242\u4f7f\u7528\u5dee\u5f02\u4f1a\u5bfc\u81f4\u6210\u50cf\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51fa3D\u6f5c\u5728\u6574\u6d41\u6d41\u6a21\u578b\uff0c\u9996\u5148\u5c06T1w\u548cT2-FLAIR\u56fe\u50cf\u8f93\u5165\u9884\u8bad\u7ec3\u81ea\u7f16\u7801\u5668\u83b7\u53d6\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\uff0c\u7136\u540e\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8bad\u7ec3\u6574\u6d41\u6d41\u6269\u6563\u6a21\u578b\u3002", "result": "T1C-RFlow\u5728\u5404\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\uff0c\u751f\u6210\u65f6\u95f4\u663e\u8457\u7f29\u77ed\uff086.9\u79d2/\u4f53\u79ef\uff09\uff0c\u5728\u80f6\u8d28\u7624\u3001\u8111\u819c\u7624\u548c\u8f6c\u79fb\u7624\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u5408\u6210T1C\u56fe\u50cf\u4e0e\u771f\u5b9e\u56fe\u50cf\u9ad8\u5ea6\u76f8\u4f3c\uff0c\u4e14\u751f\u6210\u65f6\u95f4\u5927\u5927\u7f29\u77ed\uff0c\u6709\u671b\u5f00\u53d1\u51fa\u5b9e\u7528\u7684\u65e0\u5bf9\u6bd4\u5242\u8111\u80bf\u7624MRI\u65b9\u6cd5\u3002"}}
{"id": "2509.24209", "pdf": "https://arxiv.org/pdf/2509.24209", "abs": "https://arxiv.org/abs/2509.24209", "authors": ["Yingdong Hu", "Yisheng He", "Jinnan Chen", "Weihao Yuan", "Kejie Qiu", "Zehong Lin", "Siyu Zhu", "Zilong Dong", "Jun Zhang"], "title": "Forge4D: Feed-Forward 4D Human Reconstruction and Interpolation from Uncalibrated Sparse-view Videos", "categories": ["cs.CV"], "comment": null, "summary": "Instant reconstruction of dynamic 3D humans from uncalibrated sparse-view videos is critical for numerous downstream applications. Existing methods, however, are either limited by the slow reconstruction speeds or incapable of generating novel-time representations. To address these challenges, we propose Forge4D, a feed-forward 4D human reconstruction and interpolation model that efficiently reconstructs temporally aligned representations from uncalibrated sparse-view videos, enabling both novel view and novel time synthesis. Our model simplifies the 4D reconstruction and interpolation problem as a joint task of streaming 3D Gaussian reconstruction and dense motion prediction. For the task of streaming 3D Gaussian reconstruction, we first reconstruct static 3D Gaussians from uncalibrated sparse-view images and then introduce learnable state tokens to enforce temporal consistency in a memory-friendly manner by interactively updating shared information across different timestamps. For novel time synthesis, we design a novel motion prediction module to predict dense motions for each 3D Gaussian between two adjacent frames, coupled with an occlusion-aware Gaussian fusion process to interpolate 3D Gaussians at arbitrary timestamps. To overcome the lack of the ground truth for dense motion supervision, we formulate dense motion prediction as a dense point matching task and introduce a self-supervised retargeting loss to optimize this module. An additional occlusion-aware optical flow loss is introduced to ensure motion consistency with plausible human movement, providing stronger regularization. Extensive experiments demonstrate the effectiveness of our model on both in-domain and out-of-domain datasets. Project page and code at: https://zhenliuzju.github.io/huyingdong/Forge4D.", "AI": {"tldr": "Forge4D\u662f\u4e00\u4e2a\u524d\u99884D\u4eba\u4f53\u91cd\u5efa\u548c\u63d2\u503c\u6a21\u578b\uff0c\u80fd\u591f\u4ece\u65e0\u6807\u5b9a\u7a00\u758f\u89c6\u89d2\u89c6\u9891\u4e2d\u9ad8\u6548\u91cd\u5efa\u65f6\u95f4\u5bf9\u9f50\u7684\u8868\u793a\uff0c\u652f\u6301\u65b0\u89c6\u89d2\u548c\u65b0\u65f6\u95f4\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u91cd\u5efa\u901f\u5ea6\u6162\uff0c\u8981\u4e48\u65e0\u6cd5\u751f\u6210\u65b0\u65f6\u95f4\u8868\u793a\u3002\u9700\u8981\u89e3\u51b3\u4ece\u65e0\u6807\u5b9a\u7a00\u758f\u89c6\u89d2\u89c6\u9891\u4e2d\u5b9e\u65f6\u91cd\u5efa\u52a8\u60013D\u4eba\u4f53\u7684\u6311\u6218\u3002", "method": "\u5c064D\u91cd\u5efa\u548c\u63d2\u503c\u95ee\u9898\u7b80\u5316\u4e3a\u6d41\u5f0f3D\u9ad8\u65af\u91cd\u5efa\u548c\u7a20\u5bc6\u8fd0\u52a8\u9884\u6d4b\u7684\u8054\u5408\u4efb\u52a1\u3002\u4f7f\u7528\u53ef\u5b66\u4e60\u72b6\u6001\u4ee4\u724c\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u8bbe\u8ba1\u8fd0\u52a8\u9884\u6d4b\u6a21\u5757\u9884\u6d4b\u76f8\u90bb\u5e27\u95f4\u7684\u7a20\u5bc6\u8fd0\u52a8\uff0c\u5e76\u901a\u8fc7\u906e\u6321\u611f\u77e5\u9ad8\u65af\u878d\u5408\u8fc7\u7a0b\u63d2\u503c\u4efb\u610f\u65f6\u95f4\u6233\u76843D\u9ad8\u65af\u3002", "result": "\u5728\u57df\u5185\u548c\u57df\u5916\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "Forge4D\u80fd\u591f\u9ad8\u6548\u91cd\u5efa\u65f6\u95f4\u5bf9\u9f50\u76844D\u4eba\u4f53\u8868\u793a\uff0c\u652f\u6301\u65b0\u89c6\u89d2\u548c\u65b0\u65f6\u95f4\u5408\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.24241", "pdf": "https://arxiv.org/pdf/2509.24241", "abs": "https://arxiv.org/abs/2509.24241", "authors": ["Seungwook Kim", "Seunghyeon Lee", "Minsu Cho"], "title": "FreeAction: Training-Free Techniques for Enhanced Fidelity of Trajectory-to-Video Generation", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages, 4 figures, accepted to CoRL 2025 LSRW workshop", "summary": "Generating realistic robot videos from explicit action trajectories is a critical step toward building effective world models and robotics foundation models. We introduce two training-free, inference-time techniques that fully exploit explicit action parameters in diffusion-based robot video generation. Instead of treating action vectors as passive conditioning signals, our methods actively incorporate them to guide both the classifier-free guidance process and the initialization of Gaussian latents. First, action-scaled classifier-free guidance dynamically modulates guidance strength in proportion to action magnitude, enhancing controllability over motion intensity. Second, action-scaled noise truncation adjusts the distribution of initially sampled noise to better align with the desired motion dynamics. Experiments on real robot manipulation datasets demonstrate that these techniques significantly improve action coherence and visual quality across diverse robot environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u5728\u63a8\u7406\u65f6\u5229\u7528\u663e\u5f0f\u52a8\u4f5c\u53c2\u6570\u7684\u6280\u672f\uff0c\u901a\u8fc7\u52a8\u4f5c\u7f29\u653e\u7684\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u548c\u52a8\u4f5c\u7f29\u653e\u7684\u566a\u58f0\u622a\u65ad\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u673a\u5668\u4eba\u89c6\u9891\u751f\u6210\u7684\u52a8\u4f5c\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u4ece\u663e\u5f0f\u52a8\u4f5c\u8f68\u8ff9\u751f\u6210\u903c\u771f\u7684\u673a\u5668\u4eba\u89c6\u9891\u662f\u6784\u5efa\u6709\u6548\u4e16\u754c\u6a21\u578b\u548c\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u9700\u8981\u5145\u5206\u5229\u7528\u52a8\u4f5c\u53c2\u6570\u6765\u589e\u5f3a\u751f\u6210\u89c6\u9891\u7684\u63a7\u5236\u6027\u548c\u8d28\u91cf\u3002", "method": "1. \u52a8\u4f5c\u7f29\u653e\u7684\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\uff1a\u6839\u636e\u52a8\u4f5c\u5e45\u5ea6\u52a8\u6001\u8c03\u8282\u5f15\u5bfc\u5f3a\u5ea6\uff1b2. \u52a8\u4f5c\u7f29\u653e\u7684\u566a\u58f0\u622a\u65ad\uff1a\u8c03\u6574\u521d\u59cb\u91c7\u6837\u566a\u58f0\u5206\u5e03\u4ee5\u66f4\u597d\u5730\u5339\u914d\u671f\u671b\u8fd0\u52a8\u52a8\u6001\u3002", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9b\u6280\u672f\u663e\u8457\u63d0\u9ad8\u4e86\u4e0d\u540c\u673a\u5668\u4eba\u73af\u5883\u4e2d\u7684\u52a8\u4f5c\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "\u901a\u8fc7\u4e3b\u52a8\u5229\u7528\u663e\u5f0f\u52a8\u4f5c\u53c2\u6570\u6765\u5f15\u5bfc\u6269\u6563\u8fc7\u7a0b\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u89c6\u9891\u751f\u6210\u7684\u63a7\u5236\u6027\u548c\u8d28\u91cf\uff0c\u4e3a\u6784\u5efa\u66f4\u597d\u7684\u4e16\u754c\u6a21\u578b\u548c\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u652f\u6491\u3002"}}
{"id": "2509.24267", "pdf": "https://arxiv.org/pdf/2509.24267", "abs": "https://arxiv.org/abs/2509.24267", "authors": ["Fangrui Huang", "Alan Wang", "Binxu Li", "Bailey Trang", "Ridvan Yesiloglu", "Tianyu Hua", "Wei Peng", "Ehsan Adeli"], "title": "Cycle Diffusion Model for Counterfactual Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deep generative models have demonstrated remarkable success in medical image synthesis. However, ensuring conditioning faithfulness and high-quality synthetic images for direct or counterfactual generation remains a challenge. In this work, we introduce a cycle training framework to fine-tune diffusion models for improved conditioning adherence and enhanced synthetic image realism. Our approach, Cycle Diffusion Model (CDM), enforces consistency between generated and original images by incorporating cycle constraints, enabling more reliable direct and counterfactual generation. Experiments on a combined 3D brain MRI dataset (from ABCD, HCP aging & young adults, ADNI, and PPMI) show that our method improves conditioning accuracy and enhances image quality as measured by FID and SSIM. The results suggest that the cycle strategy used in CDM can be an effective method for refining diffusion-based medical image generation, with applications in data augmentation, counterfactual, and disease progression modeling.", "AI": {"tldr": "\u63d0\u51faCycle Diffusion Model (CDM)\uff0c\u901a\u8fc7\u5faa\u73af\u8bad\u7ec3\u6846\u67b6\u6539\u8fdb\u6269\u6563\u6a21\u578b\uff0c\u63d0\u9ad8\u6761\u4ef6\u5fe0\u5b9e\u5ea6\u548c\u5408\u6210\u56fe\u50cf\u8d28\u91cf\uff0c\u5e94\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5408\u6210\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u786e\u4fdd\u6761\u4ef6\u5fe0\u5b9e\u5ea6\u548c\u9ad8\u8d28\u91cf\u5408\u6210\u56fe\u50cf\uff08\u7279\u522b\u662f\u76f4\u63a5\u6216\u53cd\u4e8b\u5b9e\u751f\u6210\uff09\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u5f15\u5165\u5faa\u73af\u8bad\u7ec3\u6846\u67b6\u6765\u5fae\u8c03\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u52a0\u5165\u5faa\u73af\u7ea6\u675f\u5f3a\u5236\u751f\u6210\u56fe\u50cf\u4e0e\u539f\u59cb\u56fe\u50cf\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\uff0c\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u76f4\u63a5\u548c\u53cd\u4e8b\u5b9e\u751f\u6210\u3002", "result": "\u5728\u7ec4\u5408\u76843D\u8111\u90e8MRI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6761\u4ef6\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7FID\u548cSSIM\u6307\u6807\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "CDM\u4e2d\u4f7f\u7528\u7684\u5faa\u73af\u7b56\u7565\u53ef\u4ee5\u6210\u4e3a\u6539\u8fdb\u57fa\u4e8e\u6269\u6563\u7684\u533b\u5b66\u56fe\u50cf\u751f\u6210\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5728\u6570\u636e\u589e\u5f3a\u3001\u53cd\u4e8b\u5b9e\u548c\u75be\u75c5\u8fdb\u5c55\u5efa\u6a21\u4e2d\u5177\u6709\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.24288", "pdf": "https://arxiv.org/pdf/2509.24288", "abs": "https://arxiv.org/abs/2509.24288", "authors": ["Sai Raj Kishore Perla", "Aditya Vora", "Sauradip Nag", "Ali Mahdavi-Amiri", "Hao Zhang"], "title": "ASIA: Adaptive 3D Segmentation using Few Image Annotations", "categories": ["cs.CV"], "comment": "SIGGRAPH Asia, 2025. Project Page: https://sairajk.github.io/asia/", "summary": "We introduce ASIA (Adaptive 3D Segmentation using few Image Annotations), a novel framework that enables segmentation of possibly non-semantic and non-text-describable \"parts\" in 3D. Our segmentation is controllable through a few user-annotated in-the-wild images, which are easier to collect than multi-view images, less demanding to annotate than 3D models, and more precise than potentially ambiguous text descriptions. Our method leverages the rich priors of text-to-image diffusion models, such as Stable Diffusion (SD), to transfer segmentations from image space to 3D, even when the annotated and target objects differ significantly in geometry or structure. During training, we optimize a text token for each segment and fine-tune our model with a novel cross-view part correspondence loss. At inference, we segment multi-view renderings of the 3D mesh, fuse the labels in UV-space via voting, refine them with our novel Noise Optimization technique, and finally map the UV-labels back onto the mesh. ASIA provides a practical and generalizable solution for both semantic and non-semantic 3D segmentation tasks, outperforming existing methods by a noticeable margin in both quantitative and qualitative evaluations.", "AI": {"tldr": "ASIA\u662f\u4e00\u4e2a\u57fa\u4e8e\u5c11\u91cf\u56fe\u50cf\u6807\u6ce8\u76843D\u5206\u5272\u6846\u67b6\uff0c\u80fd\u591f\u5206\u52723D\u5bf9\u8c61\u4e2d\u7684\u975e\u8bed\u4e49\u548c\u975e\u6587\u672c\u53ef\u63cf\u8ff0\u90e8\u5206\uff0c\u901a\u8fc7\u5229\u7528\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u5148\u9a8c\u77e5\u8bc6\u5b9e\u73b0\u4ece\u56fe\u50cf\u7a7a\u95f4\u52303D\u7684\u5206\u5272\u8fc1\u79fb\u3002", "motivation": "\u73b0\u6709\u76843D\u5206\u5272\u65b9\u6cd5\u9700\u8981\u591a\u89c6\u89d2\u56fe\u50cf\u30013D\u6a21\u578b\u6807\u6ce8\u6216\u6587\u672c\u63cf\u8ff0\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u8981\u4e48\u6536\u96c6\u56f0\u96be\uff0c\u8981\u4e48\u6807\u6ce8\u8981\u6c42\u9ad8\uff0c\u8981\u4e48\u5b58\u5728\u6a21\u7cca\u6027\u3002ASIA\u65e8\u5728\u901a\u8fc7\u5c11\u91cf\u7528\u6237\u6807\u6ce8\u7684\u91ce\u5916\u56fe\u50cf\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4f18\u5316\u6bcf\u4e2a\u5206\u5272\u7247\u6bb5\u7684\u6587\u672ctoken\uff0c\u4f7f\u7528\u65b0\u7684\u8de8\u89c6\u89d2\u90e8\u5206\u5bf9\u5e94\u635f\u5931\u8fdb\u884c\u5fae\u8c03\uff0c\u5728\u63a8\u7406\u65f6\u5bf93D\u7f51\u683c\u7684\u591a\u89c6\u89d2\u6e32\u67d3\u8fdb\u884c\u5206\u5272\uff0c\u901a\u8fc7\u6295\u7968\u5728UV\u7a7a\u95f4\u878d\u5408\u6807\u7b7e\uff0c\u4f7f\u7528\u566a\u58f0\u4f18\u5316\u6280\u672f\u8fdb\u884c\u7cbe\u70bc\uff0c\u6700\u540e\u5c06UV\u6807\u7b7e\u6620\u5c04\u56de\u7f51\u683c\u3002", "result": "ASIA\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u90fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u8bed\u4e49\u548c\u975e\u8bed\u4e493D\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6cdb\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "ASIA\u6846\u67b6\u901a\u8fc7\u5c11\u91cf\u56fe\u50cf\u6807\u6ce8\u5b9e\u73b0\u4e86\u9ad8\u6548\u76843D\u5206\u5272\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u51e0\u4f55\u6216\u7ed3\u6784\u5dee\u5f02\u8f83\u5927\u7684\u5bf9\u8c61\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u4e3a3D\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.24308", "pdf": "https://arxiv.org/pdf/2509.24308", "abs": "https://arxiv.org/abs/2509.24308", "authors": ["Yuhang Cao", "Haojun Yan", "Danya Yao"], "title": "OMeGa: Joint Optimization of Explicit Meshes and Gaussian Splats for Robust Scene-Level Surface Reconstruction", "categories": ["cs.CV"], "comment": "12 pages, 9 figures", "summary": "Neural rendering with Gaussian splatting has advanced novel view synthesis, and most methods reconstruct surfaces via post-hoc mesh extraction. However, existing methods suffer from two limitations: (i) inaccurate geometry in texture-less indoor regions, and (ii) the decoupling of mesh extraction from optimization, thereby missing the opportunity to leverage mesh geometry to guide splat optimization. In this paper, we present OMeGa, an end-to-end framework that jointly optimizes an explicit triangle mesh and 2D Gaussian splats via a flexible binding strategy, where spatial attributes of Gaussian Splats are expressed in the mesh frame and texture attributes are retained on splats. To further improve reconstruction accuracy, we integrate mesh constraints and monocular normal supervision into the optimization, thereby regularizing geometry learning. In addition, we propose a heuristic, iterative mesh-refinement strategy that splits high-error faces and prunes unreliable ones to further improve the detail and accuracy of the reconstructed mesh. OMeGa achieves state-of-the-art performance on challenging indoor reconstruction benchmarks, reducing Chamfer-$L_1$ by 47.3\\% over the 2DGS baseline while maintaining competitive novel-view rendering quality. The experimental results demonstrate that OMeGa effectively addresses prior limitations in indoor texture-less reconstruction.", "AI": {"tldr": "OMeGa\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316\u663e\u5f0f\u4e09\u89d2\u7f51\u683c\u548c2D\u9ad8\u65af\u6e85\u5c04\uff0c\u901a\u8fc7\u7f51\u683c\u7ea6\u675f\u548c\u6cd5\u7ebf\u76d1\u7763\u63d0\u5347\u5ba4\u5185\u7eb9\u7406\u7f3a\u5931\u533a\u57df\u7684\u51e0\u4f55\u91cd\u5efa\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\u5728\u7eb9\u7406\u7f3a\u5931\u7684\u5ba4\u5185\u533a\u57df\u5b58\u5728\u51e0\u4f55\u4e0d\u51c6\u786e\u95ee\u9898\uff0c\u4e14\u7f51\u683c\u63d0\u53d6\u4e0e\u4f18\u5316\u8fc7\u7a0b\u5206\u79bb\uff0c\u65e0\u6cd5\u5229\u7528\u7f51\u683c\u51e0\u4f55\u6307\u5bfc\u6e85\u5c04\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u7075\u6d3b\u7684\u7ed1\u5b9a\u7b56\u7565\uff0c\u5c06\u9ad8\u65af\u6e85\u5c04\u7684\u7a7a\u95f4\u5c5e\u6027\u8868\u8fbe\u5728\u7f51\u683c\u6846\u67b6\u4e2d\uff0c\u4fdd\u7559\u7eb9\u7406\u5c5e\u6027\uff1b\u96c6\u6210\u7f51\u683c\u7ea6\u675f\u548c\u5355\u76ee\u6cd5\u7ebf\u76d1\u7763\uff1b\u91c7\u7528\u542f\u53d1\u5f0f\u8fed\u4ee3\u7f51\u683c\u7ec6\u5316\u7b56\u7565\u3002", "result": "\u5728\u6311\u6218\u6027\u5ba4\u5185\u91cd\u5efa\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5c06Chamfer-L1\u8bef\u5dee\u964d\u4f4e47.3%\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u65b0\u89c6\u89d2\u6e32\u67d3\u8d28\u91cf\u3002", "conclusion": "OMeGa\u6709\u6548\u89e3\u51b3\u4e86\u5148\u524d\u5ba4\u5185\u7eb9\u7406\u7f3a\u5931\u91cd\u5efa\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u51e0\u4f55\u91cd\u5efa\u3002"}}
{"id": "2509.24335", "pdf": "https://arxiv.org/pdf/2509.24335", "abs": "https://arxiv.org/abs/2509.24335", "authors": ["Guolin Ke", "Hui Xue"], "title": "Hyperspherical Latents Improve Continuous-Token Autoregressive Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Autoregressive (AR) models are promising for image generation, yet continuous-token AR variants often trail latent diffusion and masked-generation models. The core issue is heterogeneous variance in VAE latents, which is amplified during AR decoding, especially under classifier-free guidance (CFG), and can cause variance collapse. We propose SphereAR to address this issue. Its core design is to constrain all AR inputs and outputs -- including after CFG -- to lie on a fixed-radius hypersphere (constant $\\ell_2$ norm), leveraging hyperspherical VAEs. Our theoretical analysis shows that hyperspherical constraint removes the scale component (the primary cause of variance collapse), thereby stabilizing AR decoding. Empirically, on ImageNet generation, SphereAR-H (943M) sets a new state of the art for AR models, achieving FID 1.34. Even at smaller scales, SphereAR-L (479M) reaches FID 1.54 and SphereAR-B (208M) reaches 1.92, matching or surpassing much larger baselines such as MAR-H (943M, 1.55) and VAR-d30 (2B, 1.92). To our knowledge, this is the first time a pure next-token AR image generator with raster order surpasses diffusion and masked-generation models at comparable parameter scales.", "AI": {"tldr": "SphereAR\u901a\u8fc7\u5c06\u81ea\u56de\u5f52\u6a21\u578b\u7684\u8f93\u5165\u8f93\u51fa\u7ea6\u675f\u5728\u56fa\u5b9a\u534a\u5f84\u7684\u8d85\u7403\u9762\u4e0a\uff0c\u89e3\u51b3\u4e86VAE\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5f02\u8d28\u65b9\u5dee\u5bfc\u81f4\u7684\u65b9\u5dee\u5d29\u6e83\u95ee\u9898\uff0c\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u6269\u6563\u548c\u63a9\u7801\u751f\u6210\u6a21\u578b\u3002", "motivation": "\u8fde\u7eedtoken\u81ea\u56de\u5f52\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u901a\u5e38\u843d\u540e\u4e8e\u6f5c\u5728\u6269\u6563\u548c\u63a9\u7801\u751f\u6210\u6a21\u578b\uff0c\u4e3b\u8981\u95ee\u9898\u662fVAE\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u5f02\u8d28\u65b9\u5dee\u5728\u81ea\u56de\u5f52\u89e3\u7801\u8fc7\u7a0b\u4e2d\u88ab\u653e\u5927\uff0c\u7279\u522b\u662f\u5728\u5206\u7c7b\u5668\u65e0\u5173\u5f15\u5bfc\u4e0b\u4f1a\u5bfc\u81f4\u65b9\u5dee\u5d29\u6e83\u3002", "method": "\u63d0\u51faSphereAR\u65b9\u6cd5\uff0c\u6838\u5fc3\u8bbe\u8ba1\u662f\u5c06\u6240\u6709\u81ea\u56de\u5f52\u8f93\u5165\u8f93\u51fa\uff08\u5305\u62ecCFG\u540e\uff09\u7ea6\u675f\u5728\u56fa\u5b9a\u534a\u5f84\u7684\u8d85\u7403\u9762\u4e0a\uff08\u6052\u5b9a\u21132\u8303\u6570\uff09\uff0c\u5229\u7528\u8d85\u7403\u9762VAE\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\u8d85\u7403\u9762\u7ea6\u675f\u79fb\u9664\u4e86\u5c3a\u5ea6\u5206\u91cf\uff08\u65b9\u5dee\u5d29\u6e83\u7684\u4e3b\u8981\u539f\u56e0\uff09\uff0c\u4ece\u800c\u7a33\u5b9a\u81ea\u56de\u5f52\u89e3\u7801\u3002", "result": "\u5728ImageNet\u751f\u6210\u4efb\u52a1\u4e0a\uff0cSphereAR-H\uff08943M\uff09\u521b\u4e0b\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u7684\u65b0\u8bb0\u5f55\uff0cFID\u8fbe\u52301.34\u3002\u66f4\u5c0f\u89c4\u6a21\u7684SphereAR-L\uff08479M\uff09\u8fbe\u5230FID 1.54\uff0cSphereAR-B\uff08208M\uff09\u8fbe\u52301.92\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u66f4\u5927\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u7eafnext-token\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u5668\uff08\u5149\u6805\u987a\u5e8f\uff09\u5728\u53ef\u6bd4\u53c2\u6570\u89c4\u6a21\u4e0b\u8d85\u8d8a\u6269\u6563\u548c\u63a9\u7801\u751f\u6210\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u8d85\u7403\u9762\u7ea6\u675f\u5728\u7a33\u5b9a\u81ea\u56de\u5f52\u89e3\u7801\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.24353", "pdf": "https://arxiv.org/pdf/2509.24353", "abs": "https://arxiv.org/abs/2509.24353", "authors": ["Yixuan Ren", "Hanyu Wang", "Hao Chen", "Bo He", "Abhinav Shrivastava"], "title": "NeRV-Diffusion: Diffuse Implicit Neural Representations for Video Synthesis", "categories": ["cs.CV"], "comment": "Project Page: https://nerv-diffusion.github.io/", "summary": "We present NeRV-Diffusion, an implicit latent video diffusion model that synthesizes videos via generating neural network weights. The generated weights can be rearranged as the parameters of a convolutional neural network, which forms an implicit neural representation (INR), and decodes into videos with frame indices as the input. Our framework consists of two stages: 1) A hypernetworkbased tokenizer that encodes raw videos from pixel space to neural parameter space, where the bottleneck latent serves as INR weights to decode. 2) An implicit diffusion transformer that denoises on the latent INR weights. In contrast to traditional video tokenizers that encode videos into frame-wise feature maps, NeRV-Diffusion compresses and generates a video holistically as a unified neural network. This enables efficient and high-quality video synthesis via obviating temporal cross-frame attentions in the denoiser and decoding video latent with dedicated decoders. To achieve Gaussian-distributed INR weights with high expressiveness, we reuse the bottleneck latent across all NeRV layers, as well as reform its weight assignment, upsampling connection and input coordinates. We also introduce SNR-adaptive loss weighting and scheduled sampling for effective training of the implicit diffusion model. NeRV-Diffusion reaches superior video generation quality over previous INR-based models and comparable performance to most recent state-of-the-art non-implicit models on real-world video benchmarks including UCF-101 and Kinetics-600. It also brings a smooth INR weight space that facilitates seamless interpolations between frames or videos.", "AI": {"tldr": "NeRV-Diffusion\u662f\u4e00\u79cd\u9690\u5f0f\u6f5c\u5728\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u751f\u6210\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\u6765\u5408\u6210\u89c6\u9891\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a\u57fa\u4e8e\u8d85\u7f51\u7edc\u7684\u6807\u8bb0\u5668\u5c06\u89c6\u9891\u7f16\u7801\u4e3a\u795e\u7ecf\u53c2\u6570\u7a7a\u95f4\uff0c\u4ee5\u53ca\u9690\u5f0f\u6269\u6563\u53d8\u6362\u5668\u5728\u6f5c\u5728INR\u6743\u91cd\u4e0a\u8fdb\u884c\u53bb\u566a\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u6807\u8bb0\u5668\u5c06\u89c6\u9891\u7f16\u7801\u4e3a\u9010\u5e27\u7279\u5f81\u56fe\uff0c\u800cNeRV-Diffusion\u5c06\u89c6\u9891\u6574\u4f53\u538b\u7f29\u5e76\u751f\u6210\u4e3a\u4e00\u4e2a\u7edf\u4e00\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u907f\u514d\u4e86\u53bb\u566a\u5668\u4e2d\u7684\u65f6\u95f4\u8de8\u5e27\u6ce8\u610f\u529b\uff0c\u5b9e\u73b0\u9ad8\u6548\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u5408\u6210\u3002", "method": "1) \u57fa\u4e8e\u8d85\u7f51\u7edc\u7684\u6807\u8bb0\u5668\u5c06\u539f\u59cb\u89c6\u9891\u4ece\u50cf\u7d20\u7a7a\u95f4\u7f16\u7801\u5230\u795e\u7ecf\u53c2\u6570\u7a7a\u95f4\uff1b2) \u9690\u5f0f\u6269\u6563\u53d8\u6362\u5668\u5728\u6f5c\u5728INR\u6743\u91cd\u4e0a\u8fdb\u884c\u53bb\u566a\uff1b3) \u91cd\u7528\u74f6\u9888\u6f5c\u5728\u8de8\u6240\u6709NeRV\u5c42\uff0c\u91cd\u65b0\u8bbe\u8ba1\u6743\u91cd\u5206\u914d\u3001\u4e0a\u91c7\u6837\u8fde\u63a5\u548c\u8f93\u5165\u5750\u6807\uff1b4) \u5f15\u5165SNR\u81ea\u9002\u5e94\u635f\u5931\u52a0\u6743\u548c\u8ba1\u5212\u91c7\u6837\u3002", "result": "\u5728UCF-101\u548cKinetics-600\u7b49\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNeRV-Diffusion\u5728\u89c6\u9891\u751f\u6210\u8d28\u91cf\u4e0a\u4f18\u4e8e\u4e4b\u524d\u7684INR\u6a21\u578b\uff0c\u4e0e\u6700\u65b0\u975e\u9690\u5f0f\u6a21\u578b\u6027\u80fd\u76f8\u5f53\uff0c\u5e76\u63d0\u4f9b\u4e86\u5e73\u6ed1\u7684INR\u6743\u91cd\u7a7a\u95f4\uff0c\u4fbf\u4e8e\u5e27\u95f4\u6216\u89c6\u9891\u95f4\u7684\u65e0\u7f1d\u63d2\u503c\u3002", "conclusion": "NeRV-Diffusion\u901a\u8fc7\u5c06\u89c6\u9891\u8868\u793a\u4e3a\u7edf\u4e00\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u5408\u6210\uff0c\u5728\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u548c\u6269\u6563\u6a21\u578b\u4e4b\u95f4\u5efa\u7acb\u4e86\u6709\u6548\u8fde\u63a5\uff0c\u4e3a\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\u3002"}}
{"id": "2509.24365", "pdf": "https://arxiv.org/pdf/2509.24365", "abs": "https://arxiv.org/abs/2509.24365", "authors": ["Jitai Hao", "Hao Liu", "Xinyan Xiao", "Qiang Huang", "Jun Yu"], "title": "Uni-X: Mitigating Modality Conflict with a Two-End-Separated Architecture for Unified Multimodal Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Unified Multimodal Models (UMMs) built on shared autoregressive (AR) transformers are attractive for their architectural simplicity. However, we identify a critical limitation: when trained on multimodal inputs, modality-shared transformers suffer from severe gradient conflicts between vision and text, particularly in shallow and deep layers. We trace this issue to the fundamentally different low-level statistical properties of images and text, while noting that conflicts diminish in middle layers where representations become more abstract and semantically aligned. To overcome this challenge, we propose Uni-X, a two-end-separated, middle-shared architecture. Uni-X dedicates its initial and final layers to modality-specific processing, while maintaining shared parameters in the middle layers for high-level semantic fusion. This X-shaped design not only eliminates gradient conflicts at both ends but also further alleviates residual conflicts in the shared layers. Extensive experiments validate the effectiveness of Uni-X. Under identical training conditions, Uni-X achieves superior training efficiency compared to strong baselines. When scaled to 3B parameters with larger training data, Uni-X matches or surpasses 7B AR-based UMMs, achieving a GenEval score of 82 for image generation alongside strong performance in text and vision understanding tasks. These results establish Uni-X as a parameter-efficient and scalable foundation for future unified multimodal modeling. Our code is available at https://github.com/CURRENTF/Uni-X", "AI": {"tldr": "Uni-X\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u7aef\u5206\u79bb\u3001\u4e2d\u95f4\u5171\u4eab\u7684X\u5f62\u67b6\u6784\uff0c\u901a\u8fc7\u5c06\u521d\u59cb\u548c\u6700\u7ec8\u5c42\u7528\u4e8e\u6a21\u6001\u7279\u5b9a\u5904\u7406\uff0c\u4e2d\u95f4\u5c42\u5171\u4eab\u53c2\u6570\u8fdb\u884c\u9ad8\u7ea7\u8bed\u4e49\u878d\u5408\uff0c\u89e3\u51b3\u4e86\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u89c6\u89c9\u548c\u6587\u672c\u4e4b\u95f4\u7684\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\u3002", "motivation": "\u57fa\u4e8e\u5171\u4eab\u81ea\u56de\u5f52\u53d8\u6362\u5668\u7684\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u5b58\u5728\u4e25\u91cd\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6d45\u5c42\u548c\u6df1\u5c42\uff0c\u8fd9\u662f\u7531\u4e8e\u56fe\u50cf\u548c\u6587\u672c\u5728\u4f4e\u5c42\u7edf\u8ba1\u7279\u6027\u4e0a\u7684\u6839\u672c\u5dee\u5f02\u9020\u6210\u7684\u3002", "method": "Uni-X\u91c7\u7528\u4e24\u7aef\u5206\u79bb\u3001\u4e2d\u95f4\u5171\u4eab\u7684X\u5f62\u67b6\u6784\uff1a\u521d\u59cb\u548c\u6700\u7ec8\u5c42\u4e13\u95e8\u7528\u4e8e\u6a21\u6001\u7279\u5b9a\u5904\u7406\uff0c\u4e2d\u95f4\u5c42\u4fdd\u6301\u5171\u4eab\u53c2\u6570\u8fdb\u884c\u9ad8\u7ea7\u8bed\u4e49\u878d\u5408\u3002", "result": "\u5728\u76f8\u540c\u8bad\u7ec3\u6761\u4ef6\u4e0b\uff0cUni-X\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u8bad\u7ec3\u6548\u7387\u3002\u5f53\u6269\u5c55\u523030\u4ebf\u53c2\u6570\u5e76\u4f7f\u7528\u66f4\u5927\u8bad\u7ec3\u6570\u636e\u65f6\uff0cUni-X\u5339\u914d\u6216\u8d85\u8d8a\u4e8670\u4ebf\u53c2\u6570\u7684AR\u57faUMMs\uff0c\u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u83b7\u5f9782\u7684GenEval\u5206\u6570\uff0c\u540c\u65f6\u5728\u6587\u672c\u548c\u89c6\u89c9\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u5f3a\u52b2\u3002", "conclusion": "Uni-X\u4e3a\u672a\u6765\u7edf\u4e00\u591a\u6a21\u6001\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53c2\u6570\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u7840\u67b6\u6784\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\u3002"}}
{"id": "2509.24369", "pdf": "https://arxiv.org/pdf/2509.24369", "abs": "https://arxiv.org/abs/2509.24369", "authors": ["Khawlah Bajbaa", "Abbas Anwar", "Muhammad Saqib", "Hafeez Anwar", "Nabin Sharma", "Muhammad Usman"], "title": "From Satellite to Street: A Hybrid Framework Integrating Stable Diffusion and PanoGAN for Consistent Cross-View Synthesis", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "Street view imagery has become an essential source for geospatial data collection and urban analytics, enabling the extraction of valuable insights that support informed decision-making. However, synthesizing street-view images from corresponding satellite imagery presents significant challenges due to substantial differences in appearance and viewing perspective between these two domains. This paper presents a hybrid framework that integrates diffusion-based models and conditional generative adversarial networks to generate geographically consistent street-view images from satellite imagery. Our approach uses a multi-stage training strategy that incorporates Stable Diffusion as the core component within a dual-branch architecture. To enhance the framework's capabilities, we integrate a conditional Generative Adversarial Network (GAN) that enables the generation of geographically consistent panoramic street views. Furthermore, we implement a fusion strategy that leverages the strengths of both models to create robust representations, thereby improving the geometric consistency and visual quality of the generated street-view images. The proposed framework is evaluated on the challenging Cross-View USA (CVUSA) dataset, a standard benchmark for cross-view image synthesis. Experimental results demonstrate that our hybrid approach outperforms diffusion-only methods across multiple evaluation metrics and achieves competitive performance compared to state-of-the-art GAN-based methods. The framework successfully generates realistic and geometrically consistent street-view images while preserving fine-grained local details, including street markings, secondary roads, and atmospheric elements such as clouds.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u6761\u4ef6GAN\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u536b\u661f\u56fe\u50cf\u751f\u6210\u5730\u7406\u4e00\u81f4\u6027\u7684\u8857\u666f\u56fe\u50cf\uff0c\u5728CVUSA\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u7eaf\u6269\u6563\u65b9\u6cd5\uff0c\u4e0e\u6700\u5148\u8fdb\u7684GAN\u65b9\u6cd5\u7ade\u4e89\u3002", "motivation": "\u8857\u666f\u56fe\u50cf\u662f\u91cd\u8981\u7684\u5730\u7406\u7a7a\u95f4\u6570\u636e\u6e90\uff0c\u4f46\u4ece\u536b\u661f\u56fe\u50cf\u5408\u6210\u8857\u666f\u9762\u4e34\u5916\u89c2\u548c\u89c6\u89d2\u5dee\u5f02\u7684\u6311\u6218\uff0c\u9700\u8981\u751f\u6210\u5730\u7406\u4e00\u81f4\u4e14\u89c6\u89c9\u8d28\u91cf\u9ad8\u7684\u8857\u666f\u56fe\u50cf\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u4ee5Stable Diffusion\u4e3a\u6838\u5fc3\u6784\u5efa\u53cc\u5206\u652f\u67b6\u6784\uff0c\u96c6\u6210\u6761\u4ef6GAN\u751f\u6210\u5168\u666f\u8857\u666f\uff0c\u5e76\u901a\u8fc7\u878d\u5408\u7b56\u7565\u7ed3\u5408\u4e24\u79cd\u6a21\u578b\u7684\u4f18\u52bf\u3002", "result": "\u5728CVUSA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6df7\u5408\u65b9\u6cd5\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u4f18\u4e8e\u7eaf\u6269\u6563\u65b9\u6cd5\uff0c\u4e0e\u6700\u5148\u8fdb\u7684GAN\u65b9\u6cd5\u8868\u73b0\u76f8\u5f53\uff0c\u80fd\u751f\u6210\u771f\u5b9e\u4e14\u51e0\u4f55\u4e00\u81f4\u7684\u8857\u666f\u56fe\u50cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u8de8\u89c6\u89d2\u56fe\u50cf\u5408\u6210\u7684\u6311\u6218\uff0c\u751f\u6210\u4e86\u5177\u6709\u826f\u597d\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u7684\u8857\u666f\u56fe\u50cf\uff0c\u4fdd\u7559\u4e86\u8857\u9053\u6807\u8bb0\u3001\u6b21\u8981\u9053\u8def\u548c\u5927\u6c14\u5143\u7d20\u7b49\u7ec6\u8282\u3002"}}
{"id": "2509.24416", "pdf": "https://arxiv.org/pdf/2509.24416", "abs": "https://arxiv.org/abs/2509.24416", "authors": ["Kai Liu", "Shaoqiu Zhang", "Linghe Kong", "Yulun Zhang"], "title": "CLQ: Cross-Layer Guided Orthogonal-based Quantization for Diffusion Transformers", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 5 figures. Code is released at   https://github.com/Kai-Liu001/CLQ", "summary": "Visual generation quality has been greatly promoted with the rapid advances in diffusion transformers (DiTs), which is attributed to the scaling of model size and complexity. However, these attributions also hinder the practical deployment of DiTs on edge devices, limiting their development and application. Serve as an efficient model compression technique, model post-training quantization (PTQ) can reduce the memory consumption and speed up the inference, with inevitable performance degradation. To alleviate the degradation, we propose CLQ, a cross-layer guided orthogonal-based quantization method for DiTs. To be specific, CLQ consists of three key designs. First, we observe that the calibration data used by most of the PTQ methods can not honestly represent the distribution of the activations. Therefore, we propose cross-block calibration (CBC) to obtain accurate calibration data, with which the quantization can be better guided. Second, we propose orthogonal-based smoothing (OBS), which quantifies the outlier score of each channel and leverages block Hadamard matrix to smooth the outliers with negligible overhead. Third, we propose cross-layer parameter searching (CLPS) to search. We evaluate CLQ with both image generation and video generation models and successfully compress the model into W4A4 with negligible degradation in visual quality and metrics. CLQ achieves 3.98x memory saving and 3.95x speedup. Our code is available at \\hyperlink{https://github.com/Kai-Liu001/CLQ}{https://github.com/Kai-Liu001/CLQ}.", "AI": {"tldr": "CLQ\u662f\u4e00\u79cd\u9488\u5bf9\u6269\u6563\u53d8\u6362\u5668(DiTs)\u7684\u8de8\u5c42\u5f15\u5bfc\u6b63\u4ea4\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u5757\u6821\u51c6\u3001\u6b63\u4ea4\u5e73\u6ed1\u548c\u8de8\u5c42\u53c2\u6570\u641c\u7d22\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff0c\u5b9e\u73b0W4A4\u91cf\u5316\uff0c\u5728\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u7684\u540c\u65f6\u83b7\u5f973.98\u500d\u5185\u5b58\u8282\u7701\u548c3.95\u500d\u52a0\u901f\u3002", "motivation": "\u6269\u6563\u53d8\u6362\u5668(DiTs)\u867d\u7136\u63d0\u5347\u4e86\u89c6\u89c9\u751f\u6210\u8d28\u91cf\uff0c\u4f46\u6a21\u578b\u89c4\u6a21\u548c\u590d\u6742\u5ea6\u7684\u589e\u957f\u963b\u788d\u4e86\u5176\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u9645\u90e8\u7f72\u3002\u6a21\u578b\u540e\u8bad\u7ec3\u91cf\u5316(PTQ)\u4f5c\u4e3a\u9ad8\u6548\u538b\u7f29\u6280\u672f\u53ef\u4ee5\u51cf\u5c11\u5185\u5b58\u6d88\u8017\u548c\u52a0\u901f\u63a8\u7406\uff0c\u4f46\u4f1a\u5e26\u6765\u6027\u80fd\u4e0b\u964d\u3002", "method": "1. \u8de8\u5757\u6821\u51c6(CBC)\uff1a\u83b7\u53d6\u51c6\u786e\u7684\u6821\u51c6\u6570\u636e\u4ee5\u6307\u5bfc\u91cf\u5316\uff1b2. \u6b63\u4ea4\u5e73\u6ed1(OBS)\uff1a\u91cf\u5316\u6bcf\u4e2a\u901a\u9053\u7684\u5f02\u5e38\u503c\u5206\u6570\uff0c\u5229\u7528\u5757Hadamard\u77e9\u9635\u5e73\u6ed1\u5f02\u5e38\u503c\uff1b3. \u8de8\u5c42\u53c2\u6570\u641c\u7d22(CLPS)\uff1a\u641c\u7d22\u6700\u4f18\u53c2\u6570\u914d\u7f6e\u3002", "result": "\u6210\u529f\u5c06\u6a21\u578b\u538b\u7f29\u4e3aW4A4\u683c\u5f0f\uff0c\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u6307\u6807\u4e0a\u4ec5\u6709\u53ef\u5ffd\u7565\u7684\u4e0b\u964d\u3002\u5b9e\u73b0\u4e863.98\u500d\u5185\u5b58\u8282\u7701\u548c3.95\u500d\u52a0\u901f\u3002", "conclusion": "CLQ\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86DiTs\u91cf\u5316\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e0a\u90fd\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c\u3002"}}
{"id": "2509.24421", "pdf": "https://arxiv.org/pdf/2509.24421", "abs": "https://arxiv.org/abs/2509.24421", "authors": ["Yuanyuan Gao", "Yuning Gong", "Yifei Liu", "Li Jingfeng", "Zhihang Zhong", "Dingwen Zhang", "Yanci Zhang", "Dan Xu", "Xiao Sun"], "title": "Proxy-GS: Efficient 3D Gaussian Splatting via Proxy Mesh", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has emerged as an efficient approach for achieving photorealistic rendering. Recent MLP-based variants further improve visual fidelity but introduce substantial decoding overhead during rendering. To alleviate computation cost, several pruning strategies and level-of-detail (LOD) techniques have been introduced, aiming to effectively reduce the number of Gaussian primitives in large-scale scenes. However, our analysis reveals that significant redundancy still remains due to the lack of occlusion awareness. In this work, we propose Proxy-GS, a novel pipeline that exploits a proxy to introduce Gaussian occlusion awareness from any view. At the core of our approach is a fast proxy system capable of producing precise occlusion depth maps at a resolution of 1000x1000 under 1ms. This proxy serves two roles: first, it guides the culling of anchors and Gaussians to accelerate rendering speed. Second, it guides the densification towards surfaces during training, avoiding inconsistencies in occluded regions, and improving the rendering quality. In heavily occluded scenarios, such as the MatrixCity Streets dataset, Proxy-GS not only equips MLP-based Gaussian splatting with stronger rendering capability but also achieves faster rendering speed. Specifically, it achieves more than 2.5x speedup over Octree-GS, and consistently delivers substantially higher rendering quality. Code will be public upon acceptance.", "AI": {"tldr": "Proxy-GS\u662f\u4e00\u79cd\u65b0\u9896\u76843D\u9ad8\u65af\u6e85\u5c04\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u4ee3\u7406\u7cfb\u7edf\u5b9e\u73b0\u906e\u6321\u611f\u77e5\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u6e32\u67d3\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6e32\u67d3\u901f\u5ea6\u3002", "motivation": "\u73b0\u67093D\u9ad8\u65af\u6e85\u5c04\u65b9\u6cd5\u5728\u5927\u578b\u573a\u666f\u4e2d\u5b58\u5728\u663e\u8457\u5197\u4f59\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u906e\u6321\u611f\u77e5\u80fd\u529b\uff0c\u5bfc\u81f4\u6e32\u67d3\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u4f7f\u7528\u5feb\u901f\u4ee3\u7406\u7cfb\u7edf\u751f\u6210\u7cbe\u786e\u906e\u6321\u6df1\u5ea6\u56fe\uff0c\u6307\u5bfc\u951a\u70b9\u548c\u9ad8\u65af\u51fd\u6570\u7684\u5254\u9664\uff0c\u5e76\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f15\u5bfc\u5bc6\u5ea6\u5316\u8fc7\u7a0b\u3002", "result": "\u5728MatrixCity Streets\u7b49\u906e\u6321\u4e25\u91cd\u573a\u666f\u4e2d\uff0cProxy-GS\u4e0d\u4ec5\u63d0\u5347\u4e86MLP\u57fa\u9ad8\u65af\u6e85\u5c04\u7684\u6e32\u67d3\u80fd\u529b\uff0c\u8fd8\u5b9e\u73b0\u4e86\u8d85\u8fc72.5\u500d\u7684\u6e32\u67d3\u52a0\u901f\u3002", "conclusion": "Proxy-GS\u901a\u8fc7\u906e\u6321\u611f\u77e5\u6709\u6548\u89e3\u51b3\u4e863D\u9ad8\u65af\u6e85\u5c04\u4e2d\u7684\u5197\u4f59\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6e32\u67d3\u6548\u7387\u3002"}}
{"id": "2509.24427", "pdf": "https://arxiv.org/pdf/2509.24427", "abs": "https://arxiv.org/abs/2509.24427", "authors": ["Ailing Zhang", "Lina Lei", "Dehong Kong", "Zhixin Wang", "Jiaqi Xu", "Fenglong Song", "Chun-Le Guo", "Chang Liu", "Fan Li", "Jie Chen"], "title": "UI2V-Bench: An Understanding-based Image-to-video Generation Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "Generative diffusion models are developing rapidly and attracting increasing attention due to their wide range of applications. Image-to-Video (I2V) generation has become a major focus in the field of video synthesis. However, existing evaluation benchmarks primarily focus on aspects such as video quality and temporal consistency, while largely overlooking the model's ability to understand the semantics of specific subjects in the input image or to ensure that the generated video aligns with physical laws and human commonsense. To address this gap, we propose UI2V-Bench, a novel benchmark for evaluating I2V models with a focus on semantic understanding and reasoning. It introduces four primary evaluation dimensions: spatial understanding, attribute binding, category understanding, and reasoning. To assess these dimensions, we design two evaluation methods based on Multimodal Large Language Models (MLLMs): an instance-level pipeline for fine-grained semantic understanding, and a feedback-based reasoning pipeline that enables step-by-step causal assessment for more accurate evaluation. UI2V-Bench includes approximately 500 carefully constructed text-image pairs and evaluates a range of both open source and closed-source I2V models across all defined dimensions. We further incorporate human evaluations, which show strong alignment with the proposed MLLM-based metrics. Overall, UI2V-Bench fills a critical gap in I2V evaluation by emphasizing semantic comprehension and reasoning ability, offering a robust framework and dataset to support future research and model development in the field.", "AI": {"tldr": "\u63d0\u51fa\u4e86UI2V-Bench\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u8bc4\u4f30\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u8bed\u4e49\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u586b\u8865\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u89c6\u9891\u8d28\u91cf\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u8bc4\u4f30\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u89c6\u9891\u8d28\u91cf\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\uff0c\u800c\u5ffd\u89c6\u4e86\u6a21\u578b\u5bf9\u8f93\u5165\u56fe\u50cf\u7279\u5b9a\u4e3b\u4f53\u8bed\u4e49\u7684\u7406\u89e3\u80fd\u529b\uff0c\u4ee5\u53ca\u751f\u6210\u89c6\u9891\u662f\u5426\u7b26\u5408\u7269\u7406\u89c4\u5f8b\u548c\u4eba\u7c7b\u5e38\u8bc6\u3002", "method": "\u8bbe\u8ba1\u4e86\u56db\u4e2a\u4e3b\u8981\u8bc4\u4f30\u7ef4\u5ea6\uff1a\u7a7a\u95f4\u7406\u89e3\u3001\u5c5e\u6027\u7ed1\u5b9a\u3001\u7c7b\u522b\u7406\u89e3\u548c\u63a8\u7406\uff1b\u5f00\u53d1\u4e86\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e24\u79cd\u8bc4\u4f30\u65b9\u6cd5\uff1a\u5b9e\u4f8b\u7ea7\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7406\u89e3\u6d41\u6c34\u7ebf\u548c\u57fa\u4e8e\u53cd\u9988\u7684\u63a8\u7406\u6d41\u6c34\u7ebf\uff1b\u5305\u542b\u7ea6500\u4e2a\u7cbe\u5fc3\u6784\u5efa\u7684\u6587\u672c-\u56fe\u50cf\u5bf9\u3002", "result": "\u8bc4\u4f30\u4e86\u591a\u4e2a\u5f00\u6e90\u548c\u95ed\u6e90\u7684\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u53d1\u73b0\u4eba\u7c7b\u8bc4\u4f30\u4e0e\u63d0\u51fa\u7684\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6307\u6807\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "UI2V-Bench\u901a\u8fc7\u5f3a\u8c03\u8bed\u4e49\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u586b\u8865\u4e86\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u8bc4\u4f30\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4e3a\u8be5\u9886\u57df\u672a\u6765\u7814\u7a76\u548c\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u6846\u67b6\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2509.24526", "pdf": "https://arxiv.org/pdf/2509.24526", "abs": "https://arxiv.org/abs/2509.24526", "authors": ["Zheyuan Hu", "Chieh-Hsin Lai", "Yuki Mitsufuji", "Stefano Ermon"], "title": "CMT: Mid-Training for Efficient Learning of Consistency, Mean Flow, and Flow Map Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Preprint", "summary": "Flow map models such as Consistency Models (CM) and Mean Flow (MF) enable few-step generation by learning the long jump of the ODE solution of diffusion models, yet training remains unstable, sensitive to hyperparameters, and costly. Initializing from a pre-trained diffusion model helps, but still requires converting infinitesimal steps into a long-jump map, leaving instability unresolved. We introduce mid-training, the first concept and practical method that inserts a lightweight intermediate stage between the (diffusion) pre-training and the final flow map training (i.e., post-training) for vision generation. Concretely, Consistency Mid-Training (CMT) is a compact and principled stage that trains a model to map points along a solver trajectory from a pre-trained model, starting from a prior sample, directly to the solver-generated clean sample. It yields a trajectory-consistent and stable initialization. This initializer outperforms random and diffusion-based baselines and enables fast, robust convergence without heuristics. Initializing post-training with CMT weights further simplifies flow map learning. Empirically, CMT achieves state of the art two step FIDs: 1.97 on CIFAR-10, 1.32 on ImageNet 64x64, and 1.84 on ImageNet 512x512, while using up to 98% less training data and GPU time, compared to CMs. On ImageNet 256x256, CMT reaches 1-step FID 3.34 while cutting total training time by about 50% compared to MF from scratch (FID 3.43). This establishes CMT as a principled, efficient, and general framework for training flow map models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Consistency Mid-Training (CMT)\u65b9\u6cd5\uff0c\u5728\u6269\u6563\u9884\u8bad\u7ec3\u548c\u6d41\u6620\u5c04\u8bad\u7ec3\u4e4b\u95f4\u63d2\u5165\u8f7b\u91cf\u7ea7\u4e2d\u95f4\u9636\u6bb5\uff0c\u663e\u8457\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u6d41\u6620\u5c04\u6a21\u578b\u5982\u4e00\u81f4\u6027\u6a21\u578b\u548c\u5747\u503c\u6d41\u6a21\u578b\u867d\u7136\u80fd\u5b9e\u73b0\u5c11\u6b65\u751f\u6210\uff0c\u4f46\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3001\u5bf9\u8d85\u53c2\u6570\u654f\u611f\u4e14\u6210\u672c\u9ad8\u6602\uff0c\u5373\u4f7f\u4ece\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u521d\u59cb\u5316\u4e5f\u65e0\u6cd5\u5b8c\u5168\u89e3\u51b3\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e2d\u95f4\u8bad\u7ec3\u6982\u5ff5\uff0c\u5728\u6269\u6563\u9884\u8bad\u7ec3\u548c\u6700\u7ec8\u6d41\u6620\u5c04\u8bad\u7ec3\u4e4b\u95f4\u63d2\u5165\u8f7b\u91cf\u7ea7\u9636\u6bb5CMT\uff0c\u8bad\u7ec3\u6a21\u578b\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u6c42\u89e3\u5668\u8f68\u8ff9\u4e0a\u7684\u70b9\u76f4\u63a5\u6620\u5c04\u5230\u6c42\u89e3\u5668\u751f\u6210\u7684\u5e72\u51c0\u6837\u672c\u3002", "result": "CMT\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u5148\u8fdb\u76842\u6b65FID\uff1aCIFAR-10\u4e3a1.97\uff0cImageNet 64x64\u4e3a1.32\uff0cImageNet 512x512\u4e3a1.84\uff0c\u76f8\u6bd4\u4e00\u81f4\u6027\u6a21\u578b\u51cf\u5c1198%\u8bad\u7ec3\u6570\u636e\u548cGPU\u65f6\u95f4\u3002", "conclusion": "CMT\u4e3a\u8bad\u7ec3\u6d41\u6620\u5c04\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u3001\u9ad8\u6548\u4e14\u901a\u7528\u7684\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2509.24531", "pdf": "https://arxiv.org/pdf/2509.24531", "abs": "https://arxiv.org/abs/2509.24531", "authors": ["Kaizhen Zhu", "Mokai Pan", "Zhechuan Yu", "Jingya Wang", "Jingyi Yu", "Ye Shi"], "title": "Diffusion Bridge or Flow Matching? A Unifying Framework and Comparative Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion Bridge and Flow Matching have both demonstrated compelling empirical performance in transformation between arbitrary distributions. However, there remains confusion about which approach is generally preferable, and the substantial discrepancies in their modeling assumptions and practical implementations have hindered a unified theoretical account of their relative merits. We have, for the first time, provided a unified theoretical and experimental validation of these two models. We recast their frameworks through the lens of Stochastic Optimal Control and prove that the cost function of the Diffusion Bridge is lower, guiding the system toward more stable and natural trajectories. Simultaneously, from the perspective of Optimal Transport, interpolation coefficients $t$ and $1-t$ of Flow Matching become increasingly ineffective when the training data size is reduced. To corroborate these theoretical claims, we propose a novel, powerful architecture for Diffusion Bridge built on a latent Transformer, and implement a Flow Matching model with the same structure to enable a fair performance comparison in various experiments. Comprehensive experiments are conducted across Image Inpainting, Super-Resolution, Deblurring, Denoising, Translation, and Style Transfer tasks, systematically varying both the distributional discrepancy (different difficulty) and the training data size. Extensive empirical results align perfectly with our theoretical predictions and allow us to delineate the respective advantages and disadvantages of these two models. Our code is available at https://anonymous.4open.science/r/DBFM-3E8E/.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9Diffusion Bridge\u548cFlow Matching\u4e24\u79cd\u6a21\u578b\u8fdb\u884c\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u901a\u8fc7\u968f\u673a\u6700\u4f18\u63a7\u5236\u548c\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u5206\u6790\u4e86\u5b83\u4eec\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u8bbe\u8ba1\u4e86\u516c\u5e73\u7684\u5b9e\u9a8c\u6bd4\u8f83\u3002", "motivation": "\u76ee\u524d\u5bf9\u4e8eDiffusion Bridge\u548cFlow Matching\u54ea\u79cd\u65b9\u6cd5\u66f4\u4f18\u5b58\u5728\u56f0\u60d1\uff0c\u4e24\u79cd\u65b9\u6cd5\u5728\u5efa\u6a21\u5047\u8bbe\u548c\u5b9e\u73b0\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u5206\u6790\u6846\u67b6\u6765\u6bd4\u8f83\u5b83\u4eec\u7684\u76f8\u5bf9\u4f18\u52bf\u3002", "method": "\u901a\u8fc7\u968f\u673a\u6700\u4f18\u63a7\u5236\u7406\u8bba\u91cd\u65b0\u6784\u5efa\u4e24\u79cd\u6a21\u578b\u6846\u67b6\uff0c\u8bc1\u660eDiffusion Bridge\u7684\u6210\u672c\u51fd\u6570\u66f4\u4f4e\uff1b\u4ece\u6700\u4f18\u4f20\u8f93\u89d2\u5ea6\u5206\u6790Flow Matching\u63d2\u503c\u7cfb\u6570\u7684\u5c40\u9650\u6027\uff1b\u8bbe\u8ba1\u57fa\u4e8e\u6f5c\u5728Transformer\u7684\u65b0\u67b6\u6784\u8fdb\u884c\u516c\u5e73\u6bd4\u8f83\u5b9e\u9a8c\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eDiffusion Bridge\u80fd\u5f15\u5bfc\u7cfb\u7edf\u8d70\u5411\u66f4\u7a33\u5b9a\u81ea\u7136\u7684\u8f68\u8ff9\uff0c\u800cFlow Matching\u5728\u8bad\u7ec3\u6570\u636e\u51cf\u5c11\u65f6\u63d2\u503c\u7cfb\u6570\u6548\u679c\u4e0b\u964d\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u9884\u6d4b\uff0c\u5728\u56fe\u50cf\u4fee\u590d\u3001\u8d85\u5206\u8fa8\u7387\u3001\u53bb\u6a21\u7cca\u3001\u53bb\u566a\u3001\u8f6c\u6362\u548c\u98ce\u683c\u8fc1\u79fb\u7b49\u4efb\u52a1\u4e2d\u7cfb\u7edf\u6bd4\u8f83\u4e86\u4e24\u79cd\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u660e\u786e\u4e86\u4e24\u79cd\u6a21\u578b\u5404\u81ea\u7684\u4f18\u7f3a\u70b9\uff0c\u4e3a\u9009\u62e9\u5408\u9002\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u548c\u5b9e\u9a8c\u4f9d\u636e\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.24652", "pdf": "https://arxiv.org/pdf/2509.24652", "abs": "https://arxiv.org/abs/2509.24652", "authors": ["Adil Kaan Akan"], "title": "Learning Object-Centric Representations Based on Slots in Real World Scenarios", "categories": ["cs.CV"], "comment": "PhD Thesis, overlap with arXiv:2507.20855 and arXiv:2501.15878", "summary": "A central goal in AI is to represent scenes as compositions of discrete objects, enabling fine-grained, controllable image and video generation. Yet leading diffusion models treat images holistically and rely on text conditioning, creating a mismatch for object-level editing. This thesis introduces a framework that adapts powerful pretrained diffusion models for object-centric synthesis while retaining their generative capacity.   We identify a core challenge: balancing global scene coherence with disentangled object control. Our method integrates lightweight, slot-based conditioning into pretrained models, preserving their visual priors while providing object-specific manipulation. For images, SlotAdapt augments diffusion models with a register token for background/style and slot-conditioned modules for objects, reducing text-conditioning bias and achieving state-of-the-art results in object discovery, segmentation, compositional editing, and controllable image generation.   We further extend the framework to video. Using Invariant Slot Attention (ISA) to separate object identity from pose and a Transformer-based temporal aggregator, our approach maintains consistent object representations and dynamics across frames. This yields new benchmarks in unsupervised video object segmentation and reconstruction, and supports advanced editing tasks such as object removal, replacement, and insertion without explicit supervision.   Overall, this work establishes a general and scalable approach to object-centric generative modeling for images and videos. By bridging human object-based perception and machine learning, it expands the design space for interactive, structured, and user-driven generative tools in creative, scientific, and practical domains.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86SlotAdapt\u6846\u67b6\uff0c\u5c06\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u9002\u914d\u4e3a\u9762\u5411\u5bf9\u8c61\u7684\u5408\u6210\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u751f\u6210\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u5bf9\u8c61\u63a7\u5236\uff0c\u9002\u7528\u4e8e\u56fe\u50cf\u548c\u89c6\u9891\u7684\u751f\u6210\u4e0e\u7f16\u8f91\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6269\u6563\u6a21\u578b\u4ee5\u6574\u4f53\u65b9\u5f0f\u5904\u7406\u56fe\u50cf\u3001\u4f9d\u8d56\u6587\u672c\u6761\u4ef6\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u7b26\u5408\u4eba\u7c7b\u5bf9\u8c61\u611f\u77e5\u7684\u7ec6\u7c92\u5ea6\u53ef\u63a7\u751f\u6210\u4e0e\u7f16\u8f91\u3002", "method": "\u96c6\u6210\u8f7b\u91cf\u7ea7\u7684\u57fa\u4e8e\u69fd\u7684\u6761\u4ef6\u673a\u5236\u5230\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\uff0c\u4f7f\u7528\u5bc4\u5b58\u5668\u4ee4\u724c\u5904\u7406\u80cc\u666f/\u98ce\u683c\uff0c\u69fd\u6761\u4ef6\u6a21\u5757\u5904\u7406\u5bf9\u8c61\uff1b\u89c6\u9891\u4e2d\u91c7\u7528\u4e0d\u53d8\u69fd\u6ce8\u610f\u529b\u548c\u57fa\u4e8eTransformer\u7684\u65f6\u95f4\u805a\u5408\u5668\u3002", "result": "\u5728\u5bf9\u8c61\u53d1\u73b0\u3001\u5206\u5272\u3001\u7ec4\u5408\u7f16\u8f91\u548c\u53ef\u63a7\u56fe\u50cf\u751f\u6210\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff1b\u5728\u65e0\u76d1\u7763\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u548c\u91cd\u5efa\u65b9\u9762\u5efa\u7acb\u65b0\u57fa\u51c6\uff0c\u652f\u6301\u5bf9\u8c61\u79fb\u9664\u3001\u66ff\u6362\u548c\u63d2\u5165\u7b49\u9ad8\u7ea7\u7f16\u8f91\u4efb\u52a1\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u53ef\u6269\u5c55\u7684\u9762\u5411\u5bf9\u8c61\u751f\u6210\u5efa\u6a21\u65b9\u6cd5\uff0c\u5f25\u5408\u4e86\u4eba\u7c7b\u5bf9\u8c61\u611f\u77e5\u4e0e\u673a\u5668\u5b66\u4e60\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u521b\u610f\u3001\u79d1\u5b66\u548c\u5b9e\u9645\u9886\u57df\u7684\u4ea4\u4e92\u5f0f\u7ed3\u6784\u5316\u751f\u6210\u5de5\u5177\u6269\u5c55\u4e86\u8bbe\u8ba1\u7a7a\u95f4\u3002"}}
{"id": "2509.24695", "pdf": "https://arxiv.org/pdf/2509.24695", "abs": "https://arxiv.org/abs/2509.24695", "authors": ["Junsong Chen", "Yuyang Zhao", "Jincheng Yu", "Ruihang Chu", "Junyu Chen", "Shuai Yang", "Xianbang Wang", "Yicheng Pan", "Daquan Zhou", "Huan Ling", "Haozhe Liu", "Hongwei Yi", "Hao Zhang", "Muyang Li", "Yukang Chen", "Han Cai", "Sanja Fidler", "Ping Luo", "Song Han", "Enze Xie"], "title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer", "categories": ["cs.CV", "cs.AI"], "comment": "21 pages, 15 figures, 7 tables", "summary": "We introduce SANA-Video, a small diffusion model that can efficiently generate videos up to 720x1280 resolution and minute-length duration. SANA-Video synthesizes high-resolution, high-quality and long videos with strong text-video alignment at a remarkably fast speed, deployable on RTX 5090 GPU. Two core designs ensure our efficient, effective and long video generation: (1) Linear DiT: We leverage linear attention as the core operation, which is more efficient than vanilla attention given the large number of tokens processed in video generation. (2) Constant-Memory KV cache for Block Linear Attention: we design block-wise autoregressive approach for long video generation by employing a constant-memory state, derived from the cumulative properties of linear attention. This KV cache provides the Linear DiT with global context at a fixed memory cost, eliminating the need for a traditional KV cache and enabling efficient, minute-long video generation. In addition, we explore effective data filters and model training strategies, narrowing the training cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of MovieGen. Given its low cost, SANA-Video achieves competitive performance compared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B and SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover, SANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating the inference speed of generating a 5-second 720p video from 71s to 29s (2.4x speedup). In summary, SANA-Video enables low-cost, high-quality video generation.", "AI": {"tldr": "SANA-Video\u662f\u4e00\u4e2a\u5c0f\u578b\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u9ad8\u6548\u751f\u6210720x1280\u5206\u8fa8\u7387\u3001\u5206\u949f\u957f\u5ea6\u7684\u89c6\u9891\uff0c\u5177\u6709\u5feb\u901f\u751f\u6210\u901f\u5ea6\uff0c\u53ef\u5728RTX 5090 GPU\u4e0a\u90e8\u7f72\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u751f\u6210\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4f4e\u6210\u672c\u3001\u9ad8\u8d28\u91cf\u7684\u957f\u89c6\u9891\u751f\u6210\u3002", "method": "\u91c7\u7528\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\uff08Linear DiT\uff09\u548c\u6052\u5b9a\u5185\u5b58KV\u7f13\u5b58\u6280\u672f\uff0c\u7ed3\u5408\u6709\u6548\u7684\u6570\u636e\u8fc7\u6ee4\u548c\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u76f8\u6bd4\u73b0\u4ee3\u6700\u5148\u8fdb\u7684\u5c0f\u578b\u6269\u6563\u6a21\u578b\uff0c\u6027\u80fd\u76f8\u5f53\u4f46\u901f\u5ea6\u5feb16\u500d\uff0c\u8bad\u7ec3\u6210\u672c\u4ec5\u4e3aMovieGen\u76841%\uff0c\u5728RTX 5090\u4e0a\u751f\u62105\u79d2720p\u89c6\u9891\u4ece71\u79d2\u52a0\u901f\u523029\u79d2\u3002", "conclusion": "SANA-Video\u5b9e\u73b0\u4e86\u4f4e\u6210\u672c\u3001\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u751f\u6210\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24702", "pdf": "https://arxiv.org/pdf/2509.24702", "abs": "https://arxiv.org/abs/2509.24702", "authors": ["Yutong Hao", "Chen Chen", "Ajmal Saeed Mian", "Chang Xu", "Daochang Liu"], "title": "Enhancing Physical Plausibility in Video Generation by Reasoning the Implausibility", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models can generate realistic videos, but existing methods rely on implicitly learning physical reasoning from large-scale text-video datasets, which is costly, difficult to scale, and still prone to producing implausible motions that violate fundamental physical laws. We introduce a training-free framework that improves physical plausibility at inference time by explicitly reasoning about implausibility and guiding the generation away from it. Specifically, we employ a lightweight physics-aware reasoning pipeline to construct counterfactual prompts that deliberately encode physics-violating behaviors. Then, we propose a novel Synchronized Decoupled Guidance (SDG) strategy, which leverages these prompts through synchronized directional normalization to counteract lagged suppression and trajectory-decoupled denoising to mitigate cumulative trajectory bias, ensuring that implausible content is suppressed immediately and consistently throughout denoising. Experiments across different physical domains show that our approach substantially enhances physical fidelity while maintaining photorealism, despite requiring no additional training. Ablation studies confirm the complementary effectiveness of both the physics-aware reasoning component and SDG. In particular, the aforementioned two designs of SDG are also individually validated to contribute critically to the suppression of implausible content and the overall gains in physical plausibility. This establishes a new and plug-and-play physics-aware paradigm for video generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7269\u7406\u611f\u77e5\u63a8\u7406\u548c\u540c\u6b65\u89e3\u8026\u5f15\u5bfc\u7b56\u7565\uff0c\u5728\u63a8\u7406\u65f6\u663e\u5f0f\u6291\u5236\u8fdd\u53cd\u7269\u7406\u89c4\u5f8b\u7684\u5185\u5bb9\uff0c\u63d0\u9ad8\u751f\u6210\u89c6\u9891\u7684\u7269\u7406\u5408\u7406\u6027\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u901a\u8fc7\u5927\u89c4\u6a21\u6587\u672c-\u89c6\u9891\u6570\u636e\u96c6\u9690\u5f0f\u5b66\u4e60\u7269\u7406\u63a8\u7406\uff0c\u6210\u672c\u9ad8\u3001\u96be\u4ee5\u6269\u5c55\uff0c\u4e14\u5bb9\u6613\u4ea7\u751f\u8fdd\u53cd\u57fa\u672c\u7269\u7406\u5b9a\u5f8b\u7684\u4e0d\u5408\u7406\u8fd0\u52a8\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7269\u7406\u611f\u77e5\u63a8\u7406\u7ba1\u9053\u6784\u5efa\u53cd\u4e8b\u5b9e\u63d0\u793a\uff0c\u7f16\u7801\u7269\u7406\u8fdd\u89c4\u884c\u4e3a\uff1b\u63d0\u51fa\u540c\u6b65\u89e3\u8026\u5f15\u5bfc\u7b56\u7565\uff0c\u5305\u62ec\u540c\u6b65\u65b9\u5411\u5f52\u4e00\u5316\u5bf9\u6297\u6ede\u540e\u6291\u5236\u548c\u8f68\u8ff9\u89e3\u8026\u53bb\u566a\u7f13\u89e3\u7d2f\u79ef\u8f68\u8ff9\u504f\u5dee\u3002", "result": "\u5728\u4e0d\u540c\u7269\u7406\u9886\u57df\u7684\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u7269\u7406\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u7167\u7247\u7ea7\u771f\u5b9e\u611f\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u7269\u7406\u611f\u77e5\u63a8\u7406\u7ec4\u4ef6\u548cSDG\u7684\u4e92\u8865\u6709\u6548\u6027\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u79cd\u65b0\u7684\u5373\u63d2\u5373\u7528\u7269\u7406\u611f\u77e5\u89c6\u9891\u751f\u6210\u8303\u5f0f\uff0c\u80fd\u591f\u5728\u63a8\u7406\u65f6\u6709\u6548\u63d0\u5347\u751f\u6210\u89c6\u9891\u7684\u7269\u7406\u5408\u7406\u6027\u3002"}}
{"id": "2509.24758", "pdf": "https://arxiv.org/pdf/2509.24758", "abs": "https://arxiv.org/abs/2509.24758", "authors": ["Jiaqi Chen", "Xinhao Ji", "Yuanyuan Gao", "Hao Li", "Yuning Gong", "Yifei Liu", "Dan Xu", "Zhihang Zhong", "Dingwen Zhang", "Xiao Sun"], "title": "ExGS: Extreme 3D Gaussian Compression with Diffusion Priors", "categories": ["cs.CV"], "comment": null, "summary": "Neural scene representations, such as 3D Gaussian Splatting (3DGS), have enabled high-quality neural rendering; however, their large storage and transmission costs hinder deployment in resource-constrained environments. Existing compression methods either rely on costly optimization, which is slow and scene-specific, or adopt training-free pruning and quantization, which degrade rendering quality under high compression ratios. In contrast, recent data-driven approaches provide a promising direction to overcome this trade-off, enabling efficient compression while preserving high rendering quality. We introduce \\textbf{ExGS}, a novel feed-forward framework that unifies \\textbf{Universal Gaussian Compression} (UGC) with \\textbf{GaussPainter} for \\textbf{Ex}treme 3D\\textbf{GS} compression. \\textbf{UGC} performs re-optimization-free pruning to aggressively reduce Gaussian primitives while retaining only essential information, whereas \\textbf{GaussPainter} leverages powerful diffusion priors with mask-guided refinement to restore high-quality renderings from heavily pruned Gaussian scenes. Unlike conventional inpainting, GaussPainter not only fills in missing regions but also enhances visible pixels, yielding substantial improvements in degraded renderings. To ensure practicality, it adopts a lightweight VAE and a one-step diffusion design, enabling real-time restoration. Our framework can even achieve over $100\\times$ compression (reducing a typical 354.77 MB model to about 3.31 MB) while preserving fidelity and significantly improving image quality under challenging conditions. These results highlight the central role of diffusion priors in bridging the gap between extreme compression and high-quality neural rendering. Our code repository will be released at \\href{https://github.com/chenttt2001/ExGS}{here}.", "AI": {"tldr": "ExGS\u662f\u4e00\u4e2a\u7edf\u4e00\u901a\u7528\u9ad8\u65af\u538b\u7f29(UGC)\u548cGaussPainter\u7684\u524d\u9988\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u6781\u7aef3DGS\u538b\u7f29\uff0c\u53ef\u5728\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u8d85\u8fc7100\u500d\u7684\u538b\u7f29\u6bd4\u3002", "motivation": "\u795e\u7ecf\u573a\u666f\u8868\u793a(\u59823DGS)\u867d\u7136\u80fd\u5b9e\u73b0\u9ad8\u8d28\u91cf\u795e\u7ecf\u6e32\u67d3\uff0c\u4f46\u5176\u5de8\u5927\u7684\u5b58\u50a8\u548c\u4f20\u8f93\u6210\u672c\u963b\u788d\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u3002\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6602\u8d35\u7684\u4f18\u5316\uff0c\u8981\u4e48\u5728\u9ad8\u8d28\u91cf\u538b\u7f29\u4e0b\u4f1a\u964d\u4f4e\u6e32\u67d3\u8d28\u91cf\u3002", "method": "\u7ed3\u5408UGC\u8fdb\u884c\u65e0\u91cd\u4f18\u5316\u526a\u679d\u6765\u5927\u5e45\u51cf\u5c11\u9ad8\u65af\u57fa\u5143\uff0c\u540c\u65f6\u4f7f\u7528GaussPainter\u5229\u7528\u6269\u6563\u5148\u9a8c\u548c\u63a9\u7801\u5f15\u5bfc\u7ec6\u5316\u6765\u4ece\u4e25\u91cd\u526a\u679d\u7684\u9ad8\u65af\u573a\u666f\u4e2d\u6062\u590d\u9ad8\u8d28\u91cf\u6e32\u67d3\u3002GaussPainter\u4e0d\u4ec5\u586b\u5145\u7f3a\u5931\u533a\u57df\uff0c\u8fd8\u589e\u5f3a\u53ef\u89c1\u50cf\u7d20\u3002", "result": "\u8be5\u6846\u67b6\u53ef\u5b9e\u73b0\u8d85\u8fc7100\u500d\u7684\u538b\u7f29(\u5c06\u5178\u578b\u7684354.77MB\u6a21\u578b\u538b\u7f29\u81f3\u7ea63.31MB)\uff0c\u540c\u65f6\u4fdd\u6301\u4fdd\u771f\u5ea6\uff0c\u5e76\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "\u6269\u6563\u5148\u9a8c\u5728\u5f25\u5408\u6781\u7aef\u538b\u7f29\u548c\u9ad8\u8d28\u91cf\u795e\u7ecf\u6e32\u67d3\u4e4b\u95f4\u7684\u5dee\u8ddd\u4e2d\u53d1\u6325\u7740\u6838\u5fc3\u4f5c\u7528\u3002"}}
{"id": "2509.24791", "pdf": "https://arxiv.org/pdf/2509.24791", "abs": "https://arxiv.org/abs/2509.24791", "authors": ["Cheng Shi", "Yizhou Yu", "Sibei Yang"], "title": "Vision Function Layer in Multimodal LLMs", "categories": ["cs.CV"], "comment": "Accepted at NeurIPS 2025 (preview; camera-ready in preparation)", "summary": "This study identifies that visual-related functional decoding is distributed across different decoder layers in Multimodal Large Language Models (MLLMs). Typically, each function, such as counting, grounding, or OCR recognition, narrows down to two or three layers, which we define as Vision Function Layers (VFL). Additionally, the depth and its order of different VFLs exhibits a consistent pattern across different MLLMs, which is well-aligned with human behaviors (e.g., recognition occurs first, followed by counting, and then grounding). These findings are derived from Visual Token Swapping, our novel analytical framework that modifies targeted KV cache entries to precisely elucidate layer-specific functions during decoding. Furthermore, these insights offer substantial utility in tailoring MLLMs for real-world downstream applications. For instance, when LoRA training is selectively applied to VFLs whose functions align with the training data, VFL-LoRA not only outperform full-LoRA but also prevent out-of-domain function forgetting. Moreover, by analyzing the performance differential on training data when particular VFLs are ablated, VFL-select automatically classifies data by function, enabling highly efficient data selection to directly bolster corresponding capabilities. Consequently, VFL-select surpasses human experts in data selection, and achieves 98% of full-data performance with only 20% of the original dataset. This study delivers deeper comprehension of MLLM visual processing, fostering the creation of more efficient, interpretable, and robust models.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4e2d\u7684\u89c6\u89c9\u76f8\u5173\u529f\u80fd\u89e3\u7801\u5206\u5e03\u5728\u4e0d\u540c\u7684\u89e3\u7801\u5668\u5c42\uff0c\u6bcf\u4e2a\u529f\u80fd\uff08\u5982\u8ba1\u6570\u3001\u5b9a\u4f4d\u3001OCR\u8bc6\u522b\uff09\u96c6\u4e2d\u57282-3\u4e2a\u7279\u5b9a\u5c42\uff08\u89c6\u89c9\u529f\u80fd\u5c42VFL\uff09\uff0c\u4e14\u4e0d\u540cVFL\u7684\u6df1\u5ea6\u987a\u5e8f\u5728\u4e0d\u540cMLLMs\u4e2d\u5448\u73b0\u4e00\u81f4\u6a21\u5f0f\u3002", "motivation": "\u7406\u89e3MLLMs\u4e2d\u89c6\u89c9\u5904\u7406\u7684\u5177\u4f53\u673a\u5236\uff0c\u63a2\u7d22\u89c6\u89c9\u529f\u80fd\u5728\u4e0d\u540c\u89e3\u7801\u5c42\u7684\u5206\u5e03\u89c4\u5f8b\uff0c\u4e3a\u6a21\u578b\u4f18\u5316\u548c\u5e94\u7528\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u89c6\u89c9\u4ee4\u724c\u4ea4\u6362\uff08Visual Token Swapping\uff09\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u4fee\u6539\u7279\u5b9aKV\u7f13\u5b58\u6761\u76ee\u6765\u7cbe\u786e\u63ed\u793a\u89e3\u7801\u8fc7\u7a0b\u4e2d\u7684\u5c42\u7279\u5b9a\u529f\u80fd\u3002", "result": "\u8bc6\u522b\u51fa\u89c6\u89c9\u529f\u80fd\u5c42\uff08VFL\uff09\uff0c\u53d1\u73b0\u5176\u6df1\u5ea6\u987a\u5e8f\u4e0e\u4eba\u7c7b\u884c\u4e3a\u4e00\u81f4\uff08\u8bc6\u522b\u2192\u8ba1\u6570\u2192\u5b9a\u4f4d\uff09\u3002\u57fa\u4e8eVFL\u7684LoRA\u8bad\u7ec3\u4f18\u4e8e\u5168\u53c2\u6570\u8bad\u7ec3\uff0c\u4e14VFL-select\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u4ec5\u752820%\u6570\u636e\u5373\u53ef\u8fbe\u523098%\u5168\u6570\u636e\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u6df1\u5316\u4e86\u5bf9MLLM\u89c6\u89c9\u5904\u7406\u673a\u5236\u7684\u7406\u89e3\uff0c\u4e3a\u5f00\u53d1\u66f4\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u548c\u9c81\u68d2\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.24798", "pdf": "https://arxiv.org/pdf/2509.24798", "abs": "https://arxiv.org/abs/2509.24798", "authors": ["Lei Tong", "Zhihua Liu", "Chaochao Lu", "Dino Oglic", "Tom Diethe", "Philip Teare", "Sotirios A. Tsaftaris", "Chen Jin"], "title": "Causal-Adapter: Taming Text-to-Image Diffusion for Faithful Counterfactual Generation", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 26 figures", "summary": "We present Causal-Adapter, a modular framework that adapts frozen text-to-image diffusion backbones for counterfactual image generation. Our method enables causal interventions on target attributes, consistently propagating their effects to causal dependents without altering the core identity of the image. In contrast to prior approaches that rely on prompt engineering without explicit causal structure, Causal-Adapter leverages structural causal modeling augmented with two attribute regularization strategies: prompt-aligned injection, which aligns causal attributes with textual embeddings for precise semantic control, and a conditioned token contrastive loss to disentangle attribute factors and reduce spurious correlations. Causal-Adapter achieves state-of-the-art performance on both synthetic and real-world datasets, with up to 91\\% MAE reduction on Pendulum for accurate attribute control and 87\\% FID reduction on ADNI for high-fidelity MRI image generation. These results show that our approach enables robust, generalizable counterfactual editing with faithful attribute modification and strong identity preservation.", "AI": {"tldr": "Causal-Adapter\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u9002\u914d\u51bb\u7ed3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u53cd\u4e8b\u5b9e\u56fe\u50cf\u751f\u6210\uff0c\u5229\u7528\u7ed3\u6784\u56e0\u679c\u5efa\u6a21\u548c\u5c5e\u6027\u6b63\u5219\u5316\u7b56\u7565\u5b9e\u73b0\u7cbe\u786e\u7684\u5c5e\u6027\u5e72\u9884\u548c\u8eab\u4efd\u4fdd\u6301\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u63d0\u793a\u5de5\u7a0b\u800c\u7f3a\u4e4f\u660e\u786e\u7684\u56e0\u679c\u7ed3\u6784\uff0c\u65e0\u6cd5\u4e00\u81f4\u5730\u5c06\u76ee\u6807\u5c5e\u6027\u7684\u56e0\u679c\u5e72\u9884\u6548\u679c\u4f20\u64ad\u5230\u56e0\u679c\u4f9d\u8d56\u9879\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u6838\u5fc3\u8eab\u4efd\u4e0d\u53d8\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u56e0\u679c\u5efa\u6a21\uff0c\u7ed3\u5408\u4e24\u79cd\u5c5e\u6027\u6b63\u5219\u5316\u7b56\u7565\uff1a\u63d0\u793a\u5bf9\u9f50\u6ce8\u5165\uff08\u5bf9\u9f50\u56e0\u679c\u5c5e\u6027\u4e0e\u6587\u672c\u5d4c\u5165\uff09\u548c\u6761\u4ef6\u6807\u8bb0\u5bf9\u6bd4\u635f\u5931\uff08\u89e3\u8026\u5c5e\u6027\u56e0\u5b50\u5e76\u51cf\u5c11\u865a\u5047\u76f8\u5173\u6027\uff09\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0cPendulum\u6570\u636e\u96c6MAE\u51cf\u5c1191%\uff0cADNI\u6570\u636e\u96c6FID\u51cf\u5c1187%\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771fMRI\u56fe\u50cf\u751f\u6210\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u9c81\u68d2\u3001\u53ef\u6cdb\u5316\u7684\u53cd\u4e8b\u5b9e\u7f16\u8f91\uff0c\u5177\u6709\u5fe0\u5b9e\u7684\u5c5e\u6027\u4fee\u6539\u548c\u5f3a\u5927\u7684\u8eab\u4efd\u4fdd\u6301\u80fd\u529b\u3002"}}
{"id": "2509.24875", "pdf": "https://arxiv.org/pdf/2509.24875", "abs": "https://arxiv.org/abs/2509.24875", "authors": ["Nikos Kostagiolas", "Pantelis Georgiades", "Yannis Panagakis", "Mihalis A. Nicolaou"], "title": "Environment-Aware Satellite Image Generation with Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Diffusion-based foundation models have recently garnered much attention in the field of generative modeling due to their ability to generate images of high quality and fidelity. Although not straightforward, their recent application to the field of remote sensing signaled the first successful trials towards harnessing the large volume of publicly available datasets containing multimodal information. Despite their success, existing methods face considerable limitations: they rely on limited environmental context, struggle with missing or corrupted data, and often fail to reliably reflect user intentions in generated outputs. In this work, we propose a novel diffusion model conditioned on environmental context, that is able to generate satellite images by conditioning from any combination of three different control signals: a) text, b) metadata, and c) visual data. In contrast to previous works, the proposed method is i) to our knowledge, the first of its kind to condition satellite image generation on dynamic environmental conditions as part of its control signals, and ii) incorporating a metadata fusion strategy that models attribute embedding interactions to account for partially corrupt and/or missing observations. Our method outperforms previous methods both qualitatively (robustness to missing metadata, higher responsiveness to control inputs) and quantitatively (higher fidelity, accuracy, and quality of generations measured using 6 different metrics) in the trials of single-image and temporal generation. The reported results support our hypothesis that conditioning on environmental context can improve the performance of foundation models for satellite imagery, and render our model a promising candidate for usage in downstream tasks. The collected 3-modal dataset is to our knowledge, the first publicly-available dataset to combine data from these three different mediums.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u73af\u5883\u4e0a\u4e0b\u6587\u6761\u4ef6\u7684\u65b0\u578b\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u901a\u8fc7\u6587\u672c\u3001\u5143\u6570\u636e\u548c\u89c6\u89c9\u6570\u636e\u4e09\u79cd\u63a7\u5236\u4fe1\u53f7\u7684\u4efb\u610f\u7ec4\u5408\u751f\u6210\u536b\u661f\u56fe\u50cf\u3002\u8be5\u65b9\u6cd5\u9996\u6b21\u5728\u536b\u661f\u56fe\u50cf\u751f\u6210\u4e2d\u5f15\u5165\u52a8\u6001\u73af\u5883\u6761\u4ef6\u4f5c\u4e3a\u63a7\u5236\u4fe1\u53f7\uff0c\u5e76\u91c7\u7528\u5143\u6570\u636e\u878d\u5408\u7b56\u7565\u5904\u7406\u90e8\u5206\u635f\u574f\u6216\u7f3a\u5931\u7684\u89c2\u6d4b\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u9065\u611f\u9886\u57df\u5e94\u7528\u65f6\u9762\u4e34\u73af\u5883\u4e0a\u4e0b\u6587\u6709\u9650\u3001\u96be\u4ee5\u5904\u7406\u7f3a\u5931\u6216\u635f\u574f\u6570\u636e\u3001\u65e0\u6cd5\u53ef\u9760\u53cd\u6620\u7528\u6237\u610f\u56fe\u7b49\u663e\u8457\u9650\u5236\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u73af\u5883\u4e0a\u4e0b\u6587\u6761\u4ef6\u7684\u6269\u6563\u6a21\u578b\uff0c\u652f\u6301\u6587\u672c\u3001\u5143\u6570\u636e\u548c\u89c6\u89c9\u6570\u636e\u4e09\u79cd\u63a7\u5236\u4fe1\u53f7\u7684\u4efb\u610f\u7ec4\u5408\u8f93\u5165\uff0c\u91c7\u7528\u5143\u6570\u636e\u878d\u5408\u7b56\u7565\u5efa\u6a21\u5c5e\u6027\u5d4c\u5165\u4ea4\u4e92\u4ee5\u5904\u7406\u90e8\u5206\u635f\u574f\u6216\u7f3a\u5931\u7684\u89c2\u6d4b\u6570\u636e\u3002", "result": "\u5728\u5355\u56fe\u50cf\u548c\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u8bd5\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\uff08\u4f7f\u75286\u79cd\u4e0d\u540c\u6307\u6807\uff09\u4e0a\u5747\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u5bf9\u7f3a\u5931\u5143\u6570\u636e\u7684\u9c81\u68d2\u6027\u3001\u5bf9\u63a7\u5236\u8f93\u5165\u7684\u9ad8\u54cd\u5e94\u6027\uff0c\u4ee5\u53ca\u66f4\u9ad8\u7684\u4fdd\u771f\u5ea6\u3001\u51c6\u786e\u6027\u548c\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "\u73af\u5883\u4e0a\u4e0b\u6587\u6761\u4ef6\u5316\u80fd\u591f\u63d0\u9ad8\u536b\u661f\u56fe\u50cf\u57fa\u7840\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8be5\u6a21\u578b\u6709\u671b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5f97\u5230\u5e94\u7528\u3002\u6536\u96c6\u7684\u4e09\u6a21\u6001\u6570\u636e\u96c6\u662f\u9996\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u7ed3\u5408\u8fd9\u4e09\u79cd\u4e0d\u540c\u5a92\u4ecb\u6570\u636e\u7684\u96c6\u5408\u3002"}}
{"id": "2509.24878", "pdf": "https://arxiv.org/pdf/2509.24878", "abs": "https://arxiv.org/abs/2509.24878", "authors": ["Jiuhong Xiao", "Roshan Nayak", "Ning Zhang", "Daniel Tortei", "Giuseppe Loianno"], "title": "ThermalGen: Style-Disentangled Flow-Based Generative Models for RGB-to-Thermal Image Translation", "categories": ["cs.CV", "cs.RO"], "comment": "23 pages including the checklist and appendix. Accepted at NeurIPS   2025", "summary": "Paired RGB-thermal data is crucial for visual-thermal sensor fusion and cross-modality tasks, including important applications such as multi-modal image alignment and retrieval. However, the scarcity of synchronized and calibrated RGB-thermal image pairs presents a major obstacle to progress in these areas. To overcome this challenge, RGB-to-Thermal (RGB-T) image translation has emerged as a promising solution, enabling the synthesis of thermal images from abundant RGB datasets for training purposes. In this study, we propose ThermalGen, an adaptive flow-based generative model for RGB-T image translation, incorporating an RGB image conditioning architecture and a style-disentangled mechanism. To support large-scale training, we curated eight public satellite-aerial, aerial, and ground RGB-T paired datasets, and introduced three new large-scale satellite-aerial RGB-T datasets--DJI-day, Bosonplus-day, and Bosonplus-night--captured across diverse times, sensor types, and geographic regions. Extensive evaluations across multiple RGB-T benchmarks demonstrate that ThermalGen achieves comparable or superior translation performance compared to existing GAN-based and diffusion-based methods. To our knowledge, ThermalGen is the first RGB-T image translation model capable of synthesizing thermal images that reflect significant variations in viewpoints, sensor characteristics, and environmental conditions. Project page: http://xjh19971.github.io/ThermalGen", "AI": {"tldr": "\u63d0\u51fa\u4e86ThermalGen\uff0c\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u6d41\u7684RGB-\u70ed\u56fe\u50cf\u8f6c\u6362\u751f\u6210\u6a21\u578b\uff0c\u80fd\u591f\u4eceRGB\u56fe\u50cf\u5408\u6210\u70ed\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86RGB-\u70ed\u914d\u5bf9\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "motivation": "\u540c\u6b65\u6821\u51c6\u7684RGB-\u70ed\u56fe\u50cf\u5bf9\u7a00\u7f3a\uff0c\u963b\u788d\u4e86\u89c6\u89c9-\u70ed\u4f20\u611f\u5668\u878d\u5408\u548c\u8de8\u6a21\u6001\u4efb\u52a1\u7684\u53d1\u5c55\uff0cRGB-\u70ed\u56fe\u50cf\u8f6c\u6362\u6210\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u6709\u524d\u666f\u65b9\u6848\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u6d41\u751f\u6210\u6a21\u578b\uff0c\u7ed3\u5408RGB\u56fe\u50cf\u6761\u4ef6\u67b6\u6784\u548c\u98ce\u683c\u89e3\u8026\u673a\u5236\uff0c\u6784\u5efa\u4e86\u5305\u542b\u536b\u661f-\u822a\u7a7a\u3001\u822a\u7a7a\u548c\u5730\u9762RGB-\u70ed\u914d\u5bf9\u6570\u636e\u7684\u5927\u89c4\u6a21\u8bad\u7ec3\u96c6\u3002", "result": "\u5728\u591a\u4e2aRGB-\u70ed\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cThermalGen\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709GAN\u548c\u6269\u6563\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u8f6c\u6362\u6027\u80fd\uff0c\u80fd\u591f\u5408\u6210\u53cd\u6620\u89c6\u89d2\u3001\u4f20\u611f\u5668\u7279\u6027\u548c\u73af\u5883\u6761\u4ef6\u663e\u8457\u53d8\u5316\u7684\u70ed\u56fe\u50cf\u3002", "conclusion": "ThermalGen\u662f\u9996\u4e2a\u80fd\u591f\u5408\u6210\u53cd\u6620\u591a\u79cd\u53d8\u5316\u56e0\u7d20\u7684\u70ed\u56fe\u50cf\u7684RGB-\u70ed\u8f6c\u6362\u6a21\u578b\uff0c\u4e3aRGB-\u70ed\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24888", "pdf": "https://arxiv.org/pdf/2509.24888", "abs": "https://arxiv.org/abs/2509.24888", "authors": ["Fankai Jia", "Daisong Gan", "Zhe Zhang", "Zhaochi Wen", "Chenchen Dan", "Dong Liang", "Haifeng Wang"], "title": "MMRQA: Signal-Enhanced Multimodal Large Language Models for MRI Quality Assessment", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Magnetic resonance imaging (MRI) quality assessment is crucial for clinical decision-making, yet remains challenging due to data scarcity and protocol variability. Traditional approaches face fundamental trade-offs: signal-based methods like MRIQC provide quantitative metrics but lack semantic understanding, while deep learning approaches achieve high accuracy but sacrifice interpretability. To address these limitations, we introduce the Multimodal MRI Quality Assessment (MMRQA) framework, pioneering the integration of multimodal large language models (MLLMs) with acquisition-aware signal processing. MMRQA combines three key innovations: robust metric extraction via MRQy augmented with simulated artifacts, structured transformation of metrics into question-answer pairs using Qwen, and parameter-efficient fusion through Low-Rank Adaptation (LoRA) of LLaVA-OneVision. Evaluated on MR-ART, FastMRI, and MyConnectome benchmarks, MMRQA achieves state-of-the-art performance with strong zero-shot generalization, as validated by comprehensive ablation studies. By bridging quantitative analysis with semantic reasoning, our framework generates clinically interpretable outputs that enhance quality control in dynamic medical settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86MMRQA\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u91c7\u96c6\u611f\u77e5\u4fe1\u53f7\u5904\u7406\u76f8\u7ed3\u5408\uff0c\u7528\u4e8eMRI\u8d28\u91cf\u8bc4\u4f30\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3MRI\u8d28\u91cf\u8bc4\u4f30\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u534f\u8bae\u53d8\u5f02\u6027\u95ee\u9898\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u5728\u5b9a\u91cf\u6307\u6807\u4e0e\u8bed\u4e49\u7406\u89e3\u4e4b\u95f4\u7684\u6743\u8861\u9650\u5236\u3002", "method": "\u7ed3\u5408\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u901a\u8fc7MRQy\u589e\u5f3a\u6a21\u62df\u4f2a\u5f71\u7684\u9c81\u68d2\u6307\u6807\u63d0\u53d6\u3001\u4f7f\u7528Qwen\u5c06\u6307\u6807\u7ed3\u6784\u5316\u8f6c\u6362\u4e3a\u95ee\u7b54\u5bf9\u3001\u901a\u8fc7LoRA\u53c2\u6570\u9ad8\u6548\u878d\u5408LLaVA-OneVision\u3002", "result": "\u5728MR-ART\u3001FastMRI\u548cMyConnectome\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5177\u6709\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u6865\u63a5\u5b9a\u91cf\u5206\u6790\u4e0e\u8bed\u4e49\u63a8\u7406\uff0c\u8be5\u6846\u67b6\u751f\u6210\u4e34\u5e8a\u53ef\u89e3\u91ca\u7684\u8f93\u51fa\uff0c\u589e\u5f3a\u4e86\u52a8\u6001\u533b\u7597\u73af\u5883\u4e2d\u7684\u8d28\u91cf\u63a7\u5236\u3002"}}
{"id": "2509.24891", "pdf": "https://arxiv.org/pdf/2509.24891", "abs": "https://arxiv.org/abs/2509.24891", "authors": ["Mostafa Mohaimen Akand Faisal", "Rabeya Amin Jhuma"], "title": "VAGUEGAN: Stealthy Poisoning and Backdoor Attacks on Image Generative Pipelines", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Generative models such as GANs and diffusion models are widely used to synthesize photorealistic images and to support downstream creative and editing tasks. While adversarial attacks on discriminative models are well studied, attacks targeting generative pipelines where small, stealthy perturbations in inputs lead to controlled changes in outputs are less explored. This study introduces VagueGAN, an attack pipeline combining a modular perturbation network PoisonerNet with a Generator Discriminator pair to craft stealthy triggers that cause targeted changes in generated images. Attack efficacy is evaluated using a custom proxy metric, while stealth is analyzed through perceptual and frequency domain measures. The transferability of the method to a modern diffusion based pipeline is further examined through ControlNet guided editing. Interestingly, the experiments show that poisoned outputs can display higher visual quality compared to clean counterparts, challenging the assumption that poisoning necessarily reduces fidelity. Unlike conventional pixel level perturbations, latent space poisoning in GANs and diffusion pipelines can retain or even enhance output aesthetics, exposing a blind spot in pixel level defenses. Moreover, carefully optimized perturbations can produce consistent, stealthy effects on generator outputs while remaining visually inconspicuous, raising concerns for the integrity of image generation pipelines.", "AI": {"tldr": "VagueGAN\u662f\u4e00\u79cd\u9488\u5bf9\u751f\u6210\u6a21\u578b\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u6a21\u5757\u5316\u6270\u52a8\u7f51\u7edcPoisonerNet\u548c\u751f\u6210\u5668-\u5224\u522b\u5668\u5bf9\uff0c\u5236\u4f5c\u9690\u853d\u7684\u89e6\u53d1\u4fe1\u53f7\uff0c\u5728\u751f\u6210\u56fe\u50cf\u4e2d\u5f15\u8d77\u76ee\u6807\u53d8\u5316\u3002", "motivation": "\u867d\u7136\u5bf9\u6297\u6027\u653b\u51fb\u5728\u5224\u522b\u6a21\u578b\u4e2d\u5df2\u6709\u6df1\u5165\u7814\u7a76\uff0c\u4f46\u5728\u751f\u6210\u7ba1\u9053\u4e2d\uff0c\u8f93\u5165\u7684\u5c0f\u578b\u9690\u853d\u6270\u52a8\u5bfc\u81f4\u8f93\u51fa\u53d7\u63a7\u53d8\u5316\u7684\u7814\u7a76\u8f83\u5c11\u63a2\u7d22\u3002", "method": "\u4f7f\u7528PoisonerNet\u4e0e\u751f\u6210\u5668-\u5224\u522b\u5668\u5bf9\u7ec4\u5408\u7684\u653b\u51fb\u7ba1\u9053\uff0c\u8bc4\u4f30\u653b\u51fb\u6548\u679c\u4f7f\u7528\u81ea\u5b9a\u4e49\u4ee3\u7406\u6307\u6807\uff0c\u9690\u853d\u6027\u901a\u8fc7\u611f\u77e5\u548c\u9891\u57df\u6d4b\u91cf\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4e2d\u6bd2\u8f93\u51fa\u53ef\u80fd\u6bd4\u5e72\u51c0\u5bf9\u5e94\u7269\u5177\u6709\u66f4\u9ad8\u7684\u89c6\u89c9\u8d28\u91cf\uff0c\u6311\u6218\u4e86\u4e2d\u6bd2\u5fc5\u7136\u964d\u4f4e\u4fdd\u771f\u5ea6\u7684\u5047\u8bbe\u3002\u6f5c\u5728\u7a7a\u95f4\u4e2d\u6bd2\u53ef\u4ee5\u4fdd\u7559\u751a\u81f3\u589e\u5f3a\u8f93\u51fa\u7f8e\u5b66\u3002", "conclusion": "\u7cbe\u5fc3\u4f18\u5316\u7684\u6270\u52a8\u53ef\u4ee5\u5728\u751f\u6210\u5668\u8f93\u51fa\u4e0a\u4ea7\u751f\u4e00\u81f4\u3001\u9690\u853d\u7684\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u4e0a\u4e0d\u660e\u663e\uff0c\u8fd9\u5f15\u53d1\u4e86\u56fe\u50cf\u751f\u6210\u7ba1\u9053\u5b8c\u6574\u6027\u7684\u62c5\u5fe7\u3002"}}
{"id": "2509.24893", "pdf": "https://arxiv.org/pdf/2509.24893", "abs": "https://arxiv.org/abs/2509.24893", "authors": ["Yu Ma", "Guoliang Wei", "Yue Cheng"], "title": "DWGS: Enhancing Sparse-View Gaussian Splatting with Hybrid-Loss Depth Estimation and Bidirectional Warping", "categories": ["cs.CV"], "comment": "14 pages, 21 figures", "summary": "Novel View Synthesis (NVS) from sparse views remains a core challenge in 3D reconstruction, typically suffering from overfitting, geometric distortion, and incomplete scene recovery due to limited multi-view constraints. Although 3D Gaussian Splatting (3DGS) enables real-time, high-fidelity rendering, it suffers from floating artifacts and structural inconsistencies under sparse-input settings. To address these issues, we propose DWGS, a novel unified framework that enhances 3DGS for sparse-view synthesis by integrating robust structural cues, virtual view constraints, and occluded region completion. Our approach introduces three principal contributions: a Hybrid-Loss Depth Estimation module that leverages dense matching priors with reprojection, point propagation, and smoothness constraints to enforce multi-view consistency; a Bidirectional Warping Virtual View Synthesis method generates virtual training views to impose stronger geometric and photometric constraints; and an Occlusion-Aware Reconstruction component that utilizes depth-difference mask and a learning-based inpainting model to recover obscured regions. Extensive experiments on standard benchmarks (LLFF, Blender, and DTU) show that DWGS achieves a new state-of-the-art, achieving up to 21.13 dB PSNR and 0.189 LPIPS, while retaining real-time inference capabilities.", "AI": {"tldr": "DWGS\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u7ed3\u6784\u7ebf\u7d22\u3001\u865a\u62df\u89c6\u56fe\u7ea6\u675f\u548c\u906e\u6321\u533a\u57df\u8865\u5168\u6765\u589e\u5f3a3D\u9ad8\u65af\u6cfc\u6e85\u5728\u7a00\u758f\u89c6\u56fe\u5408\u6210\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u7a00\u758f\u89c6\u56fe\u4e0b\u7684\u65b0\u89c6\u89d2\u5408\u6210\u5b58\u5728\u8fc7\u62df\u5408\u3001\u51e0\u4f55\u5931\u771f\u548c\u573a\u666f\u6062\u590d\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\uff0c3D\u9ad8\u65af\u6cfc\u6e85\u5728\u7a00\u758f\u8f93\u5165\u4e0b\u4f1a\u51fa\u73b0\u6d6e\u52a8\u4f2a\u5f71\u548c\u7ed3\u6784\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u4e3b\u8981\u8d21\u732e\uff1a\u6df7\u5408\u635f\u5931\u6df1\u5ea6\u4f30\u8ba1\u6a21\u5757\u3001\u53cc\u5411\u626d\u66f2\u865a\u62df\u89c6\u56fe\u5408\u6210\u65b9\u6cd5\u548c\u906e\u6321\u611f\u77e5\u91cd\u5efa\u7ec4\u4ef6\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0cPSNR\u6700\u9ad8\u8fbe21.13 dB\uff0cLPIPS\u4e3a0.189\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "DWGS\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u89c6\u56fe\u5408\u6210\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u65b0\u89c6\u89d2\u5408\u6210\u3002"}}
{"id": "2509.24899", "pdf": "https://arxiv.org/pdf/2509.24899", "abs": "https://arxiv.org/abs/2509.24899", "authors": ["Mohsen Ghafoorian", "Denis Korzhenkov", "Amirhossein Habibian"], "title": "Attention Surgery: An Efficient Recipe to Linearize Your Video Diffusion Transformer", "categories": ["cs.CV"], "comment": null, "summary": "Transformer-based video diffusion models (VDMs) deliver state-of-the-art video generation quality but are constrained by the quadratic cost of self-attention, making long sequences and high resolutions computationally expensive. While linear attention offers sub-quadratic complexity, prior attempts fail to match the expressiveness of softmax attention without costly retraining. We introduce \\textit{Attention Surgery}, an efficient framework for \\textit{linearizing} or \\textit{hybridizing} attention in pretrained VDMs without training from scratch. Inspired by recent advances in language models, our method combines a novel hybrid attention mechanism-mixing softmax and linear tokens-with a lightweight distillation and fine-tuning pipeline requiring only a few GPU-days. Additionally, we incorporate a cost-aware block-rate strategy to balance expressiveness and efficiency across layers. Applied to Wan2.1 1.3B, a state-of-the-art DiT-based VDM, Attention Surgery achieves the first competitive sub-quadratic attention video diffusion models, reducing attention cost by up to 40\\% in terms of FLOPs, while maintaining generation quality as measured on the standard VBench and VBench-2.0 benchmarks.", "AI": {"tldr": "\u63d0\u51faAttention Surgery\u6846\u67b6\uff0c\u65e0\u9700\u4ece\u5934\u8bad\u7ec3\u5373\u53ef\u5c06\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\u7ebf\u6027\u5316\u6216\u6df7\u5408\u5316\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c", "motivation": "\u57fa\u4e8eTransformer\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u867d\u7136\u751f\u6210\u8d28\u91cf\u4f18\u79c0\uff0c\u4f46\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u9636\u8ba1\u7b97\u6210\u672c\u9650\u5236\u4e86\u957f\u5e8f\u5217\u548c\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u7684\u751f\u6210\u6548\u7387", "method": "\u7ed3\u5408\u65b0\u578b\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\uff08\u6df7\u5408softmax\u548c\u7ebf\u6027token\uff09\u4e0e\u8f7b\u91cf\u7ea7\u84b8\u998f\u5fae\u8c03\u6d41\u7a0b\uff0c\u5e76\u91c7\u7528\u6210\u672c\u611f\u77e5\u7684\u5757\u7387\u7b56\u7565\u6765\u5e73\u8861\u5404\u5c42\u7684\u8868\u8fbe\u80fd\u529b\u548c\u6548\u7387", "result": "\u5728Wan2.1 1.3B\u6a21\u578b\u4e0a\u5b9e\u73b0\u9996\u4e2a\u5177\u6709\u7ade\u4e89\u529b\u7684\u4e9a\u4e8c\u9636\u6ce8\u610f\u529b\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u6ce8\u610f\u529b\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u9ad8\u8fbe40%\uff0c\u540c\u65f6\u5728VBench\u548cVBench-2.0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u751f\u6210\u8d28\u91cf", "conclusion": "Attention Surgery\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u52a0\u901f\u65b9\u6848"}}
{"id": "2509.24900", "pdf": "https://arxiv.org/pdf/2509.24900", "abs": "https://arxiv.org/abs/2509.24900", "authors": ["Zhihong Chen", "Xuehai Bai", "Yang Shi", "Chaoyou Fu", "Huanyu Zhang", "Haotian Wang", "Xiaoyan Sun", "Zhang Zhang", "Liang Wang", "Yuanxing Zhang", "Pengfei Wan", "Yi-Fan Zhang"], "title": "OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The performance of unified multimodal models for image generation and editing is fundamentally constrained by the quality and comprehensiveness of their training data. While existing datasets have covered basic tasks like style transfer and simple object manipulation, they often lack the systematic structure and challenging scenarios required for real-world applications. To address this bottleneck, we introduce OpenGPT-4o-Image, a large-scale dataset constructed using a novel methodology that combines hierarchical task taxonomy with automated data generation. Our taxonomy not only includes fundamental capabilities such as text rendering and style control but also introduces highly practical yet challenging categories like scientific imagery for chemistry illustrations and complex instruction editing requiring simultaneous execution of multiple operations. Through an automated pipeline leveraging structured resource pools and GPT-4o, we generate 80k high-quality instruction-image pairs with controlled diversity, covering 11 major domains and 51 subtasks. Extensive experiments show that fine-tuning leading models on our dataset achieves significant performance gains across multiple benchmarks, with improvements of up to 18\\% on editing tasks (UniWorld-V1 on ImgEdit-Bench) and 13% on generation tasks (Harmon on GenEval). Our work demonstrates that systematic data construction is key to advancing multimodal AI capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86OpenGPT-4o-Image\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u4efb\u52a1\u5206\u7c7b\u548c\u81ea\u52a8\u5316\u6570\u636e\u751f\u6210\u65b9\u6cd5\u6784\u5efa\uff0c\u5305\u542b80k\u9ad8\u8d28\u91cf\u6307\u4ee4-\u56fe\u50cf\u5bf9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6a21\u578b\u7684\u751f\u6210\u548c\u7f16\u8f91\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u7cfb\u7edf\u7ed3\u6784\u548c\u771f\u5b9e\u5e94\u7528\u573a\u666f\u7684\u6311\u6218\u6027\u4efb\u52a1\uff0c\u9650\u5236\u4e86\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u5c42\u6b21\u5316\u4efb\u52a1\u5206\u7c7b\u548c\u81ea\u52a8\u5316\u6570\u636e\u751f\u6210\uff0c\u5229\u7528\u7ed3\u6784\u5316\u8d44\u6e90\u6c60\u548cGPT-4o\u6784\u5efa\u5305\u542b11\u4e2a\u4e3b\u8981\u9886\u57df\u300151\u4e2a\u5b50\u4efb\u52a1\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "result": "\u5728\u9886\u5148\u6a21\u578b\u4e0a\u5fae\u8c03\u540e\uff0c\u7f16\u8f91\u4efb\u52a1\u6027\u80fd\u63d0\u534718%\uff08UniWorld-V1\u5728ImgEdit-Bench\uff09\uff0c\u751f\u6210\u4efb\u52a1\u63d0\u534713%\uff08Harmon\u5728GenEval\uff09\u3002", "conclusion": "\u7cfb\u7edf\u5316\u7684\u6570\u636e\u6784\u5efa\u662f\u63d0\u5347\u591a\u6a21\u6001AI\u80fd\u529b\u7684\u5173\u952e\u3002"}}
{"id": "2509.24913", "pdf": "https://arxiv.org/pdf/2509.24913", "abs": "https://arxiv.org/abs/2509.24913", "authors": ["Tian Xia", "Matthew Sinclair", "Andreas Schuh", "Fabio De Sousa Ribeiro", "Raghav Mehta", "Rajat Rasal", "Esther Puyol-Ant\u00f3n", "Samuel Gerber", "Kersten Petersen", "Michiel Schaap", "Ben Glocker"], "title": "Segmentor-Guided Counterfactual Fine-Tuning for Image Synthesis", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at MICCAI 2025", "summary": "Counterfactual image generation is a powerful tool for augmenting training data, de-biasing datasets, and modeling disease. Current approaches rely on external classifiers or regressors to increase the effectiveness of subject-level interventions (e.g., changing the patient's age). For structure-specific interventions (e.g., changing the area of the left lung in a chest radiograph), we show that this is insufficient, and can result in undesirable global effects across the image domain. Previous work used pixel-level label maps as guidance, requiring a user to provide hypothetical segmentations which are tedious and difficult to obtain. We propose Segmentor-guided Counterfactual Fine-Tuning (Seg-CFT), which preserves the simplicity of intervening on scalar-valued, structure-specific variables while producing locally coherent and effective counterfactuals. We demonstrate the capability of generating realistic chest radiographs, and we show promising results for modeling coronary artery disease. Code: https://github.com/biomedia-mira/seg-cft.", "AI": {"tldr": "\u63d0\u51fa\u4e86Seg-CFT\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5206\u5272\u5668\u5f15\u5bfc\u751f\u6210\u5c40\u90e8\u4e00\u81f4\u7684\u53cd\u4e8b\u5b9e\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u7ed3\u6784\u7279\u5b9a\u5e72\u9884\u65f6\u4ea7\u751f\u5168\u5c40\u4e0d\u826f\u5f71\u54cd\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u53cd\u4e8b\u5b9e\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u5206\u7c7b\u5668\u6216\u56de\u5f52\u5668\uff0c\u5bf9\u4e8e\u7ed3\u6784\u7279\u5b9a\u5e72\u9884\uff08\u5982\u6539\u53d8\u5de6\u80ba\u9762\u79ef\uff09\u6548\u679c\u4e0d\u8db3\uff0c\u4e14\u4f1a\u4ea7\u751f\u4e0d\u5e0c\u671b\u7684\u5168\u5c40\u5f71\u54cd\u3002\u5148\u524d\u5de5\u4f5c\u9700\u8981\u50cf\u7d20\u7ea7\u6807\u7b7e\u56fe\u4f5c\u4e3a\u6307\u5bfc\uff0c\u4f46\u83b7\u53d6\u8fd9\u4e9b\u5047\u8bbe\u5206\u5272\u56fe\u65e2\u7e41\u7410\u53c8\u56f0\u96be\u3002", "method": "\u63d0\u51faSegmentor-guided Counterfactual Fine-Tuning (Seg-CFT)\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u5bf9\u7ed3\u6784\u7279\u5b9a\u6807\u91cf\u53d8\u91cf\u5e72\u9884\u7b80\u5355\u6027\u7684\u540c\u65f6\uff0c\u751f\u6210\u5c40\u90e8\u4e00\u81f4\u4e14\u6709\u6548\u7684\u53cd\u4e8b\u5b9e\u56fe\u50cf\u3002", "result": "\u5c55\u793a\u4e86\u751f\u6210\u771f\u5b9e\u80f8\u90e8X\u5149\u7247\u7684\u80fd\u529b\uff0c\u5e76\u5728\u51a0\u72b6\u52a8\u8109\u75be\u75c5\u5efa\u6a21\u65b9\u9762\u663e\u793a\u51fa\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002", "conclusion": "Seg-CFT\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u751f\u6210\u7ed3\u6784\u7279\u5b9a\u7684\u53cd\u4e8b\u5b9e\u56fe\u50cf\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.24935", "pdf": "https://arxiv.org/pdf/2509.24935", "abs": "https://arxiv.org/abs/2509.24935", "authors": ["Sangeek Hyun", "MinKyu Lee", "Jae-Pil Heo"], "title": "Scalable GANs with Transformers", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Scalability has driven recent advances in generative modeling, yet its principles remain underexplored for adversarial learning. We investigate the scalability of Generative Adversarial Networks (GANs) through two design choices that have proven to be effective in other types of generative models: training in a compact Variational Autoencoder latent space and adopting purely transformer-based generators and discriminators. Training in latent space enables efficient computation while preserving perceptual fidelity, and this efficiency pairs naturally with plain transformers, whose performance scales with computational budget. Building on these choices, we analyze failure modes that emerge when naively scaling GANs. Specifically, we find issues as underutilization of early layers in the generator and optimization instability as the network scales. Accordingly, we provide simple and scale-friendly solutions as lightweight intermediate supervision and width-aware learning-rate adjustment. Our experiments show that GAT, a purely transformer-based and latent-space GANs, can be easily trained reliably across a wide range of capacities (S through XL). Moreover, GAT-XL/2 achieves state-of-the-art single-step, class-conditional generation performance (FID of 2.96) on ImageNet-256 in just 40 epochs, 6x fewer epochs than strong baselines.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86GAN\u7684\u53ef\u6269\u5c55\u6027\uff0c\u901a\u8fc7\u5728VAE\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8bad\u7ec3\u5e76\u4f7f\u7528\u7eaftransformer\u67b6\u6784\uff0c\u89e3\u51b3\u4e86GAN\u89c4\u6a21\u5316\u65f6\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u63d0\u51fa\u4e86\u8f7b\u91cf\u7ea7\u4e2d\u95f4\u76d1\u7763\u548c\u5bbd\u5ea6\u611f\u77e5\u5b66\u4e60\u7387\u8c03\u6574\u7b49\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u63a2\u7d22\u751f\u6210\u5bf9\u6297\u7f51\u7edc(GANs)\u7684\u53ef\u6269\u5c55\u6027\u539f\u5219\uff0c\u867d\u7136\u53ef\u6269\u5c55\u6027\u63a8\u52a8\u4e86\u751f\u6210\u6a21\u578b\u7684\u8fdb\u5c55\uff0c\u4f46\u5728\u5bf9\u6297\u5b66\u4e60\u4e2d\u7684\u539f\u7406\u4ecd\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5728\u7d27\u51d1\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8bad\u7ec3\uff0c\u91c7\u7528\u7eaftransformer\u751f\u6210\u5668\u548c\u5224\u522b\u5668\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4e2d\u95f4\u76d1\u7763\u548c\u5bbd\u5ea6\u611f\u77e5\u5b66\u4e60\u7387\u8c03\u6574\u89e3\u51b3\u89c4\u6a21\u5316\u95ee\u9898\u3002", "result": "GAT-XL/2\u5728ImageNet-256\u4e0a\u5b9e\u73b0\u4e86\u5355\u6b65\u3001\u7c7b\u6761\u4ef6\u751f\u6210\u7684\u6700\u5148\u8fdb\u6027\u80fd(FID\u4e3a2.96)\uff0c\u4ec5\u970040\u4e2aepoch\uff0c\u6bd4\u5f3a\u57fa\u7ebf\u5c116\u500d\u8bad\u7ec3\u5468\u671f\u3002", "conclusion": "\u7eaftransformer\u548c\u6f5c\u5728\u7a7a\u95f4\u7684GANs\u53ef\u4ee5\u5728\u5404\u79cd\u5bb9\u91cf\u8303\u56f4\u5185\u53ef\u9760\u8bad\u7ec3\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u89c4\u6a21\u5316\u89e3\u51b3\u65b9\u6848\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u751f\u6210\u6027\u80fd\u3002"}}
{"id": "2509.24979", "pdf": "https://arxiv.org/pdf/2509.24979", "abs": "https://arxiv.org/abs/2509.24979", "authors": ["Haotian Dong", "Wenjing Wang", "Chen Li", "Di Lin"], "title": "Wan-Alpha: High-Quality Text-to-Video Generation with Alpha Channel", "categories": ["cs.CV"], "comment": null, "summary": "RGBA video generation, which includes an alpha channel to represent transparency, is gaining increasing attention across a wide range of applications. However, existing methods often neglect visual quality, limiting their practical usability. In this paper, we propose \\textit{Wan-Alpha}, a new framework that generates transparent videos by learning both RGB and alpha channels jointly. We design an effective variational autoencoder (VAE) that encodes the alpha channel into the RGB latent space. Then, to support the training of our diffusion transformer, we construct a high-quality and diverse RGBA video dataset. Compared with state-of-the-art methods, our model demonstrates superior performance in visual quality, motion realism, and transparency rendering. Notably, our model can generate a wide variety of semi-transparent objects, glowing effects, and fine-grained details such as hair strands. The released model is available on our website: \\href{https://donghaotian123.github.io/Wan-Alpha/}{https://donghaotian123.github.io/Wan-Alpha/}.", "AI": {"tldr": "Wan-Alpha\u662f\u4e00\u4e2a\u751f\u6210RGBA\u900f\u660e\u89c6\u9891\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60RGB\u548calpha\u901a\u9053\uff0c\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5c06alpha\u901a\u9053\u7f16\u7801\u5230RGB\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u6784\u5efa\u9ad8\u8d28\u91cfRGBA\u89c6\u9891\u6570\u636e\u96c6\u8bad\u7ec3\u6269\u6563\u53d8\u6362\u5668\u3002", "motivation": "\u73b0\u6709RGBA\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u89c6\u89c9\u8d28\u91cf\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u6709\u6548\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5c06alpha\u901a\u9053\u7f16\u7801\u5230RGB\u6f5c\u5728\u7a7a\u95f4\uff0c\u6784\u5efa\u9ad8\u8d28\u91cfRGBA\u89c6\u9891\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u6269\u6563\u53d8\u6362\u5668\u6a21\u578b\u3002", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u89c6\u89c9\u8d28\u91cf\u3001\u8fd0\u52a8\u771f\u5b9e\u611f\u548c\u900f\u660e\u5ea6\u6e32\u67d3\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u80fd\u751f\u6210\u5404\u79cd\u534a\u900f\u660e\u7269\u4f53\u3001\u53d1\u5149\u6548\u679c\u548c\u5934\u53d1\u4e1d\u7b49\u7cbe\u7ec6\u7ec6\u8282\u3002", "conclusion": "Wan-Alpha\u6846\u67b6\u5728RGBA\u89c6\u9891\u751f\u6210\u65b9\u9762\u5177\u6709\u4f18\u8d8a\u6027\u80fd\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u900f\u660e\u89c6\u9891\u5185\u5bb9\u3002"}}
{"id": "2509.24980", "pdf": "https://arxiv.org/pdf/2509.24980", "abs": "https://arxiv.org/abs/2509.24980", "authors": ["Shuang Liang", "Jing He", "Chuanmeizhi Wang", "Lejun Liao", "Guo Zhang", "Yingcong Chen", "Yuan Yuan"], "title": "SDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose Estimation", "categories": ["cs.CV"], "comment": "18 pages, 9 figures, 9 tables", "summary": "Pre-trained diffusion models provide rich multi-scale latent features and are emerging as powerful vision backbones. While recent works such as Marigold~\\citep{ke2024repurposing} and Lotus~\\citep{he2024lotus} adapt diffusion priors for dense prediction with strong cross-domain generalization, their potential for structured outputs (e.g., human pose estimation) remains underexplored. In this paper, we propose \\textbf{SDPose}, a fine-tuning framework built upon Stable Diffusion to fully exploit pre-trained diffusion priors for human pose estimation. First, rather than modifying cross-attention modules or introducing learnable embeddings, we directly predict keypoint heatmaps in the SD U-Net's image latent space to preserve the original generative priors. Second, we map these latent features into keypoint heatmaps through a lightweight convolutional pose head, which avoids disrupting the pre-trained backbone. Finally, to prevent overfitting and enhance out-of-distribution robustness, we incorporate an auxiliary RGB reconstruction branch that preserves domain-transferable generative semantics. To evaluate robustness under domain shift, we further construct \\textbf{COCO-OOD}, a style-transferred variant of COCO with preserved annotations. With just one-fifth of the training schedule used by Sapiens on COCO, SDPose attains parity with Sapiens-1B/2B on the COCO validation set and establishes a new state of the art on the cross-domain benchmarks HumanArt and COCO-OOD. Furthermore, we showcase SDPose as a zero-shot pose annotator for downstream controllable generation tasks, including ControlNet-based image synthesis and video generation, where it delivers qualitatively superior pose guidance.", "AI": {"tldr": "SDPose\u662f\u4e00\u4e2a\u57fa\u4e8eStable Diffusion\u7684\u5fae\u8c03\u6846\u67b6\uff0c\u7528\u4e8e\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u3002\u5b83\u901a\u8fc7\u76f4\u63a5\u5728SD U-Net\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u9884\u6d4b\u5173\u952e\u70b9\u70ed\u56fe\u6765\u4fdd\u6301\u751f\u6210\u5148\u9a8c\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u5377\u79ef\u59ff\u6001\u5934\uff0c\u5e76\u52a0\u5165RGB\u91cd\u5efa\u5206\u652f\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002\u5728\u5c11\u91cf\u8bad\u7ec3\u4e0b\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5e76\u80fd\u7528\u4e8e\u96f6\u6837\u672c\u59ff\u6001\u6807\u6ce8\u3002", "motivation": "\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u591a\u5c3a\u5ea6\u6f5c\u5728\u7279\u5f81\uff0c\u4f46\u5b83\u4eec\u5728\u7ed3\u6784\u5316\u8f93\u51fa\uff08\u5982\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\uff09\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u65b9\u6cd5\u5982Marigold\u548cLotus\u4e3b\u8981\u5173\u6ce8\u5bc6\u96c6\u9884\u6d4b\uff0c\u800cSDPose\u65e8\u5728\u5145\u5206\u5229\u7528\u6269\u6563\u5148\u9a8c\u8fdb\u884c\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u3002", "method": "1. \u76f4\u63a5\u5728SD U-Net\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u9884\u6d4b\u5173\u952e\u70b9\u70ed\u56fe\uff0c\u4fdd\u6301\u539f\u59cb\u751f\u6210\u5148\u9a8c\uff1b2. \u901a\u8fc7\u8f7b\u91cf\u7ea7\u5377\u79ef\u59ff\u6001\u5934\u5c06\u6f5c\u5728\u7279\u5f81\u6620\u5c04\u5230\u5173\u952e\u70b9\u70ed\u56fe\uff1b3. \u52a0\u5165\u8f85\u52a9RGB\u91cd\u5efa\u5206\u652f\u4ee5\u9632\u6b62\u8fc7\u62df\u5408\u5e76\u589e\u5f3a\u57df\u5916\u9c81\u68d2\u6027\u3002", "result": "\u5728\u4ec5\u4f7f\u7528Sapiens\u4e94\u5206\u4e4b\u4e00\u8bad\u7ec3\u65f6\u95f4\u7684\u60c5\u51b5\u4e0b\uff0cSDPose\u5728COCO\u9a8c\u8bc1\u96c6\u4e0a\u4e0eSapiens-1B/2B\u6301\u5e73\uff0c\u5728\u8de8\u57df\u57fa\u51c6HumanArt\u548cCOCO-OOD\u4e0a\u521b\u4e0b\u65b0SOTA\u3002\u8fd8\u80fd\u4f5c\u4e3a\u96f6\u6837\u672c\u59ff\u6001\u6807\u6ce8\u5668\u7528\u4e8e\u53ef\u63a7\u751f\u6210\u4efb\u52a1\u3002", "conclusion": "SDPose\u6210\u529f\u5c55\u793a\u4e86\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u5728\u7ed3\u6784\u5316\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u4fdd\u6301\u751f\u6210\u5148\u9a8c\u548c\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u548c\u8de8\u57df\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.24997", "pdf": "https://arxiv.org/pdf/2509.24997", "abs": "https://arxiv.org/abs/2509.24997", "authors": ["Yuyang Yin", "HaoXiang Guo", "Fangfu Liu", "Mengyu Wang", "Hanwen Liang", "Eric Li", "Yikai Wang", "Xiaojie Jin", "Yao Zhao", "Yunchao Wei"], "title": "PanoWorld-X: Generating Explorable Panoramic Worlds via Sphere-Aware Video Diffusion", "categories": ["cs.CV"], "comment": "Project page: \\url{https://yuyangyin.github.io/PanoWorld-X/}", "summary": "Generating a complete and explorable 360-degree visual world enables a wide range of downstream applications. While prior works have advanced the field, they remain constrained by either narrow field-of-view limitations, which hinder the synthesis of continuous and holistic scenes, or insufficient camera controllability that restricts free exploration by users or autonomous agents. To address this, we propose PanoWorld-X, a novel framework for high-fidelity and controllable panoramic video generation with diverse camera trajectories. Specifically, we first construct a large-scale dataset of panoramic video-exploration route pairs by simulating camera trajectories in virtual 3D environments via Unreal Engine. As the spherical geometry of panoramic data misaligns with the inductive priors from conventional video diffusion, we then introduce a Sphere-Aware Diffusion Transformer architecture that reprojects equirectangular features onto the spherical surface to model geometric adjacency in latent space, significantly enhancing visual fidelity and spatiotemporal continuity. Extensive experiments demonstrate that our PanoWorld-X achieves superior performance in various aspects, including motion range, control precision, and visual quality, underscoring its potential for real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86PanoWorld-X\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u53ef\u63a7\u7684\u5168\u666f\u89c6\u9891\uff0c\u652f\u6301\u591a\u6837\u5316\u7684\u76f8\u673a\u8f68\u8ff9\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u89c6\u91ce\u53d7\u9650\u548c\u76f8\u673a\u63a7\u5236\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u89c6\u91ce\u72ed\u7a84\u6216\u76f8\u673a\u63a7\u5236\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u8fde\u7eed\u3001\u5b8c\u6574\u573a\u666f\u7684\u5408\u6210\u548c\u81ea\u7531\u63a2\u7d22\u80fd\u529b\u3002", "method": "\u9996\u5148\u5728\u865a\u62df3D\u73af\u5883\u4e2d\u901a\u8fc7Unreal Engine\u6a21\u62df\u76f8\u673a\u8f68\u8ff9\u6784\u5efa\u5927\u89c4\u6a21\u5168\u666f\u89c6\u9891-\u63a2\u7d22\u8def\u5f84\u6570\u636e\u96c6\uff1b\u7136\u540e\u63d0\u51faSphere-Aware Diffusion Transformer\u67b6\u6784\uff0c\u5c06\u7b49\u77e9\u5f62\u7279\u5f81\u91cd\u6295\u5f71\u5230\u7403\u9762\u4ee5\u5efa\u6a21\u51e0\u4f55\u90bb\u63a5\u5173\u7cfb\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPanoWorld-X\u5728\u8fd0\u52a8\u8303\u56f4\u3001\u63a7\u5236\u7cbe\u5ea6\u548c\u89c6\u89c9\u8d28\u91cf\u7b49\u65b9\u9762\u5747\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u63a7\u7684\u5168\u666f\u89c6\u9891\u3002"}}
{"id": "2509.25027", "pdf": "https://arxiv.org/pdf/2509.25027", "abs": "https://arxiv.org/abs/2509.25027", "authors": ["Xiaoxiao Ma", "Haibo Qiu", "Guohui Zhang", "Zhixiong Zeng", "Siqi Yang", "Lin Ma", "Feng Zhao"], "title": "STAGE: Stable and Generalizable GRPO for Autoregressive Image Generation", "categories": ["cs.CV"], "comment": "Code available at https://github.com/krennic999/STAGE", "summary": "Reinforcement learning has recently been explored to improve text-to-image generation, yet applying existing GRPO algorithms to autoregressive (AR) image models remains challenging. The instability of the training process easily disrupts the pretrained model capability during long runs, resulting in marginal gains, degraded image quality, and poor generalization. In this work, we revisit GRPO for AR image generation and identify two key issues: contradictory gradients from unnecessary tokens and unstable policy entropy dynamics. To address these, we introduce STAGE, a stable and generalizable framework that leverages two targeted solutions: 1) Advantage/KL reweighting. Similarity-aware reweighting to alleviate conflicting updates; and 2) Entropy reward. An entropy-based reward corresponding to reference model to stabilize learning. With the help of alleviating conflicts between tokens and an entropy reward for stabilizing training, we reduce disruption of the pretrained distribution and mitigate reward hacking, which in turn improves generalization and transfer better to other benchmarks. Experiments across multiple benchmarks show that STAGE consistently improves visual quality, stability, and cross-task generalization compared to baseline GRPO.", "AI": {"tldr": "\u63d0\u51faSTAGE\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u52bf/KL\u91cd\u52a0\u6743\u548c\u71b5\u5956\u52b1\u6765\u89e3\u51b3AR\u56fe\u50cf\u751f\u6210\u4e2dGRPO\u8bad\u7ec3\u7684\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b", "motivation": "\u73b0\u6709GRPO\u7b97\u6cd5\u5e94\u7528\u4e8e\u81ea\u56de\u5f52\u56fe\u50cf\u6a21\u578b\u65f6\u5b58\u5728\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3001\u7834\u574f\u9884\u8bad\u7ec3\u6a21\u578b\u80fd\u529b\u3001\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898", "method": "1) \u4f18\u52bf/KL\u91cd\u52a0\u6743\uff1a\u76f8\u4f3c\u6027\u611f\u77e5\u91cd\u52a0\u6743\u7f13\u89e3\u51b2\u7a81\u66f4\u65b0\uff1b2) \u71b5\u5956\u52b1\uff1a\u57fa\u4e8e\u53c2\u8003\u6a21\u578b\u7684\u71b5\u5956\u52b1\u7a33\u5b9a\u5b66\u4e60\u8fc7\u7a0b", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSTAGE\u76f8\u6bd4\u57fa\u7ebfGRPO\u6301\u7eed\u63d0\u5347\u4e86\u89c6\u89c9\u8d28\u91cf\u3001\u7a33\u5b9a\u6027\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b", "conclusion": "STAGE\u901a\u8fc7\u7f13\u89e3token\u95f4\u51b2\u7a81\u548c\u7a33\u5b9a\u8bad\u7ec3\uff0c\u51cf\u5c11\u5bf9\u9884\u8bad\u7ec3\u5206\u5e03\u7684\u7834\u574f\uff0c\u7f13\u89e3\u5956\u52b1\u653b\u51fb\uff0c\u4ece\u800c\u6539\u5584\u6cdb\u5316\u80fd\u529b"}}
{"id": "2509.25075", "pdf": "https://arxiv.org/pdf/2509.25075", "abs": "https://arxiv.org/abs/2509.25075", "authors": ["Huaizhi Qu", "Xiao Wang", "Gengwei Zhang", "Jie Peng", "Tianlong Chen"], "title": "GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM Reconstruction", "categories": ["cs.CV", "cs.CE"], "comment": null, "summary": "Cryo-electron microscopy (cryo-EM) has become a central tool for high-resolution structural biology, yet the massive scale of datasets (often exceeding 100k particle images) renders 3D reconstruction both computationally expensive and memory intensive. Traditional Fourier-space methods are efficient but lose fidelity due to repeated transforms, while recent real-space approaches based on neural radiance fields (NeRFs) improve accuracy but incur cubic memory and computation overhead. Therefore, we introduce GEM, a novel cryo-EM reconstruction framework built on 3D Gaussian Splatting (3DGS) that operates directly in real-space while maintaining high efficiency. Instead of modeling the entire density volume, GEM represents proteins with compact 3D Gaussians, each parameterized by only 11 values. To further improve the training efficiency, we designed a novel gradient computation to 3D Gaussians that contribute to each voxel. This design substantially reduced both memory footprint and training cost. On standard cryo-EM benchmarks, GEM achieves up to 48% faster training and 12% lower memory usage compared to state-of-the-art methods, while improving local resolution by as much as 38.8%. These results establish GEM as a practical and scalable paradigm for cryo-EM reconstruction, unifying speed, efficiency, and high-resolution accuracy. Our code is available at https://github.com/UNITES-Lab/GEM.", "AI": {"tldr": "GEM\u662f\u4e00\u4e2a\u57fa\u4e8e3D\u9ad8\u65af\u6e85\u5c04\u7684\u51b7\u51bb\u7535\u955c\u91cd\u5efa\u6846\u67b6\uff0c\u76f4\u63a5\u5728\u5b9e\u7a7a\u95f4\u64cd\u4f5c\uff0c\u901a\u8fc7\u7d27\u51d1\u76843D\u9ad8\u65af\u8868\u793a\u86cb\u767d\u8d28\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u8bad\u7ec3\u901f\u5ea6\u3001\u66f4\u4f4e\u7684\u5185\u5b58\u4f7f\u7528\u548c\u66f4\u9ad8\u7684\u5206\u8fa8\u7387\u3002", "motivation": "\u51b7\u51bb\u7535\u955c\u6570\u636e\u96c6\u89c4\u6a21\u5e9e\u5927\uff0c\u4f20\u7edf\u5085\u91cc\u53f6\u7a7a\u95f4\u65b9\u6cd5\u6548\u7387\u9ad8\u4f46\u4fdd\u771f\u5ea6\u4f4e\uff0c\u800c\u57fa\u4e8e\u795e\u7ecf\u8f90\u5c04\u573a\u7684\u5b9e\u7a7a\u95f4\u65b9\u6cd5\u7cbe\u5ea6\u9ad8\u4f46\u5185\u5b58\u548c\u8ba1\u7b97\u5f00\u9500\u5927\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u53c8\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u75283D\u9ad8\u65af\u6e85\u5c04\u8868\u793a\u86cb\u767d\u8d28\uff0c\u6bcf\u4e2a\u9ad8\u65af\u4ec5\u752811\u4e2a\u53c2\u6570\u53c2\u6570\u5316\u3002\u8bbe\u8ba1\u4e86\u65b0\u9896\u7684\u68af\u5ea6\u8ba1\u7b97\u6765\u4f18\u5316\u6bcf\u4e2a\u4f53\u7d20\u8d21\u732e\u76843D\u9ad8\u65af\uff0c\u5927\u5e45\u51cf\u5c11\u4e86\u5185\u5b58\u5360\u7528\u548c\u8bad\u7ec3\u6210\u672c\u3002", "result": "\u5728\u6807\u51c6\u51b7\u51bb\u7535\u955c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGEM\u5b9e\u73b0\u4e86\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5feb48%\u7684\u8bad\u7ec3\u901f\u5ea6\uff0c\u5185\u5b58\u4f7f\u7528\u964d\u4f4e12%\uff0c\u5c40\u90e8\u5206\u8fa8\u7387\u63d0\u5347\u9ad8\u8fbe38.8%\u3002", "conclusion": "GEM\u4e3a\u51b7\u51bb\u7535\u955c\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u8303\u5f0f\uff0c\u7edf\u4e00\u4e86\u901f\u5ea6\u3001\u6548\u7387\u548c\u9ad8\u5206\u8fa8\u7387\u7cbe\u5ea6\u3002"}}
{"id": "2509.25122", "pdf": "https://arxiv.org/pdf/2509.25122", "abs": "https://arxiv.org/abs/2509.25122", "authors": ["Jan Held", "Renaud Vandeghen", "Sanghyun Son", "Daniel Rebain", "Matheus Gadelha", "Yi Zhou", "Ming C. Lin", "Marc Van Droogenbroeck", "Andrea Tagliasacchi"], "title": "Triangle Splatting+: Differentiable Rendering with Opaque Triangles", "categories": ["cs.CV"], "comment": "9 pages, 6 figures, 2 tables", "summary": "Reconstructing 3D scenes and synthesizing novel views has seen rapid progress in recent years. Neural Radiance Fields demonstrated that continuous volumetric radiance fields can achieve high-quality image synthesis, but their long training and rendering times limit practicality. 3D Gaussian Splatting (3DGS) addressed these issues by representing scenes with millions of Gaussians, enabling real-time rendering and fast optimization. However, Gaussian primitives are not natively compatible with the mesh-based pipelines used in VR headsets, and real-time graphics applications. Existing solutions attempt to convert Gaussians into meshes through post-processing or two-stage pipelines, which increases complexity and degrades visual quality. In this work, we introduce Triangle Splatting+, which directly optimizes triangles, the fundamental primitive of computer graphics, within a differentiable splatting framework. We formulate triangle parametrization to enable connectivity through shared vertices, and we design a training strategy that enforces opaque triangles. The final output is immediately usable in standard graphics engines without post-processing. Experiments on the Mip-NeRF360 and Tanks & Temples datasets show that Triangle Splatting+achieves state-of-the-art performance in mesh-based novel view synthesis. Our method surpasses prior splatting approaches in visual fidelity while remaining efficient and fast to training. Moreover, the resulting semi-connected meshes support downstream applications such as physics-based simulation or interactive walkthroughs. The project page is https://trianglesplatting2.github.io/trianglesplatting2/.", "AI": {"tldr": "Triangle Splatting+ \u662f\u4e00\u79cd\u76f4\u63a5\u4f18\u5316\u4e09\u89d2\u5f62\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u5149\u6805\u5316\u6846\u67b6\u5b9e\u73b0\u5b9e\u65f6\u6e32\u67d3\uff0c\u65e0\u9700\u540e\u5904\u7406\u5373\u53ef\u5728\u6807\u51c6\u56fe\u5f62\u5f15\u64ce\u4e2d\u4f7f\u7528\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u59823D\u9ad8\u65af\u6cfc\u6e85\u867d\u7136\u80fd\u5b9e\u73b0\u5b9e\u65f6\u6e32\u67d3\uff0c\u4f46\u9ad8\u65af\u57fa\u5143\u4e0e\u57fa\u4e8e\u7f51\u683c\u7684VR\u5934\u663e\u548c\u56fe\u5f62\u5e94\u7528\u4e0d\u517c\u5bb9\uff0c\u73b0\u6709\u8f6c\u6362\u65b9\u6cd5\u4f1a\u589e\u52a0\u590d\u6742\u5ea6\u5e76\u964d\u4f4e\u89c6\u89c9\u8d28\u91cf\u3002", "method": "\u5728\u53ef\u5fae\u5206\u6cfc\u6e85\u6846\u67b6\u4e2d\u76f4\u63a5\u4f18\u5316\u4e09\u89d2\u5f62\uff0c\u8bbe\u8ba1\u4e09\u89d2\u5f62\u53c2\u6570\u5316\u4ee5\u652f\u6301\u5171\u4eab\u9876\u70b9\u8fde\u63a5\uff0c\u5e76\u91c7\u7528\u8bad\u7ec3\u7b56\u7565\u5f3a\u5236\u4e09\u89d2\u5f62\u4e0d\u900f\u660e\u3002", "result": "\u5728Mip-NeRF360\u548cTanks & Temples\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u57fa\u4e8e\u7f51\u683c\u7684\u65b0\u89c6\u89d2\u5408\u6210\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u89c6\u89c9\u4fdd\u771f\u5ea6\u4f18\u4e8e\u73b0\u6709\u6cfc\u6e85\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u534a\u8fde\u63a5\u7f51\u683c\u53ef\u76f4\u63a5\u7528\u4e8e\u6807\u51c6\u56fe\u5f62\u5f15\u64ce\uff0c\u5e76\u652f\u6301\u7269\u7406\u6a21\u62df\u548c\u4ea4\u4e92\u5f0f\u6f2b\u6e38\u7b49\u4e0b\u6e38\u5e94\u7528\uff0c\u8bad\u7ec3\u9ad8\u6548\u5feb\u901f\u3002"}}
{"id": "2509.25127", "pdf": "https://arxiv.org/pdf/2509.25127", "abs": "https://arxiv.org/abs/2509.25127", "authors": ["Mingyuan Zhou", "Yi Gu", "Huangjie Zheng", "Liangchen Song", "Guande He", "Yizhe Zhang", "Wenze Hu", "Yinfei Yang"], "title": "Score Distillation of Flow Matching Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Diffusion models achieve high-quality image generation but are limited by slow iterative sampling. Distillation methods alleviate this by enabling one- or few-step generation. Flow matching, originally introduced as a distinct framework, has since been shown to be theoretically equivalent to diffusion under Gaussian assumptions, raising the question of whether distillation techniques such as score distillation transfer directly. We provide a simple derivation -- based on Bayes' rule and conditional expectations -- that unifies Gaussian diffusion and flow matching without relying on ODE/SDE formulations. Building on this view, we extend Score identity Distillation (SiD) to pretrained text-to-image flow-matching models, including SANA, SD3-Medium, SD3.5-Medium/Large, and FLUX.1-dev, all with DiT backbones. Experiments show that, with only modest flow-matching- and DiT-specific adjustments, SiD works out of the box across these models, in both data-free and data-aided settings, without requiring teacher finetuning or architectural changes. This provides the first systematic evidence that score distillation applies broadly to text-to-image flow matching models, resolving prior concerns about stability and soundness and unifying acceleration techniques across diffusion- and flow-based generators. We will make the PyTorch implementation publicly available.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u6269\u6563\u6a21\u578b\u4e0e\u6d41\u5339\u914d\u5728\u7406\u8bba\u4e0a\u7684\u7b49\u4ef7\u6027\uff0c\u5e76\u5c06\u5206\u6570\u84b8\u998f\u6280\u672f\u6269\u5c55\u5230\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6d41\u5339\u914d\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u6559\u5e08\u5fae\u8c03\u6216\u67b6\u6784\u4fee\u6539\u7684\u52a0\u901f\u751f\u6210\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u751f\u6210\u8d28\u91cf\u9ad8\u4f46\u91c7\u6837\u901f\u5ea6\u6162\uff0c\u800c\u6d41\u5339\u914d\u4e0e\u6269\u6563\u6a21\u578b\u5728\u7406\u8bba\u4e0a\u7b49\u4ef7\uff0c\u4f46\u4e4b\u524d\u4e0d\u786e\u5b9a\u84b8\u998f\u6280\u672f\u662f\u5426\u53ef\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u6d41\u5339\u914d\u6a21\u578b\u3002", "method": "\u57fa\u4e8e\u8d1d\u53f6\u65af\u89c4\u5219\u548c\u6761\u4ef6\u671f\u671b\u7684\u7b80\u5355\u63a8\u5bfc\u7edf\u4e00\u4e86\u9ad8\u65af\u6269\u6563\u548c\u6d41\u5339\u914d\uff0c\u5e76\u5c06\u5206\u6570\u8eab\u4efd\u84b8\u998f\u6269\u5c55\u5230\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6d41\u5339\u914d\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u53ea\u9700\u9002\u5ea6\u7684\u6d41\u5339\u914d\u548cDiT\u7279\u5b9a\u8c03\u6574\uff0cSiD\u5c31\u80fd\u5728\u8fd9\u4e9b\u6a21\u578b\u4e0a\u76f4\u63a5\u5de5\u4f5c\uff0c\u5728\u65e0\u6570\u636e\u548c\u6709\u6570\u636e\u8bbe\u7f6e\u4e0b\u90fd\u6709\u6548\u3002", "conclusion": "\u5206\u6570\u84b8\u998f\u6280\u672f\u5e7f\u6cdb\u9002\u7528\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u6d41\u5339\u914d\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4e4b\u524d\u5173\u4e8e\u7a33\u5b9a\u6027\u548c\u5408\u7406\u6027\u7684\u62c5\u5fe7\uff0c\u7edf\u4e00\u4e86\u6269\u6563\u548c\u6d41\u57fa\u751f\u6210\u5668\u7684\u52a0\u901f\u6280\u672f\u3002"}}
{"id": "2509.25161", "pdf": "https://arxiv.org/pdf/2509.25161", "abs": "https://arxiv.org/abs/2509.25161", "authors": ["Kunhao Liu", "Wenbo Hu", "Jiale Xu", "Ying Shan", "Shijian Lu"], "title": "Rolling Forcing: Autoregressive Long Video Diffusion in Real Time", "categories": ["cs.CV"], "comment": "Project page: https://kunhao-liu.github.io/Rolling_Forcing_Webpage/", "summary": "Streaming video generation, as one fundamental component in interactive world models and neural game engines, aims to generate high-quality, low-latency, and temporally coherent long video streams. However, most existing work suffers from severe error accumulation that often significantly degrades the generated stream videos over long horizons. We design Rolling Forcing, a novel video generation technique that enables streaming long videos with minimal error accumulation. Rolling Forcing comes with three novel designs. First, instead of iteratively sampling individual frames, which accelerates error propagation, we design a joint denoising scheme that simultaneously denoises multiple frames with progressively increasing noise levels. This design relaxes the strict causality across adjacent frames, effectively suppressing error growth. Second, we introduce the attention sink mechanism into the long-horizon stream video generation task, which allows the model to keep key value states of initial frames as a global context anchor and thereby enhances long-term global consistency. Third, we design an efficient training algorithm that enables few-step distillation over largely extended denoising windows. This algorithm operates on non-overlapping windows and mitigates exposure bias conditioned on self-generated histories. Extensive experiments show that Rolling Forcing enables real-time streaming generation of multi-minute videos on a single GPU, with substantially reduced error accumulation.", "AI": {"tldr": "\u63d0\u51faRolling Forcing\u6280\u672f\uff0c\u901a\u8fc7\u8054\u5408\u53bb\u566a\u3001\u6ce8\u610f\u529b\u4e0b\u6c89\u548c\u9ad8\u6548\u8bad\u7ec3\u7b97\u6cd5\uff0c\u89e3\u51b3\u6d41\u5f0f\u89c6\u9891\u751f\u6210\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u5b9e\u73b0\u5355GPU\u5b9e\u65f6\u751f\u6210\u957f\u8fbe\u6570\u5206\u949f\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u3002", "motivation": "\u73b0\u6709\u6d41\u5f0f\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u5bfc\u81f4\u957f\u65f6\u57df\u89c6\u9891\u8d28\u91cf\u663e\u8457\u4e0b\u964d\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6700\u5c0f\u5316\u8bef\u5dee\u7d2f\u79ef\u7684\u6280\u672f\u3002", "method": "1. \u8054\u5408\u53bb\u566a\u65b9\u6848\uff1a\u540c\u65f6\u53bb\u566a\u591a\u5e27\uff0c\u51cf\u5c11\u76f8\u90bb\u5e27\u95f4\u7684\u4e25\u683c\u56e0\u679c\u4f9d\u8d56\uff1b2. \u6ce8\u610f\u529b\u4e0b\u6c89\u673a\u5236\uff1a\u4fdd\u7559\u521d\u59cb\u5e27\u4f5c\u4e3a\u5168\u5c40\u4e0a\u4e0b\u6587\u951a\u70b9\uff1b3. \u9ad8\u6548\u8bad\u7ec3\u7b97\u6cd5\uff1a\u5728\u6269\u5c55\u7684\u53bb\u566a\u7a97\u53e3\u4e0a\u8fdb\u884c\u5c11\u6b65\u84b8\u998f\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u5728\u5355GPU\u4e0a\u5b9e\u65f6\u751f\u6210\u957f\u8fbe\u6570\u5206\u949f\u7684\u89c6\u9891\uff0c\u663e\u8457\u51cf\u5c11\u8bef\u5dee\u7d2f\u79ef\u3002", "conclusion": "Rolling Forcing\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86\u6d41\u5f0f\u89c6\u9891\u751f\u6210\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u4e3a\u4ea4\u4e92\u5f0f\u4e16\u754c\u6a21\u578b\u548c\u795e\u7ecf\u6e38\u620f\u5f15\u64ce\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u3001\u4f4e\u5ef6\u8fdf\u7684\u957f\u89c6\u9891\u6d41\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2509.25162", "pdf": "https://arxiv.org/pdf/2509.25162", "abs": "https://arxiv.org/abs/2509.25162", "authors": ["Bowei Chen", "Sai Bi", "Hao Tan", "He Zhang", "Tianyuan Zhang", "Zhengqi Li", "Yuanjun Xiong", "Jianming Zhang", "Kai Zhang"], "title": "Aligning Visual Foundation Encoders to Tokenizers for Diffusion Models", "categories": ["cs.CV"], "comment": "Project Page: https://aligntok.github.io/", "summary": "In this work, we propose aligning pretrained visual encoders to serve as tokenizers for latent diffusion models in image generation. Unlike training a variational autoencoder (VAE) from scratch, which primarily emphasizes low-level details, our approach leverages the rich semantic structure of foundation encoders. We introduce a three-stage alignment strategy: (1) freeze the encoder and train an adapter and a decoder to establish a semantic latent space; (2) jointly optimize all components with an additional semantic preservation loss, enabling the encoder to capture perceptual details while retaining high-level semantics; and (3) refine the decoder for improved reconstruction quality. This alignment yields semantically rich image tokenizers that benefit diffusion models. On ImageNet 256$\\times$256, our tokenizer accelerates the convergence of diffusion models, reaching a gFID of 1.90 within just 64 epochs, and improves generation both with and without classifier-free guidance. Scaling to LAION, a 2B-parameter text-to-image model trained with our tokenizer consistently outperforms FLUX VAE under the same training steps. Overall, our method is simple, scalable, and establishes a semantically grounded paradigm for continuous tokenizer design.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5c06\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u5bf9\u9f50\u4f5c\u4e3a\u6f5c\u5728\u6269\u6563\u6a21\u578b\u6807\u8bb0\u5668\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u5bf9\u9f50\u7b56\u7565\u83b7\u5f97\u8bed\u4e49\u4e30\u5bcc\u7684\u56fe\u50cf\u6807\u8bb0\u5668\uff0c\u52a0\u901f\u6269\u6563\u6a21\u578b\u6536\u655b\u5e76\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u4ece\u5934\u8bad\u7ec3VAE\u4e3b\u8981\u5173\u6ce8\u4f4e\u7ea7\u7ec6\u8282\uff0c\u800c\u672c\u65b9\u6cd5\u5229\u7528\u57fa\u7840\u7f16\u7801\u5668\u7684\u4e30\u5bcc\u8bed\u4e49\u7ed3\u6784\uff0c\u65e8\u5728\u5efa\u7acb\u8bed\u4e49\u57fa\u7840\u7684\u8fde\u7eed\u6807\u8bb0\u5668\u8bbe\u8ba1\u8303\u5f0f\u3002", "method": "\u4e09\u9636\u6bb5\u5bf9\u9f50\u7b56\u7565\uff1a1)\u51bb\u7ed3\u7f16\u7801\u5668\uff0c\u8bad\u7ec3\u9002\u914d\u5668\u548c\u89e3\u7801\u5668\u5efa\u7acb\u8bed\u4e49\u6f5c\u5728\u7a7a\u95f4\uff1b2)\u8054\u5408\u4f18\u5316\u6240\u6709\u7ec4\u4ef6\u5e76\u6dfb\u52a0\u8bed\u4e49\u4fdd\u6301\u635f\u5931\uff1b3)\u7cbe\u70bc\u89e3\u7801\u5668\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "result": "\u5728ImageNet 256\u00d7256\u4e0a\uff0c\u6807\u8bb0\u5668\u52a0\u901f\u6269\u6563\u6a21\u578b\u6536\u655b\uff0c64\u4e2a\u5468\u671f\u5185\u8fbe\u5230gFID 1.90\uff1b\u5728LAION\u4e0a\uff0c2B\u53c2\u6570\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u76f8\u540c\u8bad\u7ec3\u6b65\u9aa4\u4e0b\u6301\u7eed\u4f18\u4e8eFLUX VAE\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u3001\u53ef\u6269\u5c55\uff0c\u4e3a\u8fde\u7eed\u6807\u8bb0\u5668\u8bbe\u8ba1\u5efa\u7acb\u4e86\u8bed\u4e49\u57fa\u7840\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2509.25178", "pdf": "https://arxiv.org/pdf/2509.25178", "abs": "https://arxiv.org/abs/2509.25178", "authors": ["Aryan Yazdan Parast", "Parsa Hosseini", "Hesam Asadollahzadeh", "Arshia Soltani Moakhar", "Basim Azam", "Soheil Feizi", "Naveed Akhtar"], "title": "GHOST: Hallucination-Inducing Image Generation for Multimodal LLMs", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Object hallucination in Multimodal Large Language Models (MLLMs) is a persistent failure mode that causes the model to perceive objects absent in the image. This weakness of MLLMs is currently studied using static benchmarks with fixed visual scenarios, which preempts the possibility of uncovering model-specific or unanticipated hallucination vulnerabilities. We introduce GHOST (Generating Hallucinations via Optimizing Stealth Tokens), a method designed to stress-test MLLMs by actively generating images that induce hallucination. GHOST is fully automatic and requires no human supervision or prior knowledge. It operates by optimizing in the image embedding space to mislead the model while keeping the target object absent, and then guiding a diffusion model conditioned on the embedding to generate natural-looking images. The resulting images remain visually natural and close to the original input, yet introduce subtle misleading cues that cause the model to hallucinate. We evaluate our method across a range of models, including reasoning models like GLM-4.1V-Thinking, and achieve a hallucination success rate exceeding 28%, compared to around 1% in prior data-driven discovery methods. We confirm that the generated images are both high-quality and object-free through quantitative metrics and human evaluation. Also, GHOST uncovers transferable vulnerabilities: images optimized for Qwen2.5-VL induce hallucinations in GPT-4o at a 66.5% rate. Finally, we show that fine-tuning on our images mitigates hallucination, positioning GHOST as both a diagnostic and corrective tool for building more reliable multimodal systems.", "AI": {"tldr": "GHOST\u662f\u4e00\u79cd\u81ea\u52a8\u751f\u6210\u8bf1\u5bfc\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4ea7\u751f\u7269\u4f53\u5e7b\u89c9\u7684\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u56fe\u50cf\u5d4c\u5165\u7a7a\u95f4\u6765\u8bef\u5bfc\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u76ee\u6807\u5bf9\u8c61\u5b9e\u9645\u4e0d\u5b58\u5728\uff0c\u4ece\u800c\u63ed\u793a\u6a21\u578b\u7684\u5e7b\u89c9\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\uff0c\u4f46\u5f53\u524d\u7814\u7a76\u4f7f\u7528\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e0\u6cd5\u53d1\u73b0\u6a21\u578b\u7279\u5b9a\u6216\u672a\u9884\u6599\u5230\u7684\u5e7b\u89c9\u6f0f\u6d1e\u3002\u9700\u8981\u4e00\u79cd\u4e3b\u52a8\u6d4b\u8bd5\u65b9\u6cd5\u6765\u7cfb\u7edf\u6027\u5730\u53d1\u73b0\u8fd9\u4e9b\u5f31\u70b9\u3002", "method": "GHOST\u65b9\u6cd5\u5728\u56fe\u50cf\u5d4c\u5165\u7a7a\u95f4\u8fdb\u884c\u4f18\u5316\uff0c\u8bef\u5bfc\u6a21\u578b\u611f\u77e5\u4e0d\u5b58\u5728\u7684\u5bf9\u8c61\uff0c\u7136\u540e\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u57fa\u4e8e\u4f18\u5316\u540e\u7684\u5d4c\u5165\u751f\u6210\u81ea\u7136\u56fe\u50cf\u3002\u6574\u4e2a\u8fc7\u7a0b\u5b8c\u5168\u81ea\u52a8\uff0c\u65e0\u9700\u4eba\u5de5\u76d1\u7763\u6216\u5148\u9a8c\u77e5\u8bc6\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5305\u62ecGLM-4.1V-Thinking\u5728\u5185\u7684\u591a\u4e2a\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc728%\u7684\u5e7b\u89c9\u6210\u529f\u7387\uff0c\u8fdc\u9ad8\u4e8e\u4e4b\u524d\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u7ea61%\u3002\u751f\u6210\u7684\u56fe\u50cf\u8d28\u91cf\u9ad8\u4e14\u786e\u5b9e\u4e0d\u5305\u542b\u76ee\u6807\u5bf9\u8c61\uff0c\u8fd8\u80fd\u5728\u4e0d\u540c\u6a21\u578b\u95f4\u8f6c\u79fb\u6f0f\u6d1e\u3002", "conclusion": "GHOST\u4e0d\u4ec5\u662f\u4e00\u79cd\u8bca\u65ad\u5de5\u5177\uff0c\u8fd8\u80fd\u901a\u8fc7\u5728\u5176\u751f\u6210\u7684\u56fe\u50cf\u4e0a\u8fdb\u884c\u5fae\u8c03\u6765\u7f13\u89e3\u5e7b\u89c9\u95ee\u9898\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u8bca\u65ad\u548c\u7ea0\u6b63\u624b\u6bb5\u3002"}}
{"id": "2509.25180", "pdf": "https://arxiv.org/pdf/2509.25180", "abs": "https://arxiv.org/abs/2509.25180", "authors": ["Wenkun He", "Yuchao Gu", "Junyu Chen", "Dongyun Zou", "Yujun Lin", "Zhekai Zhang", "Haocheng Xi", "Muyang Li", "Ligeng Zhu", "Jincheng Yu", "Junsong Chen", "Enze Xie", "Song Han", "Han Cai"], "title": "DC-Gen: Post-Training Diffusion Acceleration with Deeply Compressed Latent Space", "categories": ["cs.CV", "cs.AI"], "comment": "Tech Report. The first three authors contributed equally to this work", "summary": "Existing text-to-image diffusion models excel at generating high-quality images, but face significant efficiency challenges when scaled to high resolutions, like 4K image generation. While previous research accelerates diffusion models in various aspects, it seldom handles the inherent redundancy within the latent space. To bridge this gap, this paper introduces DC-Gen, a general framework that accelerates text-to-image diffusion models by leveraging a deeply compressed latent space. Rather than a costly training-from-scratch approach, DC-Gen uses an efficient post-training pipeline to preserve the quality of the base model. A key challenge in this paradigm is the representation gap between the base model's latent space and a deeply compressed latent space, which can lead to instability during direct fine-tuning. To overcome this, DC-Gen first bridges the representation gap with a lightweight embedding alignment training. Once the latent embeddings are aligned, only a small amount of LoRA fine-tuning is needed to unlock the base model's inherent generation quality. We verify DC-Gen's effectiveness on SANA and FLUX.1-Krea. The resulting DC-Gen-SANA and DC-Gen-FLUX models achieve quality comparable to their base models but with a significant speedup. Specifically, DC-Gen-FLUX reduces the latency of 4K image generation by 53x on the NVIDIA H100 GPU. When combined with NVFP4 SVDQuant, DC-Gen-FLUX generates a 4K image in just 3.5 seconds on a single NVIDIA 5090 GPU, achieving a total latency reduction of 138x compared to the base FLUX.1-Krea model. Code: https://github.com/dc-ai-projects/DC-Gen.", "AI": {"tldr": "DC-Gen\u662f\u4e00\u4e2a\u901a\u8fc7\u6df1\u5ea6\u538b\u7f29\u6f5c\u5728\u7a7a\u95f4\u6765\u52a0\u901f\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u901a\u7528\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u57fa\u7840\u6a21\u578b\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u751f\u6210\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u9ad8\u5206\u8fa8\u7387\uff08\u59824K\uff09\u751f\u6210\u65f6\u9762\u4e34\u6548\u7387\u6311\u6218\uff0c\u800c\u73b0\u6709\u7814\u7a76\u5f88\u5c11\u5904\u7406\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u56fa\u6709\u5197\u4f59\u95ee\u9898\u3002", "method": "\u91c7\u7528\u540e\u8bad\u7ec3\u7ba1\u9053\uff0c\u9996\u5148\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5d4c\u5165\u5bf9\u9f50\u8bad\u7ec3\u5f25\u5408\u57fa\u7840\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u4e0e\u6df1\u5ea6\u538b\u7f29\u6f5c\u5728\u7a7a\u95f4\u4e4b\u95f4\u7684\u8868\u793a\u5dee\u8ddd\uff0c\u7136\u540e\u4ec5\u9700\u5c11\u91cfLoRA\u5fae\u8c03\u5373\u53ef\u6062\u590d\u751f\u6210\u8d28\u91cf\u3002", "result": "\u5728SANA\u548cFLUX.1-Krea\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0cDC-Gen-FLUX\u5728NVIDIA H100 GPU\u4e0a\u5b9e\u73b04K\u56fe\u50cf\u751f\u6210\u5ef6\u8fdf\u964d\u4f4e53\u500d\uff0c\u7ed3\u5408NVFP4 SVDQuant\u540e\u53ef\u5728\u5355\u5f20NVIDIA 5090 GPU\u4e0a3.5\u79d2\u751f\u62104K\u56fe\u50cf\uff0c\u603b\u5ef6\u8fdf\u964d\u4f4e138\u500d\u3002", "conclusion": "DC-Gen\u6846\u67b6\u80fd\u591f\u5728\u4e0d\u727a\u7272\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u663e\u8457\u52a0\u901f\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0c\u4e3a\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25182", "pdf": "https://arxiv.org/pdf/2509.25182", "abs": "https://arxiv.org/abs/2509.25182", "authors": ["Junyu Chen", "Wenkun He", "Yuchao Gu", "Yuyang Zhao", "Jincheng Yu", "Junsong Chen", "Dongyun Zou", "Yujun Lin", "Zhekai Zhang", "Muyang Li", "Haocheng Xi", "Ligeng Zhu", "Enze Xie", "Song Han", "Han Cai"], "title": "DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder", "categories": ["cs.CV", "cs.AI"], "comment": "Tech Report. The first three authors contributed equally to this work", "summary": "We introduce DC-VideoGen, a post-training acceleration framework for efficient video generation. DC-VideoGen can be applied to any pre-trained video diffusion model, improving efficiency by adapting it to a deep compression latent space with lightweight fine-tuning. The framework builds on two key innovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal temporal design that achieves 32x/64x spatial and 4x temporal compression while preserving reconstruction quality and generalization to longer videos; and (ii) AE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer of pre-trained models into the new latent space. Adapting the pre-trained Wan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100 GPU. The accelerated models achieve up to 14.8x lower inference latency than their base counterparts without compromising quality, and further enable 2160x3840 video generation on a single GPU. Code: https://github.com/dc-ai-projects/DC-VideoGen.", "AI": {"tldr": "DC-VideoGen\u662f\u4e00\u4e2a\u540e\u8bad\u7ec3\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u538b\u7f29\u6f5c\u5728\u7a7a\u95f4\u548c\u8f7b\u91cf\u7ea7\u5fae\u8c03\uff0c\u53ef\u5c06\u4efb\u4f55\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e14.8\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u63a8\u7406\u5ef6\u8fdf\u5927\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u52a0\u901f\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "method": "1) \u63d0\u51fa\u6df1\u5ea6\u538b\u7f29\u89c6\u9891\u81ea\u7f16\u7801\u5668\uff0c\u91c7\u7528\u5757\u56e0\u679c\u65f6\u5e8f\u8bbe\u8ba1\uff0c\u5b9e\u73b032x/64x\u7a7a\u95f4\u548c4x\u65f6\u95f4\u538b\u7f29\uff1b2) \u5f00\u53d1AE-Adapt-V\u9002\u5e94\u7b56\u7565\uff0c\u5feb\u901f\u7a33\u5b9a\u5730\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u8fc1\u79fb\u5230\u65b0\u6f5c\u5728\u7a7a\u95f4\u3002", "result": "\u4ec5\u970010\u4e2aH100 GPU\u5929\u5373\u53ef\u5b8c\u6210\u6a21\u578b\u9002\u5e94\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe14.8\u500d\uff0c\u53ef\u5728\u5355GPU\u4e0a\u751f\u62102160x3840\u5206\u8fa8\u7387\u89c6\u9891\uff0c\u4e14\u4e0d\u635f\u5931\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "DC-VideoGen\u4e3a\u89c6\u9891\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u540e\u8bad\u7ec3\u52a0\u901f\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.25187", "pdf": "https://arxiv.org/pdf/2509.25187", "abs": "https://arxiv.org/abs/2509.25187", "authors": ["Yunyang Ge", "Xinhua Cheng", "Chengshu Zhao", "Xianyi He", "Shenghai Yuan", "Bin Lin", "Bin Zhu", "Li Yuan"], "title": "FlashI2V: Fourier-Guided Latent Shifting Prevents Conditional Image Leakage in Image-to-Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "In Image-to-Video (I2V) generation, a video is created using an input image as the first-frame condition. Existing I2V methods concatenate the full information of the conditional image with noisy latents to achieve high fidelity. However, the denoisers in these methods tend to shortcut the conditional image, which is known as conditional image leakage, leading to performance degradation issues such as slow motion and color inconsistency. In this work, we further clarify that conditional image leakage leads to overfitting to in-domain data and decreases the performance in out-of-domain scenarios. Moreover, we introduce Fourier-Guided Latent Shifting I2V, named FlashI2V, to prevent conditional image leakage. Concretely, FlashI2V consists of: (1) Latent Shifting. We modify the source and target distributions of flow matching by subtracting the conditional image information from the noisy latents, thereby incorporating the condition implicitly. (2) Fourier Guidance. We use high-frequency magnitude features obtained by the Fourier Transform to accelerate convergence and enable the adjustment of detail levels in the generated video. Experimental results show that our method effectively overcomes conditional image leakage and achieves the best generalization and performance on out-of-domain data among various I2V paradigms. With only 1.3B parameters, FlashI2V achieves a dynamic degree score of 53.01 on Vbench-I2V, surpassing CogVideoX1.5-5B-I2V and Wan2.1-I2V-14B-480P. Github page: https://pku-yuangroup.github.io/FlashI2V/", "AI": {"tldr": "FlashI2V\u901a\u8fc7\u6f5c\u5728\u504f\u79fb\u548c\u5085\u91cc\u53f6\u5f15\u5bfc\u89e3\u51b3I2V\u751f\u6210\u4e2d\u7684\u6761\u4ef6\u56fe\u50cf\u6cc4\u6f0f\u95ee\u9898\uff0c\u5728\u4ec51.3B\u53c2\u6570\u4e0b\u5728Vbench-I2V\u4e0a\u53d6\u5f9753.01\u52a8\u6001\u5ea6\u5206\u6570\uff0c\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\u3002", "motivation": "\u73b0\u6709I2V\u65b9\u6cd5\u5b58\u5728\u6761\u4ef6\u56fe\u50cf\u6cc4\u6f0f\u95ee\u9898\uff0c\u5bfc\u81f4\u8fd0\u52a8\u7f13\u6162\u3001\u989c\u8272\u4e0d\u4e00\u81f4\u7b49\u6027\u80fd\u4e0b\u964d\uff0c\u4e14\u5728\u57df\u5916\u6570\u636e\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faFlashI2V\u65b9\u6cd5\uff1a1\uff09\u6f5c\u5728\u504f\u79fb - \u901a\u8fc7\u4ece\u566a\u58f0\u6f5c\u5728\u4e2d\u51cf\u53bb\u6761\u4ef6\u56fe\u50cf\u4fe1\u606f\u6765\u9690\u5f0f\u6574\u5408\u6761\u4ef6\uff1b2\uff09\u5085\u91cc\u53f6\u5f15\u5bfc - \u4f7f\u7528\u5085\u91cc\u53f6\u53d8\u6362\u7684\u9ad8\u9891\u5e45\u5ea6\u7279\u5f81\u52a0\u901f\u6536\u655b\u5e76\u8c03\u6574\u7ec6\u8282\u7ea7\u522b\u3002", "result": "\u5728Vbench-I2V\u4e0a\u52a8\u6001\u5ea6\u5f97\u520653.01\uff0c\u8d85\u8d8aCogVideoX1.5-5B-I2V\u548cWan2.1-I2V-14B-480P\u7b49\u66f4\u5927\u6a21\u578b\uff0c\u5728\u57df\u5916\u6570\u636e\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "FlashI2V\u6709\u6548\u514b\u670d\u6761\u4ef6\u56fe\u50cf\u6cc4\u6f0f\u95ee\u9898\uff0c\u5728\u57df\u5916\u6570\u636e\u4e0a\u5177\u6709\u6700\u4f73\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\uff0c\u4ec5\u97001.3B\u53c2\u6570\u5373\u53ef\u5b9e\u73b0\u4f18\u8d8a\u7ed3\u679c\u3002"}}
{"id": "2509.25191", "pdf": "https://arxiv.org/pdf/2509.25191", "abs": "https://arxiv.org/abs/2509.25191", "authors": ["Yang Liu", "Chuanchen Luo", "Zimo Tang", "Junran Peng", "Zhaoxiang Zhang"], "title": "VGGT-X: When VGGT Meets Dense Novel View Synthesis", "categories": ["cs.CV"], "comment": "Project Page: https://dekuliutesla.github.io/vggt-x.github.io/", "summary": "We study the problem of applying 3D Foundation Models (3DFMs) to dense Novel View Synthesis (NVS). Despite significant progress in Novel View Synthesis powered by NeRF and 3DGS, current approaches remain reliant on accurate 3D attributes (e.g., camera poses and point clouds) acquired from Structure-from-Motion (SfM), which is often slow and fragile in low-texture or low-overlap captures. Recent 3DFMs showcase orders of magnitude speedup over the traditional pipeline and great potential for online NVS. But most of the validation and conclusions are confined to sparse-view settings. Our study reveals that naively scaling 3DFMs to dense views encounters two fundamental barriers: dramatically increasing VRAM burden and imperfect outputs that degrade initialization-sensitive 3D training. To address these barriers, we introduce VGGT-X, incorporating a memory-efficient VGGT implementation that scales to 1,000+ images, an adaptive global alignment for VGGT output enhancement, and robust 3DGS training practices. Extensive experiments show that these measures substantially close the fidelity gap with COLMAP-initialized pipelines, achieving state-of-the-art results in dense COLMAP-free NVS and pose estimation. Additionally, we analyze the causes of remaining gaps with COLMAP-initialized rendering, providing insights for the future development of 3D foundation models and dense NVS. Our project page is available at https://dekuliutesla.github.io/vggt-x.github.io/", "AI": {"tldr": "VGGT-X\u89e3\u51b3\u4e863D\u57fa\u7840\u6a21\u578b\u5728\u5bc6\u96c6\u65b0\u89c6\u89d2\u5408\u6210\u4e2d\u7684\u5185\u5b58\u548c\u8f93\u51fa\u8d28\u91cf\u95ee\u9898\uff0c\u901a\u8fc7\u5185\u5b58\u9ad8\u6548\u5b9e\u73b0\u3001\u81ea\u9002\u5e94\u5168\u5c40\u5bf9\u9f50\u548c\u9c81\u68d2\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u65e0COLMAP\u521d\u59cb\u5316\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u6700\u5148\u8fdb\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\u4f9d\u8d56SfM\u83b7\u53d63D\u5c5e\u6027\uff0c\u8fc7\u7a0b\u7f13\u6162\u4e14\u8106\u5f31\u30023D\u57fa\u7840\u6a21\u578b\u867d\u6709\u52a0\u901f\u6f5c\u529b\uff0c\u4f46\u5728\u5bc6\u96c6\u89c6\u56fe\u4e0b\u5b58\u5728\u5185\u5b58\u8d1f\u62c5\u5927\u548c\u8f93\u51fa\u4e0d\u5b8c\u5584\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faVGGT-X\uff0c\u5305\u542b\u5185\u5b58\u9ad8\u6548\u7684VGGT\u5b9e\u73b0\uff08\u652f\u63011000+\u56fe\u50cf\uff09\u3001\u81ea\u9002\u5e94\u5168\u5c40\u5bf9\u9f50\u589e\u5f3aVGGT\u8f93\u51fa\u3001\u9c81\u68d23DGS\u8bad\u7ec3\u5b9e\u8df5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8fd9\u4e9b\u65b9\u6cd5\u663e\u8457\u7f29\u5c0f\u4e86\u4e0eCOLMAP\u521d\u59cb\u5316\u7ba1\u7ebf\u7684\u8d28\u91cf\u5dee\u8ddd\uff0c\u5728\u5bc6\u96c6\u65e0COLMAP\u65b0\u89c6\u89d2\u5408\u6210\u548c\u59ff\u6001\u4f30\u8ba1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7ed3\u679c\u3002", "conclusion": "VGGT-X\u4e3a3D\u57fa\u7840\u6a21\u578b\u548c\u5bc6\u96c6\u65b0\u89c6\u89d2\u5408\u6210\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5206\u6790\u4e86\u4e0eCOLMAP\u521d\u59cb\u5316\u6e32\u67d3\u7684\u5269\u4f59\u5dee\u8ddd\u539f\u56e0\u3002"}}
{"id": "2509.22710", "pdf": "https://arxiv.org/pdf/2509.22710", "abs": "https://arxiv.org/abs/2509.22710", "authors": ["Pavan Reddy", "Aditya Sanjay Gujral"], "title": "Localizing Adversarial Attacks To Produces More Imperceptible Noise", "categories": ["cs.LG", "cs.AI", "cs.CV", "I.2.6; I.2.10; I.5.1"], "comment": "Published, CC BY-NC 4.0; includes 2 figures and 1 table;   InceptionV3/ImageNet evaluation", "summary": "Adversarial attacks in machine learning traditionally focus on global perturbations to input data, yet the potential of localized adversarial noise remains underexplored. This study systematically evaluates localized adversarial attacks across widely-used methods, including FGSM, PGD, and C&W, to quantify their effectiveness, imperceptibility, and computational efficiency. By introducing a binary mask to constrain noise to specific regions, localized attacks achieve significantly lower mean pixel perturbations, higher Peak Signal-to-Noise Ratios (PSNR), and improved Structural Similarity Index (SSIM) compared to global attacks. However, these benefits come at the cost of increased computational effort and a modest reduction in Attack Success Rate (ASR). Our results highlight that iterative methods, such as PGD and C&W, are more robust to localization constraints than single-step methods like FGSM, maintaining higher ASR and imperceptibility metrics. This work provides a comprehensive analysis of localized adversarial attacks, offering practical insights for advancing attack strategies and designing robust defensive systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5c40\u90e8\u5bf9\u6297\u653b\u51fb\u7684\u6709\u6548\u6027\u3001\u4e0d\u53ef\u611f\u77e5\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u53d1\u73b0\u5c40\u90e8\u653b\u51fb\u76f8\u6bd4\u5168\u5c40\u653b\u51fb\u5177\u6709\u66f4\u4f4e\u7684\u50cf\u7d20\u6270\u52a8\u548c\u66f4\u9ad8\u7684\u56fe\u50cf\u8d28\u91cf\u6307\u6807\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u66f4\u9ad8\u4e14\u653b\u51fb\u6210\u529f\u7387\u7565\u6709\u4e0b\u964d\u3002", "motivation": "\u4f20\u7edf\u5bf9\u6297\u653b\u51fb\u4e3b\u8981\u5173\u6ce8\u5168\u5c40\u6270\u52a8\uff0c\u800c\u5c40\u90e8\u5bf9\u6297\u566a\u58f0\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u5c40\u90e8\u5bf9\u6297\u653b\u51fb\u7684\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u4e8c\u5143\u63a9\u7801\u5c06\u566a\u58f0\u9650\u5236\u5728\u7279\u5b9a\u533a\u57df\uff0c\u5728FGSM\u3001PGD\u548cC&W\u7b49\u5e7f\u6cdb\u4f7f\u7528\u7684\u65b9\u6cd5\u4e0a\u8bc4\u4f30\u5c40\u90e8\u5bf9\u6297\u653b\u51fb\u3002", "result": "\u5c40\u90e8\u653b\u51fb\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u4f4e\u7684\u5e73\u5747\u50cf\u7d20\u6270\u52a8\u3001\u66f4\u9ad8\u7684PSNR\u548cSSIM\uff0c\u4f46\u8ba1\u7b97\u5de5\u4f5c\u91cf\u589e\u52a0\u4e14\u653b\u51fb\u6210\u529f\u7387\u7565\u6709\u964d\u4f4e\u3002\u8fed\u4ee3\u65b9\u6cd5(PGD\u3001C&W)\u6bd4\u5355\u6b65\u65b9\u6cd5(FGSM)\u5bf9\u5c40\u90e8\u5316\u7ea6\u675f\u66f4\u5177\u9c81\u68d2\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u5c40\u90e8\u5bf9\u6297\u653b\u51fb\u7684\u5168\u9762\u5206\u6790\uff0c\u4e3a\u6539\u8fdb\u653b\u51fb\u7b56\u7565\u548c\u8bbe\u8ba1\u9c81\u68d2\u9632\u5fa1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2509.22736", "pdf": "https://arxiv.org/pdf/2509.22736", "abs": "https://arxiv.org/abs/2509.22736", "authors": ["Merve G\u00fclle", "Junno Yun", "Ya\u015far Utku Al\u00e7alar", "Mehmet Ak\u00e7akaya"], "title": "Consistency Models as Plug-and-Play Priors for Inverse Problems", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "physics.med-ph", "stat.ML"], "comment": null, "summary": "Diffusion models have found extensive use in solving numerous inverse problems. Such diffusion inverse problem solvers aim to sample from the posterior distribution of data given the measurements, using a combination of the unconditional score function and an approximation of the posterior related to the forward process. Recently, consistency models (CMs) have been proposed to directly predict the final output from any point on the diffusion ODE trajectory, enabling high-quality sampling in just a few NFEs. CMs have also been utilized for inverse problems, but existing CM-based solvers either require additional task-specific training or utilize data fidelity operations with slow convergence, not amenable to large-scale problems. In this work, we reinterpret CMs as proximal operators of a prior, enabling their integration into plug-and-play (PnP) frameworks. We propose a solver based on PnP-ADMM, which enables us to leverage the fast convergence of conjugate gradient method. We further accelerate this with noise injection and momentum, dubbed PnP-CM, and show it maintains the convergence properties of the baseline PnP-ADMM. We evaluate our approach on a variety of inverse problems, including inpainting, super-resolution, Gaussian deblurring, and magnetic resonance imaging (MRI) reconstruction. To the best of our knowledge, this is the first CM trained for MRI datasets. Our results show that PnP-CM achieves high-quality reconstructions in as few as 4 NFEs, and can produce meaningful results in 2 steps, highlighting its effectiveness in real-world inverse problems while outperforming comparable CM-based approaches.", "AI": {"tldr": "\u63d0\u51faPnP-CM\u65b9\u6cd5\uff0c\u5c06\u4e00\u81f4\u6027\u6a21\u578b\u91cd\u65b0\u89e3\u91ca\u4e3a\u5148\u9a8c\u7684\u8fd1\u7aef\u7b97\u5b50\uff0c\u96c6\u6210\u5230PnP-ADMM\u6846\u67b6\u4e2d\uff0c\u5b9e\u73b0\u4ec5\u97002-4\u6b65NFE\u7684\u9ad8\u8d28\u91cf\u9006\u95ee\u9898\u6c42\u89e3", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4e00\u81f4\u6027\u6a21\u578b\u7684\u9006\u95ee\u9898\u6c42\u89e3\u5668\u9700\u8981\u989d\u5916\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u6216\u4f7f\u7528\u6536\u655b\u7f13\u6162\u7684\u6570\u636e\u4fdd\u771f\u5ea6\u64cd\u4f5c\uff0c\u4e0d\u9002\u5408\u5927\u89c4\u6a21\u95ee\u9898", "method": "\u5c06\u4e00\u81f4\u6027\u6a21\u578b\u91cd\u65b0\u89e3\u91ca\u4e3a\u5148\u9a8c\u7684\u8fd1\u7aef\u7b97\u5b50\uff0c\u7ed3\u5408PnP-ADMM\u6846\u67b6\uff0c\u5229\u7528\u5171\u8f6d\u68af\u5ea6\u6cd5\u5feb\u901f\u6536\u655b\uff0c\u5e76\u901a\u8fc7\u566a\u58f0\u6ce8\u5165\u548c\u52a8\u91cf\u8fdb\u4e00\u6b65\u52a0\u901f", "result": "\u5728\u4fee\u590d\u3001\u8d85\u5206\u8fa8\u7387\u3001\u9ad8\u65af\u53bb\u6a21\u7cca\u548cMRI\u91cd\u5efa\u7b49\u9006\u95ee\u9898\u4e0a\uff0cPnP-CM\u4ec5\u97004\u6b65NFE\u5373\u53ef\u83b7\u5f97\u9ad8\u8d28\u91cf\u91cd\u5efa\uff0c2\u6b65\u5373\u53ef\u4ea7\u751f\u6709\u610f\u4e49\u7ed3\u679c\uff0c\u4f18\u4e8e\u540c\u7c7bCM\u65b9\u6cd5", "conclusion": "PnP-CM\u5728\u73b0\u5b9e\u4e16\u754c\u9006\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\uff0c\u662f\u9996\u4e2a\u9488\u5bf9MRI\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u4e00\u81f4\u6027\u6a21\u578b\uff0c\u5728\u5c11\u91cfNFE\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cd\u5efa"}}
{"id": "2509.23594", "pdf": "https://arxiv.org/pdf/2509.23594", "abs": "https://arxiv.org/abs/2509.23594", "authors": ["Yixu Wang", "Yan Teng", "Yingchun Wang", "Xingjun Ma"], "title": "StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data", "categories": ["cs.CR", "cs.CV"], "comment": "ICCV 2025", "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have transformed vision model adaptation, enabling the rapid deployment of customized models. However, the compactness of LoRA adaptations introduces new safety concerns, particularly their vulnerability to model extraction attacks. This paper introduces a new focus of model extraction attacks named LoRA extraction that extracts LoRA-adaptive models based on a public pre-trained model. We then propose a novel extraction method called StolenLoRA which trains a substitute model to extract the functionality of a LoRA-adapted model using synthetic data. StolenLoRA leverages a Large Language Model to craft effective prompts for data generation, and it incorporates a Disagreement-based Semi-supervised Learning (DSL) strategy to maximize information gain from limited queries. Our experiments demonstrate the effectiveness of StolenLoRA, achieving up to a 96.60% attack success rate with only 10k queries, even in cross-backbone scenarios where the attacker and victim models utilize different pre-trained backbones. These findings reveal the specific vulnerability of LoRA-adapted models to this type of extraction and underscore the urgent need for robust defense mechanisms tailored to PEFT methods. We also explore a preliminary defense strategy based on diversified LoRA deployments, highlighting its potential to mitigate such attacks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9LoRA\u9002\u914d\u6a21\u578b\u7684\u65b0\u578b\u6a21\u578b\u63d0\u53d6\u653b\u51fb\u65b9\u6cd5StolenLoRA\uff0c\u5229\u7528\u5408\u6210\u6570\u636e\u548c\u534a\u76d1\u7763\u5b66\u4e60\u7b56\u7565\uff0c\u5728\u4ec5\u4f7f\u75281\u4e07\u6b21\u67e5\u8be2\u7684\u60c5\u51b5\u4e0b\u8fbe\u523096.60%\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u63ed\u793a\u4e86LoRA\u9002\u914d\u6a21\u578b\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "LoRA\u7b49\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u867d\u7136\u63d0\u5347\u4e86\u89c6\u89c9\u6a21\u578b\u9002\u914d\u7684\u6548\u7387\uff0c\u4f46\u5176\u7d27\u51d1\u6027\u5f15\u5165\u4e86\u65b0\u7684\u5b89\u5168\u9690\u60a3\uff0c\u7279\u522b\u662f\u5bf9\u6a21\u578b\u63d0\u53d6\u653b\u51fb\u7684\u8106\u5f31\u6027\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u9488\u5bf9LoRA\u9002\u914d\u6a21\u578b\u7684\u63d0\u53d6\u653b\u51fb\u3002", "method": "\u63d0\u51faStolenLoRA\u653b\u51fb\u65b9\u6cd5\uff1a1\uff09\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6709\u6548\u63d0\u793a\u6765\u5408\u6210\u6570\u636e\uff1b2\uff09\u91c7\u7528\u57fa\u4e8e\u5206\u6b67\u7684\u534a\u76d1\u7763\u5b66\u4e60\u7b56\u7565\u6700\u5927\u5316\u6709\u9650\u67e5\u8be2\u7684\u4fe1\u606f\u589e\u76ca\uff1b3\uff09\u8bad\u7ec3\u66ff\u4ee3\u6a21\u578b\u6765\u63d0\u53d6LoRA\u9002\u914d\u6a21\u578b\u7684\u529f\u80fd\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eStolenLoRA\u7684\u6709\u6548\u6027\uff1a\u5728\u4ec51\u4e07\u6b21\u67e5\u8be2\u4e0b\u653b\u51fb\u6210\u529f\u7387\u9ad8\u8fbe96.60%\uff0c\u5373\u4f7f\u5728\u653b\u51fb\u8005\u548c\u53d7\u5bb3\u8005\u6a21\u578b\u4f7f\u7528\u4e0d\u540c\u9884\u8bad\u7ec3\u9aa8\u5e72\u7f51\u7edc\u7684\u8de8\u9aa8\u5e72\u573a\u666f\u4e0b\u4e5f\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LoRA\u9002\u914d\u6a21\u578b\u5bf9\u6b64\u7c7b\u63d0\u53d6\u653b\u51fb\u7684\u7279\u5b9a\u8106\u5f31\u6027\uff0c\u8feb\u5207\u9700\u8981\u9488\u5bf9PEFT\u65b9\u6cd5\u7684\u9c81\u68d2\u9632\u5fa1\u673a\u5236\u3002\u521d\u6b65\u63a2\u7d22\u4e86\u57fa\u4e8e\u591a\u6837\u5316LoRA\u90e8\u7f72\u7684\u9632\u5fa1\u7b56\u7565\uff0c\u663e\u793a\u51fa\u7f13\u89e3\u6b64\u7c7b\u653b\u51fb\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.24006", "pdf": "https://arxiv.org/pdf/2509.24006", "abs": "https://arxiv.org/abs/2509.24006", "authors": ["Jintao Zhang", "Haoxu Wang", "Kai Jiang", "Shuo Yang", "Kaiwen Zheng", "Haocheng Xi", "Ziteng Wang", "Hongzhou Zhu", "Min Zhao", "Ion Stoica", "Joseph E. Gonzalez", "Jun Zhu", "Jianfei Chen"], "title": "SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "In Diffusion Transformer (DiT) models, particularly for video generation, attention latency is a major bottleneck due to the long sequence length and the quadratic complexity. We find that attention weights can be separated into two parts: a small fraction of large weights with high rank and the remaining weights with very low rank. This naturally suggests applying sparse acceleration to the first part and low-rank acceleration to the second. Based on this finding, we propose SLA (Sparse-Linear Attention), a trainable attention method that fuses sparse and linear attention to accelerate diffusion models. SLA classifies attention weights into critical, marginal, and negligible categories, applying O(N^2) attention to critical weights, O(N) attention to marginal weights, and skipping negligible ones. SLA combines these computations into a single GPU kernel and supports both forward and backward passes. With only a few fine-tuning steps using SLA, DiT models achieve a 20x reduction in attention computation, resulting in significant acceleration without loss of generation quality. Experiments show that SLA reduces attention computation by 95% without degrading end-to-end generation quality, outperforming baseline methods. In addition, we implement an efficient GPU kernel for SLA, which yields a 13.7x speedup in attention computation and a 2.2x end-to-end speedup in video generation on Wan2.1-1.3B.", "AI": {"tldr": "SLA\u662f\u4e00\u79cd\u878d\u5408\u7a00\u758f\u548c\u7ebf\u6027\u6ce8\u610f\u529b\u7684\u53ef\u8bad\u7ec3\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6ce8\u610f\u529b\u6743\u91cd\u5206\u4e3a\u5173\u952e\u3001\u8fb9\u9645\u548c\u53ef\u5ffd\u7565\u4e09\u7c7b\uff0c\u5206\u522b\u5e94\u7528O(N\u00b2)\u3001O(N)\u8ba1\u7b97\u548c\u8df3\u8fc7\uff0c\u5b9e\u73b0\u4e8620\u500d\u6ce8\u610f\u529b\u8ba1\u7b97\u51cf\u5c11\u548c2.2\u500d\u7aef\u5230\u7aef\u52a0\u901f\uff0c\u4e14\u4e0d\u635f\u5931\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u53d8\u6362\u5668(DiT)\u6a21\u578b\u5728\u89c6\u9891\u751f\u6210\u4e2d\uff0c\u7531\u4e8e\u957f\u5e8f\u5217\u957f\u5ea6\u548c\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u6ce8\u610f\u529b\u5ef6\u8fdf\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\u3002\u7814\u7a76\u53d1\u73b0\u6ce8\u610f\u529b\u6743\u91cd\u53ef\u5206\u4e3a\u9ad8\u79e9\u7684\u5927\u6743\u91cd\u548c\u4f4e\u79e9\u7684\u5c0f\u6743\u91cd\u4e24\u90e8\u5206\u3002", "method": "\u63d0\u51faSLA\u65b9\u6cd5\uff0c\u5c06\u6ce8\u610f\u529b\u6743\u91cd\u5206\u7c7b\u4e3a\u5173\u952e\u3001\u8fb9\u9645\u548c\u53ef\u5ffd\u7565\u4e09\u7c7b\uff0c\u5206\u522b\u5e94\u7528O(N\u00b2)\u6ce8\u610f\u529b\u3001O(N)\u6ce8\u610f\u529b\u548c\u8df3\u8fc7\u8ba1\u7b97\uff0c\u5e76\u96c6\u6210\u5230\u5355\u4e2aGPU\u5185\u6838\u4e2d\uff0c\u652f\u6301\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad\u3002", "result": "SLA\u5c06\u6ce8\u610f\u529b\u8ba1\u7b97\u51cf\u5c1195%\uff0c\u6ce8\u610f\u529b\u8ba1\u7b97\u52a0\u901f13.7\u500d\uff0c\u89c6\u9891\u751f\u6210\u7aef\u5230\u7aef\u52a0\u901f2.2\u500d\uff0c\u4e14\u4e0d\u964d\u4f4e\u751f\u6210\u8d28\u91cf\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SLA\u901a\u8fc7\u878d\u5408\u7a00\u758f\u548c\u7ebf\u6027\u6ce8\u610f\u529b\uff0c\u6709\u6548\u52a0\u901f\u6269\u6563\u6a21\u578b\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2509.24325", "pdf": "https://arxiv.org/pdf/2509.24325", "abs": "https://arxiv.org/abs/2509.24325", "authors": ["Jiaye Fu", "Qiankun Gao", "Chengxiang Wen", "Yanmin Wu", "Siwei Ma", "Jiaqi Zhang", "Jian Zhang"], "title": "ReCon-GS: Continuum-Preserved Guassian Streaming for Fast and Compact Reconstruction of Dynamic Scenes", "categories": ["eess.IV", "cs.CV", "cs.MM"], "comment": null, "summary": "Online free-viewpoint video (FVV) reconstruction is challenged by slow per-frame optimization, inconsistent motion estimation, and unsustainable storage demands. To address these challenges, we propose the Reconfigurable Continuum Gaussian Stream, dubbed ReCon-GS, a novel storage-aware framework that enables high fidelity online dynamic scene reconstruction and real-time rendering. Specifically, we dynamically allocate multi-level Anchor Gaussians in a density-adaptive fashion to capture inter-frame geometric deformations, thereby decomposing scene motion into compact coarse-to-fine representations. Then, we design a dynamic hierarchy reconfiguration strategy that preserves localized motion expressiveness through on-demand anchor re-hierarchization, while ensuring temporal consistency through intra-hierarchical deformation inheritance that confines transformation priors to their respective hierarchy levels. Furthermore, we introduce a storage-aware optimization mechanism that flexibly adjusts the density of Anchor Gaussians at different hierarchy levels, enabling a controllable trade-off between reconstruction fidelity and memory usage. Extensive experiments on three widely used datasets demonstrate that, compared to state-of-the-art methods, ReCon-GS improves training efficiency by approximately 15% and achieves superior FVV synthesis quality with enhanced robustness and stability. Moreover, at equivalent rendering quality, ReCon-GS slashes memory requirements by over 50% compared to leading state-of-the-art methods.", "AI": {"tldr": "ReCon-GS\u662f\u4e00\u4e2a\u5b58\u50a8\u611f\u77e5\u7684\u5728\u7ebf\u81ea\u7531\u89c6\u70b9\u89c6\u9891\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7ea7\u951a\u70b9\u9ad8\u65af\u52a8\u6001\u5206\u914d\u548c\u5c42\u6b21\u91cd\u6784\u7b56\u7565\uff0c\u5728\u4fdd\u8bc1\u91cd\u5efa\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5185\u5b58\u9700\u6c42\u3002", "motivation": "\u89e3\u51b3\u5728\u7ebf\u81ea\u7531\u89c6\u70b9\u89c6\u9891\u91cd\u5efa\u4e2d\u7684\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u9010\u5e27\u4f18\u5316\u901f\u5ea6\u6162\u3001\u8fd0\u52a8\u4f30\u8ba1\u4e0d\u4e00\u81f4\u6027\u548c\u5b58\u50a8\u9700\u6c42\u4e0d\u53ef\u6301\u7eed\u3002", "method": "1. \u5bc6\u5ea6\u81ea\u9002\u5e94\u65b9\u5f0f\u52a8\u6001\u5206\u914d\u591a\u7ea7\u951a\u70b9\u9ad8\u65af\u6765\u6355\u6349\u5e27\u95f4\u51e0\u4f55\u53d8\u5f62\uff1b2. \u52a8\u6001\u5c42\u6b21\u91cd\u6784\u7b56\u7565\u901a\u8fc7\u6309\u9700\u951a\u70b9\u91cd\u5c42\u6b21\u5316\u4fdd\u6301\u5c40\u90e8\u8fd0\u52a8\u8868\u8fbe\u80fd\u529b\uff1b3. \u5b58\u50a8\u611f\u77e5\u4f18\u5316\u673a\u5236\u7075\u6d3b\u8c03\u6574\u4e0d\u540c\u5c42\u6b21\u951a\u70b9\u9ad8\u65af\u7684\u5bc6\u5ea6\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8bad\u7ec3\u6548\u7387\u63d0\u5347\u7ea615%\uff0cFVV\u5408\u6210\u8d28\u91cf\u66f4\u4f18\uff0c\u9c81\u68d2\u6027\u548c\u7a33\u5b9a\u6027\u589e\u5f3a\u3002\u5728\u540c\u7b49\u6e32\u67d3\u8d28\u91cf\u4e0b\uff0c\u5185\u5b58\u9700\u6c42\u964d\u4f4e\u8d85\u8fc750%\u3002", "conclusion": "ReCon-GS\u901a\u8fc7\u521b\u65b0\u7684\u5b58\u50a8\u611f\u77e5\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5728\u7ebf\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e2d\u7684\u6548\u7387\u3001\u4e00\u81f4\u6027\u548c\u5b58\u50a8\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u5b9e\u65f6\u6e32\u67d3\u3002"}}
{"id": "2509.24334", "pdf": "https://arxiv.org/pdf/2509.24334", "abs": "https://arxiv.org/abs/2509.24334", "authors": ["Wankun Chen", "Feng Gao", "Yanhai Gan", "Jingchao Cao", "Junyu Dong", "Qian Du"], "title": "Wavelet-Assisted Mamba for Satellite-Derived Sea Surface Temperature Super-Resolution", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by IEEE TGRS 2025", "summary": "Sea surface temperature (SST) is an essential indicator of global climate change and one of the most intuitive factors reflecting ocean conditions. Obtaining high-resolution SST data remains challenging due to limitations in physical imaging, and super-resolution via deep neural networks is a promising solution. Recently, Mamba-based approaches leveraging State Space Models (SSM) have demonstrated significant potential for long-range dependency modeling with linear complexity. However, their application to SST data super-resolution remains largely unexplored. To this end, we propose the Wavelet-assisted Mamba Super-Resolution (WMSR) framework for satellite-derived SST data. The WMSR includes two key components: the Low-Frequency State Space Module (LFSSM) and High-Frequency Enhancement Module (HFEM). The LFSSM uses 2D-SSM to capture global information of the input data, and the robust global modeling capabilities of SSM are exploited to preserve the critical temperature information in the low-frequency component. The HFEM employs the pixel difference convolution to match and correct the high-frequency feature, achieving accurate and clear textures. Through comprehensive experiments on three SST datasets, our WMSR demonstrated superior performance over state-of-the-art methods. Our codes and datasets will be made publicly available at https://github.com/oucailab/WMSR.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u5c0f\u6ce2\u548cMamba\u7684\u6d77\u8868\u6e29\u5ea6\u8d85\u5206\u8fa8\u7387\u6846\u67b6WMSR\uff0c\u901a\u8fc7\u4f4e\u9891\u72b6\u6001\u7a7a\u95f4\u6a21\u5757\u548c\u9ad8\u9891\u589e\u5f3a\u6a21\u5757\u5206\u522b\u5904\u7406\u5168\u5c40\u4fe1\u606f\u548c\u7eb9\u7406\u7ec6\u8282\uff0c\u5728\u4e09\u4e2aSST\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u83b7\u53d6\u9ad8\u5206\u8fa8\u7387\u6d77\u8868\u6e29\u5ea6\u6570\u636e\u5177\u6709\u6311\u6218\u6027\uff0c\u800c\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684Mamba\u65b9\u6cd5\u5728\u957f\u7a0b\u4f9d\u8d56\u5efa\u6a21\u65b9\u9762\u5c55\u73b0\u6f5c\u529b\uff0c\u4f46\u5728SST\u8d85\u5206\u8fa8\u7387\u4e2d\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "WMSR\u6846\u67b6\u5305\u542b\u4f4e\u9891\u72b6\u6001\u7a7a\u95f4\u6a21\u5757\uff08\u4f7f\u75282D-SSM\u6355\u83b7\u5168\u5c40\u4fe1\u606f\uff09\u548c\u9ad8\u9891\u589e\u5f3a\u6a21\u5757\uff08\u4f7f\u7528\u50cf\u7d20\u5dee\u5206\u5377\u79ef\u6821\u6b63\u9ad8\u9891\u7279\u5f81\uff09\uff0c\u7ed3\u5408\u5c0f\u6ce2\u53d8\u6362\u5904\u7406\u4e0d\u540c\u9891\u7387\u5206\u91cf\u3002", "result": "\u5728\u4e09\u4e2aSST\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cWMSR\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u8868\u73b0\u51fa\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684WMSR\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u6d77\u8868\u6e29\u5ea6\u6570\u636e\u7684\u8d85\u5206\u8fa8\u7387\u8d28\u91cf\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u63d0\u4f9b\u3002"}}
{"id": "2509.24580", "pdf": "https://arxiv.org/pdf/2509.24580", "abs": "https://arxiv.org/abs/2509.24580", "authors": ["Lingyu Wang", "Xiangming Meng"], "title": "SAIP: A Plug-and-Play Scale-adaptive Module in Diffusion-based Inverse Problems", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Solving inverse problems with diffusion models has shown promise in tasks such as image restoration. A common approach is to formulate the problem in a Bayesian framework and sample from the posterior by combining the prior score with the likelihood score. Since the likelihood term is often intractable, estimators like DPS, DMPS, and $\\pi$GDM are widely adopted. However, these methods rely on a fixed, manually tuned scale to balance prior and likelihood contributions. Such a static design is suboptimal, as the ideal balance varies across timesteps and tasks, limiting performance and generalization. To address this issue, we propose SAIP, a plug-and-play module that adaptively refines the scale at each timestep without retraining or altering the diffusion backbone. SAIP integrates seamlessly into existing samplers and consistently improves reconstruction quality across diverse image restoration tasks, including challenging scenarios.", "AI": {"tldr": "\u63d0\u51faSAIP\u6a21\u5757\uff0c\u81ea\u9002\u5e94\u8c03\u6574\u6269\u6563\u6a21\u578b\u4e2d\u5148\u9a8c\u548c\u4f3c\u7136\u9879\u7684\u5e73\u8861\u5c3a\u5ea6\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u56fe\u50cf\u590d\u539f\u8d28\u91cf", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u7684\u624b\u52a8\u8c03\u4f18\u5c3a\u5ea6\u6765\u5e73\u8861\u5148\u9a8c\u548c\u4f3c\u7136\u9879\uff0c\u8fd9\u79cd\u9759\u6001\u8bbe\u8ba1\u5728\u65f6\u95f4\u6b65\u548c\u4efb\u52a1\u95f4\u4e0d\u662f\u6700\u4f18\u7684\uff0c\u9650\u5236\u4e86\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b", "method": "SAIP\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u81ea\u9002\u5e94\u4f18\u5316\u5c3a\u5ea6\u53c2\u6570\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u4fee\u6539\u6269\u6563\u4e3b\u5e72\u7f51\u7edc\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u91c7\u6837\u5668\u4e2d", "result": "SAIP\u5728\u591a\u79cd\u56fe\u50cf\u590d\u539f\u4efb\u52a1\u4e2d\u6301\u7eed\u6539\u5584\u91cd\u5efa\u8d28\u91cf\uff0c\u5305\u62ec\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f", "conclusion": "SAIP\u901a\u8fc7\u81ea\u9002\u5e94\u5c3a\u5ea6\u8c03\u6574\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u5728\u9006\u95ee\u9898\u6c42\u89e3\u4e2d\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u5e73\u8861\u7b56\u7565"}}
{"id": "2509.24603", "pdf": "https://arxiv.org/pdf/2509.24603", "abs": "https://arxiv.org/abs/2509.24603", "authors": ["Tianle Wang", "Sirui Zhang", "Xinyi Tong", "Peiyang Yu", "Jishang Chen", "Liangke Zhao", "Xinpu Gao", "Yves Zhu", "Tiezheng Ge", "Bo Zheng", "Duo Xu", "Yang Liu", "Xin Jin", "Feng Yu", "Songchun Zhu"], "title": "Discovering \"Words\" in Music: Unsupervised Learning of Compositional Sparse Code for Symbolic Music", "categories": ["cs.SD", "cs.CV"], "comment": null, "summary": "This paper presents an unsupervised machine learning algorithm that identifies recurring patterns -- referred to as ``music-words'' -- from symbolic music data. These patterns are fundamental to musical structure and reflect the cognitive processes involved in composition. However, extracting these patterns remains challenging because of the inherent semantic ambiguity in musical interpretation. We formulate the task of music-word discovery as a statistical optimization problem and propose a two-stage Expectation-Maximization (EM)-based learning framework: 1. Developing a music-word dictionary; 2. Reconstructing the music data. When evaluated against human expert annotations, the algorithm achieved an Intersection over Union (IoU) score of 0.61. Our findings indicate that minimizing code length effectively addresses semantic ambiguity, suggesting that human optimization of encoding systems shapes musical semantics. This approach enables computers to extract ``basic building blocks'' from music data, facilitating structural analysis and sparse encoding. The method has two primary applications. First, in AI music, it supports downstream tasks such as music generation, classification, style transfer, and improvisation. Second, in musicology, it provides a tool for analyzing compositional patterns and offers insights into the principle of minimal encoding across diverse musical styles and composers.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u76d1\u7763\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u4ece\u7b26\u53f7\u97f3\u4e50\u6570\u636e\u4e2d\u8bc6\u522b\u91cd\u590d\u6a21\u5f0f\uff08\u79f0\u4e3a\"\u97f3\u4e50\u8bcd\"\uff09\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5EM\u6846\u67b6\u89e3\u51b3\u97f3\u4e50\u8bed\u4e49\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u5728\u4eba\u7c7b\u4e13\u5bb6\u6807\u6ce8\u4e0a\u8fbe\u52300.61 IoU\u5206\u6570\u3002", "motivation": "\u97f3\u4e50\u4e2d\u7684\u91cd\u590d\u6a21\u5f0f\u53cd\u6620\u4e86\u4f5c\u66f2\u7684\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u4f46\u7531\u4e8e\u97f3\u4e50\u89e3\u91ca\u7684\u8bed\u4e49\u6a21\u7cca\u6027\uff0c\u63d0\u53d6\u8fd9\u4e9b\u6a21\u5f0f\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5c06\u97f3\u4e50\u8bcd\u53d1\u73b0\u5efa\u6a21\u4e3a\u7edf\u8ba1\u4f18\u5316\u95ee\u9898\uff0c\u91c7\u7528\u4e24\u9636\u6bb5EM\u5b66\u4e60\u6846\u67b6\uff1a1. \u5f00\u53d1\u97f3\u4e50\u8bcd\u8bcd\u5178\uff1b2. \u91cd\u6784\u97f3\u4e50\u6570\u636e\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u7f16\u7801\u957f\u5ea6\u6765\u89e3\u51b3\u8bed\u4e49\u6a21\u7cca\u6027\u3002", "result": "\u7b97\u6cd5\u5728\u4eba\u7c7b\u4e13\u5bb6\u6807\u6ce8\u8bc4\u4f30\u4e2d\u8fbe\u52300.61 IoU\u5206\u6570\uff0c\u8868\u660e\u6700\u5c0f\u5316\u7f16\u7801\u957f\u5ea6\u80fd\u6709\u6548\u89e3\u51b3\u8bed\u4e49\u6a21\u7cca\u6027\uff0c\u53cd\u6620\u4eba\u7c7b\u5bf9\u7f16\u7801\u7cfb\u7edf\u7684\u4f18\u5316\u5851\u9020\u4e86\u97f3\u4e50\u8bed\u4e49\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f7f\u8ba1\u7b97\u673a\u80fd\u4ece\u97f3\u4e50\u6570\u636e\u4e2d\u63d0\u53d6\"\u57fa\u672c\u6784\u5efa\u5757\"\uff0c\u652f\u6301AI\u97f3\u4e50\u4efb\u52a1\uff08\u751f\u6210\u3001\u5206\u7c7b\u3001\u98ce\u683c\u8fc1\u79fb\u7b49\uff09\u548c\u97f3\u4e50\u5b66\u5206\u6790\uff0c\u63ed\u793a\u4e86\u6700\u5c0f\u7f16\u7801\u539f\u5219\u5728\u4e0d\u540c\u97f3\u4e50\u98ce\u683c\u548c\u4f5c\u66f2\u5bb6\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2509.24823", "pdf": "https://arxiv.org/pdf/2509.24823", "abs": "https://arxiv.org/abs/2509.24823", "authors": ["Benedetta Tondi", "Andrea Costanzo", "Mauro Barni"], "title": "Of-SemWat: High-payload text embedding for semantic watermarking of AI-generated images with arbitrary size", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "comment": "5 pages, 2 figures", "summary": "We propose a high-payload image watermarking method for textual embedding, where a semantic description of the image - which may also correspond to the input text prompt-, is embedded inside the image. In order to be able to robustly embed high payloads in large-scale images - such as those produced by modern AI generators - the proposed approach builds upon a traditional watermarking scheme that exploits orthogonal and turbo codes for improved robustness, and integrates frequency-domain embedding and perceptual masking techniques to enhance watermark imperceptibility. Experiments show that the proposed method is extremely robust against a wide variety of image processing, and the embedded text can be retrieved also after traditional and AI inpainting, permitting to unveil the semantic modification the image has undergone via image-text mismatch analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u6587\u672c\u5d4c\u5165\u7684\u9ad8\u8f7d\u8377\u56fe\u50cf\u6c34\u5370\u65b9\u6cd5\uff0c\u5c06\u56fe\u50cf\u7684\u8bed\u4e49\u63cf\u8ff0\u5d4c\u5165\u5230\u56fe\u50cf\u4e2d\uff0c\u80fd\u591f\u5728\u5927\u89c4\u6a21AI\u751f\u6210\u56fe\u50cf\u4e2d\u9c81\u68d2\u5730\u5d4c\u5165\u9ad8\u8f7d\u8377\u4fe1\u606f\u3002", "motivation": "\u73b0\u4ee3AI\u751f\u6210\u5668\u4ea7\u751f\u7684\u5927\u89c4\u6a21\u56fe\u50cf\u9700\u8981\u80fd\u591f\u5d4c\u5165\u9ad8\u8f7d\u8377\u4fe1\u606f\u7684\u6c34\u5370\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u6c34\u5370\u7684\u4e0d\u53ef\u611f\u77e5\u6027\u548c\u5bf9\u56fe\u50cf\u5904\u7406\u7684\u9c81\u68d2\u6027\u3002", "method": "\u57fa\u4e8e\u4f20\u7edf\u6c34\u5370\u65b9\u6848\uff0c\u5229\u7528\u6b63\u4ea4\u548cturbo\u7801\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u96c6\u6210\u9891\u57df\u5d4c\u5165\u548c\u611f\u77e5\u63a9\u853d\u6280\u672f\u6765\u589e\u5f3a\u6c34\u5370\u7684\u4e0d\u53ef\u611f\u77e5\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5bf9\u591a\u79cd\u56fe\u50cf\u5904\u7406\u5177\u6709\u6781\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u5373\u4f7f\u5728\u4f20\u7edf\u548cAI\u4fee\u590d\u540e\u4e5f\u80fd\u68c0\u7d22\u5d4c\u5165\u6587\u672c\uff0c\u901a\u8fc7\u56fe\u50cf-\u6587\u672c\u4e0d\u5339\u914d\u5206\u6790\u63ed\u793a\u56fe\u50cf\u7684\u8bed\u4e49\u4fee\u6539\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u8f7d\u8377\u6587\u672c\u5d4c\u5165\uff0c\u5728\u4fdd\u6301\u6c34\u5370\u4e0d\u53ef\u611f\u77e5\u7684\u540c\u65f6\uff0c\u5bf9\u5404\u7c7b\u56fe\u50cf\u5904\u7406\u5177\u6709\u5f3a\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u56fe\u50cf\u7684\u8bed\u4e49\u4fee\u6539\u3002"}}
{"id": "2509.25003", "pdf": "https://arxiv.org/pdf/2509.25003", "abs": "https://arxiv.org/abs/2509.25003", "authors": ["Mingxing Rao", "Bowen Qu", "Daniel Moyer"], "title": "Score-based Membership Inference on Diffusion Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Membership inference attacks (MIAs) against diffusion models have emerged as a pressing privacy concern, as these models may inadvertently reveal whether a given sample was part of their training set. We present a theoretical and empirical study of score-based MIAs, focusing on the predicted noise vectors that diffusion models learn to approximate. We show that the expected denoiser output points toward a kernel-weighted local mean of nearby training samples, such that its norm encodes proximity to the training set and thereby reveals membership. Building on this observation, we propose SimA, a single-query attack that provides a principled, efficient alternative to existing multi-query methods. SimA achieves consistently strong performance across variants of DDPM, Latent Diffusion Model (LDM). Notably, we find that Latent Diffusion Models are surprisingly less vulnerable than pixel-space models, due to the strong information bottleneck imposed by their latent auto-encoder. We further investigate this by differing the regularization hyperparameters ($\\beta$ in $\\beta$-VAE) in latent channel and suggest a strategy to make LDM training more robust to MIA. Our results solidify the theory of score-based MIAs, while highlighting that Latent Diffusion class of methods requires better understanding of inversion for VAE, and not simply inversion of the Diffusion process", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u6570\u7684\u6210\u5458\u63a8\u7406\u653b\u51fbSimA\uff0c\u901a\u8fc7\u5206\u6790\u6269\u6563\u6a21\u578b\u9884\u6d4b\u7684\u566a\u58f0\u5411\u91cf\u6765\u68c0\u6d4b\u8bad\u7ec3\u6837\u672c\u6210\u5458\u8eab\u4efd\uff0c\u53d1\u73b0\u6f5c\u5728\u6269\u6563\u6a21\u578b\u6bd4\u50cf\u7d20\u7a7a\u95f4\u6a21\u578b\u66f4\u5b89\u5168\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u53ef\u80fd\u65e0\u610f\u4e2d\u6cc4\u9732\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u6210\u5458\u4fe1\u606f\uff0c\u73b0\u6709\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\u65b9\u6cd5\u6548\u7387\u4e0d\u9ad8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7406\u8bba\u57fa\u7840\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSimA\u5355\u67e5\u8be2\u653b\u51fb\u65b9\u6cd5\uff0c\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u9884\u6d4b\u566a\u58f0\u5411\u91cf\u7684\u7406\u8bba\u5206\u6790\uff0c\u5229\u7528\u53bb\u566a\u5668\u8f93\u51fa\u5411\u91cf\u7684\u8303\u6570\u7f16\u7801\u8bad\u7ec3\u96c6\u90bb\u8fd1\u5ea6\u4fe1\u606f\u3002", "result": "SimA\u5728DDPM\u548c\u6f5c\u5728\u6269\u6563\u6a21\u578b\u4e0a\u8868\u73b0\u4e00\u81f4\u5f3a\u52b2\uff0c\u53d1\u73b0\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7531\u4e8e\u6f5c\u5728\u81ea\u7f16\u7801\u5668\u7684\u4fe1\u606f\u74f6\u9888\u800c\u76f8\u5bf9\u66f4\u5b89\u5168\u3002", "conclusion": "\u57fa\u4e8e\u5206\u6570\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\u7406\u8bba\u5f97\u5230\u9a8c\u8bc1\uff0c\u6f5c\u5728\u6269\u6563\u65b9\u6cd5\u9700\u8981\u5bf9VAE\u53cd\u6f14\u6709\u66f4\u597d\u7406\u89e3\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u6269\u6563\u8fc7\u7a0b\u7684\u53cd\u6f14\u3002"}}
