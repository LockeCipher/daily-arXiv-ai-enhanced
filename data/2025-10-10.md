<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 7]
- [cs.CV](#cs.CV) [Total: 33]
- [cs.LG](#cs.LG) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [SpotDiff: Spotting and Disentangling Interference in Feature Space for Subject-Preserving Image Generation](https://arxiv.org/abs/2510.07340)
*Yongzhi Li,Saining Zhang,Yibing Chen,Boying Li,Yanxin Zhang,Xiaoyu Du*

Main category: cs.GR

TL;DR: SpotDiff是一种基于学习的方法，通过识别和解缠干扰来提取特定主题的特征，在保持主题身份的同时实现可控编辑。


<details>
  <summary>Details</summary>
Motivation: 现有优化方法计算成本高，而学习方法效率高但受干扰因素影响导致表示纠缠。需要一种既能保持高保真度又高效的方法。

Method: 利用预训练CLIP图像编码器和专门的姿态、背景专家网络，通过特征空间正交性约束来分离主题身份。构建SpotDiff10k数据集进行训练。

Result: SpotDiff在主题保持和可控编辑方面比现有方法更鲁棒，仅用10k训练样本就达到竞争性能。

Conclusion: SpotDiff通过解缠干扰实现了高效且鲁棒的个性化图像生成，在保真度和可控性方面优于现有方法。

Abstract: Personalized image generation aims to faithfully preserve a reference subject's identity while adapting to diverse text prompts. Existing optimization-based methods ensure high fidelity but are computationally expensive, while learning-based approaches offer efficiency at the cost of entangled representations influenced by nuisance factors. We introduce SpotDiff, a novel learning-based method that extracts subject-specific features by spotting and disentangling interference. Leveraging a pre-trained CLIP image encoder and specialized expert networks for pose and background, SpotDiff isolates subject identity through orthogonality constraints in the feature space. To enable principled training, we introduce SpotDiff10k, a curated dataset with consistent pose and background variations. Experiments demonstrate that SpotDiff achieves more robust subject preservation and controllable editing than prior methods, while attaining competitive performance with only 10k training samples.

</details>


### [2] [Local MAP Sampling for Diffusion Models](https://arxiv.org/abs/2510.07343)
*Shaorong Zhang,Rob Brekelmans,Greg Ver Steeg*

Main category: cs.GR

TL;DR: 提出了Local MAP Sampling (LMAPS)框架，通过沿扩散轨迹迭代求解局部MAP子问题，为基于优化的扩散求解器提供统一的概率解释，并在多个图像恢复任务中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 虽然DPS提供了解决逆问题的贝叶斯方法，但实际应用中优化方法往往表现更好却缺乏明确的概率基础。需要为优化方法提供统一的概率解释。

Method: 引入LMAPS框架，迭代求解扩散轨迹上的局部MAP子问题；开发了概率可解释的协方差近似、重新表述的目标函数以提高稳定性和可解释性，以及非可微算子的梯度近似。

Result: 在广泛的图像恢复和科学任务中实现最先进性能：运动去模糊、JPEG恢复和量化任务上获得≥2dB增益，逆散射基准上获得>1.5dB改进。

Conclusion: LMAPS为优化方法提供了统一的概率解释，连接了全局MAP估计和DPS，并在多个任务中显著提升了性能。

Abstract: Diffusion Posterior Sampling (DPS) provides a principled Bayesian approach to inverse problems by sampling from $p(x_0 \mid y)$. However, in practice, the goal of inverse problem solving is not to cover the posterior but to recover the most accurate reconstruction, where optimization-based diffusion solvers often excel despite lacking a clear probabilistic foundation. We introduce Local MAP Sampling (LMAPS), a new inference framework that iteratively solving local MAP subproblems along the diffusion trajectory. This perspective clarifies their connection to global MAP estimation and DPS, offering a unified probabilistic interpretation for optimization-based methods. Building on this foundation, we develop practical algorithms with a probabilistically interpretable covariance approximation, a reformulated objective for stability and interpretability, and a gradient approximation for non-differentiable operators. Across a broad set of image restoration and scientific tasks, LMAPS achieves state-of-the-art performance, including $\geq 2$ dB gains on motion deblurring, JPEG restoration, and quantization, and $>1.5$ dB improvements on inverse scattering benchmarks.

</details>


### [3] [NRRS: Neural Russian Roulette and Splitting](https://arxiv.org/abs/2510.07868)
*Haojie Jin,Jierui Ren,Yisong Chen,Guoping Wang,Sheng Li*

Main category: cs.GR

TL;DR: 提出了一种专为波前路径追踪设计的俄罗斯轮盘赌和分裂（RRS）框架，通过归一化RRS公式和神经网络学习RRS因子，解决了传统RRS方法与波前架构的内存预分配需求不兼容的问题。


<details>
  <summary>Details</summary>
Motivation: 传统RRS方法由于路径数量不可预测，与波前路径追踪的高度并行、批处理架构的内存预分配和调度要求存在根本性不兼容，需要新的解决方案。

Method: 引入归一化RRS公式确保路径数量有界；提出两种神经网络模型（NRRS和AID-NRRS）学习RRS因子；设计RRSNet显式整合RRS归一化；引入Mix-Depth机制根据路径深度自适应调节神经评估。

Result: 在多种复杂场景的广泛实验中，该方法在渲染质量和性能方面均优于传统启发式方法和近期RRS技术。

Conclusion: 所提出的框架成功解决了波前路径追踪中RRS的内存兼容性问题，通过神经网络学习RRS因子和自适应调节机制，实现了更好的渲染效果和性能表现。

Abstract: We propose a novel framework for Russian Roulette and Splitting (RRS) tailored to wavefront path tracing, a highly parallel rendering architecture that processes path states in batched, stage-wise execution for efficient GPU utilization. Traditional RRS methods, with unpredictable path counts, are fundamentally incompatible with wavefront's preallocated memory and scheduling requirements. To resolve this, we introduce a normalized RRS formulation with a bounded path count, enabling stable and memory-efficient execution.   Furthermore, we pioneer the use of neural networks to learn RRS factors, presenting two models: NRRS and AID-NRRS. At a high level, both feature a carefully designed RRSNet that explicitly incorporates RRS normalization, with only subtle differences in their implementation. To balance computational cost and inference accuracy, we introduce Mix-Depth, a path-depth-aware mechanism that adaptively regulates neural evaluation, further improving efficiency.   Extensive experiments demonstrate that our method outperforms traditional heuristics and recent RRS techniques in both rendering quality and performance across a variety of complex scenes.

</details>


### [4] [Variable-Rate Texture Compression: Real-Time Rendering with JPEG](https://arxiv.org/abs/2510.08166)
*Elias Kristmann,Markus Schütz,Michael Wimmer*

Main category: cs.GR

TL;DR: 本文研究了在现代GPU上使用JPEG格式进行可变速率纹理压缩的可行性，并与固定速率压缩方法BC1和ASTC进行比较。通过延迟渲染管线，能够识别所需纹理块并解码，在RTX 4090上仅增加0.3ms渲染时间。


<details>
  <summary>Details</summary>
Motivation: 虽然JPEG等可变速率压缩图像格式广泛用于高效编码图像，但由于需要随机访问单个纹理像素等特殊要求，尚未在实时渲染中得到应用。本文旨在探索在现代GPU上实现可变速率纹理压缩的可能性。

Method: 使用延迟渲染管线识别每帧所需的纹理块子集，解码这些块并为帧缓冲区的像素着色。与固定速率压缩方法BC1和ASTC进行比较分析。

Result: 尽管需要额外约0.17位/像素的开销，但JPEG在质量和压缩率方面显著优于BC1，根据图像类型，表现优于或与ASTC相当。在RTX 4090上渲染时间仅增加不到0.3ms。

Conclusion: 研究表明，即使在VR应用中，现代GPU上实现复杂的可变速率压缩方案是可行的，为实时渲染中的高效纹理压缩提供了新途径。

Abstract: Although variable-rate compressed image formats such as JPEG are widely used to efficiently encode images, they have not found their way into real-time rendering due to special requirements such as random access to individual texels. In this paper, we investigate the feasibility of variable-rate texture compression on modern GPUs using the JPEG format, and how it compares to the GPU-friendly fixed-rate compression approaches BC1 and ASTC. Using a deferred rendering pipeline, we are able to identify the subset of blocks that are needed for a given frame, decode these, and colorize the framebuffer's pixels. Despite the additional $\sim$0.17 bit per pixel that we require for our approach, JPEG maintains significantly better quality and compression rates compared to BC1, and depending on the type of image, outperforms or competes with ASTC. The JPEG rendering pipeline increases rendering duration by less than 0.3 ms on an RTX 4090, demonstrating that sophisticated variable-rate compression schemes are feasible on modern GPUs, even in VR. Source code and data sets are available at: https://github.com/elias1518693/jpeg_textures

</details>


### [5] [SViM3D: Stable Video Material Diffusion for Single Image 3D Generation](https://arxiv.org/abs/2510.08271)
*Andreas Engelhardt,Mark Boss,Vikram Voletti,Chun-Han Yao,Hendrik P. A. Lensch,Varun Jampani*

Main category: cs.GR

TL;DR: SViM3D是一个从单张图像预测多视角一致PBR材质的框架，通过扩展视频扩散模型来联合输出PBR参数和表面法线，支持重光照和3D资产生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么使用简单材质模型，要么需要额外步骤估计反射率，无法有效支持重光照和可控外观编辑。

Method: 扩展潜在视频扩散模型，基于显式相机控制联合输出空间变化的PBR参数和表面法线，引入多种机制改进质量。

Result: 在多个以对象为中心的数据集上实现了最先进的重光照和新视角合成性能，能泛化到多样化输入。

Conclusion: 该方法能够生成可用于AR/VR、电影、游戏等视觉媒体的可重光照3D资产。

Abstract: We present Stable Video Materials 3D (SViM3D), a framework to predict multi-view consistent physically based rendering (PBR) materials, given a single image. Recently, video diffusion models have been successfully used to reconstruct 3D objects from a single image efficiently. However, reflectance is still represented by simple material models or needs to be estimated in additional steps to enable relighting and controlled appearance edits. We extend a latent video diffusion model to output spatially varying PBR parameters and surface normals jointly with each generated view based on explicit camera control. This unique setup allows for relighting and generating a 3D asset using our model as neural prior. We introduce various mechanisms to this pipeline that improve quality in this ill-posed setting. We show state-of-the-art relighting and novel view synthesis performance on multiple object-centric datasets. Our method generalizes to diverse inputs, enabling the generation of relightable 3D assets useful in AR/VR, movies, games and other visual media.

</details>


### [6] [Splat the Net: Radiance Fields with Splattable Neural Primitives](https://arxiv.org/abs/2510.08491)
*Xilong Zhou,Bao-Huy Nguyen,Loïc Magne,Vladislav Golyanik,Thomas Leimkühler,Christian Theobalt*

Main category: cs.GR

TL;DR: 提出了一种可splat的神经基元表示，结合了神经辐射场的高表达能力和3D高斯泼溅的实时渲染效率，使用更少的基元和参数达到相同质量


<details>
  <summary>Details</summary>
Motivation: 现有方法中，神经辐射场表达能力强但渲染成本高，3D高斯泼溅实时高效但表达能力有限。需要一种能兼顾表达能力和渲染效率的新表示方法

Method: 使用可splat的神经基元，每个基元编码一个有界的神经密度场，通过浅层神经网络参数化。该表示允许解析求解线积分，实现高效准确的splatting核计算

Result: 在新视角合成基准测试中，匹配3D高斯泼溅的质量和速度，同时使用10倍更少的基元和6倍更少的参数

Conclusion: 该表示直接实现了表达能力和效率的平衡，无需依赖复杂的控制或适配框架

Abstract: Radiance fields have emerged as a predominant representation for modeling 3D scene appearance. Neural formulations such as Neural Radiance Fields provide high expressivity but require costly ray marching for rendering, whereas primitive-based methods such as 3D Gaussian Splatting offer real-time efficiency through splatting, yet at the expense of representational power. Inspired by advances in both these directions, we introduce splattable neural primitives, a new volumetric representation that reconciles the expressivity of neural models with the efficiency of primitive-based splatting. Each primitive encodes a bounded neural density field parameterized by a shallow neural network. Our formulation admits an exact analytical solution for line integrals, enabling efficient computation of perspectively accurate splatting kernels. As a result, our representation supports integration along view rays without the need for costly ray marching. The primitives flexibly adapt to scene geometry and, being larger than prior analytic primitives, reduce the number required per scene. On novel-view synthesis benchmarks, our approach matches the quality and speed of 3D Gaussian Splatting while using $10\times$ fewer primitives and $6\times$ fewer parameters. These advantages arise directly from the representation itself, without reliance on complex control or adaptation frameworks. The project page is https://vcai.mpi-inf.mpg.de/projects/SplatNet/.

</details>


### [7] [X2Video: Adapting Diffusion Models for Multimodal Controllable Neural Video Rendering](https://arxiv.org/abs/2510.08530)
*Zhitong Huang,Mohan Zhang,Renhan Wang,Rui Tang,Hao Zhu,Jing Liao*

Main category: cs.GR

TL;DR: X2Video是首个基于内在通道（反照率、法线、粗糙度、金属度、辐照度）引导的扩散模型，能够生成逼真视频，并支持参考图像和文本提示的多模态控制。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型缺乏对内在通道的精确控制能力，无法准确操纵颜色、材质、几何和光照等属性。X2Video旨在解决这一问题，通过内在通道指导实现精确的属性控制。

Method: 1. 扩展XRGB图像生成模型到视频领域；2. 提出混合自注意力机制确保时间一致性；3. 开发掩码交叉注意力分离全局和局部文本提示；4. 采用递归采样方法生成长视频，结合关键帧预测和帧插值。

Result: X2Video能够生成长时间、时间一致且逼真的视频，在定性和定量评估中表现优异。同时支持通过参数调节对颜色、材质、几何和光照进行编辑。

Conclusion: X2Video通过内在通道指导和创新的注意力机制，成功实现了高质量视频生成，并提供了灵活的多模态控制能力，为视频生成领域带来了新的可能性。

Abstract: We present X2Video, the first diffusion model for rendering photorealistic videos guided by intrinsic channels including albedo, normal, roughness, metallicity, and irradiance, while supporting intuitive multi-modal controls with reference images and text prompts for both global and local regions. The intrinsic guidance allows accurate manipulation of color, material, geometry, and lighting, while reference images and text prompts provide intuitive adjustments in the absence of intrinsic information. To enable these functionalities, we extend the intrinsic-guided image generation model XRGB to video generation by employing a novel and efficient Hybrid Self-Attention, which ensures temporal consistency across video frames and also enhances fidelity to reference images. We further develop a Masked Cross-Attention to disentangle global and local text prompts, applying them effectively onto respective local and global regions. For generating long videos, our novel Recursive Sampling method incorporates progressive frame sampling, combining keyframe prediction and frame interpolation to maintain long-range temporal consistency while preventing error accumulation. To support the training of X2Video, we assembled a video dataset named InteriorVideo, featuring 1,154 rooms from 295 interior scenes, complete with reliable ground-truth intrinsic channel sequences and smooth camera trajectories. Both qualitative and quantitative evaluations demonstrate that X2Video can produce long, temporally consistent, and photorealistic videos guided by intrinsic conditions. Additionally, X2Video effectively accommodates multi-modal controls with reference images, global and local text prompts, and simultaneously supports editing on color, material, geometry, and lighting through parametric tuning. Project page: https://luckyhzt.github.io/x2video

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [8] [DynamicEval: Rethinking Evaluation for Dynamic Text-to-Video Synthesis](https://arxiv.org/abs/2510.07441)
*Nithin C. Babu,Aniruddha Mahapatra,Harsh Rangwani,Rajiv Soundararajan,Kuldeep Kulkarni*

Main category: cs.CV

TL;DR: DynamicEval是一个新的文本到视频评估基准，专注于动态相机运动评估，包含精心策划的提示语和45k人工标注，提出了背景场景一致性和前景对象一致性的新评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有T2V评估基准存在两个局限：忽视动态相机运动的重要性，以及将视频级评分聚合为模型级评分时忽略了视频级评估的重要性。

Method: 构建DynamicEval基准，包含强调动态相机运动的提示语和45k人工标注；提出背景一致性指标，基于VBench运动平滑度指标生成可解释错误图，并通过对象错误图修正遮挡/去遮挡问题；提出前景一致性指标，跟踪对象实例内的点及其邻居来评估对象保真度。

Result: 实验表明，提出的指标在视频级和模型级与人类偏好相关性更强（提升超过2个百分点），建立了更全面的动态相机运动T2V模型评估基准。

Conclusion: DynamicEval通过关注动态相机运动和视频级评估，提供了更全面的T2V模型评估框架，新提出的背景和前景一致性指标显著提升了与人类判断的一致性。

Abstract: Existing text-to-video (T2V) evaluation benchmarks, such as VBench and EvalCrafter, suffer from two limitations. (i) While the emphasis is on subject-centric prompts or static camera scenes, camera motion essential for producing cinematic shots and existing metrics under dynamic motion are largely unexplored. (ii) These benchmarks typically aggregate video-level scores into a single model-level score for ranking generative models. Such aggregation, however, overlook video-level evaluation, which is vital to selecting the better video among the candidate videos generated for a given prompt. To address these gaps, we introduce DynamicEval, a benchmark consisting of systematically curated prompts emphasizing dynamic camera motion, paired with 45k human annotations on video pairs from 3k videos generated by ten T2V models. DynamicEval evaluates two key dimensions of video quality: background scene consistency and foreground object consistency. For background scene consistency, we obtain the interpretable error maps based on the Vbench motion smoothness metric. We observe that while the Vbench motion smoothness metric shows promising alignment with human judgments, it fails in two cases: occlusions/disocclusions arising from camera and foreground object movements. Building on this, we propose a new background consistency metric that leverages object error maps to correct two failure cases in a principled manner. Our second innovation is the introduction of a foreground consistency metric that tracks points and their neighbors within each object instance to assess object fidelity. Extensive experiments demonstrate that our proposed metrics achieve stronger correlations with human preferences at both the video level and the model level (an improvement of more than 2% points), establishing DynamicEval as a more comprehensive benchmark for evaluating T2V models under dynamic camera motion.

</details>


### [9] [Provably Accelerated Imaging with Restarted Inertia and Score-based Image Priors](https://arxiv.org/abs/2510.07470)
*Marien Renaud,Julien Hermant,Deliang Wei,Yu Sun*

Main category: cs.CV

TL;DR: RISP是一种改进的RED算法，通过引入重启惯性和基于分数的图像先验，在保持高质量重建的同时实现快速收敛。


<details>
  <summary>Details</summary>
Motivation: 现有方法如RED专注于设计复杂图像先验来提高重建质量，但收敛加速依赖启发式方法。需要弥合这一差距。

Method: 提出RISP算法，结合重启惯性实现快速收敛，同时使用基于分数的图像先验进行高质量重建。

Result: 实验证明RISP在多种成像逆问题中实现快速收敛和高质量重建，理论分析显示其具有比RED更快的驻点收敛速率。

Conclusion: RISP为成像逆问题提供了既快速收敛又高质量重建的解决方案，建立了与重球ODE的理论联系。

Abstract: Fast convergence and high-quality image recovery are two essential features of algorithms for solving ill-posed imaging inverse problems. Existing methods, such as regularization by denoising (RED), often focus on designing sophisticated image priors to improve reconstruction quality, while leaving convergence acceleration to heuristics. To bridge the gap, we propose Restarted Inertia with Score-based Priors (RISP) as a principled extension of RED. RISP incorporates a restarting inertia for fast convergence, while still allowing score-based image priors for high-quality reconstruction. We prove that RISP attains a faster stationary-point convergence rate than RED, without requiring the convexity of the image prior. We further derive and analyze the associated continuous-time dynamical system, offering insight into the connection between RISP and the heavy-ball ordinary differential equation (ODE). Experiments across a range of imaging inverse problems demonstrate that RISP enables fast convergence while achieving high-quality reconstructions.

</details>


### [10] [A Denoising Framework for Real-World Ultra-Low Dose Lung CT Images Based on an Image Purification Strategy](https://arxiv.org/abs/2510.07492)
*Guoliang Gong,Man Yu*

Main category: cs.CV

TL;DR: 提出一种基于图像净化策略的超低剂量CT去噪框架，解决真实临床uLDCT与正常剂量CT图像间的空间错位问题，通过频率域流匹配模型实现优异的解剖结构保持效果。


<details>
  <summary>Details</summary>
Motivation: 超低剂量CT显著降低辐射剂量但引入严重噪声和伪影，导致uLDCT与NDCT图像对存在显著空间错位，现有去噪网络难以直接应用。

Method: 构建真实临床uLDCT肺部数据集；提出图像净化策略生成结构对齐的图像对；设计频率域流匹配模型与IP策略协同工作。

Result: 在真实临床数据集上，IP策略显著提升多种主流去噪模型性能；FFM模型结合IP策略在解剖结构保持方面达到最先进水平。

Conclusion: 本研究为真实世界uLDCT去噪中的数据不匹配问题提供了有效解决方案。

Abstract: Ultra-low dose CT (uLDCT) significantly reduces radiation exposure but introduces severe noise and artifacts. It also leads to substantial spatial misalignment between uLDCT and normal dose CT (NDCT) image pairs. This poses challenges for directly applying existing denoising networks trained on synthetic noise or aligned data. To address this core challenge in uLDCT denoising, this paper proposes an innovative denoising framework based on an Image Purification (IP) strategy. First, we construct a real clinical uLDCT lung dataset. Then, we propose an Image Purification strategy that generates structurally aligned uLDCT-NDCT image pairs, providing a high-quality data foundation for network training. Building upon this, we propose a Frequency-domain Flow Matching (FFM) model, which works synergistically with the IP strategy to excellently preserve the anatomical structure integrity of denoised images. Experiments on the real clinical dataset demonstrate that our IP strategy significantly enhances the performance of multiple mainstream denoising models on the uLDCT task. Notably, our proposed FFM model combined with the IP strategy achieves state-of-the-art (SOTA) results in anatomical structure preservation. This study provides an effective solution to the data mismatch problem in real-world uLDCT denoising. Code and dataset are available at https://github.com/MonkeyDadLufy/flow-matching.

</details>


### [11] [PickStyle: Video-to-Video Style Transfer with Context-Style Adapters](https://arxiv.org/abs/2510.07546)
*Soroush Mehraban,Vida Adeli,Jacob Rommann,Babak Taati,Kyryl Truskovskyi*

Main category: cs.CV

TL;DR: PickStyle是一个视频风格迁移框架，通过插入低秩适配器到预训练视频扩散模型中，利用配对的静态图像数据进行训练，实现内容保持和风格转换的视频风格迁移。


<details>
  <summary>Details</summary>
Motivation: 解决视频风格迁移任务中缺乏配对视频数据监督的挑战，同时保持视频内容与目标风格的强对齐。

Method: 在条件模块的自注意力层插入低秩适配器；从配对图像构建合成训练片段，应用共享增强模拟相机运动；提出上下文-风格分类器自由引导（CS-CFG）方法。

Result: 在多个基准测试中，该方法实现了时间一致、风格忠实且内容保持的视频转换，在定性和定量评估上均优于现有基线方法。

Conclusion: PickStyle框架通过有效的适配器设计和训练策略，成功解决了视频风格迁移中的数据稀缺问题，实现了高质量的视频风格转换。

Abstract: We address the task of video style transfer with diffusion models, where the goal is to preserve the context of an input video while rendering it in a target style specified by a text prompt. A major challenge is the lack of paired video data for supervision. We propose PickStyle, a video-to-video style transfer framework that augments pretrained video diffusion backbones with style adapters and benefits from paired still image data with source-style correspondences for training. PickStyle inserts low-rank adapters into the self-attention layers of conditioning modules, enabling efficient specialization for motion-style transfer while maintaining strong alignment between video content and style. To bridge the gap between static image supervision and dynamic video, we construct synthetic training clips from paired images by applying shared augmentations that simulate camera motion, ensuring temporal priors are preserved. In addition, we introduce Context-Style Classifier-Free Guidance (CS-CFG), a novel factorization of classifier-free guidance into independent text (style) and video (context) directions. CS-CFG ensures that context is preserved in generated video while the style is effectively transferred. Experiments across benchmarks show that our approach achieves temporally coherent, style-faithful, and content-preserving video translations, outperforming existing baselines both qualitatively and quantitatively.

</details>


### [12] [Rectified-CFG++ for Flow Based Models](https://arxiv.org/abs/2510.07631)
*Shreshth Saini,Shashank Gupta,Alan C. Bovik*

Main category: cs.CV

TL;DR: 提出Rectified-CFG++方法，解决标准CFG在整流流模型中的离流形漂移问题，通过自适应预测-校正引导确保轨迹在数据流形附近稳定运行。


<details>
  <summary>Details</summary>
Motivation: 标准CFG在整流流模型中会引起严重的离流形漂移，导致视觉伪影、文本不对齐和不稳定行为，需要一种几何感知的引导方法。

Method: 采用自适应预测-校正引导，结合整流流的确定性效率与几何感知条件规则。每个推理步骤先执行条件RF更新锚定样本，然后应用加权条件校正。

Result: 在Flux、Stable Diffusion 3/3.5、Lumina等大规模文生图模型上，在MS-COCO、LAION-Aesthetic、T2I-CompBench等基准数据集上一致优于标准CFG。

Conclusion: Rectified-CFG++通过确保轨迹在有界管状邻域内运行，实现了稳定性和性能提升，为整流流模型提供了有效的引导方法。

Abstract: Classifier-free guidance (CFG) is the workhorse for steering large diffusion models toward text-conditioned targets, yet its native application to rectified flow (RF) based models provokes severe off-manifold drift, yielding visual artifacts, text misalignment, and brittle behaviour. We present Rectified-CFG++, an adaptive predictor-corrector guidance that couples the deterministic efficiency of rectified flows with a geometry-aware conditioning rule. Each inference step first executes a conditional RF update that anchors the sample near the learned transport path, then applies a weighted conditional correction that interpolates between conditional and unconditional velocity fields. We prove that the resulting velocity field is marginally consistent and that its trajectories remain within a bounded tubular neighbourhood of the data manifold, ensuring stability across a wide range of guidance strengths. Extensive experiments on large-scale text-to-image models (Flux, Stable Diffusion 3/3.5, Lumina) show that Rectified-CFG++ consistently outperforms standard CFG on benchmark datasets such as MS-COCO, LAION-Aesthetic, and T2I-CompBench. Project page: https://rectified-cfgpp.github.io/

</details>


### [13] [Once Is Enough: Lightweight DiT-Based Video Virtual Try-On via One-Time Garment Appearance Injection](https://arxiv.org/abs/2510.07654)
*Yanjie Pan,Qingdong He,Lidong Wang,Bo Peng,Mingmin Chi*

Main category: cs.CV

TL;DR: 提出OIE方法，基于首帧服装替换的视频虚拟试穿策略，通过图像模型替换首帧服装，然后利用姿态和掩码信息引导视频生成模型合成后续帧，实现高效参数和计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于U-Net的双分支架构难以适应基于Diffusion Transformer的扩散模型，引入服装参考分支的潜在空间特征需要修改骨干网络导致参数过多，且服装潜在特征缺乏时间特性需要额外学习。

Method: 使用图像服装转移模型替换首帧服装，在编辑后的首帧内容控制下，利用姿态和掩码信息引导视频生成模型的时间先验，顺序合成剩余帧。

Result: 实验表明该方法在保持领先性能的同时，实现了优越的参数效率和计算效率。

Conclusion: OIE方法通过首帧替换策略有效解决了双分支架构在Diffusion Transformer中的适应问题，在参数和计算效率方面表现优异。

Abstract: Video virtual try-on aims to replace the clothing of a person in a video with a target garment. Current dual-branch architectures have achieved significant success in diffusion models based on the U-Net; however, adapting them to diffusion models built upon the Diffusion Transformer remains challenging. Initially, introducing latent space features from the garment reference branch requires adding or modifying the backbone network, leading to a large number of trainable parameters. Subsequently, the latent space features of garments lack inherent temporal characteristics and thus require additional learning. To address these challenges, we propose a novel approach, OIE (Once is Enough), a virtual try-on strategy based on first-frame clothing replacement: specifically, we employ an image-based clothing transfer model to replace the clothing in the initial frame, and then, under the content control of the edited first frame, utilize pose and mask information to guide the temporal prior of the video generation model in synthesizing the remaining frames sequentially. Experiments show that our method achieves superior parameter efficiency and computational efficiency while still maintaining leading performance under these constraints.

</details>


### [14] [Controllable Video Synthesis via Variational Inference](https://arxiv.org/abs/2510.07670)
*Haoyi Duan,Yunzhi Zhang,Yilun Du,Jiajun Wu*

Main category: cs.CV

TL;DR: 提出了一种视频合成方法，支持从精确4D对象轨迹到粗粒度文本提示的多粒度用户控制，通过变分推理和逐步KL散度最小化实现高可控性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型通常针对固定输入格式训练，无法满足需要混合粒度用户控制的工作流程需求。

Method: 将任务建模为变分推理，利用多个视频生成骨干网络共同处理所有任务约束，通过逐步KL散度最小化和上下文条件分解技术解决优化挑战。

Result: 实验表明该方法相比先前工作，在可控性、多样性和3D一致性方面产生更好的样本。

Conclusion: 该方法成功实现了视频生成中多粒度控制的需求，在保持未指定元素多样性的同时确保指定元素的高可控性。

Abstract: Many video workflows benefit from a mixture of user controls with varying granularity, from exact 4D object trajectories and camera paths to coarse text prompts, while existing video generative models are typically trained for fixed input formats. We develop a video synthesis method that addresses this need and generates samples with high controllability for specified elements while maintaining diversity for under-specified ones. We cast the task as variational inference to approximate a composed distribution, leveraging multiple video generation backbones to account for all task constraints collectively. To address the optimization challenge, we break down the problem into step-wise KL divergence minimization over an annealed sequence of distributions, and further propose a context-conditioned factorization technique that reduces modes in the solution space to circumvent local optima. Experiments suggest that our method produces samples with improved controllability, diversity, and 3D consistency compared to prior works.

</details>


### [15] [RePainter: Empowering E-commerce Object Removal via Spatial-matting Reinforcement Learning](https://arxiv.org/abs/2510.07721)
*Zipeng Guo,Lichen Ma,Xiaolong Fu,Gaojing Zhou,Lan Yang,Yuchen Zhou,Linkai Liu,Yu He,Ximan Liu,Shiping Dong,Jingling Fu,Zhen Chen,Yu Shi,Junshi Huang,Jason Li,Chao Gou*

Main category: cs.CV

TL;DR: Repainter是一个基于强化学习的图像修复框架，通过空间遮罩轨迹优化和GRPO算法，专门解决电商产品图像中水印和促销文本的去除问题，在复杂场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 电商平台中产品图像的水印和促销文本等侵入性元素影响了视觉效果和广告效果，现有扩散修复方法在商业场景中存在对象移除不可靠和领域适应性有限的问题。

Method: 提出Repainter框架，集成空间遮罩轨迹优化和Group Relative Policy Optimization (GRPO)，通过调节注意力机制强调背景上下文，引入复合奖励机制平衡全局、局部和语义约束。

Result: 在EcomPaint-100K数据集上的实验表明，Repainter在复杂构图场景中显著优于现有最先进方法，有效减少视觉伪影和奖励黑客问题。

Conclusion: Repainter通过强化学习框架成功解决了电商图像修复的挑战，提供了高质量的修复效果，特别是在复杂场景中表现优异。

Abstract: In web data, product images are central to boosting user engagement and advertising efficacy on e-commerce platforms, yet the intrusive elements such as watermarks and promotional text remain major obstacles to delivering clear and appealing product visuals. Although diffusion-based inpainting methods have advanced, they still face challenges in commercial settings due to unreliable object removal and limited domain-specific adaptation. To tackle these challenges, we propose Repainter, a reinforcement learning framework that integrates spatial-matting trajectory refinement with Group Relative Policy Optimization (GRPO). Our approach modulates attention mechanisms to emphasize background context, generating higher-reward samples and reducing unwanted object insertion. We also introduce a composite reward mechanism that balances global, local, and semantic constraints, effectively reducing visual artifacts and reward hacking. Additionally, we contribute EcomPaint-100K, a high-quality, large-scale e-commerce inpainting dataset, and a standardized benchmark EcomPaint-Bench for fair evaluation. Extensive experiments demonstrate that Repainter significantly outperforms state-of-the-art methods, especially in challenging scenes with intricate compositions. We will release our code and weights upon acceptance.

</details>


### [16] [ComGS: Efficient 3D Object-Scene Composition via Surface Octahedral Probes](https://arxiv.org/abs/2510.07729)
*Jian Gao,Mengqi Yuan,Yifei Zeng,Chang Zeng,Zhihao Li,Zhenyu Chen,Weichao Qiu,Xiao-Xiao Long,Hao Zhu,Xun Cao,Yao Yao*

Main category: cs.CV

TL;DR: 提出了ComGS框架，通过表面八面体探针(SOPs)实现高效可重光照物体重建，结合场景光照估计技术，解决3D物体-场景合成中的光照不一致问题，实现实时渲染和快速编辑。


<details>
  <summary>Details</summary>
Motivation: 高斯泼溅(GS)技术虽然能实现沉浸式渲染，但在3D物体-场景合成时存在烘培外观和阴影信息导致的不一致问题，需要可重光照物体重建和场景光照估计。

Method: 1. 使用表面八面体探针(SOPs)存储光照和遮挡信息，通过插值实现高效3D查询，避免昂贵的射线追踪；2. 在物体放置位置捕获360度重建辐射场，微调扩散模型完成光照估计。

Result: 实现了至少2倍的重建加速，支持高斯场景中的实时阴影计算，渲染质量高，视觉和谐，具有生动的阴影效果，实时渲染约28 FPS，编辑仅需36秒。

Conclusion: ComGS框架成功解决了3D物体-场景合成中的光照一致性问题，实现了高效、实时的渲染和快速编辑，为沉浸式3D场景合成提供了有效解决方案。

Abstract: Gaussian Splatting (GS) enables immersive rendering, but realistic 3D object-scene composition remains challenging. Baked appearance and shadow information in GS radiance fields cause inconsistencies when combining objects and scenes. Addressing this requires relightable object reconstruction and scene lighting estimation. For relightable object reconstruction, existing Gaussian-based inverse rendering methods often rely on ray tracing, leading to low efficiency. We introduce Surface Octahedral Probes (SOPs), which store lighting and occlusion information and allow efficient 3D querying via interpolation, avoiding expensive ray tracing. SOPs provide at least a 2x speedup in reconstruction and enable real-time shadow computation in Gaussian scenes. For lighting estimation, existing Gaussian-based inverse rendering methods struggle to model intricate light transport and often fail in complex scenes, while learning-based methods predict lighting from a single image and are viewpoint-sensitive. We observe that 3D object-scene composition primarily concerns the object's appearance and nearby shadows. Thus, we simplify the challenging task of full scene lighting estimation by focusing on the environment lighting at the object's placement. Specifically, we capture a 360 degrees reconstructed radiance field of the scene at the location and fine-tune a diffusion model to complete the lighting. Building on these advances, we propose ComGS, a novel 3D object-scene composition framework. Our method achieves high-quality, real-time rendering at around 28 FPS, produces visually harmonious results with vivid shadows, and requires only 36 seconds for editing. Code and dataset are available at https://nju-3dv.github.io/projects/ComGS/.

</details>


### [17] [UltraLED: Learning to See Everything in Ultra-High Dynamic Range Scenes](https://arxiv.org/abs/2510.07741)
*Yuang Meng,Xin Jin,Lina Lei,Chun-Le Guo,Chongyi Li*

Main category: cs.CV

TL;DR: 提出UltraLED框架，仅使用单帧短曝光RAW图像实现超高动态范围场景重建，通过曝光校正和亮度感知RAW去噪来恢复暗部细节。


<details>
  <summary>Details</summary>
Motivation: 解决超高动态范围场景中同时保留高光和阴影细节的挑战，避免传统RGB包围曝光方法的错位和重影问题。

Method: 两阶段框架：首先通过比率图进行曝光校正平衡动态范围，然后使用亮度感知RAW去噪器增强暗部细节恢复。

Result: 在合成UHDR数据集上的广泛实验表明，UltraLED显著优于现有的单帧方法。

Conclusion: 仅使用单帧短曝光RAW图像即可有效重建UHDR场景，避免了动态场景中的重影和运动模糊问题。

Abstract: Ultra-high dynamic range (UHDR) scenes exhibit significant exposure disparities between bright and dark regions. Such conditions are commonly encountered in nighttime scenes with light sources. Even with standard exposure settings, a bimodal intensity distribution with boundary peaks often emerges, making it difficult to preserve both highlight and shadow details simultaneously. RGB-based bracketing methods can capture details at both ends using short-long exposure pairs, but are susceptible to misalignment and ghosting artifacts. We found that a short-exposure image already retains sufficient highlight detail. The main challenge of UHDR reconstruction lies in denoising and recovering information in dark regions. In comparison to the RGB images, RAW images, thanks to their higher bit depth and more predictable noise characteristics, offer greater potential for addressing this challenge. This raises a key question: can we learn to see everything in UHDR scenes using only a single short-exposure RAW image? In this study, we rely solely on a single short-exposure frame, which inherently avoids ghosting and motion blur, making it particularly robust in dynamic scenes. To achieve that, we introduce UltraLED, a two-stage framework that performs exposure correction via a ratio map to balance dynamic range, followed by a brightness-aware RAW denoiser to enhance detail recovery in dark regions. To support this setting, we design a 9-stop bracketing pipeline to synthesize realistic UHDR images and contribute a corresponding dataset based on diverse scenes, using only the shortest exposure as input for reconstruction. Extensive experiments show that UltraLED significantly outperforms existing single-frame approaches. Our code and dataset are made publicly available at https://srameo.github.io/projects/ultraled.

</details>


### [18] [DEGS: Deformable Event-based 3D Gaussian Splatting from RGB and Event Stream](https://arxiv.org/abs/2510.07752)
*Junhao He,Jiaxu Wang,Jia Li,Mingyuan Sun,Qiang Zhang,Jiahang Cao,Ziyi Zhang,Yi Gu,Jingkai Sun,Renjing Xu*

Main category: cs.CV

TL;DR: 提出了一种结合低帧率RGB视频和高帧率事件流来重建动态3D高斯溅射的新框架，通过事件运动先验指导变形场优化，解决了大帧间运动带来的不确定性挑战。


<details>
  <summary>Details</summary>
Motivation: 从低帧率RGB视频重建动态3D高斯溅射具有挑战性，因为大帧间运动会增加解空间的不确定性。事件相机能异步捕捉快速视觉变化且对运动模糊鲁棒，但缺乏颜色信息。结合两种模态可以解决这一挑战。

Method: 采用事件运动先验指导变形场优化：1）使用LoCM无监督微调框架从事件流中提取运动先验；2）提出几何感知数据关联方法建立事件-高斯运动对应关系；3）使用运动分解和帧间伪标签策略。

Result: 在合成和真实场景上的广泛实验表明，该方法优于现有的基于图像和事件的方法，证明能有效利用事件数据优化动态3D高斯溅射。

Conclusion: 通过结合RGB和事件模态，利用事件运动先验指导变形场优化，成功解决了低帧率视频重建动态3D高斯溅射中大帧间运动带来的挑战。

Abstract: Reconstructing Dynamic 3D Gaussian Splatting (3DGS) from low-framerate RGB videos is challenging. This is because large inter-frame motions will increase the uncertainty of the solution space. For example, one pixel in the first frame might have more choices to reach the corresponding pixel in the second frame. Event cameras can asynchronously capture rapid visual changes and are robust to motion blur, but they do not provide color information. Intuitively, the event stream can provide deterministic constraints for the inter-frame large motion by the event trajectories. Hence, combining low-temporal-resolution images with high-framerate event streams can address this challenge. However, it is challenging to jointly optimize Dynamic 3DGS using both RGB and event modalities due to the significant discrepancy between these two data modalities. This paper introduces a novel framework that jointly optimizes dynamic 3DGS from the two modalities. The key idea is to adopt event motion priors to guide the optimization of the deformation fields. First, we extract the motion priors encoded in event streams by using the proposed LoCM unsupervised fine-tuning framework to adapt an event flow estimator to a certain unseen scene. Then, we present the geometry-aware data association method to build the event-Gaussian motion correspondence, which is the primary foundation of the pipeline, accompanied by two useful strategies, namely motion decomposition and inter-frame pseudo-label. Extensive experiments show that our method outperforms existing image and event-based approaches across synthetic and real scenes and prove that our method can effectively optimize dynamic 3DGS with the help of event data.

</details>


### [19] [PrismGS: Physically-Grounded Anti-Aliasing for High-Fidelity Large-Scale 3D Gaussian Splatting](https://arxiv.org/abs/2510.07830)
*Houqiang Zhong,Zhenglong Wu,Sihua Fu,Zihan Zheng,Xin Jin,Xiaoyun Zhang,Li Song,Qiang Hu*

Main category: cs.CV

TL;DR: PrismGS是一个基于物理正则化的框架，用于改进3D高斯泼溅在大规模城市场景中的渲染质量，通过多尺度监督和尺寸正则化解决高分辨率渲染时的走样问题。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅在大型城市场景中会出现严重的走样伪影和优化不稳定问题，特别是在4K高分辨率渲染时，表现为闪烁纹理和锯齿边缘。现有方法虽然解决了可扩展性问题，但未能解决这种保真度差距。

Method: 提出PrismGS框架，包含两个协同的正则化器：1) 金字塔多尺度监督，通过对预滤波图像金字塔进行监督来强制模型学习抗走样表示；2) 显式尺寸正则化，对3D高斯的尺寸施加物理基础的下界约束，防止形成退化的视图依赖基元。

Result: 在MatrixCity、Mill-19和UrbanScene3D数据集上的实验表明，PrismGS实现了最先进的性能，相比CityGaussian获得了约1.5dB的PSNR提升，同时在4K渲染下保持优越的质量和鲁棒性。

Conclusion: PrismGS是一个即插即用的框架，能够显著改善3D高斯泼溅在大规模城市场景中的渲染质量，有效解决高分辨率渲染时的走样问题，同时与现有流水线兼容。

Abstract: 3D Gaussian Splatting (3DGS) has recently enabled real-time photorealistic rendering in compact scenes, but scaling to large urban environments introduces severe aliasing artifacts and optimization instability, especially under high-resolution (e.g., 4K) rendering. These artifacts, manifesting as flickering textures and jagged edges, arise from the mismatch between Gaussian primitives and the multi-scale nature of urban geometry. While existing ``divide-and-conquer'' pipelines address scalability, they fail to resolve this fidelity gap. In this paper, we propose PrismGS, a physically-grounded regularization framework that improves the intrinsic rendering behavior of 3D Gaussians. PrismGS integrates two synergistic regularizers. The first is pyramidal multi-scale supervision, which enforces consistency by supervising the rendering against a pre-filtered image pyramid. This compels the model to learn an inherently anti-aliased representation that remains coherent across different viewing scales, directly mitigating flickering textures. This is complemented by an explicit size regularization that imposes a physically-grounded lower bound on the dimensions of the 3D Gaussians. This prevents the formation of degenerate, view-dependent primitives, leading to more stable and plausible geometric surfaces and reducing jagged edges. Our method is plug-and-play and compatible with existing pipelines. Extensive experiments on MatrixCity, Mill-19, and UrbanScene3D demonstrate that PrismGS achieves state-of-the-art performance, yielding significant PSNR gains around 1.5 dB against CityGaussian, while maintaining its superior quality and robustness under demanding 4K rendering.

</details>


### [20] [TTOM: Test-Time Optimization and Memorization for Compositional Video Generation](https://arxiv.org/abs/2510.07940)
*Leigang Qu,Ziyang Wang,Na Zheng,Wenjie Wang,Liqiang Nie,Tat-Seng Chua*

Main category: cs.CV

TL;DR: TTOM是一个无需训练的视频生成框架，通过测试时优化和记忆机制，在推理时对齐视频生成模型与时空布局，提升文本-图像对齐效果。


<details>
  <summary>Details</summary>
Motivation: 视频基础模型在组合场景（如运动、数字、空间关系）中表现不佳，需要改进文本-图像对齐能力。

Method: 提出TTOM框架：1）集成并优化新参数，使用通用布局-注意力目标；2）在流式设置中生成视频；3）使用参数化记忆机制维护历史优化上下文，支持插入、读取、更新和删除操作。

Result: 在T2V-CompBench和Vbench基准测试中，TTOM被证明是有效、实用、可扩展且高效的框架，能够实现组合视频生成的跨模态对齐。

Conclusion: TTOM能够解耦组合性世界知识，展现出强大的可迁移性和泛化能力，为组合视频生成提供了一种有效的实时对齐解决方案。

Abstract: Video Foundation Models (VFMs) exhibit remarkable visual generation performance, but struggle in compositional scenarios (e.g., motion, numeracy, and spatial relation). In this work, we introduce Test-Time Optimization and Memorization (TTOM), a training-free framework that aligns VFM outputs with spatiotemporal layouts during inference for better text-image alignment. Rather than direct intervention to latents or attention per-sample in existing work, we integrate and optimize new parameters guided by a general layout-attention objective. Furthermore, we formulate video generation within a streaming setting, and maintain historical optimization contexts with a parametric memory mechanism that supports flexible operations, such as insert, read, update, and delete. Notably, we found that TTOM disentangles compositional world knowledge, showing powerful transferability and generalization. Experimental results on the T2V-CompBench and Vbench benchmarks establish TTOM as an effective, practical, scalable, and efficient framework to achieve cross-modal alignment for compositional video generation on the fly.

</details>


### [21] [CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving](https://arxiv.org/abs/2510.07944)
*Tianrui Zhang,Yichen Liu,Zilin Guo,Yuxin Guo,Jingcheng Ni,Chenjing Ding,Dan Xu,Lewei Lu,Zehuan Wu*

Main category: cs.CV

TL;DR: 提出了CVD-STORM模型，这是一个基于空间-时间重建VAE的跨视角视频扩散模型，能够生成具有4D重建能力的多视角长视频，并在各种控制输入下提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶技术的发展，不仅需要高保真度的可控视频生成，还需要产生深度估计等多样且有意义的几何信息，以支持全面的场景理解。

Method: 首先通过辅助4D重建任务微调VAE，增强其编码3D结构和时间动态的能力，然后将该VAE集成到视频扩散过程中以显著提升生成质量，同时联合训练高斯泼溅解码器进行动态场景重建。

Result: 实验结果表明，该模型在FID和FVD指标上均实现了显著提升，并且联合训练的高斯泼溅解码器能够有效重建动态场景，为场景理解提供有价值的几何信息。

Conclusion: CVD-STORM模型成功实现了高质量的多视角视频生成和4D场景重建，为自动驾驶等应用提供了全面的场景理解能力。

Abstract: Generative models have been widely applied to world modeling for environment simulation and future state prediction. With advancements in autonomous driving, there is a growing demand not only for high-fidelity video generation under various controls, but also for producing diverse and meaningful information such as depth estimation. To address this, we propose CVD-STORM, a cross-view video diffusion model utilizing a spatial-temporal reconstruction Variational Autoencoder (VAE) that generates long-term, multi-view videos with 4D reconstruction capabilities under various control inputs. Our approach first fine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing its ability to encode 3D structures and temporal dynamics. Subsequently, we integrate this VAE into the video diffusion process to significantly improve generation quality. Experimental results demonstrate that our model achieves substantial improvements in both FID and FVD metrics. Additionally, the jointly-trained Gaussian Splatting Decoder effectively reconstructs dynamic scenes, providing valuable geometric information for comprehensive scene understanding.

</details>


### [22] [Latent Harmony: Synergistic Unified UHD Image Restoration via Latent Space Regularization and Controllable Refinement](https://arxiv.org/abs/2510.07961)
*Yidi Liu,Xueyang Fu,Jie Huang,Jie Xiao,Dong Li,Wenlong Zhang,Lei Bai,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: Latent Harmony是一个两阶段UHD图像修复框架，通过联合正则化潜在空间和强制高频感知重建，在保持计算效率的同时提升高频细节保留能力。


<details>
  <summary>Details</summary>
Motivation: UHD图像修复面临计算效率与高频细节保留的权衡问题，传统VAE的高斯约束会丢弃退化特定的高频信息，影响重建保真度。

Method: 两阶段框架：第一阶段提出LH-VAE，通过视觉语义约束和渐进退化扰动增强语义鲁棒性；第二阶段使用HF-LoRA联合训练，包含编码器LoRA（保真度导向）和解码器LoRA（感知导向），通过交替优化和选择性梯度传播训练。

Result: 实验表明Latent Harmony在UHD和标准分辨率任务中达到最先进性能，有效平衡效率、感知质量和重建精度。

Conclusion: 该框架通过重新定义VAE用于UHD修复，成功解决了计算效率与高频细节保留的权衡问题，实现了灵活的保真度-感知权衡。

Abstract: Ultra-High Definition (UHD) image restoration faces a trade-off between computational efficiency and high-frequency detail retention. While Variational Autoencoders (VAEs) improve efficiency via latent-space processing, their Gaussian constraint often discards degradation-specific high-frequency information, hurting reconstruction fidelity. To overcome this, we propose Latent Harmony, a two-stage framework that redefines VAEs for UHD restoration by jointly regularizing the latent space and enforcing high-frequency-aware reconstruction.In Stage One, we introduce LH-VAE, which enhances semantic robustness through visual semantic constraints and progressive degradation perturbations, while latent equivariance strengthens high-frequency reconstruction.Stage Two jointly trains this refined VAE with a restoration model using High-Frequency Low-Rank Adaptation (HF-LoRA): an encoder LoRA guided by a fidelity-oriented high-frequency alignment loss to recover authentic details, and a decoder LoRA driven by a perception-oriented loss to synthesize realistic textures. Both LoRA modules are trained via alternating optimization with selective gradient propagation to preserve the pretrained latent structure.At inference, a tunable parameter {\alpha} enables flexible fidelity-perception trade-offs.Experiments show Latent Harmony achieves state-of-the-art performance across UHD and standard-resolution tasks, effectively balancing efficiency, perceptual quality, and reconstruction accuracy.

</details>


### [23] [Efficient Label Refinement for Face Parsing Under Extreme Poses Using 3D Gaussian Splatting](https://arxiv.org/abs/2510.08096)
*Ankit Gahlawat,Anirban Mukherjee,Dinesh Babu Jayagopi*

Main category: cs.CV

TL;DR: 提出了一种利用3D高斯溅射从多视角噪声预测中生成精确分割掩码的标签细化流程，无需3D标注即可显著提升极端视角下的人脸解析精度。


<details>
  <summary>Details</summary>
Motivation: 极端视角下的人脸解析由于缺乏标注数据而面临挑战，手动标注成本高且难以规模化。

Method: 联合拟合两个3D高斯溅射模型（RGB图像和初始分割图），通过共享几何实现多视角一致性，合成姿态多样的训练数据。

Result: 在具有挑战性的头部姿态上显著提升了解析精度，同时在标准视角上保持强性能，优于现有方法。

Conclusion: 该方法为提升真实场景中人脸解析的鲁棒性提供了可扩展且有效的解决方案。

Abstract: Accurate face parsing under extreme viewing angles remains a significant challenge due to limited labeled data in such poses. Manual annotation is costly and often impractical at scale. We propose a novel label refinement pipeline that leverages 3D Gaussian Splatting (3DGS) to generate accurate segmentation masks from noisy multiview predictions. By jointly fitting two 3DGS models, one to RGB images and one to their initial segmentation maps, our method enforces multiview consistency through shared geometry, enabling the synthesis of pose-diverse training data with only minimal post-processing. Fine-tuning a face parsing model on this refined dataset significantly improves accuracy on challenging head poses, while maintaining strong performance on standard views. Extensive experiments, including human evaluations, demonstrate that our approach achieves superior results compared to state-of-the-art methods, despite requiring no ground-truth 3D annotations and using only a small set of initial images. Our method offers a scalable and effective solution for improving face parsing robustness in real-world settings.

</details>


### [24] [Real-Time Motion-Controllable Autoregressive Video Diffusion](https://arxiv.org/abs/2510.08131)
*Kesen Zhao,Jiaxin Shi,Beier Zhu,Junbao Zhou,Xiaolong Shen,Yuan Zhou,Qianru Sun,Hanwang Zhang*

Main category: cs.CV

TL;DR: AR-Drag是首个RL增强的少步自回归视频扩散模型，用于实时图像到视频生成，支持多样化运动控制，显著降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 解决实时运动可控视频生成的挑战：双向扩散模型的固有延迟问题，以及现有自回归方法在少步生成中的质量下降和运动伪影问题。

Method: 首先微调基础I2V模型支持基本运动控制，然后通过基于轨迹的奖励模型进行强化学习改进。通过自滚动机制保持马尔可夫性质，并在去噪步骤中选择性引入随机性来加速训练。

Result: 实验表明AR-Drag实现了高视觉保真度和精确的运动对齐，相比最先进的运动可控VDM显著降低延迟，仅使用13亿参数。

Conclusion: AR-Drag成功解决了实时运动可控视频生成的挑战，在保持高质量的同时大幅降低了延迟，为实时视频生成提供了有效解决方案。

Abstract: Real-time motion-controllable video generation remains challenging due to the inherent latency of bidirectional diffusion models and the lack of effective autoregressive (AR) approaches. Existing AR video diffusion models are limited to simple control signals or text-to-video generation, and often suffer from quality degradation and motion artifacts in few-step generation. To address these challenges, we propose AR-Drag, the first RL-enhanced few-step AR video diffusion model for real-time image-to-video generation with diverse motion control. We first fine-tune a base I2V model to support basic motion control, then further improve it via reinforcement learning with a trajectory-based reward model. Our design preserves the Markov property through a Self-Rollout mechanism and accelerates training by selectively introducing stochasticity in denoising steps. Extensive experiments demonstrate that AR-Drag achieves high visual fidelity and precise motion alignment, significantly reducing latency compared with state-of-the-art motion-controllable VDMs, while using only 1.3B parameters. Additional visualizations can be found on our project page: https://kesenzhao.github.io/AR-Drag.github.io/.

</details>


### [25] [UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video Super-Resolution](https://arxiv.org/abs/2510.08143)
*Shian Du,Menghan Xia,Chang Liu,Quande Liu,Xintao Wang,Pengfei Wan,Xiangyang Ji*

Main category: cs.CV

TL;DR: UniMMVSR是一个统一的多模态视频超分辨率框架，能够整合文本、图像和视频等多种条件，在潜在视频扩散模型中实现高质量视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有级联视频超分辨率方法主要局限于文本到视频任务，无法利用文本以外的生成条件，这在多模态视频生成中对于保证保真度至关重要。

Method: 在潜在视频扩散模型中探索条件注入策略、训练方案和数据混合技术，设计不同的数据构建和条件利用方法，使模型能够精确利用所有条件类型。

Result: UniMMVSR显著优于现有方法，生成具有更优细节和更高多模态条件符合度的视频，并能与基础模型结合实现4K视频的多模态引导生成。

Conclusion: 该框架成功解决了多模态条件利用的挑战，为高质量视频生成提供了新的可能性。

Abstract: Cascaded video super-resolution has emerged as a promising technique for decoupling the computational burden associated with generating high-resolution videos using large foundation models. Existing studies, however, are largely confined to text-to-video tasks and fail to leverage additional generative conditions beyond text, which are crucial for ensuring fidelity in multi-modal video generation. We address this limitation by presenting UniMMVSR, the first unified generative video super-resolution framework to incorporate hybrid-modal conditions, including text, images, and videos. We conduct a comprehensive exploration of condition injection strategies, training schemes, and data mixture techniques within a latent video diffusion model. A key challenge was designing distinct data construction and condition utilization methods to enable the model to precisely utilize all condition types, given their varied correlations with the target video. Our experiments demonstrate that UniMMVSR significantly outperforms existing methods, producing videos with superior detail and a higher degree of conformity to multi-modal conditions. We also validate the feasibility of combining UniMMVSR with a base model to achieve multi-modal guided generation of 4K video, a feat previously unattainable with existing techniques.

</details>


### [26] [One Stone with Two Birds: A Null-Text-Null Frequency-Aware Diffusion Models for Text-Guided Image Inpainting](https://arxiv.org/abs/2510.08273)
*Haipeng Liu,Yang Wang,Meng Wang*

Main category: cs.CV

TL;DR: 提出NTN-Diff模型，通过频率感知的扩散模型解决文本引导图像修复中的语义一致性和未掩码区域保护问题


<details>
  <summary>Details</summary>
Motivation: 现有方法无法同时解决未掩码区域保护和掩码/未掩码区域语义一致性两个挑战，主要原因是混合频率带的纠缠导致对文本提示的鲁棒性不同

Method: 将去噪过程分为早期和晚期阶段，在早期阶段使用文本引导去噪稳定中频带，然后使用空文本去噪处理低频带，最后在晚期阶段进行文本引导去噪

Result: 实验验证NTN-Diff在文本引导扩散模型中优于现有最先进方法

Conclusion: 通过频率分解和分阶段去噪策略，NTN-Diff成功解决了文本引导图像修复中的关键挑战

Abstract: Text-guided image inpainting aims at reconstructing the masked regions as per text prompts, where the longstanding challenges lie in the preservation for unmasked regions, while achieving the semantics consistency between unmasked and inpainted masked regions. Previous arts failed to address both of them, always with either of them to be remedied. Such facts, as we observed, stem from the entanglement of the hybrid (e.g., mid-and-low) frequency bands that encode varied image properties, which exhibit different robustness to text prompts during the denoising process. In this paper, we propose a null-text-null frequency-aware diffusion models, dubbed \textbf{NTN-Diff}, for text-guided image inpainting, by decomposing the semantics consistency across masked and unmasked regions into the consistencies as per each frequency band, while preserving the unmasked regions, to circumvent two challenges in a row. Based on the diffusion process, we further divide the denoising process into early (high-level noise) and late (low-level noise) stages, where the mid-and-low frequency bands are disentangled during the denoising process. As observed, the stable mid-frequency band is progressively denoised to be semantically aligned during text-guided denoising process, which, meanwhile, serves as the guidance to the null-text denoising process to denoise low-frequency band for the masked regions, followed by a subsequent text-guided denoising process at late stage, to achieve the semantics consistency for mid-and-low frequency bands across masked and unmasked regions, while preserve the unmasked regions. Extensive experiments validate the superiority of NTN-Diff over the state-of-the-art diffusion models to text-guided diffusion models. Our code can be accessed from https://github.com/htyjers/NTN-Diff.

</details>


### [27] [LinVideo: A Post-Training Framework towards O(n) Attention in Efficient Video Generation](https://arxiv.org/abs/2510.08318)
*Yushi Huang,Xingtong Ge,Ruihao Gong,Chengtao Lv,Jun Zhang*

Main category: cs.CV

TL;DR: LinVideo是一个高效的数据无关后训练框架，通过将自注意力模块替换为线性注意力来加速视频扩散模型，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型的计算成本随序列长度呈二次方增长，而完全替换二次方注意力需要昂贵的预训练。现有线性注意力方法表达能力有限，时空建模复杂。

Method: 提出选择性迁移方法，将层选择作为二元分类问题自动渐进转换；引入任意时间分布匹配目标，在采样轨迹的任何时间步对齐样本分布。

Result: 实现1.25-2.00倍加速并保持生成质量，4步蒸馏模型进一步实现15.92倍延迟降低且视觉质量下降最小。

Conclusion: LinVideo框架有效解决了视频扩散模型的计算效率问题，在保持性能的同时显著提升推理速度。

Abstract: Video diffusion models (DMs) have enabled high-quality video synthesis. However, their computation costs scale quadratically with sequence length because self-attention has quadratic complexity. While linear attention lowers the cost, fully replacing quadratic attention requires expensive pretraining due to the limited expressiveness of linear attention and the complexity of spatiotemporal modeling in video generation. In this paper, we present LinVideo, an efficient data-free post-training framework that replaces a target number of self-attention modules with linear attention while preserving the original model's performance. First, we observe a significant disparity in the replaceability of different layers. Instead of manual or heuristic choices, we frame layer selection as a binary classification problem and propose selective transfer, which automatically and progressively converts layers to linear attention with minimal performance impact. Additionally, to overcome the ineffectiveness and inefficiency of existing objectives for this transfer process, we introduce an anytime distribution matching (ADM) objective that aligns the distributions of samples across any timestep along the sampling trajectory. This objective is efficient and recovers model performance. Extensive experiments show that our method achieves a 1.25-2.00x speedup while preserving generation quality, and our 4-step distilled model further delivers a 15.92x latency reduction with minimal visual quality drop.

</details>


### [28] [SPICE: Simple and Practical Image Clarification and Enhancement](https://arxiv.org/abs/2510.08358)
*Alexander Belyaev,Pierre-Alain Fayolle,Michael Cohen*

Main category: cs.CV

TL;DR: 提出一种简单高效的图像增强方法，用于低光照图像增强和雾霾图像（雾天、沙尘、水下图像）的清晰化处理


<details>
  <summary>Details</summary>
Motivation: 解决低光照图像和雾霾图像的质量问题，提供简单有效的增强方案

Method: 构建图像滤波器模拟低光或雾霾条件，推导近似反向滤波器以最小化增强图像中的失真

Result: 实验结果表明该方法在处理极暗图像和增强雾霾图像方面具有高度竞争力，甚至优于最先进技术

Conclusion: 该方法的关键优势在于其简单性，仅需几行MATLAB代码即可实现

Abstract: We introduce a simple and efficient method to enhance and clarify images. More specifically, we deal with low light image enhancement and clarification of hazy imagery (hazy/foggy images, images containing sand dust, and underwater images). Our method involves constructing an image filter to simulate low-light or hazy conditions and deriving approximate reverse filters to minimize distortions in the enhanced images. Experimental results show that our approach is highly competitive and often surpasses state-of-the-art techniques in handling extremely dark images and in enhancing hazy images. A key advantage of our approach lies in its simplicity: Our method is implementable with just a few lines of MATLAB code.

</details>


### [29] [Hyperspectral data augmentation with transformer-based diffusion models](https://arxiv.org/abs/2510.08363)
*Mattia Ferrari,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 提出了一种基于引导扩散模型的数据增强技术，结合轻量级Transformer网络、改进的加权损失函数和优化的余弦方差调度器，用于解决小样本高光谱图像分类中的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 新一代高光谱卫星传感器与深度学习方法结合，能够在大中尺度上区分详细的地物类别，但小样本训练时容易过拟合。

Method: 使用引导扩散模型进行数据增强，结合轻量级Transformer网络捕获复杂数据模式，采用改进的加权损失函数和优化的余弦方差调度器实现小数据集上的快速有效训练。

Result: 在PRISMA卫星获取的10种森林类型高光谱图像分类任务中，该方法在平均精度和加权平均精度上均优于其他数据增强技术，且训练过程稳定。

Conclusion: 该方法有效解决了深度生成模型在实际应用中常见的训练不稳定问题，为小样本高光谱图像分类提供了有效的解决方案。

Abstract: The introduction of new generation hyperspectral satellite sensors, combined with advancements in deep learning methodologies, has significantly enhanced the ability to discriminate detailed land-cover classes at medium-large scales. However, a significant challenge in deep learning methods is the risk of overfitting when training networks with small labeled datasets. In this work, we propose a data augmentation technique that leverages a guided diffusion model. To effectively train the model with a limited number of labeled samples and to capture complex patterns in the data, we implement a lightweight transformer network. Additionally, we introduce a modified weighted loss function and an optimized cosine variance scheduler, which facilitate fast and effective training on small datasets. We evaluate the effectiveness of the proposed method on a forest classification task with 10 different forest types using hyperspectral images acquired by the PRISMA satellite. The results demonstrate that the proposed method outperforms other data augmentation techniques in both average and weighted average accuracy. The effectiveness of the method is further highlighted by the stable training behavior of the model, which addresses a common limitation in the practical application of deep generative models for data augmentation.

</details>


### [30] [UniVideo: Unified Understanding, Generation, and Editing for Videos](https://arxiv.org/abs/2510.08377)
*Cong Wei,Quande Liu,Zixuan Ye,Qiulin Wang,Xintao Wang,Pengfei Wan,Kun Gai,Wenhu Chen*

Main category: cs.CV

TL;DR: UniVideo是一个统一的多模态视频生成和编辑框架，通过双流设计（MLLM理解指令+MMDiT生成视频）实现多种视频任务的统一处理，支持任务组合和跨任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的统一多模态模型主要局限于图像领域，本文旨在将统一建模扩展到视频领域，解决复杂多模态指令理解和视觉一致性问题。

Method: 采用双流架构：多模态大语言模型（MLLM）负责理解指令，多模态DiT（MMDiT）负责视频生成。通过联合训练统一处理文本/图像到视频生成、上下文视频生成和编辑等任务。

Result: UniVideo在文本/图像到视频生成、上下文视频生成和编辑等任务上达到或超越最先进的特定任务基线，并展现出任务组合和跨任务泛化的能力。

Conclusion: UniVideo证明了统一多模态建模在视频领域的可行性，支持任务组合和零样本泛化，为未来视频生成研究提供了新方向。

Abstract: Unified multimodal models have shown promising results in multimodal content generation and editing but remain largely limited to the image domain. In this work, we present UniVideo, a versatile framework that extends unified modeling to the video domain. UniVideo adopts a dual-stream design, combining a Multimodal Large Language Model (MLLM) for instruction understanding with a Multimodal DiT (MMDiT) for video generation. This design enables accurate interpretation of complex multimodal instructions while preserving visual consistency. Built on this architecture, UniVideo unifies diverse video generation and editing tasks under a single multimodal instruction paradigm and is jointly trained across them. Extensive experiments demonstrate that UniVideo matches or surpasses state-of-the-art task-specific baselines in text/image-to-video generation, in-context video generation and in-context video editing. Notably, the unified design of UniVideo enables two forms of generalization. First, UniVideo supports task composition, such as combining editing with style transfer, by integrating multiple capabilities within a single instruction. Second, even without explicit training on free-form video editing, UniVideo transfers its editing capability from large-scale image editing data to this setting, handling unseen instructions such as green-screening characters or changing materials within a video. Beyond these core capabilities, UniVideo also supports visual-prompt-based video generation, where the MLLM interprets visual prompts and guides the MMDiT during synthesis. To foster future research, we will release our model and code.

</details>


### [31] [VideoVerse: How Far is Your T2V Generator from a World Model?](https://arxiv.org/abs/2510.08398)
*Zeqing Wang,Xinyu Wei,Bairui Li,Zhen Guo,Jinrui Zhang,Hongyang Wei,Keze Wang,Lei Zhang*

Main category: cs.CV

TL;DR: VideoVerse是一个新的文本到视频生成基准测试，专注于评估T2V模型对现实世界中复杂时间因果关系和世界知识的理解能力，包含300个精心策划的提示、815个事件和793个二元评估问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法评估最先进的T2V模型，因为当前评估维度（如逐帧美学质量和时间一致性）已无法区分顶级模型，且缺乏对事件级时间因果关系和世界知识的系统性评估。

Method: 收集跨多个领域的代表性视频，提取具有内在时间因果关系的事件级描述，由独立标注者重写为文本到视频提示，设计包含10个维度的二元评估问题，并使用现代视觉语言模型开发基于人类偏好的问答评估流程。

Result: 构建了包含300个提示、815个事件和793个评估问题的VideoVerse基准测试，并对开源和闭源的先进T2V模型进行了系统性评估。

Conclusion: VideoVerse基准测试为评估T2V模型在构建世界模型方面的能力提供了全面框架，揭示了当前T2V生成器与世界模型之间的差距。

Abstract: The recent rapid advancement of Text-to-Video (T2V) generation technologies, which are critical to build ``world models'', makes the existing benchmarks increasingly insufficient to evaluate state-of-the-art T2V models. First, current evaluation dimensions, such as per-frame aesthetic quality and temporal consistency, are no longer able to differentiate state-of-the-art T2V models. Second, event-level temporal causality, which not only distinguishes video from other modalities but also constitutes a crucial component of world models, is severely underexplored in existing benchmarks. Third, existing benchmarks lack a systematic assessment of world knowledge, which are essential capabilities for building world models. To address these issues, we introduce VideoVerse, a comprehensive benchmark that focuses on evaluating whether a T2V model could understand complex temporal causality and world knowledge in the real world. We collect representative videos across diverse domains (e.g., natural landscapes, sports, indoor scenes, science fiction, chemical and physical experiments) and extract their event-level descriptions with inherent temporal causality, which are then rewritten into text-to-video prompts by independent annotators. For each prompt, we design a suite of binary evaluation questions from the perspective of dynamic and static properties, with a total of ten carefully defined evaluation dimensions. In total, our VideoVerse comprises 300 carefully curated prompts, involving 815 events and 793 binary evaluation questions. Consequently, a human preference aligned QA-based evaluation pipeline is developed by using modern vision-language models. Finally, we perform a systematic evaluation of state-of-the-art open-source and closed-source T2V models on VideoVerse, providing in-depth analysis on how far the current T2V generators are from world models.

</details>


### [32] [Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency](https://arxiv.org/abs/2510.08431)
*Kaiwen Zheng,Yuji Wang,Qianli Ma,Huayu Chen,Jintao Zhang,Yogesh Balaji,Jianfei Chen,Ming-Yu Liu,Jun Zhu,Qinsheng Zhang*

Main category: cs.CV

TL;DR: 本文首次将连续时间一致性蒸馏扩展到通用应用级图像和视频扩散模型，提出了分数正则化连续时间一致性模型(rCM)，解决了sCM在细节生成中的质量限制，在14B参数模型和5秒视频上实现高质量、高多样性的1-4步生成。


<details>
  <summary>Details</summary>
Motivation: 连续时间一致性模型(sCM)虽然在学术规模扩散加速中表现出色，但在大规模文本到图像和视频任务中的适用性仍不明确，主要面临雅可比向量积计算的基础设施挑战和标准评估基准的局限性。

Method: 开发了并行兼容的FlashAttention-2 JVP内核，支持超过100亿参数模型训练；提出分数正则化连续时间一致性模型(rCM)，通过分数蒸馏作为长跳跃正则器，结合sCM的前向散度和反向散度的优势。

Result: 在Cosmos-Predict2、Wan2.1等高达14B参数模型和5秒视频上验证，rCM在质量指标上匹配或超越最先进的蒸馏方法DMD2，同时在多样性方面具有显著优势，无需GAN调优或大量超参数搜索，仅需1-4步生成高保真样本，加速扩散采样15-50倍。

Conclusion: rCM为推进大规模扩散蒸馏提供了一个实用且理论基础的框架，实现了高质量和高多样性的快速生成。

Abstract: This work represents the first effort to scale up continuous-time consistency distillation to general application-level image and video diffusion models. Although continuous-time consistency model (sCM) is theoretically principled and empirically powerful for accelerating academic-scale diffusion, its applicability to large-scale text-to-image and video tasks remains unclear due to infrastructure challenges in Jacobian-vector product (JVP) computation and the limitations of standard evaluation benchmarks. We first develop a parallelism-compatible FlashAttention-2 JVP kernel, enabling sCM training on models with over 10 billion parameters and high-dimensional video tasks. Our investigation reveals fundamental quality limitations of sCM in fine-detail generation, which we attribute to error accumulation and the "mode-covering" nature of its forward-divergence objective. To remedy this, we propose the score-regularized continuous-time consistency model (rCM), which incorporates score distillation as a long-skip regularizer. This integration complements sCM with the "mode-seeking" reverse divergence, effectively improving visual quality while maintaining high generation diversity. Validated on large-scale models (Cosmos-Predict2, Wan2.1) up to 14B parameters and 5-second videos, rCM matches or surpasses the state-of-the-art distillation method DMD2 on quality metrics while offering notable advantages in diversity, all without GAN tuning or extensive hyperparameter searches. The distilled models generate high-fidelity samples in only $1\sim4$ steps, accelerating diffusion sampling by $15\times\sim50\times$. These results position rCM as a practical and theoretically grounded framework for advancing large-scale diffusion distillation.

</details>


### [33] [Hierarchical Spatial Algorithms for High-Resolution Image Quantization and Feature Extraction](https://arxiv.org/abs/2510.08449)
*Noor Islam S. Mohammad*

Main category: cs.CV

TL;DR: 提出一个模块化空间图像处理框架，包含灰度量化、色彩亮度增强、图像锐化、双向变换管道和几何特征提取，在多个数据集上展示出稳健的确定性性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个综合的图像处理框架，能够同时处理多种图像增强和特征提取任务，满足实时图像分析和计算机视觉应用的需求。

Method: 采用模块化设计，包括八级灰度量化、RGB和YCrCb色彩空间的直方图均衡化、HSV亮度调整、3×3卷积核锐化、双向变换管道（包含反锐化掩模、伽马校正和噪声放大），以及Canny边缘检测、Hough线估计、Harris角点检测等几何特征提取方法。

Result: 双向变换管道正向和反向过程准确率分别为76.10%和74.80%，球杆对齐角度估计为51.50度，球杆隔离与真实图像的相似度达到81.87%。

Conclusion: 该框架在多样化数据集上表现出稳健和确定性的性能，具有实时图像分析和计算机视觉应用的潜力。

Abstract: This study introduces a modular framework for spatial image processing, integrating grayscale quantization, color and brightness enhancement, image sharpening, bidirectional transformation pipelines, and geometric feature extraction. A stepwise intensity transformation quantizes grayscale images into eight discrete levels, producing a posterization effect that simplifies representation while preserving structural detail. Color enhancement is achieved via histogram equalization in both RGB and YCrCb color spaces, with the latter improving contrast while maintaining chrominance fidelity. Brightness adjustment is implemented through HSV value-channel manipulation, and image sharpening is performed using a 3 * 3 convolution kernel to enhance high-frequency details. A bidirectional transformation pipeline that integrates unsharp masking, gamma correction, and noise amplification achieved accuracy levels of 76.10% and 74.80% for the forward and reverse processes, respectively. Geometric feature extraction employed Canny edge detection, Hough-based line estimation (e.g., 51.50{\deg} for billiard cue alignment), Harris corner detection, and morphological window localization. Cue isolation further yielded 81.87\% similarity against ground truth images. Experimental evaluation across diverse datasets demonstrates robust and deterministic performance, highlighting its potential for real-time image analysis and computer vision.

</details>


### [34] [MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration](https://arxiv.org/abs/2510.08508)
*Lu Liu,Chunlei Cai,Shaocheng Shen,Jianfeng Liang,Weimin Ouyang,Tianxiao Ye,Jian Mao,Huiyu Duan,Jiangchao Yao,Xiaoyun Zhang,Qiang Hu,Guangtao Zhai*

Main category: cs.CV

TL;DR: MoA-VR是一个基于多智能体协作的视频修复系统，通过三个协调的智能体（退化识别、路由修复、质量评估）来模拟人类专家的处理流程，有效处理复杂的视频退化问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界视频常因采集和传输条件而遭受复杂退化（如噪声、压缩伪影、低光失真），现有方法需要专业手动选择专用模型或采用单一架构，难以泛化处理各种退化类型。

Method: 提出MoA-VR系统，包含三个智能体：基于视觉语言模型（VLM）的退化识别器、由大语言模型（LLM）驱动的自适应路由器、以及专门为修复任务设计的VLM视频质量评估模型。

Result: 大量实验表明，MoA-VR能有效处理多样化和复合退化，在客观指标和感知质量方面均优于现有基线方法。

Conclusion: 该研究展示了多模态智能和模块化推理在通用视频修复系统中的整合潜力。

Abstract: Real-world videos often suffer from complex degradations, such as noise, compression artifacts, and low-light distortions, due to diverse acquisition and transmission conditions. Existing restoration methods typically require professional manual selection of specialized models or rely on monolithic architectures that fail to generalize across varying degradations. Inspired by expert experience, we propose MoA-VR, the first \underline{M}ixture-\underline{o}f-\underline{A}gents \underline{V}ideo \underline{R}estoration system that mimics the reasoning and processing procedures of human professionals through three coordinated agents: Degradation Identification, Routing and Restoration, and Restoration Quality Assessment. Specifically, we construct a large-scale and high-resolution video degradation recognition benchmark and build a vision-language model (VLM) driven degradation identifier. We further introduce a self-adaptive router powered by large language models (LLMs), which autonomously learns effective restoration strategies by observing tool usage patterns. To assess intermediate and final processed video quality, we construct the \underline{Res}tored \underline{V}ideo \underline{Q}uality (Res-VQ) dataset and design a dedicated VLM-based video quality assessment (VQA) model tailored for restoration tasks. Extensive experiments demonstrate that MoA-VR effectively handles diverse and compound degradations, consistently outperforming existing baselines in terms of both objective metrics and perceptual quality. These results highlight the potential of integrating multimodal intelligence and modular reasoning in general-purpose video restoration systems.

</details>


### [35] [FlexTraj: Image-to-Video Generation with Flexible Point Trajectory Control](https://arxiv.org/abs/2510.08527)
*Zhiyuan Zhang,Can Wang,Dongdong Chen,Jing Liao*

Main category: cs.CV

TL;DR: FlexTraj是一个用于图像到视频生成的框架，通过统一的基于点的运动表示实现灵活轨迹控制，支持密集和稀疏轨迹控制，采用序列拼接方案实现高效训练和推理。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法在轨迹控制方面存在局限性，需要更灵活、多粒度的轨迹控制能力，同时解决条件对齐和训练效率问题。

Method: 提出统一点运动表示（分割ID、轨迹ID、颜色通道），采用序列拼接方案注入轨迹条件，使用退火训练策略逐步减少对完整监督和对齐条件的依赖。

Result: 实验证明FlexTraj支持多粒度、对齐无关的轨迹控制，适用于运动克隆、拖拽式图像到视频、运动插值、相机重定向、灵活动作控制和网格动画等多种应用。

Conclusion: FlexTraj框架在轨迹控制的灵活性、训练效率和推理性能方面表现出色，为图像到视频生成提供了强大的轨迹控制能力。

Abstract: We present FlexTraj, a framework for image-to-video generation with flexible point trajectory control. FlexTraj introduces a unified point-based motion representation that encodes each point with a segmentation ID, a temporally consistent trajectory ID, and an optional color channel for appearance cues, enabling both dense and sparse trajectory control. Instead of injecting trajectory conditions into the video generator through token concatenation or ControlNet, FlexTraj employs an efficient sequence-concatenation scheme that achieves faster convergence, stronger controllability, and more efficient inference, while maintaining robustness under unaligned conditions. To train such a unified point trajectory-controlled video generator, FlexTraj adopts an annealing training strategy that gradually reduces reliance on complete supervision and aligned condition. Experimental results demonstrate that FlexTraj enables multi-granularity, alignment-agnostic trajectory control for video generation, supporting various applications such as motion cloning, drag-based image-to-video, motion interpolation, camera redirection, flexible action control and mesh animations.

</details>


### [36] [VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning](https://arxiv.org/abs/2510.08555)
*Minghong Cai,Qiulin Wang,Zongli Ye,Wenze Liu,Quande Liu,Weicai Ye,Xintao Wang,Pengfei Wan,Kun Gai,Xiangyu Yue*

Main category: cs.CV

TL;DR: 提出了VideoCanvas框架，通过混合条件策略解决潜在视频扩散模型中的时间模糊问题，实现任意时空视频补全任务。


<details>
  <summary>Details</summary>
Motivation: 统一现有可控视频生成任务（如图像到视频、修复、扩展、插值）到一个灵活范式，但面临因果VAE引入的时间模糊问题。

Method: 采用零参数增加的上下文条件范式，通过零填充处理空间控制，通过时间RoPE插值实现时间对齐，解耦时空控制。

Result: 在VideoCanvasBench基准测试中显著优于现有条件范式，建立了灵活统一视频生成的新SOTA。

Conclusion: VideoCanvas成功解决了潜在视频扩散模型的时间模糊问题，为任意时空视频补全提供了有效解决方案。

Abstract: We introduce the task of arbitrary spatio-temporal video completion, where a video is generated from arbitrary, user-specified patches placed at any spatial location and timestamp, akin to painting on a video canvas. This flexible formulation naturally unifies many existing controllable video generation tasks--including first-frame image-to-video, inpainting, extension, and interpolation--under a single, cohesive paradigm. Realizing this vision, however, faces a fundamental obstacle in modern latent video diffusion models: the temporal ambiguity introduced by causal VAEs, where multiple pixel frames are compressed into a single latent representation, making precise frame-level conditioning structurally difficult. We address this challenge with VideoCanvas, a novel framework that adapts the In-Context Conditioning (ICC) paradigm to this fine-grained control task with zero new parameters. We propose a hybrid conditioning strategy that decouples spatial and temporal control: spatial placement is handled via zero-padding, while temporal alignment is achieved through Temporal RoPE Interpolation, which assigns each condition a continuous fractional position within the latent sequence. This resolves the VAE's temporal ambiguity and enables pixel-frame-aware control on a frozen backbone. To evaluate this new capability, we develop VideoCanvasBench, the first benchmark for arbitrary spatio-temporal video completion, covering both intra-scene fidelity and inter-scene creativity. Experiments demonstrate that VideoCanvas significantly outperforms existing conditioning paradigms, establishing a new state of the art in flexible and unified video generation.

</details>


### [37] [MultiCOIN: Multi-Modal COntrollable Video INbetweening](https://arxiv.org/abs/2510.08561)
*Maham Tanveer,Yang Zhou,Simon Niklaus,Ali Mahdavi Amiri,Hao Zhang,Krishna Kumar Singh,Nanxuan Zhao*

Main category: cs.CV

TL;DR: 本文提出了一个支持多模态控制的视频插帧框架，能够通过深度过渡、运动轨迹、文本提示等多种方式精确控制中间帧生成，解决了现有方法无法处理复杂运动和缺乏精细控制的问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频插帧方法无法生成大规模、复杂或精细的运动，缺乏对中间帧细节的精确控制，难以满足用户多样化的创作意图。

Method: 采用Diffusion Transformer架构作为视频生成模型，将所有运动控制映射为统一的基于点的稀疏表示，将内容控制和运动控制分离为两个分支进行特征编码，并采用分阶段训练策略。

Result: 通过广泛的定性和定量实验证明，多模态控制能够实现更动态、可定制和上下文准确的视觉叙事。

Conclusion: 提出的多模态控制视频插帧框架在灵活性、易用性和精确性之间取得了平衡，为视频编辑和长视频合成提供了强大的工具。

Abstract: Video inbetweening creates smooth and natural transitions between two image frames, making it an indispensable tool for video editing and long-form video synthesis. Existing works in this domain are unable to generate large, complex, or intricate motions. In particular, they cannot accommodate the versatility of user intents and generally lack fine control over the details of intermediate frames, leading to misalignment with the creative mind. To fill these gaps, we introduce \modelname{}, a video inbetweening framework that allows multi-modal controls, including depth transition and layering, motion trajectories, text prompts, and target regions for movement localization, while achieving a balance between flexibility, ease of use, and precision for fine-grained video interpolation. To achieve this, we adopt the Diffusion Transformer (DiT) architecture as our video generative model, due to its proven capability to generate high-quality long videos. To ensure compatibility between DiT and our multi-modal controls, we map all motion controls into a common sparse and user-friendly point-based representation as the video/noise input. Further, to respect the variety of controls which operate at varying levels of granularity and influence, we separate content controls and motion controls into two branches to encode the required features before guiding the denoising process, resulting in two generators, one for motion and the other for content. Finally, we propose a stage-wise training strategy to ensure that our model learns the multi-modal controls smoothly. Extensive qualitative and quantitative experiments demonstrate that multi-modal controls enable a more dynamic, customizable, and contextually accurate visual narrative.

</details>


### [38] [ResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous Driving](https://arxiv.org/abs/2510.08562)
*Zhiyu Zheng,Shaoyu Chen,Haoran Yin,Xinbang Zhang,Jialv Zou,Xinggang Wang,Qian Zhang,Lefei Zhang*

Main category: cs.CV

TL;DR: ResAD是一个用于端到端自动驾驶的归一化残差轨迹建模框架，通过预测与确定性惯性参考的偏差来解决轨迹数据时空不平衡问题，显著简化学习任务并提升性能。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶系统面临轨迹数据固有的时空不平衡问题，这导致模型学习虚假相关性而非因果推理，并优先考虑不确定的远距离预测，从而危及即时安全。

Method: 提出ResAD框架，将学习任务重新定义为预测与确定性惯性参考的残差偏差，而非直接预测未来轨迹。采用逐点归一化来重新加权优化目标，防止与远距离不确定路径点相关的大幅度误差主导学习信号。

Result: 在NAVSIM基准测试中，ResAD使用仅有两个去噪步骤的普通扩散策略实现了88.6的最先进PDMS分数，证明该方法显著简化了学习任务并提高了模型性能。

Conclusion: ResAD通过重新定义学习任务和引入归一化残差建模，有效解决了端到端自动驾驶中的轨迹数据不平衡问题，为因果推理和安全性提供了更好的解决方案。

Abstract: End-to-end autonomous driving (E2EAD) systems, which learn to predict future trajectories directly from sensor data, are fundamentally challenged by the inherent spatio-temporal imbalance of trajectory data. This imbalance creates a significant optimization burden, causing models to learn spurious correlations instead of causal inference, while also prioritizing uncertain, distant predictions, thereby compromising immediate safety. To address these issues, we propose ResAD, a novel Normalized Residual Trajectory Modeling framework. Instead of predicting the future trajectory directly, our approach reframes the learning task to predict the residual deviation from a deterministic inertial reference. The inertial reference serves as a counterfactual, forcing the model to move beyond simple pattern recognition and instead identify the underlying causal factors (e.g., traffic rules, obstacles) that necessitate deviations from a default, inertially-guided path. To deal with the optimization imbalance caused by uncertain, long-term horizons, ResAD further incorporates Point-wise Normalization of the predicted residual. It re-weights the optimization objective, preventing large-magnitude errors associated with distant, uncertain waypoints from dominating the learning signal. Extensive experiments validate the effectiveness of our framework. On the NAVSIM benchmark, ResAD achieves a state-of-the-art PDMS of 88.6 using a vanilla diffusion policy with only two denoising steps, demonstrating that our approach significantly simplifies the learning task and improves model performance. The code will be released to facilitate further research.

</details>


### [39] [D$^2$GS: Depth-and-Density Guided Gaussian Splatting for Stable and Accurate Sparse-View Reconstruction](https://arxiv.org/abs/2510.08566)
*Meixi Song,Xin Lin,Dizhe Zhang,Haodong Li,Xiangtai Li,Bo Du,Lu Qi*

Main category: cs.CV

TL;DR: 提出D²GS框架解决稀疏视图下3D高斯泼溅的性能下降问题，通过密度-深度引导的dropout策略抑制过拟合，以及距离感知的保真度增强模块改善远场重建质量。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅在稀疏视图条件下存在性能下降和不稳定性问题，主要由于近相机区域的过度拟合和远距离区域的拟合不足。

Method: 采用深度和密度引导的dropout策略自适应掩码冗余高斯，以及距离感知保真度增强模块对远场区域进行针对性监督。

Result: 在多个数据集上的实验表明，该方法显著提升了稀疏视图条件下的视觉质量和鲁棒性。

Conclusion: D²GS框架有效解决了稀疏视图3D高斯泼溅的过拟合和欠拟合问题，提高了重建质量和稳定性。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) enable real-time, high-fidelity novel view synthesis (NVS) with explicit 3D representations. However, performance degradation and instability remain significant under sparse-view conditions. In this work, we identify two key failure modes under sparse-view conditions: overfitting in regions with excessive Gaussian density near the camera, and underfitting in distant areas with insufficient Gaussian coverage. To address these challenges, we propose a unified framework D$^2$GS, comprising two key components: a Depth-and-Density Guided Dropout strategy that suppresses overfitting by adaptively masking redundant Gaussians based on density and depth, and a Distance-Aware Fidelity Enhancement module that improves reconstruction quality in under-fitted far-field areas through targeted supervision. Moreover, we introduce a new evaluation metric to quantify the stability of learned Gaussian distributions, providing insights into the robustness of the sparse-view 3DGS. Extensive experiments on multiple datasets demonstrate that our method significantly improves both visual quality and robustness under sparse view conditions. The project page can be found at: https://insta360-research-team.github.io/DDGS-website/.

</details>


### [40] [ReSplat: Learning Recurrent Gaussian Splats](https://arxiv.org/abs/2510.08575)
*Haofei Xu,Daniel Barath,Andreas Geiger,Marc Pollefeys*

Main category: cs.CV

TL;DR: ReSplat是一种前馈循环高斯溅射模型，通过迭代优化3D高斯而不显式计算梯度，利用渲染误差作为反馈信号来指导高斯更新，在减少高斯数量和提升渲染速度的同时实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统前馈高斯溅射模型因依赖单次前向传播而性能受限，需要一种能迭代优化3D高斯的方法来提升性能。

Method: 提出循环高斯溅射模型，使用渲染误差作为反馈信号指导高斯更新；引入紧凑重建模型在16倍下采样空间初始化高斯，大幅减少高斯数量。

Result: 在多种输入视图、分辨率和数据集上的实验表明，该方法在减少高斯数量和提升渲染速度的同时实现了最先进性能。

Conclusion: ReSplat通过循环优化机制和紧凑初始化策略，有效解决了前馈高斯溅射模型的性能限制问题。

Abstract: While feed-forward Gaussian splatting models provide computational efficiency and effectively handle sparse input settings, their performance is fundamentally limited by the reliance on a single forward pass during inference. We propose ReSplat, a feed-forward recurrent Gaussian splatting model that iteratively refines 3D Gaussians without explicitly computing gradients. Our key insight is that the Gaussian splatting rendering error serves as a rich feedback signal, guiding the recurrent network to learn effective Gaussian updates. This feedback signal naturally adapts to unseen data distributions at test time, enabling robust generalization. To initialize the recurrent process, we introduce a compact reconstruction model that operates in a $16 \times$ subsampled space, producing $16 \times$ fewer Gaussians than previous per-pixel Gaussian models. This substantially reduces computational overhead and allows for efficient Gaussian updates. Extensive experiments across varying of input views (2, 8, 16), resolutions ($256 \times 256$ to $540 \times 960$), and datasets (DL3DV and RealEstate10K) demonstrate that our method achieves state-of-the-art performance while significantly reducing the number of Gaussians and improving the rendering speed. Our project page is at https://haofeixu.github.io/resplat/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [41] [Biology-driven assessment of deep learning super-resolution imaging of the porosity network in dentin](https://arxiv.org/abs/2510.08407)
*Lauren Anderson,Lucas Chatelain,Nicolas Tremblay,Kathryn Grandfield,David Rousseau,Aurélien Gourrier*

Main category: cs.LG

TL;DR: 本研究测试了多种深度学习超分辨率模型，用于从低分辨率共聚焦图像恢复牙本质孔隙网络的高分辨率图像，并通过生物学驱动的评估方法验证模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前牙本质孔隙网络可视化受限于共聚焦显微镜的分辨率，需要开发能够从低分辨率图像恢复高质量图像的超分辨率方法，以扩大视野范围并加快实验采集速度。

Method: 使用三种有监督2D超分辨率模型(RCAN、pix2pix、FSRCNN)和一种无监督模型(CycleGAN)，对不同采样方案获取的实验配对高低分辨率共聚焦图像进行处理，实现2倍、4倍、8倍的像素尺寸增加。

Result: 标准图像质量评估指标与视觉感知不一致，而基于孔隙网络结构和连通性的生物学驱动评估方法能更好地解释超分辨率模型的性能差异。

Conclusion: 针对特定生物结构的超分辨率模型评估需要采用生物学驱动的分析方法，而非通用的图像质量评估指标，这能更准确地反映模型在保留关键结构特征方面的能力。

Abstract: The mechanosensory system of teeth is currently believed to partly rely on Odontoblast cells stimulation by fluid flow through a porosity network extending through dentin. Visualizing the smallest sub-microscopic porosity vessels therefore requires the highest achievable resolution from confocal fluorescence microscopy, the current gold standard. This considerably limits the extent of the field of view to very small sample regions. To overcome this limitation, we tested different deep learning (DL) super-resolution (SR) models to allow faster experimental acquisitions of lower resolution images and restore optimal image quality by post-processing. Three supervised 2D SR models (RCAN, pix2pix, FSRCNN) and one unsupervised (CycleGAN) were applied to a unique set of experimentally paired high- and low-resolution confocal images acquired with different sampling schemes, resulting in a pixel size increase of x2, x4, x8. Model performance was quantified using a broad set of similarity and distribution-based image quality assessment (IQA) metrics, which yielded inconsistent results that mostly contradicted our visual perception. This raises the question of the relevance of such generic metrics to efficiently target the specific structure of dental porosity. To resolve this conflicting information, the generated SR images were segmented taking into account the specific scales and morphology of the porosity network and analysed by comparing connected components. Additionally, the capacity of the SR models to preserve 3D porosity connectivity throughout the confocal image stacks was evaluated using graph analysis. This biology-driven assessment allowed a far better mechanistic interpretation of SR performance, highlighting differences in model sensitivity to weak intensity features and the impact of non-linearity in image generation, which explains the failure of standard IQA metrics.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [42] [SatFusion: A Unified Framework for Enhancing Satellite IoT Images via Multi-Temporal and Multi-Source Data Fusion](https://arxiv.org/abs/2510.07905)
*Yufei Tong,Guanjie Cheng,Peihan Wu,Yicheng Zhu,Kexu Lu,Feiyi Chen,Meng Xi,Junqin Huang,Shuiguang Deng*

Main category: eess.IV

TL;DR: SatFusion是一个统一的卫星物联网图像增强框架，通过多时相和多源数据融合来解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用时相和源维度的互补信息。MISR方法受限于输入图像的细粒度纹理细节，而全色锐化方法对噪声和配准误差敏感。

Method: SatFusion包含三个模块：多时相图像融合模块实现深度特征对齐，多源图像融合模块注入全色数据的纹理信息，融合组合模块自适应整合两种模态的优势并动态优化光谱一致性。

Result: 在WorldStrat、WV3、QB和GF2数据集上的实验表明，SatFusion显著提升了融合质量、在挑战性条件下的鲁棒性以及对真实Sat-IoT场景的泛化能力。

Conclusion: SatFusion通过统一的多时相和多源数据融合框架，有效解决了卫星物联网图像增强中的关键问题，具有优异的性能和实用性。

Abstract: With the rapid advancement of the digital society, the proliferation of satellites in the Satellite Internet of Things (Sat-IoT) has led to the continuous accumulation of large-scale multi-temporal and multi-source images across diverse application scenarios. However, existing methods fail to fully exploit the complementary information embedded in both temporal and source dimensions. For example, Multi-Image Super-Resolution (MISR) enhances reconstruction quality by leveraging temporal complementarity across multiple observations, yet the limited fine-grained texture details in input images constrain its performance. Conversely, pansharpening integrates multi-source images by injecting high-frequency spatial information from panchromatic data, but typically relies on pre-interpolated low-resolution inputs and assumes noise-free alignment, making it highly sensitive to noise and misregistration. To address these issues, we propose SatFusion: A Unified Framework for Enhancing Satellite IoT Images via Multi-Temporal and Multi-Source Data Fusion. Specifically, SatFusion first employs a Multi-Temporal Image Fusion (MTIF) module to achieve deep feature alignment with the panchromatic image. Then, a Multi-Source Image Fusion (MSIF) module injects fine-grained texture information from the panchromatic data. Finally, a Fusion Composition module adaptively integrates the complementary advantages of both modalities while dynamically refining spectral consistency, supervised by a weighted combination of multiple loss functions. Extensive experiments on the WorldStrat, WV3, QB, and GF2 datasets demonstrate that SatFusion significantly improves fusion quality, robustness under challenging conditions, and generalizability to real-world Sat-IoT scenarios. The code is available at: https://github.com/dllgyufei/SatFusion.git.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [43] [NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos](https://arxiv.org/abs/2510.08568)
*Hongyu Li,Lingfeng Sun,Yafei Hu,Duy Ta,Jennifer Barry,George Konidaris,Jiahui Fu*

Main category: cs.RO

TL;DR: NovaFlow是一个零样本机器人操作框架，通过视频生成和3D物体流分析，将任务描述转换为可执行计划，无需演示或特定机器人训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设任务分布内或需要特定机器人的微调数据，限制了跨平台迁移能力。目标是实现机器人零样本执行新任务。

Method: 使用视频生成模型合成任务视频，通过感知模块提取3D物体流，为刚性物体计算相对位姿并通过抓取提议和轨迹优化实现，为可变形物体使用基于粒子的动力学模型进行跟踪规划。

Result: 在桌面Franka机械臂和Spot四足移动机器人上验证了刚性、关节式和可变形物体的操作任务，实现了有效的零样本执行。

Conclusion: 通过将任务理解与底层控制解耦，NovaFlow能够自然地跨不同机器人平台迁移，无需演示或特定机器人训练。

Abstract: Enabling robots to execute novel manipulation tasks zero-shot is a central goal in robotics. Most existing methods assume in-distribution tasks or rely on fine-tuning with embodiment-matched data, limiting transfer across platforms. We present NovaFlow, an autonomous manipulation framework that converts a task description into an actionable plan for a target robot without any demonstrations. Given a task description, NovaFlow synthesizes a video using a video generation model and distills it into 3D actionable object flow using off-the-shelf perception modules. From the object flow, it computes relative poses for rigid objects and realizes them as robot actions via grasp proposals and trajectory optimization. For deformable objects, this flow serves as a tracking objective for model-based planning with a particle-based dynamics model. By decoupling task understanding from low-level control, NovaFlow naturally transfers across embodiments. We validate on rigid, articulated, and deformable object manipulation tasks using a table-top Franka arm and a Spot quadrupedal mobile robot, and achieve effective zero-shot execution without demonstrations or embodiment-specific training. Project website: https://novaflow.lhy.xyz/.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [44] [FlowLensing: Simulating Gravitational Lensing with Flow Matching](https://arxiv.org/abs/2510.07878)
*Hamees Sayed,Pranath Reddy,Michael W. Toomey,Sergei Gleyzer*

Main category: astro-ph.IM

TL;DR: FlowLensing是一个基于扩散变换器的紧凑高效流匹配模型，用于强引力透镜模拟，相比传统方法加速200倍以上，支持离散和连续参数处理，确保物理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有引力透镜模拟工具依赖光线追踪或前向建模流程，虽然精确但速度极慢，无法满足大规模模拟需求，阻碍了暗物质研究。

Method: 使用基于扩散变换器的流匹配模型，在离散和连续两种机制下运行，处理不同暗物质模型类别和连续模型参数，确保物理一致性。

Result: 模型相比传统模拟器在密集暗物质模型上实现200倍以上的加速，具有高保真度和低推理延迟。

Conclusion: FlowLensing能够实现快速、可扩展且物理一致的图像合成，为传统前向建模流程提供了实用替代方案，可推进暗物质子结构探测研究。

Abstract: Gravitational lensing is one of the most powerful probes of dark matter, yet creating high-fidelity lensed images at scale remains a bottleneck. Existing tools rely on ray-tracing or forward-modeling pipelines that, while precise, are prohibitively slow. We introduce FlowLensing, a Diffusion Transformer-based compact and efficient flow-matching model for strong gravitational lensing simulation. FlowLensing operates in both discrete and continuous regimes, handling classes such as different dark matter models as well as continuous model parameters ensuring physical consistency. By enabling scalable simulations, our model can advance dark matter studies, specifically for probing dark matter substructure in cosmological surveys. We find that our model achieves a speedup of over 200$\times$ compared to classical simulators for intensive dark matter models, with high fidelity and low inference latency. FlowLensing enables rapid, scalable, and physically consistent image synthesis, offering a practical alternative to traditional forward-modeling pipelines.

</details>
