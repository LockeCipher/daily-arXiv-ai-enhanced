{"id": "2510.01061", "pdf": "https://arxiv.org/pdf/2510.01061", "abs": "https://arxiv.org/abs/2510.01061", "authors": ["Mark Boss", "Andreas Engelhardt", "Simon Donn\u00e9", "Varun Jampani"], "title": "ReSWD: ReSTIR'd, not shaken. Combining Reservoir Sampling and Sliced Wasserstein Distance for Variance Reduction", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": null, "summary": "Distribution matching is central to many vision and graphics tasks, where the widely used Wasserstein distance is too costly to compute for high dimensional distributions. The Sliced Wasserstein Distance (SWD) offers a scalable alternative, yet its Monte Carlo estimator suffers from high variance, resulting in noisy gradients and slow convergence. We introduce Reservoir SWD (ReSWD), which integrates Weighted Reservoir Sampling into SWD to adaptively retain informative projection directions in optimization steps, resulting in stable gradients while remaining unbiased. Experiments on synthetic benchmarks and real-world tasks such as color correction and diffusion guidance show that ReSWD consistently outperforms standard SWD and other variance reduction baselines. Project page: https://reservoirswd.github.io/", "AI": {"tldr": "\u63d0\u51fa\u4e86Reservoir SWD(ReSWD)\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a0\u6743\u50a8\u5c42\u91c7\u6837\u6280\u672f\u6539\u8fdb\u5207\u7247Wasserstein\u8ddd\u79bb\uff0c\u5728\u4fdd\u6301\u65e0\u504f\u6027\u7684\u540c\u65f6\u51cf\u5c11\u65b9\u5dee\uff0c\u63d0\u4f9b\u66f4\u7a33\u5b9a\u7684\u68af\u5ea6\u3002", "motivation": "\u9ad8\u7ef4\u5206\u5e03\u8ba1\u7b97\u4e2dWasserstein\u8ddd\u79bb\u6210\u672c\u8fc7\u9ad8\uff0c\u800c\u5207\u7247Wasserstein\u8ddd\u79bb\u7684\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u5668\u65b9\u5dee\u5927\uff0c\u5bfc\u81f4\u68af\u5ea6\u566a\u58f0\u5927\u3001\u6536\u655b\u6162\u3002", "method": "\u5c06\u52a0\u6743\u50a8\u5c42\u91c7\u6837\u6574\u5408\u5230SWD\u4e2d\uff0c\u5728\u4f18\u5316\u6b65\u9aa4\u4e2d\u81ea\u9002\u5e94\u4fdd\u7559\u4fe1\u606f\u4e30\u5bcc\u7684\u6295\u5f71\u65b9\u5411\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u4efb\u52a1\uff08\u5982\u8272\u5f69\u6821\u6b63\u548c\u6269\u6563\u5f15\u5bfc\uff09\u4e2d\uff0cReSWD\u59cb\u7ec8\u4f18\u4e8e\u6807\u51c6SWD\u548c\u5176\u4ed6\u65b9\u5dee\u51cf\u5c11\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ReSWD\u901a\u8fc7\u51cf\u5c11\u65b9\u5dee\u63d0\u4f9b\u7a33\u5b9a\u68af\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u65e0\u504f\u6027\uff0c\u5728\u591a\u79cd\u89c6\u89c9\u548c\u56fe\u5f62\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.01176", "pdf": "https://arxiv.org/pdf/2510.01176", "abs": "https://arxiv.org/abs/2510.01176", "authors": ["Jiye Lee", "Chenghui Li", "Linh Tran", "Shih-En Wei", "Jason Saragih", "Alexander Richard", "Hanbyul Joo", "Shaojie Bai"], "title": "Audio Driven Real-Time Facial Animation for Social Telepresence", "categories": ["cs.GR", "cs.CV", "cs.LG", "cs.SD"], "comment": "SIGGRAPH Asia 2025. Project page:   https://jiyewise.github.io/projects/AudioRTA", "summary": "We present an audio-driven real-time system for animating photorealistic 3D facial avatars with minimal latency, designed for social interactions in virtual reality for anyone. Central to our approach is an encoder model that transforms audio signals into latent facial expression sequences in real time, which are then decoded as photorealistic 3D facial avatars. Leveraging the generative capabilities of diffusion models, we capture the rich spectrum of facial expressions necessary for natural communication while achieving real-time performance (<15ms GPU time). Our novel architecture minimizes latency through two key innovations: an online transformer that eliminates dependency on future inputs and a distillation pipeline that accelerates iterative denoising into a single step. We further address critical design challenges in live scenarios for processing continuous audio signals frame-by-frame while maintaining consistent animation quality. The versatility of our framework extends to multimodal applications, including semantic modalities such as emotion conditions and multimodal sensors with head-mounted eye cameras on VR headsets. Experimental results demonstrate significant improvements in facial animation accuracy over existing offline state-of-the-art baselines, achieving 100 to 1000 times faster inference speed. We validate our approach through live VR demonstrations and across various scenarios such as multilingual speeches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u97f3\u9891\u9a71\u52a8\u7684\u5b9e\u65f63D\u9762\u90e8\u52a8\u753b\u7cfb\u7edf\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\uff08<15ms\uff09\u7684\u5149\u771f\u5b9e\u611f\u9762\u90e8\u8868\u60c5\u751f\u6210\uff0c\u9002\u7528\u4e8eVR\u793e\u4ea4\u4e92\u52a8\u3002", "motivation": "\u4e3a\u865a\u62df\u73b0\u5b9e\u793e\u4ea4\u4e92\u52a8\u5f00\u53d1\u5b9e\u65f6\u3001\u4f4e\u5ef6\u8fdf\u7684\u5149\u771f\u5b9e\u611f\u9762\u90e8\u52a8\u753b\u7cfb\u7edf\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u65f6\u6027\u80fd\u548c\u52a8\u753b\u8d28\u91cf\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u7f16\u7801\u5668\u5c06\u97f3\u9891\u4fe1\u53f7\u8f6c\u6362\u4e3a\u6f5c\u5728\u9762\u90e8\u8868\u60c5\u5e8f\u5217\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u89e3\u7801\u4e3a3D\u9762\u90e8\u52a8\u753b\u3002\u91c7\u7528\u5728\u7ebftransformer\u6d88\u9664\u5bf9\u672a\u6765\u8f93\u5165\u7684\u4f9d\u8d56\uff0c\u5e76\u901a\u8fc7\u84b8\u998f\u6d41\u7a0b\u5c06\u8fed\u4ee3\u53bb\u566a\u52a0\u901f\u4e3a\u5355\u6b65\u5904\u7406\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u79bb\u7ebf\u65b9\u6cd5\uff0c\u9762\u90e8\u52a8\u753b\u7cbe\u5ea6\u663e\u8457\u63d0\u5347\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u9ad8100-1000\u500d\uff0c\u5728GPU\u4e0a\u5904\u7406\u65f6\u95f4\u4f4e\u4e8e15ms\u3002\u652f\u6301\u591a\u8bed\u8a00\u8bed\u97f3\u548cVR\u5934\u6234\u8bbe\u5907\u7684\u591a\u6a21\u6001\u5e94\u7528\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\u4e86\u5b9e\u65f6\u3001\u4f4e\u5ef6\u8fdf\u7684\u5149\u771f\u5b9e\u611f\u9762\u90e8\u52a8\u753b\uff0c\u4e3aVR\u793e\u4ea4\u4e92\u52a8\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5728\u591a\u8bed\u8a00\u548c\u591a\u6a21\u6001\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.00033", "pdf": "https://arxiv.org/pdf/2510.00033", "abs": "https://arxiv.org/abs/2510.00033", "authors": ["Usman Muhammad", "Jorma Laaksonen"], "title": "Hybrid Deep Learning for Hyperspectral Single Image Super-Resolution", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Hyperspectral single image super-resolution (SISR) is a challenging task due to the difficulty of restoring fine spatial details while preserving spectral fidelity across a wide range of wavelengths, which limits the performance of conventional deep learning models. To address this challenge, we introduce Spectral-Spatial Unmixing Fusion (SSUF), a novel module that can be seamlessly integrated into standard 2D convolutional architectures to enhance both spatial resolution and spectral integrity. The SSUF combines spectral unmixing with spectral--spatial feature extraction and guides a ResNet-based convolutional neural network for improved reconstruction. In addition, we propose a custom Spatial-Spectral Gradient Loss function that integrates mean squared error with spatial and spectral gradient components, encouraging accurate reconstruction of both spatial and spectral features. Experiments on three public remote sensing hyperspectral datasets demonstrate that the proposed hybrid deep learning model achieves competitive performance while reducing model complexity.", "AI": {"tldr": "\u63d0\u51faSSUF\u6a21\u5757\u548c\u7a7a\u95f4-\u5149\u8c31\u68af\u5ea6\u635f\u5931\u51fd\u6570\uff0c\u7528\u4e8e\u63d0\u5347\u9ad8\u5149\u8c31\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u7a7a\u95f4\u7ec6\u8282\u6062\u590d\u548c\u5149\u8c31\u4fdd\u771f\u5ea6", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9ad8\u5149\u8c31\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u96be\u4ee5\u540c\u65f6\u6062\u590d\u7cbe\u7ec6\u7a7a\u95f4\u7ec6\u8282\u548c\u4fdd\u6301\u5bbd\u6ce2\u957f\u8303\u56f4\u5185\u7684\u5149\u8c31\u4fdd\u771f\u5ea6", "method": "\u63d0\u51fa\u5149\u8c31-\u7a7a\u95f4\u89e3\u6df7\u878d\u5408(SSUF)\u6a21\u5757\uff0c\u7ed3\u5408\u5149\u8c31\u89e3\u6df7\u548c\u5149\u8c31-\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\uff0c\u6307\u5bfc\u57fa\u4e8eResNet\u7684CNN\u8fdb\u884c\u91cd\u5efa\uff1b\u5e76\u8bbe\u8ba1\u7a7a\u95f4-\u5149\u8c31\u68af\u5ea6\u635f\u5931\u51fd\u6570", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u9065\u611f\u9ad8\u5149\u8c31\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd", "conclusion": "SSUF\u6a21\u5757\u548c\u7a7a\u95f4-\u5149\u8c31\u68af\u5ea6\u635f\u5931\u51fd\u6570\u80fd\u6709\u6548\u63d0\u5347\u9ad8\u5149\u8c31\u8d85\u5206\u8fa8\u7387\u7684\u6027\u80fd"}}
{"id": "2510.00047", "pdf": "https://arxiv.org/pdf/2510.00047", "abs": "https://arxiv.org/abs/2510.00047", "authors": ["Sihao Ding", "Santosh Vasa", "Aditi Ramadwar"], "title": "Explanation-Driven Counterfactual Testing for Faithfulness in Vision-Language Model Explanations", "categories": ["cs.CV", "cs.AI"], "comment": "NeurIPS 2025 workshop on Regulatable ML", "summary": "Vision-Language Models (VLMs) often produce fluent Natural Language Explanations (NLEs) that sound convincing but may not reflect the causal factors driving predictions. This mismatch of plausibility and faithfulness poses technical and governance risks. We introduce Explanation-Driven Counterfactual Testing (EDCT), a fully automated verification procedure for a target VLM that treats the model's own explanation as a falsifiable hypothesis. Given an image-question pair, EDCT: (1) obtains the model's answer and NLE, (2) parses the NLE into testable visual concepts, (3) generates targeted counterfactual edits via generative inpainting, and (4) computes a Counterfactual Consistency Score (CCS) using LLM-assisted analysis of changes in both answers and explanations. Across 120 curated OK-VQA examples and multiple VLMs, EDCT uncovers substantial faithfulness gaps and provides regulator-aligned audit artifacts indicating when cited concepts fail causal tests.", "AI": {"tldr": "\u63d0\u51fa\u4e86EDCT\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u53cd\u4e8b\u5b9e\u56fe\u50cf\u6765\u9a8c\u8bc1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\uff0c\u53d1\u73b0\u6a21\u578b\u89e3\u91ca\u4e0e\u771f\u5b9e\u56e0\u679c\u56e0\u7d20\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u89e3\u91ca\u542c\u8d77\u6765\u5408\u7406\u4f46\u53ef\u80fd\u4e0d\u53cd\u6620\u771f\u5b9e\u9884\u6d4b\u539f\u56e0\uff0c\u8fd9\u79cd\u53ef\u4fe1\u6027\u4e0e\u5fe0\u5b9e\u6027\u4e0d\u5339\u914d\u5b58\u5728\u6280\u672f\u548c\u6cbb\u7406\u98ce\u9669\u3002", "method": "EDCT\u65b9\u6cd5\uff1a\u83b7\u53d6\u6a21\u578b\u7b54\u6848\u548c\u89e3\u91ca\u2192\u89e3\u6790\u4e3a\u53ef\u6d4b\u8bd5\u89c6\u89c9\u6982\u5ff5\u2192\u901a\u8fc7\u751f\u6210\u4fee\u590d\u751f\u6210\u9488\u5bf9\u6027\u53cd\u4e8b\u5b9e\u7f16\u8f91\u2192\u4f7f\u7528LLM\u8f85\u52a9\u5206\u6790\u8ba1\u7b97\u53cd\u4e8b\u5b9e\u4e00\u81f4\u6027\u5206\u6570\u3002", "result": "\u5728120\u4e2aOK-VQA\u793a\u4f8b\u548c\u591a\u4e2aVLM\u4e0a\uff0cEDCT\u63ed\u793a\u4e86\u663e\u8457\u7684\u5fe0\u5b9e\u6027\u5dee\u8ddd\uff0c\u5e76\u63d0\u4f9b\u4e86\u76d1\u7ba1\u5bf9\u9f50\u7684\u5ba1\u8ba1\u8bc1\u636e\u3002", "conclusion": "EDCT\u80fd\u591f\u81ea\u52a8\u9a8c\u8bc1VLM\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\uff0c\u8bc6\u522b\u88ab\u5f15\u7528\u6982\u5ff5\u5728\u56e0\u679c\u6d4b\u8bd5\u4e2d\u5931\u8d25\u7684\u60c5\u51b5\uff0c\u4e3a\u6a21\u578b\u6cbb\u7406\u63d0\u4f9b\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2510.00054", "pdf": "https://arxiv.org/pdf/2510.00054", "abs": "https://arxiv.org/abs/2510.00054", "authors": ["Xianjie Liu", "Yiman Hu", "Yixiong Zou", "Liang Wu", "Jian Xu", "Bo Zheng"], "title": "HiDe: Rethinking The Zoom-IN method in High Resolution MLLMs via Hierarchical Decoupling", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have made significant strides in visual understanding tasks. However, their performance on high-resolution images remains suboptimal. While existing approaches often attribute this limitation to perceptual constraints and argue that MLLMs struggle to recognize small objects, leading them to use \"zoom in\" strategies for better detail, our analysis reveals a different cause: the main issue is not object size, but rather caused by complex background interference. We systematically analyze this \"zoom in\" operation through a series of decoupling experiments and propose the Hierarchical Decoupling Framework (HiDe), a training-free framework that uses Token-wise Attention Decoupling (TAD) to decouple the question tokens and identify the key information tokens, then leverages their attention weights to achieve precise alignment with the target visual regions. Subsequently, it employs Layout-Preserving Decoupling (LPD) to decouple these regions from the background and reconstructs a compact representation that preserves essential spatial layouts while eliminating background interference. HiDe sets a new SOTA on V*Bench, HRBench4K, and HRBench8K, boosting Qwen2.5-VL 7B and InternVL3 8B to SOTA (92.1% and 91.6% on V*Bench), even surpassing RL methods. After optimization, HiDe uses 75% less memory than the previous training-free approach. Code is provided in https://github.com/Tennine2077/HiDe.", "AI": {"tldr": "\u63d0\u51fa\u4e86HiDe\u6846\u67b6\uff0c\u901a\u8fc7Token-wise Attention Decoupling\u548cLayout-Preserving Decoupling\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u7684\u80cc\u666f\u5e72\u6270\u95ee\u9898\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8ba4\u4e3aMLLMs\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u8868\u73b0\u4e0d\u4f73\u662f\u56e0\u4e3a\u611f\u77e5\u9650\u5236\u548c\u5c0f\u7269\u4f53\u8bc6\u522b\u56f0\u96be\uff0c\u4f46\u672c\u6587\u5206\u6790\u53d1\u73b0\u4e3b\u8981\u95ee\u9898\u5176\u5b9e\u662f\u590d\u6742\u80cc\u666f\u5e72\u6270\u800c\u975e\u7269\u4f53\u5927\u5c0f\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u89e3\u8026\u6846\u67b6HiDe\uff1a1) Token-wise Attention Decoupling\u89e3\u8026\u95ee\u9898token\u8bc6\u522b\u5173\u952e\u4fe1\u606ftoken\uff1b2) Layout-Preserving Decoupling\u4ece\u80cc\u666f\u4e2d\u89e3\u8026\u76ee\u6807\u533a\u57df\u5e76\u91cd\u5efa\u7d27\u51d1\u8868\u793a\u3002", "result": "\u5728V*Bench\u3001HRBench4K\u548cHRBench8K\u4e0a\u8fbe\u5230\u65b0SOTA\uff0c\u5c06Qwen2.5-VL 7B\u548cInternVL3 8B\u5206\u522b\u63d0\u5347\u81f392.1%\u548c91.6%\uff0c\u8d85\u8d8aRL\u65b9\u6cd5\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c1175%\u3002", "conclusion": "HiDe\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u7684\u80cc\u666f\u5e72\u6270\u95ee\u9898\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347MLLMs\u6027\u80fd\uff0c\u4e3a\u89c6\u89c9\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.00293", "pdf": "https://arxiv.org/pdf/2510.00293", "abs": "https://arxiv.org/abs/2510.00293", "authors": ["Samar Fares", "Nurbek Tastan", "Noor Hussein", "Karthik Nandakumar"], "title": "MOLM: Mixture of LoRA Markers", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": "21 pages, 11 figures, Under review at ICLR 2026", "summary": "Generative models can generate photorealistic images at scale. This raises urgent concerns about the ability to detect synthetically generated images and attribute these images to specific sources. While watermarking has emerged as a possible solution, existing methods remain fragile to realistic distortions, susceptible to adaptive removal, and expensive to update when the underlying watermarking key changes. We propose a general watermarking framework that formulates the encoding problem as key-dependent perturbation of the parameters of a generative model. Within this framework, we introduce Mixture of LoRA Markers (MOLM), a routing-based instantiation in which binary keys activate lightweight LoRA adapters inside residual and attention blocks. This design avoids key-specific re-training and achieves the desired properties such as imperceptibility, fidelity, verifiability, and robustness. Experiments on Stable Diffusion and FLUX show that MOLM preserves image quality while achieving robust key recovery against distortions, compression and regeneration, averaging attacks, and black-box adversarial attacks on the extractor.", "AI": {"tldr": "\u63d0\u51faMOLM\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7LoRA\u9002\u914d\u5668\u5b9e\u73b0\u751f\u6210\u6a21\u578b\u7684\u53c2\u6570\u6270\u52a8\uff0c\u5728\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u4f9b\u9c81\u68d2\u7684\u6c34\u5370\u9a8c\u8bc1\u80fd\u529b\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u80fd\u5927\u89c4\u6a21\u751f\u6210\u903c\u771f\u56fe\u50cf\uff0c\u5f15\u53d1\u5bf9\u68c0\u6d4b\u548c\u6eaf\u6e90\u5408\u6210\u56fe\u50cf\u7684\u62c5\u5fe7\u3002\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u5bf9\u771f\u5b9e\u5931\u771f\u8106\u5f31\u3001\u6613\u88ab\u81ea\u9002\u5e94\u79fb\u9664\uff0c\u4e14\u66f4\u65b0\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8def\u7531\u7684MOLM\u6846\u67b6\uff0c\u4f7f\u7528\u4e8c\u8fdb\u5236\u5bc6\u94a5\u6fc0\u6d3b\u8f7b\u91cf\u7ea7LoRA\u9002\u914d\u5668\uff0c\u907f\u514d\u5bc6\u94a5\u7279\u5b9a\u91cd\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e0d\u53ef\u611f\u77e5\u6027\u3001\u4fdd\u771f\u5ea6\u3001\u53ef\u9a8c\u8bc1\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5728Stable Diffusion\u548cFLUX\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMOLM\u5728\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u80fd\u6709\u6548\u62b5\u6297\u5931\u771f\u3001\u538b\u7f29\u3001\u518d\u751f\u3001\u5e73\u5747\u653b\u51fb\u548c\u9ed1\u76d2\u5bf9\u6297\u653b\u51fb\u3002", "conclusion": "MOLM\u6846\u67b6\u4e3a\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6c34\u5370\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u56fe\u50cf\u8d28\u91cf\u4e0e\u6c34\u5370\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.00376", "pdf": "https://arxiv.org/pdf/2510.00376", "abs": "https://arxiv.org/abs/2510.00376", "authors": ["Arpan Mahara", "Md Rezaul Karim Khan", "Naphtali Rishe", "Wenjia Wang", "Seyed Masoud Sadjadi"], "title": "Discrete Wavelet Transform as a Facilitator for Expressive Latent Space Representation in Variational Autoencoders in Satellite Imagery", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 3 Figures", "summary": "Latent Diffusion Models (LDM), a subclass of diffusion models, mitigate the computational complexity of pixel-space diffusion by operating within a compressed latent space constructed by Variational Autoencoders (VAEs), demonstrating significant advantages in Remote Sensing (RS) applications. Though numerous studies enhancing LDMs have been conducted, investigations explicitly targeting improvements within the intrinsic latent space remain scarce. This paper proposes an innovative perspective, utilizing the Discrete Wavelet Transform (DWT) to enhance the VAE's latent space representation, designed for satellite imagery. The proposed method, ExpDWT-VAE, introduces dual branches: one processes spatial domain input through convolutional operations, while the other extracts and processes frequency-domain features via 2D Haar wavelet decomposition, convolutional operation, and inverse DWT reconstruction. These branches merge to create an integrated spatial-frequency representation, further refined through convolutional and diagonal Gaussian mapping into a robust latent representation. We utilize a new satellite imagery dataset housed by the TerraFly mapping system to validate our method. Experimental results across several performance metrics highlight the efficacy of the proposed method at enhancing latent space representation.", "AI": {"tldr": "\u63d0\u51faExpDWT-VAE\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\u589e\u5f3aVAE\u7684\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\uff0c\u4e13\u95e8\u9488\u5bf9\u536b\u661f\u56fe\u50cf\u8bbe\u8ba1\uff0c\u5728\u9065\u611f\u5e94\u7528\u4e2d\u63d0\u5347\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u867d\u7136\u5df2\u6709\u8bb8\u591a\u7814\u7a76\u6539\u8fdb\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u4f46\u9488\u5bf9\u5185\u5728\u6f5c\u5728\u7a7a\u95f4\u6539\u8fdb\u7684\u7814\u7a76\u4ecd\u7136\u7a00\u7f3a\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u589e\u5f3aVAE\u7684\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\u6765\u63d0\u5347\u9065\u611f\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faExpDWT-VAE\u65b9\u6cd5\uff0c\u91c7\u7528\u53cc\u5206\u652f\u7ed3\u6784\uff1a\u4e00\u4e2a\u5206\u652f\u901a\u8fc7\u5377\u79ef\u5904\u7406\u7a7a\u95f4\u57df\u8f93\u5165\uff0c\u53e6\u4e00\u4e2a\u5206\u652f\u901a\u8fc72D Haar\u5c0f\u6ce2\u5206\u89e3\u3001\u5377\u79ef\u64cd\u4f5c\u548c\u9006DWT\u91cd\u6784\u63d0\u53d6\u548c\u5904\u7406\u9891\u57df\u7279\u5f81\u3002\u4e24\u4e2a\u5206\u652f\u878d\u5408\u5f62\u6210\u96c6\u6210\u7a7a\u95f4-\u9891\u7387\u8868\u793a\uff0c\u518d\u901a\u8fc7\u5377\u79ef\u548c\u5bf9\u89d2\u9ad8\u65af\u6620\u5c04\u7cbe\u70bc\u4e3a\u9c81\u68d2\u7684\u6f5c\u5728\u8868\u793a\u3002", "result": "\u5728TerraFly\u6620\u5c04\u7cfb\u7edf\u7684\u65b0\u536b\u661f\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u591a\u4e2a\u6027\u80fd\u6307\u6807\u7684\u5b9e\u9a8c\u7ed3\u679c\u7a81\u663e\u4e86\u6240\u63d0\u65b9\u6cd5\u5728\u589e\u5f3a\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "ExpDWT-VAE\u65b9\u6cd5\u901a\u8fc7\u96c6\u6210\u7a7a\u95f4\u548c\u9891\u57df\u7279\u5f81\uff0c\u6210\u529f\u589e\u5f3a\u4e86VAE\u7684\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\uff0c\u4e3a\u9065\u611f\u5e94\u7528\u4e2d\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u6848\u3002"}}
{"id": "2510.00438", "pdf": "https://arxiv.org/pdf/2510.00438", "abs": "https://arxiv.org/abs/2510.00438", "authors": ["Zhaoyang Li", "Dongjun Qian", "Kai Su", "Qishuai Diao", "Xiangyang Xia", "Chang Liu", "Wenfei Yang", "Tianzhu Zhang", "Zehuan Yuan"], "title": "BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion Transformer has shown remarkable abilities in generating high-fidelity videos, delivering visually coherent frames and rich details over extended durations. However, existing video generation models still fall short in subject-consistent video generation due to an inherent difficulty in parsing prompts that specify complex spatial relationships, temporal logic, and interactions among multiple subjects. To address this issue, we propose BindWeave, a unified framework that handles a broad range of subject-to-video scenarios from single-subject cases to complex multi-subject scenes with heterogeneous entities. To bind complex prompt semantics to concrete visual subjects, we introduce an MLLM-DiT framework in which a pretrained multimodal large language model performs deep cross-modal reasoning to ground entities and disentangle roles, attributes, and interactions, yielding subject-aware hidden states that condition the diffusion transformer for high-fidelity subject-consistent video generation. Experiments on the OpenS2V benchmark demonstrate that our method achieves superior performance across subject consistency, naturalness, and text relevance in generated videos, outperforming existing open-source and commercial models.", "AI": {"tldr": "BindWeave\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7MLLM-DiT\u67b6\u6784\u89e3\u51b3\u591a\u4e3b\u4f53\u4e00\u81f4\u6027\u89c6\u9891\u751f\u6210\u7684\u96be\u9898\uff0c\u5728\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u548c\u65f6\u5e8f\u903b\u8f91\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u4e3b\u4f53\u4e00\u81f4\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u3001\u65f6\u5e8f\u903b\u8f91\u548c\u591a\u4e3b\u4f53\u4ea4\u4e92\u65f6\u96be\u4ee5\u4fdd\u6301\u4e3b\u4f53\u4e00\u81f4\u6027\uff0c\u65e0\u6cd5\u51c6\u786e\u89e3\u6790\u5305\u542b\u591a\u4e2a\u4e3b\u4f53\u53ca\u5176\u4ea4\u4e92\u7684\u63d0\u793a\u8bcd\u3002", "method": "\u63d0\u51faBindWeave\u6846\u67b6\uff0c\u91c7\u7528\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6df1\u5ea6\u8de8\u6a21\u6001\u63a8\u7406\uff0c\u8bc6\u522b\u5b9e\u4f53\u5e76\u89e3\u8026\u89d2\u8272\u3001\u5c5e\u6027\u548c\u4ea4\u4e92\u5173\u7cfb\uff0c\u751f\u6210\u4e3b\u4f53\u611f\u77e5\u7684\u9690\u85cf\u72b6\u6001\u6765\u6761\u4ef6\u5316\u6269\u6563\u53d8\u6362\u5668\u3002", "result": "\u5728OpenS2V\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4e3b\u4f53\u4e00\u81f4\u6027\u3001\u81ea\u7136\u5ea6\u548c\u6587\u672c\u76f8\u5173\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u548c\u5546\u4e1a\u6a21\u578b\u3002", "conclusion": "BindWeave\u901a\u8fc7\u7edf\u4e00\u7684MLLM-DiT\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u591a\u4e3b\u4f53\u89c6\u9891\u751f\u6210\u4e2d\u7684\u4e3b\u4f53\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e3a\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.00454", "pdf": "https://arxiv.org/pdf/2510.00454", "abs": "https://arxiv.org/abs/2510.00454", "authors": ["Wang Zhang", "Huaqiu Li", "Xiaowan Hu", "Tao Jiang", "Zikang Chen", "Haoqian Wang"], "title": "Measuring and Controlling the Spectral Bias for Self-Supervised Image Denoising", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Current self-supervised denoising methods for paired noisy images typically involve mapping one noisy image through the network to the other noisy image. However, after measuring the spectral bias of such methods using our proposed Image Pair Frequency-Band Similarity, it suffers from two practical limitations. Firstly, the high-frequency structural details in images are not preserved well enough. Secondly, during the process of fitting high frequencies, the network learns high-frequency noise from the mapped noisy images. To address these challenges, we introduce a Spectral Controlling network (SCNet) to optimize self-supervised denoising of paired noisy images. First, we propose a selection strategy to choose frequency band components for noisy images, to accelerate the convergence speed of training. Next, we present a parameter optimization method that restricts the learning ability of convolutional kernels to high-frequency noise using the Lipschitz constant, without changing the network structure. Finally, we introduce the Spectral Separation and low-rank Reconstruction module (SSR module), which separates noise and high-frequency details through frequency domain separation and low-rank space reconstruction, to retain the high-frequency structural details of images. Experiments performed on synthetic and real-world datasets verify the effectiveness of SCNet.", "AI": {"tldr": "\u63d0\u51faSCNet\u7f51\u7edc\u4f18\u5316\u81ea\u76d1\u7763\u56fe\u50cf\u53bb\u566a\uff0c\u901a\u8fc7\u9891\u7387\u63a7\u5236\u89e3\u51b3\u9ad8\u9891\u7ec6\u8282\u4e22\u5931\u548c\u566a\u58f0\u5b66\u4e60\u95ee\u9898", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u53bb\u566a\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a\u9ad8\u9891\u7ed3\u6784\u7ec6\u8282\u4fdd\u7559\u4e0d\u8db3\uff0c\u4ee5\u53ca\u7f51\u7edc\u5728\u5b66\u4e60\u9ad8\u9891\u65f6\u4ece\u566a\u58f0\u56fe\u50cf\u4e2d\u5b66\u4e60\u5230\u9ad8\u9891\u566a\u58f0", "method": "1. \u9891\u7387\u5e26\u9009\u62e9\u7b56\u7565\u52a0\u901f\u8bad\u7ec3\u6536\u655b\uff1b2. \u57fa\u4e8eLipschitz\u5e38\u6570\u7684\u53c2\u6570\u4f18\u5316\u65b9\u6cd5\u9650\u5236\u5377\u79ef\u6838\u5bf9\u9ad8\u9891\u566a\u58f0\u7684\u5b66\u4e60\u80fd\u529b\uff1b3. \u9891\u8c31\u5206\u79bb\u548c\u4f4e\u79e9\u91cd\u5efa\u6a21\u5757(SSR)\u901a\u8fc7\u9891\u57df\u5206\u79bb\u548c\u4f4e\u79e9\u7a7a\u95f4\u91cd\u5efa\u5206\u79bb\u566a\u58f0\u4e0e\u9ad8\u9891\u7ec6\u8282", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SCNet\u7684\u6709\u6548\u6027", "conclusion": "SCNet\u901a\u8fc7\u9891\u8c31\u63a7\u5236\u6210\u529f\u89e3\u51b3\u4e86\u81ea\u76d1\u7763\u53bb\u566a\u4e2d\u7684\u9ad8\u9891\u7ec6\u8282\u4fdd\u7559\u548c\u566a\u58f0\u5b66\u4e60\u95ee\u9898"}}
{"id": "2510.00527", "pdf": "https://arxiv.org/pdf/2510.00527", "abs": "https://arxiv.org/abs/2510.00527", "authors": ["Taeyun Woo", "Jinah Park", "Tae-Kyun Kim"], "title": "Cascaded Diffusion Framework for Probabilistic Coarse-to-Fine Hand Pose Estimation", "categories": ["cs.CV"], "comment": "15 pages, 8 figures", "summary": "Deterministic models for 3D hand pose reconstruction, whether single-staged or cascaded, struggle with pose ambiguities caused by self-occlusions and complex hand articulations. Existing cascaded approaches refine predictions in a coarse-to-fine manner but remain deterministic and cannot capture pose uncertainties. Recent probabilistic methods model pose distributions yet are restricted to single-stage estimation, which often fails to produce accurate 3D reconstructions without refinement. To address these limitations, we propose a coarse-to-fine cascaded diffusion framework that combines probabilistic modeling with cascaded refinement. The first stage is a joint diffusion model that samples diverse 3D joint hypotheses, and the second stage is a Mesh Latent Diffusion Model (Mesh LDM) that reconstructs a 3D hand mesh conditioned on a joint sample. By training Mesh LDM with diverse joint hypotheses in a learned latent space, our framework learns distribution-aware joint-mesh relationships and robust hand priors. Furthermore, the cascaded design mitigates the difficulty of directly mapping 2D images to dense 3D poses, enhancing accuracy through sequential refinement. Experiments on FreiHAND and HO3Dv2 demonstrate that our method achieves state-of-the-art performance while effectively modeling pose distributions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u7c97\u5230\u7cbe\u7684\u7ea7\u8054\u6269\u6563\u6846\u67b6\uff0c\u7ed3\u5408\u6982\u7387\u5efa\u6a21\u4e0e\u7ea7\u8054\u4f18\u5316\uff0c\u7528\u4e8e3D\u624b\u90e8\u59ff\u6001\u91cd\u5efa\uff0c\u89e3\u51b3\u59ff\u6001\u6a21\u7cca\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u786e\u5b9a\u6027\u6a21\u578b\u96be\u4ee5\u5904\u7406\u81ea\u906e\u6321\u548c\u590d\u6742\u624b\u90e8\u5173\u8282\u5e26\u6765\u7684\u59ff\u6001\u6a21\u7cca\u6027\uff0c\u800c\u73b0\u6709\u7684\u6982\u7387\u65b9\u6cd5\u4ec5\u9650\u4e8e\u5355\u9636\u6bb5\u4f30\u8ba1\uff0c\u65e0\u6cd5\u8fdb\u884c\u7ec6\u5316\u3002", "method": "\u4f7f\u7528\u4e24\u9636\u6bb5\u7ea7\u8054\u6269\u6563\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u662f\u8054\u5408\u6269\u6563\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u76843D\u5173\u8282\u5047\u8bbe\uff0c\u7b2c\u4e8c\u9636\u6bb5\u662f\u57fa\u4e8e\u7f51\u683c\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u5728\u5173\u8282\u6837\u672c\u6761\u4ef6\u4e0b\u91cd\u5efa3D\u624b\u90e8\u7f51\u683c\u3002", "result": "\u5728FreiHAND\u548cHO3Dv2\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u6709\u6548\u5efa\u6a21\u4e86\u59ff\u6001\u5206\u5e03\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u7ed3\u5408\u4e86\u6982\u7387\u5efa\u6a21\u4e0e\u7ea7\u8054\u4f18\u5316\uff0c\u89e3\u51b3\u4e863D\u624b\u90e8\u59ff\u6001\u91cd\u5efa\u4e2d\u7684\u6a21\u7cca\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u6311\u6218\u3002"}}
{"id": "2510.00570", "pdf": "https://arxiv.org/pdf/2510.00570", "abs": "https://arxiv.org/abs/2510.00570", "authors": ["Minghao Yang", "Ren Togo", "Guang Li", "Takahiro Ogawa", "Miki Haseyama"], "title": "Adaptive Shared Experts with LoRA-Based Mixture of Experts for Multi-Task Learning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Mixture-of-Experts (MoE) has emerged as a powerful framework for multi-task learning (MTL). However, existing MoE-MTL methods often rely on single-task pretrained backbones and suffer from redundant adaptation and inefficient knowledge sharing during the transition from single-task to multi-task learning (STL to MTL). To address these limitations, we propose adaptive shared experts (ASE) within a low-rank adaptation (LoRA) based MoE, where shared experts are assigned router-computed gating weights jointly normalized with sparse experts. This design facilitates STL to MTL transition, enhances expert specialization, and cooperation. Furthermore, we incorporate fine-grained experts by increasing the number of LoRA experts while proportionally reducing their rank, enabling more effective knowledge sharing under a comparable parameter budget. Extensive experiments on the PASCAL-Context benchmark, under unified training settings, demonstrate that ASE consistently improves performance across diverse configurations and validates the effectiveness of fine-grained designs for MTL.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u5171\u4eab\u4e13\u5bb6(ASE)\u6846\u67b6\uff0c\u5728\u57fa\u4e8eLoRA\u7684MoE\u4e2d\u5f15\u5165\u5171\u4eab\u4e13\u5bb6\uff0c\u901a\u8fc7\u8def\u7531\u5668\u8ba1\u7b97\u7684\u95e8\u63a7\u6743\u91cd\u5b9e\u73b0\u4ece\u5355\u4efb\u52a1\u5230\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u5e73\u6ed1\u8fc7\u6e21\uff0c\u63d0\u5347\u4e13\u5bb6\u4e13\u4e1a\u5316\u548c\u534f\u4f5c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709MoE-MTL\u65b9\u6cd5\u4f9d\u8d56\u5355\u4efb\u52a1\u9884\u8bad\u7ec3\u9aa8\u5e72\u7f51\u7edc\uff0c\u5728\u4ece\u5355\u4efb\u52a1\u5230\u591a\u4efb\u52a1\u5b66\u4e60\u8f6c\u6362\u8fc7\u7a0b\u4e2d\u5b58\u5728\u5197\u4f59\u9002\u914d\u548c\u77e5\u8bc6\u5171\u4eab\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u5728LoRA-based MoE\u4e2d\u5f15\u5165\u81ea\u9002\u5e94\u5171\u4eab\u4e13\u5bb6\uff0c\u5171\u4eab\u4e13\u5bb6\u4e0e\u7a00\u758f\u4e13\u5bb6\u8054\u5408\u5f52\u4e00\u5316\u95e8\u63a7\u6743\u91cd\uff1b\u901a\u8fc7\u589e\u52a0LoRA\u4e13\u5bb6\u6570\u91cf\u540c\u65f6\u6309\u6bd4\u4f8b\u964d\u4f4e\u5176\u79e9\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u4e13\u5bb6\u8bbe\u8ba1\u3002", "result": "\u5728PASCAL-Context\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cASE\u5728\u4e0d\u540c\u914d\u7f6e\u4e0b\u5747\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u7ec6\u7c92\u5ea6\u8bbe\u8ba1\u5bf9MTL\u7684\u6709\u6548\u6027\u3002", "conclusion": "ASE\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86STL\u5230MTL\u8f6c\u6362\u4e2d\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5171\u4eab\u4e13\u5bb6\u548c\u7ec6\u7c92\u5ea6\u4e13\u5bb6\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u77e5\u8bc6\u5171\u4eab\u548c\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.00578", "pdf": "https://arxiv.org/pdf/2510.00578", "abs": "https://arxiv.org/abs/2510.00578", "authors": ["Guozhen Zhang", "Haiguang Wang", "Chunyu Wang", "Yuan Zhou", "Qinglin Lu", "Limin Wang"], "title": "Arbitrary Generative Video Interpolation", "categories": ["cs.CV"], "comment": null, "summary": "Video frame interpolation (VFI), which generates intermediate frames from given start and end frames, has become a fundamental function in video generation applications. However, existing generative VFI methods are constrained to synthesize a fixed number of intermediate frames, lacking the flexibility to adjust generated frame rates or total sequence duration. In this work, we present ArbInterp, a novel generative VFI framework that enables efficient interpolation at any timestamp and of any length. Specifically, to support interpolation at any timestamp, we propose the Timestamp-aware Rotary Position Embedding (TaRoPE), which modulates positions in temporal RoPE to align generated frames with target normalized timestamps. This design enables fine-grained control over frame timestamps, addressing the inflexibility of fixed-position paradigms in prior work. For any-length interpolation, we decompose long-sequence generation into segment-wise frame synthesis. We further design a novel appearance-motion decoupled conditioning strategy: it leverages prior segment endpoints to enforce appearance consistency and temporal semantics to maintain motion coherence, ensuring seamless spatiotemporal transitions across segments. Experimentally, we build comprehensive benchmarks for multi-scale frame interpolation (2x to 32x) to assess generalizability across arbitrary interpolation factors. Results show that ArbInterp outperforms prior methods across all scenarios with higher fidelity and more seamless spatiotemporal continuity. Project website: https://mcg-nju.github.io/ArbInterp-Web/.", "AI": {"tldr": "ArbInterp\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u89c6\u9891\u5e27\u63d2\u503c\u6846\u67b6\uff0c\u652f\u6301\u4efb\u610f\u65f6\u95f4\u6233\u548c\u4efb\u610f\u957f\u5ea6\u7684\u63d2\u503c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u53ea\u80fd\u751f\u6210\u56fa\u5b9a\u6570\u91cf\u4e2d\u95f4\u5e27\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u751f\u6210\u5f0f\u89c6\u9891\u5e27\u63d2\u503c\u65b9\u6cd5\u53ea\u80fd\u5408\u6210\u56fa\u5b9a\u6570\u91cf\u7684\u4e2d\u95f4\u5e27\uff0c\u7f3a\u4e4f\u8c03\u6574\u751f\u6210\u5e27\u7387\u6216\u603b\u5e8f\u5217\u65f6\u957f\u7684\u7075\u6d3b\u6027\u3002", "method": "\u63d0\u51faTimestamp-aware Rotary Position Embedding (TaRoPE)\u6765\u652f\u6301\u4efb\u610f\u65f6\u95f4\u6233\u63d2\u503c\uff0c\u4ee5\u53ca\u5c06\u957f\u5e8f\u5217\u751f\u6210\u5206\u89e3\u4e3a\u5206\u6bb5\u5e27\u5408\u6210\u7684\u4efb\u610f\u957f\u5ea6\u63d2\u503c\u65b9\u6cd5\uff0c\u91c7\u7528\u5916\u89c2-\u8fd0\u52a8\u89e3\u8026\u6761\u4ef6\u7b56\u7565\u786e\u4fdd\u8de8\u6bb5\u65f6\u7a7a\u8fde\u7eed\u6027\u3002", "result": "\u57282x\u523032x\u591a\u5c3a\u5ea6\u5e27\u63d2\u503c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cArbInterp\u5728\u6240\u6709\u573a\u666f\u4e0b\u90fd\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u4fdd\u771f\u5ea6\u548c\u66f4\u65e0\u7f1d\u7684\u65f6\u7a7a\u8fde\u7eed\u6027\u3002", "conclusion": "ArbInterp\u901a\u8fc7\u521b\u65b0\u7684\u65f6\u95f4\u6233\u611f\u77e5\u4f4d\u7f6e\u7f16\u7801\u548c\u5206\u6bb5\u751f\u6210\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u7075\u6d3b\u9ad8\u6548\u7684\u4efb\u610f\u65f6\u95f4\u6233\u548c\u4efb\u610f\u957f\u5ea6\u89c6\u9891\u5e27\u63d2\u503c\u3002"}}
{"id": "2510.00592", "pdf": "https://arxiv.org/pdf/2510.00592", "abs": "https://arxiv.org/abs/2510.00592", "authors": ["Zesheng Li", "Shuaibo Li", "Wei Ma", "Jianwei Guo", "Hongbin Zha"], "title": "Multi-level Dynamic Style Transfer for NeRFs", "categories": ["cs.CV"], "comment": "Accepted by Computational Visual Media Journal (CVMJ)", "summary": "As the application of neural radiance fields (NeRFs) in various 3D vision tasks continues to expand, numerous NeRF-based style transfer techniques have been developed. However, existing methods typically integrate style statistics into the original NeRF pipeline, often leading to suboptimal results in both content preservation and artistic stylization. In this paper, we present multi-level dynamic style transfer for NeRFs (MDS-NeRF), a novel approach that reengineers the NeRF pipeline specifically for stylization and incorporates an innovative dynamic style injection module. Particularly, we propose a multi-level feature adaptor that helps generate a multi-level feature grid representation from the content radiance field, effectively capturing the multi-scale spatial structure of the scene. In addition, we present a dynamic style injection module that learns to extract relevant style features and adaptively integrates them into the content patterns. The stylized multi-level features are then transformed into the final stylized view through our proposed multi-level cascade decoder. Furthermore, we extend our 3D style transfer method to support omni-view style transfer using 3D style references. Extensive experiments demonstrate that MDS-NeRF achieves outstanding performance for 3D style transfer, preserving multi-scale spatial structures while effectively transferring stylistic characteristics.", "AI": {"tldr": "MDS-NeRF\u662f\u4e00\u79cd\u9488\u5bf9\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u76843D\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7ea7\u7279\u5f81\u9002\u914d\u5668\u548c\u52a8\u6001\u98ce\u683c\u6ce8\u5165\u6a21\u5757\uff0c\u5728\u4fdd\u6301\u573a\u666f\u591a\u5c3a\u5ea6\u7a7a\u95f4\u7ed3\u6784\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u8d28\u91cf\u98ce\u683c\u5316\u3002", "motivation": "\u73b0\u6709NeRF\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\u901a\u5e38\u5c06\u98ce\u683c\u7edf\u8ba1\u4fe1\u606f\u76f4\u63a5\u96c6\u6210\u5230\u539f\u59cbNeRF\u6d41\u7a0b\u4e2d\uff0c\u5bfc\u81f4\u5185\u5bb9\u4fdd\u6301\u548c\u827a\u672f\u98ce\u683c\u5316\u6548\u679c\u90fd\u4e0d\u7406\u60f3\u3002", "method": "\u63d0\u51fa\u591a\u7ea7\u52a8\u6001\u98ce\u683c\u8fc1\u79fb\u6846\u67b6\uff0c\u5305\u62ec\uff1a\u591a\u7ea7\u7279\u5f81\u9002\u914d\u5668\u751f\u6210\u591a\u7ea7\u7279\u5f81\u7f51\u683c\u8868\u793a\uff1b\u52a8\u6001\u98ce\u683c\u6ce8\u5165\u6a21\u5757\u5b66\u4e60\u63d0\u53d6\u76f8\u5173\u98ce\u683c\u7279\u5f81\u5e76\u81ea\u9002\u5e94\u96c6\u6210\uff1b\u591a\u7ea7\u7ea7\u8054\u89e3\u7801\u5668\u5c06\u98ce\u683c\u5316\u7279\u5f81\u8f6c\u6362\u4e3a\u6700\u7ec8\u98ce\u683c\u5316\u89c6\u56fe\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eMDS-NeRF\u57283D\u98ce\u683c\u8fc1\u79fb\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u4fdd\u6301\u591a\u5c3a\u5ea6\u7a7a\u95f4\u7ed3\u6784\u540c\u65f6\u6709\u6548\u4f20\u9012\u98ce\u683c\u7279\u5f81\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u91cd\u6784\u4e86NeRF\u6d41\u7a0b\u4ee5\u9002\u5e94\u98ce\u683c\u5316\u9700\u6c42\uff0c\u5e76\u652f\u6301\u4f7f\u75283D\u98ce\u683c\u53c2\u8003\u8fdb\u884c\u5168\u89c6\u89d2\u98ce\u683c\u8fc1\u79fb\u3002"}}
{"id": "2510.00624", "pdf": "https://arxiv.org/pdf/2510.00624", "abs": "https://arxiv.org/abs/2510.00624", "authors": ["Mengfei Xia", "Nan Xue", "Jiapeng Zhu", "Yujun Shen"], "title": "UCD: Unconditional Discriminator Promotes Nash Equilibrium in GANs", "categories": ["cs.CV"], "comment": null, "summary": "Adversarial training turns out to be the key to one-step generation, especially for Generative Adversarial Network (GAN) and diffusion model distillation. Yet in practice, GAN training hardly converges properly and struggles in mode collapse. In this work, we quantitatively analyze the extent of Nash equilibrium in GAN training, and conclude that redundant shortcuts by inputting condition in $D$ disables meaningful knowledge extraction. We thereby propose to employ an unconditional discriminator (UCD), in which $D$ is enforced to extract more comprehensive and robust features with no condition injection. In this way, $D$ is able to leverage better knowledge to supervise $G$, which promotes Nash equilibrium in GAN literature. Theoretical guarantee on compatibility with vanilla GAN theory indicates that UCD can be implemented in a plug-in manner. Extensive experiments confirm the significant performance improvements with high efficiency. For instance, we achieved \\textbf{1.47 FID} on the ImageNet-64 dataset, surpassing StyleGAN-XL and several state-of-the-art one-step diffusion models. The code will be made publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u65e0\u6761\u4ef6\u5224\u522b\u5668(UCD)\u6765\u89e3\u51b3GAN\u8bad\u7ec3\u4e2d\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u901a\u8fc7\u5f3a\u5236\u5224\u522b\u5668\u63d0\u53d6\u66f4\u5168\u9762\u7684\u7279\u5f81\u6765\u4fc3\u8fdb\u7eb3\u4ec0\u5747\u8861\uff0c\u5728ImageNet-64\u4e0a\u8fbe\u5230\u4e861.47 FID\u7684\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u5bf9\u6297\u8bad\u7ec3\u5728\u4e00\u6b65\u751f\u6210\u4e2d\u5f88\u5173\u952e\uff0c\u4f46GAN\u8bad\u7ec3\u96be\u4ee5\u6b63\u786e\u6536\u655b\u4e14\u5bb9\u6613\u9677\u5165\u6a21\u5f0f\u5d29\u6e83\u3002\u7814\u7a76\u53d1\u73b0\u8f93\u5165\u6761\u4ef6\u5230\u5224\u522b\u5668\u4f1a\u4ea7\u751f\u5197\u4f59\u6377\u5f84\uff0c\u963b\u788d\u6709\u610f\u4e49\u7684\u7279\u5f81\u63d0\u53d6\u3002", "method": "\u63d0\u51fa\u65e0\u6761\u4ef6\u5224\u522b\u5668(UCD)\uff0c\u5f3a\u5236\u5224\u522b\u5668\u5728\u4e0d\u6ce8\u5165\u6761\u4ef6\u7684\u60c5\u51b5\u4e0b\u63d0\u53d6\u66f4\u5168\u9762\u548c\u9c81\u68d2\u7684\u7279\u5f81\uff0c\u4ece\u800c\u4e3a\u751f\u6210\u5668\u63d0\u4f9b\u66f4\u597d\u7684\u76d1\u7763\uff0c\u4fc3\u8fdb\u7eb3\u4ec0\u5747\u8861\u3002", "result": "\u5728ImageNet-64\u6570\u636e\u96c6\u4e0a\u8fbe\u52301.47 FID\uff0c\u8d85\u8d8a\u4e86StyleGAN-XL\u548c\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u4e00\u6b65\u6269\u6563\u6a21\u578b\uff0c\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u548c\u9ad8\u6548\u7387\u3002", "conclusion": "\u65e0\u6761\u4ef6\u5224\u522b\u5668\u80fd\u591f\u6709\u6548\u89e3\u51b3GAN\u8bad\u7ec3\u4e2d\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u4e0e\u539f\u59cbGAN\u7406\u8bba\u517c\u5bb9\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u63d2\u4ef6\u65b9\u5f0f\u5b9e\u73b0\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2510.00633", "pdf": "https://arxiv.org/pdf/2510.00633", "abs": "https://arxiv.org/abs/2510.00633", "authors": ["Yannick Hauri", "Luca A. Lanzend\u00f6rfer", "Till Aczel"], "title": "Virtual Fashion Photo-Shoots: Building a Large-Scale Garment-Lookbook Dataset", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Fashion image generation has so far focused on narrow tasks such as virtual try-on, where garments appear in clean studio environments. In contrast, editorial fashion presents garments through dynamic poses, diverse locations, and carefully crafted visual narratives. We introduce the task of virtual fashion photo-shoot, which seeks to capture this richness by transforming standardized garment images into contextually grounded editorial imagery. To enable this new direction, we construct the first large-scale dataset of garment-lookbook pairs, bridging the gap between e-commerce and fashion media. Because such pairs are not readily available, we design an automated retrieval pipeline that aligns garments across domains, combining visual-language reasoning with object-level localization. We construct a dataset with three garment-lookbook pair accuracy levels: high quality (10,000 pairs), medium quality (50,000 pairs), and low quality (300,000 pairs). This dataset offers a foundation for models that move beyond catalog-style generation and toward fashion imagery that reflects creativity, atmosphere, and storytelling.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u865a\u62df\u65f6\u5c1a\u6444\u5f71\u4efb\u52a1\uff0c\u65e8\u5728\u5c06\u6807\u51c6\u5316\u7684\u670d\u88c5\u56fe\u50cf\u8f6c\u5316\u4e3a\u5177\u6709\u60c5\u5883\u80cc\u666f\u7684\u7f16\u8f91\u56fe\u50cf\uff0c\u5e76\u6784\u5efa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u670d\u88c5-\u753b\u518c\u914d\u5bf9\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u65f6\u5c1a\u56fe\u50cf\u751f\u6210\u4e3b\u8981\u5173\u6ce8\u865a\u62df\u8bd5\u7a7f\u7b49\u72ed\u7a84\u4efb\u52a1\uff0c\u800c\u7f16\u8f91\u65f6\u5c1a\u901a\u8fc7\u52a8\u6001\u59ff\u52bf\u3001\u591a\u6837\u5730\u70b9\u548c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u89c6\u89c9\u53d9\u4e8b\u6765\u5c55\u793a\u670d\u88c5\u3002\u672c\u6587\u65e8\u5728\u6355\u6349\u8fd9\u79cd\u4e30\u5bcc\u6027\uff0c\u5f25\u5408\u7535\u5546\u548c\u65f6\u5c1a\u5a92\u4f53\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u8bbe\u8ba1\u81ea\u52a8\u5316\u68c0\u7d22\u7ba1\u9053\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u548c\u5bf9\u8c61\u7ea7\u5b9a\u4f4d\uff0c\u8de8\u9886\u57df\u5bf9\u9f50\u670d\u88c5\u3002\u6784\u5efa\u5305\u542b\u4e09\u4e2a\u8d28\u91cf\u7ea7\u522b\u7684\u6570\u636e\u96c6\uff1a\u9ad8\u8d28\u91cf\uff0810,000\u5bf9\uff09\u3001\u4e2d\u7b49\u8d28\u91cf\uff0850,000\u5bf9\uff09\u548c\u4f4e\u8d28\u91cf\uff08300,000\u5bf9\uff09\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u670d\u88c5-\u753b\u518c\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u4e3a\u8d85\u8d8a\u76ee\u5f55\u5f0f\u751f\u6210\u3001\u8fc8\u5411\u53cd\u6620\u521b\u610f\u3001\u6c1b\u56f4\u548c\u6545\u4e8b\u53d9\u8ff0\u7684\u65f6\u5c1a\u56fe\u50cf\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u65f6\u5c1a\u56fe\u50cf\u751f\u6210\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u751f\u6210\u66f4\u5177\u521b\u9020\u529b\u548c\u53d9\u4e8b\u6027\u7684\u7f16\u8f91\u56fe\u50cf\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u7b80\u5355\u7684\u4ea7\u54c1\u5c55\u793a\u3002"}}
{"id": "2510.00635", "pdf": "https://arxiv.org/pdf/2510.00635", "abs": "https://arxiv.org/abs/2510.00635", "authors": ["Nanxiang Jiang", "Zhaoxin Fan", "Enhan Kang", "Daiheng Gao", "Yun Zhou", "Yanxia Chang", "Zheng Zhu", "Yeying Jin", "Wenjun Wu"], "title": "Erased, But Not Forgotten: Erased Rectified Flow Transformers Still Remain Unsafe Under Concept Attack", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in text-to-image (T2I) diffusion models have enabled impressive generative capabilities, but they also raise significant safety concerns due to the potential to produce harmful or undesirable content. While concept erasure has been explored as a mitigation strategy, most existing approaches and corresponding attack evaluations are tailored to Stable Diffusion (SD) and exhibit limited effectiveness when transferred to next-generation rectified flow transformers such as Flux. In this work, we present ReFlux, the first concept attack method specifically designed to assess the robustness of concept erasure in the latest rectified flow-based T2I framework. Our approach is motivated by the observation that existing concept erasure techniques, when applied to Flux, fundamentally rely on a phenomenon known as attention localization. Building on this insight, we propose a simple yet effective attack strategy that specifically targets this property. At its core, a reverse-attention optimization strategy is introduced to effectively reactivate suppressed signals while stabilizing attention. This is further reinforced by a velocity-guided dynamic that enhances the robustness of concept reactivation by steering the flow matching process, and a consistency-preserving objective that maintains the global layout and preserves unrelated content. Extensive experiments consistently demonstrate the effectiveness and efficiency of the proposed attack method, establishing a reliable benchmark for evaluating the robustness of concept erasure strategies in rectified flow transformers.", "AI": {"tldr": "\u63d0\u51fa\u4e86ReFlux\uff0c\u7b2c\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u6700\u65b0\u6574\u6d41\u6d41T2I\u6846\u67b6\u7684\u6982\u5ff5\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cd\u5411\u6ce8\u610f\u529b\u4f18\u5316\u548c\u901f\u5ea6\u5f15\u5bfc\u52a8\u6001\u6765\u8bc4\u4f30\u6982\u5ff5\u64e6\u9664\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u5728\u5e94\u7528\u4e8eFlux\u7b49\u4e0b\u4e00\u4ee3\u6574\u6d41\u6d41\u53d8\u6362\u5668\u65f6\u6548\u679c\u6709\u9650\uff0c\u4e3b\u8981\u4f9d\u8d56\u4e8e\u6ce8\u610f\u529b\u5b9a\u4f4d\u73b0\u8c61\uff0c\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u5176\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u53cd\u5411\u6ce8\u610f\u529b\u4f18\u5316\u7b56\u7565\u91cd\u65b0\u6fc0\u6d3b\u88ab\u6291\u5236\u4fe1\u53f7\u5e76\u7a33\u5b9a\u6ce8\u610f\u529b\uff0c\u7ed3\u5408\u901f\u5ea6\u5f15\u5bfc\u52a8\u6001\u589e\u5f3a\u6982\u5ff5\u91cd\u65b0\u6fc0\u6d3b\u7684\u9c81\u68d2\u6027\uff0c\u4ee5\u53ca\u4e00\u81f4\u6027\u4fdd\u6301\u76ee\u6807\u7ef4\u6301\u5168\u5c40\u5e03\u5c40\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u4e00\u81f4\u8bc1\u660e\u4e86\u6240\u63d0\u653b\u51fb\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6548\u7387\uff0c\u4e3a\u8bc4\u4f30\u6574\u6d41\u6d41\u53d8\u6362\u5668\u4e2d\u6982\u5ff5\u64e6\u9664\u7b56\u7565\u7684\u9c81\u68d2\u6027\u5efa\u7acb\u4e86\u53ef\u9760\u57fa\u51c6\u3002", "conclusion": "ReFlux\u65b9\u6cd5\u6210\u529f\u8bc4\u4f30\u4e86\u6574\u6d41\u6d41T2I\u6846\u67b6\u4e2d\u6982\u5ff5\u64e6\u9664\u7684\u8106\u5f31\u6027\uff0c\u4e3a\u672a\u6765\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2510.00665", "pdf": "https://arxiv.org/pdf/2510.00665", "abs": "https://arxiv.org/abs/2510.00665", "authors": ["Francesco Galati", "Daniele Falcetta", "Rosa Cortese", "Ferran Prados", "Ninon Burgos", "Maria A. Zuluaga"], "title": "Multi-Domain Brain Vessel Segmentation Through Feature Disentanglement", "categories": ["cs.CV", "cs.LG"], "comment": "19 pages, 7 figures, 3 tables. Joint first authors: Francesco Galati   and Daniele Falcetta. Accepted for publication at the Journal of Machine   Learning for Biomedical Imaging (MELBA) https://melba-journal.org/2025:021.   Code available at https://github.com/i-vesseg/MultiVesSeg", "summary": "The intricate morphology of brain vessels poses significant challenges for automatic segmentation models, which usually focus on a single imaging modality. However, accurately treating brain-related conditions requires a comprehensive understanding of the cerebrovascular tree, regardless of the specific acquisition procedure. Our framework effectively segments brain arteries and veins in various datasets through image-to-image translation while avoiding domain-specific model design and data harmonization between the source and the target domain. This is accomplished by employing disentanglement techniques to independently manipulate different image properties, allowing them to move from one domain to another in a label-preserving manner. Specifically, we focus on manipulating vessel appearances during adaptation while preserving spatial information, such as shapes and locations, which are crucial for correct segmentation. Our evaluation effectively bridges large and varied domain gaps across medical centers, image modalities, and vessel types. Additionally, we conduct ablation studies on the optimal number of required annotations and other architectural choices. The results highlight our framework's robustness and versatility, demonstrating the potential of domain adaptation methodologies to perform cerebrovascular image segmentation in multiple scenarios accurately. Our code is available at https://github.com/i-vesseg/MultiVesSeg.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u6a21\u6001\u8111\u8840\u7ba1\u5206\u5272\u7684\u57df\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u50cf\u5230\u56fe\u50cf\u7ffb\u8bd1\u6280\u672f\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u8840\u7ba1\u7a7a\u95f4\u4fe1\u606f\u7684\u540c\u65f6\u9002\u5e94\u4e0d\u540c\u57df\u7684\u5916\u89c2\u5dee\u5f02\uff0c\u65e0\u9700\u7279\u5b9a\u9886\u57df\u6a21\u578b\u8bbe\u8ba1\u6216\u6570\u636e\u534f\u8c03\u3002", "motivation": "\u8111\u8840\u7ba1\u590d\u6742\u5f62\u6001\u5bf9\u81ea\u52a8\u5206\u5272\u6a21\u578b\u6784\u6210\u6311\u6218\uff0c\u800c\u51c6\u786e\u6cbb\u7597\u8111\u90e8\u75be\u75c5\u9700\u8981\u5168\u9762\u7406\u89e3\u8111\u8840\u7ba1\u6811\uff0c\u4e0d\u53d7\u7279\u5b9a\u91c7\u96c6\u7a0b\u5e8f\u9650\u5236\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53ea\u5173\u6ce8\u5355\u4e00\u6210\u50cf\u6a21\u6001\uff0c\u65e0\u6cd5\u9002\u5e94\u591a\u4e2d\u5fc3\u3001\u591a\u6a21\u6001\u7684\u5b9e\u9645\u60c5\u51b5\u3002", "method": "\u91c7\u7528\u89e3\u7f20\u6280\u672f\u72ec\u7acb\u64cd\u7eb5\u4e0d\u540c\u56fe\u50cf\u5c5e\u6027\uff0c\u5728\u57df\u9002\u5e94\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u6807\u7b7e\u4fe1\u606f\u3002\u91cd\u70b9\u662f\u5728\u9002\u5e94\u8fc7\u7a0b\u4e2d\u64cd\u7eb5\u8840\u7ba1\u5916\u89c2\uff0c\u540c\u65f6\u4fdd\u7559\u5bf9\u6b63\u786e\u5206\u5272\u81f3\u5173\u91cd\u8981\u7684\u7a7a\u95f4\u4fe1\u606f\uff08\u5982\u5f62\u72b6\u548c\u4f4d\u7f6e\uff09\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u8be5\u6846\u67b6\u80fd\u6709\u6548\u8de8\u8d8a\u533b\u7597\u4e2d\u5fc3\u3001\u56fe\u50cf\u6a21\u6001\u548c\u8840\u7ba1\u7c7b\u578b\u7684\u5927\u57df\u5dee\u8ddd\u3002\u6d88\u878d\u7814\u7a76\u786e\u5b9a\u4e86\u6240\u9700\u6807\u6ce8\u7684\u6700\u4f73\u6570\u91cf\u548c\u5176\u4ed6\u67b6\u6784\u9009\u62e9\uff0c\u8bc1\u660e\u4e86\u6846\u67b6\u7684\u9c81\u68d2\u6027\u548c\u591a\u529f\u80fd\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u57df\u9002\u5e94\u65b9\u6cd5\u5728\u591a\u573a\u666f\u4e0b\u51c6\u786e\u6267\u884c\u8111\u8840\u7ba1\u56fe\u50cf\u5206\u5272\u7684\u6f5c\u529b\uff0c\u4e3a\u591a\u6a21\u6001\u8111\u8840\u7ba1\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.00666", "pdf": "https://arxiv.org/pdf/2510.00666", "abs": "https://arxiv.org/abs/2510.00666", "authors": ["Leah Bar", "Liron Mor Yosef", "Shai Zucker", "Neta Shoham", "Inbar Seroussi", "Nir Sochen"], "title": "A Geometric Unification of Generative AI with Manifold-Probabilistic Projection Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The foundational premise of generative AI for images is the assumption that images are inherently low-dimensional objects embedded within a high-dimensional space. Additionally, it is often implicitly assumed that thematic image datasets form smooth or piecewise smooth manifolds. Common approaches overlook the geometric structure and focus solely on probabilistic methods, approximating the probability distribution through universal approximation techniques such as the kernel method. In some generative models, the low dimensional nature of the data manifest itself by the introduction of a lower dimensional latent space. Yet, the probability distribution in the latent or the manifold coordinate space is considered uninteresting and is predefined or considered uniform. This study unifies the geometric and probabilistic perspectives by providing a geometric framework and a kernel-based probabilistic method simultaneously. The resulting framework demystifies diffusion models by interpreting them as a projection mechanism onto the manifold of ``good images''. This interpretation leads to the construction of a new deterministic model, the Manifold-Probabilistic Projection Model (MPPM), which operates in both the representation (pixel) space and the latent space. We demonstrate that the Latent MPPM (LMPPM) outperforms the Latent Diffusion Model (LDM) across various datasets, achieving superior results in terms of image restoration and generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u51e0\u4f55\u548c\u6982\u7387\u89c6\u89d2\u7684\u6846\u67b6\uff0c\u5c06\u6269\u6563\u6a21\u578b\u89e3\u91ca\u4e3a\u5411\u201c\u597d\u56fe\u50cf\u201d\u6d41\u5f62\u7684\u6295\u5f71\u673a\u5236\uff0c\u5e76\u6784\u5efa\u4e86\u65b0\u7684\u786e\u5b9a\u6027\u6a21\u578bMPPM\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u4f18\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u5ffd\u89c6\u4e86\u6570\u636e\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u4ec5\u5173\u6ce8\u6982\u7387\u65b9\u6cd5\uff0c\u4e14\u5c06\u6f5c\u5728\u7a7a\u95f4\u7684\u6982\u7387\u5206\u5e03\u89c6\u4e3a\u65e0\u8da3\u6216\u9884\u5b9a\u4e49\u7684\u3002\u672c\u6587\u65e8\u5728\u7edf\u4e00\u51e0\u4f55\u548c\u6982\u7387\u89c6\u89d2\u3002", "method": "\u63d0\u51fa\u4e86\u51e0\u4f55\u6846\u67b6\u548c\u57fa\u4e8e\u6838\u7684\u6982\u7387\u65b9\u6cd5\uff0c\u5c06\u6269\u6563\u6a21\u578b\u89e3\u91ca\u4e3a\u5411\u56fe\u50cf\u6d41\u5f62\u7684\u6295\u5f71\uff0c\u6784\u5efa\u4e86\u786e\u5b9a\u6027\u6a21\u578bMPPM\uff0c\u5728\u8868\u793a\u7a7a\u95f4\u548c\u6f5c\u5728\u7a7a\u95f4\u8fd0\u884c\u3002", "result": "\u6f5c\u5728MPPM\uff08LMPPM\uff09\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\uff0c\u5728\u56fe\u50cf\u6062\u590d\u548c\u751f\u6210\u65b9\u9762\u53d6\u5f97\u66f4\u4f18\u7ed3\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u51e0\u4f55\u89e3\u91ca\uff0cMPPM\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6269\u6563\u6a21\u578b\u3002"}}
{"id": "2510.00728", "pdf": "https://arxiv.org/pdf/2510.00728", "abs": "https://arxiv.org/abs/2510.00728", "authors": ["Hongeun Kim", "Bryan Sangwoo Kim", "Jong Chul Ye"], "title": "Extreme Blind Image Restoration via Prompt-Conditioned Information Bottleneck", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Blind Image Restoration (BIR) methods have achieved remarkable success but falter when faced with Extreme Blind Image Restoration (EBIR), where inputs suffer from severe, compounded degradations beyond their training scope. Directly learning a mapping from extremely low-quality (ELQ) to high-quality (HQ) images is challenging due to the massive domain gap, often leading to unnatural artifacts and loss of detail. To address this, we propose a novel framework that decomposes the intractable ELQ-to-HQ restoration process. We first learn a projector that maps an ELQ image onto an intermediate, less-degraded LQ manifold. This intermediate image is then restored to HQ using a frozen, off-the-shelf BIR model. Our approach is grounded in information theory; we provide a novel perspective of image restoration as an Information Bottleneck problem and derive a theoretically-driven objective to train our projector. This loss function effectively stabilizes training by balancing a low-quality reconstruction term with a high-quality prior-matching term. Our framework enables Look Forward Once (LFO) for inference-time prompt refinement, and supports plug-and-play strengthening of existing image restoration models without need for finetuning. Extensive experiments under severe degradation regimes provide a thorough analysis of the effectiveness of our work.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u6781\u7aef\u76f2\u56fe\u50cf\u6062\u590d(EBIR)\u95ee\u9898\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6781\u5ea6\u4f4e\u8d28\u91cf(ELQ)\u5230\u9ad8\u8d28\u91cf(HQ)\u7684\u6062\u590d\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e24\u4e2a\u6b65\u9aa4\uff1a\u9996\u5148\u5c06ELQ\u56fe\u50cf\u6295\u5f71\u5230\u4e2d\u95f4\u4f4e\u8d28\u91cf(LQ)\u6d41\u5f62\uff0c\u7136\u540e\u4f7f\u7528\u73b0\u6210\u7684BIR\u6a21\u578b\u6062\u590d\u4e3aHQ\u3002", "motivation": "\u73b0\u6709\u7684\u76f2\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u5728\u5904\u7406\u6781\u7aef\u76f2\u56fe\u50cf\u6062\u590d(EBIR)\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5f53\u8f93\u5165\u56fe\u50cf\u906d\u53d7\u4e25\u91cd\u590d\u5408\u9000\u5316\u65f6\uff0c\u76f4\u63a5\u5b66\u4e60\u4eceELQ\u5230HQ\u7684\u6620\u5c04\u7531\u4e8e\u5de8\u5927\u7684\u9886\u57df\u5dee\u8ddd\u800c\u56f0\u96be\uff0c\u5f80\u5f80\u5bfc\u81f4\u4e0d\u81ea\u7136\u7684\u4f2a\u5f71\u548c\u7ec6\u8282\u4e22\u5931\u3002", "method": "\u57fa\u4e8e\u4fe1\u606f\u8bba\u89c6\u89d2\uff0c\u5c06\u56fe\u50cf\u6062\u590d\u89c6\u4e3a\u4fe1\u606f\u74f6\u9888\u95ee\u9898\u3002\u9996\u5148\u5b66\u4e60\u4e00\u4e2a\u6295\u5f71\u5668\u5c06ELQ\u56fe\u50cf\u6620\u5c04\u5230\u4e2d\u95f4LQ\u6d41\u5f62\uff0c\u7136\u540e\u4f7f\u7528\u51bb\u7ed3\u7684\u73b0\u6210BIR\u6a21\u578b\u5c06\u4e2d\u95f4\u56fe\u50cf\u6062\u590d\u4e3aHQ\u3002\u8bbe\u8ba1\u4e86\u7406\u8bba\u9a71\u52a8\u7684\u76ee\u6807\u51fd\u6570\u6765\u8bad\u7ec3\u6295\u5f71\u5668\u3002", "result": "\u5728\u4e25\u91cd\u9000\u5316\u673a\u5236\u4e0b\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u652f\u6301\u63a8\u7406\u65f6\u63d0\u793a\u4f18\u5316\u548c\u5373\u63d2\u5373\u7528\u5730\u589e\u5f3a\u73b0\u6709\u56fe\u50cf\u6062\u590d\u6a21\u578b\uff0c\u65e0\u9700\u5fae\u8c03\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5206\u89e3\u6062\u590d\u8fc7\u7a0b\u6709\u6548\u89e3\u51b3\u4e86EBIR\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6491\u7684\u7a33\u5b9a\u8bad\u7ec3\u65b9\u6cd5\uff0c\u80fd\u591f\u589e\u5f3a\u73b0\u6709\u6a21\u578b\u6027\u80fd\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002"}}
{"id": "2510.00806", "pdf": "https://arxiv.org/pdf/2510.00806", "abs": "https://arxiv.org/abs/2510.00806", "authors": ["Fan Yang", "Zhiyang Chen", "Yousong Zhu", "Xin Li", "Jinqiao Wang"], "title": "From Seeing to Predicting: A Vision-Language Framework for Trajectory Forecasting and Controlled Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Current video generation models produce physically inconsistent motion that violates real-world dynamics. We propose TrajVLM-Gen, a two-stage framework for physics-aware image-to-video generation. First, we employ a Vision Language Model to predict coarse-grained motion trajectories that maintain consistency with real-world physics. Second, these trajectories guide video generation through attention-based mechanisms for fine-grained motion refinement. We build a trajectory prediction dataset based on video tracking data with realistic motion patterns. Experiments on UCF-101 and MSR-VTT demonstrate that TrajVLM-Gen outperforms existing methods, achieving competitive FVD scores of 545 on UCF-101 and 539 on MSR-VTT.", "AI": {"tldr": "TrajVLM-Gen\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u7269\u7406\u611f\u77e5\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u7b26\u5408\u7269\u7406\u89c4\u5f8b\u7684\u8fd0\u52a8\u8f68\u8ff9\u6765\u751f\u6210\u66f4\u771f\u5b9e\u7684\u89c6\u9891", "motivation": "\u5f53\u524d\u89c6\u9891\u751f\u6210\u6a21\u578b\u4ea7\u751f\u7684\u8fd0\u52a8\u5b58\u5728\u7269\u7406\u4e0d\u4e00\u81f4\u6027\uff0c\u8fdd\u53cd\u4e86\u771f\u5b9e\u4e16\u754c\u52a8\u529b\u5b66\u89c4\u5f8b", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u7c97\u7c92\u5ea6\u8fd0\u52a8\u8f68\u8ff9\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u673a\u5236\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8fd0\u52a8\u7cbe\u5316", "result": "\u5728UCF-101\u548cMSR-VTT\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5206\u522b\u8fbe\u5230545\u548c539\u7684FVD\u5206\u6570", "conclusion": "TrajVLM-Gen\u80fd\u591f\u751f\u6210\u7269\u7406\u4e00\u81f4\u7684\u8fd0\u52a8\uff0c\u5728\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5"}}
{"id": "2510.00820", "pdf": "https://arxiv.org/pdf/2510.00820", "abs": "https://arxiv.org/abs/2510.00820", "authors": ["Xiangtao Kong", "Rongyuan Wu", "Shuaizheng Liu", "Lingchen Sun", "Lei Zhang"], "title": "NSARM: Next-Scale Autoregressive Modeling for Robust Real-World Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Most recent real-world image super-resolution (Real-ISR) methods employ pre-trained text-to-image (T2I) diffusion models to synthesize the high-quality image either from random Gaussian noise, which yields realistic results but is slow due to iterative denoising, or directly from the input low-quality image, which is efficient but at the price of lower output quality. These approaches train ControlNet or LoRA modules while keeping the pre-trained model fixed, which often introduces over-enhanced artifacts and hallucinations, suffering from the robustness to inputs of varying degradations. Recent visual autoregressive (AR) models, such as pre-trained Infinity, can provide strong T2I generation capabilities while offering superior efficiency by using the bitwise next-scale prediction strategy. Building upon next-scale prediction, we introduce a robust Real-ISR framework, namely Next-Scale Autoregressive Modeling (NSARM). Specifically, we train NSARM in two stages: a transformation network is first trained to map the input low-quality image to preliminary scales, followed by an end-to-end full-model fine-tuning. Such a comprehensive fine-tuning enhances the robustness of NSARM in Real-ISR tasks without compromising its generative capability. Extensive quantitative and qualitative evaluations demonstrate that as a pure AR model, NSARM achieves superior visual results over existing Real-ISR methods while maintaining a fast inference speed. Most importantly, it demonstrates much higher robustness to the quality of input images, showing stronger generalization performance. Project page: https://github.com/Xiangtaokong/NSARM", "AI": {"tldr": "\u63d0\u51faNSARM\u6846\u67b6\uff0c\u57fa\u4e8e\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u548c\u4e0b\u4e00\u5c3a\u5ea6\u9884\u6d4b\u7b56\u7565\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5b9e\u73b0\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u771f\u5b9e\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u3002", "motivation": "\u73b0\u6709Real-ISR\u65b9\u6cd5\u5b58\u5728\u901f\u5ea6\u4e0e\u8d28\u91cf\u6743\u8861\u95ee\u9898\uff1a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\u8981\u4e48\u901f\u5ea6\u6162\uff0c\u8981\u4e48\u8d28\u91cf\u4f4e\uff0c\u4e14\u5bf9\u8f93\u5165\u56fe\u50cf\u9000\u5316\u7a0b\u5ea6\u654f\u611f\uff0c\u5bb9\u6613\u4ea7\u751f\u8fc7\u5ea6\u589e\u5f3a\u4f2a\u5f71\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u9996\u5148\u8bad\u7ec3\u8f6c\u6362\u7f51\u7edc\u5c06\u4f4e\u8d28\u91cf\u56fe\u50cf\u6620\u5c04\u5230\u521d\u6b65\u5c3a\u5ea6\uff0c\u7136\u540e\u8fdb\u884c\u7aef\u5230\u7aef\u5168\u6a21\u578b\u5fae\u8c03\uff0c\u57fa\u4e8e\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u7684\u4e0b\u4e00\u5c3a\u5ea6\u9884\u6d4b\u7b56\u7565\u3002", "result": "NSARM\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709Real-ISR\u65b9\u6cd5\uff0c\u4fdd\u6301\u5feb\u901f\u63a8\u7406\u901f\u5ea6\uff0c\u5bf9\u8f93\u5165\u56fe\u50cf\u8d28\u91cf\u5177\u6709\u66f4\u9ad8\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "NSARM\u4f5c\u4e3a\u7eaf\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u5728\u771f\u5b9e\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u89c6\u89c9\u7ed3\u679c\u548c\u9c81\u68d2\u6027\uff0c\u5c55\u793a\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.00855", "pdf": "https://arxiv.org/pdf/2510.00855", "abs": "https://arxiv.org/abs/2510.00855", "authors": ["Kevin Zhang", "Kuangzhi Ge", "Xiaowei Chi", "Renrui Zhang", "Shaojun Shi", "Zhen Dong", "Sirui Han", "Shanghang Zhang"], "title": "Can World Models Benefit VLMs for World Dynamics?", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Project page: https://dyva-worldlm.github.io", "summary": "Trained on internet-scale video data, generative world models are increasingly recognized as powerful world simulators that can generate consistent and plausible dynamics over structure, motion, and physics. This raises a natural question: with the advent of strong video foundational models, might they supplant conventional vision encoder paradigms for general-purpose multimodal understanding? While recent studies have begun to explore the potential of world models on common vision tasks, these explorations typically lack a systematic investigation of generic, multimodal tasks. In this work, we strive to investigate the capabilities when world model priors are transferred into Vision-Language Models: we re-purpose a video diffusion model as a generative encoder to perform a single denoising step and treat the resulting latents as a set of visual embedding. We empirically investigate this class of models, which we refer to as World-Language Models (WorldLMs), and we find that generative encoders can capture latents useful for downstream understanding that show distinctions from conventional encoders. Naming our best-performing variant Dynamic Vision Aligner (DyVA), we further discover that this method significantly enhances spatial reasoning abilities and enables single-image models to perform multi-frame reasoning. Through the curation of a suite of visual reasoning tasks, we find DyVA to surpass both open-source and proprietary baselines, achieving state-of-the-art or comparable performance. We attribute these gains to WorldLM's inherited motion-consistency internalization from video pre-training. Finally, we systematically explore extensive model designs to highlight promising directions for future work. We hope our study can pave the way for a new family of VLMs that leverage priors from world models and are on a promising path towards generalist vision learners.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u5c06\u89c6\u9891\u751f\u6210\u4e16\u754c\u6a21\u578b\u4f5c\u4e3a\u89c6\u89c9\u7f16\u7801\u5668\u7528\u4e8e\u591a\u6a21\u6001\u7406\u89e3\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86World-Language Models (WorldLMs)\u6846\u67b6\uff0c\u5176\u4e2d\u6700\u4f73\u53d8\u4f53DyVA\u5728\u7a7a\u95f4\u63a8\u7406\u548c\u591a\u5e27\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0f\u4e16\u754c\u6a21\u578b\u5728\u89c6\u9891\u6570\u636e\u4e0a\u7684\u8bad\u7ec3\u65e5\u76ca\u5f3a\u5927\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7d22\u5b83\u4eec\u662f\u5426\u80fd\u66ff\u4ee3\u4f20\u7edf\u89c6\u89c9\u7f16\u7801\u5668\u8303\u5f0f\uff0c\u7528\u4e8e\u901a\u7528\u591a\u6a21\u6001\u7406\u89e3\u4efb\u52a1\u3002", "method": "\u5c06\u89c6\u9891\u6269\u6563\u6a21\u578b\u91cd\u65b0\u7528\u4f5c\u751f\u6210\u7f16\u7801\u5668\uff0c\u6267\u884c\u5355\u6b65\u53bb\u566a\u5e76\u5c06\u751f\u6210\u7684\u6f5c\u5728\u8868\u793a\u4f5c\u4e3a\u89c6\u89c9\u5d4c\u5165\uff0c\u6784\u5efaWorld-Language Models (WorldLMs)\u3002", "result": "WorldLMs\u80fd\u591f\u6355\u83b7\u5bf9\u4e0b\u6e38\u7406\u89e3\u6709\u7528\u7684\u6f5c\u5728\u7279\u5f81\uff0c\u5728\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u8d85\u8d8a\u5f00\u6e90\u548c\u4e13\u6709\u57fa\u7ebf\u6a21\u578b\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6216\u76f8\u5f53\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u4ece\u89c6\u9891\u9884\u8bad\u7ec3\u4e2d\u7ee7\u627f\u7684\u8fd0\u52a8\u4e00\u81f4\u6027\u5185\u90e8\u5316\u4f7fWorldLMs\u5177\u6709\u4f18\u52bf\uff0c\u8fd9\u9879\u7814\u7a76\u4e3a\u5229\u7528\u4e16\u754c\u6a21\u578b\u5148\u9a8c\u7684\u65b0\u4e00\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2510.00862", "pdf": "https://arxiv.org/pdf/2510.00862", "abs": "https://arxiv.org/abs/2510.00862", "authors": ["Hyun-kyu Ko", "Youbin Kim", "Jihyeon Park", "Dongheok Park", "Gyeongjin Kang", "Wonjun Cho", "Hyung Yi", "Eunbyung Park"], "title": "Gather-Scatter Mamba: Accelerating Propagation with Efficient State Space Model", "categories": ["cs.CV", "cs.AI"], "comment": "Code: \\url{https://github.com/Ko-Lani/GSMamba}", "summary": "State Space Models (SSMs)-most notably RNNs-have historically played a central role in sequential modeling. Although attention mechanisms such as Transformers have since dominated due to their ability to model global context, their quadratic complexity and limited scalability make them less suited for long sequences. Video super-resolution (VSR) methods have traditionally relied on recurrent architectures to propagate features across frames. However, such approaches suffer from well-known issues including vanishing gradients, lack of parallelism, and slow inference speed. Recent advances in selective SSMs like Mamba offer a compelling alternative: by enabling input-dependent state transitions with linear-time complexity, Mamba mitigates these issues while maintaining strong long-range modeling capabilities. Despite this potential, Mamba alone struggles to capture fine-grained spatial dependencies due to its causal nature and lack of explicit context aggregation. To address this, we propose a hybrid architecture that combines shifted window self-attention for spatial context aggregation with Mamba-based selective scanning for efficient temporal propagation. Furthermore, we introduce Gather-Scatter Mamba (GSM), an alignment-aware mechanism that warps features toward a center anchor frame within the temporal window before Mamba propagation and scatters them back afterward, effectively reducing occlusion artifacts and ensuring effective redistribution of aggregated information across all frames. The official implementation is provided at: https://github.com/Ko-Lani/GSMamba.", "AI": {"tldr": "\u63d0\u51fa\u4e86GSMamba\uff0c\u4e00\u79cd\u7ed3\u5408Mamba\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u548c\u79fb\u4f4d\u7a97\u53e3\u81ea\u6ce8\u610f\u529b\u7684\u6df7\u5408\u67b6\u6784\uff0c\u7528\u4e8e\u89c6\u9891\u8d85\u5206\u8fa8\u7387\uff0c\u901a\u8fc7Gather-Scatter Mamba\u673a\u5236\u89e3\u51b3\u906e\u6321\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eRNN\u7684VSR\u65b9\u6cd5\u5b58\u5728\u68af\u5ea6\u6d88\u5931\u3001\u7f3a\u4e4f\u5e76\u884c\u6027\u548c\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u800cTransformer\u5728\u957f\u5e8f\u5217\u4e0a\u5b58\u5728\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\u3002Mamba\u867d\u7136\u63d0\u4f9b\u4e86\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u9009\u62e9\u6027\u72b6\u6001\u8f6c\u79fb\uff0c\u4f46\u96be\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u4f9d\u8d56\u3002", "method": "\u7ed3\u5408\u79fb\u4f4d\u7a97\u53e3\u81ea\u6ce8\u610f\u529b\u8fdb\u884c\u7a7a\u95f4\u4e0a\u4e0b\u6587\u805a\u5408\uff0c\u4f7f\u7528Mamba\u9009\u62e9\u6027\u626b\u63cf\u8fdb\u884c\u9ad8\u6548\u65f6\u95f4\u4f20\u64ad\uff0c\u5e76\u5f15\u5165Gather-Scatter Mamba\u673a\u5236\u5728Mamba\u4f20\u64ad\u524d\u540e\u5bf9\u7279\u5f81\u8fdb\u884c\u626d\u66f2\u548c\u91cd\u5206\u5e03\u3002", "result": "GSMamba\u80fd\u591f\u6709\u6548\u51cf\u5c11\u906e\u6321\u4f2a\u5f71\uff0c\u786e\u4fdd\u805a\u5408\u4fe1\u606f\u5728\u6240\u6709\u5e27\u95f4\u7684\u6709\u6548\u91cd\u5206\u5e03\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u7684\u65f6\u95f4\u4f20\u64ad\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u67b6\u6784\u6210\u529f\u7ed3\u5408\u4e86Mamba\u7684\u9ad8\u6548\u65f6\u95f4\u5efa\u6a21\u80fd\u529b\u548c\u81ea\u6ce8\u610f\u529b\u7684\u7a7a\u95f4\u4e0a\u4e0b\u6587\u805a\u5408\u80fd\u529b\uff0c\u4e3a\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.00929", "pdf": "https://arxiv.org/pdf/2510.00929", "abs": "https://arxiv.org/abs/2510.00929", "authors": ["Victor Sechaud", "J\u00e9r\u00e9my Scanvic", "Quentin Barth\u00e9lemy", "Patrice Abry", "Juli\u00e1n Tachella"], "title": "Equivariant Splitting: Self-supervised learning from incomplete data", "categories": ["cs.CV"], "comment": null, "summary": "Self-supervised learning for inverse problems allows to train a reconstruction network from noise and/or incomplete data alone. These methods have the potential of enabling learning-based solutions when obtaining ground-truth references for training is expensive or even impossible. In this paper, we propose a new self-supervised learning strategy devised for the challenging setting where measurements are observed via a single incomplete observation model. We introduce a new definition of equivariance in the context of reconstruction networks, and show that the combination of self-supervised splitting losses and equivariant reconstruction networks results in unbiased estimates of the supervised loss. Through a series of experiments on image inpainting, accelerated magnetic resonance imaging, and compressive sensing, we demonstrate that the proposed loss achieves state-of-the-art performance in settings with highly rank-deficient forward models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u7b56\u7565\uff0c\u7528\u4e8e\u5728\u6d4b\u91cf\u4ec5\u901a\u8fc7\u5355\u4e00\u4e0d\u5b8c\u6574\u89c2\u6d4b\u6a21\u578b\u83b7\u5f97\u7684\u60c5\u51b5\u4e0b\u8bad\u7ec3\u91cd\u5efa\u7f51\u7edc\uff0c\u901a\u8fc7\u7ed3\u5408\u81ea\u76d1\u7763\u5206\u88c2\u635f\u5931\u548c\u7b49\u53d8\u91cd\u5efa\u7f51\u7edc\u5b9e\u73b0\u65e0\u504f\u76d1\u7763\u635f\u5931\u4f30\u8ba1\u3002", "motivation": "\u89e3\u51b3\u5728\u65e0\u6cd5\u83b7\u53d6\u771f\u5b9e\u53c2\u8003\u6570\u636e\u6216\u83b7\u53d6\u6210\u672c\u9ad8\u6602\u7684\u60c5\u51b5\u4e0b\uff0c\u57fa\u4e8e\u5b66\u4e60\u7684\u9006\u95ee\u9898\u89e3\u51b3\u65b9\u6848\u7684\u9700\u6c42\uff0c\u7279\u522b\u662f\u5728\u6d4b\u91cf\u4ec5\u901a\u8fc7\u5355\u4e00\u4e0d\u5b8c\u6574\u89c2\u6d4b\u6a21\u578b\u83b7\u5f97\u7684\u6311\u6218\u6027\u573a\u666f\u3002", "method": "\u5f15\u5165\u4e86\u91cd\u5efa\u7f51\u7edc\u7b49\u53d8\u6027\u7684\u65b0\u5b9a\u4e49\uff0c\u5c06\u81ea\u76d1\u7763\u5206\u88c2\u635f\u5931\u4e0e\u7b49\u53d8\u91cd\u5efa\u7f51\u7edc\u76f8\u7ed3\u5408\uff0c\u4ece\u800c\u83b7\u5f97\u76d1\u7763\u635f\u5931\u7684\u65e0\u504f\u4f30\u8ba1\u3002", "result": "\u5728\u56fe\u50cf\u4fee\u590d\u3001\u52a0\u901f\u78c1\u5171\u632f\u6210\u50cf\u548c\u538b\u7f29\u611f\u77e5\u7b49\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u9ad8\u5ea6\u79e9\u4e0d\u8db3\u524d\u5411\u6a21\u578b\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u7b56\u7565\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5355\u89c2\u6d4b\u6a21\u578b\u8bbe\u7f6e\u4e0b\u6709\u6548\uff0c\u4e3a\u65e0\u6cd5\u83b7\u53d6\u771f\u5b9e\u53c2\u8003\u6570\u636e\u7684\u9006\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.00936", "pdf": "https://arxiv.org/pdf/2510.00936", "abs": "https://arxiv.org/abs/2510.00936", "authors": ["Zanwu Liu", "Chao Yuan", "Bo Li", "Xiaowei Zhang", "Guanglin Niu"], "title": "Looking Alike From Far to Near: Enhancing Cross-Resolution Re-Identification via Feature Vector Panning", "categories": ["cs.CV"], "comment": null, "summary": "In surveillance scenarios, varying camera distances cause significant differences among pedestrian image resolutions, making it hard to match low-resolution (LR) images with high-resolution (HR) counterparts, limiting the performance of Re-Identification (ReID) tasks. Most existing Cross-Resolution ReID (CR-ReID) methods rely on super-resolution (SR) or joint learning for feature compensation, which increases training and inference complexity and has reached a performance bottleneck in recent studies. Inspired by semantic directions in the word embedding space, we empirically discover that semantic directions implying resolution differences also emerge in the feature space of ReID, and we substantiate this finding from a statistical perspective using Canonical Correlation Analysis and Pearson Correlation Analysis. Based on this interesting finding, we propose a lightweight and effective Vector Panning Feature Alignment (VPFA) framework, which conducts CR-ReID from a novel perspective of modeling the resolution-specific feature discrepancy. Extensive experimental results on multiple CR-ReID benchmarks show that our method significantly outperforms previous state-of-the-art baseline models while obtaining higher efficiency, demonstrating the effectiveness and superiority of our model based on the new finding in this paper.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u5411\u91cf\u5e73\u79fb\u7279\u5f81\u5bf9\u9f50\u6846\u67b6\uff0c\u4ece\u5efa\u6a21\u5206\u8fa8\u7387\u7279\u5b9a\u7279\u5f81\u5dee\u5f02\u7684\u65b0\u89c6\u89d2\u89e3\u51b3\u8de8\u5206\u8fa8\u7387\u884c\u4eba\u91cd\u8bc6\u522b\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u6548\u7387\u66f4\u9ad8\u3002", "motivation": "\u76d1\u63a7\u573a\u666f\u4e2d\u76f8\u673a\u8ddd\u79bb\u53d8\u5316\u5bfc\u81f4\u884c\u4eba\u56fe\u50cf\u5206\u8fa8\u7387\u5dee\u5f02\u663e\u8457\uff0c\u4f7f\u5f97\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u96be\u4ee5\u4e0e\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5339\u914d\uff0c\u9650\u5236\u4e86\u884c\u4eba\u91cd\u8bc6\u522b\u6027\u80fd\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8d85\u5206\u8fa8\u7387\u6216\u8054\u5408\u5b66\u4e60\uff0c\u589e\u52a0\u4e86\u590d\u6742\u5ea6\u4e14\u6027\u80fd\u5df2\u8fbe\u74f6\u9888\u3002", "method": "\u57fa\u4e8e\u8bed\u4e49\u65b9\u5411\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u53d1\u73b0\uff0c\u63d0\u51fa\u5411\u91cf\u5e73\u79fb\u7279\u5f81\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u5206\u8fa8\u7387\u7279\u5b9a\u7279\u5f81\u5dee\u5f02\u6765\u8fdb\u884c\u8de8\u5206\u8fa8\u7387\u884c\u4eba\u91cd\u8bc6\u522b\u3002", "result": "\u5728\u591a\u4e2a\u8de8\u5206\u8fa8\u7387\u884c\u4eba\u91cd\u8bc6\u522b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u6a21\u578b\uff0c\u540c\u65f6\u83b7\u5f97\u66f4\u9ad8\u7684\u6548\u7387\u3002", "conclusion": "\u57fa\u4e8e\u5206\u8fa8\u7387\u7279\u5b9a\u7279\u5f81\u5dee\u5f02\u5efa\u6a21\u7684\u65b0\u89c6\u89d2\u662f\u89e3\u51b3\u8de8\u5206\u8fa8\u7387\u884c\u4eba\u91cd\u8bc6\u522b\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\u5728\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002"}}
{"id": "2510.00948", "pdf": "https://arxiv.org/pdf/2510.00948", "abs": "https://arxiv.org/abs/2510.00948", "authors": ["Ziqing Zhang", "Kai Liu", "Zheng Chen", "Xi Li", "Yucong Chen", "Bingnan Duan", "Linghe Kong", "Yulun Zhang"], "title": "InfVSR: Breaking Length Limits of Generic Video Super-Resolution", "categories": ["cs.CV"], "comment": "Code will be available at https://github.com/Kai-Liu001/InfVSR", "summary": "Real-world videos often extend over thousands of frames. Existing video super-resolution (VSR) approaches, however, face two persistent challenges when processing long sequences: (1) inefficiency due to the heavy cost of multi-step denoising for full-length sequences; and (2) poor scalability hindered by temporal decomposition that causes artifacts and discontinuities. To break these limits, we propose InfVSR, which novelly reformulates VSR as an autoregressive-one-step-diffusion paradigm. This enables streaming inference while fully leveraging pre-trained video diffusion priors. First, we adapt the pre-trained DiT into a causal structure, maintaining both local and global coherence via rolling KV-cache and joint visual guidance. Second, we distill the diffusion process into a single step efficiently, with patch-wise pixel supervision and cross-chunk distribution matching. Together, these designs enable efficient and scalable VSR for unbounded-length videos. To fill the gap in long-form video evaluation, we build a new benchmark tailored for extended sequences and further introduce semantic-level metrics to comprehensively assess temporal consistency. Our method pushes the frontier of long-form VSR, achieves state-of-the-art quality with enhanced semantic consistency, and delivers up to 58x speed-up over existing methods such as MGLD-VSR. Code will be available at https://github.com/Kai-Liu001/InfVSR.", "AI": {"tldr": "\u63d0\u51fa\u4e86InfVSR\u65b9\u6cd5\uff0c\u5c06\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u91cd\u65b0\u5b9a\u4e49\u4e3a\u81ea\u56de\u5f52\u5355\u6b65\u6269\u6563\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u5904\u7406\u4e2d\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5728\u5904\u7406\u957f\u5e8f\u5217\u65f6\u9762\u4e34\u6548\u7387\u4f4e\u4e0b\u548c\u53ef\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\uff0c\u9700\u8981\u7a81\u7834\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u5c06\u9884\u8bad\u7ec3\u7684DiT\u8c03\u6574\u4e3a\u56e0\u679c\u7ed3\u6784\uff0c\u901a\u8fc7\u6eda\u52a8KV\u7f13\u5b58\u548c\u8054\u5408\u89c6\u89c9\u6307\u5bfc\u4fdd\u6301\u5c40\u90e8\u548c\u5168\u5c40\u4e00\u81f4\u6027\uff1b\u5c06\u6269\u6563\u8fc7\u7a0b\u84b8\u998f\u4e3a\u5355\u6b65\uff0c\u4f7f\u7528\u8865\u4e01\u7ea7\u50cf\u7d20\u76d1\u7763\u548c\u8de8\u5757\u5206\u5e03\u5339\u914d\u3002", "result": "\u5b9e\u73b0\u4e86\u65e0\u754c\u957f\u5ea6\u89c6\u9891\u7684\u9ad8\u6548\u53ef\u6269\u5c55VSR\uff0c\u5728\u8d28\u91cf\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u8bed\u4e49\u4e00\u81f4\u6027\u589e\u5f3a\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u5982MGLD-VSR\u5feb58\u500d\u3002", "conclusion": "InfVSR\u63a8\u52a8\u4e86\u957f\u683c\u5f0f\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u7684\u524d\u6cbf\u53d1\u5c55\uff0c\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.00974", "pdf": "https://arxiv.org/pdf/2510.00974", "abs": "https://arxiv.org/abs/2510.00974", "authors": ["Siheng Wan", "Zhengtao Yao", "Zhengdao Li", "Junhao Dong", "Yanshu Li", "Yikai Li", "Linshan Li", "Haoyan Xu", "Yijiang Li", "Zhikang Dong", "Huacan Wang", "Jifeng Shen"], "title": "JEPA-T: Joint-Embedding Predictive Architecture with Text Fusion for Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Modern Text-to-Image (T2I) generation increasingly relies on token-centric architectures that are trained with self-supervision, yet effectively fusing text with visual tokens remains a challenge. We propose \\textbf{JEPA-T}, a unified multimodal framework that encodes images and captions into discrete visual and textual tokens, processed by a joint-embedding predictive Transformer. To enhance fusion, we incorporate cross-attention after the feature predictor for conditional denoising while maintaining a task-agnostic backbone. Additionally, raw texts embeddings are injected prior to the flow matching loss to improve alignment during training. During inference, the same network performs both class-conditional and free-text image generation by iteratively denoising visual tokens conditioned on text. Evaluations on ImageNet-1K demonstrate that JEPA-T achieves strong data efficiency, open-vocabulary generalization, and consistently outperforms non-fusion and late-fusion baselines. Our approach shows that late architectural fusion combined with objective-level alignment offers an effective balance between conditioning strength and backbone generality in token-based T2I.The code is now available: https://github.com/justin-herry/JEPA-T.git", "AI": {"tldr": "JEPA-T\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5d4c\u5165\u9884\u6d4bTransformer\u5904\u7406\u56fe\u50cf\u548c\u6587\u672c\u7684\u79bb\u6563\u6807\u8bb0\uff0c\u7ed3\u5408\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u539f\u59cb\u6587\u672c\u5d4c\u5165\u6ce8\u5165\uff0c\u5728ImageNet-1K\u4e0a\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6570\u636e\u6548\u7387\u548c\u5f00\u653e\u8bcd\u6c47\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u4ee3\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u57fa\u4e8e\u6807\u8bb0\u7684\u67b6\u6784\u5728\u878d\u5408\u6587\u672c\u4e0e\u89c6\u89c9\u6807\u8bb0\u65b9\u9762\u7684\u6311\u6218\uff0c\u5bfb\u6c42\u5728\u6761\u4ef6\u5f3a\u5ea6\u4e0e\u9aa8\u5e72\u7f51\u7edc\u901a\u7528\u6027\u4e4b\u95f4\u627e\u5230\u6709\u6548\u5e73\u8861\u3002", "method": "\u5c06\u56fe\u50cf\u548c\u6807\u9898\u7f16\u7801\u4e3a\u79bb\u6563\u7684\u89c6\u89c9\u548c\u6587\u672c\u6807\u8bb0\uff0c\u4f7f\u7528\u8054\u5408\u5d4c\u5165\u9884\u6d4bTransformer\u5904\u7406\uff0c\u5728\u7279\u5f81\u9884\u6d4b\u5668\u540e\u52a0\u5165\u4ea4\u53c9\u6ce8\u610f\u529b\u8fdb\u884c\u6761\u4ef6\u53bb\u566a\uff0c\u5e76\u5728\u8bad\u7ec3\u65f6\u6ce8\u5165\u539f\u59cb\u6587\u672c\u5d4c\u5165\u4ee5\u6539\u5584\u5bf9\u9f50\u3002", "result": "\u5728ImageNet-1K\u8bc4\u4f30\u4e2d\uff0cJEPA-T\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6570\u636e\u6548\u7387\u548c\u5f00\u653e\u8bcd\u6c47\u6cdb\u5316\u80fd\u529b\uff0c\u59cb\u7ec8\u4f18\u4e8e\u975e\u878d\u5408\u548c\u540e\u671f\u878d\u5408\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u665a\u671f\u67b6\u6784\u878d\u5408\u4e0e\u76ee\u6807\u7ea7\u5bf9\u9f50\u76f8\u7ed3\u5408\uff0c\u4e3a\u57fa\u4e8e\u6807\u8bb0\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u6761\u4ef6\u5f3a\u5ea6\u4e0e\u9aa8\u5e72\u901a\u7528\u6027\u4e4b\u95f4\u7684\u6709\u6548\u5e73\u8861\u3002"}}
{"id": "2510.00996", "pdf": "https://arxiv.org/pdf/2510.00996", "abs": "https://arxiv.org/abs/2510.00996", "authors": ["Dongli Xu", "Aleksei Tiulpin", "Matthew B. Blaschko"], "title": "SoftCFG: Uncertainty-guided Stable Guidance for Visual autoregressive Model", "categories": ["cs.CV"], "comment": "preprint", "summary": "Autoregressive (AR) models have emerged as powerful tools for image generation by modeling images as sequences of discrete tokens. While Classifier-Free Guidance (CFG) has been adopted to improve conditional generation, its application in AR models faces two key issues: guidance diminishing, where the conditional-unconditional gap quickly vanishes as decoding progresses, and over-guidance, where strong conditions distort visual coherence. To address these challenges, we propose SoftCFG, an uncertainty-guided inference method that distributes adaptive perturbations across all tokens in the sequence. The key idea behind SoftCFG is to let each generated token contribute certainty-weighted guidance, ensuring that the signal persists across steps while resolving conflicts between text guidance and visual context. To further stabilize long-sequence generation, we introduce Step Normalization, which bounds cumulative perturbations of SoftCFG. Our method is training-free, model-agnostic, and seamlessly integrates with existing AR pipelines. Experiments show that SoftCFG significantly improves image quality over standard CFG and achieves state-of-the-art FID on ImageNet 256 among autoregressive models.", "AI": {"tldr": "\u63d0\u51fa\u4e86SoftCFG\u65b9\u6cd5\uff0c\u89e3\u51b3\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u5b58\u5728\u7684\u5f15\u5bfc\u8870\u51cf\u548c\u8fc7\u5ea6\u5f15\u5bfc\u95ee\u9898\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u5f15\u5bfc\u548c\u6b65\u957f\u5f52\u4e00\u5316\u6765\u63d0\u5347\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u81ea\u56de\u5f52\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u5e94\u7528\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u65f6\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u5f15\u5bfc\u8870\u51cf\uff08\u6761\u4ef6-\u65e0\u6761\u4ef6\u5dee\u8ddd\u968f\u89e3\u7801\u8fdb\u5c55\u5feb\u901f\u6d88\u5931\uff09\u548c\u8fc7\u5ea6\u5f15\u5bfc\uff08\u5f3a\u6761\u4ef6\u7834\u574f\u89c6\u89c9\u8fde\u8d2f\u6027\uff09\u3002", "method": "\u63d0\u51fa\u4e86SoftCFG\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u63a8\u7406\u673a\u5236\uff0c\u5728\u6240\u6709\u5e8f\u5217\u6807\u8bb0\u4e0a\u5206\u5e03\u81ea\u9002\u5e94\u6270\u52a8\uff0c\u8ba9\u6bcf\u4e2a\u751f\u6210\u7684\u6807\u8bb0\u8d21\u732e\u786e\u5b9a\u6027\u52a0\u6743\u7684\u5f15\u5bfc\uff1b\u540c\u65f6\u5f15\u5165\u6b65\u957f\u5f52\u4e00\u5316\u6765\u9650\u5236SoftCFG\u7684\u7d2f\u79ef\u6270\u52a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSoftCFG\u663e\u8457\u4f18\u4e8e\u6807\u51c6CFG\uff0c\u5728ImageNet 256\u4e0a\u5b9e\u73b0\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u6700\u5148\u8fdb\u7684FID\u5206\u6570\u3002", "conclusion": "SoftCFG\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u80fd\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u81ea\u56de\u5f52\u6d41\u7a0b\u4e2d\uff0c\u6709\u6548\u89e3\u51b3\u4e86CFG\u5728AR\u6a21\u578b\u4e2d\u7684\u5f15\u5bfc\u95ee\u9898\u3002"}}
{"id": "2510.01010", "pdf": "https://arxiv.org/pdf/2510.01010", "abs": "https://arxiv.org/abs/2510.01010", "authors": ["Yuxiang Guo", "Jiang Liu", "Ze Wang", "Hao Chen", "Ximeng Sun", "Yang Zhao", "Jialian Wu", "Xiaodong Yu", "Zicheng Liu", "Emad Barsoum"], "title": "ImageDoctor: Diagnosing Text-to-Image Generation via Grounded Image Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement of text-to-image (T2I) models has increased the need for reliable human preference modeling, a demand further amplified by recent progress in reinforcement learning for preference alignment. However, existing approaches typically quantify the quality of a generated image using a single scalar, limiting their ability to provide comprehensive and interpretable feedback on image quality. To address this, we introduce ImageDoctor, a unified multi-aspect T2I model evaluation framework that assesses image quality across four complementary dimensions: plausibility, semantic alignment, aesthetics, and overall quality. ImageDoctor also provides pixel-level flaw indicators in the form of heatmaps, which highlight misaligned or implausible regions, and can be used as a dense reward for T2I model preference alignment. Inspired by the diagnostic process, we improve the detail sensitivity and reasoning capability of ImageDoctor by introducing a \"look-think-predict\" paradigm, where the model first localizes potential flaws, then generates reasoning, and finally concludes the evaluation with quantitative scores. Built on top of a vision-language model and trained through a combination of supervised fine-tuning and reinforcement learning, ImageDoctor demonstrates strong alignment with human preference across multiple datasets, establishing its effectiveness as an evaluation metric. Furthermore, when used as a reward model for preference tuning, ImageDoctor significantly improves generation quality -- achieving an improvement of 10% over scalar-based reward models.", "AI": {"tldr": "ImageDoctor\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u65b9\u9762\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u56fe\u50cf\u8d28\u91cf\u5e76\u63d0\u4f9b\u50cf\u7d20\u7ea7\u7f3a\u9677\u70ed\u56fe\uff0c\u53ef\u4f5c\u4e3a\u5bc6\u96c6\u5956\u52b1\u7528\u4e8e\u6a21\u578b\u504f\u597d\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u5355\u4e00\u6807\u91cf\u91cf\u5316\u751f\u6210\u56fe\u50cf\u8d28\u91cf\uff0c\u65e0\u6cd5\u63d0\u4f9b\u5168\u9762\u4e14\u53ef\u89e3\u91ca\u7684\u53cd\u9988\uff0c\u9650\u5236\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u8bc4\u4f30\u7684\u5168\u9762\u6027\u3002", "method": "\u5f15\u5165\"\u89c2\u5bdf-\u601d\u8003-\u9884\u6d4b\"\u8303\u5f0f\uff1a\u5148\u5b9a\u4f4d\u6f5c\u5728\u7f3a\u9677\uff0c\u751f\u6210\u63a8\u7406\uff0c\u6700\u540e\u7ed9\u51fa\u5b9a\u91cf\u8bc4\u5206\u3002\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4e0e\u4eba\u7c7b\u504f\u597d\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\u7528\u4e8e\u504f\u597d\u8c03\u4f18\u65f6\uff0c\u6bd4\u57fa\u4e8e\u6807\u91cf\u7684\u5956\u52b1\u6a21\u578b\u63d0\u534710%\u7684\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "ImageDoctor\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\u6709\u6548\uff0c\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\u80fd\u663e\u8457\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u63d0\u4f9b\u4e86\u5168\u9762\u4e14\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2510.01031", "pdf": "https://arxiv.org/pdf/2510.01031", "abs": "https://arxiv.org/abs/2510.01031", "authors": ["Pol Labarbarie", "Vincent Itier", "William Puech"], "title": "Secure and reversible face anonymization with diffusion models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Face images processed by computer vision algorithms contain sensitive personal information that malicious actors can capture without consent. These privacy and security risks highlight the need for effective face anonymization methods. Current methods struggle to propose a good trade-off between a secure scheme with high-quality image generation and reversibility for later person authentication. Diffusion-based approaches produce high-quality anonymized images but lack the secret key mechanism to ensure that only authorized parties can reverse the process. In this paper, we introduce, to our knowledge, the first secure, high-quality reversible anonymization method based on a diffusion model. We propose to combine the secret key with the latent faces representation of the diffusion model. To preserve identity-irrelevant features, generation is constrained by a facial mask, maintaining high-quality images. By using a deterministic forward and backward diffusion process, our approach enforces that the original face can be recovered with the correct secret key. We also show that the proposed method produces anonymized faces that are less visually similar to the original faces, compared to other previous work.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5b89\u5168\u3001\u9ad8\u8d28\u91cf\u53ef\u9006\u4eba\u8138\u533f\u540d\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5bc6\u94a5\u4e0e\u6f5c\u5728\u4eba\u8138\u8868\u793a\uff0c\u5728\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u53ef\u9006\u6062\u590d", "motivation": "\u5f53\u524d\u4eba\u8138\u533f\u540d\u5316\u65b9\u6cd5\u96be\u4ee5\u5728\u5b89\u5168\u65b9\u6848\u3001\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u548c\u53ef\u9006\u6027\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\uff0c\u6269\u6563\u6a21\u578b\u7f3a\u4e4f\u5bc6\u94a5\u673a\u5236\u786e\u4fdd\u53ea\u6709\u6388\u6743\u65b9\u624d\u80fd\u9006\u8f6c\u8fc7\u7a0b", "method": "\u5c06\u5bc6\u94a5\u4e0e\u6269\u6563\u6a21\u578b\u7684\u6f5c\u5728\u4eba\u8138\u8868\u793a\u76f8\u7ed3\u5408\uff0c\u4f7f\u7528\u9762\u90e8\u63a9\u7801\u7ea6\u675f\u751f\u6210\u4ee5\u4fdd\u7559\u8eab\u4efd\u65e0\u5173\u7279\u5f81\uff0c\u901a\u8fc7\u786e\u5b9a\u6027\u524d\u5411\u548c\u540e\u5411\u6269\u6563\u8fc7\u7a0b\u786e\u4fdd\u539f\u59cb\u4eba\u8138\u53ef\u6062\u590d", "result": "\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u533f\u540d\u5316\u4eba\u8138\u4e0e\u539f\u59cb\u4eba\u8138\u7684\u89c6\u89c9\u76f8\u4f3c\u5ea6\u4f4e\u4e8e\u5176\u4ed6\u5148\u524d\u5de5\u4f5c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u56fe\u50cf", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u548c\u56fe\u50cf\u8d28\u91cf\u4e4b\u95f4\u5b9e\u73b0\u4e86\u826f\u597d\u5e73\u8861\uff0c\u4e3a\u53ef\u9006\u4eba\u8138\u533f\u540d\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.01047", "pdf": "https://arxiv.org/pdf/2510.01047", "abs": "https://arxiv.org/abs/2510.01047", "authors": ["Xiao Li", "Jiaqi Zhang", "Shuxiang Zhang", "Tianshui Chen", "Liang Lin", "Guangrun Wang"], "title": "Authentic Discrete Diffusion Model", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "We propose an Authentic Discrete Diffusion (ADD) framework that fundamentally redefines prior pseudo-discrete approaches by preserving core diffusion characteristics directly in the one-hot space through a suite of coordinated mechanisms. Unlike conventional \"pseudo\" discrete diffusion (PDD) methods, ADD reformulates the diffusion input by directly using float-encoded one-hot class data, without relying on diffusing in the continuous latent spaces or masking policies. At its core, a timestep-conditioned cross-entropy loss is introduced between the diffusion model's outputs and the original one-hot labels. This synergistic design establishes a bridge between discriminative and generative learning. Our experiments demonstrate that ADD not only achieves superior performance on classification tasks compared to the baseline, but also exhibits excellent text generation capabilities on Image captioning. Extensive ablations validate the measurable gains of each component.", "AI": {"tldr": "\u63d0\u51faAuthentic Discrete Diffusion (ADD)\u6846\u67b6\uff0c\u5728one-hot\u7a7a\u95f4\u4e2d\u4fdd\u6301\u6838\u5fc3\u6269\u6563\u7279\u6027\uff0c\u901a\u8fc7\u534f\u8c03\u673a\u5236\u91cd\u65b0\u5b9a\u4e49\u4f2a\u79bb\u6563\u6269\u6563\u65b9\u6cd5", "motivation": "\u4f20\u7edf\u4f2a\u79bb\u6563\u6269\u6563\u65b9\u6cd5\u4f9d\u8d56\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u6269\u6563\u6216\u63a9\u7801\u7b56\u7565\uff0cADD\u65e8\u5728\u76f4\u63a5\u5728one-hot\u7a7a\u95f4\u5b9e\u73b0\u771f\u6b63\u7684\u79bb\u6563\u6269\u6563", "method": "\u4f7f\u7528\u6d6e\u70b9\u7f16\u7801\u7684one-hot\u7c7b\u522b\u6570\u636e\u4f5c\u4e3a\u6269\u6563\u8f93\u5165\uff0c\u5f15\u5165\u65f6\u95f4\u6b65\u6761\u4ef6\u4ea4\u53c9\u71b5\u635f\u5931\uff0c\u5728\u6269\u6563\u6a21\u578b\u8f93\u51fa\u548c\u539f\u59cbone-hot\u6807\u7b7e\u4e4b\u95f4\u5efa\u7acb\u6865\u6881", "result": "\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u4e2d\u5c55\u73b0\u51fa\u4f18\u79c0\u7684\u6587\u672c\u751f\u6210\u80fd\u529b", "conclusion": "ADD\u901a\u8fc7\u534f\u8c03\u673a\u5236\u5728one-hot\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u4e86\u771f\u6b63\u7684\u79bb\u6563\u6269\u6563\uff0c\u8fde\u63a5\u4e86\u5224\u522b\u6027\u548c\u751f\u6210\u6027\u5b66\u4e60"}}
{"id": "2510.01119", "pdf": "https://arxiv.org/pdf/2510.01119", "abs": "https://arxiv.org/abs/2510.01119", "authors": ["Zhanpeng Luo", "Haoxi Ran", "Li Lu"], "title": "Instant4D: 4D Gaussian Splatting in Minutes", "categories": ["cs.CV"], "comment": "Accepted by NeurIPS 25", "summary": "Dynamic view synthesis has seen significant advances, yet reconstructing scenes from uncalibrated, casual video remains challenging due to slow optimization and complex parameter estimation. In this work, we present Instant4D, a monocular reconstruction system that leverages native 4D representation to efficiently process casual video sequences within minutes, without calibrated cameras or depth sensors. Our method begins with geometric recovery through deep visual SLAM, followed by grid pruning to optimize scene representation. Our design significantly reduces redundancy while maintaining geometric integrity, cutting model size to under 10% of its original footprint. To handle temporal dynamics efficiently, we introduce a streamlined 4D Gaussian representation, achieving a 30x speed-up and reducing training time to within two minutes, while maintaining competitive performance across several benchmarks. Our method reconstruct a single video within 10 minutes on the Dycheck dataset or for a typical 200-frame video. We further apply our model to in-the-wild videos, showcasing its generalizability. Our project website is published at https://instant4d.github.io/.", "AI": {"tldr": "Instant4D\u662f\u4e00\u4e2a\u5355\u76ee\u91cd\u5efa\u7cfb\u7edf\uff0c\u4f7f\u7528\u539f\u751f4D\u8868\u793a\u4ece\u975e\u6807\u5b9a\u89c6\u9891\u4e2d\u5feb\u901f\u91cd\u5efa\u52a8\u6001\u573a\u666f\uff0c\u5728\u51e0\u5206\u949f\u5185\u5b8c\u6210\u5904\u7406\uff0c\u65e0\u9700\u6807\u5b9a\u76f8\u673a\u6216\u6df1\u5ea6\u4f20\u611f\u5668\u3002", "motivation": "\u52a8\u6001\u89c6\u56fe\u5408\u6210\u867d\u6709\u8fdb\u5c55\uff0c\u4f46\u4ece\u975e\u6807\u5b9a\u3001\u968f\u610f\u62cd\u6444\u7684\u89c6\u9891\u91cd\u5efa\u573a\u666f\u4ecd\u5177\u6311\u6218\uff0c\u4e3b\u8981\u95ee\u9898\u5305\u62ec\u4f18\u5316\u901f\u5ea6\u6162\u548c\u53c2\u6570\u4f30\u8ba1\u590d\u6742\u3002", "method": "\u9996\u5148\u901a\u8fc7\u6df1\u5ea6\u89c6\u89c9SLAM\u8fdb\u884c\u51e0\u4f55\u6062\u590d\uff0c\u7136\u540e\u901a\u8fc7\u7f51\u683c\u526a\u679d\u4f18\u5316\u573a\u666f\u8868\u793a\uff0c\u5f15\u5165\u7b80\u5316\u76844D\u9ad8\u65af\u8868\u793a\u9ad8\u6548\u5904\u7406\u65f6\u95f4\u52a8\u6001\u3002", "result": "\u663e\u8457\u51cf\u5c11\u5197\u4f59\u540c\u65f6\u4fdd\u6301\u51e0\u4f55\u5b8c\u6574\u6027\uff0c\u6a21\u578b\u5927\u5c0f\u964d\u81f3\u539f\u59cb10%\u4ee5\u4e0b\uff0c\u5b9e\u73b030\u500d\u52a0\u901f\uff0c\u8bad\u7ec3\u65f6\u95f4\u63a7\u5236\u57282\u5206\u949f\u5185\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728Dycheck\u6570\u636e\u96c6\u4e0a10\u5206\u949f\u5185\u91cd\u5efa\u5355\u4e2a\u89c6\u9891\uff0c\u9002\u7528\u4e8e200\u5e27\u5178\u578b\u89c6\u9891\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u91ce\u5916\u89c6\u9891\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.01126", "pdf": "https://arxiv.org/pdf/2510.01126", "abs": "https://arxiv.org/abs/2510.01126", "authors": ["Yuxiang Feng", "Keyang Zhang", "Hassane Ouchouid", "Ashwil Kaniamparambil", "Ioannis Souflas", "Panagiotis Angeloudis"], "title": "Strategic Fusion of Vision Language Models: Shapley-Credited Context-Aware Dawid-Skene for Multi-Label Tasks in Autonomous Driving", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages", "summary": "Large vision-language models (VLMs) are increasingly used in autonomous-vehicle (AV) stacks, but hallucination limits their reliability in safety-critical pipelines. We present Shapley-credited Context-Aware Dawid-Skene with Agreement, a game-theoretic fusion method for multi-label understanding of ego-view dashcam video. It learns per-model, per-label, context-conditioned reliabilities from labelled history and, at inference, converts each model's report into an agreement-guardrailed log-likelihood ratio that is combined with a contextual prior and a public reputation state updated via Shapley-based team credit. The result is calibrated, thresholdable posteriors that (i) amplify agreement among reliable models, (ii) preserve uniquely correct single-model signals, and (iii) adapt to drift. To specialise general VLMs, we curate 1,000 real-world dashcam clips with structured annotations (scene description, manoeuvre recommendation, rationale) via an automatic pipeline that fuses HDD ground truth, vehicle kinematics, and YOLOv11 + BoT-SORT tracking, guided by a three-step chain-of-thought prompt; three heterogeneous VLMs are then fine-tuned with LoRA. We evaluate with Hamming distance, Micro-Macro-F1, and average per-video latency. Empirically, the proposed method achieves a 23% reduction in Hamming distance, 55% improvement in Macro-F1, and 47% improvement in Micro-F1 when comparing with the best single model, supporting VLM fusion as a calibrated, interpretable, and robust decision-support component for AV pipelines.", "AI": {"tldr": "\u63d0\u51faShapley-credited Context-Aware Dawid-Skene with Agreement\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e2d\u591a\u6807\u7b7e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u878d\u5408\uff0c\u663e\u8457\u964d\u4f4e\u5e7b\u89c9\u5e76\u63d0\u9ad8\u53ef\u9760\u6027\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5e7b\u89c9\u95ee\u9898\u9650\u5236\u4e86\u5176\u5728\u5b89\u5168\u5173\u952e\u7ba1\u9053\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6821\u51c6\u548c\u89e3\u91ca\u7684\u878d\u5408\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u535a\u5f08\u8bba\u878d\u5408\u65b9\u6cd5\uff0c\u5b66\u4e60\u6bcf\u4e2a\u6a21\u578b\u3001\u6bcf\u4e2a\u6807\u7b7e\u3001\u4e0a\u4e0b\u6587\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u6027\uff0c\u5728\u63a8\u7406\u65f6\u5c06\u6a21\u578b\u62a5\u544a\u8f6c\u6362\u4e3a\u534f\u8bae\u4fdd\u62a4\u7684\u5bf9\u6570\u4f3c\u7136\u6bd4\uff0c\u7ed3\u5408\u4e0a\u4e0b\u6587\u5148\u9a8c\u548c\u57fa\u4e8eShapley\u7684\u56e2\u961f\u4fe1\u8a89\u72b6\u6001\u66f4\u65b0\u3002", "result": "\u4e0e\u6700\u4f73\u5355\u6a21\u578b\u76f8\u6bd4\uff0cHamming\u8ddd\u79bb\u51cf\u5c1123%\uff0cMacro-F1\u63d0\u9ad855%\uff0cMicro-F1\u63d0\u9ad847%\uff0c\u652f\u6301VLM\u878d\u5408\u4f5c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7ba1\u9053\u7684\u6821\u51c6\u3001\u53ef\u89e3\u91ca\u548c\u9c81\u68d2\u51b3\u7b56\u652f\u6301\u7ec4\u4ef6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u653e\u5927\u53ef\u9760\u6a21\u578b\u95f4\u7684\u4e00\u81f4\u6027\uff0c\u4fdd\u7559\u72ec\u7279\u6b63\u786e\u7684\u5355\u6a21\u578b\u4fe1\u53f7\uff0c\u5e76\u9002\u5e94\u6f02\u79fb\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u53ef\u9760\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u878d\u5408\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.01174", "pdf": "https://arxiv.org/pdf/2510.01174", "abs": "https://arxiv.org/abs/2510.01174", "authors": ["Yanzhe Chen", "Kevin Qinghong Lin", "Mike Zheng Shou"], "title": "Code2Video: A Code-centric Paradigm for Educational Video Generation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC", "cs.MM"], "comment": "Project Page: https://showlab.github.io/Code2Video/", "summary": "While recent generative models advance pixel-space video synthesis, they remain limited in producing professional educational videos, which demand disciplinary knowledge, precise visual structures, and coherent transitions, limiting their applicability in educational scenarios. Intuitively, such requirements are better addressed through the manipulation of a renderable environment, which can be explicitly controlled via logical commands (e.g., code). In this work, we propose Code2Video, a code-centric agent framework for generating educational videos via executable Python code. The framework comprises three collaborative agents: (i) Planner, which structures lecture content into temporally coherent flows and prepares corresponding visual assets; (ii) Coder, which converts structured instructions into executable Python codes while incorporating scope-guided auto-fix to enhance efficiency; and (iii) Critic, which leverages vision-language models (VLM) with visual anchor prompts to refine spatial layout and ensure clarity. To support systematic evaluation, we build MMMC, a benchmark of professionally produced, discipline-specific educational videos. We evaluate MMMC across diverse dimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and particularly, TeachQuiz, a novel end-to-end metric that quantifies how well a VLM, after unlearning, can recover knowledge by watching the generated videos. Our results demonstrate the potential of Code2Video as a scalable, interpretable, and controllable approach, achieving 40% improvement over direct code generation and producing videos comparable to human-crafted tutorials. The code and datasets are available at https://github.com/showlab/Code2Video.", "AI": {"tldr": "Code2Video\u662f\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7801\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u6267\u884c\u7684Python\u4ee3\u7801\u751f\u6210\u6559\u80b2\u89c6\u9891\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u4e13\u4e1a\u6559\u80b2\u89c6\u9891\u5236\u4f5c\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u50cf\u7d20\u7a7a\u95f4\u89c6\u9891\u751f\u6210\u6a21\u578b\u96be\u4ee5\u5236\u4f5c\u4e13\u4e1a\u6559\u80b2\u89c6\u9891\uff0c\u56e0\u4e3a\u8fd9\u7c7b\u89c6\u9891\u9700\u8981\u5b66\u79d1\u77e5\u8bc6\u3001\u7cbe\u786e\u7684\u89c6\u89c9\u7ed3\u6784\u548c\u8fde\u8d2f\u7684\u8fc7\u6e21\u3002\u901a\u8fc7\u53ef\u6e32\u67d3\u73af\u5883\u7684\u4ee3\u7801\u63a7\u5236\u80fd\u66f4\u597d\u5730\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\u3002", "method": "\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u534f\u4f5c\u667a\u80fd\u4f53\uff1aPlanner\uff08\u89c4\u5212\u5185\u5bb9\u6d41\u7a0b\u548c\u89c6\u89c9\u8d44\u6e90\uff09\u3001Coder\uff08\u5c06\u6307\u4ee4\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u5305\u542b\u8303\u56f4\u5f15\u5bfc\u7684\u81ea\u52a8\u4fee\u590d\uff09\u3001Critic\uff08\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u7a7a\u95f4\u5e03\u5c40\u548c\u6e05\u6670\u5ea6\uff09\u3002", "result": "\u5728MMMC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCode2Video\u76f8\u6bd4\u76f4\u63a5\u4ee3\u7801\u751f\u6210\u63d0\u5347\u4e8640%\u7684\u6548\u679c\uff0c\u751f\u6210\u7684\u89c6\u9891\u8d28\u91cf\u53ef\u4e0e\u4eba\u5de5\u5236\u4f5c\u7684\u6559\u7a0b\u76f8\u5ab2\u7f8e\u3002", "conclusion": "Code2Video\u5c55\u793a\u4e86\u4f5c\u4e3a\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u63a7\u65b9\u6cd5\u7684\u6f5c\u529b\uff0c\u4e3a\u6559\u80b2\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.01183", "pdf": "https://arxiv.org/pdf/2510.01183", "abs": "https://arxiv.org/abs/2510.01183", "authors": ["Jiahao Wang", "Luoxin Ye", "TaiMing Lu", "Junfei Xiao", "Jiahan Zhang", "Yuxiang Guo", "Xijun Liu", "Rama Chellappa", "Cheng Peng", "Alan Yuille", "Jieneng Chen"], "title": "EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory", "categories": ["cs.CV"], "comment": "Code available at: https://github.com/JiahaoPlus/EvoWorld", "summary": "Humans possess a remarkable ability to mentally explore and replay 3D environments they have previously experienced. Inspired by this mental process, we present EvoWorld: a world model that bridges panoramic video generation with evolving 3D memory to enable spatially consistent long-horizon exploration. Given a single panoramic image as input, EvoWorld first generates future video frames by leveraging a video generator with fine-grained view control, then evolves the scene's 3D reconstruction using a feedforward plug-and-play transformer, and finally synthesizes futures by conditioning on geometric reprojections from this evolving explicit 3D memory. Unlike prior state-of-the-arts that synthesize videos only, our key insight lies in exploiting this evolving 3D reconstruction as explicit spatial guidance for the video generation process, projecting the reconstructed geometry onto target viewpoints to provide rich spatial cues that significantly enhance both visual realism and geometric consistency. To evaluate long-range exploration capabilities, we introduce the first comprehensive benchmark spanning synthetic outdoor environments, Habitat indoor scenes, and challenging real-world scenarios, with particular emphasis on loop-closure detection and spatial coherence over extended trajectories. Extensive experiments demonstrate that our evolving 3D memory substantially improves visual fidelity and maintains spatial scene coherence compared to existing approaches, representing a significant advance toward long-horizon spatially consistent world modeling.", "AI": {"tldr": "EvoWorld\u662f\u4e00\u4e2a\u7ed3\u5408\u5168\u666f\u89c6\u9891\u751f\u6210\u4e0e\u6f14\u53163D\u8bb0\u5fc6\u7684\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u5229\u7528\u6f14\u5316\u76843D\u91cd\u5efa\u4f5c\u4e3a\u663e\u5f0f\u7a7a\u95f4\u5f15\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u89c6\u89c9\u771f\u5b9e\u611f\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "motivation": "\u53d7\u4eba\u7c7b\u80fd\u591f\u5728\u8111\u6d77\u4e2d\u63a2\u7d22\u548c\u91cd\u653e3D\u73af\u5883\u7684\u80fd\u529b\u542f\u53d1\uff0c\u65e8\u5728\u5b9e\u73b0\u7a7a\u95f4\u4e00\u81f4\u7684\u957f\u65f6\u7a0b\u63a2\u7d22\u3002", "method": "\u9996\u5148\u901a\u8fc7\u5177\u6709\u7ec6\u7c92\u5ea6\u89c6\u56fe\u63a7\u5236\u7684\u89c6\u9891\u751f\u6210\u5668\u751f\u6210\u672a\u6765\u89c6\u9891\u5e27\uff0c\u7136\u540e\u4f7f\u7528\u524d\u9988\u5373\u63d2\u5373\u7528transformer\u6f14\u5316\u573a\u666f\u76843D\u91cd\u5efa\uff0c\u6700\u540e\u57fa\u4e8e\u8fd9\u4e2a\u6f14\u5316\u663e\u5f0f3D\u8bb0\u5fc6\u7684\u51e0\u4f55\u91cd\u6295\u5f71\u5408\u6210\u672a\u6765\u5e27\u3002", "result": "\u5728\u5408\u6210\u6237\u5916\u73af\u5883\u3001Habitat\u5ba4\u5185\u573a\u666f\u548c\u771f\u5b9e\u573a\u666f\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6f14\u53163D\u8bb0\u5fc6\u663e\u8457\u63d0\u9ad8\u4e86\u89c6\u89c9\u4fdd\u771f\u5ea6\u5e76\u4fdd\u6301\u4e86\u7a7a\u95f4\u573a\u666f\u4e00\u81f4\u6027\u3002", "conclusion": "EvoWorld\u4ee3\u8868\u4e86\u5411\u957f\u65f6\u7a0b\u7a7a\u95f4\u4e00\u81f4\u4e16\u754c\u5efa\u6a21\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u6f14\u53163D\u8bb0\u5fc6\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9\u771f\u5b9e\u6027\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2510.01186", "pdf": "https://arxiv.org/pdf/2510.01186", "abs": "https://arxiv.org/abs/2510.01186", "authors": ["Fei Shen", "Weihao Xu", "Rui Yan", "Dong Zhang", "Xiangbo Shu", "Jinhui Tang"], "title": "IMAGEdit: Let Any Subject Transform", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we present IMAGEdit, a training-free framework for any number of video subject editing that manipulates the appearances of multiple designated subjects while preserving non-target regions, without finetuning or retraining. We achieve this by providing robust multimodal conditioning and precise mask sequences through a prompt-guided multimodal alignment module and a prior-based mask retargeting module. We first leverage large models' understanding and generation capabilities to produce multimodal information and mask motion sequences for multiple subjects across various types. Then, the obtained prior mask sequences are fed into a pretrained mask-driven video generation model to synthesize the edited video. With strong generalization capability, IMAGEdit remedies insufficient prompt-side multimodal conditioning and overcomes mask boundary entanglement in videos with any number of subjects, thereby significantly expanding the applicability of video editing. More importantly, IMAGEdit is compatible with any mask-driven video generation model, significantly improving overall performance. Extensive experiments on our newly constructed multi-subject benchmark MSVBench verify that IMAGEdit consistently surpasses state-of-the-art methods. Code, models, and datasets are publicly available at https://github.com/XWH-A/IMAGEdit.", "AI": {"tldr": "IMAGEdit\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u591a\u4e3b\u4f53\u7f16\u8f91\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u7f16\u8f91\u4efb\u610f\u6570\u91cf\u7684\u89c6\u9891\u4e3b\u4f53\u5916\u89c2\uff0c\u540c\u65f6\u4fdd\u7559\u975e\u76ee\u6807\u533a\u57df\uff0c\u65e0\u9700\u5fae\u8c03\u6216\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u5728\u591a\u4e3b\u4f53\u7f16\u8f91\u65f6\u5b58\u5728\u591a\u6a21\u6001\u6761\u4ef6\u4e0d\u8db3\u548c\u63a9\u7801\u8fb9\u754c\u7ea0\u7f20\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u9002\u7528\u6027\u3002", "method": "\u901a\u8fc7\u63d0\u793a\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u5bf9\u9f50\u6a21\u5757\u548c\u57fa\u4e8e\u5148\u9a8c\u7684\u63a9\u7801\u91cd\u5b9a\u5411\u6a21\u5757\uff0c\u63d0\u4f9b\u9c81\u68d2\u7684\u591a\u6a21\u6001\u6761\u4ef6\u548c\u7cbe\u786e\u7684\u63a9\u7801\u5e8f\u5217\uff0c\u7136\u540e\u5229\u7528\u9884\u8bad\u7ec3\u7684\u63a9\u7801\u9a71\u52a8\u89c6\u9891\u751f\u6210\u6a21\u578b\u5408\u6210\u7f16\u8f91\u540e\u7684\u89c6\u9891\u3002", "result": "\u5728\u65b0\u5efa\u7684\u591a\u4e3b\u4f53\u57fa\u51c6MSVBench\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cIMAGEdit\u6301\u7eed\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "IMAGEdit\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u6269\u5c55\u4e86\u89c6\u9891\u7f16\u8f91\u7684\u9002\u7528\u6027\uff0c\u4e14\u4e0e\u4efb\u4f55\u63a9\u7801\u9a71\u52a8\u89c6\u9891\u751f\u6210\u6a21\u578b\u517c\u5bb9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2510.00050", "pdf": "https://arxiv.org/pdf/2510.00050", "abs": "https://arxiv.org/abs/2510.00050", "authors": ["Youquan Fu", "Ruiyang Si", "Hongfa Wang", "Dongzhan Zhou", "Jiacheng Sun", "Ping Luo", "Di Hu", "Hongyuan Zhang", "Xuelong Li"], "title": "Object-AVEdit: An Object-level Audio-Visual Editing Model", "categories": ["cs.MM", "cs.AI", "cs.CV", "cs.SD", "eess.AS"], "comment": null, "summary": "There is a high demand for audio-visual editing in video post-production and the film making field. While numerous models have explored audio and video editing, they struggle with object-level audio-visual operations. Specifically, object-level audio-visual editing requires the ability to perform object addition, replacement, and removal across both audio and visual modalities, while preserving the structural information of the source instances during the editing process. In this paper, we present \\textbf{Object-AVEdit}, achieving the object-level audio-visual editing based on the inversion-regeneration paradigm. To achieve the object-level controllability during editing, we develop a word-to-sounding-object well-aligned audio generation model, bridging the gap in object-controllability between audio and current video generation models. Meanwhile, to achieve the better structural information preservation and object-level editing effect, we propose an inversion-regeneration holistically-optimized editing algorithm, ensuring both information retention during the inversion and better regeneration effect. Extensive experiments demonstrate that our editing model achieved advanced results in both audio-video object-level editing tasks with fine audio-visual semantic alignment. In addition, our developed audio generation model also achieved advanced performance. More results on our project page: https://gewu-lab.github.io/Object_AVEdit-website/.", "AI": {"tldr": "Object-AVEdit\u662f\u4e00\u4e2a\u57fa\u4e8e\u53cd\u6f14-\u518d\u751f\u8303\u5f0f\u7684\u5bf9\u8c61\u7ea7\u97f3\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f00\u53d1\u8bcd-\u53d1\u58f0\u5bf9\u8c61\u5bf9\u9f50\u7684\u97f3\u9891\u751f\u6210\u6a21\u578b\u548c\u6574\u4f53\u4f18\u5316\u7684\u7f16\u8f91\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u8de8\u6a21\u6001\u7684\u5bf9\u8c61\u6dfb\u52a0\u3001\u66ff\u6362\u548c\u5220\u9664\u64cd\u4f5c\u3002", "motivation": "\u5f53\u524d\u97f3\u89c6\u9891\u7f16\u8f91\u6a21\u578b\u96be\u4ee5\u5904\u7406\u5bf9\u8c61\u7ea7\u7684\u8de8\u6a21\u6001\u64cd\u4f5c\uff0c\u9700\u8981\u80fd\u591f\u5728\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u540c\u65f6\u5904\u7406\u97f3\u9891\u548c\u89c6\u89c9\u6a21\u6001\u7684\u5bf9\u8c61\u64cd\u4f5c\uff0c\u5e76\u4fdd\u6301\u6e90\u5b9e\u4f8b\u7684\u7ed3\u6784\u4fe1\u606f\u3002", "method": "\u57fa\u4e8e\u53cd\u6f14-\u518d\u751f\u8303\u5f0f\uff0c\u5f00\u53d1\u8bcd-\u53d1\u58f0\u5bf9\u8c61\u5bf9\u9f50\u7684\u97f3\u9891\u751f\u6210\u6a21\u578b\u6765\u5f25\u5408\u97f3\u9891\u548c\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e4b\u95f4\u7684\u5bf9\u8c61\u53ef\u63a7\u6027\u5dee\u8ddd\uff0c\u5e76\u63d0\u51fa\u6574\u4f53\u4f18\u5316\u7684\u7f16\u8f91\u7b97\u6cd5\u6765\u4fdd\u8bc1\u4fe1\u606f\u4fdd\u7559\u548c\u66f4\u597d\u7684\u518d\u751f\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u97f3\u89c6\u9891\u5bf9\u8c61\u7ea7\u7f16\u8f91\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u5148\u8fdb\u6210\u679c\uff0c\u5177\u6709\u826f\u597d\u7684\u97f3\u89c6\u9891\u8bed\u4e49\u5bf9\u9f50\u6548\u679c\uff0c\u97f3\u9891\u751f\u6210\u6a21\u578b\u4e5f\u8868\u73b0\u51fa\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "Object-AVEdit\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9\u8c61\u7ea7\u97f3\u89c6\u9891\u7f16\u8f91\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u97f3\u9891\u751f\u6210\u6a21\u578b\u548c\u7f16\u8f91\u7b97\u6cd5\u89e3\u51b3\u4e86\u8de8\u6a21\u6001\u5bf9\u8c61\u64cd\u4f5c\u7684\u6311\u6218\u3002"}}
{"id": "2510.00051", "pdf": "https://arxiv.org/pdf/2510.00051", "abs": "https://arxiv.org/abs/2510.00051", "authors": ["Trinh Ngoc Huynh", "Nguyen Duc Kien", "Nguyen Hai Anh", "Dinh Tran Hiep", "Manuela Vaneckova", "Tomas Uher", "Jeroen Van Schependom", "Stijn Denissen", "Tran Quoc Long", "Nguyen Linh Trung", "Guy Nagels"], "title": "Latent Representation Learning from 3D Brain MRI for Interpretable Prediction in Multiple Sclerosis", "categories": ["eess.IV", "cs.CV", "q-bio.QM"], "comment": "The abstract has been condensed to under 1920 characters", "summary": "We present InfoVAE-Med3D, a latent-representation learning approach for 3D brain MRI that targets interpretable biomarkers of cognitive decline. Standard statistical models and shallow machine learning often lack power, while most deep learning methods behave as black boxes. Our method extends InfoVAE to explicitly maximize mutual information between images and latent variables, producing compact, structured embeddings that retain clinically meaningful content. We evaluate on two cohorts: a large healthy-control dataset (n=6527) with chronological age, and a clinical multiple sclerosis dataset from Charles University in Prague (n=904) with age and Symbol Digit Modalities Test (SDMT) scores. The learned latents support accurate brain-age and SDMT regression, preserve key medical attributes, and form intuitive clusters that aid interpretation. Across reconstruction and downstream prediction tasks, InfoVAE-Med3D consistently outperforms other VAE variants, indicating stronger information capture in the embedding space. By uniting predictive performance with interpretability, InfoVAE-Med3D offers a practical path toward MRI-based biomarkers and more transparent analysis of cognitive deterioration in neurological disease.", "AI": {"tldr": "InfoVAE-Med3D\u662f\u4e00\u79cd\u7528\u4e8e3D\u8111\u90e8MRI\u7684\u6f5c\u5728\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e8\u5728\u53d1\u73b0\u8ba4\u77e5\u8870\u9000\u7684\u53ef\u89e3\u91ca\u751f\u7269\u6807\u5fd7\u7269\u3002\u8be5\u65b9\u6cd5\u6269\u5c55\u4e86InfoVAE\uff0c\u901a\u8fc7\u6700\u5927\u5316\u56fe\u50cf\u4e0e\u6f5c\u5728\u53d8\u91cf\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\uff0c\u751f\u6210\u7d27\u51d1\u3001\u7ed3\u6784\u5316\u7684\u5d4c\u5165\uff0c\u4fdd\u7559\u4e34\u5e8a\u610f\u4e49\u5185\u5bb9\u3002\u5728\u4e24\u4e2a\u961f\u5217\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u8111\u9f84\u548cSDMT\u56de\u5f52\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u5176\u4ed6VAE\u53d8\u4f53\u3002", "motivation": "\u6807\u51c6\u7edf\u8ba1\u6a21\u578b\u548c\u6d45\u5c42\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u6548\u529b\uff0c\u800c\u5927\u591a\u6570\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8868\u73b0\u4e3a\u9ed1\u76d2\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9884\u6d4b\u6027\u80fd\u53c8\u5177\u6709\u53ef\u89e3\u91ca\u6027\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u795e\u7ecf\u75be\u75c5\u4e2d\u7684\u8ba4\u77e5\u8870\u9000\u3002", "method": "\u6269\u5c55InfoVAE\u65b9\u6cd5\uff0c\u660e\u786e\u6700\u5927\u5316\u56fe\u50cf\u4e0e\u6f5c\u5728\u53d8\u91cf\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\uff0c\u751f\u6210\u7d27\u51d1\u4e14\u7ed3\u6784\u5316\u7684\u5d4c\u5165\u3002\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff1a\u5927\u578b\u5065\u5eb7\u5bf9\u7167\u7ec4\uff08n=6527\uff09\u548c\u4e34\u5e8a\u591a\u53d1\u6027\u786c\u5316\u6570\u636e\u96c6\uff08n=904\uff09\u3002", "result": "\u5b66\u4e60\u7684\u6f5c\u5728\u53d8\u91cf\u652f\u6301\u51c6\u786e\u7684\u8111\u9f84\u548cSDMT\u56de\u5f52\uff0c\u4fdd\u7559\u5173\u952e\u533b\u5b66\u5c5e\u6027\uff0c\u5e76\u5f62\u6210\u76f4\u89c2\u7684\u805a\u7c7b\u4ee5\u5e2e\u52a9\u89e3\u91ca\u3002\u5728\u91cd\u5efa\u548c\u4e0b\u6e38\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cInfoVAE-Med3D\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6VAE\u53d8\u4f53\uff0c\u8868\u660e\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5177\u6709\u66f4\u5f3a\u7684\u4fe1\u606f\u6355\u83b7\u80fd\u529b\u3002", "conclusion": "InfoVAE-Med3D\u901a\u8fc7\u7ed3\u5408\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u57fa\u4e8eMRI\u7684\u751f\u7269\u6807\u5fd7\u7269\u548c\u795e\u7ecf\u75be\u75c5\u8ba4\u77e5\u8870\u9000\u7684\u900f\u660e\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2510.00260", "pdf": "https://arxiv.org/pdf/2510.00260", "abs": "https://arxiv.org/abs/2510.00260", "authors": ["Debottam Dutta", "Chaitanya Amballa", "Zhongweiyang Xu", "Yu-Lin Wei", "Romit Roy Choudhury"], "title": "Learning Energy-based Variational Latent Prior for VAEs", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Variational Auto-Encoders (VAEs) are known to generate blurry and inconsistent samples. One reason for this is the \"prior hole\" problem. A prior hole refers to regions that have high probability under the VAE's prior but low probability under the VAE's posterior. This means that during data generation, high probability samples from the prior could have low probability under the posterior, resulting in poor quality data. Ideally, a prior needs to be flexible enough to match the posterior while retaining the ability to generate samples fast. Generative models continue to address this tradeoff. This paper proposes to model the prior as an energy-based model (EBM). While EBMs are known to offer the flexibility to match posteriors (and also improving the ELBO), they are traditionally slow in sample generation due to their dependency on MCMC methods. Our key idea is to bring a variational approach to tackle the normalization constant in EBMs, thus bypassing the expensive MCMC approaches. The variational form can be approximated with a sampler network, and we show that such an approach to training priors can be formulated as an alternating optimization problem. Moreover, the same sampler reduces to an implicit variational prior during generation, providing efficient and fast sampling. We compare our Energy-based Variational Latent Prior (EVaLP) method to multiple SOTA baselines and show improvements in image generation quality, reduced prior holes, and better sampling efficiency.", "AI": {"tldr": "\u63d0\u51faEVaLP\u65b9\u6cd5\uff0c\u4f7f\u7528\u80fd\u91cf\u57fa\u6a21\u578b\u4f5c\u4e3aVAE\u5148\u9a8c\uff0c\u901a\u8fc7\u53d8\u5206\u65b9\u6cd5\u89e3\u51b3EBM\u5f52\u4e00\u5316\u5e38\u6570\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u91c7\u6837\u548c\u6539\u5584\u751f\u6210\u8d28\u91cf", "motivation": "\u89e3\u51b3VAE\u4e2d'\u5148\u9a8c\u7a7a\u6d1e'\u95ee\u9898\uff0c\u5373\u5148\u9a8c\u9ad8\u6982\u7387\u533a\u57df\u5728\u540e\u9a8c\u4e2d\u6982\u7387\u4f4e\uff0c\u5bfc\u81f4\u751f\u6210\u6837\u672c\u6a21\u7cca\u548c\u4e0d\u4e00\u81f4", "method": "\u5c06\u5148\u9a8c\u5efa\u6a21\u4e3a\u80fd\u91cf\u57fa\u6a21\u578b\uff0c\u5f15\u5165\u53d8\u5206\u65b9\u6cd5\u5904\u7406EBM\u5f52\u4e00\u5316\u5e38\u6570\uff0c\u4f7f\u7528\u91c7\u6837\u5668\u7f51\u7edc\u8fd1\u4f3c\u53d8\u5206\u5f62\u5f0f\uff0c\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\u8bad\u7ec3", "result": "\u76f8\u6bd4SOTA\u57fa\u7ebf\uff0c\u5728\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3001\u51cf\u5c11\u5148\u9a8c\u7a7a\u6d1e\u548c\u91c7\u6837\u6548\u7387\u65b9\u9762\u5747\u6709\u6539\u8fdb", "conclusion": "EVaLP\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86VAE\u5148\u9a8c\u7a7a\u6d1e\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7075\u6d3b\u5148\u9a8c\u5339\u914d\u548c\u9ad8\u6548\u91c7\u6837"}}
{"id": "2510.00430", "pdf": "https://arxiv.org/pdf/2510.00430", "abs": "https://arxiv.org/abs/2510.00430", "authors": ["Suhyeon Lee", "Jong Chul Ye"], "title": "Plug-and-Play Prompt Refinement via Latent Feedback for Diffusion Model Alignment", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "23 pages, 15 figures", "summary": "Despite the recent progress, reinforcement learning (RL)-based fine-tuning of diffusion models often struggles with generalization, composability, and robustness against reward hacking. Recent studies have explored prompt refinement as a modular alternative, but most adopt a feed-forward approach that applies a single refined prompt throughout the entire sampling trajectory, thereby failing to fully leverage the sequential nature of reinforcement learning. To address this, here we introduce PromptLoop, a plug-and-play RL framework that incorporates latent feedback into step-wise prompt refinement. Rather than modifying diffusion model weights, a multimodal large language model (MLLM) is trained with RL to iteratively update prompts based on intermediate latent states of diffusion models. This design achieves a structural analogy to the Diffusion RL approach, while retaining the flexibility and generality of prompt-based alignment. Extensive experiments across diverse reward functions and diffusion backbones demonstrate that PromptLoop (i) achieves effective reward optimization, (ii) generalizes seamlessly to unseen models, (iii) composes orthogonally with existing alignment methods, and (iv) mitigates over-optimization and reward hacking.", "AI": {"tldr": "PromptLoop\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63d2\u4ef6\u5f0f\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6269\u6563\u6a21\u578b\u91c7\u6837\u8fc7\u7a0b\u4e2d\u8fed\u4ee3\u66f4\u65b0\u63d0\u793a\uff0c\u5b9e\u73b0\u6709\u6548\u7684\u5956\u52b1\u4f18\u5316\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6269\u6563\u6a21\u578b\u5fae\u8c03\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u3001\u7ec4\u5408\u6027\u548c\u6297\u5956\u52b1\u653b\u51fb\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u800c\u73b0\u6709\u7684\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u5927\u591a\u91c7\u7528\u524d\u9988\u65b9\u5f0f\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u7684\u5e8f\u5217\u7279\u6027\u3002", "method": "\u8bad\u7ec3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff0c\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4e2d\u95f4\u6f5c\u72b6\u6001\u8fed\u4ee3\u66f4\u65b0\u63d0\u793a\uff0c\u800c\u4e0d\u662f\u4fee\u6539\u6269\u6563\u6a21\u578b\u6743\u91cd\uff0c\u5b9e\u73b0\u7ed3\u6784\u4e0a\u7c7b\u4f3c\u4e8eDiffusion RL\u4f46\u4fdd\u6301\u63d0\u793a\u5bf9\u9f50\u7684\u7075\u6d3b\u6027\u3002", "result": "\u5728\u5404\u79cd\u5956\u52b1\u51fd\u6570\u548c\u6269\u6563\u9aa8\u5e72\u7f51\u7edc\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPromptLoop\u80fd\u6709\u6548\u4f18\u5316\u5956\u52b1\u3001\u65e0\u7f1d\u6cdb\u5316\u5230\u672a\u89c1\u6a21\u578b\u3001\u4e0e\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u6b63\u4ea4\u7ec4\u5408\uff0c\u5e76\u51cf\u8f7b\u8fc7\u4f18\u5316\u548c\u5956\u52b1\u653b\u51fb\u95ee\u9898\u3002", "conclusion": "PromptLoop\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u63d2\u4ef6\u5f0fRL\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u63d0\u793a\u4f18\u5316\u5b9e\u73b0\u4e86\u6269\u6563\u6a21\u578b\u7684\u5956\u52b1\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6cdb\u5316\u6027\u548c\u7ec4\u5408\u6027\u3002"}}
