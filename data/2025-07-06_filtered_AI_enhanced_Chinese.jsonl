{"id": "2507.02257", "pdf": "https://arxiv.org/pdf/2507.02257", "abs": "https://arxiv.org/abs/2507.02257", "authors": ["Stephen Pasch", "Joel K. Salzman", "Changxi Zheng"], "title": "Gbake: Baking 3D Gaussian Splats into Reflection Probes", "categories": ["cs.GR"], "comment": "SIGGRAPH 2025 Posters", "summary": "The growing popularity of 3D Gaussian Splatting has created the need to integrate traditional computer graphics techniques and assets in splatted environments. Since 3D Gaussian primitives encode lighting and geometry jointly as appearance, meshes are relit improperly when inserted directly in a mixture of 3D Gaussians and thus appear noticeably out of place. We introduce GBake, a specialized tool for baking reflection probes from Gaussian-splatted scenes that enables realistic reflection mapping of traditional 3D meshes in the Unity game engine.", "AI": {"tldr": "GBake\u5de5\u5177\u7528\u4e8e\u4ece3D\u9ad8\u65af\u6cfc\u6e85\u573a\u666f\u4e2d\u70d8\u7119\u53cd\u5c04\u63a2\u9488\uff0c\u4ee5\u5728Unity\u4e2d\u5b9e\u73b0\u4f20\u7edf3D\u7f51\u683c\u7684\u903c\u771f\u53cd\u5c04\u6620\u5c04\u3002", "motivation": "\u89e3\u51b33D\u9ad8\u65af\u6cfc\u6e85\u73af\u5883\u4e2d\u4f20\u7edf\u7f51\u683c\u56e0\u5149\u7167\u548c\u51e0\u4f55\u8054\u5408\u7f16\u7801\u800c\u65e0\u6cd5\u6b63\u786e\u91cd\u5149\u7167\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1GBake\u5de5\u5177\uff0c\u4ece\u9ad8\u65af\u6cfc\u6e85\u573a\u666f\u4e2d\u70d8\u7119\u53cd\u5c04\u63a2\u9488\u3002", "result": "\u5b9e\u73b0\u4e86\u4f20\u7edf3D\u7f51\u683c\u5728\u9ad8\u65af\u6cfc\u6e85\u73af\u5883\u4e2d\u7684\u903c\u771f\u53cd\u5c04\u6620\u5c04\u3002", "conclusion": "GBake\u5de5\u5177\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u7f51\u683c\u5728\u9ad8\u65af\u6cfc\u6e85\u73af\u5883\u4e2d\u7684\u5149\u7167\u4e0d\u5339\u914d\u95ee\u9898\u3002"}}
{"id": "2507.02674", "pdf": "https://arxiv.org/pdf/2507.02674", "abs": "https://arxiv.org/abs/2507.02674", "authors": ["Tom Kneiphof", "Reinhard Klein"], "title": "Real-time Image-based Lighting of Glints", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Image-based lighting is a widely used technique to reproduce shading under real-world lighting conditions, especially in real-time rendering applications. A particularly challenging scenario involves materials exhibiting a sparkling or glittering appearance, caused by discrete microfacets scattered across their surface. In this paper, we propose an efficient approximation for image-based lighting of glints, enabling fully dynamic material properties and environment maps. Our novel approach is grounded in real-time glint rendering under area light illumination and employs standard environment map filtering techniques. Crucially, our environment map filtering process is sufficiently fast to be executed on a per-frame basis. Our method assumes that the environment map is partitioned into few homogeneous regions of constant radiance. By filtering the corresponding indicator functions with the normal distribution function, we obtain the probabilities for individual microfacets to reflect light from each region. During shading, these probabilities are utilized to hierarchically sample a multinomial distribution, facilitated by our novel dual-gated Gaussian approximation of binomial distributions. We validate that our real-time approximation is close to ground-truth renderings for a range of material properties and lighting conditions, and demonstrate robust and stable performance, with little overhead over rendering glints from a single directional light. Compared to rendering smooth materials without glints, our approach requires twice as much memory to store the prefiltered environment map.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9e\u65f6\u6e32\u67d3\u4e2d\u57fa\u4e8e\u56fe\u50cf\u7684\u95ea\u5149\u7167\u660e\uff0c\u652f\u6301\u52a8\u6001\u6750\u8d28\u5c5e\u6027\u548c\u73af\u5883\u8d34\u56fe\u3002", "motivation": "\u89e3\u51b3\u5728\u5b9e\u65f6\u6e32\u67d3\u4e2d\uff0c\u5177\u6709\u95ea\u5149\u6216\u95ea\u70c1\u5916\u89c2\u7684\u6750\u8d28\u5728\u57fa\u4e8e\u56fe\u50cf\u7167\u660e\u4e0b\u7684\u6311\u6218\u3002", "method": "\u5229\u7528\u5b9e\u65f6\u95ea\u5149\u6e32\u67d3\u548c\u6807\u51c6\u73af\u5883\u8d34\u56fe\u6ee4\u6ce2\u6280\u672f\uff0c\u901a\u8fc7\u5206\u533a\u73af\u5883\u8d34\u56fe\u5e76\u6ee4\u6ce2\uff0c\u7ed3\u5408\u65b0\u578b\u53cc\u95e8\u9ad8\u65af\u8fd1\u4f3c\u65b9\u6cd5\u8fdb\u884c\u91c7\u6837\u3002", "result": "\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u6750\u8d28\u548c\u5149\u7167\u6761\u4ef6\u4e0b\u63a5\u8fd1\u771f\u5b9e\u6e32\u67d3\uff0c\u6027\u80fd\u7a33\u5b9a\u4e14\u5f00\u9500\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5b9e\u65f6\u6e32\u67d3\u4e2d\u9ad8\u6548\u4e14\u63a5\u8fd1\u771f\u5b9e\uff0c\u4ec5\u9700\u4e24\u500d\u5185\u5b58\u5b58\u50a8\u9884\u6ee4\u6ce2\u73af\u5883\u8d34\u56fe\u3002"}}
{"id": "2507.02803", "pdf": "https://arxiv.org/pdf/2507.02803", "abs": "https://arxiv.org/abs/2507.02803", "authors": ["Gent Serifi", "Marcel C. B\u00fchler"], "title": "HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars", "categories": ["cs.CV", "cs.GR"], "comment": "Project page: https://gserifi.github.io/HyperGaussians", "summary": "We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for high-quality animatable face avatars. Creating such detailed face avatars from videos is a challenging problem and has numerous applications in augmented and virtual reality. While tremendous successes have been achieved for static faces, animatable avatars from monocular videos still fall in the uncanny valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face through a collection of 3D Gaussian primitives. 3DGS excels at rendering static faces, but the state-of-the-art still struggles with nonlinear deformations, complex lighting effects, and fine details. While most related works focus on predicting better Gaussian parameters from expression codes, we rethink the 3D Gaussian representation itself and how to make it more expressive. Our insights lead to a novel extension of 3D Gaussians to high-dimensional multivariate Gaussians, dubbed 'HyperGaussians'. The higher dimensionality increases expressivity through conditioning on a learnable local embedding. However, splatting HyperGaussians is computationally expensive because it requires inverting a high-dimensional covariance matrix. We solve this by reparameterizing the covariance matrix, dubbed the 'inverse covariance trick'. This trick boosts the efficiency so that HyperGaussians can be seamlessly integrated into existing models. To demonstrate this, we plug in HyperGaussians into the state-of-the-art in fast monocular face avatars: FlashAvatar. Our evaluation on 19 subjects from 4 face datasets shows that HyperGaussians outperform 3DGS numerically and visually, particularly for high-frequency details like eyeglass frames, teeth, complex facial movements, and specular reflections.", "AI": {"tldr": "HyperGaussians\u662f\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u8d28\u91cf\u53ef\u52a8\u753b\u5316\u9762\u90e8\u5934\u50cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u975e\u7ebf\u6027\u53d8\u5f62\u548c\u590d\u6742\u5149\u7167\u4e0b\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u67093D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u5728\u9759\u6001\u9762\u90e8\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u5728\u53ef\u52a8\u753b\u5316\u5934\u50cf\u4e2d\u4ecd\u5b58\u5728\u7ec6\u8282\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u975e\u7ebf\u6027\u53d8\u5f62\u548c\u590d\u6742\u5149\u7167\u4e0b\u3002", "method": "\u63d0\u51faHyperGaussians\uff0c\u901a\u8fc7\u9ad8\u7ef4\u591a\u53d8\u91cf\u9ad8\u65af\u5206\u5e03\u589e\u5f3a\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u2018\u9006\u534f\u65b9\u5dee\u6280\u5de7\u2019\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u57284\u4e2a\u9762\u90e8\u6570\u636e\u96c6\u4e0a\u768419\u4e2a\u53d7\u8bd5\u8005\u4e2d\uff0cHyperGaussians\u5728\u6570\u503c\u548c\u89c6\u89c9\u4e0a\u5747\u4f18\u4e8e3DGS\uff0c\u5c24\u5176\u5728\u773c\u955c\u6846\u3001\u7259\u9f7f\u7b49\u9ad8\u9891\u7ec6\u8282\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "HyperGaussians\u4e3a\u9ad8\u8d28\u91cf\u53ef\u52a8\u753b\u5316\u9762\u90e8\u5934\u50cf\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u7ec6\u8282\u8868\u73b0\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u53d6\u5f97\u7a81\u7834\u3002"}}
{"id": "2507.02861", "pdf": "https://arxiv.org/pdf/2507.02861", "abs": "https://arxiv.org/abs/2507.02861", "authors": ["Zhening Huang", "Xiaoyang Wu", "Fangcheng Zhong", "Hengshuang Zhao", "Matthias Nie\u00dfner", "Joan Lasenby"], "title": "LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "Project Page: https://litereality.github.io; Video:   https://www.youtube.com/watch?v=ecK9m3LXg2c&feature=youtu.be", "summary": "We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor environments into compact, realistic, and interactive 3D virtual replicas. LiteReality not only reconstructs scenes that visually resemble reality but also supports key features essential for graphics pipelines -- such as object individuality, articulation, high-quality physically based rendering materials, and physically based interaction. At its core, LiteReality first performs scene understanding and parses the results into a coherent 3D layout and objects with the help of a structured scene graph. It then reconstructs the scene by retrieving the most visually similar 3D artist-crafted models from a curated asset database. Next, the Material Painting module enhances realism by recovering high-quality, spatially varying materials. Finally, the reconstructed scene is integrated into a simulation engine with basic physical properties to enable interactive behavior. The resulting scenes are compact, editable, and fully compatible with standard graphics pipelines, making them suitable for applications in AR/VR, gaming, robotics, and digital twins. In addition, LiteReality introduces a training-free object retrieval module that achieves state-of-the-art similarity performance on the Scan2CAD benchmark, along with a robust material painting module capable of transferring appearances from images of any style to 3D assets -- even under severe misalignment, occlusion, and poor lighting. We demonstrate the effectiveness of LiteReality on both real-life scans and public datasets. Project page: https://litereality.github.io; Video: https://www.youtube.com/watch?v=ecK9m3LXg2c", "AI": {"tldr": "LiteReality\u662f\u4e00\u4e2a\u5c06RGB-D\u626b\u63cf\u8f6c\u6362\u4e3a\u7d27\u51d1\u3001\u903c\u771f\u4e14\u4ea4\u4e92\u5f0f3D\u865a\u62df\u526f\u672c\u7684\u65b0\u6d41\u7a0b\uff0c\u652f\u6301\u9ad8\u8d28\u91cf\u6e32\u67d3\u548c\u7269\u7406\u4ea4\u4e92\u3002", "motivation": "\u65e8\u5728\u5c06\u5ba4\u5185\u73af\u5883\u7684RGB-D\u626b\u63cf\u8f6c\u6362\u4e3a\u529f\u80fd\u5b8c\u5907\u76843D\u865a\u62df\u526f\u672c\uff0c\u9002\u7528\u4e8eAR/VR\u3001\u6e38\u620f\u3001\u673a\u5668\u4eba\u7b49\u9886\u57df\u3002", "method": "\u901a\u8fc7\u573a\u666f\u7406\u89e3\u30013D\u6a21\u578b\u68c0\u7d22\u3001\u6750\u8d28\u589e\u5f3a\u548c\u7269\u7406\u5f15\u64ce\u96c6\u6210\uff0c\u6784\u5efa\u53ef\u7f16\u8f91\u4e14\u517c\u5bb9\u6807\u51c6\u56fe\u5f62\u7ba1\u7ebf\u7684\u573a\u666f\u3002", "result": "\u5728Scan2CAD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u76f8\u4f3c\u6027\u6027\u80fd\uff0c\u5e76\u80fd\u5904\u7406\u590d\u6742\u6761\u4ef6\u4e0b\u7684\u6750\u8d28\u8fc1\u79fb\u3002", "conclusion": "LiteReality\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u903c\u771f\u76843D\u91cd\u5efa\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.02252", "pdf": "https://arxiv.org/pdf/2507.02252", "abs": "https://arxiv.org/abs/2507.02252", "authors": ["Zeyu Lei", "Hongyuan Yu", "Jinlin Wu", "Zhen Chen"], "title": "SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Precise surgical interventions are vital to patient safety, and advanced enhancement algorithms have been developed to assist surgeons in decision-making. Despite significant progress, these algorithms are typically designed for single tasks in specific scenarios, limiting their effectiveness in complex real-world situations. To address this limitation, we propose SurgVisAgent, an end-to-end intelligent surgical vision agent built on multimodal large language models (MLLMs). SurgVisAgent dynamically identifies distortion categories and severity levels in endoscopic images, enabling it to perform a variety of enhancement tasks such as low-light enhancement, overexposure correction, motion blur elimination, and smoke removal. Specifically, to achieve superior surgical scenario understanding, we design a prior model that provides domain-specific knowledge. Additionally, through in-context few-shot learning and chain-of-thought (CoT) reasoning, SurgVisAgent delivers customized image enhancements tailored to a wide range of distortion types and severity levels, thereby addressing the diverse requirements of surgeons. Furthermore, we construct a comprehensive benchmark simulating real-world surgical distortions, on which extensive experiments demonstrate that SurgVisAgent surpasses traditional single-task models, highlighting its potential as a unified solution for surgical assistance.", "AI": {"tldr": "SurgVisAgent\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u667a\u80fd\u624b\u672f\u89c6\u89c9\u4ee3\u7406\uff0c\u80fd\u591f\u52a8\u6001\u8bc6\u522b\u5185\u7aa5\u955c\u56fe\u50cf\u7684\u5931\u771f\u7c7b\u522b\u548c\u4e25\u91cd\u7a0b\u5ea6\uff0c\u6267\u884c\u591a\u79cd\u589e\u5f3a\u4efb\u52a1\uff0c\u4f18\u4e8e\u4f20\u7edf\u5355\u4efb\u52a1\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7b97\u6cd5\u901a\u5e38\u9488\u5bf9\u5355\u4e00\u4efb\u52a1\u8bbe\u8ba1\uff0c\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u624b\u672f\u573a\u666f\u7684\u591a\u6837\u5316\u9700\u6c42\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u6548\u679c\u3002", "method": "\u63d0\u51faSurgVisAgent\uff0c\u7ed3\u5408\u5148\u9a8c\u6a21\u578b\u3001\u4e0a\u4e0b\u6587\u5c11\u6837\u672c\u5b66\u4e60\u548c\u94fe\u5f0f\u63a8\u7406\uff08CoT\uff09\uff0c\u5b9e\u73b0\u5b9a\u5236\u5316\u56fe\u50cf\u589e\u5f3a\u3002", "result": "\u5728\u6a21\u62df\u771f\u5b9e\u624b\u672f\u5931\u771f\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSurgVisAgent\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u5355\u4efb\u52a1\u6a21\u578b\u3002", "conclusion": "SurgVisAgent\u6709\u671b\u6210\u4e3a\u624b\u672f\u8f85\u52a9\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.02270", "pdf": "https://arxiv.org/pdf/2507.02270", "abs": "https://arxiv.org/abs/2507.02270", "authors": ["Fanghai Yi", "Zehong Zheng", "Zexiao Liang", "Yihang Dong", "Xiyang Fang", "Wangyu Wu", "Xuhang Chen"], "title": "MAC-Lookup: Multi-Axis Conditional Lookup Model for Underwater Image Enhancement", "categories": ["cs.CV"], "comment": "Accepted by IEEE SMC 2025", "summary": "Enhancing underwater images is crucial for exploration. These images face visibility and color issues due to light changes, water turbidity, and bubbles. Traditional prior-based methods and pixel-based methods often fail, while deep learning lacks sufficient high-quality datasets. We introduce the Multi-Axis Conditional Lookup (MAC-Lookup) model, which enhances visual quality by improving color accuracy, sharpness, and contrast. It includes Conditional 3D Lookup Table Color Correction (CLTCC) for preliminary color and quality correction and Multi-Axis Adaptive Enhancement (MAAE) for detail refinement. This model prevents over-enhancement and saturation while handling underwater challenges. Extensive experiments show that MAC-Lookup excels in enhancing underwater images by restoring details and colors better than existing methods. The code is https://github.com/onlycatdoraemon/MAC-Lookup.", "AI": {"tldr": "MAC-Lookup\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u989c\u8272\u6821\u6b63\u548c\u7ec6\u8282\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u6c34\u4e0b\u56fe\u50cf\u8d28\u91cf\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u6c34\u4e0b\u56fe\u50cf\u56e0\u5149\u7ebf\u3001\u6d51\u6d4a\u5ea6\u548c\u6c14\u6ce1\u7b49\u95ee\u9898\u5bfc\u81f4\u8d28\u91cf\u4e0b\u964d\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u4f20\u7edf\u5148\u9a8c\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff09\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51faMAC-Lookup\u6a21\u578b\uff0c\u5305\u542bCLTCC\uff08\u989c\u8272\u6821\u6b63\uff09\u548cMAAE\uff08\u7ec6\u8282\u589e\u5f3a\uff09\uff0c\u907f\u514d\u8fc7\u589e\u5f3a\u548c\u9971\u548c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMAC-Lookup\u5728\u6062\u590d\u7ec6\u8282\u548c\u989c\u8272\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MAC-Lookup\u6709\u6548\u89e3\u51b3\u4e86\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\u95ee\u9898\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.02314", "pdf": "https://arxiv.org/pdf/2507.02314", "abs": "https://arxiv.org/abs/2507.02314", "authors": ["JaeHyuck Choi", "MinJun Kim", "JeHyeong Hong"], "title": "MAGIC: Mask-Guided Diffusion Inpainting with Multi-Level Perturbations and Context-Aware Alignment for Few-Shot Anomaly Generation", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 6 figures", "summary": "Few-shot anomaly generation is emerging as a practical solution for augmenting the scarce anomaly data in industrial quality control settings. An ideal generator would meet three demands at once, namely (i) keep the normal background intact, (ii) inpaint anomalous regions to tightly overlap with the corresponding anomaly masks, and (iii) generate anomalous regions in a semantically valid location, while still producing realistic, diverse appearances from only a handful of real examples. Existing diffusion-based methods usually satisfy at most two of these requirements: global anomaly generators corrupt the background, whereas mask-guided ones often falter when the mask is imprecise or misplaced. We propose MAGIC--Mask-guided inpainting with multi-level perturbations and Context-aware alignment--to resolve all three issues. At its core, MAGIC fine-tunes a Stable Diffusion inpainting backbone that preserves normal regions and ensures strict adherence of the synthesized anomaly to the supplied mask, directly addressing background corruption and misalignment. To offset the diversity loss that fine-tuning can cause, MAGIC adds two complementary perturbation strategies: (i) Gaussian prompt-level perturbation applied during fine-tuning and inference that broadens the global appearance of anomalies while avoiding low-fidelity textual appearances, and (ii) mask-guided spatial noise injection that enriches local texture variations. Additionally, the context-aware mask alignment module forms semantic correspondences and relocates masks so that every anomaly remains plausibly contained within the host object, eliminating out-of-boundary artifacts. Under a consistent identical evaluation protocol on the MVTec-AD dataset, MAGIC outperforms previous state-of-the-arts in downstream anomaly tasks.", "AI": {"tldr": "MAGIC\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5c11\u6837\u672c\u5f02\u5e38\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7ea7\u6270\u52a8\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u5bf9\u9f50\uff0c\u89e3\u51b3\u4e86\u80cc\u666f\u4fdd\u62a4\u3001\u5f02\u5e38\u533a\u57df\u7cbe\u786e\u751f\u6210\u548c\u8bed\u4e49\u5408\u7406\u6027\u4e09\u5927\u9700\u6c42\u3002", "motivation": "\u5de5\u4e1a\u8d28\u91cf\u63a7\u5236\u4e2d\u5f02\u5e38\u6570\u636e\u7a00\u7f3a\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u80cc\u666f\u4fdd\u62a4\u3001\u5f02\u5e38\u533a\u57df\u7cbe\u786e\u751f\u6210\u548c\u8bed\u4e49\u5408\u7406\u6027\u3002", "method": "MAGIC\u57fa\u4e8eStable Diffusion\u4fee\u590d\u6a21\u578b\uff0c\u7ed3\u5408\u9ad8\u65af\u63d0\u793a\u7ea7\u6270\u52a8\u548c\u63a9\u7801\u5f15\u5bfc\u7684\u7a7a\u95f4\u566a\u58f0\u6ce8\u5165\uff0c\u5e76\u5f15\u5165\u4e0a\u4e0b\u6587\u611f\u77e5\u63a9\u7801\u5bf9\u9f50\u6a21\u5757\u3002", "result": "\u5728MVTec-AD\u6570\u636e\u96c6\u4e0a\uff0cMAGIC\u5728\u4e0b\u6e38\u5f02\u5e38\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MAGIC\u901a\u8fc7\u591a\u7ea7\u6270\u52a8\u548c\u4e0a\u4e0b\u6587\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5c11\u6837\u672c\u5f02\u5e38\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.02316", "pdf": "https://arxiv.org/pdf/2507.02316", "abs": "https://arxiv.org/abs/2507.02316", "authors": ["Zecheng Zhao", "Selena Song", "Tong Chen", "Zhi Chen", "Shazia Sadiq", "Yadan Luo"], "title": "Are Synthetic Videos Useful? A Benchmark for Retrieval-Centric Evaluation of Synthetic Videos", "categories": ["cs.CV"], "comment": "7 pages, 10 figures", "summary": "Text-to-video (T2V) synthesis has advanced rapidly, yet current evaluation metrics primarily capture visual quality and temporal consistency, offering limited insight into how synthetic videos perform in downstream tasks such as text-to-video retrieval (TVR). In this work, we introduce SynTVA, a new dataset and benchmark designed to evaluate the utility of synthetic videos for building retrieval models. Based on 800 diverse user queries derived from MSRVTT training split, we generate synthetic videos using state-of-the-art T2V models and annotate each video-text pair along four key semantic alignment dimensions: Object \\& Scene, Action, Attribute, and Prompt Fidelity. Our evaluation framework correlates general video quality assessment (VQA) metrics with these alignment scores, and examines their predictive power for downstream TVR performance. To explore pathways of scaling up, we further develop an Auto-Evaluator to estimate alignment quality from existing metrics. Beyond benchmarking, our results show that SynTVA is a valuable asset for dataset augmentation, enabling the selection of high-utility synthetic samples that measurably improve TVR outcomes. Project page and dataset can be found at https://jasoncodemaker.github.io/SynTVA/.", "AI": {"tldr": "SynTVA\u662f\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5408\u6210\u89c6\u9891\u5728\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\uff08TVR\uff09\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\u3002\u901a\u8fc7800\u4e2a\u591a\u6837\u5316\u7684\u7528\u6237\u67e5\u8be2\u751f\u6210\u5408\u6210\u89c6\u9891\uff0c\u5e76\u6807\u6ce8\u89c6\u9891-\u6587\u672c\u5bf9\u7684\u8bed\u4e49\u5bf9\u9f50\u7ef4\u5ea6\uff0c\u7814\u7a76\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u4e0e\u4e0b\u6e38TVR\u6027\u80fd\u7684\u5173\u7cfb\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u89c6\u9891\uff08T2V\uff09\u5408\u6210\u7684\u8bc4\u4f30\u6307\u6807\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u8d28\u91cf\u548c\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u7f3a\u4e4f\u5bf9\u5408\u6210\u89c6\u9891\u5728\u4e0b\u6e38\u4efb\u52a1\uff08\u5982TVR\uff09\u4e2d\u8868\u73b0\u7684\u8bc4\u4ef7\u3002", "method": "\u57fa\u4e8eMSRVTT\u8bad\u7ec3\u96c6\u7684800\u4e2a\u7528\u6237\u67e5\u8be2\u751f\u6210\u5408\u6210\u89c6\u9891\uff0c\u6807\u6ce8\u56db\u4e2a\u8bed\u4e49\u5bf9\u9f50\u7ef4\u5ea6\uff08\u5bf9\u8c61\u4e0e\u573a\u666f\u3001\u52a8\u4f5c\u3001\u5c5e\u6027\u3001\u63d0\u793a\u4fdd\u771f\u5ea6\uff09\uff0c\u5e76\u5f00\u53d1Auto-Evaluator\u4f30\u8ba1\u5bf9\u9f50\u8d28\u91cf\u3002", "result": "SynTVA\u4e0d\u4ec5\u662f\u4e00\u4e2a\u6709\u6548\u7684\u57fa\u51c6\u5de5\u5177\uff0c\u8fd8\u80fd\u901a\u8fc7\u9009\u62e9\u9ad8\u8d28\u91cf\u5408\u6210\u6837\u672c\u663e\u8457\u63d0\u5347TVR\u6027\u80fd\u3002", "conclusion": "SynTVA\u4e3a\u5408\u6210\u89c6\u9891\u7684\u8bc4\u4f30\u548c\u6570\u636e\u96c6\u589e\u5f3a\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.02321", "pdf": "https://arxiv.org/pdf/2507.02321", "abs": "https://arxiv.org/abs/2507.02321", "authors": ["Nina Konovalova", "Maxim Nikolaev", "Andrey Kuznetsov", "Aibek Alanov"], "title": "Heeding the Inner Voice: Aligning ControlNet Training via Intermediate Features Feedback", "categories": ["cs.CV"], "comment": "code available at https://github.com/ControlGenAI/InnerControl", "summary": "Despite significant progress in text-to-image diffusion models, achieving precise spatial control over generated outputs remains challenging. ControlNet addresses this by introducing an auxiliary conditioning module, while ControlNet++ further refines alignment through a cycle consistency loss applied only to the final denoising steps. However, this approach neglects intermediate generation stages, limiting its effectiveness. We propose InnerControl, a training strategy that enforces spatial consistency across all diffusion steps. Our method trains lightweight convolutional probes to reconstruct input control signals (e.g., edges, depth) from intermediate UNet features at every denoising step. These probes efficiently extract signals even from highly noisy latents, enabling pseudo ground truth controls for training. By minimizing the discrepancy between predicted and target conditions throughout the entire diffusion process, our alignment loss improves both control fidelity and generation quality. Combined with established techniques like ControlNet++, InnerControl achieves state-of-the-art performance across diverse conditioning methods (e.g., edges, depth).", "AI": {"tldr": "InnerControl\u901a\u8fc7\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5377\u79ef\u63a2\u9488\u5728\u6269\u6563\u8fc7\u7a0b\u7684\u6bcf\u4e00\u6b65\u5f3a\u5236\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u7684\u7a7a\u95f4\u63a7\u5236\u7cbe\u5ea6\u548c\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5c3d\u7ba1\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u751f\u6210\u8f93\u51fa\u4e2d\u5b9e\u73b0\u7cbe\u786e\u7684\u7a7a\u95f4\u63a7\u5236\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982ControlNet++\uff09\u4ec5\u5173\u6ce8\u6700\u7ec8\u53bb\u566a\u6b65\u9aa4\uff0c\u5ffd\u7565\u4e86\u4e2d\u95f4\u751f\u6210\u9636\u6bb5\u3002", "method": "\u63d0\u51faInnerControl\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5377\u79ef\u63a2\u9488\u4ece\u4e2d\u95f4UNet\u7279\u5f81\u91cd\u5efa\u8f93\u5165\u63a7\u5236\u4fe1\u53f7\uff08\u5982\u8fb9\u7f18\u3001\u6df1\u5ea6\uff09\uff0c\u5e76\u5728\u6574\u4e2a\u6269\u6563\u8fc7\u7a0b\u4e2d\u6700\u5c0f\u5316\u9884\u6d4b\u4e0e\u76ee\u6807\u6761\u4ef6\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "result": "InnerControl\u7ed3\u5408ControlNet++\u7b49\u6280\u672f\uff0c\u5728\u591a\u79cd\u6761\u4ef6\u65b9\u6cd5\uff08\u5982\u8fb9\u7f18\u3001\u6df1\u5ea6\uff09\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "InnerControl\u901a\u8fc7\u5168\u8fc7\u7a0b\u7684\u6269\u6563\u4e00\u81f4\u6027\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u95f4\u63a7\u5236\u7684\u7cbe\u786e\u6027\u548c\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2507.02358", "pdf": "https://arxiv.org/pdf/2507.02358", "abs": "https://arxiv.org/abs/2507.02358", "authors": ["Anlin Zheng", "Haochen Wang", "Yucheng Zhao", "Weipeng Deng", "Tiancai Wang", "Xiangyu Zhang", "Xiaojuan Qi"], "title": "Holistic Tokenizer for Autoregressive Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": "17 pages, 10 figures", "summary": "The vanilla autoregressive image generation model generates visual tokens in a step-by-step fashion, which limits the ability to capture holistic relationships among token sequences. Moreover, most visual tokenizers map local image patches into latent tokens, leading to limited global information. To address this, we introduce \\textit{Hita}, a novel image tokenizer for autoregressive (AR) image generation. It introduces a holistic-to-local tokenization scheme with learnable holistic queries and local patch tokens. Besides, Hita incorporates two key strategies for improved alignment with the AR generation process: 1) it arranges a sequential structure with holistic tokens at the beginning followed by patch-level tokens while using causal attention to maintain awareness of previous tokens; and 2) before feeding the de-quantized tokens into the decoder, Hita adopts a lightweight fusion module to control information flow to prioritize holistic tokens. Extensive experiments show that Hita accelerates the training speed of AR generators and outperforms those trained with vanilla tokenizers, achieving \\textbf{2.59 FID} and \\textbf{281.9 IS} on the ImageNet benchmark. A detailed analysis of the holistic representation highlights its ability to capture global image properties such as textures, materials, and shapes. Additionally, Hita also demonstrates effectiveness in zero-shot style transfer and image in-painting. The code is available at \\href{https://github.com/CVMI-Lab/Hita}{https://github.com/CVMI-Lab/Hita}", "AI": {"tldr": "Hita\u662f\u4e00\u79cd\u65b0\u578b\u56fe\u50cf\u6807\u8bb0\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40\u5230\u5c40\u90e8\u7684\u6807\u8bb0\u5316\u65b9\u6848\u548c\u5173\u952e\u7b56\u7565\u6539\u8fdb\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u901f\u5ea6\u548c\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6a21\u578b\u9010\u6b65\u751f\u6210\u89c6\u89c9\u6807\u8bb0\uff0c\u96be\u4ee5\u6355\u6349\u6807\u8bb0\u5e8f\u5217\u7684\u6574\u4f53\u5173\u7cfb\uff0c\u4e14\u89c6\u89c9\u6807\u8bb0\u5316\u65b9\u6cd5\u901a\u5e38\u7f3a\u4e4f\u5168\u5c40\u4fe1\u606f\u3002", "method": "Hita\u91c7\u7528\u5168\u5c40\u5230\u5c40\u90e8\u7684\u6807\u8bb0\u5316\u65b9\u6848\uff0c\u7ed3\u5408\u53ef\u5b66\u4e60\u7684\u5168\u5c40\u67e5\u8be2\u548c\u5c40\u90e8\u8865\u4e01\u6807\u8bb0\uff0c\u5e76\u5f15\u5165\u987a\u5e8f\u7ed3\u6784\u548c\u8f7b\u91cf\u7ea7\u878d\u5408\u6a21\u5757\u4f18\u5316\u4fe1\u606f\u6d41\u3002", "result": "\u5728ImageNet\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHita\u8fbe\u52302.59 FID\u548c281.9 IS\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6807\u8bb0\u5316\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u96f6\u6837\u672c\u98ce\u683c\u8fc1\u79fb\u548c\u56fe\u50cf\u4fee\u590d\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "Hita\u901a\u8fc7\u6539\u8fdb\u6807\u8bb0\u5316\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u7684\u5168\u5c40\u4fe1\u606f\u6355\u6349\u80fd\u529b\u548c\u751f\u6210\u8d28\u91cf\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.02363", "pdf": "https://arxiv.org/pdf/2507.02363", "abs": "https://arxiv.org/abs/2507.02363", "authors": ["Jiahao Wu", "Rui Peng", "Jianbo Jiao", "Jiayu Yang", "Luyang Tang", "Kaiqiang Xiong", "Jie Liang", "Jinbo Yan", "Runling Liu", "Ronggang Wang"], "title": "LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local Implicit Feature Decoupling", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Due to the complex and highly dynamic motions in the real world, synthesizing dynamic videos from multi-view inputs for arbitrary viewpoints is challenging. Previous works based on neural radiance field or 3D Gaussian splatting are limited to modeling fine-scale motion, greatly restricting their application. In this paper, we introduce LocalDyGS, which consists of two parts to adapt our method to both large-scale and fine-scale motion scenes: 1) We decompose a complex dynamic scene into streamlined local spaces defined by seeds, enabling global modeling by capturing motion within each local space. 2) We decouple static and dynamic features for local space motion modeling. A static feature shared across time steps captures static information, while a dynamic residual field provides time-specific features. These are combined and decoded to generate Temporal Gaussians, modeling motion within each local space. As a result, we propose a novel dynamic scene reconstruction framework to model highly dynamic real-world scenes more realistically. Our method not only demonstrates competitive performance on various fine-scale datasets compared to state-of-the-art (SOTA) methods, but also represents the first attempt to model larger and more complex highly dynamic scenes. Project page: https://wujh2001.github.io/LocalDyGS/.", "AI": {"tldr": "LocalDyGS\u662f\u4e00\u79cd\u52a8\u6001\u573a\u666f\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u590d\u6742\u52a8\u6001\u573a\u666f\u4e3a\u5c40\u90e8\u7a7a\u95f4\u5e76\u89e3\u8026\u9759\u6001\u4e0e\u52a8\u6001\u7279\u5f81\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5927\u89c4\u6a21\u548c\u7cbe\u7ec6\u5c3a\u5ea6\u8fd0\u52a8\u573a\u666f\u7684\u5efa\u6a21\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u795e\u7ecf\u8f90\u5c04\u573a\u62163D\u9ad8\u65af\u6e85\u5c04\uff09\u5728\u5efa\u6a21\u590d\u6742\u52a8\u6001\u8fd0\u52a8\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5bf9\u5927\u89c4\u6a21\u8fd0\u52a8\u7684\u9002\u5e94\u6027\u4e0d\u8db3\u3002", "method": "1) \u5c06\u52a8\u6001\u573a\u666f\u5206\u89e3\u4e3a\u7531\u79cd\u5b50\u5b9a\u4e49\u7684\u5c40\u90e8\u7a7a\u95f4\uff1b2) \u89e3\u8026\u9759\u6001\u548c\u52a8\u6001\u7279\u5f81\uff0c\u9759\u6001\u7279\u5f81\u6355\u6349\u9759\u6001\u4fe1\u606f\uff0c\u52a8\u6001\u6b8b\u5dee\u573a\u63d0\u4f9b\u65f6\u95f4\u7279\u5b9a\u7279\u5f81\uff0c\u7ed3\u5408\u751f\u6210\u65f6\u95f4\u9ad8\u65af\u6a21\u578b\u3002", "result": "\u5728\u7cbe\u7ec6\u5c3a\u5ea6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u9996\u6b21\u6210\u529f\u5efa\u6a21\u5927\u89c4\u6a21\u590d\u6742\u52a8\u6001\u573a\u666f\u3002", "conclusion": "LocalDyGS\u4e3a\u52a8\u6001\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2507.02373", "pdf": "https://arxiv.org/pdf/2507.02373", "abs": "https://arxiv.org/abs/2507.02373", "authors": ["Xizhe Xue", "Yang Zhou", "Dawei Yan", "Ying Li", "Haokui Zhang", "Rong Xiao"], "title": "UVLM: Benchmarking Video Language Model for Underwater World Understanding", "categories": ["cs.CV"], "comment": "13 pages, 4 figures, 3 tables", "summary": "Recently, the remarkable success of large language models (LLMs) has achieved a profound impact on the field of artificial intelligence. Numerous advanced works based on LLMs have been proposed and applied in various scenarios. Among them, video language models (VidLMs) are particularly widely used. However, existing works primarily focus on terrestrial scenarios, overlooking the highly demanding application needs of underwater observation. To overcome this gap, we introduce UVLM, an under water observation benchmark which is build through a collaborative approach combining human expertise and AI models. To ensure data quality, we have conducted in-depth considerations from multiple perspectives. First, to address the unique challenges of underwater environments, we selected videos that represent typical underwater challenges including light variations, water turbidity, and diverse viewing angles to construct the dataset. Second, to ensure data diversity, the dataset covers a wide range of frame rates, resolutions, 419 classes of marine animals, and various static plants and terrains. Next, for task diversity, we adopted a structured design where observation targets are categorized into two major classes: biological and environmental. Each category includes content observation and change/action observation, totaling 20 distinct task types. Finally, we designed several challenging evaluation metrics to enable quantitative comparison and analysis of different methods. Experiments on two representative VidLMs demonstrate that fine-tuning VidLMs on UVLM significantly improves underwater world understanding while also showing potential for slight improvements on existing in-air VidLM benchmarks, such as VideoMME and Perception text. The dataset and prompt engineering will be released publicly.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6c34\u4e0b\u89c2\u6d4b\u57fa\u51c6UVLM\uff0c\u586b\u8865\u4e86\u73b0\u6709\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u5728\u9646\u5730\u573a\u666f\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u591a\u89d2\u5ea6\u6570\u636e\u8bbe\u8ba1\u548c\u4efb\u52a1\u591a\u6837\u6027\u63d0\u5347\u6c34\u4e0b\u73af\u5883\u7406\u89e3\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u9646\u5730\u573a\u666f\uff0c\u5ffd\u89c6\u4e86\u6c34\u4e0b\u89c2\u6d4b\u7684\u9ad8\u9700\u6c42\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u6784\u5efa\u4e13\u95e8\u7684\u6c34\u4e0b\u6570\u636e\u96c6\u548c\u4efb\u52a1\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u548cAI\u6a21\u578b\u6784\u5efaUVLM\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u5178\u578b\u6c34\u4e0b\u6311\u6218\uff08\u5982\u5149\u7ebf\u53d8\u5316\u3001\u6c34\u8d28\u6d51\u6d4a\u7b49\uff09\uff0c\u5e76\u8bbe\u8ba1\u591a\u6837\u5316\u7684\u4efb\u52a1\u548c\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728UVLM\u4e0a\u5fae\u8c03\u7684\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u663e\u8457\u63d0\u5347\u6c34\u4e0b\u7406\u89e3\u80fd\u529b\uff0c\u540c\u65f6\u5bf9\u73b0\u6709\u9646\u5730\u57fa\u51c6\u4e5f\u6709\u8f7b\u5fae\u6539\u8fdb\u3002", "conclusion": "UVLM\u4e3a\u6c34\u4e0b\u89c2\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u57fa\u51c6\uff0c\u672a\u6765\u5c06\u516c\u5f00\u6570\u636e\u96c6\u548c\u63d0\u793a\u5de5\u7a0b\u3002"}}
{"id": "2507.02405", "pdf": "https://arxiv.org/pdf/2507.02405", "abs": "https://arxiv.org/abs/2507.02405", "authors": ["Ayantika Das", "Moitreya Chaudhuri", "Koushik Bhat", "Keerthi Ram", "Mihail Bota", "Mohanasankar Sivaprakasam"], "title": "PosDiffAE: Position-aware Diffusion Auto-encoder For High-Resolution Brain Tissue Classification Incorporating Artifact Restoration", "categories": ["cs.CV"], "comment": "Published in IEEE Journal of Biomedical and Health Informatics (Early   Access Available) https://ieeexplore.ieee.org/document/10989734", "summary": "Denoising diffusion models produce high-fidelity image samples by capturing the image distribution in a progressive manner while initializing with a simple distribution and compounding the distribution complexity. Although these models have unlocked new applicabilities, the sampling mechanism of diffusion does not offer means to extract image-specific semantic representation, which is inherently provided by auto-encoders. The encoding component of auto-encoders enables mapping between a specific image and its latent space, thereby offering explicit means of enforcing structures in the latent space. By integrating an encoder with the diffusion model, we establish an auto-encoding formulation, which learns image-specific representations and offers means to organize the latent space. In this work, First, we devise a mechanism to structure the latent space of a diffusion auto-encoding model, towards recognizing region-specific cellular patterns in brain images. We enforce the representations to regress positional information of the patches from high-resolution images. This creates a conducive latent space for differentiating tissue types of the brain. Second, we devise an unsupervised tear artifact restoration technique based on neighborhood awareness, utilizing latent representations and the constrained generation capability of diffusion models during inference. Third, through representational guidance and leveraging the inference time steerable noising and denoising capability of diffusion, we devise an unsupervised JPEG artifact restoration technique.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u81ea\u52a8\u7f16\u7801\u5668\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u56fe\u50cf\u7279\u5b9a\u8868\u793a\u5e76\u7ec4\u7ec7\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e94\u7528\u4e8e\u8111\u56fe\u50cf\u5206\u6790\u548c\u4f2a\u5f71\u4fee\u590d\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u867d\u7136\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u4f46\u7f3a\u4e4f\u63d0\u53d6\u56fe\u50cf\u8bed\u4e49\u8868\u793a\u7684\u80fd\u529b\uff0c\u800c\u81ea\u52a8\u7f16\u7801\u5668\u80fd\u63d0\u4f9b\u8fd9\u79cd\u80fd\u529b\u3002\u901a\u8fc7\u7ed3\u5408\u4e24\u8005\uff0c\u5b9e\u73b0\u5bf9\u8111\u56fe\u50cf\u533a\u57df\u7279\u5f02\u6027\u6a21\u5f0f\u7684\u5206\u6790\u548c\u4f2a\u5f71\u4fee\u590d\u3002", "method": "\u96c6\u6210\u7f16\u7801\u5668\u4e0e\u6269\u6563\u6a21\u578b\uff0c\u5f62\u6210\u81ea\u52a8\u7f16\u7801\u7ed3\u6784\uff1b\u8bbe\u8ba1\u673a\u5236\u7ec4\u7ec7\u6f5c\u5728\u7a7a\u95f4\u4ee5\u8bc6\u522b\u8111\u56fe\u50cf\u533a\u57df\u6a21\u5f0f\uff1b\u63d0\u51fa\u57fa\u4e8e\u90bb\u57df\u611f\u77e5\u7684\u65e0\u76d1\u7763\u6495\u88c2\u4f2a\u5f71\u4fee\u590d\u6280\u672f\uff1b\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u5b9e\u73b0\u65e0\u76d1\u7763JPEG\u4f2a\u5f71\u4fee\u590d\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u652f\u6301\u8111\u7ec4\u7ec7\u7c7b\u578b\u533a\u5206\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u5b9e\u73b0\u4e86\u65e0\u76d1\u7763\u7684\u6495\u88c2\u548cJPEG\u4f2a\u5f71\u4fee\u590d\u3002", "conclusion": "\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u81ea\u52a8\u7f16\u7801\u5668\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u56fe\u50cf\u8bed\u4e49\u8868\u793a\u80fd\u529b\u548c\u4f2a\u5f71\u4fee\u590d\u6548\u679c\u3002"}}
{"id": "2507.02419", "pdf": "https://arxiv.org/pdf/2507.02419", "abs": "https://arxiv.org/abs/2507.02419", "authors": ["Yiming Zhong", "Xiaolin Zhang", "Ligang Liu", "Yao Zhao", "Yunchao Wei"], "title": "AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars", "categories": ["cs.CV"], "comment": null, "summary": "Similar to facial beautification in real life, 3D virtual avatars require personalized customization to enhance their visual appeal, yet this area remains insufficiently explored. Although current 3D Gaussian editing methods can be adapted for facial makeup purposes, these methods fail to meet the fundamental requirements for achieving realistic makeup effects: 1) ensuring a consistent appearance during drivable expressions, 2) preserving the identity throughout the makeup process, and 3) enabling precise control over fine details. To address these, we propose a specialized 3D makeup method named AvatarMakeup, leveraging a pretrained diffusion model to transfer makeup patterns from a single reference photo of any individual. We adopt a coarse-to-fine idea to first maintain the consistent appearance and identity, and then to refine the details. In particular, the diffusion model is employed to generate makeup images as supervision. Due to the uncertainties in diffusion process, the generated images are inconsistent across different viewpoints and expressions. Therefore, we propose a Coherent Duplication method to coarsely apply makeup to the target while ensuring consistency across dynamic and multiview effects. Coherent Duplication optimizes a global UV map by recoding the averaged facial attributes among the generated makeup images. By querying the global UV map, it easily synthesizes coherent makeup guidance from arbitrary views and expressions to optimize the target avatar. Given the coarse makeup avatar, we further enhance the makeup by incorporating a Refinement Module into the diffusion model to achieve high makeup quality. Experiments demonstrate that AvatarMakeup achieves state-of-the-art makeup transfer quality and consistency throughout animation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAvatarMakeup\u76843D\u5316\u5986\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u4ece\u5355\u5f20\u53c2\u8003\u7167\u7247\u4e2d\u8f6c\u79fb\u5316\u5986\u6a21\u5f0f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u8868\u60c5\u548c\u591a\u89c6\u89d2\u4e0b\u7684\u4e00\u81f4\u6027\u3001\u8eab\u4efd\u4fdd\u6301\u53ca\u7ec6\u8282\u63a7\u5236\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u67093D\u9ad8\u65af\u7f16\u8f91\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u73b0\u771f\u5b9e\u5316\u5986\u6548\u679c\u7684\u57fa\u672c\u8981\u6c42\uff0c\u5305\u62ec\u52a8\u6001\u8868\u60c5\u4e0b\u7684\u5916\u89c2\u4e00\u81f4\u6027\u3001\u5316\u5986\u8fc7\u7a0b\u4e2d\u7684\u8eab\u4efd\u4fdd\u6301\u4ee5\u53ca\u5bf9\u7cbe\u7ec6\u7ec6\u8282\u7684\u7cbe\u786e\u63a7\u5236\u3002", "method": "\u91c7\u7528\u4ece\u7c97\u5230\u7ec6\u7684\u7b56\u7565\uff0c\u9996\u5148\u901a\u8fc7Coherent Duplication\u65b9\u6cd5\u7c97\u7c92\u5ea6\u5730\u5e94\u7528\u5316\u5986\u5e76\u786e\u4fdd\u4e00\u81f4\u6027\uff0c\u7136\u540e\u901a\u8fc7Refinement Module\u8fdb\u4e00\u6b65\u4f18\u5316\u5316\u5986\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAvatarMakeup\u5728\u5316\u5986\u8f6c\u79fb\u8d28\u91cf\u548c\u52a8\u753b\u4e00\u81f4\u6027\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002", "conclusion": "AvatarMakeup\u901a\u8fc7\u521b\u65b0\u7684\u65b9\u6cd5\u89e3\u51b3\u4e863D\u865a\u62df\u5934\u50cf\u5316\u5986\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5316\u5986\u6548\u679c\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2507.02445", "pdf": "https://arxiv.org/pdf/2507.02445", "abs": "https://arxiv.org/abs/2507.02445", "authors": ["Hailong Yan", "Junjian Huang", "Tingwen Huang"], "title": "IGDNet: Zero-Shot Robust Underexposed Image Enhancement via Illumination-Guided and Denoising", "categories": ["cs.CV", "eess.IV"], "comment": "Submitted to IEEE Transactions on Artificial Intelligence (TAI) on   Oct.31, 2024", "summary": "Current methods for restoring underexposed images typically rely on supervised learning with paired underexposed and well-illuminated images. However, collecting such datasets is often impractical in real-world scenarios. Moreover, these methods can lead to over-enhancement, distorting well-illuminated regions. To address these issues, we propose IGDNet, a Zero-Shot enhancement method that operates solely on a single test image, without requiring guiding priors or training data. IGDNet exhibits strong generalization ability and effectively suppresses noise while restoring illumination. The framework comprises a decomposition module and a denoising module. The former separates the image into illumination and reflection components via a dense connection network, while the latter enhances non-uniformly illuminated regions using an illumination-guided pixel adaptive correction method. A noise pair is generated through downsampling and refined iteratively to produce the final result. Extensive experiments on four public datasets demonstrate that IGDNet significantly improves visual quality under complex lighting conditions. Quantitative results on metrics like PSNR (20.41dB) and SSIM (0.860dB) show that it outperforms 14 state-of-the-art unsupervised methods. The code will be released soon.", "AI": {"tldr": "IGDNet\u662f\u4e00\u79cd\u96f6\u6837\u672c\u589e\u5f3a\u65b9\u6cd5\uff0c\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u6216\u5148\u9a8c\u77e5\u8bc6\uff0c\u4ec5\u901a\u8fc7\u5355\u5f20\u6d4b\u8bd5\u56fe\u50cf\u6062\u590d\u5149\u7167\u5e76\u6291\u5236\u566a\u58f0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6210\u5bf9\u6570\u636e\u96c6\u4e14\u53ef\u80fd\u5bfc\u81f4\u8fc7\u589e\u5f3a\uff0cIGDNet\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u89e3\u6a21\u5757\u548c\u53bb\u566a\u6a21\u5757\u5206\u79bb\u56fe\u50cf\u7684\u5149\u7167\u4e0e\u53cd\u5c04\u6210\u5206\uff0c\u5e76\u8fed\u4ee3\u4f18\u5316\u566a\u58f0\u5bf9\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cPSNR\u548cSSIM\u6307\u6807\u4f18\u4e8e14\u79cd\u65e0\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "IGDNet\u5728\u590d\u6742\u5149\u7167\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u89c6\u89c9\u8d28\u91cf\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.02565", "pdf": "https://arxiv.org/pdf/2507.02565", "abs": "https://arxiv.org/abs/2507.02565", "authors": ["Buzhen Huang", "Chen Li", "Chongyang Xu", "Dongyue Lu", "Jinnan Chen", "Yangang Wang", "Gim Hee Lee"], "title": "Reconstructing Close Human Interaction with Appearance and Proxemics Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Due to visual ambiguities and inter-person occlusions, existing human pose estimation methods cannot recover plausible close interactions from in-the-wild videos. Even state-of-the-art large foundation models~(\\eg, SAM) cannot accurately distinguish human semantics in such challenging scenarios. In this work, we find that human appearance can provide a straightforward cue to address these obstacles. Based on this observation, we propose a dual-branch optimization framework to reconstruct accurate interactive motions with plausible body contacts constrained by human appearances, social proxemics, and physical laws. Specifically, we first train a diffusion model to learn the human proxemic behavior and pose prior knowledge. The trained network and two optimizable tensors are then incorporated into a dual-branch optimization framework to reconstruct human motions and appearances. Several constraints based on 3D Gaussians, 2D keypoints, and mesh penetrations are also designed to assist the optimization. With the proxemics prior and diverse constraints, our method is capable of estimating accurate interactions from in-the-wild videos captured in complex environments. We further build a dataset with pseudo ground-truth interaction annotations, which may promote future research on pose estimation and human behavior understanding. Experimental results on several benchmarks demonstrate that our method outperforms existing approaches. The code and data are available at https://www.buzhenhuang.com/works/CloseApp.html.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4eba\u7c7b\u5916\u89c2\u7684\u53cc\u5206\u652f\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u590d\u6742\u89c6\u9891\u4e2d\u91cd\u5efa\u51c6\u786e\u7684\u4ea4\u4e92\u52a8\u4f5c\uff0c\u7ed3\u5408\u793e\u4ea4\u8ddd\u79bb\u548c\u7269\u7406\u7ea6\u675f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9\u6a21\u7cca\u548c\u906e\u6321\u60c5\u51b5\u4e0b\u96be\u4ee5\u6062\u590d\u771f\u5b9e\u7684\u4ea4\u4e92\u52a8\u4f5c\uff0c\u5373\u4f7f\u662f\u5927\u578b\u57fa\u7840\u6a21\u578b\u4e5f\u65e0\u6cd5\u51c6\u786e\u533a\u5206\u4eba\u7c7b\u8bed\u4e49\u3002", "method": "\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u5b66\u4e60\u793e\u4ea4\u8ddd\u79bb\u548c\u59ff\u6001\u5148\u9a8c\uff0c\u7ed3\u5408\u53cc\u5206\u652f\u4f18\u5316\u6846\u67b6\u548c\u591a\u79cd\u7ea6\u675f\uff083D\u9ad8\u65af\u30012D\u5173\u952e\u70b9\u3001\u7f51\u683c\u7a7f\u900f\uff09\u91cd\u5efa\u52a8\u4f5c\u548c\u5916\u89c2\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u5e26\u6709\u4f2a\u771f\u5b9e\u4ea4\u4e92\u6ce8\u91ca\u7684\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u590d\u6742\u73af\u5883\u4e2d\u51c6\u786e\u4f30\u8ba1\u4ea4\u4e92\u52a8\u4f5c\uff0c\u4e3a\u672a\u6765\u59ff\u6001\u4f30\u8ba1\u548c\u884c\u4e3a\u7406\u89e3\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.02687", "pdf": "https://arxiv.org/pdf/2507.02687", "abs": "https://arxiv.org/abs/2507.02687", "authors": ["JungWoo Chae", "Jiyoon Kim", "JaeWoong Choi", "Kyungyul Kim", "Sangheum Hwang"], "title": "APT: Adaptive Personalized Training for Diffusion Models with Limited Data", "categories": ["cs.CV", "cs.AI", "60J60, 68T07", "I.2.6; I.2.10; I.4.9"], "comment": "CVPR 2025 camera ready. Project page: https://lgcnsai.github.io/apt", "summary": "Personalizing diffusion models using limited data presents significant challenges, including overfitting, loss of prior knowledge, and degradation of text alignment. Overfitting leads to shifts in the noise prediction distribution, disrupting the denoising trajectory and causing the model to lose semantic coherence. In this paper, we propose Adaptive Personalized Training (APT), a novel framework that mitigates overfitting by employing adaptive training strategies and regularizing the model's internal representations during fine-tuning. APT consists of three key components: (1) Adaptive Training Adjustment, which introduces an overfitting indicator to detect the degree of overfitting at each time step bin and applies adaptive data augmentation and adaptive loss weighting based on this indicator; (2)Representation Stabilization, which regularizes the mean and variance of intermediate feature maps to prevent excessive shifts in noise prediction; and (3) Attention Alignment for Prior Knowledge Preservation, which aligns the cross-attention maps of the fine-tuned model with those of the pretrained model to maintain prior knowledge and semantic coherence. Through extensive experiments, we demonstrate that APT effectively mitigates overfitting, preserves prior knowledge, and outperforms existing methods in generating high-quality, diverse images with limited reference data.", "AI": {"tldr": "APT\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u8bad\u7ec3\u7b56\u7565\u548c\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6709\u9650\u6570\u636e\u4e0b\u6269\u6563\u6a21\u578b\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5148\u9a8c\u77e5\u8bc6\u548c\u6587\u672c\u5bf9\u9f50\u3002", "motivation": "\u89e3\u51b3\u6709\u9650\u6570\u636e\u4e0b\u6269\u6563\u6a21\u578b\u4e2a\u6027\u5316\u8bad\u7ec3\u4e2d\u7684\u8fc7\u62df\u5408\u3001\u5148\u9a8c\u77e5\u8bc6\u4e22\u5931\u548c\u6587\u672c\u5bf9\u9f50\u9000\u5316\u95ee\u9898\u3002", "method": "APT\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u81ea\u9002\u5e94\u8bad\u7ec3\u8c03\u6574\u3001\u8868\u793a\u7a33\u5b9a\u5316\u548c\u6ce8\u610f\u529b\u5bf9\u9f50\uff0c\u5206\u522b\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u7b56\u7565\u3001\u6b63\u5219\u5316\u7279\u5f81\u56fe\u548c\u4fdd\u6301\u6ce8\u610f\u529b\u5bf9\u9f50\u6765\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAPT\u80fd\u6709\u6548\u51cf\u5c11\u8fc7\u62df\u5408\uff0c\u4fdd\u6301\u5148\u9a8c\u77e5\u8bc6\uff0c\u5e76\u5728\u6709\u9650\u6570\u636e\u4e0b\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u56fe\u50cf\u3002", "conclusion": "APT\u662f\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u6709\u9650\u6570\u636e\u4e0b\u7684\u6269\u6563\u6a21\u578b\u4e2a\u6027\u5316\u8bad\u7ec3\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.02691", "pdf": "https://arxiv.org/pdf/2507.02691", "abs": "https://arxiv.org/abs/2507.02691", "authors": ["Xiangyang Luo", "Ye Zhu", "Yunfei Liu", "Lijian Lin", "Cong Wan", "Zijian Cai", "Shao-Lun Huang", "Yu Li"], "title": "CanonSwap: High-Fidelity and Consistent Video Face Swapping via Canonical Space Modulation", "categories": ["cs.CV"], "comment": "ICCV Accepted", "summary": "Video face swapping aims to address two primary challenges: effectively transferring the source identity to the target video and accurately preserving the dynamic attributes of the target face, such as head poses, facial expressions, lip-sync, \\etc. Existing methods mainly focus on achieving high-quality identity transfer but often fall short in maintaining the dynamic attributes of the target face, leading to inconsistent results. We attribute this issue to the inherent coupling of facial appearance and motion in videos. To address this, we propose CanonSwap, a novel video face-swapping framework that decouples motion information from appearance information. Specifically, CanonSwap first eliminates motion-related information, enabling identity modification within a unified canonical space. Subsequently, the swapped feature is reintegrated into the original video space, ensuring the preservation of the target face's dynamic attributes. To further achieve precise identity transfer with minimal artifacts and enhanced realism, we design a Partial Identity Modulation module that adaptively integrates source identity features using a spatial mask to restrict modifications to facial regions. Additionally, we introduce several fine-grained synchronization metrics to comprehensively evaluate the performance of video face swapping methods. Extensive experiments demonstrate that our method significantly outperforms existing approaches in terms of visual quality, temporal consistency, and identity preservation. Our project page are publicly available at https://luoxyhappy.github.io/CanonSwap/.", "AI": {"tldr": "CanonSwap\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u6362\u8138\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u9762\u90e8\u5916\u89c2\u548c\u8fd0\u52a8\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u6301\u76ee\u6807\u9762\u90e8\u52a8\u6001\u5c5e\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6362\u8138\u65b9\u6cd5\u5728\u8eab\u4efd\u8f6c\u79fb\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u96be\u4ee5\u4fdd\u6301\u76ee\u6807\u9762\u90e8\u7684\u52a8\u6001\u5c5e\u6027\uff08\u5982\u8868\u60c5\u3001\u59ff\u6001\u7b49\uff09\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u4e00\u81f4\u3002", "method": "CanonSwap\u901a\u8fc7\u5c06\u8fd0\u52a8\u4fe1\u606f\u4e0e\u5916\u89c2\u4fe1\u606f\u89e3\u8026\uff0c\u5148\u5728\u7edf\u4e00\u89c4\u8303\u7a7a\u95f4\u8fdb\u884c\u8eab\u4efd\u4fee\u6539\uff0c\u518d\u5c06\u5176\u91cd\u65b0\u6574\u5408\u5230\u539f\u59cb\u89c6\u9891\u7a7a\u95f4\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u90e8\u5206\u8eab\u4efd\u8c03\u5236\u6a21\u5757\u548c\u7ec6\u7c92\u5ea6\u540c\u6b65\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCanonSwap\u5728\u89c6\u89c9\u8d28\u91cf\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u8eab\u4efd\u4fdd\u6301\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CanonSwap\u901a\u8fc7\u89e3\u8026\u548c\u7cbe\u7ec6\u8c03\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u6362\u8138\u6548\u679c\u3002"}}
{"id": "2507.02713", "pdf": "https://arxiv.org/pdf/2507.02713", "abs": "https://arxiv.org/abs/2507.02713", "authors": ["Qin Guo", "Ailing Zeng", "Dongxu Yue", "Ceyuan Yang", "Yang Cao", "Hanzhong Guo", "Fei Shen", "Wei Liu", "Xihui Liu", "Dan Xu"], "title": "UniMC: Taming Diffusion Transformer for Unified Keypoint-Guided Multi-Class Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Although significant advancements have been achieved in the progress of keypoint-guided Text-to-Image diffusion models, existing mainstream keypoint-guided models encounter challenges in controlling the generation of more general non-rigid objects beyond humans (e.g., animals). Moreover, it is difficult to generate multiple overlapping humans and animals based on keypoint controls solely. These challenges arise from two main aspects: the inherent limitations of existing controllable methods and the lack of suitable datasets. First, we design a DiT-based framework, named UniMC, to explore unifying controllable multi-class image generation. UniMC integrates instance- and keypoint-level conditions into compact tokens, incorporating attributes such as class, bounding box, and keypoint coordinates. This approach overcomes the limitations of previous methods that struggled to distinguish instances and classes due to their reliance on skeleton images as conditions. Second, we propose HAIG-2.9M, a large-scale, high-quality, and diverse dataset designed for keypoint-guided human and animal image generation. HAIG-2.9M includes 786K images with 2.9M instances. This dataset features extensive annotations such as keypoints, bounding boxes, and fine-grained captions for both humans and animals, along with rigorous manual inspection to ensure annotation accuracy. Extensive experiments demonstrate the high quality of HAIG-2.9M and the effectiveness of UniMC, particularly in heavy occlusions and multi-class scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDiT\u7684\u6846\u67b6UniMC\uff0c\u7528\u4e8e\u7edf\u4e00\u63a7\u5236\u591a\u7c7b\u56fe\u50cf\u751f\u6210\uff0c\u5e76\u53d1\u5e03\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u96c6HAIG-2.9M\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5173\u952e\u70b9\u5f15\u5bfc\u6a21\u578b\u5728\u975e\u521a\u6027\u7269\u4f53\u751f\u6210\u548c\u591a\u5b9e\u4f8b\u91cd\u53e0\u751f\u6210\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u5173\u952e\u70b9\u5f15\u5bfc\u6a21\u578b\u5728\u751f\u6210\u975e\u521a\u6027\u7269\u4f53\uff08\u5982\u52a8\u7269\uff09\u548c\u591a\u5b9e\u4f8b\u91cd\u53e0\u573a\u666f\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e3b\u8981\u7531\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u56fa\u6709\u7f3a\u9677\u548c\u7f3a\u4e4f\u5408\u9002\u7684\u6570\u636e\u96c6\u3002", "method": "\u8bbe\u8ba1\u4e86UniMC\u6846\u67b6\uff0c\u5c06\u5b9e\u4f8b\u548c\u5173\u952e\u70b9\u6761\u4ef6\u6574\u5408\u4e3a\u7d27\u51d1\u4ee4\u724c\uff0c\u5e76\u63d0\u51fa\u4e86HAIG-2.9M\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e30\u5bcc\u7684\u6ce8\u91ca\u548c\u4e25\u683c\u7684\u4eba\u5de5\u68c0\u67e5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eHAIG-2.9M\u6570\u636e\u96c6\u7684\u9ad8\u8d28\u91cf\u548cUniMC\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u906e\u6321\u548c\u591a\u7c7b\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "UniMC\u548cHAIG-2.9M\u4e3a\u5173\u952e\u70b9\u5f15\u5bfc\u7684\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.02714", "pdf": "https://arxiv.org/pdf/2507.02714", "abs": "https://arxiv.org/abs/2507.02714", "authors": ["Yuxuan Wang", "Tianwei Cao", "Huayu Zhang", "Zhongjiang He", "Kongming Liang", "Zhanyu Ma"], "title": "FairHuman: Boosting Hand and Face Quality in Human Image Generation with Minimum Potential Delay Fairness in Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV 2025", "summary": "Image generation has achieved remarkable progress with the development of large-scale text-to-image models, especially diffusion-based models. However, generating human images with plausible details, such as faces or hands, remains challenging due to insufficient supervision of local regions during training. To address this issue, we propose FairHuman, a multi-objective fine-tuning approach designed to enhance both global and local generation quality fairly. Specifically, we first construct three learning objectives: a global objective derived from the default diffusion objective function and two local objectives for hands and faces based on pre-annotated positional priors. Subsequently, we derive the optimal parameter updating strategy under the guidance of the Minimum Potential Delay (MPD) criterion, thereby attaining fairness-ware optimization for this multi-objective problem. Based on this, our proposed method can achieve significant improvements in generating challenging local details while maintaining overall quality. Extensive experiments showcase the effectiveness of our method in improving the performance of human image generation under different scenarios.", "AI": {"tldr": "FairHuman\u901a\u8fc7\u591a\u76ee\u6807\u5fae\u8c03\u65b9\u6cd5\u63d0\u5347\u56fe\u50cf\u751f\u6210\u8d28\u91cf\uff0c\u7279\u522b\u5173\u6ce8\u5c40\u90e8\u7ec6\u8282\u5982\u624b\u548c\u8138\uff0c\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u751f\u6210\u4eba\u7c7b\u56fe\u50cf\u65f6\uff0c\u5c40\u90e8\u7ec6\u8282\uff08\u5982\u8138\u548c\u624b\uff09\u7684\u751f\u6210\u8d28\u91cf\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u8bad\u7ec3\u76d1\u7763\u3002", "method": "\u63d0\u51faFairHuman\u65b9\u6cd5\uff0c\u7ed3\u5408\u5168\u5c40\u76ee\u6807\u548c\u4e24\u4e2a\u5c40\u90e8\u76ee\u6807\uff08\u624b\u548c\u8138\uff09\uff0c\u57fa\u4e8eMPD\u51c6\u5219\u4f18\u5316\u53c2\u6570\u66f4\u65b0\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5c40\u90e8\u7ec6\u8282\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6574\u4f53\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "FairHuman\u901a\u8fc7\u516c\u5e73\u4f18\u5316\u591a\u76ee\u6807\u95ee\u9898\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4eba\u7c7b\u56fe\u50cf\u751f\u6210\u7684\u5c40\u90e8\u7ec6\u8282\u8d28\u91cf\u3002"}}
{"id": "2507.02790", "pdf": "https://arxiv.org/pdf/2507.02790", "abs": "https://arxiv.org/abs/2507.02790", "authors": ["Xiangfeng Wang", "Xiao Li", "Yadong Wei", "Xueyu Song", "Yang Song", "Xiaoqiang Xia", "Fangrui Zeng", "Zaiyi Chen", "Liu Liu", "Gu Xu", "Tong Xu"], "title": "From Long Videos to Engaging Clips: A Human-Inspired Video Editing Framework with Multimodal Narrative Understanding", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "The rapid growth of online video content, especially on short video platforms, has created a growing demand for efficient video editing techniques that can condense long-form videos into concise and engaging clips. Existing automatic editing methods predominantly rely on textual cues from ASR transcripts and end-to-end segment selection, often neglecting the rich visual context and leading to incoherent outputs. In this paper, we propose a human-inspired automatic video editing framework (HIVE) that leverages multimodal narrative understanding to address these limitations. Our approach incorporates character extraction, dialogue analysis, and narrative summarization through multimodal large language models, enabling a holistic understanding of the video content. To further enhance coherence, we apply scene-level segmentation and decompose the editing process into three subtasks: highlight detection, opening/ending selection, and pruning of irrelevant content. To facilitate research in this area, we introduce DramaAD, a novel benchmark dataset comprising over 800 short drama episodes and 500 professionally edited advertisement clips. Experimental results demonstrate that our framework consistently outperforms existing baselines across both general and advertisement-oriented editing tasks, significantly narrowing the quality gap between automatic and human-edited videos.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u53d9\u4e8b\u7406\u89e3\u7684\u81ea\u52a8\u89c6\u9891\u7f16\u8f91\u6846\u67b6\uff08HIVE\uff09\uff0c\u901a\u8fc7\u89d2\u8272\u63d0\u53d6\u3001\u5bf9\u8bdd\u5206\u6790\u548c\u53d9\u4e8b\u6458\u8981\u63d0\u5347\u89c6\u9891\u7f16\u8f91\u8d28\u91cf\uff0c\u5e76\u5728\u65b0\u6570\u636e\u96c6DramaAD\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u7ebf\u7d22\uff0c\u5ffd\u89c6\u89c6\u89c9\u4e0a\u4e0b\u6587\uff0c\u5bfc\u81f4\u8f93\u51fa\u4e0d\u8fde\u8d2f\u3002", "method": "\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u89d2\u8272\u63d0\u53d6\u3001\u5bf9\u8bdd\u5206\u6790\u548c\u53d9\u4e8b\u6458\u8981\uff0c\u5e76\u5206\u4e09\u6b65\u5b8c\u6210\u7f16\u8f91\u4efb\u52a1\uff1a\u9ad8\u5149\u68c0\u6d4b\u3001\u5f00\u5934/\u7ed3\u5c3e\u9009\u62e9\u548c\u65e0\u5173\u5185\u5bb9\u4fee\u526a\u3002", "result": "\u5728DramaAD\u6570\u636e\u96c6\u4e0a\uff0cHIVE\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u7f29\u5c0f\u4e86\u81ea\u52a8\u4e0e\u4eba\u5de5\u7f16\u8f91\u89c6\u9891\u7684\u8d28\u91cf\u5dee\u8ddd\u3002", "conclusion": "HIVE\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u53d9\u4e8b\u7406\u89e3\u63d0\u5347\u4e86\u81ea\u52a8\u89c6\u9891\u7f16\u8f91\u7684\u8fde\u8d2f\u6027\u548c\u8d28\u91cf\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.02792", "pdf": "https://arxiv.org/pdf/2507.02792", "abs": "https://arxiv.org/abs/2507.02792", "authors": ["Liheng Zhang", "Lexi Pang", "Hang Ye", "Xiaoxuan Ma", "Yizhou Wang"], "title": "RichControl: Structure- and Appearance-Rich Training-Free Spatial Control for Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image (T2I) diffusion models have shown remarkable success in generating high-quality images from text prompts. Recent efforts extend these models to incorporate conditional images (e.g., depth or pose maps) for fine-grained spatial control. Among them, feature injection methods have emerged as a training-free alternative to traditional fine-tuning approaches. However, they often suffer from structural misalignment, condition leakage, and visual artifacts, especially when the condition image diverges significantly from natural RGB distributions. By revisiting existing methods, we identify a core limitation: the synchronous injection of condition features fails to account for the trade-off between domain alignment and structural preservation during denoising. Inspired by this observation, we propose a flexible feature injection framework that decouples the injection timestep from the denoising process. At its core is a structure-rich injection module, which enables the model to better adapt to the evolving interplay between alignment and structure preservation throughout the diffusion steps, resulting in more faithful structural generation. In addition, we introduce appearance-rich prompting and a restart refinement strategy to further enhance appearance control and visual quality. Together, these designs enable training-free generation that is both structure-rich and appearance-rich. Extensive experiments show that our approach achieves state-of-the-art performance across diverse zero-shot conditioning scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7279\u5f81\u6ce8\u5165\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u7ed3\u6784\u9519\u4f4d\u548c\u89c6\u89c9\u4f2a\u5f71\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7279\u5f81\u6ce8\u5165\u65b9\u6cd5\u5728\u6761\u4ef6\u56fe\u50cf\u4e0e\u81ea\u7136RGB\u5206\u5e03\u5dee\u5f02\u8f83\u5927\u65f6\uff0c\u5b58\u5728\u7ed3\u6784\u9519\u4f4d\u3001\u6761\u4ef6\u6cc4\u6f0f\u548c\u89c6\u89c9\u4f2a\u5f71\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u6ce8\u5165\u65f6\u95f4\u6b65\u4e0e\u53bb\u566a\u8fc7\u7a0b\u7684\u6846\u67b6\uff0c\u5305\u542b\u7ed3\u6784\u4e30\u5bcc\u7684\u6ce8\u5165\u6a21\u5757\u3001\u5916\u89c2\u4e30\u5bcc\u7684\u63d0\u793a\u548c\u91cd\u542f\u7ec6\u5316\u7b56\u7565\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u96f6\u6837\u672c\u6761\u4ef6\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u7ed3\u6784\u4e30\u5bcc\u4e14\u5916\u89c2\u4e30\u5bcc\u7684\u751f\u6210\u6548\u679c\u3002"}}
{"id": "2507.02813", "pdf": "https://arxiv.org/pdf/2507.02813", "abs": "https://arxiv.org/abs/2507.02813", "authors": ["Fangfu Liu", "Hao Li", "Jiawei Chi", "Hanyang Wang", "Minghui Yang", "Fudong Wang", "Yueqi Duan"], "title": "LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion", "categories": ["cs.CV"], "comment": "Project page: https://liuff19.github.io/LangScene-X", "summary": "Recovering 3D structures with open-vocabulary scene understanding from 2D images is a fundamental but daunting task. Recent developments have achieved this by performing per-scene optimization with embedded language information. However, they heavily rely on the calibrated dense-view reconstruction paradigm, thereby suffering from severe rendering artifacts and implausible semantic synthesis when limited views are available. In this paper, we introduce a novel generative framework, coined LangScene-X, to unify and generate 3D consistent multi-modality information for reconstruction and understanding. Powered by the generative capability of creating more consistent novel observations, we can build generalizable 3D language-embedded scenes from only sparse views. Specifically, we first train a TriMap video diffusion model that can generate appearance (RGBs), geometry (normals), and semantics (segmentation maps) from sparse inputs through progressive knowledge integration. Furthermore, we propose a Language Quantized Compressor (LQC), trained on large-scale image datasets, to efficiently encode language embeddings, enabling cross-scene generalization without per-scene retraining. Finally, we reconstruct the language surface fields by aligning language information onto the surface of 3D scenes, enabling open-ended language queries. Extensive experiments on real-world data demonstrate the superiority of our LangScene-X over state-of-the-art methods in terms of quality and generalizability. Project Page: https://liuff19.github.io/LangScene-X.", "AI": {"tldr": "LangScene-X\u662f\u4e00\u79cd\u65b0\u578b\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u89c6\u56fe\u751f\u62103D\u4e00\u81f4\u7684\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u7528\u4e8e\u91cd\u5efa\u548c\u7406\u89e3\u573a\u666f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u6709\u9650\u89c6\u56fe\u4e0b\u7684\u6e32\u67d3\u548c\u8bed\u4e49\u5408\u6210\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5bc6\u96c6\u89c6\u56fe\u91cd\u5efa\uff0c\u5728\u7a00\u758f\u89c6\u56fe\u4e0b\u5b58\u5728\u6e32\u67d3\u4f2a\u5f71\u548c\u8bed\u4e49\u5408\u6210\u4e0d\u51c6\u786e\u7684\u95ee\u9898\uff0cLangScene-X\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408TriMap\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210RGB\u3001\u51e0\u4f55\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u5e76\u5f15\u5165\u8bed\u8a00\u91cf\u5316\u538b\u7f29\u5668\uff08LQC\uff09\u7f16\u7801\u8bed\u8a00\u5d4c\u5165\uff0c\u6700\u7ec8\u901a\u8fc7\u8bed\u8a00\u8868\u9762\u573a\u5bf9\u9f50\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u67e5\u8be2\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLangScene-X\u5728\u8d28\u91cf\u548c\u6cdb\u5316\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "LangScene-X\u901a\u8fc7\u751f\u6210\u4e00\u81f4\u7684\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u7a00\u758f\u89c6\u56fe\u4e0b\u7684\u9ad8\u8d28\u91cf3D\u91cd\u5efa\u548c\u573a\u666f\u7406\u89e3\u3002"}}
{"id": "2507.02827", "pdf": "https://arxiv.org/pdf/2507.02827", "abs": "https://arxiv.org/abs/2507.02827", "authors": ["Ying Yu", "Hang Xiao", "Siyao Li", "Jiarui Li", "Haotian Tang", "Hanyu Liu", "Chao Li"], "title": "USAD: An Unsupervised Data Augmentation Spatio-Temporal Attention Diffusion Network", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The primary objective of human activity recognition (HAR) is to infer ongoing human actions from sensor data, a task that finds broad applications in health monitoring, safety protection, and sports analysis. Despite proliferating research, HAR still faces key challenges, including the scarcity of labeled samples for rare activities, insufficient extraction of high-level features, and suboptimal model performance on lightweight devices. To address these issues, this paper proposes a comprehensive optimization approach centered on multi-attention interaction mechanisms. First, an unsupervised, statistics-guided diffusion model is employed to perform data augmentation, thereby alleviating the problems of labeled data scarcity and severe class imbalance. Second, a multi-branch spatio-temporal interaction network is designed, which captures multi-scale features of sequential data through parallel residual branches with 3*3, 5*5, and 7*7 convolutional kernels. Simultaneously, temporal attention mechanisms are incorporated to identify critical time points, while spatial attention enhances inter-sensor interactions. A cross-branch feature fusion unit is further introduced to improve the overall feature representation capability. Finally, an adaptive multi-loss function fusion strategy is integrated, allowing for dynamic adjustment of loss weights and overall model optimization. Experimental results on three public datasets, WISDM, PAMAP2, and OPPORTUNITY, demonstrate that the proposed unsupervised data augmentation spatio-temporal attention diffusion network (USAD) achieves accuracies of 98.84%, 93.81%, and 80.92% respectively, significantly outperforming existing approaches. Furthermore, practical deployment on embedded devices verifies the efficiency and feasibility of the proposed method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6ce8\u610f\u529b\u4ea4\u4e92\u673a\u5236\u7684\u7efc\u5408\u4f18\u5316\u65b9\u6cd5\uff08USAD\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u4e2d\u7684\u6807\u7b7e\u6570\u636e\u7a00\u7f3a\u3001\u7279\u5f81\u63d0\u53d6\u4e0d\u8db3\u548c\u8bbe\u5907\u6027\u80fd\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff08HAR\uff09\u9762\u4e34\u6807\u7b7e\u6570\u636e\u7a00\u7f3a\u3001\u9ad8\u7ea7\u7279\u5f81\u63d0\u53d6\u4e0d\u8db3\u548c\u8f7b\u91cf\u8bbe\u5907\u6027\u80fd\u4e0d\u4f73\u7b49\u6311\u6218\uff0c\u4e9f\u9700\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u65e0\u76d1\u7763\u7edf\u8ba1\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u8fdb\u884c\u6570\u636e\u589e\u5f3a\uff1b\u8bbe\u8ba1\u591a\u5206\u652f\u65f6\u7a7a\u4ea4\u4e92\u7f51\u7edc\u6355\u6349\u591a\u5c3a\u5ea6\u7279\u5f81\uff1b\u5f15\u5165\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\u548c\u8de8\u5206\u652f\u7279\u5f81\u878d\u5408\u5355\u5143\uff1b\u96c6\u6210\u81ea\u9002\u5e94\u591a\u635f\u5931\u51fd\u6570\u878d\u5408\u7b56\u7565\u3002", "result": "\u5728WISDM\u3001PAMAP2\u548cOPPORTUNITY\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523098.84%\u300193.81%\u548c80.92%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\u3002", "conclusion": "USAD\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86HAR\u7684\u5173\u952e\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5e76\u5177\u5907\u5b9e\u9645\u90e8\u7f72\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.02844", "pdf": "https://arxiv.org/pdf/2507.02844", "abs": "https://arxiv.org/abs/2507.02844", "authors": ["Ziqi Miao", "Yi Ding", "Lijun Li", "Jing Shao"], "title": "Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection", "categories": ["cs.CV", "cs.CL", "cs.CR"], "comment": "16 pages", "summary": "With the emergence of strong visual-language capabilities, multimodal large language models (MLLMs) have demonstrated tremendous potential for real-world applications. However, the security vulnerabilities exhibited by the visual modality pose significant challenges to deploying such models in open-world environments. Recent studies have successfully induced harmful responses from target MLLMs by encoding harmful textual semantics directly into visual inputs. However, in these approaches, the visual modality primarily serves as a trigger for unsafe behavior, often exhibiting semantic ambiguity and lacking grounding in realistic scenarios. In this work, we define a novel setting: visual-centric jailbreak, where visual information serves as a necessary component in constructing a complete and realistic jailbreak context. Building on this setting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates contextual dialogue using four distinct visual-focused strategies, dynamically generating auxiliary images when necessary to construct a visual-centric jailbreak scenario. To maximize attack effectiveness, it incorporates automatic toxicity obfuscation and semantic refinement to produce a final attack prompt that reliably triggers harmful responses from the target black-box MLLMs. Specifically, VisCo achieves a toxicity score of 4.78 and an Attack Success Rate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming the baseline, which performs a toxicity score of 2.48 and an ASR of 22.2%. The code is available at https://github.com/Dtc7w3PQ/Visco-Attack.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u89c6\u89c9\u4e2d\u5fc3\u8d8a\u72f1\u653b\u51fb\uff08VisCo Attack\uff09\uff0c\u901a\u8fc7\u89c6\u89c9\u4fe1\u606f\u6784\u5efa\u5b8c\u6574\u7684\u8d8a\u72f1\u573a\u666f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u653b\u51fb\u6548\u679c\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5f00\u653e\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\u95ee\u9898\u4e9f\u5f85\u89e3\u51b3\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u89c6\u89c9\u89e6\u53d1\uff0c\u7f3a\u4e4f\u73b0\u5b9e\u573a\u666f\u7684\u8bed\u4e49\u57fa\u7840\u3002", "method": "\u63d0\u51faVisCo\u653b\u51fb\uff0c\u91c7\u7528\u56db\u79cd\u89c6\u89c9\u7b56\u7565\u6784\u5efa\u4e0a\u4e0b\u6587\u5bf9\u8bdd\uff0c\u52a8\u6001\u751f\u6210\u8f85\u52a9\u56fe\u50cf\uff0c\u5e76\u7ed3\u5408\u6bd2\u6027\u6a21\u7cca\u548c\u8bed\u4e49\u4f18\u5316\u751f\u6210\u653b\u51fb\u63d0\u793a\u3002", "result": "VisCo\u5728MM-SafetyBench\u4e0a\u5bf9GPT-4o\u7684\u6bd2\u6027\u8bc4\u5206\u4e3a4.78\uff0c\u653b\u51fb\u6210\u529f\u7387\u4e3a85%\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "VisCo Attack\u4e3a\u89c6\u89c9\u4e2d\u5fc3\u8d8a\u72f1\u653b\u51fb\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u9a8c\u8bc1\u4e86\u89c6\u89c9\u4fe1\u606f\u5728\u5b89\u5168\u6f0f\u6d1e\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2507.02857", "pdf": "https://arxiv.org/pdf/2507.02857", "abs": "https://arxiv.org/abs/2507.02857", "authors": ["Ziye Li", "Hao Luo", "Xincheng Shuai", "Henghui Ding"], "title": "AnyI2V: Animating Any Conditional Image with Motion Control", "categories": ["cs.CV"], "comment": "ICCV 2025, Project Page: https://henghuiding.com/AnyI2V/", "summary": "Recent advancements in video generation, particularly in diffusion models, have driven notable progress in text-to-video (T2V) and image-to-video (I2V) synthesis. However, challenges remain in effectively integrating dynamic motion signals and flexible spatial constraints. Existing T2V methods typically rely on text prompts, which inherently lack precise control over the spatial layout of generated content. In contrast, I2V methods are limited by their dependence on real images, which restricts the editability of the synthesized content. Although some methods incorporate ControlNet to introduce image-based conditioning, they often lack explicit motion control and require computationally expensive training. To address these limitations, we propose AnyI2V, a training-free framework that animates any conditional images with user-defined motion trajectories. AnyI2V supports a broader range of modalities as the conditional image, including data types such as meshes and point clouds that are not supported by ControlNet, enabling more flexible and versatile video generation. Additionally, it supports mixed conditional inputs and enables style transfer and editing via LoRA and text prompts. Extensive experiments demonstrate that the proposed AnyI2V achieves superior performance and provides a new perspective in spatial- and motion-controlled video generation. Code is available at https://henghuiding.com/AnyI2V/.", "AI": {"tldr": "AnyI2V\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7528\u6237\u5b9a\u4e49\u7684\u8fd0\u52a8\u8f68\u8ff9\u4e3a\u4efb\u610f\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u52a8\u753b\uff0c\u652f\u6301\u591a\u79cd\u6a21\u6001\u8f93\u5165\uff0c\u5e76\u5b9e\u73b0\u98ce\u683c\u8fc1\u79fb\u548c\u7f16\u8f91\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709T2V\u548cI2V\u65b9\u6cd5\u5728\u52a8\u6001\u8fd0\u52a8\u4fe1\u53f7\u548c\u7a7a\u95f4\u7ea6\u675f\u6574\u5408\u4e0a\u7684\u4e0d\u8db3\uff0c\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u89c6\u9891\u751f\u6210\u63a7\u5236\u3002", "method": "\u63d0\u51faAnyI2V\u6846\u67b6\uff0c\u652f\u6301\u591a\u79cd\u6761\u4ef6\u56fe\u50cf\u6a21\u6001\uff08\u5982\u7f51\u683c\u548c\u70b9\u4e91\uff09\uff0c\u7ed3\u5408LoRA\u548c\u6587\u672c\u63d0\u793a\u5b9e\u73b0\u98ce\u683c\u8fc1\u79fb\u548c\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAnyI2V\u5728\u7a7a\u95f4\u548c\u8fd0\u52a8\u63a7\u5236\u7684\u89c6\u9891\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "AnyI2V\u4e3a\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u652f\u6301\u66f4\u7075\u6d3b\u7684\u63a7\u5236\u548c\u7f16\u8f91\u3002"}}
{"id": "2507.02860", "pdf": "https://arxiv.org/pdf/2507.02860", "abs": "https://arxiv.org/abs/2507.02860", "authors": ["Xin Zhou", "Dingkang Liang", "Kaijin Chen", "Tianrui Feng", "Xiwu Chen", "Hongkai Lin", "Yikang Ding", "Feiyang Tan", "Hengshuang Zhao", "Xiang Bai"], "title": "Less is Enough: Training-Free Video Diffusion Acceleration via Runtime-Adaptive Caching", "categories": ["cs.CV"], "comment": "The code is made available at   https://github.com/H-EmbodVis/EasyCache. Project page:   https://h-embodvis.github.io/EasyCache/", "summary": "Video generation models have demonstrated remarkable performance, yet their broader adoption remains constrained by slow inference speeds and substantial computational costs, primarily due to the iterative nature of the denoising process. Addressing this bottleneck is essential for democratizing advanced video synthesis technologies and enabling their integration into real-world applications. This work proposes EasyCache, a training-free acceleration framework for video diffusion models. EasyCache introduces a lightweight, runtime-adaptive caching mechanism that dynamically reuses previously computed transformation vectors, avoiding redundant computations during inference. Unlike prior approaches, EasyCache requires no offline profiling, pre-computation, or extensive parameter tuning. We conduct comprehensive studies on various large-scale video generation models, including OpenSora, Wan2.1, and HunyuanVideo. Our method achieves leading acceleration performance, reducing inference time by up to 2.1-3.3$\\times$ compared to the original baselines while maintaining high visual fidelity with a significant up to 36% PSNR improvement compared to the previous SOTA method. This improvement makes our EasyCache a efficient and highly accessible solution for high-quality video generation in both research and practical applications. The code is available at https://github.com/H-EmbodVis/EasyCache.", "AI": {"tldr": "EasyCache\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u91cd\u7528\u8ba1\u7b97\u8fc7\u7684\u53d8\u6362\u5411\u91cf\uff0c\u663e\u8457\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u5e76\u4fdd\u6301\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u6162\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002EasyCache\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63a8\u52a8\u9ad8\u8d28\u91cf\u89c6\u9891\u5408\u6210\u6280\u672f\u7684\u666e\u53ca\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u8fd0\u884c\u65f6\u81ea\u9002\u5e94\u7684\u7f13\u5b58\u673a\u5236\uff0c\u52a8\u6001\u91cd\u7528\u5148\u524d\u8ba1\u7b97\u7684\u53d8\u6362\u5411\u91cf\uff0c\u907f\u514d\u5197\u4f59\u8ba1\u7b97\u3002\u65e0\u9700\u79bb\u7ebf\u5206\u6790\u3001\u9884\u8ba1\u7b97\u6216\u5927\u91cf\u53c2\u6570\u8c03\u6574\u3002", "result": "\u5728\u591a\u4e2a\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e0a\u6d4b\u8bd5\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c112.1-3.3\u500d\uff0cPSNR\u63d0\u5347\u9ad8\u8fbe36%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "EasyCache\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6613\u4e8e\u4f7f\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2507.02862", "pdf": "https://arxiv.org/pdf/2507.02862", "abs": "https://arxiv.org/abs/2507.02862", "authors": ["Xiang Fan", "Xiaohang Sun", "Kushan Thakkar", "Zhu Liu", "Vimal Bhat", "Ranjay Krishna", "Xiang Hao"], "title": "RefTok: Reference-Based Tokenization for Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Effectively handling temporal redundancy remains a key challenge in learning video models. Prevailing approaches often treat each set of frames independently, failing to effectively capture the temporal dependencies and redundancies inherent in videos. To address this limitation, we introduce RefTok, a novel reference-based tokenization method capable of capturing complex temporal dynamics and contextual information. Our method encodes and decodes sets of frames conditioned on an unquantized reference frame. When decoded, RefTok preserves the continuity of motion and the appearance of objects across frames. For example, RefTok retains facial details despite head motion, reconstructs text correctly, preserves small patterns, and maintains the legibility of handwriting from the context. Across 4 video datasets (K600, UCF-101, BAIR Robot Pushing, and DAVIS), RefTok significantly outperforms current state-of-the-art tokenizers (Cosmos and MAGVIT) and improves all evaluated metrics (PSNR, SSIM, LPIPS) by an average of 36.7% at the same or higher compression ratios. When a video generation model is trained using RefTok's latents on the BAIR Robot Pushing task, the generations not only outperform MAGVIT-B but the larger MAGVIT-L, which has 4x more parameters, across all generation metrics by an average of 27.9%.", "AI": {"tldr": "RefTok\u662f\u4e00\u79cd\u57fa\u4e8e\u53c2\u8003\u7684\u6807\u8bb0\u5316\u65b9\u6cd5\uff0c\u6709\u6548\u6355\u6349\u89c6\u9891\u4e2d\u7684\u65f6\u95f4\u52a8\u6001\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u72ec\u7acb\u5904\u7406\u5e27\u96c6\uff0c\u672a\u80fd\u6709\u6548\u6355\u6349\u89c6\u9891\u4e2d\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u5197\u4f59\u6027\u3002", "method": "\u5f15\u5165RefTok\uff0c\u901a\u8fc7\u672a\u91cf\u5316\u7684\u53c2\u8003\u5e27\u7f16\u7801\u548c\u89e3\u7801\u5e27\u96c6\uff0c\u4fdd\u7559\u8fd0\u52a8\u548c\u5bf9\u8c61\u5916\u89c2\u7684\u8fde\u7eed\u6027\u3002", "result": "\u57284\u4e2a\u89c6\u9891\u6570\u636e\u96c6\u4e0a\uff0cRefTok\u5e73\u5747\u63d0\u534736.7%\u7684\u6307\u6807\uff0c\u5e76\u5728\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u4f18\u4e8e\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u3002", "conclusion": "RefTok\u5728\u89c6\u9891\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u52a8\u6001\u6355\u6349\u548c\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2507.02092", "pdf": "https://arxiv.org/pdf/2507.02092", "abs": "https://arxiv.org/abs/2507.02092", "authors": ["Alexi Gladstone", "Ganesh Nanduru", "Md Mofijul Islam", "Peixuan Han", "Hyeonjeong Ha", "Aman Chadha", "Yilun Du", "Heng Ji", "Jundong Li", "Tariq Iqbal"], "title": "Energy-Based Transformers are Scalable Learners and Thinkers", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Inference-time computation techniques, analogous to human System 2 Thinking, have recently become popular for improving model performances. However, most existing approaches suffer from several limitations: they are modality-specific (e.g., working only in text), problem-specific (e.g., verifiable domains like math and coding), or require additional supervision/training on top of unsupervised pretraining (e.g., verifiers or verifiable rewards). In this paper, we ask the question \"Is it possible to generalize these System 2 Thinking approaches, and develop models that learn to think solely from unsupervised learning?\" Interestingly, we find the answer is yes, by learning to explicitly verify the compatibility between inputs and candidate-predictions, and then re-framing prediction problems as optimization with respect to this verifier. Specifically, we train Energy-Based Transformers (EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy value to every input and candidate-prediction pair, enabling predictions through gradient descent-based energy minimization until convergence. Across both discrete (text) and continuous (visual) modalities, we find EBTs scale faster than the dominant Transformer++ approach during training, achieving an up to 35% higher scaling rate with respect to data, batch size, parameters, FLOPs, and depth. During inference, EBTs improve performance with System 2 Thinking by 29% more than the Transformer++ on language tasks, and EBTs outperform Diffusion Transformers on image denoising while using fewer forward passes. Further, we find that EBTs achieve better results than existing models on most downstream tasks given the same or worse pretraining performance, suggesting that EBTs generalize better than existing approaches. Consequently, EBTs are a promising new paradigm for scaling both the learning and thinking capabilities of models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684Energy-Based Transformers\uff08EBTs\uff09\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u5b9e\u73b0\u7c7b\u4f3c\u4eba\u7c7bSystem 2 Thinking\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u65b9\u6cd5\u5b58\u5728\u6a21\u6001\u6216\u95ee\u9898\u7279\u5f02\u6027\uff0c\u6216\u9700\u8981\u989d\u5916\u76d1\u7763\u8bad\u7ec3\u3002\u672c\u6587\u63a2\u8ba8\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u5b9e\u73b0\u901a\u7528\u7684System 2 Thinking\u3002", "method": "\u8bad\u7ec3EBTs\u4e3a\u8f93\u5165\u548c\u5019\u9009\u9884\u6d4b\u5bf9\u5206\u914d\u80fd\u91cf\u503c\uff0c\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u80fd\u91cf\u6700\u5c0f\u5316\u8fdb\u884c\u9884\u6d4b\u3002", "result": "EBTs\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u6570\u636e\u3001\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u7387\u66f4\u9ad8\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "conclusion": "EBTs\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b0\u8303\u5f0f\uff0c\u53ef\u6269\u5c55\u6a21\u578b\u7684\u5b66\u4e60\u548c\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2507.02129", "pdf": "https://arxiv.org/pdf/2507.02129", "abs": "https://arxiv.org/abs/2507.02129", "authors": ["Xiao Li", "Liangji Zhu", "Anand Rangarajan", "Sanjay Ranka"], "title": "Generative Latent Diffusion for Efficient Spatiotemporal Data Reduction", "categories": ["cs.LG", "cs.CV"], "comment": "10 pages", "summary": "Generative models have demonstrated strong performance in conditional settings and can be viewed as a form of data compression, where the condition serves as a compact representation. However, their limited controllability and reconstruction accuracy restrict their practical application to data compression. In this work, we propose an efficient latent diffusion framework that bridges this gap by combining a variational autoencoder with a conditional diffusion model. Our method compresses only a small number of keyframes into latent space and uses them as conditioning inputs to reconstruct the remaining frames via generative interpolation, eliminating the need to store latent representations for every frame. This approach enables accurate spatiotemporal reconstruction while significantly reducing storage costs. Experimental results across multiple datasets show that our method achieves up to 10 times higher compression ratios than rule-based state-of-the-art compressors such as SZ3, and up to 63 percent better performance than leading learning-based methods under the same reconstruction error.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u9ad8\u6548\u6f5c\u5728\u6269\u6563\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u538b\u7f29\u7684\u6027\u80fd\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u5728\u6761\u4ef6\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u53ef\u63a7\u6027\u548c\u91cd\u5efa\u7cbe\u5ea6\u4e0d\u8db3\u9650\u5236\u4e86\u5176\u5728\u6570\u636e\u538b\u7f29\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u53d8\u5206\u81ea\u7f16\u7801\u5668\u538b\u7f29\u5c11\u91cf\u5173\u952e\u5e27\u5230\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\uff0c\u5229\u7528\u751f\u6210\u63d2\u503c\u91cd\u5efa\u5176\u4f59\u5e27\uff0c\u907f\u514d\u5b58\u50a8\u6bcf\u5e27\u7684\u6f5c\u5728\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u538b\u7f29\u6bd4\u4f18\u4e8eSZ3\u7b49\u89c4\u5219\u538b\u7f29\u566810\u500d\uff0c\u91cd\u5efa\u8bef\u5dee\u4e0b\u6027\u80fd\u9886\u5148\u5b66\u4e60\u578b\u65b9\u6cd563%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u663e\u8457\u964d\u4f4e\u5b58\u50a8\u6210\u672c\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u65f6\u7a7a\u91cd\u5efa\u3002"}}
{"id": "2507.02302", "pdf": "https://arxiv.org/pdf/2507.02302", "abs": "https://arxiv.org/abs/2507.02302", "authors": ["Dohoon Kim", "Donghun Kang", "Taesup Moon"], "title": "DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "22 pages, 5 figures, ACL 2025 Main", "summary": "Domain-Adaptive Pre-training (DAP) has recently gained attention for its effectiveness in fine-tuning pre-trained models. Building on this, continual DAP has been explored to develop pre-trained models capable of incrementally incorporating different domain datasets. However, existing continual DAP methods face several limitations: (1) high computational cost and GPU memory usage during training; (2) sensitivity to incremental data order; and (3) providing a single, generalized model for all end tasks, which contradicts the essence of DAP. In this paper, we propose DoMIX, a novel approach that addresses these challenges by leveraging LoRA modules, a representative parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient and parallel domain-adaptive pre-training that is robust to domain order and effectively utilizes accumulated knowledge to provide tailored pre-trained models for specific tasks. We also demonstrate that our method can be extended beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available at https://github.com/dohoonkim-ai/DoMIX.", "AI": {"tldr": "DoMIX\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLoRA\u6a21\u5757\u7684\u9ad8\u6548\u3001\u5e76\u884c\u4e14\u5bf9\u9886\u57df\u987a\u5e8f\u9c81\u68d2\u7684\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u9886\u57df\u987a\u5e8f\u654f\u611f\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6301\u7eed\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u9886\u57df\u987a\u5e8f\u654f\u611f\u4ee5\u53ca\u65e0\u6cd5\u4e3a\u7279\u5b9a\u4efb\u52a1\u63d0\u4f9b\u5b9a\u5236\u5316\u6a21\u578b\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528LoRA\u6a21\u5757\uff08\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff09\u5b9e\u73b0\u9ad8\u6548\u5e76\u884c\u8bad\u7ec3\uff0c\u540c\u65f6\u5bf9\u9886\u57df\u987a\u5e8f\u9c81\u68d2\uff0c\u5e76\u80fd\u79ef\u7d2f\u77e5\u8bc6\u4e3a\u7279\u5b9a\u4efb\u52a1\u63d0\u4f9b\u5b9a\u5236\u5316\u6a21\u578b\u3002", "result": "DoMIX\u5728\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u53ef\u6269\u5c55\u5230\u6807\u51c6LLM\u5fae\u8c03\u573a\u666f\u3002", "conclusion": "DoMIX\u4e3a\u6301\u7eed\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u66f4\u5e7f\u6cdb\u573a\u666f\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.02619", "pdf": "https://arxiv.org/pdf/2507.02619", "abs": "https://arxiv.org/abs/2507.02619", "authors": ["Hazal Mogultay Ozcan", "Sinan Kalkan", "Fatos T. Yarman-Vural"], "title": "L-VAE: Variational Auto-Encoder with Learnable Beta for Disentangled Representation", "categories": ["cs.LG", "cs.CV"], "comment": "The paper is under revision at Machine Vision and Applications", "summary": "In this paper, we propose a novel model called Learnable VAE (L-VAE), which learns a disentangled representation together with the hyperparameters of the cost function. L-VAE can be considered as an extension of \\b{eta}-VAE, wherein the hyperparameter, \\b{eta}, is empirically adjusted. L-VAE mitigates the limitations of \\b{eta}-VAE by learning the relative weights of the terms in the loss function to control the dynamic trade-off between disentanglement and reconstruction losses. In the proposed model, the weight of the loss terms and the parameters of the model architecture are learned concurrently. An additional regularization term is added to the loss function to prevent bias towards either reconstruction or disentanglement losses. Experimental analyses show that the proposed L-VAE finds an effective balance between reconstruction fidelity and disentangling the latent dimensions. Comparisons of the proposed L-VAE against \\b{eta}-VAE, VAE, ControlVAE, DynamicVAE, and {\\sigma}-VAE on datasets, such as dSprites, MPI3D-complex, Falcor3D, and Isaac3D reveals that L-VAE consistently provides the best or the second best performances measured by a set of disentanglement metrics. Moreover, qualitative experiments on CelebA dataset, confirm the success of the L-VAE model for disentangling the facial attributes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aL-VAE\u7684\u65b0\u6a21\u578b\uff0c\u901a\u8fc7\u5b66\u4e60\u635f\u5931\u51fd\u6570\u7684\u8d85\u53c2\u6570\u548c\u6a21\u578b\u53c2\u6570\uff0c\u52a8\u6001\u5e73\u8861\u89e3\u8026\u548c\u91cd\u5efa\u635f\u5931\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u03b2-VAE\u4e2d\u8d85\u53c2\u6570\u03b7\u9700\u624b\u52a8\u8c03\u6574\u7684\u95ee\u9898\uff0c\u52a8\u6001\u4f18\u5316\u635f\u5931\u51fd\u6570\u7684\u6743\u91cd\u3002", "method": "L-VAE\u540c\u65f6\u5b66\u4e60\u635f\u5931\u9879\u6743\u91cd\u548c\u6a21\u578b\u53c2\u6570\uff0c\u5e76\u6dfb\u52a0\u6b63\u5219\u5316\u9879\u4ee5\u907f\u514d\u504f\u5411\u91cd\u5efa\u6216\u89e3\u8026\u635f\u5931\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u6216\u63a5\u8fd1\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9a\u6027\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u4eba\u8138\u5c5e\u6027\u89e3\u8026\u4e0a\u7684\u6210\u529f\u3002", "conclusion": "L-VAE\u6709\u6548\u5e73\u8861\u4e86\u89e3\u8026\u548c\u91cd\u5efa\u6027\u80fd\uff0c\u4e3a\u53d8\u5206\u81ea\u7f16\u7801\u5668\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u52a8\u6001\u4f18\u5316\u65b9\u6848\u3002"}}
{"id": "2507.02671", "pdf": "https://arxiv.org/pdf/2507.02671", "abs": "https://arxiv.org/abs/2507.02671", "authors": ["Francesco Di Salvo", "Hanh Huyen My Nguyen", "Christian Ledig"], "title": "Embedding-Based Federated Data Sharing via Differentially Private Conditional VAEs", "categories": ["cs.LG", "cs.CV", "eess.IV"], "comment": "Accepted to MICCAI 2025", "summary": "Deep Learning (DL) has revolutionized medical imaging, yet its adoption is constrained by data scarcity and privacy regulations, limiting access to diverse datasets. Federated Learning (FL) enables decentralized training but suffers from high communication costs and is often restricted to a single downstream task, reducing flexibility. We propose a data-sharing method via Differentially Private (DP) generative models. By adopting foundation models, we extract compact, informative embeddings, reducing redundancy and lowering computational overhead. Clients collaboratively train a Differentially Private Conditional Variational Autoencoder (DP-CVAE) to model a global, privacy-aware data distribution, supporting diverse downstream tasks. Our approach, validated across multiple feature extractors, enhances privacy, scalability, and efficiency, outperforming traditional FL classifiers while ensuring differential privacy. Additionally, DP-CVAE produces higher-fidelity embeddings than DP-CGAN while requiring $5{\\times}$ fewer parameters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5dee\u5206\u9690\u79c1\u751f\u6210\u6a21\u578b\u7684\u6570\u636e\u5171\u4eab\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u7d27\u51d1\u5d4c\u5165\uff0c\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u652f\u6301\u591a\u6837\u5316\u4e0b\u6e38\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u9690\u79c1\u95ee\u9898\uff0c\u540c\u65f6\u514b\u670d\u8054\u90a6\u5b66\u4e60\u7684\u9ad8\u901a\u4fe1\u6210\u672c\u548c\u5355\u4e00\u4efb\u52a1\u9650\u5236\u3002", "method": "\u91c7\u7528\u5dee\u5206\u9690\u79c1\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08DP-CVAE\uff09\u5efa\u6a21\u5168\u5c40\u9690\u79c1\u611f\u77e5\u6570\u636e\u5206\u5e03\uff0c\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u5d4c\u5165\u3002", "result": "\u65b9\u6cd5\u5728\u9690\u79c1\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u5206\u7c7b\u5668\uff0c\u4e14\u751f\u6210\u7684\u5d4c\u5165\u8d28\u91cf\u9ad8\u4e8eDP-CGAN\u3002", "conclusion": "DP-CVAE\u65b9\u6cd5\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\uff0c\u63d0\u5347\u4e86\u6570\u636e\u5171\u4eab\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u4efb\u52a1\u3002"}}
