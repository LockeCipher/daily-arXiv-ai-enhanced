{"id": "2508.11203", "pdf": "https://arxiv.org/pdf/2508.11203", "abs": "https://arxiv.org/abs/2508.11203", "authors": ["Seungmi Lee", "Kwan Yun", "Junyong Noh"], "title": "StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.MM", "51-04", "I.3.8; I.4.9"], "comment": "Pacific graphics 2025, CGF, 15 pages", "summary": "We introduce StyleMM, a novel framework that can construct a stylized 3D Morphable Model (3DMM) based on user-defined text descriptions specifying a target style. Building upon a pre-trained mesh deformation network and a texture generator for original 3DMM-based realistic human faces, our approach fine-tunes these models using stylized facial images generated via text-guided image-to-image (i2i) translation with a diffusion model, which serve as stylization targets for the rendered mesh. To prevent undesired changes in identity, facial alignment, or expressions during i2i translation, we introduce a stylization method that explicitly preserves the facial attributes of the source image. By maintaining these critical attributes during image stylization, the proposed approach ensures consistent 3D style transfer across the 3DMM parameter space through image-based training. Once trained, StyleMM enables feed-forward generation of stylized face meshes with explicit control over shape, expression, and texture parameters, producing meshes with consistent vertex connectivity and animatability. Quantitative and qualitative evaluations demonstrate that our approach outperforms state-of-the-art methods in terms of identity-level facial diversity and stylization capability. The code and videos are available at [kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).", "AI": {"tldr": "StyleMM\u662f\u4e00\u4e2a\u57fa\u4e8e\u6587\u672c\u63cf\u8ff0\u76843D\u53ef\u53d8\u5f62\u6a21\u578b\uff083DMM\uff09\u98ce\u683c\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u98ce\u683c\u5316\u56fe\u50cf\u5e76\u5fae\u8c03\u9884\u8bad\u7ec3\u7f51\u7edc\uff0c\u5b9e\u73b03D\u98ce\u683c\u8f6c\u6362\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u57283D\u98ce\u683c\u8f6c\u6362\u4e2d\u96be\u4ee5\u4fdd\u6301\u9762\u90e8\u5c5e\u6027\uff08\u5982\u8eab\u4efd\u3001\u5bf9\u9f50\u548c\u8868\u60c5\uff09\u7684\u4e00\u81f4\u6027\uff0cStyleMM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u7f51\u683c\u53d8\u5f62\u7f51\u7edc\u548c\u7eb9\u7406\u751f\u6210\u5668\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u98ce\u683c\u5316\u56fe\u50cf\u4f5c\u4e3a\u76ee\u6807\uff0c\u901a\u8fc7\u56fe\u50cf\u8bad\u7ec3\u5b9e\u73b03D\u98ce\u683c\u8f6c\u6362\u3002", "result": "StyleMM\u5728\u8eab\u4efd\u591a\u6837\u6027\u548c\u98ce\u683c\u5316\u80fd\u529b\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u5bf9\u5f62\u72b6\u3001\u8868\u60c5\u548c\u7eb9\u7406\u53c2\u6570\u7684\u663e\u5f0f\u63a7\u5236\u3002", "conclusion": "StyleMM\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u76843D\u98ce\u683c\u8f6c\u6362\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u751f\u6210\u98ce\u683c\u5316\u4e14\u53ef\u52a8\u753b\u76843D\u4eba\u8138\u6a21\u578b\u3002"}}
{"id": "2508.11476", "pdf": "https://arxiv.org/pdf/2508.11476", "abs": "https://arxiv.org/abs/2508.11476", "authors": ["Qian Liang", "Zichong Chen", "Yang Zhou", "Hui Huang"], "title": "SPG: Style-Prompting Guidance for Style-Specific Content Creation", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted to the Journal track of Pacific Graphics 2025", "summary": "Although recent text-to-image (T2I) diffusion models excel at aligning generated images with textual prompts, controlling the visual style of the output remains a challenging task. In this work, we propose Style-Prompting Guidance (SPG), a novel sampling strategy for style-specific image generation. SPG constructs a style noise vector and leverages its directional deviation from unconditional noise to guide the diffusion process toward the target style distribution. By integrating SPG with Classifier-Free Guidance (CFG), our method achieves both semantic fidelity and style consistency. SPG is simple, robust, and compatible with controllable frameworks like ControlNet and IPAdapter, making it practical and widely applicable. Extensive experiments demonstrate the effectiveness and generality of our approach compared to state-of-the-art methods. Code is available at https://github.com/Rumbling281441/SPG.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStyle-Prompting Guidance (SPG)\u7684\u65b0\u91c7\u6837\u7b56\u7565\uff0c\u7528\u4e8e\u98ce\u683c\u7279\u5b9a\u7684\u56fe\u50cf\u751f\u6210\uff0c\u7ed3\u5408Classifier-Free Guidance (CFG)\u5b9e\u73b0\u8bed\u4e49\u4fdd\u771f\u548c\u98ce\u683c\u4e00\u81f4\u6027\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u4e0e\u6587\u672c\u63d0\u793a\u5bf9\u9f50\u7684\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u63a7\u5236\u8f93\u51fa\u56fe\u50cf\u7684\u89c6\u89c9\u98ce\u683c\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "SPG\u6784\u5efa\u98ce\u683c\u566a\u58f0\u5411\u91cf\uff0c\u5e76\u5229\u7528\u5176\u4e0e\u65e0\u6761\u4ef6\u566a\u58f0\u7684\u65b9\u5411\u504f\u5dee\u5f15\u5bfc\u6269\u6563\u8fc7\u7a0b\u671d\u5411\u76ee\u6807\u98ce\u683c\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSPG\u5728\u98ce\u683c\u751f\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u517c\u5bb9ControlNet\u548cIPAdapter\u7b49\u53ef\u63a7\u6846\u67b6\u3002", "conclusion": "SPG\u662f\u4e00\u79cd\u7b80\u5355\u3001\u9c81\u68d2\u4e14\u5e7f\u6cdb\u9002\u7528\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u98ce\u683c\u4e00\u81f4\u7684\u56fe\u50cf\u751f\u6210\u3002"}}
{"id": "2508.10931", "pdf": "https://arxiv.org/pdf/2508.10931", "abs": "https://arxiv.org/abs/2508.10931", "authors": ["Wenqi Guo", "Shan Du"], "title": "VSF: Simple, Efficient, and Effective Negative Guidance in Few-Step Image Generation Models By \\underline{V}alue \\underline{S}ign \\underline{F}lip", "categories": ["cs.CV"], "comment": null, "summary": "We introduce Value Sign Flip (VSF), a simple and efficient method for incorporating negative prompt guidance in few-step diffusion and flow-matching image generation models. Unlike existing approaches such as classifier-free guidance (CFG), NASA, and NAG, VSF dynamically suppresses undesired content by flipping the sign of attention values from negative prompts. Our method requires only small computational overhead and integrates effectively with MMDiT-style architectures such as Stable Diffusion 3.5 Turbo, as well as cross-attention-based models like Wan. We validate VSF on challenging datasets with complex prompt pairs and demonstrate superior performance in both static image and video generation tasks. Experimental results show that VSF significantly improves negative prompt adherence compared to prior methods in few-step models, and even CFG in non-few-step models, while maintaining competitive image quality. Code and ComfyUI node are available in https://github.com/weathon/VSF/tree/main.", "AI": {"tldr": "VSF\u662f\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ffb\u8f6c\u8d1f\u63d0\u793a\u7684\u6ce8\u610f\u529b\u503c\u7b26\u53f7\uff0c\u52a8\u6001\u6291\u5236\u4e0d\u60f3\u8981\u7684\u5185\u5bb9\uff0c\u9002\u7528\u4e8e\u5c11\u6b65\u6269\u6563\u548c\u6d41\u5339\u914d\u56fe\u50cf\u751f\u6210\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982CFG\u3001NASA\u548cNAG\u5728\u8d1f\u63d0\u793a\u5f15\u5bfc\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cVSF\u65e8\u5728\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "VSF\u901a\u8fc7\u7ffb\u8f6c\u8d1f\u63d0\u793a\u7684\u6ce8\u610f\u529b\u503c\u7b26\u53f7\uff0c\u52a8\u6001\u6291\u5236\u4e0d\u60f3\u8981\u7684\u5185\u5bb9\uff0c\u8ba1\u7b97\u5f00\u9500\u5c0f\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVSF\u5728\u5c11\u6b65\u6a21\u578b\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5728\u975e\u5c11\u6b65\u6a21\u578b\u4e2d\u4e5f\u80fd\u4e0eCFG\u7ade\u4e89\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "VSF\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u8d1f\u63d0\u793a\u5f15\u5bfc\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u751f\u6210\u4efb\u52a1\u3002"}}
{"id": "2508.10935", "pdf": "https://arxiv.org/pdf/2508.10935", "abs": "https://arxiv.org/abs/2508.10935", "authors": ["Qi Liu", "Yabei Li", "Hongsong Wang", "Lei He"], "title": "HQ-OV3D: A High Box Quality Open-World 3D Detection Framework based on Diffision Model", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Traditional closed-set 3D detection frameworks fail to meet the demands of open-world applications like autonomous driving. Existing open-vocabulary 3D detection methods typically adopt a two-stage pipeline consisting of pseudo-label generation followed by semantic alignment. While vision-language models (VLMs) recently have dramatically improved the semantic accuracy of pseudo-labels, their geometric quality, particularly bounding box precision, remains commonly neglected.To address this issue, we propose a High Box Quality Open-Vocabulary 3D Detection (HQ-OV3D) framework, dedicated to generate and refine high-quality pseudo-labels for open-vocabulary classes. The framework comprises two key components: an Intra-Modality Cross-Validated (IMCV) Proposal Generator that utilizes cross-modality geometric consistency to generate high-quality initial 3D proposals, and an Annotated-Class Assisted (ACA) Denoiser that progressively refines 3D proposals by leveraging geometric priors from annotated categories through a DDIM-based denoising mechanism.Compared to the state-of-the-art method, training with pseudo-labels generated by our approach achieves a 7.37% improvement in mAP on novel classes, demonstrating the superior quality of the pseudo-labels produced by our framework. HQ-OV3D can serve not only as a strong standalone open-vocabulary 3D detector but also as a plug-in high-quality pseudo-label generator for existing open-vocabulary detection or annotation pipelines.", "AI": {"tldr": "\u63d0\u51faHQ-OV3D\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u751f\u6210\u548c\u4f18\u5316\uff0c\u63d0\u5347\u5f00\u653e\u8bcd\u6c473D\u68c0\u6d4b\u7684\u51e0\u4f55\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u5c01\u95ed\u96c63D\u68c0\u6d4b\u65e0\u6cd5\u6ee1\u8db3\u5f00\u653e\u4e16\u754c\u5e94\u7528\u9700\u6c42\uff0c\u73b0\u6709\u5f00\u653e\u8bcd\u6c47\u65b9\u6cd5\u5ffd\u89c6\u51e0\u4f55\u8d28\u91cf\u3002", "method": "\u63d0\u51faIMCV\u63d0\u6848\u751f\u6210\u5668\u548cACA\u53bb\u566a\u5668\uff0c\u5229\u7528\u8de8\u6a21\u6001\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u6807\u6ce8\u7c7b\u51e0\u4f55\u5148\u9a8c\u4f18\u5316\u4f2a\u6807\u7b7e\u3002", "result": "\u5728\u65b0\u578b\u7c7b\u4e0amAP\u63d0\u53477.37%\uff0c\u4f2a\u6807\u7b7e\u8d28\u91cf\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HQ-OV3D\u53ef\u4f5c\u4e3a\u72ec\u7acb\u68c0\u6d4b\u5668\u6216\u63d2\u4ef6\u4f2a\u6807\u7b7e\u751f\u6210\u5668\uff0c\u63d0\u5347\u5f00\u653e\u8bcd\u6c473D\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2508.10936", "pdf": "https://arxiv.org/pdf/2508.10936", "abs": "https://arxiv.org/abs/2508.10936", "authors": ["Cheng Chen", "Hao Huang", "Saurabh Bagchi"], "title": "Vision-Only Gaussian Splatting for Collaborative Semantic Occupancy Prediction", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Collaborative perception enables connected vehicles to share information, overcoming occlusions and extending the limited sensing range inherent in single-agent (non-collaborative) systems. Existing vision-only methods for 3D semantic occupancy prediction commonly rely on dense 3D voxels, which incur high communication costs, or 2D planar features, which require accurate depth estimation or additional supervision, limiting their applicability to collaborative scenarios. To address these challenges, we propose the first approach leveraging sparse 3D semantic Gaussian splatting for collaborative 3D semantic occupancy prediction. By sharing and fusing intermediate Gaussian primitives, our method provides three benefits: a neighborhood-based cross-agent fusion that removes duplicates and suppresses noisy or inconsistent Gaussians; a joint encoding of geometry and semantics in each primitive, which reduces reliance on depth supervision and allows simple rigid alignment; and sparse, object-centric messages that preserve structural information while reducing communication volume. Extensive experiments demonstrate that our approach outperforms single-agent perception and baseline collaborative methods by +8.42 and +3.28 points in mIoU, and +5.11 and +22.41 points in IoU, respectively. When further reducing the number of transmitted Gaussians, our method still achieves a +1.9 improvement in mIoU, using only 34.6% communication volume, highlighting robust performance under limited communication budgets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f3D\u8bed\u4e49\u9ad8\u65af\u6cfc\u6e85\u7684\u534f\u4f5c3D\u8bed\u4e49\u5360\u636e\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5171\u4eab\u548c\u878d\u5408\u9ad8\u65af\u57fa\u5143\uff0c\u964d\u4f4e\u4e86\u901a\u4fe1\u6210\u672c\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u89c9\u65b9\u6cd5\u5728\u534f\u4f5c\u611f\u77e5\u4e2d\u56e0\u5bc6\u96c63D\u4f53\u7d20\u62162D\u5e73\u9762\u7279\u5f81\u5bfc\u81f4\u7684\u9ad8\u901a\u4fe1\u6210\u672c\u6216\u4f9d\u8d56\u6df1\u5ea6\u4f30\u8ba1\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u7a00\u758f3D\u8bed\u4e49\u9ad8\u65af\u6cfc\u6e85\uff0c\u901a\u8fc7\u5171\u4eab\u548c\u878d\u5408\u9ad8\u65af\u57fa\u5143\uff0c\u5b9e\u73b0\u8de8\u4ee3\u7406\u878d\u5408\u3001\u51e0\u4f55\u4e0e\u8bed\u4e49\u8054\u5408\u7f16\u7801\u53ca\u7a00\u758f\u5bf9\u8c61\u4e2d\u5fc3\u6d88\u606f\u4f20\u9012\u3002", "result": "\u5728mIoU\u548cIoU\u6307\u6807\u4e0a\u5206\u522b\u6bd4\u5355\u4ee3\u7406\u548c\u57fa\u7ebf\u534f\u4f5c\u65b9\u6cd5\u63d0\u53478.42/3.28\u548c5.11/22.41\u5206\uff0c\u901a\u4fe1\u91cf\u51cf\u5c11\u81f334.6%\u65f6\u4ecd\u4fdd\u6301\u6027\u80fd\u4f18\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u534f\u4f5c\u611f\u77e5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u901a\u4fe1\u6210\u672c\uff0c\u9002\u7528\u4e8e\u6709\u9650\u901a\u4fe1\u9884\u7b97\u7684\u573a\u666f\u3002"}}
{"id": "2508.10937", "pdf": "https://arxiv.org/pdf/2508.10937", "abs": "https://arxiv.org/abs/2508.10937", "authors": ["Jiarui Yang", "Hang Guo", "Wen Huang", "Tao Dai", "Shutao Xia"], "title": "Personalized Face Super-Resolution with Identity Decoupling and Fitting", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, face super-resolution (FSR) methods have achieved remarkable progress, generally maintaining high image fidelity and identity (ID) consistency under standard settings. However, in extreme degradation scenarios (e.g., scale $> 8\\times$), critical attributes and ID information are often severely lost in the input image, making it difficult for conventional models to reconstruct realistic and ID-consistent faces. Existing methods tend to generate hallucinated faces under such conditions, producing restored images lacking authentic ID constraints. To address this challenge, we propose a novel FSR method with Identity Decoupling and Fitting (IDFSR), designed to enhance ID restoration under large scaling factors while mitigating hallucination effects. Our approach involves three key designs: 1) \\textbf{Masking} the facial region in the low-resolution (LR) image to eliminate unreliable ID cues; 2) \\textbf{Warping} a reference image to align with the LR input, providing style guidance; 3) Leveraging \\textbf{ID embeddings} extracted from ground truth (GT) images for fine-grained ID modeling and personalized adaptation. We first pretrain a diffusion-based model to explicitly decouple style and ID by forcing it to reconstruct masked LR face regions using both style and identity embeddings. Subsequently, we freeze most network parameters and perform lightweight fine-tuning of the ID embedding using a small set of target ID images. This embedding encodes fine-grained facial attributes and precise ID information, significantly improving both ID consistency and perceptual quality. Extensive quantitative evaluations and visual comparisons demonstrate that the proposed IDFSR substantially outperforms existing approaches under extreme degradation, particularly achieving superior performance on ID consistency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9762\u90e8\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5IDFSR\uff0c\u901a\u8fc7\u8eab\u4efd\u89e3\u8026\u548c\u62df\u5408\uff0c\u5728\u6781\u7aef\u9000\u5316\u573a\u666f\u4e0b\u63d0\u5347\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6781\u7aef\u9000\u5316\uff08\u59828\u500d\u4ee5\u4e0a\u7f29\u653e\uff09\u4e0b\u5bb9\u6613\u751f\u6210\u7f3a\u4e4f\u771f\u5b9e\u8eab\u4efd\u7ea6\u675f\u7684\u5e7b\u89c9\u9762\u90e8\uff0cIDFSR\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "1\uff09\u63a9\u853d\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u7684\u9762\u90e8\u533a\u57df\uff1b2\uff09\u5bf9\u9f50\u53c2\u8003\u56fe\u50cf\u4ee5\u63d0\u4f9b\u98ce\u683c\u6307\u5bfc\uff1b3\uff09\u5229\u7528\u771f\u5b9e\u56fe\u50cf\u7684ID\u5d4c\u5165\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5efa\u6a21\u3002", "result": "IDFSR\u5728\u6781\u7aef\u9000\u5316\u6761\u4ef6\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u8eab\u4efd\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "IDFSR\u901a\u8fc7\u8eab\u4efd\u89e3\u8026\u548c\u62df\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6781\u7aef\u9000\u5316\u573a\u666f\u4e0b\u7684\u9762\u90e8\u8d85\u5206\u8fa8\u7387\u6027\u80fd\u3002"}}
{"id": "2508.10963", "pdf": "https://arxiv.org/pdf/2508.10963", "abs": "https://arxiv.org/abs/2508.10963", "authors": ["Zixiang Yang", "Yue Ma", "Yinhan Zhang", "Shanhui Mo", "Dongrui Liu", "Linfeng Zhang"], "title": "EVCtrl: Efficient Control Adapter for Visual Generation", "categories": ["cs.CV"], "comment": null, "summary": "Visual generation includes both image and video generation, training probabilistic models to create coherent, diverse, and semantically faithful content from scratch. While early research focused on unconditional sampling, practitioners now demand controllable generation that allows precise specification of layout, pose, motion, or style. While ControlNet grants precise spatial-temporal control, its auxiliary branch markedly increases latency and introduces redundant computation in both uncontrolled regions and denoising steps, especially for video. To address this problem, we introduce EVCtrl, a lightweight, plug-and-play control adapter that slashes overhead without retraining the model. Specifically, we propose a spatio-temporal dual caching strategy for sparse control information. For spatial redundancy, we first profile how each layer of DiT-ControlNet responds to fine-grained control, then partition the network into global and local functional zones. A locality-aware cache focuses computation on the local zones that truly need the control signal, skipping the bulk of redundant computation in global regions. For temporal redundancy, we selectively omit unnecessary denoising steps to improve efficiency. Extensive experiments on CogVideo-Controlnet, Wan2.1-Controlnet, and Flux demonstrate that our method is effective in image and video control generation without the need for training. For example, it achieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and Wan2.1-Controlnet, respectively, with almost no degradation in generation quality.Codes are available in the supplementary materials.", "AI": {"tldr": "EVCtrl\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u63a7\u5236\u9002\u914d\u5668\uff0c\u901a\u8fc7\u65f6\u7a7a\u53cc\u7f13\u5b58\u7b56\u7565\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3ControlNet\u5728\u89c6\u9891\u751f\u6210\u4e2d\u56e0\u5197\u4f59\u8ba1\u7b97\u5bfc\u81f4\u7684\u9ad8\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u65f6\u7a7a\u53cc\u7f13\u5b58\u7b56\u7565\uff1a\u7a7a\u95f4\u4e0a\u5206\u533a\u5904\u7406\u63a7\u5236\u4fe1\u53f7\uff0c\u65f6\u95f4\u4e0a\u9009\u62e9\u6027\u8df3\u8fc7\u5197\u4f59\u53bb\u566a\u6b65\u9aa4\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u5b9e\u73b02\u500d\u4ee5\u4e0a\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u751f\u6210\u8d28\u91cf\u51e0\u4e4e\u65e0\u635f\u5931\u3002", "conclusion": "EVCtrl\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u56fe\u50cf\u548c\u89c6\u9891\u7684\u7cbe\u786e\u63a7\u5236\u751f\u6210\u3002"}}
{"id": "2508.11106", "pdf": "https://arxiv.org/pdf/2508.11106", "abs": "https://arxiv.org/abs/2508.11106", "authors": ["Xinjie Gao", "Bi'an Du", "Wei Hu"], "title": "HierOctFusion: Multi-scale Octree-based 3D Shape Generation via Part-Whole-Hierarchy Message Passing", "categories": ["cs.CV"], "comment": null, "summary": "3D content generation remains a fundamental yet challenging task due to the inherent structural complexity of 3D data. While recent octree-based diffusion models offer a promising balance between efficiency and quality through hierarchical generation, they often overlook two key insights: 1) existing methods typically model 3D objects as holistic entities, ignoring their semantic part hierarchies and limiting generalization; and 2) holistic high-resolution modeling is computationally expensive, whereas real-world objects are inherently sparse and hierarchical, making them well-suited for layered generation. Motivated by these observations, we propose HierOctFusion, a part-aware multi-scale octree diffusion model that enhances hierarchical feature interaction for generating fine-grained and sparse object structures. Furthermore, we introduce a cross-attention conditioning mechanism that injects part-level information into the generation process, enabling semantic features to propagate effectively across hierarchical levels from parts to the whole. Additionally, we construct a 3D dataset with part category annotations using a pre-trained segmentation model to facilitate training and evaluation. Experiments demonstrate that HierOctFusion achieves superior shape quality and efficiency compared to prior methods.", "AI": {"tldr": "HierOctFusion\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u516b\u53c9\u6811\u7684\u5206\u5c42\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u4ea4\u4e92\u548c\u8bed\u4e49\u90e8\u5206\u4fe1\u606f\u6ce8\u5165\uff0c\u63d0\u5347\u4e863D\u5185\u5bb9\u751f\u6210\u7684\u7cbe\u7ec6\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c063D\u5bf9\u8c61\u89c6\u4e3a\u6574\u4f53\uff0c\u5ffd\u7565\u4e86\u8bed\u4e49\u90e8\u5206\u5c42\u6b21\u7ed3\u6784\uff0c\u4e14\u9ad8\u5206\u8fa8\u7387\u5efa\u6a21\u8ba1\u7b97\u6210\u672c\u9ad8\u3002HierOctFusion\u65e8\u5728\u901a\u8fc7\u5206\u5c42\u751f\u6210\u548c\u90e8\u5206\u611f\u77e5\u5efa\u6a21\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u5c3a\u5ea6\u516b\u53c9\u6811\u6269\u6563\u6a21\u578b\uff0c\u5f15\u5165\u8de8\u6ce8\u610f\u529b\u673a\u5236\u6ce8\u5165\u90e8\u5206\u7ea7\u4fe1\u606f\uff0c\u5e76\u6784\u5efa\u5e26\u6709\u90e8\u5206\u7c7b\u522b\u6807\u6ce8\u76843D\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHierOctFusion\u5728\u5f62\u72b6\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HierOctFusion\u901a\u8fc7\u5206\u5c42\u548c\u90e8\u5206\u611f\u77e5\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u5185\u5bb9\u751f\u6210\u7684\u6027\u80fd\u3002"}}
{"id": "2508.11134", "pdf": "https://arxiv.org/pdf/2508.11134", "abs": "https://arxiv.org/abs/2508.11134", "authors": ["Bing Liu", "Le Wang", "Hao Liu", "Mingming Liu"], "title": "Residual-based Efficient Bidirectional Diffusion Model for Image Dehazing and Haze Generation", "categories": ["cs.CV"], "comment": "7 pages, 5 figures, 2025 ICME Accepted", "summary": "Current deep dehazing methods only focus on removing haze from hazy images, lacking the capability to translate between hazy and haze-free images. To address this issue, we propose a residual-based efficient bidirectional diffusion model (RBDM) that can model the conditional distributions for both dehazing and haze generation. Firstly, we devise dual Markov chains that can effectively shift the residuals and facilitate bidirectional smooth transitions between them. Secondly, the RBDM perturbs the hazy and haze-free images at individual timesteps and predicts the noise in the perturbed data to simultaneously learn the conditional distributions. Finally, to enhance performance on relatively small datasets and reduce computational costs, our method introduces a unified score function learned on image patches instead of entire images. Our RBDM successfully implements size-agnostic bidirectional transitions between haze-free and hazy images with only 15 sampling steps. Extensive experiments demonstrate that the proposed method achieves superior or at least comparable performance to state-of-the-art methods on both synthetic and real-world datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b8b\u5dee\u7684\u9ad8\u6548\u53cc\u5411\u6269\u6563\u6a21\u578b\uff08RBDM\uff09\uff0c\u7528\u4e8e\u5b9e\u73b0\u6709\u96fe\u548c\u65e0\u96fe\u56fe\u50cf\u4e4b\u95f4\u7684\u53cc\u5411\u8f6c\u6362\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u53bb\u96fe\u65b9\u6cd5\u4ec5\u5173\u6ce8\u53bb\u96fe\uff0c\u7f3a\u4e4f\u6709\u96fe\u548c\u65e0\u96fe\u56fe\u50cf\u4e4b\u95f4\u7684\u53cc\u5411\u8f6c\u6362\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u53cc\u9a6c\u5c14\u53ef\u592b\u94fe\u4ee5\u5e73\u6ed1\u8f6c\u6362\u6b8b\u5dee\uff0c\u901a\u8fc7\u6270\u52a8\u56fe\u50cf\u5e76\u9884\u6d4b\u566a\u58f0\u5b66\u4e60\u6761\u4ef6\u5206\u5e03\uff0c\u5f15\u5165\u7edf\u4e00\u8bc4\u5206\u51fd\u6570\u4ee5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u6216\u81f3\u5c11\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\uff0c\u4ec5\u970015\u6b65\u91c7\u6837\u3002", "conclusion": "RBDM\u6210\u529f\u5b9e\u73b0\u4e86\u6709\u96fe\u548c\u65e0\u96fe\u56fe\u50cf\u4e4b\u95f4\u7684\u53cc\u5411\u8f6c\u6362\uff0c\u4e14\u9ad8\u6548\u4f4e\u8017\u3002"}}
{"id": "2508.11153", "pdf": "https://arxiv.org/pdf/2508.11153", "abs": "https://arxiv.org/abs/2508.11153", "authors": ["Maoquan Zhang", "Bisser Raytchev", "Xiujuan Sun"], "title": "LEARN: A Story-Driven Layout-to-Image Generation Framework for STEM Instruction", "categories": ["cs.CV"], "comment": "The International Conference on Neural Information Processing   (ICONIP) 2025", "summary": "LEARN is a layout-aware diffusion framework designed to generate pedagogically aligned illustrations for STEM education. It leverages a curated BookCover dataset that provides narrative layouts and structured visual cues, enabling the model to depict abstract and sequential scientific concepts with strong semantic alignment. Through layout-conditioned generation, contrastive visual-semantic training, and prompt modulation, LEARN produces coherent visual sequences that support mid-to-high-level reasoning in line with Bloom's taxonomy while reducing extraneous cognitive load as emphasized by Cognitive Load Theory. By fostering spatially organized and story-driven narratives, the framework counters fragmented attention often induced by short-form media and promotes sustained conceptual focus. Beyond static diagrams, LEARN demonstrates potential for integration with multimodal systems and curriculum-linked knowledge graphs to create adaptive, exploratory educational content. As the first generative approach to unify layout-based storytelling, semantic structure learning, and cognitive scaffolding, LEARN represents a novel direction for generative AI in education. The code and dataset will be released to facilitate future research and practical deployment.", "AI": {"tldr": "LEARN\u662f\u4e00\u4e2a\u5e03\u5c40\u611f\u77e5\u7684\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u4e0eSTEM\u6559\u80b2\u6559\u5b66\u5bf9\u9f50\u7684\u63d2\u56fe\uff0c\u901a\u8fc7\u5e03\u5c40\u6761\u4ef6\u751f\u6210\u548c\u89c6\u89c9\u8bed\u4e49\u8bad\u7ec3\uff0c\u652f\u6301\u4e2d\u9ad8\u5c42\u6b21\u7684\u63a8\u7406\u3002", "motivation": "\u89e3\u51b3STEM\u6559\u80b2\u4e2d\u62bd\u8c61\u79d1\u5b66\u6982\u5ff5\u7684\u89c6\u89c9\u8868\u8fbe\u95ee\u9898\uff0c\u51cf\u5c11\u8ba4\u77e5\u8d1f\u8377\uff0c\u63d0\u5347\u5b66\u4e60\u6548\u679c\u3002", "method": "\u5229\u7528BookCover\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u5e03\u5c40\u6761\u4ef6\u751f\u6210\u3001\u5bf9\u6bd4\u89c6\u89c9\u8bed\u4e49\u8bad\u7ec3\u548c\u63d0\u793a\u8c03\u5236\u3002", "result": "\u751f\u6210\u8fde\u8d2f\u7684\u89c6\u89c9\u5e8f\u5217\uff0c\u652f\u6301Bloom\u5206\u7c7b\u6cd5\u7684\u4e2d\u9ad8\u5c42\u6b21\u63a8\u7406\uff0c\u51cf\u5c11\u8ba4\u77e5\u8d1f\u8377\u3002", "conclusion": "LEARN\u4e3a\u6559\u80b2\u751f\u6210AI\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u672a\u6765\u53ef\u7ed3\u5408\u591a\u6a21\u6001\u7cfb\u7edf\u548c\u77e5\u8bc6\u56fe\u8c31\u3002"}}
{"id": "2508.11165", "pdf": "https://arxiv.org/pdf/2508.11165", "abs": "https://arxiv.org/abs/2508.11165", "authors": ["Bing Liu", "Le Wang", "Mingming Liu", "Hao Liu", "Rui Yao", "Yong Zhou", "Peng Liu", "Tongqiang Xia"], "title": "Semi-supervised Image Dehazing via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models", "categories": ["cs.CV"], "comment": "10 pages, 4 figures", "summary": "Existing dehazing methods deal with real-world haze images with difficulty, especially scenes with thick haze. One of the main reasons is the lack of real-world paired data and robust priors. To avoid the costly collection of paired hazy and clear images, we propose an efficient semi-supervised image dehazing method via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models (EM-B3DM) with a two-stage learning scheme. In the first stage, we employ the EM algorithm to decouple the joint distribution of paired hazy and clear images into two conditional distributions, which are then modeled using a unified Brownian Bridge diffusion model to directly capture the structural and content-related correlations between hazy and clear images. In the second stage, we leverage the pre-trained model and large-scale unpaired hazy and clear images to further improve the performance of image dehazing. Additionally, we introduce a detail-enhanced Residual Difference Convolution block (RDC) to capture gradient-level information, significantly enhancing the model's representation capability. Extensive experiments demonstrate that our EM-B3DM achieves superior or at least comparable performance to state-of-the-art methods on both synthetic and real-world datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eEM-B3DM\u7684\u534a\u76d1\u7763\u56fe\u50cf\u53bb\u96fe\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5b66\u4e60\u65b9\u6848\u89e3\u51b3\u4e86\u771f\u5b9e\u4e16\u754c\u96fe\u973e\u56fe\u50cf\u5904\u7406\u7684\u96be\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u771f\u5b9e\u4e16\u754c\u7684\u539a\u96fe\u573a\u666f\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u771f\u5b9e\u914d\u5bf9\u6570\u636e\u548c\u9c81\u68d2\u5148\u9a8c\u3002", "method": "\u91c7\u7528EM\u7b97\u6cd5\u548c\u53cc\u5411\u5e03\u6717\u6865\u6269\u6563\u6a21\u578b\uff08EM-B3DM\uff09\uff0c\u5206\u4e24\u9636\u6bb5\u5b66\u4e60\uff1a\u7b2c\u4e00\u9636\u6bb5\u7528EM\u7b97\u6cd5\u89e3\u8026\u8054\u5408\u5206\u5e03\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u5927\u89c4\u6a21\u672a\u914d\u5bf9\u6570\u636e\u63d0\u5347\u6027\u80fd\u3002\u8fd8\u5f15\u5165RDC\u5757\u589e\u5f3a\u7ec6\u8282\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cEM-B3DM\u8868\u73b0\u4f18\u4e8e\u6216\u81f3\u5c11\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "EM-B3DM\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u53bb\u96fe\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u7f3a\u4e4f\u914d\u5bf9\u6570\u636e\u7684\u573a\u666f\u3002"}}
{"id": "2508.11183", "pdf": "https://arxiv.org/pdf/2508.11183", "abs": "https://arxiv.org/abs/2508.11183", "authors": ["Zhenghao Chen", "Zicong Chen", "Lei Liu", "Yiming Wu", "Dong Xu"], "title": "Versatile Video Tokenization with Generative 2D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Video tokenization procedure is critical for a wide range of video processing tasks. Most existing approaches directly transform video into fixed-grid and patch-wise tokens, which exhibit limited versatility. Spatially, uniformly allocating a fixed number of tokens often leads to over-encoding in low-information regions. Temporally, reducing redundancy remains challenging without explicitly distinguishing between static and dynamic content. In this work, we propose the Gaussian Video Transformer (GVT), a versatile video tokenizer built upon a generative 2D Gaussian Splatting (2DGS) strategy. We first extract latent rigid features from a video clip and represent them with a set of 2D Gaussians generated by our proposed Spatio-Temporal Gaussian Embedding (STGE) mechanism in a feed-forward manner. Such generative 2D Gaussians not only enhance spatial adaptability by assigning higher (resp., lower) rendering weights to regions with higher (resp., lower) information content during rasterization, but also improve generalization by avoiding per-video optimization.To enhance the temporal versatility, we introduce a Gaussian Set Partitioning (GSP) strategy that separates the 2D Gaussians into static and dynamic sets, which explicitly model static content shared across different time-steps and dynamic content specific to each time-step, enabling a compact representation.We primarily evaluate GVT on the video reconstruction, while also assessing its performance on action recognition and compression using the UCF101, Kinetics, and DAVIS datasets. Extensive experiments demonstrate that GVT achieves a state-of-the-art video reconstruction quality, outperforms the baseline MAGVIT-v2 in action recognition, and delivers comparable compression performance.", "AI": {"tldr": "GVT\u662f\u4e00\u79cd\u57fa\u4e8e\u751f\u62102D\u9ad8\u65af\u5206\u5e03\u7684\u89c6\u9891\u6807\u8bb0\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a7a\u95f4\u81ea\u9002\u5e94\u548c\u65f6\u95f4\u5206\u79bb\u7b56\u7565\u63d0\u5347\u89c6\u9891\u5904\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6807\u8bb0\u5316\u65b9\u6cd5\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u5b58\u5728\u5197\u4f59\u548c\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faGVT\uff0c\u7ed3\u54082D\u9ad8\u65af\u5206\u5e03\u548cSTGE\u673a\u5236\uff0c\u901a\u8fc7GSP\u7b56\u7565\u5206\u79bb\u9759\u6001\u548c\u52a8\u6001\u5185\u5bb9\u3002", "result": "GVT\u5728\u89c6\u9891\u91cd\u5efa\u3001\u52a8\u4f5c\u8bc6\u522b\u548c\u538b\u7f29\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "GVT\u901a\u8fc7\u751f\u6210\u5f0f\u9ad8\u65af\u5206\u5e03\u548c\u65f6\u7a7a\u5206\u79bb\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5904\u7406\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2508.11196", "pdf": "https://arxiv.org/pdf/2508.11196", "abs": "https://arxiv.org/abs/2508.11196", "authors": ["Jiajin Guan", "Haibo Mei", "Bonan Zhang", "Dan Liu", "Yuanshuang Fu", "Yue Zhang"], "title": "UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in vision-language models (VLMs) have demonstrated strong generalization in natural image tasks. However, their performance often degrades on unmanned aerial vehicle (UAV)-based aerial imagery, which features high resolution, complex spatial semantics, and strict real-time constraints. These challenges limit the applicability of general-purpose VLMs to structured aerial reasoning tasks. To address these challenges, we propose UAV-VL-R1, a lightweight VLM explicitly designed for aerial visual reasoning. It is trained using a hybrid method that combines supervised fine-tuning (SFT) and multi-stage reinforcement learning (RL). We leverage the group relative policy optimization (GRPO) algorithm to promote structured and interpretable reasoning through rule-guided rewards and intra-group policy alignment. To support model training and evaluation, we introduce a high-resolution visual question answering dataset named HRVQA-VL, which consists of 50,019 annotated samples covering eight UAV-relevant reasoning tasks, including object counting, transportation recognition, and spatial scene inference. Experimental results show that UAV-VL-R1 achieves a 48.17% higher zero-shot accuracy than the Qwen2-VL-2B-Instruct baseline and even outperforms its 72B-scale variant, which is 36x larger, on multiple tasks. Ablation studies reveal that while SFT improves semantic alignment, it may reduce reasoning diversity in mathematical tasks. GRPO-based RL compensates for this limitation by enhancing logical flexibility and the robustness of inference. Additionally, UAV-VL-R1 requires only 3.9GB of memory under FP16 inference and can be quantized to 2.5GB with INT8, supporting real-time deployment on resource-constrained UAV platforms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8bed\u8a00\u6a21\u578bUAV-VL-R1\uff0c\u4e13\u4e3a\u65e0\u4eba\u673a\u822a\u62cd\u56fe\u50cf\u8bbe\u8ba1\uff0c\u901a\u8fc7\u6df7\u5408\u8bad\u7ec3\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u4eba\u673a\u822a\u62cd\u56fe\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u5176\u9ad8\u5206\u8fa8\u7387\u3001\u590d\u6742\u7a7a\u95f4\u8bed\u4e49\u548c\u5b9e\u65f6\u6027\u8981\u6c42\u3002", "method": "\u91c7\u7528\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u591a\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408GRPO\u7b97\u6cd5\u63d0\u5347\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\u3002", "result": "UAV-VL-R1\u5728\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u6bd4\u57fa\u7ebf\u6a21\u578b\u51c6\u786e\u7387\u63d0\u534748.17%\uff0c\u4e14\u5185\u5b58\u5360\u7528\u4f4e\uff0c\u9002\u5408\u5b9e\u65f6\u90e8\u7f72\u3002", "conclusion": "UAV-VL-R1\u5728\u822a\u62cd\u56fe\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0cGRPO\u7b97\u6cd5\u5f25\u8865\u4e86SFT\u5728\u6570\u5b66\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.11284", "pdf": "https://arxiv.org/pdf/2508.11284", "abs": "https://arxiv.org/abs/2508.11284", "authors": ["Yilin Mi", "Qixin Yan", "Zheng-Peng Duan", "Chunle Guo", "Hubery Yin", "Hao Liu", "Chen Li", "Chongyi Li"], "title": "TimeMachine: Fine-Grained Facial Age Editing with Identity Preservation", "categories": ["cs.CV"], "comment": null, "summary": "With the advancement of generative models, facial image editing has made significant progress. However, achieving fine-grained age editing while preserving personal identity remains a challenging task.In this paper, we propose TimeMachine, a novel diffusion-based framework that achieves accurate age editing while keeping identity features unchanged. To enable fine-grained age editing, we inject high-precision age information into the multi-cross attention module, which explicitly separates age-related and identity-related features. This design facilitates more accurate disentanglement of age attributes, thereby allowing precise and controllable manipulation of facial aging.Furthermore, we propose an Age Classifier Guidance (ACG) module that predicts age directly in the latent space, instead of performing denoising image reconstruction during training. By employing a lightweight module to incorporate age constraints, this design enhances age editing accuracy by modest increasing training cost. Additionally, to address the lack of large-scale, high-quality facial age datasets, we construct a HFFA dataset (High-quality Fine-grained Facial-Age dataset) which contains one million high-resolution images labeled with identity and facial attributes. Experimental results demonstrate that TimeMachine achieves state-of-the-art performance in fine-grained age editing while preserving identity consistency.", "AI": {"tldr": "TimeMachine\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u5165\u9ad8\u7cbe\u5ea6\u5e74\u9f84\u4fe1\u606f\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u5e74\u9f84\u7f16\u8f91\uff0c\u540c\u65f6\u4fdd\u6301\u8eab\u4efd\u7279\u5f81\u4e0d\u53d8\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u9762\u90e8\u56fe\u50cf\u7f16\u8f91\u4e2d\u96be\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u5e74\u9f84\u7f16\u8f91\u4e14\u4fdd\u6301\u8eab\u4efd\u7279\u5f81\u4e0d\u53d8\uff0c\u56e0\u6b64\u63d0\u51faTimeMachine\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u5206\u79bb\u5e74\u9f84\u548c\u8eab\u4efd\u7279\u5f81\uff0c\u5e76\u5f15\u5165Age Classifier Guidance\u6a21\u5757\u5728\u6f5c\u5728\u7a7a\u95f4\u9884\u6d4b\u5e74\u9f84\uff0c\u540c\u65f6\u6784\u5efaHFFA\u6570\u636e\u96c6\u652f\u6301\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTimeMachine\u5728\u7ec6\u7c92\u5ea6\u5e74\u9f84\u7f16\u8f91\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u4fdd\u6301\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "TimeMachine\u901a\u8fc7\u521b\u65b0\u7684\u6a21\u5757\u8bbe\u8ba1\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5e74\u9f84\u7f16\u8f91\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8eab\u4efd\u7279\u5f81\u3002"}}
{"id": "2508.11313", "pdf": "https://arxiv.org/pdf/2508.11313", "abs": "https://arxiv.org/abs/2508.11313", "authors": ["Weijia Liu", "Jiuxin Cao", "Bo Miao", "Zhiheng Fu", "Xuelin Zhu", "Jiawei Ge", "Bo Liu", "Mehwish Nasim", "Ajmal Mian"], "title": "Denoise-then-Retrieve: Text-Conditioned Video Denoising for Video Moment Retrieval", "categories": ["cs.CV"], "comment": "Accepted by IJCAI 2025", "summary": "Current text-driven Video Moment Retrieval (VMR) methods encode all video clips, including irrelevant ones, disrupting multimodal alignment and hindering optimization. To this end, we propose a denoise-then-retrieve paradigm that explicitly filters text-irrelevant clips from videos and then retrieves the target moment using purified multimodal representations. Following this paradigm, we introduce the Denoise-then-Retrieve Network (DRNet), comprising Text-Conditioned Denoising (TCD) and Text-Reconstruction Feedback (TRF) modules. TCD integrates cross-attention and structured state space blocks to dynamically identify noisy clips and produce a noise mask to purify multimodal video representations. TRF further distills a single query embedding from purified video representations and aligns it with the text embedding, serving as auxiliary supervision for denoising during training. Finally, we perform conditional retrieval using text embeddings on purified video representations for accurate VMR. Experiments on Charades-STA and QVHighlights demonstrate that our approach surpasses state-of-the-art methods on all metrics. Furthermore, our denoise-then-retrieve paradigm is adaptable and can be seamlessly integrated into advanced VMR models to boost performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u566a\u540e\u68c0\u7d22\u8303\u5f0f\uff08DRNet\uff09\uff0c\u901a\u8fc7\u8fc7\u6ee4\u6587\u672c\u65e0\u5173\u89c6\u9891\u7247\u6bb5\u63d0\u5347\u89c6\u9891\u65f6\u523b\u68c0\u7d22\uff08VMR\uff09\u6027\u80fd\u3002", "motivation": "\u73b0\u6709VMR\u65b9\u6cd5\u7f16\u7801\u6240\u6709\u89c6\u9891\u7247\u6bb5\uff08\u5305\u62ec\u65e0\u5173\u5185\u5bb9\uff09\uff0c\u7834\u574f\u591a\u6a21\u6001\u5bf9\u9f50\u5e76\u963b\u788d\u4f18\u5316\u3002", "method": "\u5f15\u5165DRNet\uff0c\u5305\u542b\u6587\u672c\u6761\u4ef6\u53bb\u566a\uff08TCD\uff09\u548c\u6587\u672c\u91cd\u6784\u53cd\u9988\uff08TRF\uff09\u6a21\u5757\uff0c\u52a8\u6001\u8bc6\u522b\u566a\u58f0\u7247\u6bb5\u5e76\u51c0\u5316\u591a\u6a21\u6001\u8868\u793a\u3002", "result": "\u5728Charades-STA\u548cQVHighlights\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u53bb\u566a\u540e\u68c0\u7d22\u8303\u5f0f\u53ef\u63d0\u5347VMR\u6027\u80fd\uff0c\u5e76\u517c\u5bb9\u73b0\u6709\u6a21\u578b\u3002"}}
{"id": "2508.11330", "pdf": "https://arxiv.org/pdf/2508.11330", "abs": "https://arxiv.org/abs/2508.11330", "authors": ["Yanghao Wang", "Long Chen"], "title": "Noise Matters: Optimizing Matching Noise for Diffusion Classifiers", "categories": ["cs.CV"], "comment": null, "summary": "Although today's pretrained discriminative vision-language models (e.g., CLIP) have demonstrated strong perception abilities, such as zero-shot image classification, they also suffer from the bag-of-words problem and spurious bias. To mitigate these problems, some pioneering studies leverage powerful generative models (e.g., pretrained diffusion models) to realize generalizable image classification, dubbed Diffusion Classifier (DC). Specifically, by randomly sampling a Gaussian noise, DC utilizes the differences of denoising effects with different category conditions to classify categories. Unfortunately, an inherent and notorious weakness of existing DCs is noise instability: different random sampled noises lead to significant performance changes. To achieve stable classification performance, existing DCs always ensemble the results of hundreds of sampled noises, which significantly reduces the classification speed. To this end, we firstly explore the role of noise in DC, and conclude that: there are some ``good noises'' that can relieve the instability. Meanwhile, we argue that these good noises should meet two principles: Frequency Matching and Spatial Matching. Regarding both principles, we propose a novel Noise Optimization method to learn matching (i.e., good) noise for DCs: NoOp. For frequency matching, NoOp first optimizes a dataset-specific noise: Given a dataset and a timestep t, optimize one randomly initialized parameterized noise. For Spatial Matching, NoOp trains a Meta-Network that adopts an image as input and outputs image-specific noise offset. The sum of optimized noise and noise offset will be used in DC to replace random noise. Extensive ablations on various datasets demonstrated the effectiveness of NoOp.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faNoOp\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u566a\u58f0\u89e3\u51b3\u6269\u6563\u5206\u7c7b\u5668\uff08DC\uff09\u7684\u566a\u58f0\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u63d0\u5347\u5206\u7c7b\u901f\u5ea6\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u5206\u7c7b\u5668\uff08DC\uff09\u56e0\u566a\u58f0\u4e0d\u7a33\u5b9a\u6027\u9700\u5927\u91cf\u566a\u58f0\u91c7\u6837\uff0c\u5bfc\u81f4\u5206\u7c7b\u901f\u5ea6\u6162\u3002\u7814\u7a76\u53d1\u73b0\u5b58\u5728\u201c\u597d\u566a\u58f0\u201d\u53ef\u7f13\u89e3\u6b64\u95ee\u9898\u3002", "method": "\u63d0\u51faNoOp\u65b9\u6cd5\uff0c\u901a\u8fc7\u9891\u7387\u5339\u914d\u548c\u7a7a\u95f4\u5339\u914d\u539f\u5219\u4f18\u5316\u566a\u58f0\uff1a1\uff09\u4f18\u5316\u6570\u636e\u96c6\u7279\u5b9a\u566a\u58f0\uff1b2\uff09\u8bad\u7ec3Meta-Network\u751f\u6210\u56fe\u50cf\u7279\u5b9a\u566a\u58f0\u504f\u79fb\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86NoOp\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "NoOp\u901a\u8fc7\u4f18\u5316\u566a\u58f0\u89e3\u51b3\u4e86DC\u7684\u566a\u58f0\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u7a33\u5b9a\u7684\u56fe\u50cf\u5206\u7c7b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.11334", "pdf": "https://arxiv.org/pdf/2508.11334", "abs": "https://arxiv.org/abs/2508.11334", "authors": ["Md Asgor Hossain Reaj", "Rajan Das Gupta", "Md Yeasin Rahat", "Nafiz Fahad", "Md Jawadul Hasan", "Tze Hui Liew"], "title": "GANDiff FR: Hybrid GAN Diffusion Synthesis for Causal Bias Attribution in Face Recognition", "categories": ["cs.CV"], "comment": "Accepted in ICCVDM '25", "summary": "We introduce GANDiff FR, the first synthetic framework that precisely controls demographic and environmental factors to measure, explain, and reduce bias with reproducible rigor. GANDiff FR unifies StyleGAN3-based identity-preserving generation with diffusion-based attribute control, enabling fine-grained manipulation of pose around 30 degrees, illumination (four directions), and expression (five levels) under ceteris paribus conditions. We synthesize 10,000 demographically balanced faces across five cohorts validated for realism via automated detection (98.2%) and human review (89%) to isolate and quantify bias drivers. Benchmarking ArcFace, CosFace, and AdaFace under matched operating points shows AdaFace reduces inter-group TPR disparity by 60% (2.5% vs. 6.3%), with illumination accounting for 42% of residual bias. Cross-dataset evaluation on RFW, BUPT, and CASIA WebFace confirms strong synthetic-to-real transfer (r 0.85). Despite around 20% computational overhead relative to pure GANs, GANDiff FR yields three times more attribute-conditioned variants, establishing a reproducible, regulation-aligned (EU AI Act) standard for fairness auditing. Code and data are released to support transparent, scalable bias evaluation.", "AI": {"tldr": "GANDiff FR\u662f\u4e00\u4e2a\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7cbe\u786e\u63a7\u5236\u4eba\u53e3\u7edf\u8ba1\u548c\u73af\u5883\u56e0\u7d20\u6765\u6d4b\u91cf\u3001\u89e3\u91ca\u548c\u51cf\u5c11\u504f\u89c1\uff0c\u7ed3\u5408StyleGAN3\u548c\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u5c5e\u6027\u63a7\u5236\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5408\u6210\u6570\u636e\u7684\u771f\u5b9e\u6027\u548c\u504f\u89c1\u9a71\u52a8\u56e0\u7d20\u3002", "motivation": "\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u53ef\u91cd\u590d\u3001\u53ef\u91cf\u5316\u7684\u65b9\u6cd5\u6765\u6d4b\u91cf\u548c\u51cf\u5c11\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u4e2d\u7684\u504f\u89c1\uff0c\u540c\u65f6\u6ee1\u8db3\u76d1\u7ba1\u8981\u6c42\uff08\u5982\u6b27\u76dfAI\u6cd5\u6848\uff09\u3002", "method": "\u7ed3\u5408StyleGAN3\uff08\u8eab\u4efd\u4fdd\u7559\u751f\u6210\uff09\u548c\u6269\u6563\u6a21\u578b\uff08\u5c5e\u6027\u63a7\u5236\uff09\uff0c\u751f\u621010,000\u5f20\u4eba\u53e3\u5e73\u8861\u7684\u4eba\u8138\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u68c0\u6d4b\u548c\u4eba\u5de5\u8bc4\u5ba1\u9a8c\u8bc1\u771f\u5b9e\u6027\u3002", "result": "AdaFace\u5c06\u7ec4\u95f4TPR\u5dee\u5f02\u51cf\u5c11\u4e8660%\uff0c\u5149\u7167\u56e0\u7d20\u5360\u5269\u4f59\u504f\u89c1\u768442%\u3002\u5408\u6210\u6570\u636e\u5728\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\uff08r=0.85\uff09\u3002", "conclusion": "GANDiff FR\u4e3a\u516c\u5e73\u6027\u5ba1\u8ba1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u91cd\u590d\u3001\u7b26\u5408\u76d1\u7ba1\u6807\u51c6\u7684\u6846\u67b6\uff0c\u5c3d\u7ba1\u8ba1\u7b97\u5f00\u9500\u8f83\u9ad8\uff0c\u4f46\u80fd\u751f\u6210\u66f4\u591a\u5c5e\u6027\u6761\u4ef6\u53d8\u4f53\u3002"}}
{"id": "2508.11409", "pdf": "https://arxiv.org/pdf/2508.11409", "abs": "https://arxiv.org/abs/2508.11409", "authors": ["Zhiming Liu", "Nantheera Anantrasirichai"], "title": "RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator", "categories": ["cs.CV"], "comment": null, "summary": "Atmospheric turbulence severely degrades video quality by introducing distortions such as geometric warping, blur, and temporal flickering, posing significant challenges to both visual clarity and temporal consistency. Current state-of-the-art methods are based on transformer and 3D architectures and require multi-frame input, but their large computational cost and memory usage limit real-time deployment, especially in resource-constrained scenarios. In this work, we propose RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator, designed for efficient and temporally consistent video restoration under AT conditions. RMFAT adopts a lightweight recurrent framework that restores each frame using only two inputs at a time, significantly reducing temporal window size and computational burden. It further integrates multi-scale feature encoding and decoding with temporal warping modules at both encoder and decoder stages to enhance spatial detail and temporal coherence. Extensive experiments on synthetic and real-world atmospheric turbulence datasets demonstrate that RMFAT not only outperforms existing methods in terms of clarity restoration (with nearly a 9\\% improvement in SSIM) but also achieves significantly improved inference speed (more than a fourfold reduction in runtime), making it particularly suitable for real-time atmospheric turbulence suppression tasks.", "AI": {"tldr": "RMFAT\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5faa\u73af\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u6062\u590d\u53d7\u5927\u6c14\u6e4d\u6d41\u5f71\u54cd\u7684\u89c6\u9891\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u8d1f\u62c5\u5e76\u63d0\u5347\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u5927\u6c14\u6e4d\u6d41\u5bfc\u81f4\u89c6\u9891\u8d28\u91cf\u4e0b\u964d\uff0c\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5b9e\u65f6\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u5faa\u73af\u6846\u67b6\uff0c\u4ec5\u9700\u4e24\u5e27\u8f93\u5165\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u7f16\u7801\u548c\u89e3\u7801\u4ee5\u53ca\u65f6\u95f4\u626d\u66f2\u6a21\u5757\u3002", "result": "\u5728\u6e05\u6670\u5ea6\u6062\u590d\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08SSIM\u63d0\u53479%\uff0c\u8fd0\u884c\u65f6\u95f4\u51cf\u5c11\u56db\u500d\uff09\u3002", "conclusion": "RMFAT\u9002\u7528\u4e8e\u5b9e\u65f6\u5927\u6c14\u6e4d\u6d41\u6291\u5236\u4efb\u52a1\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2508.11431", "pdf": "https://arxiv.org/pdf/2508.11431", "abs": "https://arxiv.org/abs/2508.11431", "authors": ["Simona Kocour", "Assia Benbihi", "Torsten Sattler"], "title": "Remove360: Benchmarking Residuals After Object Removal in 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2503.17574", "summary": "Understanding what semantic information persists after object removal is critical for privacy-preserving 3D reconstruction and editable scene representations. In this work, we introduce a novel benchmark and evaluation framework to measure semantic residuals, the unintended semantic traces left behind, after object removal in 3D Gaussian Splatting. We conduct experiments across a diverse set of indoor and outdoor scenes, showing that current methods can preserve semantic information despite the absence of visual geometry. We also release Remove360, a dataset of pre/post-removal RGB images and object-level masks captured in real-world environments. While prior datasets have focused on isolated object instances, Remove360 covers a broader and more complex range of indoor and outdoor scenes, enabling evaluation of object removal in the context of full-scene representations. Given ground truth images of a scene before and after object removal, we assess whether we can truly eliminate semantic presence, and if downstream models can still infer what was removed. Our findings reveal critical limitations in current 3D object removal techniques and underscore the need for more robust solutions capable of handling real-world complexity. The evaluation framework is available at github.com/spatial-intelligence-ai/Remove360.git. Data are available at huggingface.co/datasets/simkoc/Remove360.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\u548c\u6570\u636e\u96c6Remove360\uff0c\u7528\u4e8e\u8861\u91cf3D\u9ad8\u65af\u6cfc\u6e85\u4e2d\u7269\u4f53\u79fb\u9664\u540e\u9057\u7559\u7684\u8bed\u4e49\u75d5\u8ff9\uff08\u8bed\u4e49\u6b8b\u7559\uff09\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6280\u672f\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7814\u7a76\u7269\u4f53\u79fb\u9664\u540e\u8bed\u4e49\u4fe1\u606f\u7684\u6b8b\u7559\u95ee\u9898\u5bf9\u9690\u79c1\u4fdd\u62a4\u76843D\u91cd\u5efa\u548c\u53ef\u7f16\u8f91\u573a\u666f\u8868\u793a\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u521b\u5efaRemove360\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u5206\u6790\u7269\u4f53\u79fb\u9664\u524d\u540e\u7684RGB\u56fe\u50cf\u548c\u5bf9\u8c61\u7ea7\u63a9\u7801\uff0c\u8bc4\u4f30\u8bed\u4e49\u6b8b\u7559\u60c5\u51b5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u65b9\u6cd5\u5728\u89c6\u89c9\u51e0\u4f55\u7f3a\u5931\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u7559\u8bed\u4e49\u4fe1\u606f\uff0c\u4f46\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\u3002", "conclusion": "\u5f53\u524d3D\u7269\u4f53\u79fb\u9664\u6280\u672f\u9700\u6539\u8fdb\u4ee5\u5e94\u5bf9\u771f\u5b9e\u4e16\u754c\u7684\u590d\u6742\u6027\uff0cRemove360\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002"}}
{"id": "2508.11433", "pdf": "https://arxiv.org/pdf/2508.11433", "abs": "https://arxiv.org/abs/2508.11433", "authors": ["Qian Liang", "Yujia Wu", "Kuncheng Li", "Jiwei Wei", "Shiyuan He", "Jinyu Guo", "Ning Xie"], "title": "MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) with unified architectures excel across a wide range of vision-language tasks, yet aligning them with personalized image generation remains a significant challenge. Existing methods for MLLMs are frequently subject-specific, demanding a data-intensive fine-tuning process for every new subject, which limits their scalability. In this paper, we introduce MM-R1, a framework that integrates a cross-modal Chain-of-Thought (X-CoT) reasoning strategy to unlock the inherent potential of unified MLLMs for personalized image generation. Specifically, we structure personalization as an integrated visual reasoning and generation process: (1) grounding subject concepts by interpreting and understanding user-provided images and contextual cues, and (2) generating personalized images conditioned on both the extracted subject representations and user prompts. To further enhance the reasoning capability, we adopt Grouped Reward Proximal Policy Optimization (GRPO) to explicitly align the generation. Experiments demonstrate that MM-R1 unleashes the personalization capability of unified MLLMs to generate images with high subject fidelity and strong text alignment in a zero-shot manner.", "AI": {"tldr": "MM-R1\u6846\u67b6\u901a\u8fc7\u8de8\u6a21\u6001\u601d\u7ef4\u94fe\u63a8\u7406\u7b56\u7565\uff0c\u89e3\u9501\u7edf\u4e00MLLMs\u5728\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u6f5c\u529b\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u9ad8\u4fdd\u771f\u751f\u6210\u3002", "motivation": "\u73b0\u6709MLLMs\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u4e3b\u9898\uff0c\u9700\u5927\u91cf\u6570\u636e\u5fae\u8c03\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "\u91c7\u7528\u8de8\u6a21\u6001\u601d\u7ef4\u94fe\u63a8\u7406\uff08X-CoT\uff09\u548c\u5206\u7ec4\u5956\u52b1\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\uff0c\u7ed3\u5408\u89c6\u89c9\u63a8\u7406\u4e0e\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMM-R1\u80fd\u96f6\u6837\u672c\u751f\u6210\u9ad8\u4fdd\u771f\u4e14\u6587\u672c\u5bf9\u9f50\u7684\u4e2a\u6027\u5316\u56fe\u50cf\u3002", "conclusion": "MM-R1\u4e3a\u7edf\u4e00MLLMs\u7684\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11484", "pdf": "https://arxiv.org/pdf/2508.11484", "abs": "https://arxiv.org/abs/2508.11484", "authors": ["Xiaoxue Wu", "Bingjie Gao", "Yu Qiao", "Yaohui Wang", "Xinyuan Chen"], "title": "CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models", "categories": ["cs.CV"], "comment": "27 pages, 20 figures", "summary": "Despite significant advances in video synthesis, research into multi-shot video generation remains in its infancy. Even with scaled-up models and massive datasets, the shot transition capabilities remain rudimentary and unstable, largely confining generated videos to single-shot sequences. In this work, we introduce CineTrans, a novel framework for generating coherent multi-shot videos with cinematic, film-style transitions. To facilitate insights into the film editing style, we construct a multi-shot video-text dataset Cine250K with detailed shot annotations. Furthermore, our analysis of existing video diffusion models uncovers a correspondence between attention maps in the diffusion model and shot boundaries, which we leverage to design a mask-based control mechanism that enables transitions at arbitrary positions and transfers effectively in a training-free setting. After fine-tuning on our dataset with the mask mechanism, CineTrans produces cinematic multi-shot sequences while adhering to the film editing style, avoiding unstable transitions or naive concatenations. Finally, we propose specialized evaluation metrics for transition control, temporal consistency and overall quality, and demonstrate through extensive experiments that CineTrans significantly outperforms existing baselines across all criteria.", "AI": {"tldr": "CineTrans\u662f\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u5177\u6709\u7535\u5f71\u98ce\u683c\u8fc7\u6e21\u7684\u591a\u955c\u5934\u89c6\u9891\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u6570\u636e\u96c6\u548c\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u7a33\u5b9a\u8fc7\u6e21\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u9891\u5408\u6210\u6280\u672f\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u591a\u955c\u5934\u89c6\u9891\u751f\u6210\u7684\u7814\u7a76\u4ecd\u5904\u4e8e\u521d\u7ea7\u9636\u6bb5\uff0c\u73b0\u6709\u6a21\u578b\u7684\u8fc7\u6e21\u80fd\u529b\u6709\u9650\u4e14\u4e0d\u7a33\u5b9a\u3002", "method": "\u6784\u5efa\u591a\u955c\u5934\u89c6\u9891\u6587\u672c\u6570\u636e\u96c6Cine250K\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u6ce8\u610f\u529b\u56fe\u8bbe\u8ba1\u57fa\u4e8e\u63a9\u7801\u7684\u63a7\u5236\u673a\u5236\uff0c\u5b9e\u73b0\u4efb\u610f\u4f4d\u7f6e\u7684\u8fc7\u6e21\u3002", "result": "CineTrans\u751f\u6210\u7684\u89c6\u9891\u5177\u6709\u7535\u5f71\u7f16\u8f91\u98ce\u683c\uff0c\u907f\u514d\u4e86\u4e0d\u7a33\u5b9a\u8fc7\u6e21\u6216\u7b80\u5355\u62fc\u63a5\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "CineTrans\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u96c6\u548c\u63a7\u5236\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u955c\u5934\u89c6\u9891\u751f\u6210\u7684\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2508.11538", "pdf": "https://arxiv.org/pdf/2508.11538", "abs": "https://arxiv.org/abs/2508.11538", "authors": ["Sitong Gong", "Lu Zhang", "Yunzhi Zhuge", "Xu Jia", "Pingping Zhang", "Huchuan Lu"], "title": "Reinforcing Video Reasoning Segmentation to Think Before It Segments", "categories": ["cs.CV"], "comment": "12 pages", "summary": "Video reasoning segmentation (VRS) endeavors to delineate referred objects in videos guided by implicit instructions that encapsulate human intent and temporal logic. Previous approaches leverage large vision language models (LVLMs) to encode object semantics into <SEG> tokens for mask prediction. However, this paradigm suffers from limited interpretability during inference and suboptimal performance due to inadequate spatiotemporal reasoning. Drawing inspiration from seminal breakthroughs in reinforcement learning, we introduce Veason-R1, a specialized LVLM for VRS that emphasizes structured reasoning in segmentation. Veason-R1 is trained through Group Relative Policy Optimization (GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, we curate high-quality CoT training data to instill structured reasoning trajectories, bridging video-level semantics and frame-level spatial grounding, yielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPO fine-tuning encourages efficient exploration of the reasoning space by optimizing reasoning chains. To this end, we incorporate a holistic reward mechanism that synergistically enhances spatial alignment and temporal consistency, bolstering keyframe localization and fine-grained grounding. Comprehensive empirical evaluations demonstrate that Veason-R1 achieves state-of-the-art performance on multiple benchmarks, surpassing prior art by significant margins (e.g., +1.3 J &F in ReVOS and +10.0 J &F in ReasonVOS), while exhibiting robustness to hallucinations (+8.8 R). Our code and model weights will be available at Veason-R1.", "AI": {"tldr": "Veason-R1\u662f\u4e00\u79cd\u4e13\u7528\u4e8e\u89c6\u9891\u63a8\u7406\u5206\u5272\uff08VRS\uff09\u7684LVLM\uff0c\u901a\u8fc7GRPO\u548cCoT\u521d\u59cb\u5316\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u7a7a\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u63a8\u7406\u65f6\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u4e14\u6027\u80fd\u4e0d\u4f73\uff0c\u4e3b\u8981\u7531\u4e8e\u65f6\u7a7a\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u91c7\u7528GRPO\u548cCoT\u521d\u59cb\u5316\u8bad\u7ec3\u6a21\u578b\uff0c\u7ed3\u5408\u9ad8\u8d28\u91cfCoT\u6570\u636e\uff0c\u4f18\u5316\u63a8\u7406\u94fe\u548c\u5956\u52b1\u673a\u5236\u3002", "result": "\u5728ReVOS\u548cReasonVOS\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\uff08\u5982+1.3 J&F\u548c+10.0 J&F\uff09\uff0c\u4e14\u6297\u5e7b\u89c9\u80fd\u529b\u589e\u5f3a\uff08+8.8 R\uff09\u3002", "conclusion": "Veason-R1\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86VRS\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.11550", "pdf": "https://arxiv.org/pdf/2508.11550", "abs": "https://arxiv.org/abs/2508.11550", "authors": ["Zuo Zuo", "Jiahao Dong", "Yanyun Qu", "Zongze Wu"], "title": "Training-Free Anomaly Generation via Dual-Attention Enhancement in Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "Industrial anomaly detection (AD) plays a significant role in manufacturing where a long-standing challenge is data scarcity. A growing body of works have emerged to address insufficient anomaly data via anomaly generation. However, these anomaly generation methods suffer from lack of fidelity or need to be trained with extra data. To this end, we propose a training-free anomaly generation framework dubbed AAG, which is based on Stable Diffusion (SD)'s strong generation ability for effective anomaly image generation. Given a normal image, mask and a simple text prompt, AAG can generate realistic and natural anomalies in the specific regions and simultaneously keep contents in other regions unchanged. In particular, we propose Cross-Attention Enhancement (CAE) to re-engineer the cross-attention mechanism within Stable Diffusion based on the given mask. CAE increases the similarity between visual tokens in specific regions and text embeddings, which guides these generated visual tokens in accordance with the text description. Besides, generated anomalies need to be more natural and plausible with object in given image. We propose Self-Attention Enhancement (SAE) which improves similarity between each normal visual token and anomaly visual tokens. SAE ensures that generated anomalies are coherent with original pattern. Extensive experiments on MVTec AD and VisA datasets demonstrate effectiveness of AAG in anomaly generation and its utility. Furthermore, anomaly images generated by AAG can bolster performance of various downstream anomaly inspection tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eStable Diffusion\u7684\u8bad\u7ec3\u514d\u8d39\u5f02\u5e38\u751f\u6210\u6846\u67b6AAG\uff0c\u901a\u8fc7\u6539\u8fdb\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u751f\u6210\u9ad8\u8d28\u91cf\u5f02\u5e38\u56fe\u50cf\uff0c\u63d0\u5347\u4e0b\u6e38\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u4e2d\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u7a81\u51fa\uff0c\u73b0\u6709\u5f02\u5e38\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u903c\u771f\u6027\u6216\u9700\u989d\u5916\u8bad\u7ec3\u6570\u636e\u3002", "method": "AAG\u6846\u67b6\u5229\u7528Stable Diffusion\uff0c\u7ed3\u5408Cross-Attention Enhancement\uff08CAE\uff09\u548cSelf-Attention Enhancement\uff08SAE\uff09\u673a\u5236\uff0c\u901a\u8fc7\u6587\u672c\u63d0\u793a\u548c\u63a9\u7801\u751f\u6210\u903c\u771f\u5f02\u5e38\u56fe\u50cf\u3002", "result": "\u5728MVTec AD\u548cVisA\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86AAG\u7684\u6709\u6548\u6027\uff0c\u751f\u6210\u7684\u5f02\u5e38\u56fe\u50cf\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "AAG\u4e3a\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u5f02\u5e38\u751f\u6210\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.11603", "pdf": "https://arxiv.org/pdf/2508.11603", "abs": "https://arxiv.org/abs/2508.11603", "authors": ["Zhe Zhu", "Honghua Chen", "Peng Li", "Mingqiang Wei"], "title": "CoreEditor: Consistent 3D Editing via Correspondence-constrained Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Text-driven 3D editing seeks to modify 3D scenes according to textual descriptions, and most existing approaches tackle this by adapting pre-trained 2D image editors to multi-view inputs. However, without explicit control over multi-view information exchange, they often fail to maintain cross-view consistency, leading to insufficient edits and blurry details. We introduce CoreEditor, a novel framework for consistent text-to-3D editing. The key innovation is a correspondence-constrained attention mechanism that enforces precise interactions between pixels expected to remain consistent throughout the diffusion denoising process. Beyond relying solely on geometric alignment, we further incorporate semantic similarity estimated during denoising, enabling more reliable correspondence modeling and robust multi-view editing. In addition, we design a selective editing pipeline that allows users to choose preferred results from multiple candidates, offering greater flexibility and user control. Extensive experiments show that CoreEditor produces high-quality, 3D-consistent edits with sharper details, significantly outperforming prior methods.", "AI": {"tldr": "CoreEditor\u901a\u8fc7\u5f15\u5165\u5bf9\u5e94\u7ea6\u675f\u6ce8\u610f\u529b\u673a\u5236\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u56fe\u7f16\u8f91\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u9009\u62e9\u6027\u7f16\u8f91\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u7f16\u8f91\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u89c6\u56fe\u4fe1\u606f\u4ea4\u6362\u4e2d\u7f3a\u4e4f\u663e\u5f0f\u63a7\u5236\uff0c\u5bfc\u81f4\u7f16\u8f91\u4e0d\u4e00\u81f4\u548c\u7ec6\u8282\u6a21\u7cca\u3002", "method": "\u63d0\u51faCoreEditor\u6846\u67b6\uff0c\u91c7\u7528\u5bf9\u5e94\u7ea6\u675f\u6ce8\u610f\u529b\u673a\u5236\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u5efa\u6a21\uff0c\u8bbe\u8ba1\u9009\u62e9\u6027\u7f16\u8f91\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCoreEditor\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u30013D\u4e00\u81f4\u7684\u7f16\u8f91\u7ed3\u679c\uff0c\u7ec6\u8282\u66f4\u6e05\u6670\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CoreEditor\u901a\u8fc7\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u63a7\u5236\u548c\u7528\u6237\u4ea4\u4e92\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u9a71\u52a8\u76843D\u7f16\u8f91\u6548\u679c\u3002"}}
{"id": "2508.11624", "pdf": "https://arxiv.org/pdf/2508.11624", "abs": "https://arxiv.org/abs/2508.11624", "authors": ["Niki Foteinopoulou", "Ignas Budvytis", "Stephan Liwicki"], "title": "LoRAtorio: An intrinsic approach to LoRA Skill Composition", "categories": ["cs.CV"], "comment": "32 pages, 17 figures", "summary": "Low-Rank Adaptation (LoRA) has become a widely adopted technique in text-to-image diffusion models, enabling the personalisation of visual concepts such as characters, styles, and objects. However, existing approaches struggle to effectively compose multiple LoRA adapters, particularly in open-ended settings where the number and nature of required skills are not known in advance. In this work, we present LoRAtorio, a novel train-free framework for multi-LoRA composition that leverages intrinsic model behaviour. Our method is motivated by two key observations: (1) LoRA adapters trained on narrow domains produce denoised outputs that diverge from the base model, and (2) when operating out-of-distribution, LoRA outputs show behaviour closer to the base model than when conditioned in distribution. The balance between these two observations allows for exceptional performance in the single LoRA scenario, which nevertheless deteriorates when multiple LoRAs are loaded. Our method operates in the latent space by dividing it into spatial patches and computing cosine similarity between each patch's predicted noise and that of the base model. These similarities are used to construct a spatially-aware weight matrix, which guides a weighted aggregation of LoRA outputs. To address domain drift, we further propose a modification to classifier-free guidance that incorporates the base model's unconditional score into the composition. We extend this formulation to a dynamic module selection setting, enabling inference-time selection of relevant LoRA adapters from a large pool. LoRAtorio achieves state-of-the-art performance, showing up to a 1.3% improvement in ClipScore and a 72.43% win rate in GPT-4V pairwise evaluations, and generalises effectively to multiple latent diffusion models.", "AI": {"tldr": "LoRAtorio\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u591aLoRA\u7ec4\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6a21\u578b\u5185\u5728\u884c\u4e3a\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u591aLoRA\u7ec4\u5408\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591aLoRA\u7ec4\u5408\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u5f00\u653e\u73af\u5883\u4e0b\uff0c\u65e0\u6cd5\u9884\u5148\u77e5\u9053\u6240\u9700\u6280\u80fd\u7684\u6570\u91cf\u548c\u6027\u8d28\u3002", "method": "\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5212\u5206\u7a7a\u95f4\u5757\uff0c\u8ba1\u7b97\u6bcf\u4e2a\u5757\u7684\u9884\u6d4b\u566a\u58f0\u4e0e\u57fa\u7840\u6a21\u578b\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\uff0c\u6784\u5efa\u7a7a\u95f4\u611f\u77e5\u6743\u91cd\u77e9\u9635\uff0c\u52a0\u6743\u805a\u5408LoRA\u8f93\u51fa\u3002", "result": "LoRAtorio\u5728ClipScore\u4e0a\u63d0\u53471.3%\uff0c\u5728GPT-4V\u8bc4\u4f30\u4e2d\u80dc\u738772.43%\uff0c\u4e14\u9002\u7528\u4e8e\u591a\u79cd\u6f5c\u5728\u6269\u6563\u6a21\u578b\u3002", "conclusion": "LoRAtorio\u901a\u8fc7\u52a8\u6001\u6a21\u5757\u9009\u62e9\u548c\u52a0\u6743\u805a\u5408\uff0c\u5728\u591aLoRA\u7ec4\u5408\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.11049", "pdf": "https://arxiv.org/pdf/2508.11049", "abs": "https://arxiv.org/abs/2508.11049", "authors": ["Kelin Yu", "Sheng Zhang", "Harshit Soora", "Furong Huang", "Heng Huang", "Pratap Tokekar", "Ruohan Gao"], "title": "GenFlowRL: Shaping Rewards with Generative Object-Centric Flow in Visual Reinforcement Learning", "categories": ["cs.RO", "cs.CV"], "comment": "Published at ICCV 2025", "summary": "Recent advances have shown that video generation models can enhance robot learning by deriving effective robot actions through inverse dynamics. However, these methods heavily depend on the quality of generated data and struggle with fine-grained manipulation due to the lack of environment feedback. While video-based reinforcement learning improves policy robustness, it remains constrained by the uncertainty of video generation and the challenges of collecting large-scale robot datasets for training diffusion models. To address these limitations, we propose GenFlowRL, which derives shaped rewards from generated flow trained from diverse cross-embodiment datasets. This enables learning generalizable and robust policies from diverse demonstrations using low-dimensional, object-centric features. Experiments on 10 manipulation tasks, both in simulation and real-world cross-embodiment evaluations, demonstrate that GenFlowRL effectively leverages manipulation features extracted from generated object-centric flow, consistently achieving superior performance across diverse and challenging scenarios. Our Project Page: https://colinyu1.github.io/genflowrl", "AI": {"tldr": "GenFlowRL\u901a\u8fc7\u4ece\u591a\u6837\u5316\u7684\u8de8\u4f53\u73b0\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u751f\u6210\u7684\u6d41\u6765\u5851\u9020\u5956\u52b1\uff0c\u4ece\u800c\u5b66\u4e60\u901a\u7528\u4e14\u9c81\u68d2\u7684\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u4f9d\u8d56\u751f\u6210\u6570\u636e\u8d28\u91cf\u4e14\u7f3a\u4e4f\u73af\u5883\u53cd\u9988\uff0c\u96be\u4ee5\u5904\u7406\u7cbe\u7ec6\u64cd\u4f5c\uff1b\u89c6\u9891\u5f3a\u5316\u5b66\u4e60\u53d7\u9650\u4e8e\u89c6\u9891\u751f\u6210\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u6536\u96c6\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faGenFlowRL\uff0c\u5229\u7528\u4ece\u8de8\u4f53\u73b0\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u751f\u6210\u6d41\u63d0\u53d6\u4f4e\u7ef4\u3001\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u7279\u5f81\uff0c\u5851\u9020\u5956\u52b1\u4ee5\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u572810\u4e2a\u64cd\u4f5c\u4efb\u52a1\u7684\u4eff\u771f\u548c\u771f\u5b9e\u8de8\u4f53\u73b0\u8bc4\u4f30\u4e2d\uff0cGenFlowRL\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u6709\u6548\u5229\u7528\u751f\u6210\u7684\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u6d41\u7279\u5f81\u3002", "conclusion": "GenFlowRL\u901a\u8fc7\u751f\u6210\u6d41\u63d0\u53d6\u7684\u7279\u5f81\uff0c\u5728\u591a\u6837\u5316\u548c\u6311\u6218\u6027\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2508.11211", "pdf": "https://arxiv.org/pdf/2508.11211", "abs": "https://arxiv.org/abs/2508.11211", "authors": ["Zhenhao Li", "Long Yang", "Xiaojie Yin", "Haijun Yu", "Jiazhou Wang", "Hongbin Han", "Weigang Hu", "Yixing Huang"], "title": "Efficient Image-to-Image Schr\u00f6dinger Bridge for CT Field of View Extension", "categories": ["eess.IV", "cs.CV"], "comment": "10 pages", "summary": "Computed tomography (CT) is a cornerstone imaging modality for non-invasive, high-resolution visualization of internal anatomical structures. However, when the scanned object exceeds the scanner's field of view (FOV), projection data are truncated, resulting in incomplete reconstructions and pronounced artifacts near FOV boundaries. Conventional reconstruction algorithms struggle to recover accurate anatomy from such data, limiting clinical reliability. Deep learning approaches have been explored for FOV extension, with diffusion generative models representing the latest advances in image synthesis. Yet, conventional diffusion models are computationally demanding and slow at inference due to their iterative sampling process. To address these limitations, we propose an efficient CT FOV extension framework based on the image-to-image Schr\\\"odinger Bridge (I$^2$SB) diffusion model. Unlike traditional diffusion models that synthesize images from pure Gaussian noise, I$^2$SB learns a direct stochastic mapping between paired limited-FOV and extended-FOV images. This direct correspondence yields a more interpretable and traceable generative process, enhancing anatomical consistency and structural fidelity in reconstructions. I$^2$SB achieves superior quantitative performance, with root-mean-square error (RMSE) values of 49.8\\,HU on simulated noisy data and 152.0HU on real data, outperforming state-of-the-art diffusion models such as conditional denoising diffusion probabilistic models (cDDPM) and patch-based diffusion methods. Moreover, its one-step inference enables reconstruction in just 0.19s per 2D slice, representing over a 700-fold speedup compared to cDDPM (135s) and surpassing diffusionGAN (0.58s), the second fastest. This combination of accuracy and efficiency makes I$^2$SB highly suitable for real-time or clinical deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSchr\u00f6dinger Bridge\u6269\u6563\u6a21\u578b\uff08I\u00b2SB\uff09\u7684\u9ad8\u6548CT\u89c6\u91ce\u6269\u5c55\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edfCT\u626b\u63cf\u5728\u89c6\u91ce\u53d7\u9650\u65f6\u4f1a\u4ea7\u751f\u622a\u65ad\u6570\u636e\uff0c\u5bfc\u81f4\u91cd\u5efa\u4e0d\u5b8c\u6574\u548c\u4f2a\u5f71\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u89e3\u51b3\u3002\u6269\u6563\u6a21\u578b\u867d\u5148\u8fdb\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u63a8\u7406\u6162\u3002", "method": "\u91c7\u7528I\u00b2SB\u6269\u6563\u6a21\u578b\uff0c\u76f4\u63a5\u5b66\u4e60\u6709\u9650\u89c6\u91ce\u4e0e\u6269\u5c55\u89c6\u91ce\u56fe\u50cf\u7684\u968f\u673a\u6620\u5c04\uff0c\u907f\u514d\u4f20\u7edf\u6269\u6563\u6a21\u578b\u4ece\u7eaf\u566a\u58f0\u5408\u6210\u7684\u4f4e\u6548\u95ee\u9898\u3002", "result": "I\u00b2SB\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0cRMSE\u5206\u522b\u4e3a49.8HU\u548c152.0HU\uff0c\u63a8\u7406\u901f\u5ea6\u8fbe0.19\u79d2/\u5207\u7247\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb700\u500d\u4ee5\u4e0a\u3002", "conclusion": "I\u00b2SB\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u7684\u4f18\u52bf\u4f7f\u5176\u9002\u5408\u5b9e\u65f6\u6216\u4e34\u5e8a\u90e8\u7f72\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6269\u6563\u6a21\u578b\u7684\u8ba1\u7b97\u74f6\u9888\u3002"}}
{"id": "2508.11216", "pdf": "https://arxiv.org/pdf/2508.11216", "abs": "https://arxiv.org/abs/2508.11216", "authors": ["Han Zhang", "Xue-Cheng Tai", "Jean-Michel Morel", "Raymond H. Chan"], "title": "Fluid Dynamics and Domain Reconstruction from Noisy Flow Images Using Physics-Informed Neural Networks and Quasi-Conformal Mapping", "categories": ["math.NA", "cs.CV", "cs.NA"], "comment": null, "summary": "Blood flow imaging provides important information for hemodynamic behavior within the vascular system and plays an essential role in medical diagnosis and treatment planning. However, obtaining high-quality flow images remains a significant challenge. In this work, we address the problem of denoising flow images that may suffer from artifacts due to short acquisition times or device-induced errors. We formulate this task as an optimization problem, where the objective is to minimize the discrepancy between the modeled velocity field, constrained to satisfy the Navier-Stokes equations, and the observed noisy velocity data. To solve this problem, we decompose it into two subproblems: a fluid subproblem and a geometry subproblem. The fluid subproblem leverages a Physics-Informed Neural Network to reconstruct the velocity field from noisy observations, assuming a fixed domain. The geometry subproblem aims to infer the underlying flow region by optimizing a quasi-conformal mapping that deforms a reference domain. These two subproblems are solved in an alternating Gauss-Seidel fashion, iteratively refining both the velocity field and the domain. Upon convergence, the framework yields a high-quality reconstruction of the flow image. We validate the proposed method through experiments on synthetic flow data in a converging channel geometry under varying levels of Gaussian noise, and on real-like flow data in an aortic geometry with signal-dependent noise. The results demonstrate the effectiveness and robustness of the approach. Additionally, ablation studies are conducted to assess the influence of key hyperparameters.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7ea6\u675f\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u53bb\u566a\u8840\u6d41\u56fe\u50cf\uff0c\u901a\u8fc7\u4ea4\u66ff\u6c42\u89e3\u6d41\u4f53\u548c\u51e0\u4f55\u5b50\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u91cd\u5efa\u3002", "motivation": "\u8840\u6d41\u6210\u50cf\u5728\u533b\u5b66\u8bca\u65ad\u548c\u6cbb\u7597\u89c4\u5212\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9ad8\u8d28\u91cf\u56fe\u50cf\u83b7\u53d6\u4ecd\u5177\u6311\u6218\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u56e0\u77ed\u91c7\u96c6\u65f6\u95f4\u6216\u8bbe\u5907\u8bef\u5dee\u5bfc\u81f4\u7684\u56fe\u50cf\u566a\u58f0\u95ee\u9898\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u4f18\u5316\u95ee\u9898\uff0c\u5206\u89e3\u4e3a\u6d41\u4f53\u5b50\u95ee\u9898\uff08\u4f7f\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u91cd\u5efa\u901f\u5ea6\u573a\uff09\u548c\u51e0\u4f55\u5b50\u95ee\u9898\uff08\u4f18\u5316\u51c6\u5171\u5f62\u6620\u5c04\u63a8\u65ad\u6d41\u52a8\u533a\u57df\uff09\uff0c\u4ea4\u66ff\u6c42\u89e3\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u8840\u6d41\u6570\u636e\u4e0a\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u8bc4\u4f30\u4e86\u5173\u952e\u8d85\u53c2\u6570\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u8d28\u91cf\u91cd\u5efa\u8840\u6d41\u56fe\u50cf\uff0c\u4e3a\u533b\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2508.11375", "pdf": "https://arxiv.org/pdf/2508.11375", "abs": "https://arxiv.org/abs/2508.11375", "authors": ["Zonglin Wu", "Yule Xue", "Qianxiang Hu", "Yaoyao Feng", "Yuqi Ma", "Shanxiong Chen"], "title": "AnatoMaskGAN: GNN-Driven Slice Feature Fusion and Noise Augmentation for Medical Semantic Image Synthesis", "categories": ["eess.IV", "cs.CV", "I.4.9"], "comment": "8 pages", "summary": "Medical semantic-mask synthesis boosts data augmentation and analysis, yet most GAN-based approaches still produce one-to-one images and lack spatial consistency in complex scans. To address this, we propose AnatoMaskGAN, a novel synthesis framework that embeds slice-related spatial features to precisely aggregate inter-slice contextual dependencies, introduces diverse image-augmentation strategies, and optimizes deep feature learning to improve performance on complex medical images. Specifically, we design a GNN-based strongly correlated slice-feature fusion module to model spatial relationships between slices and integrate contextual information from neighboring slices, thereby capturing anatomical details more comprehensively; we introduce a three-dimensional spatial noise-injection strategy that weights and fuses spatial features with noise to enhance modeling of structural diversity; and we incorporate a grayscale-texture classifier to optimize grayscale distribution and texture representation during generation. Extensive experiments on the public L2R-OASIS and L2R-Abdomen CT datasets show that AnatoMaskGAN raises PSNR on L2R-OASIS to 26.50 dB (0.43 dB higher than the current state of the art) and achieves an SSIM of 0.8602 on L2R-Abdomen CT--a 0.48 percentage-point gain over the best model, demonstrating its superiority in reconstruction accuracy and perceptual quality. Ablation studies that successively remove the slice-feature fusion module, spatial 3D noise-injection strategy, and grayscale-texture classifier reveal that each component contributes significantly to PSNR, SSIM, and LPIPS, further confirming the independent value of each core design in enhancing reconstruction accuracy and perceptual quality.", "AI": {"tldr": "AnatoMaskGAN\u901a\u8fc7\u5d4c\u5165\u5207\u7247\u76f8\u5173\u7a7a\u95f4\u7279\u5f81\u548c\u5f15\u5165\u591a\u6837\u5316\u7684\u56fe\u50cf\u589e\u5f3a\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u590d\u6742\u533b\u5b66\u56fe\u50cf\u7684\u751f\u6210\u8d28\u91cf\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709GAN\u65b9\u6cd5\u5728\u590d\u6742\u533b\u5b66\u626b\u63cf\u4e2d\u7f3a\u4e4f\u7a7a\u95f4\u4e00\u81f4\u6027\u548c\u591a\u6837\u6027\uff0c\u9650\u5236\u4e86\u6570\u636e\u589e\u5f3a\u548c\u5206\u6790\u7684\u6548\u679c\u3002", "method": "\u63d0\u51faAnatoMaskGAN\u6846\u67b6\uff0c\u5305\u62ecGNN\u5207\u7247\u7279\u5f81\u878d\u5408\u6a21\u5757\u3001\u4e09\u7ef4\u7a7a\u95f4\u566a\u58f0\u6ce8\u5165\u7b56\u7565\u548c\u7070\u5ea6\u7eb9\u7406\u5206\u7c7b\u5668\u3002", "result": "\u5728L2R-OASIS\u548cL2R-Abdomen CT\u6570\u636e\u96c6\u4e0a\uff0cPSNR\u548cSSIM\u663e\u8457\u63d0\u5347\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AnatoMaskGAN\u7684\u6838\u5fc3\u8bbe\u8ba1\u5747\u5bf9\u63d0\u5347\u91cd\u5efa\u7cbe\u5ea6\u548c\u611f\u77e5\u8d28\u91cf\u6709\u663e\u8457\u8d21\u732e\u3002"}}
{"id": "2508.11391", "pdf": "https://arxiv.org/pdf/2508.11391", "abs": "https://arxiv.org/abs/2508.11391", "authors": ["Yinggan Tang", "Quanwei Hu"], "title": "LKFMixer: Exploring Large Kernel Feature For Efficient Image Super-Resolution", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The success of self-attention (SA) in Transformer demonstrates the importance of non-local information to image super-resolution (SR), but the huge computing power required makes it difficult to implement lightweight models. To solve this problem, we propose a pure convolutional neural network (CNN) model, LKFMixer, which utilizes large convolutional kernel to simulate the ability of self-attention to capture non-local features. Specifically, we increase the kernel size to 31 to obtain the larger receptive field as possible, and reduce the parameters and computations by coordinate decomposition. Meanwhile, a spatial feature modulation block (SFMB) is designed to enhance the focus of feature information on both spatial and channel dimension. In addition, by introducing feature selection block (FSB), the model can adaptively adjust the weights between local features and non-local features. Extensive experiments show that the proposed LKFMixer family outperform other state-of-the-art (SOTA) methods in terms of SR performance and reconstruction quality. In particular, compared with SwinIR-light on Manga109 dataset, LKFMixer-L achieves 0.6dB PSNR improvement at $\\times$4 scale, while the inference speed is $\\times$5 times faster. The code is available at https://github.com/Supereeeee/LKFMixer.", "AI": {"tldr": "LKFMixer\u662f\u4e00\u79cd\u7eaf\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u901a\u8fc7\u5927\u5377\u79ef\u6838\u6a21\u62df\u81ea\u6ce8\u610f\u529b\u7684\u975e\u5c40\u90e8\u7279\u5f81\u6355\u6349\u80fd\u529b\uff0c\u89e3\u51b3\u4e86Transformer\u8ba1\u7b97\u91cf\u5927\u7684\u95ee\u9898\uff0c\u5e76\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u81ea\u6ce8\u610f\u529b\u5728Transformer\u4e2d\u7684\u6210\u529f\u8868\u660e\u975e\u5c40\u90e8\u4fe1\u606f\u5bf9\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u5f88\u91cd\u8981\uff0c\u4f46\u5176\u8ba1\u7b97\u91cf\u5927\uff0c\u96be\u4ee5\u5b9e\u73b0\u8f7b\u91cf\u5316\u6a21\u578b\u3002", "method": "\u63d0\u51faLKFMixer\u6a21\u578b\uff0c\u4f7f\u752831x31\u5927\u5377\u79ef\u6838\u6a21\u62df\u81ea\u6ce8\u610f\u529b\uff0c\u901a\u8fc7\u5750\u6807\u5206\u89e3\u51cf\u5c11\u53c2\u6570\u548c\u8ba1\u7b97\u91cf\uff0c\u5e76\u8bbe\u8ba1\u4e86\u7a7a\u95f4\u7279\u5f81\u8c03\u5236\u5757\uff08SFMB\uff09\u548c\u7279\u5f81\u9009\u62e9\u5757\uff08FSB\uff09\u589e\u5f3a\u7279\u5f81\u5904\u7406\u3002", "result": "LKFMixer\u5728\u8d85\u5206\u8fa8\u7387\u6027\u80fd\u548c\u91cd\u5efa\u8d28\u91cf\u4e0a\u4f18\u4e8e\u5176\u4ed6SOTA\u65b9\u6cd5\uff0c\u4f8b\u5982\u5728Manga109\u6570\u636e\u96c6\u4e0a\u6bd4SwinIR-light PSNR\u63d0\u53470.6dB\uff0c\u63a8\u7406\u901f\u5ea6\u5feb5\u500d\u3002", "conclusion": "LKFMixer\u901a\u8fc7\u5927\u5377\u79ef\u6838\u548c\u4f18\u5316\u6a21\u5757\u8bbe\u8ba1\uff0c\u6709\u6548\u5e73\u8861\u4e86\u975e\u5c40\u90e8\u7279\u5f81\u6355\u6349\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u8f7b\u91cf\u5316\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
