<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 18]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [SeHDR: Single-Exposure HDR Novel View Synthesis via 3D Gaussian Bracketing](https://arxiv.org/abs/2509.20400)
*Yiyu Li,Haoyuan Wang,Ke Xu,Gerhard Petrus Hancke,Rynson W. H. Lau*

Main category: cs.GR

TL;DR: SeHDR是一种新颖的高动态范围3D高斯泼溅方法，能够从单曝光的多视角LDR图像生成HDR新视角，无需传统方法所需的多曝光图像。


<details>
  <summary>Details</summary>
Motivation: 现有HDR-3DGS方法需要从不同曝光的多视角LDR图像中学习，这些图像难以捕获且容易产生误差（如物体运动模糊和校准不准确）。SeHDR旨在从单曝光的多视角LDR图像中学习HDR场景表示。

Method: 1）从单曝光LDR输入学习基础3D高斯；2）估计具有相同几何但不同线性颜色的多个3D高斯（基于曝光操作）；3）提出可微分神经曝光融合（NeEF）将基础和估计的3D高斯集成到HDR高斯中进行新视角渲染。

Result: 大量实验表明，SeHDR优于现有方法以及精心设计的基线方法。

Conclusion: SeHDR成功解决了从单曝光LDR图像学习HDR场景表示的挑战性问题，通过估计括号化3D高斯并进行神经融合，实现了高质量的HDR新视角生成。

Abstract: This paper presents SeHDR, a novel high dynamic range 3D Gaussian Splatting (HDR-3DGS) approach for generating HDR novel views given multi-view LDR images. Unlike existing methods that typically require the multi-view LDR input images to be captured from different exposures, which are tedious to capture and more likely to suffer from errors (e.g., object motion blurs and calibration/alignment inaccuracies), our approach learns the HDR scene representation from multi-view LDR images of a single exposure. Our key insight to this ill-posed problem is that by first estimating Bracketed 3D Gaussians (i.e., with different exposures) from single-exposure multi-view LDR images, we may then be able to merge these bracketed 3D Gaussians into an HDR scene representation. Specifically, SeHDR first learns base 3D Gaussians from single-exposure LDR inputs, where the spherical harmonics parameterize colors in a linear color space. We then estimate multiple 3D Gaussians with identical geometry but varying linear colors conditioned on exposure manipulations. Finally, we propose the Differentiable Neural Exposure Fusion (NeEF) to integrate the base and estimated 3D Gaussians into HDR Gaussians for novel view rendering. Extensive experiments demonstrate that SeHDR outperforms existing methods as well as carefully designed baselines.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [Seedream 4.0: Toward Next-generation Multimodal Image Generation](https://arxiv.org/abs/2509.20427)
*Team Seedream,Yunpeng Chen,Yu Gao,Lixue Gong,Meng Guo,Qiushan Guo,Zhiyao Guo,Xiaoxia Hou,Weilin Huang,Yixuan Huang,Xiaowen Jian,Huafeng Kuang,Zhichao Lai,Fanshi Li,Liang Li,Xiaochen Lian,Chao Liao,Liyang Liu,Wei Liu,Yanzuo Lu,Zhengxiong Luo,Tongtong Ou,Guang Shi,Yichun Shi,Shiqi Sun,Yu Tian,Zhi Tian,Peng Wang,Rui Wang,Xun Wang,Ye Wang,Guofeng Wu,Jie Wu,Wenxu Wu,Yonghui Wu,Xin Xia,Xuefeng Xiao,Shuang Xu,Xin Yan,Ceyuan Yang,Jianchao Yang,Zhonghua Zhai,Chenlin Zhang,Heng Zhang,Qi Zhang,Xinyu Zhang,Yuwei Zhang,Shijia Zhao,Wenliang Zhao,Wenjia Zhu*

Main category: cs.CV

TL;DR: Seedream 4.0是一个高效的多模态图像生成系统，统一了文本到图像合成、图像编辑和多图像组合功能，采用高效的扩散变换器和优化的VAE实现快速高分辨率图像生成。


<details>
  <summary>Details</summary>
Motivation: 将文本到图像合成、图像编辑和多图像组合统一到单一框架中，扩展传统T2I系统为更交互式和多维度的创意工具。

Method: 开发高效扩散变换器与强大VAE，减少图像token数量；进行多模态后训练联合训练T2I和图像编辑任务；集成对抗蒸馏、分布匹配、量化和推测解码等推理加速技术。

Result: 在T2I和多模态图像编辑上达到最先进水平，生成2K图像推理时间仅1.8秒，在复杂任务中展示卓越的多模态能力。

Conclusion: Seedream 4.0将传统T2I系统扩展为更交互式和多维度的创意工具，推动了生成AI在创意和专业应用领域的边界。

Abstract: We introduce Seedream 4.0, an efficient and high-performance multimodal image generation system that unifies text-to-image (T2I) synthesis, image editing, and multi-image composition within a single framework. We develop a highly efficient diffusion transformer with a powerful VAE which also can reduce the number of image tokens considerably. This allows for efficient training of our model, and enables it to fast generate native high-resolution images (e.g., 1K-4K). Seedream 4.0 is pretrained on billions of text-image pairs spanning diverse taxonomies and knowledge-centric concepts. Comprehensive data collection across hundreds of vertical scenarios, coupled with optimized strategies, ensures stable and large-scale training, with strong generalization. By incorporating a carefully fine-tuned VLM model, we perform multi-modal post-training for training both T2I and image editing tasks jointly. For inference acceleration, we integrate adversarial distillation, distribution matching, and quantization, as well as speculative decoding. It achieves an inference time of up to 1.8 seconds for generating a 2K image (without a LLM/VLM as PE model). Comprehensive evaluations reveal that Seedream 4.0 can achieve state-of-the-art results on both T2I and multimodal image editing. In particular, it demonstrates exceptional multimodal capabilities in complex tasks, including precise image editing and in-context reasoning, and also allows for multi-image reference, and can generate multiple output images. This extends traditional T2I systems into an more interactive and multidimensional creative tool, pushing the boundary of generative AI for both creativity and professional applications. Seedream 4.0 is now accessible on https://www.volcengine.com/experience/ark?launch=seedream.

</details>


### [3] [Shared Neural Space: Unified Precomputed Feature Encoding for Multi-Task and Cross Domain Vision](https://arxiv.org/abs/2509.20481)
*Jing Li,Oskar Bartosz,Chengyu Wang,Michal Wnuczynski,Dilshan Godaliyadda,Michael Polley*

Main category: cs.CV

TL;DR: 提出了通用神经空间（NS）框架，通过编码器-解码器架构预计算跨视觉和成像任务的特征，使多个下游AI模块共享同一特征空间，减少冗余并提高跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型多为特定高精度任务定制，对于需要执行一系列模块化任务的应用来说效率低下，因为每个任务都需要映射到不同的潜在域。

Method: 使用轻量级CNN骨干网络构建编码器-解码器框架，学习具有变换感知和泛化能力的表示，使多个下游模块能在共享特征空间中高效运行。

Result: 实验证明成像和视觉模块（如去马赛克、去噪、深度估计和语义分割）可以在NS中高效执行，该架构减少了冗余并改善了跨域泛化。

Conclusion: NS架构为高效的多任务视觉管道奠定了基础，相比大型transformer骨干网络更轻量，具有更广泛的硬件适用性。

Abstract: The majority of AI models in imaging and vision are customized to perform on specific high-precision task. However, this strategy is inefficient for applications with a series of modular tasks, since each requires a mapping into a disparate latent domain. To address this inefficiency, we proposed a universal Neural Space (NS), where an encoder-decoder framework pre-computes features across vision and imaging tasks. Our encoder learns transformation aware, generalizable representations, which enable multiple downstream AI modules to share the same feature space. This architecture reduces redundancy, improves generalization across domain shift, and establishes a foundation for effecient multi-task vision pipelines. Furthermore, as opposed to larger transformer backbones, our backbone is lightweight and CNN-based, allowing for wider across hardware. We furthur demonstrate that imaging and vision modules, such as demosaicing, denoising, depth estimation and semantic segmentation can be performed efficiently in the NS.

</details>


### [4] [InstructVTON: Optimal Auto-Masking and Natural-Language-Guided Interactive Style Control for Inpainting-Based Virtual Try-On](https://arxiv.org/abs/2509.20524)
*Julien Han,Shuwen Qiu,Qi Li,Xingzi Xu,Mehmet Saygin Seyfioglu,Kavosh Asadi,Karim Bouyarmane*

Main category: cs.CV

TL;DR: InstructVTON是一个基于指令的交互式虚拟试穿系统，通过自然语言指导实现细粒度和复杂的样式控制，支持单件或多件服装的试穿。


<details>
  <summary>Details</summary>
Motivation: 传统的基于掩码的虚拟试穿方法存在局限性，需要精确的掩码绘制，且在某些复杂样式场景下无法实现（如将长袖卷起）。InstructVTON旨在简化用户体验，通过自动化掩码生成实现更灵活的样式控制。

Method: 利用视觉语言模型（VLMs）和图像分割模型自动生成二进制掩码，基于用户提供的图像和自由文本样式指令。系统与现有虚拟试穿模型互操作，实现多轮图像生成。

Result: InstructVTON能够实现传统掩码方法无法完成的复杂试穿场景，达到最先进的样式控制效果。

Conclusion: 该系统通过自然语言指令和自动化掩码生成，显著简化了虚拟试穿的交互过程，提供了更灵活和强大的样式控制能力。

Abstract: We present InstructVTON, an instruction-following interactive virtual try-on system that allows fine-grained and complex styling control of the resulting generation, guided by natural language, on single or multiple garments. A computationally efficient and scalable formulation of virtual try-on formulates the problem as an image-guided or image-conditioned inpainting task. These inpainting-based virtual try-on models commonly use a binary mask to control the generation layout. Producing a mask that yields desirable result is difficult, requires background knowledge, might be model dependent, and in some cases impossible with the masking-based approach (e.g. trying on a long-sleeve shirt with "sleeves rolled up" styling on a person wearing long-sleeve shirt with sleeves down, where the mask will necessarily cover the entire sleeve). InstructVTON leverages Vision Language Models (VLMs) and image segmentation models for automated binary mask generation. These masks are generated based on user-provided images and free-text style instructions. InstructVTON simplifies the end-user experience by removing the necessity of a precisely drawn mask, and by automating execution of multiple rounds of image generation for try-on scenarios that cannot be achieved with masking-based virtual try-on models alone. We show that InstructVTON is interoperable with existing virtual try-on models to achieve state-of-the-art results with styling control.

</details>


### [5] [FreeInsert: Personalized Object Insertion with Geometric and Style Control](https://arxiv.org/abs/2509.20756)
*Yuhong Zhang,Han Wang,Yiwen Wang,Rong Xie,Li Song*

Main category: cs.CV

TL;DR: FreeInsert是一个无需训练的图像编辑框架，通过3D几何信息实现个性化对象插入，解决现有方法在几何控制和风格一致性方面的限制。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑方法在个性化对象插入任务中存在几何控制不足和风格一致性差的问题，且通常需要大量训练。

Method: 将2D对象转换为3D，在3D层面进行交互式编辑，然后从指定视角重新渲染为2D图像，结合扩散适配器实现几何、风格和内容控制。

Result: 该方法能够生成几何控制精确、风格一致的编辑图像，无需额外训练。

Conclusion: FreeInsert通过3D几何信息有效解决了图像编辑中的几何控制和风格一致性问题，为个性化对象插入提供了新思路。

Abstract: Text-to-image diffusion models have made significant progress in image generation, allowing for effortless customized generation. However, existing image editing methods still face certain limitations when dealing with personalized image composition tasks. First, there is the issue of lack of geometric control over the inserted objects. Current methods are confined to 2D space and typically rely on textual instructions, making it challenging to maintain precise geometric control over the objects. Second, there is the challenge of style consistency. Existing methods often overlook the style consistency between the inserted object and the background, resulting in a lack of realism. In addition, the challenge of inserting objects into images without extensive training remains significant. To address these issues, we propose \textit{FreeInsert}, a novel training-free framework that customizes object insertion into arbitrary scenes by leveraging 3D geometric information. Benefiting from the advances in existing 3D generation models, we first convert the 2D object into 3D, perform interactive editing at the 3D level, and then re-render it into a 2D image from a specified view. This process introduces geometric controls such as shape or view. The rendered image, serving as geometric control, is combined with style and content control achieved through diffusion adapters, ultimately producing geometrically controlled, style-consistent edited images via the diffusion model.

</details>


### [6] [CusEnhancer: A Zero-Shot Scene and Controllability Enhancement Method for Photo Customization via ResInversion](https://arxiv.org/abs/2509.20775)
*Maoye Ren,Praneetha Vaddamanu,Jianjin Xu,Fernando De la Torre Frade*

Main category: cs.CV

TL;DR: CustomEnhancer是一个零样本增强框架，通过三流融合生成方法和ResInversion反演技术，提升文本到图像扩散模型在人物照片合成中的场景多样性、身份保真度和训练自由控制能力。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像扩散模型在合成真实人物照片时面临场景退化、控制不足和感知身份不理想的问题，需要改进身份定制模型。

Method: 提出CustomEnhancer框架，采用三流融合PerGeneration方法结合两个兼容的反向潜在空间，并引入ResInversion反演方法通过预扩散机制进行噪声校正，将反演时间减少129倍。

Result: 实验表明CustomEnhancer在场景多样性、身份保真度和训练自由控制方面达到最先进水平，ResInversion相比NTI方法效率显著提升。

Conclusion: 该框架实现了对个性化模型生成过程的全面训练自由控制，为个性化模型提供精确控制，无需为每个模型重新训练控制器。

Abstract: Recently remarkable progress has been made in synthesizing realistic human photos using text-to-image diffusion models. However, current approaches face degraded scenes, insufficient control, and suboptimal perceptual identity. We introduce CustomEnhancer, a novel framework to augment existing identity customization models. CustomEnhancer is a zero-shot enhancement pipeline that leverages face swapping techniques, pretrained diffusion model, to obtain additional representations in a zeroshot manner for encoding into personalized models. Through our proposed triple-flow fused PerGeneration approach, which identifies and combines two compatible counter-directional latent spaces to manipulate a pivotal space of personalized model, we unify the generation and reconstruction processes, realizing generation from three flows. Our pipeline also enables comprehensive training-free control over the generation process of personalized models, offering precise controlled personalization for them and eliminating the need for controller retraining for per-model. Besides, to address the high time complexity of null-text inversion (NTI), we introduce ResInversion, a novel inversion method that performs noise rectification via a pre-diffusion mechanism, reducing the inversion time by 129 times. Experiments demonstrate that CustomEnhancer reach SOTA results at scene diversity, identity fidelity, training-free controls, while also showing the efficiency of our ResInversion over NTI. The code will be made publicly available upon paper acceptance.

</details>


### [7] [DAC-LoRA: Dynamic Adversarial Curriculum for Efficient and Robust Few-Shot Adaptation](https://arxiv.org/abs/2509.20792)
*Ved Umrajkar*

Main category: cs.CV

TL;DR: DAC-LoRA是一个将对抗训练集成到参数高效微调(PEFT)中的新框架，通过动态对抗课程逐步提升CLIP等视觉语言模型的对抗鲁棒性，同时保持清洁准确率。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型(VLMs)在自动驾驶、医疗诊断等安全关键应用中广泛使用，但即使采用LoRA等参数高效微调方法，这些模型仍然容易受到对抗攻击的影响。CLIP作为众多下游VLM的骨干网络，其漏洞可能在整个多模态AI生态系统中传播。

Method: 提出动态对抗课程DAC-LoRA框架，核心原理是基于一阶平稳条件(FOSC)和TRADES启发式损失，设计智能的渐进式攻击课程。该方法可以集成到标准PEFT流程中，通过逐步增加攻击难度来提升模型鲁棒性。

Result: DAC-LoRA在对抗鲁棒性方面实现了显著提升，同时没有显著损害清洁准确率。该方法轻量、有效且具有广泛适用性。

Conclusion: DAC-LoRA提供了一个有效、轻量且广泛适用的方法，证明可以将对抗训练无缝集成到PEFT流程中，显著增强视觉语言模型的鲁棒性。

Abstract: Vision-Language Models (VLMs) are foundational to critical applications like autonomous driving, medical diagnosis, and content moderation. While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA enable their efficient adaptation to specialized tasks, these models remain vulnerable to adversarial attacks that can compromise safety-critical decisions. CLIP, the backbone for numerous downstream VLMs, is a high-value target whose vulnerabilities can cascade across the multimodal AI ecosystem. We propose Dynamic Adversarial Curriculum DAC-LoRA, a novel framework that integrates adversarial training into PEFT. The core principle of our method i.e. an intelligent curriculum of progressively challenging attack, is general and can potentially be applied to any iterative attack method. Guided by the First-Order Stationary Condition (FOSC) and a TRADES-inspired loss, DAC-LoRA achieves substantial improvements in adversarial robustness without significantly compromising clean accuracy. Our work presents an effective, lightweight, and broadly applicable method to demonstrate that the DAC-LoRA framework can be easily integrated into a standard PEFT pipeline to significantly enhance robustness.

</details>


### [8] [Integrating Object Interaction Self-Attention and GAN-Based Debiasing for Visual Question Answering](https://arxiv.org/abs/2509.20884)
*Zhifei Li,Feng Qiu,Yiran Wang,Yujing Xia,Kui Xiao,Miao Zhang,Yan Zhang*

Main category: cs.CV

TL;DR: IOG-VQA模型通过结合对象交互自注意力机制和基于GAN的去偏框架，有效解决了VQA任务中的数据集偏见问题，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有VQA模型容易受到训练数据偏见的影响，过度依赖表面模式，难以泛化到多样化的问题和图像。需要同时解决对象交互理解和数据集偏见问题。

Method: 提出IOG-VQA模型，集成对象交互自注意力机制（捕捉图像中对象间的复杂交互）和基于GAN的去偏框架（生成无偏见数据分布）。

Result: 在VQA-CP v1和VQA-CP v2数据集上的实验表明，该模型在现有方法中表现出色，特别是在处理有偏见和不平衡数据分布方面。

Conclusion: 同时解决对象交互和数据集偏见对于推进VQA任务至关重要，IOG-VQA模型为此提供了有效解决方案。

Abstract: Visual Question Answering (VQA) presents a unique challenge by requiring models to understand and reason about visual content to answer questions accurately. Existing VQA models often struggle with biases introduced by the training data, leading to over-reliance on superficial patterns and inadequate generalization to diverse questions and images. This paper presents a novel model, IOG-VQA, which integrates Object Interaction Self-Attention and GAN-Based Debiasing to enhance VQA model performance. The self-attention mechanism allows our model to capture complex interactions between objects within an image, providing a more comprehensive understanding of the visual context. Meanwhile, the GAN-based debiasing framework generates unbiased data distributions, helping the model to learn more robust and generalizable features. By leveraging these two components, IOG-VQA effectively combines visual and textual information to address the inherent biases in VQA datasets. Extensive experiments on the VQA-CP v1 and VQA-CP v2 datasets demonstrate that our model shows excellent performance compared with the existing methods, particularly in handling biased and imbalanced data distributions highlighting the importance of addressing both object interactions and dataset biases in advancing VQA tasks. Our code is available at https://github.com/HubuKG/IOG-VQA.

</details>


### [9] [Nuclear Diffusion Models for Low-Rank Background Suppression in Videos](https://arxiv.org/abs/2509.20886)
*Tristan S. W. Stevens,Oisín Nolan,Jean-Luc Robert,Ruud J. G. van Sloun*

Main category: cs.CV

TL;DR: 提出了一种结合低秩时间建模和扩散后验采样的混合框架（Nuclear Diffusion），用于视频去雾，在心脏超声去雾任务中优于传统RPCA方法。


<details>
  <summary>Details</summary>
Motivation: 视频序列常包含结构化噪声和背景伪影，传统RPCA方法中的稀疏性假设难以捕捉真实视频数据的丰富变异性。

Method: 集成低秩时间建模与扩散后验采样的混合框架，通过结合基于模型的时间模型和深度生成先验进行视频恢复。

Result: 在心脏超声去雾任务中，相比传统RPCA方法，在对比度增强（gCNR）和信号保留（KS统计量）方面表现出更好的去雾性能。

Conclusion: 结合基于模型的时间模型与深度生成先验的方法在高保真视频恢复方面具有巨大潜力。

Abstract: Video sequences often contain structured noise and background artifacts that obscure dynamic content, posing challenges for accurate analysis and restoration. Robust principal component methods address this by decomposing data into low-rank and sparse components. Still, the sparsity assumption often fails to capture the rich variability present in real video data. To overcome this limitation, a hybrid framework that integrates low-rank temporal modeling with diffusion posterior sampling is proposed. The proposed method, Nuclear Diffusion, is evaluated on a real-world medical imaging problem, namely cardiac ultrasound dehazing, and demonstrates improved dehazing performance compared to traditional RPCA concerning contrast enhancement (gCNR) and signal preservation (KS statistic). These results highlight the potential of combining model-based temporal models with deep generative priors for high-fidelity video restoration.

</details>


### [10] [SimDiff: Simulator-constrained Diffusion Model for Physically Plausible Motion Generation](https://arxiv.org/abs/2509.20927)
*Akihisa Watanabe,Jiawei Ren,Li Siyao,Yichen Peng,Erwin Wu,Edgar Simo-Serra*

Main category: cs.CV

TL;DR: SimDiff是一种模拟器约束的扩散模型，通过将环境参数直接集成到去噪过程中，无需在推理时重复调用模拟器即可高效生成物理合理的人体运动。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将基于模拟器的运动投影层纳入扩散过程以确保物理合理性，但由于模拟器的顺序性质，这些方法计算成本高昂且无法并行化。

Method: 将模拟器运动投影解释为扩散过程中的引导形式，提出SimDiff模型，通过条件化环境参数（如重力、风力）直接在去噪过程中生成物理合理的运动。

Result: SimDiff能够高效生成物理合理的运动，无需推理时的重复模拟器调用，并对不同物理系数提供细粒度控制，成功推广到未见过的环境参数组合。

Conclusion: SimDiff展示了组合泛化能力，为物理合理的人体运动生成提供了一种高效且可控的解决方案。

Abstract: Generating physically plausible human motion is crucial for applications such as character animation and virtual reality. Existing approaches often incorporate a simulator-based motion projection layer to the diffusion process to enforce physical plausibility. However, such methods are computationally expensive due to the sequential nature of the simulator, which prevents parallelization. We show that simulator-based motion projection can be interpreted as a form of guidance, either classifier-based or classifier-free, within the diffusion process. Building on this insight, we propose SimDiff, a Simulator-constrained Diffusion Model that integrates environment parameters (e.g., gravity, wind) directly into the denoising process. By conditioning on these parameters, SimDiff generates physically plausible motions efficiently, without repeated simulator calls at inference, and also provides fine-grained control over different physical coefficients. Moreover, SimDiff successfully generalizes to unseen combinations of environmental parameters, demonstrating compositional generalization.

</details>


### [11] [SiNGER: A Clearer Voice Distills Vision Transformers Further](https://arxiv.org/abs/2509.20986)
*Geunhyeok Yu,Sunjae Jeong,Yoonyoung Choi,Jaeseung Kim,Hyoseok Hwang*

Main category: cs.CV

TL;DR: SiNGER是一个新的知识蒸馏框架，通过奇异零空间引导的能量重分配来抑制Vision Transformers中的高范数伪影，同时保留信息信号，提升学生模型的性能。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers会产生高范数伪影，这些伪影在知识蒸馏过程中会主导目标函数，导致学生模型过度拟合伪影而忽视信息信号，从而削弱大模型的增益。现有方法在去除伪影时面临抑制伪影与保留信息信号之间的权衡问题。

Method: 提出SiNGER框架，通过零空间引导的扰动来精炼教师特征，在抑制伪影的同时保留信息信号。使用基于LoRA的适配器高效实现扰动，只需最小结构修改。然后将精炼后的教师特征蒸馏给学生模型。

Result: 大量实验表明，SiNGER能持续提升学生模型性能，在多个下游任务中达到最先进水平，并产生更清晰、更可解释的表征。

Conclusion: SiNGER有效解决了Vision Transformers知识蒸馏中的伪影问题，通过零空间引导的能量重分配实现了伪影抑制和信息保留的良好平衡，为视觉基础模型的蒸馏提供了新思路。

Abstract: Vision Transformers are widely adopted as the backbone of vision foundation models, but they are known to produce high-norm artifacts that degrade representation quality. When knowledge distillation transfers these features to students, high-norm artifacts dominate the objective, so students overfit to artifacts and underweight informative signals, diminishing the gains from larger models. Prior work attempted to remove artifacts but encountered an inherent trade-off between artifact suppression and preserving informative signals from teachers. To address this, we introduce Singular Nullspace-Guided Energy Reallocation (SiNGER), a novel distillation framework that suppresses artifacts while preserving informative signals. The key idea is principled teacher feature refinement: during refinement, we leverage the nullspace-guided perturbation to preserve information while suppressing artifacts. Then, the refined teacher's features are distilled to a student. We implement this perturbation efficiently with a LoRA-based adapter that requires minimal structural modification. Extensive experiments show that \oursname consistently improves student models, achieving state-of-the-art performance in multiple downstream tasks and producing clearer and more interpretable representations.

</details>


### [12] [A Single Neuron Works: Precise Concept Erasure in Text-to-Image Diffusion Models](https://arxiv.org/abs/2509.21008)
*Qinqin He,Jiaqi Weng,Jialing Tao,Hui Xue*

Main category: cs.CV

TL;DR: SNCE是一种基于单神经元操作的概念擦除方法，通过稀疏自编码器将文本嵌入映射到解耦的潜在空间，精确识别并抑制有害概念对应的单个神经元，实现精准概念擦除同时保持图像质量


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法难以在精确移除目标概念的同时最小化图像质量退化，需要更精准的方法来防止有害内容生成

Method: 训练稀疏自编码器(SAE)将文本嵌入映射到稀疏解耦潜在空间，设计基于调制频率评分的神经元识别方法定位有害概念对应神经元，通过抑制该神经元激活实现概念擦除

Result: 在多个基准测试中达到最先进的目标概念擦除效果，保持模型对非目标概念的生成能力，对抗攻击鲁棒性显著优于现有方法

Conclusion: SNCE通过单神经元操作实现了精准的概念擦除，在安全性和图像质量保持方面表现出色，为文本到图像模型的安全应用提供了有效解决方案

Abstract: Text-to-image models exhibit remarkable capabilities in image generation. However, they also pose safety risks of generating harmful content. A key challenge of existing concept erasure methods is the precise removal of target concepts while minimizing degradation of image quality. In this paper, we propose Single Neuron-based Concept Erasure (SNCE), a novel approach that can precisely prevent harmful content generation by manipulating only a single neuron. Specifically, we train a Sparse Autoencoder (SAE) to map text embeddings into a sparse, disentangled latent space, where individual neurons align tightly with atomic semantic concepts. To accurately locate neurons responsible for harmful concepts, we design a novel neuron identification method based on the modulated frequency scoring of activation patterns. By suppressing activations of the harmful concept-specific neuron, SNCE achieves surgical precision in concept erasure with minimal disruption to image quality. Experiments on various benchmarks demonstrate that SNCE achieves state-of-the-art results in target concept erasure, while preserving the model's generation capabilities for non-target concepts. Additionally, our method exhibits strong robustness against adversarial attacks, significantly outperforming existing methods.

</details>


### [13] [UniTransfer: Video Concept Transfer via Progressive Spatial and Timestep Decomposition](https://arxiv.org/abs/2509.21086)
*Guojun Lei,Rong Zhang,Chi Wang,Tianhang Liu,Hong Li,Zhiyuan Ma,Weiwei Xu*

Main category: cs.CV

TL;DR: UniTransfer是一种新颖的视频概念迁移架构，通过空间和时间步分解实现精确可控的视频概念迁移。


<details>
  <summary>Details</summary>
Motivation: 现有的视频概念迁移方法在精确控制和编辑性方面存在局限，需要一种能够对视频不同组件进行细粒度控制的架构。

Method: 提出空间分解（前景主体、背景、运动流）和时间步分解（三阶段去噪过程），采用双到单流DiT架构和Chain-of-Prompt机制，利用LLM进行渐进式指导。

Result: 在OpenAnimal数据集上的实验表明，该方法在视觉保真度和可编辑性方面均超越现有基线方法。

Conclusion: UniTransfer通过分解策略实现了高质量、可控的视频概念迁移，为视频编辑领域提供了新的解决方案。

Abstract: We propose a novel architecture UniTransfer, which introduces both spatial and diffusion timestep decomposition in a progressive paradigm, achieving precise and controllable video concept transfer. Specifically, in terms of spatial decomposition, we decouple videos into three key components: the foreground subject, the background, and the motion flow. Building upon this decomposed formulation, we further introduce a dual-to-single-stream DiT-based architecture for supporting fine-grained control over different components in the videos. We also introduce a self-supervised pretraining strategy based on random masking to enhance the decomposed representation learning from large-scale unlabeled video data. Inspired by the Chain-of-Thought reasoning paradigm, we further revisit the denoising diffusion process and propose a Chain-of-Prompt (CoP) mechanism to achieve the timestep decomposition. We decompose the denoising process into three stages of different granularity and leverage large language models (LLMs) for stage-specific instructions to guide the generation progressively. We also curate an animal-centric video dataset called OpenAnimal to facilitate the advancement and benchmarking of research in video concept transfer. Extensive experiments demonstrate that our method achieves high-quality and controllable video concept transfer across diverse reference images and scenes, surpassing existing baselines in both visual fidelity and editability. Web Page: https://yu-shaonian.github.io/UniTransfer-Web/

</details>


### [14] [MotionFlow:Learning Implicit Motion Flow for Complex Camera Trajectory Control in Video Generation](https://arxiv.org/abs/2509.21119)
*Guojun Lei,Chi Wang,Yikai Wang,Hong Li,Ying Song,Weiwei Xu*

Main category: cs.CV

TL;DR: 提出了一种将相机和物体运动统一转换为像素运动的新方法，通过稳定扩散网络学习参考运动图，结合语义对象先验生成符合指定相机轨迹的视频


<details>
  <summary>Details</summary>
Motivation: 现有方法通常分别学习相机和物体运动，容易导致相对运动混淆。为解决相机轨迹引导视频生成中的一致性和泛化性问题

Method: 将相机和物体运动转换为像素运动，使用稳定扩散网络学习参考运动图，结合语义对象先验输入图像到视频网络生成视频

Result: 大量实验验证模型显著优于当前最先进方法

Conclusion: 该方法能准确跟随指定相机轨迹同时保持一致的物体运动，解决了现有方法的局限性

Abstract: Generating videos guided by camera trajectories poses significant challenges in achieving consistency and generalizability, particularly when both camera and object motions are present. Existing approaches often attempt to learn these motions separately, which may lead to confusion regarding the relative motion between the camera and the objects. To address this challenge, we propose a novel approach that integrates both camera and object motions by converting them into the motion of corresponding pixels. Utilizing a stable diffusion network, we effectively learn reference motion maps in relation to the specified camera trajectory. These maps, along with an extracted semantic object prior, are then fed into an image-to-video network to generate the desired video that can accurately follow the designated camera trajectory while maintaining consistent object motions. Extensive experiments verify that our model outperforms SOTA methods by a large margin.

</details>


### [15] [Evaluating the Evaluators: Metrics for Compositional Text-to-Image Generation](https://arxiv.org/abs/2509.21227)
*Seyed Amir Kasaei,Ali Aghayari,Arash Marioriyad,Niki Sepasian,MohammadAmin Fazli,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: 本文对文本-图像生成评估中广泛使用的自动化指标进行了全面研究，分析了这些指标与人类判断的一致性程度。研究发现没有单一指标在所有任务中表现一致，性能因组合问题的类型而异。


<details>
  <summary>Details</summary>
Motivation: 文本-图像生成技术发展迅速，但评估输出是否真正捕捉到提示中的对象、属性和关系仍是一个核心挑战。当前评估主要依赖自动化指标，但这些指标往往基于惯例或流行度采用，而非经过人类判断验证。

Method: 本研究对广泛使用的组合文本-图像评估指标进行了广泛分析，超越了简单的相关性分析，考察了它们在不同组合挑战中的行为，并比较了不同指标家族与人类判断的一致性。

Result: 结果显示：1）没有单一指标在所有任务中表现一致；2）VQA-based指标虽然流行但并非普遍优越；3）某些基于嵌入的指标在特定情况下表现更强；4）仅基于图像的指标对组合评估贡献很小。

Conclusion: 这些发现强调了在可信评估和作为生成奖励模型使用时，需要谨慎和透明地选择指标。评估指标的选择直接影响该领域的评估和报告进展。

Abstract: Text-image generation has advanced rapidly, but assessing whether outputs truly capture the objects, attributes, and relations described in prompts remains a central challenge. Evaluation in this space relies heavily on automated metrics, yet these are often adopted by convention or popularity rather than validated against human judgment. Because evaluation and reported progress in the field depend directly on these metrics, it is critical to understand how well they reflect human preferences. To address this, we present a broad study of widely used metrics for compositional text-image evaluation. Our analysis goes beyond simple correlation, examining their behavior across diverse compositional challenges and comparing how different metric families align with human judgments. The results show that no single metric performs consistently across tasks: performance varies with the type of compositional problem. Notably, VQA-based metrics, though popular, are not uniformly superior, while certain embedding-based metrics prove stronger in specific cases. Image-only metrics, as expected, contribute little to compositional evaluation, as they are designed for perceptual quality rather than alignment. These findings underscore the importance of careful and transparent metric selection, both for trustworthy evaluation and for their use as reward models in generation. Project page is available at \href{https://amirkasaei.com/eval-the-evals/}{this URL}.

</details>


### [16] [Dense Semantic Matching with VGGT Prior](https://arxiv.org/abs/2509.21263)
*Songlin Yang,Tianyi Wei,Yushi Lan,Zeqi Xiao,Anyi Rao,Xingang Pan*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D几何基础模型VGGT的语义匹配方法，解决了现有方法在几何歧义和最近邻规则方面的局限性，通过保留VGGT内在优势并适应语义匹配场景，实现了优越的几何感知和匹配可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有语义匹配方法存在两个主要问题：(i) 几何歧义：依赖2D基础模型特征难以区分对称结构，需要额外微调但缺乏泛化能力；(ii) 最近邻规则：像素级匹配忽略跨图像不可见性和流形保持。这些问题需要几何感知的像素描述符和整体密集对应机制。

Method: 提出一种方法：(i) 保留VGGT内在优势，重用早期特征阶段，微调后期阶段，并添加语义头用于双向对应；(ii) 在数据稀缺情况下通过循环一致性训练策略、合成数据增强和渐进式训练方法（包括混叠伪影缓解）来适应语义匹配场景。

Result: 大量实验表明，该方法在几何感知、匹配可靠性和流形保持方面表现优越，超越了之前的基线方法。

Conclusion: 通过利用3D几何基础模型VGGT并针对语义匹配场景进行适配，成功解决了现有方法的局限性，为语义匹配任务提供了更有效的解决方案。

Abstract: Semantic matching aims to establish pixel-level correspondences between instances of the same category and represents a fundamental task in computer vision. Existing approaches suffer from two limitations: (i) Geometric Ambiguity: Their reliance on 2D foundation model features (e.g., Stable Diffusion, DINO) often fails to disambiguate symmetric structures, requiring extra fine-tuning yet lacking generalization; (ii) Nearest-Neighbor Rule: Their pixel-wise matching ignores cross-image invisibility and neglects manifold preservation. These challenges call for geometry-aware pixel descriptors and holistic dense correspondence mechanisms. Inspired by recent advances in 3D geometric foundation models, we turn to VGGT, which provides geometry-grounded features and holistic dense matching capabilities well aligned with these needs. However, directly transferring VGGT is challenging, as it was originally designed for geometry matching within cross views of a single instance, misaligned with cross-instance semantic matching, and further hindered by the scarcity of dense semantic annotations. To address this, we propose an approach that (i) retains VGGT's intrinsic strengths by reusing early feature stages, fine-tuning later ones, and adding a semantic head for bidirectional correspondences; and (ii) adapts VGGT to the semantic matching scenario under data scarcity through cycle-consistent training strategy, synthetic data augmentation, and progressive training recipe with aliasing artifact mitigation. Extensive experiments demonstrate that our approach achieves superior geometry awareness, matching reliability, and manifold preservation, outperforming previous baselines.

</details>


### [17] [MedVSR: Medical Video Super-Resolution with Cross State-Space Propagation](https://arxiv.org/abs/2509.21265)
*Xinyu Liu,Guolei Sun,Cheng Wang,Yixuan Yuan,Ender Konukoglu*

Main category: cs.CV

TL;DR: MedVSR是一个专门针对医学视频超分辨率的框架，通过交叉状态空间传播解决对齐问题，使用内部状态空间重建模块增强组织结构并减少伪影。


<details>
  <summary>Details</summary>
Motivation: 高分辨率医学视频对准确诊断至关重要，但硬件限制和生理约束使其难以获取。现有VSR模型在处理医学视频时面临相机抖动、噪声、帧间突变等挑战，容易产生伪影和扭曲特征。

Method: 提出MedVSR框架：1）交叉状态空间传播（CSSP）通过状态空间模型将远距离帧投影为控制矩阵，选择性传播一致信息以解决对齐问题；2）内部状态空间重建（ISSR）模块结合长距离空间特征学习和大核短距离信息聚合来增强组织结构。

Result: 在四个医学场景数据集（包括内窥镜和白内障手术）上的实验表明，MedVSR在重建性能和效率上显著优于现有VSR模型。

Conclusion: MedVSR通过专门设计的对齐和重建模块，有效解决了医学视频超分辨率的独特挑战，为临床诊断提供了更可靠的视频增强方案。

Abstract: High-resolution (HR) medical videos are vital for accurate diagnosis, yet are hard to acquire due to hardware limitations and physiological constraints. Clinically, the collected low-resolution (LR) medical videos present unique challenges for video super-resolution (VSR) models, including camera shake, noise, and abrupt frame transitions, which result in significant optical flow errors and alignment difficulties. Additionally, tissues and organs exhibit continuous and nuanced structures, but current VSR models are prone to introducing artifacts and distorted features that can mislead doctors. To this end, we propose MedVSR, a tailored framework for medical VSR. It first employs Cross State-Space Propagation (CSSP) to address the imprecise alignment by projecting distant frames as control matrices within state-space models, enabling the selective propagation of consistent and informative features to neighboring frames for effective alignment. Moreover, we design an Inner State-Space Reconstruction (ISSR) module that enhances tissue structures and reduces artifacts with joint long-range spatial feature learning and large-kernel short-range information aggregation. Experiments across four datasets in diverse medical scenarios, including endoscopy and cataract surgeries, show that MedVSR significantly outperforms existing VSR models in reconstruction performance and efficiency. Code released at https://github.com/CUHK-AIM-Group/MedVSR.

</details>


### [18] [NewtonGen: Physics-Consistent and Controllable Text-to-Video Generation via Neural Newtonian Dynamics](https://arxiv.org/abs/2509.21309)
*Yu Yuan,Xijun Wang,Tharindu Wickremasinghe,Zeeshan Nadir,Bole Ma,Stanley H. Chan*

Main category: cs.CV

TL;DR: NewtonGen是一个结合数据驱动合成与可学习物理原理的框架，通过可训练的神经牛顿动力学实现物理一致的视频生成和精确参数控制


<details>
  <summary>Details</summary>
Motivation: 当前大规模文本到视频生成的主要瓶颈是物理一致性和可控性，现有模型常产生不真实的运动且缺乏精确参数控制

Method: 提出NewtonGen框架，核心是可训练的神经牛顿动力学，能够建模和预测各种牛顿运动，将潜在动力学约束注入视频生成过程

Result: 通过联合利用数据先验和动力学指导，NewtonGen实现了物理一致的视频合成和精确参数控制

Conclusion: 该框架解决了当前模型仅从外观学习运动分布而缺乏对底层动力学理解的根本限制

Abstract: A primary bottleneck in large-scale text-to-video generation today is physical consistency and controllability. Despite recent advances, state-of-the-art models often produce unrealistic motions, such as objects falling upward, or abrupt changes in velocity and direction. Moreover, these models lack precise parameter control, struggling to generate physically consistent dynamics under different initial conditions. We argue that this fundamental limitation stems from current models learning motion distributions solely from appearance, while lacking an understanding of the underlying dynamics. In this work, we propose NewtonGen, a framework that integrates data-driven synthesis with learnable physical principles. At its core lies trainable Neural Newtonian Dynamics (NND), which can model and predict a variety of Newtonian motions, thereby injecting latent dynamical constraints into the video generation process. By jointly leveraging data priors and dynamical guidance, NewtonGen enables physically consistent video synthesis with precise parameter control.

</details>


### [19] [SD3.5-Flash: Distribution-Guided Distillation of Generative Flows](https://arxiv.org/abs/2509.21318)
*Hmrishav Bandyopadhyay,Rahim Entezari,Jim Scott,Reshinth Adithyan,Yi-Zhe Song,Varun Jampani*

Main category: cs.CV

TL;DR: SD3.5-Flash是一个高效的少步蒸馏框架，可将高质量图像生成技术部署到消费级设备上


<details>
  <summary>Details</summary>
Motivation: 让计算密集的整流流模型能够在资源受限的消费设备上运行，实现生成式AI的真正普及

Method: 采用重新制定的分布匹配目标进行少步蒸馏，引入时间步共享减少梯度噪声和分时步微调改善提示对齐，结合文本编码器重构和专用量化等管道优化

Result: SD3.5-Flash在包括大规模用户研究在内的广泛评估中，持续优于现有的少步方法

Conclusion: 该框架实现了从手机到台式机的全设备覆盖，使先进的生成式AI能够真正实用化部署

Abstract: We present SD3.5-Flash, an efficient few-step distillation framework that brings high-quality image generation to accessible consumer devices. Our approach distills computationally prohibitive rectified flow models through a reformulated distribution matching objective tailored specifically for few-step generation. We introduce two key innovations: "timestep sharing" to reduce gradient noise and "split-timestep fine-tuning" to improve prompt alignment. Combined with comprehensive pipeline optimizations like text encoder restructuring and specialized quantization, our system enables both rapid generation and memory-efficient deployment across different hardware configurations. This democratizes access across the full spectrum of devices, from mobile phones to desktop computers. Through extensive evaluation including large-scale user studies, we demonstrate that SD3.5-Flash consistently outperforms existing few-step methods, making advanced generative AI truly accessible for practical deployment.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [20] [Beyond Visual Similarity: Rule-Guided Multimodal Clustering with explicit domain rules](https://arxiv.org/abs/2509.20501)
*Kishor Datta Gupta,Mohd Ariful Haque,Marufa Kamal,Ahmed Rafi Hasan,Md. Mahfuzur Rahman,Roy George*

Main category: cs.LG

TL;DR: DARTVAE是一个规则引导的多模态聚类框架，通过将领域特定约束嵌入到表示学习中，结合VAE架构和LLM生成的规则知识图谱，实现更符合领域语义的聚类分析。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法仅依赖输入数据的相似性，无法有效捕捉领域特定的结构或语义约束，限制了在复杂领域中的应用效果。

Method: 扩展VAE架构，嵌入显式规则、语义表示和数据驱动特征到统一潜在空间，通过规则一致性和违反惩罚在损失函数中强制约束合规性。规则由LLM生成并构建为知识图谱。

Result: 在飞机和汽车数据集上的实验表明，规则引导聚类产生更具操作意义和可解释性的聚类结果（如分离无人机、统一隐形飞机、区分SUV和轿车），同时改善了传统聚类指标。

Conclusion: DARTVAE通过结合规则编码和学习表示，比纯数据驱动模型实现更有意义和一致的聚类结果，但面临LLM规则幻觉、规则冲突、过度拟合和计算复杂度等挑战。

Abstract: Traditional clustering techniques often rely solely on similarity in the input data, limiting their ability to capture structural or semantic constraints that are critical in many domains. We introduce the Domain Aware Rule Triggered Variational Autoencoder (DARTVAE), a rule guided multimodal clustering framework that incorporates domain specific constraints directly into the representation learning process. DARTVAE extends the VAE architecture by embedding explicit rules, semantic representations, and data driven features into a unified latent space, while enforcing constraint compliance through rule consistency and violation penalties in the loss function. Unlike conventional clustering methods that rely only on visual similarity or apply rules as post hoc filters, DARTVAE treats rules as first class learning signals. The rules are generated by LLMs, structured into knowledge graphs, and enforced through a loss function combining reconstruction, KL divergence, consistency, and violation penalties. Experiments on aircraft and automotive datasets demonstrate that rule guided clustering produces more operationally meaningful and interpretable clusters for example, isolating UAVs, unifying stealth aircraft, or separating SUVs from sedans while improving traditional clustering metrics. However, the framework faces challenges: LLM generated rules may hallucinate or conflict, excessive rules risk overfitting, and scaling to complex domains increases computational and consistency difficulties. By combining rule encodings with learned representations, DARTVAE achieves more meaningful and consistent clustering outcomes than purely data driven models, highlighting the utility of constraint guided multimodal clustering for complex, knowledge intensive settings.

</details>


### [21] [FHRFormer: A Self-supervised Transformer Approach for Fetal Heart Rate Inpainting and Forecasting](https://arxiv.org/abs/2509.20852)
*Kjersti Engan,Neel Kanwal,Anita Yeconia,Ladislaus Blacy,Yuda Munyaw,Estomih Mduma,Hege Ersdal*

Main category: cs.LG

TL;DR: 本文提出了一种基于掩码变换器自编码器的方法，用于重建缺失的胎儿心率信号，通过捕捉数据的空间和频率特征来提高AI风险预测算法的准确性。


<details>
  <summary>Details</summary>
Motivation: 大约10%的新生儿需要呼吸辅助，5%需要通气支持。胎儿心率监测在产前护理中至关重要，但可穿戴设备在孕妇运动时容易出现信号丢失，传统插值方法无法保持信号频谱特征，限制了AI分析效果。

Method: 使用掩码变换器自编码器方法重建缺失的FHR信号，该方法能够同时捕捉数据的空间和频率成分，对不同程度的缺失数据具有鲁棒性，可用于信号修复和预测。

Result: 该方法在不同缺失持续时间下表现出鲁棒性，能够有效重建FHR信号，支持AI风险算法开发。

Conclusion: 该方法可应用于回顾性研究数据集，未来有望集成到可穿戴FHR监测设备中，实现更早、更稳健的风险检测。

Abstract: Approximately 10\% of newborns require assistance to initiate breathing at birth, and around 5\% need ventilation support. Fetal heart rate (FHR) monitoring plays a crucial role in assessing fetal well-being during prenatal care, enabling the detection of abnormal patterns and supporting timely obstetric interventions to mitigate fetal risks during labor. Applying artificial intelligence (AI) methods to analyze large datasets of continuous FHR monitoring episodes with diverse outcomes may offer novel insights into predicting the risk of needing breathing assistance or interventions. Recent advances in wearable FHR monitors have enabled continuous fetal monitoring without compromising maternal mobility. However, sensor displacement during maternal movement, as well as changes in fetal or maternal position, often lead to signal dropouts, resulting in gaps in the recorded FHR data. Such missing data limits the extraction of meaningful insights and complicates automated (AI-based) analysis. Traditional approaches to handle missing data, such as simple interpolation techniques, often fail to preserve the spectral characteristics of the signals. In this paper, we propose a masked transformer-based autoencoder approach to reconstruct missing FHR signals by capturing both spatial and frequency components of the data. The proposed method demonstrates robustness across varying durations of missing data and can be used for signal inpainting and forecasting. The proposed approach can be applied retrospectively to research datasets to support the development of AI-based risk algorithms. In the future, the proposed method could be integrated into wearable FHR monitoring devices to achieve earlier and more robust risk detection.

</details>


### [22] [A Unified Framework for Diffusion Model Unlearning with f-Divergence](https://arxiv.org/abs/2509.21167)
*Nicola Novello,Federico Fontana,Luigi Cinque,Deniz Gunduz,Andrea M. Tonello*

Main category: cs.LG

TL;DR: 本文提出了一个基于f-散度的统一框架，用于扩散模型的机器遗忘，展示了MSE方法只是该框架的特例，并分析了不同f-散度在遗忘效果和概念保留之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像模型遗忘方法主要依赖MSE损失，但这种方法存在局限性。作者希望建立一个更通用的框架，允许使用不同的f-散度来优化遗忘过程。

Method: 提出了基于f-散度的统一框架，将MSE方法视为特例。通过分析不同f-散度的性质，研究其对算法收敛性和遗忘质量的影响。

Result: 该框架提供了灵活性，可以根据具体应用选择最优散度，在激进遗忘和概念保留之间实现更好的平衡。

Conclusion: f-散度框架为扩散模型遗忘提供了更通用的解决方案，超越了传统MSE方法的限制，允许根据需求定制最优的遗忘策略。

Abstract: Machine unlearning aims to remove specific knowledge from a trained model. While diffusion models (DMs) have shown remarkable generative capabilities, existing unlearning methods for text-to-image (T2I) models often rely on minimizing the mean squared error (MSE) between the output distribution of a target and an anchor concept. We show that this MSE-based approach is a special case of a unified $f$-divergence-based framework, in which any $f$-divergence can be utilized. We analyze the benefits of using different $f$-divergences, that mainly impact the convergence properties of the algorithm and the quality of unlearning. The proposed unified framework offers a flexible paradigm that allows to select the optimal divergence for a specific application, balancing different trade-offs between aggressive unlearning and concept preservation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [23] [KeyWorld: Key Frame Reasoning Enables Effective and Efficient World Models](https://arxiv.org/abs/2509.21027)
*Sibo Li,Qianyue Hao,Yu Shang,Yong Li*

Main category: cs.RO

TL;DR: KeyWorld是一个改进文本条件机器人世界模型的框架，通过将Transformer计算集中在少量语义关键帧上，同时使用轻量级卷积模型填充中间帧，实现了5.68倍的加速和更好的物理有效性。


<details>
  <summary>Details</summary>
Motivation: 当前机器人世界模型存在推理速度慢和生成轨迹物理合理性不足的问题，主要原因是帧到帧生成方法的冗余计算以及忽视关键过渡的语义重要性。

Method: KeyWorld首先通过迭代简化机器人运动轨迹识别关键帧，然后训练DiT模型从文本任务描述生成这些物理意义关键帧，最后使用轻量级插值器高效重建完整视频。

Result: 在LIBERO基准测试中，KeyWorld相比帧到帧生成基线实现了5.68倍加速，且关注运动感知关键帧进一步提升了生成视频的物理有效性，特别是在复杂任务上。

Conclusion: 该方法为在实时机器人控制和其他需要高效有效世界模型的领域中部署世界模型提供了一条实用路径。

Abstract: Robotic world models are a promising paradigm for forecasting future environment states, yet their inference speed and the physical plausibility of generated trajectories remain critical bottlenecks, limiting their real-world applications. This stems from the redundancy of the prevailing frame-to-frame generation approach, where the model conducts costly computation on similar frames, as well as neglecting the semantic importance of key transitions. To address this inefficiency, we propose KeyWorld, a framework that improves text-conditioned robotic world models by concentrating transformers computation on a few semantic key frames while employing a lightweight convolutional model to fill the intermediate frames. Specifically, KeyWorld first identifies significant transitions by iteratively simplifying the robot's motion trajectories, obtaining the ground truth key frames. Then, a DiT model is trained to reason and generate these physically meaningful key frames from textual task descriptions. Finally, a lightweight interpolator efficiently reconstructs the full video by inpainting all intermediate frames. Evaluations on the LIBERO benchmark demonstrate that KeyWorld achieves a 5.68$\times$ acceleration compared to the frame-to-frame generation baseline, and focusing on the motion-aware key frames further contributes to the physical validity of the generated videos, especially on complex tasks. Our approach highlights a practical path toward deploying world models in real-time robotic control and other domains requiring both efficient and effective world models. Code is released at https://anonymous.4open.science/r/Keyworld-E43D.

</details>
