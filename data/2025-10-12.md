<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 7]
- [cs.CV](#cs.CV) [Total: 33]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [SpotDiff: Spotting and Disentangling Interference in Feature Space for Subject-Preserving Image Generation](https://arxiv.org/abs/2510.07340)
*Yongzhi Li,Saining Zhang,Yibing Chen,Boying Li,Yanxin Zhang,Xiaoyu Du*

Main category: cs.GR

TL;DR: SpotDiff是一种基于学习的方法，通过识别和解缠干扰来提取特定主题的特征，在保持主题身份的同时实现可控编辑。


<details>
  <summary>Details</summary>
Motivation: 现有优化方法计算成本高，而学习方法效率高但受干扰因素影响导致特征纠缠。需要一种既能高效生成又能保持主题身份的方法。

Method: 利用预训练的CLIP图像编码器和专门的姿态、背景专家网络，通过特征空间正交性约束来分离主题身份。创建SpotDiff10k数据集进行训练。

Result: SpotDiff在主题保持和可控编辑方面比现有方法更鲁棒，仅用10k训练样本就达到竞争性性能。

Conclusion: SpotDiff通过解缠干扰实现了高效且忠实的个性化图像生成，在保持主题身份的同时支持可控编辑。

Abstract: Personalized image generation aims to faithfully preserve a reference subject's identity while adapting to diverse text prompts. Existing optimization-based methods ensure high fidelity but are computationally expensive, while learning-based approaches offer efficiency at the cost of entangled representations influenced by nuisance factors. We introduce SpotDiff, a novel learning-based method that extracts subject-specific features by spotting and disentangling interference. Leveraging a pre-trained CLIP image encoder and specialized expert networks for pose and background, SpotDiff isolates subject identity through orthogonality constraints in the feature space. To enable principled training, we introduce SpotDiff10k, a curated dataset with consistent pose and background variations. Experiments demonstrate that SpotDiff achieves more robust subject preservation and controllable editing than prior methods, while attaining competitive performance with only 10k training samples.

</details>


### [2] [Local MAP Sampling for Diffusion Models](https://arxiv.org/abs/2510.07343)
*Shaorong Zhang,Rob Brekelmans,Greg Ver Steeg*

Main category: cs.GR

TL;DR: LMAPS是一种新的扩散模型推理框架，通过迭代求解扩散轨迹上的局部MAP子问题，为基于优化的方法提供统一的概率解释，在图像恢复和科学任务中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散后验采样方法虽然提供了贝叶斯框架，但在实际逆问题中，优化方法往往表现更好但缺乏明确的概率基础。需要建立连接概率采样和优化方法的统一框架。

Method: 提出局部MAP采样框架，在扩散轨迹上迭代求解局部MAP子问题；开发了概率可解释的协方差近似、重新制定的目标函数以提高稳定性和可解释性，以及针对不可微算子的梯度近似。

Result: 在广泛的图像恢复和科学任务中实现最先进性能：运动去模糊、JPEG恢复和量化任务上获得≥2dB增益，逆散射基准上获得>1.5dB改进。

Conclusion: LMAPS为基于优化的扩散求解器提供了统一的概率解释，弥合了概率采样和优化方法之间的差距，在实际逆问题中实现了卓越的重建精度。

Abstract: Diffusion Posterior Sampling (DPS) provides a principled Bayesian approach to inverse problems by sampling from $p(x_0 \mid y)$. However, in practice, the goal of inverse problem solving is not to cover the posterior but to recover the most accurate reconstruction, where optimization-based diffusion solvers often excel despite lacking a clear probabilistic foundation. We introduce Local MAP Sampling (LMAPS), a new inference framework that iteratively solving local MAP subproblems along the diffusion trajectory. This perspective clarifies their connection to global MAP estimation and DPS, offering a unified probabilistic interpretation for optimization-based methods. Building on this foundation, we develop practical algorithms with a probabilistically interpretable covariance approximation, a reformulated objective for stability and interpretability, and a gradient approximation for non-differentiable operators. Across a broad set of image restoration and scientific tasks, LMAPS achieves state-of-the-art performance, including $\geq 2$ dB gains on motion deblurring, JPEG restoration, and quantization, and $>1.5$ dB improvements on inverse scattering benchmarks.

</details>


### [3] [NRRS: Neural Russian Roulette and Splitting](https://arxiv.org/abs/2510.07868)
*Haojie Jin,Jierui Ren,Yisong Chen,Guoping Wang,Sheng Li*

Main category: cs.GR

TL;DR: 提出了一种专为波前路径追踪设计的新型俄罗斯轮盘赌和分裂框架，通过归一化RRS公式和神经网络学习RRS因子，解决了传统方法与波前架构不兼容的问题。


<details>
  <summary>Details</summary>
Motivation: 传统RRS方法由于路径数量不可预测，与波前路径追踪的预分配内存和调度需求存在根本性不兼容，需要开发适配该架构的RRS解决方案。

Method: 引入归一化RRS公式确保路径数量有界；提出两种神经网络模型（NRRS和AID-NRRS）学习RRS因子；设计Mix-Depth机制根据路径深度自适应调节神经评估。

Result: 在多种复杂场景中，该方法在渲染质量和性能方面均优于传统启发式方法和近期RRS技术。

Conclusion: 所提出的框架成功解决了RRS与波前路径追踪的兼容性问题，通过神经网络学习和自适应机制实现了更优的渲染效果和效率。

Abstract: We propose a novel framework for Russian Roulette and Splitting (RRS) tailored to wavefront path tracing, a highly parallel rendering architecture that processes path states in batched, stage-wise execution for efficient GPU utilization. Traditional RRS methods, with unpredictable path counts, are fundamentally incompatible with wavefront's preallocated memory and scheduling requirements. To resolve this, we introduce a normalized RRS formulation with a bounded path count, enabling stable and memory-efficient execution.   Furthermore, we pioneer the use of neural networks to learn RRS factors, presenting two models: NRRS and AID-NRRS. At a high level, both feature a carefully designed RRSNet that explicitly incorporates RRS normalization, with only subtle differences in their implementation. To balance computational cost and inference accuracy, we introduce Mix-Depth, a path-depth-aware mechanism that adaptively regulates neural evaluation, further improving efficiency.   Extensive experiments demonstrate that our method outperforms traditional heuristics and recent RRS techniques in both rendering quality and performance across a variety of complex scenes.

</details>


### [4] [Variable-Rate Texture Compression: Real-Time Rendering with JPEG](https://arxiv.org/abs/2510.08166)
*Elias Kristmann,Markus Schütz,Michael Wimmer*

Main category: cs.GR

TL;DR: 本文研究了在现代GPU上使用JPEG格式进行可变速率纹理压缩的可行性，并与固定速率压缩方法BC1和ASTC进行比较，展示了JPEG在质量和压缩率方面的优势。


<details>
  <summary>Details</summary>
Motivation: 虽然可变速率压缩图像格式如JPEG被广泛用于高效编码图像，但由于需要随机访问单个纹理像素等特殊要求，它们尚未在实时渲染中得到应用。本文旨在探索在现代GPU上实现可变速率纹理压缩的可能性。

Method: 使用延迟渲染管线识别每帧所需的纹理块子集，解码这些块并为帧缓冲器的像素着色。该方法需要额外的约0.17位/像素开销。

Result: 与BC1相比，JPEG保持了显著更好的质量和压缩率；根据图像类型，JPEG优于或与ASTC竞争。在RTX 4090上，JPEG渲染管线仅增加不到0.3毫秒的渲染时间。

Conclusion: 研究表明，复杂的可变速率压缩方案在现代GPU上是可行的，即使在VR应用中也是如此，为实时渲染中的高质量纹理压缩提供了新途径。

Abstract: Although variable-rate compressed image formats such as JPEG are widely used to efficiently encode images, they have not found their way into real-time rendering due to special requirements such as random access to individual texels. In this paper, we investigate the feasibility of variable-rate texture compression on modern GPUs using the JPEG format, and how it compares to the GPU-friendly fixed-rate compression approaches BC1 and ASTC. Using a deferred rendering pipeline, we are able to identify the subset of blocks that are needed for a given frame, decode these, and colorize the framebuffer's pixels. Despite the additional $\sim$0.17 bit per pixel that we require for our approach, JPEG maintains significantly better quality and compression rates compared to BC1, and depending on the type of image, outperforms or competes with ASTC. The JPEG rendering pipeline increases rendering duration by less than 0.3 ms on an RTX 4090, demonstrating that sophisticated variable-rate compression schemes are feasible on modern GPUs, even in VR. Source code and data sets are available at: https://github.com/elias1518693/jpeg_textures

</details>


### [5] [SViM3D: Stable Video Material Diffusion for Single Image 3D Generation](https://arxiv.org/abs/2510.08271)
*Andreas Engelhardt,Mark Boss,Vikram Voletti,Chun-Han Yao,Hendrik P. A. Lensch,Varun Jampani*

Main category: cs.GR

TL;DR: SViM3D是一个从单张图像预测多视角一致PBR材质和法线的框架，通过视频扩散模型实现3D资产生成，支持重光照和外观编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从单张图像重建3D物体时，反射率仍由简单材质模型表示或需要额外步骤估计，限制了重光照和外观编辑能力。

Method: 扩展潜在视频扩散模型，基于显式相机控制联合输出空间变化的PBR参数、法线和生成视图，并引入多种机制改进质量。

Result: 在多个以物体为中心的数据集上实现了最先进的重光照和新视角合成性能，能够泛化到多样化输入。

Conclusion: 该方法能够生成可用于AR/VR、电影、游戏等视觉媒体的可重光照3D资产。

Abstract: We present Stable Video Materials 3D (SViM3D), a framework to predict multi-view consistent physically based rendering (PBR) materials, given a single image. Recently, video diffusion models have been successfully used to reconstruct 3D objects from a single image efficiently. However, reflectance is still represented by simple material models or needs to be estimated in additional steps to enable relighting and controlled appearance edits. We extend a latent video diffusion model to output spatially varying PBR parameters and surface normals jointly with each generated view based on explicit camera control. This unique setup allows for relighting and generating a 3D asset using our model as neural prior. We introduce various mechanisms to this pipeline that improve quality in this ill-posed setting. We show state-of-the-art relighting and novel view synthesis performance on multiple object-centric datasets. Our method generalizes to diverse inputs, enabling the generation of relightable 3D assets useful in AR/VR, movies, games and other visual media.

</details>


### [6] [Splat the Net: Radiance Fields with Splattable Neural Primitives](https://arxiv.org/abs/2510.08491)
*Xilong Zhou,Bao-Huy Nguyen,Loïc Magne,Vladislav Golyanik,Thomas Leimkühler,Christian Theobalt*

Main category: cs.GR

TL;DR: 提出了一种可拼接的神经基元表示方法，将神经模型的表达能力与基元拼接的效率相结合，在保持实时渲染质量的同时显著减少了基元数量和参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，神经辐射场表达能力强但渲染成本高，而3D高斯拼接渲染效率高但表达能力有限。需要一种既能保持高质量表达又能实现实时渲染的新表示方法。

Method: 每个基元编码一个有界的神经密度场，通过浅层神经网络参数化。该公式允许对线积分进行精确解析求解，从而支持无需光线行进的高效拼接计算。

Result: 在新视角合成基准测试中，该方法在质量和速度上与3D高斯拼接相当，但使用的基元数量减少10倍，参数数量减少6倍。

Conclusion: 提出的可拼接神经基元表示方法成功平衡了表达能力和渲染效率，无需依赖复杂的控制或适应框架即可实现显著改进。

Abstract: Radiance fields have emerged as a predominant representation for modeling 3D scene appearance. Neural formulations such as Neural Radiance Fields provide high expressivity but require costly ray marching for rendering, whereas primitive-based methods such as 3D Gaussian Splatting offer real-time efficiency through splatting, yet at the expense of representational power. Inspired by advances in both these directions, we introduce splattable neural primitives, a new volumetric representation that reconciles the expressivity of neural models with the efficiency of primitive-based splatting. Each primitive encodes a bounded neural density field parameterized by a shallow neural network. Our formulation admits an exact analytical solution for line integrals, enabling efficient computation of perspectively accurate splatting kernels. As a result, our representation supports integration along view rays without the need for costly ray marching. The primitives flexibly adapt to scene geometry and, being larger than prior analytic primitives, reduce the number required per scene. On novel-view synthesis benchmarks, our approach matches the quality and speed of 3D Gaussian Splatting while using $10\times$ fewer primitives and $6\times$ fewer parameters. These advantages arise directly from the representation itself, without reliance on complex control or adaptation frameworks. The project page is https://vcai.mpi-inf.mpg.de/projects/SplatNet/.

</details>


### [7] [X2Video: Adapting Diffusion Models for Multimodal Controllable Neural Video Rendering](https://arxiv.org/abs/2510.08530)
*Zhitong Huang,Mohan Zhang,Renhan Wang,Rui Tang,Hao Zhu,Jing Liao*

Main category: cs.GR

TL;DR: X2Video是首个基于内在通道（反照率、法线、粗糙度、金属度、辐照度）引导的扩散模型，能够生成逼真视频，同时支持参考图像和文本提示的多模态控制。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型缺乏对颜色、材质、几何和光照的精确控制能力，需要一种能够通过内在通道和多模态输入实现精确视频渲染的方法。

Method: 扩展XRGB图像生成模型到视频领域，采用混合自注意力确保时间一致性，开发掩码交叉注意力分离全局和局部文本提示，使用递归采样方法生成长视频。

Result: X2Video能够生成长时间、时间一致且逼真的视频，有效支持多模态控制，并允许通过参数调整对颜色、材质、几何和光照进行编辑。

Conclusion: X2Video通过内在通道引导和多模态控制，实现了对视频内容的精确操控，为逼真视频生成提供了新的解决方案。

Abstract: We present X2Video, the first diffusion model for rendering photorealistic videos guided by intrinsic channels including albedo, normal, roughness, metallicity, and irradiance, while supporting intuitive multi-modal controls with reference images and text prompts for both global and local regions. The intrinsic guidance allows accurate manipulation of color, material, geometry, and lighting, while reference images and text prompts provide intuitive adjustments in the absence of intrinsic information. To enable these functionalities, we extend the intrinsic-guided image generation model XRGB to video generation by employing a novel and efficient Hybrid Self-Attention, which ensures temporal consistency across video frames and also enhances fidelity to reference images. We further develop a Masked Cross-Attention to disentangle global and local text prompts, applying them effectively onto respective local and global regions. For generating long videos, our novel Recursive Sampling method incorporates progressive frame sampling, combining keyframe prediction and frame interpolation to maintain long-range temporal consistency while preventing error accumulation. To support the training of X2Video, we assembled a video dataset named InteriorVideo, featuring 1,154 rooms from 295 interior scenes, complete with reliable ground-truth intrinsic channel sequences and smooth camera trajectories. Both qualitative and quantitative evaluations demonstrate that X2Video can produce long, temporally consistent, and photorealistic videos guided by intrinsic conditions. Additionally, X2Video effectively accommodates multi-modal controls with reference images, global and local text prompts, and simultaneously supports editing on color, material, geometry, and lighting through parametric tuning. Project page: https://luckyhzt.github.io/x2video

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [8] [DynamicEval: Rethinking Evaluation for Dynamic Text-to-Video Synthesis](https://arxiv.org/abs/2510.07441)
*Nithin C. Babu,Aniruddha Mahapatra,Harsh Rangwani,Rajiv Soundararajan,Kuldeep Kulkarni*

Main category: cs.CV

TL;DR: 提出了DynamicEval基准，专注于动态相机运动的文本到视频评估，包含45k人工标注和新的背景一致性、前景一致性指标，在视频级和模型级评估中比现有方法提升2%以上。


<details>
  <summary>Details</summary>
Motivation: 现有T2V评估基准存在两个局限：忽视动态相机运动的重要性，以及仅关注模型级评分而忽略视频级评估。

Method: 构建包含动态相机运动提示的基准，收集45k人工标注；提出基于物体错误图的背景一致性指标和基于点跟踪的前景一致性指标。

Result: 新指标在视频级和模型级评估中与人类偏好相关性更强，比现有方法提升超过2个百分点。

Conclusion: DynamicEval为动态相机运动下的T2V模型评估提供了更全面的基准，新指标能有效解决现有方法的失败案例。

Abstract: Existing text-to-video (T2V) evaluation benchmarks, such as VBench and EvalCrafter, suffer from two limitations. (i) While the emphasis is on subject-centric prompts or static camera scenes, camera motion essential for producing cinematic shots and existing metrics under dynamic motion are largely unexplored. (ii) These benchmarks typically aggregate video-level scores into a single model-level score for ranking generative models. Such aggregation, however, overlook video-level evaluation, which is vital to selecting the better video among the candidate videos generated for a given prompt. To address these gaps, we introduce DynamicEval, a benchmark consisting of systematically curated prompts emphasizing dynamic camera motion, paired with 45k human annotations on video pairs from 3k videos generated by ten T2V models. DynamicEval evaluates two key dimensions of video quality: background scene consistency and foreground object consistency. For background scene consistency, we obtain the interpretable error maps based on the Vbench motion smoothness metric. We observe that while the Vbench motion smoothness metric shows promising alignment with human judgments, it fails in two cases: occlusions/disocclusions arising from camera and foreground object movements. Building on this, we propose a new background consistency metric that leverages object error maps to correct two failure cases in a principled manner. Our second innovation is the introduction of a foreground consistency metric that tracks points and their neighbors within each object instance to assess object fidelity. Extensive experiments demonstrate that our proposed metrics achieve stronger correlations with human preferences at both the video level and the model level (an improvement of more than 2% points), establishing DynamicEval as a more comprehensive benchmark for evaluating T2V models under dynamic camera motion.

</details>


### [9] [Provably Accelerated Imaging with Restarted Inertia and Score-based Image Priors](https://arxiv.org/abs/2510.07470)
*Marien Renaud,Julien Hermant,Deliang Wei,Yu Sun*

Main category: cs.CV

TL;DR: 提出了RISP方法，通过重启惯性机制加速收敛，同时使用基于分数的图像先验保证重建质量，在多种成像逆问题中实现快速收敛和高质量重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法如RED专注于设计复杂图像先验来提高重建质量，但收敛加速依赖启发式方法，缺乏理论保证。

Method: RISP结合了重启惯性机制和基于分数的图像先验，是RED的原则性扩展，通过连续时间动力学系统分析其与重球ODE的联系。

Result: RISP比RED获得更快的驻点收敛速率，且不要求图像先验的凸性，实验证明在多种成像逆问题中实现快速收敛和高质量重建。

Conclusion: RISP为成像逆问题提供了一种兼具快速收敛和高质量重建的解决方案，填补了现有方法的理论空白。

Abstract: Fast convergence and high-quality image recovery are two essential features of algorithms for solving ill-posed imaging inverse problems. Existing methods, such as regularization by denoising (RED), often focus on designing sophisticated image priors to improve reconstruction quality, while leaving convergence acceleration to heuristics. To bridge the gap, we propose Restarted Inertia with Score-based Priors (RISP) as a principled extension of RED. RISP incorporates a restarting inertia for fast convergence, while still allowing score-based image priors for high-quality reconstruction. We prove that RISP attains a faster stationary-point convergence rate than RED, without requiring the convexity of the image prior. We further derive and analyze the associated continuous-time dynamical system, offering insight into the connection between RISP and the heavy-ball ordinary differential equation (ODE). Experiments across a range of imaging inverse problems demonstrate that RISP enables fast convergence while achieving high-quality reconstructions.

</details>


### [10] [A Denoising Framework for Real-World Ultra-Low Dose Lung CT Images Based on an Image Purification Strategy](https://arxiv.org/abs/2510.07492)
*Guoliang Gong,Man Yu*

Main category: cs.CV

TL;DR: 提出基于图像净化策略的超低剂量CT去噪框架，通过生成结构对齐的图像对和数据驱动方法解决真实临床数据中的空间错位问题，在肺部CT数据集上取得SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 超低剂量CT显著降低辐射但引入严重噪声和伪影，导致与正常剂量CT图像对存在空间错位，现有去噪网络无法直接应用。

Method: 构建真实临床uLDCT肺部数据集，提出图像净化策略生成结构对齐图像对，设计频域流匹配模型协同工作以保持解剖结构完整性。

Result: 在真实临床数据集上，IP策略显著提升多个主流去噪模型性能，FFM模型结合IP策略在解剖结构保持方面达到SOTA效果。

Conclusion: 本研究为真实世界uLDCT去噪中的数据不匹配问题提供了有效解决方案。

Abstract: Ultra-low dose CT (uLDCT) significantly reduces radiation exposure but introduces severe noise and artifacts. It also leads to substantial spatial misalignment between uLDCT and normal dose CT (NDCT) image pairs. This poses challenges for directly applying existing denoising networks trained on synthetic noise or aligned data. To address this core challenge in uLDCT denoising, this paper proposes an innovative denoising framework based on an Image Purification (IP) strategy. First, we construct a real clinical uLDCT lung dataset. Then, we propose an Image Purification strategy that generates structurally aligned uLDCT-NDCT image pairs, providing a high-quality data foundation for network training. Building upon this, we propose a Frequency-domain Flow Matching (FFM) model, which works synergistically with the IP strategy to excellently preserve the anatomical structure integrity of denoised images. Experiments on the real clinical dataset demonstrate that our IP strategy significantly enhances the performance of multiple mainstream denoising models on the uLDCT task. Notably, our proposed FFM model combined with the IP strategy achieves state-of-the-art (SOTA) results in anatomical structure preservation. This study provides an effective solution to the data mismatch problem in real-world uLDCT denoising. Code and dataset are available at https://github.com/MonkeyDadLufy/flow-matching.

</details>


### [11] [PickStyle: Video-to-Video Style Transfer with Context-Style Adapters](https://arxiv.org/abs/2510.07546)
*Soroush Mehraban,Vida Adeli,Jacob Rommann,Babak Taati,Kyryl Truskovskyi*

Main category: cs.CV

TL;DR: PickStyle是一个视频风格迁移框架，通过插入低秩适配器到预训练视频扩散模型中，利用配对的静态图像数据进行训练，实现内容保持且风格一致的视频风格转换。


<details>
  <summary>Details</summary>
Motivation: 解决视频风格迁移任务中缺乏配对视频数据监督的挑战，需要在保持输入视频内容的同时，根据文本提示渲染目标风格。

Method: 在条件模块的自注意力层插入低秩适配器，从配对的源-风格图像数据中训练；构建合成训练剪辑模拟相机运动；提出上下文-风格分类器自由引导(CS-CFG)将引导分解为独立的文本(风格)和视频(上下文)方向。

Result: 在多个基准测试中，该方法实现了时间连贯、风格忠实且内容保持的视频转换，在定性和定量评估上都优于现有基线方法。

Conclusion: PickStyle框架通过有效的适配器设计和训练策略，成功解决了视频风格迁移中的数据稀缺问题，实现了高质量的视频风格转换效果。

Abstract: We address the task of video style transfer with diffusion models, where the goal is to preserve the context of an input video while rendering it in a target style specified by a text prompt. A major challenge is the lack of paired video data for supervision. We propose PickStyle, a video-to-video style transfer framework that augments pretrained video diffusion backbones with style adapters and benefits from paired still image data with source-style correspondences for training. PickStyle inserts low-rank adapters into the self-attention layers of conditioning modules, enabling efficient specialization for motion-style transfer while maintaining strong alignment between video content and style. To bridge the gap between static image supervision and dynamic video, we construct synthetic training clips from paired images by applying shared augmentations that simulate camera motion, ensuring temporal priors are preserved. In addition, we introduce Context-Style Classifier-Free Guidance (CS-CFG), a novel factorization of classifier-free guidance into independent text (style) and video (context) directions. CS-CFG ensures that context is preserved in generated video while the style is effectively transferred. Experiments across benchmarks show that our approach achieves temporally coherent, style-faithful, and content-preserving video translations, outperforming existing baselines both qualitatively and quantitatively.

</details>


### [12] [Rectified-CFG++ for Flow Based Models](https://arxiv.org/abs/2510.07631)
*Shreshth Saini,Shashank Gupta,Alan C. Bovik*

Main category: cs.CV

TL;DR: 提出了Rectified-CFG++，一种自适应预测器-校正器引导方法，解决了标准CFG在整流流模型中的离流形漂移问题，通过几何感知的条件规则确保轨迹稳定在数据流形附近。


<details>
  <summary>Details</summary>
Motivation: 标准分类器无关引导(CFG)在整流流(RF)模型中会引起严重的离流形漂移，导致视觉伪影、文本不对齐和脆弱行为，需要一种更稳定的引导方法。

Method: 采用自适应预测器-校正器引导，每个推理步骤先执行条件RF更新将样本锚定在学习到的传输路径附近，然后应用加权条件校正，在条件和无条件速度场之间插值。

Result: 在Flux、Stable Diffusion 3/3.5、Lumina等大规模文本到图像模型上的实验表明，Rectified-CFG++在MS-COCO、LAION-Aesthetic和T2I-CompBench等基准数据集上始终优于标准CFG。

Conclusion: Rectified-CFG++将整流流的确定性效率与几何感知条件规则相结合，确保速度场的边际一致性和轨迹在数据流形有界管状邻域内的稳定性，适用于广泛的引导强度。

Abstract: Classifier-free guidance (CFG) is the workhorse for steering large diffusion models toward text-conditioned targets, yet its native application to rectified flow (RF) based models provokes severe off-manifold drift, yielding visual artifacts, text misalignment, and brittle behaviour. We present Rectified-CFG++, an adaptive predictor-corrector guidance that couples the deterministic efficiency of rectified flows with a geometry-aware conditioning rule. Each inference step first executes a conditional RF update that anchors the sample near the learned transport path, then applies a weighted conditional correction that interpolates between conditional and unconditional velocity fields. We prove that the resulting velocity field is marginally consistent and that its trajectories remain within a bounded tubular neighbourhood of the data manifold, ensuring stability across a wide range of guidance strengths. Extensive experiments on large-scale text-to-image models (Flux, Stable Diffusion 3/3.5, Lumina) show that Rectified-CFG++ consistently outperforms standard CFG on benchmark datasets such as MS-COCO, LAION-Aesthetic, and T2I-CompBench. Project page: https://rectified-cfgpp.github.io/

</details>


### [13] [Once Is Enough: Lightweight DiT-Based Video Virtual Try-On via One-Time Garment Appearance Injection](https://arxiv.org/abs/2510.07654)
*Yanjie Pan,Qingdong He,Lidong Wang,Bo Peng,Mingmin Chi*

Main category: cs.CV

TL;DR: 提出OIE方法，基于首帧服装替换的视频虚拟试穿策略，通过图像模型替换首帧服装，然后利用姿态和掩码信息控制视频生成模型合成剩余帧


<details>
  <summary>Details</summary>
Motivation: 当前基于U-Net的双分支架构难以适应基于Diffusion Transformer的扩散模型，引入服装参考分支需要修改骨干网络导致参数过多，且服装潜在特征缺乏时间特性需要额外学习

Method: 使用图像服装转移模型替换首帧服装，在编辑后的首帧内容控制下，利用姿态和掩码信息引导视频生成模型的时间先验，顺序合成剩余帧

Result: 实验表明该方法在参数效率和计算效率方面表现优异，同时在约束条件下保持领先性能

Conclusion: OIE方法通过首帧替换策略有效解决了基于Diffusion Transformer的视频虚拟试穿中的参数效率和计算效率问题

Abstract: Video virtual try-on aims to replace the clothing of a person in a video with a target garment. Current dual-branch architectures have achieved significant success in diffusion models based on the U-Net; however, adapting them to diffusion models built upon the Diffusion Transformer remains challenging. Initially, introducing latent space features from the garment reference branch requires adding or modifying the backbone network, leading to a large number of trainable parameters. Subsequently, the latent space features of garments lack inherent temporal characteristics and thus require additional learning. To address these challenges, we propose a novel approach, OIE (Once is Enough), a virtual try-on strategy based on first-frame clothing replacement: specifically, we employ an image-based clothing transfer model to replace the clothing in the initial frame, and then, under the content control of the edited first frame, utilize pose and mask information to guide the temporal prior of the video generation model in synthesizing the remaining frames sequentially. Experiments show that our method achieves superior parameter efficiency and computational efficiency while still maintaining leading performance under these constraints.

</details>


### [14] [Controllable Video Synthesis via Variational Inference](https://arxiv.org/abs/2510.07670)
*Haoyi Duan,Yunzhi Zhang,Yilun Du,Jiajun Wu*

Main category: cs.CV

TL;DR: 提出了一种视频合成方法，通过变分推理近似组合分布，利用多个视频生成骨干网络共同处理所有任务约束，实现高可控性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型通常针对固定输入格式训练，而实际视频工作流需要混合不同粒度的用户控制，从精确的4D对象轨迹到粗略的文本提示。

Method: 将任务建模为变分推理来近似组合分布，使用多个视频生成骨干网络；通过逐步KL散度最小化和上下文条件分解技术解决优化挑战。

Result: 实验表明，与先前工作相比，该方法生成的样本在可控性、多样性和3D一致性方面都有所提升。

Conclusion: 该方法能够为指定元素提供高可控性，同时为未指定元素保持多样性，解决了视频生成中混合控制需求的问题。

Abstract: Many video workflows benefit from a mixture of user controls with varying granularity, from exact 4D object trajectories and camera paths to coarse text prompts, while existing video generative models are typically trained for fixed input formats. We develop a video synthesis method that addresses this need and generates samples with high controllability for specified elements while maintaining diversity for under-specified ones. We cast the task as variational inference to approximate a composed distribution, leveraging multiple video generation backbones to account for all task constraints collectively. To address the optimization challenge, we break down the problem into step-wise KL divergence minimization over an annealed sequence of distributions, and further propose a context-conditioned factorization technique that reduces modes in the solution space to circumvent local optima. Experiments suggest that our method produces samples with improved controllability, diversity, and 3D consistency compared to prior works.

</details>


### [15] [RePainter: Empowering E-commerce Object Removal via Spatial-matting Reinforcement Learning](https://arxiv.org/abs/2510.07721)
*Zipeng Guo,Lichen Ma,Xiaolong Fu,Gaojing Zhou,Lan Yang,Yuchen Zhou,Linkai Liu,Yu He,Ximan Liu,Shiping Dong,Jingling Fu,Zhen Chen,Yu Shi,Junshi Huang,Jason Li,Chao Gou*

Main category: cs.CV

TL;DR: 提出了Repainter强化学习框架，通过空间抠图轨迹优化和GRPO策略，解决电商产品图像中水印和促销文本去除问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 电商平台中产品图像的水印和促销文本等侵入性元素严重影响视觉效果，现有扩散修复方法在商业场景中面临对象去除不可靠和领域适应有限的问题。

Method: 结合空间抠图轨迹优化与Group Relative Policy Optimization(GRPO)，通过调节注意力机制强调背景上下文，并引入平衡全局、局部和语义约束的复合奖励机制。

Result: 在复杂场景中显著优于最先进方法，有效减少视觉伪影和奖励攻击，同时贡献了EcomPaint-100K数据集和EcomPaint-Bench基准。

Conclusion: Repainter框架在电商图像修复任务中表现出色，特别是在复杂构图场景下，为商业应用提供了可靠解决方案。

Abstract: In web data, product images are central to boosting user engagement and advertising efficacy on e-commerce platforms, yet the intrusive elements such as watermarks and promotional text remain major obstacles to delivering clear and appealing product visuals. Although diffusion-based inpainting methods have advanced, they still face challenges in commercial settings due to unreliable object removal and limited domain-specific adaptation. To tackle these challenges, we propose Repainter, a reinforcement learning framework that integrates spatial-matting trajectory refinement with Group Relative Policy Optimization (GRPO). Our approach modulates attention mechanisms to emphasize background context, generating higher-reward samples and reducing unwanted object insertion. We also introduce a composite reward mechanism that balances global, local, and semantic constraints, effectively reducing visual artifacts and reward hacking. Additionally, we contribute EcomPaint-100K, a high-quality, large-scale e-commerce inpainting dataset, and a standardized benchmark EcomPaint-Bench for fair evaluation. Extensive experiments demonstrate that Repainter significantly outperforms state-of-the-art methods, especially in challenging scenes with intricate compositions. We will release our code and weights upon acceptance.

</details>


### [16] [ComGS: Efficient 3D Object-Scene Composition via Surface Octahedral Probes](https://arxiv.org/abs/2510.07729)
*Jian Gao,Mengqi Yuan,Yifei Zeng,Chang Zeng,Zhihao Li,Zhenyu Chen,Weichao Qiu,Xiao-Xiao Long,Hao Zhu,Xun Cao,Yao Yao*

Main category: cs.CV

TL;DR: 提出了ComGS框架，通过Surface Octahedral Probes实现高效的可重光照物体重建，结合基于扩散模型的场景光照估计，解决了3D物体-场景合成中的光照不一致问题。


<details>
  <summary>Details</summary>
Motivation: 高斯泼溅技术虽然能实现沉浸式渲染，但在3D物体-场景合成时存在烘培外观和阴影信息导致的不一致问题，需要可重光照的物体重建和场景光照估计。

Method: 使用Surface Octahedral Probes存储光照和遮挡信息，通过插值实现高效3D查询；简化场景光照估计，在物体放置位置重建360度辐射场，并用扩散模型补全光照。

Result: 实现了约28FPS的高质量实时渲染，产生视觉和谐的合成结果和生动的阴影，编辑仅需36秒，重建速度至少提升2倍。

Conclusion: ComGS框架成功解决了3D物体-场景合成中的光照一致性问题，实现了高效、高质量的实时渲染。

Abstract: Gaussian Splatting (GS) enables immersive rendering, but realistic 3D object-scene composition remains challenging. Baked appearance and shadow information in GS radiance fields cause inconsistencies when combining objects and scenes. Addressing this requires relightable object reconstruction and scene lighting estimation. For relightable object reconstruction, existing Gaussian-based inverse rendering methods often rely on ray tracing, leading to low efficiency. We introduce Surface Octahedral Probes (SOPs), which store lighting and occlusion information and allow efficient 3D querying via interpolation, avoiding expensive ray tracing. SOPs provide at least a 2x speedup in reconstruction and enable real-time shadow computation in Gaussian scenes. For lighting estimation, existing Gaussian-based inverse rendering methods struggle to model intricate light transport and often fail in complex scenes, while learning-based methods predict lighting from a single image and are viewpoint-sensitive. We observe that 3D object-scene composition primarily concerns the object's appearance and nearby shadows. Thus, we simplify the challenging task of full scene lighting estimation by focusing on the environment lighting at the object's placement. Specifically, we capture a 360 degrees reconstructed radiance field of the scene at the location and fine-tune a diffusion model to complete the lighting. Building on these advances, we propose ComGS, a novel 3D object-scene composition framework. Our method achieves high-quality, real-time rendering at around 28 FPS, produces visually harmonious results with vivid shadows, and requires only 36 seconds for editing. Code and dataset are available at https://nju-3dv.github.io/projects/ComGS/.

</details>


### [17] [UltraLED: Learning to See Everything in Ultra-High Dynamic Range Scenes](https://arxiv.org/abs/2510.07741)
*Yuang Meng,Xin Jin,Lina Lei,Chun-Le Guo,Chongyi Li*

Main category: cs.CV

TL;DR: 提出UltraLED框架，使用单帧短曝光RAW图像进行超高清动态范围重建，通过曝光校正和亮度感知RAW去噪来同时保留高光和阴影细节。


<details>
  <summary>Details</summary>
Motivation: 解决UHDR场景中高光和暗部细节难以同时保留的问题，避免传统RGB包围曝光方法的错位和重影伪影，利用RAW图像的高位深和可预测噪声特性。

Method: 两阶段框架：首先通过比率图进行曝光校正以平衡动态范围，然后使用亮度感知RAW去噪器增强暗部细节恢复。构建9档包围曝光数据集进行训练。

Result: 在广泛实验中，UltraLED显著优于现有的单帧方法，能够有效避免重影和运动模糊，在动态场景中表现稳健。

Conclusion: 仅使用单帧短曝光RAW图像即可实现UHDR重建，该方法在动态场景中具有鲁棒性，代码和数据集已公开。

Abstract: Ultra-high dynamic range (UHDR) scenes exhibit significant exposure disparities between bright and dark regions. Such conditions are commonly encountered in nighttime scenes with light sources. Even with standard exposure settings, a bimodal intensity distribution with boundary peaks often emerges, making it difficult to preserve both highlight and shadow details simultaneously. RGB-based bracketing methods can capture details at both ends using short-long exposure pairs, but are susceptible to misalignment and ghosting artifacts. We found that a short-exposure image already retains sufficient highlight detail. The main challenge of UHDR reconstruction lies in denoising and recovering information in dark regions. In comparison to the RGB images, RAW images, thanks to their higher bit depth and more predictable noise characteristics, offer greater potential for addressing this challenge. This raises a key question: can we learn to see everything in UHDR scenes using only a single short-exposure RAW image? In this study, we rely solely on a single short-exposure frame, which inherently avoids ghosting and motion blur, making it particularly robust in dynamic scenes. To achieve that, we introduce UltraLED, a two-stage framework that performs exposure correction via a ratio map to balance dynamic range, followed by a brightness-aware RAW denoiser to enhance detail recovery in dark regions. To support this setting, we design a 9-stop bracketing pipeline to synthesize realistic UHDR images and contribute a corresponding dataset based on diverse scenes, using only the shortest exposure as input for reconstruction. Extensive experiments show that UltraLED significantly outperforms existing single-frame approaches. Our code and dataset are made publicly available at https://srameo.github.io/projects/ultraled.

</details>


### [18] [DEGS: Deformable Event-based 3D Gaussian Splatting from RGB and Event Stream](https://arxiv.org/abs/2510.07752)
*Junhao He,Jiaxu Wang,Jia Li,Mingyuan Sun,Qiang Zhang,Jiahang Cao,Ziyi Zhang,Yi Gu,Jingkai Sun,Renjing Xu*

Main category: cs.CV

TL;DR: 提出了一种结合低帧率RGB视频和高帧率事件流来重建动态3D高斯溅射的新框架，通过事件运动先验指导变形场优化，解决了大帧间运动带来的不确定性挑战。


<details>
  <summary>Details</summary>
Motivation: 从低帧率RGB视频重建动态3D高斯溅射具有挑战性，因为大帧间运动会增加解空间的不确定性。事件相机可以异步捕捉快速视觉变化且对运动模糊鲁棒，但缺乏颜色信息。结合两种模态可以解决这一挑战。

Method: 采用事件运动先验指导变形场优化：1) 使用LoCM无监督微调框架从事件流中提取运动先验；2) 提出几何感知数据关联方法建立事件-高斯运动对应关系；3) 采用运动分解和帧间伪标签策略。

Result: 在合成和真实场景上的大量实验表明，该方法优于现有的基于图像和事件的方法，证明能够有效利用事件数据优化动态3D高斯溅射。

Conclusion: 提出的框架成功解决了多模态数据差异带来的挑战，通过事件运动先验有效指导了动态3D高斯溅射的优化，在动态场景重建方面取得了显著改进。

Abstract: Reconstructing Dynamic 3D Gaussian Splatting (3DGS) from low-framerate RGB videos is challenging. This is because large inter-frame motions will increase the uncertainty of the solution space. For example, one pixel in the first frame might have more choices to reach the corresponding pixel in the second frame. Event cameras can asynchronously capture rapid visual changes and are robust to motion blur, but they do not provide color information. Intuitively, the event stream can provide deterministic constraints for the inter-frame large motion by the event trajectories. Hence, combining low-temporal-resolution images with high-framerate event streams can address this challenge. However, it is challenging to jointly optimize Dynamic 3DGS using both RGB and event modalities due to the significant discrepancy between these two data modalities. This paper introduces a novel framework that jointly optimizes dynamic 3DGS from the two modalities. The key idea is to adopt event motion priors to guide the optimization of the deformation fields. First, we extract the motion priors encoded in event streams by using the proposed LoCM unsupervised fine-tuning framework to adapt an event flow estimator to a certain unseen scene. Then, we present the geometry-aware data association method to build the event-Gaussian motion correspondence, which is the primary foundation of the pipeline, accompanied by two useful strategies, namely motion decomposition and inter-frame pseudo-label. Extensive experiments show that our method outperforms existing image and event-based approaches across synthetic and real scenes and prove that our method can effectively optimize dynamic 3DGS with the help of event data.

</details>


### [19] [PrismGS: Physically-Grounded Anti-Aliasing for High-Fidelity Large-Scale 3D Gaussian Splatting](https://arxiv.org/abs/2510.07830)
*Houqiang Zhong,Zhenglong Wu,Sihua Fu,Zihan Zheng,Xin Jin,Xiaoyun Zhang,Li Song,Qiang Hu*

Main category: cs.CV

TL;DR: PrismGS提出了一种基于物理正则化的框架，通过金字塔多尺度监督和显式尺寸正则化，解决了3D高斯泼溅在大规模城市场景中的走样伪影和优化不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅在紧凑场景中能实现实时逼真渲染，但在大规模城市场景中会出现严重的走样伪影和优化不稳定问题，特别是在高分辨率渲染时，表现为闪烁纹理和锯齿边缘。现有方法虽然解决了可扩展性问题，但未能解决这种保真度差距。

Method: PrismGS集成了两个协同的正则化器：1）金字塔多尺度监督，通过对预过滤图像金字塔进行渲染监督来强制一致性；2）显式尺寸正则化，对3D高斯的尺寸施加物理基础的下界约束。

Result: 在MatrixCity、Mill-19和UrbanScene3D数据集上的广泛实验表明，PrismGS实现了最先进的性能，相比CityGaussian获得了约1.5 dB的PSNR提升，同时在苛刻的4K渲染下保持了优越的质量和鲁棒性。

Conclusion: PrismGS是一个即插即用的框架，与现有流程兼容，通过物理基础的正则化显著改善了3D高斯的固有渲染行为，解决了大规模城市场景中的走样问题。

Abstract: 3D Gaussian Splatting (3DGS) has recently enabled real-time photorealistic rendering in compact scenes, but scaling to large urban environments introduces severe aliasing artifacts and optimization instability, especially under high-resolution (e.g., 4K) rendering. These artifacts, manifesting as flickering textures and jagged edges, arise from the mismatch between Gaussian primitives and the multi-scale nature of urban geometry. While existing ``divide-and-conquer'' pipelines address scalability, they fail to resolve this fidelity gap. In this paper, we propose PrismGS, a physically-grounded regularization framework that improves the intrinsic rendering behavior of 3D Gaussians. PrismGS integrates two synergistic regularizers. The first is pyramidal multi-scale supervision, which enforces consistency by supervising the rendering against a pre-filtered image pyramid. This compels the model to learn an inherently anti-aliased representation that remains coherent across different viewing scales, directly mitigating flickering textures. This is complemented by an explicit size regularization that imposes a physically-grounded lower bound on the dimensions of the 3D Gaussians. This prevents the formation of degenerate, view-dependent primitives, leading to more stable and plausible geometric surfaces and reducing jagged edges. Our method is plug-and-play and compatible with existing pipelines. Extensive experiments on MatrixCity, Mill-19, and UrbanScene3D demonstrate that PrismGS achieves state-of-the-art performance, yielding significant PSNR gains around 1.5 dB against CityGaussian, while maintaining its superior quality and robustness under demanding 4K rendering.

</details>


### [20] [TTOM: Test-Time Optimization and Memorization for Compositional Video Generation](https://arxiv.org/abs/2510.07940)
*Leigang Qu,Ziyang Wang,Na Zheng,Wenjie Wang,Liqiang Nie,Tat-Seng Chua*

Main category: cs.CV

TL;DR: TTOM是一个无需训练的视频生成框架，通过测试时优化和记忆机制来提升视频基础模型在组合场景中的表现，实现更好的文本-图像对齐。


<details>
  <summary>Details</summary>
Motivation: 视频基础模型在视觉生成方面表现出色，但在组合场景（如运动、数字和空间关系）中存在困难，需要改进文本-图像对齐。

Method: 提出TTOM框架，在推理时通过优化新参数来对齐时空布局，采用流式生成设置，并维护历史优化上下文的参数化记忆机制。

Result: 在T2V-CompBench和Vbench基准测试中，TTOM表现出强大的可迁移性和泛化能力，有效实现了组合视频生成的跨模态对齐。

Conclusion: TTOM是一个有效、实用、可扩展且高效的框架，能够在推理时动态实现组合视频生成的跨模态对齐。

Abstract: Video Foundation Models (VFMs) exhibit remarkable visual generation performance, but struggle in compositional scenarios (e.g., motion, numeracy, and spatial relation). In this work, we introduce Test-Time Optimization and Memorization (TTOM), a training-free framework that aligns VFM outputs with spatiotemporal layouts during inference for better text-image alignment. Rather than direct intervention to latents or attention per-sample in existing work, we integrate and optimize new parameters guided by a general layout-attention objective. Furthermore, we formulate video generation within a streaming setting, and maintain historical optimization contexts with a parametric memory mechanism that supports flexible operations, such as insert, read, update, and delete. Notably, we found that TTOM disentangles compositional world knowledge, showing powerful transferability and generalization. Experimental results on the T2V-CompBench and Vbench benchmarks establish TTOM as an effective, practical, scalable, and efficient framework to achieve cross-modal alignment for compositional video generation on the fly.

</details>


### [21] [CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving](https://arxiv.org/abs/2510.07944)
*Tianrui Zhang,Yichen Liu,Zilin Guo,Yuxin Guo,Jingcheng Ni,Chenjing Ding,Dan Xu,Lewei Lu,Zehuan Wu*

Main category: cs.CV

TL;DR: 提出CVD-STORM，一种基于空间-时间重构VAE的跨视角视频扩散模型，能够生成具有4D重构能力的多视角长视频，并在各种控制输入下提升生成质量和几何信息提取。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶发展，不仅需要高质量可控视频生成，还需要生成深度估计等多样化有意义信息，现有生成模型在4D重构能力方面存在不足。

Method: 首先通过辅助4D重构任务微调VAE，增强其3D结构和时间动态编码能力，然后将该VAE集成到视频扩散过程中提升生成质量，同时联合训练高斯泼溅解码器进行动态场景重构。

Result: 实验结果显示模型在FID和FVD指标上取得显著提升，联合训练的高斯泼溅解码器能有效重构动态场景，提供有价值的几何信息。

Conclusion: CVD-STORM通过空间-时间重构VAE和扩散模型结合，成功实现了高质量多视角视频生成与4D场景重构，为全面场景理解提供了有效解决方案。

Abstract: Generative models have been widely applied to world modeling for environment simulation and future state prediction. With advancements in autonomous driving, there is a growing demand not only for high-fidelity video generation under various controls, but also for producing diverse and meaningful information such as depth estimation. To address this, we propose CVD-STORM, a cross-view video diffusion model utilizing a spatial-temporal reconstruction Variational Autoencoder (VAE) that generates long-term, multi-view videos with 4D reconstruction capabilities under various control inputs. Our approach first fine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing its ability to encode 3D structures and temporal dynamics. Subsequently, we integrate this VAE into the video diffusion process to significantly improve generation quality. Experimental results demonstrate that our model achieves substantial improvements in both FID and FVD metrics. Additionally, the jointly-trained Gaussian Splatting Decoder effectively reconstructs dynamic scenes, providing valuable geometric information for comprehensive scene understanding.

</details>


### [22] [Latent Harmony: Synergistic Unified UHD Image Restoration via Latent Space Regularization and Controllable Refinement](https://arxiv.org/abs/2510.07961)
*Yidi Liu,Xueyang Fu,Jie Huang,Jie Xiao,Dong Li,Wenlong Zhang,Lei Bai,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: Latent Harmony是一个用于超高清图像恢复的两阶段框架，通过联合正则化潜在空间和强制高频感知重建，解决了VAE在计算效率与高频细节保留之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 超高清图像恢复面临计算效率与高频细节保留的权衡。虽然变分自编码器通过潜在空间处理提高了效率，但其高斯约束通常会丢弃退化特定的高频信息，损害重建保真度。

Method: 第一阶段提出LH-VAE，通过视觉语义约束和渐进退化扰动增强语义鲁棒性，同时潜在等变性强化高频重建。第二阶段通过高频低秩适应（HF-LoRA）联合训练精炼的VAE与恢复模型，包括编码器LoRA和解码器LoRA，通过交替优化和选择性梯度传播进行训练。

Result: 实验表明Latent Harmony在超高清和标准分辨率任务中实现了最先进的性能，有效平衡了效率、感知质量和重建精度。

Conclusion: Latent Harmony通过重新定义VAE用于超高清恢复，成功解决了计算效率与高频细节保留的权衡问题，在推理时通过可调参数实现灵活的保真度-感知权衡。

Abstract: Ultra-High Definition (UHD) image restoration faces a trade-off between computational efficiency and high-frequency detail retention. While Variational Autoencoders (VAEs) improve efficiency via latent-space processing, their Gaussian constraint often discards degradation-specific high-frequency information, hurting reconstruction fidelity. To overcome this, we propose Latent Harmony, a two-stage framework that redefines VAEs for UHD restoration by jointly regularizing the latent space and enforcing high-frequency-aware reconstruction.In Stage One, we introduce LH-VAE, which enhances semantic robustness through visual semantic constraints and progressive degradation perturbations, while latent equivariance strengthens high-frequency reconstruction.Stage Two jointly trains this refined VAE with a restoration model using High-Frequency Low-Rank Adaptation (HF-LoRA): an encoder LoRA guided by a fidelity-oriented high-frequency alignment loss to recover authentic details, and a decoder LoRA driven by a perception-oriented loss to synthesize realistic textures. Both LoRA modules are trained via alternating optimization with selective gradient propagation to preserve the pretrained latent structure.At inference, a tunable parameter {\alpha} enables flexible fidelity-perception trade-offs.Experiments show Latent Harmony achieves state-of-the-art performance across UHD and standard-resolution tasks, effectively balancing efficiency, perceptual quality, and reconstruction accuracy.

</details>


### [23] [Efficient Label Refinement for Face Parsing Under Extreme Poses Using 3D Gaussian Splatting](https://arxiv.org/abs/2510.08096)
*Ankit Gahlawat,Anirban Mukherjee,Dinesh Babu Jayagopi*

Main category: cs.CV

TL;DR: 提出一种基于3D高斯泼溅的标签精炼流程，通过联合拟合RGB图像和初始分割图的3DGS模型，生成多视角一致的准确分割掩码，用于增强极端视角下的人脸解析性能。


<details>
  <summary>Details</summary>
Motivation: 极端视角下的人脸解析由于缺乏标注数据而面临挑战，手动标注成本高昂且难以扩展。

Method: 联合拟合两个3DGS模型（RGB图像和初始分割图），通过共享几何实现多视角一致性，合成姿态多样的训练数据。

Result: 在精炼数据集上微调的人脸解析模型在挑战性头部姿态上准确率显著提升，同时在标准视角上保持强性能。

Conclusion: 该方法无需真实3D标注，仅使用少量初始图像即可实现可扩展且有效的人脸解析鲁棒性提升。

Abstract: Accurate face parsing under extreme viewing angles remains a significant challenge due to limited labeled data in such poses. Manual annotation is costly and often impractical at scale. We propose a novel label refinement pipeline that leverages 3D Gaussian Splatting (3DGS) to generate accurate segmentation masks from noisy multiview predictions. By jointly fitting two 3DGS models, one to RGB images and one to their initial segmentation maps, our method enforces multiview consistency through shared geometry, enabling the synthesis of pose-diverse training data with only minimal post-processing. Fine-tuning a face parsing model on this refined dataset significantly improves accuracy on challenging head poses, while maintaining strong performance on standard views. Extensive experiments, including human evaluations, demonstrate that our approach achieves superior results compared to state-of-the-art methods, despite requiring no ground-truth 3D annotations and using only a small set of initial images. Our method offers a scalable and effective solution for improving face parsing robustness in real-world settings.

</details>


### [24] [Real-Time Motion-Controllable Autoregressive Video Diffusion](https://arxiv.org/abs/2510.08131)
*Kesen Zhao,Jiaxin Shi,Beier Zhu,Junbao Zhou,Xiaolong Shen,Yuan Zhou,Qianru Sun,Hanwang Zhang*

Main category: cs.CV

TL;DR: AR-Drag是一个强化学习增强的少步自回归视频扩散模型，用于实时图像到视频生成，支持多样化运动控制，显著降低延迟并保持高质量输出。


<details>
  <summary>Details</summary>
Motivation: 解决实时运动可控视频生成的挑战，包括双向扩散模型的固有延迟问题，以及现有自回归视频扩散模型在少步生成中的质量下降和运动伪影问题。

Method: 首先微调基础图像到视频模型以支持基本运动控制，然后通过基于轨迹的奖励模型进行强化学习改进，采用自展开机制保持马尔可夫性质，并在去噪步骤中选择性引入随机性来加速训练。

Result: AR-Drag实现了高视觉保真度和精确的运动对齐，与最先进的运动可控视频扩散模型相比显著降低延迟，仅使用13亿参数。

Conclusion: AR-Drag是首个RL增强的少步自回归视频扩散模型，成功解决了实时运动可控视频生成的挑战，在延迟和生成质量方面都表现出色。

Abstract: Real-time motion-controllable video generation remains challenging due to the inherent latency of bidirectional diffusion models and the lack of effective autoregressive (AR) approaches. Existing AR video diffusion models are limited to simple control signals or text-to-video generation, and often suffer from quality degradation and motion artifacts in few-step generation. To address these challenges, we propose AR-Drag, the first RL-enhanced few-step AR video diffusion model for real-time image-to-video generation with diverse motion control. We first fine-tune a base I2V model to support basic motion control, then further improve it via reinforcement learning with a trajectory-based reward model. Our design preserves the Markov property through a Self-Rollout mechanism and accelerates training by selectively introducing stochasticity in denoising steps. Extensive experiments demonstrate that AR-Drag achieves high visual fidelity and precise motion alignment, significantly reducing latency compared with state-of-the-art motion-controllable VDMs, while using only 1.3B parameters. Additional visualizations can be found on our project page: https://kesenzhao.github.io/AR-Drag.github.io/.

</details>


### [25] [UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video Super-Resolution](https://arxiv.org/abs/2510.08143)
*Shian Du,Menghan Xia,Chang Liu,Quande Liu,Xintao Wang,Pengfei Wan,Xiangyang Ji*

Main category: cs.CV

TL;DR: 提出了UniMMVSR，首个统一的多模态视频超分辨率框架，支持文本、图像和视频作为生成条件，显著提升了视频细节和对多模态条件的遵从度。


<details>
  <summary>Details</summary>
Motivation: 现有级联视频超分辨率方法主要局限于文本到视频任务，未能利用文本之外的其他生成条件，而这些条件对于确保多模态视频生成的保真度至关重要。

Method: 在潜在视频扩散模型中探索了条件注入策略、训练方案和数据混合技术，设计了不同的数据构建和条件利用方法，使模型能够精确利用所有条件类型。

Result: UniMMVSR显著优于现有方法，生成的视频具有更优的细节和更高的多模态条件遵从度，能够与基础模型结合实现4K视频的多模态引导生成。

Conclusion: UniMMVSR是首个统一的多模态视频超分辨率框架，成功解决了多模态条件利用的挑战，为高质量视频生成开辟了新途径。

Abstract: Cascaded video super-resolution has emerged as a promising technique for decoupling the computational burden associated with generating high-resolution videos using large foundation models. Existing studies, however, are largely confined to text-to-video tasks and fail to leverage additional generative conditions beyond text, which are crucial for ensuring fidelity in multi-modal video generation. We address this limitation by presenting UniMMVSR, the first unified generative video super-resolution framework to incorporate hybrid-modal conditions, including text, images, and videos. We conduct a comprehensive exploration of condition injection strategies, training schemes, and data mixture techniques within a latent video diffusion model. A key challenge was designing distinct data construction and condition utilization methods to enable the model to precisely utilize all condition types, given their varied correlations with the target video. Our experiments demonstrate that UniMMVSR significantly outperforms existing methods, producing videos with superior detail and a higher degree of conformity to multi-modal conditions. We also validate the feasibility of combining UniMMVSR with a base model to achieve multi-modal guided generation of 4K video, a feat previously unattainable with existing techniques.

</details>


### [26] [One Stone with Two Birds: A Null-Text-Null Frequency-Aware Diffusion Models for Text-Guided Image Inpainting](https://arxiv.org/abs/2510.08273)
*Haipeng Liu,Yang Wang,Meng Wang*

Main category: cs.CV

TL;DR: 提出NTN-Diff方法，通过频率感知的扩散模型解决文本引导图像修复中的语义一致性和未掩码区域保护问题


<details>
  <summary>Details</summary>
Motivation: 现有方法无法同时解决未掩码区域保护和掩码/未掩码区域语义一致性两个挑战，这源于混合频率带在去噪过程中对文本提示的不同鲁棒性

Method: 将去噪过程分为早期和晚期阶段，在早期阶段通过空文本去噪处理低频带，在晚期阶段通过文本引导去噪处理中频带，实现频率带解耦

Result: 实验验证NTN-Diff在文本引导图像修复任务上优于现有最先进的扩散模型

Conclusion: 通过频率感知的扩散模型设计，成功解决了文本引导图像修复中的语义一致性和区域保护双重挑战

Abstract: Text-guided image inpainting aims at reconstructing the masked regions as per text prompts, where the longstanding challenges lie in the preservation for unmasked regions, while achieving the semantics consistency between unmasked and inpainted masked regions. Previous arts failed to address both of them, always with either of them to be remedied. Such facts, as we observed, stem from the entanglement of the hybrid (e.g., mid-and-low) frequency bands that encode varied image properties, which exhibit different robustness to text prompts during the denoising process. In this paper, we propose a null-text-null frequency-aware diffusion models, dubbed \textbf{NTN-Diff}, for text-guided image inpainting, by decomposing the semantics consistency across masked and unmasked regions into the consistencies as per each frequency band, while preserving the unmasked regions, to circumvent two challenges in a row. Based on the diffusion process, we further divide the denoising process into early (high-level noise) and late (low-level noise) stages, where the mid-and-low frequency bands are disentangled during the denoising process. As observed, the stable mid-frequency band is progressively denoised to be semantically aligned during text-guided denoising process, which, meanwhile, serves as the guidance to the null-text denoising process to denoise low-frequency band for the masked regions, followed by a subsequent text-guided denoising process at late stage, to achieve the semantics consistency for mid-and-low frequency bands across masked and unmasked regions, while preserve the unmasked regions. Extensive experiments validate the superiority of NTN-Diff over the state-of-the-art diffusion models to text-guided diffusion models. Our code can be accessed from https://github.com/htyjers/NTN-Diff.

</details>


### [27] [LinVideo: A Post-Training Framework towards O(n) Attention in Efficient Video Generation](https://arxiv.org/abs/2510.08318)
*Yushi Huang,Xingtong Ge,Ruihao Gong,Chengtao Lv,Jun Zhang*

Main category: cs.CV

TL;DR: LinVideo是一个高效的无数据后训练框架，通过将自注意力模块替换为线性注意力来加速视频扩散模型，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型的计算成本随序列长度呈二次方增长，而完全替换为线性注意力需要昂贵的预训练。

Method: 提出选择性迁移方法自动选择可替换的层，并引入任意时间步分布匹配目标来对齐采样轨迹中的分布。

Result: 实现了1.25-2.00倍加速，4步蒸馏模型进一步达到15.92倍延迟降低，视觉质量损失最小。

Conclusion: LinVideo在保持视频生成质量的同时显著提高了效率，为视频扩散模型的实际应用提供了可行方案。

Abstract: Video diffusion models (DMs) have enabled high-quality video synthesis. However, their computation costs scale quadratically with sequence length because self-attention has quadratic complexity. While linear attention lowers the cost, fully replacing quadratic attention requires expensive pretraining due to the limited expressiveness of linear attention and the complexity of spatiotemporal modeling in video generation. In this paper, we present LinVideo, an efficient data-free post-training framework that replaces a target number of self-attention modules with linear attention while preserving the original model's performance. First, we observe a significant disparity in the replaceability of different layers. Instead of manual or heuristic choices, we frame layer selection as a binary classification problem and propose selective transfer, which automatically and progressively converts layers to linear attention with minimal performance impact. Additionally, to overcome the ineffectiveness and inefficiency of existing objectives for this transfer process, we introduce an anytime distribution matching (ADM) objective that aligns the distributions of samples across any timestep along the sampling trajectory. This objective is efficient and recovers model performance. Extensive experiments show that our method achieves a 1.25-2.00x speedup while preserving generation quality, and our 4-step distilled model further delivers a 15.92x latency reduction with minimal visual quality drop.

</details>


### [28] [SPICE: Simple and Practical Image Clarification and Enhancement](https://arxiv.org/abs/2510.08358)
*Alexander Belyaev,Pierre-Alain Fayolle,Michael Cohen*

Main category: cs.CV

TL;DR: 提出一种简单高效的图像增强方法，用于低光照图像增强和雾霾图像清晰化，通过构建图像滤波器模拟低光/雾霾条件并推导近似反向滤波器来最小化失真。


<details>
  <summary>Details</summary>
Motivation: 解决低光照图像增强和雾霾图像（包括雾天、沙尘、水下图像）清晰化的问题，旨在开发一种简单但有效的解决方案。

Method: 构建图像滤波器模拟低光或雾霾条件，然后推导近似反向滤波器来最小化增强图像中的失真。

Result: 实验结果表明该方法在处理极暗图像和增强雾霾图像方面具有高度竞争力，甚至经常超越现有最先进技术。

Conclusion: 该方法的关键优势在于其简单性，仅需几行MATLAB代码即可实现，为图像增强提供了一种高效且易于实现的解决方案。

Abstract: We introduce a simple and efficient method to enhance and clarify images. More specifically, we deal with low light image enhancement and clarification of hazy imagery (hazy/foggy images, images containing sand dust, and underwater images). Our method involves constructing an image filter to simulate low-light or hazy conditions and deriving approximate reverse filters to minimize distortions in the enhanced images. Experimental results show that our approach is highly competitive and often surpasses state-of-the-art techniques in handling extremely dark images and in enhancing hazy images. A key advantage of our approach lies in its simplicity: Our method is implementable with just a few lines of MATLAB code.

</details>


### [29] [Hyperspectral data augmentation with transformer-based diffusion models](https://arxiv.org/abs/2510.08363)
*Mattia Ferrari,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 提出了一种基于引导扩散模型的数据增强技术，结合轻量级Transformer网络、改进的加权损失函数和优化的余弦方差调度器，用于解决小样本高光谱图像分类中的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 新一代高光谱卫星传感器与深度学习方法结合，能够在大中尺度上区分详细的地表覆盖类别，但小样本训练时存在过拟合风险。

Method: 使用引导扩散模型进行数据增强，实现轻量级Transformer网络，引入改进的加权损失函数和优化的余弦方差调度器，以在小数据集上实现快速有效训练。

Result: 在PRISMA卫星获取的10种森林类型高光谱图像分类任务中，该方法在平均精度和加权平均精度上均优于其他数据增强技术，且模型训练行为稳定。

Conclusion: 该方法有效解决了深度生成模型在实际应用中数据增强的常见限制，为小样本高光谱图像分类提供了稳定有效的解决方案。

Abstract: The introduction of new generation hyperspectral satellite sensors, combined with advancements in deep learning methodologies, has significantly enhanced the ability to discriminate detailed land-cover classes at medium-large scales. However, a significant challenge in deep learning methods is the risk of overfitting when training networks with small labeled datasets. In this work, we propose a data augmentation technique that leverages a guided diffusion model. To effectively train the model with a limited number of labeled samples and to capture complex patterns in the data, we implement a lightweight transformer network. Additionally, we introduce a modified weighted loss function and an optimized cosine variance scheduler, which facilitate fast and effective training on small datasets. We evaluate the effectiveness of the proposed method on a forest classification task with 10 different forest types using hyperspectral images acquired by the PRISMA satellite. The results demonstrate that the proposed method outperforms other data augmentation techniques in both average and weighted average accuracy. The effectiveness of the method is further highlighted by the stable training behavior of the model, which addresses a common limitation in the practical application of deep generative models for data augmentation.

</details>


### [30] [UniVideo: Unified Understanding, Generation, and Editing for Videos](https://arxiv.org/abs/2510.08377)
*Cong Wei,Quande Liu,Zixuan Ye,Qiulin Wang,Xintao Wang,Pengfei Wan,Kun Gai,Wenhu Chen*

Main category: cs.CV

TL;DR: UniVideo是一个统一的多模态视频生成和编辑框架，通过双流设计（MLLM用于指令理解，MMDiT用于视频生成）将多种视频任务统一到单一多模态指令范式下。


<details>
  <summary>Details</summary>
Motivation: 现有的统一多模态模型主要局限于图像领域，需要扩展到视频领域以实现更广泛的多模态内容生成和编辑能力。

Method: 采用双流架构：多模态大语言模型（MLLM）负责理解复杂多模态指令，多模态DiT（MMDiT）负责视频生成，通过联合训练统一处理多种视频任务。

Result: 在文本/图像到视频生成、上下文视频生成和编辑等任务上达到或超越最先进的特定任务基线，并展现出任务组合和零样本泛化能力。

Conclusion: UniVideo成功将统一建模扩展到视频领域，支持任务组合和零样本泛化，为未来视频生成和编辑研究提供了有力工具。

Abstract: Unified multimodal models have shown promising results in multimodal content generation and editing but remain largely limited to the image domain. In this work, we present UniVideo, a versatile framework that extends unified modeling to the video domain. UniVideo adopts a dual-stream design, combining a Multimodal Large Language Model (MLLM) for instruction understanding with a Multimodal DiT (MMDiT) for video generation. This design enables accurate interpretation of complex multimodal instructions while preserving visual consistency. Built on this architecture, UniVideo unifies diverse video generation and editing tasks under a single multimodal instruction paradigm and is jointly trained across them. Extensive experiments demonstrate that UniVideo matches or surpasses state-of-the-art task-specific baselines in text/image-to-video generation, in-context video generation and in-context video editing. Notably, the unified design of UniVideo enables two forms of generalization. First, UniVideo supports task composition, such as combining editing with style transfer, by integrating multiple capabilities within a single instruction. Second, even without explicit training on free-form video editing, UniVideo transfers its editing capability from large-scale image editing data to this setting, handling unseen instructions such as green-screening characters or changing materials within a video. Beyond these core capabilities, UniVideo also supports visual-prompt-based video generation, where the MLLM interprets visual prompts and guides the MMDiT during synthesis. To foster future research, we will release our model and code.

</details>


### [31] [VideoVerse: How Far is Your T2V Generator from a World Model?](https://arxiv.org/abs/2510.08398)
*Zeqing Wang,Xinyu Wei,Bairui Li,Zhen Guo,Jinrui Zhang,Hongyang Wei,Keze Wang,Lei Zhang*

Main category: cs.CV

TL;DR: VideoVerse是一个新的文本到视频生成基准测试，专注于评估模型对现实世界中复杂时间因果关系和世界知识的理解能力，包含300个精心设计的提示和793个二元评估问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估最先进的文本到视频模型时存在不足：无法区分顶级模型的差异、缺乏事件级时间因果关系评估、缺少对世界知识的系统评估。

Method: 收集跨领域代表性视频，提取具有时间因果关系的事件级描述，由独立标注者改写为文本到视频提示，设计包含10个评估维度的二元评估问题，开发基于现代视觉语言模型的人类偏好对齐评估流程。

Result: 构建了包含300个提示、815个事件和793个评估问题的VideoVerse基准，并对开源和闭源的最先进文本到视频模型进行了系统评估。

Conclusion: VideoVerse基准为评估文本到视频模型是否接近世界模型提供了全面框架，揭示了当前模型与世界模型之间的差距。

Abstract: The recent rapid advancement of Text-to-Video (T2V) generation technologies, which are critical to build ``world models'', makes the existing benchmarks increasingly insufficient to evaluate state-of-the-art T2V models. First, current evaluation dimensions, such as per-frame aesthetic quality and temporal consistency, are no longer able to differentiate state-of-the-art T2V models. Second, event-level temporal causality, which not only distinguishes video from other modalities but also constitutes a crucial component of world models, is severely underexplored in existing benchmarks. Third, existing benchmarks lack a systematic assessment of world knowledge, which are essential capabilities for building world models. To address these issues, we introduce VideoVerse, a comprehensive benchmark that focuses on evaluating whether a T2V model could understand complex temporal causality and world knowledge in the real world. We collect representative videos across diverse domains (e.g., natural landscapes, sports, indoor scenes, science fiction, chemical and physical experiments) and extract their event-level descriptions with inherent temporal causality, which are then rewritten into text-to-video prompts by independent annotators. For each prompt, we design a suite of binary evaluation questions from the perspective of dynamic and static properties, with a total of ten carefully defined evaluation dimensions. In total, our VideoVerse comprises 300 carefully curated prompts, involving 815 events and 793 binary evaluation questions. Consequently, a human preference aligned QA-based evaluation pipeline is developed by using modern vision-language models. Finally, we perform a systematic evaluation of state-of-the-art open-source and closed-source T2V models on VideoVerse, providing in-depth analysis on how far the current T2V generators are from world models.

</details>


### [32] [Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency](https://arxiv.org/abs/2510.08431)
*Kaiwen Zheng,Yuji Wang,Qianli Ma,Huayu Chen,Jintao Zhang,Yogesh Balaji,Jianfei Chen,Ming-Yu Liu,Jun Zhu,Qinsheng Zhang*

Main category: cs.CV

TL;DR: 本文首次将连续时间一致性蒸馏扩展到通用应用级图像和视频扩散模型，提出了分数正则化连续时间一致性模型(rCM)，解决了sCM在细节生成中的质量问题，在14B参数模型和5秒视频上实现1-4步高质量生成，加速15-50倍。


<details>
  <summary>Details</summary>
Motivation: 虽然连续时间一致性模型(sCM)在学术规模扩散加速中表现优异，但其在大规模文本到图像和视频任务中的适用性仍不明确，主要面临Jacobian-vector乘积计算的基础设施挑战和标准评估基准的局限性。

Method: 开发了并行兼容的FlashAttention-2 JVP内核，支持超过100亿参数模型训练；提出分数正则化连续时间一致性模型(rCM)，将分数蒸馏作为长跳跃正则器，结合sCM的前向散度和反向散度的模式寻求特性。

Result: 在Cosmos-Predict2、Wan2.1等高达14B参数的大规模模型和5秒视频任务上，rCM在质量指标上匹配或超越最先进的蒸馏方法DMD2，同时在多样性方面具有显著优势，无需GAN调优或大量超参数搜索。

Conclusion: rCM作为一个实用且理论基础的框架，为推进大规模扩散蒸馏提供了有效解决方案，能够在仅1-4步内生成高保真样本，实现15-50倍的扩散采样加速。

Abstract: This work represents the first effort to scale up continuous-time consistency distillation to general application-level image and video diffusion models. Although continuous-time consistency model (sCM) is theoretically principled and empirically powerful for accelerating academic-scale diffusion, its applicability to large-scale text-to-image and video tasks remains unclear due to infrastructure challenges in Jacobian-vector product (JVP) computation and the limitations of standard evaluation benchmarks. We first develop a parallelism-compatible FlashAttention-2 JVP kernel, enabling sCM training on models with over 10 billion parameters and high-dimensional video tasks. Our investigation reveals fundamental quality limitations of sCM in fine-detail generation, which we attribute to error accumulation and the "mode-covering" nature of its forward-divergence objective. To remedy this, we propose the score-regularized continuous-time consistency model (rCM), which incorporates score distillation as a long-skip regularizer. This integration complements sCM with the "mode-seeking" reverse divergence, effectively improving visual quality while maintaining high generation diversity. Validated on large-scale models (Cosmos-Predict2, Wan2.1) up to 14B parameters and 5-second videos, rCM matches or surpasses the state-of-the-art distillation method DMD2 on quality metrics while offering notable advantages in diversity, all without GAN tuning or extensive hyperparameter searches. The distilled models generate high-fidelity samples in only $1\sim4$ steps, accelerating diffusion sampling by $15\times\sim50\times$. These results position rCM as a practical and theoretically grounded framework for advancing large-scale diffusion distillation.

</details>


### [33] [Hierarchical Spatial Algorithms for High-Resolution Image Quantization and Feature Extraction](https://arxiv.org/abs/2510.08449)
*Noor Islam S. Mohammad*

Main category: cs.CV

TL;DR: 提出了一个模块化空间图像处理框架，包含灰度量化、色彩亮度增强、图像锐化、双向变换管道和几何特征提取，在多种数据集上展现了稳健的实时图像分析能力。


<details>
  <summary>Details</summary>
Motivation: 开发一个集成多种图像处理技术的模块化框架，以简化图像表示同时保持结构细节，提升实时图像分析和计算机视觉应用的性能。

Method: 采用分步强度变换进行灰度量化，在RGB和YCrCb色彩空间进行直方图均衡化，通过HSV值通道调整亮度，使用3×3卷积核进行图像锐化，并集成Canny边缘检测、Hough线估计、Harris角点检测等几何特征提取方法。

Result: 双向变换管道前向和反向过程准确率分别达到76.10%和74.80%，球杆对齐角度估计为51.50度，球杆隔离与真实图像的相似度达到81.87%。

Conclusion: 该框架在多样化数据集上表现出稳健和确定性的性能，展现了在实时图像分析和计算机视觉应用中的潜力。

Abstract: This study introduces a modular framework for spatial image processing, integrating grayscale quantization, color and brightness enhancement, image sharpening, bidirectional transformation pipelines, and geometric feature extraction. A stepwise intensity transformation quantizes grayscale images into eight discrete levels, producing a posterization effect that simplifies representation while preserving structural detail. Color enhancement is achieved via histogram equalization in both RGB and YCrCb color spaces, with the latter improving contrast while maintaining chrominance fidelity. Brightness adjustment is implemented through HSV value-channel manipulation, and image sharpening is performed using a 3 * 3 convolution kernel to enhance high-frequency details. A bidirectional transformation pipeline that integrates unsharp masking, gamma correction, and noise amplification achieved accuracy levels of 76.10% and 74.80% for the forward and reverse processes, respectively. Geometric feature extraction employed Canny edge detection, Hough-based line estimation (e.g., 51.50{\deg} for billiard cue alignment), Harris corner detection, and morphological window localization. Cue isolation further yielded 81.87\% similarity against ground truth images. Experimental evaluation across diverse datasets demonstrates robust and deterministic performance, highlighting its potential for real-time image analysis and computer vision.

</details>


### [34] [MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration](https://arxiv.org/abs/2510.08508)
*Lu Liu,Chunlei Cai,Shaocheng Shen,Jianfeng Liang,Weimin Ouyang,Tianxiao Ye,Jian Mao,Huiyu Duan,Jiangchao Yao,Xiaoyun Zhang,Qiang Hu,Guangtao Zhai*

Main category: cs.CV

TL;DR: MoA-VR是一个基于多智能体协作的视频修复系统，通过三个协调的智能体（退化识别、路由修复、质量评估）模拟人类专家的处理流程，有效应对复杂多样的视频退化问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界视频常因采集和传输条件不同而遭受复杂退化（如噪声、压缩伪影、低光失真），现有方法需要人工选择专用模型或采用单一架构，难以泛化处理各种退化情况。

Method: 提出MoA-VR系统，包含三个智能体：基于视觉语言模型的退化识别器、由大语言模型驱动的自适应路由器、以及专门为修复任务设计的视频质量评估模型。

Result: 大量实验表明，MoA-VR能有效处理多样化和复合退化，在客观指标和感知质量方面均优于现有基线方法。

Conclusion: 该研究展示了多模态智能和模块化推理在通用视频修复系统中的整合潜力，为处理复杂视频退化问题提供了新思路。

Abstract: Real-world videos often suffer from complex degradations, such as noise, compression artifacts, and low-light distortions, due to diverse acquisition and transmission conditions. Existing restoration methods typically require professional manual selection of specialized models or rely on monolithic architectures that fail to generalize across varying degradations. Inspired by expert experience, we propose MoA-VR, the first \underline{M}ixture-\underline{o}f-\underline{A}gents \underline{V}ideo \underline{R}estoration system that mimics the reasoning and processing procedures of human professionals through three coordinated agents: Degradation Identification, Routing and Restoration, and Restoration Quality Assessment. Specifically, we construct a large-scale and high-resolution video degradation recognition benchmark and build a vision-language model (VLM) driven degradation identifier. We further introduce a self-adaptive router powered by large language models (LLMs), which autonomously learns effective restoration strategies by observing tool usage patterns. To assess intermediate and final processed video quality, we construct the \underline{Res}tored \underline{V}ideo \underline{Q}uality (Res-VQ) dataset and design a dedicated VLM-based video quality assessment (VQA) model tailored for restoration tasks. Extensive experiments demonstrate that MoA-VR effectively handles diverse and compound degradations, consistently outperforming existing baselines in terms of both objective metrics and perceptual quality. These results highlight the potential of integrating multimodal intelligence and modular reasoning in general-purpose video restoration systems.

</details>


### [35] [FlexTraj: Image-to-Video Generation with Flexible Point Trajectory Control](https://arxiv.org/abs/2510.08527)
*Zhiyuan Zhang,Can Wang,Dongdong Chen,Jing Liao*

Main category: cs.CV

TL;DR: FlexTraj是一个用于图像到视频生成的框架，通过灵活的点轨迹控制实现多粒度运动控制，支持密集和稀疏轨迹，采用序列拼接方案实现高效训练和推理。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法缺乏对点轨迹的灵活控制能力，难以实现多粒度的运动控制，需要一种统一的点轨迹表示和高效的训练策略。

Method: 提出统一的点轨迹表示（分割ID、轨迹ID、颜色通道），采用序列拼接方案替代传统条件注入方法，使用退火训练策略逐步减少对完整监督和条件对齐的依赖。

Result: 实验结果表明FlexTraj能够实现多粒度、对齐无关的轨迹控制，支持运动克隆、拖拽式图像到视频、运动插值、相机重定向等多种应用。

Conclusion: FlexTraj框架通过统一的点轨迹表示和高效的训练策略，成功实现了灵活的点轨迹控制视频生成，在控制精度和效率方面表现出色。

Abstract: We present FlexTraj, a framework for image-to-video generation with flexible point trajectory control. FlexTraj introduces a unified point-based motion representation that encodes each point with a segmentation ID, a temporally consistent trajectory ID, and an optional color channel for appearance cues, enabling both dense and sparse trajectory control. Instead of injecting trajectory conditions into the video generator through token concatenation or ControlNet, FlexTraj employs an efficient sequence-concatenation scheme that achieves faster convergence, stronger controllability, and more efficient inference, while maintaining robustness under unaligned conditions. To train such a unified point trajectory-controlled video generator, FlexTraj adopts an annealing training strategy that gradually reduces reliance on complete supervision and aligned condition. Experimental results demonstrate that FlexTraj enables multi-granularity, alignment-agnostic trajectory control for video generation, supporting various applications such as motion cloning, drag-based image-to-video, motion interpolation, camera redirection, flexible action control and mesh animations.

</details>


### [36] [VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning](https://arxiv.org/abs/2510.08555)
*Minghong Cai,Qiulin Wang,Zongli Ye,Wenze Liu,Quande Liu,Weicai Ye,Xintao Wang,Pengfei Wan,Kun Gai,Xiangyu Yue*

Main category: cs.CV

TL;DR: 提出了VideoCanvas框架，通过零参数修改实现任意时空视频补全，将多种可控视频生成任务统一到单一范式下，解决了因果VAE引入的时间模糊问题。


<details>
  <summary>Details</summary>
Motivation: 现有潜在视频扩散模型存在时间模糊问题，多个像素帧被压缩到单个潜在表示中，使得精确的帧级控制变得困难。需要一种能够处理任意时空补全的统一框架。

Method: 采用混合条件策略：空间控制通过零填充处理，时间对齐通过时间RoPE插值实现，为每个条件分配潜在序列中的连续分数位置。基于In-Context Conditioning范式，无需新参数。

Result: 在VideoCanvasBench基准测试中显著优于现有条件范式，在帧内保真度和帧间创造力方面都表现出色，建立了灵活统一视频生成的新技术水平。

Conclusion: VideoCanvas成功解决了因果VAE的时间模糊问题，实现了像素帧感知的精确控制，为各种可控视频生成任务提供了统一的解决方案。

Abstract: We introduce the task of arbitrary spatio-temporal video completion, where a video is generated from arbitrary, user-specified patches placed at any spatial location and timestamp, akin to painting on a video canvas. This flexible formulation naturally unifies many existing controllable video generation tasks--including first-frame image-to-video, inpainting, extension, and interpolation--under a single, cohesive paradigm. Realizing this vision, however, faces a fundamental obstacle in modern latent video diffusion models: the temporal ambiguity introduced by causal VAEs, where multiple pixel frames are compressed into a single latent representation, making precise frame-level conditioning structurally difficult. We address this challenge with VideoCanvas, a novel framework that adapts the In-Context Conditioning (ICC) paradigm to this fine-grained control task with zero new parameters. We propose a hybrid conditioning strategy that decouples spatial and temporal control: spatial placement is handled via zero-padding, while temporal alignment is achieved through Temporal RoPE Interpolation, which assigns each condition a continuous fractional position within the latent sequence. This resolves the VAE's temporal ambiguity and enables pixel-frame-aware control on a frozen backbone. To evaluate this new capability, we develop VideoCanvasBench, the first benchmark for arbitrary spatio-temporal video completion, covering both intra-scene fidelity and inter-scene creativity. Experiments demonstrate that VideoCanvas significantly outperforms existing conditioning paradigms, establishing a new state of the art in flexible and unified video generation.

</details>


### [37] [MultiCOIN: Multi-Modal COntrollable Video INbetweening](https://arxiv.org/abs/2510.08561)
*Maham Tanveer,Yang Zhou,Simon Niklaus,Ali Mahdavi Amiri,Hao Zhang,Krishna Kumar Singh,Nanxuan Zhao*

Main category: cs.CV

TL;DR: 提出了一个多模态控制的视频插帧框架，支持深度过渡、运动轨迹、文本提示等多种控制方式，实现灵活、精确的视频插值


<details>
  <summary>Details</summary>
Motivation: 现有视频插帧方法无法生成复杂运动，缺乏对中间帧细节的精细控制，难以满足用户多样化创意需求

Method: 采用Diffusion Transformer架构，将多模态控制映射为统一的点表示，分离内容和运动控制为两个分支，采用分阶段训练策略

Result: 实验证明多模态控制能够实现更动态、可定制和上下文准确的视觉叙事

Conclusion: 该框架在灵活性、易用性和精细控制之间取得了平衡，为视频编辑和合成提供了强大的工具

Abstract: Video inbetweening creates smooth and natural transitions between two image frames, making it an indispensable tool for video editing and long-form video synthesis. Existing works in this domain are unable to generate large, complex, or intricate motions. In particular, they cannot accommodate the versatility of user intents and generally lack fine control over the details of intermediate frames, leading to misalignment with the creative mind. To fill these gaps, we introduce \modelname{}, a video inbetweening framework that allows multi-modal controls, including depth transition and layering, motion trajectories, text prompts, and target regions for movement localization, while achieving a balance between flexibility, ease of use, and precision for fine-grained video interpolation. To achieve this, we adopt the Diffusion Transformer (DiT) architecture as our video generative model, due to its proven capability to generate high-quality long videos. To ensure compatibility between DiT and our multi-modal controls, we map all motion controls into a common sparse and user-friendly point-based representation as the video/noise input. Further, to respect the variety of controls which operate at varying levels of granularity and influence, we separate content controls and motion controls into two branches to encode the required features before guiding the denoising process, resulting in two generators, one for motion and the other for content. Finally, we propose a stage-wise training strategy to ensure that our model learns the multi-modal controls smoothly. Extensive qualitative and quantitative experiments demonstrate that multi-modal controls enable a more dynamic, customizable, and contextually accurate visual narrative.

</details>


### [38] [ResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous Driving](https://arxiv.org/abs/2510.08562)
*Zhiyu Zheng,Shaoyu Chen,Haoran Yin,Xinbang Zhang,Jialv Zou,Xinggang Wang,Qian Zhang,Lefei Zhang*

Main category: cs.CV

TL;DR: ResAD提出了一种归一化残差轨迹建模框架，通过预测与确定性惯性参考的残差偏差来解决端到端自动驾驶中的时空不平衡问题，显著简化学习任务并提升性能。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶系统面临轨迹数据固有的时空不平衡问题，导致模型学习虚假相关性而非因果推理，并优先考虑不确定的远距离预测，从而危及即时安全。

Method: 提出ResAD框架：1）预测与确定性惯性参考的残差偏差而非直接预测轨迹；2）采用逐点归一化处理不确定长期视野引起的优化不平衡问题，重新加权优化目标。

Result: 在NAVSIM基准测试中，ResAD使用仅两个去噪步骤的普通扩散策略实现了88.6的最优PDMS分数，表明该方法显著简化了学习任务并提高了模型性能。

Conclusion: ResAD通过残差轨迹建模和归一化处理，有效解决了端到端自动驾驶中的时空不平衡问题，为因果推理和安全性提供了更好的解决方案。

Abstract: End-to-end autonomous driving (E2EAD) systems, which learn to predict future trajectories directly from sensor data, are fundamentally challenged by the inherent spatio-temporal imbalance of trajectory data. This imbalance creates a significant optimization burden, causing models to learn spurious correlations instead of causal inference, while also prioritizing uncertain, distant predictions, thereby compromising immediate safety. To address these issues, we propose ResAD, a novel Normalized Residual Trajectory Modeling framework. Instead of predicting the future trajectory directly, our approach reframes the learning task to predict the residual deviation from a deterministic inertial reference. The inertial reference serves as a counterfactual, forcing the model to move beyond simple pattern recognition and instead identify the underlying causal factors (e.g., traffic rules, obstacles) that necessitate deviations from a default, inertially-guided path. To deal with the optimization imbalance caused by uncertain, long-term horizons, ResAD further incorporates Point-wise Normalization of the predicted residual. It re-weights the optimization objective, preventing large-magnitude errors associated with distant, uncertain waypoints from dominating the learning signal. Extensive experiments validate the effectiveness of our framework. On the NAVSIM benchmark, ResAD achieves a state-of-the-art PDMS of 88.6 using a vanilla diffusion policy with only two denoising steps, demonstrating that our approach significantly simplifies the learning task and improves model performance. The code will be released to facilitate further research.

</details>


### [39] [D$^2$GS: Depth-and-Density Guided Gaussian Splatting for Stable and Accurate Sparse-View Reconstruction](https://arxiv.org/abs/2510.08566)
*Meixi Song,Xin Lin,Dizhe Zhang,Haodong Li,Xiangtai Li,Bo Du,Lu Qi*

Main category: cs.CV

TL;DR: 提出了D²GS框架，通过深度密度引导的dropout策略和距离感知保真度增强模块，解决稀疏视图下3D高斯溅射的过拟合和欠拟合问题。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射在稀疏视图条件下存在性能下降和不稳定的问题，主要源于相机附近高斯密度过高的过拟合和远处区域高斯覆盖不足的欠拟合。

Method: 1. 深度密度引导dropout策略：基于密度和深度自适应掩码冗余高斯；2. 距离感知保真度增强模块：通过针对性监督提升远场区域重建质量；3. 提出新的评估指标量化高斯分布稳定性。

Result: 在多个数据集上的实验表明，该方法在稀疏视图条件下显著提升了视觉质量和鲁棒性。

Conclusion: D²GS框架有效解决了稀疏视图下3D高斯溅射的过拟合和欠拟合问题，提高了重建质量和稳定性。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) enable real-time, high-fidelity novel view synthesis (NVS) with explicit 3D representations. However, performance degradation and instability remain significant under sparse-view conditions. In this work, we identify two key failure modes under sparse-view conditions: overfitting in regions with excessive Gaussian density near the camera, and underfitting in distant areas with insufficient Gaussian coverage. To address these challenges, we propose a unified framework D$^2$GS, comprising two key components: a Depth-and-Density Guided Dropout strategy that suppresses overfitting by adaptively masking redundant Gaussians based on density and depth, and a Distance-Aware Fidelity Enhancement module that improves reconstruction quality in under-fitted far-field areas through targeted supervision. Moreover, we introduce a new evaluation metric to quantify the stability of learned Gaussian distributions, providing insights into the robustness of the sparse-view 3DGS. Extensive experiments on multiple datasets demonstrate that our method significantly improves both visual quality and robustness under sparse view conditions. The project page can be found at: https://insta360-research-team.github.io/DDGS-website/.

</details>


### [40] [ReSplat: Learning Recurrent Gaussian Splats](https://arxiv.org/abs/2510.08575)
*Haofei Xu,Daniel Barath,Andreas Geiger,Marc Pollefeys*

Main category: cs.CV

TL;DR: ReSplat是一种前馈循环高斯泼溅模型，通过迭代优化3D高斯而不显式计算梯度，利用渲染误差作为反馈信号，在减少高斯数量和提升渲染速度的同时实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统前馈高斯泼溅模型依赖单次前向传播，性能受限。需要一种能迭代优化高斯表示的方法，同时保持计算效率。

Method: 提出循环高斯泼溅模型，使用渲染误差作为反馈信号指导高斯更新；引入紧凑重建模型在16倍降采样空间初始化高斯，大幅减少高斯数量。

Result: 在多种输入视图(2,8,16)、分辨率(256×256到540×960)和数据集(DL3DV、RealEstate10K)上的实验表明，该方法在显著减少高斯数量和提升渲染速度的同时达到最先进性能。

Conclusion: ReSplat通过循环反馈机制和紧凑初始化策略，成功解决了前馈高斯泼溅模型的性能限制，实现了高效且鲁棒的3D重建。

Abstract: While feed-forward Gaussian splatting models provide computational efficiency and effectively handle sparse input settings, their performance is fundamentally limited by the reliance on a single forward pass during inference. We propose ReSplat, a feed-forward recurrent Gaussian splatting model that iteratively refines 3D Gaussians without explicitly computing gradients. Our key insight is that the Gaussian splatting rendering error serves as a rich feedback signal, guiding the recurrent network to learn effective Gaussian updates. This feedback signal naturally adapts to unseen data distributions at test time, enabling robust generalization. To initialize the recurrent process, we introduce a compact reconstruction model that operates in a $16 \times$ subsampled space, producing $16 \times$ fewer Gaussians than previous per-pixel Gaussian models. This substantially reduces computational overhead and allows for efficient Gaussian updates. Extensive experiments across varying of input views (2, 8, 16), resolutions ($256 \times 256$ to $540 \times 960$), and datasets (DL3DV and RealEstate10K) demonstrate that our method achieves state-of-the-art performance while significantly reducing the number of Gaussians and improving the rendering speed. Our project page is at https://haofeixu.github.io/resplat/.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [41] [NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos](https://arxiv.org/abs/2510.08568)
*Hongyu Li,Lingfeng Sun,Yafei Hu,Duy Ta,Jennifer Barry,George Konidaris,Jiahui Fu*

Main category: cs.RO

TL;DR: NovaFlow是一个零样本机器人操作框架，能够将任务描述转化为可执行计划，无需演示或特定机器人训练，支持刚性、关节和可变形物体的操作。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要任务分布内数据或针对特定机器人的微调，限制了跨平台迁移能力。

Method: 使用视频生成模型将任务描述合成为视频，通过感知模块提取3D物体流，为刚性物体计算相对位姿，为可变形物体提供基于粒子动力学模型的跟踪目标。

Result: 在桌面Franka机械臂和Spot四足移动机器人上验证了刚性、关节和可变形物体操作任务的有效零样本执行。

Conclusion: 通过将任务理解与底层控制解耦，NovaFlow实现了跨机器人平台的零样本操作能力。

Abstract: Enabling robots to execute novel manipulation tasks zero-shot is a central goal in robotics. Most existing methods assume in-distribution tasks or rely on fine-tuning with embodiment-matched data, limiting transfer across platforms. We present NovaFlow, an autonomous manipulation framework that converts a task description into an actionable plan for a target robot without any demonstrations. Given a task description, NovaFlow synthesizes a video using a video generation model and distills it into 3D actionable object flow using off-the-shelf perception modules. From the object flow, it computes relative poses for rigid objects and realizes them as robot actions via grasp proposals and trajectory optimization. For deformable objects, this flow serves as a tracking objective for model-based planning with a particle-based dynamics model. By decoupling task understanding from low-level control, NovaFlow naturally transfers across embodiments. We validate on rigid, articulated, and deformable object manipulation tasks using a table-top Franka arm and a Spot quadrupedal mobile robot, and achieve effective zero-shot execution without demonstrations or embodiment-specific training. Project website: https://novaflow.lhy.xyz/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [42] [Biology-driven assessment of deep learning super-resolution imaging of the porosity network in dentin](https://arxiv.org/abs/2510.08407)
*Lauren Anderson,Lucas Chatelain,Nicolas Tremblay,Kathryn Grandfield,David Rousseau,Aurélien Gourrier*

Main category: cs.LG

TL;DR: 本研究测试了多种深度学习超分辨率模型，用于从低分辨率共聚焦图像中恢复牙齿孔隙网络的高分辨率图像，并通过生物驱动的评估方法验证模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前牙齿机械感觉系统的研究受限于共聚焦显微镜的高分辨率但小视野范围，需要开发能够从低分辨率图像恢复高质量图像的方法来加速实验采集。

Method: 使用三种有监督2D超分辨率模型（RCAN、pix2pix、FSRCNN）和一种无监督模型（CycleGAN），在实验配对的低分辨率和高分辨率共聚焦图像上进行训练和测试，像素尺寸增加倍数为2、4、8倍。

Result: 传统图像质量评估指标与视觉感知不一致，通过基于孔隙网络结构和连通性的生物驱动评估方法，发现不同模型在保留弱强度特征和3D连通性方面存在差异。

Conclusion: 标准图像质量评估指标不适用于牙齿孔隙网络这种特定结构，生物驱动的评估方法能更好地解释超分辨率模型的性能差异，特别是对弱强度特征敏感性和图像生成非线性的影响。

Abstract: The mechanosensory system of teeth is currently believed to partly rely on Odontoblast cells stimulation by fluid flow through a porosity network extending through dentin. Visualizing the smallest sub-microscopic porosity vessels therefore requires the highest achievable resolution from confocal fluorescence microscopy, the current gold standard. This considerably limits the extent of the field of view to very small sample regions. To overcome this limitation, we tested different deep learning (DL) super-resolution (SR) models to allow faster experimental acquisitions of lower resolution images and restore optimal image quality by post-processing. Three supervised 2D SR models (RCAN, pix2pix, FSRCNN) and one unsupervised (CycleGAN) were applied to a unique set of experimentally paired high- and low-resolution confocal images acquired with different sampling schemes, resulting in a pixel size increase of x2, x4, x8. Model performance was quantified using a broad set of similarity and distribution-based image quality assessment (IQA) metrics, which yielded inconsistent results that mostly contradicted our visual perception. This raises the question of the relevance of such generic metrics to efficiently target the specific structure of dental porosity. To resolve this conflicting information, the generated SR images were segmented taking into account the specific scales and morphology of the porosity network and analysed by comparing connected components. Additionally, the capacity of the SR models to preserve 3D porosity connectivity throughout the confocal image stacks was evaluated using graph analysis. This biology-driven assessment allowed a far better mechanistic interpretation of SR performance, highlighting differences in model sensitivity to weak intensity features and the impact of non-linearity in image generation, which explains the failure of standard IQA metrics.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [43] [FlowLensing: Simulating Gravitational Lensing with Flow Matching](https://arxiv.org/abs/2510.07878)
*Hamees Sayed,Pranath Reddy,Michael W. Toomey,Sergei Gleyzer*

Main category: astro-ph.IM

TL;DR: FlowLensing是一个基于扩散变换器的高效流匹配模型，用于强引力透镜模拟，相比传统方法加速200倍以上，支持离散和连续参数，确保物理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有引力透镜模拟工具依赖光线追踪或前向建模管道，虽然精确但速度极慢，无法满足大规模模拟需求，阻碍了暗物质研究。

Method: 使用基于扩散变换器的紧凑高效流匹配模型，在离散和连续参数下运行，处理不同暗物质模型，确保物理一致性。

Result: 模型相比传统暗物质模型模拟器实现200倍以上的加速，具有高保真度和低推理延迟。

Conclusion: FlowLensing能够实现快速、可扩展且物理一致的图像合成，为传统前向建模管道提供了实用替代方案，可推进暗物质子结构研究。

Abstract: Gravitational lensing is one of the most powerful probes of dark matter, yet creating high-fidelity lensed images at scale remains a bottleneck. Existing tools rely on ray-tracing or forward-modeling pipelines that, while precise, are prohibitively slow. We introduce FlowLensing, a Diffusion Transformer-based compact and efficient flow-matching model for strong gravitational lensing simulation. FlowLensing operates in both discrete and continuous regimes, handling classes such as different dark matter models as well as continuous model parameters ensuring physical consistency. By enabling scalable simulations, our model can advance dark matter studies, specifically for probing dark matter substructure in cosmological surveys. We find that our model achieves a speedup of over 200$\times$ compared to classical simulators for intensive dark matter models, with high fidelity and low inference latency. FlowLensing enables rapid, scalable, and physically consistent image synthesis, offering a practical alternative to traditional forward-modeling pipelines.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [44] [SatFusion: A Unified Framework for Enhancing Satellite IoT Images via Multi-Temporal and Multi-Source Data Fusion](https://arxiv.org/abs/2510.07905)
*Yufei Tong,Guanjie Cheng,Peihan Wu,Yicheng Zhu,Kexu Lu,Feiyi Chen,Meng Xi,Junqin Huang,Shuiguang Deng*

Main category: eess.IV

TL;DR: SatFusion是一个统一框架，通过多时相和多源数据融合来增强卫星物联网图像质量，结合了MISR和pansharpening的优势，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法充分利用时相和源维度的互补信息：MISR受限于输入图像的细粒度纹理细节，pansharpening对噪声和配准误差敏感。需要开发能同时利用多时相和多源信息的统一方法。

Method: SatFusion框架包含三个模块：多时相图像融合模块实现与全色图像的深度特征对齐；多源图像融合模块注入全色数据的细粒度纹理信息；融合组合模块自适应整合两种模态的优势并动态优化光谱一致性。

Result: 在WorldStrat、WV3、QB和GF2数据集上的实验表明，SatFusion显著提高了融合质量、在挑战性条件下的鲁棒性，以及对真实Sat-IoT场景的泛化能力。

Conclusion: SatFusion通过统一的多时相和多源融合框架，有效提升了卫星物联网图像增强的性能，为实际应用提供了更可靠的解决方案。

Abstract: With the rapid advancement of the digital society, the proliferation of satellites in the Satellite Internet of Things (Sat-IoT) has led to the continuous accumulation of large-scale multi-temporal and multi-source images across diverse application scenarios. However, existing methods fail to fully exploit the complementary information embedded in both temporal and source dimensions. For example, Multi-Image Super-Resolution (MISR) enhances reconstruction quality by leveraging temporal complementarity across multiple observations, yet the limited fine-grained texture details in input images constrain its performance. Conversely, pansharpening integrates multi-source images by injecting high-frequency spatial information from panchromatic data, but typically relies on pre-interpolated low-resolution inputs and assumes noise-free alignment, making it highly sensitive to noise and misregistration. To address these issues, we propose SatFusion: A Unified Framework for Enhancing Satellite IoT Images via Multi-Temporal and Multi-Source Data Fusion. Specifically, SatFusion first employs a Multi-Temporal Image Fusion (MTIF) module to achieve deep feature alignment with the panchromatic image. Then, a Multi-Source Image Fusion (MSIF) module injects fine-grained texture information from the panchromatic data. Finally, a Fusion Composition module adaptively integrates the complementary advantages of both modalities while dynamically refining spectral consistency, supervised by a weighted combination of multiple loss functions. Extensive experiments on the WorldStrat, WV3, QB, and GF2 datasets demonstrate that SatFusion significantly improves fusion quality, robustness under challenging conditions, and generalizability to real-world Sat-IoT scenarios. The code is available at: https://github.com/dllgyufei/SatFusion.git.

</details>
