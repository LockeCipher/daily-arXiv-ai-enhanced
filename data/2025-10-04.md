<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 3]
- [cs.CV](#cs.CV) [Total: 29]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.LG](#cs.LG) [Total: 4]
- [physics.optics](#physics.optics) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust Physics-Based Dynamics](https://arxiv.org/abs/2510.01619)
*Changmin Lee,Jihyun Lee,Tae-Kyun Kim*

Main category: cs.GR

TL;DR: MPMAvatar是一个从多视角视频创建3D人体化身的框架，支持高度真实、鲁棒的动画和自由视角的光线真实渲染。


<details>
  <summary>Details</summary>
Motivation: 现有基于物理模拟的方法在建模宽松服装的物理合理动态方面存在准确性或鲁棒性不足的问题，特别是在处理新动画输入时。

Method: 使用基于物质点法的模拟器，结合各向异性本构模型和新型碰撞处理算法来建模复杂服装变形和与身体的接触，并与基于3D高斯溅射的可渲染规范化身结合。

Result: MPMAvatar在动态建模准确性、渲染准确性和鲁棒性及效率方面显著优于现有最先进的基于物理的化身方法，并能以零样本方式泛化到未见过的交互。

Conclusion: 该方法成功解决了建模宽松服装物理动态的挑战，实现了高保真渲染和物理真实动画，并展示了优于现有方法的性能。

Abstract: While there has been significant progress in the field of 3D avatar creation from visual observations, modeling physically plausible dynamics of humans with loose garments remains a challenging problem. Although a few existing works address this problem by leveraging physical simulation, they suffer from limited accuracy or robustness to novel animation inputs. In this work, we present MPMAvatar, a framework for creating 3D human avatars from multi-view videos that supports highly realistic, robust animation, as well as photorealistic rendering from free viewpoints. For accurate and robust dynamics modeling, our key idea is to use a Material Point Method-based simulator, which we carefully tailor to model garments with complex deformations and contact with the underlying body by incorporating an anisotropic constitutive model and a novel collision handling algorithm. We combine this dynamics modeling scheme with our canonical avatar that can be rendered using 3D Gaussian Splatting with quasi-shadowing, enabling high-fidelity rendering for physically realistic animations. In our experiments, we demonstrate that MPMAvatar significantly outperforms the existing state-of-the-art physics-based avatar in terms of (1) dynamics modeling accuracy, (2) rendering accuracy, and (3) robustness and efficiency. Additionally, we present a novel application in which our avatar generalizes to unseen interactions in a zero-shot manner-which was not achievable with previous learning-based methods due to their limited simulation generalizability. Our project page is at: https://KAISTChangmin.github.io/MPMAvatar/

</details>


### [2] [ROI-GS: Interest-based Local Quality 3D Gaussian Splatting](https://arxiv.org/abs/2510.01978)
*Quoc-Anh Bui,Gilles Rougeron,Géraldine Morin,Simone Gasparini*

Main category: cs.GR

TL;DR: ROI-GS：一种面向感兴趣区域的高效3D高斯泼溅重建框架，通过对象感知方法提升局部细节质量，同时减少模型大小和训练时间。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法在场景中均匀分配资源，限制了感兴趣区域的精细细节重建，并导致模型尺寸膨胀。

Method: 提出对象感知框架，包括对象引导的相机选择、针对性对象训练，以及将高保真对象重建无缝集成到全局场景中。

Result: 显著提升局部质量（PSNR最高提升2.96 dB），模型大小减少约17%，对单对象场景训练更快，优于现有方法。

Conclusion: ROI-GS在保持实时性能的同时，有效提升了感兴趣区域的细节重建质量并优化了资源利用。

Abstract: We tackle the challenge of efficiently reconstructing 3D scenes with high detail on objects of interest. Existing 3D Gaussian Splatting (3DGS) methods allocate resources uniformly across the scene, limiting fine detail to Regions Of Interest (ROIs) and leading to inflated model size. We propose ROI-GS, an object-aware framework that enhances local details through object-guided camera selection, targeted Object training, and seamless integration of high-fidelity object of interest reconstructions into the global scene. Our method prioritizes higher resolution details on chosen objects while maintaining real-time performance. Experiments show that ROI-GS significantly improves local quality (up to 2.96 dB PSNR), while reducing overall model size by $\approx 17\%$ of baseline and achieving faster training for a scene with a single object of interest, outperforming existing methods.

</details>


### [3] [Spec-Gloss Surfels and Normal-Diffuse Priors for Relightable Glossy Objects](https://arxiv.org/abs/2510.02069)
*Georgios Kouros,Minye Wu,Tinne Tuytelaars*

Main category: cs.GR

TL;DR: 提出了一种可重光照框架，将微表面BRDF与高光-光泽度参数化集成到2D高斯泼溅中，通过延迟着色实现更物理一致的材料分解，在复杂光泽场景中实现高质量几何和材质重建。


<details>
  <summary>Details</summary>
Motivation: 光泽物体的精确重建和重光照是一个长期挑战，因为物体形状、材质属性和光照难以分离。现有神经渲染方法通常依赖简化的BRDF模型或耦合漫反射和镜面反射分量的参数化，限制了材质恢复的忠实度和重光照保真度。

Method: 将微表面BRDF与高光-光泽度参数化集成到2D高斯泼溅中，采用延迟着色；使用表面法线和漫反射颜色的扩散先验指导早期优化并缓解歧义；对环境贴图进行由粗到细的优化以加速收敛并保持高动态范围镜面反射。

Result: 在复杂光泽场景上的广泛实验表明，该方法实现了高质量的几何和材质重建，与现有高斯泼溅方法相比，在新光照下提供了更真实和一致的重光照效果。

Conclusion: 该方法通过物理一致的材质分解和由粗到细的优化策略，显著提升了光泽物体的重建和重光照质量，为神经渲染中的材质恢复和光照编辑提供了更可靠的解决方案。

Abstract: Accurate reconstruction and relighting of glossy objects remain a longstanding challenge, as object shape, material properties, and illumination are inherently difficult to disentangle. Existing neural rendering approaches often rely on simplified BRDF models or parameterizations that couple diffuse and specular components, which restricts faithful material recovery and limits relighting fidelity. We propose a relightable framework that integrates a microfacet BRDF with the specular-glossiness parameterization into 2D Gaussian Splatting with deferred shading. This formulation enables more physically consistent material decomposition, while diffusion-based priors for surface normals and diffuse color guide early-stage optimization and mitigate ambiguity. A coarse-to-fine optimization of the environment map accelerates convergence and preserves high-dynamic-range specular reflections. Extensive experiments on complex, glossy scenes demonstrate that our method achieves high-quality geometry and material reconstruction, delivering substantially more realistic and consistent relighting under novel illumination compared to existing Gaussian splatting methods.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [4] [LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration](https://arxiv.org/abs/2510.01339)
*Alessio Spagnoletti,Andrés Almansa,Marcelo Pereyra*

Main category: cs.CV

TL;DR: 提出了LVTINO，首个基于视频一致性模型的零样本视频修复方法，解决了帧间应用图像扩散模型导致的时间不一致问题


<details>
  <summary>Details</summary>
Motivation: 现有基于图像扩散模型的零样本图像修复方法难以直接扩展到高分辨率视频修复，因为需要同时恢复空间细节和捕捉时间依赖关系，帧间独立处理会导致时间不一致

Method: 利用视频一致性模型(VCMs)作为先验，构建零样本或即插即用视频逆求解器，通过条件机制绕过自动微分，仅需少量神经网络评估

Result: 在多种视频逆问题上显著优于当前基于图像LDM的逐帧方法，在重建保真度和计算效率方面都达到了新水平

Conclusion: LVTINO通过视频一致性模型实现了高质量、时间一致的视频修复，为视频逆问题提供了新的基准

Abstract: Computational imaging methods increasingly rely on powerful generative diffusion models to tackle challenging image restoration tasks. In particular, state-of-the-art zero-shot image inverse solvers leverage distilled text-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy and perceptual quality with high computational efficiency. However, extending these advances to high-definition video restoration remains a significant challenge, due to the need to recover fine spatial detail while capturing subtle temporal dependencies. Consequently, methods that naively apply image-based LDM priors on a frame-by-frame basis often result in temporally inconsistent reconstructions. We address this challenge by leveraging recent advances in Video Consistency Models (VCMs), which distill video latent diffusion models into fast generators that explicitly capture temporal causality. Building on this foundation, we propose LVTINO, the first zero-shot or plug-and-play inverse solver for high definition video restoration with priors encoded by VCMs. Our conditioning mechanism bypasses the need for automatic differentiation and achieves state-of-the-art video reconstruction quality with only a few neural function evaluations, while ensuring strong measurement consistency and smooth temporal transitions across frames. Extensive experiments on a diverse set of video inverse problems show significant perceptual improvements over current state-of-the-art methods that apply image LDMs frame by frame, establishing a new benchmark in both reconstruction fidelity and computational efficiency.

</details>


### [5] [Image Generation Based on Image Style Extraction](https://arxiv.org/abs/2510.01347)
*Shuochen Chang*

Main category: cs.CV

TL;DR: 提出基于风格提取的三阶段训练图像生成方法，通过风格编码器和投影层将风格表示与文本表示对齐，实现细粒度文本引导的风格化图像生成。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像生成中细粒度风格难以用自然语言精确描述和控制的问题，同时解决风格参考图像的引导信息难以与传统文本引导生成对齐的挑战。

Method: 使用风格编码器和风格投影层提取单张风格参考图像的细粒度风格表示，并将其注入预训练生成模型，不改变下游生成模型的结构框架。构建包含图像、风格标签和文本描述三元组的Style30k-captions数据集进行训练。

Result: 实现了细粒度控制的风格化图像生成，能够从单张风格参考图像中提取风格表示并与文本条件对齐。

Conclusion: 该方法能够最大化预训练生成模型的生成能力，通过风格提取和投影技术实现细粒度文本引导的风格化图像生成。

Abstract: Image generation based on text-to-image generation models is a task with practical application scenarios that fine-grained styles cannot be precisely described and controlled in natural language, while the guidance information of stylized reference images is difficult to be directly aligned with the textual conditions of traditional textual guidance generation. This study focuses on how to maximize the generative capability of the pretrained generative model, by obtaining fine-grained stylistic representations from a single given stylistic reference image, and injecting the stylistic representations into the generative body without changing the structural framework of the downstream generative model, so as to achieve fine-grained controlled stylized image generation. In this study, we propose a three-stage training style extraction-based image generation method, which uses a style encoder and a style projection layer to align the style representations with the textual representations to realize fine-grained textual cue-based style guide generation. In addition, this study constructs the Style30k-captions dataset, whose samples contain a triad of images, style labels, and text descriptions, to train the style encoder and style projection layer in this experiment.

</details>


### [6] [Purrception: Variational Flow Matching for Vector-Quantized Image Generation](https://arxiv.org/abs/2510.01478)
*Răzvan-Andrei Matişan,Vincent Tao Hu,Grigory Bartosh,Björn Ommer,Cees G. M. Snoek,Max Welling,Jan-Willem van de Meent,Mohammad Mahdi Derakhshani,Floor Eijkelboom*

Main category: cs.CV

TL;DR: Purrception是一种用于矢量量化图像生成的变分流匹配方法，结合了连续传输动力学和显式分类监督，在ImageNet-1k 256x256生成任务上实现了更快的训练收敛和竞争力的FID分数。


<details>
  <summary>Details</summary>
Motivation: 现有方法在矢量量化图像生成中要么使用连续方法缺乏离散监督，要么使用分类方法缺乏几何感知能力。Purrception旨在结合两者的优势，在保持连续传输动态的同时提供显式分类监督。

Method: 通过将变分流匹配适配到矢量量化潜在空间，学习码本索引的分类后验分布，同时在连续嵌入空间中计算速度场，结合了连续方法的几何感知和分类方法的离散监督。

Result: 在ImageNet-1k 256x256生成任务上，训练收敛速度比连续流匹配和离散流匹配基线更快，同时达到了与最先进模型相竞争的FID分数。

Conclusion: 变分流匹配可以有效地桥接连续传输和离散监督，提高图像生成的训练效率，证明了该方法在矢量量化图像生成中的有效性。

Abstract: We introduce Purrception, a variational flow matching approach for vector-quantized image generation that provides explicit categorical supervision while maintaining continuous transport dynamics. Our method adapts Variational Flow Matching to vector-quantized latents by learning categorical posteriors over codebook indices while computing velocity fields in the continuous embedding space. This combines the geometric awareness of continuous methods with the discrete supervision of categorical approaches, enabling uncertainty quantification over plausible codes and temperature-controlled generation. We evaluate Purrception on ImageNet-1k 256x256 generation. Training converges faster than both continuous flow matching and discrete flow matching baselines while achieving competitive FID scores with state-of-the-art models. This demonstrates that Variational Flow Matching can effectively bridge continuous transport and discrete supervision for improved training efficiency in image generation.

</details>


### [7] [AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging](https://arxiv.org/abs/2510.01498)
*Yuxuan Ou,Ning Bi,Jiazhen Pan,Jiancheng Yang,Boliang Yu,Usama Zidan,Regent Lee,Vicente Grau*

Main category: cs.CV

TL;DR: 提出了一种统一的深度学习框架，通过条件扩散模型和多任务学习，从非对比CT扫描中同时生成合成对比增强CT图像并分割主动脉腔和血栓，避免了多阶段方法的误差累积问题。


<details>
  <summary>Details</summary>
Motivation: 传统对比增强CT扫描需要碘造影剂，存在肾毒性、患者过敏和环境危害等风险。现有方法采用多阶段流程，先生成图像再分割，导致误差累积且无法共享语义和解剖结构信息。

Method: 集成条件扩散模型与多任务学习，实现端到端的图像合成和分割联合优化。共享编码器和解码器参数，采用半监督训练策略处理缺失分割标签的临床数据。

Result: 在264名患者队列中表现优于现有方法：图像合成PSNR达25.61 dB，腔分割Dice分数0.89，血栓分割Dice分数0.53，腔直径MAE降至4.19 mm，血栓面积误差降至33.85%。

Conclusion: 该统一框架在减少造影剂使用的同时，提高了图像合成和分割的准确性，为临床AAA评估提供了更安全有效的解决方案。

Abstract: While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic aneurysms (AAA), the required iodinated contrast agents pose significant risks, including nephrotoxicity, patient allergies, and environmental harm. To reduce contrast agent use, recent deep learning methods have focused on generating synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a multi-stage pipeline that first generates images and then performs segmentation, which leads to error accumulation and fails to leverage shared semantic and anatomical structures. To address this, we propose a unified deep learning framework that generates synthetic CECT images from NCCT scans while simultaneously segmenting the aortic lumen and thrombus. Our approach integrates conditional diffusion models (CDM) with multi-task learning, enabling end-to-end joint optimization of image synthesis and anatomical segmentation. Unlike previous multitask diffusion models, our approach requires no initial predictions (e.g., a coarse segmentation mask), shares both encoder and decoder parameters across tasks, and employs a semi-supervised training strategy to learn from scans with missing segmentation labels, a common constraint in real-world clinical data. We evaluated our method on a cohort of 264 patients, where it consistently outperformed state-of-the-art single-task and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61 dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation, it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to nnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.

</details>


### [8] [Towards Better Optimization For Listwise Preference in Diffusion Models](https://arxiv.org/abs/2510.01540)
*Jiamu Bai,Xin Yu,Meilong Xu,Weitao Lu,Xin Pan,Kiwan Maeng,Daniel Kifer,Jian Wang,Yu Wang*

Main category: cs.CV

TL;DR: 提出了Diffusion-LPO框架，通过列表式偏好优化来改进文本到图像扩散模型的对齐效果，相比成对偏好优化能更好地利用人类反馈中的排序信息。


<details>
  <summary>Details</summary>
Motivation: 现有DPO方法主要依赖成对偏好，但人类对图像的偏好反馈通常包含隐含的排序信息，这些信息比成对比较更能精确表达人类偏好。

Method: 基于Plackett-Luce模型推导列表式DPO目标，给定标题时将用户反馈聚合成图像排名列表，强制整个排名的一致性，鼓励每个样本优于其所有低排名替代项。

Result: 在文本到图像生成、图像编辑和个性化偏好对齐等任务中，Diffusion-LPO在视觉质量和偏好对齐方面始终优于成对DPO基线方法。

Conclusion: Diffusion-LPO是一个简单有效的列表式偏好优化框架，能够更好地利用人类反馈中的排序信息来对齐扩散模型与人类偏好。

Abstract: Reinforcement learning from human feedback (RLHF) has proven effectiveness for aligning text-to-image (T2I) diffusion models with human preferences. Although Direct Preference Optimization (DPO) is widely adopted for its computational efficiency and avoidance of explicit reward modeling, its applications to diffusion models have primarily relied on pairwise preferences. The precise optimization of listwise preferences remains largely unaddressed. In practice, human feedback on image preferences often contains implicit ranked information, which conveys more precise human preferences than pairwise comparisons. In this work, we propose Diffusion-LPO, a simple and effective framework for Listwise Preference Optimization in diffusion models with listwise data. Given a caption, we aggregate user feedback into a ranked list of images and derive a listwise extension of the DPO objective under the Plackett-Luce model. Diffusion-LPO enforces consistency across the entire ranking by encouraging each sample to be preferred over all of its lower-ranked alternatives. We empirically demonstrate the effectiveness of Diffusion-LPO across various tasks, including text-to-image generation, image editing, and personalized preference alignment. Diffusion-LPO consistently outperforms pairwise DPO baselines on visual quality and preference alignment.

</details>


### [9] [NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems](https://arxiv.org/abs/2510.01608)
*Roman Jacome,Romario Gualdrón-Hurtado,Leon Suarez,Henry Arguello*

Main category: cs.CV

TL;DR: 提出了一种新的正则化方法NPN，通过神经网络在感知矩阵的零空间低维投影中寻找解，而不是在图像域施加结构约束，提高了逆问题重建的可解释性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统逆问题求解方法通常忽略感知矩阵零空间的特定结构，导致先验信息利用不充分。本文旨在设计能够捕捉与感知过程正交信息的特定感知矩阵先验。

Method: 提出非线性零空间投影(NPN)方法，使用神经网络在感知矩阵的零空间中进行低维投影，该正则化与现有重建框架兼容且可补充传统图像域先验。

Result: 在压缩感知、去模糊、超分辨率、CT和MRI等多种成像逆问题中，NPN先验能持续提升重建保真度，适用于plug-and-play方法、展开网络、深度图像先验和扩散模型。

Conclusion: NPN通过关注零空间结构提供了解释性强且灵活的正则化方法，理论保证收敛性和重建精度，在多种逆问题中表现优异。

Abstract: Imaging inverse problems aims to recover high-dimensional signals from undersampled, noisy measurements, a fundamentally ill-posed task with infinite solutions in the null-space of the sensing operator. To resolve this ambiguity, prior information is typically incorporated through handcrafted regularizers or learned models that constrain the solution space. However, these priors typically ignore the task-specific structure of that null-space. In this work, we propose \textit{Non-Linear Projections of the Null-Space} (NPN), a novel class of regularization that, instead of enforcing structural constraints in the image domain, promotes solutions that lie in a low-dimensional projection of the sensing matrix's null-space with a neural network. Our approach has two key advantages: (1) Interpretability: by focusing on the structure of the null-space, we design sensing-matrix-specific priors that capture information orthogonal to the signal components that are fundamentally blind to the sensing process. (2) Flexibility: NPN is adaptable to various inverse problems, compatible with existing reconstruction frameworks, and complementary to conventional image-domain priors. We provide theoretical guarantees on convergence and reconstruction accuracy when used within plug-and-play methods. Empirical results across diverse sensing matrices demonstrate that NPN priors consistently enhance reconstruction fidelity in various imaging inverse problems, such as compressive sensing, deblurring, super-resolution, computed tomography, and magnetic resonance imaging, with plug-and-play methods, unrolling networks, deep image prior, and diffusion models.

</details>


### [10] [Joint Deblurring and 3D Reconstruction for Macrophotography](https://arxiv.org/abs/2510.01640)
*Yifan Zhao,Liangchen Li,Yuqi Zhou,Kai Wang,Yan Liang,Juyong Zhang*

Main category: cs.CV

TL;DR: 提出了一种用于微距摄影的联合去模糊和3D重建方法，从多视角模糊图像中联合优化物体的清晰3D模型和每个像素的离焦模糊核。


<details>
  <summary>Details</summary>
Motivation: 微距镜头具有高分辨率和大放大倍率的优势，但离焦模糊问题严重阻碍了清晰成像和高质量3D重建。传统方法需要大量图像和标注，且目前没有针对微距摄影的多视角3D重建方法。

Method: 采用可微分渲染方法自监督优化3D模型和离焦模糊核，从多视角模糊图像出发，联合优化清晰3D模型和每个像素的模糊核。

Result: 大量实验表明，从少量多视角图像中，该方法不仅能实现高质量图像去模糊，还能恢复高保真度的3D外观。

Conclusion: 该方法成功解决了微距摄影中的离焦模糊问题，实现了联合去模糊和高质量3D重建。

Abstract: Macro lens has the advantages of high resolution and large magnification, and 3D modeling of small and detailed objects can provide richer information. However, defocus blur in macrophotography is a long-standing problem that heavily hinders the clear imaging of the captured objects and high-quality 3D reconstruction of them. Traditional image deblurring methods require a large number of images and annotations, and there is currently no multi-view 3D reconstruction method for macrophotography. In this work, we propose a joint deblurring and 3D reconstruction method for macrophotography. Starting from multi-view blurry images captured, we jointly optimize the clear 3D model of the object and the defocus blur kernel of each pixel. The entire framework adopts a differentiable rendering method to self-supervise the optimization of the 3D model and the defocus blur kernel. Extensive experiments show that from a small number of multi-view images, our proposed method can not only achieve high-quality image deblurring but also recover high-fidelity 3D appearance.

</details>


### [11] [FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring](https://arxiv.org/abs/2510.01641)
*Xiaoyang Liu,Zhengyan Zhou,Zihang Xu,Jiezhang Cao,Zheng Chen,Yulun Zhang*

Main category: cs.CV

TL;DR: FideDiff是一种新颖的单步扩散模型，通过将运动去模糊重新表述为扩散过程，训练一致性模型实现高质量单步去模糊，在保持高保真度的同时显著减少了推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的图像去模糊方法存在推理时间过长和保真度不足的问题，限制了其在真实世界工业应用中的潜力。

Method: 将运动去模糊重新表述为扩散过程，每个时间步代表逐渐模糊的图像；训练一致性模型将所有时间步对齐到同一清晰图像；集成Kernel ControlNet进行模糊核估计；引入自适应时间步预测。

Result: 在全参考指标上取得优越性能，超越了之前的扩散方法，与其他最先进模型性能相当。

Conclusion: FideDiff为预训练扩散模型在高保真图像恢复任务中的应用提供了新方向，为在真实世界工业应用中进一步推进扩散模型建立了坚实基础。

Abstract: Recent advancements in image motion deblurring, driven by CNNs and transformers, have made significant progress. Large-scale pre-trained diffusion models, which are rich in true-world modeling, have shown great promise for high-quality image restoration tasks such as deblurring, demonstrating stronger generative capabilities than CNN and transformer-based methods. However, challenges such as unbearable inference time and compromised fidelity still limit the full potential of the diffusion models. To address this, we introduce FideDiff, a novel single-step diffusion model designed for high-fidelity deblurring. We reformulate motion deblurring as a diffusion-like process where each timestep represents a progressively blurred image, and we train a consistency model that aligns all timesteps to the same clean image. By reconstructing training data with matched blur trajectories, the model learns temporal consistency, enabling accurate one-step deblurring. We further enhance model performance by integrating Kernel ControlNet for blur kernel estimation and introducing adaptive timestep prediction. Our model achieves superior performance on full-reference metrics, surpassing previous diffusion-based methods and matching the performance of other state-of-the-art models. FideDiff offers a new direction for applying pre-trained diffusion models to high-fidelity image restoration tasks, establishing a robust baseline for further advancing diffusion models in real-world industrial applications. Our dataset and code will be available at https://github.com/xyLiu339/FideDiff.

</details>


### [12] [Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery](https://arxiv.org/abs/2510.01662)
*Minh Tran,Maksim Siniukov,Zhangyu Jin,Mohammad Soleymani*

Main category: cs.CV

TL;DR: 提出了一种名为离散面部编码（DFE）的无监督方法，通过残差向量量化变分自编码器从3D网格序列中学习紧凑且可解释的面部表情字典，在多个心理任务上优于传统FACS系统。


<details>
  <summary>Details</summary>
Motivation: 现有的面部表情编码系统（如FACS）存在覆盖范围有限和人工标注成本高的问题，需要开发更有效的数据驱动替代方案。

Method: 使用3D可变形模型提取身份不变的表情特征，然后通过残差向量量化变分自编码器将这些特征编码为来自共享码本的离散标记序列，每个标记捕获特定的可重用面部变形模式。

Result: DFE比FACS和其他面部编码方法捕获更精确的面部行为，在压力检测、人格预测和抑郁检测三个心理任务上，基于学习标记的简单词袋模型持续优于FACS基线和强图像视频表示学习模型。

Conclusion: DFE作为FACS的可扩展有效替代方案，在心理和情感计算应用中具有潜力，能够覆盖更广泛的面部表情显示。

Abstract: Facial expression analysis is central to understanding human behavior, yet existing coding systems such as the Facial Action Coding System (FACS) are constrained by limited coverage and costly manual annotation. In this work, we introduce Discrete Facial Encoding (DFE), an unsupervised, data-driven alternative of compact and interpretable dictionary of facial expressions from 3D mesh sequences learned through a Residual Vector Quantized Variational Autoencoder (RVQ-VAE). Our approach first extracts identity-invariant expression features from images using a 3D Morphable Model (3DMM), effectively disentangling factors such as head pose and facial geometry. We then encode these features using an RVQ-VAE, producing a sequence of discrete tokens from a shared codebook, where each token captures a specific, reusable facial deformation pattern that contributes to the overall expression. Through extensive experiments, we demonstrate that Discrete Facial Encoding captures more precise facial behaviors than FACS and other facial encoding alternatives. We evaluate the utility of our representation across three high-level psychological tasks: stress detection, personality prediction, and depression detection. Using a simple Bag-of-Words model built on top of the learned tokens, our system consistently outperforms both FACS-based pipelines and strong image and video representation learning models such as Masked Autoencoders. Further analysis reveals that our representation covers a wider variety of facial displays, highlighting its potential as a scalable and effective alternative to FACS for psychological and affective computing applications.

</details>


### [13] [UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction](https://arxiv.org/abs/2510.01669)
*Jin Cao,Hongrui Wu,Ziyong Feng,Hujun Bao,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: UniVerse是一个统一的鲁棒重建框架，通过将不一致的多视角图像转换为初始视频，使用视频扩散模型恢复为一致图像，然后重建3D场景。


<details>
  <summary>Details</summary>
Motivation: 解决从不一致多视角图像进行3D重建的挑战，现有方法依赖密集观测且优化复杂，需要更通用的解决方案。

Method: 将鲁棒重建解耦为恢复和重建两个子任务，使用视频扩散模型学习通用场景先验来恢复不一致图像。

Result: 在合成和真实数据集上表现出强大的泛化能力和优越性能，还能控制重建3D场景的风格。

Conclusion: UniVerse通过解耦策略和扩散模型的通用先验学习，有效解决了鲁棒3D重建问题，具有良好泛化性和可控性。

Abstract: This paper tackles the challenge of robust reconstruction, i.e., the task of reconstructing a 3D scene from a set of inconsistent multi-view images. Some recent works have attempted to simultaneously remove image inconsistencies and perform reconstruction by integrating image degradation modeling into neural 3D scene representations.However, these methods rely heavily on dense observations for robustly optimizing model parameters.To address this issue, we propose to decouple robust reconstruction into two subtasks: restoration and reconstruction, which naturally simplifies the optimization process.To this end, we introduce UniVerse, a unified framework for robust reconstruction based on a video diffusion model. Specifically, UniVerse first converts inconsistent images into initial videos, then uses a specially designed video diffusion model to restore them into consistent images, and finally reconstructs the 3D scenes from these restored images.Compared with case-by-case per-view degradation modeling, the diffusion model learns a general scene prior from large-scale data, making it applicable to diverse image inconsistencies.Extensive experiments on both synthetic and real-world datasets demonstrate the strong generalization capability and superior performance of our method in robust reconstruction. Moreover, UniVerse can control the style of the reconstructed 3D scene. Project page: https://jin-cao-tma.github.io/UniVerse.github.io/

</details>


### [14] [FreeViS: Training-free Video Stylization with Inconsistent References](https://arxiv.org/abs/2510.01686)
*Jiacong Xu,Yiqun Mei,Ke Zhang,Vishal M. Patel*

Main category: cs.CV

TL;DR: FreeViS是一个无需训练的视频风格化框架，通过整合多个风格化参考到预训练的I2V模型，生成具有丰富风格细节和强时间一致性的风格化视频。


<details>
  <summary>Details</summary>
Motivation: 现有视频风格化方法存在时间一致性差、风格丰富度不足的问题，而训练专用模型需要配对视频数据且计算成本高。

Method: 整合多个风格化参考到预训练I2V模型，使用高频补偿约束内容布局和运动，结合基于光流的运动线索保留低显著性区域的风格纹理。

Result: FreeViS在风格化保真度和时间一致性方面表现优异，优于现有基线方法，获得强人类偏好。

Conclusion: 该无需训练的方法为高质量、时间一致性的视频风格化提供了实用且经济的解决方案。

Abstract: Video stylization plays a key role in content creation, but it remains a challenging problem. Na\"ively applying image stylization frame-by-frame hurts temporal consistency and reduces style richness. Alternatively, training a dedicated video stylization model typically requires paired video data and is computationally expensive. In this paper, we propose FreeViS, a training-free video stylization framework that generates stylized videos with rich style details and strong temporal coherence. Our method integrates multiple stylized references to a pretrained image-to-video (I2V) model, effectively mitigating the propagation errors observed in prior works, without introducing flickers and stutters. In addition, it leverages high-frequency compensation to constrain the content layout and motion, together with flow-based motion cues to preserve style textures in low-saliency regions. Through extensive evaluations, FreeViS delivers higher stylization fidelity and superior temporal consistency, outperforming recent baselines and achieving strong human preference. Our training-free pipeline offers a practical and economic solution for high-quality, temporally coherent video stylization. The code and videos can be accessed via https://xujiacong.github.io/FreeViS/

</details>


### [15] [PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal Positional Encoding and Reinforcement Learning](https://arxiv.org/abs/2510.01715)
*Raahul Krishna Durairaju,K. Saruladha*

Main category: cs.CV

TL;DR: PyramidStyler是一个基于Transformer的神经风格迁移框架，采用金字塔位置编码和强化学习优化，在保持实时推理速度的同时显著降低了内容和风格损失。


<details>
  <summary>Details</summary>
Motivation: 现有的CNN和Transformer模型在处理复杂风格和高分辨率输入时存在效率问题，需要更高效的风格迁移方法。

Method: 提出PyramidStyler框架，包含金字塔位置编码（PPE）来捕获多尺度特征，并引入强化学习动态优化风格化过程。

Result: 在COCO和WikiArt数据集上训练4000轮后，内容损失降低62.6%至2.07，风格损失降低57.4%至0.86，推理时间1.39秒。使用RL后进一步改善至内容损失2.03，风格损失0.75，速度仅轻微下降至1.40秒。

Conclusion: PyramidStyler实现了实时高质量艺术渲染，在媒体和设计领域具有广泛应用前景。

Abstract: Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based algorithm, enabling AI-driven artistic image synthesis. However, existing CNN and transformer-based models struggle to scale efficiently to complex styles and high-resolution inputs. We introduce PyramidStyler, a transformer framework with Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding that captures both local details and global context while reducing computational load. We further incorporate reinforcement learning to dynamically optimize stylization, accelerating convergence. Trained on Microsoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to 2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s inference--and yields further improvements (content 2.03; style 0.75) with minimal speed penalty (1.40 s) when using RL. These results demonstrate real-time, high-quality artistic rendering, with broad applications in media and design.

</details>


### [16] [LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for Large-Scale Scene Reconstruction](https://arxiv.org/abs/2510.01767)
*Sheng-Hsiang Hung,Ting-Yu Yen,Wei-Fang Sun,Simon See,Shih-Hsuan Hung,Hung-Kuo Chu*

Main category: cs.CV

TL;DR: LoBE-GS是一个负载均衡的高效3D高斯泼溅框架，通过深度感知分区和优化策略解决大规模场景重建中的负载不平衡问题，实现2倍训练加速。


<details>
  <summary>Details</summary>
Motivation: 现有分治法在扩展3D高斯泼溅到大规模无界场景时存在负载不平衡和粗到细流水线效率低下的问题。

Method: 引入深度感知分区方法减少预处理时间，采用基于优化的策略平衡可见高斯分布，结合可见性裁剪和选择性致密化技术降低训练成本。

Result: 在大规模城市场景数据集上评估显示，LoBE-GS相比现有最优方法实现2倍端到端训练加速，同时保持重建质量。

Conclusion: LoBE-GS成功解决了大规模3D高斯泼溅的扩展性问题，实现了高效且可扩展的大场景重建。

Abstract: 3D Gaussian Splatting (3DGS) has established itself as an efficient representation for real-time, high-fidelity 3D scene reconstruction. However, scaling 3DGS to large and unbounded scenes such as city blocks remains difficult. Existing divide-and-conquer methods alleviate memory pressure by partitioning the scene into blocks, but introduce new bottlenecks: (i) partitions suffer from severe load imbalance since uniform or heuristic splits do not reflect actual computational demands, and (ii) coarse-to-fine pipelines fail to exploit the coarse stage efficiently, often reloading the entire model and incurring high overhead. In this work, we introduce LoBE-GS, a novel Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning method that reduces preprocessing from hours to minutes, an optimization-based strategy that balances visible Gaussians -- a strong proxy for computational load -- across blocks, and two lightweight techniques, visibility cropping and selective densification, to further reduce training cost. Evaluations on large-scale urban and outdoor datasets show that LoBE-GS consistently achieves up to $2\times$ faster end-to-end training time than state-of-the-art baselines, while maintaining reconstruction quality and enabling scalability to scenes infeasible with vanilla 3DGS.

</details>


### [17] [Pack and Force Your Memory: Long-form and Consistent Video Generation](https://arxiv.org/abs/2510.01784)
*Xiaofei Wu,Guozhen Zhang,Zhiyong Xu,Yuan Zhou,Qinglin Lu,Xuming He*

Main category: cs.CV

TL;DR: 提出MemoryPack和Direct Forcing两个创新方法，解决长视频生成中的长期依赖建模和错误累积问题，提升分钟级时间一致性和推理可靠性。


<details>
  <summary>Details</summary>
Motivation: 长视频生成面临双重挑战：需要捕获长期依赖关系，同时防止自回归解码中的错误累积。

Method: 1. MemoryPack：可学习的上下文检索机制，利用文本和图像信息作为全局指导，联合建模短期和长期依赖；2. Direct Forcing：高效的单步近似策略，改善训练-推理对齐，减少推理中的错误传播。

Result: 实现了分钟级时间一致性，计算效率高且保持线性复杂度，显著提升了长视频生成的上下文一致性和可靠性。

Conclusion: MemoryPack和Direct Forcing共同增强了自回归视频模型的上下文一致性和实用性，推动了长视频生成的实际应用。

Abstract: Long-form video generation presents a dual challenge: models must capture long-range dependencies while preventing the error accumulation inherent in autoregressive decoding. To address these challenges, we make two contributions. First, for dynamic context modeling, we propose MemoryPack, a learnable context-retrieval mechanism that leverages both textual and image information as global guidance to jointly model short- and long-term dependencies, achieving minute-level temporal consistency. This design scales gracefully with video length, preserves computational efficiency, and maintains linear complexity. Second, to mitigate error accumulation, we introduce Direct Forcing, an efficient single-step approximating strategy that improves training-inference alignment and thereby curtails error propagation during inference. Together, MemoryPack and Direct Forcing substantially enhance the context consistency and reliability of long-form video generation, advancing the practical usability of autoregressive video models.

</details>


### [18] [Leveraging Prior Knowledge of Diffusion Model for Person Search](https://arxiv.org/abs/2510.01841)
*Giyeol Kim,Sooyoung Yang,Jihyong Oh,Myungjoo Kang,Chanho Eom*

Main category: cs.CV

TL;DR: DiffPS是一个新的人员搜索框架，利用预训练扩散模型解决检测和重识别任务间的优化冲突，通过三个专门模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用ImageNet预训练骨干网络可能无法充分捕捉复杂空间上下文和细粒度身份线索，且共享骨干网络导致检测和重识别任务优化目标冲突。

Method: 提出DiffPS框架，包含三个模块：扩散引导区域提议网络(DGRPN)用于改进人员定位，多尺度频率细化网络(MSFRN)减轻形状偏差，语义自适应特征聚合网络(SFAN)利用文本对齐的扩散特征。

Result: 在CUHK-SYSU和PRW数据集上达到了新的最先进性能。

Conclusion: 扩散先验知识能有效解决人员搜索中检测和重识别任务的优化冲突，提升整体性能。

Abstract: Person search aims to jointly perform person detection and re-identification by localizing and identifying a query person within a gallery of uncropped scene images. Existing methods predominantly utilize ImageNet pre-trained backbones, which may be suboptimal for capturing the complex spatial context and fine-grained identity cues necessary for person search. Moreover, they rely on a shared backbone feature for both person detection and re-identification, leading to suboptimal features due to conflicting optimization objectives. In this paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a novel framework that leverages a pre-trained diffusion model while eliminating the optimization conflict between two sub-tasks. We analyze key properties of diffusion priors and propose three specialized modules: (i) Diffusion-Guided Region Proposal Network (DGRPN) for enhanced person localization, (ii) Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and (iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage text-aligned diffusion features. DiffPS sets a new state-of-the-art on CUHK-SYSU and PRW.

</details>


### [19] [4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing](https://arxiv.org/abs/2510.01991)
*Lei Liu,Can Wang,Zhenghao Chen,Dong Xu*

Main category: cs.CV

TL;DR: 4DGS-Craft是一个一致且交互式的4D高斯溅射编辑框架，通过4D感知的InstructPix2Pix模型、多视图网格模块、高斯选择机制和基于LLM的用户意图理解模块，解决了视图、时间和非编辑区域一致性问题，并能处理复杂文本指令。


<details>
  <summary>Details</summary>
Motivation: 现有的4D高斯溅射编辑方法在视图、时间和非编辑区域一致性方面存在问题，且难以处理复杂的文本指令，因此需要开发一个更一致和可控的4D场景编辑框架。

Method: 1. 引入4D感知的InstructPix2Pix模型，结合4D VGGT几何特征确保视图和时间一致性；2. 使用多视图网格模块迭代优化多视图输入图像并联合优化底层4D场景；3. 提出高斯选择机制仅优化编辑区域的高斯；4. 设计基于LLM的用户意图理解模块，将复杂指令分解为原子操作序列。

Result: 与相关工作相比，该方法实现了更一致和可控的4D场景编辑，能够处理复杂的用户指令并保持编辑质量。

Conclusion: 4DGS-Craft通过综合的4D一致性保证机制和用户交互设计，有效解决了4D高斯溅射编辑中的一致性和复杂性挑战，为4D场景编辑提供了更强大的工具。

Abstract: Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges with view, temporal, and non-editing region consistency, as well as with handling complex text instructions. To address these issues, we propose 4DGS-Craft, a consistent and interactive 4DGS editing framework. We first introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal consistency. This model incorporates 4D VGGT geometry features extracted from the initial scene, enabling it to capture underlying 4D geometric structures during editing. We further enhance this model with a multi-view grid module that enforces consistency by iteratively refining multi-view input images while jointly optimizing the underlying 4D scene. Furthermore, we preserve the consistency of non-edited regions through a novel Gaussian selection mechanism, which identifies and optimizes only the Gaussians within the edited regions. Beyond consistency, facilitating user interaction is also crucial for effective 4DGS editing. Therefore, we design an LLM-based module for user intent understanding. This module employs a user instruction template to define atomic editing operations and leverages an LLM for reasoning. As a result, our framework can interpret user intent and decompose complex instructions into a logical sequence of atomic operations, enabling it to handle intricate user commands and further enhance editing performance. Compared to related works, our approach enables more consistent and controllable 4D scene editing. Our code will be made available upon acceptance.

</details>


### [20] [Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing Routing in Lightweight Image Super-Resolution](https://arxiv.org/abs/2510.01997)
*Junyu Wu,Jie Tang,Jie Liu,Gangshan Wu*

Main category: cs.CV

TL;DR: 提出了Pure-Pass（PP）像素级掩码机制，通过固定颜色中心点识别纯像素并免除其昂贵计算，集成到ATD-light模型中实现更优的超分辨率性能。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级超分辨率方法如CAMixer存在适应性差、掩码粒度粗和空间灵活性不足等限制，需要更精细的像素级处理机制。

Method: 使用Pure-Pass像素级掩码机制，基于固定颜色中心点对像素进行分类，实现细粒度、空间灵活的掩码处理，并集成到ATD-light模型中。

Result: PP-ATD-light在相似计算量下，重建质量和参数效率均优于CAMixer-ATD-light，实现了更优的超分辨率性能。

Conclusion: Pure-Pass机制通过像素级掩码有效提升了轻量级超分辨率模型的性能，在保持自适应灵活性的同时显著减少了计算开销。

Abstract: Image Super-Resolution (SR) aims to reconstruct high-resolution images from low-resolution counterparts, but the computational complexity of deep learning-based methods often hinders practical deployment. CAMixer is the pioneering work to integrate the advantages of existing lightweight SR methods and proposes a content-aware mixer to route token mixers of varied complexities according to the difficulty of content recovery. However, several limitations remain, such as poor adaptability, coarse-grained masking and spatial inflexibility, among others. We propose Pure-Pass (PP), a pixel-level masking mechanism that identifies pure pixels and exempts them from expensive computations. PP utilizes fixed color center points to classify pixels into distinct categories, enabling fine-grained, spatially flexible masking while maintaining adaptive flexibility. Integrated into the state-of-the-art ATD-light model, PP-ATD-light achieves superior SR performance with minimal overhead, outperforming CAMixer-ATD-light in reconstruction quality and parameter efficiency when saving a similar amount of computation.

</details>


### [21] [GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing](https://arxiv.org/abs/2510.02034)
*Mengtian Li,Yunshu Bai,Yimin Chu,Yijun Shen,Zhongmei Li,Weifeng Ge,Zhifeng Xie,Chaofeng Chen*

Main category: cs.CV

TL;DR: GaussianMorphing是一个从多视角图像进行语义感知3D形状和纹理变形的新框架，通过网格引导的3D高斯泼溅实现高保真几何和外观建模，无需标记数据。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖点云或需要预定义同胚映射来处理无纹理数据，存在局限性。需要开发能够同时处理几何和纹理变形的新方法。

Method: 采用统一变形策略，将3D高斯锚定到重建的网格面片上，通过拓扑感知约束确保几何一致性和纹理保真度，并利用网格拓扑作为几何先验建立无监督语义对应。

Result: 在TexMorph基准测试中，GaussianMorphing显著优于现有2D/3D方法，颜色一致性误差(ΔE)降低22.2%，EI降低26.2%。

Conclusion: 该框架在保持局部细节和全局语义一致性的同时，实现了高质量的3D形状和纹理变形，无需标记数据。

Abstract: We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape and texture morphing from multi-view images. Previous approaches usually rely on point clouds or require pre-defined homeomorphic mappings for untextured data. Our method overcomes these limitations by leveraging mesh-guided 3D Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling. The core of our framework is a unified deformation strategy that anchors 3DGaussians to reconstructed mesh patches, ensuring geometrically consistent transformations while preserving texture fidelity through topology-aware constraints. In parallel, our framework establishes unsupervised semantic correspondence by using the mesh topology as a geometric prior and maintains structural integrity via physically plausible point trajectories. This integrated approach preserves both local detail and global semantic coherence throughout the morphing process with out requiring labeled data. On our proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior 2D/3D methods, reducing color consistency error ($\Delta E$) by 22.2% and EI by 26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/

</details>


### [22] [Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers](https://arxiv.org/abs/2510.02043)
*Sahil Bhandary Karnoor,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: 提出InPose方法，使用预训练扩散模型仅基于旋转测量进行姿态估计，通过似然项引导生成符合稀疏身体测量的姿态序列，实现零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有基于条件扩散模型的方法泛化能力差，主要因为位置测量高度受用户体型影响，需要解决跨用户姿态估计的泛化问题。

Method: 将姿态估计建模为逆问题，使用预训练扩散模型仅以旋转测量为条件，通过从测量位置导出的似然项引导模型先验。

Result: InPose方法能够为零样本泛化生成高度可能的姿态序列，准确解释稀疏的身体测量数据。

Conclusion: 提出的InPose方法通过分离旋转条件和位置似然引导，有效解决了跨用户姿态估计的泛化问题。

Abstract: Pose estimation refers to tracking a human's full body posture, including their head, torso, arms, and legs. The problem is challenging in practical settings where the number of body sensors are limited. Past work has shown promising results using conditional diffusion models, where the pose prediction is conditioned on both <location, rotation> measurements from the sensors. Unfortunately, nearly all these approaches generalize poorly across users, primarly because location measurements are highly influenced by the body size of the user. In this paper, we formulate pose estimation as an inverse problem and design an algorithm capable of zero-shot generalization. Our idea utilizes a pre-trained diffusion model and conditions it on rotational measurements alone; the priors from this model are then guided by a likelihood term, derived from the measured locations. Thus, given any user, our proposed InPose method generatively estimates the highly likely sequence of poses that best explains the sparse on-body measurements.

</details>


### [23] [VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation](https://arxiv.org/abs/2510.02086)
*Arman Behnam*

Main category: cs.CV

TL;DR: 提出VGDM框架，结合视觉Transformer和扩散模型进行脑肿瘤检测与分割，通过全局上下文推理和迭代去噪提升体积精度和边界精度。


<details>
  <summary>Details</summary>
Motivation: 传统卷积架构如U-Net在捕捉长距离依赖方面能力有限，限制了在复杂肿瘤结构上的性能。扩散模型在生成高保真医学图像和细化分割边界方面显示出强大潜力。

Method: 在扩散过程核心嵌入视觉Transformer，利用全局上下文推理和迭代去噪。Transformer主干能够更有效地建模整个MRI体积的空间关系，扩散细化减少体素级误差并恢复细粒度肿瘤细节。

Result: 在MRI脑肿瘤数据集上的实验验证显示，在Dice相似性和Hausdorff距离指标上持续提升。

Conclusion: 这种混合设计为神经肿瘤学提供了改进鲁棒性和可扩展性的途径，超越传统U-Net基线，展示了Transformer引导的扩散模型在推进肿瘤分割技术前沿的潜力。

Abstract: Accurate detection and segmentation of brain tumors from magnetic resonance imaging (MRI) are essential for diagnosis, treatment planning, and clinical monitoring. While convolutional architectures such as U-Net have long been the backbone of medical image segmentation, their limited capacity to capture long-range dependencies constrains performance on complex tumor structures. Recent advances in diffusion models have demonstrated strong potential for generating high-fidelity medical images and refining segmentation boundaries.   In this work, we propose VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation framework, a transformer-driven diffusion framework for brain tumor detection and segmentation. By embedding a vision transformer at the core of the diffusion process, the model leverages global contextual reasoning together with iterative denoising to enhance both volumetric accuracy and boundary precision. The transformer backbone enables more effective modeling of spatial relationships across entire MRI volumes, while diffusion refinement mitigates voxel-level errors and recovers fine-grained tumor details.   This hybrid design provides a pathway toward improved robustness and scalability in neuro-oncology, moving beyond conventional U-Net baselines. Experimental validation on MRI brain tumor datasets demonstrates consistent gains in Dice similarity and Hausdorff distance, underscoring the potential of transformer-guided diffusion models to advance the state of the art in tumor segmentation.

</details>


### [24] [TempoControl: Temporal Attention Guidance for Text-to-Video Models](https://arxiv.org/abs/2510.02226)
*Shira Schiber,Ofir Lindenbaum,Idan Schwartz*

Main category: cs.CV

TL;DR: TempoControl是一种无需重新训练或额外监督的方法，通过优化交叉注意力图来实现视频生成中的时间对齐控制。


<details>
  <summary>Details</summary>
Motivation: 现有的生成视频模型缺乏细粒度的时间控制，无法让用户指定特定视觉元素在生成序列中出现的时间。

Method: 利用文本到视频扩散模型中的交叉注意力图，通过相关性、能量和熵三个互补原则来引导概念的时间安排。

Result: 该方法能够精确控制时间安排，同时确保高质量和多样性的视频生成，适用于时间重排序、动作和音频对齐等多种应用。

Conclusion: TempoControl提供了一种有效的时间对齐控制方法，增强了生成视频模型的时间控制能力。

Abstract: Recent advances in generative video models have enabled the creation of high-quality videos based on natural language prompts. However, these models frequently lack fine-grained temporal control, meaning they do not allow users to specify when particular visual elements should appear within a generated sequence. In this work, we introduce TempoControl, a method that allows for temporal alignment of visual concepts during inference, without requiring retraining or additional supervision. TempoControl utilizes cross-attention maps, a key component of text-to-video diffusion models, to guide the timing of concepts through a novel optimization approach. Our method steers attention using three complementary principles: aligning its temporal shape with a control signal (via correlation), amplifying it where visibility is needed (via energy), and maintaining spatial focus (via entropy). TempoControl allows precise control over timing while ensuring high video quality and diversity. We demonstrate its effectiveness across various video generation applications, including temporal reordering for single and multiple objects, as well as action and audio-aligned generation.

</details>


### [25] [DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing](https://arxiv.org/abs/2510.02253)
*Zihan Zhou,Shilin Lu,Shuli Leng,Shaocong Zhang,Zhuming Lian,Xinlei Yu,Adams Wai-Kin Kong*

Main category: cs.CV

TL;DR: DragFlow是首个有效利用FLUX强大先验进行拖拽编辑的框架，通过区域化编辑范式、个性化适配器和多模态大语言模型，在拖拽编辑任务中取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型从UNet转向DiT，生成先验变得更强大，但拖拽编辑尚未从中受益。现有方法在目标区域存在扭曲问题，因为早期模型的先验不足以将优化后的潜在空间投影回自然图像流形。

Method: 提出区域化编辑范式，使用仿射变换提供更丰富一致的特征监督；集成预训练个性化适配器增强主体一致性；使用梯度掩码硬约束保护背景保真度；利用多模态大语言模型解决任务歧义。

Result: 在DragBench-DR和ReD Bench上的广泛实验表明，DragFlow超越了基于点和基于区域的基线方法，在拖拽图像编辑中创造了新的最先进水平。

Conclusion: DragFlow成功利用FLUX的丰富先验，通过区域化编辑解决了传统点式拖拽的局限性，为拖拽编辑任务提供了更强大可靠的解决方案。

Abstract: Drag-based image editing has long suffered from distortions in the target region, largely because the priors of earlier base models, Stable Diffusion, are insufficient to project optimized latents back onto the natural image manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow matching (e.g., SD3.5, FLUX), generative priors have become significantly stronger, enabling advances across diverse editing tasks. However, drag-based editing has yet to benefit from these stronger priors. This work proposes the first framework to effectively harness FLUX's rich prior for drag-based editing, dubbed DragFlow, achieving substantial gains over baselines. We first show that directly applying point-based drag editing to DiTs performs poorly: unlike the highly compressed features of UNets, DiT features are insufficiently structured to provide reliable guidance for point-wise motion supervision. To overcome this limitation, DragFlow introduces a region-based editing paradigm, where affine transformations enable richer and more consistent feature supervision. Additionally, we integrate pretrained open-domain personalization adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving background fidelity through gradient mask-based hard constraints. Multimodal large language models (MLLMs) are further employed to resolve task ambiguities. For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench) featuring region-level dragging instructions. Extensive experiments on DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and region-based baselines, setting a new state-of-the-art in drag-based image editing. Code and datasets will be publicly available upon publication.

</details>


### [26] [NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes](https://arxiv.org/abs/2510.02266)
*Shiyi Zhang,Dong Liang,Yihang Zhou*

Main category: cs.CV

TL;DR: NeuroSwift通过集成AutoKL和CLIP适配器的扩散模型，实现了跨被试的视觉刺激重建，仅需1小时训练即可达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决fMRI数据解码中跨被试重建的挑战，包括被试间神经表征差异和大脑对复杂视觉输入的抽象语义编码问题。

Method: 结合AutoKL适配器处理低级特征和CLIP适配器处理语义特征，通过预训练和微调策略，仅更新17%参数实现跨被试泛化。

Result: 在轻量级GPU（三块RTX 4090）上仅需1小时训练即可超越现有方法，实现最先进的跨被试视觉重建性能。

Conclusion: NeuroSwift提供了一种高效且有效的跨被试视觉重建解决方案，显著降低了计算需求并提升了重建准确性。

Abstract: Reconstructing visual information from brain activity via computer vision technology provides an intuitive understanding of visual neural mechanisms. Despite progress in decoding fMRI data with generative models, achieving accurate cross-subject reconstruction of visual stimuli remains challenging and computationally demanding. This difficulty arises from inter-subject variability in neural representations and the brain's abstract encoding of core semantic features in complex visual inputs. To address these challenges, we propose NeuroSwift, which integrates complementary adapters via diffusion: AutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter is trained on Stable Diffusion generated images paired with COCO captions to emulate higher visual cortex encoding. For cross-subject generalization, we pretrain on one subject and then fine-tune only 17 percent of parameters (fully connected layers) for new subjects, while freezing other components. This enables state-of-the-art performance with only one hour of training per subject on lightweight GPUs (three RTX 4090), and it outperforms existing methods.

</details>


### [27] [Self-Forcing++: Towards Minute-Scale High-Quality Video Generation](https://arxiv.org/abs/2510.02283)
*Justin Cui,Jie Wu,Ming Li,Tao Yang,Xiaojie Li,Rui Wang,Andrew Bai,Yuanhao Ban,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 提出了一种简单有效的方法来缓解长视频生成中的质量退化问题，无需长视频教师监督或重新训练长视频数据集，通过利用教师模型的丰富知识为自生成长视频中的采样片段提供指导。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像和视频生成方面取得了革命性进展，但基于transformer架构的计算成本过高，特别是在生成长视频时。现有自回归方法通过从短时双向教师蒸馏，但学生模型在超出训练范围时会出现明显的质量退化。

Method: 利用教师模型的丰富知识为自生成长视频中的采样片段提供指导，保持时间一致性，同时将视频长度扩展到教师能力的20倍，避免过度曝光和误差累积问题。

Result: 方法能够生成长达4分15秒的视频，相当于基础模型位置嵌入支持的最大跨度的99.9%，比基线模型长50倍以上。在标准基准测试和改进基准测试中，该方法在保真度和一致性方面都显著优于基线方法。

Conclusion: 提出的方法有效缓解了长视频生成中的质量退化问题，无需额外监督或重新训练，实现了高质量的长视频生成。

Abstract: Diffusion models have revolutionized image and video generation, achieving unprecedented visual quality. However, their reliance on transformer architectures incurs prohibitively high computational costs, particularly when extending generation to long videos. Recent work has explored autoregressive formulations for long video generation, typically by distilling from short-horizon bidirectional teachers. Nevertheless, given that teacher models cannot synthesize long videos, the extrapolation of student models beyond their training horizon often leads to pronounced quality degradation, arising from the compounding of errors within the continuous latent space. In this paper, we propose a simple yet effective approach to mitigate quality degradation in long-horizon video generation without requiring supervision from long-video teachers or retraining on long video datasets. Our approach centers on exploiting the rich knowledge of teacher models to provide guidance for the student model through sampled segments drawn from self-generated long videos. Our method maintains temporal consistency while scaling video length by up to 20x beyond teacher's capability, avoiding common issues such as over-exposure and error-accumulation without recomputing overlapping frames like previous methods. When scaling up the computation, our method shows the capability of generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the maximum span supported by our base model's position embedding and more than 50x longer than that of our baseline model. Experiments on standard benchmarks and our proposed improved benchmark demonstrate that our approach substantially outperforms baseline methods in both fidelity and consistency. Our long-horizon videos demo can be found at https://self-forcing-plus-plus.github.io/

</details>


### [28] [Learning to Generate Object Interactions with Physics-Guided Video Diffusion](https://arxiv.org/abs/2510.02284)
*David Romero,Ariana Bermudez,Hao Li,Fabio Pizzati,Ivan Laptev*

Main category: cs.CV

TL;DR: KineMask是一种物理引导的视频生成方法，通过两阶段训练策略实现逼真的刚体控制和物体交互，结合低级运动控制和高级文本条件，显著提升物体交互的物理合理性。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在物理交互方面存在不足，缺乏基于物理的控制机制，限制了其在机器人学和决策模拟中的应用潜力。

Method: 提出两阶段训练策略，通过物体掩码逐步移除未来运动监督，在合成场景上训练视频扩散模型，并整合低级运动控制与高级文本条件。

Result: 实验显示KineMask在物体交互方面相比同类模型有显著提升，消融研究验证了低级和高级条件在视频扩散模型中的互补作用。

Conclusion: KineMask通过物理引导的视频生成方法有效解决了物体交互的物理合理性问题，为复杂动态现象合成提供了有效支持。

Abstract: Recent models for video generation have achieved remarkable progress and are now deployed in film, social media production, and advertising. Beyond their creative potential, such models also hold promise as world simulators for robotics and embodied decision making. Despite strong advances, however, current approaches still struggle to generate physically plausible object interactions and lack physics-grounded control mechanisms. To address this limitation, we introduce KineMask, an approach for physics-guided video generation that enables realistic rigid body control, interactions, and effects. Given a single image and a specified object velocity, our method generates videos with inferred motions and future object interactions. We propose a two-stage training strategy that gradually removes future motion supervision via object masks. Using this strategy we train video diffusion models (VDMs) on synthetic scenes of simple interactions and demonstrate significant improvements of object interactions in real scenes. Furthermore, KineMask integrates low-level motion control with high-level textual conditioning via predictive scene descriptions, leading to effective support for synthesis of complex dynamical phenomena. Extensive experiments show that KineMask achieves strong improvements over recent models of comparable size. Ablation studies further highlight the complementary roles of low- and high-level conditioning in VDMs. Our code, model, and data will be made publicly available.

</details>


### [29] [MultiModal Action Conditioned Video Generation](https://arxiv.org/abs/2510.02287)
*Yichen Li,Antonio Torralba*

Main category: cs.CV

TL;DR: 该论文提出引入多模态精细动作来改进视频模型，通过整合本体感觉、运动感觉、力触觉和肌肉激活等多感官信息，实现更精确的实时控制。


<details>
  <summary>Details</summary>
Motivation: 当前视频模型缺乏精细控制能力，无法作为世界模型使用。通用家用机器人需要实时精细运动控制来处理精细任务和紧急情况。

Method: 开发了特征学习范式来对齐多模态信息，同时保留每个模态的独特信息；提出正则化方案来增强动作轨迹特征在表示复杂交互动态中的因果性。

Result: 实验表明，整合多模态感官提高了模拟精度并减少了时间漂移。广泛的消融研究和下游应用证明了该方法的有效性和实用性。

Conclusion: 通过引入多模态精细动作，能够有效模拟精细交互动态，为机器人精细控制提供了可行的解决方案。

Abstract: Current video models fail as world model as they lack fine-graiend control. General-purpose household robots require real-time fine motor control to handle delicate tasks and urgent situations. In this work, we introduce fine-grained multimodal actions to capture such precise control. We consider senses of proprioception, kinesthesia, force haptics, and muscle activation. Such multimodal senses naturally enables fine-grained interactions that are difficult to simulate with text-conditioned generative models. To effectively simulate fine-grained multisensory actions, we develop a feature learning paradigm that aligns these modalities while preserving the unique information each modality provides. We further propose a regularization scheme to enhance causality of the action trajectory features in representing intricate interaction dynamics. Experiments show that incorporating multimodal senses improves simulation accuracy and reduces temporal drift. Extensive ablation studies and downstream applications demonstrate the effectiveness and practicality of our work.

</details>


### [30] [NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation](https://arxiv.org/abs/2510.02307)
*Ruozhen He,Moayed Haji-Ali,Ziyan Yang,Vicente Ordonez*

Main category: cs.CV

TL;DR: NoiseShift是一种无需训练的方法，通过根据分辨率大小重新校准去噪器的噪声水平，显著提升低分辨率图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像扩散模型在不同分辨率下泛化能力不足的问题，特别是高分辨率训练模型在生成低分辨率图像时出现的质量下降。

Method: 提出NoiseShift方法，重新校准去噪器在不同分辨率下的噪声水平，无需改变模型架构或采样计划，与现有模型兼容。

Result: 在多个模型上显著提升低分辨率图像质量：SD3.5提升15.89%，SD3提升8.56%，Flux-Dev提升2.44%（LAION-COCO数据集）；CelebA数据集上分别提升10.36%、5.19%和3.02%。

Conclusion: NoiseShift能有效缓解分辨率相关的伪影，提升低分辨率图像生成质量，为预算有限的用户提供高质量的低分辨率图像生成方案。

Abstract: Text-to-image diffusion models trained on a fixed set of resolutions often fail to generalize, even when asked to generate images at lower resolutions than those seen during training. High-resolution text-to-image generators are currently unable to easily offer an out-of-the-box budget-efficient alternative to their users who might not need high-resolution images. We identify a key technical insight in diffusion models that when addressed can help tackle this limitation: Noise schedulers have unequal perceptual effects across resolutions. The same level of noise removes disproportionately more signal from lower-resolution images than from high-resolution images, leading to a train-test mismatch. We propose NoiseShift, a training-free method that recalibrates the noise level of the denoiser conditioned on resolution size. NoiseShift requires no changes to model architecture or sampling schedule and is compatible with existing models. When applied to Stable Diffusion 3, Stable Diffusion 3.5, and Flux-Dev, quality at low resolutions is significantly improved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and Flux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by 10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results demonstrate the effectiveness of NoiseShift in mitigating resolution-dependent artifacts and enhancing the quality of low-resolution image generation.

</details>


### [31] [StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions](https://arxiv.org/abs/2510.02314)
*Bo-Hsu Ke,You-Zhe Xie,Yu-Lun Liu,Wei-Chen Chiu*

Main category: cs.CV

TL;DR: 本文分析了3D高斯溅射(3DGS)对图像级投毒攻击的脆弱性，提出了一种基于密度引导的投毒方法，通过在低密度区域注入高斯点来嵌入视角依赖的虚幻物体。


<details>
  <summary>Details</summary>
Motivation: 随着NeRF和3DGS等3D场景表示方法的普及，解决其安全漏洞变得至关重要。本文旨在分析3DGS对投毒攻击的鲁棒性。

Method: 提出密度引导投毒方法：1) 使用核密度估计识别低密度区域；2) 战略性地注入高斯点嵌入视角依赖的虚幻物体；3) 引入自适应噪声策略破坏多视角一致性。

Result: 大量实验表明该方法相比现有技术具有优越性能，提出的KDE评估协议能够系统评估攻击难度。

Conclusion: 该研究揭示了3DGS的安全漏洞，提出的攻击方法和评估协议为未来研究提供了客观基准。

Abstract: 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As these methods become prevalent, addressing their vulnerabilities becomes critical. We analyze 3DGS robustness against image-level poisoning attacks and propose a novel density-guided poisoning method. Our method strategically injects Gaussian points into low-density regions identified via Kernel Density Estimation (KDE), embedding viewpoint-dependent illusory objects clearly visible from poisoned views while minimally affecting innocent views. Additionally, we introduce an adaptive noise strategy to disrupt multi-view consistency, further enhancing attack effectiveness. We propose a KDE-based evaluation protocol to assess attack difficulty systematically, enabling objective benchmarking for future research. Extensive experiments demonstrate our method's superior performance compared to state-of-the-art techniques. Project page: https://hentci.github.io/stealthattack/

</details>


### [32] [Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity](https://arxiv.org/abs/2510.02315)
*Eric Tillmann Bill,Enis Simsar,Thomas Hofmann*

Main category: cs.CV

TL;DR: 提出了首个理论框架和两种架构无关算法来解决文本到图像模型在多主体生成中的属性泄漏、身份纠缠和主体遗漏问题，通过随机最优控制视角优化采样动态。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像模型在单主体提示上表现优秀，但在多主体描述中经常出现属性泄漏、身份纠缠和主体遗漏等问题，需要系统性的解决方案。

Method: 1) 基于随机最优控制理论，将流匹配视为控制问题；2) 提出无需训练的单次更新测试时控制器；3) 开发轻量级微调方法Adjoint Matching，通过反向伴随信号训练控制网络；4) 统一现有注意力启发式方法，扩展到扩散模型。

Result: 在Stable Diffusion 3.5、FLUX和Stable Diffusion XL上，两种算法均能持续提升多主体对齐效果，同时保持基础模型风格。测试时控制可在普通GPU上高效运行，微调控制器在有限提示上训练后能泛化到未见提示。

Conclusion: FOCUS方法在多主体保真度方面达到最先进水平，为多主体文本到图像生成提供了首个专门设计的微调路径和理论框架。

Abstract: Text-to-image (T2I) models excel on single-entity prompts but struggle with multi-subject descriptions, often showing attribute leakage, identity entanglement, and subject omissions. We introduce the first theoretical framework with a principled, optimizable objective for steering sampling dynamics toward multi-subject fidelity. Viewing flow matching (FM) through stochastic optimal control (SOC), we formulate subject disentanglement as control over a trained FM sampler. This yields two architecture-agnostic algorithms: (i) a training-free test-time controller that perturbs the base velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight fine-tuning rule that regresses a control network to a backward adjoint signal while preserving base-model capabilities. The same formulation unifies prior attention heuristics, extends to diffusion models via a flow-diffusion correspondence, and provides the first fine-tuning route explicitly designed for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and Stable Diffusion XL, both algorithms consistently improve multi-subject alignment while maintaining base-model style. Test-time control runs efficiently on commodity GPUs, and fine-tuned controllers trained on limited prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal Control for Unentangled Subjects), which achieves state-of-the-art multi-subject fidelity across models.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [33] [An Efficient Quality Metric for Video Frame Interpolation Based on Motion-Field Divergence](https://arxiv.org/abs/2510.01361)
*Conall Daly,Darren Ramsook,Anil Kokaram*

Main category: eess.IV

TL;DR: 提出了一种新的视频帧插值质量评估指标PSNR_DIV，通过运动发散加权增强PSNR，在BVI-VFI数据集上比FloLPIPS提升0.09皮尔逊相关系数，速度提升2.5倍，内存使用减少4倍。


<details>
  <summary>Details</summary>
Motivation: 现有质量评估指标如PSNR、SSIM、LPIPS忽略了时间一致性，而专门针对视频帧插值的指标如FloLPIPS存在计算效率低的问题，限制了实际应用。

Method: 提出PSNR_DIV指标，通过运动发散加权来增强PSNR，该方法改编自档案胶片修复技术，用于检测时间不一致性，通过突出运动场中的奇点来加权图像误差。

Result: 在BVI-VFI数据集（180个序列，涵盖多种帧率、分辨率和插值方法）上的评估显示，PSNR_DIV相比FloLPIPS皮尔逊线性相关系数提升0.09，速度提升2.5倍，内存使用减少4倍，在所有内容类别上性能一致且对运动估计器鲁棒。

Conclusion: PSNR_DIV的高效性和准确性使其能够快速进行质量评估，并可作为训练视频帧插值神经网络的损失函数，具有实际应用价值。

Abstract: Video frame interpolation is a fundamental tool for temporal video enhancement, but existing quality metrics struggle to evaluate the perceptual impact of interpolation artefacts effectively. Metrics like PSNR, SSIM and LPIPS ignore temporal coherence. State-of-the-art quality metrics tailored towards video frame interpolation, like FloLPIPS, have been developed but suffer from computational inefficiency that limits their practical application. We present $\text{PSNR}_{\text{DIV}}$, a novel full-reference quality metric that enhances PSNR through motion divergence weighting, a technique adapted from archival film restoration where it was developed to detect temporal inconsistencies. Our approach highlights singularities in motion fields which is then used to weight image errors. Evaluation on the BVI-VFI dataset (180 sequences across multiple frame rates, resolutions and interpolation methods) shows $\text{PSNR}_{\text{DIV}}$ achieves statistically significant improvements: +0.09 Pearson Linear Correlation Coefficient over FloLPIPS, while being 2.5$\times$ faster and using 4$\times$ less memory. Performance remains consistent across all content categories and are robust to the motion estimator used. The efficiency and accuracy of $\text{PSNR}_{\text{DIV}}$ enables fast quality evaluation and practical use as a loss function for training neural networks for video frame interpolation tasks. An implementation of our metric is available at www.github.com/conalld/psnr-div.

</details>


### [34] [Median2Median: Zero-shot Suppression of Structured Noise in Images](https://arxiv.org/abs/2510.01666)
*Jianxu Wang,Ge Wang*

Main category: eess.IV

TL;DR: 提出了Median2Median（M2M）零样本去噪框架，专门针对结构化噪声设计，通过新颖的采样策略生成伪独立子图像对，在保持i.i.d.噪声下性能的同时，在相关噪声下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界图像常被具有强各向异性相关的结构化噪声降质，现有方法难以去除。数据驱动方法依赖大量高质量标签且泛化性有限，而零样本方法仅对独立同分布噪声有效。

Method: M2M框架引入方向性插值和广义中值滤波的采样策略，从单张噪声输入生成伪独立子图像对，采用随机分配策略扩大采样空间并消除系统偏差，适用于Noise2Noise训练。

Result: 在真实模拟研究中，M2M在i.i.d.噪声下与最先进零样本方法性能相当，在相关噪声下始终优于它们。

Conclusion: M2M是结构化噪声抑制的高效、无数据解决方案，标志着超越严格i.i.d.假设的有效零样本去噪的第一步。

Abstract: Image denoising is a fundamental problem in computer vision and medical imaging. However, real-world images are often degraded by structured noise with strong anisotropic correlations that existing methods struggle to remove. Most data-driven approaches rely on large datasets with high-quality labels and still suffer from limited generalizability, whereas existing zero-shot methods avoid this limitation but remain effective only for independent and identically distributed (i.i.d.) noise. To address this gap, we propose Median2Median (M2M), a zero-shot denoising framework designed for structured noise. M2M introduces a novel sampling strategy that generates pseudo-independent sub-image pairs from a single noisy input. This strategy leverages directional interpolation and generalized median filtering to adaptively exclude values distorted by structured artifacts. To further enlarge the effective sampling space and eliminate systematic bias, a randomized assignment strategy is employed, ensuring that the sampled sub-image pairs are suitable for Noise2Noise training. In our realistic simulation studies, M2M performs on par with state-of-the-art zero-shot methods under i.i.d. noise, while consistently outperforming them under correlated noise. These findings establish M2M as an efficient, data-free solution for structured noise suppression and mark the first step toward effective zero-shot denoising beyond the strict i.i.d. assumption.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [35] [ZK-WAGON: Imperceptible Watermark for Image Generation Models using ZK-SNARKs](https://arxiv.org/abs/2510.01967)
*Aadarsh Anantha Ramakrishnan,Shubham Agarwal,Selvanayagam S,Kunwar Singh*

Main category: cs.CR

TL;DR: ZK-WAGON是首个使用ZK-SNARKs技术为图像生成模型添加水印的系统，能够在不暴露模型权重、生成提示或敏感内部信息的情况下提供可验证的来源证明。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成模型变得强大且易用，合成媒体的真实性、所有权和滥用问题变得至关重要。传统水印方法要么降低图像质量，要么容易被移除，或需要访问机密模型内部信息，无法实现安全可扩展的部署。

Method: 提出选择性层ZK电路创建(SL-ZKCC)方法，选择性地将图像生成模型的关键层转换为电路，显著减少证明生成时间。生成的ZK-SNARK证明通过最低有效位(LSB)隐写术不可察觉地嵌入到生成图像中。

Result: 该系统在GAN和扩散模型上进行了演示，提供了一个安全、模型无关的可信AI图像生成流水线。

Conclusion: ZK-WAGON为图像生成模型提供了一种安全、可验证的水印解决方案，解决了合成媒体的真实性和所有权问题，同时保护模型机密信息。

Abstract: As image generation models grow increasingly powerful and accessible, concerns around authenticity, ownership, and misuse of synthetic media have become critical. The ability to generate lifelike images indistinguishable from real ones introduces risks such as misinformation, deepfakes, and intellectual property violations. Traditional watermarking methods either degrade image quality, are easily removed, or require access to confidential model internals - making them unsuitable for secure and scalable deployment. We are the first to introduce ZK-WAGON, a novel system for watermarking image generation models using the Zero-Knowledge Succinct Non Interactive Argument of Knowledge (ZK-SNARKs). Our approach enables verifiable proof of origin without exposing model weights, generation prompts, or any sensitive internal information. We propose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively convert key layers of an image generation model into a circuit, reducing proof generation time significantly. Generated ZK-SNARK proofs are imperceptibly embedded into a generated image via Least Significant Bit (LSB) steganography. We demonstrate this system on both GAN and Diffusion models, providing a secure, model-agnostic pipeline for trustworthy AI image generation.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [36] [Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation](https://arxiv.org/abs/2510.01284)
*Chetwin Low,Weimin Wang,Calder Katyal*

Main category: cs.MM

TL;DR: Ovi是一个统一的音频-视频生成范式，通过双模态DiT模块的块级跨模态融合，实现自然同步的音频视频生成，无需单独管道或后处理对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的音频-视频生成方法通常依赖复杂的多阶段架构或声音与视觉的顺序合成，这导致了同步问题和复杂的流程。

Method: 使用具有相同架构的视频和音频塔，通过块级交换时间信息（缩放RoPE嵌入）和语义信息（双向交叉注意力）进行联合训练，实现细粒度多模态融合。

Result: 模型能够生成具有自然语音和准确、上下文匹配音效的电影级视频片段，音频塔能够生成逼真的音效以及传达丰富说话人身份和情感的语音。

Conclusion: Ovi提供了一个统一的音频-视频生成框架，实现了自然的同步效果和高质量的生成结果，为电影级视频生成提供了有效解决方案。

Abstract: Audio-video generation has often relied on complex multi-stage architectures or sequential synthesis of sound and visuals. We introduce Ovi, a unified paradigm for audio-video generation that models the two modalities as a single generative process. By using blockwise cross-modal fusion of twin-DiT modules, Ovi achieves natural synchronization and removes the need for separate pipelines or post hoc alignment. To facilitate fine-grained multimodal fusion modeling, we initialize an audio tower with an architecture identical to that of a strong pretrained video model. Trained from scratch on hundreds of thousands of hours of raw audio, the audio tower learns to generate realistic sound effects, as well as speech that conveys rich speaker identity and emotion. Fusion is obtained by jointly training the identical video and audio towers via blockwise exchange of timing (via scaled-RoPE embeddings) and semantics (through bidirectional cross-attention) on a vast video corpus. Our model enables cinematic storytelling with natural speech and accurate, context-matched sound effects, producing movie-grade video clips. All the demos, code and model weights are published at https://aaxwaz.github.io/Ovi

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [37] [Aligning Video Models with Human Social Judgments via Behavior-Guided Fine-Tuning](https://arxiv.org/abs/2510.01502)
*Kathy Garcia,Leyla Isik*

Main category: q-bio.NC

TL;DR: 研究发现预训练视频模型在社会相似性感知方面存在模态差距，语言嵌入比视频模型更符合人类判断。通过人类行为数据微调视频模型，可以显著提升与人类社会感知的对齐度。


<details>
  <summary>Details</summary>
Motivation: 探究现代视频和语言模型是否能捕捉人类在社会视频中感知的相似性结构，以及如何利用人类行为数据将这种结构注入模型中。

Method: 构建包含49,000多个三选一相似性判断的基准数据集，使用混合三元组-RSA目标通过LoRA微调TimeSformer视频模型，使其成对距离与人类相似性对齐。

Result: 微调后的视频模型在保留视频上显著提高了与人类感知的对齐度，增加了与语言嵌入的共享方差，并编码了更多社会情感属性。

Conclusion: 预训练视频模型在社会识别方面存在差距，行为引导的微调能够使视频表征更接近人类的社会感知。

Abstract: Humans intuitively perceive complex social signals in visual scenes, yet it remains unclear whether state-of-the-art AI models encode the same similarity structure. We study (Q1) whether modern video and language models capture human-perceived similarity in social videos, and (Q2) how to instill this structure into models using human behavioral data. To address this, we introduce a new benchmark of over 49,000 odd-one-out similarity judgments on 250 three-second video clips of social interactions, and discover a modality gap: despite the task being visual, caption-based language embeddings align better with human similarity than any pretrained video model. We close this gap by fine-tuning a TimeSformer video model on these human judgments with our novel hybrid triplet-RSA objective using low-rank adaptation (LoRA), aligning pairwise distances to human similarity. This fine-tuning protocol yields significantly improved alignment with human perceptions on held-out videos in terms of both explained variance and odd-one-out triplet accuracy. Variance partitioning shows that the fine-tuned video model increases shared variance with language embeddings and explains additional unique variance not captured by the language model. Finally, we test transfer via linear probes and find that human-similarity fine-tuning strengthens the encoding of social-affective attributes (intimacy, valence, dominance, communication) relative to the pretrained baseline. Overall, our findings highlight a gap in pretrained video models' social recognition and demonstrate that behavior-guided fine-tuning shapes video representations toward human social perception.

</details>


### [38] [Uncovering Semantic Selectivity of Latent Groups in Higher Visual Cortex with Mutual Information-Guided Diffusion](https://arxiv.org/abs/2510.02182)
*Yule Wang,Joseph Yu,Chengrui Li,Weihan Li,Anqi Wu*

Main category: q-bio.NC

TL;DR: 提出了MIG-Vis方法，利用扩散模型生成能力可视化和验证神经潜在子空间中编码的视觉语义属性，揭示了高级视觉皮层中结构化语义表征的证据。


<details>
  <summary>Details</summary>
Motivation: 理解高级视觉区域神经群体如何编码以物体为中心的视觉信息是计算神经科学的核心挑战。现有方法间接且对神经群体本身结构提供有限见解，未能揭示特征特异性视觉信息在神经群体中的分布和组织方式。

Method: 首先使用变分自编码器从神经群体推断出群体级解缠的神经潜在子空间，然后提出互信息引导的扩散合成过程来可视化每个潜在组编码的特定视觉语义特征。

Result: 在两个猕猴下颞叶皮层的多会话神经发放数据集上验证，合成结果表明该方法识别出具有清晰语义选择性的神经潜在组，包括物体姿态、类别间转换和类内内容等多样化视觉特征。

Conclusion: 这些发现为高级视觉皮层中结构化语义表征提供了直接、可解释的证据，推进了对其编码原理的理解。

Abstract: Understanding how neural populations in higher visual areas encode object-centered visual information remains a central challenge in computational neuroscience. Prior works have investigated representational alignment between artificial neural networks and the visual cortex. Nevertheless, these findings are indirect and offer limited insights to the structure of neural populations themselves. Similarly, decoding-based methods have quantified semantic features from neural populations but have not uncovered their underlying organizations. This leaves open a scientific question: "how feature-specific visual information is distributed across neural populations in higher visual areas, and whether it is organized into structured, semantically meaningful subspaces." To tackle this problem, we present MIG-Vis, a method that leverages the generative power of diffusion models to visualize and validate the visual-semantic attributes encoded in neural latent subspaces. Our method first uses a variational autoencoder to infer a group-wise disentangled neural latent subspace from neural populations. Subsequently, we propose a mutual information (MI)-guided diffusion synthesis procedure to visualize the specific visual-semantic features encoded by each latent group. We validate MIG-Vis on multi-session neural spiking datasets from the inferior temporal (IT) cortex of two macaques. The synthesized results demonstrate that our method identifies neural latent groups with clear semantic selectivity to diverse visual features, including object pose, inter-category transformations, and intra-class content. These findings provide direct, interpretable evidence of structured semantic representation in the higher visual cortex and advance our understanding of its encoding principles.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [39] [Unsupervised Dynamic Feature Selection for Robust Latent Spaces in Vision Tasks](https://arxiv.org/abs/2510.01758)
*Bruno Corcuera,Carlos Eiras-Franco,Brais Cancela*

Main category: cs.LG

TL;DR: 提出了一种无监督动态特征选择方法，通过移除图像中的误导性和冗余信息来增强潜在表示，提高模型的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 视觉任务中的潜在表示常受噪声和无关特征影响，这会降低模型性能和泛化能力，需要一种方法来识别和移除这些不良特征。

Method: 采用无监督动态特征选择框架，为每个实例识别并移除图像中的误导或冗余信息，确保只有最相关特征贡献于潜在空间。

Result: 在图像数据集上的实验表明，配备无监督DFS的模型在聚类和图像生成等任务中显著提升了泛化性能，计算成本增加极小。

Conclusion: 无监督动态特征选择方法能有效增强潜在表示，提高模型性能，且具有广泛适用性。

Abstract: Latent representations are critical for the performance and robustness of machine learning models, as they encode the essential features of data in a compact and informative manner. However, in vision tasks, these representations are often affected by noisy or irrelevant features, which can degrade the model's performance and generalization capabilities. This paper presents a novel approach for enhancing latent representations using unsupervised Dynamic Feature Selection (DFS). For each instance, the proposed method identifies and removes misleading or redundant information in images, ensuring that only the most relevant features contribute to the latent space. By leveraging an unsupervised framework, our approach avoids reliance on labeled data, making it broadly applicable across various domains and datasets. Experiments conducted on image datasets demonstrate that models equipped with unsupervised DFS achieve significant improvements in generalization performance across various tasks, including clustering and image generation, while incurring a minimal increase in the computational cost.

</details>


### [40] [$\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models](https://arxiv.org/abs/2510.01982)
*Yujie Zhou,Pengyang Ling,Jiazi Bu,Yibin Wang,Yuhang Zang,Jiaqi Wang,Li Niu,Guangtao Zhai*

Main category: cs.LG

TL;DR: 提出了Granular-GRPO框架，通过奇异随机采样策略和多粒度优势集成模块，解决流模型强化学习中奖励信号稀疏和狭窄的问题，实现更精确全面的采样方向评估。


<details>
  <summary>Details</summary>
Motivation: 现有方法在探索高价值样本时，由于稀疏和狭窄的奖励信号导致偏好对齐效果不佳，需要更精确的采样方向奖励评估。

Method: 1. 奇异随机采样策略：支持逐步随机探索，增强奖励与注入噪声的相关性；2. 多粒度优势集成模块：聚合多个扩散尺度的优势计算，提供更全面的采样方向评估。

Result: 在各种奖励模型上的实验表明，G²RPO显著优于现有的基于流的GRPO基线方法，显示出其有效性和鲁棒性。

Conclusion: G²RPO框架通过改进的采样策略和多粒度评估机制，有效解决了流模型强化学习中的奖励评估问题，实现了更好的偏好对齐效果。

Abstract: The integration of online reinforcement learning (RL) into diffusion and flow models has recently emerged as a promising approach for aligning generative models with human preferences. Stochastic sampling via Stochastic Differential Equations (SDE) is employed during the denoising process to generate diverse denoising directions for RL exploration. While existing methods effectively explore potential high-value samples, they suffer from sub-optimal preference alignment due to sparse and narrow reward signals. To address these challenges, we propose a novel Granular-GRPO ($\text{G}^2$RPO ) framework that achieves precise and comprehensive reward assessments of sampling directions in reinforcement learning of flow models. Specifically, a Singular Stochastic Sampling strategy is introduced to support step-wise stochastic exploration while enforcing a high correlation between the reward and the injected noise, thereby facilitating a faithful reward for each SDE perturbation. Concurrently, to eliminate the bias inherent in fixed-granularity denoising, we introduce a Multi-Granularity Advantage Integration module that aggregates advantages computed at multiple diffusion scales, producing a more comprehensive and robust evaluation of the sampling directions. Experiments conducted on various reward models, including both in-domain and out-of-domain evaluations, demonstrate that our $\text{G}^2$RPO significantly outperforms existing flow-based GRPO baselines,highlighting its effectiveness and robustness.

</details>


### [41] [Continual Personalization for Diffusion Models](https://arxiv.org/abs/2510.02296)
*Yu-Chien Liao,Jr-Jen Chen,Chi-Pin Huang,Ci-Siang Lin,Meng-Lin Wu,Yu-Chiang Frank Wang*

Main category: cs.LG

TL;DR: 提出Concept Neuron Selection (CNS)方法，通过在扩散模型中识别与目标概念相关的神经元，实现增量式个性化学习，避免灾难性遗忘并保持零样本生成能力。


<details>
  <summary>Details</summary>
Motivation: 在真实应用中增量更新扩散模型具有实用性，但计算挑战大。需要解决灾难性遗忘问题，同时保持模型的零样本文本到图像生成能力。

Method: CNS方法独特识别扩散模型中与目标概念相关的神经元，以增量方式微调这些概念神经元，并联合保留先前学到的知识。

Result: 在真实数据集上的评估显示，CNS以最少的参数调整实现了最先进的性能，在单概念和多概念个性化任务中都优于先前方法。

Conclusion: CNS实现了无融合操作，减少了持续个性化所需的内存存储和处理时间，为扩散模型的增量学习提供了有效的解决方案。

Abstract: Updating diffusion models in an incremental setting would be practical in real-world applications yet computationally challenging. We present a novel learning strategy of Concept Neuron Selection (CNS), a simple yet effective approach to perform personalization in a continual learning scheme. CNS uniquely identifies neurons in diffusion models that are closely related to the target concepts. In order to mitigate catastrophic forgetting problems while preserving zero-shot text-to-image generation ability, CNS finetunes concept neurons in an incremental manner and jointly preserves knowledge learned of previous concepts. Evaluation of real-world datasets demonstrates that CNS achieves state-of-the-art performance with minimal parameter adjustments, outperforming previous methods in both single and multi-concept personalization works. CNS also achieves fusion-free operation, reducing memory storage and processing time for continual personalization.

</details>


### [42] [Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models](https://arxiv.org/abs/2510.02300)
*Runqian Wang,Yilun Du*

Main category: cs.LG

TL;DR: 提出了Equilibrium Matching (EqM)生成建模框架，通过平衡动力学视角学习隐式能量景观的平衡梯度，在推理时采用基于优化的采样过程，在ImageNet 256×256上达到1.90 FID。


<details>
  <summary>Details</summary>
Motivation: 传统扩散和基于流的生成模型使用非平衡、时间条件动力学，EqM旨在通过平衡动力学视角改进生成建模。

Method: EqM丢弃传统模型中的非平衡时间条件动力学，学习隐式能量景观的平衡梯度，在推理时使用可调节步长、自适应优化器和自适应计算的梯度下降进行采样。

Result: 在ImageNet 256×256上达到1.90 FID，超越了扩散/流模型的生成性能，并能处理部分噪声图像去噪、OOD检测和图像合成等任务。

Conclusion: EqM通过用统一的平衡景观替代时间条件速度，在流模型和能量基模型之间建立了更紧密的桥梁，提供了优化驱动推理的简单途径。

Abstract: We introduce Equilibrium Matching (EqM), a generative modeling framework built from an equilibrium dynamics perspective. EqM discards the non-equilibrium, time-conditional dynamics in traditional diffusion and flow-based generative models and instead learns the equilibrium gradient of an implicit energy landscape. Through this approach, we can adopt an optimization-based sampling process at inference time, where samples are obtained by gradient descent on the learned landscape with adjustable step sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation performance of diffusion/flow models empirically, achieving an FID of 1.90 on ImageNet 256$\times$256. EqM is also theoretically justified to learn and sample from the data manifold. Beyond generation, EqM is a flexible framework that naturally handles tasks including partially noised image denoising, OOD detection, and image composition. By replacing time-conditional velocities with a unified equilibrium landscape, EqM offers a tighter bridge between flow and energy-based models and a simple route to optimization-driven inference.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [43] [Towards Photonic Band Diagram Generation with Transformer-Latent Diffusion Models](https://arxiv.org/abs/2510.01749)
*Valentin Delchevalerie,Nicolas Roy,Arnaud Bougaham,Alexandre Mayer,Benoît Frénay,Michaël Lobet*

Main category: physics.optics

TL;DR: 本文提出了首个基于扩散模型的光子晶体能带图生成方法，该方法结合Transformer编码器和潜在扩散模型，能够从输入结构生成对应的能带图，为光子学领域的新代理建模策略铺平道路。


<details>
  <summary>Details</summary>
Motivation: 光子晶体在纳米尺度上精细控制光传播，在光子和量子技术发展中起核心作用。计算能带图需要求解多个配置下的麦克斯韦方程，数值计算成本高昂，特别是在逆向设计技术的优化循环中。

Method: 将Transformer编码器与潜在扩散模型耦合：Transformer提取输入结构的上下文嵌入，潜在扩散模型生成对应的能带图。

Result: 开发了首个基于扩散模型的能带图生成方法，能够推广并扩展到任意三维结构。

Conclusion: Transformer和扩散模型非常适合捕捉光子学中固有的复杂干涉和散射现象，为光子学领域的新代理建模策略开辟了道路。

Abstract: Photonic crystals enable fine control over light propagation at the nanoscale, and thus play a central role in the development of photonic and quantum technologies. Photonic band diagrams (BDs) are a key tool to investigate light propagation into such inhomogeneous structured materials. However, computing BDs requires solving Maxwell's equations across many configurations, making it numerically expensive, especially when embedded in optimization loops for inverse design techniques, for example. To address this challenge, we introduce the first approach for BD generation based on diffusion models, with the capacity to later generalize and scale to arbitrary three dimensional structures. Our method couples a transformer encoder, which extracts contextual embeddings from the input structure, with a latent diffusion model to generate the corresponding BD. In addition, we provide insights into why transformers and diffusion models are well suited to capture the complex interference and scattering phenomena inherent to photonics, paving the way for new surrogate modeling strategies in this domain.

</details>
