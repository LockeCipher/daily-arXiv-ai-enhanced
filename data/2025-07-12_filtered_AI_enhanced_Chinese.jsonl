{"id": "2507.07133", "pdf": "https://arxiv.org/pdf/2507.07133", "abs": "https://arxiv.org/abs/2507.07133", "authors": ["Mathieu Tuli", "Kaveh Kamali", "David B. Lindell"], "title": "Generative Panoramic Image Stitching", "categories": ["cs.GR", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce the task of generative panoramic image stitching, which aims to synthesize seamless panoramas that are faithful to the content of multiple reference images containing parallax effects and strong variations in lighting, camera capture settings, or style. In this challenging setting, traditional image stitching pipelines fail, producing outputs with ghosting and other artifacts. While recent generative models are capable of outpainting content consistent with multiple reference images, they fail when tasked with synthesizing large, coherent regions of a panorama. To address these limitations, we propose a method that fine-tunes a diffusion-based inpainting model to preserve a scene's content and layout based on multiple reference images. Once fine-tuned, the model outpaints a full panorama from a single reference image, producing a seamless and visually coherent result that faithfully integrates content from all reference images. Our approach significantly outperforms baselines for this task in terms of image quality and the consistency of image structure and scene layout when evaluated on captured datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u5f0f\u5168\u666f\u56fe\u50cf\u62fc\u63a5\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u89c6\u5dee\u3001\u5149\u7167\u548c\u98ce\u683c\u5dee\u5f02\u4e0b\u7684\u62fc\u63a5\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u62fc\u63a5\u65b9\u6cd5\u5728\u89c6\u5dee\u548c\u5149\u7167\u53d8\u5316\u4e0b\u4f1a\u4ea7\u751f\u4f2a\u5f71\uff0c\u800c\u73b0\u6709\u751f\u6210\u6a21\u578b\u96be\u4ee5\u5408\u6210\u5927\u8303\u56f4\u8fde\u8d2f\u7684\u5168\u666f\u56fe\u3002", "method": "\u901a\u8fc7\u5fae\u8c03\u6269\u6563\u5f0f\u4fee\u590d\u6a21\u578b\uff0c\u57fa\u4e8e\u591a\u5f20\u53c2\u8003\u56fe\u50cf\u4fdd\u7559\u573a\u666f\u5185\u5bb9\u548c\u5e03\u5c40\uff0c\u751f\u6210\u65e0\u7f1d\u5168\u666f\u56fe\u3002", "result": "\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u573a\u666f\u5e03\u5c40\u4e00\u81f4\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u65e0\u7f1d\u4e14\u89c6\u89c9\u8fde\u8d2f\u7684\u5168\u666f\u56fe\uff0c\u5fe0\u5b9e\u6574\u5408\u591a\u5f20\u53c2\u8003\u56fe\u50cf\u5185\u5bb9\u3002"}}
{"id": "2507.07136", "pdf": "https://arxiv.org/pdf/2507.07136", "abs": "https://arxiv.org/abs/2507.07136", "authors": ["Wanhua Li", "Yujie Zhao", "Minghan Qin", "Yang Liu", "Yuanhao Cai", "Chuang Gan", "Hanspeter Pfister"], "title": "LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+ FPS", "categories": ["cs.GR"], "comment": "Project Page: https://langsplat-v2.github.io", "summary": "In this paper, we introduce LangSplatV2, which achieves high-dimensional feature splatting at 476.2 FPS and 3D open-vocabulary text querying at 384.6 FPS for high-resolution images, providing a 42 $\\times$ speedup and a 47 $\\times$ boost over LangSplat respectively, along with improved query accuracy. LangSplat employs Gaussian Splatting to embed 2D CLIP language features into 3D, significantly enhancing speed and learning a precise 3D language field with SAM semantics. Such advancements in 3D language fields are crucial for applications that require language interaction within complex scenes. However, LangSplat does not yet achieve real-time inference performance (8.2 FPS), even with advanced A100 GPUs, severely limiting its broader application. In this paper, we first conduct a detailed time analysis of LangSplat, identifying the heavyweight decoder as the primary speed bottleneck. Our solution, LangSplatV2 assumes that each Gaussian acts as a sparse code within a global dictionary, leading to the learning of a 3D sparse coefficient field that entirely eliminates the need for a heavyweight decoder. By leveraging this sparsity, we further propose an efficient sparse coefficient splatting method with CUDA optimization, rendering high-dimensional feature maps at high quality while incurring only the time cost of splatting an ultra-low-dimensional feature. Our experimental results demonstrate that LangSplatV2 not only achieves better or competitive query accuracy but is also significantly faster. Codes and demos are available at our project page: https://langsplat-v2.github.io.", "AI": {"tldr": "LangSplatV2\u901a\u8fc7\u7a00\u758f\u7f16\u7801\u548c\u9ad8\u65af\u6e85\u5c04\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u8bed\u8a00\u7279\u5f81\u7684\u5904\u7406\u901f\u5ea6\u548c\u67e5\u8be2\u51c6\u786e\u6027\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "LangSplat\u5728\u5b9e\u65f6\u63a8\u7406\u6027\u80fd\u4e0a\u8868\u73b0\u4e0d\u4f73\uff088.2 FPS\uff09\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002LangSplatV2\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7a00\u758f\u7f16\u7801\u5047\u8bbe\uff0c\u5b66\u4e603D\u7a00\u758f\u7cfb\u6570\u573a\uff0c\u6d88\u9664\u91cd\u578b\u89e3\u7801\u5668\u9700\u6c42\uff0c\u5e76\u7ed3\u5408CUDA\u4f18\u5316\u7684\u7a00\u758f\u7cfb\u6570\u6e85\u5c04\u65b9\u6cd5\u3002", "result": "LangSplatV2\u8fbe\u5230476.2 FPS\u7684\u9ad8\u7ef4\u7279\u5f81\u6e85\u5c04\u548c384.6 FPS\u76843D\u5f00\u653e\u8bcd\u6c47\u67e5\u8be2\uff0c\u901f\u5ea6\u548c\u51c6\u786e\u6027\u5747\u663e\u8457\u63d0\u5347\u3002", "conclusion": "LangSplatV2\u5728\u4fdd\u6301\u6216\u63d0\u5347\u67e5\u8be2\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u4f18\u5316\uff0c\u4e3a\u590d\u6742\u573a\u666f\u4e2d\u7684\u8bed\u8a00\u4ea4\u4e92\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07387", "pdf": "https://arxiv.org/pdf/2507.07387", "abs": "https://arxiv.org/abs/2507.07387", "authors": ["Chengan He", "Jorge Alejandro Amador Herrera", "Zhixin Shu", "Xin Sun", "Yao Feng", "S\u00f6ren Pirk", "Dominik L. Michels", "Meng Zhang", "Tuanfeng Y. Wang", "Julie Dorsey", "Holly Rushmeier", "Yi Zhou"], "title": "Digital Salon: An AI and Physics-Driven Tool for 3D Hair Grooming and Simulation", "categories": ["cs.GR", "cs.HC"], "comment": null, "summary": "We introduce Digital Salon, a comprehensive hair authoring system that supports real-time 3D hair generation, simulation, and rendering. Unlike existing methods that focus on isolated parts of 3D hair modeling and involve a heavy computation process or network training, Digital Salon offers a holistic and interactive system that lowers the technical barriers of 3D hair modeling through natural language-based interaction. The system guides users through four key stages: text-guided hair retrieval, real-time hair simulation, interactive hair refinement, and hair-conditioned image generation. This cohesive workflow makes advanced hair design accessible to users of varying skill levels and dramatically streamlines the creative process in digital media with an intuitive, versatile, and efficient solution for hair modeling. User studies show that our system can outperform traditional hair modeling workflows for rapid prototyping. Furthermore, we provide insights into the benefits of our system with future potential of deploying our system in real salon environments. More details can be found on our project page: https://digital-salon.github.io/.", "AI": {"tldr": "Digital Salon\u662f\u4e00\u4e2a\u652f\u6301\u5b9e\u65f63D\u5934\u53d1\u751f\u6210\u3001\u6a21\u62df\u548c\u6e32\u67d3\u7684\u7efc\u5408\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u964d\u4f4e\u6280\u672f\u95e8\u69db\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e13\u6ce8\u4e8e3D\u5934\u53d1\u5efa\u6a21\u7684\u5b64\u7acb\u90e8\u5206\uff0c\u8ba1\u7b97\u91cf\u5927\u6216\u9700\u8981\u7f51\u7edc\u8bad\u7ec3\uff0cDigital Salon\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u5168\u9762\u4e14\u4ea4\u4e92\u5f0f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7cfb\u7edf\u5305\u62ec\u56db\u4e2a\u5173\u952e\u9636\u6bb5\uff1a\u6587\u672c\u5f15\u5bfc\u7684\u5934\u53d1\u68c0\u7d22\u3001\u5b9e\u65f6\u5934\u53d1\u6a21\u62df\u3001\u4ea4\u4e92\u5f0f\u5934\u53d1\u7cbe\u4fee\u548c\u5934\u53d1\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u5de5\u4f5c\u6d41\u7a0b\u3002", "conclusion": "Digital Salon\u4e3a\u4e0d\u540c\u6280\u80fd\u6c34\u5e73\u7684\u7528\u6237\u63d0\u4f9b\u4e86\u76f4\u89c2\u3001\u591a\u529f\u80fd\u4e14\u9ad8\u6548\u7684\u5934\u53d1\u5efa\u6a21\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u6c99\u9f99\u73af\u5883\u4e2d\u90e8\u7f72\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.07465", "pdf": "https://arxiv.org/pdf/2507.07465", "abs": "https://arxiv.org/abs/2507.07465", "authors": ["Wei Yao", "Shuzhao Xie", "Letian Li", "Weixiang Zhang", "Zhixin Lai", "Shiqi Dai", "Ke Zhang", "Zhi Wang"], "title": "SD-GS: Structured Deformable 3D Gaussians for Efficient Dynamic Scene Reconstruction", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Current 4D Gaussian frameworks for dynamic scene reconstruction deliver impressive visual fidelity and rendering speed, however, the inherent trade-off between storage costs and the ability to characterize complex physical motions significantly limits the practical application of these methods. To tackle these problems, we propose SD-GS, a compact and efficient dynamic Gaussian splatting framework for complex dynamic scene reconstruction, featuring two key contributions. First, we introduce a deformable anchor grid, a hierarchical and memory-efficient scene representation where each anchor point derives multiple 3D Gaussians in its local spatiotemporal region and serves as the geometric backbone of the 3D scene. Second, to enhance modeling capability for complex motions, we present a deformation-aware densification strategy that adaptively grows anchors in under-reconstructed high-dynamic regions while reducing redundancy in static areas, achieving superior visual quality with fewer anchors. Experimental results demonstrate that, compared to state-of-the-art methods, SD-GS achieves an average of 60\\% reduction in model size and an average of 100\\% improvement in FPS, significantly enhancing computational efficiency while maintaining or even surpassing visual quality.", "AI": {"tldr": "SD-GS\u662f\u4e00\u79cd\u7d27\u51d1\u9ad8\u6548\u7684\u52a8\u6001\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u53d8\u5f62\u951a\u70b9\u7f51\u683c\u548c\u53d8\u5f62\u611f\u77e5\u7684\u5bc6\u96c6\u5316\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5b58\u50a8\u6210\u672c\u5e76\u63d0\u5347\u4e86\u6e32\u67d3\u901f\u5ea6\u3002", "motivation": "\u5f53\u524d4D\u9ad8\u65af\u6846\u67b6\u5728\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e2d\u5b58\u5728\u5b58\u50a8\u6210\u672c\u4e0e\u590d\u6742\u8fd0\u52a8\u5efa\u6a21\u80fd\u529b\u7684\u6743\u8861\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u53ef\u53d8\u5f62\u951a\u70b9\u7f51\u683c\u4f5c\u4e3a\u5c42\u6b21\u5316\u3001\u5185\u5b58\u9ad8\u6548\u7684\u573a\u666f\u8868\u793a\uff0c\u5e76\u7ed3\u5408\u53d8\u5f62\u611f\u77e5\u5bc6\u96c6\u5316\u7b56\u7565\uff0c\u52a8\u6001\u8c03\u6574\u951a\u70b9\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cSD-GS\u6a21\u578b\u5927\u5c0f\u5e73\u5747\u51cf\u5c1160%\uff0cFPS\u63d0\u5347100%\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u8d85\u8d8a\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "SD-GS\u5728\u5b58\u50a8\u6548\u7387\u548c\u8ba1\u7b97\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u590d\u6742\u52a8\u6001\u573a\u666f\u91cd\u5efa\u3002"}}
{"id": "2507.07623", "pdf": "https://arxiv.org/pdf/2507.07623", "abs": "https://arxiv.org/abs/2507.07623", "authors": ["Hannah Dr\u00f6ge", "Janelle Pfeifer", "Saskia Rabich", "Markus Plack", "Reinhard Klein", "Matthias B. Hullin"], "title": "Capture Stage Environments: A Guide to Better Matting", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Capture stages are high-end sources of state-of-the-art recordings for downstream applications in movies, games, and other media. One crucial step in almost all pipelines is the matting of images to isolate the captured performances from the background. While common matting algorithms deliver remarkable performance in other applications like teleconferencing and mobile entertainment, we found that they struggle significantly with the peculiarities of capture stage content. The goal of our work is to share insights into those challenges as a curated list of those characteristics along with a constructive discussion for proactive intervention and present a guideline to practitioners for an improved workflow to mitigate unresolved challenges. To this end, we also demonstrate an efficient pipeline to adapt state-of-the-art approaches to such custom setups without the need of extensive annotations, both offline and real-time. For an objective evaluation, we propose a validation methodology based on a leading diffusion model that highlights the benefits of our approach.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u9ad8\u7aef\u6355\u6349\u573a\u666f\u4e2d\u56fe\u50cf\u62a0\u56fe\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u5de5\u4f5c\u6d41\u7a0b\u7684\u6307\u5357\uff0c\u5e76\u5c55\u793a\u4e86\u4e00\u79cd\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u7684\u9ad8\u6548\u7ba1\u9053\u3002", "motivation": "\u73b0\u6709\u62a0\u56fe\u7b97\u6cd5\u5728\u6355\u6349\u573a\u666f\u5185\u5bb9\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u89e3\u51b3\u5176\u7279\u6b8a\u6027\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u6539\u8fdb\u5de5\u4f5c\u6d41\u7a0b\u6307\u5357\uff0c\u5e76\u5c55\u793a\u79bb\u7ebf\u4e0e\u5b9e\u65f6\u7684\u9ad8\u6548\u7ba1\u9053\uff0c\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u3002", "result": "\u901a\u8fc7\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "conclusion": "\u8bba\u6587\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u89e3\u51b3\u6355\u6349\u573a\u666f\u62a0\u56fe\u6311\u6218\u7684\u5b9e\u7528\u6307\u5357\u548c\u9ad8\u6548\u65b9\u6cd5\u3002"}}
{"id": "2507.07139", "pdf": "https://arxiv.org/pdf/2507.07139", "abs": "https://arxiv.org/abs/2507.07139", "authors": ["Renyang Liu", "Guanlin Li", "Tianwei Zhang", "See-Kiong Ng"], "title": "Image Can Bring Your Memory Back: A Novel Multi-Modal Guided Attack against Image Generation Model Unlearning", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": null, "summary": "Recent advances in image generation models (IGMs), particularly diffusion-based architectures such as Stable Diffusion (SD), have markedly enhanced the quality and diversity of AI-generated visual content. However, their generative capability has also raised significant ethical, legal, and societal concerns, including the potential to produce harmful, misleading, or copyright-infringing content. To mitigate these concerns, machine unlearning (MU) emerges as a promising solution by selectively removing undesirable concepts from pretrained models. Nevertheless, the robustness and effectiveness of existing unlearning techniques remain largely unexplored, particularly in the presence of multi-modal adversarial inputs.   To bridge this gap, we propose Recall, a novel adversarial framework explicitly designed to compromise the robustness of unlearned IGMs. Unlike existing approaches that predominantly rely on adversarial text prompts, Recall exploits the intrinsic multi-modal conditioning capabilities of diffusion models by efficiently optimizing adversarial image prompts with guidance from a single semantically relevant reference image. Extensive experiments across ten state-of-the-art unlearning methods and diverse tasks show that Recall consistently outperforms existing baselines in terms of adversarial effectiveness, computational efficiency, and semantic fidelity with the original textual prompt. These findings reveal critical vulnerabilities in current unlearning mechanisms and underscore the need for more robust solutions to ensure the safety and reliability of generative models. Code and data are publicly available at \\textcolor{blue}{https://github.com/ryliu68/RECALL}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRecall\u7684\u65b0\u578b\u5bf9\u6297\u6846\u67b6\uff0c\u65e8\u5728\u6d4b\u8bd5\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff08\u5982Stable Diffusion\uff09\u5728\u53bb\u5b66\u4e60\u6280\u672f\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5c3d\u7ba1\u53bb\u5b66\u4e60\u6280\u672f\uff08MU\uff09\u88ab\u63d0\u51fa\u4ee5\u89e3\u51b3\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff08IGMs\uff09\u53ef\u80fd\u4ea7\u751f\u7684\u6709\u5bb3\u6216\u4fb5\u6743\u5185\u5bb9\uff0c\u4f46\u5176\u5728\u591a\u6a21\u6001\u5bf9\u6297\u8f93\u5165\u4e0b\u7684\u6709\u6548\u6027\u5c1a\u672a\u5145\u5206\u9a8c\u8bc1\u3002", "method": "Recall\u901a\u8fc7\u4f18\u5316\u5bf9\u6297\u6027\u56fe\u50cf\u63d0\u793a\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u591a\u6a21\u6001\u6761\u4ef6\u80fd\u529b\uff0c\u6d4b\u8bd5\u73b0\u6709\u53bb\u5b66\u4e60\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRecall\u5728\u5bf9\u6297\u6548\u679c\u3001\u8ba1\u7b97\u6548\u7387\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u53bb\u5b66\u4e60\u673a\u5236\u7684\u8106\u5f31\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u53bb\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u786e\u4fdd\u751f\u6210\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.07733", "pdf": "https://arxiv.org/pdf/2507.07733", "abs": "https://arxiv.org/abs/2507.07733", "authors": ["Yongyang Zhou", "Fang-Lue Zhang", "Zichen Wang", "Lei Zhang"], "title": "RTR-GS: 3D Gaussian Splatting for Inverse Rendering with Radiance Transfer and Reflection", "categories": ["cs.GR", "cs.CV"], "comment": "16 pages", "summary": "3D Gaussian Splatting (3DGS) has demonstrated impressive capabilities in novel view synthesis. However, rendering reflective objects remains a significant challenge, particularly in inverse rendering and relighting. We introduce RTR-GS, a novel inverse rendering framework capable of robustly rendering objects with arbitrary reflectance properties, decomposing BRDF and lighting, and delivering credible relighting results. Given a collection of multi-view images, our method effectively recovers geometric structure through a hybrid rendering model that combines forward rendering for radiance transfer with deferred rendering for reflections. This approach successfully separates high-frequency and low-frequency appearances, mitigating floating artifacts caused by spherical harmonic overfitting when handling high-frequency details. We further refine BRDF and lighting decomposition using an additional physically-based deferred rendering branch. Experimental results show that our method enhances novel view synthesis, normal estimation, decomposition, and relighting while maintaining efficient training inference process.", "AI": {"tldr": "RTR-GS\u662f\u4e00\u79cd\u65b0\u7684\u9006\u6e32\u67d3\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u4efb\u610f\u53cd\u5c04\u5c5e\u6027\u7684\u7269\u4f53\uff0c\u5206\u89e3BRDF\u548c\u5149\u7167\uff0c\u5e76\u63d0\u4f9b\u53ef\u4fe1\u7684\u91cd\u5149\u7167\u7ed3\u679c\u3002", "motivation": "\u5c3d\u7ba13D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u5728\u65b0\u89c6\u89d2\u5408\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u6e32\u67d3\u53cd\u5c04\u7269\u4f53\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u9006\u6e32\u67d3\u548c\u91cd\u5149\u7167\u65b9\u9762\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u524d\u5411\u6e32\u67d3\u548c\u5ef6\u8fdf\u6e32\u67d3\u7684\u6df7\u5408\u6a21\u578b\uff0cRTR-GS\u6709\u6548\u6062\u590d\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u5206\u79bb\u9ad8\u9891\u548c\u4f4e\u9891\u5916\u89c2\u3002\u8fdb\u4e00\u6b65\u4f7f\u7528\u57fa\u4e8e\u7269\u7406\u7684\u5ef6\u8fdf\u6e32\u67d3\u5206\u652f\u4f18\u5316BRDF\u548c\u5149\u7167\u5206\u89e3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u65b0\u89c6\u89d2\u5408\u6210\u3001\u6cd5\u7ebf\u4f30\u8ba1\u3001\u5206\u89e3\u548c\u91cd\u5149\u7167\u7684\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u3002", "conclusion": "RTR-GS\u5728\u9006\u6e32\u67d3\u548c\u91cd\u5149\u7167\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u9ad8\u9891\u7387\u7ec6\u8282\u5904\u7406\u4e2d\u7684\u95ee\u9898\u3002"}}
{"id": "2507.07151", "pdf": "https://arxiv.org/pdf/2507.07151", "abs": "https://arxiv.org/abs/2507.07151", "authors": ["Zongmeng Zhang", "Wengang Zhou", "Jie Zhao", "Houqiang Li"], "title": "Robust Multimodal Large Language Models Against Modality Conflict", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "ICML 2025", "summary": "Despite the impressive capabilities of multimodal large language models (MLLMs) in vision-language tasks, they are prone to hallucinations in real-world scenarios. This paper investigates the hallucination phenomenon in MLLMs from the perspective of modality conflict. Unlike existing works focusing on the conflicts between model responses and inputs, we study the inherent conflicts in inputs from different modalities that place MLLMs in a dilemma and directly lead to hallucinations. We formally define the modality conflict and construct a dataset named Multimodal Modality Conflict (MMMC) to simulate this phenomenon in vision-language tasks. Three methods based on prompt engineering, supervised fine-tuning, and reinforcement learning are proposed to alleviate the hallucination caused by modality conflict. Extensive experiments are conducted on the MMMC dataset to analyze the merits and demerits of these methods. Our results show that the reinforcement learning method achieves the best performance in mitigating the hallucination under modality conflict, while the supervised fine-tuning method shows promising and stable performance. Our work sheds light on the unnoticed modality conflict that leads to hallucinations and provides more insights into the robustness of MLLMs.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u63d0\u51fa\u6a21\u6001\u51b2\u7a81\u662f\u5bfc\u81f4\u5e7b\u89c9\u7684\u539f\u56e0\uff0c\u5e76\u6784\u5efa\u4e86MMMC\u6570\u636e\u96c6\u548c\u4e09\u79cd\u7f13\u89e3\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u6a21\u578b\u54cd\u5e94\u4e0e\u8f93\u5165\u7684\u51b2\u7a81\uff0c\u800c\u5ffd\u7565\u4e86\u591a\u6a21\u6001\u8f93\u5165\u95f4\u7684\u56fa\u6709\u51b2\u7a81\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u3001\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u4e09\u79cd\u65b9\u6cd5\u7f13\u89e3\u6a21\u6001\u51b2\u7a81\u5f15\u53d1\u7684\u5e7b\u89c9\uff0c\u5e76\u5728MMMC\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u7f13\u89e3\u6a21\u6001\u51b2\u7a81\u5bfc\u81f4\u7684\u5e7b\u89c9\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u8868\u73b0\u7a33\u5b9a\u4e14\u6709\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6a21\u6001\u51b2\u7a81\u5bf9\u5e7b\u89c9\u7684\u5f71\u54cd\uff0c\u4e3a\u63d0\u5347MLLMs\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.07157", "pdf": "https://arxiv.org/pdf/2507.07157", "abs": "https://arxiv.org/abs/2507.07157", "authors": ["Arshak Rezvani", "Ali Akbari", "Kosar Sanjar Arani", "Maryam Mirian", "Emad Arasteh", "Martin J. McKeown"], "title": "Interpretable EEG-to-Image Generation with Semantic Prompts", "categories": ["cs.CV", "cs.LG", "eess.SP"], "comment": "Actionable Interpretability Workshop (non-archival) at the 42   International Conference on Machine Learning", "summary": "Decoding visual experience from brain signals offers exciting possibilities for neuroscience and interpretable AI. While EEG is accessible and temporally precise, its limitations in spatial detail hinder image reconstruction. Our model bypasses direct EEG-to-image generation by aligning EEG signals with multilevel semantic captions -- ranging from object-level to abstract themes -- generated by a large language model. A transformer-based EEG encoder maps brain activity to these captions through contrastive learning. During inference, caption embeddings retrieved via projection heads condition a pretrained latent diffusion model for image generation. This text-mediated framework yields state-of-the-art visual decoding on the EEGCVPR dataset, with interpretable alignment to known neurocognitive pathways. Dominant EEG-caption associations reflected the importance of different semantic levels extracted from perceived images. Saliency maps and t-SNE projections reveal semantic topography across the scalp. Our model demonstrates how structured semantic mediation enables cognitively aligned visual decoding from EEG.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u8bed\u4e49\u5bf9\u9f50EEG\u4fe1\u53f7\u4e0e\u591a\u7ea7\u6587\u672c\u63cf\u8ff0\uff0c\u95f4\u63a5\u751f\u6210\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86EEG\u89c6\u89c9\u89e3\u7801\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "EEG\u5728\u89c6\u89c9\u89e3\u7801\u4e2d\u7a7a\u95f4\u5206\u8fa8\u7387\u6709\u9650\uff0c\u76f4\u63a5\u751f\u6210\u56fe\u50cf\u6548\u679c\u4e0d\u4f73\u3002\u901a\u8fc7\u8bed\u4e49\u4e2d\u4ecb\uff08\u591a\u7ea7\u6587\u672c\u63cf\u8ff0\uff09\u53ef\u4ee5\u66f4\u597d\u5730\u89e3\u7801\u5927\u8111\u6d3b\u52a8\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eTransformer\u7684EEG\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5c06EEG\u4fe1\u53f7\u5bf9\u9f50\u5230\u591a\u7ea7\u8bed\u4e49\u63cf\u8ff0\uff08\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\uff09\u3002\u63a8\u7406\u65f6\uff0c\u5229\u7528\u6295\u5f71\u5934\u63d0\u53d6\u7684\u6587\u672c\u5d4c\u5165\u6761\u4ef6\u9884\u8bad\u7ec3\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u751f\u6210\u56fe\u50cf\u3002", "result": "\u5728EEGCVPR\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u89e3\u7801\u6027\u80fd\uff0cEEG\u4e0e\u8bed\u4e49\u63cf\u8ff0\u7684\u5173\u8054\u63ed\u793a\u4e86\u795e\u7ecf\u8ba4\u77e5\u8def\u5f84\u7684\u5c42\u6b21\u7ed3\u6784\u3002", "conclusion": "\u901a\u8fc7\u8bed\u4e49\u4e2d\u4ecb\u7684\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u4e0e\u8ba4\u77e5\u5bf9\u9f50\u7684EEG\u89c6\u89c9\u89e3\u7801\uff0c\u4e3a\u795e\u7ecf\u79d1\u5b66\u548c\u53ef\u89e3\u91caAI\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.07202", "pdf": "https://arxiv.org/pdf/2507.07202", "abs": "https://arxiv.org/abs/2507.07202", "authors": ["Mohamed Elmoghany", "Ryan Rossi", "Seunghyun Yoon", "Subhojyoti Mukherjee", "Eslam Bakr", "Puneet Mathur", "Gang Wu", "Viet Dac Lai", "Nedim Lipka", "Ruiyi Zhang", "Varun Manjunatha", "Chien Nguyen", "Daksh Dangi", "Abel Salinas", "Mohammad Taesiri", "Hongjie Chen", "Xiaolei Huang", "Joe Barrow", "Nesreen Ahmed", "Hoda Eldardiry", "Namyong Park", "Yu Wang", "Jaemin Cho", "Anh Totti Nguyen", "Zhengzhong Tu", "Thien Nguyen", "Dinesh Manocha", "Mohamed Elhoseiny", "Franck Dernoncourt"], "title": "A Survey on Long-Video Storytelling Generation: Architectures, Consistency, and Cinematic Quality", "categories": ["cs.CV"], "comment": null, "summary": "Despite the significant progress that has been made in video generative models, existing state-of-the-art methods can only produce videos lasting 5-16 seconds, often labeled \"long-form videos\". Furthermore, videos exceeding 16 seconds struggle to maintain consistent character appearances and scene layouts throughout the narrative. In particular, multi-subject long videos still fail to preserve character consistency and motion coherence. While some methods can generate videos up to 150 seconds long, they often suffer from frame redundancy and low temporal diversity. Recent work has attempted to produce long-form videos featuring multiple characters, narrative coherence, and high-fidelity detail. We comprehensively studied 32 papers on video generation to identify key architectural components and training strategies that consistently yield these qualities. We also construct a comprehensive novel taxonomy of existing methods and present comparative tables that categorize papers by their architectural designs and performance characteristics.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u957f\u89c6\u9891\u751f\u6210\u7684\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u65b0\u7684\u5206\u7c7b\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u7684\u957f\u89c6\u9891\uff08\u8d85\u8fc716\u79d2\uff09\u96be\u4ee5\u4fdd\u6301\u89d2\u8272\u4e00\u81f4\u6027\u548c\u573a\u666f\u5e03\u5c40\uff0c\u4e14\u591a\u4e3b\u9898\u957f\u89c6\u9891\u8868\u73b0\u66f4\u5dee\u3002", "method": "\u7814\u7a76\u4e8632\u7bc7\u89c6\u9891\u751f\u6210\u8bba\u6587\uff0c\u8bc6\u522b\u5173\u952e\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u6784\u5efa\u4e86\u65b0\u7684\u5206\u7c7b\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u6539\u8fdb\u957f\u89c6\u9891\u751f\u6210\u7684\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u5206\u7c7b\u8868\u548c\u6027\u80fd\u6bd4\u8f83\u3002", "conclusion": "\u8bba\u6587\u4e3a\u957f\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u548c\u5206\u7c7b\u6846\u67b6\u3002"}}
{"id": "2507.07333", "pdf": "https://arxiv.org/pdf/2507.07333", "abs": "https://arxiv.org/abs/2507.07333", "authors": ["Hui Pang", "Sunil Hadap", "Violetta Shevchenko", "Rahul Suresh", "Amin Banitalebi-Dehkordi"], "title": "Scalable and Realistic Virtual Try-on Application for Foundation Makeup with Kubelka-Munk Theory", "categories": ["cs.CV", "I.4.9"], "comment": "Presented at the workshop Three questions about virtual try-on at   CVPR 2025", "summary": "Augmented reality is revolutionizing beauty industry with virtual try-on (VTO) applications, which empowers users to try a wide variety of products using their phones without the hassle of physically putting on real products. A critical technical challenge in foundation VTO applications is the accurate synthesis of foundation-skin tone color blending while maintaining the scalability of the method across diverse product ranges. In this work, we propose a novel method to approximate well-established Kubelka-Munk (KM) theory for faster image synthesis while preserving foundation-skin tone color blending realism. Additionally, we build a scalable end-to-end framework for realistic foundation makeup VTO solely depending on the product information available on e-commerce sites. We validate our method using real-world makeup images, demonstrating that our framework outperforms other techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKubelka-Munk\u7406\u8bba\u7684\u5feb\u901f\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u865a\u62df\u8bd5\u5986\uff08VTO\uff09\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u7aef\u5230\u7aef\u6846\u67b6\u3002", "motivation": "\u89e3\u51b3\u865a\u62df\u8bd5\u5986\u4e2d\u7c89\u5e95\u4e0e\u80a4\u8272\u51c6\u786e\u878d\u5408\u7684\u6280\u672f\u6311\u6218\uff0c\u540c\u65f6\u4fdd\u6301\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u8fd1\u4f3cKubelka-Munk\u7406\u8bba\u4ee5\u5b9e\u73b0\u5feb\u901f\u56fe\u50cf\u5408\u6210\uff0c\u5e76\u6784\u5efa\u4f9d\u8d56\u7535\u5546\u4ea7\u54c1\u4fe1\u606f\u7684\u7aef\u5230\u7aef\u6846\u67b6\u3002", "result": "\u5728\u771f\u5b9e\u5316\u5986\u56fe\u50cf\u4e0a\u9a8c\u8bc1\uff0c\u4f18\u4e8e\u5176\u4ed6\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u771f\u5b9e\u611f\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.07394", "pdf": "https://arxiv.org/pdf/2507.07394", "abs": "https://arxiv.org/abs/2507.07394", "authors": ["Zhimin Zhang", "Bi'an Du", "Caoyuan Ma", "Zheng Wang", "Wei Hu"], "title": "Behave Your Motion: Habit-preserved Cross-category Animal Motion Transfer", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Animal motion embodies species-specific behavioral habits, making the transfer of motion across categories a critical yet complex task for applications in animation and virtual reality. Existing motion transfer methods, primarily focused on human motion, emphasize skeletal alignment (motion retargeting) or stylistic consistency (motion style transfer), often neglecting the preservation of distinct habitual behaviors in animals. To bridge this gap, we propose a novel habit-preserved motion transfer framework for cross-category animal motion. Built upon a generative framework, our model introduces a habit-preservation module with category-specific habit encoder, allowing it to learn motion priors that capture distinctive habitual characteristics. Furthermore, we integrate a large language model (LLM) to facilitate the motion transfer to previously unobserved species. To evaluate the effectiveness of our approach, we introduce the DeformingThings4D-skl dataset, a quadruped dataset with skeletal bindings, and conduct extensive experiments and quantitative analyses, which validate the superiority of our proposed model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4fdd\u7559\u52a8\u7269\u4e60\u60ef\u7684\u8de8\u7c7b\u522b\u8fd0\u52a8\u8f6c\u79fb\u6846\u67b6\uff0c\u7ed3\u5408\u751f\u6210\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u52a8\u7269\u4e60\u60ef\u884c\u4e3a\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8fd0\u52a8\u8f6c\u79fb\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4eba\u7c7b\u8fd0\u52a8\uff0c\u5ffd\u7565\u4e86\u52a8\u7269\u7279\u6709\u7684\u4e60\u60ef\u884c\u4e3a\uff0c\u5bfc\u81f4\u8de8\u7c7b\u522b\u52a8\u7269\u8fd0\u52a8\u8f6c\u79fb\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u57fa\u4e8e\u751f\u6210\u6846\u67b6\uff0c\u5f15\u5165\u4e60\u60ef\u4fdd\u7559\u6a21\u5757\u548c\u7c7b\u522b\u7279\u5b9a\u4e60\u60ef\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5904\u7406\u672a\u89c2\u5bdf\u5230\u7684\u7269\u79cd\u3002", "result": "\u5728DeformingThings4D-skl\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u4fdd\u7559\u4e86\u52a8\u7269\u4e60\u60ef\u884c\u4e3a\uff0c\u4e3a\u8de8\u7c7b\u522b\u52a8\u7269\u8fd0\u52a8\u8f6c\u79fb\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.07395", "pdf": "https://arxiv.org/pdf/2507.07395", "abs": "https://arxiv.org/abs/2507.07395", "authors": ["Yongtang Bao", "Chengjie Tang", "Yuze Wang", "Haojie Li"], "title": "Seg-Wild: Interactive Segmentation based on 3D Gaussian Splatting for Unconstrained Image Collections", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing and segmenting scenes from unconstrained photo collections obtained from the Internet is a novel but challenging task. Unconstrained photo collections are easier to get than well-captured photo collections. These unconstrained images suffer from inconsistent lighting and transient occlusions, which makes segmentation challenging. Previous segmentation methods cannot address transient occlusions or accurately restore the scene's lighting conditions. Therefore, we propose Seg-Wild, an interactive segmentation method based on 3D Gaussian Splatting for unconstrained image collections, suitable for in-the-wild scenes. We integrate multi-dimensional feature embeddings for each 3D Gaussian and calculate the feature similarity between the feature embeddings and the segmentation target to achieve interactive segmentation in the 3D scene. Additionally, we introduce the Spiky 3D Gaussian Cutter (SGC) to smooth abnormal 3D Gaussians. We project the 3D Gaussians onto a 2D plane and calculate the ratio of 3D Gaussians that need to be cut using the SAM mask. We also designed a benchmark to evaluate segmentation quality in in-the-wild scenes. Experimental results demonstrate that compared to previous methods, Seg-Wild achieves better segmentation results and reconstruction quality. Our code will be available at https://github.com/Sugar0725/Seg-Wild.", "AI": {"tldr": "Seg-Wild\u662f\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u4ea4\u4e92\u5f0f\u5206\u5272\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u65e0\u7ea6\u675f\u56fe\u50cf\u96c6\u5408\uff0c\u89e3\u51b3\u4e86\u5149\u7167\u4e0d\u4e00\u81f4\u548c\u77ac\u6001\u906e\u6321\u95ee\u9898\u3002", "motivation": "\u65e0\u7ea6\u675f\u7167\u7247\u96c6\u5408\u6613\u4e8e\u83b7\u53d6\u4f46\u5b58\u5728\u5149\u7167\u4e0d\u4e00\u81f4\u548c\u77ac\u6001\u906e\u6321\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u3002", "method": "\u7ed3\u5408\u591a\u7ef4\u7279\u5f81\u5d4c\u5165\u548c3D\u9ad8\u65af\u76f8\u4f3c\u6027\u8ba1\u7b97\uff0c\u5f15\u5165Spiky 3D Gaussian Cutter\u5e73\u6ed1\u5f02\u5e38\u9ad8\u65af\uff0c\u5e76\u901a\u8fc7SAM\u63a9\u6a21\u8ba1\u7b97\u5207\u5272\u6bd4\u4f8b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSeg-Wild\u5728\u5206\u5272\u548c\u91cd\u5efa\u8d28\u91cf\u4e0a\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "Seg-Wild\u4e3a\u65e0\u7ea6\u675f\u573a\u666f\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5206\u5272\u548c\u91cd\u5efa\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07410", "pdf": "https://arxiv.org/pdf/2507.07410", "abs": "https://arxiv.org/abs/2507.07410", "authors": ["Xinan Zhang", "Muhammad Zubair Irshad", "Anthony Yezzi", "Yi-Chang Tsai", "Zsolt Kira"], "title": "EscherNet++: Simultaneous Amodal Completion and Scalable View Synthesis through Masked Fine-Tuning and Enhanced Feed-Forward 3D Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "We propose EscherNet++, a masked fine-tuned diffusion model that can synthesize novel views of objects in a zero-shot manner with amodal completion ability. Existing approaches utilize multiple stages and complex pipelines to first hallucinate missing parts of the image and then perform novel view synthesis, which fail to consider cross-view dependencies and require redundant storage and computing for separate stages. Instead, we apply masked fine-tuning including input-level and feature-level masking to enable an end-to-end model with the improved ability to synthesize novel views and conduct amodal completion. In addition, we empirically integrate our model with other feed-forward image-to-mesh models without extra training and achieve competitive results with reconstruction time decreased by 95%, thanks to its ability to synthesize arbitrary query views. Our method's scalable nature further enhances fast 3D reconstruction. Despite fine-tuning on a smaller dataset and batch size, our method achieves state-of-the-art results, improving PSNR by 3.9 and Volume IoU by 0.28 on occluded tasks in 10-input settings, while also generalizing to real-world occluded reconstruction.", "AI": {"tldr": "EscherNet++\u662f\u4e00\u79cd\u63a9\u7801\u5fae\u8c03\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u96f6\u6837\u672c\u5408\u6210\u7269\u4f53\u7684\u65b0\u89c6\u89d2\u5e76\u5177\u5907\u975e\u6a21\u6001\u8865\u5168\u80fd\u529b\u3002\u76f8\u6bd4\u591a\u9636\u6bb5\u590d\u6742\u6d41\u7a0b\u7684\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u7aef\u5230\u7aef\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u65b0\u89c6\u89d2\u5408\u6210\u548c\u975e\u6a21\u6001\u8865\u5168\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u91c7\u7528\u591a\u9636\u6bb5\u6d41\u7a0b\uff0c\u5ffd\u7565\u4e86\u8de8\u89c6\u89d2\u4f9d\u8d56\u4e14\u8ba1\u7b97\u5197\u4f59\u3002EscherNet++\u65e8\u5728\u901a\u8fc7\u63a9\u7801\u5fae\u8c03\u5b9e\u73b0\u9ad8\u6548\u3001\u7aef\u5230\u7aef\u7684\u65b0\u89c6\u89d2\u5408\u6210\u548c\u975e\u6a21\u6001\u8865\u5168\u3002", "method": "\u91c7\u7528\u8f93\u5165\u7ea7\u548c\u7279\u5f81\u7ea7\u63a9\u7801\u5fae\u8c03\uff0c\u6784\u5efa\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u5e76\u4e0e\u5176\u4ed6\u524d\u9988\u56fe\u50cf\u5230\u7f51\u683c\u6a21\u578b\u7ed3\u5408\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5728\u8f83\u5c0f\u6570\u636e\u96c6\u548c\u6279\u91cf\u4e0b\uff0cPSNR\u63d0\u53473.9\uff0cVolume IoU\u63d0\u53470.28\uff0c\u91cd\u5efa\u65f6\u95f4\u51cf\u5c1195%\uff0c\u4e14\u9002\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u906e\u6321\u91cd\u5efa\u3002", "conclusion": "EscherNet++\u901a\u8fc7\u63a9\u7801\u5fae\u8c03\u548c\u7aef\u5230\u7aef\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u65b0\u89c6\u89d2\u5408\u6210\u548c\u975e\u6a21\u6001\u8865\u5168\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.07464", "pdf": "https://arxiv.org/pdf/2507.07464", "abs": "https://arxiv.org/abs/2507.07464", "authors": ["Chang-Hwan Son"], "title": "Degradation-Agnostic Statistical Facial Feature Transformation for Blind Face Restoration in Adverse Weather Conditions", "categories": ["cs.CV"], "comment": null, "summary": "With the increasing deployment of intelligent CCTV systems in outdoor environments, there is a growing demand for face recognition systems optimized for challenging weather conditions. Adverse weather significantly degrades image quality, which in turn reduces recognition accuracy. Although recent face image restoration (FIR) models based on generative adversarial networks (GANs) and diffusion models have shown progress, their performance remains limited due to the lack of dedicated modules that explicitly address weather-induced degradations. This leads to distorted facial textures and structures. To address these limitations, we propose a novel GAN-based blind FIR framework that integrates two key components: local Statistical Facial Feature Transformation (SFFT) and Degradation-Agnostic Feature Embedding (DAFE). The local SFFT module enhances facial structure and color fidelity by aligning the local statistical distributions of low-quality (LQ) facial regions with those of high-quality (HQ) counterparts. Complementarily, the DAFE module enables robust statistical facial feature extraction under adverse weather conditions by aligning LQ and HQ encoder representations, thereby making the restoration process adaptive to severe weather-induced degradations. Experimental results demonstrate that the proposed degradation-agnostic SFFT model outperforms existing state-of-the-art FIR methods based on GAN and diffusion models, particularly in suppressing texture distortions and accurately reconstructing facial structures. Furthermore, both the SFFT and DAFE modules are empirically validated in enhancing structural fidelity and perceptual quality in face restoration under challenging weather scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGAN\u7684\u76f2\u4eba\u8138\u56fe\u50cf\u6062\u590d\u6846\u67b6\uff0c\u7ed3\u5408\u5c40\u90e8\u7edf\u8ba1\u7279\u5f81\u53d8\u6362\u548c\u9000\u5316\u65e0\u5173\u7279\u5f81\u5d4c\u5165\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u4eba\u8138\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u4eba\u8138\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\uff0c\u5bfc\u81f4\u8bc6\u522b\u7cbe\u5ea6\u964d\u4f4e\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u9488\u5bf9\u6027\u6a21\u5757\u800c\u8868\u73b0\u6709\u9650\u3002", "method": "\u63d0\u51fa\u5c40\u90e8\u7edf\u8ba1\u7279\u5f81\u53d8\u6362\uff08SFFT\uff09\u548c\u9000\u5316\u65e0\u5173\u7279\u5f81\u5d4c\u5165\uff08DAFE\uff09\u6a21\u5757\uff0c\u5206\u522b\u589e\u5f3a\u5c40\u90e8\u7edf\u8ba1\u5206\u5e03\u5bf9\u9f50\u548c\u7279\u5f81\u63d0\u53d6\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u6291\u5236\u7eb9\u7406\u5931\u771f\u548c\u91cd\u5efa\u9762\u90e8\u7ed3\u6784\u65b9\u9762\u4f18\u4e8e\u73b0\u6709GAN\u548c\u6269\u6563\u6a21\u578b\u65b9\u6cd5\u3002", "conclusion": "SFFT\u548cDAFE\u6a21\u5757\u6709\u6548\u63d0\u5347\u4e86\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u4eba\u8138\u6062\u590d\u6548\u679c\uff0c\u5177\u6709\u663e\u8457\u7684\u7ed3\u6784\u4fdd\u771f\u5ea6\u548c\u611f\u77e5\u8d28\u91cf\u63d0\u5347\u3002"}}
{"id": "2507.07510", "pdf": "https://arxiv.org/pdf/2507.07510", "abs": "https://arxiv.org/abs/2507.07510", "authors": ["Binxu Li", "Minkai Xu", "Meihua Dang", "Stefano Ermon"], "title": "Divergence Minimization Preference Optimization for Diffusion Model Alignment", "categories": ["cs.CV", "cs.LG"], "comment": "24 pages, 8 figures", "summary": "Diffusion models have achieved remarkable success in generating realistic and versatile images from text prompts. Inspired by the recent advancements of language models, there is an increasing interest in further improving the models by aligning with human preferences. However, we investigate alignment from a divergence minimization perspective and reveal that existing preference optimization methods are typically trapped in suboptimal mean-seeking optimization. In this paper, we introduce Divergence Minimization Preference Optimization (DMPO), a novel and principled method for aligning diffusion models by minimizing reverse KL divergence, which asymptotically enjoys the same optimization direction as original RL. We provide rigorous analysis to justify the effectiveness of DMPO and conduct comprehensive experiments to validate its empirical strength across both human evaluations and automatic metrics. Our extensive results show that diffusion models fine-tuned with DMPO can consistently outperform or match existing techniques, specifically outperforming all existing diffusion alignment baselines by at least 64.6% in PickScore across all evaluation datasets, demonstrating the method's superiority in aligning generative behavior with desired outputs. Overall, DMPO unlocks a robust and elegant pathway for preference alignment, bridging principled theory with practical performance in diffusion models.", "AI": {"tldr": "DMPO\u662f\u4e00\u79cd\u901a\u8fc7\u6700\u5c0f\u5316\u53cd\u5411KL\u6563\u5ea6\u6765\u5bf9\u9f50\u6269\u6563\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u5b58\u5728\u6b21\u4f18\u5747\u503c\u5bfb\u6c42\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDMPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u53cd\u5411KL\u6563\u5ea6\u5bf9\u9f50\u6269\u6563\u6a21\u578b\u3002", "result": "DMPO\u5728\u4eba\u7c7b\u8bc4\u4f30\u548c\u81ea\u52a8\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cPickScore\u63d0\u534764.6%\u3002", "conclusion": "DMPO\u4e3a\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u7406\u8bba\u4e25\u8c28\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u504f\u597d\u5bf9\u9f50\u9014\u5f84\u3002"}}
{"id": "2507.07519", "pdf": "https://arxiv.org/pdf/2507.07519", "abs": "https://arxiv.org/abs/2507.07519", "authors": ["Bangning Wei", "Joshua Maraval", "Meriem Outtas", "Kidiyo Kpalma", "Nicolas Ramin", "Lu Zhang"], "title": "MUVOD: A Novel Multi-view Video Object Segmentation Dataset and A Benchmark for 3D Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "The application of methods based on Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3D GS) have steadily gained popularity in the field of 3D object segmentation in static scenes. These approaches demonstrate efficacy in a range of 3D scene understanding and editing tasks. Nevertheless, the 4D object segmentation of dynamic scenes remains an underexplored field due to the absence of a sufficiently extensive and accurately labelled multi-view video dataset. In this paper, we present MUVOD, a new multi-view video dataset for training and evaluating object segmentation in reconstructed real-world scenarios. The 17 selected scenes, describing various indoor or outdoor activities, are collected from different sources of datasets originating from various types of camera rigs. Each scene contains a minimum of 9 views and a maximum of 46 views. We provide 7830 RGB images (30 frames per video) with their corresponding segmentation mask in 4D motion, meaning that any object of interest in the scene could be tracked across temporal frames of a given view or across different views belonging to the same camera rig. This dataset, which contains 459 instances of 73 categories, is intended as a basic benchmark for the evaluation of multi-view video segmentation methods. We also present an evaluation metric and a baseline segmentation approach to encourage and evaluate progress in this evolving field. Additionally, we propose a new benchmark for 3D object segmentation task with a subset of annotated multi-view images selected from our MUVOD dataset. This subset contains 50 objects of different conditions in different scenarios, providing a more comprehensive analysis of state-of-the-art 3D object segmentation methods. Our proposed MUVOD dataset is available at https://volumetric-repository.labs.b-com.com/#/muvod.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MUVOD\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u52a8\u6001\u573a\u666f\u76844D\u5bf9\u8c61\u5206\u5272\uff0c\u586b\u8865\u4e86\u591a\u89c6\u89d2\u89c6\u9891\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bc4\u4f30\u6307\u6807\u548c\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u52a8\u6001\u573a\u666f\u76844D\u5bf9\u8c61\u5206\u5272\u7814\u7a76\u4e0d\u8db3\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u5e7f\u6cdb\u4e14\u51c6\u786e\u6807\u6ce8\u7684\u591a\u89c6\u89d2\u89c6\u9891\u6570\u636e\u96c6\u3002", "method": "\u6536\u96c6\u4e8617\u4e2a\u573a\u666f\u7684\u591a\u89c6\u89d2\u89c6\u9891\u6570\u636e\uff0c\u63d0\u4f9b\u4e867830\u5f20RGB\u56fe\u50cf\u53ca\u5176\u5206\u5272\u63a9\u7801\uff0c\u5e76\u63d0\u51fa\u8bc4\u4f30\u6307\u6807\u548c\u57fa\u7ebf\u65b9\u6cd5\u3002", "result": "MUVOD\u6570\u636e\u96c6\u5305\u542b459\u4e2a\u5b9e\u4f8b\u548c73\u4e2a\u7c7b\u522b\uff0c\u652f\u6301\u8de8\u65f6\u95f4\u548c\u89c6\u89d2\u7684\u5bf9\u8c61\u8ddf\u8e2a\uff0c\u5e76\u63d0\u4f9b\u4e863D\u5bf9\u8c61\u5206\u5272\u7684\u57fa\u51c6\u5b50\u96c6\u3002", "conclusion": "MUVOD\u6570\u636e\u96c6\u4e3a\u52a8\u6001\u573a\u666f\u5206\u5272\u63d0\u4f9b\u4e86\u57fa\u7840\u57fa\u51c6\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u3002"}}
{"id": "2507.07578", "pdf": "https://arxiv.org/pdf/2507.07578", "abs": "https://arxiv.org/abs/2507.07578", "authors": ["Chunyan Wang", "Dong Zhang", "Jinhui Tang"], "title": "Diffusion-Guided Knowledge Distillation for Weakly-Supervised Low-Light Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Weakly-supervised semantic segmentation aims to assign category labels to each pixel using weak annotations, significantly reducing manual annotation costs. Although existing methods have achieved remarkable progress in well-lit scenarios, their performance significantly degrades in low-light environments due to two fundamental limitations: severe image quality degradation (e.g., low contrast, noise, and color distortion) and the inherent constraints of weak supervision. These factors collectively lead to unreliable class activation maps and semantically ambiguous pseudo-labels, ultimately compromising the model's ability to learn discriminative feature representations. To address these problems, we propose Diffusion-Guided Knowledge Distillation for Weakly-Supervised Low-light Semantic Segmentation (DGKD-WLSS), a novel framework that synergistically combines Diffusion-Guided Knowledge Distillation (DGKD) with Depth-Guided Feature Fusion (DGF2). DGKD aligns normal-light and low-light features via diffusion-based denoising and knowledge distillation, while DGF2 integrates depth maps as illumination-invariant geometric priors to enhance structural feature learning. Extensive experiments demonstrate the effectiveness of DGKD-WLSS, which achieves state-of-the-art performance in weakly supervised semantic segmentation tasks under low-light conditions. The source codes have been released at:https://github.com/ChunyanWang1/DGKD-WLSS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6269\u6563\u5f15\u5bfc\u77e5\u8bc6\u84b8\u998f\uff08DGKD\uff09\u548c\u6df1\u5ea6\u5f15\u5bfc\u7279\u5f81\u878d\u5408\uff08DGF2\uff09\u7684\u65b0\u6846\u67b6DGKD-WLSS\uff0c\u7528\u4e8e\u89e3\u51b3\u5f31\u76d1\u7763\u4f4e\u5149\u8bed\u4e49\u5206\u5272\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4f4e\u5149\u73af\u5883\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4e3b\u8981\u7531\u4e8e\u56fe\u50cf\u8d28\u91cf\u9000\u5316\uff08\u5982\u4f4e\u5bf9\u6bd4\u5ea6\u3001\u566a\u58f0\u548c\u989c\u8272\u5931\u771f\uff09\u548c\u5f31\u76d1\u7763\u7684\u56fa\u6709\u9650\u5236\uff0c\u5bfc\u81f4\u4e0d\u53ef\u9760\u7684\u7c7b\u6fc0\u6d3b\u56fe\u548c\u8bed\u4e49\u6a21\u7cca\u7684\u4f2a\u6807\u7b7e\u3002", "method": "DGKD-WLSS\u901a\u8fc7\u6269\u6563\u5f15\u5bfc\u77e5\u8bc6\u84b8\u998f\uff08DGKD\uff09\u5bf9\u9f50\u6b63\u5e38\u5149\u548c\u4f4e\u5149\u7279\u5f81\uff0c\u540c\u65f6\u5229\u7528\u6df1\u5ea6\u5f15\u5bfc\u7279\u5f81\u878d\u5408\uff08DGF2\uff09\u6574\u5408\u6df1\u5ea6\u56fe\u4f5c\u4e3a\u5149\u7167\u4e0d\u53d8\u7684\u51e0\u4f55\u5148\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDGKD-WLSS\u5728\u4f4e\u5149\u6761\u4ef6\u4e0b\u7684\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "DGKD-WLSS\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u5149\u73af\u5883\u4e0b\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u7684\u6311\u6218\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2507.07591", "pdf": "https://arxiv.org/pdf/2507.07591", "abs": "https://arxiv.org/abs/2507.07591", "authors": ["Kuiyuan Sun", "Yuxuan Zhang", "Jichao Zhang", "Jiaming Liu", "Wei Wang", "Niculae Sebe", "Yao Zhao"], "title": "Stable-Hair v2: Real-World Hair Transfer via Multiple-View Diffusion Model", "categories": ["cs.CV"], "comment": "14 pages", "summary": "While diffusion-based methods have shown impressive capabilities in capturing diverse and complex hairstyles, their ability to generate consistent and high-quality multi-view outputs -- crucial for real-world applications such as digital humans and virtual avatars -- remains underexplored. In this paper, we propose Stable-Hair v2, a novel diffusion-based multi-view hair transfer framework. To the best of our knowledge, this is the first work to leverage multi-view diffusion models for robust, high-fidelity, and view-consistent hair transfer across multiple perspectives. We introduce a comprehensive multi-view training data generation pipeline comprising a diffusion-based Bald Converter, a data-augment inpainting model, and a face-finetuned multi-view diffusion model to generate high-quality triplet data, including bald images, reference hairstyles, and view-aligned source-bald pairs. Our multi-view hair transfer model integrates polar-azimuth embeddings for pose conditioning and temporal attention layers to ensure smooth transitions between views. To optimize this model, we design a novel multi-stage training strategy consisting of pose-controllable latent IdentityNet training, hair extractor training, and temporal attention training. Extensive experiments demonstrate that our method accurately transfers detailed and realistic hairstyles to source subjects while achieving seamless and consistent results across views, significantly outperforming existing methods and establishing a new benchmark in multi-view hair transfer. Code is publicly available at https://github.com/sunkymepro/StableHairV2.", "AI": {"tldr": "Stable-Hair v2\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u591a\u89c6\u89d2\u5934\u53d1\u8f6c\u79fb\u6846\u67b6\uff0c\u9996\u6b21\u5229\u7528\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u89c6\u89d2\u4e00\u81f4\u7684\u5934\u53d1\u8f6c\u79fb\u3002", "motivation": "\u6269\u6563\u65b9\u6cd5\u5728\u6355\u6349\u591a\u6837\u590d\u6742\u53d1\u578b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u751f\u6210\u4e00\u81f4\u9ad8\u8d28\u91cf\u591a\u89c6\u89d2\u8f93\u51fa\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\uff0c\u8fd9\u5bf9\u6570\u5b57\u4eba\u7c7b\u548c\u865a\u62df\u5316\u8eab\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u591a\u89c6\u89d2\u8bad\u7ec3\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u5305\u62ec\u6269\u6563\u57fa\u7684Bald Converter\u3001\u6570\u636e\u589e\u5f3a\u4fee\u590d\u6a21\u578b\u548c\u9762\u90e8\u5fae\u8c03\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\u3002\u6a21\u578b\u6574\u5408\u6781\u5750\u6807\u5d4c\u5165\u548c\u65f6\u5e8f\u6ce8\u610f\u529b\u5c42\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u51c6\u786e\u8f6c\u79fb\u7ec6\u8282\u4e30\u5bcc\u7684\u53d1\u578b\uff0c\u5e76\u5728\u591a\u89c6\u89d2\u95f4\u5b9e\u73b0\u65e0\u7f1d\u4e00\u81f4\u6548\u679c\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Stable-Hair v2\u5728\u591a\u89c6\u89d2\u5934\u53d1\u8f6c\u79fb\u9886\u57df\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.07605", "pdf": "https://arxiv.org/pdf/2507.07605", "abs": "https://arxiv.org/abs/2507.07605", "authors": ["Nermin Samet", "Gilles Puy", "Renaud Marlet"], "title": "LOSC: LiDAR Open-voc Segmentation Consolidator", "categories": ["cs.CV"], "comment": null, "summary": "We study the use of image-based Vision-Language Models (VLMs) for open-vocabulary segmentation of lidar scans in driving settings. Classically, image semantics can be back-projected onto 3D point clouds. Yet, resulting point labels are noisy and sparse. We consolidate these labels to enforce both spatio-temporal consistency and robustness to image-level augmentations. We then train a 3D network based on these refined labels. This simple method, called LOSC, outperforms the SOTA of zero-shot open-vocabulary semantic and panoptic segmentation on both nuScenes and SemanticKITTI, with significant margins.", "AI": {"tldr": "LOSC\u65b9\u6cd5\u901a\u8fc7\u6574\u5408\u56fe\u50cf\u8bed\u4e49\u6807\u7b7e\u5e76\u8bad\u7ec33D\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u548c\u5168\u666f\u5206\u5272\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6fc0\u5149\u96f7\u8fbe\u626b\u63cf\u4e2d\u56fe\u50cf\u8bed\u4e49\u6807\u7b7e\u566a\u58f0\u5927\u4e14\u7a00\u758f\u7684\u95ee\u9898\u3002", "method": "\u6574\u5408\u6807\u7b7e\u4ee5\u786e\u4fdd\u65f6\u7a7a\u4e00\u81f4\u6027\uff0c\u5e76\u5bf9\u56fe\u50cf\u7ea7\u589e\u5f3a\u5177\u6709\u9c81\u68d2\u6027\uff0c\u968f\u540e\u8bad\u7ec33D\u7f51\u7edc\u3002", "result": "\u5728nuScenes\u548cSemanticKITTI\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u96f6\u6837\u672c\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u65b9\u6cd5\u3002", "conclusion": "LOSC\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6fc0\u5149\u96f7\u8fbe\u626b\u63cf\u7684\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2507.07633", "pdf": "https://arxiv.org/pdf/2507.07633", "abs": "https://arxiv.org/abs/2507.07633", "authors": ["Zhitao Wang", "Hengyu Man", "Wenrui Li", "Xingtao Wang", "Xiaopeng Fan", "Debin Zhao"], "title": "T-GVC: Trajectory-Guided Generative Video Coding at Ultra-Low Bitrates", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Recent advances in video generation techniques have given rise to an emerging paradigm of generative video coding, aiming to achieve semantically accurate reconstructions in Ultra-Low Bitrate (ULB) scenarios by leveraging strong generative priors. However, most existing methods are limited by domain specificity (e.g., facial or human videos) or an excessive dependence on high-level text guidance, which often fails to capture motion details and results in unrealistic reconstructions. To address these challenges, we propose a Trajectory-Guided Generative Video Coding framework (dubbed T-GVC). T-GVC employs a semantic-aware sparse motion sampling pipeline to effectively bridge low-level motion tracking with high-level semantic understanding by extracting pixel-wise motion as sparse trajectory points based on their semantic importance, not only significantly reducing the bitrate but also preserving critical temporal semantic information. In addition, by incorporating trajectory-aligned loss constraints into diffusion processes, we introduce a training-free latent space guidance mechanism to ensure physically plausible motion patterns without sacrificing the inherent capabilities of generative models. Experimental results demonstrate that our framework outperforms both traditional codecs and state-of-the-art end-to-end video compression methods under ULB conditions. Furthermore, additional experiments confirm that our approach achieves more precise motion control than existing text-guided methods, paving the way for a novel direction of generative video coding guided by geometric motion modeling.", "AI": {"tldr": "T-GVC\u6846\u67b6\u901a\u8fc7\u8f68\u8ff9\u5f15\u5bfc\u751f\u6210\u89c6\u9891\u7f16\u7801\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8d85\u4f4e\u6bd4\u7279\u7387\u4e0b\u8bed\u4e49\u91cd\u5efa\u7684\u5c40\u9650\u6027\uff0c\u7ed3\u5408\u8bed\u4e49\u611f\u77e5\u7a00\u758f\u8fd0\u52a8\u91c7\u6837\u548c\u6269\u6563\u8fc7\u7a0b\u7ea6\u675f\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u8fd0\u52a8\u63a7\u5236\u548c\u903c\u771f\u7684\u91cd\u5efa\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u5728\u8d85\u4f4e\u6bd4\u7279\u7387\u573a\u666f\u4e0b\u53d7\u9650\u4e8e\u9886\u57df\u7279\u5f02\u6027\u6216\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u5f15\u5bfc\uff0c\u5bfc\u81f4\u8fd0\u52a8\u7ec6\u8282\u4e22\u5931\u548c\u91cd\u5efa\u4e0d\u771f\u5b9e\u3002", "method": "\u63d0\u51faT-GVC\u6846\u67b6\uff0c\u91c7\u7528\u8bed\u4e49\u611f\u77e5\u7a00\u758f\u8fd0\u52a8\u91c7\u6837\u7ba1\u9053\uff0c\u7ed3\u5408\u8f68\u8ff9\u5bf9\u9f50\u635f\u5931\u7ea6\u675f\u548c\u6269\u6563\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4f4e\u6bd4\u7279\u7387\u4e0b\u7684\u9ad8\u6548\u89c6\u9891\u7f16\u7801\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cT-GVC\u5728\u8d85\u4f4e\u6bd4\u7279\u7387\u4e0b\u4f18\u4e8e\u4f20\u7edf\u7f16\u89e3\u7801\u5668\u548c\u7aef\u5230\u7aef\u89c6\u9891\u538b\u7f29\u65b9\u6cd5\uff0c\u4e14\u8fd0\u52a8\u63a7\u5236\u66f4\u7cbe\u786e\u3002", "conclusion": "T-GVC\u4e3a\u57fa\u4e8e\u51e0\u4f55\u8fd0\u52a8\u5efa\u6a21\u7684\u751f\u6210\u89c6\u9891\u7f16\u7801\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.07704", "pdf": "https://arxiv.org/pdf/2507.07704", "abs": "https://arxiv.org/abs/2507.07704", "authors": ["Bardia Hejazi", "Keerthana Chand", "Tobias Fritsch", "Giovanni Bruno"], "title": "D-CNN and VQ-VAE Autoencoders for Compression and Denoising of Industrial X-ray Computed Tomography Images", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The ever-growing volume of data in imaging sciences stemming from the advancements in imaging technologies, necessitates efficient and reliable storage solutions for such large datasets. This study investigates the compression of industrial X-ray computed tomography (XCT) data using deep learning autoencoders and examines how these compression algorithms affect the quality of the recovered data. Two network architectures with different compression rates were used, a deep convolution neural network (D-CNN) and a vector quantized variational autoencoder (VQ-VAE). The XCT data used was from a sandstone sample with a complex internal pore network. The quality of the decoded images obtained from the two different deep learning architectures with different compression rates were quantified and compared to the original input data. In addition, to improve image decoding quality metrics, we introduced a metric sensitive to edge preservation, which is crucial for three-dimensional data analysis. We showed that different architectures and compression rates are required depending on the specific characteristics needed to be preserved for later analysis. The findings presented here can aid scientists to determine the requirements and strategies for their data storage and analysis needs.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u81ea\u52a8\u7f16\u7801\u5668\u538b\u7f29\u5de5\u4e1aX\u5c04\u7ebf\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\uff08XCT\uff09\u6570\u636e\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u538b\u7f29\u7b97\u6cd5\u5bf9\u6570\u636e\u6062\u590d\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u6210\u50cf\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u6210\u50cf\u79d1\u5b66\u4e2d\u7684\u6570\u636e\u91cf\u6025\u5267\u589e\u957f\uff0c\u9700\u8981\u9ad8\u6548\u53ef\u9760\u7684\u5b58\u50a8\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u7f51\u7edc\u67b6\u6784\uff08D-CNN\u548cVQ-VAE\uff09\u4ee5\u4e0d\u540c\u538b\u7f29\u7387\u5bf9XCT\u6570\u636e\u8fdb\u884c\u538b\u7f29\uff0c\u5e76\u5f15\u5165\u5bf9\u8fb9\u7f18\u4fdd\u62a4\u654f\u611f\u7684\u6307\u6807\u6765\u8bc4\u4f30\u89e3\u7801\u56fe\u50cf\u8d28\u91cf\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u4e0d\u540c\u67b6\u6784\u548c\u538b\u7f29\u7387\u9002\u7528\u4e8e\u9700\u8981\u4fdd\u7559\u7279\u5b9a\u7279\u5f81\u7684\u6570\u636e\u5206\u6790\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u79d1\u5b66\u5bb6\u786e\u5b9a\u6570\u636e\u5b58\u50a8\u548c\u5206\u6790\u9700\u6c42\u63d0\u4f9b\u4e86\u7b56\u7565\u6307\u5bfc\u3002"}}
{"id": "2507.07708", "pdf": "https://arxiv.org/pdf/2507.07708", "abs": "https://arxiv.org/abs/2507.07708", "authors": ["Wei Shang", "Dongwei Ren", "Wanying Zhang", "Pengfei Zhu", "Qinghua Hu", "Wangmeng Zuo"], "title": "Motion-Aware Adaptive Pixel Pruning for Efficient Local Motion Deblurring", "categories": ["cs.CV", "I.4.3"], "comment": "Accepted by ACMMM 2025", "summary": "Local motion blur in digital images originates from the relative motion between dynamic objects and static imaging systems during exposure. Existing deblurring methods face significant challenges in addressing this problem due to their inefficient allocation of computational resources and inadequate handling of spatially varying blur patterns. To overcome these limitations, we first propose a trainable mask predictor that identifies blurred regions in the image. During training, we employ blur masks to exclude sharp regions. For inference optimization, we implement structural reparameterization by converting $3\\times 3$ convolutions to computationally efficient $1\\times 1$ convolutions, enabling pixel-level pruning of sharp areas to reduce computation. Second, we develop an intra-frame motion analyzer that translates relative pixel displacements into motion trajectories, establishing adaptive guidance for region-specific blur restoration. Our method is trained end-to-end using a combination of reconstruction loss, reblur loss, and mask loss guided by annotated blur masks. Extensive experiments demonstrate superior performance over state-of-the-art methods on both local and global blur datasets while reducing FLOPs by 49\\% compared to SOTA models (e.g., LMD-ViT). The source code is available at https://github.com/shangwei5/M2AENet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5c40\u90e8\u8fd0\u52a8\u6a21\u7cca\u53bb\u9664\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u8bad\u7ec3\u63a9\u7801\u9884\u6d4b\u5668\u548c\u5e27\u5185\u8fd0\u52a8\u5206\u6790\u5668\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u548c\u6a21\u7cca\u6a21\u5f0f\u5904\u7406\u3002", "motivation": "\u73b0\u6709\u53bb\u6a21\u7cca\u65b9\u6cd5\u5728\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u548c\u7a7a\u95f4\u53d8\u5316\u6a21\u7cca\u6a21\u5f0f\u5904\u7406\u4e0a\u6548\u7387\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u4f7f\u7528\u53ef\u8bad\u7ec3\u63a9\u7801\u9884\u6d4b\u5668\u8bc6\u522b\u6a21\u7cca\u533a\u57df\uff0c\u5e76\u901a\u8fc7\u7ed3\u6784\u91cd\u53c2\u6570\u5316\u4f18\u5316\u8ba1\u7b97\uff1b\u5f00\u53d1\u5e27\u5185\u8fd0\u52a8\u5206\u6790\u5668\u751f\u6210\u8fd0\u52a8\u8f68\u8ff9\u6307\u5bfc\u6a21\u7cca\u6062\u590d\u3002", "result": "\u5728\u5c40\u90e8\u548c\u5168\u5c40\u6a21\u7cca\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8ba1\u7b97\u91cf\u51cf\u5c1149%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\uff0c\u4e3a\u5c40\u90e8\u8fd0\u52a8\u6a21\u7cca\u53bb\u9664\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.07828", "pdf": "https://arxiv.org/pdf/2507.07828", "abs": "https://arxiv.org/abs/2507.07828", "authors": ["Richard Dirauf", "Florian Wolz", "Dario Zanca", "Bj\u00f6rn Eskofier"], "title": "Benchmarking Content-Based Puzzle Solvers on Corrupted Jigsaw Puzzles", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at ICIAP 2025", "summary": "Content-based puzzle solvers have been extensively studied, demonstrating significant progress in computational techniques. However, their evaluation often lacks realistic challenges crucial for real-world applications, such as the reassembly of fragmented artefacts or shredded documents. In this work, we investigate the robustness of State-Of-The-Art content-based puzzle solvers introducing three types of jigsaw puzzle corruptions: missing pieces, eroded edges, and eroded contents. Evaluating both heuristic and deep learning-based solvers, we analyse their ability to handle these corruptions and identify key limitations. Our results show that solvers developed for standard puzzles have a rapid decline in performance if more pieces are corrupted. However, deep learning models can significantly improve their robustness through fine-tuning with augmented data. Notably, the advanced Positional Diffusion model adapts particularly well, outperforming its competitors in most experiments. Based on our findings, we highlight promising research directions for enhancing the automated reconstruction of real-world artefacts.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5185\u5bb9\u62fc\u56fe\u6c42\u89e3\u5668\u5728\u7f3a\u5931\u3001\u8fb9\u7f18\u8150\u8680\u548c\u5185\u5bb9\u8150\u8680\u4e09\u79cd\u635f\u574f\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u6807\u51c6\u6c42\u89e3\u5668\u6027\u80fd\u4e0b\u964d\u660e\u663e\uff0c\u4f46\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u53ef\u663e\u8457\u63d0\u5347\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u5185\u5bb9\u62fc\u56fe\u6c42\u89e3\u5668\u7684\u8bc4\u4f30\u7f3a\u4e4f\u5bf9\u73b0\u5b9e\u6311\u6218\uff08\u5982\u6587\u7269\u6216\u6587\u4ef6\u788e\u7247\u91cd\u7ec4\uff09\u7684\u8003\u91cf\uff0c\u56e0\u6b64\u7814\u7a76\u5176\u9c81\u68d2\u6027\u3002", "method": "\u5f15\u5165\u4e09\u79cd\u62fc\u56fe\u635f\u574f\u7c7b\u578b\uff0c\u8bc4\u4f30\u542f\u53d1\u5f0f\u548c\u6df1\u5ea6\u5b66\u4e60\u6c42\u89e3\u5668\u7684\u8868\u73b0\uff0c\u5206\u6790\u5176\u5c40\u9650\u6027\u3002", "result": "\u6807\u51c6\u6c42\u89e3\u5668\u6027\u80fd\u968f\u635f\u574f\u589e\u52a0\u5feb\u901f\u4e0b\u964d\uff0c\u4f46\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982Positional Diffusion\uff09\u901a\u8fc7\u5fae\u8c03\u663e\u8457\u63d0\u5347\u9c81\u68d2\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u63d0\u5347\u73b0\u5b9e\u6587\u7269\u81ea\u52a8\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.07878", "pdf": "https://arxiv.org/pdf/2507.07878", "abs": "https://arxiv.org/abs/2507.07878", "authors": ["Jiayi Wu", "Tianfu Wang", "Md Abu Bakr Siddique", "Md Jahidul Islam", "Cornelia Fermuller", "Yiannis Aloimonos", "Christopher A. Metzler"], "title": "Single-Step Latent Diffusion for Underwater Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Underwater image restoration algorithms seek to restore the color, contrast, and appearance of a scene that is imaged underwater. They are a critical tool in applications ranging from marine ecology and aquaculture to underwater construction and archaeology. While existing pixel-domain diffusion-based image restoration approaches are effective at restoring simple scenes with limited depth variation, they are computationally intensive and often generate unrealistic artifacts when applied to scenes with complex geometry and significant depth variation. In this work we overcome these limitations by combining a novel network architecture (SLURPP) with an accurate synthetic data generation pipeline. SLURPP combines pretrained latent diffusion models -- which encode strong priors on the geometry and depth of scenes -- with an explicit scene decomposition -- which allows one to model and account for the effects of light attenuation and backscattering. To train SLURPP we design a physics-based underwater image synthesis pipeline that applies varied and realistic underwater degradation effects to existing terrestrial image datasets. This approach enables the generation of diverse training data with dense medium/degradation annotations. We evaluate our method extensively on both synthetic and real-world benchmarks and demonstrate state-of-the-art performance. Notably, SLURPP is over 200X faster than existing diffusion-based methods while offering ~ 3 dB improvement in PSNR on synthetic benchmarks. It also offers compelling qualitative improvements on real-world data. Project website https://tianfwang.github.io/slurpp/.", "AI": {"tldr": "SLURPP\u662f\u4e00\u79cd\u65b0\u578b\u6c34\u4e0b\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u663e\u5f0f\u573a\u666f\u5206\u89e3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u573a\u666f\u4e0b\u7684\u6062\u590d\u6548\u679c\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u6c34\u4e0b\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u5728\u590d\u6742\u51e0\u4f55\u548c\u6df1\u5ea6\u53d8\u5316\u573a\u666f\u4e2d\u8ba1\u7b97\u91cf\u5927\u4e14\u6613\u4ea7\u751f\u4e0d\u771f\u5b9e\u4f2a\u5f71\uff0cSLURPP\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "SLURPP\u7ed3\u5408\u9884\u8bad\u7ec3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u663e\u5f0f\u573a\u666f\u5206\u89e3\uff0c\u5e76\u5229\u7528\u7269\u7406\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\u8bad\u7ec3\u6a21\u578b\u3002", "result": "SLURPP\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb200\u500d\uff0cPSNR\u63d0\u5347\u7ea63 dB\u3002", "conclusion": "SLURPP\u901a\u8fc7\u521b\u65b0\u67b6\u6784\u548c\u5408\u6210\u6570\u636e\u751f\u6210\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u6c34\u4e0b\u56fe\u50cf\u6062\u590d\u3002"}}
{"id": "2507.07908", "pdf": "https://arxiv.org/pdf/2507.07908", "abs": "https://arxiv.org/abs/2507.07908", "authors": ["Xiao Yang", "Yuxuan Fan", "Can Liu", "Houcheng Su", "Weichen Guo", "Jiyao Wang", "Dengbo He"], "title": "Not Only Consistency: Enhance Test-Time Adaptation with Spatio-temporal Inconsistency for Remote Physiological Measurement", "categories": ["cs.CV"], "comment": null, "summary": "Remote photoplethysmography (rPPG) has emerged as a promising non-invasive method for monitoring physiological signals using the camera. Although various domain adaptation and generalization methods were proposed to promote the adaptability of deep-based rPPG models in unseen deployment environments, considerations in aspects like privacy concerns and real-time adaptation restrict their application in real-world deployment. Thus, we aim to propose a novel fully Test-Time Adaptation (TTA) strategy tailored for rPPG tasks in this work. Specifically, based on prior knowledge in physiology and our observations, we noticed not only there is spatio-temporal consistency in the frequency domain of rPPG signals, but also that inconsistency in the time domain was significant. Given this, by leveraging both consistency and inconsistency priors, we introduce an innovative expert knowledge-based self-supervised \\textbf{C}onsistency-\\textbf{i}n\\textbf{C}onsistency-\\textbf{i}ntegration (\\textbf{CiCi}) framework to enhances model adaptation during inference. Besides, our approach further incorporates a gradient dynamic control mechanism to mitigate potential conflicts between priors, ensuring stable adaptation across instances. Through extensive experiments on five diverse datasets under the TTA protocol, our method consistently outperforms existing techniques, presenting state-of-the-art performance in real-time self-supervised adaptation without accessing source data. The code will be released later.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCiCi\u7684\u65b0\u578b\u6d4b\u8bd5\u65f6\u95f4\u81ea\u9002\u5e94\uff08TTA\uff09\u7b56\u7565\uff0c\u7528\u4e8e\u8fdc\u7a0b\u5149\u7535\u5bb9\u79ef\u63cf\u8bb0\uff08rPPG\uff09\u4efb\u52a1\uff0c\u7ed3\u5408\u751f\u7406\u5b66\u5148\u9a8c\u77e5\u8bc6\uff0c\u5229\u7528\u9891\u7387\u57df\u7684\u4e00\u81f4\u6027\u548c\u65f6\u95f4\u57df\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u63d0\u5347\u6a21\u578b\u5728\u63a8\u7406\u65f6\u7684\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6rPPG\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u90e8\u7f72\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u53d7\u9650\uff0c\u9690\u79c1\u95ee\u9898\u548c\u5b9e\u65f6\u9002\u5e94\u6027\u9700\u6c42\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u57fa\u4e8e\u751f\u7406\u5b66\u5148\u9a8c\u77e5\u8bc6\uff0c\u63d0\u51faCiCi\u6846\u67b6\uff0c\u7ed3\u5408\u9891\u7387\u57df\u4e00\u81f4\u6027\u548c\u65f6\u95f4\u57df\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u5f15\u5165\u68af\u5ea6\u52a8\u6001\u63a7\u5236\u673a\u5236\u4ee5\u7a33\u5b9a\u9002\u5e94\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9e\u65f6\u81ea\u76d1\u7763\u9002\u5e94\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u65e0\u9700\u8bbf\u95ee\u6e90\u6570\u636e\u3002", "conclusion": "CiCi\u6846\u67b6\u5728rPPG\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5b9e\u65f6\u81ea\u76d1\u7763\u9002\u5e94\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.07966", "pdf": "https://arxiv.org/pdf/2507.07966", "abs": "https://arxiv.org/abs/2507.07966", "authors": ["Yukang Chen", "Wei Huang", "Baifeng Shi", "Qinghao Hu", "Hanrong Ye", "Ligeng Zhu", "Zhijian Liu", "Pavlo Molchanov", "Jan Kautz", "Xiaojuan Qi", "Sifei Liu", "Hongxu Yin", "Yao Lu", "Song Han"], "title": "Scaling RL to Long Videos", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Code and models are available at https://github.com/NVlabs/Long-RL", "summary": "We introduce a full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical components: (1) a large-scale dataset, LongVideo-Reason, comprising 52K long video QA pairs with high-quality reasoning annotations across diverse domains such as sports, games, and vlogs; (2) a two-stage training pipeline that extends VLMs with chain-of-thought supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a training infrastructure for long video RL, named Multi-modal Reinforcement Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a vLLM-based engine tailored for long video, using cached video embeddings for efficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves strong performance on long video QA benchmarks such as VideoMME. It also outperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal reasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on our LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to 2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent performance gains as the number of input video frames scales. LongVILA-R1 marks a firm step towards long video reasoning in VLMs. In addition, we release our training system for public availability that supports RL training on various modalities (video, text, and audio), various models (VILA and Qwen series), and even image and video generation models. On a single A100 node (8 GPUs), it supports RL training on hour-long videos (e.g., 3,600 frames / around 256k tokens).", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5168\u6808\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6269\u5c55\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u957f\u89c6\u9891\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5305\u542b\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3001\u4e24\u9636\u6bb5\u8bad\u7ec3\u7ba1\u9053\u548c\u9ad8\u6548\u8bad\u7ec3\u57fa\u7840\u8bbe\u65bd\u3002", "motivation": "\u89e3\u51b3\u957f\u89c6\u9891\u63a8\u7406\u7684\u72ec\u7279\u6311\u6218\uff0c\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u957f\u89c6\u9891\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u6574\u5408\u5927\u89c4\u6a21\u6570\u636e\u96c6LongVideo-Reason\u3001\u4e24\u9636\u6bb5\u8bad\u7ec3\u7ba1\u9053\uff08CoT-SFT\u548c\u5f3a\u5316\u5b66\u4e60\uff09\u4ee5\u53ca\u9ad8\u6548\u8bad\u7ec3\u57fa\u7840\u8bbe\u65bdMR-SP\u3002", "result": "LongVILA-R1-7B\u5728\u957f\u89c6\u9891QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u8d85\u8d8aVideo-R1-7B\uff0c\u5339\u914dGemini-1.5-Pro\uff0c\u4e14MR-SP\u7cfb\u7edf\u8bad\u7ec3\u901f\u5ea6\u63d0\u53472.1\u500d\u3002", "conclusion": "LongVILA-R1\u4e3a\u957f\u89c6\u9891\u63a8\u7406\u8fc8\u51fa\u91cd\u8981\u4e00\u6b65\uff0c\u5e76\u516c\u5f00\u652f\u6301\u591a\u6a21\u6001\u548c\u591a\u6a21\u578b\u8bad\u7ec3\u7684\u7cfb\u7edf\u3002"}}
{"id": "2507.07978", "pdf": "https://arxiv.org/pdf/2507.07978", "abs": "https://arxiv.org/abs/2507.07978", "authors": ["Longfei Li", "Zhiwen Fan", "Wenyan Cong", "Xinhang Liu", "Yuyang Yin", "Matt Foutter", "Panwang Pan", "Chenyu You", "Yue Wang", "Zhangyang Wang", "Yao Zhao", "Marco Pavone", "Yunchao Wei"], "title": "Martian World Models: Controllable Video Synthesis with Physically Accurate 3D Reconstructions", "categories": ["cs.CV"], "comment": "Project Page: https://marsgenai.github.io", "summary": "Synthesizing realistic Martian landscape videos is crucial for mission rehearsal and robotic simulation. However, this task poses unique challenges due to the scarcity of high-quality Martian data and the significant domain gap between Martian and terrestrial imagery. To address these challenges, we propose a holistic solution composed of two key components: 1) A data curation pipeline Multimodal Mars Synthesis (M3arsSynth), which reconstructs 3D Martian environments from real stereo navigation images, sourced from NASA's Planetary Data System (PDS), and renders high-fidelity multiview 3D video sequences. 2) A Martian terrain video generator, MarsGen, which synthesizes novel videos visually realistic and geometrically consistent with the 3D structure encoded in the data. Our M3arsSynth engine spans a wide range of Martian terrains and acquisition dates, enabling the generation of physically accurate 3D surface models at metric-scale resolution. MarsGen, fine-tuned on M3arsSynth data, synthesizes videos conditioned on an initial image frame and, optionally, camera trajectories or textual prompts, allowing for video generation in novel environments. Experimental results show that our approach outperforms video synthesis models trained on terrestrial datasets, achieving superior visual fidelity and 3D structural consistency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5408\u6210\u706b\u661f\u666f\u89c2\u89c6\u9891\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u6570\u636e\u91cd\u5efa\u548c\u89c6\u9891\u751f\u6210\u4e24\u90e8\u5206\uff0c\u89e3\u51b3\u4e86\u706b\u661f\u6570\u636e\u7a00\u7f3a\u548c\u9886\u57df\u5dee\u5f02\u95ee\u9898\u3002", "motivation": "\u4e3a\u4efb\u52a1\u6f14\u7ec3\u548c\u673a\u5668\u4eba\u6a21\u62df\u63d0\u4f9b\u903c\u771f\u7684\u706b\u661f\u666f\u89c2\u89c6\u9891\uff0c\u4f46\u706b\u661f\u6570\u636e\u7a00\u7f3a\u4e14\u4e0e\u5730\u7403\u56fe\u50cf\u5dee\u5f02\u5927\u3002", "method": "1) M3arsSynth\uff1a\u4eceNASA\u7acb\u4f53\u56fe\u50cf\u91cd\u5efa3D\u706b\u661f\u73af\u5883\u5e76\u6e32\u67d3\u89c6\u9891\uff1b2) MarsGen\uff1a\u57fa\u4e8e3D\u7ed3\u6784\u751f\u6210\u903c\u771f\u89c6\u9891\u3002", "result": "\u65b9\u6cd5\u5728\u89c6\u89c9\u903c\u771f\u5ea6\u548c3D\u7ed3\u6784\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u5730\u7403\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u706b\u661f\u89c6\u9891\uff0c\u9002\u7528\u4e8e\u4efb\u52a1\u6a21\u62df\u548c\u673a\u5668\u4eba\u8bad\u7ec3\u3002"}}
{"id": "2507.07982", "pdf": "https://arxiv.org/pdf/2507.07982", "abs": "https://arxiv.org/abs/2507.07982", "authors": ["Haoyu Wu", "Diankun Wu", "Tianyu He", "Junliang Guo", "Yang Ye", "Yueqi Duan", "Jiang Bian"], "title": "Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages, project page: https://GeometryForcing.github.io", "summary": "Videos inherently represent 2D projections of a dynamic 3D world. However, our analysis suggests that video diffusion models trained solely on raw video data often fail to capture meaningful geometric-aware structure in their learned representations. To bridge this gap between video diffusion models and the underlying 3D nature of the physical world, we propose Geometry Forcing, a simple yet effective method that encourages video diffusion models to internalize latent 3D representations. Our key insight is to guide the model's intermediate representations toward geometry-aware structure by aligning them with features from a pretrained geometric foundation model. To this end, we introduce two complementary alignment objectives: Angular Alignment, which enforces directional consistency via cosine similarity, and Scale Alignment, which preserves scale-related information by regressing unnormalized geometric features from normalized diffusion representation. We evaluate Geometry Forcing on both camera view-conditioned and action-conditioned video generation tasks. Experimental results demonstrate that our method substantially improves visual quality and 3D consistency over the baseline methods. Project page: https://GeometryForcing.github.io.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGeometry Forcing\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u51e0\u4f55\u5bf9\u9f50\u76ee\u6807\u63d0\u5347\u89c6\u9891\u6269\u6563\u6a21\u578b\u5bf93D\u7ed3\u6784\u7684\u6355\u6349\u80fd\u529b\u3002", "motivation": "\u89c6\u9891\u6269\u6563\u6a21\u578b\u4ec5\u57fa\u4e8e\u539f\u59cb\u89c6\u9891\u6570\u636e\u8bad\u7ec3\u65f6\uff0c\u5f80\u5f80\u65e0\u6cd5\u6355\u6349\u6709\u610f\u4e49\u7684\u51e0\u4f55\u611f\u77e5\u7ed3\u6784\u3002\u672c\u6587\u65e8\u5728\u5f25\u5408\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e0e\u7269\u7406\u4e16\u754c3D\u672c\u8d28\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u63d0\u51faGeometry Forcing\u65b9\u6cd5\uff0c\u901a\u8fc7Angular Alignment\u548cScale Alignment\u4e24\u4e2a\u51e0\u4f55\u5bf9\u9f50\u76ee\u6807\uff0c\u5f15\u5bfc\u6a21\u578b\u5b66\u4e60\u51e0\u4f55\u611f\u77e5\u7684\u6f5c\u5728\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u76f8\u673a\u89c6\u89d2\u548c\u52a8\u4f5c\u6761\u4ef6\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8d28\u91cf\u548c3D\u4e00\u81f4\u6027\u3002", "conclusion": "Geometry Forcing\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u51e0\u4f55\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2507.07997", "pdf": "https://arxiv.org/pdf/2507.07997", "abs": "https://arxiv.org/abs/2507.07997", "authors": ["Mingkai Jia", "Wei Yin", "Xiaotao Hu", "Jiaxin Guo", "Xiaoyang Guo", "Qian Zhang", "Xiao-Xiao Long", "Ping Tan"], "title": "MGVQ: Could VQ-VAE Beat VAE? A Generalizable Tokenizer with Multi-group Quantization", "categories": ["cs.CV"], "comment": null, "summary": "Vector Quantized Variational Autoencoders (VQ-VAEs) are fundamental models that compress continuous visual data into discrete tokens. Existing methods have tried to improve the quantization strategy for better reconstruction quality, however, there still exists a large gap between VQ-VAEs and VAEs. To narrow this gap, we propose \\NickName, a novel method to augment the representation capability of discrete codebooks, facilitating easier optimization for codebooks and minimizing information loss, thereby enhancing reconstruction quality. Specifically, we propose to retain the latent dimension to preserve encoded features and incorporate a set of sub-codebooks for quantization. Furthermore, we construct comprehensive zero-shot benchmarks featuring resolutions of 512p and 2k to evaluate the reconstruction performance of existing methods rigorously. \\NickName~achieves the \\textbf{state-of-the-art performance on both ImageNet and $8$ zero-shot benchmarks} across all VQ-VAEs. Notably, compared with SD-VAE, we outperform them on ImageNet significantly, with rFID $\\textbf{0.49}$ v.s. $\\textbf{0.91}$, and achieve superior PSNR on all zero-shot benchmarks. These results highlight the superiority of \\NickName~in reconstruction and pave the way for preserving fidelity in HD image processing tasks. Code will be publicly available at https://github.com/MKJia/MGVQ.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\\NickName\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u79bb\u6563\u7801\u672c\u7684\u8868\u793a\u80fd\u529b\uff0c\u4f18\u5316\u91cf\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86VQ-VAEs\u7684\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709VQ-VAEs\u4e0eVAEs\u5728\u91cd\u5efa\u8d28\u91cf\u4e0a\u5b58\u5728\u8f83\u5927\u5dee\u8ddd\uff0c\u9700\u6539\u8fdb\u91cf\u5316\u7b56\u7565\u4ee5\u7f29\u5c0f\u5dee\u8ddd\u3002", "method": "\u4fdd\u7559\u6f5c\u5728\u7ef4\u5ea6\u4ee5\u4fdd\u7559\u7f16\u7801\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u4e00\u7ec4\u5b50\u7801\u672c\u8fdb\u884c\u91cf\u5316\u3002", "result": "\\NickName\u5728ImageNet\u548c8\u4e2a\u96f6\u6837\u672c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8eSD-VAE\u3002", "conclusion": "\\NickName\u5728\u91cd\u5efa\u4efb\u52a1\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u4e3a\u9ad8\u6e05\u56fe\u50cf\u5904\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
