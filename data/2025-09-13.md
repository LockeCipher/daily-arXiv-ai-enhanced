<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 18]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [CameraVDP: Perceptual Display Assessment with Uncertainty Estimation via Camera and Visual Difference Prediction](https://arxiv.org/abs/2509.08947)
*Yancheng Cai,Robert Wanat,Rafal Mantiuk*

Main category: cs.GR

TL;DR: 通过相机重建流水线结合视觉差异预测器，实现了能够评估显示器异常可见性的精确测量方法


<details>
  <summary>Details</summary>
Motivation: 传统的显示器测量方法无法抓取空间变化的显示异常，而相机测量又引入光学和光度异常，需要结合人眼视觉系统进行感知评估

Method: 开发CameraVDP框架，结合HDR图像堆叠、MTF逆变换、暗角校正、几何反变形、单应变换和颜色校正的相机重建流水线，以及视觉差异预测器(VDP)来模拟人眼对不同制激物的可见性

Result: 通过缺陷像素检测、色彩效应识别和显示器不均匀性评估三个应用验证框架，不确定性分析框架能估算缺陷像素检测的理论上限和VDP质量分数的信心区间

Conclusion: CameraVDP框架能够将相机转换为精确的显示器测量仪器，并通过视觉模型评估异常的可见性，为显示器质量评估提供了全面的解决方案

Abstract: Accurate measurement of images produced by electronic displays is critical for the evaluation of both traditional and computational displays. Traditional display measurement methods based on sparse radiometric sampling and fitting a model are inadequate for capturing spatially varying display artifacts, as they fail to capture high-frequency and pixel-level distortions. While cameras offer sufficient spatial resolution, they introduce optical, sampling, and photometric distortions. Furthermore, the physical measurement must be combined with a model of a visual system to assess whether the distortions are going to be visible. To enable perceptual assessment of displays, we propose a combination of a camera-based reconstruction pipeline with a visual difference predictor, which account for both the inaccuracy of camera measurements and visual difference prediction. The reconstruction pipeline combines HDR image stacking, MTF inversion, vignetting correction, geometric undistortion, homography transformation, and color correction, enabling cameras to function as precise display measurement instruments. By incorporating a Visual Difference Predictor (VDP), our system models the visibility of various stimuli under different viewing conditions for the human visual system. We validate the proposed CameraVDP framework through three applications: defective pixel detection, color fringing awareness, and display non-uniformity evaluation. Our uncertainty analysis framework enables the estimation of the theoretical upper bound for defect pixel detection performance and provides confidence intervals for VDP quality scores.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [Diffusion-Based Action Recognition Generalizes to Untrained Domains](https://arxiv.org/abs/2509.08908)
*Rogerio Guimaraes,Frank Xiao,Pietro Perona,Markus Marks*

Main category: cs.CV

TL;DR: 使用视觉扩散模型(VDM)特征和transformer聚合，实现跨物种、视角和场景的人类级动作识别泛化能力


<details>
  <summary>Details</summary>
Motivation: 人类能够识别跨物种、视角和场景变化的相同动作，但当前深度学习模型在此类泛化方面存在困难

Method: 利用条件于扩散过程早期时间步的视觉扩散模型生成特征，通过transformer聚合，突出语义信息而非像素级细节

Result: 在跨物种、跨视角和跨场景的三个泛化基准测试中都达到了新的最先进水平

Conclusion: 该方法使机器动作识别更接近人类级别的鲁棒性，显著提升了跨域泛化能力

Abstract: Humans can recognize the same actions despite large context and viewpoint variations, such as differences between species (walking in spiders vs. horses), viewpoints (egocentric vs. third-person), and contexts (real life vs movies). Current deep learning models struggle with such generalization. We propose using features generated by a Vision Diffusion Model (VDM), aggregated via a transformer, to achieve human-like action recognition across these challenging conditions. We find that generalization is enhanced by the use of a model conditioned on earlier timesteps of the diffusion process to highlight semantic information over pixel level details in the extracted features. We experimentally explore the generalization properties of our approach in classifying actions across animal species, across different viewing angles, and different recording contexts. Our model sets a new state-of-the-art across all three generalization benchmarks, bringing machine action recognition closer to human-like robustness. Project page: $\href{https://www.vision.caltech.edu/actiondiff/}{\texttt{vision.caltech.edu/actiondiff}}$ Code: $\href{https://github.com/frankyaoxiao/ActionDiff}{\texttt{github.com/frankyaoxiao/ActionDiff}}$

</details>


### [3] [Discovering Divergent Representations between Text-to-Image Models](https://arxiv.org/abs/2509.08940)
*Lisa Dunlap,Joseph E. Gonzalez,Trevor Darrell,Fabian Caba Heilbron,Josef Sivic,Bryan Russell*

Main category: cs.CV

TL;DR: CompCon算法通过进化搜索发现不同文本到图像模型之间的视觉表示差异，识别出特定提示词触发的属性差异，并建立了包含60个输入依赖差异的数据集进行验证。


<details>
  <summary>Details</summary>
Motivation: 研究不同生成模型学习到的视觉表示在何时以及如何产生分歧，旨在发现一个模型生成而另一个模型不生成的视觉属性，以及触发这些差异的提示类型。

Method: 提出CompCon进化搜索算法，通过自动化数据生成管道创建ID2数据集（包含60个输入依赖差异），并与多种LLM和VLM基线方法进行比较。

Result: 成功比较了流行的文本到图像模型，发现了不同的表示差异，如PixArt在提及孤独的提示中描绘湿漉街道，Stable Diffusion 3.5在媒体职业中描绘非裔美国人。

Conclusion: CompCon能够有效发现不同生成模型之间的视觉表示差异，为理解模型间的概念分歧提供了系统化的分析方法。

Abstract: In this paper, we investigate when and how visual representations learned by two different generative models diverge. Given two text-to-image models, our goal is to discover visual attributes that appear in images generated by one model but not the other, along with the types of prompts that trigger these attribute differences. For example, "flames" might appear in one model's outputs when given prompts expressing strong emotions, while the other model does not produce this attribute given the same prompts. We introduce CompCon (Comparing Concepts), an evolutionary search algorithm that discovers visual attributes more prevalent in one model's output than the other, and uncovers the prompt concepts linked to these visual differences. To evaluate CompCon's ability to find diverging representations, we create an automated data generation pipeline to produce ID2, a dataset of 60 input-dependent differences, and compare our approach to several LLM- and VLM-powered baselines. Finally, we use CompCon to compare popular text-to-image models, finding divergent representations such as how PixArt depicts prompts mentioning loneliness with wet streets and Stable Diffusion 3.5 depicts African American people in media professions. Code at: https://github.com/adobe-research/CompCon

</details>


### [4] [Integrating Anatomical Priors into a Causal Diffusion Model](https://arxiv.org/abs/2509.09054)
*Binxu Li,Wei Peng,Mingjie Li,Ehsan Adeli,Kilian M. Pohl*

Main category: cs.CV

TL;DR: PCGM是一种新的3D脑MRI反事实生成方法，通过概率图模型整合解剖约束，使用ControlNet编码空间掩码来指导扩散模型，生成高质量且解剖学合理的脑部MRI图像，能够复现疾病对大脑皮层的细微影响。


<details>
  <summary>Details</summary>
Motivation: 现有反事实模型难以生成解剖学合理的脑MRI图像，缺乏对细微解剖细节的显式归纳偏置，无法保留医学相关的局部变异。

Method: 提出概率因果图模型(PCGM)，在体素级别整合解剖约束作为先验，通过概率图模块捕获解剖约束并转换为空间二值掩码，使用3D ControlNet编码掩码来约束新的反事实去噪UNet，最终通过3D扩散解码器生成高质量脑MRI。

Result: 在多个数据集上的实验表明，PCGM生成的脑MRI质量优于多个基线方法，首次证明从反事实图像中提取的脑测量能够复现神经科学文献中报道的疾病对皮层脑区的细微影响。

Conclusion: PCGM在合成MRI用于研究细微形态差异方面取得了重要里程碑，为脑MRI研究提供了高质量的反事实图像生成解决方案。

Abstract: 3D brain MRI studies often examine subtle morphometric differences between cohorts that are hard to detect visually. Given the high cost of MRI acquisition, these studies could greatly benefit from image syntheses, particularly counterfactual image generation, as seen in other domains, such as computer vision. However, counterfactual models struggle to produce anatomically plausible MRIs due to the lack of explicit inductive biases to preserve fine-grained anatomical details. This shortcoming arises from the training of the models aiming to optimize for the overall appearance of the images (e.g., via cross-entropy) rather than preserving subtle, yet medically relevant, local variations across subjects. To preserve subtle variations, we propose to explicitly integrate anatomical constraints on a voxel-level as prior into a generative diffusion framework. Called Probabilistic Causal Graph Model (PCGM), the approach captures anatomical constraints via a probabilistic graph module and translates those constraints into spatial binary masks of regions where subtle variations occur. The masks (encoded by a 3D extension of ControlNet) constrain a novel counterfactual denoising UNet, whose encodings are then transferred into high-quality brain MRIs via our 3D diffusion decoder. Extensive experiments on multiple datasets demonstrate that PCGM generates structural brain MRIs of higher quality than several baseline approaches. Furthermore, we show for the first time that brain measurements extracted from counterfactuals (generated by PCGM) replicate the subtle effects of a disease on cortical brain regions previously reported in the neuroscience literature. This achievement is an important milestone in the use of synthetic MRIs in studies investigating subtle morphological differences.

</details>


### [5] [ALL-PET: A Low-resource and Low-shot PET Foundation Model in the Projection Domain](https://arxiv.org/abs/2509.09130)
*Bin Huang,Kang Chen,Bingxuan Li,Huafeng Liu,Qiegen Liu*

Main category: cs.CV

TL;DR: ALL-PET是一个低资源、低样本的PET投影域基础模型，通过潜在扩散模型和三项创新技术，仅用500个样本就能实现高质量sinogram生成，并在多种PET任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决PET成像中标记数据有限和计算资源不足的问题，克服数据稀缺和效率限制，构建大规模基础模型。

Method: 使用潜在扩散模型(LDM)，包含三个关键技术：1) Radon掩码增强策略(RMAS)生成多样化训练样本；2) 正负掩码约束嵌入几何一致性；3) 透明医学注意力(TMA)机制增强病灶相关区域。

Result: 仅用500个样本就实现高质量sinogram生成，性能与大数据集训练模型相当，在低剂量重建、衰减校正、延迟帧预测和示踪剂分离等任务中表现良好，内存使用低于24GB。

Conclusion: ALL-PET成功解决了PET成像中的数据稀缺问题，提供了一个高效、可解释且任务自适应的基础模型框架，在资源受限环境下表现出卓越性能。

Abstract: Building large-scale foundation model for PET imaging is hindered by limited access to labeled data and insufficient computational resources. To overcome data scarcity and efficiency limitations, we propose ALL-PET, a low-resource, low-shot PET foundation model operating directly in the projection domain. ALL-PET leverages a latent diffusion model (LDM) with three key innovations. First, we design a Radon mask augmentation strategy (RMAS) that generates over 200,000 structurally diverse training samples by projecting randomized image-domain masks into sinogram space, significantly improving generalization with minimal data. This is extended by a dynamic multi-mask (DMM) mechanism that varies mask quantity and distribution, enhancing data diversity without added model complexity. Second, we implement positive/negative mask constraints to embed strict geometric consistency, reducing parameter burden while preserving generation quality. Third, we introduce transparent medical attention (TMA), a parameter-free, geometry-driven mechanism that enhances lesion-related regions in raw projection data. Lesion-focused attention maps are derived from coarse segmentation, covering both hypermetabolic and hypometabolic areas, and projected into sinogram space for physically consistent guidance. The system supports clinician-defined ROI adjustments, ensuring flexible, interpretable, and task-adaptive emphasis aligned with PET acquisition physics. Experimental results show ALL-PET achieves high-quality sinogram generation using only 500 samples, with performance comparable to models trained on larger datasets. ALL-PET generalizes across tasks including low-dose reconstruction, attenuation correction, delayed-frame prediction, and tracer separation, operating efficiently with memory use under 24GB.

</details>


### [6] [RT-DETR++ for UAV Object Detection](https://arxiv.org/abs/2509.09157)
*Yuan Shufang*

Main category: cs.CV

TL;DR: RT-DETR++通过改进RT-DETR模型的编码器，引入通道门控注意力上采样/下采样机制和CSP-PAC特征融合技术，在保持实时检测速度的同时，显著提升了无人机图像中小目标和密集目标的检测性能。


<details>
  <summary>Details</summary>
Motivation: 无人机图像中的目标检测面临小目标密集、尺度变化大、遮挡严重等挑战，需要开发既能保持实时性又能提升检测精度的算法。

Method: 1. 引入通道门控注意力机制的上采样/下采样(AU/AD)双路径系统，减少特征层传播中的误差并保留细节；2. 在特征融合中采用CSP-PAC技术，通过并行空洞卷积在同一层处理局部和上下文信息，促进多尺度特征融合。

Result: 新设计的neck结构在小目标和密集目标检测方面表现出优越性能，模型在保持实时检测速度的同时不增加计算复杂度。

Conclusion: 本研究为实时检测系统中的特征编码设计提供了有效方法，解决了无人机图像目标检测的关键挑战。

Abstract: Object detection in unmanned aerial vehicle (UAV) imagery presents significant challenges. Issues such as densely packed small objects, scale variations, and occlusion are commonplace. This paper introduces RT-DETR++, which enhances the encoder component of the RT-DETR model. Our improvements focus on two key aspects. First, we introduce a channel-gated attention-based upsampling/downsampling (AU/AD) mechanism. This dual-path system minimizes errors and preserves details during feature layer propagation. Second, we incorporate CSP-PAC during feature fusion. This technique employs parallel hollow convolutions to process local and contextual information within the same layer, facilitating the integration of multi-scale features. Evaluation demonstrates that our novel neck design achieves superior performance in detecting small and densely packed objects. The model maintains sufficient speed for real-time detection without increasing computational complexity. This study provides an effective approach for feature encoding design in real-time detection systems.

</details>


### [7] [Bridging the Gap Between Ideal and Real-world Evaluation: Benchmarking AI-Generated Image Detection in Challenging Scenarios](https://arxiv.org/abs/2509.09172)
*Chunxiao Li,Xiaoxiao Wang,Meiling Li,Boming Miao,Peng Sun,Yunjian Zhang,Xiangyang Ji,Yao Zhu*

Main category: cs.CV

TL;DR: 本文提出了Real-World Robustness Dataset (RRDataset)，用于在真实世界复杂条件下全面评估AI生成图像检测模型的性能，涵盖场景泛化、网络传输鲁棒性和重数字化鲁棒性三个维度。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的快速发展，高度逼真的图像合成对数字安全和媒体可信度提出了新挑战。现有AI生成图像检测方法在复杂真实世界条件下的性能评估存在研究空白。

Method: 构建RRDataset数据集，包含7大场景的高质量图像；评估17个检测器和10个视觉语言模型；进行192人参与的大规模人类研究，探索人类在检测AI生成图像方面的少样本学习能力。

Result: 基准测试结果揭示了当前AI检测方法在真实世界条件下的局限性，强调了借鉴人类适应性来开发更鲁棒检测算法的重要性。

Conclusion: 该研究填补了AI生成图像检测在真实世界评估方面的空白，为开发更鲁棒的检测算法提供了重要数据集和基准，并强调了人类学习能力对算法改进的借鉴价值。

Abstract: With the rapid advancement of generative models, highly realistic image synthesis has posed new challenges to digital security and media credibility. Although AI-generated image detection methods have partially addressed these concerns, a substantial research gap remains in evaluating their performance under complex real-world conditions. This paper introduces the Real-World Robustness Dataset (RRDataset) for comprehensive evaluation of detection models across three dimensions: 1) Scenario Generalization: RRDataset encompasses high-quality images from seven major scenarios (War and Conflict, Disasters and Accidents, Political and Social Events, Medical and Public Health, Culture and Religion, Labor and Production, and everyday life), addressing existing dataset gaps from a content perspective. 2) Internet Transmission Robustness: examining detector performance on images that have undergone multiple rounds of sharing across various social media platforms. 3) Re-digitization Robustness: assessing model effectiveness on images altered through four distinct re-digitization methods. We benchmarked 17 detectors and 10 vision-language models (VLMs) on RRDataset and conducted a large-scale human study involving 192 participants to investigate human few-shot learning capabilities in detecting AI-generated images. The benchmarking results reveal the limitations of current AI detection methods under real-world conditions and underscore the importance of drawing on human adaptability to develop more robust detection algorithms.

</details>


### [8] [Dark-ISP: Enhancing RAW Image Processing for Low-Light Object Detection](https://arxiv.org/abs/2509.09183)
*Jiasheng Guo,Xin Gao,Yuxiang Yan,Guanghao Li,Jian Pu*

Main category: cs.CV

TL;DR: 这篇论文提出了一种轻量级自适应性图像信号处理插件Dark-ISP，直接处理黑暗环境下的Bayer RAW图像，通过分解传统ISP流水线为可微分组件并使其与检测目标对齐，在低光物体检测任务上达到了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 低光环境下的物体检测面临图像质量恶化的挑战。虽然RAW图像比RGB图像具有更好的潜力，但现有方法或者使用有信息损失的RAW-RGB图像，或者采用复杂框架，需要一种更高效的解决方案。

Method: 将传统ISP流水线解构为序列线性模块（传感器校准）和非线性模块（音调映射），将其重构为可微分组件通过任务驱动损失进行优化。每个模块都具备内容感知适应性和物理信息先验知识。利用ISP流水线的内在级联结构，设计了自我推动机制以促进子模块之间的协作。

Result: 在三个RAW图像数据集上进行了涉广实验，证明该方法在具有挑战性的低光环境下，以最少的参数量超越了现有的RGB和RAW基于检测的方法，获得了更优异的结果。

Conclusion: Dark-ISP提供了一种轻量级且自适应的解决方案，能够直接处理黑暗环境下的Bayer RAW图像，实现了无缝端到端训练，为低光物体检测领域带来了重要进展。

Abstract: Low-light Object detection is crucial for many real-world applications but remains challenging due to degraded image quality. While recent studies have shown that RAW images offer superior potential over RGB images, existing approaches either use RAW-RGB images with information loss or employ complex frameworks. To address these, we propose a lightweight and self-adaptive Image Signal Processing (ISP) plugin, Dark-ISP, which directly processes Bayer RAW images in dark environments, enabling seamless end-to-end training for object detection. Our key innovations are: (1) We deconstruct conventional ISP pipelines into sequential linear (sensor calibration) and nonlinear (tone mapping) sub-modules, recasting them as differentiable components optimized through task-driven losses. Each module is equipped with content-aware adaptability and physics-informed priors, enabling automatic RAW-to-RGB conversion aligned with detection objectives. (2) By exploiting the ISP pipeline's intrinsic cascade structure, we devise a Self-Boost mechanism that facilitates cooperation between sub-modules. Through extensive experiments on three RAW image datasets, we demonstrate that our method outperforms state-of-the-art RGB- and RAW-based detection approaches, achieving superior results with minimal parameters in challenging low-light environments.

</details>


### [9] [Plug-and-play Diffusion Models for Image Compressive Sensing with Data Consistency Projection](https://arxiv.org/abs/2509.09365)
*Xiaodong Wang,Ping Wang,Zhangyuan Li,Xin Yuan*

Main category: cs.CV

TL;DR: 本文提出了一个统一框架，将PnP方法与DDIM扩散模型相结合来解决单像素成像等病态逆问题，通过解耦扩散过程并引入混合数据一致性模块来提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 探索PnP方法与DDIM扩散模型之间的联系，特别是在解决病态逆问题方面的潜力，重点关注单像素成像应用。

Method: 将扩散过程解耦为三个可解释阶段：去噪、数据一致性强制和采样；提出混合数据一致性模块，线性组合多个PnP式保真项；直接在去噪估计上应用混合校正。

Result: 在单像素成像任务上的实验结果表明，该方法实现了更好的重建质量。

Conclusion: 通过统一框架成功整合了学习先验与物理前向模型，混合数据一致性模块在不破坏扩散采样轨迹的情况下改善了测量一致性。

Abstract: We explore the connection between Plug-and-Play (PnP) methods and Denoising Diffusion Implicit Models (DDIM) for solving ill-posed inverse problems, with a focus on single-pixel imaging. We begin by identifying key distinctions between PnP and diffusion models-particularly in their denoising mechanisms and sampling procedures. By decoupling the diffusion process into three interpretable stages: denoising, data consistency enforcement, and sampling, we provide a unified framework that integrates learned priors with physical forward models in a principled manner. Building upon this insight, we propose a hybrid data-consistency module that linearly combines multiple PnP-style fidelity terms. This hybrid correction is applied directly to the denoised estimate, improving measurement consistency without disrupting the diffusion sampling trajectory. Experimental results on single-pixel imaging tasks demonstrate that our method achieves better reconstruction quality.

</details>


### [10] [Decoupling Clinical and Class-Agnostic Features for Reliable Few-Shot Adaptation under Shift](https://arxiv.org/abs/2509.09397)
*Umaima Rahman,Raza Imam,Mohammad Yaqub,Dwarikanath Mahapatra*

Main category: cs.CV

TL;DR: DRiFt是一个医疗视觉语言模型框架，通过特征解耦将临床相关信号与任务无关噪声分离，使用LoRA参数高效调优和可学习提示令牌，提高模型在分布偏移下的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医疗视觉语言模型在临床决策支持中具有潜力，但由于成像协议和自由文本报告的变异性，模型容易学习任务无关的相关性，导致在真实世界部署中的可靠性问题。

Method: 提出结构化特征解耦框架DRiFt，使用LoRA参数高效调优和可学习提示令牌显式分离临床相关信号与任务无关噪声；通过为多样化医疗数据集生成标题来策划高质量、临床基础的图像-文本对，增强跨模态对齐。

Result: 在分布内性能上比现有基于提示的方法提高+11.4% Top-1准确率和+3.3% Macro-F1分数，同时在未见数据集上保持强鲁棒性。消融研究表明特征解耦和仔细对齐显著提升模型泛化能力。

Conclusion: 该方法有助于构建更安全、更可信的临床用视觉语言模型，特征解耦和跨模态对齐是提高模型在领域偏移下泛化能力和减少不可预测行为的关键因素。

Abstract: Medical vision-language models (VLMs) offer promise for clinical decision support, yet their reliability under distribution shifts remains a major concern for safe deployment. These models often learn task-agnostic correlations due to variability in imaging protocols and free-text reports, limiting their generalizability and increasing the risk of failure in real-world settings. We propose DRiFt, a structured feature decoupling framework that explicitly separates clinically relevant signals from task-agnostic noise using parameter-efficient tuning (LoRA) and learnable prompt tokens. To enhance cross-modal alignment and reduce uncertainty, we curate high-quality, clinically grounded image-text pairs by generating captions for a diverse medical dataset. Our approach improves in-distribution performance by +11.4% Top-1 accuracy and +3.3% Macro-F1 over prior prompt-based methods, while maintaining strong robustness across unseen datasets. Ablation studies reveal that disentangling task-relevant features and careful alignment significantly enhance model generalization and reduce unpredictable behavior under domain shift. These insights contribute toward building safer, more trustworthy VLMs for clinical use. The code is available at https://github.com/rumaima/DRiFt.

</details>


### [11] [FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolution](https://arxiv.org/abs/2509.09427)
*Yuchan Jie,Yushen Xu,Xiaosong Li,Fuqiang Zhou,Jianming Lv,Huafeng Li*

Main category: cs.CV

TL;DR: FS-Diff是一种基于扩散模型的联合图像融合和超分辨率方法，通过语义引导和清晰度感知机制，在低分辨率多模态图像中恢复丰富的细节和语义信息。


<details>
  <summary>Details</summary>
Motivation: 在军事侦察和远程检测等实际应用中，多模态图像的目标和背景结构容易损坏，分辨率低且语义信息弱，导致现有融合技术效果不佳。

Method: 将图像融合和超分辨率统一为条件生成问题，利用清晰度感知机制进行自适应低分辨率感知和跨模态特征提取，采用双向特征Mamba提取全局特征，通过改进的U-Net网络实现随机迭代去噪过程。

Result: 在六个公共数据集和自建的AVMS数据集上的实验表明，FS-Diff在多种放大倍数下优于现有最先进方法，能够恢复更丰富的细节和语义信息。

Conclusion: FS-Diff通过语义引导和扩散模型成功解决了低分辨率多模态图像的联合融合和超分辨率问题，为实际应用提供了有效的解决方案。

Abstract: As an influential information fusion and low-level vision technique, image fusion integrates complementary information from source images to yield an informative fused image. A few attempts have been made in recent years to jointly realize image fusion and super-resolution. However, in real-world applications such as military reconnaissance and long-range detection missions, the target and background structures in multimodal images are easily corrupted, with low resolution and weak semantic information, which leads to suboptimal results in current fusion techniques. In response, we propose FS-Diff, a semantic guidance and clarity-aware joint image fusion and super-resolution method. FS-Diff unifies image fusion and super-resolution as a conditional generation problem. It leverages semantic guidance from the proposed clarity sensing mechanism for adaptive low-resolution perception and cross-modal feature extraction. Specifically, we initialize the desired fused result as pure Gaussian noise and introduce the bidirectional feature Mamba to extract the global features of the multimodal images. Moreover, utilizing the source images and semantics as conditions, we implement a random iterative denoising process via a modified U-Net network. This network istrained for denoising at multiple noise levels to produce high-resolution fusion results with cross-modal features and abundant semantic information. We also construct a powerful aerial view multiscene (AVMS) benchmark covering 600 pairs of images. Extensive joint image fusion and super-resolution experiments on six public and our AVMS datasets demonstrated that FS-Diff outperforms the state-of-the-art methods at multiple magnifications and can recover richer details and semantics in the fused images. The code is available at https://github.com/XylonXu01/FS-Diff.

</details>


### [12] [FlexiD-Fuse: Flexible number of inputs multi-modal medical image fusion based on diffusion model](https://arxiv.org/abs/2509.09456)
*Yushen Xu,Xiaosong Li,Yuchun Wang,Xiaoqi Cheng,Huafeng Li,Haishu Tan*

Main category: cs.CV

TL;DR: FlexiD-Fuse是一个基于扩散模型的医学图像融合网络，能够处理任意数量的输入模态，解决了现有方法只能处理固定数量模态输入的限制。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像融合方法只能处理固定数量的模态输入（如双模态或三模态），无法直接处理变化数量的输入，这限制了其在临床环境中的应用。

Method: 提出FlexiD-Fuse扩散网络，将扩散融合问题转化为基于扩散过程和分层贝叶斯建模的最大似然估计问题，通过将EM算法整合到扩散采样迭代过程中，实现任意数量输入模态的高质量融合。

Result: 在哈佛数据集上使用9个流行指标进行评估，结果显示该方法在变化输入数量的医学图像融合中达到最佳性能，同时在红外-可见光、多曝光和多焦点图像融合任务中也表现出优越性。

Conclusion: FlexiD-Fuse方法能够有效处理任意数量的输入模态，在医学图像融合和其他多模态图像融合任务中均表现出卓越的性能和通用性。

Abstract: Different modalities of medical images provide unique physiological and anatomical information for diseases. Multi-modal medical image fusion integrates useful information from different complementary medical images with different modalities, producing a fused image that comprehensively and objectively reflects lesion characteristics to assist doctors in clinical diagnosis. However, existing fusion methods can only handle a fixed number of modality inputs, such as accepting only two-modal or tri-modal inputs, and cannot directly process varying input quantities, which hinders their application in clinical settings. To tackle this issue, we introduce FlexiD-Fuse, a diffusion-based image fusion network designed to accommodate flexible quantities of input modalities. It can end-to-end process two-modal and tri-modal medical image fusion under the same weight. FlexiD-Fuse transforms the diffusion fusion problem, which supports only fixed-condition inputs, into a maximum likelihood estimation problem based on the diffusion process and hierarchical Bayesian modeling. By incorporating the Expectation-Maximization algorithm into the diffusion sampling iteration process, FlexiD-Fuse can generate high-quality fused images with cross-modal information from source images, independently of the number of input images. We compared the latest two and tri-modal medical image fusion methods, tested them on Harvard datasets, and evaluated them using nine popular metrics. The experimental results show that our method achieves the best performance in medical image fusion with varying inputs. Meanwhile, we conducted extensive extension experiments on infrared-visible, multi-exposure, and multi-focus image fusion tasks with arbitrary numbers, and compared them with the perspective SOTA methods. The results of the extension experiments consistently demonstrate the effectiveness and superiority of our method.

</details>


### [13] [Improving Video Diffusion Transformer Training by Multi-Feature Fusion and Alignment from Self-Supervised Vision Encoders](https://arxiv.org/abs/2509.09547)
*Dohun Lee,Hyeonho Jeong,Jiwook Kim,Duygu Ceylan,Jong Chul Ye*

Main category: cs.CV

TL;DR: 提出Align4Gen方法，通过将视频生成器的中间特征与预训练视觉编码器的特征表示对齐，提升视频扩散模型的生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前视频扩散模型主要关注架构创新和训练目标改进，而对特征表示能力的提升关注较少。研究发现特征对齐可以显著改善视频生成效果。

Method: 提出多特征融合和对齐方法，首先分析评估不同视觉编码器的判别性和时序一致性，然后将其集成到视频扩散模型训练中。

Result: 在无条件视频生成和类别条件视频生成任务上，Align4Gen方法在各种评估指标上均显示出改进效果。

Conclusion: 特征对齐是提升视频扩散模型性能的有效策略，Align4Gen方法通过多特征融合和对齐显著改善了视频生成质量。

Abstract: Video diffusion models have advanced rapidly in the recent years as a result of series of architectural innovations (e.g., diffusion transformers) and use of novel training objectives (e.g., flow matching). In contrast, less attention has been paid to improving the feature representation power of such models. In this work, we show that training video diffusion models can benefit from aligning the intermediate features of the video generator with feature representations of pre-trained vision encoders. We propose a new metric and conduct an in-depth analysis of various vision encoders to evaluate their discriminability and temporal consistency, thereby assessing their suitability for video feature alignment. Based on the analysis, we present Align4Gen which provides a novel multi-feature fusion and alignment method integrated into video diffusion model training. We evaluate Align4Gen both for unconditional and class-conditional video generation tasks and show that it results in improved video generation as quantified by various metrics. Full video results are available on our project page: https://align4gen.github.io/align4gen/

</details>


### [14] [PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient Fine-Tuning for Remote Sensing Change Detection](https://arxiv.org/abs/2509.09572)
*Sijun Dong,Yuxuan Hu,LiBo Wang,Geng Chen,Xiaoliang Meng*

Main category: cs.CV

TL;DR: PeftCD是一个基于视觉基础模型(VFMs)和参数高效微调(PEFT)的变化检测框架，通过LoRA和Adapter模块实现高效任务适应，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决多时相多源遥感影像中伪变化普遍、标记样本稀缺和跨域泛化困难的问题

Method: 使用权重共享的Siamese编码器，集成LoRA和Adapter模块进行参数高效微调，采用SAM2和DINOv3作为骨干网络，配合轻量级解码器

Result: 在SYSU-CD(IoU 73.81%)、WHUCD(92.05%)、MSRSCD(64.07%)、MLCD(76.89%)、CDD(97.01%)、S2Looking(52.25%)和LEVIR-CD(85.62%)等多个数据集上达到state-of-the-art性能，具有精确的边界划分和强伪变化抑制能力

Conclusion: PeftCD在准确性、效率和泛化性之间达到了最佳平衡，为大规模VFM适应实际遥感变化检测应用提供了强大且可扩展的范式

Abstract: To tackle the prevalence of pseudo changes, the scarcity of labeled samples, and the difficulty of cross-domain generalization in multi-temporal and multi-source remote sensing imagery, we propose PeftCD, a change detection framework built upon Vision Foundation Models (VFMs) with Parameter-Efficient Fine-Tuning (PEFT). At its core, PeftCD employs a weight-sharing Siamese encoder derived from a VFM, into which LoRA and Adapter modules are seamlessly integrated. This design enables highly efficient task adaptation by training only a minimal set of additional parameters. To fully unlock the potential of VFMs, we investigate two leading backbones: the Segment Anything Model v2 (SAM2), renowned for its strong segmentation priors, and DINOv3, a state-of-the-art self-supervised representation learner. The framework is complemented by a deliberately lightweight decoder, ensuring the focus remains on the powerful feature representations from the backbones. Extensive experiments demonstrate that PeftCD achieves state-of-the-art performance across multiple public datasets, including SYSU-CD (IoU 73.81%), WHUCD (92.05%), MSRSCD (64.07%), MLCD (76.89%), CDD (97.01%), S2Looking (52.25%) and LEVIR-CD (85.62%), with notably precise boundary delineation and strong suppression of pseudo-changes. In summary, PeftCD presents an optimal balance of accuracy, efficiency, and generalization. It offers a powerful and scalable paradigm for adapting large-scale VFMs to real-world remote sensing change detection applications. The code and pretrained models will be released at https://github.com/dyzy41/PeftCD.

</details>


### [15] [Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis](https://arxiv.org/abs/2509.09595)
*Yikang Ding,Jiwen Liu,Wenyuan Zhang,Zekun Wang,Wentao Hu,Liyuan Cui,Mingming Lao,Yingchao Shao,Hui Liu,Xiaohan Li,Ming Chen,Xiaoqiang Liu,Yu-Shen Liu,Pengfei Wan*

Main category: cs.CV

TL;DR: Kling-Avatar是一个新颖的级联框架，通过多模态指令理解和照片级真实感人像生成，解决了现有音频驱动avatar视频生成方法在叙事连贯性和角色表现力方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将指令条件仅视为由声学或视觉线索驱动的低级跟踪，而没有建模指令传达的交流目的，这影响了叙事连贯性和角色表现力。

Method: 采用两阶段流水线：第一阶段使用多模态大语言模型导演生成蓝图视频，控制角色动作和情感等高级语义；第二阶段在蓝图关键帧指导下，使用首尾帧策略并行生成多个子片段。

Result: 能够生成生动、流畅、长达1080p和48fps的长时视频，在唇同步准确性、情感和动态表现力、指令可控性、身份保持和跨域泛化方面表现优异。

Conclusion: Kling-Avatar为基于语义的高保真音频驱动avatar合成建立了新的基准，适用于数字人直播和视频博客等实际应用。

Abstract: Recent advances in audio-driven avatar video generation have significantly enhanced audio-visual realism. However, existing methods treat instruction conditioning merely as low-level tracking driven by acoustic or visual cues, without modeling the communicative purpose conveyed by the instructions. This limitation compromises their narrative coherence and character expressiveness. To bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that unifies multimodal instruction understanding with photorealistic portrait generation. Our approach adopts a two-stage pipeline. In the first stage, we design a multimodal large language model (MLLM) director that produces a blueprint video conditioned on diverse instruction signals, thereby governing high-level semantics such as character motion and emotions. In the second stage, guided by blueprint keyframes, we generate multiple sub-clips in parallel using a first-last frame strategy. This global-to-local framework preserves fine-grained details while faithfully encoding the high-level intent behind multimodal instructions. Our parallel architecture also enables fast and stable generation of long-duration videos, making it suitable for real-world applications such as digital human livestreaming and vlogging. To comprehensively evaluate our method, we construct a benchmark of 375 curated samples covering diverse instructions and challenging scenarios. Extensive experiments demonstrate that Kling-Avatar is capable of generating vivid, fluent, long-duration videos at up to 1080p and 48 fps, achieving superior performance in lip synchronization accuracy, emotion and dynamic expressiveness, instruction controllability, identity preservation, and cross-domain generalization. These results establish Kling-Avatar as a new benchmark for semantically grounded, high-fidelity audio-driven avatar synthesis.

</details>


### [16] [Mechanistic Learning with Guided Diffusion Models to Predict Spatio-Temporal Brain Tumor Growth](https://arxiv.org/abs/2509.09610)
*Daria Laslo,Efthymios Georgiou,Marius George Linguraru,Andreas Rauschecker,Sabine Muller,Catherine R. Jutzeler,Sarah Bruningk*

Main category: cs.CV

TL;DR: 一种结合机制模型和导向去噪模型的混合框架，用于预测脑罐的时空进展并生成叔伪未来MRI图像


<details>
  <summary>Details</summary>
Motivation: 预测脑罐的时空进展对临床决策至关重要，需要一种能在数据有限情况下进行生物学信息图像生成的方法

Method: 采用常微分方程机制模型捕捉脑罐动态，包括放疗效果，然后通过梯度导向DDIM模型进行图像合成，使其与预测增长和病人解剖结构一致

Result: 在BraTS成人和儿童细胞脏瘤数据集上训练，在60个纵向屿散中线细胞脏瘤病例上评估，生成了现实的随访扫描图像，并通过空间相似性指标验证

Conclusion: 该方法能够在数据有限情况下进行生物学信息图像生成，提供考虑机制前知的生成式时空预测，为临床决策提供有价值的脑罐增长预测工具

Abstract: Predicting the spatio-temporal progression of brain tumors is essential for guiding clinical decisions in neuro-oncology. We propose a hybrid mechanistic learning framework that combines a mathematical tumor growth model with a guided denoising diffusion implicit model (DDIM) to synthesize anatomically feasible future MRIs from preceding scans. The mechanistic model, formulated as a system of ordinary differential equations, captures temporal tumor dynamics including radiotherapy effects and estimates future tumor burden. These estimates condition a gradient-guided DDIM, enabling image synthesis that aligns with both predicted growth and patient anatomy. We train our model on the BraTS adult and pediatric glioma datasets and evaluate on 60 axial slices of in-house longitudinal pediatric diffuse midline glioma (DMG) cases. Our framework generates realistic follow-up scans based on spatial similarity metrics. It also introduces tumor growth probability maps, which capture both clinically relevant extent and directionality of tumor growth as shown by 95th percentile Hausdorff Distance. The method enables biologically informed image generation in data-limited scenarios, offering generative-space-time predictions that account for mechanistic priors.

</details>


### [17] [Geometric Neural Distance Fields for Learning Human Motion Priors](https://arxiv.org/abs/2509.09667)
*Zhengdi Yu,Simone Foti,Linguang Zhang,Amy Zhao,Cem Keskin,Stefanos Zafeiriou,Tolga Birdal*

Main category: cs.CV

TL;DR: NRMF是一种新颖的3D生成式人体运动先验模型，通过神经距离场在关节旋转、角速度和角加速度的乘积空间中显式建模人体运动，实现鲁棒、时间一致且物理合理的3D运动恢复。


<details>
  <summary>Details</summary>
Motivation: 现有基于VAE或扩散的方法无法充分建模人体运动的几何结构和物理约束，需要一种能够显式建模运动动力学并在测试时生成物理合理运动的方法。

Method: 在关节旋转、角速度和角加速度的乘积空间上构建神经距离场，提出自适应步长混合算法进行投影，以及几何积分器来生成真实运动轨迹。

Result: 在AMASS数据集上训练后，NRMF在去噪、运动插值和部分2D/3D观测拟合等多种任务上表现出显著且一致的性能提升，具有良好的泛化能力。

Conclusion: NRMF通过几何感知的神经距离场建模和创新的投影与积分方法，为3D人体运动恢复提供了强大且通用的先验模型，在多个任务上优于现有方法。

Abstract: We introduce Neural Riemannian Motion Fields (NRMF), a novel 3D generative human motion prior that enables robust, temporally consistent, and physically plausible 3D motion recovery. Unlike existing VAE or diffusion-based methods, our higher-order motion prior explicitly models the human motion in the zero level set of a collection of neural distance fields (NDFs) corresponding to pose, transition (velocity), and acceleration dynamics. Our framework is rigorous in the sense that our NDFs are constructed on the product space of joint rotations, their angular velocities, and angular accelerations, respecting the geometry of the underlying articulations. We further introduce: (i) a novel adaptive-step hybrid algorithm for projecting onto the set of plausible motions, and (ii) a novel geometric integrator to "roll out" realistic motion trajectories during test-time-optimization and generation. Our experiments show significant and consistent gains: trained on the AMASS dataset, NRMF remarkably generalizes across multiple input modalities and to diverse tasks ranging from denoising to motion in-betweening and fitting to partial 2D / 3D observations.

</details>


### [18] [Locality in Image Diffusion Models Emerges from Data Statistics](https://arxiv.org/abs/2509.09672)
*Artem Lukoianov,Chenyang Yuan,Justin Solomon,Vincent Sitzmann*

Main category: cs.CV

TL;DR: 通过分析发现深度漏渌模型的局部性并非来自卷积神经网络的归纳偏见，而是图像数据集的统计特性导致的结果，并提出了更准确匹配深度漏渌模型的分析式漏渌器


<details>
  <summary>Details</summary>
Motivation: 解释深度漏渌模型与理论最优漏渌器之间的性能差距，识别引起这种差距的根本原因

Method: 通过理论分析和实验验证，证明最优参数化线性漏渌器也体现出类似的局部性质，并分析这种局部性来自于自然图像数据集中像素相关性的统计特性

Result: 得到了更准确匹配深度漏渌模型预测分数的分析式漏渌器，超越了之前依靠卷积神经网络归纳偏见的专家手工模型

Conclusion: 深度漏渌模型的局部性主要来自于图像数据的统计特性，而非卷积神经网络的归纳偏见，这一发现为理论分析漏渌器的设计提供了新的视角

Abstract: Among generative models, diffusion models are uniquely intriguing due to the existence of a closed-form optimal minimizer of their training objective, often referred to as the optimal denoiser. However, diffusion using this optimal denoiser merely reproduces images in the training set and hence fails to capture the behavior of deep diffusion models. Recent work has attempted to characterize this gap between the optimal denoiser and deep diffusion models, proposing analytical, training-free models that can generate images that resemble those generated by a trained UNet. The best-performing method hypothesizes that shift equivariance and locality inductive biases of convolutional neural networks are the cause of the performance gap, hence incorporating these assumptions into its analytical model. In this work, we present evidence that the locality in deep diffusion models emerges as a statistical property of the image dataset, not due to the inductive bias of convolutional neural networks. Specifically, we demonstrate that an optimal parametric linear denoiser exhibits similar locality properties to the deep neural denoisers. We further show, both theoretically and experimentally, that this locality arises directly from the pixel correlations present in natural image datasets. Finally, we use these insights to craft an analytical denoiser that better matches scores predicted by a deep diffusion model than the prior expert-crafted alternative.

</details>


### [19] [FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark](https://arxiv.org/abs/2509.09680)
*Rongyao Fang,Aldrich Yu,Chengqi Duan,Linjiang Huang,Shuai Bai,Yuxuan Cai,Kun Wang,Si Liu,Xihui Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: FLUX-Reason-6M是一个包含600万高质量图像和2000万双语描述的大规模推理数据集，PRISM-Bench是包含7个评估轨道的新基准，旨在解决开源文本生成图像模型在推理能力方面的性能差距。


<details>
  <summary>Details</summary>
Motivation: 开源文本生成图像模型因缺乏大规模推理数据集和全面评估基准，导致与闭源系统存在性能差距，需要解决这一问题。

Method: 构建FLUX-Reason-6M数据集（600万FLUX生成图像+2000万双语描述），设计生成思维链(GCoT)提供详细生成步骤；创建PRISM-Bench评估基准（7个评估轨道，包括长文本挑战）。

Result: 对19个领先模型的评估揭示了关键性能差距和改进需求，数据集和基准为社区提供了前所未有的资源。

Conclusion: 该工作为推理导向的文本生成图像研究提供了重要资源和评估标准，将推动该领域的下一波发展。

Abstract: The advancement of open-source text-to-image (T2I) models has been hindered by the absence of large-scale, reasoning-focused datasets and comprehensive evaluation benchmarks, resulting in a performance gap compared to leading closed-source systems. To address this challenge, We introduce FLUX-Reason-6M and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark). FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality FLUX-generated images and 20 million bilingual (English and Chinese) descriptions specifically designed to teach complex reasoning. The image are organized according to six key characteristics: Imagination, Entity, Text rendering, Style, Affection, and Composition, and design explicit Generation Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation steps. The whole data curation takes 15,000 A100 GPU days, providing the community with a resource previously unattainable outside of large industrial labs. PRISM-Bench offers a novel evaluation standard with seven distinct tracks, including a formidable Long Text challenge using GCoT. Through carefully designed prompts, it utilizes advanced vision-language models for nuanced human-aligned assessment of prompt-image alignment and image aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench reveals critical performance gaps and highlights specific areas requiring improvement. Our dataset, benchmark, and evaluation code are released to catalyze the next wave of reasoning-oriented T2I generation. Project page: https://flux-reason-6m.github.io/ .

</details>
