{"id": "2507.14624", "pdf": "https://arxiv.org/pdf/2507.14624", "abs": "https://arxiv.org/abs/2507.14624", "authors": ["Yaru Liu", "Derek Nowrouzezahri", "Morgan Mcguire"], "title": "Real-Time Scene Reconstruction using Light Field Probes", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Reconstructing photo-realistic large-scale scenes from images, for example at city scale, is a long-standing problem in computer graphics. Neural rendering is an emerging technique that enables photo-realistic image synthesis from previously unobserved viewpoints; however, state-of-the-art neural rendering methods have difficulty efficiently rendering a high complex large-scale scene because these methods typically trade scene size, fidelity, and rendering speed for quality. The other stream of techniques utilizes scene geometries for reconstruction. But the cost of building and maintaining a large set of geometry data increases as scene size grows. Our work explores novel view synthesis methods that efficiently reconstruct complex scenes without explicit use of scene geometries. Specifically, given sparse images of the scene (captured from the real world), we reconstruct intermediate, multi-scale, implicit representations of scene geometries. In this way, our method avoids explicitly relying on scene geometry, significantly reducing the computational cost of maintaining large 3D data. Unlike current methods, we reconstruct the scene using a probe data structure. Probe data hold highly accurate depth information of dense data points, enabling the reconstruction of highly complex scenes. By reconstructing the scene using probe data, the rendering cost is independent of the complexity of the scene. As such, our approach combines geometry reconstruction and novel view synthesis. Moreover, when rendering large-scale scenes, compressing and streaming probe data is more efficient than using explicit scene geometry. Therefore, our neural representation approach can potentially be applied to virtual reality (VR) and augmented reality (AR) applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a2\u9488\u6570\u636e\u7ed3\u6784\u7684\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u91cd\u5efa\u590d\u6742\u5927\u573a\u666f\uff0c\u65e0\u9700\u663e\u5f0f\u4f9d\u8d56\u573a\u666f\u51e0\u4f55\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u96be\u4ee5\u5e73\u8861\u8d28\u91cf\u3001\u901f\u5ea6\u548c\u590d\u6742\u5ea6\uff0c\u800c\u4f20\u7edf\u51e0\u4f55\u91cd\u5efa\u65b9\u6cd5\u6210\u672c\u9ad8\u3002", "method": "\u5229\u7528\u7a00\u758f\u56fe\u50cf\u91cd\u5efa\u591a\u5c3a\u5ea6\u9690\u5f0f\u51e0\u4f55\u8868\u793a\uff0c\u901a\u8fc7\u63a2\u9488\u6570\u636e\u7ed3\u6784\u5b58\u50a8\u9ad8\u7cbe\u5ea6\u6df1\u5ea6\u4fe1\u606f\uff0c\u5b9e\u73b0\u9ad8\u6548\u6e32\u67d3\u3002", "result": "\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u6e32\u67d3\u6548\u7387\u4e0e\u573a\u666f\u590d\u6742\u5ea6\u65e0\u5173\uff0c\u9002\u7528\u4e8eVR/AR\u5e94\u7528\u3002", "conclusion": "\u63a2\u9488\u6570\u636e\u7ed3\u6784\u7ed3\u5408\u795e\u7ecf\u8868\u793a\uff0c\u4e3a\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14841", "pdf": "https://arxiv.org/pdf/2507.14841", "abs": "https://arxiv.org/abs/2507.14841", "authors": ["Xiang Tang", "Ruotong Li", "Xiaopeng Fan"], "title": "Towards Geometric and Textural Consistency 3D Scene Generation via Single Image-guided Model Generation and Layout Optimization", "categories": ["cs.GR", "cs.CV"], "comment": "15 pages, 8 figures, Project page: https://xdlbw.github.io/sing3d/", "summary": "In recent years, 3D generation has made great strides in both academia and industry. However, generating 3D scenes from a single RGB image remains a significant challenge, as current approaches often struggle to ensure both object generation quality and scene coherence in multi-object scenarios. To overcome these limitations, we propose a novel three-stage framework for 3D scene generation with explicit geometric representations and high-quality textural details via single image-guided model generation and spatial layout optimization. Our method begins with an image instance segmentation and inpainting phase, which recovers missing details of occluded objects in the input images, thereby achieving complete generation of foreground 3D assets. Subsequently, our approach captures the spatial geometry of reference image by constructing pseudo-stereo viewpoint for camera parameter estimation and scene depth inference, while employing a model selection strategy to ensure optimal alignment between the 3D assets generated in the previous step and the input. Finally, through model parameterization and minimization of the Chamfer distance between point clouds in 3D and 2D space, our approach optimizes layout parameters to produce an explicit 3D scene representation that maintains precise alignment with input guidance image. Extensive experiments on multi-object scene image sets have demonstrated that our approach not only outperforms state-of-the-art methods in terms of geometric accuracy and texture fidelity of individual generated 3D models, but also has significant advantages in scene layout synthesis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u56fe\u50cf\u5f15\u5bfc\u751f\u6210\u548c\u7a7a\u95f4\u5e03\u5c40\u4f18\u5316\uff0c\u89e3\u51b3\u4ece\u5355RGB\u56fe\u50cf\u751f\u62103D\u573a\u666f\u7684\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728\u591a\u7269\u4f53\u573a\u666f\u4e2d\u96be\u4ee5\u4fdd\u8bc1\u751f\u6210\u8d28\u91cf\u548c\u573a\u666f\u4e00\u81f4\u6027\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a\u56fe\u50cf\u5b9e\u4f8b\u5206\u5272\u4e0e\u4fee\u590d\u3001\u4f2a\u7acb\u4f53\u89c6\u89d2\u6784\u5efa\u4e0e\u76f8\u673a\u53c2\u6570\u4f30\u8ba1\u3001\u6a21\u578b\u53c2\u6570\u5316\u4e0e\u5e03\u5c40\u4f18\u5316\u3002", "result": "\u5728\u51e0\u4f55\u7cbe\u5ea6\u548c\u7eb9\u7406\u4fdd\u771f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5728\u573a\u666f\u5e03\u5c40\u5408\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e863D\u573a\u666f\u751f\u6210\u7684\u5b8c\u6574\u6027\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2507.15399", "pdf": "https://arxiv.org/pdf/2507.15399", "abs": "https://arxiv.org/abs/2507.15399", "authors": ["Etai Sella", "Noam Atia", "Ron Mokady", "Hadar Averbuch-Elor"], "title": "Blended Point Cloud Diffusion for Localized Text-guided Shape Editing", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted to ICCV 2025. Project Page:   https://tau-vailab.github.io/BlendedPC/", "summary": "Natural language offers a highly intuitive interface for enabling localized fine-grained edits of 3D shapes. However, prior works face challenges in preserving global coherence while locally modifying the input 3D shape. In this work, we introduce an inpainting-based framework for editing shapes represented as point clouds. Our approach leverages foundation 3D diffusion models for achieving localized shape edits, adding structural guidance in the form of a partial conditional shape, ensuring that other regions correctly preserve the shape's identity. Furthermore, to encourage identity preservation also within the local edited region, we propose an inference-time coordinate blending algorithm which balances reconstruction of the full shape with inpainting at a progression of noise levels during the inference process. Our coordinate blending algorithm seamlessly blends the original shape with its edited version, enabling a fine-grained editing of 3D shapes, all while circumventing the need for computationally expensive and often inaccurate inversion. Extensive experiments show that our method outperforms alternative techniques across a wide range of metrics that evaluate both fidelity to the original shape and also adherence to the textual description.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fee\u590d\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u7f16\u8f91\u70b9\u4e91\u8868\u793a\u76843D\u5f62\u72b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5f15\u5bfc\u548c\u5750\u6807\u6df7\u5408\u7b97\u6cd5\u5b9e\u73b0\u5c40\u90e8\u7f16\u8f91\u5e76\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u4e3a3D\u5f62\u72b6\u7684\u5c40\u90e8\u7ec6\u7c92\u5ea6\u7f16\u8f91\u63d0\u4f9b\u4e86\u76f4\u89c2\u63a5\u53e3\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u5c40\u90e8\u4fee\u6539\u65f6\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\u3002", "method": "\u5229\u7528\u57fa\u78403D\u6269\u6563\u6a21\u578b\u8fdb\u884c\u5c40\u90e8\u5f62\u72b6\u7f16\u8f91\uff0c\u7ed3\u5408\u90e8\u5206\u6761\u4ef6\u5f62\u72b6\u7684\u7ed3\u6784\u5f15\u5bfc\uff0c\u5e76\u63d0\u51fa\u4e86\u63a8\u7406\u65f6\u5750\u6807\u6df7\u5408\u7b97\u6cd5\u4ee5\u5e73\u8861\u91cd\u5efa\u4e0e\u4fee\u590d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u771f\u5ea6\u548c\u6587\u672c\u63cf\u8ff0\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u5176\u4ed6\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e863D\u5f62\u72b6\u7684\u7ec6\u7c92\u5ea6\u7f16\u8f91\uff0c\u540c\u65f6\u907f\u514d\u4e86\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4e0d\u51c6\u786e\u7684\u9006\u8fc7\u7a0b\u3002"}}
{"id": "2507.15454", "pdf": "https://arxiv.org/pdf/2507.15454", "abs": "https://arxiv.org/abs/2507.15454", "authors": ["Ruijie Zhu", "Mulin Yu", "Linning Xu", "Lihan Jiang", "Yixuan Li", "Tianzhu Zhang", "Jiangmiao Pang", "Bo Dai"], "title": "ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.HC"], "comment": "Accepted by ICCV 2025", "summary": "3D Gaussian Splatting is renowned for its high-fidelity reconstructions and real-time novel view synthesis, yet its lack of semantic understanding limits object-level perception. In this work, we propose ObjectGS, an object-aware framework that unifies 3D scene reconstruction with semantic understanding. Instead of treating the scene as a unified whole, ObjectGS models individual objects as local anchors that generate neural Gaussians and share object IDs, enabling precise object-level reconstruction. During training, we dynamically grow or prune these anchors and optimize their features, while a one-hot ID encoding with a classification loss enforces clear semantic constraints. We show through extensive experiments that ObjectGS not only outperforms state-of-the-art methods on open-vocabulary and panoptic segmentation tasks, but also integrates seamlessly with applications like mesh extraction and scene editing. Project page: https://ruijiezhu94.github.io/ObjectGS_page", "AI": {"tldr": "ObjectGS\u662f\u4e00\u4e2a\u7ed3\u54083D\u9ad8\u65af\u6cfc\u6e85\u4e0e\u8bed\u4e49\u7406\u89e3\u7684\u5bf9\u8c61\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u5c40\u90e8\u951a\u70b9\u548c\u5bf9\u8c61ID\u5b9e\u73b0\u7cbe\u786e\u7684\u5bf9\u8c61\u7ea7\u91cd\u5efa\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u7f3a\u4e4f\u8bed\u4e49\u7406\u89e3\uff0c\u9650\u5236\u4e86\u5bf9\u8c61\u7ea7\u611f\u77e5\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7edf\u4e003D\u91cd\u5efa\u4e0e\u8bed\u4e49\u7406\u89e3\u7684\u65b9\u6cd5\u3002", "method": "ObjectGS\u5c06\u573a\u666f\u4e2d\u7684\u5bf9\u8c61\u5efa\u6a21\u4e3a\u5c40\u90e8\u951a\u70b9\uff0c\u751f\u6210\u795e\u7ecf\u9ad8\u65af\u5e76\u5171\u4eab\u5bf9\u8c61ID\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u951a\u70b9\u548c\u4f18\u5316\u7279\u5f81\uff0c\u7ed3\u5408\u5206\u7c7b\u635f\u5931\u5b9e\u73b0\u8bed\u4e49\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cObjectGS\u5728\u5f00\u653e\u8bcd\u6c47\u548c\u5168\u666f\u5206\u5272\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u65e0\u7f1d\u96c6\u6210\u7f51\u683c\u63d0\u53d6\u548c\u573a\u666f\u7f16\u8f91\u7b49\u5e94\u7528\u3002", "conclusion": "ObjectGS\u6210\u529f\u5b9e\u73b0\u4e863D\u573a\u666f\u91cd\u5efa\u4e0e\u8bed\u4e49\u7406\u89e3\u7684\u7edf\u4e00\uff0c\u4e3a\u5bf9\u8c61\u7ea7\u611f\u77e5\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15629", "pdf": "https://arxiv.org/pdf/2507.15629", "abs": "https://arxiv.org/abs/2507.15629", "authors": ["Zuo-Liang Zhu", "Jian Yang", "Beibei Wang"], "title": "Gaussian Splatting with Discretized SDF for Relightable Assets", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "3D Gaussian splatting (3DGS) has shown its detailed expressive ability and highly efficient rendering speed in the novel view synthesis (NVS) task. The application to inverse rendering still faces several challenges, as the discrete nature of Gaussian primitives makes it difficult to apply geometry constraints. Recent works introduce the signed distance field (SDF) as an extra continuous representation to regularize the geometry defined by Gaussian primitives. It improves the decomposition quality, at the cost of increasing memory usage and complicating training. Unlike these works, we introduce a discretized SDF to represent the continuous SDF in a discrete manner by encoding it within each Gaussian using a sampled value. This approach allows us to link the SDF with the Gaussian opacity through an SDF-to-opacity transformation, enabling rendering the SDF via splatting and avoiding the computational cost of ray marching.The key challenge is to regularize the discrete samples to be consistent with the underlying SDF, as the discrete representation can hardly apply the gradient-based constraints (\\eg Eikonal loss). For this, we project Gaussians onto the zero-level set of SDF and enforce alignment with the surface from splatting, namely a projection-based consistency loss. Thanks to the discretized SDF, our method achieves higher relighting quality, while requiring no extra memory beyond GS and avoiding complex manually designed optimization. The experiments reveal that our method outperforms existing Gaussian-based inverse rendering methods. Our code is available at https://github.com/NK-CS-ZZL/DiscretizedSDF.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79bb\u6563\u5316SDF\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u8fde\u7eedSDF\u7f16\u7801\u5230\u9ad8\u65af\u57fa\u5143\u4e2d\uff0c\u89e3\u51b3\u4e863D\u9ad8\u65af\u6e85\u5c04\u5728\u9006\u6e32\u67d3\u4e2d\u7684\u51e0\u4f55\u7ea6\u675f\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u91cd\u5149\u7167\u8d28\u91cf\u4e14\u65e0\u9700\u989d\u5916\u5185\u5b58\u3002", "motivation": "3D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u5728\u9006\u6e32\u67d3\u4e2d\u56e0\u79bb\u6563\u9ad8\u65af\u57fa\u5143\u96be\u4ee5\u5e94\u7528\u51e0\u4f55\u7ea6\u675f\uff0c\u73b0\u6709\u65b9\u6cd5\u5f15\u5165SDF\u4f46\u589e\u52a0\u5185\u5b58\u548c\u8bad\u7ec3\u590d\u6742\u5ea6\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u79bb\u6563\u5316SDF\uff0c\u5c06\u5176\u7f16\u7801\u5230\u9ad8\u65af\u57fa\u5143\u4e2d\uff0c\u5e76\u901a\u8fc7SDF-to-opacity\u8f6c\u6362\u94fe\u63a5SDF\u4e0e\u9ad8\u65af\u4e0d\u900f\u660e\u5ea6\uff0c\u907f\u514d\u5149\u7ebf\u6b65\u8fdb\u8ba1\u7b97\u3002\u4f7f\u7528\u6295\u5f71\u4e00\u81f4\u6027\u635f\u5931\u6b63\u5219\u5316\u79bb\u6563\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u91cd\u5149\u7167\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u9ad8\u65af\u7684\u9006\u6e32\u67d3\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u989d\u5916\u5185\u5b58\u6216\u590d\u6742\u4f18\u5316\u8bbe\u8ba1\u3002", "conclusion": "\u79bb\u6563\u5316SDF\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e863DGS\u5728\u9006\u6e32\u67d3\u4e2d\u7684\u51e0\u4f55\u7ea6\u675f\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u4e14\u7b80\u5316\u4e86\u5b9e\u73b0\u3002"}}
{"id": "2507.14367", "pdf": "https://arxiv.org/pdf/2507.14367", "abs": "https://arxiv.org/abs/2507.14367", "authors": ["Weiming Ren", "Raghav Goyal", "Zhiming Hu", "Tristan Ty Aumentado-Armstrong", "Iqbal Mohomed", "Alex Levinshtein"], "title": "Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution", "categories": ["cs.CV"], "comment": "12 pages, 17 figures and 7 tables", "summary": "Generative super-resolution (GSR) currently sets the state-of-the-art in terms of perceptual image quality, overcoming the \"regression-to-the-mean\" blur of prior non-generative models. However, from a human perspective, such models do not fully conform to the optimal balance between quality and fidelity. Instead, a different class of artifacts, in which generated details fail to perceptually match the low resolution image (LRI) or ground-truth image (GTI), is a critical but under studied issue in GSR, limiting its practical deployments. In this work, we focus on measuring, analyzing, and mitigating these artifacts (i.e., \"hallucinations\"). We observe that hallucinations are not well-characterized with existing image metrics or quality models, as they are orthogonal to both exact fidelity and no-reference quality. Instead, we take advantage of a multimodal large language model (MLLM) by constructing a prompt that assesses hallucinatory visual elements and generates a \"Hallucination Score\" (HS). We find that our HS is closely aligned with human evaluations, and also provides complementary insights to prior image metrics used for super-resolution (SR) models. In addition, we find certain deep feature distances have strong correlations with HS. We therefore propose to align the GSR models by using such features as differentiable reward functions to mitigate hallucinations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u751f\u6210\u8d85\u5206\u8fa8\u7387\uff08GSR\uff09\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u5e7b\u89c9\u8bc4\u5206\uff08HS\uff09\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u7279\u5f81\u8ddd\u79bb\u4f18\u5316GSR\u6a21\u578b\u4ee5\u51cf\u5c11\u5e7b\u89c9\u3002", "motivation": "GSR\u6a21\u578b\u867d\u7136\u5728\u9ad8\u611f\u77e5\u56fe\u50cf\u8d28\u91cf\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u751f\u6210\u7684\u7ec6\u8282\u53ef\u80fd\u4e0e\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\uff08LRI\uff09\u6216\u771f\u5b9e\u56fe\u50cf\uff08GTI\uff09\u4e0d\u5339\u914d\uff0c\u8fd9\u79cd\u5e7b\u89c9\u95ee\u9898\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5229\u7528MLLM\u6784\u5efa\u63d0\u793a\uff0c\u8bc4\u4f30\u5e7b\u89c9\u89c6\u89c9\u5143\u7d20\u5e76\u751f\u6210HS\uff1b\u53d1\u73b0\u67d0\u4e9b\u6df1\u5ea6\u7279\u5f81\u8ddd\u79bb\u4e0eHS\u5f3a\u76f8\u5173\uff0c\u63d0\u51fa\u7528\u8fd9\u4e9b\u7279\u5f81\u4f5c\u4e3a\u53ef\u5fae\u5956\u52b1\u51fd\u6570\u4f18\u5316GSR\u6a21\u578b\u3002", "result": "HS\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5e76\u4e3a\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u63d0\u4f9b\u4e86\u8865\u5145\u6027\u89c1\u89e3\uff1b\u901a\u8fc7\u7279\u5f81\u8ddd\u79bb\u4f18\u5316\u6709\u6548\u51cf\u5c11\u4e86\u5e7b\u89c9\u3002", "conclusion": "HS\u662f\u8861\u91cf\u548c\u51cf\u5c11GSR\u5e7b\u89c9\u7684\u6709\u6548\u5de5\u5177\uff0c\u7279\u5f81\u8ddd\u79bb\u4f18\u5316\u65b9\u6cd5\u4e3a\u6a21\u578b\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.14432", "pdf": "https://arxiv.org/pdf/2507.14432", "abs": "https://arxiv.org/abs/2507.14432", "authors": ["Han Gong", "Qiyue Li", "Zhi Liu", "Hao Zhou", "Peng Yuan Zhou", "Zhu Li", "Jie Li"], "title": "Adaptive 3D Gaussian Splatting Video Streaming", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the quality of volumetric video representation. Meanwhile, in contrast to conventional volumetric video, 3DGS video poses significant challenges for streaming due to its substantially larger data volume and the heightened complexity involved in compression and transmission. To address these issues, we introduce an innovative framework for 3DGS volumetric video streaming. Specifically, we design a 3DGS video construction method based on the Gaussian deformation field. By employing hybrid saliency tiling and differentiated quality modeling of 3DGS video, we achieve efficient data compression and adaptation to bandwidth fluctuations while ensuring high transmission quality. Then we build a complete 3DGS video streaming system and validate the transmission performance. Through experimental evaluation, our method demonstrated superiority over existing approaches in various aspects, including video quality, compression effectiveness, and transmission rate.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u53d8\u5f62\u573a\u76843DGS\u89c6\u9891\u6d41\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u663e\u8457\u6027\u5206\u5757\u548c\u5dee\u5f02\u5316\u8d28\u91cf\u5efa\u6a21\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u538b\u7f29\u548c\u5e26\u5bbd\u9002\u5e94\u3002", "motivation": "3DGS\u89c6\u9891\u56e0\u5176\u6570\u636e\u91cf\u5927\u548c\u4f20\u8f93\u590d\u6742\u5ea6\u9ad8\uff0c\u5bf9\u6d41\u5a92\u4f53\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u9ad8\u65af\u53d8\u5f62\u573a\u76843DGS\u89c6\u9891\u6784\u5efa\u65b9\u6cd5\uff0c\u7ed3\u5408\u6df7\u5408\u663e\u8457\u6027\u5206\u5757\u548c\u5dee\u5f02\u5316\u8d28\u91cf\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u9891\u8d28\u91cf\u3001\u538b\u7f29\u6548\u679c\u548c\u4f20\u8f93\u901f\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e863DGS\u89c6\u9891\u6d41\u7684\u9ad8\u6548\u4f20\u8f93\u95ee\u9898\u3002"}}
{"id": "2507.14449", "pdf": "https://arxiv.org/pdf/2507.14449", "abs": "https://arxiv.org/abs/2507.14449", "authors": ["Zhe Cao", "Jin Zhang", "Ruiheng Zhang"], "title": "IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark", "categories": ["cs.CV"], "comment": "11 pages, 7 figures. This paper is accepted by ICCV 2025", "summary": "Real-world infrared imagery presents unique challenges for vision-language models due to the scarcity of aligned text data and domain-specific characteristics. Although existing methods have advanced the field, their reliance on synthetic infrared images generated through style transfer from visible images, which limits their ability to capture the unique characteristics of the infrared modality. To address this, we propose IRGPT, the first multi-modal large language model for real-world infrared images, built upon a large-scale InfraRed-Text Dataset (IR-TD) comprising over 260K authentic image-text pairs. The proposed IR-TD dataset contains real infrared images paired with meticulously handcrafted texts, where the initial drafts originated from two complementary processes: (1) LLM-generated descriptions of visible images, and (2) rule-based descriptions of annotations. Furthermore, we introduce a bi-cross-modal curriculum transfer learning strategy that systematically transfers knowledge from visible to infrared domains by considering the difficulty scores of both infrared-visible and infrared-text. Evaluated on a benchmark of 9 tasks (e.g., recognition, grounding), IRGPT achieves state-of-the-art performance even compared with larger-scale models.", "AI": {"tldr": "IRGPT\u662f\u4e00\u79cd\u9488\u5bf9\u771f\u5b9e\u7ea2\u5916\u56fe\u50cf\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u7ea2\u5916-\u6587\u672c\u6570\u636e\u96c6\uff08IR-TD\uff09\u548c\u53cc\u8de8\u6a21\u6001\u8bfe\u7a0b\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u7ea2\u5916\u56fe\u50cf\u9886\u57df\u6570\u636e\u7a00\u7f3a\u548c\u7279\u6027\u72ec\u7279\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u771f\u5b9e\u7ea2\u5916\u56fe\u50cf\u56e0\u6570\u636e\u7a00\u7f3a\u548c\u9886\u57df\u7279\u6027\u5bfc\u81f4\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u53d7\u9650\u95ee\u9898\u3002", "method": "\u6784\u5efaIR-TD\u6570\u636e\u96c6\uff0826\u4e07\u771f\u5b9e\u7ea2\u5916\u56fe\u50cf-\u6587\u672c\u5bf9\uff09\uff0c\u63d0\u51fa\u53cc\u8de8\u6a21\u6001\u8bfe\u7a0b\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\uff0c\u4ece\u53ef\u89c1\u5149\u5230\u7ea2\u5916\u57df\u7cfb\u7edf\u8fc1\u79fb\u77e5\u8bc6\u3002", "result": "\u57289\u9879\u4efb\u52a1\uff08\u5982\u8bc6\u522b\u3001\u5b9a\u4f4d\uff09\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "IRGPT\u901a\u8fc7\u771f\u5b9e\u6570\u636e\u548c\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ea2\u5916\u56fe\u50cf\u7684\u591a\u6a21\u6001\u5904\u7406\u80fd\u529b\u3002"}}
{"id": "2507.14454", "pdf": "https://arxiv.org/pdf/2507.14454", "abs": "https://arxiv.org/abs/2507.14454", "authors": ["Han Gong", "Qiyue Li", "Jie Li", "Zhi Liu"], "title": "Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation", "categories": ["cs.CV", "cs.MM", "eess.IV"], "comment": null, "summary": "3D Gaussian splatting video (3DGS) streaming has recently emerged as a research hotspot in both academia and industry, owing to its impressive ability to deliver immersive 3D video experiences. However, research in this area is still in its early stages, and several fundamental challenges, such as tiling, quality assessment, and bitrate adaptation, require further investigation. In this paper, we tackle these challenges by proposing a comprehensive set of solutions. Specifically, we propose an adaptive 3DGS tiling technique guided by saliency analysis, which integrates both spatial and temporal features. Each tile is encoded into versions possessing dedicated deformation fields and multiple quality levels for adaptive selection. We also introduce a novel quality assessment framework for 3DGS video that jointly evaluates spatial-domain degradation in 3DGS representations during streaming and the quality of the resulting 2D rendered images. Additionally, we develop a meta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS video streaming, achieving optimal performance across varying network conditions. Extensive experiments demonstrate that our proposed approaches significantly outperform state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf93D\u9ad8\u65af\u6cfc\u6e85\u89c6\u9891\uff083DGS\uff09\u6d41\u5a92\u4f53\u7684\u5168\u9762\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u62ec\u57fa\u4e8e\u663e\u8457\u6027\u5206\u6790\u7684\u81ea\u9002\u5e94\u5206\u5757\u6280\u672f\u3001\u65b0\u578b\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\u548c\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u6bd4\u7279\u7387\u7b97\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "3DGS\u6d41\u5a92\u4f53\u5728\u63d0\u4f9b\u6c89\u6d78\u5f0f3D\u89c6\u9891\u4f53\u9a8c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ecd\u9762\u4e34\u5206\u5757\u3001\u8d28\u91cf\u8bc4\u4f30\u548c\u6bd4\u7279\u7387\u9002\u5e94\u7b49\u57fa\u7840\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86\u81ea\u9002\u5e943DGS\u5206\u5757\u6280\u672f\uff08\u7ed3\u5408\u663e\u8457\u6027\u548c\u65f6\u7a7a\u7279\u5f81\uff09\u3001\u65b0\u578b\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff08\u8054\u5408\u8bc4\u4f303DGS\u8868\u793a\u548c2D\u6e32\u67d3\u56fe\u50cf\u8d28\u91cf\uff09\u548c\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u6bd4\u7279\u7387\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u672c\u6587\u7684\u89e3\u51b3\u65b9\u6848\u4e3a3DGS\u6d41\u5a92\u4f53\u7684\u5173\u952e\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.14500", "pdf": "https://arxiv.org/pdf/2507.14500", "abs": "https://arxiv.org/abs/2507.14500", "authors": ["Zhiyuan Hua", "Dehao Yuan", "Cornelia Ferm\u00fcller"], "title": "Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "This paper introduces a robust framework for motion segmentation and egomotion estimation using event-based normal flow, tailored specifically for neuromorphic vision sensors. In contrast to traditional methods that rely heavily on optical flow or explicit depth estimation, our approach exploits the sparse, high-temporal-resolution event data and incorporates geometric constraints between normal flow, scene structure, and inertial measurements. The proposed optimization-based pipeline iteratively performs event over-segmentation, isolates independently moving objects via residual analysis, and refines segmentations using hierarchical clustering informed by motion similarity and temporal consistency. Experimental results on the EVIMO2v2 dataset validate that our method achieves accurate segmentation and translational motion estimation without requiring full optical flow computation. This approach demonstrates significant advantages at object boundaries and offers considerable potential for scalable, real-time robotic and navigation applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u6cd5\u5411\u6d41\u7684\u8fd0\u52a8\u5206\u5272\u4e0e\u81ea\u8fd0\u52a8\u4f30\u8ba1\u6846\u67b6\uff0c\u5229\u7528\u795e\u7ecf\u5f62\u6001\u89c6\u89c9\u4f20\u611f\u5668\u6570\u636e\uff0c\u901a\u8fc7\u51e0\u4f55\u7ea6\u675f\u548c\u4f18\u5316\u6d41\u7a0b\u5b9e\u73b0\u9ad8\u6548\u5206\u5272\u4e0e\u8fd0\u52a8\u4f30\u8ba1\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5149\u6d41\u6216\u6df1\u5ea6\u4f30\u8ba1\uff0c\u800c\u795e\u7ecf\u5f62\u6001\u4f20\u611f\u5668\u7684\u9ad8\u65f6\u7a7a\u5206\u8fa8\u7387\u6570\u636e\u672a\u88ab\u5145\u5206\u5229\u7528\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u4e8b\u4ef6\u6570\u636e\u7279\u6027\uff0c\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u8fd0\u52a8\u5206\u5272\u4e0e\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4f18\u5316\u6d41\u7a0b\uff0c\u5305\u62ec\u4e8b\u4ef6\u8fc7\u5206\u5272\u3001\u6b8b\u5dee\u5206\u6790\u5206\u79bb\u8fd0\u52a8\u7269\u4f53\u3001\u57fa\u4e8e\u8fd0\u52a8\u76f8\u4f3c\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u5c42\u6b21\u805a\u7c7b\u3002", "result": "\u5728EVIMO2v2\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u65e0\u9700\u5b8c\u6574\u5149\u6d41\u8ba1\u7b97\u5373\u53ef\u5b9e\u73b0\u7cbe\u786e\u5206\u5272\u548c\u8fd0\u52a8\u4f30\u8ba1\uff0c\u5c24\u5176\u5728\u7269\u4f53\u8fb9\u754c\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5b9e\u65f6\u673a\u5668\u4eba\u53ca\u5bfc\u822a\u5e94\u7528\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u5c55\u73b0\u4e86\u4e8b\u4ef6\u6570\u636e\u7684\u72ec\u7279\u4f18\u52bf\u3002"}}
{"id": "2507.14501", "pdf": "https://arxiv.org/pdf/2507.14501", "abs": "https://arxiv.org/abs/2507.14501", "authors": ["Jiahui Zhang", "Yuelei Li", "Anpei Chen", "Muyu Xu", "Kunhao Liu", "Jianyuan Wang", "Xiao-Xiao Long", "Hanxue Liang", "Zexiang Xu", "Hao Su", "Christian Theobalt", "Christian Rupprecht", "Andrea Vedaldi", "Hanspeter Pfister", "Shijian Lu", "Fangneng Zhan"], "title": "Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey", "categories": ["cs.CV"], "comment": "A project page associated with this survey is available at   https://fnzhan.com/projects/Feed-Forward-3D", "summary": "3D reconstruction and view synthesis are foundational problems in computer vision, graphics, and immersive technologies such as augmented reality (AR), virtual reality (VR), and digital twins. Traditional methods rely on computationally intensive iterative optimization in a complex chain, limiting their applicability in real-world scenarios. Recent advances in feed-forward approaches, driven by deep learning, have revolutionized this field by enabling fast and generalizable 3D reconstruction and view synthesis. This survey offers a comprehensive review of feed-forward techniques for 3D reconstruction and view synthesis, with a taxonomy according to the underlying representation architectures including point cloud, 3D Gaussian Splatting (3DGS), Neural Radiance Fields (NeRF), etc. We examine key tasks such as pose-free reconstruction, dynamic 3D reconstruction, and 3D-aware image and video synthesis, highlighting their applications in digital humans, SLAM, robotics, and beyond. In addition, we review commonly used datasets with detailed statistics, along with evaluation protocols for various downstream tasks. We conclude by discussing open research challenges and promising directions for future work, emphasizing the potential of feed-forward approaches to advance the state of the art in 3D vision.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u524d\u9988\u65b9\u6cd5\u76843D\u91cd\u5efa\u548c\u89c6\u56fe\u5408\u6210\u6280\u672f\uff0c\u5206\u7c7b\u8ba8\u8bba\u4e86\u4e0d\u540c\u8868\u793a\u67b6\u6784\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5e94\u7528\u3001\u6570\u636e\u96c6\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf3D\u91cd\u5efa\u548c\u89c6\u56fe\u5408\u6210\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u5b9e\u9645\u573a\u666f\uff0c\u800c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u524d\u9988\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5feb\u901f\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u8ba8\u8bba\u70b9\u4e91\u30013D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u3001\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u7b49\u8868\u793a\u67b6\u6784\uff0c\u5206\u6790\u5176\u5728\u59ff\u6001\u65e0\u5173\u91cd\u5efa\u3001\u52a8\u60013D\u91cd\u5efa\u7b49\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u603b\u7ed3\u4e86\u524d\u9988\u65b9\u6cd5\u5728\u6570\u5b57\u4eba\u3001SLAM\u3001\u673a\u5668\u4eba\u7b49\u9886\u57df\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u4f9b\u4e86\u5e38\u7528\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\u7684\u8be6\u7ec6\u7edf\u8ba1\u3002", "conclusion": "\u524d\u9988\u65b9\u6cd5\u57283D\u89c6\u89c9\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4ecd\u5b58\u5728\u5f00\u653e\u6311\u6218\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u503c\u5f97\u5173\u6ce8\u3002"}}
{"id": "2507.14505", "pdf": "https://arxiv.org/pdf/2507.14505", "abs": "https://arxiv.org/abs/2507.14505", "authors": ["Jiahao Ma", "Tianyu Wang", "Miaomiao Liu", "David Ahmedt-Aristizabal", "Chuong Nguyen"], "title": "DCHM: Depth-Consistent Human Modeling for Multiview Detection", "categories": ["cs.CV"], "comment": "multi-view detection, sparse-view reconstruction", "summary": "Multiview pedestrian detection typically involves two stages: human modeling and pedestrian localization. Human modeling represents pedestrians in 3D space by fusing multiview information, making its quality crucial for detection accuracy. However, existing methods often introduce noise and have low precision. While some approaches reduce noise by fitting on costly multiview 3D annotations, they often struggle to generalize across diverse scenes. To eliminate reliance on human-labeled annotations and accurately model humans, we propose Depth-Consistent Human Modeling (DCHM), a framework designed for consistent depth estimation and multiview fusion in global coordinates. Specifically, our proposed pipeline with superpixel-wise Gaussian Splatting achieves multiview depth consistency in sparse-view, large-scaled, and crowded scenarios, producing precise point clouds for pedestrian localization. Extensive validations demonstrate that our method significantly reduces noise during human modeling, outperforming previous state-of-the-art baselines. Additionally, to our knowledge, DCHM is the first to reconstruct pedestrians and perform multiview segmentation in such a challenging setting. Code is available on the \\href{https://jiahao-ma.github.io/DCHM/}{project page}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDCHM\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u4e00\u81f4\u6027\u548c\u591a\u89c6\u56fe\u878d\u5408\u6539\u8fdb\u884c\u4eba\u68c0\u6d4b\u4e2d\u7684\u4eba\u4f53\u5efa\u6a21\uff0c\u663e\u8457\u51cf\u5c11\u566a\u58f0\u5e76\u63d0\u5347\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4eba\u4f53\u5efa\u6a21\u4e2d\u5f15\u5165\u566a\u58f0\u4e14\u7cbe\u5ea6\u4f4e\uff0c\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u591a\u6837\u573a\u666f\u3002", "method": "\u91c7\u7528\u8d85\u50cf\u7d20\u7ea7\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u5b9e\u73b0\u7a00\u758f\u89c6\u56fe\u3001\u5927\u89c4\u6a21\u548c\u62e5\u6324\u573a\u666f\u4e0b\u7684\u591a\u89c6\u56fe\u6df1\u5ea6\u4e00\u81f4\u6027\uff0c\u751f\u6210\u7cbe\u786e\u70b9\u4e91\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDCHM\u663e\u8457\u51cf\u5c11\u566a\u58f0\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u9996\u6b21\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\u91cd\u5efa\u884c\u4eba\u548c\u591a\u89c6\u56fe\u5206\u5272\u3002", "conclusion": "DCHM\u65e0\u9700\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\uff0c\u80fd\u51c6\u786e\u5efa\u6a21\u884c\u4eba\uff0c\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\uff0c\u662f\u884c\u4eba\u68c0\u6d4b\u9886\u57df\u7684\u91cd\u5927\u8fdb\u6b65\u3002"}}
{"id": "2507.14553", "pdf": "https://arxiv.org/pdf/2507.14553", "abs": "https://arxiv.org/abs/2507.14553", "authors": ["Xiaoran Wu"], "title": "Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Clutter in photos is a distraction preventing photographers from conveying the intended emotions or stories to the audience. Photography amateurs frequently include clutter in their photos due to unconscious negligence or the lack of experience in creating a decluttered, aesthetically appealing scene for shooting. We are thus motivated to develop a camera guidance system that provides solutions and guidance for clutter identification and removal. We estimate and visualize the contribution of objects to the overall aesthetics and content of a photo, based on which users can interactively identify clutter. Suggestions on getting rid of clutter, as well as a tool that removes cluttered objects computationally, are provided to guide users to deal with different kinds of clutter and improve their photographic work. Two technical novelties underpin interactions in our system: a clutter distinguishment algorithm with aesthetics evaluations for objects and an iterative image inpainting algorithm based on generative adversarial nets that reconstructs missing regions of removed objects for high-resolution images. User studies demonstrate that our system provides flexible interfaces and accurate algorithms that allow users to better identify distractions and take higher quality images within less time.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u76f8\u673a\u5f15\u5bfc\u7cfb\u7edf\uff0c\u5e2e\u52a9\u7528\u6237\u8bc6\u522b\u548c\u53bb\u9664\u7167\u7247\u4e2d\u7684\u6742\u4e71\u5185\u5bb9\uff0c\u63d0\u5347\u7167\u7247\u7f8e\u5b66\u8d28\u91cf\u3002", "motivation": "\u7167\u7247\u4e2d\u7684\u6742\u4e71\u5185\u5bb9\u4f1a\u5206\u6563\u6ce8\u610f\u529b\uff0c\u5f71\u54cd\u60c5\u611f\u4f20\u8fbe\uff0c\u4e1a\u4f59\u6444\u5f71\u5e08\u5e38\u56e0\u7ecf\u9a8c\u4e0d\u8db3\u800c\u5ffd\u89c6\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u7f8e\u5b66\u8bc4\u4f30\u7b97\u6cd5\u548c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u56fe\u50cf\u4fee\u590d\u6280\u672f\uff0c\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u6742\u4e71\u8bc6\u522b\u4e0e\u53bb\u9664\u5de5\u5177\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u7cfb\u7edf\u80fd\u5e2e\u52a9\u7528\u6237\u66f4\u5feb\u8bc6\u522b\u6742\u4e71\u5e76\u62cd\u6444\u66f4\u9ad8\u8d28\u91cf\u7684\u7167\u7247\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u7075\u6d3b\u754c\u9762\u548c\u7cbe\u51c6\u7b97\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7528\u6237\u7684\u6444\u5f71\u4f53\u9a8c\u548c\u7167\u7247\u8d28\u91cf\u3002"}}
{"id": "2507.14575", "pdf": "https://arxiv.org/pdf/2507.14575", "abs": "https://arxiv.org/abs/2507.14575", "authors": ["Andrea Moschetto", "Lemuel Puglisi", "Alec Sargood", "Pierluigi Dell'Acqua", "Francesco Guarnera", "Sebastiano Battiato", "Daniele Rav\u00ec"], "title": "Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image contrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering distinct diagnostic insights. However, acquiring all desired modalities increases scan time and cost, motivating research into computational methods for cross-modal synthesis. To address this, recent approaches aim to synthesize missing MRI contrasts from those already acquired, reducing acquisition time while preserving diagnostic quality. Image-to-image (I2I) translation provides a promising framework for this task. In this paper, we present a comprehensive benchmark of generative models$\\unicode{x2013}$specifically, Generative Adversarial Networks (GANs), diffusion models, and flow matching (FM) techniques$\\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All frameworks are implemented with comparable settings and evaluated on three publicly available MRI datasets of healthy adults. Our quantitative and qualitative analyses show that the GAN-based Pix2Pix model outperforms diffusion and FM-based methods in terms of structural fidelity, image quality, and computational efficiency. Consistent with existing literature, these results suggest that flow-based models are prone to overfitting on small datasets and simpler tasks, and may require more data to match or surpass GAN performance. These findings offer practical guidance for deploying I2I translation techniques in real-world MRI workflows and highlight promising directions for future research in cross-modal medical image synthesis. Code and models are publicly available at https://github.com/AndreaMoschetto/medical-I2I-benchmark.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86GAN\u3001\u6269\u6563\u6a21\u578b\u548c\u6d41\u5339\u914d\u6280\u672f\u5728MRI\u56fe\u50cf\u5408\u6210\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0Pix2Pix\u5728\u7ed3\u6784\u4fdd\u771f\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "\u51cf\u5c11MRI\u626b\u63cf\u65f6\u95f4\u548c\u6210\u672c\uff0c\u901a\u8fc7\u8ba1\u7b97\u5408\u6210\u7f3a\u5931\u7684\u6a21\u6001\u3002", "method": "\u4f7f\u7528GAN\u3001\u6269\u6563\u6a21\u578b\u548c\u6d41\u5339\u914d\u6280\u672f\u8fdb\u884cT1w\u5230T2w\u76842D MRI\u56fe\u50cf\u5408\u6210\uff0c\u5e76\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u3002", "result": "Pix2Pix\u5728\u7ed3\u6784\u4fdd\u771f\u5ea6\u3001\u56fe\u50cf\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "GAN\u5728\u5c0f\u578b\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u4e3a\u5b9e\u9645MRI\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2507.14596", "pdf": "https://arxiv.org/pdf/2507.14596", "abs": "https://arxiv.org/abs/2507.14596", "authors": ["Doriand Petit", "Steve Bourgeois", "Vincent Gay-Bellile", "Florian Chabot", "Lo\u00efc Barthe"], "title": "DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF", "categories": ["cs.CV"], "comment": "Published at ICCV'25", "summary": "3D semantic segmentation provides high-level scene understanding for applications in robotics, autonomous systems, \\textit{etc}. Traditional methods adapt exclusively to either task-specific goals (open-vocabulary segmentation) or scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the first method addressing the broader problem of 3D Open-Vocabulary Sub-concepts Discovery, which aims to provide a 3D semantic segmentation that adapts to both the scene and user queries. We build DiSCO-3D on Neural Fields representations, combining unsupervised segmentation with weak open-vocabulary guidance. Our evaluations demonstrate that DiSCO-3D achieves effective performance in Open-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in the edge cases of both open-vocabulary and unsupervised segmentation.", "AI": {"tldr": "DiSCO-3D\u662f\u4e00\u79cd\u7ed3\u5408\u65e0\u76d1\u7763\u5206\u5272\u548c\u5f00\u653e\u8bcd\u6c47\u6307\u5bfc\u76843D\u5f00\u653e\u8bcd\u6c47\u5b50\u6982\u5ff5\u53d1\u73b0\u65b9\u6cd5\uff0c\u5728\u5f00\u653e\u8bcd\u6c47\u548c\u65e0\u76d1\u7763\u5206\u5272\u7684\u8fb9\u7f18\u6848\u4f8b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4ec5\u9002\u5e94\u7279\u5b9a\u4efb\u52a1\u76ee\u6807\u6216\u573a\u666f\u5185\u5bb9\uff0c\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u573a\u666f\u548c\u7528\u6237\u67e5\u8be2\u7684\u9700\u6c42\u3002DiSCO-3D\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u57fa\u4e8e\u795e\u7ecf\u573a\u8868\u793a\uff0c\u7ed3\u5408\u65e0\u76d1\u7763\u5206\u5272\u548c\u5f31\u5f00\u653e\u8bcd\u6c47\u6307\u5bfc\u3002", "result": "\u5728\u5f00\u653e\u8bcd\u6c47\u5b50\u6982\u5ff5\u53d1\u73b0\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u5f00\u653e\u8bcd\u6c47\u548c\u65e0\u76d1\u7763\u5206\u5272\u7684\u8fb9\u7f18\u6848\u4f8b\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "DiSCO-3D\u4e3a3D\u8bed\u4e49\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u540c\u65f6\u9002\u5e94\u573a\u666f\u548c\u7528\u6237\u67e5\u8be2\u3002"}}
{"id": "2507.14632", "pdf": "https://arxiv.org/pdf/2507.14632", "abs": "https://arxiv.org/abs/2507.14632", "authors": ["Haiquan Wen", "Tianxiao Li", "Zhenglin Huang", "Yiwei He", "Guangliang Cheng"], "title": "BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in generative AI have dramatically improved image and video synthesis capabilities, significantly increasing the risk of misinformation through sophisticated fake content. In response, detection methods have evolved from traditional approaches to multimodal large language models (MLLMs), offering enhanced transparency and interpretability in identifying synthetic media. However, current detection systems remain fundamentally limited by their single-modality design. These approaches analyze images or videos separately, making them ineffective against synthetic content that combines multiple media formats. To address these challenges, we introduce \\textbf{BusterX++}, a novel framework designed specifically for cross-modal detection and explanation of synthetic media. Our approach incorporates an advanced reinforcement learning (RL) post-training strategy that eliminates cold-start. Through Multi-stage Training, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and substantial performance improvements. To enable comprehensive evaluation, we also present \\textbf{GenBuster++}, a cross-modal benchmark leveraging state-of-the-art image and video generation techniques. This benchmark comprises 4,000 images and video clips, meticulously curated by human experts using a novel filtering methodology to ensure high quality, diversity, and real-world applicability. Extensive experiments demonstrate the effectiveness and generalizability of our approach.", "AI": {"tldr": "BusterX++\u662f\u4e00\u4e2a\u65b0\u578b\u8de8\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u89e3\u91ca\u5408\u6210\u5a92\u4f53\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7b56\u7565\u548c\u591a\u9636\u6bb5\u8bad\u7ec3\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u751f\u6210\u5f0fAI\u7684\u8fdb\u6b65\u589e\u52a0\u4e86\u865a\u5047\u4fe1\u606f\u7684\u98ce\u9669\uff0c\u73b0\u6709\u5355\u6a21\u6001\u68c0\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u5e94\u5bf9\u591a\u6a21\u6001\u5408\u6210\u5185\u5bb9\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u591a\u9636\u6bb5\u8bad\u7ec3\u3001\u601d\u7ef4\u5956\u52b1\u548c\u6df7\u5408\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eBusterX++\u5728\u6027\u80fd\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u5e76\u5177\u6709\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "BusterX++\u4e3a\u8de8\u6a21\u6001\u5408\u6210\u5a92\u4f53\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5f15\u5165\u9ad8\u8d28\u91cf\u57fa\u51c6GenBuster++\u3002"}}
{"id": "2507.14797", "pdf": "https://arxiv.org/pdf/2507.14797", "abs": "https://arxiv.org/abs/2507.14797", "authors": ["Beier Zhu", "Ruoyu Wang", "Tong Zhao", "Hanwang Zhang", "Chi Zhang"], "title": "Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models", "categories": ["cs.CV"], "comment": "To appear in ICCV 2025", "summary": "Diffusion models (DMs) have achieved state-of-the-art generative performance but suffer from high sampling latency due to their sequential denoising nature. Existing solver-based acceleration methods often face image quality degradation under a low-latency budget. In this paper, we propose the Ensemble Parallel Direction solver (dubbed as \\ours), a novel ODE solver that mitigates truncation errors by incorporating multiple parallel gradient evaluations in each ODE step. Importantly, since the additional gradient computations are independent, they can be fully parallelized, preserving low-latency sampling.   Our method optimizes a small set of learnable parameters in a distillation fashion, ensuring minimal training overhead.   In addition, our method can serve as a plugin to improve existing ODE samplers. Extensive experiments on various image synthesis benchmarks demonstrate the effectiveness of our \\ours~in achieving high-quality and low-latency sampling. For example, at the same latency level of 5 NFE, EPD achieves an FID of 4.47 on CIFAR-10, 7.97 on FFHQ, 8.17 on ImageNet, and 8.26 on LSUN Bedroom, surpassing existing learning-based solvers by a significant margin. Codes are available in https://github.com/BeierZhu/EPD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEPD\u7684\u65b0\u578bODE\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u5e76\u884c\u68af\u5ea6\u8bc4\u4f30\u51cf\u5c11\u622a\u65ad\u8bef\u5dee\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u4f4e\u5ef6\u8fdf\u91c7\u6837\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u56e0\u987a\u5e8f\u53bb\u566a\u5bfc\u81f4\u91c7\u6837\u5ef6\u8fdf\u9ad8\uff0c\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u5728\u4f4e\u5ef6\u8fdf\u4e0b\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\u3002", "method": "EPD\u901a\u8fc7\u5e76\u884c\u68af\u5ea6\u8bc4\u4f30\u51cf\u5c11\u622a\u65ad\u8bef\u5dee\uff0c\u53ef\u5b8c\u5168\u5e76\u884c\u5316\uff0c\u4e14\u901a\u8fc7\u84b8\u998f\u65b9\u5f0f\u4f18\u5316\u5c11\u91cf\u53ef\u5b66\u4e60\u53c2\u6570\u3002", "result": "\u5728\u591a\u4e2a\u56fe\u50cf\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEPD\u57285 NFE\u5ef6\u8fdf\u4e0b\u8868\u73b0\u4f18\u5f02\uff0cFID\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "EPD\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684ODE\u6c42\u89e3\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u6548\u7387\u548c\u8d28\u91cf\u3002"}}
{"id": "2507.14801", "pdf": "https://arxiv.org/pdf/2507.14801", "abs": "https://arxiv.org/abs/2507.14801", "authors": ["Xiangyu Chen", "Kaiwen Zhu", "Yuandong Pu", "Shuo Cao", "Xiaohui Li", "Wenlong Zhang", "Yihao Liu", "Yu Qiao", "Jiantao Zhou", "Chao Dong"], "title": "Exploring Scalable Unified Modeling for General Low-Level Vision", "categories": ["cs.CV"], "comment": null, "summary": "Low-level vision involves a wide spectrum of tasks, including image restoration, enhancement, stylization, and feature extraction, which differ significantly in both task formulation and output domains. To address the challenge of unified modeling across such diverse tasks, we propose a Visual task Prompt-based Image Processing (VPIP) framework that leverages input-target image pairs as visual prompts to guide the model in performing a variety of low-level vision tasks. The framework comprises an end-to-end image processing backbone, a prompt encoder, and a prompt interaction module, enabling flexible integration with various architectures and effective utilization of task-specific visual representations. Based on this design, we develop a unified low-level vision model, GenLV, and evaluate its performance across multiple representative tasks. To explore the scalability of this approach, we extend the framework along two dimensions: model capacity and task diversity. We construct a large-scale benchmark consisting of over 100 low-level vision tasks and train multiple versions of the model with varying scales. Experimental results show that the proposed method achieves considerable performance across a wide range of tasks. Notably, increasing the number of training tasks enhances generalization, particularly for tasks with limited data, indicating the model's ability to learn transferable representations through joint training. Further evaluations in zero-shot generalization, few-shot transfer, and task-specific fine-tuning scenarios demonstrate the model's strong adaptability, confirming the effectiveness, scalability, and potential of the proposed framework as a unified foundation for general low-level vision modeling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u63d0\u793a\u7684\u7edf\u4e00\u4f4e\u5c42\u89c6\u89c9\u4efb\u52a1\u5904\u7406\u6846\u67b6VPIP\uff0c\u901a\u8fc7\u8f93\u5165-\u76ee\u6807\u56fe\u50cf\u5bf9\u4f5c\u4e3a\u63d0\u793a\uff0c\u652f\u6301\u591a\u79cd\u4efb\u52a1\u3002\u5f00\u53d1\u4e86\u7edf\u4e00\u6a21\u578bGenLV\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u89e3\u51b3\u4f4e\u5c42\u89c6\u89c9\u4efb\u52a1\u591a\u6837\u6027\u5e26\u6765\u7684\u7edf\u4e00\u5efa\u6a21\u6311\u6218\uff0c\u63a2\u7d22\u591a\u4efb\u52a1\u8054\u5408\u8bad\u7ec3\u7684\u6f5c\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u5305\u542b\u56fe\u50cf\u5904\u7406\u4e3b\u5e72\u3001\u63d0\u793a\u7f16\u7801\u5668\u548c\u63d0\u793a\u4ea4\u4e92\u6a21\u5757\u7684VPIP\u6846\u67b6\uff0c\u652f\u6301\u7075\u6d3b\u67b6\u6784\u96c6\u6210\u548c\u4efb\u52a1\u7279\u5b9a\u8868\u793a\u5229\u7528\u3002", "result": "\u5728\u591a\u4efb\u52a1\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u589e\u52a0\u8bad\u7ec3\u4efb\u52a1\u6570\u91cf\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u5c24\u5176\u5728\u6570\u636e\u6709\u9650\u7684\u4efb\u52a1\u4e0a\u3002", "conclusion": "VPIP\u6846\u67b6\u6709\u6548\u3001\u53ef\u6269\u5c55\uff0c\u4e3a\u96f6\u6837\u672c\u6cdb\u5316\u3001\u5c11\u6837\u672c\u8fc1\u79fb\u548c\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u63d0\u4f9b\u4e86\u7edf\u4e00\u57fa\u7840\u3002"}}
{"id": "2507.14809", "pdf": "https://arxiv.org/pdf/2507.14809", "abs": "https://arxiv.org/abs/2507.14809", "authors": ["Zesen Zhong", "Duomin Zhang", "Yijia Li"], "title": "Light Future: Multimodal Action Frame Prediction via InstructPix2Pix", "categories": ["cs.CV", "cs.MM", "cs.RO", "I.2.10; I.4.8"], "comment": "9 pages including appendix, 5 tables, 8 figures, to be submitted to   WACV 2026", "summary": "Predicting future motion trajectories is a critical capability across domains such as robotics, autonomous systems, and human activity forecasting, enabling safer and more intelligent decision-making. This paper proposes a novel, efficient, and lightweight approach for robot action prediction, offering significantly reduced computational cost and inference latency compared to conventional video prediction models. Importantly, it pioneers the adaptation of the InstructPix2Pix model for forecasting future visual frames in robotic tasks, extending its utility beyond static image editing. We implement a deep learning-based visual prediction framework that forecasts what a robot will observe 100 frames (10 seconds) into the future, given a current image and a textual instruction. We repurpose and fine-tune the InstructPix2Pix model to accept both visual and textual inputs, enabling multimodal future frame prediction. Experiments on the RoboTWin dataset (generated based on real-world scenarios) demonstrate that our method achieves superior SSIM and PSNR compared to state-of-the-art baselines in robot action prediction tasks. Unlike conventional video prediction models that require multiple input frames, heavy computation, and slow inference latency, our approach only needs a single image and a text prompt as input. This lightweight design enables faster inference, reduced GPU demands, and flexible multimodal control, particularly valuable for applications like robotics and sports motion trajectory analytics, where motion trajectory precision is prioritized over visual fidelity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u9884\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528InstructPix2Pix\u6a21\u578b\u8fdb\u884c\u591a\u6a21\u6001\u672a\u6765\u5e27\u9884\u6d4b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u548c\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u9884\u6d4b\u672a\u6765\u8fd0\u52a8\u8f68\u8ff9\u5728\u673a\u5668\u4eba\u3001\u81ea\u4e3b\u7cfb\u7edf\u548c\u4eba\u7c7b\u6d3b\u52a8\u9884\u6d4b\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u6709\u52a9\u4e8e\u66f4\u5b89\u5168\u548c\u667a\u80fd\u7684\u51b3\u7b56\u3002", "method": "\u901a\u8fc7\u6539\u8fdbInstructPix2Pix\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u63a5\u53d7\u89c6\u89c9\u548c\u6587\u672c\u8f93\u5165\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u672a\u6765\u5e27\u9884\u6d4b\u3002\u4ec5\u9700\u5355\u5f20\u56fe\u50cf\u548c\u6587\u672c\u63d0\u793a\u5373\u53ef\u9884\u6d4b\u672a\u676510\u79d2\u7684\u89c6\u89c9\u5e27\u3002", "result": "\u5728RoboTWin\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728SSIM\u548cPSNR\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u52a8\u4f5c\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u7075\u6d3b\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5bf9\u8fd0\u52a8\u8f68\u8ff9\u7cbe\u5ea6\u8981\u6c42\u9ad8\u7684\u573a\u666f\u3002"}}
{"id": "2507.14826", "pdf": "https://arxiv.org/pdf/2507.14826", "abs": "https://arxiv.org/abs/2507.14826", "authors": ["Fu-Jen Tsai", "Yan-Tsung Peng", "Yen-Yu Lin", "Chia-Wen Lin"], "title": "PHATNet: A Physics-guided Haze Transfer Network for Domain-adaptive Real-world Image Dehazing", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "Image dehazing aims to remove unwanted hazy artifacts in images. Although previous research has collected paired real-world hazy and haze-free images to improve dehazing models' performance in real-world scenarios, these models often experience significant performance drops when handling unseen real-world hazy images due to limited training data. This issue motivates us to develop a flexible domain adaptation method to enhance dehazing performance during testing. Observing that predicting haze patterns is generally easier than recovering clean content, we propose the Physics-guided Haze Transfer Network (PHATNet) which transfers haze patterns from unseen target domains to source-domain haze-free images, creating domain-specific fine-tuning sets to update dehazing models for effective domain adaptation. Additionally, we introduce a Haze-Transfer-Consistency loss and a Content-Leakage Loss to enhance PHATNet's disentanglement ability. Experimental results demonstrate that PHATNet significantly boosts state-of-the-art dehazing models on benchmark real-world image dehazing datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u5f15\u5bfc\u7684\u96fe\u973e\u8f6c\u79fb\u7f51\u7edc\uff08PHATNet\uff09\uff0c\u901a\u8fc7\u5c06\u76ee\u6807\u57df\u7684\u96fe\u973e\u6a21\u5f0f\u8f6c\u79fb\u5230\u6e90\u57df\u7684\u65e0\u96fe\u56fe\u50cf\u4e0a\uff0c\u751f\u6210\u7279\u5b9a\u9886\u57df\u7684\u5fae\u8c03\u6570\u636e\u96c6\uff0c\u4ece\u800c\u63d0\u5347\u53bb\u96fe\u6a21\u578b\u7684\u57df\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u53bb\u96fe\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u7684\u771f\u5b9e\u96fe\u973e\u56fe\u50cf\u4e0a\u6027\u80fd\u4e0b\u964d\u660e\u663e\uff0c\u4e3b\u8981\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u6709\u9650\u3002", "method": "\u63d0\u51faPHATNet\uff0c\u901a\u8fc7\u96fe\u973e\u8f6c\u79fb\u751f\u6210\u5fae\u8c03\u6570\u636e\u96c6\uff0c\u5e76\u5f15\u5165\u96fe\u973e\u8f6c\u79fb\u4e00\u81f4\u6027\u548c\u5185\u5bb9\u6cc4\u6f0f\u635f\u5931\u4ee5\u589e\u5f3a\u89e3\u8026\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPHATNet\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u53bb\u96fe\u6a21\u578b\u5728\u771f\u5b9e\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "PHATNet\u901a\u8fc7\u57df\u9002\u5e94\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u53bb\u96fe\u6a21\u578b\u5728\u672a\u89c1\u6570\u636e\u4e0a\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002"}}
{"id": "2507.14833", "pdf": "https://arxiv.org/pdf/2507.14833", "abs": "https://arxiv.org/abs/2507.14833", "authors": ["Haoxuan Zhang", "Wenju Cui", "Yuzhu Cao", "Tao Tan", "Jie Liu", "Yunsong Peng", "Jian Zheng"], "title": "Paired Image Generation with Diffusion-Guided Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The segmentation of mass lesions in digital breast tomosynthesis (DBT) images is very significant for the early screening of breast cancer. However, the high-density breast tissue often leads to high concealment of the mass lesions, which makes manual annotation difficult and time-consuming. As a result, there is a lack of annotated data for model training. Diffusion models are commonly used for data augmentation, but the existing methods face two challenges. First, due to the high concealment of lesions, it is difficult for the model to learn the features of the lesion area. This leads to the low generation quality of the lesion areas, thus limiting the quality of the generated images. Second, existing methods can only generate images and cannot generate corresponding annotations, which restricts the usability of the generated images in supervised training. In this work, we propose a paired image generation method. The method does not require external conditions and can achieve the generation of paired images by training an extra diffusion guider for the conditional diffusion model. During the experimental phase, we generated paired DBT slices and mass lesion masks. Then, we incorporated them into the supervised training process of the mass lesion segmentation task. The experimental results show that our method can improve the generation quality without external conditions. Moreover, it contributes to alleviating the shortage of annotated data, thus enhancing the performance of downstream tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u6761\u4ef6\u7684\u914d\u5bf9\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u4e73\u817a\u65ad\u5c42\u626b\u63cf\u56fe\u50cf\u4e2d\u75c5\u7076\u5206\u5272\u7684\u6570\u636e\u6807\u6ce8\u4e0d\u8db3\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u751f\u6210\u8d28\u91cf\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u4e73\u817a\u65ad\u5c42\u626b\u63cf\u56fe\u50cf\u4e2d\u75c5\u7076\u7684\u9ad8\u9690\u853d\u6027\u5bfc\u81f4\u6807\u6ce8\u56f0\u96be\uff0c\u73b0\u6709\u6269\u6563\u6a21\u578b\u751f\u6210\u8d28\u91cf\u4f4e\u4e14\u65e0\u6cd5\u751f\u6210\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u76d1\u7763\u8bad\u7ec3\u7684\u6548\u679c\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3\u989d\u5916\u7684\u6269\u6563\u5f15\u5bfc\u5668\uff0c\u5b9e\u73b0\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u914d\u5bf9\u56fe\u50cf\u751f\u6210\uff0c\u751f\u6210\u4e73\u817a\u65ad\u5c42\u626b\u63cf\u5207\u7247\u548c\u75c5\u7076\u63a9\u7801\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u5916\u90e8\u6761\u4ef6\u5373\u53ef\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u7f13\u89e3\u6807\u6ce8\u6570\u636e\u77ed\u7f3a\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u914d\u5bf9\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u6807\u6ce8\u4e0d\u8db3\u548c\u751f\u6210\u8d28\u91cf\u4f4e\u7684\u95ee\u9898\uff0c\u5bf9\u4e73\u817a\u75c5\u7076\u5206\u5272\u4efb\u52a1\u6709\u663e\u8457\u5e2e\u52a9\u3002"}}
{"id": "2507.14851", "pdf": "https://arxiv.org/pdf/2507.14851", "abs": "https://arxiv.org/abs/2507.14851", "authors": ["Muhammad Kamran Janjua", "Amirhosein Ghasemabadi", "Kunlin Zhang", "Mohammad Salameh", "Chao Gao", "Di Niu"], "title": "Grounding Degradations in Natural Language for All-In-One Video Restoration", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": "17 pages", "summary": "In this work, we propose an all-in-one video restoration framework that grounds degradation-aware semantic context of video frames in natural language via foundation models, offering interpretable and flexible guidance. Unlike prior art, our method assumes no degradation knowledge in train or test time and learns an approximation to the grounded knowledge such that the foundation model can be safely disentangled during inference adding no extra cost. Further, we call for standardization of benchmarks in all-in-one video restoration, and propose two benchmarks in multi-degradation setting, three-task (3D) and four-task (4D), and two time-varying composite degradation benchmarks; one of the latter being our proposed dataset with varying snow intensity, simulating how weather degradations affect videos naturally. We compare our method with prior works and report state-of-the-art performance on all benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u591a\u4efb\u52a1\u89c6\u9891\u4fee\u590d\u6846\u67b6\uff0c\u65e0\u9700\u5148\u9a8c\u9000\u5316\u77e5\u8bc6\uff0c\u901a\u8fc7\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u6307\u5bfc\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u4fee\u590d\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u9000\u5316\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u548c\u57fa\u7840\u6a21\u578b\uff0c\u5b9e\u73b0\u65e0\u9700\u9000\u5316\u77e5\u8bc6\u7684\u89c6\u9891\u4fee\u590d\u3002", "method": "\u5229\u7528\u57fa\u7840\u6a21\u578b\u5c06\u89c6\u9891\u5e27\u7684\u9000\u5316\u611f\u77e5\u8bed\u4e49\u4e0a\u4e0b\u6587\u5d4c\u5165\u81ea\u7136\u8bed\u8a00\uff0c\u5b66\u4e60\u8fd1\u4f3c\u77e5\u8bc6\u4ee5\u5728\u63a8\u7406\u65f6\u65e0\u9700\u989d\u5916\u6210\u672c\u3002\u63d0\u51fa\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u5305\u62ec\u591a\u9000\u5316\u4efb\u52a1\u548c\u65f6\u95f4\u53d8\u5316\u590d\u5408\u9000\u5316\u6570\u636e\u96c6\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff08\u5305\u62ec\u65b0\u63d0\u51fa\u7684\u96ea\u5f3a\u5ea6\u53d8\u5316\u6570\u636e\u96c6\uff09\uff0c\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u89c6\u9891\u4fee\u590d\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u3001\u53ef\u89e3\u91ca\u4e14\u65e0\u9700\u9000\u5316\u77e5\u8bc6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u547c\u5401\u6807\u51c6\u5316\u57fa\u51c6\u7684\u5efa\u7acb\u3002"}}
{"id": "2507.14867", "pdf": "https://arxiv.org/pdf/2507.14867", "abs": "https://arxiv.org/abs/2507.14867", "authors": ["Zhaoqiang Xia", "Hexiang Huang", "Haoyu Chen", "Xiaoyi Feng", "Guoying Zhao"], "title": "Hybrid-supervised Hypergraph-enhanced Transformer for Micro-gesture Based Emotion Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Micro-gestures are unconsciously performed body gestures that can convey the emotion states of humans and start to attract more research attention in the fields of human behavior understanding and affective computing as an emerging topic. However, the modeling of human emotion based on micro-gestures has not been explored sufficiently. In this work, we propose to recognize the emotion states based on the micro-gestures by reconstructing the behavior patterns with a hypergraph-enhanced Transformer in a hybrid-supervised framework. In the framework, hypergraph Transformer based encoder and decoder are separately designed by stacking the hypergraph-enhanced self-attention and multiscale temporal convolution modules. Especially, to better capture the subtle motion of micro-gestures, we construct a decoder with additional upsampling operations for a reconstruction task in a self-supervised learning manner. We further propose a hypergraph-enhanced self-attention module where the hyperedges between skeleton joints are gradually updated to present the relationships of body joints for modeling the subtle local motion. Lastly, for exploiting the relationship between the emotion states and local motion of micro-gestures, an emotion recognition head from the output of encoder is designed with a shallow architecture and learned in a supervised way. The end-to-end framework is jointly trained in a one-stage way by comprehensively utilizing self-reconstruction and supervision information. The proposed method is evaluated on two publicly available datasets, namely iMiGUE and SMG, and achieves the best performance under multiple metrics, which is superior to the existing methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5fae\u624b\u52bf\u7684\u60c5\u7eea\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d85\u56fe\u589e\u5f3a\u7684Transformer\u5728\u6df7\u5408\u76d1\u7763\u6846\u67b6\u4e2d\u91cd\u5efa\u884c\u4e3a\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u60c5\u7eea\u8bc6\u522b\u7684\u6027\u80fd\u3002", "motivation": "\u5fae\u624b\u52bf\u4f5c\u4e3a\u65e0\u610f\u8bc6\u7684\u8eab\u4f53\u52a8\u4f5c\uff0c\u80fd\u591f\u4f20\u8fbe\u4eba\u7c7b\u60c5\u7eea\u72b6\u6001\uff0c\u4f46\u5728\u60c5\u7eea\u5efa\u6a21\u65b9\u9762\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u91c7\u7528\u8d85\u56fe\u589e\u5f3a\u7684Transformer\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u548c\u76d1\u7763\u5b66\u4e60\uff0c\u8bbe\u8ba1\u4e86\u591a\u5c3a\u5ea6\u65f6\u95f4\u5377\u79ef\u6a21\u5757\u548c\u8d85\u56fe\u589e\u5f3a\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\uff08iMiGUE\u548cSMG\uff09\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6df7\u5408\u76d1\u7763\u6846\u67b6\u548c\u8d85\u56fe\u589e\u5f3a\u7684Transformer\uff0c\u6709\u6548\u6355\u6349\u5fae\u624b\u52bf\u7684\u7ec6\u5fae\u8fd0\u52a8\uff0c\u63d0\u5347\u4e86\u60c5\u7eea\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.14921", "pdf": "https://arxiv.org/pdf/2507.14921", "abs": "https://arxiv.org/abs/2507.14921", "authors": ["Xiufeng Huang", "Ka Chun Cheung", "Runmin Cong", "Simon See", "Renjie Wan"], "title": "Stereo-GS: Multi-View Stereo Vision Model for Generalizable 3D Gaussian Splatting Reconstruction", "categories": ["cs.CV"], "comment": "ACMMM2025. Non-camera-ready version", "summary": "Generalizable 3D Gaussian Splatting reconstruction showcases advanced Image-to-3D content creation but requires substantial computational resources and large datasets, posing challenges to training models from scratch. Current methods usually entangle the prediction of 3D Gaussian geometry and appearance, which rely heavily on data-driven priors and result in slow regression speeds. To address this, we propose \\method, a disentangled framework for efficient 3D Gaussian prediction. Our method extracts features from local image pairs using a stereo vision backbone and fuses them via global attention blocks. Dedicated point and Gaussian prediction heads generate multi-view point-maps for geometry and Gaussian features for appearance, combined as GS-maps to represent the 3DGS object. A refinement network enhances these GS-maps for high-quality reconstruction. Unlike existing methods that depend on camera parameters, our approach achieves pose-free 3D reconstruction, improving robustness and practicality. By reducing resource demands while maintaining high-quality outputs, \\method provides an efficient, scalable solution for real-world 3D content generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u6846\u67b6\\method\uff0c\u7528\u4e8e\u9ad8\u6548\u9884\u6d4b3D\u9ad8\u65af\uff0c\u901a\u8fc7\u7acb\u4f53\u89c6\u89c9\u9aa8\u5e72\u63d0\u53d6\u7279\u5f81\u5e76\u878d\u5408\uff0c\u751f\u6210\u51e0\u4f55\u548c\u5916\u89c2\u7684\u9ad8\u65af\u7279\u5f81\uff0c\u5b9e\u73b0\u65e0\u59ff\u6001\u7684\u9ad8\u8d28\u91cf3D\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u57283D\u9ad8\u65af\u51e0\u4f55\u548c\u5916\u89c2\u9884\u6d4b\u4e0a\u5b58\u5728\u8026\u5408\uff0c\u4f9d\u8d56\u6570\u636e\u9a71\u52a8\u5148\u9a8c\u4e14\u56de\u5f52\u901f\u5ea6\u6162\uff0c\u9700\u89e3\u51b3\u8ba1\u7b97\u8d44\u6e90\u548c\u5927\u6570\u636e\u96c6\u9700\u6c42\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u7acb\u4f53\u89c6\u89c9\u9aa8\u5e72\u63d0\u53d6\u5c40\u90e8\u56fe\u50cf\u5bf9\u7279\u5f81\uff0c\u901a\u8fc7\u5168\u5c40\u6ce8\u610f\u529b\u5757\u878d\u5408\uff0c\u751f\u6210\u591a\u89c6\u70b9\u51e0\u4f55\u70b9\u56fe\u548c\u5916\u89c2\u9ad8\u65af\u7279\u5f81\uff0c\u7ed3\u5408\u4e3aGS\u56fe\u8868\u793a3DGS\u5bf9\u8c61\uff0c\u5e76\u901a\u8fc7\u7ec6\u5316\u7f51\u7edc\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "result": "\u5b9e\u73b0\u4e86\u65e0\u59ff\u6001\u76843D\u91cd\u5efa\uff0c\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u8d44\u6e90\u9700\u6c42\u5e76\u4fdd\u6301\u9ad8\u8d28\u91cf\u8f93\u51fa\u3002", "conclusion": "\\method\u4e3a\u73b0\u5b9e\u4e16\u754c\u76843D\u5185\u5bb9\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14959", "pdf": "https://arxiv.org/pdf/2507.14959", "abs": "https://arxiv.org/abs/2507.14959", "authors": ["Saeid Ghafouri", "Mohsen Fayyaz", "Xiangchen Li", "Deepu John", "Bo Ji", "Dimitrios Nikolopoulos", "Hans Vandierendonck"], "title": "Polymorph: Energy-Efficient Multi-Label Classification for Video Streams on Embedded Devices", "categories": ["cs.CV", "cs.PF"], "comment": null, "summary": "Real-time multi-label video classification on embedded devices is constrained by limited compute and energy budgets. Yet, video streams exhibit structural properties such as label sparsity, temporal continuity, and label co-occurrence that can be leveraged for more efficient inference. We introduce Polymorph, a context-aware framework that activates a minimal set of lightweight Low Rank Adapters (LoRA) per frame. Each adapter specializes in a subset of classes derived from co-occurrence patterns and is implemented as a LoRA weight over a shared backbone. At runtime, Polymorph dynamically selects and composes only the adapters needed to cover the active labels, avoiding full-model switching and weight merging. This modular strategy improves scalability while reducing latency and energy overhead. Polymorph achieves 40% lower energy consumption and improves mAP by 9 points over strong baselines on the TAO dataset. Polymorph is open source at https://github.com/inference-serving/polymorph/.", "AI": {"tldr": "Polymorph\u662f\u4e00\u79cd\u4e0a\u4e0b\u6587\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6fc0\u6d3b\u8f7b\u91cf\u7ea7\u4f4e\u79e9\u9002\u914d\u5668\uff08LoRA\uff09\u5b9e\u73b0\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u591a\u6807\u7b7e\u89c6\u9891\u5206\u7c7b\uff0c\u663e\u8457\u964d\u4f4e\u80fd\u8017\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5d4c\u5165\u5f0f\u8bbe\u5907\u5728\u5b9e\u65f6\u591a\u6807\u7b7e\u89c6\u9891\u5206\u7c7b\u4e2d\u53d7\u9650\u4e8e\u8ba1\u7b97\u548c\u80fd\u6e90\u9884\u7b97\uff0c\u4f46\u89c6\u9891\u6d41\u5177\u6709\u6807\u7b7e\u7a00\u758f\u6027\u3001\u65f6\u95f4\u8fde\u7eed\u6027\u548c\u6807\u7b7e\u5171\u73b0\u6027\u7b49\u7ed3\u6784\u7279\u6027\uff0c\u53ef\u88ab\u5229\u7528\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "method": "Polymorph\u6846\u67b6\u52a8\u6001\u9009\u62e9\u548c\u7ec4\u5408\u8f7b\u91cf\u7ea7LoRA\u9002\u914d\u5668\uff0c\u6bcf\u4e2a\u9002\u914d\u5668\u4e13\u6ce8\u4e8e\u57fa\u4e8e\u5171\u73b0\u6a21\u5f0f\u7684\u5b50\u7c7b\uff0c\u907f\u514d\u5168\u6a21\u578b\u5207\u6362\u548c\u6743\u91cd\u5408\u5e76\u3002", "result": "\u5728TAO\u6570\u636e\u96c6\u4e0a\uff0cPolymorph\u80fd\u8017\u964d\u4f4e40%\uff0cmAP\u63d0\u53479\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "Polymorph\u901a\u8fc7\u6a21\u5757\u5316\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u89c6\u9891\u5206\u7c7b\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u4e14\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.15036", "pdf": "https://arxiv.org/pdf/2507.15036", "abs": "https://arxiv.org/abs/2507.15036", "authors": ["Lyes Saad Saoud", "Irfan Hussain"], "title": "EBA-AI: Ethics-Guided Bias-Aware AI for Efficient Underwater Image Enhancement and Coral Reef Monitoring", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Underwater image enhancement is vital for marine conservation, particularly coral reef monitoring. However, AI-based enhancement models often face dataset bias, high computational costs, and lack of transparency, leading to potential misinterpretations. This paper introduces EBA-AI, an ethics-guided bias-aware AI framework to address these challenges. EBA-AI leverages CLIP embeddings to detect and mitigate dataset bias, ensuring balanced representation across varied underwater environments. It also integrates adaptive processing to optimize energy efficiency, significantly reducing GPU usage while maintaining competitive enhancement quality. Experiments on LSUI400, Oceanex, and UIEB100 show that while PSNR drops by a controlled 1.0 dB, computational savings enable real-time feasibility for large-scale marine monitoring. Additionally, uncertainty estimation and explainability techniques enhance trust in AI-driven environmental decisions. Comparisons with CycleGAN, FunIEGAN, RAUNENet, WaterNet, UGAN, PUGAN, and UTUIE validate EBA-AI's effectiveness in balancing efficiency, fairness, and interpretability in underwater image processing. By addressing key limitations of AI-driven enhancement, this work contributes to sustainable, bias-aware, and computationally efficient marine conservation efforts. For interactive visualizations, animations, source code, and access to the preprint, visit: https://lyessaadsaoud.github.io/EBA-AI/", "AI": {"tldr": "EBA-AI\u662f\u4e00\u4e2a\u57fa\u4e8e\u4f26\u7406\u548c\u504f\u89c1\u7684AI\u6846\u67b6\uff0c\u7528\u4e8e\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\uff0c\u89e3\u51b3\u6570\u636e\u96c6\u504f\u89c1\u3001\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u900f\u660e\u5ea6\u95ee\u9898\u3002", "motivation": "\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\u5bf9\u6d77\u6d0b\u4fdd\u62a4\uff08\u5982\u73ca\u745a\u7901\u76d1\u6d4b\uff09\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709AI\u6a21\u578b\u5b58\u5728\u6570\u636e\u96c6\u504f\u89c1\u3001\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u7f3a\u4e4f\u900f\u660e\u5ea6\u7684\u95ee\u9898\u3002", "method": "EBA-AI\u5229\u7528CLIP\u5d4c\u5165\u68c0\u6d4b\u548c\u51cf\u8f7b\u6570\u636e\u96c6\u504f\u89c1\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u5904\u7406\u4f18\u5316\u80fd\u6548\uff0c\u51cf\u5c11GPU\u4f7f\u7528\u3002", "result": "\u5b9e\u9a8c\u663e\u793aPSNR\u4e0b\u964d1.0 dB\uff0c\u4f46\u8ba1\u7b97\u6548\u7387\u63d0\u5347\uff0c\u652f\u6301\u5b9e\u65f6\u5927\u89c4\u6a21\u76d1\u6d4b\u3002\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u53ef\u89e3\u91ca\u6027\u6280\u672f\u589e\u5f3a\u4fe1\u4efb\u3002", "conclusion": "EBA-AI\u5728\u6548\u7387\u3001\u516c\u5e73\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u53ef\u6301\u7eed\u6d77\u6d0b\u4fdd\u62a4\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2507.15064", "pdf": "https://arxiv.org/pdf/2507.15064", "abs": "https://arxiv.org/abs/2507.15064", "authors": ["Shuyuan Tu", "Zhen Xing", "Xintong Han", "Zhi-Qi Cheng", "Qi Dai", "Chong Luo", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation", "categories": ["cs.CV", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2411.17697", "summary": "Current diffusion models for human image animation often struggle to maintain identity (ID) consistency, especially when the reference image and driving video differ significantly in body size or position. We introduce StableAnimator++, the first ID-preserving video diffusion framework with learnable pose alignment, capable of generating high-quality videos conditioned on a reference image and a pose sequence without any post-processing. Building upon a video diffusion model, StableAnimator++ contains carefully designed modules for both training and inference, striving for identity consistency. In particular, StableAnimator++ first uses learnable layers to predict the similarity transformation matrices between the reference image and the driven poses via injecting guidance from Singular Value Decomposition (SVD). These matrices align the driven poses with the reference image, mitigating misalignment to a great extent. StableAnimator++ then computes image and face embeddings using off-the-shelf encoders, refining the face embeddings via a global content-aware Face Encoder. To further maintain ID, we introduce a distribution-aware ID Adapter that counteracts interference caused by temporal layers while preserving ID via distribution alignment. During the inference stage, we propose a novel Hamilton-Jacobi-Bellman (HJB) based face optimization integrated into the denoising process, guiding the diffusion trajectory for enhanced facial fidelity. Experiments on benchmarks show the effectiveness of StableAnimator++ both qualitatively and quantitatively.", "AI": {"tldr": "StableAnimator++\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u8eab\u4efd\u4fdd\u6301\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u59ff\u6001\u5bf9\u9f50\u548c\u5206\u5e03\u611f\u77e5ID\u9002\u914d\u5668\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8eab\u4efd\u4e00\u81f4\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u7c7b\u56fe\u50cf\u52a8\u753b\u6269\u6563\u6a21\u578b\u5728\u53c2\u8003\u56fe\u50cf\u548c\u9a71\u52a8\u89c6\u9891\u5dee\u5f02\u8f83\u5927\u65f6\u96be\u4ee5\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\u3002", "method": "StableAnimator++\u901a\u8fc7\u53ef\u5b66\u4e60\u5c42\u9884\u6d4b\u76f8\u4f3c\u53d8\u6362\u77e9\u9635\uff0c\u7ed3\u5408SVD\u5f15\u5bfc\u5bf9\u9f50\u59ff\u6001\uff0c\u5e76\u4f7f\u7528\u56fe\u50cf\u548c\u9762\u90e8\u5d4c\u5165\u589e\u5f3a\u8eab\u4efd\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u5f15\u5165\u5206\u5e03\u611f\u77e5ID\u9002\u914d\u5668\u548cHJB\u9762\u90e8\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eStableAnimator++\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u4e0a\u5747\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "StableAnimator++\u901a\u8fc7\u521b\u65b0\u7684\u5bf9\u9f50\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u9002\u7528\u4e8e\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2507.15085", "pdf": "https://arxiv.org/pdf/2507.15085", "abs": "https://arxiv.org/abs/2507.15085", "authors": ["Peirong Zhang", "Haowei Xu", "Jiaxin Zhang", "Guitao Xu", "Xuhan Zheng", "Zhenhua Yang", "Junle Liu", "Yuyi Zhang", "Lianwen Jin"], "title": "Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of State-of-the-Art Generative Models for OCR", "categories": ["cs.CV"], "comment": null, "summary": "Text image is a unique and crucial information medium that integrates visual aesthetics and linguistic semantics in modern e-society. Due to their subtlety and complexity, the generation of text images represents a challenging and evolving frontier in the image generation field. The recent surge of specialized image generators (\\emph{e.g.}, Flux-series) and unified generative models (\\emph{e.g.}, GPT-4o), which demonstrate exceptional fidelity, raises a natural question: can they master the intricacies of text image generation and editing? Motivated by this, we assess current state-of-the-art generative models' capabilities in terms of text image generation and editing. We incorporate various typical optical character recognition (OCR) tasks into our evaluation and broaden the concept of text-based generation tasks into OCR generative tasks. We select 33 representative tasks and categorize them into five categories: document, handwritten text, scene text, artistic text, and complex \\& layout-rich text. For comprehensive evaluation, we examine six models across both closed-source and open-source domains, using tailored, high-quality image inputs and prompts. Through this evaluation, we draw crucial observations and identify the weaknesses of current generative models for OCR tasks. We argue that photorealistic text image generation and editing should be internalized as foundational skills into general-domain generative models, rather than being delegated to specialized solutions, and we hope this empirical analysis can provide valuable insights for the community to achieve this goal. This evaluation is online and will be continuously updated at our GitHub repository.", "AI": {"tldr": "\u8bc4\u4f30\u5f53\u524d\u6700\u5148\u8fdb\u7684\u751f\u6210\u6a21\u578b\u5728\u6587\u672c\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u65b9\u9762\u7684\u80fd\u529b\uff0c\u63d0\u51fa\u5c06\u903c\u771f\u6587\u672c\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u4f5c\u4e3a\u901a\u7528\u751f\u6210\u6a21\u578b\u7684\u57fa\u7840\u6280\u80fd\u3002", "motivation": "\u7531\u4e8e\u6587\u672c\u56fe\u50cf\u7684\u590d\u6742\u6027\u548c\u91cd\u8981\u6027\uff0c\u7814\u7a76\u73b0\u6709\u751f\u6210\u6a21\u578b\u662f\u5426\u80fd\u638c\u63e1\u5176\u751f\u6210\u548c\u7f16\u8f91\u7684\u7ec6\u8282\u3002", "method": "\u9009\u62e933\u4e2a\u4ee3\u8868\u6027\u4efb\u52a1\uff0c\u5206\u4e3a\u4e94\u7c7b\uff0c\u8bc4\u4f30\u516d\u79cd\u6a21\u578b\u5728\u5c01\u95ed\u548c\u5f00\u6e90\u9886\u57df\u7684\u8868\u73b0\u3002", "result": "\u8bc6\u522b\u5f53\u524d\u751f\u6210\u6a21\u578b\u5728OCR\u4efb\u52a1\u4e2d\u7684\u5f31\u70b9\uff0c\u63d0\u51fa\u6539\u8fdb\u65b9\u5411\u3002", "conclusion": "\u903c\u771f\u6587\u672c\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u5e94\u4f5c\u4e3a\u901a\u7528\u751f\u6210\u6a21\u578b\u7684\u57fa\u7840\u6280\u80fd\uff0c\u800c\u975e\u4f9d\u8d56\u4e13\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15212", "pdf": "https://arxiv.org/pdf/2507.15212", "abs": "https://arxiv.org/abs/2507.15212", "authors": ["Yusuke Yoshiyasu", "Leyuan Sun", "Ryusuke Sagawa"], "title": "MeshMamba: State Space Models for Articulated 3D Mesh Generation and Reconstruction", "categories": ["cs.CV"], "comment": "Accepted at ICCV2025", "summary": "In this paper, we introduce MeshMamba, a neural network model for learning 3D articulated mesh models by employing the recently proposed Mamba State Space Models (Mamba-SSMs). MeshMamba is efficient and scalable in handling a large number of input tokens, enabling the generation and reconstruction of body mesh models with more than 10,000 vertices, capturing clothing and hand geometries. The key to effectively learning MeshMamba is the serialization technique of mesh vertices into orderings that are easily processed by Mamba. This is achieved by sorting the vertices based on body part annotations or the 3D vertex locations of a template mesh, such that the ordering respects the structure of articulated shapes. Based on MeshMamba, we design 1) MambaDiff3D, a denoising diffusion model for generating 3D articulated meshes and 2) Mamba-HMR, a 3D human mesh recovery model that reconstructs a human body shape and pose from a single image. Experimental results showed that MambaDiff3D can generate dense 3D human meshes in clothes, with grasping hands, etc., and outperforms previous approaches in the 3D human shape generation task. Additionally, Mamba-HMR extends the capabilities of previous non-parametric human mesh recovery approaches, which were limited to handling body-only poses using around 500 vertex tokens, to the whole-body setting with face and hands, while achieving competitive performance in (near) real-time.", "AI": {"tldr": "MeshMamba\u5229\u7528Mamba-SSMs\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a213D\u7f51\u683c\u6570\u636e\uff0c\u751f\u6210\u548c\u91cd\u5efa\u5305\u542b\u8863\u7269\u548c\u624b\u90e8\u7ec6\u8282\u7684\u5bc6\u96c6\u4eba\u4f53\u7f51\u683c\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a213D\u7f51\u683c\u6570\u636e\u65f6\u7684\u6548\u7387\u95ee\u9898\uff0c\u5e76\u6269\u5c55\u4eba\u4f53\u7f51\u683c\u751f\u6210\u548c\u91cd\u5efa\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u9876\u70b9\u5e8f\u5217\u5316\u6280\u672f\u5c06\u7f51\u683c\u9876\u70b9\u6392\u5e8f\uff0c\u5229\u7528Mamba-SSMs\u5904\u7406\uff0c\u8bbe\u8ba1\u4e86MambaDiff3D\u548cMamba-HMR\u4e24\u4e2a\u6a21\u578b\u3002", "result": "MambaDiff3D\u5728\u751f\u62103D\u4eba\u4f53\u7f51\u683c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cMamba-HMR\u5b9e\u73b0\u4e86\u5168\u8eab\u7f51\u683c\u91cd\u5efa\u5e76\u63a5\u8fd1\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "MeshMamba\u4e3a3D\u4eba\u4f53\u7f51\u683c\u751f\u6210\u548c\u91cd\u5efa\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15216", "pdf": "https://arxiv.org/pdf/2507.15216", "abs": "https://arxiv.org/abs/2507.15216", "authors": ["Yuping Qiu", "Rui Zhu", "Ying-cong Chen"], "title": "Improving Joint Embedding Predictive Architecture with Diffusion Noise", "categories": ["cs.CV"], "comment": null, "summary": "Self-supervised learning has become an incredibly successful method for feature learning, widely applied to many downstream tasks. It has proven especially effective for discriminative tasks, surpassing the trending generative models. However, generative models perform better in image generation and detail enhancement. Thus, it is natural for us to find a connection between SSL and generative models to further enhance the representation capacity of SSL. As generative models can create new samples by approximating the data distribution, such modeling should also lead to a semantic understanding of the raw visual data, which is necessary for recognition tasks. This enlightens us to combine the core principle of the diffusion model: diffusion noise, with SSL to learn a competitive recognition model. Specifically, diffusion noise can be viewed as a particular state of mask that reveals a close relationship between masked image modeling (MIM) and diffusion models. In this paper, we propose N-JEPA (Noise-based JEPA) to incorporate diffusion noise into MIM by the position embedding of masked tokens. The multi-level noise schedule is a series of feature augmentations to further enhance the robustness of our model. We perform a comprehensive study to confirm its effectiveness in the classification of downstream tasks. Codes will be released soon in public.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faN-JEPA\u65b9\u6cd5\uff0c\u5c06\u6269\u6563\u566a\u58f0\u5f15\u5165\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\uff08MIM\uff09\uff0c\u4ee5\u589e\u5f3a\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u7684\u8868\u5f81\u80fd\u529b\uff0c\u5e76\u5728\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u5728\u7279\u5f81\u5b66\u4e60\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u751f\u6210\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u548c\u7ec6\u8282\u589e\u5f3a\u4e0a\u66f4\u4f18\u3002\u8bba\u6587\u65e8\u5728\u7ed3\u5408SSL\u4e0e\u751f\u6210\u6a21\u578b\u7684\u6838\u5fc3\u601d\u60f3\uff08\u5982\u6269\u6563\u566a\u58f0\uff09\uff0c\u4ee5\u63d0\u5347SSL\u7684\u8868\u5f81\u80fd\u529b\u3002", "method": "\u63d0\u51faN-JEPA\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a9\u7801\u6807\u8bb0\u7684\u4f4d\u7f6e\u5d4c\u5165\u5c06\u6269\u6563\u566a\u58f0\u5f15\u5165MIM\uff0c\u5e76\u91c7\u7528\u591a\u7ea7\u566a\u58f0\u8c03\u5ea6\u4f5c\u4e3a\u7279\u5f81\u589e\u5f3a\u624b\u6bb5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9eN-JEPA\u5728\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u7ed3\u5408\u6269\u6563\u566a\u58f0\u4e0eSSL\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u8868\u5f81\u80fd\u529b\uff0cN-JEPA\u4e3a\u8fd9\u4e00\u65b9\u5411\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.15249", "pdf": "https://arxiv.org/pdf/2507.15249", "abs": "https://arxiv.org/abs/2507.15249", "authors": ["Yanbing Zhang", "Zhe Wang", "Qin Zhou", "Mengping Yang"], "title": "FreeCus: Free Lunch Subject-driven Customization in Diffusion Transformers", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "In light of recent breakthroughs in text-to-image (T2I) generation, particularly with diffusion transformers (DiT), subject-driven technologies are increasingly being employed for high-fidelity customized production that preserves subject identity from reference inputs, enabling thrilling design workflows and engaging entertainment. Existing alternatives typically require either per-subject optimization via trainable text embeddings or training specialized encoders for subject feature extraction on large-scale datasets. Such dependencies on training procedures fundamentally constrain their practical applications. More importantly, current methodologies fail to fully leverage the inherent zero-shot potential of modern diffusion transformers (e.g., the Flux series) for authentic subject-driven synthesis. To bridge this gap, we propose FreeCus, a genuinely training-free framework that activates DiT's capabilities through three key innovations: 1) We introduce a pivotal attention sharing mechanism that captures the subject's layout integrity while preserving crucial editing flexibility. 2) Through a straightforward analysis of DiT's dynamic shifting, we propose an upgraded variant that significantly improves fine-grained feature extraction. 3) We further integrate advanced Multimodal Large Language Models (MLLMs) to enrich cross-modal semantic representations. Extensive experiments reflect that our method successfully unlocks DiT's zero-shot ability for consistent subject synthesis across diverse contexts, achieving state-of-the-art or comparable results compared to approaches that require additional training. Notably, our framework demonstrates seamless compatibility with existing inpainting pipelines and control modules, facilitating more compelling experiences. Our code is available at: https://github.com/Monalissaa/FreeCus.", "AI": {"tldr": "FreeCus\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6ce8\u610f\u529b\u5171\u4eab\u673a\u5236\u548c\u6539\u8fdb\u7684\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\uff0c\u6fc0\u6d3b\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u4e3b\u9898\u9a71\u52a8\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\uff0c\u4e14\u672a\u80fd\u5145\u5206\u5229\u7528\u73b0\u4ee3\u6269\u6563\u53d8\u6362\u5668\u7684\u96f6\u6837\u672c\u6f5c\u529b\u3002", "method": "\u63d0\u51faFreeCus\u6846\u67b6\uff0c\u5305\u62ec\u6ce8\u610f\u529b\u5171\u4eab\u673a\u5236\u3001\u6539\u8fdb\u7684\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\uff0c\u5e76\u6574\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFreeCus\u6210\u529f\u89e3\u9501\u4e86DiT\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u5728\u591a\u6837\u573a\u666f\u4e2d\u5b9e\u73b0\u4e00\u81f4\u7684\u4e3b\u9898\u5408\u6210\uff0c\u6027\u80fd\u8fbe\u5230\u6216\u8d85\u8d8a\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u3002", "conclusion": "FreeCus\u5c55\u793a\u4e86\u4e0e\u73b0\u6709\u4fee\u590d\u6d41\u7a0b\u548c\u63a7\u5236\u6a21\u5757\u7684\u65e0\u7f1d\u517c\u5bb9\u6027\uff0c\u4e3a\u8bbe\u8ba1\u5de5\u4f5c\u6d41\u548c\u5a31\u4e50\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15269", "pdf": "https://arxiv.org/pdf/2507.15269", "abs": "https://arxiv.org/abs/2507.15269", "authors": ["Fangqiu Yi", "Jingyu Xu", "Jiawei Shao", "Chi Zhang", "Xuelong Li"], "title": "Conditional Video Generation for High-Efficiency Video Compression", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Perceptual studies demonstrate that conditional diffusion models excel at reconstructing video content aligned with human visual perception. Building on this insight, we propose a video compression framework that leverages conditional diffusion models for perceptually optimized reconstruction. Specifically, we reframe video compression as a conditional generation task, where a generative model synthesizes video from sparse, yet informative signals. Our approach introduces three key modules: (1) Multi-granular conditioning that captures both static scene structure and dynamic spatio-temporal cues; (2) Compact representations designed for efficient transmission without sacrificing semantic richness; (3) Multi-condition training with modality dropout and role-aware embeddings, which prevent over-reliance on any single modality and enhance robustness. Extensive experiments show that our method significantly outperforms both traditional and neural codecs on perceptual quality metrics such as Fr\\'echet Video Distance (FVD) and LPIPS, especially under high compression ratios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u89c6\u9891\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u6a21\u578b\u4ece\u7a00\u758f\u4fe1\u53f7\u4e2d\u91cd\u5efa\u89c6\u9891\uff0c\u663e\u8457\u63d0\u5347\u4e86\u611f\u77e5\u8d28\u91cf\u3002", "motivation": "\u5229\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5728\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u5bf9\u9f50\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u4f18\u5316\u89c6\u9891\u538b\u7f29\u7684\u611f\u77e5\u8d28\u91cf\u3002", "method": "\u5c06\u89c6\u9891\u538b\u7f29\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6761\u4ef6\u751f\u6210\u4efb\u52a1\uff0c\u5f15\u5165\u591a\u7c92\u5ea6\u6761\u4ef6\u3001\u7d27\u51d1\u8868\u793a\u548c\u591a\u6761\u4ef6\u8bad\u7ec3\u6a21\u5757\u3002", "result": "\u5728FVD\u548cLPIPS\u7b49\u611f\u77e5\u8d28\u91cf\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u548c\u795e\u7ecf\u7f16\u89e3\u7801\u5668\uff0c\u5c24\u5176\u5728\u9ad8\u538b\u7f29\u6bd4\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u6a21\u578b\u5b9e\u73b0\u4e86\u611f\u77e5\u4f18\u5316\u7684\u89c6\u9891\u538b\u7f29\uff0c\u4e3a\u672a\u6765\u89c6\u9891\u7f16\u7801\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.15346", "pdf": "https://arxiv.org/pdf/2507.15346", "abs": "https://arxiv.org/abs/2507.15346", "authors": ["Muhammad Aqeel", "Kidus Dagnaw Bellete", "Francesco Setti"], "title": "RoadFusion: Latent Diffusion Model for Pavement Defect Detection", "categories": ["cs.CV"], "comment": "Accepted to ICIAP 2025", "summary": "Pavement defect detection faces critical challenges including limited annotated data, domain shift between training and deployment environments, and high variability in defect appearances across different road conditions. We propose RoadFusion, a framework that addresses these limitations through synthetic anomaly generation with dual-path feature adaptation. A latent diffusion model synthesizes diverse, realistic defects using text prompts and spatial masks, enabling effective training under data scarcity. Two separate feature adaptors specialize representations for normal and anomalous inputs, improving robustness to domain shift and defect variability. A lightweight discriminator learns to distinguish fine-grained defect patterns at the patch level. Evaluated on six benchmark datasets, RoadFusion achieves consistently strong performance across both classification and localization tasks, setting new state-of-the-art in multiple metrics relevant to real-world road inspection.", "AI": {"tldr": "RoadFusion\u901a\u8fc7\u5408\u6210\u5f02\u5e38\u751f\u6210\u548c\u53cc\u8def\u5f84\u7279\u5f81\u9002\u5e94\u89e3\u51b3\u8def\u9762\u7f3a\u9677\u68c0\u6d4b\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u3001\u9886\u57df\u504f\u79fb\u548c\u7f3a\u9677\u591a\u6837\u6027\u95ee\u9898\u3002", "motivation": "\u8def\u9762\u7f3a\u9677\u68c0\u6d4b\u9762\u4e34\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u3001\u8bad\u7ec3\u4e0e\u90e8\u7f72\u73af\u5883\u95f4\u7684\u9886\u57df\u504f\u79fb\u4ee5\u53ca\u4e0d\u540c\u9053\u8def\u6761\u4ef6\u4e0b\u7f3a\u9677\u5916\u89c2\u7684\u9ad8\u53d8\u5f02\u6027\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faRoadFusion\u6846\u67b6\uff0c\u5305\u62ec\u6f5c\u5728\u6269\u6563\u6a21\u578b\u751f\u6210\u5408\u6210\u7f3a\u9677\u3001\u53cc\u8def\u5f84\u7279\u5f81\u9002\u5e94\u5668\u5206\u522b\u5904\u7406\u6b63\u5e38\u548c\u5f02\u5e38\u8f93\u5165\uff0c\u4ee5\u53ca\u8f7b\u91cf\u7ea7\u5224\u522b\u5668\u5b66\u4e60\u7ec6\u7c92\u5ea6\u7f3a\u9677\u6a21\u5f0f\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cRoadFusion\u5728\u5206\u7c7b\u548c\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u591a\u9879\u6307\u6807\u8fbe\u5230\u65b0\u6700\u4f18\u3002", "conclusion": "RoadFusion\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u7279\u5f81\u9002\u5e94\u6709\u6548\u63d0\u5347\u4e86\u8def\u9762\u7f3a\u9677\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2507.15520", "pdf": "https://arxiv.org/pdf/2507.15520", "abs": "https://arxiv.org/abs/2507.15520", "authors": ["Hanting Li", "Fei Zhou", "Xin Sun", "Yang Hua", "Jungong Han", "Liang-Jie Zhang"], "title": "SAIGFormer: A Spatially-Adaptive Illumination-Guided Network for Low-Light Image Enhancement", "categories": ["cs.CV"], "comment": "11 pages, 10 figures, 6 tables", "summary": "Recent Transformer-based low-light enhancement methods have made promising progress in recovering global illumination. However, they still struggle with non-uniform lighting scenarios, such as backlit and shadow, appearing as over-exposure or inadequate brightness restoration. To address this challenge, we present a Spatially-Adaptive Illumination-Guided Transformer (SAIGFormer) framework that enables accurate illumination restoration. Specifically, we propose a dynamic integral image representation to model the spatially-varying illumination, and further construct a novel Spatially-Adaptive Integral Illumination Estimator ($\\text{SAI}^2\\text{E}$). Moreover, we introduce an Illumination-Guided Multi-head Self-Attention (IG-MSA) mechanism, which leverages the illumination to calibrate the lightness-relevant features toward visual-pleased illumination enhancement. Extensive experiments on five standard low-light datasets and a cross-domain benchmark (LOL-Blur) demonstrate that our SAIGFormer significantly outperforms state-of-the-art methods in both quantitative and qualitative metrics. In particular, our method achieves superior performance in non-uniform illumination enhancement while exhibiting strong generalization capabilities across multiple datasets. Code is available at https://github.com/LHTcode/SAIGFormer.git.", "AI": {"tldr": "SAIGFormer\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u4f4e\u5149\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u79ef\u5206\u56fe\u50cf\u8868\u793a\u548c\u5149\u7167\u5f15\u5bfc\u7684\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u975e\u5747\u5300\u5149\u7167\u573a\u666f\u4e0b\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709Transformer\u65b9\u6cd5\u5728\u975e\u5747\u5300\u5149\u7167\u573a\u666f\uff08\u5982\u80cc\u5149\u548c\u9634\u5f71\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u8fc7\u66dd\u6216\u4eae\u5ea6\u6062\u590d\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u79ef\u5206\u56fe\u50cf\u8868\u793a\u5efa\u6a21\u7a7a\u95f4\u53d8\u5316\u5149\u7167\uff0c\u6784\u5efaSAI2E\u4f30\u8ba1\u5668\uff0c\u5e76\u5f15\u5165\u5149\u7167\u5f15\u5bfc\u7684\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff08IG-MSA\uff09\u3002", "result": "\u5728\u4e94\u4e2a\u6807\u51c6\u4f4e\u5149\u6570\u636e\u96c6\u548c\u8de8\u57df\u57fa\u51c6\uff08LOL-Blur\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5b9a\u91cf\u548c\u5b9a\u6027\u6307\u6807\u5747\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SAIGFormer\u5728\u975e\u5747\u5300\u5149\u7167\u589e\u5f3a\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u5e76\u5177\u6709\u5f3a\u5927\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.15577", "pdf": "https://arxiv.org/pdf/2507.15577", "abs": "https://arxiv.org/abs/2507.15577", "authors": ["Hugo Carlesso", "Maria Eliza Patulea", "Moncef Garouani", "Radu Tudor Ionescu", "Josiane Mothe"], "title": "GeMix: Conditional GAN-Based Mixup for Improved Medical Image Augmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Mixup has become a popular augmentation strategy for image classification, yet its naive pixel-wise interpolation often produces unrealistic images that can hinder learning, particularly in high-stakes medical applications. We propose GeMix, a two-stage framework that replaces heuristic blending with a learned, label-aware interpolation powered by class-conditional GANs. First, a StyleGAN2-ADA generator is trained on the target dataset. During augmentation, we sample two label vectors from Dirichlet priors biased toward different classes and blend them via a Beta-distributed coefficient. Then, we condition the generator on this soft label to synthesize visually coherent images that lie along a continuous class manifold. We benchmark GeMix on the large-scale COVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101, EfficientNet-B0). When combined with real data, our method increases macro-F1 over traditional mixup for all backbones, reducing the false negative rate for COVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup, delivering stronger regularization and greater semantic fidelity, without disrupting existing training pipelines. We publicly release our code at https://github.com/hugocarlesso/GeMix to foster reproducibility and further research.", "AI": {"tldr": "GeMix\u662f\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6GAN\u7684\u4e24\u9636\u6bb5\u56fe\u50cf\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u8f6f\u6807\u7b7e\u5408\u6210\u89c6\u89c9\u8fde\u8d2f\u7684\u56fe\u50cf\uff0c\u4f18\u4e8e\u4f20\u7edfmixup\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfmixup\u7684\u50cf\u7d20\u7ea7\u63d2\u503c\u751f\u6210\u4e0d\u771f\u5b9e\u56fe\u50cf\uff0c\u53ef\u80fd\u5f71\u54cd\u5b66\u4e60\u6548\u679c\uff0c\u5c24\u5176\u5728\u533b\u5b66\u7b49\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u3002", "method": "\u4f7f\u7528StyleGAN2-ADA\u751f\u6210\u5668\uff0c\u901a\u8fc7Dirichlet\u548cBeta\u5206\u5e03\u91c7\u6837\u6807\u7b7e\u5411\u91cf\uff0c\u751f\u6210\u8fde\u7eed\u7c7b\u6d41\u5f62\u4e0a\u7684\u56fe\u50cf\u3002", "result": "\u5728COVIDx-CT-3\u6570\u636e\u96c6\u4e0a\uff0cGeMix\u7ed3\u5408\u771f\u5b9e\u6570\u636e\u63d0\u5347\u4e86\u6240\u6709\u9aa8\u5e72\u7f51\u7edc\u7684macro-F1\uff0c\u964d\u4f4e\u4e86COVID-19\u68c0\u6d4b\u7684\u5047\u9634\u6027\u7387\u3002", "conclusion": "GeMix\u662f\u4f20\u7edfmixup\u7684\u76f4\u63a5\u66ff\u4ee3\u65b9\u6848\uff0c\u63d0\u4f9b\u66f4\u5f3a\u7684\u6b63\u5219\u5316\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\uff0c\u4e14\u4e0d\u7834\u574f\u73b0\u6709\u8bad\u7ec3\u6d41\u7a0b\u3002"}}
{"id": "2507.15602", "pdf": "https://arxiv.org/pdf/2507.15602", "abs": "https://arxiv.org/abs/2507.15602", "authors": ["Zihui Gao", "Jia-Wang Bian", "Guosheng Lin", "Hao Chen", "Chunhua Shen"], "title": "SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Surface reconstruction and novel view rendering from sparse-view images are challenging. Signed Distance Function (SDF)-based methods struggle with fine details, while 3D Gaussian Splatting (3DGS)-based approaches lack global geometry coherence. We propose a novel hybrid method that combines the strengths of both approaches: SDF captures coarse geometry to enhance 3DGS-based rendering, while newly rendered images from 3DGS refine the details of SDF for accurate surface reconstruction. As a result, our method surpasses state-of-the-art approaches in surface reconstruction and novel view synthesis on the DTU and MobileBrick datasets. Code will be released at https://github.com/Gaozihui/SurfaceSplat.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408SDF\u548c3DGS\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u7a00\u758f\u89c6\u56fe\u56fe\u50cf\u4e2d\u7684\u8868\u9762\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u6e32\u67d3\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7a00\u758f\u89c6\u56fe\u56fe\u50cf\u4e2d\u7684\u8868\u9762\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u6e32\u67d3\u5b58\u5728\u6311\u6218\uff0cSDF\u65b9\u6cd5\u7ec6\u8282\u4e0d\u8db3\uff0c3DGS\u65b9\u6cd5\u7f3a\u4e4f\u5168\u5c40\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "method": "\u7ed3\u5408SDF\u7684\u7c97\u51e0\u4f55\u6355\u83b7\u548c3DGS\u7684\u7ec6\u8282\u6e32\u67d3\uff0c\u4e92\u76f8\u4f18\u5316\u3002", "result": "\u5728DTU\u548cMobileBrick\u6570\u636e\u96c6\u4e0a\uff0c\u8868\u9762\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u5408\u6210\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6df7\u5408\u65b9\u6cd5\u6709\u6548\u7ed3\u5408\u4e86SDF\u548c3DGS\u7684\u4f18\u52bf\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2507.15606", "pdf": "https://arxiv.org/pdf/2507.15606", "abs": "https://arxiv.org/abs/2507.15606", "authors": ["Ru Jia", "Xiaozhuang Ma", "Jianji Wang", "Nanning Zheng"], "title": "CylinderPlane: Nested Cylinder Representation for 3D-aware Image Generation", "categories": ["cs.CV", "68T45", "I.4.5"], "comment": "5 pages, 4 figures, to be published", "summary": "While the proposal of the Tri-plane representation has advanced the development of the 3D-aware image generative models, problems rooted in its inherent structure, such as multi-face artifacts caused by sharing the same features in symmetric regions, limit its ability to generate 360$^\\circ$ view images. In this paper, we propose CylinderPlane, a novel implicit representation based on Cylindrical Coordinate System, to eliminate the feature ambiguity issue and ensure multi-view consistency in 360$^\\circ$. Different from the inevitable feature entanglement in Cartesian coordinate-based Tri-plane representation, the cylindrical coordinate system explicitly separates features at different angles, allowing our cylindrical representation possible to achieve high-quality, artifacts-free 360$^\\circ$ image synthesis. We further introduce the nested cylinder representation that composites multiple cylinders at different scales, thereby enabling the model more adaptable to complex geometry and varying resolutions. The combination of cylinders with different resolutions can effectively capture more critical locations and multi-scale features, greatly facilitates fine detail learning and robustness to different resolutions. Moreover, our representation is agnostic to implicit rendering methods and can be easily integrated into any neural rendering pipeline. Extensive experiments on both synthetic dataset and unstructured in-the-wild images demonstrate that our proposed representation achieves superior performance over previous methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5706\u67f1\u5750\u6807\u7cfb\u7684\u9690\u5f0f\u8868\u793a\u65b9\u6cd5CylinderPlane\uff0c\u89e3\u51b3\u4e86Tri-plane\u8868\u793a\u4e2d\u7684\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u65e0\u4f2a\u5f71\u7684360\u00b0\u56fe\u50cf\u5408\u6210\u3002", "motivation": "Tri-plane\u8868\u793a\u57283D\u611f\u77e5\u56fe\u50cf\u751f\u6210\u4e2d\u5b58\u5728\u5bf9\u79f0\u533a\u57df\u7279\u5f81\u5171\u4eab\u5bfc\u81f4\u7684\u591a\u89c6\u89d2\u4f2a\u5f71\u95ee\u9898\uff0c\u9650\u5236\u4e86360\u00b0\u89c6\u56fe\u56fe\u50cf\u7684\u751f\u6210\u80fd\u529b\u3002", "method": "\u63d0\u51faCylinderPlane\uff0c\u57fa\u4e8e\u5706\u67f1\u5750\u6807\u7cfb\u5206\u79bb\u4e0d\u540c\u89d2\u5ea6\u7684\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u5d4c\u5957\u5706\u67f1\u8868\u793a\u4ee5\u5904\u7406\u590d\u6742\u51e0\u4f55\u548c\u591a\u5206\u8fa8\u7387\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u56fe\u50cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u591a\u89c6\u89d2\u4e00\u81f4\u56fe\u50cf\u751f\u6210\u3002", "conclusion": "CylinderPlane\u901a\u8fc7\u5706\u67f1\u5750\u6807\u7cfb\u6709\u6548\u89e3\u51b3\u4e86\u7279\u5f81\u6a21\u7cca\u95ee\u9898\uff0c\u63d0\u5347\u4e86360\u00b0\u56fe\u50cf\u5408\u6210\u7684\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.15683", "pdf": "https://arxiv.org/pdf/2507.15683", "abs": "https://arxiv.org/abs/2507.15683", "authors": ["Boni Hu", "Zhenyu Xia", "Lin Chen", "Pengcheng Han", "Shuhui Bu"], "title": "Hi^2-GSLoc: Dual-Hierarchical Gaussian-Specific Visual Relocalization for Remote Sensing", "categories": ["cs.CV"], "comment": "17 pages, 11 figures", "summary": "Visual relocalization, which estimates the 6-degree-of-freedom (6-DoF) camera pose from query images, is fundamental to remote sensing and UAV applications. Existing methods face inherent trade-offs: image-based retrieval and pose regression approaches lack precision, while structure-based methods that register queries to Structure-from-Motion (SfM) models suffer from computational complexity and limited scalability. These challenges are particularly pronounced in remote sensing scenarios due to large-scale scenes, high altitude variations, and domain gaps of existing visual priors. To overcome these limitations, we leverage 3D Gaussian Splatting (3DGS) as a novel scene representation that compactly encodes both 3D geometry and appearance. We introduce $\\mathrm{Hi}^2$-GSLoc, a dual-hierarchical relocalization framework that follows a sparse-to-dense and coarse-to-fine paradigm, fully exploiting the rich semantic information and geometric constraints inherent in Gaussian primitives. To handle large-scale remote sensing scenarios, we incorporate partitioned Gaussian training, GPU-accelerated parallel matching, and dynamic memory management strategies. Our approach consists of two stages: (1) a sparse stage featuring a Gaussian-specific consistent render-aware sampling strategy and landmark-guided detector for robust and accurate initial pose estimation, and (2) a dense stage that iteratively refines poses through coarse-to-fine dense rasterization matching while incorporating reliability verification. Through comprehensive evaluation on simulation data, public datasets, and real flight experiments, we demonstrate that our method delivers competitive localization accuracy, recall rate, and computational efficiency while effectively filtering unreliable pose estimates. The results confirm the effectiveness of our approach for practical remote sensing applications.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.15690", "pdf": "https://arxiv.org/pdf/2507.15690", "abs": "https://arxiv.org/abs/2507.15690", "authors": ["Hung Nguyen", "Runfa Li", "An Le", "Truong Nguyen"], "title": "DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian Splatting", "categories": ["cs.CV", "eess.IV", "eess.SP"], "comment": "6 pages, 4 figures", "summary": "Sparse-view 3D Gaussian Splatting (3DGS) presents significant challenges in reconstructing high-quality novel views, as it often overfits to the widely-varying high-frequency (HF) details of the sparse training views. While frequency regularization can be a promising approach, its typical reliance on Fourier transforms causes difficult parameter tuning and biases towards detrimental HF learning. We propose DWTGS, a framework that rethinks frequency regularization by leveraging wavelet-space losses that provide additional spatial supervision. Specifically, we supervise only the low-frequency (LF) LL subbands at multiple DWT levels, while enforcing sparsity on the HF HH subband in a self-supervised manner. Experiments across benchmarks show that DWTGS consistently outperforms Fourier-based counterparts, as this LF-centric strategy improves generalization and reduces HF hallucinations.", "AI": {"tldr": "DWTGS\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u53d8\u6362\u7684\u7a00\u758f\u89c6\u56fe3D\u9ad8\u65af\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4e\u9891\u76d1\u7763\u548c\u9ad8\u9891\u7a00\u758f\u5316\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7a00\u758f\u89c6\u56fe3D\u9ad8\u65af\u91cd\u5efa\u5bb9\u6613\u8fc7\u62df\u5408\u9ad8\u9891\u7ec6\u8282\uff0c\u4f20\u7edf\u5085\u91cc\u53f6\u53d8\u6362\u65b9\u6cd5\u53c2\u6570\u8c03\u4f18\u56f0\u96be\u4e14\u504f\u5411\u6709\u5bb3\u9ad8\u9891\u5b66\u4e60\u3002", "method": "\u5229\u7528\u5c0f\u6ce2\u7a7a\u95f4\u635f\u5931\u63d0\u4f9b\u989d\u5916\u7a7a\u95f4\u76d1\u7763\uff0c\u4ec5\u76d1\u7763\u4f4e\u9891LL\u5b50\u5e26\uff0c\u540c\u65f6\u5bf9\u9ad8\u9891HH\u5b50\u5e26\u8fdb\u884c\u81ea\u76d1\u7763\u7a00\u758f\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDWTGS\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5085\u91cc\u53f6\u53d8\u6362\u65b9\u6cd5\uff0c\u4f4e\u9891\u7b56\u7565\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u5e76\u51cf\u5c11\u9ad8\u9891\u5e7b\u89c9\u3002", "conclusion": "DWTGS\u901a\u8fc7\u5c0f\u6ce2\u53d8\u6362\u7684\u4f4e\u9891\u76d1\u7763\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u89c6\u56fe\u91cd\u5efa\u4e2d\u7684\u9ad8\u9891\u8fc7\u62df\u5408\u95ee\u9898\u3002"}}
{"id": "2507.15724", "pdf": "https://arxiv.org/pdf/2507.15724", "abs": "https://arxiv.org/abs/2507.15724", "authors": ["Guoxuan Xia", "Harleen Hanspal", "Petru-Daniel Tudosiu", "Shifeng Zhang", "Sarah Parisot"], "title": "A Practical Investigation of Spatially-Controlled Image Generation with Transformers", "categories": ["cs.CV"], "comment": "preprint", "summary": "Enabling image generation models to be spatially controlled is an important area of research, empowering users to better generate images according to their own fine-grained specifications via e.g. edge maps, poses. Although this task has seen impressive improvements in recent times, a focus on rapidly producing stronger models has come at the cost of detailed and fair scientific comparison. Differing training data, model architectures and generation paradigms make it difficult to disentangle the factors contributing to performance. Meanwhile, the motivations and nuances of certain approaches become lost in the literature. In this work, we aim to provide clear takeaways across generation paradigms for practitioners wishing to develop transformer-based systems for spatially-controlled generation, clarifying the literature and addressing knowledge gaps. We perform controlled experiments on ImageNet across diffusion-based/flow-based and autoregressive (AR) models. First, we establish control token prefilling as a simple, general and performant baseline approach for transformers. We then investigate previously underexplored sampling time enhancements, showing that extending classifier-free guidance to control, as well as softmax truncation, have a strong impact on control-generation consistency. Finally, we re-clarify the motivation of adapter-based approaches, demonstrating that they mitigate \"forgetting\" and maintain generation quality when trained on limited downstream data, but underperform full training in terms of generation-control consistency. Code will be released upon publication.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u7a7a\u95f4\u63a7\u5236\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b9e\u9a8c\u6f84\u6e05\u4e86\u4e0d\u540c\u751f\u6210\u8303\u5f0f\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e9b\u6027\u80fd\u63d0\u5347\u7684\u6280\u672f\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u7a7a\u95f4\u63a7\u5236\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u79d1\u5b66\u6bd4\u8f83\u4e0a\u7684\u4e0d\u8db3\uff0c\u6f84\u6e05\u6587\u732e\u4e2d\u7684\u77e5\u8bc6\u7a7a\u767d\uff0c\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u6e05\u6670\u7684\u6307\u5bfc\u3002", "method": "\u5728ImageNet\u4e0a\u8fdb\u884c\u4e86\u6269\u6563\u6a21\u578b\u3001\u6d41\u6a21\u578b\u548c\u81ea\u56de\u5f52\u6a21\u578b\u7684\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u63d0\u51fa\u4e86\u63a7\u5236\u6807\u8bb0\u9884\u586b\u5145\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u63a2\u7d22\u4e86\u91c7\u6837\u65f6\u95f4\u589e\u5f3a\u6280\u672f\u3002", "result": "\u63a7\u5236\u6807\u8bb0\u9884\u586b\u5145\u8868\u73b0\u4f18\u5f02\uff0c\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u548csoftmax\u622a\u65ad\u663e\u8457\u63d0\u5347\u63a7\u5236\u4e0e\u751f\u6210\u7684\u4e00\u81f4\u6027\uff0c\u9002\u914d\u5668\u65b9\u6cd5\u5728\u6709\u9650\u6570\u636e\u4e0b\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u4f46\u4e00\u81f4\u6027\u8f83\u5dee\u3002", "conclusion": "\u8bba\u6587\u4e3a\u57fa\u4e8eTransformer\u7684\u7a7a\u95f4\u63a7\u5236\u751f\u6210\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u6f84\u6e05\u4e86\u6280\u672f\u7ec6\u8282\u5e76\u586b\u8865\u4e86\u77e5\u8bc6\u7a7a\u767d\u3002"}}
{"id": "2507.15728", "pdf": "https://arxiv.org/pdf/2507.15728", "abs": "https://arxiv.org/abs/2507.15728", "authors": ["Wenqi Ouyang", "Zeqi Xiao", "Danni Yang", "Yifan Zhou", "Shuai Yang", "Lei Yang", "Jianlou Si", "Xingang Pan"], "title": "TokensGen: Harnessing Condensed Tokens for Long Video Generation", "categories": ["cs.CV"], "comment": "Project page: https://vicky0522.github.io/tokensgen-webpage/", "summary": "Generating consistent long videos is a complex challenge: while diffusion-based generative models generate visually impressive short clips, extending them to longer durations often leads to memory bottlenecks and long-term inconsistency. In this paper, we propose TokensGen, a novel two-stage framework that leverages condensed tokens to address these issues. Our method decomposes long video generation into three core tasks: (1) inner-clip semantic control, (2) long-term consistency control, and (3) inter-clip smooth transition. First, we train To2V (Token-to-Video), a short video diffusion model guided by text and video tokens, with a Video Tokenizer that condenses short clips into semantically rich tokens. Second, we introduce T2To (Text-to-Token), a video token diffusion transformer that generates all tokens at once, ensuring global consistency across clips. Finally, during inference, an adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips, reducing boundary artifacts and enhancing smooth transitions. Experimental results demonstrate that our approach significantly enhances long-term temporal and content coherence without incurring prohibitive computational overhead. By leveraging condensed tokens and pre-trained short video models, our method provides a scalable, modular solution for long video generation, opening new possibilities for storytelling, cinematic production, and immersive simulations. Please see our project page at https://vicky0522.github.io/tokensgen-webpage/ .", "AI": {"tldr": "TokensGen\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u538b\u7f29\u7684token\u89e3\u51b3\u957f\u89c6\u9891\u751f\u6210\u4e2d\u7684\u5185\u5b58\u548c\u4e00\u81f4\u6027\u6311\u6218\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u77ed\u89c6\u9891\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u6269\u5c55\u5230\u957f\u89c6\u9891\u65f6\u9762\u4e34\u5185\u5b58\u74f6\u9888\u548c\u957f\u671f\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u8bad\u7ec3To2V\uff08Token-to-Video\uff09\u6a21\u578b\u548cT2To\uff08Text-to-Token\uff09\u6a21\u578b\uff0c\u4ee5\u53ca\u4f7f\u7528\u81ea\u9002\u5e94FIFO-Diffusion\u7b56\u7565\u5e73\u6ed1\u8fde\u63a5\u7247\u6bb5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u957f\u89c6\u9891\u7684\u65f6\u5e8f\u548c\u5185\u5bb9\u4e00\u81f4\u6027\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u53ef\u63a7\u3002", "conclusion": "TokensGen\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u4e3a\u957f\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u6545\u4e8b\u53d9\u8ff0\u3001\u7535\u5f71\u5236\u4f5c\u548c\u6c89\u6d78\u5f0f\u6a21\u62df\u3002"}}
{"id": "2507.15748", "pdf": "https://arxiv.org/pdf/2507.15748", "abs": "https://arxiv.org/abs/2507.15748", "authors": ["Jisu Shin", "Richard Shaw", "Seunghyun Shin", "Anton Pelykh", "Zhensong Zhang", "Hae-Gon Jeon", "Eduardo Perez-Pellitero"], "title": "Appearance Harmonization via Bilateral Grid Prediction with Transformers for 3DGS", "categories": ["cs.CV"], "comment": "10 pages, 3 figures, NeurIPS 2025 under review", "summary": "Modern camera pipelines apply extensive on-device processing, such as exposure adjustment, white balance, and color correction, which, while beneficial individually, often introduce photometric inconsistencies across views. These appearance variations violate multi-view consistency and degrade the quality of novel view synthesis. Joint optimization of scene representations and per-image appearance embeddings has been proposed to address this issue, but at the cost of increased computational complexity and slower training. In this work, we propose a transformer-based method that predicts spatially adaptive bilateral grids to correct photometric variations in a multi-view consistent manner, enabling robust cross-scene generalization without the need for scene-specific retraining. By incorporating the learned grids into the 3D Gaussian Splatting pipeline, we improve reconstruction quality while maintaining high training efficiency. Extensive experiments show that our approach outperforms or matches existing scene-specific optimization methods in reconstruction fidelity and convergence speed.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u7a7a\u95f4\u81ea\u9002\u5e94\u53cc\u8fb9\u7f51\u683c\u6765\u6821\u6b63\u591a\u89c6\u89d2\u95f4\u7684\u5149\u5ea6\u53d8\u5316\uff0c\u63d0\u53473D\u9ad8\u65af\u6e85\u5c04\u7ba1\u7ebf\u7684\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u73b0\u4ee3\u76f8\u673a\u5904\u7406\u6d41\u7a0b\u5bfc\u81f4\u591a\u89c6\u89d2\u95f4\u5149\u5ea6\u4e0d\u4e00\u81f4\uff0c\u5f71\u54cd\u65b0\u89c6\u89d2\u5408\u6210\u8d28\u91cf\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u4f18\u5316\u573a\u666f\u8868\u793a\u548c\u6bcf\u5f20\u56fe\u50cf\u7684\u5916\u89c2\u5d4c\u5165\u6765\u89e3\u51b3\uff0c\u4f46\u589e\u52a0\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u8bad\u7ec3\u65f6\u95f4\u3002", "method": "\u4f7f\u7528Transformer\u9884\u6d4b\u7a7a\u95f4\u81ea\u9002\u5e94\u53cc\u8fb9\u7f51\u683c\uff0c\u6821\u6b63\u5149\u5ea6\u53d8\u5316\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u52303D\u9ad8\u65af\u6e85\u5c04\u7ba1\u7ebf\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u6536\u655b\u901f\u5ea6\u4e0a\u4f18\u4e8e\u6216\u5339\u914d\u73b0\u6709\u573a\u666f\u7279\u5b9a\u4f18\u5316\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u573a\u666f\u7279\u5b9a\u91cd\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u8de8\u573a\u666f\u7684\u9c81\u68d2\u6cdb\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2507.15765", "pdf": "https://arxiv.org/pdf/2507.15765", "abs": "https://arxiv.org/abs/2507.15765", "authors": ["Feng-Qi Cui", "Anyang Tong", "Jinyang Huang", "Jie Zhang", "Dan Guo", "Zhi Liu", "Meng Wang"], "title": "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization", "categories": ["cs.CV"], "comment": "Accepted by ACM MM'25", "summary": "Dynamic Facial Expression Recognition (DFER) plays a critical role in affective computing and human-computer interaction. Although existing methods achieve comparable performance, they inevitably suffer from performance degradation under sample heterogeneity caused by multi-source data and individual expression variability. To address these challenges, we propose a novel framework, called Heterogeneity-aware Distributional Framework (HDF), and design two plug-and-play modules to enhance time-frequency modeling and mitigate optimization imbalance caused by hard samples. Specifically, the Time-Frequency Distributional Attention Module (DAM) captures both temporal consistency and frequency robustness through a dual-branch attention design, improving tolerance to sequence inconsistency and visual style shifts. Then, based on gradient sensitivity and information bottleneck principles, an adaptive optimization module Distribution-aware Scaling Module (DSM) is introduced to dynamically balance classification and contrastive losses, enabling more stable and discriminative representation learning. Extensive experiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF significantly improves both recognition accuracy and robustness. Our method achieves superior weighted average recall (WAR) and unweighted average recall (UAR) while maintaining strong generalization across diverse and imbalanced scenarios. Codes are released at https://github.com/QIcita/HDF_DFER.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHDF\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4-\u9891\u7387\u5206\u5e03\u6ce8\u610f\u529b\u6a21\u5757\u548c\u81ea\u9002\u5e94\u4f18\u5316\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u4e2d\u7684\u6837\u672c\u5f02\u8d28\u6027\u548c\u4f18\u5316\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc6\u522b\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6e90\u6570\u636e\u548c\u4e2a\u4f53\u8868\u8fbe\u53d8\u5f02\u6027\u5bfc\u81f4\u7684\u6837\u672c\u5f02\u8d28\u6027\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u65f6\u95f4-\u9891\u7387\u5206\u5e03\u6ce8\u610f\u529b\u6a21\u5757\uff08DAM\uff09\u548c\u5206\u5e03\u611f\u77e5\u7f29\u653e\u6a21\u5757\uff08DSM\uff09\uff0c\u5206\u522b\u589e\u5f3a\u65f6\u95f4-\u9891\u7387\u5efa\u6a21\u548c\u4f18\u5316\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u5728DFEW\u548cFERV39k\u6570\u636e\u96c6\u4e0a\uff0cHDF\u663e\u8457\u63d0\u9ad8\u4e86\u8bc6\u522b\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u53d6\u5f97\u4e86\u4f18\u5f02\u7684WAR\u548cUAR\u3002", "conclusion": "HDF\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6837\u672c\u5f02\u8d28\u6027\u548c\u4f18\u5316\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.15793", "pdf": "https://arxiv.org/pdf/2507.15793", "abs": "https://arxiv.org/abs/2507.15793", "authors": ["Ghassen Baklouti", "Julio Silva-Rodr\u00edguez", "Jose Dolz", "Houda Bahig", "Ismail Ben Ayed"], "title": "Regularized Low-Rank Adaptation for Few-Shot Organ Segmentation", "categories": ["cs.CV"], "comment": "Accepted at MICCAI 2025", "summary": "Parameter-efficient fine-tuning (PEFT) of pre-trained foundation models is increasingly attracting interest in medical imaging due to its effectiveness and computational efficiency. Among these methods, Low-Rank Adaptation (LoRA) is a notable approach based on the assumption that the adaptation inherently occurs in a low-dimensional subspace. While it has shown good performance, its implementation requires a fixed and unalterable rank, which might be challenging to select given the unique complexities and requirements of each medical imaging downstream task. Inspired by advancements in natural image processing, we introduce a novel approach for medical image segmentation that dynamically adjusts the intrinsic rank during adaptation. Viewing the low-rank representation of the trainable weight matrices as a singular value decomposition, we introduce an l_1 sparsity regularizer to the loss function, and tackle it with a proximal optimizer. The regularizer could be viewed as a penalty on the decomposition rank. Hence, its minimization enables to find task-adapted ranks automatically. Our method is evaluated in a realistic few-shot fine-tuning setting, where we compare it first to the standard LoRA and then to several other PEFT methods across two distinguishable tasks: base organs and novel organs. Our extensive experiments demonstrate the significant performance improvements driven by our method, highlighting its efficiency and robustness against suboptimal rank initialization. Our code is publicly available: https://github.com/ghassenbaklouti/ARENA", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u79e9\u7684PEFT\u65b9\u6cd5\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u901a\u8fc7l1\u6b63\u5219\u5316\u81ea\u52a8\u4f18\u5316\u79e9\u9009\u62e9\u3002", "motivation": "\u89e3\u51b3LoRA\u65b9\u6cd5\u4e2d\u56fa\u5b9a\u79e9\u9009\u62e9\u5bf9\u533b\u5b66\u56fe\u50cf\u4efb\u52a1\u590d\u6742\u6027\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165l1\u7a00\u758f\u6b63\u5219\u5316\u5230\u635f\u5931\u51fd\u6570\uff0c\u4f7f\u7528\u8fd1\u7aef\u4f18\u5316\u5668\u52a8\u6001\u8c03\u6574\u79e9\u3002", "result": "\u5728\u5c11\u91cf\u6837\u672c\u5fae\u8c03\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6807\u51c6LoRA\u548c\u5176\u4ed6PEFT\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u3001\u9c81\u68d2\uff0c\u80fd\u81ea\u52a8\u9002\u5e94\u4efb\u52a1\u9700\u6c42\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2507.15824", "pdf": "https://arxiv.org/pdf/2507.15824", "abs": "https://arxiv.org/abs/2507.15824", "authors": ["Enes Sanli", "Baris Sarper Tezcan", "Aykut Erdem", "Erkut Erdem"], "title": "Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models", "categories": ["cs.CV"], "comment": null, "summary": "Recent progress in text-to-video (T2V) generation has enabled the synthesis of visually compelling and temporally coherent videos from natural language. However, these models often fall short in basic physical commonsense, producing outputs that violate intuitive expectations around causality, object behavior, and tool use. Addressing this gap, we present PhysVidBench, a benchmark designed to evaluate the physical reasoning capabilities of T2V systems. The benchmark includes 383 carefully curated prompts, emphasizing tool use, material properties, and procedural interactions, and domains where physical plausibility is crucial. For each prompt, we generate videos using diverse state-of-the-art models and adopt a three-stage evaluation pipeline: (1) formulate grounded physics questions from the prompt, (2) caption the generated video with a vision-language model, and (3) task a language model to answer several physics-involved questions using only the caption. This indirect strategy circumvents common hallucination issues in direct video-based evaluation. By highlighting affordances and tool-mediated actions, areas overlooked in current T2V evaluations, PhysVidBench provides a structured, interpretable framework for assessing physical commonsense in generative video models.", "AI": {"tldr": "PhysVidBench\u662f\u4e00\u4e2a\u8bc4\u4f30\u6587\u672c\u5230\u89c6\u9891\uff08T2V\uff09\u751f\u6210\u6a21\u578b\u7269\u7406\u5e38\u8bc6\u7684\u57fa\u51c6\uff0c\u5305\u542b383\u4e2a\u63d0\u793a\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u8bc4\u4f30\u6d41\u7a0b\u95f4\u63a5\u6d4b\u8bd5\u6a21\u578b\u7684\u7269\u7406\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524dT2V\u6a21\u578b\u5728\u7269\u7406\u5e38\u8bc6\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u5982\u8fdd\u53cd\u56e0\u679c\u5173\u7cfb\u548c\u5bf9\u8c61\u884c\u4e3a\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u8bc4\u4f30\u5176\u7269\u7406\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPhysVidBench\u57fa\u51c6\uff0c\u5305\u542b383\u4e2a\u63d0\u793a\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u8bc4\u4f30\u6d41\u7a0b\uff1a\u751f\u6210\u7269\u7406\u95ee\u9898\u3001\u89c6\u9891\u63cf\u8ff0\u3001\u8bed\u8a00\u6a21\u578b\u56de\u7b54\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u95f4\u63a5\u8bc4\u4f30\u7b56\u7565\uff0c\u907f\u514d\u4e86\u76f4\u63a5\u89c6\u9891\u8bc4\u4f30\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u4e3aT2V\u6a21\u578b\u7684\u7269\u7406\u5e38\u8bc6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u6846\u67b6\u3002", "conclusion": "PhysVidBench\u586b\u8865\u4e86\u5f53\u524dT2V\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u751f\u6210\u89c6\u9891\u6a21\u578b\u7684\u7269\u7406\u5e38\u8bc6\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2507.15856", "pdf": "https://arxiv.org/pdf/2507.15856", "abs": "https://arxiv.org/abs/2507.15856", "authors": ["Jiawei Yang", "Tianhong Li", "Lijie Fan", "Yonglong Tian", "Yue Wang"], "title": "Latent Denoising Makes Good Visual Tokenizers", "categories": ["cs.CV"], "comment": "Code is available at: https://github.com/Jiawei-Yang/DeTok", "summary": "Despite their fundamental role, it remains unclear what properties could make visual tokenizers more effective for generative modeling. We observe that modern generative models share a conceptually similar training objective -- reconstructing clean signals from corrupted inputs such as Gaussian noise or masking -- a process we term denoising. Motivated by this insight, we propose aligning tokenizer embeddings directly with the downstream denoising objective, encouraging latent embeddings to be more easily reconstructed even when heavily corrupted. To achieve this, we introduce the Latent Denoising Tokenizer (l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images from latent embeddings corrupted by interpolative noise and random masking. Extensive experiments on ImageNet 256x256 demonstrate that our tokenizer consistently outperforms standard tokenizers across six representative generative models. Our findings highlight denoising as a fundamental design principle for tokenizer development, and we hope it could motivate new perspectives for future tokenizer design.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u5206\u8bcd\u5668\uff08l-DeTok\uff09\uff0c\u901a\u8fc7\u76f4\u63a5\u4e0e\u53bb\u566a\u76ee\u6807\u5bf9\u9f50\uff0c\u63d0\u5347\u751f\u6210\u6a21\u578b\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u4ee3\u751f\u6210\u6a21\u578b\u7684\u8bad\u7ec3\u76ee\u6807\uff08\u5982\u53bb\u566a\uff09\u4e0e\u5206\u8bcd\u5668\u7684\u6f5c\u5728\u5d4c\u5165\u672a\u5bf9\u9f50\uff0c\u9650\u5236\u4e86\u751f\u6210\u6548\u679c\u3002", "method": "\u63d0\u51faLatent Denoising Tokenizer\uff08l-DeTok\uff09\uff0c\u901a\u8fc7\u91cd\u6784\u88ab\u566a\u58f0\u548c\u63a9\u7801\u6c61\u67d3\u7684\u6f5c\u5728\u5d4c\u5165\u6765\u8bad\u7ec3\u5206\u8bcd\u5668\u3002", "result": "\u5728ImageNet 256x256\u4e0a\uff0cl-DeTok\u5728\u516d\u79cd\u4ee3\u8868\u6027\u751f\u6210\u6a21\u578b\u4e2d\u5747\u4f18\u4e8e\u6807\u51c6\u5206\u8bcd\u5668\u3002", "conclusion": "\u53bb\u566a\u5e94\u4f5c\u4e3a\u5206\u8bcd\u5668\u8bbe\u8ba1\u7684\u57fa\u672c\u539f\u5219\uff0c\u4e3a\u672a\u6765\u5206\u8bcd\u5668\u5f00\u53d1\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.14308", "pdf": "https://arxiv.org/pdf/2507.14308", "abs": "https://arxiv.org/abs/2507.14308", "authors": ["Jingjia Chen", "Haoyang Pei", "Christoph Maier", "Mary Bruno", "Qiuting Wen", "Seon-Hi Shin", "William Moore", "Hersh Chandarana", "Li Feng"], "title": "Self-Supervised Joint Reconstruction and Denoising of T2-Weighted PROPELLER MRI of the Lungs at 0.55T", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Purpose: This study aims to improve 0.55T T2-weighted PROPELLER lung MRI through a self-supervised joint reconstruction and denoising model.   Methods: T2-weighted 0.55T lung MRI dataset including 44 patients with previous covid infection were used. A self-supervised learning framework was developed, where each blade of the PROPELLER acquisition was split along the readout direction into two partitions. One subset trains the unrolled reconstruction network, while the other subset is used for loss calculation, enabling self-supervised training without clean targets and leveraging matched noise statistics for denoising. For comparison, Marchenko-Pastur Principal Component Analysis (MPPCA) was performed along the coil dimension, followed by conventional parallel imaging reconstruction. The quality of the reconstructed lung MRI was assessed visually by two experienced radiologists independently.   Results: The proposed self-supervised model improved the clarity and structural integrity of the lung images. For cases with available CT scans, the reconstructed images demonstrated strong alignment with corresponding CT images. Additionally, the proposed model enables further scan time reduction by requiring only half the number of blades. Reader evaluations confirmed that the proposed method outperformed MPPCA-denoised images across all categories (Wilcoxon signed-rank test, p<0.001), with moderate inter-reader agreement (weighted Cohen's kappa=0.55; percentage of exact and within +/-1 point agreement=91%).   Conclusion: By leveraging intrinsic structural redundancies between two disjoint splits of k-space subsets, the proposed self-supervised learning model effectively reconstructs the image while suppressing the noise for 0.55T T2-weighted lung MRI with PROPELLER sampling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u8054\u5408\u91cd\u5efa\u548c\u53bb\u566a\u6a21\u578b\uff0c\u7528\u4e8e\u6539\u8fdb0.55T T2\u52a0\u6743PROPELLER\u80ba\u90e8MRI\u6210\u50cf\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u63d0\u5347\u4f4e\u573a\u5f3a\u80ba\u90e8MRI\u7684\u56fe\u50cf\u6e05\u6670\u5ea6\u548c\u7ed3\u6784\u5b8c\u6574\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u626b\u63cf\u65f6\u95f4\u3002", "method": "\u4f7f\u752844\u4f8bCOVID-19\u5eb7\u590d\u60a3\u8005\u76840.55T T2\u52a0\u6743\u80ba\u90e8MRI\u6570\u636e\uff0c\u5c06PROPELLER\u91c7\u96c6\u7684\u6bcf\u4e2a\u53f6\u7247\u6cbf\u8bfb\u51fa\u65b9\u5411\u5206\u4e3a\u4e24\u90e8\u5206\uff0c\u4e00\u90e8\u5206\u7528\u4e8e\u8bad\u7ec3\u91cd\u5efa\u7f51\u7edc\uff0c\u53e6\u4e00\u90e8\u5206\u7528\u4e8e\u635f\u5931\u8ba1\u7b97\uff0c\u5b9e\u73b0\u65e0\u6e05\u6d01\u76ee\u6807\u7684\u81ea\u76d1\u7763\u8bad\u7ec3\u3002", "result": "\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u6e05\u6670\u5ea6\u548c\u7ed3\u6784\u5b8c\u6574\u6027\uff0c\u4e0eCT\u56fe\u50cf\u5bf9\u9f50\u826f\u597d\uff0c\u4e14\u4ec5\u9700\u4e00\u534a\u53f6\u7247\u6570\u91cf\u5373\u53ef\u5b8c\u6210\u626b\u63cf\u3002\u653e\u5c04\u79d1\u533b\u5e08\u8bc4\u4f30\u663e\u793a\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edfMPPCA\u53bb\u566a\uff08p<0.001\uff09\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528k\u7a7a\u95f4\u5b50\u96c6\u7684\u5185\u5728\u7ed3\u6784\u5197\u4f59\uff0c\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u6709\u6548\u5b9e\u73b0\u4e860.55T T2\u52a0\u6743PROPELLER\u80ba\u90e8MRI\u7684\u56fe\u50cf\u91cd\u5efa\u548c\u53bb\u566a\u3002"}}
{"id": "2507.14542", "pdf": "https://arxiv.org/pdf/2507.14542", "abs": "https://arxiv.org/abs/2507.14542", "authors": ["Yipeng Zhang", "Yuanyi Ding", "Chenda Duan", "Atsuro Daida", "Hiroki Nariai", "Vwani Roychowdhury"], "title": "Self-Supervised Distillation of Legacy Rule-Based Methods for Enhanced EEG-Based Decision-Making", "categories": ["cs.CE", "cs.CV"], "comment": null, "summary": "High-frequency oscillations (HFOs) in intracranial Electroencephalography (iEEG) are critical biomarkers for localizing the epileptogenic zone in epilepsy treatment. However, traditional rule-based detectors for HFOs suffer from unsatisfactory precision, producing false positives that require time-consuming manual review. Supervised machine learning approaches have been used to classify the detection results, yet they typically depend on labeled datasets, which are difficult to acquire due to the need for specialized expertise. Moreover, accurate labeling of HFOs is challenging due to low inter-rater reliability and inconsistent annotation practices across institutions. The lack of a clear consensus on what constitutes a pathological HFO further challenges supervised refinement approaches. To address this, we leverage the insight that legacy detectors reliably capture clinically relevant signals despite their relatively high false positive rates. We thus propose the Self-Supervised to Label Discovery (SS2LD) framework to refine the large set of candidate events generated by legacy detectors into a precise set of pathological HFOs. SS2LD employs a variational autoencoder (VAE) for morphological pre-training to learn meaningful latent representation of the detected events. These representations are clustered to derive weak supervision for pathological events. A classifier then uses this supervision to refine detection boundaries, trained on real and VAE-augmented data. Evaluated on large multi-institutional interictal iEEG datasets, SS2LD outperforms state-of-the-art methods. SS2LD offers a scalable, label-efficient, and clinically effective strategy to identify pathological HFOs using legacy detectors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u6807\u7b7e\u53d1\u73b0\u6846\u67b6\uff08SS2LD\uff09\uff0c\u7528\u4e8e\u4ece\u4f20\u7edf\u68c0\u6d4b\u5668\u751f\u6210\u7684\u5927\u91cf\u5019\u9009\u4e8b\u4ef6\u4e2d\u7cbe\u786e\u8bc6\u522b\u75c5\u7406\u9ad8\u9891\u632f\u8361\uff08HFOs\uff09\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684HFO\u68c0\u6d4b\u5668\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u4e14\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u96be\u4ee5\u83b7\u53d6\u7684\u6807\u6ce8\u6570\u636e\u3002SS2LD\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5229\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u8fdb\u884c\u5f62\u6001\u9884\u8bad\u7ec3\uff0c\u5b66\u4e60\u4e8b\u4ef6\u7684\u6f5c\u5728\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u805a\u7c7b\u751f\u6210\u5f31\u76d1\u7763\u4fe1\u53f7\uff0c\u8bad\u7ec3\u5206\u7c7b\u5668\u4f18\u5316\u68c0\u6d4b\u8fb9\u754c\u3002", "result": "\u5728\u591a\u673a\u6784\u95f4\u671fiEEG\u6570\u636e\u96c6\u4e0a\uff0cSS2LD\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u75c5\u7406HFO\u8bc6\u522b\u7b56\u7565\u3002", "conclusion": "SS2LD\u4e3a\u5229\u7528\u4f20\u7edf\u68c0\u6d4b\u5668\u8bc6\u522b\u75c5\u7406HFO\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14760", "pdf": "https://arxiv.org/pdf/2507.14760", "abs": "https://arxiv.org/abs/2507.14760", "authors": ["Cassandra Tong Ye", "Shamus Li", "Tyler King", "Kristina Monakhova"], "title": "QUTCC: Quantile Uncertainty Training and Conformal Calibration for Imaging Inverse Problems", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Deep learning models often hallucinate, producing realistic artifacts that are not truly present in the sample. This can have dire consequences for scientific and medical inverse problems, such as MRI and microscopy denoising, where accuracy is more important than perceptual quality. Uncertainty quantification techniques, such as conformal prediction, can pinpoint outliers and provide guarantees for image regression tasks, improving reliability. However, existing methods utilize a linear constant scaling factor to calibrate uncertainty bounds, resulting in larger, less informative bounds. We propose QUTCC, a quantile uncertainty training and calibration technique that enables nonlinear, non-uniform scaling of quantile predictions to enable tighter uncertainty estimates. Using a U-Net architecture with a quantile embedding, QUTCC enables the prediction of the full conditional distribution of quantiles for the imaging task. During calibration, QUTCC generates uncertainty bounds by iteratively querying the network for upper and lower quantiles, progressively refining the bounds to obtain a tighter interval that captures the desired coverage. We evaluate our method on several denoising tasks as well as compressive MRI reconstruction. Our method successfully pinpoints hallucinations in image estimates and consistently achieves tighter uncertainty intervals than prior methods while maintaining the same statistical coverage.", "AI": {"tldr": "\u63d0\u51faQUTCC\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u6821\u51c6\u91cf\u5316\u9884\u6d4b\uff0c\u63d0\u4f9b\u66f4\u7d27\u7684\u4e0d\u786e\u5b9a\u6027\u533a\u95f4\uff0c\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60\u5728\u533b\u5b66\u6210\u50cf\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u533b\u5b66\u6210\u50cf\u4efb\u52a1\u4e2d\u53ef\u80fd\u4ea7\u751f\u5e7b\u89c9\uff0c\u5f71\u54cd\u51c6\u786e\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u786e\u5b9a\u6027\u8fb9\u754c\u8fc7\u5927\u4e14\u4e0d\u7cbe\u786e\u3002", "method": "QUTCC\u7ed3\u5408U-Net\u67b6\u6784\u548c\u91cf\u5316\u5d4c\u5165\uff0c\u9884\u6d4b\u6761\u4ef6\u5206\u5e03\uff0c\u901a\u8fc7\u8fed\u4ee3\u6821\u51c6\u751f\u6210\u66f4\u7d27\u7684\u4e0d\u786e\u5b9a\u6027\u533a\u95f4\u3002", "result": "QUTCC\u5728\u53bb\u566a\u548cMRI\u91cd\u5efa\u4efb\u52a1\u4e2d\u6210\u529f\u8bc6\u522b\u5e7b\u89c9\uff0c\u5e76\u63d0\u4f9b\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u7d27\u7684\u4e0d\u786e\u5b9a\u6027\u533a\u95f4\u3002", "conclusion": "QUTCC\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u533b\u5b66\u6210\u50cf\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u3002"}}
{"id": "2507.15078", "pdf": "https://arxiv.org/pdf/2507.15078", "abs": "https://arxiv.org/abs/2507.15078", "authors": ["Fumio Hashimoto", "Kuang Gong"], "title": "PET Image Reconstruction Using Deep Diffusion Image Prior", "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "comment": "11 pages, 11 figures", "summary": "Diffusion models have shown great promise in medical image denoising and reconstruction, but their application to Positron Emission Tomography (PET) imaging remains limited by tracer-specific contrast variability and high computational demands. In this work, we proposed an anatomical prior-guided PET image reconstruction method based on diffusion models, inspired by the deep diffusion image prior (DDIP) framework. The proposed method alternated between diffusion sampling and model fine-tuning guided by the PET sinogram, enabling the reconstruction of high-quality images from various PET tracers using a score function pretrained on a dataset of another tracer. To improve computational efficiency, the half-quadratic splitting (HQS) algorithm was adopted to decouple network optimization from iterative PET reconstruction. The proposed method was evaluated using one simulation and two clinical datasets. For the simulation study, a model pretrained on [$^{18}$F]FDG data was tested on amyloid-negative PET data to assess out-of-distribution (OOD) performance. For the clinical-data validation, ten low-dose [$^{18}$F]FDG datasets and one [$^{18}$F]Florbetapir dataset were tested on a model pretrained on data from another tracer. Experiment results show that the proposed PET reconstruction method can generalize robustly across tracer distributions and scanner types, providing an efficient and versatile reconstruction framework for low-dose PET imaging.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684PET\u56fe\u50cf\u91cd\u5efa\u65b9\u6cd5\uff0c\u7ed3\u5408\u89e3\u5256\u5b66\u5148\u9a8c\u548c\u534a\u4e8c\u6b21\u5206\u88c2\u7b97\u6cd5\uff0c\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u8de8\u793a\u8e2a\u5242\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3PET\u6210\u50cf\u4e2d\u793a\u8e2a\u5242\u7279\u5f02\u6027\u5bf9\u6bd4\u5ea6\u53d8\u5316\u548c\u9ad8\u8ba1\u7b97\u9700\u6c42\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6269\u6563\u91c7\u6837\u548c\u6a21\u578b\u5fae\u8c03\u4ea4\u66ff\u8fdb\u884c\uff0c\u7ed3\u5408HQS\u7b97\u6cd5\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728\u6a21\u62df\u548c\u4e34\u5e8a\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u9ad8\u6548\u6027\uff0c\u9002\u7528\u4e8e\u4f4e\u5242\u91cfPET\u6210\u50cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4f4e\u5242\u91cfPET\u6210\u50cf\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u91cd\u5efa\u6846\u67b6\u3002"}}
{"id": "2507.15340", "pdf": "https://arxiv.org/pdf/2507.15340", "abs": "https://arxiv.org/abs/2507.15340", "authors": ["Marc Boubnovski Martell", "Kristofer Linton-Reid", "Mitchell Chen", "Sumeet Hindocha", "Benjamin Hunter", "Marco A. Calzado", "Richard Lee", "Joram M. Posma", "Eric O. Aboagye"], "title": "MedSR-Impact: Transformer-Based Super-Resolution for Lung CT Segmentation, Radiomics, Classification, and Prognosis", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "High-resolution volumetric computed tomography (CT) is essential for accurate diagnosis and treatment planning in thoracic diseases; however, it is limited by radiation dose and hardware costs. We present the Transformer Volumetric Super-Resolution Network (\\textbf{TVSRN-V2}), a transformer-based super-resolution (SR) framework designed for practical deployment in clinical lung CT analysis. Built from scalable components, including Through-Plane Attention Blocks (TAB) and Swin Transformer V2 -- our model effectively reconstructs fine anatomical details in low-dose CT volumes and integrates seamlessly with downstream analysis pipelines. We evaluate its effectiveness on three critical lung cancer tasks -- lobe segmentation, radiomics, and prognosis -- across multiple clinical cohorts. To enhance robustness across variable acquisition protocols, we introduce pseudo-low-resolution augmentation, simulating scanner diversity without requiring private data. TVSRN-V2 demonstrates a significant improvement in segmentation accuracy (+4\\% Dice), higher radiomic feature reproducibility, and enhanced predictive performance (+0.06 C-index and AUC). These results indicate that SR-driven recovery of structural detail significantly enhances clinical decision support, positioning TVSRN-V2 as a well-engineered, clinically viable system for dose-efficient imaging and quantitative analysis in real-world CT workflows.", "AI": {"tldr": "TVSRN-V2\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u4f4e\u5242\u91cfCT\u56fe\u50cf\u7684\u5206\u8fa8\u7387\uff0c\u663e\u8457\u6539\u5584\u80ba\u764c\u8bca\u65ad\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387CT\u5bf9\u80f8\u90e8\u75be\u75c5\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u53d7\u9650\u4e8e\u8f90\u5c04\u5242\u91cf\u548c\u786c\u4ef6\u6210\u672c\u3002TVSRN-V2\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u4e34\u5e8a\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528Through-Plane Attention Blocks\u548cSwin Transformer V2\u6784\u5efa\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u4f2a\u4f4e\u5206\u8fa8\u7387\u589e\u5f3a\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "result": "\u5728\u80ba\u53f6\u5206\u5272\u3001\u653e\u5c04\u7ec4\u5b66\u548c\u9884\u540e\u4efb\u52a1\u4e2d\uff0cTVSRN-V2\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u51c6\u786e\u6027\uff08+4% Dice\uff09\u3001\u7279\u5f81\u53ef\u91cd\u590d\u6027\u548c\u9884\u6d4b\u6027\u80fd\uff08+0.06 C-index\u548cAUC\uff09\u3002", "conclusion": "TVSRN-V2\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u4e34\u5e8a\u9002\u7528\u7684\u7cfb\u7edf\uff0c\u80fd\u591f\u901a\u8fc7\u8d85\u5206\u8fa8\u7387\u6280\u672f\u63d0\u5347CT\u56fe\u50cf\u8d28\u91cf\uff0c\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u3002"}}
{"id": "2507.15361", "pdf": "https://arxiv.org/pdf/2507.15361", "abs": "https://arxiv.org/abs/2507.15361", "authors": ["Muhammad Aqeel", "Maham Nazir", "Zanxi Ruan", "Francesco Setti"], "title": "Latent Space Synergy: Text-Guided Data Augmentation for Direct Diffusion Biomedical Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Accepted to CVGMMI Workshop at ICIAP 2025", "summary": "Medical image segmentation suffers from data scarcity, particularly in polyp detection where annotation requires specialized expertise. We present SynDiff, a framework combining text-guided synthetic data generation with efficient diffusion-based segmentation. Our approach employs latent diffusion models to generate clinically realistic synthetic polyps through text-conditioned inpainting, augmenting limited training data with semantically diverse samples. Unlike traditional diffusion methods requiring iterative denoising, we introduce direct latent estimation enabling single-step inference with T x computational speedup. On CVC-ClinicDB, SynDiff achieves 96.0% Dice and 92.9% IoU while maintaining real-time capability suitable for clinical deployment. The framework demonstrates that controlled synthetic augmentation improves segmentation robustness without distribution shift. SynDiff bridges the gap between data-hungry deep learning models and clinical constraints, offering an efficient solution for deployment in resourcelimited medical settings.", "AI": {"tldr": "SynDiff\u7ed3\u5408\u6587\u672c\u5f15\u5bfc\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u9ad8\u6548\u6269\u6563\u5206\u5272\uff0c\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff08\u5982\u606f\u8089\u68c0\u6d4b\uff09\u56e0\u6807\u6ce8\u9700\u4e13\u4e1a\u77e5\u8bc6\u800c\u6570\u636e\u7a00\u7f3a\uff0c\u9650\u5236\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u901a\u8fc7\u6587\u672c\u6761\u4ef6\u4fee\u590d\u751f\u6210\u4e34\u5e8a\u771f\u5b9e\u7684\u5408\u6210\u606f\u8089\uff0c\u5e76\u5f15\u5165\u76f4\u63a5\u6f5c\u5728\u4f30\u8ba1\u5b9e\u73b0\u5355\u6b65\u63a8\u7406\uff0c\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728CVC-ClinicDB\u4e0a\u8fbe\u523096.0% Dice\u548c92.9% IoU\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "SynDiff\u901a\u8fc7\u53ef\u63a7\u5408\u6210\u589e\u5f3a\u63d0\u5347\u4e86\u5206\u5272\u9c81\u68d2\u6027\uff0c\u4e3a\u8d44\u6e90\u6709\u9650\u7684\u533b\u7597\u573a\u666f\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15476", "pdf": "https://arxiv.org/pdf/2507.15476", "abs": "https://arxiv.org/abs/2507.15476", "authors": ["Cong Chen", "Ming Chen", "Hoileong Lee", "Yan Li", "Jiyang Yu"], "title": "A Steel Surface Defect Detection Method Based on Lightweight Convolution Optimization", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Surface defect detection of steel, especially the recognition of multi-scale defects, has always been a major challenge in industrial manufacturing. Steel surfaces not only have defects of various sizes and shapes, which limit the accuracy of traditional image processing and detection methods in complex environments. However, traditional defect detection methods face issues of insufficient accuracy and high miss-detection rates when dealing with small target defects. To address this issue, this study proposes a detection framework based on deep learning, specifically YOLOv9s, combined with the C3Ghost module, SCConv module, and CARAFE upsampling operator, to improve detection accuracy and model performance. First, the SCConv module is used to reduce feature redundancy and optimize feature representation by reconstructing the spatial and channel dimensions. Second, the C3Ghost module is introduced to enhance the model's feature extraction ability by reducing redundant computations and parameter volume, thereby improving model efficiency. Finally, the CARAFE upsampling operator, which can more finely reorganize feature maps in a content-aware manner, optimizes the upsampling process and ensures detailed restoration of high-resolution defect regions. Experimental results demonstrate that the proposed model achieves higher accuracy and robustness in steel surface defect detection tasks compared to other methods, effectively addressing defect detection problems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eYOLOv9s\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408C3Ghost\u6a21\u5757\u3001SCConv\u6a21\u5757\u548cCARAFE\u4e0a\u91c7\u6837\u7b97\u5b50\uff0c\u7528\u4e8e\u63d0\u9ad8\u94a2\u8868\u9762\u591a\u5c3a\u5ea6\u7f3a\u9677\u68c0\u6d4b\u7684\u7cbe\u5ea6\u548c\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u94a2\u8868\u9762\u7f3a\u9677\u68c0\u6d4b\u5728\u5de5\u4e1a\u5236\u9020\u4e2d\u5177\u6709\u6311\u6218\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u5bf9\u5c0f\u76ee\u6807\u7f3a\u9677\u68c0\u6d4b\u7cbe\u5ea6\u4e0d\u8db3\u4e14\u6f0f\u68c0\u7387\u9ad8\u3002", "method": "\u91c7\u7528SCConv\u6a21\u5757\u4f18\u5316\u7279\u5f81\u8868\u793a\uff0cC3Ghost\u6a21\u5757\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0cCARAFE\u4e0a\u91c7\u6837\u7b97\u5b50\u7cbe\u7ec6\u5316\u91cd\u7ec4\u7279\u5f81\u56fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u94a2\u8868\u9762\u7f3a\u9677\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u94a2\u8868\u9762\u7f3a\u9677\u68c0\u6d4b\u95ee\u9898\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002"}}
