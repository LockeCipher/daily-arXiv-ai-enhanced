{"id": "2508.20080", "pdf": "https://arxiv.org/pdf/2508.20080", "abs": "https://arxiv.org/abs/2508.20080", "authors": ["Changha Shin", "Woong Oh Cho", "Seon Joo Kim"], "title": "Seam360GS: Seamless 360\u00b0 Gaussian Splatting from Real-World Omnidirectional Images", "categories": ["cs.CV", "cs.GR"], "comment": "Accepted to ICCV 2025. 10 pages main text, 4 figures, 4 tables,   supplementary material included", "summary": "360-degree visual content is widely shared on platforms such as YouTube and plays a central role in virtual reality, robotics, and autonomous navigation. However, consumer-grade dual-fisheye systems consistently yield imperfect panoramas due to inherent lens separation and angular distortions. In this work, we introduce a novel calibration framework that incorporates a dual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approach not only simulates the realistic visual artifacts produced by dual-fisheye cameras but also enables the synthesis of seamlessly rendered 360-degree images. By jointly optimizing 3D Gaussian parameters alongside calibration variables that emulate lens gaps and angular distortions, our framework transforms imperfect omnidirectional inputs into flawless novel view synthesis. Extensive evaluations on real-world datasets confirm that our method produces seamless renderings-even from imperfect images-and outperforms existing 360-degree rendering models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u53cc\u9c7c\u773c\u76f8\u673a\u6a21\u578b\u96c6\u6210\u52303D\u9ad8\u65af\u6e85\u5c04\u7ba1\u9053\u4e2d\u7684\u65b0\u9896\u6821\u51c6\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u6709\u7f3a\u9677\u7684\u5168\u5411\u8f93\u5165\u751f\u6210\u65e0\u7f1d\u7684360\u5ea6\u65b0\u89c6\u89d2\u5408\u6210", "motivation": "\u6d88\u8d39\u7ea7\u53cc\u9c7c\u773c\u7cfb\u7edf\u7531\u4e8e\u56fa\u6709\u7684\u955c\u5934\u5206\u79bb\u548c\u89d2\u5ea6\u5931\u771f\uff0c\u603b\u662f\u4ea7\u751f\u4e0d\u5b8c\u7f8e\u7684\u5168\u666f\u56fe\u50cf\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u5904\u7406\u8fd9\u4e9b\u89c6\u89c9\u4f2a\u5f71\u5e76\u5b9e\u73b0\u65e0\u7f1d\u6e32\u67d3", "method": "\u5c06\u53cc\u9c7c\u773c\u76f8\u673a\u6a21\u578b\u6574\u5408\u52303D\u9ad8\u65af\u6e85\u5c04\u6d41\u7a0b\u4e2d\uff0c\u8054\u5408\u4f18\u53163D\u9ad8\u65af\u53c2\u6570\u548c\u6a21\u62df\u955c\u5934\u95f4\u9699\u3001\u89d2\u5ea6\u5931\u771f\u7684\u6821\u51c6\u53d8\u91cf", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8bc1\u5b9e\uff0c\u8be5\u65b9\u6cd5\u5373\u4f7f\u4ece\u4e0d\u5b8c\u7f8e\u56fe\u50cf\u4e5f\u80fd\u4ea7\u751f\u65e0\u7f1d\u6e32\u67d3\uff0c\u5e76\u4e14\u4f18\u4e8e\u73b0\u6709\u7684360\u5ea6\u6e32\u67d3\u6a21\u578b", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5730\u5c06\u4e0d\u5b8c\u7f8e\u7684\u5168\u5411\u8f93\u5165\u8f6c\u6362\u4e3a\u5b8c\u7f8e\u7684\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u4e3a\u53cc\u9c7c\u773c\u76f8\u673a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6821\u51c6\u548c\u6e32\u67d3\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.19254", "pdf": "https://arxiv.org/pdf/2508.19254", "abs": "https://arxiv.org/abs/2508.19254", "authors": ["Jookyung Song", "Mookyoung Kang", "Nojun Kwak"], "title": "Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "6 pages, 4 figures, NeurIPS Creative AI Track 2025", "summary": "This paper presents a real-time generative drawing system that interprets and integrates both formal intent - the structural, compositional, and stylistic attributes of a sketch - and contextual intent - the semantic and thematic meaning inferred from its visual content - into a unified transformation process. Unlike conventional text-prompt-based generative systems, which primarily capture high-level contextual descriptions, our approach simultaneously analyzes ground-level intuitive geometric features such as line trajectories, proportions, and spatial arrangement, and high-level semantic cues extracted via vision-language models. These dual intent signals are jointly conditioned in a multi-stage generation pipeline that combines contour-preserving structural control with style- and content-aware image synthesis. Implemented with a touchscreen-based interface and distributed inference architecture, the system achieves low-latency, two-stage transformation while supporting multi-user collaboration on shared canvases. The resulting platform enables participants, regardless of artistic expertise, to engage in synchronous, co-authored visual creation, redefining human-AI interaction as a process of co-creation and mutual enhancement.", "AI": {"tldr": "\u4e00\u4e2a\u5b9e\u65f6\u751f\u6210\u5f0f\u7ed8\u753b\u7cfb\u7edf\uff0c\u540c\u65f6\u89e3\u91ca\u7ed3\u6784\u610f\u56fe\u548c\u8bed\u4e49\u610f\u56fe\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u751f\u6210\u6d41\u6c34\u7ebf\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u7684\u534f\u4f5c\u7ed8\u753b\u3002", "motivation": "\u4f20\u7edf\u7684\u6587\u672c\u63d0\u793a\u751f\u6210\u7cfb\u7edf\u4e3b\u8981\u6293\u53d6\u9ad8\u7ea7\u522b\u8bed\u4e49\u63cf\u8ff0\uff0c\u7f3a\u4e4f\u5bf9\u57fa\u7840\u7ea7\u51e0\u4f55\u7279\u5f81\u7684\u5206\u6790\u3002\u672c\u6587\u65b9\u6cd5\u5c1d\u8bd5\u540c\u65f6\u89e3\u91ca\u7ed3\u6784\u6027\u548c\u8bed\u4e49\u6027\u610f\u56fe\uff0c\u4ee5\u652f\u6301\u66f4\u81ea\u7136\u7684\u4eba\u5de5\u667a\u80fd\u534f\u4f5c\u521b\u4f5c\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u540c\u65f6\u6761\u4ef6\u5316\u5730\u9762\u7ea7\u76f4\u89c2\u51e0\u4f55\u7279\u5f81\uff08\u7ebf\u6761\u8f68\u8ff9\u3001\u6bd4\u4f8b\u3001\u7a7a\u95f4\u5e03\u5c40\uff09\u548c\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u7684\u9ad8\u7ea7\u8bed\u4e49\u7ebf\u7d22\u3002\u7ed3\u5408\u8fb9\u7f18\u4fdd\u6301\u7684\u7ed3\u6784\u63a7\u5236\u4e0e\u98ce\u683c-\u5185\u5bb9\u611f\u77e5\u7684\u56fe\u50cf\u5408\u6210\u3002", "result": "\u5b9e\u73b0\u4e86\u57fa\u4e8e\u89e6\u6478\u5c4f\u754c\u9762\u548c\u5206\u5e03\u5f0f\u63a8\u7406\u67b6\u6784\u7684\u4f4e\u5ef6\u8fdf\u4e24\u9636\u6bb5\u8f6c\u6362\uff0c\u652f\u6301\u591a\u7528\u6237\u5728\u5171\u4eab\u753b\u5e03\u4e0a\u534f\u4f5c\u3002\u65e0\u8bba\u827a\u672f\u4e13\u4e1a\u77e5\u8bc6\u5982\u4f55\uff0c\u53c2\u4e0e\u8005\u90fd\u80fd\u8fdb\u884c\u540c\u6b65\u7684\u5171\u540c\u521b\u4f5c\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u91cd\u65b0\u5b9a\u4e49\u4eba\u5de5\u667a\u80fd\u4e0e\u4eba\u7c7b\u7684\u4ea4\u4e92\u4e3a\u4e00\u79cd\u5171\u521b\u548c\u76f8\u4e92\u589e\u5f3a\u7684\u8fc7\u7a0b\uff0c\u4e3a\u53ef\u63a5\u53d7\u6027\u5f3a\u7684\u751f\u6210\u5f0f\u521b\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.19320", "pdf": "https://arxiv.org/pdf/2508.19320", "abs": "https://arxiv.org/abs/2508.19320", "authors": ["Ming Chen", "Liyuan Cui", "Wenyuan Zhang", "Haoxian Zhang", "Yan Zhou", "Xiaohan Li", "Xiaoqiang Liu", "Pengfei Wan"], "title": "MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Technical Report. Project Page: https://chenmingthu.github.io/milm/", "summary": "Recently, interactive digital human video generation has attracted widespread attention and achieved remarkable progress. However, building such a practical system that can interact with diverse input signals in real time remains challenging to existing methods, which often struggle with high latency, heavy computational cost, and limited controllability. In this work, we introduce an autoregressive video generation framework that enables interactive multimodal control and low-latency extrapolation in a streaming manner. With minimal modifications to a standard large language model (LLM), our framework accepts multimodal condition encodings including audio, pose, and text, and outputs spatially and semantically coherent representations to guide the denoising process of a diffusion head. To support this, we construct a large-scale dialogue dataset of approximately 20,000 hours from multiple sources, providing rich conversational scenarios for training. We further introduce a deep compression autoencoder with up to 64$\\times$ reduction ratio, which effectively alleviates the long-horizon inference burden of the autoregressive model. Extensive experiments on duplex conversation, multilingual human synthesis, and interactive world model highlight the advantages of our approach in low latency, high efficiency, and fine-grained multimodal controllability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u56de\u5f52\u6a21\u578b\u548c\u6269\u6563\u5934\u7684\u4ea4\u4e92\u5f0f\u6570\u5b57\u4eba\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u652f\u6301\u591a\u6a21\u6001\u8f93\u5165\u63a7\u5236\u548c\u4f4e\u5ef6\u8fdf\u6d41\u5f0f\u751f\u6210\uff0c\u901a\u8fc7\u6df1\u5ea6\u538b\u7f29\u81ea\u7f16\u7801\u5668\u5b9e\u73b064\u500d\u538b\u7f29\u6bd4\u6765\u964d\u4f4e\u8ba1\u7b97\u8d1f\u62c5\u3002", "motivation": "\u73b0\u6709\u4ea4\u4e92\u5f0f\u6570\u5b57\u4eba\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u9ad8\u5ef6\u8fdf\u3001\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u6709\u9650\u53ef\u63a7\u6027\u7b49\u95ee\u9898\uff0c\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u5b9e\u65f6\u54cd\u5e94\u591a\u6837\u5316\u8f93\u5165\u4fe1\u53f7\u7684\u5b9e\u7528\u7cfb\u7edf\u3002", "method": "\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u6784\u5efa\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u63a5\u53d7\u97f3\u9891\u3001\u59ff\u6001\u548c\u6587\u672c\u7b49\u591a\u6a21\u6001\u6761\u4ef6\u7f16\u7801\uff0c\u8f93\u51fa\u7a7a\u95f4\u548c\u8bed\u4e49\u4e00\u81f4\u7684\u8868\u5f81\u6765\u6307\u5bfc\u6269\u6563\u5934\u7684\u53bb\u566a\u8fc7\u7a0b\u3002\u4f7f\u7528\u6df1\u5ea6\u538b\u7f29\u81ea\u7f16\u7801\u5668\u5b9e\u73b064\u500d\u538b\u7f29\u6bd4\u3002", "result": "\u5728\u53cc\u5de5\u5bf9\u8bdd\u3001\u591a\u8bed\u8a00\u4eba\u50cf\u5408\u6210\u548c\u4ea4\u4e92\u5f0f\u4e16\u754c\u6a21\u578b\u7b49\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u6548\u7387\u548c\u7ec6\u7c92\u5ea6\u591a\u6a21\u6001\u53ef\u63a7\u6027\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6700\u5c0f\u5316\u5bf9\u6807\u51c6LLM\u7684\u4fee\u6539\uff0c\u5b9e\u73b0\u4e86\u4ea4\u4e92\u5f0f\u591a\u6a21\u6001\u63a7\u5236\u548c\u4f4e\u5ef6\u8fdf\u5916\u63a8\uff0c\u6784\u5efa\u7684\u5927\u89c4\u6a21\u5bf9\u8bdd\u6570\u636e\u96c6(\u7ea620,000\u5c0f\u65f6)\u4e3a\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u5bf9\u8bdd\u573a\u666f\u3002"}}
{"id": "2508.19349", "pdf": "https://arxiv.org/pdf/2508.19349", "abs": "https://arxiv.org/abs/2508.19349", "authors": ["Mahdieh Behjat Khatooni", "Mohsen Soryani"], "title": "EffNetViTLoRA: An Efficient Hybrid Deep Learning Approach for Alzheimer's Disease Diagnosis", "categories": ["cs.CV"], "comment": null, "summary": "Alzheimer's disease (AD) is one of the most prevalent neurodegenerative disorders worldwide. As it progresses, it leads to the deterioration of cognitive functions. Since AD is irreversible, early diagnosis is crucial for managing its progression. Mild Cognitive Impairment (MCI) represents an intermediate stage between Cognitively Normal (CN) individuals and those with AD, and is considered a transitional phase from normal cognition to Alzheimer's disease. Diagnosing MCI is particularly challenging due to the subtle differences between adjacent diagnostic categories. In this study, we propose EffNetViTLoRA, a generalized end-to-end model for AD diagnosis using the whole Alzheimer's Disease Neuroimaging Initiative (ADNI) Magnetic Resonance Imaging (MRI) dataset. Our model integrates a Convolutional Neural Network (CNN) with a Vision Transformer (ViT) to capture both local and global features from MRI images. Unlike previous studies that rely on limited subsets of data, our approach is trained on the full T1-weighted MRI dataset from ADNI, resulting in a more robust and unbiased model. This comprehensive methodology enhances the model's clinical reliability. Furthermore, fine-tuning large pretrained models often yields suboptimal results when source and target dataset domains differ. To address this, we incorporate Low-Rank Adaptation (LoRA) to effectively adapt the pretrained ViT model to our target domain. This method enables efficient knowledge transfer and reduces the risk of overfitting. Our model achieves a classification accuracy of 92.52% and an F1-score of 92.76% across three diagnostic categories: AD, MCI, and CN for full ADNI dataset.", "AI": {"tldr": "EffNetViTLoRA\u6a21\u578b\u7ed3\u5408CNN\u548cViT\uff0c\u4f7f\u7528\u5b8c\u6574ADNI MRI\u6570\u636e\u96c6\u8fdb\u884c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\uff0c\u901a\u8fc7LoRA\u5fae\u8c03\u5b9e\u73b092.52%\u51c6\u786e\u7387", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u65e9\u671f\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0cMCI\u9636\u6bb5\u8bca\u65ad\u56f0\u96be\uff0c\u73b0\u6709\u7814\u7a76\u591a\u4f7f\u7528\u6709\u9650\u6570\u636e\u5b50\u96c6\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u548c\u5168\u9762\u7684\u8bca\u65ad\u65b9\u6cd5", "method": "\u96c6\u6210CNN\u548cVision Transformer\u6355\u6349MRI\u56fe\u50cf\u7684\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\uff0c\u4f7f\u7528\u5b8c\u6574ADNI T1\u52a0\u6743MRI\u6570\u636e\u96c6\uff0c\u91c7\u7528LoRA\u6280\u672f\u8fdb\u884c\u9ad8\u6548\u9886\u57df\u9002\u5e94", "result": "\u5728AD\u3001MCI\u548cCN\u4e09\u4e2a\u8bca\u65ad\u7c7b\u522b\u4e0a\u8fbe\u523092.52%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u548c92.76%\u7684F1\u5206\u6570", "conclusion": "\u63d0\u51fa\u7684EffNetViTLoRA\u6a21\u578b\u5728\u5b8c\u6574\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0cLoRA\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u9886\u57df\u5dee\u5f02\u95ee\u9898\uff0c\u5177\u6709\u4e34\u5e8a\u53ef\u9760\u6027"}}
{"id": "2508.19527", "pdf": "https://arxiv.org/pdf/2508.19527", "abs": "https://arxiv.org/abs/2508.19527", "authors": ["Zhiting Gao", "Dan Song", "Diqiong Jiang", "Chao Xue", "An-An Liu"], "title": "MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment", "categories": ["cs.CV"], "comment": "11 pages, 5 figures", "summary": "Motion generation is essential for animating virtual characters and embodied agents. While recent text-driven methods have made significant strides, they often struggle with achieving precise alignment between linguistic descriptions and motion semantics, as well as with the inefficiencies of slow, multi-step inference. To address these issues, we introduce TMR++ Aligned Preference Optimization (TAPO), an innovative framework that aligns subtle motion variations with textual modifiers and incorporates iterative adjustments to reinforce semantic grounding. To further enable real-time synthesis, we propose MotionFLUX, a high-speed generation framework based on deterministic rectified flow matching. Unlike traditional diffusion models, which require hundreds of denoising steps, MotionFLUX constructs optimal transport paths between noise distributions and motion spaces, facilitating real-time synthesis. The linearized probability paths reduce the need for multi-step sampling typical of sequential methods, significantly accelerating inference time without sacrificing motion quality. Experimental results demonstrate that, together, TAPO and MotionFLUX form a unified system that outperforms state-of-the-art approaches in both semantic consistency and motion quality, while also accelerating generation speed. The code and pretrained models will be released.", "AI": {"tldr": "TAPO\u548cMotionFLUX\u6846\u67b6\u89e3\u51b3\u4e86\u6587\u672c\u9a71\u52a8\u52a8\u4f5c\u751f\u6210\u4e2d\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u63a8\u7406\u6548\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u9ad8\u8d28\u91cf\u52a8\u4f5c\u5408\u6210", "motivation": "\u73b0\u6709\u6587\u672c\u9a71\u52a8\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\u5728\u8bed\u8a00\u63cf\u8ff0\u4e0e\u52a8\u4f5c\u8bed\u4e49\u7684\u7cbe\u786e\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u4e14\u591a\u6b65\u63a8\u7406\u6548\u7387\u4f4e\u4e0b", "method": "TAPO\u6846\u67b6\u901a\u8fc7\u504f\u597d\u4f18\u5316\u5bf9\u9f50\u7ec6\u5fae\u52a8\u4f5c\u53d8\u5316\u4e0e\u6587\u672c\u4fee\u9970\u7b26\uff0cMotionFLUX\u57fa\u4e8e\u786e\u5b9a\u6027\u6574\u6d41\u6d41\u5339\u914d\u6784\u5efa\u566a\u58f0\u5206\u5e03\u4e0e\u52a8\u4f5c\u7a7a\u95f4\u7684\u6700\u4f18\u4f20\u8f93\u8def\u5f84", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u7cfb\u7edf\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u52a8\u4f5c\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u52a0\u901f\u751f\u6210\u901f\u5ea6", "conclusion": "TAPO\u548cMotionFLUX\u5f62\u6210\u4e86\u4e00\u4e2a\u7edf\u4e00\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u52a8\u4f5c\u751f\u6210\u4e2d\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u5b9e\u65f6\u5408\u6210\u6311\u6218"}}
{"id": "2508.19575", "pdf": "https://arxiv.org/pdf/2508.19575", "abs": "https://arxiv.org/abs/2508.19575", "authors": ["Zhu Xu", "Zhaowen Wang", "Yuxin Peng", "Yang Liu"], "title": "Interact-Custom: Customized Human Object Interaction Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Compositional Customized Image Generation aims to customize multiple target concepts within generation content, which has gained attention for its wild application.Existing approaches mainly concentrate on the target entity's appearance preservation, while neglecting the fine-grained interaction control among target entities.To enable the model of such interaction control capability, we focus on human object interaction scenario and propose the task of Customized Human Object Interaction Image Generation(CHOI), which simultaneously requires identity preservation for target human object and the interaction semantic control between them.Two primary challenges exist for CHOI:(1)simultaneous identity preservation and interaction control demands require the model to decompose the human object into self-contained identity features and pose-oriented interaction features, while the current HOI image datasets fail to provide ideal samples for such feature-decomposed learning.(2)inappropriate spatial configuration between human and object may lead to the lack of desired interaction semantics.To tackle it, we first process a large-scale dataset, where each sample encompasses the same pair of human object involving different interactive poses.Then we design a two-stage model Interact-Custom, which firstly explicitly models the spatial configuration by generating a foreground mask depicting the interaction behavior, then under the guidance of this mask, we generate the target human object interacting while preserving their identities features.Furthermore, if the background image and the union location of where the target human object should appear are provided by users, Interact-Custom also provides the optional functionality to specify them, offering high content controllability. Extensive experiments on our tailored metrics for CHOI task demonstrate the effectiveness of our approach.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5b9a\u5236\u5316\u4eba\u673a\u4ea4\u4e92\u56fe\u50cf\u751f\u6210\u4efb\u52a1(CHOI)\uff0c\u901a\u8fc7Interact-Custom\u6a21\u578b\u89e3\u51b3\u540c\u65f6\u4fdd\u6301\u76ee\u6807\u8eab\u4efd\u7279\u5f81\u548c\u63a7\u5236\u4ea4\u4e92\u8bed\u4e49\u7684\u6311\u6218\uff0c\u5305\u62ec\u5927\u89c4\u6a21\u6570\u636e\u96c6\u6784\u5efa\u548c\u4e24\u9636\u6bb5\u751f\u6210\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u76ee\u6807\u5b9e\u4f53\u7684\u5916\u89c2\u4fdd\u6301\uff0c\u800c\u5ffd\u7565\u4e86\u76ee\u6807\u5b9e\u4f53\u4e4b\u95f4\u7684\u7ec6\u7c92\u5ea6\u4ea4\u4e92\u63a7\u5236\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u7814\u7a76\u8005\u4e13\u6ce8\u4e8e\u4eba\u673a\u4ea4\u4e92\u573a\u666f\uff0c\u63d0\u51fa\u4e86CHOI\u4efb\u52a1\uff0c\u8981\u6c42\u540c\u65f6\u4fdd\u6301\u76ee\u6807\u4eba\u673a\u8eab\u4efd\u5e76\u63a7\u5236\u5b83\u4eec\u4e4b\u95f4\u7684\u4ea4\u4e92\u8bed\u4e49\u3002", "method": "\u9996\u5148\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u6837\u672c\u5305\u542b\u76f8\u540c\u7684\u4eba\u673a\u5bf9\u6d89\u53ca\u4e0d\u540c\u7684\u4ea4\u4e92\u59ff\u52bf\u3002\u7136\u540e\u8bbe\u8ba1\u4e24\u9636\u6bb5\u6a21\u578bInteract-Custom\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u751f\u6210\u63cf\u8ff0\u4ea4\u4e92\u884c\u4e3a\u7684\u524d\u666f\u63a9\u6a21\u6765\u663e\u5f0f\u5efa\u6a21\u7a7a\u95f4\u914d\u7f6e\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5728\u8be5\u63a9\u6a21\u6307\u5bfc\u4e0b\u751f\u6210\u4fdd\u6301\u8eab\u4efd\u7279\u5f81\u7684\u76ee\u6807\u4eba\u673a\u4ea4\u4e92\u56fe\u50cf\u3002\u8fd8\u63d0\u4f9b\u53ef\u9009\u529f\u80fd\u5141\u8bb8\u7528\u6237\u6307\u5b9a\u80cc\u666f\u56fe\u50cf\u548c\u8054\u5408\u4f4d\u7f6e\u3002", "result": "\u5728\u9488\u5bf9CHOI\u4efb\u52a1\u5b9a\u5236\u7684\u6307\u6807\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u6a21\u578b\u80fd\u591f\u6210\u529f\u4fdd\u6301\u76ee\u6807\u8eab\u4efd\u7279\u5f81\u7684\u540c\u65f6\u5b9e\u73b0\u7cbe\u786e\u7684\u4ea4\u4e92\u8bed\u4e49\u63a7\u5236\u3002", "conclusion": "\u63d0\u51fa\u7684Interact-Custom\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u5b9a\u5236\u5316\u4eba\u673a\u4ea4\u4e92\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u8eab\u4efd\u4fdd\u6301\u548c\u4ea4\u4e92\u63a7\u5236\u53cc\u91cd\u6311\u6218\uff0c\u901a\u8fc7\u7a7a\u95f4\u914d\u7f6e\u5efa\u6a21\u548c\u4e24\u9636\u6bb5\u751f\u6210\u7b56\u7565\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u4ea4\u4e92\u56fe\u50cf\u751f\u6210\u3002"}}
{"id": "2508.19626", "pdf": "https://arxiv.org/pdf/2508.19626", "abs": "https://arxiv.org/abs/2508.19626", "authors": ["Jiajun Sun", "Zhen Yu", "Siyuan Yan", "Jason J. Ong", "Zongyuan Ge", "Lei Zhang"], "title": "Controllable Skin Synthesis via Lesion-Focused Vector Autoregression Model", "categories": ["cs.CV"], "comment": "11 pages, 4 figures", "summary": "Skin images from real-world clinical practice are often limited, resulting in a shortage of training data for deep-learning models. While many studies have explored skin image synthesis, existing methods often generate low-quality images and lack control over the lesion's location and type. To address these limitations, we present LF-VAR, a model leveraging quantified lesion measurement scores and lesion type labels to guide the clinically relevant and controllable synthesis of skin images. It enables controlled skin synthesis with specific lesion characteristics based on language prompts. We train a multiscale lesion-focused Vector Quantised Variational Auto-Encoder (VQVAE) to encode images into discrete latent representations for structured tokenization. Then, a Visual AutoRegressive (VAR) Transformer trained on tokenized representations facilitates image synthesis. Lesion measurement from the lesion region and types as conditional embeddings are integrated to enhance synthesis fidelity. Our method achieves the best overall FID score (average 0.74) among seven lesion types, improving upon the previous state-of-the-art (SOTA) by 6.3%. The study highlights our controllable skin synthesis model's effectiveness in generating high-fidelity, clinically relevant synthetic skin images. Our framework code is available at https://github.com/echosun1996/LF-VAR.", "AI": {"tldr": "LF-VAR\u662f\u4e00\u4e2a\u53ef\u63a7\u76ae\u80a4\u56fe\u50cf\u5408\u6210\u6a21\u578b\uff0c\u5229\u7528\u91cf\u5316\u75c5\u53d8\u6d4b\u91cf\u5206\u6570\u548c\u75c5\u53d8\u7c7b\u578b\u6807\u7b7e\uff0c\u901a\u8fc7\u8bed\u8a00\u63d0\u793a\u751f\u6210\u5177\u6709\u7279\u5b9a\u75c5\u53d8\u7279\u5f81\u7684\u9ad8\u8d28\u91cf\u76ae\u80a4\u56fe\u50cf\uff0c\u5728FID\u8bc4\u5206\u4e0a\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u63d0\u53476.3%\u3002", "motivation": "\u771f\u5b9e\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u76ae\u80a4\u56fe\u50cf\u6570\u636e\u6709\u9650\uff0c\u73b0\u6709\u5408\u6210\u65b9\u6cd5\u751f\u6210\u7684\u56fe\u50cf\u8d28\u91cf\u4f4e\u4e14\u65e0\u6cd5\u63a7\u5236\u75c5\u53d8\u4f4d\u7f6e\u548c\u7c7b\u578b\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u751f\u6210\u4e34\u5e8a\u76f8\u5173\u4e14\u53ef\u63a7\u7684\u9ad8\u8d28\u91cf\u76ae\u80a4\u56fe\u50cf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u591a\u5c3a\u5ea6\u75c5\u53d8\u805a\u7126\u7684VQVAE\u5c06\u56fe\u50cf\u7f16\u7801\u4e3a\u79bb\u6563\u6f5c\u5728\u8868\u793a\uff0c\u7136\u540e\u8bad\u7ec3\u89c6\u89c9\u81ea\u56de\u5f52\u53d8\u6362\u5668\u8fdb\u884c\u56fe\u50cf\u5408\u6210\uff0c\u6574\u5408\u75c5\u53d8\u6d4b\u91cf\u548c\u7c7b\u578b\u4f5c\u4e3a\u6761\u4ef6\u5d4c\u5165\u6765\u589e\u5f3a\u5408\u6210\u4fdd\u771f\u5ea6\u3002", "result": "\u5728\u4e03\u79cd\u75c5\u53d8\u7c7b\u578b\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73FID\u8bc4\u5206\uff08\u5e73\u57470.74\uff09\uff0c\u6bd4\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u5347\u4e866.3%\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u3001\u4e34\u5e8a\u76f8\u5173\u7684\u5408\u6210\u76ae\u80a4\u56fe\u50cf\u3002", "conclusion": "LF-VAR\u6a21\u578b\u5728\u53ef\u63a7\u76ae\u80a4\u56fe\u50cf\u5408\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u76ae\u80a4\u75c5\u5b66\u7814\u7a76\u548c\u4e34\u5e8a\u5b9e\u8df5\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19647", "pdf": "https://arxiv.org/pdf/2508.19647", "abs": "https://arxiv.org/abs/2508.19647", "authors": ["Bikash Kumar Badatya", "Vipul Baghel", "Ravi Hegde"], "title": "UTAL-GNN: Unsupervised Temporal Action Localization using Graph Neural Networks", "categories": ["cs.CV", "I.2.10; I.5.4"], "comment": "This paper has been accepted at the ICIP Satellite Workshop 2025", "summary": "Fine-grained action localization in untrimmed sports videos presents a significant challenge due to rapid and subtle motion transitions over short durations. Existing supervised and weakly supervised solutions often rely on extensive annotated datasets and high-capacity models, making them computationally intensive and less adaptable to real-world scenarios. In this work, we introduce a lightweight and unsupervised skeleton-based action localization pipeline that leverages spatio-temporal graph neural representations. Our approach pre-trains an Attention-based Spatio-Temporal Graph Convolutional Network (ASTGCN) on a pose-sequence denoising task with blockwise partitions, enabling it to learn intrinsic motion dynamics without any manual labeling. At inference, we define a novel Action Dynamics Metric (ADM), computed directly from low-dimensional ASTGCN embeddings, which detects motion boundaries by identifying inflection points in its curvature profile. Our method achieves a mean Average Precision (mAP) of 82.66% and average localization latency of 29.09 ms on the DSV Diving dataset, matching state-of-the-art supervised performance while maintaining computational efficiency. Furthermore, it generalizes robustly to unseen, in-the-wild diving footage without retraining, demonstrating its practical applicability for lightweight, real-time action analysis systems in embedded or dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65e0\u76d1\u7763\u7684\u57fa\u4e8e\u9aa8\u67b6\u7684\u52a8\u4f5c\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u4f7f\u7528\u65f6\u7a7a\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u52a8\u4f5c\u52a8\u6001\u5ea6\u91cf\u6765\u68c0\u6d4b\u8fd0\u52a8\u8fb9\u754c\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u8fbe\u5230\u6709\u76d1\u7763\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u672a\u4fee\u526a\u4f53\u80b2\u89c6\u9891\u4e2d\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u5b9a\u4f4d\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u590d\u6742\u6a21\u578b\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u9002\u5e94\u771f\u5b9e\u573a\u666f\u3002", "method": "\u4f7f\u7528\u6ce8\u610f\u529b\u65f6\u7a7a\u56fe\u5377\u79ef\u7f51\u7edc\uff08ASTGCN\uff09\u5728\u59ff\u6001\u5e8f\u5217\u53bb\u566a\u4efb\u52a1\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5b66\u4e60\u5185\u5728\u8fd0\u52a8\u52a8\u6001\uff1b\u5b9a\u4e49\u52a8\u4f5c\u52a8\u6001\u5ea6\u91cf\uff08ADM\uff09\u4ece\u4f4e\u7ef4\u5d4c\u5165\u4e2d\u68c0\u6d4b\u8fd0\u52a8\u8fb9\u754c\u70b9\u3002", "result": "\u5728DSV Diving\u6570\u636e\u96c6\u4e0a\u8fbe\u523082.66%\u7684\u5e73\u5747\u7cbe\u5ea6\u548c29.09\u6beb\u79d2\u7684\u5e73\u5747\u5b9a\u4f4d\u5ef6\u8fdf\uff0c\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7684\u6709\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\uff0c\u4e14\u5728\u672a\u89c1\u8fc7\u7684\u771f\u5b9e\u8df3\u6c34\u89c6\u9891\u4e0a\u8868\u73b0\u9c81\u68d2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5d4c\u5165\u5f0f\u6216\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8f7b\u91cf\u7ea7\u5b9e\u65f6\u52a8\u4f5c\u5206\u6790\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u4f18\u79c0\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2508.19649", "pdf": "https://arxiv.org/pdf/2508.19649", "abs": "https://arxiv.org/abs/2508.19649", "authors": ["Dongjin Kim", "Jaekyun Ko", "Muhammad Kashif Ali", "Tae Hyun Kim"], "title": "IDF: Iterative Dynamic Filtering Networks for Generalizable Image Denoising", "categories": ["cs.CV"], "comment": "ICCV 2025. Project Page: https://dongjinkim9.github.io/projects/idf/", "summary": "Image denoising is a fundamental challenge in computer vision, with applications in photography and medical imaging. While deep learning-based methods have shown remarkable success, their reliance on specific noise distributions limits generalization to unseen noise types and levels. Existing approaches attempt to address this with extensive training data and high computational resources but they still suffer from overfitting. To address these issues, we conduct image denoising by utilizing dynamically generated kernels via efficient operations. This approach helps prevent overfitting and improves resilience to unseen noise. Specifically, our method leverages a Feature Extraction Module for robust noise-invariant features, Global Statistics and Local Correlation Modules to capture comprehensive noise characteristics and structural correlations. The Kernel Prediction Module then employs these cues to produce pixel-wise varying kernels adapted to local structures, which are then applied iteratively for denoising. This ensures both efficiency and superior restoration quality. Despite being trained on single-level Gaussian noise, our compact model (~ 0.04 M) excels across diverse noise types and levels, demonstrating the promise of iterative dynamic filtering for practical image denoising.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u751f\u6210\u6838\u7684\u8fed\u4ee3\u56fe\u50cf\u53bb\u566a\u65b9\u6cd5\uff0c\u901a\u8fc7\u7279\u5f81\u63d0\u53d6\u548c\u6838\u9884\u6d4b\u6a21\u5757\u751f\u6210\u50cf\u7d20\u7ea7\u53d8\u5316\u7684\u81ea\u9002\u5e94\u6838\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u6027\u7684\u540c\u65f6\u63d0\u5347\u5bf9\u672a\u77e5\u566a\u58f0\u7c7b\u578b\u7684\u6cdb\u5316\u80fd\u529b", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u7279\u5b9a\u566a\u58f0\u5206\u5e03\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u4e14\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90", "method": "\u4f7f\u7528\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u83b7\u53d6\u566a\u58f0\u4e0d\u53d8\u7279\u5f81\uff0c\u5168\u5c40\u7edf\u8ba1\u548c\u5c40\u90e8\u76f8\u5173\u6a21\u5757\u6355\u83b7\u566a\u58f0\u7279\u6027\uff0c\u6838\u9884\u6d4b\u6a21\u5757\u751f\u6210\u50cf\u7d20\u7ea7\u53d8\u5316\u7684\u81ea\u9002\u5e94\u6838\u8fdb\u884c\u8fed\u4ee3\u53bb\u566a", "result": "\u4ec5\u4f7f\u7528\u5355\u7ea7\u9ad8\u65af\u566a\u58f0\u8bad\u7ec3\u7684\u7d27\u51d1\u6a21\u578b\uff08\u7ea60.04M\u53c2\u6570\uff09\u5728\u591a\u79cd\u566a\u58f0\u7c7b\u578b\u548c\u7ea7\u522b\u4e0a\u8868\u73b0\u4f18\u5f02", "conclusion": "\u8fed\u4ee3\u52a8\u6001\u6ee4\u6ce2\u65b9\u6cd5\u4e3a\u5b9e\u9645\u56fe\u50cf\u53bb\u566a\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6548\u7387"}}
{"id": "2508.19664", "pdf": "https://arxiv.org/pdf/2508.19664", "abs": "https://arxiv.org/abs/2508.19664", "authors": ["Weicheng Liao", "Zan Chen", "Jianyang Xie", "Yalin Zheng", "Yuhui Ma", "Yitian Zhao"], "title": "A Frequency-Aware Self-Supervised Learning for Ultra-Wide-Field Image Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Ultra-Wide-Field (UWF) retinal imaging has revolutionized retinal diagnostics by providing a comprehensive view of the retina. However, it often suffers from quality-degrading factors such as blurring and uneven illumination, which obscure fine details and mask pathological information. While numerous retinal image enhancement methods have been proposed for other fundus imageries, they often fail to address the unique requirements in UWF, particularly the need to preserve pathological details. In this paper, we propose a novel frequency-aware self-supervised learning method for UWF image enhancement. It incorporates frequency-decoupled image deblurring and Retinex-guided illumination compensation modules. An asymmetric channel integration operation is introduced in the former module, so as to combine global and local views by leveraging high- and low-frequency information, ensuring the preservation of fine and broader structural details. In addition, a color preservation unit is proposed in the latter Retinex-based module, to provide multi-scale spatial and frequency information, enabling accurate illumination estimation and correction. Experimental results demonstrate that the proposed work not only enhances visualization quality but also improves disease diagnosis performance by restoring and correcting fine local details and uneven intensity. To the best of our knowledge, this work is the first attempt for UWF image enhancement, offering a robust and clinically valuable tool for improving retinal disease management.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9891\u7387\u611f\u77e5\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u8d85\u5bbd\u89c6\u91ce\u89c6\u7f51\u819c\u56fe\u50cf\u589e\u5f3a\uff0c\u5305\u542b\u9891\u7387\u89e3\u8026\u53bb\u6a21\u7cca\u548cRetinex\u5f15\u5bfc\u7684\u7167\u660e\u8865\u507f\u6a21\u5757\uff0c\u6709\u6548\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548c\u75be\u75c5\u8bca\u65ad\u6027\u80fd\u3002", "motivation": "\u8d85\u5bbd\u89c6\u91ce\u89c6\u7f51\u819c\u6210\u50cf\u867d\u7136\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u89c6\u7f51\u819c\u89c6\u56fe\uff0c\u4f46\u7ecf\u5e38\u53d7\u5230\u6a21\u7cca\u548c\u5149\u7167\u4e0d\u5747\u7b49\u8d28\u91cf\u9000\u5316\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u8fd9\u4e9b\u56e0\u7d20\u4f1a\u63a9\u76d6\u7cbe\u7ec6\u7ec6\u8282\u548c\u75c5\u7406\u4fe1\u606f\u3002\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3UWF\u56fe\u50cf\u7684\u7279\u6b8a\u9700\u6c42\uff0c\u7279\u522b\u662f\u4fdd\u7559\u75c5\u7406\u7ec6\u8282\u7684\u8981\u6c42\u3002", "method": "\u91c7\u7528\u9891\u7387\u611f\u77e5\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1\uff09\u9891\u7387\u89e3\u8026\u56fe\u50cf\u53bb\u6a21\u7cca\u6a21\u5757\uff0c\u901a\u8fc7\u975e\u5bf9\u79f0\u901a\u9053\u6574\u5408\u64cd\u4f5c\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u89c6\u56fe\uff1b2\uff09Retinex\u5f15\u5bfc\u7684\u7167\u660e\u8865\u507f\u6a21\u5757\uff0c\u5305\u542b\u989c\u8272\u4fdd\u7559\u5355\u5143\uff0c\u63d0\u4f9b\u591a\u5c3a\u5ea6\u7a7a\u95f4\u548c\u9891\u7387\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u53ef\u89c6\u5316\u8d28\u91cf\uff0c\u8fd8\u901a\u8fc7\u6062\u590d\u548c\u6821\u6b63\u7cbe\u7ec6\u5c40\u90e8\u7ec6\u8282\u548c\u4e0d\u5747\u5300\u5f3a\u5ea6\u6765\u6539\u5584\u75be\u75c5\u8bca\u65ad\u6027\u80fd\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u8d85\u5bbd\u89c6\u91ce\u56fe\u50cf\u589e\u5f3a\u7684\u5c1d\u8bd5\uff0c\u4e3a\u6539\u5584\u89c6\u7f51\u819c\u75be\u75c5\u7ba1\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u4e14\u5177\u6709\u4e34\u5e8a\u4ef7\u503c\u7684\u5de5\u5177\u3002"}}
{"id": "2508.19699", "pdf": "https://arxiv.org/pdf/2508.19699", "abs": "https://arxiv.org/abs/2508.19699", "authors": ["Yupeng Zhang", "Dezhi Zheng", "Ping Lu", "Han Zhang", "Lei Wang", "Liping xiang", "Cheng Luo", "Kaijun Deng", "Xiaowen Fu", "Linlin Shen", "Jinbao Wang"], "title": "LabelGS: Label-Aware 3D Gaussian Splatting for 3D Scene Segmentation", "categories": ["cs.CV"], "comment": "PRCV 2025", "summary": "3D Gaussian Splatting (3DGS) has emerged as a novel explicit representation for 3D scenes, offering both high-fidelity reconstruction and efficient rendering. However, 3DGS lacks 3D segmentation ability, which limits its applicability in tasks that require scene understanding. The identification and isolating of specific object components is crucial. To address this limitation, we propose Label-aware 3D Gaussian Splatting (LabelGS), a method that augments the Gaussian representation with object label.LabelGS introduces cross-view consistent semantic masks for 3D Gaussians and employs a novel Occlusion Analysis Model to avoid overfitting occlusion during optimization, Main Gaussian Labeling model to lift 2D semantic prior to 3D Gaussian and Gaussian Projection Filter to avoid Gaussian label conflict. Our approach achieves effective decoupling of Gaussian representations and refines the 3DGS optimization process through a random region sampling strategy, significantly improving efficiency. Extensive experiments demonstrate that LabelGS outperforms previous state-of-the-art methods, including Feature-3DGS, in the 3D scene segmentation task. Notably, LabelGS achieves a remarkable 22X speedup in training compared to Feature-3DGS, at a resolution of 1440X1080. Our code will be at https://github.com/garrisonz/LabelGS.", "AI": {"tldr": "LabelGS\u662f\u4e00\u79cd\u589e\u5f3a3D\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5bf9\u8c61\u6807\u7b7e\u5b9e\u73b03D\u573a\u666f\u5206\u5272\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u6e32\u67d3\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u901f\u5ea6\u548c\u5206\u5272\u6027\u80fd", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85(3DGS)\u867d\u7136\u80fd\u591f\u5b9e\u73b0\u9ad8\u4fdd\u771f\u91cd\u5efa\u548c\u9ad8\u6548\u6e32\u67d3\uff0c\u4f46\u7f3a\u4e4f3D\u5206\u5272\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5176\u5728\u9700\u8981\u573a\u666f\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e3a\u9ad8\u65af\u8868\u793a\u6dfb\u52a0\u5bf9\u8c61\u6807\u7b7e\u80fd\u529b", "method": "\u63d0\u51faLabelGS\u65b9\u6cd5\uff0c\u5f15\u5165\u8de8\u89c6\u56fe\u4e00\u81f4\u76843D\u9ad8\u65af\u8bed\u4e49\u63a9\u7801\uff0c\u91c7\u7528\u906e\u6321\u5206\u6790\u6a21\u578b\u907f\u514d\u4f18\u5316\u8fc7\u7a0b\u4e2d\u7684\u8fc7\u62df\u5408\uff0c\u4f7f\u7528\u4e3b\u9ad8\u65af\u6807\u8bb0\u6a21\u578b\u5c062D\u8bed\u4e49\u5148\u9a8c\u63d0\u5347\u52303D\u9ad8\u65af\uff0c\u5e76\u901a\u8fc7\u9ad8\u65af\u6295\u5f71\u6ee4\u6ce2\u5668\u907f\u514d\u6807\u7b7e\u51b2\u7a81", "result": "LabelGS\u57283D\u573a\u666f\u5206\u5272\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5305\u62ecFeature-3DGS\u5728\u5185\u7684\u5148\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u57281440\u00d71080\u5206\u8fa8\u7387\u4e0b\u5b9e\u73b0\u4e8622\u500d\u7684\u8bad\u7ec3\u52a0\u901f", "conclusion": "LabelGS\u6210\u529f\u89e3\u51b3\u4e863DGS\u7f3a\u4e4f\u5206\u5272\u80fd\u529b\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u5bf9\u8c61\u6807\u7b7e\u548c\u4f18\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u51c6\u786e\u76843D\u573a\u666f\u5206\u5272\uff0c\u4e3a3D\u573a\u666f\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.19754", "pdf": "https://arxiv.org/pdf/2508.19754", "abs": "https://arxiv.org/abs/2508.19754", "authors": ["Yue Wu", "Yufan Wu", "Wen Li", "Yuxi Lu", "Kairui Feng", "Xuanhong Chen"], "title": "FastAvatar: Towards Unified Fast High-Fidelity 3D Avatar Reconstruction with Large Gaussian Reconstruction Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Despite significant progress in 3D avatar reconstruction, it still faces challenges such as high time complexity, sensitivity to data quality, and low data utilization. We propose FastAvatar, a feedforward 3D avatar framework capable of flexibly leveraging diverse daily recordings (e.g., a single image, multi-view observations, or monocular video) to reconstruct a high-quality 3D Gaussian Splatting (3DGS) model within seconds, using only a single unified model. FastAvatar's core is a Large Gaussian Reconstruction Transformer featuring three key designs: First, a variant VGGT-style transformer architecture aggregating multi-frame cues while injecting initial 3D prompt to predict an aggregatable canonical 3DGS representation; Second, multi-granular guidance encoding (camera pose, FLAME expression, head pose) mitigating animation-induced misalignment for variable-length inputs; Third, incremental Gaussian aggregation via landmark tracking and sliced fusion losses. Integrating these features, FastAvatar enables incremental reconstruction, i.e., improving quality with more observations, unlike prior work wasting input data. This yields a quality-speed-tunable paradigm for highly usable avatar modeling. Extensive experiments show that FastAvatar has higher quality and highly competitive speed compared to existing methods.", "AI": {"tldr": "FastAvatar\u662f\u4e00\u4e2a\u5feb\u901f3D\u5934\u50cf\u91cd\u5efa\u6846\u67b6\uff0c\u4f7f\u7528\u5355\u4e00\u7edf\u4e00\u6a21\u578b\u5728\u51e0\u79d2\u5185\u4ece\u5355\u5f20\u56fe\u50cf\u3001\u591a\u89c6\u89d2\u89c2\u6d4b\u6216\u5355\u76ee\u89c6\u9891\u4e2d\u91cd\u5efa\u9ad8\u8d28\u91cf\u76843D\u9ad8\u65af\u6e85\u5c04\u6a21\u578b\u3002", "motivation": "\u5f53\u524d3D\u5934\u50cf\u91cd\u5efa\u9762\u4e34\u65f6\u95f4\u590d\u6742\u5ea6\u8fc7\u9ad8\u3001\u5bf9\u6570\u636e\u8d28\u91cf\u654f\u611f\u4ee5\u53ca\u6570\u636e\u5229\u7528\u7387\u4f4e\u7b49\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7075\u6d3b\u5229\u7528\u591a\u6837\u5316\u65e5\u5e38\u8bb0\u5f55\u6570\u636e\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5927\u578b\u9ad8\u65af\u91cd\u5efaTransformer\u67b6\u6784\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a\u53d8\u4f53VGGT-style transformer\u805a\u5408\u591a\u5e27\u7ebf\u7d22\u5e76\u6ce8\u5165\u521d\u59cb3D\u63d0\u793a\uff1b\u591a\u7c92\u5ea6\u5f15\u5bfc\u7f16\u7801\u7f13\u89e3\u52a8\u753b\u5f15\u8d77\u7684\u9519\u4f4d\uff1b\u901a\u8fc7\u5730\u6807\u8ddf\u8e2a\u548c\u5207\u7247\u878d\u5408\u635f\u5931\u8fdb\u884c\u589e\u91cf\u9ad8\u65af\u805a\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFastAvatar\u5728\u8d28\u91cf\u548c\u901f\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u589e\u91cf\u91cd\u5efa\uff0c\u5373\u968f\u7740\u89c2\u6d4b\u6570\u636e\u589e\u52a0\u800c\u63d0\u5347\u8d28\u91cf\u3002", "conclusion": "FastAvatar\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8d28\u91cf-\u901f\u5ea6\u53ef\u8c03\u7684\u9ad8\u5ea6\u53ef\u7528\u5934\u50cf\u5efa\u6a21\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u5feb\u901f3D\u5934\u50cf\u91cd\u5efa\u7684\u7a81\u7834\u3002"}}
{"id": "2508.19786", "pdf": "https://arxiv.org/pdf/2508.19786", "abs": "https://arxiv.org/abs/2508.19786", "authors": ["Han Jiao", "Jiakai Sun", "Yexing Xu", "Lei Zhao", "Wei Xing", "Huaizhong Lin"], "title": "MAPo : Motion-Aware Partitioning of Deformable 3D Gaussian Splatting for High-Fidelity Dynamic Scene Reconstruction", "categories": ["cs.CV"], "comment": "8 pages, 9 figures, Anonymous AAAI Submission", "summary": "3D Gaussian Splatting, known for enabling high-quality static scene reconstruction with fast rendering, is increasingly being applied to dynamic scene reconstruction. A common strategy involves learning a deformation field to model the temporal changes of a canonical set of 3D Gaussians. However, these deformation-based methods often produce blurred renderings and lose fine motion details in highly dynamic regions due to the inherent limitations of a single, unified model in representing diverse motion patterns. To address these challenges, we introduce Motion-Aware Partitioning of Deformable 3D Gaussian Splatting (MAPo), a novel framework for high-fidelity dynamic scene reconstruction. Its core is a dynamic score-based partitioning strategy that distinguishes between high- and low-dynamic 3D Gaussians. For high-dynamic 3D Gaussians, we recursively partition them temporally and duplicate their deformation networks for each new temporal segment, enabling specialized modeling to capture intricate motion details. Concurrently, low-dynamic 3DGs are treated as static to reduce computational costs. However, this temporal partitioning strategy for high-dynamic 3DGs can introduce visual discontinuities across frames at the partition boundaries. To address this, we introduce a cross-frame consistency loss, which not only ensures visual continuity but also further enhances rendering quality. Extensive experiments demonstrate that MAPo achieves superior rendering quality compared to baselines while maintaining comparable computational costs, particularly in regions with complex or rapid motions.", "AI": {"tldr": "MAPo\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u8bc4\u5206\u5206\u533a\u7b56\u7565\uff0c\u5c063D\u9ad8\u65af\u5206\u4e3a\u9ad8\u52a8\u6001\u548c\u4f4e\u52a8\u6001\u533a\u57df\uff0c\u5bf9\u9ad8\u52a8\u6001\u533a\u57df\u8fdb\u884c\u65f6\u95f4\u5206\u533a\u548c\u7f51\u7edc\u590d\u5236\u4ee5\u6355\u6349\u7cbe\u7ec6\u8fd0\u52a8\u7ec6\u8282\uff0c\u540c\u65f6\u4f7f\u7528\u8de8\u5e27\u4e00\u81f4\u6027\u635f\u5931\u786e\u4fdd\u89c6\u89c9\u8fde\u7eed\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u53d8\u5f62\u7684\u52a8\u6001\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u52a8\u6001\u533a\u57df\u65f6\u5f80\u5f80\u4ea7\u751f\u6a21\u7cca\u6e32\u67d3\u5e76\u4e22\u5931\u7cbe\u7ec6\u8fd0\u52a8\u7ec6\u8282\uff0c\u56e0\u4e3a\u5355\u4e00\u7edf\u4e00\u6a21\u578b\u96be\u4ee5\u8868\u793a\u591a\u6837\u5316\u7684\u8fd0\u52a8\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u8bc4\u5206\u5206\u533a\u7b56\u7565\uff0c\u533a\u5206\u9ad8\u52a8\u6001\u548c\u4f4e\u52a8\u60013D\u9ad8\u65af\uff1b\u5bf9\u9ad8\u52a8\u6001\u9ad8\u65af\u8fdb\u884c\u65f6\u95f4\u9012\u5f52\u5206\u533a\u5e76\u4e3a\u6bcf\u4e2a\u65f6\u95f4\u6bb5\u590d\u5236\u53d8\u5f62\u7f51\u7edc\uff1b\u5f15\u5165\u8de8\u5e27\u4e00\u81f4\u6027\u635f\u5931\u89e3\u51b3\u5206\u533a\u8fb9\u754c\u89c6\u89c9\u4e0d\u8fde\u7eed\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMAPo\u5728\u4fdd\u6301\u53ef\u6bd4\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u4f18\u8d8a\u7684\u6e32\u67d3\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u590d\u6742\u6216\u5feb\u901f\u8fd0\u52a8\u7684\u533a\u57df\u3002", "conclusion": "MAPo\u6846\u67b6\u901a\u8fc7\u8fd0\u52a8\u611f\u77e5\u5206\u533a\u548c\u8de8\u5e27\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u60013D\u9ad8\u65af\u6cfc\u6e85\u4e2d\u7684\u8fd0\u52a8\u7ec6\u8282\u4e22\u5931\u548c\u89c6\u89c9\u4e0d\u8fde\u7eed\u95ee\u9898\uff0c\u4e3a\u9ad8\u4fdd\u771f\u52a8\u6001\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19789", "pdf": "https://arxiv.org/pdf/2508.19789", "abs": "https://arxiv.org/abs/2508.19789", "authors": ["Xiuchao Wu", "Pengfei Zhu", "Jiangjing Lyu", "Xinguo Liu", "Jie Guo", "Yanwen Guo", "Weiwei Xu", "Chengfei Lyu"], "title": "StableIntrinsic: Detail-preserving One-step Diffusion Model for Multi-view Material Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Recovering material information from images has been extensively studied in computer graphics and vision. Recent works in material estimation leverage diffusion model showing promising results. However, these diffusion-based methods adopt a multi-step denoising strategy, which is time-consuming for each estimation. Such stochastic inference also conflicts with the deterministic material estimation task, leading to a high variance estimated results. In this paper, we introduce StableIntrinsic, a one-step diffusion model for multi-view material estimation that can produce high-quality material parameters with low variance. To address the overly-smoothing problem in one-step diffusion, StableIntrinsic applies losses in pixel space, with each loss designed based on the properties of the material. Additionally, StableIntrinsic introduces a Detail Injection Network (DIN) to eliminate the detail loss caused by VAE encoding, while further enhancing the sharpness of material prediction results. The experimental results indicate that our method surpasses the current state-of-the-art techniques by achieving a $9.9\\%$ improvement in the Peak Signal-to-Noise Ratio (PSNR) of albedo, and by reducing the Mean Square Error (MSE) for metallic and roughness by $44.4\\%$ and $60.0\\%$, respectively.", "AI": {"tldr": "StableIntrinsic\u662f\u4e00\u4e2a\u5355\u6b65\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u591a\u89c6\u89d2\u6750\u8d28\u4f30\u8ba1\uff0c\u80fd\u591f\u5feb\u901f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u4f4e\u65b9\u5dee\u7684\u6750\u8d28\u53c2\u6570\uff0c\u5728\u591a\u9879\u6307\u6807\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6750\u8d28\u4f30\u8ba1\u65b9\u6cd5\u91c7\u7528\u591a\u6b65\u53bb\u566a\u7b56\u7565\uff0c\u8017\u65f6\u4e14\u7ed3\u679c\u65b9\u5dee\u5927\uff0c\u4e0e\u786e\u5b9a\u6027\u6750\u8d28\u4f30\u8ba1\u4efb\u52a1\u5b58\u5728\u51b2\u7a81\u3002", "method": "\u63d0\u51fa\u5355\u6b65\u6269\u6563\u6a21\u578bStableIntrinsic\uff0c\u5728\u50cf\u7d20\u7a7a\u95f4\u8bbe\u8ba1\u57fa\u4e8e\u6750\u8d28\u7279\u6027\u7684\u635f\u5931\u51fd\u6570\uff0c\u5e76\u5f15\u5165\u7ec6\u8282\u6ce8\u5165\u7f51\u7edc(DIN)\u6765\u6d88\u9664VAE\u7f16\u7801\u9020\u6210\u7684\u7ec6\u8282\u635f\u5931\u3002", "result": "\u5728PSNR\u6307\u6807\u4e0a\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u63d0\u53479.9%\uff0c\u91d1\u5c5e\u548c\u7c97\u7cd9\u5ea6\u7684MSE\u5206\u522b\u964d\u4f4e44.4%\u548c60.0%\u3002", "conclusion": "StableIntrinsic\u901a\u8fc7\u5355\u6b65\u6269\u6563\u548c\u7ec6\u8282\u589e\u5f3a\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u9ad8\u8d28\u91cf\u7684\u6750\u8d28\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u591a\u6b65\u6269\u6563\u65b9\u6cd5\u7684\u6548\u7387\u548c\u65b9\u5dee\u95ee\u9898\u3002"}}
{"id": "2508.19791", "pdf": "https://arxiv.org/pdf/2508.19791", "abs": "https://arxiv.org/abs/2508.19791", "authors": ["Shay Shomer Chai", "Wenxuan Peng", "Bharath Hariharan", "Hadar Averbuch-Elor"], "title": "Not Every Gift Comes in Gold Paper or with a Red Ribbon: Exploring Color Perception in Text-to-Image Models", "categories": ["cs.CV"], "comment": "Project webpage: https://tau-vailab.github.io/color-edit/", "summary": "Text-to-image generation has recently seen remarkable success, granting users with the ability to create high-quality images through the use of text. However, contemporary methods face challenges in capturing the precise semantics conveyed by complex multi-object prompts. Consequently, many works have sought to mitigate such semantic misalignments, typically via inference-time schemes that modify the attention layers of the denoising networks. However, prior work has mostly utilized coarse metrics, such as the cosine similarity between text and image CLIP embeddings, or human evaluations, which are challenging to conduct on a larger-scale. In this work, we perform a case study on colors -- a fundamental attribute commonly associated with objects in text prompts, which offer a rich test bed for rigorous evaluation. Our analysis reveals that pretrained models struggle to generate images that faithfully reflect multiple color attributes-far more so than with single-color prompts-and that neither inference-time techniques nor existing editing methods reliably resolve these semantic misalignments. Accordingly, we introduce a dedicated image editing technique, mitigating the issue of multi-object semantic alignment for prompts containing multiple colors. We demonstrate that our approach significantly boosts performance over a wide range of metrics, considering images generated by various text-to-image diffusion-based techniques.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u591a\u5bf9\u8c61\u989c\u8272\u5c5e\u6027\u7684\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u95e8\u7684\u56fe\u50cf\u7f16\u8f91\u6280\u672f\u6765\u6539\u5584\u591a\u989c\u8272\u63d0\u793a\u7684\u8bed\u4e49\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u591a\u5bf9\u8c61\u63d0\u793a\u65f6\u96be\u4ee5\u51c6\u786e\u6355\u6349\u7cbe\u786e\u8bed\u4e49\uff0c\u7279\u522b\u662f\u5728\u989c\u8272\u5c5e\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u9700\u8981\u4e13\u95e8\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5206\u6790\u989c\u8272\u5c5e\u6027\u5bf9\u9f50\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u4e13\u95e8\u7684\u56fe\u50cf\u7f16\u8f91\u6280\u672f\u6765\u7f13\u89e3\u591a\u5bf9\u8c61\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5305\u542b\u591a\u4e2a\u989c\u8272\u7684\u63d0\u793a\u4e2d\u3002", "result": "\u7814\u7a76\u8868\u660e\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u5904\u7406\u591a\u989c\u8272\u5c5e\u6027\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u63d0\u51fa\u7684\u7f16\u8f91\u6280\u672f\u5728\u5404\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u9488\u5bf9\u591a\u989c\u8272\u63d0\u793a\u7684\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\u9700\u8981\u4e13\u95e8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u51fa\u7684\u56fe\u50cf\u7f16\u8f91\u6280\u672f\u80fd\u6709\u6548\u6539\u5584\u591a\u5bf9\u8c61\u8bed\u4e49\u5bf9\u9f50\uff0c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u8d28\u91cf\u63d0\u5347\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2508.19798", "pdf": "https://arxiv.org/pdf/2508.19798", "abs": "https://arxiv.org/abs/2508.19798", "authors": ["Muhammad Ali", "Omar Ali AlSuwaidi"], "title": "FusionSort: Enhanced Cluttered Waste Segmentation with Advanced Decoding and Comprehensive Modality Optimization", "categories": ["cs.CV"], "comment": null, "summary": "In the realm of waste management, automating the sorting process for non-biodegradable materials presents considerable challenges due to the complexity and variability of waste streams. To address these challenges, we introduce an enhanced neural architecture that builds upon an existing Encoder-Decoder structure to improve the accuracy and efficiency of waste sorting systems. Our model integrates several key innovations: a Comprehensive Attention Block within the decoder, which refines feature representations by combining convolutional and upsampling operations. In parallel, we utilize attention through the Mamba architecture, providing an additional performance boost. We also introduce a Data Fusion Block that fuses images with more than three channels. To achieve this, we apply PCA transformation to reduce the dimensionality while retaining the maximum variance and essential information across three dimensions, which are then used for further processing. We evaluated the model on RGB, hyperspectral, multispectral, and a combination of RGB and hyperspectral data. The results demonstrate that our approach outperforms existing methods by a significant margin.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.19808", "pdf": "https://arxiv.org/pdf/2508.19808", "abs": "https://arxiv.org/abs/2508.19808", "authors": ["Kaixuan Lu", "Mehmet Onurcan Kaya", "Dim P. Papadopoulos"], "title": "AutoQ-VIS: Improving Unsupervised Video Instance Segmentation via Automatic Quality Assessment", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025 Workshop LIMIT", "summary": "Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 $\\text{AP}_{50}$ on YouTubeVIS-2019 val set, surpassing the previous state-of-the-art VideoCutLER by 4.4$\\%$, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. The source code of our method is available at https://github.com/wcbup/AutoQ-VIS.", "AI": {"tldr": "AutoQ-VIS\u662f\u4e00\u4e2a\u65e0\u76d1\u7763\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u8d28\u91cf\u5f15\u5bfc\u7684\u81ea\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e8652.6 AP50\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e4b\u524d\u7684\u6700\u4f18\u65b9\u6cd54.4%\u3002", "motivation": "\u89c6\u9891\u5b9e\u4f8b\u5206\u5272(VIS)\u9700\u8981\u50cf\u7d20\u7ea7\u63a9\u7801\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u6807\u6ce8\uff0c\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709\u65e0\u76d1\u7763\u65b9\u6cd5\u4f9d\u8d56\u5408\u6210\u6570\u636e\uff0c\u4f46\u5b58\u5728\u5408\u6210\u5230\u771f\u5b9e\u57df\u7684\u5dee\u8ddd\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u8d28\u91cf\u5f15\u5bfc\u7684\u81ea\u8bad\u7ec3\u6846\u67b6\uff0c\u5efa\u7acb\u4f2a\u6807\u7b7e\u751f\u6210\u548c\u81ea\u52a8\u8d28\u91cf\u8bc4\u4f30\u7684\u95ed\u73af\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4ece\u5408\u6210\u89c6\u9891\u5230\u771f\u5b9e\u89c6\u9891\u7684\u6e10\u8fdb\u5f0f\u9002\u5e94\u3002", "result": "\u5728YouTubeVIS-2019\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u523052.6 AP50\uff0c\u6bd4\u4e4b\u524d\u7684state-of-the-art VideoCutLER\u63d0\u53474.4%\uff0c\u4e14\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u3002", "conclusion": "\u8bc1\u660e\u4e86\u8d28\u91cf\u611f\u77e5\u81ea\u8bad\u7ec3\u5728\u65e0\u76d1\u7763\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u6210\u529f\u5f25\u5408\u4e86\u5408\u6210\u5230\u771f\u5b9e\u57df\u7684\u5dee\u8ddd\u3002"}}
{"id": "2508.19852", "pdf": "https://arxiv.org/pdf/2508.19852", "abs": "https://arxiv.org/abs/2508.19852", "authors": ["Binjie Zhang", "Mike Zheng Shou"], "title": "Ego-centric Predictive Model Conditioned on Hand Trajectories", "categories": ["cs.CV"], "comment": "Code: github.com/binjiezhang/Ego-PM (branch: main)", "summary": "In egocentric scenarios, anticipating both the next action and its visual outcome is essential for understanding human-object interactions and for enabling robotic planning. However, existing paradigms fall short of jointly modeling these aspects. Vision-Language-Action (VLA) models focus on action prediction but lack explicit modeling of how actions influence the visual scene, while video prediction models generate future frames without conditioning on specific actions, often resulting in implausible or contextually inconsistent outcomes. To bridge this gap, we propose a unified two-stage predictive framework that jointly models action and visual future in egocentric scenarios, conditioned on hand trajectories. In the first stage, we perform consecutive state modeling to process heterogeneous inputs (visual observations, language, and action history) and explicitly predict future hand trajectories. In the second stage, we introduce causal cross-attention to fuse multi-modal cues, leveraging inferred action signals to guide an image-based Latent Diffusion Model (LDM) for frame-by-frame future video generation. Our approach is the first unified model designed to handle both egocentric human activity understanding and robotic manipulation tasks, providing explicit predictions of both upcoming actions and their visual consequences. Extensive experiments on Ego4D, BridgeData, and RLBench demonstrate that our method outperforms state-of-the-art baselines in both action prediction and future video synthesis.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u7684\u4e24\u9636\u6bb5\u9884\u6d4b\u6846\u67b6\uff0c\u8054\u5408\u5efa\u6a21\u81ea\u6211\u4e2d\u5fc3\u573a\u666f\u4e2d\u7684\u52a8\u4f5c\u548c\u89c6\u89c9\u672a\u6765\uff0c\u901a\u8fc7\u624b\u90e8\u8f68\u8ff9\u6761\u4ef6\u5316\uff0c\u5b9e\u73b0\u52a8\u4f5c\u9884\u6d4b\u548c\u672a\u6765\u89c6\u9891\u751f\u6210\u7684\u7edf\u4e00\u5904\u7406\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u5efa\u6a21\u52a8\u4f5c\u9884\u6d4b\u548c\u89c6\u89c9\u7ed3\u679c\u3002VLA\u6a21\u578b\u53ea\u5173\u6ce8\u52a8\u4f5c\u9884\u6d4b\u800c\u7f3a\u4e4f\u5bf9\u89c6\u89c9\u573a\u666f\u5f71\u54cd\u7684\u663e\u5f0f\u5efa\u6a21\uff0c\u89c6\u9891\u9884\u6d4b\u6a21\u578b\u5219\u751f\u6210\u4e0d\u4f9d\u8d56\u7279\u5b9a\u52a8\u4f5c\u7684\u672a\u6765\u5e27\uff0c\u5e38\u5bfc\u81f4\u4e0d\u5408\u7406\u7ed3\u679c\u3002\u9700\u8981\u7edf\u4e00\u6846\u67b6\u6765\u540c\u65f6\u5904\u7406\u8fd9\u4e24\u4e2a\u65b9\u9762\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u8fde\u7eed\u72b6\u6001\u5efa\u6a21\u5904\u7406\u591a\u6a21\u6001\u8f93\u5165\u5e76\u9884\u6d4b\u672a\u6765\u624b\u90e8\u8f68\u8ff9\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5f15\u5165\u56e0\u679c\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u591a\u6a21\u6001\u7ebf\u7d22\uff0c\u5229\u7528\u63a8\u65ad\u7684\u52a8\u4f5c\u4fe1\u53f7\u6307\u5bfc\u57fa\u4e8e\u56fe\u50cf\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u8fdb\u884c\u9010\u5e27\u672a\u6765\u89c6\u9891\u751f\u6210\u3002", "result": "\u5728Ego4D\u3001BridgeData\u548cRLBench\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u52a8\u4f5c\u9884\u6d4b\u548c\u672a\u6765\u89c6\u9891\u5408\u6210\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u7edf\u4e00\u5904\u7406\u81ea\u6211\u4e2d\u5fc3\u4eba\u7c7b\u6d3b\u52a8\u7406\u89e3\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u6a21\u578b\uff0c\u80fd\u591f\u663e\u5f0f\u9884\u6d4b\u5373\u5c06\u53d1\u751f\u7684\u52a8\u4f5c\u53ca\u5176\u89c6\u89c9\u540e\u679c\u3002"}}
{"id": "2508.19895", "pdf": "https://arxiv.org/pdf/2508.19895", "abs": "https://arxiv.org/abs/2508.19895", "authors": ["Ziyun Qian", "Runyu Xiao", "Shuyuan Tu", "Wei Xue", "Dingkang Yang", "Mingcheng Li", "Dongliang Kou", "Minghao Han", "Zizhi Chen", "Lihua Zhang"], "title": "PersonaAnimator: Personalized Motion Transfer from Unconstrained Videos", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in motion generation show remarkable progress. However, several limitations remain: (1) Existing pose-guided character motion transfer methods merely replicate motion without learning its style characteristics, resulting in inexpressive characters. (2) Motion style transfer methods rely heavily on motion capture data, which is difficult to obtain. (3) Generated motions sometimes violate physical laws. To address these challenges, this paper pioneers a new task: Video-to-Video Motion Personalization. We propose a novel framework, PersonaAnimator, which learns personalized motion patterns directly from unconstrained videos. This enables personalized motion transfer. To support this task, we introduce PersonaVid, the first video-based personalized motion dataset. It contains 20 motion content categories and 120 motion style categories. We further propose a Physics-aware Motion Style Regularization mechanism to enforce physical plausibility in the generated motions. Extensive experiments show that PersonaAnimator outperforms state-of-the-art motion transfer methods and sets a new benchmark for the Video-to-Video Motion Personalization task.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PersonaAnimator\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u65e0\u7ea6\u675f\u89c6\u9891\u4e2d\u5b66\u4e60\u4e2a\u6027\u5316\u8fd0\u52a8\u6a21\u5f0f\uff0c\u5b9e\u73b0\u4e86\u89c6\u9891\u5230\u89c6\u9891\u7684\u8fd0\u52a8\u4e2a\u6027\u5316\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5b66\u4e60\u8fd0\u52a8\u98ce\u683c\u3001\u4f9d\u8d56\u52a8\u6355\u6570\u636e\u3001\u8fdd\u53cd\u7269\u7406\u89c4\u5f8b\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u5c40\u9650\uff1a(1)\u59ff\u6001\u5f15\u5bfc\u7684\u8fd0\u52a8\u8fc1\u79fb\u65b9\u6cd5\u4ec5\u590d\u5236\u8fd0\u52a8\u800c\u4e0d\u5b66\u4e60\u98ce\u683c\u7279\u5f81\uff0c\u5bfc\u81f4\u89d2\u8272\u8868\u73b0\u529b\u4e0d\u8db3\uff1b(2)\u8fd0\u52a8\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u96be\u4ee5\u83b7\u53d6\u7684\u52a8\u4f5c\u6355\u6349\u6570\u636e\uff1b(3)\u751f\u6210\u7684\u8fd0\u52a8\u6709\u65f6\u8fdd\u53cd\u7269\u7406\u89c4\u5f8b\u3002", "method": "\u63d0\u51faPersonaAnimator\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u65e0\u7ea6\u675f\u89c6\u9891\u4e2d\u5b66\u4e60\u4e2a\u6027\u5316\u8fd0\u52a8\u6a21\u5f0f\uff1b\u6784\u5efa\u9996\u4e2a\u57fa\u4e8e\u89c6\u9891\u7684\u4e2a\u6027\u5316\u8fd0\u52a8\u6570\u636e\u96c6PersonaVid\uff08\u5305\u542b20\u4e2a\u8fd0\u52a8\u5185\u5bb9\u7c7b\u522b\u548c120\u4e2a\u8fd0\u52a8\u98ce\u683c\u7c7b\u522b\uff09\uff1b\u63d0\u51fa\u7269\u7406\u611f\u77e5\u7684\u8fd0\u52a8\u98ce\u683c\u6b63\u5219\u5316\u673a\u5236\u786e\u4fdd\u751f\u6210\u8fd0\u52a8\u7684\u7269\u7406\u5408\u7406\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cPersonaAnimator\u5728\u8fd0\u52a8\u8fc1\u79fb\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u4e3a\u89c6\u9891\u5230\u89c6\u9891\u8fd0\u52a8\u4e2a\u6027\u5316\u4efb\u52a1\u8bbe\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f00\u521b\u4e86\u89c6\u9891\u5230\u89c6\u9891\u8fd0\u52a8\u4e2a\u6027\u5316\u65b0\u4efb\u52a1\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u5b66\u4e60\u4e2a\u6027\u5316\u8fd0\u52a8\u6a21\u5f0f\u5e76\u786e\u4fdd\u7269\u7406\u5408\u7406\u6027\uff0c\u4e3a\u8fd0\u52a8\u751f\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19927", "pdf": "https://arxiv.org/pdf/2508.19927", "abs": "https://arxiv.org/abs/2508.19927", "authors": ["Fayaz Ali", "Muhammad Zawish", "Steven Davy", "Radu Timofte"], "title": "WaveHiT-SR: Hierarchical Wavelet Network for Efficient Image Super-Resolution", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 5 figures", "summary": "Transformers have demonstrated promising performance in computer vision tasks, including image super-resolution (SR). The quadratic computational complexity of window self-attention mechanisms in many transformer-based SR methods forces the use of small, fixed windows, limiting the receptive field. In this paper, we propose a new approach by embedding the wavelet transform within a hierarchical transformer framework, called (WaveHiT-SR). First, using adaptive hierarchical windows instead of static small windows allows to capture features across different levels and greatly improve the ability to model long-range dependencies. Secondly, the proposed model utilizes wavelet transforms to decompose images into multiple frequency subbands, allowing the network to focus on both global and local features while preserving structural details. By progressively reconstructing high-resolution images through hierarchical processing, the network reduces computational complexity without sacrificing performance. The multi-level decomposition strategy enables the network to capture fine-grained information in lowfrequency components while enhancing high-frequency textures. Through extensive experimentation, we confirm the effectiveness and efficiency of our WaveHiT-SR. Our refined versions of SwinIR-Light, SwinIR-NG, and SRFormer-Light deliver cutting-edge SR results, achieving higher efficiency with fewer parameters, lower FLOPs, and faster speeds.", "AI": {"tldr": "WaveHiT-SR\u662f\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u53d8\u6362\u7684\u5206\u5c42Transformer\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5206\u5c42\u7a97\u53e3\u548c\u591a\u9891\u6bb5\u5206\u89e3\uff0c\u5728\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\u63d0\u5347\u957f\u8ddd\u79bb\u4f9d\u8d56\u5efa\u6a21\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u8d85\u5206\u8fa8\u7387\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eTransformer\u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u7531\u4e8e\u7a97\u53e3\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u53ea\u80fd\u4f7f\u7528\u5c0f\u7684\u56fa\u5b9a\u7a97\u53e3\uff0c\u9650\u5236\u4e86\u611f\u53d7\u91ce\u548c\u957f\u8ddd\u79bb\u4f9d\u8d56\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u63d0\u51faWaveHiT-SR\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u81ea\u9002\u5e94\u5206\u5c42\u7a97\u53e3\u66ff\u4ee3\u9759\u6001\u5c0f\u7a97\u53e3\uff1b2\uff09\u5229\u7528\u5c0f\u6ce2\u53d8\u6362\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u591a\u4e2a\u9891\u7387\u5b50\u5e26\uff1b3\uff09\u901a\u8fc7\u5206\u5c42\u5904\u7406\u9010\u6b65\u91cd\u5efa\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u65b9\u6cd5\u5728SwinIR-Light\u3001SwinIR-NG\u548cSRFormer-Light\u7b49\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8d85\u5206\u8fa8\u7387\u7ed3\u679c\uff0c\u5177\u6709\u66f4\u5c11\u7684\u53c2\u6570\u3001\u66f4\u4f4e\u7684FLOPs\u548c\u66f4\u5feb\u7684\u901f\u5ea6\u3002", "conclusion": "WaveHiT-SR\u901a\u8fc7\u7ed3\u5408\u5c0f\u6ce2\u53d8\u6362\u548c\u5206\u5c42Transformer\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u3002"}}
{"id": "2508.20020", "pdf": "https://arxiv.org/pdf/2508.20020", "abs": "https://arxiv.org/abs/2508.20020", "authors": ["Yuhao Chen", "Shubin Chen", "Liang Lin", "Guangrun Wang"], "title": "GS: Generative Segmentation via Label Diffusion", "categories": ["cs.CV"], "comment": "12 pages, 7 figures, 5 tables", "summary": "Language-driven image segmentation is a fundamental task in vision-language understanding, requiring models to segment regions of an image corresponding to natural language expressions. Traditional methods approach this as a discriminative problem, assigning each pixel to foreground or background based on semantic alignment. Recently, diffusion models have been introduced to this domain, but existing approaches remain image-centric: they either (i) use image diffusion models as visual feature extractors, (ii) synthesize segmentation data via image generation to train discriminative models, or (iii) perform diffusion inversion to extract attention cues from pre-trained image diffusion models-thereby treating segmentation as an auxiliary process. In this paper, we propose GS (Generative Segmentation), a novel framework that formulates segmentation itself as a generative task via label diffusion. Instead of generating images conditioned on label maps and text, GS reverses the generative process: it directly generates segmentation masks from noise, conditioned on both the input image and the accompanying language description. This paradigm makes label generation the primary modeling target, enabling end-to-end training with explicit control over spatial and semantic fidelity. To demonstrate the effectiveness of our approach, we evaluate GS on Panoptic Narrative Grounding (PNG), a representative and challenging benchmark for multimodal segmentation that requires panoptic-level reasoning guided by narrative captions. Experimental results show that GS significantly outperforms existing discriminative and diffusion-based methods, setting a new state-of-the-art for language-driven segmentation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGS\uff08\u751f\u6210\u5f0f\u5206\u5272\uff09\u6846\u67b6\uff0c\u5c06\u56fe\u50cf\u5206\u5272\u91cd\u65b0\u5b9a\u4e49\u4e3a\u751f\u6210\u5f0f\u4efb\u52a1\uff0c\u901a\u8fc7\u6807\u7b7e\u6269\u6563\u76f4\u63a5\u751f\u6210\u5206\u5272\u63a9\u7801\uff0c\u5728Panoptic Narrative Grounding\u57fa\u51c6\u4e0a\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u8bed\u8a00\u9a71\u52a8\u7684\u56fe\u50cf\u5206\u5272\u89c6\u4e3a\u5224\u522b\u5f0f\u95ee\u9898\uff0c\u73b0\u6709\u6269\u6563\u6a21\u578b\u65b9\u6cd5\u4ecd\u4ee5\u56fe\u50cf\u4e3a\u4e2d\u5fc3\uff0c\u5c06\u5206\u5272\u4f5c\u4e3a\u8f85\u52a9\u8fc7\u7a0b\u3002\u672c\u6587\u5e0c\u671b\u5c06\u5206\u5272\u672c\u8eab\u4f5c\u4e3a\u751f\u6210\u4efb\u52a1\u6765\u5904\u7406\u3002", "method": "\u63d0\u51faGS\u6846\u67b6\uff0c\u901a\u8fc7\u6807\u7b7e\u6269\u6563\u76f4\u63a5\u751f\u6210\u5206\u5272\u63a9\u7801\uff0c\u800c\u4e0d\u662f\u751f\u6210\u56fe\u50cf\u3002\u8be5\u65b9\u6cd5\u4ee5\u8f93\u5165\u56fe\u50cf\u548c\u8bed\u8a00\u63cf\u8ff0\u4e3a\u6761\u4ef6\uff0c\u4ece\u566a\u58f0\u76f4\u63a5\u751f\u6210\u5206\u5272\u6807\u7b7e\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u5728Panoptic Narrative Grounding\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGS\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u5224\u522b\u5f0f\u548c\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\uff0c\u521b\u9020\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u5c06\u5206\u5272\u91cd\u65b0\u5b9a\u4e49\u4e3a\u751f\u6210\u4efb\u52a1\u7684\u65b9\u6cd5\u6709\u6548\uff0cGS\u6846\u67b6\u901a\u8fc7\u76f4\u63a5\u751f\u6210\u6807\u7b7e\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u7a7a\u95f4\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u63a7\u5236\uff0c\u4e3a\u8bed\u8a00\u9a71\u52a8\u5206\u5272\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.20072", "pdf": "https://arxiv.org/pdf/2508.20072", "abs": "https://arxiv.org/abs/2508.20072", "authors": ["Zhixuan Liang", "Yizhuo Li", "Tianshuo Yang", "Chengyue Wu", "Sitong Mao", "Liuao Pei", "Xiaokang Yang", "Jiangmiao Pang", "Yao Mu", "Ping Luo"], "title": "Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "15 pages", "summary": "Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions to robot actions. However, prevailing VLA decoders either generate actions autoregressively in a fixed left-to-right order or attach continuous diffusion or flow matching heads outside the backbone, demanding specialized training and iterative sampling that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a single-transformer policy that models discretized action chunks with discrete diffusion and is trained with the same cross-entropy objective as the VLM backbone. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary remasking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pretrained vision language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO, 71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv Bridge, improving over both autoregressive and continuous diffusion baselines. These findings indicate that discrete-diffusion action decoder supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u79bb\u6563\u6e0c\u6563VLA\u6a21\u578b\uff0c\u901a\u8fc7\u79bb\u6563\u6e0c\u6563\u6280\u672f\u5c06\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e0e\u673a\u5668\u4eba\u52a8\u4f5c\u63a7\u5236\u7edf\u4e00\uff0c\u91c7\u7528\u8de8\u71b5\u6bdb\u635f\u5931\u8fdb\u884c\u8bad\u7ec3\uff0c\u652f\u6301\u5e76\u884c\u89e3\u7801\u548c\u9510\u6cbb\u91cd\u5e2e\u52a9\u9519\u8bef\u4fee\u6b63\uff0c\u5728\u591a\u4e2a\u73af\u5883\u4e2d\u8d85\u8d8a\u4e86\u81ea\u56de\u5f52\u548c\u8fde\u7eed\u6e0c\u6563\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u73b0\u6709VLA\u89e3\u7801\u5668\u6216\u8005\u91c7\u7528\u56fa\u5b9a\u5de6\u5230\u53f3\u7684\u81ea\u56de\u5f52\u751f\u6210\u65b9\u5f0f\uff0c\u6216\u8005\u5728\u6838\u5fc3\u6a21\u578b\u5916\u9644\u52a0\u8fde\u7eed\u6e0c\u6563\u5934\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u9700\u8981\u4e13\u95e8\u8bad\u7ec3\u548c\u8fed\u4ee3\u91c7\u6837\uff0c\u5f71\u54cd\u4e86\u7edf\u4e00\u53ef\u6269\u5c55\u7684\u67b6\u6784\u8bbe\u8ba1\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5355\u4e00transformer\u7b56\u7565\uff0c\u4f7f\u7528\u79bb\u6563\u6e0c\u6563\u5bf9\u79bb\u6563\u5316\u7684\u52a8\u4f5c\u5757\u8fdb\u884c\u5efa\u6a21\uff0c\u4e0eVLM\u6838\u5fc3\u91c7\u7528\u76f8\u540c\u7684\u8de8\u71b5\u6bdb\u635f\u5931\u8fdb\u884c\u8bad\u7ec3\u3002\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u4e86\u4e8c\u6b21\u91cd\u65b0\u906e\u7f6e\u6280\u672f\u6765\u91cd\u65b0\u5904\u7406\u4e0d\u786e\u5b9a\u7684\u9884\u6d4b\u3002", "result": "\u5728LIBERO\u4e0a\u8fbe\u523096.3%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u5728SimplerEnv Fractal\u4e0a\u8fbe\u523071.2%\u7684\u89c6\u89c9\u5339\u914d\u7387\uff0c\u5728SimplerEnv Bridge\u4e0a\u8fbe\u523049.3%\u7684\u603b\u4f53\u6027\u80fd\uff0c\u90fd\u8d85\u8fc7\u4e86\u81ea\u56de\u5f52\u548c\u8fde\u7eed\u6e0c\u6563\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u79bb\u6563\u6e0c\u6563\u52a8\u4f5c\u89e3\u7801\u5668\u652f\u6301\u7cbe\u786e\u7684\u52a8\u4f5c\u5efa\u6a21\u548c\u4e00\u81f4\u6027\u8bad\u7ec3\uff0c\u4e3a\u5c06VLA\u6269\u5c55\u5230\u66f4\u5927\u6a21\u578b\u548c\u6570\u636e\u96c6\u57f9\u57fa\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.19508", "pdf": "https://arxiv.org/pdf/2508.19508", "abs": "https://arxiv.org/abs/2508.19508", "authors": ["Tian Qiu", "Alan Zoubi", "Yiyuan Lin", "Ruiming Du", "Lailiang Cheng", "Yu Jiang"], "title": "DATR: Diffusion-based 3D Apple Tree Reconstruction Framework with Sparse-View", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Digital twin applications offered transformative potential by enabling real-time monitoring and robotic simulation through accurate virtual replicas of physical assets. The key to these systems is 3D reconstruction with high geometrical fidelity. However, existing methods struggled under field conditions, especially with sparse and occluded views. This study developed a two-stage framework (DATR) for the reconstruction of apple trees from sparse views. The first stage leverages onboard sensors and foundation models to semi-automatically generate tree masks from complex field images. Tree masks are used to filter out background information in multi-modal data for the single-image-to-3D reconstruction at the second stage. This stage consists of a diffusion model and a large reconstruction model for respective multi view and implicit neural field generation. The training of the diffusion model and LRM was achieved by using realistic synthetic apple trees generated by a Real2Sim data generator. The framework was evaluated on both field and synthetic datasets. The field dataset includes six apple trees with field-measured ground truth, while the synthetic dataset featured structurally diverse trees. Evaluation results showed that our DATR framework outperformed existing 3D reconstruction methods across both datasets and achieved domain-trait estimation comparable to industrial-grade stationary laser scanners while improving the throughput by $\\sim$360 times, demonstrating strong potential for scalable agricultural digital twin systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6DATR\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u89c6\u89d2\u91cd\u5efa\u82f9\u679c\u68113D\u6a21\u578b\uff0c\u5728\u91ce\u5916\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5904\u7406\u901f\u5ea6\u6bd4\u5de5\u4e1a\u7ea7\u6fc0\u5149\u626b\u63cf\u4eea\u5feb\u7ea6360\u500d\u3002", "motivation": "\u6570\u5b57\u5b6a\u751f\u5e94\u7528\u9700\u8981\u9ad8\u7cbe\u5ea6\u76843D\u91cd\u5efa\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u91ce\u5916\u7a00\u758f\u548c\u906e\u6321\u89c6\u89d2\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u519c\u4e1a\u73af\u5883\u4e2d\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u673a\u8f7d\u4f20\u611f\u5668\u548c\u57fa\u7840\u6a21\u578b\u4ece\u590d\u6742\u91ce\u5916\u56fe\u50cf\u4e2d\u534a\u81ea\u52a8\u751f\u6210\u6811\u6728\u63a9\u7801\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u6269\u6563\u6a21\u578b\u548c\u5927\u91cd\u5efa\u6a21\u578b\u8fdb\u884c\u5355\u56fe\u50cf\u52303D\u91cd\u5efa\uff0c\u901a\u8fc7Real2Sim\u6570\u636e\u751f\u6210\u5668\u751f\u6210\u5408\u6210\u82f9\u679c\u6811\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "DATR\u6846\u67b6\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u90fd\u4f18\u4e8e\u73b0\u67093D\u91cd\u5efa\u65b9\u6cd5\uff0c\u5728\u9886\u57df\u7279\u5f81\u4f30\u8ba1\u65b9\u9762\u4e0e\u5de5\u4e1a\u7ea7\u56fa\u5b9a\u6fc0\u5149\u626b\u63cf\u4eea\u76f8\u5f53\uff0c\u540c\u65f6\u5904\u7406\u541e\u5410\u91cf\u63d0\u9ad8\u4e86\u7ea6360\u500d\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u6784\u5efa\u53ef\u6269\u5c55\u519c\u4e1a\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u91ce\u5916\u7a00\u758f\u89c6\u89d2\u6761\u4ef6\u4e0b\u76843D\u91cd\u5efa\u6311\u6218\u3002"}}
