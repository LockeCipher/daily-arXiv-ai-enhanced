{"id": "2509.07522", "pdf": "https://arxiv.org/pdf/2509.07522", "abs": "https://arxiv.org/abs/2509.07522", "authors": ["Jierui Ren", "Haojie Jin", "Bo Pang", "Yisong Chen", "Guoping Wang", "Sheng Li"], "title": "Neural Cone Radiosity for Interactive Global Illumination with Glossy Materials", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Modeling of high-frequency outgoing radiance distributions has long been a key challenge in rendering, particularly for glossy material. Such distributions concentrate radiative energy within a narrow lobe and are highly sensitive to changes in view direction. However, existing neural radiosity methods, which primarily rely on positional feature encoding, exhibit notable limitations in capturing these high-frequency, strongly view-dependent radiance distributions. To address this, we propose a highly-efficient approach by reflectance-aware ray cone encoding based on the neural radiosity framework, named neural cone radiosity. The core idea is to employ a pre-filtered multi-resolution hash grid to accurately approximate the glossy BSDF lobe, embedding view-dependent reflectance characteristics directly into the encoding process through continuous spatial aggregation. Our design not only significantly improves the network's ability to model high-frequency reflection distributions but also effectively handles surfaces with a wide range of glossiness levels, from highly glossy to low-gloss finishes. Meanwhile, our method reduces the network's burden in fitting complex radiance distributions, allowing the overall architecture to remain compact and efficient. Comprehensive experimental results demonstrate that our method consistently produces high-quality, noise-free renderings in real time under various glossiness conditions, and delivers superior fidelity and realism compared to baseline approaches.", "AI": {"tldr": "\u63d0\u51fa\u795e\u7ecf\u9525\u8f90\u5c04\u5ea6\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cd\u5c04\u611f\u77e5\u7684\u5c04\u7ebf\u9525\u7f16\u7801\u6709\u6548\u89e3\u51b3\u9ad8\u9891\u51fa\u5c04\u8f90\u5c04\u5ea6\u5206\u5e03\u7684\u5efa\u6a21\u96be\u9898\uff0c\u5728\u4fdd\u6301\u7d27\u51d1\u7f51\u7edc\u7ed3\u6784\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5b9e\u65f6\u6e32\u67d3\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u8f90\u5c04\u5ea6\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4f4d\u7f6e\u7279\u5f81\u7f16\u7801\uff0c\u5728\u6355\u6349\u9ad8\u9891\u3001\u5f3a\u89c6\u89d2\u4f9d\u8d56\u7684\u8f90\u5c04\u5ea6\u5206\u5e03\u65b9\u9762\u5b58\u5728\u660e\u663e\u5c40\u9650\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5149\u6cfd\u6750\u8d28\u3002", "method": "\u57fa\u4e8e\u9884\u6ee4\u6ce2\u591a\u5206\u8fa8\u7387\u54c8\u5e0c\u7f51\u683c\uff0c\u901a\u8fc7\u8fde\u7eed\u7a7a\u95f4\u805a\u5408\u5c06\u89c6\u89d2\u4f9d\u8d56\u7684\u53cd\u5c04\u7279\u6027\u76f4\u63a5\u5d4c\u5165\u7f16\u7801\u8fc7\u7a0b\uff0c\u51c6\u786e\u8fd1\u4f3c\u5149\u6cfdBSDF\u6ce2\u74e3\u3002", "result": "\u5728\u5404\u79cd\u5149\u6cfd\u5ea6\u6761\u4ef6\u4e0b\u5b9e\u65f6\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u65e0\u566a\u58f0\u7684\u6e32\u67d3\u7ed3\u679c\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u4fdd\u771f\u5ea6\u548c\u771f\u5b9e\u611f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7f51\u7edc\u5bf9\u9ad8\u9891\u53cd\u5c04\u5206\u5e03\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u6709\u6548\u5904\u7406\u4ece\u9ad8\u5149\u5230\u4f4e\u5149\u7b49\u5404\u79cd\u5149\u6cfd\u5ea6\u8868\u9762\uff0c\u540c\u65f6\u51cf\u8f7b\u4e86\u7f51\u7edc\u62df\u5408\u590d\u6742\u8f90\u5c04\u5ea6\u5206\u5e03\u7684\u8d1f\u62c5\u3002"}}
{"id": "2509.07021", "pdf": "https://arxiv.org/pdf/2509.07021", "abs": "https://arxiv.org/abs/2509.07021", "authors": ["Jiarui Chen", "Yikeng Chen", "Yingshuang Zou", "Ye Huang", "Peng Wang", "Yuan Liu", "Yujing Sun", "Wenping Wang"], "title": "MEGS$^{2}$: Memory-Efficient Gaussian Splatting via Spherical Gaussians and Unified Pruning", "categories": ["cs.CV", "cs.AI"], "comment": "14 pages, 4 figures", "summary": "3D Gaussian Splatting (3DGS) has emerged as a dominant novel-view synthesis technique, but its high memory consumption severely limits its applicability on edge devices. A growing number of 3DGS compression methods have been proposed to make 3DGS more efficient, yet most only focus on storage compression and fail to address the critical bottleneck of rendering memory. To address this problem, we introduce MEGS$^{2}$, a novel memory-efficient framework that tackles this challenge by jointly optimizing two key factors: the total primitive number and the parameters per primitive, achieving unprecedented memory compression. Specifically, we replace the memory-intensive spherical harmonics with lightweight arbitrarily-oriented spherical Gaussian lobes as our color representations. More importantly, we propose a unified soft pruning framework that models primitive-number and lobe-number pruning as a single constrained optimization problem. Experiments show that MEGS$^{2}$ achieves a 50% static VRAM reduction and a 40% rendering VRAM reduction compared to existing methods, while maintaining comparable rendering quality.", "AI": {"tldr": "MEGS\u00b2\u662f\u4e00\u4e2a\u5185\u5b58\u9ad8\u6548\u76843D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u57fa\u5143\u6570\u91cf\u548c\u6bcf\u4e2a\u57fa\u5143\u7684\u53c2\u6570\uff0c\u5b9e\u73b0\u4e86\u524d\u6240\u672a\u6709\u7684\u5185\u5b58\u538b\u7f29\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u51cf\u5c1150%\u9759\u6001VRAM\u548c40%\u6e32\u67d3VRAM\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u867d\u7136\u5728\u65b0\u89c6\u89d2\u5408\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u9ad8\u5185\u5b58\u6d88\u8017\u4e25\u91cd\u9650\u5236\u4e86\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u3002\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u5927\u591a\u53ea\u5173\u6ce8\u5b58\u50a8\u538b\u7f29\uff0c\u672a\u80fd\u89e3\u51b3\u6e32\u67d3\u5185\u5b58\u7684\u5173\u952e\u74f6\u9888\u95ee\u9898\u3002", "method": "1) \u7528\u8f7b\u91cf\u7ea7\u7684\u4efb\u610f\u65b9\u5411\u7403\u9762\u9ad8\u65af\u74e3\u66ff\u4ee3\u5185\u5b58\u5bc6\u96c6\u7684\u7403\u8c10\u51fd\u6570\u4f5c\u4e3a\u989c\u8272\u8868\u793a\uff1b2) \u63d0\u51fa\u7edf\u4e00\u7684\u8f6f\u526a\u679d\u6846\u67b6\uff0c\u5c06\u57fa\u5143\u6570\u91cf\u548c\u74e3\u6570\u91cf\u526a\u679d\u5efa\u6a21\u4e3a\u5355\u4e00\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMEGS\u00b2\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u4e8650%\u7684\u9759\u6001VRAM\u51cf\u5c11\u548c40%\u7684\u6e32\u67d3VRAM\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u7684\u6e32\u67d3\u8d28\u91cf\u3002", "conclusion": "MEGS\u00b2\u901a\u8fc7\u8054\u5408\u4f18\u5316\u57fa\u5143\u6570\u91cf\u548c\u53c2\u6570\u6548\u7387\uff0c\u6210\u529f\u89e3\u51b3\u4e863DGS\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.07050", "pdf": "https://arxiv.org/pdf/2509.07050", "abs": "https://arxiv.org/abs/2509.07050", "authors": ["Juan Manuel Contreras"], "title": "Automated Evaluation of Gender Bias Across 13 Large Multimodal Models", "categories": ["cs.CV", "cs.AI", "cs.CY", "I.2.7; F.2.2"], "comment": null, "summary": "Large multimodal models (LMMs) have revolutionized text-to-image generation, but they risk perpetuating the harmful social biases in their training data. Prior work has identified gender bias in these models, but methodological limitations prevented large-scale, comparable, cross-model analysis. To address this gap, we introduce the Aymara Image Fairness Evaluation, a benchmark for assessing social bias in AI-generated images. We test 13 commercially available LMMs using 75 procedurally-generated, gender-neutral prompts to generate people in stereotypically-male, stereotypically-female, and non-stereotypical professions. We then use a validated LLM-as-a-judge system to score the 965 resulting images for gender representation. Our results reveal (p < .001 for all): 1) LMMs systematically not only reproduce but actually amplify occupational gender stereotypes relative to real-world labor data, generating men in 93.0% of images for male-stereotyped professions but only 22.5% for female-stereotyped professions; 2) Models exhibit a strong default-male bias, generating men in 68.3% of the time for non-stereotyped professions; and 3) The extent of bias varies dramatically across models, with overall male representation ranging from 46.7% to 73.3%. Notably, the top-performing model de-amplified gender stereotypes and approached gender parity, achieving the highest fairness scores. This variation suggests high bias is not an inevitable outcome but a consequence of design choices. Our work provides the most comprehensive cross-model benchmark of gender bias to date and underscores the necessity of standardized, automated evaluation tools for promoting accountability and fairness in AI development.", "AI": {"tldr": "Aymara Image Fairness Evaluation\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b(LMMs)\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u7cfb\u7edf\u6027\u653e\u5927\u804c\u4e1a\u6027\u522b\u523b\u677f\u5370\u8c61\uff0c\u5b58\u5728\u663e\u8457\u7684\u9ed8\u8ba4\u7537\u6027\u504f\u89c1\uff0c\u4e14\u4e0d\u540c\u6a21\u578b\u95f4\u7684\u504f\u89c1\u7a0b\u5ea6\u5dee\u5f02\u5f88\u5927\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u867d\u7136\u8bc6\u522b\u4e86\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u7684\u6027\u522b\u504f\u89c1\uff0c\u4f46\u65b9\u6cd5\u5b66\u9650\u5236\u963b\u788d\u4e86\u5927\u89c4\u6a21\u3001\u53ef\u6bd4\u8f83\u7684\u8de8\u6a21\u578b\u5206\u6790\uff0c\u9700\u8981\u5f00\u53d1\u6807\u51c6\u5316\u8bc4\u4f30\u5de5\u5177\u6765\u4fc3\u8fdbAI\u516c\u5e73\u6027\u3002", "method": "\u4f7f\u752875\u4e2a\u7a0b\u5e8f\u751f\u6210\u7684\u6027\u522b\u4e2d\u6027\u63d0\u793a\u8bcd\uff0c\u6d4b\u8bd513\u4e2a\u5546\u4e1aLMM\u6a21\u578b\u751f\u6210\u523b\u677f\u7537\u6027\u3001\u523b\u677f\u5973\u6027\u548c\u975e\u523b\u677f\u804c\u4e1a\u7684\u4eba\u7269\u56fe\u50cf\uff0c\u7136\u540e\u4f7f\u7528\u7ecf\u8fc7\u9a8c\u8bc1\u7684LLM-as-a-judge\u7cfb\u7edf\u5bf9965\u5f20\u56fe\u50cf\u8fdb\u884c\u6027\u522b\u8868\u5f81\u8bc4\u5206\u3002", "result": "LMMs\u7cfb\u7edf\u6027\u653e\u5927\u804c\u4e1a\u6027\u522b\u523b\u677f\u5370\u8c61\uff1a\u7537\u6027\u523b\u677f\u804c\u4e1a\u4e2d93.0%\u751f\u6210\u7537\u6027\uff0c\u5973\u6027\u523b\u677f\u804c\u4e1a\u4e2d\u4ec522.5%\u751f\u6210\u7537\u6027\uff1b\u975e\u523b\u677f\u804c\u4e1a\u4e2d68.3%\u9ed8\u8ba4\u751f\u6210\u7537\u6027\uff1b\u4e0d\u540c\u6a21\u578b\u7537\u6027\u8868\u5f81\u6bd4\u4f8b\u4ece46.7%\u523073.3%\u4e0d\u7b49\u3002", "conclusion": "\u504f\u89c1\u7a0b\u5ea6\u56e0\u6a21\u578b\u8bbe\u8ba1\u9009\u62e9\u800c\u5f02\uff0c\u9ad8\u6027\u80fd\u6a21\u578b\u53ef\u5b9e\u73b0\u6027\u522b\u5e73\u7b49\uff0c\u8868\u660e\u9ad8\u504f\u89c1\u5e76\u975e\u4e0d\u53ef\u907f\u514d\u3002\u7814\u7a76\u63d0\u4f9b\u4e86\u6700\u5168\u9762\u7684\u8de8\u6a21\u578b\u6027\u522b\u504f\u89c1\u57fa\u51c6\uff0c\u5f3a\u8c03\u6807\u51c6\u5316\u81ea\u52a8\u5316\u8bc4\u4f30\u5de5\u5177\u5bf9AI\u516c\u5e73\u6027\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2509.07178", "pdf": "https://arxiv.org/pdf/2509.07178", "abs": "https://arxiv.org/abs/2509.07178", "authors": ["Muhammad Saad Saeed", "Ijaz Ul Haq", "Khalid Malik"], "title": "Realism to Deception: Investigating Deepfake Detectors Against Face Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Face enhancement techniques are widely used to enhance facial appearance. However, they can inadvertently distort biometric features, leading to significant decrease in the accuracy of deepfake detectors. This study hypothesizes that these techniques, while improving perceptual quality, can degrade the performance of deepfake detectors. To investigate this, we systematically evaluate whether commonly used face enhancement methods can serve an anti-forensic role by reducing detection accuracy. We use both traditional image processing methods and advanced GAN-based enhancements to evaluate the robustness of deepfake detectors. We provide a comprehensive analysis of the effectiveness of these enhancement techniques, focusing on their impact on Na\\\"ive, Spatial, and Frequency-based detection methods. Furthermore, we conduct adversarial training experiments to assess whether exposure to face enhancement transformations improves model robustness. Experiments conducted on the FaceForensics++, DeepFakeDetection, and CelebDF-v2 datasets indicate that even basic enhancement filters can significantly reduce detection accuracy achieving ASR up to 64.63\\%. In contrast, GAN-based techniques further exploit these vulnerabilities, achieving ASR up to 75.12\\%. Our results demonstrate that face enhancement methods can effectively function as anti-forensic tools, emphasizing the need for more resilient and adaptive forensic methods.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u9762\u90e8\u589e\u5f3a\u6280\u672f\u5bf9\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5373\u4f7f\u57fa\u7840\u589e\u5f3a\u7b5b\u955c\u4e5f\u80fd\u663e\u8457\u964d\u4f4e\u68c0\u6d4b\u51c6\u786e\u6027\uff0cGAN\u57fa\u589e\u5f3a\u6280\u672f\u66f4\u80fd\u5b8c\u5168\u523a\u6d3b\u68c0\u6d4b\u5668\u7684\u810f\u6f0f\u3002", "motivation": "\u9762\u90e8\u589e\u5f3a\u6280\u672f\u867d\u80fd\u6539\u5584\u9762\u90e8\u5916\u89c2\uff0c\u4f46\u53ef\u80fd\u65e0\u610f\u4e2d\u626d\u66f2\u751f\u7269\u8bc6\u522b\u7279\u5f81\uff0c\u5bfc\u81f4\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\u51c6\u786e\u6027\u4e0b\u964d\u3002\u672c\u7814\u7a76\u60f3\u8981\u8bc6\u522b\u8fd9\u4e9b\u589e\u5f3a\u6280\u672f\u662f\u5426\u53ef\u4ee5\u4f5c\u4e3a\u53cd\u4f8b\u8bc1\u5de5\u5177\u4f7f\u7528\u3002", "method": "\u7cfb\u7edf\u6027\u8bc4\u4f30\u4f20\u7edf\u56fe\u50cf\u5904\u7406\u65b9\u6cd5\u548cGAN\u57fa\u589e\u5f3a\u6280\u672f\u5bf9\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\u7a33\u5065\u6027\u7684\u5f71\u54cd\uff0c\u5206\u6790\u5bf9Na\u00efve\u3001Spatial\u548cFrequency-based\u68c0\u6d4b\u65b9\u6cd5\u7684\u6548\u679c\uff0c\u5e76\u8fdb\u884c\u5bf9\u6297\u6027\u8bad\u7ec3\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u7840\u589e\u5f3a\u7b5b\u955c\u80fd\u5c06\u68c0\u6d4b\u51c6\u786e\u6027\u964d\u4f4e\u81f364.63%\uff0cGAN\u57fa\u6280\u672f\u66f4\u80fd\u8fbe\u523075.12%\u7684\u653b\u51fb\u6210\u529f\u7387\u3002\u5bf9\u6297\u6027\u8bad\u7ec3\u5e76\u4e0d\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u7a33\u5065\u6027\u3002", "conclusion": "\u9762\u90e8\u589e\u5f3a\u6280\u672f\u53ef\u4ee5\u6709\u6548\u5730\u4f5c\u4e3a\u53cd\u4f8b\u8bc1\u5de5\u5177\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u66f4\u52a0\u5f39\u6027\u548c\u9002\u5e94\u6027\u5f3a\u7684\u4f8b\u8bc1\u65b9\u6cd5\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2509.07295", "pdf": "https://arxiv.org/pdf/2509.07295", "abs": "https://arxiv.org/abs/2509.07295", "authors": ["Ji Xie", "Trevor Darrell", "Luke Zettlemoyer", "XuDong Wang"], "title": "Reconstruction Alignment Improves Unified Multimodal Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "28 pages, 24 figures and 10 tables", "summary": "Unified multimodal models (UMMs) unify visual understanding and generation within a single architecture. However, conventional training relies on image-text pairs (or sequences) whose captions are typically sparse and miss fine-grained visual details--even when they use hundreds of words to describe a simple image. We introduce Reconstruction Alignment (RecA), a resource-efficient post-training method that leverages visual understanding encoder embeddings as dense \"text prompts,\" providing rich supervision without captions. Concretely, RecA conditions a UMM on its own visual understanding embeddings and optimizes it to reconstruct the input image with a self-supervised reconstruction loss, thereby realigning understanding and generation. Despite its simplicity, RecA is broadly applicable: across autoregressive, masked-autoregressive, and diffusion-based UMMs, it consistently improves generation and editing fidelity. With only 27 GPU-hours, post-training with RecA substantially improves image generation performance on GenEval (0.73$\\rightarrow$0.90) and DPGBench (80.93$\\rightarrow$88.15), while also boosting editing benchmarks (ImgEdit 3.38$\\rightarrow$3.75, GEdit 6.94$\\rightarrow$7.25). Notably, RecA surpasses much larger open-source models and applies broadly across diverse UMM architectures, establishing it as an efficient and general post-training alignment strategy for UMMs", "AI": {"tldr": "RecA\u662f\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u89c6\u89c9\u7406\u89e3\u7f16\u7801\u5668\u5d4c\u5165\u4f5c\u4e3a\u5bc6\u96c6\u6587\u672c\u63d0\u793a\uff0c\u65e0\u9700\u6807\u6ce8\u5373\u53ef\u63d0\u4f9b\u4e30\u5bcc\u76d1\u7763\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u7684\u751f\u6210\u548c\u7f16\u8f91\u6027\u80fd", "motivation": "\u4f20\u7edf\u591a\u6a21\u6001\u6a21\u578b\u8bad\u7ec3\u4f9d\u8d56\u7a00\u758f\u7684\u56fe\u50cf-\u6587\u672c\u5bf9\uff0c\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7ec6\u8282\u76d1\u7763\uff0c\u5bfc\u81f4\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u4e0d\u5bf9\u9f50", "method": "\u63d0\u51fa\u91cd\u5efa\u5bf9\u9f50(RecA)\u65b9\u6cd5\uff0c\u8ba9\u6a21\u578b\u57fa\u4e8e\u81ea\u8eab\u7684\u89c6\u89c9\u7406\u89e3\u5d4c\u5165\u91cd\u5efa\u8f93\u5165\u56fe\u50cf\uff0c\u4f7f\u7528\u81ea\u76d1\u7763\u91cd\u5efa\u635f\u5931\u6765\u5bf9\u9f50\u7406\u89e3\u548c\u751f\u6210", "result": "\u4ec5\u752827 GPU\u5c0f\u65f6\uff0c\u5728GenEval(0.73\u21920.90)\u548cDPGBench(80.93\u219288.15)\u4e0a\u663e\u8457\u63d0\u5347\u751f\u6210\u6027\u80fd\uff0c\u7f16\u8f91\u57fa\u51c6\u4e5f\u6709\u6539\u5584(ImgEdit 3.38\u21923.75, GEdit 6.94\u21927.25)\uff0c\u8d85\u8d8a\u66f4\u5927\u5f00\u6e90\u6a21\u578b", "conclusion": "RecA\u662f\u4e00\u79cd\u9ad8\u6548\u901a\u7528\u7684\u540e\u8bad\u7ec3\u5bf9\u9f50\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u591a\u6a21\u6001\u6a21\u578b\u67b6\u6784\uff0c\u80fd\u6709\u6548\u63d0\u5347\u89c6\u89c9\u751f\u6210\u548c\u7f16\u8f91\u7684\u4fdd\u771f\u5ea6"}}
{"id": "2509.07435", "pdf": "https://arxiv.org/pdf/2509.07435", "abs": "https://arxiv.org/abs/2509.07435", "authors": ["Ze-Xin Yin", "Jiaxiong Qiu", "Liu Liu", "Xinjie Wang", "Wei Sui", "Zhizhong Su", "Jian Yang", "Jin Xie"], "title": "DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset Generation", "categories": ["cs.CV"], "comment": "14 pages, 7 figures, project page:   https://zx-yin.github.io/dreamlifting/", "summary": "The labor- and experience-intensive creation of 3D assets with physically based rendering (PBR) materials demands an autonomous 3D asset creation pipeline. However, most existing 3D generation methods focus on geometry modeling, either baking textures into simple vertex colors or leaving texture synthesis to post-processing with image diffusion models. To achieve end-to-end PBR-ready 3D asset generation, we present Lightweight Gaussian Asset Adapter (LGAA), a novel framework that unifies the modeling of geometry and PBR materials by exploiting multi-view (MV) diffusion priors from a novel perspective. The LGAA features a modular design with three components. Specifically, the LGAA Wrapper reuses and adapts network layers from MV diffusion models, which encapsulate knowledge acquired from billions of images, enabling better convergence in a data-efficient manner. To incorporate multiple diffusion priors for geometry and PBR synthesis, the LGAA Switcher aligns multiple LGAA Wrapper layers encapsulating different knowledge. Then, a tamed variational autoencoder (VAE), termed LGAA Decoder, is designed to predict 2D Gaussian Splatting (2DGS) with PBR channels. Finally, we introduce a dedicated post-processing procedure to effectively extract high-quality, relightable mesh assets from the resulting 2DGS. Extensive quantitative and qualitative experiments demonstrate the superior performance of LGAA with both text-and image-conditioned MV diffusion models. Additionally, the modular design enables flexible incorporation of multiple diffusion priors, and the knowledge-preserving scheme leads to efficient convergence trained on merely 69k multi-view instances. Our code, pre-trained weights, and the dataset used will be publicly available via our project page: https://zx-yin.github.io/dreamlifting/.", "AI": {"tldr": "LGAA\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684PBR\u5c31\u7eea3D\u8d44\u4ea7\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u6269\u6563\u5148\u9a8c\u7edf\u4e00\u5efa\u6a21\u51e0\u4f55\u548c\u6750\u8d28\uff0c\u4f7f\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u548c\u9ad8\u8d28\u91cf\u53ef\u91cd\u5149\u7167\u7f51\u683c\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u76843D\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u51e0\u4f55\u5efa\u6a21\uff0c\u5c06\u7eb9\u7406\u70d8\u7119\u4e3a\u7b80\u5355\u9876\u70b9\u989c\u8272\u6216\u7559\u7ed9\u540e\u5904\u7406\uff0c\u7f3a\u4e4f\u7aef\u5230\u7aef\u7684PBR\u6750\u8d28\u751f\u6210\u80fd\u529b\uff0c\u9700\u8981\u81ea\u4e3b\u76843D\u8d44\u4ea7\u521b\u5efa\u6d41\u7a0b\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff1aLGAA Wrapper\u91cd\u7528\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\u5c42\uff0cLGAA Switcher\u5bf9\u9f50\u591a\u4e2a\u6269\u6563\u5148\u9a8c\uff0cLGAA Decoder\u9884\u6d4b\u5e26PBR\u901a\u9053\u76842D\u9ad8\u65af\u6e85\u5c04\uff0c\u6700\u540e\u901a\u8fc7\u540e\u5904\u7406\u63d0\u53d6\u9ad8\u8d28\u91cf\u53ef\u91cd\u5149\u7167\u7f51\u683c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eLGAA\u5728\u6587\u672c\u548c\u56fe\u50cf\u6761\u4ef6\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6a21\u5757\u5316\u8bbe\u8ba1\u652f\u6301\u7075\u6d3b\u96c6\u6210\u591a\u4e2a\u6269\u6563\u5148\u9a8c\uff0c\u4ec5\u970069k\u591a\u89c6\u89d2\u5b9e\u4f8b\u5373\u53ef\u9ad8\u6548\u6536\u655b\u3002", "conclusion": "LGAA\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684PBR\u5c31\u7eea3D\u8d44\u4ea7\u751f\u6210\uff0c\u7edf\u4e00\u4e86\u51e0\u4f55\u548c\u6750\u8d28\u7684\u5efa\u6a21\uff0c\u5177\u6709\u9ad8\u6548\u6536\u655b\u548c\u9ad8\u8d28\u91cf\u8f93\u51fa\u7684\u4f18\u52bf\u3002"}}
{"id": "2509.07456", "pdf": "https://arxiv.org/pdf/2509.07456", "abs": "https://arxiv.org/abs/2509.07456", "authors": ["Sai Siddhartha Chary Aylapuram", "Veeraraju Elluru", "Shivang Agarwal"], "title": "Bias-Aware Machine Unlearning: Towards Fairer Vision Models via Controllable Forgetting", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted for publication at ICCV 2025 UnMe workshop", "summary": "Deep neural networks often rely on spurious correlations in training data, leading to biased or unfair predictions in safety-critical domains such as medicine and autonomous driving. While conventional bias mitigation typically requires retraining from scratch or redesigning data pipelines, recent advances in machine unlearning provide a promising alternative for post-hoc model correction. In this work, we investigate \\textit{Bias-Aware Machine Unlearning}, a paradigm that selectively removes biased samples or feature representations to mitigate diverse forms of bias in vision models. Building on privacy-preserving unlearning techniques, we evaluate various strategies including Gradient Ascent, LoRA, and Teacher-Student distillation. Through empirical analysis on three benchmark datasets, CUB-200-2011 (pose bias), CIFAR-10 (synthetic patch bias), and CelebA (gender bias in smile detection), we demonstrate that post-hoc unlearning can substantially reduce subgroup disparities, with improvements in demographic parity of up to \\textbf{94.86\\%} on CUB-200, \\textbf{30.28\\%} on CIFAR-10, and \\textbf{97.37\\%} on CelebA. These gains are achieved with minimal accuracy loss and with methods scoring an average of 0.62 across the 3 settings on the joint evaluation of utility, fairness, quality, and privacy. Our findings establish machine unlearning as a practical framework for enhancing fairness in deployed vision systems without necessitating full retraining.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u9057\u5fd8\u6280\u672f\u7684\u504f\u7f6e\u611f\u77e5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u540e\u9009\u62e9\u6027\u79fb\u9664\u504f\u7f6e\u6837\u672c\u6216\u7279\u5f81\u8868\u793a\u6765\u51cf\u5c11\u89c6\u89c9\u6a21\u578b\u4e2d\u7684\u5404\u79cd\u504f\u7f6e\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u516c\u5e73\u6027\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5bb9\u6613\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u865a\u5047\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u5728\u533b\u7597\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\u4ea7\u751f\u6709\u504f\u6216\u4e0d\u516c\u5e73\u7684\u9884\u6d4b\u3002\u4f20\u7edf\u504f\u7f6e\u7f13\u89e3\u65b9\u6cd5\u9700\u8981\u4ece\u5934\u91cd\u65b0\u8bad\u7ec3\u6216\u91cd\u65b0\u8bbe\u8ba1\u6570\u636e\u7ba1\u9053\uff0c\u6210\u672c\u9ad8\u6602\u3002", "method": "\u57fa\u4e8e\u9690\u79c1\u4fdd\u62a4\u9057\u5fd8\u6280\u672f\uff0c\u8bc4\u4f30\u4e86\u68af\u5ea6\u4e0a\u5347\u3001LoRA\u548c\u5e08\u751f\u84b8\u998f\u7b49\u591a\u79cd\u7b56\u7565\uff0c\u5728CUB-200-2011\uff08\u59ff\u6001\u504f\u7f6e\uff09\u3001CIFAR-10\uff08\u5408\u6210\u8865\u4e01\u504f\u7f6e\uff09\u548cCelebA\uff08\u5fae\u7b11\u68c0\u6d4b\u4e2d\u7684\u6027\u522b\u504f\u7f6e\uff09\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\u3002", "result": "\u540e\u8bad\u7ec3\u9057\u5fd8\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u5b50\u7ec4\u5dee\u5f02\uff0c\u5728CUB-200\u4e0a\u4eba\u53e3\u7edf\u8ba1\u516c\u5e73\u6027\u63d0\u534794.86%\uff0cCIFAR-10\u63d0\u534730.28%\uff0cCelebA\u63d0\u534797.37%\uff0c\u51c6\u786e\u7387\u635f\u5931\u6700\u5c0f\uff0c\u5728\u6548\u7528\u3001\u516c\u5e73\u6027\u3001\u8d28\u91cf\u548c\u9690\u79c1\u7684\u8054\u5408\u8bc4\u4f30\u4e2d\u5e73\u5747\u5f97\u52060.62\u3002", "conclusion": "\u673a\u5668\u9057\u5fd8\u6280\u672f\u4e3a\u589e\u5f3a\u5df2\u90e8\u7f72\u89c6\u89c9\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u6846\u67b6\uff0c\u65e0\u9700\u8fdb\u884c\u5b8c\u6574\u7684\u91cd\u65b0\u8bad\u7ec3\u3002"}}
{"id": "2509.07472", "pdf": "https://arxiv.org/pdf/2509.07472", "abs": "https://arxiv.org/abs/2509.07472", "authors": ["Wenshuo Gao", "Xicheng Lan", "Shuai Yang"], "title": "ANYPORTAL: Zero-Shot Consistent Video Background Replacement", "categories": ["cs.CV"], "comment": "8 pages, ICCV 2025, Website: https://gaowenshuo.github.io/AnyPortal/", "summary": "Despite the rapid advancements in video generation technology, creating high-quality videos that precisely align with user intentions remains a significant challenge. Existing methods often fail to achieve fine-grained control over video details, limiting their practical applicability. We introduce ANYPORTAL, a novel zero-shot framework for video background replacement that leverages pre-trained diffusion models. Our framework collaboratively integrates the temporal prior of video diffusion models with the relighting capabilities of image diffusion models in a zero-shot setting. To address the critical challenge of foreground consistency, we propose a Refinement Projection Algorithm, which enables pixel-level detail manipulation to ensure precise foreground preservation. ANYPORTAL is training-free and overcomes the challenges of achieving foreground consistency and temporally coherent relighting. Experimental results demonstrate that ANYPORTAL achieves high-quality results on consumer-grade GPUs, offering a practical and efficient solution for video content creation and editing.", "AI": {"tldr": "ANYPORTAL\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u89c6\u9891\u80cc\u666f\u66ff\u6362\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u89c6\u9891\u7f16\u8f91\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u4fdd\u6301\u524d\u666f\u4e00\u81f4\u6027\u548c\u65f6\u95f4\u8fde\u8d2f\u6027", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6280\u672f\u5728\u7cbe\u786e\u63a7\u5236\u89c6\u9891\u7ec6\u8282\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u96be\u4ee5\u5b9e\u73b0\u7528\u6237\u610f\u56fe\u7684\u7cbe\u786e\u5bf9\u9f50\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528", "method": "\u63d0\u51fa\u96f6\u6837\u672c\u6846\u67b6ANYPORTAL\uff0c\u7ed3\u5408\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u65f6\u95f4\u5148\u9a8c\u548c\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u91cd\u65b0\u5149\u7167\u80fd\u529b\uff0c\u4f7f\u7528\u7cbe\u70bc\u6295\u5f71\u7b97\u6cd5\u5b9e\u73b0\u50cf\u7d20\u7ea7\u7ec6\u8282\u64cd\u4f5c", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eANYPORTAL\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7ed3\u679c\uff0c\u4e3a\u89c6\u9891\u5185\u5bb9\u521b\u4f5c\u548c\u7f16\u8f91\u63d0\u4f9b\u4e86\u5b9e\u7528\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848", "conclusion": "ANYPORTAL\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u9891\u80cc\u666f\u66ff\u6362\u4e2d\u7684\u524d\u666f\u4e00\u81f4\u6027\u548c\u65f6\u95f4\u8fde\u8d2f\u5149\u7167\u6311\u6218\uff0c\u4e3a\u96f6\u6837\u672c\u89c6\u9891\u7f16\u8f91\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5"}}
{"id": "2509.07488", "pdf": "https://arxiv.org/pdf/2509.07488", "abs": "https://arxiv.org/abs/2509.07488", "authors": ["Xiao Li", "Bharat Gandhi", "Ming Zhan", "Mohit Nehra", "Zhicheng Zhang", "Yuchen Sun", "Meijia Song", "Naisheng Zhang", "Xi Wang"], "title": "Fine-Tuning Vision-Language Models for Visual Navigation Assistance", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We address vision-language-driven indoor navigation to assist visually impaired individuals in reaching a target location using images and natural language guidance. Traditional navigation systems are ineffective indoors due to the lack of precise location data. Our approach integrates vision and language models to generate step-by-step navigational instructions, enhancing accessibility and independence. We fine-tune the BLIP-2 model with Low Rank Adaptation (LoRA) on a manually annotated indoor navigation dataset. We propose an evaluation metric that refines the BERT F1 score by emphasizing directional and sequential variables, providing a more comprehensive measure of navigational performance. After applying LoRA, the model significantly improved in generating directional instructions, overcoming limitations in the original BLIP-2 model.", "AI": {"tldr": "\u901a\u8fc7\u7ec6\u8c03BLIP-2\u6a21\u578b\u4f7f\u7528LoRA\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed9\u89c6\u969c\u4eba\u58eb\u63d0\u4f9b\u5ba4\u5185\u5bfc\u822a\u7684\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\u7cfb\u7edf\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e13\u95e8\u7684\u8bc4\u4f30\u6307\u6807\u6765\u91cf\u5316\u5bfc\u822a\u6307\u4ee4\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u5bfc\u822a\u7cfb\u7edf\u5728\u5ba4\u5185\u73af\u5883\u4e2d\u6548\u679c\u5dee\uff0c\u7f3a\u4e4f\u7cbe\u786e\u4f4d\u7f6e\u6570\u636e\uff0c\u9700\u8981\u4e3a\u89c6\u969c\u4eba\u58eb\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u5ba4\u5185\u5bfc\u822a\u5e2e\u52a9\u3002", "method": "\u4f7f\u7528\u4f4e\u79e9\u9002\u914d(LoRA)\u6280\u672f\u5bf9BLIP-2\u6a21\u578b\u8fdb\u884c\u7ec6\u8c03\uff0c\u57fa\u4e8e\u624b\u52a8\u6ce8\u91ca\u7684\u5ba4\u5185\u5bfc\u822a\u6570\u636e\u96c6\u751f\u6210\u6b65\u9aa4\u5f0f\u5bfc\u822a\u6307\u4ee4\u3002", "result": "\u7ec6\u8c03\u540e\u7684\u6a21\u578b\u5728\u751f\u6210\u65b9\u5411\u6307\u4ee4\u65b9\u9762\u663e\u8457\u63d0\u5347\uff0c\u5145\u5206\u5145\u5206\u514b\u670d\u4e86\u539f\u59cbBLIP-2\u6a21\u578b\u7684\u9650\u5236\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u578b\uff0c\u4e3a\u89c6\u969c\u4eba\u58eb\u63d0\u4f9b\u4e86\u66f4\u52a0\u51c6\u786e\u7684\u5ba4\u5185\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u8bbf\u95ee\u6027\u548c\u72ec\u7acb\u6027\u3002"}}
{"id": "2509.07493", "pdf": "https://arxiv.org/pdf/2509.07493", "abs": "https://arxiv.org/abs/2509.07493", "authors": ["Wenzhi Guo", "Bing Wang"], "title": "DiGS: Accurate and Complete Surface Reconstruction from 3D Gaussians via Direct SDF Learning", "categories": ["cs.CV", "cs.CG"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has recently emerged as a powerful paradigm for photorealistic view synthesis, representing scenes with spatially distributed Gaussian primitives. While highly effective for rendering, achieving accurate and complete surface reconstruction remains challenging due to the unstructured nature of the representation and the absence of explicit geometric supervision. In this work, we propose DiGS, a unified framework that embeds Signed Distance Field (SDF) learning directly into the 3DGS pipeline, thereby enforcing strong and interpretable surface priors. By associating each Gaussian with a learnable SDF value, DiGS explicitly aligns primitives with underlying geometry and improves cross-view consistency. To further ensure dense and coherent coverage, we design a geometry-guided grid growth strategy that adaptively distributes Gaussians along geometry-consistent regions under a multi-scale hierarchy. Extensive experiments on standard benchmarks, including DTU, Mip-NeRF 360, and Tanks& Temples, demonstrate that DiGS consistently improves reconstruction accuracy and completeness while retaining high rendering fidelity.", "AI": {"tldr": "DiGS\u5c06\u7b26\u53f7\u8ddd\u79bb\u573a(SDF)\u5b66\u4e60\u5d4c\u51653D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u5f15\u5bfc\u7684\u7f51\u683c\u589e\u957f\u7b56\u7565\u63d0\u5347\u8868\u9762\u91cd\u5efa\u7684\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6e32\u67d3\u8d28\u91cf", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u5728\u6e32\u67d3\u65b9\u9762\u6548\u679c\u51fa\u8272\uff0c\u4f46\u7531\u4e8e\u8868\u793a\u7684\u65e0\u7ed3\u6784\u6027\u548c\u7f3a\u4e4f\u663e\u5f0f\u51e0\u4f55\u76d1\u7763\uff0c\u96be\u4ee5\u5b9e\u73b0\u51c6\u786e\u5b8c\u6574\u7684\u8868\u9762\u91cd\u5efa", "method": "\u4e3a\u6bcf\u4e2a\u9ad8\u65af\u5173\u8054\u53ef\u5b66\u4e60\u7684SDF\u503c\uff0c\u8bbe\u8ba1\u51e0\u4f55\u5f15\u5bfc\u7684\u591a\u5c3a\u5ea6\u7f51\u683c\u589e\u957f\u7b56\u7565\uff0c\u81ea\u9002\u5e94\u5730\u5728\u51e0\u4f55\u4e00\u81f4\u533a\u57df\u5206\u5e03\u9ad8\u65af", "result": "\u5728DTU\u3001Mip-NeRF 360\u548cTanks&Temples\u7b49\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDiGS\u6301\u7eed\u63d0\u5347\u4e86\u91cd\u5efa\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6e32\u67d3\u4fdd\u771f\u5ea6", "conclusion": "DiGS\u901a\u8fc7\u5c06SDF\u5b66\u4e60\u4e0e3DGS\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u8868\u9762\u91cd\u5efa\u8d28\u91cf\uff0c\u4e3a3D\u91cd\u5efa\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.07538", "pdf": "https://arxiv.org/pdf/2509.07538", "abs": "https://arxiv.org/abs/2509.07538", "authors": ["Peijin Xie", "Shun Qian", "Bingquan Liu", "Dexin Wang", "Lin Sun", "Xiangzheng Zhang"], "title": "TextlessRAG: End-to-End Visual Document RAG by Speech Without Text", "categories": ["cs.CV"], "comment": "5 pages, 4 figures,", "summary": "Document images encapsulate a wealth of knowledge, while the portability of spoken queries enables broader and flexible application scenarios. Yet, no prior work has explored knowledge base question answering over visual document images with queries provided directly in speech. We propose TextlessRAG, the first end-to-end framework for speech-based question answering over large-scale document images. Unlike prior methods, TextlessRAG eliminates ASR, TTS and OCR, directly interpreting speech, retrieving relevant visual knowledge, and generating answers in a fully textless pipeline. To further boost performance, we integrate a layout-aware reranking mechanism to refine retrieval. Experiments demonstrate substantial improvements in both efficiency and accuracy. To advance research in this direction, we also release the first bilingual speech--document RAG dataset, featuring Chinese and English voice queries paired with multimodal document content. Both the dataset and our pipeline will be made available at repository:https://github.com/xiepeijinhit-hue/textlessrag", "AI": {"tldr": "TextlessRAG\u662f\u9996\u4e2a\u7aef\u5230\u7aef\u7684\u8bed\u97f3\u6587\u6863\u95ee\u7b54\u6846\u67b6\uff0c\u65e0\u9700ASR\u3001TTS\u548cOCR\uff0c\u76f4\u63a5\u5728\u8bed\u97f3\u548c\u89c6\u89c9\u6587\u6863\u4e4b\u95f4\u8fdb\u884c\u95ee\u7b54\uff0c\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u53cc\u8bed\u8bed\u97f3-\u6587\u6863RAG\u6570\u636e\u96c6\u3002", "motivation": "\u6587\u6863\u56fe\u50cf\u5305\u542b\u4e30\u5bcc\u77e5\u8bc6\uff0c\u8bed\u97f3\u67e5\u8be2\u5177\u6709\u4fbf\u643a\u6027\u548c\u7075\u6d3b\u6027\uff0c\u4f46\u4e4b\u524d\u6ca1\u6709\u5de5\u4f5c\u63a2\u7d22\u8fc7\u76f4\u63a5\u5728\u8bed\u97f3\u67e5\u8be2\u4e0b\u5bf9\u89c6\u89c9\u6587\u6863\u56fe\u50cf\u8fdb\u884c\u77e5\u8bc6\u5e93\u95ee\u7b54\u3002", "method": "\u63d0\u51faTextlessRAG\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u6d88\u9664ASR\u3001TTS\u548cOCR\uff0c\u76f4\u63a5\u89e3\u91ca\u8bed\u97f3\u3001\u68c0\u7d22\u76f8\u5173\u89c6\u89c9\u77e5\u8bc6\u5e76\u751f\u6210\u7b54\u6848\uff0c\u91c7\u7528\u5b8c\u5168\u65e0\u6587\u672c\u7684\u6d41\u7a0b\uff0c\u5e76\u96c6\u6210\u4e86\u5e03\u5c40\u611f\u77e5\u91cd\u6392\u5e8f\u673a\u5236\u6765\u4f18\u5316\u68c0\u7d22\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8bed\u97f3-\u6587\u6863\u95ee\u7b54\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u53d1\u5e03\u7684\u53cc\u8bed\u6570\u636e\u96c6\u548c\u5b8c\u6574\u6d41\u7a0b\u5c06\u4fc3\u8fdb\u8be5\u9886\u57df\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2509.07552", "pdf": "https://arxiv.org/pdf/2509.07552", "abs": "https://arxiv.org/abs/2509.07552", "authors": ["Peng Li", "Yisheng He", "Yingdong Hu", "Yuan Dong", "Weihao Yuan", "Yuan Liu", "Zilong Dong", "Yike Guo"], "title": "PanoLAM: Large Avatar Model for Gaussian Full-Head Synthesis from One-shot Unposed Image", "categories": ["cs.CV"], "comment": null, "summary": "We present a feed-forward framework for Gaussian full-head synthesis from a single unposed image. Unlike previous work that relies on time-consuming GAN inversion and test-time optimization, our framework can reconstruct the Gaussian full-head model given a single unposed image in a single forward pass. This enables fast reconstruction and rendering during inference. To mitigate the lack of large-scale 3D head assets, we propose a large-scale synthetic dataset from trained 3D GANs and train our framework using only synthetic data. For efficient high-fidelity generation, we introduce a coarse-to-fine Gaussian head generation pipeline, where sparse points from the FLAME model interact with the image features by transformer blocks for feature extraction and coarse shape reconstruction, which are then densified for high-fidelity reconstruction. To fully leverage the prior knowledge residing in pretrained 3D GANs for effective reconstruction, we propose a dual-branch framework that effectively aggregates the structured spherical triplane feature and unstructured point-based features for more effective Gaussian head reconstruction. Experimental results show the effectiveness of our framework towards existing work.", "AI": {"tldr": "\u57fa\u4e8e\u5355\u5f20\u65e0\u59ff\u6001\u56fe\u7247\u901a\u8fc7\u524d\u5411\u4f20\u64ad\u6846\u67b6\u5feb\u901f\u5408\u6210\u9ad8\u4fdd\u771f\u7684\u9ad8\u65af\u5168\u5934\u6a21\u578b\uff0c\u907f\u514d\u4e86GAN\u9006\u5411\u548c\u6d4b\u8bd5\u65f6\u4f18\u5316\u7684\u8017\u65f6\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u4ece\u5355\u5f20\u65e0\u59ff\u6001\u56fe\u7247\u5feb\u901f\u91cd\u5efa9AD\u5168\u5934\u6a21\u578b\u7684\u6311\u6218\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u4e2dGAN\u9006\u5411\u548c\u6d4b\u8bd5\u65f6\u4f18\u5316\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u4f7f\u7528\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u63d0\u51fa\u7531\u7c97\u5230\u7ec6\u7684\u9ad8\u65af\u5934\u90e8\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7FLAME\u6a21\u578b\u7a00\u758f\u70b9\u4e0e\u56fe\u50cf\u7279\u5f81\u4ea4\u4e92\uff0c\u5e76\u4f7f\u7528\u53cc\u5206\u652f\u6846\u67b6\u805a\u5408\u7ed3\u6784\u5316\u7403\u9762\u4e09\u5e73\u9762\u7279\u5f81\u548c\u975e\u7ed3\u6784\u5316\u70b9\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u6846\u67b6\u80fd\u591f\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u5b8c\u6210\u9ad8\u4fdd\u771f\u7684\u5168\u5934\u91cd\u5efa\uff0c\u5728\u901f\u5ea6\u548c\u6548\u679c\u4e0a\u90fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u524d\u5411\u4f20\u64ad\u6846\u67b6\u4e3a\u5355\u56fe3D\u5934\u90e8\u91cd\u5efa\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u5feb\u901f\u6e32\u67d3\u548c\u5b9e\u65f6\u5e94\u7528\u521b\u9020\u4e86\u6761\u4ef6\u3002"}}
{"id": "2509.07774", "pdf": "https://arxiv.org/pdf/2509.07774", "abs": "https://arxiv.org/abs/2509.07774", "authors": ["Yimin Pan", "Matthias Nie\u00dfner", "Tobias Kirschstein"], "title": "HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "This is the arXiv preprint of the paper \"Hair Strand Reconstruction   based on 3D Gaussian Splatting\" published at BMVC 2025. Project website:   https://yimin-pan.github.io/hair-gs/", "summary": "Human hair reconstruction is a challenging problem in computer vision, with growing importance for applications in virtual reality and digital human modeling. Recent advances in 3D Gaussians Splatting (3DGS) provide efficient and explicit scene representations that naturally align with the structure of hair strands. In this work, we extend the 3DGS framework to enable strand-level hair geometry reconstruction from multi-view images. Our multi-stage pipeline first reconstructs detailed hair geometry using a differentiable Gaussian rasterizer, then merges individual Gaussian segments into coherent strands through a novel merging scheme, and finally refines and grows the strands under photometric supervision.   While existing methods typically evaluate reconstruction quality at the geometric level, they often neglect the connectivity and topology of hair strands. To address this, we propose a new evaluation metric that serves as a proxy for assessing topological accuracy in strand reconstruction. Extensive experiments on both synthetic and real-world datasets demonstrate that our method robustly handles a wide range of hairstyles and achieves efficient reconstruction, typically completing within one hour.   The project page can be found at: https://yimin-pan.github.io/hair-gs/", "AI": {"tldr": "\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u591a\u89c6\u89d2\u56fe\u50cf\u91cd\u5efa\u53d1\u4e1d\u7ea7\u5934\u53d1\u51e0\u4f55\u7684\u7aef\u5230\u7aef\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u6d41\u7a0b\u5b9e\u73b0\u9ad8\u6548\u7cbe\u786e\u7684\u5934\u53d1\u91cd\u5efa", "motivation": "\u865a\u62df\u73b0\u5b9e\u548c\u6570\u5b57\u4eba\u5efa\u6a21\u5e94\u7528\u4e2d\u9700\u8981\u9ad8\u8d28\u91cf\u7684\u5934\u53d1\u91cd\u5efa\uff0c\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u53d1\u4e1d\u7684\u8fde\u63a5\u6027\u548c\u62d3\u6251\u7ed3\u6784\uff0c3DGS\u7684\u663e\u5f0f\u8868\u793a\u4e0e\u53d1\u4e1d\u7ed3\u6784\u5929\u7136\u5951\u5408", "method": "\u591a\u9636\u6bb5\u6d41\u6c34\u7ebf\uff1a\u9996\u5148\u4f7f\u7528\u53ef\u5fae\u5206\u9ad8\u65af\u5149\u6805\u5316\u5668\u91cd\u5efa\u8be6\u7ec6\u5934\u53d1\u51e0\u4f55\uff0c\u7136\u540e\u901a\u8fc7\u65b0\u9896\u7684\u5408\u5e76\u65b9\u6848\u5c06\u9ad8\u65af\u6bb5\u5408\u5e76\u4e3a\u8fde\u8d2f\u53d1\u4e1d\uff0c\u6700\u540e\u5728\u5149\u5ea6\u76d1\u7763\u4e0b\u4f18\u5316\u548c\u751f\u957f\u53d1\u4e1d", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u7a33\u5065\u5904\u7406\u5404\u79cd\u53d1\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u91cd\u5efa\uff08\u901a\u5e38\u5728\u4e00\u5c0f\u65f6\u5185\u5b8c\u6210\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u8bc4\u4f30\u62d3\u6251\u51c6\u786e\u6027\u7684\u65b0\u6307\u6807", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u6269\u5c55\u4e863DGS\u6846\u67b6\u7528\u4e8e\u53d1\u4e1d\u7ea7\u5934\u53d1\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u62d3\u6251\u7ed3\u6784\u7684\u95ee\u9898\uff0c\u4e3a\u5934\u53d1\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.07782", "pdf": "https://arxiv.org/pdf/2509.07782", "abs": "https://arxiv.org/abs/2509.07782", "authors": ["Hugo Blanc", "Jean-Emmanuel Deschaud", "Alexis Paljic"], "title": "RayGaussX: Accelerating Gaussian-Based Ray Marching for Real-Time and High-Quality Novel View Synthesis", "categories": ["cs.CV"], "comment": "Project page with videos and code: https://raygaussx.github.io/", "summary": "RayGauss has achieved state-of-the-art rendering quality for novel-view synthesis on synthetic and indoor scenes by representing radiance and density fields with irregularly distributed elliptical basis functions, rendered via volume ray casting using a Bounding Volume Hierarchy (BVH). However, its computational cost prevents real-time rendering on real-world scenes. Our approach, RayGaussX, builds on RayGauss by introducing key contributions that accelerate both training and inference. Specifically, we incorporate volumetric rendering acceleration strategies such as empty-space skipping and adaptive sampling, enhance ray coherence, and introduce scale regularization to reduce false-positive intersections. Additionally, we propose a new densification criterion that improves density distribution in distant regions, leading to enhanced graphical quality on larger scenes. As a result, RayGaussX achieves 5x to 12x faster training and 50x to 80x higher rendering speeds (FPS) on real-world datasets while improving visual quality by up to +0.56 dB in PSNR. Project page with videos and code: https://raygaussx.github.io/.", "AI": {"tldr": "RayGaussX\u5728RayGauss\u57fa\u7840\u4e0a\u8fdb\u884c\u4f18\u5316\uff0c\u901a\u8fc7\u5f15\u5165\u4f53\u79ef\u6e32\u67d3\u52a0\u901f\u7b56\u7565\u3001\u589e\u5f3a\u5149\u7ebf\u4e00\u81f4\u6027\u3001\u5c3a\u5ea6\u6b63\u5219\u5316\u548c\u65b0\u7684\u81f4\u5bc6\u5316\u51c6\u5219\uff0c\u5b9e\u73b0\u4e865-12\u500d\u8bad\u7ec3\u52a0\u901f\u548c50-80\u500d\u6e32\u67d3\u901f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u63d0\u9ad8\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "RayGauss\u867d\u7136\u5728\u5408\u6210\u548c\u5ba4\u5185\u573a\u666f\u7684\u65b0\u89c6\u89d2\u5408\u6210\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6e32\u67d3\u8d28\u91cf\uff0c\u4f46\u5176\u8ba1\u7b97\u6210\u672c\u963b\u788d\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u5b9e\u65f6\u6e32\u67d3\u3002", "method": "\u5f15\u5165\u4f53\u79ef\u6e32\u67d3\u52a0\u901f\u7b56\u7565\uff08\u7a7a\u7a7a\u95f4\u8df3\u8fc7\u548c\u81ea\u9002\u5e94\u91c7\u6837\uff09\u3001\u589e\u5f3a\u5149\u7ebf\u4e00\u81f4\u6027\u3001\u5c3a\u5ea6\u6b63\u5219\u5316\u4ee5\u51cf\u5c11\u8bef\u62a5\u4ea4\u96c6\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u81f4\u5bc6\u5316\u51c6\u5219\u6765\u6539\u5584\u8fdc\u8ddd\u79bb\u533a\u57df\u7684\u5bc6\u5ea6\u5206\u5e03\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e865-12\u500d\u8bad\u7ec3\u52a0\u901f\u548c50-80\u500d\u6e32\u67d3\u901f\u5ea6\u63d0\u5347\uff08FPS\uff09\uff0c\u89c6\u89c9\u8d28\u91cf\u63d0\u5347\u9ad8\u8fbe+0.56 dB PSNR\u3002", "conclusion": "RayGaussX\u6210\u529f\u89e3\u51b3\u4e86RayGauss\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u6e32\u67d3\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u9002\u7528\u4e8e\u66f4\u5927\u89c4\u6a21\u7684\u573a\u666f\u3002"}}
{"id": "2509.07798", "pdf": "https://arxiv.org/pdf/2509.07798", "abs": "https://arxiv.org/abs/2509.07798", "authors": ["Maja Schlereth", "Moritz Schillinger", "Katharina Breininger"], "title": "Faster, Self-Supervised Super-Resolution for Anisotropic Multi-View MRI Using a Sparse Coordinate Loss", "categories": ["cs.CV"], "comment": "11 pages, 2 figures", "summary": "Acquiring images in high resolution is often a challenging task. Especially in the medical sector, image quality has to be balanced with acquisition time and patient comfort. To strike a compromise between scan time and quality for Magnetic Resonance (MR) imaging, two anisotropic scans with different low-resolution (LR) orientations can be acquired. Typically, LR scans are analyzed individually by radiologists, which is time consuming and can lead to inaccurate interpretation. To tackle this, we propose a novel approach for fusing two orthogonal anisotropic LR MR images to reconstruct anatomical details in a unified representation. Our multi-view neural network is trained in a self-supervised manner, without requiring corresponding high-resolution (HR) data. To optimize the model, we introduce a sparse coordinate-based loss, enabling the integration of LR images with arbitrary scaling. We evaluate our method on MR images from two independent cohorts. Our results demonstrate comparable or even improved super-resolution (SR) performance compared to state-of-the-art (SOTA) self-supervised SR methods for different upsampling scales. By combining a patient-agnostic offline and a patient-specific online phase, we achieve a substantial speed-up of up to ten times for patient-specific reconstruction while achieving similar or better SR quality. Code is available at https://github.com/MajaSchle/tripleSR.", "AI": {"tldr": "\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06\u4e24\u4e2a\u6b63\u4ea4\u5404\u5f02\u6027\u4f4e\u5206\u8fa8\u7387\u78c1\u5171\u632f\u6210\u50cf\u878d\u5408\uff0c\u91cd\u5efa\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u65e0\u9700\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u8bad\u7ec3\uff0c\u91cd\u5efa\u901f\u5ea6\u63d0\u534710\u500d", "motivation": "\u533b\u5b66MR\u6210\u50cf\u4e2d\uff0c\u9ad8\u5206\u8fa8\u7387\u626b\u63cf\u8017\u65f6\u957f\u4e14\u75c5\u4eba\u4e0d\u8212\u9002\uff0c\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u5206\u6790\u7c97\u7cd6\u6613\u9519\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700HR\u6570\u636e\u7684\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u65b9\u6cd5", "method": "\u63d0\u51fa\u591a\u89c6\u89d2\u795e\u7ecf\u7f51\u7edc\uff0c\u91c7\u7528\u81ea\u76d1\u7763\u65b9\u5f0f\u8bad\u7ec3\uff0c\u4e0d\u9700\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u3002\u5f15\u5165\u7a00\u758f\u5750\u6807\u57fa\u635f\u5931\u51fd\u6570\uff0c\u652f\u6301\u4efb\u610f\u7f29\u653e\u6bd4\u4f8b\u7684LR\u56fe\u50cf\u878d\u5408\u3002\u7ed3\u5408\u75c5\u4eba\u65e0\u5173\u79bb\u7ebf\u548c\u75c5\u4eba\u7279\u5b9a\u5728\u7ebf\u4e24\u4e2a\u9636\u6bb5", "result": "\u5728\u4e24\u4e2a\u72ec\u7acb\u7684MR\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u8d85\u5206\u8fa8\u7387\u6027\u80fd\u4e0e\u73b0\u6709\u81ea\u76d1\u7763SOTA\u65b9\u6cd5\u76f8\u6bd4\u6216\u66f4\u4f18\uff0c\u75c5\u4eba\u7279\u5b9a\u91cd\u5efa\u901f\u5ea6\u63d0\u5347\u8fbe10\u500d\uff0c\u8d28\u91cf\u4fdd\u6301\u6216\u63d0\u5347", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u65e0\u9700\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u878d\u5408\u591a\u4e2a\u4f4e\u5206\u8fa8\u7387\u626b\u63cf\u5feb\u901f\u91cd\u5efa\u9ad8\u8d28\u91cfMR\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u533b\u5b66\u56fe\u50cf\u5904\u7406\u6548\u7387\u548c\u8bca\u65ad\u51c6\u786e\u6027"}}
{"id": "2509.07809", "pdf": "https://arxiv.org/pdf/2509.07809", "abs": "https://arxiv.org/abs/2509.07809", "authors": ["Mahtab Dahaghin", "Milind G. Padalkar", "Matteo Toso", "Alessio Del Bue"], "title": "SplatFill: 3D Scene Inpainting via Depth-Guided Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has enabled the creation of highly realistic 3D scene representations from sets of multi-view images. However, inpainting missing regions, whether due to occlusion or scene editing, remains a challenging task, often leading to blurry details, artifacts, and inconsistent geometry. In this work, we introduce SplatFill, a novel depth-guided approach for 3DGS scene inpainting that achieves state-of-the-art perceptual quality and improved efficiency. Our method combines two key ideas: (1) joint depth-based and object-based supervision to ensure inpainted Gaussians are accurately placed in 3D space and aligned with surrounding geometry, and (2) we propose a consistency-aware refinement scheme that selectively identifies and corrects inconsistent regions without disrupting the rest of the scene. Evaluations on the SPIn-NeRF dataset demonstrate that SplatFill not only surpasses existing NeRF-based and 3DGS-based inpainting methods in visual fidelity but also reduces training time by 24.5%. Qualitative results show our method delivers sharper details, fewer artifacts, and greater coherence across challenging viewpoints.", "AI": {"tldr": "SplatFill\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6df1\u5ea6\u5f15\u5bfc3D\u9ad8\u65af\u6e85\u5c04\u573a\u666f\u4fee\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u6df1\u5ea6\u76d1\u7763\u548c\u4e00\u81f4\u6027\u611f\u77e5\u7ec6\u5316\uff0c\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u6548\u7387\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73", "motivation": "3D\u9ad8\u65af\u6e85\u5c04(3DGS)\u867d\u7136\u80fd\u521b\u5efa\u9ad8\u5ea6\u903c\u771f\u76843D\u573a\u666f\u8868\u793a\uff0c\u4f46\u5728\u4fee\u590d\u56e0\u906e\u6321\u6216\u573a\u666f\u7f16\u8f91\u5bfc\u81f4\u7684\u7f3a\u5931\u533a\u57df\u65f6\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u5e38\u5bfc\u81f4\u6a21\u7cca\u7ec6\u8282\u3001\u4f2a\u5f71\u548c\u51e0\u4f55\u4e0d\u4e00\u81f4", "method": "\u7ed3\u5408\u4e24\u79cd\u5173\u952e\u601d\u60f3\uff1a(1)\u8054\u5408\u6df1\u5ea6\u57fa\u548c\u5bf9\u8c61\u57fa\u76d1\u7763\uff0c\u786e\u4fdd\u4fee\u590d\u7684\u9ad8\u65af\u51c6\u786e\u653e\u7f6e\u57283D\u7a7a\u95f4\u4e2d\u5e76\u4e0e\u5468\u56f4\u51e0\u4f55\u5bf9\u9f50\uff1b(2)\u4e00\u81f4\u6027\u611f\u77e5\u7ec6\u5316\u65b9\u6848\uff0c\u9009\u62e9\u6027\u8bc6\u522b\u548c\u4fee\u6b63\u4e0d\u4e00\u81f4\u533a\u57df\u800c\u4e0d\u7834\u574f\u573a\u666f\u5176\u4ed6\u90e8\u5206", "result": "\u5728SPIn-NeRF\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cSplatFill\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u4e0a\u8d85\u8d8a\u73b0\u6709NeRF\u57fa\u548c3DGS\u57fa\u4fee\u590d\u65b9\u6cd5\uff0c\u540c\u65f6\u51cf\u5c1124.5%\u7684\u8bad\u7ec3\u65f6\u95f4\uff0c\u63d0\u4f9b\u66f4\u6e05\u6670\u7684\u7ec6\u8282\u3001\u66f4\u5c11\u4f2a\u5f71\u548c\u66f4\u597d\u7684\u89c6\u89d2\u4e00\u81f4\u6027", "conclusion": "SplatFill\u901a\u8fc7\u6df1\u5ea6\u5f15\u5bfc\u548c\u76d1\u7763\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e863DGS\u573a\u666f\u4fee\u590d\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u548c\u9ad8\u6548\u7387\u7684\u573a\u666f\u4fee\u590d"}}
{"id": "2509.07920", "pdf": "https://arxiv.org/pdf/2509.07920", "abs": "https://arxiv.org/abs/2509.07920", "authors": ["Ao Li", "Jinpeng Liu", "Yixuan Zhu", "Yansong Tang"], "title": "ScoreHOI: Physically Plausible Reconstruction of Human-Object Interaction via Score-Guided Diffusion", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Joint reconstruction of human-object interaction marks a significant milestone in comprehending the intricate interrelations between humans and their surrounding environment. Nevertheless, previous optimization methods often struggle to achieve physically plausible reconstruction results due to the lack of prior knowledge about human-object interactions. In this paper, we introduce ScoreHOI, an effective diffusion-based optimizer that introduces diffusion priors for the precise recovery of human-object interactions. By harnessing the controllability within score-guided sampling, the diffusion model can reconstruct a conditional distribution of human and object pose given the image observation and object feature. During inference, the ScoreHOI effectively improves the reconstruction results by guiding the denoising process with specific physical constraints. Furthermore, we propose a contact-driven iterative refinement approach to enhance the contact plausibility and improve the reconstruction accuracy. Extensive evaluations on standard benchmarks demonstrate ScoreHOI's superior performance over state-of-the-art methods, highlighting its ability to achieve a precise and robust improvement in joint human-object interaction reconstruction.", "AI": {"tldr": "ScoreHOI\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u5f15\u5165\u6269\u6563\u5148\u9a8c\u548c\u7269\u7406\u7ea6\u675f\u6765\u6539\u8fdb\u4eba-\u7269\u4ea4\u4e92\u7684\u8054\u5408\u91cd\u5efa\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd", "motivation": "\u4ee5\u5f80\u4f18\u5316\u65b9\u6cd5\u7531\u4e8e\u7f3a\u4e4f\u4eba-\u7269\u4ea4\u4e92\u5148\u9a8c\u77e5\u8bc6\uff0c\u96be\u4ee5\u5b9e\u73b0\u7269\u7406\u4e0a\u5408\u7406\u7684\u91cd\u5efa\u7ed3\u679c\uff0c\u9700\u8981\u5f15\u5165\u5148\u9a8c\u77e5\u8bc6\u6765\u63d0\u5347\u91cd\u5efa\u8d28\u91cf", "method": "\u63d0\u51faScoreHOI\u6269\u6563\u4f18\u5316\u5668\uff0c\u5229\u7528\u5206\u6570\u5f15\u5bfc\u91c7\u6837\u7684\u53ef\u63a7\u6027\u91cd\u5efa\u6761\u4ef6\u5206\u5e03\uff1b\u63d0\u51fa\u63a5\u89e6\u9a71\u52a8\u7684\u8fed\u4ee3\u7ec6\u5316\u65b9\u6cd5\u589e\u5f3a\u63a5\u89e6\u5408\u7406\u6027\uff1b\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u7279\u5b9a\u7269\u7406\u7ea6\u675f\u6307\u5bfc", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u548c\u9c81\u68d2\u7684\u8054\u5408\u4eba-\u7269\u4ea4\u4e92\u91cd\u5efa\u6539\u8fdb", "conclusion": "ScoreHOI\u901a\u8fc7\u6269\u6563\u5148\u9a8c\u548c\u7269\u7406\u7ea6\u675f\u6709\u6548\u63d0\u5347\u4e86\u4eba-\u7269\u4ea4\u4e92\u91cd\u5efa\u7684\u51c6\u786e\u6027\u548c\u7269\u7406\u5408\u7406\u6027\uff0c\u4e3a\u7406\u89e3\u4eba-\u73af\u5883\u590d\u6742\u76f8\u4e92\u5173\u7cfb\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u624b\u6bb5"}}
{"id": "2509.07936", "pdf": "https://arxiv.org/pdf/2509.07936", "abs": "https://arxiv.org/abs/2509.07936", "authors": ["Kimiaki Shirahama", "Miki Yanobu", "Kaduki Yamashita", "Miho Ohsaki"], "title": "Feature Space Analysis by Guided Diffusion Model", "categories": ["cs.CV", "eess.IV"], "comment": "19 pages, 13 figures, codes:   https://github.com/KimiakiShirahama/FeatureSpaceAnalysisByGuidedDiffusionModel", "summary": "One of the key issues in Deep Neural Networks (DNNs) is the black-box nature of their internal feature extraction process. Targeting vision-related domains, this paper focuses on analysing the feature space of a DNN by proposing a decoder that can generate images whose features are guaranteed to closely match a user-specified feature. Owing to this guarantee that is missed in past studies, our decoder allows us to evidence which of various attributes in an image are encoded into a feature by the DNN, by generating images whose features are in proximity to that feature. Our decoder is implemented as a guided diffusion model that guides the reverse image generation of a pre-trained diffusion model to minimise the Euclidean distance between the feature of a clean image estimated at each step and the user-specified feature. One practical advantage of our decoder is that it can analyse feature spaces of different DNNs with no additional training and run on a single COTS GPU. The experimental results targeting CLIP's image encoder, ResNet-50 and vision transformer demonstrate that images generated by our decoder have features remarkably similar to the user-specified ones and reveal valuable insights into these DNNs' feature spaces.", "AI": {"tldr": "\u901a\u8fc7\u5bfc\u5411\u6e29\u5ea6\u6a21\u578b\u5b9e\u73b0\u7684\u89e3\u7801\u5668\uff0c\u751f\u6210\u4e0e\u6307\u5b9a\u7279\u5f81\u5bc6\u5207\u5339\u914d\u7684\u56fe\u50cf\uff0c\u7528\u4e8e\u89e3\u6790DNN\u7279\u5f81\u7a7a\u95f4\u7684\u9ed1\u76d2\u6027\u8d28", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b\u7684\u9ed1\u76d2\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u751f\u6210\u7279\u5f81\u76f8\u4f3c\u56fe\u50cf\u6765\u63ed\u793aDNN\u7f16\u7801\u7684\u56fe\u50cf\u5c5e\u6027", "method": "\u4f7f\u7528\u5bfc\u5411\u6e29\u5ea6\u6a21\u578b\u5b9e\u73b0\u89e3\u7801\u5668\uff0c\u5728\u9884\u8bad\u7ec3\u6e29\u5ea6\u6a21\u578b\u7684\u53cd\u5411\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u4e2d\u6307\u5bfc\uff0c\u6700\u5c0f\u5316\u6e05\u6d01\u56fe\u50cf\u4f30\u8ba1\u7279\u5f81\u4e0e\u7528\u6237\u6307\u5b9a\u7279\u5f81\u7684\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb", "result": "\u5728CLIP\u56fe\u50cf\u7f16\u7801\u5668\u3001ResNet-50\u548c\u89c6\u89c9Transformer\u4e0a\u9a8c\u8bc1\uff0c\u751f\u6210\u56fe\u50cf\u7684\u7279\u5f81\u4e0e\u6307\u5b9a\u7279\u5f81\u9ad8\u5ea6\u76f8\u4f3c\uff0c\u63ed\u793a\u4e86\u8fd9\u4e9bDNN\u7279\u5f81\u7a7a\u95f4\u7684\u4ef7\u503c\u89c1\u89e3", "conclusion": "\u8be5\u89e3\u7801\u5668\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5206\u6790\u4e0d\u540cDNN\u7279\u5f81\u7a7a\u95f4\uff0c\u5728\u5355\u5361GPU\u4e0a\u8fd0\u884c\uff0c\u4e3a\u89e3\u91caDNN\u9ed1\u76d2\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5"}}
{"id": "2509.07463", "pdf": "https://arxiv.org/pdf/2509.07463", "abs": "https://arxiv.org/abs/2509.07463", "authors": ["Sven Kirchner", "Nils Purschke", "Ross Greer", "Alois C. Knoll"], "title": "DepthVision: Robust Vision-Language Understanding through GAN-Based LiDAR-to-RGB Synthesis", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Ensuring reliable robot operation when visual input is degraded or insufficient remains a central challenge in robotics. This letter introduces DepthVision, a framework for multimodal scene understanding designed to address this problem. Unlike existing Vision-Language Models (VLMs), which use only camera-based visual input alongside language, DepthVision synthesizes RGB images from sparse LiDAR point clouds using a conditional generative adversarial network (GAN) with an integrated refiner network. These synthetic views are then combined with real RGB data using a Luminance-Aware Modality Adaptation (LAMA), which blends the two types of data dynamically based on ambient lighting conditions. This approach compensates for sensor degradation, such as darkness or motion blur, without requiring any fine-tuning of downstream vision-language models. We evaluate DepthVision on real and simulated datasets across various models and tasks, with particular attention to safety-critical tasks. The results demonstrate that our approach improves performance in low-light conditions, achieving substantial gains over RGB-only baselines while preserving compatibility with frozen VLMs. This work highlights the potential of LiDAR-guided RGB synthesis for achieving robust robot operation in real-world environments.", "AI": {"tldr": "DepthVision\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u573a\u666f\u7406\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u6761\u4ef6GAN\u4ece\u7a00\u758fLiDAR\u70b9\u4e91\u5408\u6210RGB\u56fe\u50cf\uff0c\u5e76\u7ed3\u5408\u771f\u5b9eRGB\u6570\u636e\uff0c\u5728\u5149\u7167\u6761\u4ef6\u4e0d\u4f73\u65f6\u63d0\u5347\u673a\u5668\u4eba\u89c6\u89c9\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u8f93\u5165\u9000\u5316\u6216\u4e0d\u8db3\u65f6\u673a\u5668\u4eba\u53ef\u9760\u64cd\u4f5c\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u9ed1\u6697\u3001\u8fd0\u52a8\u6a21\u7cca\u7b49\u4f20\u611f\u5668\u9000\u5316\u60c5\u51b5\u4e0b\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u4ece\u7a00\u758fLiDAR\u70b9\u4e91\u5408\u6210RGB\u56fe\u50cf\uff0c\u901a\u8fc7Luminance-Aware Modality Adaptation\uff08LAMA\uff09\u6839\u636e\u73af\u5883\u5149\u7167\u6761\u4ef6\u52a8\u6001\u878d\u5408\u5408\u6210\u56fe\u50cf\u548c\u771f\u5b9eRGB\u6570\u636e\u3002", "result": "\u5728\u771f\u5b9e\u548c\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4e\u5149\u7167\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528RGB\u7684\u57fa\u7ebf\u6709\u5927\u5e45\u6539\u8fdb\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u51bb\u7ed3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u517c\u5bb9\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86LiDAR\u5f15\u5bfc\u7684RGB\u5408\u6210\u5728\u5b9e\u73b0\u771f\u5b9e\u73af\u5883\u4e2d\u9c81\u68d2\u673a\u5668\u4eba\u64cd\u4f5c\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u65e0\u9700\u5bf9\u4e0b\u6e38\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002"}}
