<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 21]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.LG](#cs.LG) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering](https://arxiv.org/abs/2506.13814)
*Lufei Liu,Tor M. Aamodt*

Main category: cs.GR

TL;DR: ReFrame通过缓存中间特征优化实时渲染任务，在质量损失可忽略的情况下平均提速1.4倍。


<details>
  <summary>Details</summary>
Motivation: 利用渲染任务中的时间连贯性，避免冗余计算以提高效率。

Method: 扩展缓存中间特征的方法至实时渲染，探索不同缓存策略以平衡质量与性能。

Result: 在三种实时渲染任务中，平均提速1.4倍且质量损失可忽略。

Conclusion: ReFrame适用于多种编码器-解码器网络，有效优化渲染性能。

Abstract: Graphics rendering applications increasingly leverage neural networks in tasks such as denoising, supersampling, and frame extrapolation to improve image quality while maintaining frame rates. The temporal coherence inherent in these tasks presents an opportunity to reuse intermediate results from previous frames and avoid redundant computations. Recent work has shown that caching intermediate features to be reused in subsequent inferences is an effective method to reduce latency in diffusion models. We extend this idea to real-time rendering and present ReFrame, which explores different caching policies to optimize trade-offs between quality and performance in rendering workloads. ReFrame can be applied to a variety of encoder-decoder style networks commonly found in rendering pipelines. Experimental results show that we achieve 1.4x speedup on average with negligible quality loss in three real-time rendering tasks. Code available: https://ubc-aamodt-group.github.io/reframe-layer-caching/

</details>


### [2] [ImmerseGen: Agent-Guided Immersive World Generation with Alpha-Textured Proxies](https://arxiv.org/abs/2506.14315)
*Jinyan Yuan,Bangbang Yang,Keke Wang,Panwang Pan,Lin Ma,Xuehai Zhang,Xiao Liu,Zhaopeng Cui,Yuewen Ma*

Main category: cs.GR

TL;DR: ImmerseGen是一种新型的代理引导框架，用于紧凑且逼真的3D场景建模，通过轻量级几何代理和RGBA纹理合成，简化了传统复杂建模流程。


<details>
  <summary>Details</summary>
Motivation: 现有的3D场景建模方法通常依赖高多边形网格或大量3D高斯分布，导致流程复杂或视觉效果受限。本文旨在证明无需复杂建模即可实现沉浸式体验。

Method: ImmerseGen通过分层轻量级几何代理（如简化地形和广告牌网格）表示场景，并合成RGBA纹理。提出地形条件纹理和RGBA资产纹理技术，结合VLM代理增强语义网格分析。

Result: 实验表明，ImmerseGen在逼真度、空间一致性和渲染效率上优于现有方法，适用于移动VR设备的实时渲染。

Conclusion: ImmerseGen通过简化建模流程和直接纹理合成，实现了高效且逼真的3D场景生成，为沉浸式VR体验提供了新思路。

Abstract: Automatic creation of 3D scenes for immersive VR presence has been a significant research focus for decades. However, existing methods often rely on either high-poly mesh modeling with post-hoc simplification or massive 3D Gaussians, resulting in a complex pipeline or limited visual realism. In this paper, we demonstrate that such exhaustive modeling is unnecessary for achieving compelling immersive experience. We introduce ImmerseGen, a novel agent-guided framework for compact and photorealistic world modeling. ImmerseGen represents scenes as hierarchical compositions of lightweight geometric proxies, i.e., simplified terrain and billboard meshes, and generates photorealistic appearance by synthesizing RGBA textures onto these proxies. Specifically, we propose terrain-conditioned texturing for user-centric base world synthesis, and RGBA asset texturing for midground and foreground scenery.This reformulation offers several advantages: (i) it simplifies modeling by enabling agents to guide generative models in producing coherent textures that integrate seamlessly with the scene; (ii) it bypasses complex geometry creation and decimation by directly synthesizing photorealistic textures on proxies, preserving visual quality without degradation; (iii) it enables compact representations suitable for real-time rendering on mobile VR headsets. To automate scene creation from text prompts, we introduce VLM-based modeling agents enhanced with semantic grid-based analysis for improved spatial reasoning and accurate asset placement. ImmerseGen further enriches scenes with dynamic effects and ambient audio to support multisensory immersion. Experiments on scene generation and live VR showcases demonstrate that ImmerseGen achieves superior photorealism, spatial coherence and rendering efficiency compared to prior methods. Project webpage: https://immersegen.github.io.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [3] [CDST: Color Disentangled Style Transfer for Universal Style Reference Customization](https://arxiv.org/abs/2506.13770)
*Shiwen Zhang,Zhuowei Chen,Lang Chen,Yanze Wu*

Main category: cs.CV

TL;DR: CDST是一种新颖的双流风格迁移训练范式，通过完全分离颜色与风格，实现无需调优的通用风格迁移。


<details>
  <summary>Details</summary>
Motivation: 解决传统风格迁移中颜色与风格混淆的问题，并首次实现无需调优的特征保留风格迁移。

Method: 采用双流训练范式，通过多特征图像嵌入压缩和基于Diffusion UNet解耦定律的新风格定义。

Result: 在多种风格迁移任务中取得最先进的结果，并通过实验和人工评估验证其优越性。

Conclusion: CDST为风格迁移领域提供了高效、通用的解决方案，具有显著的应用潜力。

Abstract: We introduce Color Disentangled Style Transfer (CDST), a novel and efficient two-stream style transfer training paradigm which completely isolates color from style and forces the style stream to be color-blinded. With one same model, CDST unlocks universal style transfer capabilities in a tuning-free manner during inference. Especially, the characteristics-preserved style transfer with style and content references is solved in the tuning-free way for the first time. CDST significantly improves the style similarity by multi-feature image embeddings compression and preserves strong editing capability via our new CDST style definition inspired by Diffusion UNet disentanglement law. By conducting thorough qualitative and quantitative experiments and human evaluations, we demonstrate that CDST achieves state-of-the-art results on various style transfer tasks.

</details>


### [4] [Hidden Bias in the Machine: Stereotypes in Text-to-Image Models](https://arxiv.org/abs/2506.13780)
*Sedat Porikli,Vedat Porikli*

Main category: cs.CV

TL;DR: 研究探讨了文本到图像（T2I）模型如何复制和放大社会偏见，通过生成多样化的提示和图像分析揭示了性别、种族等方面的显著差异。


<details>
  <summary>Details</summary>
Motivation: 调查T2I模型在生成图像时是否复制和放大社会偏见，以促进更公平的视觉生成系统。

Method: 使用Stable Diffusion 1.5和Flux-1模型生成16,000多张图像，并收集8,000张Google图像进行比较，分析人类中心因素的差异。

Result: 发现生成的图像在性别、种族等方面存在显著差异，并强化了社会叙事中的有害刻板印象。

Conclusion: 强调需要更包容的数据集和开发实践，以减少生成视觉系统中的偏见。

Abstract: Text-to-Image (T2I) models have transformed visual content creation, producing highly realistic images from natural language prompts. However, concerns persist around their potential to replicate and magnify existing societal biases. To investigate these issues, we curated a diverse set of prompts spanning thematic categories such as occupations, traits, actions, ideologies, emotions, family roles, place descriptions, spirituality, and life events. For each of the 160 unique topics, we crafted multiple prompt variations to reflect a wide range of meanings and perspectives. Using Stable Diffusion 1.5 (UNet-based) and Flux-1 (DiT-based) models with original checkpoints, we generated over 16,000 images under consistent settings. Additionally, we collected 8,000 comparison images from Google Image Search. All outputs were filtered to exclude abstract, distorted, or nonsensical results. Our analysis reveals significant disparities in the representation of gender, race, age, somatotype, and other human-centric factors across generated images. These disparities often mirror and reinforce harmful stereotypes embedded in societal narratives. We discuss the implications of these findings and emphasize the need for more inclusive datasets and development practices to foster fairness in generative visual systems.

</details>


### [5] [Fake it till You Make it: Reward Modeling as Discriminative Prediction](https://arxiv.org/abs/2506.13846)
*Runtao Liu,Jiahao Zhan,Yingqing He,Chen Wei,Alan Yuille,Qifeng Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为GAN-RM的高效奖励建模框架，通过对抗训练避免人工标注和显式质量维度设计，仅需少量目标样本即可实现有效的奖励建模。


<details>
  <summary>Details</summary>
Motivation: 当前奖励建模方法依赖大量人工标注或复杂质量维度设计，实现复杂且不完整。本文旨在简化这一过程。

Method: 利用GAN的对抗训练思想，通过区分少量目标样本与模型生成样本，训练奖励模型，无需人工标注或显式质量维度。

Result: 实验证明GAN-RM在多种应用中有效，包括测试时缩放和训练后优化方法。

Conclusion: GAN-RM提供了一种高效且无需复杂人工干预的奖励建模方法，适用于多种视觉生成模型增强任务。

Abstract: An effective reward model plays a pivotal role in reinforcement learning for post-training enhancement of visual generative models. However, current approaches of reward modeling suffer from implementation complexity due to their reliance on extensive human-annotated preference data or meticulously engineered quality dimensions that are often incomplete and engineering-intensive. Inspired by adversarial training in generative adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward modeling framework that eliminates manual preference annotation and explicit quality dimension engineering. Our method trains the reward model through discrimination between a small set of representative, unpaired target samples(denoted as Preference Proxy Data) and model-generated ordinary outputs, requiring only a few hundred target samples. Comprehensive experiments demonstrate our GAN-RM's effectiveness across multiple key applications including test-time scaling implemented as Best-of-N sample filtering, post-training approaches like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO).

</details>


### [6] [FADPNet: Frequency-Aware Dual-Path Network for Face Super-Resolution](https://arxiv.org/abs/2506.14121)
*Siyu Xu,Wenjie Li,Guangwei Gao,Jian Yang,Guo-Jun Qi,Chia-Wen Lin*

Main category: cs.CV

TL;DR: FADPNet是一种频率感知双路径网络，通过分离处理低频和高频面部特征，优化计算资源分配，提升面部超分辨率性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法对所有面部像素一视同仁，导致计算资源分配不优和性能下降。CNN对高频特征敏感，Mamba擅长低频特征且复杂度低，因此结合两者优势。

Method: 提出FADPNet，分解特征为低频和高频，分别用Mamba和CNN处理。低频用LFEB模块，高频用DPA和HFR模块。

Result: 方法在超分辨率质量和模型效率间取得平衡，优于现有方法。

Conclusion: FADPNet通过频率感知设计，显著提升了面部超分辨率的性能和效率。

Abstract: Face super-resolution (FSR) under limited computational costs remains an open problem. Existing approaches typically treat all facial pixels equally, resulting in suboptimal allocation of computational resources and degraded FSR performance. CNN is relatively sensitive to high-frequency facial features, such as component contours and facial outlines. Meanwhile, Mamba excels at capturing low-frequency features like facial color and fine-grained texture, and does so with lower complexity than Transformers. Motivated by these observations, we propose FADPNet, a Frequency-Aware Dual-Path Network that decomposes facial features into low- and high-frequency components and processes them via dedicated branches. For low-frequency regions, we introduce a Mamba-based Low-Frequency Enhancement Block (LFEB), which combines state-space attention with squeeze-and-excitation operations to extract low-frequency global interactions and emphasize informative channels. For high-frequency regions, we design a CNN-based Deep Position-Aware Attention (DPA) module to enhance spatially-dependent structural details, complemented by a lightweight High-Frequency Refinement (HFR) module that further refines frequency-specific representations. Through the above designs, our method achieves an excellent balance between FSR quality and model efficiency, outperforming existing approaches.

</details>


### [7] [KDMOS:Knowledge Distillation for Motion Segmentation](https://arxiv.org/abs/2506.14130)
*Chunyu Cao,Jintao Cheng,Zeyu Chen,Linfan Zhan,Rui Fan,Zhijian He,Xiaoyu Tang*

Main category: cs.CV

TL;DR: 提出了一种基于logits的知识蒸馏框架（KDMOS），用于运动目标分割（MOS），通过BEV投影模型（学生）和非投影模型（教师）的结合，提升精度并保持实时性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在精度和实时性之间难以平衡，且运动与非运动类别严重不平衡，需改进。

Method: 采用BEV投影模型（学生）和非投影模型（教师），通过解耦运动与非运动类别并应用定制蒸馏策略，优化网络架构并减少参数。

Result: 在SemanticKITTI-MOS隐藏测试集上达到78.8%的IoU，并在Apollo数据集上表现优异。

Conclusion: KDMOS框架有效提升了MOS任务的精度和效率，解决了类别不平衡问题，并减少了过拟合。

Abstract: Motion Object Segmentation (MOS) is crucial for autonomous driving, as it enhances localization, path planning, map construction, scene flow estimation, and future state prediction. While existing methods achieve strong performance, balancing accuracy and real-time inference remains a challenge. To address this, we propose a logits-based knowledge distillation framework for MOS, aiming to improve accuracy while maintaining real-time efficiency. Specifically, we adopt a Bird's Eye View (BEV) projection-based model as the student and a non-projection model as the teacher. To handle the severe imbalance between moving and non-moving classes, we decouple them and apply tailored distillation strategies, allowing the teacher model to better learn key motion-related features. This approach significantly reduces false positives and false negatives. Additionally, we introduce dynamic upsampling, optimize the network architecture, and achieve a 7.69% reduction in parameter count, mitigating overfitting. Our method achieves a notable IoU of 78.8% on the hidden test set of the SemanticKITTI-MOS dataset and delivers competitive results on the Apollo dataset. The KDMOS implementation is available at https://github.com/SCNU-RISLAB/KDMOS.

</details>


### [8] [VideoMAR: Autoregressive Video Generatio with Continuous Tokens](https://arxiv.org/abs/2506.14168)
*Hu Yu,Biao Gong,Hangjie Yuan,DanDan Zheng,Weilong Chai,Jingdong Chen,Kecheng Zheng,Feng Zhao*

Main category: cs.CV

TL;DR: VideoMAR是一种高效的解码器自回归图像到视频模型，结合了时间帧生成和空间掩码生成，显著减少了参数、训练数据和GPU资源需求。


<details>
  <summary>Details</summary>
Motivation: 探索自回归模型在视频生成中的潜力，解决长序列建模的高成本和难度问题。

Method: 提出时间因果性和空间双向性作为视频自回归模型的基本原则，采用下一帧扩散损失、时间短到长课程学习和空间渐进分辨率训练。

Result: 在VBench-I2V基准测试中，VideoMAR超越先前最佳模型（Cosmos I2V），参数、训练数据和GPU资源需求大幅降低。

Conclusion: VideoMAR展示了自回归模型在视频生成中的高效性和潜力，同时具备时空外推能力。

Abstract: Masked-based autoregressive models have demonstrated promising image generation capability in continuous space. However, their potential for video generation remains under-explored. In this paper, we propose \textbf{VideoMAR}, a concise and efficient decoder-only autoregressive image-to-video model with continuous tokens, composing temporal frame-by-frame and spatial masked generation. We first identify temporal causality and spatial bi-directionality as the first principle of video AR models, and propose the next-frame diffusion loss for the integration of mask and video generation. Besides, the huge cost and difficulty of long sequence autoregressive modeling is a basic but crucial issue. To this end, we propose the temporal short-to-long curriculum learning and spatial progressive resolution training, and employ progressive temperature strategy at inference time to mitigate the accumulation error. Furthermore, VideoMAR replicates several unique capacities of language models to video generation. It inherently bears high efficiency due to simultaneous temporal-wise KV cache and spatial-wise parallel generation, and presents the capacity of spatial and temporal extrapolation via 3D rotary embeddings. On the VBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos I2V) while requiring significantly fewer parameters ($9.3\%$), training data ($0.5\%$), and GPU resources ($0.2\%$).

</details>


### [9] [Meta-SurDiff: Classification Diffusion Model Optimized by Meta Learning is Reliable for Online Surgical Phase Recognition](https://arxiv.org/abs/2506.14181)
*Yufei Li,Jirui Wu,Long Tian,Liming Wang,Xiaonan Liu,Zijun Liu,Xiyang Liu*

Main category: cs.CV

TL;DR: 论文提出了一种基于元学习优化的分类扩散模型（Meta-SurDiff），用于解决手术视频中的不确定性，以实现可靠的在线手术阶段识别。


<details>
  <summary>Details</summary>
Motivation: 手术视频中存在不可避免的不确定性（如帧模糊和阶段分布不平衡），现有深度模型未充分建模这些不确定性，影响识别的可靠性。

Method: 使用分类扩散模型评估模糊帧的置信度，并通过元学习优化扩散模型以增强分类边界对不同手术阶段的鲁棒性。

Result: 在五个广泛使用的数据集（Cholec80、AutoLaparo、M2Cai16、OphNet、NurViD）上验证了Meta-SurDiff的有效性。

Conclusion: Meta-SurDiff通过建模不确定性，显著提升了在线手术阶段识别的可靠性。

Abstract: Online surgical phase recognition has drawn great attention most recently due to its potential downstream applications closely related to human life and health. Despite deep models have made significant advances in capturing the discriminative long-term dependency of surgical videos to achieve improved recognition, they rarely account for exploring and modeling the uncertainty in surgical videos, which should be crucial for reliable online surgical phase recognition. We categorize the sources of uncertainty into two types, frame ambiguity in videos and unbalanced distribution among surgical phases, which are inevitable in surgical videos. To address this pivot issue, we introduce a meta-learning-optimized classification diffusion model (Meta-SurDiff), to take full advantage of the deep generative model and meta-learning in achieving precise frame-level distribution estimation for reliable online surgical phase recognition. For coarse recognition caused by ambiguous video frames, we employ a classification diffusion model to assess the confidence of recognition results at a finer-grained frame-level instance. For coarse recognition caused by unbalanced phase distribution, we use a meta-learning based objective to learn the diffusion model, thus enhancing the robustness of classification boundaries for different surgical phases.We establish effectiveness of Meta-SurDiff in online surgical phase recognition through extensive experiments on five widely used datasets using more than four practical metrics. The datasets include Cholec80, AutoLaparo, M2Cai16, OphNet, and NurViD, where OphNet comes from ophthalmic surgeries, NurViD is the daily care dataset, while the others come from laparoscopic surgeries. We will release the code upon acceptance.

</details>


### [10] [HRGS: Hierarchical Gaussian Splatting for Memory-Efficient High-Resolution 3D Reconstruction](https://arxiv.org/abs/2506.14229)
*Changbai Li,Haodong Zhu,Hanlin Chen,Juan Zhang,Tongfei Chen,Shuo Yang,Shuwei Shao,Wenhao Dong,Baochang Zhang*

Main category: cs.CV

TL;DR: HRGS提出了一种内存高效的分层高斯溅射框架，通过块级优化和重要性驱动的高斯修剪，解决了3DGS在高分辨率场景中的内存扩展问题。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射（3DGS）在实时3D场景重建中取得了显著进展，但在高分辨率场景下面临内存扩展问题。

Method: HRGS首先生成一个全局的粗略高斯表示，然后将场景划分为多个块，并用高分辨率数据细化每个块。通过高斯分区和训练数据分区优化任务分配，并引入重要性驱动的高斯修剪（IDGP）减少计算需求。

Result: 在三个基准测试中，HRGS在高分辨率新视角合成（NVS）和表面重建任务中实现了最先进的性能。

Conclusion: HRGS能够在内存限制下实现高质量、高分辨率的3D场景重建。

Abstract: 3D Gaussian Splatting (3DGS) has made significant strides in real-time 3D scene reconstruction, but faces memory scalability issues in high-resolution scenarios. To address this, we propose Hierarchical Gaussian Splatting (HRGS), a memory-efficient framework with hierarchical block-level optimization. First, we generate a global, coarse Gaussian representation from low-resolution data. Then, we partition the scene into multiple blocks, refining each block with high-resolution data. The partitioning involves two steps: Gaussian partitioning, where irregular scenes are normalized into a bounded cubic space with a uniform grid for task distribution, and training data partitioning, where only relevant observations are retained for each block. By guiding block refinement with the coarse Gaussian prior, we ensure seamless Gaussian fusion across adjacent blocks. To reduce computational demands, we introduce Importance-Driven Gaussian Pruning (IDGP), which computes importance scores for each Gaussian and removes those with minimal contribution, speeding up convergence and reducing memory usage. Additionally, we incorporate normal priors from a pretrained model to enhance surface reconstruction quality. Our method enables high-quality, high-resolution 3D scene reconstruction even under memory constraints. Extensive experiments on three benchmarks show that HRGS achieves state-of-the-art performance in high-resolution novel view synthesis (NVS) and surface reconstruction tasks.

</details>


### [11] [FRIDU: Functional Map Refinement with Guided Image Diffusion](https://arxiv.org/abs/2506.14322)
*Avigail Cohen Rimon,Mirela Ben-Chen,Or Litany*

Main category: cs.CV

TL;DR: 提出了一种基于图像扩散模型的功能映射优化方法，通过训练模型直接生成准确的功能映射，并在推理时利用点映射作为指导。


<details>
  <summary>Details</summary>
Motivation: 现有功能映射优化方法效率不高，且难以满足正交性和与拉普拉斯-贝尔特拉米算子的交换性等目标。

Method: 将功能映射视为2D图像，训练图像扩散模型直接在功能空间生成准确映射，推理时利用点映射作为指导。

Result: 方法在功能映射优化任务中与现有最优方法竞争，展示了扩散模型在功能映射处理中的潜力。

Conclusion: 基于扩散模型的功能映射优化方法高效且灵活，为功能映射处理提供了新途径。

Abstract: We propose a novel approach for refining a given correspondence map between two shapes. A correspondence map represented as a functional map, namely a change of basis matrix, can be additionally treated as a 2D image. With this perspective, we train an image diffusion model directly in the space of functional maps, enabling it to generate accurate maps conditioned on an inaccurate initial map. The training is done purely in the functional space, and thus is highly efficient. At inference time, we use the pointwise map corresponding to the current functional map as guidance during the diffusion process. The guidance can additionally encourage different functional map objectives, such as orthogonality and commutativity with the Laplace-Beltrami operator. We show that our approach is competitive with state-of-the-art methods of map refinement and that guided diffusion models provide a promising pathway to functional map processing.

</details>


### [12] [Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models](https://arxiv.org/abs/2506.14399)
*Tian Xia,Fabio De Sousa Ribeiro,Rajat R Rasal,Avinash Kori,Raghav Mehta,Ben Glocker*

Main category: cs.CV

TL;DR: 论文提出了一种解耦的无分类器引导（DCFG）方法，用于改进反事实图像生成中的属性控制和干预保真度。


<details>
  <summary>Details</summary>
Motivation: 标准无分类器引导（CFG）在全局权重下可能导致身份保留不佳和虚假属性变化（属性放大现象），因此需要更灵活的控制方法。

Method: 提出DCFG框架，通过属性分割嵌入策略解耦语义输入，对用户定义的属性组进行选择性引导。基于因果图将属性分为干预组和不变组，并分别应用不同的引导。

Result: 在CelebA-HQ、MIMIC-CXR和EMBED数据集上的实验表明，DCFG提高了干预保真度，减少了意外变化，并增强了可逆性。

Conclusion: DCFG能够实现更忠实和可解释的反事实图像生成。

Abstract: Counterfactual image generation aims to simulate realistic visual outcomes under specific causal interventions. Diffusion models have recently emerged as a powerful tool for this task, combining DDIM inversion with conditional generation via classifier-free guidance (CFG). However, standard CFG applies a single global weight across all conditioning variables, which can lead to poor identity preservation and spurious attribute changes - a phenomenon known as attribute amplification. To address this, we propose Decoupled Classifier-Free Guidance (DCFG), a flexible and model-agnostic framework that introduces group-wise conditioning control. DCFG builds on an attribute-split embedding strategy that disentangles semantic inputs, enabling selective guidance on user-defined attribute groups. For counterfactual generation, we partition attributes into intervened and invariant sets based on a causal graph and apply distinct guidance to each. Experiments on CelebA-HQ, MIMIC-CXR, and EMBED show that DCFG improves intervention fidelity, mitigates unintended changes, and enhances reversibility, enabling more faithful and interpretable counterfactual image generation.

</details>


### [13] [Causally Steered Diffusion for Automated Video Counterfactual Generation](https://arxiv.org/abs/2506.14404)
*Nikos Spyrou,Athanasios Vlontzos,Paraskevas Pegios,Thomas Melistas,Nefeli Gkouti,Yannis Panagakis,Giorgos Papanastasiou,Sotirios A. Tsaftaris*

Main category: cs.CV

TL;DR: 提出了一种基于因果关系的视频编辑框架，利用视觉语言模型（VLM）生成忠实于因果关系的反事实视频，无需修改底层编辑系统。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像（T2I）扩散模型在视频编辑中难以保持因果关系，可能导致不真实或误导性结果。

Method: 通过优化基于因果图的文本提示，引导生成过程，不依赖编辑系统的内部机制或微调。

Result: 实验表明，该方法能有效生成忠实于因果关系的反事实视频，并通过标准视频质量指标和因果有效性等标准验证。

Conclusion: 该方法兼容任何黑盒视频编辑系统，在医疗和数字媒体等领域具有广泛应用潜力。

Abstract: Adapting text-to-image (T2I) latent diffusion models for video editing has shown strong visual fidelity and controllability, but challenges remain in maintaining causal relationships in video content. Edits affecting causally dependent attributes risk generating unrealistic or misleading outcomes if these relationships are ignored. In this work, we propose a causally faithful framework for counterfactual video generation, guided by a vision-language model (VLM). Our method is agnostic to the underlying video editing system and does not require access to its internal mechanisms or finetuning. Instead, we guide the generation by optimizing text prompts based on an assumed causal graph, addressing the challenge of latent space control in LDMs. We evaluate our approach using standard video quality metrics and counterfactual-specific criteria, such as causal effectiveness and minimality. Our results demonstrate that causally faithful video counterfactuals can be effectively generated within the learned distribution of LDMs through prompt-based causal steering. With its compatibility with any black-box video editing system, our method holds significant potential for generating realistic "what-if" video scenarios in diverse areas such as healthcare and digital media.

</details>


### [14] [Exploring Diffusion with Test-Time Training on Efficient Image Restoration](https://arxiv.org/abs/2506.14541)
*Rongchang Lu,Tianduo Luo,Yunzhi Zhang,Conghan Yue,Pei Yang,Guibao Liu,Changyang Gu*

Main category: cs.CV

TL;DR: DiffRWKVIR是一种结合测试时训练和高效扩散的新框架，解决了图像恢复中的特征融合、计算瓶颈和扩散效率问题，通过三项创新在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 图像恢复中存在特征融合不高效、计算瓶颈和扩散过程效率低的问题，需要一种新的方法来解决这些挑战。

Method: 提出DiffRWKVIR框架，包含三项创新：Omni-Scale 2D State Evolution实现全局上下文感知；Chunk-Optimized Flash Processing加速并行处理；Prior-Guided Efficient Diffusion提取紧凑图像先验表示。

Result: 在超分辨率和修复任务中，DiffRWKVIR在PSNR、SSIM、LPIPS和效率指标上优于SwinIR、HAT和MambaIR/v2。

Conclusion: DiffRWKVIR为高效自适应的图像恢复提供了新范式，优化了硬件利用率。

Abstract: Image restoration faces challenges including ineffective feature fusion, computational bottlenecks and inefficient diffusion processes. To address these, we propose DiffRWKVIR, a novel framework unifying Test-Time Training (TTT) with efficient diffusion. Our approach introduces three key innovations: (1) Omni-Scale 2D State Evolution extends RWKV's location-dependent parameterization to hierarchical multi-directional 2D scanning, enabling global contextual awareness with linear complexity O(L); (2) Chunk-Optimized Flash Processing accelerates intra-chunk parallelism by 3.2x via contiguous chunk processing (O(LCd) complexity), reducing sequential dependencies and computational overhead; (3) Prior-Guided Efficient Diffusion extracts a compact Image Prior Representation (IPR) in only 5-20 steps, proving 45% faster training/inference than DiffIR while solving computational inefficiency in denoising. Evaluated across super-resolution and inpainting benchmarks (Set5, Set14, BSD100, Urban100, Places365), DiffRWKVIR outperforms SwinIR, HAT, and MambaIR/v2 in PSNR, SSIM, LPIPS, and efficiency metrics. Our method establishes a new paradigm for adaptive, high-efficiency image restoration with optimized hardware utilization.

</details>


### [15] [DreamLight: Towards Harmonious and Consistent Image Relighting](https://arxiv.org/abs/2506.14549)
*Yong Liu,Wenpeng Xiao,Qianqian Wang,Junlin Chen,Shiyin Wang,Yitong Wang,Xinglong Wu,Yansong Tang*

Main category: cs.CV

TL;DR: DreamLight是一种通用的图像重光照模型，支持基于图像和文本的重光照，通过统一输入格式和预训练扩散模型的语义先验生成自然效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注基于图像的重光照，且依赖复杂的环境映射或像素级转换，难以实现前景与背景的自然光照交互。

Method: 采用统一输入格式，利用预训练扩散模型的语义先验，提出位置引导光适配器（PGLA）和频谱前景修复器（SFF）模块。

Result: 实验和用户研究表明，DreamLight在重光照任务中表现出色。

Conclusion: DreamLight通过创新的模块设计和数据重组，显著提升了图像重光照的自然性和一致性。

Abstract: We introduce a model named DreamLight for universal image relighting in this work, which can seamlessly composite subjects into a new background while maintaining aesthetic uniformity in terms of lighting and color tone. The background can be specified by natural images (image-based relighting) or generated from unlimited text prompts (text-based relighting). Existing studies primarily focus on image-based relighting, while with scant exploration into text-based scenarios. Some works employ intricate disentanglement pipeline designs relying on environment maps to provide relevant information, which grapples with the expensive data cost required for intrinsic decomposition and light source. Other methods take this task as an image translation problem and perform pixel-level transformation with autoencoder architecture. While these methods have achieved decent harmonization effects, they struggle to generate realistic and natural light interaction effects between the foreground and background. To alleviate these challenges, we reorganize the input data into a unified format and leverage the semantic prior provided by the pretrained diffusion model to facilitate the generation of natural results. Moreover, we propose a Position-Guided Light Adapter (PGLA) that condenses light information from different directions in the background into designed light query embeddings, and modulates the foreground with direction-biased masked attention. In addition, we present a post-processing module named Spectral Foreground Fixer (SFF) to adaptively reorganize different frequency components of subject and relighted background, which helps enhance the consistency of foreground appearance. Extensive comparisons and user study demonstrate that our DreamLight achieves remarkable relighting performance.

</details>


### [16] [Risk Estimation of Knee Osteoarthritis Progression via Predictive Multi-task Modelling from Efficient Diffusion Model using X-ray Images](https://arxiv.org/abs/2506.14560)
*David Butler,Adrian Hilton,Gustavo Carneiro*

Main category: cs.CV

TL;DR: 提出了一种可解释的机器学习方法，通过多任务预测模型生成高质量未来图像，用于膝关节骨关节炎（OA）进展风险评估，同时定位解剖标志点，提升了预测性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在膝关节OA风险评估中缺乏可解释性，且生成未来图像的复杂性高，限制了临床采用。

Method: 采用扩散模型在类别条件潜在空间中生成未来图像，结合多任务预测模型分类未来OA严重程度并预测解剖标志点。

Result: 在Osteoarthritis Initiative数据集上，AUC提升2%至0.71，推理时间缩短约9%。

Conclusion: 该方法在提升预测性能的同时，提供了更高的可解释性和效率，有望推动临床采用。

Abstract: Medical imaging plays a crucial role in assessing knee osteoarthritis (OA) risk by enabling early detection and disease monitoring. Recent machine learning methods have improved risk estimation (i.e., predicting the likelihood of disease progression) and predictive modelling (i.e., the forecasting of future outcomes based on current data) using medical images, but clinical adoption remains limited due to their lack of interpretability. Existing approaches that generate future images for risk estimation are complex and impractical. Additionally, previous methods fail to localize anatomical knee landmarks, limiting interpretability. We address these gaps with a new interpretable machine learning method to estimate the risk of knee OA progression via multi-task predictive modelling that classifies future knee OA severity and predicts anatomical knee landmarks from efficiently generated high-quality future images. Such image generation is achieved by leveraging a diffusion model in a class-conditioned latent space to forecast disease progression, offering a visual representation of how particular health conditions may evolve. Applied to the Osteoarthritis Initiative dataset, our approach improves the state-of-the-art (SOTA) by 2\%, achieving an AUC of 0.71 in predicting knee OA progression while offering ~9% faster inference time.

</details>


### [17] [Align Your Flow: Scaling Continuous-Time Flow Map Distillation](https://arxiv.org/abs/2506.14603)
*Amirmojtaba Sabour,Sanja Fidler,Karsten Kreis*

Main category: cs.CV

TL;DR: 论文提出了一种名为Align Your Flow的流映射模型，通过新的连续时间目标和训练技术，改进了生成模型的效率和性能，并在图像生成任务中取得了领先的成果。


<details>
  <summary>Details</summary>
Motivation: 扩散和流模型在生成任务中表现优异，但需要多步采样。一致性模型虽然可以一步生成，但性能随步数增加而下降。流映射通过连接任意两个噪声级别解决了这一问题。

Method: 提出了两种新的连续时间目标训练流映射，结合自引导和对抗微调技术，提升了模型性能。

Result: 在ImageNet 64x64和512x512上实现了领先的少步生成性能，文本到图像任务中表现优于现有非对抗训练模型。

Conclusion: 流映射模型在效率和性能上均优于现有方法，为生成任务提供了新的解决方案。

Abstract: Diffusion- and flow-based models have emerged as state-of-the-art generative modeling approaches, but they require many sampling steps. Consistency models can distill these models into efficient one-step generators; however, unlike flow- and diffusion-based methods, their performance inevitably degrades when increasing the number of steps, which we show both analytically and empirically. Flow maps generalize these approaches by connecting any two noise levels in a single step and remain effective across all step counts. In this paper, we introduce two new continuous-time objectives for training flow maps, along with additional novel training techniques, generalizing existing consistency and flow matching objectives. We further demonstrate that autoguidance can improve performance, using a low-quality model for guidance during distillation, and an additional boost can be achieved by adversarial finetuning, with minimal loss in sample diversity. We extensively validate our flow map models, called Align Your Flow, on challenging image generation benchmarks and achieve state-of-the-art few-step generation performance on both ImageNet 64x64 and 512x512, using small and efficient neural networks. Finally, we show text-to-image flow map models that outperform all existing non-adversarially trained few-step samplers in text-conditioned synthesis.

</details>


### [18] [Unsupervised Imaging Inverse Problems with Diffusion Distribution Matching](https://arxiv.org/abs/2506.14605)
*Giacomo Meanti,Thomas Ryckeboer,Michael Arbel,Julien Mairal*

Main category: cs.CV

TL;DR: 本文提出了一种基于无配对数据集的图像恢复方法，通过条件流匹配和分布匹配损失学习前向模型，适用于真实场景。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要完整前向模型或配对数据的限制，适用于前向模型未知或配对数据难以获取的场景。

Method: 使用条件流匹配建模退化观测分布，并通过分布匹配损失学习前向模型。

Result: 在去模糊和非均匀点扩散函数校准任务中优于单图像盲方法和无监督方法，在盲超分辨率任务中达到先进水平。

Conclusion: 该方法在真实场景中表现优异，且数据获取成本低，适用于传统耗时实验的应用（如镜头校准）。

Abstract: This work addresses image restoration tasks through the lens of inverse problems using unpaired datasets. In contrast to traditional approaches -- which typically assume full knowledge of the forward model or access to paired degraded and ground-truth images -- the proposed method operates under minimal assumptions and relies only on small, unpaired datasets. This makes it particularly well-suited for real-world scenarios, where the forward model is often unknown or misspecified, and collecting paired data is costly or infeasible. The method leverages conditional flow matching to model the distribution of degraded observations, while simultaneously learning the forward model via a distribution-matching loss that arises naturally from the framework. Empirically, it outperforms both single-image blind and unsupervised approaches on deblurring and non-uniform point spread function (PSF) calibration tasks. It also matches state-of-the-art performance on blind super-resolution. We also showcase the effectiveness of our method with a proof of concept for lens calibration: a real-world application traditionally requiring time-consuming experiments and specialized equipment. In contrast, our approach achieves this with minimal data acquisition effort.

</details>


### [19] [3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D Gaussian-Splatting](https://arxiv.org/abs/2506.14642)
*Yuke Xing,Jiarui Wang,Peizhi Niu,Wenjie Huang,Guangtao Zhai,Yiling Xu*

Main category: cs.CV

TL;DR: 3DGS-IEval-15K是首个针对压缩3D高斯泼溅（3DGS）表示的大规模图像质量评估数据集，包含15,200张图像，用于评估不同压缩算法的感知影响。


<details>
  <summary>Details</summary>
Motivation: 3DGS在实时渲染中表现优异，但高存储需求限制了其实际应用。目前缺乏评估压缩算法感知影响的综合框架。

Method: 通过6种代表性3DGS算法在10个真实场景中渲染图像，覆盖20个视角和不同压缩级别，收集60名观众的主观评价数据。

Result: 数据集通过场景多样性和MOS分布分析验证质量，并建立了30种IQA指标的基准。

Conclusion: 3DGS-IEval-15K为开发3DGS专用IQA指标提供了基础，并支持研究3DGS特有的视角依赖质量分布模式。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a promising approach for novel view synthesis, offering real-time rendering with high visual fidelity. However, its substantial storage requirements present significant challenges for practical applications. While recent state-of-the-art (SOTA) 3DGS methods increasingly incorporate dedicated compression modules, there is a lack of a comprehensive framework to evaluate their perceptual impact. Therefore we present 3DGS-IEval-15K, the first large-scale image quality assessment (IQA) dataset specifically designed for compressed 3DGS representations. Our dataset encompasses 15,200 images rendered from 10 real-world scenes through 6 representative 3DGS algorithms at 20 strategically selected viewpoints, with different compression levels leading to various distortion effects. Through controlled subjective experiments, we collect human perception data from 60 viewers. We validate dataset quality through scene diversity and MOS distribution analysis, and establish a comprehensive benchmark with 30 representative IQA metrics covering diverse types. As the largest-scale 3DGS quality assessment dataset to date, our work provides a foundation for developing 3DGS specialized IQA metrics, and offers essential data for investigating view-dependent quality distribution patterns unique to 3DGS. The database is publicly available at https://github.com/YukeXing/3DGS-IEval-15K.

</details>


### [20] [Iterative Camera-LiDAR Extrinsic Optimization via Surrogate Diffusion](https://arxiv.org/abs/2506.14706)
*Ni Ou,Zhuo Chen,Xinru Zhang,Junzheng Wang*

Main category: cs.CV

TL;DR: 提出了一种基于替代扩散的迭代框架，用于提升相机和LiDAR外参标定方法的性能，无需修改架构。


<details>
  <summary>Details</summary>
Motivation: 现有端到端标定方法多为单步预测，缺乏迭代优化能力，难以满足更高精度需求。

Method: 通过替代扩散框架对初始外参进行迭代优化，原始标定方法作为替代去噪器逐步估计最终外参。

Result: 实验表明，该框架显著提升了四种先进标定方法的精度、鲁棒性和稳定性。

Conclusion: 提出的扩散模型为外参标定提供了一种通用的迭代优化方案，优于其他迭代方法和单步方法。

Abstract: Cameras and LiDAR are essential sensors for autonomous vehicles. The fusion of camera and LiDAR data addresses the limitations of individual sensors but relies on precise extrinsic calibration. Recently, numerous end-to-end calibration methods have been proposed; however, most predict extrinsic parameters in a single step and lack iterative optimization capabilities. To address the increasing demand for higher accuracy, we propose a versatile iterative framework based on surrogate diffusion. This framework can enhance the performance of any calibration method without requiring architectural modifications. Specifically, the initial extrinsic parameters undergo iterative refinement through a denoising process, in which the original calibration method serves as a surrogate denoiser to estimate the final extrinsics at each step. For comparative analysis, we selected four state-of-the-art calibration methods as surrogate denoisers and compared the results of our diffusion process with those of two other iterative approaches. Extensive experiments demonstrate that when integrated with our diffusion model, all calibration methods achieve higher accuracy, improved robustness, and greater stability compared to other iterative techniques and their single-step counterparts.

</details>


### [21] [SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads Synthesis Using Gaussian Splatting](https://arxiv.org/abs/2506.14742)
*Ziqiao Peng,Wentao Hu,Junyuan Ma,Xiangyu Zhu,Xiaomei Zhang,Hao Zhao,Hui Tian,Jun He,Hongyan Liu,Zhaoxin Fan*

Main category: cs.CV

TL;DR: SyncTalk++通过动态肖像渲染器、面部同步控制器和头部同步稳定器解决了语音驱动视频中的同步问题，提升了真实感和渲染速度。


<details>
  <summary>Details</summary>
Motivation: 语音驱动视频中身份、唇动、表情和头部姿势的同步是实现真实感的关键，但现有方法存在同步不足的问题。

Method: 采用高斯散射的动态肖像渲染器保持身份一致性，面部同步控制器对齐唇动和表情，头部同步稳定器优化头部姿势，并引入表情生成器和躯干修复器增强鲁棒性。

Result: SyncTalk++在同步性和真实感上优于现有方法，渲染速度达101帧/秒。

Conclusion: SyncTalk++通过多模块协同解决了同步问题，显著提升了语音驱动视频的质量和效率。

Abstract: Achieving high synchronization in the synthesis of realistic, speech-driven talking head videos presents a significant challenge. A lifelike talking head requires synchronized coordination of subject identity, lip movements, facial expressions, and head poses. The absence of these synchronizations is a fundamental flaw, leading to unrealistic results. To address the critical issue of synchronization, identified as the ''devil'' in creating realistic talking heads, we introduce SyncTalk++, which features a Dynamic Portrait Renderer with Gaussian Splatting to ensure consistent subject identity preservation and a Face-Sync Controller that aligns lip movements with speech while innovatively using a 3D facial blendshape model to reconstruct accurate facial expressions. To ensure natural head movements, we propose a Head-Sync Stabilizer, which optimizes head poses for greater stability. Additionally, SyncTalk++ enhances robustness to out-of-distribution (OOD) audio by incorporating an Expression Generator and a Torso Restorer, which generate speech-matched facial expressions and seamless torso regions. Our approach maintains consistency and continuity in visual details across frames and significantly improves rendering speed and quality, achieving up to 101 frames per second. Extensive experiments and user studies demonstrate that SyncTalk++ outperforms state-of-the-art methods in synchronization and realism. We recommend watching the supplementary video: https://ziqiaopeng.github.io/synctalk++.

</details>


### [22] [Cost-Aware Routing for Efficient Text-To-Image Generation](https://arxiv.org/abs/2506.14753)
*Qinchan,Li,Kenneth Chen,Changyue,Su,Wittawat Jitkrittum,Qi Sun,Patsorn Sangkloy*

Main category: cs.CV

TL;DR: 本文提出了一种基于路由的框架，根据提示的复杂度动态选择最适合的文本到图像生成方法，以平衡计算成本与生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成高质量图像但计算成本高，需要一种方法根据提示复杂度动态调整计算资源。

Method: 通过自动路由机制，将提示分配到不同的生成函数（如不同步数的扩散模型或其他独立模型），以优化计算与质量的平衡。

Result: 在COCO和DiffusionDB数据集上，路由到九个预训练模型的方法平均质量高于单独使用任一模型。

Conclusion: 该方法通过学习动态路由，实现了计算成本与生成质量的最优权衡。

Abstract: Diffusion models are well known for their ability to generate a high-fidelity image for an input prompt through an iterative denoising process. Unfortunately, the high fidelity also comes at a high computational cost due the inherently sequential generative process. In this work, we seek to optimally balance quality and computational cost, and propose a framework to allow the amount of computation to vary for each prompt, depending on its complexity. Each prompt is automatically routed to the most appropriate text-to-image generation function, which may correspond to a distinct number of denoising steps of a diffusion model, or a disparate, independent text-to-image model. Unlike uniform cost reduction techniques (e.g., distillation, model quantization), our approach achieves the optimal trade-off by learning to reserve expensive choices (e.g., 100+ denoising steps) only for a few complex prompts, and employ more economical choices (e.g., small distilled model) for less sophisticated prompts. We empirically demonstrate on COCO and DiffusionDB that by learning to route to nine already-trained text-to-image models, our approach is able to deliver an average quality that is higher than that achievable by any of these models alone.

</details>


### [23] [CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal Diffusion](https://arxiv.org/abs/2506.14769)
*Jiahua Ma,Yiran Qin,Yixiong Li,Xuanqi Liao,Yulan Guo,Ruimao Zhang*

Main category: cs.CV

TL;DR: CDP通过结合历史动作序列和缓存机制，提升了扩散策略在机器人控制中的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 硬件限制和实时约束导致专家示范数据质量下降，影响机器人行为学习效果。

Method: 提出基于Transformer的因果扩散策略（CDP），利用历史动作序列和缓存机制优化动作预测。

Result: 在模拟和真实环境中，CDP显著优于现有方法，尤其在输入质量下降时表现稳健。

Conclusion: CDP通过时间连续性推理，为不完美条件下的机器人控制提供了实用解决方案。

Abstract: Diffusion Policy (DP) enables robots to learn complex behaviors by imitating expert demonstrations through action diffusion. However, in practical applications, hardware limitations often degrade data quality, while real-time constraints restrict model inference to instantaneous state and scene observations. These limitations seriously reduce the efficacy of learning from expert demonstrations, resulting in failures in object localization, grasp planning, and long-horizon task execution. To address these challenges, we propose Causal Diffusion Policy (CDP), a novel transformer-based diffusion model that enhances action prediction by conditioning on historical action sequences, thereby enabling more coherent and context-aware visuomotor policy learning. To further mitigate the computational cost associated with autoregressive inference, a caching mechanism is also introduced to store attention key-value pairs from previous timesteps, substantially reducing redundant computations during execution. Extensive experiments in both simulated and real-world environments, spanning diverse 2D and 3D manipulation tasks, demonstrate that CDP uniquely leverages historical action sequences to achieve significantly higher accuracy than existing methods. Moreover, even when faced with degraded input observation quality, CDP maintains remarkable precision by reasoning through temporal continuity, which highlights its practical robustness for robotic control under realistic, imperfect conditions.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [24] [GAF: Gaussian Action Field as a Dvnamic World Model for Robotic Mlanipulation](https://arxiv.org/abs/2506.14135)
*Ying Chai,Litao Deng,Ruizhi Shao,Jiajun Zhang,Liangjun Xing,Hongwen Zhang,Yebin Liu*

Main category: cs.RO

TL;DR: 论文提出了一种V-4D-A框架，通过高斯动作场（GAF）直接从运动感知的4D表示中进行动作推理，显著提升了机器人操作任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（V-A或V-3D-A）在复杂动态场景中动作推理不准确，需要一种更高效的框架。

Method: GAF扩展了3D高斯泼溅（3DGS），引入可学习运动属性，支持场景重建、未来帧预测和初始动作估计，并通过GAF引导的扩散模型优化动作。

Result: GAF在重建质量上提升11.5385 dB PSNR和-0.5574 LPIPS，机器人操作任务成功率提高10.33%。

Conclusion: V-4D-A框架通过GAF显著提升了动作推理和机器人操作的性能。

Abstract: Accurate action inference is critical for vision-based robotic manipulation. Existing approaches typically follow either a Vision-to-Action (V-A) paradigm, predicting actions directly from visual inputs, or a Vision-to-3D-to-Action (V-3D-A) paradigm, leveraging intermediate 3D representations. However, these methods often struggle with action inaccuracies due to the complexity and dynamic nature of manipulation scenes. In this paper, we propose a V-4D-A framework that enables direct action reasoning from motion-aware 4D representations via a Gaussian Action Field (GAF). GAF extends 3D Gaussian Splatting (3DGS) by incorporating learnable motion attributes, allowing simultaneous modeling of dynamic scenes and manipulation actions. To learn time-varying scene geometry and action-aware robot motion, GAF supports three key query types: reconstruction of the current scene, prediction of future frames, and estimation of initial action via robot motion. Furthermore, the high-quality current and future frames generated by GAF facilitate manipulation action refinement through a GAF-guided diffusion model. Extensive experiments demonstrate significant improvements, with GAF achieving +11.5385 dB PSNR and -0.5574 LPIPS improvements in reconstruction quality, while boosting the average success rate in robotic manipulation tasks by 10.33% over state-of-the-art methods. Project page: http://chaiying1.github.io/GAF.github.io/project_page/

</details>


### [25] [AMPLIFY: Actionless Motion Priors for Robot Learning from Videos](https://arxiv.org/abs/2506.14198)
*Jeremy A. Collins,Loránd Cheng,Kunal Aneja,Albert Wilcox,Benjamin Joffe,Animesh Garg*

Main category: cs.RO

TL;DR: AMPLIFY框架利用大规模无动作视频数据，通过关键点轨迹生成紧凑的运动标记，分离视觉运动预测与动作推断，显著提升低数据量下的策略学习效果。


<details>
  <summary>Details</summary>
Motivation: 机器人动作标注数据稀缺且昂贵，而无动作视频数据丰富但难以转化为有效策略，因此需要一种方法利用这些数据提升策略泛化能力。

Method: AMPLIFY通过关键点轨迹生成运动标记，分离视觉运动预测（使用大量无动作视频训练前向动力学模型）与动作推断（使用少量标注数据训练逆向动力学模型）。

Result: 实验显示，AMPLIFY在MSE和像素预测精度上分别提升3.7倍和2.5倍，策略学习在低数据量下提升1.2-2.2倍，并能从无动作人类视频中学习。

Conclusion: AMPLIFY提供了一种利用异构数据构建高效、可泛化世界模型的新范式，适用于机器人控制及其他领域。

Abstract: Action-labeled data for robotics is scarce and expensive, limiting the generalization of learned policies. In contrast, vast amounts of action-free video data are readily available, but translating these observations into effective policies remains a challenge. We introduce AMPLIFY, a novel framework that leverages large-scale video data by encoding visual dynamics into compact, discrete motion tokens derived from keypoint trajectories. Our modular approach separates visual motion prediction from action inference, decoupling the challenges of learning what motion defines a task from how robots can perform it. We train a forward dynamics model on abundant action-free videos and an inverse dynamics model on a limited set of action-labeled examples, allowing for independent scaling. Extensive evaluations demonstrate that the learned dynamics are both accurate, achieving up to 3.7x better MSE and over 2.5x better pixel prediction accuracy compared to prior approaches, and broadly useful. In downstream policy learning, our dynamics predictions enable a 1.2-2.2x improvement in low-data regimes, a 1.4x average improvement by learning from action-free human videos, and the first generalization to LIBERO tasks from zero in-distribution action data. Beyond robotic control, we find the dynamics learned by AMPLIFY to be a versatile latent world model, enhancing video prediction quality. Our results present a novel paradigm leveraging heterogeneous data sources to build efficient, generalizable world models. More information can be found at https://amplify-robotics.github.io/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [26] [Enclosing Prototypical Variational Autoencoder for Explainable Out-of-Distribution Detection](https://arxiv.org/abs/2506.14390)
*Conrad Orglmeister,Erik Bochinski,Volker Eiselein,Elvira Fleig*

Main category: cs.LG

TL;DR: 论文提出了一种结合自解释原型变分模型与自编码器的OOD检测方法，通过变分自编码器学习潜在空间，用于分类、OOD检测和重建，并在真实铁路数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 为了提高深度机器学习模型在安全相关应用中的可靠性和可解释性，需要开发一种能够检测分布外样本并解释决策的方法。

Method: 使用变分自编码器学习潜在空间，定义高斯混合分布作为分布内区域，并引入限制损失以保持潜在空间的紧凑性。

Result: 在常见OOD检测基准和真实铁路数据集上表现优于现有方法。

Conclusion: 该方法通过结合自解释性和OOD检测能力，为安全相关应用提供了可靠的解决方案。

Abstract: Understanding the decision-making and trusting the reliability of Deep Machine Learning Models is crucial for adopting such methods to safety-relevant applications. We extend self-explainable Prototypical Variational models with autoencoder-based out-of-distribution (OOD) detection: A Variational Autoencoder is applied to learn a meaningful latent space which can be used for distance-based classification, likelihood estimation for OOD detection, and reconstruction. The In-Distribution (ID) region is defined by a Gaussian mixture distribution with learned prototypes representing the center of each mode. Furthermore, a novel restriction loss is introduced that promotes a compact ID region in the latent space without collapsing it into single points. The reconstructive capabilities of the Autoencoder ensure the explainability of the prototypes and the ID region of the classifier, further aiding the discrimination of OOD samples. Extensive evaluations on common OOD detection benchmarks as well as a large-scale dataset from a real-world railway application demonstrate the usefulness of the approach, outperforming previous methods.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [27] [Latent Anomaly Detection: Masked VQ-GAN for Unsupervised Segmentation in Medical CBCT](https://arxiv.org/abs/2506.14209)
*Pengwei Wang*

Main category: eess.IV

TL;DR: 提出了一种无监督训练方法，用于自动识别ONJ影像中的异常，通过两阶段训练流程实现，减少手动标注负担。


<details>
  <summary>Details</summary>
Motivation: 由于ONJ影像中标记数据的稀缺性，监督训练不切实际，需要开发无监督方法。

Method: 两阶段训练流程：1) 使用VQ-GAN重建正常样本；2) 应用随机和ONJ特定掩码训练新编码器。

Result: 在模拟和真实患者数据上成功实现分割。

Conclusion: 该方法提供快速初始分割解决方案，并有望直接用于3D打印。

Abstract: Advances in treatment technology now allow for the use of customizable 3D-printed hydrogel wound dressings for patients with osteoradionecrosis (ORN) of the jaw (ONJ). Meanwhile, deep learning has enabled precise segmentation of 3D medical images using tools like nnUNet.   However, the scarcity of labeled data in ONJ imaging makes supervised training impractical. This study aims to develop an unsupervised training approach for automatically identifying anomalies in imaging scans.   We propose a novel two-stage training pipeline. In the first stage, a VQ-GAN is trained to accurately reconstruct normal subjects. In the second stage, random cube masking and ONJ-specific masking are applied to train a new encoder capable of recovering the data.   The proposed method achieves successful segmentation on both simulated and real patient data.   This approach provides a fast initial segmentation solution, reducing the burden of manual labeling. Additionally, it has the potential to be directly used for 3D printing when combined with hand-tuned post-processing.

</details>


### [28] [orGAN: A Synthetic Data Augmentation Pipeline for Simultaneous Generation of Surgical Images and Ground Truth Labels](https://arxiv.org/abs/2506.14303)
*Niran Nataraj,Maina Sogabe,Kenji Kawashima*

Main category: eess.IV

TL;DR: orGAN是一种基于GAN的系统，用于生成高保真、带注释的手术出血图像，解决了医学影像中数据多样性不足、伦理问题和成本高的挑战。


<details>
  <summary>Details</summary>
Motivation: 手术中出血检测和定位因高质量数据稀缺而困难，传统方法面临伦理和成本问题。

Method: 利用小型“模拟器官”数据集和StyleGAN结合关系位置学习，生成逼真出血图像，并通过LaMa修复模块提供精确注释。

Result: 评估中，orGAN生成的图像在手术场景中达到90%检测准确率和99%帧级准确率。

Conclusion: orGAN显著提升了伦理、高效且低成本生成真实出血数据集的能力，支持AI在手术中的广泛应用。

Abstract: Deep learning in medical imaging faces obstacles: limited data diversity, ethical issues, high acquisition costs, and the need for precise annotations. Bleeding detection and localization during surgery is especially challenging due to the scarcity of high-quality datasets that reflect real surgical scenarios. We propose orGAN, a GAN-based system for generating high-fidelity, annotated surgical images of bleeding. By leveraging small "mimicking organ" datasets, synthetic models that replicate tissue properties and bleeding, our approach reduces ethical concerns and data-collection costs. orGAN builds on StyleGAN with Relational Positional Learning to simulate bleeding events realistically and mark bleeding coordinates. A LaMa-based inpainting module then restores clean, pre-bleed visuals, enabling precise pixel-level annotations. In evaluations, a balanced dataset of orGAN and mimicking-organ images achieved 90% detection accuracy in surgical settings and up to 99% frame-level accuracy. While our development data lack diverse organ morphologies and contain intraoperative artifacts, orGAN markedly advances ethical, efficient, and cost-effective creation of realistic annotated bleeding datasets, supporting broader integration of AI in surgical practice.

</details>


### [29] [Compressed Video Super-Resolution based on Hierarchical Encoding](https://arxiv.org/abs/2506.14381)
*Yuxuan Jiang,Siyue Teng,Qiang Zhu,Chen Feng,Chengxi Zeng,Fan Zhang,Shuyuan Zhu,Bing Zeng,David Bull*

Main category: eess.IV

TL;DR: VSR-HE是一种通用视频超分辨率方法，专注于提升压缩内容的感知质量，通过分层编码变换块消除压缩伪影，并在多种压缩设置下训练和评估。


<details>
  <summary>Details</summary>
Motivation: 针对高压缩场景，提升低分辨率视频的感知质量，消除H.265/HEVC编码引入的压缩伪影。

Method: 采用分层编码变换块，对低分辨率视频进行四倍上采样（如180p到720p），并在多种量化参数下优化。

Result: 模型能够有效恢复细粒度细节并保持视觉保真度，适用于通用视频内容和说话人视频。

Conclusion: VSR-HE在压缩视频超分辨率任务中表现出色，已提交至ICME 2025挑战赛。

Abstract: This paper presents a general-purpose video super-resolution (VSR) method, dubbed VSR-HE, specifically designed to enhance the perceptual quality of compressed content. Targeting scenarios characterized by heavy compression, the method upscales low-resolution videos by a ratio of four, from 180p to 720p or from 270p to 1080p. VSR-HE adopts hierarchical encoding transformer blocks and has been sophisticatedly optimized to eliminate a wide range of compression artifacts commonly introduced by H.265/HEVC encoding across various quantization parameter (QP) levels. To ensure robustness and generalization, the model is trained and evaluated under diverse compression settings, allowing it to effectively restore fine-grained details and preserve visual fidelity. The proposed VSR-HE has been officially submitted to the ICME 2025 Grand Challenge on VSR for Video Conferencing (Team BVI-VSR), under both the Track 1 (General-Purpose Real-World Video Content) and Track 2 (Talking Head Videos).

</details>
