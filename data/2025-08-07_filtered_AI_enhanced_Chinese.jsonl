{"id": "2508.04078", "pdf": "https://arxiv.org/pdf/2508.04078", "abs": "https://arxiv.org/abs/2508.04078", "authors": ["Zhan Li", "Huangying Zhan", "Changyang Li", "Qingan Yan", "Yi Xu"], "title": "RLGS: Reinforcement Learning-Based Adaptive Hyperparameter Tuning for Gaussian Splatting", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "14 pages, 9 figures", "summary": "Hyperparameter tuning in 3D Gaussian Splatting (3DGS) is a labor-intensive and expert-driven process, often resulting in inconsistent reconstructions and suboptimal results. We propose RLGS, a plug-and-play reinforcement learning framework for adaptive hyperparameter tuning in 3DGS through lightweight policy modules, dynamically adjusting critical hyperparameters such as learning rates and densification thresholds. The framework is model-agnostic and seamlessly integrates into existing 3DGS pipelines without architectural modifications. We demonstrate its generalization ability across multiple state-of-the-art 3DGS variants, including Taming-3DGS and 3DGS-MCMC, and validate its robustness across diverse datasets. RLGS consistently enhances rendering quality. For example, it improves Taming-3DGS by 0.7dB PSNR on the Tanks and Temple (TNT) dataset, under a fixed Gaussian budget, and continues to yield gains even when baseline performance saturates. Our results suggest that RLGS provides an effective and general solution for automating hyperparameter tuning in 3DGS training, bridging a gap in applying reinforcement learning to 3DGS.", "AI": {"tldr": "RLGS\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u8c03\u65743D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u4e2d\u7684\u8d85\u53c2\u6570\uff0c\u63d0\u5347\u6e32\u67d3\u8d28\u91cf\u3002", "motivation": "3DGS\u4e2d\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u901a\u5e38\u4f9d\u8d56\u4e13\u5bb6\u7ecf\u9a8c\u4e14\u8017\u65f6\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u4e00\u81f4\u6216\u6b21\u4f18\u3002", "method": "RLGS\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7b56\u7565\u6a21\u5757\u52a8\u6001\u8c03\u6574\u5173\u952e\u8d85\u53c2\u6570\uff08\u5982\u5b66\u4e60\u7387\u548c\u5bc6\u5ea6\u9608\u503c\uff09\uff0c\u65e0\u9700\u4fee\u6539\u73b0\u67093DGS\u67b6\u6784\u3002", "result": "RLGS\u5728\u591a\u4e2a3DGS\u53d8\u4f53\uff08\u5982Taming-3DGS\u548c3DGS-MCMC\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f8b\u5982\u5728TNT\u6570\u636e\u96c6\u4e0a\u63d0\u53470.7dB PSNR\u3002", "conclusion": "RLGS\u4e3a3DGS\u8d85\u53c2\u6570\u8c03\u4f18\u63d0\u4f9b\u4e86\u901a\u7528\u4e14\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u5f3a\u5316\u5b66\u4e60\u57283DGS\u4e2d\u7684\u5e94\u7528\u7a7a\u767d\u3002"}}
{"id": "2508.04326", "pdf": "https://arxiv.org/pdf/2508.04326", "abs": "https://arxiv.org/abs/2508.04326", "authors": ["Ke Li", "Mana Masuda", "Susanne Schmidt", "Shohei Mori"], "title": "Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned and Addressed for XR Research", "categories": ["cs.GR"], "comment": "This work has been submitted to the IEEE TVCG journal for possible   publication", "summary": "The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF), has revolutionized interactive photorealistic view synthesis and presents enormous opportunities for XR research and applications. However, despite the exponential growth of RF research, RF-related contributions to the XR community remain sparse. To better understand this research gap, we performed a systematic survey of current RF literature to analyze (i) how RF is envisioned for XR applications, (ii) how they have already been implemented, and (iii) the remaining research gaps. We collected 365 RF contributions related to XR from computer vision, computer graphics, robotics, multimedia, human-computer interaction, and XR communities, seeking to answer the above research questions. Among the 365 papers, we performed an analysis of 66 papers that already addressed a detailed aspect of RF research for XR. With this survey, we extended and positioned XR-specific RF research topics in the broader RF research field and provide a helpful resource for the XR community to navigate within the rapid development of RF research.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8c03\u67e5\u4e86\u8f90\u5c04\u573a\uff08RF\uff09\u6280\u672f\u5728XR\u9886\u57df\u7684\u5e94\u7528\u73b0\u72b6\uff0c\u5206\u6790\u4e86\u7814\u7a76\u5dee\u8ddd\uff0c\u5e76\u63d0\u4f9b\u4e86\u8d44\u6e90\u5bfc\u822a\u3002", "motivation": "\u5c3d\u7ba1RF\u6280\u672f\uff08\u59823DGS\u548cNeRF\uff09\u5728XR\u9886\u57df\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5176\u5b9e\u9645\u8d21\u732e\u4ecd\u8f83\u5c11\uff0c\u9700\u7cfb\u7edf\u6027\u7814\u7a76\u4ee5\u586b\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u6536\u96c6\u5e76\u5206\u6790\u4e86365\u7bc7\u4e0eXR\u76f8\u5173\u7684RF\u6587\u732e\uff0c\u91cd\u70b9\u7814\u7a76\u4e86\u5176\u4e2d66\u7bc7\u8be6\u7ec6\u63a2\u8ba8RF\u5728XR\u4e2d\u5e94\u7528\u7684\u8bba\u6587\u3002", "result": "\u7814\u7a76\u53d1\u73b0RF\u5728XR\u4e2d\u7684\u5e94\u7528\u4ecd\u5904\u4e8e\u65e9\u671f\u9636\u6bb5\uff0c\u660e\u786e\u4e86\u7814\u7a76\u5dee\u8ddd\uff0c\u5e76\u4e3aXR\u793e\u533a\u63d0\u4f9b\u4e86\u8d44\u6e90\u5bfc\u822a\u3002", "conclusion": "\u672c\u6587\u586b\u8865\u4e86RF\u5728XR\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u548c\u8d44\u6e90\u3002"}}
{"id": "2508.03727", "pdf": "https://arxiv.org/pdf/2508.03727", "abs": "https://arxiv.org/abs/2508.03727", "authors": ["Tai Hyoung Rhee", "Dong-guw Lee", "Ayoung Kim"], "title": "TIR-Diffusion: Diffusion-based Thermal Infrared Image Denoising via Latent and Wavelet Domain Optimization", "categories": ["cs.CV", "cs.RO", "eess.IV"], "comment": "Accepted at Thermal Infrared in Robotics (TIRO) Workshop, ICRA 2025", "summary": "Thermal infrared imaging exhibits considerable potentials for robotic perception tasks, especially in environments with poor visibility or challenging lighting conditions. However, TIR images typically suffer from heavy non-uniform fixed-pattern noise, complicating tasks such as object detection, localization, and mapping. To address this, we propose a diffusion-based TIR image denoising framework leveraging latent-space representations and wavelet-domain optimization. Utilizing a pretrained stable diffusion model, our method fine-tunes the model via a novel loss function combining latent-space and discrete wavelet transform (DWT) / dual-tree complex wavelet transform (DTCWT) losses. Additionally, we implement a cascaded refinement stage to enhance fine details, ensuring high-fidelity denoising results. Experiments on benchmark datasets demonstrate superior performance of our approach compared to state-of-the-art denoising methods. Furthermore, our method exhibits robust zero-shot generalization to diverse and challenging real-world TIR datasets, underscoring its effectiveness for practical robotic deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u70ed\u7ea2\u5916\u56fe\u50cf\u53bb\u566a\u6846\u67b6\uff0c\u7ed3\u5408\u6f5c\u5728\u7a7a\u95f4\u548c\u5c0f\u6ce2\u57df\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53bb\u566a\u6027\u80fd\u3002", "motivation": "\u70ed\u7ea2\u5916\u56fe\u50cf\u5728\u673a\u5668\u4eba\u611f\u77e5\u4efb\u52a1\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u53d7\u9650\u4e8e\u56fa\u5b9a\u6a21\u5f0f\u566a\u58f0\uff0c\u5f71\u54cd\u76ee\u6807\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7b49\u4efb\u52a1\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684\u7a33\u5b9a\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u548c\u5c0f\u6ce2\u53d8\u6362\u635f\u5931\u51fd\u6570\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u91c7\u7528\u7ea7\u8054\u7ec6\u5316\u9636\u6bb5\u589e\u5f3a\u7ec6\u8282\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u53bb\u566a\u65b9\u6cd5\uff0c\u4e14\u5177\u6709\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.03735", "pdf": "https://arxiv.org/pdf/2508.03735", "abs": "https://arxiv.org/abs/2508.03735", "authors": ["Gopalji Gaur", "Mohammadreza Zolfaghari", "Thomas Brox"], "title": "StorySync: Training-Free Subject Consistency in Text-to-Image Generation via Region Harmonization", "categories": ["cs.CV", "cs.AI"], "comment": "14 pages, 10 figures, GCPR", "summary": "Generating a coherent sequence of images that tells a visual story, using text-to-image diffusion models, often faces the critical challenge of maintaining subject consistency across all story scenes. Existing approaches, which typically rely on fine-tuning or retraining models, are computationally expensive, time-consuming, and often interfere with the model's pre-existing capabilities. In this paper, we follow a training-free approach and propose an efficient consistent-subject-generation method. This approach works seamlessly with pre-trained diffusion models by introducing masked cross-image attention sharing to dynamically align subject features across a batch of images, and Regional Feature Harmonization to refine visually similar details for improved subject consistency. Experimental results demonstrate that our approach successfully generates visually consistent subjects across a variety of scenarios while maintaining the creative abilities of the diffusion model.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u3001\u9ad8\u6548\u7684\u4e3b\u9898\u4e00\u81f4\u6027\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u56fe\u50cf\u6ce8\u610f\u529b\u5171\u4eab\u548c\u533a\u57df\u7279\u5f81\u534f\u8c03\uff0c\u5728\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4e2d\u5b9e\u73b0\u89c6\u89c9\u6545\u4e8b\u7684\u4e3b\u9898\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5fae\u8c03\u6216\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u53ef\u80fd\u5e72\u6270\u6a21\u578b\u539f\u6709\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8bad\u7ec3\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u5f15\u5165\u8de8\u56fe\u50cf\u6ce8\u610f\u529b\u5171\u4eab\u548c\u533a\u57df\u7279\u5f81\u534f\u8c03\uff0c\u52a8\u6001\u5bf9\u9f50\u4e3b\u9898\u7279\u5f81\u5e76\u4f18\u5316\u89c6\u89c9\u7ec6\u8282\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5728\u591a\u79cd\u573a\u666f\u4e0b\u751f\u6210\u89c6\u89c9\u4e00\u81f4\u7684\u4e3b\u9898\uff0c\u540c\u65f6\u4fdd\u7559\u6269\u6563\u6a21\u578b\u7684\u521b\u9020\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4e3b\u9898\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u539f\u6709\u80fd\u529b\u3002"}}
{"id": "2508.03754", "pdf": "https://arxiv.org/pdf/2508.03754", "abs": "https://arxiv.org/abs/2508.03754", "authors": ["Bevin V", "Ananthakrishnan P V", "Ragesh KR", "Sanjay M", "Vineeth S", "Bibin Wilson"], "title": "Generating Synthetic Invoices via Layout-Preserving Content Replacement", "categories": ["cs.CV"], "comment": null, "summary": "The performance of machine learning models for automated invoice processing is critically dependent on large-scale, diverse datasets. However, the acquisition of such datasets is often constrained by privacy regulations and the high cost of manual annotation. To address this, we present a novel pipeline for generating high-fidelity, synthetic invoice documents and their corresponding structured data. Our method first utilizes Optical Character Recognition (OCR) to extract the text content and precise spatial layout from a source invoice. Select data fields are then replaced with contextually realistic, synthetic content generated by a large language model (LLM). Finally, we employ an inpainting technique to erase the original text from the image and render the new, synthetic text in its place, preserving the exact layout and font characteristics. This process yields a pair of outputs: a visually realistic new invoice image and a perfectly aligned structured data file (JSON) reflecting the synthetic content. Our approach provides a scalable and automated solution to amplify small, private datasets, enabling the creation of large, varied corpora for training more robust and accurate document intelligence models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u9ad8\u4fdd\u771f\u5408\u6210\u53d1\u7968\u6587\u6863\u53ca\u5176\u7ed3\u6784\u5316\u6570\u636e\u7684\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u7528\u4e8e\u89e3\u51b3\u6570\u636e\u9690\u79c1\u548c\u6807\u6ce8\u6210\u672c\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u53d1\u7968\u5904\u7406\u4e2d\u56e0\u9690\u79c1\u6cd5\u89c4\u548c\u6807\u6ce8\u6210\u672c\u9ad8\u800c\u96be\u4ee5\u83b7\u53d6\u5927\u89c4\u6a21\u591a\u6837\u5316\u6570\u636e\u96c6\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408OCR\u63d0\u53d6\u6587\u672c\u548c\u5e03\u5c40\uff0c\u7528LLM\u751f\u6210\u4e0a\u4e0b\u6587\u5408\u7406\u7684\u5408\u6210\u5185\u5bb9\uff0c\u5e76\u901a\u8fc7\u56fe\u50cf\u4fee\u590d\u6280\u672f\u66ff\u6362\u539f\u59cb\u6587\u672c\uff0c\u751f\u6210\u65b0\u53d1\u7968\u56fe\u50cf\u548c\u5bf9\u5e94\u7684\u7ed3\u6784\u5316\u6570\u636e\u3002", "result": "\u751f\u6210\u4e86\u89c6\u89c9\u903c\u771f\u7684\u5408\u6210\u53d1\u7968\u56fe\u50cf\u548c\u5b8c\u5168\u5bf9\u9f50\u7684\u7ed3\u6784\u5316\u6570\u636e\u6587\u4ef6\uff08JSON\uff09\uff0c\u4e3a\u8bad\u7ec3\u66f4\u9c81\u68d2\u7684\u6587\u6863\u667a\u80fd\u6a21\u578b\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5c0f\u89c4\u6a21\u79c1\u6709\u6570\u636e\u96c6\u7684\u6269\u5c55\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u6587\u6863\u667a\u80fd\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2508.03775", "pdf": "https://arxiv.org/pdf/2508.03775", "abs": "https://arxiv.org/abs/2508.03775", "authors": ["Mingyu Liu", "Zian Mao", "Zhu Liu", "Haoran Zhang", "Jintao Guo", "Xiaoya He", "Xi Huang", "Shufen Chu", "Chun Cheng", "Jun Ding", "Yujun Xie"], "title": "4D-PreNet: A Unified Preprocessing Framework for 4D-STEM Data Analysis", "categories": ["cs.CV", "cond-mat.mtrl-sci", "cs.AI", "I.2.10; I.5.1; J.2"], "comment": "17 pages,5 figures", "summary": "Automated experimentation with real time data analysis in scanning transmission electron microscopy (STEM) often require end-to-end framework. The four-dimensional scanning transmission electron microscopy (4D-STEM) with high-throughput data acquisition has been constrained by the critical bottleneck results from data preprocessing. Pervasive noise, beam center drift, and elliptical distortions during high-throughput acquisition inevitably corrupt diffraction patterns, systematically biasing quantitative measurements. Yet, conventional correction algorithms are often material-specific and fail to provide a robust, generalizable solution. In this work, we present 4D-PreNet, an end-to-end deep-learning pipeline that integrates attention-enhanced U-Net and ResNet architectures to simultaneously perform denoising, center correction, and elliptical distortion calibration. The network is trained on large, simulated datasets encompassing a wide range of noise levels, drift magnitudes, and distortion types, enabling it to generalize effectively to experimental data acquired under varying conditions. Quantitative evaluations demonstrate that our pipeline reduces mean squared error by up to 50% during denoising and achieves sub-pixel center localization in the center detection task, with average errors below 0.04 pixels. The outputs are bench-marked against traditional algorithms, highlighting improvements in both noise suppression and restoration of diffraction patterns, thereby facilitating high-throughput, reliable 4D-STEM real-time analysis for automated characterization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a4D-PreNet\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b34D-STEM\u6570\u636e\u9884\u5904\u7406\u4e2d\u7684\u566a\u58f0\u3001\u4e2d\u5fc3\u6f02\u79fb\u548c\u692d\u5706\u7578\u53d8\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u8d28\u91cf\u548c\u5b9e\u65f6\u5206\u6790\u80fd\u529b\u3002", "motivation": "\u9ad8\u541e\u5410\u91cf4D-STEM\u6570\u636e\u9884\u5904\u7406\u4e2d\u7684\u566a\u58f0\u3001\u4e2d\u5fc3\u6f02\u79fb\u548c\u692d\u5706\u7578\u53d8\u95ee\u9898\u9650\u5236\u4e86\u5b9e\u65f6\u5206\u6790\u7684\u53ef\u9760\u6027\uff0c\u4f20\u7edf\u7b97\u6cd5\u7f3a\u4e4f\u901a\u7528\u6027\u3002", "method": "\u7ed3\u5408\u6ce8\u610f\u529b\u589e\u5f3a\u7684U-Net\u548cResNet\u67b6\u6784\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u6a21\u62df\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u5b9e\u73b0\u53bb\u566a\u3001\u4e2d\u5fc3\u6821\u6b63\u548c\u692d\u5706\u7578\u53d8\u6821\u51c6\u3002", "result": "\u53bb\u566a\u4efb\u52a1\u4e2d\u5747\u65b9\u8bef\u5dee\u964d\u4f4e50%\uff0c\u4e2d\u5fc3\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5e73\u5747\u8bef\u5dee\u4f4e\u4e8e0.04\u50cf\u7d20\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7b97\u6cd5\u3002", "conclusion": "4D-PreNet\u4e3a4D-STEM\u7684\u9ad8\u901a\u91cf\u5b9e\u65f6\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.03789", "pdf": "https://arxiv.org/pdf/2508.03789", "abs": "https://arxiv.org/abs/2508.03789", "authors": ["Yuhang Ma", "Xiaoshi Wu", "Keqiang Sun", "Hongsheng Li"], "title": "HPSv3: Towards Wide-Spectrum Human Preference Score", "categories": ["cs.CV"], "comment": "ICCV2025", "summary": "Evaluating text-to-image generation models requires alignment with human perception, yet existing human-centric metrics are constrained by limited data coverage, suboptimal feature extraction, and inefficient loss functions. To address these challenges, we introduce Human Preference Score v3 (HPSv3). (1) We release HPDv3, the first wide-spectrum human preference dataset integrating 1.08M text-image pairs and 1.17M annotated pairwise comparisons from state-of-the-art generative models and low to high-quality real-world images. (2) We introduce a VLM-based preference model trained using an uncertainty-aware ranking loss for fine-grained ranking. Besides, we propose Chain-of-Human-Preference (CoHP), an iterative image refinement method that enhances quality without extra data, using HPSv3 to select the best image at each step. Extensive experiments demonstrate that HPSv3 serves as a robust metric for wide-spectrum image evaluation, and CoHP offers an efficient and human-aligned approach to improve image generation quality. The code and dataset are available at the HPSv3 Homepage.", "AI": {"tldr": "HPSv3\u662f\u4e00\u79cd\u65b0\u7684\u4eba\u7c7b\u504f\u597d\u8bc4\u5206\u65b9\u6cd5\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u6539\u8fdb\u7684\u635f\u5931\u51fd\u6570\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u8bc4\u4f30\u6548\u679c\uff0c\u5e76\u63d0\u51fa\u4e86CoHP\u65b9\u6cd5\u4f18\u5316\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u7c7b\u4e2d\u5fc3\u6307\u6807\u5728\u6570\u636e\u8986\u76d6\u3001\u7279\u5f81\u63d0\u53d6\u548c\u635f\u5931\u51fd\u6570\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u66f4\u7b26\u5408\u4eba\u7c7b\u611f\u77e5\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faHPSv3\uff0c\u5305\u62ecHPDv3\u6570\u636e\u96c6\uff081.08M\u6587\u672c-\u56fe\u50cf\u5bf9\u548c1.17M\u6807\u6ce8\u6bd4\u8f83\uff09\u3001\u57fa\u4e8eVLM\u7684\u504f\u597d\u6a21\u578b\u548cCoHP\u8fed\u4ee3\u4f18\u5316\u65b9\u6cd5\u3002", "result": "HPSv3\u5728\u5e7f\u6cdb\u56fe\u50cf\u8bc4\u4f30\u4e2d\u8868\u73b0\u7a33\u5065\uff0cCoHP\u65e0\u9700\u989d\u5916\u6570\u636e\u5373\u53ef\u63d0\u5347\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "HPSv3\u548cCoHP\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u7684\u8bc4\u4f30\u548c\u4f18\u5316\u65b9\u6cd5\u3002"}}
{"id": "2508.03925", "pdf": "https://arxiv.org/pdf/2508.03925", "abs": "https://arxiv.org/abs/2508.03925", "authors": ["Shen Zhu", "Yinzhu Jin", "Ifrah Zawar", "P. Thomas Fletcher"], "title": "Point-Based Shape Representation Generation with a Correspondence-Preserving Diffusion Model", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We propose a diffusion model designed to generate point-based shape representations with correspondences. Traditional statistical shape models have considered point correspondences extensively, but current deep learning methods do not take them into account, focusing on unordered point clouds instead. Current deep generative models for point clouds do not address generating shapes with point correspondences between generated shapes. This work aims to formulate a diffusion model that is capable of generating realistic point-based shape representations, which preserve point correspondences that are present in the training data. Using shape representation data with correspondences derived from Open Access Series of Imaging Studies 3 (OASIS-3), we demonstrate that our correspondence-preserving model effectively generates point-based hippocampal shape representations that are highly realistic compared to existing methods. We further demonstrate the applications of our generative model by downstream tasks, such as conditional generation of healthy and AD subjects and predicting morphological changes of disease progression by counterfactual generation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u5bf9\u5e94\u5173\u7cfb\u7684\u70b9\u57fa\u5f62\u72b6\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5ffd\u7565\u70b9\u5bf9\u5e94\u5173\u7cfb\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7edf\u8ba1\u5f62\u72b6\u6a21\u578b\u5173\u6ce8\u70b9\u5bf9\u5e94\u5173\u7cfb\uff0c\u4f46\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4ec5\u5173\u6ce8\u65e0\u5e8f\u70b9\u4e91\uff0c\u65e0\u6cd5\u751f\u6210\u5177\u6709\u70b9\u5bf9\u5e94\u5173\u7cfb\u7684\u5f62\u72b6\u3002", "method": "\u8bbe\u8ba1\u6269\u6563\u6a21\u578b\uff0c\u5229\u7528OASIS-3\u6570\u636e\u751f\u6210\u4fdd\u7559\u70b9\u5bf9\u5e94\u5173\u7cfb\u7684\u70b9\u57fa\u5f62\u72b6\u8868\u793a\u3002", "result": "\u6a21\u578b\u751f\u6210\u7684\u5f62\u72b6\u9ad8\u5ea6\u903c\u771f\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u6761\u4ef6\u751f\u6210\u548c\u53cd\u4e8b\u5b9e\u751f\u6210\uff09\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u751f\u6210\u5177\u6709\u70b9\u5bf9\u5e94\u5173\u7cfb\u7684\u5f62\u72b6\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.04016", "pdf": "https://arxiv.org/pdf/2508.04016", "abs": "https://arxiv.org/abs/2508.04016", "authors": ["Weilun Feng", "Haotong Qin", "Chuanguang Yang", "Xiangqi Li", "Han Yang", "Yuqi Li", "Zhulin An", "Libo Huang", "Michele Magno", "Yongjun Xu"], "title": "$\\text{S}^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion transformers have emerged as the mainstream paradigm for video generation models. However, the use of up to billions of parameters incurs significant computational costs. Quantization offers a promising solution by reducing memory usage and accelerating inference. Nonetheless, we observe that the joint modeling of spatial and temporal information in video diffusion models (V-DMs) leads to extremely long token sequences, which introduces high calibration variance and learning challenges. To address these issues, we propose \\textbf{$\\text{S}^2$Q-VDiT}, a post-training quantization framework for V-DMs that leverages \\textbf{S}alient data and \\textbf{S}parse token distillation. During the calibration phase, we identify that quantization performance is highly sensitive to the choice of calibration data. To mitigate this, we introduce \\textit{Hessian-aware Salient Data Selection}, which constructs high-quality calibration datasets by considering both diffusion and quantization characteristics unique to V-DMs. To tackle the learning challenges, we further analyze the sparse attention patterns inherent in V-DMs. Based on this observation, we propose \\textit{Attention-guided Sparse Token Distillation}, which exploits token-wise attention distributions to emphasize tokens that are more influential to the model's output. Under W4A6 quantization, $\\text{S}^2$Q-VDiT achieves lossless performance while delivering $3.9\\times$ model compression and $1.3\\times$ inference acceleration. Code will be available at \\href{https://github.com/wlfeng0509/s2q-vdit}{https://github.com/wlfeng0509/s2q-vdit}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aS\u00b2Q-VDiT\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08V-DMs\uff09\uff0c\u901a\u8fc7\u663e\u8457\u6570\u636e\u9009\u62e9\u548c\u7a00\u758f\u4ee4\u724c\u84b8\u998f\u89e3\u51b3\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "motivation": "\u89c6\u9891\u6269\u6563\u6a21\u578b\u56e0\u8054\u5408\u5efa\u6a21\u65f6\u7a7a\u4fe1\u606f\u5bfc\u81f4\u957f\u4ee4\u724c\u5e8f\u5217\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u91cf\u5316\u867d\u80fd\u7f13\u89e3\u4f46\u9762\u4e34\u6821\u51c6\u65b9\u5dee\u548c\u5b66\u4e60\u6311\u6218\u3002", "method": "\u63d0\u51faHessian-aware\u663e\u8457\u6570\u636e\u9009\u62e9\u548c\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u7a00\u758f\u4ee4\u724c\u84b8\u998f\uff0c\u4f18\u5316\u6821\u51c6\u6570\u636e\u548c\u4ee4\u724c\u5b66\u4e60\u3002", "result": "\u5728W4A6\u91cf\u5316\u4e0b\uff0cS\u00b2Q-VDiT\u5b9e\u73b0\u65e0\u635f\u6027\u80fd\uff0c\u6a21\u578b\u538b\u7f293.9\u500d\uff0c\u63a8\u7406\u52a0\u901f1.3\u500d\u3002", "conclusion": "S\u00b2Q-VDiT\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u91cf\u5316\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2508.04041", "pdf": "https://arxiv.org/pdf/2508.04041", "abs": "https://arxiv.org/abs/2508.04041", "authors": ["Tongshun Zhang", "Pingling Liu", "Zijian Zhang", "Qiuzhan Zhou"], "title": "SPJFNet: Self-Mining Prior-Guided Joint Frequency Enhancement for Ultra-Efficient Dark Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Current dark image restoration methods suffer from severe efficiency bottlenecks, primarily stemming from: (1) computational burden and error correction costs associated with reliance on external priors (manual or cross-modal); (2) redundant operations in complex multi-stage enhancement pipelines; and (3) indiscriminate processing across frequency components in frequency-domain methods, leading to excessive global computational demands. To address these challenges, we propose an Efficient Self-Mining Prior-Guided Joint Frequency Enhancement Network (SPJFNet). Specifically, we first introduce a Self-Mining Guidance Module (SMGM) that generates lightweight endogenous guidance directly from the network, eliminating dependence on external priors and thereby bypassing error correction overhead while improving inference speed. Second, through meticulous analysis of different frequency domain characteristics, we reconstruct and compress multi-level operation chains into a single efficient operation via lossless wavelet decomposition and joint Fourier-based advantageous frequency enhancement, significantly reducing parameters. Building upon this foundation, we propose a Dual-Frequency Guidance Framework (DFGF) that strategically deploys specialized high/low frequency branches (wavelet-domain high-frequency enhancement and Fourier-domain low-frequency restoration), decoupling frequency processing to substantially reduce computational complexity. Rigorous evaluation across multiple benchmarks demonstrates that SPJFNet not only surpasses state-of-the-art performance but also achieves significant efficiency improvements, substantially reducing model complexity and computational overhead. Code is available at https://github.com/bywlzts/SPJFNet.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u81ea\u6316\u6398\u5148\u9a8c\u5f15\u5bfc\u8054\u5408\u9891\u7387\u589e\u5f3a\u7f51\u7edc\uff08SPJFNet\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6697\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u5728\u6548\u7387\u4e0a\u7684\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6697\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u5b58\u5728\u6548\u7387\u74f6\u9888\uff0c\u4e3b\u8981\u6e90\u4e8e\u5bf9\u5916\u90e8\u5148\u9a8c\u7684\u4f9d\u8d56\u3001\u591a\u9636\u6bb5\u589e\u5f3a\u7ba1\u9053\u7684\u5197\u4f59\u64cd\u4f5c\u4ee5\u53ca\u9891\u7387\u57df\u65b9\u6cd5\u4e2d\u5bf9\u9891\u7387\u7ec4\u4ef6\u7684\u65e0\u5dee\u522b\u5904\u7406\u3002", "method": "SPJFNet\u901a\u8fc7\u81ea\u6316\u6398\u5f15\u5bfc\u6a21\u5757\uff08SMGM\uff09\u751f\u6210\u8f7b\u91cf\u7ea7\u5185\u751f\u5f15\u5bfc\uff0c\u907f\u514d\u5916\u90e8\u5148\u9a8c\u4f9d\u8d56\uff1b\u901a\u8fc7\u65e0\u635f\u5c0f\u6ce2\u5206\u89e3\u548c\u8054\u5408\u5085\u91cc\u53f6\u589e\u5f3a\u4f18\u5316\u9891\u7387\u5904\u7406\uff1b\u63d0\u51fa\u53cc\u9891\u5f15\u5bfc\u6846\u67b6\uff08DFGF\uff09\u5206\u79bb\u9ad8\u4f4e\u9891\u5904\u7406\u3002", "result": "SPJFNet\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e0d\u4ec5\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u8fd8\u663e\u8457\u964d\u4f4e\u4e86\u6a21\u578b\u590d\u6742\u5ea6\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "SPJFNet\u901a\u8fc7\u5185\u751f\u5f15\u5bfc\u548c\u9ad8\u6548\u9891\u7387\u5904\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6697\u56fe\u50cf\u6062\u590d\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2508.04049", "pdf": "https://arxiv.org/pdf/2508.04049", "abs": "https://arxiv.org/abs/2508.04049", "authors": ["Jiayi He", "Xu Wang", "Shengeng Tang", "Yaxiong Wang", "Lechao Cheng", "Dan Guo"], "title": "Motion is the Choreographer: Learning Latent Pose Dynamics for Seamless Sign Language Generation", "categories": ["cs.CV"], "comment": "9 pages, 6 figures", "summary": "Sign language video generation requires producing natural signing motions with realistic appearances under precise semantic control, yet faces two critical challenges: excessive signer-specific data requirements and poor generalization. We propose a new paradigm for sign language video generation that decouples motion semantics from signer identity through a two-phase synthesis framework. First, we construct a signer-independent multimodal motion lexicon, where each gloss is stored as identity-agnostic pose, gesture, and 3D mesh sequences, requiring only one recording per sign. This compact representation enables our second key innovation: a discrete-to-continuous motion synthesis stage that transforms retrieved gloss sequences into temporally coherent motion trajectories, followed by identity-aware neural rendering to produce photorealistic videos of arbitrary signers. Unlike prior work constrained by signer-specific datasets, our method treats motion as a first-class citizen: the learned latent pose dynamics serve as a portable \"choreography layer\" that can be visually realized through different human appearances. Extensive experiments demonstrate that disentangling motion from identity is not just viable but advantageous - enabling both high-quality synthesis and unprecedented flexibility in signer personalization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u624b\u8bed\u89c6\u9891\u751f\u6210\u8303\u5f0f\uff0c\u901a\u8fc7\u89e3\u8026\u8fd0\u52a8\u8bed\u4e49\u4e0e\u8eab\u4efd\u4fe1\u606f\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5408\u6210\u548c\u4e2a\u6027\u5316\u7075\u6d3b\u6027\u3002", "motivation": "\u89e3\u51b3\u624b\u8bed\u89c6\u9891\u751f\u6210\u4e2d\u6570\u636e\u9700\u6c42\u8fc7\u9ad8\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u5408\u6210\u6846\u67b6\uff1a\u6784\u5efa\u8eab\u4efd\u65e0\u5173\u7684\u591a\u6a21\u6001\u8fd0\u52a8\u8bcd\u5178\uff0c\u5e76\u901a\u8fc7\u79bb\u6563\u5230\u8fde\u7eed\u7684\u8fd0\u52a8\u5408\u6210\u4e0e\u8eab\u4efd\u611f\u77e5\u795e\u7ecf\u6e32\u67d3\u751f\u6210\u89c6\u9891\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u89e3\u8026\u8fd0\u52a8\u4e0e\u8eab\u4efd\u4e0d\u4ec5\u53ef\u884c\uff0c\u8fd8\u80fd\u63d0\u5347\u5408\u6210\u8d28\u91cf\u548c\u4e2a\u6027\u5316\u7075\u6d3b\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u624b\u8bed\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.04090", "pdf": "https://arxiv.org/pdf/2508.04090", "abs": "https://arxiv.org/abs/2508.04090", "authors": ["Yi-Ting Chen", "Ting-Hsuan Liao", "Pengsheng Guo", "Alexander Schwing", "Jia-Bin Huang"], "title": "Bridging Diffusion Models and 3D Representations: A 3D Consistent Super-Resolution Framework", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "We propose 3D Super Resolution (3DSR), a novel 3D Gaussian-splatting-based super-resolution framework that leverages off-the-shelf diffusion-based 2D super-resolution models. 3DSR encourages 3D consistency across views via the use of an explicit 3D Gaussian-splatting-based scene representation. This makes the proposed 3DSR different from prior work, such as image upsampling or the use of video super-resolution, which either don't consider 3D consistency or aim to incorporate 3D consistency implicitly. Notably, our method enhances visual quality without additional fine-tuning, ensuring spatial coherence within the reconstructed scene. We evaluate 3DSR on MipNeRF360 and LLFF data, demonstrating that it produces high-resolution results that are visually compelling, while maintaining structural consistency in 3D reconstructions. Code will be released.", "AI": {"tldr": "3DSR\u662f\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u5229\u7528\u73b0\u6210\u76842D\u6269\u6563\u6a21\u578b\u5b9e\u73b03D\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u56fe\u50cf\u4e0a\u91c7\u6837\u6216\u89c6\u9891\u8d85\u5206\u8fa8\u7387\uff09\u672a\u660e\u786e\u8003\u86513D\u4e00\u81f4\u6027\u6216\u4ec5\u9690\u5f0f\u5904\u7406\uff0c3DSR\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u75283D\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u573a\u666f\uff0c\u7ed3\u54082D\u6269\u6563\u6a21\u578b\uff0c\u65e0\u9700\u989d\u5916\u5fae\u8c03\u5373\u53ef\u63d0\u5347\u89c6\u89c9\u8d28\u91cf\u3002", "result": "\u5728MipNeRF360\u548cLLFF\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0c\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u4e14\u89c6\u89c9\u5438\u5f15\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u4fdd\u63013D\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "conclusion": "3DSR\u901a\u8fc7\u663e\u5f0f3D\u8868\u793a\u5b9e\u73b0\u9ad8\u8d28\u91cf\u8d85\u5206\u8fa8\u7387\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2508.04099", "pdf": "https://arxiv.org/pdf/2508.04099", "abs": "https://arxiv.org/abs/2508.04099", "authors": ["Zexu Huang", "Min Xu", "Stuart Perry"], "title": "DET-GS: Depth- and Edge-Aware Regularization for High-Fidelity 3D Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) represents a significant advancement in the field of efficient and high-fidelity novel view synthesis. Despite recent progress, achieving accurate geometric reconstruction under sparse-view conditions remains a fundamental challenge. Existing methods often rely on non-local depth regularization, which fails to capture fine-grained structures and is highly sensitive to depth estimation noise. Furthermore, traditional smoothing methods neglect semantic boundaries and indiscriminately degrade essential edges and textures, consequently limiting the overall quality of reconstruction. In this work, we propose DET-GS, a unified depth and edge-aware regularization framework for 3D Gaussian Splatting. DET-GS introduces a hierarchical geometric depth supervision framework that adaptively enforces multi-level geometric consistency, significantly enhancing structural fidelity and robustness against depth estimation noise. To preserve scene boundaries, we design an edge-aware depth regularization guided by semantic masks derived from Canny edge detection. Furthermore, we introduce an RGB-guided edge-preserving Total Variation loss that selectively smooths homogeneous regions while rigorously retaining high-frequency details and textures. Extensive experiments demonstrate that DET-GS achieves substantial improvements in both geometric accuracy and visual fidelity, outperforming state-of-the-art (SOTA) methods on sparse-view novel view synthesis benchmarks.", "AI": {"tldr": "DET-GS\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6df1\u5ea6\u548c\u8fb9\u7f18\u611f\u77e5\u6b63\u5219\u5316\u6846\u67b6\uff0c\u7528\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u89c6\u56fe\u6761\u4ef6\u4e0b\u7684\u51e0\u4f55\u91cd\u5efa\u7cbe\u5ea6\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "motivation": "\u5728\u7a00\u758f\u89c6\u56fe\u6761\u4ef6\u4e0b\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u51c6\u786e\u7684\u51e0\u4f55\u91cd\u5efa\uff0c\u4e14\u4f20\u7edf\u5e73\u6ed1\u65b9\u6cd5\u4f1a\u7834\u574f\u8bed\u4e49\u8fb9\u754c\u548c\u7ec6\u8282\u3002", "method": "DET-GS\u91c7\u7528\u5206\u5c42\u51e0\u4f55\u6df1\u5ea6\u76d1\u7763\u6846\u67b6\u548c\u8fb9\u7f18\u611f\u77e5\u6df1\u5ea6\u6b63\u5219\u5316\uff0c\u7ed3\u5408RGB\u5f15\u5bfc\u7684\u8fb9\u7f18\u4fdd\u7559\u603b\u53d8\u5dee\u635f\u5931\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDET-GS\u5728\u51e0\u4f55\u7cbe\u5ea6\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DET-GS\u901a\u8fc7\u6df1\u5ea6\u548c\u8fb9\u7f18\u611f\u77e5\u6b63\u5219\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u89c6\u56fe\u91cd\u5efa\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2508.04122", "pdf": "https://arxiv.org/pdf/2508.04122", "abs": "https://arxiv.org/abs/2508.04122", "authors": ["Maximilian Ulmer", "Wout Boerdijk", "Rudolph Triebel", "Maximilian Durner"], "title": "Conditional Latent Diffusion Models for Zero-Shot Instance Segmentation", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "This paper presents OC-DiT, a novel class of diffusion models designed for object-centric prediction, and applies it to zero-shot instance segmentation. We propose a conditional latent diffusion framework that generates instance masks by conditioning the generative process on object templates and image features within the diffusion model's latent space. This allows our model to effectively disentangle object instances through the diffusion process, which is guided by visual object descriptors and localized image cues. Specifically, we introduce two model variants: a coarse model for generating initial object instance proposals, and a refinement model that refines all proposals in parallel. We train these models on a newly created, large-scale synthetic dataset comprising thousands of high-quality object meshes. Remarkably, our model achieves state-of-the-art performance on multiple challenging real-world benchmarks, without requiring any retraining on target data. Through comprehensive ablation studies, we demonstrate the potential of diffusion models for instance segmentation tasks.", "AI": {"tldr": "OC-DiT\u662f\u4e00\u79cd\u7528\u4e8e\u5bf9\u8c61\u4e2d\u5fc3\u9884\u6d4b\u7684\u65b0\u578b\u6269\u6563\u6a21\u578b\uff0c\u5e94\u7528\u4e8e\u96f6\u6837\u672c\u5b9e\u4f8b\u5206\u5272\uff0c\u901a\u8fc7\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u6846\u67b6\u751f\u6210\u5b9e\u4f8b\u63a9\u7801\uff0c\u5e76\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u4e2d\u5bf9\u8c61\u5b9e\u4f8b\u7684\u5206\u79bb\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\uff0c\u65e0\u9700\u5bf9\u76ee\u6807\u6570\u636e\u8fdb\u884c\u91cd\u65b0\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u6846\u67b6\uff0c\u7ed3\u5408\u5bf9\u8c61\u6a21\u677f\u548c\u56fe\u50cf\u7279\u5f81\u751f\u6210\u5b9e\u4f8b\u63a9\u7801\uff1b\u5f15\u5165\u7c97\u7c92\u5ea6\u6a21\u578b\u548c\u7ec6\u5316\u6a21\u578b\u4e24\u4e2a\u53d8\u4f53\uff0c\u5206\u522b\u7528\u4e8e\u751f\u6210\u521d\u59cb\u5bf9\u8c61\u5b9e\u4f8b\u63d0\u6848\u548c\u5e76\u884c\u7ec6\u5316\u63d0\u6848\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f18\u6027\u80fd\uff0c\u65e0\u9700\u5bf9\u76ee\u6807\u6570\u636e\u8fdb\u884c\u91cd\u65b0\u8bad\u7ec3\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u5728\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0cOC-DiT\u901a\u8fc7\u5bf9\u8c61\u4e2d\u5fc3\u9884\u6d4b\u548c\u96f6\u6837\u672c\u80fd\u529b\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.04123", "pdf": "https://arxiv.org/pdf/2508.04123", "abs": "https://arxiv.org/abs/2508.04123", "authors": ["Zheng Cheng", "Wenri Wang", "Guangyong Chen", "Yakun Ju", "Yihua Cheng", "Zhisong Liu", "Yanda Meng", "Jintao Song"], "title": "Excavate the potential of Single-Scale Features: A Decomposition Network for Water-Related Optical Image Enhancement", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Underwater image enhancement (UIE) techniques aim to improve visual quality of images captured in aquatic environments by addressing degradation issues caused by light absorption and scattering effects, including color distortion, blurring, and low contrast. Current mainstream solutions predominantly employ multi-scale feature extraction (MSFE) mechanisms to enhance reconstruction quality through multi-resolution feature fusion. However, our extensive experiments demonstrate that high-quality image reconstruction does not necessarily rely on multi-scale feature fusion. Contrary to popular belief, our experiments show that single-scale feature extraction alone can match or surpass the performance of multi-scale methods, significantly reducing complexity. To comprehensively explore single-scale feature potential in underwater enhancement, we propose an innovative Single-Scale Decomposition Network (SSD-Net). This architecture introduces an asymmetrical decomposition mechanism that disentangles input image into clean layer along with degradation layer. The former contains scene-intrinsic information and the latter encodes medium-induced interference. It uniquely combines CNN's local feature extraction capabilities with Transformer's global modeling strengths through two core modules: 1) Parallel Feature Decomposition Block (PFDB), implementing dual-branch feature space decoupling via efficient attention operations and adaptive sparse transformer; 2) Bidirectional Feature Communication Block (BFCB), enabling cross-layer residual interactions for complementary feature mining and fusion. This synergistic design preserves feature decomposition independence while establishing dynamic cross-layer information pathways, effectively enhancing degradation decoupling capacity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSSD-Net\u7684\u5355\u5c3a\u5ea6\u5206\u89e3\u7f51\u7edc\uff0c\u7528\u4e8e\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\uff0c\u901a\u8fc7\u5355\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u5206\u89e3\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u4e0e\u591a\u5c3a\u5ea6\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u590d\u6742\u5ea6\u3002", "motivation": "\u5f53\u524d\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\u6280\u672f\u4e3b\u8981\u4f9d\u8d56\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\uff0c\u4f46\u5b9e\u9a8c\u8868\u660e\u5355\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u540c\u6837\u6709\u6548\uff0c\u751a\u81f3\u66f4\u4f18\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5355\u5c3a\u5ea6\u7279\u5f81\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faSSD-Net\uff0c\u91c7\u7528\u4e0d\u5bf9\u79f0\u5206\u89e3\u673a\u5236\u5c06\u8f93\u5165\u56fe\u50cf\u5206\u89e3\u4e3a\u5e72\u51c0\u5c42\u548c\u9000\u5316\u5c42\uff0c\u7ed3\u5408CNN\u7684\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u548cTransformer\u7684\u5168\u5c40\u5efa\u6a21\u80fd\u529b\uff0c\u901a\u8fc7PFDB\u548cBFCB\u6a21\u5757\u5b9e\u73b0\u7279\u5f81\u89e3\u8026\u548c\u878d\u5408\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cSSD-Net\u5728\u6027\u80fd\u4e0a\u5339\u914d\u6216\u8d85\u8d8a\u591a\u5c3a\u5ea6\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u5355\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u5728\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\u4e2d\u5177\u6709\u6f5c\u529b\uff0cSSD-Net\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u589e\u5f3a\u6548\u679c\u3002"}}
{"id": "2508.04147", "pdf": "https://arxiv.org/pdf/2508.04147", "abs": "https://arxiv.org/abs/2508.04147", "authors": ["Lijuan Liu", "Wenfa Li", "Dongbo Zhang", "Shuo Wang", "Shaohui Jiao"], "title": "IDCNet: Guided Video Diffusion for Metric-Consistent RGBD Scene Generation with Precise Camera Control", "categories": ["cs.CV"], "comment": "10 pages, 7 figures", "summary": "We present IDC-Net (Image-Depth Consistency Network), a novel framework designed to generate RGB-D video sequences under explicit camera trajectory control. Unlike approaches that treat RGB and depth generation separately, IDC-Net jointly synthesizes both RGB images and corresponding depth maps within a unified geometry-aware diffusion model. The joint learning framework strengthens spatial and geometric alignment across frames, enabling more precise camera control in the generated sequences. To support the training of this camera-conditioned model and ensure high geometric fidelity, we construct a camera-image-depth consistent dataset with metric-aligned RGB videos, depth maps, and accurate camera poses, which provides precise geometric supervision with notably improved inter-frame geometric consistency. Moreover, we introduce a geometry-aware transformer block that enables fine-grained camera control, enhancing control over the generated sequences. Extensive experiments show that IDC-Net achieves improvements over state-of-the-art approaches in both visual quality and geometric consistency of generated scene sequences. Notably, the generated RGB-D sequences can be directly feed for downstream 3D Scene reconstruction tasks without extra post-processing steps, showcasing the practical benefits of our joint learning framework. See more at https://idcnet-scene.github.io.", "AI": {"tldr": "IDC-Net\u662f\u4e00\u4e2a\u8054\u5408\u751f\u6210RGB\u56fe\u50cf\u548c\u6df1\u5ea6\u56fe\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u7cbe\u786e\u7684\u76f8\u673a\u8f68\u8ff9\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06RGB\u548c\u6df1\u5ea6\u751f\u6210\u5206\u5f00\u5904\u7406\uff0c\u5bfc\u81f4\u51e0\u4f55\u4e00\u81f4\u6027\u4e0d\u8db3\uff0cIDC-Net\u65e8\u5728\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u63d0\u5347\u7a7a\u95f4\u548c\u51e0\u4f55\u5bf9\u9f50\u3002", "method": "IDC-Net\u91c7\u7528\u7edf\u4e00\u7684\u51e0\u4f55\u611f\u77e5\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u76f8\u673a-\u56fe\u50cf-\u6df1\u5ea6\u4e00\u81f4\u7684\u6570\u636e\u96c6\u548c\u51e0\u4f55\u611f\u77e5Transformer\u5757\uff0c\u5b9e\u73b0\u7cbe\u7ec6\u76f8\u673a\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cIDC-Net\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u7684RGB-D\u5e8f\u5217\u53ef\u76f4\u63a5\u7528\u4e8e\u4e0b\u6e383D\u573a\u666f\u91cd\u5efa\u4efb\u52a1\u3002", "conclusion": "IDC-Net\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86RGB-D\u5e8f\u5217\u7684\u751f\u6210\u8d28\u91cf\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.04153", "pdf": "https://arxiv.org/pdf/2508.04153", "abs": "https://arxiv.org/abs/2508.04153", "authors": ["Yihua Shao", "Xiaofeng Lin", "Xinwei Long", "Siyu Chen", "Minxi Yan", "Yang Liu", "Ziyang Yan", "Ao Ma", "Hao Tang", "Jingcai Guo"], "title": "ICM-Fusion: In-Context Meta-Optimized LoRA Fusion for Multi-Task Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Enabling multi-task adaptation in pre-trained Low-Rank Adaptation (LoRA) models is crucial for enhancing their generalization capabilities. Most existing pre-trained LoRA fusion methods decompose weight matrices, sharing similar parameters while merging divergent ones. However, this paradigm inevitably induces inter-weight conflicts and leads to catastrophic domain forgetting. While incremental learning enables adaptation to multiple tasks, it struggles to achieve generalization in few-shot scenarios. Consequently, when the weight data follows a long-tailed distribution, it can lead to forgetting in the fused weights. To address this issue, we propose In-Context Meta LoRA Fusion (ICM-Fusion), a novel framework that synergizes meta-learning with in-context adaptation. The key innovation lies in our task vector arithmetic, which dynamically balances conflicting optimization directions across domains through learned manifold projections. ICM-Fusion obtains the optimal task vector orientation for the fused model in the latent space by adjusting the orientation of the task vectors. Subsequently, the fused LoRA is reconstructed by a self-designed Fusion VAE (F-VAE) to realize multi-task LoRA generation. We have conducted extensive experiments on visual and linguistic tasks, and the experimental results demonstrate that ICM-Fusion can be adapted to a wide range of architectural models and applied to various tasks. Compared to the current pre-trained LoRA fusion method, ICM-Fusion fused LoRA can significantly reduce the multi-tasking loss and can even achieve task enhancement in few-shot scenarios.", "AI": {"tldr": "ICM-Fusion\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5143\u5b66\u4e60\u548c\u4e0a\u4e0b\u6587\u9002\u5e94\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5e73\u8861\u4efb\u52a1\u5411\u91cf\u65b9\u5411\uff0c\u663e\u8457\u51cf\u5c11\u591a\u4efb\u52a1\u635f\u5931\u5e76\u63d0\u5347\u5c11\u6837\u672c\u573a\u666f\u4e0b\u7684\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709LoRA\u878d\u5408\u65b9\u6cd5\u4e2d\u56e0\u6743\u91cd\u51b2\u7a81\u548c\u9886\u57df\u9057\u5fd8\u5bfc\u81f4\u7684\u591a\u4efb\u52a1\u9002\u5e94\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u5143\u5b66\u4e60\u548c\u4e0a\u4e0b\u6587\u9002\u5e94\uff0c\u901a\u8fc7\u4efb\u52a1\u5411\u91cf\u7b97\u672f\u52a8\u6001\u5e73\u8861\u4f18\u5316\u65b9\u5411\uff0c\u5e76\u4f7f\u7528Fusion VAE\u91cd\u6784\u878d\u5408LoRA\u3002", "result": "\u5b9e\u9a8c\u8868\u660eICM-Fusion\u80fd\u663e\u8457\u51cf\u5c11\u591a\u4efb\u52a1\u635f\u5931\uff0c\u5e76\u5728\u5c11\u6837\u672c\u573a\u666f\u4e2d\u5b9e\u73b0\u4efb\u52a1\u589e\u5f3a\u3002", "conclusion": "ICM-Fusion\u662f\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u591a\u4efb\u52a1LoRA\u878d\u5408\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u67b6\u6784\u548c\u4efb\u52a1\u3002"}}
{"id": "2508.04161", "pdf": "https://arxiv.org/pdf/2508.04161", "abs": "https://arxiv.org/abs/2508.04161", "authors": ["Yuqin Cao", "Yixuan Gao", "Wei Sun", "Xiaohong Liu", "Yulun Zhang", "Xiongkuo Min"], "title": "Audio-Assisted Face Video Restoration with Temporal and Identity Complementary Learning", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "comment": null, "summary": "Face videos accompanied by audio have become integral to our daily lives, while they often suffer from complex degradations. Most face video restoration methods neglect the intrinsic correlations between the visual and audio features, especially in mouth regions. A few audio-aided face video restoration methods have been proposed, but they only focus on compression artifact removal. In this paper, we propose a General Audio-assisted face Video restoration Network (GAVN) to address various types of streaming video distortions via identity and temporal complementary learning. Specifically, GAVN first captures inter-frame temporal features in the low-resolution space to restore frames coarsely and save computational cost. Then, GAVN extracts intra-frame identity features in the high-resolution space with the assistance of audio signals and face landmarks to restore more facial details. Finally, the reconstruction module integrates temporal features and identity features to generate high-quality face videos. Experimental results demonstrate that GAVN outperforms the existing state-of-the-art methods on face video compression artifact removal, deblurring, and super-resolution. Codes will be released upon publication.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u97f3\u9891\u8f85\u52a9\u4eba\u8138\u89c6\u9891\u4fee\u590d\u7f51\u7edc\uff08GAVN\uff09\uff0c\u901a\u8fc7\u8eab\u4efd\u548c\u65f6\u95f4\u4e92\u8865\u5b66\u4e60\u89e3\u51b3\u591a\u79cd\u89c6\u9891\u5931\u771f\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u89c6\u89c9\u4e0e\u97f3\u9891\u7279\u5f81\u7684\u5185\u5728\u5173\u8054\uff0c\u5c24\u5176\u662f\u5634\u90e8\u533a\u57df\uff0c\u4e14\u4ec5\u5173\u6ce8\u538b\u7f29\u4f2a\u5f71\u53bb\u9664\u3002", "method": "GAVN\u5206\u4e24\u6b65\uff1a\u4f4e\u5206\u8fa8\u7387\u7a7a\u95f4\u6355\u83b7\u5e27\u95f4\u65f6\u95f4\u7279\u5f81\u8fdb\u884c\u7c97\u4fee\u590d\uff0c\u9ad8\u5206\u8fa8\u7387\u7a7a\u95f4\u7ed3\u5408\u97f3\u9891\u4fe1\u53f7\u548c\u9762\u90e8\u6807\u5fd7\u63d0\u53d6\u5e27\u5185\u8eab\u4efd\u7279\u5f81\u4ee5\u6062\u590d\u7ec6\u8282\u3002", "result": "GAVN\u5728\u538b\u7f29\u4f2a\u5f71\u53bb\u9664\u3001\u53bb\u6a21\u7cca\u548c\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GAVN\u901a\u8fc7\u7ed3\u5408\u97f3\u9891\u8f85\u52a9\u548c\u4e92\u8865\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4eba\u8138\u89c6\u9891\u4fee\u590d\u8d28\u91cf\u3002"}}
{"id": "2508.04176", "pdf": "https://arxiv.org/pdf/2508.04176", "abs": "https://arxiv.org/abs/2508.04176", "authors": ["Jin Kuang", "Dong Liu", "Yukuang Zhang", "Shengsheng Wang"], "title": "Uncertainty-Aware Spatial Color Correlation for Low-Light Image Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Most existing low-light image enhancement approaches primarily focus on architectural innovations, while often overlooking the intrinsic uncertainty within feature representations particularly under extremely dark conditions where degraded gradient and noise dominance severely impair model reliability and causal reasoning. To address these issues, we propose U2CLLIE, a novel framework that integrates uncertainty-aware enhancement and spatial-color causal correlation modeling. From the perspective of entropy-based uncertainty, our framework introduces two key components: (1) An Uncertainty-Aware Dual-domain Denoise (UaD) Module, which leverages Gaussian-Guided Adaptive Frequency Domain Feature Enhancement (G2AF) to suppress frequency-domain noise and optimize entropy-driven representations. This module enhances spatial texture extraction and frequency-domain noise suppression/structure refinement, effectively mitigating gradient vanishing and noise dominance. (2) A hierarchical causality-aware framework, where a Luminance Enhancement Network (LEN) first performs coarse brightness enhancement on dark regions. Then, during the encoder-decoder phase, two asymmetric causal correlation modeling modules Neighborhood Correlation State Space (NeCo) and Adaptive Spatial-Color Calibration (AsC) collaboratively construct hierarchical causal constraints. These modules reconstruct and reinforce neighborhood structure and color consistency in the feature space. Extensive experiments demonstrate that U2CLLIE achieves state-of-the-art performance across multiple benchmark datasets, exhibiting robust performance and strong generalization across various scenes.", "AI": {"tldr": "U2CLLIE\u662f\u4e00\u4e2a\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u589e\u5f3a\u548c\u7a7a\u95f4-\u989c\u8272\u56e0\u679c\u5efa\u6a21\u7684\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u57df\u53bb\u566a\u548c\u5c42\u6b21\u56e0\u679c\u7ea6\u675f\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u7279\u5f81\u8868\u793a\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5c24\u5176\u5728\u6781\u6697\u6761\u4ef6\u4e0b\uff0c\u68af\u5ea6\u9000\u5316\u548c\u566a\u58f0\u4e3b\u5bfc\u4f1a\u635f\u5bb3\u6a21\u578b\u53ef\u9760\u6027\u548c\u56e0\u679c\u63a8\u7406\u3002", "method": "\u63d0\u51faUaD\u6a21\u5757\uff08\u9ad8\u65af\u5f15\u5bfc\u81ea\u9002\u5e94\u9891\u57df\u7279\u5f81\u589e\u5f3a\uff09\u548c\u5c42\u6b21\u56e0\u679c\u6846\u67b6\uff08LEN\u3001NeCo\u3001AsC\u6a21\u5757\uff09\uff0c\u5206\u522b\u4f18\u5316\u7279\u5f81\u8868\u793a\u548c\u5efa\u6a21\u56e0\u679c\u7ea6\u675f\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "U2CLLIE\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u548c\u56e0\u679c\u5efa\u6a21\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4e2d\u7684\u5173\u952e\u95ee\u9898\u3002"}}
{"id": "2508.04181", "pdf": "https://arxiv.org/pdf/2508.04181", "abs": "https://arxiv.org/abs/2508.04181", "authors": ["Sungrae Hong"], "title": "Deeper Inside Deep ViT", "categories": ["cs.CV"], "comment": "8 pages, 5 figures", "summary": "There have been attempts to create large-scale structures in vision models similar to LLM, such as ViT-22B. While this research has provided numerous analyses and insights, our understanding of its practical utility remains incomplete. Therefore, we examine how this model structure reacts and train in a local environment. We also highlight the instability in training and make some model modifications to stabilize it. The ViT-22B model, trained from scratch, overall outperformed ViT in terms of performance under the same parameter size. Additionally, we venture into the task of image generation, which has not been attempted in ViT-22B. We propose an image generation architecture using ViT and investigate which between ViT and ViT-22B is a more suitable structure for image generation.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86ViT-22B\u6a21\u578b\u5728\u672c\u5730\u73af\u5883\u4e2d\u7684\u8bad\u7ec3\u8868\u73b0\uff0c\u6539\u8fdb\u4e86\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\uff0c\u5e76\u5c1d\u8bd5\u4e86\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u3002", "motivation": "\u5c3d\u7ba1ViT-22B\u7b49\u5927\u89c4\u6a21\u89c6\u89c9\u6a21\u578b\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u5c1a\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u9700\u8981\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u8bad\u7ec3\u8868\u73b0\u548c\u6269\u5c55\u529f\u80fd\u3002", "method": "\u5206\u6790\u4e86ViT-22B\u5728\u672c\u5730\u73af\u5883\u4e2d\u7684\u8bad\u7ec3\u884c\u4e3a\uff0c\u63d0\u51fa\u6539\u8fdb\u4ee5\u7a33\u5b9a\u8bad\u7ec3\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8eViT\u7684\u56fe\u50cf\u751f\u6210\u67b6\u6784\u3002", "result": "ViT-22B\u5728\u76f8\u540c\u53c2\u6570\u89c4\u6a21\u4e0b\u6027\u80fd\u4f18\u4e8eViT\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u3002", "conclusion": "ViT-22B\u5728\u6027\u80fd\u548c\u6269\u5c55\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u5408\u8fdb\u4e00\u6b65\u63a2\u7d22\u56fe\u50cf\u751f\u6210\u7b49\u65b0\u4efb\u52a1\u3002"}}
{"id": "2508.04190", "pdf": "https://arxiv.org/pdf/2508.04190", "abs": "https://arxiv.org/abs/2508.04190", "authors": ["Fengyi Wu", "Yimian Dai", "Tianfang Zhang", "Yixuan Ding", "Jian Yang", "Ming-Ming Cheng", "Zhenming Peng"], "title": "RPCANet++: Deep Interpretable Robust PCA for Sparse Object Segmentation", "categories": ["cs.CV"], "comment": "Project Webpage: https://fengyiwu98.github.io/rpcanetx", "summary": "Robust principal component analysis (RPCA) decomposes an observation matrix into low-rank background and sparse object components. This capability has enabled its application in tasks ranging from image restoration to segmentation. However, traditional RPCA models suffer from computational burdens caused by matrix operations, reliance on finely tuned hyperparameters, and rigid priors that limit adaptability in dynamic scenarios. To solve these limitations, we propose RPCANet++, a sparse object segmentation framework that fuses the interpretability of RPCA with efficient deep architectures. Our approach unfolds a relaxed RPCA model into a structured network comprising a Background Approximation Module (BAM), an Object Extraction Module (OEM), and an Image Restoration Module (IRM). To mitigate inter-stage transmission loss in the BAM, we introduce a Memory-Augmented Module (MAM) to enhance background feature preservation, while a Deep Contrast Prior Module (DCPM) leverages saliency cues to expedite object extraction. Extensive experiments on diverse datasets demonstrate that RPCANet++ achieves state-of-the-art performance under various imaging scenarios. We further improve interpretability via visual and numerical low-rankness and sparsity measurements. By combining the theoretical strengths of RPCA with the efficiency of deep networks, our approach sets a new baseline for reliable and interpretable sparse object segmentation. Codes are available at our Project Webpage https://fengyiwu98.github.io/rpcanetx.", "AI": {"tldr": "RPCANet++ \u662f\u4e00\u4e2a\u7ed3\u5408\u4e86\u9c81\u68d2\u4e3b\u6210\u5206\u5206\u6790\uff08RPCA\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u9ad8\u6548\u7a00\u758f\u76ee\u6807\u5206\u5272\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf RPCA \u7684\u8ba1\u7b97\u8d1f\u62c5\u548c\u9002\u5e94\u6027\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf RPCA \u6a21\u578b\u5b58\u5728\u8ba1\u7b97\u8d1f\u62c5\u5927\u3001\u4f9d\u8d56\u7cbe\u7ec6\u8c03\u53c2\u548c\u52a8\u6001\u573a\u666f\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u65b9\u6cd5\u3002", "method": "RPCANet++ \u5c06\u677e\u5f1b\u7684 RPCA \u6a21\u578b\u5c55\u5f00\u4e3a\u7ed3\u6784\u5316\u7f51\u7edc\uff0c\u5305\u542b\u80cc\u666f\u8fd1\u4f3c\u6a21\u5757\uff08BAM\uff09\u3001\u76ee\u6807\u63d0\u53d6\u6a21\u5757\uff08OEM\uff09\u548c\u56fe\u50cf\u6062\u590d\u6a21\u5757\uff08IRM\uff09\uff0c\u5e76\u5f15\u5165\u8bb0\u5fc6\u589e\u5f3a\u6a21\u5757\uff08MAM\uff09\u548c\u6df1\u5ea6\u5bf9\u6bd4\u5148\u9a8c\u6a21\u5757\uff08DCPM\uff09\u4f18\u5316\u6027\u80fd\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRPCANet++ \u5728\u5404\u79cd\u6210\u50cf\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u53ef\u89c6\u5316\u548c\u6570\u503c\u6d4b\u91cf\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "RPCANet++ \u7ed3\u5408\u4e86 RPCA \u7684\u7406\u8bba\u4f18\u52bf\u548c\u6df1\u5ea6\u7f51\u7edc\u7684\u6548\u7387\uff0c\u4e3a\u53ef\u9760\u4e14\u53ef\u89e3\u91ca\u7684\u7a00\u758f\u76ee\u6807\u5206\u5272\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2508.04224", "pdf": "https://arxiv.org/pdf/2508.04224", "abs": "https://arxiv.org/abs/2508.04224", "authors": ["Jiahui Li", "Shengeng Tang", "Jingxuan He", "Gang Huang", "Zhangye Wang", "Yantao Pan", "Lechao Cheng"], "title": "SplitGaussian: Reconstructing Dynamic Scenes via Visual Geometry Decomposition", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing dynamic 3D scenes from monocular video remains fundamentally challenging due to the need to jointly infer motion, structure, and appearance from limited observations. Existing dynamic scene reconstruction methods based on Gaussian Splatting often entangle static and dynamic elements in a shared representation, leading to motion leakage, geometric distortions, and temporal flickering. We identify that the root cause lies in the coupled modeling of geometry and appearance across time, which hampers both stability and interpretability. To address this, we propose \\textbf{SplitGaussian}, a novel framework that explicitly decomposes scene representations into static and dynamic components. By decoupling motion modeling from background geometry and allowing only the dynamic branch to deform over time, our method prevents motion artifacts in static regions while supporting view- and time-dependent appearance refinement. This disentangled design not only enhances temporal consistency and reconstruction fidelity but also accelerates convergence. Extensive experiments demonstrate that SplitGaussian outperforms prior state-of-the-art methods in rendering quality, geometric stability, and motion separation.", "AI": {"tldr": "SplitGaussian\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5206\u89e3\u9759\u6001\u548c\u52a8\u6001\u7ec4\u4ef6\uff0c\u89e3\u51b3\u4e86\u52a8\u60013D\u573a\u666f\u91cd\u5efa\u4e2d\u7684\u8fd0\u52a8\u6cc4\u6f0f\u548c\u51e0\u4f55\u5931\u771f\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u7684\u52a8\u6001\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\u5c06\u9759\u6001\u548c\u52a8\u6001\u5143\u7d20\u8026\u5408\u8868\u793a\uff0c\u5bfc\u81f4\u8fd0\u52a8\u6cc4\u6f0f\u3001\u51e0\u4f55\u5931\u771f\u548c\u65f6\u95f4\u95ea\u70c1\u3002", "method": "SplitGaussian\u5c06\u573a\u666f\u8868\u793a\u5206\u89e3\u4e3a\u9759\u6001\u548c\u52a8\u6001\u7ec4\u4ef6\uff0c\u89e3\u8026\u8fd0\u52a8\u5efa\u6a21\u4e0e\u80cc\u666f\u51e0\u4f55\uff0c\u4ec5\u52a8\u6001\u5206\u652f\u968f\u65f6\u95f4\u53d8\u5f62\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSplitGaussian\u5728\u6e32\u67d3\u8d28\u91cf\u3001\u51e0\u4f55\u7a33\u5b9a\u6027\u548c\u8fd0\u52a8\u5206\u79bb\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SplitGaussian\u901a\u8fc7\u89e3\u8026\u8bbe\u8ba1\u63d0\u9ad8\u4e86\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u91cd\u5efa\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u52a0\u901f\u4e86\u6536\u655b\u3002"}}
{"id": "2508.04228", "pdf": "https://arxiv.org/pdf/2508.04228", "abs": "https://arxiv.org/abs/2508.04228", "authors": ["Kangrui Cen", "Baixuan Zhao", "Yi Xin", "Siqi Luo", "Guangtao Zhai", "Xiaohong Liu"], "title": "LayerT2V: Interactive Multi-Object Trajectory Layering for Video Generation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": "Project webpage: https://kr-panghu.github.io/LayerT2V/", "summary": "Controlling object motion trajectories in Text-to-Video (T2V) generation is a challenging and relatively under-explored area, particularly in scenarios involving multiple moving objects. Most community models and datasets in the T2V domain are designed for single-object motion, limiting the performance of current generative models in multi-object tasks. Additionally, existing motion control methods in T2V either lack support for multi-object motion scenes or experience severe performance degradation when object trajectories intersect, primarily due to the semantic conflicts in colliding regions. To address these limitations, we introduce LayerT2V, the first approach for generating video by compositing background and foreground objects layer by layer. This layered generation enables flexible integration of multiple independent elements within a video, positioning each element on a distinct \"layer\" and thus facilitating coherent multi-object synthesis while enhancing control over the generation process. Extensive experiments demonstrate the superiority of LayerT2V in generating complex multi-object scenarios, showcasing 1.4x and 4.5x improvements in mIoU and AP50 metrics over state-of-the-art (SOTA) methods. Project page and code are available at https://kr-panghu.github.io/LayerT2V/ .", "AI": {"tldr": "LayerT2V\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u751f\u6210\u89c6\u9891\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u591a\u5bf9\u8c61\u8fd0\u52a8\u8f68\u8ff9\u63a7\u5236\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\uff08T2V\uff09\u5728\u591a\u5bf9\u8c61\u8fd0\u52a8\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u7531\u4e8e\u8bed\u4e49\u51b2\u7a81\u548c\u7f3a\u4e4f\u591a\u5bf9\u8c61\u652f\u6301\u3002", "method": "\u901a\u8fc7\u5206\u5c42\u751f\u6210\u80cc\u666f\u548c\u524d\u666f\u5bf9\u8c61\uff0c\u5c06\u6bcf\u4e2a\u5143\u7d20\u7f6e\u4e8e\u72ec\u7acb\u5c42\u4e0a\uff0c\u5b9e\u73b0\u591a\u5bf9\u8c61\u5408\u6210\u7684\u7075\u6d3b\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cLayerT2V\u5728mIoU\u548cAP50\u6307\u6807\u4e0a\u5206\u522b\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u4e861.4\u500d\u548c4.5\u500d\u3002", "conclusion": "LayerT2V\u5728\u591a\u5bf9\u8c61\u89c6\u9891\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u590d\u6742\u573a\u666f\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.04229", "pdf": "https://arxiv.org/pdf/2508.04229", "abs": "https://arxiv.org/abs/2508.04229", "authors": ["Yu Liu", "Zhijie Liu", "Xiao Ren", "You-Fu Li", "He Kong"], "title": "Intention Enhanced Diffusion Model for Multimodal Pedestrian Trajectory Prediction", "categories": ["cs.CV"], "comment": "To be presented at the 28th IEEE International Conference on   Intelligent Transportation Systems (ITSC), 2025", "summary": "Predicting pedestrian motion trajectories is critical for path planning and motion control of autonomous vehicles. However, accurately forecasting crowd trajectories remains a challenging task due to the inherently multimodal and uncertain nature of human motion. Recent diffusion-based models have shown promising results in capturing the stochasticity of pedestrian behavior for trajectory prediction. However, few diffusion-based approaches explicitly incorporate the underlying motion intentions of pedestrians, which can limit the interpretability and precision of prediction models. In this work, we propose a diffusion-based multimodal trajectory prediction model that incorporates pedestrians' motion intentions into the prediction framework. The motion intentions are decomposed into lateral and longitudinal components, and a pedestrian intention recognition module is introduced to enable the model to effectively capture these intentions. Furthermore, we adopt an efficient guidance mechanism that facilitates the generation of interpretable trajectories. The proposed framework is evaluated on two widely used human trajectory prediction benchmarks, ETH and UCY, on which it is compared against state-of-the-art methods. The experimental results demonstrate that our method achieves competitive performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u591a\u6a21\u6001\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff0c\u7ed3\u5408\u884c\u4eba\u8fd0\u52a8\u610f\u56fe\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u5bf9\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6269\u6563\u6a21\u578b\u672a\u5145\u5206\u7ed3\u5408\u884c\u4eba\u8fd0\u52a8\u610f\u56fe\uff0c\u9650\u5236\u4e86\u9884\u6d4b\u7684\u7cbe\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u6563\u6a21\u578b\uff0c\u5206\u89e3\u884c\u4eba\u8fd0\u52a8\u610f\u56fe\u4e3a\u6a2a\u5411\u548c\u7eb5\u5411\u5206\u91cf\uff0c\u5e76\u5f15\u5165\u610f\u56fe\u8bc6\u522b\u6a21\u5757\u548c\u9ad8\u6548\u5f15\u5bfc\u673a\u5236\u3002", "result": "\u5728ETH\u548cUCY\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u4f18\u5f02\uff0c\u4e0e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u7ed3\u5408\u884c\u4eba\u8fd0\u52a8\u610f\u56fe\u7684\u6269\u6563\u6a21\u578b\u80fd\u6709\u6548\u63d0\u5347\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2508.04297", "pdf": "https://arxiv.org/pdf/2508.04297", "abs": "https://arxiv.org/abs/2508.04297", "authors": ["Yaopeng Lou", "Liao Shen", "Tianqi Liu", "Jiaqi Li", "Zihao Huang", "Huiqiang Sun", "Zhiguo Cao"], "title": "MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction", "categories": ["cs.CV"], "comment": "This work is accepted by ICCV 2025", "summary": "We present Multi-Baseline Gaussian Splatting (MuRF), a generalized feed-forward approach for novel view synthesis that effectively handles diverse baseline settings, including sparse input views with both small and large baselines. Specifically, we integrate features from Multi-View Stereo (MVS) and Monocular Depth Estimation (MDE) to enhance feature representations for generalizable reconstruction. Next, We propose a projection-and-sampling mechanism for deep depth fusion, which constructs a fine probability volume to guide the regression of the feature map. Furthermore, We introduce a reference-view loss to improve geometry and optimization efficiency. We leverage 3D Gaussian representations to accelerate training and inference time while enhancing rendering quality. MuRF achieves state-of-the-art performance across multiple baseline settings and diverse scenarios ranging from simple objects (DTU) to complex indoor and outdoor scenes (RealEstate10K). We also demonstrate promising zero-shot performance on the LLFF and Mip-NeRF 360 datasets.", "AI": {"tldr": "MuRF\u662f\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u56fe\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u89c6\u89d2\u7acb\u4f53\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7279\u5f81\uff0c\u4ee5\u53ca\u6df1\u5ea6\u878d\u5408\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5728\u7a00\u758f\u8f93\u5165\u89c6\u56fe\u4e0b\u7684\u9ad8\u8d28\u91cf\u6e32\u67d3\u3002", "motivation": "\u89e3\u51b3\u591a\u57fa\u7ebf\u8bbe\u7f6e\u4e0b\uff08\u5305\u62ec\u7a00\u758f\u8f93\u5165\u89c6\u56fe\uff09\u7684\u65b0\u9896\u89c6\u56fe\u5408\u6210\u95ee\u9898\uff0c\u63d0\u5347\u6e32\u67d3\u8d28\u91cf\u548c\u6548\u7387\u3002", "method": "\u7ed3\u5408MVS\u548cMDE\u7279\u5f81\uff0c\u63d0\u51fa\u6df1\u5ea6\u878d\u5408\u673a\u5236\u548c\u53c2\u8003\u89c6\u56fe\u635f\u5931\uff0c\u5229\u75283D\u9ad8\u65af\u8868\u793a\u52a0\u901f\u8bad\u7ec3\u548c\u63a8\u7406\u3002", "result": "\u5728DTU\u3001RealEstate10K\u7b49\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5e76\u5728LLFF\u548cMip-NeRF 360\u4e0a\u5c55\u793a\u4e86\u96f6\u6837\u672c\u6027\u80fd\u3002", "conclusion": "MuRF\u5728\u591a\u57fa\u7ebf\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2508.04324", "pdf": "https://arxiv.org/pdf/2508.04324", "abs": "https://arxiv.org/abs/2508.04324", "authors": ["Xiaoxuan He", "Siming Fu", "Yuke Zhao", "Wanli Li", "Jian Yang", "Dacheng Yin", "Fengyun Rao", "Bo Zhang"], "title": "TempFlow-GRPO: When Timing Matters for GRPO in Flow Models", "categories": ["cs.CV"], "comment": null, "summary": "Recent flow matching models for text-to-image generation have achieved remarkable quality, yet their integration with reinforcement learning for human preference alignment remains suboptimal, hindering fine-grained reward-based optimization. We observe that the key impediment to effective GRPO training of flow models is the temporal uniformity assumption in existing approaches: sparse terminal rewards with uniform credit assignment fail to capture the varying criticality of decisions across generation timesteps, resulting in inefficient exploration and suboptimal convergence. To remedy this shortcoming, we introduce \\textbf{TempFlow-GRPO} (Temporal Flow GRPO), a principled GRPO framework that captures and exploits the temporal structure inherent in flow-based generation. TempFlow-GRPO introduces two key innovations: (i) a trajectory branching mechanism that provides process rewards by concentrating stochasticity at designated branching points, enabling precise credit assignment without requiring specialized intermediate reward models; and (ii) a noise-aware weighting scheme that modulates policy optimization according to the intrinsic exploration potential of each timestep, prioritizing learning during high-impact early stages while ensuring stable refinement in later phases. These innovations endow the model with temporally-aware optimization that respects the underlying generative dynamics, leading to state-of-the-art performance in human preference alignment and standard text-to-image benchmarks.", "AI": {"tldr": "TempFlow-GRPO\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684GRPO\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u611f\u77e5\u4f18\u5316\u6539\u8fdb\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6d41\u5339\u914d\u6a21\u578b\u4e2d\u91c7\u7528\u5747\u5300\u65f6\u95f4\u5047\u8bbe\uff0c\u5bfc\u81f4\u5956\u52b1\u5206\u914d\u4e0d\u7cbe\u786e\uff0c\u63a2\u7d22\u6548\u7387\u4f4e\u3002", "method": "\u5f15\u5165\u8f68\u8ff9\u5206\u652f\u673a\u5236\u548c\u566a\u58f0\u611f\u77e5\u6743\u91cd\u65b9\u6848\uff0c\u5206\u522b\u4f18\u5316\u5956\u52b1\u5206\u914d\u548c\u65f6\u95f4\u6b65\u5b66\u4e60\u4f18\u5148\u7ea7\u3002", "result": "\u5728\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u548c\u6807\u51c6\u6587\u672c\u5230\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "TempFlow-GRPO\u901a\u8fc7\u65f6\u95f4\u7ed3\u6784\u611f\u77e5\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2508.04441", "pdf": "https://arxiv.org/pdf/2508.04441", "abs": "https://arxiv.org/abs/2508.04441", "authors": ["Jonas Ammeling", "Jonathan Ganz", "Emely Rosbach", "Ludwig Lausser", "Christof A. Bertram", "Katharina Breininger", "Marc Aubreville"], "title": "Benchmarking Foundation Models for Mitotic Figure Classification", "categories": ["cs.CV"], "comment": null, "summary": "The performance of deep learning models is known to scale with data quantity and diversity. In pathology, as in many other medical imaging domains, the availability of labeled images for a specific task is often limited. Self-supervised learning techniques have enabled the use of vast amounts of unlabeled data to train large-scale neural networks, i.e., foundation models, that can address the limited data problem by providing semantically rich feature vectors that can generalize well to new tasks with minimal training effort increasing model performance and robustness. In this work, we investigate the use of foundation models for mitotic figure classification. The mitotic count, which can be derived from this classification task, is an independent prognostic marker for specific tumors and part of certain tumor grading systems. In particular, we investigate the data scaling laws on multiple current foundation models and evaluate their robustness to unseen tumor domains. Next to the commonly used linear probing paradigm, we also adapt the models using low-rank adaptation (LoRA) of their attention mechanisms. We compare all models against end-to-end-trained baselines, both CNNs and Vision Transformers. Our results demonstrate that LoRA-adapted foundation models provide superior performance to those adapted with standard linear probing, reaching performance levels close to 100% data availability with only 10% of training data. Furthermore, LoRA-adaptation of the most recent foundation models almost closes the out-of-domain performance gap when evaluated on unseen tumor domains. However, full fine-tuning of traditional architectures still yields competitive performance.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u57fa\u7840\u6a21\u578b\u5728\u75c5\u7406\u5b66\u6709\u4e1d\u5206\u88c2\u5206\u7c7b\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u75c5\u7406\u5b66\u4e2d\u6709\u4e1d\u5206\u88c2\u5206\u7c7b\u4efb\u52a1\u4e2d\u6807\u8bb0\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\uff0c\u5229\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u57fa\u7840\u6a21\u578b\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u57fa\u7840\u6a21\u578b\uff0c\u91c7\u7528\u7ebf\u6027\u63a2\u6d4b\u548cLoRA\u9002\u5e94\u65b9\u6cd5\uff0c\u5e76\u4e0e\u7aef\u5230\u7aef\u8bad\u7ec3\u7684CNN\u548cVision Transformer\u57fa\u7ebf\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "LoRA\u9002\u5e94\u65b9\u6cd5\u5728\u4ec510%\u8bad\u7ec3\u6570\u636e\u4e0b\u63a5\u8fd1100%\u6570\u636e\u53ef\u7528\u6027\u7684\u6027\u80fd\uff0c\u4e14\u5728\u672a\u89c1\u80bf\u7624\u57df\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "LoRA\u9002\u5e94\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u57fa\u7840\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u4f20\u7edf\u67b6\u6784\u7684\u5b8c\u5168\u5fae\u8c03\u4ecd\u5177\u7ade\u4e89\u529b\u3002"}}
{"id": "2508.04467", "pdf": "https://arxiv.org/pdf/2508.04467", "abs": "https://arxiv.org/abs/2508.04467", "authors": ["Shuzhou Yang", "Xiaodong Cun", "Xiaoyu Li", "Yaowei Li", "Jian Zhang"], "title": "4DVD: Cascaded Dense-view Video Diffusion Model for High-quality 4D Content Generation", "categories": ["cs.CV"], "comment": null, "summary": "Given the high complexity of directly generating high-dimensional data such as 4D, we present 4DVD, a cascaded video diffusion model that generates 4D content in a decoupled manner. Unlike previous multi-view video methods that directly model 3D space and temporal features simultaneously with stacked cross view/temporal attention modules, 4DVD decouples this into two subtasks: coarse multi-view layout generation and structure-aware conditional generation, and effectively unifies them. Specifically, given a monocular video, 4DVD first predicts the dense view content of its layout with superior cross-view and temporal consistency. Based on the produced layout priors, a structure-aware spatio-temporal generation branch is developed, combining these coarse structural priors with the exquisite appearance content of input monocular video to generate final high-quality dense-view videos. Benefit from this, explicit 4D representation~(such as 4D Gaussian) can be optimized accurately, enabling wider practical application. To train 4DVD, we collect a dynamic 3D object dataset, called D-Objaverse, from the Objaverse benchmark and render 16 videos with 21 frames for each object. Extensive experiments demonstrate our state-of-the-art performance on both novel view synthesis and 4D generation. Our project page is https://4dvd.github.io/", "AI": {"tldr": "4DVD\u662f\u4e00\u79cd\u7ea7\u8054\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u89e3\u8026\u65b9\u5f0f\u751f\u62104D\u5185\u5bb9\uff0c\u5206\u4e3a\u7c97\u7c92\u5ea6\u591a\u89c6\u56fe\u5e03\u5c40\u751f\u6210\u548c\u7ed3\u6784\u611f\u77e5\u6761\u4ef6\u751f\u6210\u4e24\u4e2a\u5b50\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e864D\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u76f4\u63a5\u751f\u6210\u9ad8\u7ef4\u6570\u636e\uff08\u59824D\uff09\u590d\u6742\u5ea6\u9ad8\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5efa\u6a213D\u7a7a\u95f4\u548c\u65f6\u95f4\u7279\u5f81\uff0c\u56e0\u6b64\u63d0\u51fa\u89e3\u8026\u65b9\u6cd5\u4ee5\u7b80\u5316\u4efb\u52a1\u5e76\u63d0\u5347\u6548\u679c\u3002", "method": "4DVD\u9996\u5148\u9884\u6d4b\u591a\u89c6\u56fe\u5e03\u5c40\uff0c\u518d\u7ed3\u5408\u8f93\u5165\u5355\u76ee\u89c6\u9891\u7684\u7cbe\u7ec6\u5916\u89c2\u5185\u5bb9\uff0c\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u65f6\u7a7a\u751f\u6210\u5206\u652f\u751f\u6210\u9ad8\u8d28\u91cf\u5bc6\u96c6\u89c6\u56fe\u89c6\u9891\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c4DVD\u5728\u65b0\u89c6\u89d2\u5408\u6210\u548c4D\u751f\u6210\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u80fd\u51c6\u786e\u4f18\u5316\u663e\u5f0f4D\u8868\u793a\uff08\u59824D\u9ad8\u65af\uff09\u3002", "conclusion": "4DVD\u901a\u8fc7\u89e3\u8026\u548c\u7ea7\u8054\u65b9\u6cd5\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf4D\u5185\u5bb9\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u7684\u53ef\u80fd\u3002"}}
{"id": "2508.04485", "pdf": "https://arxiv.org/pdf/2508.04485", "abs": "https://arxiv.org/abs/2508.04485", "authors": ["Bowen Chai", "Zheng Chen", "Libo Zhu", "Wenbo Li", "Yong Guo", "Yulun Zhang"], "title": "QuantVSR: Low-Bit Post-Training Quantization for Real-World Video Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have shown superior performance in real-world video super-resolution (VSR). However, the slow processing speeds and heavy resource consumption of diffusion models hinder their practical application and deployment. Quantization offers a potential solution for compressing the VSR model. Nevertheless, quantizing VSR models is challenging due to their temporal characteristics and high fidelity requirements. To address these issues, we propose QuantVSR, a low-bit quantization model for real-world VSR. We propose a spatio-temporal complexity aware (STCA) mechanism, where we first utilize the calibration dataset to measure both spatial and temporal complexities for each layer. Based on these statistics, we allocate layer-specific ranks to the low-rank full-precision (FP) auxiliary branch. Subsequently, we jointly refine the FP and low-bit branches to achieve simultaneous optimization. In addition, we propose a learnable bias alignment (LBA) module to reduce the biased quantization errors. Extensive experiments on synthetic and real-world datasets demonstrate that our method obtains comparable performance with the FP model and significantly outperforms recent leading low-bit quantization methods. Code is available at: https://github.com/bowenchai/QuantVSR.", "AI": {"tldr": "QuantVSR\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6bd4\u7279\u91cf\u5316\u6a21\u578b\uff0c\u901a\u8fc7\u65f6\u7a7a\u590d\u6742\u5ea6\u611f\u77e5\u673a\u5236\u548c\u53ef\u5b66\u4e60\u504f\u5dee\u5bf9\u9f50\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u4e2d\u91cf\u5316\u5e26\u6765\u7684\u6311\u6218\uff0c\u6027\u80fd\u63a5\u8fd1\u5168\u7cbe\u5ea6\u6a21\u578b\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5904\u7406\u901f\u5ea6\u6162\u4e14\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u91cf\u5316\u662f\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9762\u4e34\u65f6\u7a7a\u7279\u6027\u548c\u9ad8\u4fdd\u771f\u5ea6\u8981\u6c42\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faQuantVSR\uff0c\u91c7\u7528\u65f6\u7a7a\u590d\u6742\u5ea6\u611f\u77e5\u673a\u5236\uff08STCA\uff09\u5206\u914d\u5c42\u7279\u5b9a\u79e9\uff0c\u5e76\u8054\u5408\u4f18\u5316\u5168\u7cbe\u5ea6\u548c\u4f4e\u6bd4\u7279\u5206\u652f\uff1b\u5f15\u5165\u53ef\u5b66\u4e60\u504f\u5dee\u5bf9\u9f50\u6a21\u5757\uff08LBA\uff09\u51cf\u5c11\u91cf\u5316\u8bef\u5dee\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cQuantVSR\u6027\u80fd\u63a5\u8fd1\u5168\u7cbe\u5ea6\u6a21\u578b\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u4f4e\u6bd4\u7279\u91cf\u5316\u65b9\u6cd5\u3002", "conclusion": "QuantVSR\u901a\u8fc7STCA\u548cLBA\u6a21\u5757\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u7684\u91cf\u5316\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.04549", "pdf": "https://arxiv.org/pdf/2508.04549", "abs": "https://arxiv.org/abs/2508.04549", "authors": ["Quang-Trung Truong", "Yuk-Kwan Wong", "Vo Hoang Kim Tuyen Dang", "Rinaldi Gotama", "Duc Thanh Nguyen", "Sai-Kit Yeung"], "title": "MSC: A Marine Wildlife Video Dataset with Grounded Segmentation and Clip-Level Captioning", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Published at ACMMM2025 (Dataset track)", "summary": "Marine videos present significant challenges for video understanding due to the dynamics of marine objects and the surrounding environment, camera motion, and the complexity of underwater scenes. Existing video captioning datasets, typically focused on generic or human-centric domains, often fail to generalize to the complexities of the marine environment and gain insights about marine life. To address these limitations, we propose a two-stage marine object-oriented video captioning pipeline. We introduce a comprehensive video understanding benchmark that leverages the triplets of video, text, and segmentation masks to facilitate visual grounding and captioning, leading to improved marine video understanding and analysis, and marine video generation. Additionally, we highlight the effectiveness of video splitting in order to detect salient object transitions in scene changes, which significantly enrich the semantics of captioning content. Our dataset and code have been released at https://msc.hkustvgd.com.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u6d77\u6d0b\u89c6\u9891\u5b57\u5e55\u751f\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408\u89c6\u9891\u3001\u6587\u672c\u548c\u5206\u5272\u63a9\u7801\uff0c\u63d0\u5347\u6d77\u6d0b\u89c6\u9891\u7406\u89e3\u4e0e\u5206\u6790\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5b57\u5e55\u6570\u636e\u96c6\u96be\u4ee5\u9002\u5e94\u6d77\u6d0b\u73af\u5883\u7684\u590d\u6742\u6027\uff0c\u9700\u9488\u5bf9\u6027\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u7ba1\u9053\uff0c\u7ed3\u5408\u89c6\u9891\u3001\u6587\u672c\u548c\u5206\u5272\u63a9\u7801\uff0c\u5e76\u5229\u7528\u89c6\u9891\u5206\u5272\u68c0\u6d4b\u663e\u8457\u5bf9\u8c61\u53d8\u5316\u3002", "result": "\u6784\u5efa\u4e86\u7efc\u5408\u89c6\u9891\u7406\u89e3\u57fa\u51c6\uff0c\u663e\u8457\u4e30\u5bcc\u4e86\u5b57\u5e55\u5185\u5bb9\u7684\u8bed\u4e49\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6d77\u6d0b\u89c6\u9891\u7406\u89e3\u4e0e\u5206\u6790\u80fd\u529b\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2508.04551", "pdf": "https://arxiv.org/pdf/2508.04551", "abs": "https://arxiv.org/abs/2508.04551", "authors": ["Angang Zhang", "Fang Deng", "Hao Chen", "Zhongjian Chen", "Junyan Li"], "title": "Two-Way Garment Transfer: Unified Diffusion Framework for Dressing and Undressing Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "While recent advances in virtual try-on (VTON) have achieved realistic garment transfer to human subjects, its inverse task, virtual try-off (VTOFF), which aims to reconstruct canonical garment templates from dressed humans, remains critically underexplored and lacks systematic investigation. Existing works predominantly treat them as isolated tasks: VTON focuses on garment dressing while VTOFF addresses garment extraction, thereby neglecting their complementary symmetry. To bridge this fundamental gap, we propose the Two-Way Garment Transfer Model (TWGTM), to the best of our knowledge, the first unified framework for joint clothing-centric image synthesis that simultaneously resolves both mask-guided VTON and mask-free VTOFF through bidirectional feature disentanglement. Specifically, our framework employs dual-conditioned guidance from both latent and pixel spaces of reference images to seamlessly bridge the dual tasks. On the other hand, to resolve the inherent mask dependency asymmetry between mask-guided VTON and mask-free VTOFF, we devise a phased training paradigm that progressively bridges this modality gap. Extensive qualitative and quantitative experiments conducted across the DressCode and VITON-HD datasets validate the efficacy and competitive edge of our proposed approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u7edf\u4e00\u6846\u67b6TWGTM\uff0c\u540c\u65f6\u89e3\u51b3\u865a\u62df\u8bd5\u7a7f\uff08VTON\uff09\u548c\u865a\u62df\u8131\u8863\uff08VTOFF\uff09\u4efb\u52a1\uff0c\u901a\u8fc7\u53cc\u5411\u7279\u5f81\u89e3\u8026\u548c\u5206\u9636\u6bb5\u8bad\u7ec3\u5b9e\u73b0\u4e92\u8865\u5bf9\u79f0\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5c06VTON\u548cVTOFF\u89c6\u4e3a\u72ec\u7acb\u4efb\u52a1\uff0c\u5ffd\u7565\u4e86\u5b83\u4eec\u7684\u4e92\u8865\u6027\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\u4ee5\u540c\u65f6\u89e3\u51b3\u8fd9\u4e24\u4e2a\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u53cc\u5411\u7279\u5f81\u89e3\u8026\u548c\u53cc\u6761\u4ef6\u5f15\u5bfc\uff08\u6f5c\u5728\u7a7a\u95f4\u548c\u50cf\u7d20\u7a7a\u95f4\uff09\uff0c\u5e76\u901a\u8fc7\u5206\u9636\u6bb5\u8bad\u7ec3\u89e3\u51b3\u63a9\u6a21\u4f9d\u8d56\u4e0d\u5bf9\u79f0\u95ee\u9898\u3002", "result": "\u5728DressCode\u548cVITON-HD\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u7ade\u4e89\u529b\u3002", "conclusion": "TWGTM\u662f\u9996\u4e2a\u7edf\u4e00\u89e3\u51b3VTON\u548cVTOFF\u7684\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u4e92\u8865\u5bf9\u79f0\u6027\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.04559", "pdf": "https://arxiv.org/pdf/2508.04559", "abs": "https://arxiv.org/abs/2508.04559", "authors": ["Jinxi Liu", "Zijian He", "Guangrun Wang", "Guanbin Li", "Liang Lin"], "title": "One Model For All: Partial Diffusion for Unified Try-On and Try-Off in Any Pose", "categories": ["cs.CV"], "comment": null, "summary": "Recent diffusion-based approaches have made significant advances in image-based virtual try-on, enabling more realistic and end-to-end garment synthesis. However, most existing methods remain constrained by their reliance on exhibition garments and segmentation masks, as well as their limited ability to handle flexible pose variations. These limitations reduce their practicality in real-world scenarios-for instance, users cannot easily transfer garments worn by one person onto another, and the generated try-on results are typically restricted to the same pose as the reference image. In this paper, we introduce \\textbf{OMFA} (\\emph{One Model For All}), a unified diffusion framework for both virtual try-on and try-off that operates without the need for exhibition garments and supports arbitrary poses. For example, OMFA enables removing garments from a source person (try-off) and transferring them onto a target person (try-on), while also allowing the generated target to appear in novel poses-even without access to multi-pose images of that person. OMFA is built upon a novel \\emph{partial diffusion} strategy that selectively applies noise and denoising to individual components of the joint input-such as the garment, the person image, or the face-enabling dynamic subtask control and efficient bidirectional garment-person transformation. The framework is entirely mask-free and requires only a single portrait and a target pose as input, making it well-suited for real-world applications. Additionally, by leveraging SMPL-X-based pose conditioning, OMFA supports multi-view and arbitrary-pose try-on from just one image. Extensive experiments demonstrate that OMFA achieves state-of-the-art results on both try-on and try-off tasks, providing a practical and generalizable solution for virtual garment synthesis. The project page is here: https://onemodelforall.github.io/.", "AI": {"tldr": "OMFA\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6269\u6563\u6846\u67b6\uff0c\u652f\u6301\u65e0\u9700\u5c55\u793a\u670d\u88c5\u548c\u4efb\u610f\u59ff\u52bf\u7684\u865a\u62df\u8bd5\u7a7f\u548c\u8bd5\u8131\uff0c\u901a\u8fc7\u90e8\u5206\u6269\u6563\u7b56\u7565\u5b9e\u73b0\u9ad8\u6548\u7684\u53cc\u5411\u670d\u88c5-\u4eba\u7269\u8f6c\u6362\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5c55\u793a\u670d\u88c5\u548c\u5206\u5272\u63a9\u7801\uff0c\u4e14\u96be\u4ee5\u5904\u7406\u7075\u6d3b\u59ff\u52bf\u53d8\u5316\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002OMFA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u5b9e\u7528\u7684\u865a\u62df\u670d\u88c5\u5408\u6210\u65b9\u6848\u3002", "method": "OMFA\u91c7\u7528\u90e8\u5206\u6269\u6563\u7b56\u7565\uff0c\u9009\u62e9\u6027\u5bf9\u8054\u5408\u8f93\u5165\u7684\u5404\u4e2a\u7ec4\u4ef6\uff08\u5982\u670d\u88c5\u3001\u4eba\u7269\u56fe\u50cf\u6216\u9762\u90e8\uff09\u65bd\u52a0\u566a\u58f0\u548c\u53bb\u566a\uff0c\u652f\u6301\u52a8\u6001\u5b50\u4efb\u52a1\u63a7\u5236\u548c\u53cc\u5411\u8f6c\u6362\u3002", "result": "OMFA\u5728\u8bd5\u7a7f\u548c\u8bd5\u8131\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u4ec5\u9700\u5355\u5f20\u8096\u50cf\u548c\u76ee\u6807\u59ff\u52bf\u5373\u53ef\u5b9e\u73b0\u591a\u89c6\u89d2\u548c\u4efb\u610f\u59ff\u52bf\u7684\u8bd5\u7a7f\u3002", "conclusion": "OMFA\u4e3a\u865a\u62df\u670d\u88c5\u5408\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u573a\u666f\u3002"}}
{"id": "2508.04565", "pdf": "https://arxiv.org/pdf/2508.04565", "abs": "https://arxiv.org/abs/2508.04565", "authors": ["Yunbi Liu", "Enqi Tang", "Shiyu Li", "Lei Ma", "Juncheng Li", "Shu Lou", "Yongchu Pan", "Qingshan Liu"], "title": "TAlignDiff: Automatic Tooth Alignment assisted by Diffusion-based Transformation Learning", "categories": ["cs.CV"], "comment": "Submitted to AAAI 2026", "summary": "Orthodontic treatment hinges on tooth alignment, which significantly affects occlusal function, facial aesthetics, and patients' quality of life. Current deep learning approaches predominantly concentrate on predicting transformation matrices through imposing point-to-point geometric constraints for tooth alignment. Nevertheless, these matrices are likely associated with the anatomical structure of the human oral cavity and possess particular distribution characteristics that the deterministic point-to-point geometric constraints in prior work fail to capture. To address this, we introduce a new automatic tooth alignment method named TAlignDiff, which is supported by diffusion-based transformation learning. TAlignDiff comprises two main components: a primary point cloud-based regression network (PRN) and a diffusion-based transformation matrix denoising module (DTMD). Geometry-constrained losses supervise PRN learning for point cloud-level alignment. DTMD, as an auxiliary module, learns the latent distribution of transformation matrices from clinical data. We integrate point cloud-based transformation regression and diffusion-based transformation modeling into a unified framework, allowing bidirectional feedback between geometric constraints and diffusion refinement. Extensive ablation and comparative experiments demonstrate the effectiveness and superiority of our method, highlighting its potential in orthodontic treatment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTAlignDiff\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u70b9\u4e91\u56de\u5f52\u548c\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u7259\u9f7f\u5bf9\u9f50\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u901a\u8fc7\u70b9\u5bf9\u70b9\u51e0\u4f55\u7ea6\u675f\u9884\u6d4b\u53d8\u6362\u77e9\u9635\uff0c\u4f46\u5ffd\u7565\u4e86\u5176\u4e0e\u53e3\u8154\u89e3\u5256\u7ed3\u6784\u7684\u5173\u8054\u53ca\u5206\u5e03\u7279\u6027\u3002", "method": "TAlignDiff\u5305\u542b\u70b9\u4e91\u56de\u5f52\u7f51\u7edc\uff08PRN\uff09\u548c\u6269\u6563\u53d8\u6362\u77e9\u9635\u53bb\u566a\u6a21\u5757\uff08DTMD\uff09\uff0c\u7ed3\u5408\u51e0\u4f55\u7ea6\u675f\u548c\u6269\u6563\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u4e14\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u5728\u6b63\u7578\u6cbb\u7597\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "TAlignDiff\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u7ea6\u675f\u548c\u6269\u6563\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u7259\u9f7f\u5bf9\u9f50\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.04568", "pdf": "https://arxiv.org/pdf/2508.04568", "abs": "https://arxiv.org/abs/2508.04568", "authors": ["Yijie Li", "Wei Zhang", "Xi Zhu", "Ye Wu", "Yogesh Rathi", "Lauren J. O'Donnell", "Fan Zhang"], "title": "DDTracking: A Deep Generative Framework for Diffusion MRI Tractography with Streamline Local-Global Spatiotemporal Modeling", "categories": ["cs.CV"], "comment": "Preprint version. The content may be updated in the future", "summary": "This paper presents DDTracking, a novel deep generative framework for diffusion MRI tractography that formulates streamline propagation as a conditional denoising diffusion process. In DDTracking, we introduce a dual-pathway encoding network that jointly models local spatial encoding (capturing fine-scale structural details at each streamline point) and global temporal dependencies (ensuring long-range consistency across the entire streamline). Furthermore, we design a conditional diffusion model module, which leverages the learned local and global embeddings to predict streamline propagation orientations for tractography in an end-to-end trainable manner. We conduct a comprehensive evaluation across diverse, independently acquired dMRI datasets, including both synthetic and clinical data. Experiments on two well-established benchmarks with ground truth (ISMRM Challenge and TractoInferno) demonstrate that DDTracking largely outperforms current state-of-the-art tractography methods. Furthermore, our results highlight DDTracking's strong generalizability across heterogeneous datasets, spanning varying health conditions, age groups, imaging protocols, and scanner types. Collectively, DDTracking offers anatomically plausible and robust tractography, presenting a scalable, adaptable, and end-to-end learnable solution for broad dMRI applications. Code is available at: https://github.com/yishengpoxiao/DDtracking.git", "AI": {"tldr": "DDTracking\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u7684\u6269\u6563MRI\u7ea4\u7ef4\u675f\u8ffd\u8e2a\u65b9\u6cd5\uff0c\u901a\u8fc7\u6761\u4ef6\u53bb\u566a\u6269\u6563\u8fc7\u7a0b\u5b9e\u73b0\u6d41\u7ebf\u4f20\u64ad\uff0c\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u7f16\u7801\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7ea4\u7ef4\u675f\u8ffd\u8e2a\u65b9\u6cd5\u5728\u6355\u6349\u5c40\u90e8\u7ec6\u8282\u548c\u5168\u5c40\u4e00\u81f4\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u63d0\u4f9b\u66f4\u9c81\u68d2\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u53cc\u8def\u5f84\u7f16\u7801\u7f51\u7edc\uff08\u5c40\u90e8\u7a7a\u95f4\u7f16\u7801\u548c\u5168\u5c40\u65f6\u95f4\u4f9d\u8d56\u5efa\u6a21\uff09\u548c\u6761\u4ef6\u6269\u6563\u6a21\u578b\u6a21\u5757\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\uff08\u5305\u62ec\u5408\u6210\u548c\u4e34\u5e8a\u6570\u636e\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728ISMRM Challenge\u548cTractoInferno\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DDTracking\u63d0\u4f9b\u4e86\u89e3\u5256\u5b66\u4e0a\u5408\u7406\u4e14\u9c81\u68d2\u7684\u7ea4\u7ef4\u675f\u8ffd\u8e2a\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684dMRI\u5e94\u7528\u3002"}}
{"id": "2508.04658", "pdf": "https://arxiv.org/pdf/2508.04658", "abs": "https://arxiv.org/abs/2508.04658", "authors": ["Akhil Saketh Reddy Sabbella", "Ch. Lakshmi Prachothan", "Eswar Kumar Panta"], "title": "YOLOv8-Based Deep Learning Model for Automated Poultry Disease Detection and Health Monitoring paper", "categories": ["cs.CV", "cs.AI"], "comment": "6 Pages, 9 Figures, 2 Tables", "summary": "In the poultry industry, detecting chicken illnesses is essential to avoid financial losses. Conventional techniques depend on manual observation, which is laborious and prone to mistakes. Using YOLO v8 a deep learning model for real-time object recognition. This study suggests an AI based approach, by developing a system that analyzes high resolution chicken photos, YOLO v8 detects signs of illness, such as abnormalities in behavior and appearance. A sizable, annotated dataset has been used to train the algorithm, which provides accurate real-time identification of infected chicken and prompt warnings to farm operators for prompt action. By facilitating early infection identification, eliminating the need for human inspection, and enhancing biosecurity in large-scale farms, this AI technology improves chicken health management. The real-time features of YOLO v8 provide a scalable and effective method for improving farm management techniques.", "AI": {"tldr": "\u4f7f\u7528YOLO v8\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5b9e\u65f6\u68c0\u6d4b\u9e21\u53ea\u75be\u75c5\uff0c\u66ff\u4ee3\u4f20\u7edf\u4eba\u5de5\u89c2\u5bdf\uff0c\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u9e21\u75c5\u68c0\u6d4b\u4f9d\u8d56\u4eba\u5de5\uff0c\u6548\u7387\u4f4e\u4e14\u6613\u51fa\u9519\uff0c\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u4ee5\u51cf\u5c11\u7ecf\u6d4e\u635f\u5931\u3002", "method": "\u57fa\u4e8eYOLO v8\u5f00\u53d1\u7cfb\u7edf\uff0c\u5206\u6790\u9ad8\u5206\u8fa8\u7387\u9e21\u53ea\u7167\u7247\uff0c\u68c0\u6d4b\u884c\u4e3a\u548c\u5916\u8c8c\u5f02\u5e38\u3002", "result": "\u8bad\u7ec3\u540e\u7684\u7b97\u6cd5\u80fd\u5b9e\u65f6\u51c6\u786e\u8bc6\u522b\u75c5\u9e21\uff0c\u5e76\u5411\u519c\u573a\u4e3b\u53d1\u51fa\u8b66\u62a5\uff0c\u63d0\u5347\u751f\u7269\u5b89\u5168\u3002", "conclusion": "YOLO v8\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u5b9e\u65f6\u68c0\u6d4b\u65b9\u6848\uff0c\u4f18\u5316\u519c\u573a\u7ba1\u7406\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3002"}}
{"id": "2508.04705", "pdf": "https://arxiv.org/pdf/2508.04705", "abs": "https://arxiv.org/abs/2508.04705", "authors": ["Ziyang Leng", "Jiawei Yang", "Wenlong Yi", "Bolei Zhou"], "title": "Occupancy Learning with Spatiotemporal Memory", "categories": ["cs.CV"], "comment": "Accepted to ICCV2025. Project website:   https://matthew-leng.github.io/stocc", "summary": "3D occupancy becomes a promising perception representation for autonomous driving to model the surrounding environment at a fine-grained scale. However, it remains challenging to efficiently aggregate 3D occupancy over time across multiple input frames due to the high processing cost and the uncertainty and dynamics of voxels. To address this issue, we propose ST-Occ, a scene-level occupancy representation learning framework that effectively learns the spatiotemporal feature with temporal consistency. ST-Occ consists of two core designs: a spatiotemporal memory that captures comprehensive historical information and stores it efficiently through a scene-level representation and a memory attention that conditions the current occupancy representation on the spatiotemporal memory with a model of uncertainty and dynamic awareness. Our method significantly enhances the spatiotemporal representation learned for 3D occupancy prediction tasks by exploiting the temporal dependency between multi-frame inputs. Experiments show that our approach outperforms the state-of-the-art methods by a margin of 3 mIoU and reduces the temporal inconsistency by 29%.", "AI": {"tldr": "ST-Occ\u662f\u4e00\u79cd\u7528\u4e8e3D\u5360\u7528\u9884\u6d4b\u7684\u65f6\u7a7a\u7279\u5f81\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u573a\u666f\u7ea7\u8868\u793a\u548c\u8bb0\u5fc6\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u65f6\u7a7a\u4e00\u81f4\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u591a\u5e27\u8f93\u5165\u4e0b3D\u5360\u7528\u9884\u6d4b\u7684\u9ad8\u5904\u7406\u6210\u672c\u548c\u65f6\u7a7a\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faST-Occ\u6846\u67b6\uff0c\u5305\u542b\u65f6\u7a7a\u8bb0\u5fc6\u548c\u8bb0\u5fc6\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5229\u7528\u5386\u53f2\u4fe1\u606f\u4f18\u5316\u5f53\u524d\u5360\u7528\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cST-Occ\u57283D\u5360\u7528\u9884\u6d4b\u4efb\u52a1\u4e2d\u6027\u80fd\u63d0\u53473 mIoU\uff0c\u65f6\u7a7a\u4e0d\u4e00\u81f4\u6027\u964d\u4f4e29%\u3002", "conclusion": "ST-Occ\u901a\u8fc7\u9ad8\u6548\u5229\u7528\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u5360\u7528\u9884\u6d4b\u7684\u6027\u80fd\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.03734", "pdf": "https://arxiv.org/pdf/2508.03734", "abs": "https://arxiv.org/abs/2508.03734", "authors": ["Xiaoling Luo", "Ruli Zheng", "Qiaojian Zheng", "Zibo Du", "Shuo Yang", "Meidan Ding", "Qihao Xu", "Chengliang Liu", "Linlin Shen"], "title": "A Survey of Multimodal Ophthalmic Diagnostics: From Task-Specific Approaches to Foundational Models", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Visual impairment represents a major global health challenge, with multimodal imaging providing complementary information that is essential for accurate ophthalmic diagnosis. This comprehensive survey systematically reviews the latest advances in multimodal deep learning methods in ophthalmology up to the year 2025. The review focuses on two main categories: task-specific multimodal approaches and large-scale multimodal foundation models. Task-specific approaches are designed for particular clinical applications such as lesion detection, disease diagnosis, and image synthesis. These methods utilize a variety of imaging modalities including color fundus photography, optical coherence tomography, and angiography. On the other hand, foundation models combine sophisticated vision-language architectures and large language models pretrained on diverse ophthalmic datasets. These models enable robust cross-modal understanding, automated clinical report generation, and decision support. The survey critically examines important datasets, evaluation metrics, and methodological innovations including self-supervised learning, attention-based fusion, and contrastive alignment. It also discusses ongoing challenges such as variability in data, limited annotations, lack of interpretability, and issues with generalizability across different patient populations. Finally, the survey outlines promising future directions that emphasize the use of ultra-widefield imaging and reinforcement learning-based reasoning frameworks to create intelligent, interpretable, and clinically applicable AI systems for ophthalmology.", "AI": {"tldr": "\u7efc\u8ff0\u4e86\u622a\u81f32025\u5e74\u773c\u79d1\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5305\u62ec\u4efb\u52a1\u7279\u5b9a\u65b9\u6cd5\u548c\u57fa\u7840\u6a21\u578b\uff0c\u8ba8\u8bba\u4e86\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6307\u6807\u3001\u521b\u65b0\u65b9\u6cd5\u53ca\u6311\u6218\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u89c6\u89c9\u969c\u788d\u662f\u5168\u7403\u91cd\u5927\u5065\u5eb7\u6311\u6218\uff0c\u591a\u6a21\u6001\u6210\u50cf\u4e3a\u773c\u79d1\u8bca\u65ad\u63d0\u4f9b\u4e92\u8865\u4fe1\u606f\uff0c\u9700\u7cfb\u7edf\u603b\u7ed3\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u6700\u65b0\u8fdb\u5c55\u3002", "method": "\u5206\u4e3a\u4efb\u52a1\u7279\u5b9a\u65b9\u6cd5\u548c\u57fa\u7840\u6a21\u578b\u4e24\u7c7b\uff0c\u524d\u8005\u9488\u5bf9\u7279\u5b9a\u4e34\u5e8a\u4efb\u52a1\uff0c\u540e\u8005\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u67b6\u6784\u548c\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u4efb\u52a1\u7279\u5b9a\u65b9\u6cd5\u7528\u4e8e\u75c5\u7076\u68c0\u6d4b\u3001\u75be\u75c5\u8bca\u65ad\u7b49\uff1b\u57fa\u7840\u6a21\u578b\u652f\u6301\u8de8\u6a21\u6001\u7406\u89e3\u3001\u81ea\u52a8\u62a5\u544a\u751f\u6210\u548c\u51b3\u7b56\u652f\u6301\u3002", "conclusion": "\u672a\u6765\u65b9\u5411\u5305\u62ec\u8d85\u5e7f\u57df\u6210\u50cf\u548c\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u5f00\u53d1\u667a\u80fd\u3001\u53ef\u89e3\u91ca\u4e14\u4e34\u5e8a\u9002\u7528\u7684\u773c\u79d1AI\u7cfb\u7edf\u3002"}}
{"id": "2508.03742", "pdf": "https://arxiv.org/pdf/2508.03742", "abs": "https://arxiv.org/abs/2508.03742", "authors": ["Weiwei Cao", "Jianpeng Zhang", "Zhongyi Shui", "Sinuo Wang", "Zeli Chen", "Xi Li", "Le Lu", "Xianghua Ye", "Tingbo Liang", "Qi Zhang", "Ling Zhang"], "title": "Boosting Vision Semantic Density with Anatomy Normality Modeling for Medical Vision-language Pre-training", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Vision-language pre-training (VLP) has great potential for developing multifunctional and general medical diagnostic capabilities. However, aligning medical images with a low signal-to-noise ratio (SNR) to reports with a high SNR presents a semantic density gap, leading to visual alignment bias. In this paper, we propose boosting vision semantic density to improve alignment effectiveness. On one hand, we enhance visual semantics through disease-level vision contrastive learning, which strengthens the model's ability to differentiate between normal and abnormal samples for each anatomical structure. On the other hand, we introduce an anatomical normality modeling method to model the distribution of normal samples for each anatomy, leveraging VQ-VAE for reconstructing normal vision embeddings in the latent space. This process amplifies abnormal signals by leveraging distribution shifts in abnormal samples, enhancing the model's perception and discrimination of abnormal attributes. The enhanced visual representation effectively captures the diagnostic-relevant semantics, facilitating more efficient and accurate alignment with the diagnostic report. We conduct extensive experiments on two chest CT datasets, CT-RATE and Rad-ChestCT, and an abdominal CT dataset, MedVL-CT69K, and comprehensively evaluate the diagnosis performance across multiple tasks in the chest and abdominal CT scenarios, achieving state-of-the-art zero-shot performance. Notably, our method achieved an average AUC of 84.9% across 54 diseases in 15 organs, significantly surpassing existing methods. Additionally, we demonstrate the superior transfer learning capabilities of our pre-trained model. Code is available at https://github.com/alibaba-damo-academy/ViSD-Boost.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u589e\u5f3a\u89c6\u89c9\u8bed\u4e49\u5bc6\u5ea6\u6765\u6539\u8fdb\u533b\u5b66\u56fe\u50cf\u4e0e\u62a5\u544a\u5bf9\u9f50\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u75be\u75c5\u7ea7\u89c6\u89c9\u5bf9\u6bd4\u5b66\u4e60\u548c\u89e3\u5256\u5b66\u6b63\u6001\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u4e0e\u62a5\u544a\u4e4b\u95f4\u5b58\u5728\u8bed\u4e49\u5bc6\u5ea6\u5dee\u8ddd\uff0c\u5bfc\u81f4\u89c6\u89c9\u5bf9\u9f50\u504f\u5dee\uff0c\u5f71\u54cd\u8bca\u65ad\u6548\u679c\u3002", "method": "\u901a\u8fc7\u75be\u75c5\u7ea7\u89c6\u89c9\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u89c6\u89c9\u8bed\u4e49\uff0c\u5e76\u5229\u7528VQ-VAE\u5efa\u6a21\u89e3\u5256\u5b66\u6b63\u6001\u5206\u5e03\uff0c\u653e\u5927\u5f02\u5e38\u4fe1\u53f7\u3002", "result": "\u5728\u4e24\u4e2a\u80f8\u90e8CT\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u8179\u90e8CT\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u5e73\u5747AUC\u8fbe84.9%\u3002", "conclusion": "\u589e\u5f3a\u7684\u89c6\u89c9\u8868\u793a\u80fd\u66f4\u9ad8\u6548\u51c6\u786e\u5730\u4e0e\u8bca\u65ad\u62a5\u544a\u5bf9\u9f50\uff0c\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u8fc1\u79fb\u5b66\u4e60\u80fd\u529b\u3002"}}
{"id": "2508.03755", "pdf": "https://arxiv.org/pdf/2508.03755", "abs": "https://arxiv.org/abs/2508.03755", "authors": ["Wenwu Gong", "Lili Yang"], "title": "LRTuckerRep: Low-rank Tucker Representation Model for Multi-dimensional Data Completion", "categories": ["cs.LG", "cs.CV", "cs.NA", "math.NA"], "comment": null, "summary": "Multi-dimensional data completion is a critical problem in computational sciences, particularly in domains such as computer vision, signal processing, and scientific computing. Existing methods typically leverage either global low-rank approximations or local smoothness regularization, but each suffers from notable limitations: low-rank methods are computationally expensive and may disrupt intrinsic data structures, while smoothness-based approaches often require extensive manual parameter tuning and exhibit poor generalization. In this paper, we propose a novel Low-Rank Tucker Representation (LRTuckerRep) model that unifies global and local prior modeling within a Tucker decomposition. Specifically, LRTuckerRep encodes low rankness through a self-adaptive weighted nuclear norm on the factor matrices and a sparse Tucker core, while capturing smoothness via a parameter-free Laplacian-based regularization on the factor spaces. To efficiently solve the resulting nonconvex optimization problem, we develop two iterative algorithms with provable convergence guarantees. Extensive experiments on multi-dimensional image inpainting and traffic data imputation demonstrate that LRTuckerRep achieves superior completion accuracy and robustness under high missing rates compared to baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f4e\u79e9Tucker\u8868\u793a\uff08LRTuckerRep\uff09\u6a21\u578b\uff0c\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u5148\u9a8c\u5efa\u6a21\uff0c\u7528\u4e8e\u591a\u7ef4\u6570\u636e\u8865\u5168\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u4f4e\u79e9\u8fd1\u4f3c\u6216\u5c40\u90e8\u5e73\u6ed1\u6b63\u5219\u5316\uff09\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u53c2\u6570\u8c03\u4f18\u56f0\u96be\u6216\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "LRTuckerRep\u901a\u8fc7Tucker\u5206\u89e3\u7edf\u4e00\u5168\u5c40\u548c\u5c40\u90e8\u5148\u9a8c\u5efa\u6a21\uff0c\u4f7f\u7528\u81ea\u9002\u5e94\u52a0\u6743\u6838\u8303\u6570\u548c\u7a00\u758fTucker\u6838\u5fc3\u7f16\u7801\u4f4e\u79e9\u6027\uff0c\u5e76\u901a\u8fc7\u65e0\u53c2\u6570\u62c9\u666e\u62c9\u65af\u6b63\u5219\u5316\u6355\u83b7\u5e73\u6ed1\u6027\u3002", "result": "\u5728\u56fe\u50cf\u4fee\u590d\u548c\u4ea4\u901a\u6570\u636e\u8865\u5168\u5b9e\u9a8c\u4e2d\uff0cLRTuckerRep\u5728\u9ad8\u7f3a\u5931\u7387\u4e0b\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u8865\u5168\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "LRTuckerRep\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u591a\u7ef4\u6570\u636e\u8865\u5168\u65b9\u6cd5\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002"}}
{"id": "2508.03759", "pdf": "https://arxiv.org/pdf/2508.03759", "abs": "https://arxiv.org/abs/2508.03759", "authors": ["Tatwadarshi P. Nagarhalli", "Shruti S. Pawar", "Soham A. Dahanukar", "Uday Aswalekar", "Ashwini M. Save", "Sanket D. Patil"], "title": "Assessing the Impact of Image Super Resolution on White Blood Cell Classification Accuracy", "categories": ["eess.IV", "cs.CV", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Accurately classifying white blood cells from microscopic images is essential to identify several illnesses and conditions in medical diagnostics. Many deep learning technologies are being employed to quickly and automatically classify images. However, most of the time, the resolution of these microscopic pictures is quite low, which might make it difficult to classify them correctly. Some picture improvement techniques, such as image super-resolution, are being utilized to improve the resolution of the photos to get around this issue. The suggested study uses large image dimension upscaling to investigate how picture-enhancing approaches affect classification performance. The study specifically looks at how deep learning models may be able to understand more complex visual information by capturing subtler morphological changes when image resolution is increased using cutting-edge techniques. The model may learn from standard and augmented data since the improved images are incorporated into the training process. This dual method seeks to comprehend the impact of image resolution on model performance and enhance classification accuracy. A well-known model for picture categorization is used to conduct extensive testing and thoroughly evaluate the effectiveness of this approach. This research intends to create more efficient image identification algorithms customized to a particular dataset of white blood cells by understanding the trade-offs between ordinary and enhanced images.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u56fe\u50cf\u589e\u5f3a\u6280\u672f\uff08\u5982\u8d85\u5206\u8fa8\u7387\uff09\u5bf9\u767d\u7ec6\u80de\u5206\u7c7b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u7ed3\u5408\u6807\u51c6\u4e0e\u589e\u5f3a\u6570\u636e\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u4f4e\u5206\u8fa8\u7387\u7684\u663e\u5fae\u56fe\u50cf\u53ef\u80fd\u5bfc\u81f4\u5206\u7c7b\u4e0d\u51c6\u786e\uff0c\u56fe\u50cf\u589e\u5f3a\u6280\u672f\u6709\u671b\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6280\u672f\u63d0\u5347\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5e76\u5c06\u589e\u5f3a\u56fe\u50cf\u878d\u5165\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u7ed3\u5408\u6807\u51c6\u6570\u636e\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u56fe\u50cf\u589e\u5f3a\u5bf9\u5206\u7c7b\u6027\u80fd\u7684\u79ef\u6781\u5f71\u54cd\uff0c\u6a21\u578b\u80fd\u6355\u6349\u66f4\u7ec6\u5fae\u7684\u5f62\u6001\u53d8\u5316\u3002", "conclusion": "\u56fe\u50cf\u589e\u5f3a\u6280\u672f\u53ef\u663e\u8457\u63d0\u5347\u767d\u7ec6\u80de\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u7279\u5b9a\u6570\u636e\u96c6\u5b9a\u5236\u66f4\u9ad8\u6548\u7684\u56fe\u50cf\u8bc6\u522b\u7b97\u6cd5\u3002"}}
{"id": "2508.04642", "pdf": "https://arxiv.org/pdf/2508.04642", "abs": "https://arxiv.org/abs/2508.04642", "authors": ["Baihui Xiao", "Chengjian Feng", "Zhijian Huang", "Feng yan", "Yujie Zhong", "Lin Ma"], "title": "RoboTron-Sim: Improving Real-World Driving via Simulated Hard-Case", "categories": ["cs.RO", "cs.CV"], "comment": "ICCV 2025", "summary": "Collecting real-world data for rare high-risk scenarios, long-tailed driving events, and complex interactions remains challenging, leading to poor performance of existing autonomous driving systems in these critical situations. In this paper, we propose RoboTron-Sim that improves real-world driving in critical situations by utilizing simulated hard cases. First, we develop a simulated dataset called Hard-case Augmented Synthetic Scenarios (HASS), which covers 13 high-risk edge-case categories, as well as balanced environmental conditions such as day/night and sunny/rainy. Second, we introduce Scenario-aware Prompt Engineering (SPE) and an Image-to-Ego Encoder (I2E Encoder) to enable multimodal large language models to effectively learn real-world challenging driving skills from HASS, via adapting to environmental deviations and hardware differences between real-world and simulated scenarios. Extensive experiments on nuScenes show that RoboTron-Sim improves driving performance in challenging scenarios by around 50%, achieving state-of-the-art results in real-world open-loop planning. Qualitative results further demonstrate the effectiveness of RoboTron-Sim in better managing rare high-risk driving scenarios. Project page: https://stars79689.github.io/RoboTron-Sim/", "AI": {"tldr": "RoboTron-Sim\u901a\u8fc7\u6a21\u62df\u9ad8\u98ce\u9669\u573a\u666f\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u8868\u73b0\uff0c\u4f7f\u7528HASS\u6570\u636e\u96c6\u548cSPE\u3001I2E\u7f16\u7801\u5668\u6280\u672f\uff0c\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u63d0\u534750%\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u7f55\u89c1\u9ad8\u98ce\u9669\u573a\u666f\u548c\u590d\u6742\u4ea4\u4e92\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u56e0\u771f\u5b9e\u6570\u636e\u6536\u96c6\u56f0\u96be\u3002", "method": "\u5f00\u53d1HASS\u6a21\u62df\u6570\u636e\u96c6\uff0c\u6db5\u76d613\u7c7b\u9ad8\u98ce\u9669\u573a\u666f\uff1b\u63d0\u51faSPE\u548cI2E\u7f16\u7801\u5668\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u6a21\u62df\u6570\u636e\u5b66\u4e60\u771f\u5b9e\u9a7e\u9a76\u6280\u80fd\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0c\u9a7e\u9a76\u6027\u80fd\u63d0\u5347\u7ea650%\uff0c\u8fbe\u5230\u5f00\u73af\u89c4\u5212\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "RoboTron-Sim\u80fd\u6709\u6548\u7ba1\u7406\u7f55\u89c1\u9ad8\u98ce\u9669\u9a7e\u9a76\u573a\u666f\uff0c\u663e\u8457\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2508.04648", "pdf": "https://arxiv.org/pdf/2508.04648", "abs": "https://arxiv.org/abs/2508.04648", "authors": ["Robin Swanson", "Esther Y. H. Lin", "Masen Lamb", "Suresh Sivanandam", "Kiriakos N. Kutulakos"], "title": "Super Resolved Imaging with Adaptive Optics", "categories": ["astro-ph.IM", "cs.CV"], "comment": "Accepted to ICCV 2025 (IEEE/CVF International Conference on Computer   Vision)", "summary": "Astronomical telescopes suffer from a tradeoff between field of view (FoV) and image resolution: increasing the FoV leads to an optical field that is under-sampled by the science camera. This work presents a novel computational imaging approach to overcome this tradeoff by leveraging the existing adaptive optics (AO) systems in modern ground-based telescopes. Our key idea is to use the AO system's deformable mirror to apply a series of learned, precisely controlled distortions to the optical wavefront, producing a sequence of images that exhibit distinct, high-frequency, sub-pixel shifts. These images can then be jointly upsampled to yield the final super-resolved image. Crucially, we show this can be done while simultaneously maintaining the core AO operation--correcting for the unknown and rapidly changing wavefront distortions caused by Earth's atmosphere. To achieve this, we incorporate end-to-end optimization of both the induced mirror distortions and the upsampling algorithm, such that telescope-specific optics and temporal statistics of atmospheric wavefront distortions are accounted for. Our experimental results with a hardware prototype, as well as simulations, demonstrate significant SNR improvements of up to 12 dB over non-AO super-resolution baselines, using only existing telescope optics and no hardware modifications. Moreover, by using a precise bench-top replica of a complete telescope and AO system, we show that our methodology can be readily transferred to an operational telescope. Project webpage: https://www.cs.toronto.edu/~robin/aosr/", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u81ea\u9002\u5e94\u5149\u5b66\u7cfb\u7edf\u5b9e\u73b0\u671b\u8fdc\u955c\u89c6\u573a\u4e0e\u5206\u8fa8\u7387\u6743\u8861\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d8\u5f62\u955c\u4ea7\u751f\u9ad8\u9891\u5b50\u50cf\u7d20\u4f4d\u79fb\u56fe\u50cf\uff0c\u8054\u5408\u4e0a\u91c7\u6837\u83b7\u5f97\u8d85\u5206\u8fa8\u7387\u56fe\u50cf\u3002", "motivation": "\u89e3\u51b3\u5929\u6587\u671b\u8fdc\u955c\u89c6\u573a\u4e0e\u5206\u8fa8\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5229\u7528\u73b0\u6709\u81ea\u9002\u5e94\u5149\u5b66\u7cfb\u7edf\u63d0\u5347\u6210\u50cf\u8d28\u91cf\u3002", "method": "\u5229\u7528\u81ea\u9002\u5e94\u5149\u5b66\u7cfb\u7edf\u7684\u53d8\u5f62\u955c\u65bd\u52a0\u5b66\u4e60\u63a7\u5236\u7684\u6ce2\u524d\u7578\u53d8\uff0c\u751f\u6210\u9ad8\u9891\u5b50\u50cf\u7d20\u4f4d\u79fb\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u7aef\u5230\u7aef\u4f18\u5316\u8054\u5408\u4e0a\u91c7\u6837\u3002", "result": "\u5b9e\u9a8c\u548c\u4eff\u771f\u663e\u793a\u4fe1\u566a\u6bd4\u63d0\u5347\u9ad8\u8fbe12 dB\uff0c\u65e0\u9700\u786c\u4ef6\u4fee\u6539\u5373\u53ef\u5b9e\u73b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u6709\u6548\u63d0\u5347\u671b\u8fdc\u955c\u6210\u50cf\u5206\u8fa8\u7387\uff0c\u4e14\u6613\u4e8e\u79fb\u690d\u5230\u5b9e\u9645\u671b\u8fdc\u955c\u7cfb\u7edf\u4e2d\u3002"}}
