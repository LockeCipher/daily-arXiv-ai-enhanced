<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 3]
- [cs.CV](#cs.CV) [Total: 107]
- [cs.CR](#cs.CR) [Total: 2]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [DiffTex: Differentiable Texturing for Architectural Proxy Models](https://arxiv.org/abs/2509.23336)
*Weidan Xiong,Yongli Wu,Bochuan Zeng,Jianwei Guo,Dani Lischinski,Daniel Cohen-Or,Hui Huang*

Main category: cs.GR

TL;DR: 提出一种从无序RGB照片自动生成建筑代理模型纹理图的自动化方法，通过可微分渲染优化混合参数，确保纹理的光度一致性和透视一致性。


<details>
  <summary>Details</summary>
Motivation: 建筑代理模型的几何简化导致颜色和几何细节丢失，需要纹理来补偿这些损失，但从无序照片中保留原始密集重建的丰富纹理信息仍然具有挑战性。

Method: 建立UV图上纹理与输入图像像素之间的对应关系，每个纹理的颜色计算为相关像素值的加权混合，使用可微分渲染优化混合参数以确保光度一致性和透视一致性。

Result: 实验结果表明该方法在不同建筑模型和不同摄影条件下都具有有效性和鲁棒性，能够创建保持视觉保真度和结构细节的高质量纹理。

Conclusion: 该方法能够自动生成高质量的建筑纹理，有效补偿几何简化带来的细节损失，为建筑代理模型提供逼真的纹理映射。

Abstract: Simplified proxy models are commonly used to represent architectural structures, reducing storage requirements and enabling real-time rendering. However, the geometric simplifications inherent in proxies result in a loss of fine color and geometric details, making it essential for textures to compensate for the loss. Preserving the rich texture information from the original dense architectural reconstructions remains a daunting task, particularly when working with unordered RGB photographs. We propose an automated method for generating realistic texture maps for architectural proxy models at the texel level from an unordered collection of registered photographs. Our approach establishes correspondences between texels on a UV map and pixels in the input images, with each texel's color computed as a weighted blend of associated pixel values. Using differentiable rendering, we optimize blending parameters to ensure photometric and perspective consistency, while maintaining seamless texture coherence. Experimental results demonstrate the effectiveness and robustness of our method across diverse architectural models and varying photographic conditions, enabling the creation of high-quality textures that preserve visual fidelity and structural detail.

</details>


### [2] [ZeroScene: A Zero-Shot Framework for 3D Scene Generation from a Single Image and Controllable Texture Editing](https://arxiv.org/abs/2509.23607)
*Xiang Tang,Ruotong Li,Xiaopeng Fan*

Main category: cs.GR

TL;DR: ZeroScene是一个零样本的3D场景生成系统，能够从单张图像重建3D场景并支持纹理编辑，通过结合大视觉模型先验知识实现场景一致性和多视角纹理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有单图像3D场景重建方法在复杂环境中难以同时保证单个资产质量和整体场景连贯性，纹理编辑技术也难以保持局部连续性和多视角一致性。

Method: 提取对象级2D分割和深度信息推断空间关系，联合优化点云的3D和2D投影损失来更新对象姿态；通过扩散模型约束和掩码引导渐进图像生成策略保持纹理一致性，并使用PBR材质估计增强真实感。

Result: 实验结果表明，该框架不仅确保生成资产的几何和外观准确性，还能忠实重建场景布局并生成与文本提示高度一致的详细纹理。

Conclusion: ZeroScene成功实现了零样本的单图像到3D场景重建和纹理编辑，在保持场景连贯性和多视角纹理一致性方面表现出色。

Abstract: In the field of 3D content generation, single image scene reconstruction methods still struggle to simultaneously ensure the quality of individual assets and the coherence of the overall scene in complex environments, while texture editing techniques often fail to maintain both local continuity and multi-view consistency. In this paper, we propose a novel system ZeroScene, which leverages the prior knowledge of large vision models to accomplish both single image-to-3D scene reconstruction and texture editing in a zero-shot manner. ZeroScene extracts object-level 2D segmentation and depth information from input images to infer spatial relationships within the scene. It then jointly optimizes 3D and 2D projection losses of the point cloud to update object poses for precise scene alignment, ultimately constructing a coherent and complete 3D scene that encompasses both foreground and background. Moreover, ZeroScene supports texture editing of objects in the scene. By imposing constraints on the diffusion model and introducing a mask-guided progressive image generation strategy, we effectively maintain texture consistency across multiple viewpoints and further enhance the realism of rendered results through Physically Based Rendering (PBR) material estimation. Experimental results demonstrate that our framework not only ensures the geometric and appearance accuracy of generated assets, but also faithfully reconstructs scene layouts and produces highly detailed textures that closely align with text prompts.

</details>


### [3] [Diff-3DCap: Shape Captioning with Diffusion Models](https://arxiv.org/abs/2509.23718)
*Zhenyu Shu,Jiawei Wen,Shiyang Li,Shiqing Xin,Ligang Liu*

Main category: cs.GR

TL;DR: Diff-3DCap使用投影视图表示3D对象，并采用连续扩散模型进行3D形状描述生成，无需额外分类器即可达到最先进方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统3D形状描述方法依赖昂贵的体素表示或物体检测技术，但效果不佳，需要更有效的解决方案。

Method: 使用投影视图序列表示3D对象，通过连续扩散模型在正向阶段用高斯噪声扰动嵌入描述，在反向阶段预测重构注释，利用预训练视觉语言模型的视觉嵌入作为引导信号。

Result: 实验结果表明Diff-3DCap能够达到当前最先进方法的可比性能。

Conclusion: Diff-3DCap提供了一种有效的3D形状描述方法，通过扩散模型和视觉语言模型的结合，无需额外分类器即可获得良好性能。

Abstract: The task of 3D shape captioning occupies a significant place within the domain of computer graphics and has garnered considerable interest in recent years. Traditional approaches to this challenge frequently depend on the utilization of costly voxel representations or object detection techniques, yet often fail to deliver satisfactory outcomes. To address the above challenges, in this paper, we introduce Diff-3DCap, which employs a sequence of projected views to represent a 3D object and a continuous diffusion model to facilitate the captioning process. More precisely, our approach utilizes the continuous diffusion model to perturb the embedded captions during the forward phase by introducing Gaussian noise and then predicts the reconstructed annotation during the reverse phase. Embedded within the diffusion framework is a commitment to leveraging a visual embedding obtained from a pre-trained visual-language model, which naturally allows the embedding to serve as a guiding signal, eliminating the need for an additional classifier. Extensive results of our experiments indicate that Diff-3DCap can achieve performance comparable to that of the current state-of-the-art methods.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [4] [Deep Learning Empowered Super-Resolution: A Comprehensive Survey and Future Prospects](https://arxiv.org/abs/2509.22692)
*Le Zhang,Ao Li,Qibin Hou,Ce Zhu,Yonina C. Eldar*

Main category: cs.CV

TL;DR: 这是一篇关于超分辨率技术的全面综述论文，涵盖了单图像超分辨率、视频超分辨率、立体超分辨率和光场超分辨率等多个领域，分析了超过150种SISR方法、近70种VSR方法以及约30种SSR和LFSR技术。


<details>
  <summary>Details</summary>
Motivation: 现有的超分辨率综述大多专注于特定领域，缺乏对该领域的全面概述。本文旨在提供一个深入全面的综述，涵盖超分辨率技术的各个分支。

Method: 对不同类型的超分辨率方法进行了系统分类和分析，包括方法论、数据集、评估协议、实证结果和复杂度分析，并根据不同的骨干结构进行了分类。

Result: 提供了超过150种SISR方法、近70种VSR方法以及约30种SSR和LFSR技术的详细分析，并创建了一个专门的GitHub仓库来促进相关工作的访问。

Conclusion: 这篇综述论文将成为该领域研究人员的宝贵资源，并为未来的研究提供指导，同时探讨了该领域中尚未充分研究的开放性问题。

Abstract: Super-resolution (SR) has garnered significant attention within the computer vision community, driven by advances in deep learning (DL) techniques and the growing demand for high-quality visual applications. With the expansion of this field, numerous surveys have emerged. Most existing surveys focus on specific domains, lacking a comprehensive overview of this field. Here, we present an in-depth review of diverse SR methods, encompassing single image super-resolution (SISR), video super-resolution (VSR), stereo super-resolution (SSR), and light field super-resolution (LFSR). We extensively cover over 150 SISR methods, nearly 70 VSR approaches, and approximately 30 techniques for SSR and LFSR. We analyze methodologies, datasets, evaluation protocols, empirical results, and complexity. In addition, we conducted a taxonomy based on each backbone structure according to the diverse purposes. We also explore valuable yet under-studied open issues in the field. We believe that this work will serve as a valuable resource and offer guidance to researchers in this domain. To facilitate access to related work, we created a dedicated repository available at https://github.com/AVC2-UESTC/Holistic-Super-Resolution-Review.

</details>


### [5] [LayoutAgent: A Vision-Language Agent Guided Compositional Diffusion for Spatial Layout Planning](https://arxiv.org/abs/2509.22720)
*Zezhong Fan,Xiaohan Li,Luyi Ma,Kai Zhao,Liang Peng,Topojoy Biswas,Evren Korpeoglu,Kaushiki Nag,Kannan Achan*

Main category: cs.CV

TL;DR: LayoutAgent是一个结合视觉语言推理和组合扩散的智能框架，用于生成具有语义关系和物理合理性的多物体场景布局。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型缺乏显式空间推理能力，导致物体布局不真实；而传统机器人空间规划方法难以捕捉视觉场景的语义丰富性。需要弥合这一差距。

Method: 首先使用视觉语言模型进行输入预处理（分割、物体尺寸估计、场景图构建、提示重写），然后利用组合扩散方法合成符合场景图关系的边界框，最后通过前景条件图像生成器渲染完整场景。

Result: 实验表明LayoutAgent在布局连贯性、空间真实性和美学对齐方面优于其他最先进的布局生成模型。

Conclusion: LayoutAgent成功地将视觉语言推理与组合扩散相结合，实现了更真实的多物体场景布局生成。

Abstract: Designing realistic multi-object scenes requires not only generating images, but also planning spatial layouts that respect semantic relations and physical plausibility. On one hand, while recent advances in diffusion models have enabled high-quality image generation, they lack explicit spatial reasoning, leading to unrealistic object layouts. On the other hand, traditional spatial planning methods in robotics emphasize geometric and relational consistency, but they struggle to capture semantic richness in visual scenes. To bridge this gap, in this paper, we propose LayoutAgent, an agentic framework that unifies vision-language reasoning with compositional diffusion for layout generation. Given multiple input images with target objects in them, our method first employs visual-language model to preprocess the inputs through segmentation, object size estimation, scene graph construction, and prompt rewriting. Then we leverage compositional diffusion-a method traditionally used in robotics-to synthesize bounding boxes that respect object relations encoded in the scene graph for spatial layouts. In the end, a foreground-conditioned image generator composes the complete scene by rendering the objects into the planned layout guided by designed prompts. Experiments demonstrate that LayoutAgent outperforms other state-of-the-art layout generation models in layout coherence, spatial realism and aesthetic alignment.

</details>


### [6] [MILR: Improving Multimodal Image Generation via Test-Time Latent Reasoning](https://arxiv.org/abs/2509.22761)
*Yapeng Mi,Hengli Li,Yanpeng Zhao,Chenxi Li,Huimin Wu,Xiaojian Ma,Song-Chun Zhu,Ying Nian Wu,Qing Li*

Main category: cs.CV

TL;DR: MILR是一种在测试时联合推理图像和文本的方法，在统一潜在向量空间中通过策略梯度搜索实现跨模态推理，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于推理的图像生成方法要么局限于单一模态（图像或文本），要么依赖高质量推理数据进行微调，存在局限性。

Method: 在统一多模态理解和生成（MUG）框架中，通过策略梯度方法在离散图像和文本标记的向量表示中进行搜索，由图像质量评判器指导推理过程。

Result: 在GenEval、T2I-CompBench和WISE基准测试中均达到最先进结果，在知识密集型WISE上总体得分0.63，比基线提高80%。

Conclusion: 在统一潜在空间中的联合推理是MILR强性能的关键，该方法在时间和文化推理方面展现出显著能力。

Abstract: Reasoning-augmented machine learning systems have shown improved performance in various domains, including image generation. However, existing reasoning-based methods for image generation either restrict reasoning to a single modality (image or text) or rely on high-quality reasoning data for fine-tuning. To tackle these limitations, we propose MILR, a test-time method that jointly reasons over image and text in a unified latent vector space. Reasoning in MILR is performed by searching through vector representations of discrete image and text tokens. Practically, this is implemented via the policy gradient method, guided by an image quality critic. We instantiate MILR within the unified multimodal understanding and generation (MUG) framework that natively supports language reasoning before image synthesis and thus facilitates cross-modal reasoning. The intermediate model outputs, which are to be optimized, serve as the unified latent space, enabling MILR to operate entirely at test time. We evaluate MILR on GenEval, T2I-CompBench, and WISE, achieving state-of-the-art results on all benchmarks. Notably, on knowledge-intensive WISE, MILR attains an overall score of 0.63, improving over the baseline by 80%. Our further analysis indicates that joint reasoning in the unified latent space is the key to its strong performance. Moreover, our qualitative studies reveal MILR's non-trivial ability in temporal and cultural reasoning, highlighting the efficacy of our reasoning method.

</details>


### [7] [UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation](https://arxiv.org/abs/2509.25079)
*Guanjun Wu,Jiemin Fang,Chen Yang,Sikuang Li,Taoran Yi,Jia Lu,Zanwei Zhou,Jiazhong Cen,Lingxi Xie,Xiaopeng Zhang,Wei Wei,Wenyu Liu,Xinggang Wang,Qi Tian*

Main category: cs.CV

TL;DR: UniLat3D是一个统一的3D资产生成框架，通过几何-外观统一VAE将高分辨率稀疏特征压缩到紧凑的UniLat潜在空间中，实现单阶段直接生成，避免了传统两阶段方法中的几何-纹理不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 传统3D生成方法采用两阶段流程（先几何后外观），导致几何-纹理不对齐和成本高昂的问题，需要更高效统一的生成方案。

Method: 提出几何-外观统一VAE，将高分辨率稀疏特征压缩为紧凑的UniLat潜在表示；基于此训练单一流匹配模型，直接从高斯噪声映射到UniLat。

Result: 在公开数据集上训练，UniLat3D能在数秒内从单张图像生成高质量3D资产，在视觉保真度和几何质量方面表现优异。

Conclusion: UniLat3D通过统一的潜在表示和单阶段生成流程，有效解决了传统方法的对齐问题，实现了高效高质量的3D资产生成。

Abstract: High-fidelity 3D asset generation is crucial for various industries. While recent 3D pretrained models show strong capability in producing realistic content, most are built upon diffusion models and follow a two-stage pipeline that first generates geometry and then synthesizes appearance. Such a decoupled design tends to produce geometry-texture misalignment and non-negligible cost. In this paper, we propose UniLat3D, a unified framework that encodes geometry and appearance in a single latent space, enabling direct single-stage generation. Our key contribution is a geometry-appearance Unified VAE, which compresses high-resolution sparse features into a compact latent representation -- UniLat. UniLat integrates structural and visual information into a dense low-resolution latent, which can be efficiently decoded into diverse 3D formats, e.g., 3D Gaussians and meshes. Based on this unified representation, we train a single flow-matching model to map Gaussian noise directly into UniLat, eliminating redundant stages. Trained solely on public datasets, UniLat3D produces high-quality 3D assets in seconds from a single image, achieving superior appearance fidelity and geometric quality. More demos \& code are available at https://unilat3d.github.io/

</details>


### [8] [DEFT: Decompositional Efficient Fine-Tuning for Text-to-Image Models](https://arxiv.org/abs/2509.22793)
*Komal Kumar,Rao Muhammad Anwer,Fahad Shahbaz Khan,Salman Khan,Ivan Laptev,Hisham Cholakkal*

Main category: cs.CV

TL;DR: DEFT是一种分解式高效微调框架，通过将预训练权重矩阵的更新分解为两个可训练矩阵：一个在低秩子空间补集上的投影和一个低秩更新，实现了在个性化、多任务统一和可编辑性之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像模型高效微调中面临的挑战：在有限图像中学习新概念、保持多任务指令能力、维持可编辑性之间取得平衡，同时最小化计算资源和可训练参数。

Method: DEFT框架将预训练权重矩阵更新分解为两个组件：1) 在由低秩矩阵张成的低秩子空间补集上的投影；2) 低秩更新。单个可训练低秩矩阵定义子空间，另一个可训练低秩矩阵在该子空间内实现灵活参数适应。

Result: 在Dreambooth、Dreambench Plus、InsDet和VisualCloze数据集上的广泛实验表明，DEFT在个性化、对象场景适应和视觉上下文学习的通用图像生成框架中实现了最先进的性能。

Conclusion: DEFT框架展示了高效微调的新兴特性，在保持模型可编辑性的同时，有效平衡了目标分布对齐、新概念学习和多任务统一能力。

Abstract: Efficient fine-tuning of pre-trained Text-to-Image (T2I) models involves adjusting the model to suit a particular task or dataset while minimizing computational resources and limiting the number of trainable parameters. However, it often faces challenges in striking a trade-off between aligning with the target distribution: learning a novel concept from a limited image for personalization and retaining the instruction ability needed for unifying multiple tasks, all while maintaining editability (aligning with a variety of prompts or in-context generation). In this work, we introduce DEFT, Decompositional Efficient Fine-Tuning, an efficient fine-tuning framework that adapts a pre-trained weight matrix by decomposing its update into two components with two trainable matrices: (1) a projection onto the complement of a low-rank subspace spanned by a low-rank matrix, and (2) a low-rank update. The single trainable low-rank matrix defines the subspace, while the other trainable low-rank matrix enables flexible parameter adaptation within that subspace. We conducted extensive experiments on the Dreambooth and Dreambench Plus datasets for personalization, the InsDet dataset for object and scene adaptation, and the VisualCloze dataset for a universal image generation framework through visual in-context learning with both Stable Diffusion and a unified model. Our results demonstrated state-of-the-art performance, highlighting the emergent properties of efficient fine-tuning. Our code is available on \href{https://github.com/MAXNORM8650/DEFT}{DEFTBase}.

</details>


### [9] [VideoScore2: Think before You Score in Generative Video Evaluation](https://arxiv.org/abs/2509.22799)
*Xuan He,Dongfu Jiang,Ping Nie,Minghao Liu,Zhengxuan Jiang,Mingyi Su,Wentao Ma,Junru Lin,Chun Ye,Yi Lu,Keming Wu,Benjamin Schneider,Quy Duc Do,Zhuofeng Li,Yiming Jia,Yuxuan Zhang,Guo Cheng,Haozhe Wang,Wangchunshu Zhou,Qunshu Lin,Yuanxing Zhang,Ge Zhang,Wenhao Huang,Wenhu Chen*

Main category: cs.CV

TL;DR: VideoScore2是一个多维度、可解释的视频评估框架，通过显式评估视觉质量、文本-视频对齐和物理/常识一致性，提供详细的推理过程，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的视频评估方法局限于单一不透明分数，缺乏可解释性，或仅提供粗略分析，无法全面捕捉视频质量评估的多方面特性。

Method: 使用包含27,168个人工标注视频的大规模数据集VideoFeedback2，采用两阶段训练流程：监督微调后使用组相对策略优化(GRPO)进行强化学习，以增强分析鲁棒性。

Result: VideoScore2在内部基准VideoScore-Bench-v2上达到44.35的准确率（提升5.94），在四个外部基准上平均性能达到50.37（提升4.32）。

Conclusion: VideoScore2提供了可解释的评估，通过有效的奖励建模为最佳N采样搭建了评估与可控生成之间的桥梁。

Abstract: Recent advances in text-to-video generation have produced increasingly realistic and diverse content, yet evaluating such videos remains a fundamental challenge due to their multi-faceted nature encompassing visual quality, semantic alignment, and physical consistency. Existing evaluators and reward models are limited to single opaque scores, lack interpretability, or provide only coarse analysis, making them insufficient for capturing the comprehensive nature of video quality assessment. We present VideoScore2, a multi-dimensional, interpretable, and human-aligned framework that explicitly evaluates visual quality, text-to-video alignment, and physical/common-sense consistency while producing detailed chain-of-thought rationales. Our model is trained on a large-scale dataset VideoFeedback2 containing 27,168 human-annotated videos with both scores and reasoning traces across three dimensions, using a two-stage pipeline of supervised fine-tuning followed by reinforcement learning with Group Relative Policy Optimization (GRPO) to enhance analytical robustness. Extensive experiments demonstrate that VideoScore2 achieves superior performance with 44.35 (+5.94) accuracy on our in-domain benchmark VideoScore-Bench-v2 and 50.37 (+4.32) average performance across four out-of-domain benchmarks (VideoGenReward-Bench, VideoPhy2, etc), while providing interpretable assessments that bridge the gap between evaluation and controllable generation through effective reward modeling for Best-of-N sampling. Project Page: https://tiger-ai-lab.github.io/VideoScore2/

</details>


### [10] [Seeing Isn't Believing: Context-Aware Adversarial Patch Synthesis via Conditional GAN](https://arxiv.org/abs/2509.22836)
*Roie Kazoom,Alon Goldberg,Hodaya Cohen,Ofer Hadar*

Main category: cs.CV

TL;DR: 提出了一种完全可控的对抗性补丁生成框架，攻击者可以自由选择输入图像和目标类别，实现精确的错误分类结果。该方法结合生成式U-Net设计和Grad-CAM引导的补丁放置，在保持视觉真实性的同时最大化攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有对抗性补丁攻击方法大多依赖不切实际的白盒假设、无目标攻击目标，或产生视觉上明显的补丁，限制了实际应用。需要一种既能确保真实性、目标控制，又能在黑盒环境中适用的方法。

Method: 结合生成式U-Net架构和Grad-CAM引导的补丁放置，实现语义感知的定位策略，在保持视觉真实性的同时优化攻击效果。

Result: 在卷积网络和视觉变换器上的广泛实验表明，该方法在所有设置下都达到了最先进的性能，攻击成功率和目标类别成功率均超过99%，超越了现有的白盒攻击、无目标基线和产生可检测伪影的方法。

Conclusion: 该框架通过同时确保真实性、目标控制和黑盒适用性，为对抗性鲁棒性研究建立了新基准，弥合了理论攻击强度与实际隐蔽性之间的差距。

Abstract: Adversarial patch attacks pose a severe threat to deep neural networks, yet most existing approaches rely on unrealistic white-box assumptions, untargeted objectives, or produce visually conspicuous patches that limit real-world applicability. In this work, we introduce a novel framework for fully controllable adversarial patch generation, where the attacker can freely choose both the input image x and the target class y target, thereby dictating the exact misclassification outcome. Our method combines a generative U-Net design with Grad-CAM-guided patch placement, enabling semantic-aware localization that maximizes attack effectiveness while preserving visual realism. Extensive experiments across convolutional networks (DenseNet-121, ResNet-50) and vision transformers (ViT-B/16, Swin-B/16, among others) demonstrate that our approach achieves state-of-the-art performance across all settings, with attack success rates (ASR) and target-class success (TCS) consistently exceeding 99%.   Importantly, we show that our method not only outperforms prior white-box attacks and untargeted baselines, but also surpasses existing non-realistic approaches that produce detectable artifacts. By simultaneously ensuring realism, targeted control, and black-box applicability-the three most challenging dimensions of patch-based attacks-our framework establishes a new benchmark for adversarial robustness research, bridging the gap between theoretical attack strength and practical stealthiness.

</details>


### [11] [ControlEvents: Controllable Synthesis of Event Camera Datawith Foundational Prior from Image Diffusion Models](https://arxiv.org/abs/2509.22864)
*Yixuan Hu,Yuxuan Xue,Simon Klenk,Daniel Cremers,Gerard Pons-Moll*

Main category: cs.CV

TL;DR: ControlEvents是一个基于扩散模型的生成方法，能够通过文本标签、2D骨架和3D身体姿态等控制信号合成高质量的事件数据，显著降低事件数据标注成本。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有高时间分辨率和高动态范围等生物启发特性，但获取大规模标注的事件数据仍然具有挑战性且成本高昂。

Method: 利用Stable Diffusion等基础模型的扩散先验，通过最小化微调和有限标注数据实现高质量事件数据生成，支持文本标签、2D骨架和3D姿态等多种控制信号。

Result: 实验表明合成的标注事件数据在视觉识别、2D骨架估计和3D姿态估计等任务中都能提升模型性能，并且能够基于训练中未见过的文本标签生成事件数据。

Conclusion: 该方法简化了数据生成流程，显著降低了标注事件数据集的成本，并继承了基础模型强大的基于文本的生成能力。

Abstract: In recent years, event cameras have gained significant attention due to their bio-inspired properties, such as high temporal resolution and high dynamic range. However, obtaining large-scale labeled ground-truth data for event-based vision tasks remains challenging and costly. In this paper, we present ControlEvents, a diffusion-based generative model designed to synthesize high-quality event data guided by diverse control signals such as class text labels, 2D skeletons, and 3D body poses. Our key insight is to leverage the diffusion prior from foundation models, such as Stable Diffusion, enabling high-quality event data generation with minimal fine-tuning and limited labeled data. Our method streamlines the data generation process and significantly reduces the cost of producing labeled event datasets. We demonstrate the effectiveness of our approach by synthesizing event data for visual recognition, 2D skeleton estimation, and 3D body pose estimation. Our experiments show that the synthesized labeled event data enhances model performance in all tasks. Additionally, our approach can generate events based on unseen text labels during training, illustrating the powerful text-based generation capabilities inherited from foundation models.

</details>


### [12] [Learning Unified Representation of 3D Gaussian Splatting](https://arxiv.org/abs/2509.22917)
*Yuelin Xin,Yuheng Liu,Xiaohui Xie,Xinke Li*

Main category: cs.CV

TL;DR: 提出了一种基于连续子流形场的3D高斯泼溅嵌入表示方法，解决了原始高斯参数作为神经网络特征时的不唯一性和异构性问题。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅的参数化表示难以作为神经网络特征学习，因为高斯参数具有非唯一性和异构性，直接使用会导致模型高度依赖数据。

Method: 基于连续子流形场构建3D高斯泼溅的嵌入表示，封装高斯基元的本质信息，保持颜色和几何结构的同时确保唯一映射和通道同质性。

Result: 该方法为3D高斯泼溅学习提供了更原则性的表示方法。

Conclusion: 提出的嵌入表示方法能够更好地支持基于3D高斯泼溅的学习系统。

Abstract: A well-designed vectorized representation is crucial for the learning systems natively based on 3D Gaussian Splatting. While 3DGS enables efficient and explicit 3D reconstruction, its parameter-based representation remains hard to learn as features, especially for neural-network-based models. Directly feeding raw Gaussian parameters into learning frameworks fails to address the non-unique and heterogeneous nature of the Gaussian parameterization, yielding highly data-dependent models. This challenge motivates us to explore a more principled approach to represent 3D Gaussian Splatting in neural networks that preserves the underlying color and geometric structure while enforcing unique mapping and channel homogeneity. In this paper, we propose an embedding representation of 3DGS based on continuous submanifold fields that encapsulate the intrinsic information of Gaussian primitives, thereby benefiting the learning of 3DGS.

</details>


### [13] [Soft-Di[M]O: Improving One-Step Discrete Image Generation with Soft Embeddings](https://arxiv.org/abs/2509.22925)
*Yuanzhi Zhu,Xi Wang,Stéphane Lathuilière,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: 本文提出了软嵌入方法，将一步生成器中的离散标记替换为生成器输出分布的期望嵌入，解决了离散标记阻碍梯度流动的问题，使一步生成器可进行端到端训练和后蒸馏优化。


<details>
  <summary>Details</summary>
Motivation: 解决一步生成器存在的两个关键限制：继承教师模型的建模偏差，以及离散标记输出阻碍梯度流动，无法进行对抗训练、基于奖励的微调和测试时嵌入优化等后蒸馏优化。

Method: 引入软嵌入方法，用生成器输出分布的期望嵌入替换离散标记，保持表示保真度的同时提供完全可微的连续替代方案，与教师骨干网络和标记器解码器兼容。

Result: 在多个MDM教师模型上，Soft-Di[M]O实现了一流的一步生成结果：改进的类到图像性能，在ImageNet-256上通过GAN优化达到1.56的一步FID，文本到图像任务上获得更高的GenEval和HPS分数，并通过TTEO获得进一步增益。

Conclusion: 软嵌入方法使一步生成器可进行端到端训练，支持GAN优化、可微奖励微调和测试时嵌入优化，显著提升了一步生成器的性能。

Abstract: One-step generators distilled from Masked Diffusion Models (MDMs) compress multiple sampling steps into a single forward pass, enabling efficient text and image synthesis. However, they suffer two key limitations: they inherit modeling bias from the teacher, and their discrete token outputs block gradient flow, preventing post-distillation refinements such as adversarial training, reward-based fine-tuning, and Test-Time Embedding Optimization (TTEO). In this work, we introduce soft embeddings, a simple relaxation that replaces discrete tokens with the expected embeddings under the generator's output distribution. Soft embeddings preserve representation fidelity for one-step discrete generator while providing a fully differentiable continuous surrogate that is compatible with teacher backbones and tokenizer decoders. Integrating soft embeddings into the Di[M]O distillation framework (denoted Soft-Di[M]O) makes one-step generators end-to-end trainable and enables straightforward application of GAN-based refinement, differentiable reward fine-tuning, and TTEO. Empirically, across multiple MDM teachers (e.g., MaskBit, MaskGen), Soft-Di[M]O achieves state-of-the-art one-step results: improved class-to-image performance, a one-step FID of 1.56 on ImageNet-256 with GAN-based refinement, along with higher GenEval and HPS scores on text-to-image with reward fine-tuning, and further gains from TTEO.

</details>


### [14] [FishAI 2.0: Marine Fish Image Classification with Multi-modal Few-shot Learning](https://arxiv.org/abs/2509.22930)
*Chenghan Yang,Peng Zhou,Dong-Sheng Zhang,Yueyun Wang,Hong-Bin Shen,Xiaoyong Pan*

Main category: cs.CV

TL;DR: FishAI 2.0是一个智能海洋鱼类识别框架，通过多模态小样本深度学习和图像生成技术解决稀有物种数据稀缺问题，显著提升了识别准确率。


<details>
  <summary>Details</summary>
Motivation: 传统海洋生物图像识别面临数据集不完整和模型精度不足的挑战，特别是在稀有物种的小样本条件下，数据稀缺严重影响性能。

Method: 使用分层海洋鱼类基准数据集训练模型，利用DeepSeek生成高质量文本描述，通过Stable Diffusion 2进行图像增强，构建多模态特征空间，并采用基于CLIP的模型进行小样本图像识别。

Result: FishAI 2.0在科级别达到Top-1准确率91.67%和Top-5准确率97.97%，在属和种级别分别达到87.58%和85.42%，显著优于基线模型。

Conclusion: FishAI 2.0提高了海洋鱼类识别的效率和准确性，为海洋生态监测和保护提供了可扩展的技术解决方案，具有科学价值和实际应用价值。

Abstract: Traditional marine biological image recognition faces challenges of incomplete datasets and unsatisfactory model accuracy, particularly for few-shot conditions of rare species where data scarcity significantly hampers the performance. To address these issues, this study proposes an intelligent marine fish recognition framework, FishAI 2.0, integrating multimodal few-shot deep learning techniques with image generation for data augmentation. First, a hierarchical marine fish benchmark dataset, which provides a comprehensive data foundation for subsequent model training, is utilized to train the FishAI 2.0 model. To address the data scarcity of rare classes, the large language model DeepSeek was employed to generate high-quality textual descriptions, which are input into Stable Diffusion 2 for image augmentation through a hierarchical diffusion strategy that extracts latent encoding to construct a multimodal feature space. The enhanced visual-textual datasets were then fed into a Contrastive Language-Image Pre-Training (CLIP) based model, enabling robust few-shot image recognition. Experimental results demonstrate that FishAI 2.0 achieves a Top-1 accuracy of 91.67 percent and Top-5 accuracy of 97.97 percent at the family level, outperforming baseline CLIP and ViT models with a substantial margin for the minority classes with fewer than 10 training samples. To better apply FishAI 2.0 to real-world scenarios, at the genus and species level, FishAI 2.0 respectively achieves a Top-1 accuracy of 87.58 percent and 85.42 percent, demonstrating practical utility. In summary, FishAI 2.0 improves the efficiency and accuracy of marine fish identification and provides a scalable technical solution for marine ecological monitoring and conservation, highlighting its scientific value and practical applicability.

</details>


### [15] [Copyright Infringement Detection in Text-to-Image Diffusion Models via Differential Privacy](https://arxiv.org/abs/2509.23022)
*Xiafeng Man,Zhipeng Wei,Jingjing Chen*

Main category: cs.CV

TL;DR: 提出了D-Plus-Minus (DPM)框架，基于差分隐私理论检测扩散模型中的版权侵权内容，无需访问原始训练数据或文本提示。


<details>
  <summary>Details</summary>
Motivation: 大型视觉模型如Stable Diffusion可能记忆并复制受版权保护的内容，现有检测方法缺乏鲁棒性和理论基础。

Method: 引入条件敏感性指标，通过微调模型在两个相反方向（学习/遗忘）来模拟包含和排除过程，并使用统计指标在正交提示分布上计算置信度分数。

Result: DPM能够可靠地检测侵权内容，并构建了CIDD数据集用于标准化基准测试。

Conclusion: DPM为生成AI时代的知识产权保护提供了可解释且实用的解决方案。

Abstract: The widespread deployment of large vision models such as Stable Diffusion raises significant legal and ethical concerns, as these models can memorize and reproduce copyrighted content without authorization. Existing detection approaches often lack robustness and fail to provide rigorous theoretical underpinnings. To address these gaps, we formalize the concept of copyright infringement and its detection from the perspective of Differential Privacy (DP), and introduce the conditional sensitivity metric, a concept analogous to sensitivity in DP, that quantifies the deviation in a diffusion model's output caused by the inclusion or exclusion of a specific training data point. To operationalize this metric, we propose D-Plus-Minus (DPM), a novel post-hoc detection framework that identifies copyright infringement in text-to-image diffusion models. Specifically, DPM simulates inclusion and exclusion processes by fine-tuning models in two opposing directions: learning or unlearning. Besides, to disentangle concept-specific influence from the global parameter shifts induced by fine-tuning, DPM computes confidence scores over orthogonal prompt distributions using statistical metrics. Moreover, to facilitate standardized benchmarking, we also construct the Copyright Infringement Detection Dataset (CIDD), a comprehensive resource for evaluating detection across diverse categories. Our results demonstrate that DPM reliably detects infringement content without requiring access to the original training dataset or text prompts, offering an interpretable and practical solution for safeguarding intellectual property in the era of generative AI.

</details>


### [16] [Perceptual Influence: Improving the Perceptual Loss Design for Low-Dose CT Enhancement](https://arxiv.org/abs/2509.23025)
*Gabriel A. Viana,Luis F. Alves Pereira,Tsang Ing Ren,George D. C. Cavalcanti,Jan Sijbers*

Main category: cs.CV

TL;DR: 本文提出了感知影响度概念和评估框架，系统分析了感知损失在低剂量CT图像增强中的设计选择，发现文献中广泛使用的配置表现不佳，优化后的感知损失设计能显著改善重建CT图像的质量。


<details>
  <summary>Details</summary>
Motivation: 感知损失已成为训练网络增强低剂量CT图像的有力工具，但感知损失的设计涉及关键但未被充分探索的决策，包括特征表示层级、预训练编码器数据集以及感知分量的相对重要性。

Method: 引入感知影响度概念（量化感知损失项对总损失的相对贡献），提出原则性框架评估损失设计选择对模型训练性能的影响，通过系统实验比较不同配置。

Result: 文献中广泛使用的感知损失配置表现不如优化设计的替代方案，更好的感知损失设计能显著改善重建CT图像的噪声降低和结构保真度，无需改变网络架构。

Conclusion: 提供了基于统计分析的目标指导原则，支持感知损失在低剂量CT去噪中的有效使用，证明了优化感知损失设计的重要性。

Abstract: Perceptual losses have emerged as powerful tools for training networks to enhance Low-Dose Computed Tomography (LDCT) images, offering an alternative to traditional pixel-wise losses such as Mean Squared Error, which often lead to over-smoothed reconstructions and loss of clinically relevant details in LDCT images. The perceptual losses operate in a latent feature space defined by a pretrained encoder and aim to preserve semantic content by comparing high-level features rather than raw pixel values. However, the design of perceptual losses involves critical yet underexplored decisions, including the feature representation level, the dataset used to pretrain the encoder, and the relative importance assigned to the perceptual component during optimization. In this work, we introduce the concept of perceptual influence (a metric that quantifies the relative contribution of the perceptual loss term to the total loss) and propose a principled framework to assess the impact of the loss design choices on the model training performance. Through systematic experimentation, we show that the widely used configurations in the literature to set up a perceptual loss underperform compared to better-designed alternatives. Our findings show that better perceptual loss designs lead to significant improvements in noise reduction and structural fidelity of reconstructed CT images, without requiring any changes to the network architecture. We also provide objective guidelines, supported by statistical analysis, to inform the effective use of perceptual losses in LDCT denoising. Our source code is available at https://github.com/vngabriel/perceptual-influence.

</details>


### [17] [Follow-Your-Preference: Towards Preference-Aligned Image Inpainting](https://arxiv.org/abs/2509.23082)
*Yutao Shen,Junkun Yuan,Toru Aonishi,Hideki Nakayama,Yue Ma*

Main category: cs.CV

TL;DR: 该论文重新审视图像修复中的偏好对齐问题，使用直接偏好优化方法进行对齐训练，并利用公共奖励模型构建偏好训练数据集。研究发现大多数奖励模型能提供有效奖励分数，偏好数据在不同模型和基准测试中表现出稳健趋势，但奖励模型存在可观察的偏见。通过简单集成这些模型可以缓解偏见，获得稳健和泛化性强的结果。


<details>
  <summary>Details</summary>
Motivation: 重新审视图像修复中偏好对齐的基本问题，而不是引入新方法，旨在建立一个简单而坚实的基准线。

Method: 利用直接偏好优化方法进行对齐训练，使用公共奖励模型构建偏好训练数据集，在九个奖励模型、两个基准测试和两个基线模型上进行实验，采用简单集成策略缓解奖励模型偏见。

Result: 对齐模型在标准指标、GPT-4评估和人工评估中显著优于先前模型，无需改变模型结构或使用新数据集。

Conclusion: 该工作为图像修复中的偏好对齐提供了一个简单而有效的基准线，通过缓解奖励模型偏见获得了稳健和泛化性强的结果。

Abstract: This paper investigates image inpainting with preference alignment. Instead of introducing a novel method, we go back to basics and revisit fundamental problems in achieving such alignment. We leverage the prominent direct preference optimization approach for alignment training and employ public reward models to construct preference training datasets. Experiments are conducted across nine reward models, two benchmarks, and two baseline models with varying structures and generative algorithms. Our key findings are as follows: (1) Most reward models deliver valid reward scores for constructing preference data, even if some of them are not reliable evaluators. (2) Preference data demonstrates robust trends in both candidate scaling and sample scaling across models and benchmarks. (3) Observable biases in reward models, particularly in brightness, composition, and color scheme, render them susceptible to cause reward hacking. (4) A simple ensemble of these models yields robust and generalizable results by mitigating such biases. Built upon these observations, our alignment models significantly outperform prior models across standard metrics, GPT-4 assessments, and human evaluations, without any changes to model structures or the use of new datasets. We hope our work can set a simple yet solid baseline, pushing this promising frontier. Our code is open-sourced at: https://github.com/shenytzzz/Follow-Your-Preference.

</details>


### [18] [Stochastic Interpolants via Conditional Dependent Coupling](https://arxiv.org/abs/2509.23122)
*Chenrui Ma,Xi Xiao,Tianyang Wang,Xiao Wang,Yanning Shen*

Main category: cs.CV

TL;DR: 提出基于条件依赖耦合策略的统一多阶段生成框架，解决图像生成模型在计算成本与保真度之间的权衡问题，通过多阶段插值轨迹分解生成过程，实现端到端优化和知识共享。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型面临计算成本与保真度的权衡挑战：基于预训练VAE的模型存在信息丢失和细节有限问题，像素空间模型计算成本过高，级联模型无法端到端优化且各阶段分布学习不准确。

Method: 提出条件依赖耦合策略，将生成过程分解为多阶段插值轨迹，使用统一的扩散Transformer建模整个过程，避免分离模块并实现知识共享。

Result: 大量实验表明，该方法在多个分辨率下同时实现了高保真度和高效率。

Conclusion: 该统一多阶段生成框架成功解决了图像生成中计算效率与保真度的权衡问题，通过端到端优化和知识共享实现了优越性能。

Abstract: Existing image generation models face critical challenges regarding the trade-off between computation and fidelity. Specifically, models relying on a pretrained Variational Autoencoder (VAE) suffer from information loss, limited detail, and the inability to support end-to-end training. In contrast, models operating directly in the pixel space incur prohibitive computational cost. Although cascade models can mitigate computational cost, stage-wise separation prevents effective end-to-end optimization, hampers knowledge sharing, and often results in inaccurate distribution learning within each stage. To address these challenges, we introduce a unified multistage generative framework based on our proposed Conditional Dependent Coupling strategy. It decomposes the generative process into interpolant trajectories at multiple stages, ensuring accurate distribution learning while enabling end-to-end optimization. Importantly, the entire process is modeled as a single unified Diffusion Transformer, eliminating the need for disjoint modules and also enabling knowledge sharing. Extensive experiments demonstrate that our method achieves both high fidelity and efficiency across multiple resolutions.

</details>


### [19] [WeatherCycle: Unpaired Multi-Weather Restoration via Color Space Decoupled Cycle Learning](https://arxiv.org/abs/2509.23150)
*Wenxuan Fang,Jiangwei Weng,Jianjun Qian,Jian Yang,Jun Li*

Main category: cs.CV

TL;DR: WeatherCycle是一个统一的非配对框架，通过双向降解-内容转换循环和降解感知课程正则化来解决多天气条件下的图像恢复问题。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常依赖特定任务的物理先验，其狭窄的焦点限制了在多样化真实天气场景中的可扩展性和泛化能力。

Method: 采用亮度-色度分解策略解耦降解与内容，提出亮度降解引导模块(LDGM)学习亮度降解先验，并引入难度感知对比正则化(DACR)模块增强语义一致性。

Result: 在多个多天气数据集上的广泛实验表明，该方法在无监督方法中达到了最先进的性能，对复杂天气降解具有强泛化能力。

Conclusion: WeatherCycle通过统一的框架有效解决了多天气条件下的图像恢复问题，无需复杂天气建模即可实现降解与内容的解耦。

Abstract: Unsupervised image restoration under multi-weather conditions remains a fundamental yet underexplored challenge. While existing methods often rely on task-specific physical priors, their narrow focus limits scalability and generalization to diverse real-world weather scenarios. In this work, we propose \textbf{WeatherCycle}, a unified unpaired framework that reformulates weather restoration as a bidirectional degradation-content translation cycle, guided by degradation-aware curriculum regularization. At its core, WeatherCycle employs a \textit{lumina-chroma decomposition} strategy to decouple degradation from content without modeling complex weather, enabling domain conversion between degraded and clean images. To model diverse and complex degradations, we propose a \textit{Lumina Degradation Guidance Module} (LDGM), which learns luminance degradation priors from a degraded image pool and injects them into clean images via frequency-domain amplitude modulation, enabling controllable and realistic degradation modeling. Additionally, we incorporate a \textit{Difficulty-Aware Contrastive Regularization (DACR)} module that identifies hard samples via a CLIP-based classifier and enforces contrastive alignment between hard samples and restored features to enhance semantic consistency and robustness. Extensive experiments across serve multi-weather datasets, demonstrate that our method achieves state-of-the-art performance among unsupervised approaches, with strong generalization to complex weather degradations.

</details>


### [20] [Sparse2Dense: A Keypoint-driven Generative Framework for Human Video Compression and Vertex Prediction](https://arxiv.org/abs/2509.23169)
*Bolin Chen,Ru-Ling Liao,Yan Ye,Jie Chen,Shanzhi Yin,Xinrui Ju,Shiqi Wang,Yibo Fan*

Main category: cs.CV

TL;DR: Sparse2Dense是一个基于关键点的生成框架，利用极稀疏的3D关键点作为传输符号，实现超低码率人体视频压缩和精确人体顶点预测。


<details>
  <summary>Details</summary>
Motivation: 解决带宽受限多媒体应用中同时实现超低码率人体视频压缩和准确顶点预测的挑战，需要协调动态运动建模、细节外观合成和几何一致性。

Method: 提出多任务学习和关键点感知的深度生成模型，通过紧凑3D关键点编码复杂人体运动，并利用这些稀疏关键点估计密集运动以实现时间一致和真实纹理的视频合成。集成顶点预测器通过联合优化学习人体顶点几何。

Result: 实验表明Sparse2Dense在人体视频压缩方面优于传统/生成视频编解码器，同时能够实现精确的人体顶点预测。

Conclusion: Sparse2Dense有望促进带宽高效的人体中心媒体传输，如实时运动分析、虚拟人体动画和沉浸式娱乐。

Abstract: For bandwidth-constrained multimedia applications, simultaneously achieving ultra-low bitrate human video compression and accurate vertex prediction remains a critical challenge, as it demands the harmonization of dynamic motion modeling, detailed appearance synthesis, and geometric consistency. To address this challenge, we propose Sparse2Dense, a keypoint-driven generative framework that leverages extremely sparse 3D keypoints as compact transmitted symbols to enable ultra-low bitrate human video compression and precise human vertex prediction. The key innovation is the multi-task learning-based and keypoint-aware deep generative model, which could encode complex human motion via compact 3D keypoints and leverage these sparse keypoints to estimate dense motion for video synthesis with temporal coherence and realistic textures. Additionally, a vertex predictor is integrated to learn human vertex geometry through joint optimization with video generation, ensuring alignment between visual content and geometric structure. Extensive experiments demonstrate that the proposed Sparse2Dense framework achieves competitive compression performance for human video over traditional/generative video codecs, whilst enabling precise human vertex prediction for downstream geometry applications. As such, Sparse2Dense is expected to facilitate bandwidth-efficient human-centric media transmission, such as real-time motion analysis, virtual human animation, and immersive entertainment.

</details>


### [21] [UltraUNet: Real-Time Ultrasound Tongue Segmentation for Diverse Linguistic and Imaging Conditions](https://arxiv.org/abs/2509.23225)
*Alisher Myrgyyassov,Zhen Song,Yu Sun,Bruce Xiao Wang,Min Ney Wong,Yongping Zheng*

Main category: cs.CV

TL;DR: 提出UltraUNet轻量级编码器-解码器架构，用于实时超声舌轮廓分割，达到250帧/秒速度，在8个数据集上表现出高准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 超声舌成像是一种非侵入性、成本效益高的工具，但实时舌轮廓分割面临信噪比低、成像变异性和计算需求等挑战。

Method: 采用轻量级编码器-解码器架构，包含领域特定创新：轻量级Squeeze-and-Excitation块、小批量稳定性的Group Normalization、基于求和的跳跃连接以减少内存和计算开销，并集成超声特定增强如去噪和模糊模拟。

Result: 在8个数据集上评估显示高准确性和鲁棒性，单数据集Dice=0.855、MSD=0.993px，跨数据集Dice平均为0.734和0.761。

Conclusion: UltraUNet为语音研究、临床诊断和言语运动障碍分析提供了快速准确的解决方案。

Abstract: Ultrasound tongue imaging (UTI) is a non-invasive and cost-effective tool for studying speech articulation, motor control, and related disorders. However, real-time tongue contour segmentation remains challenging due to low signal-to-noise ratios, imaging variability, and computational demands. We propose UltraUNet, a lightweight encoder-decoder architecture optimized for real-time segmentation of tongue contours in ultrasound images. UltraUNet incorporates domain-specific innovations such as lightweight Squeeze-and-Excitation blocks, Group Normalization for small-batch stability, and summation-based skip connections to reduce memory and computational overhead. It achieves 250 frames per second and integrates ultrasound-specific augmentations like denoising and blur simulation. Evaluations on 8 datasets demonstrate high accuracy and robustness, with single-dataset Dice = 0.855 and MSD = 0.993px, and cross-dataset Dice averaging 0.734 and 0.761. UltraUNet provides a fast, accurate solution for speech research, clinical diagnostics, and analysis of speech motor disorders.

</details>


### [22] [OracleGS: Grounding Generative Priors for Sparse-View Gaussian Splatting](https://arxiv.org/abs/2509.23258)
*Atakan Topaloglu,Kunyi Li,Michael Niemeyer,Nassir Navab,A. Murat Tekalp,Federico Tombari*

Main category: cs.CV

TL;DR: OracleGS通过结合生成模型的完整性和回归模型的几何保真度，解决了稀疏视图新视角合成中的几何模糊问题。它使用预训练的3D感知扩散模型生成完整场景，然后利用多视角立体模型作为3D感知oracle验证生成视图的3D不确定性，通过不确定性加权损失指导3D高斯溅射优化。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图新视角合成存在严重的几何模糊问题。现有方法陷入两难：回归模型几何保真但不完整，生成模型能补全场景但常引入结构不一致。需要一种能兼顾生成完整性和回归保真度的解决方案。

Method: 提出"提议-验证"框架：1）使用预训练3D感知扩散模型合成新视角以生成完整场景；2）将多视角立体模型重新用作3D感知oracle，通过其注意力图验证生成视图的3D不确定性；3）使用不确定性信号通过加权损失指导3D高斯溅射优化。

Result: 在Mip-NeRF 360和NeRF Synthetic等数据集上优于最先进方法。该方法将强大的生成先验条件化于多视角几何证据，过滤幻觉伪影，同时在约束不足区域保留合理补全。

Conclusion: OracleGS成功调和了生成完整性与回归保真度之间的权衡，通过条件化生成先验于多视角几何证据，实现了更准确可靠的稀疏视图新视角合成。

Abstract: Sparse-view novel view synthesis is fundamentally ill-posed due to severe geometric ambiguity. Current methods are caught in a trade-off: regressive models are geometrically faithful but incomplete, whereas generative models can complete scenes but often introduce structural inconsistencies. We propose OracleGS, a novel framework that reconciles generative completeness with regressive fidelity for sparse view Gaussian Splatting. Instead of using generative models to patch incomplete reconstructions, our "propose-and-validate" framework first leverages a pre-trained 3D-aware diffusion model to synthesize novel views to propose a complete scene. We then repurpose a multi-view stereo (MVS) model as a 3D-aware oracle to validate the 3D uncertainties of generated views, using its attention maps to reveal regions where the generated views are well-supported by multi-view evidence versus where they fall into regions of high uncertainty due to occlusion, lack of texture, or direct inconsistency. This uncertainty signal directly guides the optimization of a 3D Gaussian Splatting model via an uncertainty-weighted loss. Our approach conditions the powerful generative prior on multi-view geometric evidence, filtering hallucinatory artifacts while preserving plausible completions in under-constrained regions, outperforming state-of-the-art methods on datasets including Mip-NeRF 360 and NeRF Synthetic.

</details>


### [23] [Vid-Freeze: Protecting Images from Malicious Image-to-Video Generation via Temporal Freezing](https://arxiv.org/abs/2509.23279)
*Rohit Chowdhury,Aniruddha Bala,Rohan Jaiswal,Siddharth Roheda*

Main category: cs.CV

TL;DR: Vid-Freeze是一种针对图像到视频生成模型的对抗攻击方法，通过添加精心设计的对抗扰动来抑制注意力机制，阻止运动合成，从而保护图像不被恶意使用。


<details>
  <summary>Details</summary>
Motivation: 随着图像到视频生成模型的快速发展，从静态图像合成视频带来了严重风险，可能被用于创建欺骗性或恶意内容。现有防御方法如I2VGuard尝试免疫图像，但有效阻止运动的方法仍有待探索。

Method: 提出Vid-Freeze方法，通过添加精心设计的对抗扰动来显式攻击I2V模型的注意力机制，完全破坏运动合成，同时保持输入图像的语义保真度。

Result: 实验表明该方法提供了出色的保护效果，免疫后的图像生成静止或接近静止的视频，有效阻止恶意内容创建。

Conclusion: 注意力攻击是防止I2V生成模型被滥用的一个有效且有前景的防御方向，为鲁棒和主动的防御提供了重要思路。

Abstract: The rapid progress of image-to-video (I2V) generation models has introduced significant risks, enabling video synthesis from static images and facilitating deceptive or malicious content creation. While prior defenses such as I2VGuard attempt to immunize images, effective and principled protection to block motion remains underexplored. In this work, we introduce Vid-Freeze - a novel attention-suppressing adversarial attack that adds carefully crafted adversarial perturbations to images. Our method explicitly targets the attention mechanism of I2V models, completely disrupting motion synthesis while preserving semantic fidelity of the input image. The resulting immunized images generate stand-still or near-static videos, effectively blocking malicious content creation. Our experiments demonstrate the impressive protection provided by the proposed approach, highlighting the importance of attention attacks as a promising direction for robust and proactive defenses against misuse of I2V generation models.

</details>


### [24] [Seeing the Unseen in Low-light Spike Streams](https://arxiv.org/abs/2509.23304)
*Liwen Hu,Yang Li,Mianzhi Liu,Yijia Guo,Shenghao Xie,Ziluo Ding,Tiejun Huang,Lei Ma*

Main category: cs.CV

TL;DR: Diff-SPK是首个基于扩散模型的脉冲相机重建方法，通过ETFI技术聚合低光条件下稀疏的脉冲流信息，利用ControlNet生成高质量的高速场景图像，在低光高速场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统脉冲相机重建方法在低光高速场景下难以处理严重的噪声和稀疏信息，需要新的方法来补充纹理信息。

Method: 提出ETFI（增强纹理从脉冲间隔）技术聚合低光脉冲流信息，作为ControlNet的条件输入生成高速场景，并引入ETFI特征融合模块提升生成质量。

Result: 在真实低光脉冲流上的实验表明Diff-SPK具有优越性能，同时建立了首个低光脉冲流重建基准数据集。

Conclusion: Diff-SPK成功利用生成先验补充低光条件下的纹理信息，为脉冲相机在低光高速视觉任务中的应用提供了有效解决方案。

Abstract: Spike camera, a type of neuromorphic sensor with high-temporal resolution, shows great promise for high-speed visual tasks. Unlike traditional cameras, spike camera continuously accumulates photons and fires asynchronous spike streams. Due to unique data modality, spike streams require reconstruction methods to become perceptible to the human eye.   However, lots of methods struggle to handle spike streams in low-light high-speed scenarios due to severe noise and sparse information. In this work, we propose Diff-SPK, the first diffusion-based reconstruction method for spike camera. Diff-SPK effectively leverages generative priors to supplement texture information in low-light conditions. Specifically, it first employs an \textbf{E}nhanced \textbf{T}exture \textbf{f}rom Inter-spike \textbf{I}nterval (ETFI) to aggregate sparse information from low-light spike streams. Then, ETFI serves as a conditioning input for ControlNet to generate the high-speed scenes. To improve the quality of results, we introduce an ETFI-based feature fusion module during the generation process.   Moreover, we establish the first bona fide benchmark for the low-light spike stream reconstruction task. It significantly surpasses existing reconstruction datasets in scale and provides quantitative illumination information. The performance on real low-light spike streams demonstrates the superiority of Diff-SPK.

</details>


### [25] [Balanced Diffusion-Guided Fusion for Multimodal Remote Sensing Classification](https://arxiv.org/abs/2509.23310)
*Hao Liu,Yongjie Zheng,Yuhan Kang,Mingyang Zhang,Maoguo Gong,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 提出平衡扩散引导融合框架，利用多模态扩散特征指导多分支网络进行土地覆盖分类，解决模态不平衡问题并提升分类性能


<details>
  <summary>Details</summary>
Motivation: 多模态DDPM预训练存在模态不平衡问题，如何有效利用扩散特征指导互补多样性特征提取仍待解决

Method: 采用自适应模态掩码策略使DDPM获得平衡数据分布，通过特征融合、组通道注意力和交叉注意力机制分层指导CNN、Mamba和Transformer网络的特征提取，并开发互学习策略增强分支间协作

Result: 在四个多模态遥感数据集上的实验表明，该方法实现了优越的分类性能

Conclusion: BDGF框架通过平衡扩散特征引导和多分支协作，有效解决了多模态遥感数据分类中的模态不平衡问题

Abstract: Deep learning-based techniques for the analysis of multimodal remote sensing data have become popular due to their ability to effectively integrate complementary spatial, spectral, and structural information from different sensors. Recently, denoising diffusion probabilistic models (DDPMs) have attracted attention in the remote sensing community due to their powerful ability to capture robust and complex spatial-spectral distributions. However, pre-training multimodal DDPMs may result in modality imbalance, and effectively leveraging diffusion features to guide complementary diversity feature extraction remains an open question. To address these issues, this paper proposes a balanced diffusion-guided fusion (BDGF) framework that leverages multimodal diffusion features to guide a multi-branch network for land-cover classification. Specifically, we propose an adaptive modality masking strategy to encourage the DDPMs to obtain a modality-balanced rather than spectral image-dominated data distribution. Subsequently, these diffusion features hierarchically guide feature extraction among CNN, Mamba, and transformer networks by integrating feature fusion, group channel attention, and cross-attention mechanisms. Finally, a mutual learning strategy is developed to enhance inter-branch collaboration by aligning the probability entropy and feature similarity of individual subnetworks. Extensive experiments on four multimodal remote sensing datasets demonstrate that the proposed method achieves superior classification performance. The code is available at https://github.com/HaoLiu-XDU/BDGF.

</details>


### [26] [WorldSplat: Gaussian-Centric Feed-Forward 4D Scene Generation for Autonomous Driving](https://arxiv.org/abs/2509.23402)
*Ziyue Zhu,Zhanqian Wu,Zhenxin Zhu,Lijun Zhou,Haiyang Sun,Bing Wan,Kun Ma,Guang Chen,Hangjun Ye,Jin Xie,jian Yang*

Main category: cs.CV

TL;DR: WorldSplat是一个用于4D驾驶场景生成的创新框架，通过4D感知的潜在扩散模型和增强的视频扩散模型，生成高质量、时空一致的多视角驾驶视频。


<details>
  <summary>Details</summary>
Motivation: 现有的驾驶场景生成方法主要关注合成多样化和高保真度的驾驶视频，但缺乏3D一致性和稀疏视角覆盖，难以支持高质量的新视角合成。而3D/4D重建方法虽然改进了真实驾驶场景的新视角合成，但缺乏生成能力。

Method: 采用前馈框架，包含两个关键步骤：(i) 引入4D感知的潜在扩散模型，整合多模态信息以前馈方式生成像素对齐的4D高斯；(ii) 使用增强的视频扩散模型对这些高斯渲染的新视角视频进行细化。

Result: 在基准数据集上的广泛实验表明，WorldSplat能够有效生成高保真度、时空一致的多视角驾驶视频。

Conclusion: WorldSplat成功克服了场景生成与重建之间的困境，为自动驾驶系统提供了可扩展和可控的训练数据生成解决方案。

Abstract: Recent advances in driving-scene generation and reconstruction have demonstrated significant potential for enhancing autonomous driving systems by producing scalable and controllable training data. Existing generation methods primarily focus on synthesizing diverse and high-fidelity driving videos; however, due to limited 3D consistency and sparse viewpoint coverage, they struggle to support convenient and high-quality novel-view synthesis (NVS). Conversely, recent 3D/4D reconstruction approaches have significantly improved NVS for real-world driving scenes, yet inherently lack generative capabilities. To overcome this dilemma between scene generation and reconstruction, we propose \textbf{WorldSplat}, a novel feed-forward framework for 4D driving-scene generation. Our approach effectively generates consistent multi-track videos through two key steps: ((i)) We introduce a 4D-aware latent diffusion model integrating multi-modal information to produce pixel-aligned 4D Gaussians in a feed-forward manner. ((ii)) Subsequently, we refine the novel view videos rendered from these Gaussians using a enhanced video diffusion model. Extensive experiments conducted on benchmark datasets demonstrate that \textbf{WorldSplat} effectively generates high-fidelity, temporally and spatially consistent multi-track novel view driving videos.

</details>


### [27] [FM-SIREN & FM-FINER: Nyquist-Informed Frequency Multiplier for Implicit Neural Representation with Periodic Activation](https://arxiv.org/abs/2509.23438)
*Mohammed Alsakabi,Wael Mobeirek,John M. Dolan,Ozan K. Tonguz*

Main category: cs.CV

TL;DR: 提出FM-SIREN和FM-FINER方法，通过为周期性激活函数分配神经元特定的频率乘子，减少隐式神经表示中的特征冗余，提升信号重建性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于周期性激活的隐式神经表示网络存在隐藏特征冗余问题，同一层中的神经元由于使用固定频率乘子而捕获重叠的频率分量，限制了多层感知机的表达能力。

Method: 受离散正弦变换等经典信号处理方法启发，为周期性激活函数分配基于奈奎斯特频率的神经元特定频率乘子，引入频率多样性而不需要超参数调优或增加网络深度。

Result: 该方法将特征冗余减少近50%，在1D音频、2D图像、3D形状拟合以及神经辐射场合成等多种INR任务中持续改善信号重建性能，优于基线方法同时保持效率。

Conclusion: 这种简单而原则性的修改通过减少特征冗余，有效提升了周期性激活基隐式神经表示网络的表达能力，在各种任务中表现出优越性能。

Abstract: Existing periodic activation-based implicit neural representation (INR) networks, such as SIREN and FINER, suffer from hidden feature redundancy, where neurons within a layer capture overlapping frequency components due to the use of a fixed frequency multiplier. This redundancy limits the expressive capacity of multilayer perceptrons (MLPs). Drawing inspiration from classical signal processing methods such as the Discrete Sine Transform (DST), we propose FM-SIREN and FM-FINER, which assign Nyquist-informed, neuron-specific frequency multipliers to periodic activations. Unlike existing approaches, our design introduces frequency diversity without requiring hyperparameter tuning or additional network depth. This simple yet principled modification reduces the redundancy of features by nearly 50% and consistently improves signal reconstruction across diverse INR tasks, including fitting 1D audio, 2D image and 3D shape, and synthesis of neural radiance fields (NeRF), outperforming their baseline counterparts while maintaining efficiency.

</details>


### [28] [No Concept Left Behind: Test-Time Optimization for Compositional Text-to-Image Generation](https://arxiv.org/abs/2509.23457)
*Mohammad Hossein Sameti,Amir M. Mansourian,Arash Marioriyad,Soheil Fadaee Oshyani,Mohammad Hossein Rohban,Mahdieh Soleymani Baghshah*

Main category: cs.CV

TL;DR: 提出了一种细粒度的测试时优化框架，通过将提示分解为语义概念并在全局和概念级别评估对齐，使用细粒度CLIP计算概念级对应关系，通过迭代提示优化循环提高文本到图像生成的组合忠实度。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像模型在处理复杂提示时经常无法忠实渲染所有元素，会遗漏或错误表示特定对象和属性，需要改进组合忠实度。

Method: 将输入提示分解为语义概念，在全局和概念级别评估对齐，使用细粒度CLIP计算概念级对应关系，通过迭代提示优化循环让大语言模型提出改进的提示。

Result: 在DrawBench和CompBench提示上的实验表明，该方法显著提高了概念覆盖率和人类评判的忠实度，优于标准测试时优化和基础文本到图像模型。

Conclusion: 提出的细粒度测试时优化框架有效提高了文本到图像生成的组合忠实度，通过概念级评估和迭代提示优化实现了更好的概念覆盖和生成质量。

Abstract: Despite recent advances in text-to-image (T2I) models, they often fail to faithfully render all elements of complex prompts, frequently omitting or misrepresenting specific objects and attributes. Test-time optimization has emerged as a promising approach to address this limitation by refining generation without the need for retraining. In this paper, we propose a fine-grained test-time optimization framework that enhances compositional faithfulness in T2I generation. Unlike most of prior approaches that rely solely on a global image/text similarity score, our method decomposes the input prompt into semantic concepts and evaluates alignment at both the global and concept levels. A fine-grained variant of CLIP is used to compute concept-level correspondence, producing detailed feedback on missing or inaccurate concepts. This feedback is fed into an iterative prompt refinement loop, enabling the large language model to propose improved prompts. Experiments on DrawBench and CompBench prompts demonstrate that our method significantly improves concept coverage and human-judged faithfulness over both standard test-time optimization and the base T2I model. Code is available at: https://github.com/AmirMansurian/NoConceptLeftBehind

</details>


### [29] [RestoRect: Degraded Image Restoration via Latent Rectified Flow & Feature Distillation](https://arxiv.org/abs/2509.23480)
*Shourya Verma,Mengbo Wang,Nadia Atallah Lanman,Ananth Grama*

Main category: cs.CV

TL;DR: 提出RestoRect方法，通过潜在整流流特征蒸馏解决图像恢复中速度与性能的权衡问题，结合Retinex理论、可学习各向异性扩散和三角色彩空间极化，实现快速高质量的图像恢复。


<details>
  <summary>Details</summary>
Motivation: 当前图像恢复方法面临关键权衡：高性能模型速度太慢，快速模型效果差。现有静态特征匹配方法无法捕捉现代transformer架构的动态特征生成过程。

Method: 应用整流流将特征蒸馏重新表述为生成过程，学生通过潜在空间中的可学习轨迹学习合成教师质量特征。结合Retinex理论进行物理分解、可学习各向异性扩散约束和三角色彩空间极化，引入特征层提取损失实现不同网络架构间的鲁棒知识迁移。

Result: 在15个图像恢复数据集、4个任务、8个指标上展示了优越结果，实现了更好的训练稳定性、更快的收敛和推理速度，同时保持恢复质量。

Conclusion: RestoRect方法成功解决了图像恢复中速度与性能的权衡问题，通过潜在整流流特征蒸馏实现了快速高质量的图像恢复。

Abstract: Current approaches for restoration of degraded images face a critical trade-off: high-performance models are too slow for practical use, while fast models produce poor results. Knowledge distillation transfers teacher knowledge to students, but existing static feature matching methods cannot capture how modern transformer architectures dynamically generate features. We propose 'RestoRect', a novel Latent Rectified Flow Feature Distillation method for restoring degraded images. We apply rectified flow to reformulate feature distillation as a generative process where students learn to synthesize teacher-quality features through learnable trajectories in latent space. Our framework combines Retinex theory for physics-based decomposition with learnable anisotropic diffusion constraints, and trigonometric color space polarization. We introduce a Feature Layer Extraction loss for robust knowledge transfer between different network architectures through cross-normalized transformer feature alignment with percentile-based outlier detection. RestoRect achieves better training stability, and faster convergence and inference while preserving restoration quality. We demonstrate superior results across 15 image restoration datasets, covering 4 tasks, on 8 metrics.

</details>


### [30] [Orientation-anchored Hyper-Gaussian for 4D Reconstruction from Casual Videos](https://arxiv.org/abs/2509.23492)
*Junyi Wu,Jiachen Tao,Haoxuan Wang,Gaowen Liu,Ramana Rao Kompella,Yan Yan*

Main category: cs.CV

TL;DR: OriGS是一种基于场景方向的4D重建框架，通过全局方向场和方向感知超高斯表示，在单目视频中实现高质量动态场景重建。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯泼溅的动态场景重建方法依赖低秩假设，难以建模无约束动态中复杂的区域特定变形。

Method: 首先估计全局方向场作为结构指导，然后提出方向感知超高斯表示，将时间、空间、几何和方向嵌入统一概率状态，通过条件切片推断区域特定变形。

Result: 实验表明，在具有挑战性的真实世界动态场景中，OriGS相比主流方法具有更优的重建保真度。

Conclusion: OriGS通过基于方向的表示成功解决了复杂动态场景重建中的区域特定变形建模问题。

Abstract: We present Orientation-anchored Gaussian Splatting (OriGS), a novel framework for high-quality 4D reconstruction from casually captured monocular videos. While recent advances extend 3D Gaussian Splatting to dynamic scenes via various motion anchors, such as graph nodes or spline control points, they often rely on low-rank assumptions and fall short in modeling complex, region-specific deformations inherent to unconstrained dynamics. OriGS addresses this by introducing a hyperdimensional representation grounded in scene orientation. We first estimate a Global Orientation Field that propagates principal forward directions across space and time, serving as stable structural guidance for dynamic modeling. Built upon this, we propose Orientation-aware Hyper-Gaussian, a unified formulation that embeds time, space, geometry, and orientation into a coherent probabilistic state. This enables inferring region-specific deformation through principled conditioned slicing, adaptively capturing diverse local dynamics in alignment with global motion intent. Experiments demonstrate the superior reconstruction fidelity of OriGS over mainstream methods in challenging real-world dynamic scenes.

</details>


### [31] [Calibrated and Resource-Aware Super-Resolution for Reliable Driver Behavior Analysis](https://arxiv.org/abs/2509.23535)
*Ibne Farabi Shihab,Weiheng Chai,Jiyang Wang,Sanjeda Akter,Senem Velipasalar Gursoy,Anuj Sharma*

Main category: cs.CV

TL;DR: 提出资源感知自适应超分辨率框架，优化模型校准和关键事件的高精度召回，在安全关键指标上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 驾驶员监控系统不仅需要高精度，还需要可靠的置信度分数以确保安全关键部署。直接低分辨率训练虽然整体精度高，但预测校准差，在安全关键场景中可能危险。

Method: 资源感知自适应超分辨率框架，通过轻量级伪影检测器（0.3M参数，5.2ms开销）过滤超分辨率引起的幻觉，优化模型校准和关键事件的精确度-召回率。

Result: 在安全中心指标上实现最先进性能：最佳校准（ECE 5.8% vs 基线6.2%）、最高AUPR用于困倦检测（0.78 vs 0.74）、手机使用检测的优越精确度-召回率（0.74 vs 0.71）。

Conclusion: 虽然低分辨率训练的视频模型作为强通用基线，但自适应框架代表了安全关键应用中可靠性至关重要的最先进解决方案。

Abstract: Driver monitoring systems require not just high accuracy but reliable, well-calibrated confidence scores for safety-critical deployment. While direct low-resolution training yields high overall accuracy, it produces poorly calibrated predictions that can be dangerous in safety-critical scenarios. We propose a resource-aware adaptive super-resolution framework that optimizes for model calibration and high precision-recall on critical events. Our approach achieves state-of-the-art performance on safety-centric metrics: best calibration (ECE of 5.8\% vs 6.2\% for LR-trained baselines), highest AUPR for drowsiness detection (0.78 vs 0.74), and superior precision-recall for phone use detection (0.74 vs 0.71). A lightweight artifact detector (0.3M parameters, 5.2ms overhead) provides additional safety by filtering SR-induced hallucinations. While LR-trained video models serve as strong general-purpose baselines, our adaptive framework represents the state-of-the-art solution for safety-critical applications where reliability is paramount.

</details>


### [32] [From Fields to Splats: A Cross-Domain Survey of Real-Time Neural Scene Representations](https://arxiv.org/abs/2509.23555)
*Javed Ahmad,Penggang Gao,Donatien Delehelle,Mennuti Canio,Nikhil Deshpande,Jesús Ortiz,Darwin G. Caldwell,Yonas Teodros Tefera*

Main category: cs.CV

TL;DR: 本文综述了3D高斯泼溅(3DGS)如何取代NeRF成为神经场景表示的主流技术，分析了其在SLAM、远程呈现、机器人操作和3D内容生成等领域的应用优势。


<details>
  <summary>Details</summary>
Motivation: 随着神经场景表示技术的发展，3DGS因其显式表示、高效优化和高质量渲染能力，正在各个领域迅速取代基于NeRF的方法。本文旨在系统分析3DGS的技术优势、适应性和局限性。

Method: 通过系统比较不同领域的具体技术流程，围绕统一的研究问题组织综述：3DGS的技术优势、对不同输入模态的适应性以及现有局限性。

Result: 研究表明3DGS在真实感渲染、几何保真度和计算效率之间实现了良好平衡，能够满足多种应用场景的需求。

Conclusion: 3DGS为神经渲染技术提供了新的发展路线图，不仅可用于图像合成，还能在真实和虚拟环境中支持感知、交互和内容创建等多种任务。

Abstract: Neural scene representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have transformed how 3D environments are modeled, rendered, and interpreted. NeRF introduced view-consistent photorealism via volumetric rendering; 3DGS has rapidly emerged as an explicit, efficient alternative that supports high-quality rendering, faster optimization, and integration into hybrid pipelines for enhanced photorealism and task-driven scene understanding. This survey examines how 3DGS is being adopted across SLAM, telepresence and teleoperation, robotic manipulation, and 3D content generation. Despite their differences, these domains share common goals: photorealistic rendering, meaningful 3D structure, and accurate downstream tasks. We organize the review around unified research questions that explain why 3DGS is increasingly displacing NeRF-based approaches: What technical advantages drive its adoption? How does it adapt to different input modalities and domain-specific constraints? What limitations remain? By systematically comparing domain-specific pipelines, we show that 3DGS balances photorealism, geometric fidelity, and computational efficiency. The survey offers a roadmap for leveraging neural rendering not only for image synthesis but also for perception, interaction, and content creation across real and virtual environments.

</details>


### [33] [Towards Interpretable Visual Decoding with Attention to Brain Representations](https://arxiv.org/abs/2509.23566)
*Pinyuan Feng,Hossein Adeli,Wenxuan Guo,Fan Cheng,Ethan Hwang,Nikolaus Kriegeskorte*

Main category: cs.CV

TL;DR: 提出NeuroAdapter框架，直接基于大脑表征条件化潜在扩散模型，无需中间特征空间，在保持重建质量的同时提供更好的生成过程透明度。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过将大脑信号映射到中间图像或文本特征空间来指导生成过程，这掩盖了不同大脑区域对最终重建输出的贡献效果。

Method: 开发NeuroAdapter框架，直接条件化潜在扩散模型于大脑表征；提出IBBI双向可解释性框架，通过分析扩散去噪步骤中的交叉注意力机制来揭示不同皮层区域如何影响生成轨迹。

Result: 在公共fMRI数据集上展现出与先前工作相当的视觉重建质量，同时提供了大脑信号如何塑造生成过程的更大透明度。

Conclusion: 端到端脑到图像解码具有潜力，并建立了通过视觉神经科学视角解释扩散模型的路径。

Abstract: Recent work has demonstrated that complex visual stimuli can be decoded from human brain activity using deep generative models, helping brain science researchers interpret how the brain represents real-world scenes. However, most current approaches leverage mapping brain signals into intermediate image or text feature spaces before guiding the generative process, masking the effect of contributions from different brain areas on the final reconstruction output. In this work, we propose NeuroAdapter, a visual decoding framework that directly conditions a latent diffusion model on brain representations, bypassing the need for intermediate feature spaces. Our method demonstrates competitive visual reconstruction quality on public fMRI datasets compared to prior work, while providing greater transparency into how brain signals shape the generation process. To this end, we contribute an Image-Brain BI-directional interpretability framework (IBBI) which investigates cross-attention mechanisms across diffusion denoising steps to reveal how different cortical areas influence the unfolding generative trajectory. Our results highlight the potential of end-to-end brain-to-image decoding and establish a path toward interpreting diffusion models through the lens of visual neuroscience.

</details>


### [34] [RobuQ: Pushing DiTs to W1.58A2 via Robust Activation Quantization](https://arxiv.org/abs/2509.23582)
*Kaicheng Yang,Xun Zhang,Haotong Qin,Yucheng Lin,Kaisen Yang,Xianglong Yan,Yulun Zhang*

Main category: cs.CV

TL;DR: 提出了RobuQ框架，通过鲁棒激活量化和激活混合精度网络，首次在ImageNet-1K等大型数据集上实现了平均2位激活量化的稳定图像生成。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器(DiTs)在图像生成中表现出色，但计算和内存成本高，阻碍了实际部署。激活量化是DiTs极低位量化的主要瓶颈。

Method: 建立强三元权重基线，提出RobustQuantizer进行鲁棒激活量化，利用Hadamard变换将未知分布转换为正态分布，并提出首个仅激活混合精度网络(AMPN)管道。

Result: 在无条件/条件图像生成实验中，RobuQ在亚4位量化配置下实现了DiT量化的最先进性能，首次在ImageNet-1K等大型数据集上实现平均2位激活量化的稳定竞争性图像生成。

Conclusion: RobuQ框架成功解决了DiTs的激活量化挑战，为极低位DiT量化提供了有效解决方案，显著降低了计算和内存成本。

Abstract: Diffusion Transformers (DiTs) have recently emerged as a powerful backbone for image generation, demonstrating superior scalability and performance over U-Net architectures. However, their practical deployment is hindered by substantial computational and memory costs. While Quantization-Aware Training (QAT) has shown promise for U-Nets, its application to DiTs faces unique challenges, primarily due to the sensitivity and distributional complexity of activations. In this work, we identify activation quantization as the primary bottleneck for pushing DiTs to extremely low-bit settings. To address this, we propose a systematic QAT framework for DiTs, named RobuQ. We start by establishing a strong ternary weight (W1.58A4) DiT baseline. Building upon this, we propose RobustQuantizer to achieve robust activation quantization. Our theoretical analyses show that the Hadamard transform can convert unknown per-token distributions into per-token normal distributions, providing a strong foundation for this method. Furthermore, we propose AMPN, the first Activation-only Mixed-Precision Network pipeline for DiTs. This method applies ternary weights across the entire network while allocating different activation precisions to each layer to eliminate information bottlenecks. Through extensive experiments on unconditional and conditional image generation, our RobuQ framework achieves state-of-the-art performance for DiT quantization in sub-4-bit quantization configuration. To the best of our knowledge, RobuQ is the first achieving stable and competitive image generation on large datasets like ImageNet-1K with activations quantized to average 2 bits. The code and models will be available at https://github.com/racoonykc/RobuQ .

</details>


### [35] [VividFace: High-Quality and Efficient One-Step Diffusion For Video Face Enhancement](https://arxiv.org/abs/2509.23584)
*Shulian Zhang,Yong Guo,Long Peng,Ziyang Wang,Ye Chen,Wenbo Li,Xiao Zhang,Yulun Zhang,Jian Chen*

Main category: cs.CV

TL;DR: VividFace是一个高效的一步扩散框架，用于视频人脸增强，通过单步流匹配范式直接从退化输入映射到高质量输出，显著减少推理时间。


<details>
  <summary>Details</summary>
Motivation: 解决当前视频人脸增强方法面临的三个关键挑战：复杂面部纹理建模与时间一致性问题、高质量训练数据缺乏导致的模型泛化限制、以及推理过程中重复去噪步骤导致的低效率问题。

Method: 基于预训练的WANX视频生成模型，采用单步流匹配范式；提出联合潜在-像素人脸聚焦训练策略，通过随机切换面部区域优化和全局重建，在潜在空间和像素空间提供显式监督；引入MLLM驱动的数据筛选流程自动选择高质量视频人脸数据集。

Result: 在感知质量、身份保持和时间稳定性方面达到最先进水平，同时显著提高了推理效率。

Conclusion: VividFace在视频人脸增强任务中实现了优异的性能表现，为研究社区提供了实用的资源，有效解决了现有方法的效率和泛化问题。

Abstract: Video Face Enhancement (VFE) seeks to reconstruct high-quality facial regions from degraded video sequences, a capability that underpins numerous applications including video conferencing, film restoration, and surveillance. Despite substantial progress in the field, current methods that primarily rely on video super-resolution and generative frameworks continue to face three fundamental challenges: (1) faithfully modeling intricate facial textures while preserving temporal consistency; (2) restricted model generalization due to the lack of high-quality face video training data; and (3) low efficiency caused by repeated denoising steps during inference. To address these challenges, we propose VividFace, a novel and efficient one-step diffusion framework for video face enhancement. Built upon the pretrained WANX video generation model, our method leverages powerful spatiotemporal priors through a single-step flow matching paradigm, enabling direct mapping from degraded inputs to high-quality outputs with significantly reduced inference time. To further boost efficiency, we propose a Joint Latent-Pixel Face-Focused Training strategy that employs stochastic switching between facial region optimization and global reconstruction, providing explicit supervision in both latent and pixel spaces through a progressive two-stage training process. Additionally, we introduce an MLLM-driven data curation pipeline for automated selection of high-quality video face datasets, enhancing model generalization. Extensive experiments demonstrate that VividFace achieves state-of-the-art results in perceptual quality, identity preservation, and temporal stability, while offering practical resources for the research community.

</details>


### [36] [VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration](https://arxiv.org/abs/2509.23601)
*Han Hu,Zhuoran Zheng,Liang Li,Chen Lyu*

Main category: cs.CV

TL;DR: VAMamba是一个视觉自适应Mamba框架，通过QCLAM和GPS-SS2D两个创新组件解决了传统Mamba方法中固定扫描模式和低效特征利用的问题，在图像恢复任务中实现了更好的性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于Mamba的图像恢复方法受限于固定的扫描模式和低效的特征利用，无法适应多样的图像退化情况，限制了恢复性能和计算效率。

Method: 提出VAMamba框架：1) QCLAM使用FIFO缓存存储历史表示，通过相似性指导智能特征融合；2) GPS-SS2D使用Vision Transformer生成分数图估计像素重要性，采用贪心策略确定最优扫描路径。

Result: 在多种恢复任务上的广泛实验表明，VAMamba在恢复质量和效率方面均优于现有方法，建立了自适应图像恢复的新基准。

Conclusion: VAMamba通过自适应扫描和智能特征融合，能够有针对性地关注退化区域，同时保持高计算效率，为图像恢复任务提供了有效的解决方案。

Abstract: Recent Mamba-based image restoration methods have achieved promising results but remain   limited by fixed scanning patterns and inefficient feature utilization. Conventional Mamba   architectures rely on predetermined paths that cannot adapt to diverse degradations, constraining   both restoration performance and computational efficiency. To overcome these limitations, we   propose VAMamba, a Visual Adaptive Mamba framework with two key innovations. First,   QCLAM(Queue-basedCacheLow-rankAdaptiveMemory)enhancesfeaturelearningthrougha   FIFO cache that stores historical representations. Similarity between current LoRA-adapted and   cached features guides intelligent fusion, enabling dynamic reuse while effectively controlling   memorygrowth.Second, GPS-SS2D(GreedyPathScanSS2D)introducesadaptive scanning. A   Vision Transformer generates score maps to estimate pixel importance, and a greedy strategy de termines optimal forward and backward scanning paths. These learned trajectories replace rigid   patterns, enabling SS2D to perform targeted feature extraction. The integration of QCLAM and   GPS-SS2D allows VAMamba to adaptively focus on degraded regions while maintaining high   computational efficiency. Extensive experiments across diverse restoration tasks demonstrate   that VAMamba consistently outperforms existing approaches in both restoration quality and   efficiency, establishing new benchmarks for adaptive image restoration. Our code is available   at https://github.com/WaterHQH/VAMamba.

</details>


### [37] [MAN: Latent Diffusion Enhanced Multistage Anti-Noise Network for Efficient and High-Quality Low-Dose CT Image Denoising](https://arxiv.org/abs/2509.23603)
*Tangtangfang Fang,Jingxi Hu,Xiangjian He,Jiaqi Yang*

Main category: cs.CV

TL;DR: 提出MAN模型，一种潜在扩散增强的多阶段抗噪声网络，用于高效高质量的低剂量CT图像去噪，相比传统扩散模型推理速度提升60倍以上。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在低剂量CT去噪中表现出色，但计算成本极高，推理时间过长，阻碍了临床采用。需要开发高效且高质量的解决方案。

Method: 在压缩潜在空间中操作，使用感知优化的自动编码器，通过基于注意力的条件U-Net进行快速确定性条件去噪扩散过程，大幅降低计算开销。

Result: 在LDCT和投影数据集上，模型实现了优越的感知质量，超越CNN/GAN方法，同时与计算量大的扩散模型如DDPM和Dn-Dp相媲美，推理速度比像素空间扩散去噪器快60倍以上。

Conclusion: 通过弥合高保真度和临床可行性之间的差距，为医学成像中的先进生成模型展示了实用路径。

Abstract: While diffusion models have set a new benchmark for quality in Low-Dose Computed Tomography (LDCT) denoising, their clinical adoption is critically hindered by extreme computational costs, with inference times often exceeding thousands of seconds per scan. To overcome this barrier, we introduce MAN, a Latent Diffusion Enhanced Multistage Anti-Noise Network for Efficient and High-Quality Low-Dose CT Image Denoising task. Our method operates in a compressed latent space via a perceptually-optimized autoencoder, enabling an attention-based conditional U-Net to perform the fast, deterministic conditional denoising diffusion process with drastically reduced overhead. On the LDCT and Projection dataset, our model achieves superior perceptual quality, surpassing CNN/GAN-based methods while rivaling the reconstruction fidelity of computationally heavy diffusion models like DDPM and Dn-Dp. Most critically, in the inference stage, our model is over 60x faster than representative pixel space diffusion denoisers, while remaining competitive on PSNR/SSIM scores. By bridging the gap between high fidelity and clinical viability, our work demonstrates a practical path forward for advanced generative models in medical imaging.

</details>


### [38] [VMDiff: Visual Mixing Diffusion for Limitless Cross-Object Synthesis](https://arxiv.org/abs/2509.23605)
*Zeren Xiong,Yue Yu,Zedong Zhang,Shuo Chen,Jian Yang,Jun Li*

Main category: cs.CV

TL;DR: 提出了Visual Mixing Diffusion (VMDiff)框架，通过噪声和潜在层面的融合解决多源图像融合中的共存生成和偏差生成问题。


<details>
  <summary>Details</summary>
Motivation: 解决图像到图像生成中多源视觉线索融合时存在的两个关键挑战：共存生成（多个对象简单并列而无真正融合）和偏差生成（一个对象因语义不平衡主导输出）。

Method: VMDiff框架包含：(1) 混合采样过程，结合引导去噪、反演和球面插值实现结构感知融合；(2) 高效自适应调整模块，引入基于相似性的评分自动搜索最优参数。

Result: 在780个概念对的基准测试中，该方法在视觉质量、语义一致性和人类评价的创造性方面优于强基线方法。

Conclusion: VMDiff是一个简单有效的扩散框架，能够成功合成单一连贯对象，解决多源图像融合中的关键挑战。

Abstract: Creating novel images by fusing visual cues from multiple sources is a fundamental yet underexplored problem in image-to-image generation, with broad applications in artistic creation, virtual reality and visual media. Existing methods often face two key challenges: coexistent generation, where multiple objects are simply juxtaposed without true integration, and bias generation, where one object dominates the output due to semantic imbalance. To address these issues, we propose Visual Mixing Diffusion (VMDiff), a simple yet effective diffusion-based framework that synthesizes a single, coherent object by integrating two input images at both noise and latent levels. Our approach comprises: (1) a hybrid sampling process that combines guided denoising, inversion, and spherical interpolation with adjustable parameters to achieve structure-aware fusion, mitigating coexistent generation; and (2) an efficient adaptive adjustment module, which introduces a novel similarity-based score to automatically and adaptively search for optimal parameters, countering semantic bias. Experiments on a curated benchmark of 780 concept pairs demonstrate that our method outperforms strong baselines in visual quality, semantic consistency, and human-rated creativity.

</details>


### [39] [FlowLUT: Efficient Image Enhancement via Differentiable LUTs and Iterative Flow Matching](https://arxiv.org/abs/2509.23608)
*Liubing Hu,Chen Wu,Anrui Wang,Dianjie Lu,Guijuan Zhang,Zhuoran Zheng*

Main category: cs.CV

TL;DR: FlowLUT是一个端到端的图像增强模型，结合了3D LUT的高效性、多先验知识和流匹配重建的参数无关特性，实现了实时处理与高表现力的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习图像增强方法在计算效率与表示能力之间的权衡问题，传统3D LUT虽然实时但缺乏表示灵活性且依赖固定先验。

Method: 使用可微分3D LUT集合进行色彩空间变换，轻量级内容感知网络预测融合权重，创新迭代流匹配方法恢复局部结构细节，整体模型在复合损失函数下联合优化。

Result: 在三个基准测试上的广泛实验结果表明该方法具有有效性。

Conclusion: FlowLUT成功解决了图像增强中效率与表现力的权衡问题，通过集成多种技术实现了场景自适应的实时高质量增强。

Abstract: Deep learning-based image enhancement methods face a fundamental trade-off between computational efficiency and representational capacity. For example, although a conventional three-dimensional Look-Up Table (3D LUT) can process a degraded image in real time, it lacks representational flexibility and depends solely on a fixed prior. To address this problem, we introduce FlowLUT, a novel end-to-end model that integrates the efficiency of LUTs, multiple priors, and the parameter-independent characteristic of flow-matched reconstructed images. Specifically, firstly, the input image is transformed in color space by a collection of differentiable 3D LUTs (containing a large number of 3D LUTs with different priors). Subsequently, a lightweight content-aware dynamically predicts fusion weights, enabling scene-adaptive color correction with $\mathcal{O}(1)$ complexity. Next, a lightweight fusion prediction network runs on multiple 3D LUTs, with $\mathcal{O}(1)$ complexity for scene-adaptive color correction.Furthermore, to address the inherent representation limitations of LUTs, we design an innovative iterative flow matching method to restore local structural details and eliminate artifacts. Finally, the entire model is jointly optimized under a composite loss function enforcing perceptual and structural fidelity. Extensive experimental results demonstrate the effectiveness of our method on three benchmarks.

</details>


### [40] [DiffInk: Glyph- and Style-Aware Latent Diffusion Transformer for Text to Online Handwriting Generation](https://arxiv.org/abs/2509.23624)
*Wei Pan,Huiguo He,Hiuyi Cheng,Yilin Shi,Lianwen Jin*

Main category: cs.CV

TL;DR: DiffInk是首个用于全行手写生成的潜在扩散Transformer框架，通过InkVAE和InkDiT组件实现高精度的字形和风格保真度。


<details>
  <summary>Details</summary>
Motivation: 现有文本到手写生成方法主要关注字符或单词级别，导致全文本行生成效率低下且缺乏整体结构建模。

Method: 提出InkVAE序列变分自编码器，使用OCR损失和风格分类损失进行潜在空间正则化；然后提出InkDiT潜在扩散Transformer，整合目标文本和参考风格生成连贯笔迹轨迹。

Result: 实验结果显示DiffInk在字形准确性和风格保真度上优于现有最先进方法，并显著提高了生成效率。

Conclusion: DiffInk通过双正则化潜在空间和扩散Transformer框架，实现了高效且高质量的全行手写生成。

Abstract: Deep generative models have advanced text-to-online handwriting generation (TOHG), which aims to synthesize realistic pen trajectories conditioned on textual input and style references. However, most existing methods still primarily focus on character- or word-level generation, resulting in inefficiency and a lack of holistic structural modeling when applied to full text lines. To address these issues, we propose DiffInk, the first latent diffusion Transformer framework for full-line handwriting generation. We first introduce InkVAE, a novel sequential variational autoencoder enhanced with two complementary latent-space regularization losses: (1) an OCR-based loss enforcing glyph-level accuracy, and (2) a style-classification loss preserving writing style. This dual regularization yields a semantically structured latent space where character content and writer styles are effectively disentangled. We then introduce InkDiT, a novel latent diffusion Transformer that integrates target text and reference styles to generate coherent pen trajectories. Experimental results demonstrate that DiffInk outperforms existing state-of-the-art methods in both glyph accuracy and style fidelity, while significantly improving generation efficiency. Code will be made publicly available.

</details>


### [41] [LightFair: Towards an Efficient Alternative for Fair T2I Diffusion via Debiasing Pre-trained Text Encoders](https://arxiv.org/abs/2509.23639)
*Boyu Han,Qianqian Xu,Shilong Bao,Zhiyong Yang,Kangli Zi,Qingming Huang*

Main category: cs.CV

TL;DR: LightFair是一种轻量级方法，通过微调文本编码器的嵌入来提升文本到图像扩散模型的公平性，采用距离约束去偏策略和两阶段采样策略，在保持生成质量的同时显著降低训练负担。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么需要全参数训练，要么依赖辅助网络进行校正，导致训练或采样负担重且性能不佳。文本编码器作为最可微调和前端模块，其输出的嵌入在不同属性图像上存在显著偏差，且噪声预测网络会进一步放大这种不平衡。

Method: 提出协作距离约束去偏策略，通过平衡嵌入距离来改善公平性，无需辅助参考；引入两阶段文本引导采样策略，限制去偏文本编码器的干预时机以保持生成质量。

Result: 在Stable Diffusion v1.5上，该方法仅需1/4的训练负担即可实现最先进的去偏效果，采样负担几乎没有增加。

Conclusion: LightFair是一种有效且高效的公平性提升方法，通过针对性处理文本编码器偏差，在低负担下实现了优异的去偏性能。

Abstract: This paper explores a novel lightweight approach LightFair to achieve fair text-to-image diffusion models (T2I DMs) by addressing the adverse effects of the text encoder. Most existing methods either couple different parts of the diffusion model for full-parameter training or rely on auxiliary networks for correction. They incur heavy training or sampling burden and unsatisfactory performance. Since T2I DMs consist of multiple components, with the text encoder being the most fine-tunable and front-end module, this paper focuses on mitigating bias by fine-tuning text embeddings. To validate feasibility, we observe that the text encoder's neutral embedding output shows substantial skewness across image embeddings of various attributes in the CLIP space. More importantly, the noise prediction network further amplifies this imbalance. To finetune the text embedding, we propose a collaborative distance-constrained debiasing strategy that balances embedding distances to improve fairness without auxiliary references. However, mitigating bias can compromise the original generation quality. To address this, we introduce a two-stage text-guided sampling strategy to limit when the debiased text encoder intervenes. Extensive experiments demonstrate that LightFair is effective and efficient. Notably, on Stable Diffusion v1.5, our method achieves SOTA debiasing at just $1/4$ of the training burden, with virtually no increase in sampling burden. The code is available at https://github.com/boyuh/LightFair.

</details>


### [42] [Sparse-Up: Learnable Sparse Upsampling for 3D Generation with High-Fidelity Textures](https://arxiv.org/abs/2509.23646)
*Lu Xiao,Jiale Zhang,Yang Liu,Taicheng Huang,Xin Tian*

Main category: cs.CV

TL;DR: 提出了Sparse-Up框架，通过稀疏体素引导纹理重建，使用表面锚定和视域分区技术突破分辨率限制，在保持多视角一致性的同时有效保留高频纹理细节。


<details>
  <summary>Details</summary>
Motivation: 现有方法在创建高保真3D资产时面临'像素级痛点'：高频细节丢失。现有方法要么牺牲跨视角一致性导致纹理撕裂，要么受限于显式体素的分辨率上限而丢失精细纹理细节。

Method: 1. 使用稀疏体素指导纹理重建确保多视角一致性；2. 表面锚定采用可学习上采样策略将体素约束在网格表面，消除70%以上冗余体素；3. 视域分区引入图像块引导的体素分区方案，仅在可见局部块上进行梯度监督和反向传播。

Result: 显著减少高分辨率体素训练时的内存消耗，同时保持几何一致性并保留纹理中的高频细节。

Conclusion: Sparse-Up是一个内存高效的高保真纹理建模框架，通过创新的稀疏体素方法有效解决了3D资产创建中的高频细节保留问题。

Abstract: The creation of high-fidelity 3D assets is often hindered by a 'pixel-level pain point': the loss of high-frequency details. Existing methods often trade off one aspect for another: either sacrificing cross-view consistency, resulting in torn or drifting textures, or remaining trapped by the resolution ceiling of explicit voxels, forfeiting fine texture detail. In this work, we propose Sparse-Up, a memory-efficient, high-fidelity texture modeling framework that effectively preserves high-frequency details. We use sparse voxels to guide texture reconstruction and ensure multi-view consistency, while leveraging surface anchoring and view-domain partitioning to break through resolution constraints. Surface anchoring employs a learnable upsampling strategy to constrain voxels to the mesh surface, eliminating over 70% of redundant voxels present in traditional voxel upsampling. View-domain partitioning introduces an image patch-guided voxel partitioning scheme, supervising and back-propagating gradients only on visible local patches. Through these two strategies, we can significantly reduce memory consumption during high-resolution voxel training without sacrificing geometric consistency, while preserving high-frequency details in textures.

</details>


### [43] [QuantSparse: Comprehensively Compressing Video Diffusion Transformer with Model Quantization and Attention Sparsification](https://arxiv.org/abs/2509.23681)
*Weilun Feng,Chuanguang Yang,Haotong Qin,Mingqiang Wu,Yuqi Li,Xiangqi Li,Zhulin An,Libo Huang,Yulun Zhang,Michele Magno,Yongjun Xu*

Main category: cs.CV

TL;DR: 提出了QuantSparse框架，将模型量化与注意力稀疏化相结合，通过多尺度显著注意力蒸馏和二阶稀疏注意力重参数化，在显著降低存储和计算成本的同时保持视频生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器在视频生成方面表现出色，但计算和内存成本过高阻碍实际部署。单独使用量化或稀疏化在激进压缩下会导致严重性能下降，需要一种统一的压缩方法。

Method: 1. 多尺度显著注意力蒸馏：利用全局结构指导和局部显著监督缓解量化偏差
2. 二阶稀疏注意力重参数化：利用二阶残差的时间稳定性高效恢复稀疏化丢失的信息

Result: 在HunyuanVideo-13B上，QuantSparse达到20.88 PSNR，显著优于最先进的量化基线Q-VDiT（16.85 PSNR），同时存储减少3.68倍，端到端推理加速1.88倍。

Conclusion: QuantSparse成功将量化与稀疏化相结合，在保持高质量视频生成的同时大幅提升效率，为扩散变换器的实际部署提供了有效解决方案。

Abstract: Diffusion transformers exhibit remarkable video generation capability, yet their prohibitive computational and memory costs hinder practical deployment. Model quantization and attention sparsification are two promising directions for compression, but each alone suffers severe performance degradation under aggressive compression. Combining them promises compounded efficiency gains, but naive integration is ineffective. The sparsity-induced information loss exacerbates quantization noise, leading to amplified attention shifts. To address this, we propose \textbf{QuantSparse}, a unified framework that integrates model quantization with attention sparsification. Specifically, we introduce \textit{Multi-Scale Salient Attention Distillation}, which leverages both global structural guidance and local salient supervision to mitigate quantization-induced bias. In addition, we develop \textit{Second-Order Sparse Attention Reparameterization}, which exploits the temporal stability of second-order residuals to efficiently recover information lost under sparsity. Experiments on HunyuanVideo-13B demonstrate that QuantSparse achieves 20.88 PSNR, substantially outperforming the state-of-the-art quantization baseline Q-VDiT (16.85 PSNR), while simultaneously delivering a \textbf{3.68$\times$} reduction in storage and \textbf{1.88$\times$} acceleration in end-to-end inference. Our code will be released in https://github.com/wlfeng0509/QuantSparse.

</details>


### [44] [CrimEdit: Controllable Editing for Counterfactual Object Removal, Insertion, and Movement](https://arxiv.org/abs/2509.23708)
*Boseong Jeon,Junghyuk Lee,Jimin Park,Kwanyoung Kim,Jingi Jung,Sangwon Lee,Hyunbo Shim*

Main category: cs.CV

TL;DR: CrimEdit是一个统一的扩散模型，通过联合训练移除和插入任务嵌入，利用无分类器引导技术，在单一模型中实现对象移除、可控效果插入和高效对象移动。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理对象效果（如阴影和反射）时性能有限，且缺乏对无分类器引导在统一模型中处理对象移除和插入任务影响的研究。

Method: 在单一模型中联合训练移除和插入任务嵌入，并将其应用于无分类器引导方案，同时扩展任务提示以支持空间不同区域的对象移动。

Result: 实验表明CrimEdit在对象移除、可控效果插入和对象移动方面表现优异，无需额外训练或分离的移除和插入阶段。

Conclusion: CrimEdit通过统一模型和引导技术，实现了高效的对象编辑，包括移除、插入和移动，展示了在复合编辑任务中的优越性能。

Abstract: Recent works on object removal and insertion have enhanced their performance by handling object effects such as shadows and reflections, using diffusion models trained on counterfactual datasets. However, the performance impact of applying classifier-free guidance to handle object effects across removal and insertion tasks within a unified model remains largely unexplored. To address this gap and improve efficiency in composite editing, we propose CrimEdit, which jointly trains the task embeddings for removal and insertion within a single model and leverages them in a classifier-free guidance scheme -- enhancing the removal of both objects and their effects, and enabling controllable synthesis of object effects during insertion. CrimEdit also extends these two task prompts to be applied to spatially distinct regions, enabling object movement (repositioning) within a single denoising step. By employing both guidance techniques, extensive experiments show that CrimEdit achieves superior object removal, controllable effect insertion, and efficient object movement without requiring additional training or separate removal and insertion stages.

</details>


### [45] [DiffPCN: Latent Diffusion Model Based on Multi-view Depth Images for Point Cloud Completion](https://arxiv.org/abs/2509.23723)
*Zijun Li,Hongyu Yan,Shijie Li,Kunming Luo,Li Lu,Xulei Yang,Weisi Lin*

Main category: cs.CV

TL;DR: DiffPCN是一个基于扩散模型的点云补全框架，采用从粗到精的两阶段方法，首先生成粗糙点云，然后通过去噪和上采样进行细化。


<details>
  <summary>Details</summary>
Motivation: 由于点云的无结构和不规则特性，潜在扩散模型在点云补全任务中的潜力尚未充分探索。

Method: 将无序点云投影为结构化深度图像，使用DepthLDM合成多视角深度图像形成粗糙点云，然后通过点去噪网络去除异常值，最后使用关联感知点上采样器生成密集高保真输出。

Result: 实验结果表明DiffPCN在几何精度和形状完整性方面达到最先进性能，显著提高了点云补全的鲁棒性和一致性。

Conclusion: DiffPCN通过结合扩散模型的强大生成能力和点云处理技术，为点云补全任务提供了一种有效的解决方案。

Abstract: Latent diffusion models (LDMs) have demonstrated remarkable generative capabilities across various low-level vision tasks. However, their potential for point cloud completion remains underexplored due to the unstructured and irregular nature of point clouds. In this work, we propose DiffPCN, a novel diffusion-based coarse-to-fine framework for point cloud completion. Our approach comprises two stages: an initial stage for generating coarse point clouds, and a refinement stage that improves their quality through point denoising and upsampling. Specifically, we first project the unordered and irregular partial point cloud into structured depth images, which serve as conditions for a well-designed DepthLDM to synthesize completed multi-view depth images that are used to form coarse point clouds. In this way, our DiffPCN can yield high-quality and high-completeness coarse point clouds by leveraging LDM' s powerful generation and comprehension capabilities. Then, since LDMs inevitably introduce outliers into the generated depth maps, we design a Point Denoising Network to remove artifacts from the coarse point cloud by predicting a per-point distance score. Finally, we devise an Association-Aware Point Upsampler, which guides the upsampling process by leveraging local association features between the input point cloud and the corresponding coarse points, further yielding a dense and high-fidelity output. Experimental results demonstrate that our DiffPCN achieves state-of-the-art performance in geometric accuracy and shape completeness, significantly improving the robustness and consistency of point cloud completion.

</details>


### [46] [M3DLayout: A Multi-Source Dataset of 3D Indoor Layouts and Structured Descriptions for 3D Generation](https://arxiv.org/abs/2509.23728)
*Yiheng Zhang,Zhuojiang Cai,Mingdao Wang,Meitong Guo,Tianxiao Li,Li Lin,Yuwang Wang*

Main category: cs.CV

TL;DR: 提出了M3DLayout数据集，这是一个用于3D室内布局生成的大规模多源数据集，包含15,080个布局和258k+物体实例，整合了真实扫描、CAD设计和程序生成场景三种来源。


<details>
  <summary>Details</summary>
Motivation: 当前3D室内布局生成模型受限于现有数据集的规模、多样性和标注质量不足，需要更丰富的数据资源来提升学习能力。

Method: 构建包含真实扫描、专业CAD设计和程序生成场景的多源数据集，每个布局都配有结构化文本描述，包括全局场景摘要、大型家具关系布局和小物品细粒度排列。

Result: 实验表明M3DLayout为布局生成模型提供了坚实基础，多源组成增强了多样性，特别是Inf3DLayout子集提供了丰富的小物体信息，能够生成更复杂详细的场景。

Conclusion: M3DLayout可作为推进文本驱动3D场景合成研究的有价值资源，其多样性和丰富标注有助于模型学习复杂的空间和语义模式。

Abstract: In text-driven 3D scene generation, object layout serves as a crucial intermediate representation that bridges high-level language instructions with detailed geometric output. It not only provides a structural blueprint for ensuring physical plausibility but also supports semantic controllability and interactive editing. However, the learning capabilities of current 3D indoor layout generation models are constrained by the limited scale, diversity, and annotation quality of existing datasets. To address this, we introduce M3DLayout, a large-scale, multi-source dataset for 3D indoor layout generation. M3DLayout comprises 15,080 layouts and over 258k object instances, integrating three distinct sources: real-world scans, professional CAD designs, and procedurally generated scenes. Each layout is paired with detailed structured text describing global scene summaries, relational placements of large furniture, and fine-grained arrangements of smaller items. This diverse and richly annotated resource enables models to learn complex spatial and semantic patterns across a wide variety of indoor environments. To assess the potential of M3DLayout, we establish a benchmark using a text-conditioned diffusion model. Experimental results demonstrate that our dataset provides a solid foundation for training layout generation models. Its multi-source composition enhances diversity, notably through the Inf3DLayout subset which provides rich small-object information, enabling the generation of more complex and detailed scenes. We hope that M3DLayout can serve as a valuable resource for advancing research in text-driven 3D scene synthesis.

</details>


### [47] [HieraTok: Multi-Scale Visual Tokenizer Improves Image Reconstruction and Generation](https://arxiv.org/abs/2509.23736)
*Cong Chen,Ziyuan Huang,Cheng Zou,Muzhi Zhu,Kaixiang Ji,Jiajia Liu,Jingdong Chen,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: HieraTok是一个新颖的多尺度Vision Transformer tokenizer，通过多尺度下采样和尺度因果注意力机制，在图像重建和生成任务中显著优于单尺度tokenizer。


<details>
  <summary>Details</summary>
Motivation: 克服单尺度表示建模的固有局限性，实现从低分辨率全局语义特征到高分辨率结构细节的信息渐进流动。

Method: 使用多尺度下采样生成多尺度token序列，结合尺度因果注意力机制实现信息从粗到细的流动。

Result: 相比单尺度tokenizer，rFID提升27.2%(1.47→1.07)，下游生成任务收敛速度提升1.38倍，gFID提升18.9%(16.4→13.3)，最佳rFID达到0.45，gFID达到1.82。

Conclusion: HieraTok是首个多尺度ViT tokenizer，其平滑且均匀分布的潜在空间显著提升了视觉生成任务的性能。

Abstract: In this work, we present HieraTok, a novel multi-scale Vision Transformer (ViT)-based tokenizer that overcomes the inherent limitation of modeling single-scale representations. This is realized through two key designs: (1) multi-scale downsampling applied to the token map generated by the tokenizer encoder, producing a sequence of multi-scale tokens, and (2) a scale-causal attention mechanism that enables the progressive flow of information from low-resolution global semantic features to high-resolution structural details. Coupling these designs, HieraTok achieves significant improvements in both image reconstruction and generation tasks. Under identical settings, the multi-scale visual tokenizer outperforms its single-scale counterpart by a 27.2\% improvement in rFID ($1.47 \rightarrow 1.07$). When integrated into downstream generation frameworks, it achieves a $1.38\times$ faster convergence rate and an 18.9\% boost in gFID ($16.4 \rightarrow 13.3$), which may be attributed to the smoother and more uniformly distributed latent space. Furthermore, by scaling up the tokenizer's training, we demonstrate its potential by a sota rFID of 0.45 and a gFID of 1.82 among ViT tokenizers. To the best of our knowledge, we are the first to introduce multi-scale ViT-based tokenizer in image reconstruction and image generation. We hope our findings and designs advance the ViT-based tokenizers in visual generation tasks.

</details>


### [48] [UniAlignment: Semantic Alignment for Unified Image Generation, Understanding, Manipulation and Perception](https://arxiv.org/abs/2509.23760)
*Xinyang Song,Libin Wang,Weining Wang,Shaozhen Liu,Dandan Zheng,Jingdong Chen,Qi Li,Zhenan Sun*

Main category: cs.CV

TL;DR: 提出了UniAlignment，一个基于单一扩散变换器的统一多模态生成框架，通过双流扩散训练策略增强跨模态一致性和指令跟随鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在文本到图像生成中的成功激发了将其扩展到多模态任务的需求，但现有方法依赖视觉语言模型或模块化设计，导致架构碎片化和计算效率低下。

Method: 采用单一扩散变换器架构，引入双流扩散训练策略，包含内在模态语义对齐和跨模态语义对齐，并创建SemGen-Bench基准来评估复杂文本指令下的多模态语义一致性。

Result: 在多个任务和基准测试中的广泛实验表明，UniAlignment优于现有基线方法。

Conclusion: 该研究展示了扩散模型在统一多模态生成中的巨大潜力。

Abstract: The remarkable success of diffusion models in text-to-image generation has sparked growing interest in expanding their capabilities to a variety of multi-modal tasks, including image understanding, manipulation, and perception. These tasks require advanced semantic comprehension across both visual and textual modalities, especially in scenarios involving complex semantic instructions. However, existing approaches often rely heavily on vision-language models (VLMs) or modular designs for semantic guidance, leading to fragmented architectures and computational inefficiency. To address these challenges, we propose UniAlignment, a unified multimodal generation framework within a single diffusion transformer. UniAlignment introduces a dual-stream diffusion training strategy that incorporates both intrinsic-modal semantic alignment and cross-modal semantic alignment, thereby enhancing the model's cross-modal consistency and instruction-following robustness. Additionally, we present SemGen-Bench, a new benchmark specifically designed to evaluate multimodal semantic consistency under complex textual instructions. Extensive experiments across multiple tasks and benchmarks demonstrate that UniAlignment outperforms existing baselines, underscoring the significant potential of diffusion models in unified multimodal generation.

</details>


### [49] [Texture Vector-Quantization and Reconstruction Aware Prediction for Generative Super-Resolution](https://arxiv.org/abs/2509.23774)
*Qifan Li,Jiale Zou,Jinhua Zhang,Wei Long,Xinyu Zhou,Shuhang Gu*

Main category: cs.CV

TL;DR: 提出纹理向量量化和重建感知预测策略，解决VQ方法在视觉先验建模中的量化误差和次优预测问题，实现高质量超分辨率重建


<details>
  <summary>Details</summary>
Motivation: 现有VQ方法存在两个问题：1）视觉特征丰富导致VQ编码产生较大量化误差；2）使用代码级监督训练预测器无法考虑最终重建误差，导致先验建模精度次优

Method: 1. 纹理向量量化：利用超分辨率任务特性，仅对缺失纹理引入码本建模先验；2. 重建感知预测：使用直通估计器直接以图像级监督训练索引预测器

Result: 提出的TVQ&RAP生成式SR模型能够以较小计算成本实现照片级真实感的超分辨率结果

Conclusion: 通过纹理向量量化和重建感知预测策略，有效解决了VQ方法在视觉先验建模中的局限性，实现了高效高质量的超分辨率重建

Abstract: Vector-quantized based models have recently demonstrated strong potential for visual prior modeling. However, existing VQ-based methods simply encode visual features with nearest codebook items and train index predictor with code-level supervision. Due to the richness of visual signal, VQ encoding often leads to large quantization error. Furthermore, training predictor with code-level supervision can not take the final reconstruction errors into consideration, result in sub-optimal prior modeling accuracy. In this paper we address the above two issues and propose a Texture Vector-Quantization and a Reconstruction Aware Prediction strategy. The texture vector-quantization strategy leverages the task character of super-resolution and only introduce codebook to model the prior of missing textures. While the reconstruction aware prediction strategy makes use of the straight-through estimator to directly train index predictor with image-level supervision. Our proposed generative SR model (TVQ&RAP) is able to deliver photo-realistic SR results with small computational cost.

</details>


### [50] [Uni4D-LLM: A Unified SpatioTemporal-Aware VLM for 4D Understanding and Generation](https://arxiv.org/abs/2509.23828)
*Hanyu Zhou,Gim Hee Lee*

Main category: cs.CV

TL;DR: Uni4D-LLM是首个统一4D场景理解和生成的视觉语言模型框架，通过共享表示和架构实现时空感知的统一处理。


<details>
  <summary>Details</summary>
Motivation: 现有3D和4D方法通常在语义理解上使用自回归模型，在内容生成上使用扩散模型，这种范式差距阻碍了单一模型同时处理理解和生成任务，特别是在需要时空建模的动态4D场景中。

Method: 1) 提取语义特征用于理解，注入噪声的外观特征用于生成，结合4D几何线索，通过自适应交叉注意力融合成时空感知的视觉表示；2) 将自回归和扩散集成到单一LLM中，使用任务特定的头部；3) 在多样4D视觉语言数据集上进行指令微调以提高泛化能力。

Result: 在多个基准测试上的广泛实验表明，Uni4D-LLM相比最先进模型取得了竞争性或更优的结果，实现了4D场景理解和生成的真正统一。

Conclusion: Uni4D-LLM通过共享表示和架构的设计，成功实现了4D场景理解和生成的统一，为物理世界的视觉语言建模提供了新的解决方案。

Abstract: Vision-language models (VLMs) have demonstrated strong performance in 2D scene understanding and generation, but extending this unification to the physical world remains an open challenge. Existing 3D and 4D approaches typically embed scene geometry into autoregressive model for semantic understanding and diffusion model for content generation. This paradigm gap prevents a single model from jointly handling both tasks, especially in dynamic 4D settings where spatiotemporal modeling is critical. We propose Uni4D-LLM, the first unified VLM framework with spatiotemporal awareness for 4D scene understanding and generation. Our design is guided by two key insights: 1) Unification requires a shared representation. We extract semantic features for understanding and noisy-injected appearance features for generation, incorporate 4D geometric cues, and fuse them into a spatiotemporal-aware visual representation through adaptive cross-attention. 2) Unification requires a shared architecture. Both autoregression and diffusion are built on Transformer backbones, and this enables integration into a single LLM with task-specific heads. By aligning visual and linguistic representations, our Uni4D-LLM produces predictions for both understanding and generation within one Transformer-based framework. We further apply instruction fine-tuning on diverse 4D vision-language datasets to improve generalization across tasks. Extensive experiments on multiple benchmarks demonstrate that Uni4D-LLM achieves competitive or superior results compared to state-of-the-art models and offers the first true unification of 4D scene understanding and generation.

</details>


### [51] [FairViT-GAN: A Hybrid Vision Transformer with Adversarial Debiasing for Fair and Explainable Facial Beauty Prediction](https://arxiv.org/abs/2509.23859)
*Djamel Eddine Boukhari*

Main category: cs.CV

TL;DR: 提出FairViT-GAN框架，结合CNN和ViT优势，通过对抗性去偏机制解决面部美预测中的准确性和公平性问题。


<details>
  <summary>Details</summary>
Motivation: 现有面部美预测模型存在架构限制、人口统计偏见和缺乏透明度等问题，需要同时解决准确性和公平性挑战。

Method: 使用CNN分支提取局部特征，ViT分支建模全局上下文，并引入对抗性去偏机制使特征表示对受保护属性不变。

Result: 在SCUT-FBP5500基准测试中达到新SOTA，皮尔逊相关0.9230，RMSE 0.2650，种族子组间性能差距减少82.9%，对抗者分类准确率降至52.1%。

Conclusion: FairViT-GAN为开发负责任的主观视觉评估AI系统提供了稳健、透明且更公平的蓝图。

Abstract: Facial Beauty Prediction (FBP) has made significant strides with the application of deep learning, yet state-of-the-art models often exhibit critical limitations, including architectural constraints, inherent demographic biases, and a lack of transparency. Existing methods, primarily based on Convolutional Neural Networks (CNNs), excel at capturing local texture but struggle with global facial harmony, while Vision Transformers (ViTs) effectively model long-range dependencies but can miss fine-grained details. Furthermore, models trained on benchmark datasets can inadvertently learn and perpetuate societal biases related to protected attributes like ethnicity. To address these interconnected challenges, we propose \textbf{FairViT-GAN}, a novel hybrid framework that synergistically integrates a CNN branch for local feature extraction and a ViT branch for global context modeling. More significantly, we introduce an adversarial debiasing mechanism where the feature extractor is explicitly trained to produce representations that are invariant to protected attributes, thereby actively mitigating algorithmic bias. Our framework's transparency is enhanced by visualizing the distinct focus of each architectural branch. Extensive experiments on the SCUT-FBP5500 benchmark demonstrate that FairViT-GAN not only sets a new state-of-the-art in predictive accuracy, achieving a Pearson Correlation of \textbf{0.9230} and reducing RMSE to \textbf{0.2650}, but also excels in fairness. Our analysis reveals a remarkable \textbf{82.9\% reduction in the performance gap} between ethnic subgroups, with the adversary's classification accuracy dropping to near-random chance (52.1\%). We believe FairViT-GAN provides a robust, transparent, and significantly fairer blueprint for developing responsible AI systems for subjective visual assessment.

</details>


### [52] [Not All Tokens are Guided Equal: Improving Guidance in Visual Autoregressive Models](https://arxiv.org/abs/2509.23876)
*Ky Dan Nguyen,Hoang Lam Tran,Anh-Dung Dinh,Daochang Liu,Weidong Cai,Xiuying Wang,Chang Xu*

Main category: cs.CV

TL;DR: 提出信息接地引导(IGG)机制，通过注意力机制将引导信号锚定到语义重要区域，解决自回归图像生成模型中因渐进分辨率缩放导致的信息不一致问题。


<details>
  <summary>Details</summary>
Motivation: 自回归图像生成模型面临关键弱点：渐进分辨率缩放导致跨时间步的补丁间信息不一致，这些不一致会分散引导信号，使其偏离条件信息，留下模糊不忠实的特征。

Method: 开发信息接地引导(IGG)机制，通过注意力自适应地增强采样过程中的信息丰富补丁，确保引导和内容保持紧密对齐。

Result: 在类别条件和文本到图像生成任务中，IGG生成更清晰、更连贯且语义接地的图像，为基于自回归的方法设立了新基准。

Conclusion: IGG通过将引导锚定到语义重要区域，有效解决了自回归图像生成中的信息不一致问题，显著提升了生成质量。

Abstract: Autoregressive (AR) models based on next-scale prediction are rapidly emerging as a powerful tool for image generation, but they face a critical weakness: information inconsistencies between patches across timesteps introduced by progressive resolution scaling. These inconsistencies scatter guidance signals, causing them to drift away from conditioning information and leaving behind ambiguous, unfaithful features. We tackle this challenge with Information-Grounding Guidance (IGG), a novel mechanism that anchors guidance to semantically important regions through attention. By adaptively reinforcing informative patches during sampling, IGG ensures that guidance and content remain tightly aligned. Across both class-conditioned and text-to-image generation tasks, IGG delivers sharper, more coherent, and semantically grounded images, setting a new benchmark for AR-based methods.

</details>


### [53] [Tunable-Generalization Diffusion Powered by Self-Supervised Contextual Sub-Data for Low-Dose CT Reconstruction](https://arxiv.org/abs/2509.23885)
*Guoquan Wei,Zekun Zhou,Liu Shi,Wenzhe Shan,Qiegen Liu*

Main category: cs.CV

TL;DR: 提出SuperDiff方法，一种基于自监督上下文子数据的可调泛化扩散模型，用于低剂量CT重建，无需配对数据且具有良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的低剂量CT去噪方法严重依赖配对数据且泛化能力差，扩散模型需要学习干净数据分布，这在医疗临床应用中难以满足。自监督方法面临从当前剂量预训练模型扩展到其他剂量时泛化能力显著下降的挑战。

Method: 1. 设计上下文子数据相似性自适应感知策略用于投影域去噪；2. 结合知识蒸馏与潜在扩散模型优化图像细节；3. 使用像素级自校正融合技术增强图像保真度；4. 灵活应用于上下剂量甚至未见剂量的泛化。

Result: 在数据集和真实数据上的全面定性和定量评估表明，SuperDiff在重建和泛化性能方面始终优于现有最先进方法。

Conclusion: SuperDiff通过双域策略级联实现自监督低剂量CT去噪，仅需低剂量CT投影域数据进行训练和测试，在重建质量和泛化能力方面表现出色。

Abstract: Current models based on deep learning for low-dose CT denoising rely heavily on paired data and generalize poorly. Even the more concerned diffusion models need to learn the distribution of clean data for reconstruction, which is difficult to satisfy in medical clinical applications. At the same time, self-supervised-based methods face the challenge of significant degradation of generalizability of models pre-trained for the current dose to expand to other doses. To address these issues, this paper proposes a novel method of tunable-generalization diffusion powered by self-supervised contextual sub-data for low-dose CT reconstruction, named SuperDiff. Firstly, a contextual subdata similarity adaptive sensing strategy is designed for denoising centered on the LDCT projection domain, which provides an initial prior for the subsequent progress. Subsequently, the initial prior is used to combine knowledge distillation with a deep combination of latent diffusion models for optimizing image details. The pre-trained model is used for inference reconstruction, and the pixel-level self-correcting fusion technique is proposed for fine-grained reconstruction of the image domain to enhance the image fidelity, using the initial prior and the LDCT image as a guide. In addition, the technique is flexibly applied to the generalization of upper and lower doses or even unseen doses. Dual-domain strategy cascade for self-supervised LDCT denoising, SuperDiff requires only LDCT projection domain data for training and testing. Full qualitative and quantitative evaluations on both datasets and real data show that SuperDiff consistently outperforms existing state-of-the-art methods in terms of reconstruction and generalization performance.

</details>


### [54] [Token Painter: Training-Free Text-Guided Image Inpainting via Mask Autoregressive Models](https://arxiv.org/abs/2509.23919)
*Longtao Jiang,Mingfei Han,Lei Chen,Yongqiang Yu,Feng Zhao,Xiaojun Chang,Zhihui Li*

Main category: cs.CV

TL;DR: 提出了一种基于掩码自回归模型的免训练文本引导图像修复方法Token Painter，通过双流编码器信息融合和自适应解码器注意力增强，在保持背景一致性的同时生成与文本提示对齐的修复内容。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在文本引导图像修复中面临挑战：在潜在空间建模整个图像导致结果难以与提示细节对齐且难以保持背景一致性。掩码自回归模型虽然支持图像修复，但直接应用会忽略提示或与背景不协调。

Method: 1. 双流编码器信息融合(DEIF)：在频域融合文本和背景的语义和上下文信息，生成引导标记；2. 自适应解码器注意力增强(ADAE)：自适应增强对引导标记和修复标记的注意力分数。

Result: 大量实验表明，该免训练方法在几乎所有指标上都优于现有最先进方法，并提供了更优越的视觉结果。

Conclusion: Token Painter通过分析注意力图并利用背景标记对文本标记的影响，实现了更好的文本对齐和背景协调性，为文本引导图像修复提供了有效的解决方案。

Abstract: Text-guided image inpainting aims to inpaint masked image regions based on a textual prompt while preserving the background. Although diffusion-based methods have become dominant, their property of modeling the entire image in latent space makes it challenging for the results to align well with prompt details and maintain a consistent background. To address these issues, we explore Mask AutoRegressive (MAR) models for this task. MAR naturally supports image inpainting by generating latent tokens corresponding to mask regions, enabling better local controllability without altering the background. However, directly applying MAR to this task makes the inpainting content either ignore the prompts or be disharmonious with the background context. Through analysis of the attention maps from the inpainting images, we identify the impact of background tokens on text tokens during the MAR generation, and leverage this to design \textbf{Token Painter}, a training-free text-guided image inpainting method based on MAR. Our approach introduces two key components: (1) Dual-Stream Encoder Information Fusion (DEIF), which fuses the semantic and context information from text and background in frequency domain to produce novel guidance tokens, allowing MAR to generate text-faithful inpainting content while keeping harmonious with background context. (2) Adaptive Decoder Attention Score Enhancing (ADAE), which adaptively enhances attention scores on guidance tokens and inpainting tokens to further enhance the alignment of prompt details and the content visual quality. Extensive experiments demonstrate that our training-free method outperforms prior state-of-the-art methods across almost all metrics and delivers superior visual results. Codes will be released.

</details>


### [55] [CrashSplat: 2D to 3D Vehicle Damage Segmentation in Gaussian Splatting](https://arxiv.org/abs/2509.23947)
*Dragoş-Andrei Chileban,Andrei-Ştefan Bulzan,Cosmin Cernǎzanu-Glǎvan*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯泼溅的汽车损伤检测方法，通过单视图分割实现3D损伤分割，特别适用于只在单视角可见的小损伤检测。


<details>
  <summary>Details</summary>
Motivation: 现有方法多基于2D图像分析，而3D重建方法能提供更全面和几何精确的损伤表示。3D高斯泼溅技术能从有限视角生成准确的3D重建，但现有方法需要多视角一致性，不适用于只在单视角可见的损伤。

Method: 提出自动汽车损伤检测流程，通过2D掩码提升实现3D损伤分割。采用无需学习的方法进行单视图3D高斯泼溅分割：将高斯投影到图像平面，使用SfM获取相机参数，通过Z缓冲和深度/不透明度的正态分布模型进行过滤。

Result: 该方法在具有挑战性的汽车损伤检测场景中特别有效，如划痕和小凹陷等目标物体可能只在单视角清晰可见的情况。

Conclusion: 提出的单视图3D高斯泼溅分割方法为汽车损伤检测提供了一种有效的解决方案，特别适用于多视角一致性方法不适用或不可行的场景。

Abstract: Automatic car damage detection has been a topic of significant interest for the auto insurance industry as it promises faster, accurate, and cost-effective damage assessments. However, few works have gone beyond 2D image analysis to leverage 3D reconstruction methods, which have the potential to provide a more comprehensive and geometrically accurate representation of the damage. Moreover, recent methods employing 3D representations for novel view synthesis, particularly 3D Gaussian Splatting (3D-GS), have demonstrated the ability to generate accurate and coherent 3D reconstructions from a limited number of views. In this work we introduce an automatic car damage detection pipeline that performs 3D damage segmentation by up-lifting 2D masks. Additionally, we propose a simple yet effective learning-free approach for single-view 3D-GS segmentation. Specifically, Gaussians are projected onto the image plane using camera parameters obtained via Structure from Motion (SfM). They are then filtered through an algorithm that utilizes Z-buffering along with a normal distribution model of depth and opacities. Through experiments we found that this method is particularly effective for challenging scenarios like car damage detection, where target objects (e.g., scratches, small dents) may only be clearly visible in a single view, making multi-view consistency approaches impractical or impossible. The code is publicly available at: https://github.com/DragosChileban/CrashSplat.

</details>


### [56] [HunyuanImage 3.0 Technical Report](https://arxiv.org/abs/2509.23951)
*Siyu Cao,Hangting Chen,Peng Chen,Yiji Cheng,Yutao Cui,Xinchi Deng,Ying Dong,Kipper Gong,Tianpeng Gu,Xiusen Gu,Tiankai Hang,Duojun Huang,Jie Jiang,Zhengkai Jiang,Weijie Kong,Changlin Li,Donghao Li,Junzhe Li,Xin Li,Yang Li,Zhenxi Li,Zhimin Li,Jiaxin Lin,Linus,Lucaz Liu,Shu Liu,Songtao Liu,Yu Liu,Yuhong Liu,Yanxin Long,Fanbin Lu,Qinglin Lu,Yuyang Peng,Yuanbo Peng,Xiangwei Shen,Yixuan Shi,Jiale Tao,Yangyu Tao,Qi Tian,Pengfei Wan,Chunyu Wang,Kai Wang,Lei Wang,Linqing Wang,Lucas Wang,Qixun Wang,Weiyan Wang,Hao Wen,Bing Wu,Jianbing Wu,Yue Wu,Senhao Xie,Fang Yang,Miles Yang,Xiaofeng Yang,Xuan Yang,Zhantao Yang,Jingmiao Yu,Zheng Yuan,Chao Zhang,Jian-Wei Zhang,Peizhen Zhang,Shi-Xue Zhang,Tao Zhang,Weigang Zhang,Yepeng Zhang,Yingfang Zhang,Zihao Zhang,Zijian Zhang,Penghao Zhao,Zhiyuan Zhao,Xuefei Zhe,Jianchen Zhu,Zhao Zhong*

Main category: cs.CV

TL;DR: HunyuanImage 3.0是一个统一多模态理解和生成的自回归框架，通过精心数据准备、先进架构设计、思维链机制等关键技术，训练了超过800亿参数的MoE模型，在文本图像对齐和视觉质量方面达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 开发一个统一多模态理解和生成的强大基础模型，推动多模态生态系统发展，为社区提供最先进的图像生成能力。

Method: 采用自回归框架统一多模态任务，包含数据准备、架构设计、原生思维链、渐进预训练、积极后训练等关键技术，训练了800亿参数的MoE模型。

Result: 模型在文本图像对齐和视觉质量评估中达到最先进水平，是当前最大最强的开源图像生成模型。

Conclusion: HunyuanImage 3.0成功展示了统一多模态框架的潜力，通过开源代码和权重促进多模态研究社区的发展。

Abstract: We present HunyuanImage 3.0, a native multimodal model that unifies multimodal understanding and generation within an autoregressive framework, with its image generation module publicly available. The achievement of HunyuanImage 3.0 relies on several key components, including meticulous data curation, advanced architecture design, a native Chain-of-Thoughts schema, progressive model pre-training, aggressive model post-training, and an efficient infrastructure that enables large-scale training and inference. With these advancements, we successfully trained a Mixture-of-Experts (MoE) model comprising over 80 billion parameters in total, with 13 billion parameters activated per token during inference, making it the largest and most powerful open-source image generative model to date. We conducted extensive experiments and the results of automatic and human evaluation of text-image alignment and visual quality demonstrate that HunyuanImage 3.0 rivals previous state-of-the-art models. By releasing the code and weights of HunyuanImage 3.0, we aim to enable the community to explore new ideas with a state-of-the-art foundation model, fostering a dynamic and vibrant multimodal ecosystem. All open source assets are publicly available at https://github.com/Tencent-Hunyuan/HunyuanImage-3.0

</details>


### [57] [Reinforcement Learning with Inverse Rewards for World Model Post-training](https://arxiv.org/abs/2509.23958)
*Yang Ye,Tianyu He,Shuo Yang,Jiang Bian*

Main category: cs.CV

TL;DR: 提出了RLIR框架，通过逆向动力学模型从生成视频中恢复输入动作，为视频世界模型提供可验证的奖励信号，提升动作跟随能力


<details>
  <summary>Details</summary>
Motivation: 现有视频世界模型在动作跟随能力方面存在不足，但直接应用强化学习后训练方法不现实，因为大规模偏好标注成本过高且难以构建基于规则的视频验证器

Method: 使用逆向动力学模型将高维视频模态映射到低维动作空间，通过恢复输入动作来获得可验证的奖励信号，采用组相对策略优化进行优化

Result: 在自回归和扩散范式下，动作跟随能力提升5-10%，视觉质量提升高达10%，获得更高的人类偏好评分

Conclusion: RLIR是首个专门设计用于增强视频世界模型动作跟随能力的后训练方法，有效解决了奖励信号获取的难题

Abstract: World models simulate dynamic environments, enabling agents to interact with diverse input modalities. Although recent advances have improved the visual quality and temporal consistency of video world models, their ability of accurately modeling human-specified actions remains under-explored. Reinforcement learning presents a promising approach for directly improving the suboptimal action-following capability of pre-trained models, assuming that an appropriate reward function can be defined. However, transferring reinforcement learning post-training methods to world model is impractical due to the prohibitive cost of large-scale preference annotations and the infeasibility of constructing rule-based video verifiers. To address this gap, we propose Reinforcement Learning with Inverse Rewards (RLIR), a post-training framework that derives verifiable reward signals by recovering input actions from generated videos using an Inverse Dynamics Model. By mapping high-dimensional video modality to a low-dimensional action space, RLIR provides an objective and verifiable reward for optimization via Group Relative Policy Optimization. Experiments across autoregressive and diffusion paradigms demonstrate 5-10% gains in action-following, up to 10% improvements in visual quality, and higher human preference scores, establishing RLIR as the first post-training method specifically designed to enhance action-following in video world models.

</details>


### [58] [VFSI: Validity First Spatial Intelligence for Constraint-Guided Traffic Diffusion](https://arxiv.org/abs/2509.23971)
*Kargi Chauhan,Leilani H. Gilpin*

Main category: cs.CV

TL;DR: 提出VFSI方法，通过基于能量的引导在扩散采样过程中强制执行物理约束，无需重新训练模型，显著提高交通仿真的物理有效性


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型生成的交通仿真虽然真实但系统性地违反物理约束，50%的轨迹违反基本物理定律，如车辆碰撞、驶离道路、在建筑物内生成

Method: VFSI方法在扩散采样过程中使用基于能量的引导，将碰撞避免和运动学约束作为能量函数，引导去噪过程生成物理有效的轨迹

Result: 在Waymo开放运动数据集的200个城市场景中，VFSI将碰撞率降低67%（24.6%到8.1%），整体有效性提高87%（50.3%到94.2%），同时改进真实感指标（ADE：1.34m到1.21m）

Conclusion: 在推理过程中显式强制执行约束对于物理有效的交通仿真是必要且充分的，模型无关的方法证明了这一点

Abstract: Modern diffusion models generate realistic traffic simulations but systematically violate physical constraints. In a large-scale evaluation of SceneDiffuser++, a state-of-the-art traffic simulator, we find that 50% of generated trajectories violate basic physical laws - vehicles collide, drive off roads, and spawn inside buildings. This reveals a fundamental limitation: current models treat physical validity as an emergent property rather than an architectural requirement. We propose Validity-First Spatial Intelligence (VFSI), which enforces constraints through energy-based guidance during diffusion sampling, without model retraining. By incorporating collision avoidance and kinematic constraints as energy functions, we guide the denoising process toward physically valid trajectories. Across 200 urban scenarios from the Waymo Open Motion Dataset, VFSI reduces collision rates by 67% (24.6% to 8.1%) and improves overall validity by 87% (50.3% to 94.2%), while simultaneously improving realism metrics (ADE: 1.34m to 1.21m). Our model-agnostic approach demonstrates that explicit constraint enforcement during inference is both necessary and sufficient for physically valid traffic simulation.

</details>


### [59] [Towards Redundancy Reduction in Diffusion Models for Efficient Video Super-Resolution](https://arxiv.org/abs/2509.23980)
*Jinpei Guo,Yifei Ji,Zheng Chen,Yufei Wang,Sizhuo Ma,Yong Guo,Yulun Zhang,Jian Wang*

Main category: cs.CV

TL;DR: OASIS是一种高效的单步扩散模型，通过注意力专业化路由和渐进式训练策略，在视频超分辨率任务中实现了SOTA性能和6.2倍加速。


<details>
  <summary>Details</summary>
Motivation: 直接应用生成式扩散模型到视频超分辨率会产生冗余，因为低质量视频已包含大量内容信息，导致计算开销增加和学习负担加重。

Method: 提出注意力专业化路由，根据注意力头内在行为分配不同模式；采用渐进式训练策略，从时间一致退化到不一致退化。

Result: 在合成和真实世界数据集上实现SOTA性能，相比SeedVR2等单步扩散基线提供6.2倍加速。

Conclusion: OASIS通过减少冗余和有效保留预训练知识，使扩散模型更好地适应视频超分辨率任务。

Abstract: Diffusion models have recently shown promising results for video super-resolution (VSR). However, directly adapting generative diffusion models to VSR can result in redundancy, since low-quality videos already preserve substantial content information. Such redundancy leads to increased computational overhead and learning burden, as the model performs superfluous operations and must learn to filter out irrelevant information. To address this problem, we propose OASIS, an efficient $\textbf{o}$ne-step diffusion model with $\textbf{a}$ttention $\textbf{s}$pecialization for real-world v$\textbf{i}$deo $\textbf{s}$uper-resolution. OASIS incorporates an attention specialization routing that assigns attention heads to different patterns according to their intrinsic behaviors. This routing mitigates redundancy while effectively preserving pretrained knowledge, allowing diffusion models to better adapt to VSR and achieve stronger performance. Moreover, we propose a simple yet effective progressive training strategy, which starts with temporally consistent degradations and then shifts to inconsistent settings. This strategy facilitates learning under complex degradations. Extensive experiments demonstrate that OASIS achieves state-of-the-art performance on both synthetic and real-world datasets. OASIS also provides superior inference speed, offering a $\textbf{6.2$\times$}$ speedup over one-step diffusion baselines such as SeedVR2. The code will be available at \href{https://github.com/jp-guo/OASIS}{https://github.com/jp-guo/OASIS}.

</details>


### [60] [$\mathbf{R}^3$: Reconstruction, Raw, and Rain: Deraining Directly in the Bayer Domain](https://arxiv.org/abs/2509.24022)
*Nate Rothschild,Moshe Kimhi,Avi Mendelson,Chaim Baskin*

Main category: cs.CV

TL;DR: 该论文提出直接在原始Bayer格式上学习图像重建，相比传统的sRGB后处理流程能获得更好的去雨效果，并引入了新的评估指标ICS。


<details>
  <summary>Details</summary>
Motivation: 传统图像重建网络在sRGB图像上训练，但图像信号处理(ISP)流程会不可逆地混合颜色、裁剪动态范围和模糊细节。本文旨在证明这些损失是可以避免的。

Method: 使用原始Bayer马赛克直接学习重建；创建Raw-Rain基准数据集；引入信息守恒分数(ICS)评估指标；比较后ISP和Bayer重建流程。

Result: 在测试集上，原始域模型将sRGB结果提升了+0.99 dB PSNR和+1.2% ICS，同时运行速度更快且计算量减半。

Conclusion: 研究结果支持低层视觉采用ISP后处理范式，并为端到端可学习相机管道开辟了道路。

Abstract: Image reconstruction from corrupted images is crucial across many domains. Most reconstruction networks are trained on post-ISP sRGB images, even though the image-signal-processing pipeline irreversibly mixes colors, clips dynamic range, and blurs fine detail. This paper uses the rain degradation problem as a use case to show that these losses are avoidable, and demonstrates that learning directly on raw Bayer mosaics yields superior reconstructions. To substantiate the claim, we (i) evaluate post-ISP and Bayer reconstruction pipelines, (ii) curate Raw-Rain, the first public benchmark of real rainy scenes captured in both 12-bit Bayer and bit-depth-matched sRGB, and (iii) introduce Information Conservation Score (ICS), a color-invariant metric that aligns more closely with human opinion than PSNR or SSIM. On the test split, our raw-domain model improves sRGB results by up to +0.99 dB PSNR and +1.2% ICS, while running faster with half of the GFLOPs. The results advocate an ISP-last paradigm for low-level vision and open the door to end-to-end learnable camera pipelines.

</details>


### [61] [Autoregressive Video Generation beyond Next Frames Prediction](https://arxiv.org/abs/2509.24081)
*Sucheng Ren,Chen Chen,Zhenbang Wang,Liangchen Song,Xiangxin Zhu,Alan Yuille,Yinfei Yang,Jiasen Lu*

Main category: cs.CV

TL;DR: VideoAR提出了一种统一的视频生成框架，支持多种预测单元（完整帧、关键细节帧、多尺度细化、时空立方体），其中时空立方体作为预测单元在质量和效率上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 质疑传统逐帧预测的合理性，认为帧不一定是视频自回归的自然原子单元，需要探索更合适的预测单元。

Method: 开发了VideoAR框架，支持多种预测单元选择，特别是引入时空立方体作为预测单元，让自回归模型能同时在空间和时间维度上操作。

Result: 基于立方体的预测在VBench基准测试中超越了最先进方法，实现了更快的推理速度，并能无缝扩展到分钟级序列。

Conclusion: 这项工作鼓励重新思考视频和其他时空域中的序列分解方式，打破了逐帧预测的限制。

Abstract: Autoregressive models for video generation typically operate frame-by-frame, extending next-token prediction from language to video's temporal dimension. We question that unlike word as token is universally agreed in language if frame is a appropriate prediction unit? To address this, we present VideoAR, a unified framework that supports a spectrum of prediction units including full frames, key-detail frames, multiscale refinements, and spatiotemporal cubes. Among these designs, we find model video generation using \textit{spatiotemporal} cubes as prediction units, which allows autoregressive models to operate across both spatial and temporal dimensions simultaneously. This approach eliminates the assumption that frames are the natural atomic units for video autoregression. We evaluate VideoAR across diverse prediction strategies, finding that cube-based prediction consistently delivers superior quality, speed, and temporal coherence. By removing the frame-by-frame constraint, our video generator surpasses state-of-the-art baselines on VBench while achieving faster inference and enabling seamless scaling to minute-long sequences. We hope this work will motivate rethinking sequence decomposition in video and other spatiotemporal domains.

</details>


### [62] [GANji: A Framework for Introductory AI Image Generation](https://arxiv.org/abs/2509.24128)
*Chandon Hamel,Mike Busch*

Main category: cs.CV

TL;DR: GANji是一个轻量级框架，用于比较VAE、GAN和DDPM在日文汉字生成上的性能，发现DDPM图像质量最好但采样速度最慢。


<details>
  <summary>Details</summary>
Motivation: 生成模型比较研究通常需要大量计算资源，这为研究人员和实践者设置了障碍。

Method: 使用10,314个日文汉字字符数据集，系统比较VAE、GAN和DDPM三种基础AI图像生成技术的性能。

Result: DDPM获得最高图像保真度（FID得分26.2），但其采样时间比其他模型慢2,000多倍。

Conclusion: GANji框架是揭示模型架构、计算成本和视觉质量之间基本权衡的有效且易用的工具，适合教育和研究用途。

Abstract: The comparative study of generative models often requires significant computational resources, creating a barrier for researchers and practitioners. This paper introduces GANji, a lightweight framework for benchmarking foundational AI image generation techniques using a dataset of 10,314 Japanese Kanji characters. It systematically compares the performance of a Variational Autoencoder (VAE), a Generative Adversarial Network (GAN), and a Denoising Diffusion Probabilistic Model (DDPM). The results demonstrate that while the DDPM achieves the highest image fidelity, with a Fr\'echet Inception Distance (FID) score of 26.2, its sampling time is over 2,000 times slower than the other models. The GANji framework is an effective and accessible tool for revealing the fundamental trade-offs between model architecture, computational cost, and visual quality, making it ideal for both educational and research purposes.

</details>


### [63] [Asymmetric VAE for One-Step Video Super-Resolution Acceleration](https://arxiv.org/abs/2509.24142)
*Jianze Li,Yong Guo,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: FastVSR是一个高效的视频超分辨率模型，通过高压缩VAE（f16）和稳定训练框架，在保持性能的同时大幅提升推理速度，相比多步模型加速111.9倍，相比现有单步模型加速3.92倍。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的视频超分辨率方法虽然将采样步骤减少到一步，但在推理效率方面仍有很大优化空间，需要进一步降低计算成本。

Method: 采用高压缩VAE（空间压缩比16，f16），设计f16 VAE结构并引入稳定训练框架，使用像素重排和通道复制实现额外上采样，提出下界引导训练策略简化训练目标。

Result: 实验结果显示FastVSR相比多步模型加速111.9倍，相比现有单步模型加速3.92倍，同时保持了良好的性能。

Conclusion: FastVSR通过高压缩VAE和优化训练策略，在视频超分辨率任务中实现了显著的速度提升，为实时应用提供了可行的解决方案。

Abstract: Diffusion models have significant advantages in the field of real-world video super-resolution and have demonstrated strong performance in past research. In recent diffusion-based video super-resolution (VSR) models, the number of sampling steps has been reduced to just one, yet there remains significant room for further optimization in inference efficiency. In this paper, we propose FastVSR, which achieves substantial reductions in computational cost by implementing a high compression VAE (spatial compression ratio of 16, denoted as f16). We design the structure of the f16 VAE and introduce a stable training framework. We employ pixel shuffle and channel replication to achieve additional upsampling. Furthermore, we propose a lower-bound-guided training strategy, which introduces a simpler training objective as a lower bound for the VAE's performance. It makes the training process more stable and easier to converge. Experimental results show that FastVSR achieves speedups of 111.9 times compared to multi-step models and 3.92 times compared to existing one-step models. We will release code and models at https://github.com/JianzeLi-114/FastVSR.

</details>


### [64] [LatXGen: Towards Radiation-Free and Accurate Quantitative Analysis of Sagittal Spinal Alignment Via Cross-Modal Radiographic View Synthesis](https://arxiv.org/abs/2509.24165)
*Moxin Zhao,Nan Meng,Jason Pui Yin Cheung,Chris Yuk Kwan Tang,Chenxi Yu,Wenting Zhong,Pengyu Lu,Chang Shi,Yipeng Zhuang,Teng Zhang*

Main category: cs.CV

TL;DR: LatXGen是一个生成式框架，可从背部RGBD图像合成侧位脊柱X光片，实现无辐射的矢状面脊柱对齐评估。


<details>
  <summary>Details</summary>
Motivation: 青少年特发性脊柱侧凸需要冠状面和矢状面的三维评估，但现有无辐射方法主要关注冠状面，矢状面评估仍缺乏可靠的无辐射解决方案。

Method: 采用双阶段架构：首先估计侧位脊柱结构，然后合成相应X光片。引入基于注意力的快速傅里叶卷积模块整合解剖特征，以及空间变形网络建模侧位形态变化。

Result: LatXGen生成解剖学准确的X光片，在视觉保真度和定量指标上均优于现有GAN方法。构建了包含3,264对RGBD和侧位X光片的大规模配对数据集。

Conclusion: 该研究为矢状面脊柱评估提供了有前景的无辐射解决方案，推动了青少年特发性脊柱侧凸的全面评估。

Abstract: Adolescent Idiopathic Scoliosis (AIS) is a complex three-dimensional spinal deformity, and accurate morphological assessment requires evaluating both coronal and sagittal alignment. While previous research has made significant progress in developing radiation-free methods for coronal plane assessment, reliable and accurate evaluation of sagittal alignment without ionizing radiation remains largely underexplored. To address this gap, we propose LatXGen, a novel generative framework that synthesizes realistic lateral spinal radiographs from posterior Red-Green-Blue and Depth (RGBD) images of unclothed backs. This enables accurate, radiation-free estimation of sagittal spinal alignment. LatXGen tackles two core challenges: (1) inferring sagittal spinal morphology changes from a lateral perspective based on posteroanterior surface geometry, and (2) performing cross-modality translation from RGBD input to the radiographic domain. The framework adopts a dual-stage architecture that progressively estimates lateral spinal structure and synthesizes corresponding radiographs. To enhance anatomical consistency, we introduce an attention-based Fast Fourier Convolution (FFC) module for integrating anatomical features from RGBD images and 3D landmarks, and a Spatial Deformation Network (SDN) to model morphological variations in the lateral view. Additionally, we construct the first large-scale paired dataset for this task, comprising 3,264 RGBD and lateral radiograph pairs. Experimental results demonstrate that LatXGen produces anatomically accurate radiographs and outperforms existing GAN-based methods in both visual fidelity and quantitative metrics. This study offers a promising, radiation-free solution for sagittal spine assessment and advances comprehensive AIS evaluation.

</details>


### [65] [Tumor Synthesis conditioned on Radiomics](https://arxiv.org/abs/2509.24182)
*Jonghun Kim,Inye Na,Eun Sook Ko,Hyunjin Park*

Main category: cs.CV

TL;DR: 提出了一种基于放射组学特征的肿瘤生成模型，使用GAN生成肿瘤掩膜，扩散模型生成肿瘤纹理，可根据用户指定的放射组学特征在任意位置生成肿瘤图像。


<details>
  <summary>Details</summary>
Motivation: 由于隐私问题，医学图像分析中获取大型3D数据集具有挑战性，现有生成模型在输出多样性方面存在局限，无法准确表示3D医学图像。

Method: 结合GAN和扩散模型的方法：GAN生成肿瘤掩膜，扩散模型生成肿瘤纹理，两者都以放射组学特征为生成条件。

Result: 在四个不同器官（肾脏、肺、乳腺、脑）的CT和MRI上测试，合成图像有效辅助下游任务训练，并通过专家评估验证真实性。

Conclusion: 该方法可用于治疗规划，通过生成多样化肿瘤图像帮助医生更好地理解肿瘤特征变化。

Abstract: Due to privacy concerns, obtaining large datasets is challenging in medical image analysis, especially with 3D modalities like Computed Tomography (CT) and Magnetic Resonance Imaging (MRI). Existing generative models, developed to address this issue, often face limitations in output diversity and thus cannot accurately represent 3D medical images. We propose a tumor-generation model that utilizes radiomics features as generative conditions. Radiomics features are high-dimensional handcrafted semantic features that are biologically well-grounded and thus are good candidates for conditioning. Our model employs a GAN-based model to generate tumor masks and a diffusion-based approach to generate tumor texture conditioned on radiomics features. Our method allows the user to generate tumor images according to user-specified radiomics features such as size, shape, and texture at an arbitrary location. This enables the physicians to easily visualize tumor images to better understand tumors according to changing radiomics features. Our approach allows for the removal, manipulation, and repositioning of tumors, generating various tumor types in different scenarios. The model has been tested on tumors in four different organs (kidney, lung, breast, and brain) across CT and MRI. The synthesized images are shown to effectively aid in training for downstream tasks and their authenticity was also evaluated through expert evaluations. Our method has potential usage in treatment planning with diverse synthesized tumors.

</details>


### [66] [Simulating Post-Neoadjuvant Chemotherapy Breast Cancer MRI via Diffusion Model with Prompt Tuning](https://arxiv.org/abs/2509.24185)
*Jonghun Kim,Hyunjin Park*

Main category: cs.CV

TL;DR: 使用扩散模型从治疗前的DCE-MRI图像生成乳腺癌新辅助化疗后的图像，结合临床因素提示调优，能准确预测肿瘤大小变化。


<details>
  <summary>Details</summary>
Motivation: 新辅助化疗是乳腺癌常见术前疗法，准确预测其反应有助于治疗规划。现有方法需要随访DCE-MRI监测反应，本研究旨在提前预测治疗效果。

Method: 采用扩散模型从治疗前DCE-MRI的最大强度投影图像生成治疗后图像，引入提示调优机制整合临床因素。

Result: 模型在图像质量指标上优于其他生成模型，能更好地生成反映pCR相关肿瘤大小变化的图像，消融研究验证了方法设计。

Conclusion: 该方法有助于实现精准医疗，为新辅助化疗响应预测提供有效工具。

Abstract: Neoadjuvant chemotherapy (NAC) is a common therapy option before the main surgery for breast cancer. Response to NAC is monitored using follow-up dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI). Accurate prediction of NAC response helps with treatment planning. Here, we adopt maximum intensity projection images from DCE-MRI to generate post-treatment images (i.e., 3 or 12 weeks after NAC) from pre-treatment images leveraging the emerging diffusion model. We introduce prompt tuning to account for the known clinical factors affecting response to NAC. Our model performed better than other generative models in image quality metrics. Our model was better at generating images that reflected changes in tumor size according to pCR compared to other models. Ablation study confirmed the design choices of our method. Our study has the potential to help with precision medicine.

</details>


### [67] [An Efficient 3D Latent Diffusion Model for T1-contrast Enhanced MRI Generation](https://arxiv.org/abs/2509.24194)
*Zach Eidex,Mojtaba Safari,Jie Ding,Richard Qiu,Justin Roper,David Yu,Hui-Kuo Shu,Zhen Tian,Hui Mao,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 开发了一种高效的3D深度学习框架T1C-RFlow，用于从多参数MRI生成T1对比增强图像，避免使用钆基对比剂，并显著提高生成速度。


<details>
  <summary>Details</summary>
Motivation: 钆基对比剂在T1w MRI中常用于增强病灶可视化，但对有肾源性系统性纤维化风险的患者受限，且对比剂使用差异会导致成像不一致。

Method: 提出3D潜在整流流模型，首先将T1w和T2-FLAIR图像输入预训练自编码器获取潜在空间表示，然后在潜在空间中训练整流流扩散模型。

Result: T1C-RFlow在各项指标上优于基准模型，生成时间显著缩短（6.9秒/体积），在胶质瘤、脑膜瘤和转移瘤数据集上均表现优异。

Conclusion: 该方法生成的合成T1C图像与真实图像高度相似，且生成时间大大缩短，有望开发出实用的无对比剂脑肿瘤MRI方法。

Abstract: Objective: Gadolinium-based contrast agents (GBCAs) are commonly employed with T1w MRI to enhance lesion visualization but are restricted in patients at risk of nephrogenic systemic fibrosis and variations in GBCA administration can introduce imaging inconsistencies. This study develops an efficient 3D deep-learning framework to generate T1-contrast enhanced images (T1C) from pre-contrast multiparametric MRI. Approach: We propose the 3D latent rectified flow (T1C-RFlow) model for generating high-quality T1C images. First, T1w and T2-FLAIR images are input into a pretrained autoencoder to acquire an efficient latent space representation. A rectified flow diffusion model is then trained in this latent space representation. The T1C-RFlow model was trained on a curated dataset comprised of the BraTS 2024 glioma (GLI; 1480 patients), meningioma (MEN; 1141 patients), and metastases (MET; 1475 patients) datasets. Selected patients were split into train (N=2860), validation (N=612), and test (N=614) sets. Results: Both qualitative and quantitative results demonstrate that the T1C-RFlow model outperforms benchmark 3D models (pix2pix, DDPM, Diffusion Transformers (DiT-3D)) trained in the same latent space. T1C-RFlow achieved the following metrics - GLI: NMSE 0.044 +/- 0.047, SSIM 0.935 +/- 0.025; MEN: NMSE 0.046 +/- 0.029, SSIM 0.937 +/- 0.021; MET: NMSE 0.098 +/- 0.088, SSIM 0.905 +/- 0.082. T1C-RFlow had the best tumor reconstruction performance and significantly faster denoising times (6.9 s/volume, 200 steps) than conventional DDPM models in both latent space (37.7s, 1000 steps) and patch-based in image space (4.3 hr/volume). Significance: Our proposed method generates synthetic T1C images that closely resemble ground truth T1C in much less time than previous diffusion models. Further development may permit a practical method for contrast-agent-free MRI for brain tumors.

</details>


### [68] [Forge4D: Feed-Forward 4D Human Reconstruction and Interpolation from Uncalibrated Sparse-view Videos](https://arxiv.org/abs/2509.24209)
*Yingdong Hu,Yisheng He,Jinnan Chen,Weihao Yuan,Kejie Qiu,Zehong Lin,Siyu Zhu,Zilong Dong,Jun Zhang*

Main category: cs.CV

TL;DR: Forge4D是一个前馈4D人体重建和插值模型，能够从无标定稀疏视角视频中高效重建时间对齐的表示，支持新视角和新时间合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么重建速度慢，要么无法生成新时间表示。需要解决从无标定稀疏视角视频中实时重建动态3D人体的挑战。

Method: 将4D重建和插值问题简化为流式3D高斯重建和稠密运动预测的联合任务。使用可学习状态令牌保持时间一致性，设计运动预测模块预测相邻帧间的稠密运动，并通过遮挡感知高斯融合过程插值任意时间戳的3D高斯。

Result: 在域内和域外数据集上的广泛实验证明了模型的有效性。

Conclusion: Forge4D能够高效重建时间对齐的4D人体表示，支持新视角和新时间合成，解决了现有方法的局限性。

Abstract: Instant reconstruction of dynamic 3D humans from uncalibrated sparse-view videos is critical for numerous downstream applications. Existing methods, however, are either limited by the slow reconstruction speeds or incapable of generating novel-time representations. To address these challenges, we propose Forge4D, a feed-forward 4D human reconstruction and interpolation model that efficiently reconstructs temporally aligned representations from uncalibrated sparse-view videos, enabling both novel view and novel time synthesis. Our model simplifies the 4D reconstruction and interpolation problem as a joint task of streaming 3D Gaussian reconstruction and dense motion prediction. For the task of streaming 3D Gaussian reconstruction, we first reconstruct static 3D Gaussians from uncalibrated sparse-view images and then introduce learnable state tokens to enforce temporal consistency in a memory-friendly manner by interactively updating shared information across different timestamps. For novel time synthesis, we design a novel motion prediction module to predict dense motions for each 3D Gaussian between two adjacent frames, coupled with an occlusion-aware Gaussian fusion process to interpolate 3D Gaussians at arbitrary timestamps. To overcome the lack of the ground truth for dense motion supervision, we formulate dense motion prediction as a dense point matching task and introduce a self-supervised retargeting loss to optimize this module. An additional occlusion-aware optical flow loss is introduced to ensure motion consistency with plausible human movement, providing stronger regularization. Extensive experiments demonstrate the effectiveness of our model on both in-domain and out-of-domain datasets. Project page and code at: https://zhenliuzju.github.io/huyingdong/Forge4D.

</details>


### [69] [FreeAction: Training-Free Techniques for Enhanced Fidelity of Trajectory-to-Video Generation](https://arxiv.org/abs/2509.24241)
*Seungwook Kim,Seunghyeon Lee,Minsu Cho*

Main category: cs.CV

TL;DR: 提出了两种无需训练、在推理时利用显式动作参数的技术，通过动作缩放的无分类器引导和动作缩放的噪声截断，显著提升了基于扩散模型的机器人视频生成的动作一致性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 从显式动作轨迹生成逼真的机器人视频是构建有效世界模型和机器人基础模型的关键步骤，需要充分利用动作参数来增强生成视频的控制性和质量。

Method: 1. 动作缩放的无分类器引导：根据动作幅度动态调节引导强度；2. 动作缩放的噪声截断：调整初始采样噪声分布以更好地匹配期望运动动态。

Result: 在真实机器人操作数据集上的实验表明，这些技术显著提高了不同机器人环境中的动作一致性和视觉质量。

Conclusion: 通过主动利用显式动作参数来引导扩散过程，可以有效提升机器人视频生成的控制性和质量，为构建更好的世界模型和机器人基础模型提供了重要技术支撑。

Abstract: Generating realistic robot videos from explicit action trajectories is a critical step toward building effective world models and robotics foundation models. We introduce two training-free, inference-time techniques that fully exploit explicit action parameters in diffusion-based robot video generation. Instead of treating action vectors as passive conditioning signals, our methods actively incorporate them to guide both the classifier-free guidance process and the initialization of Gaussian latents. First, action-scaled classifier-free guidance dynamically modulates guidance strength in proportion to action magnitude, enhancing controllability over motion intensity. Second, action-scaled noise truncation adjusts the distribution of initially sampled noise to better align with the desired motion dynamics. Experiments on real robot manipulation datasets demonstrate that these techniques significantly improve action coherence and visual quality across diverse robot environments.

</details>


### [70] [Cycle Diffusion Model for Counterfactual Image Generation](https://arxiv.org/abs/2509.24267)
*Fangrui Huang,Alan Wang,Binxu Li,Bailey Trang,Ridvan Yesiloglu,Tianyu Hua,Wei Peng,Ehsan Adeli*

Main category: cs.CV

TL;DR: 提出Cycle Diffusion Model (CDM)，通过循环训练框架改进扩散模型，提高条件忠实度和合成图像质量，应用于医学图像生成。


<details>
  <summary>Details</summary>
Motivation: 当前深度生成模型在医学图像合成中取得了显著成功，但确保条件忠实度和高质量合成图像（特别是直接或反事实生成）仍然是一个挑战。

Method: 引入循环训练框架来微调扩散模型，通过加入循环约束强制生成图像与原始图像之间的一致性，实现更可靠的直接和反事实生成。

Result: 在组合的3D脑部MRI数据集上的实验表明，该方法提高了条件准确性，并通过FID和SSIM指标提升了图像质量。

Conclusion: CDM中使用的循环策略可以成为改进基于扩散的医学图像生成的有效方法，在数据增强、反事实和疾病进展建模中具有应用价值。

Abstract: Deep generative models have demonstrated remarkable success in medical image synthesis. However, ensuring conditioning faithfulness and high-quality synthetic images for direct or counterfactual generation remains a challenge. In this work, we introduce a cycle training framework to fine-tune diffusion models for improved conditioning adherence and enhanced synthetic image realism. Our approach, Cycle Diffusion Model (CDM), enforces consistency between generated and original images by incorporating cycle constraints, enabling more reliable direct and counterfactual generation. Experiments on a combined 3D brain MRI dataset (from ABCD, HCP aging & young adults, ADNI, and PPMI) show that our method improves conditioning accuracy and enhances image quality as measured by FID and SSIM. The results suggest that the cycle strategy used in CDM can be an effective method for refining diffusion-based medical image generation, with applications in data augmentation, counterfactual, and disease progression modeling.

</details>


### [71] [ASIA: Adaptive 3D Segmentation using Few Image Annotations](https://arxiv.org/abs/2509.24288)
*Sai Raj Kishore Perla,Aditya Vora,Sauradip Nag,Ali Mahdavi-Amiri,Hao Zhang*

Main category: cs.CV

TL;DR: ASIA是一个基于少量图像标注的3D分割框架，能够分割3D对象中的非语义和非文本可描述部分，通过利用文本到图像扩散模型的先验知识实现从图像空间到3D的分割迁移。


<details>
  <summary>Details</summary>
Motivation: 现有的3D分割方法需要多视角图像、3D模型标注或文本描述，这些方法要么收集困难，要么标注要求高，要么存在模糊性。ASIA旨在通过少量用户标注的野外图像来解决这些问题。

Method: 方法包括优化每个分割片段的文本token，使用新的跨视角部分对应损失进行微调，在推理时对3D网格的多视角渲染进行分割，通过投票在UV空间融合标签，使用噪声优化技术进行精炼，最后将UV标签映射回网格。

Result: ASIA在定量和定性评估中都显著优于现有方法，为语义和非语义3D分割任务提供了实用且可泛化的解决方案。

Conclusion: ASIA框架通过少量图像标注实现了高效的3D分割，特别是在处理几何或结构差异较大的对象时表现出色，为3D分割任务提供了新的可行方案。

Abstract: We introduce ASIA (Adaptive 3D Segmentation using few Image Annotations), a novel framework that enables segmentation of possibly non-semantic and non-text-describable "parts" in 3D. Our segmentation is controllable through a few user-annotated in-the-wild images, which are easier to collect than multi-view images, less demanding to annotate than 3D models, and more precise than potentially ambiguous text descriptions. Our method leverages the rich priors of text-to-image diffusion models, such as Stable Diffusion (SD), to transfer segmentations from image space to 3D, even when the annotated and target objects differ significantly in geometry or structure. During training, we optimize a text token for each segment and fine-tune our model with a novel cross-view part correspondence loss. At inference, we segment multi-view renderings of the 3D mesh, fuse the labels in UV-space via voting, refine them with our novel Noise Optimization technique, and finally map the UV-labels back onto the mesh. ASIA provides a practical and generalizable solution for both semantic and non-semantic 3D segmentation tasks, outperforming existing methods by a noticeable margin in both quantitative and qualitative evaluations.

</details>


### [72] [OMeGa: Joint Optimization of Explicit Meshes and Gaussian Splats for Robust Scene-Level Surface Reconstruction](https://arxiv.org/abs/2509.24308)
*Yuhang Cao,Haojun Yan,Danya Yao*

Main category: cs.CV

TL;DR: OMeGa是一个端到端框架，联合优化显式三角网格和2D高斯溅射，通过网格约束和法线监督提升室内纹理缺失区域的几何重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有神经渲染方法在纹理缺失的室内区域存在几何不准确问题，且网格提取与优化过程分离，无法利用网格几何指导溅射优化。

Method: 提出灵活的绑定策略，将高斯溅射的空间属性表达在网格框架中，保留纹理属性；集成网格约束和单目法线监督；采用启发式迭代网格细化策略。

Result: 在挑战性室内重建基准测试中达到最先进性能，将Chamfer-L1误差降低47.3%，同时保持竞争力的新视角渲染质量。

Conclusion: OMeGa有效解决了先前室内纹理缺失重建中的局限性，实现了更精确的几何重建。

Abstract: Neural rendering with Gaussian splatting has advanced novel view synthesis, and most methods reconstruct surfaces via post-hoc mesh extraction. However, existing methods suffer from two limitations: (i) inaccurate geometry in texture-less indoor regions, and (ii) the decoupling of mesh extraction from optimization, thereby missing the opportunity to leverage mesh geometry to guide splat optimization. In this paper, we present OMeGa, an end-to-end framework that jointly optimizes an explicit triangle mesh and 2D Gaussian splats via a flexible binding strategy, where spatial attributes of Gaussian Splats are expressed in the mesh frame and texture attributes are retained on splats. To further improve reconstruction accuracy, we integrate mesh constraints and monocular normal supervision into the optimization, thereby regularizing geometry learning. In addition, we propose a heuristic, iterative mesh-refinement strategy that splits high-error faces and prunes unreliable ones to further improve the detail and accuracy of the reconstructed mesh. OMeGa achieves state-of-the-art performance on challenging indoor reconstruction benchmarks, reducing Chamfer-$L_1$ by 47.3\% over the 2DGS baseline while maintaining competitive novel-view rendering quality. The experimental results demonstrate that OMeGa effectively addresses prior limitations in indoor texture-less reconstruction.

</details>


### [73] [Hyperspherical Latents Improve Continuous-Token Autoregressive Generation](https://arxiv.org/abs/2509.24335)
*Guolin Ke,Hui Xue*

Main category: cs.CV

TL;DR: SphereAR通过将自回归模型的输入输出约束在固定半径的超球面上，解决了VAE潜在空间中异质方差导致的方差崩溃问题，在图像生成任务上超越了扩散和掩码生成模型。


<details>
  <summary>Details</summary>
Motivation: 连续token自回归模型在图像生成中通常落后于潜在扩散和掩码生成模型，主要问题是VAE潜在空间中的异质方差在自回归解码过程中被放大，特别是在分类器无关引导下会导致方差崩溃。

Method: 提出SphereAR方法，核心设计是将所有自回归输入输出（包括CFG后）约束在固定半径的超球面上（恒定ℓ2范数），利用超球面VAE。理论分析表明超球面约束移除了尺度分量（方差崩溃的主要原因），从而稳定自回归解码。

Result: 在ImageNet生成任务上，SphereAR-H（943M）创下了自回归模型的新记录，FID达到1.34。更小规模的SphereAR-L（479M）达到FID 1.54，SphereAR-B（208M）达到1.92，匹配或超越了更大的基线模型。

Conclusion: 这是首次纯next-token自回归图像生成器（光栅顺序）在可比参数规模下超越扩散和掩码生成模型，证明了超球面约束在稳定自回归解码中的有效性。

Abstract: Autoregressive (AR) models are promising for image generation, yet continuous-token AR variants often trail latent diffusion and masked-generation models. The core issue is heterogeneous variance in VAE latents, which is amplified during AR decoding, especially under classifier-free guidance (CFG), and can cause variance collapse. We propose SphereAR to address this issue. Its core design is to constrain all AR inputs and outputs -- including after CFG -- to lie on a fixed-radius hypersphere (constant $\ell_2$ norm), leveraging hyperspherical VAEs. Our theoretical analysis shows that hyperspherical constraint removes the scale component (the primary cause of variance collapse), thereby stabilizing AR decoding. Empirically, on ImageNet generation, SphereAR-H (943M) sets a new state of the art for AR models, achieving FID 1.34. Even at smaller scales, SphereAR-L (479M) reaches FID 1.54 and SphereAR-B (208M) reaches 1.92, matching or surpassing much larger baselines such as MAR-H (943M, 1.55) and VAR-d30 (2B, 1.92). To our knowledge, this is the first time a pure next-token AR image generator with raster order surpasses diffusion and masked-generation models at comparable parameter scales.

</details>


### [74] [NeRV-Diffusion: Diffuse Implicit Neural Representations for Video Synthesis](https://arxiv.org/abs/2509.24353)
*Yixuan Ren,Hanyu Wang,Hao Chen,Bo He,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: NeRV-Diffusion是一种隐式潜在视频扩散模型，通过生成神经网络权重来合成视频。该框架包含两个阶段：基于超网络的标记器将视频编码为神经参数空间，以及隐式扩散变换器在潜在INR权重上进行去噪。


<details>
  <summary>Details</summary>
Motivation: 传统视频标记器将视频编码为逐帧特征图，而NeRV-Diffusion将视频整体压缩并生成为一个统一的神经网络，避免了去噪器中的时间跨帧注意力，实现高效高质量的视频合成。

Method: 1) 基于超网络的标记器将原始视频从像素空间编码到神经参数空间；2) 隐式扩散变换器在潜在INR权重上进行去噪；3) 重用瓶颈潜在跨所有NeRV层，重新设计权重分配、上采样连接和输入坐标；4) 引入SNR自适应损失加权和计划采样。

Result: 在UCF-101和Kinetics-600等真实世界视频基准测试中，NeRV-Diffusion在视频生成质量上优于之前的INR模型，与最新非隐式模型性能相当，并提供了平滑的INR权重空间，便于帧间或视频间的无缝插值。

Conclusion: NeRV-Diffusion通过将视频表示为统一的神经网络，实现了高效高质量的视频合成，在隐式神经表示和扩散模型之间建立了有效连接，为视频生成提供了新的范式。

Abstract: We present NeRV-Diffusion, an implicit latent video diffusion model that synthesizes videos via generating neural network weights. The generated weights can be rearranged as the parameters of a convolutional neural network, which forms an implicit neural representation (INR), and decodes into videos with frame indices as the input. Our framework consists of two stages: 1) A hypernetworkbased tokenizer that encodes raw videos from pixel space to neural parameter space, where the bottleneck latent serves as INR weights to decode. 2) An implicit diffusion transformer that denoises on the latent INR weights. In contrast to traditional video tokenizers that encode videos into frame-wise feature maps, NeRV-Diffusion compresses and generates a video holistically as a unified neural network. This enables efficient and high-quality video synthesis via obviating temporal cross-frame attentions in the denoiser and decoding video latent with dedicated decoders. To achieve Gaussian-distributed INR weights with high expressiveness, we reuse the bottleneck latent across all NeRV layers, as well as reform its weight assignment, upsampling connection and input coordinates. We also introduce SNR-adaptive loss weighting and scheduled sampling for effective training of the implicit diffusion model. NeRV-Diffusion reaches superior video generation quality over previous INR-based models and comparable performance to most recent state-of-the-art non-implicit models on real-world video benchmarks including UCF-101 and Kinetics-600. It also brings a smooth INR weight space that facilitates seamless interpolations between frames or videos.

</details>


### [75] [Uni-X: Mitigating Modality Conflict with a Two-End-Separated Architecture for Unified Multimodal Models](https://arxiv.org/abs/2509.24365)
*Jitai Hao,Hao Liu,Xinyan Xiao,Qiang Huang,Jun Yu*

Main category: cs.CV

TL;DR: Uni-X提出了一种两端分离、中间共享的X形架构，通过将初始和最终层用于模态特定处理，中间层共享参数进行高级语义融合，解决了统一多模态模型中视觉和文本之间的梯度冲突问题。


<details>
  <summary>Details</summary>
Motivation: 基于共享自回归变换器的统一多模态模型存在严重梯度冲突问题，特别是在浅层和深层，这是由于图像和文本在低层统计特性上的根本差异造成的。

Method: Uni-X采用两端分离、中间共享的X形架构：初始和最终层专门用于模态特定处理，中间层保持共享参数进行高级语义融合。

Result: 在相同训练条件下，Uni-X实现了更优的训练效率。当扩展到30亿参数并使用更大训练数据时，Uni-X匹配或超越了70亿参数的AR基UMMs，在图像生成方面获得82的GenEval分数，同时在文本和视觉理解任务中表现强劲。

Conclusion: Uni-X为未来统一多模态建模提供了一个参数高效且可扩展的基础架构，有效解决了梯度冲突问题。

Abstract: Unified Multimodal Models (UMMs) built on shared autoregressive (AR) transformers are attractive for their architectural simplicity. However, we identify a critical limitation: when trained on multimodal inputs, modality-shared transformers suffer from severe gradient conflicts between vision and text, particularly in shallow and deep layers. We trace this issue to the fundamentally different low-level statistical properties of images and text, while noting that conflicts diminish in middle layers where representations become more abstract and semantically aligned. To overcome this challenge, we propose Uni-X, a two-end-separated, middle-shared architecture. Uni-X dedicates its initial and final layers to modality-specific processing, while maintaining shared parameters in the middle layers for high-level semantic fusion. This X-shaped design not only eliminates gradient conflicts at both ends but also further alleviates residual conflicts in the shared layers. Extensive experiments validate the effectiveness of Uni-X. Under identical training conditions, Uni-X achieves superior training efficiency compared to strong baselines. When scaled to 3B parameters with larger training data, Uni-X matches or surpasses 7B AR-based UMMs, achieving a GenEval score of 82 for image generation alongside strong performance in text and vision understanding tasks. These results establish Uni-X as a parameter-efficient and scalable foundation for future unified multimodal modeling. Our code is available at https://github.com/CURRENTF/Uni-X

</details>


### [76] [From Satellite to Street: A Hybrid Framework Integrating Stable Diffusion and PanoGAN for Consistent Cross-View Synthesis](https://arxiv.org/abs/2509.24369)
*Khawlah Bajbaa,Abbas Anwar,Muhammad Saqib,Hafeez Anwar,Nabin Sharma,Muhammad Usman*

Main category: cs.CV

TL;DR: 提出了一种结合扩散模型和条件GAN的混合框架，用于从卫星图像生成地理一致性的街景图像，在CVUSA数据集上表现优于纯扩散方法，与最先进的GAN方法竞争。


<details>
  <summary>Details</summary>
Motivation: 街景图像是重要的地理空间数据源，但从卫星图像合成街景面临外观和视角差异的挑战，需要生成地理一致且视觉质量高的街景图像。

Method: 采用多阶段训练策略，以Stable Diffusion为核心构建双分支架构，集成条件GAN生成全景街景，并通过融合策略结合两种模型的优势。

Result: 在CVUSA数据集上的实验表明，该混合方法在多个评估指标上优于纯扩散方法，与最先进的GAN方法表现相当，能生成真实且几何一致的街景图像。

Conclusion: 提出的混合框架成功解决了跨视角图像合成的挑战，生成了具有良好几何一致性和视觉质量的街景图像，保留了街道标记、次要道路和大气元素等细节。

Abstract: Street view imagery has become an essential source for geospatial data collection and urban analytics, enabling the extraction of valuable insights that support informed decision-making. However, synthesizing street-view images from corresponding satellite imagery presents significant challenges due to substantial differences in appearance and viewing perspective between these two domains. This paper presents a hybrid framework that integrates diffusion-based models and conditional generative adversarial networks to generate geographically consistent street-view images from satellite imagery. Our approach uses a multi-stage training strategy that incorporates Stable Diffusion as the core component within a dual-branch architecture. To enhance the framework's capabilities, we integrate a conditional Generative Adversarial Network (GAN) that enables the generation of geographically consistent panoramic street views. Furthermore, we implement a fusion strategy that leverages the strengths of both models to create robust representations, thereby improving the geometric consistency and visual quality of the generated street-view images. The proposed framework is evaluated on the challenging Cross-View USA (CVUSA) dataset, a standard benchmark for cross-view image synthesis. Experimental results demonstrate that our hybrid approach outperforms diffusion-only methods across multiple evaluation metrics and achieves competitive performance compared to state-of-the-art GAN-based methods. The framework successfully generates realistic and geometrically consistent street-view images while preserving fine-grained local details, including street markings, secondary roads, and atmospheric elements such as clouds.

</details>


### [77] [CLQ: Cross-Layer Guided Orthogonal-based Quantization for Diffusion Transformers](https://arxiv.org/abs/2509.24416)
*Kai Liu,Shaoqiu Zhang,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: CLQ是一种针对扩散变换器(DiTs)的跨层引导正交量化方法，通过跨块校准、正交平滑和跨层参数搜索三个关键技术，实现W4A4量化，在保持视觉质量的同时获得3.98倍内存节省和3.95倍加速。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器(DiTs)虽然提升了视觉生成质量，但模型规模和复杂度的增长阻碍了其在边缘设备上的实际部署。模型后训练量化(PTQ)作为高效压缩技术可以减少内存消耗和加速推理，但会带来性能下降。

Method: 1. 跨块校准(CBC)：获取准确的校准数据以指导量化；2. 正交平滑(OBS)：量化每个通道的异常值分数，利用块Hadamard矩阵平滑异常值；3. 跨层参数搜索(CLPS)：搜索最优参数配置。

Result: 成功将模型压缩为W4A4格式，在视觉质量和指标上仅有可忽略的下降。实现了3.98倍内存节省和3.95倍加速。

Conclusion: CLQ方法有效解决了DiTs量化中的性能下降问题，为边缘设备部署提供了可行的解决方案，在图像和视频生成模型上都取得了良好效果。

Abstract: Visual generation quality has been greatly promoted with the rapid advances in diffusion transformers (DiTs), which is attributed to the scaling of model size and complexity. However, these attributions also hinder the practical deployment of DiTs on edge devices, limiting their development and application. Serve as an efficient model compression technique, model post-training quantization (PTQ) can reduce the memory consumption and speed up the inference, with inevitable performance degradation. To alleviate the degradation, we propose CLQ, a cross-layer guided orthogonal-based quantization method for DiTs. To be specific, CLQ consists of three key designs. First, we observe that the calibration data used by most of the PTQ methods can not honestly represent the distribution of the activations. Therefore, we propose cross-block calibration (CBC) to obtain accurate calibration data, with which the quantization can be better guided. Second, we propose orthogonal-based smoothing (OBS), which quantifies the outlier score of each channel and leverages block Hadamard matrix to smooth the outliers with negligible overhead. Third, we propose cross-layer parameter searching (CLPS) to search. We evaluate CLQ with both image generation and video generation models and successfully compress the model into W4A4 with negligible degradation in visual quality and metrics. CLQ achieves 3.98x memory saving and 3.95x speedup. Our code is available at \hyperlink{https://github.com/Kai-Liu001/CLQ}{https://github.com/Kai-Liu001/CLQ}.

</details>


### [78] [Proxy-GS: Efficient 3D Gaussian Splatting via Proxy Mesh](https://arxiv.org/abs/2509.24421)
*Yuanyuan Gao,Yuning Gong,Yifei Liu,Li Jingfeng,Zhihang Zhong,Dingwen Zhang,Yanci Zhang,Dan Xu,Xiao Sun*

Main category: cs.CV

TL;DR: Proxy-GS是一种新颖的3D高斯溅射方法，通过引入代理系统实现遮挡感知，在保持高质量渲染的同时显著提升渲染速度。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯溅射方法在大型场景中存在显著冗余，主要原因是缺乏遮挡感知能力，导致渲染效率低下。

Method: 使用快速代理系统生成精确遮挡深度图，指导锚点和高斯函数的剔除，并在训练过程中引导密度化过程。

Result: 在MatrixCity Streets等遮挡严重场景中，Proxy-GS不仅提升了MLP基高斯溅射的渲染能力，还实现了超过2.5倍的渲染加速。

Conclusion: Proxy-GS通过遮挡感知有效解决了3D高斯溅射中的冗余问题，在保持高质量的同时显著提升渲染效率。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as an efficient approach for achieving photorealistic rendering. Recent MLP-based variants further improve visual fidelity but introduce substantial decoding overhead during rendering. To alleviate computation cost, several pruning strategies and level-of-detail (LOD) techniques have been introduced, aiming to effectively reduce the number of Gaussian primitives in large-scale scenes. However, our analysis reveals that significant redundancy still remains due to the lack of occlusion awareness. In this work, we propose Proxy-GS, a novel pipeline that exploits a proxy to introduce Gaussian occlusion awareness from any view. At the core of our approach is a fast proxy system capable of producing precise occlusion depth maps at a resolution of 1000x1000 under 1ms. This proxy serves two roles: first, it guides the culling of anchors and Gaussians to accelerate rendering speed. Second, it guides the densification towards surfaces during training, avoiding inconsistencies in occluded regions, and improving the rendering quality. In heavily occluded scenarios, such as the MatrixCity Streets dataset, Proxy-GS not only equips MLP-based Gaussian splatting with stronger rendering capability but also achieves faster rendering speed. Specifically, it achieves more than 2.5x speedup over Octree-GS, and consistently delivers substantially higher rendering quality. Code will be public upon acceptance.

</details>


### [79] [UI2V-Bench: An Understanding-based Image-to-video Generation Benchmark](https://arxiv.org/abs/2509.24427)
*Ailing Zhang,Lina Lei,Dehong Kong,Zhixin Wang,Jiaqi Xu,Fenglong Song,Chun-Le Guo,Chang Liu,Fan Li,Jie Chen*

Main category: cs.CV

TL;DR: 提出了UI2V-Bench基准，专注于评估图像到视频生成模型的语义理解和推理能力，填补现有评估主要关注视频质量和时序一致性的不足。


<details>
  <summary>Details</summary>
Motivation: 现有图像到视频生成评估基准主要关注视频质量和时序一致性，而忽视了模型对输入图像特定主体语义的理解能力，以及生成视频是否符合物理规律和人类常识。

Method: 设计了四个主要评估维度：空间理解、属性绑定、类别理解和推理；开发了基于多模态大语言模型的两种评估方法：实例级细粒度语义理解流水线和基于反馈的推理流水线；包含约500个精心构建的文本-图像对。

Result: 评估了多个开源和闭源的图像到视频生成模型，发现人类评估与提出的基于多模态大语言模型的指标高度一致。

Conclusion: UI2V-Bench通过强调语义理解和推理能力，填补了图像到视频生成评估的关键空白，为该领域未来研究和模型开发提供了稳健的框架和数据集。

Abstract: Generative diffusion models are developing rapidly and attracting increasing attention due to their wide range of applications. Image-to-Video (I2V) generation has become a major focus in the field of video synthesis. However, existing evaluation benchmarks primarily focus on aspects such as video quality and temporal consistency, while largely overlooking the model's ability to understand the semantics of specific subjects in the input image or to ensure that the generated video aligns with physical laws and human commonsense. To address this gap, we propose UI2V-Bench, a novel benchmark for evaluating I2V models with a focus on semantic understanding and reasoning. It introduces four primary evaluation dimensions: spatial understanding, attribute binding, category understanding, and reasoning. To assess these dimensions, we design two evaluation methods based on Multimodal Large Language Models (MLLMs): an instance-level pipeline for fine-grained semantic understanding, and a feedback-based reasoning pipeline that enables step-by-step causal assessment for more accurate evaluation. UI2V-Bench includes approximately 500 carefully constructed text-image pairs and evaluates a range of both open source and closed-source I2V models across all defined dimensions. We further incorporate human evaluations, which show strong alignment with the proposed MLLM-based metrics. Overall, UI2V-Bench fills a critical gap in I2V evaluation by emphasizing semantic comprehension and reasoning ability, offering a robust framework and dataset to support future research and model development in the field.

</details>


### [80] [CMT: Mid-Training for Efficient Learning of Consistency, Mean Flow, and Flow Map Models](https://arxiv.org/abs/2509.24526)
*Zheyuan Hu,Chieh-Hsin Lai,Yuki Mitsufuji,Stefano Ermon*

Main category: cs.CV

TL;DR: 本文提出了Consistency Mid-Training (CMT)方法，在扩散预训练和流映射训练之间插入轻量级中间阶段，显著提高训练稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有流映射模型如一致性模型和均值流模型虽然能实现少步生成，但训练不稳定、对超参数敏感且成本高昂，即使从预训练扩散模型初始化也无法完全解决不稳定性问题。

Method: 提出中间训练概念，在扩散预训练和最终流映射训练之间插入轻量级阶段CMT，训练模型将预训练模型求解器轨迹上的点直接映射到求解器生成的干净样本。

Result: CMT在多个数据集上取得最先进的2步FID：CIFAR-10为1.97，ImageNet 64x64为1.32，ImageNet 512x512为1.84，相比一致性模型减少98%训练数据和GPU时间。

Conclusion: CMT为训练流映射模型提供了一个原则性、高效且通用的框架，显著提高了训练稳定性和效率。

Abstract: Flow map models such as Consistency Models (CM) and Mean Flow (MF) enable few-step generation by learning the long jump of the ODE solution of diffusion models, yet training remains unstable, sensitive to hyperparameters, and costly. Initializing from a pre-trained diffusion model helps, but still requires converting infinitesimal steps into a long-jump map, leaving instability unresolved. We introduce mid-training, the first concept and practical method that inserts a lightweight intermediate stage between the (diffusion) pre-training and the final flow map training (i.e., post-training) for vision generation. Concretely, Consistency Mid-Training (CMT) is a compact and principled stage that trains a model to map points along a solver trajectory from a pre-trained model, starting from a prior sample, directly to the solver-generated clean sample. It yields a trajectory-consistent and stable initialization. This initializer outperforms random and diffusion-based baselines and enables fast, robust convergence without heuristics. Initializing post-training with CMT weights further simplifies flow map learning. Empirically, CMT achieves state of the art two step FIDs: 1.97 on CIFAR-10, 1.32 on ImageNet 64x64, and 1.84 on ImageNet 512x512, while using up to 98% less training data and GPU time, compared to CMs. On ImageNet 256x256, CMT reaches 1-step FID 3.34 while cutting total training time by about 50% compared to MF from scratch (FID 3.43). This establishes CMT as a principled, efficient, and general framework for training flow map models.

</details>


### [81] [Diffusion Bridge or Flow Matching? A Unifying Framework and Comparative Analysis](https://arxiv.org/abs/2509.24531)
*Kaizhen Zhu,Mokai Pan,Zhechuan Yu,Jingya Wang,Jingyi Yu,Ye Shi*

Main category: cs.CV

TL;DR: 本文首次对Diffusion Bridge和Flow Matching两种模型进行了统一的理论和实验验证，通过随机最优控制和最优传输理论分析了它们的优缺点，并设计了公平的实验比较。


<details>
  <summary>Details</summary>
Motivation: 目前对于Diffusion Bridge和Flow Matching哪种方法更优存在困惑，两种方法在建模假设和实现上存在显著差异，缺乏统一的理论分析框架来比较它们的相对优势。

Method: 通过随机最优控制理论重新构建两种模型框架，证明Diffusion Bridge的成本函数更低；从最优传输角度分析Flow Matching插值系数的局限性；设计基于潜在Transformer的新架构进行公平比较实验。

Result: 理论分析表明Diffusion Bridge能引导系统走向更稳定自然的轨迹，而Flow Matching在训练数据减少时插值系数效果下降。实验验证了理论预测，在图像修复、超分辨率、去模糊、去噪、转换和风格迁移等任务中系统比较了两种方法。

Conclusion: 研究明确了两种模型各自的优缺点，为选择合适的方法提供了理论指导和实验依据，代码已开源。

Abstract: Diffusion Bridge and Flow Matching have both demonstrated compelling empirical performance in transformation between arbitrary distributions. However, there remains confusion about which approach is generally preferable, and the substantial discrepancies in their modeling assumptions and practical implementations have hindered a unified theoretical account of their relative merits. We have, for the first time, provided a unified theoretical and experimental validation of these two models. We recast their frameworks through the lens of Stochastic Optimal Control and prove that the cost function of the Diffusion Bridge is lower, guiding the system toward more stable and natural trajectories. Simultaneously, from the perspective of Optimal Transport, interpolation coefficients $t$ and $1-t$ of Flow Matching become increasingly ineffective when the training data size is reduced. To corroborate these theoretical claims, we propose a novel, powerful architecture for Diffusion Bridge built on a latent Transformer, and implement a Flow Matching model with the same structure to enable a fair performance comparison in various experiments. Comprehensive experiments are conducted across Image Inpainting, Super-Resolution, Deblurring, Denoising, Translation, and Style Transfer tasks, systematically varying both the distributional discrepancy (different difficulty) and the training data size. Extensive empirical results align perfectly with our theoretical predictions and allow us to delineate the respective advantages and disadvantages of these two models. Our code is available at https://anonymous.4open.science/r/DBFM-3E8E/.

</details>


### [82] [Learning Object-Centric Representations Based on Slots in Real World Scenarios](https://arxiv.org/abs/2509.24652)
*Adil Kaan Akan*

Main category: cs.CV

TL;DR: 该论文提出了SlotAdapt框架，将预训练扩散模型适配为面向对象的合成方法，在保持生成能力的同时实现细粒度对象控制，适用于图像和视频的生成与编辑。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散模型以整体方式处理图像、依赖文本条件的问题，实现更符合人类对象感知的细粒度可控生成与编辑。

Method: 集成轻量级的基于槽的条件机制到预训练模型中，使用寄存器令牌处理背景/风格，槽条件模块处理对象；视频中采用不变槽注意力和基于Transformer的时间聚合器。

Result: 在对象发现、分割、组合编辑和可控图像生成方面达到最先进水平；在无监督视频对象分割和重建方面建立新基准，支持对象移除、替换和插入等高级编辑任务。

Conclusion: 建立了一种通用且可扩展的面向对象生成建模方法，弥合了人类对象感知与机器学习之间的差距，为创意、科学和实际领域的交互式结构化生成工具扩展了设计空间。

Abstract: A central goal in AI is to represent scenes as compositions of discrete objects, enabling fine-grained, controllable image and video generation. Yet leading diffusion models treat images holistically and rely on text conditioning, creating a mismatch for object-level editing. This thesis introduces a framework that adapts powerful pretrained diffusion models for object-centric synthesis while retaining their generative capacity.   We identify a core challenge: balancing global scene coherence with disentangled object control. Our method integrates lightweight, slot-based conditioning into pretrained models, preserving their visual priors while providing object-specific manipulation. For images, SlotAdapt augments diffusion models with a register token for background/style and slot-conditioned modules for objects, reducing text-conditioning bias and achieving state-of-the-art results in object discovery, segmentation, compositional editing, and controllable image generation.   We further extend the framework to video. Using Invariant Slot Attention (ISA) to separate object identity from pose and a Transformer-based temporal aggregator, our approach maintains consistent object representations and dynamics across frames. This yields new benchmarks in unsupervised video object segmentation and reconstruction, and supports advanced editing tasks such as object removal, replacement, and insertion without explicit supervision.   Overall, this work establishes a general and scalable approach to object-centric generative modeling for images and videos. By bridging human object-based perception and machine learning, it expands the design space for interactive, structured, and user-driven generative tools in creative, scientific, and practical domains.

</details>


### [83] [SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer](https://arxiv.org/abs/2509.24695)
*Junsong Chen,Yuyang Zhao,Jincheng Yu,Ruihang Chu,Junyu Chen,Shuai Yang,Xianbang Wang,Yicheng Pan,Daquan Zhou,Huan Ling,Haozhe Liu,Hongwei Yi,Hao Zhang,Muyang Li,Yukang Chen,Han Cai,Sanja Fidler,Ping Luo,Song Han,Enze Xie*

Main category: cs.CV

TL;DR: SANA-Video是一个小型扩散模型，能够高效生成720x1280分辨率、分钟长度的视频，具有快速生成速度，可在RTX 5090 GPU上部署。


<details>
  <summary>Details</summary>
Motivation: 解决现有视频生成模型计算成本高、生成速度慢的问题，实现低成本、高质量的长视频生成。

Method: 采用线性注意力机制（Linear DiT）和恒定内存KV缓存技术，结合有效的数据过滤和训练策略。

Result: 相比现代最先进的小型扩散模型，性能相当但速度快16倍，训练成本仅为MovieGen的1%，在RTX 5090上生成5秒720p视频从71秒加速到29秒。

Conclusion: SANA-Video实现了低成本、高质量的视频生成，为实际部署提供了可行的解决方案。

Abstract: We introduce SANA-Video, a small diffusion model that can efficiently generate videos up to 720x1280 resolution and minute-length duration. SANA-Video synthesizes high-resolution, high-quality and long videos with strong text-video alignment at a remarkably fast speed, deployable on RTX 5090 GPU. Two core designs ensure our efficient, effective and long video generation: (1) Linear DiT: We leverage linear attention as the core operation, which is more efficient than vanilla attention given the large number of tokens processed in video generation. (2) Constant-Memory KV cache for Block Linear Attention: we design block-wise autoregressive approach for long video generation by employing a constant-memory state, derived from the cumulative properties of linear attention. This KV cache provides the Linear DiT with global context at a fixed memory cost, eliminating the need for a traditional KV cache and enabling efficient, minute-long video generation. In addition, we explore effective data filters and model training strategies, narrowing the training cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of MovieGen. Given its low cost, SANA-Video achieves competitive performance compared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B and SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover, SANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating the inference speed of generating a 5-second 720p video from 71s to 29s (2.4x speedup). In summary, SANA-Video enables low-cost, high-quality video generation.

</details>


### [84] [Enhancing Physical Plausibility in Video Generation by Reasoning the Implausibility](https://arxiv.org/abs/2509.24702)
*Yutong Hao,Chen Chen,Ajmal Saeed Mian,Chang Xu,Daochang Liu*

Main category: cs.CV

TL;DR: 提出了一种无需训练的视频生成框架，通过物理感知推理和同步解耦引导策略，在推理时显式抑制违反物理规律的内容，提高生成视频的物理合理性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型通过大规模文本-视频数据集隐式学习物理推理，成本高、难以扩展，且容易产生违反基本物理定律的不合理运动。

Method: 使用轻量级物理感知推理管道构建反事实提示，编码物理违规行为；提出同步解耦引导策略，包括同步方向归一化对抗滞后抑制和轨迹解耦去噪缓解累积轨迹偏差。

Result: 在不同物理领域的实验中，该方法显著提高了物理保真度，同时保持照片级真实感，且无需额外训练。消融研究证实了物理感知推理组件和SDG的互补有效性。

Conclusion: 建立了一种新的即插即用物理感知视频生成范式，能够在推理时有效提升生成视频的物理合理性。

Abstract: Diffusion models can generate realistic videos, but existing methods rely on implicitly learning physical reasoning from large-scale text-video datasets, which is costly, difficult to scale, and still prone to producing implausible motions that violate fundamental physical laws. We introduce a training-free framework that improves physical plausibility at inference time by explicitly reasoning about implausibility and guiding the generation away from it. Specifically, we employ a lightweight physics-aware reasoning pipeline to construct counterfactual prompts that deliberately encode physics-violating behaviors. Then, we propose a novel Synchronized Decoupled Guidance (SDG) strategy, which leverages these prompts through synchronized directional normalization to counteract lagged suppression and trajectory-decoupled denoising to mitigate cumulative trajectory bias, ensuring that implausible content is suppressed immediately and consistently throughout denoising. Experiments across different physical domains show that our approach substantially enhances physical fidelity while maintaining photorealism, despite requiring no additional training. Ablation studies confirm the complementary effectiveness of both the physics-aware reasoning component and SDG. In particular, the aforementioned two designs of SDG are also individually validated to contribute critically to the suppression of implausible content and the overall gains in physical plausibility. This establishes a new and plug-and-play physics-aware paradigm for video generation.

</details>


### [85] [ExGS: Extreme 3D Gaussian Compression with Diffusion Priors](https://arxiv.org/abs/2509.24758)
*Jiaqi Chen,Xinhao Ji,Yuanyuan Gao,Hao Li,Yuning Gong,Yifei Liu,Dan Xu,Zhihang Zhong,Dingwen Zhang,Xiao Sun*

Main category: cs.CV

TL;DR: ExGS是一个统一通用高斯压缩(UGC)和GaussPainter的前馈框架，用于实现极端3DGS压缩，可在保持渲染质量的同时实现超过100倍的压缩比。


<details>
  <summary>Details</summary>
Motivation: 神经场景表示(如3DGS)虽然能实现高质量神经渲染，但其巨大的存储和传输成本阻碍了在资源受限环境中的部署。现有压缩方法要么依赖昂贵的优化，要么在高质量压缩下会降低渲染质量。

Method: 结合UGC进行无重优化剪枝来大幅减少高斯基元，同时使用GaussPainter利用扩散先验和掩码引导细化来从严重剪枝的高斯场景中恢复高质量渲染。GaussPainter不仅填充缺失区域，还增强可见像素。

Result: 该框架可实现超过100倍的压缩(将典型的354.77MB模型压缩至约3.31MB)，同时保持保真度，并在挑战性条件下显著提升图像质量。

Conclusion: 扩散先验在弥合极端压缩和高质量神经渲染之间的差距中发挥着核心作用。

Abstract: Neural scene representations, such as 3D Gaussian Splatting (3DGS), have enabled high-quality neural rendering; however, their large storage and transmission costs hinder deployment in resource-constrained environments. Existing compression methods either rely on costly optimization, which is slow and scene-specific, or adopt training-free pruning and quantization, which degrade rendering quality under high compression ratios. In contrast, recent data-driven approaches provide a promising direction to overcome this trade-off, enabling efficient compression while preserving high rendering quality. We introduce \textbf{ExGS}, a novel feed-forward framework that unifies \textbf{Universal Gaussian Compression} (UGC) with \textbf{GaussPainter} for \textbf{Ex}treme 3D\textbf{GS} compression. \textbf{UGC} performs re-optimization-free pruning to aggressively reduce Gaussian primitives while retaining only essential information, whereas \textbf{GaussPainter} leverages powerful diffusion priors with mask-guided refinement to restore high-quality renderings from heavily pruned Gaussian scenes. Unlike conventional inpainting, GaussPainter not only fills in missing regions but also enhances visible pixels, yielding substantial improvements in degraded renderings. To ensure practicality, it adopts a lightweight VAE and a one-step diffusion design, enabling real-time restoration. Our framework can even achieve over $100\times$ compression (reducing a typical 354.77 MB model to about 3.31 MB) while preserving fidelity and significantly improving image quality under challenging conditions. These results highlight the central role of diffusion priors in bridging the gap between extreme compression and high-quality neural rendering. Our code repository will be released at \href{https://github.com/chenttt2001/ExGS}{here}.

</details>


### [86] [Vision Function Layer in Multimodal LLMs](https://arxiv.org/abs/2509.24791)
*Cheng Shi,Yizhou Yu,Sibei Yang*

Main category: cs.CV

TL;DR: 研究发现多模态大语言模型（MLLMs）中的视觉相关功能解码分布在不同的解码器层，每个功能（如计数、定位、OCR识别）集中在2-3个特定层（视觉功能层VFL），且不同VFL的深度顺序在不同MLLMs中呈现一致模式。


<details>
  <summary>Details</summary>
Motivation: 理解MLLMs中视觉处理的具体机制，探索视觉功能在不同解码层的分布规律，为模型优化和应用提供理论基础。

Method: 提出视觉令牌交换（Visual Token Swapping）分析框架，通过修改特定KV缓存条目来精确揭示解码过程中的层特定功能。

Result: 识别出视觉功能层（VFL），发现其深度顺序与人类行为一致（识别→计数→定位）。基于VFL的LoRA训练优于全参数训练，且VFL-select数据选择方法仅用20%数据即可达到98%全数据性能。

Conclusion: 该研究深化了对MLLM视觉处理机制的理解，为开发更高效、可解释和鲁棒的模型提供了新途径。

Abstract: This study identifies that visual-related functional decoding is distributed across different decoder layers in Multimodal Large Language Models (MLLMs). Typically, each function, such as counting, grounding, or OCR recognition, narrows down to two or three layers, which we define as Vision Function Layers (VFL). Additionally, the depth and its order of different VFLs exhibits a consistent pattern across different MLLMs, which is well-aligned with human behaviors (e.g., recognition occurs first, followed by counting, and then grounding). These findings are derived from Visual Token Swapping, our novel analytical framework that modifies targeted KV cache entries to precisely elucidate layer-specific functions during decoding. Furthermore, these insights offer substantial utility in tailoring MLLMs for real-world downstream applications. For instance, when LoRA training is selectively applied to VFLs whose functions align with the training data, VFL-LoRA not only outperform full-LoRA but also prevent out-of-domain function forgetting. Moreover, by analyzing the performance differential on training data when particular VFLs are ablated, VFL-select automatically classifies data by function, enabling highly efficient data selection to directly bolster corresponding capabilities. Consequently, VFL-select surpasses human experts in data selection, and achieves 98% of full-data performance with only 20% of the original dataset. This study delivers deeper comprehension of MLLM visual processing, fostering the creation of more efficient, interpretable, and robust models.

</details>


### [87] [Causal-Adapter: Taming Text-to-Image Diffusion for Faithful Counterfactual Generation](https://arxiv.org/abs/2509.24798)
*Lei Tong,Zhihua Liu,Chaochao Lu,Dino Oglic,Tom Diethe,Philip Teare,Sotirios A. Tsaftaris,Chen Jin*

Main category: cs.CV

TL;DR: Causal-Adapter是一个模块化框架，通过适配冻结的文本到图像扩散模型实现反事实图像生成，利用结构因果建模和属性正则化策略实现精确的属性干预和身份保持。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖提示工程而缺乏明确的因果结构，无法一致地将目标属性的因果干预效果传播到因果依赖项，同时保持图像核心身份不变。

Method: 采用结构因果建模，结合两种属性正则化策略：提示对齐注入（对齐因果属性与文本嵌入）和条件标记对比损失（解耦属性因子并减少虚假相关性）。

Result: 在合成和真实数据集上达到最先进性能，Pendulum数据集MAE减少91%，ADNI数据集FID减少87%，实现了高保真MRI图像生成。

Conclusion: 该方法能够实现鲁棒、可泛化的反事实编辑，具有忠实的属性修改和强大的身份保持能力。

Abstract: We present Causal-Adapter, a modular framework that adapts frozen text-to-image diffusion backbones for counterfactual image generation. Our method enables causal interventions on target attributes, consistently propagating their effects to causal dependents without altering the core identity of the image. In contrast to prior approaches that rely on prompt engineering without explicit causal structure, Causal-Adapter leverages structural causal modeling augmented with two attribute regularization strategies: prompt-aligned injection, which aligns causal attributes with textual embeddings for precise semantic control, and a conditioned token contrastive loss to disentangle attribute factors and reduce spurious correlations. Causal-Adapter achieves state-of-the-art performance on both synthetic and real-world datasets, with up to 91\% MAE reduction on Pendulum for accurate attribute control and 87\% FID reduction on ADNI for high-fidelity MRI image generation. These results show that our approach enables robust, generalizable counterfactual editing with faithful attribute modification and strong identity preservation.

</details>


### [88] [Environment-Aware Satellite Image Generation with Diffusion Models](https://arxiv.org/abs/2509.24875)
*Nikos Kostagiolas,Pantelis Georgiades,Yannis Panagakis,Mihalis A. Nicolaou*

Main category: cs.CV

TL;DR: 提出了一种基于环境上下文条件的新型扩散模型，能够通过文本、元数据和视觉数据三种控制信号的任意组合生成卫星图像。该方法首次在卫星图像生成中引入动态环境条件作为控制信号，并采用元数据融合策略处理部分损坏或缺失的观测数据。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在遥感领域应用时面临环境上下文有限、难以处理缺失或损坏数据、无法可靠反映用户意图等显著限制。

Method: 开发了基于环境上下文条件的扩散模型，支持文本、元数据和视觉数据三种控制信号的任意组合输入，采用元数据融合策略建模属性嵌入交互以处理部分损坏或缺失的观测数据。

Result: 在单图像和时间序列生成试验中，该方法在定性和定量评估（使用6种不同指标）上均优于先前方法，表现出对缺失元数据的鲁棒性、对控制输入的高响应性，以及更高的保真度、准确性和生成质量。

Conclusion: 环境上下文条件化能够提高卫星图像基础模型的性能，该模型有望在下游任务中得到应用。收集的三模态数据集是首个公开可用的结合这三种不同媒介数据的集合。

Abstract: Diffusion-based foundation models have recently garnered much attention in the field of generative modeling due to their ability to generate images of high quality and fidelity. Although not straightforward, their recent application to the field of remote sensing signaled the first successful trials towards harnessing the large volume of publicly available datasets containing multimodal information. Despite their success, existing methods face considerable limitations: they rely on limited environmental context, struggle with missing or corrupted data, and often fail to reliably reflect user intentions in generated outputs. In this work, we propose a novel diffusion model conditioned on environmental context, that is able to generate satellite images by conditioning from any combination of three different control signals: a) text, b) metadata, and c) visual data. In contrast to previous works, the proposed method is i) to our knowledge, the first of its kind to condition satellite image generation on dynamic environmental conditions as part of its control signals, and ii) incorporating a metadata fusion strategy that models attribute embedding interactions to account for partially corrupt and/or missing observations. Our method outperforms previous methods both qualitatively (robustness to missing metadata, higher responsiveness to control inputs) and quantitatively (higher fidelity, accuracy, and quality of generations measured using 6 different metrics) in the trials of single-image and temporal generation. The reported results support our hypothesis that conditioning on environmental context can improve the performance of foundation models for satellite imagery, and render our model a promising candidate for usage in downstream tasks. The collected 3-modal dataset is to our knowledge, the first publicly-available dataset to combine data from these three different mediums.

</details>


### [89] [ThermalGen: Style-Disentangled Flow-Based Generative Models for RGB-to-Thermal Image Translation](https://arxiv.org/abs/2509.24878)
*Jiuhong Xiao,Roshan Nayak,Ning Zhang,Daniel Tortei,Giuseppe Loianno*

Main category: cs.CV

TL;DR: 提出了ThermalGen，一种基于自适应流的RGB-热图像转换生成模型，能够从RGB图像合成热图像，解决了RGB-热配对数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 同步校准的RGB-热图像对稀缺，阻碍了视觉-热传感器融合和跨模态任务的发展，RGB-热图像转换成为解决这一问题的有前景方案。

Method: 采用自适应流生成模型，结合RGB图像条件架构和风格解耦机制，构建了包含卫星-航空、航空和地面RGB-热配对数据的大规模训练集。

Result: 在多个RGB-热基准测试中，ThermalGen实现了与现有GAN和扩散方法相当或更优的转换性能，能够合成反映视角、传感器特性和环境条件显著变化的热图像。

Conclusion: ThermalGen是首个能够合成反映多种变化因素的热图像的RGB-热转换模型，为RGB-热数据稀缺问题提供了有效解决方案。

Abstract: Paired RGB-thermal data is crucial for visual-thermal sensor fusion and cross-modality tasks, including important applications such as multi-modal image alignment and retrieval. However, the scarcity of synchronized and calibrated RGB-thermal image pairs presents a major obstacle to progress in these areas. To overcome this challenge, RGB-to-Thermal (RGB-T) image translation has emerged as a promising solution, enabling the synthesis of thermal images from abundant RGB datasets for training purposes. In this study, we propose ThermalGen, an adaptive flow-based generative model for RGB-T image translation, incorporating an RGB image conditioning architecture and a style-disentangled mechanism. To support large-scale training, we curated eight public satellite-aerial, aerial, and ground RGB-T paired datasets, and introduced three new large-scale satellite-aerial RGB-T datasets--DJI-day, Bosonplus-day, and Bosonplus-night--captured across diverse times, sensor types, and geographic regions. Extensive evaluations across multiple RGB-T benchmarks demonstrate that ThermalGen achieves comparable or superior translation performance compared to existing GAN-based and diffusion-based methods. To our knowledge, ThermalGen is the first RGB-T image translation model capable of synthesizing thermal images that reflect significant variations in viewpoints, sensor characteristics, and environmental conditions. Project page: http://xjh19971.github.io/ThermalGen

</details>


### [90] [MMRQA: Signal-Enhanced Multimodal Large Language Models for MRI Quality Assessment](https://arxiv.org/abs/2509.24888)
*Fankai Jia,Daisong Gan,Zhe Zhang,Zhaochi Wen,Chenchen Dan,Dong Liang,Haifeng Wang*

Main category: cs.CV

TL;DR: 提出了MMRQA框架，首次将多模态大语言模型与采集感知信号处理相结合，用于MRI质量评估，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决MRI质量评估中的数据稀缺和协议变异性问题，克服传统方法在定量指标与语义理解之间的权衡限制。

Method: 结合三个关键创新：通过MRQy增强模拟伪影的鲁棒指标提取、使用Qwen将指标结构化转换为问答对、通过LoRA参数高效融合LLaVA-OneVision。

Result: 在MR-ART、FastMRI和MyConnectome基准测试中达到最先进性能，具有强大的零样本泛化能力。

Conclusion: 通过桥接定量分析与语义推理，该框架生成临床可解释的输出，增强了动态医疗环境中的质量控制。

Abstract: Magnetic resonance imaging (MRI) quality assessment is crucial for clinical decision-making, yet remains challenging due to data scarcity and protocol variability. Traditional approaches face fundamental trade-offs: signal-based methods like MRIQC provide quantitative metrics but lack semantic understanding, while deep learning approaches achieve high accuracy but sacrifice interpretability. To address these limitations, we introduce the Multimodal MRI Quality Assessment (MMRQA) framework, pioneering the integration of multimodal large language models (MLLMs) with acquisition-aware signal processing. MMRQA combines three key innovations: robust metric extraction via MRQy augmented with simulated artifacts, structured transformation of metrics into question-answer pairs using Qwen, and parameter-efficient fusion through Low-Rank Adaptation (LoRA) of LLaVA-OneVision. Evaluated on MR-ART, FastMRI, and MyConnectome benchmarks, MMRQA achieves state-of-the-art performance with strong zero-shot generalization, as validated by comprehensive ablation studies. By bridging quantitative analysis with semantic reasoning, our framework generates clinically interpretable outputs that enhance quality control in dynamic medical settings.

</details>


### [91] [VAGUEGAN: Stealthy Poisoning and Backdoor Attacks on Image Generative Pipelines](https://arxiv.org/abs/2509.24891)
*Mostafa Mohaimen Akand Faisal,Rabeya Amin Jhuma*

Main category: cs.CV

TL;DR: VagueGAN是一种针对生成模型的攻击方法，通过结合模块化扰动网络PoisonerNet和生成器-判别器对，制作隐蔽的触发信号，在生成图像中引起目标变化。


<details>
  <summary>Details</summary>
Motivation: 虽然对抗性攻击在判别模型中已有深入研究，但在生成管道中，输入的小型隐蔽扰动导致输出受控变化的研究较少探索。

Method: 使用PoisonerNet与生成器-判别器对组合的攻击管道，评估攻击效果使用自定义代理指标，隐蔽性通过感知和频域测量分析。

Result: 实验显示中毒输出可能比干净对应物具有更高的视觉质量，挑战了中毒必然降低保真度的假设。潜在空间中毒可以保留甚至增强输出美学。

Conclusion: 精心优化的扰动可以在生成器输出上产生一致、隐蔽的效果，同时保持视觉上不明显，这引发了图像生成管道完整性的担忧。

Abstract: Generative models such as GANs and diffusion models are widely used to synthesize photorealistic images and to support downstream creative and editing tasks. While adversarial attacks on discriminative models are well studied, attacks targeting generative pipelines where small, stealthy perturbations in inputs lead to controlled changes in outputs are less explored. This study introduces VagueGAN, an attack pipeline combining a modular perturbation network PoisonerNet with a Generator Discriminator pair to craft stealthy triggers that cause targeted changes in generated images. Attack efficacy is evaluated using a custom proxy metric, while stealth is analyzed through perceptual and frequency domain measures. The transferability of the method to a modern diffusion based pipeline is further examined through ControlNet guided editing. Interestingly, the experiments show that poisoned outputs can display higher visual quality compared to clean counterparts, challenging the assumption that poisoning necessarily reduces fidelity. Unlike conventional pixel level perturbations, latent space poisoning in GANs and diffusion pipelines can retain or even enhance output aesthetics, exposing a blind spot in pixel level defenses. Moreover, carefully optimized perturbations can produce consistent, stealthy effects on generator outputs while remaining visually inconspicuous, raising concerns for the integrity of image generation pipelines.

</details>


### [92] [DWGS: Enhancing Sparse-View Gaussian Splatting with Hybrid-Loss Depth Estimation and Bidirectional Warping](https://arxiv.org/abs/2509.24893)
*Yu Ma,Guoliang Wei,Yue Cheng*

Main category: cs.CV

TL;DR: DWGS是一个统一框架，通过整合结构线索、虚拟视图约束和遮挡区域补全来增强3D高斯泼溅在稀疏视图合成中的性能。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图下的新视角合成存在过拟合、几何失真和场景恢复不完整的问题，3D高斯泼溅在稀疏输入下会出现浮动伪影和结构不一致。

Method: 提出三个主要贡献：混合损失深度估计模块、双向扭曲虚拟视图合成方法和遮挡感知重建组件。

Result: 在标准基准测试中达到最先进水平，PSNR最高达21.13 dB，LPIPS为0.189，同时保持实时推理能力。

Conclusion: DWGS框架有效解决了稀疏视图合成中的关键问题，实现了高质量的新视角合成。

Abstract: Novel View Synthesis (NVS) from sparse views remains a core challenge in 3D reconstruction, typically suffering from overfitting, geometric distortion, and incomplete scene recovery due to limited multi-view constraints. Although 3D Gaussian Splatting (3DGS) enables real-time, high-fidelity rendering, it suffers from floating artifacts and structural inconsistencies under sparse-input settings. To address these issues, we propose DWGS, a novel unified framework that enhances 3DGS for sparse-view synthesis by integrating robust structural cues, virtual view constraints, and occluded region completion. Our approach introduces three principal contributions: a Hybrid-Loss Depth Estimation module that leverages dense matching priors with reprojection, point propagation, and smoothness constraints to enforce multi-view consistency; a Bidirectional Warping Virtual View Synthesis method generates virtual training views to impose stronger geometric and photometric constraints; and an Occlusion-Aware Reconstruction component that utilizes depth-difference mask and a learning-based inpainting model to recover obscured regions. Extensive experiments on standard benchmarks (LLFF, Blender, and DTU) show that DWGS achieves a new state-of-the-art, achieving up to 21.13 dB PSNR and 0.189 LPIPS, while retaining real-time inference capabilities.

</details>


### [93] [Attention Surgery: An Efficient Recipe to Linearize Your Video Diffusion Transformer](https://arxiv.org/abs/2509.24899)
*Mohsen Ghafoorian,Denis Korzhenkov,Amirhossein Habibian*

Main category: cs.CV

TL;DR: 提出Attention Surgery框架，无需从头训练即可将预训练视频扩散模型中的注意力机制线性化或混合化，在保持生成质量的同时显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的视频扩散模型虽然生成质量优秀，但自注意力机制的二阶计算成本限制了长序列和高分辨率视频的生成效率

Method: 结合新型混合注意力机制（混合softmax和线性token）与轻量级蒸馏微调流程，并采用成本感知的块率策略来平衡各层的表达能力和效率

Result: 在Wan2.1 1.3B模型上实现首个具有竞争力的亚二阶注意力视频扩散模型，注意力计算成本降低高达40%，同时在VBench和VBench-2.0基准测试中保持生成质量

Conclusion: Attention Surgery框架成功解决了视频扩散模型的计算效率问题，为高质量视频生成提供了实用的加速方案

Abstract: Transformer-based video diffusion models (VDMs) deliver state-of-the-art video generation quality but are constrained by the quadratic cost of self-attention, making long sequences and high resolutions computationally expensive. While linear attention offers sub-quadratic complexity, prior attempts fail to match the expressiveness of softmax attention without costly retraining. We introduce \textit{Attention Surgery}, an efficient framework for \textit{linearizing} or \textit{hybridizing} attention in pretrained VDMs without training from scratch. Inspired by recent advances in language models, our method combines a novel hybrid attention mechanism-mixing softmax and linear tokens-with a lightweight distillation and fine-tuning pipeline requiring only a few GPU-days. Additionally, we incorporate a cost-aware block-rate strategy to balance expressiveness and efficiency across layers. Applied to Wan2.1 1.3B, a state-of-the-art DiT-based VDM, Attention Surgery achieves the first competitive sub-quadratic attention video diffusion models, reducing attention cost by up to 40\% in terms of FLOPs, while maintaining generation quality as measured on the standard VBench and VBench-2.0 benchmarks.

</details>


### [94] [OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing](https://arxiv.org/abs/2509.24900)
*Zhihong Chen,Xuehai Bai,Yang Shi,Chaoyou Fu,Huanyu Zhang,Haotian Wang,Xiaoyan Sun,Zhang Zhang,Liang Wang,Yuanxing Zhang,Pengfei Wan,Yi-Fan Zhang*

Main category: cs.CV

TL;DR: 提出了OpenGPT-4o-Image数据集，通过层次化任务分类和自动化数据生成方法构建，包含80k高质量指令-图像对，显著提升了多模态模型的生成和编辑性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏系统结构和真实应用场景的挑战性任务，限制了统一多模态模型的性能。

Method: 结合层次化任务分类和自动化数据生成，利用结构化资源池和GPT-4o构建包含11个主要领域、51个子任务的大规模数据集。

Result: 在领先模型上微调后，编辑任务性能提升18%（UniWorld-V1在ImgEdit-Bench），生成任务提升13%（Harmon在GenEval）。

Conclusion: 系统化的数据构建是提升多模态AI能力的关键。

Abstract: The performance of unified multimodal models for image generation and editing is fundamentally constrained by the quality and comprehensiveness of their training data. While existing datasets have covered basic tasks like style transfer and simple object manipulation, they often lack the systematic structure and challenging scenarios required for real-world applications. To address this bottleneck, we introduce OpenGPT-4o-Image, a large-scale dataset constructed using a novel methodology that combines hierarchical task taxonomy with automated data generation. Our taxonomy not only includes fundamental capabilities such as text rendering and style control but also introduces highly practical yet challenging categories like scientific imagery for chemistry illustrations and complex instruction editing requiring simultaneous execution of multiple operations. Through an automated pipeline leveraging structured resource pools and GPT-4o, we generate 80k high-quality instruction-image pairs with controlled diversity, covering 11 major domains and 51 subtasks. Extensive experiments show that fine-tuning leading models on our dataset achieves significant performance gains across multiple benchmarks, with improvements of up to 18\% on editing tasks (UniWorld-V1 on ImgEdit-Bench) and 13% on generation tasks (Harmon on GenEval). Our work demonstrates that systematic data construction is key to advancing multimodal AI capabilities.

</details>


### [95] [Segmentor-Guided Counterfactual Fine-Tuning for Image Synthesis](https://arxiv.org/abs/2509.24913)
*Tian Xia,Matthew Sinclair,Andreas Schuh,Fabio De Sousa Ribeiro,Raghav Mehta,Rajat Rasal,Esther Puyol-Antón,Samuel Gerber,Kersten Petersen,Michiel Schaap,Ben Glocker*

Main category: cs.CV

TL;DR: 提出了Seg-CFT方法，通过结构分割器引导生成局部一致的反事实图像，解决了传统方法在结构特定干预时产生全局不良影响的问题。


<details>
  <summary>Details</summary>
Motivation: 当前反事实图像生成方法依赖外部分类器或回归器，对于结构特定干预（如改变左肺面积）效果不足，且会产生不希望的全局影响。先前工作需要像素级标签图作为指导，但获取这些假设分割图既繁琐又困难。

Method: 提出Segmentor-guided Counterfactual Fine-Tuning (Seg-CFT)方法，在保持对结构特定标量变量干预简单性的同时，生成局部一致且有效的反事实图像。

Result: 展示了生成真实胸部X光片的能力，并在冠状动脉疾病建模方面显示出有希望的结果。

Conclusion: Seg-CFT方法能够有效生成结构特定的反事实图像，避免了传统方法的局限性，在医学图像分析中具有应用潜力。

Abstract: Counterfactual image generation is a powerful tool for augmenting training data, de-biasing datasets, and modeling disease. Current approaches rely on external classifiers or regressors to increase the effectiveness of subject-level interventions (e.g., changing the patient's age). For structure-specific interventions (e.g., changing the area of the left lung in a chest radiograph), we show that this is insufficient, and can result in undesirable global effects across the image domain. Previous work used pixel-level label maps as guidance, requiring a user to provide hypothetical segmentations which are tedious and difficult to obtain. We propose Segmentor-guided Counterfactual Fine-Tuning (Seg-CFT), which preserves the simplicity of intervening on scalar-valued, structure-specific variables while producing locally coherent and effective counterfactuals. We demonstrate the capability of generating realistic chest radiographs, and we show promising results for modeling coronary artery disease. Code: https://github.com/biomedia-mira/seg-cft.

</details>


### [96] [Scalable GANs with Transformers](https://arxiv.org/abs/2509.24935)
*Sangeek Hyun,MinKyu Lee,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 本文研究了GAN的可扩展性，通过在VAE潜在空间中训练并使用纯transformer架构，解决了GAN规模化时的失败模式，提出了轻量级中间监督和宽度感知学习率调整等解决方案。


<details>
  <summary>Details</summary>
Motivation: 探索生成对抗网络(GANs)的可扩展性原则，虽然可扩展性推动了生成模型的进展，但在对抗学习中的原理仍未充分探索。

Method: 在紧凑的变分自编码器潜在空间中训练，采用纯transformer生成器和判别器，通过轻量级中间监督和宽度感知学习率调整解决规模化问题。

Result: GAT-XL/2在ImageNet-256上实现了单步、类条件生成的最先进性能(FID为2.96)，仅需40个epoch，比强基线少6倍训练周期。

Conclusion: 纯transformer和潜在空间的GANs可以在各种容量范围内可靠训练，通过适当的规模化解决方案可以实现高效的生成性能。

Abstract: Scalability has driven recent advances in generative modeling, yet its principles remain underexplored for adversarial learning. We investigate the scalability of Generative Adversarial Networks (GANs) through two design choices that have proven to be effective in other types of generative models: training in a compact Variational Autoencoder latent space and adopting purely transformer-based generators and discriminators. Training in latent space enables efficient computation while preserving perceptual fidelity, and this efficiency pairs naturally with plain transformers, whose performance scales with computational budget. Building on these choices, we analyze failure modes that emerge when naively scaling GANs. Specifically, we find issues as underutilization of early layers in the generator and optimization instability as the network scales. Accordingly, we provide simple and scale-friendly solutions as lightweight intermediate supervision and width-aware learning-rate adjustment. Our experiments show that GAT, a purely transformer-based and latent-space GANs, can be easily trained reliably across a wide range of capacities (S through XL). Moreover, GAT-XL/2 achieves state-of-the-art single-step, class-conditional generation performance (FID of 2.96) on ImageNet-256 in just 40 epochs, 6x fewer epochs than strong baselines.

</details>


### [97] [Wan-Alpha: High-Quality Text-to-Video Generation with Alpha Channel](https://arxiv.org/abs/2509.24979)
*Haotian Dong,Wenjing Wang,Chen Li,Di Lin*

Main category: cs.CV

TL;DR: Wan-Alpha是一个生成RGBA透明视频的新框架，通过联合学习RGB和alpha通道，使用变分自编码器将alpha通道编码到RGB潜在空间，并构建高质量RGBA视频数据集训练扩散变换器。


<details>
  <summary>Details</summary>
Motivation: 现有RGBA视频生成方法往往忽视视觉质量，限制了实际应用。

Method: 设计有效的变分自编码器将alpha通道编码到RGB潜在空间，构建高质量RGBA视频数据集，训练扩散变换器模型。

Result: 相比最先进方法，在视觉质量、运动真实感和透明度渲染方面表现更优，能生成各种半透明物体、发光效果和头发丝等精细细节。

Conclusion: Wan-Alpha框架在RGBA视频生成方面具有优越性能，能够生成高质量的透明视频内容。

Abstract: RGBA video generation, which includes an alpha channel to represent transparency, is gaining increasing attention across a wide range of applications. However, existing methods often neglect visual quality, limiting their practical usability. In this paper, we propose \textit{Wan-Alpha}, a new framework that generates transparent videos by learning both RGB and alpha channels jointly. We design an effective variational autoencoder (VAE) that encodes the alpha channel into the RGB latent space. Then, to support the training of our diffusion transformer, we construct a high-quality and diverse RGBA video dataset. Compared with state-of-the-art methods, our model demonstrates superior performance in visual quality, motion realism, and transparency rendering. Notably, our model can generate a wide variety of semi-transparent objects, glowing effects, and fine-grained details such as hair strands. The released model is available on our website: \href{https://donghaotian123.github.io/Wan-Alpha/}{https://donghaotian123.github.io/Wan-Alpha/}.

</details>


### [98] [SDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose Estimation](https://arxiv.org/abs/2509.24980)
*Shuang Liang,Jing He,Chuanmeizhi Wang,Lejun Liao,Guo Zhang,Yingcong Chen,Yuan Yuan*

Main category: cs.CV

TL;DR: SDPose是一个基于Stable Diffusion的微调框架，用于人体姿态估计。它通过直接在SD U-Net的潜在空间中预测关键点热图来保持生成先验，使用轻量级卷积姿态头，并加入RGB重建分支增强泛化能力。在少量训练下达到SOTA性能，并能用于零样本姿态标注。


<details>
  <summary>Details</summary>
Motivation: 预训练扩散模型提供了丰富的多尺度潜在特征，但它们在结构化输出（如人体姿态估计）方面的潜力尚未充分探索。现有方法如Marigold和Lotus主要关注密集预测，而SDPose旨在充分利用扩散先验进行人体姿态估计。

Method: 1. 直接在SD U-Net的潜在空间中预测关键点热图，保持原始生成先验；2. 通过轻量级卷积姿态头将潜在特征映射到关键点热图；3. 加入辅助RGB重建分支以防止过拟合并增强域外鲁棒性。

Result: 在仅使用Sapiens五分之一训练时间的情况下，SDPose在COCO验证集上与Sapiens-1B/2B持平，在跨域基准HumanArt和COCO-OOD上创下新SOTA。还能作为零样本姿态标注器用于可控生成任务。

Conclusion: SDPose成功展示了预训练扩散模型在结构化姿态估计任务中的潜力，通过保持生成先验和增强泛化能力，实现了优异的性能和跨域鲁棒性。

Abstract: Pre-trained diffusion models provide rich multi-scale latent features and are emerging as powerful vision backbones. While recent works such as Marigold~\citep{ke2024repurposing} and Lotus~\citep{he2024lotus} adapt diffusion priors for dense prediction with strong cross-domain generalization, their potential for structured outputs (e.g., human pose estimation) remains underexplored. In this paper, we propose \textbf{SDPose}, a fine-tuning framework built upon Stable Diffusion to fully exploit pre-trained diffusion priors for human pose estimation. First, rather than modifying cross-attention modules or introducing learnable embeddings, we directly predict keypoint heatmaps in the SD U-Net's image latent space to preserve the original generative priors. Second, we map these latent features into keypoint heatmaps through a lightweight convolutional pose head, which avoids disrupting the pre-trained backbone. Finally, to prevent overfitting and enhance out-of-distribution robustness, we incorporate an auxiliary RGB reconstruction branch that preserves domain-transferable generative semantics. To evaluate robustness under domain shift, we further construct \textbf{COCO-OOD}, a style-transferred variant of COCO with preserved annotations. With just one-fifth of the training schedule used by Sapiens on COCO, SDPose attains parity with Sapiens-1B/2B on the COCO validation set and establishes a new state of the art on the cross-domain benchmarks HumanArt and COCO-OOD. Furthermore, we showcase SDPose as a zero-shot pose annotator for downstream controllable generation tasks, including ControlNet-based image synthesis and video generation, where it delivers qualitatively superior pose guidance.

</details>


### [99] [PanoWorld-X: Generating Explorable Panoramic Worlds via Sphere-Aware Video Diffusion](https://arxiv.org/abs/2509.24997)
*Yuyang Yin,HaoXiang Guo,Fangfu Liu,Mengyu Wang,Hanwen Liang,Eric Li,Yikai Wang,Xiaojie Jin,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: 提出了PanoWorld-X框架，用于生成高保真、可控的全景视频，支持多样化的相机轨迹，解决了现有方法视野受限和相机控制不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在视野狭窄或相机控制性不足的问题，限制了连续、完整场景的合成和自由探索能力。

Method: 首先在虚拟3D环境中通过Unreal Engine模拟相机轨迹构建大规模全景视频-探索路径数据集；然后提出Sphere-Aware Diffusion Transformer架构，将等矩形特征重投影到球面以建模几何邻接关系。

Result: 广泛实验表明，PanoWorld-X在运动范围、控制精度和视觉质量等方面均取得优越性能。

Conclusion: 该框架在真实世界应用中具有巨大潜力，能够生成高质量、可控的全景视频。

Abstract: Generating a complete and explorable 360-degree visual world enables a wide range of downstream applications. While prior works have advanced the field, they remain constrained by either narrow field-of-view limitations, which hinder the synthesis of continuous and holistic scenes, or insufficient camera controllability that restricts free exploration by users or autonomous agents. To address this, we propose PanoWorld-X, a novel framework for high-fidelity and controllable panoramic video generation with diverse camera trajectories. Specifically, we first construct a large-scale dataset of panoramic video-exploration route pairs by simulating camera trajectories in virtual 3D environments via Unreal Engine. As the spherical geometry of panoramic data misaligns with the inductive priors from conventional video diffusion, we then introduce a Sphere-Aware Diffusion Transformer architecture that reprojects equirectangular features onto the spherical surface to model geometric adjacency in latent space, significantly enhancing visual fidelity and spatiotemporal continuity. Extensive experiments demonstrate that our PanoWorld-X achieves superior performance in various aspects, including motion range, control precision, and visual quality, underscoring its potential for real-world applications.

</details>


### [100] [STAGE: Stable and Generalizable GRPO for Autoregressive Image Generation](https://arxiv.org/abs/2509.25027)
*Xiaoxiao Ma,Haibo Qiu,Guohui Zhang,Zhixiong Zeng,Siqi Yang,Lin Ma,Feng Zhao*

Main category: cs.CV

TL;DR: 提出STAGE框架，通过优势/KL重加权和熵奖励来解决AR图像生成中GRPO训练的不稳定问题，提升图像质量和泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有GRPO算法应用于自回归图像模型时存在训练不稳定、破坏预训练模型能力、图像质量下降和泛化能力差的问题

Method: 1) 优势/KL重加权：相似性感知重加权缓解冲突更新；2) 熵奖励：基于参考模型的熵奖励稳定学习过程

Result: 在多个基准测试中，STAGE相比基线GRPO持续提升了视觉质量、稳定性和跨任务泛化能力

Conclusion: STAGE通过缓解token间冲突和稳定训练，减少对预训练分布的破坏，缓解奖励攻击，从而改善泛化能力

Abstract: Reinforcement learning has recently been explored to improve text-to-image generation, yet applying existing GRPO algorithms to autoregressive (AR) image models remains challenging. The instability of the training process easily disrupts the pretrained model capability during long runs, resulting in marginal gains, degraded image quality, and poor generalization. In this work, we revisit GRPO for AR image generation and identify two key issues: contradictory gradients from unnecessary tokens and unstable policy entropy dynamics. To address these, we introduce STAGE, a stable and generalizable framework that leverages two targeted solutions: 1) Advantage/KL reweighting. Similarity-aware reweighting to alleviate conflicting updates; and 2) Entropy reward. An entropy-based reward corresponding to reference model to stabilize learning. With the help of alleviating conflicts between tokens and an entropy reward for stabilizing training, we reduce disruption of the pretrained distribution and mitigate reward hacking, which in turn improves generalization and transfer better to other benchmarks. Experiments across multiple benchmarks show that STAGE consistently improves visual quality, stability, and cross-task generalization compared to baseline GRPO.

</details>


### [101] [GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM Reconstruction](https://arxiv.org/abs/2509.25075)
*Huaizhi Qu,Xiao Wang,Gengwei Zhang,Jie Peng,Tianlong Chen*

Main category: cs.CV

TL;DR: GEM是一个基于3D高斯溅射的冷冻电镜重建框架，直接在实空间操作，通过紧凑的3D高斯表示蛋白质，实现了更快的训练速度、更低的内存使用和更高的分辨率。


<details>
  <summary>Details</summary>
Motivation: 冷冻电镜数据集规模庞大，传统傅里叶空间方法效率高但保真度低，而基于神经辐射场的实空间方法精度高但内存和计算开销大。需要一种既能保持高精度又高效的方法。

Method: 使用3D高斯溅射表示蛋白质，每个高斯仅用11个参数参数化。设计了新颖的梯度计算来优化每个体素贡献的3D高斯，大幅减少了内存占用和训练成本。

Result: 在标准冷冻电镜基准测试中，GEM实现了比最先进方法快48%的训练速度，内存使用降低12%，局部分辨率提升高达38.8%。

Conclusion: GEM为冷冻电镜重建提供了一个实用且可扩展的范式，统一了速度、效率和高分辨率精度。

Abstract: Cryo-electron microscopy (cryo-EM) has become a central tool for high-resolution structural biology, yet the massive scale of datasets (often exceeding 100k particle images) renders 3D reconstruction both computationally expensive and memory intensive. Traditional Fourier-space methods are efficient but lose fidelity due to repeated transforms, while recent real-space approaches based on neural radiance fields (NeRFs) improve accuracy but incur cubic memory and computation overhead. Therefore, we introduce GEM, a novel cryo-EM reconstruction framework built on 3D Gaussian Splatting (3DGS) that operates directly in real-space while maintaining high efficiency. Instead of modeling the entire density volume, GEM represents proteins with compact 3D Gaussians, each parameterized by only 11 values. To further improve the training efficiency, we designed a novel gradient computation to 3D Gaussians that contribute to each voxel. This design substantially reduced both memory footprint and training cost. On standard cryo-EM benchmarks, GEM achieves up to 48% faster training and 12% lower memory usage compared to state-of-the-art methods, while improving local resolution by as much as 38.8%. These results establish GEM as a practical and scalable paradigm for cryo-EM reconstruction, unifying speed, efficiency, and high-resolution accuracy. Our code is available at https://github.com/UNITES-Lab/GEM.

</details>


### [102] [Triangle Splatting+: Differentiable Rendering with Opaque Triangles](https://arxiv.org/abs/2509.25122)
*Jan Held,Renaud Vandeghen,Sanghyun Son,Daniel Rebain,Matheus Gadelha,Yi Zhou,Ming C. Lin,Marc Van Droogenbroeck,Andrea Tagliasacchi*

Main category: cs.CV

TL;DR: Triangle Splatting+ 是一种直接优化三角形的方法，通过可微分光栅化框架实现实时渲染，无需后处理即可在标准图形引擎中使用。


<details>
  <summary>Details</summary>
Motivation: 现有方法如3D高斯泼溅虽然能实现实时渲染，但高斯基元与基于网格的VR头显和图形应用不兼容，现有转换方法会增加复杂度并降低视觉质量。

Method: 在可微分泼溅框架中直接优化三角形，设计三角形参数化以支持共享顶点连接，并采用训练策略强制三角形不透明。

Result: 在Mip-NeRF360和Tanks & Temples数据集上实现了基于网格的新视角合成的最先进性能，视觉保真度优于现有泼溅方法。

Conclusion: 该方法生成的半连接网格可直接用于标准图形引擎，并支持物理模拟和交互式漫游等下游应用，训练高效快速。

Abstract: Reconstructing 3D scenes and synthesizing novel views has seen rapid progress in recent years. Neural Radiance Fields demonstrated that continuous volumetric radiance fields can achieve high-quality image synthesis, but their long training and rendering times limit practicality. 3D Gaussian Splatting (3DGS) addressed these issues by representing scenes with millions of Gaussians, enabling real-time rendering and fast optimization. However, Gaussian primitives are not natively compatible with the mesh-based pipelines used in VR headsets, and real-time graphics applications. Existing solutions attempt to convert Gaussians into meshes through post-processing or two-stage pipelines, which increases complexity and degrades visual quality. In this work, we introduce Triangle Splatting+, which directly optimizes triangles, the fundamental primitive of computer graphics, within a differentiable splatting framework. We formulate triangle parametrization to enable connectivity through shared vertices, and we design a training strategy that enforces opaque triangles. The final output is immediately usable in standard graphics engines without post-processing. Experiments on the Mip-NeRF360 and Tanks & Temples datasets show that Triangle Splatting+achieves state-of-the-art performance in mesh-based novel view synthesis. Our method surpasses prior splatting approaches in visual fidelity while remaining efficient and fast to training. Moreover, the resulting semi-connected meshes support downstream applications such as physics-based simulation or interactive walkthroughs. The project page is https://trianglesplatting2.github.io/trianglesplatting2/.

</details>


### [103] [Score Distillation of Flow Matching Models](https://arxiv.org/abs/2509.25127)
*Mingyuan Zhou,Yi Gu,Huangjie Zheng,Liangchen Song,Guande He,Yizhe Zhang,Wenze Hu,Yinfei Yang*

Main category: cs.CV

TL;DR: 本文证明扩散模型与流匹配在理论上的等价性，并将分数蒸馏技术扩展到预训练的文本到图像流匹配模型，实现了无需教师微调或架构修改的加速生成。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成质量高但采样速度慢，而流匹配与扩散模型在理论上等价，但之前不确定蒸馏技术是否可以直接应用于流匹配模型。

Method: 基于贝叶斯规则和条件期望的简单推导统一了高斯扩散和流匹配，并将分数身份蒸馏扩展到预训练的文本到图像流匹配模型。

Result: 实验表明，只需适度的流匹配和DiT特定调整，SiD就能在这些模型上直接工作，在无数据和有数据设置下都有效。

Conclusion: 分数蒸馏技术广泛适用于文本到图像流匹配模型，解决了之前关于稳定性和合理性的担忧，统一了扩散和流基生成器的加速技术。

Abstract: Diffusion models achieve high-quality image generation but are limited by slow iterative sampling. Distillation methods alleviate this by enabling one- or few-step generation. Flow matching, originally introduced as a distinct framework, has since been shown to be theoretically equivalent to diffusion under Gaussian assumptions, raising the question of whether distillation techniques such as score distillation transfer directly. We provide a simple derivation -- based on Bayes' rule and conditional expectations -- that unifies Gaussian diffusion and flow matching without relying on ODE/SDE formulations. Building on this view, we extend Score identity Distillation (SiD) to pretrained text-to-image flow-matching models, including SANA, SD3-Medium, SD3.5-Medium/Large, and FLUX.1-dev, all with DiT backbones. Experiments show that, with only modest flow-matching- and DiT-specific adjustments, SiD works out of the box across these models, in both data-free and data-aided settings, without requiring teacher finetuning or architectural changes. This provides the first systematic evidence that score distillation applies broadly to text-to-image flow matching models, resolving prior concerns about stability and soundness and unifying acceleration techniques across diffusion- and flow-based generators. We will make the PyTorch implementation publicly available.

</details>


### [104] [Rolling Forcing: Autoregressive Long Video Diffusion in Real Time](https://arxiv.org/abs/2509.25161)
*Kunhao Liu,Wenbo Hu,Jiale Xu,Ying Shan,Shijian Lu*

Main category: cs.CV

TL;DR: 提出Rolling Forcing技术，通过联合去噪、注意力下沉和高效训练算法，解决流式视频生成中的误差累积问题，实现单GPU实时生成长达数分钟的高质量视频。


<details>
  <summary>Details</summary>
Motivation: 现有流式视频生成方法存在严重的误差累积问题，导致长时域视频质量显著下降，需要一种能够最小化误差累积的技术。

Method: 1. 联合去噪方案：同时去噪多帧，减少相邻帧间的严格因果依赖；2. 注意力下沉机制：保留初始帧作为全局上下文锚点；3. 高效训练算法：在扩展的去噪窗口上进行少步蒸馏。

Result: 实验证明该方法能在单GPU上实时生成长达数分钟的视频，显著减少误差累积。

Conclusion: Rolling Forcing技术有效解决了流式视频生成中的误差累积问题，为交互式世界模型和神经游戏引擎提供了高质量、低延迟的长视频流生成能力。

Abstract: Streaming video generation, as one fundamental component in interactive world models and neural game engines, aims to generate high-quality, low-latency, and temporally coherent long video streams. However, most existing work suffers from severe error accumulation that often significantly degrades the generated stream videos over long horizons. We design Rolling Forcing, a novel video generation technique that enables streaming long videos with minimal error accumulation. Rolling Forcing comes with three novel designs. First, instead of iteratively sampling individual frames, which accelerates error propagation, we design a joint denoising scheme that simultaneously denoises multiple frames with progressively increasing noise levels. This design relaxes the strict causality across adjacent frames, effectively suppressing error growth. Second, we introduce the attention sink mechanism into the long-horizon stream video generation task, which allows the model to keep key value states of initial frames as a global context anchor and thereby enhances long-term global consistency. Third, we design an efficient training algorithm that enables few-step distillation over largely extended denoising windows. This algorithm operates on non-overlapping windows and mitigates exposure bias conditioned on self-generated histories. Extensive experiments show that Rolling Forcing enables real-time streaming generation of multi-minute videos on a single GPU, with substantially reduced error accumulation.

</details>


### [105] [Aligning Visual Foundation Encoders to Tokenizers for Diffusion Models](https://arxiv.org/abs/2509.25162)
*Bowei Chen,Sai Bi,Hao Tan,He Zhang,Tianyuan Zhang,Zhengqi Li,Yuanjun Xiong,Jianming Zhang,Kai Zhang*

Main category: cs.CV

TL;DR: 提出一种将预训练视觉编码器对齐作为潜在扩散模型标记器的方法，通过三阶段对齐策略获得语义丰富的图像标记器，加速扩散模型收敛并提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统从头训练VAE主要关注低级细节，而本方法利用基础编码器的丰富语义结构，旨在建立语义基础的连续标记器设计范式。

Method: 三阶段对齐策略：1)冻结编码器，训练适配器和解码器建立语义潜在空间；2)联合优化所有组件并添加语义保持损失；3)精炼解码器提升重建质量。

Result: 在ImageNet 256×256上，标记器加速扩散模型收敛，64个周期内达到gFID 1.90；在LAION上，2B参数文本到图像模型在相同训练步骤下持续优于FLUX VAE。

Conclusion: 该方法简单、可扩展，为连续标记器设计建立了语义基础的新范式。

Abstract: In this work, we propose aligning pretrained visual encoders to serve as tokenizers for latent diffusion models in image generation. Unlike training a variational autoencoder (VAE) from scratch, which primarily emphasizes low-level details, our approach leverages the rich semantic structure of foundation encoders. We introduce a three-stage alignment strategy: (1) freeze the encoder and train an adapter and a decoder to establish a semantic latent space; (2) jointly optimize all components with an additional semantic preservation loss, enabling the encoder to capture perceptual details while retaining high-level semantics; and (3) refine the decoder for improved reconstruction quality. This alignment yields semantically rich image tokenizers that benefit diffusion models. On ImageNet 256$\times$256, our tokenizer accelerates the convergence of diffusion models, reaching a gFID of 1.90 within just 64 epochs, and improves generation both with and without classifier-free guidance. Scaling to LAION, a 2B-parameter text-to-image model trained with our tokenizer consistently outperforms FLUX VAE under the same training steps. Overall, our method is simple, scalable, and establishes a semantically grounded paradigm for continuous tokenizer design.

</details>


### [106] [GHOST: Hallucination-Inducing Image Generation for Multimodal LLMs](https://arxiv.org/abs/2509.25178)
*Aryan Yazdan Parast,Parsa Hosseini,Hesam Asadollahzadeh,Arshia Soltani Moakhar,Basim Azam,Soheil Feizi,Naveed Akhtar*

Main category: cs.CV

TL;DR: GHOST是一种自动生成诱导多模态大语言模型产生物体幻觉的图像的方法，通过优化图像嵌入空间来误导模型，同时保持目标对象实际不存在，从而揭示模型的幻觉漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型存在物体幻觉问题，但当前研究使用静态基准测试，无法发现模型特定或未预料到的幻觉漏洞。需要一种主动测试方法来系统性地发现这些弱点。

Method: GHOST方法在图像嵌入空间进行优化，误导模型感知不存在的对象，然后引导扩散模型基于优化后的嵌入生成自然图像。整个过程完全自动，无需人工监督或先验知识。

Result: 该方法在包括GLM-4.1V-Thinking在内的多个模型上实现了超过28%的幻觉成功率，远高于之前数据驱动方法的约1%。生成的图像质量高且确实不包含目标对象，还能在不同模型间转移漏洞。

Conclusion: GHOST不仅是一种诊断工具，还能通过在其生成的图像上进行微调来缓解幻觉问题，为构建更可靠的多模态系统提供了诊断和纠正手段。

Abstract: Object hallucination in Multimodal Large Language Models (MLLMs) is a persistent failure mode that causes the model to perceive objects absent in the image. This weakness of MLLMs is currently studied using static benchmarks with fixed visual scenarios, which preempts the possibility of uncovering model-specific or unanticipated hallucination vulnerabilities. We introduce GHOST (Generating Hallucinations via Optimizing Stealth Tokens), a method designed to stress-test MLLMs by actively generating images that induce hallucination. GHOST is fully automatic and requires no human supervision or prior knowledge. It operates by optimizing in the image embedding space to mislead the model while keeping the target object absent, and then guiding a diffusion model conditioned on the embedding to generate natural-looking images. The resulting images remain visually natural and close to the original input, yet introduce subtle misleading cues that cause the model to hallucinate. We evaluate our method across a range of models, including reasoning models like GLM-4.1V-Thinking, and achieve a hallucination success rate exceeding 28%, compared to around 1% in prior data-driven discovery methods. We confirm that the generated images are both high-quality and object-free through quantitative metrics and human evaluation. Also, GHOST uncovers transferable vulnerabilities: images optimized for Qwen2.5-VL induce hallucinations in GPT-4o at a 66.5% rate. Finally, we show that fine-tuning on our images mitigates hallucination, positioning GHOST as both a diagnostic and corrective tool for building more reliable multimodal systems.

</details>


### [107] [DC-Gen: Post-Training Diffusion Acceleration with Deeply Compressed Latent Space](https://arxiv.org/abs/2509.25180)
*Wenkun He,Yuchao Gu,Junyu Chen,Dongyun Zou,Yujun Lin,Zhekai Zhang,Haocheng Xi,Muyang Li,Ligeng Zhu,Jincheng Yu,Junsong Chen,Enze Xie,Song Han,Han Cai*

Main category: cs.CV

TL;DR: DC-Gen是一个通过深度压缩潜在空间来加速文本到图像扩散模型的通用框架，在保持基础模型质量的同时显著提升生成速度。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型在高分辨率（如4K）生成时面临效率挑战，而现有研究很少处理潜在空间中的固有冗余问题。

Method: 采用后训练管道，首先通过轻量级嵌入对齐训练弥合基础模型潜在空间与深度压缩潜在空间之间的表示差距，然后仅需少量LoRA微调即可恢复生成质量。

Result: 在SANA和FLUX.1-Krea上验证有效，DC-Gen-FLUX在NVIDIA H100 GPU上实现4K图像生成延迟降低53倍，结合NVFP4 SVDQuant后可在单张NVIDIA 5090 GPU上3.5秒生成4K图像，总延迟降低138倍。

Conclusion: DC-Gen框架能够在不牺牲质量的前提下显著加速文本到图像扩散模型，为高分辨率图像生成提供了高效的解决方案。

Abstract: Existing text-to-image diffusion models excel at generating high-quality images, but face significant efficiency challenges when scaled to high resolutions, like 4K image generation. While previous research accelerates diffusion models in various aspects, it seldom handles the inherent redundancy within the latent space. To bridge this gap, this paper introduces DC-Gen, a general framework that accelerates text-to-image diffusion models by leveraging a deeply compressed latent space. Rather than a costly training-from-scratch approach, DC-Gen uses an efficient post-training pipeline to preserve the quality of the base model. A key challenge in this paradigm is the representation gap between the base model's latent space and a deeply compressed latent space, which can lead to instability during direct fine-tuning. To overcome this, DC-Gen first bridges the representation gap with a lightweight embedding alignment training. Once the latent embeddings are aligned, only a small amount of LoRA fine-tuning is needed to unlock the base model's inherent generation quality. We verify DC-Gen's effectiveness on SANA and FLUX.1-Krea. The resulting DC-Gen-SANA and DC-Gen-FLUX models achieve quality comparable to their base models but with a significant speedup. Specifically, DC-Gen-FLUX reduces the latency of 4K image generation by 53x on the NVIDIA H100 GPU. When combined with NVFP4 SVDQuant, DC-Gen-FLUX generates a 4K image in just 3.5 seconds on a single NVIDIA 5090 GPU, achieving a total latency reduction of 138x compared to the base FLUX.1-Krea model. Code: https://github.com/dc-ai-projects/DC-Gen.

</details>


### [108] [DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder](https://arxiv.org/abs/2509.25182)
*Junyu Chen,Wenkun He,Yuchao Gu,Yuyang Zhao,Jincheng Yu,Junsong Chen,Dongyun Zou,Yujun Lin,Zhekai Zhang,Muyang Li,Haocheng Xi,Ligeng Zhu,Enze Xie,Song Han,Han Cai*

Main category: cs.CV

TL;DR: DC-VideoGen是一个后训练加速框架，通过深度压缩潜在空间和轻量级微调，可将任何预训练视频扩散模型的推理延迟降低14.8倍，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成模型计算成本高，推理延迟大，限制了实际应用。需要一种高效的方法来加速预训练模型，同时保持生成质量。

Method: 1) 提出深度压缩视频自编码器，采用块因果时序设计，实现32x/64x空间和4x时间压缩；2) 开发AE-Adapt-V适应策略，快速稳定地将预训练模型迁移到新潜在空间。

Result: 仅需10个H100 GPU天即可完成模型适应，推理延迟降低高达14.8倍，可在单GPU上生成2160x3840分辨率视频，且不损失生成质量。

Conclusion: DC-VideoGen为视频生成模型提供了一种高效的后训练加速方案，显著提升了推理效率，具有重要的实际应用价值。

Abstract: We introduce DC-VideoGen, a post-training acceleration framework for efficient video generation. DC-VideoGen can be applied to any pre-trained video diffusion model, improving efficiency by adapting it to a deep compression latent space with lightweight fine-tuning. The framework builds on two key innovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal temporal design that achieves 32x/64x spatial and 4x temporal compression while preserving reconstruction quality and generalization to longer videos; and (ii) AE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer of pre-trained models into the new latent space. Adapting the pre-trained Wan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100 GPU. The accelerated models achieve up to 14.8x lower inference latency than their base counterparts without compromising quality, and further enable 2160x3840 video generation on a single GPU. Code: https://github.com/dc-ai-projects/DC-VideoGen.

</details>


### [109] [FlashI2V: Fourier-Guided Latent Shifting Prevents Conditional Image Leakage in Image-to-Video Generation](https://arxiv.org/abs/2509.25187)
*Yunyang Ge,Xinhua Cheng,Chengshu Zhao,Xianyi He,Shenghai Yuan,Bin Lin,Bin Zhu,Li Yuan*

Main category: cs.CV

TL;DR: FlashI2V通过潜在偏移和傅里叶引导解决I2V生成中的条件图像泄漏问题，在仅1.3B参数下在Vbench-I2V上取得53.01动态度分数，超越更大模型。


<details>
  <summary>Details</summary>
Motivation: 现有I2V方法存在条件图像泄漏问题，导致运动缓慢、颜色不一致等性能下降，且在域外数据上表现不佳。

Method: 提出FlashI2V方法：1）潜在偏移 - 通过从噪声潜在中减去条件图像信息来隐式整合条件；2）傅里叶引导 - 使用傅里叶变换的高频幅度特征加速收敛并调整细节级别。

Result: 在Vbench-I2V上动态度得分53.01，超越CogVideoX1.5-5B-I2V和Wan2.1-I2V-14B-480P等更大模型，在域外数据上表现最佳。

Conclusion: FlashI2V有效克服条件图像泄漏问题，在域外数据上具有最佳泛化能力和性能，仅需1.3B参数即可实现优越结果。

Abstract: In Image-to-Video (I2V) generation, a video is created using an input image as the first-frame condition. Existing I2V methods concatenate the full information of the conditional image with noisy latents to achieve high fidelity. However, the denoisers in these methods tend to shortcut the conditional image, which is known as conditional image leakage, leading to performance degradation issues such as slow motion and color inconsistency. In this work, we further clarify that conditional image leakage leads to overfitting to in-domain data and decreases the performance in out-of-domain scenarios. Moreover, we introduce Fourier-Guided Latent Shifting I2V, named FlashI2V, to prevent conditional image leakage. Concretely, FlashI2V consists of: (1) Latent Shifting. We modify the source and target distributions of flow matching by subtracting the conditional image information from the noisy latents, thereby incorporating the condition implicitly. (2) Fourier Guidance. We use high-frequency magnitude features obtained by the Fourier Transform to accelerate convergence and enable the adjustment of detail levels in the generated video. Experimental results show that our method effectively overcomes conditional image leakage and achieves the best generalization and performance on out-of-domain data among various I2V paradigms. With only 1.3B parameters, FlashI2V achieves a dynamic degree score of 53.01 on Vbench-I2V, surpassing CogVideoX1.5-5B-I2V and Wan2.1-I2V-14B-480P. Github page: https://pku-yuangroup.github.io/FlashI2V/

</details>


### [110] [VGGT-X: When VGGT Meets Dense Novel View Synthesis](https://arxiv.org/abs/2509.25191)
*Yang Liu,Chuanchen Luo,Zimo Tang,Junran Peng,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: VGGT-X解决了3D基础模型在密集新视角合成中的内存和输出质量问题，通过内存高效实现、自适应全局对齐和鲁棒训练方法，在无COLMAP初始化的情况下达到最先进效果。


<details>
  <summary>Details</summary>
Motivation: 现有新视角合成方法依赖SfM获取3D属性，过程缓慢且脆弱。3D基础模型虽有加速潜力，但在密集视图下存在内存负担大和输出不完善的问题。

Method: 提出VGGT-X，包含内存高效的VGGT实现（支持1000+图像）、自适应全局对齐增强VGGT输出、鲁棒3DGS训练实践。

Result: 实验表明这些方法显著缩小了与COLMAP初始化管线的质量差距，在密集无COLMAP新视角合成和姿态估计中达到最先进结果。

Conclusion: VGGT-X为3D基础模型和密集新视角合成的未来发展提供了重要见解，分析了与COLMAP初始化渲染的剩余差距原因。

Abstract: We study the problem of applying 3D Foundation Models (3DFMs) to dense Novel View Synthesis (NVS). Despite significant progress in Novel View Synthesis powered by NeRF and 3DGS, current approaches remain reliant on accurate 3D attributes (e.g., camera poses and point clouds) acquired from Structure-from-Motion (SfM), which is often slow and fragile in low-texture or low-overlap captures. Recent 3DFMs showcase orders of magnitude speedup over the traditional pipeline and great potential for online NVS. But most of the validation and conclusions are confined to sparse-view settings. Our study reveals that naively scaling 3DFMs to dense views encounters two fundamental barriers: dramatically increasing VRAM burden and imperfect outputs that degrade initialization-sensitive 3D training. To address these barriers, we introduce VGGT-X, incorporating a memory-efficient VGGT implementation that scales to 1,000+ images, an adaptive global alignment for VGGT output enhancement, and robust 3DGS training practices. Extensive experiments show that these measures substantially close the fidelity gap with COLMAP-initialized pipelines, achieving state-of-the-art results in dense COLMAP-free NVS and pose estimation. Additionally, we analyze the causes of remaining gaps with COLMAP-initialized rendering, providing insights for the future development of 3D foundation models and dense NVS. Our project page is available at https://dekuliutesla.github.io/vggt-x.github.io/

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [111] [StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data](https://arxiv.org/abs/2509.23594)
*Yixu Wang,Yan Teng,Yingchun Wang,Xingjun Ma*

Main category: cs.CR

TL;DR: 本文提出了一种针对LoRA适配模型的新型模型提取攻击方法StolenLoRA，利用合成数据和半监督学习策略，在仅使用1万次查询的情况下达到96.60%的攻击成功率，揭示了LoRA适配模型的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: LoRA等参数高效微调方法虽然提升了视觉模型适配的效率，但其紧凑性引入了新的安全隐患，特别是对模型提取攻击的脆弱性。本文旨在研究针对LoRA适配模型的提取攻击。

Method: 提出StolenLoRA攻击方法：1）利用大语言模型生成有效提示来合成数据；2）采用基于分歧的半监督学习策略最大化有限查询的信息增益；3）训练替代模型来提取LoRA适配模型的功能。

Result: 实验证明StolenLoRA的有效性：在仅1万次查询下攻击成功率高达96.60%，即使在攻击者和受害者模型使用不同预训练骨干网络的跨骨干场景下也表现良好。

Conclusion: 研究揭示了LoRA适配模型对此类提取攻击的特定脆弱性，迫切需要针对PEFT方法的鲁棒防御机制。初步探索了基于多样化LoRA部署的防御策略，显示出缓解此类攻击的潜力。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have transformed vision model adaptation, enabling the rapid deployment of customized models. However, the compactness of LoRA adaptations introduces new safety concerns, particularly their vulnerability to model extraction attacks. This paper introduces a new focus of model extraction attacks named LoRA extraction that extracts LoRA-adaptive models based on a public pre-trained model. We then propose a novel extraction method called StolenLoRA which trains a substitute model to extract the functionality of a LoRA-adapted model using synthetic data. StolenLoRA leverages a Large Language Model to craft effective prompts for data generation, and it incorporates a Disagreement-based Semi-supervised Learning (DSL) strategy to maximize information gain from limited queries. Our experiments demonstrate the effectiveness of StolenLoRA, achieving up to a 96.60% attack success rate with only 10k queries, even in cross-backbone scenarios where the attacker and victim models utilize different pre-trained backbones. These findings reveal the specific vulnerability of LoRA-adapted models to this type of extraction and underscore the urgent need for robust defense mechanisms tailored to PEFT methods. We also explore a preliminary defense strategy based on diversified LoRA deployments, highlighting its potential to mitigate such attacks.

</details>


### [112] [Of-SemWat: High-payload text embedding for semantic watermarking of AI-generated images with arbitrary size](https://arxiv.org/abs/2509.24823)
*Benedetta Tondi,Andrea Costanzo,Mauro Barni*

Main category: cs.CR

TL;DR: 提出一种用于文本嵌入的高载荷图像水印方法，将图像的语义描述嵌入到图像中，能够在大规模AI生成图像中鲁棒地嵌入高载荷信息。


<details>
  <summary>Details</summary>
Motivation: 现代AI生成器产生的大规模图像需要能够嵌入高载荷信息的水印方法，同时保持水印的不可感知性和对图像处理的鲁棒性。

Method: 基于传统水印方案，利用正交和turbo码提高鲁棒性，集成频域嵌入和感知掩蔽技术来增强水印的不可感知性。

Result: 实验表明该方法对多种图像处理具有极强的鲁棒性，即使在传统和AI修复后也能检索嵌入文本，通过图像-文本不匹配分析揭示图像的语义修改。

Conclusion: 该方法成功实现了高载荷文本嵌入，在保持水印不可感知的同时，对各类图像处理具有强鲁棒性，能够有效检测图像的语义修改。

Abstract: We propose a high-payload image watermarking method for textual embedding, where a semantic description of the image - which may also correspond to the input text prompt-, is embedded inside the image. In order to be able to robustly embed high payloads in large-scale images - such as those produced by modern AI generators - the proposed approach builds upon a traditional watermarking scheme that exploits orthogonal and turbo codes for improved robustness, and integrates frequency-domain embedding and perceptual masking techniques to enhance watermark imperceptibility. Experiments show that the proposed method is extremely robust against a wide variety of image processing, and the embedded text can be retrieved also after traditional and AI inpainting, permitting to unveil the semantic modification the image has undergone via image-text mismatch analysis.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [113] [Consistency Models as Plug-and-Play Priors for Inverse Problems](https://arxiv.org/abs/2509.22736)
*Merve Gülle,Junno Yun,Yaşar Utku Alçalar,Mehmet Akçakaya*

Main category: eess.IV

TL;DR: 提出PnP-CM方法，将一致性模型重新解释为先验的近端算子，集成到PnP-ADMM框架中，实现仅需2-4步NFE的高质量逆问题求解


<details>
  <summary>Details</summary>
Motivation: 现有基于一致性模型的逆问题求解器需要额外任务特定训练或使用收敛缓慢的数据保真度操作，不适合大规模问题

Method: 将一致性模型重新解释为先验的近端算子，结合PnP-ADMM框架，利用共轭梯度法快速收敛，并通过噪声注入和动量进一步加速

Result: 在修复、超分辨率、高斯去模糊和MRI重建等逆问题上，PnP-CM仅需4步NFE即可获得高质量重建，2步即可产生有意义结果，优于同类CM方法

Conclusion: PnP-CM在现实世界逆问题中表现出高效性，是首个针对MRI数据集训练的一致性模型，在少量NFE下实现高质量重建

Abstract: Diffusion models have found extensive use in solving numerous inverse problems. Such diffusion inverse problem solvers aim to sample from the posterior distribution of data given the measurements, using a combination of the unconditional score function and an approximation of the posterior related to the forward process. Recently, consistency models (CMs) have been proposed to directly predict the final output from any point on the diffusion ODE trajectory, enabling high-quality sampling in just a few NFEs. CMs have also been utilized for inverse problems, but existing CM-based solvers either require additional task-specific training or utilize data fidelity operations with slow convergence, not amenable to large-scale problems. In this work, we reinterpret CMs as proximal operators of a prior, enabling their integration into plug-and-play (PnP) frameworks. We propose a solver based on PnP-ADMM, which enables us to leverage the fast convergence of conjugate gradient method. We further accelerate this with noise injection and momentum, dubbed PnP-CM, and show it maintains the convergence properties of the baseline PnP-ADMM. We evaluate our approach on a variety of inverse problems, including inpainting, super-resolution, Gaussian deblurring, and magnetic resonance imaging (MRI) reconstruction. To the best of our knowledge, this is the first CM trained for MRI datasets. Our results show that PnP-CM achieves high-quality reconstructions in as few as 4 NFEs, and can produce meaningful results in 2 steps, highlighting its effectiveness in real-world inverse problems while outperforming comparable CM-based approaches.

</details>


### [114] [ReCon-GS: Continuum-Preserved Guassian Streaming for Fast and Compact Reconstruction of Dynamic Scenes](https://arxiv.org/abs/2509.24325)
*Jiaye Fu,Qiankun Gao,Chengxiang Wen,Yanmin Wu,Siwei Ma,Jiaqi Zhang,Jian Zhang*

Main category: eess.IV

TL;DR: ReCon-GS是一个存储感知的在线自由视点视频重建框架，通过多级锚点高斯动态分配和层次重构策略，在保证重建质量的同时显著降低内存需求。


<details>
  <summary>Details</summary>
Motivation: 解决在线自由视点视频重建中的三个主要挑战：逐帧优化速度慢、运动估计不一致性和存储需求不可持续。

Method: 1. 密度自适应方式动态分配多级锚点高斯来捕捉帧间几何变形；2. 动态层次重构策略通过按需锚点重层次化保持局部运动表达能力；3. 存储感知优化机制灵活调整不同层次锚点高斯的密度。

Result: 在三个广泛使用的数据集上，相比最先进方法，训练效率提升约15%，FVV合成质量更优，鲁棒性和稳定性增强。在同等渲染质量下，内存需求降低超过50%。

Conclusion: ReCon-GS通过创新的存储感知框架，成功解决了在线动态场景重建中的效率、一致性和存储挑战，实现了高质量实时渲染。

Abstract: Online free-viewpoint video (FVV) reconstruction is challenged by slow per-frame optimization, inconsistent motion estimation, and unsustainable storage demands. To address these challenges, we propose the Reconfigurable Continuum Gaussian Stream, dubbed ReCon-GS, a novel storage-aware framework that enables high fidelity online dynamic scene reconstruction and real-time rendering. Specifically, we dynamically allocate multi-level Anchor Gaussians in a density-adaptive fashion to capture inter-frame geometric deformations, thereby decomposing scene motion into compact coarse-to-fine representations. Then, we design a dynamic hierarchy reconfiguration strategy that preserves localized motion expressiveness through on-demand anchor re-hierarchization, while ensuring temporal consistency through intra-hierarchical deformation inheritance that confines transformation priors to their respective hierarchy levels. Furthermore, we introduce a storage-aware optimization mechanism that flexibly adjusts the density of Anchor Gaussians at different hierarchy levels, enabling a controllable trade-off between reconstruction fidelity and memory usage. Extensive experiments on three widely used datasets demonstrate that, compared to state-of-the-art methods, ReCon-GS improves training efficiency by approximately 15% and achieves superior FVV synthesis quality with enhanced robustness and stability. Moreover, at equivalent rendering quality, ReCon-GS slashes memory requirements by over 50% compared to leading state-of-the-art methods.

</details>


### [115] [Wavelet-Assisted Mamba for Satellite-Derived Sea Surface Temperature Super-Resolution](https://arxiv.org/abs/2509.24334)
*Wankun Chen,Feng Gao,Yanhai Gan,Jingchao Cao,Junyu Dong,Qian Du*

Main category: eess.IV

TL;DR: 提出了基于小波和Mamba的海表温度超分辨率框架WMSR，通过低频状态空间模块和高频增强模块分别处理全局信息和纹理细节，在三个SST数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 获取高分辨率海表温度数据具有挑战性，而基于状态空间模型的Mamba方法在长程依赖建模方面展现潜力，但在SST超分辨率中应用尚未充分探索。

Method: WMSR框架包含低频状态空间模块（使用2D-SSM捕获全局信息）和高频增强模块（使用像素差分卷积校正高频特征），结合小波变换处理不同频率分量。

Result: 在三个SST数据集上的综合实验表明，WMSR相比现有最先进方法表现出更优越的性能。

Conclusion: 提出的WMSR框架能够有效提升海表温度数据的超分辨率质量，代码和数据集将公开提供。

Abstract: Sea surface temperature (SST) is an essential indicator of global climate change and one of the most intuitive factors reflecting ocean conditions. Obtaining high-resolution SST data remains challenging due to limitations in physical imaging, and super-resolution via deep neural networks is a promising solution. Recently, Mamba-based approaches leveraging State Space Models (SSM) have demonstrated significant potential for long-range dependency modeling with linear complexity. However, their application to SST data super-resolution remains largely unexplored. To this end, we propose the Wavelet-assisted Mamba Super-Resolution (WMSR) framework for satellite-derived SST data. The WMSR includes two key components: the Low-Frequency State Space Module (LFSSM) and High-Frequency Enhancement Module (HFEM). The LFSSM uses 2D-SSM to capture global information of the input data, and the robust global modeling capabilities of SSM are exploited to preserve the critical temperature information in the low-frequency component. The HFEM employs the pixel difference convolution to match and correct the high-frequency feature, achieving accurate and clear textures. Through comprehensive experiments on three SST datasets, our WMSR demonstrated superior performance over state-of-the-art methods. Our codes and datasets will be made publicly available at https://github.com/oucailab/WMSR.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [116] [Discovering "Words" in Music: Unsupervised Learning of Compositional Sparse Code for Symbolic Music](https://arxiv.org/abs/2509.24603)
*Tianle Wang,Sirui Zhang,Xinyi Tong,Peiyang Yu,Jishang Chen,Liangke Zhao,Xinpu Gao,Yves Zhu,Tiezheng Ge,Bo Zheng,Duo Xu,Yang Liu,Xin Jin,Feng Yu,Songchun Zhu*

Main category: cs.SD

TL;DR: 提出一种无监督机器学习算法，从符号音乐数据中识别重复模式（称为"音乐词"），通过两阶段EM框架解决音乐语义模糊性问题，在人类专家标注上达到0.61 IoU分数。


<details>
  <summary>Details</summary>
Motivation: 音乐中的重复模式反映了作曲的认知过程，但由于音乐解释的语义模糊性，提取这些模式具有挑战性。

Method: 将音乐词发现建模为统计优化问题，采用两阶段EM学习框架：1. 开发音乐词词典；2. 重构音乐数据，通过最小化编码长度来解决语义模糊性。

Result: 算法在人类专家标注评估中达到0.61 IoU分数，表明最小化编码长度能有效解决语义模糊性，反映人类对编码系统的优化塑造了音乐语义。

Conclusion: 该方法使计算机能从音乐数据中提取"基本构建块"，支持AI音乐任务（生成、分类、风格迁移等）和音乐学分析，揭示了最小编码原则在不同音乐风格和作曲家中的应用。

Abstract: This paper presents an unsupervised machine learning algorithm that identifies recurring patterns -- referred to as ``music-words'' -- from symbolic music data. These patterns are fundamental to musical structure and reflect the cognitive processes involved in composition. However, extracting these patterns remains challenging because of the inherent semantic ambiguity in musical interpretation. We formulate the task of music-word discovery as a statistical optimization problem and propose a two-stage Expectation-Maximization (EM)-based learning framework: 1. Developing a music-word dictionary; 2. Reconstructing the music data. When evaluated against human expert annotations, the algorithm achieved an Intersection over Union (IoU) score of 0.61. Our findings indicate that minimizing code length effectively addresses semantic ambiguity, suggesting that human optimization of encoding systems shapes musical semantics. This approach enables computers to extract ``basic building blocks'' from music data, facilitating structural analysis and sparse encoding. The method has two primary applications. First, in AI music, it supports downstream tasks such as music generation, classification, style transfer, and improvisation. Second, in musicology, it provides a tool for analyzing compositional patterns and offers insights into the principle of minimal encoding across diverse musical styles and composers.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [117] [Localizing Adversarial Attacks To Produces More Imperceptible Noise](https://arxiv.org/abs/2509.22710)
*Pavan Reddy,Aditya Sanjay Gujral*

Main category: cs.LG

TL;DR: 该研究系统评估了局部对抗攻击的有效性、不可感知性和计算效率，发现局部攻击相比全局攻击具有更低的像素扰动和更高的图像质量指标，但计算成本更高且攻击成功率略有下降。


<details>
  <summary>Details</summary>
Motivation: 传统对抗攻击主要关注全局扰动，而局部对抗噪声的潜力尚未充分探索，本研究旨在系统评估局部对抗攻击的性能。

Method: 通过引入二元掩码将噪声限制在特定区域，在FGSM、PGD和C&W等广泛使用的方法上评估局部对抗攻击。

Result: 局部攻击实现了显著更低的平均像素扰动、更高的PSNR和SSIM，但计算工作量增加且攻击成功率略有降低。迭代方法(PGD、C&W)比单步方法(FGSM)对局部化约束更具鲁棒性。

Conclusion: 本研究提供了局部对抗攻击的全面分析，为改进攻击策略和设计鲁棒防御系统提供了实用见解。

Abstract: Adversarial attacks in machine learning traditionally focus on global perturbations to input data, yet the potential of localized adversarial noise remains underexplored. This study systematically evaluates localized adversarial attacks across widely-used methods, including FGSM, PGD, and C&W, to quantify their effectiveness, imperceptibility, and computational efficiency. By introducing a binary mask to constrain noise to specific regions, localized attacks achieve significantly lower mean pixel perturbations, higher Peak Signal-to-Noise Ratios (PSNR), and improved Structural Similarity Index (SSIM) compared to global attacks. However, these benefits come at the cost of increased computational effort and a modest reduction in Attack Success Rate (ASR). Our results highlight that iterative methods, such as PGD and C&W, are more robust to localization constraints than single-step methods like FGSM, maintaining higher ASR and imperceptibility metrics. This work provides a comprehensive analysis of localized adversarial attacks, offering practical insights for advancing attack strategies and designing robust defensive systems.

</details>


### [118] [SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention](https://arxiv.org/abs/2509.24006)
*Jintao Zhang,Haoxu Wang,Kai Jiang,Shuo Yang,Kaiwen Zheng,Haocheng Xi,Ziteng Wang,Hongzhou Zhu,Min Zhao,Ion Stoica,Joseph E. Gonzalez,Jun Zhu,Jianfei Chen*

Main category: cs.LG

TL;DR: SLA是一种融合稀疏和线性注意力的可训练注意力方法，通过将注意力权重分为关键、边际和可忽略三类，分别应用O(N²)、O(N)计算和跳过，实现了20倍注意力计算减少和2.2倍端到端加速，且不损失生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器(DiT)模型在视频生成中，由于长序列长度和二次复杂度，注意力延迟成为主要瓶颈。研究发现注意力权重可分为高秩的大权重和低秩的小权重两部分。

Method: 提出SLA方法，将注意力权重分类为关键、边际和可忽略三类，分别应用O(N²)注意力、O(N)注意力和跳过计算，并集成到单个GPU内核中，支持前向和反向传播。

Result: SLA将注意力计算减少95%，注意力计算加速13.7倍，视频生成端到端加速2.2倍，且不降低生成质量，优于基线方法。

Conclusion: SLA通过融合稀疏和线性注意力，有效加速扩散模型的注意力计算，在保持生成质量的同时实现显著性能提升。

Abstract: In Diffusion Transformer (DiT) models, particularly for video generation, attention latency is a major bottleneck due to the long sequence length and the quadratic complexity. We find that attention weights can be separated into two parts: a small fraction of large weights with high rank and the remaining weights with very low rank. This naturally suggests applying sparse acceleration to the first part and low-rank acceleration to the second. Based on this finding, we propose SLA (Sparse-Linear Attention), a trainable attention method that fuses sparse and linear attention to accelerate diffusion models. SLA classifies attention weights into critical, marginal, and negligible categories, applying O(N^2) attention to critical weights, O(N) attention to marginal weights, and skipping negligible ones. SLA combines these computations into a single GPU kernel and supports both forward and backward passes. With only a few fine-tuning steps using SLA, DiT models achieve a 20x reduction in attention computation, resulting in significant acceleration without loss of generation quality. Experiments show that SLA reduces attention computation by 95% without degrading end-to-end generation quality, outperforming baseline methods. In addition, we implement an efficient GPU kernel for SLA, which yields a 13.7x speedup in attention computation and a 2.2x end-to-end speedup in video generation on Wan2.1-1.3B.

</details>


### [119] [SAIP: A Plug-and-Play Scale-adaptive Module in Diffusion-based Inverse Problems](https://arxiv.org/abs/2509.24580)
*Lingyu Wang,Xiangming Meng*

Main category: cs.LG

TL;DR: 提出SAIP模块，自适应调整扩散模型中先验和似然项的平衡尺度，无需重新训练即可提升图像复原质量


<details>
  <summary>Details</summary>
Motivation: 现有方法使用固定的手动调优尺度来平衡先验和似然项，这种静态设计在时间步和任务间不是最优的，限制了性能和泛化能力

Method: SAIP是一个即插即用模块，在每个时间步自适应优化尺度参数，无需重新训练或修改扩散主干网络，可无缝集成到现有采样器中

Result: SAIP在多种图像复原任务中持续改善重建质量，包括具有挑战性的场景

Conclusion: SAIP通过自适应尺度调整解决了现有方法的局限性，为扩散模型在逆问题求解中提供了更优的平衡策略

Abstract: Solving inverse problems with diffusion models has shown promise in tasks such as image restoration. A common approach is to formulate the problem in a Bayesian framework and sample from the posterior by combining the prior score with the likelihood score. Since the likelihood term is often intractable, estimators like DPS, DMPS, and $\pi$GDM are widely adopted. However, these methods rely on a fixed, manually tuned scale to balance prior and likelihood contributions. Such a static design is suboptimal, as the ideal balance varies across timesteps and tasks, limiting performance and generalization. To address this issue, we propose SAIP, a plug-and-play module that adaptively refines the scale at each timestep without retraining or altering the diffusion backbone. SAIP integrates seamlessly into existing samplers and consistently improves reconstruction quality across diverse image restoration tasks, including challenging scenarios.

</details>


### [120] [Score-based Membership Inference on Diffusion Models](https://arxiv.org/abs/2509.25003)
*Mingxing Rao,Bowen Qu,Daniel Moyer*

Main category: cs.LG

TL;DR: 本文提出了一种基于分数的成员推理攻击SimA，通过分析扩散模型预测的噪声向量来检测训练样本成员身份，发现潜在扩散模型比像素空间模型更安全。


<details>
  <summary>Details</summary>
Motivation: 扩散模型可能无意中泄露训练数据集的成员信息，现有的成员推理攻击方法效率不高，需要更高效的理论基础方法。

Method: 提出SimA单查询攻击方法，基于扩散模型预测噪声向量的理论分析，利用去噪器输出向量的范数编码训练集邻近度信息。

Result: SimA在DDPM和潜在扩散模型上表现一致强劲，发现潜在扩散模型由于潜在自编码器的信息瓶颈而相对更安全。

Conclusion: 基于分数的成员推理攻击理论得到验证，潜在扩散方法需要对VAE反演有更好理解，而不仅仅是扩散过程的反演。

Abstract: Membership inference attacks (MIAs) against diffusion models have emerged as a pressing privacy concern, as these models may inadvertently reveal whether a given sample was part of their training set. We present a theoretical and empirical study of score-based MIAs, focusing on the predicted noise vectors that diffusion models learn to approximate. We show that the expected denoiser output points toward a kernel-weighted local mean of nearby training samples, such that its norm encodes proximity to the training set and thereby reveals membership. Building on this observation, we propose SimA, a single-query attack that provides a principled, efficient alternative to existing multi-query methods. SimA achieves consistently strong performance across variants of DDPM, Latent Diffusion Model (LDM). Notably, we find that Latent Diffusion Models are surprisingly less vulnerable than pixel-space models, due to the strong information bottleneck imposed by their latent auto-encoder. We further investigate this by differing the regularization hyperparameters ($\beta$ in $\beta$-VAE) in latent channel and suggest a strategy to make LDM training more robust to MIA. Our results solidify the theory of score-based MIAs, while highlighting that Latent Diffusion class of methods requires better understanding of inversion for VAE, and not simply inversion of the Diffusion process

</details>
