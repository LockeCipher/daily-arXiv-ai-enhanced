{"id": "2507.23006", "pdf": "https://arxiv.org/pdf/2507.23006", "abs": "https://arxiv.org/abs/2507.23006", "authors": ["Zhensheng Yuan", "Haozhi Huang", "Zhen Xiong", "Di Wang", "Guanghua Yang"], "title": "Robust and Efficient 3D Gaussian Splatting for Urban Scene Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "We present a framework that enables fast reconstruction and real-time rendering of urban-scale scenes while maintaining robustness against appearance variations across multi-view captures. Our approach begins with scene partitioning for parallel training, employing a visibility-based image selection strategy to optimize training efficiency. A controllable level-of-detail (LOD) strategy explicitly regulates Gaussian density under a user-defined budget, enabling efficient training and rendering while maintaining high visual fidelity. The appearance transformation module mitigates the negative effects of appearance inconsistencies across images while enabling flexible adjustments. Additionally, we utilize enhancement modules, such as depth regularization, scale regularization, and antialiasing, to improve reconstruction fidelity. Experimental results demonstrate that our method effectively reconstructs urban-scale scenes and outperforms previous approaches in both efficiency and quality. The source code is available at: https://yzslab.github.io/REUrbanGS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u91cd\u5efa\u548c\u5b9e\u65f6\u6e32\u67d3\u57ce\u5e02\u89c4\u6a21\u573a\u666f\u7684\u6846\u67b6\uff0c\u4f18\u5316\u8bad\u7ec3\u6548\u7387\u5e76\u4fdd\u6301\u591a\u89c6\u89d2\u6355\u83b7\u7684\u5916\u89c2\u53d8\u5316\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u57ce\u5e02\u89c4\u6a21\u573a\u666f\u91cd\u5efa\u4e2d\u8bad\u7ec3\u6548\u7387\u4f4e\u3001\u5916\u89c2\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u573a\u666f\u5206\u533a\u5e76\u884c\u8bad\u7ec3\uff0c\u57fa\u4e8e\u53ef\u89c1\u6027\u7684\u56fe\u50cf\u9009\u62e9\u7b56\u7565\uff0c\u53ef\u63a7\u7684LOD\u7b56\u7565\u8c03\u8282\u9ad8\u65af\u5bc6\u5ea6\uff0c\u5916\u89c2\u53d8\u6362\u6a21\u5757\u5904\u7406\u4e0d\u4e00\u81f4\u6027\uff0c\u589e\u5f3a\u6a21\u5757\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u9ad8\u6548\u91cd\u5efa\u57ce\u5e02\u89c4\u6a21\u573a\u666f\uff0c\u5728\u6548\u7387\u548c\u8d28\u91cf\u4e0a\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u57ce\u5e02\u89c4\u6a21\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.23027", "pdf": "https://arxiv.org/pdf/2507.23027", "abs": "https://arxiv.org/abs/2507.23027", "authors": ["Krishan Agyakari Raja Babu", "Om Prabhu", "Annu", "Mohanasankar Sivaprakasam"], "title": "Recovering Diagnostic Value: Super-Resolution-Aided Echocardiographic Classification in Resource-Constrained Imaging", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at the MICCAI Workshop on \"Medical Image Computing in   Resource Constrained Settings & Knowledge Interchange (MIRASOL)\" 2025", "summary": "Automated cardiac interpretation in resource-constrained settings (RCS) is often hindered by poor-quality echocardiographic imaging, limiting the effectiveness of downstream diagnostic models. While super-resolution (SR) techniques have shown promise in enhancing magnetic resonance imaging (MRI) and computed tomography (CT) scans, their application to echocardiography-a widely accessible but noise-prone modality-remains underexplored. In this work, we investigate the potential of deep learning-based SR to improve classification accuracy on low-quality 2D echocardiograms. Using the publicly available CAMUS dataset, we stratify samples by image quality and evaluate two clinically relevant tasks of varying complexity: a relatively simple Two-Chamber vs. Four-Chamber (2CH vs. 4CH) view classification and a more complex End-Diastole vs. End-Systole (ED vs. ES) phase classification. We apply two widely used SR models-Super-Resolution Generative Adversarial Network (SRGAN) and Super-Resolution Residual Network (SRResNet), to enhance poor-quality images and observe significant gains in performance metric-particularly with SRResNet, which also offers computational efficiency. Our findings demonstrate that SR can effectively recover diagnostic value in degraded echo scans, making it a viable tool for AI-assisted care in RCS, achieving more with less.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6df1\u5ea6\u5b66\u4e60\u8d85\u5206\u8fa8\u7387\u6280\u672f\uff08SR\uff09\u5728\u63d0\u5347\u4f4e\u8d28\u91cf\u8d85\u58f0\u5fc3\u52a8\u56fe\u5206\u7c7b\u51c6\u786e\u6027\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\u4e2d\u3002", "motivation": "\u5728\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\u4e2d\uff0c\u4f4e\u8d28\u91cf\u7684\u8d85\u58f0\u5fc3\u52a8\u56fe\u9650\u5236\u4e86\u8bca\u65ad\u6a21\u578b\u7684\u6548\u80fd\uff0c\u800c\u8d85\u5206\u8fa8\u7387\u6280\u672f\u5728\u6b64\u9886\u57df\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528CAMUS\u6570\u636e\u96c6\uff0c\u6309\u56fe\u50cf\u8d28\u91cf\u5206\u5c42\uff0c\u8bc4\u4f30\u4e86\u4e24\u79cdSR\u6a21\u578b\uff08SRGAN\u548cSRResNet\uff09\u5728\u4e24\u79cd\u4e34\u5e8a\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "SRResNet\u5728\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548c\u5206\u7c7b\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u663e\u8457\uff0c\u4e14\u8ba1\u7b97\u9ad8\u6548\u3002", "conclusion": "\u8d85\u5206\u8fa8\u7387\u6280\u672f\u80fd\u6709\u6548\u6062\u590d\u4f4e\u8d28\u91cf\u8d85\u58f0\u5fc3\u52a8\u56fe\u7684\u8bca\u65ad\u4ef7\u503c\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u73af\u5883\u4e2d\u7684AI\u8f85\u52a9\u8bca\u65ad\u3002"}}
{"id": "2507.23033", "pdf": "https://arxiv.org/pdf/2507.23033", "abs": "https://arxiv.org/abs/2507.23033", "authors": ["Ranxi Lin", "Canming Yao", "Jiayi Li", "Weihang Liu", "Xin Lou", "Pingqiang Zhou"], "title": "Adaptive Time-step Training for Enhancing Spike-Based Neural Radiance Fields", "categories": ["cs.CV", "cs.NE"], "comment": null, "summary": "Neural Radiance Fields (NeRF)-based models have achieved remarkable success in 3D reconstruction and rendering tasks. However, during both training and inference, these models rely heavily on dense point sampling along rays from multiple viewpoints, resulting in a surge in floating-point operations and severely limiting their use in resource-constrained scenarios like edge computing. Spiking Neural Networks (SNNs), which communicate via binary spikes over discrete time steps, offer a promising alternative due to their energy-efficient nature. Given the inherent variability in scene scale and texture complexity in neural rendering and the prevailing practice of training separate models per scene, we propose a spike-based NeRF framework with a dynamic time step training strategy, termed Pretrain-Adaptive Time-step Adjustment (PATA). This approach automatically explores the trade-off between rendering quality and time step length during training. Consequently, it enables scene-adaptive inference with variable time steps and reduces the additional consumption of computational resources in the inference process. Anchoring to the established Instant-NGP architecture, we evaluate our method across diverse datasets. The experimental results show that PATA can preserve rendering fidelity while reducing inference time steps by 64\\% and running power by 61.55\\%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u52a8\u6001\u65f6\u95f4\u6b65\u957f\u8bad\u7ec3\u7b56\u7565\uff08PATA\uff09\uff0c\u7528\u4e8e\u4f18\u5316NeRF\u6a21\u578b\u7684\u8d44\u6e90\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\u3002", "motivation": "NeRF\u6a21\u578b\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u4e2d\u4f9d\u8d56\u5bc6\u96c6\u70b9\u91c7\u6837\uff0c\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u9650\u5236\u4e86\u5176\u5728\u8fb9\u7f18\u8ba1\u7b97\u7b49\u8d44\u6e90\u53d7\u9650\u573a\u666f\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faPATA\u7b56\u7565\uff0c\u7ed3\u5408\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u7684\u52a8\u6001\u65f6\u95f4\u6b65\u957f\u8c03\u6574\uff0c\u81ea\u52a8\u5e73\u8861\u6e32\u67d3\u8d28\u91cf\u548c\u65f6\u95f4\u6b65\u957f\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cPATA\u5728\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u6b65\u957f64%\u548c\u8fd0\u884c\u529f\u801761.55%\u3002", "conclusion": "PATA\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684NeRF\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.23058", "pdf": "https://arxiv.org/pdf/2507.23058", "abs": "https://arxiv.org/abs/2507.23058", "authors": ["Alexandru Buburuzan"], "title": "Reference-Guided Diffusion Inpainting For Multimodal Counterfactual Generation", "categories": ["cs.CV", "cs.AI"], "comment": "A dissertation submitted to The University of Manchester for the   degree of Bachelor of Science in Artificial Intelligence", "summary": "Safety-critical applications, such as autonomous driving and medical image analysis, require extensive multimodal data for rigorous testing. Synthetic data methods are gaining prominence due to the cost and complexity of gathering real-world data, but they demand a high degree of realism and controllability to be useful. This work introduces two novel methods for synthetic data generation in autonomous driving and medical image analysis, namely MObI and AnydoorMed, respectively. MObI is a first-of-its-kind framework for Multimodal Object Inpainting that leverages a diffusion model to produce realistic and controllable object inpaintings across perceptual modalities, demonstrated simultaneously for camera and lidar. Given a single reference RGB image, MObI enables seamless object insertion into existing multimodal scenes at a specified 3D location, guided by a bounding box, while maintaining semantic consistency and multimodal coherence. Unlike traditional inpainting methods that rely solely on edit masks, this approach uses 3D bounding box conditioning to ensure accurate spatial positioning and realistic scaling. AnydoorMed extends this paradigm to the medical imaging domain, focusing on reference-guided inpainting for mammography scans. It leverages a diffusion-based model to inpaint anomalies with impressive detail preservation, maintaining the reference anomaly's structural integrity while semantically blending it with the surrounding tissue. Together, these methods demonstrate that foundation models for reference-guided inpainting in natural images can be readily adapted to diverse perceptual modalities, paving the way for the next generation of systems capable of constructing highly realistic, controllable and multimodal counterfactual scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u578b\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5MObI\u548cAnydoorMed\uff0c\u5206\u522b\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u548c\u533b\u5b66\u5f71\u50cf\u5206\u6790\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u9ad8\u771f\u5b9e\u611f\u548c\u53ef\u63a7\u6027\u3002", "motivation": "\u7531\u4e8e\u771f\u5b9e\u6570\u636e\u91c7\u96c6\u6210\u672c\u9ad8\u4e14\u590d\u6742\uff0c\u5408\u6210\u6570\u636e\u65b9\u6cd5\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u9700\u8981\u9ad8\u771f\u5b9e\u611f\u548c\u53ef\u63a7\u6027\u3002", "method": "MObI\u5229\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u591a\u6a21\u6001\u5bf9\u8c61\u4fee\u590d\uff0c\u652f\u6301\u76f8\u673a\u548c\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\uff1bAnydoorMed\u4e13\u6ce8\u4e8e\u533b\u5b66\u5f71\u50cf\u7684\u53c2\u8003\u5f15\u5bfc\u4fee\u590d\u3002", "result": "\u4e24\u79cd\u65b9\u6cd5\u5747\u80fd\u751f\u6210\u9ad8\u5ea6\u771f\u5b9e\u3001\u53ef\u63a7\u7684\u591a\u6a21\u6001\u5408\u6210\u6570\u636e\uff0c\u652f\u6301\u590d\u6742\u7684\u53cd\u4e8b\u5b9e\u573a\u666f\u6784\u5efa\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u53c2\u8003\u5f15\u5bfc\u4fee\u590d\u65b9\u6cd5\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u4e0d\u540c\u611f\u77e5\u6a21\u6001\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u9ad8\u771f\u5b9e\u611f\u5408\u6210\u6570\u636e\u7cfb\u7edf\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2507.23143", "pdf": "https://arxiv.org/pdf/2507.23143", "abs": "https://arxiv.org/abs/2507.23143", "authors": ["Xiaochen Zhao", "Hongyi Xu", "Guoxian Song", "You Xie", "Chenxu Zhang", "Xiu Li", "Linjie Luo", "Jinli Suo", "Yebin Liu"], "title": "X-NeMo: Expressive Neural Motion Reenactment via Disentangled Latent Attention", "categories": ["cs.CV"], "comment": "ICLR 2025, code is available at   https://github.com/bytedance/x-nemo-inference", "summary": "We propose X-NeMo, a novel zero-shot diffusion-based portrait animation pipeline that animates a static portrait using facial movements from a driving video of a different individual. Our work first identifies the root causes of the key issues in prior approaches, such as identity leakage and difficulty in capturing subtle and extreme expressions. To address these challenges, we introduce a fully end-to-end training framework that distills a 1D identity-agnostic latent motion descriptor from driving image, effectively controlling motion through cross-attention during image generation. Our implicit motion descriptor captures expressive facial motion in fine detail, learned end-to-end from a diverse video dataset without reliance on pretrained motion detectors. We further enhance expressiveness and disentangle motion latents from identity cues by supervising their learning with a dual GAN decoder, alongside spatial and color augmentations. By embedding the driving motion into a 1D latent vector and controlling motion via cross-attention rather than additive spatial guidance, our design eliminates the transmission of spatial-aligned structural clues from the driving condition to the diffusion backbone, substantially mitigating identity leakage. Extensive experiments demonstrate that X-NeMo surpasses state-of-the-art baselines, producing highly expressive animations with superior identity resemblance. Our code and models are available for research.", "AI": {"tldr": "X-NeMo\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u96f6\u6837\u672c\u8096\u50cf\u52a8\u753b\u65b9\u6cd5\uff0c\u901a\u8fc7\u9a71\u52a8\u89c6\u9891\u7684\u9762\u90e8\u52a8\u4f5c\u751f\u6210\u9759\u6001\u8096\u50cf\u7684\u52a8\u753b\uff0c\u89e3\u51b3\u4e86\u8eab\u4efd\u6cc4\u6f0f\u548c\u8868\u60c5\u6355\u6349\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u8eab\u4efd\u6cc4\u6f0f\u95ee\u9898\u4ee5\u53ca\u96be\u4ee5\u6355\u6349\u7ec6\u5fae\u548c\u6781\u7aef\u8868\u60c5\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7aef\u5230\u7aef\u8bad\u7ec3\u6846\u67b6\uff0c\u4ece\u9a71\u52a8\u56fe\u50cf\u4e2d\u63d0\u53d61D\u8eab\u4efd\u65e0\u5173\u7684\u6f5c\u5728\u8fd0\u52a8\u63cf\u8ff0\u7b26\uff0c\u5e76\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u63a7\u5236\u56fe\u50cf\u751f\u6210\u3002", "result": "X-NeMo\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u9ad8\u8868\u73b0\u529b\u4e14\u8eab\u4efd\u76f8\u4f3c\u7684\u52a8\u753b\u3002", "conclusion": "X-NeMo\u901a\u8fc7\u521b\u65b0\u7684\u8fd0\u52a8\u63cf\u8ff0\u7b26\u548c\u8bad\u7ec3\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8096\u50cf\u52a8\u753b\u7684\u8d28\u91cf\u548c\u8eab\u4efd\u4fdd\u6301\u80fd\u529b\u3002"}}
{"id": "2507.23150", "pdf": "https://arxiv.org/pdf/2507.23150", "abs": "https://arxiv.org/abs/2507.23150", "authors": ["Philip Wootaek Shin", "Vishal Gaur", "Rahul Ramachandran", "Manil Maskey", "Jack Sampson", "Vijaykrishnan Narayanan", "Sujit Roy"], "title": "Towards High-Resolution Alignment and Super-Resolution of Multi-Sensor Satellite Imagery", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "High-resolution satellite imagery is essential for geospatial analysis, yet differences in spatial resolution across satellite sensors present challenges for data fusion and downstream applications. Super-resolution techniques can help bridge this gap, but existing methods rely on artificially downscaled images rather than real sensor data and are not well suited for heterogeneous satellite sensors with differing spectral, temporal characteristics. In this work, we develop a preliminary framework to align and Harmonized Landsat Sentinel 30m(HLS 30) imagery using Harmonized Landsat Sentinel 10m(HLS10) as a reference from the HLS dataset. Our approach aims to bridge the resolution gap between these sensors and improve the quality of super-resolved Landsat imagery. Quantitative and qualitative evaluations demonstrate the effectiveness of our method, showing its potential for enhancing satellite-based sensing applications. This study provides insights into the feasibility of heterogeneous satellite image super-resolution and highlights key considerations for future advancements in the field.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521d\u6b65\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u9f50\u548c\u534f\u8c0330\u7c73\u5206\u8fa8\u7387\u7684HLS\u5f71\u50cf\u4e0e10\u7c73\u5206\u8fa8\u7387\u7684HLS\u5f71\u50cf\uff0c\u4ee5\u6539\u5584\u8d85\u5206\u8fa8\u7387Landsat\u5f71\u50cf\u7684\u8d28\u91cf\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u5f71\u50cf\u5bf9\u5730\u7406\u7a7a\u95f4\u5206\u6790\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4e0d\u540c\u536b\u661f\u4f20\u611f\u5668\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\u5dee\u5f02\u7ed9\u6570\u636e\u878d\u5408\u548c\u4e0b\u6e38\u5e94\u7528\u5e26\u6765\u6311\u6218\u3002\u73b0\u6709\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u964d\u5c3a\u5ea6\u56fe\u50cf\uff0c\u4e0d\u9002\u7528\u4e8e\u5177\u6709\u4e0d\u540c\u5149\u8c31\u548c\u65f6\u95f4\u7279\u6027\u7684\u5f02\u8d28\u536b\u661f\u4f20\u611f\u5668\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u4ee5HLS10\u4e3a\u53c2\u8003\uff0c\u5bf9\u9f50\u548c\u534f\u8c03HLS30\u5f71\u50cf\uff0c\u65e8\u5728\u5f25\u5408\u4f20\u611f\u5668\u4e4b\u95f4\u7684\u5206\u8fa8\u7387\u5dee\u8ddd\u3002", "result": "\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u793a\u4e86\u5176\u5728\u589e\u5f3a\u536b\u661f\u9065\u611f\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f02\u8d28\u536b\u661f\u5f71\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u53ef\u884c\u6027\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u53d1\u5c55\u7684\u5173\u952e\u8003\u8651\u56e0\u7d20\u3002"}}
{"id": "2507.23162", "pdf": "https://arxiv.org/pdf/2507.23162", "abs": "https://arxiv.org/abs/2507.23162", "authors": ["Xu Cao", "Takafumi Taketomi"], "title": "Neural Multi-View Self-Calibrated Photometric Stereo without Photometric Stereo Cues", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "We propose a neural inverse rendering approach that jointly reconstructs geometry, spatially varying reflectance, and lighting conditions from multi-view images captured under varying directional lighting. Unlike prior multi-view photometric stereo methods that require light calibration or intermediate cues such as per-view normal maps, our method jointly optimizes all scene parameters from raw images in a single stage. We represent both geometry and reflectance as neural implicit fields and apply shadow-aware volume rendering. A spatial network first predicts the signed distance and a reflectance latent code for each scene point. A reflectance network then estimates reflectance values conditioned on the latent code and angularly encoded surface normal, view, and light directions. The proposed method outperforms state-of-the-art normal-guided approaches in shape and lighting estimation accuracy, generalizes to view-unaligned multi-light images, and handles objects with challenging geometry and reflectance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u9006\u5411\u6e32\u67d3\u65b9\u6cd5\uff0c\u4ece\u591a\u89c6\u89d2\u56fe\u50cf\u4e2d\u8054\u5408\u91cd\u5efa\u51e0\u4f55\u3001\u7a7a\u95f4\u53d8\u5316\u53cd\u5c04\u7387\u548c\u5149\u7167\u6761\u4ef6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5149\u7167\u6821\u51c6\u6216\u4e2d\u95f4\u7ebf\u7d22\uff08\u5982\u6bcf\u89c6\u89d2\u6cd5\u7ebf\u56fe\uff09\uff0c\u800c\u672c\u6587\u65b9\u6cd5\u76f4\u63a5\u4ece\u539f\u59cb\u56fe\u50cf\u4e2d\u8054\u5408\u4f18\u5316\u6240\u6709\u573a\u666f\u53c2\u6570\u3002", "method": "\u4f7f\u7528\u795e\u7ecf\u9690\u5f0f\u573a\u8868\u793a\u51e0\u4f55\u548c\u53cd\u5c04\u7387\uff0c\u7ed3\u5408\u9634\u5f71\u611f\u77e5\u4f53\u79ef\u6e32\u67d3\u3002\u7a7a\u95f4\u7f51\u7edc\u9884\u6d4b\u6709\u7b26\u53f7\u8ddd\u79bb\u548c\u53cd\u5c04\u7387\u6f5c\u5728\u7801\uff0c\u53cd\u5c04\u7387\u7f51\u7edc\u57fa\u4e8e\u6f5c\u5728\u7801\u548c\u89d2\u5ea6\u7f16\u7801\u7684\u65b9\u5411\u4f30\u8ba1\u53cd\u5c04\u7387\u3002", "result": "\u5728\u5f62\u72b6\u548c\u5149\u7167\u4f30\u8ba1\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u89c6\u89d2\u672a\u5bf9\u9f50\u7684\u591a\u5149\u6e90\u56fe\u50cf\uff0c\u5e76\u80fd\u5904\u7406\u590d\u6742\u51e0\u4f55\u548c\u53cd\u5c04\u7387\u7269\u4f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u4e2d\u95f4\u7ebf\u7d22\uff0c\u76f4\u63a5\u4ece\u56fe\u50cf\u4e2d\u8054\u5408\u4f18\u5316\u573a\u666f\u53c2\u6570\uff0c\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2507.23185", "pdf": "https://arxiv.org/pdf/2507.23185", "abs": "https://arxiv.org/abs/2507.23185", "authors": ["Jongwook Si", "Sungyoung Kim"], "title": "Single Image Rain Streak Removal Using Harris Corner Loss and R-CBAM Network", "categories": ["cs.CV", "eess.IV"], "comment": "21 pages", "summary": "The problem of single-image rain streak removal goes beyond simple noise suppression, requiring the simultaneous preservation of fine structural details and overall visual quality. In this study, we propose a novel image restoration network that effectively constrains the restoration process by introducing a Corner Loss, which prevents the loss of object boundaries and detailed texture information during restoration. Furthermore, we propose a Residual Convolutional Block Attention Module (R-CBAM) Block into the encoder and decoder to dynamically adjust the importance of features in both spatial and channel dimensions, enabling the network to focus more effectively on regions heavily affected by rain streaks. Quantitative evaluations conducted on the Rain100L and Rain100H datasets demonstrate that the proposed method significantly outperforms previous approaches, achieving a PSNR of 33.29 dB on Rain100L and 26.16 dB on Rain100H.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Corner Loss\u548cR-CBAM\u6a21\u5757\u7684\u56fe\u50cf\u53bb\u96e8\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53bb\u96e8\u6548\u679c\u3002", "motivation": "\u5355\u56fe\u50cf\u53bb\u96e8\u95ee\u9898\u4e0d\u4ec5\u9700\u8981\u6291\u5236\u566a\u58f0\uff0c\u8fd8\u9700\u4fdd\u7559\u7ec6\u8282\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "method": "\u5f15\u5165Corner Loss\u9632\u6b62\u8fb9\u754c\u548c\u7ec6\u8282\u4e22\u5931\uff0c\u5e76\u4f7f\u7528R-CBAM\u6a21\u5757\u52a8\u6001\u8c03\u6574\u7279\u5f81\u91cd\u8981\u6027\u3002", "result": "\u5728Rain100L\u548cRain100H\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523033.29 dB\u548c26.16 dB\u7684PSNR\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u53bb\u96e8\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.23202", "pdf": "https://arxiv.org/pdf/2507.23202", "abs": "https://arxiv.org/abs/2507.23202", "authors": ["Chengwei Xia", "Fan Ma", "Ruijie Quan", "Kun Zhan", "Yi Yang"], "title": "Adversarial-Guided Diffusion for Multimodal LLM Attacks", "categories": ["cs.CV"], "comment": null, "summary": "This paper addresses the challenge of generating adversarial image using a diffusion model to deceive multimodal large language models (MLLMs) into generating the targeted responses, while avoiding significant distortion of the clean image. To address the above challenges, we propose an adversarial-guided diffusion (AGD) approach for adversarial attack MLLMs. We introduce adversarial-guided noise to ensure attack efficacy. A key observation in our design is that, unlike most traditional adversarial attacks which embed high-frequency perturbations directly into the clean image, AGD injects target semantics into the noise component of the reverse diffusion. Since the added noise in a diffusion model spans the entire frequency spectrum, the adversarial signal embedded within it also inherits this full-spectrum property. Importantly, during reverse diffusion, the adversarial image is formed as a linear combination of the clean image and the noise. Thus, when applying defenses such as a simple low-pass filtering, which act independently on each component, the adversarial image within the noise component is less likely to be suppressed, as it is not confined to the high-frequency band. This makes AGD inherently robust to variety defenses. Extensive experiments demonstrate that our AGD outperforms state-of-the-art methods in attack performance as well as in model robustness to some defenses.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff08AGD\uff09\uff0c\u901a\u8fc7\u5c06\u76ee\u6807\u8bed\u4e49\u6ce8\u5165\u53cd\u5411\u6269\u6563\u7684\u566a\u58f0\u4e2d\uff0c\u6709\u6548\u6b3a\u9a97\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u901a\u5e38\u5728\u9ad8\u9891\u6bb5\u5d4c\u5165\u6270\u52a8\uff0c\u5bb9\u6613\u88ab\u9632\u5fa1\u63aa\u65bd\uff08\u5982\u4f4e\u901a\u6ee4\u6ce2\uff09\u6291\u5236\u3002AGD\u65e8\u5728\u901a\u8fc7\u5168\u9891\u8c31\u566a\u58f0\u5d4c\u5165\u5bf9\u6297\u4fe1\u53f7\uff0c\u63d0\u9ad8\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u5bf9\u6297\u5f15\u5bfc\u6269\u6563\uff08AGD\uff09\u65b9\u6cd5\uff0c\u5c06\u76ee\u6807\u8bed\u4e49\u6ce8\u5165\u53cd\u5411\u6269\u6563\u7684\u566a\u58f0\u4e2d\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u5168\u9891\u8c31\u7279\u6027\uff0c\u4f7f\u5bf9\u6297\u4fe1\u53f7\u4e0d\u6613\u88ab\u9632\u5fa1\u63aa\u65bd\u6291\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAGD\u5728\u653b\u51fb\u6027\u80fd\u548c\u5bf9\u6297\u9632\u5fa1\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AGD\u901a\u8fc7\u5168\u9891\u8c31\u566a\u58f0\u5d4c\u5165\u5bf9\u6297\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2507.23268", "pdf": "https://arxiv.org/pdf/2507.23268", "abs": "https://arxiv.org/abs/2507.23268", "authors": ["Shuai Wang", "Ziteng Gao", "Chenhui Zhu", "Weilin Huang", "Limin Wang"], "title": "PixNerd: Pixel Neural Field Diffusion", "categories": ["cs.CV"], "comment": "a single-scale, single-stage, efficient, end-to-end pixel space   diffusion model", "summary": "The current success of diffusion transformers heavily depends on the compressed latent space shaped by the pre-trained variational autoencoder(VAE). However, this two-stage training paradigm inevitably introduces accumulated errors and decoding artifacts. To address the aforementioned problems, researchers return to pixel space at the cost of complicated cascade pipelines and increased token complexity. In contrast to their efforts, we propose to model the patch-wise decoding with neural field and present a single-scale, single-stage, efficient, end-to-end solution, coined as pixel neural field diffusion~(PixelNerd). Thanks to the efficient neural field representation in PixNerd, we directly achieved 2.15 FID on ImageNet $256\\times256$ and 2.84 FID on ImageNet $512\\times512$ without any complex cascade pipeline or VAE. We also extend our PixNerd framework to text-to-image applications. Our PixNerd-XXL/16 achieved a competitive 0.73 overall score on the GenEval benchmark and 80.9 overall score on the DPG benchmark.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPixelNerd\u7684\u5355\u5c3a\u5ea6\u3001\u5355\u9636\u6bb5\u3001\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u795e\u7ecf\u573a\u5efa\u6a21\u8865\u4e01\u89e3\u7801\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u4e24\u9636\u6bb5\u8bad\u7ec3\u4e2d\u7684\u7d2f\u79ef\u8bef\u5dee\u548c\u89e3\u7801\u4f2a\u5f71\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6269\u6563\u53d8\u6362\u5668\u4e2d\u56e0\u9884\u8bad\u7ec3\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u538b\u7f29\u6f5c\u5728\u7a7a\u95f4\u5f15\u5165\u7684\u7d2f\u79ef\u8bef\u5dee\u548c\u89e3\u7801\u4f2a\u5f71\u95ee\u9898\u3002", "method": "\u63d0\u51faPixelNerd\u6846\u67b6\uff0c\u5229\u7528\u795e\u7ecf\u573a\u8868\u793a\u76f4\u63a5\u5efa\u6a21\u50cf\u7d20\u7a7a\u95f4\u4e2d\u7684\u8865\u4e01\u89e3\u7801\uff0c\u65e0\u9700\u590d\u6742\u7ea7\u8054\u6d41\u7a0b\u6216VAE\u3002", "result": "\u5728ImageNet 256\u00d7256\u548c512\u00d7512\u4e0a\u5206\u522b\u8fbe\u52302.15\u548c2.84 FID\uff0c\u540c\u65f6\u5728\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "PixelNerd\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u590d\u6742\u6027\u3002"}}
{"id": "2507.23277", "pdf": "https://arxiv.org/pdf/2507.23277", "abs": "https://arxiv.org/abs/2507.23277", "authors": ["Gyeongjin Kang", "Seungtae Nam", "Xiangyu Sun", "Sameh Khamis", "Abdelrahman Mohamed", "Eunbyung Park"], "title": "iLRM: An Iterative Large 3D Reconstruction Model", "categories": ["cs.CV"], "comment": "Project page: https://gynjn.github.io/iLRM/", "summary": "Feed-forward 3D modeling has emerged as a promising approach for rapid and high-quality 3D reconstruction. In particular, directly generating explicit 3D representations, such as 3D Gaussian splatting, has attracted significant attention due to its fast and high-quality rendering, as well as numerous applications. However, many state-of-the-art methods, primarily based on transformer architectures, suffer from severe scalability issues because they rely on full attention across image tokens from multiple input views, resulting in prohibitive computational costs as the number of views or image resolution increases. Toward a scalable and efficient feed-forward 3D reconstruction, we introduce an iterative Large 3D Reconstruction Model (iLRM) that generates 3D Gaussian representations through an iterative refinement mechanism, guided by three core principles: (1) decoupling the scene representation from input-view images to enable compact 3D representations; (2) decomposing fully-attentional multi-view interactions into a two-stage attention scheme to reduce computational costs; and (3) injecting high-resolution information at every layer to achieve high-fidelity reconstruction. Experimental results on widely used datasets, such as RE10K and DL3DV, demonstrate that iLRM outperforms existing methods in both reconstruction quality and speed. Notably, iLRM exhibits superior scalability, delivering significantly higher reconstruction quality under comparable computational cost by efficiently leveraging a larger number of input views.", "AI": {"tldr": "iLRM\u63d0\u51fa\u4e86\u4e00\u79cd\u8fed\u4ee3\u5f0f\u5927\u89c4\u6a213D\u91cd\u5efa\u6a21\u578b\uff0c\u901a\u8fc7\u89e3\u8026\u573a\u666f\u8868\u793a\u3001\u5206\u89e3\u591a\u89c6\u56fe\u4ea4\u4e92\u548c\u6ce8\u5165\u9ad8\u5206\u8fa8\u7387\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u56e0\u5168\u6ce8\u610f\u529b\u673a\u5236\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u96be\u4ee5\u6269\u5c55\u3002iLRM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u76843D\u91cd\u5efa\u3002", "method": "iLRM\u91c7\u7528\u8fed\u4ee3\u7ec6\u5316\u673a\u5236\uff0c\u89e3\u8026\u573a\u666f\u8868\u793a\uff0c\u5206\u89e3\u591a\u89c6\u56fe\u4ea4\u4e92\u4e3a\u4e24\u9636\u6bb5\u6ce8\u610f\u529b\uff0c\u5e76\u5728\u6bcf\u5c42\u6ce8\u5165\u9ad8\u5206\u8fa8\u7387\u4fe1\u606f\u3002", "result": "\u5728RE10K\u548cDL3DV\u6570\u636e\u96c6\u4e0a\uff0ciLRM\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5177\u6709\u66f4\u9ad8\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "iLRM\u901a\u8fc7\u521b\u65b0\u7684\u8fed\u4ee3\u673a\u5236\u548c\u6ce8\u610f\u529b\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u91cd\u5efa\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2507.23278", "pdf": "https://arxiv.org/pdf/2507.23278", "abs": "https://arxiv.org/abs/2507.23278", "authors": ["Hao Tang", "Chenwei Xie", "Xiaoyi Bao", "Tingyu Weng", "Pandeng Li", "Yun Zheng", "Liwei Wang"], "title": "UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we propose UniLIP, which extends CLIP to reconstruction, generation and editing, thereby building a unified tokenizer upon its exceptional comprehension capabilities. Previous CLIP-based unified methods often require additional diffusion decoders or quantization to support reconstruction and generation tasks, leading to inconsistent reconstruction or degradation of original comprehension performance.In contrast, we introduce a two-stage training scheme and a self-distillation strategy that progressively integrates reconstruction capabilities into CLIP, allowing it to maintain original comprehension performance while achieving effective image reconstruction. Furthermore, we propose a dual-condition architecture to connect the MLLM and diffusion transformer, using both learnable queries and the last layer multimodal hidden states as joint conditions. This method not only enables the utilization of the MLLM's strong reasoning capabilities in generation tasks, but also maximizes the exploitation of the rich information in UniLIP features during editing tasks. In text-to-image generation tasks, UniLIP obtains scores of 0.87 and 0.53 on GenEval and WISE benchmark respectively, surpassing all previous unified models of similar scale. In image editing, UniLIP also achieves a score of 3.62 on the ImgEdit Benchmark, surpassing recent state-of-the-art models such as BAGEL and UniWorld-V1. UniLIP effectively expand the application scope of CLIP, enabling continuous CLIP features to not only serve as the optimal choice for understanding tasks but also achieve highly competitive performance in generation and editing tasks.", "AI": {"tldr": "UniLIP\u6269\u5c55\u4e86CLIP\u7684\u529f\u80fd\uff0c\u652f\u6301\u91cd\u5efa\u3001\u751f\u6210\u548c\u7f16\u8f91\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u548c\u81ea\u84b8\u998f\u7b56\u7565\u4fdd\u6301\u539f\u6709\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u5728\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709CLIP\u7edf\u4e00\u65b9\u6cd5\u9700\u8981\u989d\u5916\u6a21\u5757\u652f\u6301\u91cd\u5efa\u548c\u751f\u6210\u4efb\u52a1\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4e00\u81f4\u6216\u7406\u89e3\u80fd\u529b\u4e0b\u964d\u3002UniLIP\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u548c\u81ea\u84b8\u998f\u7b56\u7565\uff0c\u63d0\u51fa\u53cc\u6761\u4ef6\u67b6\u6784\u8fde\u63a5MLLM\u548c\u6269\u6563\u53d8\u6362\u5668\uff0c\u5229\u7528\u53ef\u5b66\u4e60\u67e5\u8be2\u548c\u591a\u6a21\u6001\u9690\u85cf\u72b6\u6001\u3002", "result": "\u5728\u751f\u6210\u4efb\u52a1\u4e2d\uff0cGenEval\u548cWISE\u5f97\u5206\u5206\u522b\u4e3a0.87\u548c0.53\uff1b\u5728\u7f16\u8f91\u4efb\u52a1\u4e2d\uff0cImgEdit\u5f97\u5206\u4e3a3.62\uff0c\u5747\u8d85\u8d8a\u540c\u7c7b\u6a21\u578b\u3002", "conclusion": "UniLIP\u6210\u529f\u6269\u5c55\u4e86CLIP\u7684\u5e94\u7528\u8303\u56f4\uff0c\u5728\u7406\u89e3\u548c\u751f\u6210/\u7f16\u8f91\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.23300", "pdf": "https://arxiv.org/pdf/2507.23300", "abs": "https://arxiv.org/abs/2507.23300", "authors": ["Hanshen Zhu", "Zhen Zhu", "Kaile Zhang", "Yiming Gong", "Yuliang Liu", "Xiang Bai"], "title": "Training-free Geometric Image Editing on Diffusion Models", "categories": ["cs.CV"], "comment": "24 pages, 22 figures, ICCV", "summary": "We tackle the task of geometric image editing, where an object within an image is repositioned, reoriented, or reshaped while preserving overall scene coherence. Previous diffusion-based editing methods often attempt to handle all relevant subtasks in a single step, proving difficult when transformations become large or structurally complex. We address this by proposing a decoupled pipeline that separates object transformation, source region inpainting, and target region refinement. Both inpainting and refinement are implemented using a training-free diffusion approach, FreeFine. In experiments on our new GeoBench benchmark, which contains both 2D and 3D editing scenarios, FreeFine outperforms state-of-the-art alternatives in image fidelity, and edit precision, especially under demanding transformations. Code and benchmark are available at: https://github.com/CIawevy/FreeFine", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u7684\u56fe\u50cf\u51e0\u4f55\u7f16\u8f91\u6d41\u7a0b\uff0c\u901a\u8fc7\u5206\u79bb\u5bf9\u8c61\u53d8\u6362\u3001\u6e90\u533a\u57df\u4fee\u590d\u548c\u76ee\u6807\u533a\u57df\u7ec6\u5316\uff0c\u63d0\u5347\u4e86\u7f16\u8f91\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6269\u6563\u65b9\u6cd5\u5728\u590d\u6742\u6216\u5927\u89c4\u6a21\u53d8\u6362\u65f6\u96be\u4ee5\u4fdd\u6301\u573a\u666f\u4e00\u81f4\u6027\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8bad\u7ec3\u65e0\u5173\u7684\u6269\u6563\u65b9\u6cd5FreeFine\uff0c\u5206\u522b\u5904\u7406\u5bf9\u8c61\u53d8\u6362\u3001\u6e90\u533a\u57df\u4fee\u590d\u548c\u76ee\u6807\u533a\u57df\u7ec6\u5316\u3002", "result": "\u5728GeoBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFreeFine\u5728\u56fe\u50cf\u4fdd\u771f\u5ea6\u548c\u7f16\u8f91\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u89e3\u8026\u7684\u7f16\u8f91\u6d41\u7a0b\u548cFreeFine\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u51e0\u4f55\u7f16\u8f91\u7684\u6548\u679c\u3002"}}
{"id": "2507.23340", "pdf": "https://arxiv.org/pdf/2507.23340", "abs": "https://arxiv.org/abs/2507.23340", "authors": ["Xingyue Peng", "Yuandong Lyu", "Lang Zhang", "Jian Zhu", "Songtao Wang", "Jiaxin Deng", "Songxin Lu", "Weiliang Ma", "Dangen She", "Peng Jia", "XianPeng Lang"], "title": "MagicRoad: Semantic-Aware 3D Road Surface Reconstruction via Obstacle Inpainting", "categories": ["cs.CV"], "comment": null, "summary": "Road surface reconstruction is essential for autonomous driving, supporting centimeter-accurate lane perception and high-definition mapping in complex urban environments.While recent methods based on mesh rendering or 3D Gaussian splatting (3DGS) achieve promising results under clean and static conditions, they remain vulnerable to occlusions from dynamic agents, visual clutter from static obstacles, and appearance degradation caused by lighting and weather changes. We present a robust reconstruction framework that integrates occlusion-aware 2D Gaussian surfels with semantic-guided color enhancement to recover clean, consistent road surfaces. Our method leverages a planar-adapted Gaussian representation for efficient large-scale modeling, employs segmentation-guided video inpainting to remove both dynamic and static foreground objects, and enhances color coherence via semantic-aware correction in HSV space. Extensive experiments on urban-scale datasets demonstrate that our framework produces visually coherent and geometrically faithful reconstructions, significantly outperforming prior methods under real-world conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u8def\u9762\u91cd\u5efa\u6846\u67b6\uff0c\u7ed3\u5408\u906e\u6321\u611f\u77e5\u76842D\u9ad8\u65af\u9762\u5143\u548c\u8bed\u4e49\u5f15\u5bfc\u7684\u989c\u8272\u589e\u5f3a\uff0c\u4ee5\u6062\u590d\u5e72\u51c0\u3001\u4e00\u81f4\u7684\u8def\u9762\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u906e\u6321\u3001\u89c6\u89c9\u6742\u4e71\u548c\u5149\u7167\u5929\u6c14\u53d8\u5316\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5e73\u9762\u9002\u5e94\u7684\u9ad8\u65af\u8868\u793a\u8fdb\u884c\u5927\u89c4\u6a21\u5efa\u6a21\uff0c\u7ed3\u5408\u5206\u5272\u5f15\u5bfc\u7684\u89c6\u9891\u4fee\u590d\u548c\u8bed\u4e49\u611f\u77e5\u7684HSV\u7a7a\u95f4\u989c\u8272\u6821\u6b63\u3002", "result": "\u5728\u57ce\u5e02\u573a\u666f\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u89c6\u89c9\u4e00\u81f4\u548c\u51e0\u4f55\u7cbe\u786e\u7684\u91cd\u5efa\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u590d\u6742\u771f\u5b9e\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u8def\u9762\u91cd\u5efa\u652f\u6301\u3002"}}
{"id": "2507.23357", "pdf": "https://arxiv.org/pdf/2507.23357", "abs": "https://arxiv.org/abs/2507.23357", "authors": ["Radu-Andrei Bourceanu", "Neil De La Fuente", "Jan Grimm", "Andrei Jardan", "Andriy Manucharyan", "Cornelius Weiss", "Roman Pflugfelder"], "title": "IN45023 Neural Network Design Patterns in Computer Vision Seminar Report, Summer 2025", "categories": ["cs.CV"], "comment": null, "summary": "This report analyzes the evolution of key design patterns in computer vision by examining six influential papers. The analy- sis begins with foundational architectures for image recognition. We review ResNet, which introduced residual connections to overcome the vanishing gradient problem and enable effective training of significantly deeper convolutional networks. Subsequently, we examine the Vision Transformer (ViT), which established a new paradigm by applying the Transformer ar- chitecture to sequences of image patches, demonstrating the efficacy of attention-based models for large-scale image recogni- tion. Building on these visual representation backbones, we investigate generative models. Generative Adversarial Networks (GANs) are analyzed for their novel adversarial training process, which challenges a generator against a discriminator to learn complex data distributions. Then, Latent Diffusion Models (LDMs) are covered, which improve upon prior generative methods by performing a sequential denoising process in a perceptually compressed latent space. LDMs achieve high-fidelity synthesis with greater computational efficiency, representing the current state-of-the-art for image generation. Finally, we explore self-supervised learning techniques that reduce dependency on labeled data. DINO is a self-distillation framework in which a student network learns to match the output of a momentum-updated teacher, yielding features with strong k-NN classification performance. We conclude with Masked Autoencoders (MAE), which utilize an asymmetric encoder-decoder design to reconstruct heavily masked inputs, providing a highly scalable and effective method for pre-training large-scale vision models.", "AI": {"tldr": "\u5206\u6790\u516d\u7bc7\u5173\u952e\u8bba\u6587\uff0c\u63a2\u8ba8\u8ba1\u7b97\u673a\u89c6\u89c9\u8bbe\u8ba1\u6a21\u5f0f\u7684\u6f14\u53d8\uff0c\u6db5\u76d6ResNet\u3001ViT\u3001GANs\u3001LDMs\u3001DINO\u548cMAE\u7b49\u6280\u672f\u3002", "motivation": "\u7814\u7a76\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u5173\u952e\u8bbe\u8ba1\u6a21\u5f0f\u53ca\u5176\u6f14\u53d8\uff0c\u4ee5\u63a8\u52a8\u56fe\u50cf\u8bc6\u522b\u3001\u751f\u6210\u6a21\u578b\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u5206\u6790\u516d\u7bc7\u4ee3\u8868\u6027\u8bba\u6587\uff0c\u603b\u7ed3\u5176\u67b6\u6784\u3001\u8bad\u7ec3\u65b9\u6cd5\u548c\u521b\u65b0\u70b9\u3002", "result": "ResNet\u89e3\u51b3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0cViT\u5f15\u5165\u6ce8\u610f\u529b\u673a\u5236\uff0cGANs\u548cLDMs\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0cDINO\u548cMAE\u4f18\u5316\u81ea\u76d1\u7763\u5b66\u4e60\u3002", "conclusion": "\u8ba1\u7b97\u673a\u89c6\u89c9\u8bbe\u8ba1\u6a21\u5f0f\u4e0d\u65ad\u521b\u65b0\uff0c\u4ece\u6b8b\u5dee\u8fde\u63a5\u5230\u6ce8\u610f\u529b\u673a\u5236\uff0c\u518d\u5230\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u63a8\u52a8\u4e86\u9886\u57df\u8fdb\u6b65\u3002"}}
{"id": "2507.23372", "pdf": "https://arxiv.org/pdf/2507.23372", "abs": "https://arxiv.org/abs/2507.23372", "authors": ["Yijie Zhu", "Lingsen Zhang", "Zitong Yu", "Rui Shao", "Tao Tan", "Liqiang Nie"], "title": "UniEmo: Unifying Emotional Understanding and Generation with Learnable Expert Queries", "categories": ["cs.CV"], "comment": null, "summary": "Emotional understanding and generation are often treated as separate tasks, yet they are inherently complementary and can mutually enhance each other. In this paper, we propose the UniEmo, a unified framework that seamlessly integrates these two tasks. The key challenge lies in the abstract nature of emotions, necessitating the extraction of visual representations beneficial for both tasks. To address this, we propose a hierarchical emotional understanding chain with learnable expert queries that progressively extracts multi-scale emotional features, thereby serving as a foundational step for unification. Simultaneously, we fuse these expert queries and emotional representations to guide the diffusion model in generating emotion-evoking images. To enhance the diversity and fidelity of the generated emotional images, we further introduce the emotional correlation coefficient and emotional condition loss into the fusion process. This step facilitates fusion and alignment for emotional generation guided by the understanding. In turn, we demonstrate that joint training allows the generation component to provide implicit feedback to the understanding part. Furthermore, we propose a novel data filtering algorithm to select high-quality and diverse emotional images generated by the well-trained model, which explicitly feedback into the understanding part. Together, these generation-driven dual feedback processes enhance the model's understanding capacity. Extensive experiments show that UniEmo significantly outperforms state-of-the-art methods in both emotional understanding and generation tasks. The code for the proposed method is available at https://github.com/JiuTian-VL/UniEmo.", "AI": {"tldr": "UniEmo\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u60c5\u611f\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u6574\u5408\uff0c\u901a\u8fc7\u5206\u5c42\u60c5\u611f\u7406\u89e3\u94fe\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u5b9e\u73b0\u4e92\u8865\u589e\u5f3a\u3002", "motivation": "\u60c5\u611f\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u901a\u5e38\u88ab\u5206\u5f00\u5904\u7406\uff0c\u4f46\u5b83\u4eec\u672c\u8d28\u4e92\u8865\uff0c\u53ef\u4ee5\u76f8\u4e92\u589e\u5f3a\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u60c5\u611f\u7406\u89e3\u94fe\u548c\u53ef\u5b66\u4e60\u4e13\u5bb6\u67e5\u8be2\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u751f\u6210\u60c5\u611f\u56fe\u50cf\uff0c\u5e76\u5f15\u5165\u60c5\u611f\u76f8\u5173\u7cfb\u6570\u548c\u6761\u4ef6\u635f\u5931\u3002", "result": "UniEmo\u5728\u60c5\u611f\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8054\u5408\u8bad\u7ec3\u548c\u751f\u6210\u9a71\u52a8\u7684\u53cc\u91cd\u53cd\u9988\u8fc7\u7a0b\u63d0\u5347\u4e86\u6a21\u578b\u7684\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2507.23374", "pdf": "https://arxiv.org/pdf/2507.23374", "abs": "https://arxiv.org/abs/2507.23374", "authors": ["Shuangkang Fang", "I-Chao Shen", "Takeo Igarashi", "Yufeng Wang", "ZeSheng Wang", "Yi Yang", "Wenrui Ding", "Shuchang Zhou"], "title": "NeRF Is a Valuable Assistant for 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "Accepted by ICCV", "summary": "We introduce NeRF-GS, a novel framework that jointly optimizes Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework leverages the inherent continuous spatial representation of NeRF to mitigate several limitations of 3DGS, including sensitivity to Gaussian initialization, limited spatial awareness, and weak inter-Gaussian correlations, thereby enhancing its performance. In NeRF-GS, we revisit the design of 3DGS and progressively align its spatial features with NeRF, enabling both representations to be optimized within the same scene through shared 3D spatial information. We further address the formal distinctions between the two approaches by optimizing residual vectors for both implicit features and Gaussian positions to enhance the personalized capabilities of 3DGS. Experimental results on benchmark datasets show that NeRF-GS surpasses existing methods and achieves state-of-the-art performance. This outcome confirms that NeRF and 3DGS are complementary rather than competing, offering new insights into hybrid approaches that combine 3DGS and NeRF for efficient 3D scene representation.", "AI": {"tldr": "NeRF-GS\u7ed3\u5408NeRF\u548c3DGS\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u63d0\u53473D\u573a\u666f\u8868\u793a\u6027\u80fd\u3002", "motivation": "\u89e3\u51b33DGS\u5bf9\u9ad8\u65af\u521d\u59cb\u5316\u654f\u611f\u3001\u7a7a\u95f4\u611f\u77e5\u6709\u9650\u53ca\u9ad8\u65af\u95f4\u5173\u8054\u5f31\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5171\u4eab3D\u7a7a\u95f4\u4fe1\u606f\u5bf9\u9f50NeRF\u548c3DGS\u7684\u7a7a\u95f4\u7279\u5f81\uff0c\u4f18\u5316\u6b8b\u5dee\u5411\u91cf\u589e\u5f3a3DGS\u4e2a\u6027\u5316\u80fd\u529b\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230SOTA\u3002", "conclusion": "NeRF\u548c3DGS\u662f\u4e92\u8865\u7684\uff0c\u4e3a\u6df7\u5408\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.23411", "pdf": "https://arxiv.org/pdf/2507.23411", "abs": "https://arxiv.org/abs/2507.23411", "authors": ["Lemar Abdi", "Francisco Caetano", "Amaan Valiuddin", "Christiaan Viviers", "Hamdi Joudeh", "Fons van der Sommen"], "title": "Out-of-Distribution Detection in Medical Imaging via Diffusion Trajectories", "categories": ["cs.CV"], "comment": "Accepted at Uncertainty for Safe Utilization of Machine Learning in   Medical Imaging, MICCAI 2025", "summary": "In medical imaging, unsupervised out-of-distribution (OOD) detection offers an attractive approach for identifying pathological cases with extremely low incidence rates. In contrast to supervised methods, OOD-based approaches function without labels and are inherently robust to data imbalances. Current generative approaches often rely on likelihood estimation or reconstruction error, but these methods can be computationally expensive, unreliable, and require retraining if the inlier data changes. These limitations hinder their ability to distinguish nominal from anomalous inputs efficiently, consistently, and robustly. We propose a reconstruction-free OOD detection method that leverages the forward diffusion trajectories of a Stein score-based denoising diffusion model (SBDDM). By capturing trajectory curvature via the estimated Stein score, our approach enables accurate anomaly scoring with only five diffusion steps. A single SBDDM pre-trained on a large, semantically aligned medical dataset generalizes effectively across multiple Near-OOD and Far-OOD benchmarks, achieving state-of-the-art performance while drastically reducing computational cost during inference. Compared to existing methods, SBDDM achieves a relative improvement of up to 10.43% and 18.10% for Near-OOD and Far-OOD detection, making it a practical building block for real-time, reliable computer-aided diagnosis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eStein\u5206\u6570\u7684\u53bb\u566a\u6269\u6563\u6a21\u578b\uff08SBDDM\uff09\u7684\u65e0\u76d1\u7763OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u6563\u8f68\u8ff9\u66f2\u7387\u5b9e\u73b0\u9ad8\u6548\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u751f\u6210\u65b9\u6cd5\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u4e0d\u53ef\u9760\u53ca\u9700\u91cd\u65b0\u8bad\u7ec3\u7b49\u95ee\u9898\uff0c\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "method": "\u5229\u7528SBDDM\u7684\u524d\u5411\u6269\u6563\u8f68\u8ff9\uff0c\u901a\u8fc7Stein\u5206\u6570\u6355\u83b7\u66f2\u7387\uff0c\u4ec5\u9700\u4e94\u6b65\u6269\u6563\u5373\u53ef\u5b8c\u6210\u5f02\u5e38\u8bc4\u5206\u3002", "result": "\u5728Near-OOD\u548cFar-OOD\u68c0\u6d4b\u4e2d\u5206\u522b\u76f8\u5bf9\u63d0\u534710.43%\u548c18.10%\uff0c\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "SBDDM\u4e3a\u5b9e\u65f6\u53ef\u9760\u7684\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2507.23483", "pdf": "https://arxiv.org/pdf/2507.23483", "abs": "https://arxiv.org/abs/2507.23483", "authors": ["Mutian Xu", "Chongjie Ye", "Haolin Liu", "Yushuang Wu", "Jiahao Chang", "Xiaoguang Han"], "title": "Stable-Sim2Real: Exploring Simulation of Real-Captured 3D Data with Two-Stage Depth Diffusion", "categories": ["cs.CV"], "comment": "ICCV 2025 (Highlight). Project page:   https://mutianxu.github.io/stable-sim2real/", "summary": "3D data simulation aims to bridge the gap between simulated and real-captured 3D data, which is a fundamental problem for real-world 3D visual tasks. Most 3D data simulation methods inject predefined physical priors but struggle to capture the full complexity of real data. An optimal approach involves learning an implicit mapping from synthetic to realistic data in a data-driven manner, but progress in this solution has met stagnation in recent studies. This work explores a new solution path of data-driven 3D simulation, called Stable-Sim2Real, based on a novel two-stage depth diffusion model. The initial stage finetunes Stable-Diffusion to generate the residual between the real and synthetic paired depth, producing a stable but coarse depth, where some local regions may deviate from realistic patterns. To enhance this, both the synthetic and initial output depth are fed into a second-stage diffusion, where diffusion loss is adjusted to prioritize these distinct areas identified by a 3D discriminator. We provide a new benchmark scheme to evaluate 3D data simulation methods. Extensive experiments show that training the network with the 3D simulated data derived from our method significantly enhances performance in real-world 3D visual tasks. Moreover, the evaluation demonstrates the high similarity between our 3D simulated data and real-captured patterns. Project page: https://mutianxu.github.io/stable-sim2real/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStable-Sim2Real\u7684\u4e24\u9636\u6bb5\u6df1\u5ea6\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u6570\u636e\u9a71\u52a8\u76843D\u6570\u636e\u6a21\u62df\uff0c\u663e\u8457\u63d0\u5347\u4e86\u771f\u5b9e\u4e16\u754c3D\u89c6\u89c9\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u67093D\u6570\u636e\u6a21\u62df\u65b9\u6cd5\u96be\u4ee5\u5b8c\u5168\u6355\u6349\u771f\u5b9e\u6570\u636e\u590d\u6742\u6027\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6df1\u5ea6\u6269\u6563\u6a21\u578b\uff1a\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u7c97\u7565\u6df1\u5ea6\u6b8b\u5dee\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u8c03\u6574\u6269\u6563\u635f\u5931\u4f18\u5316\u5c40\u90e8\u533a\u57df\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u8be5\u65b9\u6cd5\u751f\u6210\u76843D\u6a21\u62df\u6570\u636e\u663e\u8457\u63d0\u5347\u4e86\u771f\u5b9e\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4e14\u4e0e\u771f\u5b9e\u6570\u636e\u9ad8\u5ea6\u76f8\u4f3c\u3002", "conclusion": "Stable-Sim2Real\u4e3a3D\u6570\u636e\u6a21\u62df\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u65b0\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.23487", "pdf": "https://arxiv.org/pdf/2507.23487", "abs": "https://arxiv.org/abs/2507.23487", "authors": ["Jinshan Zhen", "Yuanyue Ge", "Tianxiao Zhu", "Hui Zhao", "Ya Xiong"], "title": "Online Estimation of Table-Top Grown Strawberry Mass in Field Conditions with Occlusions", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted by IROS 2025", "summary": "Accurate mass estimation of table-top grown strawberries under field conditions remains challenging due to frequent occlusions and pose variations. This study proposes a vision-based pipeline integrating RGB-D sensing and deep learning to enable non-destructive, real-time and online mass estimation. The method employed YOLOv8-Seg for instance segmentation, Cycle-consistent generative adversarial network (CycleGAN) for occluded region completion, and tilt-angle correction to refine frontal projection area calculations. A polynomial regression model then mapped the geometric features to mass. Experiments demonstrated mean mass estimation errors of 8.11% for isolated strawberries and 10.47% for occluded cases. CycleGAN outperformed large mask inpainting (LaMa) model in occlusion recovery, achieving superior pixel area ratios (PAR) (mean: 0.978 vs. 1.112) and higher intersection over union (IoU) scores (92.3% vs. 47.7% in the [0.9-1] range). This approach addresses critical limitations of traditional methods, offering a robust solution for automated harvesting and yield monitoring with complex occlusion patterns.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRGB-D\u4f20\u611f\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u8349\u8393\u8d28\u91cf\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u4f8b\u5206\u5272\u3001\u906e\u6321\u4fee\u590d\u548c\u89d2\u5ea6\u6821\u6b63\u5b9e\u73b0\u5b9e\u65f6\u5728\u7ebf\u4f30\u8ba1\uff0c\u8bef\u5dee\u7387\u4f4e\u3002", "motivation": "\u89e3\u51b3\u7530\u95f4\u79cd\u690d\u8349\u8393\u56e0\u906e\u6321\u548c\u59ff\u6001\u53d8\u5316\u5bfc\u81f4\u7684\u8d28\u91cf\u4f30\u8ba1\u96be\u9898\u3002", "method": "\u7ed3\u5408YOLOv8-Seg\u5b9e\u4f8b\u5206\u5272\u3001CycleGAN\u906e\u6321\u4fee\u590d\u548c\u503e\u659c\u89d2\u5ea6\u6821\u6b63\uff0c\u901a\u8fc7\u591a\u9879\u5f0f\u56de\u5f52\u6a21\u578b\u6620\u5c04\u51e0\u4f55\u7279\u5f81\u5230\u8d28\u91cf\u3002", "result": "\u5b64\u7acb\u8349\u8393\u548c\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u5e73\u5747\u8d28\u91cf\u4f30\u8ba1\u8bef\u5dee\u5206\u522b\u4e3a8.11%\u548c10.47%\uff0cCycleGAN\u5728\u906e\u6321\u4fee\u590d\u4e0a\u4f18\u4e8eLaMa\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u906e\u6321\u6761\u4ef6\u4e0b\u7684\u81ea\u52a8\u5316\u6536\u83b7\u548c\u4ea7\u91cf\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.23569", "pdf": "https://arxiv.org/pdf/2507.23569", "abs": "https://arxiv.org/abs/2507.23569", "authors": ["Maxime Pietrantoni", "Gabriela Csurka", "Torsten Sattler"], "title": "Gaussian Splatting Feature Fields for Privacy-Preserving Visual Localization", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Visual localization is the task of estimating a camera pose in a known environment. In this paper, we utilize 3D Gaussian Splatting (3DGS)-based representations for accurate and privacy-preserving visual localization. We propose Gaussian Splatting Feature Fields (GSFFs), a scene representation for visual localization that combines an explicit geometry model (3DGS) with an implicit feature field. We leverage the dense geometric information and differentiable rasterization algorithm from 3DGS to learn robust feature representations grounded in 3D. In particular, we align a 3D scale-aware feature field and a 2D feature encoder in a common embedding space through a contrastive framework. Using a 3D structure-informed clustering procedure, we further regularize the representation learning and seamlessly convert the features to segmentations, which can be used for privacy-preserving visual localization. Pose refinement, which involves aligning either feature maps or segmentations from a query image with those rendered from the GSFFs scene representation, is used to achieve localization. The resulting privacy- and non-privacy-preserving localization pipelines, evaluated on multiple real-world datasets, show state-of-the-art performances.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u7684\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5GSFFs\uff0c\u7ed3\u5408\u663e\u5f0f\u51e0\u4f55\u6a21\u578b\u4e0e\u9690\u5f0f\u7279\u5f81\u573a\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u5b9a\u4f4d\u3002", "motivation": "\u89c6\u89c9\u5b9a\u4f4d\u9700\u8981\u7cbe\u786e\u4f30\u8ba1\u76f8\u673a\u4f4d\u59ff\uff0c\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u6027\u80fd\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u63d0\u51faGSFFs\uff0c\u7ed3\u54083DGS\u7684\u51e0\u4f55\u4fe1\u606f\u4e0e\u9690\u5f0f\u7279\u5f81\u573a\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f503D\u7279\u5f81\u573a\u4e0e2D\u7279\u5f81\u7f16\u7801\u5668\uff0c\u5e76\u5229\u7528\u805a\u7c7b\u6b63\u5219\u5316\u7279\u5f81\u5b66\u4e60\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0c\u9690\u79c1\u548c\u975e\u9690\u79c1\u4fdd\u62a4\u5b9a\u4f4d\u7ba1\u9053\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "GSFFs\u901a\u8fc7\u7ed3\u5408\u663e\u5f0f\u51e0\u4f55\u4e0e\u9690\u5f0f\u7279\u5f81\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u89c6\u89c9\u5b9a\u4f4d\u3002"}}
{"id": "2507.23620", "pdf": "https://arxiv.org/pdf/2507.23620", "abs": "https://arxiv.org/abs/2507.23620", "authors": ["Yucheng Xie", "Fu Feng", "Ruixiao Shi", "Jing Wang", "Yong Rui", "Xin Geng"], "title": "DivControl: Knowledge Diversion for Controllable Image Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Diffusion models have advanced from text-to-image (T2I) to image-to-image (I2I) generation by incorporating structured inputs such as depth maps, enabling fine-grained spatial control. However, existing methods either train separate models for each condition or rely on unified architectures with entangled representations, resulting in poor generalization and high adaptation costs for novel conditions. To this end, we propose DivControl, a decomposable pretraining framework for unified controllable generation and efficient adaptation. DivControl factorizes ControlNet via SVD into basic components-pairs of singular vectors-which are disentangled into condition-agnostic learngenes and condition-specific tailors through knowledge diversion during multi-condition training. Knowledge diversion is implemented via a dynamic gate that performs soft routing over tailors based on the semantics of condition instructions, enabling zero-shot generalization and parameter-efficient adaptation to novel conditions. To further improve condition fidelity and training efficiency, we introduce a representation alignment loss that aligns condition embeddings with early diffusion features. Extensive experiments demonstrate that DivControl achieves state-of-the-art controllability with 36.4$\\times$ less training cost, while simultaneously improving average performance on basic conditions. It also delivers strong zero-shot and few-shot performance on unseen conditions, demonstrating superior scalability, modularity, and transferability.", "AI": {"tldr": "DivControl\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5206\u89e3\u7684\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u5206\u6d41\u548c\u52a8\u6001\u95e8\u63a7\u5b9e\u73b0\u7edf\u4e00\u53ef\u63a7\u751f\u6210\u548c\u9ad8\u6548\u9002\u5e94\uff0c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u56fe\u50cf\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u8981\u4e48\u9700\u8981\u4e3a\u6bcf\u4e2a\u6761\u4ef6\u8bad\u7ec3\u5355\u72ec\u6a21\u578b\uff0c\u8981\u4e48\u4f9d\u8d56\u7ea0\u7f20\u8868\u793a\u7684\u7edf\u4e00\u67b6\u6784\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u9002\u5e94\u6210\u672c\u9ad8\u3002", "method": "DivControl\u901a\u8fc7SVD\u5206\u89e3ControlNet\u4e3a\u57fa\u672c\u7ec4\u4ef6\uff0c\u5229\u7528\u77e5\u8bc6\u5206\u6d41\u548c\u52a8\u6001\u95e8\u63a7\u5b9e\u73b0\u6761\u4ef6\u65e0\u5173\u7684learngenes\u548c\u6761\u4ef6\u7279\u5b9a\u7684tailors\uff0c\u5e76\u901a\u8fc7\u8868\u793a\u5bf9\u9f50\u635f\u5931\u63d0\u5347\u6761\u4ef6\u4fdd\u771f\u5ea6\u3002", "result": "DivControl\u5728\u53ef\u63a7\u6027\u4e0a\u8fbe\u5230SOTA\uff0c\u8bad\u7ec3\u6210\u672c\u964d\u4f4e36.4\u500d\uff0c\u540c\u65f6\u5728\u672a\u89c1\u6761\u4ef6\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u6027\u80fd\u3002", "conclusion": "DivControl\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u53ef\u6269\u5c55\u6027\u3001\u6a21\u5757\u5316\u548c\u8fc1\u79fb\u80fd\u529b\uff0c\u4e3a\u53ef\u63a7\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.23652", "pdf": "https://arxiv.org/pdf/2507.23652", "abs": "https://arxiv.org/abs/2507.23652", "authors": ["Kunpeng Qiu", "Zhiying Zhou", "Yongxin Guo"], "title": "Adaptively Distilled ControlNet: Accelerated Training and Superior Sampling for Medical Image Synthesis", "categories": ["cs.CV"], "comment": "Accepted by MICCAI2025", "summary": "Medical image annotation is constrained by privacy concerns and labor-intensive labeling, significantly limiting the performance and generalization of segmentation models. While mask-controllable diffusion models excel in synthesis, they struggle with precise lesion-mask alignment. We propose \\textbf{Adaptively Distilled ControlNet}, a task-agnostic framework that accelerates training and optimization through dual-model distillation. Specifically, during training, a teacher model, conditioned on mask-image pairs, regularizes a mask-only student model via predicted noise alignment in parameter space, further enhanced by adaptive regularization based on lesion-background ratios. During sampling, only the student model is used, enabling privacy-preserving medical image generation. Comprehensive evaluations on two distinct medical datasets demonstrate state-of-the-art performance: TransUNet improves mDice/mIoU by 2.4%/4.2% on KiTS19, while SANet achieves 2.6%/3.5% gains on Polyps, highlighting its effectiveness and superiority. Code is available at GitHub.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4efb\u52a1\u65e0\u5173\u7684\u6846\u67b6Adaptively Distilled ControlNet\uff0c\u901a\u8fc7\u53cc\u6a21\u578b\u84b8\u998f\u52a0\u901f\u8bad\u7ec3\u548c\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u6807\u6ce8\u4e2d\u7684\u9690\u79c1\u548c\u6807\u6ce8\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u6807\u6ce8\u53d7\u9690\u79c1\u548c\u4eba\u5de5\u6807\u6ce8\u9650\u5236\uff0c\u5f71\u54cd\u5206\u5272\u6a21\u578b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002\u73b0\u6709mask-controllable\u6269\u6563\u6a21\u578b\u5728\u5408\u6210\u56fe\u50cf\u65f6\u96be\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684\u75c5\u7076-\u63a9\u6a21\u5bf9\u9f50\u3002", "method": "\u91c7\u7528\u53cc\u6a21\u578b\u84b8\u998f\u6846\u67b6\uff1a\u6559\u5e08\u6a21\u578b\u57fa\u4e8e\u63a9\u6a21-\u56fe\u50cf\u5bf9\u8bad\u7ec3\uff0c\u901a\u8fc7\u53c2\u6570\u7a7a\u95f4\u4e2d\u7684\u566a\u58f0\u5bf9\u9f50\u548c\u75c5\u7076-\u80cc\u666f\u6bd4\u7684\u81ea\u9002\u5e94\u6b63\u5219\u5316\u6307\u5bfc\u5b66\u751f\u6a21\u578b\u3002\u91c7\u6837\u65f6\u4ec5\u4f7f\u7528\u5b66\u751f\u6a21\u578b\uff0c\u4fdd\u62a4\u9690\u79c1\u3002", "result": "\u5728\u4e24\u4e2a\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff1aTransUNet\u5728KiTS19\u4e0amDice/mIoU\u63d0\u53472.4%/4.2%\uff0cSANet\u5728Polyps\u4e0a\u63d0\u53472.6%/3.5%\u3002", "conclusion": "Adaptively Distilled ControlNet\u5728\u533b\u5b66\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u4f18\u8d8a\u6027\uff0c\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\u3002"}}
{"id": "2507.23683", "pdf": "https://arxiv.org/pdf/2507.23683", "abs": "https://arxiv.org/abs/2507.23683", "authors": ["Jialei Chen", "Wuhao Xu", "Sipeng He", "Baoru Huang", "Dongchun Ren"], "title": "I2V-GS: Infrastructure-to-Vehicle View Transformation with Gaussian Splatting for Autonomous Driving Data Generation", "categories": ["cs.CV"], "comment": null, "summary": "Vast and high-quality data are essential for end-to-end autonomous driving systems. However, current driving data is mainly collected by vehicles, which is expensive and inefficient. A potential solution lies in synthesizing data from real-world images. Recent advancements in 3D reconstruction demonstrate photorealistic novel view synthesis, highlighting the potential of generating driving data from images captured on the road. This paper introduces a novel method, I2V-GS, to transfer the Infrastructure view To the Vehicle view with Gaussian Splatting. Reconstruction from sparse infrastructure viewpoints and rendering under large view transformations is a challenging problem. We adopt the adaptive depth warp to generate dense training views. To further expand the range of views, we employ a cascade strategy to inpaint warped images, which also ensures inpainting content is consistent across views. To further ensure the reliability of the diffusion model, we utilize the cross-view information to perform a confidenceguided optimization. Moreover, we introduce RoadSight, a multi-modality, multi-view dataset from real scenarios in infrastructure views. To our knowledge, I2V-GS is the first framework to generate autonomous driving datasets with infrastructure-vehicle view transformation. Experimental results demonstrate that I2V-GS significantly improves synthesis quality under vehicle view, outperforming StreetGaussian in NTA-Iou, NTL-Iou, and FID by 45.7%, 34.2%, and 14.9%, respectively.", "AI": {"tldr": "I2V-GS\u662f\u4e00\u79cd\u901a\u8fc7\u9ad8\u65af\u6563\u5c04\u5c06\u57fa\u7840\u8bbe\u65bd\u89c6\u56fe\u8f6c\u6362\u4e3a\u8f66\u8f86\u89c6\u56fe\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5408\u6210\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u4e3b\u8981\u4f9d\u8d56\u8f66\u8f86\u91c7\u96c6\uff0c\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e\uff0c\u800c\u901a\u8fc7\u5408\u6210\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u7684\u6570\u636e\u662f\u4e00\u79cd\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u6df1\u5ea6\u626d\u66f2\u751f\u6210\u5bc6\u96c6\u8bad\u7ec3\u89c6\u56fe\uff0c\u4f7f\u7528\u7ea7\u8054\u7b56\u7565\u6269\u5c55\u89c6\u56fe\u8303\u56f4\uff0c\u5e76\u901a\u8fc7\u8de8\u89c6\u56fe\u4fe1\u606f\u8fdb\u884c\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u4f18\u5316\u3002", "result": "I2V-GS\u5728\u8f66\u8f86\u89c6\u56fe\u4e0b\u7684\u5408\u6210\u8d28\u91cf\u663e\u8457\u4f18\u4e8eStreetGaussian\uff0cNTA-Iou\u3001NTL-Iou\u548cFID\u5206\u522b\u63d0\u534745.7%\u300134.2%\u548c14.9%\u3002", "conclusion": "I2V-GS\u662f\u9996\u4e2a\u5b9e\u73b0\u57fa\u7840\u8bbe\u65bd-\u8f66\u8f86\u89c6\u56fe\u8f6c\u6362\u751f\u6210\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u7684\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u9ad8\u6548\u7684\u6570\u636e\u5408\u6210\u6f5c\u529b\u3002"}}
{"id": "2507.23685", "pdf": "https://arxiv.org/pdf/2507.23685", "abs": "https://arxiv.org/abs/2507.23685", "authors": ["Zihan Cheng", "Liangtai Zhou", "Dian Chen", "Ni Tang", "Xiaotong Luo", "Yanyun Qu"], "title": "UniLDiff: Unlocking the Power of Diffusion Priors for All-in-One Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "All-in-One Image Restoration (AiOIR) has emerged as a promising yet challenging research direction. To address its core challenges, we propose a novel unified image restoration framework based on latent diffusion models (LDMs). Our approach structurally integrates low-quality visual priors into the diffusion process, unlocking the powerful generative capacity of diffusion models for diverse degradations. Specifically, we design a Degradation-Aware Feature Fusion (DAFF) module to enable adaptive handling of diverse degradation types. Furthermore, to mitigate detail loss caused by the high compression and iterative sampling of LDMs, we design a Detail-Aware Expert Module (DAEM) in the decoder to enhance texture and fine-structure recovery. Extensive experiments across multi-task and mixed degradation settings demonstrate that our method consistently achieves state-of-the-art performance, highlighting the practical potential of diffusion priors for unified image restoration. Our code will be released.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\u7684\u7edf\u4e00\u56fe\u50cf\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7Degradation-Aware Feature Fusion\uff08DAFF\uff09\u6a21\u5757\u548cDetail-Aware Expert Module\uff08DAEM\uff09\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u5bf9\u591a\u6837\u5316\u9000\u5316\u7684\u81ea\u9002\u5e94\u5904\u7406\u548c\u7ec6\u8282\u6062\u590d\u3002", "motivation": "\u89e3\u51b3All-in-One Image Restoration\uff08AiOIR\uff09\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u5f3a\u5927\u751f\u6210\u80fd\u529b\u5904\u7406\u591a\u6837\u5316\u9000\u5316\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\uff0c\u8bbe\u8ba1DAFF\u6a21\u5757\u81ea\u9002\u5e94\u5904\u7406\u9000\u5316\u7c7b\u578b\uff0cDAEM\u6a21\u5757\u589e\u5f3a\u7ec6\u8282\u6062\u590d\u3002", "result": "\u5728\u591a\u4efb\u52a1\u548c\u6df7\u5408\u9000\u5316\u8bbe\u7f6e\u4e0b\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u6027\u80fd\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u6269\u6563\u5148\u9a8c\u5728\u7edf\u4e00\u56fe\u50cf\u4fee\u590d\u4e2d\u5177\u6709\u5b9e\u9645\u6f5c\u529b\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2507.23704", "pdf": "https://arxiv.org/pdf/2507.23704", "abs": "https://arxiv.org/abs/2507.23704", "authors": ["Zhenyang Li", "Xiaoyang Bai", "Tongchen Zhang", "Pengfei Shen", "Weiwei Xu", "Yifan Peng"], "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": "17 pages, 8 figures", "summary": "High-fidelity 3D video reconstruction is essential for enabling real-time rendering of dynamic scenes with realistic motion in virtual and augmented reality (VR/AR). The deformation field paradigm of 3D Gaussian splatting has achieved near-photorealistic results in video reconstruction due to the great representation capability of deep deformation networks. However, in videos with complex motion and significant scale variations, deformation networks often overfit to irregular Gaussian trajectories, leading to suboptimal visual quality. Moreover, the gradient-based densification strategy designed for static scene reconstruction proves inadequate to address the absence of dynamic content. In light of these challenges, we propose a flow-empowered velocity field modeling scheme tailored for Gaussian video reconstruction, dubbed FlowGaussian-VR. It consists of two core components: a velocity field rendering (VFR) pipeline which enables optical flow-based optimization, and a flow-assisted adaptive densification (FAD) strategy that adjusts the number and size of Gaussians in dynamic regions. We validate our model's effectiveness on multi-view dynamic reconstruction and novel view synthesis with multiple real-world datasets containing challenging motion scenarios, demonstrating not only notable visual improvements (over 2.5 dB gain in PSNR) and less blurry artifacts in dynamic textures, but also regularized and trackable per-Gaussian trajectories.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFlowGaussian-VR\u7684\u6d41\u589e\u5f3a\u901f\u5ea6\u573a\u5efa\u6a21\u65b9\u6848\uff0c\u7528\u4e8e\u89e3\u51b3\u590d\u6742\u8fd0\u52a8\u548c\u5c3a\u5ea6\u53d8\u5316\u89c6\u9891\u4e2d\u76843D\u9ad8\u65af\u91cd\u5efa\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8d28\u91cf\u548c\u52a8\u6001\u5185\u5bb9\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u9ad8\u4fdd\u771f3D\u89c6\u9891\u91cd\u5efa\u5bf9VR/AR\u4e2d\u7684\u52a8\u6001\u573a\u666f\u5b9e\u65f6\u6e32\u67d3\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u8fd0\u52a8\u548c\u5c3a\u5ea6\u53d8\u5316\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u89c6\u89c9\u8d28\u91cf\u4e0b\u964d\u3002", "method": "FlowGaussian-VR\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u57fa\u4e8e\u5149\u6d41\u4f18\u5316\u7684\u901f\u5ea6\u573a\u6e32\u67d3\uff08VFR\uff09\u6d41\u7a0b\u548c\u6d41\u8f85\u52a9\u81ea\u9002\u5e94\u5bc6\u5ea6\u5316\uff08FAD\uff09\u7b56\u7565\uff0c\u7528\u4e8e\u52a8\u6001\u533a\u57df\u7684\u9ad8\u65af\u8c03\u6574\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8d28\u91cf\uff08PSNR\u589e\u76ca\u8d85\u8fc72.5 dB\uff09\uff0c\u51cf\u5c11\u4e86\u52a8\u6001\u7eb9\u7406\u7684\u6a21\u7cca\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u65af\u8f68\u8ff9\u7684\u89c4\u8303\u5316\u8ddf\u8e2a\u3002", "conclusion": "FlowGaussian-VR\u901a\u8fc7\u6d41\u589e\u5f3a\u7684\u901f\u5ea6\u573a\u5efa\u6a21\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u52a8\u6001\u573a\u666f\u4e2d\u7684\u91cd\u5efa\u95ee\u9898\uff0c\u4e3aVR/AR\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u76843D\u89c6\u9891\u91cd\u5efa\u65b9\u6848\u3002"}}
{"id": "2507.23772", "pdf": "https://arxiv.org/pdf/2507.23772", "abs": "https://arxiv.org/abs/2507.23772", "authors": ["Di Li", "Jie Feng", "Jiahao Chen", "Weisheng Dong", "Guanbin Li", "Yuhui Zheng", "Mingtao Feng", "Guangming Shi"], "title": "SeqAffordSplat: Scene-level Sequential Affordance Reasoning on 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "3D affordance reasoning, the task of associating human instructions with the functional regions of 3D objects, is a critical capability for embodied agents. Current methods based on 3D Gaussian Splatting (3DGS) are fundamentally limited to single-object, single-step interactions, a paradigm that falls short of addressing the long-horizon, multi-object tasks required for complex real-world applications. To bridge this gap, we introduce the novel task of Sequential 3D Gaussian Affordance Reasoning and establish SeqAffordSplat, a large-scale benchmark featuring 1800+ scenes to support research on long-horizon affordance understanding in complex 3DGS environments. We then propose SeqSplatNet, an end-to-end framework that directly maps an instruction to a sequence of 3D affordance masks. SeqSplatNet employs a large language model that autoregressively generates text interleaved with special segmentation tokens, guiding a conditional decoder to produce the corresponding 3D mask. To handle complex scene geometry, we introduce a pre-training strategy, Conditional Geometric Reconstruction, where the model learns to reconstruct complete affordance region masks from known geometric observations, thereby building a robust geometric prior. Furthermore, to resolve semantic ambiguities, we design a feature injection mechanism that lifts rich semantic features from 2D Vision Foundation Models (VFM) and fuses them into the 3D decoder at multiple scales. Extensive experiments demonstrate that our method sets a new state-of-the-art on our challenging benchmark, effectively advancing affordance reasoning from single-step interactions to complex, sequential tasks at the scene level.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4efb\u52a1Sequential 3D Gaussian Affordance Reasoning\uff0c\u5e76\u5efa\u7acb\u4e86SeqAffordSplat\u57fa\u51c6\u548cSeqSplatNet\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u590d\u6742\u573a\u666f\u4e2d\u7684\u957f\u65f6\u7a0b3D\u529f\u80fd\u533a\u57df\u63a8\u7406\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u5bf9\u8c61\u3001\u5355\u6b65\u4ea4\u4e92\uff0c\u65e0\u6cd5\u6ee1\u8db3\u590d\u6742\u73b0\u5b9e\u4efb\u52a1\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faSeqSplatNet\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6761\u4ef6\u89e3\u7801\u5668\uff0c\u751f\u6210\u5e8f\u5217\u5316\u76843D\u529f\u80fd\u533a\u57df\u63a9\u7801\uff1b\u5f15\u5165\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u7279\u5f81\u6ce8\u5165\u673a\u5236\u3002", "result": "\u5728SeqAffordSplat\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u4f18\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u4ece\u5355\u6b65\u4ea4\u4e92\u5230\u590d\u6742\u573a\u666f\u7ea7\u4efb\u52a1\u7684\u63d0\u5347\u3002", "conclusion": "SeqSplatNet\u6709\u6548\u63a8\u52a8\u4e863D\u529f\u80fd\u533a\u57df\u63a8\u7406\u4ece\u5355\u6b65\u4ea4\u4e92\u5411\u590d\u6742\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.23784", "pdf": "https://arxiv.org/pdf/2507.23784", "abs": "https://arxiv.org/abs/2507.23784", "authors": ["Jessica Bader", "Leander Girrbach", "Stephan Alaniz", "Zeynep Akata"], "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted at ICCV 2025", "summary": "Concept Bottleneck Models (CBMs) and other concept-based interpretable models show great promise for making AI applications more transparent, which is essential in fields like medicine. Despite their success, we demonstrate that CBMs struggle to reliably identify the correct concepts under distribution shifts. To assess the robustness of CBMs to concept variations, we introduce SUB: a fine-grained image and concept benchmark containing 38,400 synthetic images based on the CUB dataset. To create SUB, we select a CUB subset of 33 bird classes and 45 concepts to generate images which substitute a specific concept, such as wing color or belly pattern. We introduce a novel Tied Diffusion Guidance (TDG) method to precisely control generated images, where noise sharing for two parallel denoising processes ensures that both the correct bird class and the correct attribute are generated. This novel benchmark enables rigorous evaluation of CBMs and similar interpretable models, contributing to the development of more robust methods. Our code is available at https://github.com/ExplainableML/sub and the dataset at http://huggingface.co/datasets/Jessica-bader/SUB.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86SUB\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CBMs\uff09\u5728\u5206\u5e03\u53d8\u5316\u4e0b\u8bc6\u522b\u6982\u5ff5\u7684\u53ef\u9760\u6027\uff0c\u5e76\u5f00\u53d1\u4e86Tied Diffusion Guidance\uff08TDG\uff09\u65b9\u6cd5\u7cbe\u786e\u751f\u6210\u56fe\u50cf\u3002", "motivation": "\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CBMs\uff09\u5728\u900f\u660eAI\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5206\u5e03\u53d8\u5316\u4e0b\u8bc6\u522b\u6982\u5ff5\u7684\u53ef\u9760\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7SUB\u57fa\u51c6\uff08\u57fa\u4e8eCUB\u6570\u636e\u96c6\u768438,400\u5f20\u5408\u6210\u56fe\u50cf\uff09\u548cTDG\u65b9\u6cd5\uff08\u5171\u4eab\u566a\u58f0\u7684\u53cc\u5e76\u884c\u53bb\u566a\u8fc7\u7a0b\uff09\u7cbe\u786e\u63a7\u5236\u751f\u6210\u56fe\u50cf\u3002", "result": "SUB\u57fa\u51c6\u548cTDG\u65b9\u6cd5\u6210\u529f\u751f\u6210\u5177\u6709\u7279\u5b9a\u6982\u5ff5\u7684\u56fe\u50cf\uff0c\u4e3aCBMs\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5de5\u5177\u3002", "conclusion": "SUB\u548cTDG\u4e3a\u6982\u5ff5\u6a21\u578b\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u66f4\u53ef\u9760\u900f\u660eAI\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.23785", "pdf": "https://arxiv.org/pdf/2507.23785", "abs": "https://arxiv.org/abs/2507.23785", "authors": ["Bowen Zhang", "Sicheng Xu", "Chuxin Wang", "Jiaolong Yang", "Feng Zhao", "Dong Chen", "Baining Guo"], "title": "Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis", "categories": ["cs.CV"], "comment": "ICCV 2025. Project page: https://gvfdiffusion.github.io/", "summary": "In this paper, we present a novel framework for video-to-4D generation that creates high-quality dynamic 3D content from single video inputs. Direct 4D diffusion modeling is extremely challenging due to costly data construction and the high-dimensional nature of jointly representing 3D shape, appearance, and motion. We address these challenges by introducing a Direct 4DMesh-to-GS Variation Field VAE that directly encodes canonical Gaussian Splats (GS) and their temporal variations from 3D animation data without per-instance fitting, and compresses high-dimensional animations into a compact latent space. Building upon this efficient representation, we train a Gaussian Variation Field diffusion model with temporal-aware Diffusion Transformer conditioned on input videos and canonical GS. Trained on carefully-curated animatable 3D objects from the Objaverse dataset, our model demonstrates superior generation quality compared to existing methods. It also exhibits remarkable generalization to in-the-wild video inputs despite being trained exclusively on synthetic data, paving the way for generating high-quality animated 3D content. Project page: https://gvfdiffusion.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5355\u89c6\u9891\u8f93\u5165\u751f\u6210\u9ad8\u8d28\u91cf\u52a8\u60013D\u5185\u5bb9\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u7f16\u7801\u9ad8\u65af\u70b9\u4e91\u53ca\u5176\u65f6\u95f4\u53d8\u5316\uff0c\u89e3\u51b3\u4e864D\u6269\u6563\u5efa\u6a21\u7684\u6311\u6218\u3002", "motivation": "\u76f4\u63a54D\u6269\u6563\u5efa\u6a21\u56e0\u6570\u636e\u6784\u5efa\u6210\u672c\u9ad8\u548c\u9ad8\u7ef4\u8868\u793a\u56f0\u96be\u800c\u6781\u5177\u6311\u6218\u6027\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u751f\u6210\u52a8\u60013D\u5185\u5bb9\u3002", "method": "\u5f15\u5165Direct 4DMesh-to-GS Variation Field VAE\uff0c\u76f4\u63a5\u7f16\u7801\u9ad8\u65af\u70b9\u4e91\u53ca\u5176\u65f6\u95f4\u53d8\u5316\uff0c\u5e76\u8bad\u7ec3\u4e00\u4e2a\u57fa\u4e8e\u8f93\u5165\u89c6\u9891\u548c\u9ad8\u65af\u70b9\u4e91\u7684\u65f6\u95f4\u611f\u77e5\u6269\u6563\u53d8\u6362\u5668\u6a21\u578b\u3002", "result": "\u5728Objaverse\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u771f\u5b9e\u89c6\u9891\u8f93\u5165\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u751f\u6210\u9ad8\u8d28\u91cf\u52a8\u60013D\u5185\u5bb9\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u5c55\u793a\u4e86\u5728\u5408\u6210\u6570\u636e\u8bad\u7ec3\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.23001", "pdf": "https://arxiv.org/pdf/2507.23001", "abs": "https://arxiv.org/abs/2507.23001", "authors": ["Jamil Fayyad", "Nourhan Bayasi", "Ziyang Yu", "Homayoun Najjaran"], "title": "LesionGen: A Concept-Guided Diffusion Model for Dermatology Image Synthesis", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted at the MICCAI 2025 ISIC Workshop", "summary": "Deep learning models for skin disease classification require large, diverse, and well-annotated datasets. However, such resources are often limited due to privacy concerns, high annotation costs, and insufficient demographic representation. While text-to-image diffusion probabilistic models (T2I-DPMs) offer promise for medical data synthesis, their use in dermatology remains underexplored, largely due to the scarcity of rich textual descriptions in existing skin image datasets. In this work, we introduce LesionGen, a clinically informed T2I-DPM framework for dermatology image synthesis. Unlike prior methods that rely on simplistic disease labels, LesionGen is trained on structured, concept-rich dermatological captions derived from expert annotations and pseudo-generated, concept-guided reports. By fine-tuning a pretrained diffusion model on these high-quality image-caption pairs, we enable the generation of realistic and diverse skin lesion images conditioned on meaningful dermatological descriptions. Our results demonstrate that models trained solely on our synthetic dataset achieve classification accuracy comparable to those trained on real images, with notable gains in worst-case subgroup performance. Code and data are available here.", "AI": {"tldr": "LesionGen\u662f\u4e00\u4e2a\u57fa\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6982\u7387\u6a21\u578b\u7684\u76ae\u80a4\u75c5\u56fe\u50cf\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u8d28\u91cf\u56fe\u50cf-\u63cf\u8ff0\u5bf9\u751f\u6210\u591a\u6837\u4e14\u771f\u5b9e\u7684\u76ae\u80a4\u75c5\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u76ae\u80a4\u75c5\u5206\u7c7b\u9700\u8981\u5927\u91cf\u591a\u6837\u4e14\u6807\u6ce8\u826f\u597d\u7684\u6570\u636e\u96c6\uff0c\u4f46\u9690\u79c1\u95ee\u9898\u3001\u6807\u6ce8\u6210\u672c\u9ad8\u548c\u4eba\u53e3\u4ee3\u8868\u6027\u4e0d\u8db3\u9650\u5236\u4e86\u6570\u636e\u53ef\u7528\u6027\u3002", "method": "LesionGen\u5229\u7528\u4e13\u5bb6\u6807\u6ce8\u548c\u4f2a\u751f\u6210\u7684\u6982\u5ff5\u4e30\u5bcc\u63cf\u8ff0\uff0c\u5fae\u8c03\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\uff0c\u751f\u6210\u57fa\u4e8e\u76ae\u80a4\u75c5\u5b66\u63cf\u8ff0\u7684\u56fe\u50cf\u3002", "result": "\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5206\u7c7b\u51c6\u786e\u7387\u63a5\u8fd1\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u4e14\u5728\u8868\u73b0\u6700\u5dee\u7684\u5b50\u7ec4\u4e2d\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "LesionGen\u4e3a\u76ae\u80a4\u75c5\u56fe\u50cf\u5408\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u7f13\u89e3\u4e86\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002"}}
{"id": "2507.23010", "pdf": "https://arxiv.org/pdf/2507.23010", "abs": "https://arxiv.org/abs/2507.23010", "authors": ["Siwoo Park"], "title": "Investigating the Invertibility of Multimodal Latent Spaces: Limitations of Optimization-Based Methods", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.SD", "eess.AS"], "comment": null, "summary": "This paper investigates the inverse capabilities and broader utility of multimodal latent spaces within task-specific AI (Artificial Intelligence) models. While these models excel at their designed forward tasks (e.g., text-to-image generation, audio-to-text transcription), their potential for inverse mappings remains largely unexplored. We propose an optimization-based framework to infer input characteristics from desired outputs, applying it bidirectionally across Text-Image (BLIP, Flux.1-dev) and Text-Audio (Whisper-Large-V3, Chatterbox-TTS) modalities.   Our central hypothesis posits that while optimization can guide models towards inverse tasks, their multimodal latent spaces will not consistently support semantically meaningful and perceptually coherent inverse mappings. Experimental results consistently validate this hypothesis. We demonstrate that while optimization can force models to produce outputs that align textually with targets (e.g., a text-to-image model generating an image that an image captioning model describes correctly, or an ASR model transcribing optimized audio accurately), the perceptual quality of these inversions is chaotic and incoherent. Furthermore, when attempting to infer the original semantic input from generative models, the reconstructed latent space embeddings frequently lack semantic interpretability, aligning with nonsensical vocabulary tokens.   These findings highlight a critical limitation. multimodal latent spaces, primarily optimized for specific forward tasks, do not inherently possess the structure required for robust and interpretable inverse mappings. Our work underscores the need for further research into developing truly semantically rich and invertible multimodal latent spaces.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u6f5c\u5728\u7a7a\u95f4\u5728\u4efb\u52a1\u7279\u5b9aAI\u6a21\u578b\u4e2d\u7684\u9006\u5411\u80fd\u529b\u53ca\u5176\u5e7f\u6cdb\u7528\u9014\uff0c\u53d1\u73b0\u5176\u9006\u5411\u6620\u5c04\u5728\u8bed\u4e49\u548c\u611f\u77e5\u4e0a\u7f3a\u4e4f\u4e00\u81f4\u6027\u3002", "motivation": "\u7814\u7a76\u591a\u6a21\u6001\u6f5c\u5728\u7a7a\u95f4\u5728\u9006\u5411\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u586b\u8865\u73b0\u6709\u6a21\u578b\u5728\u9006\u5411\u6620\u5c04\u80fd\u529b\u4e0a\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4f18\u5316\u7684\u6846\u67b6\uff0c\u5728\u6587\u672c-\u56fe\u50cf\u548c\u6587\u672c-\u97f3\u9891\u6a21\u6001\u4e2d\u53cc\u5411\u63a8\u65ad\u8f93\u5165\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f18\u5316\u867d\u80fd\u5b9e\u73b0\u76ee\u6807\u5bf9\u9f50\uff0c\u4f46\u9006\u5411\u6620\u5c04\u7684\u611f\u77e5\u8d28\u91cf\u6df7\u4e71\u4e14\u8bed\u4e49\u4e0d\u53ef\u89e3\u91ca\u3002", "conclusion": "\u591a\u6a21\u6001\u6f5c\u5728\u7a7a\u95f4\u7f3a\u4e4f\u7a33\u5065\u9006\u5411\u6620\u5c04\u7684\u7ed3\u6784\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5f00\u53d1\u8bed\u4e49\u4e30\u5bcc\u4e14\u53ef\u9006\u7684\u6f5c\u5728\u7a7a\u95f4\u3002"}}
{"id": "2507.23273", "pdf": "https://arxiv.org/pdf/2507.23273", "abs": "https://arxiv.org/abs/2507.23273", "authors": ["Jaeseok Park", "Chanoh Park", "Minsu Kim", "Soohwan Kim"], "title": "GSFusion:Globally Optimized LiDAR-Inertial-Visual Mapping for Gaussian Splatting", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic mapping, conventional approaches based on camera sensor, even RGB-D, suffer from fundamental limitations such as high computational load, failure in environments with poor texture or illumination, and short operational ranges. LiDAR emerges as a robust alternative, but its integration with 3DGS introduces new challenges, such as the need for exceptional global alignment for photorealistic quality and prolonged optimization times caused by sparse data. To address these challenges, we propose GSFusion, an online LiDAR-Inertial-Visual mapping system that ensures high-precision map consistency through a surfel-to-surfel constraint in the global pose-graph optimization. To handle sparse data, our system employs a pixel-aware Gaussian initialization strategy for efficient representation and a bounded sigmoid constraint to prevent uncontrolled Gaussian growth. Experiments on public and our datasets demonstrate our system outperforms existing 3DGS SLAM systems in terms of rendering quality and map-building efficiency.", "AI": {"tldr": "GSFusion\u662f\u4e00\u79cd\u5728\u7ebfLiDAR-\u60ef\u6027-\u89c6\u89c9\u6620\u5c04\u7cfb\u7edf\uff0c\u901a\u8fc7\u5168\u5c40\u4f4d\u59ff\u56fe\u4f18\u5316\u4e2d\u7684surfel-to-surfel\u7ea6\u675f\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5730\u56fe\u4e00\u81f4\u6027\uff0c\u89e3\u51b3\u4e863D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u4e0eLiDAR\u7ed3\u5408\u65f6\u7684\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u76f8\u673a\u4f20\u611f\u5668\uff08\u5305\u62ecRGB-D\uff09\u76843DGS\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u8d1f\u8f7d\u9ad8\u3001\u5728\u7eb9\u7406\u6216\u5149\u7167\u5dee\u7684\u73af\u5883\u4e2d\u5931\u6548\u4ee5\u53ca\u64cd\u4f5c\u8303\u56f4\u77ed\u7b49\u5c40\u9650\u6027\u3002LiDAR\u867d\u4e3a\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u4e0e3DGS\u7ed3\u5408\u65f6\u9762\u4e34\u5168\u5c40\u5bf9\u9f50\u7cbe\u5ea6\u548c\u7a00\u758f\u6570\u636e\u5bfc\u81f4\u4f18\u5316\u65f6\u95f4\u957f\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faGSFusion\u7cfb\u7edf\uff0c\u91c7\u7528surfel-to-surfel\u7ea6\u675f\u5b9e\u73b0\u5168\u5c40\u4f4d\u59ff\u56fe\u4f18\u5316\uff0c\u5e76\u901a\u8fc7\u50cf\u7d20\u611f\u77e5\u7684\u9ad8\u65af\u521d\u59cb\u5316\u7b56\u7565\u548c\u8fb9\u754csigmoid\u7ea6\u675f\u5904\u7406\u7a00\u758f\u6570\u636e\u3002", "result": "\u5728\u516c\u5f00\u548c\u81ea\u6709\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGSFusion\u5728\u6e32\u67d3\u8d28\u91cf\u548c\u5730\u56fe\u6784\u5efa\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u67093DGS SLAM\u7cfb\u7edf\u3002", "conclusion": "GSFusion\u901a\u8fc7\u521b\u65b0\u7684\u7ea6\u675f\u548c\u521d\u59cb\u5316\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86LiDAR\u4e0e3DGS\u7ed3\u5408\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2507.23676", "pdf": "https://arxiv.org/pdf/2507.23676", "abs": "https://arxiv.org/abs/2507.23676", "authors": ["Rabeya Tus Sadia", "Qiang Cheng"], "title": "DepMicroDiff: Diffusion-Based Dependency-Aware Multimodal Imputation for Microbiome Data", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Microbiome data analysis is essential for understanding host health and disease, yet its inherent sparsity and noise pose major challenges for accurate imputation, hindering downstream tasks such as biomarker discovery. Existing imputation methods, including recent diffusion-based models, often fail to capture the complex interdependencies between microbial taxa and overlook contextual metadata that can inform imputation. We introduce DepMicroDiff, a novel framework that combines diffusion-based generative modeling with a Dependency-Aware Transformer (DAT) to explicitly capture both mutual pairwise dependencies and autoregressive relationships. DepMicroDiff is further enhanced by VAE-based pretraining across diverse cancer datasets and conditioning on patient metadata encoded via a large language model (LLM). Experiments on TCGA microbiome datasets show that DepMicroDiff substantially outperforms state-of-the-art baselines, achieving higher Pearson correlation (up to 0.712), cosine similarity (up to 0.812), and lower RMSE and MAE across multiple cancer types, demonstrating its robustness and generalizability for microbiome imputation.", "AI": {"tldr": "DepMicroDiff\u662f\u4e00\u79cd\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u4f9d\u8d56\u611f\u77e5Transformer\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5fae\u751f\u7269\u7ec4\u6570\u636e\u63d2\u8865\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5fae\u751f\u7269\u7ec4\u6570\u636e\u7684\u7a00\u758f\u6027\u548c\u566a\u58f0\u5bf9\u51c6\u786e\u63d2\u8865\u6784\u6210\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u6355\u6349\u5fae\u751f\u7269\u95f4\u7684\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u548c\u4e0a\u4e0b\u6587\u5143\u6570\u636e\u3002", "method": "\u7ed3\u5408\u6269\u6563\u751f\u6210\u6a21\u578b\u548c\u4f9d\u8d56\u611f\u77e5Transformer\uff08DAT\uff09\uff0c\u5229\u7528VAE\u9884\u8bad\u7ec3\u548cLLM\u7f16\u7801\u7684\u5143\u6570\u636e\u8fdb\u884c\u6761\u4ef6\u5316\u3002", "result": "\u5728TCGA\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cPearson\u76f8\u5173\u7cfb\u6570\u8fbe0.712\uff0c\u4f59\u5f26\u76f8\u4f3c\u5ea6\u8fbe0.812\uff0cRMSE\u548cMAE\u66f4\u4f4e\u3002", "conclusion": "DepMicroDiff\u5728\u5fae\u751f\u7269\u7ec4\u6570\u636e\u63d2\u8865\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
