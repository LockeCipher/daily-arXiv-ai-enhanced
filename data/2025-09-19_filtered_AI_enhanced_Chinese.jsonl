{"id": "2509.15130", "pdf": "https://arxiv.org/pdf/2509.15130", "abs": "https://arxiv.org/abs/2509.15130", "authors": ["Chenxi Song", "Yanming Yang", "Tong Zhao", "Ruibo Li", "Chi Zhang"], "title": "WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Project Webpage: https://worldforge-agi.github.io/", "summary": "Recent video diffusion models demonstrate strong potential in spatial intelligence tasks due to their rich latent world priors. However, this potential is hindered by their limited controllability and geometric inconsistency, creating a gap between their strong priors and their practical use in 3D/4D tasks. As a result, current approaches often rely on retraining or fine-tuning, which risks degrading pretrained knowledge and incurs high computational costs. To address this, we propose WorldForge, a training-free, inference-time framework composed of three tightly coupled modules. Intra-Step Recursive Refinement introduces a recursive refinement mechanism during inference, which repeatedly optimizes network predictions within each denoising step to enable precise trajectory injection. Flow-Gated Latent Fusion leverages optical flow similarity to decouple motion from appearance in the latent space and selectively inject trajectory guidance into motion-related channels. Dual-Path Self-Corrective Guidance compares guided and unguided denoising paths to adaptively correct trajectory drift caused by noisy or misaligned structural signals. Together, these components inject fine-grained, trajectory-aligned guidance without training, achieving both accurate motion control and photorealistic content generation. Extensive experiments across diverse benchmarks validate our method's superiority in realism, trajectory consistency, and visual fidelity. This work introduces a novel plug-and-play paradigm for controllable video synthesis, offering a new perspective on leveraging generative priors for spatial intelligence.", "AI": {"tldr": "WorldForge\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u63a8\u7406\u65f6\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u8026\u5408\u6a21\u5757\u5b9e\u73b0\u7cbe\u786e\u7684\u8fd0\u52a8\u8f68\u8ff9\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u89c6\u9891\u6269\u6563\u6a21\u578b\u57283D/4D\u4efb\u52a1\u4e2d\u7684\u53ef\u63a7\u6027\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6269\u6563\u6a21\u578b\u867d\u7136\u5177\u6709\u4e30\u5bcc\u7684\u7a7a\u95f4\u667a\u80fd\u5148\u9a8c\u77e5\u8bc6\uff0c\u4f46\u53d7\u9650\u4e8e\u6709\u9650\u7684\u53ef\u63a7\u6027\u548c\u51e0\u4f55\u4e0d\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u5176\u5f3a\u5148\u9a8c\u77e5\u8bc6\u96be\u4ee5\u5b9e\u9645\u5e94\u7528\u4e8e3D/4D\u4efb\u52a1\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\uff0c\u8fd9\u4f1a\u964d\u4f4e\u9884\u8bad\u7ec3\u77e5\u8bc6\u8d28\u91cf\u5e76\u5e26\u6765\u9ad8\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u7d27\u5bc6\u8026\u5408\u7684\u6a21\u5757\uff1a1\uff09\u6b65\u5185\u9012\u5f52\u7ec6\u5316\u673a\u5236\uff0c\u5728\u6bcf\u4e2a\u53bb\u566a\u6b65\u9aa4\u5185\u91cd\u590d\u4f18\u5316\u7f51\u7edc\u9884\u6d4b\u4ee5\u5b9e\u73b0\u7cbe\u786e\u8f68\u8ff9\u6ce8\u5165\uff1b2\uff09\u6d41\u95e8\u63a7\u6f5c\u5728\u878d\u5408\uff0c\u5229\u7528\u5149\u6d41\u76f8\u4f3c\u6027\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5c06\u8fd0\u52a8\u4e0e\u5916\u89c2\u89e3\u8026\uff0c\u5e76\u9009\u62e9\u6027\u5730\u5c06\u8f68\u8ff9\u5f15\u5bfc\u6ce8\u5165\u8fd0\u52a8\u76f8\u5173\u901a\u9053\uff1b3\uff09\u53cc\u8def\u5f84\u81ea\u6821\u6b63\u5f15\u5bfc\uff0c\u6bd4\u8f83\u6709\u5f15\u5bfc\u548c\u65e0\u5f15\u5bfc\u7684\u53bb\u566a\u8def\u5f84\u4ee5\u81ea\u9002\u5e94\u6821\u6b63\u8f68\u8ff9\u6f02\u79fb\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u6027\u3001\u8f68\u8ff9\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u65b9\u9762\u7684\u4f18\u8d8a\u6027\uff0c\u80fd\u591f\u5728\u4e0d\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u7cbe\u786e\u7684\u8fd0\u52a8\u63a7\u5236\u548c\u903c\u771f\u7684\u5185\u5bb9\u751f\u6210\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u53ef\u63a7\u89c6\u9891\u5408\u6210\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5373\u63d2\u5373\u7528\u8303\u5f0f\uff0c\u4e3a\u5229\u7528\u751f\u6210\u5148\u9a8c\u8fdb\u884c\u7a7a\u95f4\u667a\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2509.14476", "pdf": "https://arxiv.org/pdf/2509.14476", "abs": "https://arxiv.org/abs/2509.14476", "authors": ["Jiasen Lu", "Liangchen Song", "Mingze Xu", "Byeongjoo Ahn", "Yanjun Wang", "Chen Chen", "Afshin Dehghan", "Yinfei Yang"], "title": "AToken: A Unified Tokenizer for Vision", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "30 pages, 14 figures", "summary": "We present AToken, the first unified visual tokenizer that achieves both high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets. Unlike existing tokenizers that specialize in either reconstruction or understanding for single modalities, AToken encodes these diverse visual inputs into a shared 4D latent space, unifying both tasks and modalities in a single framework. Specifically, we introduce a pure transformer architecture with 4D rotary position embeddings to process visual inputs of arbitrary resolutions and temporal durations. To ensure stable training, we introduce an adversarial-free training objective that combines perceptual and Gram matrix losses, achieving state-of-the-art reconstruction quality. By employing a progressive training curriculum, AToken gradually expands from single images, videos, and 3D, and supports both continuous and discrete latent tokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01 rFVD with 32.6% MSRVTT retrieval for videos, and 28.19 PSNR with 90.9% classification accuracy for 3D. In downstream applications, AToken enables both visual generation tasks (e.g., image generation with continuous and discrete tokens, text-to-video generation, image-to-3D synthesis) and understanding tasks (e.g., multimodal LLMs), achieving competitive performance across all benchmarks. These results shed light on the next-generation multimodal AI systems built upon unified visual tokenization.", "AI": {"tldr": "AToken\u662f\u9996\u4e2a\u7edf\u4e00\u7684\u89c6\u89c9\u5206\u8bcd\u5668\uff0c\u80fd\u591f\u5728\u56fe\u50cf\u3001\u89c6\u9891\u548c3D\u8d44\u4ea7\u4e0a\u540c\u65f6\u5b9e\u73b0\u9ad8\u4fdd\u771f\u91cd\u5efa\u548c\u8bed\u4e49\u7406\u89e3\uff0c\u901a\u8fc74D\u6f5c\u5728\u7a7a\u95f4\u7edf\u4e00\u591a\u79cd\u89c6\u89c9\u6a21\u6001\u548c\u5904\u7406\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u5206\u8bcd\u5668\u901a\u5e38\u4e13\u6ce8\u4e8e\u5355\u4e00\u6a21\u6001\u7684\u91cd\u5efa\u6216\u7406\u89e3\u4efb\u52a1\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u6846\u67b6\u6765\u5904\u7406\u591a\u79cd\u89c6\u89c9\u8f93\u5165\u7c7b\u578b\u3002AToken\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u591a\u6a21\u6001AI\u7cfb\u7edf\u63d0\u4f9b\u7edf\u4e00\u7684\u89c6\u89c9\u5206\u8bcd\u57fa\u7840\u3002", "method": "\u91c7\u7528\u7eafTransformer\u67b6\u6784\uff0c\u5f15\u51654D\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\u5904\u7406\u4efb\u610f\u5206\u8fa8\u7387\u548c\u65f6\u957f\u7684\u89c6\u89c9\u8f93\u5165\u3002\u4f7f\u7528\u65e0\u5bf9\u6297\u8bad\u7ec3\u76ee\u6807\uff0c\u7ed3\u5408\u611f\u77e5\u635f\u5931\u548cGram\u77e9\u9635\u635f\u5931\u786e\u4fdd\u7a33\u5b9a\u8bad\u7ec3\u3002\u901a\u8fc7\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u8bfe\u7a0b\u4ece\u5355\u56fe\u50cf\u6269\u5c55\u5230\u89c6\u9891\u548c3D\uff0c\u652f\u6301\u8fde\u7eed\u548c\u79bb\u6563\u6f5c\u5728token\u3002", "result": "\u5728\u56fe\u50cf\u4e0a\u8fbe\u52300.21 rFID\u548c82.2% ImageNet\u51c6\u786e\u7387\uff1b\u89c6\u9891\u4e0a\u8fbe\u52303.01 rFVD\u548c32.6% MSRVTT\u68c0\u7d22\u7387\uff1b3D\u4e0a\u8fbe\u523028.19 PSNR\u548c90.9%\u5206\u7c7b\u51c6\u786e\u7387\u3002\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u652f\u6301\u89c6\u89c9\u751f\u6210\u548c\u7406\u89e3\u4efb\u52a1\uff0c\u5728\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "AToken\u5c55\u793a\u4e86\u7edf\u4e00\u89c6\u89c9\u5206\u8bcd\u5728\u6784\u5efa\u4e0b\u4e00\u4ee3\u591a\u6a21\u6001AI\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u540c\u65f6\u5904\u7406\u91cd\u5efa\u548c\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.14550", "pdf": "https://arxiv.org/pdf/2509.14550", "abs": "https://arxiv.org/abs/2509.14550", "authors": ["Penghao Rao", "Tieyong Zeng"], "title": "Edge-Aware Normalized Attention for Efficient and Detail-Preserving Single Image Super-Resolution", "categories": ["cs.CV", "68T45, 68T07, 68U10"], "comment": "13 pages", "summary": "Single-image super-resolution (SISR) remains highly ill-posed because recovering structurally faithful high-frequency content from a single low-resolution observation is ambiguous. Existing edge-aware methods often attach edge priors or attention branches onto increasingly complex backbones, yet ad hoc fusion frequently introduces redundancy, unstable optimization, or limited structural gains. We address this gap with an edge-guided attention mechanism that derives an adaptive modulation map from jointly encoded edge features and intermediate feature activations, then applies it to normalize and reweight responses, selectively amplifying structurally salient regions while suppressing spurious textures. In parallel, we integrate this mechanism into a lightweight residual design trained under a composite objective combining pixel-wise, perceptual, and adversarial terms to balance fidelity, perceptual realism, and training stability. Extensive experiments on standard SISR benchmarks demonstrate consistent improvements in structural sharpness and perceptual quality over SRGAN, ESRGAN, and prior edge-attention baselines at comparable model complexity. The proposed formulation provides (i) a parameter-efficient path to inject edge priors, (ii) stabilized adversarial refinement through a tailored multiterm loss, and (iii) enhanced edge fidelity without resorting to deeper or heavily overparameterized architectures. These results highlight the effectiveness of principled edge-conditioned modulation for advancing perceptual super-resolution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fb9\u7f18\u5f15\u5bfc\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u5236\u56fe\u9009\u62e9\u6027\u589e\u5f3a\u7ed3\u6784\u663e\u8457\u533a\u57df\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u6b8b\u5dee\u8bbe\u8ba1\u548c\u590d\u5408\u635f\u5931\u51fd\u6570\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u590d\u6742\u5ea6\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u4e86\u8d85\u5206\u8fa8\u7387\u7684\u7ed3\u6784\u6e05\u6670\u5ea6\u548c\u611f\u77e5\u8d28\u91cf\u3002", "motivation": "\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387(SISR)\u662f\u4e00\u4e2a\u9ad8\u5ea6\u4e0d\u9002\u5b9a\u95ee\u9898\uff0c\u73b0\u6709\u8fb9\u7f18\u611f\u77e5\u65b9\u6cd5\u5f80\u5f80\u5728\u590d\u6742\u9aa8\u5e72\u7f51\u7edc\u4e0a\u9644\u52a0\u8fb9\u7f18\u5148\u9a8c\u6216\u6ce8\u610f\u529b\u5206\u652f\uff0c\u4f46\u4e34\u65f6\u878d\u5408\u7ecf\u5e38\u5f15\u5165\u5197\u4f59\u3001\u4f18\u5316\u4e0d\u7a33\u5b9a\u6216\u7ed3\u6784\u589e\u76ca\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u8fb9\u7f18\u5f15\u5bfc\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ece\u8054\u5408\u7f16\u7801\u7684\u8fb9\u7f18\u7279\u5f81\u548c\u4e2d\u95f4\u7279\u5f81\u6fc0\u6d3b\u4e2d\u63a8\u5bfc\u81ea\u9002\u5e94\u8c03\u5236\u56fe\uff0c\u7528\u4e8e\u5f52\u4e00\u5316\u548c\u91cd\u52a0\u6743\u54cd\u5e94\uff1b\u96c6\u6210\u5230\u8f7b\u91cf\u7ea7\u6b8b\u5dee\u8bbe\u8ba1\u4e2d\uff0c\u4f7f\u7528\u7ed3\u5408\u50cf\u7d20\u7ea7\u3001\u611f\u77e5\u548c\u5bf9\u6297\u9879\u7684\u590d\u5408\u76ee\u6807\u51fd\u6570\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u6807\u51c6SISR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4SRGAN\u3001ESRGAN\u548c\u73b0\u6709\u8fb9\u7f18\u6ce8\u610f\u529b\u57fa\u7ebf\uff0c\u5728\u53ef\u6bd4\u6a21\u578b\u590d\u6742\u5ea6\u4e0b\u5b9e\u73b0\u4e86\u7ed3\u6784\u6e05\u6670\u5ea6\u548c\u611f\u77e5\u8d28\u91cf\u7684\u4e00\u81f4\u63d0\u5347\u3002", "conclusion": "\u8be5\u516c\u5f0f\u63d0\u4f9b\u4e86\u53c2\u6570\u9ad8\u6548\u7684\u8fb9\u7f18\u5148\u9a8c\u6ce8\u5165\u8def\u5f84\u3001\u901a\u8fc7\u5b9a\u5236\u591a\u672f\u8bed\u635f\u5931\u7a33\u5b9a\u5bf9\u6297\u7ec6\u5316\uff0c\u4ee5\u53ca\u5728\u4e0d\u4f9d\u8d56\u66f4\u6df1\u6216\u8fc7\u5ea6\u53c2\u6570\u5316\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\u589e\u5f3a\u8fb9\u7f18\u4fdd\u771f\u5ea6\uff0c\u8bc1\u660e\u4e86\u539f\u5219\u6027\u8fb9\u7f18\u6761\u4ef6\u8c03\u5236\u5728\u63a8\u8fdb\u611f\u77e5\u8d85\u5206\u8fa8\u7387\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.14560", "pdf": "https://arxiv.org/pdf/2509.14560", "abs": "https://arxiv.org/abs/2509.14560", "authors": ["Zhaonan Wang", "Manyi Li", "ShiQing Xin", "Changhe Tu"], "title": "Adaptive and Iterative Point Cloud Denoising with Score-Based Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "Point cloud denoising task aims to recover the clean point cloud from the scanned data coupled with different levels or patterns of noise. The recent state-of-the-art methods often train deep neural networks to update the point locations towards the clean point cloud, and empirically repeat the denoising process several times in order to obtain the denoised results. It is not clear how to efficiently arrange the iterative denoising processes to deal with different levels or patterns of noise. In this paper, we propose an adaptive and iterative point cloud denoising method based on the score-based diffusion model. For a given noisy point cloud, we first estimate the noise variation and determine an adaptive denoising schedule with appropriate step sizes, then invoke the trained network iteratively to update point clouds following the adaptive schedule. To facilitate this adaptive and iterative denoising process, we design the network architecture and a two-stage sampling strategy for the network training to enable feature fusion and gradient fusion for iterative denoising. Compared to the state-of-the-art point cloud denoising methods, our approach obtains clean and smooth denoised point clouds, while preserving the shape boundary and details better. Our results not only outperform the other methods both qualitatively and quantitatively, but also are preferable on the synthetic dataset with different patterns of noises, as well as the real-scanned dataset.", "AI": {"tldr": "\u57fa\u4e8e\u8bc4\u5206\u6a21\u578b\u7684\u9002\u5e94\u6027\u8fed\u4ee3\u70b9\u4e91\u53bb\u566a\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f30\u8ba1\u566a\u58f0\u5f3a\u5ea6\u5236\u5b9a\u9002\u5e94\u8fed\u4ee3\u8ba1\u5212\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5177\u6709\u7279\u5f81\u878d\u5408\u548c\u68af\u5ea6\u878d\u5408\u7684\u7f51\u7edc\u7ed3\u6784", "motivation": "\u89e3\u51b3\u73b0\u6709\u70b9\u4e91\u53bb\u566a\u65b9\u6cd5\u5728\u8fed\u4ee3\u6b21\u6570\u548c\u566a\u58f0\u6a21\u5f0f\u9002\u5e94\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u65e0\u6cd5\u9ad8\u6548\u5904\u7406\u4e0d\u540c\u6c34\u5e73\u6216\u6a21\u5f0f\u7684\u566a\u58f0", "method": "\u4f7f\u7528\u8bc4\u5206\u6a21\u578b\u6846\u67b6\uff0c\u9996\u5148\u4f30\u8ba1\u566a\u58f0\u5f3a\u5ea6\u5e76\u5236\u5b9a\u9002\u5e94\u6027\u8fed\u4ee3\u8ba1\u5212\uff0c\u7136\u540e\u901a\u8fc7\u8bad\u7ec3\u597d\u7684\u7f51\u7edc\u8fdb\u884c\u8fed\u4ee3\u66f4\u65b0\u3002\u8bbe\u8ba1\u4e86\u5177\u6709\u7279\u5f81\u878d\u5408\u548c\u68af\u5ea6\u878d\u5408\u7684\u7f51\u7edc\u7ed3\u6784\u4ee5\u652f\u6301\u8fed\u4ee3\u53bb\u566a", "result": "\u5728\u5408\u6210\u548c\u5b9e\u9645\u626b\u63cf\u6570\u636e\u96c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u8d85\u8fc7\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u7684\u7ed3\u679c\uff0c\u80fd\u591f\u83b7\u5f97\u6e05\u6d01\u5e73\u6ed1\u7684\u53bb\u566a\u70b9\u4e91\uff0c\u540c\u65f6\u4fdd\u7559\u5f62\u72b6\u8fb9\u754c\u548c\u7ec6\u8282", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u9002\u5e94\u6027\u8fed\u4ee3\u8ba1\u5212\u548c\u7279\u5f81\u878d\u5408\u7f51\u7edc\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u540c\u566a\u58f0\u6a21\u5f0f\u7684\u53bb\u566a\u95ee\u9898\uff0c\u5728\u8d28\u91cf\u548c\u6570\u91cf\u4e0a\u90fd\u8868\u73b0\u4f18\u5f02"}}
{"id": "2509.14565", "pdf": "https://arxiv.org/pdf/2509.14565", "abs": "https://arxiv.org/abs/2509.14565", "authors": ["Li Gao", "Hongyang Sun", "Liu Liu", "Yunhao Li", "Yang Cai"], "title": "DiffVL: Diffusion-Based Visual Localization on 2D Maps via BEV-Conditioned GPS Denoising", "categories": ["cs.CV"], "comment": null, "summary": "Accurate visual localization is crucial for autonomous driving, yet existing methods face a fundamental dilemma: While high-definition (HD) maps provide high-precision localization references, their costly construction and maintenance hinder scalability, which drives research toward standard-definition (SD) maps like OpenStreetMap. Current SD-map-based approaches primarily focus on Bird's-Eye View (BEV) matching between images and maps, overlooking a ubiquitous signal-noisy GPS. Although GPS is readily available, it suffers from multipath errors in urban environments. We propose DiffVL, the first framework to reformulate visual localization as a GPS denoising task using diffusion models. Our key insight is that noisy GPS trajectory, when conditioned on visual BEV features and SD maps, implicitly encode the true pose distribution, which can be recovered through iterative diffusion refinement. DiffVL, unlike prior BEV-matching methods (e.g., OrienterNet) or transformer-based registration approaches, learns to reverse GPS noise perturbations by jointly modeling GPS, SD map, and visual signals, achieving sub-meter accuracy without relying on HD maps. Experiments on multiple datasets demonstrate that our method achieves state-of-the-art accuracy compared to BEV-matching baselines. Crucially, our work proves that diffusion models can enable scalable localization by treating noisy GPS as a generative prior-making a paradigm shift from traditional matching-based methods.", "AI": {"tldr": "DiffVL\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c6\u89c9\u5b9a\u4f4d\u6846\u67b6\uff0c\u5c06\u89c6\u89c9\u5b9a\u4f4d\u91cd\u65b0\u5b9a\u4e49\u4e3aGPS\u53bb\u566a\u4efb\u52a1\uff0c\u5229\u7528SD\u5730\u56fe\u548c\u89c6\u89c9BEV\u7279\u5f81\u6765\u6062\u590d\u566a\u58f0GPS\u8f68\u8ff9\u4e2d\u7684\u771f\u5b9e\u4f4d\u59ff\u5206\u5e03\uff0c\u5b9e\u73b0\u4e86\u4e9a\u7c73\u7ea7\u7cbe\u5ea6\u800c\u65e0\u9700HD\u5730\u56fe\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u9762\u4e34HD\u5730\u56fe\u6210\u672c\u9ad8\u3001SD\u5730\u56fe\u65b9\u6cd5\u5ffd\u7565\u566a\u58f0GPS\u4fe1\u53f7\u7684\u95ee\u9898\u3002GPS\u5728\u57ce\u5e02\u573a\u666f\u4e2d\u5bb9\u6613\u53d7\u5230\u591a\u8def\u5f84\u8bef\u5dee\u5f71\u54cd\uff0c\u4f46\u4ecd\u7136\u662f\u666e\u904d\u53ef\u7528\u7684\u4fe1\u53f7\u6e90\u3002", "method": "\u63d0\u51faDiffVL\u6846\u67b6\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u5c06\u566a\u58f0GPS\u8f68\u8ff9\u4e0e\u89c6\u89c9BEV\u7279\u5f81\u548cSD\u5730\u56fe\u8054\u5408\u5efa\u6a21\uff0c\u901a\u8fc7\u8fed\u4ee3\u6269\u6563\u7ec6\u5316\u6765\u6062\u590d\u771f\u5b9e\u4f4d\u59ff\u5206\u5e03\uff0c\u53cd\u5411\u5b66\u4e60GPS\u566a\u58f0\u6270\u52a8\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4BEV\u5339\u914d\u57fa\u7ebf\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\u6c34\u5e73\uff0c\u5b9e\u73b0\u4e86\u4e9a\u7c73\u7ea7\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u5c06\u566a\u58f0GPS\u4f5c\u4e3a\u751f\u6210\u5148\u9a8c\u6765\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u5b9a\u4f4d\uff0c\u8fd9\u4ee3\u8868\u4e86\u4ece\u4f20\u7edf\u57fa\u4e8e\u5339\u914d\u65b9\u6cd5\u5411\u751f\u6210\u5f0f\u65b9\u6cd5\u7684\u8303\u5f0f\u8f6c\u53d8\u3002"}}
{"id": "2509.14638", "pdf": "https://arxiv.org/pdf/2509.14638", "abs": "https://arxiv.org/abs/2509.14638", "authors": ["Mingsong Li", "Lin Liu", "Hongjun Wang", "Haoxing Chen", "Xijun Gu", "Shizhan Liu", "Dong Gong", "Junbo Zhao", "Zhenzhong Lan", "Jianguo Li"], "title": "MultiEdit: Advancing Instruction-based Image Editing on Diverse and Challenging Tasks", "categories": ["cs.CV"], "comment": null, "summary": "Current instruction-based image editing (IBIE) methods struggle with challenging editing tasks, as both editing types and sample counts of existing datasets are limited. Moreover, traditional dataset construction often contains noisy image-caption pairs, which may introduce biases and limit model capabilities in complex editing scenarios. To address these limitations, we introduce MultiEdit, a comprehensive dataset featuring over 107K high-quality image editing samples. It encompasses 6 challenging editing tasks through a diverse collection of 18 non-style-transfer editing types and 38 style transfer operations, covering a spectrum from sophisticated style transfer to complex semantic operations like person reference editing and in-image text editing. We employ a novel dataset construction pipeline that utilizes two multi-modal large language models (MLLMs) to generate visual-adaptive editing instructions and produce high-fidelity edited images, respectively. Extensive experiments demonstrate that fine-tuning foundational open-source models with our MultiEdit-Train set substantially improves models' performance on sophisticated editing tasks in our proposed MultiEdit-Test benchmark, while effectively preserving their capabilities on the standard editing benchmark. We believe MultiEdit provides a valuable resource for advancing research into more diverse and challenging IBIE capabilities. Our dataset is available at https://huggingface.co/datasets/inclusionAI/MultiEdit.", "AI": {"tldr": "MultiEdit\u662f\u4e00\u4e2a\u5305\u542b10.7\u4e07\u9ad8\u8d28\u91cf\u56fe\u50cf\u7f16\u8f91\u6837\u672c\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u6db5\u76d66\u4e2a\u6311\u6218\u6027\u7f16\u8f91\u4efb\u52a1\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6d41\u6c34\u7ebf\u751f\u6210\u89c6\u89c9\u81ea\u9002\u5e94\u7f16\u8f91\u6307\u4ee4\u548c\u9ad8\u8d28\u91cf\u7f16\u8f91\u56fe\u50cf\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u7f16\u8f91\u4efb\u52a1\u65f6\u7684\u5c40\u9650\u6027\uff0c\u73b0\u6709\u6570\u636e\u96c6\u7f16\u8f91\u7c7b\u578b\u548c\u6837\u672c\u6570\u91cf\u6709\u9650\uff0c\u4e14\u4f20\u7edf\u6570\u636e\u96c6\u6784\u5efa\u5b58\u5728\u566a\u58f0\u56fe\u50cf-\u6807\u9898\u5bf9\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u590d\u6742\u7f16\u8f91\u573a\u666f\u4e2d\u80fd\u529b\u53d7\u9650\u3002", "method": "\u91c7\u7528\u65b0\u9896\u7684\u6570\u636e\u96c6\u6784\u5efa\u6d41\u6c34\u7ebf\uff0c\u5229\u7528\u4e24\u4e2a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5206\u522b\u751f\u6210\u89c6\u89c9\u81ea\u9002\u5e94\u7f16\u8f91\u6307\u4ee4\u548c\u5236\u4f5c\u9ad8\u4fdd\u771f\u5ea6\u7f16\u8f91\u56fe\u50cf\uff0c\u6db5\u76d618\u79cd\u975e\u98ce\u683c\u8f6c\u6362\u7f16\u8f91\u7c7b\u578b\u548c38\u79cd\u98ce\u683c\u8f6c\u6362\u64cd\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528MultiEdit-Train\u96c6\u5fae\u8c03\u57fa\u7840\u5f00\u6e90\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u5728\u590d\u6742\u7f16\u8f91\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u540c\u65f6\u6709\u6548\u4fdd\u6301\u4e86\u5728\u6807\u51c6\u7f16\u8f91\u57fa\u51c6\u4e0a\u7684\u80fd\u529b\u3002", "conclusion": "MultiEdit\u4e3a\u63a8\u8fdb\u66f4\u591a\u6837\u5316\u3001\u66f4\u5177\u6311\u6218\u6027\u7684\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u80fd\u529b\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u6570\u636e\u96c6\u5df2\u5728HuggingFace\u4e0a\u516c\u5f00\u3002"}}
{"id": "2509.14685", "pdf": "https://arxiv.org/pdf/2509.14685", "abs": "https://arxiv.org/abs/2509.14685", "authors": ["Kazuma Nagata", "Naoshi Kaneko"], "title": "DACoN: DINO for Anime Paint Bucket Colorization with Any Number of Reference Images", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "Automatic colorization of line drawings has been widely studied to reduce the labor cost of hand-drawn anime production. Deep learning approaches, including image/video generation and feature-based correspondence, have improved accuracy but struggle with occlusions, pose variations, and viewpoint changes. To address these challenges, we propose DACoN, a framework that leverages foundation models to capture part-level semantics, even in line drawings. Our method fuses low-resolution semantic features from foundation models with high-resolution spatial features from CNNs for fine-grained yet robust feature extraction. In contrast to previous methods that rely on the Multiplex Transformer and support only one or two reference images, DACoN removes this constraint, allowing any number of references. Quantitative and qualitative evaluations demonstrate the benefits of using multiple reference images, achieving superior colorization performance. Our code and model are available at https://github.com/kzmngt/DACoN.", "AI": {"tldr": "DACoN\u662f\u4e00\u4e2a\u5229\u7528\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u7ebf\u6761\u753b\u81ea\u52a8\u4e0a\u8272\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u57fa\u7840\u6a21\u578b\u7684\u4f4e\u5206\u8fa8\u7387\u8bed\u4e49\u7279\u5f81\u548cCNN\u7684\u9ad8\u5206\u8fa8\u7387\u7a7a\u95f4\u7279\u5f81\uff0c\u652f\u6301\u4efb\u610f\u6570\u91cf\u7684\u53c2\u8003\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u906e\u6321\u3001\u59ff\u6001\u53d8\u5316\u548c\u89c6\u89d2\u53d8\u5316\u7b49\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u81ea\u52a8\u4e0a\u8272\u65b9\u6cd5\u5728\u5904\u7406\u906e\u6321\u3001\u59ff\u6001\u53d8\u5316\u548c\u89c6\u89d2\u53d8\u5316\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u4e14\u901a\u5e38\u53ea\u80fd\u652f\u63011-2\u4e2a\u53c2\u8003\u56fe\u50cf\uff0c\u9650\u5236\u4e86\u4e0a\u8272\u6548\u679c\u3002", "method": "\u63d0\u51faDACoN\u6846\u67b6\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u90e8\u5206\u7ea7\u8bed\u4e49\u7279\u5f81\uff08\u5373\u4f7f\u5728\u7ebf\u6761\u753b\u4e2d\uff09\uff0c\u5e76\u5c06\u5176\u4e0eCNN\u63d0\u53d6\u7684\u9ad8\u5206\u8fa8\u7387\u7a7a\u95f4\u7279\u5f81\u878d\u5408\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u4e14\u9c81\u68d2\u7684\u7279\u5f81\u63d0\u53d6\uff0c\u540c\u65f6\u79fb\u9664\u4e86\u5bf9\u53c2\u8003\u56fe\u50cf\u6570\u91cf\u7684\u9650\u5236\u3002", "result": "\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u8868\u660e\uff0c\u4f7f\u7528\u591a\u4e2a\u53c2\u8003\u56fe\u50cf\u80fd\u5e26\u6765\u663e\u8457\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u4e0a\u8272\u6027\u80fd\u3002", "conclusion": "DACoN\u901a\u8fc7\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u548cCNN\u7684\u7a7a\u95f4\u7ec6\u8282\u63d0\u53d6\u80fd\u529b\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u5f3a\u5927\u548c\u7075\u6d3b\u7684\u81ea\u52a8\u4e0a\u8272\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.14739", "pdf": "https://arxiv.org/pdf/2509.14739", "abs": "https://arxiv.org/abs/2509.14739", "authors": ["Jinlong Fan", "Bingyu Hu", "Xingguang Li", "Yuxiang Yang", "Jing Zhang"], "title": "FMGS-Avatar: Mesh-Guided 2D Gaussian Splatting with Foundation Model Priors for 3D Monocular Avatar Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing high-fidelity animatable human avatars from monocular videos remains challenging due to insufficient geometric information in single-view observations. While recent 3D Gaussian Splatting methods have shown promise, they struggle with surface detail preservation due to the free-form nature of 3D Gaussian primitives. To address both the representation limitations and information scarcity, we propose a novel method, \\textbf{FMGS-Avatar}, that integrates two key innovations. First, we introduce Mesh-Guided 2D Gaussian Splatting, where 2D Gaussian primitives are attached directly to template mesh faces with constrained position, rotation, and movement, enabling superior surface alignment and geometric detail preservation. Second, we leverage foundation models trained on large-scale datasets, such as Sapiens, to complement the limited visual cues from monocular videos. However, when distilling multi-modal prior knowledge from foundation models, conflicting optimization objectives can emerge as different modalities exhibit distinct parameter sensitivities. We address this through a coordinated training strategy with selective gradient isolation, enabling each loss component to optimize its relevant parameters without interference. Through this combination of enhanced representation and coordinated information distillation, our approach significantly advances 3D monocular human avatar reconstruction. Experimental evaluation demonstrates superior reconstruction quality compared to existing methods, with notable gains in geometric accuracy and appearance fidelity while providing rich semantic information. Additionally, the distilled prior knowledge within a shared canonical space naturally enables spatially and temporally consistent rendering under novel views and poses.", "AI": {"tldr": "FMGS-Avatar\u662f\u4e00\u79cd\u4ece\u5355\u76ee\u89c6\u9891\u91cd\u5efa\u9ad8\u4fdd\u771f\u53ef\u52a8\u753b\u4eba\u4f53\u5316\u8eab\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f51\u683c\u5f15\u5bfc\u76842D\u9ad8\u65af\u6cfc\u6e85\u548c\u57fa\u7840\u6a21\u578b\u77e5\u8bc6\u84b8\u998f\uff0c\u89e3\u51b3\u4e86\u51e0\u4f55\u4fe1\u606f\u4e0d\u8db3\u548c\u8868\u793a\u9650\u5236\u7684\u95ee\u9898\u3002", "motivation": "\u4ece\u5355\u76ee\u89c6\u9891\u91cd\u5efa\u9ad8\u4fdd\u771f\u53ef\u52a8\u753b\u4eba\u4f53\u5316\u8eab\u9762\u4e34\u51e0\u4f55\u4fe1\u606f\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u73b0\u67093D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u7531\u4e8e\u81ea\u7531\u5f62\u5f0f\u76843D\u9ad8\u65af\u57fa\u5143\u800c\u96be\u4ee5\u4fdd\u6301\u8868\u9762\u7ec6\u8282\u3002", "method": "\u63d0\u51fa\u7f51\u683c\u5f15\u5bfc\u76842D\u9ad8\u65af\u6cfc\u6e85\uff0c\u5c062D\u9ad8\u65af\u57fa\u5143\u76f4\u63a5\u9644\u52a0\u5230\u6a21\u677f\u7f51\u683c\u9762\u4e0a\uff0c\u5e76\u5229\u7528\u57fa\u7840\u6a21\u578b\uff08\u5982Sapiens\uff09\u8865\u5145\u5355\u76ee\u89c6\u9891\u7684\u6709\u9650\u89c6\u89c9\u7ebf\u7d22\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u68af\u5ea6\u9694\u79bb\u7684\u534f\u8c03\u8bad\u7ec3\u7b56\u7565\u89e3\u51b3\u591a\u6a21\u6001\u4f18\u5316\u51b2\u7a81\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u5728\u51e0\u4f55\u7cbe\u5ea6\u548c\u5916\u89c2\u4fdd\u771f\u5ea6\u65b9\u9762\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u63d0\u4f9b\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u652f\u6301\u65b0\u9896\u89c6\u89d2\u548c\u59ff\u6001\u4e0b\u65f6\u7a7a\u4e00\u81f4\u7684\u6e32\u67d3\u3002", "conclusion": "\u901a\u8fc7\u589e\u5f3a\u8868\u793a\u548c\u534f\u8c03\u4fe1\u606f\u84b8\u998f\u7684\u7ed3\u5408\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63a8\u8fdb\u4e863D\u5355\u76ee\u4eba\u4f53\u5316\u8eab\u91cd\u5efa\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u51e0\u4f55\u7ec6\u8282\u4fdd\u6301\u548c\u8bed\u4e49\u4fe1\u606f\u63d0\u53d6\u3002"}}
{"id": "2509.14777", "pdf": "https://arxiv.org/pdf/2509.14777", "abs": "https://arxiv.org/abs/2509.14777", "authors": ["Sunwoo Cho", "Yejin Jung", "Nam Ik Cho", "Jae Woong Soh"], "title": "Dataset Distillation for Super-Resolution without Class Labels and Pre-trained Models", "categories": ["cs.CV"], "comment": null, "summary": "Training deep neural networks has become increasingly demanding, requiring large datasets and significant computational resources, especially as model complexity advances. Data distillation methods, which aim to improve data efficiency, have emerged as promising solutions to this challenge. In the field of single image super-resolution (SISR), the reliance on large training datasets highlights the importance of these techniques. Recently, a generative adversarial network (GAN) inversion-based data distillation framework for SR was proposed, showing potential for better data utilization. However, the current method depends heavily on pre-trained SR networks and class-specific information, limiting its generalizability and applicability. To address these issues, we introduce a new data distillation approach for image SR that does not need class labels or pre-trained SR models. In particular, we first extract high-gradient patches and categorize images based on CLIP features, then fine-tune a diffusion model on the selected patches to learn their distribution and synthesize distilled training images. Experimental results show that our method achieves state-of-the-art performance while using significantly less training data and requiring less computational time. Specifically, when we train a baseline Transformer model for SR with only 0.68\\% of the original dataset, the performance drop is just 0.3 dB. In this case, diffusion model fine-tuning takes 4 hours, and SR model training completes within 1 hour, much shorter than the 11-hour training time with the full dataset.", "AI": {"tldr": "\u4e00\u79cd\u65e0\u9700\u7c7b\u6807\u7b7e\u6216\u9884\u8bad\u7ec3SR\u6a21\u578b\u7684\u65b0\u578b\u6570\u636e\u840c\u84ec\u65b9\u6cd5\uff0c\u901a\u8fc7CLIP\u7279\u5f81\u5206\u7c7b\u548c\u6a21\u578b\u5fae\u8c03\u6765\u751f\u6210\u8d85\u5206\u8fa8\u7387\u8bad\u7ec3\u6570\u636e\uff0c\u5728\u4ec5\u4f7f\u75280.68%\u539f\u59cb\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfGAN\u53cd\u5411\u6570\u636e\u840c\u84ec\u65b9\u6cd5\u5bf9\u9884\u8bad\u7ec3SR\u6a21\u578b\u548c\u7c7b\u522b\u6807\u7b7e\u7684\u4f9d\u8d56\u6027\uff0c\u63d0\u9ad8\u65b9\u6cd5\u7684\u901a\u7528\u6027\u548c\u9002\u7528\u6027\u3002", "method": "\u9996\u5148\u63d0\u53d6\u9ad8\u68af\u5ea6\u8865\u4e01\u5e76\u4f7f\u7528CLIP\u7279\u5f81\u8fdb\u884c\u56fe\u50cf\u5206\u7c7b\uff0c\u7136\u540e\u5bf9\u9009\u62e9\u7684\u8865\u4e01\u5fae\u8c03\u6a21\u578b\u4ee5\u5b66\u4e60\u5176\u5206\u5e03\u5e76\u5408\u6210\u840c\u84ec\u8bad\u7ec3\u56fe\u50cf\u3002", "result": "\u5728\u4ec5\u4f7f\u75280.68%\u539f\u59cb\u6570\u636e\u8bad\u7ec3Transformer SR\u6a21\u578b\u65f6\uff0c\u6027\u80fd\u4e0b\u964d\u4ec50.3dB\uff0c\u6a21\u578b\u5fae\u8c03\u8017\u65f64\u5c0f\u65f6\uff0cSR\u6a21\u578b\u8bad\u7ec3\u8017\u65f61\u5c0f\u65f6\uff0c\u8fdc\u5c11\u4e8e\u4f7f\u7528\u5168\u91cf\u6570\u636e\u768411\u5c0f\u65f6\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u72ec\u7279\u7684\u6570\u636e\u840c\u84ec\u6548\u679c\uff0c\u5728\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u6570\u636e\u548c\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u72ec\u5236\u7684\u6027\u80fd\u6c34\u5e73\u3002"}}
{"id": "2509.14780", "pdf": "https://arxiv.org/pdf/2509.14780", "abs": "https://arxiv.org/abs/2509.14780", "authors": ["Sina Amirrajab", "Zohaib Salahuddin", "Sheng Kuang", "Henry C. Woodruff", "Philippe Lambin"], "title": "Radiology Report Conditional 3D CT Generation with Multi Encoder Latent diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "Text to image latent diffusion models have recently advanced medical image synthesis, but applications to 3D CT generation remain limited. Existing approaches rely on simplified prompts, neglecting the rich semantic detail in full radiology reports, which reduces text image alignment and clinical fidelity. We propose Report2CT, a radiology report conditional latent diffusion framework for synthesizing 3D chest CT volumes directly from free text radiology reports, incorporating both findings and impression sections using multiple text encoder. Report2CT integrates three pretrained medical text encoders (BiomedVLP CXR BERT, MedEmbed, and ClinicalBERT) to capture nuanced clinical context. Radiology reports and voxel spacing information condition a 3D latent diffusion model trained on 20000 CT volumes from the CT RATE dataset. Model performance was evaluated using Frechet Inception Distance (FID) for real synthetic distributional similarity and CLIP based metrics for semantic alignment, with additional qualitative and quantitative comparisons against GenerateCT model. Report2CT generated anatomically consistent CT volumes with excellent visual quality and text image alignment. Multi encoder conditioning improved CLIP scores, indicating stronger preservation of fine grained clinical details in the free text radiology reports. Classifier free guidance further enhanced alignment with only a minor trade off in FID. We ranked first in the VLM3D Challenge at MICCAI 2025 on Text Conditional CT Generation and achieved state of the art performance across all evaluation metrics. By leveraging complete radiology reports and multi encoder text conditioning, Report2CT advances 3D CT synthesis, producing clinically faithful and high quality synthetic data.", "AI": {"tldr": "Report2CT\u662f\u4e00\u4e2a\u57fa\u4e8e\u653e\u5c04\u62a5\u544a\u6761\u4ef6\u5316\u76843D\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u76f4\u63a5\u4ece\u5b8c\u6574\u7684\u653e\u5c04\u62a5\u544a\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u76843D\u80f8\u90e8CT\u56fe\u50cf\uff0c\u5728MICCAI 2025 VLM3D\u6311\u6218\u8d5b\u4e2d\u53d6\u5f97\u7b2c\u4e00\u540d\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u57283D CT\u751f\u6210\u65b9\u9762\u5e94\u7528\u6709\u9650\uff0c\u4e14\u901a\u5e38\u4f7f\u7528\u7b80\u5316\u7684\u63d0\u793a\u8bcd\uff0c\u5ffd\u7565\u4e86\u653e\u5c04\u62a5\u544a\u4e2d\u4e30\u5bcc\u7684\u8bed\u4e49\u7ec6\u8282\uff0c\u5bfc\u81f4\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u5ea6\u548c\u4e34\u5e8a\u4fdd\u771f\u5ea6\u964d\u4f4e\u3002", "method": "\u4f7f\u7528\u4e09\u4e2a\u9884\u8bad\u7ec3\u7684\u533b\u5b66\u6587\u672c\u7f16\u7801\u5668\uff08BiomedVLP CXR BERT\u3001MedEmbed\u548cClinicalBERT\uff09\u6355\u83b7\u4e34\u5e8a\u4e0a\u4e0b\u6587\uff0c\u7ed3\u5408\u653e\u5c04\u62a5\u544a\u548c\u4f53\u7d20\u95f4\u8ddd\u4fe1\u606f\uff0c\u5728CT RATE\u6570\u636e\u96c6\u768420000\u4e2aCT\u4f53\u79ef\u4e0a\u8bad\u7ec33D\u6f5c\u5728\u6269\u6563\u6a21\u578b\u3002", "result": "\u751f\u6210\u7684CT\u4f53\u79ef\u5177\u6709\u89e3\u5256\u4e00\u81f4\u6027\u548c\u4f18\u5f02\u7684\u89c6\u89c9\u8d28\u91cf\uff0c\u591a\u7f16\u7801\u5668\u6761\u4ef6\u5316\u63d0\u9ad8\u4e86CLIP\u5206\u6570\uff0c\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u5bf9\u9f50\u5ea6\uff0c\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u5b8c\u6574\u7684\u653e\u5c04\u62a5\u544a\u548c\u591a\u7f16\u7801\u5668\u6587\u672c\u6761\u4ef6\u5316\uff0cReport2CT\u63a8\u8fdb\u4e863D CT\u5408\u6210\u6280\u672f\uff0c\u80fd\u591f\u751f\u6210\u4e34\u5e8a\u4fdd\u771f\u5ea6\u9ad8\u3001\u8d28\u91cf\u4f18\u5f02\u7684\u5408\u6210\u6570\u636e\u3002"}}
{"id": "2509.14841", "pdf": "https://arxiv.org/pdf/2509.14841", "abs": "https://arxiv.org/abs/2509.14841", "authors": ["Hongjun Wang", "Jiyuan Chen", "Zhengwei Yin", "Xuan Song", "Yinqiang Zheng"], "title": "Not All Degradations Are Equal: A Targeted Feature Denoising Framework for Generalizable Image Super-Resolution", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Generalizable Image Super-Resolution aims to enhance model generalization capabilities under unknown degradations. To achieve this goal, the models are expected to focus only on image content-related features instead of overfitting degradations. Recently, numerous approaches such as Dropout and Feature Alignment have been proposed to suppress models' natural tendency to overfit degradations and yield promising results. Nevertheless, these works have assumed that models overfit to all degradation types (e.g., blur, noise, JPEG), while through careful investigations in this paper, we discover that models predominantly overfit to noise, largely attributable to its distinct degradation pattern compared to other degradation types. In this paper, we propose a targeted feature denoising framework, comprising noise detection and denoising modules. Our approach presents a general solution that can be seamlessly integrated with existing super-resolution models without requiring architectural modifications. Our framework demonstrates superior performance compared to previous regularization-based methods across five traditional benchmarks and datasets, encompassing both synthetic and real-world scenarios.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6027\u7684\u7279\u5f81\u53bb\u566a\u6846\u67b6\uff0c\u4e13\u95e8\u5904\u7406\u8d85\u5206\u8fa8\u4e2d\u6a21\u578b\u8fc7\u62df\u5408\u566a\u58f0\u7684\u95ee\u9898\uff0c\u65e0\u9700\u6539\u52a8\u73b0\u6709\u6a21\u578b\u7ed3\u6784\u5373\u53ef\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5047\u8bbe\u6a21\u578b\u4f1a\u8fc7\u62df\u5408\u6240\u6709\u964d\u7ea7\u7c7b\u578b\uff0c\u4f46\u7ecf\u8be6\u7ec6\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u4e3b\u8981\u8fc7\u62df\u5408\u566a\u58f0\uff0c\u56e0\u4e3a\u566a\u58f0\u5177\u6709\u4e0e\u5176\u4ed6\u964d\u7ea7\u4e0d\u540c\u7684\u7279\u5f81\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u5305\u542b\u566a\u58f0\u68c0\u6d4b\u548c\u53bb\u566a\u6a21\u5757\u7684\u9488\u5bf9\u6027\u7279\u5f81\u53bb\u566a\u6846\u67b6\uff0c\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u8d85\u5206\u8fa8\u6a21\u578b\u4e2d\u800c\u65e0\u9700\u6539\u52a8\u7f51\u7edc\u7ed3\u6784\u3002", "result": "\u57285\u4e2a\u4f20\u7edf\u6d4b\u8bd5\u96c6\u548c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u8d85\u8fc7\u4e4b\u524d\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u5305\u62ec\u5408\u6210\u548c\u5b9e\u9645\u573a\u666f\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u9488\u5bf9\u6027\u5730\u5904\u7406\u566a\u58f0\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u4e3a\u901a\u7528\u6027\u56fe\u50cf\u8d85\u5206\u8fa8\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u793a\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2509.14866", "pdf": "https://arxiv.org/pdf/2509.14866", "abs": "https://arxiv.org/abs/2509.14866", "authors": ["Ali Salar", "Qing Liu", "Guoying Zhao"], "title": "Controllable Localized Face Anonymization Via Diffusion Inpainting", "categories": ["cs.CV"], "comment": null, "summary": "The growing use of portrait images in computer vision highlights the need to protect personal identities. At the same time, anonymized images must remain useful for downstream computer vision tasks. In this work, we propose a unified framework that leverages the inpainting ability of latent diffusion models to generate realistic anonymized images. Unlike prior approaches, we have complete control over the anonymization process by designing an adaptive attribute-guidance module that applies gradient correction during the reverse denoising process, aligning the facial attributes of the generated image with those of the synthesized target image. Our framework also supports localized anonymization, allowing users to specify which facial regions are left unchanged. Extensive experiments conducted on the public CelebA-HQ and FFHQ datasets show that our method outperforms state-of-the-art approaches while requiring no additional model training. The source code is available on our page.", "AI": {"tldr": "\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u7edf\u4e00\u533f\u540d\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5c5e\u6027\u5f15\u5bfc\u6a21\u5757\u63a7\u5236\u533f\u540d\u5316\u8fc7\u7a0b\uff0c\u652f\u6301\u5c40\u90e8\u533f\u540d\u5316\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u8096\u50cf\u56fe\u50cf\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u4fdd\u62a4\u4e2a\u4eba\u8eab\u4efd\u9690\u79c1\uff0c\u540c\u65f6\u786e\u4fdd\u533f\u540d\u5316\u540e\u7684\u56fe\u50cf\u4ecd\u53ef\u7528\u4e8e\u4e0b\u6e38\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u3002", "method": "\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u4fee\u590d\u80fd\u529b\uff0c\u8bbe\u8ba1\u81ea\u9002\u5e94\u5c5e\u6027\u5f15\u5bfc\u6a21\u5757\uff0c\u5728\u53cd\u5411\u53bb\u566a\u8fc7\u7a0b\u4e2d\u5e94\u7528\u68af\u5ea6\u6821\u6b63\uff0c\u4f7f\u751f\u6210\u56fe\u50cf\u7684\u9762\u90e8\u5c5e\u6027\u4e0e\u5408\u6210\u76ee\u6807\u56fe\u50cf\u5bf9\u9f50\uff0c\u652f\u6301\u5c40\u90e8\u533f\u540d\u5316\u3002", "result": "\u5728CelebA-HQ\u548cFFHQ\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u989d\u5916\u6a21\u578b\u8bad\u7ec3\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u80fd\u591f\u6709\u6548\u751f\u6210\u903c\u771f\u7684\u533f\u540d\u5316\u56fe\u50cf\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u5b9e\u7528\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.14890", "pdf": "https://arxiv.org/pdf/2509.14890", "abs": "https://arxiv.org/abs/2509.14890", "authors": ["Antoine Legrand", "Renaud Detry", "Christophe De Vleeschouwer"], "title": "NeRF-based Visualization of 3D Cues Supporting Data-Driven Spacecraft Pose Estimation", "categories": ["cs.CV"], "comment": "under review (8 pages, 2 figures)", "summary": "On-orbit operations require the estimation of the relative 6D pose, i.e., position and orientation, between a chaser spacecraft and its target. While data-driven spacecraft pose estimation methods have been developed, their adoption in real missions is hampered by the lack of understanding of their decision process. This paper presents a method to visualize the 3D visual cues on which a given pose estimator relies. For this purpose, we train a NeRF-based image generator using the gradients back-propagated through the pose estimation network. This enforces the generator to render the main 3D features exploited by the spacecraft pose estimation network. Experiments demonstrate that our method recovers the relevant 3D cues. Furthermore, they offer additional insights on the relationship between the pose estimation network supervision and its implicit representation of the target spacecraft.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u53ef\u89c6\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3NeRF\u57fa\u4e8e\u68af\u5ea6\u7684\u56fe\u50cf\u751f\u6210\u5668\uff0c\u6765\u5448\u73b0\u7a7a\u95f4\u8bbe\u5907\u4f4d\u7f6e\u4f30\u8ba1\u7f51\u7edc\u4f9d\u8d56\u76843D\u89c6\u89c9\u7ebf\u7d22\uff0c\u4ee5\u63d0\u9ad8\u5176\u51b3\u7b56\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5b9e\u9645\u5b9a\u4f4d\u4efb\u52a1\u9700\u89816D\u76f8\u5bf9\u4f4d\u7f6e\u4f30\u8ba1\uff0c\u4f46\u76ee\u524d\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u56e0\u51b3\u7b56\u8fc7\u7a0b\u4e0d\u53ef\u89e3\u91ca\u800c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u53ef\u89c6\u5316\u7a7a\u95f4\u8bbe\u5907\u4f4d\u7f6e\u4f30\u8ba1\u7f51\u7edc\u6240\u4f9d\u8d56\u76843D\u89c6\u89c9\u7279\u5f81\u3002", "method": "\u8bad\u7ec3\u57fa\u4e8eNeRF\u7684\u56fe\u50cf\u751f\u6210\u5668\uff0c\u901a\u8fc7\u4f4d\u7f6e\u4f30\u8ba1\u7f51\u7edc\u7684\u68af\u5ea6\u53cd\u5411\u4f20\u64ad\u6765\u5f3a\u5316\u751f\u6210\u5668\u6e32\u67d3\u51fa\u7a7a\u95f4\u8bbe\u5907\u4f4d\u7f6e\u4f30\u8ba1\u7f51\u7edc\u4e3b\u8981\u5229\u7528\u76843D\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u6062\u590d\u5173\u952e\u76843D\u89c6\u89c9\u7ebf\u7d22\uff0c\u5e76\u4e3a\u4f4d\u7f6e\u4f30\u8ba1\u7f51\u7edc\u7684\u76d1\u7763\u65b9\u5f0f\u4e0e\u9690\u5f0f\u8868\u5f81\u4e4b\u95f4\u7684\u5173\u7cfb\u63d0\u4f9b\u4e86\u66f4\u6df1\u5165\u7684\u89c1\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u53ef\u89c6\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u63ed\u793a\u7a7a\u95f4\u8bbe\u5907\u4f4d\u7f6e\u4f30\u8ba1\u7f51\u7edc\u7684\u51b3\u7b56\u57fa\u7840\uff0c\u4fc3\u8fdb\u4e86\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u5b9e\u9645\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2509.14981", "pdf": "https://arxiv.org/pdf/2509.14981", "abs": "https://arxiv.org/abs/2509.14981", "authors": ["Chuan Fang", "Heng Li", "Yixun Liang", "Jia Zheng", "Yongsen Mao", "Yuan Liu", "Rui Tang", "Zihan Zhou", "Ping Tan"], "title": "SPATIALGEN: Layout-guided 3D Indoor Scene Generation", "categories": ["cs.CV"], "comment": "3D scene ggeneration; diffusion model; Scene reconstruction and   understanding", "summary": "Creating high-fidelity 3D models of indoor environments is essential for applications in design, virtual reality, and robotics. However, manual 3D modeling remains time-consuming and labor-intensive. While recent advances in generative AI have enabled automated scene synthesis, existing methods often face challenges in balancing visual quality, diversity, semantic consistency, and user control. A major bottleneck is the lack of a large-scale, high-quality dataset tailored to this task. To address this gap, we introduce a comprehensive synthetic dataset, featuring 12,328 structured annotated scenes with 57,440 rooms, and 4.7M photorealistic 2D renderings. Leveraging this dataset, we present SpatialGen, a novel multi-view multi-modal diffusion model that generates realistic and semantically consistent 3D indoor scenes. Given a 3D layout and a reference image (derived from a text prompt), our model synthesizes appearance (color image), geometry (scene coordinate map), and semantic (semantic segmentation map) from arbitrary viewpoints, while preserving spatial consistency across modalities. SpatialGen consistently generates superior results to previous methods in our experiments. We are open-sourcing our data and models to empower the community and advance the field of indoor scene understanding and generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86SpatialGen\u591a\u89c6\u56fe\u591a\u6a21\u6001\u6269\u6563\u6a21\u578b\uff0c\u4f7f\u7528\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u8bed\u4e49\u4e00\u81f4\u76843D\u5ba4\u5185\u573a\u666f", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u3001\u591a\u6837\u6027\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u7528\u6237\u63a7\u5236\u65b9\u9762\u7684\u5e73\u8861\u95ee\u9898\uff0c\u586b\u8865\u7f3a\u4e4f\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u7a7a\u767d", "method": "\u57fa\u4e8e12,328\u4e2a\u7ed3\u6784\u5316\u6807\u6ce8\u573a\u666f\u7684\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u591a\u89c6\u56fe\u591a\u6a21\u6001\u6269\u6563\u6a21\u578b\uff0c\u4ece3D\u5e03\u5c40\u548c\u53c2\u8003\u56fe\u50cf\u751f\u6210\u5916\u89c2\u3001\u51e0\u4f55\u548c\u8bed\u4e49\u4fe1\u606f", "result": "\u5728\u5b9e\u9a8c\u4e2d\u59cb\u7ec8\u751f\u6210\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u7684\u7ed3\u679c\uff0c\u80fd\u591f\u4ece\u4efb\u610f\u89c6\u89d2\u5408\u6210\u4fdd\u6301\u8de8\u6a21\u6001\u7a7a\u95f4\u4e00\u81f4\u6027\u7684\u573a\u666f", "conclusion": "\u901a\u8fc7\u5f00\u6e90\u6570\u636e\u548c\u6a21\u578b\u63a8\u52a8\u5ba4\u5185\u573a\u666f\u7406\u89e3\u548c\u751f\u6210\u9886\u57df\u7684\u53d1\u5c55\uff0c\u4e3a\u8bbe\u8ba1\u3001\u865a\u62df\u73b0\u5b9e\u548c\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.15011", "pdf": "https://arxiv.org/pdf/2509.15011", "abs": "https://arxiv.org/abs/2509.15011", "authors": ["Vasiliki Ismiroglou", "Malte Pedersen", "Stefan H. Bengtson", "Andreas Aakerberg", "Thomas B. Moeslund"], "title": "Sea-ing Through Scattered Rays: Revisiting the Image Formation Model for Realistic Underwater Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In recent years, the underwater image formation model has found extensive use in the generation of synthetic underwater data. Although many approaches focus on scenes primarily affected by discoloration, they often overlook the model's ability to capture the complex, distance-dependent visibility loss present in highly turbid environments. In this work, we propose an improved synthetic data generation pipeline that includes the commonly omitted forward scattering term, while also considering a nonuniform medium. Additionally, we collected the BUCKET dataset under controlled turbidity conditions to acquire real turbid footage with the corresponding reference images. Our results demonstrate qualitative improvements over the reference model, particularly under increasing turbidity, with a selection rate of 82. 5\\% by survey participants. Data and code can be accessed on the project page: vap.aau.dk/sea-ing-through-scattered-rays.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6c34\u4e0b\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u5305\u542b\u524d\u5411\u6563\u5c04\u9879\u548c\u975e\u5747\u5300\u4ecb\u8d28\uff0c\u5e76\u6536\u96c6\u4e86BUCKET\u6570\u636e\u96c6\u9a8c\u8bc1\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6c34\u4e0b\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u53d8\u8272\u95ee\u9898\uff0c\u5ffd\u7565\u4e86\u9ad8\u6d4a\u5ea6\u73af\u5883\u4e2d\u8ddd\u79bb\u76f8\u5173\u7684\u80fd\u89c1\u5ea6\u635f\u5931\u590d\u6742\u6027\u3002", "method": "\u63d0\u51fa\u5305\u542b\u524d\u5411\u6563\u5c04\u9879\u548c\u975e\u5747\u5300\u4ecb\u8d28\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u5e76\u6536\u96c6\u53d7\u63a7\u6d4a\u5ea6\u6761\u4ef6\u4e0b\u7684\u771f\u5b9e\u6570\u636e\u96c6BUCKET\u3002", "result": "\u5728\u589e\u52a0\u6d4a\u5ea6\u6761\u4ef6\u4e0b\u663e\u793a\u51fa\u5b9a\u6027\u6539\u8fdb\uff0c\u8c03\u67e5\u53c2\u4e0e\u8005\u9009\u62e9\u7387\u8fbe\u523082.5%\u3002", "conclusion": "\u6539\u8fdb\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u6a21\u62df\u9ad8\u6d4a\u5ea6\u6c34\u4e0b\u73af\u5883\uff0c\u4e3a\u6c34\u4e0b\u89c6\u89c9\u7814\u7a76\u63d0\u4f9b\u66f4\u771f\u5b9e\u7684\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2509.15031", "pdf": "https://arxiv.org/pdf/2509.15031", "abs": "https://arxiv.org/abs/2509.15031", "authors": ["Chau Pham", "Quan Dao", "Mahesh Bhosale", "Yunjie Tian", "Dimitris Metaxas", "David Doermann"], "title": "AutoEdit: Automatic Hyperparameter Tuning for Image Editing", "categories": ["cs.CV"], "comment": "Accepted to NeurIPS 2025", "summary": "Recent advances in diffusion models have revolutionized text-guided image editing, yet existing editing methods face critical challenges in hyperparameter identification. To get the reasonable editing performance, these methods often require the user to brute-force tune multiple interdependent hyperparameters, such as inversion timesteps and attention modification, \\textit{etc.} This process incurs high computational costs due to the huge hyperparameter search space. We consider searching optimal editing's hyperparameters as a sequential decision-making task within the diffusion denoising process. Specifically, we propose a reinforcement learning framework, which establishes a Markov Decision Process that dynamically adjusts hyperparameters across denoising steps, integrating editing objectives into a reward function. The method achieves time efficiency through proximal policy optimization while maintaining optimal hyperparameter configurations. Experiments demonstrate significant reduction in search time and computational overhead compared to existing brute-force approaches, advancing the practical deployment of a diffusion-based image editing framework in the real world.", "AI": {"tldr": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u52a8\u6001\u8c03\u6574\u6563\u5e03\u6a21\u578b\u56fe\u50cf\u7f16\u8f91\u7684\u8d85\u53c2\u6570\uff0c\u907f\u514d\u66b4\u529b\u641c\u7d22\u5e76\u63d0\u9ad8\u6548\u7387", "motivation": "\u73b0\u6709\u6563\u5e03\u6a21\u578b\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u9700\u8981\u66b4\u529b\u8c03\u6574\u591a\u4e2a\u76f8\u4e92\u4f9d\u8d56\u7684\u8d85\u53c2\u6570\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5e94\u7528\u904d\u5386\u56f0\u96be", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5efa\u7acb\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u52a8\u6001\u8c03\u6574\u53bb\u566a\u6b65\u9aa4\u4e2d\u7684\u8d85\u53c2\u6570\uff0c\u901a\u8fc7\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u5b9e\u73b0\u65f6\u95f4\u6548\u7387", "result": "\u5b9e\u9a8c\u8868\u660e\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u641c\u7d22\u65f6\u95f4\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u8d85\u8d8a\u73b0\u6709\u66b4\u529b\u641c\u7d22\u65b9\u6cd5", "conclusion": "\u8be5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u63a8\u8fdb\u4e86\u6563\u5e03\u6a21\u578b\u56fe\u50cf\u7f16\u8f91\u5728\u5b9e\u9645\u4e2d\u7684\u90e8\u7f72\u5e94\u7528"}}
{"id": "2509.15123", "pdf": "https://arxiv.org/pdf/2509.15123", "abs": "https://arxiv.org/abs/2509.15123", "authors": ["Fang Li", "Hao Zhang", "Narendra Ahuja"], "title": "RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes", "categories": ["cs.CV"], "comment": "NeurIPS 2025", "summary": "Although COLMAP has long remained the predominant method for camera parameter optimization in static scenes, it is constrained by its lengthy runtime and reliance on ground truth (GT) motion masks for application to dynamic scenes. Many efforts attempted to improve it by incorporating more priors as supervision such as GT focal length, motion masks, 3D point clouds, camera poses, and metric depth, which, however, are typically unavailable in casually captured RGB videos. In this paper, we propose a novel method for more accurate and efficient camera parameter optimization in dynamic scenes solely supervised by a single RGB video. Our method consists of three key components: (1) Patch-wise Tracking Filters, to establish robust and maximally sparse hinge-like relations across the RGB video. (2) Outlier-aware Joint Optimization, for efficient camera parameter optimization by adaptive down-weighting of moving outliers, without reliance on motion priors. (3) A Two-stage Optimization Strategy, to enhance stability and optimization speed by a trade-off between the Softplus limits and convex minima in losses. We visually and numerically evaluate our camera estimates. To further validate accuracy, we feed the camera estimates into a 4D reconstruction method and assess the resulting 3D scenes, and rendered 2D RGB and depth maps. We perform experiments on 4 real-world datasets (NeRF-DS, DAVIS, iPhone, and TUM-dynamics) and 1 synthetic dataset (MPI-Sintel), demonstrating that our method estimates camera parameters more efficiently and accurately with a single RGB video as the only supervision.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u9700\u5355RGB\u89c6\u9891\u5373\u53ef\u5728\u52a8\u6001\u573a\u666f\u4e2d\u8fdb\u884c\u66f4\u51c6\u786e\u3001\u9ad8\u6548\u76f8\u673a\u53c2\u6570\u4f18\u5316\u7684\u65b0\u65b9\u6cd5\uff0c\u5305\u542b\u8865\u4e01\u8ddf\u8e2a\u6ee4\u6ce2\u5668\u3001\u5f02\u5e38\u503c\u611f\u77e5\u8054\u5408\u4f18\u5316\u548c\u4e24\u9636\u6bb5\u4f18\u5316\u7b56\u7565\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\u3002", "motivation": "COLMAP\u5728\u52a8\u6001\u573a\u666f\u4e2d\u9700\u8981\u5730\u9762\u771f\u5b9e\u8fd0\u52a8\u63a9\u7801\u4e14\u8fd0\u884c\u65f6\u95f4\u957f\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u989d\u5916\u76d1\u7763\u4fe1\u606f\u5982GT\u7126\u8ddd\u30013D\u70b9\u4e91\u7b49\uff0c\u800c\u8fd9\u4e9b\u5728\u666e\u901aRGB\u89c6\u9891\u4e2d\u901a\u5e38\u4e0d\u53ef\u5f97\u3002", "method": "1) \u8865\u4e01\u8ddf\u8e2a\u6ee4\u6ce2\u5668\u5efa\u7acb\u7a00\u758f\u94f0\u94fe\u5173\u7cfb\uff1b2) \u5f02\u5e38\u503c\u611f\u77e5\u8054\u5408\u4f18\u5316\u81ea\u9002\u5e94\u964d\u6743\u79fb\u52a8\u5f02\u5e38\u503c\uff1b3) \u4e24\u9636\u6bb5\u4f18\u5316\u7b56\u7565\u5e73\u8861Softplus\u9650\u5236\u548c\u51f8\u6700\u5c0f\u503c\u3002", "result": "\u57284\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u548c1\u4e2a\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4ec5\u7528\u5355RGB\u89c6\u9891\u5c31\u80fd\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u5730\u4f30\u8ba1\u76f8\u673a\u53c2\u6570\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4ec5\u4f7f\u7528\u5355RGB\u89c6\u9891\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u573a\u666f\u4e2d\u76f8\u673a\u53c2\u6570\u4f18\u5316\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u63d0\u5347\uff0c\u65e0\u9700\u4f9d\u8d56\u8fd0\u52a8\u5148\u9a8c\u6216\u5176\u4ed6\u989d\u5916\u76d1\u7763\u4fe1\u606f\u3002"}}
{"id": "2509.15159", "pdf": "https://arxiv.org/pdf/2509.15159", "abs": "https://arxiv.org/abs/2509.15159", "authors": ["Saket S. Chaturvedi", "Gaurav Bagwe", "Lan Zhang", "Xiaoyong Yuan"], "title": "AIP: Subverting Retrieval-Augmented Generation via Adversarial Instructional Prompt", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted at EMNLP 2025 Conference", "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by retrieving relevant documents from external sources to improve factual accuracy and verifiability. However, this reliance introduces new attack surfaces within the retrieval pipeline, beyond the LLM itself. While prior RAG attacks have exposed such vulnerabilities, they largely rely on manipulating user queries, which is often infeasible in practice due to fixed or protected user inputs. This narrow focus overlooks a more realistic and stealthy vector: instructional prompts, which are widely reused, publicly shared, and rarely audited. Their implicit trust makes them a compelling target for adversaries to manipulate RAG behavior covertly.   We introduce a novel attack for Adversarial Instructional Prompt (AIP) that exploits adversarial instructional prompts to manipulate RAG outputs by subtly altering retrieval behavior. By shifting the attack surface to the instructional prompts, AIP reveals how trusted yet seemingly benign interface components can be weaponized to degrade system integrity. The attack is crafted to achieve three goals: (1) naturalness, to evade user detection; (2) utility, to encourage use of prompts; and (3) robustness, to remain effective across diverse query variations. We propose a diverse query generation strategy that simulates realistic linguistic variation in user queries, enabling the discovery of prompts that generalize across paraphrases and rephrasings. Building on this, a genetic algorithm-based joint optimization is developed to evolve adversarial prompts by balancing attack success, clean-task utility, and stealthiness. Experimental results show that AIP achieves up to 95.23% ASR while preserving benign functionality. These findings uncover a critical and previously overlooked vulnerability in RAG systems, emphasizing the need to reassess the shared instructional prompts.", "AI": {"tldr": "\u901a\u8fc7\u653b\u51fb\u6307\u4ee4\u63d0\u793a\u6765\u64cd\u63a7RAG\u7cfb\u7edf\u7684\u68c0\u7d22\u884c\u4e3a\uff0c\u5b9e\u73b0\u9ad8\u654c\u653b\u6210\u529f\u7387\u4e14\u4fdd\u6301\u6b63\u5e38\u529f\u80fd\u7684\u9690\u85d5\u653b\u51fb", "motivation": "\u73b0\u6709RAG\u653b\u51fb\u4e3b\u8981\u4f9d\u8d56\u64cd\u63a7\u7528\u6237\u67e5\u8be2\uff0c\u800c\u6307\u4ee4\u63d0\u793a\u4f5c\u4e3a\u5e7f\u6cdb\u91cd\u7528\u3001\u516c\u5f00\u5171\u4eab\u4e14\u5c11\u88ab\u5ba1\u8ba1\u7684\u7ec4\u4ef6\uff0c\u6210\u4e3a\u66f4\u73b0\u5b9e\u7684\u653b\u51fb\u9762", "method": "\u4f7f\u7528\u591a\u6837\u5316\u67e5\u8be2\u751f\u6210\u7b56\u7565\u6a21\u62df\u771f\u5b9e\u8bed\u8a00\u53d8\u5316\uff0c\u57fa\u4e8e\u9057\u4f20\u7b97\u6cd5\u8fdb\u884c\u805a\u5408\u4f18\u5316\uff0c\u5e73\u8861\u653b\u51fb\u6210\u529f\u7387\u3001\u6e05\u6d01\u4efb\u52a1\u6548\u7528\u548c\u9690\u85d5\u6027", "result": "AIP\u653b\u51fb\u8fbe\u5230\u670095.23%\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6b63\u5e38\u7684\u529f\u80fd\u6027\u80fd", "conclusion": "\u53d1\u73b0\u4e86RAG\u7cfb\u7edf\u4e2d\u88ab\u5ffd\u89c6\u7684\u5173\u952e\u6f0f\u6d1e\uff0c\u5f3a\u8c03\u91cd\u65b0\u8bc4\u4f30\u5171\u4eab\u6307\u4ee4\u63d0\u793a\u7684\u5b89\u5168\u6027"}}
{"id": "2509.15177", "pdf": "https://arxiv.org/pdf/2509.15177", "abs": "https://arxiv.org/abs/2509.15177", "authors": ["Ali Nazari", "Bardiya Kariminia", "Mohsen Ebrahimi Moghaddam"], "title": "A Race Bias Free Face Aging Model for Reliable Kinship Verification", "categories": ["cs.CV"], "comment": null, "summary": "The age gap in kinship verification addresses the time difference between the photos of the parent and the child. Moreover, their same-age photos are often unavailable, and face aging models are racially biased, which impacts the likeness of photos. Therefore, we propose a face aging GAN model, RA-GAN, consisting of two new modules, RACEpSp and a feature mixer, to produce racially unbiased images. The unbiased synthesized photos are used in kinship verification to investigate the results of verifying same-age parent-child images. The experiments demonstrate that our RA-GAN outperforms SAM-GAN on an average of 13.14\\% across all age groups, and CUSP-GAN in the 60+ age group by 9.1\\% in terms of racial accuracy. Moreover, RA-GAN can preserve subjects' identities better than SAM-GAN and CUSP-GAN across all age groups. Additionally, we demonstrate that transforming parent and child images from the KinFaceW-I and KinFaceW-II datasets to the same age can enhance the verification accuracy across all age groups. The accuracy increases with our RA-GAN for the kinship relationships of father-son and father-daughter, mother-son, and mother-daughter, which are 5.22, 5.12, 1.63, and 0.41, respectively, on KinFaceW-I. Additionally, the accuracy for the relationships of father-daughter, father-son, and mother-son is 2.9, 0.39, and 1.6 on KinFaceW-II, respectively. The code is available at~\\href{https://github.com/bardiya2254kariminia/An-Age-Transformation-whitout-racial-bias-for-Kinship-verification}{Github}", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aRA-GAN\u7684\u65e0\u79cd\u65cf\u504f\u89c1\u9762\u90e8\u8001\u5316\u6a21\u578b\uff0c\u901a\u8fc7RACEpSp\u548c\u7279\u5f81\u6df7\u5408\u5668\u6a21\u5757\uff0c\u751f\u6210\u65e0\u79cd\u65cf\u504f\u89c1\u7684\u540c\u9f84\u5316\u56fe\u50cf\uff0c\u4ece\u800c\u63d0\u9ad8\u4eb2\u7c4d\u9a8c\u8bc1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4eb2\u7c4d\u9a8c\u8bc1\u4e2d\u5b58\u5728\u5e74\u9f84\u5dee\u5f02\u95ee\u9898\uff0c\u800c\u73b0\u6709\u9762\u90e8\u8001\u5316\u6a21\u578b\u5b58\u5728\u79cd\u65cf\u504f\u89c1\uff0c\u5f71\u54cd\u56fe\u50cf\u76f8\u4f3c\u5ea6\u8ba4\u5b9a\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1RA-GAN\u6a21\u578b\uff0c\u5305\u542bRACEpSp\u6a21\u5757\u548c\u7279\u5f81\u6df7\u5408\u5668\uff0c\u751f\u6210\u65e0\u79cd\u65cf\u504f\u89c1\u7684\u9762\u90e8\u8001\u5316\u56fe\u50cf\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eKinFaceW-I\u548cKinFaceW-II\u6570\u636e\u96c6\u7684\u4eb2\u5b50\u56fe\u50cf\u540c\u9f84\u5316\u5904\u7406\u3002", "result": "RA-GAN\u5728\u6240\u6709\u5e74\u9f84\u6bb5\u6bd4SAM-GAN\u5747\u5e7413.14%\uff0c\u572860+\u5e74\u9f84\u6bb5\u6bd4CUSP-GAN\u9ad89.1%\u7684\u79cd\u65cf\u51c6\u786e\u6027\uff0c\u5e76\u66f4\u597d\u4fdd\u7559\u4e3b\u4f53\u8eab\u4efd\u3002\u5728KinFaceW-I\u6570\u636e\u96c6\u4e0a\uff0c\u7236\u5b50\u3001\u7236\u5973\u3001\u6bcd\u5b50\u3001\u6bcd\u5973\u5173\u7cfb\u7684\u9a8c\u8bc1\u51c6\u786e\u6027\u5206\u522b\u63d0\u9ad85.22\u30015.12\u30011.63\u548c0.41\uff1b\u5728KinFaceW-II\u4e0a\uff0c\u7236\u5973\u3001\u7236\u5b50\u3001\u6bcd\u5b50\u5173\u7cfb\u7684\u51c6\u786e\u6027\u5206\u522b\u63d0\u9ad82.9\u30010.39\u548c1.6\u3002", "conclusion": "RA-GAN\u80fd\u591f\u6709\u6548\u51cf\u5c11\u9762\u90e8\u8001\u5316\u6a21\u578b\u7684\u79cd\u65cf\u504f\u89c1\uff0c\u751f\u6210\u66f4\u51c6\u786e\u7684\u540c\u9f84\u5316\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u5347\u4eb2\u7c4d\u9a8c\u8bc1\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u4e0d\u540c\u4eb2\u7c4d\u5173\u7cfb\u4e2d\u90fd\u53d6\u5f97\u4e86\u660e\u663e\u6539\u5584\u3002"}}
{"id": "2509.15178", "pdf": "https://arxiv.org/pdf/2509.15178", "abs": "https://arxiv.org/abs/2509.15178", "authors": ["Zaiquan Yang", "Yuhao Liu", "Gerhard Hancke", "Rynson W. H. Lau"], "title": "Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding", "categories": ["cs.CV"], "comment": null, "summary": "Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal tube of a video, as specified by the input text query. In this paper, we utilize multimodal large language models (MLLMs) to explore a zero-shot solution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to dynamically assign special tokens, referred to as \\textit{grounding tokens}, for grounding the text query; and (2) MLLMs often suffer from suboptimal grounding due to the inability to fully integrate the cues in the text query (\\textit{e.g.}, attributes, actions) for inference. Based on these insights, we propose a MLLM-based zero-shot framework for STVG, which includes novel decomposed spatio-temporal highlighting (DSTH) and temporal-augmented assembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH strategy first decouples the original query into attribute and action sub-queries for inquiring the existence of the target both spatially and temporally. It then uses a novel logit-guided re-attention (LRA) module to learn latent variables as spatial and temporal prompts, by regularizing token predictions for each sub-query. These prompts highlight attribute and action cues, respectively, directing the model's attention to reliable spatial and temporal related visual regions. In addition, as the spatial grounding by the attribute sub-query should be temporally consistent, we introduce the TAS strategy to assemble the predictions using the original video frames and the temporal-augmented frames as inputs to help improve temporal consistency. We evaluate our method on various MLLMs, and show that it outperforms SOTA methods on three common STVG benchmarks.   The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4e86\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(MLLMs)\u7684\u96f6\u68ad\u51fb\u65b9\u6848\u6765\u89e3\u51b3\u65f6\u7a7a\u89c6\u9891\u57fa\u51c6(STVG)\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u89e3\u5f0f\u65f6\u7a7a\u9ad8\u4eae\u7b56\u7565\u548c\u65f6\u95f4\u589e\u5f3a\u7ec4\u88c5\u7b56\u7565\u6765\u63d0\u5347MLLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728\u4e09\u4e2aSTVG\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u597d\u65b9\u6cd5\u3002", "motivation": "\u53d1\u73b0MLLMs\u5728STVG\u4efb\u52a1\u4e2d\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a(1)MLLMs\u5f80\u5f80\u52a8\u6001\u5206\u914d\u7279\u6b8a\u6807\u8bb0(\u57fa\u51c6\u6807\u8bb0)\u6765\u57fa\u51c6\u6587\u672c\u67e5\u8be2\uff1b(2)MLLMs\u56e0\u65e0\u6cd5\u5145\u5206\u6574\u5408\u6587\u672c\u67e5\u8be2\u4e2d\u7684\u7ebf\u7d22(\u5982\u5c5e\u6027\u3001\u52a8\u4f5c)\u800c\u5bfc\u81f4\u57fa\u51c6\u6548\u679c\u6b21\u4f18\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u5145\u5206\u53d1\u6325MLLMs\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8eMLLMs\u7684\u96f6\u68ad\u51fbSTVG\u6846\u67b6\uff0c\u5305\u62ec\uff1a\n1. \u5206\u89e3\u5f0f\u65f6\u7a7a\u9ad8\u4eae(DSTH)\u7b56\u7565\uff1a\u5c06\u539f\u59cb\u67e5\u8be2\u89e3\u8026\u4e3a\u5c5e\u6027\u548c\u52a8\u4f5c\u5b50\u67e5\u8be2\uff0c\u4f7f\u7528\u65b0\u7684\u5bf9\u6570\u5f15\u5bfc\u91cd\u5173\u6ce8(LRA)\u6a21\u5757\u5b66\u4e60\u6f5c\u5728\u53d8\u91cf\u4f5c\u4e3a\u7a7a\u95f4\u548c\u65f6\u95f4\u63d0\u793a\uff0c\u6309\u5b50\u67e5\u8be2\u6b63\u5219\u5316\u6807\u8bb0\u9884\u6d4b\n2. \u65f6\u95f4\u589e\u5f3a\u7ec4\u88c5(TAS)\u7b56\u7565\uff1a\u4f7f\u7528\u539f\u59cb\u89c6\u9891\u5e27\u548c\u65f6\u95f4\u589e\u5f3a\u5e27\u4f5c\u4e3a\u8f93\u5165\u6765\u7ec4\u88c5\u9884\u6d4b\uff0c\u63d0\u9ad8\u65f6\u95f4\u4e00\u81f4\u6027", "result": "\u5728\u591a\u4e2aMLLMs\u4e0a\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff0c\u5728\u4e09\u4e2a\u5e38\u89c1\u7684STVG\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u597d\u7684\u65b9\u6cd5", "conclusion": "\u901a\u8fc7DSTH\u548cTAS\u7b56\u7565\uff0c\u6210\u529f\u5730\u5145\u5206\u53d1\u6325\u4e86MLLMs\u5728STVG\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u96f6\u68ad\u51fb\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u793a\u4e86MLLMs\u5728\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b"}}
{"id": "2509.15185", "pdf": "https://arxiv.org/pdf/2509.15185", "abs": "https://arxiv.org/abs/2509.15185", "authors": ["Xiaoyu Yue", "Zidong Wang", "Yuqing Wang", "Wenlong Zhang", "Xihui Liu", "Wanli Ouyang", "Lei Bai", "Luping Zhou"], "title": "Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation", "categories": ["cs.CV"], "comment": "Accepted by NeurIPS 2025", "summary": "Recent studies have demonstrated the importance of high-quality visual representations in image generation and have highlighted the limitations of generative models in image understanding. As a generative paradigm originally designed for natural language, autoregressive models face similar challenges. In this work, we present the first systematic investigation into the mechanisms of applying the next-token prediction paradigm to the visual domain. We identify three key properties that hinder the learning of high-level visual semantics: local and conditional dependence, inter-step semantic inconsistency, and spatial invariance deficiency. We show that these issues can be effectively addressed by introducing self-supervised objectives during training, leading to a novel training framework, Self-guided Training for AutoRegressive models (ST-AR). Without relying on pre-trained representation models, ST-AR significantly enhances the image understanding ability of autoregressive models and leads to improved generation quality. Specifically, ST-AR brings approximately 42% FID improvement for LlamaGen-L and 49% FID improvement for LlamaGen-XL, while maintaining the same sampling strategy.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u5728\u89c6\u89c9\u9886\u57df\u7684\u5e94\u7528\u673a\u5236\uff0c\u53d1\u73b0\u4e86\u963b\u788d\u9ad8\u7ea7\u89c6\u89c9\u8bed\u4e49\u5b66\u4e60\u7684\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u81ea\u5f15\u5bfc\u8bad\u7ec3\u6846\u67b6ST-AR\u6765\u6709\u6548\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u7406\u89e3\u548c\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u8fd1\u671f\u7814\u7a76\u8868\u660e\u9ad8\u8d28\u91cf\u89c6\u89c9\u8868\u5f81\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u751f\u6210\u6a21\u578b\u5728\u56fe\u50cf\u7406\u89e3\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002\u4f5c\u4e3a\u539f\u672c\u4e3a\u81ea\u7136\u8bed\u8a00\u8bbe\u8ba1\u7684\u751f\u6210\u8303\u5f0f\uff0c\u81ea\u56de\u5f52\u6a21\u578b\u5728\u89c6\u89c9\u9886\u57df\u9762\u4e34\u7c7b\u4f3c\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u81ea\u5f15\u5bfc\u8bad\u7ec3\u6846\u67b6ST-AR\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f15\u5165\u81ea\u76d1\u7763\u76ee\u6807\u6765\u89e3\u51b3\u81ea\u56de\u5f52\u6a21\u578b\u5728\u89c6\u89c9\u9886\u57df\u9762\u4e34\u7684\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u5c40\u90e8\u6761\u4ef6\u4f9d\u8d56\u3001\u6b65\u95f4\u8bed\u4e49\u4e0d\u4e00\u81f4\u6027\u548c\u7a7a\u95f4\u4e0d\u53d8\u6027\u7f3a\u5931\u3002", "result": "ST-AR\u663e\u8457\u63d0\u5347\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u7684\u56fe\u50cf\u7406\u89e3\u80fd\u529b\u5e76\u6539\u5584\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u5728\u4e0d\u6539\u53d8\u91c7\u6837\u7b56\u7565\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3aLlamaGen-L\u5e26\u6765\u7ea642%\u7684FID\u63d0\u5347\uff0c\u4e3aLlamaGen-XL\u5e26\u676549%\u7684FID\u63d0\u5347\u3002", "conclusion": "\u81ea\u5f15\u5bfc\u8bad\u7ec3\u6846\u67b6ST-AR\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u5728\u89c6\u89c9\u9886\u57df\u7684\u6838\u5fc3\u6311\u6218\uff0c\u8bc1\u660e\u4e86\u5728\u4e0d\u4f9d\u8d56\u9884\u8bad\u7ec3\u8868\u5f81\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u8bad\u7ec3\u7b56\u7565\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2509.15212", "pdf": "https://arxiv.org/pdf/2509.15212", "abs": "https://arxiv.org/abs/2509.15212", "authors": ["Yuming Jiang", "Siteng Huang", "Shengke Xue", "Yaxi Zhao", "Jun Cen", "Sicong Leng", "Kehan Li", "Jiayan Guo", "Kexiang Wang", "Mingxiu Chen", "Fan Wang", "Deli Zhao", "Xin Li"], "title": "RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation", "categories": ["cs.CV", "cs.RO"], "comment": "GitHub Project: https://github.com/alibaba-damo-academy/RynnVLA-001", "summary": "This paper presents RynnVLA-001, a vision-language-action(VLA) model built upon large-scale video generative pretraining from human demonstrations. We propose a novel two-stage pretraining methodology. The first stage, Ego-Centric Video Generative Pretraining, trains an Image-to-Video model on 12M ego-centric manipulation videos to predict future frames conditioned on an initial frame and a language instruction. The second stage, Human-Centric Trajectory-Aware Modeling, extends this by jointly predicting future keypoint trajectories, thereby effectively bridging visual frame prediction with action prediction. Furthermore, to enhance action representation, we propose ActionVAE, a variational autoencoder that compresses sequences of actions into compact latent embeddings, reducing the complexity of the VLA output space. When finetuned on the same downstream robotics datasets, RynnVLA-001 achieves superior performance over state-of-the-art baselines, demonstrating that the proposed pretraining strategy provides a more effective initialization for VLA models.", "AI": {"tldr": "RynnVLA-001\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u9884\u8bad\u7ec3\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u751f\u6210\u9884\u8bad\u7ec3\u9884\u6d4b\u672a\u6765\u5e27\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u4eba\u7c7b\u4e2d\u5fc3\u8f68\u8ff9\u611f\u77e5\u5efa\u6a21\u8054\u5408\u9884\u6d4b\u5173\u952e\u70b9\u8f68\u8ff9\uff0c\u5e76\u4f7f\u7528ActionVAE\u538b\u7f29\u52a8\u4f5c\u5e8f\u5217\uff0c\u5728\u4e0b\u6e38\u673a\u5668\u4eba\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4e2d\u52a8\u4f5c\u9884\u6d4b\u7684\u590d\u6742\u6027\uff0c\u5e76\u5efa\u7acb\u89c6\u89c9\u5e27\u9884\u6d4b\u4e0e\u52a8\u4f5c\u9884\u6d4b\u4e4b\u95f4\u7684\u6709\u6548\u6865\u6881\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u6765\u63d0\u5347\u6a21\u578b\u521d\u59cb\u5316\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u9884\u8bad\u7ec3\u65b9\u6cd5\uff1a1\uff09\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u751f\u6210\u9884\u8bad\u7ec3\uff1a\u57281200\u4e07\u81ea\u6211\u4e2d\u5fc3\u64cd\u4f5c\u89c6\u9891\u4e0a\u8bad\u7ec3\u56fe\u50cf\u5230\u89c6\u9891\u6a21\u578b\uff0c\u57fa\u4e8e\u521d\u59cb\u5e27\u548c\u8bed\u8a00\u6307\u4ee4\u9884\u6d4b\u672a\u6765\u5e27\uff1b2\uff09\u4eba\u7c7b\u4e2d\u5fc3\u8f68\u8ff9\u611f\u77e5\u5efa\u6a21\uff1a\u8054\u5408\u9884\u6d4b\u672a\u6765\u5173\u952e\u70b9\u8f68\u8ff9\uff1b\u540c\u65f6\u63d0\u51faActionVAE\u53d8\u5206\u81ea\u7f16\u7801\u5668\u538b\u7f29\u52a8\u4f5c\u5e8f\u5217\u4e3a\u7d27\u51d1\u6f5c\u5728\u5d4c\u5165\u3002", "result": "\u5728\u76f8\u540c\u4e0b\u6e38\u673a\u5668\u4eba\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u540e\uff0cRynnVLA-001\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u9884\u8bad\u7ec3\u7b56\u7565\u4e3aVLA\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u521d\u59cb\u5316\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u9884\u8bad\u7ec3\u65b9\u6cd5\u548cActionVAE\u52a8\u4f5c\u8868\u793a\u589e\u5f3a\u6280\u672f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3aVLA\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u521d\u59cb\u5316\u65b9\u6848\u3002"}}
{"id": "2509.15219", "pdf": "https://arxiv.org/pdf/2509.15219", "abs": "https://arxiv.org/abs/2509.15219", "authors": ["Haichao Zhang", "Yi Xu", "Yun Fu"], "title": "Out-of-Sight Trajectories: Tracking, Fusion, and Prediction", "categories": ["cs.CV", "cs.LG", "cs.MA", "cs.MM", "cs.RO", "68T45, 68U10, 68T07, 68T40, 93C85, 93E11, 62M20, 62M10, 68U05, 94A12", "F.2.2; I.2.9; I.2.10; I.4.1; I.4.8; I.4.9; I.5.4; I.3.7"], "comment": null, "summary": "Trajectory prediction is a critical task in computer vision and autonomous systems, playing a key role in autonomous driving, robotics, surveillance, and virtual reality. Existing methods often rely on complete and noise-free observational data, overlooking the challenges associated with out-of-sight objects and the inherent noise in sensor data caused by limited camera coverage, obstructions, and the absence of ground truth for denoised trajectories. These limitations pose safety risks and hinder reliable prediction in real-world scenarios. In this extended work, we present advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the noise-free visual trajectories of out-of-sight objects using noisy sensor data. Building on our previous research, we broaden the scope of Out-of-Sight Trajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending its applicability to autonomous driving, robotics, surveillance, and virtual reality. Our enhanced Vision-Positioning Denoising Module leverages camera calibration to establish a vision-positioning mapping, addressing the lack of visual references, while effectively denoising noisy sensor data in an unsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB datasets, our approach achieves state-of-the-art performance in both trajectory denoising and prediction, significantly surpassing previous baselines. Additionally, we introduce comparisons with traditional denoising methods, such as Kalman filtering, and adapt recent trajectory prediction models to our task, providing a comprehensive benchmark. This work represents the first initiative to integrate vision-positioning projection for denoising noisy sensor trajectories of out-of-sight agents, paving the way for future advances. The code and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST", "AI": {"tldr": "\u63d0\u51fa\u4e86OST\u4efb\u52a1\uff0c\u901a\u8fc7\u89c6\u89c9-\u5b9a\u4f4d\u6620\u5c04\u548c\u53bb\u566a\u6a21\u5757\uff0c\u5728\u65e0\u76d1\u7763\u65b9\u5f0f\u4e0b\u9884\u6d4b\u89c6\u7ebf\u5916\u7269\u4f53\u7684\u65e0\u566a\u58f0\u8f68\u8ff9\uff0c\u5728Vi-Fi\u548cJRDB\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5b8c\u6574\u65e0\u566a\u58f0\u89c2\u6d4b\u6570\u636e\uff0c\u5ffd\u7565\u4e86\u89c6\u7ebf\u5916\u7269\u4f53\u548c\u4f20\u611f\u5668\u566a\u58f0\u7684\u6311\u6218\uff0c\u8fd9\u4e9b\u9650\u5236\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5e26\u6765\u5b89\u5168\u98ce\u9669\u5e76\u963b\u788d\u53ef\u9760\u9884\u6d4b", "method": "\u589e\u5f3a\u7684\u89c6\u89c9-\u5b9a\u4f4d\u53bb\u566a\u6a21\u5757\u5229\u7528\u76f8\u673a\u6807\u5b9a\u5efa\u7acb\u89c6\u89c9-\u5b9a\u4f4d\u6620\u5c04\uff0c\u4ee5\u65e0\u76d1\u7763\u65b9\u5f0f\u6709\u6548\u53bb\u566a\u4f20\u611f\u5668\u6570\u636e\uff0c\u6269\u5c55\u5e94\u7528\u5230\u884c\u4eba\u548c\u8f66\u8f86", "result": "\u5728Vi-Fi\u548cJRDB\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u8f68\u8ff9\u53bb\u566a\u548c\u9884\u6d4b\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u8d85\u8d8a\u5148\u524d\u57fa\u7ebf\uff0c\u5e76\u4e0e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7b49\u4f20\u7edf\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u6574\u5408\u89c6\u89c9-\u5b9a\u4f4d\u6295\u5f71\u6765\u53bb\u566a\u89c6\u7ebf\u5916\u667a\u80fd\u4f53\u566a\u58f0\u4f20\u611f\u5668\u8f68\u8ff9\u7684\u5de5\u4f5c\uff0c\u4e3a\u672a\u6765\u8fdb\u5c55\u94fa\u5e73\u4e86\u9053\u8def"}}
{"id": "2509.15220", "pdf": "https://arxiv.org/pdf/2509.15220", "abs": "https://arxiv.org/abs/2509.15220", "authors": ["Fangjinhua Wang", "Qingshan Xu", "Yew-Soon Ong", "Marc Pollefeys"], "title": "Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model", "categories": ["cs.CV"], "comment": "Accepted to IEEE T-PAMI 2025. Code: https://github.com/cvg/diffmvs", "summary": "To reconstruct the 3D geometry from calibrated images, learning-based multi-view stereo (MVS) methods typically perform multi-view depth estimation and then fuse depth maps into a mesh or point cloud. To improve the computational efficiency, many methods initialize a coarse depth map and then gradually refine it in higher resolutions. Recently, diffusion models achieve great success in generation tasks. Starting from a random noise, diffusion models gradually recover the sample with an iterative denoising process. In this paper, we propose a novel MVS framework, which introduces diffusion models in MVS. Specifically, we formulate depth refinement as a conditional diffusion process. Considering the discriminative characteristic of depth estimation, we design a condition encoder to guide the diffusion process. To improve efficiency, we propose a novel diffusion network combining lightweight 2D U-Net and convolutional GRU. Moreover, we propose a novel confidence-based sampling strategy to adaptively sample depth hypotheses based on the confidence estimated by diffusion model. Based on our novel MVS framework, we propose two novel MVS methods, DiffMVS and CasDiffMVS. DiffMVS achieves competitive performance with state-of-the-art efficiency in run-time and GPU memory. CasDiffMVS achieves state-of-the-art performance on DTU, Tanks & Temples and ETH3D. Code is available at: https://github.com/cvg/diffmvs.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u591a\u89c6\u56fe\u7acb\u4f53\u89c6\u89c9\u6846\u67b6\uff0c\u5c06\u6df1\u5ea6\u56fe\u7ec6\u5316\u5efa\u6a21\u4e3a\u6761\u4ef6\u6269\u6563\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u76843D\u91cd\u5efa", "motivation": "\u4f20\u7edf\u591a\u89c6\u56fe\u7acb\u4f53\u89c6\u89c9\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u4f4e\uff0c\u800c\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e0c\u671b\u5c06\u6269\u6563\u6a21\u578b\u7684\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u5e94\u7528\u4e8e\u6df1\u5ea6\u56fe\u7ec6\u5316", "method": "\u8bbe\u8ba1\u6761\u4ef6\u7f16\u7801\u5668\u6307\u5bfc\u6269\u6563\u8fc7\u7a0b\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea72D U-Net\u548c\u5377\u79efGRU\u7684\u6269\u6563\u7f51\u7edc\uff0c\u63d0\u51fa\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u81ea\u9002\u5e94\u91c7\u6837\u7b56\u7565\uff0c\u5f00\u53d1\u4e86DiffMVS\u548cCasDiffMVS\u4e24\u79cd\u65b9\u6cd5", "result": "DiffMVS\u5728\u8fd0\u884c\u65f6\u95f4\u548cGPU\u5185\u5b58\u4f7f\u7528\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6548\u7387\uff0cCasDiffMVS\u5728DTU\u3001Tanks & Temples\u548cETH3D\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd", "conclusion": "\u5c06\u6269\u6563\u6a21\u578b\u5f15\u5165\u591a\u89c6\u56fe\u7acb\u4f53\u89c6\u89c9\u662f\u6709\u6548\u7684\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u5728\u6548\u7387\u548c\u6027\u80fd\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff0c\u4e3a3D\u91cd\u5efa\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.15225", "pdf": "https://arxiv.org/pdf/2509.15225", "abs": "https://arxiv.org/abs/2509.15225", "authors": ["Silvio Mazzucco", "Carl Persson", "Mattia Segu", "Pier Luigi Dovesi", "Federico Tombari", "Luc Van Gool", "Matteo Poggi"], "title": "Lost in Translation? Vocabulary Alignment for Source-Free Domain Adaptation in Open-Vocabulary Semantic Segmentation", "categories": ["cs.CV"], "comment": "BMVC 2025 - Project Page: https://thegoodailab.org/blog/vocalign -   Code: https://github.com/Sisso16/VocAlign", "summary": "We introduce VocAlign, a novel source-free domain adaptation framework specifically designed for VLMs in open-vocabulary semantic segmentation. Our method adopts a student-teacher paradigm enhanced with a vocabulary alignment strategy, which improves pseudo-label generation by incorporating additional class concepts. To ensure efficiency, we use Low-Rank Adaptation (LoRA) to fine-tune the model, preserving its original capabilities while minimizing computational overhead. In addition, we propose a Top-K class selection mechanism for the student model, which significantly reduces memory requirements while further improving adaptation performance. Our approach achieves a notable 6.11 mIoU improvement on the CityScapes dataset and demonstrates superior performance on zero-shot segmentation benchmarks, setting a new standard for source-free adaptation in the open-vocabulary setting.", "AI": {"tldr": "VocAlign\u662f\u4e00\u4e2a\u9488\u5bf9\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u7684\u6e90\u81ea\u7531\u9886\u57df\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u91c7\u7528\u5e08\u751f\u8303\u5f0f\u7ed3\u5408\u8bcd\u6c47\u5bf9\u9f50\u7b56\u7565\uff0c\u901a\u8fc7LoRA\u5fae\u8c03\u548cTop-K\u7c7b\u522b\u9009\u62e9\u673a\u5236\uff0c\u5728CityScapes\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e866.11 mIoU\u7684\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u6e90\u81ea\u7531\u9886\u57df\u81ea\u9002\u5e94\u95ee\u9898\uff0c\u9700\u8981\u5728\u4e0d\u8bbf\u95ee\u6e90\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6a21\u578b\u5728\u65b0\u9886\u57df\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u91c7\u7528\u5e08\u751f\u8303\u5f0f\uff0c\u7ed3\u5408\u8bcd\u6c47\u5bf9\u9f50\u7b56\u7565\u6539\u8fdb\u4f2a\u6807\u7b7e\u751f\u6210\uff1b\u4f7f\u7528LoRA\u8fdb\u884c\u9ad8\u6548\u5fae\u8c03\u4ee5\u4fdd\u6301\u539f\u59cb\u80fd\u529b\uff1b\u63d0\u51faTop-K\u7c7b\u522b\u9009\u62e9\u673a\u5236\u964d\u4f4e\u5185\u5b58\u9700\u6c42\u3002", "result": "\u5728CityScapes\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e866.11 mIoU\u7684\u663e\u8457\u6539\u8fdb\uff0c\u5728\u96f6\u6837\u672c\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "VocAlign\u4e3a\u5f00\u653e\u8bcd\u6c47\u8bbe\u7f6e\u4e2d\u7684\u6e90\u81ea\u7531\u81ea\u9002\u5e94\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2509.15076", "pdf": "https://arxiv.org/pdf/2509.15076", "abs": "https://arxiv.org/abs/2509.15076", "authors": ["Mohammad Saleh Vahdatpour", "Maryam Eyvazi", "Yanqing Zhang"], "title": "Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models", "categories": ["cs.LG", "cs.CV"], "comment": "Published at ICCVW 2025", "summary": "Air pollution remains a critical threat to public health and environmental sustainability, yet conventional monitoring systems are often constrained by limited spatial coverage and accessibility. This paper proposes an AI-driven agent that predicts ambient air pollution levels from sky images and synthesizes realistic visualizations of pollution scenarios using generative modeling. Our approach combines statistical texture analysis with supervised learning for pollution classification, and leverages vision-language model (VLM)-guided image generation to produce interpretable representations of air quality conditions. The generated visuals simulate varying degrees of pollution, offering a foundation for user-facing interfaces that improve transparency and support informed environmental decision-making. These outputs can be seamlessly integrated into intelligent applications aimed at enhancing situational awareness and encouraging behavioral responses based on real-time forecasts. We validate our method using a dataset of urban sky images and demonstrate its effectiveness in both pollution level estimation and semantically consistent visual synthesis. The system design further incorporates human-centered user experience principles to ensure accessibility, clarity, and public engagement in air quality forecasting. To support scalable and energy-efficient deployment, future iterations will incorporate a green CNN architecture enhanced with FPGA-based incremental learning, enabling real-time inference on edge platforms.", "AI": {"tldr": "\u57fa\u4e8e\u5929\u7a7a\u56fe\u50cf\u7684AI\u7a7a\u6c14\u8d28\u91cf\u9884\u6d4b\u4e0e\u53ef\u89c6\u5316\u7cfb\u7edf\uff0c\u7ed3\u5408\u7edf\u8ba1\u7eb9\u7406\u5206\u6790\u548c\u751f\u6210\u6a21\u578b\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u7a7a\u6c14\u6c61\u67d3\u53ef\u89c6\u5316", "motivation": "\u4f20\u7edf\u7a7a\u6c14\u8d28\u91cf\u76d1\u6d4b\u7cfb\u7edf\u7a7a\u95f4\u8986\u76d6\u6709\u9650\u4e14\u53ef\u8bbf\u95ee\u6027\u5dee\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u76f4\u89c2\u3001\u900f\u660e\u7684\u7a7a\u6c14\u8d28\u91cf\u53ef\u89c6\u5316\u65b9\u6cd5", "method": "\u7ed3\u5408\u7edf\u8ba1\u7eb9\u7406\u5206\u6790\u548c\u76d1\u7763\u5b66\u4e60\u8fdb\u884c\u6c61\u67d3\u5206\u7c7b\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5f15\u5bfc\u7684\u56fe\u50cf\u751f\u6210\u6280\u672f\u5408\u6210\u6c61\u67d3\u573a\u666f\u53ef\u89c6\u5316", "result": "\u5728\u57ce\u5e02\u573a\u666f\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6c61\u67d3\u6c34\u5e73\u4f30\u8ba1\u548c\u8bed\u4e49\u4e00\u81f4\u89c6\u89c9\u5408\u6210\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u9762\u5411\u7528\u6237\u7684\u667a\u80fd\u5e94\u7528\u63d0\u4f9b\u57fa\u7840\uff0c\u672a\u6765\u5c06\u96c6\u6210\u7eff\u8272CNN\u67b6\u6784\u548cFPGA\u589e\u91cf\u5b66\u4e60\u4ee5\u5b9e\u73b0\u8fb9\u7f18\u5e73\u53f0\u5b9e\u65f6\u63a8\u7406"}}
{"id": "2509.15124", "pdf": "https://arxiv.org/pdf/2509.15124", "abs": "https://arxiv.org/abs/2509.15124", "authors": ["Sanduni Pinnawala", "Annabelle Hartanto", "Ivor J. A. Simpson", "Peter A. Wijeratne"], "title": "Learning Mechanistic Subtypes of Neurodegeneration with a Physics-Informed Variational Autoencoder Mixture Model", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "13 pages, 5 figures, accepted at SASHIMI workshop, MICCAI 2025", "summary": "Modelling the underlying mechanisms of neurodegenerative diseases demands methods that capture heterogeneous and spatially varying dynamics from sparse, high-dimensional neuroimaging data. Integrating partial differential equation (PDE) based physics knowledge with machine learning provides enhanced interpretability and utility over classic numerical methods. However, current physics-integrated machine learning methods are limited to considering a single PDE, severely limiting their application to diseases where multiple mechanisms are responsible for different groups (i.e., subtypes) and aggravating problems with model misspecification and degeneracy. Here, we present a deep generative model for learning mixtures of latent dynamic models governed by physics-based PDEs, going beyond traditional approaches that assume a single PDE structure. Our method integrates reaction-diffusion PDEs within a variational autoencoder (VAE) mixture model framework, supporting inference of subtypes of interpretable latent variables (e.g. diffusivity and reaction rates) from neuroimaging data. We evaluate our method on synthetic benchmarks and demonstrate its potential for uncovering mechanistic subtypes of Alzheimer's disease progression from positron emission tomography (PET) data.", "AI": {"tldr": "\u4e00\u79cd\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u504f\u5fae\u65b9\u7a0b\u7269\u7406\u77e5\u8bc6\u548c\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\u6df7\u5408\u6a21\u578b\uff0c\u4ece\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u4e2d\u63a8\u65ad\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u7684\u673a\u5236\u5b50\u578b", "motivation": "\u73b0\u6709\u7684\u7269\u7406\u96c6\u6210\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u53ea\u8003\u8651\u5355\u4e00\u504f\u5fae\u65b9\u7a0b\uff0c\u65e0\u6cd5\u5904\u7406\u591a\u673a\u5236\u5bfc\u81f4\u7684\u5f02\u8d28\u6027\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5", "method": "\u5c06\u53cd\u5e94-\u6269\u6563\u504f\u5fae\u65b9\u7a0b\u96c6\u6210\u5230\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\u6df7\u5408\u6a21\u578b\u6846\u67b6\u4e2d\uff0c\u652f\u6301\u4ece\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u63a8\u65ad\u89e3\u91ca\u6027\u9690\u53d8\u91cf", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u65b9\u6cd5\uff0c\u5e76\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5PET\u6570\u636e\u4e2d\u5c55\u793a\u4e86\u53d1\u73b0\u673a\u5236\u5b50\u578b\u7684\u6f5c\u529b", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u7a00\u758f\u9ad8\u7ef4\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u4e2d\u63a8\u65ad\u591a\u91cd\u7269\u7406\u673a\u5236\u5b50\u578b\uff0c\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89e3\u91ca\u6027\u548c\u5e94\u7528\u4ef7\u503c"}}
