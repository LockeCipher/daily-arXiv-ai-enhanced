<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 22]
- [cs.HC](#cs.HC) [Total: 1]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Wavelet-GS: 3D Gaussian Splatting with Wavelet Decomposition](https://arxiv.org/abs/2507.12498)
*Beizhen Zhao,Yifan Zhou,Sicheng Yu,Zijian Wang,Hao Wang*

Main category: cs.GR

TL;DR: 本文提出了一种结合小波分解的3D高斯泼溅优化框架，通过将点云分解为高低频组件并分别优化，解决了复杂场景重建中结构不完整和光照效果不清晰的问题，在多个数据集上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯泼溅方法在复杂场景重建中面临重大挑战，主要表现为整体结构轮廓不完整和局部光照效果不清晰，需要一种新的方法来同时解决这些问题。

Method: 提出了一种解耦优化框架，将小波分解集成到3D高斯泼溅和2D采样中。通过3D小波分解将点云分为高频和低频组件：低频组件捕获全局结构轮廓并通过体素化管理高斯分布；高频组件恢复精细几何和纹理细节，并结合重光照模块减少光照伪影。同时对训练图像应用2D小波分解模拟辐射变化，为高频细节重建提供指导。

Result: 在具有挑战性的数据集上进行的大量实验表明，该方法在各种指标上都达到了最先进的性能，超越了现有方法。

Conclusion: 该方法成功推进了3D场景重建领域的发展，通过解耦的小波分解优化框架有效解决了复杂场景重建中的关键问题，实现了更完整的结构重建和更清晰的光照效果。

Abstract: 3D Gaussian Splatting (3DGS) has revolutionized 3D scene reconstruction, which effectively balances rendering quality, efficiency, and speed. However, existing 3DGS approaches usually generate plausible outputs and face significant challenges in complex scene reconstruction, manifesting as incomplete holistic structural outlines and unclear local lighting effects. To address these issues simultaneously, we propose a novel decoupled optimization framework, which integrates wavelet decomposition into 3D Gaussian Splatting and 2D sampling. Technically, through 3D wavelet decomposition, our approach divides point clouds into high-frequency and low-frequency components, enabling targeted optimization for each. The low-frequency component captures global structural outlines and manages the distribution of Gaussians through voxelization. In contrast, the high-frequency component restores intricate geometric and textural details while incorporating a relight module to mitigate lighting artifacts and enhance photorealistic rendering. Additionally, a 2D wavelet decomposition is applied to the training images, simulating radiance variations. This provides critical guidance for high-frequency detail reconstruction, ensuring seamless integration of details with the global structure. Extensive experiments on challenging datasets demonstrate our method achieves state-of-the-art performance across various metrics, surpassing existing approaches and advancing the field of 3D scene reconstruction.

</details>


### [2] [VolSegGS: Segmentation and Tracking in Dynamic Volumetric Scenes via Deformable 3D Gaussians](https://arxiv.org/abs/2507.12667)
*Siyuan Yao,Chaoli Wang*

Main category: cs.GR

TL;DR: VolSegGS是一种基于高斯泼溅的框架，支持动态体积场景的交互式分割和跟踪，适用于低计算需求的实时可视化探索。


<details>
  <summary>Details</summary>
Motivation: 大规模时间依赖模拟数据的可视化需要大量资源，现有方法注重重建质量而非交互探索，因此需要一种支持实时交互分割和跟踪的解决方案。

Method: 利用可变形3D高斯表示动态场景，结合视图无关颜色和亲和力场网络实现精确分割，并通过高斯嵌入实现连续跟踪。

Result: VolSegGS在多个时变数据集上表现优异，支持实时交互和灵活的分割与跟踪功能。

Conclusion: VolSegGS为时变体积数据的分析和可视化提供了高效、低计算需求的解决方案，具有广泛应用潜力。

Abstract: Visualization of large-scale time-dependent simulation data is crucial for domain scientists to analyze complex phenomena, but it demands significant I/O bandwidth, storage, and computational resources. To enable effective visualization on local, low-end machines, recent advances in view synthesis techniques, such as neural radiance fields, utilize neural networks to generate novel visualizations for volumetric scenes. However, these methods focus on reconstruction quality rather than facilitating interactive visualization exploration, such as feature extraction and tracking. We introduce VolSegGS, a novel Gaussian splatting framework that supports interactive segmentation and tracking in dynamic volumetric scenes for exploratory visualization and analysis. Our approach utilizes deformable 3D Gaussians to represent a dynamic volumetric scene, allowing for real-time novel view synthesis. For accurate segmentation, we leverage the view-independent colors of Gaussians for coarse-level segmentation and refine the results with an affinity field network for fine-level segmentation. Additionally, by embedding segmentation results within the Gaussians, we ensure that their deformation enables continuous tracking of segmented regions over time. We demonstrate the effectiveness of VolSegGS with several time-varying datasets and compare our solutions against state-of-the-art methods. With the ability to interact with a dynamic scene in real time and provide flexible segmentation and tracking capabilities, VolSegGS offers a powerful solution under low computational demands. This framework unlocks exciting new possibilities for time-varying volumetric data analysis and visualization.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [3] [Reconstruct, Inpaint, Finetune: Dynamic Novel-view Synthesis from Monocular Videos](https://arxiv.org/abs/2507.12646)
*Kaihua Chen,Tarasha Khurana,Deva Ramanan*

Main category: cs.CV

TL;DR: 提出了一种基于3D重建和2D视频扩散模型的新视角合成方法CogNVS，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决动态场景新视角合成中现有方法依赖昂贵优化或无法保持几何一致性的问题。

Method: 结合3D场景重建和自监督2D视频扩散模型（CogNVS），实现隐藏像素的修复。

Result: CogNVS在动态场景新视角合成中表现优于大多数现有方法。

Conclusion: CogNVS通过自监督和零-shot应用，为动态场景新视角合成提供了高效解决方案。

Abstract: We explore novel-view synthesis for dynamic scenes from monocular videos. Prior approaches rely on costly test-time optimization of 4D representations or do not preserve scene geometry when trained in a feed-forward manner. Our approach is based on three key insights: (1) covisible pixels (that are visible in both the input and target views) can be rendered by first reconstructing the dynamic 3D scene and rendering the reconstruction from the novel-views and (2) hidden pixels in novel views can be "inpainted" with feed-forward 2D video diffusion models. Notably, our video inpainting diffusion model (CogNVS) can be self-supervised from 2D videos, allowing us to train it on a large corpus of in-the-wild videos. This in turn allows for (3) CogNVS to be applied zero-shot to novel test videos via test-time finetuning. We empirically verify that CogNVS outperforms almost all prior art for novel-view synthesis of dynamic scenes from monocular videos.

</details>


### [4] [Integrated Oculomics and Lipidomics Reveal Microvascular Metabolic Signatures Associated with Cardiovascular Health in a Healthy Cohort](https://arxiv.org/abs/2507.12663)
*Inamullah,Ernesto Elias Vidal Rosas,Imran Razzak,Shoaib Jameel*

Main category: cs.CV

TL;DR: 该研究结合视网膜微血管特征与血清脂质组数据，提出了一种创新的成像组学框架，用于早期心血管疾病风险的无症状生物标志物检测。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，但现有风险分层方法常无法检测早期亚临床变化。研究旨在填补视网膜微血管特征与脂质组数据关联的空白。

Method: 采用深度学习图像处理量化视网膜微血管特征，结合UHPLC ESI HRMS技术分析血清脂质组数据，进行大规模协变量调整和分层相关性分析。

Result: 研究发现视网膜动脉宽度、血管密度与特定脂质亚类（如TAGs、DAGs和Cers）之间存在强相关性，揭示了代谢压力下的微血管重塑机制。

Conclusion: 该研究为早期心血管疾病发病机制提供了新见解，并可能支持非侵入性生物标志物的开发，改善早期检测和个性化预防。

Abstract: Cardiovascular disease (CVD) remains the leading global cause of mortality, yet current risk stratification methods often fail to detect early, subclinical changes. Previous studies have generally not integrated retinal microvasculature characteristics with comprehensive serum lipidomic profiles as potential indicators of CVD risk. In this study, an innovative imaging omics framework was introduced, combining retinal microvascular traits derived through deep learning based image processing with serum lipidomic data to highlight asymptomatic biomarkers of cardiovascular risk beyond the conventional lipid panel. This represents the first large scale, covariate adjusted and stratified correlation analysis conducted in a healthy population, which is essential for identifying early indicators of disease. Retinal phenotypes were quantified using automated image analysis tools, while serum lipid profiling was performed by Ultra High Performance Liquid Chromatography Electrospray ionization High resolution mass spectrometry (UHPLC ESI HRMS). Strong, age- and sex-independent correlations were established, particularly between average artery width, vessel density, and lipid subclasses such as triacylglycerols (TAGs), diacylglycerols (DAGs), and ceramides (Cers). These associations suggest a converging mechanism of microvascular remodeling under metabolic stress. By linking detailed   vascular structural phenotypes to specific lipid species, this study fills a critical gap in the understanding of early CVD pathogenesis. This integration not only offers a novel perspective on microvascular metabolic associations but also presents a significant opportunity for the identification of robust, non-invasive biomarkers. Ultimately, these findings may support improved early detection, targeted prevention, and personalized approaches in cardiovascular healthcare.

</details>


### [5] [Domain-Enhanced Dual-Branch Model for Efficient and Interpretable Accident Anticipation](https://arxiv.org/abs/2507.12755)
*Yanchen Guan,Haicheng Liao,Chengyue Wang,Bonan Wang,Jiaxun Zhang,Jia Hu,Zhenning Li*

Main category: cs.CV

TL;DR: 提出一种基于双分支架构的交通事故预测框架，结合视觉和文本数据，通过特征聚合和大模型提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 开发精确且计算高效的交通事故预测系统，以支持自动驾驶技术的及时干预和损失预防。

Method: 采用双分支架构整合行车记录仪视频和事故报告文本数据，结合GPT-4o和Long-CLIP等大模型进行特征聚合，并利用提示工程生成标准化反馈。

Result: 在多个基准数据集上验证了方法的预测准确性、响应速度、计算效率和可解释性，达到最新技术水平。

Conclusion: 该框架为交通事故预测领域设定了新的性能标准。

Abstract: Developing precise and computationally efficient traffic accident anticipation system is crucial for contemporary autonomous driving technologies, enabling timely intervention and loss prevention. In this paper, we propose an accident anticipation framework employing a dual-branch architecture that effectively integrates visual information from dashcam videos with structured textual data derived from accident reports. Furthermore, we introduce a feature aggregation method that facilitates seamless integration of multimodal inputs through large models (GPT-4o, Long-CLIP), complemented by targeted prompt engineering strategies to produce actionable feedback and standardized accident archives. Comprehensive evaluations conducted on benchmark datasets (DAD, CCD, and A3D) validate the superior predictive accuracy, enhanced responsiveness, reduced computational overhead, and improved interpretability of our approach, thus establishing a new benchmark for state-of-the-art performance in traffic accident anticipation.

</details>


### [6] [HairShifter: Consistent and High-Fidelity Video Hair Transfer via Anchor-Guided Animation](https://arxiv.org/abs/2507.12758)
*Wangzheng Shi,Yinglin Zheng,Yuxin Lin,Jianmin Bao,Ming Zeng,Dong Chen*

Main category: cs.CV

TL;DR: HairShifter提出了一种新颖的“锚帧+动画”框架，用于高质量视频发型转移，解决了时空一致性和动态适应性问题。


<details>
  <summary>Details</summary>
Motivation: 视频发型转移在社交媒体、游戏等领域需求增长，但现有方法难以满足时空一致性和动态适应性要求。

Method: 结合图像发型转移模块（IHT）和多尺度门控SPADE解码器，实现高质量单帧转换和时空一致性。

Result: 实验表明HairShifter在视频发型转移中表现优异，兼具视觉质量、时间一致性和可扩展性。

Conclusion: HairShifter为视频发型转移提供了新方向，并建立了该领域的强基线。

Abstract: Hair transfer is increasingly valuable across domains such as social media, gaming, advertising, and entertainment. While significant progress has been made in single-image hair transfer, video-based hair transfer remains challenging due to the need for temporal consistency, spatial fidelity, and dynamic adaptability. In this work, we propose HairShifter, a novel "Anchor Frame + Animation" framework that unifies high-quality image hair transfer with smooth and coherent video animation. At its core, HairShifter integrates a Image Hair Transfer (IHT) module for precise per-frame transformation and a Multi-Scale Gated SPADE Decoder to ensure seamless spatial blending and temporal coherence. Our method maintains hairstyle fidelity across frames while preserving non-hair regions. Extensive experiments demonstrate that HairShifter achieves state-of-the-art performance in video hairstyle transfer, combining superior visual quality, temporal consistency, and scalability. The code will be publicly available. We believe this work will open new avenues for video-based hairstyle transfer and establish a robust baseline in this field.

</details>


### [7] [Think-Before-Draw: Decomposing Emotion Semantics & Fine-Grained Controllable Expressive Talking Head Generation](https://arxiv.org/abs/2507.12761)
*Hanlei Shi,Leyuan Qu,Yu Liu,Di Gao,Yuhua Zheng,Taihao Li*

Main category: cs.CV

TL;DR: 论文提出Think-Before-Draw框架，通过Chain-of-Thought将抽象情感标签转化为面部肌肉运动描述，并结合渐进式去噪策略优化微表情动态，实现了更自然的情感表达。


<details>
  <summary>Details</summary>
Motivation: 当前基于文本的情感头部生成方法依赖离散情感标签，简化了面部肌肉运动的动态复杂性，导致表达不自然。

Method: 引入Chain-of-Thought将情感标签转化为面部肌肉运动描述，采用渐进式去噪策略优化微表情动态。

Result: 在MEAD和HDTF基准测试中达到最优性能，并展示了零样本生成能力。

Conclusion: Think-Before-Draw框架通过语义解析和表达优化，显著提升了情感头部生成的自然性和表现力。

Abstract: Emotional talking-head generation has emerged as a pivotal research area at the intersection of computer vision and multimodal artificial intelligence, with its core value lying in enhancing human-computer interaction through immersive and empathetic engagement.With the advancement of multimodal large language models, the driving signals for emotional talking-head generation has shifted from audio and video to more flexible text. However, current text-driven methods rely on predefined discrete emotion label texts, oversimplifying the dynamic complexity of real facial muscle movements and thus failing to achieve natural emotional expressiveness.This study proposes the Think-Before-Draw framework to address two key challenges: (1) In-depth semantic parsing of emotions--by innovatively introducing Chain-of-Thought (CoT), abstract emotion labels are transformed into physiologically grounded facial muscle movement descriptions, enabling the mapping from high-level semantics to actionable motion features; and (2) Fine-grained expressiveness optimization--inspired by artists' portrait painting process, a progressive guidance denoising strategy is proposed, employing a "global emotion localization--local muscle control" mechanism to refine micro-expression dynamics in generated videos.Our experiments demonstrate that our approach achieves state-of-the-art performance on widely-used benchmarks, including MEAD and HDTF. Additionally, we collected a set of portrait images to evaluate our model's zero-shot generation capability.

</details>


### [8] [World Model-Based End-to-End Scene Generation for Accident Anticipation in Autonomous Driving](https://arxiv.org/abs/2507.12762)
*Yanchen Guan,Haicheng Liao,Chengyue Wang,Xingcheng Liu,Jiaxun Zhang,Zhenning Li*

Main category: cs.CV

TL;DR: 提出了一种结合生成场景增强和自适应时序推理的框架，用于提升交通事故预测的准确性和提前时间。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶系统中事故预测的两大挑战：高质量训练数据稀缺和关键物体线索缺失。

Method: 开发视频生成管道和动态预测模型，结合领域知识提示的生成场景和增强的时空关系建模。

Result: 在公开和新发布的数据集上验证了框架的有效性，提升了事故预测的准确性和提前时间。

Conclusion: 该框架为自动驾驶安全应用中的数据与建模限制提供了稳健解决方案。

Abstract: Reliable anticipation of traffic accidents is essential for advancing autonomous driving systems. However, this objective is limited by two fundamental challenges: the scarcity of diverse, high-quality training data and the frequent absence of crucial object-level cues due to environmental disruptions or sensor deficiencies. To tackle these issues, we propose a comprehensive framework combining generative scene augmentation with adaptive temporal reasoning. Specifically, we develop a video generation pipeline that utilizes a world model guided by domain-informed prompts to create high-resolution, statistically consistent driving scenarios, particularly enriching the coverage of edge cases and complex interactions. In parallel, we construct a dynamic prediction model that encodes spatio-temporal relationships through strengthened graph convolutions and dilated temporal operators, effectively addressing data incompleteness and transient visual noise. Furthermore, we release a new benchmark dataset designed to better capture diverse real-world driving risks. Extensive experiments on public and newly released datasets confirm that our framework enhances both the accuracy and lead time of accident anticipation, offering a robust solution to current data and modeling limitations in safety-critical autonomous driving applications.

</details>


### [9] [Local Representative Token Guided Merging for Text-to-Image Generation](https://arxiv.org/abs/2507.12771)
*Min-Jeong Lee,Hee-Dong Kim,Seong-Whan Lee*

Main category: cs.CV

TL;DR: ReToM是一种新的token合并策略，通过局部代表性token引导合并，提升图像生成模型的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 解决Stable Diffusion等图像生成模型中注意力操作的高计算复杂度问题，同时保持生成质量。

Method: 提出局部代表性token引导合并（ReToM），定义局部窗口并调整大小，选择最具代表性的token进行合并。

Result: 实验显示ReToM在FID上提升6.2%，CLIP分数更高，同时保持推理时间。

Conclusion: ReToM在视觉质量和计算效率之间取得了良好平衡。

Abstract: Stable diffusion is an outstanding image generation model for text-to-image, but its time-consuming generation process remains a challenge due to the quadratic complexity of attention operations. Recent token merging methods improve efficiency by reducing the number of tokens during attention operations, but often overlook the characteristics of attention-based image generation models, limiting their effectiveness. In this paper, we propose local representative token guided merging (ReToM), a novel token merging strategy applicable to any attention mechanism in image generation. To merge tokens based on various contextual information, ReToM defines local boundaries as windows within attention inputs and adjusts window sizes. Furthermore, we introduce a representative token, which represents the most representative token per window by computing similarity at a specific timestep and selecting the token with the highest average similarity. This approach preserves the most salient local features while minimizing computational overhead. Experimental results show that ReToM achieves a 6.2% improvement in FID and higher CLIP scores compared to the baseline, while maintaining comparable inference time. We empirically demonstrate that ReToM is effective in balancing visual quality and computational efficiency.

</details>


### [10] [DMQ: Dissecting Outliers of Diffusion Models for Post-Training Quantization](https://arxiv.org/abs/2507.12933)
*Dongyeun Lee,Jiwan Hur,Hyounguk Shon,Jae Young Lee,Junmo Kim*

Main category: cs.CV

TL;DR: 提出DMQ方法，结合LES和PTS，优化扩散模型的量化性能，显著提升低比特宽度下的生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型计算成本高，现有量化方法忽视异常值，导致低比特宽度性能下降。

Method: 结合LES优化通道缩放因子，引入PTS处理高方差通道，采用自适应时间步加权和投票算法。

Result: 在W4A6和W4A8等低比特宽度下显著优于现有方法，保持高质量图像生成。

Conclusion: DMQ有效解决了扩散模型量化中的关键问题，提升了低比特宽度下的性能和稳定性。

Abstract: Diffusion models have achieved remarkable success in image generation but come with significant computational costs, posing challenges for deployment in resource-constrained environments. Recent post-training quantization (PTQ) methods have attempted to mitigate this issue by focusing on the iterative nature of diffusion models. However, these approaches often overlook outliers, leading to degraded performance at low bit-widths. In this paper, we propose a DMQ which combines Learned Equivalent Scaling (LES) and channel-wise Power-of-Two Scaling (PTS) to effectively address these challenges. Learned Equivalent Scaling optimizes channel-wise scaling factors to redistribute quantization difficulty between weights and activations, reducing overall quantization error. Recognizing that early denoising steps, despite having small quantization errors, crucially impact the final output due to error accumulation, we incorporate an adaptive timestep weighting scheme to prioritize these critical steps during learning. Furthermore, identifying that layers such as skip connections exhibit high inter-channel variance, we introduce channel-wise Power-of-Two Scaling for activations. To ensure robust selection of PTS factors even with small calibration set, we introduce a voting algorithm that enhances reliability. Extensive experiments demonstrate that our method significantly outperforms existing works, especially at low bit-widths such as W4A6 (4-bit weight, 6-bit activation) and W4A8, maintaining high image generation quality and model stability. The code is available at https://github.com/LeeDongYeun/dmq.

</details>


### [11] [LoViC: Efficient Long Video Generation with Context Compression](https://arxiv.org/abs/2507.12952)
*Jiaxiu Jiang,Wenbo Li,Jingjing Ren,Yuping Qiu,Yong Guo,Xiaogang Xu,Han Wu,Wangmeng Zuo*

Main category: cs.CV

TL;DR: LoViC是一个基于扩散变换器（DiT）的框架，通过分段生成过程解决长视频生成的挑战，核心是FlexFormer，支持可变长度输入和统一潜在表示。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散变换器在文本到视频生成方面取得进展，但由于自注意力的二次复杂度，生成长视频仍具挑战性。现有方法（如稀疏注意力）往往牺牲时间连贯性或可扩展性。

Method: 提出LoViC框架，核心为FlexFormer，通过联合压缩视频和文本到统一潜在表示，支持可变长度输入和线性调整压缩率。采用位置感知机制编码时间上下文。

Result: 实验验证了LoViC在预测、回溯、插值和多镜头生成等任务中的有效性和多功能性。

Conclusion: LoViC通过FlexFormer和分段生成过程，实现了长视频的高效生成，同时保持时间连贯性和可扩展性。

Abstract: Despite recent advances in diffusion transformers (DiTs) for text-to-video generation, scaling to long-duration content remains challenging due to the quadratic complexity of self-attention. While prior efforts -- such as sparse attention and temporally autoregressive models -- offer partial relief, they often compromise temporal coherence or scalability. We introduce LoViC, a DiT-based framework trained on million-scale open-domain videos, designed to produce long, coherent videos through a segment-wise generation process. At the core of our approach is FlexFormer, an expressive autoencoder that jointly compresses video and text into unified latent representations. It supports variable-length inputs with linearly adjustable compression rates, enabled by a single query token design based on the Q-Former architecture. Additionally, by encoding temporal context through position-aware mechanisms, our model seamlessly supports prediction, retradiction, interpolation, and multi-shot generation within a unified paradigm. Extensive experiments across diverse tasks validate the effectiveness and versatility of our approach.

</details>


### [12] [RGB Pre-Training Enhanced Unobservable Feature Latent Diffusion Model for Spectral Reconstruction](https://arxiv.org/abs/2507.12967)
*Keli Deng,Jie Nie,Yuntao Qian*

Main category: cs.CV

TL;DR: 该论文提出了一种基于预训练RGB潜在扩散模型（RGB-LDM）的方法，通过扩展为不可观测特征LDM（ULDM）来解决光谱重建问题，实现了光谱-空间联合分布学习。


<details>
  <summary>Details</summary>
Motivation: 光谱重建（SR）需要从RGB图像重建高光谱图像（HSIs），但RGB传感器无法捕捉全部光谱信息，因此需要有效构建光谱-空间联合分布以补充不可观测特征。

Method: 提出两阶段流程：1）光谱结构表示学习，训练光谱不可观测特征自编码器（SpeUAE）提取和压缩特征；2）光谱-空间联合分布学习，通过SpeUAE和空间自编码器（SpaAE）编码结构，ULDM建模分布。

Result: 实验表明，该方法在SR和下游重光照任务中达到最优性能。

Conclusion: 通过分离不可观测特征并利用RGB预训练模型的空间知识，ULDM在紧凑潜在空间中有效学习联合分布，显著提升了光谱重建效果。

Abstract: Spectral reconstruction (SR) is a crucial problem in image processing that requires reconstructing hyperspectral images (HSIs) from the corresponding RGB images. A key difficulty in SR is estimating the unobservable feature, which encapsulates significant spectral information not captured by RGB imaging sensors. The solution lies in effectively constructing the spectral-spatial joint distribution conditioned on the RGB image to complement the unobservable feature. Since HSIs share a similar spatial structure with the corresponding RGB images, it is rational to capitalize on the rich spatial knowledge in RGB pre-trained models for spectral-spatial joint distribution learning. To this end, we extend the RGB pre-trained latent diffusion model (RGB-LDM) to an unobservable feature LDM (ULDM) for SR. As the RGB-LDM and its corresponding spatial autoencoder (SpaAE) already excel in spatial knowledge, the ULDM can focus on modeling spectral structure. Moreover, separating the unobservable feature from the HSI reduces the redundant spectral information and empowers the ULDM to learn the joint distribution in a compact latent space. Specifically, we propose a two-stage pipeline consisting of spectral structure representation learning and spectral-spatial joint distribution learning to transform the RGB-LDM into the ULDM. In the first stage, a spectral unobservable feature autoencoder (SpeUAE) is trained to extract and compress the unobservable feature into a 3D manifold aligned with RGB space. In the second stage, the spectral and spatial structures are sequentially encoded by the SpeUAE and the SpaAE, respectively. The ULDM is then acquired to model the distribution of the coded unobservable feature with guidance from the corresponding RGB images. Experimental results on SR and downstream relighting tasks demonstrate that our proposed method achieves state-of-the-art performance.

</details>


### [13] [Resurrect Mask AutoRegressive Modeling for Efficient and Scalable Image Generation](https://arxiv.org/abs/2507.13032)
*Yi Xin,Le Zhuo,Qi Qin,Siqi Luo,Yuewen Cao,Bin Fu,Yangfan He,Hongsheng Li,Guangtao Zhai,Xiaohong Liu,Peng Gao*

Main category: cs.CV

TL;DR: MaskGIL改进MAR架构，通过双向注意力和2D RoPE提升图像生成质量，达到与AR模型相当的性能，同时减少推理步骤。


<details>
  <summary>Details</summary>
Motivation: 传统MAR模型在图像生成上表现不如AR模型，研究旨在通过改进架构提升MAR的性能。

Method: 评估图像分词器，引入双向注意力和2D RoPE，构建MaskGIL模型，并扩展至文本驱动生成。

Result: MaskGIL在ImageNet 256x256上FID达3.71，仅需8步推理，性能媲美AR模型。

Conclusion: MaskGIL不仅提升图像生成效率，还扩展至文本驱动和实时语音转图像应用。

Abstract: AutoRegressive (AR) models have made notable progress in image generation, with Masked AutoRegressive (MAR) models gaining attention for their efficient parallel decoding. However, MAR models have traditionally underperformed when compared to standard AR models. This study refines the MAR architecture to improve image generation quality. We begin by evaluating various image tokenizers to identify the most effective one. Subsequently, we introduce an improved Bidirectional LLaMA architecture by replacing causal attention with bidirectional attention and incorporating 2D RoPE, which together form our advanced model, MaskGIL. Scaled from 111M to 1.4B parameters, MaskGIL achieves a FID score of 3.71, matching state-of-the-art AR models in the ImageNet 256x256 benchmark, while requiring only 8 inference steps compared to the 256 steps of AR models. Furthermore, we develop a text-driven MaskGIL model with 775M parameters for generating images from text at various resolutions. Beyond image generation, MaskGIL extends to accelerate AR-based generation and enable real-time speech-to-image conversion. Our codes and models are available at https://github.com/synbol/MaskGIL.

</details>


### [14] [Label-Consistent Dataset Distillation with Detector-Guided Refinement](https://arxiv.org/abs/2507.13074)
*Yawen Zou,Guang Li,Zi Wang,Chunzhi Gu,Chao Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于检测器引导的数据集蒸馏框架，通过预训练检测器优化合成样本，确保标签一致性和图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在数据集蒸馏中生成的样本存在标签不一致或结构细节不足的问题，影响下游任务性能。

Method: 利用预训练检测器识别异常样本，通过扩散模型生成候选样本，并基于检测器置信度和多样性选择最优候选。

Result: 实验表明，该方法能生成高质量、细节丰富的代表性图像，在验证集上达到最优性能。

Conclusion: 提出的检测器引导框架有效解决了标签不一致和图像质量问题，提升了数据集蒸馏的性能。

Abstract: Dataset distillation (DD) aims to generate a compact yet informative dataset that achieves performance comparable to the original dataset, thereby reducing demands on storage and computational resources. Although diffusion models have made significant progress in dataset distillation, the generated surrogate datasets often contain samples with label inconsistencies or insufficient structural detail, leading to suboptimal downstream performance. To address these issues, we propose a detector-guided dataset distillation framework that explicitly leverages a pre-trained detector to identify and refine anomalous synthetic samples, thereby ensuring label consistency and improving image quality. Specifically, a detector model trained on the original dataset is employed to identify anomalous images exhibiting label mismatches or low classification confidence. For each defective image, multiple candidates are generated using a pre-trained diffusion model conditioned on the corresponding image prototype and label. The optimal candidate is then selected by jointly considering the detector's confidence score and dissimilarity to existing qualified synthetic samples, thereby ensuring both label accuracy and intra-class diversity. Experimental results demonstrate that our method can synthesize high-quality representative images with richer details, achieving state-of-the-art performance on the validation set.

</details>


### [15] [DiffOSeg: Omni Medical Image Segmentation via Multi-Expert Collaboration Diffusion Model](https://arxiv.org/abs/2507.13087)
*Han Zhang,Xiangde Luo,Yong Chen,Kang Li*

Main category: cs.CV

TL;DR: DiffOSeg是一个基于扩散的两阶段框架，旨在同时实现共识驱动和偏好驱动的医学图像分割。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中标注变异性问题显著，传统深度学习方法无法捕捉标注者偏差。

Method: 提出DiffOSeg框架，第一阶段通过概率共识策略建立群体共识，第二阶段通过自适应提示捕捉专家偏好。

Result: 在LIDC-IDRI和NPC-170数据集上表现优于现有方法。

Conclusion: DiffOSeg成功结合共识和偏好驱动分割，提供更全面的视角。

Abstract: Annotation variability remains a substantial challenge in medical image segmentation, stemming from ambiguous imaging boundaries and diverse clinical expertise. Traditional deep learning methods producing single deterministic segmentation predictions often fail to capture these annotator biases. Although recent studies have explored multi-rater segmentation, existing methods typically focus on a single perspective -- either generating a probabilistic ``gold standard'' consensus or preserving expert-specific preferences -- thus struggling to provide a more omni view. In this study, we propose DiffOSeg, a two-stage diffusion-based framework, which aims to simultaneously achieve both consensus-driven (combining all experts' opinions) and preference-driven (reflecting experts' individual assessments) segmentation. Stage I establishes population consensus through a probabilistic consensus strategy, while Stage II captures expert-specific preference via adaptive prompts. Demonstrated on two public datasets (LIDC-IDRI and NPC-170), our model outperforms existing state-of-the-art methods across all evaluated metrics. Source code is available at https://github.com/string-ellipses/DiffOSeg .

</details>


### [16] [GLAD: Generalizable Tuning for Vision-Language Models](https://arxiv.org/abs/2507.13089)
*Yuqi Peng,Pengfei Wang,Jianzhuang Liu,Shifeng Chen*

Main category: cs.CV

TL;DR: 论文提出了一种名为GLAD的通用框架，通过LoRA和梯度正则化技术解决了现有提示调优方法在少样本场景中的过拟合问题，并在多个基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有提示调优方法在少样本场景中容易过拟合，且依赖复杂架构和敏感超参数，限制了通用性。

Method: GLAD结合了LoRA和梯度正则化技术，优化模型参数以增强鲁棒性。

Result: 在15个基准数据集上，GLAD在基础到新类泛化、图像域泛化和跨数据集泛化方面优于现有方法。

Conclusion: GLAD提供了一种简单且通用的解决方案，显著提升了少样本学习中的模型性能。

Abstract: Pre-trained vision-language models, such as CLIP, show impressive zero-shot recognition ability and can be easily transferred to specific downstream tasks via prompt tuning, even with limited training data. However, existing prompt tuning methods face two main challenges: (1) In few-shot scenarios, data scarcity often leads to overfitting, making the model sensitive to changes in the input domain. (2) To mitigate overfitting, these methods typically rely on complex task-specific model architectures and sensitive hyperparameter tuning, severely restricting their general applicability. To address these issues, we propose a simpler and more general framework called GLAD (Generalizable LoRA tuning with RegulArized GraDient). We show that merely applying LoRA achieves performance in downstream tasks comparable to current state-of-the-art prompt-based methods. While LoRA is effective and easy to use, it remains susceptible to overfitting in few-shot learning scenarios. To mitigate this risk, we introduce a gradient-based regularization technique. This technique effectively steers the optimization trajectory, encouraging the model to find a more stable parameter region that is robust to variations in data distribution. Through extensive experiments conducted on 15 benchmark datasets, we demonstrate that GLAD outperforms previous tuning approaches in terms of base-to-novel class generalization, image domain generalization, and cross-dataset generalization. The code will be publicly available.

</details>


### [17] [Leveraging Language Prior for Infrared Small Target Detection](https://arxiv.org/abs/2507.13113)
*Pranav Singh,Pravendra Singh*

Main category: cs.CV

TL;DR: 提出了一种结合语言先验的多模态红外小目标检测框架，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有红外小目标检测方法仅依赖图像模态，限制了性能提升。结合语言先验的多模态方法有望改进检测能力。

Method: 利用GPT-4生成目标位置文本描述，结合语言引导的注意力权重增强检测能力，并构建了多模态红外数据集。

Result: 在NUAA-SIRST和IRSTD-1k子集上，IoU、nIoU、Pd和Fa指标分别提升了9.74%、13.02%、1.25%、67.87%和4.41%、2.04%、2.01%、113.43%。

Conclusion: 多模态方法显著提升了红外小目标检测性能，验证了语言先验的有效性。

Abstract: IRSTD (InfraRed Small Target Detection) detects small targets in infrared blurry backgrounds and is essential for various applications. The detection task is challenging due to the small size of the targets and their sparse distribution in infrared small target datasets. Although existing IRSTD methods and datasets have led to significant advancements, they are limited by their reliance solely on the image modality. Recent advances in deep learning and large vision-language models have shown remarkable performance in various visual recognition tasks. In this work, we propose a novel multimodal IRSTD framework that incorporates language priors to guide small target detection. We leverage language-guided attention weights derived from the language prior to enhance the model's ability for IRSTD, presenting a novel approach that combines textual information with image data to improve IRSTD capabilities. Utilizing the state-of-the-art GPT-4 vision model, we generate text descriptions that provide the locations of small targets in infrared images, employing careful prompt engineering to ensure improved accuracy. Due to the absence of multimodal IR datasets, existing IRSTD methods rely solely on image data. To address this shortcoming, we have curated a multimodal infrared dataset that includes both image and text modalities for small target detection, expanding upon the popular IRSTD-1k and NUDT-SIRST datasets. We validate the effectiveness of our approach through extensive experiments and comprehensive ablation studies. The results demonstrate significant improvements over the state-of-the-art method, with relative percentage differences of 9.74%, 13.02%, 1.25%, and 67.87% in IoU, nIoU, Pd, and Fa on the NUAA-SIRST subset, and 4.41%, 2.04%, 2.01%, and 113.43% on the IRSTD-1k subset of the LangIR dataset, respectively.

</details>


### [18] [Synthesizing Reality: Leveraging the Generative AI-Powered Platform Midjourney for Construction Worker Detection](https://arxiv.org/abs/2507.13221)
*Hongyang Zhao,Tianyu Liang,Sina Davari,Daeho Kim*

Main category: cs.CV

TL;DR: 研究提出了一种利用Midjourney生成合成图像的方法，用于解决建筑工人检测中数据不足的问题，并在真实数据集上取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在视觉AI中表现优异，但建筑领域的数据多样性和数量不足限制了其应用。

Method: 通过Midjourney生成12,000张合成图像，强调真实性和多样性，并用于DNN训练。

Result: 在真实数据集上，模型的平均精度为0.937（IoU=0.5）和0.642（IoU=0.5-0.95）；在合成数据集上表现更优。

Conclusion: 生成式AI在解决DNN训练数据不足方面具有潜力，但也存在局限性。

Abstract: While recent advancements in deep neural networks (DNNs) have substantially enhanced visual AI's capabilities, the challenge of inadequate data diversity and volume remains, particularly in construction domain. This study presents a novel image synthesis methodology tailored for construction worker detection, leveraging the generative-AI platform Midjourney. The approach entails generating a collection of 12,000 synthetic images by formulating 3000 different prompts, with an emphasis on image realism and diversity. These images, after manual labeling, serve as a dataset for DNN training. Evaluation on a real construction image dataset yielded promising results, with the model attaining average precisions (APs) of 0.937 and 0.642 at intersection-over-union (IoU) thresholds of 0.5 and 0.5 to 0.95, respectively. Notably, the model demonstrated near-perfect performance on the synthetic dataset, achieving APs of 0.994 and 0.919 at the two mentioned thresholds. These findings reveal both the potential and weakness of generative AI in addressing DNN training data scarcity.

</details>


### [19] [Leveraging Pre-Trained Visual Models for AI-Generated Video Detection](https://arxiv.org/abs/2507.13224)
*Keerthi Veeramachaneni,Praveen Tirupattur,Amrit Singh Bedi,Mubarak Shah*

Main category: cs.CV

TL;DR: 提出了一种利用预训练视觉模型检测AI生成视频的新方法，无需额外训练即可实现高准确率。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成视频质量的提升，检测这类内容对于打击虚假信息、保护隐私和安全至关重要。现有方法主要针对DeepFakes，而视频生成技术已超越此范围，亟需通用检测方法。

Method: 利用预训练视觉模型提取特征，区分真实与生成视频，并通过简单线性分类层进一步提升性能。

Result: 在包含10,000个AI生成视频和4,000个真实视频的数据集上，平均检测准确率超过90%。

Conclusion: 该方法高效且无需额外训练，计划公开代码、模型和数据集以支持后续研究。

Abstract: Recent advances in Generative AI (GenAI) have led to significant improvements in the quality of generated visual content. As AI-generated visual content becomes increasingly indistinguishable from real content, the challenge of detecting the generated content becomes critical in combating misinformation, ensuring privacy, and preventing security threats. Although there has been substantial progress in detecting AI-generated images, current methods for video detection are largely focused on deepfakes, which primarily involve human faces. However, the field of video generation has advanced beyond DeepFakes, creating an urgent need for methods capable of detecting AI-generated videos with generic content. To address this gap, we propose a novel approach that leverages pre-trained visual models to distinguish between real and generated videos. The features extracted from these pre-trained models, which have been trained on extensive real visual content, contain inherent signals that can help distinguish real from generated videos. Using these extracted features, we achieve high detection performance without requiring additional model training, and we further improve performance by training a simple linear classification layer on top of the extracted features. We validated our method on a dataset we compiled (VID-AID), which includes around 10,000 AI-generated videos produced by 9 different text-to-video models, along with 4,000 real videos, totaling over 7 hours of video content. Our evaluation shows that our approach achieves high detection accuracy, above 90% on average, underscoring its effectiveness. Upon acceptance, we plan to publicly release the code, the pre-trained models, and our dataset to support ongoing research in this critical area.

</details>


### [20] [Efficient Adaptation of Pre-trained Vision Transformer underpinned by Approximately Orthogonal Fine-Tuning Strategy](https://arxiv.org/abs/2507.13260)
*Yiting Yang,Hao Luo,Yuan Sun,Qingsen Yan,Haokui Zhang,Wei Dong,Guoqing Wang,Peng Wang,Yang Yang,Hengtao Shen*

Main category: cs.CV

TL;DR: 论文提出了一种近似正交微调（AOFT）策略，通过生成近似正交向量来构建低秩权重矩阵，以提升Vision Transformers在下游任务中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 观察到预训练ViT权重矩阵的向量具有近似正交性，而低秩适应矩阵（如LoRA和Adapter）的向量缺乏这一特性，作者希望通过引入近似正交性来增强模型的泛化能力。

Method: 提出AOFT策略，使用单个可学习向量生成一组近似正交向量，用于构建低秩适应矩阵，使其与预训练权重矩阵的性质对齐。

Result: 实验结果表明，AOFT在多个下游图像分类任务中表现优异，验证了其增强泛化能力的有效性。

Conclusion: AOFT通过引入近似正交性，显著提升了ViT在下游任务中的泛化能力，为参数高效微调提供了新思路。

Abstract: A prevalent approach in Parameter-Efficient Fine-Tuning (PEFT) of pre-trained Vision Transformers (ViT) involves freezing the majority of the backbone parameters and solely learning low-rank adaptation weight matrices to accommodate downstream tasks. These low-rank matrices are commonly derived through the multiplication structure of down-projection and up-projection matrices, exemplified by methods such as LoRA and Adapter. In this work, we observe an approximate orthogonality among any two row or column vectors within any weight matrix of the backbone parameters; however, this property is absent in the vectors of the down/up-projection matrices. Approximate orthogonality implies a reduction in the upper bound of the model's generalization error, signifying that the model possesses enhanced generalization capability. If the fine-tuned down/up-projection matrices were to exhibit this same property as the pre-trained backbone matrices, could the generalization capability of fine-tuned ViTs be further augmented? To address this question, we propose an Approximately Orthogonal Fine-Tuning (AOFT) strategy for representing the low-rank weight matrices. This strategy employs a single learnable vector to generate a set of approximately orthogonal vectors, which form the down/up-projection matrices, thereby aligning the properties of these matrices with those of the backbone. Extensive experimental results demonstrate that our method achieves competitive performance across a range of downstream image classification tasks, confirming the efficacy of the enhanced generalization capability embedded in the down/up-projection matrices.

</details>


### [21] [DiffClean: Diffusion-based Makeup Removal for Accurate Age Estimation](https://arxiv.org/abs/2507.13292)
*Ekta Balkrishna Gavas,Chinmay Hegde,Nasir Memon,Sudipta Banerjee*

Main category: cs.CV

TL;DR: DiffClean利用文本引导扩散模型消除化妆痕迹，提升年龄验证和面部识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 化妆可能干扰年龄验证和面部识别，需要一种方法来防御化妆攻击。

Method: 提出DiffClean，通过文本引导扩散模型消除化妆痕迹。

Result: 在数字模拟和真实化妆图像上，DiffClean显著提升了年龄验证（4.8%）和面部识别（8.9%）的准确性。

Conclusion: DiffClean能有效防御化妆攻击，提升年龄验证和面部识别的性能。

Abstract: Accurate age verification can protect underage users from unauthorized access to online platforms and e-commerce sites that provide age-restricted services. However, accurate age estimation can be confounded by several factors, including facial makeup that can induce changes to alter perceived identity and age to fool both humans and machines. In this work, we propose DiffClean which erases makeup traces using a text-guided diffusion model to defend against makeup attacks. DiffClean improves age estimation (minor vs. adult accuracy by 4.8%) and face verification (TMR by 8.9% at FMR=0.01%) over competing baselines on digitally simulated and real makeup images.

</details>


### [22] [FashionPose: Text to Pose to Relight Image Generation for Personalized Fashion Visualization](https://arxiv.org/abs/2507.13311)
*Chuancheng Shi,Yixiang Chen,Burong Lei,Jichao Chen*

Main category: cs.CV

TL;DR: FashionPose是一个统一的文本到姿势到光照生成框架，用于时尚电商中的个性化服装预览。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖预定义姿势、缺乏语义灵活性和光照适应性的问题。

Method: 通过自然语言描述预测2D人体姿势，使用扩散模型生成高保真人物图像，并应用轻量级光照模块。

Result: 实验表明，FashionPose实现了细粒度姿势合成和高效一致的光照控制。

Conclusion: FashionPose为个性化虚拟时尚展示提供了实用解决方案。

Abstract: Realistic and controllable garment visualization is critical for fashion e-commerce, where users expect personalized previews under diverse poses and lighting conditions. Existing methods often rely on predefined poses, limiting semantic flexibility and illumination adaptability. To address this, we introduce FashionPose, the first unified text-to-pose-to-relighting generation framework. Given a natural language description, our method first predicts a 2D human pose, then employs a diffusion model to generate high-fidelity person images, and finally applies a lightweight relighting module, all guided by the same textual input. By replacing explicit pose annotations with text-driven conditioning, FashionPose enables accurate pose alignment, faithful garment rendering, and flexible lighting control. Experiments demonstrate fine-grained pose synthesis and efficient, consistent relighting, providing a practical solution for personalized virtual fashion display.

</details>


### [23] [Taming Diffusion Transformer for Real-Time Mobile Video Generation](https://arxiv.org/abs/2507.13343)
*Yushu Wu,Yanyu Li,Anil Kag,Ivan Skorokhodov,Willi Menapace,Ke Ma,Arpit Sahni,Ju Hu,Aliaksandr Siarohin,Dhritiman Sagar,Yanzhi Wang,Sergey Tulyakov*

Main category: cs.CV

TL;DR: 提出了一种针对Diffusion Transformers（DiT）的优化方法，显著提升视频生成速度，实现移动设备上的实时性能。


<details>
  <summary>Details</summary>
Motivation: DiT在视频生成任务中表现优异，但计算成本高，难以在资源受限设备（如智能手机）上实现实时生成。

Method: 1. 使用高压缩变分自编码器（VAE）降低输入数据维度；2. 提出KD引导的三级剪枝策略缩小模型；3. 开发针对DiT的对抗性步数蒸馏技术。

Result: 优化后模型在iPhone 16 Pro Max上实现每秒10帧以上的生成速度。

Conclusion: 证明了在移动设备上实现实时高质量视频生成的可行性。

Abstract: Diffusion Transformers (DiT) have shown strong performance in video generation tasks, but their high computational cost makes them impractical for resource-constrained devices like smartphones, and real-time generation is even more challenging. In this work, we propose a series of novel optimizations to significantly accelerate video generation and enable real-time performance on mobile platforms. First, we employ a highly compressed variational autoencoder (VAE) to reduce the dimensionality of the input data without sacrificing visual quality. Second, we introduce a KD-guided, sensitivity-aware tri-level pruning strategy to shrink the model size to suit mobile platform while preserving critical performance characteristics. Third, we develop an adversarial step distillation technique tailored for DiT, which allows us to reduce the number of inference steps to four. Combined, these optimizations enable our model to achieve over 10 frames per second (FPS) generation on an iPhone 16 Pro Max, demonstrating the feasibility of real-time, high-quality video generation on mobile devices.

</details>


### [24] [Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos with Spatio-Temporal Diffusion Models](https://arxiv.org/abs/2507.13344)
*Yudong Jin,Sida Peng,Xuan Wang,Tao Xie,Zhen Xu,Yifan Yang,Yujun Shen,Hujun Bao,Xiaowei Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种滑动迭代去噪方法，用于提升4D扩散模型在稀疏视角视频输入下的时空一致性，从而生成高质量的新视角视频。


<details>
  <summary>Details</summary>
Motivation: 解决现有4D扩散模型在生成新视角视频时时空一致性不足的问题。

Method: 定义潜在网格编码图像、相机位姿和人体姿态，通过滑动窗口沿时空维度交替去噪，最终解码为目标视角视频。

Result: 在DNA-Rendering和ActorsHQ数据集上验证了方法的高质量和一致性，显著优于现有方法。

Conclusion: 滑动迭代去噪方法有效提升了4D扩散模型的时空一致性，且GPU内存消耗可控。

Abstract: This paper addresses the challenge of high-fidelity view synthesis of humans with sparse-view videos as input. Previous methods solve the issue of insufficient observation by leveraging 4D diffusion models to generate videos at novel viewpoints. However, the generated videos from these models often lack spatio-temporal consistency, thus degrading view synthesis quality. In this paper, we propose a novel sliding iterative denoising process to enhance the spatio-temporal consistency of the 4D diffusion model. Specifically, we define a latent grid in which each latent encodes the image, camera pose, and human pose for a certain viewpoint and timestamp, then alternately denoising the latent grid along spatial and temporal dimensions with a sliding window, and finally decode the videos at target viewpoints from the corresponding denoised latents. Through the iterative sliding, information flows sufficiently across the latent grid, allowing the diffusion model to obtain a large receptive field and thus enhance the 4D consistency of the output, while making the GPU memory consumption affordable. The experiments on the DNA-Rendering and ActorsHQ datasets demonstrate that our method is able to synthesize high-quality and consistent novel-view videos and significantly outperforms the existing approaches. See our project page for interactive demos and video results: https://diffuman4d.github.io/ .

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [25] [NLI4VolVis: Natural Language Interaction for Volume Visualization via LLM Multi-Agents and Editable 3D Gaussian Splatting](https://arxiv.org/abs/2507.12621)
*Kuangshi Ai,Kaiyuan Tang,Chaoli Wang*

Main category: cs.HC

TL;DR: NLI4VolVis是一个基于自然语言的交互式体积可视化系统，通过多视图语义分割和视觉语言模型实现语义级交互，解决了传统方法的局限。


<details>
  <summary>Details</summary>
Motivation: 传统体积可视化方法存在设计僵化和计算成本高的问题，而现有新视角合成方法对非专家用户不友好且缺乏语义交互支持。

Method: 系统结合多视图语义分割、视觉语言模型和多代理大语言模型架构，支持自然语言查询和编辑体积场景。

Result: NLI4VolVis实现了开放词汇对象查询、实时场景编辑、最佳视角选择和2D风格化等功能，并通过案例和用户研究验证了其可用性。

Conclusion: NLI4VolVis显著提升了体积数据探索的易用性和可访问性，推荐读者查看案例研究和演示视频。

Abstract: Traditional volume visualization (VolVis) methods, like direct volume rendering, suffer from rigid transfer function designs and high computational costs. Although novel view synthesis approaches enhance rendering efficiency, they require additional learning effort for non-experts and lack support for semantic-level interaction. To bridge this gap, we propose NLI4VolVis, an interactive system that enables users to explore, query, and edit volumetric scenes using natural language. NLI4VolVis integrates multi-view semantic segmentation and vision-language models to extract and understand semantic components in a scene. We introduce a multi-agent large language model architecture equipped with extensive function-calling tools to interpret user intents and execute visualization tasks. The agents leverage external tools and declarative VolVis commands to interact with the VolVis engine powered by 3D editable Gaussians, enabling open-vocabulary object querying, real-time scene editing, best-view selection, and 2D stylization. We validate our system through case studies and a user study, highlighting its improved accessibility and usability in volumetric data exploration. We strongly recommend readers check our case studies, demo video, and source code at https://nli4volvis.github.io/.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [26] [Pixel Perfect MegaMed: A Megapixel-Scale Vision-Language Foundation Model for Generating High Resolution Medical Images](https://arxiv.org/abs/2507.12698)
*Zahra TehraniNasab,Amar Kumar,Tal Arbel*

Main category: eess.IV

TL;DR: Pixel Perfect MegaMed 是一种基于多尺度变换器架构的视觉-语言基础模型，用于生成1024x1024分辨率的高质量医学图像，解决了传统生成模型在保留细节上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统生成模型（如GANs或VAEs）在生成高分辨率医学图像时难以保留关键细节，影响诊断准确性。

Method: 采用多尺度变换器架构，结合视觉-语言对齐技术，生成超高分辨率医学图像。

Result: 在CheXpert数据集上生成临床可信的胸部X光片，并在数据增强任务中提升分类性能。

Conclusion: Pixel Perfect MegaMed 在医学图像合成中表现出色，尤其在低数据场景下具有实用价值。

Abstract: Medical image synthesis presents unique challenges due to the inherent complexity and high-resolution details required in clinical contexts. Traditional generative architectures such as Generative Adversarial Networks (GANs) or Variational Auto Encoder (VAEs) have shown great promise for high-resolution image generation but struggle with preserving fine-grained details that are key for accurate diagnosis. To address this issue, we introduce Pixel Perfect MegaMed, the first vision-language foundation model to synthesize images at resolutions of 1024x1024. Our method deploys a multi-scale transformer architecture designed specifically for ultra-high resolution medical image generation, enabling the preservation of both global anatomical context and local image-level details. By leveraging vision-language alignment techniques tailored to medical terminology and imaging modalities, Pixel Perfect MegaMed bridges the gap between textual descriptions and visual representations at unprecedented resolution levels. We apply our model to the CheXpert dataset and demonstrate its ability to generate clinically faithful chest X-rays from text prompts. Beyond visual quality, these high-resolution synthetic images prove valuable for downstream tasks such as classification, showing measurable performance gains when used for data augmentation, particularly in low-data regimes. Our code is accessible through the project website - https://tehraninasab.github.io/pixelperfect-megamed.

</details>


### [27] [From Variability To Accuracy: Conditional Bernoulli Diffusion Models with Consensus-Driven Correction for Thin Structure Segmentation](https://arxiv.org/abs/2507.12985)
*Jinseo An,Min Jin Lee,Kyu Won Shim,Helen Hong*

Main category: eess.IV

TL;DR: 提出了一种基于多扩散模型共识的框架，用于修正面部CT图像中眼眶骨的分割结果，显著提高了模糊区域的召回率并保持了薄结构的连续性。


<details>
  <summary>Details</summary>
Motivation: 眼眶骨分割在定制植入物中至关重要，但现有方法在模糊边界和薄结构区域表现不佳，常出现断开或欠分割问题。

Method: 使用条件伯努利扩散模型生成多样化的分割结果，并通过共识驱动修正结合位置邻近性、共识水平和梯度方向相似性来优化模糊区域。

Result: 实验表明，该方法优于现有方法，显著提高了模糊区域的召回率，同时保持了薄结构的连续性。

Conclusion: 该方法自动化了分割修正过程，可应用于图像引导的手术规划和手术中。

Abstract: Accurate segmentation of orbital bones in facial computed tomography (CT) images is essential for the creation of customized implants for reconstruction of defected orbital bones, particularly challenging due to the ambiguous boundaries and thin structures such as the orbital medial wall and orbital floor. In these ambiguous regions, existing segmentation approaches often output disconnected or under-segmented results. We propose a novel framework that corrects segmentation results by leveraging consensus from multiple diffusion model outputs. Our approach employs a conditional Bernoulli diffusion model trained on diverse annotation patterns per image to generate multiple plausible segmentations, followed by a consensus-driven correction that incorporates position proximity, consensus level, and gradient direction similarity to correct challenging regions. Experimental results demonstrate that our method outperforms existing methods, significantly improving recall in ambiguous regions while preserving the continuity of thin structures. Furthermore, our method automates the manual process of segmentation result correction and can be applied to image-guided surgical planning and surgery.

</details>


### [28] [fastWDM3D: Fast and Accurate 3D Healthy Tissue Inpainting](https://arxiv.org/abs/2507.13146)
*Alicia Durrer,Florentin Bieder,Paul Friedrich,Bjoern Menze,Philippe C. Cattin,Florian Kofler*

Main category: eess.IV

TL;DR: 本文提出了一种快速且高质量的3D健康组织修复方法fastWDM3D，结合了DDPMs和GANs的优势，并通过方差保持噪声调度实现了高效采样。


<details>
  <summary>Details</summary>
Motivation: 健康组织修复在肿瘤生长模型和图像配准中有重要应用，但现有方法采样速度较慢。

Method: 结合DDPMs与GANs，采用方差保持噪声调度，开发了3D小波扩散模型fastWDM3D，无需对抗训练即可实现高效修复。

Result: 在BraTS测试集上，fastWDM3D的SSIM为0.8571，MSE为0.0079，PSNR为22.26，仅需2步采样，耗时1.81秒/图像，速度提升800倍。

Conclusion: fastWDM3D是一种快速且准确的健康组织修复方法，性能优于现有DDPMs。

Abstract: Healthy tissue inpainting has significant applications, including the generation of pseudo-healthy baselines for tumor growth models and the facilitation of image registration. In previous editions of the BraTS Local Synthesis of Healthy Brain Tissue via Inpainting Challenge, denoising diffusion probabilistic models (DDPMs) demonstrated qualitatively convincing results but suffered from low sampling speed. To mitigate this limitation, we adapted a 2D image generation approach, combining DDPMs with generative adversarial networks (GANs) and employing a variance-preserving noise schedule, for the task of 3D inpainting. Our experiments showed that the variance-preserving noise schedule and the selected reconstruction losses can be effectively utilized for high-quality 3D inpainting in a few time steps without requiring adversarial training. We applied our findings to a different architecture, a 3D wavelet diffusion model (WDM3D) that does not include a GAN component. The resulting model, denoted as fastWDM3D, obtained a SSIM of 0.8571, a MSE of 0.0079, and a PSNR of 22.26 on the BraTS inpainting test set. Remarkably, it achieved these scores using only two time steps, completing the 3D inpainting process in 1.81 s per image. When compared to other DDPMs used for healthy brain tissue inpainting, our model is up to 800 x faster while still achieving superior performance metrics. Our proposed method, fastWDM3D, represents a promising approach for fast and accurate healthy tissue inpainting. Our code is available at https://github.com/AliciaDurrer/fastWDM3D.

</details>


### [29] [SpectraLift: Physics-Guided Spectral-Inversion Network for Self-Supervised Hyperspectral Image Super-Resolution](https://arxiv.org/abs/2507.13339)
*Ritik Shah,Marco F. Duarte*

Main category: eess.IV

TL;DR: SpectraLift是一种自监督框架，通过融合低分辨率高光谱图像（LR-HSI）和高分辨率多光谱图像（HR-MSI），无需点扩散函数校准或真实高分辨率高光谱图像（HR-HSI），即可恢复高空间分辨率的高光谱图像。


<details>
  <summary>Details</summary>
Motivation: 高空间分辨率高光谱图像（HSI）在遥感和医学成像中至关重要，但传统方法需要不切实际的校准或真实数据。

Method: SpectraLift使用多光谱图像的光谱响应函数（SRF）训练轻量级MLP网络，通过合成低分辨率多光谱图像（LR-MSI）和LR-HSI作为输入输出，优化光谱重建损失。

Result: SpectraLift在PSNR、SAM、SSIM和RMSE基准测试中优于现有方法，且收敛速度快。

Conclusion: SpectraLift提供了一种高效、自监督的高光谱图像融合方法，适用于实际场景。

Abstract: High-spatial-resolution hyperspectral images (HSI) are essential for applications such as remote sensing and medical imaging, yet HSI sensors inherently trade spatial detail for spectral richness. Fusing high-spatial-resolution multispectral images (HR-MSI) with low-spatial-resolution hyperspectral images (LR-HSI) is a promising route to recover fine spatial structures without sacrificing spectral fidelity. Most state-of-the-art methods for HSI-MSI fusion demand point spread function (PSF) calibration or ground truth high resolution HSI (HR-HSI), both of which are impractical to obtain in real world settings. We present SpectraLift, a fully self-supervised framework that fuses LR-HSI and HR-MSI inputs using only the MSI's Spectral Response Function (SRF). SpectraLift trains a lightweight per-pixel multi-layer perceptron (MLP) network using ($i$)~a synthetic low-spatial-resolution multispectral image (LR-MSI) obtained by applying the SRF to the LR-HSI as input, ($ii$)~the LR-HSI as the output, and ($iii$)~an $\ell_1$ spectral reconstruction loss between the estimated and true LR-HSI as the optimization objective. At inference, SpectraLift uses the trained network to map the HR-MSI pixel-wise into a HR-HSI estimate. SpectraLift converges in minutes, is agnostic to spatial blur and resolution, and outperforms state-of-the-art methods on PSNR, SAM, SSIM, and RMSE benchmarks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [30] [Physically Based Neural LiDAR Resimulation](https://arxiv.org/abs/2507.12489)
*Richard Marcus,Marc Stamminger*

Main category: cs.RO

TL;DR: 本文提出了一种改进的LiDAR模拟方法，通过显式建模传感器特性（如滚动快门、激光功率变化和强度衰减），实现了比现有技术更准确的LiDAR模拟。


<details>
  <summary>Details</summary>
Motivation: 当前LiDAR模拟方法在传感器特定效果（如滚动快门和激光功率变化）方面表现不足，需要更精确的模拟方法。

Method: 显式建模LiDAR传感器特性（如滚动快门、激光功率变化和强度衰减），并与其他先进方法进行定量和定性比较。

Result: 实验表明，该方法在LiDAR模拟准确性上优于现有技术，并展示了高级重模拟能力（如生成相机视角的高分辨率LiDAR扫描）。

Conclusion: 该方法显著提升了LiDAR模拟的准确性，并具备高级重模拟能力，代码和数据集已开源。

Abstract: Methods for Novel View Synthesis (NVS) have recently found traction in the field of LiDAR simulation and large-scale 3D scene reconstruction. While solutions for faster rendering or handling dynamic scenes have been proposed, LiDAR specific effects remain insufficiently addressed. By explicitly modeling sensor characteristics such as rolling shutter, laser power variations, and intensity falloff, our method achieves more accurate LiDAR simulation compared to existing techniques. We demonstrate the effectiveness of our approach through quantitative and qualitative comparisons with state-of-the-art methods, as well as ablation studies that highlight the importance of each sensor model component. Beyond that, we show that our approach exhibits advanced resimulation capabilities, such as generating high resolution LiDAR scans in the camera perspective.   Our code and the resulting dataset are available at https://github.com/richardmarcus/PBNLiDAR.

</details>


### [31] [Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities](https://arxiv.org/abs/2507.13019)
*Liuyi Wang,Xinyuan Xia,Hui Zhao,Hanqing Wang,Tai Wang,Yilun Chen,Chengju Liu,Qijun Chen,Jiangmiao Pang*

Main category: cs.RO

TL;DR: VLN-PE是一个物理真实的视觉与语言导航平台，支持多种机器人类型，揭示了现有方法在物理部署中的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉与语言导航（VLN）方法在物理机器人部署中的理想化假设问题。

Method: 引入VLN-PE平台，评估多种技术管道，包括分类模型、扩散模型和基于大语言模型的路径规划。

Result: 性能显著下降，原因包括机器人观测空间有限、环境光照变化及物理挑战。

Conclusion: VLN-PE为改进跨具身适应性提供了新途径，并启发社区重新思考VLN的局限性。

Abstract: Recent Vision-and-Language Navigation (VLN) advancements are promising, but their idealized assumptions about robot movement and control fail to reflect physically embodied deployment challenges. To bridge this gap, we introduce VLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and wheeled robots. For the first time, we systematically evaluate several ego-centric VLN methods in physical robotic settings across different technical pipelines, including classification models for single-step discrete action prediction, a diffusion model for dense waypoint prediction, and a train-free, map-based large language model (LLM) integrated with path planning. Our results reveal significant performance degradation due to limited robot observation space, environmental lighting variations, and physical challenges like collisions and falls. This also exposes locomotion constraints for legged robots in complex environments. VLN-PE is highly extensible, allowing seamless integration of new scenes beyond MP3D, thereby enabling more comprehensive VLN evaluation. Despite the weak generalization of current models in physical deployment, VLN-PE provides a new pathway for improving cross-embodiment's overall adaptability. We hope our findings and tools inspire the community to rethink VLN limitations and advance robust, practical VLN models. The code is available at https://crystalsixone.github.io/vln_pe.github.io/.

</details>
