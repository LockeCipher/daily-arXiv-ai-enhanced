<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 22]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [FlowDrag: 3D-aware Drag-based Image Editing with Mesh-guided Deformation Vector Flow Fields](https://arxiv.org/abs/2507.08285)
*Gwanhyeong Koo,Sunjae Yoon,Younghwan Lee,Ji Woo Hong,Chang D. Yoo*

Main category: cs.GR

TL;DR: FlowDrag利用几何信息改进拖拽编辑，通过3D网格变形和UNet去噪实现更精确的编辑，并提出了带真实数据的VFD基准。


<details>
  <summary>Details</summary>
Motivation: 当前拖拽编辑方法因仅关注用户定义点而忽略整体几何，导致不一致性和伪影。

Method: 构建3D网格，通过能量函数指导变形，结合UNet去噪实现2D投影。

Result: FlowDrag在VFD Bench和DragBench上优于现有方法。

Conclusion: FlowDrag通过几何信息和真实基准提升了拖拽编辑的准确性和一致性。

Abstract: Drag-based editing allows precise object manipulation through point-based control, offering user convenience. However, current methods often suffer from a geometric inconsistency problem by focusing exclusively on matching user-defined points, neglecting the broader geometry and leading to artifacts or unstable edits. We propose FlowDrag, which leverages geometric information for more accurate and coherent transformations. Our approach constructs a 3D mesh from the image, using an energy function to guide mesh deformation based on user-defined drag points. The resulting mesh displacements are projected into 2D and incorporated into a UNet denoising process, enabling precise handle-to-target point alignment while preserving structural integrity. Additionally, existing drag-editing benchmarks provide no ground truth, making it difficult to assess how accurately the edits match the intended transformations. To address this, we present VFD (VidFrameDrag) benchmark dataset, which provides ground-truth frames using consecutive shots in a video dataset. FlowDrag outperforms existing drag-based editing methods on both VFD Bench and DragBench.

</details>


### [2] [Advancing Multimodal LLMs by Large-Scale 3D Visual Instruction Dataset Generation](https://arxiv.org/abs/2507.08513)
*Liu He,Xiao Zeng,Yizhi Song,Albert Y. C. Chen,Lu Xia,Shashwat Verma,Sankalp Dayal,Min Sun,Cheng-Hao Kuo,Daniel Aliaga*

Main category: cs.GR

TL;DR: 论文提出了一种合成生成方法，用于创建大规模3D视觉指令数据集，以解决多模态大语言模型（MLLMs）在捕捉相机-物体关系上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在相机-物体关系（如物体方向、相机视角和镜头）的准确捕捉上表现不佳，主要因为训练数据的多样性和文本描述不足。

Method: 通过合成生成流水线，利用3D资产、渲染和扩散模型生成逼真图像，并结合大语言模型（LLMs）生成文本提示，构建了Ultimate3D数据集（240K VQAs）。

Result: 在相机-物体关系识别任务上，基于该数据集微调的MLLMs比商业模型平均准确率提高了33.4%。

Conclusion: 提出的方法显著提升了MLLMs的性能，其代码、数据集和基准将为广泛MLLM应用提供支持。

Abstract: Multimodal Large Language Models (MLLMs) struggle with accurately capturing camera-object relations, especially for object orientation, camera viewpoint, and camera shots. This stems from the fact that existing MLLMs are trained on images with limited diverse camera-object relations and corresponding textual descriptions. To address this, we propose a synthetic generation pipeline to create large-scale 3D visual instruction datasets. Our framework takes 3D assets as input and uses rendering and diffusion-based image generation models to create photorealistic images preserving precise camera-object relations. Additionally, large language models (LLMs) are used to generate text prompts for guiding visual instruction tuning and controlling image generation. We create Ultimate3D, a dataset of 240K VQAs with precise camera-object annotations, and corresponding benchmark. MLLMs fine-tuned on our proposed dataset outperform commercial models by a large margin, achieving an average accuracy improvement of 33.4% on camera-object relation recognition tasks. Our code, dataset, and benchmark will contribute to broad MLLM applications.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [3] [Towards Evaluating Robustness of Prompt Adherence in Text to Image Models](https://arxiv.org/abs/2507.08039)
*Sujith Vemishetty,Advitiya Arora,Anupama Sharma*

Main category: cs.CV

TL;DR: 本文提出了一种评估文本到图像模型的新框架，重点关注其对提示的遵循能力，并发现现有模型在生成简单二进制图像时表现不佳。


<details>
  <summary>Details</summary>
Motivation: 多模态LLM和文本到图像模型的研究相对不足，其可靠性和性能评估缺乏系统性，因此需要建立全面的评估框架。

Method: 通过创建新数据集和评估管道，利用gpt-4o生成文本描述作为基准，再通过文本到图像模型生成图像，最后比较描述差异。

Result: 实验表明，模型在生成仅包含两个变化因素的简单二进制图像时表现不佳，且无法遵循输入数据集的分布。

Conclusion: 现有文本到图像模型在遵循提示和生成特定分布图像方面存在局限性，需要进一步改进。

Abstract: The advancements in the domain of LLMs in recent years have surprised many, showcasing their remarkable capabilities and diverse applications. Their potential applications in various real-world scenarios have led to significant research on their reliability and effectiveness. On the other hand, multimodal LLMs and Text-to-Image models have only recently gained prominence, especially when compared to text-only LLMs. Their reliability remains constrained due to insufficient research on assessing their performance and robustness. This paper aims to establish a comprehensive evaluation framework for Text-to-Image models, concentrating particularly on their adherence to prompts. We created a novel dataset that aimed to assess the robustness of these models in generating images that conform to the specified factors of variation in the input text prompts. Our evaluation studies present findings on three variants of Stable Diffusion models: Stable Diffusion 3 Medium, Stable Diffusion 3.5 Large, and Stable Diffusion 3.5 Large Turbo, and two variants of Janus models: Janus Pro 1B and Janus Pro 7B. We introduce a pipeline that leverages text descriptions generated by the gpt-4o model for our ground-truth images, which are then used to generate artificial images by passing these descriptions to the Text-to-Image models. We then pass these generated images again through gpt-4o using the same system prompt and compare the variation between the two descriptions. Our results reveal that these models struggle to create simple binary images with only two factors of variation: a simple geometric shape and its location. We also show, using pre-trained VAEs on our dataset, that they fail to generate images that follow our input dataset distribution.

</details>


### [4] [ConsNoTrainLoRA: Data-driven Weight Initialization of Low-rank Adapters using Constraints](https://arxiv.org/abs/2507.08044)
*Debasmit Das,Hyoungwoo Park,Munawar Hayat,Seokeon Choi,Sungrack Yun,Fatih Porikli*

Main category: cs.CV

TL;DR: 提出了一种名为CNTLoRA的数据驱动权重初始化方法，用于改进LoRA微调的收敛性和性能。该方法通过预训练权重和微调激活向量的闭式估计初始化LoRA权重，无需训练，且支持可变秩矩阵。实验表明其在多个下游任务中优于标准方法。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA权重矩阵随机初始化且固定秩，限制了微调的性能和收敛速度。本文旨在通过数据驱动的初始化方法解决这一问题。

Method: 将LoRA初始化视为域偏移问题，利用预训练和微调激活之间的约束关系，推导出闭式权重估计，并支持可变秩矩阵初始化。

Result: 在图像生成、分类和理解等任务中，CNTLoRA在定量和定性上均优于标准及数据驱动初始化方法。

Conclusion: CNTLoRA提供了一种无需训练的初始化方案，显著提升了LoRA微调的性能和收敛速度，并通过实验验证了其有效性。

Abstract: Foundation models are pre-trained on large-scale datasets and subsequently fine-tuned on small-scale datasets using parameter-efficient fine-tuning (PEFT) techniques like low-rank adapters (LoRA). In most previous works, LoRA weight matrices are randomly initialized with a fixed rank across all attachment points. In this paper, we improve convergence and final performance of LoRA fine-tuning, using our proposed data-driven weight initialization method, ConsNoTrainLoRA (CNTLoRA). We express LoRA initialization as a domain shift problem where we use multiple constraints relating the pre-training and fine-tuning activations. By reformulating these constraints, we obtain a closed-form estimate of LoRA weights that depends on pre-training weights and fine-tuning activation vectors and hence requires no training during initialization. This weight estimate is decomposed to initialize the up and down matrices with proposed flexibility of variable ranks. With the proposed initialization method, we fine-tune on downstream tasks such as image generation, image classification and image understanding. Both quantitative and qualitative results demonstrate that CNTLoRA outperforms standard and data-driven weight initialization methods. Extensive analyses and ablations further elucidate the design choices of our framework, providing an optimal recipe for faster convergence and enhanced performance.

</details>


### [5] [The relative importance of being Gaussian](https://arxiv.org/abs/2507.08059)
*F. Alberto Grünbaum,Tondgi Xu*

Main category: cs.CV

TL;DR: 论文探讨了在非高斯噪声条件下使用扩散模型进行去噪的性能表现，而非修改算法以适应噪声类型。


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型在非高斯噪声（如均匀分布或Beta分布）下的表现，而非针对噪声类型调整算法。

Method: 使用原始算法，但替换高斯噪声为其他类型（如均匀分布、Beta分布或混合高斯噪声），并在小型设备上进行实验。

Result: 实验结果表明算法在非高斯噪声下的性能表现，但未具体说明结果优劣。

Conclusion: 探索不同噪声条件下算法的表现是一个有趣的挑战，未来可在不同情境下进一步验证。

Abstract: The remarkable results for denoising in computer vision using diffusion models given in \cite{SDWMG,HJA,HHG} yield a robust mathematical justification for algorithms based on crucial properties of a sequence of Gaussian independent $N(0,1)$ random variables. In particular the derivations use the fact that a Gaussian distribution is determined by its mean and variance and that the sum of two Gaussians is another Gaussian.   \bigskip   The issue raised in this short note is the following: suppose we use the algorithm without any changes but replace the nature of the noise and use, for instance, uniformly distributed noise or noise with a Beta distribution, or noise which is a random superposition of two Gaussians with very different variances. One could, of course, try to modify the algorithm keeping in mind the nature of the noise, but this is not what we do. Instead we study the performance of the algorithm when used with noise that is very far in nature from the Gaussian case, where it is designed to work well.   Usually these algorithms are implemented on very powerful computers. Our experiments are all carried out on a small laptop and for the smallest possible image size. Exploring how our observations are confirmed or changed when dealing in different situations remains an interesting challenge.

</details>


### [6] [An Object-Based Deep Learning Approach for Building Height Estimation from Single SAR Images](https://arxiv.org/abs/2507.08096)
*Babak Memar,Luigi Russo,Silvia Liberata Ullo,Paolo Gamba*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的自动化方法，利用单张超高分辨率SAR图像估算建筑物高度，并在多大陆数据集上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 准确的建筑物高度估算对城市应用至关重要，而现有方法在跨城市和跨大陆的泛化能力上存在不足。

Method: 采用基于边界框检测和目标回归的深度学习模型，利用单张VHR COSMO-SkyMed图像进行高度估算。

Result: 模型在欧洲城市表现优异（MAE约2.20米），显著优于现有方法，但在亚洲城市因高楼和城市类型差异泛化能力有所下降。

Conclusion: 深度学习在单张VHR SAR数据的跨城市和跨大陆建筑物高度估算中具有显著潜力。

Abstract: Accurate estimation of building heights using very high resolution (VHR) synthetic aperture radar (SAR) imagery is crucial for various urban applications. This paper introduces a Deep Learning (DL)-based methodology for automated building height estimation from single VHR COSMO-SkyMed images: an object-based regression approach based on bounding box detection followed by height estimation. This model was trained and evaluated on a unique multi-continental dataset comprising eight geographically diverse cities across Europe, North and South America, and Asia, employing a cross-validation strategy to explicitly assess out-of-distribution (OOD) generalization. The results demonstrate highly promising performance, particularly on European cities where the model achieves a Mean Absolute Error (MAE) of approximately one building story (2.20 m in Munich), significantly outperforming recent state-of-the-art methods in similar OOD scenarios. Despite the increased variability observed when generalizing to cities in other continents, particularly in Asia with its distinct urban typologies and prevalence of high-rise structures, this study underscores the significant potential of DL for robust cross-city and cross-continental transfer learning in building height estimation from single VHR SAR data.

</details>


### [7] [RegGS: Unposed Sparse Views Gaussian Splatting with 3DGS Registration](https://arxiv.org/abs/2507.08136)
*Chong Cheng,Yu Hu,Sicheng Yu,Beizhen Zhao,Zijian Wang,Hao Wang*

Main category: cs.CV

TL;DR: RegGS提出了一种基于3D高斯注册的框架，用于重建无姿态稀疏视图，通过熵正则化的Sinkhorn算法和联合3DGS注册模块，实现了高保真的局部高斯对齐和高质量新视角合成。


<details>
  <summary>Details</summary>
Motivation: 解决优化型3DGS方法在稀疏视图下因先验知识不足而表现不佳，以及前馈高斯方法因输入格式限制难以扩展更多视图的问题。

Method: 使用熵正则化的Sinkhorn算法计算最优传输Mixture 2-Wasserstein距离，作为GMM对齐度量，并结合光度一致性和深度几何设计联合3DGS注册模块。

Result: 在RE10K和ACID数据集上，RegGS实现了高保真的局部高斯对齐，精确的相机姿态估计和高质量的新视角合成。

Conclusion: RegGS通过全局一致的3D高斯表示，有效解决了稀疏视图重建的挑战，为无姿态场景重建提供了新思路。

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated its potential in reconstructing scenes from unposed images. However, optimization-based 3DGS methods struggle with sparse views due to limited prior knowledge. Meanwhile, feed-forward Gaussian approaches are constrained by input formats, making it challenging to incorporate more input views. To address these challenges, we propose RegGS, a 3D Gaussian registration-based framework for reconstructing unposed sparse views. RegGS aligns local 3D Gaussians generated by a feed-forward network into a globally consistent 3D Gaussian representation. Technically, we implement an entropy-regularized Sinkhorn algorithm to efficiently solve the optimal transport Mixture 2-Wasserstein $(\text{MW}_2)$ distance, which serves as an alignment metric for Gaussian mixture models (GMMs) in $\mathrm{Sim}(3)$ space. Furthermore, we design a joint 3DGS registration module that integrates the $\text{MW}_2$ distance, photometric consistency, and depth geometry. This enables a coarse-to-fine registration process while accurately estimating camera poses and aligning the scene. Experiments on the RE10K and ACID datasets demonstrate that RegGS effectively registers local Gaussians with high fidelity, achieving precise pose estimation and high-quality novel-view synthesis. Project page: https://3dagentworld.github.io/reggs/.

</details>


### [8] [Temporally Consistent Amodal Completion for 3D Human-Object Interaction Reconstruction](https://arxiv.org/abs/2507.08137)
*Hyungjun Doh,Dong In Lee,Seunggeun Chi,Pin-Hao Huang,Kwonjoon Lee,Sangpil Kim,Karthik Ramani*

Main category: cs.CV

TL;DR: 提出了一种从单目视频重建动态人-物交互的新框架，解决了遮挡和时间不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 传统3D重建方法假设静态物体或动态主体完全可见，在遮挡场景下性能下降。

Method: 利用模态补全推断遮挡区域的完整结构，结合时间上下文增强视频序列的一致性，无需模板。

Result: 在单目视频上验证，处理遮挡和时间稳定性优于现有技术。

Conclusion: 该框架显著提升了动态场景中复杂细节的恢复能力。

Abstract: We introduce a novel framework for reconstructing dynamic human-object interactions from monocular video that overcomes challenges associated with occlusions and temporal inconsistencies. Traditional 3D reconstruction methods typically assume static objects or full visibility of dynamic subjects, leading to degraded performance when these assumptions are violated-particularly in scenarios where mutual occlusions occur. To address this, our framework leverages amodal completion to infer the complete structure of partially obscured regions. Unlike conventional approaches that operate on individual frames, our method integrates temporal context, enforcing coherence across video sequences to incrementally refine and stabilize reconstructions. This template-free strategy adapts to varying conditions without relying on predefined models, significantly enhancing the recovery of intricate details in dynamic scenes. We validate our approach using 3D Gaussian Splatting on challenging monocular videos, demonstrating superior precision in handling occlusions and maintaining temporal stability compared to existing techniques.

</details>


### [9] [Adaptive Diffusion Denoised Smoothing : Certified Robustness via Randomized Smoothing with Differentially Private Guided Denoising Diffusion](https://arxiv.org/abs/2507.08163)
*Frederick Shpilevskiy,Saiyue Lyu,Krishnamurthy Dj Dvijotham,Mathias Lécuyer,Pierre-André Noël*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose Adaptive Diffusion Denoised Smoothing, a method for certifying the predictions of a vision model against adversarial examples, while adapting to the input. Our key insight is to reinterpret a guided denoising diffusion model as a long sequence of adaptive Gaussian Differentially Private (GDP) mechanisms refining a pure noise sample into an image. We show that these adaptive mechanisms can be composed through a GDP privacy filter to analyze the end-to-end robustness of the guided denoising process, yielding a provable certification that extends the adaptive randomized smoothing analysis. We demonstrate that our design, under a specific guiding strategy, can improve both certified accuracy and standard accuracy on ImageNet for an $\ell_2$ threat model.

</details>


### [10] [HNOSeg-XS: Extremely Small Hartley Neural Operator for Efficient and Resolution-Robust 3D Image Segmentation](https://arxiv.org/abs/2507.08205)
*Ken C. L. Wong,Hongzhi Wang,Tanveer Syeda-Mahmood*

Main category: cs.CV

TL;DR: 提出了一种基于Hartley变换的HNOSeg-XS架构，用于医学图像分割，具有分辨率鲁棒性、高效性和参数效率。


<details>
  <summary>Details</summary>
Motivation: 解决CNN和Transformer在医学图像分割中因计算成本和内存占用高而导致的输入尺寸限制问题，以及离散训练带来的次优结果。

Method: 通过可学习的偏微分方程建模图像分割，利用Fourier神经算子的零样本超分辨率特性，并用Hartley变换替代Fourier变换。

Result: 在多个数据集上测试，HNOSeg-XS表现出色，参数少于34.7k，推理时间<0.24秒，内存占用<1.8 GiB。

Conclusion: HNOSeg-XS是一种高效、分辨率鲁棒的医学图像分割方法，优于传统CNN和Transformer模型。

Abstract: In medical image segmentation, convolutional neural networks (CNNs) and transformers are dominant. For CNNs, given the local receptive fields of convolutional layers, long-range spatial correlations are captured through consecutive convolutions and pooling. However, as the computational cost and memory footprint can be prohibitively large, 3D models can only afford fewer layers than 2D models with reduced receptive fields and abstract levels. For transformers, although long-range correlations can be captured by multi-head attention, its quadratic complexity with respect to input size is computationally demanding. Therefore, either model may require input size reduction to allow more filters and layers for better segmentation. Nevertheless, given their discrete nature, models trained with patch-wise training or image downsampling may produce suboptimal results when applied on higher resolutions. To address this issue, here we propose the resolution-robust HNOSeg-XS architecture. We model image segmentation by learnable partial differential equations through the Fourier neural operator which has the zero-shot super-resolution property. By replacing the Fourier transform by the Hartley transform and reformulating the problem in the frequency domain, we created the HNOSeg-XS model, which is resolution robust, fast, memory efficient, and extremely parameter efficient. When tested on the BraTS'23, KiTS'23, and MVSeg'23 datasets with a Tesla V100 GPU, HNOSeg-XS showed its superior resolution robustness with fewer than 34.7k model parameters. It also achieved the overall best inference time (< 0.24 s) and memory efficiency (< 1.8 GiB) compared to the tested CNN and transformer models.

</details>


### [11] [M2DAO-Talker: Harmonizing Multi-granular Motion Decoupling and Alternating Optimization for Talking-head Generation](https://arxiv.org/abs/2507.08307)
*Kui Jiang,Shiyu Liu,Junjun Jiang,Xin Yang,Hongxun Yang,Xiaopeng Fan*

Main category: cs.CV

TL;DR: M2DAO-Talker提出了一种统一的音频驱动说话头生成框架，通过多粒度运动解耦和交替优化解决了现有方法的渲染伪影问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D方法在运动建模和内容合成方面存在渲染伪影（如运动模糊、时间抖动和局部穿透），限制了生成视频的质量。

Method: 提出三步框架：视频预处理、运动表示和渲染重建。具体包括2D肖像预处理、多粒度运动解耦策略和交替优化策略。

Result: M2DAO-Talker在多个数据集上表现优异，生成质量提升2.43 dB PSNR，用户评价视频真实感提升0.64，推理速度达150 FPS。

Conclusion: M2DAO-Talker通过统一框架和优化策略显著提升了说话头生成的视觉质量和真实感。

Abstract: Audio-driven talking head generation holds significant potential for film production. While existing 3D methods have advanced motion modeling and content synthesis, they often produce rendering artifacts, such as motion blur, temporal jitter, and local penetration, due to limitations in representing stable, fine-grained motion fields. Through systematic analysis, we reformulate talking head generation into a unified framework comprising three steps: video preprocessing, motion representation, and rendering reconstruction. This framework underpins our proposed M2DAO-Talker, which addresses current limitations via multi-granular motion decoupling and alternating optimization.Specifically, we devise a novel 2D portrait preprocessing pipeline to extract frame-wise deformation control conditions (motion region segmentation masks, and camera parameters) to facilitate motion representation. To ameliorate motion modeling, we elaborate a multi-granular motion decoupling strategy, which independently models non-rigid (oral and facial) and rigid (head) motions for improved reconstruction accuracy.Meanwhile, a motion consistency constraint is developed to ensure head-torso kinematic consistency, thereby mitigating penetration artifacts caused by motion aliasing. In addition, an alternating optimization strategy is designed to iteratively refine facial and oral motion parameters, enabling more realistic video generation.Experiments across multiple datasets show that M2DAO-Talker achieves state-of-the-art performance, with the 2.43 dB PSNR improvement in generation quality and 0.64 gain in user-evaluated video realness versus TalkingGaussian while with 150 FPS inference speed. Our project homepage is https://m2dao-talker.github.io/M2DAO-Talk.github.io

</details>


### [12] [Unsupervised Methods for Video Quality Improvement: A Survey of Restoration and Enhancement Techniques](https://arxiv.org/abs/2507.08375)
*Alexandra Malyugina,Yini Li,Joanne Lin,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: 本文综述了视频修复与增强技术，重点探讨无监督方法，涵盖常见退化类型、传统与深度学习方法、无监督方法分类及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 视频修复与增强对提升视觉质量和下游计算机视觉任务性能至关重要，无监督方法因其无需标注数据而备受关注。

Method: 综述包括常见退化分析、传统与深度学习方法回顾、无监督方法分类（如域转换、自监督信号设计等）及损失函数分类。

Result: 总结了无监督方法的优势与局限，并讨论了合成数据集在客观评估中的作用。

Conclusion: 指出了该领域的关键挑战与未来研究方向，强调无监督方法的潜力。

Abstract: Video restoration and enhancement are critical not only for improving visual quality, but also as essential pre-processing steps to boost the performance of a wide range of downstream computer vision tasks. This survey presents a comprehensive review of video restoration and enhancement techniques with a particular focus on unsupervised approaches. We begin by outlining the most common video degradations and their underlying causes, followed by a review of early conventional and deep learning methods-based, highlighting their strengths and limitations. We then present an in-depth overview of unsupervised methods, categorise by their fundamental approaches, including domain translation, self-supervision signal design and blind spot or noise-based methods. We also provide a categorization of loss functions employed in unsupervised video restoration and enhancement, and discuss the role of paired synthetic datasets in enabling objective evaluation. Finally, we identify key challenges and outline promising directions for future research in this field.

</details>


### [13] [From Enhancement to Understanding: Build a Generalized Bridge for Low-light Vision via Semantically Consistent Unsupervised Fine-tuning](https://arxiv.org/abs/2507.08380)
*Sen Wang,Shao Zeng,Tianjun Gu,Zhizhong Zhang,Ruixin Zhang,Shouhong Ding,Jingyun Zhang,Jun Wang,Xin Tan,Yuan Xie,Lizhuang Ma*

Main category: cs.CV

TL;DR: 论文提出了一种名为GEFU的通用增强与理解框架，通过生成扩散模型和语义一致性无监督微调（SCUF）解决低光视觉中的增强与理解问题，显著提升了图像质量和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法将低光增强与视觉理解分开处理，前者依赖先验限制泛化能力，后者因标注数据稀缺而缺乏扩展性。GEFU旨在构建两者间的通用桥梁。

Method: 利用预训练生成扩散模型优化图像，提出SCUF框架，包括光照感知图像提示和循环注意力适配器，并通过语义一致性约束避免训练中的语义退化。

Result: 实验表明，该方法在图像质量和分类、检测、分割等任务上优于现有方法。

Conclusion: GEFU通过统一增强与理解，显著提升了泛化能力和扩展性，为低光视觉任务提供了新思路。

Abstract: Low-level enhancement and high-level visual understanding in low-light vision have traditionally been treated separately. Low-light enhancement improves image quality for downstream tasks, but existing methods rely on physical or geometric priors, limiting generalization. Evaluation mainly focuses on visual quality rather than downstream performance. Low-light visual understanding, constrained by scarce labeled data, primarily uses task-specific domain adaptation, which lacks scalability. To address these challenges, we build a generalized bridge between low-light enhancement and low-light understanding, which we term Generalized Enhancement For Understanding (GEFU). This paradigm improves both generalization and scalability. To address the diverse causes of low-light degradation, we leverage pretrained generative diffusion models to optimize images, achieving zero-shot generalization performance. Building on this, we propose Semantically Consistent Unsupervised Fine-tuning (SCUF). Specifically, to overcome text prompt limitations, we introduce an illumination-aware image prompt to explicitly guide image generation and propose a cycle-attention adapter to maximize its semantic potential. To mitigate semantic degradation in unsupervised training, we propose caption and reflectance consistency to learn high-level semantics and image-level spatial semantics. Extensive experiments demonstrate that our proposed method outperforms current state-of-the-art methods in traditional image quality and GEFU tasks including classification, detection, and semantic segmentation.

</details>


### [14] [Subject-Consistent and Pose-Diverse Text-to-Image Generation](https://arxiv.org/abs/2507.08396)
*Zhanxin Gao,Beier Zhu,Liang Yao,Jian Yang,Ying Tai*

Main category: cs.CV

TL;DR: CoDi是一个文本到图像（T2I）框架，通过两阶段策略（身份传输和身份细化）实现主题一致性和姿势多样性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在保持主题一致性时会牺牲布局和姿势多样性，限制了视觉叙事的表达。

Method: CoDi采用两阶段策略：身份传输（IT）和身份细化（IR）。IT在早期去噪步骤中通过最优传输传递身份特征，IR在后期细化身份特征。

Result: 实验表明，CoDi在主题一致性、姿势多样性和提示保真度方面均表现优异。

Conclusion: CoDi在视觉感知和性能上均优于现有方法，代码已开源。

Abstract: Subject-consistent generation (SCG)-aiming to maintain a consistent subject identity across diverse scenes-remains a challenge for text-to-image (T2I) models. Existing training-free SCG methods often achieve consistency at the cost of layout and pose diversity, hindering expressive visual storytelling. To address the limitation, we propose subject-Consistent and pose-Diverse T2I framework, dubbed as CoDi, that enables consistent subject generation with diverse pose and layout. Motivated by the progressive nature of diffusion, where coarse structures emerge early and fine details are refined later, CoDi adopts a two-stage strategy: Identity Transport (IT) and Identity Refinement (IR). IT operates in the early denoising steps, using optimal transport to transfer identity features to each target image in a pose-aware manner. This promotes subject consistency while preserving pose diversity. IR is applied in the later denoising steps, selecting the most salient identity features to further refine subject details. Extensive qualitative and quantitative results on subject consistency, pose diversity, and prompt fidelity demonstrate that CoDi achieves both better visual perception and stronger performance across all metrics. The code is provided in https://github.com/NJU-PCALab/CoDi.

</details>


### [15] [Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated Diffusion Transformers](https://arxiv.org/abs/2507.08422)
*Wongi Jeong,Kyungryeol Lee,Hoigi Seo,Se Young Chun*

Main category: cs.CV

TL;DR: 提出了一种名为RALU的训练免费框架，通过空间维度加速扩散变换器的推理，显著减少计算量，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器虽然在高保真图像和视频生成中表现出色，但计算量大，阻碍实际应用。现有方法主要利用时间维度加速，而RALU则专注于空间维度优化。

Method: RALU采用三阶段混合分辨率采样：1）低分辨率去噪捕捉全局语义结构，2）对易产生伪影的区域进行区域自适应上采样，3）全分辨率上采样细化细节。通过噪声时间步重调度稳定分辨率转换。

Result: 在FLUX上实现7.0倍加速，Stable Diffusion 3上实现3.0倍加速，且图像质量损失最小。

Conclusion: RALU是一种高效的空间维度加速方法，可与现有时间维度加速技术互补，进一步降低推理延迟而不影响生成质量。

Abstract: Diffusion transformers have emerged as an alternative to U-net-based diffusion models for high-fidelity image and video generation, offering superior scalability. However, their heavy computation remains a major obstacle to real-world deployment. Existing acceleration methods primarily exploit the temporal dimension such as reusing cached features across diffusion timesteps. Here, we propose Region-Adaptive Latent Upsampling (RALU), a training-free framework that accelerates inference along spatial dimension. RALU performs mixed-resolution sampling across three stages: 1) low-resolution denoising latent diffusion to efficiently capture global semantic structure, 2) region-adaptive upsampling on specific regions prone to artifacts at full-resolution, and 3) all latent upsampling at full-resolution for detail refinement. To stabilize generations across resolution transitions, we leverage noise-timestep rescheduling to adapt the noise level across varying resolutions. Our method significantly reduces computation while preserving image quality by achieving up to 7.0$\times$ speed-up on FLUX and 3.0$\times$ on Stable Diffusion 3 with minimal degradation. Furthermore, RALU is complementary to existing temporal accelerations such as caching methods, thus can be seamlessly integrated to further reduce inference latency without compromising generation quality.

</details>


### [16] [RePaintGS: Reference-Guided Gaussian Splatting for Realistic and View-Consistent 3D Scene Inpainting](https://arxiv.org/abs/2507.08434)
*Ji Hyun Seo,Byounhyun Yoo,Gerard Jounghyun Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D场景修复方法，通过利用参考视图生成几何和外观一致的修复结果。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景修复方法因视角间不一致性导致修复结果不自然，需要一种能保持几何和外观一致性的新方法。

Method: 利用参考视图估计其他视图的修复相似性，调整其贡献以构建准确几何，并通过参考视图引导优化。

Result: 实验表明，该方法在几何保真度和外观一致性上优于现有方法。

Conclusion: 该方法通过参考视图有效解决了3D场景修复中的一致性问题，适用于复杂场景。

Abstract: Radiance field methods, such as Neural Radiance Field or 3D Gaussian Splatting, have emerged as seminal 3D representations for synthesizing realistic novel views. For practical applications, there is ongoing research on flexible scene editing techniques, among which object removal is a representative task. However, removing objects exposes occluded regions, often leading to unnatural appearances. Thus, studies have employed image inpainting techniques to replace such regions with plausible content - a task referred to as 3D scene inpainting. However, image inpainting methods produce one of many plausible completions for each view, leading to inconsistencies between viewpoints. A widely adopted approach leverages perceptual cues to blend inpainted views smoothly. However, it is prone to detail loss and can fail when there are perceptual inconsistencies across views. In this paper, we propose a novel 3D scene inpainting method that reliably produces realistic and perceptually consistent results even for complex scenes by leveraging a reference view. Given the inpainted reference view, we estimate the inpainting similarity of the other views to adjust their contribution in constructing an accurate geometry tailored to the reference. This geometry is then used to warp the reference inpainting to other views as pseudo-ground truth, guiding the optimization to match the reference appearance. Comparative evaluation studies have shown that our approach improves both the geometric fidelity and appearance consistency of inpainted scenes.

</details>


### [17] [Vision Foundation Models as Effective Visual Tokenizers for Autoregressive Image Generation](https://arxiv.org/abs/2507.08441)
*Anlin Zheng,Xin Wen,Xuanyang Zhang,Chuofan Ma,Tiancai Wang,Gang Yu,Xiangyu Zhang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 论文提出了一种基于预训练视觉基础模型的图像分词器VFMTok，通过区域自适应量化框架和语义重建目标，显著提升了图像重建与生成质量。


<details>
  <summary>Details</summary>
Motivation: 探索利用预训练视觉基础模型构建图像分词器，填补该领域的空白。

Method: 使用冻结的视觉基础模型作为编码器，引入区域自适应量化框架和语义重建目标。

Result: VFMTok在图像重建和生成质量上取得显著提升，gFID达2.07，加速模型收敛三倍，无需分类器自由指导即可实现高保真类别条件合成。

Conclusion: VFMTok通过创新设计在图像生成任务中表现出色，代码将开源以促进社区发展。

Abstract: Leveraging the powerful representations of pre-trained vision foundation models -- traditionally used for visual comprehension -- we explore a novel direction: building an image tokenizer directly atop such models, a largely underexplored area. Specifically, we employ a frozen vision foundation model as the encoder of our tokenizer. To enhance its effectiveness, we introduce two key components: (1) a region-adaptive quantization framework that reduces redundancy in the pre-trained features on regular 2D grids, and (2) a semantic reconstruction objective that aligns the tokenizer's outputs with the foundation model's representations to preserve semantic fidelity. Based on these designs, our proposed image tokenizer, VFMTok, achieves substantial improvements in image reconstruction and generation quality, while also enhancing token efficiency. It further boosts autoregressive (AR) generation -- achieving a gFID of 2.07 on ImageNet benchmarks, while accelerating model convergence by three times, and enabling high-fidelity class-conditional synthesis without the need for classifier-free guidance (CFG). The code will be released publicly to benefit the community.

</details>


### [18] [SAM2RL: Towards Reinforcement Learning Memory Control in Segment Anything Model 2](https://arxiv.org/abs/2507.08548)
*Alen Adamyan,Tomáš Čížek,Matej Straka,Klara Janouskova,Martin Schmid*

Main category: cs.CV

TL;DR: SAM 2在目标分割任务中表现优异，成为视觉目标跟踪的SOTA。本文提出用强化学习优化其内存更新，显著超越现有启发式方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手工规则更新内存，难以处理干扰、遮挡和运动。本文探索强化学习作为替代方案，以优化内存控制。

Method: 将内存控制建模为序列决策问题，使用强化学习优化SAM 2的内存更新策略。

Result: 在过拟合实验中，相对SAM 2的改进超过现有启发式方法的三倍。

Conclusion: 强化学习是优化内存控制的有效替代方案，揭示了内存银行的未开发潜力。

Abstract: Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks and has become the state-of-the-art for visual object tracking. The model stores information from previous frames in a memory bank, enabling temporal consistency across video sequences. Recent methods augment SAM 2 with hand-crafted update rules to better handle distractors, occlusions, and object motion. We propose a fundamentally different approach using reinforcement learning for optimizing memory updates in SAM 2 by framing memory control as a sequential decision-making problem. In an overfitting setup with a separate agent per video, our method achieves a relative improvement over SAM 2 that exceeds by more than three times the gains of existing heuristics. These results reveal the untapped potential of the memory bank and highlight reinforcement learning as a powerful alternative to hand-crafted update rules for memory control in visual object tracking.

</details>


### [19] [Image Translation with Kernel Prediction Networks for Semantic Segmentation](https://arxiv.org/abs/2507.08554)
*Cristina Mata,Michael S. Ryoo,Henrik Turbell*

Main category: cs.CV

TL;DR: DA-KPN是一种新的图像翻译方法，通过轻量级翻译函数和语义匹配保证，在低数据量下提升语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有GAN方法在语义分割中无法保证语义匹配的问题，特别是在低数据量下。

Method: 使用Domain Adversarial Kernel Prediction Network（DA-KPN），通过轻量级翻译函数和多尺度判别器实现像素级语义匹配。

Result: 在syn2real基准测试中优于基于GAN的方法，并在人脸解析任务中表现相当。

Conclusion: DA-KPN在低数据量下有效提升语义分割性能，并保证语义匹配。

Abstract: Semantic segmentation relies on many dense pixel-wise annotations to achieve the best performance, but owing to the difficulty of obtaining accurate annotations for real world data, practitioners train on large-scale synthetic datasets. Unpaired image translation is one method used to address the ensuing domain gap by generating more realistic training data in low-data regimes. Current methods for unpaired image translation train generative adversarial networks (GANs) to perform the translation and enforce pixel-level semantic matching through cycle consistency. These methods do not guarantee that the semantic matching holds, posing a problem for semantic segmentation where performance is sensitive to noisy pixel labels. We propose a novel image translation method, Domain Adversarial Kernel Prediction Network (DA-KPN), that guarantees semantic matching between the synthetic label and translation. DA-KPN estimates pixel-wise input transformation parameters of a lightweight and simple translation function. To ensure the pixel-wise transformation is realistic, DA-KPN uses multi-scale discriminators to distinguish between translated and target samples. We show DA-KPN outperforms previous GAN-based methods on syn2real benchmarks for semantic segmentation with limited access to real image labels and achieves comparable performance on face parsing.

</details>


### [20] [OnlineBEV: Recurrent Temporal Fusion in Bird's Eye View Representations for Multi-Camera 3D Perception](https://arxiv.org/abs/2507.08644)
*Junho Koh,Youngwoo Lee,Jungho Kim,Dongyoung Lee,Jun Won Choi*

Main category: cs.CV

TL;DR: OnlineBEV提出了一种新的时序3D感知方法，通过循环结构结合BEV特征，利用MBFNet实现时序特征对齐，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 尽管时序BEV特征结合能提升3D感知性能，但动态物体运动导致特征变化限制了性能提升。OnlineBEV旨在解决这一问题。

Method: 采用循环结构结合BEV特征，使用MBFNet提取运动特征并动态对齐历史与当前特征，通过时序一致性学习损失显式对齐特征。

Result: 在nuScenes测试集上达到63.9% NDS，性能优于当前最佳方法SOLOFusion。

Conclusion: OnlineBEV通过时序特征对齐显著提升了3D感知性能，成为相机仅3D目标检测任务的SOTA方法。

Abstract: Multi-view camera-based 3D perception can be conducted using bird's eye view (BEV) features obtained through perspective view-to-BEV transformations. Several studies have shown that the performance of these 3D perception methods can be further enhanced by combining sequential BEV features obtained from multiple camera frames. However, even after compensating for the ego-motion of an autonomous agent, the performance gain from temporal aggregation is limited when combining a large number of image frames. This limitation arises due to dynamic changes in BEV features over time caused by object motion. In this paper, we introduce a novel temporal 3D perception method called OnlineBEV, which combines BEV features over time using a recurrent structure. This structure increases the effective number of combined features with minimal memory usage. However, it is critical to spatially align the features over time to maintain strong performance. OnlineBEV employs the Motion-guided BEV Fusion Network (MBFNet) to achieve temporal feature alignment. MBFNet extracts motion features from consecutive BEV frames and dynamically aligns historical BEV features with current ones using these motion features. To enforce temporal feature alignment explicitly, we use Temporal Consistency Learning Loss, which captures discrepancies between historical and target BEV features. Experiments conducted on the nuScenes benchmark demonstrate that OnlineBEV achieves significant performance gains over the current best method, SOLOFusion. OnlineBEV achieves 63.9% NDS on the nuScenes test set, recording state-of-the-art performance in the camera-only 3D object detection task.

</details>


### [21] [L-CLIPScore: a Lightweight Embedding-based Captioning Metric for Evaluating and Training](https://arxiv.org/abs/2507.08710)
*Li Li,Yingzhe Peng,Xu Yang,Ruoxi Cheng,Haiyang Xu,Ming Yan,Fei Huang*

Main category: cs.CV

TL;DR: 提出了一种基于轻量级CLIP（L-CLIP）的嵌入式字幕评估指标L-CLIPScore，用于高效评估字幕质量和训练字幕模型。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP模型计算资源需求高，需要一种轻量化的替代方案。

Method: 通过权重复用和矩阵分解压缩模型参数，设计多模态相似性调节器（SR）损失进行知识蒸馏。

Result: L-CLIP在保持多模态对齐能力的同时，显著减少计算资源和运行时间。

Conclusion: L-CLIPScore作为评估指标高效有效，但需与n-gram指标混合使用以避免训练失败。

Abstract: We propose a novel embedding-based captioning metric termed as L-CLIPScore that can be used for efficiently evaluating caption quality and training captioning model. L-CLIPScore is calculated from a lightweight CLIP (L-CLIP), which is a dual-encoder architecture compressed and distilled from CLIP. To compress, we apply two powerful techniques which are weight multiplexing and matrix decomposition for reducing the parameters of encoders and word embedding matrix, respectively. To distill, we design a novel multi-modal Similarity Regulator (SR) loss to transfer more vision-language alignment knowledge. Specifically, SR loss amplifies the multi-modal embedding similarity if the given image-text pair is matched and diminishes the similarity if the pair is non-matched. By compressing and distilling by this novel SR loss, our L-CLIP achieves comparable multi-modal alignment ability to the original CLIP while it requires fewer computation resources and running time. We carry out exhaustive experiments to validate the efficiency and effectiveness of L-CLIPScore when using it as the judge to evaluate caption quality. We also discover that when using L-CLIPScore as the supervisor to train the captioning model, it should be mixed up by an n-gram-based metric and meanwhile analyze why using L-CLIPScore only will cause fail training.

</details>


### [22] [RoundaboutHD: High-Resolution Real-World Urban Environment Benchmark for Multi-Camera Vehicle Tracking](https://arxiv.org/abs/2507.08729)
*Yuqiang Lin,Sam Lockyer,Mingxuan Sui,Li Gan,Florian Stanek,Markus Zarbock,Wenbin Li,Adrian Evans,Nic Zhang*

Main category: cs.CV

TL;DR: 介绍了RoundaboutHD，一个高分辨率多摄像头车辆跟踪数据集，填补了现有数据集的不足，支持多种任务。


<details>
  <summary>Details</summary>
Motivation: 现有公开数据集在场景复杂性、分辨率和多样性方面不足，无法满足真实世界需求。

Method: 通过四个非重叠高分辨率摄像头采集40分钟视频数据，标注512辆车的身份和跨摄像头关联数据。

Result: 提供了车辆检测、单摄像头跟踪、车辆重识别和多摄像头跟踪的基线结果。

Conclusion: RoundaboutHD为多摄像头车辆跟踪研究提供了更接近真实场景的数据和挑战。

Abstract: The multi-camera vehicle tracking (MCVT) framework holds significant potential for smart city applications, including anomaly detection, traffic density estimation, and suspect vehicle tracking. However, current publicly available datasets exhibit limitations, such as overly simplistic scenarios, low-resolution footage, and insufficiently diverse conditions, creating a considerable gap between academic research and real-world scenario. To fill this gap, we introduce RoundaboutHD, a comprehensive, high-resolution multi-camera vehicle tracking benchmark dataset specifically designed to represent real-world roundabout scenarios. RoundaboutHD provides a total of 40 minutes of labelled video footage captured by four non-overlapping, high-resolution (4K resolution, 15 fps) cameras. In total, 512 unique vehicle identities are annotated across different camera views, offering rich cross-camera association data. RoundaboutHD offers temporal consistency video footage and enhanced challenges, including increased occlusions and nonlinear movement inside the roundabout. In addition to the full MCVT dataset, several subsets are also available for object detection, single camera tracking, and image-based vehicle re-identification (ReID) tasks. Vehicle model information and camera modelling/ geometry information are also included to support further analysis. We provide baseline results for vehicle detection, single-camera tracking, image-based vehicle re-identification, and multi-camera tracking. The dataset and the evaluation code are publicly available at: https://github.com/siri-rouser/RoundaboutHD.git

</details>


### [23] [From One to More: Contextual Part Latents for 3D Generation](https://arxiv.org/abs/2507.08772)
*Shaocong Dong,Lihe Ding,Xiao Chen,Yaokun Li,Yuxin Wang,Yucheng Wang,Qi Wang,Jaehyeok Kim,Chenjian Gao,Zhanpeng Huang,Zibin Wang,Tianfan Xue,Dan Xu*

Main category: cs.CV

TL;DR: CoPart提出了一种基于部分感知的扩散框架，通过分解3D对象为上下文部分潜在表示，解决了现有3D生成方法在细节、部分独立性和可控性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成方法在复杂多部分几何、部分独立性和细粒度控制方面存在不足，CoPart受人类3D设计工作流程启发，旨在解决这些问题。

Method: CoPart采用部分感知扩散框架，分解3D对象为部分潜在表示，并开发了互指导策略联合去噪，同时构建了Partverse数据集支持训练。

Result: 实验表明，CoPart在部分级别编辑、关节对象生成和场景组合方面表现出色，具有前所未有的可控性。

Conclusion: CoPart通过部分分解和关系建模，显著提升了3D生成的细节和可控性，为未来研究提供了新方向。

Abstract: Recent advances in 3D generation have transitioned from multi-view 2D rendering approaches to 3D-native latent diffusion frameworks that exploit geometric priors in ground truth data. Despite progress, three key limitations persist: (1) Single-latent representations fail to capture complex multi-part geometries, causing detail degradation; (2) Holistic latent coding neglects part independence and interrelationships critical for compositional design; (3) Global conditioning mechanisms lack fine-grained controllability. Inspired by human 3D design workflows, we propose CoPart - a part-aware diffusion framework that decomposes 3D objects into contextual part latents for coherent multi-part generation. This paradigm offers three advantages: i) Reduces encoding complexity through part decomposition; ii) Enables explicit part relationship modeling; iii) Supports part-level conditioning. We further develop a mutual guidance strategy to fine-tune pre-trained diffusion models for joint part latent denoising, ensuring both geometric coherence and foundation model priors. To enable large-scale training, we construct Partverse - a novel 3D part dataset derived from Objaverse through automated mesh segmentation and human-verified annotations. Extensive experiments demonstrate CoPart's superior capabilities in part-level editing, articulated object generation, and scene composition with unprecedented controllability.

</details>


### [24] [Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective](https://arxiv.org/abs/2507.08801)
*Hangjie Yuan,Weihua Chen,Jun Cen,Hu Yu,Jingyun Liang,Shuning Chang,Zhihui Lin,Tao Feng,Pengwei Liu,Jiazheng Xing,Hao Luo,Jiasheng Tang,Fan Wang,Yi Yang*

Main category: cs.CV

TL;DR: Lumos-1是一种基于LLM架构的自回归视频生成器，通过改进的3D RoPE和AR-DF策略解决了现有方法的延迟和架构不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有自回归视频生成器要么偏离标准LLM架构，要么依赖笨重的外部文本编码器，或存在高延迟问题。

Method: 提出MM-RoPE方案，结合3D RoPE和文本RoPE，并引入AR-DF策略解决帧间损失不平衡问题。

Result: 在48个GPU上预训练，性能与EMU3、COSMOS-Video2World和OpenSoraPlan相当。

Conclusion: Lumos-1通过最小架构修改和高效训练技术，实现了高性能的视频生成。

Abstract: Autoregressive large language models (LLMs) have unified a vast range of language tasks, inspiring preliminary efforts in autoregressive video generation. Existing autoregressive video generators either diverge from standard LLM architectures, depend on bulky external text encoders, or incur prohibitive latency due to next-token decoding. In this paper, we introduce Lumos-1, an autoregressive video generator that retains the LLM architecture with minimal architectural modifications. To inject spatiotemporal correlations in LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its imbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE scheme that preserves the original textual RoPE while providing comprehensive frequency spectra and scaled 3D positions for modeling multimodal spatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy that obeys intra-frame bidirectionality and inter-frame temporal causality. Based on this dependency strategy, we identify the issue of frame-wise loss imbalance caused by spatial information redundancy and solve it by proposing Autoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal tube masking during training with a compatible inference-time masking policy to avoid quality degradation. By using memory-efficient training techniques, we pre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on GenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code and models are available at https://github.com/alibaba-damo-academy/Lumos.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [25] [Learning human-to-robot handovers through 3D scene reconstruction](https://arxiv.org/abs/2507.08726)
*Yuekun Wu,Yik Lung Pang,Andrea Cavallaro,Changjae Oh*

Main category: cs.RO

TL;DR: 提出了一种仅从RGB图像学习机器人交接任务的方法，无需真实机器人训练或数据收集，利用稀疏视图高斯泼溅重建场景生成机器人演示。


<details>
  <summary>Details</summary>
Motivation: 解决从模拟到真实环境的视觉域差距问题，减少对真实机器人训练和数据收集的依赖。

Method: 使用稀疏视图高斯泼溅重建人类到机器人交接场景，生成图像-动作对演示，并直接训练机器人策略。

Result: 在16种家庭物品上测试，策略可直接部署到真实环境，实验证明其有效性。

Conclusion: H2RH-SGS为人类到机器人交接任务提供了新的有效表示方法。

Abstract: Learning robot manipulation policies from raw, real-world image data requires a large number of robot-action trials in the physical environment. Although training using simulations offers a cost-effective alternative, the visual domain gap between simulation and robot workspace remains a major limitation. Gaussian Splatting visual reconstruction methods have recently provided new directions for robot manipulation by generating realistic environments. In this paper, we propose the first method for learning supervised-based robot handovers solely from RGB images without the need of real-robot training or real-robot data collection. The proposed policy learner, Human-to-Robot Handover using Sparse-View Gaussian Splatting (H2RH-SGS), leverages sparse-view Gaussian Splatting reconstruction of human-to-robot handover scenes to generate robot demonstrations containing image-action pairs captured with a camera mounted on the robot gripper. As a result, the simulated camera pose changes in the reconstructed scene can be directly translated into gripper pose changes. We train a robot policy on demonstrations collected with 16 household objects and {\em directly} deploy this policy in the real environment. Experiments in both Gaussian Splatting reconstructed scene and real-world human-to-robot handover experiments demonstrate that H2RH-SGS serves as a new and effective representation for the human-to-robot handover task.

</details>
