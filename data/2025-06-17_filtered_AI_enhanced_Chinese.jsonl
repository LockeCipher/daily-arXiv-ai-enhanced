{"id": "2506.12348", "pdf": "https://arxiv.org/pdf/2506.12348", "abs": "https://arxiv.org/abs/2506.12348", "authors": ["Zaiqiang Wu", "I-Chao Shen", "Takeo Igarashi"], "title": "Real-Time Per-Garment Virtual Try-On with Temporal Consistency for Loose-Fitting Garments", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Per-garment virtual try-on methods collect garment-specific datasets and train networks tailored to each garment to achieve superior results. However, these approaches often struggle with loose-fitting garments due to two key limitations: (1) They rely on human body semantic maps to align garments with the body, but these maps become unreliable when body contours are obscured by loose-fitting garments, resulting in degraded outcomes; (2) They train garment synthesis networks on a per-frame basis without utilizing temporal information, leading to noticeable jittering artifacts. To address these challenges, we propose a two-stage approach for robust semantic map estimation. First, we extract a garment-invariant representation from the raw input image. This representation is then passed through an auxiliary network to estimate the semantic map. This enhances the robustness of semantic map estimation under loose-fitting garments during garment-specific dataset generation. Furthermore, we introduce a recurrent garment synthesis framework that incorporates temporal dependencies to improve frame-to-frame coherence while maintaining real-time performance. We conducted qualitative and quantitative evaluations to demonstrate that our method outperforms existing approaches in both image quality and temporal coherence. Ablation studies further validate the effectiveness of the garment-invariant representation and the recurrent synthesis framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u670d\u88c5\u4e0d\u53d8\u8868\u793a\u548c\u6539\u8fdb\u8bed\u4e49\u56fe\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u5bbd\u677e\u670d\u88c5\u865a\u62df\u8bd5\u7a7f\u4e2d\u7684\u8bed\u4e49\u56fe\u4e0d\u53ef\u9760\u548c\u5e27\u95f4\u6296\u52a8\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5bbd\u677e\u670d\u88c5\u8bd5\u7a7f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u8bed\u4e49\u56fe\u4f30\u8ba1\u4e0d\u53ef\u9760\u548c\u7f3a\u4e4f\u65f6\u95f4\u4fe1\u606f\u5bfc\u81f4\u6296\u52a8\u3002", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u63d0\u53d6\u670d\u88c5\u4e0d\u53d8\u8868\u793a\uff0c\u901a\u8fc7\u8f85\u52a9\u7f51\u7edc\u4f30\u8ba1\u8bed\u4e49\u56fe\uff1b\u5f15\u5165\u5faa\u73af\u5408\u6210\u6846\u67b6\u5229\u7528\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "result": "\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5bbd\u677e\u670d\u88c5\u865a\u62df\u8bd5\u7a7f\u7684\u6548\u679c\uff0c\u89e3\u51b3\u4e86\u8bed\u4e49\u56fe\u4e0d\u53ef\u9760\u548c\u5e27\u95f4\u6296\u52a8\u95ee\u9898\u3002"}}
{"id": "2506.12847", "pdf": "https://arxiv.org/pdf/2506.12847", "abs": "https://arxiv.org/abs/2506.12847", "authors": ["Zhelun Shen", "Chenming Wu", "Junsheng Zhou", "Chen Zhao", "Kaisiyuan Wang", "Hang Zhou", "Yingying Li", "Haocheng Feng", "Wei He", "Jingdong Wang"], "title": "iDiT-HOI: Inpainting-based Hand Object Interaction Reenactment via Video Diffusion Transformer", "categories": ["cs.GR", "cs.CV"], "comment": "Technical report, 12 pages", "summary": "Digital human video generation is gaining traction in fields like education and e-commerce, driven by advancements in head-body animation and lip-syncing technologies. However, realistic Hand-Object Interaction (HOI) - the complex dynamics between human hands and objects - continues to pose challenges. Generating natural and believable HOI reenactments is difficult due to issues such as occlusion between hands and objects, variations in object shapes and orientations, and the necessity for precise physical interactions, and importantly, the ability to generalize to unseen humans and objects. This paper presents a novel framework iDiT-HOI that enables in-the-wild HOI reenactment generation. Specifically, we propose a unified inpainting-based token process method, called Inp-TPU, with a two-stage video diffusion transformer (DiT) model. The first stage generates a key frame by inserting the designated object into the hand region, providing a reference for subsequent frames. The second stage ensures temporal coherence and fluidity in hand-object interactions. The key contribution of our method is to reuse the pretrained model's context perception capabilities without introducing additional parameters, enabling strong generalization to unseen objects and scenarios, and our proposed paradigm naturally supports long video generation. Comprehensive evaluations demonstrate that our approach outperforms existing methods, particularly in challenging real-world scenes, offering enhanced realism and more seamless hand-object interactions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aiDiT-HOI\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u91ce\u5916\u624b-\u7269\u4ea4\u4e92\uff08HOI\uff09\u91cd\u6f14\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u89c6\u9891\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u6a21\u578b\u548c\u7edf\u4e00\u7684\u57fa\u4e8e\u4fee\u590d\u7684\u6807\u8bb0\u5904\u7406\u65b9\u6cd5\uff08Inp-TPU\uff09\uff0c\u5b9e\u73b0\u4e86\u5bf9\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u548c\u573a\u666f\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u624b-\u7269\u4ea4\u4e92\uff08HOI\uff09\u7684\u590d\u6742\u6027\uff08\u5982\u906e\u6321\u3001\u7269\u4f53\u5f62\u72b6\u53d8\u5316\u548c\u7cbe\u786e\u7269\u7406\u4ea4\u4e92\u9700\u6c42\uff09\u4f7f\u5f97\u751f\u6210\u81ea\u7136\u4e14\u53ef\u4fe1\u7684HOI\u91cd\u6f14\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faiDiT-HOI\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5DiT\u6a21\u578b\uff1a\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u5173\u952e\u5e27\uff0c\u5c06\u6307\u5b9a\u7269\u4f53\u63d2\u5165\u624b\u90e8\u533a\u57df\uff1b\u7b2c\u4e8c\u9636\u6bb5\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u6d41\u7545\u6027\u3002\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u771f\u5b9e\u611f\u548c\u66f4\u6d41\u7545\u7684\u624b-\u7269\u4ea4\u4e92\u3002", "conclusion": "iDiT-HOI\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684Inp-TPU\u65b9\u6cd5\u548c\u4e24\u9636\u6bb5DiT\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86HOI\u91cd\u6f14\u7684\u6311\u6218\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u957f\u89c6\u9891\u751f\u6210\u652f\u6301\u3002"}}
{"id": "2506.13348", "pdf": "https://arxiv.org/pdf/2506.13348", "abs": "https://arxiv.org/abs/2506.13348", "authors": ["Mae Younes", "Adnane Boukhayma"], "title": "TextureSplat: Per-Primitive Texture Mapping for Reflective Gaussian Splatting", "categories": ["cs.GR", "cs.CV"], "comment": "Code will be available at https://github.com/maeyounes/TextureSplat", "summary": "Gaussian Splatting have demonstrated remarkable novel view synthesis performance at high rendering frame rates. Optimization-based inverse rendering within complex capture scenarios remains however a challenging problem. A particular case is modelling complex surface light interactions for highly reflective scenes, which results in intricate high frequency specular radiance components. We hypothesize that such challenging settings can benefit from increased representation power. We hence propose a method that tackles this issue through a geometrically and physically grounded Gaussian Splatting borne radiance field, where normals and material properties are spatially variable in the primitive's local space. Using per-primitive texture maps for this purpose, we also propose to harness the GPU hardware to accelerate rendering at test time via unified material texture atlas.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u7684\u8f90\u5c04\u573a\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u9ad8\u53cd\u5c04\u573a\u666f\u4e2d\u590d\u6742\u8868\u9762\u5149\u4ea4\u4e92\u7684\u5efa\u6a21\u95ee\u9898\u3002", "motivation": "\u9ad8\u53cd\u5c04\u573a\u666f\u4e2d\u7684\u590d\u6742\u5149\u4ea4\u4e92\u5efa\u6a21\u662f\u4f18\u5316\u9006\u6e32\u67d3\u7684\u6311\u6218\u6027\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5f3a\u7684\u8868\u793a\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u51e0\u4f55\u548c\u7269\u7406\u57fa\u7840\u7684\u9ad8\u65af\u6cfc\u6e85\u8f90\u5c04\u573a\uff0c\u5229\u7528\u5c40\u90e8\u7a7a\u95f4\u4e2d\u7684\u6cd5\u7ebf\u548c\u6750\u8d28\u5c5e\u6027\u53d8\u5316\uff0c\u5e76\u7ed3\u5408GPU\u786c\u4ef6\u52a0\u901f\u6e32\u67d3\u3002", "result": "\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5efa\u6a21\u9ad8\u9891\u955c\u9762\u8f90\u5c04\u5206\u91cf\uff0c\u5e76\u5b9e\u73b0\u5b9e\u65f6\u6e32\u67d3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u53cd\u5c04\u573a\u666f\u7684\u9006\u6e32\u67d3\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.13756", "pdf": "https://arxiv.org/pdf/2506.13756", "abs": "https://arxiv.org/abs/2506.13756", "authors": ["Jingwei Ma", "Vivek Jayaram", "Brian Curless", "Ira Kemelmacher-Shlizerman", "Steven M. Seitz"], "title": "UltraZoom: Generating Gigapixel Images from Regular Photos", "categories": ["cs.GR", "cs.CV"], "comment": "Project page: https://ultra-zoom.github.io/", "summary": "We present UltraZoom, a system for generating gigapixel-resolution images of objects from casually captured inputs, such as handheld phone photos. Given a full-shot image (global, low-detail) and one or more close-ups (local, high-detail), UltraZoom upscales the full image to match the fine detail and scale of the close-up examples. To achieve this, we construct a per-instance paired dataset from the close-ups and adapt a pretrained generative model to learn object-specific low-to-high resolution mappings. At inference, we apply the model in a sliding window fashion over the full image. Constructing these pairs is non-trivial: it requires registering the close-ups within the full image for scale estimation and degradation alignment. We introduce a simple, robust method for getting registration on arbitrary materials in casual, in-the-wild captures. Together, these components form a system that enables seamless pan and zoom across the entire object, producing consistent, photorealistic gigapixel imagery from minimal input.", "AI": {"tldr": "UltraZoom\u7cfb\u7edf\u901a\u8fc7\u7ed3\u5408\u4f4e\u5206\u8fa8\u7387\u5168\u5c40\u56fe\u50cf\u548c\u9ad8\u5206\u8fa8\u7387\u5c40\u90e8\u56fe\u50cf\uff0c\u751f\u6210\u8d85\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u3002", "motivation": "\u89e3\u51b3\u4ece\u968f\u610f\u62cd\u6444\u7684\u624b\u673a\u7167\u7247\u4e2d\u751f\u6210\u8d85\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u9700\u6c42\u3002", "method": "\u6784\u5efa\u5b9e\u4f8b\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u8c03\u6574\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\uff0c\u5b66\u4e60\u4ece\u4f4e\u5206\u8fa8\u7387\u5230\u9ad8\u5206\u8fa8\u7387\u7684\u6620\u5c04\uff0c\u5e76\u4f7f\u7528\u6ed1\u52a8\u7a97\u53e3\u65b9\u6cd5\u63a8\u7406\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u4e00\u81f4\u4e14\u903c\u771f\u7684\u5343\u5146\u50cf\u7d20\u56fe\u50cf\u3002", "conclusion": "UltraZoom\u7cfb\u7edf\u901a\u8fc7\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4ece\u5c11\u91cf\u8f93\u5165\u751f\u6210\u9ad8\u8d28\u91cf\u8d85\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u3002"}}
{"id": "2506.12323", "pdf": "https://arxiv.org/pdf/2506.12323", "abs": "https://arxiv.org/abs/2506.12323", "authors": ["Janet Wang", "Yunbei Zhang", "Zhengming Ding", "Jihun Hamm"], "title": "Doctor Approved: Generating Medically Accurate Skin Disease Images through AI-Expert Feedback", "categories": ["cs.CV"], "comment": null, "summary": "Paucity of medical data severely limits the generalizability of diagnostic ML models, as the full spectrum of disease variability can not be represented by a small clinical dataset. To address this, diffusion models (DMs) have been considered as a promising avenue for synthetic image generation and augmentation. However, they frequently produce medically inaccurate images, deteriorating the model performance. Expert domain knowledge is critical for synthesizing images that correctly encode clinical information, especially when data is scarce and quality outweighs quantity. Existing approaches for incorporating human feedback, such as reinforcement learning (RL) and Direct Preference Optimization (DPO), rely on robust reward functions or demand labor-intensive expert evaluations. Recent progress in Multimodal Large Language Models (MLLMs) reveals their strong visual reasoning capabilities, making them adept candidates as evaluators. In this work, we propose a novel framework, coined MAGIC (Medically Accurate Generation of Images through AI-Expert Collaboration), that synthesizes clinically accurate skin disease images for data augmentation. Our method creatively translates expert-defined criteria into actionable feedback for image synthesis of DMs, significantly improving clinical accuracy while reducing the direct human workload. Experiments demonstrate that our method greatly improves the clinical quality of synthesized skin disease images, with outputs aligning with dermatologist assessments. Additionally, augmenting training data with these synthesized images improves diagnostic accuracy by +9.02% on a challenging 20-condition skin disease classification task, and by +13.89% in the few-shot setting.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMAGIC\u6846\u67b6\uff0c\u901a\u8fc7AI\u4e0e\u4e13\u5bb6\u534f\u4f5c\u751f\u6210\u533b\u5b66\u51c6\u786e\u7684\u76ae\u80a4\u75c5\u56fe\u50cf\uff0c\u63d0\u5347\u6570\u636e\u589e\u5f3a\u6548\u679c\u3002", "motivation": "\u533b\u5b66\u6570\u636e\u7a00\u7f3a\u9650\u5236\u4e86\u8bca\u65adML\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u73b0\u6709\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u56fe\u50cf\u533b\u5b66\u51c6\u786e\u6027\u4e0d\u8db3\u3002", "method": "\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4f5c\u4e3a\u8bc4\u4f30\u5668\uff0c\u5c06\u4e13\u5bb6\u5b9a\u4e49\u7684\u6807\u51c6\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u53cd\u9988\uff0c\u4f18\u5316\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u5408\u6210\u3002", "result": "\u751f\u6210\u7684\u76ae\u80a4\u75c5\u56fe\u50cf\u4e34\u5e8a\u8d28\u91cf\u663e\u8457\u63d0\u5347\uff0c\u8bca\u65ad\u51c6\u786e\u7387\u572820\u79cd\u76ae\u80a4\u75c5\u5206\u7c7b\u4efb\u52a1\u4e2d\u63d0\u9ad89.02%\uff0c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u63d0\u9ad813.89%\u3002", "conclusion": "MAGIC\u6846\u67b6\u6709\u6548\u7ed3\u5408\u4e13\u5bb6\u77e5\u8bc6\u4e0eAI\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u533b\u5b66\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u5347\u8bca\u65ad\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2506.12324", "pdf": "https://arxiv.org/pdf/2506.12324", "abs": "https://arxiv.org/abs/2506.12324", "authors": ["Yuantao Wang", "Haowei Yang", "Wei Zhang", "Shijian Lu"], "title": "UniDet-D: A Unified Dynamic Spectral Attention Model for Object Detection under Adverse Weathers", "categories": ["cs.CV"], "comment": null, "summary": "Real-world object detection is a challenging task where the captured images/videos often suffer from complex degradations due to various adverse weather conditions such as rain, fog, snow, low-light, etc. Despite extensive prior efforts, most existing methods are designed for one specific type of adverse weather with constraints of poor generalization, under-utilization of visual features while handling various image degradations. Leveraging a theoretical analysis on how critical visual details are lost in adverse-weather images, we design UniDet-D, a unified framework that tackles the challenge of object detection under various adverse weather conditions, and achieves object detection and image restoration within a single network. Specifically, the proposed UniDet-D incorporates a dynamic spectral attention mechanism that adaptively emphasizes informative spectral components while suppressing irrelevant ones, enabling more robust and discriminative feature representation across various degradation types. Extensive experiments show that UniDet-D achieves superior detection accuracy across different types of adverse-weather degradation. Furthermore, UniDet-D demonstrates superior generalization towards unseen adverse weather conditions such as sandstorms and rain-fog mixtures, highlighting its great potential for real-world deployment.", "AI": {"tldr": "UniDet-D\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u79cd\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u7269\u4f53\u68c0\u6d4b\u95ee\u9898\uff0c\u7ed3\u5408\u52a8\u6001\u5149\u8c31\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u5347\u7279\u5f81\u8868\u793a\u548c\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u7269\u4f53\u68c0\u6d4b\u5e38\u56e0\u6076\u52a3\u5929\u6c14\u6761\u4ef6\uff08\u5982\u96e8\u3001\u96fe\u3001\u96ea\u7b49\uff09\u5bfc\u81f4\u56fe\u50cf\u9000\u5316\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u5355\u4e00\u5929\u6c14\u8bbe\u8ba1\uff0c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51faUniDet-D\u6846\u67b6\uff0c\u7ed3\u5408\u52a8\u6001\u5149\u8c31\u6ce8\u610f\u529b\u673a\u5236\uff0c\u81ea\u9002\u5e94\u5f3a\u8c03\u4fe1\u606f\u5149\u8c31\u6210\u5206\uff0c\u6291\u5236\u65e0\u5173\u6210\u5206\uff0c\u5b9e\u73b0\u7269\u4f53\u68c0\u6d4b\u4e0e\u56fe\u50cf\u6062\u590d\u3002", "result": "\u5b9e\u9a8c\u8868\u660eUniDet-D\u5728\u591a\u79cd\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u68c0\u6d4b\u7cbe\u5ea6\u4f18\u5f02\uff0c\u4e14\u5bf9\u672a\u89c1\u8fc7\u7684\u65b0\u5929\u6c14\u6761\u4ef6\uff08\u5982\u6c99\u5c18\u66b4\u3001\u96e8\u96fe\u6df7\u5408\uff09\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "UniDet-D\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u90e8\u7f72\u7684\u6f5c\u529b\uff0c\u4e3a\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u7269\u4f53\u68c0\u6d4b\u63d0\u4f9b\u4e86\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.12400", "pdf": "https://arxiv.org/pdf/2506.12400", "abs": "https://arxiv.org/abs/2506.12400", "authors": ["Hongbi Zhou", "Zhangkai Ni"], "title": "Perceptual-GS: Scene-adaptive Perceptual Densification for Gaussian Splatting", "categories": ["cs.CV"], "comment": "Accepted to International Conference on Machine Learning (ICML) 2025", "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for novel view synthesis. However, existing methods struggle to adaptively optimize the distribution of Gaussian primitives based on scene characteristics, making it challenging to balance reconstruction quality and efficiency. Inspired by human perception, we propose scene-adaptive perceptual densification for Gaussian Splatting (Perceptual-GS), a novel framework that integrates perceptual sensitivity into the 3DGS training process to address this challenge. We first introduce a perception-aware representation that models human visual sensitivity while constraining the number of Gaussian primitives. Building on this foundation, we develop a \\cameraready{perceptual sensitivity-adaptive distribution} to allocate finer Gaussian granularity to visually critical regions, enhancing reconstruction quality and robustness. Extensive evaluations on multiple datasets, including BungeeNeRF for large-scale scenes, demonstrate that Perceptual-GS achieves state-of-the-art performance in reconstruction quality, efficiency, and robustness. The code is publicly available at: https://github.com/eezkni/Perceptual-GS", "AI": {"tldr": "Perceptual-GS\u662f\u4e00\u79cd\u57fa\u4e8e\u4eba\u7c7b\u611f\u77e5\u76843D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u4f18\u5316\u9ad8\u65af\u57fa\u5143\u5206\u5e03\uff0c\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6839\u636e\u573a\u666f\u7279\u5f81\u81ea\u9002\u5e94\u4f18\u5316\u9ad8\u65af\u57fa\u5143\u5206\u5e03\uff0c\u5bfc\u81f4\u91cd\u5efa\u8d28\u91cf\u4e0e\u6548\u7387\u96be\u4ee5\u5e73\u8861\u3002", "method": "\u63d0\u51fa\u611f\u77e5\u654f\u611f\u8868\u793a\u6a21\u578b\uff0c\u7ed3\u5408\u4eba\u7c7b\u89c6\u89c9\u654f\u611f\u5ea6\uff0c\u5e76\u5f00\u53d1\u611f\u77e5\u654f\u611f\u81ea\u9002\u5e94\u5206\u5e03\u7b56\u7565\uff0c\u4f18\u5316\u9ad8\u65af\u57fa\u5143\u5206\u914d\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\uff08\u5305\u62ec\u5927\u573a\u666fBungeeNeRF\uff09\u4e0a\u9a8c\u8bc1\uff0cPerceptual-GS\u5728\u91cd\u5efa\u8d28\u91cf\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u4e0a\u8fbe\u5230\u6700\u4f18\u3002", "conclusion": "Perceptual-GS\u901a\u8fc7\u611f\u77e5\u654f\u611f\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u7684\u6027\u80fd\u3002"}}
{"id": "2506.12447", "pdf": "https://arxiv.org/pdf/2506.12447", "abs": "https://arxiv.org/abs/2506.12447", "authors": ["Nathanael L. Baisa", "Babu Pallam", "Amudhavel Jayavel"], "title": "CLIP-HandID: Vision-Language Model for Hand-Based Person Identification", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces a new approach to person identification based on hand images, designed specifically for criminal investigations. The method is particularly valuable in serious crimes like sexual abuse, where hand images are often the sole identifiable evidence available. Our proposed method, CLIP-HandID, leverages pre-trained foundational vision-language model, particularly CLIP, to efficiently learn discriminative deep feature representations from hand images given as input to the image encoder of CLIP using textual prompts as semantic guidance. We propose to learn pseudo-tokens that represent specific visual contexts or appearance attributes using textual inversion network since labels of hand images are indexes instead text descriptions. The learned pseudo-tokens are incorporated into textual prompts which are given as input to the text encoder of the CLIP to leverage its multi-modal reasoning to enhance its generalization for identification. Through extensive evaluations on two large, publicly available hand datasets with multi-ethnic representation, we show that our method substantially surpasses existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u624b\u90e8\u56fe\u50cf\u7684\u4eba\u7269\u8bc6\u522b\u65b0\u65b9\u6cd5CLIP-HandID\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684CLIP\u6a21\u578b\u548c\u6587\u672c\u63d0\u793a\u589e\u5f3a\u8bc6\u522b\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u6027\u4fb5\u7b49\u4e25\u91cd\u72af\u7f6a\u4e2d\uff0c\u624b\u90e8\u56fe\u50cf\u5e38\u662f\u552f\u4e00\u53ef\u7528\u7684\u8bc1\u636e\uff0c\u9700\u8981\u9ad8\u6548\u8bc6\u522b\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408CLIP\u6a21\u578b\u7684\u89c6\u89c9\u8bed\u8a00\u80fd\u529b\uff0c\u901a\u8fc7\u6587\u672c\u53cd\u8f6c\u7f51\u7edc\u5b66\u4e60\u4f2a\u6807\u8bb0\uff0c\u589e\u5f3a\u591a\u6a21\u6001\u63a8\u7406\u3002", "result": "\u5728\u4e24\u4e2a\u5927\u578b\u516c\u5f00\u624b\u90e8\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CLIP-HandID\u4e3a\u72af\u7f6a\u8c03\u67e5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u624b\u90e8\u56fe\u50cf\u8bc6\u522b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.12517", "pdf": "https://arxiv.org/pdf/2506.12517", "abs": "https://arxiv.org/abs/2506.12517", "authors": ["Yunhao Shui", "Xuekuan Wang", "Feng Qiu", "Yuqiu Huang", "Jinzhu Li", "Haoyu Zheng", "Jinru Han", "Zhuo Zeng", "Pengpeng Zhang", "Jiarui Han", "Keqiang Sun"], "title": "Retrieval Augmented Comic Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "We present RaCig, a novel system for generating comic-style image sequences with consistent characters and expressive gestures. RaCig addresses two key challenges: (1) maintaining character identity and costume consistency across frames, and (2) producing diverse and vivid character gestures. Our approach integrates a retrieval-based character assignment module, which aligns characters in textual prompts with reference images, and a regional character injection mechanism that embeds character features into specified image regions. Experimental results demonstrate that RaCig effectively generates engaging comic narratives with coherent characters and dynamic interactions. The source code will be publicly available to support further research in this area.", "AI": {"tldr": "RaCig\u662f\u4e00\u4e2a\u751f\u6210\u6f2b\u753b\u98ce\u683c\u56fe\u50cf\u5e8f\u5217\u7684\u7cfb\u7edf\uff0c\u89e3\u51b3\u89d2\u8272\u4e00\u81f4\u6027\u548c\u751f\u52a8\u624b\u52bf\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u6f2b\u753b\u751f\u6210\u4e2d\u89d2\u8272\u8eab\u4efd\u548c\u670d\u88c5\u4e00\u81f4\u6027\u4ee5\u53ca\u624b\u52bf\u591a\u6837\u6027\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u68c0\u7d22\u5f0f\u89d2\u8272\u5206\u914d\u6a21\u5757\u548c\u533a\u57df\u89d2\u8272\u6ce8\u5165\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eRaCig\u80fd\u751f\u6210\u8fde\u8d2f\u89d2\u8272\u548c\u52a8\u6001\u4e92\u52a8\u7684\u6f2b\u753b\u53d9\u4e8b\u3002", "conclusion": "\u7cfb\u7edf\u4ee3\u7801\u5c06\u516c\u5f00\uff0c\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2506.12520", "pdf": "https://arxiv.org/pdf/2506.12520", "abs": "https://arxiv.org/abs/2506.12520", "authors": ["Saemee Choi", "Sohyun Jeong", "Jaegul Choo", "Jinhee Kim"], "title": "Good Noise Makes Good Edits: A Training-Free Diffusion-Based Video Editing with Image and Text Prompts", "categories": ["cs.CV"], "comment": null, "summary": "We propose ImEdit, the first zero-shot, training-free video editing method conditioned on both images and text. The proposed method introduces $\\rho$-start sampling and dilated dual masking to construct well-structured noise maps for coherent and accurate edits. We further present zero image guidance, a controllable negative prompt strategy, for visual fidelity. Both quantitative and qualitative evaluations show that our method outperforms state-of-the-art methods across all metrics.", "AI": {"tldr": "ImEdit\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u96f6\u6837\u672c\u7684\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\uff0c\u652f\u6301\u56fe\u50cf\u548c\u6587\u672c\u6761\u4ef6\u7f16\u8f91\uff0c\u901a\u8fc7\u03c1-start\u91c7\u6837\u548c\u6269\u5f20\u53cc\u63a9\u7801\u6280\u672f\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u8bad\u7ec3\u6216\u6837\u672c\u652f\u6301\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u6548\u7387\u3002ImEdit\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u96f6\u6837\u672c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u03c1-start\u91c7\u6837\u548c\u6269\u5f20\u53cc\u63a9\u7801\u6280\u672f\u6784\u5efa\u7ed3\u6784\u5316\u566a\u58f0\u56fe\uff0c\u7ed3\u5408\u96f6\u56fe\u50cf\u5f15\u5bfc\u548c\u53ef\u63a7\u8d1f\u63d0\u793a\u7b56\u7565\u63d0\u5347\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "result": "\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u8868\u660e\uff0cImEdit\u5728\u6240\u6709\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ImEdit\u4e3a\u89c6\u9891\u7f16\u8f91\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u4e14\u9ad8\u8d28\u91cf\u7684\u96f6\u6837\u672c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.12530", "pdf": "https://arxiv.org/pdf/2506.12530", "abs": "https://arxiv.org/abs/2506.12530", "authors": ["Xingzhong Hou", "Jie Wu", "Boxiao Liu", "Yi Zhang", "Guanglu Song", "Yunpeng Liu", "Yu Liu", "Haihang You"], "title": "Towards Seamless Borders: A Method for Mitigating Inconsistencies in Image Inpainting and Outpainting", "categories": ["cs.CV"], "comment": null, "summary": "Image inpainting is the task of reconstructing missing or damaged parts of an image in a way that seamlessly blends with the surrounding content. With the advent of advanced generative models, especially diffusion models and generative adversarial networks, inpainting has achieved remarkable improvements in visual quality and coherence. However, achieving seamless continuity remains a significant challenge. In this work, we propose two novel methods to address discrepancy issues in diffusion-based inpainting models. First, we introduce a modified Variational Autoencoder that corrects color imbalances, ensuring that the final inpainted results are free of color mismatches. Second, we propose a two-step training strategy that improves the blending of generated and existing image content during the diffusion process. Through extensive experiments, we demonstrate that our methods effectively reduce discontinuity and produce high-quality inpainting results that are coherent and visually appealing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u65b9\u6cd5\uff0c\u6539\u8fdb\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u4fee\u590d\u6280\u672f\uff0c\u89e3\u51b3\u989c\u8272\u4e0d\u5339\u914d\u548c\u5185\u5bb9\u4e0d\u8fde\u8d2f\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u751f\u6210\u6a21\u578b\uff08\u5982\u6269\u6563\u6a21\u578b\u548cGAN\uff09\u5728\u56fe\u50cf\u4fee\u590d\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5b9e\u73b0\u65e0\u7f1d\u8fde\u7eed\u6027\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "1. \u5f15\u5165\u6539\u8fdb\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4ee5\u6821\u6b63\u989c\u8272\u5931\u8861\uff1b2. \u63d0\u51fa\u4e24\u6b65\u8bad\u7ec3\u7b56\u7565\u4f18\u5316\u6269\u6563\u8fc7\u7a0b\u4e2d\u751f\u6210\u5185\u5bb9\u4e0e\u73b0\u6709\u5185\u5bb9\u7684\u878d\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e0d\u8fde\u7eed\u6027\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u8fde\u8d2f\u4e14\u89c6\u89c9\u5438\u5f15\u4eba\u7684\u4fee\u590d\u7ed3\u679c\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u4fee\u590d\u7684\u8fde\u8d2f\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002"}}
{"id": "2506.12633", "pdf": "https://arxiv.org/pdf/2506.12633", "abs": "https://arxiv.org/abs/2506.12633", "authors": ["Changhyun Choi", "Sungha Kim", "H. Jin Kim"], "title": "Performance Plateaus in Inference-Time Scaling for Text-to-Image Diffusion Without External Models", "categories": ["cs.CV", "cs.LG"], "comment": "MOSS workshop at ICML 2025 accepted", "summary": "Recently, it has been shown that investing computing resources in searching for good initial noise for a text-to-image diffusion model helps improve performance. However, previous studies required external models to evaluate the resulting images, which is impossible on GPUs with small VRAM. For these reasons, we apply Best-of-N inference-time scaling to algorithms that optimize the initial noise of a diffusion model without external models across multiple datasets and backbones. We demonstrate that inference-time scaling for text-to-image diffusion models in this setting quickly reaches a performance plateau, and a relatively small number of optimization steps suffices to achieve the maximum achievable performance with each algorithm.", "AI": {"tldr": "\u7814\u7a76\u663e\u793a\uff0c\u901a\u8fc7\u4f18\u5316\u521d\u59cb\u566a\u58f0\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u9700\u5916\u90e8\u6a21\u578b\u8bc4\u4f30\u3002\u672c\u6587\u63d0\u51fa\u65e0\u9700\u5916\u90e8\u6a21\u578b\u7684Best-of-N\u63a8\u7406\u65f6\u7f29\u653e\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u5176\u5728\u5c0f\u89c4\u6a21\u4f18\u5316\u6b65\u9aa4\u4e0b\u5373\u53ef\u8fbe\u5230\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f9d\u8d56\u5916\u90e8\u6a21\u578b\u8bc4\u4f30\u7684\u9650\u5236\uff0c\u5c24\u5176\u662f\u5728\u5c0fVRAM GPU\u4e0a\u65e0\u6cd5\u8fd0\u884c\u7684\u95ee\u9898\u3002", "method": "\u5e94\u7528Best-of-N\u63a8\u7406\u65f6\u7f29\u653e\u65b9\u6cd5\uff0c\u4f18\u5316\u6269\u6563\u6a21\u578b\u521d\u59cb\u566a\u58f0\uff0c\u65e0\u9700\u5916\u90e8\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5c0f\u89c4\u6a21\u4f18\u5316\u6b65\u9aa4\u4e0b\u5373\u53ef\u8fbe\u5230\u6027\u80fd\u5cf0\u503c\u3002", "conclusion": "Best-of-N\u63a8\u7406\u65f6\u7f29\u653e\u65b9\u6cd5\u5728\u4f18\u5316\u521d\u59cb\u566a\u58f0\u65f6\u9ad8\u6548\u4e14\u65e0\u9700\u5916\u90e8\u6a21\u578b\u652f\u6301\u3002"}}
{"id": "2506.12680", "pdf": "https://arxiv.org/pdf/2506.12680", "abs": "https://arxiv.org/abs/2506.12680", "authors": ["Chen-Bin Feng", "Kangdao Liu", "Jian Sun", "Jiping Jin", "Yiguo Jiang", "Chi-Man Vong"], "title": "3D Hand Mesh-Guided AI-Generated Malformed Hand Refinement with Hand Pose Transformation via Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "The malformed hands in the AI-generated images seriously affect the authenticity of the images. To refine malformed hands, existing depth-based approaches use a hand depth estimator to guide the refinement of malformed hands. Due to the performance limitations of the hand depth estimator, many hand details cannot be represented, resulting in errors in the generated hands, such as confusing the palm and the back of the hand. To solve this problem, we propose a 3D mesh-guided refinement framework using a diffusion pipeline. We use a state-of-the-art 3D hand mesh estimator, which provides more details of the hands. For training, we collect and reannotate a dataset consisting of RGB images and 3D hand mesh. Then we design a diffusion inpainting model to generate refined outputs guided by 3D hand meshes. For inference, we propose a double check algorithm to facilitate the 3D hand mesh estimator to obtain robust hand mesh guidance to obtain our refined results. Beyond malformed hand refinement, we propose a novel hand pose transformation method. It increases the flexibility and diversity of the malformed hand refinement task. We made the restored images mimic the hand poses of the reference images. The pose transformation requires no additional training. Extensive experimental results demonstrate the superior performance of our proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u7f51\u683c\u548c\u6269\u6563\u7ba1\u7ebf\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4fee\u590dAI\u751f\u6210\u56fe\u50cf\u4e2d\u7684\u7578\u5f62\u624b\u90e8\uff0c\u5e76\u901a\u8fc7\u59ff\u6001\u53d8\u6362\u589e\u52a0\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u7684\u65b9\u6cd5\u56e0\u624b\u90e8\u6df1\u5ea6\u4f30\u8ba1\u5668\u7684\u6027\u80fd\u9650\u5236\uff0c\u65e0\u6cd5\u51c6\u786e\u8868\u793a\u624b\u90e8\u7ec6\u8282\uff0c\u5bfc\u81f4\u751f\u6210\u9519\u8bef\u3002", "method": "\u4f7f\u7528\u5148\u8fdb\u76843D\u624b\u90e8\u7f51\u683c\u4f30\u8ba1\u5668\u63d0\u4f9b\u66f4\u591a\u7ec6\u8282\uff0c\u8bbe\u8ba1\u6269\u6563\u4fee\u590d\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u53cc\u68c0\u67e5\u7b97\u6cd5\u4f18\u5316\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fee\u590d\u7578\u5f62\u624b\u90e8\u548c\u59ff\u6001\u53d8\u6362\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u624b\u90e8\u4fee\u590d\u7684\u771f\u5b9e\u6027\u548c\u591a\u6837\u6027\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u59ff\u6001\u53d8\u6362\u3002"}}
{"id": "2506.12716", "pdf": "https://arxiv.org/pdf/2506.12716", "abs": "https://arxiv.org/abs/2506.12716", "authors": ["Wen-Hsuan Chu", "Lei Ke", "Jianmeng Liu", "Mingxiao Huo", "Pavel Tokmakov", "Katerina Fragkiadaki"], "title": "Generative 4D Scene Gaussian Splatting with Object View-Synthesis Priors", "categories": ["cs.CV"], "comment": "This is an updated and extended version of our CVPR paper \"Robust   Multi-Object 4D Generation in Complex Video Scenarios\"", "summary": "We tackle the challenge of generating dynamic 4D scenes from monocular, multi-object videos with heavy occlusions, and introduce GenMOJO, a novel approach that integrates rendering-based deformable 3D Gaussian optimization with generative priors for view synthesis. While existing models perform well on novel view synthesis for isolated objects, they struggle to generalize to complex, cluttered scenes. To address this, GenMOJO decomposes the scene into individual objects, optimizing a differentiable set of deformable Gaussians per object. This object-wise decomposition allows leveraging object-centric diffusion models to infer unobserved regions in novel viewpoints. It performs joint Gaussian splatting to render the full scene, capturing cross-object occlusions, and enabling occlusion-aware supervision. To bridge the gap between object-centric priors and the global frame-centric coordinate system of videos, GenMOJO uses differentiable transformations that align generative and rendering constraints within a unified framework. The resulting model generates 4D object reconstructions over space and time, and produces accurate 2D and 3D point tracks from monocular input. Quantitative evaluations and perceptual human studies confirm that GenMOJO generates more realistic novel views of scenes and produces more accurate point tracks compared to existing approaches.", "AI": {"tldr": "GenMOJO\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u53ef\u53d8\u5f623D\u9ad8\u65af\u4f18\u5316\u548c\u751f\u6210\u5148\u9a8c\uff0c\u4ece\u5355\u76ee\u591a\u76ee\u6807\u89c6\u9891\u4e2d\u751f\u6210\u52a8\u60014D\u573a\u666f\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u906e\u6321\u573a\u666f\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u5b64\u7acb\u5bf9\u8c61\u7684\u65b0\u89c6\u89d2\u5408\u6210\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u3001\u6742\u4e71\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "GenMOJO\u5c06\u573a\u666f\u5206\u89e3\u4e3a\u72ec\u7acb\u5bf9\u8c61\uff0c\u4e3a\u6bcf\u4e2a\u5bf9\u8c61\u4f18\u5316\u53ef\u53d8\u5f62\u9ad8\u65af\u96c6\uff0c\u5229\u7528\u5bf9\u8c61\u4e2d\u5fc3\u6269\u6563\u6a21\u578b\u63a8\u65ad\u672a\u89c2\u6d4b\u533a\u57df\uff0c\u5e76\u901a\u8fc7\u8054\u5408\u9ad8\u65af\u6e85\u5c04\u6e32\u67d3\u5b8c\u6574\u573a\u666f\u3002", "result": "GenMOJO\u751f\u6210\u4e86\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0a\u76844D\u5bf9\u8c61\u91cd\u5efa\uff0c\u5e76\u4ece\u5355\u76ee\u8f93\u5165\u4e2d\u751f\u6210\u51c6\u786e\u76842D\u548c3D\u70b9\u8f68\u8ff9\u3002", "conclusion": "GenMOJO\u5728\u751f\u6210\u573a\u666f\u65b0\u89c6\u89d2\u548c\u70b9\u8f68\u8ff9\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9a\u91cf\u8bc4\u4f30\u548c\u4eba\u7c7b\u611f\u77e5\u7814\u7a76\u8bc1\u5b9e\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2506.12727", "pdf": "https://arxiv.org/pdf/2506.12727", "abs": "https://arxiv.org/abs/2506.12727", "authors": ["Minhyuk Choi", "Injae Kim", "Hyunwoo J. Kim"], "title": "Efficient multi-view training for 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has emerged as a preferred choice alongside Neural Radiance Fields (NeRF) in inverse rendering due to its superior rendering speed. Currently, the common approach in 3DGS is to utilize \"single-view\" mini-batch training, where only one image is processed per iteration, in contrast to NeRF's \"multi-view\" mini-batch training, which leverages multiple images. We observe that such single-view training can lead to suboptimal optimization due to increased variance in mini-batch stochastic gradients, highlighting the necessity for multi-view training. However, implementing multi-view training in 3DGS poses challenges. Simply rendering multiple images per iteration incurs considerable overhead and may result in suboptimal Gaussian densification due to its reliance on single-view assumptions. To address these issues, we modify the rasterization process to minimize the overhead associated with multi-view training and propose a 3D distance-aware D-SSIM loss and multi-view adaptive density control that better suits multi-view scenarios. Our experiments demonstrate that the proposed methods significantly enhance the performance of 3DGS and its variants, freeing 3DGS from the constraints of single-view training.", "AI": {"tldr": "3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u56e0\u5176\u6e32\u67d3\u901f\u5ea6\u5feb\u6210\u4e3a\u9006\u5411\u6e32\u67d3\u7684\u9996\u9009\uff0c\u4f46\u5355\u89c6\u56fe\u8bad\u7ec3\u5bfc\u81f4\u4f18\u5316\u4e0d\u4f73\u3002\u672c\u6587\u63d0\u51fa\u591a\u89c6\u56fe\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u5149\u6805\u5316\u8fc7\u7a0b\u548c\u5f15\u5165\u65b0\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5355\u89c6\u56fe\u8bad\u7ec3\u57283DGS\u4e2d\u5bfc\u81f4\u68af\u5ea6\u65b9\u5dee\u5927\uff0c\u4f18\u5316\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u591a\u89c6\u56fe\u8bad\u7ec3\u3002", "method": "\u6539\u8fdb\u5149\u6805\u5316\u4ee5\u51cf\u5c11\u591a\u89c6\u56fe\u8bad\u7ec3\u5f00\u9500\uff0c\u63d0\u51fa3D\u8ddd\u79bb\u611f\u77e5D-SSIM\u635f\u5931\u548c\u591a\u89c6\u56fe\u81ea\u9002\u5e94\u5bc6\u5ea6\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u53473DGS\u53ca\u5176\u53d8\u4f53\u6027\u80fd\u3002", "conclusion": "\u591a\u89c6\u56fe\u8bad\u7ec3\u65b9\u6cd5\u89e3\u653e\u4e863DGS\u7684\u5355\u89c6\u56fe\u9650\u5236\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2506.12738", "pdf": "https://arxiv.org/pdf/2506.12738", "abs": "https://arxiv.org/abs/2506.12738", "authors": ["Hang Xu", "Wei Yu", "Jiangtong Tan", "Zhen Zou", "Feng Zhao"], "title": "Adaptive Dropout: Unleashing Dropout across Layers for Generalizable Image Super-Resolution", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 8 figures, CVPR2025", "summary": "Blind Super-Resolution (blind SR) aims to enhance the model's generalization ability with unknown degradation, yet it still encounters severe overfitting issues. Some previous methods inspired by dropout, which enhances generalization by regularizing features, have shown promising results in blind SR. Nevertheless, these methods focus solely on regularizing features before the final layer and overlook the need for generalization in features at intermediate layers. Without explicit regularization of features at intermediate layers, the blind SR network struggles to obtain well-generalized feature representations. However, the key challenge is that directly applying dropout to intermediate layers leads to a significant performance drop, which we attribute to the inconsistency in training-testing and across layers it introduced. Therefore, we propose Adaptive Dropout, a new regularization method for blind SR models, which mitigates the inconsistency and facilitates application across intermediate layers of networks. Specifically, for training-testing inconsistency, we re-design the form of dropout and integrate the features before and after dropout adaptively. For inconsistency in generalization requirements across different layers, we innovatively design an adaptive training strategy to strengthen feature propagation by layer-wise annealing. Experimental results show that our method outperforms all past regularization methods on both synthetic and real-world benchmark datasets, also highly effective in other image restoration tasks. Code is available at \\href{https://github.com/xuhang07/Adpative-Dropout}{https://github.com/xuhang07/Adpative-Dropout}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAdaptive Dropout\u7684\u65b0\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u76f2\u8d85\u5206\u8fa8\u7387\uff08blind SR\uff09\u4e2d\u4e2d\u95f4\u5c42\u7279\u5f81\u6cdb\u5316\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u8bad\u7ec3\u7b56\u7565\u548c\u7279\u5f81\u6574\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u76f2\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u5728\u672a\u77e5\u9000\u5316\u60c5\u51b5\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u6700\u7ec8\u5c42\u7684\u7279\u5f81\u6b63\u5219\u5316\uff0c\u5ffd\u7565\u4e86\u4e2d\u95f4\u5c42\u7279\u5f81\u6cdb\u5316\u7684\u91cd\u8981\u6027\uff0c\u76f4\u63a5\u5e94\u7528Dropout\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faAdaptive Dropout\uff0c\u91cd\u65b0\u8bbe\u8ba1Dropout\u5f62\u5f0f\u5e76\u81ea\u9002\u5e94\u6574\u5408\u7279\u5f81\uff0c\u540c\u65f6\u901a\u8fc7\u5206\u5c42\u9000\u706b\u7b56\u7565\u89e3\u51b3\u4e0d\u540c\u5c42\u6cdb\u5316\u9700\u6c42\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u4ee5\u5f80\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u5e76\u5728\u5176\u4ed6\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "Adaptive Dropout\u6709\u6548\u89e3\u51b3\u4e86\u76f2\u8d85\u5206\u8fa8\u7387\u4e2d\u7279\u5f81\u6cdb\u5316\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3a\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u6b63\u5219\u5316\u601d\u8def\u3002"}}
{"id": "2506.12782", "pdf": "https://arxiv.org/pdf/2506.12782", "abs": "https://arxiv.org/abs/2506.12782", "authors": ["Szabolcs Velkei", "Csaba Goldschmidt", "K\u00e1roly Vass"], "title": "A large-scale, physically-based synthetic dataset for satellite pose estimation", "categories": ["cs.CV", "68U10 (Primary), 68T45 (Secondary)", "I.4.8; I.2.10"], "comment": "8 pages, 6 figures", "summary": "The Deep Learning Visual Space Simulation System (DLVS3) introduces a novel synthetic dataset generator and a simulation pipeline specifically designed for training and testing satellite pose estimation solutions. This work introduces the DLVS3-HST-V1 dataset, which focuses on the Hubble Space Telescope (HST) as a complex, articulated target. The dataset is generated using advanced real-time and offline rendering technologies, integrating high-fidelity 3D models, dynamic lighting (including secondary sources like Earth reflection), and physically accurate material properties. The pipeline supports the creation of large-scale, richly annotated image sets with ground-truth 6-DoF pose and keypoint data, semantic segmentation, depth, and normal maps. This enables the training and benchmarking of deep learning-based pose estimation solutions under realistic, diverse, and challenging visual conditions. The paper details the dataset generation process, the simulation architecture, and the integration with deep learning frameworks, and positions DLVS3 as a significant step toward closing the domain gap for autonomous spacecraft operations in proximity and servicing missions.", "AI": {"tldr": "DLVS3\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u536b\u661f\u59ff\u6001\u4f30\u8ba1\u7684\u5408\u6210\u6570\u636e\u96c6\u751f\u6210\u5668\u548c\u4eff\u771f\u6d41\u7a0b\uff0c\u5e76\u53d1\u5e03\u4e86DLVS3-HST-V1\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u54c8\u52c3\u592a\u7a7a\u671b\u8fdc\u955c\u8fd9\u4e00\u590d\u6742\u76ee\u6807\u3002", "motivation": "\u89e3\u51b3\u536b\u661f\u59ff\u6001\u4f30\u8ba1\u4e2d\u771f\u5b9e\u6570\u636e\u83b7\u53d6\u56f0\u96be\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u586b\u8865\u9886\u57df\u5dee\u8ddd\u3002", "method": "\u4f7f\u7528\u5b9e\u65f6\u548c\u79bb\u7ebf\u6e32\u67d3\u6280\u672f\u751f\u6210\u9ad8\u4fdd\u771f3D\u6a21\u578b\uff0c\u7ed3\u5408\u52a8\u6001\u5149\u7167\u548c\u7269\u7406\u51c6\u786e\u7684\u6750\u8d28\u5c5e\u6027\uff0c\u751f\u6210\u5927\u89c4\u6a21\u3001\u591a\u6807\u6ce8\u7684\u56fe\u50cf\u96c6\u3002", "result": "DLVS3-HST-V1\u6570\u636e\u96c6\u652f\u63016-DoF\u59ff\u6001\u3001\u5173\u952e\u70b9\u6570\u636e\u3001\u8bed\u4e49\u5206\u5272\u3001\u6df1\u5ea6\u548c\u6cd5\u7ebf\u56fe\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u591a\u6837\u5316\u7684\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6761\u4ef6\u3002", "conclusion": "DLVS3\u4e3a\u81ea\u4e3b\u822a\u5929\u5668\u64cd\u4f5c\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u4eff\u771f\u5de5\u5177\uff0c\u7f29\u5c0f\u4e86\u9886\u57df\u5dee\u8ddd\u3002"}}
{"id": "2506.12787", "pdf": "https://arxiv.org/pdf/2506.12787", "abs": "https://arxiv.org/abs/2506.12787", "authors": ["Mufan Liu", "Cixiao Zhang", "Qi Yang", "Yujie Cao", "Yiling Xu", "Yin Xu", "Shu Sun", "Mingzeng Dai", "Yunfeng Guan"], "title": "Rasterizing Wireless Radiance Field via Deformable 2D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Modeling the wireless radiance field (WRF) is fundamental to modern communication systems, enabling key tasks such as localization, sensing, and channel estimation. Traditional approaches, which rely on empirical formulas or physical simulations, often suffer from limited accuracy or require strong scene priors. Recent neural radiance field (NeRF-based) methods improve reconstruction fidelity through differentiable volumetric rendering, but their reliance on computationally expensive multilayer perceptron (MLP) queries hinders real-time deployment. To overcome these challenges, we introduce Gaussian splatting (GS) to the wireless domain, leveraging its efficiency in modeling optical radiance fields to enable compact and accurate WRF reconstruction. Specifically, we propose SwiftWRF, a deformable 2D Gaussian splatting framework that synthesizes WRF spectra at arbitrary positions under single-sided transceiver mobility. SwiftWRF employs CUDA-accelerated rasterization to render spectra at over 100000 fps and uses a lightweight MLP to model the deformation of 2D Gaussians, effectively capturing mobility-induced WRF variations. In addition to novel spectrum synthesis, the efficacy of SwiftWRF is further underscored in its applications in angle-of-arrival (AoA) and received signal strength indicator (RSSI) prediction. Experiments conducted on both real-world and synthetic indoor scenes demonstrate that SwiftWRF can reconstruct WRF spectra up to 500x faster than existing state-of-the-art methods, while significantly enhancing its signal quality. Code and datasets will be released.", "AI": {"tldr": "SwiftWRF\u662f\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u7684\u9ad8\u6548\u65e0\u7ebf\u8f90\u5c04\u573a\u5efa\u6a21\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u901f\u5ea6\u548c\u4fe1\u53f7\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u65e0\u7ebf\u8f90\u5c04\u573a\u5efa\u6a21\u65b9\u6cd5\u7cbe\u5ea6\u6709\u9650\u6216\u4f9d\u8d56\u5f3a\u573a\u666f\u5148\u9a8c\uff0c\u800c\u57fa\u4e8eNeRF\u7684\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5b9e\u65f6\u90e8\u7f72\u3002", "method": "\u63d0\u51faSwiftWRF\u6846\u67b6\uff0c\u5229\u7528\u53ef\u53d8\u5f622D\u9ad8\u65af\u6cfc\u6e85\u548cCUDA\u52a0\u901f\u6805\u683c\u5316\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7MLP\u5efa\u6a21\u53d8\u5f62\uff0c\u5b9e\u73b0\u9ad8\u6548\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSwiftWRF\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb500\u500d\uff0c\u4fe1\u53f7\u8d28\u91cf\u663e\u8457\u63d0\u5347\uff0c\u9002\u7528\u4e8eAoA\u548cRSSI\u9884\u6d4b\u3002", "conclusion": "SwiftWRF\u4e3a\u65e0\u7ebf\u8f90\u5c04\u573a\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.12793", "pdf": "https://arxiv.org/pdf/2506.12793", "abs": "https://arxiv.org/abs/2506.12793", "authors": ["Wenhao Shen", "Gangjian Zhang", "Jianfeng Zhang", "Yu Feng", "Nanjie Yao", "Xuanmeng Zhang", "Hao Wang"], "title": "SMPL Normal Map Is All You Need for Single-view Textured Human Reconstruction", "categories": ["cs.CV"], "comment": "Accepted to ICME 2025 (Oral)", "summary": "Single-view textured human reconstruction aims to reconstruct a clothed 3D digital human by inputting a monocular 2D image. Existing approaches include feed-forward methods, limited by scarce 3D human data, and diffusion-based methods, prone to erroneous 2D hallucinations. To address these issues, we propose a novel SMPL normal map Equipped 3D Human Reconstruction (SEHR) framework, integrating a pretrained large 3D reconstruction model with human geometry prior. SEHR performs single-view human reconstruction without using a preset diffusion model in one forward propagation. Concretely, SEHR consists of two key components: SMPL Normal Map Guidance (SNMG) and SMPL Normal Map Constraint (SNMC). SNMG incorporates SMPL normal maps into an auxiliary network to provide improved body shape guidance. SNMC enhances invisible body parts by constraining the model to predict an extra SMPL normal Gaussians. Extensive experiments on two benchmark datasets demonstrate that SEHR outperforms existing state-of-the-art methods.", "AI": {"tldr": "SEHR\u6846\u67b6\u901a\u8fc7\u7ed3\u5408SMPL\u6cd5\u7ebf\u56fe\u5f15\u5bfc\u548c\u7ea6\u675f\uff0c\u5b9e\u73b0\u4e86\u5355\u89c6\u89d23D\u4eba\u4f53\u91cd\u5efa\uff0c\u65e0\u9700\u9884\u8bbe\u6269\u6563\u6a21\u578b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u7a00\u7f3a\u76843D\u4eba\u4f53\u6570\u636e\u6216\u6613\u4ea7\u751f2D\u5e7b\u89c9\uff0cSEHR\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "SEHR\u5305\u542bSMPL\u6cd5\u7ebf\u56fe\u5f15\u5bfc\uff08SNMG\uff09\u548c\u7ea6\u675f\uff08SNMC\uff09\uff0c\u901a\u8fc7\u8f85\u52a9\u7f51\u7edc\u548c\u989d\u5916\u9884\u6d4b\u589e\u5f3a\u91cd\u5efa\u6548\u679c\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSEHR\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "SEHR\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5355\u89c6\u89d23D\u4eba\u4f53\u91cd\u5efa\u7684\u6311\u6218\uff0c\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2506.12824", "pdf": "https://arxiv.org/pdf/2506.12824", "abs": "https://arxiv.org/abs/2506.12824", "authors": ["Haoyou Deng", "Zhiqiang Li", "Feng Zhang", "Qingbo Lu", "Zisheng Cao", "Yuanjie Shao", "Shuhang Gu", "Changxin Gao", "Nong Sang"], "title": "Learning Unpaired Image Dehazing with Physics-based Rehazy Generation", "categories": ["cs.CV"], "comment": null, "summary": "Overfitting to synthetic training pairs remains a critical challenge in image dehazing, leading to poor generalization capability to real-world scenarios. To address this issue, existing approaches utilize unpaired realistic data for training, employing CycleGAN or contrastive learning frameworks. Despite their progress, these methods often suffer from training instability, resulting in limited dehazing performance. In this paper, we propose a novel training strategy for unpaired image dehazing, termed Rehazy, to improve both dehazing performance and training stability. This strategy explores the consistency of the underlying clean images across hazy images and utilizes hazy-rehazy pairs for effective learning of real haze characteristics. To favorably construct hazy-rehazy pairs, we develop a physics-based rehazy generation pipeline, which is theoretically validated to reliably produce high-quality rehazy images. Additionally, leveraging the rehazy strategy, we introduce a dual-branch framework for dehazing network training, where a clean branch provides a basic dehazing capability in a synthetic manner, and a hazy branch enhances the generalization ability with hazy-rehazy pairs. Moreover, we design a new dehazing network within these branches to improve the efficiency, which progressively restores clean scenes from coarse to fine. Extensive experiments on four benchmarks demonstrate the superior performance of our approach, exceeding the previous state-of-the-art methods by 3.58 dB on the SOTS-Indoor dataset and by 1.85 dB on the SOTS-Outdoor dataset in PSNR. Our code will be publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRehazy\u7684\u65b0\u578b\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u5229\u7528\u6a21\u7cca-\u518d\u6a21\u7cca\u5bf9\u548c\u53cc\u5206\u652f\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u53bb\u96fe\u7684\u6027\u80fd\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u56fe\u50cf\u53bb\u96fe\u4e2d\u56e0\u8fc7\u62df\u5408\u5408\u6210\u6570\u636e\u800c\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u4e14\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faRehazy\u7b56\u7565\uff0c\u5229\u7528\u6a21\u7cca-\u518d\u6a21\u7cca\u5bf9\u5b66\u4e60\u771f\u5b9e\u96fe\u7279\u6027\uff0c\u5e76\u8bbe\u8ba1\u53cc\u5206\u652f\u6846\u67b6\u548c\u6e10\u8fdb\u5f0f\u53bb\u96fe\u7f51\u7edc\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cPSNR\u6307\u6807\u5206\u522b\u63d0\u53473.58 dB\u548c1.85 dB\u3002", "conclusion": "Rehazy\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.12835", "pdf": "https://arxiv.org/pdf/2506.12835", "abs": "https://arxiv.org/abs/2506.12835", "authors": ["Di Kong", "Qianhui Wan"], "title": "DiffS-NOCS: 3D Point Cloud Reconstruction through Coloring Sketches to NOCS Maps Using Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing a 3D point cloud from a given conditional sketch is challenging. Existing methods often work directly in 3D space, but domain variability and difficulty in reconstructing accurate 3D structures from 2D sketches remain significant obstacles. Moreover, ideal models should also accept prompts for control, in addition with the sparse sketch, posing challenges in multi-modal fusion. We propose DiffS-NOCS (Diffusion-based Sketch-to-NOCS Map), which leverages ControlNet with a modified multi-view decoder to generate NOCS maps with embedded 3D structure and position information in 2D space from sketches. The 3D point cloud is reconstructed by combining multiple NOCS maps from different views. To enhance sketch understanding, we integrate a viewpoint encoder for extracting viewpoint features. Additionally, we design a feature-level multi-view aggregation network as the denoising module, facilitating cross-view information exchange and improving 3D consistency in NOCS map generation. Experiments on ShapeNet demonstrate that DiffS-NOCS achieves controllable and fine-grained point cloud reconstruction aligned with sketches.", "AI": {"tldr": "DiffS-NOCS\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210NOCS\u5730\u56fe\u4ece2D\u8349\u56fe\u91cd\u5efa3D\u70b9\u4e91\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u878d\u5408\u548c3D\u7ed3\u6784\u91cd\u5efa\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u57283D\u7a7a\u95f4\u4e2d\u76f4\u63a5\u64cd\u4f5c\uff0c\u96be\u4ee5\u4ece2D\u8349\u56fe\u4e2d\u51c6\u786e\u91cd\u5efa3D\u7ed3\u6784\uff0c\u4e14\u7f3a\u4e4f\u591a\u6a21\u6001\u63a7\u5236\u80fd\u529b\u3002", "method": "\u4f7f\u7528ControlNet\u548c\u6539\u8fdb\u7684\u591a\u89c6\u89d2\u89e3\u7801\u5668\u751f\u6210NOCS\u5730\u56fe\uff0c\u7ed3\u5408\u89c6\u89d2\u7f16\u7801\u5668\u548c\u591a\u89c6\u89d2\u805a\u5408\u7f51\u7edc\u63d0\u53473D\u4e00\u81f4\u6027\u3002", "result": "\u5728ShapeNet\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDiffS-NOCS\u80fd\u591f\u5b9e\u73b0\u53ef\u63a7\u4e14\u7cbe\u7ec6\u7684\u70b9\u4e91\u91cd\u5efa\u3002", "conclusion": "DiffS-NOCS\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u548c\u591a\u89c6\u89d2\u4fe1\u606f\u4ea4\u6362\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8349\u56fe\u52303D\u70b9\u4e91\u7684\u91cd\u5efa\u6548\u679c\u3002"}}
{"id": "2506.12853", "pdf": "https://arxiv.org/pdf/2506.12853", "abs": "https://arxiv.org/abs/2506.12853", "authors": ["Jie Liu", "Zheng Hui"], "title": "EraserDiT: Fast Video Inpainting with Diffusion Transformer Model", "categories": ["cs.CV"], "comment": null, "summary": "Video object removal and inpainting are critical tasks in the fields of computer vision and multimedia processing, aimed at restoring missing or corrupted regions in video sequences. Traditional methods predominantly rely on flow-based propagation and spatio-temporal Transformers, but these approaches face limitations in effectively leveraging long-term temporal features and ensuring temporal consistency in the completion results, particularly when dealing with large masks. Consequently, performance on extensive masked areas remains suboptimal. To address these challenges, this paper introduces a novel video inpainting approach leveraging the Diffusion Transformer (DiT). DiT synergistically combines the advantages of diffusion models and transformer architectures to maintain long-term temporal consistency while ensuring high-quality inpainting results. We propose a Circular Position-Shift strategy to further enhance long-term temporal consistency during the inference stage. Additionally, the proposed method automatically detects objects within videos, interactively removes specified objects, and generates corresponding prompts. In terms of processing speed, it takes only 180 seconds (testing on one NVIDIA A100 GPU) to complete a video with a resolution of $1080 \\times 1920$ with 121 frames without any acceleration method. Experimental results indicate that the proposed method demonstrates superior performance in content fidelity, texture restoration, and temporal consistency. Project page: https://jieliu95.github.io/EraserDiT_demo.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u7684\u89c6\u9891\u4fee\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u53d8\u6362\u5668\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u957f\u671f\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u5927\u9762\u79ef\u63a9\u7801\u4fee\u590d\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u4fee\u590d\u65b9\u6cd5\u5728\u957f\u671f\u65f6\u95f4\u7279\u5f81\u5229\u7528\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5927\u9762\u79ef\u63a9\u7801\u65f6\u6548\u679c\u8f83\u5dee\u3002", "method": "\u91c7\u7528\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u7ed3\u5408\u73af\u5f62\u4f4d\u7f6e\u504f\u79fb\u7b56\u7565\uff0c\u589e\u5f3a\u957f\u671f\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5e76\u81ea\u52a8\u68c0\u6d4b\u548c\u79fb\u9664\u89c6\u9891\u4e2d\u7684\u6307\u5b9a\u5bf9\u8c61\u3002", "result": "\u57281080\u00d71920\u5206\u8fa8\u7387\u3001121\u5e27\u7684\u89c6\u9891\u4e0a\u4ec5\u9700180\u79d2\u5b8c\u6210\u4fee\u590d\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728\u5185\u5bb9\u4fdd\u771f\u5ea6\u3001\u7eb9\u7406\u6062\u590d\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u4fee\u590d\u7684\u8d28\u91cf\u548c\u6548\u7387\uff0c\u5c24\u5176\u5728\u957f\u671f\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2506.12896", "pdf": "https://arxiv.org/pdf/2506.12896", "abs": "https://arxiv.org/abs/2506.12896", "authors": ["Taiga Hayami", "Kakeru Koizumi", "Hiroshi Watanabe"], "title": "Efficient Neural Video Representation via Structure-Preseving Patch Decoding", "categories": ["cs.CV"], "comment": null, "summary": "Implicit Neural Representations (INRs) have attracted significant interest for their ability to model complex signals by mapping spatial and temporal coordinates to signal values. In the context of neural video representation, several decoding strategies have been explored to balance compactness and reconstruction quality, including pixel-wise, frame-wise, and patch-wise methods. Patch-wise decoding aims to combine the flexibility of pixel-based models with the efficiency of frame-based approaches. However, conventional uniform patch division often leads to discontinuities at patch boundaries, as independently reconstructed regions may fail to form a coherent global structure. To address this limitation, we propose a neural video representation method based on Structure-Preserving Patches (SPPs). Our approach rearranges each frame into a set of spatially structured patch frames using a PixelUnshuffle-like operation. This rearrangement maintains the spatial coherence of the original frame while enabling patch-level decoding. The network learns to predict these rearranged patch frames, which supports a global-to-local fitting strategy and mitigates degradation caused by upsampling. Experiments on standard video datasets show that the proposed method improves reconstruction quality and compression performance compared to existing INR-based video representation methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u4fdd\u6301\u5757\uff08SPPs\uff09\u7684\u795e\u7ecf\u89c6\u9891\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u6392\u5217\u5e27\u4e3a\u7a7a\u95f4\u7ed3\u6784\u5316\u7684\u5757\u5e27\uff0c\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u548c\u538b\u7f29\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5747\u5300\u5757\u5212\u5206\u5728\u5757\u8fb9\u754c\u5904\u53ef\u80fd\u5bfc\u81f4\u4e0d\u8fde\u7eed\u6027\uff0c\u5f71\u54cd\u5168\u5c40\u7ed3\u6784\u7684\u8fde\u8d2f\u6027\u3002", "method": "\u4f7f\u7528\u7c7b\u4f3cPixelUnshuffle\u7684\u64cd\u4f5c\u5c06\u5e27\u91cd\u65b0\u6392\u5217\u4e3a\u7a7a\u95f4\u7ed3\u6784\u5316\u7684\u5757\u5e27\uff0c\u7f51\u7edc\u5b66\u4e60\u9884\u6d4b\u8fd9\u4e9b\u5757\u5e27\uff0c\u652f\u6301\u5168\u5c40\u5230\u5c40\u90e8\u7684\u62df\u5408\u7b56\u7565\u3002", "result": "\u5728\u6807\u51c6\u89c6\u9891\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u57fa\u4e8eINR\u7684\u89c6\u9891\u8868\u793a\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u538b\u7f29\u6027\u80fd\u3002", "conclusion": "SPPs\u65b9\u6cd5\u901a\u8fc7\u4fdd\u6301\u7a7a\u95f4\u8fde\u8d2f\u6027\u548c\u5c40\u90e8\u89e3\u7801\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u5757\u5212\u5206\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2506.12945", "pdf": "https://arxiv.org/pdf/2506.12945", "abs": "https://arxiv.org/abs/2506.12945", "authors": ["Hyunjin Kim", "Haebeom Jung", "Jaesik Park"], "title": "Metropolis-Hastings Sampling for 3D Gaussian Reconstruction", "categories": ["cs.CV"], "comment": "Project Page: https://hjhyunjinkim.github.io/MH-3DGS", "summary": "We propose an adaptive sampling framework for 3D Gaussian Splatting (3DGS) that leverages comprehensive multi-view photometric error signals within a unified Metropolis-Hastings approach. Traditional 3DGS methods heavily rely on heuristic-based density-control mechanisms (e.g., cloning, splitting, and pruning), which can lead to redundant computations or the premature removal of beneficial Gaussians. Our framework overcomes these limitations by reformulating densification and pruning as a probabilistic sampling process, dynamically inserting and relocating Gaussians based on aggregated multi-view errors and opacity scores. Guided by Bayesian acceptance tests derived from these error-based importance scores, our method substantially reduces reliance on heuristics, offers greater flexibility, and adaptively infers Gaussian distributions without requiring predefined scene complexity. Experiments on benchmark datasets, including Mip-NeRF360, Tanks and Temples, and Deep Blending, show that our approach reduces the number of Gaussians needed, enhancing computational efficiency while matching or modestly surpassing the view-synthesis quality of state-of-the-art models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMetropolis-Hastings\u7684\u81ea\u9002\u5e94\u91c7\u6837\u6846\u67b6\uff0c\u7528\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u5149\u5ea6\u8bef\u5dee\u4fe1\u53f7\u52a8\u6001\u8c03\u6574\u9ad8\u65af\u5206\u5e03\uff0c\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u3002", "motivation": "\u4f20\u7edf3DGS\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u5bc6\u5ea6\u63a7\u5236\u673a\u5236\uff08\u5982\u514b\u9686\u3001\u5206\u88c2\u548c\u4fee\u526a\uff09\uff0c\u53ef\u80fd\u5bfc\u81f4\u5197\u4f59\u8ba1\u7b97\u6216\u8fc7\u65e9\u79fb\u9664\u6709\u7528\u9ad8\u65af\u3002", "method": "\u5c06\u5bc6\u5ea6\u63a7\u5236\u548c\u4fee\u526a\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6982\u7387\u91c7\u6837\u8fc7\u7a0b\uff0c\u57fa\u4e8e\u591a\u89c6\u89d2\u8bef\u5dee\u548c\u4e0d\u900f\u660e\u5ea6\u5206\u6570\u52a8\u6001\u63d2\u5165\u548c\u91cd\u5b9a\u4f4d\u9ad8\u65af\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u6240\u9700\u9ad8\u65af\u6570\u91cf\uff0c\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u7565\u5fae\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\u7684\u89c6\u56fe\u5408\u6210\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6982\u7387\u91c7\u6837\u51cf\u5c11\u5bf9\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u4f9d\u8d56\uff0c\u7075\u6d3b\u9002\u5e94\u573a\u666f\u590d\u6742\u5ea6\uff0c\u63d0\u5347\u4e863DGS\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002"}}
{"id": "2506.13110", "pdf": "https://arxiv.org/pdf/2506.13110", "abs": "https://arxiv.org/abs/2506.13110", "authors": ["Jinguang Tong", "Xuesong li", "Fahira Afzal Maken", "Sundaram Muthu", "Lars Petersson", "Chuong Nguyen", "Hongdong Li"], "title": "GS-2DGS: Geometrically Supervised 2DGS for Reflective Object Reconstruction", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "3D modeling of highly reflective objects remains challenging due to strong view-dependent appearances. While previous SDF-based methods can recover high-quality meshes, they are often time-consuming and tend to produce over-smoothed surfaces. In contrast, 3D Gaussian Splatting (3DGS) offers the advantage of high speed and detailed real-time rendering, but extracting surfaces from the Gaussians can be noisy due to the lack of geometric constraints. To bridge the gap between these approaches, we propose a novel reconstruction method called GS-2DGS for reflective objects based on 2D Gaussian Splatting (2DGS). Our approach combines the rapid rendering capabilities of Gaussian Splatting with additional geometric information from foundation models. Experimental results on synthetic and real datasets demonstrate that our method significantly outperforms Gaussian-based techniques in terms of reconstruction and relighting and achieves performance comparable to SDF-based methods while being an order of magnitude faster. Code is available at https://github.com/hirotong/GS2DGS", "AI": {"tldr": "\u63d0\u51faGS-2DGS\u65b9\u6cd5\uff0c\u7ed3\u54082D\u9ad8\u65af\u6cfc\u6e85\u4e0e\u51e0\u4f55\u4fe1\u606f\uff0c\u9ad8\u6548\u91cd\u5efa\u9ad8\u53cd\u5c04\u7269\u4f53\u3002", "motivation": "\u89e3\u51b3\u9ad8\u53cd\u5c04\u7269\u4f533D\u5efa\u6a21\u4e2d\u4f20\u7edfSDF\u65b9\u6cd5\u8017\u65f6\u4e14\u5e73\u6ed1\uff0c\u800c3D\u9ad8\u65af\u6cfc\u6e85\u7f3a\u4e4f\u51e0\u4f55\u7ea6\u675f\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e2D\u9ad8\u65af\u6cfc\u6e85\uff0c\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u7684\u51e0\u4f55\u4fe1\u606f\uff0c\u63d0\u51faGS-2DGS\u65b9\u6cd5\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0c\u91cd\u5efa\u548c\u91cd\u5149\u7167\u6548\u679c\u663e\u8457\u4f18\u4e8e\u9ad8\u65af\u65b9\u6cd5\uff0c\u901f\u5ea6\u6bd4SDF\u65b9\u6cd5\u5feb\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "GS-2DGS\u5728\u9ad8\u53cd\u5c04\u7269\u4f53\u91cd\u5efa\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e0e\u9ad8\u8d28\u91cf\u7684\u5e73\u8861\u3002"}}
{"id": "2506.13138", "pdf": "https://arxiv.org/pdf/2506.13138", "abs": "https://arxiv.org/abs/2506.13138", "authors": ["Jiamin Wang", "Yichen Yao", "Xiang Feng", "Hang Wu", "Yaming Wang", "Qingqiu Huang", "Yuexin Ma", "Xinge Zhu"], "title": "STAGE: A Stream-Centric Generative World Model for Long-Horizon Driving-Scene Simulation", "categories": ["cs.CV"], "comment": null, "summary": "The generation of temporally consistent, high-fidelity driving videos over extended horizons presents a fundamental challenge in autonomous driving world modeling. Existing approaches often suffer from error accumulation and feature misalignment due to inadequate decoupling of spatio-temporal dynamics and limited cross-frame feature propagation mechanisms. To address these limitations, we present STAGE (Streaming Temporal Attention Generative Engine), a novel auto-regressive framework that pioneers hierarchical feature coordination and multi-phase optimization for sustainable video synthesis. To achieve high-quality long-horizon driving video generation, we introduce Hierarchical Temporal Feature Transfer (HTFT) and a novel multi-stage training strategy. HTFT enhances temporal consistency between video frames throughout the video generation process by modeling the temporal and denoising process separately and transferring denoising features between frames. The multi-stage training strategy is to divide the training into three stages, through model decoupling and auto-regressive inference process simulation, thereby accelerating model convergence and reducing error accumulation. Experiments on the Nuscenes dataset show that STAGE has significantly surpassed existing methods in the long-horizon driving video generation task. In addition, we also explored STAGE's ability to generate unlimited-length driving videos. We generated 600 frames of high-quality driving videos on the Nuscenes dataset, which far exceeds the maximum length achievable by existing methods.", "AI": {"tldr": "STAGE\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u7279\u5f81\u534f\u8c03\u548c\u591a\u9636\u6bb5\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u9a7e\u9a76\u89c6\u9891\u751f\u6210\u4e2d\u7684\u9519\u8bef\u7d2f\u79ef\u548c\u7279\u5f81\u5bf9\u9f50\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u7a7a\u52a8\u6001\u89e3\u8026\u548c\u8de8\u5e27\u7279\u5f81\u4f20\u64ad\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u9519\u8bef\u7d2f\u79ef\u548c\u7279\u5f81\u9519\u4f4d\uff0c\u9650\u5236\u4e86\u9ad8\u8d28\u91cf\u957f\u65f6\u7a0b\u9a7e\u9a76\u89c6\u9891\u7684\u751f\u6210\u3002", "method": "STAGE\u91c7\u7528\u5206\u5c42\u65f6\u95f4\u7279\u5f81\u4f20\u9012\uff08HTFT\uff09\u548c\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5206\u522b\u5efa\u6a21\u65f6\u95f4\u548c\u53bb\u566a\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u7279\u5f81\u4f20\u9012\u589e\u5f3a\u5e27\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5728Nuscenes\u6570\u636e\u96c6\u4e0a\uff0cSTAGE\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u6210\u529f\u751f\u6210\u4e86600\u5e27\u9ad8\u8d28\u91cf\u9a7e\u9a76\u89c6\u9891\uff0c\u8fdc\u8d85\u5176\u4ed6\u65b9\u6cd5\u7684\u6781\u9650\u3002", "conclusion": "STAGE\u901a\u8fc7\u521b\u65b0\u6027\u7684\u5206\u5c42\u7279\u5f81\u534f\u8c03\u548c\u591a\u9636\u6bb5\u4f18\u5316\uff0c\u4e3a\u957f\u65f6\u7a0b\u9a7e\u9a76\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.13260", "pdf": "https://arxiv.org/pdf/2506.13260", "abs": "https://arxiv.org/abs/2506.13260", "authors": ["Yining Shi", "Kun Jiang", "Qiang Meng", "Ke Wang", "Jiabao Wang", "Wenchao Sun", "Tuopu Wen", "Mengmeng Yang", "Diange Yang"], "title": "COME: Adding Scene-Centric Forecasting Control to Occupancy World Model", "categories": ["cs.CV"], "comment": null, "summary": "World models are critical for autonomous driving to simulate environmental dynamics and generate synthetic data. Existing methods struggle to disentangle ego-vehicle motion (perspective shifts) from scene evolvement (agent interactions), leading to suboptimal predictions. Instead, we propose to separate environmental changes from ego-motion by leveraging the scene-centric coordinate systems. In this paper, we introduce COME: a framework that integrates scene-centric forecasting Control into the Occupancy world ModEl. Specifically, COME first generates ego-irrelevant, spatially consistent future features through a scene-centric prediction branch, which are then converted into scene condition using a tailored ControlNet. These condition features are subsequently injected into the occupancy world model, enabling more accurate and controllable future occupancy predictions. Experimental results on the nuScenes-Occ3D dataset show that COME achieves consistent and significant improvements over state-of-the-art (SOTA) methods across diverse configurations, including different input sources (ground-truth, camera-based, fusion-based occupancy) and prediction horizons (3s and 8s). For example, under the same settings, COME achieves 26.3% better mIoU metric than DOME and 23.7% better mIoU metric than UniScene. These results highlight the efficacy of disentangled representation learning in enhancing spatio-temporal prediction fidelity for world models. Code and videos will be available at https://github.com/synsin0/COME.", "AI": {"tldr": "COME\u6846\u67b6\u901a\u8fc7\u5206\u79bb\u73af\u5883\u53d8\u5316\u4e0e\u81ea\u8f66\u8fd0\u52a8\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e16\u754c\u6a21\u578b\u7684\u9884\u6d4b\u7cbe\u5ea6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u533a\u5206\u81ea\u8f66\u8fd0\u52a8\u4e0e\u573a\u666f\u52a8\u6001\u53d8\u5316\uff0c\u5bfc\u81f4\u9884\u6d4b\u6548\u679c\u4e0d\u4f73\u3002", "method": "COME\u5229\u7528\u573a\u666f\u4e2d\u5fc3\u5750\u6807\u7cfb\uff0c\u901a\u8fc7\u573a\u666f\u4e2d\u5fc3\u9884\u6d4b\u5206\u652f\u751f\u6210\u4e0e\u81ea\u8f66\u65e0\u5173\u7684\u672a\u6765\u7279\u5f81\uff0c\u518d\u901a\u8fc7ControlNet\u8f6c\u6362\u4e3a\u573a\u666f\u6761\u4ef6\uff0c\u6ce8\u5165\u5360\u7528\u4e16\u754c\u6a21\u578b\u3002", "result": "\u5728nuScenes-Occ3D\u6570\u636e\u96c6\u4e0a\uff0cCOME\u6bd4DOME\u548cUniScene\u5206\u522b\u63d0\u5347\u4e8626.3%\u548c23.7%\u7684mIoU\u6307\u6807\u3002", "conclusion": "\u89e3\u8026\u8868\u793a\u5b66\u4e60\u80fd\u663e\u8457\u63d0\u5347\u4e16\u754c\u6a21\u578b\u7684\u65f6\u7a7a\u9884\u6d4b\u51c6\u786e\u6027\u3002"}}
{"id": "2506.13298", "pdf": "https://arxiv.org/pdf/2506.13298", "abs": "https://arxiv.org/abs/2506.13298", "authors": ["Jeonghoon Park", "Juyoung Lee", "Chaeyeon Chung", "Jaeseong Lee", "Jaegul Choo", "Jindong Gu"], "title": "Fair Generation without Unfair Distortions: Debiasing Text-to-Image Generation with Entanglement-Free Attention", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in diffusion-based text-to-image (T2I) models have enabled the generation of high-quality and photorealistic images from text descriptions. However, they often exhibit societal biases related to gender, race, and socioeconomic status, thereby reinforcing harmful stereotypes and shaping public perception in unintended ways. While existing bias mitigation methods demonstrate effectiveness, they often encounter attribute entanglement, where adjustments to attributes relevant to the bias (i.e., target attributes) unintentionally alter attributes unassociated with the bias (i.e., non-target attributes), causing undesirable distribution shifts. To address this challenge, we introduce Entanglement-Free Attention (EFA), a method that accurately incorporates target attributes (e.g., White, Black, Asian, and Indian) while preserving non-target attributes (e.g., background details) during bias mitigation. At inference time, EFA randomly samples a target attribute with equal probability and adjusts the cross-attention in selected layers to incorporate the sampled attribute, achieving a fair distribution of target attributes. Extensive experiments demonstrate that EFA outperforms existing methods in mitigating bias while preserving non-target attributes, thereby maintaining the output distribution and generation capability of the original model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEntanglement-Free Attention (EFA)\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6269\u6563\u5f0f\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u975e\u76ee\u6807\u5c5e\u6027\u7684\u5e72\u6270\u3002", "motivation": "\u6269\u6563\u5f0fT2I\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u65f6\u5b58\u5728\u793e\u4f1a\u504f\u89c1\uff08\u5982\u6027\u522b\u3001\u79cd\u65cf\u7b49\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8c03\u6574\u76ee\u6807\u5c5e\u6027\u65f6\u4f1a\u5e72\u6270\u975e\u76ee\u6807\u5c5e\u6027\uff0c\u5bfc\u81f4\u4e0d\u5e0c\u671b\u7684\u5206\u5e03\u504f\u79fb\u3002", "method": "\u63d0\u51faEFA\u65b9\u6cd5\uff0c\u901a\u8fc7\u968f\u673a\u91c7\u6837\u76ee\u6807\u5c5e\u6027\u5e76\u8c03\u6574\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\uff0c\u5b9e\u73b0\u76ee\u6807\u5c5e\u6027\u7684\u516c\u5e73\u5206\u5e03\uff0c\u540c\u65f6\u4fdd\u7559\u975e\u76ee\u6807\u5c5e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEFA\u5728\u51cf\u5c11\u504f\u89c1\u7684\u540c\u65f6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u4fdd\u6301\u539f\u59cb\u6a21\u578b\u7684\u8f93\u51fa\u5206\u5e03\u548c\u751f\u6210\u80fd\u529b\u3002", "conclusion": "EFA\u6709\u6548\u89e3\u51b3\u4e86\u504f\u89c1\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u4e86\u975e\u76ee\u6807\u5c5e\u6027\u7684\u5e72\u6270\uff0c\u4e3aT2I\u6a21\u578b\u7684\u516c\u5e73\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.13307", "pdf": "https://arxiv.org/pdf/2506.13307", "abs": "https://arxiv.org/abs/2506.13307", "authors": ["Sol\u00e8ne Debuys\u00e8re", "Nicolas Trouv\u00e9", "Nathan Letheule", "Olivier L\u00e9v\u00eaque", "Elise Colin"], "title": "Quantitative Comparison of Fine-Tuning Techniques for Pretrained Latent Diffusion Models in the Generation of Unseen SAR Image Concepts", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This work investigates the adaptation of large pre-trained latent diffusion models to a radically new imaging domain: Synthetic Aperture Radar (SAR). While these generative models, originally trained on natural images, demonstrate impressive capabilities in text-to-image synthesis, they are not natively adapted to represent SAR data, which involves different physics, statistical distributions, and visual characteristics. Using a sizeable SAR dataset (on the order of 100,000 to 1 million images), we address the fundamental question of fine-tuning such models for this unseen modality. We explore and compare multiple fine-tuning strategies, including full model fine-tuning and parameter-efficient approaches like Low-Rank Adaptation (LoRA), focusing separately on the UNet diffusion backbone and the text encoder components. To evaluate generative quality, we combine several metrics: statistical distance from real SAR distributions, textural similarity via GLCM descriptors, and semantic alignment assessed with a CLIP model fine-tuned on SAR data. Our results show that a hybrid tuning strategy yields the best performance: full fine-tuning of the UNet is better at capturing low-level SAR-specific patterns, while LoRA-based partial tuning of the text encoder, combined with embedding learning of the <SAR> token, suffices to preserve prompt alignment. This work provides a methodical strategy for adapting foundation models to unconventional imaging modalities beyond natural image domains.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06\u9884\u8bad\u7ec3\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u9002\u5e94\u4e8e\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff08SAR\uff09\u8fd9\u4e00\u5168\u65b0\u6210\u50cf\u9886\u57df\uff0c\u901a\u8fc7\u591a\u79cd\u5fae\u8c03\u7b56\u7565\u4f18\u5316\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46SAR\u6570\u636e\u7684\u7269\u7406\u7279\u6027\u3001\u7edf\u8ba1\u5206\u5e03\u548c\u89c6\u89c9\u7279\u5f81\u4e0e\u4e4b\u4e0d\u540c\uff0c\u9700\u8981\u4e13\u95e8\u9002\u914d\u3002", "method": "\u4f7f\u7528\u5927\u89c4\u6a21SAR\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e86\u5168\u6a21\u578b\u5fae\u8c03\u548c\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\uff08\u5982LoRA\uff09\uff0c\u5206\u522b\u9488\u5bf9UNet\u6269\u6563\u4e3b\u5e72\u548c\u6587\u672c\u7f16\u7801\u5668\u3002", "result": "\u6df7\u5408\u5fae\u8c03\u7b56\u7565\u8868\u73b0\u6700\u4f73\uff1aUNet\u5168\u5fae\u8c03\u6355\u6349SAR\u4f4e\u5c42\u7279\u5f81\uff0cLoRA\u90e8\u5206\u5fae\u8c03\u6587\u672c\u7f16\u7801\u5668\u4fdd\u6301\u63d0\u793a\u5bf9\u9f50\u3002", "conclusion": "\u4e3a\u5c06\u57fa\u7840\u6a21\u578b\u9002\u5e94\u4e8e\u975e\u81ea\u7136\u56fe\u50cf\u9886\u57df\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7b56\u7565\u3002"}}
{"id": "2506.13355", "pdf": "https://arxiv.org/pdf/2506.13355", "abs": "https://arxiv.org/abs/2506.13355", "authors": ["Yan Chen", "Hanlin Shang", "Ce Liu", "Yuxuan Chen", "Hui Li", "Weihao Yuan", "Hao Zhu", "Zilong Dong", "Siyu Zhu"], "title": "DicFace: Dirichlet-Constrained Variational Codebook Learning for Temporally Coherent Video Face Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Video face restoration faces a critical challenge in maintaining temporal consistency while recovering fine facial details from degraded inputs. This paper presents a novel approach that extends Vector-Quantized Variational Autoencoders (VQ-VAEs), pretrained on static high-quality portraits, into a video restoration framework through variational latent space modeling. Our key innovation lies in reformulating discrete codebook representations as Dirichlet-distributed continuous variables, enabling probabilistic transitions between facial features across frames. A spatio-temporal Transformer architecture jointly models inter-frame dependencies and predicts latent distributions, while a Laplacian-constrained reconstruction loss combined with perceptual (LPIPS) regularization enhances both pixel accuracy and visual quality. Comprehensive evaluations on blind face restoration, video inpainting, and facial colorization tasks demonstrate state-of-the-art performance. This work establishes an effective paradigm for adapting intensive image priors, pretrained on high-quality images, to video restoration while addressing the critical challenge of flicker artifacts. The source code has been open-sourced and is available at https://github.com/fudan-generative-vision/DicFace.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eVQ-VAEs\u7684\u89c6\u9891\u4eba\u8138\u4fee\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d8\u5206\u6f5c\u5728\u7a7a\u95f4\u5efa\u6a21\u5b9e\u73b0\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89c6\u9891\u4eba\u8138\u4fee\u590d\u5728\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u540c\u65f6\u6062\u590d\u7ec6\u8282\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u6269\u5c55VQ-VAEs\u4e3a\u89c6\u9891\u6846\u67b6\uff0c\u5c06\u79bb\u6563\u7801\u672c\u8868\u793a\u4e3aDirichlet\u5206\u5e03\u8fde\u7eed\u53d8\u91cf\uff0c\u7ed3\u5408\u65f6\u7a7aTransformer\u548c\u62c9\u666e\u62c9\u65af\u7ea6\u675f\u91cd\u5efa\u635f\u5931\u3002", "result": "\u5728\u76f2\u4fee\u590d\u3001\u89c6\u9891\u4fee\u590d\u548c\u7740\u8272\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u8d28\u91cf\u56fe\u50cf\u5148\u9a8c\u9002\u5e94\u89c6\u9891\u4fee\u590d\u63d0\u4f9b\u4e86\u6709\u6548\u8303\u5f0f\uff0c\u5e76\u89e3\u51b3\u4e86\u95ea\u70c1\u95ee\u9898\u3002"}}
{"id": "2506.13391", "pdf": "https://arxiv.org/pdf/2506.13391", "abs": "https://arxiv.org/abs/2506.13391", "authors": ["Zhen Wang", "Hongyi Liu", "Zhihui Wei"], "title": "Zero-Shot Solving of Imaging Inverse Problems via Noise-Refined Likelihood Guided Diffusion Models", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Diffusion models have achieved remarkable success in imaging inverse problems owing to their powerful generative capabilities. However, existing approaches typically rely on models trained for specific degradation types, limiting their generalizability to various degradation scenarios. To address this limitation, we propose a zero-shot framework capable of handling various imaging inverse problems without model retraining. We introduce a likelihood-guided noise refinement mechanism that derives a closed-form approximation of the likelihood score, simplifying score estimation and avoiding expensive gradient computations. This estimated score is subsequently utilized to refine the model-predicted noise, thereby better aligning the restoration process with the generative framework of diffusion models. In addition, we integrate the Denoising Diffusion Implicit Models (DDIM) sampling strategy to further improve inference efficiency. The proposed mechanism can be applied to both optimization-based and sampling-based schemes, providing an effective and flexible zero-shot solution for imaging inverse problems. Extensive experiments demonstrate that our method achieves superior performance across multiple inverse problems, particularly in compressive sensing, delivering high-quality reconstructions even at an extremely low sampling rate (5%).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u6846\u67b6\uff0c\u901a\u8fc7\u4f3c\u7136\u5f15\u5bfc\u7684\u566a\u58f0\u7ec6\u5316\u673a\u5236\u5904\u7406\u591a\u79cd\u6210\u50cf\u9006\u95ee\u9898\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u9000\u5316\u7c7b\u578b\u8bad\u7ec3\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5f15\u5165\u4f3c\u7136\u5f15\u5bfc\u7684\u566a\u58f0\u7ec6\u5316\u673a\u5236\uff0c\u7ed3\u5408DDIM\u91c7\u6837\u7b56\u7565\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002", "result": "\u5728\u591a\u79cd\u9006\u95ee\u9898\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u538b\u7f29\u611f\u77e5\u4e2d\uff0c\u5373\u4f7f\u91c7\u6837\u7387\u6781\u4f4e\uff085%\uff09\u4e5f\u80fd\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u96f6\u6837\u672c\u6210\u50cf\u9006\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.13445", "pdf": "https://arxiv.org/pdf/2506.13445", "abs": "https://arxiv.org/abs/2506.13445", "authors": ["Waqar Tanveer", "Laura Fern\u00e1ndez-Robles", "Eduardo Fidalgo", "V\u00edctor Gonz\u00e1lez-Castro", "Enrique Alegre"], "title": "Overcoming Occlusions in the Wild: A Multi-Task Age Head Approach to Age Estimation", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Facial age estimation has achieved considerable success under controlled conditions. However, in unconstrained real-world scenarios, which are often referred to as 'in the wild', age estimation remains challenging, especially when faces are partially occluded, which may obscure their visibility. To address this limitation, we propose a new approach integrating generative adversarial networks (GANs) and transformer architectures to enable robust age estimation from occluded faces. We employ an SN-Patch GAN to effectively remove occlusions, while an Attentive Residual Convolution Module (ARCM), paired with a Swin Transformer, enhances feature representation. Additionally, we introduce a Multi-Task Age Head (MTAH) that combines regression and distribution learning, further improving age estimation under occlusion. Experimental results on the FG-NET, UTKFace, and MORPH datasets demonstrate that our proposed approach surpasses existing state-of-the-art techniques for occluded facial age estimation by achieving an MAE of $3.00$, $4.54$, and $2.53$ years, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408GAN\u548cTransformer\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u906e\u6321\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u5e74\u9f84\u4f30\u8ba1\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u5728\u65e0\u7ea6\u675f\u7684\u771f\u5b9e\u573a\u666f\u4e2d\uff0c\u906e\u6321\u4f1a\u964d\u4f4e\u9762\u90e8\u5e74\u9f84\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528SN-Patch GAN\u53bb\u9664\u906e\u6321\uff0c\u7ed3\u5408ARCM\u548cSwin Transformer\u589e\u5f3a\u7279\u5f81\u8868\u793a\uff0c\u5e76\u5f15\u5165MTAH\u8fdb\u884c\u591a\u4efb\u52a1\u5b66\u4e60\u3002", "result": "\u5728FG-NET\u3001UTKFace\u548cMORPH\u6570\u636e\u96c6\u4e0a\uff0cMAE\u5206\u522b\u4e3a3.00\u30014.54\u548c2.53\u5e74\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u906e\u6321\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u5e74\u9f84\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.13465", "pdf": "https://arxiv.org/pdf/2506.13465", "abs": "https://arxiv.org/abs/2506.13465", "authors": ["Zerui Gong", "Zhonghua Wu", "Qingyi Tao", "Qinyue Li", "Chen Change Loy"], "title": "SA-LUT: Spatial Adaptive 4D Look-Up Table for Photorealistic Style Transfer", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Photorealistic style transfer (PST) enables real-world color grading by adapting reference image colors while preserving content structure. Existing methods mainly follow either approaches: generation-based methods that prioritize stylistic fidelity at the cost of content integrity and efficiency, or global color transformation methods such as LUT, which preserve structure but lack local adaptability. To bridge this gap, we propose Spatial Adaptive 4D Look-Up Table (SA-LUT), combining LUT efficiency with neural network adaptability. SA-LUT features: (1) a Style-guided 4D LUT Generator that extracts multi-scale features from the style image to predict a 4D LUT, and (2) a Context Generator using content-style cross-attention to produce a context map. This context map enables spatially-adaptive adjustments, allowing our 4D LUT to apply precise color transformations while preserving structural integrity. To establish a rigorous evaluation framework for photorealistic style transfer, we introduce PST50, the first benchmark specifically designed for PST assessment. Experiments demonstrate that SA-LUT substantially outperforms state-of-the-art methods, achieving a 66.7% reduction in LPIPS score compared to 3D LUT approaches, while maintaining real-time performance at 16 FPS for video stylization. Our code and benchmark are available at https://github.com/Ry3nG/SA-LUT", "AI": {"tldr": "SA-LUT\u662f\u4e00\u79cd\u7ed3\u5408LUT\u6548\u7387\u548c\u795e\u7ecf\u7f51\u7edc\u9002\u5e94\u6027\u7684\u7a7a\u95f4\u81ea\u9002\u5e944D\u67e5\u627e\u8868\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9e\u73b0\u903c\u771f\u7684\u98ce\u683c\u8fc1\u79fb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u98ce\u683c\u4fdd\u771f\u5ea6\u548c\u5185\u5bb9\u5b8c\u6574\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0cSA-LUT\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "SA-LUT\u5305\u62ec\u98ce\u683c\u5f15\u5bfc\u76844D LUT\u751f\u6210\u5668\u548c\u57fa\u4e8e\u5185\u5bb9-\u98ce\u683c\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u4e0a\u4e0b\u6587\u751f\u6210\u5668\uff0c\u5b9e\u73b0\u7a7a\u95f4\u81ea\u9002\u5e94\u8c03\u6574\u3002", "result": "SA-LUT\u5728LPIPS\u8bc4\u5206\u4e0a\u6bd43D LUT\u65b9\u6cd5\u964d\u4f4e\u4e8666.7%\uff0c\u5e76\u5728\u89c6\u9891\u98ce\u683c\u5316\u4e2d\u4fdd\u630116 FPS\u7684\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "SA-LUT\u5728\u903c\u771f\u98ce\u683c\u8fc1\u79fb\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u9996\u4e2a\u4e13\u95e8\u7528\u4e8ePST\u8bc4\u4f30\u7684\u57fa\u51c6PST50\u3002"}}
{"id": "2506.13476", "pdf": "https://arxiv.org/pdf/2506.13476", "abs": "https://arxiv.org/abs/2506.13476", "authors": ["Xiem HoangVan", "Dang Bui Dinh", "Thanh Nguyen Canh", "Van-Truong Nguyen"], "title": "ESRPCB: an Edge guided Super-Resolution model and Ensemble learning for tiny Printed Circuit Board Defect detection", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "Published in Engineering Applications of Artificial Intelligence", "summary": "Printed Circuit Boards (PCBs) are critical components in modern electronics, which require stringent quality control to ensure proper functionality. However, the detection of defects in small-scale PCBs images poses significant challenges as a result of the low resolution of the captured images, leading to potential confusion between defects and noise. To overcome these challenges, this paper proposes a novel framework, named ESRPCB (edgeguided super-resolution for PCBs defect detection), which combines edgeguided super-resolution with ensemble learning to enhance PCBs defect detection. The framework leverages the edge information to guide the EDSR (Enhanced Deep Super-Resolution) model with a novel ResCat (Residual Concatenation) structure, enabling it to reconstruct high-resolution images from small PCBs inputs. By incorporating edge features, the super-resolution process preserves critical structural details, ensuring that tiny defects remain distinguishable in the enhanced image. Following this, a multi-modal defect detection model employs ensemble learning to analyze the super-resolved", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aESRPCB\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u8fb9\u7f18\u5f15\u5bfc\u8d85\u5206\u8fa8\u7387\u548c\u96c6\u6210\u5b66\u4e60\uff0c\u4ee5\u63d0\u5347PCB\u7f3a\u9677\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5c0f\u5c3a\u5bf8PCB\u56fe\u50cf\u7684\u4f4e\u5206\u8fa8\u7387\u5bfc\u81f4\u7f3a\u9677\u4e0e\u566a\u58f0\u96be\u4ee5\u533a\u5206\uff0c\u4e9f\u9700\u4e00\u79cd\u65b9\u6cd5\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u4ee5\u51c6\u786e\u68c0\u6d4b\u7f3a\u9677\u3002", "method": "ESRPCB\u6846\u67b6\u5229\u7528\u8fb9\u7f18\u4fe1\u606f\u5f15\u5bfcEDSR\u6a21\u578b\uff0c\u7ed3\u5408ResCat\u7ed3\u6784\u91cd\u5efa\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u96c6\u6210\u5b66\u4e60\u8fdb\u884c\u591a\u6a21\u6001\u7f3a\u9677\u68c0\u6d4b\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u533a\u5206\u5fae\u5c0f\u7f3a\u9677\u4e0e\u566a\u58f0\uff0c\u63d0\u5347\u7f3a\u9677\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "ESRPCB\u6846\u67b6\u4e3aPCB\u7f3a\u9677\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.13484", "pdf": "https://arxiv.org/pdf/2506.13484", "abs": "https://arxiv.org/abs/2506.13484", "authors": ["Martina Pastorino", "Michael Alibani", "Nicola Acito", "Gabriele Moser"], "title": "Deep Diffusion Models and Unsupervised Hyperspectral Unmixing for Realistic Abundance Map Synthesis", "categories": ["cs.CV", "eess.IV"], "comment": "CVPRw2025", "summary": "This paper presents a novel methodology for generating realistic abundance maps from hyperspectral imagery using an unsupervised, deep-learning-driven approach. Our framework integrates blind linear hyperspectral unmixing with state-of-the-art diffusion models to enhance the realism and diversity of synthetic abundance maps. First, we apply blind unmixing to extract endmembers and abundance maps directly from raw hyperspectral data. These abundance maps then serve as inputs to a diffusion model, which acts as a generative engine to synthesize highly realistic spatial distributions. Diffusion models have recently revolutionized image synthesis by offering superior performance, flexibility, and stability, making them well-suited for high-dimensional spectral data. By leveraging this combination of physically interpretable unmixing and deep generative modeling, our approach enables the simulation of hyperspectral sensor outputs under diverse imaging conditions--critical for data augmentation, algorithm benchmarking, and model evaluation in hyperspectral analysis. Notably, our method is entirely unsupervised, ensuring adaptability to different datasets without the need for labeled training data. We validate our approach using real hyperspectral imagery from the PRISMA space mission for Earth observation, demonstrating its effectiveness in producing realistic synthetic abundance maps that capture the spatial and spectral characteristics of natural scenes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u7684\u8d85\u5149\u8c31\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408\u76f2\u7ebf\u6027\u89e3\u6df7\u548c\u6269\u6563\u6a21\u578b\uff0c\u751f\u6210\u9ad8\u771f\u5b9e\u611f\u7684\u4e30\u5ea6\u56fe\u3002", "motivation": "\u89e3\u51b3\u8d85\u5149\u8c31\u5206\u6790\u4e2d\u6570\u636e\u589e\u5f3a\u3001\u7b97\u6cd5\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u8bc4\u4f30\u7684\u9700\u6c42\uff0c\u7279\u522b\u662f\u5728\u7f3a\u4e4f\u6807\u8bb0\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u7ed3\u5408\u76f2\u7ebf\u6027\u89e3\u6df7\u63d0\u53d6\u7aef\u5143\u548c\u4e30\u5ea6\u56fe\uff0c\u518d\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u771f\u5b9e\u611f\u7684\u5408\u6210\u4e30\u5ea6\u56fe\u3002", "result": "\u5728PRISMA\u7a7a\u95f4\u4efb\u52a1\u7684\u5b9e\u9645\u8d85\u5149\u8c31\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u751f\u6210\u7684\u4e30\u5ea6\u56fe\u5177\u6709\u81ea\u7136\u573a\u666f\u7684\u7a7a\u95f4\u548c\u5149\u8c31\u7279\u5f81\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8d85\u5149\u8c31\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u3001\u9002\u5e94\u6027\u5f3a\u7684\u6570\u636e\u751f\u6210\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u7684\u6210\u50cf\u6761\u4ef6\u3002"}}
{"id": "2506.13492", "pdf": "https://arxiv.org/pdf/2506.13492", "abs": "https://arxiv.org/abs/2506.13492", "authors": ["Chengrui Zhang", "Maizhen Ning", "Zihao Zhou", "Jie Sun", "Kaizhu Huang", "Qiufeng Wang"], "title": "GeoSDF: Plane Geometry Diagram Synthesis via Signed Distance Field", "categories": ["cs.CV"], "comment": null, "summary": "Plane Geometry Diagram Synthesis has been a crucial task in computer graphics, with applications ranging from educational tools to AI-driven mathematical reasoning. Traditionally, we rely on computer tools (e.g., Matplotlib and GeoGebra) to manually generate precise diagrams, but it usually requires huge, complicated calculations cost. Recently, researchers start to work on learning-based methods (e.g., Stable Diffusion and GPT4) to automatically generate diagrams, saving operational cost but usually suffering from limited realism and insufficient accuracy. In this paper, we propose a novel framework GeoSDF to automatically generate diagrams efficiently and accurately with Signed Distance Field (SDF). Specifically, we first represent geometric elements in the SDF, then construct a series of constraint functions to represent geometric relationships, next we optimize such constraint functions to get an optimized field of both elements and constraints, finally by rendering the optimized field, we can obtain the synthesized diagram. In our GeoSDF, we define a symbolic language to easily represent geometric elements and those constraints, and our synthesized geometry diagrams can be self-verified in the SDF, ensuring both mathematical accuracy and visual plausibility. In experiments, our GeoSDF synthesized both normal high-school level and IMO-level geometry diagrams. Through both qualitative and quantitative analysis, we can see that synthesized diagrams are realistic and accurate, and our synthesizing process is simple and efficient. Furthermore, we obtain a very high accuracy of solving geometry problems (over 95\\% while the current SOTA accuracy is around 75%) by leveraging our self-verification property. All of these demonstrate the advantage of GeoSDF, paving the way for more sophisticated, accurate, and flexible generation of geometric diagrams for a wide array of applications.", "AI": {"tldr": "GeoSDF\u6846\u67b6\u5229\u7528SDF\u81ea\u52a8\u751f\u6210\u51e0\u4f55\u56fe\uff0c\u9ad8\u6548\u4e14\u51c6\u786e\uff0c\u652f\u6301\u81ea\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u51e0\u4f55\u95ee\u9898\u6c42\u89e3\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u51e0\u4f55\u56fe\u751f\u6210\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5b66\u4e60\u578b\u65b9\u6cd5\u867d\u8282\u7701\u6210\u672c\u4f46\u7cbe\u5ea6\u548c\u771f\u5b9e\u611f\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8eSDF\u8868\u793a\u51e0\u4f55\u5143\u7d20\uff0c\u6784\u5efa\u7ea6\u675f\u51fd\u6570\u5e76\u4f18\u5316\uff0c\u901a\u8fc7\u6e32\u67d3\u751f\u6210\u56fe\uff0c\u5b9a\u4e49\u7b26\u53f7\u8bed\u8a00\u7b80\u5316\u8868\u793a\u3002", "result": "\u751f\u6210\u9ad8\u4e2d\u548cIMO\u7ea7\u51e0\u4f55\u56fe\uff0c\u771f\u5b9e\u4e14\u51c6\u786e\uff0c\u51e0\u4f55\u95ee\u9898\u6c42\u89e3\u7cbe\u5ea6\u8fbe95%\u3002", "conclusion": "GeoSDF\u4e3a\u51e0\u4f55\u56fe\u751f\u6210\u63d0\u4f9b\u9ad8\u6548\u3001\u51c6\u786e\u3001\u7075\u6d3b\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u3002"}}
{"id": "2506.13508", "pdf": "https://arxiv.org/pdf/2506.13508", "abs": "https://arxiv.org/abs/2506.13508", "authors": ["Jungeon Kim", "Geonsoo Park", "Seungyong Lee"], "title": "Multiview Geometric Regularization of Gaussian Splatting for Accurate Radiance Fields", "categories": ["cs.CV"], "comment": "Accepted to Computer Graphics Forum (EGSR 2025)", "summary": "Recent methods, such as 2D Gaussian Splatting and Gaussian Opacity Fields, have aimed to address the geometric inaccuracies of 3D Gaussian Splatting while retaining its superior rendering quality. However, these approaches still struggle to reconstruct smooth and reliable geometry, particularly in scenes with significant color variation across viewpoints, due to their per-point appearance modeling and single-view optimization constraints. In this paper, we propose an effective multiview geometric regularization strategy that integrates multiview stereo (MVS) depth, RGB, and normal constraints into Gaussian Splatting initialization and optimization. Our key insight is the complementary relationship between MVS-derived depth points and Gaussian Splatting-optimized positions: MVS robustly estimates geometry in regions of high color variation through local patch-based matching and epipolar constraints, whereas Gaussian Splatting provides more reliable and less noisy depth estimates near object boundaries and regions with lower color variation. To leverage this insight, we introduce a median depth-based multiview relative depth loss with uncertainty estimation, effectively integrating MVS depth information into Gaussian Splatting optimization. We also propose an MVS-guided Gaussian Splatting initialization to avoid Gaussians falling into suboptimal positions. Extensive experiments validate that our approach successfully combines these strengths, enhancing both geometric accuracy and rendering quality across diverse indoor and outdoor scenes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u89c6\u89d2\u51e0\u4f55\u6b63\u5219\u5316\u7b56\u7565\uff0c\u7ed3\u5408MVS\u6df1\u5ea6\u3001RGB\u548c\u6cd5\u7ebf\u7ea6\u675f\uff0c\u6539\u8fdb\u4e863D\u9ad8\u65af\u6cfc\u6e85\u7684\u51e0\u4f55\u7cbe\u5ea6\u548c\u6e32\u67d3\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u59822D\u9ad8\u65af\u6cfc\u6e85\u548c\u9ad8\u65af\u4e0d\u900f\u660e\u5ea6\u573a\uff09\u5728\u989c\u8272\u53d8\u5316\u5927\u7684\u573a\u666f\u4e2d\u51e0\u4f55\u91cd\u5efa\u4e0d\u51c6\u786e\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u591a\u89c6\u89d2\u7acb\u4f53\uff08MVS\uff09\u6df1\u5ea6\u3001RGB\u548c\u6cd5\u7ebf\u7ea6\u675f\uff0c\u7ed3\u5408\u9ad8\u65af\u6cfc\u6e85\u4f18\u5316\uff0c\u63d0\u51fa\u4e2d\u503c\u6df1\u5ea6\u635f\u5931\u548cMVS\u5f15\u5bfc\u7684\u521d\u59cb\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5ba4\u5185\u5916\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u51e0\u4f55\u7cbe\u5ea6\u548c\u6e32\u67d3\u8d28\u91cf\u3002", "conclusion": "\u591a\u89c6\u89d2\u51e0\u4f55\u6b63\u5219\u5316\u7b56\u7565\u6709\u6548\u7ed3\u5408\u4e86MVS\u548c\u9ad8\u65af\u6cfc\u6e85\u7684\u4f18\u52bf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2506.13516", "pdf": "https://arxiv.org/pdf/2506.13516", "abs": "https://arxiv.org/abs/2506.13516", "authors": ["Yihui Li", "Chengxin Lv", "Hongyu Yang", "Di Huang"], "title": "Micro-macro Gaussian Splatting with Enhanced Scalability for Unconstrained Scene Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing 3D scenes from unconstrained image collections poses significant challenges due to variations in appearance. In this paper, we propose Scalable Micro-macro Wavelet-based Gaussian Splatting (SMW-GS), a novel method that enhances 3D reconstruction across diverse scales by decomposing scene representations into global, refined, and intrinsic components. SMW-GS incorporates the following innovations: Micro-macro Projection, which enables Gaussian points to sample multi-scale details with improved diversity; and Wavelet-based Sampling, which refines feature representations using frequency-domain information to better capture complex scene appearances. To achieve scalability, we further propose a large-scale scene promotion strategy, which optimally assigns camera views to scene partitions by maximizing their contributions to Gaussian points, achieving consistent and high-quality reconstructions even in expansive environments. Extensive experiments demonstrate that SMW-GS significantly outperforms existing methods in both reconstruction quality and scalability, particularly excelling in large-scale urban environments with challenging illumination variations. Project is available at https://github.com/Kidleyh/SMW-GS.", "AI": {"tldr": "SMW-GS\u662f\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u7684\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u5206\u89e3\u548c\u9891\u7387\u57df\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u7ea6\u675f\u56fe\u50cf\u96c6\u5408\u76843D\u573a\u666f\u91cd\u5efa\u8d28\u91cf\u4e0e\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u89e3\u51b3\u65e0\u7ea6\u675f\u56fe\u50cf\u96c6\u5408\u4e2d\u56e0\u5916\u89c2\u53d8\u5316\u5bfc\u81f4\u76843D\u573a\u666f\u91cd\u5efa\u6311\u6218\u3002", "method": "\u63d0\u51faSMW-GS\u65b9\u6cd5\uff0c\u5305\u62ec\u5fae\u5b8f\u89c2\u6295\u5f71\u548c\u5c0f\u6ce2\u91c7\u6837\uff0c\u7ed3\u5408\u5927\u89c4\u6a21\u573a\u666f\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u57ce\u5e02\u73af\u5883\u3002", "conclusion": "SMW-GS\u901a\u8fc7\u591a\u5c3a\u5ea6\u5206\u89e3\u548c\u9891\u7387\u57df\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u53ef\u6269\u5c55\u76843D\u573a\u666f\u91cd\u5efa\u3002"}}
{"id": "2506.13545", "pdf": "https://arxiv.org/pdf/2506.13545", "abs": "https://arxiv.org/abs/2506.13545", "authors": ["Yuan Gao", "Shaoyan Pan", "Mingzhe Hu", "Huiqiao Xie", "Jill Remick", "Chih-Wei Chang", "Justin Roper", "Zhen Tian", "Xiaofeng Yang"], "title": "Limited-Angle CBCT Reconstruction via Geometry-Integrated Cycle-domain Denoising Diffusion Probabilistic Models", "categories": ["cs.CV"], "comment": null, "summary": "Cone-beam CT (CBCT) is widely used in clinical radiotherapy for image-guided treatment, improving setup accuracy, adaptive planning, and motion management. However, slow gantry rotation limits performance by introducing motion artifacts, blurring, and increased dose. This work aims to develop a clinically feasible method for reconstructing high-quality CBCT volumes from consecutive limited-angle acquisitions, addressing imaging challenges in time- or dose-constrained settings. We propose a limited-angle (LA) geometry-integrated cycle-domain (LA-GICD) framework for CBCT reconstruction, comprising two denoising diffusion probabilistic models (DDPMs) connected via analytic cone-beam forward and back projectors. A Projection-DDPM completes missing projections, followed by back-projection, and an Image-DDPM refines the volume. This dual-domain design leverages complementary priors from projection and image spaces to achieve high-quality reconstructions from limited-angle (<= 90 degrees) scans. Performance was evaluated against full-angle reconstruction. Four board-certified medical physicists conducted assessments. A total of 78 planning CTs in common CBCT geometries were used for training and evaluation. The method achieved a mean absolute error of 35.5 HU, SSIM of 0.84, and PSNR of 29.8 dB, with visibly reduced artifacts and improved soft-tissue clarity. LA-GICD's geometry-aware dual-domain learning, embedded in analytic forward/backward operators, enabled artifact-free, high-contrast reconstructions from a single 90-degree scan, reducing acquisition time and dose four-fold. LA-GICD improves limited-angle CBCT reconstruction with strong data fidelity and anatomical realism. It offers a practical solution for short-arc acquisitions, enhancing CBCT use in radiotherapy by providing clinically applicable images with reduced scan time and dose for more accurate, personalized treatments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u57df\u6269\u6563\u6a21\u578b\u7684\u6709\u9650\u89d2\u5ea6CBCT\u91cd\u5efa\u65b9\u6cd5\uff08LA-GICD\uff09\uff0c\u663e\u8457\u51cf\u5c11\u626b\u63cf\u65f6\u95f4\u548c\u5242\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002", "motivation": "CBCT\u5728\u653e\u7597\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u6162\u901f\u65cb\u8f6c\u5bfc\u81f4\u8fd0\u52a8\u4f2a\u5f71\u548c\u5242\u91cf\u589e\u52a0\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002\u9700\u8981\u4e00\u79cd\u5728\u6709\u9650\u89d2\u5ea6\u626b\u63cf\u4e0b\u4ecd\u80fd\u9ad8\u8d28\u91cf\u91cd\u5efa\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLA-GICD\u6846\u67b6\uff0c\u7ed3\u5408\u6295\u5f71\u57df\u548c\u56fe\u50cf\u57df\u7684\u53cc\u6269\u6563\u6a21\u578b\uff08DDPM\uff09\uff0c\u901a\u8fc7\u89e3\u6790\u6295\u5f71\u548c\u53cd\u6295\u5f71\u64cd\u4f5c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "result": "\u572890\u5ea6\u626b\u63cf\u4e0b\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee35.5 HU\uff0cSSIM 0.84\uff0cPSNR 29.8 dB\uff0c\u4f2a\u5f71\u51cf\u5c11\uff0c\u8f6f\u7ec4\u7ec7\u6e05\u6670\u5ea6\u63d0\u5347\u3002", "conclusion": "LA-GICD\u4e3a\u6709\u9650\u89d2\u5ea6CBCT\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u51cf\u5c11\u626b\u63cf\u65f6\u95f4\u548c\u5242\u91cf\uff0c\u63d0\u5347\u653e\u7597\u4e2d\u7684\u56fe\u50cf\u8d28\u91cf\u3002"}}
{"id": "2506.13552", "pdf": "https://arxiv.org/pdf/2506.13552", "abs": "https://arxiv.org/abs/2506.13552", "authors": ["Guohuan Xie", "Syed Ariff Syed Hesham", "Wenya Guo", "Bing Li", "Ming-Ming Cheng", "Guolei Sun", "Yun Liu"], "title": "A Comprehensive Survey on Video Scene Parsing:Advances, Challenges, and Prospects", "categories": ["cs.CV"], "comment": null, "summary": "Video Scene Parsing (VSP) has emerged as a cornerstone in computer vision, facilitating the simultaneous segmentation, recognition, and tracking of diverse visual entities in dynamic scenes. In this survey, we present a holistic review of recent advances in VSP, covering a wide array of vision tasks, including Video Semantic Segmentation (VSS), Video Instance Segmentation (VIS), Video Panoptic Segmentation (VPS), as well as Video Tracking and Segmentation (VTS), and Open-Vocabulary Video Segmentation (OVVS). We systematically analyze the evolution from traditional hand-crafted features to modern deep learning paradigms -- spanning from fully convolutional networks to the latest transformer-based architectures -- and assess their effectiveness in capturing both local and global temporal contexts. Furthermore, our review critically discusses the technical challenges, ranging from maintaining temporal consistency to handling complex scene dynamics, and offers a comprehensive comparative study of datasets and evaluation metrics that have shaped current benchmarking standards. By distilling the key contributions and shortcomings of state-of-the-art methodologies, this survey highlights emerging trends and prospective research directions that promise to further elevate the robustness and adaptability of VSP in real-world applications.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u89c6\u9891\u573a\u666f\u89e3\u6790\uff08VSP\uff09\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u6db5\u76d6\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\uff0c\u5206\u6790\u4e86\u4ece\u4f20\u7edf\u624b\u5de5\u7279\u5f81\u5230\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u6f14\u53d8\uff0c\u5e76\u63a2\u8ba8\u4e86\u6280\u672f\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u89c6\u9891\u573a\u666f\u89e3\u6790\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5206\u6563\u4e14\u7f3a\u4e4f\u7cfb\u7edf\u6027\u603b\u7ed3\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7cfb\u7edf\u56de\u987e\u4e86VSP\u76f8\u5173\u4efb\u52a1\uff08\u5982VSS\u3001VIS\u3001VPS\u7b49\uff09\u7684\u65b9\u6cd5\u6f14\u53d8\uff0c\u4ece\u5377\u79ef\u7f51\u7edc\u5230Transformer\u67b6\u6784\uff0c\u5e76\u8bc4\u4f30\u5176\u65f6\u7a7a\u4e0a\u4e0b\u6587\u6355\u6349\u80fd\u529b\u3002", "result": "\u603b\u7ed3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u8d21\u732e\u4e0e\u4e0d\u8db3\uff0c\u6bd4\u8f83\u4e86\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "VSP\u9886\u57df\u4ecd\u6709\u6311\u6218\uff0c\u4f46\u65b0\u5174\u6280\u672f\uff08\u5982Transformer\uff09\u548c\u66f4\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u5c06\u63a8\u52a8\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.13558", "pdf": "https://arxiv.org/pdf/2506.13558", "abs": "https://arxiv.org/abs/2506.13558", "authors": ["Yu Yang", "Alan Liang", "Jianbiao Mei", "Yukai Ma", "Yong Liu", "Gim Hee Lee"], "title": "X-Scene: Large-Scale Driving Scene Generation with High Fidelity and Flexible Controllability", "categories": ["cs.CV"], "comment": "28 pages, 9 figures, Project page at https://x-scene.github.io/", "summary": "Diffusion models are advancing autonomous driving by enabling realistic data synthesis, predictive end-to-end planning, and closed-loop simulation, with a primary focus on temporally consistent generation. However, the generation of large-scale 3D scenes that require spatial coherence remains underexplored. In this paper, we propose X-Scene, a novel framework for large-scale driving scene generation that achieves both geometric intricacy and appearance fidelity, while offering flexible controllability. Specifically, X-Scene supports multi-granular control, including low-level conditions such as user-provided or text-driven layout for detailed scene composition and high-level semantic guidance such as user-intent and LLM-enriched text prompts for efficient customization. To enhance geometrical and visual fidelity, we introduce a unified pipeline that sequentially generates 3D semantic occupancy and the corresponding multiview images, while ensuring alignment between modalities. Additionally, we extend the generated local region into a large-scale scene through consistency-aware scene outpainting, which extrapolates new occupancy and images conditioned on the previously generated area, enhancing spatial continuity and preserving visual coherence. The resulting scenes are lifted into high-quality 3DGS representations, supporting diverse applications such as scene exploration. Comprehensive experiments demonstrate that X-Scene significantly advances controllability and fidelity for large-scale driving scene generation, empowering data generation and simulation for autonomous driving.", "AI": {"tldr": "X-Scene\u662f\u4e00\u4e2a\u7528\u4e8e\u5927\u89c4\u6a21\u9a7e\u9a76\u573a\u666f\u751f\u6210\u7684\u65b0\u6846\u67b6\uff0c\u652f\u6301\u591a\u7c92\u5ea6\u63a7\u5236\u548c\u51e0\u4f55\u4e0e\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u4e3b\u8981\u5173\u6ce8\u65f6\u95f4\u4e00\u81f4\u6027\u751f\u6210\uff0c\u800c\u5927\u89c4\u6a213D\u573a\u666f\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\u751f\u6210\u7814\u7a76\u4e0d\u8db3\u3002", "method": "X-Scene\u901a\u8fc7\u7edf\u4e00\u7ba1\u9053\u751f\u62103D\u8bed\u4e49\u5360\u636e\u548c\u591a\u89c6\u56fe\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u4e00\u81f4\u6027\u611f\u77e5\u7684\u573a\u666f\u5916\u63a8\u6269\u5c55\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cX-Scene\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u9a7e\u9a76\u573a\u666f\u751f\u6210\u7684\u53ef\u63a7\u6027\u548c\u4fdd\u771f\u5ea6\u3002", "conclusion": "X-Scene\u4e3a\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u751f\u6210\u548c\u6a21\u62df\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.13589", "pdf": "https://arxiv.org/pdf/2506.13589", "abs": "https://arxiv.org/abs/2506.13589", "authors": ["Zhucun Xue", "Jiangning Zhang", "Xurong Xie", "Yuxuan Cai", "Yong Liu", "Xiangtai Li", "Dacheng Tao"], "title": "Omni-AdaVideoRAG: Omni-Contextual Adaptive Retrieval-Augmented for Efficient Long Video Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) struggle with long videos due to fixed context windows and weak long-term dependency modeling. Existing Retrieval-Augmented Generation (RAG) methods for videos use static retrieval strategies, leading to inefficiencies for simple queries and information loss for complex tasks. To address this, we propose AdaVideoRAG, a novel framework that dynamically adapts retrieval granularity based on query complexity using a lightweight intent classifier. Our framework employs an Omni-Knowledge Indexing module to build hierarchical databases from text (captions, ASR, OCR), visual features, and semantic graphs, enabling optimal resource allocation across tasks. We also introduce the HiVU benchmark for comprehensive evaluation. Experiments demonstrate improved efficiency and accuracy for long-video understanding, with seamless integration into existing MLLMs. AdaVideoRAG establishes a new paradigm for adaptive retrieval in video analysis. Codes will be open-sourced at https://github.com/xzc-zju/AdaVideoRAG.", "AI": {"tldr": "AdaVideoRAG\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u68c0\u7d22\u7c92\u5ea6\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u610f\u56fe\u5206\u7c7b\u5668\u4f18\u5316\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7b80\u5355\u67e5\u8be2\u548c\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6548\u7387\u4e0e\u4fe1\u606f\u635f\u5931\u95ee\u9898\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u56e0\u56fa\u5b9a\u4e0a\u4e0b\u6587\u7a97\u53e3\u548c\u5f31\u957f\u671f\u4f9d\u8d56\u5efa\u6a21\u80fd\u529b\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\u91c7\u7528\u9759\u6001\u68c0\u7d22\u7b56\u7565\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u548c\u4fe1\u606f\u4e22\u5931\u3002", "method": "\u63d0\u51faAdaVideoRAG\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u610f\u56fe\u5206\u7c7b\u5668\u52a8\u6001\u8c03\u6574\u68c0\u7d22\u7c92\u5ea6\uff0c\u5e76\u5229\u7528\u5168\u77e5\u8bc6\u7d22\u5f15\u6a21\u5757\u6784\u5efa\u5206\u5c42\u6570\u636e\u5e93\uff08\u6587\u672c\u3001\u89c6\u89c9\u7279\u5f81\u3001\u8bed\u4e49\u56fe\uff09\uff0c\u5b9e\u73b0\u4efb\u52a1\u8d44\u6e90\u6700\u4f18\u5206\u914d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAdaVideoRAG\u5728\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5e76\u80fd\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709MLLMs\u4e2d\u3002", "conclusion": "AdaVideoRAG\u4e3a\u89c6\u9891\u5206\u6790\u4e2d\u7684\u81ea\u9002\u5e94\u68c0\u7d22\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2506.13691", "pdf": "https://arxiv.org/pdf/2506.13691", "abs": "https://arxiv.org/abs/2506.13691", "authors": ["Zhucun Xue", "Jiangning Zhang", "Teng Hu", "Haoyang He", "Yinan Chen", "Yuxuan Cai", "Yabiao Wang", "Chengjie Wang", "Yong Liu", "Xiangtai Li", "Dacheng Tao"], "title": "UltraVideo: High-Quality UHD Video Dataset with Comprehensive Captions", "categories": ["cs.CV"], "comment": null, "summary": "The quality of the video dataset (image quality, resolution, and fine-grained caption) greatly influences the performance of the video generation model. The growing demand for video applications sets higher requirements for high-quality video generation models. For example, the generation of movie-level Ultra-High Definition (UHD) videos and the creation of 4K short video content. However, the existing public datasets cannot support related research and applications. In this paper, we first propose a high-quality open-sourced UHD-4K (22.4\\% of which are 8K) text-to-video dataset named UltraVideo, which contains a wide range of topics (more than 100 kinds), and each video has 9 structured captions with one summarized caption (average of 824 words). Specifically, we carefully design a highly automated curation process with four stages to obtain the final high-quality dataset: \\textit{i)} collection of diverse and high-quality video clips. \\textit{ii)} statistical data filtering. \\textit{iii)} model-based data purification. \\textit{iv)} generation of comprehensive, structured captions. In addition, we expand Wan to UltraWan-1K/-4K, which can natively generate high-quality 1K/4K videos with more consistent text controllability, demonstrating the effectiveness of our data curation.We believe that this work can make a significant contribution to future research on UHD video generation. UltraVideo dataset and UltraWan models are available at https://xzc-zju.github.io/projects/UltraVideo.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u9ad8\u8d28\u91cfUHD-4K\u6587\u672c\u5230\u89c6\u9891\u6570\u636e\u96c6UltraVideo\uff0c\u5e76\u6269\u5c55UltraWan\u6a21\u578b\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf1K/4K\u89c6\u9891\u3002", "motivation": "\u73b0\u6709\u516c\u5171\u6570\u636e\u96c6\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u7814\u7a76\u9700\u6c42\uff0c\u5982\u7535\u5f71\u7ea7UHD\u89c6\u9891\u548c4K\u77ed\u89c6\u9891\u5185\u5bb9\u3002", "method": "\u8bbe\u8ba1\u56db\u9636\u6bb5\u81ea\u52a8\u5316\u6570\u636e\u7b5b\u9009\u6d41\u7a0b\uff1a\u89c6\u9891\u7247\u6bb5\u6536\u96c6\u3001\u7edf\u8ba1\u8fc7\u6ee4\u3001\u6a21\u578b\u51c0\u5316\u3001\u751f\u6210\u7ed3\u6784\u5316\u5b57\u5e55\u3002", "result": "\u6784\u5efa\u4e86UltraVideo\u6570\u636e\u96c6\uff08\u542b8K\u89c6\u989122.4%\uff09\uff0c\u5e76\u6269\u5c55UltraWan\u6a21\u578b\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002", "conclusion": "UltraVideo\u548cUltraWan\u4e3aUHD\u89c6\u9891\u751f\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002"}}
{"id": "2506.12184", "pdf": "https://arxiv.org/pdf/2506.12184", "abs": "https://arxiv.org/abs/2506.12184", "authors": ["Stanley Lewis", "Vishal Chandra", "Tom Gao", "Odest Chadwicke Jenkins"], "title": "SPLATART: Articulated Gaussian Splatting with Estimated Object Structure", "categories": ["cs.RO", "cs.CV"], "comment": "7 pages, Accepted to the 2025 RSS Workshop on Gaussian   Representations for Robot Autonomy. Contact: Stanley Lewis, stanlew@umich.edu", "summary": "Representing articulated objects remains a difficult problem within the field of robotics. Objects such as pliers, clamps, or cabinets require representations that capture not only geometry and color information, but also part seperation, connectivity, and joint parametrization. Furthermore, learning these representations becomes even more difficult with each additional degree of freedom. Complex articulated objects such as robot arms may have seven or more degrees of freedom, and the depth of their kinematic tree may be notably greater than the tools, drawers, and cabinets that are the typical subjects of articulated object research. To address these concerns, we introduce SPLATART - a pipeline for learning Gaussian splat representations of articulated objects from posed images, of which a subset contains image space part segmentations. SPLATART disentangles the part separation task from the articulation estimation task, allowing for post-facto determination of joint estimation and representation of articulated objects with deeper kinematic trees than previously exhibited. In this work, we present data on the SPLATART pipeline as applied to the syntheic Paris dataset objects, and qualitative results on a real-world object under spare segmentation supervision. We additionally present on articulated serial chain manipulators to demonstrate usage on deeper kinematic tree structures.", "AI": {"tldr": "SPLATART\u662f\u4e00\u79cd\u4ece\u5e26\u59ff\u6001\u7684\u56fe\u50cf\u4e2d\u5b66\u4e60\u9ad8\u65af\u6e85\u5c04\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u590d\u6742\u94f0\u63a5\u7269\u4f53\u7684\u8868\u793a\u95ee\u9898\uff0c\u652f\u6301\u66f4\u6df1\u5c42\u6b21\u7684\u8fd0\u52a8\u6811\u7ed3\u6784\u3002", "motivation": "\u94f0\u63a5\u7269\u4f53\uff08\u5982\u94b3\u5b50\u3001\u5939\u5b50\u6216\u6a71\u67dc\uff09\u7684\u8868\u793a\u4e0d\u4ec5\u9700\u8981\u51e0\u4f55\u548c\u989c\u8272\u4fe1\u606f\uff0c\u8fd8\u9700\u8981\u90e8\u5206\u5206\u79bb\u3001\u8fde\u63a5\u6027\u548c\u5173\u8282\u53c2\u6570\u5316\u3002\u968f\u7740\u81ea\u7531\u5ea6\u589e\u52a0\uff0c\u5b66\u4e60\u8fd9\u4e9b\u8868\u793a\u53d8\u5f97\u66f4\u52a0\u56f0\u96be\u3002", "method": "SPLATART\u901a\u8fc7\u5206\u79bb\u90e8\u5206\u5206\u5272\u4efb\u52a1\u548c\u5173\u8282\u4f30\u8ba1\u4efb\u52a1\uff0c\u4ece\u5e26\u59ff\u6001\u7684\u56fe\u50cf\u4e2d\u5b66\u4e60\u9ad8\u65af\u6e85\u5c04\u8868\u793a\uff0c\u652f\u6301\u66f4\u6df1\u5c42\u6b21\u7684\u8fd0\u52a8\u6811\u7ed3\u6784\u3002", "result": "\u5728\u5408\u6210Paris\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86SPLATART\u7684\u6548\u679c\uff0c\u5e76\u5728\u771f\u5b9e\u7269\u4f53\u4e0a\u63d0\u4f9b\u4e86\u7a00\u758f\u5206\u5272\u76d1\u7763\u7684\u5b9a\u6027\u7ed3\u679c\uff0c\u8fd8\u5c55\u793a\u4e86\u5728\u66f4\u6df1\u8fd0\u52a8\u6811\u7ed3\u6784\uff08\u5982\u4e32\u8054\u673a\u68b0\u81c2\uff09\u4e0a\u7684\u5e94\u7528\u3002", "conclusion": "SPLATART\u4e3a\u89e3\u51b3\u590d\u6742\u94f0\u63a5\u7269\u4f53\u7684\u8868\u793a\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8fd0\u52a8\u6811\u7ed3\u6784\u8f83\u6df1\u7684\u573a\u666f\u3002"}}
{"id": "2506.12269", "pdf": "https://arxiv.org/pdf/2506.12269", "abs": "https://arxiv.org/abs/2506.12269", "authors": ["Babak Naderi", "Ross Cutler", "Juhee Cho", "Nabakumar Khongbantabam", "Dejan Ivkovic"], "title": "ICME 2025 Grand Challenge on Video Super-Resolution for Video Conferencing", "categories": ["eess.IV", "cs.CV", "cs.MM"], "comment": null, "summary": "Super-Resolution (SR) is a critical task in computer vision, focusing on reconstructing high-resolution (HR) images from low-resolution (LR) inputs. The field has seen significant progress through various challenges, particularly in single-image SR. Video Super-Resolution (VSR) extends this to the temporal domain, aiming to enhance video quality using methods like local, uni-, bi-directional propagation, or traditional upscaling followed by restoration. This challenge addresses VSR for conferencing, where LR videos are encoded with H.265 at fixed QPs. The goal is to upscale videos by a specific factor, providing HR outputs with enhanced perceptual quality under a low-delay scenario using causal models. The challenge included three tracks: general-purpose videos, talking head videos, and screen content videos, with separate datasets provided by the organizers for training, validation, and testing. We open-sourced a new screen content dataset for the SR task in this challenge. Submissions were evaluated through subjective tests using a crowdsourced implementation of the ITU-T Rec P.910.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u89c6\u9891\u8d85\u5206\u8fa8\u7387\uff08VSR\uff09\u5728\u4f1a\u8bae\u573a\u666f\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u7814\u7a76\u4e86\u4f4e\u5ef6\u8fdf\u6761\u4ef6\u4e0b\u901a\u8fc7\u56e0\u679c\u6a21\u578b\u63d0\u5347\u89c6\u9891\u8d28\u91cf\u7684\u65b9\u6cd5\uff0c\u5e76\u5f00\u6e90\u4e86\u4e00\u4e2a\u65b0\u7684\u5c4f\u5e55\u5185\u5bb9\u6570\u636e\u96c6\u3002", "motivation": "\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u5728\u4f1a\u8bae\u573a\u666f\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u4f4e\u5ef6\u8fdf\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u4ecd\u6709\u5f85\u63d0\u5347\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u56e0\u679c\u6a21\u578b\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u5c40\u90e8\u3001\u5355\u5411\u3001\u53cc\u5411\u4f20\u64ad\u7b49VSR\u65b9\u6cd5\uff0c\u5e76\u5728H.265\u7f16\u7801\u7684\u4f4e\u5206\u8fa8\u7387\u89c6\u9891\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002\u6311\u6218\u8d5b\u5206\u4e3a\u4e09\u4e2a\u8d5b\u9053\uff0c\u5206\u522b\u9488\u5bf9\u4e0d\u540c\u7c7b\u578b\u89c6\u9891\u3002", "result": "\u6311\u6218\u8d5b\u901a\u8fc7\u4e3b\u89c2\u6d4b\u8bd5\u8bc4\u4f30\u4e86\u63d0\u4ea4\u7684\u6a21\u578b\uff0c\u5e76\u5f00\u6e90\u4e86\u4e00\u4e2a\u65b0\u7684\u5c4f\u5e55\u5185\u5bb9\u6570\u636e\u96c6\uff0c\u4e3aVSR\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u8d44\u6e90\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u5728\u4f1a\u8bae\u573a\u666f\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u548c\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u4e86\u4f4e\u5ef6\u8fdf\u6761\u4ef6\u4e0bVSR\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.12344", "pdf": "https://arxiv.org/pdf/2506.12344", "abs": "https://arxiv.org/abs/2506.12344", "authors": ["Haoyu Zhai", "Shuo Wang", "Pirouz Naghavi", "Qingying Hao", "Gang Wang"], "title": "Restoring Gaussian Blurred Face Images for Deanonymization Attacks", "categories": ["cs.CR", "cs.CV"], "comment": "18 pages, 16 figures, IEEE Transaction format", "summary": "Gaussian blur is widely used to blur human faces in sensitive photos before the photos are posted on the Internet. However, it is unclear to what extent the blurred faces can be restored and used to re-identify the person, especially under a high-blurring setting. In this paper, we explore this question by developing a deblurring method called Revelio. The key intuition is to leverage a generative model's memorization effect and approximate the inverse function of Gaussian blur for face restoration. Compared with existing methods, we design the deblurring process to be identity-preserving. It uses a conditional Diffusion model for preliminary face restoration and then uses an identity retrieval model to retrieve related images to further enhance fidelity. We evaluate Revelio with large public face image datasets and show that it can effectively restore blurred faces, especially under a high-blurring setting. It has a re-identification accuracy of 95.9%, outperforming existing solutions. The result suggests that Gaussian blur should not be used for face anonymization purposes. We also demonstrate the robustness of this method against mismatched Gaussian kernel sizes and functions, and test preliminary countermeasures and adaptive attacks to inspire future work.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRevelio\u7684\u53bb\u6a21\u7cca\u65b9\u6cd5\uff0c\u7528\u4e8e\u6062\u590d\u9ad8\u65af\u6a21\u7cca\u7684\u4eba\u8138\u56fe\u50cf\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728\u9ad8\u6a21\u7cca\u8bbe\u7f6e\u4e0b\u7684\u6709\u6548\u6027\u3002", "motivation": "\u63a2\u8ba8\u9ad8\u65af\u6a21\u7cca\u5904\u7406\u540e\u7684\u4eba\u8138\u56fe\u50cf\u80fd\u5426\u88ab\u6709\u6548\u6062\u590d\u5e76\u7528\u4e8e\u91cd\u65b0\u8bc6\u522b\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u6a21\u7cca\u8bbe\u7f6e\u4e0b\u3002", "method": "\u7ed3\u5408\u751f\u6210\u6a21\u578b\u7684\u8bb0\u5fc6\u6548\u5e94\u548c\u6761\u4ef6\u6269\u6563\u6a21\u578b\u8fdb\u884c\u521d\u6b65\u6062\u590d\uff0c\u518d\u901a\u8fc7\u8eab\u4efd\u68c0\u7d22\u6a21\u578b\u589e\u5f3a\u4fdd\u771f\u5ea6\u3002", "result": "Revelio\u5728\u9ad8\u6a21\u7cca\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u91cd\u65b0\u8bc6\u522b\u51c6\u786e\u7387\u8fbe95.9%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u9ad8\u65af\u6a21\u7cca\u4e0d\u9002\u7528\u4e8e\u4eba\u8138\u533f\u540d\u5316\uff0c\u672a\u6765\u9700\u63a2\u7d22\u66f4\u5b89\u5168\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2506.12411", "pdf": "https://arxiv.org/pdf/2506.12411", "abs": "https://arxiv.org/abs/2506.12411", "authors": ["Mengyuan Sun", "Yu Li", "Yuchen Liu", "Bo Du", "Yunjie Ge"], "title": "InverTune: Removing Backdoors from Multimodal Contrastive Learning Models via Trigger Inversion and Activation Tuning", "categories": ["cs.CR", "cs.CV"], "comment": null, "summary": "Multimodal contrastive learning models like CLIP have demonstrated remarkable vision-language alignment capabilities, yet their vulnerability to backdoor attacks poses critical security risks. Attackers can implant latent triggers that persist through downstream tasks, enabling malicious control of model behavior upon trigger presentation. Despite great success in recent defense mechanisms, they remain impractical due to strong assumptions about attacker knowledge or excessive clean data requirements. In this paper, we introduce InverTune, the first backdoor defense framework for multimodal models under minimal attacker assumptions, requiring neither prior knowledge of attack targets nor access to the poisoned dataset. Unlike existing defense methods that rely on the same dataset used in the poisoning stage, InverTune effectively identifies and removes backdoor artifacts through three key components, achieving robust protection against backdoor attacks. Specifically, InverTune first exposes attack signatures through adversarial simulation, probabilistically identifying the target label by analyzing model response patterns. Building on this, we develop a gradient inversion technique to reconstruct latent triggers through activation pattern analysis. Finally, a clustering-guided fine-tuning strategy is employed to erase the backdoor function with only a small amount of arbitrary clean data, while preserving the original model capabilities. Experimental results show that InverTune reduces the average attack success rate (ASR) by 97.87% against the state-of-the-art (SOTA) attacks while limiting clean accuracy (CA) degradation to just 3.07%. This work establishes a new paradigm for securing multimodal systems, advancing security in foundation model deployment without compromising performance.", "AI": {"tldr": "InverTune\u662f\u4e00\u79cd\u9488\u5bf9\u591a\u6a21\u6001\u6a21\u578b\u7684\u540e\u95e8\u9632\u5fa1\u6846\u67b6\uff0c\u65e0\u9700\u653b\u51fb\u8005\u77e5\u8bc6\u6216\u4e2d\u6bd2\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5bf9\u6297\u6a21\u62df\u3001\u68af\u5ea6\u53cd\u6f14\u548c\u805a\u7c7b\u5fae\u8c03\uff0c\u663e\u8457\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6a21\u578b\uff08\u5982CLIP\uff09\u6613\u53d7\u540e\u95e8\u653b\u51fb\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u56e0\u5047\u8bbe\u8fc7\u5f3a\u6216\u6570\u636e\u9700\u6c42\u9ad8\u800c\u4e0d\u5b9e\u7528\u3002", "method": "InverTune\u901a\u8fc7\u5bf9\u6297\u6a21\u62df\u66b4\u9732\u653b\u51fb\u7279\u5f81\uff0c\u68af\u5ea6\u53cd\u6f14\u91cd\u5efa\u6f5c\u5728\u89e6\u53d1\u5668\uff0c\u805a\u7c7b\u5fae\u8c03\u6d88\u9664\u540e\u95e8\u529f\u80fd\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cInverTune\u5c06\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e97.87%\uff0c\u4ec5\u635f\u59313.07%\u7684\u5e72\u51c0\u6570\u636e\u51c6\u786e\u7387\u3002", "conclusion": "InverTune\u4e3a\u591a\u6a21\u6001\u7cfb\u7edf\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5728\u4e0d\u5f71\u54cd\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u5b89\u5168\u6027\u3002"}}
{"id": "2506.12475", "pdf": "https://arxiv.org/pdf/2506.12475", "abs": "https://arxiv.org/abs/2506.12475", "authors": ["Fangwei Hao", "Ji Du", "Desheng Kong", "Jiesheng Wu", "Jing Xu", "Ping Li"], "title": "Efficient Star Distillation Attention Network for Lightweight Image Super-Resolution", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "In recent years, the performance of lightweight Single-Image Super-Resolution (SISR) has been improved significantly with the application of Convolutional Neural Networks (CNNs) and Large Kernel Attention (LKA). However, existing information distillation modules for lightweight SISR struggle to map inputs into High-Dimensional Non-Linear (HDNL) feature spaces, limiting their representation learning. And their LKA modules possess restricted ability to capture the multi-shape multi-scale information for long-range dependencies while encountering a quadratic increase in the computational burden with increasing convolutional kernel size of its depth-wise convolutional layer. To address these issues, we firstly propose a Star Distillation Module (SDM) to enhance the discriminative representation learning via information distillation in the HDNL feature spaces. Besides, we present a Multi-shape Multi-scale Large Kernel Attention (MM-LKA) module to learn representative long-range dependencies while incurring low computational and memory footprints, leading to improving the performance of CNN-based self-attention significantly. Integrating SDM and MM-LKA, we develop a Residual Star Distillation Attention Module (RSDAM) and take it as the building block of the proposed efficient Star Distillation Attention Network (SDAN) which possesses high reconstruction efficiency to recover a higher-quality image from the corresponding low-resolution (LR) counterpart. When compared with other lightweight state-of-the-art SISR methods, extensive experiments show that our SDAN with low model complexity yields superior performance quantitatively and visually.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cdStar Distillation Module (SDM)\u548cMulti-shape Multi-scale Large Kernel Attention (MM-LKA)\u6a21\u5757\uff0c\u7ed3\u5408\u4e3aRSDAM\uff0c\u6784\u5efa\u4e86\u9ad8\u6548\u7684SDAN\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f7b\u91cf\u7ea7\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08SISR\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8f7b\u91cf\u7ea7SISR\u65b9\u6cd5\u5728\u4fe1\u606f\u84b8\u998f\u548c\u9ad8\u7ef4\u975e\u7ebf\u6027\u7279\u5f81\u7a7a\u95f4\u6620\u5c04\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u4e14LKA\u6a21\u5757\u5728\u6355\u6349\u591a\u5f62\u72b6\u591a\u5c3a\u5ea6\u4fe1\u606f\u548c\u957f\u7a0b\u4f9d\u8d56\u65f6\u8ba1\u7b97\u8d1f\u62c5\u5927\u3002", "method": "\u63d0\u51faSDM\u589e\u5f3a\u9ad8\u7ef4\u975e\u7ebf\u6027\u7279\u5f81\u7a7a\u95f4\u7684\u4fe1\u606f\u84b8\u998f\uff0c\u8bbe\u8ba1MM-LKA\u6a21\u5757\u9ad8\u6548\u6355\u6349\u957f\u7a0b\u4f9d\u8d56\uff0c\u7ed3\u5408\u4e3aRSDAM\u6a21\u5757\uff0c\u6784\u5efaSDAN\u7f51\u7edc\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSDAN\u5728\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u4e0b\uff0c\u5b9a\u91cf\u548c\u89c6\u89c9\u4e0a\u5747\u4f18\u4e8e\u5176\u4ed6\u8f7b\u91cf\u7ea7SISR\u65b9\u6cd5\u3002", "conclusion": "SDAN\u901a\u8fc7SDM\u548cMM-LKA\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f7b\u91cf\u7ea7SISR\u7684\u6027\u80fd\u548c\u91cd\u5efa\u6548\u7387\u3002"}}
{"id": "2506.12693", "pdf": "https://arxiv.org/pdf/2506.12693", "abs": "https://arxiv.org/abs/2506.12693", "authors": ["Ali Zafari", "Xi Chen", "Shirin Jalali"], "title": "Zero-shot denoising via neural compression: Theoretical and algorithmic framework", "categories": ["eess.IV", "cs.CV", "cs.IT", "math.IT"], "comment": null, "summary": "Zero-shot denoising aims to denoise observations without access to training samples or clean reference images. This setting is particularly relevant in practical imaging scenarios involving specialized domains such as medical imaging or biology. In this work, we propose the Zero-Shot Neural Compression Denoiser (ZS-NCD), a novel denoising framework based on neural compression. ZS-NCD treats a neural compression network as an untrained model, optimized directly on patches extracted from a single noisy image. The final reconstruction is then obtained by aggregating the outputs of the trained model over overlapping patches. Thanks to the built-in entropy constraints of compression architectures, our method naturally avoids overfitting and does not require manual regularization or early stopping. Through extensive experiments, we show that ZS-NCD achieves state-of-the-art performance among zero-shot denoisers for both Gaussian and Poisson noise, and generalizes well to both natural and non-natural images. Additionally, we provide new finite-sample theoretical results that characterize upper bounds on the achievable reconstruction error of general maximum-likelihood compression-based denoisers. These results further establish the theoretical foundations of compression-based denoising. Our code is available at: github.com/Computational-Imaging-RU/ZS-NCDenoiser.", "AI": {"tldr": "ZS-NCD\u662f\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u538b\u7f29\u7684\u96f6\u6837\u672c\u53bb\u566a\u6846\u67b6\uff0c\u65e0\u9700\u8bad\u7ec3\u6837\u672c\u6216\u5e72\u51c0\u53c2\u8003\u56fe\u50cf\uff0c\u9002\u7528\u4e8e\u533b\u5b66\u6216\u751f\u7269\u7b49\u4e13\u4e1a\u9886\u57df\u3002", "motivation": "\u89e3\u51b3\u96f6\u6837\u672c\u53bb\u566a\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u533b\u5b66\u6216\u751f\u7269\u7b49\u4e13\u4e1a\u9886\u57df\u4e2d\u7f3a\u4e4f\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u3002", "method": "\u5229\u7528\u795e\u7ecf\u538b\u7f29\u7f51\u7edc\u4f5c\u4e3a\u672a\u8bad\u7ec3\u6a21\u578b\uff0c\u76f4\u63a5\u5728\u5355\u4e2a\u566a\u58f0\u56fe\u50cf\u7684\u63d0\u53d6\u5757\u4e0a\u8fdb\u884c\u4f18\u5316\uff0c\u5e76\u901a\u8fc7\u805a\u5408\u91cd\u53e0\u5757\u7684\u8f93\u51fa\u6765\u91cd\u5efa\u56fe\u50cf\u3002", "result": "ZS-NCD\u5728\u96f6\u6837\u672c\u53bb\u566a\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u9ad8\u65af\u548c\u6cca\u677e\u566a\u58f0\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u81ea\u7136\u548c\u975e\u81ea\u7136\u56fe\u50cf\u3002", "conclusion": "ZS-NCD\u901a\u8fc7\u5185\u7f6e\u71b5\u7ea6\u675f\u907f\u514d\u8fc7\u62df\u5408\uff0c\u65e0\u9700\u624b\u52a8\u6b63\u5219\u5316\u6216\u63d0\u524d\u505c\u6b62\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u538b\u7f29\u53bb\u566a\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2506.12719", "pdf": "https://arxiv.org/pdf/2506.12719", "abs": "https://arxiv.org/abs/2506.12719", "authors": ["Hu Xu", "Yang Jingling", "Jia Sihan", "Bi Yuda", "Calhoun Vince"], "title": "GM-LDM: Latent Diffusion Model for Brain Biomarker Identification through Functional Data-Driven Gray Matter Synthesis", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Generative models based on deep learning have shown significant potential in medical imaging, particularly for modality transformation and multimodal fusion in MRI-based brain imaging. This study introduces GM-LDM, a novel framework that leverages the latent diffusion model (LDM) to enhance the efficiency and precision of MRI generation tasks. GM-LDM integrates a 3D autoencoder, pre-trained on the large-scale ABCD MRI dataset, achieving statistical consistency through KL divergence loss. We employ a Vision Transformer (ViT)-based encoder-decoder as the denoising network to optimize generation quality. The framework flexibly incorporates conditional data, such as functional network connectivity (FNC) data, enabling personalized brain imaging, biomarker identification, and functional-to-structural information translation for brain diseases like schizophrenia.", "AI": {"tldr": "GM-LDM\u662f\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347MRI\u751f\u6210\u4efb\u52a1\u7684\u6548\u7387\u548c\u7cbe\u5ea6\uff0c\u652f\u6301\u4e2a\u6027\u5316\u8111\u6210\u50cf\u548c\u529f\u80fd\u5230\u7ed3\u6784\u4fe1\u606f\u7684\u8f6c\u6362\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u751f\u6210\u6a21\u578b\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u9700\u8981\u66f4\u9ad8\u6548\u7684MRI\u751f\u6210\u65b9\u6cd5\u4ee5\u652f\u6301\u4e2a\u6027\u5316\u533b\u7597\u548c\u75be\u75c5\u7814\u7a76\u3002", "method": "\u7ed3\u54083D\u81ea\u52a8\u7f16\u7801\u5668\u548c\u9884\u8bad\u7ec3ViT\u7f16\u7801\u5668-\u89e3\u7801\u5668\uff0c\u5229\u7528KL\u6563\u5ea6\u635f\u5931\u5b9e\u73b0\u7edf\u8ba1\u4e00\u81f4\u6027\uff0c\u5e76\u7075\u6d3b\u6574\u5408\u6761\u4ef6\u6570\u636e\uff08\u5982FNC\uff09\u3002", "result": "GM-LDM\u80fd\u591f\u9ad8\u6548\u751f\u6210MRI\u56fe\u50cf\uff0c\u652f\u6301\u4e2a\u6027\u5316\u8111\u6210\u50cf\u548c\u75be\u75c5\uff08\u5982\u7cbe\u795e\u5206\u88c2\u75c7\uff09\u7684\u751f\u7269\u6807\u5fd7\u7269\u8bc6\u522b\u3002", "conclusion": "GM-LDM\u4e3aMRI\u751f\u6210\u548c\u529f\u80fd-\u7ed3\u6784\u4fe1\u606f\u8f6c\u6362\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.13187", "pdf": "https://arxiv.org/pdf/2506.13187", "abs": "https://arxiv.org/abs/2506.13187", "authors": ["Yibo Yang", "Sihao Liu", "Chuan Rao", "Bang An", "Tiancheng Shen", "Philip H. S. Torr", "Ming-Hsuan Yang", "Bernard Ghanem"], "title": "Dynamic Context-oriented Decomposition for Task-aware Low-rank Adaptation with Less Forgetting and Faster Convergence", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Conventional low-rank adaptation methods build adapters without considering data context, leading to sub-optimal fine-tuning performance and severe forgetting of inherent world knowledge. In this paper, we propose context-oriented decomposition adaptation (CorDA), a novel method that initializes adapters in a task-aware manner. Concretely, we develop context-oriented singular value decomposition, where we collect covariance matrices of input activations for each linear layer using sampled data from the target task, and apply SVD to the product of weight matrix and its corresponding covariance matrix. By doing so, the task-specific capability is compacted into the principal components. Thanks to the task awareness, our method enables two optional adaptation modes, knowledge-preserved mode (KPM) and instruction-previewed mode (IPM), providing flexibility to choose between freezing the principal components to preserve their associated knowledge or adapting them to better learn a new task. We further develop CorDA++ by deriving a metric that reflects the compactness of task-specific principal components, and then introducing dynamic covariance selection and dynamic rank allocation strategies based on the same metric. The two strategies provide each layer with the most representative covariance matrix and a proper rank allocation. Experimental results show that CorDA++ outperforms CorDA by a significant margin. CorDA++ in KPM not only achieves better fine-tuning performance than LoRA, but also mitigates the forgetting of pre-trained knowledge in both large language models and vision language models. For IPM, our method exhibits faster convergence, \\emph{e.g.,} 4.5x speedup over QLoRA, and improves adaptation performance in various scenarios, outperforming strong baseline methods. Our method has been integrated into the PEFT library developed by Hugging Face.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCorDA\u7684\u65b0\u578b\u4f4e\u79e9\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u7684\u65b9\u5f0f\u521d\u59cb\u5316\u9002\u914d\u5668\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u56e0\u5ffd\u7565\u6570\u636e\u4e0a\u4e0b\u6587\u5bfc\u81f4\u7684\u6027\u80fd\u4e0d\u4f73\u548c\u77e5\u8bc6\u9057\u5fd8\u95ee\u9898\u3002\u8fdb\u4e00\u6b65\u4f18\u5316\u7684CorDA++\u901a\u8fc7\u52a8\u6001\u7b56\u7565\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u4f4e\u79e9\u9002\u5e94\u65b9\u6cd5\u4e0d\u8003\u8651\u6570\u636e\u4e0a\u4e0b\u6587\uff0c\u5bfc\u81f4\u5fae\u8c03\u6027\u80fd\u4e0d\u4f73\u548c\u77e5\u8bc6\u9057\u5fd8\u3002", "method": "\u63d0\u51fa\u4e0a\u4e0b\u6587\u5bfc\u5411\u7684\u5947\u5f02\u503c\u5206\u89e3\uff08CorDA\uff09\uff0c\u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u521d\u59cb\u5316\u9002\u914d\u5668\uff0c\u5e76\u5f00\u53d1\u52a8\u6001\u7b56\u7565\u4f18\u5316\uff08CorDA++\uff09\u3002", "result": "CorDA++\u5728\u77e5\u8bc6\u4fdd\u7559\u6a21\u5f0f\uff08KPM\uff09\u548c\u6307\u4ee4\u9884\u89c8\u6a21\u5f0f\uff08IPM\uff09\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8eLoRA\u548cQLoRA\u7b49\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CorDA++\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u79e9\u9002\u5e94\u7684\u6027\u80fd\uff0c\u5e76\u88ab\u96c6\u6210\u5230Hugging Face\u7684PEFT\u5e93\u4e2d\u3002"}}
{"id": "2506.13443", "pdf": "https://arxiv.org/pdf/2506.13443", "abs": "https://arxiv.org/abs/2506.13443", "authors": ["Kang Chen", "Bin Huang", "Xuebin Yang", "Junyan Zhang", "Qiegen Liu"], "title": "PRO: Projection Domain Synthesis for CT Imaging", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Synthesizing high quality CT images remains a signifi-cant challenge due to the limited availability of annotat-ed data and the complex nature of CT imaging. In this work, we present PRO, a novel framework that, to the best of our knowledge, is the first to perform CT image synthesis in the projection domain using latent diffusion models. Unlike previous approaches that operate in the image domain, PRO learns rich structural representa-tions from raw projection data and leverages anatomi-cal text prompts for controllable synthesis. This projec-tion domain strategy enables more faithful modeling of underlying imaging physics and anatomical structures. Moreover, PRO functions as a foundation model, capa-ble of generalizing across diverse downstream tasks by adjusting its generative behavior via prompt inputs. Experimental results demonstrated that incorporating our synthesized data significantly improves perfor-mance across multiple downstream tasks, including low-dose and sparse-view reconstruction, even with limited training data. These findings underscore the versatility and scalability of PRO in data generation for various CT applications. These results highlight the potential of projection domain synthesis as a powerful tool for data augmentation and robust CT imaging. Our source code is publicly available at: https://github.com/yqx7150/PRO.", "AI": {"tldr": "PRO\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u9996\u6b21\u5728\u6295\u5f71\u57df\u4e2d\u4f7f\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5408\u6210\u9ad8\u8d28\u91cfCT\u56fe\u50cf\uff0c\u901a\u8fc7\u89e3\u5256\u5b66\u6587\u672c\u63d0\u793a\u5b9e\u73b0\u53ef\u63a7\u5408\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3CT\u56fe\u50cf\u5408\u6210\u4e2d\u6807\u6ce8\u6570\u636e\u6709\u9650\u548c\u6210\u50cf\u590d\u6742\u6027\u9ad8\u7684\u6311\u6218\u3002", "method": "\u5728\u6295\u5f71\u57df\u4e2d\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5b66\u4e60\u7ed3\u6784\u8868\u793a\uff0c\u7ed3\u5408\u89e3\u5256\u5b66\u6587\u672c\u63d0\u793a\u8fdb\u884c\u53ef\u63a7\u5408\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5408\u6210\u6570\u636e\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u5242\u91cf\u548c\u7a00\u758f\u89c6\u56fe\u91cd\u5efa\u7b49\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "PRO\u5c55\u793a\u4e86\u6295\u5f71\u57df\u5408\u6210\u5728\u6570\u636e\u589e\u5f3a\u548cCT\u6210\u50cf\u4e2d\u7684\u6f5c\u529b\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2506.13579", "pdf": "https://arxiv.org/pdf/2506.13579", "abs": "https://arxiv.org/abs/2506.13579", "authors": ["Andrew Zhang", "Anushka Sivakumar", "Chiawei Tang", "Chris Thomas"], "title": "Flexible-length Text Infilling for Discrete Diffusion Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Discrete diffusion models are a new class of text generators that offer advantages such as bidirectional context use, parallelizable generation, and flexible prompting compared to autoregressive models. However, a critical limitation of discrete diffusion models is their inability to perform flexible-length or flexible-position text infilling without access to ground-truth positional data. We introduce \\textbf{DDOT} (\\textbf{D}iscrete \\textbf{D}iffusion with \\textbf{O}ptimal \\textbf{T}ransport Position Coupling), the first discrete diffusion model to overcome this challenge. DDOT jointly denoises token values and token positions, employing a novel sample-level Optimal Transport (OT) coupling. This coupling preserves relative token ordering while dynamically adjusting the positions and length of infilled segments, a capability previously missing in text diffusion. Our method is orthogonal to existing discrete text diffusion methods and is compatible with various pretrained text denoisers. Extensive experiments on text infilling benchmarks such as One-Billion-Word and Yelp demonstrate that DDOT outperforms naive diffusion baselines. Furthermore, DDOT achieves performance on par with state-of-the-art non-autoregressive models and enables significant improvements in training efficiency and flexibility.", "AI": {"tldr": "DDOT\u662f\u4e00\u79cd\u65b0\u7684\u79bb\u6563\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u8054\u5408\u53bb\u566a\u6807\u8bb0\u503c\u548c\u4f4d\u7f6e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u65e0\u6cd5\u7075\u6d3b\u586b\u5145\u6587\u672c\u957f\u5ea6\u548c\u4f4d\u7f6e\u7684\u9650\u5236\u3002", "motivation": "\u79bb\u6563\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u751f\u6210\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u65e0\u6cd5\u7075\u6d3b\u586b\u5145\u6587\u672c\u957f\u5ea6\u548c\u4f4d\u7f6e\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "DDOT\u901a\u8fc7\u6837\u672c\u7ea7\u6700\u4f18\u4f20\u8f93\u8026\u5408\uff0c\u8054\u5408\u53bb\u566a\u6807\u8bb0\u503c\u548c\u4f4d\u7f6e\uff0c\u52a8\u6001\u8c03\u6574\u586b\u5145\u6bb5\u7684\u4f4d\u7f6e\u548c\u957f\u5ea6\u3002", "result": "DDOT\u5728\u6587\u672c\u586b\u5145\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u975e\u81ea\u56de\u5f52\u6a21\u578b\u6027\u80fd\u76f8\u5f53\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "DDOT\u662f\u9996\u4e2a\u514b\u670d\u79bb\u6563\u6269\u6563\u6a21\u578b\u7075\u6d3b\u586b\u5145\u9650\u5236\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.13614", "pdf": "https://arxiv.org/pdf/2506.13614", "abs": "https://arxiv.org/abs/2506.13614", "authors": ["Gregory Bellchambers"], "title": "Exploiting the Exact Denoising Posterior Score in Training-Free Guidance of Diffusion Models", "categories": ["stat.ML", "cs.CV", "cs.LG"], "comment": null, "summary": "The success of diffusion models has driven interest in performing conditional sampling via training-free guidance of the denoising process to solve image restoration and other inverse problems. A popular class of methods, based on Diffusion Posterior Sampling (DPS), attempts to approximate the intractable posterior score function directly. In this work, we present a novel expression for the exact posterior score for purely denoising tasks that is tractable in terms of the unconditional score function. We leverage this result to analyze the time-dependent error in the DPS score for denoising tasks and compute step sizes on the fly to minimize the error at each time step. We demonstrate that these step sizes are transferable to related inverse problems such as colorization, random inpainting, and super resolution. Despite its simplicity, this approach is competitive with state-of-the-art techniques and enables sampling with fewer time steps than DPS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7cbe\u786e\u540e\u9a8c\u8bc4\u5206\u8868\u8fbe\u5f0f\uff0c\u7528\u4e8e\u7eaf\u53bb\u566a\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6b65\u957f\u6765\u6700\u5c0f\u5316\u8bef\u5dee\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u9006\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u6761\u4ef6\u91c7\u6837\u4e2d\u7684\u5e94\u7528\u5f15\u53d1\u4e86\u5174\u8da3\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\uff08\u5982DPS\uff09\u96be\u4ee5\u76f4\u63a5\u8fd1\u4f3c\u540e\u9a8c\u8bc4\u5206\u51fd\u6570\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7cbe\u786e\u540e\u9a8c\u8bc4\u5206\u8868\u8fbe\u5f0f\uff0c\u52a8\u6001\u8ba1\u7b97\u6b65\u957f\u4ee5\u6700\u5c0f\u5316\u8bef\u5dee\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728\u591a\u79cd\u9006\u95ee\u9898\u4e2d\u7684\u9002\u7528\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u53bb\u566a\u3001\u7740\u8272\u3001\u968f\u673a\u4fee\u590d\u548c\u8d85\u5206\u8fa8\u7387\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u91c7\u6837\u6b65\u6570\u5c11\u4e8eDPS\u3002", "conclusion": "\u5c3d\u7ba1\u65b9\u6cd5\u7b80\u5355\uff0c\u4f46\u80fd\u4e0e\u6700\u5148\u8fdb\u6280\u672f\u7ade\u4e89\uff0c\u5e76\u51cf\u5c11\u91c7\u6837\u65f6\u95f4\u3002"}}
{"id": "2506.13642", "pdf": "https://arxiv.org/pdf/2506.13642", "abs": "https://arxiv.org/abs/2506.13642", "authors": ["Shaolei Zhang", "Shoutao Guo", "Qingkai Fang", "Yan Zhou", "Yang Feng"], "title": "Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.SD", "eess.AS"], "comment": "Code: https://github.com/ictnlp/Stream-Omni , Model:   https://huggingface.co/ICTNLP/stream-omni-8b", "summary": "The emergence of GPT-4o-like large multimodal models (LMMs) has raised the exploration of integrating text, vision, and speech modalities to support more flexible multimodal interaction. Existing LMMs typically concatenate representation of modalities along the sequence dimension and feed them into a large language model (LLM) backbone. While sequence-dimension concatenation is straightforward for modality integration, it often relies heavily on large-scale data to learn modality alignments. In this paper, we aim to model the relationships between modalities more purposefully, thereby achieving more efficient and flexible modality alignments. To this end, we propose Stream-Omni, a large language-vision-speech model with efficient modality alignments, which can simultaneously support interactions under various modality combinations. Stream-Omni employs LLM as the backbone and aligns the vision and speech to the text based on their relationships. For vision that is semantically complementary to text, Stream-Omni uses sequence-dimension concatenation to achieve vision-text alignment. For speech that is semantically consistent with text, Stream-Omni introduces a CTC-based layer-dimension mapping to achieve speech-text alignment. In this way, Stream-Omni can achieve modality alignments with less data (especially speech), enabling the transfer of text capabilities to other modalities. Experiments on various benchmarks demonstrate that Stream-Omni achieves strong performance on visual understanding, speech interaction, and vision-grounded speech interaction tasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously provide intermediate text outputs (such as ASR transcriptions and model responses) during speech interaction, offering users a comprehensive multimodal experience.", "AI": {"tldr": "Stream-Omni\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u6a21\u6001\u5bf9\u9f50\u6a21\u578b\uff0c\u901a\u8fc7\u5e8f\u5217\u7ef4\u5ea6\u548c\u5c42\u7ef4\u5ea6\u6620\u5c04\u5b9e\u73b0\u6587\u672c\u3001\u89c6\u89c9\u548c\u8bed\u97f3\u7684\u7075\u6d3b\u4ea4\u4e92\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u901a\u5e38\u4f9d\u8d56\u5927\u89c4\u6a21\u6570\u636e\u5b66\u4e60\u6a21\u6001\u5bf9\u9f50\uff0c\u672c\u6587\u65e8\u5728\u66f4\u9ad8\u6548\u5730\u5efa\u6a21\u6a21\u6001\u5173\u7cfb\u3002", "method": "Stream-Omni\u57fa\u4e8eLLM\u4e3b\u5e72\uff0c\u901a\u8fc7\u5e8f\u5217\u7ef4\u5ea6\u5bf9\u9f50\u89c6\u89c9\u4e0e\u6587\u672c\uff0c\u901a\u8fc7CTC\u5c42\u7ef4\u5ea6\u6620\u5c04\u5bf9\u9f50\u8bed\u97f3\u4e0e\u6587\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660eStream-Omni\u5728\u89c6\u89c9\u7406\u89e3\u3001\u8bed\u97f3\u4ea4\u4e92\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u80fd\u63d0\u4f9b\u4e2d\u95f4\u6587\u672c\u8f93\u51fa\u3002", "conclusion": "Stream-Omni\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u6a21\u6001\u5bf9\u9f50\uff0c\u652f\u6301\u7075\u6d3b\u4ea4\u4e92\u5e76\u51cf\u5c11\u6570\u636e\u9700\u6c42\u3002"}}
{"id": "2506.13667", "pdf": "https://arxiv.org/pdf/2506.13667", "abs": "https://arxiv.org/abs/2506.13667", "authors": ["Bi Yuda", "Jia Sihan", "Gao Yutong", "Abrol Anees", "Fu Zening", "Calhoun Vince"], "title": "MultiViT2: A Data-augmented Multimodal Neuroimaging Prediction Framework via Latent Diffusion Model", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Multimodal medical imaging integrates diverse data types, such as structural and functional neuroimaging, to provide complementary insights that enhance deep learning predictions and improve outcomes. This study focuses on a neuroimaging prediction framework based on both structural and functional neuroimaging data. We propose a next-generation prediction model, \\textbf{MultiViT2}, which combines a pretrained representative learning base model with a vision transformer backbone for prediction output. Additionally, we developed a data augmentation module based on the latent diffusion model that enriches input data by generating augmented neuroimaging samples, thereby enhancing predictive performance through reduced overfitting and improved generalizability. We show that MultiViT2 significantly outperforms the first-generation model in schizophrenia classification accuracy and demonstrates strong scalability and portability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u533b\u5b66\u5f71\u50cf\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bMultiViT2\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u548c\u89c6\u89c9Transformer\uff0c\u5e76\u901a\u8fc7\u6f5c\u5728\u6269\u6563\u6a21\u578b\u589e\u5f3a\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cbe\u795e\u5206\u88c2\u75c7\u5206\u7c7b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u533b\u5b66\u5f71\u50cf\uff08\u5982\u7ed3\u6784\u548c\u529f\u80fd\u795e\u7ecf\u5f71\u50cf\uff09\u80fd\u63d0\u4f9b\u4e92\u8865\u4fe1\u606f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u9884\u6d4b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u4ecd\u6709\u4e0d\u8db3\u3002", "method": "\u63d0\u51faMultiViT2\u6a21\u578b\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u548c\u89c6\u89c9Transformer\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u6570\u636e\u589e\u5f3a\u6a21\u5757\u3002", "result": "MultiViT2\u5728\u7cbe\u795e\u5206\u88c2\u75c7\u5206\u7c7b\u4e2d\u663e\u8457\u4f18\u4e8e\u7b2c\u4e00\u4ee3\u6a21\u578b\uff0c\u5e76\u8868\u73b0\u51fa\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u79fb\u690d\u6027\u3002", "conclusion": "MultiViT2\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u548c\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.13754", "pdf": "https://arxiv.org/pdf/2506.13754", "abs": "https://arxiv.org/abs/2506.13754", "authors": ["Edward Li", "Zichen Wang", "Jiahe Huang", "Jeong Joon Park"], "title": "VideoPDE: Unified Generative PDE Solving via Video Inpainting Diffusion Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Submitted to NeurIPS 2025. Project page: https://videopde.github.io/", "summary": "We present a unified framework for solving partial differential equations (PDEs) using video-inpainting diffusion transformer models. Unlike existing methods that devise specialized strategies for either forward or inverse problems under full or partial observation, our approach unifies these tasks under a single, flexible generative framework. Specifically, we recast PDE-solving as a generalized inpainting problem, e.g., treating forward prediction as inferring missing spatiotemporal information of future states from initial conditions. To this end, we design a transformer-based architecture that conditions on arbitrary patterns of known data to infer missing values across time and space. Our method proposes pixel-space video diffusion models for fine-grained, high-fidelity inpainting and conditioning, while enhancing computational efficiency through hierarchical modeling. Extensive experiments show that our video inpainting-based diffusion model offers an accurate and versatile solution across a wide range of PDEs and problem setups, outperforming state-of-the-art baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u9891\u4fee\u590d\u6269\u6563\u53d8\u6362\u5668\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDEs\uff09\uff0c\u5c06\u524d\u5411\u548c\u9006\u5411\u95ee\u9898\u7edf\u4e00\u4e3a\u4e00\u4e2a\u7075\u6d3b\u7684\u751f\u6210\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u95ee\u9898\u8bbe\u8ba1\u7b56\u7565\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cdPDEs\u548c\u95ee\u9898\u8bbe\u7f6e\u3002", "method": "\u5c06PDE\u6c42\u89e3\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5e7f\u4e49\u4fee\u590d\u95ee\u9898\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u53d8\u6362\u5668\u7684\u67b6\u6784\uff0c\u5229\u7528\u50cf\u7d20\u7a7a\u95f4\u89c6\u9891\u6269\u6563\u6a21\u578b\u8fdb\u884c\u7cbe\u7ec6\u4fee\u590d\u548c\u6761\u4ef6\u63a8\u65ad\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cdPDEs\u548c\u95ee\u9898\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aPDE\u6c42\u89e3\u63d0\u4f9b\u4e86\u4e00\u4e2a\u51c6\u786e\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.13763", "pdf": "https://arxiv.org/pdf/2506.13763", "abs": "https://arxiv.org/abs/2506.13763", "authors": ["Yixian Xu", "Shengjie Luo", "Liwei Wang", "Di He", "Chang Liu"], "title": "Diagnosing and Improving Diffusion Models by Estimating the Optimal Loss Value", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "29 pages, 8 figures, 3 tables. Preprint. Work in Progress", "summary": "Diffusion models have achieved remarkable success in generative modeling. Despite more stable training, the loss of diffusion models is not indicative of absolute data-fitting quality, since its optimal value is typically not zero but unknown, leading to confusion between large optimal loss and insufficient model capacity. In this work, we advocate the need to estimate the optimal loss value for diagnosing and improving diffusion models. We first derive the optimal loss in closed form under a unified formulation of diffusion models, and develop effective estimators for it, including a stochastic variant scalable to large datasets with proper control of variance and bias. With this tool, we unlock the inherent metric for diagnosing the training quality of mainstream diffusion model variants, and develop a more performant training schedule based on the optimal loss. Moreover, using models with 120M to 1.5B parameters, we find that the power law is better demonstrated after subtracting the optimal loss from the actual training loss, suggesting a more principled setting for investigating the scaling law for diffusion models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f30\u8ba1\u6269\u6563\u6a21\u578b\u6700\u4f18\u635f\u5931\u503c\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bca\u65ad\u548c\u6539\u8fdb\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u7684\u635f\u5931\u503c\u65e0\u6cd5\u76f4\u63a5\u53cd\u6620\u6570\u636e\u62df\u5408\u8d28\u91cf\uff0c\u56e0\u4e3a\u5176\u6700\u4f18\u503c\u901a\u5e38\u672a\u77e5\uff0c\u8fd9\u5bfc\u81f4\u96be\u4ee5\u533a\u5206\u6a21\u578b\u5bb9\u91cf\u4e0d\u8db3\u4e0e\u6700\u4f18\u635f\u5931\u8f83\u5927\u7684\u60c5\u51b5\u3002", "method": "\u63a8\u5bfc\u4e86\u6269\u6563\u6a21\u578b\u6700\u4f18\u635f\u5931\u7684\u95ed\u5f0f\u89e3\uff0c\u5e76\u5f00\u53d1\u4e86\u6709\u6548\u7684\u4f30\u8ba1\u5668\uff0c\u5305\u62ec\u9002\u7528\u4e8e\u5927\u6570\u636e\u96c6\u7684\u53ef\u6269\u5c55\u968f\u673a\u53d8\u4f53\u3002", "result": "\u901a\u8fc7\u6700\u4f18\u635f\u5931\u5de5\u5177\uff0c\u6539\u8fdb\u4e86\u4e3b\u6d41\u6269\u6563\u6a21\u578b\u7684\u8bad\u7ec3\u8d28\u91cf\u8bca\u65ad\uff0c\u5e76\u5f00\u53d1\u4e86\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u8ba1\u5212\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u51cf\u53bb\u6700\u4f18\u635f\u5931\u540e\u80fd\u66f4\u597d\u5730\u5c55\u793a\u5e42\u5f8b\u5173\u7cfb\u3002", "conclusion": "\u4f30\u8ba1\u6700\u4f18\u635f\u5931\u4e3a\u6269\u6563\u6a21\u578b\u7684\u8bca\u65ad\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u540c\u65f6\u4e3a\u7814\u7a76\u6269\u6563\u6a21\u578b\u7684\u7f29\u653e\u5b9a\u5f8b\u63d0\u4f9b\u4e86\u66f4\u5408\u7406\u7684\u8bbe\u7f6e\u3002"}}
