<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 3]
- [cs.CV](#cs.CV) [Total: 23]
- [eess.IV](#eess.IV) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [STREAMINGGS: Voxel-Based Streaming 3D Gaussian Splatting with Memory Optimization and Architectural Support](https://arxiv.org/abs/2506.09070)
*Chenqi Zhang,Yu Feng,Jieru Zhao,Guangda Liu,Wenchao Ding,Chentao Wu,Minyi Guo*

Main category: cs.GR

TL;DR: STREAMINGGS通过算法-架构协同设计，解决了3DGS在移动设备上的实时性问题，显著提升了速度和能效。


<details>
  <summary>Details</summary>
Motivation: 3DGS在资源受限的移动设备上无法满足90 FPS的实时性要求，现有加速器忽视了内存效率。

Method: 提出STREAMINGGS，采用完全流式算法-架构协同设计，从基于瓦片的渲染转变为基于内存的渲染。

Result: 设计实现了45.7倍的速度提升和62.9倍的能效节省。

Conclusion: STREAMINGGS显著优化了3DGS在移动设备上的性能，解决了实时性和内存效率问题。

Abstract: 3D Gaussian Splatting (3DGS) has gained popularity for its efficiency and sparse Gaussian-based representation. However, 3DGS struggles to meet the real-time requirement of 90 frames per second (FPS) on resource-constrained mobile devices, achieving only 2 to 9 FPS.Existing accelerators focus on compute efficiency but overlook memory efficiency, leading to redundant DRAM traffic. We introduce STREAMINGGS, a fully streaming 3DGS algorithm-architecture co-design that achieves fine-grained pipelining and reduces DRAM traffic by transforming from a tile-centric rendering to a memory-centric rendering. Results show that our design achieves up to 45.7 $\times$ speedup and 62.9 $\times$ energy savings over mobile Ampere GPUs.

</details>


### [2] [VideoMat: Extracting PBR Materials from Video Diffusion Models](https://arxiv.org/abs/2506.09665)
*Jacob Munkberg,Zian Wang,Ruofan Liang,Tianchang Shen,Jon Hasselgren*

Main category: cs.GR

TL;DR: 利用视频扩散模型、视频内在分解和基于物理的可微分渲染，通过文本提示或单张图像为3D模型生成高质量材质。


<details>
  <summary>Details</summary>
Motivation: 解决从文本或单张图像生成3D模型材质的挑战，确保材质与几何和光照条件一致。

Method: 1. 使用视频扩散模型生成多视角一致的材质视频；2. 提取视频的内在属性（基础色、粗糙度、金属度）；3. 结合可微分路径追踪器生成PBR材质。

Result: 生成与常见内容创作工具兼容的高质量PBR材质。

Conclusion: 该方法通过结合扩散模型和物理渲染，实现了从文本或图像到3D模型材质的有效生成。

Abstract: We leverage finetuned video diffusion models, intrinsic decomposition of videos, and physically-based differentiable rendering to generate high quality materials for 3D models given a text prompt or a single image. We condition a video diffusion model to respect the input geometry and lighting condition. This model produces multiple views of a given 3D model with coherent material properties. Secondly, we use a recent model to extract intrinsics (base color, roughness, metallic) from the generated video. Finally, we use the intrinsics alongside the generated video in a differentiable path tracer to robustly extract PBR materials directly compatible with common content creation tools.

</details>


### [3] [TransGI: Real-Time Dynamic Global Illumination With Object-Centric Neural Transfer Model](https://arxiv.org/abs/2506.09909)
*Yijie Deng,Lei Han,Lu Fang*

Main category: cs.GR

TL;DR: TransGI是一种新型神经渲染方法，用于实时高保真全局光照，通过对象中心神经传输模型和辐射共享照明系统实现高效渲染。


<details>
  <summary>Details</summary>
Motivation: 解决现有神经渲染算法在实时渲染和任意光照条件下的局限性，特别是材料表示的紧凑性和表达能力问题。

Method: 采用基于MLP的解码器和顶点附着潜在特征的对象中心神经传输模型，以及局部光探针和跨探针辐射共享策略。

Result: 实验结果显示，每帧渲染时间小于10毫秒，渲染质量显著优于基线方法。

Conclusion: TransGI在实时性和渲染质量之间取得了平衡，适用于动态光照条件下的高效渲染。

Abstract: Neural rendering algorithms have revolutionized computer graphics, yet their impact on real-time rendering under arbitrary lighting conditions remains limited due to strict latency constraints in practical applications. The key challenge lies in formulating a compact yet expressive material representation. To address this, we propose TransGI, a novel neural rendering method for real-time, high-fidelity global illumination. It comprises an object-centric neural transfer model for material representation and a radiance-sharing lighting system for efficient illumination. Traditional BSDF representations and spatial neural material representations lack expressiveness, requiring thousands of ray evaluations to converge to noise-free colors. Conversely, real-time methods trade quality for efficiency by supporting only diffuse materials. In contrast, our object-centric neural transfer model achieves compactness and expressiveness through an MLP-based decoder and vertex-attached latent features, supporting glossy effects with low memory overhead. For dynamic, varying lighting conditions, we introduce local light probes capturing scene radiance, coupled with an across-probe radiance-sharing strategy for efficient probe generation. We implemented our method in a real-time rendering engine, combining compute shaders and CUDA-based neural networks. Experimental results demonstrate that our method achieves real-time performance of less than 10 ms to render a frame and significantly improved rendering quality compared to baseline methods.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [4] [FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation](https://arxiv.org/abs/2506.09081)
*Zheqi He,Yesheng Liu,Jing-shu Zheng,Xuejing Li,Richeng Xuan,Jin-Ge Yao,Xi Yang*

Main category: cs.CV

TL;DR: FlagEvalMM是一个开源的多模态模型评估框架，支持多种视觉-语言任务，通过独立评估服务和高效工具提升评估效率。


<details>
  <summary>Details</summary>
Motivation: 为多模态研究提供一个全面、灵活且高效的评估工具，以促进模型性能的准确分析。

Method: 通过独立评估服务解耦模型推理与评估，利用vLLM、SGLang等工具加速推理，并采用异步数据加载提高效率。

Result: 实验表明FlagEvalMM能准确高效地分析模型优缺点，适用于多模态研究。

Conclusion: FlagEvalMM是一个有价值的开源工具，可推动多模态研究的进展。

Abstract: We present FlagEvalMM, an open-source evaluation framework designed to comprehensively assess multimodal models across a diverse range of vision-language understanding and generation tasks, such as visual question answering, text-to-image/video generation, and image-text retrieval. We decouple model inference from evaluation through an independent evaluation service, thus enabling flexible resource allocation and seamless integration of new tasks and models. Moreover, FlagEvalMM utilizes advanced inference acceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to significantly enhance evaluation efficiency. Extensive experiments show that FlagEvalMM offers accurate and efficient insights into model strengths and limitations, making it a valuable tool for advancing multimodal research. The framework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.

</details>


### [5] [Seedance 1.0: Exploring the Boundaries of Video Generation Models](https://arxiv.org/abs/2506.09113)
*Yu Gao,Haoyuan Guo,Tuyen Hoang,Weilin Huang,Lu Jiang,Fangyuan Kong,Huixia Li,Jiashi Li,Liang Li,Xiaojie Li,Xunsong Li,Yifu Li,Shanchuan Lin,Zhijie Lin,Jiawei Liu,Shu Liu,Xiaonan Nie,Zhiwu Qing,Yuxi Ren,Li Sun,Zhi Tian,Rui Wang,Sen Wang,Guoqiang Wei,Guohong Wu,Jie Wu,Ruiqi Xia,Fei Xiao,Xuefeng Xiao,Jiangqiao Yan,Ceyuan Yang,Jianchao Yang,Runkai Yang,Tao Yang,Yihang Yang,Zilyu Ye,Xuejiao Zeng,Yan Zeng,Heng Zhang,Yang Zhao,Xiaozheng Zheng,Peihao Zhu,Jiaxin Zou,Feilong Zuo*

Main category: cs.CV

TL;DR: Seedance 1.0是一款高性能、推理高效的视频生成基础模型，通过多源数据增强、高效架构设计、优化后训练方法和模型加速技术，显著提升了视频生成的提示跟随、运动合理性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型在视频生成中难以平衡提示跟随、运动合理性和视觉质量，Seedance 1.0旨在解决这一问题。

Method: 采用多源数据增强、高效架构设计、优化后训练方法（如精细监督微调和视频特定RLHF）及多阶段蒸馏策略加速模型。

Result: Seedance 1.0在1080p分辨率下仅需41.4秒生成5秒视频，具有高质量、快速生成、时空流畅性和结构稳定性。

Conclusion: Seedance 1.0在复杂多主体场景中表现出色，支持多镜头叙事连贯性，是当前视频生成领域的先进模型。

Abstract: Notable breakthroughs in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still face critical challenges in simultaneously balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0, a high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient architecture design with proposed training paradigm, which allows for natively supporting multi-shot generation and jointly learning of both text-to-video and image-to-video tasks. (iii) carefully-optimized post-training approaches leveraging fine-grained supervised fine-tuning, and video-specific RLHF with multi-dimensional reward mechanisms for comprehensive performance improvements; (iv) excellent model acceleration achieving ~10x inference speedup through multi-stage distillation strategies and system-level optimizations. Seedance 1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds (NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation having superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation.

</details>


### [6] [Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models](https://arxiv.org/abs/2506.09229)
*Sungwon Hwang,Hyojin Jang,Kinam Kim,Minho Park,Jaegul choo*

Main category: cs.CV

TL;DR: 本文提出了一种新的正则化技术CREPA，用于改进视频扩散模型的微调，解决了跨帧语义一致性的问题。


<details>
  <summary>Details</summary>
Motivation: 用户级微调视频扩散模型以生成反映训练数据特定属性的视频具有挑战性且未充分探索，但实际意义重大。

Method: 首先尝试将REPA技术直接应用于视频扩散模型，发现其效果有限；随后提出CREPA，通过将帧的隐藏状态与相邻帧的外部特征对齐来改进语义一致性。

Result: 在CogVideoX-5B和Hunyuan Video等大规模视频扩散模型上的实验表明，CREPA显著提升了视觉保真度和跨帧语义连贯性。

Conclusion: CREPA是一种广泛适用的技术，能够有效提升视频扩散模型的微调效果。

Abstract: Fine-tuning Video Diffusion Models (VDMs) at the user level to generate videos that reflect specific attributes of training data presents notable challenges, yet remains underexplored despite its practical importance. Meanwhile, recent work such as Representation Alignment (REPA) has shown promise in improving the convergence and quality of DiT-based image diffusion models by aligning, or assimilating, its internal hidden states with external pretrained visual features, suggesting its potential for VDM fine-tuning. In this work, we first propose a straightforward adaptation of REPA for VDMs and empirically show that, while effective for convergence, it is suboptimal in preserving semantic consistency across frames. To address this limitation, we introduce Cross-frame Representation Alignment (CREPA), a novel regularization technique that aligns hidden states of a frame with external features from neighboring frames. Empirical evaluations on large-scale VDMs, including CogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual fidelity and cross-frame semantic coherence when fine-tuned with parameter-efficient methods such as LoRA. We further validate CREPA across diverse datasets with varying attributes, confirming its broad applicability. Project page: https://crepavideo.github.io

</details>


### [7] [Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation](https://arxiv.org/abs/2506.09350)
*Shanchuan Lin,Ceyuan Yang,Hao He,Jianwen Jiang,Yuxi Ren,Xin Xia,Yang Zhao,Xuefeng Xiao,Lu Jiang*

Main category: cs.CV

TL;DR: 提出了一种自回归对抗后训练方法（AAPT），将预训练的潜在视频扩散模型转化为实时交互式视频生成器。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模视频生成模型计算量大，无法满足实时和交互式应用的需求。

Method: 采用自回归方式逐帧生成潜在帧，利用单次神经函数评估（1NFE）实现高效生成，并通过对抗训练优化架构。

Result: 8B模型在单H100上实现736x416分辨率、24fps的实时视频生成，或在8xH100上实现1280x720分辨率的长达一分钟的视频生成。

Conclusion: AAPT方法通过对抗训练和高效架构设计，显著提升了视频生成的实时性和交互性。

Abstract: Existing large-scale video generation models are computationally intensive, preventing adoption in real-time and interactive applications. In this work, we propose autoregressive adversarial post-training (AAPT) to transform a pre-trained latent video diffusion model into a real-time, interactive video generator. Our model autoregressively generates a latent frame at a time using a single neural function evaluation (1NFE). The model can stream the result to the user in real time and receive interactive responses as controls to generate the next latent frame. Unlike existing approaches, our method explores adversarial training as an effective paradigm for autoregressive generation. This not only allows us to design an architecture that is more efficient for one-step generation while fully utilizing the KV cache, but also enables training the model in a student-forcing manner that proves to be effective in reducing error accumulation during long video generation. Our experiments demonstrate that our 8B model achieves real-time, 24fps, streaming video generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to a minute long (1440 frames). Visit our research website at https://seaweed-apt.com/2

</details>


### [8] [SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing](https://arxiv.org/abs/2506.09363)
*Hongguang Zhu,Yunchao Wei,Mengyu Wang,Siyu Jiao,Yan Fang,Jiannan Huang,Yao Zhao*

Main category: cs.CV

TL;DR: SAGE提出了一种语义增强擦除方法，通过循环自检和自擦除将概念词擦除转化为概念域擦除，同时结合全局-局部协作保留机制，提升无关概念的保留效果，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在文本到图像生成中表现优异，但预训练中可能包含敏感信息，导致安全风险。现有方法将不安全概念视为固定词反复擦除，陷入“词概念深渊”，限制了泛化擦除能力。

Method: 提出语义增强擦除（SAGE），通过循环自检和自擦除将概念词擦除转化为概念域擦除；结合全局-局部协作保留机制，平衡擦除与保留。

Result: 实验表明SAGE在安全生成方面全面优于其他方法，且无需额外预处理数据。

Conclusion: SAGE通过语义空间关系和协作机制，有效解决了概念擦除的泛化问题，提升了扩散模型的安全性。

Abstract: Diffusion models (DMs) have achieved significant progress in text-to-image generation. However, the inevitable inclusion of sensitive information during pre-training poses safety risks, such as unsafe content generation and copyright infringement. Concept erasing finetunes weights to unlearn undesirable concepts, and has emerged as a promising solution. However, existing methods treat unsafe concept as a fixed word and repeatedly erase it, trapping DMs in ``word concept abyss'', which prevents generalized concept-related erasing. To escape this abyss, we introduce semantic-augment erasing which transforms concept word erasure into concept domain erasure by the cyclic self-check and self-erasure. It efficiently explores and unlearns the boundary representation of concept domain through semantic spatial relationships between original and training DMs, without requiring additional preprocessed data. Meanwhile, to mitigate the retention degradation of irrelevant concepts while erasing unsafe concepts, we further propose the global-local collaborative retention mechanism that combines global semantic relationship alignment with local predicted noise preservation, effectively expanding the retentive receptive field for irrelevant concepts. We name our method SAGE, and extensive experiments demonstrate the comprehensive superiority of SAGE compared with other methods in the safe generation of DMs. The code and weights will be open-sourced at https://github.com/KevinLight831/SAGE.

</details>


### [9] [UniForward: Unified 3D Scene and Semantic Field Reconstruction via Feed-Forward Gaussian Splatting from Only Sparse-View Images](https://arxiv.org/abs/2506.09378)
*Qijian Tian,Xin Tan,Jingyu Gong,Yuan Xie,Lizhuang Ma*

Main category: cs.CV

TL;DR: 提出了一种前馈高斯泼溅模型UniForward，通过仅使用未校准和未定位的稀疏视图图像，统一重建3D场景和语义场。


<details>
  <summary>Details</summary>
Motivation: 结合3D场景与语义场以提升环境感知与理解能力，解决嵌入语义到3D表示、实现通用实时重建及仅用图像输入的实用性问题。

Method: 嵌入语义特征到3D高斯中，通过双分支解耦解码器预测；提出损失引导视图采样器，无需深度或掩码真值；使用光度损失和蒸馏损失端到端训练。

Result: 实时从稀疏视图图像重建高质量3D场景和语义场，支持任意视角一致语义特征渲染和开放词汇密集分割掩码解码。

Conclusion: UniForward在新视角合成和分割任务中表现优异，实现了3D场景与语义场重建的统一。

Abstract: We propose a feed-forward Gaussian Splatting model that unifies 3D scene and semantic field reconstruction. Combining 3D scenes with semantic fields facilitates the perception and understanding of the surrounding environment. However, key challenges include embedding semantics into 3D representations, achieving generalizable real-time reconstruction, and ensuring practical applicability by using only images as input without camera parameters or ground truth depth. To this end, we propose UniForward, a feed-forward model to predict 3D Gaussians with anisotropic semantic features from only uncalibrated and unposed sparse-view images. To enable the unified representation of the 3D scene and semantic field, we embed semantic features into 3D Gaussians and predict them through a dual-branch decoupled decoder. During training, we propose a loss-guided view sampler to sample views from easy to hard, eliminating the need for ground truth depth or masks required by previous methods and stabilizing the training process. The whole model can be trained end-to-end using a photometric loss and a distillation loss that leverages semantic features from a pre-trained 2D semantic model. At the inference stage, our UniForward can reconstruct 3D scenes and the corresponding semantic fields in real time from only sparse-view images. The reconstructed 3D scenes achieve high-quality rendering, and the reconstructed 3D semantic field enables the rendering of view-consistent semantic features from arbitrary views, which can be further decoded into dense segmentation masks in an open-vocabulary manner. Experiments on novel view synthesis and novel view segmentation demonstrate that our method achieves state-of-the-art performances for unifying 3D scene and semantic field reconstruction.

</details>


### [10] [Noise Conditional Variational Score Distillation](https://arxiv.org/abs/2506.09416)
*Xinyu Peng,Ziyang Zheng,Yaoming Wang,Han Li,Nuowen Kan,Wenrui Dai,Chenglin Li,Junni Zou,Hongkai Xiong*

Main category: cs.CV

TL;DR: NCVSD是一种新方法，将预训练的扩散模型蒸馏为生成去噪器，通过揭示无条件评分函数隐含地表征去噪后验分布的评分函数。该方法支持快速生成和迭代优化，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究如何将预训练的扩散模型高效地蒸馏为生成去噪器，同时保留快速生成和迭代优化的优势。

Method: 通过将无条件评分函数的洞察整合到VSD框架中，实现生成去噪器的可扩展学习，支持多步采样和零-shot概率推理。

Result: 实验表明，NCVSD在类条件图像生成和逆问题求解中表现优异，优于教师扩散模型，并与更大规模的Consistency模型相当。

Conclusion: NCVSD是一种高效的生成去噪器学习方法，兼具快速生成和高质量采样的优势，适用于多种任务。

Abstract: We propose Noise Conditional Variational Score Distillation (NCVSD), a novel method for distilling pretrained diffusion models into generative denoisers. We achieve this by revealing that the unconditional score function implicitly characterizes the score function of denoising posterior distributions. By integrating this insight into the Variational Score Distillation (VSD) framework, we enable scalable learning of generative denoisers capable of approximating samples from the denoising posterior distribution across a wide range of noise levels. The proposed generative denoisers exhibit desirable properties that allow fast generation while preserve the benefit of iterative refinement: (1) fast one-step generation through sampling from pure Gaussian noise at high noise levels; (2) improved sample quality by scaling the test-time compute with multi-step sampling; and (3) zero-shot probabilistic inference for flexible and controllable sampling. We evaluate NCVSD through extensive experiments, including class-conditional image generation and inverse problem solving. By scaling the test-time compute, our method outperforms teacher diffusion models and is on par with consistency models of larger sizes. Additionally, with significantly fewer NFEs than diffusion-based methods, we achieve record-breaking LPIPS on inverse problems.

</details>


### [11] [TinySplat: Feedforward Approach for Generating Compact 3D Scene Representation](https://arxiv.org/abs/2506.09479)
*Zetian Song,Jiaye Fu,Jiaqi Zhang,Xiaohan Lu,Chuanmin Jia,Siwei Ma,Wen Gao*

Main category: cs.CV

TL;DR: TinySplat提出了一种无需训练的压缩框架，显著降低了3D高斯数据的存储成本，同时保持高质量重建。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯泼溅（3DGS）方法存储成本高，且传统压缩方法因架构不兼容无法适用。

Method: TinySplat通过View-Projection Transformation（VPT）减少几何冗余，Visibility-Aware Basis Reduction（VABR）降低感知冗余，并结合视频编解码器处理空间冗余。

Result: 实验表明，TinySplat实现了100倍以上的压缩比，存储大小仅为现有方法的6%，编码和解码时间分别减少75%和99%。

Conclusion: TinySplat是一种高效的3D场景表示压缩方法，适用于实时应用。

Abstract: The recent development of feedforward 3D Gaussian Splatting (3DGS) presents a new paradigm to reconstruct 3D scenes. Using neural networks trained on large-scale multi-view datasets, it can directly infer 3DGS representations from sparse input views. Although the feedforward approach achieves high reconstruction speed, it still suffers from the substantial storage cost of 3D Gaussians. Existing 3DGS compression methods relying on scene-wise optimization are not applicable due to architectural incompatibilities. To overcome this limitation, we propose TinySplat, a complete feedforward approach for generating compact 3D scene representations. Built upon standard feedforward 3DGS methods, TinySplat integrates a training-free compression framework that systematically eliminates key sources of redundancy. Specifically, we introduce View-Projection Transformation (VPT) to reduce geometric redundancy by projecting geometric parameters into a more compact space. We further present Visibility-Aware Basis Reduction (VABR), which mitigates perceptual redundancy by aligning feature energy along dominant viewing directions via basis transformation. Lastly, spatial redundancy is addressed through an off-the-shelf video codec. Comprehensive experimental results on multiple benchmark datasets demonstrate that TinySplat achieves over 100x compression for 3D Gaussian data generated by feedforward methods. Compared to the state-of-the-art compression approach, we achieve comparable quality with only 6% of the storage size. Meanwhile, our compression framework requires only 25% of the encoding time and 1% of the decoding time.

</details>


### [12] [Marrying Autoregressive Transformer and Diffusion with Multi-Reference Autoregression](https://arxiv.org/abs/2506.09482)
*Dingcheng Zhen,Qian Qiao,Tan Yu,Kangxi Wu,Ziwei Zhang,Siyuan Liu,Shunshun Yin,Ming Tao*

Main category: cs.CV

TL;DR: TransDiff结合自回归Transformer和扩散模型，显著提升图像生成性能，并引入多参考自回归（MRAR）进一步优化。


<details>
  <summary>Details</summary>
Motivation: 结合自回归Transformer和扩散模型的优势，提升图像生成的质量和效率。

Method: 使用扩散模型估计图像样本分布，并引入MRAR范式通过多参考生成图像。

Result: 在ImageNet 256x256上，FID为1.61，IS为293.4，推理速度显著快于其他方法；MRAR将FID降至1.42。

Conclusion: TransDiff为图像生成领域开辟了新方向，MRAR进一步提升了性能。

Abstract: We introduce TransDiff, the first image generation model that marries Autoregressive (AR) Transformer with diffusion models. In this joint modeling framework, TransDiff encodes labels and images into high-level semantic features and employs a diffusion model to estimate the distribution of image samples. On the ImageNet 256x256 benchmark, TransDiff significantly outperforms other image generation models based on standalone AR Transformer or diffusion models. Specifically, TransDiff achieves a Fr\'echet Inception Distance (FID) of 1.61 and an Inception Score (IS) of 293.4, and further provides x2 faster inference latency compared to state-of-the-art methods based on AR Transformer and x112 faster inference compared to diffusion-only models. Furthermore, building on the TransDiff model, we introduce a novel image generation paradigm called Multi-Reference Autoregression (MRAR), which performs autoregressive generation by predicting the next image. MRAR enables the model to reference multiple previously generated images, thereby facilitating the learning of more diverse representations and improving the quality of generated images in subsequent iterations. By applying MRAR, the performance of TransDiff is improved, with the FID reduced from 1.61 to 1.42. We expect TransDiff to open up a new frontier in the field of image generation.

</details>


### [13] [Generalized Gaussian Entropy Model for Point Cloud Attribute Compression with Dynamic Likelihood Intervals](https://arxiv.org/abs/2506.09510)
*Changhao Peng,Yuqi Ye,Wei Gao*

Main category: cs.CV

TL;DR: 论文提出了一种广义高斯熵模型和动态调整似然区间的方法，显著提升了点云属性压缩的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在熵参数估计中未充分利用信息，且固定似然区间限制了模型性能。

Method: 引入广义高斯熵模型控制尾部形状，并提出均值误差判别器（MED）动态调整似然区间。

Result: 实验表明，该方法在三种基于VAE的点云属性压缩模型中显著提升了率失真性能。

Conclusion: 该方法不仅适用于点云压缩，还可推广至图像和视频压缩任务。

Abstract: Gaussian and Laplacian entropy models are proved effective in learned point cloud attribute compression, as they assist in arithmetic coding of latents. However, we demonstrate through experiments that there is still unutilized information in entropy parameters estimated by neural networks in current methods, which can be used for more accurate probability estimation. Thus we introduce generalized Gaussian entropy model, which controls the tail shape through shape parameter to more accurately estimate the probability of latents. Meanwhile, to the best of our knowledge, existing methods use fixed likelihood intervals for each integer during arithmetic coding, which limits model performance. We propose Mean Error Discriminator (MED) to determine whether the entropy parameter estimation is accurate and then dynamically adjust likelihood intervals. Experiments show that our method significantly improves rate-distortion (RD) performance on three VAE-based models for point cloud attribute compression, and our method can be applied to other compression tasks, such as image and video compression.

</details>


### [14] [HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for Dynamic Scene](https://arxiv.org/abs/2506.09518)
*Jianing Chen,Zehao Li,Yujun Cai,Hao Jiang,Chengxuan Qian,Juyuan Kang,Shuqin Gao,Honglong Zhao,Tianlu Mao,Yucheng Zhang*

Main category: cs.CV

TL;DR: HAIF-GS提出了一种基于稀疏锚点驱动的动态3D场景重建框架，解决了现有方法在冗余高斯更新、运动监督不足和非刚性变形建模弱的问题。


<details>
  <summary>Details</summary>
Motivation: 动态3D场景重建是3D视觉中的基础挑战，现有方法在动态建模中存在冗余更新、运动监督不足和复杂变形建模弱的问题。

Method: HAIF-GS通过锚点过滤器识别运动相关区域，利用自监督的流引导变形模块和多级锚点传播机制，实现高效且一致的动态建模。

Result: 实验表明，HAIF-GS在渲染质量、时间一致性和重建效率上显著优于现有动态3DGS方法。

Conclusion: HAIF-GS为动态3D场景重建提供了一种高效且一致的解决方案。

Abstract: Reconstructing dynamic 3D scenes from monocular videos remains a fundamental challenge in 3D vision. While 3D Gaussian Splatting (3DGS) achieves real-time rendering in static settings, extending it to dynamic scenes is challenging due to the difficulty of learning structured and temporally consistent motion representations. This challenge often manifests as three limitations in existing methods: redundant Gaussian updates, insufficient motion supervision, and weak modeling of complex non-rigid deformations. These issues collectively hinder coherent and efficient dynamic reconstruction. To address these limitations, we propose HAIF-GS, a unified framework that enables structured and consistent dynamic modeling through sparse anchor-driven deformation. It first identifies motion-relevant regions via an Anchor Filter to suppresses redundant updates in static areas. A self-supervised Induced Flow-Guided Deformation module induces anchor motion using multi-frame feature aggregation, eliminating the need for explicit flow labels. To further handle fine-grained deformations, a Hierarchical Anchor Propagation mechanism increases anchor resolution based on motion complexity and propagates multi-level transformations. Extensive experiments on synthetic and real-world benchmarks validate that HAIF-GS significantly outperforms prior dynamic 3DGS methods in rendering quality, temporal coherence, and reconstruction efficiency.

</details>


### [15] [Gaussian Herding across Pens: An Optimal Transport Perspective on Global Gaussian Reduction for 3DGS](https://arxiv.org/abs/2506.09534)
*Tao Wang,Mengyu Li,Geduo Zeng,Cheng Meng,Qiong Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于最优传输的3D高斯分布压缩方法，显著减少冗余高斯基元，同时保持渲染质量。


<details>
  <summary>Details</summary>
Motivation: 3D高斯分布渲染技术（3DGS）通常需要大量冗余高斯基元，占用过多内存和渲染资源，现有压缩方法缺乏全局保真度保证。

Method: 采用最优传输视角，将3DGS压缩问题转化为全局高斯混合缩减问题，通过KD树分区最小化复合传输散度，并分离几何与外观属性进行微调。

Result: 实验表明，该方法仅需10%的高斯基元即可达到与原始3DGS相当的渲染质量（PSNR、SSIM、LPIPS），且优于现有压缩技术。

Conclusion: 该方法适用于任何3DGS流程，提供了一种高效且通用的轻量级神经渲染解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for radiance field rendering, but it typically requires millions of redundant Gaussian primitives, overwhelming memory and rendering budgets. Existing compaction approaches address this by pruning Gaussians based on heuristic importance scores, without global fidelity guarantee. To bridge this gap, we propose a novel optimal transport perspective that casts 3DGS compaction as global Gaussian mixture reduction. Specifically, we first minimize the composite transport divergence over a KD-tree partition to produce a compact geometric representation, and then decouple appearance from geometry by fine-tuning color and opacity attributes with far fewer Gaussian primitives. Experiments on benchmark datasets show that our method (i) yields negligible loss in rendering quality (PSNR, SSIM, LPIPS) compared to vanilla 3DGS with only 10% Gaussians; and (ii) consistently outperforms state-of-the-art 3DGS compaction techniques. Notably, our method is applicable to any stage of vanilla or accelerated 3DGS pipelines, providing an efficient and agnostic pathway to lightweight neural rendering.

</details>


### [16] [Consistent Story Generation with Asymmetry Zigzag Sampling](https://arxiv.org/abs/2506.09612)
*Mingxiao LI,mang ning,Marie-Francine Moens*

Main category: cs.CV

TL;DR: 提出了一种无需训练的新型采样策略“Zigzag Sampling with Asymmetric Prompts and Visual Sharing”，用于提升视觉故事生成中的主题一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在保持多图像主题一致性方面效果有限，且资源消耗大。

Method: 采用Zigzag采样机制，结合非对称提示和视觉共享模块，以增强主题一致性。

Result: 实验表明，该方法在生成连贯一致的视觉故事上显著优于现有方法。

Conclusion: 该方法为视觉故事生成提供了一种高效且无需训练的新解决方案。

Abstract: Text-to-image generation models have made significant progress in producing high-quality images from textual descriptions, yet they continue to struggle with maintaining subject consistency across multiple images, a fundamental requirement for visual storytelling. Existing methods attempt to address this by either fine-tuning models on large-scale story visualization datasets, which is resource-intensive, or by using training-free techniques that share information across generations, which still yield limited success. In this paper, we introduce a novel training-free sampling strategy called Zigzag Sampling with Asymmetric Prompts and Visual Sharing to enhance subject consistency in visual story generation. Our approach proposes a zigzag sampling mechanism that alternates between asymmetric prompting to retain subject characteristics, while a visual sharing module transfers visual cues across generated images to %further enforce consistency. Experimental results, based on both quantitative metrics and qualitative evaluations, demonstrate that our method significantly outperforms previous approaches in generating coherent and consistent visual stories. The code is available at https://github.com/Mingxiao-Li/Asymmetry-Zigzag-StoryDiffusion.

</details>


### [17] [DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning](https://arxiv.org/abs/2506.09644)
*Dongxu Liu,Yuang Peng,Haomiao Tang,Yuwei Chen,Chunrui Han,Zheng Ge,Daxin Jiang,Mingxue Liao*

Main category: cs.CV

TL;DR: DGAE通过扩散模型引导解码器，解决高压缩比下自编码器的性能下降问题，同时减少潜在空间维度，提升生成模型的效率和紧凑性。


<details>
  <summary>Details</summary>
Motivation: 解决自编码器在高压缩比下性能下降及GAN训练不稳定的问题，同时优化潜在空间维度以实现更高效的表示。

Method: 提出DGAE，利用扩散模型引导解码器恢复潜在表示中未完全解码的信息信号。

Result: DGAE在高空间压缩率下有效缓解性能下降，潜在空间缩小2倍，并在ImageNet-1K图像生成中表现优异。

Conclusion: DGAE通过改进解码器表达能力，实现了紧凑高效的潜在表示，并加速扩散模型的收敛。

Abstract: Autoencoders empower state-of-the-art image and video generative models by compressing pixels into a latent space through visual tokenization. Although recent advances have alleviated the performance degradation of autoencoders under high compression ratios, addressing the training instability caused by GAN remains an open challenge. While improving spatial compression, we also aim to minimize the latent space dimensionality, enabling more efficient and compact representations. To tackle these challenges, we focus on improving the decoder's expressiveness. Concretely, we propose DGAE, which employs a diffusion model to guide the decoder in recovering informative signals that are not fully decoded from the latent representation. With this design, DGAE effectively mitigates the performance degradation under high spatial compression rates. At the same time, DGAE achieves state-of-the-art performance with a 2x smaller latent space. When integrated with Diffusion Models, DGAE demonstrates competitive performance on image generation for ImageNet-1K and shows that this compact latent representation facilitates faster convergence of the diffusion model.

</details>


### [18] [Self-Supervised Multi-Part Articulated Objects Modeling via Deformable Gaussian Splatting and Progressive Primitive Segmentation](https://arxiv.org/abs/2506.09663)
*Haowen Wang,Xiaoping Yuan,Zhao Jin,Zhen Zhao,Zhengping Che,Yousong Xue,Jin Tian,Yakun Huang,Jian Tang*

Main category: cs.CV

TL;DR: DeGSS提出了一种统一框架，将铰接物体编码为可变形3D高斯场，嵌入几何、外观和运动信息，支持无监督分割和精确建模。


<details>
  <summary>Details</summary>
Motivation: 现有方法在缺乏人工标注时难以构建铰接物体的统一表示，而DeGSS旨在解决这一问题。

Method: DeGSS将每个交互状态建模为共享场的平滑变形，通过变形轨迹实现渐进式粗到细分割，识别刚性部件。

Result: 实验表明，DeGSS在准确性和稳定性上优于现有方法，并发布了新数据集RS-Art。

Conclusion: DeGSS提供了一种紧凑且连续的表示方法，支持部件级重建和运动关系建模。

Abstract: Articulated objects are ubiquitous in everyday life, and accurate 3D representations of their geometry and motion are critical for numerous applications. However, in the absence of human annotation, existing approaches still struggle to build a unified representation for objects that contain multiple movable parts. We introduce DeGSS, a unified framework that encodes articulated objects as deformable 3D Gaussian fields, embedding geometry, appearance, and motion in one compact representation. Each interaction state is modeled as a smooth deformation of a shared field, and the resulting deformation trajectories guide a progressive coarse-to-fine part segmentation that identifies distinct rigid components, all in an unsupervised manner. The refined field provides a spatially continuous, fully decoupled description of every part, supporting part-level reconstruction and precise modeling of their kinematic relationships. To evaluate generalization and realism, we enlarge the synthetic PartNet-Mobility benchmark and release RS-Art, a real-to-sim dataset that pairs RGB captures with accurately reverse-engineered 3D models. Extensive experiments demonstrate that our method outperforms existing methods in both accuracy and stability.

</details>


### [19] [ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models](https://arxiv.org/abs/2506.09740)
*Qin Zhou,Zhiyang Zhang,Jinglong Wang,Xiaobin Li,Jing Zhang,Qian Yu,Lu Sheng,Dong Xu*

Main category: cs.CV

TL;DR: 本文提出了一种基于ELBO的方法（ELBO-T2IAlign），用于校准扩散模型中像素与文本的对齐问题，无需训练且适用于多种架构。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在文本-图像对齐上存在假设完美对齐的问题，但实际上存在像素级和类别级的对齐偏差，尤其是在小尺寸、遮挡或罕见物体上。

Method: 使用零样本参考图像分割作为代理任务评估对齐性，提出基于ELBO的校准方法ELBO-T2IAlign。

Result: 实验验证了该方法在图像分割和生成任务中的有效性，解决了训练数据偏差导致的对齐问题。

Conclusion: ELBO-T2IAlign是一种简单通用的方法，能够有效校准扩散模型中的文本-图像对齐问题。

Abstract: Diffusion models excel at image generation. Recent studies have shown that these models not only generate high-quality images but also encode text-image alignment information through attention maps or loss functions. This information is valuable for various downstream tasks, including segmentation, text-guided image editing, and compositional image generation. However, current methods heavily rely on the assumption of perfect text-image alignment in diffusion models, which is not the case. In this paper, we propose using zero-shot referring image segmentation as a proxy task to evaluate the pixel-level image and class-level text alignment of popular diffusion models. We conduct an in-depth analysis of pixel-text misalignment in diffusion models from the perspective of training data bias. We find that misalignment occurs in images with small sized, occluded, or rare object classes. Therefore, we propose ELBO-T2IAlign, a simple yet effective method to calibrate pixel-text alignment in diffusion models based on the evidence lower bound (ELBO) of likelihood. Our method is training-free and generic, eliminating the need to identify the specific cause of misalignment and works well across various diffusion model architectures. Extensive experiments on commonly used benchmark datasets on image segmentation and generation have verified the effectiveness of our proposed calibration approach.

</details>


### [20] [DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction](https://arxiv.org/abs/2506.09836)
*Junli Deng,Ping Shi,Qipei Li,Jinyang Guo*

Main category: cs.CV

TL;DR: DynaSplat通过动态-静态分离和分层运动建模扩展高斯泼溅技术，实现复杂动态场景的高精度重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理真实世界动态场景的复杂性，DynaSplat旨在解决这一问题。

Method: 结合变形偏移统计和2D运动流一致性分类动态与静态元素，采用分层运动建模捕捉全局与局部运动，并引入基于物理的不透明度估计。

Result: 在多个数据集上表现优于现有方法，提供更准确、真实且高效的动态场景重建。

Conclusion: DynaSplat为动态场景重建提供了一种直观、紧凑且高效的解决方案。

Abstract: Reconstructing intricate, ever-changing environments remains a central ambition in computer vision, yet existing solutions often crumble before the complexity of real-world dynamics. We present DynaSplat, an approach that extends Gaussian Splatting to dynamic scenes by integrating dynamic-static separation and hierarchical motion modeling. First, we classify scene elements as static or dynamic through a novel fusion of deformation offset statistics and 2D motion flow consistency, refining our spatial representation to focus precisely where motion matters. We then introduce a hierarchical motion modeling strategy that captures both coarse global transformations and fine-grained local movements, enabling accurate handling of intricate, non-rigid motions. Finally, we integrate physically-based opacity estimation to ensure visually coherent reconstructions, even under challenging occlusions and perspective shifts. Extensive experiments on challenging datasets reveal that DynaSplat not only surpasses state-of-the-art alternatives in accuracy and realism but also provides a more intuitive, compact, and efficient route to dynamic scene reconstruction.

</details>


### [21] [The Less You Depend, The More You Learn: Synthesizing Novel Views from Sparse, Unposed Images without Any 3D Knowledge](https://arxiv.org/abs/2506.09885)
*Haoru Wang,Kai Ye,Yangyan Li,Wenzheng Chen,Baoquan Chen*

Main category: cs.CV

TL;DR: 论文提出了一种减少3D先验知识和相机姿态依赖的新颖视图合成方法，通过数据驱动的方式实现高质量3D重建。


<details>
  <summary>Details</summary>
Motivation: 探索3D知识在新颖视图合成中的作用，并验证减少其依赖是否可行。

Method: 提出了一种最小化3D先验和姿态依赖的框架，直接从稀疏2D图像中学习隐式3D感知。

Result: 实验表明，该方法能生成逼真且3D一致的新视图，性能与依赖姿态输入的方法相当。

Conclusion: 数据驱动的范式在减少3D知识依赖方面具有可行性和有效性。

Abstract: We consider the problem of generalizable novel view synthesis (NVS), which aims to generate photorealistic novel views from sparse or even unposed 2D images without per-scene optimization. This task remains fundamentally challenging, as it requires inferring 3D structure from incomplete and ambiguous 2D observations. Early approaches typically rely on strong 3D knowledge, including architectural 3D inductive biases (e.g., embedding explicit 3D representations, such as NeRF or 3DGS, into network design) and ground-truth camera poses for both input and target views. While recent efforts have sought to reduce the 3D inductive bias or the dependence on known camera poses of input views, critical questions regarding the role of 3D knowledge and the necessity of circumventing its use remain under-explored. In this work, we conduct a systematic analysis on the 3D knowledge and uncover a critical trend: the performance of methods that requires less 3D knowledge accelerates more as data scales, eventually achieving performance on par with their 3D knowledge-driven counterparts, which highlights the increasing importance of reducing dependence on 3D knowledge in the era of large-scale data. Motivated by and following this trend, we propose a novel NVS framework that minimizes 3D inductive bias and pose dependence for both input and target views. By eliminating this 3D knowledge, our method fully leverages data scaling and learns implicit 3D awareness directly from sparse 2D images, without any 3D inductive bias or pose annotation during training. Extensive experiments demonstrate that our model generates photorealistic and 3D-consistent novel views, achieving even comparable performance with methods that rely on posed inputs, thereby validating the feasibility and effectiveness of our data-centric paradigm. Project page: https://pku-vcl-geometry.github.io/Less3Depend/ .

</details>


### [22] [Only-Style: Stylistic Consistency in Image Generation without Content Leakage](https://arxiv.org/abs/2506.09916)
*Tilemachos Aravanis,Panagiotis Filntisis,Petros Maragos,George Retsinas*

Main category: cs.CV

TL;DR: 论文提出Only-Style方法，通过自适应调整风格对齐参数，有效减少内容泄漏，同时保持风格一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以分离语义内容和风格元素，导致内容泄漏问题。

Method: 提出Only-Style方法，定位内容泄漏并自适应调整风格对齐参数。

Result: 在多样实例中显著优于现有方法，实现无内容泄漏的稳健风格一致性。

Conclusion: Only-Style方法有效解决了内容泄漏问题，同时保持了风格一致性。

Abstract: Generating images in a consistent reference visual style remains a challenging computer vision task. State-of-the-art methods aiming for style-consistent generation struggle to effectively separate semantic content from stylistic elements, leading to content leakage from the image provided as a reference to the targets. To address this challenge, we propose Only-Style: a method designed to mitigate content leakage in a semantically coherent manner while preserving stylistic consistency. Only-Style works by localizing content leakage during inference, allowing the adaptive tuning of a parameter that controls the style alignment process, specifically within the image patches containing the subject in the reference image. This adaptive process best balances stylistic consistency with leakage elimination. Moreover, the localization of content leakage can function as a standalone component, given a reference-target image pair, allowing the adaptive tuning of any method-specific parameter that provides control over the impact of the stylistic reference. In addition, we propose a novel evaluation framework to quantify the success of style-consistent generations in avoiding undesired content leakage. Our approach demonstrates a significant improvement over state-of-the-art methods through extensive evaluation across diverse instances, consistently achieving robust stylistic consistency without undesired content leakage.

</details>


### [23] [HadaNorm: Diffusion Transformer Quantization through Mean-Centered Transformations](https://arxiv.org/abs/2506.09932)
*Marco Federici,Riccardo Del Chiaro,Boris van Breugel,Paul Whatmough,Markus Nagel*

Main category: cs.CV

TL;DR: HadaNorm是一种新型线性变换方法，通过归一化和Hadamard变换有效减少异常值，实现更激进的激活量化，提升扩散模型的量化效果。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中表现优异，但其高内存和计算需求限制了在资源受限设备上的部署。现有PTQ方法难以处理异常值，且高压缩率需额外变换。

Method: 提出HadaNorm，通过归一化激活特征通道并应用Hadamard变换，减少异常值，实现更高效的量化。

Result: HadaNorm在Transformer块各组件中持续降低量化误差，相比现有方法实现了更优的效率-性能权衡。

Conclusion: HadaNorm为扩散模型的量化提供了高效解决方案，显著提升了在资源受限设备上的部署潜力。

Abstract: Diffusion models represent the cutting edge in image generation, but their high memory and computational demands hinder deployment on resource-constrained devices. Post-Training Quantization (PTQ) offers a promising solution by reducing the bitwidth of matrix operations. However, standard PTQ methods struggle with outliers, and achieving higher compression often requires transforming model weights and activations before quantization. In this work, we propose HadaNorm, a novel linear transformation that extends existing approaches and effectively mitigates outliers by normalizing activations feature channels before applying Hadamard transformations, enabling more aggressive activation quantization. We demonstrate that HadaNorm consistently reduces quantization error across the various components of transformer blocks, achieving superior efficiency-performance trade-offs when compared to state-of-the-art methods.

</details>


### [24] [UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting](https://arxiv.org/abs/2506.09952)
*Ziyi Wang,Yanran Zhang,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: UniPre3D是首个适用于任意尺度和架构的点云统一预训练方法，通过高斯基元预测和可微分渲染实现端到端优化。


<details>
  <summary>Details</summary>
Motivation: 解决点云数据尺度多样性导致的统一表示学习难题，填补现有预训练方法在对象和场景级点云上的空白。

Method: 预测高斯基元作为预训练任务，利用可微分高斯渲染实现像素级监督，并结合预训练图像模型的2D特征。

Result: 在多种对象和场景级任务中验证了方法的普适性。

Conclusion: UniPre3D为点云统一表示学习提供了有效解决方案。

Abstract: The scale diversity of point cloud data presents significant challenges in developing unified representation learning techniques for 3D vision. Currently, there are few unified 3D models, and no existing pre-training method is equally effective for both object- and scene-level point clouds. In this paper, we introduce UniPre3D, the first unified pre-training method that can be seamlessly applied to point clouds of any scale and 3D models of any architecture. Our approach predicts Gaussian primitives as the pre-training task and employs differentiable Gaussian splatting to render images, enabling precise pixel-level supervision and end-to-end optimization. To further regulate the complexity of the pre-training task and direct the model's focus toward geometric structures, we integrate 2D features from pre-trained image models to incorporate well-established texture knowledge. We validate the universal effectiveness of our proposed method through extensive experiments across a variety of object- and scene-level tasks, using diverse point cloud models as backbones. Code is available at https://github.com/wangzy22/UniPre3D.

</details>


### [25] [Text-Aware Image Restoration with Diffusion Models](https://arxiv.org/abs/2506.09993)
*Jaewon Min,Jin Hyeon Kim,Paul Hyunbin Cho,Jaeeun Lee,Jihye Park,Minkyu Park,Sangpil Kim,Hyunhee Park,Seungryong Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新的图像修复任务TAIR，专注于同时恢复视觉内容和文本保真度，并提出了多任务扩散框架TeReDiff，显著提升了文本识别准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的图像修复方法在自然图像修复中表现良好，但在文本区域重建时容易产生错误的文本幻觉现象，因此需要一种能够同时恢复视觉内容和文本保真度的方法。

Method: 提出了TAIR任务和SA-Text基准数据集，并设计了一个多任务扩散框架TeReDiff，通过结合扩散模型内部特征和文本检测模块，实现联合训练。

Result: 实验表明，TeReDiff在文本识别准确性上显著优于现有方法。

Conclusion: TAIR任务和TeReDiff框架为解决文本图像修复中的文本幻觉问题提供了有效方案。

Abstract: Image restoration aims to recover degraded images. However, existing diffusion-based restoration methods, despite great success in natural image restoration, often struggle to faithfully reconstruct textual regions in degraded images. Those methods frequently generate plausible but incorrect text-like patterns, a phenomenon we refer to as text-image hallucination. In this paper, we introduce Text-Aware Image Restoration (TAIR), a novel restoration task that requires the simultaneous recovery of visual contents and textual fidelity. To tackle this task, we present SA-Text, a large-scale benchmark of 100K high-quality scene images densely annotated with diverse and complex text instances. Furthermore, we propose a multi-task diffusion framework, called TeReDiff, that integrates internal features from diffusion models into a text-spotting module, enabling both components to benefit from joint training. This allows for the extraction of rich text representations, which are utilized as prompts in subsequent denoising steps. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art restoration methods, achieving significant gains in text recognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/

</details>


### [26] [PlayerOne: Egocentric World Simulator](https://arxiv.org/abs/2506.09995)
*Yuanpeng Tu,Hao Luo,Xi Chen,Xiang Bai,Fan Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: PlayerOne是首个以自我为中心的逼真世界模拟器，通过动态环境实现沉浸式探索，支持精确控制人体运动和场景一致性建模。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够根据用户输入的自我中心场景图像构建对应世界并生成与真实人体运动严格对齐的自我中心视频的模拟器。

Method: 采用从粗到细的训练流程，包括大规模文本-视频对预训练和同步运动-视频数据微调，设计了部分解耦的运动注入方案和联合重建框架。

Result: 实验结果表明，PlayerOne在精确控制不同人体运动和多样化场景的世界一致性建模方面具有出色的泛化能力。

Conclusion: PlayerOne为自我中心真实世界模拟开辟了新方向，为世界建模及其多样化应用提供了新思路。

Abstract: We introduce PlayerOne, the first egocentric realistic world simulator, facilitating immersive and unrestricted exploration within vividly dynamic environments. Given an egocentric scene image from the user, PlayerOne can accurately construct the corresponding world and generate egocentric videos that are strictly aligned with the real scene human motion of the user captured by an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that first performs pretraining on large-scale egocentric text-video pairs for coarse-level egocentric understanding, followed by finetuning on synchronous motion-video data extracted from egocentric-exocentric video datasets with our automatic construction pipeline. Besides, considering the varying importance of different components, we design a part-disentangled motion injection scheme, enabling precise control of part-level movements. In addition, we devise a joint reconstruction framework that progressively models both the 4D scene and video frames, ensuring scene consistency in the long-form video generation. Experimental results demonstrate its great generalization ability in precise control of varying human movements and worldconsistent modeling of diverse scenarios. It marks the first endeavor into egocentric real-world simulation and can pave the way for the community to delve into fresh frontiers of world modeling and its diverse applications.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [27] [Sampling Theory for Super-Resolution with Implicit Neural Representations](https://arxiv.org/abs/2506.09949)
*Mahrokh Najaf,Gregory Ongie*

Main category: eess.IV

TL;DR: 研究了隐式神经表示（INRs）在解决线性逆问题中的样本复杂度，发现通过特定正则化方法可以从低通傅里叶样本中精确恢复图像。


<details>
  <summary>Details</summary>
Motivation: 探索INRs在解决计算机视觉和计算成像中的逆问题时的样本复杂度，填补传统像素表示与INRs之间的知识空白。

Method: 使用单隐藏层ReLU激活的INR和傅里叶特征层，结合广义权重衰减正则化，研究从低通傅里叶样本中恢复图像的采样需求。

Result: 确定了INR训练问题中精确恢复图像所需的足够傅里叶样本数量，并通过实验验证了低宽度INR的恢复概率。

Conclusion: INRs在特定条件下可以从低通傅里叶样本中精确恢复图像，为逆问题提供了新的理论支持。

Abstract: Implicit neural representations (INRs) have emerged as a powerful tool for solving inverse problems in computer vision and computational imaging. INRs represent images as continuous domain functions realized by a neural network taking spatial coordinates as inputs. However, unlike traditional pixel representations, little is known about the sample complexity of estimating images using INRs in the context of linear inverse problems. Towards this end, we study the sampling requirements for recovery of a continuous domain image from its low-pass Fourier samples by fitting a single hidden-layer INR with ReLU activation and a Fourier features layer using a generalized form of weight decay regularization. Our key insight is to relate minimizers of this non-convex parameter space optimization problem to minimizers of a convex penalty defined over an infinite-dimensional space of measures. We identify a sufficient number of Fourier samples for which an image realized by an INR is exactly recoverable by solving the INR training problem. To validate our theory, we empirically assess the probability of achieving exact recovery of images realized by low-width single hidden-layer INRs, and illustrate the performance of INRs on super-resolution recovery of continuous domain phantom images.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [28] [Reconstructing Heterogeneous Biomolecules via Hierarchical Gaussian Mixtures and Part Discovery](https://arxiv.org/abs/2506.09063)
*Shayan Shekarforoush,David B. Lindell,Marcus A. Brubaker,David J. Fleet*

Main category: q-bio.QM

TL;DR: 论文提出了一种名为CryoSPIRE的新框架，通过分层高斯混合模型处理冷冻电镜图像中的非刚体构象灵活性和组成变化问题。


<details>
  <summary>Details</summary>
Motivation: 冷冻电镜（Cryo-EM）在分子生物学中具有重要应用，但现有方法难以处理构象灵活性和组成变化的粒子成像问题。

Method: 采用分层高斯混合模型，结合部分分割的初始推断过程，以处理构象和组成变化。

Result: CryoSPIRE在复杂实验数据集上揭示了具有生物学意义的结构，并在CryoBench基准测试中达到新最优性能。

Conclusion: CryoSPIRE框架为冷冻电镜中的非刚体结构建模提供了有效解决方案，推动了该领域的技术进步。

Abstract: Cryo-EM is a transformational paradigm in molecular biology where computational methods are used to infer 3D molecular structure at atomic resolution from extremely noisy 2D electron microscope images. At the forefront of research is how to model the structure when the imaged particles exhibit non-rigid conformational flexibility and compositional variation where parts are sometimes missing. We introduce a novel 3D reconstruction framework with a hierarchical Gaussian mixture model, inspired in part by Gaussian Splatting for 4D scene reconstruction. In particular, the structure of the model is grounded in an initial process that infers a part-based segmentation of the particle, providing essential inductive bias in order to handle both conformational and compositional variability. The framework, called CryoSPIRE, is shown to reveal biologically meaningful structures on complex experimental datasets, and establishes a new state-of-the-art on CryoBench, a benchmark for cryo-EM heterogeneity methods.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [29] [Ming-Omni: A Unified Multimodal Model for Perception and Generation](https://arxiv.org/abs/2506.09344)
*Inclusion AI,Biao Gong,Cheng Zou,Chuanyang Zheng,Chunluan Zhou,Canxiang Yan,Chunxiang Jin,Chunjie Shen,Dandan Zheng,Fudong Wang,Furong Xu,GuangMing Yao,Jun Zhou,Jingdong Chen,Jianxin Sun,Jiajia Liu,Jianjiang Zhu,Jun Peng,Kaixiang Ji,Kaiyou Song,Kaimeng Ren,Libin Wang,Lixiang Ru,Lele Xie,Longhua Tan,Lyuxin Xue,Lan Wang,Mochen Bai,Ning Gao,Pei Chen,Qingpei Guo,Qinglong Zhang,Qiang Xu,Rui Liu,Ruijie Xiong,Sirui Gao,Tinghao Liu,Taisong Li,Weilong Chai,Xinyu Xiao,Xiaomei Wang,Xiaoxue Chen,Xiao Lu,Xiaoyu Li,Xingning Dong,Xuzheng Yu,Yi Yuan,Yuting Gao,Yunxiao Sun,Yipeng Chen,Yifei Wu,Yongjie Lyu,Ziping Ma,Zipeng Feng,Zhijiang Fang,Zhihao Qiu,Ziyuan Huang,Zhengyu He*

Main category: cs.AI

TL;DR: Ming-Omni是一个统一的多模态模型，支持图像、文本、音频和视频处理，并在语音和图像生成方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 旨在通过单一模型高效处理多模态输入，避免任务专用模型或结构调整的需求。

Method: 使用专用编码器提取多模态标记，并通过MoE架构和模态特定路由器处理。

Result: 实验表明Ming-Omni在多模态感知和生成任务中表现优异，支持音频和图像生成。

Conclusion: Ming-Omni是首个开源且支持与GPT-4o相当多模态的模型，代码和权重已公开。

Abstract: We propose Ming-Omni, a unified multimodal model capable of processing images, text, audio, and video, while demonstrating strong proficiency in both speech and image generation. Ming-Omni employs dedicated encoders to extract tokens from different modalities, which are then processed by Ling, an MoE architecture equipped with newly proposed modality-specific routers. This design enables a single model to efficiently process and fuse multimodal inputs within a unified framework, thereby facilitating diverse tasks without requiring separate models, task-specific fine-tuning, or structural redesign. Importantly, Ming-Omni extends beyond conventional multimodal models by supporting audio and image generation. This is achieved through the integration of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for high-quality image generation, which also allow the model to engage in context-aware chatting, perform text-to-speech conversion, and conduct versatile image editing. Our experimental results showcase Ming-Omni offers a powerful solution for unified perception and generation across all modalities. Notably, our proposed Ming-Omni is the first open-source model we are aware of to match GPT-4o in modality support, and we release all code and model weights to encourage further research and development in the community.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [30] [WD-DETR: Wavelet Denoising-Enhanced Real-Time Object Detection Transformer for Robot Perception with Event Cameras](https://arxiv.org/abs/2506.09098)
*Yangjie Cui,Boyang Gao,Yiwei Zhang,Xin Dong,Jinwu Xiang,Daochun Li,Zhan Tu*

Main category: cs.RO

TL;DR: 论文提出了一种基于小波去噪的检测变换器（WD-DETR）网络，用于事件相机的目标检测，通过小波变换去噪和动态重组卷积块（DRCB）提升性能，并在多个数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的事件相机检测方法在密集事件表示中存在噪声累积问题，导致检测性能下降，本文旨在解决这一问题。

Method: 提出WD-DETR网络，包括密集事件表示、小波变换去噪、基于变换器的目标预测，以及动态重组卷积块（DRCB）以减少推理时间。

Result: 在DSEC、Gen1和1Mpx数据集上表现优于现有方法，并在NVIDIA Jetson Orin NX上实现35 FPS的高帧率。

Conclusion: WD-DETR通过小波去噪和高效架构设计，显著提升了事件相机的实时目标检测性能。

Abstract: Previous studies on event camera sensing have demonstrated certain detection performance using dense event representations. However, the accumulated noise in such dense representations has received insufficient attention, which degrades the representation quality and increases the likelihood of missed detections. To address this challenge, we propose the Wavelet Denoising-enhanced DEtection TRansformer, i.e., WD-DETR network, for event cameras. In particular, a dense event representation is presented first, which enables real-time reconstruction of events as tensors. Then, a wavelet transform method is designed to filter noise in the event representations. Such a method is integrated into the backbone for feature extraction. The extracted features are subsequently fed into a transformer-based network for object prediction. To further reduce inference time, we incorporate the Dynamic Reorganization Convolution Block (DRCB) as a fusion module within the hybrid encoder. The proposed method has been evaluated on three event-based object detection datasets, i.e., DSEC, Gen1, and 1Mpx. The results demonstrate that WD-DETR outperforms tested state-of-the-art methods. Additionally, we implement our approach on a common onboard computer for robots, the NVIDIA Jetson Orin NX, achieving a high frame rate of approximately 35 FPS using TensorRT FP16, which is exceptionally well-suited for real-time perception of onboard robotic systems.

</details>
