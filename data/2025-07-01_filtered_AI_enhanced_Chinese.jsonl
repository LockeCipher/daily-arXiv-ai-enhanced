{"id": "2506.22799", "pdf": "https://arxiv.org/pdf/2506.22799", "abs": "https://arxiv.org/abs/2506.22799", "authors": ["Minchao Jiang", "Shunyu Jia", "Jiaming Gu", "Xiaoyuan Lu", "Guangming Zhu", "Anqi Dong", "Liang Zhang"], "title": "VoteSplat: Hough Voting Gaussian Splatting for 3D Scene Understanding", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Accepted to ICCV 2025", "summary": "3D Gaussian Splatting (3DGS) has become horsepower in high-quality, real-time rendering for novel view synthesis of 3D scenes. However, existing methods focus primarily on geometric and appearance modeling, lacking deeper scene understanding while also incurring high training costs that complicate the originally streamlined differentiable rendering pipeline. To this end, we propose VoteSplat, a novel 3D scene understanding framework that integrates Hough voting with 3DGS. Specifically, Segment Anything Model (SAM) is utilized for instance segmentation, extracting objects, and generating 2D vote maps. We then embed spatial offset vectors into Gaussian primitives. These offsets construct 3D spatial votes by associating them with 2D image votes, while depth distortion constraints refine localization along the depth axis. For open-vocabulary object localization, VoteSplat maps 2D image semantics to 3D point clouds via voting points, reducing training costs associated with high-dimensional CLIP features while preserving semantic unambiguity. Extensive experiments demonstrate effectiveness of VoteSplat in open-vocabulary 3D instance localization, 3D point cloud understanding, click-based 3D object localization, hierarchical segmentation, and ablation studies. Our code is available at https://sy-ja.github.io/votesplat/", "AI": {"tldr": "VoteSplat\u7ed3\u54083D\u9ad8\u65af\u6cfc\u6e85\u4e0e\u970d\u592b\u6295\u7968\uff0c\u63d0\u53473D\u573a\u666f\u7406\u89e3\uff0c\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u3002", "motivation": "\u73b0\u67093DGS\u65b9\u6cd5\u7f3a\u4e4f\u6df1\u5ea6\u573a\u666f\u7406\u89e3\u4e14\u8bad\u7ec3\u6210\u672c\u9ad8\u3002", "method": "\u5229\u7528SAM\u5b9e\u4f8b\u5206\u5272\u751f\u62102D\u6295\u7968\u56fe\uff0c\u5d4c\u5165\u7a7a\u95f4\u504f\u79fb\u5411\u91cf\u81f3\u9ad8\u65af\u57fa\u5143\uff0c\u7ed3\u5408\u6df1\u5ea6\u7ea6\u675f\u4f18\u5316\u5b9a\u4f4d\u3002", "result": "\u5728\u5f00\u653e\u8bcd\u6c473D\u5b9e\u4f8b\u5b9a\u4f4d\u3001\u70b9\u4e91\u7406\u89e3\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "VoteSplat\u9ad8\u6548\u4e14\u591a\u529f\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cd3D\u573a\u666f\u7406\u89e3\u4efb\u52a1\u3002"}}
{"id": "2506.22849", "pdf": "https://arxiv.org/pdf/2506.22849", "abs": "https://arxiv.org/abs/2506.22849", "authors": ["Michael A. Kern", "Alain Galvan", "David Oldcorn", "Daniel Skinner", "Rohan Mehalwal", "Leo Reyes Lozano", "Matth\u00e4us G. Chajdas"], "title": "DOBB-BVH: Efficient Ray Traversal by Transforming Wide BVHs into Oriented Bounding Box Trees using Discrete Rotations", "categories": ["cs.GR"], "comment": "10 pages main content, 3 pages appendix", "summary": "Oriented bounding box (OBB) bounding volume hierarchies offer a more precise fit than axis-aligned bounding box hierarchies in scenarios with thin elongated and arbitrarily rotated geometry, enhancing intersection test performance in ray tracing. However, determining optimally oriented bounding boxes can be computationally expensive and have high memory requirements. Recent research has shown that pre-built hierarchies can be efficiently converted to OBB hierarchies on the GPU in a bottom-up pass, yielding significant ray tracing traversal improvements. In this paper, we introduce a novel OBB construction technique where all internal node children share a consistent OBB transform, chosen from a fixed set of discrete quantized rotations. This allows for efficient encoding and reduces the computational complexity of OBB transformations. We further extend our approach to hierarchies with multiple children per node by leveraging Discrete Orientation Polytopes (k-DOPs), demonstrating improvements in traversal performance while limiting the build time impact for real-time applications. Our method is applied as a post-processing step, integrating seamlessly into existing hierarchy construction pipelines. Despite a 12.6% increase in build time, our experimental results demonstrate an average improvement of 18.5% in primary, 32.4% in secondary rays, and maximum gain of 65% in ray intersection performance, highlighting its potential for advancing real-time applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684OBB\u6784\u5efa\u6280\u672f\uff0c\u901a\u8fc7\u56fa\u5b9a\u79bb\u6563\u65cb\u8f6c\u96c6\u5b9e\u73b0\u9ad8\u6548\u7f16\u7801\u548c\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u5149\u7ebf\u8ffd\u8e2a\u6027\u80fd\u3002", "motivation": "\u5728\u8584\u957f\u548c\u4efb\u610f\u65cb\u8f6c\u51e0\u4f55\u573a\u666f\u4e2d\uff0cOBB\u6bd4AABB\u66f4\u7cbe\u786e\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5185\u5b58\u9700\u6c42\u5927\u3002", "method": "\u91c7\u7528\u56fa\u5b9a\u79bb\u6563\u65cb\u8f6c\u96c6\u7edf\u4e00\u5185\u90e8\u8282\u70b9\u7684OBB\u53d8\u6362\uff0c\u7ed3\u5408k-DOPs\u6269\u5c55\u81f3\u591a\u5b50\u8282\u70b9\u5c42\u6b21\u7ed3\u6784\uff0c\u4f5c\u4e3a\u540e\u5904\u7406\u6b65\u9aa4\u96c6\u6210\u5230\u73b0\u6709\u6d41\u7a0b\u3002", "result": "\u6784\u5efa\u65f6\u95f4\u589e\u52a012.6%\uff0c\u4f46\u4e3b\u5149\u7ebf\u6027\u80fd\u63d0\u534718.5%\uff0c\u6b21\u5149\u7ebf\u63d0\u534732.4%\uff0c\u6700\u5927\u5149\u7ebf\u76f8\u4ea4\u6027\u80fd\u63d0\u534765%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u663e\u8457\u63d0\u5347\u5149\u7ebf\u8ffd\u8e2a\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.22973", "pdf": "https://arxiv.org/pdf/2506.22973", "abs": "https://arxiv.org/abs/2506.22973", "authors": ["AmirHossein Naghi Razlighi", "Elaheh Badali Golezani", "Shohreh Kasaei"], "title": "Confident Splatting: Confidence-Based Compression of 3D Gaussian Splatting via Learnable Beta Distributions", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "3D Gaussian Splatting enables high-quality real-time rendering but often produces millions of splats, resulting in excessive storage and computational overhead. We propose a novel lossy compression method based on learnable confidence scores modeled as Beta distributions. Each splat's confidence is optimized through reconstruction-aware losses, enabling pruning of low-confidence splats while preserving visual fidelity. The proposed approach is architecture-agnostic and can be applied to any Gaussian Splatting variant. In addition, the average confidence values serve as a new metric to assess the quality of the scene. Extensive experiments demonstrate favorable trade-offs between compression and fidelity compared to prior work. Our code and data are publicly available at https://github.com/amirhossein-razlighi/Confident-Splatting", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u53ef\u5b66\u4e60\u7f6e\u4fe1\u5206\u6570\u76843D\u9ad8\u65af\u6cfc\u6e85\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u7f6e\u4fe1\u5206\u6570\u5b9e\u73b0\u9ad8\u6548\u538b\u7f29\u548c\u89c6\u89c9\u4fdd\u771f\u3002", "motivation": "\u89e3\u51b33D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u4e2d\u56e0\u751f\u6210\u6570\u767e\u4e07\u4e2a\u6cfc\u6e85\u70b9\u5bfc\u81f4\u7684\u5b58\u50a8\u548c\u8ba1\u7b97\u5f00\u9500\u8fc7\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eBeta\u5206\u5e03\u7684\u53ef\u5b66\u4e60\u7f6e\u4fe1\u5206\u6570\u6a21\u578b\uff0c\u901a\u8fc7\u91cd\u5efa\u611f\u77e5\u635f\u5931\u4f18\u5316\u6bcf\u4e2a\u6cfc\u6e85\u70b9\u7684\u7f6e\u4fe1\u5206\u6570\uff0c\u5b9e\u73b0\u4f4e\u7f6e\u4fe1\u5ea6\u6cfc\u6e85\u70b9\u7684\u526a\u679d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u538b\u7f29\u548c\u4fdd\u771f\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u4f18\u4e8e\u5148\u524d\u5de5\u4f5c\u7684\u5e73\u8861\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u67b6\u6784\u65e0\u5173\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u9ad8\u65af\u6cfc\u6e85\u53d8\u4f53\uff0c\u540c\u65f6\u7f6e\u4fe1\u5e73\u5747\u503c\u53ef\u4f5c\u4e3a\u573a\u666f\u8d28\u91cf\u7684\u65b0\u8bc4\u4f30\u6307\u6807\u3002"}}
{"id": "2506.23957", "pdf": "https://arxiv.org/pdf/2506.23957", "abs": "https://arxiv.org/abs/2506.23957", "authors": ["Zinuo You", "Stamatios Georgoulis", "Anpei Chen", "Siyu Tang", "Dengxin Dai"], "title": "GaVS: 3D-Grounded Video Stabilization via Temporally-Consistent Local Reconstruction and Rendering", "categories": ["cs.GR", "cs.CV"], "comment": "siggraph 2025, project website: https://sinoyou.github.io/gavs", "summary": "Video stabilization is pivotal for video processing, as it removes unwanted shakiness while preserving the original user motion intent. Existing approaches, depending on the domain they operate, suffer from several issues (e.g. geometric distortions, excessive cropping, poor generalization) that degrade the user experience. To address these issues, we introduce \\textbf{GaVS}, a novel 3D-grounded approach that reformulates video stabilization as a temporally-consistent `local reconstruction and rendering' paradigm. Given 3D camera poses, we augment a reconstruction model to predict Gaussian Splatting primitives, and finetune it at test-time, with multi-view dynamics-aware photometric supervision and cross-frame regularization, to produce temporally-consistent local reconstructions. The model are then used to render each stabilized frame. We utilize a scene extrapolation module to avoid frame cropping. Our method is evaluated on a repurposed dataset, instilled with 3D-grounded information, covering samples with diverse camera motions and scene dynamics. Quantitatively, our method is competitive with or superior to state-of-the-art 2D and 2.5D approaches in terms of conventional task metrics and new geometry consistency. Qualitatively, our method produces noticeably better results compared to alternatives, validated by the user study.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u7684\u89c6\u9891\u7a33\u5b9a\u65b9\u6cd5GaVS\uff0c\u901a\u8fc7\u5c40\u90e8\u91cd\u5efa\u548c\u6e32\u67d3\u8303\u5f0f\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u51e0\u4f55\u5931\u771f\u3001\u8fc7\u5ea6\u88c1\u526a\u548c\u6cdb\u5316\u6027\u5dee\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7a33\u5b9a\u65b9\u6cd5\u5b58\u5728\u51e0\u4f55\u5931\u771f\u3001\u8fc7\u5ea6\u88c1\u526a\u548c\u6cdb\u5316\u6027\u5dee\u7b49\u95ee\u9898\uff0c\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u91c7\u75283D\u76f8\u673a\u59ff\u6001\uff0c\u901a\u8fc7\u9ad8\u65af\u6563\u5c04\u57fa\u5143\u9884\u6d4b\u548c\u6d4b\u8bd5\u65f6\u5fae\u8c03\uff0c\u7ed3\u5408\u591a\u89c6\u89d2\u52a8\u6001\u611f\u77e5\u5149\u5ea6\u76d1\u7763\u548c\u8de8\u5e27\u6b63\u5219\u5316\uff0c\u5b9e\u73b0\u65f6\u95f4\u4e00\u81f4\u7684\u5c40\u90e8\u91cd\u5efa\u548c\u6e32\u67d3\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u76f8\u673a\u8fd0\u52a8\u548c\u573a\u666f\u52a8\u6001\u6570\u636e\u96c6\u4e0a\uff0cGaVS\u5728\u4f20\u7edf\u4efb\u52a1\u6307\u6807\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u67092D\u548c2.5D\u65b9\u6cd5\u3002", "conclusion": "GaVS\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2506.24108", "pdf": "https://arxiv.org/pdf/2506.24108", "abs": "https://arxiv.org/abs/2506.24108", "authors": ["Shai Yehezkel", "Omer Dahary", "Andrey Voynov", "Daniel Cohen-Or"], "title": "Navigating with Annealing Guidance Scale in Diffusion Space", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": "Project page:   https://annealing-guidance.github.io/annealing-guidance/", "summary": "Denoising diffusion models excel at generating high-quality images conditioned on text prompts, yet their effectiveness heavily relies on careful guidance during the sampling process. Classifier-Free Guidance (CFG) provides a widely used mechanism for steering generation by setting the guidance scale, which balances image quality and prompt alignment. However, the choice of the guidance scale has a critical impact on the convergence toward a visually appealing and prompt-adherent image. In this work, we propose an annealing guidance scheduler which dynamically adjusts the guidance scale over time based on the conditional noisy signal. By learning a scheduling policy, our method addresses the temperamental behavior of CFG. Empirical results demonstrate that our guidance scheduler significantly enhances image quality and alignment with the text prompt, advancing the performance of text-to-image generation. Notably, our novel scheduler requires no additional activations or memory consumption, and can seamlessly replace the common classifier-free guidance, offering an improved trade-off between prompt alignment and quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u6307\u5bfc\u5c3a\u5ea6\u7684\u9000\u706b\u6307\u5bfc\u8c03\u5ea6\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u8d28\u91cf\u548c\u63d0\u793a\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u7684\u65e0\u5206\u7c7b\u5668\u6307\u5bfc\uff08CFG\uff09\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\u5bf9\u6307\u5bfc\u5c3a\u5ea6\u7684\u9009\u62e9\u654f\u611f\uff0c\u5f71\u54cd\u56fe\u50cf\u8d28\u91cf\u548c\u63d0\u793a\u5bf9\u9f50\u3002", "method": "\u901a\u8fc7\u57fa\u4e8e\u6761\u4ef6\u566a\u58f0\u4fe1\u53f7\u52a8\u6001\u8c03\u6574\u6307\u5bfc\u5c3a\u5ea6\u7684\u9000\u706b\u6307\u5bfc\u8c03\u5ea6\u5668\uff0c\u5b66\u4e60\u8c03\u5ea6\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\u548c\u6587\u672c\u63d0\u793a\u5bf9\u9f50\uff0c\u4e14\u65e0\u9700\u989d\u5916\u6fc0\u6d3b\u6216\u5185\u5b58\u6d88\u8017\u3002", "conclusion": "\u63d0\u51fa\u7684\u8c03\u5ea6\u5668\u53ef\u65e0\u7f1d\u66ff\u4ee3\u5e38\u89c1\u7684\u65e0\u5206\u7c7b\u5668\u6307\u5bfc\uff0c\u4f18\u5316\u4e86\u63d0\u793a\u5bf9\u9f50\u4e0e\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2506.22899", "pdf": "https://arxiv.org/pdf/2506.22899", "abs": "https://arxiv.org/abs/2506.22899", "authors": ["Ehsan Pajouheshgar", "Yitao Xu", "Ali Abbasi", "Alexander Mordvintsev", "Wenzel Jakob", "Sabine S\u00fcsstrunk"], "title": "Neural Cellular Automata: From Cells to Pixels", "categories": ["cs.CV", "cs.GR", "cs.LG", "cs.MA", "eess.IV"], "comment": "6 pages, 5 figures, first draft", "summary": "Neural Cellular Automata (NCAs) are bio-inspired systems in which identical cells self-organize to form complex and coherent patterns by repeatedly applying simple local rules. NCAs display striking emergent behaviors including self-regeneration, generalization and robustness to unseen situations, and spontaneous motion. Despite their success in texture synthesis and morphogenesis, NCAs remain largely confined to low-resolution grids. This limitation stems from (1) training time and memory requirements that grow quadratically with grid size, (2) the strictly local propagation of information which impedes long-range cell communication, and (3) the heavy compute demands of real-time inference at high resolution. In this work, we overcome this limitation by pairing NCA with a tiny, shared implicit decoder, inspired by recent advances in implicit neural representations. Following NCA evolution on a coarse grid, a lightweight decoder renders output images at arbitrary resolution. We also propose novel loss functions for both morphogenesis and texture synthesis tasks, specifically tailored for high-resolution output with minimal memory and computation overhead. Combining our proposed architecture and loss functions brings substantial improvement in quality, efficiency, and performance. NCAs equipped with our implicit decoder can generate full-HD outputs in real time while preserving their self-organizing, emergent properties. Moreover, because each MLP processes cell states independently, inference remains highly parallelizable and efficient. We demonstrate the applicability of our approach across multiple NCA variants (on 2D, 3D grids, and 3D meshes) and multiple tasks, including texture generation and morphogenesis (growing patterns from a seed), showing that with our proposed framework, NCAs seamlessly scale to high-resolution outputs with minimal computational overhead.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9690\u5f0f\u89e3\u7801\u5668\u7684\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\uff08NCA\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u9ad8\u5206\u8fa8\u7387\u7f51\u683c\u4e0b\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u751f\u6210\u9ad8\u6e05\u8f93\u51fa\u3002", "motivation": "NCA\u5728\u4f4e\u5206\u8fa8\u7387\u7f51\u683c\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9ad8\u5206\u8fa8\u7387\u4e0b\u56e0\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u5267\u589e\u3001\u4fe1\u606f\u4f20\u64ad\u53d7\u9650\u800c\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u8f7b\u91cf\u7ea7\u9690\u5f0f\u89e3\u7801\u5668\uff0c\u5728\u7c97\u7f51\u683c\u4e0a\u8fdb\u884cNCA\u6f14\u5316\u540e\u6e32\u67d3\u4efb\u610f\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9488\u5bf9\u9ad8\u5206\u8fa8\u7387\u8f93\u51fa\u7684\u65b0\u578b\u635f\u5931\u51fd\u6570\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86NCA\u5728\u9ad8\u5206\u8fa8\u7387\u4e0b\u7684\u8d28\u91cf\u3001\u6548\u7387\u548c\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u751f\u6210\u5168\u9ad8\u6e05\u8f93\u51fa\u3002", "conclusion": "\u7ed3\u5408\u9690\u5f0f\u89e3\u7801\u5668\u7684NCA\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u6269\u5c55\u81f3\u9ad8\u5206\u8fa8\u7387\u4efb\u52a1\uff0c\u540c\u65f6\u4fdd\u6301\u81ea\u7ec4\u7ec7\u548c\u6d8c\u73b0\u7279\u6027\u3002"}}
{"id": "2506.22509", "pdf": "https://arxiv.org/pdf/2506.22509", "abs": "https://arxiv.org/abs/2506.22509", "authors": ["Hang Xu", "Jie Huang", "Linjiang Huang", "Dong Li", "Yidi Liu", "Feng Zhao"], "title": "FreeDNA: Endowing Domain Adaptation of Diffusion-Based Dense Prediction with Training-Free Domain Noise Alignment", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV2025", "summary": "Domain Adaptation(DA) for dense prediction tasks is an important topic, which enhances the dense prediction model's performance when tested on its unseen domain. Recently, with the development of Diffusion-based Dense Prediction (DDP) models, the exploration of DA designs tailored to this framework is worth exploring, since the diffusion model is effective in modeling the distribution transformation that comprises domain information. In this work, we propose a training-free mechanism for DDP frameworks, endowing them with DA capabilities. Our motivation arises from the observation that the exposure bias (e.g., noise statistics bias) in diffusion brings domain shift, and different domains in conditions of DDP models can also be effectively captured by the noise prediction statistics. Based on this, we propose a training-free Domain Noise Alignment (DNA) approach, which alleviates the variations of noise statistics to domain changes during the diffusion sampling process, thereby achieving domain adaptation. Specifically, when the source domain is available, we directly adopt the DNA method to achieve domain adaptation by aligning the noise statistics of the target domain with those of the source domain. For the more challenging source-free DA, inspired by the observation that regions closer to the source domain exhibit higher confidence meeting variations of sampling noise, we utilize the statistics from the high-confidence regions progressively to guide the noise statistic adjustment during the sampling process. Notably, our method demonstrates the effectiveness of enhancing the DA capability of DDP models across four common dense prediction tasks. Code is available at \\href{https://github.com/xuhang07/FreeDNA}{https://github.com/xuhang07/FreeDNA}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u9886\u57df\u9002\u5e94\u65b9\u6cd5\uff08DNA\uff09\uff0c\u901a\u8fc7\u8c03\u6574\u6269\u6563\u91c7\u6837\u8fc7\u7a0b\u4e2d\u7684\u566a\u58f0\u7edf\u8ba1\u91cf\uff0c\u589e\u5f3a\u6269\u6563\u5bc6\u96c6\u9884\u6d4b\u6a21\u578b\u7684\u8de8\u57df\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u5efa\u6a21\u5305\u542b\u9886\u57df\u4fe1\u606f\u7684\u5206\u5e03\u8f6c\u6362\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u566a\u58f0\u7edf\u8ba1\u504f\u5dee\u4f1a\u5bfc\u81f4\u9886\u57df\u504f\u79fb\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faDomain Noise Alignment (DNA)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u9f50\u76ee\u6807\u57df\u548c\u6e90\u57df\u7684\u566a\u58f0\u7edf\u8ba1\u91cf\u5b9e\u73b0\u9886\u57df\u9002\u5e94\uff1b\u5728\u65e0\u6e90\u57df\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u9ad8\u7f6e\u4fe1\u533a\u57df\u7684\u7edf\u8ba1\u91cf\u9010\u6b65\u8c03\u6574\u566a\u58f0\u3002", "result": "\u5728\u56db\u79cd\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86DNA\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9886\u57df\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "DNA\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u9886\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6269\u6563\u5bc6\u96c6\u9884\u6d4b\u6a21\u578b\u3002"}}
{"id": "2506.23854", "pdf": "https://arxiv.org/pdf/2506.23854", "abs": "https://arxiv.org/abs/2506.23854", "authors": ["Yida Wang", "Xueyang Zhang", "Kun Zhan", "Peng Jia", "Xianpeng Lang"], "title": "HiNeuS: High-fidelity Neural Surface Mitigating Low-texture and Reflective Ambiguity", "categories": ["cs.CV", "cs.GR"], "comment": "Published in International Conference on Computer Vision (ICCV) 2025", "summary": "Neural surface reconstruction faces persistent challenges in reconciling geometric fidelity with photometric consistency under complex scene conditions. We present HiNeuS, a unified framework that holistically addresses three core limitations in existing approaches: multi-view radiance inconsistency, missing keypoints in textureless regions, and structural degradation from over-enforced Eikonal constraints during joint optimization. To resolve these issues through a unified pipeline, we introduce: 1) Differential visibility verification through SDF-guided ray tracing, resolving reflection ambiguities via continuous occlusion modeling; 2) Planar-conformal regularization via ray-aligned geometry patches that enforce local surface coherence while preserving sharp edges through adaptive appearance weighting; and 3) Physically-grounded Eikonal relaxation that dynamically modulates geometric constraints based on local radiance gradients, enabling detail preservation without sacrificing global regularity. Unlike prior methods that handle these aspects through sequential optimizations or isolated modules, our approach achieves cohesive integration where appearance-geometry constraints evolve synergistically throughout training. Comprehensive evaluations across synthetic and real-world datasets demonstrate state-of-the-art performance, including a 21.4% reduction in Chamfer distance over reflection-aware baselines and 2.32 dB PSNR improvement against neural rendering counterparts. Qualitative analyses reveal superior capability in recovering specular instruments, urban layouts with centimeter-scale infrastructure, and low-textured surfaces without local patch collapse. The method's generalizability is further validated through successful application to inverse rendering tasks, including material decomposition and view-consistent relighting.", "AI": {"tldr": "HiNeuS\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u795e\u7ecf\u8868\u9762\u91cd\u5efa\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u89d2\u8f90\u5c04\u4e0d\u4e00\u81f4\u3001\u65e0\u7eb9\u7406\u533a\u57df\u5173\u952e\u70b9\u7f3a\u5931\u548cEikonal\u7ea6\u675f\u8fc7\u5f3a\u5bfc\u81f4\u7684\u7ed3\u6784\u9000\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e0b\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u51e0\u4f55\u4fdd\u771f\u5ea6\u548c\u5149\u5ea6\u4e00\u81f4\u6027\uff0cHiNeuS\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u89e3\u51b3\u8fd9\u4e9b\u6838\u5fc3\u95ee\u9898\u3002", "method": "1) \u57fa\u4e8eSDF\u7684\u5c04\u7ebf\u8ffd\u8e2a\u89e3\u51b3\u53cd\u5c04\u6a21\u7cca\uff1b2) \u5e73\u9762\u5171\u5f62\u6b63\u5219\u5316\u4fdd\u6301\u5c40\u90e8\u8868\u9762\u4e00\u81f4\u6027\uff1b3) \u52a8\u6001\u8c03\u6574Eikonal\u7ea6\u675f\u4ee5\u4fdd\u7559\u7ec6\u8282\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cChamfer\u8ddd\u79bb\u964d\u4f4e21.4%\uff0cPSNR\u63d0\u53472.32 dB\uff0c\u5e76\u80fd\u6062\u590d\u9ad8\u5149\u7269\u4f53\u548c\u65e0\u7eb9\u7406\u8868\u9762\u3002", "conclusion": "HiNeuS\u901a\u8fc7\u534f\u540c\u4f18\u5316\u5b9e\u73b0\u4e86\u51e0\u4f55\u4e0e\u5916\u89c2\u7ea6\u675f\u7684\u7edf\u4e00\uff0c\u9002\u7528\u4e8e\u9006\u5411\u6e32\u67d3\u4efb\u52a1\uff0c\u5982\u6750\u8d28\u5206\u89e3\u548c\u89c6\u89d2\u4e00\u81f4\u91cd\u5149\u7167\u3002"}}
{"id": "2506.22531", "pdf": "https://arxiv.org/pdf/2506.22531", "abs": "https://arxiv.org/abs/2506.22531", "authors": ["Prasen Kumar Sharma", "Neeraj Matiyali", "Siddharth Srivastava", "Gaurav Sharma"], "title": "Preserve Anything: Controllable Image Synthesis with Object Preservation", "categories": ["cs.CV"], "comment": "Accepted at ICCV 2025", "summary": "We introduce \\textit{Preserve Anything}, a novel method for controlled image synthesis that addresses key limitations in object preservation and semantic consistency in text-to-image (T2I) generation. Existing approaches often fail (i) to preserve multiple objects with fidelity, (ii) maintain semantic alignment with prompts, or (iii) provide explicit control over scene composition. To overcome these challenges, the proposed method employs an N-channel ControlNet that integrates (i) object preservation with size and placement agnosticism, color and detail retention, and artifact elimination, (ii) high-resolution, semantically consistent backgrounds with accurate shadows, lighting, and prompt adherence, and (iii) explicit user control over background layouts and lighting conditions. Key components of our framework include object preservation and background guidance modules, enforcing lighting consistency and a high-frequency overlay module to retain fine details while mitigating unwanted artifacts. We introduce a benchmark dataset consisting of 240K natural images filtered for aesthetic quality and 18K 3D-rendered synthetic images with metadata such as lighting, camera angles, and object relationships. This dataset addresses the deficiencies of existing benchmarks and allows a complete evaluation. Empirical results demonstrate that our method achieves state-of-the-art performance, significantly improving feature-space fidelity (FID 15.26) and semantic alignment (CLIP-S 32.85) while maintaining competitive aesthetic quality. We also conducted a user study to demonstrate the efficacy of the proposed work on unseen benchmark and observed a remarkable improvement of $\\sim25\\%$, $\\sim19\\%$, $\\sim13\\%$, and $\\sim14\\%$ in terms of prompt alignment, photorealism, the presence of AI artifacts, and natural aesthetics over existing works.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cPreserve Anything\u201d\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5bf9\u8c61\u4fdd\u7559\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u901a\u8fc7N\u901a\u9053ControlNet\u5b9e\u73b0\u591a\u5bf9\u8c61\u4fdd\u7559\u3001\u8bed\u4e49\u5bf9\u9f50\u548c\u573a\u666f\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u5bf9\u8c61\u4fdd\u7559\u3001\u8bed\u4e49\u5bf9\u9f50\u548c\u573a\u666f\u63a7\u5236\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528N\u901a\u9053ControlNet\uff0c\u7ed3\u5408\u5bf9\u8c61\u4fdd\u7559\u6a21\u5757\u3001\u80cc\u666f\u5f15\u5bfc\u6a21\u5757\u548c\u9ad8\u9891\u8986\u76d6\u6a21\u5757\uff0c\u5b9e\u73b0\u591a\u5bf9\u8c61\u4fdd\u7559\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u7528\u6237\u63a7\u5236\u3002", "result": "\u65b9\u6cd5\u5728\u7279\u5f81\u7a7a\u95f4\u4fdd\u771f\u5ea6\uff08FID 15.26\uff09\u548c\u8bed\u4e49\u5bf9\u9f50\uff08CLIP-S 32.85\uff09\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u7528\u6237\u7814\u7a76\u663e\u793a\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u5bf9\u8c61\u4fdd\u7559\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u7528\u6237\u63a7\u5236\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.22710", "pdf": "https://arxiv.org/pdf/2506.22710", "abs": "https://arxiv.org/abs/2506.22710", "authors": ["Jiang Yuan", "JI Ma", "Bo Wang", "Guanzhou Ke", "Weiming Hu"], "title": "LightBSR: Towards Lightweight Blind Super-Resolution via Discriminative Implicit Degradation Representation Learning", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Implicit degradation estimation-based blind super-resolution (IDE-BSR) hinges on extracting the implicit degradation representation (IDR) of the LR image and adapting it to LR image features to guide HR detail restoration. Although IDE-BSR has shown potential in dealing with noise interference and complex degradations, existing methods ignore the importance of IDR discriminability for BSR and instead over-complicate the adaptation process to improve effect, resulting in a significant increase in the model's parameters and computations. In this paper, we focus on the discriminability optimization of IDR and propose a new powerful and lightweight BSR model termed LightBSR. Specifically, we employ a knowledge distillation-based learning framework. We first introduce a well-designed degradation-prior-constrained contrastive learning technique during teacher stage to make the model more focused on distinguishing different degradation types. Then we utilize a feature alignment technique to transfer the degradation-related knowledge acquired by the teacher to the student for practical inferencing. Extensive experiments demonstrate the effectiveness of IDR discriminability-driven BSR model design. The proposed LightBSR can achieve outstanding performance with minimal complexity across a range of blind SR tasks. Our code is accessible at: https://github.com/MJ-NCEPU/LightBSR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u76f2\u8d85\u5206\u8fa8\u7387\u6a21\u578bLightBSR\uff0c\u901a\u8fc7\u4f18\u5316\u9690\u5f0f\u9000\u5316\u8868\u793a\uff08IDR\uff09\u7684\u533a\u5206\u6027\uff0c\u91c7\u7528\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86IDR\u533a\u5206\u6027\u5bf9\u76f2\u8d85\u5206\u8fa8\u7387\u7684\u91cd\u8981\u6027\uff0c\u4e14\u8fc7\u5ea6\u590d\u6742\u5316\u9002\u5e94\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u6a21\u578b\u53c2\u6570\u548c\u8ba1\u7b97\u91cf\u663e\u8457\u589e\u52a0\u3002", "method": "\u4f7f\u7528\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u5305\u62ec\u9000\u5316\u5148\u9a8c\u7ea6\u675f\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\u548c\u7279\u5f81\u5bf9\u9f50\u6280\u672f\uff0c\u4ee5\u63d0\u5347IDR\u533a\u5206\u6027\u3002", "result": "LightBSR\u5728\u591a\u79cd\u76f2\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u590d\u6742\u5ea6\u6781\u4f4e\u3002", "conclusion": "\u4f18\u5316IDR\u533a\u5206\u6027\u53ef\u663e\u8457\u63d0\u5347\u76f2\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u7684\u6027\u80fd\uff0cLightBSR\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u8f7b\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22736", "pdf": "https://arxiv.org/pdf/2506.22736", "abs": "https://arxiv.org/abs/2506.22736", "authors": ["Dayong Su", "Yafei Zhang", "Huafeng Li", "Jinxing Li", "Yu Liu"], "title": "UniFuse: A Unified All-in-One Framework for Multi-Modal Medical Image Fusion Under Diverse Degradations and Misalignments", "categories": ["cs.CV"], "comment": "Accepted by ICCV2025", "summary": "Current multimodal medical image fusion typically assumes that source images are of high quality and perfectly aligned at the pixel level. Its effectiveness heavily relies on these conditions and often deteriorates when handling misaligned or degraded medical images. To address this, we propose UniFuse, a general fusion framework. By embedding a degradation-aware prompt learning module, UniFuse seamlessly integrates multi-directional information from input images and correlates cross-modal alignment with restoration, enabling joint optimization of both tasks within a unified framework. Additionally, we design an Omni Unified Feature Representation scheme, which leverages Spatial Mamba to encode multi-directional features and mitigate modality differences in feature alignment. To enable simultaneous restoration and fusion within an All-in-One configuration, we propose a Universal Feature Restoration & Fusion module, incorporating the Adaptive LoRA Synergistic Network (ALSN) based on LoRA principles. By leveraging ALSN's adaptive feature representation along with degradation-type guidance, we enable joint restoration and fusion within a single-stage framework. Compared to staged approaches, UniFuse unifies alignment, restoration, and fusion within a single framework. Experimental results across multiple datasets demonstrate the method's effectiveness and significant advantages over existing approaches.", "AI": {"tldr": "UniFuse\u662f\u4e00\u4e2a\u901a\u7528\u7684\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u5d4c\u5165\u9000\u5316\u611f\u77e5\u63d0\u793a\u5b66\u4e60\u6a21\u5757\u548cOmni\u7edf\u4e00\u7279\u5f81\u8868\u793a\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u5bf9\u9f50\u3001\u6062\u590d\u548c\u878d\u5408\u7684\u8054\u5408\u4f18\u5316\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u672a\u5bf9\u9f50\u6216\u9000\u5316\u533b\u5b66\u56fe\u50cf\u65f6\u6548\u679c\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faUniFuse\u6846\u67b6\uff0c\u5305\u62ec\u9000\u5316\u611f\u77e5\u63d0\u793a\u5b66\u4e60\u6a21\u5757\u3001Omni\u7edf\u4e00\u7279\u5f81\u8868\u793a\u65b9\u6848\u548c\u901a\u7528\u7279\u5f81\u6062\u590d\u4e0e\u878d\u5408\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUniFuse\u5728\u591a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "UniFuse\u6210\u529f\u5730\u5c06\u5bf9\u9f50\u3001\u6062\u590d\u548c\u878d\u5408\u7edf\u4e00\u5728\u4e00\u4e2a\u6846\u67b6\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2506.22756", "pdf": "https://arxiv.org/pdf/2506.22756", "abs": "https://arxiv.org/abs/2506.22756", "authors": ["Tao Tang", "Likui Zhang", "Youpeng Wen", "Kaidong Zhang", "Jia-Wang Bian", "xia zhou", "Tianyi Yan", "Kun Zhan", "Peng Jia", "Hefeng Wu", "Liang Lin", "Xiaodan Liang"], "title": "RoboPearls: Editable Video Simulation for Robot Manipulation", "categories": ["cs.CV", "cs.RO"], "comment": "ICCV 2025", "summary": "The development of generalist robot manipulation policies has seen significant progress, driven by large-scale demonstration data across diverse environments. However, the high cost and inefficiency of collecting real-world demonstrations hinder the scalability of data acquisition. While existing simulation platforms enable controlled environments for robotic learning, the challenge of bridging the sim-to-real gap remains. To address these challenges, we propose RoboPearls, an editable video simulation framework for robotic manipulation. Built on 3D Gaussian Splatting (3DGS), RoboPearls enables the construction of photo-realistic, view-consistent simulations from demonstration videos, and supports a wide range of simulation operators, including various object manipulations, powered by advanced modules like Incremental Semantic Distillation (ISD) and 3D regularized NNFM Loss (3D-NNFM). Moreover, by incorporating large language models (LLMs), RoboPearls automates the simulation production process in a user-friendly manner through flexible command interpretation and execution. Furthermore, RoboPearls employs a vision-language model (VLM) to analyze robotic learning issues to close the simulation loop for performance enhancement. To demonstrate the effectiveness of RoboPearls, we conduct extensive experiments on multiple datasets and scenes, including RLBench, COLOSSEUM, Ego4D, Open X-Embodiment, and a real-world robot, which demonstrate our satisfactory simulation performance.", "AI": {"tldr": "RoboPearls\u662f\u4e00\u4e2a\u57fa\u4e8e3D\u9ad8\u65af\u6563\u5c04\u7684\u53ef\u7f16\u8f91\u89c6\u9891\u4eff\u771f\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u4eff\u771f\u751f\u4ea7\u548c\u6027\u80fd\u589e\u5f3a\u89e3\u51b3\u73b0\u5b9e\u6570\u636e\u6536\u96c6\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u6f14\u793a\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e\uff0c\u4eff\u771f\u5e73\u53f0\u867d\u63d0\u4f9b\u53ef\u63a7\u73af\u5883\uff0c\u4f46\u5b58\u5728\u4eff\u771f\u4e0e\u73b0\u5b9e\u7684\u5dee\u8ddd\u95ee\u9898\u3002", "method": "RoboPearls\u5229\u75283D\u9ad8\u65af\u6563\u5c04\u6784\u5efa\u903c\u771f\u4eff\u771f\uff0c\u7ed3\u5408\u589e\u91cf\u8bed\u4e49\u84b8\u998f\u548c3D\u6b63\u5219\u5316NNFM\u635f\u5931\uff0c\u5e76\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5316\u4eff\u771f\u751f\u4ea7\u4e0e\u5206\u6790\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u573a\u666f\uff08\u5982RLBench\u3001COLOSSEUM\u7b49\uff09\u4e0a\u5b9e\u9a8c\u9a8c\u8bc1\u4e86RoboPearls\u7684\u6709\u6548\u6027\u548c\u4eff\u771f\u6027\u80fd\u3002", "conclusion": "RoboPearls\u901a\u8fc7\u81ea\u52a8\u5316\u4eff\u771f\u548c\u6027\u80fd\u589e\u5f3a\uff0c\u4e3a\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u6570\u636e\u6536\u96c6\u548c\u4eff\u771f\u5dee\u8ddd\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2506.22762", "pdf": "https://arxiv.org/pdf/2506.22762", "abs": "https://arxiv.org/abs/2506.22762", "authors": ["Dinh Phu Tran", "Dao Duy Hung", "Daeyoung Kim"], "title": "VSRM: A Robust Mamba-Based Framework for Video Super-Resolution", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Video super-resolution remains a major challenge in low-level vision tasks. To date, CNN- and Transformer-based methods have delivered impressive results. However, CNNs are limited by local receptive fields, while Transformers struggle with quadratic complexity, posing challenges for processing long sequences in VSR. Recently, Mamba has drawn attention for its long-sequence modeling, linear complexity, and large receptive fields. In this work, we propose VSRM, a novel \\textbf{V}ideo \\textbf{S}uper-\\textbf{R}esolution framework that leverages the power of \\textbf{M}amba. VSRM introduces Spatial-to-Temporal Mamba and Temporal-to-Spatial Mamba blocks to extract long-range spatio-temporal features and enhance receptive fields efficiently. To better align adjacent frames, we propose Deformable Cross-Mamba Alignment module. This module utilizes a deformable cross-mamba mechanism to make the compensation stage more dynamic and flexible, preventing feature distortions. Finally, we minimize the frequency domain gaps between reconstructed and ground-truth frames by proposing a simple yet effective Frequency Charbonnier-like loss that better preserves high-frequency content and enhances visual quality. Through extensive experiments, VSRM achieves state-of-the-art results on diverse benchmarks, establishing itself as a solid foundation for future research.", "AI": {"tldr": "VSRM\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMamba\u7684\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u5230\u65f6\u95f4\u548c\u65f6\u95f4\u5230\u7a7a\u95f4\u7684Mamba\u5757\u63d0\u53d6\u957f\u8ddd\u79bb\u65f6\u7a7a\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u53ef\u53d8\u5f62\u4ea4\u53c9Mamba\u5bf9\u9f50\u6a21\u5757\u548c\u9891\u7387\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709CNN\u548cTransformer\u65b9\u6cd5\u5728\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u90e8\u611f\u53d7\u91ce\u9650\u5236\u6216\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u800cMamba\u56e0\u5176\u957f\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\u548c\u7ebf\u6027\u590d\u6742\u5ea6\u6210\u4e3a\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "method": "VSRM\u91c7\u7528\u7a7a\u95f4\u5230\u65f6\u95f4\u548c\u65f6\u95f4\u5230\u7a7a\u95f4\u7684Mamba\u5757\u63d0\u53d6\u7279\u5f81\uff0c\u5f15\u5165\u53ef\u53d8\u5f62\u4ea4\u53c9Mamba\u5bf9\u9f50\u6a21\u5757\u52a8\u6001\u8865\u507f\u5e27\u95f4\u5bf9\u9f50\uff0c\u5e76\u4f7f\u7528\u9891\u7387Charbonnier-like\u635f\u5931\u51cf\u5c11\u9891\u57df\u5dee\u8ddd\u3002", "result": "VSRM\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u9ad8\u8d28\u91cf\u91cd\u5efa\u80fd\u529b\u3002", "conclusion": "VSRM\u4e3a\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u7ed3\u5408Mamba\u7684\u4f18\u52bf\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2506.22800", "pdf": "https://arxiv.org/pdf/2506.22800", "abs": "https://arxiv.org/abs/2506.22800", "authors": ["Sicong Du", "Jiarun Liu", "Qifeng Chen", "Hao-Xiang Chen", "Tai-Jiang Mu", "Sheng Yang"], "title": "RGE-GS: Reward-Guided Expansive Driving Scene Reconstruction via Diffusion Priors", "categories": ["cs.CV"], "comment": null, "summary": "A single-pass driving clip frequently results in incomplete scanning of the road structure, making reconstructed scene expanding a critical requirement for sensor simulators to effectively regress driving actions. Although contemporary 3D Gaussian Splatting (3DGS) techniques achieve remarkable reconstruction quality, their direct extension through the integration of diffusion priors often introduces cumulative physical inconsistencies and compromises training efficiency. To address these limitations, we present RGE-GS, a novel expansive reconstruction framework that synergizes diffusion-based generation with reward-guided Gaussian integration. The RGE-GS framework incorporates two key innovations: First, we propose a reward network that learns to identify and prioritize consistently generated patterns prior to reconstruction phases, thereby enabling selective retention of diffusion outputs for spatial stability. Second, during the reconstruction process, we devise a differentiated training strategy that automatically adjust Gaussian optimization progress according to scene converge metrics, which achieving better convergence than baseline methods. Extensive evaluations of publicly available datasets demonstrate that RGE-GS achieves state-of-the-art performance in reconstruction quality. Our source-code will be made publicly available at https://github.com/CN-ADLab/RGE-GS. (Camera-ready version incorporating reviewer suggestions will be updated soon.)", "AI": {"tldr": "RGE-GS\u662f\u4e00\u79cd\u7ed3\u5408\u6269\u6563\u751f\u6210\u548c\u5956\u52b1\u5f15\u5bfc\u9ad8\u65af\u79ef\u5206\u7684\u65b0\u578b\u6269\u5c55\u91cd\u5efa\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u67093D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u4e2d\u7269\u7406\u4e0d\u4e00\u81f4\u548c\u8bad\u7ec3\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u5355\u6b21\u9a7e\u9a76\u7247\u6bb5\u5e38\u5bfc\u81f4\u9053\u8def\u7ed3\u6784\u626b\u63cf\u4e0d\u5b8c\u6574\uff0c\u9700\u8981\u6269\u5c55\u91cd\u5efa\u4ee5\u652f\u6301\u4f20\u611f\u5668\u6a21\u62df\u5668\u6709\u6548\u56de\u5f52\u9a7e\u9a76\u884c\u4e3a\u3002\u73b0\u67093D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u867d\u8d28\u91cf\u9ad8\uff0c\u4f46\u76f4\u63a5\u7ed3\u5408\u6269\u6563\u5148\u9a8c\u4f1a\u5f15\u5165\u7269\u7406\u4e0d\u4e00\u81f4\u5e76\u964d\u4f4e\u6548\u7387\u3002", "method": "\u63d0\u51faRGE-GS\u6846\u67b6\uff0c\u5305\u542b\u5956\u52b1\u7f51\u7edc\uff08\u4f18\u5148\u9009\u62e9\u7a33\u5b9a\u751f\u6210\u6a21\u5f0f\uff09\u548c\u5dee\u5f02\u5316\u8bad\u7ec3\u7b56\u7565\uff08\u6839\u636e\u573a\u666f\u6536\u655b\u6307\u6807\u8c03\u6574\u9ad8\u65af\u4f18\u5316\u8fdb\u5ea6\uff09\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cRGE-GS\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "RGE-GS\u901a\u8fc7\u5956\u52b1\u5f15\u5bfc\u548c\u5dee\u5f02\u5316\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2506.22833", "pdf": "https://arxiv.org/pdf/2506.22833", "abs": "https://arxiv.org/abs/2506.22833", "authors": ["Shashikant Verma", "Shanmuganathan Raman"], "title": "SemFaceEdit: Semantic Face Editing on Generative Radiance Manifolds", "categories": ["cs.CV"], "comment": null, "summary": "Despite multiple view consistency offered by 3D-aware GAN techniques, the resulting images often lack the capacity for localized editing. In response, generative radiance manifolds emerge as an efficient approach for constrained point sampling within volumes, effectively reducing computational demands and enabling the learning of fine details. This work introduces SemFaceEdit, a novel method that streamlines the appearance and geometric editing process by generating semantic fields on generative radiance manifolds. Utilizing latent codes, our method effectively disentangles the geometry and appearance associated with different facial semantics within the generated image. In contrast to existing methods that can change the appearance of the entire radiance field, our method enables the precise editing of particular facial semantics while preserving the integrity of other regions. Our network comprises two key modules: the Geometry module, which generates semantic radiance and occupancy fields, and the Appearance module, which is responsible for predicting RGB radiance. We jointly train both modules in adversarial settings to learn semantic-aware geometry and appearance descriptors. The appearance descriptors are then conditioned on their respective semantic latent codes by the Appearance Module, facilitating disentanglement and enhanced control. Our experiments highlight SemFaceEdit's superior performance in semantic field-based editing, particularly in achieving improved radiance field disentanglement.", "AI": {"tldr": "SemFaceEdit\u662f\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u8f90\u5c04\u6d41\u5f62\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u573a\u5b9e\u73b0\u9762\u90e8\u56fe\u50cf\u7684\u7cbe\u786e\u5c40\u90e8\u7f16\u8f91\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u4ed6\u533a\u57df\u7684\u5b8c\u6574\u6027\u3002", "motivation": "\u73b0\u67093D\u611f\u77e5GAN\u6280\u672f\u867d\u7136\u63d0\u4f9b\u591a\u89c6\u89d2\u4e00\u81f4\u6027\uff0c\u4f46\u7f3a\u4e4f\u5c40\u90e8\u7f16\u8f91\u80fd\u529b\uff0c\u751f\u6210\u8f90\u5c04\u6d41\u5f62\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u9014\u5f84\u3002", "method": "SemFaceEdit\u901a\u8fc7\u51e0\u4f55\u6a21\u5757\u548c\u5916\u89c2\u6a21\u5757\u8054\u5408\u8bad\u7ec3\uff0c\u751f\u6210\u8bed\u4e49\u8f90\u5c04\u548c\u5360\u7528\u573a\uff0c\u5e76\u5229\u7528\u6f5c\u5728\u7801\u89e3\u8026\u51e0\u4f55\u4e0e\u5916\u89c2\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSemFaceEdit\u5728\u8bed\u4e49\u573a\u7f16\u8f91\u548c\u8f90\u5c04\u573a\u89e3\u8026\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u9762\u90e8\u8bed\u4e49\u7f16\u8f91\u3002", "conclusion": "SemFaceEdit\u901a\u8fc7\u8bed\u4e49\u573a\u548c\u6f5c\u5728\u7801\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9762\u90e8\u56fe\u50cf\u7684\u5c40\u90e8\u7f16\u8f91\u80fd\u529b\u548c\u89e3\u8026\u6548\u679c\u3002"}}
{"id": "2506.22850", "pdf": "https://arxiv.org/pdf/2506.22850", "abs": "https://arxiv.org/abs/2506.22850", "authors": ["Aalok Gangopadhyay", "Shashikant Verma", "Shanmuganathan Raman"], "title": "DMD-Net: Deep Mesh Denoising Network", "categories": ["cs.CV"], "comment": null, "summary": "We present Deep Mesh Denoising Network (DMD-Net), an end-to-end deep learning framework, for solving the mesh denoising problem. DMD-Net consists of a Graph Convolutional Neural Network in which aggregation is performed in both the primal as well as the dual graph. This is realized in the form of an asymmetric two-stream network, which contains a primal-dual fusion block that enables communication between the primal-stream and the dual-stream. We develop a Feature Guided Transformer (FGT) paradigm, which consists of a feature extractor, a transformer, and a denoiser. The feature extractor estimates the local features, that guide the transformer to compute a transformation, which is applied to the noisy input mesh to obtain a useful intermediate representation. This is further processed by the denoiser to obtain the denoised mesh. Our network is trained on a large scale dataset of 3D objects. We perform exhaustive ablation studies to demonstrate that each component in our network is essential for obtaining the best performance. We show that our method obtains competitive or better results when compared with the state-of-the-art mesh denoising algorithms. We demonstrate that our method is robust to various kinds of noise. We observe that even in the presence of extremely high noise, our method achieves excellent performance.", "AI": {"tldr": "DMD-Net\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u7f51\u683c\u53bb\u566a\uff0c\u7ed3\u5408\u4e86\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u53cc\u6d41\u7f51\u7edc\uff0c\u901a\u8fc7\u7279\u5f81\u5f15\u5bfc\u53d8\u6362\u5668\u5b9e\u73b0\u9ad8\u6548\u53bb\u566a\u3002", "motivation": "\u89e3\u51b3\u7f51\u683c\u53bb\u566a\u95ee\u9898\uff0c\u63d0\u5347\u5728\u590d\u6742\u566a\u58f0\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u53cc\u6d41\u7f51\u7edc\uff0c\u7ed3\u5408\u7279\u5f81\u63d0\u53d6\u5668\u3001\u53d8\u6362\u5668\u548c\u53bb\u566a\u5668\u3002", "result": "\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5bf9\u9ad8\u566a\u58f0\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "DMD-Net\u5728\u7f51\u683c\u53bb\u566a\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u566a\u58f0\u573a\u666f\u3002"}}
{"id": "2506.22982", "pdf": "https://arxiv.org/pdf/2506.22982", "abs": "https://arxiv.org/abs/2506.22982", "authors": ["Atharv Mittal", "Agam Pandey", "Amritanshu Tiwari", "Sukrit Jindal", "Swadesh Swain"], "title": "Revisiting CroPA: A Reproducibility Study and Enhancements for Cross-Prompt Adversarial Transferability in Vision-Language Models", "categories": ["cs.CV"], "comment": "Accepted to MLRC 2025", "summary": "Large Vision-Language Models (VLMs) have revolutionized computer vision, enabling tasks such as image classification, captioning, and visual question answering. However, they remain highly vulnerable to adversarial attacks, particularly in scenarios where both visual and textual modalities can be manipulated. In this study, we conduct a comprehensive reproducibility study of \"An Image is Worth 1000 Lies: Adversarial Transferability Across Prompts on Vision-Language Models\" validating the Cross-Prompt Attack (CroPA) and confirming its superior cross-prompt transferability compared to existing baselines. Beyond replication we propose several key improvements: (1) A novel initialization strategy that significantly improves Attack Success Rate (ASR). (2) Investigate cross-image transferability by learning universal perturbations. (3) A novel loss function targeting vision encoder attention mechanisms to improve generalization. Our evaluation across prominent VLMs -- including Flamingo, BLIP-2, and InstructBLIP as well as extended experiments on LLaVA validates the original results and demonstrates that our improvements consistently boost adversarial effectiveness. Our work reinforces the importance of studying adversarial vulnerabilities in VLMs and provides a more robust framework for generating transferable adversarial examples, with significant implications for understanding the security of VLMs in real-world applications.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5bf9\u6297\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u9a8c\u8bc1\u4e86\u8de8\u63d0\u793a\u653b\u51fb\uff08CroPA\uff09\u7684\u4f18\u8d8a\u6027\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u5bf9\u6297\u653b\u51fb\u9ad8\u5ea6\u8106\u5f31\uff0c\u5c24\u5176\u662f\u5728\u591a\u6a21\u6001\u88ab\u64cd\u7eb5\u7684\u573a\u666f\u4e0b\u3002\u672c\u6587\u65e8\u5728\u9a8c\u8bc1\u548c\u6539\u8fdb\u8de8\u63d0\u793a\u653b\u51fb\u65b9\u6cd5\u3002", "method": "1. \u63d0\u51fa\u65b0\u9896\u7684\u521d\u59cb\u5316\u7b56\u7565\u63d0\u9ad8\u653b\u51fb\u6210\u529f\u7387\uff1b2. \u7814\u7a76\u8de8\u56fe\u50cf\u53ef\u8f6c\u79fb\u6027\uff1b3. \u8bbe\u8ba1\u9488\u5bf9\u89c6\u89c9\u7f16\u7801\u5668\u6ce8\u610f\u529b\u673a\u5236\u7684\u65b0\u635f\u5931\u51fd\u6570\u3002", "result": "\u6539\u8fdb\u65b9\u6cd5\u5728\u591a\u4e2aVLMs\uff08\u5982Flamingo\u3001BLIP-2\u7b49\uff09\u4e0a\u9a8c\u8bc1\u4e86\u539f\u59cb\u7ed3\u679c\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u653b\u51fb\u6548\u679c\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86VLMs\u5bf9\u6297\u6f0f\u6d1e\u7684\u91cd\u8981\u6027\uff0c\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u5bf9\u6297\u6837\u672c\u751f\u6210\u6846\u67b6\uff0c\u5bf9\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u6027\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2506.23038", "pdf": "https://arxiv.org/pdf/2506.23038", "abs": "https://arxiv.org/abs/2506.23038", "authors": ["Xinrong Hu", "Yiyu Shi"], "title": "Inpainting is All You Need: A Diffusion-based Augmentation Method for Semi-supervised Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Collecting pixel-level labels for medical datasets can be a laborious and expensive process, and enhancing segmentation performance with a scarcity of labeled data is a crucial challenge. This work introduces AugPaint, a data augmentation framework that utilizes inpainting to generate image-label pairs from limited labeled data. AugPaint leverages latent diffusion models, known for their ability to generate high-quality in-domain images with low overhead, and adapts the sampling process for the inpainting task without need for retraining. Specifically, given a pair of image and label mask, we crop the area labeled with the foreground and condition on it during reversed denoising process for every noise level. Masked background area would gradually be filled in, and all generated images are paired with the label mask. This approach ensures the accuracy of match between synthetic images and label masks, setting it apart from existing dataset generation methods. The generated images serve as valuable supervision for training downstream segmentation models, effectively addressing the challenge of limited annotations. We conducted extensive evaluations of our data augmentation method on four public medical image segmentation datasets, including CT, MRI, and skin imaging. Results across all datasets demonstrate that AugPaint outperforms state-of-the-art label-efficient methodologies, significantly improving segmentation performance.", "AI": {"tldr": "AugPaint\u662f\u4e00\u79cd\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u901a\u8fc7\u4fee\u590d\u751f\u6210\u56fe\u50cf-\u6807\u7b7e\u5bf9\uff0c\u663e\u8457\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u6570\u636e\u96c6\u7684\u50cf\u7d20\u7ea7\u6807\u7b7e\u6536\u96c6\u8017\u65f6\u4e14\u6602\u8d35\uff0c\u5982\u4f55\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u4e0b\u63d0\u5347\u5206\u5272\u6027\u80fd\u662f\u5173\u952e\u6311\u6218\u3002", "method": "AugPaint\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u8fdb\u884c\u4fee\u590d\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u751f\u6210\u4e0e\u6807\u7b7e\u63a9\u7801\u5339\u914d\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6570\u636e\u96c6\u4e0a\uff0cAugPaint\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u5206\u5272\u6027\u80fd\u3002", "conclusion": "AugPaint\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf-\u6807\u7b7e\u5bf9\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002"}}
{"id": "2506.23042", "pdf": "https://arxiv.org/pdf/2506.23042", "abs": "https://arxiv.org/abs/2506.23042", "authors": ["Hung Nguyen", "An Le", "Runfa Li", "Truong Nguyen"], "title": "From Coarse to Fine: Learnable Discrete Wavelet Transforms for Efficient 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "Accepted to ICCV Workshop", "summary": "3D Gaussian Splatting has emerged as a powerful approach in novel view synthesis, delivering rapid training and rendering but at the cost of an ever-growing set of Gaussian primitives that strains memory and bandwidth. We introduce AutoOpti3DGS, a training-time framework that automatically restrains Gaussian proliferation without sacrificing visual fidelity. The key idea is to feed the input images to a sequence of learnable Forward and Inverse Discrete Wavelet Transforms, where low-pass filters are kept fixed, high-pass filters are learnable and initialized to zero, and an auxiliary orthogonality loss gradually activates fine frequencies. This wavelet-driven, coarse-to-fine process delays the formation of redundant fine Gaussians, allowing 3DGS to capture global structure first and refine detail only when necessary. Through extensive experiments, AutoOpti3DGS requires just a single filter learning-rate hyper-parameter, integrates seamlessly with existing efficient 3DGS frameworks, and consistently produces sparser scene representations more compatible with memory or storage-constrained hardware.", "AI": {"tldr": "AutoOpti3DGS\u901a\u8fc7\u5c0f\u6ce2\u53d8\u6362\u63a7\u5236\u9ad8\u65af\u589e\u6b96\uff0c\u63d0\u53473D\u9ad8\u65af\u6e85\u5c04\u7684\u5185\u5b58\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b33D\u9ad8\u65af\u6e85\u5c04\u65b9\u6cd5\u4e2d\u9ad8\u65af\u57fa\u5143\u6570\u91cf\u589e\u957f\u5bfc\u81f4\u7684\u5185\u5b58\u548c\u5e26\u5bbd\u538b\u529b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53ef\u5b66\u4e60\u7684\u6b63\u5411\u548c\u9006\u5411\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\uff0c\u56fa\u5b9a\u4f4e\u901a\u6ee4\u6ce2\u5668\uff0c\u53ef\u5b66\u4e60\u9ad8\u901a\u6ee4\u6ce2\u5668\uff0c\u5e76\u901a\u8fc7\u6b63\u4ea4\u6027\u635f\u5931\u9010\u6b65\u6fc0\u6d3b\u9ad8\u9891\u7ec6\u8282\u3002", "result": "AutoOpti3DGS\u4ec5\u9700\u4e00\u4e2a\u8d85\u53c2\u6570\uff0c\u751f\u6210\u66f4\u7a00\u758f\u7684\u573a\u666f\u8868\u793a\uff0c\u9002\u7528\u4e8e\u5185\u5b58\u53d7\u9650\u7684\u786c\u4ef6\u3002", "conclusion": "AutoOpti3DGS\u6709\u6548\u5e73\u8861\u4e86\u89c6\u89c9\u8d28\u91cf\u4e0e\u5185\u5b58\u6548\u7387\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u3002"}}
{"id": "2506.23044", "pdf": "https://arxiv.org/pdf/2506.23044", "abs": "https://arxiv.org/abs/2506.23044", "authors": ["Guo-Hua Wang", "Shanshan Zhao", "Xinjie Zhang", "Liangfu Cao", "Pengxin Zhan", "Lunhao Duan", "Shiyin Lu", "Minghao Fu", "Xiaohao Chen", "Jianshan Zhao", "Yang Li", "Qing-Guo Chen"], "title": "Ovis-U1 Technical Report", "categories": ["cs.CV", "cs.AI"], "comment": "A unified model for multimodal understanding, text-to-image   generation, and image editing. GitHub: https://github.com/AIDC-AI/Ovis-U1", "summary": "In this report, we introduce Ovis-U1, a 3-billion-parameter unified model that integrates multimodal understanding, text-to-image generation, and image editing capabilities. Building on the foundation of the Ovis series, Ovis-U1 incorporates a diffusion-based visual decoder paired with a bidirectional token refiner, enabling image generation tasks comparable to leading models like GPT-4o. Unlike some previous models that use a frozen MLLM for generation tasks, Ovis-U1 utilizes a new unified training approach starting from a language model. Compared to training solely on understanding or generation tasks, unified training yields better performance, demonstrating the enhancement achieved by integrating these two tasks. Ovis-U1 achieves a score of 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent state-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In text-to-image generation, it excels with scores of 83.72 and 0.89 on the DPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves 4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the initial version of the Ovis unified model series, Ovis-U1 pushes the boundaries of multimodal understanding, generation, and editing.", "AI": {"tldr": "Ovis-U1\u662f\u4e00\u4e2a30\u4ebf\u53c2\u6570\u7684\u591a\u6a21\u6001\u7edf\u4e00\u6a21\u578b\uff0c\u96c6\u6210\u4e86\u7406\u89e3\u3001\u751f\u6210\u548c\u7f16\u8f91\u80fd\u529b\uff0c\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u5148\u8fdb\u6a21\u578b\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u8bad\u7ec3\u65b9\u6cd5\u63d0\u5347\u591a\u6a21\u6001\u4efb\u52a1\uff08\u7406\u89e3\u3001\u751f\u6210\u3001\u7f16\u8f91\uff09\u7684\u6027\u80fd\uff0c\u7a81\u7834\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u6269\u6563\u5f0f\u89c6\u89c9\u89e3\u7801\u5668\u548c\u53cc\u5411\u4ee4\u724c\u7cbe\u70bc\u5668\uff0c\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7edf\u4e00\u8bad\u7ec3\u3002", "result": "\u5728OpenCompass\u3001DPG-Bench\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8aRistretto-3B\u7b49\u6a21\u578b\u3002", "conclusion": "Ovis-U1\u4e3a\u591a\u6a21\u6001\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u8fb9\u754c\u3002"}}
{"id": "2506.23135", "pdf": "https://arxiv.org/pdf/2506.23135", "abs": "https://arxiv.org/abs/2506.23135", "authors": ["Yu Shang", "Xin Zhang", "Yinzhou Tang", "Lei Jin", "Chen Gao", "Wei Wu", "Yong Li"], "title": "RoboScape: Physics-informed Embodied World Model", "categories": ["cs.CV", "cs.RO"], "comment": "17 pages", "summary": "World models have become indispensable tools for embodied intelligence, serving as powerful simulators capable of generating realistic robotic videos while addressing critical data scarcity challenges. However, current embodied world models exhibit limited physical awareness, particularly in modeling 3D geometry and motion dynamics, resulting in unrealistic video generation for contact-rich robotic scenarios. In this paper, we present RoboScape, a unified physics-informed world model that jointly learns RGB video generation and physics knowledge within an integrated framework. We introduce two key physics-informed joint training tasks: temporal depth prediction that enhances 3D geometric consistency in video rendering, and keypoint dynamics learning that implicitly encodes physical properties (e.g., object shape and material characteristics) while improving complex motion modeling. Extensive experiments demonstrate that RoboScape generates videos with superior visual fidelity and physical plausibility across diverse robotic scenarios. We further validate its practical utility through downstream applications including robotic policy training with generated data and policy evaluation. Our work provides new insights for building efficient physics-informed world models to advance embodied intelligence research. The code is available at: https://github.com/tsinghua-fib-lab/RoboScape.", "AI": {"tldr": "RoboScape\u662f\u4e00\u4e2a\u7269\u7406\u4fe1\u606f\u7edf\u4e00\u7684\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60RGB\u89c6\u9891\u751f\u6210\u548c\u7269\u7406\u77e5\u8bc6\uff0c\u63d0\u53473D\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u8fd0\u52a8\u52a8\u6001\u5efa\u6a21\uff0c\u751f\u6210\u66f4\u771f\u5b9e\u4e14\u7269\u7406\u5408\u7406\u7684\u673a\u5668\u4eba\u573a\u666f\u89c6\u9891\u3002", "motivation": "\u5f53\u524d\u7684\u4e16\u754c\u6a21\u578b\u5728\u7269\u7406\u611f\u77e5\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u7279\u522b\u662f\u57283D\u51e0\u4f55\u548c\u8fd0\u52a8\u52a8\u6001\u5efa\u6a21\u4e0a\uff0c\u5bfc\u81f4\u63a5\u89e6\u4e30\u5bcc\u7684\u673a\u5668\u4eba\u573a\u666f\u89c6\u9891\u751f\u6210\u4e0d\u771f\u5b9e\u3002", "method": "\u63d0\u51faRoboScape\uff0c\u901a\u8fc7\u4e24\u4e2a\u5173\u952e\u7269\u7406\u4fe1\u606f\u8054\u5408\u8bad\u7ec3\u4efb\u52a1\uff1a\u65f6\u95f4\u6df1\u5ea6\u9884\u6d4b\u548c\u5173\u952e\u70b9\u52a8\u6001\u5b66\u4e60\uff0c\u589e\u5f3a\u89c6\u9891\u6e32\u67d3\u76843D\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u590d\u6742\u8fd0\u52a8\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRoboScape\u751f\u6210\u7684\u89c6\u9891\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u7269\u7406\u5408\u7406\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u4e0b\u6e38\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u3002", "conclusion": "RoboScape\u4e3a\u6784\u5efa\u9ad8\u6548\u7684\u7269\u7406\u4fe1\u606f\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u63a8\u52a8\u4e86\u5177\u8eab\u667a\u80fd\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.23138", "pdf": "https://arxiv.org/pdf/2506.23138", "abs": "https://arxiv.org/abs/2506.23138", "authors": ["Shiyu Wu", "Mingzhen Sun", "Weining Wang", "Yequan Wang", "Jing Liu"], "title": "VisualPrompter: Prompt Optimization with Visual Feedback for Text-to-Image Synthesis", "categories": ["cs.CV"], "comment": "12 pages, 5 figures", "summary": "Since there exists a notable gap between user-provided and model-preferred prompts, generating high-quality and satisfactory images using diffusion models often requires prompt engineering to optimize user inputs. Current studies on text-to-image prompt engineering can effectively enhance the style and aesthetics of generated images. However, they often neglect the semantic alignment between generated images and user descriptions, resulting in visually appealing but content-wise unsatisfying outputs. In this work, we propose VisualPrompter, a novel training-free prompt engineering framework that refines user inputs to model-preferred sentences. In particular, VisualPrompter utilizes an automatic self-reflection module to identify the missing concepts in generated images and a target-specific prompt optimization mechanism to revise the prompts in a fine-grained manner. Extensive experiments demonstrate the effectiveness of our VisualPrompter, which achieves new state-of-the-art performance on multiple benchmarks for text-image alignment evaluation. Additionally, our framework features a plug-and-play design, making it highly adaptable to various generative models.", "AI": {"tldr": "VisualPrompter\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u63d0\u793a\u5de5\u7a0b\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u53cd\u601d\u548c\u7ec6\u7c92\u5ea6\u4f18\u5316\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u8bed\u4e49\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u867d\u80fd\u63d0\u5347\u56fe\u50cf\u98ce\u683c\u548c\u7f8e\u89c2\u5ea6\uff0c\u4f46\u5e38\u5ffd\u7565\u751f\u6210\u56fe\u50cf\u4e0e\u7528\u6237\u63cf\u8ff0\u7684\u8bed\u4e49\u5bf9\u9f50\uff0c\u5bfc\u81f4\u5185\u5bb9\u4e0d\u6ee1\u610f\u3002", "method": "\u63d0\u51faVisualPrompter\u6846\u67b6\uff0c\u5305\u542b\u81ea\u52a8\u53cd\u601d\u6a21\u5757\u8bc6\u522b\u7f3a\u5931\u6982\u5ff5\uff0c\u4ee5\u53ca\u76ee\u6807\u7279\u5b9a\u63d0\u793a\u4f18\u5316\u673a\u5236\u7ec6\u7c92\u5ea6\u4fee\u8ba2\u63d0\u793a\u3002", "result": "\u5728\u591a\u4e2a\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u8bc4\u4f30\u57fa\u51c6\u4e0a\u8fbe\u5230\u65b0\u6700\u4f18\u6027\u80fd\uff0c\u4e14\u6846\u67b6\u8bbe\u8ba1\u4e3a\u5373\u63d2\u5373\u7528\uff0c\u9002\u914d\u591a\u79cd\u751f\u6210\u6a21\u578b\u3002", "conclusion": "VisualPrompter\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u7684\u5185\u5bb9\u6ee1\u610f\u5ea6\u3002"}}
{"id": "2506.23153", "pdf": "https://arxiv.org/pdf/2506.23153", "abs": "https://arxiv.org/abs/2506.23153", "authors": ["Huiqiang Sun", "Xingyi Li", "Juewen Peng", "Liao Shen", "Zhiguo Cao", "Ke Xian", "Guosheng Lin"], "title": "Dynamic View Synthesis from Small Camera Motion Videos", "categories": ["cs.CV"], "comment": "Accepted by TVCG", "summary": "Novel view synthesis for dynamic $3$D scenes poses a significant challenge. Many notable efforts use NeRF-based approaches to address this task and yield impressive results. However, these methods rely heavily on sufficient motion parallax in the input images or videos. When the camera motion range becomes limited or even stationary (i.e., small camera motion), existing methods encounter two primary challenges: incorrect representation of scene geometry and inaccurate estimation of camera parameters. These challenges make prior methods struggle to produce satisfactory results or even become invalid. To address the first challenge, we propose a novel Distribution-based Depth Regularization (DDR) that ensures the rendering weight distribution to align with the true distribution. Specifically, unlike previous methods that use depth loss to calculate the error of the expectation, we calculate the expectation of the error by using Gumbel-softmax to differentiably sample points from discrete rendering weight distribution. Additionally, we introduce constraints that enforce the volume density of spatial points before the object boundary along the ray to be near zero, ensuring that our model learns the correct geometry of the scene. To demystify the DDR, we further propose a visualization tool that enables observing the scene geometry representation at the rendering weight level. For the second challenge, we incorporate camera parameter learning during training to enhance the robustness of our model to camera parameters. We conduct extensive experiments to demonstrate the effectiveness of our approach in representing scenes with small camera motion input, and our results compare favorably to state-of-the-art methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5e03\u6df1\u5ea6\u6b63\u5219\u5316\uff08DDR\uff09\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u52a8\u60013D\u573a\u666f\u4e2d\u76f8\u673a\u8fd0\u52a8\u8303\u56f4\u6709\u9650\u65f6\u7684\u51e0\u4f55\u8868\u793a\u548c\u76f8\u673a\u53c2\u6570\u4f30\u8ba1\u95ee\u9898\u3002", "motivation": "\u73b0\u6709NeRF\u65b9\u6cd5\u5728\u76f8\u673a\u8fd0\u52a8\u8303\u56f4\u6709\u9650\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u51e0\u4f55\u8868\u793a\u9519\u8bef\u548c\u76f8\u673a\u53c2\u6570\u4f30\u8ba1\u4e0d\u51c6\u786e\u3002", "method": "\u63d0\u51faDDR\u65b9\u6cd5\uff0c\u901a\u8fc7Gumbel-softmax\u91c7\u6837\u548c\u51e0\u4f55\u7ea6\u675f\u6539\u8fdb\u6df1\u5ea6\u6b63\u5219\u5316\uff0c\u5e76\u5f15\u5165\u76f8\u673a\u53c2\u6570\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5c0f\u76f8\u673a\u8fd0\u52a8\u8f93\u5165\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DDR\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5c0f\u76f8\u673a\u8fd0\u52a8\u573a\u666f\u4e0b\u7684\u51e0\u4f55\u548c\u76f8\u673a\u53c2\u6570\u95ee\u9898\u3002"}}
{"id": "2506.23157", "pdf": "https://arxiv.org/pdf/2506.23157", "abs": "https://arxiv.org/abs/2506.23157", "authors": ["Hanyu Zhou", "Haonan Wang", "Haoyue Liu", "Yuxing Duan", "Luxin Yan", "Gim Hee Lee"], "title": "STD-GS: Exploring Frame-Event Interaction for SpatioTemporal-Disentangled Gaussian Splatting to Reconstruct High-Dynamic Scene", "categories": ["cs.CV"], "comment": null, "summary": "High-dynamic scene reconstruction aims to represent static background with rigid spatial features and dynamic objects with deformed continuous spatiotemporal features. Typically, existing methods adopt unified representation model (e.g., Gaussian) to directly match the spatiotemporal features of dynamic scene from frame camera. However, this unified paradigm fails in the potential discontinuous temporal features of objects due to frame imaging and the heterogeneous spatial features between background and objects. To address this issue, we disentangle the spatiotemporal features into various latent representations to alleviate the spatiotemporal mismatching between background and objects. In this work, we introduce event camera to compensate for frame camera, and propose a spatiotemporal-disentangled Gaussian splatting framework for high-dynamic scene reconstruction. As for dynamic scene, we figure out that background and objects have appearance discrepancy in frame-based spatial features and motion discrepancy in event-based temporal features, which motivates us to distinguish the spatiotemporal features between background and objects via clustering. As for dynamic object, we discover that Gaussian representations and event data share the consistent spatiotemporal characteristic, which could serve as a prior to guide the spatiotemporal disentanglement of object Gaussians. Within Gaussian splatting framework, the cumulative scene-object disentanglement can improve the spatiotemporal discrimination between background and objects to render the time-continuous dynamic scene. Extensive experiments have been performed to verify the superiority of the proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u7a7a\u89e3\u8026\u7684\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u52a8\u6001\u573a\u666f\u91cd\u5efa\uff0c\u901a\u8fc7\u4e8b\u4ef6\u76f8\u673a\u8865\u507f\u5e27\u76f8\u673a\uff0c\u89e3\u51b3\u65f6\u7a7a\u7279\u5f81\u4e0d\u5339\u914d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u91c7\u7528\u7edf\u4e00\u8868\u793a\u6a21\u578b\uff08\u5982\u9ad8\u65af\uff09\u76f4\u63a5\u5339\u914d\u52a8\u6001\u573a\u666f\u7684\u65f6\u7a7a\u7279\u5f81\uff0c\u4f46\u65e0\u6cd5\u5904\u7406\u6f5c\u5728\u7684\u65f6\u95f4\u4e0d\u8fde\u7eed\u6027\u548c\u80cc\u666f\u4e0e\u5bf9\u8c61\u7684\u5f02\u8d28\u6027\u3002", "method": "\u5f15\u5165\u4e8b\u4ef6\u76f8\u673a\uff0c\u63d0\u51fa\u65f6\u7a7a\u89e3\u8026\u7684\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u7c7b\u533a\u5206\u80cc\u666f\u4e0e\u5bf9\u8c61\u7684\u65f6\u7a7a\u7279\u5f81\uff0c\u5e76\u5229\u7528\u9ad8\u65af\u8868\u793a\u4e0e\u4e8b\u4ef6\u6570\u636e\u7684\u4e00\u81f4\u6027\u6307\u5bfc\u5bf9\u8c61\u9ad8\u65af\u89e3\u8026\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u63d0\u9ad8\u4e86\u80cc\u666f\u4e0e\u5bf9\u8c61\u7684\u65f6\u7a7a\u533a\u5206\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u65f6\u95f4\u8fde\u7eed\u7684\u52a8\u6001\u573a\u666f\u6e32\u67d3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u65f6\u7a7a\u89e3\u8026\u663e\u8457\u6539\u5584\u4e86\u9ad8\u52a8\u6001\u573a\u666f\u7684\u91cd\u5efa\u6548\u679c\u3002"}}
{"id": "2506.23205", "pdf": "https://arxiv.org/pdf/2506.23205", "abs": "https://arxiv.org/abs/2506.23205", "authors": ["Dequan Kong", "Zhe Zhu", "Honghua Chen", "Mingqiang Wei"], "title": "BridgeShape: Latent Diffusion Schr\u00f6dinger Bridge for 3D Shape Completion", "categories": ["cs.CV"], "comment": null, "summary": "Existing diffusion-based 3D shape completion methods typically use a conditional paradigm, injecting incomplete shape information into the denoising network via deep feature interactions (e.g., concatenation, cross-attention) to guide sampling toward complete shapes, often represented by voxel-based distance functions. However, these approaches fail to explicitly model the optimal global transport path, leading to suboptimal completions. Moreover, performing diffusion directly in voxel space imposes resolution constraints, limiting the generation of fine-grained geometric details. To address these challenges, we propose BridgeShape, a novel framework for 3D shape completion via latent diffusion Schr\\\"odinger bridge. The key innovations lie in two aspects: (i) BridgeShape formulates shape completion as an optimal transport problem, explicitly modeling the transition between incomplete and complete shapes to ensure a globally coherent transformation. (ii) We introduce a Depth-Enhanced Vector Quantized Variational Autoencoder (VQ-VAE) to encode 3D shapes into a compact latent space, leveraging self-projected multi-view depth information enriched with strong DINOv2 features to enhance geometric structural perception. By operating in a compact yet structurally informative latent space, BridgeShape effectively mitigates resolution constraints and enables more efficient and high-fidelity 3D shape completion. BridgeShape achieves state-of-the-art performance on large-scale 3D shape completion benchmarks, demonstrating superior fidelity at higher resolutions and for unseen object classes.", "AI": {"tldr": "BridgeShape\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6269\u6563Schr\u00f6dinger\u6865\u76843D\u5f62\u72b6\u8865\u5168\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u6700\u4f18\u4f20\u8f93\u8def\u5f84\u548c\u7d27\u51d1\u7684\u6f5c\u5728\u7a7a\u95f4\u7f16\u7801\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5168\u5c40\u4e00\u81f4\u6027\u548c\u5206\u8fa8\u7387\u9650\u5236\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u76843D\u5f62\u72b6\u8865\u5168\u65b9\u6cd5\u901a\u8fc7\u6df1\u5ea6\u7279\u5f81\u4ea4\u4e92\u6ce8\u5165\u4e0d\u5b8c\u6574\u5f62\u72b6\u4fe1\u606f\uff0c\u4f46\u672a\u80fd\u663e\u5f0f\u5efa\u6a21\u6700\u4f18\u5168\u5c40\u4f20\u8f93\u8def\u5f84\uff0c\u4e14\u53d7\u9650\u4e8e\u4f53\u7d20\u7a7a\u95f4\u7684\u5206\u8fa8\u7387\u7ea6\u675f\u3002", "method": "BridgeShape\u5c06\u5f62\u72b6\u8865\u5168\u5efa\u6a21\u4e3a\u6700\u4f18\u4f20\u8f93\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u6df1\u5ea6\u589e\u5f3a\u7684VQ-VAE\u7f16\u78013D\u5f62\u72b6\u5230\u7d27\u51d1\u6f5c\u5728\u7a7a\u95f4\uff0c\u7ed3\u5408\u591a\u89c6\u89d2\u6df1\u5ea6\u4fe1\u606f\u548cDINOv2\u7279\u5f81\u589e\u5f3a\u51e0\u4f55\u611f\u77e5\u3002", "result": "BridgeShape\u5728\u5927\u89c4\u6a213D\u5f62\u72b6\u8865\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u652f\u6301\u66f4\u9ad8\u5206\u8fa8\u7387\u548c\u672a\u89c1\u7269\u4f53\u7c7b\u522b\u7684\u8865\u5168\u3002", "conclusion": "BridgeShape\u901a\u8fc7\u6f5c\u5728\u6269\u6563Schr\u00f6dinger\u6865\u548c\u6df1\u5ea6\u589e\u5f3a\u7f16\u7801\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u5f62\u72b6\u8865\u5168\u7684\u5168\u5c40\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2506.23207", "pdf": "https://arxiv.org/pdf/2506.23207", "abs": "https://arxiv.org/abs/2506.23207", "authors": ["Zhen Tan", "Xieyuanli Chen", "Lei Feng", "Yangbing Ge", "Shuaifeng Zhi", "Jiaxiong Liu", "Dewen Hu"], "title": "TVG-SLAM: Robust Gaussian Splatting SLAM with Tri-view Geometric Constraints", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled RGB-only SLAM systems to achieve high-fidelity scene representation. However, the heavy reliance of existing systems on photometric rendering loss for camera tracking undermines their robustness, especially in unbounded outdoor environments with severe viewpoint and illumination changes. To address these challenges, we propose TVG-SLAM, a robust RGB-only 3DGS SLAM system that leverages a novel tri-view geometry paradigm to ensure consistent tracking and high-quality mapping. We introduce a dense tri-view matching module that aggregates reliable pairwise correspondences into consistent tri-view matches, forming robust geometric constraints across frames. For tracking, we propose Hybrid Geometric Constraints, which leverage tri-view matches to construct complementary geometric cues alongside photometric loss, ensuring accurate and stable pose estimation even under drastic viewpoint shifts and lighting variations. For mapping, we propose a new probabilistic initialization strategy that encodes geometric uncertainty from tri-view correspondences into newly initialized Gaussians. Additionally, we design a Dynamic Attenuation of Rendering Trust mechanism to mitigate tracking drift caused by mapping latency. Experiments on multiple public outdoor datasets show that our TVG-SLAM outperforms prior RGB-only 3DGS-based SLAM systems. Notably, in the most challenging dataset, our method improves tracking robustness, reducing the average Absolute Trajectory Error (ATE) by 69.0\\% while achieving state-of-the-art rendering quality. The implementation of our method will be released as open-source.", "AI": {"tldr": "TVG-SLAM\u662f\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684RGB-only SLAM\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e09\u89c6\u56fe\u51e0\u4f55\u8303\u5f0f\u63d0\u5347\u8ddf\u8e2a\u548c\u5efa\u56fe\u7684\u9c81\u68d2\u6027\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8f68\u8ff9\u8bef\u5dee\u3002", "motivation": "\u73b0\u6709RGB-only SLAM\u7cfb\u7edf\u4f9d\u8d56\u5149\u5ea6\u6e32\u67d3\u635f\u5931\uff0c\u5728\u65e0\u8fb9\u754c\u6237\u5916\u73af\u5883\u4e2d\u56e0\u89c6\u89d2\u548c\u5149\u7167\u53d8\u5316\u5bfc\u81f4\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e09\u89c6\u56fe\u5339\u914d\u6a21\u5757\u548c\u6df7\u5408\u51e0\u4f55\u7ea6\u675f\uff0c\u7ed3\u5408\u51e0\u4f55\u4e0e\u5149\u5ea6\u635f\u5931\uff1b\u5f15\u5165\u6982\u7387\u521d\u59cb\u5316\u7b56\u7565\u548c\u52a8\u6001\u6e32\u67d3\u4fe1\u4efb\u8870\u51cf\u673a\u5236\u3002", "result": "\u5728\u591a\u4e2a\u6237\u5916\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6700\u6311\u6218\u6027\u6570\u636e\u96c6\u7684\u7edd\u5bf9\u8f68\u8ff9\u8bef\u5dee\u964d\u4f4e69.0%\uff0c\u6e32\u67d3\u8d28\u91cf\u8fbe\u5230SOTA\u3002", "conclusion": "TVG-SLAM\u901a\u8fc7\u51e0\u4f55\u589e\u5f3a\u663e\u8457\u63d0\u5347\u4e86RGB-only SLAM\u7684\u9c81\u68d2\u6027\u548c\u6e32\u67d3\u8d28\u91cf\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2506.23254", "pdf": "https://arxiv.org/pdf/2506.23254", "abs": "https://arxiv.org/abs/2506.23254", "authors": ["Aradhana Mishra", "Bumshik Lee"], "title": "PixelBoost: Leveraging Brownian Motion for Realistic-Image Super-Resolution", "categories": ["cs.CV", "cs.AI", "cs.MM", "eess.IV"], "comment": null, "summary": "Diffusion-model-based image super-resolution techniques often face a trade-off between realistic image generation and computational efficiency. This issue is exacerbated when inference times by decreasing sampling steps, resulting in less realistic and hazy images. To overcome this challenge, we introduce a novel diffusion model named PixelBoost that underscores the significance of embracing the stochastic nature of Brownian motion in advancing image super-resolution, resulting in a high degree of realism, particularly focusing on texture and edge definitions. By integrating controlled stochasticity into the training regimen, our proposed model avoids convergence to local optima, effectively capturing and reproducing the inherent uncertainty of image textures and patterns. Our proposed model demonstrates superior objective results in terms of learned perceptual image patch similarity (LPIPS), lightness order error (LOE), peak signal-to-noise ratio(PSNR), structural similarity index measure (SSIM), as well as visual quality. To determine the edge enhancement, we evaluated the gradient magnitude and pixel value, and our proposed model exhibited a better edge reconstruction capability. Additionally, our model demonstrates adaptive learning capabilities by effectively adjusting to Brownian noise patterns and introduces a sigmoidal noise sequencing method that simplifies training, resulting in faster inference speeds.", "AI": {"tldr": "PixelBoost\u662f\u4e00\u79cd\u65b0\u578b\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u5e03\u6717\u8fd0\u52a8\u7684\u968f\u673a\u6027\u63d0\u5347\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff0c\u517c\u987e\u771f\u5b9e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\u65f6\u5bfc\u81f4\u7684\u56fe\u50cf\u6a21\u7cca\u548c\u771f\u5b9e\u6027\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u6574\u5408\u53d7\u63a7\u968f\u673a\u6027\u5230\u8bad\u7ec3\u4e2d\uff0c\u907f\u514d\u5c40\u90e8\u6700\u4f18\uff0c\u5e76\u91c7\u7528sigmoidal\u566a\u58f0\u6392\u5e8f\u65b9\u6cd5\u7b80\u5316\u8bad\u7ec3\u3002", "result": "\u5728LPIPS\u3001LOE\u3001PSNR\u3001SSIM\u7b49\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8fb9\u7f18\u91cd\u5efa\u80fd\u529b\u66f4\u5f3a\uff0c\u63a8\u7406\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "PixelBoost\u901a\u8fc7\u968f\u673a\u6027\u548c\u81ea\u9002\u5e94\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u771f\u5b9e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2506.23263", "pdf": "https://arxiv.org/pdf/2506.23263", "abs": "https://arxiv.org/abs/2506.23263", "authors": ["Lei-lei Li", "Jianwu Fang", "Junbin Xiao", "Shanmin Pang", "Hongkai Yu", "Chen Lv", "Jianru Xue", "Tat-Seng Chua"], "title": "Causal-Entity Reflected Egocentric Traffic Accident Video Synthesis", "categories": ["cs.CV"], "comment": "Accepted by ICCV2025", "summary": "Egocentricly comprehending the causes and effects of car accidents is crucial for the safety of self-driving cars, and synthesizing causal-entity reflected accident videos can facilitate the capability test to respond to unaffordable accidents in reality. However, incorporating causal relations as seen in real-world videos into synthetic videos remains challenging. This work argues that precisely identifying the accident participants and capturing their related behaviors are of critical importance. In this regard, we propose a novel diffusion model, Causal-VidSyn, for synthesizing egocentric traffic accident videos. To enable causal entity grounding in video diffusion, Causal-VidSyn leverages the cause descriptions and driver fixations to identify the accident participants and behaviors, facilitated by accident reason answering and gaze-conditioned selection modules. To support Causal-VidSyn, we further construct Drive-Gaze, the largest driver gaze dataset (with 1.54M frames of fixations) in driving accident scenarios. Extensive experiments show that Causal-VidSyn surpasses state-of-the-art video diffusion models in terms of frame quality and causal sensitivity in various tasks, including accident video editing, normal-to-accident video diffusion, and text-to-video generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCausal-VidSyn\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u5408\u6210\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u4ea4\u901a\u4e8b\u6545\u89c6\u9891\uff0c\u901a\u8fc7\u539f\u56e0\u63cf\u8ff0\u548c\u9a7e\u9a76\u5458\u6ce8\u89c6\u70b9\u8bc6\u522b\u4e8b\u6545\u53c2\u4e0e\u8005\u548c\u884c\u4e3a\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4e3a\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7684\u5b89\u5168\u6d4b\u8bd5\u63d0\u4f9b\u5408\u6210\u4e8b\u6545\u89c6\u9891\uff0c\u4ee5\u5e94\u5bf9\u73b0\u5b9e\u4e2d\u96be\u4ee5\u627f\u53d7\u7684\u4e8b\u6545\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u5408\u6210\u89c6\u9891\u4e2d\u878d\u5165\u771f\u5b9e\u89c6\u9891\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\u3002", "method": "\u63d0\u51faCausal-VidSyn\u6269\u6563\u6a21\u578b\uff0c\u5229\u7528\u539f\u56e0\u63cf\u8ff0\u548c\u9a7e\u9a76\u5458\u6ce8\u89c6\u70b9\uff0c\u901a\u8fc7\u4e8b\u6545\u539f\u56e0\u56de\u7b54\u548c\u6ce8\u89c6\u6761\u4ef6\u9009\u62e9\u6a21\u5757\u8bc6\u522b\u4e8b\u6545\u53c2\u4e0e\u8005\u548c\u884c\u4e3a\u3002", "result": "Causal-VidSyn\u5728\u5e27\u8d28\u91cf\u548c\u56e0\u679c\u654f\u611f\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u652f\u6301\u4e8b\u6545\u89c6\u9891\u7f16\u8f91\u3001\u6b63\u5e38\u5230\u4e8b\u6545\u89c6\u9891\u6269\u6563\u548c\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u7b49\u4efb\u52a1\u3002", "conclusion": "Causal-VidSyn\u901a\u8fc7\u7ed3\u5408\u56e0\u679c\u5173\u7cfb\u548c\u9a7e\u9a76\u5458\u6ce8\u89c6\u70b9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5408\u6210\u4e8b\u6545\u89c6\u9891\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.23282", "pdf": "https://arxiv.org/pdf/2506.23282", "abs": "https://arxiv.org/abs/2506.23282", "authors": ["Hanwen Zhang", "Congqi Cao", "Qinyi Lv", "Lingtong Min", "Yanning Zhang"], "title": "Autoregressive Denoising Score Matching is a Good Video Anomaly Detector", "categories": ["cs.CV"], "comment": null, "summary": "Video anomaly detection (VAD) is an important computer vision problem. Thanks to the mode coverage capabilities of generative models, the likelihood-based paradigm is catching growing interest, as it can model normal distribution and detect out-of-distribution anomalies. However, these likelihood-based methods are blind to the anomalies located in local modes near the learned distribution. To handle these ``unseen\" anomalies, we dive into three gaps uniquely existing in VAD regarding scene, motion and appearance. Specifically, we first build a noise-conditioned score transformer for denoising score matching. Then, we introduce a scene-dependent and motion-aware score function by embedding the scene condition of input sequences into our model and assigning motion weights based on the difference between key frames of input sequences. Next, to solve the problem of blindness in principle, we integrate unaffected visual information via a novel autoregressive denoising score matching mechanism for inference. Through autoregressively injecting intensifying Gaussian noise into the denoised data and estimating the corresponding score function, we compare the denoised data with the original data to get a difference and aggregate it with the score function for an enhanced appearance perception and accumulate the abnormal context. With all three gaps considered, we can compute a more comprehensive anomaly indicator. Experiments on three popular VAD benchmarks demonstrate the state-of-the-art performance of our method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u566a\u58f0\u6761\u4ef6\u8bc4\u5206\u53d8\u6362\u5668\u548c\u573a\u666f\u4f9d\u8d56\u7684\u8fd0\u52a8\u611f\u77e5\u8bc4\u5206\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u5c40\u90e8\u5f02\u5e38\u76f2\u533a\u7684\u95ee\u9898\u3002", "motivation": "\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u4e2d\uff0c\u4f20\u7edf\u57fa\u4e8e\u4f3c\u7136\u7684\u65b9\u6cd5\u65e0\u6cd5\u68c0\u6d4b\u5230\u5c40\u90e8\u6a21\u5f0f\u9644\u8fd1\u7684\u5f02\u5e38\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5173\u6ce8\u573a\u666f\u3001\u8fd0\u52a8\u548c\u5916\u89c2\u4e09\u4e2a\u65b9\u9762\u7684\u72ec\u7279\u5dee\u8ddd\u3002", "method": "1. \u6784\u5efa\u566a\u58f0\u6761\u4ef6\u8bc4\u5206\u53d8\u6362\u5668\u8fdb\u884c\u53bb\u566a\u8bc4\u5206\u5339\u914d\uff1b2. \u5f15\u5165\u573a\u666f\u4f9d\u8d56\u548c\u8fd0\u52a8\u611f\u77e5\u7684\u8bc4\u5206\u51fd\u6570\uff1b3. \u4f7f\u7528\u81ea\u56de\u5f52\u53bb\u566a\u8bc4\u5206\u5339\u914d\u673a\u5236\u589e\u5f3a\u5f02\u5e38\u611f\u77e5\u3002", "result": "\u5728\u4e09\u4e2a\u6d41\u884c\u7684\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u7efc\u5408\u8003\u8651\u573a\u666f\u3001\u8fd0\u52a8\u548c\u5916\u89c2\u4e09\u4e2a\u65b9\u9762\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u672c\u6587\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2506.23295", "pdf": "https://arxiv.org/pdf/2506.23295", "abs": "https://arxiv.org/abs/2506.23295", "authors": ["Xiang Xu"], "title": "DiffFit: Disentangled Garment Warping and Texture Refinement for Virtual Try-On", "categories": ["cs.CV"], "comment": null, "summary": "Virtual try-on (VTON) aims to synthesize realistic images of a person wearing a target garment, with broad applications in e-commerce and digital fashion. While recent advances in latent diffusion models have substantially improved visual quality, existing approaches still struggle with preserving fine-grained garment details, achieving precise garment-body alignment, maintaining inference efficiency, and generalizing to diverse poses and clothing styles. To address these challenges, we propose DiffFit, a novel two-stage latent diffusion framework for high-fidelity virtual try-on. DiffFit adopts a progressive generation strategy: the first stage performs geometry-aware garment warping, aligning the garment with the target body through fine-grained deformation and pose adaptation. The second stage refines texture fidelity via a cross-modal conditional diffusion model that integrates the warped garment, the original garment appearance, and the target person image for high-quality rendering. By decoupling geometric alignment and appearance refinement, DiffFit effectively reduces task complexity and enhances both generation stability and visual realism. It excels in preserving garment-specific attributes such as textures, wrinkles, and lighting, while ensuring accurate alignment with the human body. Extensive experiments on large-scale VTON benchmarks demonstrate that DiffFit achieves superior performance over existing state-of-the-art methods in both quantitative metrics and perceptual evaluations.", "AI": {"tldr": "DiffFit\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6f5c\u5728\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u4fdd\u771f\u865a\u62df\u8bd5\u7a7f\uff0c\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u670d\u88c5\u53d8\u5f62\u548c\u8de8\u6a21\u6001\u6761\u4ef6\u6269\u6563\u6a21\u578b\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u7ec6\u8282\u4fdd\u7559\u3001\u5bf9\u9f50\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u865a\u62df\u8bd5\u7a7f\u65b9\u6cd5\u5728\u4fdd\u7559\u670d\u88c5\u7ec6\u8282\u3001\u7cbe\u786e\u5bf9\u9f50\u3001\u6548\u7387\u548c\u591a\u6837\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "DiffFit\u91c7\u7528\u4e24\u9636\u6bb5\u7b56\u7565\uff1a\u7b2c\u4e00\u9636\u6bb5\u8fdb\u884c\u51e0\u4f55\u611f\u77e5\u7684\u670d\u88c5\u53d8\u5f62\u548c\u5bf9\u9f50\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u8de8\u6a21\u6001\u6761\u4ef6\u6269\u6563\u6a21\u578b\u4f18\u5316\u7eb9\u7406\u4fdd\u771f\u5ea6\u3002", "result": "DiffFit\u5728\u5b9a\u91cf\u6307\u6807\u548c\u611f\u77e5\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u4fdd\u7559\u670d\u88c5\u7ec6\u8282\u5e76\u5b9e\u73b0\u7cbe\u786e\u5bf9\u9f50\u3002", "conclusion": "DiffFit\u901a\u8fc7\u89e3\u8026\u51e0\u4f55\u5bf9\u9f50\u548c\u5916\u89c2\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u865a\u62df\u8bd5\u7a7f\u7684\u751f\u6210\u7a33\u5b9a\u6027\u548c\u89c6\u89c9\u771f\u5b9e\u611f\u3002"}}
{"id": "2506.23308", "pdf": "https://arxiv.org/pdf/2506.23308", "abs": "https://arxiv.org/abs/2506.23308", "authors": ["Yiming Huang", "Long Bai", "Beilei Cui", "Yanheng Li", "Tong Chen", "Jie Wang", "Jinlin Wu", "Zhen Lei", "Hongbin Liu", "Hongliang Ren"], "title": "Endo-4DGX: Robust Endoscopic Scene Reconstruction and Illumination Correction with Gaussian Splatting", "categories": ["cs.CV"], "comment": "MICCAI 2025. Project Page:   https://lastbasket.github.io/MICCAI-2025-Endo-4DGX/", "summary": "Accurate reconstruction of soft tissue is crucial for advancing automation in image-guided robotic surgery. The recent 3D Gaussian Splatting (3DGS) techniques and their variants, 4DGS, achieve high-quality renderings of dynamic surgical scenes in real-time. However, 3D-GS-based methods still struggle in scenarios with varying illumination, such as low light and over-exposure. Training 3D-GS in such extreme light conditions leads to severe optimization problems and devastating rendering quality. To address these challenges, we present Endo-4DGX, a novel reconstruction method with illumination-adaptive Gaussian Splatting designed specifically for endoscopic scenes with uneven lighting. By incorporating illumination embeddings, our method effectively models view-dependent brightness variations. We introduce a region-aware enhancement module to model the sub-area lightness at the Gaussian level and a spatial-aware adjustment module to learn the view-consistent brightness adjustment. With the illumination adaptive design, Endo-4DGX achieves superior rendering performance under both low-light and over-exposure conditions while maintaining geometric accuracy. Additionally, we employ an exposure control loss to restore the appearance from adverse exposure to the normal level for illumination-adaptive optimization. Experimental results demonstrate that Endo-4DGX significantly outperforms combinations of state-of-the-art reconstruction and restoration methods in challenging lighting environments, underscoring its potential to advance robot-assisted surgical applications. Our code is available at https://github.com/lastbasket/Endo-4DGX.", "AI": {"tldr": "Endo-4DGX\u662f\u4e00\u79cd\u9488\u5bf9\u5185\u7aa5\u955c\u573a\u666f\u4e2d\u4e0d\u5747\u5300\u5149\u7167\u7684\u65b0\u578b\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u5149\u7167\u81ea\u9002\u5e94\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u89e3\u51b3\u4e863DGS\u5728\u6781\u7aef\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u5728\u56fe\u50cf\u5f15\u5bfc\u673a\u5668\u4eba\u624b\u672f\u4e2d\uff0c\u8f6f\u7ec4\u7ec7\u7684\u7cbe\u786e\u91cd\u5efa\u5bf9\u81ea\u52a8\u5316\u81f3\u5173\u91cd\u8981\u30023DGS\u53ca\u5176\u53d8\u4f534DGS\u5728\u52a8\u6001\u624b\u672f\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5b9e\u65f6\u6e32\u67d3\uff0c\u4f46\u5728\u6781\u7aef\u5149\u7167\u6761\u4ef6\u4e0b\uff08\u5982\u4f4e\u5149\u548c\u8fc7\u66dd\uff09\u8868\u73b0\u4e0d\u4f73\u3002", "method": "Endo-4DGX\u7ed3\u5408\u5149\u7167\u5d4c\u5165\u3001\u533a\u57df\u611f\u77e5\u589e\u5f3a\u6a21\u5757\u548c\u7a7a\u95f4\u611f\u77e5\u8c03\u6574\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u5149\u7167\u81ea\u9002\u5e94\u7684\u91cd\u5efa\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u66dd\u5149\u63a7\u5236\u635f\u5931\u4ee5\u6062\u590d\u5916\u89c2\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEndo-4DGX\u5728\u6311\u6218\u6027\u5149\u7167\u73af\u5883\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u91cd\u5efa\u548c\u6062\u590d\u65b9\u6cd5\u7684\u7ec4\u5408\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u51e0\u4f55\u7cbe\u5ea6\u3002", "conclusion": "Endo-4DGX\u5728\u6781\u7aef\u5149\u7167\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u6709\u671b\u63a8\u52a8\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u7684\u5e94\u7528\u3002"}}
{"id": "2506.23323", "pdf": "https://arxiv.org/pdf/2506.23323", "abs": "https://arxiv.org/abs/2506.23323", "authors": ["Quang-Huy Che", "Vinh-Tiep Nguyen"], "title": "FastSeg: Efficient Training-Free Open-Vocabulary Segmentation via Hierarchical Attention Refinement Method", "categories": ["cs.CV"], "comment": null, "summary": "Open-vocabulary semantic segmentation (OVSS) aims to segment objects from arbitrary text categories without requiring densely annotated datasets. Although contrastive learning based models enable zero-shot segmentation, they often lose fine spatial precision at pixel level, due to global representation bias. In contrast, diffusion-based models naturally encode fine-grained spatial features via attention mechanisms that capture both global context and local details. However, they often face challenges in balancing the number of iterations with the quality of the segmentation. In this work, we propose FastSeg, a novel and efficient training-free framework with only (1+1)-step of reverse process of a pretrained diffusion model (e.g., Stable Diffusion). Moreover, instead of running multiple times for different classes, FastSeg performs segmentation for all classes at once. To further enhance the segmentation quality, FastSeg introduces three key components: (i) a dual-prompt mechanism for discriminative, class-aware attention extraction, (ii) a Hierarchical Attention Refinement Method (HARD) that enhances fused cross-attention using scale-aligned selfattention maps, and (iii) a Test-Time Flipping (TTF) scheme designed to improve spatial consistency. Extensive experiments show that FastSeg achieves state-of-the-art training-free performance, obtaining 43.8% average mIoU across PASCAL VOC, PASCAL Context, and COCO Object benchmarks while maintaining superior inference efficiency. Our results demonstrate that FastSeg provides a strong foundation for extendability, bridging the gap between segmentation quality and inference efficiency.", "AI": {"tldr": "FastSeg\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u8bad\u7ec3\u65e0\u5173\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff08\u5982Stable Diffusion\uff09\u7684\uff081+1\uff09\u6b65\u53cd\u5411\u8fc7\u7a0b\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\uff08OVSS\uff09\uff0c\u5e76\u5728\u5355\u6b21\u8fd0\u884c\u4e2d\u5b8c\u6210\u6240\u6709\u7c7b\u522b\u7684\u5206\u5272\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5bf9\u6bd4\u5b66\u4e60\u6a21\u578b\u5728\u50cf\u7d20\u7ea7\u522b\u7a7a\u95f4\u7cbe\u5ea6\u4e0d\u8db3\u548c\u6269\u6563\u6a21\u578b\u5728\u8fed\u4ee3\u6b21\u6570\u4e0e\u5206\u5272\u8d28\u91cf\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faFastSeg\u6846\u67b6\uff0c\u5305\u542b\u53cc\u63d0\u793a\u673a\u5236\u3001\u5206\u5c42\u6ce8\u610f\u529b\u7ec6\u5316\u65b9\u6cd5\uff08HARD\uff09\u548c\u6d4b\u8bd5\u65f6\u7ffb\u8f6c\uff08TTF\uff09\u65b9\u6848\uff0c\u4ee5\u63d0\u9ad8\u5206\u5272\u8d28\u91cf\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5728PASCAL VOC\u3001PASCAL Context\u548cCOCO Object\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747mIoU\u8fbe\u523043.8%\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u63a8\u7406\u3002", "conclusion": "FastSeg\u5728\u5206\u5272\u8d28\u91cf\u548c\u63a8\u7406\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u4e3a\u6269\u5c55\u6027\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2506.23347", "pdf": "https://arxiv.org/pdf/2506.23347", "abs": "https://arxiv.org/abs/2506.23347", "authors": ["Yi Liu", "Shengqian Li", "Zuzeng Lin", "Feng Wang", "Si Liu"], "title": "CycleVAR: Repurposing Autoregressive Model for Unsupervised One-Step Image Translation", "categories": ["cs.CV"], "comment": null, "summary": "The current conditional autoregressive image generation methods have shown promising results, yet their potential remains largely unexplored in the practical unsupervised image translation domain, which operates without explicit cross-domain correspondences. A critical limitation stems from the discrete quantization inherent in traditional Vector Quantization-based frameworks, which disrupts gradient flow between the Variational Autoencoder decoder and causal Transformer, impeding end-to-end optimization during adversarial training in image space. To tackle this issue, we propose using Softmax Relaxed Quantization, a novel approach that reformulates codebook selection as a continuous probability mixing process via Softmax, thereby preserving gradient propagation. Building upon this differentiable foundation, we introduce CycleVAR, which reformulates image-to-image translation as image-conditional visual autoregressive generation by injecting multi-scale source image tokens as contextual prompts, analogous to prefix-based conditioning in language models. CycleVAR exploits two modes to generate the target image tokens, including (1) serial multi-step generation, enabling iterative refinement across scales, and (2) parallel one-step generation synthesizing all resolution outputs in a single forward pass. Experimental findings indicate that the parallel one-step generation mode attains superior translation quality with quicker inference speed than the serial multi-step mode in unsupervised scenarios. Furthermore, both quantitative and qualitative results indicate that CycleVAR surpasses previous state-of-the-art unsupervised image translation models, \\textit{e}.\\textit{g}., CycleGAN-Turbo.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCycleVAR\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7Softmax Relaxed Quantization\u89e3\u51b3\u4f20\u7edf\u91cf\u5316\u65b9\u6cd5\u5728\u56fe\u50cf\u7ffb\u8bd1\u4e2d\u7684\u68af\u5ea6\u95ee\u9898\uff0c\u5e76\u5728\u65e0\u76d1\u7763\u56fe\u50cf\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63a2\u7d22\u6761\u4ef6\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u5728\u65e0\u76d1\u7763\u56fe\u50cf\u7ffb\u8bd1\u9886\u57df\u7684\u6f5c\u529b\uff0c\u89e3\u51b3\u4f20\u7edf\u91cf\u5316\u65b9\u6cd5\u5bfc\u81f4\u7684\u68af\u5ea6\u4e2d\u65ad\u95ee\u9898\u3002", "method": "\u4f7f\u7528Softmax Relaxed Quantization\u4fdd\u6301\u68af\u5ea6\u4f20\u64ad\uff0c\u5e76\u5f15\u5165CycleVAR\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u6e90\u56fe\u50cf\u6807\u8bb0\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u63d0\u793a\uff0c\u5b9e\u73b0\u56fe\u50cf\u5230\u56fe\u50cf\u7684\u7ffb\u8bd1\u3002", "result": "CycleVAR\u5728\u65e0\u76d1\u7763\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5982CycleGAN-Turbo\uff09\uff0c\u5e76\u884c\u5355\u6b65\u751f\u6210\u6a21\u5f0f\u5728\u8d28\u91cf\u548c\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u4e32\u884c\u591a\u6b65\u6a21\u5f0f\u3002", "conclusion": "CycleVAR\u901a\u8fc7\u6539\u8fdb\u91cf\u5316\u65b9\u6cd5\u548c\u591a\u5c3a\u5ea6\u751f\u6210\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u76d1\u7763\u56fe\u50cf\u7ffb\u8bd1\u7684\u6027\u80fd\u3002"}}
{"id": "2506.23353", "pdf": "https://arxiv.org/pdf/2506.23353", "abs": "https://arxiv.org/abs/2506.23353", "authors": ["Siyuan Chai", "Xiaodong Guo", "Tong Liu"], "title": "Layer Decomposition and Morphological Reconstruction for Task-Oriented Infrared Image Enhancement", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Infrared image helps improve the perception capabilities of autonomous driving in complex weather conditions such as fog, rain, and low light. However, infrared image often suffers from low contrast, especially in non-heat-emitting targets like bicycles, which significantly affects the performance of downstream high-level vision tasks. Furthermore, achieving contrast enhancement without amplifying noise and losing important information remains a challenge. To address these challenges, we propose a task-oriented infrared image enhancement method. Our approach consists of two key components: layer decomposition and saliency information extraction. First, we design an layer decomposition method for infrared images, which enhances scene details while preserving dark region features, providing more features for subsequent saliency information extraction. Then, we propose a morphological reconstruction-based saliency extraction method that effectively extracts and enhances target information without amplifying noise. Our method improves the image quality for object detection and semantic segmentation tasks. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4efb\u52a1\u5bfc\u5411\u7684\u7ea2\u5916\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u5206\u89e3\u548c\u663e\u8457\u6027\u4fe1\u606f\u63d0\u53d6\uff0c\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u4ee5\u652f\u6301\u76ee\u6807\u68c0\u6d4b\u548c\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u3002", "motivation": "\u7ea2\u5916\u56fe\u50cf\u5728\u590d\u6742\u5929\u6c14\u6761\u4ef6\u4e0b\uff08\u5982\u96fe\u3001\u96e8\u3001\u4f4e\u5149\uff09\u80fd\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7684\u611f\u77e5\u80fd\u529b\uff0c\u4f46\u4f4e\u5bf9\u6bd4\u5ea6\u548c\u566a\u58f0\u95ee\u9898\u5f71\u54cd\u4e86\u9ad8\u7ea7\u89c6\u89c9\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5c42\u5206\u89e3\uff08\u589e\u5f3a\u573a\u666f\u7ec6\u8282\u5e76\u4fdd\u7559\u6697\u533a\u7279\u5f81\uff09\u548c\u57fa\u4e8e\u5f62\u6001\u91cd\u5efa\u7684\u663e\u8457\u6027\u63d0\u53d6\uff08\u589e\u5f3a\u76ee\u6807\u4fe1\u606f\u4e14\u4e0d\u653e\u5927\u566a\u58f0\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u76ee\u6807\u68c0\u6d4b\u548c\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u7ea2\u5916\u56fe\u50cf\u4f4e\u5bf9\u6bd4\u5ea6\u548c\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u9ad8\u7ea7\u89c6\u89c9\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2506.23418", "pdf": "https://arxiv.org/pdf/2506.23418", "abs": "https://arxiv.org/abs/2506.23418", "authors": ["Parham Rezaei", "Arash Marioriyad", "Mahdieh Soleymani Baghshah", "Mohammad Hossein Rohban"], "title": "Why Settle for Mid: A Probabilistic Viewpoint to Spatial Relationship Alignment in Text-to-image Models", "categories": ["cs.CV"], "comment": "12 main pages, 18 figures, and 16 tables", "summary": "Despite the ability of text-to-image models to generate high-quality, realistic, and diverse images, they face challenges in compositional generation, often struggling to accurately represent details specified in the input prompt. A prevalent issue in compositional generation is the misalignment of spatial relationships, as models often fail to faithfully generate images that reflect the spatial configurations specified between objects in the input prompts. To address this challenge, we propose a novel probabilistic framework for modeling the relative spatial positioning of objects in a scene, leveraging the concept of Probability of Superiority (PoS). Building on this insight, we make two key contributions. First, we introduce a novel evaluation metric, PoS-based Evaluation (PSE), designed to assess the alignment of 2D and 3D spatial relationships between text and image, with improved adherence to human judgment. Second, we propose PoS-based Generation (PSG), an inference-time method that improves the alignment of 2D and 3D spatial relationships in T2I models without requiring fine-tuning. PSG employs a Part-of-Speech PoS-based reward function that can be utilized in two distinct ways: (1) as a gradient-based guidance mechanism applied to the cross-attention maps during the denoising steps, or (2) as a search-based strategy that evaluates a set of initial noise vectors to select the best one. Extensive experiments demonstrate that the PSE metric exhibits stronger alignment with human judgment compared to traditional center-based metrics, providing a more nuanced and reliable measure of complex spatial relationship accuracy in text-image alignment. Furthermore, PSG significantly enhances the ability of text-to-image models to generate images with specified spatial configurations, outperforming state-of-the-art methods across multiple evaluation metrics and benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u4f18\u52bf\uff08PoS\uff09\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u7a7a\u95f4\u5173\u7cfb\u751f\u6210\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff08PSE\uff09\u548c\u751f\u6210\u65b9\u6cd5\uff08PSG\uff09\u3002", "motivation": "\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u751f\u6210\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u65f6\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u5c24\u5176\u662f\u7a7a\u95f4\u914d\u7f6e\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faPoS\u6846\u67b6\uff0c\u5305\u62ecPSE\u8bc4\u4f30\u6307\u6807\u548cPSG\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u5f15\u5bfc\u6216\u641c\u7d22\u7b56\u7565\u4f18\u5316\u7a7a\u95f4\u5173\u7cfb\u751f\u6210\u3002", "result": "PSE\u6307\u6807\u4e0e\u4eba\u7c7b\u5224\u65ad\u66f4\u4e00\u81f4\uff0cPSG\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u95f4\u5173\u7cfb\u751f\u6210\u7684\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PoS\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u7a7a\u95f4\u5173\u7cfb\u5bf9\u9f50\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u573a\u666f\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2506.23440", "pdf": "https://arxiv.org/pdf/2506.23440", "abs": "https://arxiv.org/abs/2506.23440", "authors": ["Mahesh Bhosale", "Abdul Wasi", "Yuanhao Zhai", "Yunjie Tian", "Samuel Border", "Nan Xi", "Pinaki Sarder", "Junsong Yuan", "David Doermann", "Xuan Gong"], "title": "PathDiff: Histopathology Image Synthesis with Unpaired Text and Mask Conditions", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "Diffusion-based generative models have shown promise in synthesizing histopathology images to address data scarcity caused by privacy constraints. Diagnostic text reports provide high-level semantic descriptions, and masks offer fine-grained spatial structures essential for representing distinct morphological regions. However, public datasets lack paired text and mask data for the same histopathological images, limiting their joint use in image generation. This constraint restricts the ability to fully exploit the benefits of combining both modalities for enhanced control over semantics and spatial details. To overcome this, we propose PathDiff, a diffusion framework that effectively learns from unpaired mask-text data by integrating both modalities into a unified conditioning space. PathDiff allows precise control over structural and contextual features, generating high-quality, semantically accurate images. PathDiff also improves image fidelity, text-image alignment, and faithfulness, enhancing data augmentation for downstream tasks like nuclei segmentation and classification. Extensive experiments demonstrate its superiority over existing methods.", "AI": {"tldr": "PathDiff\u662f\u4e00\u79cd\u6269\u6563\u6846\u67b6\uff0c\u5229\u7528\u672a\u914d\u5bf9\u7684\u63a9\u7801-\u6587\u672c\u6570\u636e\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u75c5\u7406\u5b66\u56fe\u50cf\uff0c\u63d0\u5347\u8bed\u4e49\u548c\u7a7a\u95f4\u7ec6\u8282\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u75c5\u7406\u5b66\u56fe\u50cf\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u540c\u65f6\u5229\u7528\u6587\u672c\u548c\u63a9\u7801\u6570\u636e\u589e\u5f3a\u751f\u6210\u56fe\u50cf\u7684\u8bed\u4e49\u548c\u7ed3\u6784\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faPathDiff\u6846\u67b6\uff0c\u5c06\u672a\u914d\u5bf9\u7684\u63a9\u7801\u548c\u6587\u672c\u6570\u636e\u6574\u5408\u5230\u7edf\u4e00\u7684\u6761\u4ef6\u7a7a\u95f4\u4e2d\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002", "result": "PathDiff\u5728\u56fe\u50cf\u4fdd\u771f\u5ea6\u3001\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u548c\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u6838\u5206\u5272\u548c\u5206\u7c7b\uff09\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PathDiff\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u548c\u63a9\u7801\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u75c5\u7406\u5b66\u56fe\u50cf\u751f\u6210\u7684\u8bed\u4e49\u548c\u7ed3\u6784\u63a7\u5236\u80fd\u529b\u3002"}}
{"id": "2506.23460", "pdf": "https://arxiv.org/pdf/2506.23460", "abs": "https://arxiv.org/abs/2506.23460", "authors": ["Dewen Zeng", "Xinrong Hu", "Yu-Jen Chen", "Yawen Wu", "Xiaowei Xu", "Yiyu Shi"], "title": "Contrastive Learning with Diffusion Features for Weakly Supervised Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Weakly supervised semantic segmentation (WSSS) methods using class labels often rely on class activation maps (CAMs) to localize objects. However, traditional CAM-based methods struggle with partial activations and imprecise object boundaries due to optimization discrepancies between classification and segmentation. Recently, the conditional diffusion model (CDM) has been used as an alternative for generating segmentation masks in WSSS, leveraging its strong image generation capabilities tailored to specific class distributions. By modifying or perturbing the condition during diffusion sampling, the related objects can be highlighted in the generated images. Yet, the saliency maps generated by CDMs are prone to noise from background alterations during reverse diffusion. To alleviate the problem, we introduce Contrastive Learning with Diffusion Features (CLDF), a novel method that uses contrastive learning to train a pixel decoder to map the diffusion features from a frozen CDM to a low-dimensional embedding space for segmentation. Specifically, we integrate gradient maps generated from CDM external classifier with CAMs to identify foreground and background pixels with fewer false positives/negatives for contrastive learning, enabling robust pixel embedding learning. Experimental results on four segmentation tasks from two public medical datasets demonstrate that our method significantly outperforms existing baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5CLDF\uff0c\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u548c\u6269\u6563\u6a21\u578b\u7279\u5f81\u6539\u8fdb\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u4f20\u7edfCAM\u65b9\u6cd5\u5728\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u4e2d\u5b58\u5728\u90e8\u5206\u6fc0\u6d3b\u548c\u8fb9\u754c\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\uff0c\u800c\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\u6613\u53d7\u80cc\u666f\u566a\u58f0\u5e72\u6270\u3002", "method": "\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u6269\u6563\u6a21\u578b\u7279\u5f81\uff0c\u901a\u8fc7\u68af\u5ea6\u56fe\u548cCAM\u8bc6\u522b\u524d\u666f\u4e0e\u80cc\u666f\u50cf\u7d20\uff0c\u8bad\u7ec3\u50cf\u7d20\u89e3\u7801\u5668\u751f\u6210\u4f4e\u7ef4\u5d4c\u5165\u7a7a\u95f4\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5171\u533b\u5b66\u6570\u636e\u96c6\u7684\u56db\u4e2a\u5206\u5272\u4efb\u52a1\u4e2d\uff0cCLDF\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "CLDF\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u6269\u6563\u7279\u5f81\u6709\u6548\u89e3\u51b3\u4e86\u566a\u58f0\u548c\u8fb9\u754c\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2506.23461", "pdf": "https://arxiv.org/pdf/2506.23461", "abs": "https://arxiv.org/abs/2506.23461", "authors": ["Yun Xing", "Qing Guo", "Xiaoguang Li", "Yihao Huang", "Xiaofeng Cao", "Di Lin", "Ivor Tsang", "Lei Ma"], "title": "Time-variant Image Inpainting via Interactive Distribution Transition Estimation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this work, we focus on a novel and practical task, i.e., Time-vAriant iMage inPainting (TAMP). The aim of TAMP is to restore a damaged target image by leveraging the complementary information from a reference image, where both images captured the same scene but with a significant time gap in between, i.e., time-variant images. Different from conventional reference-guided image inpainting, the reference image under TAMP setup presents significant content distinction to the target image and potentially also suffers from damages. Such an application frequently happens in our daily lives to restore a damaged image by referring to another reference image, where there is no guarantee of the reference image's source and quality. In particular, our study finds that even state-of-the-art (SOTA) reference-guided image inpainting methods fail to achieve plausible results due to the chaotic image complementation. To address such an ill-posed problem, we propose a novel Interactive Distribution Transition Estimation (InDiTE) module which interactively complements the time-variant images with adaptive semantics thus facilitate the restoration of damaged regions. To further boost the performance, we propose our TAMP solution, namely Interactive Distribution Transition Estimation-driven Diffusion (InDiTE-Diff), which integrates InDiTE with SOTA diffusion model and conducts latent cross-reference during sampling. Moreover, considering the lack of benchmarks for TAMP task, we newly assembled a dataset, i.e., TAMP-Street, based on existing image and mask datasets. We conduct experiments on the TAMP-Street datasets under two different time-variant image inpainting settings, which show our method consistently outperform SOTA reference-guided image inpainting methods for solving TAMP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65f6\u95f4\u53d8\u5f02\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\uff08TAMP\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u4ea4\u4e92\u5f0f\u5206\u5e03\u8f6c\u79fb\u4f30\u8ba1\u6a21\u5757\uff08InDiTE\uff09\u548c\u6269\u6563\u6a21\u578b\uff08InDiTE-Diff\uff09\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u53d8\u5f02\u56fe\u50cf\u4fee\u590d\u4e2d\u7684\u590d\u6742\u4e92\u8865\u95ee\u9898\uff0c\u5e76\u5728\u65b0\u6570\u636e\u96c6TAMP-Street\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edf\u53c2\u8003\u5f15\u5bfc\u7684\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u5728\u5904\u7406\u65f6\u95f4\u53d8\u5f02\u56fe\u50cf\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u53c2\u8003\u56fe\u50cf\u4e0e\u76ee\u6807\u56fe\u50cf\u5185\u5bb9\u5dee\u5f02\u5927\u4e14\u53ef\u80fd\u53d7\u635f\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5b9e\u9645\u95ee\u9898\u3002", "method": "\u63d0\u51faInDiTE\u6a21\u5757\u4ea4\u4e92\u8865\u5145\u65f6\u95f4\u53d8\u5f02\u56fe\u50cf\u7684\u8bed\u4e49\uff0c\u5e76\u7ed3\u5408\u6269\u6563\u6a21\u578b\uff08InDiTE-Diff\uff09\u8fdb\u884c\u6f5c\u5728\u4ea4\u53c9\u53c2\u8003\u3002", "result": "\u5728TAMP-Street\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u53c2\u8003\u5f15\u5bfc\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u3002", "conclusion": "InDiTE-Diff\u4e3a\u65f6\u95f4\u53d8\u5f02\u56fe\u50cf\u4fee\u590d\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5c55\u793a\u4e86\u6f5c\u529b\u3002"}}
{"id": "2506.23479", "pdf": "https://arxiv.org/pdf/2506.23479", "abs": "https://arxiv.org/abs/2506.23479", "authors": ["Zhaojie Zeng", "Yuesong Wang", "Chao Yang", "Tao Guan", "Lili Ju"], "title": "Instant GaussianImage: A Generalizable and Self-Adaptive Image Representation via 2D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Implicit Neural Representation (INR) has demonstrated remarkable advances in the field of image representation but demands substantial GPU resources. GaussianImage recently pioneered the use of Gaussian Splatting to mitigate this cost, however, the slow training process limits its practicality, and the fixed number of Gaussians per image limits its adaptability to varying information entropy. To address these issues, we propose in this paper a generalizable and self-adaptive image representation framework based on 2D Gaussian Splatting. Our method employs a network to quickly generate a coarse Gaussian representation, followed by minimal fine-tuning steps, achieving comparable rendering quality of GaussianImage while significantly reducing training time. Moreover, our approach dynamically adjusts the number of Gaussian points based on image complexity to further enhance flexibility and efficiency in practice. Experiments on DIV2K and Kodak datasets show that our method matches or exceeds GaussianImage's rendering performance with far fewer iterations and shorter training times. Specifically, our method reduces the training time by up to one order of magnitude while achieving superior rendering performance with the same number of Gaussians.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e2D\u9ad8\u65af\u6cfc\u6e85\u7684\u81ea\u9002\u5e94\u56fe\u50cf\u8868\u793a\u6846\u67b6\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u5e76\u52a8\u6001\u8c03\u6574\u9ad8\u65af\u70b9\u6570\u91cf\u3002", "motivation": "\u89e3\u51b3\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u8bad\u7ec3\u6162\u4e14\u9ad8\u65af\u70b9\u6570\u91cf\u56fa\u5b9a\u7684\u95ee\u9898\uff0c\u63d0\u5347\u7075\u6d3b\u6027\u548c\u6548\u7387\u3002", "method": "\u4f7f\u7528\u7f51\u7edc\u5feb\u901f\u751f\u6210\u7c97\u7565\u9ad8\u65af\u8868\u793a\uff0c\u518d\u901a\u8fc7\u5c11\u91cf\u5fae\u8c03\u6b65\u9aa4\u4f18\u5316\uff0c\u52a8\u6001\u8c03\u6574\u9ad8\u65af\u70b9\u6570\u91cf\u3002", "result": "\u5728DIV2K\u548cKodak\u6570\u636e\u96c6\u4e0a\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u6e32\u67d3\u6027\u80fd\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u7075\u6d3b\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2506.23482", "pdf": "https://arxiv.org/pdf/2506.23482", "abs": "https://arxiv.org/abs/2506.23482", "authors": ["Jun Huang", "Ting Liu", "Yihang Wu", "Xiaochao Qu", "Luoqi Liu", "Xiaolin Hu"], "title": "MTADiffusion: Mask Text Alignment Diffusion Model for Object Inpainting", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Advancements in generative models have enabled image inpainting models to generate content within specific regions of an image based on provided prompts and masks. However, existing inpainting methods often suffer from problems such as semantic misalignment, structural distortion, and style inconsistency. In this work, we present MTADiffusion, a Mask-Text Alignment diffusion model designed for object inpainting. To enhance the semantic capabilities of the inpainting model, we introduce MTAPipeline, an automatic solution for annotating masks with detailed descriptions. Based on the MTAPipeline, we construct a new MTADataset comprising 5 million images and 25 million mask-text pairs. Furthermore, we propose a multi-task training strategy that integrates both inpainting and edge prediction tasks to improve structural stability. To promote style consistency, we present a novel inpainting style-consistency loss using a pre-trained VGG network and the Gram matrix. Comprehensive evaluations on BrushBench and EditBench demonstrate that MTADiffusion achieves state-of-the-art performance compared to other methods.", "AI": {"tldr": "MTADiffusion\u662f\u4e00\u79cd\u7528\u4e8e\u5bf9\u8c61\u4fee\u590d\u7684Mask-Text Alignment\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7MTAPipeline\u81ea\u52a8\u6807\u6ce8\u63a9\u7801\u548c\u8be6\u7ec6\u63cf\u8ff0\uff0c\u6784\u5efa\u4e86\u5305\u542b500\u4e07\u56fe\u50cf\u548c2500\u4e07\u63a9\u7801-\u6587\u672c\u5bf9\u7684\u6570\u636e\u96c6\u3002\u91c7\u7528\u591a\u4efb\u52a1\u8bad\u7ec3\u7b56\u7565\u548c\u98ce\u683c\u4e00\u81f4\u6027\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4fee\u590d\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u5b58\u5728\u8bed\u4e49\u4e0d\u5bf9\u9f50\u3001\u7ed3\u6784\u626d\u66f2\u548c\u98ce\u683c\u4e0d\u4e00\u81f4\u7b49\u95ee\u9898\uff0cMTADiffusion\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faMTAPipeline\u81ea\u52a8\u6807\u6ce8\u63a9\u7801\u548c\u63cf\u8ff0\uff0c\u6784\u5efaMTADataset\uff1b\u91c7\u7528\u591a\u4efb\u52a1\u8bad\u7ec3\u7b56\u7565\uff08\u4fee\u590d\u548c\u8fb9\u7f18\u9884\u6d4b\uff09\uff1b\u5f15\u5165\u98ce\u683c\u4e00\u81f4\u6027\u635f\u5931\uff08\u57fa\u4e8eVGG\u7f51\u7edc\u548cGram\u77e9\u9635\uff09\u3002", "result": "\u5728BrushBench\u548cEditBench\u4e0a\uff0cMTADiffusion\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "MTADiffusion\u901a\u8fc7\u6539\u8fdb\u8bed\u4e49\u5bf9\u9f50\u3001\u7ed3\u6784\u7a33\u5b9a\u6027\u548c\u98ce\u683c\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8c61\u4fee\u590d\u7684\u6548\u679c\u3002"}}
{"id": "2506.23513", "pdf": "https://arxiv.org/pdf/2506.23513", "abs": "https://arxiv.org/abs/2506.23513", "authors": ["Zixun Fang", "Kai Zhu", "Zhiheng Liu", "Yu Liu", "Wei Zhai", "Yang Cao", "Zheng-Jun Zha"], "title": "ViewPoint: Panoramic Video Generation with Pretrained Diffusion Models", "categories": ["cs.CV"], "comment": "https://becauseimbatman0.github.io/ViewPoint", "summary": "Panoramic video generation aims to synthesize 360-degree immersive videos, holding significant importance in the fields of VR, world models, and spatial intelligence. Existing works fail to synthesize high-quality panoramic videos due to the inherent modality gap between panoramic data and perspective data, which constitutes the majority of the training data for modern diffusion models. In this paper, we propose a novel framework utilizing pretrained perspective video models for generating panoramic videos. Specifically, we design a novel panorama representation named ViewPoint map, which possesses global spatial continuity and fine-grained visual details simultaneously. With our proposed Pano-Perspective attention mechanism, the model benefits from pretrained perspective priors and captures the panoramic spatial correlations of the ViewPoint map effectively. Extensive experiments demonstrate that our method can synthesize highly dynamic and spatially consistent panoramic videos, achieving state-of-the-art performance and surpassing previous methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89d2\u89c6\u9891\u6a21\u578b\u751f\u6210\u5168\u666f\u89c6\u9891\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7ViewPoint map\u548cPano-Perspective\u6ce8\u610f\u529b\u673a\u5236\u89e3\u51b3\u6a21\u6001\u5dee\u8ddd\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u5168\u666f\u89c6\u9891\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56e0\u5168\u666f\u6570\u636e\u4e0e\u89c6\u89d2\u6570\u636e\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u8ddd\uff0c\u96be\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u5168\u666f\u89c6\u9891\u3002", "method": "\u8bbe\u8ba1\u4e86\u5177\u6709\u5168\u5c40\u7a7a\u95f4\u8fde\u7eed\u6027\u548c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7ec6\u8282\u7684ViewPoint map\uff0c\u5e76\u5f15\u5165Pano-Perspective\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89d2\u5148\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5408\u6210\u52a8\u6001\u4e14\u7a7a\u95f4\u4e00\u81f4\u7684\u5168\u666f\u89c6\u9891\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u5168\u666f\u89c6\u9891\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3aVR\u7b49\u9886\u57df\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002"}}
{"id": "2506.23518", "pdf": "https://arxiv.org/pdf/2506.23518", "abs": "https://arxiv.org/abs/2506.23518", "authors": ["Jiwoo Park", "Tae Eun Choi", "Youngjun Jun", "Seong Jae Hwang"], "title": "WAVE: Warp-Based View Guidance for Consistent Novel View Synthesis Using a Single Image", "categories": ["cs.CV"], "comment": null, "summary": "Generating high-quality novel views of a scene from a single image requires maintaining structural coherence across different views, referred to as view consistency. While diffusion models have driven advancements in novel view synthesis, they still struggle to preserve spatial continuity across views. Diffusion models have been combined with 3D models to address the issue, but such approaches lack efficiency due to their complex multi-step pipelines. This paper proposes a novel view-consistent image generation method which utilizes diffusion models without additional modules. Our key idea is to enhance diffusion models with a training-free method that enables adaptive attention manipulation and noise reinitialization by leveraging view-guided warping to ensure view consistency. Through our comprehensive metric framework suitable for novel-view datasets, we show that our method improves view consistency across various diffusion models, demonstrating its broader applicability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u6a21\u5757\u7684\u89c6\u56fe\u4e00\u81f4\u6027\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u64cd\u7eb5\u548c\u566a\u58f0\u91cd\u65b0\u521d\u59cb\u5316\u63d0\u5347\u6269\u6563\u6a21\u578b\u7684\u89c6\u56fe\u4e00\u81f4\u6027\u3002", "motivation": "\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u591a\u89c6\u89d2\u56fe\u50cf\u65f6\u96be\u4ee5\u4fdd\u6301\u7a7a\u95f4\u8fde\u7eed\u6027\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u590d\u6742\u591a\u6b65\u6d41\u7a0b\u7684\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u5229\u7528\u89c6\u56fe\u5f15\u5bfc\u7684\u626d\u66f2\u6280\u672f\uff0c\u5b9e\u73b0\u8bad\u7ec3\u81ea\u7531\u7684\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u64cd\u7eb5\u548c\u566a\u58f0\u91cd\u65b0\u521d\u59cb\u5316\u3002", "result": "\u901a\u8fc7\u9002\u7528\u4e8e\u65b0\u89c6\u89d2\u6570\u636e\u96c6\u7684\u7efc\u5408\u6307\u6807\u6846\u67b6\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u4e0d\u540c\u6269\u6563\u6a21\u578b\u4e2d\u7684\u89c6\u56fe\u4e00\u81f4\u6027\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u6269\u6563\u6a21\u578b\u5728\u591a\u89c6\u89d2\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2506.23538", "pdf": "https://arxiv.org/pdf/2506.23538", "abs": "https://arxiv.org/abs/2506.23538", "authors": ["Yuhao Huang", "Yueyue Xu", "Haoran Dou", "Jiaxiao Deng", "Xin Yang", "Hongyu Zheng", "Dong Ni"], "title": "Uncertainty-aware Diffusion and Reinforcement Learning for Joint Plane Localization and Anomaly Diagnosis in 3D Ultrasound", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted by MICCAI 2025;10 pages, 3 figures", "summary": "Congenital uterine anomalies (CUAs) can lead to infertility, miscarriage, preterm birth, and an increased risk of pregnancy complications. Compared to traditional 2D ultrasound (US), 3D US can reconstruct the coronal plane, providing a clear visualization of the uterine morphology for assessing CUAs accurately. In this paper, we propose an intelligent system for simultaneous automated plane localization and CUA diagnosis. Our highlights are: 1) we develop a denoising diffusion model with local (plane) and global (volume/text) guidance, using an adaptive weighting strategy to optimize attention allocation to different conditions; 2) we introduce a reinforcement learning-based framework with unsupervised rewards to extract the key slice summary from redundant sequences, fully integrating information across multiple planes to reduce learning difficulty; 3) we provide text-driven uncertainty modeling for coarse prediction, and leverage it to adjust the classification probability for overall performance improvement. Extensive experiments on a large 3D uterine US dataset show the efficacy of our method, in terms of plane localization and CUA diagnosis. Code is available at https://github.com/yuhoo0302/CUA-US.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u667a\u80fd\u7cfb\u7edf\uff0c\u901a\u8fc73D\u8d85\u58f0\u81ea\u52a8\u5b9a\u4f4d\u5e73\u9762\u5e76\u8bca\u65ad\u5148\u5929\u6027\u5b50\u5bab\u5f02\u5e38\uff0c\u7ed3\u5408\u53bb\u566a\u6269\u6563\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u8bca\u65ad\u6548\u679c\u3002", "motivation": "\u5148\u5929\u6027\u5b50\u5bab\u5f02\u5e38\uff08CUAs\uff09\u53ef\u80fd\u5bfc\u81f4\u4e0d\u5b55\u3001\u6d41\u4ea7\u7b49\u95ee\u9898\uff0c\u4f20\u7edf2D\u8d85\u58f0\u96be\u4ee5\u51c6\u786e\u8bc4\u4f30\uff0c3D\u8d85\u58f0\u80fd\u63d0\u4f9b\u66f4\u6e05\u6670\u7684\u5b50\u5bab\u5f62\u6001\u53ef\u89c6\u5316\u3002", "method": "1) \u4f7f\u7528\u53bb\u566a\u6269\u6563\u6a21\u578b\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u6307\u5bfc\uff1b2) \u5f15\u5165\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u63d0\u53d6\u5173\u952e\u5207\u7247\uff1b3) \u63d0\u4f9b\u6587\u672c\u9a71\u52a8\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u4f18\u5316\u5206\u7c7b\u3002", "result": "\u5728\u5927\u89c4\u6a213D\u5b50\u5bab\u8d85\u58f0\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u63d0\u5347\u4e86\u5e73\u9762\u5b9a\u4f4d\u548cCUA\u8bca\u65ad\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u667a\u80fd\u7cfb\u7edf\u57283D\u8d85\u58f0\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5e73\u9762\u5b9a\u4f4d\u548cCUA\u8bca\u65ad\uff0c\u4e3a\u4e34\u5e8a\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2506.23542", "pdf": "https://arxiv.org/pdf/2506.23542", "abs": "https://arxiv.org/abs/2506.23542", "authors": ["Weida Wang", "Changyong He", "Jin Zeng", "Di Qiu"], "title": "Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric Attention", "categories": ["cs.CV"], "comment": "This paper has been accepted for publication at the International   Conference on Computer Vision (ICCV) 2025", "summary": "Depth images captured by Time-of-Flight (ToF) sensors are prone to noise, requiring denoising for reliable downstream applications. Previous works either focus on single-frame processing, or perform multi-frame processing without considering depth variations at corresponding pixels across frames, leading to undesirable temporal inconsistency and spatial ambiguity. In this paper, we propose a novel ToF depth denoising network leveraging motion-invariant graph fusion to simultaneously enhance temporal stability and spatial sharpness. Specifically, despite depth shifts across frames, graph structures exhibit temporal self-similarity, enabling cross-frame geometric attention for graph fusion. Then, by incorporating an image smoothness prior on the fused graph and data fidelity term derived from ToF noise distribution, we formulate a maximum a posterior problem for ToF denoising. Finally, the solution is unrolled into iterative filters whose weights are adaptively learned from the graph-informed geometric attention, producing a high-performance yet interpretable network. Experimental results demonstrate that the proposed scheme achieves state-of-the-art performance in terms of accuracy and consistency on synthetic DVToF dataset and exhibits robust generalization on the real Kinectv2 dataset. Source code will be released at \\href{https://github.com/davidweidawang/GIGA-ToF}{https://github.com/davidweidawang/GIGA-ToF}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fd0\u52a8\u4e0d\u53d8\u56fe\u878d\u5408\u7684ToF\u6df1\u5ea6\u53bb\u566a\u7f51\u7edc\uff0c\u901a\u8fc7\u8de8\u5e27\u51e0\u4f55\u6ce8\u610f\u529b\u589e\u5f3a\u65f6\u95f4\u7a33\u5b9a\u6027\u548c\u7a7a\u95f4\u6e05\u6670\u5ea6\u3002", "motivation": "ToF\u4f20\u611f\u5668\u6355\u83b7\u7684\u6df1\u5ea6\u56fe\u50cf\u5b58\u5728\u566a\u58f0\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u8003\u8651\u8de8\u5e27\u6df1\u5ea6\u53d8\u5316\uff0c\u5bfc\u81f4\u65f6\u95f4\u4e0d\u4e00\u81f4\u548c\u7a7a\u95f4\u6a21\u7cca\u3002", "method": "\u5229\u7528\u56fe\u7684\u65f6\u5e8f\u81ea\u76f8\u4f3c\u6027\u8fdb\u884c\u8de8\u5e27\u51e0\u4f55\u6ce8\u610f\u529b\u878d\u5408\uff0c\u7ed3\u5408\u56fe\u50cf\u5e73\u6ed1\u5148\u9a8c\u548cToF\u566a\u58f0\u5206\u5e03\uff0c\u6784\u5efa\u6700\u5927\u540e\u9a8c\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u6ee4\u6ce2\u5668\u5b9e\u73b0\u3002", "result": "\u5728\u5408\u6210DVToF\u6570\u636e\u96c6\u548c\u771f\u5b9eKinectv2\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u51fa\u6700\u4f18\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728ToF\u6df1\u5ea6\u53bb\u566a\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u4e14\u53ef\u89e3\u91ca\u7684\u7f51\u7edc\uff0c\u5177\u6709\u9c81\u68d2\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.23543", "pdf": "https://arxiv.org/pdf/2506.23543", "abs": "https://arxiv.org/abs/2506.23543", "authors": ["Hui Li", "Baoyou Chen", "Liwei Zhang", "Jiaye Li", "Jingdong Wang", "Siyu Zhu"], "title": "Pyramidal Patchification Flow for Visual Generation", "categories": ["cs.CV"], "comment": "10 pages, 9figures", "summary": "Diffusion transformers (DiTs) adopt Patchify, mapping patch representations to token representations through linear projections, to adjust the number of tokens input to DiT blocks and thus the computation cost. Instead of a single patch size for all the timesteps, we introduce a Pyramidal Patchification Flow (PPFlow) approach: Large patch sizes are used for high noise timesteps and small patch sizes for low noise timesteps; Linear projections are learned for each patch size; and Unpatchify is accordingly modified. Unlike Pyramidal Flow, our approach operates over full latent representations other than pyramid representations, and adopts the normal denoising process without requiring the renoising trick. We demonstrate the effectiveness of our approach through two training manners. Training from scratch achieves a $1.6\\times$ ($2.0\\times$) inference speed over SiT-B/2 for 2-level (3-level) pyramid patchification with slightly lower training FLOPs and similar image generation performance. Training from pretrained normal DiTs achieves even better performance with small training time. The code and checkpoint are at https://github.com/fudan-generative-vision/PPFlow.", "AI": {"tldr": "PPFlow\u901a\u8fc7\u52a8\u6001\u8c03\u6574patch\u5927\u5c0f\u548c\u7ebf\u6027\u6295\u5f71\uff0c\u4f18\u5316\u4e86Diffusion Transformers\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5bf9\u6240\u6709\u65f6\u95f4\u6b65\u4f7f\u7528\u56fa\u5b9apatch\u5927\u5c0f\uff0c\u9650\u5236\u4e86\u8ba1\u7b97\u6548\u7387\u3002PPFlow\u65e8\u5728\u901a\u8fc7\u52a8\u6001patch\u5927\u5c0f\u4f18\u5316\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u63d0\u51faPyramidal Patchification Flow\uff08PPFlow\uff09\uff0c\u6839\u636e\u566a\u58f0\u6c34\u5e73\u52a8\u6001\u8c03\u6574patch\u5927\u5c0f\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u5927\u5c0f\u5b66\u4e60\u7ebf\u6027\u6295\u5f71\u3002", "result": "PPFlow\u5728\u63a8\u7406\u901f\u5ea6\u4e0a\u6bd4SiT-B/2\u5feb1.6\u500d\uff082\u7ea7\uff09\u62162.0\u500d\uff083\u7ea7\uff09\uff0c\u8bad\u7ec3FLOPs\u7565\u4f4e\u4e14\u751f\u6210\u6027\u80fd\u76f8\u8fd1\u3002", "conclusion": "PPFlow\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u4ece\u5934\u8bad\u7ec3\u6216\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5fae\u8c03\u3002"}}
{"id": "2506.23547", "pdf": "https://arxiv.org/pdf/2506.23547", "abs": "https://arxiv.org/abs/2506.23547", "authors": ["Jiwon Kim", "Soohyun Hwang", "Dong-O Kim", "Changsu Han", "Min Kyu Park", "Chang-Su Kim"], "title": "Oneta: Multi-Style Image Enhancement Using Eigentransformation Functions", "categories": ["cs.CV"], "comment": null, "summary": "The first algorithm, called Oneta, for a novel task of multi-style image enhancement is proposed in this work. Oneta uses two point operators sequentially: intensity enhancement with a transformation function (TF) and color correction with a color correction matrix (CCM). This two-step enhancement model, though simple, achieves a high performance upper bound. Also, we introduce eigentransformation function (eigenTF) to represent TF compactly. The Oneta network comprises Y-Net and C-Net to predict eigenTF and CCM parameters, respectively. To support $K$ styles, Oneta employs $K$ learnable tokens. During training, each style token is learned using image pairs from the corresponding dataset. In testing, Oneta selects one of the $K$ style tokens to enhance an image accordingly. Extensive experiments show that the single Oneta network can effectively undertake six enhancement tasks -- retouching, image signal processing, low-light image enhancement, dehazing, underwater image enhancement, and white balancing -- across 30 datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOneta\u7684\u591a\u98ce\u683c\u56fe\u50cf\u589e\u5f3a\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e24\u6b65\u64cd\u4f5c\uff08\u5f3a\u5ea6\u589e\u5f3a\u548c\u989c\u8272\u6821\u6b63\uff09\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u5e76\u652f\u6301\u591a\u79cd\u98ce\u683c\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u591a\u98ce\u683c\u56fe\u50cf\u589e\u5f3a\u4efb\u52a1\uff0c\u901a\u8fc7\u7b80\u5355\u4f46\u9ad8\u6548\u7684\u4e24\u6b65\u6a21\u578b\u5b9e\u73b0\u5e7f\u6cdb\u7684\u5e94\u7528\u573a\u666f\u3002", "method": "\u4f7f\u7528Y-Net\u548cC-Net\u5206\u522b\u9884\u6d4beigenTF\u548cCCM\u53c2\u6570\uff0c\u901a\u8fc7K\u4e2a\u53ef\u5b66\u4e60\u4ee4\u724c\u652f\u6301\u591a\u79cd\u98ce\u683c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOneta\u80fd\u6709\u6548\u5904\u7406\u516d\u79cd\u589e\u5f3a\u4efb\u52a1\uff0c\u8986\u76d630\u4e2a\u6570\u636e\u96c6\u3002", "conclusion": "Oneta\u901a\u8fc7\u7b80\u6d01\u7684\u4e24\u6b65\u6a21\u578b\u5b9e\u73b0\u4e86\u591a\u98ce\u683c\u56fe\u50cf\u589e\u5f3a\u7684\u9ad8\u6027\u80fd\u3002"}}
{"id": "2506.23552", "pdf": "https://arxiv.org/pdf/2506.23552", "abs": "https://arxiv.org/abs/2506.23552", "authors": ["Mingi Kwon", "Joonghyuk Shin", "Jaeseok Jung", "Jaesik Park", "Youngjung Uh"], "title": "JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "project page: https://joonghyuk.com/jamflow-web Under review.   Preprint published on arXiv", "summary": "The intrinsic link between facial motion and speech is often overlooked in generative modeling, where talking head synthesis and text-to-speech (TTS) are typically addressed as separate tasks. This paper introduces JAM-Flow, a unified framework to simultaneously synthesize and condition on both facial motion and speech. Our approach leverages flow matching and a novel Multi-Modal Diffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT and Audio-DiT modules. These are coupled via selective joint attention layers and incorporate key architectural choices, such as temporally aligned positional embeddings and localized joint attention masking, to enable effective cross-modal interaction while preserving modality-specific strengths. Trained with an inpainting-style objective, JAM-Flow supports a wide array of conditioning inputs-including text, reference audio, and reference motion-facilitating tasks such as synchronized talking head generation from text, audio-driven animation, and much more, within a single, coherent model. JAM-Flow significantly advances multi-modal generative modeling by providing a practical solution for holistic audio-visual synthesis. project page: https://joonghyuk.com/jamflow-web", "AI": {"tldr": "JAM-Flow\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u6d41\u5339\u914d\u548c\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\uff08MM-DiT\uff09\u540c\u65f6\u5408\u6210\u9762\u90e8\u8fd0\u52a8\u548c\u8bed\u97f3\uff0c\u652f\u6301\u591a\u79cd\u8f93\u5165\u6761\u4ef6\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u5e38\u5c06\u9762\u90e8\u8fd0\u52a8\u5408\u6210\u548c\u8bed\u97f3\u5408\u6210\u89c6\u4e3a\u72ec\u7acb\u4efb\u52a1\uff0c\u5ffd\u7565\u4e86\u4e8c\u8005\u7684\u5185\u5728\u8054\u7cfb\u3002JAM-Flow\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6d41\u5339\u914d\u548cMM-DiT\u67b6\u6784\uff0c\u7ed3\u5408Motion-DiT\u548cAudio-DiT\u6a21\u5757\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u8054\u5408\u6ce8\u610f\u529b\u5c42\u5b9e\u73b0\u8de8\u6a21\u6001\u4ea4\u4e92\u3002", "result": "JAM-Flow\u652f\u6301\u591a\u79cd\u8f93\u5165\u6761\u4ef6\uff08\u5982\u6587\u672c\u3001\u53c2\u8003\u97f3\u9891\u548c\u8fd0\u52a8\uff09\uff0c\u5e76\u5728\u5355\u4e00\u6a21\u578b\u4e2d\u5b9e\u73b0\u540c\u6b65\u4efb\u52a1\u3002", "conclusion": "JAM-Flow\u4e3a\u591a\u6a21\u6001\u751f\u6210\u5efa\u6a21\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63a8\u8fdb\u4e86\u97f3\u9891-\u89c6\u89c9\u5408\u6210\u7684\u6574\u4f53\u6027\u3002"}}
{"id": "2506.23566", "pdf": "https://arxiv.org/pdf/2506.23566", "abs": "https://arxiv.org/abs/2506.23566", "authors": ["Luigi Sigillo", "Renato Giamba", "Danilo Comminiello"], "title": "Metadata, Wavelet, and Time Aware Diffusion Models for Satellite Image Super Resolution", "categories": ["cs.CV", "cs.LG"], "comment": "ICLR 2025 Workshop on Machine Learning for Remote Sensing (ML4RS)", "summary": "The acquisition of high-resolution satellite imagery is often constrained by the spatial and temporal limitations of satellite sensors, as well as the high costs associated with frequent observations. These challenges hinder applications such as environmental monitoring, disaster response, and agricultural management, which require fine-grained and high-resolution data. In this paper, we propose MWT-Diff, an innovative framework for satellite image super-resolution (SR) that combines latent diffusion models with wavelet transforms to address these challenges. At the core of the framework is a novel metadata-, wavelet-, and time-aware encoder (MWT-Encoder), which generates embeddings that capture metadata attributes, multi-scale frequency information, and temporal relationships. The embedded feature representations steer the hierarchical diffusion dynamics, through which the model progressively reconstructs high-resolution satellite imagery from low-resolution inputs. This process preserves critical spatial characteristics including textural patterns, boundary discontinuities, and high-frequency spectral components essential for detailed remote sensing analysis. The comparative analysis of MWT-Diff across multiple datasets demonstrated favorable performance compared to recent approaches, as measured by standard perceptual quality metrics including FID and LPIPS.", "AI": {"tldr": "MWT-Diff\u662f\u4e00\u79cd\u7ed3\u5408\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u5c0f\u6ce2\u53d8\u6362\u7684\u536b\u661f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u901a\u8fc7MWT-Encoder\u751f\u6210\u5d4c\u5165\u7279\u5f81\uff0c\u9010\u6b65\u91cd\u5efa\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\u83b7\u53d6\u53d7\u9650\u4e8e\u4f20\u611f\u5668\u65f6\u7a7a\u9650\u5236\u548c\u9ad8\u6210\u672c\uff0c\u5f71\u54cd\u73af\u5883\u76d1\u6d4b\u3001\u707e\u5bb3\u54cd\u5e94\u7b49\u5e94\u7528\u3002", "method": "\u63d0\u51faMWT-Diff\u6846\u67b6\uff0c\u7ed3\u5408\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u5c0f\u6ce2\u53d8\u6362\uff0c\u5229\u7528MWT-Encoder\u751f\u6210\u5d4c\u5165\u7279\u5f81\u6307\u5bfc\u6269\u6563\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u611f\u77e5\u8d28\u91cf\u6307\u6807\uff08FID\u548cLPIPS\uff09\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "MWT-Diff\u80fd\u6709\u6548\u89e3\u51b3\u536b\u661f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u95ee\u9898\uff0c\u4fdd\u7559\u5173\u952e\u7a7a\u95f4\u7279\u5f81\uff0c\u9002\u7528\u4e8e\u9065\u611f\u5206\u6790\u3002"}}
{"id": "2506.23606", "pdf": "https://arxiv.org/pdf/2506.23606", "abs": "https://arxiv.org/abs/2506.23606", "authors": ["Zhengkang Xiang", "Zizhao Li", "Amir Khodabandeh", "Kourosh Khoshelham"], "title": "SG-LDM: Semantic-Guided LiDAR Generation via Latent-Aligned Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Lidar point cloud synthesis based on generative models offers a promising solution to augment deep learning pipelines, particularly when real-world data is scarce or lacks diversity. By enabling flexible object manipulation, this synthesis approach can significantly enrich training datasets and enhance discriminative models. However, existing methods focus on unconditional lidar point cloud generation, overlooking their potential for real-world applications. In this paper, we propose SG-LDM, a Semantic-Guided Lidar Diffusion Model that employs latent alignment to enable robust semantic-to-lidar synthesis. By directly operating in the native lidar space and leveraging explicit semantic conditioning, SG-LDM achieves state-of-the-art performance in generating high-fidelity lidar point clouds guided by semantic labels. Moreover, we propose the first diffusion-based lidar translation framework based on SG-LDM, which enables cross-domain translation as a domain adaptation strategy to enhance downstream perception performance. Systematic experiments demonstrate that SG-LDM significantly outperforms existing lidar diffusion models and the proposed lidar translation framework further improves data augmentation performance in the downstream lidar segmentation task.", "AI": {"tldr": "SG-LDM\u662f\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u5f15\u5bfc\u7684\u6fc0\u5149\u96f7\u8fbe\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u6f5c\u5728\u5bf9\u9f50\u5b9e\u73b0\u8bed\u4e49\u5230\u6fc0\u5149\u96f7\u8fbe\u7684\u5408\u6210\uff0c\u5e76\u5728\u751f\u6210\u9ad8\u4fdd\u771f\u70b9\u4e91\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u8bed\u4e49\u5f15\u5bfc\u7684\u95ee\u9898\uff0c\u63d0\u5347\u6570\u636e\u589e\u5f3a\u548c\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faSG-LDM\u6a21\u578b\uff0c\u5229\u7528\u6f5c\u5728\u5bf9\u9f50\u548c\u663e\u5f0f\u8bed\u4e49\u6761\u4ef6\uff0c\u76f4\u63a5\u5728\u6fc0\u5149\u96f7\u8fbe\u7a7a\u95f4\u64cd\u4f5c\uff0c\u5e76\u5f00\u53d1\u6269\u6563\u5f0f\u6fc0\u5149\u96f7\u8fbe\u7ffb\u8bd1\u6846\u67b6\u3002", "result": "SG-LDM\u5728\u751f\u6210\u9ad8\u4fdd\u771f\u70b9\u4e91\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7ffb\u8bd1\u6846\u67b6\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6fc0\u5149\u96f7\u8fbe\u5206\u5272\u4efb\u52a1\u7684\u6570\u636e\u589e\u5f3a\u6548\u679c\u3002", "conclusion": "SG-LDM\u4e3a\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u751f\u6210\u548c\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2506.23611", "pdf": "https://arxiv.org/pdf/2506.23611", "abs": "https://arxiv.org/abs/2506.23611", "authors": ["Ziao Liu", "Zhenjia Li", "Yifeng Shi", "Xiangang Li"], "title": "AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via Structural Attention", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) is a powerful alternative to Neural Radiance Fields (NeRF), excelling in complex scene reconstruction and efficient rendering. However, it relies on high-quality point clouds from Structure-from-Motion (SfM), limiting its applicability. SfM also fails in texture-deficient or constrained-view scenarios, causing severe degradation in 3DGS reconstruction. To address this limitation, we propose AttentionGS, a novel framework that eliminates the dependency on high-quality initial point clouds by leveraging structural attention for direct 3D reconstruction from randomly initialization. In the early training stage, we introduce geometric attention to rapidly recover the global scene structure. As training progresses, we incorporate texture attention to refine fine-grained details and enhance rendering quality. Furthermore, we employ opacity-weighted gradients to guide Gaussian densification, leading to improved surface reconstruction. Extensive experiments on multiple benchmark datasets demonstrate that AttentionGS significantly outperforms state-of-the-art methods, particularly in scenarios where point cloud initialization is unreliable. Our approach paves the way for more robust and flexible 3D Gaussian Splatting in real-world applications.", "AI": {"tldr": "AttentionGS\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u6ce8\u610f\u529b\u76f4\u63a5\u4ece\u968f\u673a\u521d\u59cb\u5316\u8fdb\u884c3D\u91cd\u5efa\uff0c\u89e3\u51b3\u4e863D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u5bf9\u9ad8\u8d28\u91cf\u70b9\u4e91\u7684\u4f9d\u8d56\u95ee\u9898\u3002", "motivation": "3DGS\u4f9d\u8d56\u4e8e\u7ed3\u6784\u4ece\u8fd0\u52a8\uff08SfM\uff09\u751f\u6210\u7684\u9ad8\u8d28\u91cf\u70b9\u4e91\uff0c\u4f46\u5728\u7eb9\u7406\u7f3a\u5931\u6216\u89c6\u89d2\u53d7\u9650\u7684\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "AttentionGS\u7ed3\u5408\u51e0\u4f55\u6ce8\u610f\u529b\u548c\u7eb9\u7406\u6ce8\u610f\u529b\uff0c\u65e9\u671f\u6062\u590d\u5168\u5c40\u7ed3\u6784\uff0c\u540e\u671f\u7ec6\u5316\u7ec6\u8282\uff0c\u5e76\u4f7f\u7528\u4e0d\u900f\u660e\u5ea6\u52a0\u6743\u68af\u5ea6\u6307\u5bfc\u9ad8\u65af\u5bc6\u96c6\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAttentionGS\u5728\u70b9\u4e91\u521d\u59cb\u5316\u4e0d\u53ef\u9760\u7684\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a3DGS\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23618", "pdf": "https://arxiv.org/pdf/2506.23618", "abs": "https://arxiv.org/abs/2506.23618", "authors": ["Zhongdao Wang", "Guodongfang Zhao", "Jingjing Ren", "Bailan Feng", "Shifeng Zhang", "Wenbo Li"], "title": "TurboVSR: Fantastic Video Upscalers and Where to Find Them", "categories": ["cs.CV"], "comment": "ICCV, 2025", "summary": "Diffusion-based generative models have demonstrated exceptional promise in the video super-resolution (VSR) task, achieving a substantial advancement in detail generation relative to prior methods. However, these approaches face significant computational efficiency challenges. For instance, current techniques may require tens of minutes to super-resolve a mere 2-second, 1080p video. In this paper, we present TurboVSR, an ultra-efficient diffusion-based video super-resolution model. Our core design comprises three key aspects: (1) We employ an autoencoder with a high compression ratio of 32$\\times$32$\\times$8 to reduce the number of tokens. (2) Highly compressed latents pose substantial challenges for training. We introduce factorized conditioning to mitigate the learning complexity: we first learn to super-resolve the initial frame; subsequently, we condition the super-resolution of the remaining frames on the high-resolution initial frame and the low-resolution subsequent frames. (3) We convert the pre-trained diffusion model to a shortcut model to enable fewer sampling steps, further accelerating inference. As a result, TurboVSR performs on par with state-of-the-art VSR methods, while being 100+ times faster, taking only 7 seconds to process a 2-second long 1080p video. TurboVSR also supports image resolution by considering image as a one-frame video. Our efficient design makes SR beyond 1080p possible, results on 4K (3648$\\times$2048) image SR show surprising fine details.", "AI": {"tldr": "TurboVSR\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8d85\u9ad8\u6548\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u538b\u7f29\u6bd4\u81ea\u52a8\u7f16\u7801\u5668\u3001\u5206\u89e3\u6761\u4ef6\u548c\u5feb\u6377\u6a21\u578b\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u6027\u80fd\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u4f46\u901f\u5ea6\u5feb100\u500d\u4ee5\u4e0a\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5728\u7ec6\u8282\u751f\u6210\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\uff0c\u5904\u7406\u77ed\u89c6\u9891\u8017\u65f6\u8fc7\u957f\u3002", "method": "1. \u4f7f\u7528\u9ad8\u538b\u7f29\u6bd4\u81ea\u52a8\u7f16\u7801\u5668\u51cf\u5c11token\u6570\u91cf\uff1b2. \u5f15\u5165\u5206\u89e3\u6761\u4ef6\u964d\u4f4e\u8bad\u7ec3\u590d\u6742\u5ea6\uff1b3. \u5c06\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u8f6c\u6362\u4e3a\u5feb\u6377\u6a21\u578b\u4ee5\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\u3002", "result": "TurboVSR\u6027\u80fd\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u901f\u5ea6\u5feb100\u500d\u4ee5\u4e0a\uff0c\u652f\u63011080p\u4ee5\u4e0a\u5206\u8fa8\u7387\uff0c4K\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "TurboVSR\u901a\u8fc7\u9ad8\u6548\u8bbe\u8ba1\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u66f4\u9ad8\u5206\u8fa8\u7387\u7684\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2506.23630", "pdf": "https://arxiv.org/pdf/2506.23630", "abs": "https://arxiv.org/abs/2506.23630", "authors": ["Lorenzo Olearo", "Giorgio Longari", "Alessandro Raganato", "Rafael Pe\u00f1aloza", "Simone Melzi"], "title": "Blending Concepts with Text-to-Image Diffusion Models", "categories": ["cs.CV"], "comment": "Currently under review", "summary": "Diffusion models have dramatically advanced text-to-image generation in recent years, translating abstract concepts into high-fidelity images with remarkable ease. In this work, we examine whether they can also blend distinct concepts, ranging from concrete objects to intangible ideas, into coherent new visual entities under a zero-shot framework. Specifically, concept blending merges the key attributes of multiple concepts (expressed as textual prompts) into a single, novel image that captures the essence of each concept. We investigate four blending methods, each exploiting different aspects of the diffusion pipeline (e.g., prompt scheduling, embedding interpolation, or layer-wise conditioning). Through systematic experimentation across diverse concept categories, such as merging concrete concepts, synthesizing compound words, transferring artistic styles, and blending architectural landmarks, we show that modern diffusion models indeed exhibit creative blending capabilities without further training or fine-tuning. Our extensive user study, involving 100 participants, reveals that no single approach dominates in all scenarios: each blending technique excels under certain conditions, with factors like prompt ordering, conceptual distance, and random seed affecting the outcome. These findings highlight the remarkable compositional potential of diffusion models while exposing their sensitivity to seemingly minor input variations.", "AI": {"tldr": "\u6269\u6563\u6a21\u578b\u5728\u96f6\u6837\u672c\u6846\u67b6\u4e0b\u80fd\u591f\u5c06\u4e0d\u540c\u6982\u5ff5\uff08\u4ece\u5177\u4f53\u5bf9\u8c61\u5230\u62bd\u8c61\u60f3\u6cd5\uff09\u878d\u5408\u6210\u8fde\u8d2f\u7684\u65b0\u89c6\u89c9\u5b9e\u4f53\uff0c\u5c55\u793a\u4e86\u5176\u521b\u9020\u6027\u6df7\u5408\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u6269\u6563\u6a21\u578b\u662f\u5426\u80fd\u591f\u5728\u4e0d\u8fdb\u884c\u989d\u5916\u8bad\u7ec3\u6216\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u591a\u4e2a\u6982\u5ff5\u7684\u5173\u952e\u5c5e\u6027\u878d\u5408\u6210\u5355\u4e00\u65b0\u9896\u56fe\u50cf\u3002", "method": "\u7814\u7a76\u4e86\u56db\u79cd\u6df7\u5408\u65b9\u6cd5\uff0c\u5305\u62ec\u63d0\u793a\u8c03\u5ea6\u3001\u5d4c\u5165\u63d2\u503c\u548c\u5206\u5c42\u6761\u4ef6\u7b49\uff0c\u5229\u7528\u6269\u6563\u7ba1\u9053\u7684\u4e0d\u540c\u65b9\u9762\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u6269\u6563\u6a21\u578b\u5177\u6709\u521b\u9020\u6027\u6df7\u5408\u80fd\u529b\uff0c\u4f46\u4e0d\u540c\u65b9\u6cd5\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u8868\u73b0\u5404\u5f02\uff0c\u53d7\u63d0\u793a\u987a\u5e8f\u3001\u6982\u5ff5\u8ddd\u79bb\u548c\u968f\u673a\u79cd\u5b50\u7b49\u56e0\u7d20\u5f71\u54cd\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u5c55\u73b0\u51fa\u663e\u8457\u7684\u7ec4\u5408\u6f5c\u529b\uff0c\u4f46\u5bf9\u8f93\u5165\u53d8\u5316\u7684\u654f\u611f\u6027\u4e5f\u66b4\u9732\u65e0\u9057\u3002"}}
{"id": "2506.23641", "pdf": "https://arxiv.org/pdf/2506.23641", "abs": "https://arxiv.org/abs/2506.23641", "authors": ["Peng Huang", "Junhu Fu", "Bowen Guo", "Zeju Li", "Yuanyuan Wang", "Yi Guo"], "title": "VAP-Diffusion: Enriching Descriptions with MLLMs for Enhanced Medical Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "As the appearance of medical images is influenced by multiple underlying factors, generative models require rich attribute information beyond labels to produce realistic and diverse images. For instance, generating an image of skin lesion with specific patterns demands descriptions that go beyond diagnosis, such as shape, size, texture, and color. However, such detailed descriptions are not always accessible. To address this, we explore a framework, termed Visual Attribute Prompts (VAP)-Diffusion, to leverage external knowledge from pre-trained Multi-modal Large Language Models (MLLMs) to improve the quality and diversity of medical image generation. First, to derive descriptions from MLLMs without hallucination, we design a series of prompts following Chain-of-Thoughts for common medical imaging tasks, including dermatologic, colorectal, and chest X-ray images. Generated descriptions are utilized during training and stored across different categories. During testing, descriptions are randomly retrieved from the corresponding category for inference. Moreover, to make the generator robust to unseen combination of descriptions at the test time, we propose a Prototype Condition Mechanism that restricts test embeddings to be similar to those from training. Experiments on three common types of medical imaging across four datasets verify the effectiveness of VAP-Diffusion.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faVAP-Diffusion\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u5916\u90e8\u77e5\u8bc6\u751f\u6210\u66f4\u771f\u5b9e\u591a\u6837\u7684\u533b\u5b66\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u751f\u6210\u4e2d\u7f3a\u4e4f\u8be6\u7ec6\u63cf\u8ff0\u7684\u95ee\u9898\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u7684\u5916\u89c2\u53d7\u591a\u79cd\u56e0\u7d20\u5f71\u54cd\uff0c\u751f\u6210\u6a21\u578b\u9700\u8981\u8d85\u8d8a\u6807\u7b7e\u7684\u4e30\u5bcc\u5c5e\u6027\u4fe1\u606f\uff08\u5982\u5f62\u72b6\u3001\u5927\u5c0f\u3001\u7eb9\u7406\u7b49\uff09\u6765\u751f\u6210\u771f\u5b9e\u591a\u6837\u7684\u56fe\u50cf\uff0c\u4f46\u8fd9\u4e9b\u8be6\u7ec6\u63cf\u8ff0\u5f80\u5f80\u96be\u4ee5\u83b7\u53d6\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8eChain-of-Thoughts\u7684\u63d0\u793a\u8bcd\u4eceMLLMs\u4e2d\u63d0\u53d6\u63cf\u8ff0\uff0c\u8bad\u7ec3\u65f6\u5b58\u50a8\u63cf\u8ff0\u5e76\u6309\u7c7b\u522b\u968f\u673a\u68c0\u7d22\uff1b\u63d0\u51fa\u539f\u578b\u6761\u4ef6\u673a\u5236\uff0c\u786e\u4fdd\u6d4b\u8bd5\u65f6\u5bf9\u672a\u89c1\u63cf\u8ff0\u7ec4\u5408\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u4e09\u79cd\u5e38\u89c1\u533b\u5b66\u56fe\u50cf\u7c7b\u578b\uff08\u76ae\u80a4\u3001\u7ed3\u76f4\u80a0\u3001\u80f8\u90e8X\u5149\uff09\u7684\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86VAP-Diffusion\u7684\u6709\u6548\u6027\u3002", "conclusion": "VAP-Diffusion\u901a\u8fc7\u5229\u7528MLLMs\u7684\u5916\u90e8\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u751f\u6210\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002"}}
{"id": "2506.23676", "pdf": "https://arxiv.org/pdf/2506.23676", "abs": "https://arxiv.org/abs/2506.23676", "authors": ["Gaozheng Pei", "Ke Ma", "Dongpeng Zhang", "Chengzhi Sun", "Qianqian Xu", "Qingming Huang"], "title": "A Unified Framework for Stealthy Adversarial Generation via Latent Optimization and Transferability Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Due to their powerful image generation capabilities, diffusion-based adversarial example generation methods through image editing are rapidly gaining popularity. However, due to reliance on the discriminative capability of the diffusion model, these diffusion-based methods often struggle to generalize beyond conventional image classification tasks, such as in Deepfake detection. Moreover, traditional strategies for enhancing adversarial example transferability are challenging to adapt to these methods. To address these challenges, we propose a unified framework that seamlessly incorporates traditional transferability enhancement strategies into diffusion model-based adversarial example generation via image editing, enabling their application across a wider range of downstream tasks. Our method won first place in the \"1st Adversarial Attacks on Deepfake Detectors: A Challenge in the Era of AI-Generated Media\" competition at ACM MM25, which validates the effectiveness of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u4f20\u7edf\u5bf9\u6297\u6837\u672c\u8fc1\u79fb\u589e\u5f3a\u7b56\u7565\u878d\u5165\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff0c\u63d0\u5347\u5176\u5728\u66f4\u5e7f\u6cdb\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u5bf9\u6297\u6837\u672c\u5728\u4f20\u7edf\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728Deepfake\u68c0\u6d4b\u7b49\u4efb\u52a1\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4e14\u4f20\u7edf\u8fc1\u79fb\u589e\u5f3a\u7b56\u7565\u96be\u4ee5\u9002\u914d\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u4f20\u7edf\u8fc1\u79fb\u589e\u5f3a\u7b56\u7565\u4e0e\u6269\u6563\u6a21\u578b\u7ed3\u5408\uff0c\u901a\u8fc7\u56fe\u50cf\u7f16\u8f91\u751f\u6210\u5bf9\u6297\u6837\u672c\u3002", "result": "\u65b9\u6cd5\u5728ACM MM25\u7ade\u8d5b\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u5bf9\u6297\u6837\u672c\u65f6\u7684\u6cdb\u5316\u548c\u8fc1\u79fb\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u4efb\u52a1\u3002"}}
{"id": "2506.23690", "pdf": "https://arxiv.org/pdf/2506.23690", "abs": "https://arxiv.org/abs/2506.23690", "authors": ["Shuai Tan", "Biao Gong", "Yujie Wei", "Shiwei Zhang", "Zhuoxin Liu", "Dandan Zheng", "Jingdong Chen", "Yan Wang", "Hao Ouyang", "Kecheng Zheng", "Yujun Shen"], "title": "SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation", "categories": ["cs.CV"], "comment": "Project page: https://lucaria-academy.github.io/SynMotion/", "summary": "Diffusion-based video motion customization facilitates the acquisition of human motion representations from a few video samples, while achieving arbitrary subjects transfer through precise textual conditioning. Existing approaches often rely on semantic-level alignment, expecting the model to learn new motion concepts and combine them with other entities (e.g., ''cats'' or ''dogs'') to produce visually appealing results. However, video data involve complex spatio-temporal patterns, and focusing solely on semantics cause the model to overlook the visual complexity of motion. Conversely, tuning only the visual representation leads to semantic confusion in representing the intended action. To address these limitations, we propose SynMotion, a new motion-customized video generation model that jointly leverages semantic guidance and visual adaptation. At the semantic level, we introduce the dual-embedding semantic comprehension mechanism which disentangles subject and motion representations, allowing the model to learn customized motion features while preserving its generative capabilities for diverse subjects. At the visual level, we integrate parameter-efficient motion adapters into a pre-trained video generation model to enhance motion fidelity and temporal coherence. Furthermore, we introduce a new embedding-specific training strategy which \\textbf{alternately optimizes} subject and motion embeddings, supported by the manually constructed Subject Prior Video (SPV) training dataset. This strategy promotes motion specificity while preserving generalization across diverse subjects. Lastly, we introduce MotionBench, a newly curated benchmark with diverse motion patterns. Experimental results across both T2V and I2V settings demonstrate that \\method outperforms existing baselines. Project page: https://lucaria-academy.github.io/SynMotion/", "AI": {"tldr": "SynMotion\u662f\u4e00\u79cd\u65b0\u7684\u8fd0\u52a8\u5b9a\u5236\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u5f15\u5bfc\u548c\u89c6\u89c9\u9002\u5e94\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u4e49\u5bf9\u9f50\u548c\u89c6\u89c9\u590d\u6742\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u9891\u8fd0\u52a8\u5b9a\u5236\u4e2d\u8981\u4e48\u8fc7\u4e8e\u4f9d\u8d56\u8bed\u4e49\u5bf9\u9f50\uff0c\u5bfc\u81f4\u5ffd\u7565\u89c6\u89c9\u590d\u6742\u6027\uff0c\u8981\u4e48\u4ec5\u8c03\u6574\u89c6\u89c9\u8868\u793a\uff0c\u5bfc\u81f4\u8bed\u4e49\u6df7\u6dc6\u3002SynMotion\u65e8\u5728\u540c\u65f6\u89e3\u51b3\u8fd9\u4e24\u4e2a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u53cc\u5d4c\u5165\u8bed\u4e49\u7406\u89e3\u673a\u5236\u5206\u79bb\u4e3b\u9898\u548c\u8fd0\u52a8\u8868\u793a\uff0c\u5e76\u96c6\u6210\u53c2\u6570\u9ad8\u6548\u7684\u8fd0\u52a8\u9002\u914d\u5668\u4ee5\u589e\u5f3a\u8fd0\u52a8\u4fdd\u771f\u5ea6\u3002\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\u7b56\u7565\u8bad\u7ec3\u4e3b\u9898\u548c\u8fd0\u52a8\u5d4c\u5165\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSynMotion\u5728\u6587\u672c\u5230\u89c6\u9891\u548c\u56fe\u50cf\u5230\u89c6\u9891\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "SynMotion\u901a\u8fc7\u8054\u5408\u8bed\u4e49\u548c\u89c6\u89c9\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u8fd0\u52a8\u5b9a\u5236\u89c6\u9891\u751f\u6210\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6MotionBench\u3002"}}
{"id": "2506.23729", "pdf": "https://arxiv.org/pdf/2506.23729", "abs": "https://arxiv.org/abs/2506.23729", "authors": ["Guiyu Zhang", "Chen Shi", "Zijian Jiang", "Xunzhi Xiang", "Jingjing Qian", "Shaoshuai Shi", "Li Jiang"], "title": "Proteus-ID: ID-Consistent and Motion-Coherent Video Customization", "categories": ["cs.CV"], "comment": "Preprint. Work in progress", "summary": "Video identity customization seeks to synthesize realistic, temporally coherent videos of a specific subject, given a single reference image and a text prompt. This task presents two core challenges: (1) maintaining identity consistency while aligning with the described appearance and actions, and (2) generating natural, fluid motion without unrealistic stiffness. To address these challenges, we introduce Proteus-ID, a novel diffusion-based framework for identity-consistent and motion-coherent video customization. First, we propose a Multimodal Identity Fusion (MIF) module that unifies visual and textual cues into a joint identity representation using a Q-Former, providing coherent guidance to the diffusion model and eliminating modality imbalance. Second, we present a Time-Aware Identity Injection (TAII) mechanism that dynamically modulates identity conditioning across denoising steps, improving fine-detail reconstruction. Third, we propose Adaptive Motion Learning (AML), a self-supervised strategy that reweights the training loss based on optical-flow-derived motion heatmaps, enhancing motion realism without requiring additional inputs. To support this task, we construct Proteus-Bench, a high-quality dataset comprising 200K curated clips for training and 150 individuals from diverse professions and ethnicities for evaluation. Extensive experiments demonstrate that Proteus-ID outperforms prior methods in identity preservation, text alignment, and motion quality, establishing a new benchmark for video identity customization. Codes and data are publicly available at https://grenoble-zhang.github.io/Proteus-ID/.", "AI": {"tldr": "Proteus-ID\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u89c6\u9891\u8eab\u4efd\u5b9a\u5236\u4e2d\u7684\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u8fd0\u52a8\u8fde\u8d2f\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u8eab\u4efd\u878d\u5408\u548c\u65f6\u95f4\u611f\u77e5\u8eab\u4efd\u6ce8\u5165\u7b49\u65b9\u6cd5\u63d0\u5347\u6548\u679c\u3002", "motivation": "\u89c6\u9891\u8eab\u4efd\u5b9a\u5236\u4efb\u52a1\u9762\u4e34\u8eab\u4efd\u4e00\u81f4\u6027\u4e0e\u8fd0\u52a8\u81ea\u7136\u6027\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u8fd9\u4e24\u70b9\u3002", "method": "\u63d0\u51faMIF\u6a21\u5757\u7edf\u4e00\u89c6\u89c9\u548c\u6587\u672c\u7ebf\u7d22\uff0cTAII\u673a\u5236\u52a8\u6001\u8c03\u8282\u8eab\u4efd\u6761\u4ef6\uff0cAML\u7b56\u7565\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u589e\u5f3a\u8fd0\u52a8\u771f\u5b9e\u6027\u3002", "result": "Proteus-ID\u5728\u8eab\u4efd\u4fdd\u6301\u3001\u6587\u672c\u5bf9\u9f50\u548c\u8fd0\u52a8\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002", "conclusion": "Proteus-ID\u4e3a\u89c6\u9891\u8eab\u4efd\u5b9a\u5236\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u516c\u5f00\u3002"}}
{"id": "2506.23751", "pdf": "https://arxiv.org/pdf/2506.23751", "abs": "https://arxiv.org/abs/2506.23751", "authors": ["Annika M\u00fctze", "Sadia Ilyas", "Christian D\u00f6rpelkus", "Matthias Rottmann"], "title": "Can We Challenge Open-Vocabulary Object Detectors with Generated Content in Street Scenes?", "categories": ["cs.CV"], "comment": null, "summary": "Open-vocabulary object detectors such as Grounding DINO are trained on vast and diverse data, achieving remarkable performance on challenging datasets. Due to that, it is unclear where to find their limitations, which is of major concern when using in safety-critical applications. Real-world data does not provide sufficient control, required for a rigorous evaluation of model generalization. In contrast, synthetically generated data allows to systematically explore the boundaries of model competence/generalization. In this work, we address two research questions: 1) Can we challenge open-vocabulary object detectors with generated image content? 2) Can we find systematic failure modes of those models? To address these questions, we design two automated pipelines using stable diffusion to inpaint unusual objects with high diversity in semantics, by sampling multiple substantives from WordNet and ChatGPT. On the synthetically generated data, we evaluate and compare multiple open-vocabulary object detectors as well as a classical object detector. The synthetic data is derived from two real-world datasets, namely LostAndFound, a challenging out-of-distribution (OOD) detection benchmark, and the NuImages dataset. Our results indicate that inpainting can challenge open-vocabulary object detectors in terms of overlooking objects. Additionally, we find a strong dependence of open-vocabulary models on object location, rather than on object semantics. This provides a systematic approach to challenge open-vocabulary models and gives valuable insights on how data could be acquired to effectively improve these models.", "AI": {"tldr": "\u7814\u7a76\u8005\u4f7f\u7528\u5408\u6210\u6570\u636e\u6311\u6218\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u53d1\u73b0\u5176\u6027\u80fd\u53d7\u7269\u4f53\u4f4d\u7f6e\u800c\u975e\u8bed\u4e49\u5f71\u54cd\u3002", "motivation": "\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u5668\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f46\u771f\u5b9e\u6570\u636e\u96be\u4ee5\u7cfb\u7edf\u8bc4\u4f30\u5176\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u5229\u7528\u7a33\u5b9a\u6269\u6563\u6280\u672f\u751f\u6210\u591a\u6837\u8bed\u4e49\u7684\u5408\u6210\u6570\u636e\uff0c\u8bc4\u4f30\u591a\u79cd\u68c0\u6d4b\u5668\u3002", "result": "\u5408\u6210\u6570\u636e\u53ef\u6311\u6218\u68c0\u6d4b\u5668\uff0c\u53d1\u73b0\u5176\u6027\u80fd\u4f9d\u8d56\u7269\u4f53\u4f4d\u7f6e\u800c\u975e\u8bed\u4e49\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u4e3a\u6311\u6218\u548c\u6539\u8fdb\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u5668\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2506.23801", "pdf": "https://arxiv.org/pdf/2506.23801", "abs": "https://arxiv.org/abs/2506.23801", "authors": ["Ce Wang", "Wanjie Sun"], "title": "Controllable Reference-Based Real-World Remote Sensing Image Super-Resolution with Generative Diffusion Priors", "categories": ["cs.CV"], "comment": null, "summary": "Super-resolution (SR) techniques can enhance the spatial resolution of remote sensing images by utilizing low-resolution (LR) images to reconstruct high-resolution (HR) images, enabling more efficient large-scale earth observation applications. While single-image super-resolution (SISR) methods have shown progress, reference-based super-resolution (RefSR) offers superior performance by incorporating historical HR images alongside current LR observations. However, existing RefSR methods struggle with real-world complexities, such as cross-sensor resolution gap and significant land cover changes, often leading to under-generation or over-reliance on reference image. To address these challenges, we propose CRefDiff, a novel controllable reference-based diffusion model for real-world remote sensing image SR. To address the under-generation problem, CRefDiff is built upon the pretrained Stable Diffusion model, leveraging its powerful generative prior to produce accurate structures and textures. To mitigate over-reliance on the reference, we introduce a dual-branch fusion mechanism that adaptively integrates both local and global information from the reference image. Moreover, this novel dual-branch design enables reference strength control during inference, enhancing interactivity and flexibility of the model. Finally, a strategy named Better Start is proposed to significantly reduce the number of denoising steps, thereby accelerating the inference process. To support further research, we introduce Real-RefRSSRD, a new real-world RefSR dataset for remote sensing images, consisting of HR NAIP and LR Sentinel-2 image pairs with diverse land cover changes and significant temporal gaps. Extensive experiments on Real-RefRSSRD show that CRefDiff achieves state-of-the-art performance across various metrics and improves downstream tasks such as scene classification and semantic segmentation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCRefDiff\u7684\u65b0\u578b\u53ef\u63a7\u53c2\u8003\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u9065\u611f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4e2d\u7684\u5b9e\u9645\u95ee\u9898\uff0c\u5982\u8de8\u4f20\u611f\u5668\u5206\u8fa8\u7387\u5dee\u8ddd\u548c\u5730\u8868\u8986\u76d6\u53d8\u5316\u3002", "motivation": "\u73b0\u6709\u53c2\u8003\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u751f\u6210\u4e0d\u8db3\u6216\u8fc7\u5ea6\u4f9d\u8d56\u53c2\u8003\u56fe\u50cf\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684Stable Diffusion\u6a21\u578b\uff0c\u5f15\u5165\u53cc\u5206\u652f\u878d\u5408\u673a\u5236\u81ea\u9002\u5e94\u6574\u5408\u53c2\u8003\u56fe\u50cf\u7684\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\uff0c\u5e76\u63d0\u51faBetter Start\u7b56\u7565\u52a0\u901f\u63a8\u7406\u3002", "result": "\u5728Real-RefRSSRD\u6570\u636e\u96c6\u4e0a\uff0cCRefDiff\u5728\u5404\u9879\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u4f18\uff0c\u5e76\u63d0\u5347\u4e86\u573a\u666f\u5206\u7c7b\u548c\u8bed\u4e49\u5206\u5272\u7b49\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "CRefDiff\u901a\u8fc7\u53ef\u63a7\u53c2\u8003\u548c\u9ad8\u6548\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9065\u611f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u5b9e\u7528\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2506.23858", "pdf": "https://arxiv.org/pdf/2506.23858", "abs": "https://arxiv.org/abs/2506.23858", "authors": ["Jianzong Wu", "Liang Hou", "Haotian Yang", "Xin Tao", "Ye Tian", "Pengfei Wan", "Di Zhang", "Yunhai Tong"], "title": "VMoBA: Mixture-of-Block Attention for Video Diffusion Models", "categories": ["cs.CV"], "comment": "Code is at https://github.com/KwaiVGI/VMoBA", "summary": "The quadratic complexity of full attention mechanisms poses a significant bottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration, high-resolution videos. While various sparse attention methods have been proposed, many are designed as training-free inference accelerators or do not optimally capture the unique spatio-temporal characteristics inherent in video data when trained natively. This paper introduces Video Mixture of Block Attention (VMoBA), a novel sparse attention mechanism specifically adapted for VDMs. Motivated by an in-depth analysis of attention patterns within pre-trained video transformers, which revealed strong spatio-temporal locality, varying query importance, and head-specific concentration levels, VMoBA enhances the original MoBA framework with three key modifications: (1) a layer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to diverse spatio-temporal attention patterns and improve efficiency; (2) global block selection to prioritize the most salient query-key block interactions across an entire attention head; and (3) threshold-based block selection to dynamically determine the number of attended blocks based on their cumulative similarity. Extensive experiments demonstrate that VMoBA significantly accelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and 1.48x latency speedup, while attaining comparable or even superior generation quality to full attention. Furthermore, VMoBA exhibits competitive performance in training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for high-res video generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVMoBA\u7684\u65b0\u578b\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4e8e\u4f18\u5316\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08VDMs\uff09\u7684\u957f\u5e8f\u5217\u8bad\u7ec3\u548c\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u751f\u6210\u6548\u7387\u3002", "motivation": "\u5168\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u9650\u5236\u4e86VDMs\u5728\u957f\u89c6\u9891\u751f\u6210\u4e2d\u7684\u6548\u7387\uff0c\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u6355\u6349\u89c6\u9891\u6570\u636e\u7684\u65f6\u7a7a\u7279\u6027\u3002", "method": "VMoBA\u901a\u8fc7\u5c42\u9012\u8fdb\u7684\u5757\u5206\u533a\uff081D-2D-3D\uff09\u3001\u5168\u5c40\u5757\u9009\u62e9\u548c\u57fa\u4e8e\u9608\u503c\u7684\u5757\u9009\u62e9\uff0c\u52a8\u6001\u9002\u5e94\u89c6\u9891\u6570\u636e\u7684\u65f6\u7a7a\u6ce8\u610f\u529b\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u663e\u793aVMoBA\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\uff082.92x FLOPs\u548c1.48x\u5ef6\u8fdf\u52a0\u901f\uff09\uff0c\u751f\u6210\u8d28\u91cf\u4e0e\u5168\u6ce8\u610f\u529b\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "conclusion": "VMoBA\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u4e2d\u5747\u8868\u73b0\u51fa\u9ad8\u6548\u6027\uff0c\u4e3a\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23903", "pdf": "https://arxiv.org/pdf/2506.23903", "abs": "https://arxiv.org/abs/2506.23903", "authors": ["Hamza Rasaee", "Taha Koleilat", "Hassan Rivaz"], "title": "GroundingDINO-US-SAM: Text-Prompted Multi-Organ Segmentation in Ultrasound with LoRA-Tuned Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 3 figures, 6 figures", "summary": "Accurate and generalizable object segmentation in ultrasound imaging remains a significant challenge due to anatomical variability, diverse imaging protocols, and limited annotated data. In this study, we propose a prompt-driven vision-language model (VLM) that integrates Grounding DINO with SAM2 to enable object segmentation across multiple ultrasound organs. A total of 18 public ultrasound datasets, encompassing the breast, thyroid, liver, prostate, kidney, and paraspinal muscle, were utilized. These datasets were divided into 15 for fine-tuning and validation of Grounding DINO using Low Rank Adaptation (LoRA) to the ultrasound domain, and 3 were held out entirely for testing to evaluate performance in unseen distributions. Comprehensive experiments demonstrate that our approach outperforms state-of-the-art segmentation methods, including UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse, and SAMUS on most seen datasets while maintaining strong performance on unseen datasets without additional fine-tuning. These results underscore the promise of VLMs in scalable and robust ultrasound image analysis, reducing dependence on large, organ-specific annotated datasets. We will publish our code on code.sonography.ai after acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u9a71\u52a8\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u7ed3\u5408Grounding DINO\u548cSAM2\uff0c\u7528\u4e8e\u591a\u5668\u5b98\u8d85\u58f0\u56fe\u50cf\u5206\u5272\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u8d85\u58f0\u56fe\u50cf\u4e2d\u56e0\u89e3\u5256\u53d8\u5f02\u3001\u6210\u50cf\u534f\u8bae\u591a\u6837\u6027\u548c\u6807\u6ce8\u6570\u636e\u6709\u9650\u5bfc\u81f4\u7684\u5bf9\u8c61\u5206\u5272\u96be\u9898\u3002", "method": "\u5229\u752818\u4e2a\u516c\u5171\u8d85\u58f0\u6570\u636e\u96c6\uff0c\u901a\u8fc7LoRA\u5fae\u8c03Grounding DINO\uff0c\u5e76\u5728\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u6027\u80fd\u3002", "result": "\u5728\u591a\u6570\u6570\u636e\u96c6\u4e0a\u4f18\u4e8eUniverSeg\u3001MedSAM\u7b49\u65b9\u6cd5\uff0c\u4e14\u5728\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\u3002", "conclusion": "VLM\u5728\u8d85\u58f0\u56fe\u50cf\u5206\u6790\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u51cf\u5c11\u4e86\u5bf9\u5927\u89c4\u6a21\u5668\u5b98\u7279\u5b9a\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002"}}
{"id": "2506.24063", "pdf": "https://arxiv.org/pdf/2506.24063", "abs": "https://arxiv.org/abs/2506.24063", "authors": ["Deng Li", "Aming Wu", "Yang Li", "Yaowei Wang", "Yahong Han"], "title": "Continual Adaptation: Environment-Conditional Parameter Generation for Object Detection in Dynamic Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "In practice, environments constantly change over time and space, posing significant challenges for object detectors trained based on a closed-set assumption, i.e., training and test data share the same distribution. To this end, continual test-time adaptation has attracted much attention, aiming to improve detectors' generalization by fine-tuning a few specific parameters, e.g., BatchNorm layers. However, based on a small number of test images, fine-tuning certain parameters may affect the representation ability of other fixed parameters, leading to performance degradation. Instead, we explore a new mechanism, i.e., converting the fine-tuning process to a specific-parameter generation. Particularly, we first design a dual-path LoRA-based domain-aware adapter that disentangles features into domain-invariant and domain-specific components, enabling efficient adaptation. Additionally, a conditional diffusion-based parameter generation mechanism is presented to synthesize the adapter's parameters based on the current environment, preventing the optimization from getting stuck in local optima. Finally, we propose a class-centered optimal transport alignment method to mitigate catastrophic forgetting. Extensive experiments conducted on various continuous domain adaptive object detection tasks demonstrate the effectiveness. Meanwhile, visualization results show that the representation extracted by the generated parameters can capture more object-related information and strengthen the generalization ability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6301\u7eed\u6d4b\u8bd5\u65f6\u9002\u5e94\u673a\u5236\uff0c\u901a\u8fc7\u53c2\u6570\u751f\u6210\u800c\u975e\u5fae\u8c03\u6765\u63d0\u5347\u76ee\u6807\u68c0\u6d4b\u5668\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u56e0\u5c11\u91cf\u6d4b\u8bd5\u56fe\u50cf\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u73af\u5883\u4e2d\u6570\u636e\u5206\u5e03\u968f\u65f6\u95f4\u7a7a\u95f4\u53d8\u5316\uff0c\u4f20\u7edf\u57fa\u4e8e\u95ed\u96c6\u5047\u8bbe\u7684\u76ee\u6807\u68c0\u6d4b\u5668\u96be\u4ee5\u9002\u5e94\u3002\u6301\u7eed\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u901a\u8fc7\u5fae\u8c03\u7279\u5b9a\u53c2\u6570\uff08\u5982BatchNorm\u5c42\uff09\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5c11\u91cf\u6d4b\u8bd5\u56fe\u50cf\u53ef\u80fd\u5bfc\u81f4\u5176\u4ed6\u56fa\u5b9a\u53c2\u6570\u8868\u5f81\u80fd\u529b\u4e0b\u964d\u3002", "method": "\u8bbe\u8ba1\u4e86\u53cc\u8def\u5f84LoRA\u57df\u611f\u77e5\u9002\u914d\u5668\uff0c\u5206\u79bb\u57df\u4e0d\u53d8\u548c\u57df\u7279\u5b9a\u7279\u5f81\uff1b\u63d0\u51fa\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u7684\u53c2\u6570\u751f\u6210\u673a\u5236\uff0c\u52a8\u6001\u5408\u6210\u9002\u914d\u5668\u53c2\u6570\uff1b\u5f15\u5165\u7c7b\u4e2d\u5fc3\u6700\u4f18\u4f20\u8f93\u5bf9\u9f50\u65b9\u6cd5\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "\u5728\u591a\u79cd\u8fde\u7eed\u57df\u81ea\u9002\u5e94\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u53ef\u89c6\u5316\u7ed3\u679c\u663e\u793a\u751f\u6210\u7684\u53c2\u6570\u80fd\u63d0\u53d6\u66f4\u591a\u76ee\u6807\u76f8\u5173\u4fe1\u606f\u5e76\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u53c2\u6570\u751f\u6210\u673a\u5236\u548c\u57df\u611f\u77e5\u9002\u914d\u5668\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u76ee\u6807\u68c0\u6d4b\u5668\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u548c\u6027\u80fd\u3002"}}
{"id": "2506.24086", "pdf": "https://arxiv.org/pdf/2506.24086", "abs": "https://arxiv.org/abs/2506.24086", "authors": ["Bingfan Zhu", "Biao Jiang", "Sunyi Wang", "Shixiang Tang", "Tao Chen", "Linjie Luo", "Youyi Zheng", "Xin Chen"], "title": "MotionGPT3: Human Motion as a Second Modality", "categories": ["cs.CV", "cs.CL"], "comment": "21 pages, 8 figures", "summary": "Though recent advances in multimodal models have demonstrated strong capabilities and opportunities in unified understanding and generation, the development of unified motion-language models remains underexplored. To enable such models with high-fidelity human motion, two core challenges must be addressed. The first is the reconstruction gap between the continuous motion modality and discrete representation in an autoregressive manner, and the second is the degradation of language intelligence during unified training. Inspired by the mixture of experts, we propose MotionGPT3, a bimodal motion-language model that treats human motion as a second modality, decoupling motion modeling via separate model parameters and enabling both effective cross-modal interaction and efficient multimodal scaling training. To preserve language intelligence, the text branch retains the original structure and parameters of the pretrained language model, while a new motion branch is integrated via a shared attention mechanism, enabling bidirectional information flow between two modalities. We first employ a motion Variational Autoencoder (VAE) to encode raw human motion into latent representations. Based on this continuous latent space, the motion branch predicts motion latents directly from intermediate hidden states using a diffusion head, bypassing discrete tokenization. Extensive experiments show that our approach achieves competitive performance on both motion understanding and generation tasks while preserving strong language capabilities, establishing a unified bimodal motion diffusion framework within an autoregressive manner.", "AI": {"tldr": "MotionGPT3\u662f\u4e00\u4e2a\u53cc\u6a21\u6001\u8fd0\u52a8-\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u79bb\u53c2\u6570\u5904\u7406\u8fd0\u52a8\u5efa\u6a21\uff0c\u89e3\u51b3\u4e86\u8fd0\u52a8\u6a21\u6001\u4e0e\u79bb\u6563\u8868\u793a\u4e4b\u95f4\u7684\u91cd\u5efa\u5dee\u8ddd\u548c\u8bed\u8a00\u667a\u80fd\u9000\u5316\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u6a21\u578b\u5728\u7edf\u4e00\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u7edf\u4e00\u8fd0\u52a8-\u8bed\u8a00\u6a21\u578b\u7684\u5f00\u53d1\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u65b9\u6cd5\uff0cMotionGPT3\u5c06\u8fd0\u52a8\u4f5c\u4e3a\u7b2c\u4e8c\u6a21\u6001\uff0c\u901a\u8fc7\u5171\u4eab\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u540c\u65f6\u4fdd\u7559\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u6784\u548c\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMotionGPT3\u5728\u8fd0\u52a8\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u8bed\u8a00\u80fd\u529b\u3002", "conclusion": "MotionGPT3\u5efa\u7acb\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u53cc\u6a21\u6001\u8fd0\u52a8\u6269\u6563\u6846\u67b6\uff0c\u4e3a\u8fd0\u52a8-\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.24092", "pdf": "https://arxiv.org/pdf/2506.24092", "abs": "https://arxiv.org/abs/2506.24092", "authors": ["Moein Heidari", "Yasamin Medghalchi", "Mahdi Khoursha", "Reza Rezaeian", "Ilker Hacihaliloglu"], "title": "WaRA: Wavelet Low Rank Adaptation", "categories": ["cs.CV", "eess.IV"], "comment": "Submitted to BMVC 2025", "summary": "Parameter-efficient fine-tuning (PEFT) has gained widespread adoption across various applications. Among PEFT techniques, Low-Rank Adaptation (LoRA) and its extensions have emerged as particularly effective, allowing efficient model adaptation while significantly reducing computational overhead. However, existing approaches typically rely on global low-rank factorizations, which overlook local or multi-scale structure, failing to capture complex patterns in the weight updates. To address this, we propose WaRA, a novel PEFT method that leverages wavelet transforms to decompose the weight update matrix into a multi-resolution representation. By performing low-rank factorization in the wavelet domain and reconstructing updates through an inverse transform, WaRA obtains compressed adaptation parameters that harness multi-resolution analysis, enabling it to capture both coarse and fine-grained features while providing greater flexibility and sparser representations than standard LoRA. Through comprehensive experiments and analysis, we demonstrate that WaRA performs superior on diverse vision tasks, including image generation, classification, and semantic segmentation, significantly enhancing generated image quality while reducing computational complexity. Although WaRA was primarily designed for vision tasks, we further showcase its effectiveness in language tasks, highlighting its broader applicability and generalizability. The code is publicly available at \\href{GitHub}{https://github.com/moeinheidari7829/WaRA}.", "AI": {"tldr": "WaRA\u662f\u4e00\u79cd\u65b0\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u5229\u7528\u5c0f\u6ce2\u53d8\u6362\u5206\u89e3\u6743\u91cd\u66f4\u65b0\u77e9\u9635\uff0c\u5b9e\u73b0\u591a\u5206\u8fa8\u7387\u5206\u6790\uff0c\u4f18\u4e8e\u4f20\u7edfLoRA\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709PEFT\u65b9\u6cd5\uff08\u5982LoRA\uff09\u4f9d\u8d56\u5168\u5c40\u4f4e\u79e9\u5206\u89e3\uff0c\u5ffd\u7565\u4e86\u5c40\u90e8\u6216\u591a\u5c3a\u5ea6\u7ed3\u6784\uff0c\u65e0\u6cd5\u6355\u6349\u6743\u91cd\u66f4\u65b0\u4e2d\u7684\u590d\u6742\u6a21\u5f0f\u3002", "method": "WaRA\u901a\u8fc7\u5c0f\u6ce2\u53d8\u6362\u5c06\u6743\u91cd\u66f4\u65b0\u77e9\u9635\u5206\u89e3\u4e3a\u591a\u5206\u8fa8\u7387\u8868\u793a\uff0c\u5728\u9891\u57df\u8fdb\u884c\u4f4e\u79e9\u5206\u89e3\uff0c\u5e76\u901a\u8fc7\u9006\u53d8\u6362\u91cd\u5efa\u66f4\u65b0\u3002", "result": "WaRA\u5728\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u56fe\u50cf\u751f\u6210\u3001\u5206\u7c7b\u548c\u8bed\u4e49\u5206\u5272\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5e76\u5728\u8bed\u8a00\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u901a\u7528\u6027\u3002", "conclusion": "WaRA\u901a\u8fc7\u591a\u5206\u8fa8\u7387\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u7a00\u758f\u7684\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u8d28\u91cf\uff0c\u5e76\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2506.24096", "pdf": "https://arxiv.org/pdf/2506.24096", "abs": "https://arxiv.org/abs/2506.24096", "authors": ["Antoine Gu\u00e9don", "Diego Gomez", "Nissim Maruani", "Bingchen Gong", "George Drettakis", "Maks Ovsjanikov"], "title": "MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient Surface Reconstruction", "categories": ["cs.CV"], "comment": "10 pages. A presentation video of our approach is available at   https://youtu.be/_SGNhhNz0fE", "summary": "While recent advances in Gaussian Splatting have enabled fast reconstruction of high-quality 3D scenes from images, extracting accurate surface meshes remains a challenge. Current approaches extract the surface through costly post-processing steps, resulting in the loss of fine geometric details or requiring significant time and leading to very dense meshes with millions of vertices. More fundamentally, the a posteriori conversion from a volumetric to a surface representation limits the ability of the final mesh to preserve all geometric structures captured during training. We present MILo, a novel Gaussian Splatting framework that bridges the gap between volumetric and surface representations by differentiably extracting a mesh from the 3D Gaussians. We design a fully differentiable procedure that constructs the mesh-including both vertex locations and connectivity-at every iteration directly from the parameters of the Gaussians, which are the only quantities optimized during training. Our method introduces three key technical contributions: a bidirectional consistency framework ensuring both representations-Gaussians and the extracted mesh-capture the same underlying geometry during training; an adaptive mesh extraction process performed at each training iteration, which uses Gaussians as differentiable pivots for Delaunay triangulation; a novel method for computing signed distance values from the 3D Gaussians that enables precise surface extraction while avoiding geometric erosion. Our approach can reconstruct complete scenes, including backgrounds, with state-of-the-art quality while requiring an order of magnitude fewer mesh vertices than previous methods. Due to their light weight and empty interior, our meshes are well suited for downstream applications such as physics simulations or animation.", "AI": {"tldr": "MILo\u662f\u4e00\u79cd\u65b0\u578b\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u5730\u4ece3D\u9ad8\u65af\u4e2d\u63d0\u53d6\u7f51\u683c\uff0c\u5f25\u5408\u4e86\u4f53\u79ef\u548c\u8868\u9762\u8868\u793a\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u901a\u8fc7\u6602\u8d35\u7684\u540e\u5904\u7406\u6b65\u9aa4\u63d0\u53d6\u8868\u9762\uff0c\u5bfc\u81f4\u51e0\u4f55\u7ec6\u8282\u4e22\u5931\u6216\u751f\u6210\u5bc6\u96c6\u7f51\u683c\uff0c\u9650\u5236\u4e86\u6700\u7ec8\u7f51\u683c\u4fdd\u7559\u8bad\u7ec3\u671f\u95f4\u6355\u83b7\u7684\u6240\u6709\u51e0\u4f55\u7ed3\u6784\u7684\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5b8c\u5168\u53ef\u5fae\u5206\u7684\u8fc7\u7a0b\uff0c\u76f4\u63a5\u4ece\u9ad8\u65af\u53c2\u6570\u6784\u5efa\u7f51\u683c\uff0c\u5305\u62ec\u9876\u70b9\u4f4d\u7f6e\u548c\u8fde\u63a5\u6027\uff0c\u5e76\u5f15\u5165\u4e86\u53cc\u5411\u4e00\u81f4\u6027\u6846\u67b6\u3001\u81ea\u9002\u5e94\u7f51\u683c\u63d0\u53d6\u8fc7\u7a0b\u548c\u57fa\u4e8e\u9ad8\u65af\u7684\u6709\u7b26\u53f7\u8ddd\u79bb\u8ba1\u7b97\u65b9\u6cd5\u3002", "result": "\u80fd\u591f\u4ee5\u6700\u5148\u8fdb\u7684\u8d28\u91cf\u91cd\u5efa\u5b8c\u6574\u573a\u666f\uff0c\u540c\u65f6\u6bd4\u5148\u524d\u65b9\u6cd5\u5c11\u4e00\u4e2a\u6570\u91cf\u7ea7\u7684\u7f51\u683c\u9876\u70b9\u3002", "conclusion": "MILo\u751f\u6210\u7684\u7f51\u683c\u8f7b\u91cf\u4e14\u5185\u90e8\u4e3a\u7a7a\uff0c\u9002\u7528\u4e8e\u7269\u7406\u6a21\u62df\u6216\u52a8\u753b\u7b49\u4e0b\u6e38\u5e94\u7528\u3002"}}
{"id": "2506.24113", "pdf": "https://arxiv.org/pdf/2506.24113", "abs": "https://arxiv.org/abs/2506.24113", "authors": ["Kaiwen Zhang", "Zhenyu Tang", "Xiaotao Hu", "Xingang Pan", "Xiaoyang Guo", "Yuan Liu", "Jingwei Huang", "Li Yuan", "Qian Zhang", "Xiao-Xiao Long", "Xun Cao", "Wei Yin"], "title": "Epona: Autoregressive Diffusion World Model for Autonomous Driving", "categories": ["cs.CV"], "comment": "ICCV2025, Project Page: https://kevin-thu.github.io/Epona/", "summary": "Diffusion models have demonstrated exceptional visual quality in video generation, making them promising for autonomous driving world modeling. However, existing video diffusion-based world models struggle with flexible-length, long-horizon predictions and integrating trajectory planning. This is because conventional video diffusion models rely on global joint distribution modeling of fixed-length frame sequences rather than sequentially constructing localized distributions at each timestep. In this work, we propose Epona, an autoregressive diffusion world model that enables localized spatiotemporal distribution modeling through two key innovations: 1) Decoupled spatiotemporal factorization that separates temporal dynamics modeling from fine-grained future world generation, and 2) Modular trajectory and video prediction that seamlessly integrate motion planning with visual modeling in an end-to-end framework. Our architecture enables high-resolution, long-duration generation while introducing a novel chain-of-forward training strategy to address error accumulation in autoregressive loops. Experimental results demonstrate state-of-the-art performance with 7.4\\% FVD improvement and minutes longer prediction duration compared to prior works. The learned world model further serves as a real-time motion planner, outperforming strong end-to-end planners on NAVSIM benchmarks. Code will be publicly available at \\href{https://github.com/Kevin-thu/Epona/}{https://github.com/Kevin-thu/Epona/}.", "AI": {"tldr": "Epona\u662f\u4e00\u79cd\u81ea\u56de\u5f52\u6269\u6563\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u89e3\u8026\u65f6\u7a7a\u56e0\u5b50\u5316\u548c\u6a21\u5757\u5316\u8f68\u8ff9\u4e0e\u89c6\u9891\u9884\u6d4b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u5206\u8fa8\u7387\u3001\u957f\u65f6\u957f\u7684\u751f\u6210\uff0c\u5e76\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u9891\u6269\u6563\u7684\u4e16\u754c\u6a21\u578b\u5728\u7075\u6d3b\u957f\u5ea6\u3001\u957f\u65f6\u9884\u6d4b\u548c\u8f68\u8ff9\u89c4\u5212\u96c6\u6210\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u5176\u4f9d\u8d56\u56fa\u5b9a\u957f\u5ea6\u5e27\u5e8f\u5217\u7684\u5168\u5c40\u8054\u5408\u5206\u5e03\u5efa\u6a21\u3002", "method": "\u63d0\u51faEpona\uff0c\u91c7\u7528\u89e3\u8026\u65f6\u7a7a\u56e0\u5b50\u5316\u548c\u6a21\u5757\u5316\u8f68\u8ff9\u4e0e\u89c6\u9891\u9884\u6d4b\uff0c\u7ed3\u5408\u94fe\u5f0f\u524d\u5411\u8bad\u7ec3\u7b56\u7565\u4ee5\u51cf\u5c11\u81ea\u56de\u5f52\u5faa\u73af\u4e2d\u7684\u8bef\u5dee\u79ef\u7d2f\u3002", "result": "\u5b9e\u9a8c\u663e\u793aFVD\u63d0\u53477.4%\uff0c\u9884\u6d4b\u65f6\u957f\u663e\u8457\u5ef6\u957f\uff0c\u5e76\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u7aef\u5230\u7aef\u89c4\u5212\u5668\u3002", "conclusion": "Epona\u5728\u89c6\u9891\u751f\u6210\u548c\u8fd0\u52a8\u89c4\u5212\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4e16\u754c\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.24121", "pdf": "https://arxiv.org/pdf/2506.24121", "abs": "https://arxiv.org/abs/2506.24121", "authors": ["Sisi Dai", "Xinxin Su", "Boyan Wan", "Ruizhen Hu", "Kai Xu"], "title": "TextMesh4D: High-Quality Text-to-4D Mesh Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in diffusion generative models significantly advanced image, video, and 3D content creation from user-provided text prompts. However, the challenging problem of dynamic 3D content generation (text-to-4D) with diffusion guidance remains largely unexplored. In this paper, we introduce TextMesh4D, a novel framework for high-quality text-to-4D generation. Our approach leverages per-face Jacobians as a differentiable mesh representation and decomposes 4D generation into two stages: static object creation and dynamic motion synthesis. We further propose a flexibility-rigidity regularization term to stabilize Jacobian optimization under video diffusion priors, ensuring robust geometric performance. Experiments demonstrate that TextMesh4D achieves state-of-the-art results in terms of temporal consistency, structural fidelity, and visual realism. Moreover, TextMesh4D operates with a low GPU memory overhead-requiring only a single 24GB GPU-offering a cost-effective yet high-quality solution for text-driven 4D mesh generation. The code will be released to facilitate future research in text-to-4D generation.", "AI": {"tldr": "TextMesh4D\u662f\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u6587\u672c\u751f\u6210\u9ad8\u8d28\u91cf\u76844D\u5185\u5bb9\uff0c\u901a\u8fc7\u9759\u6001\u5bf9\u8c61\u521b\u5efa\u548c\u52a8\u6001\u8fd0\u52a8\u5408\u6210\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u5b9e\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u6269\u6563\u751f\u6210\u6a21\u578b\u5728\u56fe\u50cf\u3001\u89c6\u9891\u548c3D\u5185\u5bb9\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u52a8\u60013D\u5185\u5bb9\u751f\u6210\uff08\u6587\u672c\u52304D\uff09\u4ecd\u662f\u4e00\u4e2a\u672a\u5145\u5206\u63a2\u7d22\u7684\u6311\u6218\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6bcf\u9762Jacobian\u4f5c\u4e3a\u53ef\u5fae\u5206\u7f51\u683c\u8868\u793a\uff0c\u5c064D\u751f\u6210\u5206\u4e3a\u9759\u6001\u5bf9\u8c61\u521b\u5efa\u548c\u52a8\u6001\u8fd0\u52a8\u5408\u6210\u4e24\u9636\u6bb5\uff0c\u5e76\u5f15\u5165\u7075\u6d3b\u6027-\u521a\u6027\u6b63\u5219\u5316\u9879\u4ee5\u7a33\u5b9a\u4f18\u5316\u3002", "result": "TextMesh4D\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u3001\u7ed3\u6784\u4fdd\u771f\u5ea6\u548c\u89c6\u89c9\u771f\u5b9e\u611f\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u4e14\u4ec5\u9700\u5355\u4e2a24GB GPU\u3002", "conclusion": "TextMesh4D\u4e3a\u6587\u672c\u9a71\u52a8\u76844D\u7f51\u683c\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2506.24123", "pdf": "https://arxiv.org/pdf/2506.24123", "abs": "https://arxiv.org/abs/2506.24123", "authors": ["Yue Ma", "Qingyan Bai", "Hao Ouyang", "Ka Leong Cheng", "Qiuyu Wang", "Hongyu Liu", "Zichen Liu", "Haofan Wang", "Jingye Chen", "Yujun Shen", "Qifeng Chen"], "title": "Calligrapher: Freestyle Text Image Customization", "categories": ["cs.CV"], "comment": "Project page: https://calligrapher2025.github.io/Calligrapher Code:   https://github.com/Calligrapher2025/Calligrapher", "summary": "We introduce Calligrapher, a novel diffusion-based framework that innovatively integrates advanced text customization with artistic typography for digital calligraphy and design applications. Addressing the challenges of precise style control and data dependency in typographic customization, our framework incorporates three key technical contributions. First, we develop a self-distillation mechanism that leverages the pre-trained text-to-image generative model itself alongside the large language model to automatically construct a style-centric typography benchmark. Second, we introduce a localized style injection framework via a trainable style encoder, which comprises both Qformer and linear layers, to extract robust style features from reference images. An in-context generation mechanism is also employed to directly embed reference images into the denoising process, further enhancing the refined alignment of target styles. Extensive quantitative and qualitative evaluations across diverse fonts and design contexts confirm Calligrapher's accurate reproduction of intricate stylistic details and precise glyph positioning. By automating high-quality, visually consistent typography, Calligrapher surpasses traditional models, empowering creative practitioners in digital art, branding, and contextual typographic design.", "AI": {"tldr": "Calligrapher\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u6587\u672c\u5b9a\u5236\u4e0e\u827a\u672f\u5b57\u4f53\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u98ce\u683c\u63a7\u5236\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u6570\u5b57\u4e66\u6cd5\u548c\u8bbe\u8ba1\u4e2d\u7cbe\u786e\u98ce\u683c\u63a7\u5236\u4e0e\u6570\u636e\u4f9d\u8d56\u7684\u6311\u6218\u3002", "method": "1. \u81ea\u84b8\u998f\u673a\u5236\u6784\u5efa\u98ce\u683c\u57fa\u51c6\uff1b2. \u53ef\u8bad\u7ec3\u98ce\u683c\u7f16\u7801\u5668\u63d0\u53d6\u7279\u5f81\uff1b3. \u4e0a\u4e0b\u6587\u751f\u6210\u673a\u5236\u5d4c\u5165\u53c2\u8003\u56fe\u50cf\u3002", "result": "\u5728\u591a\u79cd\u5b57\u4f53\u548c\u8bbe\u8ba1\u573a\u666f\u4e2d\u51c6\u786e\u590d\u73b0\u98ce\u683c\u7ec6\u8282\u548c\u5b57\u5f62\u5b9a\u4f4d\uff0c\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u3002", "conclusion": "Calligrapher\u81ea\u52a8\u5316\u9ad8\u8d28\u91cf\u5b57\u4f53\u8bbe\u8ba1\uff0c\u4e3a\u6570\u5b57\u827a\u672f\u548c\u54c1\u724c\u8bbe\u8ba1\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2506.21629", "pdf": "https://arxiv.org/pdf/2506.21629", "abs": "https://arxiv.org/abs/2506.21629", "authors": ["Chenhao Zhang", "Yezhi Shen", "Fengqing Zhu"], "title": "ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded Scenes", "categories": ["cs.GR", "cs.CV"], "comment": "6 pages, Source code is available at   https://github.com/Chenhao-Z/ICP-3DGS. To appear at ICIP 2025", "summary": "In recent years, neural rendering methods such as NeRFs and 3D Gaussian Splatting (3DGS) have made significant progress in scene reconstruction and novel view synthesis. However, they heavily rely on preprocessed camera poses and 3D structural priors from structure-from-motion (SfM), which are challenging to obtain in outdoor scenarios. To address this challenge, we propose to incorporate Iterative Closest Point (ICP) with optimization-based refinement to achieve accurate camera pose estimation under large camera movements. Additionally, we introduce a voxel-based scene densification approach to guide the reconstruction in large-scale scenes. Experiments demonstrate that our approach ICP-3DGS outperforms existing methods in both camera pose estimation and novel view synthesis across indoor and outdoor scenes of various scales. Source code is available at https://github.com/Chenhao-Z/ICP-3DGS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408ICP\u4e0e\u4f18\u5316\u7ec6\u5316\u7684\u5927\u89c4\u6a21\u573a\u666f\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4f53\u7d20\u573a\u666f\u7a20\u5bc6\u5316\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86NeRF\u548c3DGS\u5728\u6237\u5916\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\uff08\u5982NeRF\u548c3DGS\uff09\u4f9d\u8d56\u9884\u5904\u7406\u76f8\u673a\u59ff\u6001\u548c3D\u7ed3\u6784\u5148\u9a8c\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6237\u5916\u573a\u666f\u4e2d\u96be\u4ee5\u83b7\u53d6\u8fd9\u4e9b\u6570\u636e\u3002", "method": "\u7ed3\u5408ICP\u4e0e\u4f18\u5316\u7ec6\u5316\u8fdb\u884c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\uff0c\u5e76\u91c7\u7528\u4f53\u7d20\u573a\u666f\u7a20\u5bc6\u5316\u6280\u672f\u6307\u5bfc\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cICP-3DGS\u5728\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u548c\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u89c4\u6a21\u7684\u5ba4\u5185\u5916\u573a\u666f\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u7684\u6311\u6218\uff0c\u5e76\u63d0\u5347\u4e86\u795e\u7ecf\u6e32\u67d3\u7684\u6027\u80fd\u3002"}}
{"id": "2506.22532", "pdf": "https://arxiv.org/pdf/2506.22532", "abs": "https://arxiv.org/abs/2506.22532", "authors": ["Mark Wrobel", "Michele Pascale", "Tina Yao", "Ruaraidh Campbell", "Elena Milano", "Michael Quail", "Jennifer Steeden", "Vivek Muthurangu"], "title": "High Resolution Isotropic 3D Cine imaging with Automated Segmentation using Concatenated 2D Real-time Imaging and Deep Learning", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Background: Conventional cardiovascular magnetic resonance (CMR) in paediatric and congenital heart disease uses 2D, breath-hold, balanced steady state free precession (bSSFP) cine imaging for assessment of function and cardiac-gated, respiratory-navigated, static 3D bSSFP whole-heart imaging for anatomical assessment. Our aim is to concatenate a stack 2D free-breathing real-time cines and use Deep Learning (DL) to create an isotropic a fully segmented 3D cine dataset from these images. Methods: Four DL models were trained on open-source data that performed: a) Interslice contrast correction; b) Interslice respiratory motion correction; c) Super-resolution (slice direction); and d) Segmentation of right and left atria and ventricles (RA, LA, RV, and LV), thoracic aorta (Ao) and pulmonary arteries (PA). In 10 patients undergoing routine cardiovascular examination, our method was validated on prospectively acquired sagittal stacks of real-time cine images. Quantitative metrics (ventricular volumes and vessel diameters) and image quality of the 3D cines were compared to conventional breath hold cine and whole heart imaging. Results: All real-time data were successfully transformed into 3D cines with a total post-processing time of <1 min in all cases. There were no significant biases in any LV or RV metrics with reasonable limits of agreement and correlation. There is also reasonable agreement for all vessel diameters, although there was a small but significant overestimation of RPA diameter. Conclusion: We have demonstrated the potential of creating a 3D-cine data from concatenated 2D real-time cine images using a series of DL models. Our method has short acquisition and reconstruction times with fully segmented data being available within 2 minutes. The good agreement with conventional imaging suggests that our method could help to significantly speed up CMR in clinical practice.", "AI": {"tldr": "\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5c062D\u5b9e\u65f6\u7535\u5f71\u56fe\u50cf\u62fc\u63a5\u5e76\u8f6c\u6362\u4e3a3D\u7535\u5f71\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5fc3\u8840\u7ba1\u78c1\u5171\u632f\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edf\u5fc3\u8840\u7ba1\u78c1\u5171\u632f\uff08CMR\uff09\u5728\u513f\u79d1\u548c\u5148\u5929\u6027\u5fc3\u810f\u75c5\u4e2d\u4f7f\u75282D\u547c\u5438\u95e8\u63a7\u6280\u672f\u548c3D\u9759\u6001\u6210\u50cf\uff0c\u4f46\u8017\u65f6\u8f83\u957f\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u5feb\u901f\u751f\u62103D\u7535\u5f71\u6570\u636e\u96c6\uff0c\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u8bad\u7ec3\u4e86\u56db\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5206\u522b\u7528\u4e8e\u5bf9\u6bd4\u6821\u6b63\u3001\u547c\u5438\u8fd0\u52a8\u6821\u6b63\u3001\u8d85\u5206\u8fa8\u7387\u548c\u5fc3\u810f\u7ed3\u6784\u5206\u5272\u3002\u572810\u540d\u60a3\u8005\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u3002", "result": "\u6210\u529f\u5c062D\u56fe\u50cf\u8f6c\u6362\u4e3a3D\u7535\u5f71\uff0c\u5904\u7406\u65f6\u95f4\u5c0f\u4e8e1\u5206\u949f\uff0c\u4e0e\u5e38\u89c4\u6210\u50cf\u65b9\u6cd5\u5728\u5fc3\u5ba4\u4f53\u79ef\u548c\u8840\u7ba1\u76f4\u5f84\u4e0a\u5177\u6709\u826f\u597d\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5feb\u901f\u751f\u62103D\u7535\u5f71\u6570\u636e\u96c6\uff0c\u6709\u671b\u663e\u8457\u63d0\u5347\u4e34\u5e8aCMR\u68c0\u67e5\u7684\u6548\u7387\u3002"}}
{"id": "2506.22826", "pdf": "https://arxiv.org/pdf/2506.22826", "abs": "https://arxiv.org/abs/2506.22826", "authors": ["Robert Beinert", "Jonas Bresch"], "title": "Denoising Multi-Color QR Codes and Stiefel-Valued Data by Relaxed Regularizations", "categories": ["math.OC", "cs.CV", "cs.NA", "math.NA", "94A08, 94A12, 65J22, 90C22, 90C25"], "comment": "9 pages, 2 figures, 3 algorithms", "summary": "The handling of manifold-valued data, for instance, plays a central role in color restoration tasks relying on circle- or sphere-valued color models, in the study of rotational or directional information related to the special orthogonal group, and in Gaussian image processing, where the pixel statistics are interpreted as values on the hyperbolic sheet. Especially, to denoise these kind of data, there have been proposed several generalizations of total variation (TV) and Tikhonov-type denoising models incorporating the underlying manifolds. Recently, a novel, numerically efficient denoising approach has been introduced, where the data are embedded in an Euclidean ambient space, the non-convex manifolds are encoded by a series of positive semi-definite, fixed-rank matrices, and the rank constraint is relaxed to obtain a convexification that can be solved using standard algorithms from convex analysis. The aim of the present paper is to extent this approach to new kinds of data like multi-binary and Stiefel-valued data. Multi-binary data can, for instance, be used to model multi-color QR codes whereas Stiefel-valued data occur in image and video-based recognition. For both new data types, we propose TV- and Tikhonov-based denoising modelstogether with easy-to-solve convexification. All derived methods are evaluated on proof-of-concept, synthetic experiments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u591a\u4e8c\u8fdb\u5236\u548cStiefel\u503c\u6570\u636e\uff0c\u901a\u8fc7\u51f8\u5316\u6280\u672f\u5b9e\u73b0\u9ad8\u6548\u53bb\u566a\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u6269\u5c55\u73b0\u6709\u7684\u6d41\u5f62\u503c\u6570\u636e\u53bb\u566a\u65b9\u6cd5\uff0c\u4ee5\u5904\u7406\u65b0\u578b\u6570\u636e\uff08\u5982\u591a\u4e8c\u8fdb\u5236\u548cStiefel\u503c\u6570\u636e\uff09\uff0c\u8fd9\u4e9b\u6570\u636e\u5728QR\u7801\u548c\u56fe\u50cf\u8bc6\u522b\u4e2d\u6709\u5e94\u7528\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5c06\u6570\u636e\u5d4c\u5165\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\uff0c\u901a\u8fc7\u534a\u6b63\u5b9a\u77e9\u9635\u7f16\u7801\u975e\u51f8\u6d41\u5f62\uff0c\u5e76\u677e\u5f1b\u79e9\u7ea6\u675f\u4ee5\u5b9e\u73b0\u51f8\u5316\u3002\u9488\u5bf9\u591a\u4e8c\u8fdb\u5236\u548cStiefel\u503c\u6570\u636e\uff0c\u63d0\u51fa\u4e86TV\u548cTikhonov\u53bb\u566a\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u5408\u6210\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u5904\u7406\u65b0\u578b\u6d41\u5f62\u503c\u6570\u636e\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22882", "pdf": "https://arxiv.org/pdf/2506.22882", "abs": "https://arxiv.org/abs/2506.22882", "authors": ["Qilong Xing", "Zikai Song", "Yuteng Ye", "Yuke Chen", "Youjia Zhang", "Na Feng", "Junqing Yu", "Wei Yang"], "title": "CA-Diff: Collaborative Anatomy Diffusion for Brain Tissue Segmentation", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "ICME 2025", "summary": "Segmentation of brain structures from MRI is crucial for evaluating brain morphology, yet existing CNN and transformer-based methods struggle to delineate complex structures accurately. While current diffusion models have shown promise in image segmentation, they are inadequate when applied directly to brain MRI due to neglecting anatomical information. To address this, we propose Collaborative Anatomy Diffusion (CA-Diff), a framework integrating spatial anatomical features to enhance segmentation accuracy of the diffusion model. Specifically, we introduce distance field as an auxiliary anatomical condition to provide global spatial context, alongside a collaborative diffusion process to model its joint distribution with anatomical structures, enabling effective utilization of anatomical features for segmentation. Furthermore, we introduce a consistency loss to refine relationships between the distance field and anatomical structures and design a time adapted channel attention module to enhance the U-Net feature fusion procedure. Extensive experiments show that CA-Diff outperforms state-of-the-art (SOTA) methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89e3\u5256\u7279\u5f81\u7684\u6269\u6563\u6a21\u578bCA-Diff\uff0c\u7528\u4e8e\u63d0\u5347\u8111MRI\u5206\u5272\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709CNN\u548cTransformer\u65b9\u6cd5\u5728\u590d\u6742\u8111\u7ed3\u6784\u5206\u5272\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u6269\u6563\u6a21\u578b\u867d\u5728\u56fe\u50cf\u5206\u5272\u4e2d\u6709\u6f5c\u529b\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u4e8e\u8111MRI\u65f6\u5ffd\u7565\u4e86\u89e3\u5256\u4fe1\u606f\u3002", "method": "\u63d0\u51faCA-Diff\u6846\u67b6\uff0c\u5f15\u5165\u8ddd\u79bb\u573a\u4f5c\u4e3a\u89e3\u5256\u6761\u4ef6\uff0c\u7ed3\u5408\u534f\u4f5c\u6269\u6563\u8fc7\u7a0b\u5efa\u6a21\u5176\u8054\u5408\u5206\u5e03\uff0c\u5e76\u8bbe\u8ba1\u4e00\u81f4\u6027\u635f\u5931\u548c\u65f6\u95f4\u9002\u5e94\u901a\u9053\u6ce8\u610f\u529b\u6a21\u5757\u4f18\u5316\u7279\u5f81\u878d\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCA-Diff\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\u3002", "conclusion": "CA-Diff\u901a\u8fc7\u6709\u6548\u5229\u7528\u89e3\u5256\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8111MRI\u5206\u5272\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2506.22952", "pdf": "https://arxiv.org/pdf/2506.22952", "abs": "https://arxiv.org/abs/2506.22952", "authors": ["Yanwu Yang", "Thomas Wolfers"], "title": "Hierarchical Characterization of Brain Dynamics via State Space-based Vector Quantization", "categories": ["eess.IV", "cs.CV", "q-bio.NC"], "comment": null, "summary": "Understanding brain dynamics through functional Magnetic Resonance Imaging (fMRI) remains a fundamental challenge in neuroscience, particularly in capturing how the brain transitions between various functional states. Recently, metastability, which refers to temporarily stable brain states, has offered a promising paradigm to quantify complex brain signals into interpretable, discretized representations. In particular, compared to cluster-based machine learning approaches, tokenization approaches leveraging vector quantization have shown promise in representation learning with powerful reconstruction and predictive capabilities. However, most existing methods ignore brain transition dependencies and lack a quantification of brain dynamics into representative and stable embeddings. In this study, we propose a Hierarchical State space-based Tokenization network, termed HST, which quantizes brain states and transitions in a hierarchical structure based on a state space-based model. We introduce a refined clustered Vector-Quantization Variational AutoEncoder (VQ-VAE) that incorporates quantization error feedback and clustering to improve quantization performance while facilitating metastability with representative and stable token representations. We validate our HST on two public fMRI datasets, demonstrating its effectiveness in quantifying the hierarchical dynamics of the brain and its potential in disease diagnosis and reconstruction performance. Our method offers a promising framework for the characterization of brain dynamics, facilitating the analysis of metastability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHST\u7684\u5206\u5c42\u72b6\u6001\u7a7a\u95f4\u6807\u8bb0\u5316\u7f51\u7edc\uff0c\u7528\u4e8e\u91cf\u5316\u5927\u8111\u72b6\u6001\u548c\u8f6c\u6362\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u7684VQ-VAE\u63d0\u5347\u6807\u8bb0\u5316\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u75be\u75c5\u8bca\u65ad\u548c\u91cd\u5efa\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u7406\u89e3\u5927\u8111\u52a8\u6001\u529f\u80fd\u72b6\u6001\u8f6c\u6362\u662f\u795e\u7ecf\u79d1\u5b66\u7684\u91cd\u8981\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5e38\u5ffd\u7565\u8f6c\u6362\u4f9d\u8d56\u6027\u548c\u7a33\u5b9a\u5d4c\u5165\u91cf\u5316\u3002", "method": "\u63d0\u51faHST\u7f51\u7edc\uff0c\u7ed3\u5408\u5206\u5c42\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u548c\u4f18\u5316\u7684VQ-VAE\uff0c\u5f15\u5165\u91cf\u5316\u8bef\u5dee\u53cd\u9988\u548c\u805a\u7c7b\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00fMRI\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86HST\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u75be\u75c5\u8bca\u65ad\u548c\u91cd\u5efa\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "HST\u4e3a\u5927\u8111\u52a8\u6001\u548c\u7a33\u5b9a\u6027\u7684\u91cf\u5316\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u6846\u67b6\u3002"}}
{"id": "2506.23184", "pdf": "https://arxiv.org/pdf/2506.23184", "abs": "https://arxiv.org/abs/2506.23184", "authors": ["Anran Liu", "Xiaofei Wang", "Jing Cai", "Chao Li"], "title": "Score-based Diffusion Model for Unpaired Virtual Histology Staining", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "11 pages, 3 figures", "summary": "Hematoxylin and eosin (H&E) staining visualizes histology but lacks specificity for diagnostic markers. Immunohistochemistry (IHC) staining provides protein-targeted staining but is restricted by tissue availability and antibody specificity. Virtual staining, i.e., computationally translating the H&E image to its IHC counterpart while preserving the tissue structure, is promising for efficient IHC generation. Existing virtual staining methods still face key challenges: 1) effective decomposition of staining style and tissue structure, 2) controllable staining process adaptable to diverse tissue and proteins, and 3) rigorous structural consistency modelling to handle the non-pixel-aligned nature of paired H&E and IHC images. This study proposes a mutual-information (MI)-guided score-based diffusion model for unpaired virtual staining. Specifically, we design 1) a global MI-guided energy function that disentangles the tissue structure and staining characteristics across modalities, 2) a novel timestep-customized reverse diffusion process for precise control of the staining intensity and structural reconstruction, and 3) a local MI-driven contrastive learning strategy to ensure the cellular level structural consistency between H&E-IHC images. Extensive experiments demonstrate the our superiority over state-of-the-art approaches, highlighting its biomedical potential. Codes will be open-sourced upon acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e92\u4fe1\u606f\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u4eceH&E\u56fe\u50cf\u865a\u62df\u751f\u6210IHC\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5206\u89e3\u67d3\u8272\u98ce\u683c\u4e0e\u7ec4\u7ec7\u7ed3\u6784\u3001\u53ef\u63a7\u67d3\u8272\u8fc7\u7a0b\u53ca\u7ed3\u6784\u4e00\u81f4\u6027\u5efa\u6a21\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "H&E\u67d3\u8272\u7f3a\u4e4f\u7279\u5f02\u6027\u6807\u8bb0\uff0c\u800cIHC\u67d3\u8272\u53d7\u9650\u4e8e\u7ec4\u7ec7\u53ef\u7528\u6027\u548c\u6297\u4f53\u7279\u5f02\u6027\u3002\u865a\u62df\u67d3\u8272\u6280\u672f\u6709\u671b\u9ad8\u6548\u751f\u6210IHC\u56fe\u50cf\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5206\u89e3\u67d3\u8272\u98ce\u683c\u3001\u53ef\u63a7\u6027\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u4e92\u4fe1\u606f\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\uff0c\u8bbe\u8ba1\u4e86\u5168\u5c40\u4e92\u4fe1\u606f\u80fd\u91cf\u51fd\u6570\u3001\u65f6\u95f4\u6b65\u5b9a\u5236\u7684\u53cd\u5411\u6269\u6563\u8fc7\u7a0b\u53ca\u5c40\u90e8\u4e92\u4fe1\u606f\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff0c\u4ee5\u5b9e\u73b0\u67d3\u8272\u98ce\u683c\u4e0e\u7ec4\u7ec7\u7ed3\u6784\u7684\u89e3\u8026\u3001\u53ef\u63a7\u67d3\u8272\u53ca\u7ec6\u80de\u7ea7\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u865a\u62df\u67d3\u8272\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5c55\u793a\u4e86\u5176\u751f\u7269\u533b\u5b66\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u865a\u62df\u67d3\u8272\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u63a7\u4e14\u7ed3\u6784\u4e00\u81f4\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5c06\u5728\u63a5\u53d7\u540e\u5f00\u6e90\u3002"}}
{"id": "2506.23221", "pdf": "https://arxiv.org/pdf/2506.23221", "abs": "https://arxiv.org/abs/2506.23221", "authors": ["B\u00e1lint Horv\u00e1th", "Bal\u00e1zs Csan\u00e1d Cs\u00e1ji"], "title": "Single Image Inpainting and Super-Resolution with Simultaneous Uncertainty Guarantees by Universal Reproducing Kernels", "categories": ["cs.LG", "cs.CV"], "comment": "23 pages, 8 figures, 6 tables", "summary": "The paper proposes a statistical learning approach to the problem of estimating missing pixels of images, crucial for image inpainting and super-resolution problems. One of the main novelties of the method is that it also provides uncertainty quantifications together with the estimated values. Our core assumption is that the underlying data-generating function comes from a Reproducing Kernel Hilbert Space (RKHS). A special emphasis is put on band-limited functions, central to signal processing, which form Paley-Wiener type RKHSs. The proposed method, which we call Simultaneously Guaranteed Kernel Interpolation (SGKI), is an extension and refinement of a recently developed kernel method. An advantage of SGKI is that it not only estimates the missing pixels, but also builds non-asymptotic confidence bands for the unobserved values, which are simultaneously guaranteed for all missing pixels. We also show how to compute these bands efficiently using Schur complements, we discuss a generalization to vector-valued functions, and we present a series of numerical experiments on various datasets containing synthetically generated and benchmark images, as well.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u8ba1\u5b66\u4e60\u65b9\u6cd5SGKI\uff0c\u7528\u4e8e\u56fe\u50cf\u4fee\u590d\u548c\u8d85\u5206\u8fa8\u7387\u95ee\u9898\u4e2d\u7684\u7f3a\u5931\u50cf\u7d20\u4f30\u8ba1\uff0c\u5e76\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "motivation": "\u89e3\u51b3\u56fe\u50cf\u4fee\u590d\u548c\u8d85\u5206\u8fa8\u7387\u4e2d\u7f3a\u5931\u50cf\u7d20\u4f30\u8ba1\u95ee\u9898\uff0c\u5e76\u91cf\u5316\u4f30\u8ba1\u503c\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u57fa\u4e8e\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\uff08RKHS\uff09\u5047\u8bbe\uff0c\u63d0\u51faSGKI\u65b9\u6cd5\uff0c\u6269\u5c55\u5e76\u6539\u8fdb\u4e86\u73b0\u6709\u6838\u65b9\u6cd5\uff0c\u5229\u7528Schur\u8865\u9ad8\u6548\u8ba1\u7b97\u975e\u6e10\u8fd1\u7f6e\u4fe1\u5e26\u3002", "result": "SGKI\u4e0d\u4ec5\u80fd\u4f30\u8ba1\u7f3a\u5931\u50cf\u7d20\uff0c\u8fd8\u80fd\u4e3a\u6240\u6709\u7f3a\u5931\u50cf\u7d20\u6784\u5efa\u540c\u65f6\u4fdd\u8bc1\u7684\u975e\u6e10\u8fd1\u7f6e\u4fe1\u5e26\uff0c\u5e76\u5728\u5408\u6210\u548c\u57fa\u51c6\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "SGKI\u65b9\u6cd5\u5728\u56fe\u50cf\u4fee\u590d\u548c\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u517c\u5177\u4f30\u8ba1\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b\u3002"}}
{"id": "2506.23309", "pdf": "https://arxiv.org/pdf/2506.23309", "abs": "https://arxiv.org/abs/2506.23309", "authors": ["Yiming Huang", "Long Bai", "Beilei Cui", "Kun Yuan", "Guankun Wang", "Mobarakol Islam", "Nicolas Padoy", "Nassir Navab", "Hongliang Ren"], "title": "SurgTPGS: Semantic 3D Surgical Scene Understanding with Text Promptable Gaussian Splatting", "categories": ["eess.IV", "cs.CV"], "comment": "MICCAI 2025. Project Page:   https://lastbasket.github.io/MICCAI-2025-SurgTPGS/", "summary": "In contemporary surgical research and practice, accurately comprehending 3D surgical scenes with text-promptable capabilities is particularly crucial for surgical planning and real-time intra-operative guidance, where precisely identifying and interacting with surgical tools and anatomical structures is paramount. However, existing works focus on surgical vision-language model (VLM), 3D reconstruction, and segmentation separately, lacking support for real-time text-promptable 3D queries. In this paper, we present SurgTPGS, a novel text-promptable Gaussian Splatting method to fill this gap. We introduce a 3D semantics feature learning strategy incorporating the Segment Anything model and state-of-the-art vision-language models. We extract the segmented language features for 3D surgical scene reconstruction, enabling a more in-depth understanding of the complex surgical environment. We also propose semantic-aware deformation tracking to capture the seamless deformation of semantic features, providing a more precise reconstruction for both texture and semantic features. Furthermore, we present semantic region-aware optimization, which utilizes regional-based semantic information to supervise the training, particularly promoting the reconstruction quality and semantic smoothness. We conduct comprehensive experiments on two real-world surgical datasets to demonstrate the superiority of SurgTPGS over state-of-the-art methods, highlighting its potential to revolutionize surgical practices. SurgTPGS paves the way for developing next-generation intelligent surgical systems by enhancing surgical precision and safety. Our code is available at: https://github.com/lastbasket/SurgTPGS.", "AI": {"tldr": "SurgTPGS\u662f\u4e00\u79cd\u65b0\u578b\u7684\u6587\u672c\u63d0\u793a\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9e\u65f63D\u624b\u672f\u573a\u666f\u67e5\u8be2\uff0c\u7ed3\u5408\u4e86\u8bed\u4e49\u7279\u5f81\u5b66\u4e60\u548c\u53d8\u5f62\u8ddf\u8e2a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u624b\u672f\u573a\u666f\u7684\u91cd\u5efa\u8d28\u91cf\u548c\u8bed\u4e49\u7406\u89e3\u3002", "motivation": "\u5f53\u524d\u624b\u672f\u7814\u7a76\u4e2d\uff0c\u7f3a\u4e4f\u652f\u6301\u5b9e\u65f6\u6587\u672c\u63d0\u793a\u76843D\u67e5\u8be2\u65b9\u6cd5\uff0c\u800c\u51c6\u786e\u7406\u89e33D\u624b\u672f\u573a\u666f\u5bf9\u624b\u672f\u89c4\u5212\u548c\u5b9e\u65f6\u5f15\u5bfc\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7ed3\u5408Segment Anything\u6a21\u578b\u548c\u5148\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u51fa3D\u8bed\u4e49\u7279\u5f81\u5b66\u4e60\u7b56\u7565\u3001\u8bed\u4e49\u611f\u77e5\u53d8\u5f62\u8ddf\u8e2a\u548c\u8bed\u4e49\u533a\u57df\u611f\u77e5\u4f18\u5316\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u624b\u672f\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSurgTPGS\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u8bed\u4e49\u5e73\u6ed1\u6027\u3002", "conclusion": "SurgTPGS\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u624b\u672f\u7cfb\u7edf\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u9ad8\u4e86\u624b\u672f\u7cbe\u5ea6\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2506.23466", "pdf": "https://arxiv.org/pdf/2506.23466", "abs": "https://arxiv.org/abs/2506.23466", "authors": ["Qiqing Liu", "Guoquan Wei", "Zekun Zhou", "Yiyang Wen", "Liu Shi", "Qiegen Liu"], "title": "FD-DiT: Frequency Domain-Directed Diffusion Transformer for Low-Dose CT Reconstruction", "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "comment": "11pages, 11 figures", "summary": "Low-dose computed tomography (LDCT) reduces radiation exposure but suffers from image artifacts and loss of detail due to quantum and electronic noise, potentially impacting diagnostic accuracy. Transformer combined with diffusion models has been a promising approach for image generation. Nevertheless, existing methods exhibit limitations in preserving finegrained image details. To address this issue, frequency domain-directed diffusion transformer (FD-DiT) is proposed for LDCT reconstruction. FD-DiT centers on a diffusion strategy that progressively introduces noise until the distribution statistically aligns with that of LDCT data, followed by denoising processing. Furthermore, we employ a frequency decoupling technique to concentrate noise primarily in high-frequency domain, thereby facilitating effective capture of essential anatomical structures and fine details. A hybrid denoising network is then utilized to optimize the overall data reconstruction process. To enhance the capability in recognizing high-frequency noise, we incorporate sliding sparse local attention to leverage the sparsity and locality of shallow-layer information, propagating them via skip connections for improving feature representation. Finally, we propose a learnable dynamic fusion strategy for optimal component integration. Experimental results demonstrate that at identical dose levels, LDCT images reconstructed by FD-DiT exhibit superior noise and artifact suppression compared to state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9891\u7387\u57df\u5bfc\u5411\u6269\u6563\u53d8\u6362\u5668\uff08FD-DiT\uff09\u7684\u4f4e\u5242\u91cfCT\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u9891\u7387\u89e3\u8026\u548c\u6df7\u5408\u53bb\u566a\u7f51\u7edc\u4f18\u5316\u56fe\u50cf\u7ec6\u8282\u3002", "motivation": "\u4f4e\u5242\u91cfCT\uff08LDCT\uff09\u867d\u51cf\u5c11\u8f90\u5c04\uff0c\u4f46\u56fe\u50cf\u566a\u58f0\u548c\u4f2a\u5f71\u5f71\u54cd\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u7559\u7ec6\u8282\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "FD-DiT\u91c7\u7528\u6269\u6563\u7b56\u7565\u9010\u6b65\u5f15\u5165\u566a\u58f0\uff0c\u7ed3\u5408\u9891\u7387\u89e3\u8026\u6280\u672f\u548c\u9ad8\u9891\u566a\u58f0\u8bc6\u522b\u65b9\u6cd5\uff0c\u4f7f\u7528\u6df7\u5408\u53bb\u566a\u7f51\u7edc\u548c\u52a8\u6001\u878d\u5408\u7b56\u7565\u4f18\u5316\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFD-DiT\u5728\u76f8\u540c\u5242\u91cf\u4e0b\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u597d\u5730\u6291\u5236\u566a\u58f0\u548c\u4f2a\u5f71\u3002", "conclusion": "FD-DiT\u901a\u8fc7\u9891\u7387\u57df\u548c\u52a8\u6001\u878d\u5408\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86LDCT\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2506.23537", "pdf": "https://arxiv.org/pdf/2506.23537", "abs": "https://arxiv.org/abs/2506.23537", "authors": ["Xinyue Li", "Zhangkai Ni", "Wenhan Yang"], "title": "AFUNet: Cross-Iterative Alignment-Fusion Synergy for HDR Reconstruction via Deep Unfolding Paradigm", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted to International Conference on Computer Vision (ICCV) 2025", "summary": "Existing learning-based methods effectively reconstruct HDR images from multi-exposure LDR inputs with extended dynamic range and improved detail, but they rely more on empirical design rather than theoretical foundation, which can impact their reliability. To address these limitations, we propose the cross-iterative Alignment and Fusion deep Unfolding Network (AFUNet), where HDR reconstruction is systematically decoupled into two interleaved subtasks -- alignment and fusion -- optimized through alternating refinement, achieving synergy between the two subtasks to enhance the overall performance. Our method formulates multi-exposure HDR reconstruction from a Maximum A Posteriori (MAP) estimation perspective, explicitly incorporating spatial correspondence priors across LDR images and naturally bridging the alignment and fusion subproblems through joint constraints. Building on the mathematical foundation, we reimagine traditional iterative optimization through unfolding -- transforming the conventional solution process into an end-to-end trainable AFUNet with carefully designed modules that work progressively. Specifically, each iteration of AFUNet incorporates an Alignment-Fusion Module (AFM) that alternates between a Spatial Alignment Module (SAM) for alignment and a Channel Fusion Module (CFM) for adaptive feature fusion, progressively bridging misaligned content and exposure discrepancies. Extensive qualitative and quantitative evaluations demonstrate AFUNet's superior performance, consistently surpassing state-of-the-art methods. Our code is available at: https://github.com/eezkni/AFUNet", "AI": {"tldr": "AFUNet\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u7684\u5bf9\u9f50\u4e0e\u878d\u5408\u5b50\u4efb\u52a1\uff0c\u4eceMAP\u4f30\u8ba1\u89d2\u5ea6\u91cd\u6784HDR\u56fe\u50cf\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u4f9d\u8d56\u7ecf\u9a8c\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u7406\u8bba\u652f\u6301\uff0c\u5f71\u54cd\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faAFUNet\uff0c\u5c06HDR\u91cd\u6784\u5206\u89e3\u4e3a\u5bf9\u9f50\u4e0e\u878d\u5408\u5b50\u4efb\u52a1\uff0c\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u5b9e\u73b0\u534f\u540c\u3002\u57fa\u4e8eMAP\u4f30\u8ba1\uff0c\u7ed3\u5408\u7a7a\u95f4\u5bf9\u5e94\u5148\u9a8c\uff0c\u8bbe\u8ba1\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3\u7684AFUNet\u3002", "result": "AFUNet\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AFUNet\u901a\u8fc7\u7406\u8bba\u9a71\u52a8\u7684\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86HDR\u56fe\u50cf\u91cd\u6784\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2506.23664", "pdf": "https://arxiv.org/pdf/2506.23664", "abs": "https://arxiv.org/abs/2506.23664", "authors": ["Fangyijie Wang", "Kevin Whelan", "F\u00e9lix Balado", "Gu\u00e9nol\u00e9 Silvestre", "Kathleen M. Curran"], "title": "Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical image data is less accessible than in other domains due to privacy and regulatory constraints. In addition, labeling requires costly, time-intensive manual image annotation by clinical experts. To overcome these challenges, synthetic medical data generation offers a promising solution. Generative AI (GenAI), employing generative deep learning models, has proven effective at producing realistic synthetic images. This study proposes a novel mask-guided GenAI approach using diffusion models to generate synthetic fetal head ultrasound images paired with segmentation masks. These synthetic pairs augment real datasets for supervised fine-tuning of the Segment Anything Model (SAM). Our results show that the synthetic data captures real image features effectively, and this approach reaches state-of-the-art fetal head segmentation, especially when trained with a limited number of real image-mask pairs. In particular, the segmentation reaches Dice Scores of 94.66\\% and 94.38\\% using a handful of ultrasound images from the Spanish and African cohorts, respectively. Our code, models, and data are available on GitHub.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u63a9\u7801\u5f15\u5bfc\u751f\u6210AI\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u5408\u6210\u80ce\u513f\u5934\u90e8\u8d85\u58f0\u56fe\u50cf\u53ca\u5176\u5206\u5272\u63a9\u7801\uff0c\u4ee5\u589e\u5f3a\u771f\u5b9e\u6570\u636e\u96c6\uff0c\u63d0\u5347\u5206\u5272\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u6570\u636e\u56e0\u9690\u79c1\u548c\u76d1\u7ba1\u9650\u5236\u96be\u4ee5\u83b7\u53d6\uff0c\u4e14\u6807\u6ce8\u6210\u672c\u9ad8\u3002\u5408\u6210\u6570\u636e\u751f\u6210\u662f\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u7684\u6709\u6548\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u5408\u6210\u80ce\u513f\u5934\u90e8\u8d85\u58f0\u56fe\u50cf\u53ca\u5176\u5206\u5272\u63a9\u7801\uff0c\u7528\u4e8e\u76d1\u7763\u5fae\u8c03Segment Anything Model (SAM)\u3002", "result": "\u5408\u6210\u6570\u636e\u80fd\u6709\u6548\u6355\u6349\u771f\u5b9e\u56fe\u50cf\u7279\u5f81\uff0c\u4f7f\u7528\u5c11\u91cf\u771f\u5b9e\u56fe\u50cf\u5bf9\u5373\u53ef\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u80ce\u513f\u5934\u90e8\u5206\u5272\u6548\u679c\uff08Dice\u5206\u6570\u5206\u522b\u4e3a94.66%\u548c94.38%\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u589e\u5f3a\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u771f\u5b9e\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2506.23731", "pdf": "https://arxiv.org/pdf/2506.23731", "abs": "https://arxiv.org/abs/2506.23731", "authors": ["Michel Meintz", "Jan Dubi\u0144ski", "Franziska Boenisch", "Adam Dziedzic"], "title": "Radioactive Watermarks in Diffusion and Autoregressive Image Generative Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Image generative models have become increasingly popular, but training them requires large datasets that are costly to collect and curate. To circumvent these costs, some parties may exploit existing models by using the generated images as training data for their own models. In general, watermarking is a valuable tool for detecting unauthorized use of generated images. However, when these images are used to train a new model, watermarking can only enable detection if the watermark persists through training and remains identifiable in the outputs of the newly trained model - a property known as radioactivity. We analyze the radioactivity of watermarks in images generated by diffusion models (DMs) and image autoregressive models (IARs). We find that existing watermarking methods for DMs fail to retain radioactivity, as watermarks are either erased during encoding into the latent space or lost in the noising-denoising process (during the training in the latent space). Meanwhile, despite IARs having recently surpassed DMs in image generation quality and efficiency, no radioactive watermarking methods have been proposed for them. To overcome this limitation, we propose the first watermarking method tailored for IARs and with radioactivity in mind - drawing inspiration from techniques in large language models (LLMs), which share IARs' autoregressive paradigm. Our extensive experimental evaluation highlights our method's effectiveness in preserving radioactivity within IARs, enabling robust provenance tracking, and preventing unauthorized use of their generated images.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u6269\u6563\u6a21\u578b\u548c\u81ea\u56de\u5f52\u56fe\u50cf\u6a21\u578b\u4e2d\u6c34\u5370\u7684\u653e\u5c04\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u6269\u6563\u6a21\u578b\u4e2d\u5931\u6548\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u81ea\u56de\u5f52\u6a21\u578b\u7684\u65b0\u578b\u653e\u5c04\u6027\u6c34\u5370\u65b9\u6cd5\u3002", "motivation": "\u8bad\u7ec3\u56fe\u50cf\u751f\u6210\u6a21\u578b\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u6210\u672c\u9ad8\u6602\u3002\u4e3a\u9632\u6b62\u672a\u7ecf\u6388\u6743\u4f7f\u7528\u751f\u6210\u56fe\u50cf\u8bad\u7ec3\u65b0\u6a21\u578b\uff0c\u9700\u8981\u6c34\u5370\u5177\u6709\u653e\u5c04\u6027\uff08\u5373\u6c34\u5370\u5728\u65b0\u6a21\u578b\u4e2d\u4ecd\u53ef\u8bc6\u522b\uff09\u3002", "method": "\u5206\u6790\u4e86\u6269\u6563\u6a21\u578b\u548c\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6c34\u5370\u653e\u5c04\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u81ea\u56de\u5f52\u6a21\u578b\u7684\u65b0\u578b\u6c34\u5370\u65b9\u6cd5\uff0c\u7075\u611f\u6765\u81ea\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u6c34\u5370\u65b9\u6cd5\u653e\u5c04\u6027\u4e0d\u8db3\uff0c\u800c\u63d0\u51fa\u7684\u81ea\u56de\u5f52\u6a21\u578b\u6c34\u5370\u65b9\u6cd5\u80fd\u6709\u6548\u4fdd\u7559\u653e\u5c04\u6027\uff0c\u652f\u6301\u6eaf\u6e90\u548c\u9632\u6b62\u672a\u7ecf\u6388\u6743\u4f7f\u7528\u3002", "conclusion": "\u81ea\u56de\u5f52\u6a21\u578b\u7684\u65b0\u578b\u6c34\u5370\u65b9\u6cd5\u89e3\u51b3\u4e86\u653e\u5c04\u6027\u95ee\u9898\uff0c\u4e3a\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u7248\u6743\u4fdd\u62a4\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
