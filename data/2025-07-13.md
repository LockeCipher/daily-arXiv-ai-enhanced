<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 6]
- [cs.CV](#cs.CV) [Total: 23]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Generative Panoramic Image Stitching](https://arxiv.org/abs/2507.07133)
*Mathieu Tuli,Kaveh Kamali,David B. Lindell*

Main category: cs.GR

TL;DR: 提出了一种基于扩散模型的生成式全景图像拼接方法，解决了传统方法和现有生成模型在多参考图像条件下的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统图像拼接方法在多参考图像（存在视差、光照或风格差异）下会产生伪影，而现有生成模型难以合成大范围连贯的全景图。

Method: 通过微调基于扩散模型的修复模型，使其能够根据多参考图像保留场景内容和布局，并从一个参考图像生成无缝的全景图。

Result: 在图像质量和场景布局一致性上显著优于基线方法。

Conclusion: 该方法能够生成无缝且视觉连贯的全景图，忠实整合多参考图像内容。

Abstract: We introduce the task of generative panoramic image stitching, which aims to synthesize seamless panoramas that are faithful to the content of multiple reference images containing parallax effects and strong variations in lighting, camera capture settings, or style. In this challenging setting, traditional image stitching pipelines fail, producing outputs with ghosting and other artifacts. While recent generative models are capable of outpainting content consistent with multiple reference images, they fail when tasked with synthesizing large, coherent regions of a panorama. To address these limitations, we propose a method that fine-tunes a diffusion-based inpainting model to preserve a scene's content and layout based on multiple reference images. Once fine-tuned, the model outpaints a full panorama from a single reference image, producing a seamless and visually coherent result that faithfully integrates content from all reference images. Our approach significantly outperforms baselines for this task in terms of image quality and the consistency of image structure and scene layout when evaluated on captured datasets.

</details>


### [2] [LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+ FPS](https://arxiv.org/abs/2507.07136)
*Wanhua Li,Yujie Zhao,Minghan Qin,Yang Liu,Yuanhao Cai,Chuang Gan,Hanspeter Pfister*

Main category: cs.GR

TL;DR: LangSplatV2通过稀疏编码和CUDA优化，显著提升了3D语言场的高维特征渲染速度，实现了476.2 FPS的高性能，比LangSplat快42倍，同时提高了查询精度。


<details>
  <summary>Details</summary>
Motivation: LangSplat在复杂场景中的语言交互应用潜力巨大，但其8.2 FPS的推理速度限制了实际应用。LangSplatV2旨在解决这一瓶颈。

Method: 通过将每个高斯视为全局字典中的稀疏编码，学习3D稀疏系数场，无需重量级解码器，并结合CUDA优化的稀疏系数渲染方法。

Result: LangSplatV2实现了476.2 FPS的高维特征渲染和384.6 FPS的3D开放词汇查询，速度提升42倍，查询精度更高。

Conclusion: LangSplatV2在速度和精度上均显著优于LangSplat，为3D语言场的高效应用提供了可行方案。

Abstract: In this paper, we introduce LangSplatV2, which achieves high-dimensional feature splatting at 476.2 FPS and 3D open-vocabulary text querying at 384.6 FPS for high-resolution images, providing a 42 $\times$ speedup and a 47 $\times$ boost over LangSplat respectively, along with improved query accuracy. LangSplat employs Gaussian Splatting to embed 2D CLIP language features into 3D, significantly enhancing speed and learning a precise 3D language field with SAM semantics. Such advancements in 3D language fields are crucial for applications that require language interaction within complex scenes. However, LangSplat does not yet achieve real-time inference performance (8.2 FPS), even with advanced A100 GPUs, severely limiting its broader application. In this paper, we first conduct a detailed time analysis of LangSplat, identifying the heavyweight decoder as the primary speed bottleneck. Our solution, LangSplatV2 assumes that each Gaussian acts as a sparse code within a global dictionary, leading to the learning of a 3D sparse coefficient field that entirely eliminates the need for a heavyweight decoder. By leveraging this sparsity, we further propose an efficient sparse coefficient splatting method with CUDA optimization, rendering high-dimensional feature maps at high quality while incurring only the time cost of splatting an ultra-low-dimensional feature. Our experimental results demonstrate that LangSplatV2 not only achieves better or competitive query accuracy but is also significantly faster. Codes and demos are available at our project page: https://langsplat-v2.github.io.

</details>


### [3] [Digital Salon: An AI and Physics-Driven Tool for 3D Hair Grooming and Simulation](https://arxiv.org/abs/2507.07387)
*Chengan He,Jorge Alejandro Amador Herrera,Zhixin Shu,Xin Sun,Yao Feng,Sören Pirk,Dominik L. Michels,Meng Zhang,Tuanfeng Y. Wang,Julie Dorsey,Holly Rushmeier,Yi Zhou*

Main category: cs.GR

TL;DR: Digital Salon是一个支持实时3D头发生成、模拟和渲染的综合系统，通过自然语言交互降低技术门槛。


<details>
  <summary>Details</summary>
Motivation: 现有方法专注于3D头发建模的孤立部分，计算复杂或需要网络训练，Digital Salon旨在提供更全面、交互式的解决方案。

Method: 系统分为四个阶段：文本引导的头发检索、实时头发模拟、交互式头发细化、以及头发条件图像生成。

Result: 用户研究表明，该系统在快速原型设计方面优于传统头发建模流程。

Conclusion: Digital Salon为数字媒体中的头发建模提供了直观、多功能且高效的解决方案，并展示了在真实沙龙环境中部署的潜力。

Abstract: We introduce Digital Salon, a comprehensive hair authoring system that supports real-time 3D hair generation, simulation, and rendering. Unlike existing methods that focus on isolated parts of 3D hair modeling and involve a heavy computation process or network training, Digital Salon offers a holistic and interactive system that lowers the technical barriers of 3D hair modeling through natural language-based interaction. The system guides users through four key stages: text-guided hair retrieval, real-time hair simulation, interactive hair refinement, and hair-conditioned image generation. This cohesive workflow makes advanced hair design accessible to users of varying skill levels and dramatically streamlines the creative process in digital media with an intuitive, versatile, and efficient solution for hair modeling. User studies show that our system can outperform traditional hair modeling workflows for rapid prototyping. Furthermore, we provide insights into the benefits of our system with future potential of deploying our system in real salon environments. More details can be found on our project page: https://digital-salon.github.io/.

</details>


### [4] [SD-GS: Structured Deformable 3D Gaussians for Efficient Dynamic Scene Reconstruction](https://arxiv.org/abs/2507.07465)
*Wei Yao,Shuzhao Xie,Letian Li,Weixiang Zhang,Zhixin Lai,Shiqi Dai,Ke Zhang,Zhi Wang*

Main category: cs.GR

TL;DR: SD-GS是一种紧凑高效的动态高斯泼溅框架，通过可变形锚点网格和变形感知密集化策略，显著减少模型大小并提升渲染速度。


<details>
  <summary>Details</summary>
Motivation: 现有4D高斯框架在动态场景重建中存在存储成本与复杂运动建模能力之间的权衡问题，限制了实际应用。

Method: 提出可变形锚点网格作为层次化、内存高效的场景表示，并引入变形感知密集化策略，动态调整锚点分布。

Result: 实验显示，SD-GS模型大小平均减少60%，FPS提升100%，同时保持或超越视觉质量。

Conclusion: SD-GS在计算效率和视觉质量上均优于现有方法，适用于复杂动态场景重建。

Abstract: Current 4D Gaussian frameworks for dynamic scene reconstruction deliver impressive visual fidelity and rendering speed, however, the inherent trade-off between storage costs and the ability to characterize complex physical motions significantly limits the practical application of these methods. To tackle these problems, we propose SD-GS, a compact and efficient dynamic Gaussian splatting framework for complex dynamic scene reconstruction, featuring two key contributions. First, we introduce a deformable anchor grid, a hierarchical and memory-efficient scene representation where each anchor point derives multiple 3D Gaussians in its local spatiotemporal region and serves as the geometric backbone of the 3D scene. Second, to enhance modeling capability for complex motions, we present a deformation-aware densification strategy that adaptively grows anchors in under-reconstructed high-dynamic regions while reducing redundancy in static areas, achieving superior visual quality with fewer anchors. Experimental results demonstrate that, compared to state-of-the-art methods, SD-GS achieves an average of 60\% reduction in model size and an average of 100\% improvement in FPS, significantly enhancing computational efficiency while maintaining or even surpassing visual quality.

</details>


### [5] [Capture Stage Environments: A Guide to Better Matting](https://arxiv.org/abs/2507.07623)
*Hannah Dröge,Janelle Pfeifer,Saskia Rabich,Markus Plack,Reinhard Klein,Matthias B. Hullin*

Main category: cs.GR

TL;DR: 论文探讨了高端捕捉舞台内容中图像抠图的挑战，并提出改进工作流程的指南和高效管道。


<details>
  <summary>Details</summary>
Motivation: 捕捉舞台内容的高端录制在电影、游戏等媒体中有广泛应用，但现有抠图算法对其特殊性问题表现不佳。

Method: 提出改进工作流程的指南，并展示无需大量标注的高效管道，适应离线与实时需求。

Result: 通过基于扩散模型的验证方法，证明了方法的有效性。

Conclusion: 为实践者提供了解决捕捉舞台内容抠图挑战的实用指南和高效解决方案。

Abstract: Capture stages are high-end sources of state-of-the-art recordings for downstream applications in movies, games, and other media. One crucial step in almost all pipelines is the matting of images to isolate the captured performances from the background. While common matting algorithms deliver remarkable performance in other applications like teleconferencing and mobile entertainment, we found that they struggle significantly with the peculiarities of capture stage content. The goal of our work is to share insights into those challenges as a curated list of those characteristics along with a constructive discussion for proactive intervention and present a guideline to practitioners for an improved workflow to mitigate unresolved challenges. To this end, we also demonstrate an efficient pipeline to adapt state-of-the-art approaches to such custom setups without the need of extensive annotations, both offline and real-time. For an objective evaluation, we propose a validation methodology based on a leading diffusion model that highlights the benefits of our approach.

</details>


### [6] [RTR-GS: 3D Gaussian Splatting for Inverse Rendering with Radiance Transfer and Reflection](https://arxiv.org/abs/2507.07733)
*Yongyang Zhou,Fang-Lue Zhang,Zichen Wang,Lei Zhang*

Main category: cs.GR

TL;DR: RTR-GS是一种新型逆渲染框架，通过混合渲染模型处理反射物体，有效分解BRDF和光照，提升新视角合成和重光照效果。


<details>
  <summary>Details</summary>
Motivation: 尽管3D高斯溅射（3DGS）在新视角合成中表现优异，但渲染反射物体仍具挑战性，尤其是在逆渲染和重光照方面。

Method: 结合前向渲染和延迟渲染的混合模型，分离高频和低频外观，并通过物理基础的延迟渲染分支优化BRDF和光照分解。

Result: 实验表明，该方法提升了新视角合成、法线估计、分解和重光照效果，同时保持高效的训练和推理过程。

Conclusion: RTR-GS框架在反射物体渲染方面表现出色，为逆渲染和重光照提供了可靠解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated impressive capabilities in novel view synthesis. However, rendering reflective objects remains a significant challenge, particularly in inverse rendering and relighting. We introduce RTR-GS, a novel inverse rendering framework capable of robustly rendering objects with arbitrary reflectance properties, decomposing BRDF and lighting, and delivering credible relighting results. Given a collection of multi-view images, our method effectively recovers geometric structure through a hybrid rendering model that combines forward rendering for radiance transfer with deferred rendering for reflections. This approach successfully separates high-frequency and low-frequency appearances, mitigating floating artifacts caused by spherical harmonic overfitting when handling high-frequency details. We further refine BRDF and lighting decomposition using an additional physically-based deferred rendering branch. Experimental results show that our method enhances novel view synthesis, normal estimation, decomposition, and relighting while maintaining efficient training inference process.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [7] [Image Can Bring Your Memory Back: A Novel Multi-Modal Guided Attack against Image Generation Model Unlearning](https://arxiv.org/abs/2507.07139)
*Renyang Liu,Guanlin Li,Tianwei Zhang,See-Kiong Ng*

Main category: cs.CV

TL;DR: 论文提出了一种名为Recall的对抗性框架，旨在测试图像生成模型（IGMs）在去学习（MU）后的鲁棒性，发现现有方法在多模态对抗输入下存在漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成模型（如Stable Diffusion）能力的提升，其可能生成有害或侵权内容的问题日益突出。去学习技术被提出以解决这一问题，但其鲁棒性尚未充分验证。

Method: Recall通过优化对抗性图像提示，利用扩散模型的多模态条件能力，测试现有去学习方法的鲁棒性。

Result: 实验表明，Recall在对抗效果、计算效率和语义保真度上均优于现有基线，揭示了当前去学习机制的脆弱性。

Conclusion: 研究强调了开发更鲁棒的去学习解决方案的必要性，以确保生成模型的安全性和可靠性。

Abstract: Recent advances in image generation models (IGMs), particularly diffusion-based architectures such as Stable Diffusion (SD), have markedly enhanced the quality and diversity of AI-generated visual content. However, their generative capability has also raised significant ethical, legal, and societal concerns, including the potential to produce harmful, misleading, or copyright-infringing content. To mitigate these concerns, machine unlearning (MU) emerges as a promising solution by selectively removing undesirable concepts from pretrained models. Nevertheless, the robustness and effectiveness of existing unlearning techniques remain largely unexplored, particularly in the presence of multi-modal adversarial inputs.   To bridge this gap, we propose Recall, a novel adversarial framework explicitly designed to compromise the robustness of unlearned IGMs. Unlike existing approaches that predominantly rely on adversarial text prompts, Recall exploits the intrinsic multi-modal conditioning capabilities of diffusion models by efficiently optimizing adversarial image prompts with guidance from a single semantically relevant reference image. Extensive experiments across ten state-of-the-art unlearning methods and diverse tasks show that Recall consistently outperforms existing baselines in terms of adversarial effectiveness, computational efficiency, and semantic fidelity with the original textual prompt. These findings reveal critical vulnerabilities in current unlearning mechanisms and underscore the need for more robust solutions to ensure the safety and reliability of generative models. Code and data are publicly available at \textcolor{blue}{https://github.com/ryliu68/RECALL}.

</details>


### [8] [Robust Multimodal Large Language Models Against Modality Conflict](https://arxiv.org/abs/2507.07151)
*Zongmeng Zhang,Wengang Zhou,Jie Zhao,Houqiang Li*

Main category: cs.CV

TL;DR: 该论文研究了多模态大语言模型（MLLMs）中的幻觉现象，从模态冲突的角度出发，提出了三种缓解方法，并在构建的数据集上验证了强化学习方法的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs在视觉语言任务中表现出色，但在实际场景中容易产生幻觉。论文旨在探索模态冲突如何直接导致幻觉现象。

Method: 论文定义了模态冲突，并构建了MMMC数据集。提出了基于提示工程、监督微调和强化学习的三种方法来缓解幻觉。

Result: 实验表明，强化学习方法在缓解模态冲突导致的幻觉中表现最佳，监督微调方法表现稳定且有潜力。

Conclusion: 论文揭示了模态冲突对幻觉的影响，为提升MLLMs的鲁棒性提供了新见解。

Abstract: Despite the impressive capabilities of multimodal large language models (MLLMs) in vision-language tasks, they are prone to hallucinations in real-world scenarios. This paper investigates the hallucination phenomenon in MLLMs from the perspective of modality conflict. Unlike existing works focusing on the conflicts between model responses and inputs, we study the inherent conflicts in inputs from different modalities that place MLLMs in a dilemma and directly lead to hallucinations. We formally define the modality conflict and construct a dataset named Multimodal Modality Conflict (MMMC) to simulate this phenomenon in vision-language tasks. Three methods based on prompt engineering, supervised fine-tuning, and reinforcement learning are proposed to alleviate the hallucination caused by modality conflict. Extensive experiments are conducted on the MMMC dataset to analyze the merits and demerits of these methods. Our results show that the reinforcement learning method achieves the best performance in mitigating the hallucination under modality conflict, while the supervised fine-tuning method shows promising and stable performance. Our work sheds light on the unnoticed modality conflict that leads to hallucinations and provides more insights into the robustness of MLLMs.

</details>


### [9] [Interpretable EEG-to-Image Generation with Semantic Prompts](https://arxiv.org/abs/2507.07157)
*Arshak Rezvani,Ali Akbari,Kosar Sanjar Arani,Maryam Mirian,Emad Arasteh,Martin J. McKeown*

Main category: cs.CV

TL;DR: 该论文提出了一种通过将EEG信号与多级语义描述对齐，间接生成图像的方法，结合对比学习和预训练的潜在扩散模型，实现了EEG信号的高效视觉解码。


<details>
  <summary>Details</summary>
Motivation: EEG信号在空间细节上的局限性阻碍了直接图像重建，因此需要一种间接方法，通过语义描述作为中介，实现更高效的视觉解码。

Method: 使用基于transformer的EEG编码器，通过对比学习将EEG信号映射到多级语义描述（由大型语言模型生成），并利用预训练的潜在扩散模型生成图像。

Result: 在EEGCVPR数据集上实现了最先进的视觉解码效果，揭示了EEG信号与语义描述之间的神经认知路径对齐。

Conclusion: 通过语义中介的结构化方法，能够实现与认知对齐的EEG视觉解码，为神经科学和可解释AI提供了新思路。

Abstract: Decoding visual experience from brain signals offers exciting possibilities for neuroscience and interpretable AI. While EEG is accessible and temporally precise, its limitations in spatial detail hinder image reconstruction. Our model bypasses direct EEG-to-image generation by aligning EEG signals with multilevel semantic captions -- ranging from object-level to abstract themes -- generated by a large language model. A transformer-based EEG encoder maps brain activity to these captions through contrastive learning. During inference, caption embeddings retrieved via projection heads condition a pretrained latent diffusion model for image generation. This text-mediated framework yields state-of-the-art visual decoding on the EEGCVPR dataset, with interpretable alignment to known neurocognitive pathways. Dominant EEG-caption associations reflected the importance of different semantic levels extracted from perceived images. Saliency maps and t-SNE projections reveal semantic topography across the scalp. Our model demonstrates how structured semantic mediation enables cognitively aligned visual decoding from EEG.

</details>


### [10] [A Survey on Long-Video Storytelling Generation: Architectures, Consistency, and Cinematic Quality](https://arxiv.org/abs/2507.07202)
*Mohamed Elmoghany,Ryan Rossi,Seunghyun Yoon,Subhojyoti Mukherjee,Eslam Bakr,Puneet Mathur,Gang Wu,Viet Dac Lai,Nedim Lipka,Ruiyi Zhang,Varun Manjunatha,Chien Nguyen,Daksh Dangi,Abel Salinas,Mohammad Taesiri,Hongjie Chen,Xiaolei Huang,Joe Barrow,Nesreen Ahmed,Hoda Eldardiry,Namyong Park,Yu Wang,Jaemin Cho,Anh Totti Nguyen,Zhengzhong Tu,Thien Nguyen,Dinesh Manocha,Mohamed Elhoseiny,Franck Dernoncourt*

Main category: cs.CV

TL;DR: 现有视频生成模型在生成长视频（超过16秒）时仍面临角色一致性和场景布局的挑战，多角色长视频尤其难以保持连贯性。本文综述了32篇论文，提出了新的分类法和比较表。


<details>
  <summary>Details</summary>
Motivation: 解决长视频生成中角色一致性和场景连贯性的问题，并总结现有方法的优缺点。

Method: 综述32篇视频生成论文，分析关键架构和训练策略，提出分类法和比较表。

Result: 识别出能够生成长视频、多角色且保持高保真度的关键组件和策略。

Conclusion: 通过系统分析，为未来长视频生成研究提供了分类和比较框架。

Abstract: Despite the significant progress that has been made in video generative models, existing state-of-the-art methods can only produce videos lasting 5-16 seconds, often labeled "long-form videos". Furthermore, videos exceeding 16 seconds struggle to maintain consistent character appearances and scene layouts throughout the narrative. In particular, multi-subject long videos still fail to preserve character consistency and motion coherence. While some methods can generate videos up to 150 seconds long, they often suffer from frame redundancy and low temporal diversity. Recent work has attempted to produce long-form videos featuring multiple characters, narrative coherence, and high-fidelity detail. We comprehensively studied 32 papers on video generation to identify key architectural components and training strategies that consistently yield these qualities. We also construct a comprehensive novel taxonomy of existing methods and present comparative tables that categorize papers by their architectural designs and performance characteristics.

</details>


### [11] [Scalable and Realistic Virtual Try-on Application for Foundation Makeup with Kubelka-Munk Theory](https://arxiv.org/abs/2507.07333)
*Hui Pang,Sunil Hadap,Violetta Shevchenko,Rahul Suresh,Amin Banitalebi-Dehkordi*

Main category: cs.CV

TL;DR: 提出了一种基于Kubelka-Munk理论的快速图像合成方法，用于虚拟试妆应用，提升了肤色与粉底颜色混合的真实性。


<details>
  <summary>Details</summary>
Motivation: 解决虚拟试妆应用中粉底与肤色混合的真实性和方法可扩展性问题。

Method: 提出了一种近似Kubelka-Munk理论的方法，并构建了一个可扩展的端到端框架，仅依赖电商网站上的产品信息。

Result: 在真实化妆图像上验证，该方法优于其他技术。

Conclusion: 该方法在保持真实性的同时提高了效率，适用于多样化的产品范围。

Abstract: Augmented reality is revolutionizing beauty industry with virtual try-on (VTO) applications, which empowers users to try a wide variety of products using their phones without the hassle of physically putting on real products. A critical technical challenge in foundation VTO applications is the accurate synthesis of foundation-skin tone color blending while maintaining the scalability of the method across diverse product ranges. In this work, we propose a novel method to approximate well-established Kubelka-Munk (KM) theory for faster image synthesis while preserving foundation-skin tone color blending realism. Additionally, we build a scalable end-to-end framework for realistic foundation makeup VTO solely depending on the product information available on e-commerce sites. We validate our method using real-world makeup images, demonstrating that our framework outperforms other techniques.

</details>


### [12] [Behave Your Motion: Habit-preserved Cross-category Animal Motion Transfer](https://arxiv.org/abs/2507.07394)
*Zhimin Zhang,Bi'an Du,Caoyuan Ma,Zheng Wang,Wei Hu*

Main category: cs.CV

TL;DR: 提出了一种保留动物习惯行为的跨类别运动转移框架，结合生成模型和大型语言模型，并在新数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有运动转移方法多关注人类运动，忽略动物习惯行为的保留，需填补这一空白。

Method: 基于生成框架，引入习惯保留模块和类别特定习惯编码器，结合大型语言模型处理未观测物种。

Result: 在DeformingThings4D-skl数据集上的实验验证了模型的有效性和优越性。

Conclusion: 该框架成功保留了动物习惯行为，为跨类别运动转移提供了新思路。

Abstract: Animal motion embodies species-specific behavioral habits, making the transfer of motion across categories a critical yet complex task for applications in animation and virtual reality. Existing motion transfer methods, primarily focused on human motion, emphasize skeletal alignment (motion retargeting) or stylistic consistency (motion style transfer), often neglecting the preservation of distinct habitual behaviors in animals. To bridge this gap, we propose a novel habit-preserved motion transfer framework for cross-category animal motion. Built upon a generative framework, our model introduces a habit-preservation module with category-specific habit encoder, allowing it to learn motion priors that capture distinctive habitual characteristics. Furthermore, we integrate a large language model (LLM) to facilitate the motion transfer to previously unobserved species. To evaluate the effectiveness of our approach, we introduce the DeformingThings4D-skl dataset, a quadruped dataset with skeletal bindings, and conduct extensive experiments and quantitative analyses, which validate the superiority of our proposed model.

</details>


### [13] [Seg-Wild: Interactive Segmentation based on 3D Gaussian Splatting for Unconstrained Image Collections](https://arxiv.org/abs/2507.07395)
*Yongtang Bao,Chengjie Tang,Yuze Wang,Haojie Li*

Main category: cs.CV

TL;DR: Seg-Wild是一种基于3D高斯泼溅的交互式分割方法，适用于无约束图像集合，解决了光照不一致和瞬态遮挡问题。


<details>
  <summary>Details</summary>
Motivation: 无约束照片集合易于获取但存在光照不一致和瞬态遮挡问题，传统分割方法无法有效处理。

Method: 结合多维特征嵌入和3D高斯相似性计算实现交互式分割，引入Spiky 3D Gaussian Cutter（SGC）平滑异常高斯。

Result: 实验表明Seg-Wild在分割和重建质量上优于现有方法。

Conclusion: Seg-Wild为无约束场景分割提供了高效解决方案，代码已开源。

Abstract: Reconstructing and segmenting scenes from unconstrained photo collections obtained from the Internet is a novel but challenging task. Unconstrained photo collections are easier to get than well-captured photo collections. These unconstrained images suffer from inconsistent lighting and transient occlusions, which makes segmentation challenging. Previous segmentation methods cannot address transient occlusions or accurately restore the scene's lighting conditions. Therefore, we propose Seg-Wild, an interactive segmentation method based on 3D Gaussian Splatting for unconstrained image collections, suitable for in-the-wild scenes. We integrate multi-dimensional feature embeddings for each 3D Gaussian and calculate the feature similarity between the feature embeddings and the segmentation target to achieve interactive segmentation in the 3D scene. Additionally, we introduce the Spiky 3D Gaussian Cutter (SGC) to smooth abnormal 3D Gaussians. We project the 3D Gaussians onto a 2D plane and calculate the ratio of 3D Gaussians that need to be cut using the SAM mask. We also designed a benchmark to evaluate segmentation quality in in-the-wild scenes. Experimental results demonstrate that compared to previous methods, Seg-Wild achieves better segmentation results and reconstruction quality. Our code will be available at https://github.com/Sugar0725/Seg-Wild.

</details>


### [14] [EscherNet++: Simultaneous Amodal Completion and Scalable View Synthesis through Masked Fine-Tuning and Enhanced Feed-Forward 3D Reconstruction](https://arxiv.org/abs/2507.07410)
*Xinan Zhang,Muhammad Zubair Irshad,Anthony Yezzi,Yi-Chang Tsai,Zsolt Kira*

Main category: cs.CV

TL;DR: EscherNet++是一种掩码微调的扩散模型，能够零样本合成物体的新视角并具备非模态完成能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法多阶段且复杂，未能考虑跨视角依赖且存储和计算冗余。

Method: 采用输入级和特征级掩码微调，实现端到端模型，提升新视角合成和非模态完成能力。

Result: 在遮挡任务中PSNR提升3.9，Volume IoU提升0.28，重建时间减少95%。

Conclusion: EscherNet++在小数据集和批量下实现SOTA，且适用于真实世界遮挡重建。

Abstract: We propose EscherNet++, a masked fine-tuned diffusion model that can synthesize novel views of objects in a zero-shot manner with amodal completion ability. Existing approaches utilize multiple stages and complex pipelines to first hallucinate missing parts of the image and then perform novel view synthesis, which fail to consider cross-view dependencies and require redundant storage and computing for separate stages. Instead, we apply masked fine-tuning including input-level and feature-level masking to enable an end-to-end model with the improved ability to synthesize novel views and conduct amodal completion. In addition, we empirically integrate our model with other feed-forward image-to-mesh models without extra training and achieve competitive results with reconstruction time decreased by 95%, thanks to its ability to synthesize arbitrary query views. Our method's scalable nature further enhances fast 3D reconstruction. Despite fine-tuning on a smaller dataset and batch size, our method achieves state-of-the-art results, improving PSNR by 3.9 and Volume IoU by 0.28 on occluded tasks in 10-input settings, while also generalizing to real-world occluded reconstruction.

</details>


### [15] [Degradation-Agnostic Statistical Facial Feature Transformation for Blind Face Restoration in Adverse Weather Conditions](https://arxiv.org/abs/2507.07464)
*Chang-Hwan Son*

Main category: cs.CV

TL;DR: 提出了一种基于GAN的盲人脸图像恢复框架，通过局部统计特征变换和退化无关特征嵌入，显著提升了恶劣天气下的人脸识别性能。


<details>
  <summary>Details</summary>
Motivation: 恶劣天气条件下的人脸图像质量下降，导致识别精度降低，现有方法缺乏针对性模块处理天气引起的退化问题。

Method: 结合局部统计特征变换（SFFT）和退化无关特征嵌入（DAFE），分别优化局部统计分布和特征提取，适应严重天气退化。

Result: 实验表明，该方法在抑制纹理失真和准确重建面部结构方面优于现有GAN和扩散模型方法。

Conclusion: SFFT和DAFE模块显著提升了恶劣天气下的人脸恢复效果，验证了其结构保真度和感知质量的优势。

Abstract: With the increasing deployment of intelligent CCTV systems in outdoor environments, there is a growing demand for face recognition systems optimized for challenging weather conditions. Adverse weather significantly degrades image quality, which in turn reduces recognition accuracy. Although recent face image restoration (FIR) models based on generative adversarial networks (GANs) and diffusion models have shown progress, their performance remains limited due to the lack of dedicated modules that explicitly address weather-induced degradations. This leads to distorted facial textures and structures. To address these limitations, we propose a novel GAN-based blind FIR framework that integrates two key components: local Statistical Facial Feature Transformation (SFFT) and Degradation-Agnostic Feature Embedding (DAFE). The local SFFT module enhances facial structure and color fidelity by aligning the local statistical distributions of low-quality (LQ) facial regions with those of high-quality (HQ) counterparts. Complementarily, the DAFE module enables robust statistical facial feature extraction under adverse weather conditions by aligning LQ and HQ encoder representations, thereby making the restoration process adaptive to severe weather-induced degradations. Experimental results demonstrate that the proposed degradation-agnostic SFFT model outperforms existing state-of-the-art FIR methods based on GAN and diffusion models, particularly in suppressing texture distortions and accurately reconstructing facial structures. Furthermore, both the SFFT and DAFE modules are empirically validated in enhancing structural fidelity and perceptual quality in face restoration under challenging weather scenarios.

</details>


### [16] [Divergence Minimization Preference Optimization for Diffusion Model Alignment](https://arxiv.org/abs/2507.07510)
*Binxu Li,Minkai Xu,Meihua Dang,Stefano Ermon*

Main category: cs.CV

TL;DR: 论文提出了一种新的扩散模型对齐方法DMPO，通过最小化反向KL散度优化模型，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法存在次优均值寻求问题，需要更有效的对齐方法。

Method: 提出DMPO方法，通过最小化反向KL散度对齐扩散模型。

Result: DMPO在人类评估和自动指标上均优于现有方法，PickScore提升至少64.6%。

Conclusion: DMPO为扩散模型偏好对齐提供了稳健且优雅的解决方案，兼具理论和实践优势。

Abstract: Diffusion models have achieved remarkable success in generating realistic and versatile images from text prompts. Inspired by the recent advancements of language models, there is an increasing interest in further improving the models by aligning with human preferences. However, we investigate alignment from a divergence minimization perspective and reveal that existing preference optimization methods are typically trapped in suboptimal mean-seeking optimization. In this paper, we introduce Divergence Minimization Preference Optimization (DMPO), a novel and principled method for aligning diffusion models by minimizing reverse KL divergence, which asymptotically enjoys the same optimization direction as original RL. We provide rigorous analysis to justify the effectiveness of DMPO and conduct comprehensive experiments to validate its empirical strength across both human evaluations and automatic metrics. Our extensive results show that diffusion models fine-tuned with DMPO can consistently outperform or match existing techniques, specifically outperforming all existing diffusion alignment baselines by at least 64.6% in PickScore across all evaluation datasets, demonstrating the method's superiority in aligning generative behavior with desired outputs. Overall, DMPO unlocks a robust and elegant pathway for preference alignment, bridging principled theory with practical performance in diffusion models.

</details>


### [17] [MUVOD: A Novel Multi-view Video Object Segmentation Dataset and A Benchmark for 3D Segmentation](https://arxiv.org/abs/2507.07519)
*Bangning Wei,Joshua Maraval,Meriem Outtas,Kidiyo Kpalma,Nicolas Ramin,Lu Zhang*

Main category: cs.CV

TL;DR: 论文介绍了MUVOD数据集，用于动态场景的4D对象分割，填补了多视角视频数据集的空白，并提供了评估指标和基线方法。


<details>
  <summary>Details</summary>
Motivation: 动态场景的4D对象分割研究不足，主要原因是缺乏高质量的多视角视频数据集。

Method: 提出MUVOD数据集，包含17个场景、7830张RGB图像及其分割掩码，支持4D运动跟踪。

Result: 数据集包含459个实例和73个类别，为多视角视频分割方法提供了基准。

Conclusion: MUVOD数据集为动态场景分割研究提供了基础，并推动了该领域的发展。

Abstract: The application of methods based on Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3D GS) have steadily gained popularity in the field of 3D object segmentation in static scenes. These approaches demonstrate efficacy in a range of 3D scene understanding and editing tasks. Nevertheless, the 4D object segmentation of dynamic scenes remains an underexplored field due to the absence of a sufficiently extensive and accurately labelled multi-view video dataset. In this paper, we present MUVOD, a new multi-view video dataset for training and evaluating object segmentation in reconstructed real-world scenarios. The 17 selected scenes, describing various indoor or outdoor activities, are collected from different sources of datasets originating from various types of camera rigs. Each scene contains a minimum of 9 views and a maximum of 46 views. We provide 7830 RGB images (30 frames per video) with their corresponding segmentation mask in 4D motion, meaning that any object of interest in the scene could be tracked across temporal frames of a given view or across different views belonging to the same camera rig. This dataset, which contains 459 instances of 73 categories, is intended as a basic benchmark for the evaluation of multi-view video segmentation methods. We also present an evaluation metric and a baseline segmentation approach to encourage and evaluate progress in this evolving field. Additionally, we propose a new benchmark for 3D object segmentation task with a subset of annotated multi-view images selected from our MUVOD dataset. This subset contains 50 objects of different conditions in different scenarios, providing a more comprehensive analysis of state-of-the-art 3D object segmentation methods. Our proposed MUVOD dataset is available at https://volumetric-repository.labs.b-com.com/#/muvod.

</details>


### [18] [Diffusion-Guided Knowledge Distillation for Weakly-Supervised Low-Light Semantic Segmentation](https://arxiv.org/abs/2507.07578)
*Chunyan Wang,Dong Zhang,Jinhui Tang*

Main category: cs.CV

TL;DR: 提出了一种结合扩散引导知识蒸馏和深度引导特征融合的新框架DGKD-WLSS，用于弱监督低光语义分割任务，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在低光环境下性能显著下降，主要由于图像质量退化和弱监督的固有限制，导致不可靠的类激活图和语义模糊的伪标签。

Method: 提出DGKD-WLSS框架，结合扩散引导知识蒸馏（DGKD）和深度引导特征融合（DGF2），通过扩散去噪和知识蒸馏对齐特征，并利用深度图增强结构特征学习。

Result: 实验证明DGKD-WLSS在低光条件下的弱监督语义分割任务中达到最先进性能。

Conclusion: DGKD-WLSS有效解决了低光环境下弱监督语义分割的挑战，代码已开源。

Abstract: Weakly-supervised semantic segmentation aims to assign category labels to each pixel using weak annotations, significantly reducing manual annotation costs. Although existing methods have achieved remarkable progress in well-lit scenarios, their performance significantly degrades in low-light environments due to two fundamental limitations: severe image quality degradation (e.g., low contrast, noise, and color distortion) and the inherent constraints of weak supervision. These factors collectively lead to unreliable class activation maps and semantically ambiguous pseudo-labels, ultimately compromising the model's ability to learn discriminative feature representations. To address these problems, we propose Diffusion-Guided Knowledge Distillation for Weakly-Supervised Low-light Semantic Segmentation (DGKD-WLSS), a novel framework that synergistically combines Diffusion-Guided Knowledge Distillation (DGKD) with Depth-Guided Feature Fusion (DGF2). DGKD aligns normal-light and low-light features via diffusion-based denoising and knowledge distillation, while DGF2 integrates depth maps as illumination-invariant geometric priors to enhance structural feature learning. Extensive experiments demonstrate the effectiveness of DGKD-WLSS, which achieves state-of-the-art performance in weakly supervised semantic segmentation tasks under low-light conditions. The source codes have been released at:https://github.com/ChunyanWang1/DGKD-WLSS.

</details>


### [19] [Stable-Hair v2: Real-World Hair Transfer via Multiple-View Diffusion Model](https://arxiv.org/abs/2507.07591)
*Kuiyuan Sun,Yuxuan Zhang,Jichao Zhang,Jiaming Liu,Wei Wang,Niculae Sebe,Yao Zhao*

Main category: cs.CV

TL;DR: Stable-Hair v2 是一种基于扩散模型的多视角头发迁移框架，首次利用多视角扩散模型实现高质量、视角一致的头发迁移。


<details>
  <summary>Details</summary>
Motivation: 现有扩散方法在多视角一致的高质量输出方面表现不足，而这对数字人和虚拟化身等应用至关重要。

Method: 提出多视角训练数据生成管道，结合极坐标嵌入和时间注意力层，采用多阶段训练策略优化模型。

Result: 实验表明，该方法能准确迁移逼真发型并实现视角间无缝一致，显著优于现有方法。

Conclusion: Stable-Hair v2 在多视角头发迁移领域设立了新基准，代码已开源。

Abstract: While diffusion-based methods have shown impressive capabilities in capturing diverse and complex hairstyles, their ability to generate consistent and high-quality multi-view outputs -- crucial for real-world applications such as digital humans and virtual avatars -- remains underexplored. In this paper, we propose Stable-Hair v2, a novel diffusion-based multi-view hair transfer framework. To the best of our knowledge, this is the first work to leverage multi-view diffusion models for robust, high-fidelity, and view-consistent hair transfer across multiple perspectives. We introduce a comprehensive multi-view training data generation pipeline comprising a diffusion-based Bald Converter, a data-augment inpainting model, and a face-finetuned multi-view diffusion model to generate high-quality triplet data, including bald images, reference hairstyles, and view-aligned source-bald pairs. Our multi-view hair transfer model integrates polar-azimuth embeddings for pose conditioning and temporal attention layers to ensure smooth transitions between views. To optimize this model, we design a novel multi-stage training strategy consisting of pose-controllable latent IdentityNet training, hair extractor training, and temporal attention training. Extensive experiments demonstrate that our method accurately transfers detailed and realistic hairstyles to source subjects while achieving seamless and consistent results across views, significantly outperforming existing methods and establishing a new benchmark in multi-view hair transfer. Code is publicly available at https://github.com/sunkymepro/StableHairV2.

</details>


### [20] [LOSC: LiDAR Open-voc Segmentation Consolidator](https://arxiv.org/abs/2507.07605)
*Nermin Samet,Gilles Puy,Renaud Marlet*

Main category: cs.CV

TL;DR: LOSC方法通过整合图像语义标签并训练3D网络，显著提升了零样本开放词汇语义和全景分割的性能。


<details>
  <summary>Details</summary>
Motivation: 解决激光雷达扫描中图像语义标签噪声大且稀疏的问题，提升分割的时空一致性和鲁棒性。

Method: 整合图像语义标签以增强时空一致性和鲁棒性，并基于这些标签训练3D网络。

Result: 在nuScenes和SemanticKITTI数据集上，LOSC方法显著优于现有零样本开放词汇分割技术。

Conclusion: LOSC是一种简单有效的方法，显著提升了激光雷达扫描的分割性能。

Abstract: We study the use of image-based Vision-Language Models (VLMs) for open-vocabulary segmentation of lidar scans in driving settings. Classically, image semantics can be back-projected onto 3D point clouds. Yet, resulting point labels are noisy and sparse. We consolidate these labels to enforce both spatio-temporal consistency and robustness to image-level augmentations. We then train a 3D network based on these refined labels. This simple method, called LOSC, outperforms the SOTA of zero-shot open-vocabulary semantic and panoptic segmentation on both nuScenes and SemanticKITTI, with significant margins.

</details>


### [21] [T-GVC: Trajectory-Guided Generative Video Coding at Ultra-Low Bitrates](https://arxiv.org/abs/2507.07633)
*Zhitao Wang,Hengyu Man,Wenrui Li,Xingtao Wang,Xiaopeng Fan,Debin Zhao*

Main category: cs.CV

TL;DR: T-GVC是一种基于轨迹引导的生成视频编码框架，通过语义感知的稀疏运动采样和扩散过程中的轨迹对齐损失约束，在超低比特率下实现高质量视频重建。


<details>
  <summary>Details</summary>
Motivation: 现有生成视频编码方法受限于领域特异性或对高层文本指导的过度依赖，导致运动细节丢失和重建不真实。

Method: T-GVC采用语义感知稀疏运动采样，提取像素级运动轨迹点，并结合扩散过程中的轨迹对齐损失约束。

Result: 实验表明，T-GVC在超低比特率下优于传统编解码器和端到端视频压缩方法，且运动控制更精确。

Conclusion: T-GVC为基于几何运动建模的生成视频编码开辟了新方向。

Abstract: Recent advances in video generation techniques have given rise to an emerging paradigm of generative video coding, aiming to achieve semantically accurate reconstructions in Ultra-Low Bitrate (ULB) scenarios by leveraging strong generative priors. However, most existing methods are limited by domain specificity (e.g., facial or human videos) or an excessive dependence on high-level text guidance, which often fails to capture motion details and results in unrealistic reconstructions. To address these challenges, we propose a Trajectory-Guided Generative Video Coding framework (dubbed T-GVC). T-GVC employs a semantic-aware sparse motion sampling pipeline to effectively bridge low-level motion tracking with high-level semantic understanding by extracting pixel-wise motion as sparse trajectory points based on their semantic importance, not only significantly reducing the bitrate but also preserving critical temporal semantic information. In addition, by incorporating trajectory-aligned loss constraints into diffusion processes, we introduce a training-free latent space guidance mechanism to ensure physically plausible motion patterns without sacrificing the inherent capabilities of generative models. Experimental results demonstrate that our framework outperforms both traditional codecs and state-of-the-art end-to-end video compression methods under ULB conditions. Furthermore, additional experiments confirm that our approach achieves more precise motion control than existing text-guided methods, paving the way for a novel direction of generative video coding guided by geometric motion modeling.

</details>


### [22] [Motion-Aware Adaptive Pixel Pruning for Efficient Local Motion Deblurring](https://arxiv.org/abs/2507.07708)
*Wei Shang,Dongwei Ren,Wanying Zhang,Pengfei Zhu,Qinghua Hu,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 提出了一种可训练的掩模预测器和帧内运动分析器，用于高效处理局部运动模糊，显著减少计算量并提升去模糊性能。


<details>
  <summary>Details</summary>
Motivation: 现有去模糊方法在计算资源分配和空间变化模糊模式处理上效率不足，需改进。

Method: 1. 训练掩模预测器识别模糊区域；2. 通过结构重参数化优化推理计算；3. 开发帧内运动分析器生成自适应模糊恢复引导。

Result: 在局部和全局模糊数据集上表现优于现有方法，计算量减少49%。

Conclusion: 该方法通过高效计算和自适应引导，显著提升了局部运动模糊的去模糊效果。

Abstract: Local motion blur in digital images originates from the relative motion between dynamic objects and static imaging systems during exposure. Existing deblurring methods face significant challenges in addressing this problem due to their inefficient allocation of computational resources and inadequate handling of spatially varying blur patterns. To overcome these limitations, we first propose a trainable mask predictor that identifies blurred regions in the image. During training, we employ blur masks to exclude sharp regions. For inference optimization, we implement structural reparameterization by converting $3\times 3$ convolutions to computationally efficient $1\times 1$ convolutions, enabling pixel-level pruning of sharp areas to reduce computation. Second, we develop an intra-frame motion analyzer that translates relative pixel displacements into motion trajectories, establishing adaptive guidance for region-specific blur restoration. Our method is trained end-to-end using a combination of reconstruction loss, reblur loss, and mask loss guided by annotated blur masks. Extensive experiments demonstrate superior performance over state-of-the-art methods on both local and global blur datasets while reducing FLOPs by 49\% compared to SOTA models (e.g., LMD-ViT). The source code is available at https://github.com/shangwei5/M2AENet.

</details>


### [23] [Benchmarking Content-Based Puzzle Solvers on Corrupted Jigsaw Puzzles](https://arxiv.org/abs/2507.07828)
*Richard Dirauf,Florian Wolz,Dario Zanca,Björn Eskofier*

Main category: cs.CV

TL;DR: 研究了内容拼图求解器在三种损坏情况下的鲁棒性，发现深度学习模型通过数据增强能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有拼图求解器在真实场景（如文物碎片重组）中缺乏对损坏情况的评估，需研究其鲁棒性。

Method: 引入三种拼图损坏类型（缺失、边缘腐蚀、内容腐蚀），评估启发式和深度学习求解器的表现。

Result: 标准求解器在损坏增加时性能下降快，但深度学习模型通过微调能显著提升鲁棒性，其中Positional Diffusion模型表现最佳。

Conclusion: 研究为提升真实场景拼图重建提供了方向，强调数据增强和模型优化的重要性。

Abstract: Content-based puzzle solvers have been extensively studied, demonstrating significant progress in computational techniques. However, their evaluation often lacks realistic challenges crucial for real-world applications, such as the reassembly of fragmented artefacts or shredded documents. In this work, we investigate the robustness of State-Of-The-Art content-based puzzle solvers introducing three types of jigsaw puzzle corruptions: missing pieces, eroded edges, and eroded contents. Evaluating both heuristic and deep learning-based solvers, we analyse their ability to handle these corruptions and identify key limitations. Our results show that solvers developed for standard puzzles have a rapid decline in performance if more pieces are corrupted. However, deep learning models can significantly improve their robustness through fine-tuning with augmented data. Notably, the advanced Positional Diffusion model adapts particularly well, outperforming its competitors in most experiments. Based on our findings, we highlight promising research directions for enhancing the automated reconstruction of real-world artefacts.

</details>


### [24] [Single-Step Latent Diffusion for Underwater Image Restoration](https://arxiv.org/abs/2507.07878)
*Jiayi Wu,Tianfu Wang,Md Abu Bakr Siddique,Md Jahidul Islam,Cornelia Fermuller,Yiannis Aloimonos,Christopher A. Metzler*

Main category: cs.CV

TL;DR: 提出了一种名为SLURPP的新网络架构，结合预训练的潜在扩散模型和显式场景分解，用于水下图像恢复，显著提升了性能和速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的水下图像恢复方法在复杂几何和深度变化场景中计算量大且易产生不真实伪影，需要改进。

Method: 结合预训练的潜在扩散模型和显式场景分解，设计物理基础的水下图像合成流程生成多样化训练数据。

Result: SLURPP比现有扩散方法快200倍以上，在合成基准测试中PSNR提升约3 dB，并在真实数据上表现优异。

Conclusion: SLURPP通过新颖架构和合成数据生成，显著提升了水下图像恢复的性能和效率。

Abstract: Underwater image restoration algorithms seek to restore the color, contrast, and appearance of a scene that is imaged underwater. They are a critical tool in applications ranging from marine ecology and aquaculture to underwater construction and archaeology. While existing pixel-domain diffusion-based image restoration approaches are effective at restoring simple scenes with limited depth variation, they are computationally intensive and often generate unrealistic artifacts when applied to scenes with complex geometry and significant depth variation. In this work we overcome these limitations by combining a novel network architecture (SLURPP) with an accurate synthetic data generation pipeline. SLURPP combines pretrained latent diffusion models -- which encode strong priors on the geometry and depth of scenes -- with an explicit scene decomposition -- which allows one to model and account for the effects of light attenuation and backscattering. To train SLURPP we design a physics-based underwater image synthesis pipeline that applies varied and realistic underwater degradation effects to existing terrestrial image datasets. This approach enables the generation of diverse training data with dense medium/degradation annotations. We evaluate our method extensively on both synthetic and real-world benchmarks and demonstrate state-of-the-art performance. Notably, SLURPP is over 200X faster than existing diffusion-based methods while offering ~ 3 dB improvement in PSNR on synthetic benchmarks. It also offers compelling qualitative improvements on real-world data. Project website https://tianfwang.github.io/slurpp/.

</details>


### [25] [Not Only Consistency: Enhance Test-Time Adaptation with Spatio-temporal Inconsistency for Remote Physiological Measurement](https://arxiv.org/abs/2507.07908)
*Xiao Yang,Yuxuan Fan,Can Liu,Houcheng Su,Weichen Guo,Jiyao Wang,Dengbo He*

Main category: cs.CV

TL;DR: 提出了一种名为CiCi的新型全测试时间自适应（TTA）策略，用于远程光电容积描记（rPPG）任务，通过结合一致性和不一致性先验知识，提升模型在推理时的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在隐私和实时适应性方面存在限制，无法满足实际部署需求，因此需要一种无需源数据的实时自适应方法。

Method: 基于生理学先验知识，提出CiCi框架，利用频率域的一致性和时间域的不一致性，结合梯度动态控制机制，实现稳定自适应。

Result: 在五个数据集上的实验表明，该方法在实时自监督适应中优于现有技术，达到最先进性能。

Conclusion: CiCi框架为rPPG任务提供了一种高效、稳定的测试时间自适应解决方案，适用于实际部署。

Abstract: Remote photoplethysmography (rPPG) has emerged as a promising non-invasive method for monitoring physiological signals using the camera. Although various domain adaptation and generalization methods were proposed to promote the adaptability of deep-based rPPG models in unseen deployment environments, considerations in aspects like privacy concerns and real-time adaptation restrict their application in real-world deployment. Thus, we aim to propose a novel fully Test-Time Adaptation (TTA) strategy tailored for rPPG tasks in this work. Specifically, based on prior knowledge in physiology and our observations, we noticed not only there is spatio-temporal consistency in the frequency domain of rPPG signals, but also that inconsistency in the time domain was significant. Given this, by leveraging both consistency and inconsistency priors, we introduce an innovative expert knowledge-based self-supervised \textbf{C}onsistency-\textbf{i}n\textbf{C}onsistency-\textbf{i}ntegration (\textbf{CiCi}) framework to enhances model adaptation during inference. Besides, our approach further incorporates a gradient dynamic control mechanism to mitigate potential conflicts between priors, ensuring stable adaptation across instances. Through extensive experiments on five diverse datasets under the TTA protocol, our method consistently outperforms existing techniques, presenting state-of-the-art performance in real-time self-supervised adaptation without accessing source data. The code will be released later.

</details>


### [26] [Scaling RL to Long Videos](https://arxiv.org/abs/2507.07966)
*Yukang Chen,Wei Huang,Baifeng Shi,Qinghao Hu,Hanrong Ye,Ligeng Zhu,Zhijian Liu,Pavlo Molchanov,Jan Kautz,Xiaojuan Qi,Sifei Liu,Hongxu Yin,Yao Lu,Song Han*

Main category: cs.CV

TL;DR: 论文提出了一种全栈框架，通过强化学习扩展视觉语言模型（VLMs）在长视频中的推理能力，包括数据集、训练管道和基础设施三部分。


<details>
  <summary>Details</summary>
Motivation: 解决长视频推理中的独特挑战，提升视觉语言模型在长视频中的表现。

Method: 1) 构建大规模数据集LongVideo-Reason；2) 两阶段训练管道（CoT-SFT和RL）；3) 多模态强化序列并行（MR-SP）基础设施。

Result: LongVILA-R1-7B在长视频QA基准测试中表现优异，MR-SP系统训练速度提升2.1倍。

Conclusion: LongVILA-R1是长视频推理的重要进展，并公开了支持多模态RL训练的系统。

Abstract: We introduce a full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical components: (1) a large-scale dataset, LongVideo-Reason, comprising 52K long video QA pairs with high-quality reasoning annotations across diverse domains such as sports, games, and vlogs; (2) a two-stage training pipeline that extends VLMs with chain-of-thought supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a training infrastructure for long video RL, named Multi-modal Reinforcement Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a vLLM-based engine tailored for long video, using cached video embeddings for efficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves strong performance on long video QA benchmarks such as VideoMME. It also outperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal reasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on our LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to 2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent performance gains as the number of input video frames scales. LongVILA-R1 marks a firm step towards long video reasoning in VLMs. In addition, we release our training system for public availability that supports RL training on various modalities (video, text, and audio), various models (VILA and Qwen series), and even image and video generation models. On a single A100 node (8 GPUs), it supports RL training on hour-long videos (e.g., 3,600 frames / around 256k tokens).

</details>


### [27] [Martian World Models: Controllable Video Synthesis with Physically Accurate 3D Reconstructions](https://arxiv.org/abs/2507.07978)
*Longfei Li,Zhiwen Fan,Wenyan Cong,Xinhang Liu,Yuyang Yin,Matt Foutter,Panwang Pan,Chenyu You,Yue Wang,Zhangyang Wang,Yao Zhao,Marco Pavone,Yunchao Wei*

Main category: cs.CV

TL;DR: 提出了一种结合数据生成和视频合成的解决方案，用于生成高质量的火星地形视频。


<details>
  <summary>Details</summary>
Motivation: 火星地形视频合成对任务演练和机器人模拟至关重要，但由于数据稀缺和火星与地球图像的领域差异，这一任务具有挑战性。

Method: 1) 数据生成管道M3arsSynth，从NASA的真实立体导航图像重建3D火星环境并渲染多视角视频；2) 视频生成器MarsGen，基于M3arsSynth数据合成视觉逼真且几何一致的视频。

Result: 实验表明，该方法优于基于地球数据集训练的模型，具有更高的视觉保真度和3D结构一致性。

Conclusion: 提出的解决方案能够有效生成高质量的火星地形视频，适用于任务模拟和机器人测试。

Abstract: Synthesizing realistic Martian landscape videos is crucial for mission rehearsal and robotic simulation. However, this task poses unique challenges due to the scarcity of high-quality Martian data and the significant domain gap between Martian and terrestrial imagery. To address these challenges, we propose a holistic solution composed of two key components: 1) A data curation pipeline Multimodal Mars Synthesis (M3arsSynth), which reconstructs 3D Martian environments from real stereo navigation images, sourced from NASA's Planetary Data System (PDS), and renders high-fidelity multiview 3D video sequences. 2) A Martian terrain video generator, MarsGen, which synthesizes novel videos visually realistic and geometrically consistent with the 3D structure encoded in the data. Our M3arsSynth engine spans a wide range of Martian terrains and acquisition dates, enabling the generation of physically accurate 3D surface models at metric-scale resolution. MarsGen, fine-tuned on M3arsSynth data, synthesizes videos conditioned on an initial image frame and, optionally, camera trajectories or textual prompts, allowing for video generation in novel environments. Experimental results show that our approach outperforms video synthesis models trained on terrestrial datasets, achieving superior visual fidelity and 3D structural consistency.

</details>


### [28] [Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling](https://arxiv.org/abs/2507.07982)
*Haoyu Wu,Diankun Wu,Tianyu He,Junliang Guo,Yang Ye,Yueqi Duan,Jiang Bian*

Main category: cs.CV

TL;DR: 论文提出Geometry Forcing方法，通过几何对齐目标增强视频扩散模型的3D感知能力，显著提升生成视频的视觉质量和3D一致性。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型仅基于原始视频数据训练时，难以捕捉几何感知的结构，因此需要一种方法弥合模型与物理世界3D本质之间的差距。

Method: 提出Geometry Forcing方法，通过Angular Alignment和Scale Alignment两个几何对齐目标，引导模型学习几何感知的潜在表示。

Result: 实验表明，该方法在相机视角和动作条件视频生成任务中，显著优于基线方法，提升了视觉质量和3D一致性。

Conclusion: Geometry Forcing是一种简单有效的方法，能够增强视频扩散模型的3D感知能力，为视频生成任务提供更高质量的3D一致性结果。

Abstract: Videos inherently represent 2D projections of a dynamic 3D world. However, our analysis suggests that video diffusion models trained solely on raw video data often fail to capture meaningful geometric-aware structure in their learned representations. To bridge this gap between video diffusion models and the underlying 3D nature of the physical world, we propose Geometry Forcing, a simple yet effective method that encourages video diffusion models to internalize latent 3D representations. Our key insight is to guide the model's intermediate representations toward geometry-aware structure by aligning them with features from a pretrained geometric foundation model. To this end, we introduce two complementary alignment objectives: Angular Alignment, which enforces directional consistency via cosine similarity, and Scale Alignment, which preserves scale-related information by regressing unnormalized geometric features from normalized diffusion representation. We evaluate Geometry Forcing on both camera view-conditioned and action-conditioned video generation tasks. Experimental results demonstrate that our method substantially improves visual quality and 3D consistency over the baseline methods. Project page: https://GeometryForcing.github.io.

</details>


### [29] [MGVQ: Could VQ-VAE Beat VAE? A Generalizable Tokenizer with Multi-group Quantization](https://arxiv.org/abs/2507.07997)
*Mingkai Jia,Wei Yin,Xiaotao Hu,Jiaxin Guo,Xiaoyang Guo,Qian Zhang,Xiao-Xiao Long,Ping Tan*

Main category: cs.CV

TL;DR: 论文提出了一种名为\NickName的新方法，通过增强离散码本的表示能力，优化码本并减少信息损失，从而提升VQ-VAEs的重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有VQ-VAEs与VAEs在重建质量上存在较大差距，需要改进量化策略以缩小这一差距。

Method: 保留潜在维度以保留编码特征，并引入一组子码本进行量化。同时构建了512p和2k分辨率的零样本基准测试。

Result: \NickName在ImageNet和8个零样本基准测试中均达到最优性能，显著优于SD-VAE（rFID 0.49 vs. 0.91）。

Conclusion: \NickName在重建任务中表现出色，为高清图像处理任务中的保真度提供了新思路。

Abstract: Vector Quantized Variational Autoencoders (VQ-VAEs) are fundamental models that compress continuous visual data into discrete tokens. Existing methods have tried to improve the quantization strategy for better reconstruction quality, however, there still exists a large gap between VQ-VAEs and VAEs. To narrow this gap, we propose \NickName, a novel method to augment the representation capability of discrete codebooks, facilitating easier optimization for codebooks and minimizing information loss, thereby enhancing reconstruction quality. Specifically, we propose to retain the latent dimension to preserve encoded features and incorporate a set of sub-codebooks for quantization. Furthermore, we construct comprehensive zero-shot benchmarks featuring resolutions of 512p and 2k to evaluate the reconstruction performance of existing methods rigorously. \NickName~achieves the \textbf{state-of-the-art performance on both ImageNet and $8$ zero-shot benchmarks} across all VQ-VAEs. Notably, compared with SD-VAE, we outperform them on ImageNet significantly, with rFID $\textbf{0.49}$ v.s. $\textbf{0.91}$, and achieve superior PSNR on all zero-shot benchmarks. These results highlight the superiority of \NickName~in reconstruction and pave the way for preserving fidelity in HD image processing tasks. Code will be publicly available at https://github.com/MKJia/MGVQ.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [30] [D-CNN and VQ-VAE Autoencoders for Compression and Denoising of Industrial X-ray Computed Tomography Images](https://arxiv.org/abs/2507.07704)
*Bardia Hejazi,Keerthana Chand,Tobias Fritsch,Giovanni Bruno*

Main category: eess.IV

TL;DR: 研究探讨了使用深度学习自动编码器压缩工业X射线计算机断层扫描（XCT）数据，并分析了不同压缩算法对数据恢复质量的影响。


<details>
  <summary>Details</summary>
Motivation: 随着成像技术的发展，数据量激增，需要高效可靠的存储解决方案。

Method: 采用两种网络架构（D-CNN和VQ-VAE）和不同压缩率，对砂岩样本的XCT数据进行压缩和解码质量评估。

Result: 不同架构和压缩率对数据恢复质量有显著影响，需根据后续分析需求选择合适方法。

Conclusion: 研究结果为科学家选择数据存储和分析策略提供了参考。

Abstract: The ever-growing volume of data in imaging sciences stemming from the advancements in imaging technologies, necessitates efficient and reliable storage solutions for such large datasets. This study investigates the compression of industrial X-ray computed tomography (XCT) data using deep learning autoencoders and examines how these compression algorithms affect the quality of the recovered data. Two network architectures with different compression rates were used, a deep convolution neural network (D-CNN) and a vector quantized variational autoencoder (VQ-VAE). The XCT data used was from a sandstone sample with a complex internal pore network. The quality of the decoded images obtained from the two different deep learning architectures with different compression rates were quantified and compared to the original input data. In addition, to improve image decoding quality metrics, we introduced a metric sensitive to edge preservation, which is crucial for three-dimensional data analysis. We showed that different architectures and compression rates are required depending on the specific characteristics needed to be preserved for later analysis. The findings presented here can aid scientists to determine the requirements and strategies for their data storage and analysis needs.

</details>
