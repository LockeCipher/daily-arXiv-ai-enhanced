{"id": "2509.25600", "pdf": "https://arxiv.org/pdf/2509.25600", "abs": "https://arxiv.org/abs/2509.25600", "authors": ["Wontaek Kim", "Tianyu Li", "Sehoon Ha"], "title": "MoReFlow: Motion Retargeting Learning through Unsupervised Flow Matching", "categories": ["cs.GR", "cs.RO"], "comment": null, "summary": "Motion retargeting holds a premise of offering a larger set of motion data for characters and robots with different morphologies. Many prior works have approached this problem via either handcrafted constraints or paired motion datasets, limiting their applicability to humanoid characters or narrow behaviors such as locomotion. Moreover, they often assume a fixed notion of retargeting, overlooking domain-specific objectives like style preservation in animation or task-space alignment in robotics. In this work, we propose MoReFlow, Motion Retargeting via Flow Matching, an unsupervised framework that learns correspondences between characters' motion embedding spaces. Our method consists of two stages. First, we train tokenized motion embeddings for each character using a VQ-VAE, yielding compact latent representations. Then, we employ flow matching with conditional coupling to align the latent spaces across characters, which simultaneously learns conditioned and unconditioned matching to achieve robust but flexible retargeting. Once trained, MoReFlow enables flexible and reversible retargeting without requiring paired data. Experiments demonstrate that MoReFlow produces high-quality motions across diverse characters and tasks, offering improved controllability, generalization, and motion realism compared to the baselines.", "AI": {"tldr": "MoReFlow\u662f\u4e00\u4e2a\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u65e0\u76d1\u7763\u8fd0\u52a8\u91cd\u5b9a\u5411\u6846\u67b6\uff0c\u901a\u8fc7VQ-VAE\u5b66\u4e60\u8fd0\u52a8\u5d4c\u5165\uff0c\u7136\u540e\u4f7f\u7528\u6761\u4ef6\u8026\u5408\u6d41\u5339\u914d\u5bf9\u9f50\u4e0d\u540c\u89d2\u8272\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u5b9e\u73b0\u7075\u6d3b\u53ef\u9006\u7684\u91cd\u5b9a\u5411\u3002", "motivation": "\u73b0\u6709\u8fd0\u52a8\u91cd\u5b9a\u5411\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u7ea6\u675f\u6216\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u4ec5\u9002\u7528\u4e8e\u4eba\u5f62\u89d2\u8272\u6216\u72ed\u7a84\u884c\u4e3a\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u9886\u57df\u7279\u5b9a\u76ee\u6807\uff08\u5982\u52a8\u753b\u98ce\u683c\u4fdd\u6301\u6216\u673a\u5668\u4eba\u4efb\u52a1\u7a7a\u95f4\u5bf9\u9f50\uff09\u7684\u8003\u8651\u3002", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528VQ-VAE\u8bad\u7ec3\u6bcf\u4e2a\u89d2\u8272\u7684\u6807\u8bb0\u5316\u8fd0\u52a8\u5d4c\u5165\uff1b2\uff09\u4f7f\u7528\u6761\u4ef6\u8026\u5408\u6d41\u5339\u914d\u5bf9\u9f50\u89d2\u8272\u95f4\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u540c\u65f6\u5b66\u4e60\u6761\u4ef6\u548c\u975e\u6761\u4ef6\u5339\u914d\u4ee5\u5b9e\u73b0\u9c81\u68d2\u7075\u6d3b\u7684\u91cd\u5b9a\u5411\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMoReFlow\u80fd\u5728\u591a\u6837\u5316\u89d2\u8272\u548c\u4efb\u52a1\u4e0a\u751f\u6210\u9ad8\u8d28\u91cf\u8fd0\u52a8\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u53ef\u63a7\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u8fd0\u52a8\u771f\u5b9e\u611f\u3002", "conclusion": "MoReFlow\u65e0\u9700\u914d\u5bf9\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u7075\u6d3b\u53ef\u9006\u7684\u8fd0\u52a8\u91cd\u5b9a\u5411\uff0c\u4e3a\u4e0d\u540c\u5f62\u6001\u7684\u89d2\u8272\u548c\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u9002\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25304", "pdf": "https://arxiv.org/pdf/2509.25304", "abs": "https://arxiv.org/abs/2509.25304", "authors": ["Haozhe Jia", "Wenshuo Chen", "Yuqi Lin", "Yang Yang", "Lei Wang", "Mang Ning", "Bowen Tian", "Songning Lai", "Nanqian Jia", "Yifan Chen", "Yutao Yue"], "title": "LUMA: Low-Dimension Unified Motion Alignment with Dual-Path Anchoring for Text-to-Motion Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "While current diffusion-based models, typically built on U-Net architectures, have shown promising results on the text-to-motion generation task, they still suffer from semantic misalignment and kinematic artifacts. Through analysis, we identify severe gradient attenuation in the deep layers of the network as a key bottleneck, leading to insufficient learning of high-level features. To address this issue, we propose \\textbf{LUMA} (\\textit{\\textbf{L}ow-dimension \\textbf{U}nified \\textbf{M}otion \\textbf{A}lignment}), a text-to-motion diffusion model that incorporates dual-path anchoring to enhance semantic alignment. The first path incorporates a lightweight MoCLIP model trained via contrastive learning without relying on external data, offering semantic supervision in the temporal domain. The second path introduces complementary alignment signals in the frequency domain, extracted from low-frequency DCT components known for their rich semantic content. These two anchors are adaptively fused through a temporal modulation mechanism, allowing the model to progressively transition from coarse alignment to fine-grained semantic refinement throughout the denoising process. Experimental results on HumanML3D and KIT-ML demonstrate that LUMA achieves state-of-the-art performance, with FID scores of 0.035 and 0.123, respectively. Furthermore, LUMA accelerates convergence by 1.4$\\times$ compared to the baseline, making it an efficient and scalable solution for high-fidelity text-to-motion generation.", "AI": {"tldr": "LUMA\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u8def\u5f84\u951a\u5b9a\u673a\u5236\u89e3\u51b3\u8bed\u4e49\u5bf9\u9f50\u548c\u8fd0\u52a8\u4f2a\u5f71\u95ee\u9898\uff0c\u5728HumanML3D\u548cKIT-ML\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eU-Net\u67b6\u6784\u7684\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u4e2d\u5b58\u5728\u8bed\u4e49\u9519\u4f4d\u548c\u8fd0\u52a8\u4f2a\u5f71\u95ee\u9898\uff0c\u5206\u6790\u53d1\u73b0\u6df1\u5c42\u7f51\u7edc\u68af\u5ea6\u8870\u51cf\u662f\u4e3b\u8981\u74f6\u9888\uff0c\u5bfc\u81f4\u9ad8\u5c42\u7279\u5f81\u5b66\u4e60\u4e0d\u8db3\u3002", "method": "\u63d0\u51faLUMA\u6a21\u578b\uff0c\u5305\u542b\u53cc\u8def\u5f84\u951a\u5b9a\uff1a1\uff09\u8f7b\u91cf\u7ea7MoCLIP\u6a21\u578b\u63d0\u4f9b\u65f6\u95f4\u57df\u8bed\u4e49\u76d1\u7763\uff1b2\uff09\u4ece\u4f4e\u9891DCT\u5206\u91cf\u63d0\u53d6\u9891\u7387\u57df\u5bf9\u9f50\u4fe1\u53f7\u3002\u901a\u8fc7\u65f6\u95f4\u8c03\u5236\u673a\u5236\u81ea\u9002\u5e94\u878d\u5408\u4e24\u4e2a\u951a\u70b9\uff0c\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u4ece\u7c97\u5230\u7ec6\u7684\u8bed\u4e49\u5bf9\u9f50\u3002", "result": "\u5728HumanML3D\u548cKIT-ML\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u5230FID 0.035\u548c0.123\u7684SOTA\u6027\u80fd\uff0c\u6536\u655b\u901f\u5ea6\u6bd4\u57fa\u7ebf\u5feb1.4\u500d\u3002", "conclusion": "LUMA\u901a\u8fc7\u53cc\u8def\u5f84\u951a\u5b9a\u548c\u65f6\u95f4\u8c03\u5236\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u4e2d\u7684\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\uff0c\u662f\u4e00\u4e2a\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u9ad8\u4fdd\u771f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.26055", "pdf": "https://arxiv.org/pdf/2509.26055", "abs": "https://arxiv.org/abs/2509.26055", "authors": ["Zhenyu Shu", "Junlong Yu", "Kai Chao", "Shiqing Xin", "Ligang Liu"], "title": "GaussEdit: Adaptive 3D Scene Editing with Text and Image Prompts", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": null, "summary": "This paper presents GaussEdit, a framework for adaptive 3D scene editing guided by text and image prompts. GaussEdit leverages 3D Gaussian Splatting as its backbone for scene representation, enabling convenient Region of Interest selection and efficient editing through a three-stage process. The first stage involves initializing the 3D Gaussians to ensure high-quality edits. The second stage employs an Adaptive Global-Local Optimization strategy to balance global scene coherence and detailed local edits and a category-guided regularization technique to alleviate the Janus problem. The final stage enhances the texture of the edited objects using a sophisticated image-to-image synthesis technique, ensuring that the results are visually realistic and align closely with the given prompts. Our experimental results demonstrate that GaussEdit surpasses existing methods in editing accuracy, visual fidelity, and processing speed. By successfully embedding user-specified concepts into 3D scenes, GaussEdit is a powerful tool for detailed and user-driven 3D scene editing, offering significant improvements over traditional methods.", "AI": {"tldr": "GaussEdit\u662f\u4e00\u4e2a\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u81ea\u9002\u5e943D\u573a\u666f\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u548c\u56fe\u50cf\u63d0\u793a\u6307\u5bfc\u7f16\u8f91\u8fc7\u7a0b\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u4f18\u5316\u7b56\u7565\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u573a\u666f\u7f16\u8f91\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf3D\u573a\u666f\u7f16\u8f91\u65b9\u6cd5\u5728\u7f16\u8f91\u7cbe\u5ea6\u3001\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u5904\u7406\u901f\u5ea6\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u6709\u6548\u5d4c\u5165\u7528\u6237\u6307\u5b9a\u6982\u5ff5\u52303D\u573a\u666f\u4e2d\u7684\u5f3a\u5927\u5de5\u5177\u3002", "method": "\u4f7f\u75283D\u9ad8\u65af\u6cfc\u6e85\u4f5c\u4e3a\u573a\u666f\u8868\u793a\u9aa8\u5e72\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a1) \u521d\u59cb\u53163D\u9ad8\u65af\u786e\u4fdd\u9ad8\u8d28\u91cf\u7f16\u8f91\uff1b2) \u81ea\u9002\u5e94\u5168\u5c40-\u5c40\u90e8\u4f18\u5316\u7b56\u7565\u5e73\u8861\u573a\u666f\u4e00\u81f4\u6027\u548c\u5c40\u90e8\u7ec6\u8282\uff1b3) \u4f7f\u7528\u56fe\u50cf\u5230\u56fe\u50cf\u5408\u6210\u6280\u672f\u589e\u5f3a\u7f16\u8f91\u5bf9\u8c61\u7eb9\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eGaussEdit\u5728\u7f16\u8f91\u7cbe\u5ea6\u3001\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u5904\u7406\u901f\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GaussEdit\u662f\u4e00\u4e2a\u529f\u80fd\u5f3a\u5927\u76843D\u573a\u666f\u7f16\u8f91\u5de5\u5177\uff0c\u901a\u8fc7\u6210\u529f\u5d4c\u5165\u7528\u6237\u6307\u5b9a\u6982\u5ff5\u52303D\u573a\u666f\u4e2d\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2509.26213", "pdf": "https://arxiv.org/pdf/2509.26213", "abs": "https://arxiv.org/abs/2509.26213", "authors": ["Dominik Drees", "Benjamin Risse"], "title": "Palace: A Library for Interactive GPU-Accelerated Large Tensor Processing and Visualization", "categories": ["cs.GR"], "comment": null, "summary": "Tensor datasets (two-, three-, or higher-dimensional) are fundamental to many scientific fields utilizing imaging or simulation technologies. Advances in these methods have led to ever-increasing data sizes and, consequently, interest and development of out-of-core processing and visualization techniques, although mostly as specialized solutions. Here we present Palace, an open-source, cross-platform, general-purpose library for interactive and accelerated out-of-core tensor processing and visualization. Through a high-performance asynchronous concurrent architecture and a simple compute-graph interface, Palace enables the interactive development of out-of-core pipelines on workstation hardware. We demonstrate on benchmarks that Palace outperforms or matches state-of-the-art systems for volume rendering and hierarchical random-walker segmentation and demonstrate applicability in use cases involving tensors from 2D images up to 4D time series datasets.", "AI": {"tldr": "Palace\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u8de8\u5e73\u53f0\u901a\u7528\u5e93\uff0c\u7528\u4e8e\u4ea4\u4e92\u5f0f\u548c\u52a0\u901f\u7684\u6838\u5916\u5f20\u91cf\u5904\u7406\u548c\u53ef\u89c6\u5316\uff0c\u901a\u8fc7\u9ad8\u6027\u80fd\u5f02\u6b65\u5e76\u53d1\u67b6\u6784\u548c\u7b80\u5355\u8ba1\u7b97\u56fe\u63a5\u53e3\u5b9e\u73b0\u5de5\u4f5c\u7ad9\u786c\u4ef6\u4e0a\u7684\u4ea4\u4e92\u5f0f\u6838\u5916\u7ba1\u9053\u5f00\u53d1\u3002", "motivation": "\u968f\u7740\u6210\u50cf\u548c\u4eff\u771f\u6280\u672f\u7684\u53d1\u5c55\uff0c\u5f20\u91cf\u6570\u636e\u96c6\u89c4\u6a21\u4e0d\u65ad\u589e\u5927\uff0c\u9700\u8981\u6838\u5916\u5904\u7406\u548c\u53ef\u89c6\u5316\u6280\u672f\uff0c\u4f46\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u591a\u4e3a\u4e13\u7528\u65b9\u6848\u3002", "method": "\u91c7\u7528\u9ad8\u6027\u80fd\u5f02\u6b65\u5e76\u53d1\u67b6\u6784\u548c\u7b80\u5355\u8ba1\u7b97\u56fe\u63a5\u53e3\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u6838\u5916\u7ba1\u9053\u5f00\u53d1\u3002", "result": "\u5728\u4f53\u79ef\u6e32\u67d3\u548c\u5206\u5c42\u968f\u673a\u6e38\u8d70\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6216\u5339\u914d\u6700\u5148\u8fdb\u7cfb\u7edf\uff0c\u9002\u7528\u4e8e\u4ece2D\u56fe\u50cf\u52304D\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u7684\u5f20\u91cf\u5904\u7406\u3002", "conclusion": "Palace\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u6838\u5916\u5f20\u91cf\u5904\u7406\u548c\u53ef\u89c6\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u591a\u4e2a\u5e94\u7528\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.25348", "pdf": "https://arxiv.org/pdf/2509.25348", "abs": "https://arxiv.org/abs/2509.25348", "authors": ["Tianwen Zhou", "Akshay Paruchuri", "Josef Spjut", "Kaan Ak\u015fit"], "title": "Editing Physiological Signals in Videos Using Latent Representations", "categories": ["cs.CV", "cs.HC", "cs.MM"], "comment": "12 pages, 8 figures, 7 tables", "summary": "Camera-based physiological signal estimation provides a non-contact and convenient means to monitor Heart Rate (HR). However, the presence of vital signals in facial videos raises significant privacy concerns, as they can reveal sensitive personal information related to the health and emotional states of an individual. To address this, we propose a learned framework that edits physiological signals in videos while preserving visual fidelity. First, we encode an input video into a latent space via a pretrained 3D Variational Autoencoder (3D VAE), while a target HR prompt is embedded through a frozen text encoder. We fuse them using a set of trainable spatio-temporal layers with Adaptive Layer Normalizations (AdaLN) to capture the strong temporal coherence of remote Photoplethysmography (rPPG) signals. We apply Feature-wise Linear Modulation (FiLM) in the decoder with a fine-tuned output layer to avoid the degradation of physiological signals during reconstruction, enabling accurate physiological modulation in the reconstructed video. Empirical results show that our method preserves visual quality with an average PSNR of 38.96 dB and SSIM of 0.98 on selected datasets, while achieving an average HR modulation error of 10.00 bpm MAE and 10.09% MAPE using a state-of-the-art rPPG estimator. Our design's controllable HR editing is useful for applications such as anonymizing biometric signals in real videos or synthesizing realistic videos with desired vital signs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc73D\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u6587\u672c\u63d0\u793a\u878d\u5408\u6765\u7f16\u8f91\u89c6\u9891\u4e2d\u7684\u751f\u7406\u4fe1\u53f7\uff0c\u5728\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u7684\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\u3002", "motivation": "\u57fa\u4e8e\u6444\u50cf\u5934\u7684\u751f\u7406\u4fe1\u53f7\u4f30\u8ba1\u5b58\u5728\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u56e0\u4e3a\u9762\u90e8\u89c6\u9891\u4e2d\u7684\u751f\u547d\u4fe1\u53f7\u53ef\u80fd\u63ed\u793a\u4e2a\u4eba\u7684\u5065\u5eb7\u548c\u60c5\u7eea\u72b6\u6001\u7b49\u654f\u611f\u4fe1\u606f\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u76843D\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5c06\u8f93\u5165\u89c6\u9891\u7f16\u7801\u5230\u6f5c\u5728\u7a7a\u95f4\uff0c\u901a\u8fc7\u51bb\u7ed3\u6587\u672c\u7f16\u7801\u5668\u5d4c\u5165\u76ee\u6807\u5fc3\u7387\u63d0\u793a\uff0c\u4f7f\u7528\u53ef\u8bad\u7ec3\u7684\u65f6\u7a7a\u5c42\u548c\u81ea\u9002\u5e94\u5c42\u5f52\u4e00\u5316\u8fdb\u884c\u878d\u5408\uff0c\u5728\u89e3\u7801\u5668\u4e2d\u5e94\u7528\u7279\u5f81\u7ea7\u7ebf\u6027\u8c03\u5236\u6765\u907f\u514d\u751f\u7406\u4fe1\u53f7\u9000\u5316\u3002", "result": "\u5728\u9009\u5b9a\u6570\u636e\u96c6\u4e0a\u5e73\u5747PSNR\u4e3a38.96 dB\uff0cSSIM\u4e3a0.98\uff0c\u4f7f\u7528\u6700\u5148\u8fdb\u7684rPPG\u4f30\u8ba1\u5668\u5b9e\u73b0\u5e73\u5747\u5fc3\u7387\u8c03\u5236\u8bef\u5dee\u4e3a10.00 bpm MAE\u548c10.09% MAPE\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u53ef\u63a7\u7684\u5fc3\u7387\u7f16\u8f91\uff0c\u53ef\u7528\u4e8e\u533f\u540d\u5316\u771f\u5b9e\u89c6\u9891\u4e2d\u7684\u751f\u7269\u7279\u5f81\u4fe1\u53f7\u6216\u5408\u6210\u5177\u6709\u6240\u9700\u751f\u547d\u4f53\u5f81\u7684\u771f\u5b9e\u89c6\u9891\u3002"}}
{"id": "2509.26233", "pdf": "https://arxiv.org/pdf/2509.26233", "abs": "https://arxiv.org/abs/2509.26233", "authors": ["Balamurugan Thambiraja", "Malte Prinzler", "Sadegh Aliakbarian", "Darren Cosker", "Justus Thies"], "title": "3DiFACE: Synthesizing and Editing Holistic 3D Facial Animation", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Creating personalized 3D animations with precise control and realistic head motions remains challenging for current speech-driven 3D facial animation methods. Editing these animations is especially complex and time consuming, requires precise control and typically handled by highly skilled animators. Most existing works focus on controlling style or emotion of the synthesized animation and cannot edit/regenerate parts of an input animation. They also overlook the fact that multiple plausible lip and head movements can match the same audio input. To address these challenges, we present 3DiFACE, a novel method for holistic speech-driven 3D facial animation. Our approach produces diverse plausible lip and head motions for a single audio input and allows for editing via keyframing and interpolation. Specifically, we propose a fully-convolutional diffusion model that can leverage the viseme-level diversity in our training corpus. Additionally, we employ a speaking-style personalization and a novel sparsely-guided motion diffusion to enable precise control and editing. Through quantitative and qualitative evaluations, we demonstrate that our method is capable of generating and editing diverse holistic 3D facial animations given a single audio input, with control between high fidelity and diversity. Code and models are available here: https://balamuruganthambiraja.github.io/3DiFACE", "AI": {"tldr": "3DiFACE\u662f\u4e00\u4e2a\u7528\u4e8e\u6574\u4f53\u8bed\u97f3\u9a71\u52a83D\u9762\u90e8\u52a8\u753b\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u4e3a\u5355\u4e00\u97f3\u9891\u8f93\u5165\u751f\u6210\u591a\u6837\u5316\u7684\u5507\u90e8\u548c\u5934\u90e8\u8fd0\u52a8\uff0c\u5e76\u652f\u6301\u901a\u8fc7\u5173\u952e\u5e27\u548c\u63d2\u503c\u8fdb\u884c\u7f16\u8f91\u3002", "motivation": "\u5f53\u524d\u8bed\u97f3\u9a71\u52a83D\u9762\u90e8\u52a8\u753b\u65b9\u6cd5\u5728\u521b\u5efa\u4e2a\u6027\u5316\u52a8\u753b\u65f6\u9762\u4e34\u7cbe\u786e\u63a7\u5236\u548c\u771f\u5b9e\u5934\u90e8\u8fd0\u52a8\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u7f16\u8f91\u590d\u6742\u8017\u65f6\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u7f16\u8f91/\u91cd\u65b0\u751f\u6210\u90e8\u5206\u8f93\u5165\u52a8\u753b\uff0c\u5ffd\u7565\u4e86\u540c\u4e00\u97f3\u9891\u8f93\u5165\u53ef\u80fd\u5bf9\u5e94\u591a\u79cd\u5408\u7406\u7684\u5507\u90e8\u548c\u5934\u90e8\u8fd0\u52a8\u3002", "method": "\u63d0\u51fa\u5b8c\u5168\u5377\u79ef\u7684\u6269\u6563\u6a21\u578b\uff0c\u5229\u7528\u8bad\u7ec3\u8bed\u6599\u5e93\u4e2d\u7684\u97f3\u7d20\u7ea7\u591a\u6837\u6027\uff1b\u91c7\u7528\u8bf4\u8bdd\u98ce\u683c\u4e2a\u6027\u5316\u548c\u65b0\u9896\u7684\u7a00\u758f\u5f15\u5bfc\u8fd0\u52a8\u6269\u6563\u6765\u5b9e\u73b0\u7cbe\u786e\u63a7\u5236\u548c\u7f16\u8f91\u3002", "result": "\u901a\u8fc7\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u5355\u4e00\u97f3\u9891\u8f93\u5165\u4e0b\u751f\u6210\u548c\u7f16\u8f91\u591a\u6837\u5316\u7684\u6574\u4f533D\u9762\u90e8\u52a8\u753b\uff0c\u5e76\u5728\u9ad8\u4fdd\u771f\u5ea6\u548c\u591a\u6837\u6027\u4e4b\u95f4\u5b9e\u73b0\u63a7\u5236\u3002", "conclusion": "3DiFACE\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u53ef\u63a7\u76843D\u9762\u90e8\u52a8\u753b\u751f\u6210\u548c\u7f16\u8f91\u80fd\u529b\u3002"}}
{"id": "2509.25603", "pdf": "https://arxiv.org/pdf/2509.25603", "abs": "https://arxiv.org/abs/2509.25603", "authors": ["Yijia Weng", "Zhicheng Wang", "Songyou Peng", "Saining Xie", "Howard Zhou", "Leonidas J. Guibas"], "title": "GaussianLens: Localized High-Resolution Reconstruction via On-Demand Gaussian Densification", "categories": ["cs.CV"], "comment": null, "summary": "We perceive our surroundings with an active focus, paying more attention to regions of interest, such as the shelf labels in a grocery store. When it comes to scene reconstruction, this human perception trait calls for spatially varying degrees of detail ready for closer inspection in critical regions, preferably reconstructed on demand. While recent works in 3D Gaussian Splatting (3DGS) achieve fast, generalizable reconstruction from sparse views, their uniform resolution output leads to high computational costs unscalable to high-resolution training. As a result, they cannot leverage available images at their original high resolution to reconstruct details. Per-scene optimization methods reconstruct finer details with adaptive density control, yet require dense observations and lengthy offline optimization. To bridge the gap between the prohibitive cost of high-resolution holistic reconstructions and the user needs for localized fine details, we propose the problem of localized high-resolution reconstruction via on-demand Gaussian densification. Given a low-resolution 3DGS reconstruction, the goal is to learn a generalizable network that densifies the initial 3DGS to capture fine details in a user-specified local region of interest (RoI), based on sparse high-resolution observations of the RoI. This formulation avoids the high cost and redundancy of uniformly high-resolution reconstructions and fully leverages high-resolution captures in critical regions. We propose GaussianLens, a feed-forward densification framework that fuses multi-modal information from the initial 3DGS and multi-view images. We further design a pixel-guided densification mechanism that effectively captures details under large resolution increases. Experiments demonstrate our method's superior performance in local fine detail reconstruction and strong scalability to images of up to $1024\\times1024$ resolution.", "AI": {"tldr": "\u63d0\u51fa\u4e86GaussianLens\u65b9\u6cd5\uff0c\u901a\u8fc7\u6309\u9700\u9ad8\u65af\u81f4\u5bc6\u5316\u5b9e\u73b0\u5c40\u90e8\u9ad8\u5206\u8fa8\u7387\u91cd\u5efa\uff0c\u5728\u7528\u6237\u6307\u5b9a\u7684\u611f\u5174\u8da3\u533a\u57df\u57fa\u4e8e\u7a00\u758f\u9ad8\u5206\u8fa8\u7387\u89c2\u6d4b\u6765\u589e\u5f3a\u521d\u59cb\u4f4e\u5206\u8fa8\u73873DGS\u91cd\u5efa\uff0c\u907f\u514d\u7edf\u4e00\u9ad8\u5206\u8fa8\u7387\u91cd\u5efa\u7684\u9ad8\u6210\u672c\u3002", "motivation": "\u4eba\u7c7b\u611f\u77e5\u5177\u6709\u7a7a\u95f4\u53d8\u5316\u7684\u7ec6\u8282\u5173\u6ce8\u7279\u6027\uff0c\u9700\u8981\u4e3a\u5173\u952e\u533a\u57df\u63d0\u4f9b\u53ef\u68c0\u67e5\u7684\u7cbe\u7ec6\u7ec6\u8282\u3002\u73b0\u67093DGS\u65b9\u6cd5\u8981\u4e48\u8f93\u51fa\u5747\u5300\u5206\u8fa8\u7387\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u8981\u4e48\u9700\u8981\u5bc6\u96c6\u89c2\u6d4b\u548c\u957f\u65f6\u95f4\u79bb\u7ebf\u4f18\u5316\u3002", "method": "GaussianLens\u524d\u5411\u81f4\u5bc6\u5316\u6846\u67b6\uff0c\u878d\u5408\u521d\u59cb3DGS\u548c\u591a\u89c6\u56fe\u56fe\u50cf\u7684\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u8bbe\u8ba1\u50cf\u7d20\u5f15\u5bfc\u7684\u81f4\u5bc6\u5316\u673a\u5236\u6765\u6709\u6548\u6355\u6349\u5927\u5206\u8fa8\u7387\u589e\u52a0\u4e0b\u7684\u7ec6\u8282\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5c40\u90e8\u7cbe\u7ec6\u7ec6\u8282\u91cd\u5efa\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u5bf9\u9ad8\u8fbe1024\u00d71024\u5206\u8fa8\u7387\u7684\u56fe\u50cf\u5177\u6709\u5f3a\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u901a\u8fc7\u6309\u9700\u5c40\u90e8\u9ad8\u5206\u8fa8\u7387\u91cd\u5efa\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u9ad8\u5206\u8fa8\u7387\u6574\u4f53\u91cd\u5efa\u7684\u9ad8\u6210\u672c\u4e0e\u7528\u6237\u5bf9\u5c40\u90e8\u7cbe\u7ec6\u7ec6\u8282\u9700\u6c42\u4e4b\u95f4\u7684\u77db\u76fe\u3002"}}
{"id": "2509.25705", "pdf": "https://arxiv.org/pdf/2509.25705", "abs": "https://arxiv.org/abs/2509.25705", "authors": ["Juyeop Kim", "Songkuk Kim", "Jong-Seok Lee"], "title": "How Diffusion Models Memorize", "categories": ["cs.CV"], "comment": null, "summary": "Despite their success in image generation, diffusion models can memorize training data, raising serious privacy and copyright concerns. Although prior work has sought to characterize, detect, and mitigate memorization, the fundamental question of why and how it occurs remains unresolved. In this paper, we revisit the diffusion and denoising process and analyze latent space dynamics to address the question: \"How do diffusion models memorize?\" We show that memorization is driven by the overestimation of training samples during early denoising, which reduces diversity, collapses denoising trajectories, and accelerates convergence toward the memorized image. Specifically: (i) memorization cannot be explained by overfitting alone, as training loss is larger under memorization due to classifier-free guidance amplifying predictions and inducing overestimation; (ii) memorized prompts inject training images into noise predictions, forcing latent trajectories to converge and steering denoising toward their paired samples; and (iii) a decomposition of intermediate latents reveals how initial randomness is quickly suppressed and replaced by memorized content, with deviations from the theoretical denoising schedule correlating almost perfectly with memorization severity. Together, these results identify early overestimation as the central underlying mechanism of memorization in diffusion models.", "AI": {"tldr": "\u6269\u6563\u6a21\u578b\u4f1a\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\uff0c\u672c\u6587\u901a\u8fc7\u5206\u6790\u53bb\u566a\u8fc7\u7a0b\u4e2d\u7684\u6f5c\u5728\u7a7a\u95f4\u52a8\u6001\uff0c\u53d1\u73b0\u8bb0\u5fc6\u4e3b\u8981\u7531\u65e9\u671f\u53bb\u566a\u9636\u6bb5\u5bf9\u8bad\u7ec3\u6837\u672c\u7684\u9ad8\u4f30\u9a71\u52a8\uff0c\u8fd9\u964d\u4f4e\u4e86\u591a\u6837\u6027\u3001\u4f7f\u53bb\u566a\u8f68\u8ff9\u574d\u7f29\u5e76\u52a0\u901f\u5411\u8bb0\u5fc6\u56fe\u50cf\u7684\u6536\u655b\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u4f1a\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\uff0c\u5f15\u53d1\u9690\u79c1\u548c\u7248\u6743\u95ee\u9898\u3002\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u8868\u5f81\u3001\u68c0\u6d4b\u548c\u7f13\u89e3\u8bb0\u5fc6\uff0c\u4f46\u5bf9\u8bb0\u5fc6\u53d1\u751f\u7684\u57fa\u672c\u539f\u56e0\u548c\u673a\u5236\u4ecd\u4e0d\u6e05\u695a\u3002", "method": "\u91cd\u65b0\u5ba1\u89c6\u6269\u6563\u548c\u53bb\u566a\u8fc7\u7a0b\uff0c\u5206\u6790\u6f5c\u5728\u7a7a\u95f4\u52a8\u6001\uff0c\u7814\u7a76\uff1a(i)\u8bb0\u5fc6\u4e0d\u80fd\u4ec5\u7528\u8fc7\u62df\u5408\u89e3\u91ca\uff1b(ii)\u8bb0\u5fc6\u63d0\u793a\u5982\u4f55\u5c06\u8bad\u7ec3\u56fe\u50cf\u6ce8\u5165\u566a\u58f0\u9884\u6d4b\uff1b(iii)\u5206\u89e3\u4e2d\u95f4\u6f5c\u5728\u53d8\u91cf\u63ed\u793a\u968f\u673a\u6027\u5982\u4f55\u88ab\u6291\u5236\u5e76\u88ab\u8bb0\u5fc6\u5185\u5bb9\u66ff\u4ee3\u3002", "result": "\u53d1\u73b0\u65e9\u671f\u9ad8\u4f30\u662f\u6269\u6563\u6a21\u578b\u8bb0\u5fc6\u7684\u6838\u5fc3\u673a\u5236\uff1a\u8bad\u7ec3\u635f\u5931\u5728\u8bb0\u5fc6\u65f6\u66f4\u5927\uff1b\u8bb0\u5fc6\u63d0\u793a\u5f3a\u5236\u6f5c\u5728\u8f68\u8ff9\u6536\u655b\uff1b\u504f\u79bb\u7406\u8bba\u53bb\u566a\u8ba1\u5212\u4e0e\u8bb0\u5fc6\u4e25\u91cd\u7a0b\u5ea6\u9ad8\u5ea6\u76f8\u5173\u3002", "conclusion": "\u65e9\u671f\u9ad8\u4f30\u662f\u6269\u6563\u6a21\u578b\u4e2d\u8bb0\u5fc6\u73b0\u8c61\u7684\u6839\u672c\u9a71\u52a8\u673a\u5236\uff0c\u8fd9\u4e3a\u7406\u89e3\u548c\u89e3\u51b3\u6269\u6563\u6a21\u578b\u7684\u8bb0\u5fc6\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u89c6\u89d2\u3002"}}
{"id": "2509.25739", "pdf": "https://arxiv.org/pdf/2509.25739", "abs": "https://arxiv.org/abs/2509.25739", "authors": ["Donghwan Kim", "Tae-Kyun Kim"], "title": "LieHMR: Autoregressive Human Mesh Recovery with $SO(3)$ Diffusion", "categories": ["cs.CV"], "comment": "17 pages, 13 figures", "summary": "We tackle the problem of Human Mesh Recovery (HMR) from a single RGB image, formulating it as an image-conditioned human pose and shape generation. While recovering 3D human pose from 2D observations is inherently ambiguous, most existing approaches have regressed a single deterministic output. Probabilistic methods attempt to address this by generating multiple plausible outputs to model the ambiguity. However, these methods often exhibit a trade-off between accuracy and sample diversity, and their single predictions are not competitive with state-of-the-art deterministic models. To overcome these limitations, we propose a novel approach that models well-aligned distribution to 2D observations. In particular, we introduce $SO(3)$ diffusion model, which generates the distribution of pose parameters represented as 3D rotations unconditional and conditional to image observations via conditioning dropout. Our model learns the hierarchical structure of human body joints using the transformer. Instead of using transformer as a denoising model, the time-independent transformer extracts latent vectors for the joints and a small MLP-based denoising model learns the per-joint distribution conditioned on the latent vector. We experimentally demonstrate and analyze that our model predicts accurate pose probability distribution effectively.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eSO(3)\u6269\u6563\u6a21\u578b\u7684\u4eba\u4f53\u7f51\u683c\u6062\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u6761\u4ef6dropout\u751f\u6210\u7b26\u54082D\u89c2\u6d4b\u76843D\u4eba\u4f53\u59ff\u6001\u5206\u5e03\uff0c\u89e3\u51b3\u5355\u76eeRGB\u56fe\u50cf\u4e2d3D\u4eba\u4f53\u59ff\u6001\u6062\u590d\u7684\u6a21\u7cca\u6027\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5355\u76eeRGB\u56fe\u50cf\u4e2d3D\u4eba\u4f53\u59ff\u6001\u6062\u590d\u7684\u56fa\u6709\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u73b0\u6709\u6982\u7387\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6837\u672c\u591a\u6837\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u4e14\u5355\u9884\u6d4b\u7ed3\u679c\u4e0d\u5982\u6700\u5148\u8fdb\u7684\u786e\u5b9a\u6027\u6a21\u578b\u3002", "method": "\u4f7f\u7528SO(3)\u6269\u6563\u6a21\u578b\u751f\u6210\u4ee53D\u65cb\u8f6c\u8868\u793a\u7684\u59ff\u6001\u53c2\u6570\u5206\u5e03\uff0c\u901a\u8fc7\u6761\u4ef6dropout\u5b9e\u73b0\u65e0\u6761\u4ef6\u548c\u56fe\u50cf\u6761\u4ef6\u751f\u6210\uff0c\u5229\u7528transformer\u5b66\u4e60\u4eba\u4f53\u5173\u8282\u7684\u5c42\u6b21\u7ed3\u6784\uff0cMLP\u53bb\u566a\u6a21\u578b\u5b66\u4e60\u57fa\u4e8e\u6f5c\u5728\u5411\u91cf\u7684\u5173\u8282\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u9884\u6d4b\u51c6\u786e\u7684\u59ff\u6001\u6982\u7387\u5206\u5e03\u3002", "conclusion": "\u63d0\u51fa\u7684SO(3)\u6269\u6563\u6a21\u578b\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5efa\u6a21\u4e0e2D\u89c2\u6d4b\u5bf9\u9f50\u76843D\u4eba\u4f53\u59ff\u6001\u5206\u5e03\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6982\u7387\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2509.25749", "pdf": "https://arxiv.org/pdf/2509.25749", "abs": "https://arxiv.org/abs/2509.25749", "authors": ["Junseo Park", "Hyeryung Jang"], "title": "ART-VITON: Measurement-Guided Latent Diffusion for Artifact-Free Virtual Try-On", "categories": ["cs.CV"], "comment": "21 pages", "summary": "Virtual try-on (VITON) aims to generate realistic images of a person wearing a target garment, requiring precise garment alignment in try-on regions and faithful preservation of identity and background in non-try-on regions. While latent diffusion models (LDMs) have advanced alignment and detail synthesis, preserving non-try-on regions remains challenging. A common post-hoc strategy directly replaces these regions with original content, but abrupt transitions often produce boundary artifacts. To overcome this, we reformulate VITON as a linear inverse problem and adopt trajectory-aligned solvers that progressively enforce measurement consistency, reducing abrupt changes in non-try-on regions. However, existing solvers still suffer from semantic drift during generation, leading to artifacts. We propose ART-VITON, a measurement-guided diffusion framework that ensures measurement adherence while maintaining artifact-free synthesis. Our method integrates residual prior-based initialization to mitigate training-inference mismatch and artifact-free measurement-guided sampling that combines data consistency, frequency-level correction, and periodic standard denoising. Experiments on VITON-HD, DressCode, and SHHQ-1.0 demonstrate that ART-VITON effectively preserves identity and background, eliminates boundary artifacts, and consistently improves visual fidelity and robustness over state-of-the-art baselines.", "AI": {"tldr": "ART-VITON\u662f\u4e00\u4e2a\u57fa\u4e8e\u6d4b\u91cf\u5f15\u5bfc\u7684\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u865a\u62df\u8bd5\u7a7f\u4efb\u52a1\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u6d4b\u91cf\u4e00\u81f4\u6027\u786e\u4fdd\u975e\u8bd5\u7a7f\u533a\u57df\u7684\u4fdd\u771f\u5ea6\uff0c\u6d88\u9664\u8fb9\u754c\u4f2a\u5f71\u3002", "motivation": "\u73b0\u6709\u7684\u865a\u62df\u8bd5\u7a7f\u65b9\u6cd5\u5728\u4f7f\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u65f6\uff0c\u96be\u4ee5\u6709\u6548\u4fdd\u6301\u975e\u8bd5\u7a7f\u533a\u57df\u7684\u8eab\u4efd\u548c\u80cc\u666f\u4fe1\u606f\uff0c\u76f4\u63a5\u66ff\u6362\u7b56\u7565\u4f1a\u5bfc\u81f4\u8fb9\u754c\u4f2a\u5f71\u95ee\u9898\u3002", "method": "\u5c06\u865a\u62df\u8bd5\u7a7f\u91cd\u65b0\u8868\u8ff0\u4e3a\u7ebf\u6027\u9006\u95ee\u9898\uff0c\u91c7\u7528\u8f68\u8ff9\u5bf9\u9f50\u6c42\u89e3\u5668\uff0c\u7ed3\u5408\u6b8b\u5dee\u5148\u9a8c\u521d\u59cb\u5316\u548c\u65e0\u4f2a\u5f71\u6d4b\u91cf\u5f15\u5bfc\u91c7\u6837\uff0c\u5305\u62ec\u6570\u636e\u4e00\u81f4\u6027\u3001\u9891\u7387\u7ea7\u6821\u6b63\u548c\u5468\u671f\u6027\u6807\u51c6\u53bb\u566a\u3002", "result": "\u5728VITON-HD\u3001DressCode\u548cSHHQ-1.0\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cART-VITON\u80fd\u6709\u6548\u4fdd\u6301\u8eab\u4efd\u548c\u80cc\u666f\uff0c\u6d88\u9664\u8fb9\u754c\u4f2a\u5f71\uff0c\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ART-VITON\u901a\u8fc7\u6d4b\u91cf\u5f15\u5bfc\u7684\u6269\u6563\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u865a\u62df\u8bd5\u7a7f\u4e2d\u7684\u975e\u8bd5\u7a7f\u533a\u57df\u4fdd\u771f\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u8bd5\u7a7f\u6548\u679c\u3002"}}
{"id": "2509.25774", "pdf": "https://arxiv.org/pdf/2509.25774", "abs": "https://arxiv.org/abs/2509.25774", "authors": ["Jeongjae Lee", "Jong Chul Ye"], "title": "PCPO: Proportionate Credit Policy Optimization for Aligning Image Generation Models", "categories": ["cs.CV"], "comment": "24 pages, 17 figures", "summary": "While reinforcement learning has advanced the alignment of text-to-image (T2I) models, state-of-the-art policy gradient methods are still hampered by training instability and high variance, hindering convergence speed and compromising image quality. Our analysis identifies a key cause of this instability: disproportionate credit assignment, in which the mathematical structure of the generative sampler produces volatile and non-proportional feedback across timesteps. To address this, we introduce Proportionate Credit Policy Optimization (PCPO), a framework that enforces proportional credit assignment through a stable objective reformulation and a principled reweighting of timesteps. This correction stabilizes the training process, leading to significantly accelerated convergence and superior image quality. The improvement in quality is a direct result of mitigating model collapse, a common failure mode in recursive training. PCPO substantially outperforms existing policy gradient baselines on all fronts, including the state-of-the-art DanceGRPO.", "AI": {"tldr": "\u63d0\u51fa\u4e86PCPO\u6846\u67b6\uff0c\u901a\u8fc7\u6bd4\u4f8b\u4fe1\u7528\u5206\u914d\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u9ad8\u65b9\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6536\u655b\u901f\u5ea6\u548c\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u6700\u5148\u8fdb\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u5728\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5bf9\u9f50\u4e2d\u53d7\u5230\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u548c\u9ad8\u65b9\u5dee\u7684\u56f0\u6270\uff0c\u8fd9\u963b\u788d\u4e86\u6536\u655b\u901f\u5ea6\u5e76\u635f\u5bb3\u4e86\u56fe\u50cf\u8d28\u91cf\u3002\u5206\u6790\u53d1\u73b0\u4e3b\u8981\u539f\u56e0\u662f\u751f\u6210\u91c7\u6837\u5668\u7684\u6570\u5b66\u7ed3\u6784\u5bfc\u81f4\u8de8\u65f6\u95f4\u6b65\u7684\u4fe1\u7528\u5206\u914d\u4e0d\u6210\u6bd4\u4f8b\u3002", "method": "\u5f15\u5165\u6bd4\u4f8b\u4fe1\u7528\u7b56\u7565\u4f18\u5316(PCPO)\u6846\u67b6\uff0c\u901a\u8fc7\u7a33\u5b9a\u7684\u76ee\u6807\u91cd\u6784\u548c\u539f\u5219\u6027\u7684\u65f6\u95f4\u6b65\u91cd\u65b0\u52a0\u6743\u6765\u5f3a\u5236\u6267\u884c\u6bd4\u4f8b\u4fe1\u7528\u5206\u914d\u3002", "result": "PCPO\u663e\u8457\u7a33\u5b9a\u4e86\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5927\u5927\u52a0\u901f\u4e86\u6536\u655b\u5e76\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u56fe\u50cf\u8d28\u91cf\u3002\u901a\u8fc7\u51cf\u8f7b\u6a21\u578b\u5d29\u6e83\u8fd9\u4e00\u9012\u5f52\u8bad\u7ec3\u4e2d\u7684\u5e38\u89c1\u6545\u969c\u6a21\u5f0f\u6765\u76f4\u63a5\u63d0\u5347\u8d28\u91cf\u3002", "conclusion": "PCPO\u5728\u6240\u6709\u65b9\u9762\u90fd\u5927\u5e45\u4f18\u4e8e\u73b0\u6709\u7684\u7b56\u7565\u68af\u5ea6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5305\u62ec\u6700\u5148\u8fdb\u7684DanceGRPO\u3002"}}
{"id": "2509.25776", "pdf": "https://arxiv.org/pdf/2509.25776", "abs": "https://arxiv.org/abs/2509.25776", "authors": ["Mingyu Kang", "Yong Suk Choi"], "title": "Editable Noise Map Inversion: Encoding Target-image into Noise For High-Fidelity Image Manipulation", "categories": ["cs.CV", "cs.AI"], "comment": "ICML 2025", "summary": "Text-to-image diffusion models have achieved remarkable success in generating high-quality and diverse images. Building on these advancements, diffusion models have also demonstrated exceptional performance in text-guided image editing. A key strategy for effective image editing involves inverting the source image into editable noise maps associated with the target image. However, previous inversion methods face challenges in adhering closely to the target text prompt. The limitation arises because inverted noise maps, while enabling faithful reconstruction of the source image, restrict the flexibility needed for desired edits. To overcome this issue, we propose Editable Noise Map Inversion (ENM Inversion), a novel inversion technique that searches for optimal noise maps to ensure both content preservation and editability. We analyze the properties of noise maps for enhanced editability. Based on this analysis, our method introduces an editable noise refinement that aligns with the desired edits by minimizing the difference between the reconstructed and edited noise maps. Extensive experiments demonstrate that ENM Inversion outperforms existing approaches across a wide range of image editing tasks in both preservation and edit fidelity with target prompts. Our approach can also be easily applied to video editing, enabling temporal consistency and content manipulation across frames.", "AI": {"tldr": "\u63d0\u51faENM Inversion\u65b9\u6cd5\uff0c\u901a\u8fc7\u641c\u7d22\u6700\u4f18\u566a\u58f0\u56fe\u6765\u5e73\u8861\u5185\u5bb9\u4fdd\u6301\u548c\u7f16\u8f91\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6269\u6563\u6a21\u578b\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5728\u9075\u5faa\u76ee\u6807\u6587\u672c\u63d0\u793a\u65b9\u9762\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5728\u5c06\u6e90\u56fe\u50cf\u53cd\u8f6c\u4e3a\u53ef\u7f16\u8f91\u566a\u58f0\u56fe\u65f6\uff0c\u96be\u4ee5\u7d27\u5bc6\u9075\u5faa\u76ee\u6807\u6587\u672c\u63d0\u793a\uff0c\u56e0\u4e3a\u53cd\u8f6c\u7684\u566a\u58f0\u56fe\u867d\u7136\u80fd\u5fe0\u5b9e\u91cd\u5efa\u6e90\u56fe\u50cf\uff0c\u4f46\u9650\u5236\u4e86\u7f16\u8f91\u7075\u6d3b\u6027\u3002", "method": "\u5206\u6790\u566a\u58f0\u56fe\u7279\u6027\u4ee5\u589e\u5f3a\u53ef\u7f16\u8f91\u6027\uff0c\u63d0\u51fa\u53ef\u7f16\u8f91\u566a\u58f0\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u91cd\u5efa\u548c\u7f16\u8f91\u566a\u58f0\u56fe\u4e4b\u95f4\u7684\u5dee\u5f02\u6765\u4e0e\u671f\u671b\u7f16\u8f91\u5bf9\u9f50\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e2d\uff0cENM Inversion\u5728\u5185\u5bb9\u4fdd\u6301\u548c\u7f16\u8f91\u4fdd\u771f\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u8f7b\u677e\u5e94\u7528\u4e8e\u89c6\u9891\u7f16\u8f91\uff0c\u5b9e\u73b0\u8de8\u5e27\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u5185\u5bb9\u64cd\u4f5c\u3002", "conclusion": "ENM Inversion\u662f\u4e00\u79cd\u6709\u6548\u7684\u566a\u58f0\u56fe\u53cd\u8f6c\u6280\u672f\uff0c\u80fd\u591f\u540c\u65f6\u786e\u4fdd\u5185\u5bb9\u4fdd\u6301\u548c\u7f16\u8f91\u80fd\u529b\uff0c\u5728\u6587\u672c\u5f15\u5bfc\u7684\u56fe\u50cf\u548c\u89c6\u9891\u7f16\u8f91\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.25805", "pdf": "https://arxiv.org/pdf/2509.25805", "abs": "https://arxiv.org/abs/2509.25805", "authors": ["Xintong Jiang", "Yixue Liu", "Mohamed Debbagh", "Yu Tian", "Valerio Hoyos-Villegas", "Viacheslav Adamchuk", "Shangpeng Sun"], "title": "Adapting SAM with Dynamic Similarity Graphs for Few-Shot Parameter-Efficient Small Dense Object Detection: A Case Study of Chickpea Pods in Field Conditions", "categories": ["cs.CV", "I.4.6; I.2.10; I.5.1; I.4.8"], "comment": "23 pages, 11 figures, 4 tables", "summary": "Parameter-Efficient Fine-Tuning (PEFT) of foundation models for agricultural computer vision tasks remains challenging due to limited training data and complex field conditions. This study introduces a Dynamic Similarity-based Graph Adaptation (DSGA) module to adapt the Segment Anything Model (SAM) under extreme data constraints for precise foreground and instance segmentation of small dense objects in complex agricultural environments. Through dynamic similarity graph construction with a learnable polynomial decay-initialized weight ranking mechanism and adaptive local feature aggregation, DSGA establishes robust spatial and dynamic similarity representation with only 4.00M trainable parameters, which is 4.26% of the original SAM. Integrating this graph-based feature adaptation with Low-Rank Adaptation (LoRA) creates a complementary optimization framework that effectively captures both local and global dependencies in image embeddings while preserving model stability and parameter efficiency. Experimental results on a challenging chickpea pod dataset demonstrated that DSGA with LoRA achieved superior performance across multiple metrics evaluated under 2, 4, 8 and 10 shots, with progressive performance gains as shot count increased. Quantitative metrics showed a 17.31% improvement in Structure-measure and a 62.36% gain in adaptive F-measure compared to the baseline SAM fine-tuning. Comprehensive ablation studies and visualization analyses through Grad-CAM and t-SNE validated the framework's effectiveness in feature discrimination. The proposed adaptation demonstrated practical utility for automated agricultural monitoring applications, achieving accurate pod-counting with an adjusted R-squared of 0.8987 for images with 10 to 120 pods under challenging field conditions.", "AI": {"tldr": "\u63d0\u51faDSGA\u6a21\u5757\u7ed3\u5408LoRA\uff0c\u5728\u6781\u5c11\u91cf\u6570\u636e\u4e0b\u4f18\u5316SAM\u6a21\u578b\uff0c\u5b9e\u73b0\u519c\u4e1a\u56fe\u50cf\u4e2d\u5bc6\u96c6\u5c0f\u7269\u4f53\u7684\u7cbe\u786e\u5206\u5272\uff0c\u4ec5\u97004.00M\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u519c\u4e1a\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\uff0c\u57fa\u7840\u6a21\u578b\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u9762\u4e34\u8bad\u7ec3\u6570\u636e\u6709\u9650\u548c\u590d\u6742\u7530\u95f4\u6761\u4ef6\u7684\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u5728\u6781\u7aef\u6570\u636e\u7ea6\u675f\u4e0b\u4ecd\u80fd\u6709\u6548\u5de5\u4f5c\u7684\u9002\u5e94\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u52a8\u6001\u76f8\u4f3c\u6027\u56fe\u9002\u5e94(DSGA)\u6a21\u5757\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u591a\u9879\u5f0f\u8870\u51cf\u521d\u59cb\u5316\u6743\u91cd\u6392\u5e8f\u673a\u5236\u6784\u5efa\u52a8\u6001\u76f8\u4f3c\u6027\u56fe\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u5c40\u90e8\u7279\u5f81\u805a\u5408\uff0c\u5e76\u4e0eLoRA\u96c6\u6210\u5f62\u6210\u4e92\u8865\u4f18\u5316\u6846\u67b6\u3002", "result": "\u5728\u9e70\u5634\u8c46\u835a\u6570\u636e\u96c6\u4e0a\uff0cDSGA+LoRA\u57282\u30014\u30018\u300110\u6837\u672c\u8bbe\u7f6e\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u7ed3\u6784\u5ea6\u91cf\u63d0\u534717.31%\uff0c\u81ea\u9002\u5e94F\u5ea6\u91cf\u63d0\u534762.36%\uff0c\u572810-120\u4e2a\u8c46\u835a\u7684\u56fe\u50cf\u4e2d\u5b9e\u73b0\u51c6\u786e\u8ba1\u6570(R\u00b2=0.8987)\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u52a8\u5316\u519c\u4e1a\u76d1\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u53c2\u6570\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u519c\u4e1a\u73af\u5883\u4e0b\u5c0f\u5bc6\u96c6\u7269\u4f53\u7684\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2509.25845", "pdf": "https://arxiv.org/pdf/2509.25845", "abs": "https://arxiv.org/abs/2509.25845", "authors": ["Jinho Chang", "Jaemin Kim", "Jong Chul Ye"], "title": "Training-Free Reward-Guided Image Editing via Trajectory Optimal Control", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages, 5 figures", "summary": "Recent advancements in diffusion and flow-matching models have demonstrated remarkable capabilities in high-fidelity image synthesis. A prominent line of research involves reward-guided guidance, which steers the generation process during inference to align with specific objectives. However, leveraging this reward-guided approach to the task of image editing, which requires preserving the semantic content of the source image while enhancing a target reward, is largely unexplored. In this work, we introduce a novel framework for training-free, reward-guided image editing. We formulate the editing process as a trajectory optimal control problem where the reverse process of a diffusion model is treated as a controllable trajectory originating from the source image, and the adjoint states are iteratively updated to steer the editing process. Through extensive experiments across distinct editing tasks, we demonstrate that our approach significantly outperforms existing inversion-based training-free guidance baselines, achieving a superior balance between reward maximization and fidelity to the source image without reward hacking.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u57fa\u4e8e\u5956\u52b1\u5f15\u5bfc\u7684\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u5c06\u7f16\u8f91\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u8f68\u8ff9\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6e90\u56fe\u50cf\u8bed\u4e49\u5185\u5bb9\u7684\u540c\u65f6\u589e\u5f3a\u76ee\u6807\u5956\u52b1\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5956\u52b1\u5f15\u5bfc\u7684\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u56fe\u50cf\u751f\u6210\uff0c\u800c\u5728\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e2d\u5982\u4f55\u4fdd\u6301\u6e90\u56fe\u50cf\u5185\u5bb9\u540c\u65f6\u589e\u5f3a\u76ee\u6807\u5956\u52b1\u7684\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u5c06\u6269\u6563\u6a21\u578b\u7684\u53cd\u5411\u8fc7\u7a0b\u89c6\u4e3a\u4ece\u6e90\u56fe\u50cf\u51fa\u53d1\u7684\u53ef\u63a7\u8f68\u8ff9\uff0c\u901a\u8fc7\u8fed\u4ee3\u66f4\u65b0\u4f34\u968f\u72b6\u6001\u6765\u5f15\u5bfc\u7f16\u8f91\u8fc7\u7a0b\uff0c\u5f62\u6210\u8f68\u8ff9\u6700\u4f18\u63a7\u5236\u95ee\u9898\u3002", "result": "\u5728\u591a\u79cd\u7f16\u8f91\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u53cd\u8f6c\u7684\u65e0\u8bad\u7ec3\u5f15\u5bfc\u57fa\u7ebf\uff0c\u5728\u5956\u52b1\u6700\u5927\u5316\u548c\u6e90\u56fe\u50cf\u4fdd\u771f\u5ea6\u4e4b\u95f4\u8fbe\u5230\u66f4\u597d\u7684\u5e73\u8861\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5956\u52b1\u5f15\u5bfc\u7684\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65e0\u9700\u8bad\u7ec3\u89e3\u51b3\u65b9\u6848\uff0c\u907f\u514d\u4e86\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u7f16\u8f91\u6548\u679c\u3002"}}
{"id": "2509.25859", "pdf": "https://arxiv.org/pdf/2509.25859", "abs": "https://arxiv.org/abs/2509.25859", "authors": ["Pasindu Ranasinghe", "Dibyayan Patra", "Bikram Banerjee", "Simit Raval"], "title": "LiDAR Point Cloud Colourisation Using Multi-Camera Fusion and Low-Light Image Enhancement", "categories": ["cs.CV", "cs.SY", "eess.SY"], "comment": null, "summary": "In recent years, the fusion of camera data with LiDAR measurements has emerged as a powerful approach to enhance spatial understanding. This study introduces a novel, hardware-agnostic methodology that generates colourised point clouds from mechanical LiDAR using multiple camera inputs, providing complete 360-degree coverage. The primary innovation lies in its robustness under low-light conditions, achieved through the integration of a low-light image enhancement module within the fusion pipeline. The system requires initial calibration to determine intrinsic camera parameters, followed by automatic computation of the geometric transformation between the LiDAR and cameras, removing the need for specialised calibration targets and streamlining the setup. The data processing framework uses colour correction to ensure uniformity across camera feeds before fusion. The algorithm was tested using a Velodyne Puck Hi-Res LiDAR and a four-camera configuration. The optimised software achieved real-time performance and reliable colourisation even under very low illumination, successfully recovering scene details that would otherwise remain undetectable.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u786c\u4ef6\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u591a\u4e2a\u6444\u50cf\u5934\u8f93\u5165\u4e3a\u673a\u68b0LiDAR\u751f\u6210\u5f69\u8272\u70b9\u4e91\uff0c\u5b9e\u73b0360\u5ea6\u8986\u76d6\uff0c\u5e76\u5728\u4f4e\u5149\u7167\u6761\u4ef6\u4e0b\u901a\u8fc7\u96c6\u6210\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u6a21\u5757\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "motivation": "\u878d\u5408\u6444\u50cf\u5934\u548cLiDAR\u6570\u636e\u4ee5\u589e\u5f3a\u7a7a\u95f4\u7406\u89e3\uff0c\u7279\u522b\u662f\u5728\u4f4e\u5149\u7167\u6761\u4ef6\u4e0b\u4fdd\u6301\u53ef\u9760\u6027\u80fd\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u5f31\u5149\u73af\u5883\u4e0b\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u786c\u4ef6\u65e0\u5173\u65b9\u6cd5\uff0c\u901a\u8fc7\u521d\u59cb\u6821\u51c6\u786e\u5b9a\u76f8\u673a\u5185\u53c2\uff0c\u81ea\u52a8\u8ba1\u7b97LiDAR\u4e0e\u76f8\u673a\u95f4\u7684\u51e0\u4f55\u53d8\u6362\uff0c\u65e0\u9700\u4e13\u7528\u6821\u51c6\u76ee\u6807\uff1b\u96c6\u6210\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u6a21\u5757\uff0c\u4f7f\u7528\u8272\u5f69\u6821\u6b63\u786e\u4fdd\u76f8\u673a\u6570\u636e\u4e00\u81f4\u6027\u3002", "result": "\u4f7f\u7528Velodyne Puck Hi-Res LiDAR\u548c\u56db\u6444\u50cf\u5934\u914d\u7f6e\u6d4b\u8bd5\uff0c\u4f18\u5316\u8f6f\u4ef6\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\uff0c\u5728\u6781\u4f4e\u5149\u7167\u4e0b\u4ecd\u80fd\u53ef\u9760\u7740\u8272\uff0c\u6210\u529f\u6062\u590d\u539f\u672c\u65e0\u6cd5\u68c0\u6d4b\u7684\u573a\u666f\u7ec6\u8282\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u7684LiDAR\u70b9\u4e91\u7740\u8272\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4f4e\u5149\u7167\u73af\u5883\uff0c\u7b80\u5316\u4e86\u8bbe\u7f6e\u6d41\u7a0b\u5e76\u5b9e\u73b0\u4e86\u5b9e\u65f6\u5904\u7406\u3002"}}
{"id": "2509.25927", "pdf": "https://arxiv.org/pdf/2509.25927", "abs": "https://arxiv.org/abs/2509.25927", "authors": ["Marco Zimmerli", "Andreas Plesner", "Till Aczel", "Roger Wattenhofer"], "title": "The Impact of Scaling Training Data on Adversarial Robustness", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG"], "comment": "Accepted at the workshop Reliable ML from Unreliable Data at NeurIPS   2025", "summary": "Deep neural networks remain vulnerable to adversarial examples despite advances in architectures and training paradigms. We investigate how training data characteristics affect adversarial robustness across 36 state-of-the-art vision models spanning supervised, self-supervised, and contrastive learning approaches, trained on datasets from 1.2M to 22B images. Models were evaluated under six black-box attack categories: random perturbations, two types of geometric masks, COCO object manipulations, ImageNet-C corruptions, and ImageNet-R style shifts. Robustness follows a logarithmic scaling law with both data volume and model size: a tenfold increase in data reduces attack success rate (ASR) on average by ~3.2%, whereas a tenfold increase in model size reduces ASR on average by ~13.4%. Notably, some self-supervised models trained on curated datasets, such as DINOv2, outperform others trained on much larger but less curated datasets, challenging the assumption that scale alone drives robustness. Adversarial fine-tuning of ResNet50s improves generalization across structural variations but not across color distributions. Human evaluation reveals persistent gaps between human and machine vision. These results show that while scaling improves robustness, data quality, architecture, and training objectives play a more decisive role than raw scale in achieving broad-spectrum adversarial resilience.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u8bad\u7ec3\u6570\u636e\u7279\u5f81\u5bf9\u5bf9\u6297\u9c81\u68d2\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u9c81\u68d2\u6027\u9075\u5faa\u6570\u636e\u91cf\u548c\u6a21\u578b\u89c4\u6a21\u7684\u5bf9\u6570\u7f29\u653e\u5b9a\u5f8b\uff0c\u4f46\u6570\u636e\u8d28\u91cf\u3001\u67b6\u6784\u548c\u8bad\u7ec3\u76ee\u6807\u6bd4\u539f\u59cb\u89c4\u6a21\u5728\u5b9e\u73b0\u5e7f\u8c31\u5bf9\u6297\u9c81\u68d2\u6027\u65b9\u9762\u66f4\u5177\u51b3\u5b9a\u6027\u4f5c\u7528\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u548c\u8bad\u7ec3\u8303\u5f0f\u6709\u6240\u8fdb\u6b65\uff0c\u4f46\u4ecd\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6837\u672c\u7684\u653b\u51fb\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u8bad\u7ec3\u6570\u636e\u7279\u5f81\u5982\u4f55\u5f71\u54cd\u5bf9\u6297\u9c81\u68d2\u6027\u3002", "method": "\u8bc4\u4f30\u4e8636\u79cd\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u6a21\u578b\uff0c\u6db5\u76d6\u76d1\u7763\u3001\u81ea\u76d1\u7763\u548c\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u4ece120\u4e07\u5230220\u4ebf\u56fe\u50cf\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u3002\u6a21\u578b\u5728\u516d\u79cd\u9ed1\u76d2\u653b\u51fb\u7c7b\u522b\u4e0b\u8fdb\u884c\u8bc4\u4f30\uff1a\u968f\u673a\u6270\u52a8\u3001\u4e24\u79cd\u51e0\u4f55\u63a9\u7801\u3001COCO\u5bf9\u8c61\u64cd\u4f5c\u3001ImageNet-C\u635f\u574f\u548cImageNet-R\u98ce\u683c\u504f\u79fb\u3002", "result": "\u9c81\u68d2\u6027\u9075\u5faa\u6570\u636e\u91cf\u548c\u6a21\u578b\u89c4\u6a21\u7684\u5bf9\u6570\u7f29\u653e\u5b9a\u5f8b\uff1a\u6570\u636e\u91cf\u589e\u52a0\u5341\u500d\uff0c\u653b\u51fb\u6210\u529f\u7387\u5e73\u5747\u964d\u4f4e\u7ea63.2%\uff1b\u6a21\u578b\u89c4\u6a21\u589e\u52a0\u5341\u500d\uff0c\u653b\u51fb\u6210\u529f\u7387\u5e73\u5747\u964d\u4f4e\u7ea613.4%\u3002\u4e00\u4e9b\u5728\u7cbe\u9009\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u81ea\u76d1\u7763\u6a21\u578b\uff08\u5982DINOv2\uff09\u8868\u73b0\u4f18\u4e8e\u5728\u66f4\u5927\u4f46\u8f83\u5c11\u7cbe\u9009\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u3002\u5bf9\u6297\u6027\u5fae\u8c03\u6539\u5584\u4e86\u8de8\u7ed3\u6784\u53d8\u5316\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u4e0d\u80fd\u6539\u5584\u8de8\u989c\u8272\u5206\u5e03\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u867d\u7136\u6269\u5c55\u89c4\u6a21\u80fd\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u4f46\u6570\u636e\u8d28\u91cf\u3001\u67b6\u6784\u548c\u8bad\u7ec3\u76ee\u6807\u5728\u5b9e\u73b0\u5e7f\u8c31\u5bf9\u6297\u9c81\u68d2\u6027\u65b9\u9762\u6bd4\u539f\u59cb\u89c4\u6a21\u66f4\u5177\u51b3\u5b9a\u6027\u4f5c\u7528\u3002\u4eba\u7c7b\u8bc4\u4f30\u63ed\u793a\u4e86\u4eba\u7c7b\u548c\u673a\u5668\u89c6\u89c9\u4e4b\u95f4\u5b58\u5728\u6301\u7eed\u5dee\u8ddd\u3002"}}
{"id": "2509.25940", "pdf": "https://arxiv.org/pdf/2509.25940", "abs": "https://arxiv.org/abs/2509.25940", "authors": ["Debottam Dutta", "Jianchong Chen", "Rajalaxmi Rajagopalan", "Yu-Lin Wei", "Romit Roy Choudhury"], "title": "CO3: Contrasting Concepts Compose Better", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We propose to improve multi-concept prompt fidelity in text-to-image diffusion models. We begin with common failure cases-prompts like \"a cat and a dog\" that sometimes yields images where one concept is missing, faint, or colliding awkwardly with another. We hypothesize that this happens when the diffusion model drifts into mixed modes that over-emphasize a single concept it learned strongly during training. Instead of re-training, we introduce a corrective sampling strategy that steers away from regions where the joint prompt behavior overlaps too strongly with any single concept in the prompt. The goal is to steer towards \"pure\" joint modes where all concepts can coexist with balanced visual presence. We further show that existing multi-concept guidance schemes can operate in unstable weight regimes that amplify imbalance; we characterize favorable regions and adapt sampling to remain within them. Our approach, CO3, is plug-and-play, requires no model tuning, and complements standard classifier-free guidance. Experiments on diverse multi-concept prompts indicate improvements in concept coverage, balance and robustness, with fewer dropped or distorted concepts compared to standard baselines and prior compositional methods. Results suggest that lightweight corrective guidance can substantially mitigate brittle semantic alignment behavior in modern diffusion systems.", "AI": {"tldr": "\u63d0\u51faCO3\u65b9\u6cd5\uff0c\u901a\u8fc7\u6821\u6b63\u91c7\u6837\u7b56\u7565\u6539\u5584\u591a\u6982\u5ff5\u63d0\u793a\u5728\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u7684\u4fdd\u771f\u5ea6\uff0c\u907f\u514d\u6982\u5ff5\u7f3a\u5931\u6216\u78b0\u649e\u95ee\u9898", "motivation": "\u89e3\u51b3\u591a\u6982\u5ff5\u63d0\u793a\uff08\u5982\u201c\u732b\u548c\u72d7\u201d\uff09\u5728\u6269\u6563\u6a21\u578b\u4e2d\u5e38\u51fa\u73b0\u7684\u6982\u5ff5\u7f3a\u5931\u3001\u6a21\u7cca\u6216\u78b0\u649e\u95ee\u9898\uff0c\u8fd9\u662f\u7531\u4e8e\u6a21\u578b\u5728\u91c7\u6837\u65f6\u504f\u5411\u8bad\u7ec3\u4e2d\u5b66\u4e60\u8f83\u5f3a\u7684\u5355\u4e00\u6982\u5ff5", "method": "\u5f15\u5165\u6821\u6b63\u91c7\u6837\u7b56\u7565\uff0c\u5f15\u5bfc\u6a21\u578b\u8fdc\u79bb\u8054\u5408\u63d0\u793a\u884c\u4e3a\u4e0e\u4efb\u4f55\u5355\u4e00\u6982\u5ff5\u91cd\u53e0\u8fc7\u5f3a\u7684\u533a\u57df\uff0c\u8f6c\u5411\u6240\u6709\u6982\u5ff5\u80fd\u5e73\u8861\u5171\u5b58\u7684\u201c\u7eaf\u201d\u8054\u5408\u6a21\u5f0f", "result": "\u5728\u591a\u6837\u5316\u591a\u6982\u5ff5\u63d0\u793a\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u6bd4\u6807\u51c6\u57fa\u7ebf\u548c\u73b0\u6709\u7ec4\u5408\u65b9\u6cd5\uff0c\u6982\u5ff5\u8986\u76d6\u5ea6\u3001\u5e73\u8861\u6027\u548c\u9c81\u68d2\u6027\u5747\u6709\u63d0\u5347\uff0c\u6982\u5ff5\u4e22\u5931\u6216\u626d\u66f2\u66f4\u5c11", "conclusion": "\u8f7b\u91cf\u7ea7\u6821\u6b63\u5f15\u5bfc\u80fd\u663e\u8457\u7f13\u89e3\u73b0\u4ee3\u6269\u6563\u7cfb\u7edf\u4e2d\u7684\u8106\u5f31\u8bed\u4e49\u5bf9\u9f50\u884c\u4e3a"}}
{"id": "2509.26010", "pdf": "https://arxiv.org/pdf/2509.26010", "abs": "https://arxiv.org/abs/2509.26010", "authors": ["Rajendra K. Ray", "Manish Kumar"], "title": "New Fourth-Order Grayscale Indicator-Based Telegraph Diffusion Model for Image Despeckling", "categories": ["cs.CV"], "comment": null, "summary": "Second-order PDE models have been widely used for suppressing multiplicative noise, but they often introduce blocky artifacts in the early stages of denoising. To resolve this, we propose a fourth-order nonlinear PDE model that integrates diffusion and wave properties. The diffusion process, guided by both the Laplacian and intensity values, reduces noise better than gradient-based methods, while the wave part keeps fine details and textures. The effectiveness of the proposed model is evaluated against two second-order anisotropic diffusion approaches using the Peak Signal-to-Noise Ratio (PSNR) and Mean Structural Similarity Index (MSSIM) for images with available ground truth. For SAR images, where a noise-free reference is unavailable, the Speckle Index (SI) is used to measure noise reduction. Additionally, we extend the proposed model to study color images by applying the denoising process independently to each channel, preserving both structure and color consistency. The same quantitative metrics PSNR and MSSIM are used for performance evaluation, ensuring a fair comparison across grayscale and color images. In all the cases, our computed results produce better results compared to existing models in this genre.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6269\u6563\u548c\u6ce2\u7279\u6027\u7684\u56db\u9636\u975e\u7ebf\u6027PDE\u6a21\u578b\uff0c\u7528\u4e8e\u6291\u5236\u4e58\u6027\u566a\u58f0\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684\u4e8c\u9636PDE\u6a21\u578b\u80fd\u66f4\u597d\u5730\u51cf\u5c11\u5757\u72b6\u4f2a\u5f71\u5e76\u4fdd\u7559\u7ec6\u8282\u7eb9\u7406\u3002", "motivation": "\u4f20\u7edf\u7684\u4e8c\u9636PDE\u6a21\u578b\u5728\u6291\u5236\u4e58\u6027\u566a\u58f0\u65f6\u4f1a\u5728\u53bb\u566a\u65e9\u671f\u5f15\u5165\u5757\u72b6\u4f2a\u5f71\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u56db\u9636\u975e\u7ebf\u6027PDE\u6a21\u578b\uff0c\u7ed3\u5408\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u548c\u5f3a\u5ea6\u503c\u6307\u5bfc\u7684\u6269\u6563\u8fc7\u7a0b\u6765\u964d\u566a\uff0c\u540c\u65f6\u5229\u7528\u6ce2\u90e8\u5206\u4fdd\u6301\u7ec6\u8282\u548c\u7eb9\u7406\u3002\u5bf9\u4e8e\u5f69\u8272\u56fe\u50cf\uff0c\u5c06\u53bb\u566a\u8fc7\u7a0b\u72ec\u7acb\u5e94\u7528\u4e8e\u6bcf\u4e2a\u901a\u9053\u3002", "result": "\u5728PSNR\u3001MSSIM\u548cSI\u7b49\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u4e8c\u9636\u5404\u5411\u5f02\u6027\u6269\u6563\u65b9\u6cd5\uff0c\u5bf9\u7070\u5ea6\u56fe\u50cf\u548c\u5f69\u8272\u56fe\u50cf\u90fd\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u53bb\u566a\u6548\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u56db\u9636PDE\u6a21\u578b\u5728\u6291\u5236\u4e58\u6027\u566a\u58f0\u65b9\u9762\u6bd4\u73b0\u6709\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0c\u80fd\u6709\u6548\u51cf\u5c11\u5757\u72b6\u4f2a\u5f71\u5e76\u4fdd\u6301\u56fe\u50cf\u7ec6\u8282\u548c\u989c\u8272\u4e00\u81f4\u6027\u3002"}}
{"id": "2509.26025", "pdf": "https://arxiv.org/pdf/2509.26025", "abs": "https://arxiv.org/abs/2509.26025", "authors": ["Shian Du", "Menghan Xia", "Chang Liu", "Xintao Wang", "Jing Wang", "Pengfei Wan", "Di Zhang", "Xiangyang Ji"], "title": "PatchVSR: Breaking Video Diffusion Resolution Limits with Patch-wise Video Super-Resolution", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Pre-trained video generation models hold great potential for generative video super-resolution (VSR). However, adapting them for full-size VSR, as most existing methods do, suffers from unnecessary intensive full-attention computation and fixed output resolution. To overcome these limitations, we make the first exploration into utilizing video diffusion priors for patch-wise VSR. This is non-trivial because pre-trained video diffusion models are not native for patch-level detail generation. To mitigate this challenge, we propose an innovative approach, called PatchVSR, which integrates a dual-stream adapter for conditional guidance. The patch branch extracts features from input patches to maintain content fidelity while the global branch extracts context features from the resized full video to bridge the generation gap caused by incomplete semantics of patches. Particularly, we also inject the patch's location information into the model to better contextualize patch synthesis within the global video frame. Experiments demonstrate that our method can synthesize high-fidelity, high-resolution details at the patch level. A tailor-made multi-patch joint modulation is proposed to ensure visual consistency across individually enhanced patches. Due to the flexibility of our patch-based paradigm, we can achieve highly competitive 4K VSR based on a 512x512 resolution base model, with extremely high efficiency.", "AI": {"tldr": "\u63d0\u51faPatchVSR\u65b9\u6cd5\uff0c\u5229\u7528\u89c6\u9891\u6269\u6563\u5148\u9a8c\u8fdb\u884c\u5206\u5757\u89c6\u9891\u8d85\u5206\u8fa8\u7387\uff0c\u901a\u8fc7\u53cc\u6d41\u9002\u914d\u5668\u6574\u5408\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\uff0c\u5b9e\u73b0\u9ad8\u6548\u9ad8\u4fdd\u771f\u76844K\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u6a21\u578b\u76f4\u63a5\u7528\u4e8e\u5168\u5c3a\u5bf8\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u5b58\u5728\u8ba1\u7b97\u5bc6\u96c6\u548c\u8f93\u51fa\u5206\u8fa8\u7387\u56fa\u5b9a\u7684\u95ee\u9898\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u5206\u5757\u5904\u7406\u65b9\u5f0f\u3002", "method": "\u63d0\u51faPatchVSR\u65b9\u6cd5\uff0c\u5305\u542b\u53cc\u6d41\u9002\u914d\u5668\uff1a\u5c40\u90e8\u5206\u652f\u63d0\u53d6\u8f93\u5165\u5757\u7279\u5f81\u4fdd\u6301\u5185\u5bb9\u4fdd\u771f\u5ea6\uff0c\u5168\u5c40\u5206\u652f\u63d0\u53d6\u5168\u89c6\u9891\u4e0a\u4e0b\u6587\u7279\u5f81\u5f25\u8865\u5757\u8bed\u4e49\u4e0d\u5b8c\u6574\u95ee\u9898\uff0c\u5e76\u6ce8\u5165\u4f4d\u7f6e\u4fe1\u606f\u4ee5\u66f4\u597d\u5730\u5728\u5168\u5c40\u5e27\u4e2d\u5b9a\u4f4d\u5757\u5408\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u5728\u5757\u7ea7\u522b\u5408\u6210\u9ad8\u4fdd\u771f\u3001\u9ad8\u5206\u8fa8\u7387\u7ec6\u8282\uff0c\u901a\u8fc7\u5b9a\u5236\u7684\u591a\u5757\u8054\u5408\u8c03\u5236\u786e\u4fdd\u589e\u5f3a\u5757\u95f4\u7684\u89c6\u89c9\u4e00\u81f4\u6027\uff0c\u57fa\u4e8e512x512\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u7ade\u4e89\u60274K\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u3002", "conclusion": "\u57fa\u4e8e\u5206\u5757\u7684\u8303\u5f0f\u5177\u6709\u7075\u6d3b\u6027\uff0c\u80fd\u591f\u4ee5\u6781\u9ad8\u6548\u7387\u5b9e\u73b0\u9ad8\u8d28\u91cf4K\u89c6\u9891\u8d85\u5206\u8fa8\u7387\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u5168\u5c3a\u5bf8\u65b9\u6cd5\u7684\u8ba1\u7b97\u9650\u5236\u3002"}}
{"id": "2509.26096", "pdf": "https://arxiv.org/pdf/2509.26096", "abs": "https://arxiv.org/abs/2509.26096", "authors": ["Shigui Li", "Wei Chen", "Delu Zeng"], "title": "EVODiff: Entropy-aware Variance Optimized Diffusion Inference", "categories": ["cs.CV", "cs.IT", "cs.LG", "math.IT", "math.OC", "stat.ML"], "comment": "NeurIPS 2025, 40 pages, 14 figures", "summary": "Diffusion models (DMs) excel in image generation, but suffer from slow inference and the training-inference discrepancies. Although gradient-based solvers like DPM-Solver accelerate the denoising inference, they lack theoretical foundations in information transmission efficiency. In this work, we introduce an information-theoretic perspective on the inference processes of DMs, revealing that successful denoising fundamentally reduces conditional entropy in reverse transitions. This principle leads to our key insights into the inference processes: (1) data prediction parameterization outperforms its noise counterpart, and (2) optimizing conditional variance offers a reference-free way to minimize both transition and reconstruction errors. Based on these insights, we propose an entropy-aware variance optimized method for the generative process of DMs, called EVODiff, which systematically reduces uncertainty by optimizing conditional entropy during denoising. Extensive experiments on DMs validate our insights and demonstrate that our method significantly and consistently outperforms state-of-the-art (SOTA) gradient-based solvers. For example, compared to the DPM-Solver++, EVODiff reduces the reconstruction error by up to 45.5\\% (FID improves from 5.10 to 2.78) at 10 function evaluations (NFE) on CIFAR-10, cuts the NFE cost by 25\\% (from 20 to 15 NFE) for high-quality samples on ImageNet-256, and improves text-to-image generation while reducing artifacts. Code is available at https://github.com/ShiguiLi/EVODiff.", "AI": {"tldr": "\u63d0\u51faEVODiff\u65b9\u6cd5\uff0c\u4ece\u4fe1\u606f\u8bba\u89d2\u5ea6\u4f18\u5316\u6269\u6563\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u901a\u8fc7\u4f18\u5316\u6761\u4ef6\u71b5\u6765\u51cf\u5c11\u4e0d\u786e\u5b9a\u6027\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u867d\u7136\u56fe\u50cf\u751f\u6210\u6548\u679c\u597d\uff0c\u4f46\u5b58\u5728\u63a8\u7406\u901f\u5ea6\u6162\u548c\u8bad\u7ec3-\u63a8\u7406\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002\u73b0\u6709\u68af\u5ea6\u6c42\u89e3\u5668\u7f3a\u4e4f\u4fe1\u606f\u4f20\u8f93\u6548\u7387\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "\u4ece\u4fe1\u606f\u8bba\u89d2\u5ea6\u5206\u6790\u6269\u6563\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\uff0c\u63d0\u51fa\u57fa\u4e8e\u6761\u4ef6\u71b5\u4f18\u5316\u7684EVODiff\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6761\u4ef6\u65b9\u5dee\u6765\u6700\u5c0f\u5316\u8f6c\u6362\u548c\u91cd\u5efa\u8bef\u5dee\u3002", "result": "\u5728CIFAR-10\u4e0a\uff0cEVODiff\u572810\u6b21\u51fd\u6570\u8bc4\u4f30\u65f6\u91cd\u5efa\u8bef\u5dee\u964d\u4f4e45.5%\uff08FID\u4ece5.10\u63d0\u5347\u52302.78\uff09\uff1b\u5728ImageNet-256\u4e0a\uff0c\u9ad8\u8d28\u91cf\u6837\u672c\u7684NFE\u6210\u672c\u51cf\u5c1125%\uff08\u4ece20\u964d\u523015\uff09\uff1b\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u63d0\u5347\u4e14\u4f2a\u5f71\u51cf\u5c11\u3002", "conclusion": "EVODiff\u901a\u8fc7\u4fe1\u606f\u8bba\u89c6\u89d2\u4f18\u5316\u6269\u6563\u6a21\u578b\u63a8\u7406\uff0c\u5728\u751f\u6210\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u68af\u5ea6\u6c42\u89e3\u5668\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u63a8\u7406\u65b9\u6cd5\u3002"}}
{"id": "2509.26325", "pdf": "https://arxiv.org/pdf/2509.26325", "abs": "https://arxiv.org/abs/2509.26325", "authors": ["Alexander Becker", "Julius Erbach", "Dominik Narnhofer", "Konrad Schindler"], "title": "Continuous Space-Time Video Super-Resolution with 3D Fourier Fields", "categories": ["cs.CV"], "comment": null, "summary": "We introduce a novel formulation for continuous space-time video super-resolution. Instead of decoupling the representation of a video sequence into separate spatial and temporal components and relying on brittle, explicit frame warping for motion compensation, we encode video as a continuous, spatio-temporally coherent 3D Video Fourier Field (VFF). That representation offers three key advantages: (1) it enables cheap, flexible sampling at arbitrary locations in space and time; (2) it is able to simultaneously capture fine spatial detail and smooth temporal dynamics; and (3) it offers the possibility to include an analytical, Gaussian point spread function in the sampling to ensure aliasing-free reconstruction at arbitrary scale. The coefficients of the proposed, Fourier-like sinusoidal basis are predicted with a neural encoder with a large spatio-temporal receptive field, conditioned on the low-resolution input video. Through extensive experiments, we show that our joint modeling substantially improves both spatial and temporal super-resolution and sets a new state of the art for multiple benchmarks: across a wide range of upscaling factors, it delivers sharper and temporally more consistent reconstructions than existing baselines, while being computationally more efficient. Project page: https://v3vsr.github.io.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fde\u7eed\u65f6\u7a7a\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u7684\u65b0\u65b9\u6cd5\u2014\u20143D\u89c6\u9891\u5085\u91cc\u53f6\u573a(VFF)\uff0c\u901a\u8fc7\u795e\u7ecf\u7f16\u7801\u5668\u9884\u6d4b\u5085\u91cc\u53f6\u7cfb\u6570\uff0c\u5b9e\u73b0\u4efb\u610f\u65f6\u7a7a\u4f4d\u7f6e\u7684\u7075\u6d3b\u91c7\u6837\u548c\u6297\u952f\u9f7f\u91cd\u5efa\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u89c6\u9891\u5206\u89e3\u4e3a\u7a7a\u95f4\u548c\u65f6\u95f4\u5206\u91cf\uff0c\u4f9d\u8d56\u8106\u5f31\u7684\u663e\u5f0f\u5e27\u626d\u66f2\u8fdb\u884c\u8fd0\u52a8\u8865\u507f\uff0c\u5b58\u5728\u5c40\u9650\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u6355\u6349\u7cbe\u7ec6\u7a7a\u95f4\u7ec6\u8282\u548c\u5e73\u6ed1\u65f6\u95f4\u52a8\u6001\u7684\u8fde\u7eed\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u5c06\u89c6\u9891\u7f16\u7801\u4e3a\u8fde\u7eed\u76843D\u89c6\u9891\u5085\u91cc\u53f6\u573a\uff0c\u4f7f\u7528\u5177\u6709\u5927\u65f6\u7a7a\u611f\u53d7\u91ce\u7684\u795e\u7ecf\u7f16\u7801\u5668\u9884\u6d4b\u5085\u91cc\u53f6\u7cfb\u6570\uff0c\u53ef\u5305\u542b\u9ad8\u65af\u70b9\u6269\u6563\u51fd\u6570\u8fdb\u884c\u6297\u952f\u9f7f\u91cd\u5efa\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5728\u5e7f\u6cdb\u7684\u653e\u5927\u56e0\u5b50\u8303\u56f4\u5185\u63d0\u4f9b\u66f4\u6e05\u6670\u3001\u65f6\u95f4\u66f4\u4e00\u81f4\u7684\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\uff0c\u540c\u65f6\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u8054\u5408\u5efa\u6a21\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u65f6\u7a7a\u8d85\u5206\u8fa8\u7387\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8fde\u7eed\u5085\u91cc\u53f6\u573a\u8868\u793a\u5728\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.26376", "pdf": "https://arxiv.org/pdf/2509.26376", "abs": "https://arxiv.org/abs/2509.26376", "authors": ["Harold Haodong Chen", "Xianfeng Wu", "Wen-Jie Shu", "Rongjin Guo", "Disen Lan", "Harry Yang", "Ying-Cong Chen"], "title": "Go with Your Gut: Scaling Confidence for Autoregressive Image Generation", "categories": ["cs.CV"], "comment": "Code: https://github.com/EnVision-Research/ScalingAR", "summary": "Test-time scaling (TTS) has demonstrated remarkable success in enhancing large language models, yet its application to next-token prediction (NTP) autoregressive (AR) image generation remains largely uncharted. Existing TTS approaches for visual AR (VAR), which rely on frequent partial decoding and external reward models, are ill-suited for NTP-based image generation due to the inherent incompleteness of intermediate decoding results. To bridge this gap, we introduce ScalingAR, the first TTS framework specifically designed for NTP-based AR image generation that eliminates the need for early decoding or auxiliary rewards. ScalingAR leverages token entropy as a novel signal in visual token generation and operates at two complementary scaling levels: (i) Profile Level, which streams a calibrated confidence state by fusing intrinsic and conditional signals; and (ii) Policy Level, which utilizes this state to adaptively terminate low-confidence trajectories and dynamically schedule guidance for phase-appropriate conditioning strength. Experiments on both general and compositional benchmarks show that ScalingAR (1) improves base models by 12.5% on GenEval and 15.2% on TIIF-Bench, (2) efficiently reduces visual token consumption by 62.0% while outperforming baselines, and (3) successfully enhances robustness, mitigating performance drops by 26.0% in challenging scenarios.", "AI": {"tldr": "ScalingAR\u662f\u9996\u4e2a\u4e13\u4e3a\u57fa\u4e8e\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u7684\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u8bbe\u8ba1\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u6846\u67b6\uff0c\u65e0\u9700\u65e9\u671f\u89e3\u7801\u6216\u8f85\u52a9\u5956\u52b1\uff0c\u901a\u8fc7token\u71b5\u4f5c\u4e3a\u89c6\u89c9token\u751f\u6210\u7684\u65b0\u4fe1\u53f7\uff0c\u5728\u914d\u7f6e\u548c\u7b56\u7565\u4e24\u4e2a\u5c42\u9762\u8fdb\u884c\u81ea\u9002\u5e94\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\u4f9d\u8d56\u9891\u7e41\u7684\u90e8\u5206\u89e3\u7801\u548c\u5916\u90e8\u5956\u52b1\u6a21\u578b\uff0c\u4e0d\u9002\u7528\u4e8e\u57fa\u4e8e\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u7684\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\uff0c\u56e0\u4e3a\u4e2d\u95f4\u89e3\u7801\u7ed3\u679c\u5177\u6709\u56fa\u6709\u7684\u4e0d\u5b8c\u6574\u6027\u3002", "method": "ScalingAR\u5229\u7528token\u71b5\u4f5c\u4e3a\u89c6\u89c9token\u751f\u6210\u7684\u65b0\u4fe1\u53f7\uff0c\u5728\u914d\u7f6e\u5c42\u9762\u901a\u8fc7\u878d\u5408\u5185\u5728\u548c\u6761\u4ef6\u4fe1\u53f7\u6765\u6d41\u5f0f\u4f20\u8f93\u6821\u51c6\u7684\u7f6e\u4fe1\u72b6\u6001\uff0c\u5728\u7b56\u7565\u5c42\u9762\u5229\u7528\u8be5\u72b6\u6001\u81ea\u9002\u5e94\u7ec8\u6b62\u4f4e\u7f6e\u4fe1\u5ea6\u8f68\u8ff9\u5e76\u52a8\u6001\u8c03\u5ea6\u6307\u5bfc\u5f3a\u5ea6\u3002", "result": "\u5728\u901a\u7528\u548c\u7ec4\u5408\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cScalingAR\u5c06\u57fa\u7840\u6a21\u578b\u5728GenEval\u4e0a\u63d0\u534712.5%\uff0c\u5728TIIF-Bench\u4e0a\u63d0\u534715.2%\uff0c\u540c\u65f6\u5c06\u89c6\u89c9token\u6d88\u8017\u51cf\u5c1162.0%\uff0c\u5e76\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\u5c06\u6027\u80fd\u4e0b\u964d\u51cf\u8f7b26.0%\u3002", "conclusion": "ScalingAR\u6210\u529f\u5730\u5c06\u6d4b\u8bd5\u65f6\u7f29\u653e\u5e94\u7528\u4e8e\u57fa\u4e8e\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u7684\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u6548\u7387\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.26391", "pdf": "https://arxiv.org/pdf/2509.26391", "abs": "https://arxiv.org/abs/2509.26391", "authors": ["Chenhui Zhu", "Yilu Wu", "Shuai Wang", "Gangshan Wu", "Limin Wang"], "title": "MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Image-to-video generation has made remarkable progress with the advancements in diffusion models, yet generating videos with realistic motion remains highly challenging. This difficulty arises from the complexity of accurately modeling motion, which involves capturing physical constraints, object interactions, and domain-specific dynamics that are not easily generalized across diverse scenarios. To address this, we propose MotionRAG, a retrieval-augmented framework that enhances motion realism by adapting motion priors from relevant reference videos through Context-Aware Motion Adaptation (CAMA). The key technical innovations include: (i) a retrieval-based pipeline extracting high-level motion features using video encoder and specialized resamplers to distill semantic motion representations; (ii) an in-context learning approach for motion adaptation implemented through a causal transformer architecture; (iii) an attention-based motion injection adapter that seamlessly integrates transferred motion features into pretrained video diffusion models. Extensive experiments demonstrate that our method achieves significant improvements across multiple domains and various base models, all with negligible computational overhead during inference. Furthermore, our modular design enables zero-shot generalization to new domains by simply updating the retrieval database without retraining any components. This research enhances the core capability of video generation systems by enabling the effective retrieval and transfer of motion priors, facilitating the synthesis of realistic motion dynamics.", "AI": {"tldr": "MotionRAG\u662f\u4e00\u4e2a\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u76f8\u5173\u53c2\u8003\u89c6\u9891\u4e2d\u63d0\u53d6\u8fd0\u52a8\u5148\u9a8c\u6765\u63d0\u5347\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u7684\u8fd0\u52a8\u7684\u771f\u5b9e\u6027\uff0c\u91c7\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u8fd0\u52a8\u9002\u5e94\u6280\u672f\uff0c\u5728\u63a8\u7406\u65f6\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u5728\u8fd0\u52a8\u771f\u5b9e\u6027\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u51c6\u786e\u5efa\u6a21\u8fd0\u52a8\u9700\u8981\u6355\u6349\u7269\u7406\u7ea6\u675f\u3001\u7269\u4f53\u4ea4\u4e92\u548c\u7279\u5b9a\u9886\u57df\u52a8\u6001\uff0c\u8fd9\u4e9b\u96be\u4ee5\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u6cdb\u5316\u3002", "method": "\u63d0\u51fa\u68c0\u7d22\u589e\u5f3a\u6846\u67b6MotionRAG\uff0c\u5305\u62ec\uff1a(1)\u57fa\u4e8e\u68c0\u7d22\u7684\u7ba1\u9053\uff0c\u4f7f\u7528\u89c6\u9891\u7f16\u7801\u5668\u548c\u4e13\u7528\u91cd\u91c7\u6837\u5668\u63d0\u53d6\u9ad8\u7ea7\u8fd0\u52a8\u7279\u5f81\uff1b(2)\u901a\u8fc7\u56e0\u679c\u53d8\u6362\u5668\u67b6\u6784\u5b9e\u73b0\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u8fd0\u52a8\u9002\u5e94\uff1b(3)\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u8fd0\u52a8\u6ce8\u5165\u9002\u914d\u5668\uff0c\u5c06\u8f6c\u79fb\u7684\u8fd0\u52a8\u7279\u5f81\u65e0\u7f1d\u96c6\u6210\u5230\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u9886\u57df\u548c\u5404\u79cd\u57fa\u7840\u6a21\u578b\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u63a8\u7406\u65f6\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002\u6a21\u5757\u5316\u8bbe\u8ba1\u652f\u6301\u96f6\u6837\u672c\u6cdb\u5316\u5230\u65b0\u9886\u57df\uff0c\u53ea\u9700\u66f4\u65b0\u68c0\u7d22\u6570\u636e\u5e93\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u4efb\u4f55\u7ec4\u4ef6\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u901a\u8fc7\u5b9e\u73b0\u8fd0\u52a8\u5148\u9a8c\u7684\u6709\u6548\u68c0\u7d22\u548c\u8f6c\u79fb\uff0c\u589e\u5f3a\u4e86\u89c6\u9891\u751f\u6210\u7cfb\u7edf\u7684\u6838\u5fc3\u80fd\u529b\uff0c\u4fc3\u8fdb\u4e86\u771f\u5b9e\u8fd0\u52a8\u52a8\u6001\u7684\u5408\u6210\u3002"}}
{"id": "2509.26398", "pdf": "https://arxiv.org/pdf/2509.26398", "abs": "https://arxiv.org/abs/2509.26398", "authors": ["Atakan Topaloglu", "Ahmet Bilican", "Cansu Korkmaz", "A. Murat Tekalp"], "title": "Image-Difficulty-Aware Evaluation of Super-Resolution Models", "categories": ["cs.CV"], "comment": "Accepted to and presented at ICIP 2025 Workshops", "summary": "Image super-resolution models are commonly evaluated by average scores (over some benchmark test sets), which fail to reflect the performance of these models on images of varying difficulty and that some models generate artifacts on certain difficult images, which is not reflected by the average scores. We propose difficulty-aware performance evaluation procedures to better differentiate between SISR models that produce visually different results on some images but yield close average performance scores over the entire test set. In particular, we propose two image-difficulty measures, the high-frequency index and rotation-invariant edge index, to predict those test images, where a model would yield significantly better visual results over another model, and an evaluation method where these visual differences are reflected on objective measures. Experimental results demonstrate the effectiveness of the proposed image-difficulty measures and evaluation methodology.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u96be\u5ea6\u611f\u77e5\u7684\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u9891\u6307\u6570\u548c\u65cb\u8f6c\u4e0d\u53d8\u8fb9\u7f18\u6307\u6570\u6765\u8bc6\u522b\u6d4b\u8bd5\u56fe\u50cf\u4e2d\u6a21\u578b\u6027\u80fd\u5dee\u5f02\uff0c\u5f25\u8865\u4f20\u7edf\u5e73\u5747\u5206\u6570\u8bc4\u4f30\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u8bc4\u4f30\u4f7f\u7528\u5e73\u5747\u5206\u6570\uff0c\u65e0\u6cd5\u53cd\u6620\u6a21\u578b\u5728\u4e0d\u540c\u96be\u5ea6\u56fe\u50cf\u4e0a\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u4e5f\u65e0\u6cd5\u6355\u6349\u67d0\u4e9b\u6a21\u578b\u5728\u56f0\u96be\u56fe\u50cf\u4e0a\u4ea7\u751f\u7684\u4f2a\u5f71\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u56fe\u50cf\u96be\u5ea6\u5ea6\u91cf\u6307\u6807\uff1a\u9ad8\u9891\u6307\u6570\u548c\u65cb\u8f6c\u4e0d\u53d8\u8fb9\u7f18\u6307\u6570\uff0c\u7528\u4e8e\u9884\u6d4b\u6a21\u578b\u5728\u54ea\u4e9b\u6d4b\u8bd5\u56fe\u50cf\u4e0a\u4f1a\u4ea7\u751f\u663e\u8457\u89c6\u89c9\u5dee\u5f02\uff0c\u5e76\u8bbe\u8ba1\u4e86\u76f8\u5e94\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u56fe\u50cf\u96be\u5ea6\u5ea6\u91cf\u548c\u8bc4\u4f30\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u533a\u5206\u4ea7\u751f\u4e0d\u540c\u89c6\u89c9\u7ed3\u679c\u7684\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u3002", "conclusion": "\u57fa\u4e8e\u56fe\u50cf\u96be\u5ea6\u7684\u8bc4\u4f30\u65b9\u6cd5\u80fd\u591f\u66f4\u51c6\u786e\u5730\u53cd\u6620\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u7684\u771f\u5b9e\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u3002"}}
{"id": "2509.26413", "pdf": "https://arxiv.org/pdf/2509.26413", "abs": "https://arxiv.org/abs/2509.26413", "authors": ["Pengze Xue", "Shanwen Wang", "Fei Zhou", "Yan Cui", "Xin Sun"], "title": "PRISM: Progressive Rain removal with Integrated State-space Modeling", "categories": ["cs.CV"], "comment": "Preprint. Submitted to an IEEE conference and currently under review.   Copyright 2025 IEEE; personal use permitted; all other uses require   permission", "summary": "Image deraining is an essential vision technique that removes rain streaks and water droplets, enhancing clarity for critical vision tasks like autonomous driving. However, current single-scale models struggle with fine-grained recovery and global consistency. To address this challenge, we propose Progressive Rain removal with Integrated State-space Modeling (PRISM), a progressive three-stage framework: Coarse Extraction Network (CENet), Frequency Fusion Network (SFNet), and Refine Network (RNet). Specifically, CENet and SFNet utilize a novel Hybrid Attention UNet (HA-UNet) for multi-scale feature aggregation by combining channel attention with windowed spatial transformers. Moreover, we propose Hybrid Domain Mamba (HDMamba) for SFNet to jointly model spatial semantics and wavelet domain characteristics. Finally, RNet recovers the fine-grained structures via an original-resolution subnetwork. Our model learns high-frequency rain characteristics while preserving structural details and maintaining global context, leading to improved image quality. Our method achieves competitive results on multiple datasets against recent deraining methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86PRISM\u6846\u67b6\uff0c\u4e00\u4e2a\u6e10\u8fdb\u5f0f\u4e09\u9636\u6bb5\u53bb\u96e8\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u805a\u5408\u548c\u6df7\u5408\u57df\u72b6\u6001\u7a7a\u95f4\u5efa\u6a21\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u7ade\u4e89\u6027\u7ed3\u679c", "motivation": "\u5f53\u524d\u5355\u5c3a\u5ea6\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u6062\u590d\u548c\u5168\u5c40\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u53bb\u96e8\u65b9\u6cd5", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1aCENet\u7c97\u63d0\u53d6\u7f51\u7edc\u3001SFNet\u9891\u7387\u878d\u5408\u7f51\u7edc\u3001RNet\u7cbe\u70bc\u7f51\u7edc\uff0c\u4f7f\u7528HA-UNet\u8fdb\u884c\u591a\u5c3a\u5ea6\u7279\u5f81\u805a\u5408\uff0cHDMamba\u8fdb\u884c\u6df7\u5408\u57df\u5efa\u6a21", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u7ade\u4e89\u6027\u7ed3\u679c\uff0c\u80fd\u591f\u5b66\u4e60\u9ad8\u9891\u96e8\u7279\u5f81\u540c\u65f6\u4fdd\u6301\u7ed3\u6784\u7ec6\u8282\u548c\u5168\u5c40\u4e0a\u4e0b\u6587", "conclusion": "PRISM\u6846\u67b6\u901a\u8fc7\u6e10\u8fdb\u5f0f\u5904\u7406\u548c\u6df7\u5408\u57df\u5efa\u6a21\uff0c\u6709\u6548\u63d0\u5347\u4e86\u56fe\u50cf\u53bb\u96e8\u7684\u8d28\u91cf\u548c\u4e00\u81f4\u6027"}}
{"id": "2509.26436", "pdf": "https://arxiv.org/pdf/2509.26436", "abs": "https://arxiv.org/abs/2509.26436", "authors": ["Donghoon Kim", "Dongyoung Lee", "Ik Joon Chang", "Sung-Ho Bae"], "title": "Post-Training Quantization via Residual Truncation and Zero Suppression for Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models achieve high-quality image generation but face deployment challenges due to their high computational requirements. Although 8-bit outlier-aware post-training quantization (PTQ) matches full-precision performance, extending PTQ to 4 bits remains challenging. Larger step sizes in 4-bit quantization amplify rounding errors in dense, low-magnitude activations, leading to the loss of fine-grained textures. We hypothesize that not only outliers but also small activations are critical for texture fidelity. To this end, we propose Quantization via Residual Truncation and Zero Suppression (QuaRTZ), a 4-bit PTQ scheme for diffusion models. QuaRTZ applies 8-bit min-max quantization for outlier handling and compresses to 4 bits via leading-zero suppression to retain LSBs, thereby preserving texture details. Our approach reduces rounding errors and improves quantization efficiency by balancing outlier preservation and LSB precision. Both theoretical derivations and empirical evaluations demonstrate the generalizability of QuaRTZ across diverse activation distributions. Notably, 4-bit QuaRTZ achieves an FID of 6.98 on FLUX.1-schnell, outperforming SVDQuant that requires auxiliary FP16 branches.", "AI": {"tldr": "\u63d0\u51faQuaRTZ\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b8b\u5dee\u622a\u65ad\u548c\u96f6\u6291\u5236\u5b9e\u73b0\u6269\u6563\u6a21\u578b\u76844\u4f4d\u91cf\u5316\uff0c\u5728\u4fdd\u6301\u7eb9\u7406\u7ec6\u8282\u7684\u540c\u65f6\u51cf\u5c11\u820d\u5165\u8bef\u5dee\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u8ba1\u7b97\u9700\u6c42\u9ad8\uff0c\u90e8\u7f72\u56f0\u96be\u30024\u4f4d\u91cf\u5316\u4e2d\u8f83\u5927\u7684\u6b65\u957f\u4f1a\u653e\u5927\u4f4e\u5e45\u503c\u6fc0\u6d3b\u7684\u820d\u5165\u8bef\u5dee\uff0c\u5bfc\u81f4\u7ec6\u7c92\u5ea6\u7eb9\u7406\u4e22\u5931\u3002", "method": "QuaRTZ\u7ed3\u54088\u4f4d\u6700\u5c0f-\u6700\u5927\u91cf\u5316\u5904\u7406\u5f02\u5e38\u503c\uff0c\u901a\u8fc7\u524d\u5bfc\u96f6\u6291\u5236\u538b\u7f29\u52304\u4f4d\u4ee5\u4fdd\u7559\u6700\u4f4e\u6709\u6548\u4f4d\uff0c\u5e73\u8861\u5f02\u5e38\u503c\u4fdd\u7559\u548cLSB\u7cbe\u5ea6\u3002", "result": "\u5728FLUX.1-schnell\u4e0a\u8fbe\u5230FID 6.98\uff0c\u4f18\u4e8e\u9700\u8981FP16\u8f85\u52a9\u5206\u652f\u7684SVDQuant\u65b9\u6cd5\u3002", "conclusion": "QuaRTZ\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u548c\u5b9e\u8bc1\u8bc4\u4f30\u8bc1\u660e\u4e86\u5bf9\u4e0d\u540c\u6fc0\u6d3b\u5206\u5e03\u7684\u901a\u7528\u6027\uff0c\u662f\u6709\u6548\u76844\u4f4dPTQ\u65b9\u6848\u3002"}}
{"id": "2509.26455", "pdf": "https://arxiv.org/pdf/2509.26455", "abs": "https://arxiv.org/abs/2509.26455", "authors": ["Hanzhou Liu", "Jia Huang", "Mi Lu", "Srikanth Saripalli", "Peng Jiang"], "title": "Stylos: Multi-View 3D Stylization with Single-Forward Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "We present Stylos, a single-forward 3D Gaussian framework for 3D style transfer that operates on unposed content, from a single image to a multi-view collection, conditioned on a separate reference style image. Stylos synthesizes a stylized 3D Gaussian scene without per-scene optimization or precomputed poses, achieving geometry-aware, view-consistent stylization that generalizes to unseen categories, scenes, and styles. At its core, Stylos adopts a Transformer backbone with two pathways: geometry predictions retain self-attention to preserve geometric fidelity, while style is injected via global cross-attention to enforce visual consistency across views. With the addition of a voxel-based 3D style loss that aligns aggregated scene features to style statistics, Stylos enforces view-consistent stylization while preserving geometry. Experiments across multiple datasets demonstrate that Stylos delivers high-quality zero-shot stylization, highlighting the effectiveness of global style-content coupling, the proposed 3D style loss, and the scalability of our framework from single view to large-scale multi-view settings.", "AI": {"tldr": "Stylos\u662f\u4e00\u4e2a\u5355\u524d\u54113D\u9ad8\u65af\u6846\u67b6\uff0c\u7528\u4e8e3D\u98ce\u683c\u8fc1\u79fb\uff0c\u652f\u6301\u4ece\u5355\u5f20\u56fe\u50cf\u5230\u591a\u89c6\u56fe\u96c6\u5408\u7684\u65e0\u59ff\u6001\u5185\u5bb9\uff0c\u901a\u8fc7\u53c2\u8003\u98ce\u683c\u56fe\u50cf\u8fdb\u884c\u6761\u4ef6\u5316\u3002", "motivation": "\u89e3\u51b3\u73b0\u67093D\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\u9700\u8981\u9010\u573a\u666f\u4f18\u5316\u6216\u9884\u8ba1\u7b97\u59ff\u6001\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u65e0\u9700\u4f18\u5316\u5373\u53ef\u751f\u6210\u51e0\u4f55\u611f\u77e5\u3001\u89c6\u89d2\u4e00\u81f4\u7684\u98ce\u683c\u53163D\u573a\u666f\u3002", "method": "\u91c7\u7528Transformer\u4e3b\u5e72\u7f51\u7edc\uff0c\u5305\u542b\u4e24\u6761\u8def\u5f84\uff1a\u51e0\u4f55\u9884\u6d4b\u4fdd\u7559\u81ea\u6ce8\u610f\u529b\u4ee5\u4fdd\u6301\u51e0\u4f55\u4fdd\u771f\u5ea6\uff0c\u98ce\u683c\u901a\u8fc7\u5168\u5c40\u4ea4\u53c9\u6ce8\u610f\u529b\u6ce8\u5165\u4ee5\u786e\u4fdd\u89c6\u89d2\u4e00\u81f4\u6027\u3002\u7ed3\u5408\u57fa\u4e8e\u4f53\u7d20\u76843D\u98ce\u683c\u635f\u5931\u6765\u5bf9\u9f50\u573a\u666f\u7279\u5f81\u4e0e\u98ce\u683c\u7edf\u8ba1\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cStylos\u80fd\u591f\u5b9e\u73b0\u9ad8\u8d28\u91cf\u96f6\u6837\u672c\u98ce\u683c\u8fc1\u79fb\uff0c\u9a8c\u8bc1\u4e86\u5168\u5c40\u98ce\u683c-\u5185\u5bb9\u8026\u5408\u30013D\u98ce\u683c\u635f\u5931\u7684\u6709\u6548\u6027\u4ee5\u53ca\u4ece\u5355\u89c6\u56fe\u5230\u5927\u89c4\u6a21\u591a\u89c6\u56fe\u8bbe\u7f6e\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "Stylos\u6846\u67b6\u901a\u8fc7\u5355\u524d\u5411\u4f20\u64ad\u5b9e\u73b0\u4e86\u65e0\u9700\u4f18\u5316\u76843D\u98ce\u683c\u8fc1\u79fb\uff0c\u5728\u4fdd\u6301\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u89c6\u89d2\u4e00\u81f4\u7684\u98ce\u683c\u5316\u6548\u679c\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.26489", "pdf": "https://arxiv.org/pdf/2509.26489", "abs": "https://arxiv.org/abs/2509.26489", "authors": ["Sattwik Basu", "Chaitanya Amballa", "Zhongweiyang Xu", "Jorge Van\u010do Sampedro", "Srihari Nelakuditi", "Romit Roy Choudhury"], "title": "Contrastive Diffusion Guidance for Spatial Inverse Problems", "categories": ["cs.CV", "cs.LG", "eess.SP"], "comment": null, "summary": "We consider the inverse problem of reconstructing the spatial layout of a place, a home floorplan for example, from a user`s movements inside that layout. Direct inversion is ill-posed since many floorplans can explain the same movement trajectories. We adopt a diffusion-based posterior sampler to generate layouts consistent with the measurements. While active research is in progress on generative inverse solvers, we find that the forward operator in our problem poses new challenges. The path-planning process inside a floorplan is a non-invertible, non-differentiable function, and causes instability while optimizing using the likelihood score. We break-away from existing approaches and reformulate the likelihood score in a smoother embedding space. The embedding space is trained with a contrastive loss which brings compatible floorplans and trajectories close to each other, while pushing mismatched pairs far apart. We show that a surrogate form of the likelihood score in this embedding space is a valid approximation of the true likelihood score, making it possible to steer the denoising process towards the posterior. Across extensive experiments, our model CoGuide produces more consistent floorplans from trajectories, and is more robust than differentiable-planner baselines and guided-diffusion methods.", "AI": {"tldr": "\u63d0\u51faCoGuide\u65b9\u6cd5\uff0c\u901a\u8fc7\u7528\u6237\u79fb\u52a8\u8f68\u8ff9\u91cd\u5efa\u7a7a\u95f4\u5e03\u5c40\uff08\u5982\u623f\u5c4b\u5e73\u9762\u56fe\uff09\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u548c\u5bf9\u6bd4\u5b66\u4e60\u5d4c\u5165\u7a7a\u95f4\u6765\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "motivation": "\u4ece\u7528\u6237\u79fb\u52a8\u8f68\u8ff9\u91cd\u5efa\u7a7a\u95f4\u5e03\u5c40\u662f\u75c5\u6001\u9006\u95ee\u9898\uff0c\u56e0\u4e3a\u591a\u4e2a\u5e03\u5c40\u53ef\u4ee5\u89e3\u91ca\u76f8\u540c\u8f68\u8ff9\u3002\u73b0\u6709\u751f\u6210\u5f0f\u9006\u6c42\u89e3\u5668\u9762\u4e34\u524d\u5411\u7b97\u5b50\u4e0d\u53ef\u9006\u3001\u4e0d\u53ef\u5fae\u5bfc\u81f4\u7684\u4f18\u5316\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6269\u6563\u540e\u9a8c\u91c7\u6837\u5668\u751f\u6210\u4e0e\u6d4b\u91cf\u4e00\u81f4\u7684\u5e03\u5c40\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u5d4c\u5165\u7a7a\u95f4\uff0c\u4f7f\u517c\u5bb9\u7684\u5e73\u9762\u56fe\u548c\u8f68\u8ff9\u9760\u8fd1\uff0c\u4e0d\u5339\u914d\u7684\u8fdc\u79bb\u3002\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5b9a\u4e49\u66ff\u4ee3\u4f3c\u7136\u5206\u6570\u6765\u6307\u5bfc\u53bb\u566a\u8fc7\u7a0b\u3002", "result": "CoGuide\u6a21\u578b\u5728\u5b9e\u9a8c\u4e2d\u6bd4\u53ef\u5fae\u5206\u89c4\u5212\u5668\u57fa\u7ebf\u548c\u5f15\u5bfc\u6269\u6563\u65b9\u6cd5\u4ea7\u751f\u66f4\u4e00\u81f4\u7684\u5e73\u9762\u56fe\uff0c\u4e14\u66f4\u7a33\u5065\u3002", "conclusion": "\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5b9a\u4e49\u66ff\u4ee3\u4f3c\u7136\u5206\u6570\u662f\u771f\u5b9e\u4f3c\u7136\u5206\u6570\u7684\u6709\u6548\u8fd1\u4f3c\uff0c\u80fd\u591f\u6210\u529f\u5f15\u5bfc\u53bb\u566a\u8fc7\u7a0b\u8d8b\u5411\u540e\u9a8c\u5206\u5e03\u3002"}}
{"id": "2509.26555", "pdf": "https://arxiv.org/pdf/2509.26555", "abs": "https://arxiv.org/abs/2509.26555", "authors": ["Agneet Chatterjee", "Rahim Entezari", "Maksym Zhuravinskyi", "Maksim Lapin", "Reshinth Adithyan", "Amit Raj", "Chitta Baral", "Yezhou Yang", "Varun Jampani"], "title": "Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation", "categories": ["cs.CV"], "comment": "NeurIPS 2025. Project Page : https://stable-cinemetrics.github.io/", "summary": "Recent advances in video generation have enabled high-fidelity video synthesis from user provided prompts. However, existing models and benchmarks fail to capture the complexity and requirements of professional video generation. Towards that goal, we introduce Stable Cinemetrics, a structured evaluation framework that formalizes filmmaking controls into four disentangled, hierarchical taxonomies: Setup, Event, Lighting, and Camera. Together, these taxonomies define 76 fine-grained control nodes grounded in industry practices. Using these taxonomies, we construct a benchmark of prompts aligned with professional use cases and develop an automated pipeline for prompt categorization and question generation, enabling independent evaluation of each control dimension. We conduct a large-scale human study spanning 10+ models and 20K videos, annotated by a pool of 80+ film professionals. Our analysis, both coarse and fine-grained reveal that even the strongest current models exhibit significant gaps, particularly in Events and Camera-related controls. To enable scalable evaluation, we train an automatic evaluator, a vision-language model aligned with expert annotations that outperforms existing zero-shot baselines. SCINE is the first approach to situate professional video generation within the landscape of video generative models, introducing taxonomies centered around cinematic controls and supporting them with structured evaluation pipelines and detailed analyses to guide future research.", "AI": {"tldr": "\u63d0\u51fa\u4e86Stable Cinemetrics\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06\u7535\u5f71\u5236\u4f5c\u63a7\u5236\u5206\u4e3a\u56db\u4e2a\u5c42\u6b21\u5316\u5206\u7c7b\u4f53\u7cfb\uff1aSetup\u3001Event\u3001Lighting\u548cCamera\uff0c\u5305\u542b76\u4e2a\u7ec6\u7c92\u5ea6\u63a7\u5236\u8282\u70b9\uff0c\u5e76\u6784\u5efa\u4e86\u4e13\u4e1a\u7528\u4f8b\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u6355\u6349\u4e13\u4e1a\u89c6\u9891\u751f\u6210\u7684\u590d\u6742\u6027\u548c\u9700\u6c42\uff0c\u9700\u8981\u5efa\u7acb\u9762\u5411\u4e13\u4e1a\u7535\u5f71\u5236\u4f5c\u7684\u7ed3\u6784\u5316\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u5f00\u53d1\u4e86\u56db\u4e2a\u89e3\u8026\u7684\u5c42\u6b21\u5316\u5206\u7c7b\u4f53\u7cfb\uff0c\u6784\u5efa\u4e13\u4e1a\u7528\u4f8b\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5efa\u7acb\u81ea\u52a8\u5316\u63d0\u793a\u5206\u7c7b\u548c\u95ee\u9898\u751f\u6210\u6d41\u7a0b\uff0c\u8fdb\u884c\u5927\u89c4\u6a21\u4eba\u7c7b\u7814\u7a76\uff0810+\u6a21\u578b\uff0c20K\u89c6\u9891\uff0c80+\u7535\u5f71\u4e13\u4e1a\u4eba\u58eb\u6807\u6ce8\uff09\u3002", "result": "\u5373\u4f7f\u5f53\u524d\u6700\u5f3a\u6a21\u578b\u5728Events\u548cCamera\u76f8\u5173\u63a7\u5236\u65b9\u9762\u4ecd\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u8bad\u7ec3\u4e86\u4f18\u4e8e\u73b0\u6709\u96f6\u6837\u672c\u57fa\u7ebf\u7684\u81ea\u52a8\u8bc4\u4f30\u5668\u3002", "conclusion": "SCINE\u662f\u9996\u4e2a\u5c06\u4e13\u4e1a\u89c6\u9891\u751f\u6210\u7f6e\u4e8e\u89c6\u9891\u751f\u6210\u6a21\u578b\u80cc\u666f\u4e0b\u7684\u65b9\u6cd5\uff0c\u5f15\u5165\u4e86\u4ee5\u7535\u5f71\u63a7\u5236\u4e3a\u4e2d\u5fc3\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u901a\u8fc7\u7ed3\u6784\u5316\u8bc4\u4f30\u6d41\u7a0b\u548c\u8be6\u7ec6\u5206\u6790\u6307\u5bfc\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2509.26641", "pdf": "https://arxiv.org/pdf/2509.26641", "abs": "https://arxiv.org/abs/2509.26641", "authors": ["Yuxin Song", "Wenkai Dong", "Shizun Wang", "Qi Zhang", "Song Xue", "Tao Yuan", "Hu Yang", "Haocheng Feng", "Hang Zhou", "Xinyan Xiao", "Jingdong Wang"], "title": "Query-Kontext: An Unified Multimodal Model for Image Generation and Editing", "categories": ["cs.CV"], "comment": "23 pages, 10 figures", "summary": "Unified Multimodal Models (UMMs) have demonstrated remarkable performance in text-to-image generation (T2I) and editing (TI2I), whether instantiated as assembled unified frameworks which couple powerful vision-language model (VLM) with diffusion-based generator, or as naive Unified Multimodal Models with an early fusion of understanding and generation modalities. We contend that in current unified frameworks, the crucial capability of multimodal generative reasoning which encompasses instruction understanding, grounding, and image referring for identity preservation and faithful reconstruction, is intrinsically entangled with high-fidelity synthesis. In this work, we introduce Query-Kontext, a novel approach that bridges the VLM and diffusion model via a multimodal ``kontext'' composed of semantic cues and coarse-grained image conditions encoded from multimodal inputs. This design delegates the complex ability of multimodal generative reasoning to powerful VLM while reserving diffusion model's role for high-quality visual synthesis. To achieve this, we propose a three-stage progressive training strategy. First, we connect the VLM to a lightweight diffusion head via multimodal kontext tokens to unleash the VLM's generative reasoning ability. Second, we scale this head to a large, pre-trained diffusion model to enhance visual detail and realism. Finally, we introduce a low-level image encoder to improve image fidelity and perform instruction tuning on downstream tasks. Furthermore, we build a comprehensive data pipeline integrating real, synthetic, and open-source datasets, covering diverse multimodal reference-to-image scenarios, including image generation, instruction-driven editing, customized generation, and multi-subject composition. Experiments show that our approach matches strong unified baselines and even outperforms task-specific state-of-the-art methods in several cases.", "AI": {"tldr": "Query-Kontext\u662f\u4e00\u79cd\u65b0\u578b\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\"kontext\"\u8fde\u63a5\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\uff0c\u5c06\u591a\u6a21\u6001\u751f\u6210\u63a8\u7406\u80fd\u529b\u59d4\u6258\u7ed9VLM\uff0c\u540c\u65f6\u4fdd\u7559\u6269\u6563\u6a21\u578b\u7684\u9ad8\u8d28\u91cf\u89c6\u89c9\u5408\u6210\u529f\u80fd\u3002", "motivation": "\u5f53\u524d\u7edf\u4e00\u6846\u67b6\u4e2d\uff0c\u591a\u6a21\u6001\u751f\u6210\u63a8\u7406\u80fd\u529b\uff08\u5305\u62ec\u6307\u4ee4\u7406\u89e3\u3001\u5b9a\u4f4d\u548c\u56fe\u50cf\u5f15\u7528\uff09\u4e0e\u9ad8\u4fdd\u771f\u5408\u6210\u80fd\u529b\u5185\u5728\u7ea0\u7f20\uff0c\u9700\u8981\u89e3\u8026\u8fd9\u4e9b\u529f\u80fd\u3002", "method": "\u63d0\u51fa\u4e09\u9636\u6bb5\u6e10\u8fdb\u8bad\u7ec3\u7b56\u7565\uff1a1\uff09\u901a\u8fc7\u591a\u6a21\u6001kontext\u4ee4\u724c\u8fde\u63a5VLM\u548c\u8f7b\u91cf\u6269\u6563\u5934\uff1b2\uff09\u6269\u5c55\u5230\u5927\u578b\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff1b3\uff09\u5f15\u5165\u4f4e\u7ea7\u56fe\u50cf\u7f16\u7801\u5668\u8fdb\u884c\u6307\u4ee4\u8c03\u4f18\u3002\u6784\u5efa\u7efc\u5408\u6570\u636e\u7ba1\u9053\u8986\u76d6\u591a\u79cd\u591a\u6a21\u6001\u53c2\u8003\u5230\u56fe\u50cf\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0e\u5f3a\u5927\u7edf\u4e00\u57fa\u7ebf\u76f8\u5f53\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u751a\u81f3\u4f18\u4e8e\u7279\u5b9a\u4efb\u52a1\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "Query-Kontext\u6210\u529f\u89e3\u8026\u4e86\u591a\u6a21\u6001\u751f\u6210\u63a8\u7406\u548c\u89c6\u89c9\u5408\u6210\uff0c\u901a\u8fc7\u59d4\u6258\u590d\u6742\u63a8\u7406\u7ed9VLM\u5e76\u4fdd\u7559\u6269\u6563\u6a21\u578b\u7684\u5408\u6210\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2509.25562", "pdf": "https://arxiv.org/pdf/2509.25562", "abs": "https://arxiv.org/abs/2509.25562", "authors": ["Yihang Chen", "Yuanhao Ban", "Yunqi Hong", "Cho-Jui Hsieh"], "title": "IRIS: Intrinsic Reward Image Synthesis", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Despite the success of Reinforcement Learning from Human Feedback (RLHF) in language reasoning, its application to autoregressive Text-to-Image (T2I) generation is often constrained by the limited availability of human preference data. This paper explores how an autoregressive T2I model can learn from internal signals without relying on external rewards or labeled data. Contrary to recent findings in text generation, we show that maximizing self-uncertainty, rather than self-certainty, improves image generation. We observe that this is because autoregressive T2I models with low uncertainty tend to generate simple and uniform images, which are less aligned with human preferences. Based on these observations, we propose IRIS (Intrinsic Reward Image Synthesis), the first framework to improve autoregressive T2I models with reinforcement learning using only an intrinsic reward. Empirical results demonstrate that applying IRIS to autoregressive T2I models achieves performance that is competitive with or superior to external rewards.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86IRIS\u6846\u67b6\uff0c\u9996\u6b21\u4f7f\u7528\u5185\u5728\u5956\u52b1\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6539\u8fdb\u81ea\u56de\u5f52\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u5956\u52b1\u6216\u6807\u8bb0\u6570\u636e\u3002", "motivation": "\u7531\u4e8e\u4eba\u7c7b\u504f\u597d\u6570\u636e\u6709\u9650\uff0c\u4f20\u7edf\u7684\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\u5728\u81ea\u56de\u5f52\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u5e94\u7528\u53d7\u9650\uff0c\u9700\u8981\u63a2\u7d22\u4e0d\u4f9d\u8d56\u5916\u90e8\u4fe1\u53f7\u7684\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51faIRIS\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5927\u5316\u6a21\u578b\u7684\u81ea\u4e0d\u786e\u5b9a\u6027\u800c\u975e\u81ea\u786e\u5b9a\u6027\u4f5c\u4e3a\u5185\u5728\u5956\u52b1\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u81ea\u56de\u5f52\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cIRIS\u6846\u67b6\u5728\u81ea\u56de\u5f52\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4e0a\u7684\u8868\u73b0\u4e0e\u5916\u90e8\u5956\u52b1\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u3002", "conclusion": "\u6700\u5927\u5316\u81ea\u4e0d\u786e\u5b9a\u6027\u53ef\u4ee5\u6709\u6548\u6539\u8fdb\u81ea\u56de\u5f52\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff0cIRIS\u6846\u67b6\u4e3a\u65e0\u5916\u90e8\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.25670", "pdf": "https://arxiv.org/pdf/2509.25670", "abs": "https://arxiv.org/abs/2509.25670", "authors": ["Kang Yang", "Yifan Liang", "Fangkun Liu", "Zhenping Xie", "Chengshi Zheng"], "title": "LTA-L2S: Lexical Tone-Aware Lip-to-Speech Synthesis for Mandarin with Cross-Lingual Transfer Learning", "categories": ["cs.SD", "cs.CV"], "comment": "Submitted to ICASSP 2026", "summary": "Lip-to-speech (L2S) synthesis for Mandarin is a significant challenge, hindered by complex viseme-to-phoneme mappings and the critical role of lexical tones in intelligibility. To address this issue, we propose Lexical Tone-Aware Lip-to-Speech (LTA-L2S). To tackle viseme-to-phoneme complexity, our model adapts an English pre-trained audio-visual self-supervised learning (SSL) model via a cross-lingual transfer learning strategy. This strategy not only transfers universal knowledge learned from extensive English data to the Mandarin domain but also circumvents the prohibitive cost of training such a model from scratch. To specifically model lexical tones and enhance intelligibility, we further employ a flow-matching model to generate the F0 contour. This generation process is guided by ASR-fine-tuned SSL speech units, which contain crucial suprasegmental information. The overall speech quality is then elevated through a two-stage training paradigm, where a flow-matching postnet refines the coarse spectrogram from the first stage. Extensive experiments demonstrate that LTA-L2S significantly outperforms existing methods in both speech intelligibility and tonal accuracy.", "AI": {"tldr": "\u63d0\u51faLexical Tone-Aware Lip-to-Speech (LTA-L2S)\u6a21\u578b\uff0c\u901a\u8fc7\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u548c\u6d41\u5339\u914d\u6a21\u578b\u89e3\u51b3\u666e\u901a\u8bdd\u5507\u8bed\u5408\u6210\u4e2d\u7684\u97f3\u7d20\u6620\u5c04\u590d\u6742\u6027\u548c\u58f0\u8c03\u5efa\u6a21\u95ee\u9898\u3002", "motivation": "\u666e\u901a\u8bdd\u5507\u8bed\u5408\u6210\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u590d\u6742\u7684\u89c6\u7d20-\u97f3\u7d20\u6620\u5c04\u5173\u7cfb\uff0c\u4ee5\u53ca\u58f0\u8c03\u5bf9\u53ef\u7406\u89e3\u6027\u7684\u5173\u952e\u4f5c\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u7279\u6027\u3002", "method": "1. \u4f7f\u7528\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\uff0c\u5c06\u82f1\u8bed\u9884\u8bad\u7ec3\u7684\u89c6\u542c\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u9002\u914d\u5230\u666e\u901a\u8bdd\u9886\u57df\uff1b2. \u91c7\u7528\u6d41\u5339\u914d\u6a21\u578b\u751f\u6210F0\u8f6e\u5ed3\uff0c\u7531ASR\u5fae\u8c03\u7684SSL\u8bed\u97f3\u5355\u5143\u6307\u5bfc\uff1b3. \u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u6d41\u5339\u914d\u540e\u5904\u7406\u7f51\u7edc\u7ec6\u5316\u7c97\u7c92\u5ea6\u9891\u8c31\u56fe\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cLTA-L2S\u5728\u8bed\u97f3\u53ef\u7406\u89e3\u6027\u548c\u58f0\u8c03\u51c6\u786e\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6a21\u578b\u6210\u529f\u89e3\u51b3\u4e86\u666e\u901a\u8bdd\u5507\u8bed\u5408\u6210\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u8de8\u8bed\u8a00\u8fc1\u79fb\u548c\u58f0\u8c03\u611f\u77e5\u5efa\u6a21\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002"}}
{"id": "2509.25792", "pdf": "https://arxiv.org/pdf/2509.25792", "abs": "https://arxiv.org/abs/2509.25792", "authors": ["Alexander Branch", "Omead Pooladzandi", "Radin Khosraviani", "Sunay Gajanan Bhat", "Jeffrey Jiang", "Gregory Pottie"], "title": "PUREVQ-GAN: Defending Data Poisoning Attacks through Vector-Quantized Bottlenecks", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "We introduce PureVQ-GAN, a defense against data poisoning that forces backdoor triggers through a discrete bottleneck using Vector-Quantized VAE with GAN discriminator. By quantizing poisoned images through a learned codebook, PureVQ-GAN destroys fine-grained trigger patterns while preserving semantic content. A GAN discriminator ensures outputs match the natural image distribution, preventing reconstruction of out-of-distribution perturbations. On CIFAR-10, PureVQ-GAN achieves 0% poison success rate (PSR) against Gradient Matching and Bullseye Polytope attacks, and 1.64% against Narcissus while maintaining 91-95% clean accuracy. Unlike diffusion-based defenses requiring hundreds of iterative refinement steps, PureVQ-GAN is over 50x faster, making it practical for real training pipelines.", "AI": {"tldr": "PureVQ-GAN\u662f\u4e00\u79cd\u57fa\u4e8e\u5411\u91cf\u91cf\u5316VAE\u548cGAN\u9274\u522b\u5668\u7684\u6570\u636e\u6295\u6bd2\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u6563\u74f6\u9888\u7834\u574f\u540e\u95e8\u89e6\u53d1\u5668\u6a21\u5f0f\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u5185\u5bb9\u5b8c\u6574\u6027\u3002", "motivation": "\u9488\u5bf9\u6570\u636e\u6295\u6bd2\u653b\u51fb\u4e2d\u540e\u95e8\u89e6\u53d1\u5668\u7684\u5a01\u80c1\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u6709\u6548\u6d88\u9664\u89e6\u53d1\u5668\u53c8\u80fd\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u9ad8\u6548\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5411\u91cf\u91cf\u5316VAE\u5c06\u6295\u6bd2\u56fe\u50cf\u901a\u8fc7\u5b66\u4e60\u7684\u7801\u672c\u8fdb\u884c\u91cf\u5316\uff0c\u7834\u574f\u7ec6\u7c92\u5ea6\u89e6\u53d1\u5668\u6a21\u5f0f\uff0c\u540c\u65f6\u7528GAN\u9274\u522b\u5668\u786e\u4fdd\u8f93\u51fa\u7b26\u5408\u81ea\u7136\u56fe\u50cf\u5206\u5e03\uff0c\u9632\u6b62\u91cd\u6784\u5206\u5e03\u5916\u6270\u52a8\u3002", "result": "\u5728CIFAR-10\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9Gradient Matching\u548cBullseye Polytope\u653b\u51fb\u5b9e\u73b00%\u6295\u6bd2\u6210\u529f\u7387\uff0c\u5bf9Narcissus\u653b\u51fb\u4e3a1.64%\uff0c\u540c\u65f6\u4fdd\u630191-95%\u7684\u5e72\u51c0\u51c6\u786e\u7387\uff0c\u6bd4\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u5feb50\u500d\u4ee5\u4e0a\u3002", "conclusion": "PureVQ-GAN\u662f\u4e00\u79cd\u9ad8\u6548\u5b9e\u7528\u7684\u6570\u636e\u6295\u6bd2\u9632\u5fa1\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u9ad8\u9632\u5fa1\u6548\u679c\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u4e86\u5904\u7406\u901f\u5ea6\u3002"}}
{"id": "2509.25817", "pdf": "https://arxiv.org/pdf/2509.25817", "abs": "https://arxiv.org/abs/2509.25817", "authors": ["Jaeyoung Kim", "Jongho Lee", "Hongjun Choi", "Sion Jang"], "title": "Personalized Scientific Figure Caption Generation: An Empirical Study on Author-Specific Writing Style Transfer", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "We study personalized figure caption generation using author profile data from scientific papers. Our experiments demonstrate that rich author profile data, combined with relevant metadata, can significantly improve the personalization performance of multimodal large language models. However, we also reveal a fundamental trade-off between matching author style and maintaining caption quality. Our findings offer valuable insights and future directions for developing practical caption automation systems that balance both objectives. This work was conducted as part of the 3rd SciCap challenge.", "AI": {"tldr": "\u4f7f\u7528\u79d1\u5b66\u8bba\u6587\u4f5c\u8005\u6863\u6848\u6570\u636e\u8fdb\u884c\u4e2a\u6027\u5316\u56fe\u8868\u6807\u9898\u751f\u6210\u7684\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u4f5c\u8005\u98ce\u683c\u5339\u914d\u4e0e\u6807\u9898\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528\u4f5c\u8005\u6863\u6848\u6570\u636e\u548c\u76f8\u5173\u5143\u6570\u636e\u6765\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e2a\u6027\u5316\u56fe\u8868\u6807\u9898\u751f\u6210\u65b9\u9762\u7684\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u4e30\u5bcc\u7684\u4f5c\u8005\u6863\u6848\u6570\u636e\u548c\u76f8\u5173\u5143\u6570\u636e\uff0c\u5728\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u4e2a\u6027\u5316\u56fe\u8868\u6807\u9898\u751f\u6210\u7684\u5b9e\u9a8c\u3002", "result": "\u4f5c\u8005\u6863\u6848\u6570\u636e\u80fd\u663e\u8457\u63d0\u5347\u4e2a\u6027\u5316\u6027\u80fd\uff0c\u4f46\u5b58\u5728\u4f5c\u8005\u98ce\u683c\u5339\u914d\u4e0e\u6807\u9898\u8d28\u91cf\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u4e3a\u5f00\u53d1\u5e73\u8861\u4e2a\u6027\u5316\u98ce\u683c\u548c\u6807\u9898\u8d28\u91cf\u7684\u5b9e\u7528\u6807\u9898\u81ea\u52a8\u5316\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u548c\u672a\u6765\u65b9\u5411\u3002"}}
