{"id": "2509.03006", "pdf": "https://arxiv.org/pdf/2509.03006", "abs": "https://arxiv.org/abs/2509.03006", "authors": ["Tzuhsuan Huang", "Cheng Yu Yeo", "Tsai-Ling Huang", "Hong-Han Shuai", "Wen-Huang Cheng", "Jun-Cheng Chen"], "title": "Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers", "categories": ["cs.CV"], "comment": "10 pages", "summary": "Recent studies on deep watermarking have predominantly focused on in-processing watermarking, which integrates the watermarking process into image generation. However, post-processing watermarking, which embeds watermarks after image generation, offers more flexibility. It can be applied to outputs from any generative model (e.g. GANs, diffusion models) without needing access to the model's internal structure. It also allows users to embed unique watermarks into individual images. Therefore, this study focuses on post-processing watermarking and enhances its robustness by incorporating an ensemble attack network during training. We construct various versions of attack networks using CNN and Transformer in both spatial and frequency domains to investigate how each combination influences the robustness of the watermarking model. Our results demonstrate that combining a CNN-based attack network in the spatial domain with a Transformer-based attack network in the frequency domain yields the highest robustness in watermarking models. Extensive evaluation on the WAVES benchmark, using average bit accuracy as the metric, demonstrates that our ensemble attack network significantly enhances the robustness of baseline watermarking methods under various stress tests. In particular, for the Regeneration Attack defined in WAVES, our method improves StegaStamp by 18.743%. The code is released at:https://github.com/aiiu-lab/DeepRobustWatermark.", "AI": {"tldr": "\u901a\u8fc7\u7ec4\u5408CNN\u548cTransformer\u653b\u51fb\u7f51\u7edc\u5728\u7a7a\u95f4\u57df\u548c\u9891\u57df\u7684\u4f18\u52bf\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96c6\u6210\u653b\u51fb\u7f51\u7edc\u7684\u540e\u5904\u7406\u6df1\u5ea6\u6c34\u5370\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u5370\u6a21\u578b\u7684\u7a33\u5065\u6027\u3002", "motivation": "\u540e\u5904\u7406\u6c34\u5370\u6280\u672f\u6bd4\u8f83\u7075\u6d3b\uff0c\u53ef\u4ee5\u5e94\u7528\u4e8e\u4efb\u4f55\u751f\u6210\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u5e76\u5141\u8bb8\u7528\u6237\u4e3a\u5355\u4e2a\u56fe\u50cf\u5d4c\u5165\u72ec\u7279\u6c34\u5370\uff0c\u4f46\u9700\u8981\u63d0\u5347\u5176\u7a33\u5065\u6027\u3002", "method": "\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6dfb\u52a0\u96c6\u6210\u653b\u51fb\u7f51\u7edc\uff0c\u6784\u5efa\u4e86CNN\u548cTransformer\u5728\u7a7a\u95f4\u57df\u548c\u9891\u57df\u7684\u5404\u79cd\u7ec4\u5408\u7248\u672c\u653b\u51fb\u7f51\u7edc\uff0c\u7814\u7a76\u4e0d\u540c\u7ec4\u5408\u5bf9\u6c34\u5370\u6a21\u578b\u7a33\u5065\u6027\u7684\u5f71\u54cd\u3002", "result": "\u7ec4\u5408\u7a7a\u95f4\u57dfCNN\u57fa\u653b\u51fb\u7f51\u7edc\u548c\u9891\u57dfTransformer\u57fa\u653b\u51fb\u7f51\u7edc\u83b7\u5f97\u6700\u9ad8\u7a33\u5065\u6027\u3002\u5728WAVES\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7ebf\u6c34\u5370\u65b9\u6cd5\u7684\u7a33\u5065\u6027\uff0c\u5728\u91cd\u751f\u653b\u51fb\u4e2dStegaStamp\u63d0\u534718.743%\u3002", "conclusion": "\u901a\u8fc7\u96c6\u6210\u591a\u79cd\u653b\u51fb\u7f51\u7edc\u5728\u8bad\u7ec3\u4e2d\u53ef\u6709\u6548\u63d0\u5347\u540e\u5904\u7406\u6c34\u5370\u6a21\u578b\u7684\u7a33\u5065\u6027\uff0c\u7a7a\u95f4\u57dfCNN\u548c\u9891\u57dfTransformer\u7684\u7ec4\u5408\u8868\u73b0\u6700\u4f18\u3002"}}
{"id": "2509.03044", "pdf": "https://arxiv.org/pdf/2509.03044", "abs": "https://arxiv.org/abs/2509.03044", "authors": ["Chengjie Huang", "Jiafeng Yan", "Jing Li", "Lu Bai"], "title": "DCDB: Dynamic Conditional Dual Diffusion Bridge for Ill-posed Multi-Tasks", "categories": ["cs.CV"], "comment": "15 pages,6 figures", "summary": "Conditional diffusion models have made impressive progress in the field of image processing, but the characteristics of constructing data distribution pathways make it difficult to exploit the intrinsic correlation between tasks in multi-task scenarios, which is even worse in ill-posed tasks with a lack of training data. In addition, traditional static condition control makes it difficult for networks to learn in multi-task scenarios with its dynamically evolving characteristics. To address these challenges, we propose a dynamic conditional double diffusion bridge training paradigm to build a general framework for ill-posed multi-tasks. Firstly, this paradigm decouples the diffusion and condition generation processes, avoiding the dependence of the diffusion model on supervised data in ill-posed tasks. Secondly, generated by the same noise schedule, dynamic conditions are used to gradually adjust their statistical characteristics, naturally embed time-related information, and reduce the difficulty of network learning. We analyze the learning objectives of the network under different conditional forms in the single-step denoising process and compare the changes in its attention weights in the network, demonstrating the superiority of our dynamic conditions. Taking dehazing and visible-infrared fusion as typical ill-posed multi-task scenarios, we achieve the best performance in multiple indicators on public datasets. The code has been publicly released at: https://anonymous.4open.science/r/DCDB-D3C2.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6761\u4ef6\u53cc\u6db2\u5316\u6865\u8bad\u7ec3\u8303\u5f0f\uff0c\u7528\u4e8e\u89e3\u51b3\u6761\u4ef6\u6db2\u5316\u6a21\u578b\u5728\u591a\u4efb\u52a1\u573a\u666f\u4e2d\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u6570\u636e\u7f3a\u4e4f\u7684\u4e0d\u826f\u95ee\u9898\u4e2d\u3002\u901a\u8fc7\u89e3\u8026\u6db2\u5316\u548c\u6761\u4ef6\u751f\u6210\u8fc7\u7a0b\uff0c\u4ee5\u53ca\u4f7f\u7528\u52a8\u6001\u6761\u4ef6\u6765\u6e10\u8fdb\u8c03\u6574\u7edf\u8ba1\u7279\u5f81\uff0c\u8be5\u65b9\u6cd5\u5728\u53bb\u971c\u548c\u53ef\u89c1-\u7ea2\u5916\u878d\u5408\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u6761\u4ef6\u6db2\u5316\u6a21\u578b\u5728\u591a\u4efb\u52a1\u573a\u666f\u4e2d\u96be\u4ee5\u5229\u7528\u4efb\u52a1\u95f4\u7684\u5185\u5728\u5173\u8054\u6027\uff0c\u5c24\u5176\u662f\u5728\u8bad\u7ec3\u6570\u636e\u7f3a\u4e4f\u7684\u4e0d\u826f\u95ee\u9898\u4e2d\u3002\u4f20\u7edf\u7684\u9759\u6001\u6761\u4ef6\u63a7\u5236\u65b9\u5f0f\u96be\u4ee5\u9002\u5e94\u591a\u4efb\u52a1\u573a\u666f\u7684\u52a8\u6001\u6f14\u5316\u7279\u6027\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u6761\u4ef6\u53cc\u6db2\u5316\u6865\u8bad\u7ec3\u8303\u5f0f\uff1a1) \u89e3\u8026\u6db2\u5316\u548c\u6761\u4ef6\u751f\u6210\u8fc7\u7a0b\uff0c\u907f\u514d\u6db2\u5316\u6a21\u578b\u5bf9\u76d1\u7763\u6570\u636e\u7684\u4f9d\u8d56\uff1b2) \u4f7f\u7528\u76f8\u540c\u7684\u566a\u58f0\u8c03\u5ea6\u751f\u6210\u52a8\u6001\u6761\u4ef6\uff0c\u6e10\u8fdb\u8c03\u6574\u7edf\u8ba1\u7279\u5f81\u5e76\u5d4c\u5165\u65f6\u95f4\u76f8\u5173\u4fe1\u606f\uff0c\u964d\u4f4e\u7f51\u7edc\u5b66\u4e60\u96be\u5ea6\u3002", "result": "\u5728\u53bb\u971c\u548c\u53ef\u89c1-\u7ea2\u5916\u878d\u5408\u4e24\u4e2a\u5178\u578b\u7684\u4e0d\u826f\u591a\u4efb\u52a1\u573a\u666f\u4e2d\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u591a\u4e2a\u6307\u6807\u4e0a\u8fbe\u5230\u4e86\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u6761\u4ef6\u63a7\u5236\u548c\u6db2\u5316\u8fc7\u7a0b\u89e3\u8026\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u4efb\u52a1\u6761\u4ef6\u6db2\u5316\u6a21\u578b\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u6570\u636e\u7f3a\u4e4f\u7684\u4e0d\u826f\u95ee\u9898\u4e2d\u663e\u793a\u51fa\u4f18\u52c3\u7684\u6027\u80fd\u3002"}}
{"id": "2509.03062", "pdf": "https://arxiv.org/pdf/2509.03062", "abs": "https://arxiv.org/abs/2509.03062", "authors": ["S M Rafiuddin"], "title": "High Cursive Complex Character Recognition using GAN External Classifier", "categories": ["cs.CV"], "comment": "Comments: 10 pages, 8 figures, published in the Proceedings of the   2nd International Conference on Computing Advancements (ICCA 2022). Paper   introduces ADA-GAN with an external classifier for complex cursive   handwritten character recognition, evaluated on MNIST and BanglaLekha   datasets, showing improved robustness compared to CNN baselines", "summary": "Handwritten characters can be trickier to classify due to their complex and cursive nature compared to simple and non-cursive characters. We present an external classifier along with a Generative Adversarial Network that can classify highly cursive and complex characters. The generator network produces fake handwritten character images, which are then used to augment the training data after adding adversarially perturbed noise and achieving a confidence score above a threshold with the discriminator network. The results show that the accuracy of convolutional neural networks decreases as character complexity increases, but our proposed model, ADA-GAN, remains more robust and effective for both cursive and complex characters.", "AI": {"tldr": "\u901a\u8fc7GAN\u751f\u6210\u5047\u624b\u5199\u5b57\u7b26\u56fe\u50cf\u5e76\u6dfb\u52a0\u5bf9\u6297\u6027\u566a\u58f0\u6765\u6269\u5145\u8bad\u7ec3\u6570\u636e\uff0cADA-GAN\u6a21\u578b\u5728\u590d\u6742\u548c\u8fde\u7eed\u5b57\u7b26\u5206\u7c7b\u4e2d\u663e\u793a\u66f4\u597d\u7684\u7a33\u5065\u6027\u548c\u6548\u679c\u3002", "motivation": "\u624b\u5199\u5b57\u7b26\u56e0\u5176\u590d\u6742\u548c\u8fde\u7eed\u6027\u8d28\uff0c\u6bd4\u7b80\u5355\u975e\u8fde\u7eed\u5b57\u7b26\u66f4\u96be\u5206\u7c7b\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5206\u7c7b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc(GAN)\uff0c\u751f\u6210\u5668\u7f51\u7edc\u751f\u4ea7\u5047\u624b\u5199\u5b57\u7b26\u56fe\u50cf\uff0c\u7ecf\u8fc7\u6dfb\u52a0\u5bf9\u6297\u6027\u6270\u52a8\u566a\u58f0\u5e76\u8fbe\u5230\u4e00\u5b9a\u4fe1\u5fc3\u9608\u503c\u540e\uff0c\u7528\u4e8e\u6269\u5145\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "result": "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u51c6\u786e\u6027\u968f\u5b57\u7b26\u590d\u6742\u5ea6\u589e\u52a0\u800c\u4e0b\u964d\uff0c\u4f46ADA-GAN\u6a21\u578b\u5728\u590d\u6742\u548c\u8fde\u7eed\u5b57\u7b26\u5206\u7c7b\u4e2d\u663e\u793a\u51fa\u66f4\u597d\u7684\u7a33\u5065\u6027\u548c\u6548\u679c\u3002", "conclusion": "ADA-GAN\u6a21\u578b\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u590d\u6742\u624b\u5199\u5b57\u7b26\u5206\u7c7b\u7684\u6027\u80fd\u3002"}}
{"id": "2509.03141", "pdf": "https://arxiv.org/pdf/2509.03141", "abs": "https://arxiv.org/abs/2509.03141", "authors": ["Mattia Litrico", "Francesco Guarnera", "Mario Valerio Giuffrida", "Daniele Rav\u00ec", "Sebastiano Battiato"], "title": "Temporally-Aware Diffusion Model for Brain Progression Modelling with Bidirectional Temporal Regularisation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Generating realistic MRIs to accurately predict future changes in the structure of brain is an invaluable tool for clinicians in assessing clinical outcomes and analysing the disease progression at the patient level. However, current existing methods present some limitations: (i) some approaches fail to explicitly capture the relationship between structural changes and time intervals, especially when trained on age-imbalanced datasets; (ii) others rely only on scan interpolation, which lack clinical utility, as they generate intermediate images between timepoints rather than future pathological progression; and (iii) most approaches rely on 2D slice-based architectures, thereby disregarding full 3D anatomical context, which is essential for accurate longitudinal predictions. We propose a 3D Temporally-Aware Diffusion Model (TADM-3D), which accurately predicts brain progression on MRI volumes. To better model the relationship between time interval and brain changes, TADM-3D uses a pre-trained Brain-Age Estimator (BAE) that guides the diffusion model in the generation of MRIs that accurately reflect the expected age difference between baseline and generated follow-up scans. Additionally, to further improve the temporal awareness of TADM-3D, we propose the Back-In-Time Regularisation (BITR), by training TADM-3D to predict bidirectionally from the baseline to follow-up (forward), as well as from the follow-up to baseline (backward). Although predicting past scans has limited clinical applications, this regularisation helps the model generate temporally more accurate scans. We train and evaluate TADM-3D on the OASIS-3 dataset, and we validate the generalisation performance on an external test set from the NACC dataset. The code will be available upon acceptance.", "AI": {"tldr": "\u63d0\u51faTADM-3D\u6a21\u578b\uff0c\u4f7f\u75283D\u6269\u6563\u6a21\u578b\u548c\u8111\u9f84\u4f30\u8ba1\u5668\u6765\u51c6\u786e\u9884\u6d4bMRI\u8111\u90e8\u7ed3\u6784\u968f\u65f6\u95f4\u7684\u53d8\u5316\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u95f4\u5173\u7cfb\u5efa\u6a21\u548c3D\u4e0a\u4e0b\u6587\u5229\u7528\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709MRI\u9884\u6d4b\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a(1)\u65e0\u6cd5\u660e\u786e\u6355\u6349\u7ed3\u6784\u53d8\u5316\u4e0e\u65f6\u95f4\u95f4\u9694\u7684\u5173\u7cfb\uff1b(2)\u4ec5\u4f9d\u8d56\u626b\u63cf\u63d2\u503c\uff0c\u7f3a\u4e4f\u4e34\u5e8a\u5b9e\u7528\u6027\uff1b(3)\u5927\u591a\u57fa\u4e8e2D\u67b6\u6784\uff0c\u5ffd\u7565\u4e86\u5b8c\u6574\u76843D\u89e3\u5256\u4e0a\u4e0b\u6587\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u8111\u90e8\u75c5\u7406\u8fdb\u5c55\u76843D\u65f6\u5e8f\u611f\u77e5\u6a21\u578b\u3002", "method": "\u63d0\u51faTADM-3D\uff083D\u65f6\u5e8f\u611f\u77e5\u6269\u6563\u6a21\u578b\uff09\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u8111\u9f84\u4f30\u8ba1\u5668(BAE)\u6307\u5bfc\u6269\u6563\u6a21\u578b\u751f\u6210\u51c6\u786e\u53cd\u6620\u5e74\u9f84\u5dee\u5f02\u7684MRI\u3002\u540c\u65f6\u63d0\u51fa\u56de\u6eaf\u65f6\u95f4\u6b63\u5219\u5316(BITR)\uff0c\u901a\u8fc7\u53cc\u5411\u8bad\u7ec3\uff08\u4ece\u57fa\u7ebf\u5230\u968f\u8bbf\u4ee5\u53ca\u4ece\u968f\u8bbf\u5230\u57fa\u7ebf\uff09\u63d0\u9ad8\u65f6\u5e8f\u51c6\u786e\u6027\u3002", "result": "\u5728OASIS-3\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u5e76\u5728NACC\u6570\u636e\u96c6\u7684\u5916\u90e8\u6d4b\u8bd5\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6cdb\u5316\u6027\u80fd\u3002\u4ee3\u7801\u5c06\u5728\u63a5\u53d7\u540e\u63d0\u4f9b\u3002", "conclusion": "TADM-3D\u901a\u8fc7\u7ed3\u5408\u8111\u9f84\u4f30\u8ba1\u548c\u53cc\u5411\u8bad\u7ec3\u6b63\u5219\u5316\uff0c\u80fd\u591f\u751f\u6210\u66f4\u51c6\u786e\u7684\u65f6\u5e8fMRI\u9884\u6d4b\uff0c\u4e3a\u4e34\u5e8a\u8bc4\u4f30\u75be\u75c5\u8fdb\u5c55\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\u3002"}}
{"id": "2509.03185", "pdf": "https://arxiv.org/pdf/2509.03185", "abs": "https://arxiv.org/abs/2509.03185", "authors": ["Debopom Sutradhar", "Ripon Kumar Debnath", "Mohaimenul Azam Khan Raiaan", "Yan Zhang", "Reem E. Mohamed", "Sami Azam"], "title": "PPORLD-EDNetLDCT: A Proximal Policy Optimization-Based Reinforcement Learning Framework for Adaptive Low-Dose CT Denoising", "categories": ["cs.CV"], "comment": "20 pages, 5 figures, 5 tables. Submitted to Computers in Biology and   Medicine for peer review", "summary": "Low-dose computed tomography (LDCT) is critical for minimizing radiation exposure, but it often leads to increased noise and reduced image quality. Traditional denoising methods, such as iterative optimization or supervised learning, often fail to preserve image quality. To address these challenges, we introduce PPORLD-EDNetLDCT, a reinforcement learning-based (RL) approach with Encoder-Decoder for LDCT. Our method utilizes a dynamic RL-based approach in which an advanced posterior policy optimization (PPO) algorithm is used to optimize denoising policies in real time, based on image quality feedback, trained via a custom gym environment. The experimental results on the low dose CT image and projection dataset demonstrate that the proposed PPORLD-EDNetLDCT model outperforms traditional denoising techniques and other DL-based methods, achieving a peak signal-to-noise ratio of 41.87, a structural similarity index measure of 0.9814 and a root mean squared error of 0.00236. Moreover, in NIH-AAPM-Mayo Clinic Low Dose CT Challenge dataset our method achived a PSNR of 41.52, SSIM of 0.9723 and RMSE of 0.0051. Furthermore, we validated the quality of denoising using a classification task in the COVID-19 LDCT dataset, where the images processed by our method improved the classification accuracy to 94\\%, achieving 4\\% higher accuracy compared to denoising without RL-based denoising. This method offers a promising solution for safer and more accurate LDCT imaging.", "AI": {"tldr": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5PPO\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc(PPORLD-EDNetLDCT)\uff0c\u901a\u8fc7\u5b9e\u65f6\u4f18\u5316\u53bb\u566a\u7b56\u7565\uff0c\u5728LDCT\u56fe\u50cf\u53bb\u566a\u4e2d\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684PSNR\u3001SSIM\u548c\u66f4\u4f4e\u7684RMSE\uff0c\u5e76\u5728\u65b0\u51a0\u75c5\u5206\u7c7b\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u51c6\u786e\u7387\u3002", "motivation": "\u4f4e\u5f3a\u5ea6CT\u6269\u6563\u80fd\u591f\u6700\u5c0f\u5316\u653e\u5c04\u6362\u53d7\uff0c\u4f46\u4f1a\u5bfc\u81f4\u566a\u58f0\u589e\u52a0\u548c\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\u3002\u4f20\u7edf\u7684\u8fed\u4ee3\u4f18\u5316\u6216\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5f80\u5f80\u65e0\u6cd5\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc(PPORLD-EDNetLDCT)\uff0c\u5229\u7528\u8fdb\u9636\u7684\u540e\u7f6e\u7b56\u7565\u4f18\u5316(PPO)\u7b97\u6cd5\uff0c\u57fa\u4e8e\u56fe\u50cf\u8d28\u91cf\u53cd\u9988\u5b9e\u65f6\u4f18\u5316\u53bb\u566a\u7b56\u7565\uff0c\u901a\u8fc7\u81ea\u5b9a\u4e49gym\u73af\u5883\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u4f4e\u5f3a\u5ea6CT\u56fe\u50cf\u548c\u6295\u5f71\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u8fbe\u5230\u4e86PSNR 41.87\u3001SSIM 0.9814\u548cRMSE 0.00236\u3002\u5728NIH-AAPM-Mayo Clinic\u6570\u636e\u96c6\u4e0a\u83b7\u5f97PSNR 41.52\u3001SSIM 0.9723\u548cRMSE 0.0051\u3002\u5728\u65b0\u51a0\u75c5LDCT\u6570\u636e\u96c6\u7684\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u53bb\u566a\u540e\u56fe\u50cf\u7684\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u5347\u523094%\uff0c\u6bd4\u65e0RL\u53bb\u566a\u65b9\u6cd5\u63d0\u9ad84%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u66f4\u5b89\u5168\u548c\u51c6\u786e\u7684\u4f4e\u5f3a\u5ea6CT\u6210\u50cf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.03212", "pdf": "https://arxiv.org/pdf/2509.03212", "abs": "https://arxiv.org/abs/2509.03212", "authors": ["Chenxi Li"], "title": "AIVA: An AI-based Virtual Companion for Emotion-aware Interaction", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have significantly improved natural language understanding and generation, enhancing Human-Computer Interaction (HCI). However, LLMs are limited to unimodal text processing and lack the ability to interpret emotional cues from non-verbal signals, hindering more immersive and empathetic interactions. This work explores integrating multimodal sentiment perception into LLMs to create emotion-aware agents. We propose \\ours, an AI-based virtual companion that captures multimodal sentiment cues, enabling emotionally aligned and animated HCI. \\ours introduces a Multimodal Sentiment Perception Network (MSPN) using a cross-modal fusion transformer and supervised contrastive learning to provide emotional cues. Additionally, we develop an emotion-aware prompt engineering strategy for generating empathetic responses and integrate a Text-to-Speech (TTS) system and animated avatar module for expressive interactions. \\ours provides a framework for emotion-aware agents with applications in companion robotics, social care, mental health, and human-centered AI.", "AI": {"tldr": "\u901a\u8fc7\u591a\u6a21\u6001\u60c5\u611f\u611f\u77e5\u7f51\u7edc\u548c\u60c5\u611f\u654f\u611f\u63d0\u793a\u5de5\u7a0b\uff0c\u5c06LLM\u5347\u7ea7\u4e3a\u80fd\u591f\u7406\u89e3\u548c\u54cd\u5e94\u975e\u8bed\u8a00\u60c5\u7eea\u4fe1\u53f7\u7684\u60c5\u611f\u654f\u611f\u4ee3\u7406", "motivation": "\u89e3\u51b3LLM\u4ec5\u80fd\u5904\u7406\u5355\u4e00\u6587\u672c\u6a21\u6001\u3001\u7f3a\u4e4f\u60c5\u611f\u611f\u77e5\u80fd\u529b\u7684\u9650\u5236\uff0c\u63d0\u5347HCI\u7684\u6c89\u6d78\u5f0f\u548c\u5171\u60c5\u6027\u4ea4\u4e92", "method": "\u8bbe\u8ba1\u591a\u6a21\u6001\u60c5\u611f\u611f\u77e5\u7f51\u7edc(MSPN)\uff0c\u91c7\u7528\u8de8\u6a21\u6001\u878d\u5408transformer\u548c\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\uff0b\u60c5\u611f\u654f\u611f\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\uff0bTTS\u548c\u52a8\u6001\u5934\u50cf\u6a21\u5757", "result": "\u5f00\u53d1\u51fa\u80fd\u591f\u6355\u6349\u591a\u6a21\u6001\u60c5\u611f\u7d22\u5f15\u3001\u751f\u6210\u60c5\u611f\u5bf9\u9f50\u54cd\u5e94\u7684\u865a\u62df\u4f34\u4fa3\uff0c\u5b9e\u73b0\u4e86\u8868\u60c5\u4e30\u5bcc\u7684\u4ea4\u4e92", "conclusion": "\u63d0\u4f9b\u4e86\u60c5\u611f\u654f\u611f\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u5728\u4f34\u4fa3\u673a\u5668\u4eba\u3001\u793e\u4f1a\u517b\u8001\u3001\u5fc3\u7406\u5065\u5eb7\u7b49\u9886\u57df\u5177\u6709\u5e94\u7528\u6f5c\u529b"}}
{"id": "2509.03221", "pdf": "https://arxiv.org/pdf/2509.03221", "abs": "https://arxiv.org/abs/2509.03221", "authors": ["Jing Zhang", "Siying Tao", "Jiao Li", "Tianhe Wang", "Junchen Wu", "Ruqian Hao", "Xiaohui Du", "Ruirong Tan", "Rui Li"], "title": "LGBP-OrgaNet: Learnable Gaussian Band Pass Fusion of CNN and Transformer Features for Robust Organoid Segmentation and Tracking", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Organoids replicate organ structure and function, playing a crucial role in fields such as tumor treatment and drug screening. Their shape and size can indicate their developmental status, but traditional fluorescence labeling methods risk compromising their structure. Therefore, this paper proposes an automated, non-destructive approach to organoid segmentation and tracking. We introduced the LGBP-OrgaNet, a deep learning-based system proficient in accurately segmenting, tracking, and quantifying organoids. The model leverages complementary information extracted from CNN and Transformer modules and introduces the innovative feature fusion module, Learnable Gaussian Band Pass Fusion, to merge data from two branches. Additionally, in the decoder, the model proposes a Bidirectional Cross Fusion Block to fuse multi-scale features, and finally completes the decoding through progressive concatenation and upsampling. SROrga demonstrates satisfactory segmentation accuracy and robustness on organoids segmentation datasets, providing a potent tool for organoid research.", "AI": {"tldr": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684LGBP-OrgaNet\u7cfb\u7edf\uff0c\u901a\u8fc7CNN\u548cTransformer\u6a21\u5757\u63d0\u53d6\u8865\u5145\u4fe1\u606f\uff0c\u91c7\u7528\u521b\u65b0\u7684\u53ef\u5b66\u4e60\u9ad8\u65af\u5e26\u901a\u878d\u5408\u6a21\u5757\u548c\u53cc\u5411\u4ea4\u53c9\u878d\u5408\u5757\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u7ec4\u7ec7\u56e2\u5206\u5272\u3001\u8ddf\u8e2a\u548c\u5b9a\u91cf\u5206\u6790", "motivation": "\u7ec4\u7ec7\u56e2\u80fd\u591f\u590d\u5236\u5668\u5b98\u7ed3\u6784\u548c\u529f\u80fd\uff0c\u5176\u5f62\u72b6\u548c\u5927\u5c0f\u53ef\u4ee5\u53cd\u6620\u53d1\u80b2\u72b6\u6001\uff0c\u4f46\u4f20\u7edf\u7684\u836f\u7269\u6807\u8bb0\u65b9\u6cd5\u5bb9\u6613\u7834\u574f\u7ec4\u7ec7\u56e2\u7ed3\u6784\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u3001\u65e0\u635f\u574f\u7684\u5206\u5272\u548c\u8ddf\u8e2a\u65b9\u6cd5", "method": "\u63d0\u51faLGBP-OrgaNet\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff1a1\uff09\u5229\u7528CNN\u548cTransformer\u6a21\u5757\u63d0\u53d6\u8865\u5145\u4fe1\u606f 2\uff09\u521b\u65b0\u7684\u53ef\u5b66\u4e60\u9ad8\u65af\u5e26\u901a\u878d\u5408\u6a21\u5757\uff08Learnable Gaussian Band Pass Fusion\uff09\u878d\u5408\u4e24\u4e2a\u5206\u652f\u6570\u636e 3\uff09\u5728\u89e3\u7801\u5668\u4e2d\u4f7f\u7528\u53cc\u5411\u4ea4\u53c9\u878d\u5408\u5757\uff08Bidirectional Cross Fusion Block\uff09\u878d\u5408\u591a\u5c3a\u5ea6\u7279\u5f81 4\uff09\u901a\u8fc7\u6e10\u8fdb\u5f0f\u8fde\u63a5\u548c\u4e0a\u91c7\u6837\u5b8c\u6210\u89e3\u7801", "result": "SROrga\u5728\u7ec4\u7ec7\u56e2\u5206\u5272\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u4e86\u6ee1\u610f\u7684\u5206\u5272\u7cbe\u5ea6\u548c\u7a33\u5065\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7ec4\u7ec7\u56e2\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u80fd\u591f\u5728\u4e0d\u7834\u574f\u7ec4\u7ec7\u56e2\u7ed3\u6784\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u81ea\u52a8\u5316\u5206\u5272\u548c\u8ddf\u8e2a"}}
{"id": "2509.03267", "pdf": "https://arxiv.org/pdf/2509.03267", "abs": "https://arxiv.org/abs/2509.03267", "authors": ["Hongxu Yang", "Edina Timko", "Levente Lippenszky", "Vanda Czipczer", "Lehel Ferenczi"], "title": "SynBT: High-quality Tumor Synthesis for Breast Tumor Segmentation by 3D Diffusion Model", "categories": ["cs.CV"], "comment": "Accepted by MICCAI 2025 Deep-Breath Workshop. Supported by IHI   SYNTHIA project", "summary": "Synthetic tumors in medical images offer controllable characteristics that facilitate the training of machine learning models, leading to an improved segmentation performance. However, the existing methods of tumor synthesis yield suboptimal performances when tumor occupies a large spatial volume, such as breast tumor segmentation in MRI with a large field-of-view (FOV), while commonly used tumor generation methods are based on small patches. In this paper, we propose a 3D medical diffusion model, called SynBT, to generate high-quality breast tumor (BT) in contrast-enhanced MRI images. The proposed model consists of a patch-to-volume autoencoder, which is able to compress the high-resolution MRIs into compact latent space, while preserving the resolution of volumes with large FOV. Using the obtained latent space feature vector, a mask-conditioned diffusion model is used to synthesize breast tumors within selected regions of breast tissue, resulting in realistic tumor appearances. We evaluated the proposed method for a tumor segmentation task, which demonstrated the proposed high-quality tumor synthesis method can facilitate the common segmentation models with performance improvement of 2-3% Dice Score on a large public dataset, and therefore provides benefits for tumor segmentation in MRI images.", "AI": {"tldr": "\u63d0\u51faSynBT\u6a21\u578b\uff0c\u4f7f\u75283D\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u4e73\u817a\u80bf\u7624MRI\u56fe\u50cf\uff0c\u901a\u8fc7patch-to-volume\u81ea\u7f16\u7801\u5668\u548cmask\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u5728\u5927FOV\u56fe\u50cf\u4e2d\u5b9e\u73b0\u771f\u5b9e\u80bf\u7624\u5408\u6210\uff0c\u63d0\u5347\u5206\u5272\u6a21\u578b\u6027\u80fd2-3% Dice\u5206\u6570", "motivation": "\u73b0\u6709\u80bf\u7624\u5408\u6210\u65b9\u6cd5\u5728\u5927\u7a7a\u95f4\u4f53\u79ef\u80bf\u7624\uff08\u5982\u4e73\u817aMRI\u5927\u89c6\u573a\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5e38\u7528\u65b9\u6cd5\u57fa\u4e8e\u5c0fpatch\uff0c\u9700\u8981\u9ad8\u8d28\u91cf\u5927FOV\u80bf\u7624\u5408\u6210\u6280\u672f", "method": "\u63d0\u51faSynBT 3D\u533b\u5b66\u6269\u6563\u6a21\u578b\uff0c\u5305\u542bpatch-to-volume\u81ea\u7f16\u7801\u5668\u538b\u7f29\u9ad8\u5206\u8fa8\u7387MRI\u5230\u7d27\u51d1\u6f5c\u5728\u7a7a\u95f4\uff0c\u4fdd\u6301\u5927FOV\u4f53\u79ef\u5206\u8fa8\u7387\uff1b\u4f7f\u7528mask\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5728\u9009\u5b9a\u4e73\u817a\u7ec4\u7ec7\u533a\u57df\u5408\u6210\u4e73\u817a\u80bf\u7624", "result": "\u5728\u80bf\u7624\u5206\u5272\u4efb\u52a1\u4e2d\u8bc4\u4f30\uff0c\u9ad8\u8d28\u91cf\u80bf\u7624\u5408\u6210\u65b9\u6cd5\u4f7f\u5e38\u89c1\u5206\u5272\u6a21\u578b\u6027\u80fd\u63d0\u53472-3% Dice\u5206\u6570\uff08\u5927\u578b\u516c\u5171\u6570\u636e\u96c6\uff09", "conclusion": "SynBT\u65b9\u6cd5\u80fd\u4e3aMRI\u56fe\u50cf\u4e2d\u7684\u80bf\u7624\u5206\u5272\u63d0\u4f9b\u76ca\u5904\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u80bf\u7624\u6539\u5584\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u548c\u5206\u5272\u6027\u80fd"}}
{"id": "2509.03324", "pdf": "https://arxiv.org/pdf/2509.03324", "abs": "https://arxiv.org/abs/2509.03324", "authors": ["Yixiong Jing", "Cheng Zhang", "Haibing Wu", "Guangming Wang", "Olaf Wysocki", "Brian Sheil"], "title": "InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds", "categories": ["cs.CV"], "comment": null, "summary": "Point clouds are widely used for infrastructure monitoring by providing geometric information, where segmentation is required for downstream tasks such as defect detection. Existing research has automated semantic segmentation of structural components, while brick-level segmentation (identifying defects such as spalling and mortar loss) has been primarily conducted from RGB images. However, acquiring high-resolution images is impractical in low-light environments like masonry tunnels. Point clouds, though robust to dim lighting, are typically unstructured, sparse, and noisy, limiting fine-grained segmentation. We present InfraDiffusion, a zero-shot framework that projects masonry point clouds into depth maps using virtual cameras and restores them by adapting the Denoising Diffusion Null-space Model (DDNM). Without task-specific training, InfraDiffusion enhances visual clarity and geometric consistency of depth maps. Experiments on masonry bridge and tunnel point cloud datasets show significant improvements in brick-level segmentation using the Segment Anything Model (SAM), underscoring its potential for automated inspection of masonry assets. Our code and data is available at https://github.com/Jingyixiong/InfraDiffusion-official-implement.", "AI": {"tldr": "InfraDiffusion\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u6846\u67b6\uff0c\u901a\u8fc7\u865a\u62df\u76f8\u673a\u5c06\u7816\u77f3\u70b9\u4e91\u8f6c\u6362\u4e3a\u6df1\u5ea6\u56fe\uff0c\u5e76\u4f7f\u7528DDNM\u8fdb\u884c\u6062\u590d\uff0c\u65e0\u9700\u7279\u5b9a\u4efb\u52a1\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347\u7816\u5757\u7ea7\u5206\u5272\u6548\u679c\u3002", "motivation": "\u5728\u4f4e\u5149\u7167\u73af\u5883\u4e0b\uff08\u5982\u7816\u77f3\u96a7\u9053\uff09\uff0c\u83b7\u53d6\u9ad8\u5206\u8fa8\u7387RGB\u56fe\u50cf\u4e0d\u5207\u5b9e\u9645\uff0c\u800c\u70b9\u4e91\u867d\u7136\u5bf9\u6697\u5149\u73af\u5883\u9c81\u68d2\uff0c\u4f46\u901a\u5e38\u662f\u975e\u7ed3\u6784\u5316\u3001\u7a00\u758f\u4e14\u5608\u6742\u7684\uff0c\u9650\u5236\u4e86\u7ec6\u7c92\u5ea6\u5206\u5272\u3002", "method": "\u5c06\u7816\u77f3\u70b9\u4e91\u901a\u8fc7\u865a\u62df\u76f8\u673a\u6295\u5f71\u4e3a\u6df1\u5ea6\u56fe\uff0c\u91c7\u7528\u53bb\u566a\u6269\u6563\u96f6\u7a7a\u95f4\u6a21\u578b\uff08DDNM\uff09\u8fdb\u884c\u6062\u590d\uff0c\u589e\u5f3a\u6df1\u5ea6\u56fe\u7684\u89c6\u89c9\u6e05\u6670\u5ea6\u548c\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u7136\u540e\u4f7f\u7528Segment Anything Model\uff08SAM\uff09\u8fdb\u884c\u7816\u5757\u7ea7\u5206\u5272\u3002", "result": "\u5728\u7816\u77f3\u6865\u6881\u548c\u96a7\u9053\u70b9\u4e91\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u7816\u5757\u7ea7\u5206\u5272\u65b9\u9762\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "InfraDiffusion\u5c55\u793a\u4e86\u5728\u7816\u77f3\u8d44\u4ea7\u81ea\u52a8\u5316\u68c0\u6d4b\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u4f4e\u5149\u7167\u73af\u5883\u4e0b\u70b9\u4e91\u6570\u636e\u7684\u7ec6\u7c92\u5ea6\u5206\u5272\u5e94\u7528\u3002"}}
{"id": "2509.02586", "pdf": "https://arxiv.org/pdf/2509.02586", "abs": "https://arxiv.org/abs/2509.02586", "authors": ["Esha Sadia Nasir", "Jiaqi Lv", "Mostafa Jahanifer", "Shan E Ahmed Raza"], "title": "MitoDetect++: A Domain-Robust Pipeline for Mitosis Detection and Atypical Subtyping", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Automated detection and classification of mitotic figures especially distinguishing atypical from normal remain critical challenges in computational pathology. We present MitoDetect++, a unified deep learning pipeline designed for the MIDOG 2025 challenge, addressing both mitosis detection and atypical mitosis classification. For detection (Track 1), we employ a U-Net-based encoder-decoder architecture with EfficientNetV2-L as the backbone, enhanced with attention modules, and trained via combined segmentation losses. For classification (Track 2), we leverage the Virchow2 vision transformer, fine-tuned efficiently using Low-Rank Adaptation (LoRA) to minimize resource consumption. To improve generalization and mitigate domain shifts, we integrate strong augmentations, focal loss, and group-aware stratified 5-fold cross-validation. At inference, we deploy test-time augmentation (TTA) to boost robustness. Our method achieves a balanced accuracy of 0.892 across validation domains, highlighting its clinical applicability and scalability across tasks.", "AI": {"tldr": "MitoDetect++\u662f\u4e00\u4e2a\u7528\u4e8eMIDOG 2025\u6311\u6218\u8d5b\u7684\u6df1\u5ea6\u5b66\u4e60\u7ba1\u9053\uff0c\u7edf\u4e00\u5904\u7406\u6709\u4e1d\u5206\u88c2\u68c0\u6d4b\u548c\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\u5206\u7c7b\uff0c\u5728\u9a8c\u8bc1\u57df\u4e0a\u8fbe\u52300.892\u7684\u5e73\u8861\u51c6\u786e\u7387", "motivation": "\u81ea\u52a8\u68c0\u6d4b\u548c\u5206\u7c7b\u6709\u4e1d\u5206\u88c2\u56fe\u50cf\uff0c\u7279\u522b\u662f\u533a\u5206\u975e\u5178\u578b\u4e0e\u6b63\u5e38\u6709\u4e1d\u5206\u88c2\uff0c\u662f\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u7684\u5173\u952e\u6311\u6218", "method": "\u4f7f\u7528U-Net\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff08EfficientNetV2-L\u4e3b\u5e72\uff09\u8fdb\u884c\u68c0\u6d4b\uff0cVirchow2\u89c6\u89c9\u53d8\u6362\u5668\uff08LoRA\u5fae\u8c03\uff09\u8fdb\u884c\u5206\u7c7b\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u6a21\u5757\u3001\u5f3a\u6570\u636e\u589e\u5f3a\u3001\u7126\u70b9\u635f\u5931\u548c\u5206\u5c42\u4ea4\u53c9\u9a8c\u8bc1", "result": "\u5728\u9a8c\u8bc1\u57df\u4e0a\u83b7\u5f970.892\u7684\u5e73\u8861\u51c6\u786e\u7387\uff0c\u663e\u793a\u51fa\u826f\u597d\u7684\u4e34\u5e8a\u9002\u7528\u6027\u548c\u8de8\u4efb\u52a1\u53ef\u6269\u5c55\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7edf\u4e00\u7684\u6df1\u5ea6\u5b66\u4e60\u7ba1\u9053\u6709\u6548\u89e3\u51b3\u4e86\u6709\u4e1d\u5206\u88c2\u68c0\u6d4b\u548c\u5206\u7c7b\u95ee\u9898\uff0c\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u4e34\u5e8a\u5b9e\u7528\u6027"}}
{"id": "2509.02601", "pdf": "https://arxiv.org/pdf/2509.02601", "abs": "https://arxiv.org/abs/2509.02601", "authors": ["Piotr Giedziun", "Jan So\u0142tysik", "Mateusz G\u00f3rczany", "Norbert Ropiak", "Marcin Przymus", "Piotr Krajewski", "Jaros\u0142aw Kwiecie\u0144", "Artur Bartczak", "Izabela Wasiak", "Mateusz Maniewski"], "title": "Foundation Model-Driven Classification of Atypical Mitotic Figures with Domain-Aware Training Strategies", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "We present a solution for the MIDOG 2025 Challenge Track~2, addressing binary classification of normal mitotic figures (NMFs) versus atypical mitotic figures (AMFs). The approach leverages pathology-specific foundation model H-optimus-0, selected based on recent cross-domain generalization benchmarks and our empirical testing, with Low-Rank Adaptation (LoRA) fine-tuning and MixUp augmentation. Implementation includes soft labels based on multi-expert consensus, hard negative mining, and adaptive focal loss, metric learning and domain adaptation. The method demonstrates both the promise and challenges of applying foundation models to this complex classification task, achieving reasonable performance in the preliminary evaluation phase.", "AI": {"tldr": "\u57fa\u4e8eH-optimus-0\u75c5\u7406\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7LoRA\u5fae\u8c03\u3001MixUp\u589e\u5e3d\u7b49\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u6b63\u5e38\u548c\u975e\u5f0f\u6709\u56f0\u6709\u56f0\u5206\u88c2\u76f8\u7684\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1", "motivation": "\u89e3\u51b3MIDOG 2025\u6311\u6218\u8d5b\u8eab\u4efd\u8bc6\u522b\u6b63\u5e38\u6709\u56f0\u5206\u88c2\u76f8(NMFs)\u4e0e\u975e\u5f0f\u6709\u56f0\u5206\u88c2\u76f8(AMFs)\u7684\u590d\u6742\u5206\u7c7b\u95ee\u9898\uff0c\u63a2\u7d22\u57fa\u7840\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u4e0e\u6311\u6218", "method": "\u9009\u62e9H-optimus-0\u75c5\u7406\u57fa\u7840\u6a21\u578b\uff0c\u91c7\u7528LoRA\u4f4e\u8d28\u91cf\u8c03\u6574\u3001MixUp\u6570\u636e\u589e\u5e3d\uff0c\u7ed3\u5408\u8f6f\u6807\u7b7e\u3001\u96be\u4f8b\u6316\u6398\u3001\u9002\u5e94\u6027\u7126\u635f\u5931\u3001\u8ddd\u79bb\u5b66\u4e60\u548c\u57df\u9002\u5e94\u6280\u672f", "result": "\u5728\u9884\u6d4b\u8bc4\u4f30\u9636\u6bb5\u53d6\u5f97\u4e86\u5408\u7406\u7684\u6027\u80fd\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u57fa\u7840\u6a21\u578b\u5728\u8be5\u590d\u6742\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c55\u793a\u4e86\u57fa\u7840\u6a21\u578b\u5728\u75c5\u7406\u5f62\u6001\u5b66\u5206\u6790\u4e2d\u7684\u4ef7\u503c\uff0c\u540c\u65f6\u4e5f\u63ed\u793a\u4e86\u5728\u590d\u6742\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6311\u6218\u4e0e\u9650\u5236"}}
{"id": "2509.02640", "pdf": "https://arxiv.org/pdf/2509.02640", "abs": "https://arxiv.org/abs/2509.02640", "authors": ["Biwen Meng", "Xi Long", "Jingxin Liu"], "title": "Adaptive Learning Strategies for Mitotic Figure Classification in MIDOG2025 Challenge", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Atypical mitotic figures (AMFs) are clinically relevant indicators of abnormal cell division, yet their reliable detection remains challenging due to morphological ambiguity and scanner variability. In this work, we investigated three variants of adapting the pathology foundation model UNI2-h for the MIDOG2025 Track 2 challenge. Starting from a LoRA-based baseline, we found that visual prompt tuning (VPT) substantially improved generalization, and that further integrating test-time augmentation (TTA) with Vahadane and Macenko stain normalization provided the best robustness. Our final submission achieved a balanced accuracy of 0.8837 and an ROC-AUC of 0.9513 on the preliminary leaderboard, ranking within the top 10 teams. These results demonstrate that prompt-based adaptation combined with stain-normalization TTA offers an effective strategy for atypical mitosis classification under diverse imaging conditions.", "AI": {"tldr": "\u4f7f\u7528UNI2-h\u75c5\u7406\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u4e09\u79cd\u53d8\u4f53\u9002\u914d\uff0c\u901a\u8fc7\u89c6\u89c9\u63d0\u793a\u8c03\u4f18(VPT)\u548c\u6d4b\u8bd5\u65f6\u589e\u5f3a(TTA)\u7ed3\u5408\u67d3\u8272\u6807\u51c6\u5316\uff0c\u5728MIDOG2025\u6311\u6218\u8d5b\u4e2d\u5b9e\u73b0\u4e8688.37%\u7684\u5e73\u8861\u51c6\u786e\u7387\u548c95.13%\u7684ROC-AUC", "motivation": "\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\u56fe\u5f62(AMFs)\u662f\u5f02\u5e38\u7ec6\u80de\u5206\u88c2\u7684\u91cd\u8981\u4e34\u5e8a\u6307\u6807\uff0c\u4f46\u7531\u4e8e\u5f62\u6001\u5b66\u6a21\u7cca\u6027\u548c\u626b\u63cf\u4eea\u53d8\u5f02\u6027\uff0c\u5176\u53ef\u9760\u68c0\u6d4b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027", "method": "\u7814\u7a76\u4e86UNI2-h\u75c5\u7406\u57fa\u7840\u6a21\u578b\u7684\u4e09\u79cd\u9002\u914d\u53d8\u4f53\uff1a\u4eceLoRA\u57fa\u7ebf\u5f00\u59cb\uff0c\u53d1\u73b0\u89c6\u89c9\u63d0\u793a\u8c03\u4f18(VPT)\u663e\u8457\u6539\u5584\u6cdb\u5316\u80fd\u529b\uff0c\u8fdb\u4e00\u6b65\u6574\u5408\u6d4b\u8bd5\u65f6\u589e\u5f3a(TTA)\u4e0eVahadane\u548cMacenko\u67d3\u8272\u6807\u51c6\u5316", "result": "\u6700\u7ec8\u63d0\u4ea4\u5728\u521d\u6b65\u6392\u884c\u699c\u4e0a\u83b7\u5f970.8837\u7684\u5e73\u8861\u51c6\u786e\u7387\u548c0.9513\u7684ROC-AUC\uff0c\u6392\u540d\u524d\u5341", "conclusion": "\u57fa\u4e8e\u63d0\u793a\u7684\u9002\u914d\u7ed3\u5408\u67d3\u8272\u6807\u51c6\u5316TTA\u4e3a\u4e0d\u540c\u6210\u50cf\u6761\u4ef6\u4e0b\u7684\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\u5206\u7c7b\u63d0\u4f9b\u4e86\u6709\u6548\u7b56\u7565"}}
{"id": "2509.03188", "pdf": "https://arxiv.org/pdf/2509.03188", "abs": "https://arxiv.org/abs/2509.03188", "authors": ["Hania Ghouse", "Muzammil Behzad"], "title": "Prompt-Guided Patch UNet-VAE with Adversarial Supervision for Adrenal Gland Segmentation in Computed Tomography Medical Images", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Segmentation of small and irregularly shaped abdominal organs, such as the adrenal glands in CT imaging, remains a persistent challenge due to severe class imbalance, poor spatial context, and limited annotated data. In this work, we propose a unified framework that combines variational reconstruction, supervised segmentation, and adversarial patch-based feedback to address these limitations in a principled and scalable manner. Our architecture is built upon a VAE-UNet backbone that jointly reconstructs input patches and generates voxel-level segmentation masks, allowing the model to learn disentangled representations of anatomical structure and appearance. We introduce a patch-based training pipeline that selectively injects synthetic patches generated from the learned latent space, and systematically study the effects of varying synthetic-to-real patch ratios during training. To further enhance output fidelity, the framework incorporates perceptual reconstruction loss using VGG features, as well as a PatchGAN-style discriminator for adversarial supervision over spatial realism. Comprehensive experiments on the BTCV dataset demonstrate that our approach improves segmentation accuracy, particularly in boundary-sensitive regions, while maintaining strong reconstruction quality. Our findings highlight the effectiveness of hybrid generative-discriminative training regimes for small-organ segmentation and provide new insights into balancing realism, diversity, and anatomical consistency in data-scarce scenarios.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u53d8\u5206\u91cd\u5efa\u3001\u76d1\u7763\u5206\u5272\u548c\u5bf9\u6297\u6027\u8865\u4e01\u53cd\u9988\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u89e3\u51b3CT\u56fe\u50cf\u4e2d\u5c0f\u5668\u5b98\u5206\u5272\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u8fb9\u754c\u654f\u611f\u533a\u57df\u7684\u5206\u5272\u51c6\u786e\u6027", "motivation": "\u89e3\u51b3\u8179\u90e8\u5c0f\u5668\u5b98\uff08\u5982\u526f\u8170\u817a\uff09\u5728CT\u6210\u50cf\u4e2d\u7684\u5206\u5272\u6311\u6218\uff0c\u5305\u62ec\u4e25\u91cd\u7c7b\u4e0d\u5e73\u8861\u3001\u7a7a\u95f4\u4e0a\u4e0b\u6587\u4fe1\u606f\u8f83\u5c11\u548c\u6807\u6ce8\u6570\u636e\u6709\u9650\u7b49\u95ee\u9898", "method": "\u57fa\u4e8eVAE-UNet\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7ed3\u5408\u53d8\u5206\u91cd\u5efa\u3001\u76d1\u7763\u5206\u5272\u548c\u5bf9\u6297\u6027\u8865\u4e01\u53cd\u9988\uff1b\u4f7f\u7528VGG\u7279\u5f81\u7684\u611f\u77e5\u91cd\u5efa\u635f\u5931\u548cPatchGAN\u5224\u522b\u5668\uff1b\u901a\u8fc7\u5b66\u4e60\u7684\u6f5c\u7a7a\u95f4\u751f\u6210\u5408\u6210\u8865\u4e01\u6765\u6269\u5145\u8bad\u7ec3\u6570\u636e", "result": "\u5728BTCV\u6570\u636e\u96c6\u4e0a\u7684\u5b8c\u6574\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5206\u5272\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u8fb9\u754c\u654f\u611f\u533a\u57df\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u91cd\u5efa\u8d28\u91cf", "conclusion": "\u6df7\u5408\u751f\u6210\u5f0f-\u5224\u522b\u5f0f\u8bad\u7ec3\u65b9\u6848\u5bf9\u5c0f\u5668\u5b98\u5206\u5272\u6709\u6548\uff0c\u4e3a\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e2d\u5e73\u8861\u771f\u5b9e\u6027\u3001\u591a\u6837\u6027\u548c\u89e3\u5256\u4e00\u81f4\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3"}}
