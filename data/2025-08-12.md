<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 61]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [eess.IV](#eess.IV) [Total: 9]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [PureSample: Neural Materials Learned by Sampling Microgeometry](https://arxiv.org/abs/2508.07240)
*Zixuan Li,Zixiong Wang,Jian Yang,Milos Hasan,Beibei Wang*

Main category: cs.GR

TL;DR: PureSample是一种基于神经网络的BRDF表示方法，通过随机行走采样学习材料行为，简化了传统BRDF模型的复杂性。


<details>
  <summary>Details</summary>
Motivation: 传统BRDF模型依赖复杂的解析推导，且难以处理空间变化材料。PureSample旨在通过神经网络简化这一过程。

Method: PureSample包含两个可学习组件：流匹配神经网络建模采样分布，以及轻量级神经网络捕捉视角相关反照率。

Result: PureSample在多层材料、多次散射微表面材料等多种复杂材料上表现优异。

Conclusion: PureSample提供了一种高效、通用的BRDF表示方法，适用于均质和非均质材料。

Abstract: Traditional physically-based material models rely on analytically derived bidirectional reflectance distribution functions (BRDFs), typically by considering statistics of micro-primitives such as facets, flakes, or spheres, sometimes combined with multi-bounce interactions such as layering and multiple scattering. These derivations are often complex and model-specific, and typically consider a statistical aggregate of a large surface area, ignoring spatial variation. Once an analytic BRDF's evaluation is defined, one still needs to design an importance sampling method for it, and a way to evaluate the pdf of that sampling distribution, requiring further model-specific derivations.   We present PureSample: a novel neural BRDF representation that allows learning a material's behavior purely by sampling forward random walks on the microgeometry, which is usually straightforward to implement. Our representation allows for efficient importance sampling, pdf evaluation, and BRDF evaluation, for homogeneous as well as spatially varying materials.   We achieve this by two learnable components: first, the sampling distribution is modeled using a flow matching neural network, which allows both importance sampling and pdf evaluation; second, we introduce a view-dependent albedo term, captured by a lightweight neural network, which allows for converting a scalar pdf value to a colored BRDF value for any pair of view and light directions.   We demonstrate PureSample on challenging materials, including multi-layered materials, multiple-scattering microfacet materials, and various other microstructures.

</details>


### [2] [Vertex Features for Neural Global Illumination](https://arxiv.org/abs/2508.07852)
*Rui Su,Honghao Dong,Haojie Jin,Yisong Chen,Guoping Wang,Sheng Li*

Main category: cs.GR

TL;DR: 提出了一种基于网格顶点的可学习神经表示方法，显著降低了内存占用并保持了渲染质量。


<details>
  <summary>Details</summary>
Motivation: 传统特征网格表示在3D场景重建和神经渲染中内存占用高，限制了并行计算硬件的效率。

Method: 将可学习特征直接存储在网格顶点上，利用几何结构作为紧凑表示，优化内存效率并提升特征表示。

Result: 实验表明，该方法内存消耗仅为基于网格表示的五分之一或更低，同时保持渲染质量并降低推理开销。

Conclusion: 神经顶点特征是一种高效且紧凑的表示方法，适用于神经渲染任务。

Abstract: Recent research on learnable neural representations has been widely adopted in the field of 3D scene reconstruction and neural rendering applications. However, traditional feature grid representations often suffer from substantial memory footprint, posing a significant bottleneck for modern parallel computing hardware. In this paper, we present neural vertex features, a generalized formulation of learnable representation for neural rendering tasks involving explicit mesh surfaces. Instead of uniformly distributing neural features throughout 3D space, our method stores learnable features directly at mesh vertices, leveraging the underlying geometry as a compact and structured representation for neural processing. This not only optimizes memory efficiency, but also improves feature representation by aligning compactly with the surface using task-specific geometric priors. We validate our neural representation across diverse neural rendering tasks, with a specific emphasis on neural radiosity. Experimental results demonstrate that our method reduces memory consumption to only one-fifth (or even less) of grid-based representations, while maintaining comparable rendering quality and lowering inference overhead.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [3] [Med-GRIM: Enhanced Zero-Shot Medical VQA using prompt-embedded Multimodal Graph RAG](https://arxiv.org/abs/2508.06496)
*Rakesh Raj Madavan,Akshat Kaimal,Hashim Faisal,Chandrakala S*

Main category: cs.CV

TL;DR: BIND和Med-GRIM通过密集编码和图检索提升医疗VQA任务性能，减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有VQA模型在复杂领域（如医疗）中缺乏精确性，需要高效解决方案。

Method: BIND改进联合嵌入空间，Med-GRIM结合图检索和提示工程，使用小语言模型降低计算开销。

Result: Med-GRIM以低成本实现高性能，并发布DermaGraph数据集支持研究。

Conclusion: BIND和Med-GRIM为医疗VQA提供了高效、精确的解决方案。

Abstract: An ensemble of trained multimodal encoders and vision-language models (VLMs) has become a standard approach for visual question answering (VQA) tasks. However, such models often fail to produce responses with the detailed precision necessary for complex, domain-specific applications such as medical VQA. Our representation model, BIND: BLIVA Integrated with Dense Encoding, extends prior multimodal work by refining the joint embedding space through dense, query-token-based encodings inspired by contrastive pretraining techniques. This refined encoder powers Med-GRIM, a model designed for medical VQA tasks that leverages graph-based retrieval and prompt engineering to integrate domain-specific knowledge. Rather than relying on compute-heavy fine-tuning of vision and language models on specific datasets, Med-GRIM applies a low-compute, modular workflow with small language models (SLMs) for efficiency. Med-GRIM employs prompt-based retrieval to dynamically inject relevant knowledge, ensuring both accuracy and robustness in its responses. By assigning distinct roles to each agent within the VQA system, Med-GRIM achieves large language model performance at a fraction of the computational cost. Additionally, to support scalable research in zero-shot multimodal medical applications, we introduce DermaGraph, a novel Graph-RAG dataset comprising diverse dermatological conditions. This dataset facilitates both multimodal and unimodal querying. The code and dataset are available at: https://github.com/Rakesh-123-cryp/Med-GRIM.git

</details>


### [4] [DiffUS: Differentiable Ultrasound Rendering from Volumetric Imaging](https://arxiv.org/abs/2508.06768)
*Noe Bertramo,Gabriel Duguey,Vivek Gopalakrishnan*

Main category: cs.CV

TL;DR: DiffUS是一个基于物理的可微分超声渲染器，能够从MRI/CT数据生成逼真的B模式超声图像，用于术中引导。


<details>
  <summary>Details</summary>
Motivation: 解决术中超声图像因噪声、伪影和与术前MRI/CT对齐困难导致的解读问题。

Method: 通过机器学习将MRI 3D扫描转换为声阻抗体积，利用射线追踪和反射-透射方程模拟超声束传播，并通过稀疏线性系统捕捉多次内部反射。

Result: 在ReMIND数据集上验证了DiffUS能够从脑MRI数据生成解剖学准确的超声图像。

Conclusion: DiffUS为术中引导提供了高质量的超声图像合成工具，支持基于梯度的优化应用。

Abstract: Intraoperative ultrasound imaging provides real-time guidance during numerous surgical procedures, but its interpretation is complicated by noise, artifacts, and poor alignment with high-resolution preoperative MRI/CT scans. To bridge the gap between reoperative planning and intraoperative guidance, we present DiffUS, a physics-based, differentiable ultrasound renderer that synthesizes realistic B-mode images from volumetric imaging. DiffUS first converts MRI 3D scans into acoustic impedance volumes using a machine learning approach. Next, we simulate ultrasound beam propagation using ray tracing with coupled reflection-transmission equations. DiffUS formulates wave propagation as a sparse linear system that captures multiple internal reflections. Finally, we reconstruct B-mode images via depth-resolved echo extraction across fan-shaped acquisition geometry, incorporating realistic artifacts including speckle noise and depth-dependent degradation. DiffUS is entirely implemented as differentiable tensor operations in PyTorch, enabling gradient-based optimization for downstream applications such as slice-to-volume registration and volumetric reconstruction. Evaluation on the ReMIND dataset demonstrates DiffUS's ability to generate anatomically accurate ultrasound images from brain MRI data.

</details>


### [5] [Evaluating Fisheye-Compatible 3D Gaussian Splatting Methods on Real Images Beyond 180 Degree Field of View](https://arxiv.org/abs/2508.06968)
*Ulas Gunes,Matias Turkulainen,Juho Kannala,Esa Rahtu*

Main category: cs.CV

TL;DR: 本文评估了Fisheye-GS和3DGUT两种基于鱼眼镜头的3D高斯泼溅方法在超过180度视场的真实图像上的表现，并提出了基于深度的初始化策略以解决强失真下SfM初始化失败的问题。


<details>
  <summary>Details</summary>
Motivation: 研究鱼眼镜头在极端失真条件下的3D重建性能，探索不同视场下的表现差异，并提出改进初始化方法。

Method: 通过室内外场景的200度鱼眼图像，对比Fisheye-GS和3DGUT在不同视场（200度、160度、120度）下的表现；提出基于UniK3D预测的深度初始化策略。

Result: Fisheye-GS在160度视场表现最佳，而3DGUT在200度视场下保持稳定；UniK3D生成的密集点云在复杂场景中表现优异。

Conclusion: 鱼眼3DGS方法在稀疏且高失真图像输入下具有实际可行性，UniK3D策略显著提升了初始化效果。

Abstract: We present the first evaluation of fisheye-based 3D Gaussian Splatting methods, Fisheye-GS and 3DGUT, on real images with fields of view exceeding 180 degree. Our study covers both indoor and outdoor scenes captured with 200 degree fisheye cameras and analyzes how each method handles extreme distortion in real world settings. We evaluate performance under varying fields of view (200 degree, 160 degree, and 120 degree) to study the tradeoff between peripheral distortion and spatial coverage. Fisheye-GS benefits from field of view (FoV) reduction, particularly at 160 degree, while 3DGUT remains stable across all settings and maintains high perceptual quality at the full 200 degree view. To address the limitations of SfM-based initialization, which often fails under strong distortion, we also propose a depth-based strategy using UniK3D predictions from only 2-3 fisheye images per scene. Although UniK3D is not trained on real fisheye data, it produces dense point clouds that enable reconstruction quality on par with SfM, even in difficult scenes with fog, glare, or sky. Our results highlight the practical viability of fisheye-based 3DGS methods for wide-angle 3D reconstruction from sparse and distortion-heavy image inputs.

</details>


### [6] [HiMat: DiT-based Ultra-High Resolution SVBRDF Generation](https://arxiv.org/abs/2508.07011)
*Zixiong Wang,Jian Yang,Yiwei Hu,Milos Hasan,Beibei Wang*

Main category: cs.CV

TL;DR: HiMat是一个高效生成4K分辨率SVBRDF的扩散框架，通过轻量级CrossStitch模块解决多图一致性问题。


<details>
  <summary>Details</summary>
Motivation: 高分辨率文本到图像生成模型的兴起为SVBRDF生成提供了机会，但如何高效生成多图对齐的SVBRDF仍具挑战性。

Method: 提出HiMat框架，引入轻量级CrossStitch模块捕捉图间依赖，保持DiT主干不变。

Result: 实验证明HiMat能生成结构一致且高频细节丰富的4K SVBRDF，并适用于其他任务如本征分解。

Conclusion: HiMat通过轻量级设计高效生成高质量SVBRDF，展现了扩散模型在复杂任务中的潜力。

Abstract: Creating highly detailed SVBRDFs is essential for 3D content creation. The rise of high-resolution text-to-image generative models, based on diffusion transformers (DiT), suggests an opportunity to finetune them for this task. However, retargeting the models to produce multiple aligned SVBRDF maps instead of just RGB images, while achieving high efficiency and ensuring consistency across different maps, remains a challenge. In this paper, we introduce HiMat: a memory- and computation-efficient diffusion-based framework capable of generating native 4K-resolution SVBRDFs. A key challenge we address is maintaining consistency across different maps in a lightweight manner, without relying on training new VAEs or significantly altering the DiT backbone (which would damage its prior capabilities). To tackle this, we introduce the CrossStitch module, a lightweight convolutional module that captures inter-map dependencies through localized operations. Its weights are initialized such that the DiT backbone operation is unchanged before finetuning starts. HiMat enables generation with strong structural coherence and high-frequency details. Results with a large set of text prompts demonstrate the effectiveness of our approach for 4K SVBRDF generation. Further experiments suggest generalization to tasks such as intrinsic decomposition.

</details>


### [7] [Matrix-3D: Omnidirectional Explorable 3D World Generation](https://arxiv.org/abs/2508.08086)
*Zhongqi Yang,Wenhang Ge,Yuqi Li,Jiaqi Chen,Haoyuan Li,Mengyin An,Fei Kang,Hua Xue,Baixin Xu,Yuyang Yin,Eric Li,Yang Liu,Yikai Wang,Hao-Xiang Guo,Yahui Zhou*

Main category: cs.CV

TL;DR: Matrix-3D提出了一种基于全景表示和条件视频生成的框架，用于生成广泛覆盖的可探索3D世界。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成场景时范围有限，Matrix-3D旨在通过全景表示和视频模型结合解决这一问题。

Method: 训练轨迹引导的全景视频扩散模型，并提出两种3D重建方法：前馈式大型全景重建模型和基于优化的管道。

Result: 实验表明，Matrix-3D在全景视频生成和3D世界生成中达到最先进性能。

Conclusion: Matrix-3D框架通过全景表示和高效重建方法，显著提升了3D世界生成的覆盖范围和质量。

Abstract: Explorable 3D world generation from a single image or text prompt forms a cornerstone of spatial intelligence. Recent works utilize video model to achieve wide-scope and generalizable 3D world generation. However, existing approaches often suffer from a limited scope in the generated scenes. In this work, we propose Matrix-3D, a framework that utilize panoramic representation for wide-coverage omnidirectional explorable 3D world generation that combines conditional video generation and panoramic 3D reconstruction. We first train a trajectory-guided panoramic video diffusion model that employs scene mesh renders as condition, to enable high-quality and geometrically consistent scene video generation. To lift the panorama scene video to 3D world, we propose two separate methods: (1) a feed-forward large panorama reconstruction model for rapid 3D scene reconstruction and (2) an optimization-based pipeline for accurate and detailed 3D scene reconstruction. To facilitate effective training, we also introduce the Matrix-Pano dataset, the first large-scale synthetic collection comprising 116K high-quality static panoramic video sequences with depth and trajectory annotations. Extensive experiments demonstrate that our proposed framework achieves state-of-the-art performance in panoramic video generation and 3D world generation. See more in https://matrix-3d.github.io.

</details>


### [8] [MILD: Multi-Layer Diffusion Strategy for Complex and Precise Multi-IP Aware Human Erasing](https://arxiv.org/abs/2508.06543)
*Jinghan Yu,Zhiyuan Ma,Yue Ma,Kaiqi Liu,Yuhan Wang,Jianjun Li*

Main category: cs.CV

TL;DR: 论文提出了一种名为MILD的新方法，用于解决复杂多实例场景下的人像擦除问题，通过多层扩散和空间解耦技术显著提升了效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂多实例场景（如人物遮挡、背景干扰）中表现不佳，主要由于数据集限制和缺乏空间解耦能力。

Method: 提出Multi-Layer Diffusion（MILD）策略，将生成过程分解为语义分离的路径，并引入Human Morphology Guidance和Spatially-Modulated Attention。

Result: MILD在挑战性的人像擦除基准测试中优于现有方法。

Conclusion: MILD通过多层扩散和空间解耦技术有效解决了复杂场景下的人像擦除问题，并展示了显著性能提升。

Abstract: Recent years have witnessed the success of diffusion models in image-customized tasks. Prior works have achieved notable progress on human-oriented erasing using explicit mask guidance and semantic-aware inpainting. However, they struggle under complex multi-IP scenarios involving human-human occlusions, human-object entanglements, and background interferences. These challenges are mainly due to: 1) Dataset limitations, as existing datasets rarely cover dense occlusions, camouflaged backgrounds, and diverse interactions; 2) Lack of spatial decoupling, where foreground instances cannot be effectively disentangled, limiting clean background restoration. In this work, we introduce a high-quality multi-IP human erasing dataset with diverse pose variations and complex backgrounds. We then propose Multi-Layer Diffusion (MILD), a novel strategy that decomposes generation into semantically separated pathways for each instance and the background. To enhance human-centric understanding, we introduce Human Morphology Guidance, integrating pose, parsing, and spatial relations. We further present Spatially-Modulated Attention to better guide attention flow. Extensive experiments show that MILD outperforms state-of-the-art methods on challenging human erasing benchmarks.

</details>


### [9] [Slice or the Whole Pie? Utility Control for AI Models](https://arxiv.org/abs/2508.06551)
*Ye Tao*

Main category: cs.CV

TL;DR: NNObfuscator是一种新型工具，允许单个AI模型根据预设条件动态调整性能，无需为不同需求训练多个模型。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法中为不同性能需求训练和维护多个模型的低效问题。

Method: 提出NNObfuscator机制，使模型能实时调整性能，支持分层访问控制。

Result: 实验验证了NNObfuscator在多种任务中的有效性，单个模型可适应广泛需求。

Conclusion: NNObfuscator提高了资源利用率，支持可持续的AI部署商业模式。

Abstract: Training deep neural networks (DNNs) has become an increasingly resource-intensive task, requiring large volumes of labeled data, substantial computational power, and considerable fine-tuning efforts to achieve optimal performance across diverse use cases. Although pre-trained models offer a useful starting point, adapting them to meet specific user needs often demands extensive customization, and infrastructure overhead. This challenge grows when a single model must support diverse appli-cations with differing requirements for performance. Traditional solutions often involve training multiple model versions to meet varying requirements, which can be inefficient and difficult to maintain. In order to overcome this challenge, we propose NNObfuscator, a novel utility control mechanism that enables AI models to dynamically modify their performance according to predefined conditions. It is different from traditional methods that need separate models for each user. Instead, NNObfuscator allows a single model to be adapted in real time, giving you controlled access to multiple levels of performance. This mechanism enables model owners set up tiered access, ensuring that free-tier users receive a baseline level of performance while premium users benefit from enhanced capabilities. The approach improves resource allocation, reduces unnecessary computation, and supports sustainable business models in AI deployment. To validate our approach, we conducted experiments on multiple tasks, including image classification, semantic segmentation, and text to image generation, using well-established models such as ResNet, DeepLab, VGG16, FCN and Stable Diffusion. Experimental results show that NNObfuscator successfully makes model more adaptable, so that a single trained model can handle a broad range of tasks without requiring a lot of changes.

</details>


### [10] [VL-MedGuide: A Visual-Linguistic Large Model for Intelligent and Explainable Skin Disease Auxiliary Diagnosis](https://arxiv.org/abs/2508.06624)
*Kexin Yu,Zihan Xu,Jialei Xie,Carter Adams*

Main category: cs.CV

TL;DR: VL-MedGuide是一个基于视觉-语言大模型的新型框架，用于皮肤病的智能辅助诊断，通过多模态概念感知和可解释的疾病推理模块，实现了高性能和透明化的诊断。


<details>
  <summary>Details</summary>
Motivation: 解决皮肤病诊断中视觉特征复杂多样以及现有模型缺乏可解释性的问题。

Method: 采用两阶段框架：多模态概念感知模块识别并描述皮肤病相关特征；可解释疾病推理模块结合视觉信息和概念进行诊断并提供透明解释。

Result: 在Derm7pt数据集上，VL-MedGuide在疾病诊断（83.55% BACC, 80.12% F1）和概念检测（76.10% BACC, 67.45% F1）上达到最优性能。

Conclusion: VL-MedGuide通过提供清晰、完整的解释，弥合了AI性能与临床实用性之间的差距。

Abstract: Accurate diagnosis of skin diseases remains a significant challenge due to the complex and diverse visual features present in dermatoscopic images, often compounded by a lack of interpretability in existing purely visual diagnostic models. To address these limitations, this study introduces VL-MedGuide (Visual-Linguistic Medical Guide), a novel framework leveraging the powerful multi-modal understanding and reasoning capabilities of Visual-Language Large Models (LVLMs) for intelligent and inherently interpretable auxiliary diagnosis of skin conditions. VL-MedGuide operates in two interconnected stages: a Multi-modal Concept Perception Module, which identifies and linguistically describes dermatologically relevant visual features through sophisticated prompt engineering, and an Explainable Disease Reasoning Module, which integrates these concepts with raw visual information via Chain-of-Thought prompting to provide precise disease diagnoses alongside transparent rationales. Comprehensive experiments on the Derm7pt dataset demonstrate that VL-MedGuide achieves state-of-the-art performance in both disease diagnosis (83.55% BACC, 80.12% F1) and concept detection (76.10% BACC, 67.45% F1), surpassing existing baselines. Furthermore, human evaluations confirm the high clarity, completeness, and trustworthiness of its generated explanations, bridging the gap between AI performance and clinical utility by offering actionable, explainable insights for dermatological practice.

</details>


### [11] [CycleDiff: Cycle Diffusion Models for Unpaired Image-to-image Translation](https://arxiv.org/abs/2508.06625)
*Shilong Zou,Yuhang Huang,Renjiao Yi,Chenyang Zhu,Kai Xu*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce a diffusion-based cross-domain image translator in the absence of paired training data. Unlike GAN-based methods, our approach integrates diffusion models to learn the image translation process, allowing for more coverable modeling of the data distribution and performance improvement of the cross-domain translation. However, incorporating the translation process within the diffusion process is still challenging since the two processes are not aligned exactly, i.e., the diffusion process is applied to the noisy signal while the translation process is conducted on the clean signal. As a result, recent diffusion-based studies employ separate training or shallow integration to learn the two processes, yet this may cause the local minimal of the translation optimization, constraining the effectiveness of diffusion models. To address the problem, we propose a novel joint learning framework that aligns the diffusion and the translation process, thereby improving the global optimality. Specifically, we propose to extract the image components with diffusion models to represent the clean signal and employ the translation process with the image components, enabling an end-to-end joint learning manner. On the other hand, we introduce a time-dependent translation network to learn the complex translation mapping, resulting in effective translation learning and significant performance improvement. Benefiting from the design of joint learning, our method enables global optimization of both processes, enhancing the optimality and achieving improved fidelity and structural consistency. We have conducted extensive experiments on RGB$\leftrightarrow$RGB and diverse cross-modality translation tasks including RGB$\leftrightarrow$Edge, RGB$\leftrightarrow$Semantics and RGB$\leftrightarrow$Depth, showcasing better generative performances than the state of the arts.

</details>


### [12] [CoDe-NeRF: Neural Rendering via Dynamic Coefficient Decomposition](https://arxiv.org/abs/2508.06632)
*Wenpeng Xing,Jie Chen,Zaifeng Yang,Tiancheng Zhao,Gaolei Li,Changting Lin,Yike Guo,Meng Han*

Main category: cs.CV

TL;DR: 本文提出了一种基于动态系数分解的神经渲染框架，用于改进复杂视点依赖外观的建模，特别是在处理镜面反射和高光时表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂镜面反射和高光时，常因光照与材质属性纠缠或优化不稳定而产生模糊反射，因此需要一种更有效的解决方案。

Method: 通过动态系数分解，将复杂外观分解为共享的静态神经基和由视点与光照条件生成的动态系数，再通过动态辐射积分器合成最终辐射。

Result: 实验结果表明，该方法在多个挑战性基准测试中能生成更锐利和真实的镜面高光。

Conclusion: 这种分解范式为神经场景表示中复杂外观的建模提供了灵活且有效的方向。

Abstract: Neural Radiance Fields (NeRF) have shown impressive performance in novel view synthesis, but challenges remain in rendering scenes with complex specular reflections and highlights. Existing approaches may produce blurry reflections due to entanglement between lighting and material properties, or encounter optimization instability when relying on physically-based inverse rendering. In this work, we present a neural rendering framework based on dynamic coefficient decomposition, aiming to improve the modeling of view-dependent appearance. Our approach decomposes complex appearance into a shared, static neural basis that encodes intrinsic material properties, and a set of dynamic coefficients generated by a Coefficient Network conditioned on view and illumination. A Dynamic Radiance Integrator then combines these components to synthesize the final radiance. Experimental results on several challenging benchmarks suggest that our method can produce sharper and more realistic specular highlights compared to existing techniques. We hope that this decomposition paradigm can provide a flexible and effective direction for modeling complex appearance in neural scene representations.

</details>


### [13] [Towards Robust Red-Green Watermarking for Autoregressive Image Generators](https://arxiv.org/abs/2508.06656)
*Denis Lukovnikov,Andreas Müller,Erwin Quiring,Asja Fischer*

Main category: cs.CV

TL;DR: 本文探讨了在自回归（AR）图像模型中应用生成内水印的方法，提出了两种基于视觉令牌聚类的水印方案，显著提升了水印的鲁棒性和检测性。


<details>
  <summary>Details</summary>
Motivation: 尽管生成内水印在潜在扩散模型（LDMs）中表现出高鲁棒性，但在自回归图像模型中的应用尚未被探索。本文旨在填补这一空白。

Method: 研究了基于令牌级水印的方案，并提出两种新方法：1）基于聚类查找表的无训练方法；2）通过微调VAE编码器直接从扰动图像预测令牌聚类。

Result: 实验表明，聚类级水印显著提升了对抗扰动和再生攻击的鲁棒性，同时保持图像质量。聚类分类进一步提高了水印检测性，优于基线方法。

Conclusion: 提出的方法不仅提升了水印的鲁棒性和检测性，还具有快速的验证运行时，适用于轻量级后处理水印场景。

Abstract: In-generation watermarking for detecting and attributing generated content has recently been explored for latent diffusion models (LDMs), demonstrating high robustness. However, the use of in-generation watermarks in autoregressive (AR) image models has not been explored yet. AR models generate images by autoregressively predicting a sequence of visual tokens that are then decoded into pixels using a vector-quantized decoder. Inspired by red-green watermarks for large language models, we examine token-level watermarking schemes that bias the next-token prediction based on prior tokens. We find that a direct transfer of these schemes works in principle, but the detectability of the watermarks decreases considerably under common image perturbations. As a remedy, we propose two novel watermarking methods that rely on visual token clustering to assign similar tokens to the same set. Firstly, we investigate a training-free approach that relies on a cluster lookup table, and secondly, we finetune VAE encoders to predict token clusters directly from perturbed images. Overall, our experiments show that cluster-level watermarks improve robustness against perturbations and regeneration attacks while preserving image quality. Cluster classification further boosts watermark detectability, outperforming a set of baselines. Moreover, our methods offer fast verification runtime, comparable to lightweight post-hoc watermarking methods.

</details>


### [14] [Edge Detection for Organ Boundaries via Top Down Refinement and SubPixel Upsampling](https://arxiv.org/abs/2508.06805)
*Aarav Mehta,Priya Deshmukh,Vikram Singh,Siddharth Malhotra,Krishnan Menon Iyer,Tanvi Iyer*

Main category: cs.CV

TL;DR: 提出了一种针对医学图像的精确边缘检测方法，通过反向细化架构提升边界定位精度，显著改善医学影像任务。


<details>
  <summary>Details</summary>
Motivation: 医学影像中器官边界的毫米级精确定位对分割、手术规划等至关重要，现有卷积网络在自然图像中表现良好，但在医学图像中定位精度不足。

Method: 采用自上而下的反向细化架构，逐步融合高层语义特征与低层细节，支持2D和体积数据，并通过轻量3D上下文聚合处理各向异性体积。

Result: 在CT和MRI数据集上，边界定位精度显著优于基线方法，下游任务（如分割、配准）性能也得到提升。

Conclusion: 该方法生成的精确器官边界对临床医学影像任务具有重要价值。

Abstract: Accurate localization of organ boundaries is critical in medical imaging for segmentation, registration, surgical planning, and radiotherapy. While deep convolutional networks (ConvNets) have advanced general-purpose edge detection to near-human performance on natural images, their outputs often lack precise localization, a limitation that is particularly harmful in medical applications where millimeter-level accuracy is required. Building on a systematic analysis of ConvNet edge outputs, we propose a medically focused crisp edge detector that adapts a novel top-down backward refinement architecture to medical images (2D and volumetric). Our method progressively upsamples and fuses high-level semantic features with fine-grained low-level cues through a backward refinement pathway, producing high-resolution, well-localized organ boundaries. We further extend the design to handle anisotropic volumes by combining 2D slice-wise refinement with light 3D context aggregation to retain computational efficiency. Evaluations on several CT and MRI organ datasets demonstrate substantially improved boundary localization under strict criteria (boundary F-measure, Hausdorff distance) compared to baseline ConvNet detectors and contemporary medical edge/contour methods. Importantly, integrating our crisp edge maps into downstream pipelines yields consistent gains in organ segmentation (higher Dice scores, lower boundary errors), more accurate image registration, and improved delineation of lesions near organ interfaces. The proposed approach produces clinically valuable, crisp organ edges that materially enhance common medical-imaging tasks.

</details>


### [15] [Low-Rank Expert Merging for Multi-Source Domain Adaptation in Person Re-Identification](https://arxiv.org/abs/2508.06831)
*Taha Mustapha Nehdi,Nairouz Mrabah,Atif Belal,Marco Pedersoli,Eric Granger*

Main category: cs.CV

TL;DR: SAGE-reID是一种高效的多源域自适应方法，通过源特定的低秩适配器和轻量级门控网络实现跨域知识迁移，显著减少计算成本和内存消耗。


<details>
  <summary>Details</summary>
Motivation: 解决多源域自适应（MSDA）方法中训练参数和计算成本高的问题，同时保持高准确性和鲁棒性。

Method: 使用源特定的低秩适配器（LoRA）和轻量级门控网络动态合并适配器，实现跨域知识迁移。

Result: 在Market-1501、DukeMTMC-reID和MSMT17基准测试中表现优于现有方法，且计算高效。

Conclusion: SAGE-reID是一种高效、低成本的MSDA方法，适用于行人重识别任务。

Abstract: Adapting person re-identification (reID) models to new target environments remains a challenging problem that is typically addressed using unsupervised domain adaptation (UDA) methods. Recent works show that when labeled data originates from several distinct sources (e.g., datasets and cameras), considering each source separately and applying multi-source domain adaptation (MSDA) typically yields higher accuracy and robustness compared to blending the sources and performing conventional UDA. However, state-of-the-art MSDA methods learn domain-specific backbone models or require access to source domain data during adaptation, resulting in significant growth in training parameters and computational cost. In this paper, a Source-free Adaptive Gated Experts (SAGE-reID) method is introduced for person reID. Our SAGE-reID is a cost-effective, source-free MSDA method that first trains individual source-specific low-rank adapters (LoRA) through source-free UDA. Next, a lightweight gating network is introduced and trained to dynamically assign optimal merging weights for fusion of LoRA experts, enabling effective cross-domain knowledge transfer. While the number of backbone parameters remains constant across source domains, LoRA experts scale linearly but remain negligible in size (<= 2% of the backbone), reducing both the memory consumption and risk of overfitting. Extensive experiments conducted on three challenging benchmarks: Market-1501, DukeMTMC-reID, and MSMT17 indicate that SAGE-reID outperforms state-of-the-art methods while being computationally efficient.

</details>


### [16] [MultiRef: Controllable Image Generation with Multiple Visual References](https://arxiv.org/abs/2508.06905)
*Ruoxi Chen,Dongping Chen,Siyuan Wu,Sinan Wang,Shiyun Lang,Petr Sushko,Gaoyang Jiang,Yao Wan,Ranjay Krishna*

Main category: cs.CV

TL;DR: 论文提出了一种多参考图像生成任务，并引入了MultiRef-bench评估框架和MultiRef数据集，实验表明现有模型在多参考条件下表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成框架主要依赖单源输入，而设计师通常从多个视觉参考中获取灵感，因此需要开发能整合多参考的生成工具。

Method: 通过RefBlend数据引擎生成合成样本，构建MultiRef-bench评估框架和MultiRef数据集，测试多个图像-文本模型和代理框架。

Result: 最佳模型OmniGen在合成样本和真实样本中分别仅达到66.6%和79.0%的准确率，表明现有系统在多参考条件下仍有挑战。

Conclusion: 研究为开发更灵活、类人的创意工具提供了方向，MultiRef数据集将促进进一步研究。

Abstract: Visual designers naturally draw inspiration from multiple visual references, combining diverse elements and aesthetic principles to create artwork. However, current image generative frameworks predominantly rely on single-source inputs -- either text prompts or individual reference images. In this paper, we focus on the task of controllable image generation using multiple visual references. We introduce MultiRef-bench, a rigorous evaluation framework comprising 990 synthetic and 1,000 real-world samples that require incorporating visual content from multiple reference images. The synthetic samples are synthetically generated through our data engine RefBlend, with 10 reference types and 33 reference combinations. Based on RefBlend, we further construct a dataset MultiRef containing 38k high-quality images to facilitate further research. Our experiments across three interleaved image-text models (i.e., OmniGen, ACE, and Show-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that even state-of-the-art systems struggle with multi-reference conditioning, with the best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in real-world cases on average compared to the golden answer. These findings provide valuable directions for developing more flexible and human-like creative tools that can effectively integrate multiple sources of visual inspiration. The dataset is publicly available at: https://multiref.github.io/.

</details>


### [17] [Talk2Image: A Multi-Agent System for Multi-Turn Image Generation and Editing](https://arxiv.org/abs/2508.06916)
*Shichao Ma,Yunhe Guo,Jiahao Su,Qihe Huang,Zhengyang Zhou,Yang Wang*

Main category: cs.CV

TL;DR: Talk2Image是一个多代理系统，用于多轮对话中的交互式图像生成和编辑，解决了现有单代理系统的意图漂移和不连贯编辑问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成任务多关注单轮场景，难以处理多轮迭代创作任务，且单代理系统易导致意图漂移和编辑不连贯。

Method: Talk2Image通过意图解析、任务分解与协作执行、多视角反馈驱动优化三个关键组件实现多轮对话中的图像生成与编辑。

Result: 实验表明，Talk2Image在可控性、连贯性和用户满意度上优于现有基线。

Conclusion: Talk2Image通过多代理协作和反馈机制，显著提升了多轮图像生成与编辑的效果。

Abstract: Text-to-image generation tasks have driven remarkable advances in diverse media applications, yet most focus on single-turn scenarios and struggle with iterative, multi-turn creative tasks. Recent dialogue-based systems attempt to bridge this gap, but their single-agent, sequential paradigm often causes intention drift and incoherent edits. To address these limitations, we present Talk2Image, a novel multi-agent system for interactive image generation and editing in multi-turn dialogue scenarios. Our approach integrates three key components: intention parsing from dialogue history, task decomposition and collaborative execution across specialized agents, and feedback-driven refinement based on a multi-view evaluation mechanism. Talk2Image enables step-by-step alignment with user intention and consistent image editing. Experiments demonstrate that Talk2Image outperforms existing baselines in controllability, coherence, and user satisfaction across iterative image generation and editing tasks.

</details>


### [18] [AR-GRPO: Training Autoregressive Image Generation Models via Reinforcement Learning](https://arxiv.org/abs/2508.06924)
*Shihao Yuan,Yahui Liu,Yang Yue,Jingyuan Zhang,Wangmeng Zuo,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.CV

TL;DR: 论文提出AR-GRPO方法，通过在线强化学习优化自回归图像生成模型，显著提升生成图像的质量和人类偏好。


<details>
  <summary>Details</summary>
Motivation: 受强化学习在大型语言模型中的成功启发，探索其在自回归图像生成模型中的应用，以提升生成图像的多维质量。

Method: 采用Group Relative Policy Optimization (GRPO)算法，结合精心设计的奖励函数，评估生成图像的感知质量、真实性和语义保真度。

Result: 在类条件和文本条件图像生成任务中，实验表明RL增强框架显著优于标准自回归基线，各项评估指标均有提升。

Conclusion: 验证了基于强化学习的优化在自回归图像生成中的可行性，为可控高质量图像合成开辟了新途径。

Abstract: Inspired by the success of reinforcement learning (RL) in refining large language models (LLMs), we propose AR-GRPO, an approach to integrate online RL training into autoregressive (AR) image generation models. We adapt the Group Relative Policy Optimization (GRPO) algorithm to refine the vanilla autoregressive models' outputs by carefully designed reward functions that evaluate generated images across multiple quality dimensions, including perceptual quality, realism, and semantic fidelity. We conduct comprehensive experiments on both class-conditional (i.e., class-to-image) and text-conditional (i.e., text-to-image) image generation tasks, demonstrating that our RL-enhanced framework significantly improves both the image quality and human preference of generated images compared to the standard AR baselines. Our results show consistent improvements across various evaluation metrics, establishing the viability of RL-based optimization for AR image generation and opening new avenues for controllable and high-quality image synthesis. The source codes and models are available at: https://github.com/Kwai-Klear/AR-GRPO.

</details>


### [19] [CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-Free Image Editing](https://arxiv.org/abs/2508.06937)
*Weiyan Xie,Han Gao,Didan Deng,Kaican Li,April Hua Liu,Yongxiang Huang,Nevin L. Zhang*

Main category: cs.CV

TL;DR: CannyEdit是一种无需训练的图像编辑框架，通过选择性Canny控制和双提示引导，解决了现有方法在文本遵从性、上下文保真度和编辑无缝性上的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在文本到图像模型的区域编辑中难以平衡文本遵从性、上下文保真度和编辑无缝性。

Method: CannyEdit采用选择性Canny控制（保留未编辑区域细节）和双提示引导（结合局部和全局提示）实现精确编辑。

Result: CannyEdit在真实图像编辑任务中表现优于现有方法，文本遵从性和上下文保真度平衡提升2.93%至10.49%，且编辑无缝性显著提高。

Conclusion: CannyEdit通过创新性方法显著提升了图像编辑的质量和自然度，优于现有技术。

Abstract: Recent advances in text-to-image (T2I) models have enabled training-free regional image editing by leveraging the generative priors of foundation models. However, existing methods struggle to balance text adherence in edited regions, context fidelity in unedited areas, and seamless integration of edits. We introduce CannyEdit, a novel training-free framework that addresses these challenges through two key innovations: (1) Selective Canny Control, which masks the structural guidance of Canny ControlNet in user-specified editable regions while strictly preserving details of the source images in unedited areas via inversion-phase ControlNet information retention. This enables precise, text-driven edits without compromising contextual integrity. (2) Dual-Prompt Guidance, which combines local prompts for object-specific edits with a global target prompt to maintain coherent scene interactions. On real-world image editing tasks (addition, replacement, removal), CannyEdit outperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent improvement in the balance of text adherence and context fidelity. In terms of editing seamlessness, user studies reveal only 49.2 percent of general users and 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited when paired with real images without edits, versus 76.08 to 89.09 percent for competitor methods.

</details>


### [20] [WeatherDiffusion: Weather-Guided Diffusion Model for Forward and Inverse Rendering](https://arxiv.org/abs/2508.06982)
*Yixin Zhu,Zuoliang Zhu,Miloš Hašan,Jian Yang,Jin Xie,Beibei Wang*

Main category: cs.CV

TL;DR: WeatherDiffusion是一个基于扩散模型的框架，用于自动驾驶场景中的正向和逆向渲染，支持天气和光照编辑，并通过提出的MAA机制提升渲染质量。


<details>
  <summary>Details</summary>
Motivation: 复杂天气和光照条件对自动驾驶场景的理解和重建提出了挑战，现有扩散模型难以控制且缺乏鲁棒性。

Method: 提出WeatherDiffusion框架，结合文本引导的预测本征图，实现可控的天气和光照编辑，并引入MAA机制提升逆向渲染质量。

Result: 在多个基准测试中优于现有方法，并在下游任务（如目标检测和图像分割）中显著提升鲁棒性。

Conclusion: WeatherDiffusion为自动驾驶场景的渲染和编辑提供了高效且可控的解决方案，具有实际应用价值。

Abstract: Forward and inverse rendering have emerged as key techniques for enabling understanding and reconstruction in the context of autonomous driving (AD). However, complex weather and illumination pose great challenges to this task. The emergence of large diffusion models has shown promise in achieving reasonable results through learning from 2D priors, but these models are difficult to control and lack robustness. In this paper, we introduce WeatherDiffusion, a diffusion-based framework for forward and inverse rendering on AD scenes with various weather and lighting conditions. Our method enables authentic estimation of material properties, scene geometry, and lighting, and further supports controllable weather and illumination editing through the use of predicted intrinsic maps guided by text descriptions. We observe that different intrinsic maps should correspond to different regions of the original image. Based on this observation, we propose Intrinsic map-aware attention (MAA) to enable high-quality inverse rendering. Additionally, we introduce a synthetic dataset (\ie WeatherSynthetic) and a real-world dataset (\ie WeatherReal) for forward and inverse rendering on AD scenes with diverse weather and lighting. Extensive experiments show that our WeatherDiffusion outperforms state-of-the-art methods on several benchmarks. Moreover, our method demonstrates significant value in downstream tasks for AD, enhancing the robustness of object detection and image segmentation in challenging weather scenarios.

</details>


### [21] [TADoc: Robust Time-Aware Document Image Dewarping](https://arxiv.org/abs/2508.06988)
*Fangmin Zhao,Weichao Zeng,Zhenhang Li,Dongbao Yang,Yu Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种动态建模文档去扭曲任务的方法，并设计了轻量级框架TADoc，同时引入新评价指标DLS。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂文档结构和高变形时效果不佳，作者认为去扭曲是一个渐进过程而非一步转换。

Method: 将任务重新建模为动态过程，设计TADoc框架，并提出DLS评价指标。

Result: 实验表明模型具有强鲁棒性，在多个基准测试中表现优越。

Conclusion: 动态建模和TADoc框架有效提升了文档去扭曲性能，DLS指标补充了现有评价不足。

Abstract: Flattening curved, wrinkled, and rotated document images captured by portable photographing devices, termed document image dewarping, has become an increasingly important task with the rise of digital economy and online working. Although many methods have been proposed recently, they often struggle to achieve satisfactory results when confronted with intricate document structures and higher degrees of deformation in real-world scenarios. Our main insight is that, unlike other document restoration tasks (e.g., deblurring), dewarping in real physical scenes is a progressive motion rather than a one-step transformation. Based on this, we have undertaken two key initiatives. Firstly, we reformulate this task, modeling it for the first time as a dynamic process that encompasses a series of intermediate states. Secondly, we design a lightweight framework called TADoc (Time-Aware Document Dewarping Network) to address the geometric distortion of document images. In addition, due to the inadequacy of OCR metrics for document images containing sparse text, the comprehensiveness of evaluation is insufficient. To address this shortcoming, we propose a new metric -- DLS (Document Layout Similarity) -- to evaluate the effectiveness of document dewarping in downstream tasks. Extensive experiments and in-depth evaluations have been conducted and the results indicate that our model possesses strong robustness, achieving superiority on several benchmarks with different document types and degrees of distortion.

</details>


### [22] [ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting](https://arxiv.org/abs/2508.07089)
*Sandro Papais,Letian Wang,Brian Cheong,Steven L. Waslander*

Main category: cs.CV

TL;DR: ForeSight是一种用于自动驾驶车辆视觉3D感知的联合检测与预测框架，通过多任务流式学习和双向学习提升性能，无需显式目标关联。


<details>
  <summary>Details</summary>
Motivation: 传统方法将检测与预测作为独立任务，无法充分利用时间线索，ForeSight旨在解决这一问题。

Method: 采用多任务流式学习和双向学习，结合检测与预测的查询记忆，利用轨迹预测增强空间推理，并通过流式预测变换器提升时间一致性。

Result: 在nuScenes数据集上，ForeSight以54.9%的EPA领先先前方法9.3%，并在mAP和minADE上表现最佳。

Conclusion: ForeSight通过联合检测与预测，显著提升了自动驾驶3D感知的性能和效率。

Abstract: We introduce ForeSight, a novel joint detection and forecasting framework for vision-based 3D perception in autonomous vehicles. Traditional approaches treat detection and forecasting as separate sequential tasks, limiting their ability to leverage temporal cues. ForeSight addresses this limitation with a multi-task streaming and bidirectional learning approach, allowing detection and forecasting to share query memory and propagate information seamlessly. The forecast-aware detection transformer enhances spatial reasoning by integrating trajectory predictions from a multiple hypothesis forecast memory queue, while the streaming forecast transformer improves temporal consistency using past forecasts and refined detections. Unlike tracking-based methods, ForeSight eliminates the need for explicit object association, reducing error propagation with a tracking-free model that efficiently scales across multi-frame sequences. Experiments on the nuScenes dataset show that ForeSight achieves state-of-the-art performance, achieving an EPA of 54.9%, surpassing previous methods by 9.3%, while also attaining the best mAP and minADE among multi-view detection and forecasting models.

</details>


### [23] [Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2508.07146)
*Yu Liu,Zhijie Liu,Xiao Ren,You-Fu Li,He Kong*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的行人轨迹预测框架，结合短期和长期运动意图，通过自适应指导和残差噪声预测器提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型缺乏对行人意图的显式语义建模，可能导致行为误解和预测精度下降。

Method: 短期意图通过残差极坐标表示建模，长期意图通过可学习的基于令牌的终点预测器估计，并结合自适应指导和残差噪声预测器优化扩散过程。

Result: 在ETH、UCY和SDD基准测试中表现优异，优于现有方法。

Conclusion: 结合意图建模的自适应扩散框架显著提升了行人轨迹预测的准确性和鲁棒性。

Abstract: Predicting pedestrian motion trajectories is critical for the path planning and motion control of autonomous vehicles. Recent diffusion-based models have shown promising results in capturing the inherent stochasticity of pedestrian behavior for trajectory prediction. However, the absence of explicit semantic modelling of pedestrian intent in many diffusion-based methods may result in misinterpreted behaviors and reduced prediction accuracy. To address the above challenges, we propose a diffusion-based pedestrian trajectory prediction framework that incorporates both short-term and long-term motion intentions. Short-term intent is modelled using a residual polar representation, which decouples direction and magnitude to capture fine-grained local motion patterns. Long-term intent is estimated through a learnable, token-based endpoint predictor that generates multiple candidate goals with associated probabilities, enabling multimodal and context-aware intention modelling. Furthermore, we enhance the diffusion process by incorporating adaptive guidance and a residual noise predictor that dynamically refines denoising accuracy. The proposed framework is evaluated on the widely used ETH, UCY, and SDD benchmarks, demonstrating competitive results against state-of-the-art methods.

</details>


### [24] [SketchAnimator: Animate Sketch via Motion Customization of Text-to-Video Diffusion Models](https://arxiv.org/abs/2508.07149)
*Ruolin Yang,Da Li,Honggang Zhang,Yi-Zhe Song*

Main category: cs.CV

TL;DR: 提出了一种名为SketchAnimator的草图动画模型，通过三个阶段（外观学习、运动学习和视频先验蒸馏）将静态草图转化为动态视频，保留草图外观的同时模仿参考视频的运动。


<details>
  <summary>Details</summary>
Motivation: 草图动画通常需要专业技能且耗时，业余用户难以完成。本文旨在简化这一过程，使非专业人士也能为草图添加创意运动。

Method: 分为三个阶段：1）外观学习，2）运动学习（利用LoRA整合草图外观和参考视频的运动信息），3）视频先验蒸馏（使用SDS更新Bezier曲线参数）。

Result: 模型能够生成保留草图外观并模仿参考视频运动的动画，且在单次运动定制任务中表现优于其他方法。

Conclusion: SketchAnimator为草图动画提供了一种高效且用户友好的解决方案，适用于非专业人士。

Abstract: Sketching is a uniquely human tool for expressing ideas and creativity. The animation of sketches infuses life into these static drawings, opening a new dimension for designers. Animating sketches is a time-consuming process that demands professional skills and extensive experience, often proving daunting for amateurs. In this paper, we propose a novel sketch animation model SketchAnimator, which enables adding creative motion to a given sketch, like "a jumping car''. Namely, given an input sketch and a reference video, we divide the sketch animation into three stages: Appearance Learning, Motion Learning and Video Prior Distillation. In stages 1 and 2, we utilize LoRA to integrate sketch appearance information and motion dynamics from the reference video into the pre-trained T2V model. In the third stage, we utilize Score Distillation Sampling (SDS) to update the parameters of the Bezier curves in each sketch frame according to the acquired motion information. Consequently, our model produces a sketch video that not only retains the original appearance of the sketch but also mirrors the dynamic movements of the reference video. We compare our method with alternative approaches and demonstrate that it generates the desired sketch video under the challenge of one-shot motion customization.

</details>


### [25] [Similarity Matters: A Novel Depth-guided Network for Image Restoration and A New Dataset](https://arxiv.org/abs/2508.07211)
*Junyi He,Liuling Chen,Hongyang Zhou,Zhang xiaoxing,Xiaobin Zhu,Shengxiang Yu,Jingyan Qin,Xu-Cheng Yin*

Main category: cs.CV

TL;DR: 论文提出了一种深度引导网络（DGN）用于图像恢复，解决了现有方法忽略深度信息的问题，并引入了一个新的大规模高分辨率数据集。


<details>
  <summary>Details</summary>
Motivation: 现有图像恢复方法常忽略深度信息，导致相似性匹配不佳、浅景深场景中注意力分散以及深景深背景下内容过度增强。

Method: DGN包含两个交互分支：深度估计分支提供结构引导，图像恢复分支执行核心任务。恢复分支通过渐进窗口自注意力和稀疏非局部注意力捕捉对象内和对象间相似性。

Result: 实验表明，该方法在多个标准基准上达到最优性能，并能很好地泛化到未见过的植物图像。

Conclusion: 深度引导网络通过联合训练提升了恢复质量，同时增强了深度估计的准确性，证明了其有效性和鲁棒性。

Abstract: Image restoration has seen substantial progress in recent years. However, existing methods often neglect depth information, which hurts similarity matching, results in attention distractions in shallow depth-of-field (DoF) scenarios, and excessive enhancement of background content in deep DoF settings. To overcome these limitations, we propose a novel Depth-Guided Network (DGN) for image restoration, together with a novel large-scale high-resolution dataset. Specifically, the network consists of two interactive branches: a depth estimation branch that provides structural guidance, and an image restoration branch that performs the core restoration task. In addition, the image restoration branch exploits intra-object similarity through progressive window-based self-attention and captures inter-object similarity via sparse non-local attention. Through joint training, depth features contribute to improved restoration quality, while the enhanced visual features from the restoration branch in turn help refine depth estimation. Notably, we also introduce a new dataset for training and evaluation, consisting of 9,205 high-resolution images from 403 plant species, with diverse depth and texture variations. Extensive experiments show that our method achieves state-of-the-art performance on several standard benchmarks and generalizes well to unseen plant images, demonstrating its effectiveness and robustness.

</details>


### [26] [Unsupervised Real-World Super-Resolution via Rectified Flow Degradation Modelling](https://arxiv.org/abs/2508.07214)
*Hongyang Zhou,Xiaobin Zhu,Liuling Chen,Junyi He,Jingyan Qin,Xu-Cheng Yin,Zhang xiaoxing*

Main category: cs.CV

TL;DR: 提出了一种基于校正流的无监督真实世界超分辨率方法，通过新模块RFDM和FGDM捕捉真实退化，提升SR网络性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法因合成数据与真实数据域差距大而难以泛化的问题。

Method: 使用RFDM和FGDM模块，分别通过连续可逆退化轨迹和傅里叶相位信息建模真实退化。

Result: 在真实数据集上显著提升了现有SR方法的性能。

Conclusion: 该方法有效解决了真实世界SR中的退化建模问题，提升了模型泛化能力。

Abstract: Unsupervised real-world super-resolution (SR) faces critical challenges due to the complex, unknown degradation distributions in practical scenarios. Existing methods struggle to generalize from synthetic low-resolution (LR) and high-resolution (HR) image pairs to real-world data due to a significant domain gap. In this paper, we propose an unsupervised real-world SR method based on rectified flow to effectively capture and model real-world degradation, synthesizing LR-HR training pairs with realistic degradation. Specifically, given unpaired LR and HR images, we propose a novel Rectified Flow Degradation Module (RFDM) that introduces degradation-transformed LR (DT-LR) images as intermediaries. By modeling the degradation trajectory in a continuous and invertible manner, RFDM better captures real-world degradation and enhances the realism of generated LR images. Additionally, we propose a Fourier Prior Guided Degradation Module (FGDM) that leverages structural information embedded in Fourier phase components to ensure more precise modeling of real-world degradation. Finally, the LR images are processed by both FGDM and RFDM, producing final synthetic LR images with real-world degradation. The synthetic LR images are paired with the given HR images to train the off-the-shelf SR networks. Extensive experiments on real-world datasets demonstrate that our method significantly enhances the performance of existing SR approaches in real-world scenarios.

</details>


### [27] [Consistent and Controllable Image Animation with Motion Linear Diffusion Transformers](https://arxiv.org/abs/2508.07246)
*Xin Ma,Yaohui Wang,Genyun Jia,Xinyuan Chen,Tien-Tsin Wong,Cunjian Chen*

Main category: cs.CV

TL;DR: MiraMo是一个高效、一致且平滑的图像动画框架，通过线性注意力、运动残差学习和DCT噪声优化解决现有问题。


<details>
  <summary>Details</summary>
Motivation: 解决图像动画中外观一致性和运动平滑性的挑战，同时提升计算效率。

Method: 采用线性注意力替代传统自注意力，引入运动残差学习和DCT噪声优化策略。

Result: 实验证明MiraMo在生成一致性、平滑性和可控性动画方面优于现有方法。

Conclusion: MiraMo在图像动画领域展现出高效性和多功能性，适用于多种任务。

Abstract: Image animation has seen significant progress, driven by the powerful generative capabilities of diffusion models. However, maintaining appearance consistency with static input images and mitigating abrupt motion transitions in generated animations remain persistent challenges. While text-to-video (T2V) generation has demonstrated impressive performance with diffusion transformer models, the image animation field still largely relies on U-Net-based diffusion models, which lag behind the latest T2V approaches. Moreover, the quadratic complexity of vanilla self-attention mechanisms in Transformers imposes heavy computational demands, making image animation particularly resource-intensive. To address these issues, we propose MiraMo, a framework designed to enhance efficiency, appearance consistency, and motion smoothness in image animation. Specifically, MiraMo introduces three key elements: (1) A foundational text-to-video architecture replacing vanilla self-attention with efficient linear attention to reduce computational overhead while preserving generation quality; (2) A novel motion residual learning paradigm that focuses on modeling motion dynamics rather than directly predicting frames, improving temporal consistency; and (3) A DCT-based noise refinement strategy during inference to suppress sudden motion artifacts, complemented by a dynamics control module to balance motion smoothness and expressiveness. Extensive experiments against state-of-the-art methods validate the superiority of MiraMo in generating consistent, smooth, and controllable animations with accelerated inference speed. Additionally, we demonstrate the versatility of MiraMo through applications in motion transfer and video editing tasks.

</details>


### [28] [SynMatch: Rethinking Consistency in Medical Image Segmentation with Sparse Annotations](https://arxiv.org/abs/2508.07298)
*Zhiqiang Shen,Peng Cao,Xiaoli Liu,Jinzhu Yang,Osmar R. Zaiane*

Main category: cs.CV

TL;DR: SynMatch通过合成图像匹配伪标签，解决了医学图像分割中标签稀缺的问题，无需改进伪标签即可提升性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中标签稀缺，现有方法因伪标签与未标记图像不一致而性能受限。

Method: SynMatch利用分割模型提取的纹理和形状特征合成图像，生成高度一致的合成图像-伪标签对。

Result: 在多种医学图像分割任务中表现优异，尤其在BSL设置下，显著优于现有方法。

Conclusion: SynMatch提供了一种无需训练参数的图像合成方法，显著提升了标签稀缺情况下的分割性能。

Abstract: Label scarcity remains a major challenge in deep learning-based medical image segmentation. Recent studies use strong-weak pseudo supervision to leverage unlabeled data. However, performance is often hindered by inconsistencies between pseudo labels and their corresponding unlabeled images. In this work, we propose \textbf{SynMatch}, a novel framework that sidesteps the need for improving pseudo labels by synthesizing images to match them instead. Specifically, SynMatch synthesizes images using texture and shape features extracted from the same segmentation model that generates the corresponding pseudo labels for unlabeled images. This design enables the generation of highly consistent synthesized-image-pseudo-label pairs without requiring any training parameters for image synthesis. We extensively evaluate SynMatch across diverse medical image segmentation tasks under semi-supervised learning (SSL), weakly-supervised learning (WSL), and barely-supervised learning (BSL) settings with increasingly limited annotations. The results demonstrate that SynMatch achieves superior performance, especially in the most challenging BSL setting. For example, it outperforms the recent strong-weak pseudo supervision-based method by 29.71\% and 10.05\% on the polyp segmentation task with 5\% and 10\% scribble annotations, respectively. The code will be released at https://github.com/Senyh/SynMatch.

</details>


### [29] [CoAR: Concept Injection into Autoregressive Models for Personalized Text-to-Image Generation](https://arxiv.org/abs/2508.07341)
*Fangtai Wu,Mushui Liu,Weijie He,Wanggui He,Hao Jiang,Zhao Wang,Yunlong Yu*

Main category: cs.CV

TL;DR: CoAR是一种新颖的框架，用于在统一自回归模型中注入主题概念，同时保持所有预训练参数冻结，显著提高了计算和内存效率。


<details>
  <summary>Details</summary>
Motivation: 探索统一自回归模型在定制图像生成中的潜力，解决现有方法因全微调或适配器导致的成本高、过拟合或灾难性遗忘问题。

Method: 采用Layerwise Multimodal Context Learning策略，仅用极少量参数学习有效的主题表示，并通过正则化防止过拟合和语言漂移。

Result: CoAR在主题驱动和风格个性化任务中表现优异，同时显著提升计算和内存效率，参数调整量少于0.05%。

Conclusion: CoAR为定制图像生成提供了一种高效且性能优越的解决方案，同时避免了传统方法的缺陷。

Abstract: The unified autoregressive (AR) model excels at multimodal understanding and generation, but its potential for customized image generation remains underexplored. Existing customized generation methods rely on full fine-tuning or adapters, making them costly and prone to overfitting or catastrophic forgetting. In this paper, we propose \textbf{CoAR}, a novel framework for injecting subject concepts into the unified AR models while keeping all pre-trained parameters completely frozen. CoAR learns effective, specific subject representations with only a minimal number of parameters using a Layerwise Multimodal Context Learning strategy. To address overfitting and language drift, we further introduce regularization that preserves the pre-trained distribution and anchors context tokens to improve subject fidelity and re-contextualization. Additionally, CoAR supports training-free subject customization in a user-provided style. Experiments demonstrate that CoAR achieves superior performance on both subject-driven personalization and style personalization, while delivering significant gains in computational and memory efficiency. Notably, CoAR tunes less than \textbf{0.05\%} of the parameters while achieving competitive performance compared to recent Proxy-Tuning. Code: https://github.com/KZF-kzf/CoAR

</details>


### [30] [SODiff: Semantic-Oriented Diffusion Model for JPEG Compression Artifacts Removal](https://arxiv.org/abs/2508.07346)
*Tingyu Yang,Jue Gong,Jinpei Guo,Wenbo Li,Yong Guo,Yulun Zhang*

Main category: cs.CV

TL;DR: SODiff是一种新颖的语义导向一步扩散模型，用于JPEG伪影去除，通过语义对齐的图像提示提取器和质量因子感知时间预测器，显著提升了恢复效果。


<details>
  <summary>Details</summary>
Motivation: JPEG高压缩比常导致严重视觉伪影，现有深度学习方法难以恢复复杂纹理细节，导致输出过度平滑。

Method: 提出SODiff模型，结合语义对齐图像提示提取器（SAIPE）和质量因子感知时间预测器，优化扩散过程。

Result: 实验表明，SODiff在视觉质量和定量指标上优于现有领先方法。

Conclusion: SODiff通过语义导向和自适应时间预测，有效提升了JPEG伪影去除的性能。

Abstract: JPEG, as a widely used image compression standard, often introduces severe visual artifacts when achieving high compression ratios. Although existing deep learning-based restoration methods have made considerable progress, they often struggle to recover complex texture details, resulting in over-smoothed outputs. To overcome these limitations, we propose SODiff, a novel and efficient semantic-oriented one-step diffusion model for JPEG artifacts removal. Our core idea is that effective restoration hinges on providing semantic-oriented guidance to the pre-trained diffusion model, thereby fully leveraging its powerful generative prior. To this end, SODiff incorporates a semantic-aligned image prompt extractor (SAIPE). SAIPE extracts rich features from low-quality (LQ) images and projects them into an embedding space semantically aligned with that of the text encoder. Simultaneously, it preserves crucial information for faithful reconstruction. Furthermore, we propose a quality factor-aware time predictor that implicitly learns the compression quality factor (QF) of the LQ image and adaptively selects the optimal denoising start timestep for the diffusion process. Extensive experimental results show that our SODiff outperforms recent leading methods in both visual quality and quantitative metrics. Code is available at: https://github.com/frakenation/SODiff

</details>


### [31] [GS4Buildings: Prior-Guided Gaussian Splatting for 3D Building Reconstruction](https://arxiv.org/abs/2508.07355)
*Qilin Zhang,Olaf Wysocki,Boris Jutzi*

Main category: cs.CV

TL;DR: GS4Buildings利用语义3D建筑模型改进2D高斯溅射，提升大规模城市建筑重建的完整性和几何精度。


<details>
  <summary>Details</summary>
Motivation: 解决2D高斯溅射在复杂城市场景中因遮挡导致重建不完整的问题。

Method: 直接从LoD2语义3D建筑模型初始化高斯，结合先验深度和法线图优化重建过程。

Result: 重建完整性提升20.5%，几何精度提升32.8%，高斯基元减少71.8%。

Conclusion: 语义建筑模型集成可推动高斯溅射技术在城市应用中的发展。

Abstract: Recent advances in Gaussian Splatting (GS) have demonstrated its effectiveness in photo-realistic rendering and 3D reconstruction. Among these, 2D Gaussian Splatting (2DGS) is particularly suitable for surface reconstruction due to its flattened Gaussian representation and integrated normal regularization. However, its performance often degrades in large-scale and complex urban scenes with frequent occlusions, leading to incomplete building reconstructions. We propose GS4Buildings, a novel prior-guided Gaussian Splatting method leveraging the ubiquity of semantic 3D building models for robust and scalable building surface reconstruction. Instead of relying on traditional Structure-from-Motion (SfM) pipelines, GS4Buildings initializes Gaussians directly from low-level Level of Detail (LoD)2 semantic 3D building models. Moreover, we generate prior depth and normal maps from the planar building geometry and incorporate them into the optimization process, providing strong geometric guidance for surface consistency and structural accuracy. We also introduce an optional building-focused mode that limits reconstruction to building regions, achieving a 71.8% reduction in Gaussian primitives and enabling a more efficient and compact representation. Experiments on urban datasets demonstrate that GS4Buildings improves reconstruction completeness by 20.5% and geometric accuracy by 32.8%. These results highlight the potential of semantic building model integration to advance GS-based reconstruction toward real-world urban applications such as smart cities and digital twins. Our project is available: https://github.com/zqlin0521/GS4Buildings.

</details>


### [32] [DIP-GS: Deep Image Prior For Gaussian Splatting Sparse View Recovery](https://arxiv.org/abs/2508.07372)
*Rajaei Khatib,Raja Giryes*

Main category: cs.CV

TL;DR: DIP-GS是一种基于深度图像先验（DIP）的3D高斯泼溅（3DGS）改进方法，解决了3DGS在稀疏视图重建中的不足，无需预训练模型即可实现竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 3DGS在多视图重建中表现优异，但在稀疏视图场景下效果不佳。本文旨在通过DIP先验提升3DGS在稀疏视图重建中的性能。

Method: 提出DIP-GS，利用DIP先验的内部结构和模式，以粗到细的方式优化3DGS参数，仅依赖输入帧而不使用预训练模型。

Result: DIP-GS在稀疏视图重建任务中取得了竞争性的SOTA结果。

Conclusion: DIP-GS通过DIP先验有效解决了3DGS在稀疏视图重建中的局限性，展示了其潜力。

Abstract: 3D Gaussian Splatting (3DGS) is a leading 3D scene reconstruction method, obtaining high-quality reconstruction with real-time rendering runtime performance. The main idea behind 3DGS is to represent the scene as a collection of 3D gaussians, while learning their parameters to fit the given views of the scene. While achieving superior performance in the presence of many views, 3DGS struggles with sparse view reconstruction, where the input views are sparse and do not fully cover the scene and have low overlaps. In this paper, we propose DIP-GS, a Deep Image Prior (DIP) 3DGS representation. By using the DIP prior, which utilizes internal structure and patterns, with coarse-to-fine manner, DIP-based 3DGS can operate in scenarios where vanilla 3DGS fails, such as sparse view recovery. Note that our approach does not use any pre-trained models such as generative models and depth estimation, but rather relies only on the input frames. Among such methods, DIP-GS obtains state-of-the-art (SOTA) competitive results on various sparse-view reconstruction tasks, demonstrating its capabilities.

</details>


### [33] [CharacterShot: Controllable and Consistent 4D Character Animation](https://arxiv.org/abs/2508.07409)
*Junyao Gao,Jiaxing Li,Wenran Liu,Yanhong Zeng,Fei Shen,Kai Chen,Yanan Sun,Cairong Zhao*

Main category: cs.CV

TL;DR: CharacterShot是一个可控且一致的4D角色动画框架，通过单张参考图像和2D姿势序列生成动态3D角色动画。


<details>
  <summary>Details</summary>
Motivation: 为设计师提供一种简单的方法，从单张图像和2D姿势序列生成高质量的4D角色动画。

Method: 1. 预训练基于DiT的2D角色动画模型；2. 通过双注意力模块和相机先验将动画从2D提升到3D；3. 使用4D高斯溅射优化生成连续稳定的4D角色表示。

Result: 在新建的CharacterBench基准测试中表现优于现有方法。

Conclusion: CharacterShot框架高效且性能优越，代码和数据集将公开。

Abstract: In this paper, we propose \textbf{CharacterShot}, a controllable and consistent 4D character animation framework that enables any individual designer to create dynamic 3D characters (i.e., 4D character animation) from a single reference character image and a 2D pose sequence. We begin by pretraining a powerful 2D character animation model based on a cutting-edge DiT-based image-to-video model, which allows for any 2D pose sequnce as controllable signal. We then lift the animation model from 2D to 3D through introducing dual-attention module together with camera prior to generate multi-view videos with spatial-temporal and spatial-view consistency. Finally, we employ a novel neighbor-constrained 4D gaussian splatting optimization on these multi-view videos, resulting in continuous and stable 4D character representations. Moreover, to improve character-centric performance, we construct a large-scale dataset Character4D, containing 13,115 unique characters with diverse appearances and motions, rendered from multiple viewpoints. Extensive experiments on our newly constructed benchmark, CharacterBench, demonstrate that our approach outperforms current state-of-the-art methods. Code, models, and datasets will be publicly available at https://github.com/Jeoyal/CharacterShot.

</details>


### [34] [CLUE: Leveraging Low-Rank Adaptation to Capture Latent Uncovered Evidence for Image Forgery Localization](https://arxiv.org/abs/2508.07413)
*Youqi Wang,Shunquan Tan,Rongxuan Peng,Bin Li,Jiwu Huang*

Main category: cs.CV

TL;DR: CLUE利用Stable Diffusion 3和Segment Anything Model，通过噪声注入和参数高效调整，实现了高精度的伪造定位。


<details>
  <summary>Details</summary>
Motivation: 图像编辑工具和生成AI的普及导致伪造图像泛滥，威胁数字媒体的真实性。

Method: 结合SD3的Rectified Flow机制和LoRA调整，以及SAM的图像编码器，提取伪造特征。

Result: CLUE在泛化性能和鲁棒性上显著优于现有方法。

Conclusion: CLUE为伪造检测提供了高效且鲁棒的解决方案。

Abstract: The increasing accessibility of image editing tools and generative AI has led to a proliferation of visually convincing forgeries, compromising the authenticity of digital media. In this paper, in addition to leveraging distortions from conventional forgeries, we repurpose the mechanism of a state-of-the-art (SOTA) text-to-image synthesis model by exploiting its internal generative process, turning it into a high-fidelity forgery localization tool. To this end, we propose CLUE (Capture Latent Uncovered Evidence), a framework that employs Low- Rank Adaptation (LoRA) to parameter-efficiently reconfigure Stable Diffusion 3 (SD3) as a forensic feature extractor. Our approach begins with the strategic use of SD3's Rectified Flow (RF) mechanism to inject noise at varying intensities into the latent representation, thereby steering the LoRAtuned denoising process to amplify subtle statistical inconsistencies indicative of a forgery. To complement the latent analysis with high-level semantic context and precise spatial details, our method incorporates contextual features from the image encoder of the Segment Anything Model (SAM), which is parameter-efficiently adapted to better trace the boundaries of forged regions. Extensive evaluations demonstrate CLUE's SOTA generalization performance, significantly outperforming prior methods. Furthermore, CLUE shows superior robustness against common post-processing attacks and Online Social Networks (OSNs). Code is publicly available at https://github.com/SZAISEC/CLUE.

</details>


### [35] [Novel View Synthesis with Gaussian Splatting: Impact on Photogrammetry Model Accuracy and Resolution](https://arxiv.org/abs/2508.07483)
*Pranav Chougule*

Main category: cs.CV

TL;DR: 本文比较了摄影测量与高斯泼溅技术在3D模型重建和视图合成中的表现，开发了改进的高斯泼溅方法，并展示了其在提升摄影测量重建质量中的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究摄影测量与高斯泼溅技术在3D重建和视图合成中的性能差异，探索高斯泼溅在生成高质量新视图和改进摄影测量模型中的应用。

Method: 创建真实场景图像数据集，分别用摄影测量和高斯泼溅构建3D模型，通过SSIM、PSNR、LPIPS和分辨率指标评估性能，改进高斯泼溅方法以支持Blender环境中的新视图渲染。

Result: 高斯泼溅能生成高质量新视图，并提升摄影测量模型的性能，两种方法各有优劣。

Conclusion: 高斯泼溅在3D重建和视图合成中具有潜力，为扩展现实、摄影测量和自动驾驶模拟提供了有价值的信息。

Abstract: In this paper, I present a comprehensive study comparing Photogrammetry and Gaussian Splatting techniques for 3D model reconstruction and view synthesis. I created a dataset of images from a real-world scene and constructed 3D models using both methods. To evaluate the performance, I compared the models using structural similarity index (SSIM), peak signal-to-noise ratio (PSNR), learned perceptual image patch similarity (LPIPS), and lp/mm resolution based on the USAF resolution chart. A significant contribution of this work is the development of a modified Gaussian Splatting repository, which I forked and enhanced to enable rendering images from novel camera poses generated in the Blender environment. This innovation allows for the synthesis of high-quality novel views, showcasing the flexibility and potential of Gaussian Splatting. My investigation extends to an augmented dataset that includes both original ground images and novel views synthesized via Gaussian Splatting. This augmented dataset was employed to generate a new photogrammetry model, which was then compared against the original photogrammetry model created using only the original images. The results demonstrate the efficacy of using Gaussian Splatting to generate novel high-quality views and its potential to improve photogrammetry-based 3D reconstructions. The comparative analysis highlights the strengths and limitations of both approaches, providing valuable information for applications in extended reality (XR), photogrammetry, and autonomous vehicle simulations. Code is available at https://github.com/pranavc2255/gaussian-splatting-novel-view-render.git.

</details>


### [36] [Exploring Multimodal Diffusion Transformers for Enhanced Prompt-based Image Editing](https://arxiv.org/abs/2508.07519)
*Joonghyuk Shin,Alchan Hwang,Yujin Kim,Daneul Kim,Jaesik Park*

Main category: cs.CV

TL;DR: 本文分析了多模态扩散变换器（MM-DiT）的注意力机制，提出了一种基于提示的图像编辑方法，适用于不同MM-DiT变体。


<details>
  <summary>Details</summary>
Motivation: 传统U-Net架构已被基于Transformer的扩散模型取代，但MM-DiT的双向注意力机制对现有编辑技术提出了挑战。

Method: 通过将注意力矩阵分解为四个块，分析其特性，并提出一种基于提示的编辑方法。

Result: 提出了一种支持从全局到局部编辑的鲁棒方法，适用于包括少步模型在内的多种MM-DiT变体。

Conclusion: 研究填补了U-Net方法与新兴架构之间的空白，提供了对MM-DiT行为模式的深入理解。

Abstract: Transformer-based diffusion models have recently superseded traditional U-Net architectures, with multimodal diffusion transformers (MM-DiT) emerging as the dominant approach in state-of-the-art models like Stable Diffusion 3 and Flux.1. Previous approaches have relied on unidirectional cross-attention mechanisms, with information flowing from text embeddings to image latents. In contrast, MMDiT introduces a unified attention mechanism that concatenates input projections from both modalities and performs a single full attention operation, allowing bidirectional information flow between text and image branches. This architectural shift presents significant challenges for existing editing techniques. In this paper, we systematically analyze MM-DiT's attention mechanism by decomposing attention matrices into four distinct blocks, revealing their inherent characteristics. Through these analyses, we propose a robust, prompt-based image editing method for MM-DiT that supports global to local edits across various MM-DiT variants, including few-step models. We believe our findings bridge the gap between existing U-Net-based methods and emerging architectures, offering deeper insights into MMDiT's behavioral patterns.

</details>


### [37] [Enhanced Generative Structure Prior for Chinese Text Image Super-resolution](https://arxiv.org/abs/2508.07537)
*Xiaoming Li,Wangmeng Zuo,Chen Change Loy*

Main category: cs.CV

TL;DR: 本文提出了一种针对中文文本图像超分辨率（SR）的新框架，通过结构先验和StyleGAN结合，恢复低分辨率字符的精确笔画。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注英文文本，对复杂脚本（如中文）的超分辨率研究较少，且缺乏结构级指导。

Method: 提出了一种结构先验，结合StyleGAN生成能力，并通过代码本机制限制生成空间，确保字符结构的完整性和多样性。

Result: 实验表明，该方法能准确恢复低分辨率中文字符的清晰笔画，适用于不规则布局的真实场景。

Conclusion: 结构先验与StyleGAN的协同作用为中文文本图像超分辨率提供了有效解决方案。

Abstract: Faithful text image super-resolution (SR) is challenging because each character has a unique structure and usually exhibits diverse font styles and layouts. While existing methods primarily focus on English text, less attention has been paid to more complex scripts like Chinese. In this paper, we introduce a high-quality text image SR framework designed to restore the precise strokes of low-resolution (LR) Chinese characters. Unlike methods that rely on character recognition priors to regularize the SR task, we propose a novel structure prior that offers structure-level guidance to enhance visual quality. Our framework incorporates this structure prior within a StyleGAN model, leveraging its generative capabilities for restoration. To maintain the integrity of character structures while accommodating various font styles and layouts, we implement a codebook-based mechanism that restricts the generative space of StyleGAN. Each code in the codebook represents the structure of a specific character, while the vector $w$ in StyleGAN controls the character's style, including typeface, orientation, and location. Through the collaborative interaction between the codebook and style, we generate a high-resolution structure prior that aligns with LR characters both spatially and structurally. Experiments demonstrate that this structure prior provides robust, character-specific guidance, enabling the accurate restoration of clear strokes in degraded characters, even for real-world LR Chinese text with irregular layouts. Our code and pre-trained models will be available at https://github.com/csxmli2016/MARCONetPlusPlus

</details>


### [38] [Splat4D: Diffusion-Enhanced 4D Gaussian Splatting for Temporally and Spatially Consistent Content Creation](https://arxiv.org/abs/2508.07557)
*Minghao Yin,Yukang Cao,Songyou Peng,Kai Han*

Main category: cs.CV

TL;DR: Splat4D是一个从单目视频生成高质量4D内容的新框架，通过多视角渲染、不一致性识别、视频扩散模型和非对称U-Net优化，实现了时空一致性和高保真度。


<details>
  <summary>Details</summary>
Motivation: 解决从单目视频生成4D内容时面临的时空一致性、细节保留和用户指导问题。

Method: 利用多视角渲染、不一致性识别、视频扩散模型和非对称U-Net进行优化。

Result: 在公开基准测试中表现优异，支持多种应用如文本/图像条件4D生成和内容编辑。

Conclusion: Splat4D在4D内容生成中表现出色，具有广泛的应用潜力。

Abstract: Generating high-quality 4D content from monocular videos for applications such as digital humans and AR/VR poses challenges in ensuring temporal and spatial consistency, preserving intricate details, and incorporating user guidance effectively. To overcome these challenges, we introduce Splat4D, a novel framework enabling high-fidelity 4D content generation from a monocular video. Splat4D achieves superior performance while maintaining faithful spatial-temporal coherence by leveraging multi-view rendering, inconsistency identification, a video diffusion model, and an asymmetric U-Net for refinement. Through extensive evaluations on public benchmarks, Splat4D consistently demonstrates state-of-the-art performance across various metrics, underscoring the efficacy of our approach. Additionally, the versatility of Splat4D is validated in various applications such as text/image conditioned 4D generation, 4D human generation, and text-guided content editing, producing coherent outcomes following user instructions.

</details>


### [39] [ShoulderShot: Generating Over-the-Shoulder Dialogue Videos](https://arxiv.org/abs/2508.07597)
*Yuang Zhang,Junqi Cheng,Haoyu Zhao,Jiaxi Gu,Fangyuan Zou,Zenghui Lu,Peng Shu*

Main category: cs.CV

TL;DR: ShoulderShot框架通过双镜头生成和循环视频技术，解决了对话视频生成中的角色一致性和空间连续性问题，并在多轮对话长度上表现优异。


<details>
  <summary>Details</summary>
Motivation: 过肩对话视频在影视作品中具有重要作用，但现有研究对其生成技术探索不足，尤其是在角色一致性、空间连续性和长对话生成方面存在挑战。

Method: 提出ShoulderShot框架，结合双镜头生成和循环视频技术，以保持角色一致性并支持长对话生成。

Result: ShoulderShot在镜头切换布局、空间连续性和对话长度灵活性上优于现有方法。

Conclusion: 该框架为实际对话视频生成提供了新的可能性。

Abstract: Over-the-shoulder dialogue videos are essential in films, short dramas, and advertisements, providing visual variety and enhancing viewers' emotional connection. Despite their importance, such dialogue scenes remain largely underexplored in video generation research. The main challenges include maintaining character consistency across different shots, creating a sense of spatial continuity, and generating long, multi-turn dialogues within limited computational budgets. Here, we present ShoulderShot, a framework that combines dual-shot generation with looping video, enabling extended dialogues while preserving character consistency. Our results demonstrate capabilities that surpass existing methods in terms of shot-reverse-shot layout, spatial continuity, and flexibility in dialogue length, thereby opening up new possibilities for practical dialogue video generation. Videos and comparisons are available at https://shouldershot.github.io.

</details>


### [40] [X2Edit: Revisiting Arbitrary-Instruction Image Editing through Self-Constructed Data and Task-Aware Representation Learning](https://arxiv.org/abs/2508.07607)
*Jian Ma,Xujie Zhu,Zihao Pan,Qirong Peng,Xu Guo,Chen Chen,Haonan Lu*

Main category: cs.CV

TL;DR: 论文介绍了X2Edit数据集和任务感知的MoE-LoRA训练方法，用于图像编辑任务，性能优于现有开源数据集。


<details>
  <summary>Details</summary>
Motivation: 现有开源数据集和编辑模块在多样性和兼容性上不足，需改进。

Method: 构建X2Edit数据集（14类任务，370万高质量数据），设计MoE-LoRA训练方法（参数仅8%），并引入对比学习。

Result: 模型性能优秀，数据集优势显著。

Conclusion: X2Edit数据集和训练方法为图像编辑任务提供了高效解决方案。

Abstract: Existing open-source datasets for arbitrary-instruction image editing remain suboptimal, while a plug-and-play editing module compatible with community-prevalent generative models is notably absent. In this paper, we first introduce the X2Edit Dataset, a comprehensive dataset covering 14 diverse editing tasks, including subject-driven generation. We utilize the industry-leading unified image generation models and expert models to construct the data. Meanwhile, we design reasonable editing instructions with the VLM and implement various scoring mechanisms to filter the data. As a result, we construct 3.7 million high-quality data with balanced categories. Second, to better integrate seamlessly with community image generation models, we design task-aware MoE-LoRA training based on FLUX.1, with only 8\% of the parameters of the full model. To further improve the final performance, we utilize the internal representations of the diffusion model and define positive/negative samples based on image editing types to introduce contrastive learning. Extensive experiments demonstrate that the model's editing performance is competitive among many excellent models. Additionally, the constructed dataset exhibits substantial advantages over existing open-source datasets. The open-source code, checkpoints, and datasets for X2Edit can be found at the following link: https://github.com/OPPO-Mente-Lab/X2Edit.

</details>


### [41] [LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering](https://arxiv.org/abs/2508.07647)
*Xiaohang Zhan,Dingming Liu*

Main category: cs.CV

TL;DR: 提出了一种无需训练的图像生成算法，通过体积渲染原理精确控制图像中物体的遮挡关系。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖提示或布局控制遮挡，但缺乏精确性，需要一种无需重新训练的方法实现精准遮挡控制。

Method: 利用预训练的图像扩散模型，结合体积渲染原理在潜在空间中“渲染”场景，通过遮挡关系和物体透射率指导生成。

Result: 在遮挡准确性上显著优于现有方法，并能通过调整物体透明度等参数实现多种视觉效果。

Conclusion: 该方法无需重新训练模型，基于物理原理实现了精确的遮挡控制，扩展了图像生成的应用场景。

Abstract: We propose a novel training-free image generation algorithm that precisely controls the occlusion relationships between objects in an image. Existing image generation methods typically rely on prompts to influence occlusion, which often lack precision. While layout-to-image methods provide control over object locations, they fail to address occlusion relationships explicitly. Given a pre-trained image diffusion model, our method leverages volume rendering principles to "render" the scene in latent space, guided by occlusion relationships and the estimated transmittance of objects. This approach does not require retraining or fine-tuning the image diffusion model, yet it enables accurate occlusion control due to its physics-grounded foundation. In extensive experiments, our method significantly outperforms existing approaches in terms of occlusion accuracy. Furthermore, we demonstrate that by adjusting the opacities of objects or concepts during rendering, our method can achieve a variety of effects, such as altering the transparency of objects, the density of mass (e.g., forests), the concentration of particles (e.g., rain, fog), the intensity of light, and the strength of lens effects, etc.

</details>


### [42] [Multi-view Normal and Distance Guidance Gaussian Splatting for Surface Reconstruction](https://arxiv.org/abs/2508.07701)
*Bo Jia,Yanan Guo,Ying Chang,Benkui Zhang,Ying Xie,Kangning Du,Lin Cao*

Main category: cs.CV

TL;DR: 论文提出了一种多视角距离和法线引导的高斯泼溅方法，解决了3D高斯泼溅在多视角场景中的几何偏差问题。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅在单视角投影平面内几何表现良好，但在多视角切换时可能出现偏差，需解决多视角场景的距离和全局匹配问题。

Method: 设计了多视角距离重投影正则化模块和多视角法线增强模块，通过约束邻近深度图和匹配3D法线实现几何深度统一和高精度重建。

Result: 实验结果表明，该方法在定量和定性评估中均优于基线，显著提升了3D高斯泼溅的表面重建能力。

Conclusion: 提出的方法有效解决了多视角几何偏差问题，提升了3D高斯泼溅的重建精度和一致性。

Abstract: 3D Gaussian Splatting (3DGS) achieves remarkable results in the field of surface reconstruction. However, when Gaussian normal vectors are aligned within the single-view projection plane, while the geometry appears reasonable in the current view, biases may emerge upon switching to nearby views. To address the distance and global matching challenges in multi-view scenes, we design multi-view normal and distance-guided Gaussian splatting. This method achieves geometric depth unification and high-accuracy reconstruction by constraining nearby depth maps and aligning 3D normals. Specifically, for the reconstruction of small indoor and outdoor scenes, we propose a multi-view distance reprojection regularization module that achieves multi-view Gaussian alignment by computing the distance loss between two nearby views and the same Gaussian surface. Additionally, we develop a multi-view normal enhancement module, which ensures consistency across views by matching the normals of pixel points in nearby views and calculating the loss. Extensive experimental results demonstrate that our method outperforms the baseline in both quantitative and qualitative evaluations, significantly enhancing the surface reconstruction capability of 3DGS.

</details>


### [43] [Grouped Speculative Decoding for Autoregressive Image Generation](https://arxiv.org/abs/2508.07747)
*Junhyuk So,Juncheol Shin,Hyunho Kook,Eunhyeok Park*

Main category: cs.CV

TL;DR: 本文提出了一种名为Grouped Speculative Decoding (GSD)的无训练加速方法，用于自回归图像模型，解决了传统方法因仅接受单一最可能标记而导致的效率低下问题。


<details>
  <summary>Details</summary>
Motivation: 自回归图像模型因其序列性质导致推理时间长，限制了实际应用。现有加速方法要么效果有限，要么需要额外训练。

Method: 提出动态GSD方法，通过评估视觉有效标记的簇而非单一目标标记，利用图像标记的冗余性和多样性。

Result: 实验表明，GSD平均加速3.7倍，且无需额外训练即可保持图像质量。

Conclusion: GSD是一种高效且无需训练的自回归图像模型加速方法，显著提升了推理效率。

Abstract: Recently, autoregressive (AR) image models have demonstrated remarkable generative capabilities, positioning themselves as a compelling alternative to diffusion models. However, their sequential nature leads to long inference times, limiting their practical scalability. In this work, we introduce Grouped Speculative Decoding (GSD), a novel, training-free acceleration method for AR image models. While recent studies have explored Speculative Decoding (SD) as a means to speed up AR image generation, existing approaches either provide only modest acceleration or require additional training. Our in-depth analysis reveals a fundamental difference between language and image tokens: image tokens exhibit inherent redundancy and diversity, meaning multiple tokens can convey valid semantics. However, traditional SD methods are designed to accept only a single most-likely token, which fails to leverage this difference, leading to excessive false-negative rejections. To address this, we propose a new SD strategy that evaluates clusters of visually valid tokens rather than relying on a single target token. Additionally, we observe that static clustering based on embedding distance is ineffective, which motivates our dynamic GSD approach. Extensive experiments show that GSD accelerates AR image models by an average of 3.7x while preserving image quality-all without requiring any additional training. The source code is available at https://github.com/junhyukso/GSD

</details>


### [44] [Comparison Reveals Commonality: Customized Image Generation through Contrastive Inversion](https://arxiv.org/abs/2508.07755)
*Minseo Kim,Minchan Kwon,Dongyeun Lee,Yunho Jeon,Junmo Kim*

Main category: cs.CV

TL;DR: 提出了一种无需额外信息的对比反转方法，通过对比输入图像提取共同概念，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 定制化图像生成需求增加，现有方法依赖额外指导（如文本提示或空间掩码），可能导致辅助特征分离不完整，影响生成质量。

Method: 采用对比学习训练目标标记和图像辅助文本标记，通过解耦交叉注意力微调提高概念保真度。

Result: 实验表明，该方法在概念表示和编辑方面均表现优异。

Conclusion: 对比反转方法在无需额外信息的情况下，实现了高质量的共同概念提取和编辑。

Abstract: The recent demand for customized image generation raises a need for techniques that effectively extract the common concept from small sets of images. Existing methods typically rely on additional guidance, such as text prompts or spatial masks, to capture the common target concept. Unfortunately, relying on manually provided guidance can lead to incomplete separation of auxiliary features, which degrades generation quality.In this paper, we propose Contrastive Inversion, a novel approach that identifies the common concept by comparing the input images without relying on additional information. We train the target token along with the image-wise auxiliary text tokens via contrastive learning, which extracts the well-disentangled true semantics of the target. Then we apply disentangled cross-attention fine-tuning to improve concept fidelity without overfitting. Experimental results and analysis demonstrate that our method achieves a balanced, high-level performance in both concept representation and editing, outperforming existing techniques.

</details>


### [45] [Correspondence as Video: Test-Time Adaption on SAM2 for Reference Segmentation in the Wild](https://arxiv.org/abs/2508.07759)
*Haoran Wang,Zekun Li,Jian Zhang,Lei Qi,Yinghuan Shi*

Main category: cs.CV

TL;DR: CAV-SAM提出了一种轻量级方法，通过将参考-目标图像对的对应关系表示为伪视频，利用SAM2的iVOS能力适应下游任务，性能提升超过5%。


<details>
  <summary>Details</summary>
Motivation: 现有参考分割方法依赖元学习，数据与计算成本高，CAV-SAM旨在以更轻量的方式适应下游任务。

Method: 将参考-目标图像对的对应关系表示为伪视频，利用SAM2的iVOS能力，结合DBST和TTGA模块。

Result: 在广泛使用的数据集上，分割性能提升超过5%。

Conclusion: CAV-SAM为轻量级适应下游任务提供了有效方法，性能显著优于现有技术。

Abstract: Large vision models like the Segment Anything Model (SAM) exhibit significant limitations when applied to downstream tasks in the wild. Consequently, reference segmentation, which leverages reference images and their corresponding masks to impart novel knowledge to the model, emerges as a promising new direction for adapting vision models. However, existing reference segmentation approaches predominantly rely on meta-learning, which still necessitates an extensive meta-training process and brings massive data and computational cost. In this study, we propose a novel approach by representing the inherent correspondence between reference-target image pairs as a pseudo video. This perspective allows the latest version of SAM, known as SAM2, which is equipped with interactive video object segmentation (iVOS) capabilities, to be adapted to downstream tasks in a lightweight manner. We term this approach Correspondence As Video for SAM (CAV-SAM). CAV-SAM comprises two key modules: the Diffusion-Based Semantic Transition (DBST) module employs a diffusion model to construct a semantic transformation sequence, while the Test-Time Geometric Alignment (TTGA) module aligns the geometric changes within this sequence through test-time fine-tuning. We evaluated CAVSAM on widely-used datasets, achieving segmentation performance improvements exceeding 5% over SOTA methods. Implementation is provided in the supplementary materials.

</details>


### [46] [Dream4D: Lifting Camera-Controlled I2V towards Spatiotemporally Consistent 4D Generation](https://arxiv.org/abs/2508.07769)
*Xiaoyan Liu,Kangrui Li,Jiaxin Liu*

Main category: cs.CV

TL;DR: Dream4D框架通过结合可控视频生成和神经4D重建，解决了4D内容合成的时空一致性问题，显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前方法在保持视角一致性和处理复杂场景动态时存在困难，尤其是在大规模多元素交互环境中。

Method: 采用两阶段架构：先通过少样本学习从单图像预测最佳相机轨迹，再通过姿态条件扩散过程生成几何一致的多视角序列，最终转换为持久4D表示。

Result: 框架首次结合视频扩散模型的时间先验和重建模型的几何感知，生成质量（如mPSNR、mSSIM）优于现有方法。

Conclusion: Dream4D通过协同可控视频生成与神经4D重建，显著提升了4D内容合成的时空一致性和质量。

Abstract: The synthesis of spatiotemporally coherent 4D content presents fundamental challenges in computer vision, requiring simultaneous modeling of high-fidelity spatial representations and physically plausible temporal dynamics. Current approaches often struggle to maintain view consistency while handling complex scene dynamics, particularly in large-scale environments with multiple interacting elements. This work introduces Dream4D, a novel framework that bridges this gap through a synergy of controllable video generation and neural 4D reconstruction. Our approach seamlessly combines a two-stage architecture: it first predicts optimal camera trajectories from a single image using few-shot learning, then generates geometrically consistent multi-view sequences via a specialized pose-conditioned diffusion process, which are finally converted into a persistent 4D representation. This framework is the first to leverage both rich temporal priors from video diffusion models and geometric awareness of the reconstruction models, which significantly facilitates 4D generation and shows higher quality (e.g., mPSNR, mSSIM) over existing methods.

</details>


### [47] [DiTVR: Zero-Shot Diffusion Transformer for Video Restoration](https://arxiv.org/abs/2508.07811)
*Sicheng Gao,Nancy Mehta,Zongwei Wu,Radu Timofte*

Main category: cs.CV

TL;DR: DiTVR是一种零样本视频修复框架，结合扩散变换器和轨迹感知注意力，通过光流轨迹对齐令牌，提升时间一致性。


<details>
  <summary>Details</summary>
Motivation: 传统回归方法生成不真实细节且需大量配对数据，而生成扩散模型难以保证时间一致性。DiTVR旨在解决这些问题。

Method: 采用扩散变换器与轨迹感知注意力结合，通过光流轨迹对齐令牌，并引入小波引导的流一致性采样器。

Result: 在视频修复基准测试中达到零样本最优性能，时间一致性和细节保留表现优异，且对光流噪声和遮挡鲁棒。

Conclusion: DiTVR通过创新的注意力机制和采样器设计，显著提升了视频修复的时间一致性和细节质量。

Abstract: Video restoration aims to reconstruct high quality video sequences from low quality inputs, addressing tasks such as super resolution, denoising, and deblurring. Traditional regression based methods often produce unrealistic details and require extensive paired datasets, while recent generative diffusion models face challenges in ensuring temporal consistency. We introduce DiTVR, a zero shot video restoration framework that couples a diffusion transformer with trajectory aware attention and a wavelet guided, flow consistent sampler. Unlike prior 3D convolutional or frame wise diffusion approaches, our attention mechanism aligns tokens along optical flow trajectories, with particular emphasis on vital layers that exhibit the highest sensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically selects relevant tokens based on motion correspondences across frames. The flow guided sampler injects data consistency only into low-frequency bands, preserving high frequency priors while accelerating convergence. DiTVR establishes a new zero shot state of the art on video restoration benchmarks, demonstrating superior temporal consistency and detail preservation while remaining robust to flow noise and occlusions.

</details>


### [48] [Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP](https://arxiv.org/abs/2508.07819)
*Ke Ma,Jun Long,Hongxiao Fei,Liujie Hua,Yueyi Luo*

Main category: cs.CV

TL;DR: 论文提出了一种架构协同设计框架，通过改进特征表示和跨模态融合，解决了预训练视觉语言模型在零样本异常检测中的适应性问题。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉语言模型在零样本异常检测中存在适应性差距，主要原因是缺乏局部归纳偏置和特征融合范式不灵活。

Method: 采用参数高效的卷积低秩适应（Conv-LoRA）适配器注入局部归纳偏置，并引入动态融合网关（DFG）自适应调制文本提示。

Result: 在工业和医学基准测试中表现出优越的准确性和鲁棒性。

Conclusion: 协同设计框架对于将基础模型适应于密集感知任务至关重要。

Abstract: Pre-trained Vision-Language Models (VLMs) face a significant adaptation gap when applied to Zero-Shot Anomaly Detection (ZSAD), stemming from their lack of local inductive biases for dense prediction and their reliance on inflexible feature fusion paradigms. We address these limitations through an Architectural Co-Design framework that jointly refines feature representation and cross-modal fusion. Our method integrates a parameter-efficient Convolutional Low-Rank Adaptation (Conv-LoRA) adapter to inject local inductive biases for fine-grained representation, and introduces a Dynamic Fusion Gateway (DFG) that leverages visual context to adaptively modulate text prompts, enabling a powerful bidirectional fusion. Extensive experiments on diverse industrial and medical benchmarks demonstrate superior accuracy and robustness, validating that this synergistic co-design is critical for robustly adapting foundation models to dense perception tasks.

</details>


### [49] [TAP: Parameter-efficient Task-Aware Prompting for Adverse Weather Removal](https://arxiv.org/abs/2508.07878)
*Hanting Wang,Shengpeng Ji,Shulei Wang,Hai Huang,Xiao Jin,Qifei Zhang,Tao Jin*

Main category: cs.CV

TL;DR: 提出了一种参数高效的All-in-One图像修复框架，通过任务感知增强提示处理多种恶劣天气退化问题，参数仅2.75M。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖专用网络模块或参数，导致参数开销大，且忽视任务间相关性。

Method: 采用两阶段训练（预训练和提示调优），通过任务感知增强提示和低秩分解捕捉任务通用与特定特征。

Result: 实验表明，该方法在多种修复任务中表现优异，参数效率高。

Conclusion: 提出的框架高效且性能优越，为多任务图像修复提供了新思路。

Abstract: Image restoration under adverse weather conditions has been extensively explored, leading to numerous high-performance methods. In particular, recent advances in All-in-One approaches have shown impressive results by training on multi-task image restoration datasets. However, most of these methods rely on dedicated network modules or parameters for each specific degradation type, resulting in a significant parameter overhead. Moreover, the relatedness across different restoration tasks is often overlooked. In light of these issues, we propose a parameter-efficient All-in-One image restoration framework that leverages task-aware enhanced prompts to tackle various adverse weather degradations.Specifically, we adopt a two-stage training paradigm consisting of a pretraining phase and a prompt-tuning phase to mitigate parameter conflicts across tasks. We first employ supervised learning to acquire general restoration knowledge, and then adapt the model to handle specific degradation via trainable soft prompts. Crucially, we enhance these task-specific prompts in a task-aware manner. We apply low-rank decomposition to these prompts to capture both task-general and task-specific characteristics, and impose contrastive constraints to better align them with the actual inter-task relatedness. These enhanced prompts not only improve the parameter efficiency of the restoration model but also enable more accurate task modeling, as evidenced by t-SNE analysis. Experimental results on different restoration tasks demonstrate that the proposed method achieves superior performance with only 2.75M parameters.

</details>


### [50] [NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and Deformable 3D Gaussian Reconstruction](https://arxiv.org/abs/2508.07897)
*Tianle Zeng,Junlei Hu,Gerardo Loza Galindo,Sharib Ali,Duygu Sarikaya,Pietro Valdastri,Dominic Jones*

Main category: cs.CV

TL;DR: 提出了一种动态高斯Splatting技术，用于解决手术图像数据集稀缺问题，生成高质量合成数据，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动方法需要大量高质量标注数据，限制了手术数据科学的应用。

Method: 使用动态高斯模型表示动态手术场景，结合动态训练调整策略和自动标注方法。

Result: 生成的照片级合成数据在PSNR上表现最佳（29.87），模型性能提升15%。

Conclusion: 该方法有效解决了数据稀缺问题，显著提升了手术自动化模型的性能。

Abstract: Computer vision-based technologies significantly enhance surgical automation by advancing tool tracking, detection, and localization. However, Current data-driven approaches are data-voracious, requiring large, high-quality labeled image datasets, which limits their application in surgical data science. Our Work introduces a novel dynamic Gaussian Splatting technique to address the data scarcity in surgical image datasets. We propose a dynamic Gaussian model to represent dynamic surgical scenes, enabling the rendering of surgical instruments from unseen viewpoints and deformations with real tissue backgrounds. We utilize a dynamic training adjustment strategy to address challenges posed by poorly calibrated camera poses from real-world scenarios. Additionally, we propose a method based on dynamic Gaussians for automatically generating annotations for our synthetic data. For evaluation, we constructed a new dataset featuring seven scenes with 14,000 frames of tool and camera motion and tool jaw articulation, with a background of an ex-vivo porcine model. Using this dataset, we synthetically replicate the scene deformation from the ground truth data, allowing direct comparisons of synthetic image quality. Experimental results illustrate that our method generates photo-realistic labeled image datasets with the highest values in Peak-Signal-to-Noise Ratio (29.87). We further evaluate the performance of medical-specific neural networks trained on real and synthetic images using an unseen real-world image dataset. Our results show that the performance of models trained on synthetic images generated by the proposed method outperforms those trained with state-of-the-art standard data augmentation by 10%, leading to an overall improvement in model performances by nearly 15%.

</details>


### [51] [Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation](https://arxiv.org/abs/2508.07901)
*Bowen Xue,Qixin Yan,Wenjing Wang,Hao Liu,Chen Li*

Main category: cs.CV

TL;DR: 提出了一种轻量级、即插即用的视频生成框架Stand-In，通过条件图像分支和受限自注意力实现身份保留，仅需少量训练数据和参数即可高效生成高质量视频。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖过多训练参数且与其他AIGC工具不兼容，需要一种更高效的身份保留视频生成方案。

Method: 在预训练视频生成模型中引入条件图像分支，通过受限自注意力和条件位置映射实现身份控制，仅需2000对数据快速学习。

Result: 仅增加约1%参数，即可在视频质量和身份保留上优于其他全参数训练方法，并支持多种任务集成。

Conclusion: Stand-In框架高效、轻量且多功能，为身份保留视频生成提供了实用解决方案。

Abstract: Generating high-fidelity human videos that match user-specified identities is important yet challenging in the field of generative AI. Existing methods often rely on an excessive number of training parameters and lack compatibility with other AIGC tools. In this paper, we propose Stand-In, a lightweight and plug-and-play framework for identity preservation in video generation. Specifically, we introduce a conditional image branch into the pre-trained video generation model. Identity control is achieved through restricted self-attentions with conditional position mapping, and can be learned quickly with only 2000 pairs. Despite incorporating and training just $\sim$1\% additional parameters, our framework achieves excellent results in video quality and identity preservation, outperforming other full-parameter training methods. Moreover, our framework can be seamlessly integrated for other tasks, such as subject-driven video generation, pose-referenced video generation, stylization, and face swapping.

</details>


### [52] [Generative Video Matting](https://arxiv.org/abs/2508.07905)
*Yongtao Ge,Kangyang Xie,Guangkai Xu,Mingyu Liu,Li Ke,Longtao Huang,Hui Xue,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 论文提出了一种新的视频抠图方法，通过大规模预训练和合成数据生成，结合视频扩散模型的先验知识，显著提升了真实场景中的泛化能力和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 传统视频抠图方法因缺乏高质量真实数据而泛化能力差，本文旨在通过合成数据和预训练模型解决这一问题。

Method: 1. 利用多样化的合成和伪标签分割数据集进行大规模预训练；2. 开发可扩展的合成数据生成流程；3. 提出基于视频扩散模型的新架构，确保时间一致性。

Result: 在三个基准数据集上表现优异，展示了强大的泛化能力和时间一致性。

Conclusion: 该方法通过合成数据和预训练模型显著提升了视频抠图的质量和泛化能力。

Abstract: Video matting has traditionally been limited by the lack of high-quality ground-truth data. Most existing video matting datasets provide only human-annotated imperfect alpha and foreground annotations, which must be composited to background images or videos during the training stage. Thus, the generalization capability of previous methods in real-world scenarios is typically poor. In this work, we propose to solve the problem from two perspectives. First, we emphasize the importance of large-scale pre-training by pursuing diverse synthetic and pseudo-labeled segmentation datasets. We also develop a scalable synthetic data generation pipeline that can render diverse human bodies and fine-grained hairs, yielding around 200 video clips with a 3-second duration for fine-tuning. Second, we introduce a novel video matting approach that can effectively leverage the rich priors from pre-trained video diffusion models. This architecture offers two key advantages. First, strong priors play a critical role in bridging the domain gap between synthetic and real-world scenes. Second, unlike most existing methods that process video matting frame-by-frame and use an independent decoder to aggregate temporal information, our model is inherently designed for video, ensuring strong temporal consistency. We provide a comprehensive quantitative evaluation across three benchmark datasets, demonstrating our approach's superior performance, and present comprehensive qualitative results in diverse real-world scenes, illustrating the strong generalization capability of our method. The code is available at https://github.com/aim-uofa/GVM.

</details>


### [53] [Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation](https://arxiv.org/abs/2508.07981)
*Fangyuan Mao,Aiming Hao,Jintao Chen,Dongxia Liu,Xiaokun Feng,Jiashu Zhu,Meiqi Wu,Chubin Chen,Jiahong Wu,Xiangxiang Chu*

Main category: cs.CV

TL;DR: Omni-Effects提出了一种统一框架，支持提示引导的效果生成和空间可控的复合效果，解决了多VFX联合训练中的干扰和空间不可控问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型受限于单效果训练，无法实现空间可控的复合效果，限制了应用。

Method: 提出LoRA-MoE和SAP两项创新，结合IIF模块，实现多效果集成和精确空间控制。

Result: 实验证明Omni-Effects能精确控制效果类别和位置，生成多样化效果。

Conclusion: Omni-Effects为VFX生成提供了高效、可控的解决方案，推动了多效果集成的研究。

Abstract: Visual effects (VFX) are essential visual enhancements fundamental to modern cinematic production. Although video generation models offer cost-efficient solutions for VFX production, current methods are constrained by per-effect LoRA training, which limits generation to single effects. This fundamental limitation impedes applications that require spatially controllable composite effects, i.e., the concurrent generation of multiple effects at designated locations. However, integrating diverse effects into a unified framework faces major challenges: interference from effect variations and spatial uncontrollability during multi-VFX joint training. To tackle these challenges, we propose Omni-Effects, a first unified framework capable of generating prompt-guided effects and spatially controllable composite effects. The core of our framework comprises two key innovations: (1) LoRA-based Mixture of Experts (LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects within a unified model while effectively mitigating cross-task interference. (2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the text token, enabling precise spatial control. Furthermore, we introduce an Independent-Information Flow (IIF) module integrated within the SAP, isolating the control signals corresponding to individual effects to prevent any unwanted blending. To facilitate this research, we construct a comprehensive VFX dataset Omni-VFX via a novel data collection pipeline combining image editing and First-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX evaluation framework for validating model performance. Extensive experiments demonstrate that Omni-Effects achieves precise spatial control and diverse effect generation, enabling users to specify both the category and location of desired effects.

</details>


### [54] [S^2VG: 3D Stereoscopic and Spatial Video Generation via Denoising Frame Matrix](https://arxiv.org/abs/2508.08048)
*Peng Dai,Feitong Tan,Qiangeng Xu,Yihua Huang,David Futschik,Ruofei Du,Sean Fanello,Yinda Zhang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 提出了一种无需姿态估计和额外训练的方法，利用现成的单目视频生成模型生成沉浸式3D视频。


<details>
  <summary>Details</summary>
Motivation: 尽管单目视频生成模型已能生成高质量视频，但生成3D立体和空间视频仍是一个未充分探索的挑战。

Method: 通过深度信息将单目视频变形到预定义视角，并采用新型帧矩阵修复框架，结合双更新方案提升修复质量。

Result: 实验验证了该方法在多种生成模型上的有效性，显著优于现有方法。

Conclusion: 该方法为3D视频生成提供了一种高效且无需额外训练的解决方案。

Abstract: While video generation models excel at producing high-quality monocular videos, generating 3D stereoscopic and spatial videos for immersive applications remains an underexplored challenge. We present a pose-free and training-free method that leverages an off-the-shelf monocular video generation model to produce immersive 3D videos. Our approach first warps the generated monocular video into pre-defined camera viewpoints using estimated depth information, then applies a novel \textit{frame matrix} inpainting framework. This framework utilizes the original video generation model to synthesize missing content across different viewpoints and timestamps, ensuring spatial and temporal consistency without requiring additional model fine-tuning. Moreover, we develop a \dualupdate~scheme that further improves the quality of video inpainting by alleviating the negative effects propagated from disoccluded areas in the latent space. The resulting multi-view videos are then adapted into stereoscopic pairs or optimized into 4D Gaussians for spatial video synthesis. We validate the efficacy of our proposed method by conducting experiments on videos from various generative models, such as Sora, Lumiere, WALT, and Zeroscope. The experiments demonstrate that our method has a significant improvement over previous methods. Project page at: https://daipengwa.github.io/S-2VG_ProjectPage/

</details>


### [55] [TBAC-UniImage: Unified Understanding and Generation by Ladder-Side Diffusion Tuning](https://arxiv.org/abs/2508.08098)
*Junzhe Xu,Yuyang Yin,Xi Chen*

Main category: cs.CV

TL;DR: TBAC-UniImage是一种新型多模态理解与生成统一模型，通过深度集成预训练扩散模型和多模态大语言模型（MLLM），解决了现有方法的浅层连接和高计算成本问题。


<details>
  <summary>Details</summary>
Motivation: 现有扩散统一模型存在两种局限：一是仅利用MLLM的最终隐藏状态作为生成条件，导致浅层连接；二是从头训练统一生成架构计算成本过高。

Method: 利用MLLM多个不同层的表示作为扩散模型的生成条件，将预训练生成器视为“梯子”，接收MLLM理解过程中多层次的指导。

Result: TBAC-UniImage实现了更深层次、更细粒度的理解与生成统一。

Conclusion: 该方法为多模态理解与生成提供了一种高效且深入的统一范式。

Abstract: This paper introduces TBAC-UniImage, a novel unified model for multimodal understanding and generation. We achieve this by deeply integrating a pre-trained Diffusion Model, acting as a generative ladder, with a Multimodal Large Language Model (MLLM). Previous diffusion-based unified models face two primary limitations. One approach uses only the MLLM's final hidden state as the generative condition. This creates a shallow connection, as the generator is isolated from the rich, hierarchical representations within the MLLM's intermediate layers. The other approach, pretraining a unified generative architecture from scratch, is computationally expensive and prohibitive for many researchers. To overcome these issues, our work explores a new paradigm. Instead of relying on a single output, we use representations from multiple, diverse layers of the MLLM as generative conditions for the diffusion model. This method treats the pre-trained generator as a ladder, receiving guidance from various depths of the MLLM's understanding process. Consequently, TBAC-UniImage achieves a much deeper and more fine-grained unification of understanding and generation.

</details>


### [56] [Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control](https://arxiv.org/abs/2508.08134)
*Zeqian Long,Mingzhe Zheng,Kunyu Feng,Xinhua Zhang,Hongyu Liu,Harry Yang,Linfeng Zhang,Qifeng Chen,Yue Ma*

Main category: cs.CV

TL;DR: 论文提出了一种无需训练和掩码的框架Follow-Your-Shape，通过轨迹差异图（TDM）和KV注入机制，实现精确可控的形状编辑，并保持非目标内容不变。


<details>
  <summary>Details</summary>
Motivation: 现有流式图像编辑模型在大规模形状变换任务中表现不佳，容易影响非目标区域。

Method: 提出TDM定位可编辑区域，结合KV注入机制实现稳定编辑。

Result: 实验证明该方法在形状替换任务中具有优越的编辑能力和视觉保真度。

Conclusion: Follow-Your-Shape框架在形状编辑任务中表现出色，解决了现有方法的局限性。

Abstract: While recent flow-based image editing models demonstrate general-purpose capabilities across diverse tasks, they often struggle to specialize in challenging scenarios -- particularly those involving large-scale shape transformations. When performing such structural edits, these methods either fail to achieve the intended shape change or inadvertently alter non-target regions, resulting in degraded background quality. We propose Follow-Your-Shape, a training-free and mask-free framework that supports precise and controllable editing of object shapes while strictly preserving non-target content. Motivated by the divergence between inversion and editing trajectories, we compute a Trajectory Divergence Map (TDM) by comparing token-wise velocity differences between the inversion and denoising paths. The TDM enables precise localization of editable regions and guides a Scheduled KV Injection mechanism that ensures stable and faithful editing. To facilitate a rigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120 new images and enriched prompt pairs specifically curated for shape-aware editing. Experiments demonstrate that our method achieves superior editability and visual fidelity, particularly in tasks requiring large-scale shape replacement.

</details>


### [57] [FantasyStyle: Controllable Stylized Distillation for 3D Gaussian Splatting](https://arxiv.org/abs/2508.08136)
*Yitong Yang,Yinglin Wang,Changshuo Wang,Huajie Wang,Shuting He*

Main category: cs.CV

TL;DR: FantasyStyle是一个基于3DGS的风格迁移框架，首次完全依赖扩散模型蒸馏，解决了多视角不一致性和VGG特征依赖问题，提升了风格迁移质量和视觉真实感。


<details>
  <summary>Details</summary>
Motivation: 当前3DGS风格迁移方法存在多视角不一致导致风格冲突，以及VGG特征难以分离风格与内容的问题。

Method: 提出Multi-View Frequency Consistency增强跨视角一致性，Controllable Stylized Distillation抑制内容泄漏，并优化了3D高斯分布。

Result: 实验表明，该方法在多种场景和风格下均优于现有技术，风格迁移质量和视觉真实感更高。

Conclusion: FantasyStyle通过扩散模型蒸馏和优化策略，显著提升了3D风格迁移的效果。

Abstract: The success of 3DGS in generative and editing applications has sparked growing interest in 3DGS-based style transfer. However, current methods still face two major challenges: (1) multi-view inconsistency often leads to style conflicts, resulting in appearance smoothing and distortion; and (2) heavy reliance on VGG features, which struggle to disentangle style and content from style images, often causing content leakage and excessive stylization. To tackle these issues, we introduce \textbf{FantasyStyle}, a 3DGS-based style transfer framework, and the first to rely entirely on diffusion model distillation. It comprises two key components: (1) \textbf{Multi-View Frequency Consistency}. We enhance cross-view consistency by applying a 3D filter to multi-view noisy latent, selectively reducing low-frequency components to mitigate stylized prior conflicts. (2) \textbf{Controllable Stylized Distillation}. To suppress content leakage from style images, we introduce negative guidance to exclude undesired content. In addition, we identify the limitations of Score Distillation Sampling and Delta Denoising Score in 3D style transfer and remove the reconstruction term accordingly. Building on these insights, we propose a controllable stylized distillation that leverages negative guidance to more effectively optimize the 3D Gaussians. Extensive experiments demonstrate that our method consistently outperforms state-of-the-art approaches, achieving higher stylization quality and visual realism across various scenes and styles.

</details>


### [58] [CD-TVD: Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data](https://arxiv.org/abs/2508.08173)
*Chongke Bi,Xin Gao,Jiangkang Deng,Guan*

Main category: cs.CV

TL;DR: CD-TVD是一个结合对比学习和改进扩散模型的框架，用于从有限的高分辨率数据中实现3D超分辨率，减少对大规模数据集的依赖。


<details>
  <summary>Details</summary>
Motivation: 大规模科学模拟生成高分辨率时变数据成本高，现有超分辨率方法依赖大量高分辨率训练数据，限制了其适用性。

Method: 结合对比学习和改进的扩散模型，通过预训练学习退化模式和细节特征，仅需一个新生成的高分辨率时间步进行微调。

Result: 在流体和大气模拟数据集上验证，CD-TVD实现了准确且资源高效的3D超分辨率。

Conclusion: CD-TVD显著提升了大规模科学模拟的数据增强能力，代码已开源。

Abstract: Large-scale scientific simulations require significant resources to generate high-resolution time-varying data (TVD). While super-resolution is an efficient post-processing strategy to reduce costs, existing methods rely on a large amount of HR training data, limiting their applicability to diverse simulation scenarios. To address this constraint, we proposed CD-TVD, a novel framework that combines contrastive learning and an improved diffusion-based super-resolution model to achieve accurate 3D super-resolution from limited time-step high-resolution data. During pre-training on historical simulation data, the contrastive encoder and diffusion superresolution modules learn degradation patterns and detailed features of high-resolution and low-resolution samples. In the training phase, the improved diffusion model with a local attention mechanism is fine-tuned using only one newly generated high-resolution timestep, leveraging the degradation knowledge learned by the encoder. This design minimizes the reliance on large-scale high-resolution datasets while maintaining the capability to recover fine-grained details. Experimental results on fluid and atmospheric simulation datasets confirm that CD-TVD delivers accurate and resource-efficient 3D super-resolution, marking a significant advancement in data augmentation for large-scale scientific simulations. The code is available at https://github.com/Xin-Gao-private/CD-TVD.

</details>


### [59] [SAGOnline: Segment Any Gaussians Online](https://arxiv.org/abs/2508.08219)
*Wentao Sun,Quanyun Wu,Hanqing Xu,Kyle Gao,Zhengsen Xu,Yiping Chen,Dedong Zhang,Lingfei Ma,John S. Zelek,Jonathan Li*

Main category: cs.CV

TL;DR: SAGOnline是一个轻量级、零样本的实时3D分割框架，通过解耦策略和GPU加速算法，解决了3D高斯场景分割中的计算成本高和空间推理限制问题。


<details>
  <summary>Details</summary>
Motivation: 当前3D高斯场景分割方法存在计算成本高、空间推理能力有限以及无法同时跟踪多对象的问题，需要一种高效且一致的解决方案。

Method: SAGOnline结合视频基础模型（如SAM2）实现视图一致的2D掩码传播，并通过GPU加速的3D掩码生成和高斯级实例标注算法，为3D基元分配唯一标识符。

Result: 在NVOS和Spin-NeRF基准测试中，SAGOnline分别达到92.7%和95.2%的mIoU，推理速度比现有方法快15-1500倍（27毫秒/帧）。

Conclusion: SAGOnline为高斯场景提供了实时3D分割和跟踪能力，推动了AR/VR和机器人应用的发展。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for explicit 3D scene representation, yet achieving efficient and consistent 3D segmentation remains challenging. Current methods suffer from prohibitive computational costs, limited 3D spatial reasoning, and an inability to track multiple objects simultaneously. We present Segment Any Gaussians Online (SAGOnline), a lightweight and zero-shot framework for real-time 3D segmentation in Gaussian scenes that addresses these limitations through two key innovations: (1) a decoupled strategy that integrates video foundation models (e.g., SAM2) for view-consistent 2D mask propagation across synthesized views; and (2) a GPU-accelerated 3D mask generation and Gaussian-level instance labeling algorithm that assigns unique identifiers to 3D primitives, enabling lossless multi-object tracking and segmentation across views. SAGOnline achieves state-of-the-art performance on NVOS (92.7% mIoU) and Spin-NeRF (95.2% mIoU) benchmarks, outperforming Feature3DGS, OmniSeg3D-gs, and SA3D by 15--1500 times in inference speed (27 ms/frame). Qualitative results demonstrate robust multi-object segmentation and tracking in complex scenes. Our contributions include: (i) a lightweight and zero-shot framework for 3D segmentation in Gaussian scenes, (ii) explicit labeling of Gaussian primitives enabling simultaneous segmentation and tracking, and (iii) the effective adaptation of 2D video foundation models to the 3D domain. This work allows real-time rendering and 3D scene understanding, paving the way for practical AR/VR and robotic applications.

</details>


### [60] [Learning User Preferences for Image Generation Model](https://arxiv.org/abs/2508.08220)
*Wenyi Mo,Ying Ba,Tianyu Zhang,Yalong Bai,Biye Li*

Main category: cs.CV

TL;DR: 论文提出了一种基于多模态大语言模型的方法，通过对比偏好损失和偏好标记来学习个性化用户偏好，显著提升了偏好预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖通用人类偏好或静态用户画像，忽略了用户偏好的动态性和多样性。

Method: 采用多模态大语言模型，引入对比偏好损失和可学习的偏好标记，以区分用户喜好并捕捉共享兴趣表示。

Result: 实验表明，该方法在偏好预测准确性上优于其他方法，并能有效识别具有相似审美倾向的用户。

Conclusion: 该方法能更精确地生成符合个人口味的图像，为个性化推荐提供了更有效的指导。

Abstract: User preference prediction requires a comprehensive and accurate understanding of individual tastes. This includes both surface-level attributes, such as color and style, and deeper content-related aspects, such as themes and composition. However, existing methods typically rely on general human preferences or assume static user profiles, often neglecting individual variability and the dynamic, multifaceted nature of personal taste. To address these limitations, we propose an approach built upon Multimodal Large Language Models, introducing contrastive preference loss and preference tokens to learn personalized user preferences from historical interactions. The contrastive preference loss is designed to effectively distinguish between user ''likes'' and ''dislikes'', while the learnable preference tokens capture shared interest representations among existing users, enabling the model to activate group-specific preferences and enhance consistency across similar users. Extensive experiments demonstrate our model outperforms other methods in preference prediction accuracy, effectively identifying users with similar aesthetic inclinations and providing more precise guidance for generating images that align with individual tastes. The project page is \texttt{https://learn-user-pref.github.io/}.

</details>


### [61] [OMGSR: You Only Need One Mid-timestep Guidance for Real-World Image Super-Resolution](https://arxiv.org/abs/2508.08227)
*Zhiqiang Wu,Zhaomang Sun,Tong Zhou,Bingtao Fu,Ji Cong,Yitong Dong,Huaqi Zhang,Xuan Tang,Mingsong Chen,Xian Wei*

Main category: cs.CV

TL;DR: OMGSR是一种基于DDPM/FM生成模型的通用框架，通过在中时间步注入低质量图像潜在分布，并引入潜在分布细化损失和重叠分块LPIPS/GAN损失，显著提升了真实世界图像超分辨率（Real-ISR）的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的一步Real-ISR模型在初始时间步注入低质量图像潜在分布，但其与高斯噪声潜在分布存在差距，限制了生成先验的有效利用。研究发现中时间步的噪声潜在分布与低质量图像潜在分布更接近。

Method: 提出OMGSR框架，在中时间步注入低质量图像潜在分布，引入潜在分布细化损失以减少分布差距，并设计重叠分块LPIPS/GAN损失消除棋盘伪影。基于DDPM/FM模型实现两种变体：OMGSR-S和OMGSR-F。

Result: 实验表明，OMGSR-S/F在512分辨率下表现均衡且优秀，OMGSR-F在所有参考指标中占据绝对优势。1k分辨率的OMGSR-F在细节生成上表现优异，并通过两阶段Tiled VAE & Diffusion生成2k分辨率图像。

Conclusion: OMGSR框架通过优化潜在分布注入时间和损失函数设计，显著提升了一步Real-ISR的性能，为高分辨率图像生成提供了有效解决方案。

Abstract: Denoising Diffusion Probabilistic Models (DDPM) and Flow Matching (FM) generative models show promising potential for one-step Real-World Image Super-Resolution (Real-ISR). Recent one-step Real-ISR models typically inject a Low-Quality (LQ) image latent distribution at the initial timestep. However, a fundamental gap exists between the LQ image latent distribution and the Gaussian noisy latent distribution, limiting the effective utilization of generative priors. We observe that the noisy latent distribution at DDPM/FM mid-timesteps aligns more closely with the LQ image latent distribution. Based on this insight, we present One Mid-timestep Guidance Real-ISR (OMGSR), a universal framework applicable to DDPM/FM-based generative models. OMGSR injects the LQ image latent distribution at a pre-computed mid-timestep, incorporating the proposed Latent Distribution Refinement loss to alleviate the latent distribution gap. We also design the Overlap-Chunked LPIPS/GAN loss to eliminate checkerboard artifacts in image generation. Within this framework, we instantiate OMGSR for DDPM/FM-based generative models with two variants: OMGSR-S (SD-Turbo) and OMGSR-F (FLUX.1-dev). Experimental results demonstrate that OMGSR-S/F achieves balanced/excellent performance across quantitative and qualitative metrics at 512-resolution. Notably, OMGSR-F establishes overwhelming dominance in all reference metrics. We further train a 1k-resolution OMGSR-F to match the default resolution of FLUX.1-dev, which yields excellent results, especially in the details of the image generation. We also generate 2k-resolution images by the 1k-resolution OMGSR-F using our two-stage Tiled VAE & Diffusion.

</details>


### [62] [StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation](https://arxiv.org/abs/2508.08248)
*Shuyuan Tu,Yueming Pan,Yinming Huang,Xintong Han,Zhen Xing,Qi Dai,Chong Luo,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: StableAvatar是一种端到端的视频扩散变换器，能够生成无限长度的高质量视频，解决了现有模型在音频同步和身份一致性上的问题。


<details>
  <summary>Details</summary>
Motivation: 当前音频驱动的虚拟化身视频生成模型难以生成长视频，且音频同步和身份一致性表现不佳。

Method: StableAvatar通过时间步感知音频适配器和音频原生引导机制优化音频建模，并采用动态加权滑动窗口策略增强视频平滑度。

Result: 实验证明StableAvatar在质量和数量上均优于现有方法。

Conclusion: StableAvatar为无限长度视频生成提供了高效解决方案，显著提升了音频同步和身份一致性。

Abstract: Current diffusion models for audio-driven avatar video generation struggle to synthesize long videos with natural audio synchronization and identity consistency. This paper presents StableAvatar, the first end-to-end video diffusion transformer that synthesizes infinite-length high-quality videos without post-processing. Conditioned on a reference image and audio, StableAvatar integrates tailored training and inference modules to enable infinite-length video generation. We observe that the main reason preventing existing models from generating long videos lies in their audio modeling. They typically rely on third-party off-the-shelf extractors to obtain audio embeddings, which are then directly injected into the diffusion model via cross-attention. Since current diffusion backbones lack any audio-related priors, this approach causes severe latent distribution error accumulation across video clips, leading the latent distribution of subsequent segments to drift away from the optimal distribution gradually. To address this, StableAvatar introduces a novel Time-step-aware Audio Adapter that prevents error accumulation via time-step-aware modulation. During inference, we propose a novel Audio Native Guidance Mechanism to further enhance the audio synchronization by leveraging the diffusion's own evolving joint audio-latent prediction as a dynamic guidance signal. To enhance the smoothness of the infinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy that fuses latent over time. Experiments on benchmarks show the effectiveness of StableAvatar both qualitatively and quantitatively.

</details>


### [63] [ReferSplat: Referring Segmentation in 3D Gaussian Splatting](https://arxiv.org/abs/2508.08252)
*Shuting He,Guangquan Jie,Changshuo Wang,Yun Zhou,Shuming Hu,Guanbin Li,Henghui Ding*

Main category: cs.CV

TL;DR: 论文提出了一种新任务R3DGS，旨在通过自然语言描述在3D高斯场景中分割目标对象，并构建了首个数据集Ref-LERF。提出的ReferSplat框架在R3DGS任务和3D开放词汇分割基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 推动具身AI的发展，解决3D多模态理解和空间关系建模的挑战。

Method: 提出ReferSplat框架，显式建模3D高斯点与自然语言表达的空间感知范式。

Result: ReferSplat在R3DGS任务和3D开放词汇分割基准上达到最先进性能。

Conclusion: R3DGS任务和ReferSplat框架为3D多模态理解提供了重要进展，数据集和代码已开源。

Abstract: We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task that aims to segment target objects in a 3D Gaussian scene based on natural language descriptions, which often contain spatial relationships or object attributes. This task requires the model to identify newly described objects that may be occluded or not directly visible in a novel view, posing a significant challenge for 3D multi-modal understanding. Developing this capability is crucial for advancing embodied AI. To support research in this area, we construct the first R3DGS dataset, Ref-LERF. Our analysis reveals that 3D multi-modal understanding and spatial relationship modeling are key challenges for R3DGS. To address these challenges, we propose ReferSplat, a framework that explicitly models 3D Gaussian points with natural language expressions in a spatially aware paradigm. ReferSplat achieves state-of-the-art performance on both the newly proposed R3DGS task and 3D open-vocabulary segmentation benchmarks. Dataset and code are available at https://github.com/heshuting555/ReferSplat.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [64] [Fading the Digital Ink: A Universal Black-Box Attack Framework for 3DGS Watermarking Systems](https://arxiv.org/abs/2508.07263)
*Qingyuan Zeng,Shu Jiang,Jiajing Lin,Zhenzhong Wang,Kay Chen Tan,Min Jiang*

Main category: cs.CR

TL;DR: 本文提出了一种名为GMEA的黑盒攻击框架，用于挑战3D高斯泼溅（3DGS）中的数字水印技术，通过多目标优化平衡水印移除与视觉质量。


<details>
  <summary>Details</summary>
Motivation: 随着3DGS的兴起，数字水印技术被用于版权保护，但其对抗攻击的鲁棒性尚未充分研究。

Method: 提出GMEA框架，将攻击建模为大规模多目标优化问题，采用间接目标函数和分组优化策略。

Result: 实验表明，GMEA能有效移除主流3DGS水印方法中的1D和2D水印，同时保持高视觉保真度。

Conclusion: 揭示了现有3DGS版权保护方案的脆弱性，呼吁开发更鲁棒的水印系统。

Abstract: With the rise of 3D Gaussian Splatting (3DGS), a variety of digital watermarking techniques, embedding either 1D bitstreams or 2D images, are used for copyright protection. However, the robustness of these watermarking techniques against potential attacks remains underexplored. This paper introduces the first universal black-box attack framework, the Group-based Multi-objective Evolutionary Attack (GMEA), designed to challenge these watermarking systems. We formulate the attack as a large-scale multi-objective optimization problem, balancing watermark removal with visual quality. In a black-box setting, we introduce an indirect objective function that blinds the watermark detector by minimizing the standard deviation of features extracted by a convolutional network, thus rendering the feature maps uninformative. To manage the vast search space of 3DGS models, we employ a group-based optimization strategy to partition the model into multiple, independent sub-optimization problems. Experiments demonstrate that our framework effectively removes both 1D and 2D watermarks from mainstream 3DGS watermarking methods while maintaining high visual fidelity. This work reveals critical vulnerabilities in existing 3DGS copyright protection schemes and calls for the development of more robust watermarking systems.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [65] [Codebook-enabled Generative End-to-end Semantic Communication Powered by Transformer](https://arxiv.org/abs/2402.16868)
*Peigen Ye,Yaping Sun,Shumin Yao,Hao Chen,Xiaodong Xu,Shuguang Cui*

Main category: cs.IT

TL;DR: 提出了一种基于高质量码本的鲁棒图像语义通信系统，通过联合构建语义编解码器和码本，设计向量到索引的转换器以减少信道噪声影响，生成图像质量优于对比方法。


<details>
  <summary>Details</summary>
Motivation: 码本生成语义通信系统中，码向量语义关系与索引距离无关，导致系统性能易受信道噪声影响，需提升系统鲁棒性。

Method: 联合构建语义编解码器和码本，设计向量到索引转换器以减少噪声影响，实现图像生成。

Result: 接收端生成的图像在视觉感知上优于对比方法（JPEG+LDPC和传统JSCC）。

Conclusion: 所提出的生成语义通信方法在数值结果和生成图像质量上优于传统方法。

Abstract: Codebook-based generative semantic communication attracts increasing attention, since only indices are required to be transmitted when the codebook is shared between transmitter and receiver. However, due to the fact that the semantic relations among code vectors are not necessarily related to the distance of the corresponding code indices, the performance of the codebook-enabled semantic communication system is susceptible to the channel noise. Thus, how to improve the system robustness against the noise requires careful design. This paper proposes a robust codebook-assisted image semantic communication system, where semantic codec and codebook are first jointly constructed, and then vector-to-index transformer is designed guided by the codebook to eliminate the effects of channel noise, and achieve image generation. Thanks to the assistance of the high-quality codebook to the Transformer, the generated images at the receiver outperform those of the compared methods in terms of visual perception. In the end, numerical results and generated images demonstrate the advantages of the generative semantic communication method over JPEG+LDPC and traditional joint source channel coding (JSCC) methods.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [66] [Sea-Undistort: A Dataset for Through-Water Image Restoration in High Resolution Airborne Bathymetric Mapping](https://arxiv.org/abs/2508.07760)
*Maximilian Kromer,Panagiotis Agrafiotis,Begüm Demir*

Main category: eess.IV

TL;DR: Sea-Undistort是一个合成的浅水图像数据集，用于训练图像恢复模型，解决了真实环境中难以获取无失真数据的问题。


<details>
  <summary>Details</summary>
Motivation: 浅水区基于图像的测深映射因水面动态、水质和光照引起的复杂光学失真（如波浪、散射和太阳耀斑）而具有挑战性。

Method: 使用Blender渲染1200对512x512的无失真和失真图像，包含太阳耀斑、波浪和散射等效果，并提供元数据。基于此数据集，评估了两种先进图像恢复方法和一种改进的轻量级扩散模型。

Result: 改进的扩散模型在真实航空数据中表现更优，生成更完整的海底数字表面模型，减少测深误差，抑制耀斑和散射，并恢复精细海底细节。

Conclusion: Sea-Undistort数据集为浅水图像恢复提供了有效的训练资源，改进的扩散模型在实际应用中表现出色。

Abstract: Accurate image-based bathymetric mapping in shallow waters remains challenging due to the complex optical distortions such as wave induced patterns, scattering and sunglint, introduced by the dynamic water surface, the water column properties, and solar illumination. In this work, we introduce Sea-Undistort, a comprehensive synthetic dataset of 1200 paired 512x512 through-water scenes rendered in Blender. Each pair comprises a distortion-free and a distorted view, featuring realistic water effects such as sun glint, waves, and scattering over diverse seabeds. Accompanied by per-image metadata such as camera parameters, sun position, and average depth, Sea-Undistort enables supervised training that is otherwise infeasible in real environments. We use Sea-Undistort to benchmark two state-of-the-art image restoration methods alongside an enhanced lightweight diffusion-based framework with an early-fusion sun-glint mask. When applied to real aerial data, the enhanced diffusion model delivers more complete Digital Surface Models (DSMs) of the seabed, especially in deeper areas, reduces bathymetric errors, suppresses glint and scattering, and crisply restores fine seabed details. Dataset, weights, and code are publicly available at https://www.magicbathy.eu/Sea-Undistort.html.

</details>


### [67] [Spatio-Temporal Conditional Diffusion Models for Forecasting Future Multiple Sclerosis Lesion Masks Conditioned on Treatments](https://arxiv.org/abs/2508.07006)
*Gian Mario Favero,Ge Ya Luo,Nima Fathi,Justin Szeto,Douglas L. Arnold,Brennan Nichyporuk,Chris Pal,Tal Arbel*

Main category: eess.IV

TL;DR: 本文提出了一种基于图像的治疗感知时空扩散模型，用于预测多发性硬化症（MS）患者的未来病变演变。


<details>
  <summary>Details</summary>
Motivation: 多发性硬化症（MS）的异质性进展需要个性化医疗，而基于图像的预测模型可以为此提供支持。

Method: 采用体素空间方法，结合多模态患者数据（如MRI和治疗信息），预测未来时间点的T2病变（NET2）演变。

Result: 在2131名患者的3D MRI数据集上验证，模型能准确预测六种不同治疗方案的NET2病变，并展示出临床应用潜力。

Conclusion: 该研究展示了基于因果关系的图像生成模型在MS数据驱动预后中的强大潜力。

Abstract: Image-based personalized medicine has the potential to transform healthcare, particularly for diseases that exhibit heterogeneous progression such as Multiple Sclerosis (MS). In this work, we introduce the first treatment-aware spatio-temporal diffusion model that is able to generate future masks demonstrating lesion evolution in MS. Our voxel-space approach incorporates multi-modal patient data, including MRI and treatment information, to forecast new and enlarging T2 (NET2) lesion masks at a future time point. Extensive experiments on a multi-centre dataset of 2131 patient 3D MRIs from randomized clinical trials for relapsing-remitting MS demonstrate that our generative model is able to accurately predict NET2 lesion masks for patients across six different treatments. Moreover, we demonstrate our model has the potential for real-world clinical applications through downstream tasks such as future lesion count and location estimation, binary lesion activity classification, and generating counterfactual future NET2 masks for several treatments with different efficacies. This work highlights the potential of causal, image-based generative models as powerful tools for advancing data-driven prognostics in MS.

</details>


### [68] [Trustworthy Medical Imaging with Large Language Models: A Study of Hallucinations Across Modalities](https://arxiv.org/abs/2508.07031)
*Anindya Bijoy Das,Shahnewaz Karim Sakib,Shibbir Ahmed*

Main category: eess.IV

TL;DR: 研究探讨了大型语言模型（LLMs）在医学影像任务中的幻觉问题，分析了图像到文本和文本到图像两种方向中的错误模式，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: LLMs在医学影像应用中常产生误导性幻觉，影响临床决策，需系统性研究其错误模式和原因。

Method: 研究通过专家标准评估LLMs在图像到文本（生成报告）和文本到图像（生成影像）任务中的幻觉，分析错误类型。

Result: 发现两种任务中均存在常见幻觉模式，如事实不一致和解剖学错误，揭示了模型架构和训练数据的影响。

Conclusion: 研究为提升LLM驱动的医学影像系统的安全性和可信度提供了见解。

Abstract: Large Language Models (LLMs) are increasingly applied to medical imaging tasks, including image interpretation and synthetic image generation. However, these models often produce hallucinations, which are confident but incorrect outputs that can mislead clinical decisions. This study examines hallucinations in two directions: image to text, where LLMs generate reports from X-ray, CT, or MRI scans, and text to image, where models create medical images from clinical prompts. We analyze errors such as factual inconsistencies and anatomical inaccuracies, evaluating outputs using expert informed criteria across imaging modalities. Our findings reveal common patterns of hallucination in both interpretive and generative tasks, with implications for clinical reliability. We also discuss factors contributing to these failures, including model architecture and training data. By systematically studying both image understanding and generation, this work provides insights into improving the safety and trustworthiness of LLM driven medical imaging systems.

</details>


### [69] [3DGS-VBench: A Comprehensive Video Quality Evaluation Benchmark for 3DGS Compression](https://arxiv.org/abs/2508.07038)
*Yuke Xing,William Gordon,Qi Yang,Kaifa Yang,Jiarui Wang,Yiling Xu*

Main category: eess.IV

TL;DR: 3DGS-VBench是一个用于评估3D高斯泼溅（3DGS）压缩算法视觉质量的大规模数据集和基准测试，包含660个压缩模型和视频序列，并提供了MOS评分和多种质量评估指标的对比。


<details>
  <summary>Details</summary>
Motivation: 3DGS的高存储需求限制了其实际应用，而现有的压缩技术缺乏系统性的视觉质量评估研究。

Method: 建立了3DGS-VBench数据集，包含11个场景的660个压缩3DGS模型和视频序列，由50名参与者标注MOS评分，并验证了数据集的可靠性。

Result: 评估了6种SOTA 3DGS压缩算法的存储效率和视觉质量，并对比了15种质量评估指标。

Conclusion: 该工作为3DGS的压缩和质量评估研究提供了专用工具，推动了相关领域的发展。

Abstract: 3D Gaussian Splatting (3DGS) enables real-time novel view synthesis with high visual fidelity, but its substantial storage requirements hinder practical deployment, prompting state-of-the-art (SOTA) 3DGS methods to incorporate compression modules. However, these 3DGS generative compression techniques introduce unique distortions lacking systematic quality assessment research. To this end, we establish 3DGS-VBench, a large-scale Video Quality Assessment (VQA) Dataset and Benchmark with 660 compressed 3DGS models and video sequences generated from 11 scenes across 6 SOTA 3DGS compression algorithms with systematically designed parameter levels. With annotations from 50 participants, we obtained MOS scores with outlier removal and validated dataset reliability. We benchmark 6 3DGS compression algorithms on storage efficiency and visual quality, and evaluate 15 quality assessment metrics across multiple paradigms. Our work enables specialized VQA model training for 3DGS, serving as a catalyst for compression and quality assessment research. The dataset is available at https://github.com/YukeXing/3DGS-VBench.

</details>


### [70] [DiffVC-OSD: One-Step Diffusion-based Perceptual Neural Video Compression Framework](https://arxiv.org/abs/2508.07682)
*Wenzhuo Ma,Zhenzhong Chen*

Main category: eess.IV

TL;DR: DiffVC-OSD是一种基于单步扩散的感知神经视频压缩框架，通过直接输入重建的潜在表示到单步扩散模型中，显著提升了感知质量和解码速度。


<details>
  <summary>Details</summary>
Motivation: 传统多步扩散方法效率较低，DiffVC-OSD旨在通过单步扩散模型提升视频压缩的感知质量和效率。

Method: 设计了Temporal Context Adapter以利用时间依赖性，并通过End-to-End Finetuning策略优化压缩性能。

Result: 实验表明，DiffVC-OSD在感知压缩性能上达到最优，解码速度快20倍，比特率降低86.92%。

Conclusion: DiffVC-OSD通过单步扩散模型显著提升了视频压缩的效率和质量，具有实际应用潜力。

Abstract: In this work, we first propose DiffVC-OSD, a One-Step Diffusion-based Perceptual Neural Video Compression framework. Unlike conventional multi-step diffusion-based methods, DiffVC-OSD feeds the reconstructed latent representation directly into a One-Step Diffusion Model, enhancing perceptual quality through a single diffusion step guided by both temporal context and the latent itself. To better leverage temporal dependencies, we design a Temporal Context Adapter that encodes conditional inputs into multi-level features, offering more fine-grained guidance for the Denoising Unet. Additionally, we employ an End-to-End Finetuning strategy to improve overall compression performance. Extensive experiments demonstrate that DiffVC-OSD achieves state-of-the-art perceptual compression performance, offers about 20$\times$ faster decoding and a 86.92\% bitrate reduction compared to the corresponding multi-step diffusion-based variant.

</details>


### [71] [Anatomy-Aware Low-Dose CT Denoising via Pretrained Vision Models and Semantic-Guided Contrastive Learning](https://arxiv.org/abs/2508.07788)
*Runze Wang,Zeli Chen,Zhiyun Song,Wei Fang,Jiajin Zhang,Danyang Tu,Yuxing Tang,Minfeng Xu,Xianghua Ye,Le Lu,Dakai Jin*

Main category: eess.IV

TL;DR: ALDEN是一种基于深度学习的低剂量CT去噪方法，通过结合预训练视觉模型的语义特征与对抗和对比学习，显著提升了去噪效果并保留了组织解剖结构。


<details>
  <summary>Details</summary>
Motivation: 现有低剂量CT去噪方法忽视了组织解剖语义，可能导致去噪效果不佳。ALDEN旨在通过引入解剖感知机制解决这一问题。

Method: ALDEN结合预训练视觉模型的语义特征，使用解剖感知判别器和语义引导的对比学习模块，动态评估组织特异性真实性并保持解剖一致性。

Result: 在两个低剂量CT去噪数据集上，ALDEN实现了最优性能，显著减少了过平滑问题，并在多器官分割任务中验证了其解剖感知能力。

Conclusion: ALDEN通过解剖感知机制显著提升了低剂量CT去噪效果，为医学影像分析提供了更可靠的解决方案。

Abstract: To reduce radiation exposure and improve the diagnostic efficacy of low-dose computed tomography (LDCT), numerous deep learning-based denoising methods have been developed to mitigate noise and artifacts. However, most of these approaches ignore the anatomical semantics of human tissues, which may potentially result in suboptimal denoising outcomes. To address this problem, we propose ALDEN, an anatomy-aware LDCT denoising method that integrates semantic features of pretrained vision models (PVMs) with adversarial and contrastive learning. Specifically, we introduce an anatomy-aware discriminator that dynamically fuses hierarchical semantic features from reference normal-dose CT (NDCT) via cross-attention mechanisms, enabling tissue-specific realism evaluation in the discriminator. In addition, we propose a semantic-guided contrastive learning module that enforces anatomical consistency by contrasting PVM-derived features from LDCT, denoised CT and NDCT, preserving tissue-specific patterns through positive pairs and suppressing artifacts via dual negative pairs. Extensive experiments conducted on two LDCT denoising datasets reveal that ALDEN achieves the state-of-the-art performance, offering superior anatomy preservation and substantially reducing over-smoothing issue of previous work. Further validation on a downstream multi-organ segmentation task (encompassing 117 anatomical structures) affirms the model's ability to maintain anatomical awareness.

</details>


### [72] [Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models](https://arxiv.org/abs/2508.07903)
*Johanna P. Müller,Anika Knupfer,Pedro Blöss,Edoardo Berardi Vittur,Bernhard Kainz,Jana Hutter*

Main category: eess.IV

TL;DR: 提出了一种新型扩散模型框架，用于生成高保真度的女性盆腔MRI图像，解决了现有模型在解剖学精确性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在生成女性盆腔图像时解剖学精确性不足，限制了其在妇科影像中的应用。数据稀缺和患者隐私问题也是关键挑战。

Method: 结合无条件与条件化Denoising Diffusion Probabilistic Models (DDPMs)和Latent Diffusion Models (LDMs)的2D和3D框架，生成解剖学一致的高保真合成图像。

Result: 生成的图像在感知和分布指标上表现优异，显著提升了诊断任务的准确性。专家盲评验证了其临床真实性。

Conclusion: 该框架为妇科影像提供了高质量的合成数据资源，支持可重复研究和公平AI发展。

Abstract: Despite significant progress in generative modelling, existing diffusion models often struggle to produce anatomically precise female pelvic images, limiting their application in gynaecological imaging, where data scarcity and patient privacy concerns are critical. To overcome these barriers, we introduce a novel diffusion-based framework for uterine MRI synthesis, integrating both unconditional and conditioned Denoising Diffusion Probabilistic Models (DDPMs) and Latent Diffusion Models (LDMs) in 2D and 3D. Our approach generates anatomically coherent, high fidelity synthetic images that closely mimic real scans and provide valuable resources for training robust diagnostic models. We evaluate generative quality using advanced perceptual and distributional metrics, benchmarking against standard reconstruction methods, and demonstrate substantial gains in diagnostic accuracy on a key classification task. A blinded expert evaluation further validates the clinical realism of our synthetic images. We release our models with privacy safeguards and a comprehensive synthetic uterine MRI dataset to support reproducible research and advance equitable AI in gynaecology.

</details>


### [73] [A Physics-Driven Neural Network with Parameter Embedding for Generating Quantitative MR Maps from Weighted Images](https://arxiv.org/abs/2508.08123)
*Lingjing Chen,Chengxiu Zhang,Yinqiao Yi,Yida Wang,Yang Song,Xu Yan,Shengfang Xu,Dalin Zhu,Mengqiu Cao,Yan Zhou,Chenglong Wang,Guang Yang*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的MRI序列参数整合方法，通过参数嵌入提升定量图像合成的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 提高临床加权MRI定量图像合成的准确性和泛化性，以加速qMRI并提升其临床应用价值。

Method: 采用物理驱动的神经网络，将MRI序列参数（TR、TE、TI）通过参数嵌入直接整合到模型中，输入T1加权、T2加权和T2-FLAIR图像，合成T1、T2和PD定量图。

Result: 在内部和外部测试数据集上表现优异，PSNR超过34 dB，SSIM高于0.92，优于传统深度学习模型，且对未见过的脑结构和病变区域具有良好泛化能力。

Conclusion: 通过参数嵌入整合MRI序列参数显著提升了定量MRI合成的性能和可靠性，展示了加速qMRI和提升临床实用性的潜力。

Abstract: We propose a deep learning-based approach that integrates MRI sequence parameters to improve the accuracy and generalizability of quantitative image synthesis from clinical weighted MRI. Our physics-driven neural network embeds MRI sequence parameters -- repetition time (TR), echo time (TE), and inversion time (TI) -- directly into the model via parameter embedding, enabling the network to learn the underlying physical principles of MRI signal formation. The model takes conventional T1-weighted, T2-weighted, and T2-FLAIR images as input and synthesizes T1, T2, and proton density (PD) quantitative maps. Trained on healthy brain MR images, it was evaluated on both internal and external test datasets. The proposed method achieved high performance with PSNR values exceeding 34 dB and SSIM values above 0.92 for all synthesized parameter maps. It outperformed conventional deep learning models in accuracy and robustness, including data with previously unseen brain structures and lesions. Notably, our model accurately synthesized quantitative maps for these unseen pathological regions, highlighting its superior generalization capability. Incorporating MRI sequence parameters via parameter embedding allows the neural network to better learn the physical characteristics of MR signals, significantly enhancing the performance and reliability of quantitative MRI synthesis. This method shows great potential for accelerating qMRI and improving its clinical utility.

</details>


### [74] [MIND: A Noise-Adaptive Denoising Framework for Medical Images Integrating Multi-Scale Transformer](https://arxiv.org/abs/2508.07817)
*Tao Tang,Chengxu Yang*

Main category: eess.IV

TL;DR: 本文提出了一种结合多尺度卷积和Transformer架构的医学图像自适应去噪模型（MI-ND），通过噪声水平估计器和噪声自适应注意力模块，显著提升了图像质量和下游诊断任务的性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像因噪声干扰影响诊断准确性，现有方法难以有效解决非均匀噪声问题。

Method: 提出MI-ND模型，集成多尺度卷积和Transformer架构，引入噪声水平估计器（NLE）和噪声自适应注意力模块（NAAB），实现噪声感知驱动的特征融合。

Result: 在PSNR、SSIM等指标上显著优于对比方法，下游诊断任务的F1分数和ROC-AUC也有所提升。

Conclusion: 该模型在结构恢复、诊断敏感性和跨模态鲁棒性方面表现突出，为医学图像增强和AI辅助诊疗提供了有效解决方案。

Abstract: The core role of medical images in disease diagnosis makes their quality directly affect the accuracy of clinical judgment. However, due to factors such as low-dose scanning, equipment limitations and imaging artifacts, medical images are often accompanied by non-uniform noise interference, which seriously affects structure recognition and lesion detection. This paper proposes a medical image adaptive denoising model (MI-ND) that integrates multi-scale convolutional and Transformer architecture, introduces a noise level estimator (NLE) and a noise adaptive attention module (NAAB), and realizes channel-spatial attention regulation and cross-modal feature fusion driven by noise perception. Systematic testing is carried out on multimodal public datasets. Experiments show that this method significantly outperforms the comparative methods in image quality indicators such as PSNR, SSIM, and LPIPS, and improves the F1 score and ROC-AUC in downstream diagnostic tasks, showing strong prac-tical value and promotional potential. The model has outstanding benefits in structural recovery, diagnostic sensitivity, and cross-modal robustness, and provides an effective solution for medical image enhancement and AI-assisted diagnosis and treatment.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [75] [AgriVLN: Vision-and-Language Navigation for Agricultural Robots](https://arxiv.org/abs/2508.07406)
*Xiaobei Zhao,Xingqi Lyu,Xiang Li*

Main category: cs.RO

TL;DR: 提出了农业场景的视觉与语言导航基准A2A和基线方法AgriVLN，通过子任务分解模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决农业机器人导航依赖人工或固定轨道的问题，填补农业场景VLN研究的空白。

Method: 开发A2A基准和AgriVLN基线方法，结合VLM和子任务分解模块STL。

Result: AgriVLN在短指令上表现良好，长指令通过STL提升SR至0.47。

Conclusion: AgriVLN在农业领域表现领先，STL显著提升长指令导航成功率。

Abstract: Agricultural robots have emerged as powerful members in agricultural tasks, nevertheless, still heavily rely on manual operation or untransportable railway for movement, resulting in limited mobility and poor adaptability. Vision-and-Language Navigation (VLN) enables robots to navigate to the target destinations following natural language instructions, demonstrating strong performance on several domains. However, none of the existing benchmarks or methods is specifically designed for agricultural scenes. To bridge this gap, we propose Agriculture to Agriculture (A2A) benchmark, containing 1,560 episodes across six diverse agricultural scenes, in which all realistic RGB videos are captured by front-facing camera on a quadruped robot at a height of 0.38 meters, aligning with the practical deployment conditions. Meanwhile, we propose Vision-and-Language Navigation for Agricultural Robots (AgriVLN) baseline based on Vision-Language Model (VLM) prompted with carefully crafted templates, which can understand both given instructions and agricultural environments to generate appropriate low-level actions for robot control. When evaluated on A2A, AgriVLN performs well on short instructions but struggles with long instructions, because it often fails to track which part of the instruction is currently being executed. To address this, we further propose Subtask List (STL) instruction decomposition module and integrate it into AgriVLN, improving Success Rate (SR) from 0.33 to 0.47. We additionally compare AgriVLN with several existing VLN methods, demonstrating the state-of-the-art performance in the agricultural domain.

</details>
