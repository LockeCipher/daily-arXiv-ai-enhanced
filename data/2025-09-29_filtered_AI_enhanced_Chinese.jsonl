{"id": "2509.21541", "pdf": "https://arxiv.org/pdf/2509.21541", "abs": "https://arxiv.org/abs/2509.21541", "authors": ["Weikai Lin", "Haoxiang Li", "Yuhao Zhu"], "title": "ControlHair: Physically-based Video Diffusion for Controllable Dynamic Hair Rendering", "categories": ["cs.GR", "cs.CV", "I.3; I.2; I.4"], "comment": "9 pages,Project website: https://ctrlhair-arxiv.netlify.app/", "summary": "Hair simulation and rendering are challenging due to complex strand dynamics, diverse material properties, and intricate light-hair interactions. Recent video diffusion models can generate high-quality videos, but they lack fine-grained control over hair dynamics. We present ControlHair, a hybrid framework that integrates a physics simulator with conditional video diffusion to enable controllable dynamic hair rendering. ControlHair adopts a three-stage pipeline: it first encodes physics parameters (e.g., hair stiffness, wind) into per-frame geometry using a simulator, then extracts per-frame control signals, and finally feeds control signals into a video diffusion model to generate videos with desired hair dynamics. This cascaded design decouples physics reasoning from video generation, supports diverse physics, and makes training the video diffusion model easy. Trained on a curated 10K video dataset, ControlHair outperforms text- and pose-conditioned baselines, delivering precisely controlled hair dynamics. We further demonstrate three use cases of ControlHair: dynamic hairstyle try-on, bullet-time effects, and cinemagraphic. ControlHair introduces the first physics-informed video diffusion framework for controllable dynamics. We provide a teaser video and experimental results on our website.", "AI": {"tldr": "ControlHair\u662f\u4e00\u4e2a\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u7269\u7406\u6a21\u62df\u5668\u548c\u6761\u4ef6\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u53ef\u63a7\u7684\u52a8\u6001\u5934\u53d1\u6e32\u67d3\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u7f3a\u4e4f\u5bf9\u5934\u53d1\u52a8\u6001\u7684\u7cbe\u7ec6\u63a7\u5236\uff0c\u800c\u5934\u53d1\u6a21\u62df\u548c\u6e32\u67d3\u9762\u4e34\u590d\u6742\u7684\u52a8\u529b\u5b66\u3001\u6750\u6599\u5c5e\u6027\u548c\u5149\u7167\u4ea4\u4e92\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a1\uff09\u7528\u7269\u7406\u6a21\u62df\u5668\u5c06\u7269\u7406\u53c2\u6570\u7f16\u7801\u4e3a\u9010\u5e27\u51e0\u4f55\uff1b2\uff09\u63d0\u53d6\u9010\u5e27\u63a7\u5236\u4fe1\u53f7\uff1b3\uff09\u5c06\u63a7\u5236\u4fe1\u53f7\u8f93\u5165\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u5177\u6709\u6240\u9700\u5934\u53d1\u52a8\u6001\u7684\u89c6\u9891\u3002", "result": "\u572810K\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0cControlHair\u5728\u5934\u53d1\u52a8\u6001\u63a7\u5236\u65b9\u9762\u4f18\u4e8e\u57fa\u4e8e\u6587\u672c\u548c\u59ff\u6001\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ControlHair\u662f\u9996\u4e2a\u7269\u7406\u4fe1\u606f\u89c6\u9891\u6269\u6563\u6846\u67b6\uff0c\u652f\u6301\u52a8\u6001\u53d1\u578b\u8bd5\u6234\u3001\u5b50\u5f39\u65f6\u95f4\u6548\u679c\u548c\u7535\u5f71\u56fe\u5f62\u7b49\u5e94\u7528\u3002"}}
{"id": "2509.21702", "pdf": "https://arxiv.org/pdf/2509.21702", "abs": "https://arxiv.org/abs/2509.21702", "authors": ["Weikai Lin", "Sushant Kondguli", "Carl Marshall", "Yuhao Zhu"], "title": "PowerGS: Display-Rendering Power Co-Optimization for Neural Rendering in Power-Constrained XR Systems", "categories": ["cs.GR", "I.3; I.4"], "comment": "10 pages, Accepted to Siggraph Asia 2025", "summary": "3D Gaussian Splatting (3DGS) combines classic image-based rendering, pointbased graphics, and modern differentiable techniques, and offers an interesting alternative to traditional physically-based rendering. 3DGS-family models are far from efficient for power-constrained Extended Reality (XR) devices, which need to operate at a Watt-level. This paper introduces PowerGS, the first framework to jointly minimize the rendering and display power in 3DGS under a quality constraint. We present a general problem formulation and show that solving the problem amounts to 1) identifying the iso-quality curve(s) in the landscape subtended by the display and rendering power and 2) identifying the power-minimal point on a given curve, which has a closed-form solution given a proper parameterization of the curves. PowerGS also readily supports foveated rendering for further power savings. Extensive experiments and user studies show that PowerGS achieves up to 86% total power reduction compared to state-of-the-art 3DGS models, with minimal loss in both subjective and objective quality. Code is available at https://github.com/horizon-research/PowerGS.", "AI": {"tldr": "PowerGS\u662f\u4e00\u4e2a\u9488\u5bf93D\u9ad8\u65af\u6cfc\u6e85(3DGS)\u7684\u529f\u8017\u4f18\u5316\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u8d28\u91cf\u7ea6\u675f\u4e0b\u8054\u5408\u6700\u5c0f\u5316\u6e32\u67d3\u548c\u663e\u793a\u529f\u8017\uff0c\u53ef\u5b9e\u73b0\u9ad8\u8fbe86%\u7684\u603b\u529f\u8017\u964d\u4f4e\u3002", "motivation": "3DGS\u7cfb\u5217\u6a21\u578b\u5728\u529f\u8017\u53d7\u9650\u7684XR\u8bbe\u5907\u4e0a\u6548\u7387\u4f4e\u4e0b\uff0c\u8fd9\u4e9b\u8bbe\u5907\u9700\u8981\u5728\u74e6\u7279\u7ea7\u522b\u8fd0\u884c\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u529f\u8017\u4f18\u5316\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u901a\u7528\u95ee\u9898\u516c\u5f0f\u5316\uff0c\u901a\u8fc7\u8bc6\u522b\u663e\u793a\u548c\u6e32\u67d3\u529f\u8017\u666f\u89c2\u4e2d\u7684\u7b49\u8d28\u91cf\u66f2\u7ebf\uff0c\u5e76\u627e\u5230\u7ed9\u5b9a\u66f2\u7ebf\u4e0a\u529f\u8017\u6700\u5c0f\u7684\u70b9\uff08\u5728\u9002\u5f53\u53c2\u6570\u5316\u4e0b\u5177\u6709\u95ed\u5f0f\u89e3\uff09\uff0c\u652f\u6301\u6ce8\u89c6\u70b9\u6e32\u67d3\u4ee5\u8fdb\u4e00\u6b65\u8282\u7701\u529f\u8017\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u548c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cPowerGS\u76f8\u6bd4\u6700\u5148\u8fdb\u76843DGS\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u8fbe86%\u7684\u603b\u529f\u8017\u964d\u4f4e\uff0c\u5728\u4e3b\u89c2\u548c\u5ba2\u89c2\u8d28\u91cf\u4e0a\u635f\u5931\u6700\u5c0f\u3002", "conclusion": "PowerGS\u662f\u9996\u4e2a\u5728\u8d28\u91cf\u7ea6\u675f\u4e0b\u8054\u5408\u4f18\u53163DGS\u6e32\u67d3\u548c\u663e\u793a\u529f\u8017\u7684\u6846\u67b6\uff0c\u4e3a\u529f\u8017\u53d7\u9650\u7684XR\u8bbe\u5907\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22222", "pdf": "https://arxiv.org/pdf/2509.22222", "abs": "https://arxiv.org/abs/2509.22222", "authors": ["Jinhyeok Kim", "Jaehun Bang", "Seunghyun Seo", "Kyungdon Joo"], "title": "Rigidity-Aware 3D Gaussian Deformation from a Single Image", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "10 pages, 11 figures, conference", "summary": "Reconstructing object deformation from a single image remains a significant challenge in computer vision and graphics. Existing methods typically rely on multi-view video to recover deformation, limiting their applicability under constrained scenarios. To address this, we propose DeformSplat, a novel framework that effectively guides 3D Gaussian deformation from only a single image. Our method introduces two main technical contributions. First, we present Gaussian-to-Pixel Matching which bridges the domain gap between 3D Gaussian representations and 2D pixel observations. This enables robust deformation guidance from sparse visual cues. Second, we propose Rigid Part Segmentation consisting of initialization and refinement. This segmentation explicitly identifies rigid regions, crucial for maintaining geometric coherence during deformation. By combining these two techniques, our approach can reconstruct consistent deformations from a single image. Extensive experiments demonstrate that our approach significantly outperforms existing methods and naturally extends to various applications,such as frame interpolation and interactive object manipulation.", "AI": {"tldr": "DeformSplat\uff1a\u4ece\u5355\u5f20\u56fe\u50cf\u91cd\u5efa3D\u9ad8\u65af\u53d8\u5f62\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u65af-\u50cf\u7d20\u5339\u914d\u548c\u521a\u6027\u90e8\u5206\u5206\u5272\u6280\u672f\u5b9e\u73b0", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u591a\u89c6\u89d2\u89c6\u9891\u91cd\u5efa\u53d8\u5f62\uff0c\u9650\u5236\u4e86\u5728\u53d7\u9650\u573a\u666f\u4e0b\u7684\u5e94\u7528\uff0c\u9700\u8981\u4ece\u5355\u5f20\u56fe\u50cf\u6062\u590d\u7269\u4f53\u53d8\u5f62\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51fa\u9ad8\u65af-\u50cf\u7d20\u5339\u914d\u6280\u672f\u5f25\u54083D\u9ad8\u65af\u8868\u793a\u4e0e2D\u50cf\u7d20\u89c2\u6d4b\u7684\u9886\u57df\u5dee\u8ddd\uff1b\u5f00\u53d1\u521a\u6027\u90e8\u5206\u5206\u5272\uff08\u521d\u59cb\u5316\u548c\u7ec6\u5316\uff09\u8bc6\u522b\u521a\u6027\u533a\u57df\u4ee5\u4fdd\u6301\u51e0\u4f55\u4e00\u81f4\u6027", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u81ea\u7136\u6269\u5c55\u5230\u5e27\u63d2\u503c\u548c\u4ea4\u4e92\u5f0f\u7269\u4f53\u64cd\u4f5c\u7b49\u5e94\u7528", "conclusion": "DeformSplat\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u5355\u5f20\u56fe\u50cf\u91cd\u5efa\u4e00\u81f4\u53d8\u5f62\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5bf9\u591a\u89c6\u89d2\u6570\u636e\u7684\u4f9d\u8d56\u95ee\u9898"}}
{"id": "2509.21375", "pdf": "https://arxiv.org/pdf/2509.21375", "abs": "https://arxiv.org/abs/2509.21375", "authors": ["Aleksa Jelaca", "Ying Jiao", "Chang Tian", "Marie-Francine Moens"], "title": "Automated Prompt Generation for Creative and Counterfactual Text-to-image Synthesis", "categories": ["cs.CV", "cs.AI"], "comment": "text-to-image generation, automatic prompt, DPO, Counterfactual", "summary": "Text-to-image generation has advanced rapidly with large-scale multimodal training, yet fine-grained controllability remains a critical challenge. Counterfactual controllability, defined as the capacity to deliberately generate images that contradict common-sense patterns, remains a major challenge but plays a crucial role in enabling creativity and exploratory applications. In this work, we address this gap with a focus on counterfactual size (e.g., generating a tiny walrus beside a giant button) and propose an automatic prompt engineering framework that adapts base prompts into revised prompts for counterfactual images. The framework comprises three components: an image evaluator that guides dataset construction by identifying successful image generations, a supervised prompt rewriter that produces revised prompts, and a DPO-trained ranker that selects the optimal revised prompt. We construct the first counterfactual size text-image dataset and enhance the image evaluator by extending Grounded SAM with refinements, achieving a 114 percent improvement over its backbone. Experiments demonstrate that our method outperforms state-of-the-art baselines and ChatGPT-4o, establishing a foundation for future research on counterfactual controllability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u52a8\u63d0\u793a\u5de5\u7a0b\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u53cd\u4e8b\u5b9e\u5c3a\u5bf8\u7684\u56fe\u50cf\uff08\u5982\u5fae\u5c0f\u7684\u6d77\u8c61\u548c\u5de8\u5927\u7684\u6309\u94ae\uff09\uff0c\u901a\u8fc7\u56fe\u50cf\u8bc4\u4f30\u5668\u3001\u76d1\u7763\u63d0\u793a\u91cd\u5199\u5668\u548cDPO\u8bad\u7ec3\u7684\u6392\u5e8f\u5668\u6765\u4f18\u5316\u63d0\u793a\uff0c\u6784\u5efa\u4e86\u9996\u4e2a\u53cd\u4e8b\u5b9e\u5c3a\u5bf8\u6587\u672c-\u56fe\u50cf\u6570\u636e\u96c6\u3002", "motivation": "\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7ec6\u7c92\u5ea6\u53ef\u63a7\u6027\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u53cd\u4e8b\u5b9e\u53ef\u63a7\u6027\uff08\u751f\u6210\u8fdd\u80cc\u5e38\u8bc6\u6a21\u5f0f\u7684\u56fe\u50cf\uff09\uff0c\u8fd9\u5bf9\u4e8e\u521b\u610f\u548c\u63a2\u7d22\u6027\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f00\u53d1\u4e86\u4e09\u7ec4\u4ef6\u6846\u67b6\uff1a\u56fe\u50cf\u8bc4\u4f30\u5668\u6307\u5bfc\u6570\u636e\u96c6\u6784\u5efa\uff0c\u76d1\u7763\u63d0\u793a\u91cd\u5199\u5668\u751f\u6210\u4fee\u8ba2\u63d0\u793a\uff0cDPO\u8bad\u7ec3\u7684\u6392\u5e8f\u5668\u9009\u62e9\u6700\u4f73\u4fee\u8ba2\u63d0\u793a\u3002\u6269\u5c55\u4e86Grounded SAM\u4ee5\u6539\u8fdb\u56fe\u50cf\u8bc4\u4f30\u5668\u3002", "result": "\u56fe\u50cf\u8bc4\u4f30\u5668\u6027\u80fd\u6bd4\u5176\u9aa8\u5e72\u7f51\u7edc\u63d0\u5347\u4e86114%\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u548cChatGPT-4o\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u53cd\u4e8b\u5b9e\u53ef\u63a7\u6027\u7684\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5728\u53cd\u4e8b\u5b9e\u5c3a\u5bf8\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.21376", "pdf": "https://arxiv.org/pdf/2509.21376", "abs": "https://arxiv.org/abs/2509.21376", "authors": ["Shiraz S Kaderuppan", "Jonathan Mar", "Andrew Irvine", "Anurag Sharma", "Muhammad Ramadan Saifuddin", "Wai Leong Eugene Wong", "Wai Lok Woo"], "title": "In silico Deep Learning Protocols for Label-Free Super-Resolution Microscopy: A Comparative Study of Network Architectures and SNR Dependence", "categories": ["cs.CV", "cs.AI"], "comment": "20 pages, 10 figures", "summary": "The field of optical microscopy spans across numerous industries and research domains, ranging from education to healthcare, quality inspection and analysis. Nonetheless, a key limitation often cited by optical microscopists refers to the limit of its lateral resolution (typically defined as ~200nm), with potential circumventions involving either costly external modules (e.g. confocal scan heads, etc) and/or specialized techniques [e.g. super-resolution (SR) fluorescent microscopy]. Addressing these challenges in a normal (non-specialist) context thus remains an aspect outside the scope of most microscope users & facilities. This study thus seeks to evaluate an alternative & economical approach to achieving SR optical microscopy, involving non-fluorescent phase-modulated microscopical modalities such as Zernike phase contrast (PCM) and differential interference contrast (DIC) microscopy. Two in silico deep neural network (DNN) architectures which we developed previously (termed O-Net and Theta-Net) are assessed on their abilities to resolve a custom-fabricated test target containing nanoscale features calibrated via atomic force microscopy (AFM). The results of our study demonstrate that although both O-Net and Theta-Net seemingly performed well when super-resolving these images, they were complementary (rather than competing) approaches to be considered for image SR, particularly under different image signal-to-noise ratios (SNRs). High image SNRs favoured the application of O-Net models, while low SNRs inclined preferentially towards Theta-Net models. These findings demonstrate the importance of model architectures (in conjunction with the source image SNR) on model performance and the SR quality of the generated images where DNN models are utilized for non-fluorescent optical nanoscopy, even where the same training dataset & number of epochs are being used.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u795e\u7ecf\u7f51\u7edc\uff08O-Net\u548cTheta-Net\uff09\u5bf9\u975e\u8367\u5149\u76f8\u4f4d\u8c03\u5236\u663e\u5fae\u955c\u56fe\u50cf\u8fdb\u884c\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u4e24\u79cd\u7f51\u7edc\u5728\u4e0d\u540c\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u5177\u6709\u4e92\u8865\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5149\u5b66\u663e\u5fae\u955c\u5206\u8fa8\u7387\u9650\u5236\uff08\u7ea6200nm\uff09\u7684\u95ee\u9898\uff0c\u907f\u514d\u4f7f\u7528\u6602\u8d35\u7684\u4e13\u7528\u8bbe\u5907\u6216\u8367\u5149\u8d85\u5206\u8fa8\u7387\u6280\u672f\uff0c\u4e3a\u666e\u901a\u663e\u5fae\u955c\u7528\u6237\u63d0\u4f9b\u7ecf\u6d4e\u53ef\u884c\u7684\u8d85\u5206\u8fa8\u7387\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u81ea\u7814\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff08O-Net\u548cTheta-Net\uff09\uff0c\u5bf9Zernike\u76f8\u886c\u548c\u5fae\u5206\u5e72\u6d89\u76f8\u886c\u663e\u5fae\u955c\u56fe\u50cf\u8fdb\u884c\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\uff0c\u901a\u8fc7\u539f\u5b50\u529b\u663e\u5fae\u955c\u6821\u51c6\u7684\u7eb3\u7c73\u7ea7\u6d4b\u8bd5\u9776\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4e24\u79cd\u7f51\u7edc\u5728\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u4e2d\u8868\u73b0\u826f\u597d\u4f46\u5177\u6709\u4e92\u8865\u6027\uff1a\u9ad8\u4fe1\u566a\u6bd4\u56fe\u50cf\u9002\u5408\u4f7f\u7528O-Net\u6a21\u578b\uff0c\u4f4e\u4fe1\u566a\u6bd4\u56fe\u50cf\u66f4\u9002\u5408Theta-Net\u6a21\u578b\u3002", "conclusion": "\u6a21\u578b\u67b6\u6784\u548c\u6e90\u56fe\u50cf\u4fe1\u566a\u6bd4\u5bf9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u975e\u8367\u5149\u5149\u5b66\u7eb3\u7c73\u663e\u5fae\u955c\u4e2d\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u5373\u4f7f\u4f7f\u7528\u76f8\u540c\u7684\u8bad\u7ec3\u6570\u636e\u548c\u8bad\u7ec3\u5468\u671f\uff0c\u4e0d\u540c\u67b6\u6784\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u8868\u73b0\u5404\u5f02\u3002"}}
{"id": "2509.21377", "pdf": "https://arxiv.org/pdf/2509.21377", "abs": "https://arxiv.org/abs/2509.21377", "authors": ["Yinfeng Yu", "Hailong Zhang", "Meiling Zhu"], "title": "Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation", "categories": ["cs.CV", "cs.AI"], "comment": "Main paper (8 pages). Accepted for publication by ECAI( European   Conference on Artificial Intelligence) 2025", "summary": "Audiovisual embodied navigation enables robots to locate audio sources by dynamically integrating visual observations from onboard sensors with the auditory signals emitted by the target. The core challenge lies in effectively leveraging multimodal cues to guide navigation. While prior works have explored basic fusion of visual and audio data, they often overlook deeper perceptual context. To address this, we propose the Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation (DMTF-AVN). Our approach uses a multi-target architecture coupled with a refined Transformer mechanism to filter and selectively fuse cross-modal information. Extensive experiments on the Replica and Matterport3D datasets demonstrate that DMTF-AVN achieves state-of-the-art performance, outperforming existing methods in success rate (SR), path efficiency (SPL), and scene adaptation (SNA). Furthermore, the model exhibits strong scalability and generalizability, paving the way for advanced multimodal fusion strategies in robotic navigation. The code and videos are available at   https://github.com/zzzmmm-svg/DMTF.", "AI": {"tldr": "\u63d0\u51faDMTF-AVN\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u67b6\u6784\u548cTransformer\u673a\u5236\u52a8\u6001\u878d\u5408\u89c6\u542c\u4fe1\u606f\uff0c\u5728\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd", "motivation": "\u73b0\u6709\u89c6\u542c\u878d\u5408\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u66f4\u6df1\u5c42\u6b21\u7684\u611f\u77e5\u4e0a\u4e0b\u6587\uff0c\u9700\u8981\u66f4\u6709\u6548\u5730\u5229\u7528\u591a\u6a21\u6001\u7ebf\u7d22\u6765\u6307\u5bfc\u5bfc\u822a", "method": "\u4f7f\u7528\u591a\u76ee\u6807\u67b6\u6784\u914d\u5408\u7cbe\u70bc\u7684Transformer\u673a\u5236\uff0c\u8fc7\u6ee4\u548c\u9009\u62e9\u6027\u878d\u5408\u8de8\u6a21\u6001\u4fe1\u606f", "result": "\u5728Replica\u548cMatterport3D\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u6210\u529f\u7387\u3001\u8def\u5f84\u6548\u7387\u548c\u573a\u666f\u9002\u5e94\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u6a21\u578b\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u5148\u8fdb\u591a\u6a21\u6001\u878d\u5408\u7b56\u7565\u94fa\u5e73\u9053\u8def"}}
{"id": "2509.21399", "pdf": "https://arxiv.org/pdf/2509.21399", "abs": "https://arxiv.org/abs/2509.21399", "authors": ["Petr Ko\u0161\u0165\u00e1l", "Pavel Kord\u00edk", "Ond\u0159ej Podsztavek"], "title": "Downscaling climate projections to 1 km with single-image super resolution", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "High-resolution climate projections are essential for local decision-making. However, available climate projections have low spatial resolution (e.g. 12.5 km), which limits their usability. We address this limitation by leveraging single-image super-resolution models to statistically downscale climate projections to 1-km resolution. Since high-resolution climate projections are unavailable for training, we train models on a high-resolution observational gridded data set and apply them to low-resolution climate projections. We propose a climate indicator-based assessment using observed climate indices computed at weather station locations to evaluate the downscaled climate projections without ground-truth high-resolution climate projections. Experiments on daily mean temperature demonstrate that single-image super-resolution models can downscale climate projections without increasing the error of climate indicators compared to low-resolution climate projections.", "AI": {"tldr": "\u4f7f\u7528\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u5c0612.5\u516c\u91cc\u5206\u8fa8\u7387\u7684\u6c14\u5019\u9884\u6d4b\u7edf\u8ba1\u964d\u5c3a\u5ea6\u52301\u516c\u91cc\u5206\u8fa8\u7387\uff0c\u901a\u8fc7\u6c14\u5019\u6307\u6807\u8bc4\u4f30\u65b9\u6cd5\u9a8c\u8bc1\u964d\u5c3a\u5ea6\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6c14\u5019\u9884\u6d4b\u7a7a\u95f4\u5206\u8fa8\u7387\u8f83\u4f4e\uff0812.5\u516c\u91cc\uff09\uff0c\u9650\u5236\u4e86\u5728\u5730\u65b9\u51b3\u7b56\u4e2d\u7684\u53ef\u7528\u6027\uff0c\u9700\u8981\u9ad8\u5206\u8fa8\u7387\u6c14\u5019\u9884\u6d4b\u6765\u652f\u6301\u672c\u5730\u51b3\u7b56\u3002", "method": "\u5728\u89c2\u6d4b\u7f51\u683c\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6a21\u578b\uff0c\u7136\u540e\u5e94\u7528\u4e8e\u4f4e\u5206\u8fa8\u7387\u6c14\u5019\u9884\u6d4b\uff1b\u63d0\u51fa\u57fa\u4e8e\u6c14\u5019\u6307\u6807\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4f7f\u7528\u6c14\u8c61\u7ad9\u89c2\u6d4b\u6570\u636e\u8ba1\u7b97\u6c14\u5019\u6307\u6807\u6765\u8bc4\u4f30\u964d\u5c3a\u5ea6\u7ed3\u679c\u3002", "result": "\u65e5\u5e73\u5747\u6e29\u5ea6\u5b9e\u9a8c\u8868\u660e\uff0c\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u80fd\u591f\u964d\u5c3a\u5ea6\u6c14\u5019\u9884\u6d4b\uff0c\u4e14\u4e0d\u4f1a\u589e\u52a0\u6c14\u5019\u6307\u6807\u8bef\u5dee\u3002", "conclusion": "\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u964d\u5c3a\u5ea6\u6c14\u5019\u9884\u6d4b\uff0c\u4e3a\u5730\u65b9\u51b3\u7b56\u63d0\u4f9b\u9ad8\u5206\u8fa8\u7387\u6c14\u5019\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2509.21433", "pdf": "https://arxiv.org/pdf/2509.21433", "abs": "https://arxiv.org/abs/2509.21433", "authors": ["Jiaqi Liu", "Lan Zhang", "Xiaoyong Yuan"], "title": "DyME: Dynamic Multi-Concept Erasure in Diffusion Models with Bi-Level Orthogonal LoRA Adaptation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Text-to-image diffusion models (DMs) inadvertently reproduce copyrighted styles and protected visual concepts, raising legal and ethical concerns. Concept erasure has emerged as a safeguard, aiming to selectively suppress such concepts through fine-tuning. However, existing methods do not scale to practical settings where providers must erase multiple and possibly conflicting concepts. The core bottleneck is their reliance on static erasure: a single checkpoint is fine-tuned to remove all target concepts, regardless of the actual erasure needs at inference. This rigid design mismatches real-world usage, where requests vary per generation, leading to degraded erasure success and reduced fidelity for non-target content. We propose DyME, an on-demand erasure framework that trains lightweight, concept-specific LoRA adapters and dynamically composes only those needed at inference. This modular design enables flexible multi-concept erasure, but naive composition causes interference among adapters, especially when many or semantically related concepts are suppressed. To overcome this, we introduce bi-level orthogonality constraints at both the feature and parameter levels, disentangling representation shifts and enforcing orthogonal adapter subspaces. We further develop ErasureBench-H, a new hierarchical benchmark with brand-series-character structure, enabling principled evaluation across semantic granularities and erasure set sizes. Experiments on ErasureBench-H and standard datasets (e.g., CIFAR-100, Imagenette) demonstrate that DyME consistently outperforms state-of-the-art baselines, achieving higher multi-concept erasure fidelity with minimal collateral degradation.", "AI": {"tldr": "DyME\u662f\u4e00\u4e2a\u6309\u9700\u6982\u5ff5\u64e6\u9664\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u8f7b\u91cf\u7ea7LoRA\u9002\u914d\u5668\u5e76\u52a8\u6001\u7ec4\u5408\u6765\u5b9e\u73b0\u7075\u6d3b\u7684\u591a\u6982\u5ff5\u64e6\u9664\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u9759\u6001\u64e6\u9664\u65b9\u6cd5\u5728\u591a\u6982\u5ff5\u548c\u51b2\u7a81\u6982\u5ff5\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4f1a\u65e0\u610f\u4e2d\u590d\u5236\u53d7\u7248\u6743\u4fdd\u62a4\u7684\u98ce\u683c\u548c\u89c6\u89c9\u6982\u5ff5\uff0c\u5f15\u53d1\u6cd5\u5f8b\u548c\u4f26\u7406\u95ee\u9898\u3002\u73b0\u6709\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u65e0\u6cd5\u6269\u5c55\u5230\u5b9e\u9645\u573a\u666f\u4e2d\u9700\u8981\u64e6\u9664\u591a\u4e2a\u53ef\u80fd\u51b2\u7a81\u6982\u5ff5\u7684\u60c5\u51b5\u3002", "method": "\u63d0\u51faDyME\u6846\u67b6\uff1a\u8bad\u7ec3\u6982\u5ff5\u7279\u5b9a\u7684LoRA\u9002\u914d\u5668\uff0c\u5728\u63a8\u7406\u65f6\u52a8\u6001\u7ec4\u5408\u6240\u9700\u9002\u914d\u5668\uff1b\u5f15\u5165\u53cc\u5c42\u6b63\u4ea4\u6027\u7ea6\u675f\uff08\u7279\u5f81\u7ea7\u548c\u53c2\u6570\u7ea7\uff09\u6765\u89e3\u8026\u8868\u793a\u504f\u79fb\u5e76\u5f3a\u5236\u6b63\u4ea4\u9002\u914d\u5668\u5b50\u7a7a\u95f4\u3002", "result": "\u5728ErasureBench-H\u548c\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDyME\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u591a\u6982\u5ff5\u64e6\u9664\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u4e86\u5bf9\u975e\u76ee\u6807\u5185\u5bb9\u7684\u9644\u5e26\u635f\u5bb3\u3002", "conclusion": "DyME\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u6b63\u4ea4\u6027\u7ea6\u675f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6982\u5ff5\u64e6\u9664\u4e2d\u7684\u5e72\u6270\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.21574", "pdf": "https://arxiv.org/pdf/2509.21574", "abs": "https://arxiv.org/abs/2509.21574", "authors": ["You Xie", "Tianpei Gu", "Zenan Li", "Chenxu Zhang", "Guoxian Song", "Xiaochen Zhao", "Chao Liang", "Jianwen Jiang", "Hongyi Xu", "Linjie Luo"], "title": "X-Streamer: Unified Human World Modeling with Audiovisual Interaction", "categories": ["cs.CV"], "comment": "Project Page at https://byteaigc.github.io/X-Streamer", "summary": "We introduce X-Streamer, an end-to-end multimodal human world modeling framework for building digital human agents capable of infinite interactions across text, speech, and video within a single unified architecture. Starting from a single portrait, X-Streamer enables real-time, open-ended video calls driven by streaming multimodal inputs. At its core is a Thinker-Actor dual-transformer architecture that unifies multimodal understanding and generation, turning a static portrait into persistent and intelligent audiovisual interactions. The Thinker module perceives and reasons over streaming user inputs, while its hidden states are translated by the Actor into synchronized multimodal streams in real time. Concretely, the Thinker leverages a pretrained large language-speech model, while the Actor employs a chunk-wise autoregressive diffusion model that cross-attends to the Thinker's hidden states to produce time-aligned multimodal responses with interleaved discrete text and audio tokens and continuous video latents. To ensure long-horizon stability, we design inter- and intra-chunk attentions with time-aligned multimodal positional embeddings for fine-grained cross-modality alignment and context retention, further reinforced by chunk-wise diffusion forcing and global identity referencing. X-Streamer runs in real time on two A100 GPUs, sustaining hours-long consistent video chat experiences from arbitrary portraits and paving the way toward unified world modeling of interactive digital humans.", "AI": {"tldr": "X-Streamer\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u591a\u6a21\u6001\u4eba\u7c7b\u4e16\u754c\u5efa\u6a21\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u5355\u5f20\u8096\u50cf\u751f\u6210\u5b9e\u65f6\u3001\u5f00\u653e\u5f0f\u7684\u89c6\u9891\u901a\u8bdd\uff0c\u652f\u6301\u6587\u672c\u3001\u8bed\u97f3\u548c\u89c6\u9891\u7684\u65e0\u9650\u4ea4\u4e92\u3002", "motivation": "\u6784\u5efa\u80fd\u591f\u5728\u5355\u4e00\u7edf\u4e00\u67b6\u6784\u4e2d\u5b9e\u73b0\u6587\u672c\u3001\u8bed\u97f3\u548c\u89c6\u9891\u65e0\u9650\u4ea4\u4e92\u7684\u6570\u5b57\u4eba\u7c7b\u4ee3\u7406\uff0c\u5c06\u9759\u6001\u8096\u50cf\u8f6c\u53d8\u4e3a\u6301\u4e45\u4e14\u667a\u80fd\u7684\u89c6\u542c\u4ea4\u4e92\u4f53\u9a8c\u3002", "method": "\u91c7\u7528Thinker-Actor\u53cc\u53d8\u6362\u5668\u67b6\u6784\uff1aThinker\u6a21\u5757\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684\u5927\u8bed\u8a00-\u8bed\u97f3\u6a21\u578b\u611f\u77e5\u548c\u63a8\u7406\u6d41\u5f0f\u7528\u6237\u8f93\u5165\uff1bActor\u6a21\u5757\u4f7f\u7528\u5206\u5757\u81ea\u56de\u5f52\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5c06Thinker\u7684\u9690\u85cf\u72b6\u6001\u8f6c\u6362\u4e3a\u65f6\u95f4\u5bf9\u9f50\u7684\u591a\u6a21\u6001\u54cd\u5e94\u3002", "result": "X-Streamer\u5728\u4e24\u4e2aA100 GPU\u4e0a\u5b9e\u65f6\u8fd0\u884c\uff0c\u80fd\u591f\u4ece\u4efb\u610f\u8096\u50cf\u7ef4\u6301\u6570\u5c0f\u65f6\u4e00\u81f4\u7684\u89c6\u9891\u804a\u5929\u4f53\u9a8c\uff0c\u5b9e\u73b0\u4e86\u4ea4\u4e92\u5f0f\u6570\u5b57\u4eba\u7c7b\u7684\u7edf\u4e00\u4e16\u754c\u5efa\u6a21\u3002", "conclusion": "X-Streamer\u4e3a\u6784\u5efa\u7edf\u4e00\u4e16\u754c\u5efa\u6a21\u7684\u4ea4\u4e92\u5f0f\u6570\u5b57\u4eba\u7c7b\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5b9e\u73b0\u4e86\u4ece\u9759\u6001\u8096\u50cf\u5230\u5b9e\u65f6\u591a\u6a21\u6001\u4ea4\u4e92\u7684\u7a81\u7834\u3002"}}
{"id": "2509.21657", "pdf": "https://arxiv.org/pdf/2509.21657", "abs": "https://arxiv.org/abs/2509.21657", "authors": ["Yixiang Dai", "Fan Jiang", "Chiyu Wang", "Mu Xu", "Yonggang Qi"], "title": "FantasyWorld: Geometry-Consistent World Modeling via Unified Video and 3D Prediction", "categories": ["cs.CV"], "comment": null, "summary": "High-quality 3D world models are pivotal for embodied intelligence and Artificial General Intelligence (AGI), underpinning applications such as AR/VR content creation and robotic navigation. Despite the established strong imaginative priors, current video foundation models lack explicit 3D grounding capabilities, thus being limited in both spatial consistency and their utility for downstream 3D reasoning tasks. In this work, we present FantasyWorld, a geometry-enhanced framework that augments frozen video foundation models with a trainable geometric branch, enabling joint modeling of video latents and an implicit 3D field in a single forward pass. Our approach introduces cross-branch supervision, where geometry cues guide video generation and video priors regularize 3D prediction, thus yielding consistent and generalizable 3D-aware video representations. Notably, the resulting latents from the geometric branch can potentially serve as versatile representations for downstream 3D tasks such as novel view synthesis and navigation, without requiring per-scene optimization or fine-tuning. Extensive experiments show that FantasyWorld effectively bridges video imagination and 3D perception, outperforming recent geometry-consistent baselines in multi-view coherence and style consistency. Ablation studies further confirm that these gains stem from the unified backbone and cross-branch information exchange.", "AI": {"tldr": "FantasyWorld\u662f\u4e00\u4e2a\u51e0\u4f55\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u4e3a\u51bb\u7ed3\u7684\u89c6\u9891\u57fa\u7840\u6a21\u578b\u6dfb\u52a0\u53ef\u8bad\u7ec3\u7684\u51e0\u4f55\u5206\u652f\uff0c\u5b9e\u73b0\u89c6\u9891\u6f5c\u5728\u7a7a\u95f4\u548c\u9690\u5f0f3D\u573a\u7684\u8054\u5408\u5efa\u6a21\uff0c\u4ece\u800c\u83b7\u5f97\u5177\u67093D\u611f\u77e5\u80fd\u529b\u7684\u89c6\u9891\u8868\u793a\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u57fa\u7840\u6a21\u578b\u867d\u7136\u5177\u6709\u5f3a\u5927\u7684\u60f3\u8c61\u529b\u5148\u9a8c\uff0c\u4f46\u7f3a\u4e4f\u660e\u786e\u76843D\u57fa\u7840\u80fd\u529b\uff0c\u5bfc\u81f4\u7a7a\u95f4\u4e00\u81f4\u6027\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u57283D\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f15\u5165\u8de8\u5206\u652f\u76d1\u7763\u673a\u5236\uff0c\u51e0\u4f55\u7ebf\u7d22\u6307\u5bfc\u89c6\u9891\u751f\u6210\uff0c\u89c6\u9891\u5148\u9a8c\u6b63\u5219\u53163D\u9884\u6d4b\uff0c\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u8054\u5408\u5efa\u6a21\u89c6\u9891\u6f5c\u5728\u7a7a\u95f4\u548c\u9690\u5f0f3D\u573a\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFantasyWorld\u6709\u6548\u6865\u63a5\u4e86\u89c6\u9891\u60f3\u8c61\u548c3D\u611f\u77e5\uff0c\u5728\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u98ce\u683c\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u51e0\u4f55\u4e00\u81f4\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u9aa8\u5e72\u7f51\u7edc\u548c\u8de8\u5206\u652f\u4fe1\u606f\u4ea4\u6362\uff0c\u5b9e\u73b0\u4e863D\u611f\u77e5\u89c6\u9891\u8868\u793a\uff0c\u4e3a\u4e0b\u6e383D\u4efb\u52a1\u63d0\u4f9b\u4e86\u65e0\u9700\u9010\u573a\u666f\u4f18\u5316\u7684\u901a\u7528\u8868\u793a\u3002"}}
{"id": "2509.21670", "pdf": "https://arxiv.org/pdf/2509.21670", "abs": "https://arxiv.org/abs/2509.21670", "authors": ["Mahindra Singh Rautela", "Alexander Most", "Siddharth Mansingh", "Bradley C. Love", "Ayan Biswas", "Diane Oyen", "Earl Lawrence"], "title": "MORPH: Shape-agnostic PDE Foundation Models", "categories": ["cs.CV", "cs.AI", "cs.LG", "physics.comp-ph"], "comment": null, "summary": "We introduce MORPH, a shape-agnostic, autoregressive foundation model for partial differential equations (PDEs). MORPH is built on a convolutional vision transformer backbone that seamlessly handles heterogeneous spatiotemporal datasets of varying data dimensionality (1D--3D) at different resolutions, multiple fields with mixed scalar and vector components. The architecture combines (i) component-wise convolution, which jointly processes scalar and vector channels to capture local interactions, (ii) inter-field cross-attention, which models and selectively propagates information between different physical fields, (iii) axial attentions, which factorizes full spatiotemporal self-attention along individual spatial and temporal axes to reduce computational burden while retaining expressivity. We pretrain multiple model variants on a diverse collection of heterogeneous PDE datasets and evaluate transfer to a range of downstream prediction tasks. Using both full-model fine-tuning and parameter-efficient low-rank adapters (LoRA), MORPH outperforms models trained from scratch in both zero-shot and full-shot generalization. Across extensive evaluations, MORPH matches or surpasses strong baselines and recent state-of-the-art models. Collectively, these capabilities present a flexible and powerful backbone for learning from heterogeneous and multimodal nature of scientific observations, charting a path toward scalable and data-efficient scientific machine learning.", "AI": {"tldr": "MORPH\u662f\u4e00\u4e2a\u5f62\u72b6\u65e0\u5173\u7684\u81ea\u56de\u5f52\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u504f\u5fae\u5206\u65b9\u7a0b(PDEs)\u3002\u5b83\u57fa\u4e8e\u5377\u79ef\u89c6\u89c9\u53d8\u6362\u5668\u67b6\u6784\uff0c\u80fd\u5904\u7406\u5f02\u6784\u65f6\u7a7a\u6570\u636e\u96c6\uff0c\u652f\u6301\u4e0d\u540c\u7ef4\u5ea6(1D-3D)\u3001\u5206\u8fa8\u7387\u548c\u6df7\u5408\u6807\u91cf/\u77e2\u91cf\u573a\u7684PDE\u6570\u636e\u3002", "motivation": "\u89e3\u51b3\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4e2d\u5904\u7406\u5f02\u6784\u3001\u591a\u6a21\u6001PDE\u6570\u636e\u7684\u6311\u6218\uff0c\u5f00\u53d1\u4e00\u4e2a\u7075\u6d3b\u4e14\u5f3a\u5927\u7684\u57fa\u7840\u6a21\u578b\u6765\u5904\u7406\u4e0d\u540c\u7ef4\u5ea6\u3001\u5206\u8fa8\u7387\u548c\u7269\u7406\u573a\u7684PDE\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5377\u79ef\u89c6\u89c9\u53d8\u6362\u5668\u67b6\u6784\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a(i)\u5206\u91cf\u5377\u79ef\u8054\u5408\u5904\u7406\u6807\u91cf\u548c\u77e2\u91cf\u901a\u9053\uff1b(ii)\u573a\u95f4\u4ea4\u53c9\u6ce8\u610f\u529b\u5efa\u6a21\u4e0d\u540c\u7269\u7406\u573a\u95f4\u4fe1\u606f\u4f20\u64ad\uff1b(iii)\u8f74\u5411\u6ce8\u610f\u529b\u5206\u89e3\u65f6\u7a7a\u81ea\u6ce8\u610f\u529b\u4ee5\u964d\u4f4e\u8ba1\u7b97\u8d1f\u62c5\u3002", "result": "\u5728\u591a\u79cd\u5f02\u6784PDE\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\uff0c\u5e76\u5728\u4e0b\u6e38\u9884\u6d4b\u4efb\u52a1\u4e2d\u8bc4\u4f30\u3002\u4f7f\u7528\u5168\u6a21\u578b\u5fae\u8c03\u548cLoRA\u9002\u914d\u5668\uff0cMORPH\u5728\u96f6\u6837\u672c\u548c\u5168\u6837\u672c\u6cdb\u5316\u4e2d\u5747\u4f18\u4e8e\u4ece\u5934\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u5f3a\u57fa\u7ebf\u548c\u6700\u65b0SOTA\u6a21\u578b\u3002", "conclusion": "MORPH\u4e3a\u5b66\u4e60\u79d1\u5b66\u89c2\u6d4b\u7684\u5f02\u6784\u548c\u591a\u6a21\u6001\u7279\u6027\u63d0\u4f9b\u4e86\u7075\u6d3b\u5f3a\u5927\u7684\u57fa\u7840\u67b6\u6784\uff0c\u4e3a\u5b9e\u73b0\u53ef\u6269\u5c55\u548c\u6570\u636e\u9ad8\u6548\u7684\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2509.21719", "pdf": "https://arxiv.org/pdf/2509.21719", "abs": "https://arxiv.org/abs/2509.21719", "authors": ["Shuning Sun", "Jialang Lu", "Xiang Chen", "Jichao Wang", "Dianjie Lu", "Guijuan Zhang", "Guangwei Gao", "Zhuoran Zheng"], "title": "DeLiVR: Differential Spatiotemporal Lie Bias for Efficient Video Deraining", "categories": ["cs.CV"], "comment": null, "summary": "Videos captured in the wild often suffer from rain streaks, blur, and noise. In addition, even slight changes in camera pose can amplify cross-frame mismatches and temporal artifacts. Existing methods rely on optical flow or heuristic alignment, which are computationally expensive and less robust. To address these challenges, Lie groups provide a principled way to represent continuous geometric transformations, making them well-suited for enforcing spatial and temporal consistency in video modeling. Building on this insight, we propose DeLiVR, an efficient video deraining method that injects spatiotemporal Lie-group differential biases directly into attention scores of the network. Specifically, the method introduces two complementary components. First, a rotation-bounded Lie relative bias predicts the in-plane angle of each frame using a compact prediction module, where normalized coordinates are rotated and compared with base coordinates to achieve geometry-consistent alignment before feature aggregation. Second, a differential group displacement computes angular differences between adjacent frames to estimate a velocity. This bias computation combines temporal decay and attention masks to focus on inter-frame relationships while precisely matching the direction of rain streaks. Extensive experimental results demonstrate the effectiveness of our method on publicly available benchmarks.", "AI": {"tldr": "DeLiVR\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u89c6\u9891\u53bb\u96e8\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u65f6\u7a7a\u674e\u7fa4\u5fae\u5206\u504f\u7f6e\u76f4\u63a5\u6ce8\u5165\u7f51\u7edc\u6ce8\u610f\u529b\u5206\u6570\u4e2d\uff0c\u5b9e\u73b0\u51e0\u4f55\u4e00\u81f4\u7684\u5e27\u5bf9\u9f50\u548c\u96e8\u7eb9\u65b9\u5411\u5339\u914d\u3002", "motivation": "\u89e3\u51b3\u91ce\u5916\u62cd\u6444\u89c6\u9891\u4e2d\u7684\u96e8\u7eb9\u3001\u6a21\u7cca\u548c\u566a\u58f0\u95ee\u9898\uff0c\u4ee5\u53ca\u76f8\u673a\u59ff\u6001\u5fae\u5c0f\u53d8\u5316\u5bfc\u81f4\u7684\u8de8\u5e27\u4e0d\u5339\u914d\u548c\u65f6\u95f4\u4f2a\u5f71\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5149\u6d41\u6216\u542f\u53d1\u5f0f\u5bf9\u9f50\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u9c81\u68d2\u6027\u5dee\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u4e92\u8865\u7ec4\u4ef6\uff1a1) \u65cb\u8f6c\u6709\u754c\u674e\u76f8\u5bf9\u504f\u7f6e\uff0c\u901a\u8fc7\u7d27\u51d1\u9884\u6d4b\u6a21\u5757\u9884\u6d4b\u6bcf\u5e27\u5e73\u9762\u5185\u89d2\u5ea6\uff0c\u5b9e\u73b0\u51e0\u4f55\u4e00\u81f4\u5bf9\u9f50\uff1b2) \u5fae\u5206\u7fa4\u4f4d\u79fb\uff0c\u8ba1\u7b97\u76f8\u90bb\u5e27\u95f4\u89d2\u5ea6\u5dee\u4f30\u8ba1\u901f\u5ea6\uff0c\u7ed3\u5408\u65f6\u95f4\u8870\u51cf\u548c\u6ce8\u610f\u529b\u63a9\u7801\u805a\u7126\u5e27\u95f4\u5173\u7cfb\u3002", "result": "\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u674e\u7fa4\u4e3a\u89c6\u9891\u5efa\u6a21\u4e2d\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u8868\u793a\u65b9\u6cd5\uff0cDeLiVR\u901a\u8fc7\u6ce8\u5165\u674e\u7fa4\u5fae\u5206\u504f\u7f6e\u5b9e\u73b0\u4e86\u9ad8\u6548\u9c81\u68d2\u7684\u89c6\u9891\u53bb\u96e8\u3002"}}
{"id": "2509.21760", "pdf": "https://arxiv.org/pdf/2509.21760", "abs": "https://arxiv.org/abs/2509.21760", "authors": ["Lan Chen", "Yuchao Gu", "Qi Mao"], "title": "UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models", "categories": ["cs.CV"], "comment": null, "summary": "Large language models, trained on extensive corpora, successfully unify diverse linguistic tasks within a single generative framework. Inspired by this, recent works like Large Vision Model (LVM) extend this paradigm to vision by organizing tasks into sequential visual sentences, where visual prompts serve as the context to guide outputs. However, such modeling requires task-specific pre-training across modalities and sources, which is costly and limits scalability to unseen tasks. Given that pre-trained video generation models inherently capture temporal sequence dependencies, we explore a more unified and scalable alternative: can a pre-trained video generation model adapt to diverse image and video tasks? To answer this, we propose UniVid, a framework that fine-tunes a video diffusion transformer to handle various vision tasks without task-specific modifications. Tasks are represented as visual sentences, where the context sequence defines both the task and the expected output modality. We evaluate the generalization of UniVid from two perspectives: (1) cross-modal inference with contexts composed of both images and videos, extending beyond LVM's uni-modal setting; (2) cross-source tasks from natural to annotated data, without multi-source pre-training. Despite being trained solely on natural video data, UniVid generalizes well in both settings. Notably, understanding and generation tasks can easily switch by simply reversing the visual sentence order in this paradigm. These findings highlight the potential of pre-trained video generation models to serve as a scalable and unified foundation for vision modeling. Our code will be released at https://github.com/CUC-MIPG/UniVid.", "AI": {"tldr": "UniVid\u6846\u67b6\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u7684\u89c6\u9891\u6269\u6563\u53d8\u6362\u5668\uff0c\u5c06\u5404\u79cd\u89c6\u89c9\u4efb\u52a1\u7edf\u4e00\u8868\u793a\u4e3a\u89c6\u89c9\u53e5\u5b50\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u4fee\u6539\u5373\u53ef\u5904\u7406\u591a\u6a21\u6001\u548c\u591a\u6e90\u4efb\u52a1\u3002", "motivation": "\u53d7\u5927\u8bed\u8a00\u6a21\u578b\u7edf\u4e00\u5904\u7406\u591a\u79cd\u4efb\u52a1\u7684\u542f\u53d1\uff0c\u5e0c\u671b\u5c06\u7c7b\u4f3c\u8303\u5f0f\u6269\u5c55\u5230\u89c6\u89c9\u9886\u57df\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u8de8\u6a21\u6001\u548c\u6765\u6e90\u7684\u4efb\u52a1\u7279\u5b9a\u9884\u8bad\u7ec3\uff0c\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\u5230\u672a\u89c1\u4efb\u52a1\u3002", "method": "\u63d0\u51faUniVid\u6846\u67b6\uff0c\u5c06\u4efb\u52a1\u8868\u793a\u4e3a\u89c6\u89c9\u53e5\u5b50\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u9891\u6269\u6563\u53d8\u6362\u5668\u8fdb\u884c\u5fae\u8c03\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5e8f\u5217\u5b9a\u4e49\u4efb\u52a1\u548c\u8f93\u51fa\u6a21\u6001\u3002", "result": "UniVid\u5728\u8de8\u6a21\u6001\u63a8\u7406\uff08\u56fe\u50cf\u548c\u89c6\u9891\u6df7\u5408\uff09\u548c\u8de8\u6e90\u4efb\u52a1\uff08\u81ea\u7136\u6570\u636e\u5230\u6807\u6ce8\u6570\u636e\uff09\u4e0a\u90fd\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5c3d\u7ba1\u4ec5\u4f7f\u7528\u81ea\u7136\u89c6\u9891\u6570\u636e\u8bad\u7ec3\u3002", "conclusion": "\u9884\u8bad\u7ec3\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u6709\u6f5c\u529b\u4f5c\u4e3a\u89c6\u89c9\u5efa\u6a21\u7684\u53ef\u6269\u5c55\u7edf\u4e00\u57fa\u7840\uff0c\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u53ef\u4ee5\u901a\u8fc7\u7b80\u5355\u53cd\u8f6c\u89c6\u89c9\u53e5\u5b50\u987a\u5e8f\u6765\u5207\u6362\u3002"}}
{"id": "2509.21787", "pdf": "https://arxiv.org/pdf/2509.21787", "abs": "https://arxiv.org/abs/2509.21787", "authors": ["Dwip Dalal", "Gautam Vashishtha", "Anku Ranui", "Aishwarya Reganti", "Parth Patwa", "Mohd Sarique", "Chandan Gupta", "Keshav Nath", "Viswanatha Reddy", "Vinija Jain", "Aman Chadha", "Amitava Das", "Amit Sheth", "Asif Ekbal"], "title": "DeHate: A Stable Diffusion-based Multimodal Approach to Mitigate Hate Speech in Images", "categories": ["cs.CV", "cs.CL"], "comment": "Defactify 3 workshop at AAAI 2024", "summary": "The rise in harmful online content not only distorts public discourse but also poses significant challenges to maintaining a healthy digital environment. In response to this, we introduce a multimodal dataset uniquely crafted for identifying hate in digital content. Central to our methodology is the innovative application of watermarked, stability-enhanced, stable diffusion techniques combined with the Digital Attention Analysis Module (DAAM). This combination is instrumental in pinpointing the hateful elements within images, thereby generating detailed hate attention maps, which are used to blur these regions from the image, thereby removing the hateful sections of the image. We release this data set as a part of the dehate shared task. This paper also describes the details of the shared task. Furthermore, we present DeHater, a vision-language model designed for multimodal dehatification tasks. Our approach sets a new standard in AI-driven image hate detection given textual prompts, contributing to the development of more ethical AI applications in social media.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc6\u522b\u6570\u5b57\u5185\u5bb9\u4e2d\u4ec7\u6068\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u6c34\u5370\u7a33\u5b9a\u6269\u6563\u6280\u672f\u548c\u6570\u5b57\u6ce8\u610f\u529b\u5206\u6790\u6a21\u5757\uff0c\u751f\u6210\u4ec7\u6068\u6ce8\u610f\u529b\u56fe\u5e76\u6a21\u7cca\u4ec7\u6068\u533a\u57df\u3002\u540c\u65f6\u4ecb\u7ecd\u4e86DeHater\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548cdehate\u5171\u4eab\u4efb\u52a1\u3002", "motivation": "\u6709\u5bb3\u5728\u7ebf\u5185\u5bb9\u7684\u589e\u52a0\u626d\u66f2\u4e86\u516c\u5171\u8bdd\u8bed\uff0c\u5bf9\u7ef4\u62a4\u5065\u5eb7\u6570\u5b57\u73af\u5883\u6784\u6210\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u4ec7\u6068\u5185\u5bb9\u68c0\u6d4b\u548c\u53bb\u9664\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6c34\u5370\u7a33\u5b9a\u589e\u5f3a\u7684\u7a33\u5b9a\u6269\u6563\u6280\u672f\u7ed3\u5408\u6570\u5b57\u6ce8\u610f\u529b\u5206\u6790\u6a21\u5757(DAAM)\uff0c\u8bc6\u522b\u56fe\u50cf\u4e2d\u7684\u4ec7\u6068\u5143\u7d20\u5e76\u751f\u6210\u4ec7\u6068\u6ce8\u610f\u529b\u56fe\uff0c\u901a\u8fc7\u6a21\u7cca\u8fd9\u4e9b\u533a\u57df\u6765\u53bb\u9664\u4ec7\u6068\u5185\u5bb9\u3002", "result": "\u5f00\u53d1\u4e86\u591a\u6a21\u6001\u4ec7\u6068\u68c0\u6d4b\u6570\u636e\u96c6\u548cDeHater\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4e3aAI\u9a71\u52a8\u7684\u56fe\u50cf\u4ec7\u6068\u68c0\u6d4b\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u7b26\u5408\u4f26\u7406\u7684AI\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u5ba1\u6838\u65b9\u9762\uff0c\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u4fc3\u8fdb\u66f4\u5065\u5eb7\u7684\u6570\u5b57\u73af\u5883\u3002"}}
{"id": "2509.21790", "pdf": "https://arxiv.org/pdf/2509.21790", "abs": "https://arxiv.org/abs/2509.21790", "authors": ["Yu Shang", "Lei Jin", "Yiding Ma", "Xin Zhang", "Chen Gao", "Wei Wu", "Yong Li"], "title": "LongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE", "categories": ["cs.CV"], "comment": "13 pages, 8 figures", "summary": "Video-based world models hold significant potential for generating high-quality embodied manipulation data. However, current video generation methods struggle to achieve stable long-horizon generation: classical diffusion-based approaches often suffer from temporal inconsistency and visual drift over multiple rollouts, while autoregressive methods tend to compromise on visual detail. To solve this, we introduce LongScape, a hybrid framework that adaptively combines intra-chunk diffusion denoising with inter-chunk autoregressive causal generation. Our core innovation is an action-guided, variable-length chunking mechanism that partitions video based on the semantic context of robotic actions. This ensures each chunk represents a complete, coherent action, enabling the model to flexibly generate diverse dynamics. We further introduce a Context-aware Mixture-of-Experts (CMoE) framework that adaptively activates specialized experts for each chunk during generation, guaranteeing high visual quality and seamless chunk transitions. Extensive experimental results demonstrate that our method achieves stable and consistent long-horizon generation over extended rollouts. Our code is available at: https://github.com/tsinghua-fib-lab/Longscape.", "AI": {"tldr": "LongScape\u662f\u4e00\u4e2a\u6df7\u5408\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u4f5c\u5f15\u5bfc\u7684\u5206\u5757\u673a\u5236\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u3001\u4e00\u81f4\u7684\u957f\u5e8f\u5217\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u5728\u957f\u5e8f\u5217\u751f\u6210\u4e2d\u7684\u95ee\u9898\uff1a\u6269\u6563\u65b9\u6cd5\u5b58\u5728\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u6f02\u79fb\uff0c\u81ea\u56de\u5f52\u65b9\u6cd5\u5219\u727a\u7272\u89c6\u89c9\u7ec6\u8282\u8d28\u91cf\u3002", "method": "\u7ed3\u5408\u5757\u5185\u6269\u6563\u53bb\u566a\u548c\u5757\u95f4\u81ea\u56de\u5f52\u56e0\u679c\u751f\u6210\uff0c\u91c7\u7528\u52a8\u4f5c\u5f15\u5bfc\u7684\u53d8\u957f\u5206\u5757\u673a\u5236\uff0c\u5e76\u5f15\u5165\u4e0a\u4e0b\u6587\u611f\u77e5\u4e13\u5bb6\u6df7\u5408\u6846\u67b6\u3002", "result": "\u5728\u6269\u5c55\u7684rollout\u4e2d\u5b9e\u73b0\u4e86\u7a33\u5b9a\u4e00\u81f4\u7684\u957f\u5e8f\u5217\u751f\u6210\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u65b9\u6cd5\u6709\u6548\u3002", "conclusion": "LongScape\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u957f\u5e8f\u5217\u89c6\u9891\u751f\u6210\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.21797", "pdf": "https://arxiv.org/pdf/2509.21797", "abs": "https://arxiv.org/abs/2509.21797", "authors": ["Yu Shang", "Yangcheng Yu", "Xin Zhang", "Xin Jin", "Haisheng Su", "Wei Wu", "Yong Li"], "title": "MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation", "categories": ["cs.CV"], "comment": "11 pages, 4 figures", "summary": "Embodied action planning is a core challenge in robotics, requiring models to generate precise actions from visual observations and language instructions. While video generation world models are promising, their reliance on pixel-level reconstruction often introduces visual redundancies that hinder action decoding and generalization. Latent world models offer a compact, motion-aware representation, but overlook the fine-grained details critical for precise manipulation. To overcome these limitations, we propose MoWM, a mixture-of-world-model framework that fuses representations from hybrid world models for embodied action planning. Our approach uses motion-aware representations from a latent model as a high-level prior, which guides the extraction of fine-grained visual features from the pixel space model. This design allows MoWM to highlight the informative visual details needed for action decoding. Extensive evaluations on the CALVIN benchmark demonstrate that our method achieves state-of-the-art task success rates and superior generalization. We also provide a comprehensive analysis of the strengths of each feature space, offering valuable insights for future research in embodied planning. The code is available at: https://github.com/tsinghua-fib-lab/MoWM.", "AI": {"tldr": "\u63d0\u51fa\u4e86MoWM\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u6f5c\u5728\u4e16\u754c\u6a21\u578b\u548c\u50cf\u7d20\u7a7a\u95f4\u6a21\u578b\u7684\u8868\u793a\u6765\u6539\u8fdb\u5177\u8eab\u52a8\u4f5c\u89c4\u5212\uff0c\u5728CALVIN\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u4e16\u754c\u6a21\u578b\u5b58\u5728\u50cf\u7d20\u7ea7\u91cd\u5efa\u5e26\u6765\u7684\u89c6\u89c9\u5197\u4f59\u95ee\u9898\uff0c\u800c\u6f5c\u5728\u4e16\u754c\u6a21\u578b\u867d\u7136\u7d27\u51d1\u4f46\u5ffd\u7565\u4e86\u7cbe\u7ec6\u7ec6\u8282\u3002\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u5229\u7528\u8fd0\u52a8\u611f\u77e5\u8868\u793a\u548c\u7cbe\u7ec6\u89c6\u89c9\u7279\u5f81\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6df7\u5408\u4e16\u754c\u6a21\u578b\u6846\u67b6\uff0c\u5c06\u6f5c\u5728\u6a21\u578b\u63d0\u4f9b\u7684\u8fd0\u52a8\u611f\u77e5\u8868\u793a\u4f5c\u4e3a\u9ad8\u7ea7\u5148\u9a8c\uff0c\u6307\u5bfc\u4ece\u50cf\u7d20\u7a7a\u95f4\u6a21\u578b\u63d0\u53d6\u7cbe\u7ec6\u89c6\u89c9\u7279\u5f81\uff0c\u7a81\u51fa\u5bf9\u52a8\u4f5c\u89e3\u7801\u6709\u4fe1\u606f\u91cf\u7684\u89c6\u89c9\u7ec6\u8282\u3002", "result": "\u5728CALVIN\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u4efb\u52a1\u6210\u529f\u7387\u548c\u4f18\u5f02\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "MoWM\u6846\u67b6\u6709\u6548\u878d\u5408\u4e86\u4e0d\u540c\u7279\u5f81\u7a7a\u95f4\u7684\u4f18\u52bf\uff0c\u4e3a\u5177\u8eab\u89c4\u5212\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.21839", "pdf": "https://arxiv.org/pdf/2509.21839", "abs": "https://arxiv.org/abs/2509.21839", "authors": ["Cheng Lei", "Jiayu Zhang", "Yue Ma", "Xinyu Wang", "Long Chen", "Liang Tang", "Yiqiang Yan", "Fei Su", "Zhicheng Zhao"], "title": "DiTraj: training-free trajectory control for video diffusion transformer", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion Transformers (DiT)-based video generation models with 3D full attention exhibit strong generative capabilities. Trajectory control represents a user-friendly task in the field of controllable video generation. However, existing methods either require substantial training resources or are specifically designed for U-Net, do not take advantage of the superior performance of DiT. To address these issues, we propose DiTraj, a simple but effective training-free framework for trajectory control in text-to-video generation, tailored for DiT. Specifically, first, to inject the object's trajectory, we propose foreground-background separation guidance: we use the Large Language Model (LLM) to convert user-provided prompts into foreground and background prompts, which respectively guide the generation of foreground and background regions in the video. Then, we analyze 3D full attention and explore the tight correlation between inter-token attention scores and position embedding. Based on this, we propose inter-frame Spatial-Temporal Decoupled 3D-RoPE (STD-RoPE). By modifying only foreground tokens' position embedding, STD-RoPE eliminates their cross-frame spatial discrepancies, strengthening cross-frame attention among them and thus enhancing trajectory control. Additionally, we achieve 3D-aware trajectory control by regulating the density of position embedding. Extensive experiments demonstrate that our method outperforms previous methods in both video quality and trajectory controllability.", "AI": {"tldr": "DiTraj\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u8f68\u8ff9\u63a7\u5236\u6846\u67b6\uff0c\u4e13\u4e3a\u6269\u6563\u53d8\u6362\u5668(DiT)\u8bbe\u8ba1\uff0c\u901a\u8fc7\u524d\u666f-\u80cc\u666f\u5206\u79bb\u5f15\u5bfc\u548c\u7a7a\u95f4-\u65f6\u95f4\u89e3\u80263D-RoPE\u5b9e\u73b0\u7cbe\u786e\u7684\u8f68\u8ff9\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u8f68\u8ff9\u63a7\u5236\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u8d44\u6e90\uff0c\u8981\u4e48\u4e13\u4e3aU-Net\u8bbe\u8ba1\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528DiT\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "method": "\u4f7f\u7528LLM\u5c06\u7528\u6237\u63d0\u793a\u8f6c\u6362\u4e3a\u524d\u666f\u548c\u80cc\u666f\u63d0\u793a\u8fdb\u884c\u5206\u79bb\u5f15\u5bfc\uff1b\u63d0\u51faSTD-RoPE\u901a\u8fc7\u4fee\u6539\u524d\u666ftoken\u7684\u4f4d\u7f6e\u5d4c\u5165\u6d88\u9664\u8de8\u5e27\u7a7a\u95f4\u5dee\u5f02\uff1b\u901a\u8fc7\u8c03\u8282\u4f4d\u7f6e\u5d4c\u5165\u5bc6\u5ea6\u5b9e\u73b03D\u611f\u77e5\u8f68\u8ff9\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u89c6\u9891\u8d28\u91cf\u548c\u8f68\u8ff9\u53ef\u63a7\u6027\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "DiTraj\u4e3aDiT-based\u89c6\u9891\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u8bad\u7ec3free\u8f68\u8ff9\u63a7\u5236\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.21853", "pdf": "https://arxiv.org/pdf/2509.21853", "abs": "https://arxiv.org/abs/2509.21853", "authors": ["Kaixuan Zhang", "Zhipeng Xiong", "Minxian Li", "Mingwu Ren", "Jiankang Deng", "Xiatian Zhu"], "title": "Dynamic Novel View Synthesis in High Dynamic Range", "categories": ["cs.CV"], "comment": null, "summary": "High Dynamic Range Novel View Synthesis (HDR NVS) seeks to learn an HDR 3D model from Low Dynamic Range (LDR) training images captured under conventional imaging conditions. Current methods primarily focus on static scenes, implicitly assuming all scene elements remain stationary and non-living. However, real-world scenarios frequently feature dynamic elements, such as moving objects, varying lighting conditions, and other temporal events, thereby presenting a significantly more challenging scenario. To address this gap, we propose a more realistic problem named HDR Dynamic Novel View Synthesis (HDR DNVS), where the additional dimension ``Dynamic'' emphasizes the necessity of jointly modeling temporal radiance variations alongside sophisticated 3D translation between LDR and HDR. To tackle this complex, intertwined challenge, we introduce HDR-4DGS, a Gaussian Splatting-based architecture featured with an innovative dynamic tone-mapping module that explicitly connects HDR and LDR domains, maintaining temporal radiance coherence by dynamically adapting tone-mapping functions according to the evolving radiance distributions across the temporal dimension. As a result, HDR-4DGS achieves both temporal radiance consistency and spatially accurate color translation, enabling photorealistic HDR renderings from arbitrary viewpoints and time instances. Extensive experiments demonstrate that HDR-4DGS surpasses existing state-of-the-art methods in both quantitative performance and visual fidelity. Source code will be released.", "AI": {"tldr": "\u63d0\u51faHDR-4DGS\u65b9\u6cd5\u89e3\u51b3HDR\u52a8\u6001\u65b0\u89c6\u89d2\u5408\u6210\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u8272\u8c03\u6620\u5c04\u6a21\u5757\u8fde\u63a5HDR\u548cLDR\u57df\uff0c\u5b9e\u73b0\u65f6\u7a7a\u8f90\u5c04\u4e00\u81f4\u6027\u548c\u51c6\u786e\u8272\u5f69\u8f6c\u6362\u3002", "motivation": "\u73b0\u6709HDR\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u573a\u666f\uff0c\u4f46\u771f\u5b9e\u4e16\u754c\u5e38\u5305\u542b\u52a8\u6001\u5143\u7d20\u5982\u79fb\u52a8\u7269\u4f53\u548c\u53d8\u5316\u5149\u7167\uff0c\u9700\u8981\u540c\u65f6\u5efa\u6a21\u65f6\u95f4\u8f90\u5c04\u53d8\u5316\u548cHDR/LDR\u8f6c\u6362\u3002", "method": "\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u67b6\u6784\uff0c\u5f15\u5165\u52a8\u6001\u8272\u8c03\u6620\u5c04\u6a21\u5757\uff0c\u6839\u636e\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u8f90\u5c04\u5206\u5e03\u7684\u53d8\u5316\u52a8\u6001\u8c03\u6574\u8272\u8c03\u6620\u5c04\u51fd\u6570\uff0c\u4fdd\u6301\u65f6\u95f4\u8f90\u5c04\u4e00\u81f4\u6027\u3002", "result": "HDR-4DGS\u5728\u5b9a\u91cf\u6027\u80fd\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u4e0a\u5747\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u80fd\u591f\u4ece\u4efb\u610f\u89c6\u89d2\u548c\u65f6\u95f4\u70b9\u751f\u6210\u903c\u771f\u7684HDR\u6e32\u67d3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86HDR\u52a8\u6001\u65b0\u89c6\u89d2\u5408\u6210\u7684\u590d\u6742\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u65f6\u7a7a\u8f90\u5c04\u4e00\u81f4\u6027\u548c\u51c6\u786e\u8272\u5f69\u8f6c\u6362\uff0c\u4e3a\u52a8\u6001\u573a\u666f\u7684HDR\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.21859", "pdf": "https://arxiv.org/pdf/2509.21859", "abs": "https://arxiv.org/abs/2509.21859", "authors": ["Minje Kim", "Tae-Kyun Kim"], "title": "SRHand: Super-Resolving Hand Images and 3D Shapes via View/Pose-aware Neural Image Representations and Explicit 3D Meshes", "categories": ["cs.CV"], "comment": "10 pages, 6 figures", "summary": "Reconstructing detailed hand avatars plays a crucial role in various applications. While prior works have focused on capturing high-fidelity hand geometry, they heavily rely on high-resolution multi-view image inputs and struggle to generalize on low-resolution images. Multi-view image super-resolution methods have been proposed to enforce 3D view consistency. These methods, however, are limited to static objects/scenes with fixed resolutions and are not applicable to articulated deformable hands. In this paper, we propose SRHand (Super-Resolution Hand), the method for reconstructing detailed 3D geometry as well as textured images of hands from low-resolution images. SRHand leverages the advantages of implicit image representation with explicit hand meshes. Specifically, we introduce a geometric-aware implicit image function (GIIF) that learns detailed hand prior by upsampling the coarse input images. By jointly optimizing the implicit image function and explicit 3D hand shapes, our method preserves multi-view and pose consistency among upsampled hand images, and achieves fine-detailed 3D reconstruction (wrinkles, nails). In experiments using the InterHand2.6M and Goliath datasets, our method significantly outperforms state-of-the-art image upsampling methods adapted to hand datasets, and 3D hand reconstruction methods, quantitatively and qualitatively. Project page: https://yunminjin2.github.io/projects/srhand", "AI": {"tldr": "SRHand\u662f\u4e00\u79cd\u4ece\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u91cd\u5efa\u8be6\u7ec63D\u624b\u90e8\u51e0\u4f55\u548c\u7eb9\u7406\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u9690\u5f0f\u56fe\u50cf\u8868\u793a\u548c\u663e\u5f0f\u624b\u90e8\u7f51\u683c\uff0c\u5b9e\u73b0\u591a\u89c6\u89d2\u4e00\u81f4\u7684\u9ad8\u8d28\u91cf\u624b\u90e8\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u5206\u8fa8\u7387\u591a\u89c6\u89d2\u56fe\u50cf\u8f93\u5165\uff0c\u96be\u4ee5\u5728\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u6cdb\u5316\uff0c\u4e14\u591a\u89c6\u89d2\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u53ef\u53d8\u5f62\u7684\u624b\u90e8\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u51e0\u4f55\u611f\u77e5\u9690\u5f0f\u56fe\u50cf\u51fd\u6570(GIIF)\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u9690\u5f0f\u56fe\u50cf\u51fd\u6570\u548c\u663e\u5f0f3D\u624b\u90e8\u5f62\u72b6\uff0c\u5728\u63d0\u5347\u56fe\u50cf\u5206\u8fa8\u7387\u7684\u540c\u65f6\u4fdd\u6301\u591a\u89c6\u89d2\u548c\u59ff\u6001\u4e00\u81f4\u6027\u3002", "result": "\u5728InterHand2.6M\u548cGoliath\u6570\u636e\u96c6\u4e0a\uff0cSRHand\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u548c3D\u624b\u90e8\u91cd\u5efa\u65b9\u6cd5\u3002", "conclusion": "SRHand\u80fd\u591f\u4ece\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u91cd\u5efa\u5305\u542b\u76b1\u7eb9\u3001\u6307\u7532\u7b49\u7cbe\u7ec6\u7ec6\u8282\u76843D\u624b\u90e8\u51e0\u4f55\u548c\u7eb9\u7406\u56fe\u50cf\u3002"}}
{"id": "2509.21864", "pdf": "https://arxiv.org/pdf/2509.21864", "abs": "https://arxiv.org/abs/2509.21864", "authors": ["Janis Keuper", "Margret Keuper"], "title": "Deepfakes: we need to re-think the concept of \"real\" images", "categories": ["cs.CV"], "comment": null, "summary": "The wide availability and low usability barrier of modern image generation models has triggered the reasonable fear of criminal misconduct and negative social implications. The machine learning community has been engaging this problem with an extensive series of publications proposing algorithmic solutions for the detection of \"fake\", e.g. entirely generated or partially manipulated images. While there is undoubtedly some progress towards technical solutions of the problem, we argue that current and prior work is focusing too much on generative algorithms and \"fake\" data-samples, neglecting a clear definition and data collection of \"real\" images. The fundamental question \"what is a real image?\" might appear to be quite philosophical, but our analysis shows that the development and evaluation of basically all current \"fake\"-detection methods is relying on only a few, quite old low-resolution datasets of \"real\" images like ImageNet. However, the technology for the acquisition of \"real\" images, aka taking photos, has drastically evolved over the last decade: Today, over 90% of all photographs are produced by smartphones which typically use algorithms to compute an image from multiple inputs (over time) from multiple sensors. Based on the fact that these image formation algorithms are typically neural network architectures which are closely related to \"fake\"-image generators, we state the position that today, we need to re-think the concept of \"real\" images. The purpose of this position paper is to raise the awareness of the current shortcomings in this active field of research and to trigger an open discussion whether the detection of \"fake\" images is a sound objective at all. At the very least, we need a clear technical definition of \"real\" images and new benchmark datasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8ba4\u4e3a\u5f53\u524d\u5047\u56fe\u50cf\u68c0\u6d4b\u7814\u7a76\u8fc7\u4e8e\u5173\u6ce8\u751f\u6210\u7b97\u6cd5\u548c\"\u5047\"\u6837\u672c\uff0c\u800c\u5ffd\u89c6\u4e86\"\u771f\u5b9e\"\u56fe\u50cf\u7684\u660e\u786e\u5b9a\u4e49\u548c\u6570\u636e\u6536\u96c6\u3002\u4f5c\u8005\u6307\u51fa\u73b0\u4ee3\u667a\u80fd\u624b\u673a\u62cd\u6444\u7684\u7167\u7247\u4e5f\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u7b97\u6cd5\u5904\u7406\uff0c\u4e0e\u5047\u56fe\u50cf\u751f\u6210\u5668\u7c7b\u4f3c\uff0c\u56e0\u6b64\u9700\u8981\u91cd\u65b0\u601d\u8003\"\u771f\u5b9e\"\u56fe\u50cf\u7684\u6982\u5ff5\u3002", "motivation": "\u968f\u7740\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u666e\u53ca\uff0c\u68c0\u6d4b\u5047\u56fe\u50cf\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002\u4f46\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u751f\u6210\u7b97\u6cd5\uff0c\u800c\u5ffd\u89c6\u4e86\"\u771f\u5b9e\"\u56fe\u50cf\u7684\u660e\u786e\u5b9a\u4e49\u548c\u6570\u636e\u6536\u96c6\u95ee\u9898\u3002\u73b0\u4ee3\u667a\u80fd\u624b\u673a\u62cd\u6444\u7684\u7167\u7247\u4e5f\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u5904\u7406\uff0c\u8fd9\u4e0e\u5047\u56fe\u50cf\u751f\u6210\u5668\u76f8\u4f3c\uff0c\u56e0\u6b64\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\"\u771f\u5b9e\"\u56fe\u50cf\u7684\u6982\u5ff5\u3002", "method": "\u8fd9\u662f\u4e00\u7bc7\u7acb\u573a\u8bba\u6587\uff0c\u901a\u8fc7\u5206\u6790\u5f53\u524d\u5047\u56fe\u50cf\u68c0\u6d4b\u9886\u57df\u7684\u73b0\u72b6\uff0c\u6307\u51fa\u7814\u7a76\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u4f5c\u8005\u5f3a\u8c03\u9700\u8981\u91cd\u65b0\u5b9a\u4e49\"\u771f\u5b9e\"\u56fe\u50cf\u7684\u6982\u5ff5\uff0c\u5e76\u521b\u5efa\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "\u5206\u6790\u53d1\u73b0\u5f53\u524d\u5047\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u5c11\u6570\u8001\u65e7\u7684\u4f4e\u5206\u8fa8\u7387\u6570\u636e\u96c6\uff08\u5982ImageNet\uff09\uff0c\u800c\u73b0\u4ee3\u56fe\u50cf\u91c7\u96c6\u6280\u672f\u5df2\u53d1\u751f\u5de8\u5927\u53d8\u5316\u3002\u8d85\u8fc790%\u7684\u7167\u7247\u7531\u667a\u80fd\u624b\u673a\u62cd\u6444\uff0c\u8fd9\u4e9b\u8bbe\u5907\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u7b97\u6cd5\u5904\u7406\u56fe\u50cf\uff0c\u4e0e\u5047\u56fe\u50cf\u751f\u6210\u5668\u5bc6\u5207\u76f8\u5173\u3002", "conclusion": "\u9700\u8981\u91cd\u65b0\u601d\u8003\"\u771f\u5b9e\"\u56fe\u50cf\u7684\u6982\u5ff5\uff0c\u5236\u5b9a\u6e05\u6670\u7684\u6280\u672f\u5b9a\u4e49\uff0c\u5e76\u521b\u5efa\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002\u4f5c\u8005\u8d28\u7591\u68c0\u6d4b\u5047\u56fe\u50cf\u662f\u5426\u662f\u4e00\u4e2a\u5408\u7406\u7684\u76ee\u6807\uff0c\u547c\u5401\u5728\u8be5\u7814\u7a76\u9886\u57df\u8fdb\u884c\u5f00\u653e\u8ba8\u8bba\u3002"}}
{"id": "2509.21887", "pdf": "https://arxiv.org/pdf/2509.21887", "abs": "https://arxiv.org/abs/2509.21887", "authors": ["Liyang Chen", "Tianze Zhou", "Xu He", "Boshi Tang", "Zhiyong Wu", "Yang Huang", "Yang Wu", "Zhongqian Sun", "Wei Yang", "Helen Meng"], "title": "StableDub: Taming Diffusion Prior for Generalized and Efficient Visual Dubbing", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "The visual dubbing task aims to generate mouth movements synchronized with the driving audio, which has seen significant progress in recent years. However, two critical deficiencies hinder their wide application: (1) Audio-only driving paradigms inadequately capture speaker-specific lip habits, which fail to generate lip movements similar to the target avatar; (2) Conventional blind-inpainting approaches frequently produce visual artifacts when handling obstructions (e.g., microphones, hands), limiting practical deployment. In this paper, we propose StableDub, a novel and concise framework integrating lip-habit-aware modeling with occlusion-robust synthesis. Specifically, building upon the Stable-Diffusion backbone, we develop a lip-habit-modulated mechanism that jointly models phonemic audio-visual synchronization and speaker-specific orofacial dynamics. To achieve plausible lip geometries and object appearances under occlusion, we introduce the occlusion-aware training strategy by explicitly exposing the occlusion objects to the inpainting process. By incorporating the proposed designs, the model eliminates the necessity for cost-intensive priors in previous methods, thereby exhibiting superior training efficiency on the computationally intensive diffusion-based backbone. To further optimize training efficiency from the perspective of model architecture, we introduce a hybrid Mamba-Transformer architecture, which demonstrates the enhanced applicability in low-resource research scenarios. Extensive experimental results demonstrate that StableDub achieves superior performance in lip habit resemblance and occlusion robustness. Our method also surpasses other methods in audio-lip sync, video quality, and resolution consistency. We expand the applicability of visual dubbing methods from comprehensive aspects, and demo videos can be found at https://stabledub.github.io.", "AI": {"tldr": "StableDub\u662f\u4e00\u4e2a\u89c6\u89c9\u914d\u97f3\u6846\u67b6\uff0c\u901a\u8fc7\u5507\u90e8\u4e60\u60ef\u611f\u77e5\u5efa\u6a21\u548c\u906e\u6321\u9c81\u68d2\u5408\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5507\u90e8\u4e60\u60ef\u76f8\u4f3c\u6027\u548c\u906e\u6321\u5904\u7406\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u914d\u97f3\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u7f3a\u9677\uff1a(1)\u4ec5\u57fa\u4e8e\u97f3\u9891\u7684\u9a71\u52a8\u8303\u5f0f\u65e0\u6cd5\u6355\u6349\u8bf4\u8bdd\u8005\u7279\u5b9a\u7684\u5507\u90e8\u4e60\u60ef\uff1b(2)\u4f20\u7edf\u76f2\u4fee\u590d\u65b9\u6cd5\u5728\u5904\u7406\u906e\u6321\u65f6\u7ecf\u5e38\u4ea7\u751f\u89c6\u89c9\u4f2a\u5f71\u3002", "method": "\u57fa\u4e8eStable-Diffusion\u9aa8\u5e72\u7f51\u7edc\uff0c\u5f00\u53d1\u4e86\u5507\u90e8\u4e60\u60ef\u8c03\u5236\u673a\u5236\uff0c\u8054\u5408\u5efa\u6a21\u97f3\u89c6\u9891\u540c\u6b65\u548c\u8bf4\u8bdd\u8005\u7279\u5b9a\u7684\u53e3\u9762\u90e8\u52a8\u6001\uff1b\u5f15\u5165\u906e\u6321\u611f\u77e5\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u5f0f\u5730\u5c06\u906e\u6321\u5bf9\u8c61\u66b4\u9732\u7ed9\u4fee\u590d\u8fc7\u7a0b\uff1b\u91c7\u7528\u6df7\u5408Mamba-Transformer\u67b6\u6784\u4f18\u5316\u8bad\u7ec3\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eStableDub\u5728\u5507\u90e8\u4e60\u60ef\u76f8\u4f3c\u6027\u548c\u906e\u6321\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u97f3\u9891-\u5507\u90e8\u540c\u6b65\u3001\u89c6\u9891\u8d28\u91cf\u548c\u5206\u8fa8\u7387\u4e00\u81f4\u6027\u65b9\u9762\u8d85\u8d8a\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "StableDub\u6d88\u9664\u4e86\u5bf9\u6602\u8d35\u5148\u9a8c\u7684\u9700\u6c42\uff0c\u5728\u8ba1\u7b97\u5bc6\u96c6\u7684\u6269\u6563\u6a21\u578b\u4e0a\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u8bad\u7ec3\u6548\u7387\uff0c\u6269\u5c55\u4e86\u89c6\u89c9\u914d\u97f3\u65b9\u6cd5\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2509.21888", "pdf": "https://arxiv.org/pdf/2509.21888", "abs": "https://arxiv.org/abs/2509.21888", "authors": ["Minjun Kang", "Inkyu Shin", "Taeyeop Lee", "In So Kweon", "Kuk-Jin Yoon"], "title": "Drag4D: Align Your Motion with Text-Driven 3D Scene Generation", "categories": ["cs.CV"], "comment": "version 1", "summary": "We introduce Drag4D, an interactive framework that integrates object motion control within text-driven 3D scene generation. This framework enables users to define 3D trajectories for the 3D objects generated from a single image, seamlessly integrating them into a high-quality 3D background. Our Drag4D pipeline consists of three stages. First, we enhance text-to-3D background generation by applying 2D Gaussian Splatting with panoramic images and inpainted novel views, resulting in dense and visually complete 3D reconstructions. In the second stage, given a reference image of the target object, we introduce a 3D copy-and-paste approach: the target instance is extracted in a full 3D mesh using an off-the-shelf image-to-3D model and seamlessly composited into the generated 3D scene. The object mesh is then positioned within the 3D scene via our physics-aware object position learning, ensuring precise spatial alignment. Lastly, the spatially aligned object is temporally animated along a user-defined 3D trajectory. To mitigate motion hallucination and ensure view-consistent temporal alignment, we develop a part-augmented, motion-conditioned video diffusion model that processes multiview image pairs together with their projected 2D trajectories. We demonstrate the effectiveness of our unified architecture through evaluations at each stage and in the final results, showcasing the harmonized alignment of user-controlled object motion within a high-quality 3D background.", "AI": {"tldr": "Drag4D\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u6d41\u7a0b\u5b9e\u73b0\u6587\u672c\u9a71\u52a8\u76843D\u573a\u666f\u751f\u6210\u4e0e\u7269\u4f53\u8fd0\u52a8\u63a7\u5236\uff0c\u652f\u6301\u7528\u6237\u5b9a\u4e493D\u8f68\u8ff9\u6765\u52a8\u753b\u5316\u7269\u4f53\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u6587\u672c\u9a71\u52a8\u76843D\u573a\u666f\u751f\u6210\u4e2d\u5b9e\u73b0\u7cbe\u786e\u7684\u7269\u4f53\u8fd0\u52a8\u63a7\u5236\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5c06\u7269\u4f53\u65e0\u7f1d\u96c6\u6210\u5230\u9ad8\u8d28\u91cf3D\u80cc\u666f\u4e2d\u5e76\u652f\u6301\u4ea4\u4e92\u5f0f\u8f68\u8ff9\u5b9a\u4e49\u7684\u7cfb\u7edf\u3002", "method": "\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a1) \u4f7f\u75282D\u9ad8\u65af\u6cfc\u6e85\u548c\u5168\u666f\u56fe\u50cf\u589e\u5f3a\u6587\u672c\u52303D\u80cc\u666f\u751f\u6210\uff1b2) \u901a\u8fc73D\u590d\u5236\u7c98\u8d34\u5c06\u76ee\u6807\u7269\u4f53\u7f51\u683c\u96c6\u6210\u5230\u573a\u666f\u4e2d\uff0c\u4f7f\u7528\u7269\u7406\u611f\u77e5\u4f4d\u7f6e\u5b66\u4e60\u8fdb\u884c\u7a7a\u95f4\u5bf9\u9f50\uff1b3) \u4f7f\u7528\u90e8\u4ef6\u589e\u5f3a\u7684\u8fd0\u52a8\u6761\u4ef6\u89c6\u9891\u6269\u6563\u6a21\u578b\u6cbf\u7528\u6237\u5b9a\u4e49\u76843D\u8f68\u8ff9\u8fdb\u884c\u65f6\u95f4\u52a8\u753b\u3002", "result": "\u5b9e\u73b0\u4e86\u5728\u9ad8\u8d28\u91cf3D\u80cc\u666f\u4e2d\u7528\u6237\u63a7\u5236\u7269\u4f53\u8fd0\u52a8\u7684\u534f\u8c03\u5bf9\u9f50\uff0c\u5c55\u793a\u4e86\u6bcf\u4e2a\u9636\u6bb5\u548c\u6700\u7ec8\u7ed3\u679c\u7684\u6709\u6548\u6027\u3002", "conclusion": "Drag4D\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u67b6\u6784\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u6587\u672c\u9a71\u52a83D\u573a\u666f\u751f\u6210\u4e0e\u4ea4\u4e92\u5f0f\u7269\u4f53\u8fd0\u52a8\u63a7\u5236\u7684\u96c6\u6210\uff0c\u89e3\u51b3\u4e86\u8fd0\u52a8\u5e7b\u89c9\u548c\u89c6\u56fe\u4e00\u81f4\u6027\u95ee\u9898\u3002"}}
{"id": "2509.21893", "pdf": "https://arxiv.org/pdf/2509.21893", "abs": "https://arxiv.org/abs/2509.21893", "authors": ["Jibin Song", "Mingi Kwon", "Jaeseok Jeong", "Youngjung Uh"], "title": "Syncphony: Synchronized Audio-to-Video Generation with Diffusion Transformers", "categories": ["cs.CV"], "comment": "Project page: https://jibin86.github.io/syncphony_project_page", "summary": "Text-to-video and image-to-video generation have made rapid progress in visual quality, but they remain limited in controlling the precise timing of motion. In contrast, audio provides temporal cues aligned with video motion, making it a promising condition for temporally controlled video generation. However, existing audio-to-video (A2V) models struggle with fine-grained synchronization due to indirect conditioning mechanisms or limited temporal modeling capacity. We present Syncphony, which generates 380x640 resolution, 24fps videos synchronized with diverse audio inputs. Our approach builds upon a pre-trained video backbone and incorporates two key components to improve synchronization: (1) Motion-aware Loss, which emphasizes learning at high-motion regions; (2) Audio Sync Guidance, which guides the full model using a visually aligned off-sync model without audio layers to better exploit audio cues at inference while maintaining visual quality. To evaluate synchronization, we propose CycleSync, a video-to-audio-based metric that measures the amount of motion cues in the generated video to reconstruct the original audio. Experiments on AVSync15 and The Greatest Hits datasets demonstrate that Syncphony outperforms existing methods in both synchronization accuracy and visual quality. Project page is available at: https://jibin86.github.io/syncphony_project_page", "AI": {"tldr": "Syncphony\u662f\u4e00\u4e2a\u97f3\u9891\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u8fd0\u52a8\u611f\u77e5\u635f\u5931\u548c\u97f3\u9891\u540c\u6b65\u5f15\u5bfc\u6280\u672f\uff0c\u80fd\u591f\u751f\u6210\u4e0e\u97f3\u9891\u7cbe\u786e\u540c\u6b65\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u89c6\u9891\u548c\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u5728\u63a7\u5236\u8fd0\u52a8\u65f6\u5e8f\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u800c\u97f3\u9891\u63d0\u4f9b\u4e86\u4e0e\u89c6\u9891\u8fd0\u52a8\u5bf9\u9f50\u7684\u65f6\u95f4\u7ebf\u7d22\uff0c\u662f\u65f6\u95f4\u63a7\u5236\u89c6\u9891\u751f\u6210\u7684\u6709\u524d\u666f\u6761\u4ef6\u3002", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u89c6\u9891\u9aa8\u5e72\u7f51\u7edc\uff0c\u5f15\u5165\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u8fd0\u52a8\u611f\u77e5\u635f\u5931\uff08\u5f3a\u8c03\u9ad8\u8fd0\u52a8\u533a\u57df\u5b66\u4e60\uff09\u548c\u97f3\u9891\u540c\u6b65\u5f15\u5bfc\uff08\u4f7f\u7528\u89c6\u89c9\u5bf9\u9f50\u7684\u65e0\u97f3\u9891\u6a21\u578b\u5f15\u5bfc\u5168\u6a21\u578b\uff09\u3002", "result": "\u5728AVSync15\u548cThe Greatest Hits\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSyncphony\u5728\u540c\u6b65\u7cbe\u5ea6\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Syncphony\u901a\u8fc7\u521b\u65b0\u7684\u540c\u6b65\u673a\u5236\u6210\u529f\u5b9e\u73b0\u4e86\u97f3\u9891\u4e0e\u89c6\u9891\u7684\u9ad8\u8d28\u91cf\u540c\u6b65\u751f\u6210\uff0c\u4e3a\u65f6\u5e8f\u63a7\u5236\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.21905", "pdf": "https://arxiv.org/pdf/2509.21905", "abs": "https://arxiv.org/abs/2509.21905", "authors": ["Qihang Wang", "Yaxiong Wang", "Lechao Cheng", "Zhun Zhong"], "title": "TDEdit: A Unified Diffusion Framework for Text-Drag Guided Image Manipulation", "categories": ["cs.CV"], "comment": null, "summary": "This paper explores image editing under the joint control of text and drag interactions. While recent advances in text-driven and drag-driven editing have achieved remarkable progress, they suffer from complementary limitations: text-driven methods excel in texture manipulation but lack precise spatial control, whereas drag-driven approaches primarily modify shape and structure without fine-grained texture guidance. To address these limitations, we propose a unified diffusion-based framework for joint drag-text image editing, integrating the strengths of both paradigms. Our framework introduces two key innovations: (1) Point-Cloud Deterministic Drag, which enhances latent-space layout control through 3D feature mapping, and (2) Drag-Text Guided Denoising, dynamically balancing the influence of drag and text conditions during denoising. Notably, our model supports flexible editing modes - operating with text-only, drag-only, or combined conditions - while maintaining strong performance in each setting. Extensive quantitative and qualitative experiments demonstrate that our method not only achieves high-fidelity joint editing but also matches or surpasses the performance of specialized text-only or drag-only approaches, establishing a versatile and generalizable solution for controllable image manipulation. Code will be made publicly available to reproduce all results presented in this work.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u6587\u672c\u548c\u62d6\u62fd\u63a7\u5236\u7684\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u6587\u672c\u9a71\u52a8\u7684\u7eb9\u7406\u7f16\u8f91\u548c\u62d6\u62fd\u9a71\u52a8\u7684\u7a7a\u95f4\u63a7\u5236\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u7684\u8054\u5408\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u9a71\u52a8\u65b9\u6cd5\u7f3a\u4e4f\u7cbe\u786e\u7a7a\u95f4\u63a7\u5236\uff0c\u800c\u62d6\u62fd\u9a71\u52a8\u65b9\u6cd5\u7f3a\u5c11\u7ec6\u7c92\u5ea6\u7eb9\u7406\u6307\u5bfc\uff0c\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u6765\u89e3\u51b3\u4e92\u8865\u6027\u9650\u5236\u3002", "method": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u542b\u70b9\u4e91\u786e\u5b9a\u6027\u62d6\u62fd\uff08\u901a\u8fc73D\u7279\u5f81\u6620\u5c04\u589e\u5f3a\u6f5c\u5728\u7a7a\u95f4\u5e03\u5c40\u63a7\u5236\uff09\u548c\u62d6\u62fd\u6587\u672c\u5f15\u5bfc\u53bb\u566a\uff08\u52a8\u6001\u5e73\u8861\u62d6\u62fd\u548c\u6587\u672c\u6761\u4ef6\u7684\u5f71\u54cd\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u8054\u5408\u7f16\u8f91\uff0c\u800c\u4e14\u5728\u7eaf\u6587\u672c\u6216\u7eaf\u62d6\u62fd\u6a21\u5f0f\u4e0b\u4e5f\u80fd\u8fbe\u5230\u6216\u8d85\u8d8a\u4e13\u95e8\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u591a\u529f\u80fd\u4e14\u53ef\u63a8\u5e7f\u7684\u53ef\u63a7\u56fe\u50cf\u64cd\u4f5c\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u7075\u6d3b\u7f16\u8f91\u6a21\u5f0f\uff08\u7eaf\u6587\u672c\u3001\u7eaf\u62d6\u62fd\u6216\u8054\u5408\u6761\u4ef6\uff09\u3002"}}
{"id": "2509.21917", "pdf": "https://arxiv.org/pdf/2509.21917", "abs": "https://arxiv.org/abs/2509.21917", "authors": ["Xianghao Kong", "Hansheng Chen", "Yuwei Guo", "Lvmin Zhang", "Gordon Wetzstein", "Maneesh Agrawala", "Anyi Rao"], "title": "Taming Flow-based I2V Models for Creative Video Editing", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Although image editing techniques have advanced significantly, video editing, which aims to manipulate videos according to user intent, remains an emerging challenge. Most existing image-conditioned video editing methods either require inversion with model-specific design or need extensive optimization, limiting their capability of leveraging up-to-date image-to-video (I2V) models to transfer the editing capability of image editing models to the video domain. To this end, we propose IF-V2V, an Inversion-Free method that can adapt off-the-shelf flow-matching-based I2V models for video editing without significant computational overhead. To circumvent inversion, we devise Vector Field Rectification with Sample Deviation to incorporate information from the source video into the denoising process by introducing a deviation term into the denoising vector field. To further ensure consistency with the source video in a model-agnostic way, we introduce Structure-and-Motion-Preserving Initialization to generate motion-aware temporally correlated noise with structural information embedded. We also present a Deviation Caching mechanism to minimize the additional computational cost for denoising vector rectification without significantly impacting editing quality. Evaluations demonstrate that our method achieves superior editing quality and consistency over existing approaches, offering a lightweight plug-and-play solution to realize visual creativity.", "AI": {"tldr": "\u63d0\u51faIF-V2V\u65b9\u6cd5\uff0c\u65e0\u9700\u53cd\u8f6c\u5373\u53ef\u5c06\u73b0\u6210\u7684\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u56fe\u50cf\u5230\u89c6\u9891\u6a21\u578b\u9002\u914d\u7528\u4e8e\u89c6\u9891\u7f16\u8f91\uff0c\u901a\u8fc7\u5411\u91cf\u573a\u6821\u6b63\u548c\u7ed3\u6784\u8fd0\u52a8\u4fdd\u6301\u521d\u59cb\u5316\u786e\u4fdd\u7f16\u8f91\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u7279\u5b9a\u6a21\u578b\u7684\u53cd\u8f6c\uff0c\u8981\u4e48\u9700\u8981\u5927\u91cf\u4f18\u5316\uff0c\u9650\u5236\u4e86\u5229\u7528\u6700\u65b0\u56fe\u50cf\u5230\u89c6\u9891\u6a21\u578b\u5c06\u56fe\u50cf\u7f16\u8f91\u80fd\u529b\u8fc1\u79fb\u5230\u89c6\u9891\u9886\u57df\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5411\u91cf\u573a\u6821\u6b63\u4e0e\u6837\u672c\u504f\u5dee\u6765\u7ed5\u8fc7\u53cd\u8f6c\uff0c\u5f15\u5165\u504f\u5dee\u9879\u5230\u53bb\u566a\u5411\u91cf\u573a\uff1b\u4f7f\u7528\u7ed3\u6784\u8fd0\u52a8\u4fdd\u6301\u521d\u59cb\u5316\u751f\u6210\u8fd0\u52a8\u611f\u77e5\u7684\u65f6\u95f4\u76f8\u5173\u566a\u58f0\uff1b\u901a\u8fc7\u504f\u5dee\u7f13\u5b58\u673a\u5236\u6700\u5c0f\u5316\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u7f16\u8f91\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u7684\u5373\u63d2\u5373\u7528\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "IF-V2V\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u9700\u53cd\u8f6c\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u7f16\u8f91\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u73b0\u6709\u56fe\u50cf\u5230\u89c6\u9891\u6a21\u578b\u8fdb\u884c\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u3002"}}
{"id": "2509.21938", "pdf": "https://arxiv.org/pdf/2509.21938", "abs": "https://arxiv.org/abs/2509.21938", "authors": ["Woosung Joung", "Daewon Chae", "Jinkyu Kim"], "title": "SemanticControl: A Training-Free Approach for Handling Loosely Aligned Visual Conditions in ControlNet", "categories": ["cs.CV", "cs.AI"], "comment": "BMVC 2025", "summary": "ControlNet has enabled detailed spatial control in text-to-image diffusion models by incorporating additional visual conditions such as depth or edge maps. However, its effectiveness heavily depends on the availability of visual conditions that are precisely aligned with the generation goal specified by text prompt-a requirement that often fails in practice, especially for uncommon or imaginative scenes. For example, generating an image of a cat cooking in a specific pose may be infeasible due to the lack of suitable visual conditions. In contrast, structurally similar cues can often be found in more common settings-for instance, poses of humans cooking are widely available and can serve as rough visual guides. Unfortunately, existing ControlNet models struggle to use such loosely aligned visual conditions, often resulting in low text fidelity or visual artifacts. To address this limitation, we propose SemanticControl, a training-free method for effectively leveraging misaligned but semantically relevant visual conditions. Our approach adaptively suppresses the influence of the visual condition where it conflicts with the prompt, while strengthening guidance from the text. The key idea is to first run an auxiliary denoising process using a surrogate prompt aligned with the visual condition (e.g., \"a human playing guitar\" for a human pose condition) to extract informative attention masks, and then utilize these masks during the denoising of the actual target prompt (e.g., cat playing guitar). Experimental results demonstrate that our method improves performance under loosely aligned conditions across various conditions, including depth maps, edge maps, and human skeletons, outperforming existing baselines. Our code is available at https://mung3477.github.io/semantic-control.", "AI": {"tldr": "\u63d0\u51faSemanticControl\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u8bed\u4e49\u76f8\u5173\u4f46\u672a\u5bf9\u9f50\u7684\u89c6\u89c9\u6761\u4ef6\u6765\u589e\u5f3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u63a7\u5236\u80fd\u529b\uff0c\u5728\u89c6\u89c9\u6761\u4ef6\u4e0e\u6587\u672c\u63d0\u793a\u4e0d\u5b8c\u5168\u5339\u914d\u65f6\u4ecd\u80fd\u4fdd\u6301\u9ad8\u6587\u672c\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709ControlNet\u6a21\u578b\u4e25\u91cd\u4f9d\u8d56\u4e0e\u6587\u672c\u63d0\u793a\u7cbe\u786e\u5bf9\u9f50\u7684\u89c6\u89c9\u6761\u4ef6\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u8fd9\u79cd\u5bf9\u9f50\u6761\u4ef6\u5f80\u5f80\u4e0d\u53ef\u5f97\uff0c\u7279\u522b\u662f\u5728\u4e0d\u5e38\u89c1\u6216\u60f3\u8c61\u573a\u666f\u4e2d\u3002\u5f53\u4f7f\u7528\u8bed\u4e49\u76f8\u5173\u4f46\u672a\u7cbe\u786e\u5bf9\u9f50\u7684\u89c6\u89c9\u6761\u4ef6\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u4f1a\u4ea7\u751f\u4f4e\u6587\u672c\u4fdd\u771f\u5ea6\u6216\u89c6\u89c9\u4f2a\u5f71\u3002", "method": "\u63d0\u51fa\u8bad\u7ec3\u65e0\u5173\u7684SemanticControl\u65b9\u6cd5\uff1a\u9996\u5148\u4f7f\u7528\u4e0e\u89c6\u89c9\u6761\u4ef6\u5bf9\u9f50\u7684\u4ee3\u7406\u63d0\u793a\u8fdb\u884c\u8f85\u52a9\u53bb\u566a\u8fc7\u7a0b\uff0c\u63d0\u53d6\u4fe1\u606f\u6027\u6ce8\u610f\u529b\u63a9\u7801\uff1b\u7136\u540e\u5728\u5b9e\u9645\u76ee\u6807\u63d0\u793a\u7684\u53bb\u566a\u8fc7\u7a0b\u4e2d\u5229\u7528\u8fd9\u4e9b\u63a9\u7801\uff0c\u81ea\u9002\u5e94\u5730\u6291\u5236\u89c6\u89c9\u6761\u4ef6\u4e0e\u63d0\u793a\u51b2\u7a81\u533a\u57df\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u52a0\u5f3a\u6587\u672c\u5f15\u5bfc\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u89c6\u89c9\u6761\u4ef6\uff08\u6df1\u5ea6\u56fe\u3001\u8fb9\u7f18\u56fe\u3001\u4eba\u4f53\u9aa8\u67b6\uff09\u4e0b\uff0c\u5728\u677e\u6563\u5bf9\u9f50\u6761\u4ef6\u4e0b\u5747\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SemanticControl\u80fd\u591f\u6709\u6548\u5229\u7528\u8bed\u4e49\u76f8\u5173\u4f46\u672a\u7cbe\u786e\u5bf9\u9f50\u7684\u89c6\u89c9\u6761\u4ef6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709ControlNet\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u7a7a\u95f4\u63a7\u5236\u80fd\u529b\u3002"}}
{"id": "2509.21953", "pdf": "https://arxiv.org/pdf/2509.21953", "abs": "https://arxiv.org/abs/2509.21953", "authors": ["Tao Wu", "Yibo Jiang", "Yehao Lu", "Zhizhong Wang", "Zeyi Huang", "Zequn Qin", "Xi Li"], "title": "MultiCrafter: High-Fidelity Multi-Subject Generation via Spatially Disentangled Attention and Identity-Aware Reinforcement Learning", "categories": ["cs.CV"], "comment": "Project Page: https://wutao-cs.github.io/MultiCrafter/", "summary": "Multi-subject image generation aims to synthesize user-provided subjects in a single image while preserving subject fidelity, ensuring prompt consistency, and aligning with human aesthetic preferences. However, existing methods, particularly those built on the In-Context-Learning paradigm, are limited by their reliance on simple reconstruction-based objectives, leading to both severe attribute leakage that compromises subject fidelity and failing to align with nuanced human preferences. To address this, we propose MultiCrafter, a framework that ensures high-fidelity, preference-aligned generation. First, we find that the root cause of attribute leakage is a significant entanglement of attention between different subjects during the generation process. Therefore, we introduce explicit positional supervision to explicitly separate attention regions for each subject, effectively mitigating attribute leakage. To enable the model to accurately plan the attention region of different subjects in diverse scenarios, we employ a Mixture-of-Experts architecture to enhance the model's capacity, allowing different experts to focus on different scenarios. Finally, we design a novel online reinforcement learning framework to align the model with human preferences, featuring a scoring mechanism to accurately assess multi-subject fidelity and a more stable training strategy tailored for the MoE architecture. Experiments validate that our framework significantly improves subject fidelity while aligning with human preferences better.", "AI": {"tldr": "MultiCrafter\u662f\u4e00\u4e2a\u591a\u4e3b\u4f53\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4f4d\u7f6e\u76d1\u7763\u89e3\u51b3\u5c5e\u6027\u6cc4\u6f0f\u95ee\u9898\uff0c\u4f7f\u7528\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u589e\u5f3a\u6a21\u578b\u80fd\u529b\uff0c\u5e76\u91c7\u7528\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6765\u5bf9\u9f50\u4eba\u7c7b\u504f\u597d\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u5c5e\u6027\u6cc4\u6f0f\u95ee\u9898\uff0c\u635f\u5bb3\u4e3b\u4f53\u4fdd\u771f\u5ea6\uff0c\u4e14\u65e0\u6cd5\u5bf9\u9f50\u4eba\u7c7b\u504f\u597d\u3002", "method": "\u5f15\u5165\u663e\u5f0f\u4f4d\u7f6e\u76d1\u7763\u5206\u79bb\u4e0d\u540c\u4e3b\u4f53\u7684\u6ce8\u610f\u529b\u533a\u57df\uff1b\u4f7f\u7528\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u589e\u5f3a\u6a21\u578b\u80fd\u529b\uff1b\u8bbe\u8ba1\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5bf9\u9f50\u4eba\u7c7b\u504f\u597d\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u4e3b\u4f53\u4fdd\u771f\u5ea6\uff0c\u5e76\u66f4\u597d\u5730\u5bf9\u9f50\u4e86\u4eba\u7c7b\u504f\u597d\u3002", "conclusion": "MultiCrafter\u901a\u8fc7\u4f4d\u7f6e\u76d1\u7763\u3001\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u548c\u5f3a\u5316\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u591a\u4e3b\u4f53\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5c5e\u6027\u6cc4\u6f0f\u548c\u504f\u597d\u5bf9\u9f50\u95ee\u9898\u3002"}}
{"id": "2509.21989", "pdf": "https://arxiv.org/pdf/2509.21989", "abs": "https://arxiv.org/abs/2509.21989", "authors": ["Abdelrahman Eldesokey", "Aleksandar Cvejic", "Bernard Ghanem", "Peter Wonka"], "title": "Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation", "categories": ["cs.CV"], "comment": "NeurIPS 2025 (Spotlight). Project Page:   https://abdo-eldesokey.github.io/mind-the-glitch/", "summary": "We propose a novel approach for disentangling visual and semantic features from the backbones of pre-trained diffusion models, enabling visual correspondence in a manner analogous to the well-established semantic correspondence. While diffusion model backbones are known to encode semantically rich features, they must also contain visual features to support their image synthesis capabilities. However, isolating these visual features is challenging due to the absence of annotated datasets. To address this, we introduce an automated pipeline that constructs image pairs with annotated semantic and visual correspondences based on existing subject-driven image generation datasets, and design a contrastive architecture to separate the two feature types. Leveraging the disentangled representations, we propose a new metric, Visual Semantic Matching (VSM), that quantifies visual inconsistencies in subject-driven image generation. Empirical results show that our approach outperforms global feature-based metrics such as CLIP, DINO, and vision--language models in quantifying visual inconsistencies while also enabling spatial localization of inconsistent regions. To our knowledge, this is the first method that supports both quantification and localization of inconsistencies in subject-driven generation, offering a valuable tool for advancing this task. Project Page:https://abdo-eldesokey.github.io/mind-the-glitch/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u9aa8\u5e72\u7f51\u7edc\u4e2d\u89e3\u8026\u89c6\u89c9\u548c\u8bed\u4e49\u7279\u5f81\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u89c6\u89c9\u5bf9\u5e94\u5173\u7cfb\uff0c\u7c7b\u4f3c\u4e8e\u8bed\u4e49\u5bf9\u5e94\u5173\u7cfb\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u9aa8\u5e72\u7f51\u7edc\u5df2\u77e5\u7f16\u7801\u4e86\u4e30\u5bcc\u7684\u8bed\u4e49\u7279\u5f81\uff0c\u4f46\u4e5f\u5fc5\u987b\u5305\u542b\u89c6\u89c9\u7279\u5f81\u4ee5\u652f\u6301\u56fe\u50cf\u5408\u6210\u80fd\u529b\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5206\u79bb\u8fd9\u4e9b\u89c6\u89c9\u7279\u5f81\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5f15\u5165\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\u6784\u5efa\u5177\u6709\u6807\u6ce8\u8bed\u4e49\u548c\u89c6\u89c9\u5bf9\u5e94\u5173\u7cfb\u7684\u56fe\u50cf\u5bf9\uff0c\u57fa\u4e8e\u73b0\u6709\u4e3b\u9898\u9a71\u52a8\u56fe\u50cf\u751f\u6210\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u5bf9\u6bd4\u67b6\u6784\u6765\u5206\u79bb\u4e24\u79cd\u7279\u5f81\u7c7b\u578b\u3002", "result": "\u5229\u7528\u89e3\u8026\u8868\u793a\u63d0\u51fa\u65b0\u6307\u6807VSM\uff0c\u5728\u91cf\u5316\u4e3b\u9898\u9a71\u52a8\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u89c6\u89c9\u4e0d\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8eCLIP\u3001DINO\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7b49\u5168\u5c40\u7279\u5f81\u6307\u6807\uff0c\u540c\u65f6\u80fd\u591f\u7a7a\u95f4\u5b9a\u4f4d\u4e0d\u4e00\u81f4\u533a\u57df\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u652f\u6301\u4e3b\u9898\u9a71\u52a8\u751f\u6210\u4e2d\u4e0d\u4e00\u81f4\u6027\u91cf\u5316\u548c\u5b9a\u4f4d\u7684\u65b9\u6cd5\uff0c\u4e3a\u63a8\u8fdb\u8be5\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\u3002"}}
{"id": "2509.21995", "pdf": "https://arxiv.org/pdf/2509.21995", "abs": "https://arxiv.org/abs/2509.21995", "authors": ["Muxi Chen", "Zhaohua Zhang", "Chenchen Zhao", "Mingyang Chen", "Wenyu Jiang", "Tianwen Jiang", "Jianhuan Zhuo", "Yu Tang", "Qiuyong Xiao", "Jihong Zhang", "Qiang Xu"], "title": "FailureAtlas:Mapping the Failure Landscape of T2I Models via Active Exploration", "categories": ["cs.CV"], "comment": null, "summary": "Static benchmarks have provided a valuable foundation for comparing Text-to-Image (T2I) models. However, their passive design offers limited diagnostic power, struggling to uncover the full landscape of systematic failures or isolate their root causes. We argue for a complementary paradigm: active exploration. We introduce FailureAtlas, the first framework designed to autonomously explore and map the vast failure landscape of T2I models at scale. FailureAtlas frames error discovery as a structured search for minimal, failure-inducing concepts. While it is a computationally explosive problem, we make it tractable with novel acceleration techniques. When applied to Stable Diffusion models, our method uncovers hundreds of thousands of previously unknown error slices (over 247,000 in SD1.5 alone) and provides the first large-scale evidence linking these failures to data scarcity in the training set. By providing a principled and scalable engine for deep model auditing, FailureAtlas establishes a new, diagnostic-first methodology to guide the development of more robust generative AI. The code is available at https://github.com/cure-lab/FailureAtlas", "AI": {"tldr": "FailureAtlas\u662f\u4e00\u4e2a\u4e3b\u52a8\u63a2\u7d22\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5931\u8d25\u6a21\u5f0f\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u641c\u7d22\u53d1\u73b0\u6700\u5c0f\u5316\u5931\u8d25\u6982\u5ff5\uff0c\u63ed\u793a\u4e86\u5927\u91cf\u672a\u77e5\u9519\u8bef\u5207\u7247\u5e76\u5efa\u7acb\u4e86\u4e0e\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u8054\u7cfb\u3002", "motivation": "\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\u5728\u6bd4\u8f83\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u65f6\u8bca\u65ad\u80fd\u529b\u6709\u9650\uff0c\u96be\u4ee5\u53d1\u73b0\u7cfb\u7edf\u6027\u5931\u8d25\u7684\u5168\u8c8c\u6216\u9694\u79bb\u6839\u672c\u539f\u56e0\uff0c\u9700\u8981\u4e3b\u52a8\u63a2\u7d22\u7684\u8865\u5145\u65b9\u6cd5\u3002", "method": "\u5c06\u9519\u8bef\u53d1\u73b0\u6784\u5efa\u4e3a\u5bf9\u6700\u5c0f\u5316\u5931\u8d25\u8bf1\u5bfc\u6982\u5ff5\u7684\u7ed3\u6784\u5316\u641c\u7d22\uff0c\u91c7\u7528\u65b0\u9896\u7684\u52a0\u901f\u6280\u672f\u5904\u7406\u8ba1\u7b97\u7206\u70b8\u95ee\u9898\u3002", "result": "\u5728Stable Diffusion\u6a21\u578b\u4e0a\u53d1\u73b0\u4e86\u8d85\u8fc7247,000\u4e2a\u672a\u77e5\u9519\u8bef\u5207\u7247\uff0c\u9996\u6b21\u5927\u89c4\u6a21\u8bc1\u636e\u8868\u660e\u8fd9\u4e9b\u5931\u8d25\u4e0e\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u76f8\u5173\u3002", "conclusion": "FailureAtlas\u5efa\u7acb\u4e86\u8bca\u65ad\u4f18\u5148\u7684\u65b0\u65b9\u6cd5\u5b66\uff0c\u4e3a\u5f00\u53d1\u66f4\u7a33\u5065\u7684\u751f\u6210\u5f0fAI\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u548c\u53ef\u6269\u5c55\u7684\u6df1\u5ea6\u6a21\u578b\u5ba1\u8ba1\u5f15\u64ce\u3002"}}
{"id": "2509.22063", "pdf": "https://arxiv.org/pdf/2509.22063", "abs": "https://arxiv.org/abs/2509.22063", "authors": ["Chao Huang", "Susan Liang", "Yapeng Tian", "Anurag Kumar", "Chenliang Xu"], "title": "High-Quality Sound Separation Across Diverse Categories via Visually-Guided Generative Modeling", "categories": ["cs.CV", "cs.SD"], "comment": "Accepted to IJCV", "summary": "We propose DAVIS, a Diffusion-based Audio-VIsual Separation framework that solves the audio-visual sound source separation task through generative learning. Existing methods typically frame sound separation as a mask-based regression problem, achieving significant progress. However, they face limitations in capturing the complex data distribution required for high-quality separation of sounds from diverse categories. In contrast, DAVIS circumvents these issues by leveraging potent generative modeling paradigms, specifically Denoising Diffusion Probabilistic Models (DDPM) and the more recent Flow Matching (FM), integrated within a specialized Separation U-Net architecture. Our framework operates by synthesizing the desired separated sound spectrograms directly from a noise distribution, conditioned concurrently on the mixed audio input and associated visual information. The inherent nature of its generative objective makes DAVIS particularly adept at producing high-quality sound separations for diverse sound categories. We present comparative evaluations of DAVIS, encompassing both its DDPM and Flow Matching variants, against leading methods on the standard AVE and MUSIC datasets. The results affirm that both variants surpass existing approaches in separation quality, highlighting the efficacy of our generative framework for tackling the audio-visual source separation task.", "AI": {"tldr": "DAVIS\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u97f3\u9891-\u89c6\u89c9\u5206\u79bb\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5b66\u4e60\u65b9\u6cd5\u89e3\u51b3\u97f3\u9891-\u89c6\u89c9\u58f0\u6e90\u5206\u79bb\u4efb\u52a1\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u63a9\u7801\u56de\u5f52\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u5206\u79bb\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u58f0\u97f3\u5206\u79bb\u89c6\u4e3a\u57fa\u4e8e\u63a9\u7801\u7684\u56de\u5f52\u95ee\u9898\uff0c\u5728\u6355\u6349\u590d\u6742\u6570\u636e\u5206\u5e03\u4ee5\u9ad8\u8d28\u91cf\u5206\u79bb\u591a\u6837\u5316\u58f0\u97f3\u7c7b\u522b\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "DAVIS\u5229\u7528\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\u548c\u6d41\u5339\u914d\u7b49\u751f\u6210\u5efa\u6a21\u8303\u5f0f\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u5206\u79bbU-Net\u67b6\u6784\u76f4\u63a5\u4ece\u566a\u58f0\u5206\u5e03\u5408\u6210\u5206\u79bb\u7684\u58f0\u97f3\u9891\u8c31\u56fe\uff0c\u540c\u65f6\u4ee5\u6df7\u5408\u97f3\u9891\u8f93\u5165\u548c\u76f8\u5173\u89c6\u89c9\u4fe1\u606f\u4e3a\u6761\u4ef6\u3002", "result": "\u5728AVE\u548cMUSIC\u6570\u636e\u96c6\u4e0a\u7684\u6bd4\u8f83\u8bc4\u4f30\u8868\u660e\uff0cDAVIS\u7684\u4e24\u4e2a\u53d8\u4f53\uff08DDPM\u548cFlow Matching\uff09\u5728\u5206\u79bb\u8d28\u91cf\u4e0a\u90fd\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DAVIS\u751f\u6210\u6846\u67b6\u5728\u89e3\u51b3\u97f3\u9891-\u89c6\u89c9\u58f0\u6e90\u5206\u79bb\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u51fa\u9ad8\u6548\u6027\uff0c\u7279\u522b\u64c5\u957f\u4e3a\u591a\u6837\u5316\u58f0\u97f3\u7c7b\u522b\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5206\u79bb\u7ed3\u679c\u3002"}}
{"id": "2509.22112", "pdf": "https://arxiv.org/pdf/2509.22112", "abs": "https://arxiv.org/abs/2509.22112", "authors": ["Jingrui Ye", "Lingting Zhu", "Runze Zhang", "Zeyu Hu", "Yingda Yin", "Lanjiong Li", "Lequan Yu", "Qingmin Liao"], "title": "Large Material Gaussian Model for Relightable 3D Generation", "categories": ["cs.CV"], "comment": null, "summary": "The increasing demand for 3D assets across various industries necessitates efficient and automated methods for 3D content creation. Leveraging 3D Gaussian Splatting, recent large reconstruction models (LRMs) have demonstrated the ability to efficiently achieve high-quality 3D rendering by integrating multiview diffusion for generation and scalable transformers for reconstruction. However, existing models fail to produce the material properties of assets, which is crucial for realistic rendering in diverse lighting environments. In this paper, we introduce the Large Material Gaussian Model (MGM), a novel framework designed to generate high-quality 3D content with Physically Based Rendering (PBR) materials, ie, albedo, roughness, and metallic properties, rather than merely producing RGB textures with uncontrolled light baking. Specifically, we first fine-tune a new multiview material diffusion model conditioned on input depth and normal maps. Utilizing the generated multiview PBR images, we explore a Gaussian material representation that not only aligns with 2D Gaussian Splatting but also models each channel of the PBR materials. The reconstructed point clouds can then be rendered to acquire PBR attributes, enabling dynamic relighting by applying various ambient light maps. Extensive experiments demonstrate that the materials produced by our method not only exhibit greater visual appeal compared to baseline methods but also enhance material modeling, thereby enabling practical downstream rendering applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5927\u578b\u6750\u8d28\u9ad8\u65af\u6a21\u578b(MGM)\uff0c\u80fd\u591f\u751f\u6210\u5177\u6709PBR\u6750\u8d28\u5c5e\u6027\u76843D\u5185\u5bb9\uff0c\u652f\u6301\u52a8\u6001\u91cd\u5149\u7167\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u6750\u8d28\u5efa\u6a21\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u67093D\u91cd\u5efa\u6a21\u578b\u65e0\u6cd5\u751f\u6210\u6750\u8d28\u5c5e\u6027\uff0c\u800c\u6750\u8d28\u5c5e\u6027\u5bf9\u4e8e\u5728\u4e0d\u540c\u5149\u7167\u73af\u5883\u4e0b\u5b9e\u73b0\u771f\u5b9e\u6e32\u67d3\u81f3\u5173\u91cd\u8981\u3002", "method": "\u9996\u5148\u5fae\u8c03\u57fa\u4e8e\u6df1\u5ea6\u548c\u6cd5\u7ebf\u56fe\u8f93\u5165\u7684\u591a\u89c6\u89d2\u6750\u8d28\u6269\u6563\u6a21\u578b\uff0c\u7136\u540e\u5229\u7528\u751f\u6210\u7684\u591a\u89c6\u89d2PBR\u56fe\u50cf\u63a2\u7d22\u9ad8\u65af\u6750\u8d28\u8868\u793a\uff0c\u8be5\u8868\u793a\u4e0e2D\u9ad8\u65af\u6cfc\u6e85\u5bf9\u9f50\u5e76\u5efa\u6a21PBR\u6750\u8d28\u7684\u5404\u4e2a\u901a\u9053\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u6750\u8d28\u5728\u89c6\u89c9\u4e0a\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u5177\u5438\u5f15\u529b\uff0c\u5e76\u589e\u5f3a\u4e86\u6750\u8d28\u5efa\u6a21\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u5b9e\u7528\u7684\u4e0b\u6e38\u6e32\u67d3\u5e94\u7528\u3002", "conclusion": "MGM\u6846\u67b6\u6210\u529f\u751f\u6210\u4e86\u5177\u6709PBR\u6750\u8d28\u5c5e\u6027\u7684\u9ad8\u8d28\u91cf3D\u5185\u5bb9\uff0c\u652f\u6301\u52a8\u6001\u91cd\u5149\u7167\uff0c\u4e3a3D\u5185\u5bb9\u521b\u5efa\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22139", "pdf": "https://arxiv.org/pdf/2509.22139", "abs": "https://arxiv.org/abs/2509.22139", "authors": ["Yicheng Jiang", "Jin Yuan", "Hua Yuan", "Yao Zhang", "Yong Rui"], "title": "REFINE-CONTROL: A Semi-supervised Distillation Method For Conditional Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": "5 pages,17 figures", "summary": "Conditional image generation models have achieved remarkable results by leveraging text-based control to generate customized images. However, the high resource demands of these models and the scarcity of well-annotated data have hindered their deployment on edge devices, leading to enormous costs and privacy concerns, especially when user data is sent to a third party. To overcome these challenges, we propose Refine-Control, a semi-supervised distillation framework. Specifically, we improve the performance of the student model by introducing a tri-level knowledge fusion loss to transfer different levels of knowledge. To enhance generalization and alleviate dataset scarcity, we introduce a semi-supervised distillation method utilizing both labeled and unlabeled data. Our experiments reveal that Refine-Control achieves significant reductions in computational cost and latency, while maintaining high-fidelity generation capabilities and controllability, as quantified by comparative metrics.", "AI": {"tldr": "Refine-Control\u662f\u4e00\u4e2a\u534a\u76d1\u7763\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u5c42\u6b21\u77e5\u8bc6\u878d\u5408\u635f\u5931\u548c\u5229\u7528\u6807\u8bb0/\u672a\u6807\u8bb0\u6570\u636e\uff0c\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u65f6\u7684\u9ad8\u8d44\u6e90\u9700\u6c42\u548c\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u907f\u514d\u5c06\u7528\u6237\u6570\u636e\u53d1\u9001\u7ed9\u7b2c\u4e09\u65b9\u5e26\u6765\u7684\u6210\u672c\u548c\u9690\u79c1\u62c5\u5fe7\u3002", "method": "\u63d0\u51fa\u534a\u76d1\u7763\u84b8\u998f\u6846\u67b6\uff0c\u5f15\u5165\u4e09\u5c42\u6b21\u77e5\u8bc6\u878d\u5408\u635f\u5931\u6765\u4f20\u9012\u4e0d\u540c\u5c42\u6b21\u7684\u77e5\u8bc6\uff0c\u540c\u65f6\u5229\u7528\u6807\u8bb0\u548c\u672a\u6807\u8bb0\u6570\u636e\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRefine-Control\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u548c\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u751f\u6210\u80fd\u529b\u548c\u53ef\u63a7\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u90e8\u7f72\u6311\u6218\uff0c\u5e73\u8861\u4e86\u6027\u80fd\u4e0e\u6548\u7387\u3002"}}
{"id": "2509.22169", "pdf": "https://arxiv.org/pdf/2509.22169", "abs": "https://arxiv.org/abs/2509.22169", "authors": ["Kirsten Odendaal", "Neela Kaushik", "Spencer Halverson"], "title": "DragGANSpace: Latent Space Exploration and Control for GANs", "categories": ["cs.CV", "cs.LG"], "comment": "6 pages with 7 figures and 3 tables", "summary": "This work integrates StyleGAN, DragGAN and Principal Component Analysis (PCA) to enhance the latent space efficiency and controllability of GAN-generated images. Style-GAN provides a structured latent space, DragGAN enables intuitive image manipulation, and PCA reduces dimensionality and facilitates cross-model alignment for more streamlined and interpretable exploration of latent spaces. We apply our techniques to the Animal Faces High Quality (AFHQ) dataset, and find that our approach of integrating PCA-based dimensionality reduction with the Drag-GAN framework for image manipulation retains performance while improving optimization efficiency. Notably, introducing PCA into the latent W+ layers of DragGAN can consistently reduce the total optimization time while maintaining good visual quality and even boosting the Structural Similarity Index Measure (SSIM) of the optimized image, particularly in shallower latent spaces (W+ layers = 3). We also demonstrate capability for aligning images generated by two StyleGAN models trained on similar but distinct data domains (AFHQ-Dog and AFHQ-Cat), and show that we can control the latent space of these aligned images to manipulate the images in an intuitive and interpretable manner. Our findings highlight the possibility for efficient and interpretable latent space control for a wide range of image synthesis and editing applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06StyleGAN\u3001DragGAN\u548cPCA\u76f8\u7ed3\u5408\uff0c\u63d0\u5347GAN\u751f\u6210\u56fe\u50cf\u7684\u6f5c\u5728\u7a7a\u95f4\u6548\u7387\u548c\u53ef\u63a7\u6027\uff0c\u5728AFHQ\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u9ad8\u4f18\u5316\u6548\u7387\u3002", "motivation": "\u65e8\u5728\u589e\u5f3aGAN\u751f\u6210\u56fe\u50cf\u7684\u6f5c\u5728\u7a7a\u95f4\u6548\u7387\u548c\u53ef\u63a7\u6027\uff0c\u901a\u8fc7\u6574\u5408\u591a\u79cd\u6280\u672f\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u56fe\u50cf\u5408\u6210\u4e0e\u7f16\u8f91\u3002", "method": "\u7ed3\u5408StyleGAN\u7684\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\u3001DragGAN\u7684\u76f4\u89c2\u56fe\u50cf\u64cd\u4f5c\u548cPCA\u7684\u964d\u7ef4\u80fd\u529b\uff0c\u5728AFHQ\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7279\u522b\u5728W+\u5c42\u5f15\u5165PCA\u964d\u7ef4\u3002", "result": "PCA\u96c6\u6210\u663e\u8457\u51cf\u5c11\u4e86\u603b\u4f18\u5316\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u597d\u7684\u89c6\u89c9\u8d28\u91cf\uff0c\u5728\u8f83\u6d45\u6f5c\u5728\u7a7a\u95f4\uff08W+\u5c42=3\uff09\u4e2d\u751a\u81f3\u63d0\u5347\u4e86SSIM\u6307\u6807\uff0c\u5e76\u5b9e\u73b0\u4e86\u8de8\u6a21\u578b\u56fe\u50cf\u5bf9\u9f50\u548c\u53ef\u63a7\u64cd\u4f5c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5e7f\u6cdb\u7684\u56fe\u50cf\u5408\u6210\u548c\u7f16\u8f91\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u6f5c\u5728\u7a7a\u95f4\u63a7\u5236\u53ef\u80fd\u6027\u3002"}}
{"id": "2509.22225", "pdf": "https://arxiv.org/pdf/2509.22225", "abs": "https://arxiv.org/abs/2509.22225", "authors": ["Jiayu Ding", "Xinpeng Liu", "Zhiyi Pan", "Shiqiang Long", "Ge Li"], "title": "Polysemous Language Gaussian Splatting via Matching-based Mask Lifting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Lifting 2D open-vocabulary understanding into 3D Gaussian Splatting (3DGS) scenes is a critical challenge. However, mainstream methods suffer from three key flaws: (i) their reliance on costly per-scene retraining prevents plug-and-play application; (ii) their restrictive monosemous design fails to represent complex, multi-concept semantics; and (iii) their vulnerability to cross-view semantic inconsistencies corrupts the final semantic representation. To overcome these limitations, we introduce MUSplat, a training-free framework that abandons feature optimization entirely. Leveraging a pre-trained 2D segmentation model, our pipeline generates and lifts multi-granularity 2D masks into 3D, where we estimate a foreground probability for each Gaussian point to form initial object groups. We then optimize the ambiguous boundaries of these initial groups using semantic entropy and geometric opacity. Subsequently, by interpreting the object's appearance across its most representative viewpoints, a Vision-Language Model (VLM) distills robust textual features that reconciles visual inconsistencies, enabling open-vocabulary querying via semantic matching. By eliminating the costly per-scene training process, MUSplat reduces scene adaptation time from hours to mere minutes. On benchmark tasks for open-vocabulary 3D object selection and semantic segmentation, MUSplat outperforms established training-based frameworks while simultaneously addressing their monosemous limitations.", "AI": {"tldr": "MUSplat\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u5c062D\u5f00\u653e\u8bcd\u6c47\u7406\u89e3\u63d0\u5347\u52303D\u9ad8\u65af\u6cfc\u6e85\u573a\u666f\u4e2d\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u9010\u573a\u666f\u8bad\u7ec3\u3001\u5355\u8bed\u4e49\u9650\u5236\u548c\u8de8\u89c6\u56fe\u8bed\u4e49\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u7f3a\u9677\uff1a(i)\u4f9d\u8d56\u6602\u8d35\u7684\u9010\u573a\u666f\u91cd\u65b0\u8bad\u7ec3\uff0c\u65e0\u6cd5\u5373\u63d2\u5373\u7528\uff1b(ii)\u9650\u5236\u6027\u7684\u5355\u8bed\u4e49\u8bbe\u8ba1\u65e0\u6cd5\u8868\u793a\u590d\u6742\u7684\u591a\u6982\u5ff5\u8bed\u4e49\uff1b(iii)\u6613\u53d7\u8de8\u89c6\u56fe\u8bed\u4e49\u4e0d\u4e00\u81f4\u5f71\u54cd\uff0c\u7834\u574f\u6700\u7ec8\u8bed\u4e49\u8868\u793a\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u76842D\u5206\u5272\u6a21\u578b\u751f\u6210\u5e76\u63d0\u5347\u591a\u7c92\u5ea62D\u63a9\u7801\u52303D\uff0c\u4e3a\u6bcf\u4e2a\u9ad8\u65af\u70b9\u4f30\u8ba1\u524d\u666f\u6982\u7387\u5f62\u6210\u521d\u59cb\u5bf9\u8c61\u7ec4\u3002\u4f7f\u7528\u8bed\u4e49\u71b5\u548c\u51e0\u4f55\u4e0d\u900f\u660e\u5ea6\u4f18\u5316\u521d\u59cb\u7ec4\u7684\u6a21\u7cca\u8fb9\u754c\uff0c\u7136\u540e\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6700\u5177\u4ee3\u8868\u6027\u7684\u89c6\u56fe\u4e2d\u89e3\u91ca\u5bf9\u8c61\u5916\u89c2\uff0c\u63d0\u70bc\u9c81\u68d2\u7684\u6587\u672c\u7279\u5f81\u4ee5\u534f\u8c03\u89c6\u89c9\u4e0d\u4e00\u81f4\uff0c\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u67e5\u8be2\u3002", "result": "\u901a\u8fc7\u6d88\u9664\u6602\u8d35\u7684\u9010\u573a\u666f\u8bad\u7ec3\u8fc7\u7a0b\uff0cMUSplat\u5c06\u573a\u666f\u9002\u5e94\u65f6\u95f4\u4ece\u6570\u5c0f\u65f6\u51cf\u5c11\u5230\u51e0\u5206\u949f\u3002\u5728\u5f00\u653e\u8bcd\u6c473D\u5bf9\u8c61\u9009\u62e9\u548c\u8bed\u4e49\u5206\u5272\u7684\u57fa\u51c6\u4efb\u52a1\u4e2d\uff0cMUSplat\u4f18\u4e8e\u5df2\u5efa\u7acb\u7684\u57fa\u4e8e\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u5b83\u4eec\u7684\u5355\u8bed\u4e49\u9650\u5236\u3002", "conclusion": "MUSplat\u6210\u529f\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u652f\u6301\u591a\u8bed\u4e49\u8868\u793a\u4e14\u80fd\u5904\u7406\u8de8\u89c6\u56fe\u8bed\u4e49\u4e0d\u4e00\u81f4\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u57283D\u5f00\u653e\u8bcd\u6c47\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.22276", "pdf": "https://arxiv.org/pdf/2509.22276", "abs": "https://arxiv.org/abs/2509.22276", "authors": ["Dinh Minh Nguyen", "Malte Avenhaus", "Thomas Lindemeier"], "title": "GS-2M: Gaussian Splatting for Joint Mesh Reconstruction and Material Decomposition", "categories": ["cs.CV"], "comment": "13 pages, 10 figures", "summary": "We propose a unified solution for mesh reconstruction and material decomposition from multi-view images based on 3D Gaussian Splatting, referred to as GS-2M. Previous works handle these tasks separately and struggle to reconstruct highly reflective surfaces, often relying on priors from external models to enhance the decomposition results. Conversely, our method addresses these two problems by jointly optimizing attributes relevant to the quality of rendered depth and normals, maintaining geometric details while being resilient to reflective surfaces. Although contemporary works effectively solve these tasks together, they often employ sophisticated neural components to learn scene properties, which hinders their performance at scale. To further eliminate these neural components, we propose a novel roughness supervision strategy based on multi-view photometric variation. When combined with a carefully designed loss and optimization process, our unified framework produces reconstruction results comparable to state-of-the-art methods, delivering triangle meshes and their associated material components for downstream tasks. We validate the effectiveness of our approach with widely used datasets from previous works and qualitative comparisons with state-of-the-art surface reconstruction methods.", "AI": {"tldr": "GS-2M\u662f\u4e00\u4e2a\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u591a\u89c6\u89d2\u56fe\u50cf\u540c\u65f6\u8fdb\u884c\u7f51\u683c\u91cd\u5efa\u548c\u6750\u8d28\u5206\u89e3\uff0c\u7279\u522b\u64c5\u957f\u5904\u7406\u9ad8\u53cd\u5c04\u8868\u9762\u4e14\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u6a21\u578b\u5148\u9a8c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u7f51\u683c\u91cd\u5efa\u548c\u6750\u8d28\u5206\u89e3\u5206\u5f00\u5904\u7406\uff0c\u96be\u4ee5\u91cd\u5efa\u9ad8\u53cd\u5c04\u8868\u9762\uff0c\u4e14\u4f9d\u8d56\u5916\u90e8\u6a21\u578b\u5148\u9a8c\u6765\u63d0\u5347\u5206\u89e3\u6548\u679c\u3002\u540c\u65f6\uff0c\u73b0\u6709\u8054\u5408\u65b9\u6cd5\u4f7f\u7528\u590d\u6742\u7684\u795e\u7ecf\u7ec4\u4ef6\uff0c\u9650\u5236\u4e86\u5176\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "method": "\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\uff0c\u8054\u5408\u4f18\u5316\u4e0e\u6e32\u67d3\u6df1\u5ea6\u548c\u6cd5\u7ebf\u8d28\u91cf\u76f8\u5173\u7684\u5c5e\u6027\uff0c\u63d0\u51fa\u57fa\u4e8e\u591a\u89c6\u89d2\u5149\u5ea6\u53d8\u5316\u7684\u65b0\u578b\u7c97\u7cd9\u5ea6\u76d1\u7763\u7b56\u7565\uff0c\u7ed3\u5408\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u8fc7\u7a0b\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u7f51\u683c\u91cd\u5efa\u7ed3\u679c\uff0c\u63d0\u4f9b\u4e09\u89d2\u5f62\u7f51\u683c\u53ca\u5176\u5173\u8054\u7684\u6750\u8d28\u7ec4\u4ef6\uff0c\u9002\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\u3002\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "GS-2M\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u7f51\u683c\u91cd\u5efa\u548c\u6750\u8d28\u5206\u89e3\uff0c\u5bf9\u9ad8\u53cd\u5c04\u8868\u9762\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4e14\u907f\u514d\u4e86\u590d\u6742\u795e\u7ecf\u7ec4\u4ef6\u7684\u4f7f\u7528\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2509.22292", "pdf": "https://arxiv.org/pdf/2509.22292", "abs": "https://arxiv.org/abs/2509.22292", "authors": ["Wonjun Lee", "Haon Park", "Doehyeon Lee", "Bumsub Ham", "Suhyun Kim"], "title": "Jailbreaking on Text-to-Video Models via Scene Splitting Strategy", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Along with the rapid advancement of numerous Text-to-Video (T2V) models, growing concerns have emerged regarding their safety risks. While recent studies have explored vulnerabilities in models like LLMs, VLMs, and Text-to-Image (T2I) models through jailbreak attacks, T2V models remain largely unexplored, leaving a significant safety gap. To address this gap, we introduce SceneSplit, a novel black-box jailbreak method that works by fragmenting a harmful narrative into multiple scenes, each individually benign. This approach manipulates the generative output space, the abstract set of all potential video outputs for a given prompt, using the combination of scenes as a powerful constraint to guide the final outcome. While each scene individually corresponds to a wide and safe space where most outcomes are benign, their sequential combination collectively restricts this space, narrowing it to an unsafe region and significantly increasing the likelihood of generating a harmful video. This core mechanism is further enhanced through iterative scene manipulation, which bypasses the safety filter within this constrained unsafe region. Additionally, a strategy library that reuses successful attack patterns further improves the attack's overall effectiveness and robustness. To validate our method, we evaluate SceneSplit across 11 safety categories on T2V models. Our results show that it achieves a high average Attack Success Rate (ASR) of 77.2% on Luma Ray2, 84.1% on Hailuo, and 78.2% on Veo2, significantly outperforming the existing baseline. Through this work, we demonstrate that current T2V safety mechanisms are vulnerable to attacks that exploit narrative structure, providing new insights for understanding and improving the safety of T2V models.", "AI": {"tldr": "SceneSplit\u662f\u4e00\u79cd\u65b0\u9896\u7684\u9ed1\u76d2\u8d8a\u72f1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6709\u5bb3\u53d9\u8ff0\u5206\u89e3\u4e3a\u591a\u4e2a\u72ec\u7acb\u826f\u6027\u7684\u573a\u666f\u6765\u7ed5\u8fc7\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u7684\u5b89\u5168\u673a\u5236\u3002", "motivation": "\u968f\u7740\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5176\u5b89\u5168\u98ce\u9669\u65e5\u76ca\u51f8\u663e\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u63a2\u7d22\u4e86LLMs\u3001VLMs\u548c\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u6f0f\u6d1e\uff0c\u4f46\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u7684\u5b89\u5168\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u5c06\u6709\u5bb3\u53d9\u8ff0\u5206\u89e3\u4e3a\u591a\u4e2a\u72ec\u7acb\u826f\u6027\u7684\u573a\u666f\uff0c\u5229\u7528\u573a\u666f\u7ec4\u5408\u7ea6\u675f\u751f\u6210\u7a7a\u95f4\uff1b\u901a\u8fc7\u8fed\u4ee3\u573a\u666f\u64cd\u7eb5\u7ed5\u8fc7\u5b89\u5168\u8fc7\u6ee4\u5668\uff1b\u4f7f\u7528\u7b56\u7565\u5e93\u590d\u7528\u6210\u529f\u653b\u51fb\u6a21\u5f0f\u3002", "result": "\u572811\u4e2a\u5b89\u5168\u7c7b\u522b\u4e0a\u8bc4\u4f30\uff0cSceneSplit\u5728Luma Ray2\u3001Hailuo\u548cVeo2\u6a21\u578b\u4e0a\u5206\u522b\u8fbe\u523077.2%\u300184.1%\u548c78.2%\u7684\u5e73\u5747\u653b\u51fb\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u5f53\u524d\u6587\u672c\u5230\u89c6\u9891\u5b89\u5168\u673a\u5236\u5bb9\u6613\u53d7\u5230\u5229\u7528\u53d9\u4e8b\u7ed3\u6784\u7684\u653b\u51fb\uff0c\u8fd9\u4e3a\u7406\u89e3\u548c\u6539\u8fdb\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2509.22300", "pdf": "https://arxiv.org/pdf/2509.22300", "abs": "https://arxiv.org/abs/2509.22300", "authors": ["Seyedmorteza Sadat", "Farnood Salehi", "Romann M. Weber"], "title": "HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "While diffusion models have made remarkable progress in image generation, their outputs can still appear unrealistic and lack fine details, especially when using fewer number of neural function evaluations (NFEs) or lower guidance scales. To address this issue, we propose a novel momentum-based sampling technique, termed history-guided sampling (HiGS), which enhances quality and efficiency of diffusion sampling by integrating recent model predictions into each inference step. Specifically, HiGS leverages the difference between the current prediction and a weighted average of past predictions to steer the sampling process toward more realistic outputs with better details and structure. Our approach introduces practically no additional computation and integrates seamlessly into existing diffusion frameworks, requiring neither extra training nor fine-tuning. Extensive experiments show that HiGS consistently improves image quality across diverse models and architectures and under varying sampling budgets and guidance scales. Moreover, using a pretrained SiT model, HiGS achieves a new state-of-the-art FID of 1.61 for unguided ImageNet generation at 256$\\times$256 with only 30 sampling steps (instead of the standard 250). We thus present HiGS as a plug-and-play enhancement to standard diffusion sampling that enables faster generation with higher fidelity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u91cf\u7684\u91c7\u6837\u6280\u672fHiGS\uff0c\u901a\u8fc7\u6574\u5408\u8fd1\u671f\u6a21\u578b\u9884\u6d4b\u6765\u63d0\u5347\u6269\u6563\u6a21\u578b\u91c7\u6837\u8d28\u91cf\u548c\u6548\u7387\uff0c\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u6216\u8bad\u7ec3\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u4ecd\u6709\u4e0d\u771f\u5b9e\u548c\u7f3a\u4e4f\u7ec6\u8282\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u8f83\u5c11NFEs\u6216\u8f83\u4f4e\u5f15\u5bfc\u5c3a\u5ea6\u65f6\u3002", "method": "\u5229\u7528\u5f53\u524d\u9884\u6d4b\u4e0e\u8fc7\u53bb\u9884\u6d4b\u52a0\u6743\u5e73\u5747\u4e4b\u95f4\u7684\u5dee\u5f02\u6765\u5f15\u5bfc\u91c7\u6837\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u66f4\u771f\u5b9e\u3001\u7ec6\u8282\u66f4\u4e30\u5bcc\u7684\u8f93\u51fa\u3002", "result": "HiGS\u5728\u5404\u79cd\u6a21\u578b\u548c\u67b6\u6784\u4e0b\u90fd\u80fd\u4e00\u81f4\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\uff0c\u4f7f\u7528SiT\u6a21\u578b\u572830\u6b65\u91c7\u6837\u4e0b\u5b9e\u73b0\u4e861.61\u7684FID\u65b0\u7eaa\u5f55\u3002", "conclusion": "HiGS\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u6269\u6563\u91c7\u6837\u589e\u5f3a\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u5feb\u901f\u5ea6\u3001\u66f4\u9ad8\u4fdd\u771f\u5ea6\u7684\u751f\u6210\u3002"}}
{"id": "2509.22323", "pdf": "https://arxiv.org/pdf/2509.22323", "abs": "https://arxiv.org/abs/2509.22323", "authors": ["Wangbo Zhao", "Yizeng Han", "Zhiwei Tang", "Jiasheng Tang", "Pengfei Zhou", "Kai Wang", "Bohan Zhuang", "Zhangyang Wang", "Fan Wang", "Yang You"], "title": "RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion Transformer", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion Transformers (DiTs) excel at visual generation yet remain hampered by slow sampling. Existing training-free accelerators - step reduction, feature caching, and sparse attention - enhance inference speed but typically rely on a uniform heuristic or a manually designed adaptive strategy for all images, leaving quality on the table. Alternatively, dynamic neural networks offer per-image adaptive acceleration, but their high fine-tuning costs limit broader applicability. To address these limitations, we introduce RAPID3: Tri-Level Reinforced Acceleration Policies for Diffusion Transformers, a framework that delivers image-wise acceleration with zero updates to the base generator. Specifically, three lightweight policy heads - Step-Skip, Cache-Reuse, and Sparse-Attention - observe the current denoising state and independently decide their corresponding speed-up at each timestep. All policy parameters are trained online via Group Relative Policy Optimization (GRPO) while the generator remains frozen. Meanwhile, an adversarially learned discriminator augments the reward signal, discouraging reward hacking by boosting returns only when generated samples stay close to the original model's distribution. Across state-of-the-art DiT backbones, including Stable Diffusion 3 and FLUX, RAPID3 achieves nearly 3x faster sampling with competitive generation quality.", "AI": {"tldr": "RAPID3\u662f\u4e00\u4e2a\u96f6\u66f4\u65b0\u57fa\u7840\u751f\u6210\u5668\u7684\u4e09\u5c42\u6b21\u5f3a\u5316\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u6b65\u957f\u8df3\u8fc7\u3001\u7f13\u5b58\u91cd\u7528\u548c\u7a00\u758f\u6ce8\u610f\u529b\u4e09\u4e2a\u8f7b\u91cf\u7ea7\u7b56\u7565\u5934\u5b9e\u73b0\u56fe\u50cf\u7ea7\u81ea\u9002\u5e94\u52a0\u901f\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u8fd13\u500d\u91c7\u6837\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u53d8\u6362\u5668\u52a0\u901f\u65b9\u6cd5\u8981\u4e48\u4f7f\u7528\u7edf\u4e00\u542f\u53d1\u5f0f\u7b56\u7565\u5bfc\u81f4\u8d28\u91cf\u635f\u5931\uff0c\u8981\u4e48\u9700\u8981\u9ad8\u6210\u672c\u7684\u5fae\u8c03\uff0c\u7f3a\u4e4f\u65e2\u9ad8\u6548\u53c8\u65e0\u9700\u66f4\u65b0\u57fa\u7840\u6a21\u578b\u7684\u56fe\u50cf\u7ea7\u81ea\u9002\u5e94\u52a0\u901f\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u8f7b\u91cf\u7ea7\u7b56\u7565\u5934\uff1a\u6b65\u957f\u8df3\u8fc7\u3001\u7f13\u5b58\u91cd\u7528\u548c\u7a00\u758f\u6ce8\u610f\u529b\uff0c\u901a\u8fc7\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u5728\u7ebf\u8bad\u7ec3\u7b56\u7565\u53c2\u6570\uff0c\u540c\u65f6\u4f7f\u7528\u5bf9\u6297\u6027\u5224\u522b\u5668\u589e\u5f3a\u5956\u52b1\u4fe1\u53f7\u9632\u6b62\u5956\u52b1\u653b\u51fb\u3002", "result": "\u5728\u5305\u62ecStable Diffusion 3\u548cFLUX\u5728\u5185\u7684\u6700\u5148\u8fdbDiT\u9aa8\u5e72\u7f51\u7edc\u4e0a\uff0cRAPID3\u5b9e\u73b0\u4e86\u8fd13\u500d\u91c7\u6837\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "RAPID3\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u9700\u66f4\u65b0\u57fa\u7840\u751f\u6210\u5668\u7684\u56fe\u50cf\u7ea7\u81ea\u9002\u5e94\u52a0\u901f\uff0c\u4e3a\u6269\u6563\u53d8\u6362\u5668\u7684\u9ad8\u6548\u91c7\u6837\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22377", "pdf": "https://arxiv.org/pdf/2509.22377", "abs": "https://arxiv.org/abs/2509.22377", "authors": ["Yasmina Kheddache", "Marc Lalonde"], "title": "Effectiveness of Large Multimodal Models in Detecting Disinformation: Experimental Results", "categories": ["cs.CV"], "comment": "9 pages", "summary": "The proliferation of disinformation, particularly in multimodal contexts combining text and images, presents a significant challenge across digital platforms. This study investigates the potential of large multimodal models (LMMs) in detecting and mitigating false information. We propose to approach multimodal disinformation detection by leveraging the advanced capabilities of the GPT-4o model. Our contributions include: (1) the development of an optimized prompt incorporating advanced prompt engineering techniques to ensure precise and consistent evaluations; (2) the implementation of a structured framework for multimodal analysis, including a preprocessing methodology for images and text to comply with the model's token limitations; (3) the definition of six specific evaluation criteria that enable a fine-grained classification of content, complemented by a self-assessment mechanism based on confidence levels; (4) a comprehensive performance analysis of the model across multiple heterogeneous datasets Gossipcop, Politifact, Fakeddit, MMFakeBench, and AMMEBA highlighting GPT-4o's strengths and limitations in disinformation detection; (5) an investigation of prediction variability through repeated testing, evaluating the stability and reliability of the model's classifications; and (6) the introduction of confidence-level and variability-based evaluation methods. These contributions provide a robust and reproducible methodological framework for automated multimodal disinformation analysis.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528GPT-4o\u6a21\u578b\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u7684\u63d0\u793a\u5de5\u7a0b\u3001\u7ed3\u6784\u5316\u5206\u6790\u65b9\u6cd5\u548c\u516d\u9879\u8bc4\u4f30\u6807\u51c6\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u73af\u5883\u4e0b\u7684\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u65e5\u76ca\u4e25\u91cd\uff0c\u9700\u8981\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u5148\u8fdb\u80fd\u529b\u6765\u68c0\u6d4b\u548c\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528GPT-4o\u6a21\u578b\uff0c\u5f00\u53d1\u4f18\u5316\u63d0\u793a\u5de5\u7a0b\uff0c\u5efa\u7acb\u7ed3\u6784\u5316\u591a\u6a21\u6001\u5206\u6790\u6846\u67b6\uff0c\u5b9a\u4e49\u516d\u9879\u8bc4\u4f30\u6807\u51c6\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6027\u80fd\u5206\u6790\u3002", "result": "\u5728Gossipcop\u3001Politifact\u3001Fakeddit\u3001MMFakeBench\u548cAMMEBA\u7b49\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u6027\u80fd\u5206\u6790\uff0c\u63ed\u793a\u4e86GPT-4o\u5728\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u4e2d\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u4e14\u53ef\u590d\u73b0\u7684\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u81ea\u52a8\u5316\u5206\u6790\u65b9\u6cd5\u8bba\u6846\u67b6\u3002"}}
{"id": "2509.22393", "pdf": "https://arxiv.org/pdf/2509.22393", "abs": "https://arxiv.org/abs/2509.22393", "authors": ["Wenqiang Wang", "Siyuan Liang", "Xiao Yan", "Xiaochun Cao"], "title": "Text Adversarial Attacks with Dynamic Outputs", "categories": ["cs.CV"], "comment": null, "summary": "Text adversarial attack methods are typically designed for static scenarios with fixed numbers of output labels and a predefined label space, relying on extensive querying of the victim model (query-based attacks) or the surrogate model (transfer-based attacks). To address this gap, we introduce the Textual Dynamic Outputs Attack (TDOA) method, which employs a clustering-based surrogate model training approach to convert the dynamic-output scenario into a static single-output scenario. To improve attack effectiveness, we propose the farthest-label targeted attack strategy, which selects adversarial vectors that deviate most from the model's coarse-grained labels, thereby maximizing disruption. We extensively evaluate TDOA on four datasets and eight victim models (e.g., ChatGPT-4o, ChatGPT-4.1), showing its effectiveness in crafting adversarial examples and its strong potential to compromise large language models with limited access. With a single query per text, TDOA achieves a maximum attack success rate of 50.81\\%. Additionally, we find that TDOA also achieves state-of-the-art performance in conventional static output scenarios, reaching a maximum ASR of 82.68\\%. Meanwhile, by conceptualizing translation tasks as classification problems with unbounded output spaces, we extend the TDOA framework to generative settings, surpassing prior results by up to 0.64 RDBLEU and 0.62 RDchrF.", "AI": {"tldr": "\u63d0\u51faTDOA\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u7c7b\u8bad\u7ec3\u4ee3\u7406\u6a21\u578b\u5c06\u52a8\u6001\u8f93\u51fa\u573a\u666f\u8f6c\u6362\u4e3a\u9759\u6001\u5355\u8f93\u51fa\u573a\u666f\uff0c\u5b9e\u73b0\u6587\u672c\u5bf9\u6297\u653b\u51fb\uff0c\u5728\u52a8\u6001\u548c\u9759\u6001\u573a\u666f\u4e0b\u5747\u53d6\u5f97\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u9759\u6001\u573a\u666f\uff0c\u65e0\u6cd5\u5904\u7406\u52a8\u6001\u8f93\u51fa\u6807\u7b7e\u7a7a\u95f4\u7684\u95ee\u9898\uff0c\u9700\u8981\u8bbe\u8ba1\u9002\u7528\u4e8e\u52a8\u6001\u8f93\u51fa\u573a\u666f\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u805a\u7c7b\u7684\u4ee3\u7406\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5c06\u52a8\u6001\u8f93\u51fa\u8f6c\u6362\u4e3a\u9759\u6001\u5355\u8f93\u51fa\uff1b\u63d0\u51fa\u6700\u8fdc\u6807\u7b7e\u76ee\u6807\u653b\u51fb\u7b56\u7565\uff0c\u9009\u62e9\u504f\u79bb\u6a21\u578b\u7c97\u7c92\u5ea6\u6807\u7b7e\u6700\u8fdc\u7684\u5bf9\u6297\u5411\u91cf\u6765\u6700\u5927\u5316\u5e72\u6270\u6548\u679c\u3002", "result": "\u57284\u4e2a\u6570\u636e\u96c6\u548c8\u4e2a\u53d7\u5bb3\u8005\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0c\u5355\u6b21\u67e5\u8be2\u6700\u5927\u653b\u51fb\u6210\u529f\u738750.81%\uff1b\u5728\u9759\u6001\u573a\u666f\u4e0b\u6700\u5927ASR\u8fbe82.68%\uff1b\u5728\u751f\u6210\u5f0f\u8bbe\u7f6e\u4e2d\u8d85\u8d8a\u5148\u524d\u7ed3\u679c0.64 RDBLEU\u548c0.62 RDchrF\u3002", "conclusion": "TDOA\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u8f93\u51fa\u573a\u666f\u7684\u6587\u672c\u5bf9\u6297\u653b\u51fb\u95ee\u9898\uff0c\u5728\u6709\u9650\u8bbf\u95ee\u6761\u4ef6\u4e0b\u5177\u6709\u7834\u574f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.22400", "pdf": "https://arxiv.org/pdf/2509.22400", "abs": "https://arxiv.org/abs/2509.22400", "authors": ["Xinhao Zhong", "Yimin Zhou", "Zhiqi Zhang", "Junhao Li", "Yi Sun", "Bin Chen", "Shu-Tao Xia", "Ke Xu"], "title": "Closing the Safety Gap: Surgical Concept Erasure in Visual Autoregressive Models", "categories": ["cs.CV"], "comment": null, "summary": "The rapid progress of visual autoregressive (VAR) models has brought new opportunities for text-to-image generation, but also heightened safety concerns. Existing concept erasure techniques, primarily designed for diffusion models, fail to generalize to VARs due to their next-scale token prediction paradigm. In this paper, we first propose a novel VAR Erasure framework VARE that enables stable concept erasure in VAR models by leveraging auxiliary visual tokens to reduce fine-tuning intensity. Building upon this, we introduce S-VARE, a novel and effective concept erasure method designed for VAR, which incorporates a filtered cross entropy loss to precisely identify and minimally adjust unsafe visual tokens, along with a preservation loss to maintain semantic fidelity, addressing the issues such as language drift and reduced diversity introduce by na\\\"ive fine-tuning. Extensive experiments demonstrate that our approach achieves surgical concept erasure while preserving generation quality, thereby closing the safety gap in autoregressive text-to-image generation by earlier methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86VARE\u6846\u67b6\u548cS-VARE\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6982\u5ff5\u64e6\u9664\u95ee\u9898\uff0c\u901a\u8fc7\u8f85\u52a9\u89c6\u89c9\u4ee4\u724c\u548c\u8fc7\u6ee4\u4ea4\u53c9\u71b5\u635f\u5931\u5b9e\u73b0\u7cbe\u51c6\u6982\u5ff5\u79fb\u9664\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u65b0\u673a\u9047\uff0c\u4f46\u4e5f\u52a0\u5267\u4e86\u5b89\u5168\u62c5\u5fe7\u3002\u73b0\u6709\u7684\u6982\u5ff5\u64e6\u9664\u6280\u672f\u4e3b\u8981\u9488\u5bf9\u6269\u6563\u6a21\u578b\uff0c\u65e0\u6cd5\u9002\u7528\u4e8eVAR\u6a21\u578b\u7684\u4e0b\u4e00\u5c3a\u5ea6\u4ee4\u724c\u9884\u6d4b\u8303\u5f0f\u3002", "method": "\u9996\u5148\u63d0\u51faVARE\u6846\u67b6\uff0c\u5229\u7528\u8f85\u52a9\u89c6\u89c9\u4ee4\u724c\u964d\u4f4e\u5fae\u8c03\u5f3a\u5ea6\uff1b\u7136\u540e\u5f15\u5165S-VARE\u65b9\u6cd5\uff0c\u7ed3\u5408\u8fc7\u6ee4\u4ea4\u53c9\u71b5\u635f\u5931\u7cbe\u786e\u8bc6\u522b\u548c\u6700\u5c0f\u5316\u8c03\u6574\u4e0d\u5b89\u5168\u89c6\u89c9\u4ee4\u724c\uff0c\u4ee5\u53ca\u4fdd\u6301\u635f\u5931\u6765\u7ef4\u6301\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5916\u79d1\u624b\u672f\u5f0f\u7684\u6982\u5ff5\u64e6\u9664\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u5f25\u8865\u4e86\u65e9\u671f\u65b9\u6cd5\u5728\u81ea\u56de\u5f52\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5b89\u5168\u5dee\u8ddd\u3002", "conclusion": "\u63d0\u51fa\u7684VARE\u548cS-VARE\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86VAR\u6a21\u578b\u7684\u6982\u5ff5\u64e6\u9664\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u7cbe\u51c6\u5b89\u5168\u63a7\u5236\u3002"}}
{"id": "2509.22414", "pdf": "https://arxiv.org/pdf/2509.22414", "abs": "https://arxiv.org/abs/2509.22414", "authors": ["Song Fei", "Tian Ye", "Lujia Wang", "Lei Zhu"], "title": "LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale Diffusion Transformer", "categories": ["cs.CV"], "comment": "Project Page: https://w2genai-lab.github.io/LucidFlux", "summary": "Universal image restoration (UIR) aims to recover images degraded by unknown mixtures while preserving semantics -- conditions under which discriminative restorers and UNet-based diffusion priors often oversmooth, hallucinate, or drift. We present LucidFlux, a caption-free UIR framework that adapts a large diffusion transformer (Flux.1) without image captions. LucidFlux introduces a lightweight dual-branch conditioner that injects signals from the degraded input and a lightly restored proxy to respectively anchor geometry and suppress artifacts. Then, a timestep- and layer-adaptive modulation schedule is designed to route these cues across the backbone's hierarchy, in order to yield coarse-to-fine and context-aware updates that protect the global structure while recovering texture. After that, to avoid the latency and instability of text prompts or MLLM captions, we enforce caption-free semantic alignment via SigLIP features extracted from the proxy. A scalable curation pipeline further filters large-scale data for structure-rich supervision. Across synthetic and in-the-wild benchmarks, LucidFlux consistently outperforms strong open-source and commercial baselines, and ablation studies verify the necessity of each component. LucidFlux shows that, for large DiTs, when, where, and what to condition on -- rather than adding parameters or relying on text prompts -- is the governing lever for robust and caption-free universal image restoration in the wild.", "AI": {"tldr": "LucidFlux\u662f\u4e00\u4e2a\u65e0\u9700\u56fe\u50cf\u63cf\u8ff0\u7684\u901a\u7528\u56fe\u50cf\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u6761\u4ef6\u5668\u548c\u81ea\u9002\u5e94\u8c03\u5236\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u5168\u5c40\u7ed3\u6784\u7684\u540c\u65f6\u6062\u590d\u7eb9\u7406\u7ec6\u8282\uff0c\u65e0\u9700\u6587\u672c\u63d0\u793a\u5373\u53ef\u5b9e\u73b0\u8bed\u4e49\u5bf9\u9f50\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u5728\u672a\u77e5\u6df7\u5408\u9000\u5316\u60c5\u51b5\u4e0b\u5bb9\u6613\u4ea7\u751f\u8fc7\u5ea6\u5e73\u6ed1\u3001\u5e7b\u89c9\u6216\u8bed\u4e49\u6f02\u79fb\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u907f\u514d\u5bf9\u56fe\u50cf\u63cf\u8ff0\u6216\u6587\u672c\u63d0\u793a\u7684\u4f9d\u8d56\u3002", "method": "\u4f7f\u7528\u5927\u578b\u6269\u6563\u53d8\u6362\u5668(Flux.1)\uff0c\u5f15\u5165\u8f7b\u91cf\u7ea7\u53cc\u5206\u652f\u6761\u4ef6\u5668\u6ce8\u5165\u9000\u5316\u8f93\u5165\u548c\u8f7b\u5ea6\u4fee\u590d\u4ee3\u7406\u4fe1\u53f7\uff0c\u8bbe\u8ba1\u65f6\u95f4\u6b65\u548c\u5c42\u7ea7\u81ea\u9002\u5e94\u8c03\u5236\u7b56\u7565\uff0c\u901a\u8fc7SigLIP\u7279\u5f81\u5b9e\u73b0\u65e0\u63cf\u8ff0\u8bed\u4e49\u5bf9\u9f50\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLucidFlux\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u5927\u7684\u5f00\u6e90\u548c\u5546\u4e1a\u57fa\u7ebf\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u5bf9\u4e8e\u5927\u578bDiT\u6a21\u578b\uff0c\u4f55\u65f6\u3001\u4f55\u5730\u4ee5\u53ca\u5982\u4f55\u6ce8\u5165\u6761\u4ef6\u4fe1\u606f\u2014\u2014\u800c\u975e\u589e\u52a0\u53c2\u6570\u6216\u4f9d\u8d56\u6587\u672c\u63d0\u793a\u2014\u2014\u662f\u5b9e\u73b0\u9c81\u68d2\u4e14\u65e0\u9700\u63cf\u8ff0\u7684\u901a\u7528\u56fe\u50cf\u4fee\u590d\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2509.22476", "pdf": "https://arxiv.org/pdf/2509.22476", "abs": "https://arxiv.org/abs/2509.22476", "authors": ["Chen Li", "Meilong Xu", "Xiaoling Hu", "Weimin Lyu", "Chao Chen"], "title": "B\u00e9zier Meets Diffusion: Robust Generation Across Domains for Medical Image Segmentation", "categories": ["cs.CV"], "comment": "17 pages, 7 figures", "summary": "Training robust learning algorithms across different medical imaging modalities is challenging due to the large domain gap. Unsupervised domain adaptation (UDA) mitigates this problem by using annotated images from the source domain and unlabeled images from the target domain to train the deep models. Existing approaches often rely on GAN-based style transfer, but these methods struggle to capture cross-domain mappings in regions with high variability. In this paper, we propose a unified framework, B\\'ezier Meets Diffusion, for cross-domain image generation. First, we introduce a B\\'ezier-curve-based style transfer strategy that effectively reduces the domain gap between source and target domains. The transferred source images enable the training of a more robust segmentation model across domains. Thereafter, using pseudo-labels generated by this segmentation model on the target domain, we train a conditional diffusion model (CDM) to synthesize high-quality, labeled target-domain images. To mitigate the impact of noisy pseudo-labels, we further develop an uncertainty-guided score matching method that improves the robustness of CDM training. Extensive experiments on public datasets demonstrate that our approach generates realistic labeled images, significantly augmenting the target domain and improving segmentation performance.", "AI": {"tldr": "\u63d0\u51faB\u00e9zier Meets Diffusion\u6846\u67b6\uff0c\u901a\u8fc7B\u00e9zier\u66f2\u7ebf\u98ce\u683c\u8fc1\u79fb\u548c\u6761\u4ef6\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u76ee\u6807\u57df\u6807\u6ce8\u56fe\u50cf\uff0c\u63d0\u5347\u8de8\u57df\u5206\u5272\u6027\u80fd", "motivation": "\u89e3\u51b3\u533b\u5b66\u5f71\u50cf\u4e2d\u4e0d\u540c\u6a21\u6001\u95f4\u57df\u5dee\u8ddd\u5927\u7684\u95ee\u9898\uff0c\u73b0\u6709GAN\u65b9\u6cd5\u5728\u9ad8\u5ea6\u53ef\u53d8\u533a\u57df\u96be\u4ee5\u6355\u6349\u8de8\u57df\u6620\u5c04", "method": "1. \u57fa\u4e8eB\u00e9zier\u66f2\u7ebf\u7684\u98ce\u683c\u8fc1\u79fb\u7b56\u7565\u51cf\u5c11\u57df\u5dee\u8ddd\uff1b2. \u4f7f\u7528\u5206\u5272\u6a21\u578b\u751f\u6210\u4f2a\u6807\u7b7e\uff1b3. \u8bad\u7ec3\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5408\u6210\u76ee\u6807\u57df\u56fe\u50cf\uff1b4. \u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u5206\u6570\u5339\u914d\u65b9\u6cd5\u63d0\u9ad8\u9c81\u68d2\u6027", "result": "\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u903c\u771f\u7684\u6807\u6ce8\u56fe\u50cf\uff0c\u663e\u8457\u589e\u5f3a\u76ee\u6807\u57df\u6570\u636e\u5e76\u6539\u5584\u5206\u5272\u6027\u80fd", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u8de8\u57df\u533b\u5b66\u56fe\u50cf\u751f\u6210\u95ee\u9898\uff0c\u4e3a\u57df\u81ea\u9002\u5e94\u5206\u5272\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2509.22481", "pdf": "https://arxiv.org/pdf/2509.22481", "abs": "https://arxiv.org/abs/2509.22481", "authors": ["Xiangmo Zhao", "Nan Yang", "Yang Wang", "Zhanwen Liu"], "title": "PSTTS: A Plug-and-Play Token Selector for Efficient Event-based Spatio-temporal Representation Learning", "categories": ["cs.CV"], "comment": null, "summary": "Mainstream event-based spatio-temporal representation learning methods typically process event streams by converting them into sequences of event frames, achieving remarkable performance. However, they neglect the high spatial sparsity and inter-frame motion redundancy inherent in event frame sequences, leading to significant computational overhead. Existing token sparsification methods for RGB videos rely on unreliable intermediate token representations and neglect the influence of event noise, making them ineffective for direct application to event data. In this paper, we propose Progressive Spatio-Temporal Token Selection (PSTTS), a Plug-and-Play module for event data without introducing any additional parameters. PSTTS exploits the spatio-temporal distribution characteristics embedded in raw event data to effectively identify and discard spatio-temporal redundant tokens, achieving an optimal trade-off between accuracy and efficiency. Specifically, PSTTS consists of two stages, Spatial Token Purification and Temporal Token Selection. Spatial Token Purification discards noise and non-event regions by assessing the spatio-temporal consistency of events within each event frame to prevent interference with subsequent temporal redundancy evaluation. Temporal Token Selection evaluates the motion pattern similarity between adjacent event frames, precisely identifying and removing redundant temporal information. We apply PSTTS to four representative backbones UniformerV2, VideoSwin, EVMamba, and ExACT on the HARDVS, DailyDVS-200, and SeACT datasets. Experimental results demonstrate that PSTTS achieves significant efficiency improvements. Specifically, PSTTS reduces FLOPs by 29-43.6% and increases FPS by 21.6-41.3% on the DailyDVS-200 dataset, while maintaining task accuracy. Our code will be available.", "AI": {"tldr": "\u63d0\u51faPSTTS\u6a21\u5757\uff0c\u901a\u8fc7\u7a7a\u95f4\u4ee4\u724c\u51c0\u5316\u548c\u65f6\u95f4\u4ee4\u724c\u9009\u62e9\u6d88\u9664\u4e8b\u4ef6\u6570\u636e\u4e2d\u7684\u65f6\u7a7a\u5197\u4f59\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4e8b\u4ef6\u5e27\u7684\u65b9\u6cd5\u5ffd\u7565\u4e86\u4e8b\u4ef6\u5e27\u5e8f\u5217\u7684\u9ad8\u7a7a\u95f4\u7a00\u758f\u6027\u548c\u5e27\u95f4\u8fd0\u52a8\u5197\u4f59\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5927\uff1b\u73b0\u6709RGB\u89c6\u9891\u7684\u4ee4\u724c\u7a00\u758f\u5316\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u4e8b\u4ef6\u6570\u636e", "method": "PSTTS\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a\u7a7a\u95f4\u4ee4\u724c\u51c0\u5316\u901a\u8fc7\u8bc4\u4f30\u4e8b\u4ef6\u5e27\u5185\u4e8b\u4ef6\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\u6765\u4e22\u5f03\u566a\u58f0\u548c\u975e\u4e8b\u4ef6\u533a\u57df\uff1b\u65f6\u95f4\u4ee4\u724c\u9009\u62e9\u901a\u8fc7\u8bc4\u4f30\u76f8\u90bb\u4e8b\u4ef6\u5e27\u95f4\u7684\u8fd0\u52a8\u6a21\u5f0f\u76f8\u4f3c\u6027\u6765\u8bc6\u522b\u548c\u79fb\u9664\u5197\u4f59\u65f6\u95f4\u4fe1\u606f", "result": "\u5728HARDVS\u3001DailyDVS-200\u548cSeACT\u6570\u636e\u96c6\u4e0a\uff0cPSTTS\u663e\u8457\u63d0\u5347\u6548\u7387\uff1a\u5728DailyDVS-200\u4e0a\u51cf\u5c11FLOPs 29-43.6%\uff0c\u63d0\u5347FPS 21.6-41.3%\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u7cbe\u5ea6", "conclusion": "PSTTS\u5229\u7528\u4e8b\u4ef6\u6570\u636e\u7684\u65f6\u7a7a\u5206\u5e03\u7279\u6027\u6709\u6548\u8bc6\u522b\u548c\u4e22\u5f03\u65f6\u7a7a\u5197\u4f59\u4ee4\u724c\uff0c\u5b9e\u73b0\u4e86\u7cbe\u5ea6\u548c\u6548\u7387\u7684\u6700\u4f73\u5e73\u8861\uff0c\u662f\u4e00\u4e2a\u65e0\u9700\u989d\u5916\u53c2\u6570\u7684\u5373\u63d2\u5373\u7528\u6a21\u5757"}}
{"id": "2509.22485", "pdf": "https://arxiv.org/pdf/2509.22485", "abs": "https://arxiv.org/abs/2509.22485", "authors": ["Guohui Zhang", "Hu Yu", "Xiaoxiao Ma", "JingHao Zhang", "Yaning Pan", "Mingde Yao", "Jie Xiao", "Linjiang Huang", "Feng Zhao"], "title": "Group Critical-token Policy Optimization for Autoregressive Image Generation", "categories": ["cs.CV"], "comment": "Code is available at https://github.com/zghhui/GCPO", "summary": "Recent studies have extended Reinforcement Learning with Verifiable Rewards (RLVR) to autoregressive (AR) visual generation and achieved promising progress. However, existing methods typically apply uniform optimization across all image tokens, while the varying contributions of different image tokens for RLVR's training remain unexplored. In fact, the key obstacle lies in how to identify more critical image tokens during AR generation and implement effective token-wise optimization for them. To tackle this challenge, we propose $\\textbf{G}$roup $\\textbf{C}$ritical-token $\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{GCPO}$), which facilitates effective policy optimization on critical tokens. We identify the critical tokens in RLVR-based AR generation from three perspectives, specifically: $\\textbf{(1)}$ Causal dependency: early tokens fundamentally determine the later tokens and final image effect due to unidirectional dependency; $\\textbf{(2)}$ Entropy-induced spatial structure: tokens with high entropy gradients correspond to image structure and bridges distinct visual regions; $\\textbf{(3)}$ RLVR-focused token diversity: tokens with low visual similarity across a group of sampled images contribute to richer token-level diversity. For these identified critical tokens, we further introduce a dynamic token-wise advantage weight to encourage exploration, based on confidence divergence between the policy model and reference model. By leveraging 30\\% of the image tokens, GCPO achieves better performance than GRPO with full tokens. Extensive experiments on multiple text-to-image benchmarks for both AR models and unified multimodal models demonstrate the effectiveness of GCPO for AR visual generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86GCPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u5173\u952e\u56fe\u50cftoken\u5e76\u8fdb\u884c\u9488\u5bf9\u6027\u4f18\u5316\uff0c\u5728AR\u89c6\u89c9\u751f\u6210\u4e2d\u4ec5\u4f7f\u752830%\u7684token\u5c31\u80fd\u8fbe\u5230\u6bd4\u5168token\u4f18\u5316\u66f4\u597d\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5bf9\u6240\u6709\u56fe\u50cftoken\u8fdb\u884c\u5747\u5300\u4f18\u5316\uff0c\u4f46\u4e0d\u540ctoken\u5bf9RLVR\u8bad\u7ec3\u7684\u8d21\u732e\u4e0d\u540c\uff0c\u9700\u8981\u8bc6\u522b\u5173\u952etoken\u5e76\u5b9e\u65bd\u6709\u6548\u7684token\u7ea7\u4f18\u5316\u3002", "method": "\u4ece\u4e09\u4e2a\u89d2\u5ea6\u8bc6\u522b\u5173\u952etoken\uff1a\u56e0\u679c\u4f9d\u8d56\u3001\u71b5\u8bf1\u5bfc\u7684\u7a7a\u95f4\u7ed3\u6784\u3001RLVR\u805a\u7126\u7684token\u591a\u6837\u6027\u3002\u5f15\u5165\u52a8\u6001token\u7ea7\u4f18\u52bf\u6743\u91cd\u6765\u9f13\u52b1\u63a2\u7d22\u3002", "result": "\u5728\u591a\u4e2a\u6587\u672c\u5230\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGCPO\u4ec5\u4f7f\u752830%\u7684token\u5c31\u6bd4GRPO\u5168token\u4f18\u5316\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "GCPO\u901a\u8fc7\u5173\u952etoken\u8bc6\u522b\u548c\u9488\u5bf9\u6027\u4f18\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86AR\u89c6\u89c9\u751f\u6210\u7684\u6027\u80fd\u3002"}}
{"id": "2509.22615", "pdf": "https://arxiv.org/pdf/2509.22615", "abs": "https://arxiv.org/abs/2509.22615", "authors": ["Yasmine Omri", "Connor Ding", "Tsachy Weissman", "Thierry Tambe"], "title": "Vision-Language Alignment from Compressed Image Representations using 2D Gaussian Splatting", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Modern vision language pipelines are driven by RGB vision encoders trained on massive image text corpora. While these pipelines have enabled impressive zero shot capabilities and strong transfer across tasks, they still inherit two structural inefficiencies from the pixel domain: (i) transmitting dense RGB images from edge devices to the cloud is energy intensive and costly, and (ii) patch based tokenization explodes sequence length, stressing attention budgets and context limits. We explore 2D Gaussian Splatting (2DGS) as an alternative visual substrate for alignment: a compact, spatially adaptive representation that parameterizes images by a set of colored anisotropic Gaussians. We develop a scalable 2DGS pipeline with structured initialization, luminance aware pruning, and batched CUDA kernels, achieving over 90x faster fitting and about 97% GPU utilization compared to prior implementations. We further adapt contrastive language image pretraining (CLIP) to 2DGS by reusing a frozen RGB-based transformer backbone with a lightweight splat aware input stem and a perceiver resampler, training only about 7% of the total parameters. On large DataComp subsets, GS encoders yield meaningful zero shot ImageNet-1K performance while compressing inputs 3 to 20x relative to pixels. While accuracy currently trails RGB encoders, our results establish 2DGS as a viable multimodal substrate, pinpoint architectural bottlenecks, and open a path toward representations that are both semantically powerful and transmission efficient for edge cloud learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u7d22\u4f7f\u75282D\u9ad8\u65af\u6cfc\u6e85(2DGS)\u4f5c\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u66ff\u4ee3\u89c6\u89c9\u8868\u793a\uff0c\u76f8\u6bd4\u4f20\u7edfRGB\u56fe\u50cf\uff0c2DGS\u80fd\u663e\u8457\u538b\u7f29\u8f93\u5165\u6570\u636e(3-20\u500d)\u5e76\u51cf\u5c11\u5e8f\u5217\u957f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u610f\u4e49\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfRGB\u89c6\u89c9\u7f16\u7801\u5668\u7684\u4e24\u4e2a\u7ed3\u6784\u6548\u7387\u95ee\u9898\uff1a(i)\u4ece\u8fb9\u7f18\u8bbe\u5907\u4f20\u8f93\u5bc6\u96c6RGB\u56fe\u50cf\u5230\u4e91\u7aef\u80fd\u8017\u9ad8\u3001\u6210\u672c\u5927\uff1b(ii)\u57fa\u4e8e\u8865\u4e01\u7684\u6807\u8bb0\u5316\u5bfc\u81f4\u5e8f\u5217\u957f\u5ea6\u7206\u70b8\uff0c\u7ed9\u6ce8\u610f\u529b\u673a\u5236\u5e26\u6765\u538b\u529b\u3002", "method": "\u5f00\u53d1\u53ef\u6269\u5c55\u76842DGS\u6d41\u6c34\u7ebf\uff0c\u5305\u62ec\u7ed3\u6784\u5316\u521d\u59cb\u5316\u3001\u4eae\u5ea6\u611f\u77e5\u526a\u679d\u548c\u6279\u5904\u7406CUDA\u5185\u6838\uff1b\u901a\u8fc7\u8f7b\u91cf\u7ea7splat\u611f\u77e5\u8f93\u5165\u4e3b\u5e72\u548c\u611f\u77e5\u5668\u91cd\u91c7\u6837\u5668\uff0c\u5c06\u5bf9\u6bd4\u8bed\u8a00\u56fe\u50cf\u9884\u8bad\u7ec3(CLIP)\u9002\u914d\u52302DGS\u8868\u793a\u3002", "result": "2DGS\u7f16\u7801\u5668\u5728DataComp\u5b50\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6709\u610f\u4e49\u7684\u96f6\u6837\u672cImageNet-1K\u6027\u80fd\uff0c\u8f93\u5165\u538b\u7f29\u6bd4\u8fbe\u52303-20\u500d\uff0c\u4ec5\u8bad\u7ec3\u603b\u53c2\u6570\u7684\u7ea67%\uff0c\u62df\u5408\u901f\u5ea6\u6bd4\u5148\u524d\u5b9e\u73b0\u5feb90\u500d\u4ee5\u4e0a\uff0cGPU\u5229\u7528\u7387\u7ea697%\u3002", "conclusion": "2DGS\u88ab\u786e\u7acb\u4e3a\u53ef\u884c\u7684\u591a\u6a21\u6001\u57fa\u7840\uff0c\u867d\u7136\u5f53\u524d\u51c6\u786e\u6027\u4ecd\u843d\u540e\u4e8eRGB\u7f16\u7801\u5668\uff0c\u4f46\u4e3a\u5f00\u53d1\u65e2\u8bed\u4e49\u5f3a\u5927\u53c8\u4f20\u8f93\u9ad8\u6548\u7684\u8fb9\u7f18-\u4e91\u7aef\u5b66\u4e60\u8868\u793a\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2509.22622", "pdf": "https://arxiv.org/pdf/2509.22622", "abs": "https://arxiv.org/abs/2509.22622", "authors": ["Shuai Yang", "Wei Huang", "Ruihang Chu", "Yicheng Xiao", "Yuyang Zhao", "Xianbang Wang", "Muyang Li", "Enze Xie", "Yingcong Chen", "Yao Lu", "Song Han", "Yukang Chen"], "title": "LongLive: Real-time Interactive Long Video Generation", "categories": ["cs.CV"], "comment": "Code, model, and demos are available at   https://github.com/NVlabs/LongLive", "summary": "We present LongLive, a frame-level autoregressive (AR) framework for real-time and interactive long video generation. Long video generation presents challenges in both efficiency and quality. Diffusion and Diffusion-Forcing models can produce high-quality videos but suffer from low efficiency due to bidirectional attention. Causal attention AR models support KV caching for faster inference, but often degrade in quality on long videos due to memory challenges during long-video training. In addition, beyond static prompt-based generation, interactive capabilities, such as streaming prompt inputs, are critical for dynamic content creation, enabling users to guide narratives in real time. This interactive requirement significantly increases complexity, especially in ensuring visual consistency and semantic coherence during prompt transitions. To address these challenges, LongLive adopts a causal, frame-level AR design that integrates a KV-recache mechanism that refreshes cached states with new prompts for smooth, adherent switches; streaming long tuning to enable long video training and to align training and inference (train-long-test-long); and short window attention paired with a frame-level attention sink, shorten as frame sink, preserving long-range consistency while enabling faster generation. With these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model to minute-long generation in just 32 GPU-days. At inference, LongLive sustains 20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both short and long videos. LongLive supports up to 240-second videos on a single H100 GPU. LongLive further supports INT8-quantized inference with only marginal quality loss.", "AI": {"tldr": "LongLive\u662f\u4e00\u4e2a\u5b9e\u65f6\u4ea4\u4e92\u5f0f\u957f\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u91c7\u7528\u5e27\u7ea7\u81ea\u56de\u5f52\u8bbe\u8ba1\uff0c\u901a\u8fc7KV\u91cd\u7f13\u5b58\u673a\u5236\u3001\u6d41\u5f0f\u957f\u8c03\u4f18\u548c\u5e27\u7ea7\u6ce8\u610f\u529b\u6c47\u805a\u6280\u672f\uff0c\u5728\u5355H100 GPU\u4e0a\u5b9e\u73b020.7 FPS\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u652f\u6301240\u79d2\u957f\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u957f\u89c6\u9891\u751f\u6210\u5728\u6548\u7387\u548c\u8d28\u91cf\u4e0a\u7684\u6311\u6218\uff1a\u6269\u6563\u6a21\u578b\u6548\u7387\u4f4e\uff0c\u56e0\u679c\u6ce8\u610f\u529b\u6a21\u578b\u5728\u957f\u89c6\u9891\u4e0a\u8d28\u91cf\u4e0b\u964d\uff0c\u540c\u65f6\u9700\u8981\u652f\u6301\u5b9e\u65f6\u4ea4\u4e92\u5f0f\u63d0\u793a\u8f93\u5165\u4ee5\u589e\u5f3a\u52a8\u6001\u5185\u5bb9\u521b\u4f5c\u80fd\u529b\u3002", "method": "\u91c7\u7528\u56e0\u679c\u5e27\u7ea7\u81ea\u56de\u5f52\u8bbe\u8ba1\uff0c\u96c6\u6210KV\u91cd\u7f13\u5b58\u673a\u5236\u5b9e\u73b0\u5e73\u6ed1\u63d0\u793a\u5207\u6362\uff0c\u6d41\u5f0f\u957f\u8c03\u4f18\u5bf9\u9f50\u8bad\u7ec3\u4e0e\u63a8\u7406\uff0c\u77ed\u7a97\u53e3\u6ce8\u610f\u529b\u914d\u5408\u5e27\u7ea7\u6ce8\u610f\u529b\u6c47\u805a\u4fdd\u6301\u957f\u7a0b\u4e00\u81f4\u6027\u3002", "result": "\u4ec5\u752832 GPU\u5929\u5fae\u8c031.3B\u53c2\u6570\u77ed\u7247\u6bb5\u6a21\u578b\u5b9e\u73b0\u5206\u949f\u7ea7\u751f\u6210\uff0c\u5355H100 GPU\u63a8\u7406\u901f\u5ea6\u8fbe20.7 FPS\uff0c\u652f\u6301240\u79d2\u89c6\u9891\uff0cINT8\u91cf\u5316\u63a8\u7406\u8d28\u91cf\u635f\u5931\u5fae\u5c0f\uff0c\u5728VBench\u4e0a\u77ed\u957f\u89c6\u9891\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "LongLive\u6210\u529f\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u751f\u6210\u7684\u6548\u7387\u3001\u8d28\u91cf\u548c\u4ea4\u4e92\u6027\u6311\u6218\uff0c\u4e3a\u5b9e\u65f6\u52a8\u6001\u5185\u5bb9\u521b\u4f5c\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22635", "pdf": "https://arxiv.org/pdf/2509.22635", "abs": "https://arxiv.org/abs/2509.22635", "authors": ["Luc Boudier", "Loris Manganelli", "Eleftherios Tsonis", "Nicolas Dufour", "Vicky Kalogeiton"], "title": "Training-Free Synthetic Data Generation with Dual IP-Adapter Guidance", "categories": ["cs.CV", "cs.LG"], "comment": "BMVC 2025. Project page:   https://www.lix.polytechnique.fr/vista/projects/2025_bmvc_dipsy/", "summary": "Few-shot image classification remains challenging due to the limited availability of labeled examples. Recent approaches have explored generating synthetic training data using text-to-image diffusion models, but often require extensive model fine-tuning or external information sources. We present a novel training-free approach, called DIPSY, that leverages IP-Adapter for image-to-image translation to generate highly discriminative synthetic images using only the available few-shot examples. DIPSY introduces three key innovations: (1) an extended classifier-free guidance scheme that enables independent control over positive and negative image conditioning; (2) a class similarity-based sampling strategy that identifies effective contrastive examples; and (3) a simple yet effective pipeline that requires no model fine-tuning or external captioning and filtering. Experiments across ten benchmark datasets demonstrate that our approach achieves state-of-the-art or comparable performance, while eliminating the need for generative model adaptation or reliance on external tools for caption generation and image filtering. Our results highlight the effectiveness of leveraging dual image prompting with positive-negative guidance for generating class-discriminative features, particularly for fine-grained classification tasks.", "AI": {"tldr": "\u63d0\u51faDIPSY\u65b9\u6cd5\uff0c\u5229\u7528IP-Adapter\u8fdb\u884c\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\uff0c\u65e0\u9700\u6a21\u578b\u5fae\u8c03\u5373\u53ef\u751f\u6210\u5224\u522b\u6027\u5408\u6210\u56fe\u50cf\uff0c\u5728\u5c11\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd", "motivation": "\u89e3\u51b3\u5c11\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u4e2d\u6807\u8bb0\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\uff0c\u907f\u514d\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u6a21\u578b\u5fae\u8c03\u6216\u5916\u90e8\u4fe1\u606f\u6e90\u7684\u4f9d\u8d56", "method": "\u4f7f\u7528IP-Adapter\u8fdb\u884c\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\uff0c\u5f15\u5165\u6269\u5c55\u7684\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u65b9\u6848\u3001\u57fa\u4e8e\u7c7b\u522b\u76f8\u4f3c\u5ea6\u7684\u91c7\u6837\u7b56\u7565\uff0c\u65e0\u9700\u6a21\u578b\u5fae\u8c03\u6216\u5916\u90e8\u5de5\u5177", "result": "\u5728\u5341\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6216\u53ef\u6bd4\u6027\u80fd\uff0c\u7279\u522b\u5728\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa", "conclusion": "DIPSY\u901a\u8fc7\u6b63\u8d1f\u5f15\u5bfc\u7684\u53cc\u91cd\u56fe\u50cf\u63d0\u793a\u6709\u6548\u751f\u6210\u7c7b\u522b\u5224\u522b\u7279\u5f81\uff0c\u4e3a\u5c11\u6837\u672c\u5206\u7c7b\u63d0\u4f9b\u9ad8\u6548\u8bad\u7ec3\u514d\u8d39\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.22646", "pdf": "https://arxiv.org/pdf/2509.22646", "abs": "https://arxiv.org/abs/2509.22646", "authors": ["Xingyu Fu", "Siyi Liu", "Yinuo Xu", "Pan Lu", "Guangqiuse Hu", "Tianbo Yang", "Taran Anantasagar", "Christopher Shen", "Yikai Mao", "Yuanzhe Liu", "Keyush Shah", "Chung Un Lee", "Yejin Choi", "James Zou", "Dan Roth", "Chris Callison-Burch"], "title": "Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project Page: https://deeptracereward.github.io/", "summary": "Can humans identify AI-generated (fake) videos and provide grounded reasons? While video generation models have advanced rapidly, a critical dimension -- whether humans can detect deepfake traces within a generated video, i.e., spatiotemporal grounded visual artifacts that reveal a video as machine generated -- has been largely overlooked. We introduce DeeptraceReward, the first fine-grained, spatially- and temporally- aware benchmark that annotates human-perceived fake traces for video generation reward. The dataset comprises 4.3K detailed annotations across 3.3K high-quality generated videos. Each annotation provides a natural-language explanation, pinpoints a bounding-box region containing the perceived trace, and marks precise onset and offset timestamps. We consolidate these annotations into 9 major categories of deepfake traces that lead humans to identify a video as AI-generated, and train multimodal language models (LMs) as reward models to mimic human judgments and localizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by 34.7% on average across fake clue identification, grounding, and explanation. Interestingly, we observe a consistent difficulty gradient: binary fake v.s. real classification is substantially easier than fine-grained deepfake trace detection; within the latter, performance degrades from natural language explanations (easiest), to spatial grounding, to temporal labeling (hardest). By foregrounding human-perceived deepfake traces, DeeptraceReward provides a rigorous testbed and training signal for socially aware and trustworthy video generation.", "AI": {"tldr": "DeeptraceReward\u662f\u9996\u4e2a\u7ec6\u7c92\u5ea6\u3001\u65f6\u7a7a\u611f\u77e5\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6807\u6ce8\u4eba\u7c7b\u611f\u77e5\u7684\u6df1\u5ea6\u4f2a\u9020\u75d5\u8ff9\uff0c\u5305\u542b4.3K\u4e2a\u8be6\u7ec6\u6807\u6ce8\uff0c\u8bad\u7ec3\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\u6765\u6a21\u4eff\u4eba\u7c7b\u5224\u65ad\u548c\u5b9a\u4f4d\u3002", "motivation": "\u89c6\u9891\u751f\u6210\u6a21\u578b\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u4eba\u7c7b\u80fd\u5426\u68c0\u6d4b\u751f\u6210\u89c6\u9891\u4e2d\u7684\u6df1\u5ea6\u4f2a\u9020\u75d5\u8ff9\u8fd9\u4e00\u5173\u952e\u7ef4\u5ea6\u88ab\u5ffd\u89c6\uff0c\u9700\u8981\u7814\u7a76\u4eba\u7c7b\u611f\u77e5\u7684\u65f6\u7a7a\u89c6\u89c9\u4f2a\u5f71\u3002", "method": "\u6784\u5efa\u5305\u542b4.3K\u8be6\u7ec6\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u6807\u6ce8\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3001\u8fb9\u754c\u6846\u533a\u57df\u548c\u7cbe\u786e\u65f6\u95f4\u6233\uff0c\u5c06\u6807\u6ce8\u6574\u5408\u4e3a9\u7c7b\u6df1\u5ea6\u4f2a\u9020\u75d5\u8ff9\uff0c\u8bad\u7ec3\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\u3002", "result": "7B\u5956\u52b1\u6a21\u578b\u5728DeeptraceReward\u4e0a\u5e73\u5747\u6bd4GPT-5\u8868\u73b0\u597d34.7%\uff0c\u89c2\u5bdf\u5230\u4e00\u81f4\u7684\u96be\u5ea6\u68af\u5ea6\uff1a\u4e8c\u5143\u5206\u7c7b\u6700\u5bb9\u6613\uff0c\u7ec6\u7c92\u5ea6\u68c0\u6d4b\u66f4\u96be\uff0c\u5176\u4e2d\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u6700\u5bb9\u6613\uff0c\u7a7a\u95f4\u5b9a\u4f4d\u6b21\u4e4b\uff0c\u65f6\u95f4\u6807\u6ce8\u6700\u96be\u3002", "conclusion": "\u901a\u8fc7\u7a81\u51fa\u4eba\u7c7b\u611f\u77e5\u7684\u6df1\u5ea6\u4f2a\u9020\u75d5\u8ff9\uff0cDeeptraceReward\u4e3a\u793e\u4f1a\u610f\u8bc6\u548c\u53ef\u4fe1\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u6d4b\u8bd5\u5e73\u53f0\u548c\u8bad\u7ec3\u4fe1\u53f7\u3002"}}
{"id": "2509.21477", "pdf": "https://arxiv.org/pdf/2509.21477", "abs": "https://arxiv.org/abs/2509.21477", "authors": ["Yuan Gao", "Hao Wu", "Qingsong Wen", "Kun Wang", "Xian Wu", "Xiaomeng Huang"], "title": "VISION: Prompting Ocean Vertical Velocity Reconstruction from Incomplete Observations", "categories": ["cs.LG", "cs.CV", "physics.ao-ph"], "comment": null, "summary": "Reconstructing subsurface ocean dynamics, such as vertical velocity fields, from incomplete surface observations poses a critical challenge in Earth science, a field long hampered by the lack of standardized, analysis-ready benchmarks. To systematically address this issue and catalyze research, we first build and release KD48, a high-resolution ocean dynamics benchmark derived from petascale simulations and curated with expert-driven denoising. Building on this benchmark, we introduce VISION, a novel reconstruction paradigm based on Dynamic Prompting designed to tackle the core problem of missing data in real-world observations. The essence of VISION lies in its ability to generate a visual prompt on-the-fly from any available subset of observations, which encodes both data availability and the ocean's physical state. More importantly, we design a State-conditioned Prompting module that efficiently injects this prompt into a universal backbone, endowed with geometry- and scale-aware operators, to guide its adaptive adjustment of computational strategies. This mechanism enables VISION to precisely handle the challenges posed by varying input combinations. Extensive experiments on the KD48 benchmark demonstrate that VISION not only substantially outperforms state-of-the-art models but also exhibits strong generalization under extreme data missing scenarios. By providing a high-quality benchmark and a robust model, our work establishes a solid infrastructure for ocean science research under data uncertainty. Our codes are available at: https://github.com/YuanGao-YG/VISION.", "AI": {"tldr": "\u63d0\u51fa\u4e86VISION\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u63d0\u793a\u673a\u5236\u4ece\u90e8\u5206\u6d77\u9762\u89c2\u6d4b\u6570\u636e\u91cd\u5efa\u6d77\u6d0b\u6b21\u8868\u5c42\u52a8\u529b\u5b66\uff0c\u5e76\u5728\u65b0\u6784\u5efa\u7684KD48\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4ece\u4e0d\u5b8c\u5907\u6d77\u9762\u89c2\u6d4b\u6570\u636e\u91cd\u5efa\u6d77\u6d0b\u6b21\u8868\u5c42\u52a8\u529b\u5b66\u7684\u6311\u6218\uff0c\u8be5\u9886\u57df\u957f\u671f\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u963b\u788d\u4e86\u7cfb\u7edf\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u6784\u5efaKD48\u9ad8\u5206\u8fa8\u7387\u6d77\u6d0b\u52a8\u529b\u5b66\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u57fa\u4e8e\u52a8\u6001\u63d0\u793a\u7684VISION\u91cd\u5efa\u8303\u5f0f\uff0c\u901a\u8fc7\u72b6\u6001\u6761\u4ef6\u63d0\u793a\u6a21\u5757\u5c06\u89c6\u89c9\u63d0\u793a\u6ce8\u5165\u901a\u7528\u4e3b\u5e72\u7f51\u7edc\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u8ba1\u7b97\u7b56\u7565\u8c03\u6574\u3002", "result": "\u5728KD48\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cVISION\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u6a21\u578b\uff0c\u5728\u6781\u7aef\u6570\u636e\u7f3a\u5931\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9b\u9ad8\u8d28\u91cf\u57fa\u51c6\u548c\u9c81\u68d2\u6a21\u578b\uff0c\u4e3a\u6570\u636e\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u6d77\u6d0b\u79d1\u5b66\u7814\u7a76\u5efa\u7acb\u4e86\u575a\u5b9e\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2509.21498", "pdf": "https://arxiv.org/pdf/2509.21498", "abs": "https://arxiv.org/abs/2509.21498", "authors": ["Arani Roy", "Shristi Das Biswas", "Kaushik Roy"], "title": "SlimDiff: Training-Free, Activation-Guided Hands-free Slimming of Diffusion Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Diffusion models (DMs), lauded for their generative performance, are computationally prohibitive due to their billion-scale parameters and iterative denoising dynamics. Existing efficiency techniques, such as quantization, timestep reduction, or pruning, offer savings in compute, memory, or runtime but are strictly bottlenecked by reliance on fine-tuning or retraining to recover performance. In this work, we introduce SlimDiff, an automated activation-informed structural compression framework that reduces both attention and feedforward dimensionalities in DMs, while being entirely gradient-free. SlimDiff reframes DM compression as a spectral approximation task, where activation covariances across denoising timesteps define low-rank subspaces that guide dynamic pruning under a fixed compression budget. This activation-aware formulation mitigates error accumulation across timesteps by applying module-wise decompositions over functional weight groups: query--key interactions, value--output couplings, and feedforward projections, rather than isolated matrix factorizations, while adaptively allocating sparsity across modules to respect the non-uniform geometry of diffusion trajectories. SlimDiff achieves up to 35\\% acceleration and $\\sim$100M parameter reduction over baselines, with generation quality on par with uncompressed models without any backpropagation. Crucially, our approach requires only about 500 calibration samples, over 70$\\times$ fewer than prior methods. To our knowledge, this is the first closed-form, activation-guided structural compression of DMs that is entirely training-free, providing both theoretical clarity and practical efficiency.", "AI": {"tldr": "SlimDiff\u662f\u4e00\u4e2a\u65e0\u9700\u68af\u5ea6\u7684\u6269\u6563\u6a21\u578b\u7ed3\u6784\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u6fc0\u6d3b\u611f\u77e5\u7684\u8c31\u8fd1\u4f3c\u65b9\u6cd5\u52a8\u6001\u4fee\u526a\u6ce8\u610f\u529b\u673a\u5236\u548c\u524d\u9988\u7f51\u7edc\u7ef4\u5ea6\uff0c\u5b9e\u73b035%\u52a0\u901f\u548c\u7ea61\u4ebf\u53c2\u6570\u51cf\u5c11\uff0c\u4e14\u65e0\u9700\u5fae\u8c03\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u867d\u7136\u751f\u6210\u6027\u80fd\u4f18\u79c0\uff0c\u4f46\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002\u73b0\u6709\u6548\u7387\u6280\u672f\u4f9d\u8d56\u4e8e\u5fae\u8c03\u6216\u91cd\u65b0\u8bad\u7ec3\u6765\u6062\u590d\u6027\u80fd\uff0c\u5b58\u5728\u74f6\u9888\u3002", "method": "\u5c06\u6269\u6563\u6a21\u578b\u538b\u7f29\u91cd\u65b0\u5b9a\u4e49\u4e3a\u8c31\u8fd1\u4f3c\u4efb\u52a1\uff0c\u5229\u7528\u53bb\u566a\u65f6\u95f4\u6b65\u7684\u6fc0\u6d3b\u534f\u65b9\u5dee\u5b9a\u4e49\u4f4e\u79e9\u5b50\u7a7a\u95f4\uff0c\u5728\u56fa\u5b9a\u538b\u7f29\u9884\u7b97\u4e0b\u6307\u5bfc\u52a8\u6001\u4fee\u526a\uff0c\u91c7\u7528\u6a21\u5757\u7ea7\u5206\u89e3\u800c\u975e\u5b64\u7acb\u77e9\u9635\u5206\u89e3\u3002", "result": "\u76f8\u6bd4\u57fa\u7ebf\u5b9e\u73b035%\u52a0\u901f\u548c\u7ea61\u4ebf\u53c2\u6570\u51cf\u5c11\uff0c\u751f\u6210\u8d28\u91cf\u4e0e\u672a\u538b\u7f29\u6a21\u578b\u76f8\u5f53\uff0c\u4ec5\u9700\u7ea6500\u4e2a\u6821\u51c6\u6837\u672c\uff0c\u6bd4\u4e4b\u524d\u65b9\u6cd5\u5c1170\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u8fd9\u662f\u7b2c\u4e00\u4e2a\u57fa\u4e8e\u95ed\u5f0f\u89e3\u3001\u6fc0\u6d3b\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\u7ed3\u6784\u538b\u7f29\u65b9\u6cd5\uff0c\u5b8c\u5168\u65e0\u9700\u8bad\u7ec3\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u6e05\u6670\u6027\u548c\u5b9e\u9645\u6548\u7387\u3002"}}
{"id": "2509.21513", "pdf": "https://arxiv.org/pdf/2509.21513", "abs": "https://arxiv.org/abs/2509.21513", "authors": ["Weiqiao Han", "Chenlin Meng", "Christopher D. Manning", "Stefano Ermon"], "title": "DistillKac: Few-Step Image Generation via Damped Wave Equations", "categories": ["cs.LG", "cs.AI", "cs.CV", "math.PR", "stat.ML"], "comment": null, "summary": "We present DistillKac, a fast image generator that uses the damped wave equation and its stochastic Kac representation to move probability mass at finite speed. In contrast to diffusion models whose reverse time velocities can become stiff and implicitly allow unbounded propagation speed, Kac dynamics enforce finite speed transport and yield globally bounded kinetic energy. Building on this structure, we introduce classifier-free guidance in velocity space that preserves square integrability under mild conditions. We then propose endpoint only distillation that trains a student to match a frozen teacher over long intervals. We prove a stability result that promotes supervision at the endpoints to closeness along the entire path. Experiments demonstrate DistillKac delivers high quality samples with very few function evaluations while retaining the numerical stability benefits of finite speed probability flows.", "AI": {"tldr": "DistillKac\u662f\u4e00\u79cd\u5feb\u901f\u56fe\u50cf\u751f\u6210\u5668\uff0c\u4f7f\u7528\u963b\u5c3c\u6ce2\u52a8\u65b9\u7a0b\u53ca\u5176\u968f\u673aKac\u8868\u793a\u6765\u4ee5\u6709\u9650\u901f\u5ea6\u79fb\u52a8\u6982\u7387\u8d28\u91cf\u3002\u76f8\u6bd4\u6269\u6563\u6a21\u578b\uff0c\u5b83\u5f3a\u5236\u6267\u884c\u6709\u9650\u901f\u5ea6\u4f20\u8f93\u5e76\u4ea7\u751f\u5168\u5c40\u6709\u754c\u52a8\u80fd\u3002", "motivation": "\u89e3\u51b3\u6269\u6563\u6a21\u578b\u4e2d\u53cd\u5411\u65f6\u95f4\u901f\u5ea6\u53ef\u80fd\u53d8\u5f97\u50f5\u786c\u4e14\u9690\u542b\u5141\u8bb8\u65e0\u754c\u4f20\u64ad\u901f\u5ea6\u7684\u95ee\u9898\uff0c\u901a\u8fc7Kac\u52a8\u529b\u5b66\u5f3a\u5236\u6267\u884c\u6709\u9650\u901f\u5ea6\u4f20\u8f93\u3002", "method": "\u4f7f\u7528\u963b\u5c3c\u6ce2\u52a8\u65b9\u7a0b\u548c\u968f\u673aKac\u8868\u793a\uff0c\u5728\u901f\u5ea6\u7a7a\u95f4\u4e2d\u5f15\u5165\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\uff0c\u63d0\u51fa\u4ec5\u7aef\u70b9\u84b8\u998f\u65b9\u6cd5\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u5339\u914d\u51bb\u7ed3\u7684\u6559\u5e08\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDistillKac\u80fd\u591f\u4ee5\u6781\u5c11\u7684\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\u751f\u6210\u9ad8\u8d28\u91cf\u6837\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u9650\u901f\u5ea6\u6982\u7387\u6d41\u7684\u6570\u503c\u7a33\u5b9a\u6027\u4f18\u52bf\u3002", "conclusion": "DistillKac\u5728\u4fdd\u6301\u6570\u503c\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u56fe\u50cf\u751f\u6210\uff0c\u8bc1\u660e\u4e86\u6709\u9650\u901f\u5ea6\u6982\u7387\u6d41\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.22049", "pdf": "https://arxiv.org/pdf/2509.22049", "abs": "https://arxiv.org/abs/2509.22049", "authors": ["Emily Honey", "Anders Helbo", "Jens Petersen"], "title": "Comparative Analysis of GAN and Diffusion for MRI-to-CT translation", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Computed tomography (CT) is essential for treatment and diagnostics; In case CT are missing or otherwise difficult to obtain, methods for generating synthetic CT (sCT) images from magnetic resonance imaging (MRI) images are sought after. Therefore, it is valuable to establish a reference for what strategies are most effective for MRI-to-CT translation. In this paper, we compare the performance of two frequently used architectures for MRI-to-CT translation: a conditional generative adversarial network (cGAN) and a conditional denoising diffusion probabilistic model (cDDPM). We chose well-established implementations to represent each architecture: Pix2Pix for cGAN, and Palette for cDDPM. We separate the classical 3D translation problem into a sequence of 2D translations on the transverse plane, to investigate the viability of a strategy that reduces the computational cost. We also investigate the impact of conditioning the generative process on a single MRI image/slice and on multiple MRI slices. The performance is assessed using a thorough evaluation protocol, including a novel slice-wise metric Similarity Of Slices (SIMOS), which measures the continuity between transverse slices when compiling the sCTs into 3D format. Our comparative analysis revealed that MRI-to-CT generative models benefit from multi-channel conditional input and using cDDPM as an architecture.", "AI": {"tldr": "\u6bd4\u8f83\u4e24\u79cdMRI\u5230CT\u56fe\u50cf\u8f6c\u6362\u65b9\u6cd5\uff1a\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc(cGAN)\u548c\u6761\u4ef6\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b(cDDPM)\uff0c\u53d1\u73b0cDDPM\u67b6\u6784\u548c\u591a\u901a\u9053\u6761\u4ef6\u8f93\u5165\u80fd\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "CT\u56fe\u50cf\u5728\u6cbb\u7597\u548c\u8bca\u65ad\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u6709\u65f6\u96be\u4ee5\u83b7\u53d6\uff0c\u56e0\u6b64\u9700\u8981\u4eceMRI\u751f\u6210\u5408\u6210CT(sCT)\u56fe\u50cf\u7684\u65b9\u6cd5\u3002\u9700\u8981\u786e\u5b9a\u54ea\u79cdMRI\u5230CT\u8f6c\u6362\u7b56\u7565\u6700\u6709\u6548\u3002", "method": "\u4f7f\u7528Pix2Pix\u4ee3\u8868cGAN\uff0cPalette\u4ee3\u8868cDDPM\uff0c\u5c063D\u8f6c\u6362\u95ee\u9898\u5206\u89e3\u4e3a2D\u6a2a\u5411\u5e73\u9762\u8f6c\u6362\u5e8f\u5217\u4ee5\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002\u7814\u7a76\u5355\u5207\u7247\u548c\u591a\u5207\u7247\u6761\u4ef6\u8f93\u5165\u7684\u5f71\u54cd\u3002", "result": "cDDPM\u67b6\u6784\u5728MRI\u5230CT\u8f6c\u6362\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u591a\u901a\u9053\u6761\u4ef6\u8f93\u5165\u80fd\u63d0\u5347\u6027\u80fd\u3002\u63d0\u51fa\u4e86\u65b0\u7684\u5207\u7247\u76f8\u4f3c\u5ea6\u6307\u6807SIMOS\u6765\u8bc4\u4f303D\u91cd\u5efa\u7684\u8fde\u7eed\u6027\u3002", "conclusion": "MRI\u5230CT\u751f\u6210\u6a21\u578b\u53d7\u76ca\u4e8e\u591a\u901a\u9053\u6761\u4ef6\u8f93\u5165\u548ccDDPM\u67b6\u6784\u7684\u4f7f\u7528\u3002"}}
{"id": "2509.22126", "pdf": "https://arxiv.org/pdf/2509.22126", "abs": "https://arxiv.org/abs/2509.22126", "authors": ["Enoal Gesny", "Eva Giboulot", "Teddy Furon", "Vivien Chappelier"], "title": "Guidance Watermarking for Diffusion Models", "categories": ["cs.CR", "cs.CV"], "comment": null, "summary": "This paper introduces a novel watermarking method for diffusion models. It is based on guiding the diffusion process using the gradient computed from any off-the-shelf watermark decoder. The gradient computation encompasses different image augmentations, increasing robustness to attacks against which the decoder was not originally robust, without retraining or fine-tuning. Our method effectively convert any \\textit{post-hoc} watermarking scheme into an in-generation embedding along the diffusion process. We show that this approach is complementary to watermarking techniques modifying the variational autoencoder at the end of the diffusion process. We validate the methods on different diffusion models and detectors. The watermarking guidance does not significantly alter the generated image for a given seed and prompt, preserving both the diversity and quality of generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\u6c34\u5370\u65b9\u6cd5\uff0c\u53ef\u5c06\u540e\u9a8c\u6c34\u5370\u65b9\u6848\u8f6c\u6362\u4e3a\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u5d4c\u5165\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\u5373\u53ef\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "motivation": "\u5c06\u540e\u9a8c\u6c34\u5370\u65b9\u6848\u6709\u6548\u8f6c\u6362\u4e3a\u6269\u6563\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u5d4c\u5165\uff0c\u589e\u5f3a\u6c34\u5370\u7684\u9c81\u68d2\u6027\u800c\u4e0d\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u3002", "method": "\u4f7f\u7528\u73b0\u6210\u6c34\u5370\u89e3\u7801\u5668\u8ba1\u7b97\u68af\u5ea6\u6765\u5f15\u5bfc\u6269\u6563\u8fc7\u7a0b\uff0c\u7ed3\u5408\u56fe\u50cf\u589e\u5f3a\u6280\u672f\u63d0\u9ad8\u5bf9\u672a\u8bad\u7ec3\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u4e0d\u540c\u6269\u6563\u6a21\u578b\u548c\u68c0\u6d4b\u5668\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0c\u6c34\u5370\u5f15\u5bfc\u4e0d\u4f1a\u663e\u8457\u6539\u53d8\u7ed9\u5b9a\u79cd\u5b50\u548c\u63d0\u793a\u7684\u751f\u6210\u56fe\u50cf\uff0c\u4fdd\u6301\u751f\u6210\u591a\u6837\u6027\u548c\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0e\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4fee\u6539\u6280\u672f\u4e92\u8865\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6c34\u5370\u5d4c\u5165\u65b9\u6848\u3002"}}
{"id": "2509.22394", "pdf": "https://arxiv.org/pdf/2509.22394", "abs": "https://arxiv.org/abs/2509.22394", "authors": ["Javier Sequeiro Gonz\u00e1lez", "Arthur Longuefosse", "Miguel D\u00edaz Benito", "\u00c1lvaro Garc\u00eda Mart\u00edn", "Fabien Baldacci"], "title": "Deep Learning-Based Cross-Anatomy CT Synthesis Using Adapted nnResU-Net with Anatomical Feature Prioritized Loss", "categories": ["eess.IV", "cs.AI", "cs.CV", "I.2; J.3"], "comment": null, "summary": "We present a patch-based 3D nnUNet adaptation for MR to CT and CBCT to CT image translation using the multicenter SynthRAD2025 dataset, covering head and neck (HN), thorax (TH), and abdomen (AB) regions. Our approach leverages two main network configurations: a standard UNet and a residual UNet, both adapted from nnUNet for image synthesis. The Anatomical Feature-Prioritized (AFP) loss was introduced, which compares multilayer features extracted from a compact segmentation network trained on TotalSegmentator labels, enhancing reconstruction of clinically relevant structures. Input volumes were normalized per-case using zscore normalization for MRIs, and clipping plus dataset level zscore normalization for CBCT and CT. Training used 3D patches tailored to each anatomical region without additional data augmentation. Models were trained for 1000 and 1500 epochs, with AFP fine-tuning performed for 500 epochs using a combined L1+AFP objective. During inference, overlapping patches were aggregated via mean averaging with step size of 0.3, and postprocessing included reverse zscore normalization. Both network configurations were applied across all regions, allowing consistent model design while capturing local adaptations through residual learning and AFP loss. Qualitative and quantitative evaluation revealed that residual networks combined with AFP yielded sharper reconstructions and improved anatomical fidelity, particularly for bone structures in MR to CT and lesions in CBCT to CT, while L1only networks achieved slightly better intensity-based metrics. This methodology provides a stable solution for cross modality medical image synthesis, demonstrating the effectiveness of combining the automatic nnUNet pipeline with residual learning and anatomically guided feature losses.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8ennUNet\u76843D\u56fe\u50cf\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u7528\u4e8eMR\u5230CT\u548cCBCT\u5230CT\u8f6c\u6362\uff0c\u5f15\u5165\u89e3\u5256\u7279\u5f81\u4f18\u5148\u635f\u5931\u51fd\u6570(AFP)\u63d0\u5347\u4e34\u5e8a\u76f8\u5173\u7ed3\u6784\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u591a\u4e2d\u5fc3\u533b\u5b66\u56fe\u50cf\u8de8\u6a21\u6001\u5408\u6210\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5934\u9888\u90e8\u3001\u80f8\u90e8\u548c\u8179\u90e8\u533a\u57df\uff0c\u9700\u8981\u63d0\u5347\u89e3\u5256\u7ed3\u6784\u7684\u91cd\u5efa\u4fdd\u771f\u5ea6\u3002", "method": "\u91c7\u7528\u6807\u51c6UNet\u548c\u6b8b\u5deeUNet\u4e24\u79cd\u7f51\u7edc\u914d\u7f6e\uff0c\u5f15\u5165AFP\u635f\u5931\u51fd\u6570\u7ed3\u5408\u7d27\u51d1\u5206\u5272\u7f51\u7edc\u7279\u5f81\uff0c\u4f7f\u75283D\u8865\u4e01\u8bad\u7ec3\uff0c\u5e76\u8fdb\u884cAFP\u5fae\u8c03\u3002", "result": "\u6b8b\u5dee\u7f51\u7edc\u7ed3\u5408AFP\u635f\u5931\u80fd\u4ea7\u751f\u66f4\u6e05\u6670\u7684\u91cd\u5efa\u7ed3\u679c\uff0c\u7279\u522b\u662f\u5728MR\u5230CT\u7684\u9aa8\u7ed3\u6784\u548cCBCT\u5230CT\u7684\u75c5\u7076\u91cd\u5efa\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8de8\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u5408\u6210\u63d0\u4f9b\u4e86\u7a33\u5b9a\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86nnUNet\u81ea\u52a8\u7ba1\u9053\u4e0e\u6b8b\u5dee\u5b66\u4e60\u53ca\u89e3\u5256\u5f15\u5bfc\u7279\u5f81\u635f\u5931\u7ed3\u5408\u7684\u6709\u6548\u6027\u3002"}}
