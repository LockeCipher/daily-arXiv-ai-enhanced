<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 3]
- [cs.CV](#cs.CV) [Total: 29]
- [cs.LG](#cs.LG) [Total: 4]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [physics.optics](#physics.optics) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust Physics-Based Dynamics](https://arxiv.org/abs/2510.01619)
*Changmin Lee,Jihyun Lee,Tae-Kyun Kim*

Main category: cs.GR

TL;DR: MPMAvatar是一个从多视角视频创建3D人体化身的框架，支持高度逼真的动画和自由视角的光线渲染。通过使用基于物质点法的模拟器和各向异性本构模型，实现了对宽松衣物复杂变形和接触的准确建模。


<details>
  <summary>Details</summary>
Motivation: 现有基于物理模拟的方法在建模宽松衣物动态时存在精度不足或对新动画输入鲁棒性差的问题，需要开发更准确和鲁棒的动态建模方法。

Method: 使用物质点法模拟器，结合各向异性本构模型和新型碰撞处理算法来建模衣物的复杂变形和身体接触；结合使用3D高斯溅射渲染的规范化身，实现高保真渲染。

Result: MPMAvatar在动态建模精度、渲染精度、鲁棒性和效率方面显著优于现有最先进的基于物理的化身方法，并能以零样本方式泛化到未见过的交互场景。

Conclusion: 该框架成功解决了宽松衣物动态建模的挑战，实现了物理上合理的动画和逼真渲染，在模拟泛化能力方面超越了先前基于学习的方法。

Abstract: While there has been significant progress in the field of 3D avatar creation from visual observations, modeling physically plausible dynamics of humans with loose garments remains a challenging problem. Although a few existing works address this problem by leveraging physical simulation, they suffer from limited accuracy or robustness to novel animation inputs. In this work, we present MPMAvatar, a framework for creating 3D human avatars from multi-view videos that supports highly realistic, robust animation, as well as photorealistic rendering from free viewpoints. For accurate and robust dynamics modeling, our key idea is to use a Material Point Method-based simulator, which we carefully tailor to model garments with complex deformations and contact with the underlying body by incorporating an anisotropic constitutive model and a novel collision handling algorithm. We combine this dynamics modeling scheme with our canonical avatar that can be rendered using 3D Gaussian Splatting with quasi-shadowing, enabling high-fidelity rendering for physically realistic animations. In our experiments, we demonstrate that MPMAvatar significantly outperforms the existing state-of-the-art physics-based avatar in terms of (1) dynamics modeling accuracy, (2) rendering accuracy, and (3) robustness and efficiency. Additionally, we present a novel application in which our avatar generalizes to unseen interactions in a zero-shot manner-which was not achievable with previous learning-based methods due to their limited simulation generalizability. Our project page is at: https://KAISTChangmin.github.io/MPMAvatar/

</details>


### [2] [ROI-GS: Interest-based Local Quality 3D Gaussian Splatting](https://arxiv.org/abs/2510.01978)
*Quoc-Anh Bui,Gilles Rougeron,Géraldine Morin,Simone Gasparini*

Main category: cs.GR

TL;DR: ROI-GS是一个对象感知的3D高斯泼溅框架，通过对象引导的相机选择、针对性对象训练和高质量对象重建的无缝集成，在保持实时性能的同时显著提升感兴趣区域的细节质量。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯泼溅方法在场景中均匀分配资源，限制了感兴趣区域的精细细节，导致模型尺寸膨胀。需要一种能够优先处理重要对象细节的方法。

Method: 提出ROI-GS框架，包括对象引导相机选择、针对性对象训练策略，以及高质量对象重建与全局场景的无缝集成。

Result: 实验显示ROI-GS显著提升局部质量（PSNR最高提升2.96 dB），模型尺寸减少约17%，对单感兴趣对象场景训练更快，优于现有方法。

Conclusion: ROI-GS通过对象感知的资源分配策略，在保持实时性能的同时有效提升了感兴趣区域的细节重建质量，并减少了模型尺寸。

Abstract: We tackle the challenge of efficiently reconstructing 3D scenes with high detail on objects of interest. Existing 3D Gaussian Splatting (3DGS) methods allocate resources uniformly across the scene, limiting fine detail to Regions Of Interest (ROIs) and leading to inflated model size. We propose ROI-GS, an object-aware framework that enhances local details through object-guided camera selection, targeted Object training, and seamless integration of high-fidelity object of interest reconstructions into the global scene. Our method prioritizes higher resolution details on chosen objects while maintaining real-time performance. Experiments show that ROI-GS significantly improves local quality (up to 2.96 dB PSNR), while reducing overall model size by $\approx 17\%$ of baseline and achieving faster training for a scene with a single object of interest, outperforming existing methods.

</details>


### [3] [Spec-Gloss Surfels and Normal-Diffuse Priors for Relightable Glossy Objects](https://arxiv.org/abs/2510.02069)
*Georgios Kouros,Minye Wu,Tinne Tuytelaars*

Main category: cs.GR

TL;DR: 提出了一种可重光照框架，将微表面BRDF与高光-光泽度参数化集成到2D高斯泼溅中，通过延迟渲染实现更物理一致的材料分解和高质量重光照效果。


<details>
  <summary>Details</summary>
Motivation: 现有神经渲染方法通常依赖简化的BRDF模型或耦合漫反射和镜面反射的参数化，限制了材料恢复的准确性和重光照保真度。

Method: 将微表面BRDF与高光-光泽度参数化集成到2D高斯泼溅中，采用延迟渲染；使用基于扩散的表面法线和漫反射颜色先验指导早期优化；采用从粗到精的环境贴图优化策略。

Result: 在复杂光泽场景上的广泛实验表明，该方法实现了高质量的几何和材料重建，相比现有高斯泼溅方法，在新光照下提供了更真实和一致的重光照效果。

Conclusion: 该方法通过物理一致的BRDF模型和有效的优化策略，显著提升了光泽物体的重建和重光照质量。

Abstract: Accurate reconstruction and relighting of glossy objects remain a longstanding challenge, as object shape, material properties, and illumination are inherently difficult to disentangle. Existing neural rendering approaches often rely on simplified BRDF models or parameterizations that couple diffuse and specular components, which restricts faithful material recovery and limits relighting fidelity. We propose a relightable framework that integrates a microfacet BRDF with the specular-glossiness parameterization into 2D Gaussian Splatting with deferred shading. This formulation enables more physically consistent material decomposition, while diffusion-based priors for surface normals and diffuse color guide early-stage optimization and mitigate ambiguity. A coarse-to-fine optimization of the environment map accelerates convergence and preserves high-dynamic-range specular reflections. Extensive experiments on complex, glossy scenes demonstrate that our method achieves high-quality geometry and material reconstruction, delivering substantially more realistic and consistent relighting under novel illumination compared to existing Gaussian splatting methods.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [4] [LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration](https://arxiv.org/abs/2510.01339)
*Alessio Spagnoletti,Andrés Almansa,Marcelo Pereyra*

Main category: cs.CV

TL;DR: 提出了LVTINO，首个基于视频一致性模型的零样本视频修复方法，解决了帧间图像扩散模型导致的时间不一致问题


<details>
  <summary>Details</summary>
Motivation: 现有基于图像扩散模型的零样本图像修复方法难以直接应用于视频修复，因为逐帧处理会导致时间不一致性

Method: 利用视频一致性模型(VCMs)作为先验，通过条件机制绕过自动微分需求，仅需少量神经网络评估即可实现高质量视频重建

Result: 在多种视频逆问题上显著优于逐帧应用图像LDM的方法，在重建保真度和计算效率方面都达到了新的水平

Conclusion: LVTINO为高清晰度视频修复建立了新的基准，实现了时间一致的高质量重建

Abstract: Computational imaging methods increasingly rely on powerful generative diffusion models to tackle challenging image restoration tasks. In particular, state-of-the-art zero-shot image inverse solvers leverage distilled text-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy and perceptual quality with high computational efficiency. However, extending these advances to high-definition video restoration remains a significant challenge, due to the need to recover fine spatial detail while capturing subtle temporal dependencies. Consequently, methods that naively apply image-based LDM priors on a frame-by-frame basis often result in temporally inconsistent reconstructions. We address this challenge by leveraging recent advances in Video Consistency Models (VCMs), which distill video latent diffusion models into fast generators that explicitly capture temporal causality. Building on this foundation, we propose LVTINO, the first zero-shot or plug-and-play inverse solver for high definition video restoration with priors encoded by VCMs. Our conditioning mechanism bypasses the need for automatic differentiation and achieves state-of-the-art video reconstruction quality with only a few neural function evaluations, while ensuring strong measurement consistency and smooth temporal transitions across frames. Extensive experiments on a diverse set of video inverse problems show significant perceptual improvements over current state-of-the-art methods that apply image LDMs frame by frame, establishing a new benchmark in both reconstruction fidelity and computational efficiency.

</details>


### [5] [Image Generation Based on Image Style Extraction](https://arxiv.org/abs/2510.01347)
*Shuochen Chang*

Main category: cs.CV

TL;DR: 提出了一种基于三阶段训练的风格提取图像生成方法，通过风格编码器和风格投影层将风格表示与文本表示对齐，实现基于文本提示的细粒度风格引导生成。


<details>
  <summary>Details</summary>
Motivation: 文本到图像生成模型中，细粒度风格难以用自然语言精确描述和控制，而风格参考图像的引导信息难以与传统文本引导生成直接对齐。

Method: 使用风格编码器和风格投影层从单个风格参考图像中提取细粒度风格表示，并将其注入生成模型而不改变其结构框架，构建了包含图像、风格标签和文本描述三元组的Style30k-captions数据集。

Result: 实现了细粒度控制的风格化图像生成，能够最大化预训练生成模型的生成能力。

Conclusion: 该方法能够有效提取和注入风格表示，实现基于文本提示的细粒度风格控制图像生成。

Abstract: Image generation based on text-to-image generation models is a task with practical application scenarios that fine-grained styles cannot be precisely described and controlled in natural language, while the guidance information of stylized reference images is difficult to be directly aligned with the textual conditions of traditional textual guidance generation. This study focuses on how to maximize the generative capability of the pretrained generative model, by obtaining fine-grained stylistic representations from a single given stylistic reference image, and injecting the stylistic representations into the generative body without changing the structural framework of the downstream generative model, so as to achieve fine-grained controlled stylized image generation. In this study, we propose a three-stage training style extraction-based image generation method, which uses a style encoder and a style projection layer to align the style representations with the textual representations to realize fine-grained textual cue-based style guide generation. In addition, this study constructs the Style30k-captions dataset, whose samples contain a triad of images, style labels, and text descriptions, to train the style encoder and style projection layer in this experiment.

</details>


### [6] [Purrception: Variational Flow Matching for Vector-Quantized Image Generation](https://arxiv.org/abs/2510.01478)
*Răzvan-Andrei Matişan,Vincent Tao Hu,Grigory Bartosh,Björn Ommer,Cees G. M. Snoek,Max Welling,Jan-Willem van de Meent,Mohammad Mahdi Derakhshani,Floor Eijkelboom*

Main category: cs.CV

TL;DR: Purrception是一种用于向量量化图像生成的变分流匹配方法，结合了连续传输动力学和显式分类监督，在ImageNet-1k 256x256生成任务中实现了更快的训练收敛和竞争性的FID分数。


<details>
  <summary>Details</summary>
Motivation: 为了解决向量量化图像生成中连续方法和离散方法各自的局限性，作者希望开发一种既能保持连续传输动力学又能提供显式分类监督的方法。

Method: 通过将变分流匹配适配到向量量化潜在空间，学习码本索引的分类后验分布，同时在连续嵌入空间中计算速度场，结合了连续方法的几何感知和离散方法的监督优势。

Result: 在ImageNet-1k 256x256生成任务中，训练收敛速度比连续流匹配和离散流匹配基线更快，同时达到了与最先进模型竞争的FID分数。

Conclusion: 变分流匹配能够有效桥接连续传输和离散监督，在图像生成中实现改进的训练效率。

Abstract: We introduce Purrception, a variational flow matching approach for vector-quantized image generation that provides explicit categorical supervision while maintaining continuous transport dynamics. Our method adapts Variational Flow Matching to vector-quantized latents by learning categorical posteriors over codebook indices while computing velocity fields in the continuous embedding space. This combines the geometric awareness of continuous methods with the discrete supervision of categorical approaches, enabling uncertainty quantification over plausible codes and temperature-controlled generation. We evaluate Purrception on ImageNet-1k 256x256 generation. Training converges faster than both continuous flow matching and discrete flow matching baselines while achieving competitive FID scores with state-of-the-art models. This demonstrates that Variational Flow Matching can effectively bridge continuous transport and discrete supervision for improved training efficiency in image generation.

</details>


### [7] [AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging](https://arxiv.org/abs/2510.01498)
*Yuxuan Ou,Ning Bi,Jiazhen Pan,Jiancheng Yang,Boliang Yu,Usama Zidan,Regent Lee,Vicente Grau*

Main category: cs.CV

TL;DR: 提出了一种统一的深度学习框架，通过条件扩散模型和多任务学习，从非对比CT扫描同时生成合成对比增强CT图像并分割主动脉腔和血栓，避免了多阶段方法的误差累积问题。


<details>
  <summary>Details</summary>
Motivation: 传统对比增强CT检查需要碘对比剂，存在肾毒性、过敏反应和环境危害等风险。现有方法采用多阶段流程，先生成图像再分割，导致误差累积且无法共享语义和解剖结构信息。

Method: 集成条件扩散模型与多任务学习，端到端联合优化图像合成和解剖分割。无需初始预测，共享编码器和解码器参数，采用半监督训练策略处理缺失分割标签的临床数据。

Result: 在264名患者队列中表现优于最先进的单任务和多阶段模型。图像合成PSNR达25.61 dB，分割方面腔Dice分数提升至0.89，血栓Dice分数提升至0.53，临床测量准确性显著改善。

Conclusion: 该统一框架通过联合优化图像合成和分割任务，在减少对比剂使用的同时提高了诊断准确性，为临床实践提供了更安全有效的替代方案。

Abstract: While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic aneurysms (AAA), the required iodinated contrast agents pose significant risks, including nephrotoxicity, patient allergies, and environmental harm. To reduce contrast agent use, recent deep learning methods have focused on generating synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a multi-stage pipeline that first generates images and then performs segmentation, which leads to error accumulation and fails to leverage shared semantic and anatomical structures. To address this, we propose a unified deep learning framework that generates synthetic CECT images from NCCT scans while simultaneously segmenting the aortic lumen and thrombus. Our approach integrates conditional diffusion models (CDM) with multi-task learning, enabling end-to-end joint optimization of image synthesis and anatomical segmentation. Unlike previous multitask diffusion models, our approach requires no initial predictions (e.g., a coarse segmentation mask), shares both encoder and decoder parameters across tasks, and employs a semi-supervised training strategy to learn from scans with missing segmentation labels, a common constraint in real-world clinical data. We evaluated our method on a cohort of 264 patients, where it consistently outperformed state-of-the-art single-task and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61 dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation, it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to nnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.

</details>


### [8] [Towards Better Optimization For Listwise Preference in Diffusion Models](https://arxiv.org/abs/2510.01540)
*Jiamu Bai,Xin Yu,Meilong Xu,Weitao Lu,Xin Pan,Kiwan Maeng,Daniel Kifer,Jian Wang,Yu Wang*

Main category: cs.CV

TL;DR: 提出了Diffusion-LPO框架，将列表式偏好优化应用于扩散模型，通过Plackett-Luce模型扩展DPO目标，在图像生成、编辑和个性化偏好对齐任务中优于成对DPO方法。


<details>
  <summary>Details</summary>
Motivation: 现有DPO方法主要依赖成对偏好，但人类对图像的反馈通常包含隐含的排名信息，能更精确地表达偏好。列表式偏好优化在扩散模型中尚未得到充分探索。

Method: 基于Plackett-Luce模型推导列表式DPO目标，给定标题时将用户反馈聚合成图像排名列表，强制每个样本优于其所有低排名替代品，确保整个排名的一致性。

Result: Diffusion-LPO在文本到图像生成、图像编辑和个性化偏好对齐等任务中，在视觉质量和偏好对齐方面始终优于成对DPO基线。

Conclusion: Diffusion-LPO是一个简单有效的列表式偏好优化框架，能够更好地利用人类反馈中的排名信息，提升扩散模型与人类偏好的对齐效果。

Abstract: Reinforcement learning from human feedback (RLHF) has proven effectiveness for aligning text-to-image (T2I) diffusion models with human preferences. Although Direct Preference Optimization (DPO) is widely adopted for its computational efficiency and avoidance of explicit reward modeling, its applications to diffusion models have primarily relied on pairwise preferences. The precise optimization of listwise preferences remains largely unaddressed. In practice, human feedback on image preferences often contains implicit ranked information, which conveys more precise human preferences than pairwise comparisons. In this work, we propose Diffusion-LPO, a simple and effective framework for Listwise Preference Optimization in diffusion models with listwise data. Given a caption, we aggregate user feedback into a ranked list of images and derive a listwise extension of the DPO objective under the Plackett-Luce model. Diffusion-LPO enforces consistency across the entire ranking by encouraging each sample to be preferred over all of its lower-ranked alternatives. We empirically demonstrate the effectiveness of Diffusion-LPO across various tasks, including text-to-image generation, image editing, and personalized preference alignment. Diffusion-LPO consistently outperforms pairwise DPO baselines on visual quality and preference alignment.

</details>


### [9] [NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems](https://arxiv.org/abs/2510.01608)
*Roman Jacome,Romario Gualdrón-Hurtado,Leon Suarez,Henry Arguello*

Main category: cs.CV

TL;DR: 提出了NPN方法，通过神经网络学习感知矩阵零空间的低维投影来正则化图像逆问题，而不是在图像域施加结构约束


<details>
  <summary>Details</summary>
Motivation: 传统正则化方法忽略了感知矩阵零空间的特定结构，而零空间包含感知过程无法获取的信号信息，需要专门的正则化方法

Method: 使用神经网络学习感知矩阵零空间的低维投影，设计感知矩阵特定的先验，可与其他重建框架兼容

Result: 在压缩感知、去模糊、超分辨率、CT和MRI等多种逆问题中，NPN先验能持续提升重建保真度

Conclusion: NPN方法通过关注零空间结构，提供了可解释且灵活的正则化策略，能有效提升各种成像逆问题的重建质量

Abstract: Imaging inverse problems aims to recover high-dimensional signals from undersampled, noisy measurements, a fundamentally ill-posed task with infinite solutions in the null-space of the sensing operator. To resolve this ambiguity, prior information is typically incorporated through handcrafted regularizers or learned models that constrain the solution space. However, these priors typically ignore the task-specific structure of that null-space. In this work, we propose \textit{Non-Linear Projections of the Null-Space} (NPN), a novel class of regularization that, instead of enforcing structural constraints in the image domain, promotes solutions that lie in a low-dimensional projection of the sensing matrix's null-space with a neural network. Our approach has two key advantages: (1) Interpretability: by focusing on the structure of the null-space, we design sensing-matrix-specific priors that capture information orthogonal to the signal components that are fundamentally blind to the sensing process. (2) Flexibility: NPN is adaptable to various inverse problems, compatible with existing reconstruction frameworks, and complementary to conventional image-domain priors. We provide theoretical guarantees on convergence and reconstruction accuracy when used within plug-and-play methods. Empirical results across diverse sensing matrices demonstrate that NPN priors consistently enhance reconstruction fidelity in various imaging inverse problems, such as compressive sensing, deblurring, super-resolution, computed tomography, and magnetic resonance imaging, with plug-and-play methods, unrolling networks, deep image prior, and diffusion models.

</details>


### [10] [Joint Deblurring and 3D Reconstruction for Macrophotography](https://arxiv.org/abs/2510.01640)
*Yifan Zhao,Liangchen Li,Yuqi Zhou,Kai Wang,Yan Liang,Juyong Zhang*

Main category: cs.CV

TL;DR: 提出了一种联合去模糊和3D重建的方法，用于解决微距摄影中的散焦模糊问题，通过可微分渲染自监督优化3D模型和散焦模糊核。


<details>
  <summary>Details</summary>
Motivation: 微距镜头具有高分辨率和大放大倍率的优势，但散焦模糊严重阻碍了清晰成像和高质量3D重建。传统方法需要大量图像和标注，且目前没有针对微距摄影的多视角3D重建方法。

Method: 从多视角模糊图像出发，联合优化物体的清晰3D模型和每个像素的散焦模糊核。整个框架采用可微分渲染方法自监督优化3D模型和散焦模糊核。

Result: 大量实验表明，从少量多视角图像中，该方法不仅能实现高质量图像去模糊，还能恢复高保真度的3D外观。

Conclusion: 该方法成功解决了微距摄影中的散焦模糊问题，实现了联合去模糊和3D重建，为小细节物体的高质量3D建模提供了有效解决方案。

Abstract: Macro lens has the advantages of high resolution and large magnification, and 3D modeling of small and detailed objects can provide richer information. However, defocus blur in macrophotography is a long-standing problem that heavily hinders the clear imaging of the captured objects and high-quality 3D reconstruction of them. Traditional image deblurring methods require a large number of images and annotations, and there is currently no multi-view 3D reconstruction method for macrophotography. In this work, we propose a joint deblurring and 3D reconstruction method for macrophotography. Starting from multi-view blurry images captured, we jointly optimize the clear 3D model of the object and the defocus blur kernel of each pixel. The entire framework adopts a differentiable rendering method to self-supervise the optimization of the 3D model and the defocus blur kernel. Extensive experiments show that from a small number of multi-view images, our proposed method can not only achieve high-quality image deblurring but also recover high-fidelity 3D appearance.

</details>


### [11] [FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring](https://arxiv.org/abs/2510.01641)
*Xiaoyang Liu,Zhengyan Zhou,Zihang Xu,Jiezhang Cao,Zheng Chen,Yulun Zhang*

Main category: cs.CV

TL;DR: FideDiff是一个单步扩散模型，通过将运动去模糊重新表述为扩散过程，训练一致性模型实现一步高质量去模糊，在保持高保真度的同时显著减少推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在图像去模糊任务中虽然生成能力强，但存在推理时间过长和保真度不足的问题，限制了其实际应用潜力。

Method: 将运动去模糊重新表述为扩散过程，每个时间步代表逐渐模糊的图像；训练一致性模型将所有时间步对齐到同一清晰图像；结合Kernel ControlNet进行模糊核估计和自适应时间步预测。

Result: 在全参考指标上表现优异，超越了之前的扩散方法，与其他最先进模型性能相当。

Conclusion: FideDiff为预训练扩散模型在高保真图像恢复任务中的应用提供了新方向，为实际工业应用建立了坚实基础。

Abstract: Recent advancements in image motion deblurring, driven by CNNs and transformers, have made significant progress. Large-scale pre-trained diffusion models, which are rich in true-world modeling, have shown great promise for high-quality image restoration tasks such as deblurring, demonstrating stronger generative capabilities than CNN and transformer-based methods. However, challenges such as unbearable inference time and compromised fidelity still limit the full potential of the diffusion models. To address this, we introduce FideDiff, a novel single-step diffusion model designed for high-fidelity deblurring. We reformulate motion deblurring as a diffusion-like process where each timestep represents a progressively blurred image, and we train a consistency model that aligns all timesteps to the same clean image. By reconstructing training data with matched blur trajectories, the model learns temporal consistency, enabling accurate one-step deblurring. We further enhance model performance by integrating Kernel ControlNet for blur kernel estimation and introducing adaptive timestep prediction. Our model achieves superior performance on full-reference metrics, surpassing previous diffusion-based methods and matching the performance of other state-of-the-art models. FideDiff offers a new direction for applying pre-trained diffusion models to high-fidelity image restoration tasks, establishing a robust baseline for further advancing diffusion models in real-world industrial applications. Our dataset and code will be available at https://github.com/xyLiu339/FideDiff.

</details>


### [12] [Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery](https://arxiv.org/abs/2510.01662)
*Minh Tran,Maksim Siniukov,Zhangyu Jin,Mohammad Soleymani*

Main category: cs.CV

TL;DR: 提出了离散面部编码（DFE），一种无监督的数据驱动方法，通过残差向量量化变分自编码器从3D网格序列学习紧凑可解释的面部表情字典，优于传统FACS系统。


<details>
  <summary>Details</summary>
Motivation: 现有面部表情编码系统如FACS存在覆盖范围有限和手动标注成本高的问题，需要一种更精确、可扩展的替代方案。

Method: 使用3D形变模型提取身份不变的表情特征，然后通过残差向量量化变分自编码器编码这些特征，生成来自共享码本的离散标记序列。

Result: DFE比FACS和其他面部编码方法捕获更精确的面部行为，在压力检测、性格预测和抑郁检测等心理任务中表现优于FACS和Masked Autoencoders等模型。

Conclusion: DFE作为一种可扩展且有效的替代方案，具有在心理学和情感计算应用中替代FACS的潜力，覆盖了更广泛的面部表情显示。

Abstract: Facial expression analysis is central to understanding human behavior, yet existing coding systems such as the Facial Action Coding System (FACS) are constrained by limited coverage and costly manual annotation. In this work, we introduce Discrete Facial Encoding (DFE), an unsupervised, data-driven alternative of compact and interpretable dictionary of facial expressions from 3D mesh sequences learned through a Residual Vector Quantized Variational Autoencoder (RVQ-VAE). Our approach first extracts identity-invariant expression features from images using a 3D Morphable Model (3DMM), effectively disentangling factors such as head pose and facial geometry. We then encode these features using an RVQ-VAE, producing a sequence of discrete tokens from a shared codebook, where each token captures a specific, reusable facial deformation pattern that contributes to the overall expression. Through extensive experiments, we demonstrate that Discrete Facial Encoding captures more precise facial behaviors than FACS and other facial encoding alternatives. We evaluate the utility of our representation across three high-level psychological tasks: stress detection, personality prediction, and depression detection. Using a simple Bag-of-Words model built on top of the learned tokens, our system consistently outperforms both FACS-based pipelines and strong image and video representation learning models such as Masked Autoencoders. Further analysis reveals that our representation covers a wider variety of facial displays, highlighting its potential as a scalable and effective alternative to FACS for psychological and affective computing applications.

</details>


### [13] [UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction](https://arxiv.org/abs/2510.01669)
*Jin Cao,Hongrui Wu,Ziyong Feng,Hujun Bao,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: UniVerse是一个统一的鲁棒重建框架，将不一致的多视角图像通过视频扩散模型转换为一致图像，然后进行3D场景重建，解决了稀疏观测下的鲁棒重建问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖密集观测来优化模型参数，难以处理稀疏观测情况下的图像不一致问题。

Method: 将鲁棒重建分解为修复和重建两个子任务：首先将不一致图像转换为初始视频，然后使用专门设计的视频扩散模型修复为一致图像，最后从修复后的图像重建3D场景。

Result: 在合成和真实数据集上的实验表明，该方法具有强大的泛化能力和优越的鲁棒重建性能，并能控制重建3D场景的风格。

Conclusion: UniVerse通过解耦修复和重建任务，利用扩散模型学习的大规模场景先验，有效解决了多视角图像不一致的鲁棒重建问题。

Abstract: This paper tackles the challenge of robust reconstruction, i.e., the task of reconstructing a 3D scene from a set of inconsistent multi-view images. Some recent works have attempted to simultaneously remove image inconsistencies and perform reconstruction by integrating image degradation modeling into neural 3D scene representations.However, these methods rely heavily on dense observations for robustly optimizing model parameters.To address this issue, we propose to decouple robust reconstruction into two subtasks: restoration and reconstruction, which naturally simplifies the optimization process.To this end, we introduce UniVerse, a unified framework for robust reconstruction based on a video diffusion model. Specifically, UniVerse first converts inconsistent images into initial videos, then uses a specially designed video diffusion model to restore them into consistent images, and finally reconstructs the 3D scenes from these restored images.Compared with case-by-case per-view degradation modeling, the diffusion model learns a general scene prior from large-scale data, making it applicable to diverse image inconsistencies.Extensive experiments on both synthetic and real-world datasets demonstrate the strong generalization capability and superior performance of our method in robust reconstruction. Moreover, UniVerse can control the style of the reconstructed 3D scene. Project page: https://jin-cao-tma.github.io/UniVerse.github.io/

</details>


### [14] [FreeViS: Training-free Video Stylization with Inconsistent References](https://arxiv.org/abs/2510.01686)
*Jiacong Xu,Yiqun Mei,Ke Zhang,Vishal M. Patel*

Main category: cs.CV

TL;DR: FreeViS是一个无需训练的视频风格化框架，通过整合多个风格化参考到预训练的图生视频模型中，生成具有丰富风格细节和强时序一致性的风格化视频。


<details>
  <summary>Details</summary>
Motivation: 现有视频风格化方法存在时序不一致、风格细节不足的问题，而训练专门的视频风格化模型需要配对视频数据且计算成本高。

Method: 整合多个风格化参考到预训练I2V模型，使用高频补偿约束内容布局和运动，结合基于光流的运动线索在低显著性区域保留风格纹理。

Result: FreeViS在风格化保真度和时序一致性方面优于现有基线方法，获得强烈的人类偏好。

Conclusion: 该无需训练的流水线为高质量、时序一致的视频风格化提供了实用且经济的解决方案。

Abstract: Video stylization plays a key role in content creation, but it remains a challenging problem. Na\"ively applying image stylization frame-by-frame hurts temporal consistency and reduces style richness. Alternatively, training a dedicated video stylization model typically requires paired video data and is computationally expensive. In this paper, we propose FreeViS, a training-free video stylization framework that generates stylized videos with rich style details and strong temporal coherence. Our method integrates multiple stylized references to a pretrained image-to-video (I2V) model, effectively mitigating the propagation errors observed in prior works, without introducing flickers and stutters. In addition, it leverages high-frequency compensation to constrain the content layout and motion, together with flow-based motion cues to preserve style textures in low-saliency regions. Through extensive evaluations, FreeViS delivers higher stylization fidelity and superior temporal consistency, outperforming recent baselines and achieving strong human preference. Our training-free pipeline offers a practical and economic solution for high-quality, temporally coherent video stylization. The code and videos can be accessed via https://xujiacong.github.io/FreeViS/

</details>


### [15] [PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal Positional Encoding and Reinforcement Learning](https://arxiv.org/abs/2510.01715)
*Raahul Krishna Durairaju,K. Saruladha*

Main category: cs.CV

TL;DR: PyramidStyler是一个基于Transformer的神经风格迁移框架，采用金字塔位置编码和强化学习优化，在保持实时推理速度的同时显著降低了内容和风格损失。


<details>
  <summary>Details</summary>
Motivation: 现有的CNN和Transformer模型在处理复杂风格和高分辨率输入时效率低下，需要更高效的架构来实现高质量的实时艺术渲染。

Method: 提出PyramidStyler框架，包含金字塔位置编码（PPE）来捕获多尺度特征，并结合强化学习动态优化风格化过程。

Result: 在COCO和WikiArt数据集上训练4000轮后，内容损失降低62.6%至2.07，风格损失降低57.4%至0.86，推理时间1.39秒；使用RL后进一步改善至内容损失2.03，风格损失0.75，推理时间1.40秒。

Conclusion: PyramidStyler实现了实时高质量的艺术渲染，在媒体和设计领域具有广泛应用前景。

Abstract: Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based algorithm, enabling AI-driven artistic image synthesis. However, existing CNN and transformer-based models struggle to scale efficiently to complex styles and high-resolution inputs. We introduce PyramidStyler, a transformer framework with Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding that captures both local details and global context while reducing computational load. We further incorporate reinforcement learning to dynamically optimize stylization, accelerating convergence. Trained on Microsoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to 2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s inference--and yields further improvements (content 2.03; style 0.75) with minimal speed penalty (1.40 s) when using RL. These results demonstrate real-time, high-quality artistic rendering, with broad applications in media and design.

</details>


### [16] [LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for Large-Scale Scene Reconstruction](https://arxiv.org/abs/2510.01767)
*Sheng-Hsiang Hung,Ting-Yu Yen,Wei-Fang Sun,Simon See,Shih-Hsuan Hung,Hung-Kuo Chu*

Main category: cs.CV

TL;DR: LoBE-GS是一个负载平衡的高效3D高斯泼溅框架，解决了大规模场景重建中的负载不均和效率问题，相比现有方法实现了2倍训练速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有分治方法在扩展3D高斯泼溅到大规模无边界场景时存在两个瓶颈：分区负载严重不均，以及粗到细流水线效率低下，导致高开销。

Method: 引入深度感知分区方法将预处理从小时级降至分钟级，基于优化的策略平衡各块可见高斯分布，并采用可见性裁剪和选择性致密化两种轻量技术。

Result: 在大规模城市和户外数据集上的评估显示，LoBE-GS相比最先进基线方法实现了2倍的端到端训练速度提升，同时保持了重建质量。

Conclusion: LoBE-GS能够扩展到传统3D高斯泼溅无法处理的大规模场景，在保持重建质量的同时显著提升了训练效率。

Abstract: 3D Gaussian Splatting (3DGS) has established itself as an efficient representation for real-time, high-fidelity 3D scene reconstruction. However, scaling 3DGS to large and unbounded scenes such as city blocks remains difficult. Existing divide-and-conquer methods alleviate memory pressure by partitioning the scene into blocks, but introduce new bottlenecks: (i) partitions suffer from severe load imbalance since uniform or heuristic splits do not reflect actual computational demands, and (ii) coarse-to-fine pipelines fail to exploit the coarse stage efficiently, often reloading the entire model and incurring high overhead. In this work, we introduce LoBE-GS, a novel Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning method that reduces preprocessing from hours to minutes, an optimization-based strategy that balances visible Gaussians -- a strong proxy for computational load -- across blocks, and two lightweight techniques, visibility cropping and selective densification, to further reduce training cost. Evaluations on large-scale urban and outdoor datasets show that LoBE-GS consistently achieves up to $2\times$ faster end-to-end training time than state-of-the-art baselines, while maintaining reconstruction quality and enabling scalability to scenes infeasible with vanilla 3DGS.

</details>


### [17] [Pack and Force Your Memory: Long-form and Consistent Video Generation](https://arxiv.org/abs/2510.01784)
*Xiaofei Wu,Guozhen Zhang,Zhiyong Xu,Yuan Zhou,Qinglin Lu,Xuming He*

Main category: cs.CV

TL;DR: 提出MemoryPack和Direct Forcing两个创新方法，解决长视频生成中的长期依赖建模和错误累积问题。


<details>
  <summary>Details</summary>
Motivation: 长视频生成面临双重挑战：需要捕获长期依赖关系，同时防止自回归解码中固有的错误累积。

Method: 1. MemoryPack：可学习的上下文检索机制，利用文本和图像信息作为全局指导，联合建模短期和长期依赖；2. Direct Forcing：高效的单步近似策略，改善训练-推理对齐，减少推理过程中的错误传播。

Result: 实现了分钟级的时间一致性，计算效率高且保持线性复杂度，显著提升了长视频生成的上下文一致性和可靠性。

Conclusion: MemoryPack和Direct Forcing共同推进了自回归视频模型的实际可用性，为长视频生成提供了有效的解决方案。

Abstract: Long-form video generation presents a dual challenge: models must capture long-range dependencies while preventing the error accumulation inherent in autoregressive decoding. To address these challenges, we make two contributions. First, for dynamic context modeling, we propose MemoryPack, a learnable context-retrieval mechanism that leverages both textual and image information as global guidance to jointly model short- and long-term dependencies, achieving minute-level temporal consistency. This design scales gracefully with video length, preserves computational efficiency, and maintains linear complexity. Second, to mitigate error accumulation, we introduce Direct Forcing, an efficient single-step approximating strategy that improves training-inference alignment and thereby curtails error propagation during inference. Together, MemoryPack and Direct Forcing substantially enhance the context consistency and reliability of long-form video generation, advancing the practical usability of autoregressive video models.

</details>


### [18] [Leveraging Prior Knowledge of Diffusion Model for Person Search](https://arxiv.org/abs/2510.01841)
*Giyeol Kim,Sooyoung Yang,Jihyong Oh,Myungjoo Kang,Chanho Eom*

Main category: cs.CV

TL;DR: DiffPS是一个新颖的人员搜索框架，利用预训练扩散模型解决传统方法中检测和重识别任务间的优化冲突，通过三个专门模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有人员搜索方法主要使用ImageNet预训练骨干网络，可能无法充分捕捉复杂空间上下文和细粒度身份线索。同时，共享骨干网络特征导致检测和重识别任务间的优化目标冲突。

Method: 提出DiffPS框架，利用预训练扩散模型，包含三个专门模块：扩散引导区域提议网络(DGRPN)用于增强人员定位、多尺度频率细化网络(MSFRN)减轻形状偏差、语义自适应特征聚合网络(SFAN)利用文本对齐的扩散特征。

Result: 在CUHK-SYSU和PRW数据集上达到了新的最先进水平。

Conclusion: DiffPS通过利用扩散先验知识成功解决了人员搜索中检测和重识别任务的优化冲突问题，显著提升了性能。

Abstract: Person search aims to jointly perform person detection and re-identification by localizing and identifying a query person within a gallery of uncropped scene images. Existing methods predominantly utilize ImageNet pre-trained backbones, which may be suboptimal for capturing the complex spatial context and fine-grained identity cues necessary for person search. Moreover, they rely on a shared backbone feature for both person detection and re-identification, leading to suboptimal features due to conflicting optimization objectives. In this paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a novel framework that leverages a pre-trained diffusion model while eliminating the optimization conflict between two sub-tasks. We analyze key properties of diffusion priors and propose three specialized modules: (i) Diffusion-Guided Region Proposal Network (DGRPN) for enhanced person localization, (ii) Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and (iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage text-aligned diffusion features. DiffPS sets a new state-of-the-art on CUHK-SYSU and PRW.

</details>


### [19] [4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing](https://arxiv.org/abs/2510.01991)
*Lei Liu,Can Wang,Zhenghao Chen,Dong Xu*

Main category: cs.CV

TL;DR: 4DGS-Craft是一个一致且交互式的4D高斯泼溅编辑框架，通过4D感知的InstructPix2Pix模型、多视图网格模块、高斯选择机制和基于LLM的用户意图理解模块，解决了视图、时间和非编辑区域一致性问题，并能处理复杂的文本指令。


<details>
  <summary>Details</summary>
Motivation: 现有的4D高斯泼溅编辑方法在视图一致性、时间一致性、非编辑区域一致性以及处理复杂文本指令方面仍面临挑战，需要开发更一致和可控的4D场景编辑框架。

Method: 1. 引入4D感知的InstructPix2Pix模型，结合4D VGGT几何特征；2. 使用多视图网格模块迭代优化多视图输入图像；3. 提出高斯选择机制仅优化编辑区域的Gaussians；4. 设计基于LLM的用户意图理解模块分解复杂指令。

Result: 与相关工作相比，该方法实现了更一致和可控的4D场景编辑，能够处理复杂的用户指令并保持编辑质量。

Conclusion: 4DGS-Craft框架通过综合运用4D感知模型、一致性增强机制和用户交互模块，有效解决了4D高斯泼溅编辑中的关键挑战，为4D场景编辑提供了更强大的解决方案。

Abstract: Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges with view, temporal, and non-editing region consistency, as well as with handling complex text instructions. To address these issues, we propose 4DGS-Craft, a consistent and interactive 4DGS editing framework. We first introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal consistency. This model incorporates 4D VGGT geometry features extracted from the initial scene, enabling it to capture underlying 4D geometric structures during editing. We further enhance this model with a multi-view grid module that enforces consistency by iteratively refining multi-view input images while jointly optimizing the underlying 4D scene. Furthermore, we preserve the consistency of non-edited regions through a novel Gaussian selection mechanism, which identifies and optimizes only the Gaussians within the edited regions. Beyond consistency, facilitating user interaction is also crucial for effective 4DGS editing. Therefore, we design an LLM-based module for user intent understanding. This module employs a user instruction template to define atomic editing operations and leverages an LLM for reasoning. As a result, our framework can interpret user intent and decompose complex instructions into a logical sequence of atomic operations, enabling it to handle intricate user commands and further enhance editing performance. Compared to related works, our approach enables more consistent and controllable 4D scene editing. Our code will be made available upon acceptance.

</details>


### [20] [Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing Routing in Lightweight Image Super-Resolution](https://arxiv.org/abs/2510.01997)
*Junyu Wu,Jie Tang,Jie Liu,Gangshan Wu*

Main category: cs.CV

TL;DR: 提出Pure-Pass(PP)像素级掩码机制，通过固定颜色中心点分类像素，免除纯像素的昂贵计算，集成到ATD-light模型中实现更优的超分辨率性能。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级超分辨率方法如CAMixer存在适应性差、掩码粒度粗和空间灵活性不足等限制，需要更精细的计算优化机制。

Method: 使用像素级掩码机制，基于固定颜色中心点将像素分类，识别纯像素并免除其昂贵计算，集成到ATD-light模型中。

Result: PP-ATD-light在重建质量和参数效率上优于CAMixer-ATD-light，同时节省相似计算量。

Conclusion: Pure-Pass机制通过精细的像素级掩码实现了高效计算，在超分辨率任务中取得了优越的性能表现。

Abstract: Image Super-Resolution (SR) aims to reconstruct high-resolution images from low-resolution counterparts, but the computational complexity of deep learning-based methods often hinders practical deployment. CAMixer is the pioneering work to integrate the advantages of existing lightweight SR methods and proposes a content-aware mixer to route token mixers of varied complexities according to the difficulty of content recovery. However, several limitations remain, such as poor adaptability, coarse-grained masking and spatial inflexibility, among others. We propose Pure-Pass (PP), a pixel-level masking mechanism that identifies pure pixels and exempts them from expensive computations. PP utilizes fixed color center points to classify pixels into distinct categories, enabling fine-grained, spatially flexible masking while maintaining adaptive flexibility. Integrated into the state-of-the-art ATD-light model, PP-ATD-light achieves superior SR performance with minimal overhead, outperforming CAMixer-ATD-light in reconstruction quality and parameter efficiency when saving a similar amount of computation.

</details>


### [21] [GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing](https://arxiv.org/abs/2510.02034)
*Mengtian Li,Yunshu Bai,Yimin Chu,Yijun Shen,Zhongmei Li,Weifeng Ge,Zhifeng Xie,Chaofeng Chen*

Main category: cs.CV

TL;DR: GaussianMorphing是一个从多视角图像进行语义感知3D形状和纹理形变的新框架，通过网格引导的3D高斯溅射实现高保真几何和外观建模，无需标记数据即可保持局部细节和全局语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖点云或需要预定义同胚映射来处理无纹理数据，存在局限性。需要一种能够同时处理几何和纹理形变，且无需监督的方法。

Method: 采用网格引导的3D高斯溅射，通过统一变形策略将3D高斯锚定到重建的网格面片上，利用网格拓扑作为几何先验建立无监督语义对应，并通过物理合理的点轨迹保持结构完整性。

Result: 在TexMorph基准测试中，GaussianMorphing显著优于先前的2D/3D方法，将颜色一致性误差(ΔE)降低了22.2%，EI降低了26.2%。

Conclusion: 该框架成功实现了语义感知的3D形状和纹理形变，在保持纹理保真度和几何一致性的同时，无需标记数据即可建立语义对应关系。

Abstract: We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape and texture morphing from multi-view images. Previous approaches usually rely on point clouds or require pre-defined homeomorphic mappings for untextured data. Our method overcomes these limitations by leveraging mesh-guided 3D Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling. The core of our framework is a unified deformation strategy that anchors 3DGaussians to reconstructed mesh patches, ensuring geometrically consistent transformations while preserving texture fidelity through topology-aware constraints. In parallel, our framework establishes unsupervised semantic correspondence by using the mesh topology as a geometric prior and maintains structural integrity via physically plausible point trajectories. This integrated approach preserves both local detail and global semantic coherence throughout the morphing process with out requiring labeled data. On our proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior 2D/3D methods, reducing color consistency error ($\Delta E$) by 22.2% and EI by 26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/

</details>


### [22] [Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers](https://arxiv.org/abs/2510.02043)
*Sahil Bhandary Karnoor,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: 提出InPose方法，使用预训练扩散模型和仅旋转测量来实现零样本泛化的姿态估计


<details>
  <summary>Details</summary>
Motivation: 现有基于条件扩散模型的方法难以跨用户泛化，主要因为位置测量高度受用户体型影响

Method: 将姿态估计建模为逆问题，使用预训练扩散模型仅以旋转测量为条件，通过似然项引导模型生成符合测量位置的姿态序列

Result: InPose方法能够零样本泛化到不同用户，生成符合稀疏体感测量数据的高概率姿态序列

Conclusion: 提出的逆问题框架结合扩散模型和旋转测量，实现了跨用户零样本泛化的姿态估计

Abstract: Pose estimation refers to tracking a human's full body posture, including their head, torso, arms, and legs. The problem is challenging in practical settings where the number of body sensors are limited. Past work has shown promising results using conditional diffusion models, where the pose prediction is conditioned on both <location, rotation> measurements from the sensors. Unfortunately, nearly all these approaches generalize poorly across users, primarly because location measurements are highly influenced by the body size of the user. In this paper, we formulate pose estimation as an inverse problem and design an algorithm capable of zero-shot generalization. Our idea utilizes a pre-trained diffusion model and conditions it on rotational measurements alone; the priors from this model are then guided by a likelihood term, derived from the measured locations. Thus, given any user, our proposed InPose method generatively estimates the highly likely sequence of poses that best explains the sparse on-body measurements.

</details>


### [23] [VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation](https://arxiv.org/abs/2510.02086)
*Arman Behnam*

Main category: cs.CV

TL;DR: 提出VGDM框架，将视觉变换器嵌入扩散过程，结合全局上下文推理和迭代去噪来提升脑肿瘤分割的准确性和边界精度。


<details>
  <summary>Details</summary>
Motivation: 传统卷积架构如U-Net在捕捉长距离依赖关系方面能力有限，限制了在复杂肿瘤结构上的性能表现。扩散模型在生成高保真医学图像和细化分割边界方面显示出强大潜力。

Method: 在扩散过程核心嵌入视觉变换器，利用变换器骨干网络建模整个MRI体积的空间关系，同时通过扩散细化减少体素级误差并恢复细粒度肿瘤细节。

Result: 在MRI脑肿瘤数据集上的实验验证显示，在Dice相似系数和Hausdorff距离指标上获得一致提升。

Conclusion: 变换器引导的扩散模型有潜力推动肿瘤分割技术发展，超越传统的U-Net基线方法。

Abstract: Accurate detection and segmentation of brain tumors from magnetic resonance imaging (MRI) are essential for diagnosis, treatment planning, and clinical monitoring. While convolutional architectures such as U-Net have long been the backbone of medical image segmentation, their limited capacity to capture long-range dependencies constrains performance on complex tumor structures. Recent advances in diffusion models have demonstrated strong potential for generating high-fidelity medical images and refining segmentation boundaries.   In this work, we propose VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation framework, a transformer-driven diffusion framework for brain tumor detection and segmentation. By embedding a vision transformer at the core of the diffusion process, the model leverages global contextual reasoning together with iterative denoising to enhance both volumetric accuracy and boundary precision. The transformer backbone enables more effective modeling of spatial relationships across entire MRI volumes, while diffusion refinement mitigates voxel-level errors and recovers fine-grained tumor details.   This hybrid design provides a pathway toward improved robustness and scalability in neuro-oncology, moving beyond conventional U-Net baselines. Experimental validation on MRI brain tumor datasets demonstrates consistent gains in Dice similarity and Hausdorff distance, underscoring the potential of transformer-guided diffusion models to advance the state of the art in tumor segmentation.

</details>


### [24] [TempoControl: Temporal Attention Guidance for Text-to-Video Models](https://arxiv.org/abs/2510.02226)
*Shira Schiber,Ofir Lindenbaum,Idan Schwartz*

Main category: cs.CV

TL;DR: TempoControl是一种无需重新训练或额外监督的方法，通过利用文本到视频扩散模型中的交叉注意力图，实现对生成视频中视觉概念时间对齐的精确控制。


<details>
  <summary>Details</summary>
Motivation: 现有的生成视频模型缺乏细粒度的时间控制能力，无法让用户指定特定视觉元素在生成序列中出现的时间点。

Method: 利用交叉注意力图，通过新颖的优化方法指导概念出现的时间，采用三个互补原则：通过相关性对齐时间形状、通过能量放大可见性区域、通过熵保持空间焦点。

Result: TempoControl能够在确保高质量和多样性的同时，实现对时间点的精确控制，适用于时间重排序、动作和音频对齐生成等多种视频生成应用。

Conclusion: 该方法为生成视频提供了有效的时间控制能力，无需额外训练或监督，在保持视频质量的同时实现了对视觉概念出现时间的精确操控。

Abstract: Recent advances in generative video models have enabled the creation of high-quality videos based on natural language prompts. However, these models frequently lack fine-grained temporal control, meaning they do not allow users to specify when particular visual elements should appear within a generated sequence. In this work, we introduce TempoControl, a method that allows for temporal alignment of visual concepts during inference, without requiring retraining or additional supervision. TempoControl utilizes cross-attention maps, a key component of text-to-video diffusion models, to guide the timing of concepts through a novel optimization approach. Our method steers attention using three complementary principles: aligning its temporal shape with a control signal (via correlation), amplifying it where visibility is needed (via energy), and maintaining spatial focus (via entropy). TempoControl allows precise control over timing while ensuring high video quality and diversity. We demonstrate its effectiveness across various video generation applications, including temporal reordering for single and multiple objects, as well as action and audio-aligned generation.

</details>


### [25] [DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing](https://arxiv.org/abs/2510.02253)
*Zihan Zhou,Shilin Lu,Shuli Leng,Shaocong Zhang,Zhuming Lian,Xinlei Yu,Adams Wai-Kin Kong*

Main category: cs.CV

TL;DR: DragFlow是首个有效利用FLUX强大先验进行拖拽式图像编辑的框架，通过区域级编辑范式、预训练适配器和MLLM解决任务歧义，在多个基准测试中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型从UNet-based DDPMs转向更可扩展的DiT with flow matching（如SD3.5、FLUX），生成先验显著增强，但拖拽式编辑尚未从中受益，现有方法在目标区域存在失真问题。

Method: 提出区域级编辑范式，使用仿射变换提供更丰富一致的特征监督；集成预训练开放域个性化适配器增强主体一致性；通过梯度掩码硬约束保持背景保真度；利用MLLM解决任务歧义。

Result: 在DragBench-DR和ReD Bench上的广泛实验表明，DragFlow超越了基于点和基于区域的基线方法，在拖拽式图像编辑中达到了新的最先进水平。

Conclusion: DragFlow成功利用了FLUX的强大先验，通过区域级编辑和多种技术集成，显著提升了拖拽式图像编辑的质量和效果，为未来相关研究提供了新方向。

Abstract: Drag-based image editing has long suffered from distortions in the target region, largely because the priors of earlier base models, Stable Diffusion, are insufficient to project optimized latents back onto the natural image manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow matching (e.g., SD3.5, FLUX), generative priors have become significantly stronger, enabling advances across diverse editing tasks. However, drag-based editing has yet to benefit from these stronger priors. This work proposes the first framework to effectively harness FLUX's rich prior for drag-based editing, dubbed DragFlow, achieving substantial gains over baselines. We first show that directly applying point-based drag editing to DiTs performs poorly: unlike the highly compressed features of UNets, DiT features are insufficiently structured to provide reliable guidance for point-wise motion supervision. To overcome this limitation, DragFlow introduces a region-based editing paradigm, where affine transformations enable richer and more consistent feature supervision. Additionally, we integrate pretrained open-domain personalization adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving background fidelity through gradient mask-based hard constraints. Multimodal large language models (MLLMs) are further employed to resolve task ambiguities. For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench) featuring region-level dragging instructions. Extensive experiments on DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and region-based baselines, setting a new state-of-the-art in drag-based image editing. Code and datasets will be publicly available upon publication.

</details>


### [26] [NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes](https://arxiv.org/abs/2510.02266)
*Shiyi Zhang,Dong Liang,Yihang Zhou*

Main category: cs.CV

TL;DR: NeuroSwift通过整合AutoKL和CLIP适配器的扩散模型，实现了从fMRI数据中准确重建视觉刺激的跨被试方法，仅需训练17%参数即可在轻量GPU上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决从脑活动重建视觉信息时面临的跨被试差异性和大脑对复杂视觉输入进行抽象语义编码的挑战，以及现有方法计算需求大的问题。

Method: 提出NeuroSwift框架，整合AutoKL适配器处理低级特征和CLIP适配器处理语义特征，通过在Stable Diffusion生成图像和COCO字幕对上训练CLIP适配器来模拟高级视觉皮层编码。采用预训练加微调策略，仅微调17%参数（全连接层）来适应新被试。

Result: 在轻量GPU（三张RTX 4090）上每个被试仅需1小时训练，就能超越现有方法，实现最先进的跨被试视觉重建性能。

Conclusion: NeuroSwift通过高效的参数微调策略和互补适配器设计，成功解决了跨被试视觉重建的挑战，为理解视觉神经机制提供了实用工具。

Abstract: Reconstructing visual information from brain activity via computer vision technology provides an intuitive understanding of visual neural mechanisms. Despite progress in decoding fMRI data with generative models, achieving accurate cross-subject reconstruction of visual stimuli remains challenging and computationally demanding. This difficulty arises from inter-subject variability in neural representations and the brain's abstract encoding of core semantic features in complex visual inputs. To address these challenges, we propose NeuroSwift, which integrates complementary adapters via diffusion: AutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter is trained on Stable Diffusion generated images paired with COCO captions to emulate higher visual cortex encoding. For cross-subject generalization, we pretrain on one subject and then fine-tune only 17 percent of parameters (fully connected layers) for new subjects, while freezing other components. This enables state-of-the-art performance with only one hour of training per subject on lightweight GPUs (three RTX 4090), and it outperforms existing methods.

</details>


### [27] [Self-Forcing++: Towards Minute-Scale High-Quality Video Generation](https://arxiv.org/abs/2510.02283)
*Justin Cui,Jie Wu,Ming Li,Tao Yang,Xiaojie Li,Rui Wang,Andrew Bai,Yuanhao Ban,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 提出了一种简单有效的方法来缓解长视频生成中的质量退化问题，无需长视频教师监督或重新训练长视频数据集，通过利用教师模型的丰富知识为自生成长视频提供指导。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像和视频生成方面取得了革命性进展，但基于transformer架构的计算成本过高，特别是在生成长视频时。现有自回归方法通过从短时双向教师蒸馏，但由于教师模型无法合成长视频，学生模型在训练范围外往往出现严重质量退化。

Method: 利用教师模型的丰富知识，通过从自生成长视频中抽取片段来为学生模型提供指导。该方法保持时间一致性，可将视频长度扩展到教师能力的20倍，避免过度曝光和错误累积问题。

Result: 当扩展计算时，该方法能生成长达4分15秒的视频，相当于基础模型位置嵌入支持的最大跨度的99.9%，比基线模型长50倍以上。在标准基准测试和改进基准测试中，该方法在保真度和一致性方面显著优于基线方法。

Conclusion: 该方法有效缓解了长视频生成中的质量退化问题，无需长视频监督或重新训练，能够生成高质量的长时视频内容。

Abstract: Diffusion models have revolutionized image and video generation, achieving unprecedented visual quality. However, their reliance on transformer architectures incurs prohibitively high computational costs, particularly when extending generation to long videos. Recent work has explored autoregressive formulations for long video generation, typically by distilling from short-horizon bidirectional teachers. Nevertheless, given that teacher models cannot synthesize long videos, the extrapolation of student models beyond their training horizon often leads to pronounced quality degradation, arising from the compounding of errors within the continuous latent space. In this paper, we propose a simple yet effective approach to mitigate quality degradation in long-horizon video generation without requiring supervision from long-video teachers or retraining on long video datasets. Our approach centers on exploiting the rich knowledge of teacher models to provide guidance for the student model through sampled segments drawn from self-generated long videos. Our method maintains temporal consistency while scaling video length by up to 20x beyond teacher's capability, avoiding common issues such as over-exposure and error-accumulation without recomputing overlapping frames like previous methods. When scaling up the computation, our method shows the capability of generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the maximum span supported by our base model's position embedding and more than 50x longer than that of our baseline model. Experiments on standard benchmarks and our proposed improved benchmark demonstrate that our approach substantially outperforms baseline methods in both fidelity and consistency. Our long-horizon videos demo can be found at https://self-forcing-plus-plus.github.io/

</details>


### [28] [Learning to Generate Object Interactions with Physics-Guided Video Diffusion](https://arxiv.org/abs/2510.02284)
*David Romero,Ariana Bermudez,Hao Li,Fabio Pizzati,Ivan Laptev*

Main category: cs.CV

TL;DR: KineMask是一种物理引导的视频生成方法，通过两阶段训练策略和对象掩码，实现了对刚体运动的真实控制和复杂物理交互的生成。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在物理交互方面仍存在不足，缺乏基于物理的控制机制。本文旨在解决这一问题，使视频生成能够模拟真实的物理交互。

Method: 提出两阶段训练策略：第一阶段使用完整的运动监督，第二阶段逐渐移除未来运动监督。结合对象掩码和预测性场景描述，将低级运动控制与高级文本条件相结合。

Result: 在合成场景和真实场景中都显著改善了对象交互效果，相比同类模型有显著提升。消融研究验证了低级和高级条件在视频扩散模型中的互补作用。

Conclusion: KineMask通过物理引导的视频生成方法，成功实现了对刚体运动的真实控制和复杂物理现象的合成，为机器人学和具身决策提供了更好的世界模拟器。

Abstract: Recent models for video generation have achieved remarkable progress and are now deployed in film, social media production, and advertising. Beyond their creative potential, such models also hold promise as world simulators for robotics and embodied decision making. Despite strong advances, however, current approaches still struggle to generate physically plausible object interactions and lack physics-grounded control mechanisms. To address this limitation, we introduce KineMask, an approach for physics-guided video generation that enables realistic rigid body control, interactions, and effects. Given a single image and a specified object velocity, our method generates videos with inferred motions and future object interactions. We propose a two-stage training strategy that gradually removes future motion supervision via object masks. Using this strategy we train video diffusion models (VDMs) on synthetic scenes of simple interactions and demonstrate significant improvements of object interactions in real scenes. Furthermore, KineMask integrates low-level motion control with high-level textual conditioning via predictive scene descriptions, leading to effective support for synthesis of complex dynamical phenomena. Extensive experiments show that KineMask achieves strong improvements over recent models of comparable size. Ablation studies further highlight the complementary roles of low- and high-level conditioning in VDMs. Our code, model, and data will be made publicly available.

</details>


### [29] [MultiModal Action Conditioned Video Generation](https://arxiv.org/abs/2510.02287)
*Yichen Li,Antonio Torralba*

Main category: cs.CV

TL;DR: 该论文提出了一种多模态动作表示方法，通过整合本体感觉、运动感觉、力触觉和肌肉激活等感官信息，实现精细控制的视频模拟，解决了现有视频模型在精细控制方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前视频模型缺乏精细控制能力，无法作为世界模型。通用家用机器人需要实时精细运动控制来处理精细任务和紧急情况。

Method: 引入多模态感官动作表示，开发特征学习范式对齐不同模态，同时保留各模态独特信息，并提出正则化方案增强动作轨迹特征的因果性。

Result: 实验表明，整合多模态感官提高了模拟精度，减少了时间漂移。消融研究和下游应用验证了方法的有效性和实用性。

Conclusion: 多模态感官的整合能够有效提升精细控制任务的模拟性能，为机器人精细操作提供了可行的解决方案。

Abstract: Current video models fail as world model as they lack fine-graiend control. General-purpose household robots require real-time fine motor control to handle delicate tasks and urgent situations. In this work, we introduce fine-grained multimodal actions to capture such precise control. We consider senses of proprioception, kinesthesia, force haptics, and muscle activation. Such multimodal senses naturally enables fine-grained interactions that are difficult to simulate with text-conditioned generative models. To effectively simulate fine-grained multisensory actions, we develop a feature learning paradigm that aligns these modalities while preserving the unique information each modality provides. We further propose a regularization scheme to enhance causality of the action trajectory features in representing intricate interaction dynamics. Experiments show that incorporating multimodal senses improves simulation accuracy and reduces temporal drift. Extensive ablation studies and downstream applications demonstrate the effectiveness and practicality of our work.

</details>


### [30] [NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation](https://arxiv.org/abs/2510.02307)
*Ruozhen He,Moayed Haji-Ali,Ziyan Yang,Vicente Ordonez*

Main category: cs.CV

TL;DR: NoiseShift是一种无需训练的方法，通过根据分辨率大小重新校准去噪器的噪声水平，显著提升扩散模型在低分辨率下的图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 高分辨率文本到图像生成器无法为不需要高分辨率图像的用户提供开箱即用的经济高效替代方案，因为模型在不同分辨率下泛化能力差。

Method: 识别噪声调度器在不同分辨率下具有不等的感知效果，提出NoiseShift方法重新校准去噪器的噪声水平，无需改变模型架构或采样计划。

Result: 在LAION-COCO数据集上，NoiseShift将SD3.5的FID提升15.89%，SD3提升8.56%，Flux-Dev提升2.44%；在CelebA数据集上，分别提升10.36%、5.19%和3.02%。

Conclusion: NoiseShift能有效缓解分辨率相关的伪影，提升低分辨率图像生成质量，且与现有模型兼容。

Abstract: Text-to-image diffusion models trained on a fixed set of resolutions often fail to generalize, even when asked to generate images at lower resolutions than those seen during training. High-resolution text-to-image generators are currently unable to easily offer an out-of-the-box budget-efficient alternative to their users who might not need high-resolution images. We identify a key technical insight in diffusion models that when addressed can help tackle this limitation: Noise schedulers have unequal perceptual effects across resolutions. The same level of noise removes disproportionately more signal from lower-resolution images than from high-resolution images, leading to a train-test mismatch. We propose NoiseShift, a training-free method that recalibrates the noise level of the denoiser conditioned on resolution size. NoiseShift requires no changes to model architecture or sampling schedule and is compatible with existing models. When applied to Stable Diffusion 3, Stable Diffusion 3.5, and Flux-Dev, quality at low resolutions is significantly improved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and Flux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by 10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results demonstrate the effectiveness of NoiseShift in mitigating resolution-dependent artifacts and enhancing the quality of low-resolution image generation.

</details>


### [31] [StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions](https://arxiv.org/abs/2510.02314)
*Bo-Hsu Ke,You-Zhe Xie,Yu-Lun Liu,Wei-Chen Chiu*

Main category: cs.CV

TL;DR: 本文分析了3D高斯溅射(3DGS)对图像级投毒攻击的脆弱性，提出了一种基于密度的投毒方法，通过在低密度区域注入高斯点来嵌入视角依赖的虚假对象，同时引入自适应噪声策略破坏多视角一致性。


<details>
  <summary>Details</summary>
Motivation: 随着NeRF和3DGS等3D场景表示方法的普及，解决其安全漏洞变得至关重要。本文旨在分析3DGS对投毒攻击的鲁棒性，并提出有效的攻击方法。

Method: 提出密度引导的投毒方法：1) 使用核密度估计识别低密度区域；2) 在这些区域战略性地注入高斯点，嵌入视角依赖的虚假对象；3) 引入自适应噪声策略破坏多视角一致性。

Result: 大量实验表明，该方法在性能上优于现有技术，能够有效嵌入虚假对象，在投毒视图中清晰可见，同时对无辜视图影响最小。

Conclusion: 本文揭示了3DGS的安全漏洞，提出了有效的投毒攻击方法，并建立了基于KDE的评估协议，为未来研究提供了客观基准。

Abstract: 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As these methods become prevalent, addressing their vulnerabilities becomes critical. We analyze 3DGS robustness against image-level poisoning attacks and propose a novel density-guided poisoning method. Our method strategically injects Gaussian points into low-density regions identified via Kernel Density Estimation (KDE), embedding viewpoint-dependent illusory objects clearly visible from poisoned views while minimally affecting innocent views. Additionally, we introduce an adaptive noise strategy to disrupt multi-view consistency, further enhancing attack effectiveness. We propose a KDE-based evaluation protocol to assess attack difficulty systematically, enabling objective benchmarking for future research. Extensive experiments demonstrate our method's superior performance compared to state-of-the-art techniques. Project page: https://hentci.github.io/stealthattack/

</details>


### [32] [Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity](https://arxiv.org/abs/2510.02315)
*Eric Tillmann Bill,Enis Simsar,Thomas Hofmann*

Main category: cs.CV

TL;DR: 提出了首个理论框架和优化目标，通过随机最优控制来引导采样过程，解决文本到图像模型在多主体生成中的属性泄漏、身份纠缠和主体遗漏问题。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型在单主体提示上表现优秀，但在多主体描述中经常出现属性泄漏、身份纠缠和主体遗漏等问题，需要专门的方法来提升多主体生成质量。

Method: 通过随机最优控制视角看待流匹配，提出了两种架构无关的算法：无需训练的单次更新测试时控制器，以及通过后向伴随信号回归控制网络的轻量微调方法。

Result: 在Stable Diffusion 3.5、FLUX和Stable Diffusion XL上，两种算法都显著提升了多主体对齐度，同时保持了基础模型的风格。测试时控制可在普通GPU上高效运行，微调控制器在有限提示上训练后能泛化到未见过的提示。

Conclusion: FOCUS方法在多个模型上实现了最先进的多主体保真度，为多主体文本到图像生成提供了有效的理论框架和实用算法。

Abstract: Text-to-image (T2I) models excel on single-entity prompts but struggle with multi-subject descriptions, often showing attribute leakage, identity entanglement, and subject omissions. We introduce the first theoretical framework with a principled, optimizable objective for steering sampling dynamics toward multi-subject fidelity. Viewing flow matching (FM) through stochastic optimal control (SOC), we formulate subject disentanglement as control over a trained FM sampler. This yields two architecture-agnostic algorithms: (i) a training-free test-time controller that perturbs the base velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight fine-tuning rule that regresses a control network to a backward adjoint signal while preserving base-model capabilities. The same formulation unifies prior attention heuristics, extends to diffusion models via a flow-diffusion correspondence, and provides the first fine-tuning route explicitly designed for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and Stable Diffusion XL, both algorithms consistently improve multi-subject alignment while maintaining base-model style. Test-time control runs efficiently on commodity GPUs, and fine-tuned controllers trained on limited prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal Control for Unentangled Subjects), which achieves state-of-the-art multi-subject fidelity across models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [33] [Unsupervised Dynamic Feature Selection for Robust Latent Spaces in Vision Tasks](https://arxiv.org/abs/2510.01758)
*Bruno Corcuera,Carlos Eiras-Franco,Brais Cancela*

Main category: cs.LG

TL;DR: 提出一种无监督动态特征选择(DFS)方法，通过移除图像中的误导性或冗余信息来增强潜在表示，提升模型泛化性能。


<details>
  <summary>Details</summary>
Motivation: 视觉任务中的潜在表示常受噪声或无关特征影响，这会降低模型性能和泛化能力。

Method: 为每个实例识别并移除图像中的误导或冗余信息，仅保留最相关特征贡献于潜在空间的无监督框架。

Result: 在图像数据集上的实验表明，配备无监督DFS的模型在聚类和图像生成等任务中显著提升泛化性能，计算成本增加极小。

Conclusion: 无监督DFS方法能有效增强潜在表示，提升模型性能且具有广泛适用性。

Abstract: Latent representations are critical for the performance and robustness of machine learning models, as they encode the essential features of data in a compact and informative manner. However, in vision tasks, these representations are often affected by noisy or irrelevant features, which can degrade the model's performance and generalization capabilities. This paper presents a novel approach for enhancing latent representations using unsupervised Dynamic Feature Selection (DFS). For each instance, the proposed method identifies and removes misleading or redundant information in images, ensuring that only the most relevant features contribute to the latent space. By leveraging an unsupervised framework, our approach avoids reliance on labeled data, making it broadly applicable across various domains and datasets. Experiments conducted on image datasets demonstrate that models equipped with unsupervised DFS achieve significant improvements in generalization performance across various tasks, including clustering and image generation, while incurring a minimal increase in the computational cost.

</details>


### [34] [$\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models](https://arxiv.org/abs/2510.01982)
*Yujie Zhou,Pengyang Ling,Jiazi Bu,Yibin Wang,Yuhang Zang,Jiaqi Wang,Li Niu,Guangtao Zhai*

Main category: cs.LG

TL;DR: 提出了Granular-GRPO框架，通过奇异随机采样策略和多粒度优势集成模块，解决扩散模型强化学习中奖励信号稀疏和窄化的问题，实现更精确全面的采样方向评估。


<details>
  <summary>Details</summary>
Motivation: 现有方法在探索高价值样本时，由于稀疏和窄化的奖励信号导致偏好对齐效果不佳，需要更精确的奖励评估机制。

Method: 引入奇异随机采样策略支持逐步随机探索，同时增强奖励与注入噪声的相关性；采用多粒度优势集成模块聚合多个扩散尺度的优势计算，消除固定粒度去噪的偏差。

Result: 在各种奖励模型上的实验表明，G^2RPO显著优于现有的基于流的GRPO基线，验证了其有效性和鲁棒性。

Conclusion: 提出的Granular-GRPO框架通过改进采样策略和优势集成机制，成功解决了扩散模型强化学习中的奖励评估问题，实现了更好的偏好对齐效果。

Abstract: The integration of online reinforcement learning (RL) into diffusion and flow models has recently emerged as a promising approach for aligning generative models with human preferences. Stochastic sampling via Stochastic Differential Equations (SDE) is employed during the denoising process to generate diverse denoising directions for RL exploration. While existing methods effectively explore potential high-value samples, they suffer from sub-optimal preference alignment due to sparse and narrow reward signals. To address these challenges, we propose a novel Granular-GRPO ($\text{G}^2$RPO ) framework that achieves precise and comprehensive reward assessments of sampling directions in reinforcement learning of flow models. Specifically, a Singular Stochastic Sampling strategy is introduced to support step-wise stochastic exploration while enforcing a high correlation between the reward and the injected noise, thereby facilitating a faithful reward for each SDE perturbation. Concurrently, to eliminate the bias inherent in fixed-granularity denoising, we introduce a Multi-Granularity Advantage Integration module that aggregates advantages computed at multiple diffusion scales, producing a more comprehensive and robust evaluation of the sampling directions. Experiments conducted on various reward models, including both in-domain and out-of-domain evaluations, demonstrate that our $\text{G}^2$RPO significantly outperforms existing flow-based GRPO baselines,highlighting its effectiveness and robustness.

</details>


### [35] [Continual Personalization for Diffusion Models](https://arxiv.org/abs/2510.02296)
*Yu-Chien Liao,Jr-Jen Chen,Chi-Pin Huang,Ci-Siang Lin,Meng-Lin Wu,Yu-Chiang Frank Wang*

Main category: cs.LG

TL;DR: 提出了概念神经元选择（CNS）方法，这是一种在持续学习场景下进行个性化扩散模型更新的简单有效策略，通过识别与目标概念相关的神经元来减少灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中增量更新扩散模型具有实用性但计算挑战大，需要解决灾难性遗忘问题同时保持零样本文本到图像生成能力。

Method: CNS独特识别扩散模型中与目标概念密切相关的神经元，以增量方式微调这些概念神经元，并联合保留先前学到的知识。

Result: 在真实数据集上的评估显示，CNS以最少的参数调整实现了最先进的性能，在单概念和多概念个性化任务中都优于先前方法。

Conclusion: CNS实现了无融合操作，减少了持续个性化所需的内存存储和处理时间，为扩散模型的增量更新提供了实用解决方案。

Abstract: Updating diffusion models in an incremental setting would be practical in real-world applications yet computationally challenging. We present a novel learning strategy of Concept Neuron Selection (CNS), a simple yet effective approach to perform personalization in a continual learning scheme. CNS uniquely identifies neurons in diffusion models that are closely related to the target concepts. In order to mitigate catastrophic forgetting problems while preserving zero-shot text-to-image generation ability, CNS finetunes concept neurons in an incremental manner and jointly preserves knowledge learned of previous concepts. Evaluation of real-world datasets demonstrates that CNS achieves state-of-the-art performance with minimal parameter adjustments, outperforming previous methods in both single and multi-concept personalization works. CNS also achieves fusion-free operation, reducing memory storage and processing time for continual personalization.

</details>


### [36] [Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models](https://arxiv.org/abs/2510.02300)
*Runqian Wang,Yilun Du*

Main category: cs.LG

TL;DR: EqM是一种基于平衡动力学的生成建模框架，通过隐式学习能量景观的平衡梯度，采用优化采样过程，在ImageNet 256×256上达到1.90 FID，并能处理去噪、OOD检测和图像合成等任务。


<details>
  <summary>Details</summary>
Motivation: 传统扩散和基于流的生成模型采用非平衡、时间条件动态，EqM旨在通过平衡视角简化生成过程，建立流模型与能量模型之间的桥梁。

Method: EqM丢弃时间条件动态，学习隐式能量景观的平衡梯度，在推理时通过梯度下降进行优化采样，支持可调步长、自适应优化器和自适应计算。

Result: EqM在ImageNet 256×256上实现1.90 FID，超越传统扩散/流模型，并能处理部分噪声图像去噪、OOD检测和图像合成等任务。

Conclusion: EqM通过统一平衡景观替代时间条件速度，提供了流模型与能量模型之间的紧密桥梁，以及优化驱动推理的简单路径。

Abstract: We introduce Equilibrium Matching (EqM), a generative modeling framework built from an equilibrium dynamics perspective. EqM discards the non-equilibrium, time-conditional dynamics in traditional diffusion and flow-based generative models and instead learns the equilibrium gradient of an implicit energy landscape. Through this approach, we can adopt an optimization-based sampling process at inference time, where samples are obtained by gradient descent on the learned landscape with adjustable step sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation performance of diffusion/flow models empirically, achieving an FID of 1.90 on ImageNet 256$\times$256. EqM is also theoretically justified to learn and sample from the data manifold. Beyond generation, EqM is a flexible framework that naturally handles tasks including partially noised image denoising, OOD detection, and image composition. By replacing time-conditional velocities with a unified equilibrium landscape, EqM offers a tighter bridge between flow and energy-based models and a simple route to optimization-driven inference.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [37] [Aligning Video Models with Human Social Judgments via Behavior-Guided Fine-Tuning](https://arxiv.org/abs/2510.01502)
*Kathy Garcia,Leyla Isik*

Main category: q-bio.NC

TL;DR: 研究发现预训练视频模型在社交视频相似性判断上与人类感知存在差距，语言模型反而表现更好。通过人类行为数据微调视频模型，可以显著提升与人类社交感知的对齐度。


<details>
  <summary>Details</summary>
Motivation: 研究人类与AI模型在社交视频相似性感知上的差异，探索如何利用人类行为数据来改善视频模型的社交感知能力。

Method: 构建包含49,000多个相似性判断的基准数据集，使用混合三元组-RSA目标通过LoRA微调TimeSformer视频模型，使其成对距离与人类相似性对齐。

Result: 微调后的视频模型在保留视频上的解释方差和三元组准确率显著提升，增强了社交情感属性（亲密感、情感效价、支配性、沟通）的编码能力。

Conclusion: 预训练视频模型在社交识别方面存在差距，基于人类行为的微调能够使视频表示更接近人类的社交感知。

Abstract: Humans intuitively perceive complex social signals in visual scenes, yet it remains unclear whether state-of-the-art AI models encode the same similarity structure. We study (Q1) whether modern video and language models capture human-perceived similarity in social videos, and (Q2) how to instill this structure into models using human behavioral data. To address this, we introduce a new benchmark of over 49,000 odd-one-out similarity judgments on 250 three-second video clips of social interactions, and discover a modality gap: despite the task being visual, caption-based language embeddings align better with human similarity than any pretrained video model. We close this gap by fine-tuning a TimeSformer video model on these human judgments with our novel hybrid triplet-RSA objective using low-rank adaptation (LoRA), aligning pairwise distances to human similarity. This fine-tuning protocol yields significantly improved alignment with human perceptions on held-out videos in terms of both explained variance and odd-one-out triplet accuracy. Variance partitioning shows that the fine-tuned video model increases shared variance with language embeddings and explains additional unique variance not captured by the language model. Finally, we test transfer via linear probes and find that human-similarity fine-tuning strengthens the encoding of social-affective attributes (intimacy, valence, dominance, communication) relative to the pretrained baseline. Overall, our findings highlight a gap in pretrained video models' social recognition and demonstrate that behavior-guided fine-tuning shapes video representations toward human social perception.

</details>


### [38] [Uncovering Semantic Selectivity of Latent Groups in Higher Visual Cortex with Mutual Information-Guided Diffusion](https://arxiv.org/abs/2510.02182)
*Yule Wang,Joseph Yu,Chengrui Li,Weihan Li,Anqi Wu*

Main category: q-bio.NC

TL;DR: MIG-Vis方法利用扩散模型可视化神经潜在子空间中的视觉语义特征，揭示了高级视觉皮层中结构化语义表征的组织方式。


<details>
  <summary>Details</summary>
Motivation: 理解高级视觉区域神经群体如何编码物体中心视觉信息是计算神经科学的核心挑战。现有方法间接且无法揭示神经群体本身的结构组织。

Method: 使用变分自编码器推断神经群体的解纠缠潜在子空间，然后提出互信息引导的扩散合成过程来可视化每个潜在组编码的特定视觉语义特征。

Result: 在两只猕猴的下颞叶皮层多会话神经发放数据集上验证，发现神经潜在组对多种视觉特征具有明确的语义选择性，包括物体姿态、类别间转换和类内内容。

Conclusion: 该方法为高级视觉皮层中结构化语义表征提供了直接、可解释的证据，推进了对编码原理的理解。

Abstract: Understanding how neural populations in higher visual areas encode object-centered visual information remains a central challenge in computational neuroscience. Prior works have investigated representational alignment between artificial neural networks and the visual cortex. Nevertheless, these findings are indirect and offer limited insights to the structure of neural populations themselves. Similarly, decoding-based methods have quantified semantic features from neural populations but have not uncovered their underlying organizations. This leaves open a scientific question: "how feature-specific visual information is distributed across neural populations in higher visual areas, and whether it is organized into structured, semantically meaningful subspaces." To tackle this problem, we present MIG-Vis, a method that leverages the generative power of diffusion models to visualize and validate the visual-semantic attributes encoded in neural latent subspaces. Our method first uses a variational autoencoder to infer a group-wise disentangled neural latent subspace from neural populations. Subsequently, we propose a mutual information (MI)-guided diffusion synthesis procedure to visualize the specific visual-semantic features encoded by each latent group. We validate MIG-Vis on multi-session neural spiking datasets from the inferior temporal (IT) cortex of two macaques. The synthesized results demonstrate that our method identifies neural latent groups with clear semantic selectivity to diverse visual features, including object pose, inter-category transformations, and intra-class content. These findings provide direct, interpretable evidence of structured semantic representation in the higher visual cortex and advance our understanding of its encoding principles.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [39] [Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation](https://arxiv.org/abs/2510.01284)
*Chetwin Low,Weimin Wang,Calder Katyal*

Main category: cs.MM

TL;DR: Ovi是一个统一的音视频生成范式，通过双模态DiT模块的块间跨模态融合，将音频和视频建模为单一生成过程，实现自然同步和高质量的音视频合成。


<details>
  <summary>Details</summary>
Motivation: 现有的音视频生成方法通常依赖复杂的多阶段架构或声音与视觉的顺序合成，需要独立的流程或事后对齐。Ovi旨在通过统一建模消除这些限制。

Method: 使用块间跨模态融合的双DiT模块，初始化音频塔与预训练视频模型相同的架构，通过缩放RoPE嵌入和双向交叉注意力联合训练音视频塔。

Result: 模型能够生成具有自然语音和准确、上下文匹配音效的电影级视频片段，音频塔学会了生成逼真的音效以及传达丰富说话人身份和情感的语音。

Conclusion: Ovi提供了一种统一的音视频生成方法，实现了自然同步和高质量的多模态内容创作，所有演示、代码和模型权重均已公开发布。

Abstract: Audio-video generation has often relied on complex multi-stage architectures or sequential synthesis of sound and visuals. We introduce Ovi, a unified paradigm for audio-video generation that models the two modalities as a single generative process. By using blockwise cross-modal fusion of twin-DiT modules, Ovi achieves natural synchronization and removes the need for separate pipelines or post hoc alignment. To facilitate fine-grained multimodal fusion modeling, we initialize an audio tower with an architecture identical to that of a strong pretrained video model. Trained from scratch on hundreds of thousands of hours of raw audio, the audio tower learns to generate realistic sound effects, as well as speech that conveys rich speaker identity and emotion. Fusion is obtained by jointly training the identical video and audio towers via blockwise exchange of timing (via scaled-RoPE embeddings) and semantics (through bidirectional cross-attention) on a vast video corpus. Our model enables cinematic storytelling with natural speech and accurate, context-matched sound effects, producing movie-grade video clips. All the demos, code and model weights are published at https://aaxwaz.github.io/Ovi

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [40] [ZK-WAGON: Imperceptible Watermark for Image Generation Models using ZK-SNARKs](https://arxiv.org/abs/2510.01967)
*Aadarsh Anantha Ramakrishnan,Shubham Agarwal,Selvanayagam S,Kunwar Singh*

Main category: cs.CR

TL;DR: ZK-WAGON是一个基于ZK-SNARKs的图像生成模型水印系统，能够在不暴露模型权重或敏感信息的情况下提供可验证的来源证明。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成模型变得越来越强大和易用，合成媒体的真实性、所有权和滥用问题变得至关重要。传统水印方法要么降低图像质量，要么容易被移除，或者需要访问机密模型内部信息，不适合安全可扩展部署。

Method: 提出选择性层ZK电路创建(SL-ZKCC)方法，将图像生成模型的关键层选择性转换为电路，显著减少证明生成时间。生成的ZK-SNARK证明通过最低有效位(LSB)隐写术不可感知地嵌入到生成的图像中。

Result: 该系统在GAN和Diffusion模型上进行了演示，提供了一个安全、模型无关的可信AI图像生成流水线。

Conclusion: ZK-WAGON为图像生成模型提供了一种安全、可验证的水印解决方案，解决了合成媒体的真实性和所有权问题，同时保护模型敏感信息不被泄露。

Abstract: As image generation models grow increasingly powerful and accessible, concerns around authenticity, ownership, and misuse of synthetic media have become critical. The ability to generate lifelike images indistinguishable from real ones introduces risks such as misinformation, deepfakes, and intellectual property violations. Traditional watermarking methods either degrade image quality, are easily removed, or require access to confidential model internals - making them unsuitable for secure and scalable deployment. We are the first to introduce ZK-WAGON, a novel system for watermarking image generation models using the Zero-Knowledge Succinct Non Interactive Argument of Knowledge (ZK-SNARKs). Our approach enables verifiable proof of origin without exposing model weights, generation prompts, or any sensitive internal information. We propose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively convert key layers of an image generation model into a circuit, reducing proof generation time significantly. Generated ZK-SNARK proofs are imperceptibly embedded into a generated image via Least Significant Bit (LSB) steganography. We demonstrate this system on both GAN and Diffusion models, providing a secure, model-agnostic pipeline for trustworthy AI image generation.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [41] [An Efficient Quality Metric for Video Frame Interpolation Based on Motion-Field Divergence](https://arxiv.org/abs/2510.01361)
*Conall Daly,Darren Ramsook,Anil Kokaram*

Main category: eess.IV

TL;DR: 提出了PSNR_DIV，一种基于运动发散加权的视频帧插值质量评估指标，在保持高效的同时显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有视频帧插值质量评估指标如PSNR、SSIM、LPIPS忽略时间一致性，而专门设计的FloLPIPS计算效率低下，限制了实际应用

Method: 通过运动发散加权增强PSNR，该方法源自档案胶片修复技术，用于检测时间不一致性。突出运动场中的奇点来加权图像误差

Result: 在BVI-VFI数据集上评估显示，PSNR_DIV相比FloLPIPS提升0.09皮尔逊线性相关系数，同时速度快2.5倍，内存使用减少4倍

Conclusion: PSNR_DIV的高效性和准确性使其能够快速评估质量，并可作为训练视频帧插值神经网络的损失函数

Abstract: Video frame interpolation is a fundamental tool for temporal video enhancement, but existing quality metrics struggle to evaluate the perceptual impact of interpolation artefacts effectively. Metrics like PSNR, SSIM and LPIPS ignore temporal coherence. State-of-the-art quality metrics tailored towards video frame interpolation, like FloLPIPS, have been developed but suffer from computational inefficiency that limits their practical application. We present $\text{PSNR}_{\text{DIV}}$, a novel full-reference quality metric that enhances PSNR through motion divergence weighting, a technique adapted from archival film restoration where it was developed to detect temporal inconsistencies. Our approach highlights singularities in motion fields which is then used to weight image errors. Evaluation on the BVI-VFI dataset (180 sequences across multiple frame rates, resolutions and interpolation methods) shows $\text{PSNR}_{\text{DIV}}$ achieves statistically significant improvements: +0.09 Pearson Linear Correlation Coefficient over FloLPIPS, while being 2.5$\times$ faster and using 4$\times$ less memory. Performance remains consistent across all content categories and are robust to the motion estimator used. The efficiency and accuracy of $\text{PSNR}_{\text{DIV}}$ enables fast quality evaluation and practical use as a loss function for training neural networks for video frame interpolation tasks. An implementation of our metric is available at www.github.com/conalld/psnr-div.

</details>


### [42] [Median2Median: Zero-shot Suppression of Structured Noise in Images](https://arxiv.org/abs/2510.01666)
*Jianxu Wang,Ge Wang*

Main category: eess.IV

TL;DR: 提出Median2Median（M2M）零样本去噪框架，专门针对结构化噪声设计，通过新颖的采样策略生成伪独立子图像对，在结构化噪声下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界图像常被具有强各向异性相关的结构化噪声降质，现有方法难以去除。数据驱动方法需要大量高质量标签且泛化性有限，而零样本方法仅对独立同分布噪声有效。

Method: M2M引入方向性插值和广义中值滤波来生成伪独立子图像对，采用随机分配策略扩大采样空间并消除系统偏差，适用于Noise2Noise训练。

Result: 在真实模拟研究中，M2M在独立同分布噪声下与最先进零样本方法表现相当，在相关噪声下始终优于它们。

Conclusion: M2M是结构化噪声抑制的高效、无数据解决方案，标志着超越严格独立同分布假设的有效零样本去噪的第一步。

Abstract: Image denoising is a fundamental problem in computer vision and medical imaging. However, real-world images are often degraded by structured noise with strong anisotropic correlations that existing methods struggle to remove. Most data-driven approaches rely on large datasets with high-quality labels and still suffer from limited generalizability, whereas existing zero-shot methods avoid this limitation but remain effective only for independent and identically distributed (i.i.d.) noise. To address this gap, we propose Median2Median (M2M), a zero-shot denoising framework designed for structured noise. M2M introduces a novel sampling strategy that generates pseudo-independent sub-image pairs from a single noisy input. This strategy leverages directional interpolation and generalized median filtering to adaptively exclude values distorted by structured artifacts. To further enlarge the effective sampling space and eliminate systematic bias, a randomized assignment strategy is employed, ensuring that the sampled sub-image pairs are suitable for Noise2Noise training. In our realistic simulation studies, M2M performs on par with state-of-the-art zero-shot methods under i.i.d. noise, while consistently outperforming them under correlated noise. These findings establish M2M as an efficient, data-free solution for structured noise suppression and mark the first step toward effective zero-shot denoising beyond the strict i.i.d. assumption.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [43] [Towards Photonic Band Diagram Generation with Transformer-Latent Diffusion Models](https://arxiv.org/abs/2510.01749)
*Valentin Delchevalerie,Nicolas Roy,Arnaud Bougaham,Alexandre Mayer,Benoît Frénay,Michaël Lobet*

Main category: physics.optics

TL;DR: 提出了首个基于扩散模型的光子能带图生成方法，结合Transformer编码器和潜在扩散模型，能够处理复杂光子结构并生成对应的能带图。


<details>
  <summary>Details</summary>
Motivation: 计算光子能带图需要求解大量麦克斯韦方程，计算成本高昂，特别是在逆向设计优化循环中。需要开发高效的替代模型来加速这一过程。

Method: 使用Transformer编码器提取输入结构的上下文嵌入，然后通过潜在扩散模型生成对应的光子能带图。

Result: 成功开发了能够生成光子能带图的扩散模型框架，为光子学领域提供了新的替代建模策略。

Conclusion: Transformer和扩散模型能够有效捕捉光子学中复杂的干涉和散射现象，为光子晶体设计开辟了新的计算高效途径。

Abstract: Photonic crystals enable fine control over light propagation at the nanoscale, and thus play a central role in the development of photonic and quantum technologies. Photonic band diagrams (BDs) are a key tool to investigate light propagation into such inhomogeneous structured materials. However, computing BDs requires solving Maxwell's equations across many configurations, making it numerically expensive, especially when embedded in optimization loops for inverse design techniques, for example. To address this challenge, we introduce the first approach for BD generation based on diffusion models, with the capacity to later generalize and scale to arbitrary three dimensional structures. Our method couples a transformer encoder, which extracts contextual embeddings from the input structure, with a latent diffusion model to generate the corresponding BD. In addition, we provide insights into why transformers and diffusion models are well suited to capture the complex interference and scattering phenomena inherent to photonics, paving the way for new surrogate modeling strategies in this domain.

</details>
