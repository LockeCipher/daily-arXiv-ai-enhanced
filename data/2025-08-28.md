<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 22]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Seam360GS: Seamless 360° Gaussian Splatting from Real-World Omnidirectional Images](https://arxiv.org/abs/2508.20080)
*Changha Shin,Woong Oh Cho,Seon Joo Kim*

Main category: cs.CV

TL;DR: 提出了一种将双鱼眼相机模型集成到3D高斯溅射管道中的新颖校准框架，能够从有缺陷的全向输入生成无缝的360度新视角合成


<details>
  <summary>Details</summary>
Motivation: 消费级双鱼眼系统由于固有的镜头分离和角度失真，总是产生不完美的全景图像，需要一种方法来处理这些视觉伪影并实现无缝渲染

Method: 将双鱼眼相机模型整合到3D高斯溅射流程中，联合优化3D高斯参数和模拟镜头间隙、角度失真的校准变量

Result: 在真实世界数据集上的广泛评估证实，该方法即使从不完美图像也能产生无缝渲染，并且优于现有的360度渲染模型

Conclusion: 该框架成功地将不完美的全向输入转换为完美的新视角合成，为双鱼眼相机系统提供了一种有效的校准和渲染解决方案

Abstract: 360-degree visual content is widely shared on platforms such as YouTube and plays a central role in virtual reality, robotics, and autonomous navigation. However, consumer-grade dual-fisheye systems consistently yield imperfect panoramas due to inherent lens separation and angular distortions. In this work, we introduce a novel calibration framework that incorporates a dual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approach not only simulates the realistic visual artifacts produced by dual-fisheye cameras but also enables the synthesis of seamlessly rendered 360-degree images. By jointly optimizing 3D Gaussian parameters alongside calibration variables that emulate lens gaps and angular distortions, our framework transforms imperfect omnidirectional inputs into flawless novel view synthesis. Extensive evaluations on real-world datasets confirm that our method produces seamless renderings-even from imperfect images-and outperforms existing 360-degree rendering models.

</details>


### [2] [Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration](https://arxiv.org/abs/2508.19254)
*Jookyung Song,Mookyoung Kang,Nojun Kwak*

Main category: cs.CV

TL;DR: 一个实时生成式绘画系统，同时解释结构意图和语义意图，通过多阶段生成流水线实现低延迟的协作绘画。


<details>
  <summary>Details</summary>
Motivation: 传统的文本提示生成系统主要抓取高级别语义描述，缺乏对基础级几何特征的分析。本文方法尝试同时解释结构性和语义性意图，以支持更自然的人工智能协作创作。

Method: 采用多阶段生成流水线，同时条件化地面级直观几何特征（线条轨迹、比例、空间布局）和通过视觉-语言模型提取的高级语义线索。结合边缘保持的结构控制与风格-内容感知的图像合成。

Result: 实现了基于触摸屏界面和分布式推理架构的低延迟两阶段转换，支持多用户在共享画布上协作。无论艺术专业知识如何，参与者都能进行同步的共同创作。

Conclusion: 该系统重新定义人工智能与人类的交互为一种共创和相互增强的过程，为可接受性强的生成式创作提供了新的可能性。

Abstract: This paper presents a real-time generative drawing system that interprets and integrates both formal intent - the structural, compositional, and stylistic attributes of a sketch - and contextual intent - the semantic and thematic meaning inferred from its visual content - into a unified transformation process. Unlike conventional text-prompt-based generative systems, which primarily capture high-level contextual descriptions, our approach simultaneously analyzes ground-level intuitive geometric features such as line trajectories, proportions, and spatial arrangement, and high-level semantic cues extracted via vision-language models. These dual intent signals are jointly conditioned in a multi-stage generation pipeline that combines contour-preserving structural control with style- and content-aware image synthesis. Implemented with a touchscreen-based interface and distributed inference architecture, the system achieves low-latency, two-stage transformation while supporting multi-user collaboration on shared canvases. The resulting platform enables participants, regardless of artistic expertise, to engage in synchronous, co-authored visual creation, redefining human-AI interaction as a process of co-creation and mutual enhancement.

</details>


### [3] [MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation](https://arxiv.org/abs/2508.19320)
*Ming Chen,Liyuan Cui,Wenyuan Zhang,Haoxian Zhang,Yan Zhou,Xiaohan Li,Xiaoqiang Liu,Pengfei Wan*

Main category: cs.CV

TL;DR: 提出了一种基于自回归模型和扩散头的交互式数字人视频生成框架，支持多模态输入控制和低延迟流式生成，通过深度压缩自编码器实现64倍压缩比来降低计算负担。


<details>
  <summary>Details</summary>
Motivation: 现有交互式数字人视频生成方法存在高延迟、高计算成本和有限可控性等问题，需要构建一个能够实时响应多样化输入信号的实用系统。

Method: 基于大型语言模型(LLM)构建自回归视频生成框架，接受音频、姿态和文本等多模态条件编码，输出空间和语义一致的表征来指导扩散头的去噪过程。使用深度压缩自编码器实现64倍压缩比。

Result: 在双工对话、多语言人像合成和交互式世界模型等任务上进行了广泛实验，证明了该方法在低延迟、高效率和细粒度多模态可控性方面的优势。

Conclusion: 该框架通过最小化对标准LLM的修改，实现了交互式多模态控制和低延迟外推，构建的大规模对话数据集(约20,000小时)为训练提供了丰富的对话场景。

Abstract: Recently, interactive digital human video generation has attracted widespread attention and achieved remarkable progress. However, building such a practical system that can interact with diverse input signals in real time remains challenging to existing methods, which often struggle with high latency, heavy computational cost, and limited controllability. In this work, we introduce an autoregressive video generation framework that enables interactive multimodal control and low-latency extrapolation in a streaming manner. With minimal modifications to a standard large language model (LLM), our framework accepts multimodal condition encodings including audio, pose, and text, and outputs spatially and semantically coherent representations to guide the denoising process of a diffusion head. To support this, we construct a large-scale dialogue dataset of approximately 20,000 hours from multiple sources, providing rich conversational scenarios for training. We further introduce a deep compression autoencoder with up to 64$\times$ reduction ratio, which effectively alleviates the long-horizon inference burden of the autoregressive model. Extensive experiments on duplex conversation, multilingual human synthesis, and interactive world model highlight the advantages of our approach in low latency, high efficiency, and fine-grained multimodal controllability.

</details>


### [4] [EffNetViTLoRA: An Efficient Hybrid Deep Learning Approach for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.19349)
*Mahdieh Behjat Khatooni,Mohsen Soryani*

Main category: cs.CV

TL;DR: EffNetViTLoRA模型结合CNN和ViT，使用完整ADNI MRI数据集进行阿尔茨海默病诊断，通过LoRA微调实现92.52%准确率


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病早期诊断至关重要，MCI阶段诊断困难，现有研究多使用有限数据子集，需要更鲁棒和全面的诊断方法

Method: 集成CNN和Vision Transformer捕捉MRI图像的局部和全局特征，使用完整ADNI T1加权MRI数据集，采用LoRA技术进行高效领域适应

Result: 在AD、MCI和CN三个诊断类别上达到92.52%的分类准确率和92.76%的F1分数

Conclusion: 提出的EffNetViTLoRA模型在完整数据集上表现出色，LoRA技术有效解决了预训练模型领域差异问题，具有临床可靠性

Abstract: Alzheimer's disease (AD) is one of the most prevalent neurodegenerative disorders worldwide. As it progresses, it leads to the deterioration of cognitive functions. Since AD is irreversible, early diagnosis is crucial for managing its progression. Mild Cognitive Impairment (MCI) represents an intermediate stage between Cognitively Normal (CN) individuals and those with AD, and is considered a transitional phase from normal cognition to Alzheimer's disease. Diagnosing MCI is particularly challenging due to the subtle differences between adjacent diagnostic categories. In this study, we propose EffNetViTLoRA, a generalized end-to-end model for AD diagnosis using the whole Alzheimer's Disease Neuroimaging Initiative (ADNI) Magnetic Resonance Imaging (MRI) dataset. Our model integrates a Convolutional Neural Network (CNN) with a Vision Transformer (ViT) to capture both local and global features from MRI images. Unlike previous studies that rely on limited subsets of data, our approach is trained on the full T1-weighted MRI dataset from ADNI, resulting in a more robust and unbiased model. This comprehensive methodology enhances the model's clinical reliability. Furthermore, fine-tuning large pretrained models often yields suboptimal results when source and target dataset domains differ. To address this, we incorporate Low-Rank Adaptation (LoRA) to effectively adapt the pretrained ViT model to our target domain. This method enables efficient knowledge transfer and reduces the risk of overfitting. Our model achieves a classification accuracy of 92.52% and an F1-score of 92.76% across three diagnostic categories: AD, MCI, and CN for full ADNI dataset.

</details>


### [5] [MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment](https://arxiv.org/abs/2508.19527)
*Zhiting Gao,Dan Song,Diqiong Jiang,Chao Xue,An-An Liu*

Main category: cs.CV

TL;DR: TAPO和MotionFLUX框架解决了文本驱动动作生成中的语义对齐和推理效率问题，实现了实时高质量动作合成


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动动作生成方法在语言描述与动作语义的精确对齐方面存在困难，且多步推理效率低下

Method: TAPO框架通过偏好优化对齐细微动作变化与文本修饰符，MotionFLUX基于确定性整流流匹配构建噪声分布与动作空间的最优传输路径

Result: 实验结果表明该系统在语义一致性和动作质量方面优于最先进方法，同时显著加速生成速度

Conclusion: TAPO和MotionFLUX形成了一个统一系统，解决了动作生成中的语义对齐和实时合成挑战

Abstract: Motion generation is essential for animating virtual characters and embodied agents. While recent text-driven methods have made significant strides, they often struggle with achieving precise alignment between linguistic descriptions and motion semantics, as well as with the inefficiencies of slow, multi-step inference. To address these issues, we introduce TMR++ Aligned Preference Optimization (TAPO), an innovative framework that aligns subtle motion variations with textual modifiers and incorporates iterative adjustments to reinforce semantic grounding. To further enable real-time synthesis, we propose MotionFLUX, a high-speed generation framework based on deterministic rectified flow matching. Unlike traditional diffusion models, which require hundreds of denoising steps, MotionFLUX constructs optimal transport paths between noise distributions and motion spaces, facilitating real-time synthesis. The linearized probability paths reduce the need for multi-step sampling typical of sequential methods, significantly accelerating inference time without sacrificing motion quality. Experimental results demonstrate that, together, TAPO and MotionFLUX form a unified system that outperforms state-of-the-art approaches in both semantic consistency and motion quality, while also accelerating generation speed. The code and pretrained models will be released.

</details>


### [6] [Interact-Custom: Customized Human Object Interaction Image Generation](https://arxiv.org/abs/2508.19575)
*Zhu Xu,Zhaowen Wang,Yuxin Peng,Yang Liu*

Main category: cs.CV

TL;DR: 该论文提出了定制化人机交互图像生成任务(CHOI)，通过Interact-Custom模型解决同时保持目标身份特征和控制交互语义的挑战，包括大规模数据集构建和两阶段生成方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注目标实体的外观保持，而忽略了目标实体之间的细粒度交互控制。为了解决这个问题，研究者专注于人机交互场景，提出了CHOI任务，要求同时保持目标人机身份并控制它们之间的交互语义。

Method: 首先处理大规模数据集，每个样本包含相同的人机对涉及不同的交互姿势。然后设计两阶段模型Interact-Custom：第一阶段通过生成描述交互行为的前景掩模来显式建模空间配置；第二阶段在该掩模指导下生成保持身份特征的目标人机交互图像。还提供可选功能允许用户指定背景图像和联合位置。

Result: 在针对CHOI任务定制的指标上进行的广泛实验证明了该方法的有效性。模型能够成功保持目标身份特征的同时实现精确的交互语义控制。

Conclusion: 提出的Interact-Custom模型有效解决了定制化人机交互图像生成中的身份保持和交互控制双重挑战，通过空间配置建模和两阶段生成策略实现了高质量的交互图像生成。

Abstract: Compositional Customized Image Generation aims to customize multiple target concepts within generation content, which has gained attention for its wild application.Existing approaches mainly concentrate on the target entity's appearance preservation, while neglecting the fine-grained interaction control among target entities.To enable the model of such interaction control capability, we focus on human object interaction scenario and propose the task of Customized Human Object Interaction Image Generation(CHOI), which simultaneously requires identity preservation for target human object and the interaction semantic control between them.Two primary challenges exist for CHOI:(1)simultaneous identity preservation and interaction control demands require the model to decompose the human object into self-contained identity features and pose-oriented interaction features, while the current HOI image datasets fail to provide ideal samples for such feature-decomposed learning.(2)inappropriate spatial configuration between human and object may lead to the lack of desired interaction semantics.To tackle it, we first process a large-scale dataset, where each sample encompasses the same pair of human object involving different interactive poses.Then we design a two-stage model Interact-Custom, which firstly explicitly models the spatial configuration by generating a foreground mask depicting the interaction behavior, then under the guidance of this mask, we generate the target human object interacting while preserving their identities features.Furthermore, if the background image and the union location of where the target human object should appear are provided by users, Interact-Custom also provides the optional functionality to specify them, offering high content controllability. Extensive experiments on our tailored metrics for CHOI task demonstrate the effectiveness of our approach.

</details>


### [7] [Controllable Skin Synthesis via Lesion-Focused Vector Autoregression Model](https://arxiv.org/abs/2508.19626)
*Jiajun Sun,Zhen Yu,Siyuan Yan,Jason J. Ong,Zongyuan Ge,Lei Zhang*

Main category: cs.CV

TL;DR: LF-VAR是一个可控皮肤图像合成模型，利用量化病变测量分数和病变类型标签，通过语言提示生成具有特定病变特征的高质量皮肤图像，在FID评分上比现有最佳方法提升6.3%。


<details>
  <summary>Details</summary>
Motivation: 真实临床实践中的皮肤图像数据有限，现有合成方法生成的图像质量低且无法控制病变位置和类型，需要开发能够生成临床相关且可控的高质量皮肤图像的解决方案。

Method: 采用多尺度病变聚焦的VQVAE将图像编码为离散潜在表示，然后训练视觉自回归变换器进行图像合成，整合病变测量和类型作为条件嵌入来增强合成保真度。

Result: 在七种病变类型上取得了最佳FID评分（平均0.74），比之前的最先进方法提升了6.3%，能够生成高保真度、临床相关的合成皮肤图像。

Conclusion: LF-VAR模型在可控皮肤图像合成方面表现出色，为皮肤病学研究和临床实践提供了有效的合成数据生成解决方案。

Abstract: Skin images from real-world clinical practice are often limited, resulting in a shortage of training data for deep-learning models. While many studies have explored skin image synthesis, existing methods often generate low-quality images and lack control over the lesion's location and type. To address these limitations, we present LF-VAR, a model leveraging quantified lesion measurement scores and lesion type labels to guide the clinically relevant and controllable synthesis of skin images. It enables controlled skin synthesis with specific lesion characteristics based on language prompts. We train a multiscale lesion-focused Vector Quantised Variational Auto-Encoder (VQVAE) to encode images into discrete latent representations for structured tokenization. Then, a Visual AutoRegressive (VAR) Transformer trained on tokenized representations facilitates image synthesis. Lesion measurement from the lesion region and types as conditional embeddings are integrated to enhance synthesis fidelity. Our method achieves the best overall FID score (average 0.74) among seven lesion types, improving upon the previous state-of-the-art (SOTA) by 6.3%. The study highlights our controllable skin synthesis model's effectiveness in generating high-fidelity, clinically relevant synthetic skin images. Our framework code is available at https://github.com/echosun1996/LF-VAR.

</details>


### [8] [UTAL-GNN: Unsupervised Temporal Action Localization using Graph Neural Networks](https://arxiv.org/abs/2508.19647)
*Bikash Kumar Badatya,Vipul Baghel,Ravi Hegde*

Main category: cs.CV

TL;DR: 提出了一种轻量级无监督的基于骨架的动作定位方法，使用时空图神经网络和动作动态度量来检测运动边界，无需人工标注即可达到有监督方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决未修剪体育视频中细粒度动作定位的挑战，现有方法依赖大量标注数据和复杂模型，计算成本高且难以适应真实场景。

Method: 使用注意力时空图卷积网络（ASTGCN）在姿态序列去噪任务上进行预训练，学习内在运动动态；定义动作动态度量（ADM）从低维嵌入中检测运动边界点。

Result: 在DSV Diving数据集上达到82.66%的平均精度和29.09毫秒的平均定位延迟，性能与最先进的有监督方法相当，且在未见过的真实跳水视频上表现鲁棒。

Conclusion: 该方法为嵌入式或动态环境中的轻量级实时动作分析系统提供了实用的解决方案，具有优秀的泛化能力和计算效率。

Abstract: Fine-grained action localization in untrimmed sports videos presents a significant challenge due to rapid and subtle motion transitions over short durations. Existing supervised and weakly supervised solutions often rely on extensive annotated datasets and high-capacity models, making them computationally intensive and less adaptable to real-world scenarios. In this work, we introduce a lightweight and unsupervised skeleton-based action localization pipeline that leverages spatio-temporal graph neural representations. Our approach pre-trains an Attention-based Spatio-Temporal Graph Convolutional Network (ASTGCN) on a pose-sequence denoising task with blockwise partitions, enabling it to learn intrinsic motion dynamics without any manual labeling. At inference, we define a novel Action Dynamics Metric (ADM), computed directly from low-dimensional ASTGCN embeddings, which detects motion boundaries by identifying inflection points in its curvature profile. Our method achieves a mean Average Precision (mAP) of 82.66% and average localization latency of 29.09 ms on the DSV Diving dataset, matching state-of-the-art supervised performance while maintaining computational efficiency. Furthermore, it generalizes robustly to unseen, in-the-wild diving footage without retraining, demonstrating its practical applicability for lightweight, real-time action analysis systems in embedded or dynamic environments.

</details>


### [9] [IDF: Iterative Dynamic Filtering Networks for Generalizable Image Denoising](https://arxiv.org/abs/2508.19649)
*Dongjin Kim,Jaekyun Ko,Muhammad Kashif Ali,Tae Hyun Kim*

Main category: cs.CV

TL;DR: 提出一种基于动态生成核的迭代图像去噪方法，通过特征提取和核预测模块生成像素级变化的自适应核，在保持高效性的同时提升对未知噪声类型的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖特定噪声分布，泛化能力有限且容易过拟合，需要大量训练数据和计算资源

Method: 使用特征提取模块获取噪声不变特征，全局统计和局部相关模块捕获噪声特性，核预测模块生成像素级变化的自适应核进行迭代去噪

Result: 仅使用单级高斯噪声训练的紧凑模型（约0.04M参数）在多种噪声类型和级别上表现优异

Conclusion: 迭代动态滤波方法为实际图像去噪应用提供了有前景的解决方案，具有良好的泛化能力和效率

Abstract: Image denoising is a fundamental challenge in computer vision, with applications in photography and medical imaging. While deep learning-based methods have shown remarkable success, their reliance on specific noise distributions limits generalization to unseen noise types and levels. Existing approaches attempt to address this with extensive training data and high computational resources but they still suffer from overfitting. To address these issues, we conduct image denoising by utilizing dynamically generated kernels via efficient operations. This approach helps prevent overfitting and improves resilience to unseen noise. Specifically, our method leverages a Feature Extraction Module for robust noise-invariant features, Global Statistics and Local Correlation Modules to capture comprehensive noise characteristics and structural correlations. The Kernel Prediction Module then employs these cues to produce pixel-wise varying kernels adapted to local structures, which are then applied iteratively for denoising. This ensures both efficiency and superior restoration quality. Despite being trained on single-level Gaussian noise, our compact model (~ 0.04 M) excels across diverse noise types and levels, demonstrating the promise of iterative dynamic filtering for practical image denoising.

</details>


### [10] [A Frequency-Aware Self-Supervised Learning for Ultra-Wide-Field Image Enhancement](https://arxiv.org/abs/2508.19664)
*Weicheng Liao,Zan Chen,Jianyang Xie,Yalin Zheng,Yuhui Ma,Yitian Zhao*

Main category: cs.CV

TL;DR: 提出了一种新颖的频率感知自监督学习方法，用于超宽视野视网膜图像增强，包含频率解耦去模糊和Retinex引导的照明补偿模块，有效提升图像质量和疾病诊断性能。


<details>
  <summary>Details</summary>
Motivation: 超宽视野视网膜成像虽然提供了全面的视网膜视图，但经常受到模糊和光照不均等质量退化因素的影响，这些因素会掩盖精细细节和病理信息。现有方法无法满足UWF图像的特殊需求，特别是保留病理细节的要求。

Method: 采用频率感知自监督学习方法，包含两个核心模块：1）频率解耦图像去模糊模块，通过非对称通道整合操作结合全局和局部视图；2）Retinex引导的照明补偿模块，包含颜色保留单元，提供多尺度空间和频率信息。

Result: 实验结果表明，该方法不仅提高了可视化质量，还通过恢复和校正精细局部细节和不均匀强度来改善疾病诊断性能。

Conclusion: 这是首个针对超宽视野图像增强的尝试，为改善视网膜疾病管理提供了一个强大且具有临床价值的工具。

Abstract: Ultra-Wide-Field (UWF) retinal imaging has revolutionized retinal diagnostics by providing a comprehensive view of the retina. However, it often suffers from quality-degrading factors such as blurring and uneven illumination, which obscure fine details and mask pathological information. While numerous retinal image enhancement methods have been proposed for other fundus imageries, they often fail to address the unique requirements in UWF, particularly the need to preserve pathological details. In this paper, we propose a novel frequency-aware self-supervised learning method for UWF image enhancement. It incorporates frequency-decoupled image deblurring and Retinex-guided illumination compensation modules. An asymmetric channel integration operation is introduced in the former module, so as to combine global and local views by leveraging high- and low-frequency information, ensuring the preservation of fine and broader structural details. In addition, a color preservation unit is proposed in the latter Retinex-based module, to provide multi-scale spatial and frequency information, enabling accurate illumination estimation and correction. Experimental results demonstrate that the proposed work not only enhances visualization quality but also improves disease diagnosis performance by restoring and correcting fine local details and uneven intensity. To the best of our knowledge, this work is the first attempt for UWF image enhancement, offering a robust and clinically valuable tool for improving retinal disease management.

</details>


### [11] [LabelGS: Label-Aware 3D Gaussian Splatting for 3D Scene Segmentation](https://arxiv.org/abs/2508.19699)
*Yupeng Zhang,Dezhi Zheng,Ping Lu,Han Zhang,Lei Wang,Liping xiang,Cheng Luo,Kaijun Deng,Xiaowen Fu,Linlin Shen,Jinbao Wang*

Main category: cs.CV

TL;DR: LabelGS是一种增强3D高斯泼溅表示的方法，通过引入对象标签实现3D场景分割，在保持高效渲染的同时显著提升训练速度和分割性能


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅(3DGS)虽然能够实现高保真重建和高效渲染，但缺乏3D分割能力，限制了其在需要场景理解任务中的应用。为了解决这一局限性，需要为高斯表示添加对象标签能力

Method: 提出LabelGS方法，引入跨视图一致的3D高斯语义掩码，采用遮挡分析模型避免优化过程中的过拟合，使用主高斯标记模型将2D语义先验提升到3D高斯，并通过高斯投影滤波器避免标签冲突

Result: LabelGS在3D场景分割任务中优于包括Feature-3DGS在内的先前最先进方法，在1440×1080分辨率下实现了22倍的训练加速

Conclusion: LabelGS成功解决了3DGS缺乏分割能力的问题，通过引入对象标签和优化策略，实现了高效准确的3D场景分割，为3D场景理解任务提供了有效的解决方案

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a novel explicit representation for 3D scenes, offering both high-fidelity reconstruction and efficient rendering. However, 3DGS lacks 3D segmentation ability, which limits its applicability in tasks that require scene understanding. The identification and isolating of specific object components is crucial. To address this limitation, we propose Label-aware 3D Gaussian Splatting (LabelGS), a method that augments the Gaussian representation with object label.LabelGS introduces cross-view consistent semantic masks for 3D Gaussians and employs a novel Occlusion Analysis Model to avoid overfitting occlusion during optimization, Main Gaussian Labeling model to lift 2D semantic prior to 3D Gaussian and Gaussian Projection Filter to avoid Gaussian label conflict. Our approach achieves effective decoupling of Gaussian representations and refines the 3DGS optimization process through a random region sampling strategy, significantly improving efficiency. Extensive experiments demonstrate that LabelGS outperforms previous state-of-the-art methods, including Feature-3DGS, in the 3D scene segmentation task. Notably, LabelGS achieves a remarkable 22X speedup in training compared to Feature-3DGS, at a resolution of 1440X1080. Our code will be at https://github.com/garrisonz/LabelGS.

</details>


### [12] [FastAvatar: Towards Unified Fast High-Fidelity 3D Avatar Reconstruction with Large Gaussian Reconstruction Transformers](https://arxiv.org/abs/2508.19754)
*Yue Wu,Yufan Wu,Wen Li,Yuxi Lu,Kairui Feng,Xuanhong Chen*

Main category: cs.CV

TL;DR: FastAvatar是一个快速3D头像重建框架，使用单一统一模型在几秒内从单张图像、多视角观测或单目视频中重建高质量的3D高斯溅射模型。


<details>
  <summary>Details</summary>
Motivation: 当前3D头像重建面临时间复杂度过高、对数据质量敏感以及数据利用率低等挑战，需要一种能够灵活利用多样化日常记录数据的高效解决方案。

Method: 采用大型高斯重建Transformer架构，包含三个关键设计：变体VGGT-style transformer聚合多帧线索并注入初始3D提示；多粒度引导编码缓解动画引起的错位；通过地标跟踪和切片融合损失进行增量高斯聚合。

Result: 实验表明FastAvatar在质量和速度方面均优于现有方法，支持增量重建，即随着观测数据增加而提升质量。

Conclusion: FastAvatar提供了一个质量-速度可调的高度可用头像建模范式，实现了高质量快速3D头像重建的突破。

Abstract: Despite significant progress in 3D avatar reconstruction, it still faces challenges such as high time complexity, sensitivity to data quality, and low data utilization. We propose FastAvatar, a feedforward 3D avatar framework capable of flexibly leveraging diverse daily recordings (e.g., a single image, multi-view observations, or monocular video) to reconstruct a high-quality 3D Gaussian Splatting (3DGS) model within seconds, using only a single unified model. FastAvatar's core is a Large Gaussian Reconstruction Transformer featuring three key designs: First, a variant VGGT-style transformer architecture aggregating multi-frame cues while injecting initial 3D prompt to predict an aggregatable canonical 3DGS representation; Second, multi-granular guidance encoding (camera pose, FLAME expression, head pose) mitigating animation-induced misalignment for variable-length inputs; Third, incremental Gaussian aggregation via landmark tracking and sliced fusion losses. Integrating these features, FastAvatar enables incremental reconstruction, i.e., improving quality with more observations, unlike prior work wasting input data. This yields a quality-speed-tunable paradigm for highly usable avatar modeling. Extensive experiments show that FastAvatar has higher quality and highly competitive speed compared to existing methods.

</details>


### [13] [MAPo : Motion-Aware Partitioning of Deformable 3D Gaussian Splatting for High-Fidelity Dynamic Scene Reconstruction](https://arxiv.org/abs/2508.19786)
*Han Jiao,Jiakai Sun,Yexing Xu,Lei Zhao,Wei Xing,Huaizhong Lin*

Main category: cs.CV

TL;DR: MAPo框架通过动态评分分区策略，将3D高斯分为高动态和低动态区域，对高动态区域进行时间分区和网络复制以捕捉精细运动细节，同时使用跨帧一致性损失确保视觉连续性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于变形的动态场景重建方法在处理高动态区域时往往产生模糊渲染并丢失精细运动细节，因为单一统一模型难以表示多样化的运动模式。

Method: 提出动态评分分区策略，区分高动态和低动态3D高斯；对高动态高斯进行时间递归分区并为每个时间段复制变形网络；引入跨帧一致性损失解决分区边界视觉不连续问题。

Result: 实验表明MAPo在保持可比计算成本的同时，相比基线方法实现了更优越的渲染质量，特别是在具有复杂或快速运动的区域。

Conclusion: MAPo框架通过运动感知分区和跨帧一致性约束，有效解决了动态3D高斯泼溅中的运动细节丢失和视觉不连续问题，为高保真动态场景重建提供了有效解决方案。

Abstract: 3D Gaussian Splatting, known for enabling high-quality static scene reconstruction with fast rendering, is increasingly being applied to dynamic scene reconstruction. A common strategy involves learning a deformation field to model the temporal changes of a canonical set of 3D Gaussians. However, these deformation-based methods often produce blurred renderings and lose fine motion details in highly dynamic regions due to the inherent limitations of a single, unified model in representing diverse motion patterns. To address these challenges, we introduce Motion-Aware Partitioning of Deformable 3D Gaussian Splatting (MAPo), a novel framework for high-fidelity dynamic scene reconstruction. Its core is a dynamic score-based partitioning strategy that distinguishes between high- and low-dynamic 3D Gaussians. For high-dynamic 3D Gaussians, we recursively partition them temporally and duplicate their deformation networks for each new temporal segment, enabling specialized modeling to capture intricate motion details. Concurrently, low-dynamic 3DGs are treated as static to reduce computational costs. However, this temporal partitioning strategy for high-dynamic 3DGs can introduce visual discontinuities across frames at the partition boundaries. To address this, we introduce a cross-frame consistency loss, which not only ensures visual continuity but also further enhances rendering quality. Extensive experiments demonstrate that MAPo achieves superior rendering quality compared to baselines while maintaining comparable computational costs, particularly in regions with complex or rapid motions.

</details>


### [14] [StableIntrinsic: Detail-preserving One-step Diffusion Model for Multi-view Material Estimation](https://arxiv.org/abs/2508.19789)
*Xiuchao Wu,Pengfei Zhu,Jiangjing Lyu,Xinguo Liu,Jie Guo,Yanwen Guo,Weiwei Xu,Chengfei Lyu*

Main category: cs.CV

TL;DR: StableIntrinsic是一个单步扩散模型，用于多视角材质估计，能够快速生成高质量、低方差的材质参数，在多项指标上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的材质估计方法采用多步去噪策略，耗时且结果方差大，与确定性材质估计任务存在冲突。

Method: 提出单步扩散模型StableIntrinsic，在像素空间设计基于材质特性的损失函数，并引入细节注入网络(DIN)来消除VAE编码造成的细节损失。

Result: 在PSNR指标上比现有最佳方法提升9.9%，金属和粗糙度的MSE分别降低44.4%和60.0%。

Conclusion: StableIntrinsic通过单步扩散和细节增强机制，实现了快速、高质量的材质估计，解决了多步扩散方法的效率和方差问题。

Abstract: Recovering material information from images has been extensively studied in computer graphics and vision. Recent works in material estimation leverage diffusion model showing promising results. However, these diffusion-based methods adopt a multi-step denoising strategy, which is time-consuming for each estimation. Such stochastic inference also conflicts with the deterministic material estimation task, leading to a high variance estimated results. In this paper, we introduce StableIntrinsic, a one-step diffusion model for multi-view material estimation that can produce high-quality material parameters with low variance. To address the overly-smoothing problem in one-step diffusion, StableIntrinsic applies losses in pixel space, with each loss designed based on the properties of the material. Additionally, StableIntrinsic introduces a Detail Injection Network (DIN) to eliminate the detail loss caused by VAE encoding, while further enhancing the sharpness of material prediction results. The experimental results indicate that our method surpasses the current state-of-the-art techniques by achieving a $9.9\%$ improvement in the Peak Signal-to-Noise Ratio (PSNR) of albedo, and by reducing the Mean Square Error (MSE) for metallic and roughness by $44.4\%$ and $60.0\%$, respectively.

</details>


### [15] [Not Every Gift Comes in Gold Paper or with a Red Ribbon: Exploring Color Perception in Text-to-Image Models](https://arxiv.org/abs/2508.19791)
*Shay Shomer Chai,Wenxuan Peng,Bharath Hariharan,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: 本文研究了文本到图像生成中多对象颜色属性的语义对齐问题，提出了一种专门的图像编辑技术来改善多颜色提示的语义对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成方法在处理复杂多对象提示时难以准确捕捉精确语义，特别是在颜色属性方面存在显著的对齐问题，需要专门的解决方案。

Method: 通过案例研究分析颜色属性对齐问题，开发了一种专门的图像编辑技术来缓解多对象语义对齐问题，特别是在包含多个颜色的提示中。

Result: 研究表明预训练模型在处理多颜色属性时表现不佳，提出的编辑技术在各种基于扩散的文本到图像生成方法中显著提升了性能表现。

Conclusion: 针对多颜色提示的语义对齐问题需要专门的解决方案，提出的图像编辑技术能有效改善多对象语义对齐，为文本到图像生成的质量提升提供了有效途径。

Abstract: Text-to-image generation has recently seen remarkable success, granting users with the ability to create high-quality images through the use of text. However, contemporary methods face challenges in capturing the precise semantics conveyed by complex multi-object prompts. Consequently, many works have sought to mitigate such semantic misalignments, typically via inference-time schemes that modify the attention layers of the denoising networks. However, prior work has mostly utilized coarse metrics, such as the cosine similarity between text and image CLIP embeddings, or human evaluations, which are challenging to conduct on a larger-scale. In this work, we perform a case study on colors -- a fundamental attribute commonly associated with objects in text prompts, which offer a rich test bed for rigorous evaluation. Our analysis reveals that pretrained models struggle to generate images that faithfully reflect multiple color attributes-far more so than with single-color prompts-and that neither inference-time techniques nor existing editing methods reliably resolve these semantic misalignments. Accordingly, we introduce a dedicated image editing technique, mitigating the issue of multi-object semantic alignment for prompts containing multiple colors. We demonstrate that our approach significantly boosts performance over a wide range of metrics, considering images generated by various text-to-image diffusion-based techniques.

</details>


### [16] [FusionSort: Enhanced Cluttered Waste Segmentation with Advanced Decoding and Comprehensive Modality Optimization](https://arxiv.org/abs/2508.19798)
*Muhammad Ali,Omar Ali AlSuwaidi*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In the realm of waste management, automating the sorting process for non-biodegradable materials presents considerable challenges due to the complexity and variability of waste streams. To address these challenges, we introduce an enhanced neural architecture that builds upon an existing Encoder-Decoder structure to improve the accuracy and efficiency of waste sorting systems. Our model integrates several key innovations: a Comprehensive Attention Block within the decoder, which refines feature representations by combining convolutional and upsampling operations. In parallel, we utilize attention through the Mamba architecture, providing an additional performance boost. We also introduce a Data Fusion Block that fuses images with more than three channels. To achieve this, we apply PCA transformation to reduce the dimensionality while retaining the maximum variance and essential information across three dimensions, which are then used for further processing. We evaluated the model on RGB, hyperspectral, multispectral, and a combination of RGB and hyperspectral data. The results demonstrate that our approach outperforms existing methods by a significant margin.

</details>


### [17] [AutoQ-VIS: Improving Unsupervised Video Instance Segmentation via Automatic Quality Assessment](https://arxiv.org/abs/2508.19808)
*Kaixuan Lu,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: AutoQ-VIS是一个无监督视频实例分割框架，通过质量引导的自训练方法，在无需人工标注的情况下实现了52.6 AP50的性能，超越之前的最优方法4.4%。


<details>
  <summary>Details</summary>
Motivation: 视频实例分割(VIS)需要像素级掩码和时间一致性标注，标注成本高昂。现有无监督方法依赖合成数据，但存在合成到真实域的差距问题。

Method: 提出质量引导的自训练框架，建立伪标签生成和自动质量评估的闭环系统，实现从合成视频到真实视频的渐进式适应。

Result: 在YouTubeVIS-2019验证集上达到52.6 AP50，比之前的state-of-the-art VideoCutLER提升4.4%，且无需人工标注。

Conclusion: 证明了质量感知自训练在无监督视频实例分割中的可行性，成功弥合了合成到真实域的差距。

Abstract: Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 $\text{AP}_{50}$ on YouTubeVIS-2019 val set, surpassing the previous state-of-the-art VideoCutLER by 4.4$\%$, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. The source code of our method is available at https://github.com/wcbup/AutoQ-VIS.

</details>


### [18] [Ego-centric Predictive Model Conditioned on Hand Trajectories](https://arxiv.org/abs/2508.19852)
*Binjie Zhang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 提出统一的两阶段预测框架，联合建模自我中心场景中的动作和视觉未来，通过手部轨迹条件化，实现动作预测和未来视频生成的统一处理。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法同时建模动作预测和视觉结果。VLA模型只关注动作预测而缺乏对视觉场景影响的显式建模，视频预测模型则生成不依赖特定动作的未来帧，常导致不合理结果。需要统一框架来同时处理这两个方面。

Method: 两阶段框架：第一阶段通过连续状态建模处理多模态输入并预测未来手部轨迹；第二阶段引入因果交叉注意力融合多模态线索，利用推断的动作信号指导基于图像的潜在扩散模型进行逐帧未来视频生成。

Result: 在Ego4D、BridgeData和RLBench数据集上的广泛实验表明，该方法在动作预测和未来视频合成方面均优于最先进的基线方法。

Conclusion: 这是首个统一处理自我中心人类活动理解和机器人操作任务的模型，能够显式预测即将发生的动作及其视觉后果。

Abstract: In egocentric scenarios, anticipating both the next action and its visual outcome is essential for understanding human-object interactions and for enabling robotic planning. However, existing paradigms fall short of jointly modeling these aspects. Vision-Language-Action (VLA) models focus on action prediction but lack explicit modeling of how actions influence the visual scene, while video prediction models generate future frames without conditioning on specific actions, often resulting in implausible or contextually inconsistent outcomes. To bridge this gap, we propose a unified two-stage predictive framework that jointly models action and visual future in egocentric scenarios, conditioned on hand trajectories. In the first stage, we perform consecutive state modeling to process heterogeneous inputs (visual observations, language, and action history) and explicitly predict future hand trajectories. In the second stage, we introduce causal cross-attention to fuse multi-modal cues, leveraging inferred action signals to guide an image-based Latent Diffusion Model (LDM) for frame-by-frame future video generation. Our approach is the first unified model designed to handle both egocentric human activity understanding and robotic manipulation tasks, providing explicit predictions of both upcoming actions and their visual consequences. Extensive experiments on Ego4D, BridgeData, and RLBench demonstrate that our method outperforms state-of-the-art baselines in both action prediction and future video synthesis.

</details>


### [19] [PersonaAnimator: Personalized Motion Transfer from Unconstrained Videos](https://arxiv.org/abs/2508.19895)
*Ziyun Qian,Runyu Xiao,Shuyuan Tu,Wei Xue,Dingkang Yang,Mingcheng Li,Dongliang Kou,Minghao Han,Zizhi Chen,Lihua Zhang*

Main category: cs.CV

TL;DR: 本文提出了PersonaAnimator框架，通过从无约束视频中学习个性化运动模式，实现了视频到视频的运动个性化，解决了现有方法无法学习运动风格、依赖动捕数据、违反物理规律等问题。


<details>
  <summary>Details</summary>
Motivation: 现有运动生成方法存在三个主要局限：(1)姿态引导的运动迁移方法仅复制运动而不学习风格特征，导致角色表现力不足；(2)运动风格迁移方法严重依赖难以获取的动作捕捉数据；(3)生成的运动有时违反物理规律。

Method: 提出PersonaAnimator框架，直接从无约束视频中学习个性化运动模式；构建首个基于视频的个性化运动数据集PersonaVid（包含20个运动内容类别和120个运动风格类别）；提出物理感知的运动风格正则化机制确保生成运动的物理合理性。

Result: 大量实验表明，PersonaAnimator在运动迁移任务上优于现有最先进方法，并为视频到视频运动个性化任务设立了新的基准。

Conclusion: 该研究开创了视频到视频运动个性化新任务，提出的框架能够有效学习个性化运动模式并确保物理合理性，为运动生成领域提供了新的解决方案。

Abstract: Recent advances in motion generation show remarkable progress. However, several limitations remain: (1) Existing pose-guided character motion transfer methods merely replicate motion without learning its style characteristics, resulting in inexpressive characters. (2) Motion style transfer methods rely heavily on motion capture data, which is difficult to obtain. (3) Generated motions sometimes violate physical laws. To address these challenges, this paper pioneers a new task: Video-to-Video Motion Personalization. We propose a novel framework, PersonaAnimator, which learns personalized motion patterns directly from unconstrained videos. This enables personalized motion transfer. To support this task, we introduce PersonaVid, the first video-based personalized motion dataset. It contains 20 motion content categories and 120 motion style categories. We further propose a Physics-aware Motion Style Regularization mechanism to enforce physical plausibility in the generated motions. Extensive experiments show that PersonaAnimator outperforms state-of-the-art motion transfer methods and sets a new benchmark for the Video-to-Video Motion Personalization task.

</details>


### [20] [WaveHiT-SR: Hierarchical Wavelet Network for Efficient Image Super-Resolution](https://arxiv.org/abs/2508.19927)
*Fayaz Ali,Muhammad Zawish,Steven Davy,Radu Timofte*

Main category: cs.CV

TL;DR: WaveHiT-SR是一种基于小波变换的分层Transformer超分辨率方法，通过自适应分层窗口和多频段分解，在降低计算复杂度的同时提升长距离依赖建模能力，实现了更高效的超分辨率性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer的超分辨率方法由于窗口自注意力机制的二次计算复杂度，只能使用小的固定窗口，限制了感受野和长距离依赖建模能力。

Method: 提出WaveHiT-SR方法：1）使用自适应分层窗口替代静态小窗口；2）利用小波变换将图像分解为多个频率子带；3）通过分层处理逐步重建高分辨率图像，降低计算复杂度。

Result: 方法在SwinIR-Light、SwinIR-NG和SRFormer-Light等模型上实现了最先进的超分辨率结果，具有更少的参数、更低的FLOPs和更快的速度。

Conclusion: WaveHiT-SR通过结合小波变换和分层Transformer框架，有效解决了传统方法计算复杂度高的问题，在保持性能的同时显著提升了效率。

Abstract: Transformers have demonstrated promising performance in computer vision tasks, including image super-resolution (SR). The quadratic computational complexity of window self-attention mechanisms in many transformer-based SR methods forces the use of small, fixed windows, limiting the receptive field. In this paper, we propose a new approach by embedding the wavelet transform within a hierarchical transformer framework, called (WaveHiT-SR). First, using adaptive hierarchical windows instead of static small windows allows to capture features across different levels and greatly improve the ability to model long-range dependencies. Secondly, the proposed model utilizes wavelet transforms to decompose images into multiple frequency subbands, allowing the network to focus on both global and local features while preserving structural details. By progressively reconstructing high-resolution images through hierarchical processing, the network reduces computational complexity without sacrificing performance. The multi-level decomposition strategy enables the network to capture fine-grained information in lowfrequency components while enhancing high-frequency textures. Through extensive experimentation, we confirm the effectiveness and efficiency of our WaveHiT-SR. Our refined versions of SwinIR-Light, SwinIR-NG, and SRFormer-Light deliver cutting-edge SR results, achieving higher efficiency with fewer parameters, lower FLOPs, and faster speeds.

</details>


### [21] [GS: Generative Segmentation via Label Diffusion](https://arxiv.org/abs/2508.20020)
*Yuhao Chen,Shubin Chen,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: 本文提出GS（生成式分割）框架，将图像分割重新定义为生成式任务，通过标签扩散直接生成分割掩码，在Panoptic Narrative Grounding基准上达到新的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法将语言驱动的图像分割视为判别式问题，现有扩散模型方法仍以图像为中心，将分割作为辅助过程。本文希望将分割本身作为生成任务来处理。

Method: 提出GS框架，通过标签扩散直接生成分割掩码，而不是生成图像。该方法以输入图像和语言描述为条件，从噪声直接生成分割标签，实现端到端训练。

Result: 在Panoptic Narrative Grounding基准测试中，GS显著优于现有的判别式和基于扩散的方法，创造了新的最先进水平。

Conclusion: 将分割重新定义为生成任务的方法有效，GS框架通过直接生成标签实现了更好的空间和语义保真度控制，为语言驱动分割提供了新思路。

Abstract: Language-driven image segmentation is a fundamental task in vision-language understanding, requiring models to segment regions of an image corresponding to natural language expressions. Traditional methods approach this as a discriminative problem, assigning each pixel to foreground or background based on semantic alignment. Recently, diffusion models have been introduced to this domain, but existing approaches remain image-centric: they either (i) use image diffusion models as visual feature extractors, (ii) synthesize segmentation data via image generation to train discriminative models, or (iii) perform diffusion inversion to extract attention cues from pre-trained image diffusion models-thereby treating segmentation as an auxiliary process. In this paper, we propose GS (Generative Segmentation), a novel framework that formulates segmentation itself as a generative task via label diffusion. Instead of generating images conditioned on label maps and text, GS reverses the generative process: it directly generates segmentation masks from noise, conditioned on both the input image and the accompanying language description. This paradigm makes label generation the primary modeling target, enabling end-to-end training with explicit control over spatial and semantic fidelity. To demonstrate the effectiveness of our approach, we evaluate GS on Panoptic Narrative Grounding (PNG), a representative and challenging benchmark for multimodal segmentation that requires panoptic-level reasoning guided by narrative captions. Experimental results show that GS significantly outperforms existing discriminative and diffusion-based methods, setting a new state-of-the-art for language-driven segmentation.

</details>


### [22] [Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies](https://arxiv.org/abs/2508.20072)
*Zhixuan Liang,Yizhuo Li,Tianshuo Yang,Chengyue Wu,Sitong Mao,Liuao Pei,Xiaokang Yang,Jiangmiao Pang,Yao Mu,Ping Luo*

Main category: cs.CV

TL;DR: 这篇论文提出了离散渌散VLA模型，通过离散渌散技术将视觉-语言模型与机器人动作控制统一，采用跨熵毛损失进行训练，支持并行解码和锐治重帮助错误修正，在多个环境中超越了自回归和连续渌散基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有VLA解码器或者采用固定左到右的自回归生成方式，或者在核心模型外附加连续渌散头，这些方法需要专门训练和迭代采样，影响了统一可扩展的架构设计。

Method: 设计了一个单一transformer策略，使用离散渌散对离散化的动作块进行建模，与VLM核心采用相同的跨熵毛损失进行训练。此外，还采用了二次重新遮置技术来重新处理不确定的预测。

Result: 在LIBERO上达到96.3%的平均成功率，在SimplerEnv Fractal上达到71.2%的视觉匹配率，在SimplerEnv Bridge上达到49.3%的总体性能，都超过了自回归和连续渌散基线模型。

Conclusion: 离散渌散动作解码器支持精确的动作建模和一致性训练，为将VLA扩展到更大模型和数据集培基了基础。

Abstract: Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions to robot actions. However, prevailing VLA decoders either generate actions autoregressively in a fixed left-to-right order or attach continuous diffusion or flow matching heads outside the backbone, demanding specialized training and iterative sampling that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a single-transformer policy that models discretized action chunks with discrete diffusion and is trained with the same cross-entropy objective as the VLM backbone. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary remasking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pretrained vision language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO, 71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv Bridge, improving over both autoregressive and continuous diffusion baselines. These findings indicate that discrete-diffusion action decoder supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [23] [DATR: Diffusion-based 3D Apple Tree Reconstruction Framework with Sparse-View](https://arxiv.org/abs/2508.19508)
*Tian Qiu,Alan Zoubi,Yiyuan Lin,Ruiming Du,Lailiang Cheng,Yu Jiang*

Main category: cs.RO

TL;DR: 本研究开发了一个两阶段框架DATR，用于从稀疏视角重建苹果树3D模型，在野外和合成数据集上均优于现有方法，处理速度比工业级激光扫描仪快约360倍。


<details>
  <summary>Details</summary>
Motivation: 数字孪生应用需要高精度的3D重建，但现有方法在野外稀疏和遮挡视角条件下表现不佳，特别是在农业环境中。

Method: 采用两阶段框架：第一阶段使用机载传感器和基础模型从复杂野外图像中半自动生成树木掩码；第二阶段使用扩散模型和大重建模型进行单图像到3D重建，通过Real2Sim数据生成器生成合成苹果树进行训练。

Result: DATR框架在两个数据集上都优于现有3D重建方法，在领域特征估计方面与工业级固定激光扫描仪相当，同时处理吞吐量提高了约360倍。

Conclusion: 该框架展示了构建可扩展农业数字孪生系统的强大潜力，能够有效处理野外稀疏视角条件下的3D重建挑战。

Abstract: Digital twin applications offered transformative potential by enabling real-time monitoring and robotic simulation through accurate virtual replicas of physical assets. The key to these systems is 3D reconstruction with high geometrical fidelity. However, existing methods struggled under field conditions, especially with sparse and occluded views. This study developed a two-stage framework (DATR) for the reconstruction of apple trees from sparse views. The first stage leverages onboard sensors and foundation models to semi-automatically generate tree masks from complex field images. Tree masks are used to filter out background information in multi-modal data for the single-image-to-3D reconstruction at the second stage. This stage consists of a diffusion model and a large reconstruction model for respective multi view and implicit neural field generation. The training of the diffusion model and LRM was achieved by using realistic synthetic apple trees generated by a Real2Sim data generator. The framework was evaluated on both field and synthetic datasets. The field dataset includes six apple trees with field-measured ground truth, while the synthetic dataset featured structurally diverse trees. Evaluation results showed that our DATR framework outperformed existing 3D reconstruction methods across both datasets and achieved domain-trait estimation comparable to industrial-grade stationary laser scanners while improving the throughput by $\sim$360 times, demonstrating strong potential for scalable agricultural digital twin systems.

</details>
