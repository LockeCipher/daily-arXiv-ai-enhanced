{"id": "2509.09952", "pdf": "https://arxiv.org/pdf/2509.09952", "abs": "https://arxiv.org/abs/2509.09952", "authors": ["Zhi Ying", "Boxiang Rong", "Jingyu Wang", "Maoyuan Xu"], "title": "Chord: Chain of Rendering Decomposition for PBR Material Estimation from Generated Texture Images", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted to SIGGRAPH Asia 2025. Project page:   https://ubisoft-laforge.github.io/world/chord", "summary": "Material creation and reconstruction are crucial for appearance modeling but traditionally require significant time and expertise from artists. While recent methods leverage visual foundation models to synthesize PBR materials from user-provided inputs, they often fall short in quality, flexibility, and user control. We propose a novel two-stage generate-and-estimate framework for PBR material generation. In the generation stage, a fine-tuned diffusion model synthesizes shaded, tileable texture images aligned with user input. In the estimation stage, we introduce a chained decomposition scheme that sequentially predicts SVBRDF channels by passing previously extracted representation as input into a single-step image-conditional diffusion model. Our method is efficient, high quality, and enables flexible user control. We evaluate our approach against existing material generation and estimation methods, demonstrating superior performance. Our material estimation method shows strong robustness on both generated textures and in-the-wild photographs. Furthermore, we highlight the flexibility of our framework across diverse applications, including text-to-material, image-to-material, structure-guided generation, and material editing.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u9898\u7684\u4e24\u9636\u6bb5\u751f\u6210-\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u8c03\u6269\u6563\u6a21\u578b\u751f\u6210\u5e73\u94fa\u7eb9\u7406\u56fe\u50cf\uff0c\u7136\u540e\u91c7\u7528\u94fe\u5f0f\u5206\u89e3\u65b9\u6848\u9884\u6d4bSVBRDF\u901a\u9053\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u7075\u6d3b\u7684PBR\u6750\u8d28\u751f\u6210\u4e0e\u91cd\u5efa", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728PBR\u6750\u8d28\u5408\u6210\u4e2d\u5b58\u5728\u7684\u8d28\u91cf\u4e0d\u9ad8\u3001\u7075\u6d3b\u6027\u5dee\u548c\u7528\u6237\u63a7\u5236\u4e0d\u8db3\u7b49\u95ee\u9898", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u751f\u6210\u9636\u6bb5\u4f7f\u7528\u7ec6\u8c03\u6269\u6563\u6a21\u578b\u751f\u6210\u5e73\u94fa\u7eb9\u7406\u56fe\u50cf\uff0c\u4f30\u8ba1\u9636\u6bb5\u91c7\u7528\u94fe\u5f0f\u5206\u89e3\u65b9\u6848\u901a\u8fc7\u5355\u6b65\u56fe\u50cf\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5e8f\u5217\u9884\u6d4bSVBRDF\u901a\u9053", "result": "\u65b9\u6cd5\u5728\u6750\u8d28\u751f\u6210\u548c\u4f30\u8ba1\u65b9\u9762\u8868\u73b0\u512a\u5f02\uff0c\u5728\u751f\u6210\u7eb9\u7406\u548c\u5b9e\u9645\u7167\u7247\u4e0a\u90fd\u663e\u793a\u51fa\u5f3a\u52b2\u7684\u7a33\u5065\u6027", "conclusion": "\u8be5\u6846\u67b6\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u4e14\u652f\u6301\u7075\u6d3b\u7684\u7528\u6237\u63a7\u5236\uff0c\u80fd\u591f\u5e94\u7528\u4e8e\u6587\u672c\u5230\u6750\u8d28\u3001\u56fe\u50cf\u5230\u6750\u8d28\u3001\u7ed3\u6784\u6307\u5bfc\u751f\u6210\u548c\u6750\u8d28\u7f16\u8f91\u7b49\u591a\u79cd\u573a\u666f"}}
{"id": "2509.09737", "pdf": "https://arxiv.org/pdf/2509.09737", "abs": "https://arxiv.org/abs/2509.09737", "authors": ["Klemen Kotar", "Wanhee Lee", "Rahul Venkatesh", "Honglin Chen", "Daniel Bear", "Jared Watrous", "Simon Kim", "Khai Loong Aw", "Lilian Naing Chen", "Stefan Stojanov", "Kevin Feigelis", "Imran Thobani", "Alex Durango", "Khaled Jedoui", "Atlas Kazemian", "Dan Yamins"], "title": "World Modeling with Probabilistic Structure Integration", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We present Probabilistic Structure Integration (PSI), a system for learning richly controllable and flexibly promptable world models from data. PSI consists of a three-step cycle. The first step, Probabilistic prediction, involves building a probabilistic graphical model Psi of the data, in the form of a random-access autoregressive sequence model. Psi supports a complete set of learned conditional distributions describing the dependence of any variables in the data on any other set of variables. In step 2, Structure extraction, we show how to extract underlying low-dimensional properties in the data, corresponding to a diverse set of meaningful \"intermediate structures\", in a zero-shot fashion via causal inference on Psi. Step 3, Integration, completes the cycle by converting these structures into new token types that are then continually mixed back into the training diet as conditioning signals and prediction targets. Each such cycle augments the capabilities of Psi, both allowing it to model the underlying data better, and creating new control handles -- akin to an LLM-like universal prompting language. We train an instance of Psi on 1.4 trillion tokens of internet video data; we use it to perform a variety of useful video prediction and understanding inferences; we extract state-of-the-art optical flow, self-supervised depth and object segmentation; and we use these structures to support a full cycle of predictive improvements.", "AI": {"tldr": "PSI\u662f\u4e00\u4e2a\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u53ef\u63a7\u5236\u3001\u53ef\u63d0\u793a\u4e16\u754c\u6a21\u578b\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u6982\u7387\u9884\u6d4b\u3001\u7ed3\u6784\u63d0\u53d6\u548c\u96c6\u6210\u4e09\u4e2a\u6b65\u9aa4\u5faa\u73af\u63d0\u5347\u6a21\u578b\u80fd\u529b\uff0c\u5728\u89c6\u9891\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86\u5148\u8fdb\u7684\u89c6\u9891\u9884\u6d4b\u548c\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u4ece\u6570\u636e\u4e2d\u81ea\u52a8\u5b66\u4e60\u4e30\u5bcc\u63a7\u5236\u7ed3\u6784\u548c\u7075\u6d3b\u63d0\u793a\u529f\u80fd\u7684\u4e16\u754c\u6a21\u578b\u7cfb\u7edf\uff0c\u5b9e\u73b0\u5bf9\u89c6\u9891\u6570\u636e\u7684\u9ad8\u6548\u7406\u89e3\u548c\u9884\u6d4b\u3002", "method": "\u4e09\u6b65\u9aa4\u5faa\u73af\uff1a1)\u6784\u5efa\u6982\u7387\u56fe\u6a21\u578bPsi\uff1b2)\u901a\u8fc7\u56e0\u679c\u63a8\u65ad\u96f6\u6837\u672c\u63d0\u53d6\u5e95\u5c42\u4f4e\u7ef4\u7ed3\u6784\uff1b3)\u5c06\u7ed3\u6784\u8f6c\u6362\u4e3a\u65b0token\u7c7b\u578b\u5e76\u96c6\u6210\u56de\u8bad\u7ec3\u4e2d\u3002", "result": "\u57281.4\u4e07\u4ebf\u89c6\u9891token\u4e0a\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5149\u6d41\u3001\u81ea\u76d1\u7763\u6df1\u5ea6\u548c\u5bf9\u8c61\u5206\u5272\uff0c\u652f\u6301\u5b8c\u6574\u7684\u9884\u6d4b\u6539\u8fdb\u5faa\u73af\u3002", "conclusion": "PSI\u7cfb\u7edf\u901a\u8fc7\u5faa\u73af\u7ed3\u6784\u96c6\u6210\u6709\u6548\u63d0\u5347\u4e86\u4e16\u754c\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\u548c\u63a7\u5236\u7075\u6d3b\u6027\uff0c\u4e3a\u89c6\u9891\u7406\u89e3\u548c\u751f\u6210\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u57fa\u7840\u6a21\u578b\u3002"}}
{"id": "2509.09742", "pdf": "https://arxiv.org/pdf/2509.09742", "abs": "https://arxiv.org/abs/2509.09742", "authors": ["Md Fazle Rasul", "Alanood Alqobaisi", "Bruhadeshwar Bezawada", "Indrakshi Ray"], "title": "Images in Motion?: A First Look into Video Leakage in Collaborative Deep Learning", "categories": ["cs.CV"], "comment": null, "summary": "Federated learning (FL) allows multiple entities to train a shared model collaboratively. Its core, privacy-preserving principle is that participants only exchange model updates, such as gradients, and never their raw, sensitive data. This approach is fundamental for applications in domains where privacy and confidentiality are important. However, the security of this very mechanism is threatened by gradient inversion attacks, which can reverse-engineer private training data directly from the shared gradients, defeating the purpose of FL. While the impact of these attacks is known for image, text, and tabular data, their effect on video data remains an unexamined area of research. This paper presents the first analysis of video data leakage in FL using gradient inversion attacks. We evaluate two common video classification approaches: one employing pre-trained feature extractors and another that processes raw video frames with simple transformations. Our initial results indicate that the use of feature extractors offers greater resilience against gradient inversion attacks. We also demonstrate that image super-resolution techniques can enhance the frames extracted through gradient inversion attacks, enabling attackers to reconstruct higher-quality videos. Our experiments validate this across scenarios where the attacker has access to zero, one, or more reference frames from the target environment. We find that although feature extractors make attacks more challenging, leakage is still possible if the classifier lacks sufficient complexity. We, therefore, conclude that video data leakage in FL is a viable threat, and the conditions under which it occurs warrant further investigation.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5206\u6790\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u89c6\u9891\u6570\u636e\u7684\u68af\u5ea6\u53cd\u6f14\u653b\u51fb\u98ce\u9669\uff0c\u53d1\u73b0\u4f7f\u7528\u7279\u5f81\u63d0\u53d6\u5668\u80fd\u63d0\u4f9b\u66f4\u597d\u4fdd\u62a4\uff0c\u4f46\u653b\u51fb\u8005\u4ecd\u53ef\u901a\u8fc7\u8d85\u5206\u8fa8\u7387\u6280\u672f\u91cd\u5efa\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u8bc1\u660e\u4e86\u89c6\u9891\u6570\u636e\u6cc4\u9732\u662f\u771f\u5b9e\u5a01\u80c1\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u7684\u6838\u5fc3\u9690\u79c1\u4fdd\u62a4\u539f\u5219\u662f\u901a\u8fc7\u4ea4\u6362\u6a21\u578b\u66f4\u65b0\u800c\u975e\u539f\u59cb\u6570\u636e\u6765\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u68af\u5ea6\u53cd\u6f14\u653b\u51fb\u80fd\u591f\u4ece\u5171\u4eab\u68af\u5ea6\u4e2d\u91cd\u5efa\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u3002\u867d\u7136\u8fd9\u79cd\u653b\u51fb\u5bf9\u56fe\u50cf\u3001\u6587\u672c\u548c\u8868\u683c\u6570\u636e\u7684\u5f71\u54cd\u5df2\u77e5\uff0c\u4f46\u5bf9\u89c6\u9891\u6570\u636e\u7684\u5f71\u54cd\u5c1a\u672a\u7814\u7a76\u3002", "method": "\u8bc4\u4f30\u4e24\u79cd\u89c6\u9891\u5206\u7c7b\u65b9\u6cd5\uff1a\u4f7f\u7528\u9884\u8bad\u7ec3\u7279\u5f81\u63d0\u53d6\u5668\u7684\u65b9\u6cd5\u548c\u5904\u7406\u539f\u59cb\u89c6\u9891\u5e27\u7684\u7b80\u5355\u53d8\u6362\u65b9\u6cd5\u3002\u6d4b\u8bd5\u4e86\u653b\u51fb\u8005\u5728\u62e5\u6709\u96f6\u4e2a\u3001\u4e00\u4e2a\u6216\u591a\u4e2a\u53c2\u8003\u5e27\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u68af\u5ea6\u53cd\u6f14\u653b\u51fb\uff0c\u5e76\u4f7f\u7528\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6280\u672f\u63d0\u5347\u91cd\u5efa\u89c6\u9891\u8d28\u91cf\u3002", "result": "\u7279\u5f81\u63d0\u53d6\u5668\u5bf9\u68af\u5ea6\u53cd\u6f14\u653b\u51fb\u5177\u6709\u66f4\u5f3a\u7684\u62b5\u6297\u529b\uff0c\u4f46\u653b\u51fb\u8005\u4ecd\u53ef\u901a\u8fc7\u8d85\u5206\u8fa8\u7387\u6280\u672f\u91cd\u5efa\u9ad8\u8d28\u91cf\u89c6\u9891\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5728\u4e0d\u540c\u53c2\u8003\u5e27\u8bbf\u95ee\u573a\u666f\u4e0b\u653b\u51fb\u7684\u53ef\u884c\u6027\uff0c\u53d1\u73b0\u5206\u7c7b\u5668\u590d\u6742\u5ea6\u4e0d\u8db3\u65f6\u4ecd\u53ef\u80fd\u53d1\u751f\u6570\u636e\u6cc4\u9732\u3002", "conclusion": "\u89c6\u9891\u6570\u636e\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u5b58\u5728\u68af\u5ea6\u53cd\u6f14\u653b\u51fb\u7684\u6cc4\u9732\u98ce\u9669\uff0c\u4f7f\u7528\u7279\u5f81\u63d0\u53d6\u5668\u80fd\u63d0\u4f9b\u4e00\u5b9a\u4fdd\u62a4\u4f46\u5e76\u975e\u7edd\u5bf9\u5b89\u5168\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6cc4\u9732\u53d1\u751f\u7684\u5177\u4f53\u6761\u4ef6\u548c\u9632\u62a4\u63aa\u65bd\u3002"}}
{"id": "2509.09971", "pdf": "https://arxiv.org/pdf/2509.09971", "abs": "https://arxiv.org/abs/2509.09971", "authors": ["Aupendu Kar", "Vishnu Raj", "Guan-Ming Su"], "title": "Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey", "categories": ["cs.CV"], "comment": null, "summary": "Event camera sensors are bio-inspired sensors which asynchronously capture per-pixel brightness changes and output a stream of events encoding the polarity, location and time of these changes. These systems are witnessing rapid advancements as an emerging field, driven by their low latency, reduced power consumption, and ultra-high capture rates. This survey explores the evolution of fusing event-stream captured with traditional frame-based capture, highlighting how this synergy significantly benefits various video restoration and 3D reconstruction tasks. The paper systematically reviews major deep learning contributions to image/video enhancement and restoration, focusing on two dimensions: temporal enhancement (such as frame interpolation and motion deblurring) and spatial enhancement (including super-resolution, low-light and HDR enhancement, and artifact reduction). This paper also explores how the 3D reconstruction domain evolves with the advancement of event driven fusion. Diverse topics are covered, with in-depth discussions on recent works for improving visual quality under challenging conditions. Additionally, the survey compiles a comprehensive list of openly available datasets, enabling reproducible research and benchmarking. By consolidating recent progress and insights, this survey aims to inspire further research into leveraging event camera systems, especially in combination with deep learning, for advanced visual media restoration and enhancement.", "AI": {"tldr": "\u8fd9\u7bc7\u8c03\u7814\u8bba\u6587\u7cfb\u7edf\u8ff0\u4e86\u4e8b\u4ef6\u76f8\u673a\u4e0e\u4f20\u7edf\u6846\u7387\u76f8\u673a\u878d\u5408\u7684\u53d1\u5c55\uff0c\u91cd\u70b9\u5173\u6ce8\u5728\u89c6\u9891\u6062\u590d\u548c3D\u91cd\u5efa\u4efb\u52a1\u4e2d\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002\u5305\u62ec\u65f6\u95f4\u589e\u5f3a\u3001\u7a7a\u95f4\u589e\u5f3a\u548c\u5f00\u653e\u6570\u636e\u96c6\u7b49\u591a\u4e2a\u65b9\u9762\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u4f5c\u4e3a\u4e00\u79cd\u65b0\u5174\u4f20\u611f\u5668\uff0c\u5177\u6709\u4f4e\u5ef6\u8fdf\u3001\u4f4e\u529f\u8017\u548c\u8d85\u9ad8\u635f\u5931\u7387\u7b49\u4f18\u52bf\uff0c\u4f46\u9700\u8981\u4e0e\u4f20\u7edf\u6846\u7387\u76f8\u673a\u878d\u5408\u624d\u80fd\u5145\u5206\u53d1\u6325\u5176\u6f5c\u529b\u3002\u8c03\u7814\u7684\u52a8\u673a\u662f\u7cfb\u7edf\u6574\u7406\u8fd9\u79cd\u878d\u5408\u6280\u672f\u7684\u53d1\u5c55\u8fdb\u5c55\uff0c\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u57fa\u7840\u3002", "method": "\u8bba\u6587\u91c7\u7528\u7cfb\u7edf\u6027\u8c03\u7814\u65b9\u6cd5\uff0c\u4ece\u4e24\u4e2a\u7ef4\u5ea6\u5168\u9762\u8bc4\u4f30\u6df1\u5ea6\u5b66\u4e60\u5728\u56fe\u50cf/\u89c6\u9891\u589e\u5f3a\u548c\u6062\u590d\u4e2d\u7684\u8d21\u732e\uff1a\u65f6\u95f4\u589e\u5f3a\uff08\u5305\u62ec\u5e27\u95f4\u63d2\u503c\u3001\u8fd0\u52a8\u53bb\u6a21\u7cca\uff09\u548c\u7a7a\u95f4\u589e\u5f3a\uff08\u5305\u62ec\u8d85\u5206\u8fa8\u7387\u3001\u4f4e\u5149\u589e\u5f3a\u3001HDR\u589e\u5f3a\u548c\u4f2a\u5f71\u51cf\u5c11\uff09\u3002\u540c\u65f6\u8ba8\u8bba\u4e8b\u4ef6\u9a71\u52a8\u878d\u5408\u5bf93D\u91cd\u5efa\u9886\u57df\u7684\u5f71\u54cd\u3002", "result": "\u8c03\u7814\u7cfb\u7edf\u6574\u7406\u4e86\u6700\u65b0\u8fdb\u5c55\u548c\u89c1\u89e3\uff0c\u7f16\u5199\u4e86\u5b8c\u6574\u7684\u5f00\u653e\u6570\u636e\u96c6\u6e05\u5355\uff0c\u652f\u6301\u53ef\u590d\u73b0\u7814\u7a76\u548c\u6d4b\u8bd5\u3002\u8bba\u6587\u5c55\u793a\u4e86\u4e8b\u4ef6\u76f8\u673a\u4e0e\u4f20\u7edf\u6846\u7387\u76f8\u673a\u878d\u5408\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u6539\u5584\u89c6\u89c9\u8d28\u91cf\u7684\u6f5c\u529b\u3002", "conclusion": "\u8fd9\u4efd\u8c03\u7814\u901a\u8fc7\u6c47\u603b\u6700\u65b0\u8fdb\u5c55\uff0c\u671f\u671b\u80fd\u591f\u6fc0\u53d1\u66f4\u591a\u7814\u7a76\u8005\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7ed3\u5408\uff0c\u4ece\u800c\u63a8\u52a8\u9ad8\u7ea7\u89c6\u89c9\u5a92\u4f53\u6062\u590d\u548c\u589e\u5f3a\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.10122", "pdf": "https://arxiv.org/pdf/2509.10122", "abs": "https://arxiv.org/abs/2509.10122", "authors": ["Zongliang Wu", "Siming Zheng", "Peng-Tao Jiang", "Xin Yuan"], "title": "Realism Control One-step Diffusion for Real-World Image Super-Resolution", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Pre-trained diffusion models have shown great potential in real-world image super-resolution (Real-ISR) tasks by enabling high-resolution reconstructions. While one-step diffusion (OSD) methods significantly improve efficiency compared to traditional multi-step approaches, they still have limitations in balancing fidelity and realism across diverse scenarios. Since the OSDs for SR are usually trained or distilled by a single timestep, they lack flexible control mechanisms to adaptively prioritize these competing objectives, which are inherently manageable in multi-step methods through adjusting sampling steps. To address this challenge, we propose a Realism Controlled One-step Diffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping strategy that enables explicit control over fidelity-realism trade-offs during the noise prediction phase with minimal training paradigm modifications and original training data. A degradation-aware sampling strategy is also introduced to align distillation regularization with the grouping strategy and enhance the controlling of trade-offs. Moreover, a visual prompt injection module is used to replace conventional text prompts with degradation-aware visual tokens, enhancing both restoration accuracy and semantic consistency. Our method achieves superior fidelity and perceptual quality while maintaining computational efficiency. Extensive experiments demonstrate that RCOD outperforms state-of-the-art OSD methods in both quantitative metrics and visual qualities, with flexible realism control capabilities in the inference stage. The code will be released.", "AI": {"tldr": "RCOD\u6846\u67b6\u901a\u8fc7\u6f5c\u5728\u57df\u5206\u7ec4\u7b56\u7565\u548c\u9000\u5316\u611f\u77e5\u91c7\u6837\uff0c\u5728\u5355\u6b65\u6269\u6563\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86\u4fdd\u771f\u5ea6\u4e0e\u771f\u5b9e\u611f\u7684\u7075\u6d3b\u63a7\u5236\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u4e86\u8d85\u5206\u8fa8\u7387\u6027\u80fd", "motivation": "\u4f20\u7edf\u5355\u6b65\u6269\u6563\u65b9\u6cd5\u5728\u771f\u5b9e\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u7075\u6d3b\u7684\u4fdd\u771f\u5ea6-\u771f\u5b9e\u611f\u6743\u8861\u63a7\u5236\u673a\u5236\uff0c\u65e0\u6cd5\u50cf\u591a\u6b65\u65b9\u6cd5\u90a3\u6837\u901a\u8fc7\u8c03\u6574\u91c7\u6837\u6b65\u9aa4\u6765\u9002\u5e94\u4e0d\u540c\u573a\u666f\u9700\u6c42", "method": "\u63d0\u51faRCOD\u6846\u67b6\uff1a1\uff09\u6f5c\u5728\u57df\u5206\u7ec4\u7b56\u7565\u5b9e\u73b0\u566a\u58f0\u9884\u6d4b\u9636\u6bb5\u7684\u663e\u5f0f\u63a7\u5236\uff1b2\uff09\u9000\u5316\u611f\u77e5\u91c7\u6837\u7b56\u7565\u5bf9\u9f50\u84b8\u998f\u6b63\u5219\u5316\uff1b3\uff09\u89c6\u89c9\u63d0\u793a\u6ce8\u5165\u6a21\u5757\u66ff\u6362\u6587\u672c\u63d0\u793a\uff0c\u4f7f\u7528\u9000\u5316\u611f\u77e5\u89c6\u89c9token", "result": "\u5728\u5b9a\u91cf\u6307\u6807\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u5355\u6b65\u6269\u6563\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u63a8\u7406\u9636\u6bb5\u7684\u7075\u6d3b\u771f\u5b9e\u611f\u63a7\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387", "conclusion": "RCOD\u6210\u529f\u89e3\u51b3\u4e86\u5355\u6b65\u6269\u6563\u6a21\u578b\u5728\u4fdd\u771f\u5ea6-\u771f\u5b9e\u611f\u6743\u8861\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u771f\u5b9e\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.10140", "pdf": "https://arxiv.org/pdf/2509.10140", "abs": "https://arxiv.org/abs/2509.10140", "authors": ["Yifan Chang", "Jie Qin", "Limeng Qiao", "Xiaofeng Wang", "Zheng Zhu", "Lin Ma", "Xingang Wang"], "title": "Scalable Training for Vector-Quantized Networks with 100% Codebook Utilization", "categories": ["cs.CV"], "comment": null, "summary": "Vector quantization (VQ) is a key component in discrete tokenizers for image generation, but its training is often unstable due to straight-through estimation bias, one-step-behind updates, and sparse codebook gradients, which lead to suboptimal reconstruction performance and low codebook usage. In this work, we analyze these fundamental challenges and provide a simple yet effective solution. To maintain high codebook usage in VQ networks (VQN) during learning annealing and codebook size expansion, we propose VQBridge, a robust, scalable, and efficient projector based on the map function method. VQBridge optimizes code vectors through a compress-process-recover pipeline, enabling stable and effective codebook training. By combining VQBridge with learning annealing, our VQN achieves full (100%) codebook usage across diverse codebook configurations, which we refer to as FVQ (FullVQ). Through extensive experiments, we demonstrate that FVQ is effective, scalable, and generalizable: it attains 100% codebook usage even with a 262k-codebook, achieves state-of-the-art reconstruction performance, consistently improves with larger codebooks, higher vector channels, or longer training, and remains effective across different VQ variants. Moreover, when integrated with LlamaGen, FVQ significantly enhances image generation performance, surpassing visual autoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID, highlighting the importance of high-quality tokenizers for strong autoregressive image generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86VQBridge\u65b9\u6cd5\u89e3\u51b3VQ\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u901a\u8fc7compress-process-recover\u7ba1\u9053\u5b9e\u73b0100%\u7801\u672c\u4f7f\u7528\u7387\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u91cd\u5efa\u548c\u751f\u6210\u6027\u80fd", "motivation": "\u89e3\u51b3\u5411\u91cf\u91cf\u5316(VQ)\u8bad\u7ec3\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5305\u62ec\u76f4\u901a\u4f30\u8ba1\u504f\u5dee\u3001\u4e00\u6b65\u6ede\u540e\u66f4\u65b0\u548c\u7a00\u758f\u7801\u672c\u68af\u5ea6\uff0c\u8fd9\u4e9b\u5bfc\u81f4\u91cd\u5efa\u6027\u80fd\u4e0d\u4f73\u548c\u7801\u672c\u4f7f\u7528\u7387\u4f4e", "method": "\u63d0\u51faVQBridge\u6295\u5f71\u5668\uff0c\u57fa\u4e8e\u6620\u5c04\u51fd\u6570\u65b9\u6cd5\uff0c\u901a\u8fc7compress-process-recover\u7ba1\u9053\u4f18\u5316\u7801\u5411\u91cf\uff0c\u7ed3\u5408\u5b66\u4e60\u9000\u706b\u5b9e\u73b0\u7a33\u5b9a\u7801\u672c\u8bad\u7ec3", "result": "\u5b9e\u73b0100%\u7801\u672c\u4f7f\u7528\u7387\uff08\u5373\u4f7f262k\u5927\u7801\u672c\uff09\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u91cd\u5efa\u6027\u80fd\uff0c\u4e0eLlamaGen\u96c6\u6210\u540e\u56fe\u50cf\u751f\u6210\u6027\u80fd\u8d85\u8d8aVAR\u6a21\u578b0.5 rFID\u548cDiT\u6a21\u578b0.2 rFID", "conclusion": "\u9ad8\u8d28\u91cf\u5206\u8bcd\u5668\u5bf9\u5f3a\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u81f3\u5173\u91cd\u8981\uff0cFVQ\u65b9\u6cd5\u6709\u6548\u3001\u53ef\u6269\u5c55\u4e14\u901a\u7528\uff0c\u5728\u4e0d\u540cVQ\u53d8\u4f53\u4e2d\u90fd\u4fdd\u6301\u6709\u6548\u6027"}}
{"id": "2509.10241", "pdf": "https://arxiv.org/pdf/2509.10241", "abs": "https://arxiv.org/abs/2509.10241", "authors": ["Elias De Smijter", "Renaud Detry", "Christophe De Vleeschouwer"], "title": "On the Geometric Accuracy of Implicit and Primitive-based Representations Derived from View Rendering Constraints", "categories": ["cs.CV"], "comment": "9 pages, 3 figures, to be presented at ASTRA25,", "summary": "We present the first systematic comparison of implicit and explicit Novel View Synthesis methods for space-based 3D object reconstruction, evaluating the role of appearance embeddings. While embeddings improve photometric fidelity by modeling lighting variation, we show they do not translate into meaningful gains in geometric accuracy - a critical requirement for space robotics applications. Using the SPEED+ dataset, we compare K-Planes, Gaussian Splatting, and Convex Splatting, and demonstrate that embeddings primarily reduce the number of primitives needed for explicit methods rather than enhancing geometric fidelity. Moreover, convex splatting achieves more compact and clutter-free representations than Gaussian splatting, offering advantages for safety-critical applications such as interaction and collision avoidance. Our findings clarify the limits of appearance embeddings for geometry-centric tasks and highlight trade-offs between reconstruction quality and representation efficiency in space scenarios.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6bd4\u8f83\u4e86\u9690\u5f0f\u548c\u663e\u5f0f\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\u5728\u7a7a\u95f43D\u7269\u4f53\u91cd\u5efa\u4e2d\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u8bc4\u4f30\u4e86\u5916\u89c2\u5d4c\u5165\u7684\u4f5c\u7528\u3002\u7814\u7a76\u53d1\u73b0\u5d4c\u5165\u867d\u7136\u80fd\u63d0\u5347\u5149\u5ea6\u4fdd\u771f\u5ea6\uff0c\u4f46\u4e0d\u4f1a\u663e\u8457\u6539\u5584\u51e0\u4f55\u7cbe\u5ea6\uff0c\u800c\u51f8\u6837\u6761\u6bd4\u9ad8\u65af\u6837\u6761\u80fd\u63d0\u4f9b\u66f4\u7d27\u51d1\u3001\u65e0\u6742\u4e71\u7684\u8868\u793a\u3002", "motivation": "\u7814\u7a76\u7a7a\u95f4\u673a\u5668\u4eba\u5e94\u7528\u4e2d3D\u91cd\u5efa\u65b9\u6cd5\u7684\u51e0\u4f55\u7cbe\u5ea6\u9700\u6c42\uff0c\u8bc4\u4f30\u5916\u89c2\u5d4c\u5165\u5728\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u65b9\u9762\u7684\u5b9e\u9645\u6548\u679c\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u5e94\u7528\u63d0\u4f9b\u66f4\u4f18\u7684\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528SPEED+\u6570\u636e\u96c6\uff0c\u6bd4\u8f83K-Planes\u3001\u9ad8\u65af\u6837\u6761\u548c\u51f8\u6837\u6761\u4e09\u79cd\u65b9\u6cd5\uff0c\u5206\u6790\u5916\u89c2\u5d4c\u5165\u5bf9\u51e0\u4f55\u7cbe\u5ea6\u548c\u8868\u793a\u6548\u7387\u7684\u5f71\u54cd\u3002", "result": "\u5916\u89c2\u5d4c\u5165\u4e3b\u8981\u51cf\u5c11\u663e\u5f0f\u65b9\u6cd5\u6240\u9700\u7684\u57fa\u672c\u5143\u7d20\u6570\u91cf\uff0c\u800c\u975e\u63d0\u5347\u51e0\u4f55\u4fdd\u771f\u5ea6\uff1b\u51f8\u6837\u6761\u76f8\u6bd4\u9ad8\u65af\u6837\u6761\u80fd\u4ea7\u751f\u66f4\u7d27\u51d1\u4e14\u65e0\u6742\u4e71\u7684\u8868\u793a\u3002", "conclusion": "\u5916\u89c2\u5d4c\u5165\u5728\u51e0\u4f55\u4e2d\u5fc3\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7a7a\u95f4\u573a\u666f\u4e2d\u9700\u8981\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u8868\u793a\u6548\u7387\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\uff0c\u51f8\u6837\u6761\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2509.10250", "pdf": "https://arxiv.org/pdf/2509.10250", "abs": "https://arxiv.org/abs/2509.10250", "authors": ["Haozhen Yan", "Yan Hong", "Suning Lang", "Jiahui Zhan", "Yikun Ji", "Yujie Gao", "Jun Lan", "Huijia Zhu", "Weiqiang Wang", "Jianfu Zhang"], "title": "GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection", "categories": ["cs.CV"], "comment": "11 pages, 5 figures", "summary": "With generative models becoming increasingly sophisticated and diverse, detecting AI-generated images has become increasingly challenging. While existing AI-genereted Image detectors achieve promising performance on in-distribution generated images, their generalization to unseen generative models remains limited. This limitation is largely attributed to their reliance on generation-specific artifacts, such as stylistic priors and compression patterns. To address these limitations, we propose GAMMA, a novel training framework designed to reduce domain bias and enhance semantic alignment. GAMMA introduces diverse manipulation strategies, such as inpainting-based manipulation and semantics-preserving perturbations, to ensure consistency between manipulated and authentic content. We employ multi-task supervision with dual segmentation heads and a classification head, enabling pixel-level source attribution across diverse generative domains. In addition, a reverse cross-attention mechanism is introduced to allow the segmentation heads to guide and correct biased representations in the classification branch. Our method achieves state-of-the-art generalization performance on the GenImage benchmark, imporving accuracy by 5.8%, but also maintains strong robustness on newly released generative model such as GPT-4o.", "AI": {"tldr": "GAMMA\u662f\u4e00\u4e2a\u65b0\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u51cf\u5c11\u9886\u57df\u504f\u5dee\u548c\u589e\u5f3a\u8bed\u4e49\u5bf9\u9f50\u6765\u63d0\u5347AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728GenImage\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e865.8%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u5668\u867d\u7136\u5728\u5206\u5e03\u5185\u751f\u6210\u56fe\u50cf\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u672a\u89c1\u8fc7\u7684\u751f\u6210\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u4e3b\u8981\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u751f\u6210\u7279\u5b9a\u7684\u4f2a\u5f71\uff08\u5982\u98ce\u683c\u5148\u9a8c\u548c\u538b\u7f29\u6a21\u5f0f\uff09\u3002", "method": "\u63d0\u51faGAMMA\u6846\u67b6\uff0c\u5f15\u5165\u591a\u6837\u5316\u64cd\u4f5c\u7b56\u7565\uff08\u5982\u57fa\u4e8e\u4fee\u590d\u7684\u64cd\u4f5c\u548c\u8bed\u4e49\u4fdd\u6301\u6270\u52a8\uff09\u786e\u4fdd\u64cd\u4f5c\u5185\u5bb9\u4e0e\u771f\u5b9e\u5185\u5bb9\u7684\u4e00\u81f4\u6027\uff1b\u91c7\u7528\u591a\u4efb\u52a1\u76d1\u7763\uff0c\u4f7f\u7528\u53cc\u91cd\u5206\u5272\u5934\u548c\u5206\u7c7b\u5934\u5b9e\u73b0\u50cf\u7d20\u7ea7\u6765\u6e90\u5f52\u56e0\uff1b\u5f15\u5165\u53cd\u5411\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u8ba9\u5206\u5272\u5934\u6307\u5bfc\u548c\u7ea0\u6b63\u5206\u7c7b\u5206\u652f\u4e2d\u7684\u504f\u5dee\u8868\u793a\u3002", "result": "\u5728GenImage\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u51c6\u786e\u7387\u63d0\u53475.8%\uff0c\u540c\u65f6\u5728\u65b0\u53d1\u5e03\u7684\u751f\u6210\u6a21\u578b\uff08\u5982GPT-4o\uff09\u4e0a\u4fdd\u6301\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "GAMMA\u901a\u8fc7\u51cf\u5c11\u9886\u57df\u504f\u5dee\u548c\u589e\u5f3a\u8bed\u4e49\u5bf9\u9f50\uff0c\u6709\u6548\u89e3\u51b3\u4e86AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u5668\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u68c0\u6d4b\u591a\u6837\u5316\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10257", "pdf": "https://arxiv.org/pdf/2509.10257", "abs": "https://arxiv.org/abs/2509.10257", "authors": ["Ema Masterl", "Tina Vipotnik Vesnaver", "\u017diga \u0160piclin"], "title": "Robustness and Diagnostic Performance of Super-Resolution Fetal Brain MRI", "categories": ["cs.CV"], "comment": "Accepted at the PIPPI Workshop of MICCAI 2025", "summary": "Fetal brain MRI relies on rapid multi-view 2D slice acquisitions to reduce motion artifacts caused by fetal movement. However, these stacks are typically low resolution, may suffer from motion corruption, and do not adequately capture 3D anatomy. Super-resolution reconstruction (SRR) methods aim to address these limitations by combining slice-to-volume registration and super-resolution techniques to generate high-resolution (HR) 3D volumes. While several SRR methods have been proposed, their comparative performance - particularly in pathological cases - and their influence on downstream volumetric analysis and diagnostic tasks remain underexplored. In this study, we applied three state-of-the-art SRR method - NiftyMIC, SVRTK, and NeSVoR - to 140 fetal brain MRI scans, including both healthy controls (HC) and pathological cases (PC) with ventriculomegaly (VM). Each HR reconstruction was segmented using the BoUNTi algorithm to extract volumes of nine principal brain structures. We evaluated visual quality, SRR success rates, volumetric measurement agreement, and diagnostic classification performance. NeSVoR demonstrated the highest and most consistent reconstruction success rate (>90%) across both HC and PC groups. Although significant differences in volumetric estimates were observed between SRR methods, classification performance for VM was not affected by the choice of SRR method. These findings highlight NeSVoR's robustness and the resilience of diagnostic performance despite SRR-induced volumetric variability.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5bf9\u6bd4\u4e86\u4e09\u79cd\u80ce\u513f\u8111\u90e8MRI\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u65b9\u6cd5\uff0c\u53d1\u73b0NeSVoR\u65b9\u6cd5\u5177\u6709\u6700\u9ad8\u7684\u91cd\u5efa\u6210\u529f\u7387\uff0c\u867d\u7136\u4e0d\u540c\u65b9\u6cd5\u5bfc\u81f4\u4f53\u79ef\u6d4b\u91cf\u5dee\u5f02\uff0c\u4f46\u8bca\u65ad\u6027\u80fd\u53d7\u5f71\u54cd\u8f83\u5c0f\u3002", "motivation": "\u80ce\u513f\u8111\u90e8MRI\u91c7\u96c6\u5b58\u5728\u5206\u8fa8\u7387\u4f4e\u3001\u8fd0\u52a8\u4f4d\u79fb\u95ee\u9898\uff0c\u9700\u8981\u8bc4\u4f30\u4e0d\u540c\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u65b9\u6cd5\u5728\u75c5\u7406\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\u53ca\u5bf9\u4e0b\u6e38\u5206\u6790\u7684\u5f71\u54cd\u3002", "method": "\u5bf9140\u4f8b\u80ce\u513f\u8111\u90e8MRI\u626b\u63cf\uff08\u5305\u62ec\u5065\u5eb7\u548c\u8111\u5ba4\u6269\u5927\u75c5\u4f8b\uff09\u5e94\u7528\u4e09\u79cdSRR\u65b9\u6cd5\uff08NiftyMIC\u3001SVRTK\u3001NeSVoR\uff09\uff0c\u901a\u8fc7BoUNTi\u7b97\u6cd5\u8fdb\u884c\u8111\u7ed3\u6784\u5206\u5272\uff0c\u8bc4\u4f30\u89c6\u89c9\u8d28\u91cf\u3001\u91cd\u5efa\u6210\u529f\u7387\u3001\u4f53\u79ef\u6d4b\u91cf\u4e00\u81f4\u6027\u548c\u8bca\u65ad\u5206\u7c7b\u6027\u80fd\u3002", "result": "NeSVoR\u5728\u5065\u5eb7\u548c\u75c5\u7406\u7ec4\u90fd\u5448\u73b0\u6700\u9ad8\u91cd\u5efa\u6210\u529f\u7387\uff08>90%\uff09\u3002\u867d\u7136\u4e0d\u540cSRR\u65b9\u6cd5\u5bfc\u81f4\u4f53\u79ef\u6d4b\u91cf\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u8111\u5ba4\u6269\u5927\u7684\u8bca\u65ad\u5206\u7c7b\u6027\u80fd\u53d7SRR\u65b9\u6cd5\u9009\u62e9\u5f71\u54cd\u8f83\u5c0f\u3002", "conclusion": "NeSVoR\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u7a33\u5065\u6027\uff0c\u867d\u7136\u4e0d\u540cSRR\u65b9\u6cd5\u5bfc\u81f4\u4f53\u79ef\u6d4b\u91cf\u5dee\u5f02\uff0c\u4f46\u8bca\u65ad\u6027\u80fd\u5f88\u5c11\u53d7\u5230\u5f71\u54cd\uff0c\u8fd9\u4e3a\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2509.10259", "pdf": "https://arxiv.org/pdf/2509.10259", "abs": "https://arxiv.org/abs/2509.10259", "authors": ["Hua Yuan", "Jin Yuan", "Yicheng Jiang", "Yao Zhang", "Xin Geng", "Yong Rui"], "title": "Mask Consistency Regularization in Object Removal", "categories": ["cs.CV"], "comment": null, "summary": "Object removal, a challenging task within image inpainting, involves seamlessly filling the removed region with content that matches the surrounding context. Despite advancements in diffusion models, current methods still face two critical challenges. The first is mask hallucination, where the model generates irrelevant or spurious content inside the masked region, and the second is mask-shape bias, where the model fills the masked area with an object that mimics the mask's shape rather than surrounding content. To address these issues, we propose Mask Consistency Regularization (MCR), a novel training strategy designed specifically for object removal tasks. During training, our approach introduces two mask perturbations: dilation and reshape, enforcing consistency between the outputs of these perturbed branches and the original mask. The dilated masks help align the model's output with the surrounding content, while reshaped masks encourage the model to break the mask-shape bias. This combination of strategies enables MCR to produce more robust and contextually coherent inpainting results. Our experiments demonstrate that MCR significantly reduces hallucinations and mask-shape bias, leading to improved performance in object removal.", "AI": {"tldr": "\u901a\u8fc7\u63d0\u51fa\u9762\u5177\u4e00\u81f4\u6027\u6b63\u5219\u5316(MCR)\u8bad\u7ec3\u7b56\u7565\uff0c\u89e3\u51b3\u56fe\u50cf\u586b\u5145\u4e2d\u7684\u9762\u5177\u5e7b\u89c9\u548c\u9762\u5177\u5f62\u72b6\u504f\u8f9b\u95ee\u9898\uff0c\u63d0\u5347\u7269\u4f53\u79fb\u9664\u6548\u679c", "motivation": "\u5f53\u524d\u6269\u6563\u6a21\u578b\u5728\u7269\u4f53\u79fb\u9664\u4efb\u52a1\u4e2d\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u9762\u5177\u5e7b\u89c9\uff08\u5728\u906e\u7f69\u533a\u57df\u751f\u6210\u65e0\u5173\u5185\u5bb9\uff09\u548c\u9762\u5177\u5f62\u72b6\u504f\u8f9b\uff08\u6309\u7167\u906e\u7f69\u5f62\u72b6\u751f\u6210\u7269\u4f53\u800c\u975e\u73af\u5883\u5185\u5bb9\uff09", "method": "\u63d0\u51faMask Consistency Regularization (MCR)\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u8bad\u7ec3\u4e2d\u5f15\u5165\u4e24\u79cd\u9762\u5177\u6279\u52a8\uff1a\u6269\u5f20\u548c\u91cd\u5f62\u3002\u6269\u5f20\u9762\u5177\u5e2e\u52a9\u5bf9\u9f50\u6a21\u578b\u8f93\u51fa\u4e0e\u5468\u56f4\u5185\u5bb9\uff0c\u91cd\u5f62\u9762\u5177\u9f13\u52b1\u6a21\u578b\u7a81\u7834\u9762\u5177\u5f62\u72b6\u504f\u8f9b", "result": "\u5b9e\u9a8c\u8868\u660eMCR\u663e\u8457\u51cf\u5c11\u4e86\u5e7b\u89c9\u73b0\u8c61\u548c\u9762\u5177\u5f62\u72b6\u504f\u8f9b\uff0c\u63d0\u5347\u4e86\u7269\u4f53\u79fb\u9664\u7684\u6027\u80fd", "conclusion": "MCR\u80fd\u591f\u751f\u6210\u66f4\u7a33\u5065\u548c\u4e0a\u4e0b\u6587\u4e00\u81f4\u7684\u586b\u5145\u7ed3\u679c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f53\u524d\u56fe\u50cf\u586b\u5145\u6a21\u578b\u5728\u7269\u4f53\u79fb\u9664\u4efb\u52a1\u4e2d\u7684\u5173\u952e\u6311\u6218"}}
{"id": "2509.10260", "pdf": "https://arxiv.org/pdf/2509.10260", "abs": "https://arxiv.org/abs/2509.10260", "authors": ["Jia Wang", "Jie Hu", "Xiaoqi Ma", "Hanghang Ma", "Yanbing Zeng", "Xiaoming Wei"], "title": "MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image (T2I) generation has achieved remarkable progress in instruction following and aesthetics. However, a persistent challenge is the prevalence of physical artifacts, such as anatomical and structural flaws, which severely degrade perceptual quality and limit application. Given the diversity and complexity of these artifacts, a systematic and fine-grained evaluation framework is required, which is lacking in current benchmarks. To fill this gap, we introduce MagicMirror, a comprehensive framework for artifacts assessment. We first establish a detailed taxonomy of generated image artifacts. Guided by this taxonomy, we manually annotate MagicData340K, the first human-annotated large-scale dataset of 340K generated images with fine-grained artifact labels. Building on this dataset, we train MagicAssessor, a Vision-Language Model (VLM) that provides detailed assessments and corresponding labels. To overcome challenges like class imbalance and reward hacking, we design a novel data sampling strategy and a multi-level reward system for Group Relative Policy Optimization (GRPO). Finally, we leverage MagicAssessor to construct MagicBench, an automated benchmark for evaluating the image artifacts of current T2I models. Our evaluation with MagicBench reveals that despite their widespread adoption, even top-tier models like GPT-image-1 are consistently plagued by significant artifacts, highlighting artifact reduction as a critical frontier for future T2I development. Project page: https://wj-inf.github.io/MagicMirror-page/.", "AI": {"tldr": "MagicMirror\u662f\u4e00\u4e2a\u5168\u9762\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4f2a\u5f71\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u9996\u4e2a\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6MagicData340K\u3001\u57fa\u4e8eVLM\u7684\u8bc4\u4f30\u6a21\u578bMagicAssessor\uff0c\u4ee5\u53ca\u81ea\u52a8\u5316\u57fa\u51c6\u6d4b\u8bd5MagicBench\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u9876\u7ea7T2I\u6a21\u578b\u4ecd\u5b58\u5728\u4e25\u91cd\u4f2a\u5f71\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u867d\u7136\u5728\u6307\u4ee4\u8ddf\u968f\u548c\u7f8e\u5b66\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u666e\u904d\u5b58\u5728\u89e3\u5256\u5b66\u548c\u7ed3\u6784\u7f3a\u9677\u7b49\u7269\u7406\u4f2a\u5f71\uff0c\u4e25\u91cd\u5f71\u54cd\u4e86\u611f\u77e5\u8d28\u91cf\u5e76\u9650\u5236\u4e86\u5e94\u7528\u3002\u73b0\u6709\u57fa\u51c6\u7f3a\u4e4f\u7cfb\u7edf\u6027\u548c\u7ec6\u7c92\u5ea6\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "1) \u5efa\u7acb\u8be6\u7ec6\u7684\u751f\u6210\u56fe\u50cf\u4f2a\u5f71\u5206\u7c7b\u6cd5\uff1b2) \u4eba\u5de5\u6807\u6ce8340K\u56fe\u50cf\u6570\u636e\u96c6MagicData340K\uff1b3) \u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578bMagicAssessor\u8fdb\u884c\u8be6\u7ec6\u8bc4\u4f30\uff1b4) \u8bbe\u8ba1\u65b0\u9896\u7684\u6570\u636e\u91c7\u6837\u7b56\u7565\u548c\u591a\u7ea7\u5956\u52b1\u7cfb\u7edf\u7528\u4e8eGRPO\u8bad\u7ec3\uff1b5) \u6784\u5efa\u81ea\u52a8\u5316\u57fa\u51c6\u6d4b\u8bd5MagicBench\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\u5373\u4f7f\u50cfGPT-image-1\u8fd9\u6837\u7684\u9876\u7ea7\u6a21\u578b\u4e5f\u6301\u7eed\u5b58\u5728\u663e\u8457\u4f2a\u5f71\uff0c\u8868\u660e\u4f2a\u5f71\u51cf\u5c11\u662f\u672a\u6765T2I\u53d1\u5c55\u7684\u5173\u952e\u524d\u6cbf\u3002", "conclusion": "MagicMirror\u6846\u67b6\u586b\u8865\u4e86T2I\u751f\u6210\u4f2a\u5f71\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u7cfb\u7edf\u8bc4\u4f30\u548c\u6539\u5584\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u5f3a\u8c03\u4e86\u89e3\u51b3\u4f2a\u5f71\u95ee\u9898\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.10312", "pdf": "https://arxiv.org/pdf/2509.10312", "abs": "https://arxiv.org/abs/2509.10312", "authors": ["Zhixin Zheng", "Xinyu Wang", "Chang Zou", "Shaobo Wang", "Linfeng Zhang"], "title": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching", "categories": ["cs.CV"], "comment": "11 pages, 11 figures; Accepted by ACM MM2025; Mainly focus on feature   caching for diffusion transformers acceleration", "summary": "Diffusion transformers have gained significant attention in recent years for their ability to generate high-quality images and videos, yet still suffer from a huge computational cost due to their iterative denoising process. Recently, feature caching has been introduced to accelerate diffusion transformers by caching the feature computation in previous timesteps and reusing it in the following timesteps, which leverage the temporal similarity of diffusion models while ignoring the similarity in the spatial dimension. In this paper, we introduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and complementary perspective for previous feature caching. Specifically, ClusCa performs spatial clustering on tokens in each timestep, computes only one token in each cluster and propagates their information to all the other tokens, which is able to reduce the number of tokens by over 90%. Extensive experiments on DiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image and text-to-video generation. Besides, it can be directly applied to any diffusion transformer without requirements for training. For instance, ClusCa achieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing the original model by 0.51%. The code is available at https://github.com/Shenyi-Z/Cache4Diffusion.", "AI": {"tldr": "ClusCa\u901a\u8fc7\u7a7a\u95f4\u805a\u7c7b\u51cf\u5c11\u6269\u6563\u53d8\u6362\u5668\u7684token\u6570\u91cf\uff0c\u5b9e\u73b04.96\u500d\u52a0\u901f\uff0c\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf", "motivation": "\u73b0\u6709\u7279\u5f81\u7f13\u5b58\u65b9\u6cd5\u53ea\u5229\u7528\u65f6\u95f4\u7ef4\u5ea6\u76f8\u4f3c\u6027\uff0c\u5ffd\u7565\u4e86\u7a7a\u95f4\u7ef4\u5ea6\u7684\u76f8\u4f3c\u6027\uff0c\u8ba1\u7b97\u6210\u672c\u4ecd\u7136\u5f88\u9ad8", "method": "\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u5bf9token\u8fdb\u884c\u7a7a\u95f4\u805a\u7c7b\uff0c\u6bcf\u4e2a\u805a\u7c7b\u53ea\u8ba1\u7b97\u4e00\u4e2atoken\u5e76\u5c06\u5176\u4fe1\u606f\u4f20\u64ad\u7ed9\u5176\u4ed6token\uff0c\u51cf\u5c1190%\u4ee5\u4e0atoken\u6570\u91cf", "result": "\u5728DiT\u3001FLUX\u548cHunyuanVideo\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0cFLUX\u52a0\u901f4.96\u500d\uff0cImageReward\u8fbe\u523099.49%\uff0c\u6bd4\u539f\u59cb\u6a21\u578b\u63d0\u53470.51%", "conclusion": "ClusCa\u662f\u73b0\u6709\u7279\u5f81\u7f13\u5b58\u65b9\u6cd5\u7684\u6b63\u4ea4\u8865\u5145\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u4efb\u4f55\u6269\u6563\u53d8\u6362\u5668\uff0c\u663e\u8457\u52a0\u901f\u63a8\u7406\u8fc7\u7a0b"}}
{"id": "2509.10334", "pdf": "https://arxiv.org/pdf/2509.10334", "abs": "https://arxiv.org/abs/2509.10334", "authors": ["Jordan Sassoon", "Michal Szczepanski", "Martyna Poreba"], "title": "I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Vision Transformers (ViTs) have recently achieved strong results in semantic segmentation, yet their deployment on resource-constrained devices remains limited due to their high memory footprint and computational cost. Quantization offers an effective strategy to improve efficiency, but ViT-based segmentation models are notoriously fragile under low precision, as quantization errors accumulate across deep encoder-decoder pipelines. We introduce I-Segmenter, the first fully integer-only ViT segmentation framework. Building on the Segmenter architecture, I-Segmenter systematically replaces floating-point operations with integer-only counterparts. To further stabilize both training and inference, we propose $\\lambda$-ShiftGELU, a novel activation function that mitigates the limitations of uniform quantization in handling long-tailed activation distributions. In addition, we remove the L2 normalization layer and replace bilinear interpolation in the decoder with nearest neighbor upsampling, ensuring integer-only execution throughout the computational graph. Extensive experiments show that I-Segmenter achieves accuracy within a reasonable margin of its FP32 baseline (5.1 % on average), while reducing model size by up to 3.8x and enabling up to 1.2x faster inference with optimized runtimes. Notably, even in one-shot PTQ with a single calibration image, I-Segmenter delivers competitive accuracy, underscoring its practicality for real-world deployment.", "AI": {"tldr": "I-Segmenter\u662f\u9996\u4e2a\u5b8c\u5168\u6574\u6570\u5316\u7684ViT\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u66ff\u6362\u6d6e\u70b9\u8fd0\u7b97\u3001\u63d0\u51fa\u03bb-ShiftGELU\u6fc0\u6d3b\u51fd\u6570\u7b49\u6280\u672f\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "motivation": "Vision Transformers\u5728\u8bed\u4e49\u5206\u5272\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u53d7\u9650\uff0c\u91cf\u5316\u6280\u672f\u867d\u7136\u80fd\u63d0\u9ad8\u6548\u7387\uff0c\u4f46ViT\u5206\u5272\u6a21\u578b\u5728\u4f4e\u7cbe\u5ea6\u4e0b\u8868\u73b0\u8106\u5f31\u3002", "method": "\u57fa\u4e8eSegmenter\u67b6\u6784\uff0c\u7cfb\u7edf\u66ff\u6362\u6d6e\u70b9\u8fd0\u7b97\u4e3a\u6574\u6570\u8fd0\u7b97\uff1b\u63d0\u51fa\u03bb-ShiftGELU\u6fc0\u6d3b\u51fd\u6570\u5904\u7406\u957f\u5c3e\u5206\u5e03\uff1b\u79fb\u9664L2\u5f52\u4e00\u5316\u5c42\uff1b\u7528\u6700\u8fd1\u90bb\u4e0a\u91c7\u6837\u66ff\u6362\u53cc\u7ebf\u6027\u63d2\u503c\u3002", "result": "\u5728\u7cbe\u5ea6\u635f\u5931\u5e73\u57475.1%\u7684\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u5927\u5c0f\u51cf\u5c113.8\u500d\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53471.2\u500d\uff1b\u5355\u56fe\u50cfPTQ\u4e5f\u80fd\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "I-Segmenter\u4e3aViT\u5206\u5272\u6a21\u578b\u7684\u8d44\u6e90\u53d7\u9650\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6548\u7387\u548c\u7cbe\u5ea6\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2509.10341", "pdf": "https://arxiv.org/pdf/2509.10341", "abs": "https://arxiv.org/abs/2509.10341", "authors": ["Botond Fazekas", "Thomas Pinetz", "Guilherme Aresta", "Taha Emre", "Hrvoje Bogunovic"], "title": "GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT", "categories": ["cs.CV"], "comment": null, "summary": "Optical Coherence Tomography (OCT) is a vital imaging modality for diagnosing and monitoring retinal diseases. However, OCT images are inherently degraded by speckle noise, which obscures fine details and hinders accurate interpretation. While numerous denoising methods exist, many struggle to balance noise reduction with the preservation of crucial anatomical structures. This paper introduces GARD (Gamma-based Anatomical Restoration and Denoising), a novel deep learning approach for OCT image despeckling that leverages the strengths of diffusion probabilistic models. Unlike conventional diffusion models that assume Gaussian noise, GARD employs a Denoising Diffusion Gamma Model to more accurately reflect the statistical properties of speckle. Furthermore, we introduce a Noise-Reduced Fidelity Term that utilizes a pre-processed, less-noisy image to guide the denoising process. This crucial addition prevents the reintroduction of high-frequency noise. We accelerate the inference process by adapting the Denoising Diffusion Implicit Model framework to our Gamma-based model. Experiments on a dataset with paired noisy and less-noisy OCT B-scans demonstrate that GARD significantly outperforms traditional denoising methods and state-of-the-art deep learning models in terms of PSNR, SSIM, and MSE. Qualitative results confirm that GARD produces sharper edges and better preserves fine anatomical details.", "AI": {"tldr": "GARD\u662f\u4e00\u79cd\u57fa\u4e8e\u4f3d\u9a6c\u6269\u6563\u6a21\u578b\u7684OCT\u56fe\u50cf\u53bb\u566a\u65b9\u6cd5\uff0c\u901a\u8fc7\u566a\u58f0\u964d\u4f4e\u4fdd\u771f\u9879\u548c\u52a0\u901f\u63a8\u7406\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u89e3\u5256\u7ed3\u6784\u7ec6\u8282\u7684\u540c\u65f6\u6709\u6548\u53bb\u9664\u6563\u6591\u566a\u58f0", "motivation": "OCT\u56fe\u50cf\u53d7\u6563\u6591\u566a\u58f0\u5f71\u54cd\u4e25\u91cd\uff0c\u73b0\u6709\u53bb\u566a\u65b9\u6cd5\u96be\u4ee5\u5728\u566a\u58f0\u53bb\u9664\u548c\u7ed3\u6784\u4fdd\u6301\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u9700\u8981\u66f4\u51c6\u786e\u7684\u566a\u58f0\u7edf\u8ba1\u6a21\u578b\u548c\u66f4\u597d\u7684\u53bb\u566a\u6307\u5bfc\u673a\u5236", "method": "\u63d0\u51faGARD\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u53bb\u566a\u6269\u6563\u4f3d\u9a6c\u6a21\u578b\u66ff\u4ee3\u4f20\u7edf\u9ad8\u65af\u6a21\u578b\uff0c\u66f4\u51c6\u786e\u53cd\u6620\u6563\u6591\u566a\u58f0\u7edf\u8ba1\u7279\u6027\uff1b2) \u5f15\u5165\u566a\u58f0\u964d\u4f4e\u4fdd\u771f\u9879\uff0c\u5229\u7528\u9884\u5904\u7406\u4f4e\u566a\u58f0\u56fe\u50cf\u6307\u5bfc\u53bb\u566a\u8fc7\u7a0b\uff1b3) \u91c7\u7528\u53bb\u566a\u6269\u6563\u9690\u5f0f\u6a21\u578b\u6846\u67b6\u52a0\u901f\u63a8\u7406", "result": "\u5728\u914d\u5bf9\u566a\u58f0-\u4f4e\u566a\u58f0OCT B\u626b\u63cf\u6570\u636e\u96c6\u4e0a\uff0cGARD\u5728PSNR\u3001SSIM\u548cMSE\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5b9a\u6027\u7ed3\u679c\u663e\u793a\u8fb9\u7f18\u66f4\u6e05\u6670\u3001\u89e3\u5256\u7ec6\u8282\u4fdd\u6301\u66f4\u597d", "conclusion": "GARD\u901a\u8fc7\u4f3d\u9a6c\u6269\u6563\u6a21\u578b\u548c\u566a\u58f0\u964d\u4f4e\u4fdd\u771f\u9879\uff0c\u6210\u529f\u89e3\u51b3\u4e86OCT\u56fe\u50cf\u53bb\u566a\u4e2d\u566a\u58f0\u53bb\u9664\u4e0e\u7ed3\u6784\u4fdd\u6301\u7684\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.10359", "pdf": "https://arxiv.org/pdf/2509.10359", "abs": "https://arxiv.org/abs/2509.10359", "authors": ["Matteo Trippodo", "Federico Becattini", "Lorenzo Seidenari"], "title": "Immunizing Images from Text to Image Editing via Adversarial Cross-Attention", "categories": ["cs.CV"], "comment": "Accepted as Regular Paper at ACM Multimedia 2025", "summary": "Recent advances in text-based image editing have enabled fine-grained manipulation of visual content guided by natural language. However, such methods are susceptible to adversarial attacks. In this work, we propose a novel attack that targets the visual component of editing methods. We introduce Attention Attack, which disrupts the cross-attention between a textual prompt and the visual representation of the image by using an automatically generated caption of the source image as a proxy for the edit prompt. This breaks the alignment between the contents of the image and their textual description, without requiring knowledge of the editing method or the editing prompt. Reflecting on the reliability of existing metrics for immunization success, we propose two novel evaluation strategies: Caption Similarity, which quantifies semantic consistency between original and adversarial edits, and semantic Intersection over Union (IoU), which measures spatial layout disruption via segmentation masks. Experiments conducted on the TEDBench++ benchmark demonstrate that our attack significantly degrades editing performance while remaining imperceptible.", "AI": {"tldr": "\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u7684\u56fe\u7247\u63cf\u8ff0\u4f5c\u4e3a\u4ee3\u7406\u63d0\u793a\uff0cAttention Attack\u653b\u51fb\u7a81\u7834\u6587\u672c\u63d0\u793a\u4e0e\u56fe\u50cf\u8868\u5f81\u4e4b\u95f4\u7684\u8de8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e72\u6270\u6587\u672c\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5bb9\u6613\u53d7\u5230\u5bf9\u51b2\u653b\u51fb\u7684\u5f71\u54cd\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u653b\u51fb\u65b9\u6cd5\u6765\u6d4b\u8bd5\u5e76\u63d0\u9ad8\u8fd9\u4e9b\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002", "method": "\u4f7f\u7528\u81ea\u52a8\u751f\u6210\u7684\u6e90\u56fe\u7247\u63cf\u8ff0\u4f5c\u4e3a\u4ee3\u7406\u63d0\u793a\uff0c\u7a81\u7834\u6587\u672c\u63d0\u793a\u4e0e\u56fe\u50cf\u8868\u5f81\u4e4b\u95f4\u7684\u8de8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u65e0\u9700\u77e5\u9053\u7f16\u8f91\u65b9\u6cd5\u6216\u7f16\u8f91\u63d0\u793a\u3002", "result": "\u5728TEDBench++\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u653b\u51fb\u663e\u8457\u964d\u4f4e\u4e86\u7f16\u8f91\u6027\u80fd\u4f46\u4fdd\u6301\u4e86\u4e0d\u53ef\u5bdf\u89c9\u6027\u3002", "conclusion": "Attention Attack\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5bf9\u51b2\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u7b56\u7565\u6765\u91cf\u5316\u653b\u51fb\u6548\u679c\u3002"}}
{"id": "2509.10441", "pdf": "https://arxiv.org/pdf/2509.10441", "abs": "https://arxiv.org/abs/2509.10441", "authors": ["Tao Han", "Wanghan Xu", "Junchao Gong", "Xiaoyu Yue", "Song Guo", "Luping Zhou", "Lei Bai"], "title": "InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Arbitrary resolution image generation provides a consistent visual experience across devices, having extensive applications for producers and consumers. Current diffusion models increase computational demand quadratically with resolution, causing 4K image generation delays over 100 seconds. To solve this, we explore the second generation upon the latent diffusion models, where the fixed latent generated by diffusion models is regarded as the content representation and we propose to decode arbitrary resolution images with a compact generated latent using a one-step generator. Thus, we present the \\textbf{InfGen}, replacing the VAE decoder with the new generator, for generating images at any resolution from a fixed-size latent without retraining the diffusion models, which simplifies the process, reducing computational complexity and can be applied to any model using the same latent space. Experiments show InfGen is capable of improving many models into the arbitrary high-resolution era while cutting 4K image generation time to under 10 seconds.", "AI": {"tldr": "InfGen\u662f\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u7b2c\u4e8c\u4ee3\u65b9\u6cd5\uff0c\u901a\u8fc7\u7528\u4e00\u6b65\u751f\u6210\u5668\u66ff\u6362VAE\u89e3\u7801\u5668\uff0c\u5b9e\u73b0\u4ece\u56fa\u5b9a\u5927\u5c0f\u6f5c\u5728\u8868\u793a\u751f\u6210\u4efb\u610f\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6", "motivation": "\u89e3\u51b3\u5f53\u524d\u6269\u6563\u6a21\u578b\u968f\u5206\u8fa8\u7387\u589e\u52a0\u8ba1\u7b97\u9700\u6c42\u5448\u4e8c\u6b21\u589e\u957f\u7684\u95ee\u9898\uff0c\u5b9e\u73b04K\u56fe\u50cf\u751f\u6210\u7684\u5feb\u901f\u9ad8\u6548\u5904\u7406", "method": "\u5c06\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u56fa\u5b9a\u6f5c\u5728\u8868\u793a\u4f5c\u4e3a\u5185\u5bb9\u8868\u793a\uff0c\u4f7f\u7528\u7d27\u51d1\u7684\u4e00\u6b65\u751f\u6210\u5668\u89e3\u7801\u4efb\u610f\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u66ff\u6362\u539f\u6709\u7684VAE\u89e3\u7801\u5668", "result": "\u5c064K\u56fe\u50cf\u751f\u6210\u65f6\u95f4\u4ece100\u591a\u79d2\u51cf\u5c11\u523010\u79d2\u4ee5\u5185\uff0c\u80fd\u591f\u5c06\u591a\u79cd\u6a21\u578b\u5347\u7ea7\u5230\u4efb\u610f\u9ad8\u5206\u8fa8\u7387\u65f6\u4ee3", "conclusion": "InfGen\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5316\u6d41\u7a0b\u3001\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u5e94\u7528\u4e8e\u4f7f\u7528\u76f8\u540c\u6f5c\u5728\u7a7a\u95f4\u7684\u4efb\u4f55\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u4efb\u610f\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210"}}
{"id": "2509.09880", "pdf": "https://arxiv.org/pdf/2509.09880", "abs": "https://arxiv.org/abs/2509.09880", "authors": ["Ya\u015far Utku Al\u00e7alar", "Junno Yun", "Mehmet Ak\u00e7akaya"], "title": "Automated Tuning for Diffusion Inverse Problem Solvers without Generative Prior Retraining", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "physics.med-ph"], "comment": "IEEE International Workshop on Computational Advances in Multi-Sensor   Adaptive Processing (CAMSAP), 2025", "summary": "Diffusion/score-based models have recently emerged as powerful generative priors for solving inverse problems, including accelerated MRI reconstruction. While their flexibility allows decoupling the measurement model from the learned prior, their performance heavily depends on carefully tuned data fidelity weights, especially under fast sampling schedules with few denoising steps. Existing approaches often rely on heuristics or fixed weights, which fail to generalize across varying measurement conditions and irregular timestep schedules. In this work, we propose Zero-shot Adaptive Diffusion Sampling (ZADS), a test-time optimization method that adaptively tunes fidelity weights across arbitrary noise schedules without requiring retraining of the diffusion prior. ZADS treats the denoising process as a fixed unrolled sampler and optimizes fidelity weights in a self-supervised manner using only undersampled measurements. Experiments on the fastMRI knee dataset demonstrate that ZADS consistently outperforms both traditional compressed sensing and recent diffusion-based methods, showcasing its ability to deliver high-fidelity reconstructions across varying noise schedules and acquisition settings.", "AI": {"tldr": "ZADS\u662f\u4e00\u79cd\u96f6\u6837\u672c\u81ea\u9002\u5e94\u6269\u6563\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u8bd5\u65f6\u4f18\u5316\u81ea\u9002\u5e94\u8c03\u6574\u4fdd\u771f\u5ea6\u6743\u91cd\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6269\u6563\u5148\u9a8c\uff0c\u5728\u52a0\u901fMRI\u91cd\u5efa\u4e2d\u4f18\u4e8e\u4f20\u7edf\u538b\u7f29\u611f\u77e5\u548c\u73b0\u6709\u6269\u6563\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u6a21\u578b\u5728\u89e3\u51b3\u9006\u95ee\u9898\u65f6\u4e25\u91cd\u4f9d\u8d56\u7cbe\u5fc3\u8c03\u6574\u7684\u6570\u636e\u4fdd\u771f\u5ea6\u6743\u91cd\uff0c\u7279\u522b\u662f\u5728\u5feb\u901f\u91c7\u6837\u8ba1\u5212\u4e2d\uff0c\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u4f9d\u8d56\u542f\u53d1\u5f0f\u6216\u56fa\u5b9a\u6743\u91cd\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u7684\u6d4b\u91cf\u6761\u4ef6\u548c\u4e0d\u89c4\u5219\u65f6\u95f4\u6b65\u8ba1\u5212\u3002", "method": "\u63d0\u51faZero-shot Adaptive Diffusion Sampling (ZADS)\uff0c\u5c06\u53bb\u566a\u8fc7\u7a0b\u89c6\u4e3a\u56fa\u5b9a\u7684\u5c55\u5f00\u91c7\u6837\u5668\uff0c\u4ec5\u4f7f\u7528\u6b20\u91c7\u6837\u6d4b\u91cf\u4ee5\u81ea\u76d1\u7763\u65b9\u5f0f\u4f18\u5316\u4fdd\u771f\u5ea6\u6743\u91cd\u3002", "result": "\u5728fastMRI\u819d\u5173\u8282\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cZADS\u59cb\u7ec8\u4f18\u4e8e\u4f20\u7edf\u538b\u7f29\u611f\u77e5\u548c\u6700\u8fd1\u7684\u6269\u6563\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u566a\u58f0\u8ba1\u5212\u548c\u91c7\u96c6\u8bbe\u7f6e\u4e0b\u63d0\u4f9b\u9ad8\u4fdd\u771f\u91cd\u5efa\u3002", "conclusion": "ZADS\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u81ea\u9002\u5e94\u8c03\u6574\u6269\u6563\u91c7\u6837\u4e2d\u7684\u4fdd\u771f\u5ea6\u6743\u91cd\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u5728\u5404\u79cd\u6761\u4ef6\u4e0b\u90fd\u80fd\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u91cd\u5efa\u6027\u80fd\u3002"}}
{"id": "2509.10096", "pdf": "https://arxiv.org/pdf/2509.10096", "abs": "https://arxiv.org/abs/2509.10096", "authors": ["Saeed Saadatnejad", "Reyhaneh Hosseininejad", "Jose Barreiros", "Katherine M. Tsui", "Alexandre Alahi"], "title": "HHI-Assist: A Dataset and Benchmark of Human-Human Interaction in Physical Assistance Scenario", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to RA-L 2025", "summary": "The increasing labor shortage and aging population underline the need for assistive robots to support human care recipients. To enable safe and responsive assistance, robots require accurate human motion prediction in physical interaction scenarios. However, this remains a challenging task due to the variability of assistive settings and the complexity of coupled dynamics in physical interactions. In this work, we address these challenges through two key contributions: (1) HHI-Assist, a dataset comprising motion capture clips of human-human interactions in assistive tasks; and (2) a conditional Transformer-based denoising diffusion model for predicting the poses of interacting agents. Our model effectively captures the coupled dynamics between caregivers and care receivers, demonstrating improvements over baselines and strong generalization to unseen scenarios. By advancing interaction-aware motion prediction and introducing a new dataset, our work has the potential to significantly enhance robotic assistance policies. The dataset and code are available at: https://sites.google.com/view/hhi-assist/home", "AI": {"tldr": "\u63d0\u51fa\u4e86HHI-Assist\u6570\u636e\u96c6\u548c\u57fa\u4e8e\u6761\u4ef6Transformer\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u7269\u7406\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u4eba\u4f53\u8fd0\u52a8\uff0c\u63d0\u5347\u8f85\u52a9\u673a\u5668\u4eba\u7684\u5b89\u5168\u6027\u548c\u54cd\u5e94\u80fd\u529b", "motivation": "\u52b3\u52a8\u529b\u77ed\u7f3a\u548c\u4eba\u53e3\u8001\u9f84\u5316\u9700\u8981\u8f85\u52a9\u673a\u5668\u4eba\uff0c\u4f46\u673a\u5668\u4eba\u9700\u8981\u51c6\u786e\u7684\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\u6765\u786e\u4fdd\u5b89\u5168\u4ea4\u4e92\uff0c\u8fd9\u5728\u7269\u7406\u4ea4\u4e92\u573a\u666f\u4e2d\u7531\u4e8e\u73af\u5883\u591a\u53d8\u6027\u548c\u8026\u5408\u52a8\u529b\u5b66\u590d\u6742\u6027\u800c\u5177\u6709\u6311\u6218\u6027", "method": "\u5f00\u53d1\u4e86HHI-Assist\u6570\u636e\u96c6\uff08\u5305\u542b\u4eba-\u4eba\u8f85\u52a9\u4ea4\u4e92\u7684\u8fd0\u52a8\u6355\u6349\u6570\u636e\uff09\u548c\u57fa\u4e8e\u6761\u4ef6Transformer\u7684\u53bb\u566a\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u4ea4\u4e92\u667a\u80fd\u4f53\u7684\u59ff\u6001", "result": "\u6a21\u578b\u6709\u6548\u6355\u6349\u4e86\u62a4\u7406\u8005\u548c\u88ab\u62a4\u7406\u8005\u4e4b\u95f4\u7684\u8026\u5408\u52a8\u529b\u5b66\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6709\u6539\u8fdb\uff0c\u5e76\u5728\u672a\u89c1\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b", "conclusion": "\u901a\u8fc7\u63a8\u8fdb\u4ea4\u4e92\u611f\u77e5\u7684\u8fd0\u52a8\u9884\u6d4b\u548c\u5f15\u5165\u65b0\u6570\u636e\u96c6\uff0c\u8fd9\u9879\u5de5\u4f5c\u6709\u6f5c\u529b\u663e\u8457\u589e\u5f3a\u673a\u5668\u4eba\u8f85\u52a9\u7b56\u7565\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2509.10098", "pdf": "https://arxiv.org/pdf/2509.10098", "abs": "https://arxiv.org/abs/2509.10098", "authors": ["Muhamad Daniel Ariff Bin Abdul Rahman", "Yusuke Monno", "Masayuki Tanaka", "Masatoshi Okutomi"], "title": "Polarization Denoising and Demosaicking: Dataset and Baseline Method", "categories": ["eess.IV", "cs.CV"], "comment": "Published in ICIP2025; Project page:   http://www.ok.sc.e.titech.ac.jp/res/PolarDem/PDD.html", "summary": "A division-of-focal-plane (DoFP) polarimeter enables us to acquire images with multiple polarization orientations in one shot and thus it is valuable for many applications using polarimetric information. The image processing pipeline for a DoFP polarimeter entails two crucial tasks: denoising and demosaicking. While polarization demosaicking for a noise-free case has increasingly been studied, the research for the joint task of polarization denoising and demosaicking is scarce due to the lack of a suitable evaluation dataset and a solid baseline method. In this paper, we propose a novel dataset and method for polarization denoising and demosaicking. Our dataset contains 40 real-world scenes and three noise-level conditions, consisting of pairs of noisy mosaic inputs and noise-free full images. Our method takes a denoising-then-demosaicking approach based on well-accepted signal processing components to offer a reproducible method. Experimental results demonstrate that our method exhibits higher image reconstruction performance than other alternative methods, offering a solid baseline.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u504f\u632f\u5149\u5668\u7684\u566a\u58f0\u9664\u53bb\u548c\u9a6c\u8d5b\u514b\u91cd\u6784\u7684\u65b0\u65b9\u6cd5\u548c\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5148\u9664\u566a\u540e\u91cd\u6784\u7684\u65b9\u5f0f\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u56fe\u50cf\u6062\u590d\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u9002\u5f53\u7684\u8bc4\u4f30\u6570\u636e\u96c6\u548c\u575a\u5b9e\u7684\u57fa\u51c6\u65b9\u6cd5\uff0c\u5bf9\u4e8e\u504f\u632f\u5149\u5668\u7684\u566a\u58f0\u9664\u53bb\u548c\u9a6c\u8d5b\u514b\u91cd\u6784\u8054\u5408\u4efb\u52a1\u7684\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u91c7\u7528\u5148\u9664\u566a\u540e\u91cd\u6784\u7684\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5e7f\u6cdb\u63a5\u53d7\u7684\u4fe1\u53f7\u5904\u7406\u7ec4\u4ef6\u6765\u63d0\u4f9b\u53ef\u590d\u73b0\u7684\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u91cd\u6784\u6027\u80fd\u65b9\u9762\u663e\u793a\u51fa\u6bd4\u5176\u4ed6\u66ff\u4ee3\u65b9\u6cd5\u66f4\u9ad8\u7684\u8868\u73b0\uff0c\u4e3a\u8be5\u9886\u57df\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u57fa\u51c6\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u7684\u65b0\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u504f\u632f\u5149\u5668\u566a\u58f0\u9664\u53bb\u548c\u9a6c\u8d5b\u514b\u91cd\u6784\u7684\u5173\u952e\u95ee\u9898\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6491\u3002"}}
