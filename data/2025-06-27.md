<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 3]
- [cs.CV](#cs.CV) [Total: 33]
- [cs.RO](#cs.RO) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [3DGH: 3D Head Generation with Composable Hair and Face](https://arxiv.org/abs/2506.20875)
*Chengan He,Junxuan Li,Tobias Kirschstein,Artem Sevastopolsky,Shunsuke Saito,Qingyang Tan,Javier Romero,Chen Cao,Holly Rushmeier,Giljoo Nam*

Main category: cs.GR

TL;DR: 3DGH是一种无条件生成3D人头模型的方法，支持可组合的头发和面部组件，通过分离建模和双生成器架构实现高效合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在头发和面部的建模上存在耦合问题，限制了灵活性和编辑能力。

Method: 采用基于模板的3D高斯散射表示和双生成器架构，结合交叉注意力机制建模头发与面部的关联。

Result: 实验表明3DGH在无条件全头图像合成和可组合3D发型编辑上优于现有方法。

Conclusion: 3DGH通过分离建模和高效架构，实现了高质量的3D人头生成和编辑。

Abstract: We present 3DGH, an unconditional generative model for 3D human heads with composable hair and face components. Unlike previous work that entangles the modeling of hair and face, we propose to separate them using a novel data representation with template-based 3D Gaussian Splatting, in which deformable hair geometry is introduced to capture the geometric variations across different hairstyles. Based on this data representation, we design a 3D GAN-based architecture with dual generators and employ a cross-attention mechanism to model the inherent correlation between hair and face. The model is trained on synthetic renderings using carefully designed objectives to stabilize training and facilitate hair-face separation. We conduct extensive experiments to validate the design choice of 3DGH, and evaluate it both qualitatively and quantitatively by comparing with several state-of-the-art 3D GAN methods, demonstrating its effectiveness in unconditional full-head image synthesis and composable 3D hairstyle editing. More details will be available on our project page: https://c-he.github.io/projects/3dgh/.

</details>


### [2] [Consistent Zero-shot 3D Texture Synthesis Using Geometry-aware Diffusion and Temporal Video Models](https://arxiv.org/abs/2506.20946)
*Donggoo Kang,Jangyeong Kim,Dasol Jeong,Junyoung Choi,Jeonga Wi,Hyunmin Lee,Joonho Gwon,Joonki Paik*

Main category: cs.GR

TL;DR: VideoTex利用视频生成模型解决3D纹理合成中的时空不一致问题，通过几何感知条件和UV扩散策略，实现高质量、稳定的纹理生成。


<details>
  <summary>Details</summary>
Motivation: 现有纹理合成方法因缺乏全局上下文和几何理解，导致不一致性。视频生成模型的成功启发利用其解决纹理合成的时空问题。

Method: 结合几何感知条件，利用3D网格结构；提出结构化的UV扩散策略，增强遮挡区域的生成。

Result: VideoTex在纹理保真度、接缝融合和稳定性上优于现有方法，实现高质量、时间稳定的纹理。

Conclusion: VideoTex为动态实时应用提供了视觉质量和时间一致性的解决方案。

Abstract: Current texture synthesis methods, which generate textures from fixed viewpoints, suffer from inconsistencies due to the lack of global context and geometric understanding. Meanwhile, recent advancements in video generation models have demonstrated remarkable success in achieving temporally consistent videos. In this paper, we introduce VideoTex, a novel framework for seamless texture synthesis that leverages video generation models to address both spatial and temporal inconsistencies in 3D textures. Our approach incorporates geometry-aware conditions, enabling precise utilization of 3D mesh structures. Additionally, we propose a structure-wise UV diffusion strategy, which enhances the generation of occluded areas by preserving semantic information, resulting in smoother and more coherent textures. VideoTex not only achieves smoother transitions across UV boundaries but also ensures high-quality, temporally stable textures across video frames. Extensive experiments demonstrate that VideoTex outperforms existing methods in texture fidelity, seam blending, and stability, paving the way for dynamic real-time applications that demand both visual quality and temporal coherence.

</details>


### [3] [FairyGen: Storied Cartoon Video from a Single Child-Drawn Character](https://arxiv.org/abs/2506.21272)
*Jiayi Zheng,Xiaodong Cun*

Main category: cs.GR

TL;DR: FairyGen是一个自动系统，能从单张儿童绘画生成故事驱动的卡通视频，并保留其独特的艺术风格。


<details>
  <summary>Details</summary>
Motivation: 传统方法主要关注角色一致性和基本动作，而FairyGen旨在通过分离角色建模与风格化背景生成，并结合电影镜头设计，实现更具表现力和连贯性的故事讲述。

Method: 系统首先使用MLLM生成结构化故事板，然后通过风格传播适配器确保视觉一致性，再通过镜头设计模块增强视觉多样性和电影质量。角色动画通过3D代理重建和MMDiT模型实现，并结合两阶段运动定制适配器。

Result: 实验表明，FairyGen生成的动画风格忠实、叙事结构自然，具有个性化和吸引力的故事动画潜力。

Conclusion: FairyGen为个性化故事动画提供了一种高效且风格一致的方法，代码将开源。

Abstract: We propose FairyGen, an automatic system for generating story-driven cartoon videos from a single child's drawing, while faithfully preserving its unique artistic style. Unlike previous storytelling methods that primarily focus on character consistency and basic motion, FairyGen explicitly disentangles character modeling from stylized background generation and incorporates cinematic shot design to support expressive and coherent storytelling. Given a single character sketch, we first employ an MLLM to generate a structured storyboard with shot-level descriptions that specify environment settings, character actions, and camera perspectives. To ensure visual consistency, we introduce a style propagation adapter that captures the character's visual style and applies it to the background, faithfully retaining the character's full visual identity while synthesizing style-consistent scenes. A shot design module further enhances visual diversity and cinematic quality through frame cropping and multi-view synthesis based on the storyboard. To animate the story, we reconstruct a 3D proxy of the character to derive physically plausible motion sequences, which are then used to fine-tune an MMDiT-based image-to-video diffusion model. We further propose a two-stage motion customization adapter: the first stage learns appearance features from temporally unordered frames, disentangling identity from motion; the second stage models temporal dynamics using a timestep-shift strategy with frozen identity weights. Once trained, FairyGen directly renders diverse and coherent video scenes aligned with the storyboard. Extensive experiments demonstrate that our system produces animations that are stylistically faithful, narratively structured natural motion, highlighting its potential for personalized and engaging story animation. The code will be available at https://github.com/GVCLab/FairyGen

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [4] [StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation](https://arxiv.org/abs/2506.20756)
*Haodong Li,Chen Wang,Jiahui Lei,Kostas Daniilidis,Lingjie Liu*

Main category: cs.CV

TL;DR: StereoDiff结合立体匹配和视频深度扩散，分别处理静态和动态区域，实现了视频深度估计的SoTA性能。


<details>
  <summary>Details</summary>
Motivation: 视频深度估计不仅仅是图像深度估计的简单扩展，静态和动态区域的时序一致性需求不同。静态区域可通过立体匹配实现全局3D线索，而动态区域需依赖大规模视频数据学习平滑过渡。

Method: 提出StereoDiff，分两阶段：立体匹配处理静态区域，视频深度扩散处理动态区域，通过频域分析展示二者互补性。

Result: 在零样本、真实世界动态视频深度基准测试中，StereoDiff表现最优，一致性和准确性显著提升。

Conclusion: StereoDiff通过结合立体匹配和视频深度扩散，有效解决了视频深度估计中静态和动态区域的不同需求，实现了高性能。

Abstract: Recent video depth estimation methods achieve great performance by following the paradigm of image depth estimation, i.e., typically fine-tuning pre-trained video diffusion models with massive data. However, we argue that video depth estimation is not a naive extension of image depth estimation. The temporal consistency requirements for dynamic and static regions in videos are fundamentally different. Consistent video depth in static regions, typically backgrounds, can be more effectively achieved via stereo matching across all frames, which provides much stronger global 3D cues. While the consistency for dynamic regions still should be learned from large-scale video depth data to ensure smooth transitions, due to the violation of triangulation constraints. Based on these insights, we introduce StereoDiff, a two-stage video depth estimator that synergizes stereo matching for mainly the static areas with video depth diffusion for maintaining consistent depth transitions in dynamic areas. We mathematically demonstrate how stereo matching and video depth diffusion offer complementary strengths through frequency domain analysis, highlighting the effectiveness of their synergy in capturing the advantages of both. Experimental results on zero-shot, real-world, dynamic video depth benchmarks, both indoor and outdoor, demonstrate StereoDiff's SoTA performance, showcasing its superior consistency and accuracy in video depth estimation.

</details>


### [5] [Leveraging Vision-Language Models to Select Trustworthy Super-Resolution Samples Generated by Diffusion Models](https://arxiv.org/abs/2506.20832)
*Cansu Korkmaz,Ahmet Murat Tekalp,Zafer Dogan*

Main category: cs.CV

TL;DR: 本文提出了一种利用视觉语言模型（VLM）从扩散生成的高分辨率图像中选择最可信样本的自动化框架，并通过混合指标Trustworthiness Score（TWS）评估其可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决超分辨率（SR）问题中传统方法在信息保真度和感知质量之间的权衡不足，以及扩散模型生成多样性样本后选择可信样本的挑战。

Method: 利用VLM（如BLIP-2、GPT-4o）通过结构化查询评估语义正确性、视觉质量和伪影存在，并设计TWS指标（结合CLIP嵌入、SSIM和小波分解）量化可靠性。

Result: 实验表明TWS与人类偏好高度相关，且VLM引导的选择能持续获得高TWS值，优于传统指标如PSNR和LPIPS。

Conclusion: 该方法为扩散SR空间的不确定性提供了可扩展、通用的解决方案，为生成SR的可信性设定了新基准。

Abstract: Super-resolution (SR) is an ill-posed inverse problem with many feasible solutions consistent with a given low-resolution image. On one hand, regressive SR models aim to balance fidelity and perceptual quality to yield a single solution, but this trade-off often introduces artifacts that create ambiguity in information-critical applications such as recognizing digits or letters. On the other hand, diffusion models generate a diverse set of SR images, but selecting the most trustworthy solution from this set remains a challenge. This paper introduces a robust, automated framework for identifying the most trustworthy SR sample from a diffusion-generated set by leveraging the semantic reasoning capabilities of vision-language models (VLMs). Specifically, VLMs such as BLIP-2, GPT-4o, and their variants are prompted with structured queries to assess semantic correctness, visual quality, and artifact presence. The top-ranked SR candidates are then ensembled to yield a single trustworthy output in a cost-effective manner. To rigorously assess the validity of VLM-selected samples, we propose a novel Trustworthiness Score (TWS) a hybrid metric that quantifies SR reliability based on three complementary components: semantic similarity via CLIP embeddings, structural integrity using SSIM on edge maps, and artifact sensitivity through multi-level wavelet decomposition. We empirically show that TWS correlates strongly with human preference in both ambiguous and natural images, and that VLM-guided selections consistently yield high TWS values. Compared to conventional metrics like PSNR, LPIPS, which fail to reflect information fidelity, our approach offers a principled, scalable, and generalizable solution for navigating the uncertainty of the diffusion SR space. By aligning outputs with human expectations and semantic correctness, this work sets a new benchmark for trustworthiness in generative SR.

</details>


### [6] [MultiHuman-Testbench: Benchmarking Image Generation for Multiple Humans](https://arxiv.org/abs/2506.20879)
*Shubhankar Borse,Seokeon Choi,Sunghyun Park,Jeongho Kim,Shreya Kadambi,Risheek Garrepalli,Sungrack Yun,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: 论文提出了MultiHuman-Testbench，一个用于评估多人生成模型的基准测试，包含1800个样本和5550张人脸图像，并提出了多维度评估指标。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门用于评估多人生成模型的基准测试，限制了该领域的研究进展。

Method: 构建包含多样文本提示和人脸图像的基准测试，提出基于分割和匈牙利匹配的新技术，并设计四项关键评估指标。

Result: 基准测试和新技术显著提高了身份相似性，为多人生成模型提供了标准化评估工具。

Conclusion: MultiHuman-Testbench为多人生成研究提供了有价值的见解和标准化工具。

Abstract: Generation of images containing multiple humans, performing complex actions, while preserving their facial identities, is a significant challenge. A major factor contributing to this is the lack of a a dedicated benchmark. To address this, we introduce MultiHuman-Testbench, a novel benchmark for rigorously evaluating generative models for multi-human generation. The benchmark comprises 1800 samples, including carefully curated text prompts, describing a range of simple to complex human actions. These prompts are matched with a total of 5,550 unique human face images, sampled uniformly to ensure diversity across age, ethnic background, and gender. Alongside captions, we provide human-selected pose conditioning images which accurately match the prompt. We propose a multi-faceted evaluation suite employing four key metrics to quantify face count, ID similarity, prompt alignment, and action detection. We conduct a thorough evaluation of a diverse set of models, including zero-shot approaches and training-based methods, with and without regional priors. We also propose novel techniques to incorporate image and region isolation using human segmentation and Hungarian matching, significantly improving ID similarity. Our proposed benchmark and key findings provide valuable insights and a standardized tool for advancing research in multi-human image generation.

</details>


### [7] [M2SFormer: Multi-Spectral and Multi-Scale Attention with Edge-Aware Difficulty Guidance for Image Forgery Localization](https://arxiv.org/abs/2506.20922)
*Ju-Hyeon Nam,Dong-Hyun Moon,Sang-Chul Lee*

Main category: cs.CV

TL;DR: M2SFormer是一种基于Transformer编码器的新型框架，旨在解决图像伪造定位中的计算开销和表示能力不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法在像素级伪造定位中虽准确但计算开销大，且对复杂或细微篡改的表示能力有限。

Method: M2SFormer通过统一多频率和多尺度注意力机制，结合全局先验图和难度引导注意力模块，提升伪造定位效果。

Result: 在多个基准数据集上的实验表明，M2SFormer优于现有最优模型，尤其在跨域伪造检测和定位中表现突出。

Conclusion: M2SFormer通过创新设计显著提升了伪造定位的准确性和泛化能力。

Abstract: Image editing techniques have rapidly advanced, facilitating both innovative use cases and malicious manipulation of digital images. Deep learning-based methods have recently achieved high accuracy in pixel-level forgery localization, yet they frequently struggle with computational overhead and limited representation power, particularly for subtle or complex tampering. In this paper, we propose M2SFormer, a novel Transformer encoder-based framework designed to overcome these challenges. Unlike approaches that process spatial and frequency cues separately, M2SFormer unifies multi-frequency and multi-scale attentions in the skip connection, harnessing global context to better capture diverse forgery artifacts. Additionally, our framework addresses the loss of fine detail during upsampling by utilizing a global prior map, a curvature metric indicating the difficulty of forgery localization, which then guides a difficulty-guided attention module to preserve subtle manipulations more effectively. Extensive experiments on multiple benchmark datasets demonstrate that M2SFormer outperforms existing state-of-the-art models, offering superior generalization in detecting and localizing forgeries across unseen domains.

</details>


### [8] [DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing](https://arxiv.org/abs/2506.20967)
*Lingling Cai,Kang Zhao,Hangjie Yuan,Xiang Wang,Yingya Zhang,Kejie Huang*

Main category: cs.CV

TL;DR: DFVEdit是一种高效的零样本视频编辑方法，专为Video DiTs设计，无需注意力修改或微调，通过流转换直接操作潜在空间，显著提升计算效率和编辑质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法应用于Video DiTs时计算开销大，需要改进。

Method: 提出DFVEdit，基于流变换统一编辑与采样，引入CDFV、ICA和ER技术。

Result: DFVEdit在Video DiTs上实现20倍推理加速和85%内存节省，编辑质量达到SOTA。

Conclusion: DFVEdit高效且通用，适用于主流Video DiTs，性能优越。

Abstract: The advent of Video Diffusion Transformers (Video DiTs) marks a milestone in video generation. However, directly applying existing video editing methods to Video DiTs often incurs substantial computational overhead, due to resource-intensive attention modification or finetuning. To alleviate this problem, we present DFVEdit, an efficient zero-shot video editing method tailored for Video DiTs. DFVEdit eliminates the need for both attention modification and fine-tuning by directly operating on clean latents via flow transformation. To be more specific, we observe that editing and sampling can be unified under the continuous flow perspective. Building upon this foundation, we propose the Conditional Delta Flow Vector (CDFV) -- a theoretically unbiased estimation of DFV -- and integrate Implicit Cross Attention (ICA) guidance as well as Embedding Reinforcement (ER) to further enhance editing quality. DFVEdit excels in practical efficiency, offering at least 20x inference speed-up and 85\% memory reduction on Video DiTs compared to attention-engineering-based editing methods. Extensive quantitative and qualitative experiments demonstrate that DFVEdit can be seamlessly applied to popular Video DiTs (e.g., CogVideoX and Wan2.1), attaining state-of-the-art performance on structural fidelity, spatial-temporal consistency, and editing quality.

</details>


### [9] [From Cradle to Cane: A Two-Pass Framework for High-Fidelity Lifespan Face Aging](https://arxiv.org/abs/2506.20977)
*Tao Liu,Dafeng Zhang,Gengchen Li,Shizhuo Liu,Yongqi Song,Senmao Li,Shiqi Yang,Boqian Li,Kai Wang,Yaxing Wang*

Main category: cs.CV

TL;DR: Cradle2Cane是一个基于扩散模型的两阶段人脸老化框架，通过自适应噪声注入和身份感知嵌入解决年龄-身份平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在年龄准确性和身份一致性之间取得平衡，尤其是在大年龄跨度或极端头部姿态下。

Method: 提出两阶段框架：第一阶段通过自适应噪声注入（AdaNI）实现年龄准确性；第二阶段通过身份感知嵌入（IDEmb）增强身份一致性。

Result: 在CelebA-HQ测试集上，Cradle2Cane在年龄准确性和身份一致性上优于现有方法。

Conclusion: Cradle2Cane有效解决了人脸老化中的Age-ID平衡问题，具有实际应用潜力。

Abstract: Face aging has become a crucial task in computer vision, with applications ranging from entertainment to healthcare. However, existing methods struggle with achieving a realistic and seamless transformation across the entire lifespan, especially when handling large age gaps or extreme head poses. The core challenge lies in balancing age accuracy and identity preservation--what we refer to as the Age-ID trade-off. Most prior methods either prioritize age transformation at the expense of identity consistency or vice versa. In this work, we address this issue by proposing a two-pass face aging framework, named Cradle2Cane, based on few-step text-to-image (T2I) diffusion models. The first pass focuses on solving age accuracy by introducing an adaptive noise injection (AdaNI) mechanism. This mechanism is guided by including prompt descriptions of age and gender for the given person as the textual condition. Also, by adjusting the noise level, we can control the strength of aging while allowing more flexibility in transforming the face. However, identity preservation is weakly ensured here to facilitate stronger age transformations. In the second pass, we enhance identity preservation while maintaining age-specific features by conditioning the model on two identity-aware embeddings (IDEmb): SVR-ArcFace and Rotate-CLIP. This pass allows for denoising the transformed image from the first pass, ensuring stronger identity preservation without compromising the aging accuracy. Both passes are jointly trained in an end-to-end way. Extensive experiments on the CelebA-HQ test dataset, evaluated through Face++ and Qwen-VL protocols, show that our Cradle2Cane outperforms existing face aging methods in age accuracy and identity consistency.

</details>


### [10] [Rethink Sparse Signals for Pose-guided Text-to-image Generation](https://arxiv.org/abs/2506.20983)
*Wenjie Xuan,Jing Zhang,Juhua Liu,Bo Du,Dacheng Tao*

Main category: cs.CV

TL;DR: 论文提出了一种基于稀疏信号（如OpenPose）的新型空间姿态控制网络（SP-Ctrl），用于姿态引导的文本到图像生成，解决了密集信号（如深度图）带来的编辑困难和文本不一致问题。


<details>
  <summary>Details</summary>
Motivation: 密集信号（如深度图）在姿态引导的图像生成中存在编辑困难和与文本提示不一致的问题，而稀疏信号（如OpenPose）因其简单性和形状无关性被重新探索。

Method: 扩展OpenPose为可学习的空间表示，引入关键点概念学习，使关键点嵌入更具区分性和表达力，并改进姿态对齐。

Result: 在人类和动物的图像生成任务中，SP-Ctrl优于其他稀疏姿态引导方法，甚至与密集信号方法性能相当，并展示了跨物种生成的潜力。

Conclusion: 稀疏信号在姿态引导的图像生成中具有潜力，SP-Ctrl为稀疏信号提供了强大的可控性，同时保持了简单性和灵活性。

Abstract: Recent works favored dense signals (e.g., depth, DensePose), as an alternative to sparse signals (e.g., OpenPose), to provide detailed spatial guidance for pose-guided text-to-image generation. However, dense representations raised new challenges, including editing difficulties and potential inconsistencies with textual prompts. This fact motivates us to revisit sparse signals for pose guidance, owing to their simplicity and shape-agnostic nature, which remains underexplored. This paper proposes a novel Spatial-Pose ControlNet(SP-Ctrl), equipping sparse signals with robust controllability for pose-guided image generation. Specifically, we extend OpenPose to a learnable spatial representation, making keypoint embeddings discriminative and expressive. Additionally, we introduce keypoint concept learning, which encourages keypoint tokens to attend to the spatial positions of each keypoint, thus improving pose alignment. Experiments on animal- and human-centric image generation tasks demonstrate that our method outperforms recent spatially controllable T2I generation approaches under sparse-pose guidance and even matches the performance of dense signal-based methods. Moreover, SP-Ctrl shows promising capabilities in diverse and cross-species generation through sparse signals. Codes will be available at https://github.com/DREAMXFAR/SP-Ctrl.

</details>


### [11] [DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting](https://arxiv.org/abs/2506.20998)
*Yeon-Ji Song,Jaein Kim,Byung-Ju Kim,Byoung-Tak Zhang*

Main category: cs.CV

TL;DR: 提出了一种新方法DBMovi-GS，用于从模糊单目视频中合成动态场景的新视角，解决了现有方法在动态和模糊场景中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有新视角合成方法依赖高分辨率图像或静态几何假设，难以处理动态物体和相机运动导致的模糊问题。

Method: 使用稀疏控制的高斯泼溅技术，从模糊视频中生成密集3D高斯，恢复清晰度并重建动态场景的3D几何。

Result: 模型在动态模糊场景下表现出色，为模糊单目视频输入设定了新视角合成的基准。

Conclusion: DBMovi-GS有效解决了动态模糊场景的新视角合成问题，提升了视觉保真度和稳定性。

Abstract: Novel view synthesis is a task of generating scenes from unseen perspectives; however, synthesizing dynamic scenes from blurry monocular videos remains an unresolved challenge that has yet to be effectively addressed. Existing novel view synthesis methods are often constrained by their reliance on high-resolution images or strong assumptions about static geometry and rigid scene priors. Consequently, their approaches lack robustness in real-world environments with dynamic object and camera motion, leading to instability and degraded visual fidelity. To address this, we propose Motion-aware Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting (DBMovi-GS), a method designed for dynamic view synthesis from blurry monocular videos. Our model generates dense 3D Gaussians, restoring sharpness from blurry videos and reconstructing detailed 3D geometry of the scene affected by dynamic motion variations. Our model achieves robust performance in novel view synthesis under dynamic blurry scenes and sets a new benchmark in realistic novel view synthesis for blurry monocular video inputs.

</details>


### [12] [Detection of Breast Cancer Lumpectomy Margin with SAM-incorporated Forward-Forward Contrastive Learning](https://arxiv.org/abs/2506.21006)
*Tyler Ward,Xiaoqin Wang,Braxton McFarland,Md Atik Ahamed,Sahar Nozad,Talal Arshad,Hafsa Nebbache,Jin Chen,Abdullah Imran*

Main category: cs.CV

TL;DR: 提出了一种结合SAM和FFCL的深度学习框架，用于提高乳腺癌术中标本边缘评估的准确性和速度。


<details>
  <summary>Details</summary>
Motivation: 当前2D标本放射成像（SR）评估边缘状态的准确性有限，导致近四分之一的患者需要额外手术。

Method: 结合FFCL预训练策略和SAM模型，通过标注SR图像并预训练ResNet-18，生成粗二进制掩码以优化边缘分割。

Result: AUC为0.8455，Dice相似性比基线模型提高27.4%，推理时间缩短至47毫秒/图像。

Conclusion: FFCL-SAM显著提升了术中边缘评估的速度和准确性，有望降低再切除率并改善乳腺癌治疗效果。

Abstract: Complete removal of cancer tumors with a negative specimen margin during lumpectomy is essential in reducing breast cancer recurrence. However, 2D specimen radiography (SR), the current method used to assess intraoperative specimen margin status, has limited accuracy, resulting in nearly a quarter of patients requiring additional surgery. To address this, we propose a novel deep learning framework combining the Segment Anything Model (SAM) with Forward-Forward Contrastive Learning (FFCL), a pre-training strategy leveraging both local and global contrastive learning for patch-level classification of SR images. After annotating SR images with regions of known maligancy, non-malignant tissue, and pathology-confirmed margins, we pre-train a ResNet-18 backbone with FFCL to classify margin status, then reconstruct coarse binary masks to prompt SAM for refined tumor margin segmentation. Our approach achieved an AUC of 0.8455 for margin classification and segmented margins with a 27.4% improvement in Dice similarity over baseline models, while reducing inference time to 47 milliseconds per image. These results demonstrate that FFCL-SAM significantly enhances both the speed and accuracy of intraoperative margin assessment, with strong potential to reduce re-excision rates and improve surgical outcomes in breast cancer treatment. Our code is available at https://github.com/tbwa233/FFCL-SAM/.

</details>


### [13] [User-in-the-Loop View Sampling with Error Peaking Visualization](https://arxiv.org/abs/2506.21009)
*Ayaka Yasunaga,Hideo Saito,Shohei Mori*

Main category: cs.CV

TL;DR: 论文提出了一种基于局部重建光场和可视化误差的方法，用于减少AR中3D标注的需求，提升新视角合成的效率和用户体验。


<details>
  <summary>Details</summary>
Motivation: 现有AR方法依赖3D标注且限制场景探索范围，导致数据收集任务繁重且场景受限。

Method: 使用局部重建光场和可视化误差，指导用户插入新视角以减少误差。

Result: 误差峰值可视化方法侵入性更低，减少用户失望，且需要更少的视角样本。该方法还可用于大场景的辐射场重建。

Conclusion: 提出的方法解放了用户对3D标注的依赖，扩展了场景探索范围，提升了新视角合成的效果和效率。

Abstract: Augmented reality (AR) provides ways to visualize missing view samples for novel view synthesis. Existing approaches present 3D annotations for new view samples and task users with taking images by aligning the AR display. This data collection task is known to be mentally demanding and limits capture areas to pre-defined small areas due to the ideal but restrictive underlying sampling theory. To free users from 3D annotations and limited scene exploration, we propose using locally reconstructed light fields and visualizing errors to be removed by inserting new views. Our results show that the error-peaking visualization is less invasive, reduces disappointment in final results, and is satisfactory with fewer view samples in our mobile view synthesis system. We also show that our approach can contribute to recent radiance field reconstruction for larger scenes, such as 3D Gaussian splatting.

</details>


### [14] [HybridQ: Hybrid Classical-Quantum Generative Adversarial Network for Skin Disease Image Generation](https://arxiv.org/abs/2506.21015)
*Qingyue Jiao,Kangyu Zheng,Yiyu Shi,Zhiding Liang*

Main category: cs.CV

TL;DR: 提出了一种经典-量子混合的生成对抗网络（GAN），能够生成彩色医学图像，解决了量子图像生成的低质量和灰度限制，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 皮肤疾病数据集的类别不平衡、隐私问题和对象偏差问题使得数据增强至关重要，而现有量子图像生成方法只能生成低质量灰度图像。

Method: 通过一种新颖的经典-量子潜在空间融合技术，开发了首个能够生成彩色医学图像的经典-量子GAN。

Result: 模型在图像生成质量和分类性能提升上优于经典深度卷积GAN和现有混合经典-量子GAN，且参数和训练时间大幅减少。

Conclusion: 该研究为量子图像生成在医学领域的应用展示了广阔前景，尤其是在量子硬件进一步发展的背景下。

Abstract: Machine learning-assisted diagnosis is gaining traction in skin disease detection, but training effective models requires large amounts of high-quality data. Skin disease datasets often suffer from class imbalance, privacy concerns, and object bias, making data augmentation essential. While classical generative models are widely used, they demand extensive computational resources and lengthy training time. Quantum computing offers a promising alternative, but existing quantum-based image generation methods can only yield grayscale low-quality images. Through a novel classical-quantum latent space fusion technique, our work overcomes this limitation and introduces the first classical-quantum generative adversarial network (GAN) capable of generating color medical images. Our model outperforms classical deep convolutional GANs and existing hybrid classical-quantum GANs in both image generation quality and classification performance boost when used as data augmentation. Moreover, the performance boost is comparable with that achieved using state-of-the-art classical generative models, yet with over 25 times fewer parameters and 10 times fewer training epochs. Such results suggest a promising future for quantum image generation as quantum hardware advances. Finally, we demonstrate the robust performance of our model on real IBM quantum machine with hardware noise.

</details>


### [15] [Instella-T2I: Pushing the Limits of 1D Discrete Latent Space Image Generation](https://arxiv.org/abs/2506.21022)
*Ze Wang,Hao Chen,Benran Hu,Jiang Liu,Ximeng Sun,Jialian Wu,Yusheng Su,Xiaodong Yu,Emad Barsoum,Zicheng Liu*

Main category: cs.CV

TL;DR: 提出了一种1D二进制图像潜在表示方法，显著减少了图像建模所需的token数量，提升了训练和推理效率。


<details>
  <summary>Details</summary>
Motivation: 传统2D网格结构的图像token化方法计算量大，1D潜在空间虽减少了token数量，但仍需进一步优化。

Method: 引入1D二进制图像潜在表示，将图像表示为二进制向量序列，替代传统one-hot编码。

Result: 在1024x1024分辨率图像上仅需128个token，比标准VQ-VAE减少32倍，训练和推理速度显著提升。

Conclusion: 该方法提供了一种高效且可扩展的图像token化替代方案，无需私有数据或后训练优化即可达到现代图像生成模型的性能。

Abstract: Image tokenization plays a critical role in reducing the computational demands of modeling high-resolution images, significantly improving the efficiency of image and multimodal understanding and generation. Recent advances in 1D latent spaces have reduced the number of tokens required by eliminating the need for a 2D grid structure. In this paper, we further advance compact discrete image representation by introducing 1D binary image latents. By representing each image as a sequence of binary vectors, rather than using traditional one-hot codebook tokens, our approach preserves high-resolution details while maintaining the compactness of 1D latents. To the best of our knowledge, our text-to-image models are the first to achieve competitive performance in both diffusion and auto-regressive generation using just 128 discrete tokens for images up to 1024x1024, demonstrating up to a 32-fold reduction in token numbers compared to standard VQ-VAEs. The proposed 1D binary latent space, coupled with simple model architectures, achieves marked improvements in speed training and inference speed. Our text-to-image models allow for a global batch size of 4096 on a single GPU node with 8 AMD MI300X GPUs, and the training can be completed within 200 GPU days. Our models achieve competitive performance compared to modern image generation models without any in-house private training data or post-training refinements, offering a scalable and efficient alternative to conventional tokenization methods.

</details>


### [16] [Improving Diffusion-Based Image Editing Faithfulness via Guidance and Scheduling](https://arxiv.org/abs/2506.21045)
*Hansam Cho,Seoung Bum Kim*

Main category: cs.CV

TL;DR: 论文提出了一种名为FGS的方法，通过引入忠实性指导和调度策略，解决了图像编辑中编辑性与忠实性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 在文本引导的扩散模型中，图像编辑的两个关键方面是编辑性和忠实性，但二者之间存在固有权衡，难以同时达到最优效果。

Method: FGS结合了忠实性指导以增强输入图像信息的保留，并引入调度策略解决编辑性与忠实性之间的不对齐问题。

Result: 实验表明，FGS在保持编辑性的同时显著提升了忠实性，且兼容多种编辑方法，适用于多样化任务。

Conclusion: FGS通过优化忠实性与编辑性的平衡，实现了高质量、精确的图像编辑。

Abstract: Text-guided diffusion models have become essential for high-quality image synthesis, enabling dynamic image editing. In image editing, two crucial aspects are editability, which determines the extent of modification, and faithfulness, which reflects how well unaltered elements are preserved. However, achieving optimal results is challenging because of the inherent trade-off between editability and faithfulness. To address this, we propose Faithfulness Guidance and Scheduling (FGS), which enhances faithfulness with minimal impact on editability. FGS incorporates faithfulness guidance to strengthen the preservation of input image information and introduces a scheduling strategy to resolve misalignment between editability and faithfulness. Experimental results demonstrate that FGS achieves superior faithfulness while maintaining editability. Moreover, its compatibility with various editing methods enables precise, high-quality image edits across diverse tasks.

</details>


### [17] [ESMStereo: Enhanced ShuffleMixer Disparity Upsampling for Real-Time and Accurate Stereo Matching](https://arxiv.org/abs/2506.21091)
*Mahmoud Tahmasebi,Saif Huq,Kevin Meehan,Marion McAfee*

Main category: cs.CV

TL;DR: 论文提出了一种名为Enhanced Shuffle Mixer (ESM)的方法，旨在解决基于小规模成本体积的立体匹配中信息不足的问题，同时实现实时性能。


<details>
  <summary>Details</summary>
Motivation: 立体匹配在自动驾驶系统中至关重要，但现有方法难以同时实现高精度和实时性能。大规模成本体积冗余且计算密集，而小规模成本体积缺乏足够信息。

Method: ESM通过将主要特征整合到视差上采样单元中，恢复关键细节。它快速提取初始视差估计的特征并与图像特征融合，通过混洗和层分割混合，再通过紧凑的特征引导沙漏网络细化。

Result: ESM在保持低计算成本和大感受野的同时，实现了高精度视差图的实时重建。ESMStereo的紧凑版本在高性能GPU上达到116 FPS，在AGX Orin上达到91 FPS。

Conclusion: ESM有效解决了小规模成本体积信息不足的问题，同时实现了实时性能，为立体匹配提供了一种高效解决方案。

Abstract: Stereo matching has become an increasingly important component of modern autonomous systems. Developing deep learning-based stereo matching models that deliver high accuracy while operating in real-time continues to be a major challenge in computer vision. In the domain of cost-volume-based stereo matching, accurate disparity estimation depends heavily on large-scale cost volumes. However, such large volumes store substantial redundant information and also require computationally intensive aggregation units for processing and regression, making real-time performance unattainable. Conversely, small-scale cost volumes followed by lightweight aggregation units provide a promising route for real-time performance, but lack sufficient information to ensure highly accurate disparity estimation. To address this challenge, we propose the Enhanced Shuffle Mixer (ESM) to mitigate information loss associated with small-scale cost volumes. ESM restores critical details by integrating primary features into the disparity upsampling unit. It quickly extracts features from the initial disparity estimation and fuses them with image features. These features are mixed by shuffling and layer splitting then refined through a compact feature-guided hourglass network to recover more detailed scene geometry. The ESM focuses on local contextual connectivity with a large receptive field and low computational cost, leading to the reconstruction of a highly accurate disparity map at real-time. The compact version of ESMStereo achieves an inference speed of 116 FPS on high-end GPUs and 91 FPS on the AGX Orin.

</details>


### [18] [CL-Splats: Continual Learning of Gaussian Splatting with Local Optimization](https://arxiv.org/abs/2506.21117)
*Jan Ackermann,Jonas Kulhanek,Shengqu Cai,Haofei Xu,Marc Pollefeys,Gordon Wetzstein,Leonidas Guibas,Songyou Peng*

Main category: cs.CV

TL;DR: CL-Splats是一种动态3D场景表示更新方法，通过增量更新高斯泼溅表示，结合变化检测模块，实现局部优化和高效计算。


<details>
  <summary>Details</summary>
Motivation: 在动态3D环境中，实时更新场景表示对机器人、混合现实和具身AI至关重要，需避免重新优化整个场景的计算开销。

Method: CL-Splats通过变化检测模块分割场景中的动态和静态部分，进行局部优化，并支持存储和恢复历史场景状态。

Result: 实验表明，CL-Splats在更新效率和重建质量上优于现有方法。

Conclusion: CL-Splats为未来实时3D场景重建任务奠定了坚实基础。

Abstract: In dynamic 3D environments, accurately updating scene representations over time is crucial for applications in robotics, mixed reality, and embodied AI. As scenes evolve, efficient methods to incorporate changes are needed to maintain up-to-date, high-quality reconstructions without the computational overhead of re-optimizing the entire scene. This paper introduces CL-Splats, which incrementally updates Gaussian splatting-based 3D representations from sparse scene captures. CL-Splats integrates a robust change-detection module that segments updated and static components within the scene, enabling focused, local optimization that avoids unnecessary re-computation. Moreover, CL-Splats supports storing and recovering previous scene states, facilitating temporal segmentation and new scene-analysis applications. Our extensive experiments demonstrate that CL-Splats achieves efficient updates with improved reconstruction quality over the state-of-the-art. This establishes a robust foundation for future real-time adaptation in 3D scene reconstruction tasks.

</details>


### [19] [Learning to See in the Extremely Dark](https://arxiv.org/abs/2506.21132)
*Hai Jiang,Binhao Guan,Zhen Liu,Xiaohong Liu,Jian Yu,Zheng Liu,Songchen Han,Shuaicheng Liu*

Main category: cs.CV

TL;DR: 论文提出了一种用于极低光RAW图像增强的数据合成方法和扩散模型框架，并创建了SIED数据集。


<details>
  <summary>Details</summary>
Motivation: 现有学习方法的极低光（0.0001 lux）RAW图像增强能力尚未探索，缺乏对应数据集。

Method: 提出数据合成管道生成极低光RAW图像，并设计扩散模型框架，包含自适应光照校正模块和颜色一致性损失。

Result: 在SIED和公开基准上验证了方法的有效性。

Conclusion: 方法在极低光RAW图像增强中表现优异，代码和数据集已开源。

Abstract: Learning-based methods have made promising advances in low-light RAW image enhancement, while their capability to extremely dark scenes where the environmental illuminance drops as low as 0.0001 lux remains to be explored due to the lack of corresponding datasets. To this end, we propose a paired-to-paired data synthesis pipeline capable of generating well-calibrated extremely low-light RAW images at three precise illuminance ranges of 0.01-0.1 lux, 0.001-0.01 lux, and 0.0001-0.001 lux, together with high-quality sRGB references to comprise a large-scale paired dataset named See-in-the-Extremely-Dark (SIED) to benchmark low-light RAW image enhancement approaches. Furthermore, we propose a diffusion-based framework that leverages the generative ability and intrinsic denoising property of diffusion models to restore visually pleasing results from extremely low-SNR RAW inputs, in which an Adaptive Illumination Correction Module (AICM) and a color consistency loss are introduced to ensure accurate exposure correction and color restoration. Extensive experiments on the proposed SIED and publicly available benchmarks demonstrate the effectiveness of our method. The code and dataset are available at https://github.com/JianghaiSCU/SIED.

</details>


### [20] [Geometry and Perception Guided Gaussians for Multiview-consistent 3D Generation from a Single Image](https://arxiv.org/abs/2506.21152)
*Pufan Li,Bi'an Du,Wei Hu*

Main category: cs.CV

TL;DR: 提出一种新方法，通过整合几何和感知先验，从单张图像生成高质量3D物体，解决多视角一致性和几何细节不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从单视图图像生成3D物体时，多视角一致性和几何细节表现不佳，需要改进。

Method: 训练三个不同的高斯分支（几何先验、感知先验和高斯噪声），通过交互优化和重投影策略增强深度一致性。

Result: 实验表明，该方法在3D重建和新视角合成上优于现有方法，生成结果更高质量且一致。

Conclusion: 该方法无需额外训练，有效整合先验信息，显著提升了单图像3D重建的准确性和一致性。

Abstract: Generating realistic 3D objects from single-view images requires natural appearance, 3D consistency, and the ability to capture multiple plausible interpretations of unseen regions. Existing approaches often rely on fine-tuning pretrained 2D diffusion models or directly generating 3D information through fast network inference or 3D Gaussian Splatting, but their results generally suffer from poor multiview consistency and lack geometric detail. To takle these issues, we present a novel method that seamlessly integrates geometry and perception priors without requiring additional model training to reconstruct detailed 3D objects from a single image. Specifically, we train three different Gaussian branches initialized from the geometry prior, perception prior and Gaussian noise, respectively. The geometry prior captures the rough 3D shapes, while the perception prior utilizes the 2D pretrained diffusion model to enhance multiview information. Subsequently, we refine 3D Gaussian branches through mutual interaction between geometry and perception priors, further enhanced by a reprojection-based strategy that enforces depth consistency. Experiments demonstrate the higher-fidelity reconstruction results of our method, outperforming existing methods on novel view synthesis and 3D reconstruction, demonstrating robust and consistent 3D object generation.

</details>


### [21] [BitMark for Infinity: Watermarking Bitwise Autoregressive Image Generative Models](https://arxiv.org/abs/2506.21209)
*Louis Kerner,Michel Meintz,Bihe Zhao,Franziska Boenisch,Adam Dziedzic*

Main category: cs.CV

TL;DR: 论文提出BitMark，一种针对Infinity文本到图像模型的鲁棒位级水印框架，旨在防止模型崩溃。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型输出在互联网上的泛滥，这些内容可能被重新用作训练数据，导致模型性能逐渐退化（模型崩溃）。水印技术可以识别生成内容，从而缓解这一问题。

Method: BitMark在Infinity图像生成过程中，直接在位级别嵌入水印，跨多尺度保留视觉保真度和生成速度，同时抵抗多种去除技术。

Result: BitMark具有高放射性，即使用水印图像训练的新模型输出也会携带水印，且水印在微调后仍可检测。

Conclusion: BitMark为图像生成模型提供了一种防止模型崩溃的可靠方法，通过检测生成内容实现。

Abstract: State-of-the-art text-to-image models like Infinity generate photorealistic images at an unprecedented speed. These models operate in a bitwise autoregressive manner over a discrete set of tokens that is practically infinite in size. However, their impressive generative power comes with a growing risk: as their outputs increasingly populate the Internet, they are likely to be scraped and reused as training data-potentially by the very same models. This phenomenon has been shown to lead to model collapse, where repeated training on generated content, especially from the models' own previous versions, causes a gradual degradation in performance. A promising mitigation strategy is watermarking, which embeds human-imperceptible yet detectable signals into generated images-enabling the identification of generated content. In this work, we introduce BitMark, a robust bitwise watermarking framework for Infinity. Our method embeds a watermark directly at the bit level of the token stream across multiple scales (also referred to as resolutions) during Infinity's image generation process. Our bitwise watermark subtly influences the bits to preserve visual fidelity and generation speed while remaining robust against a spectrum of removal techniques. Furthermore, it exhibits high radioactivity, i.e., when watermarked generated images are used to train another image generative model, this second model's outputs will also carry the watermark. The radioactive traces remain detectable even when only fine-tuning diffusion or image autoregressive models on images watermarked with our BitMark. Overall, our approach provides a principled step toward preventing model collapse in image generative models by enabling reliable detection of generated outputs.

</details>


### [22] [Video Virtual Try-on with Conditional Diffusion Transformer Inpainter](https://arxiv.org/abs/2506.21270)
*Cheng Zou,Senlin Cheng,Bolei Xu,Dandan Zheng,Xiaobo Li,Jingdong Chen,Ming Yang*

Main category: cs.CV

TL;DR: ViTI提出了一种基于条件视频修复的视频虚拟试穿方法，通过3D时空注意力扩散变换器实现更好的时空一致性。


<details>
  <summary>Details</summary>
Motivation: 现有视频虚拟试穿方法存在时空不一致问题，ViTI旨在通过视频生成而非逐帧图像试穿来解决这一问题。

Method: 采用扩散变换器构建视频修复框架，结合3D时空注意力和多阶段训练，逐步适应视频服装修复任务。

Result: 实验表明，ViTI在时空一致性和服装细节保留上优于现有方法。

Conclusion: ViTI通过视频修复任务重新定义视频虚拟试穿，显著提升了效果。

Abstract: Video virtual try-on aims to naturally fit a garment to a target person in consecutive video frames. It is a challenging task, on the one hand, the output video should be in good spatial-temporal consistency, on the other hand, the details of the given garment need to be preserved well in all the frames. Naively using image-based try-on methods frame by frame can get poor results due to severe inconsistency. Recent diffusion-based video try-on methods, though very few, happen to coincide with a similar solution: inserting temporal attention into image-based try-on model to adapt it for video try-on task, which have shown improvements but there still exist inconsistency problems. In this paper, we propose ViTI (Video Try-on Inpainter), formulate and implement video virtual try-on as a conditional video inpainting task, which is different from previous methods. In this way, we start with a video generation problem instead of an image-based try-on problem, which from the beginning has a better spatial-temporal consistency. Specifically, at first we build a video inpainting framework based on Diffusion Transformer with full 3D spatial-temporal attention, and then we progressively adapt it for video garment inpainting, with a collection of masking strategies and multi-stage training. After these steps, the model can inpaint the masked garment area with appropriate garment pixels according to the prompt with good spatial-temporal consistency. Finally, as other try-on methods, garment condition is added to the model to make sure the inpainted garment appearance and details are as expected. Both quantitative and qualitative experimental results show that ViTI is superior to previous works.

</details>


### [23] [HieraSurg: Hierarchy-Aware Diffusion Model for Surgical Video Generation](https://arxiv.org/abs/2506.21287)
*Diego Biagini,Nassir Navab,Azade Farshad*

Main category: cs.CV

TL;DR: HieraSurg是一个层次感知的手术视频生成框架，通过两阶段扩散模型实现高质量视频合成，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有手术视频生成方法缺乏对手术动作和阶段的语义一致性，无法满足实际手术模拟的需求。

Method: HieraSurg采用两阶段扩散模型：第一阶段预测粗粒度语义变化，第二阶段结合细粒度视觉特征生成视频。

Result: 在胆囊切除术视频生成任务中，HieraSurg在定量和定性上均显著优于现有方法，并能生成更高帧率的视频。

Conclusion: HieraSurg展示了在多层次抽象信息下的强大生成能力，具有实际手术应用的潜力。

Abstract: Surgical Video Synthesis has emerged as a promising research direction following the success of diffusion models in general-domain video generation. Although existing approaches achieve high-quality video generation, most are unconditional and fail to maintain consistency with surgical actions and phases, lacking the surgical understanding and fine-grained guidance necessary for factual simulation. We address these challenges by proposing HieraSurg, a hierarchy-aware surgical video generation framework consisting of two specialized diffusion models. Given a surgical phase and an initial frame, HieraSurg first predicts future coarse-grained semantic changes through a segmentation prediction model. The final video is then generated by a second-stage model that augments these temporal segmentation maps with fine-grained visual features, leading to effective texture rendering and integration of semantic information in the video space. Our approach leverages surgical information at multiple levels of abstraction, including surgical phase, action triplets, and panoptic segmentation maps. The experimental results on Cholecystectomy Surgical Video Generation demonstrate that the model significantly outperforms prior work both quantitatively and qualitatively, showing strong generalization capabilities and the ability to generate higher frame-rate videos. The model exhibits particularly fine-grained adherence when provided with existing segmentation maps, suggesting its potential for practical surgical applications.

</details>


### [24] [PanSt3R: Multi-view Consistent Panoptic Segmentation](https://arxiv.org/abs/2506.21348)
*Lojze Zust,Yohann Cabon,Juliette Marrie,Leonid Antsfeld,Boris Chidlovskii,Jerome Revaud,Gabriela Csurka*

Main category: cs.CV

TL;DR: PanSt3R提出了一种无需测试时优化的统一方法，通过单次前向预测3D几何和多视角全景分割，性能优于现有方法且速度更快。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖2D全景分割和测试时优化，未能充分利用多视角空间关系且计算成本高。

Method: 基于MUSt3R改进，联合预测3D几何和多视角全景分割，并优化掩码合并流程。

Result: 在多个基准测试中达到最优性能，速度显著快于现有方法。

Conclusion: PanSt3R是一种高效、可扩展的3D场景全景分割方法。

Abstract: Panoptic segmentation of 3D scenes, involving the segmentation and classification of object instances in a dense 3D reconstruction of a scene, is a challenging problem, especially when relying solely on unposed 2D images. Existing approaches typically leverage off-the-shelf models to extract per-frame 2D panoptic segmentations, before optimizing an implicit geometric representation (often based on NeRF) to integrate and fuse the 2D predictions. We argue that relying on 2D panoptic segmentation for a problem inherently 3D and multi-view is likely suboptimal as it fails to leverage the full potential of spatial relationships across views. In addition to requiring camera parameters, these approaches also necessitate computationally expensive test-time optimization for each scene. Instead, in this work, we propose a unified and integrated approach PanSt3R, which eliminates the need for test-time optimization by jointly predicting 3D geometry and multi-view panoptic segmentation in a single forward pass. Our approach builds upon recent advances in 3D reconstruction, specifically upon MUSt3R, a scalable multi-view version of DUSt3R, and enhances it with semantic awareness and multi-view panoptic segmentation capabilities. We additionally revisit the standard post-processing mask merging procedure and introduce a more principled approach for multi-view segmentation. We also introduce a simple method for generating novel-view predictions based on the predictions of PanSt3R and vanilla 3DGS. Overall, the proposed PanSt3R is conceptually simple, yet fast and scalable, and achieves state-of-the-art performance on several benchmarks, while being orders of magnitude faster than existing methods.

</details>


### [25] [ShotBench: Expert-Level Cinematic Understanding in Vision-Language Models](https://arxiv.org/abs/2506.21356)
*Hongbo Liu,Jingwen He,Yi Jin,Dian Zheng,Yuhao Dong,Fan Zhang,Ziqi Huang,Yinan He,Yangguang Li,Weichao Chen,Yu Qiao,Wanli Ouyang,Shengjie Zhao,Ziwei Liu*

Main category: cs.CV

TL;DR: ShotBench是一个专门用于评估电影语言理解的基准测试，揭示了现有视觉语言模型在理解电影语法方面的局限性，并提出了ShotQA数据集和ShotVL模型，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在理解电影语法方面存在不足，限制了AI在视频生成中的精确性，因此需要专门的评估工具和改进方法。

Method: 通过构建ShotBench基准测试和ShotQA数据集，利用监督微调和Group Relative Policy Optimization开发了ShotVL模型。

Result: 现有模型在ShotBench上表现不佳（平均准确率低于60%），而ShotVL显著优于所有现有模型，达到新水平。

Conclusion: ShotBench和ShotQA为电影语言理解提供了重要工具，ShotVL的推出推动了该领域的进步，相关资源已开源。

Abstract: Cinematography, the fundamental visual language of film, is essential for conveying narrative, emotion, and aesthetic quality. While recent Vision-Language Models (VLMs) demonstrate strong general visual understanding, their proficiency in comprehending the nuanced cinematic grammar embedded within individual shots remains largely unexplored and lacks robust evaluation. This critical gap limits both fine-grained visual comprehension and the precision of AI-assisted video generation. To address this, we introduce \textbf{ShotBench}, a comprehensive benchmark specifically designed for cinematic language understanding. It features over 3.5k expert-annotated QA pairs from images and video clips, meticulously curated from over 200 acclaimed (predominantly Oscar-nominated) films and spanning eight key cinematography dimensions. Our evaluation of 24 leading VLMs on ShotBench reveals their substantial limitations: even the top-performing model achieves less than 60\% average accuracy, particularly struggling with fine-grained visual cues and complex spatial reasoning. To catalyze advancement in this domain, we construct \textbf{ShotQA}, a large-scale multimodal dataset comprising approximately 70k cinematic QA pairs. Leveraging ShotQA, we develop \textbf{ShotVL} through supervised fine-tuning and Group Relative Policy Optimization. ShotVL significantly outperforms all existing open-source and proprietary models on ShotBench, establishing new \textbf{state-of-the-art} performance. We open-source our models, data, and code to foster rapid progress in this crucial area of AI-driven cinematic understanding and generation.

</details>


### [26] [GenFlow: Interactive Modular System for Image Generation](https://arxiv.org/abs/2506.21369)
*Duc-Hung Nguyen,Huu-Phuc Huynh,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: GenFlow是一个模块化框架，旨在降低生成艺术的入门门槛，通过节点编辑器和智能助手简化工作流程。


<details>
  <summary>Details</summary>
Motivation: 生成艺术需要高级技术知识，限制了其广泛应用。GenFlow旨在解决这一问题，使所有技能水平的用户都能轻松使用。

Method: GenFlow采用基于节点的编辑器和自然语言处理的智能助手，自动化部署流程，降低技术障碍。

Result: 用户研究表明，GenFlow能优化工作流程，减少任务完成时间，并通过直观界面提升用户体验。

Conclusion: GenFlow通过提升可访问性和效率，重新定义了生成艺术的工具。

Abstract: Generative art unlocks boundless creative possibilities, yet its full potential remains untapped due to the technical expertise required for advanced architectural concepts and computational workflows. To bridge this gap, we present GenFlow, a novel modular framework that empowers users of all skill levels to generate images with precision and ease. Featuring a node-based editor for seamless customization and an intelligent assistant powered by natural language processing, GenFlow transforms the complexity of workflow creation into an intuitive and accessible experience. By automating deployment processes and minimizing technical barriers, our framework makes cutting-edge generative art tools available to everyone. A user study demonstrated GenFlow's ability to optimize workflows, reduce task completion times, and enhance user understanding through its intuitive interface and adaptive features. These results position GenFlow as a groundbreaking solution that redefines accessibility and efficiency in the realm of generative art.

</details>


### [27] [Curve-Aware Gaussian Splatting for 3D Parametric Curve Reconstruction](https://arxiv.org/abs/2506.21401)
*Zhirui Gao. Renjiao Yi,Yaqiao Dai,Xuening Zhu,Wei Chen,Chenyang Zhu,Kai Xu*

Main category: cs.CV

TL;DR: 本文提出了一种端到端框架，直接从多视角边缘图重建3D参数曲线，避免了传统两阶段方法的误差累积问题。


<details>
  <summary>Details</summary>
Motivation: 现有两阶段方法存在优化间隙导致的误差累积问题，且参数曲线不适合基于渲染的多视角优化。

Method: 提出了一种双向耦合机制，将参数曲线与边缘导向的高斯分量结合，形成可微渲染的曲线感知表示（CurveGaussian），并引入动态自适应拓扑优化框架。

Result: 在ABC数据集和真实场景基准测试中表现优于两阶段方法，参数更少且效率更高。

Conclusion: 该方法通过直接优化参数曲线，实现了更高效和更鲁棒的重建。

Abstract: This paper presents an end-to-end framework for reconstructing 3D parametric curves directly from multi-view edge maps. Contrasting with existing two-stage methods that follow a sequential ``edge point cloud reconstruction and parametric curve fitting'' pipeline, our one-stage approach optimizes 3D parametric curves directly from 2D edge maps, eliminating error accumulation caused by the inherent optimization gap between disconnected stages. However, parametric curves inherently lack suitability for rendering-based multi-view optimization, necessitating a complementary representation that preserves their geometric properties while enabling differentiable rendering. We propose a novel bi-directional coupling mechanism between parametric curves and edge-oriented Gaussian components. This tight correspondence formulates a curve-aware Gaussian representation, \textbf{CurveGaussian}, that enables differentiable rendering of 3D curves, allowing direct optimization guided by multi-view evidence. Furthermore, we introduce a dynamically adaptive topology optimization framework during training to refine curve structures through linearization, merging, splitting, and pruning operations. Comprehensive evaluations on the ABC dataset and real-world benchmarks demonstrate our one-stage method's superiority over two-stage alternatives, particularly in producing cleaner and more robust reconstructions. Additionally, by directly optimizing parametric curves, our method significantly reduces the parameter count during training, achieving both higher efficiency and superior performance compared to existing approaches.

</details>


### [28] [XVerse: Consistent Multi-Subject Control of Identity and Semantic Attributes via DiT Modulation](https://arxiv.org/abs/2506.21416)
*Bowen Chen,Mengyi Zhao,Haomiao Sun,Li Chen,Xu Wang,Kang Du,Xinglong Wu*

Main category: cs.CV

TL;DR: XVerse模型通过将参考图像转换为特定文本流的偏移量，实现对多主题图像生成的精确独立控制，避免了传统方法的编辑性和一致性不足问题。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像生成中多主题控制时编辑性和一致性不足的问题。

Method: 将参考图像转换为偏移量，用于特定文本流的调制，从而独立控制每个主题。

Result: XVerse实现了高保真、可编辑的多主题图像合成，并能精确控制单个主题的特征和语义属性。

Conclusion: XVerse显著提升了复杂场景生成的能力，为个性化图像生成提供了新方法。

Abstract: Achieving fine-grained control over subject identity and semantic attributes (pose, style, lighting) in text-to-image generation, particularly for multiple subjects, often undermines the editability and coherence of Diffusion Transformers (DiTs). Many approaches introduce artifacts or suffer from attribute entanglement. To overcome these challenges, we propose a novel multi-subject controlled generation model XVerse. By transforming reference images into offsets for token-specific text-stream modulation, XVerse allows for precise and independent control for specific subject without disrupting image latents or features. Consequently, XVerse offers high-fidelity, editable multi-subject image synthesis with robust control over individual subject characteristics and semantic attributes. This advancement significantly improves personalized and complex scene generation capabilities.

</details>


### [29] [EndoFlow-SLAM: Real-Time Endoscopic SLAM with Flow-Constrained Gaussian Splatting](https://arxiv.org/abs/2506.21420)
*Taoyu Wu,Yiyi Miao,Zhuoxiao Li,Haocheng Zhao,Kang Dang,Jionglong Su,Limin Yu,Haoang Li*

Main category: cs.CV

TL;DR: 论文提出了一种结合光流损失和深度正则化的3D高斯泼溅SLAM方法，用于内窥镜场景中的高效3D重建和实时可视化。


<details>
  <summary>Details</summary>
Motivation: 内窥镜场景中存在非朗伯表面和呼吸引起的动态运动，导致传统SLAM系统性能下降，需要新的约束和优化策略。

Method: 引入光流损失作为几何约束，提出深度正则化策略，并改进3DGS细化策略以优化关键帧的渲染质量。

Result: 在C3VD静态数据集和StereoMIS动态数据集上，方法在新视角合成和姿态估计方面优于现有技术。

Conclusion: 该方法在内窥镜静态和动态场景中均表现出高性能，代码将在论文接受后公开。

Abstract: Efficient three-dimensional reconstruction and real-time visualization are critical in surgical scenarios such as endoscopy. In recent years, 3D Gaussian Splatting (3DGS) has demonstrated remarkable performance in efficient 3D reconstruction and rendering. Most 3DGS-based Simultaneous Localization and Mapping (SLAM) methods only rely on the appearance constraints for optimizing both 3DGS and camera poses. However, in endoscopic scenarios, the challenges include photometric inconsistencies caused by non-Lambertian surfaces and dynamic motion from breathing affects the performance of SLAM systems. To address these issues, we additionally introduce optical flow loss as a geometric constraint, which effectively constrains both the 3D structure of the scene and the camera motion. Furthermore, we propose a depth regularisation strategy to mitigate the problem of photometric inconsistencies and ensure the validity of 3DGS depth rendering in endoscopic scenes. In addition, to improve scene representation in the SLAM system, we improve the 3DGS refinement strategy by focusing on viewpoints corresponding to Keyframes with suboptimal rendering quality frames, achieving better rendering results. Extensive experiments on the C3VD static dataset and the StereoMIS dynamic dataset demonstrate that our method outperforms existing state-of-the-art methods in novel view synthesis and pose estimation, exhibiting high performance in both static and dynamic surgical scenes. The source code will be publicly available upon paper acceptance.

</details>


### [30] [Benchmarking Deep Learning and Vision Foundation Models for Atypical vs. Normal Mitosis Classification with Cross-Dataset Evaluation](https://arxiv.org/abs/2506.21444)
*Sweta Banerjee,Viktoria Weiss,Taryn A. Donovan,Rutger A. Fick,Thomas Conrad,Jonas Ammeling,Nils Porsche,Robert Klopfleisch,Christopher Kaltenecker,Katharina Breininger,Marc Aubreville,Christof A. Bertram*

Main category: cs.CV

TL;DR: 该研究通过深度学习模型（包括基线模型、基础模型线性探测和LoRA微调）对乳腺癌中的非典型有丝分裂（AMF）进行分类，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 非典型有丝分裂是肿瘤恶性程度的独立预后标志物，但其识别存在挑战，如低发生率、形态差异小、病理学家间一致性低以及数据集类别不平衡。

Method: 研究使用AMi-Br数据集，并引入两个新数据集AtNorM-Br和AtNorM-MD，比较了多种深度学习方法的性能，包括LoRA微调的基础模型。

Result: 在AMi-Br、AtNorM-Br和AtNorM-MD数据集上，平均平衡准确率分别达到0.8135、0.7696和0.7705，LoRA微调的Virchow基础模型表现最佳。

Conclusion: 研究表明，尽管非典型有丝分裂分类具有挑战性，但通过迁移学习和模型微调技术可以有效解决。所有代码和数据已开源。

Abstract: Atypical mitoses mark a deviation in the cell division process that can be an independent prognostically relevant marker for tumor malignancy. However, their identification remains challenging due to low prevalence, at times subtle morphological differences from normal mitoses, low inter-rater agreement among pathologists, and class imbalance in datasets. Building on the Atypical Mitosis dataset for Breast Cancer (AMi-Br), this study presents a comprehensive benchmark comparing deep learning approaches for automated atypical mitotic figure (AMF) classification, including baseline models, foundation models with linear probing, and foundation models fine-tuned with low-rank adaptation (LoRA). For rigorous evaluation, we further introduce two new hold-out AMF datasets - AtNorM-Br, a dataset of mitoses from the The TCGA breast cancer cohort, and AtNorM-MD, a multi-domain dataset of mitoses from the MIDOG++ training set. We found average balanced accuracy values of up to 0.8135, 0.7696, and 0.7705 on the in-domain AMi-Br and the out-of-domain AtNorm-Br and AtNorM-MD datasets, respectively, with the results being particularly good for LoRA-based adaptation of the Virchow-line of foundation models. Our work shows that atypical mitosis classification, while being a challenging problem, can be effectively addressed through the use of recent advances in transfer learning and model fine-tuning techniques. We make available all code and data used in this paper in this github repository: https://github.com/DeepMicroscopy/AMi-Br_Benchmark.

</details>


### [31] [Controllable 3D Placement of Objects with Scene-Aware Diffusion Models](https://arxiv.org/abs/2506.21446)
*Mohamed Omran,Dimitris Kalatzis,Jens Petersen,Amirhossein Habibian,Auke Wiggers*

Main category: cs.CV

TL;DR: 提出了一种基于视觉地图和粗略对象掩码的高质量对象放置方法，解决了精确位置和方向控制的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在对象放置时需要精心设计的掩码或提示，难以实现精确的位置和方向控制。

Method: 设计了一种条件信号，结合视觉地图和粗略掩码，支持形状和方向变化，并基于修复模型保持背景不变。

Result: 在汽车场景中验证了方法的有效性，能够实现高质量的对象放置，包括非平凡形状变化和精确位置控制。

Conclusion: 该方法在对象放置任务中表现出色，结合了位置和外观控制，为图像编辑提供了更灵活的解决方案。

Abstract: Image editing approaches have become more powerful and flexible with the advent of powerful text-conditioned generative models. However, placing objects in an environment with a precise location and orientation still remains a challenge, as this typically requires carefully crafted inpainting masks or prompts. In this work, we show that a carefully designed visual map, combined with coarse object masks, is sufficient for high quality object placement. We design a conditioning signal that resolves ambiguities, while being flexible enough to allow for changing of shapes or object orientations. By building on an inpainting model, we leave the background intact by design, in contrast to methods that model objects and background jointly. We demonstrate the effectiveness of our method in the automotive setting, where we compare different conditioning signals in novel object placement tasks. These tasks are designed to measure edit quality not only in terms of appearance, but also in terms of pose and location accuracy, including cases that require non-trivial shape changes. Lastly, we show that fine location control can be combined with appearance control to place existing objects in precise locations in a scene.

</details>


### [32] [Rethinking Oversaturation in Classifier-Free Guidance via Low Frequency](https://arxiv.org/abs/2506.21452)
*Kaiyu Song,Hanjiang Lai*

Main category: cs.CV

TL;DR: 论文提出了一种基于低频信号的改进方法（LF-CFG），通过自适应阈值检测冗余信息并降低其影响，有效缓解了分类器自由引导（CFG）中的过饱和和不真实伪影问题。


<details>
  <summary>Details</summary>
Motivation: 高引导尺度在增强条件扩散模型性能的同时，常导致过饱和和不真实伪影。研究发现低频信号中冗余信息的积累是主要原因。

Method: 提出LF-CFG方法，通过自适应阈值定位冗余信息，并根据低频信息的变化率确定合理阈值，采用降权策略减少冗余信息的影响。

Result: 实验表明，LF-CFG在多种扩散模型（如Stable Diffusion-XL等）中有效缓解了过饱和和不真实伪影。

Conclusion: LF-CFG为分类器自由引导提供了一种改进方案，显著提升了生成图像的质量。

Abstract: Classifier-free guidance (CFG) succeeds in condition diffusion models that use a guidance scale to balance the influence of conditional and unconditional terms. A high guidance scale is used to enhance the performance of the conditional term. However, the high guidance scale often results in oversaturation and unrealistic artifacts. In this paper, we introduce a new perspective based on low-frequency signals, identifying the accumulation of redundant information in these signals as the key factor behind oversaturation and unrealistic artifacts. Building on this insight, we propose low-frequency improved classifier-free guidance (LF-CFG) to mitigate these issues. Specifically, we introduce an adaptive threshold-based measurement to pinpoint the locations of redundant information. We determine a reasonable threshold by analyzing the change rate of low-frequency information between prior and current steps. We then apply a down-weight strategy to reduce the impact of redundant information in the low-frequency signals. Experimental results demonstrate that LF-CFG effectively alleviates oversaturation and unrealistic artifacts across various diffusion models, including Stable Diffusion-XL, Stable Diffusion 2.1, 3.0, 3.5, and SiT-XL.

</details>


### [33] [MADrive: Memory-Augmented Driving Scene Modeling](https://arxiv.org/abs/2506.21520)
*Polina Karpikova,Daniil Selikhanovych,Kirill Struminsky,Ruslan Musaev,Maria Golitsyna,Dmitry Baranchuk*

Main category: cs.CV

TL;DR: MADrive框架通过外部记忆库替换场景中的车辆，提升自动驾驶环境重建的真实感和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅重建方法难以支持显著改变或新颖驾驶场景的逼真合成。

Method: 引入MADrive框架，利用MAD-Cars数据集检索相似车辆资产，通过方向对齐和重新光照集成到目标场景。

Result: 实验证明，替换后的车辆提供完整多视角表示，支持显著改变配置的逼真合成。

Conclusion: MADrive扩展了现有场景重建能力，为自动驾驶环境提供更灵活和逼真的合成解决方案。

Abstract: Recent advances in scene reconstruction have pushed toward highly realistic modeling of autonomous driving (AD) environments using 3D Gaussian splatting. However, the resulting reconstructions remain closely tied to the original observations and struggle to support photorealistic synthesis of significantly altered or novel driving scenarios. This work introduces MADrive, a memory-augmented reconstruction framework designed to extend the capabilities of existing scene reconstruction methods by replacing observed vehicles with visually similar 3D assets retrieved from a large-scale external memory bank. Specifically, we release MAD-Cars, a curated dataset of ${\sim}70$K 360{\deg} car videos captured in the wild and present a retrieval module that finds the most similar car instances in the memory bank, reconstructs the corresponding 3D assets from video, and integrates them into the target scene through orientation alignment and relighting. The resulting replacements provide complete multi-view representations of vehicles in the scene, enabling photorealistic synthesis of substantially altered configurations, as demonstrated in our experiments. Project page: https://yandex-research.github.io/madrive/

</details>


### [34] [DeOcc-1-to-3: 3D De-Occlusion from a Single Image via Self-Supervised Multi-View Diffusion](https://arxiv.org/abs/2506.21544)
*Yansong Qu,Shaohui Dai,Xinyang Li,Yuze Wang,You Shen,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: 提出了一种端到端的遮挡感知多视角生成框架，用于从单张部分遮挡图像生成六种结构一致的新视角，无需先验修复或手动标注。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的视角合成模型假设输入完全可见，无法处理遮挡情况，导致视角不一致和3D重建质量下降。

Method: 构建自监督训练流程，利用遮挡-未遮挡图像对和伪真实视角，联合学习修复和多视角生成。

Result: 提出了首个遮挡感知重建基准，涵盖多种遮挡程度、物体类别和掩码模式。

Conclusion: 该方法在遮挡情况下实现了结构一致的多视角生成，为未来研究提供了标准化评估协议。

Abstract: Reconstructing 3D objects from a single image is a long-standing challenge, especially under real-world occlusions. While recent diffusion-based view synthesis models can generate consistent novel views from a single RGB image, they generally assume fully visible inputs and fail when parts of the object are occluded. This leads to inconsistent views and degraded 3D reconstruction quality. To overcome this limitation, we propose an end-to-end framework for occlusion-aware multi-view generation. Our method directly synthesizes six structurally consistent novel views from a single partially occluded image, enabling downstream 3D reconstruction without requiring prior inpainting or manual annotations. We construct a self-supervised training pipeline using the Pix2Gestalt dataset, leveraging occluded-unoccluded image pairs and pseudo-ground-truth views to teach the model structure-aware completion and view consistency. Without modifying the original architecture, we fully fine-tune the view synthesis model to jointly learn completion and multi-view generation. Additionally, we introduce the first benchmark for occlusion-aware reconstruction, encompassing diverse occlusion levels, object categories, and mask patterns. This benchmark provides a standardized protocol for evaluating future methods under partial occlusions. Our code is available at https://github.com/Quyans/DeOcc123.

</details>


### [35] [SAM4D: Segment Anything in Camera and LiDAR Streams](https://arxiv.org/abs/2506.21547)
*Jianyun Xu,Song Wang,Ziqian Ni,Chunyong Hu,Sheng Yang,Jianke Zhu,Qiang Li*

Main category: cs.CV

TL;DR: SAM4D是一个多模态时序基础模型，支持相机和LiDAR流的可提示分割，通过UMPE和MCMA技术实现跨模态对齐和时序一致性，并利用自动化数据引擎高效生成伪标签。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶场景中相机和LiDAR数据跨模态分割的挑战，同时避免人工标注的瓶颈。

Method: 提出UMPE对齐多模态特征，MCMA增强时序一致性，并开发多模态自动化数据引擎生成伪标签。

Result: 在Waymo-4DSeg数据集上验证了SAM4D的强大跨模态分割能力和高效数据标注潜力。

Conclusion: SAM4D为自动驾驶场景提供了一种高效、鲁棒的跨模态分割解决方案。

Abstract: We present SAM4D, a multi-modal and temporal foundation model designed for promptable segmentation across camera and LiDAR streams. Unified Multi-modal Positional Encoding (UMPE) is introduced to align camera and LiDAR features in a shared 3D space, enabling seamless cross-modal prompting and interaction. Additionally, we propose Motion-aware Cross-modal Memory Attention (MCMA), which leverages ego-motion compensation to enhance temporal consistency and long-horizon feature retrieval, ensuring robust segmentation across dynamically changing autonomous driving scenes. To avoid annotation bottlenecks, we develop a multi-modal automated data engine that synergizes VFM-driven video masklets, spatiotemporal 4D reconstruction, and cross-modal masklet fusion. This framework generates camera-LiDAR aligned pseudo-labels at a speed orders of magnitude faster than human annotation while preserving VFM-derived semantic fidelity in point cloud representations. We conduct extensive experiments on the constructed Waymo-4DSeg, which demonstrate the powerful cross-modal segmentation ability and great potential in data annotation of proposed SAM4D.

</details>


### [36] [Whole-Body Conditioned Egocentric Video Prediction](https://arxiv.org/abs/2506.21552)
*Yutong Bai,Danny Tran,Amir Bar,Yann LeCun,Trevor Darrell,Jitendra Malik*

Main category: cs.CV

TL;DR: 论文提出了一种通过人体动作预测第一人称视角视频的方法（PEVA），利用过去视频和相对3D身体姿态作为输入，训练自回归条件扩散变换器。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索如何通过人体动作模拟第一人称视角下环境的变化，以解决复杂现实环境和具身行为的建模挑战。

Method: 方法包括基于Nymeria大规模数据集训练自回归条件扩散变换器，并结合人体关节层次结构对运动轨迹进行建模。

Result: 设计了分层评估协议，验证了模型在具身预测和控制能力上的表现。

Conclusion: 结论表明这是从人类视角建模复杂环境和具身行为的初步尝试。

Abstract: We train models to Predict Ego-centric Video from human Actions (PEVA), given the past video and an action represented by the relative 3D body pose. By conditioning on kinematic pose trajectories, structured by the joint hierarchy of the body, our model learns to simulate how physical human actions shape the environment from a first-person point of view. We train an auto-regressive conditional diffusion transformer on Nymeria, a large-scale dataset of real-world egocentric video and body pose capture. We further design a hierarchical evaluation protocol with increasingly challenging tasks, enabling a comprehensive analysis of the model's embodied prediction and control abilities. Our work represents an initial attempt to tackle the challenges of modeling complex real-world environments and embodied agent behaviors with video prediction from the perspective of a human.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [37] [ThermalDiffusion: Visual-to-Thermal Image-to-Image Translation for Autonomous Navigation](https://arxiv.org/abs/2506.20969)
*Shruti Bansal,Wenshan Wang,Yifei Liu,Parv Maheshwari*

Main category: cs.RO

TL;DR: 论文提出利用条件扩散模型将RGB图像转换为热成像图像，以解决热成像数据不足的问题，促进热成像相机在机器人领域的应用。


<details>
  <summary>Details</summary>
Motivation: 热成像相机在夜间或恶劣环境中能提供有价值的信息，但缺乏足够的数据限制了其在机器人领域的应用。

Method: 使用条件扩散模型和自注意力机制，将现有RGB图像转换为热成像图像。

Result: 通过合成热成像数据，解决了热成像数据不足的问题。

Conclusion: 该方法有助于推动热成像相机在机器人领域的广泛应用。

Abstract: Autonomous systems rely on sensors to estimate the environment around them. However, cameras, LiDARs, and RADARs have their own limitations. In nighttime or degraded environments such as fog, mist, or dust, thermal cameras can provide valuable information regarding the presence of objects of interest due to their heat signature. They make it easy to identify humans and vehicles that are usually at higher temperatures compared to their surroundings. In this paper, we focus on the adaptation of thermal cameras for robotics and automation, where the biggest hurdle is the lack of data. Several multi-modal datasets are available for driving robotics research in tasks such as scene segmentation, object detection, and depth estimation, which are the cornerstone of autonomous systems. However, they are found to be lacking in thermal imagery. Our paper proposes a solution to augment these datasets with synthetic thermal data to enable widespread and rapid adaptation of thermal cameras. We explore the use of conditional diffusion models to convert existing RGB images to thermal images using self-attention to learn the thermal properties of real-world objects.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [38] [Lightweight Physics-Informed Zero-Shot Ultrasound Plane Wave Denoising](https://arxiv.org/abs/2506.21499)
*Hojat Asgariandehkordi,Mostafa Sharifzadeh,Hassan Rivaz*

Main category: eess.IV

TL;DR: 提出一种零样本去噪框架，用于低角度CPWC成像，通过自监督学习增强对比度，无需额外训练数据。


<details>
  <summary>Details</summary>
Motivation: 解决CPWC成像中因角度增加导致的帧率下降和模糊问题，同时提升低角度采集时的去噪能力。

Method: 将传输角度分为两个子集，分别生成含噪声的复合图像，通过自监督残差学习训练轻量级模型，分离噪声与组织信号。

Result: 在仿真、体模和活体数据上表现优于传统和基于深度学习的去噪方法，提升对比度并保留结构。

Conclusion: 该方法无需领域特定微调或配对数据，适应性强，计算成本低，适用于多种解剖区域和采集设置。

Abstract: Ultrasound Coherent Plane Wave Compounding (CPWC) enhances image contrast by combining echoes from multiple steered transmissions. While increasing the number of angles generally improves image quality, it drastically reduces the frame rate and can introduce blurring artifacts in fast-moving targets. Moreover, compounded images remain susceptible to noise, particularly when acquired with a limited number of transmissions. We propose a zero-shot denoising framework tailored for low-angle CPWC acquisitions, which enhances contrast without relying on a separate training dataset. The method divides the available transmission angles into two disjoint subsets, each used to form compound images that include higher noise levels. The new compounded images are then used to train a deep model via a self-supervised residual learning scheme, enabling it to suppress incoherent noise while preserving anatomical structures. Because angle-dependent artifacts vary between the subsets while the underlying tissue response is similar, this physics-informed pairing allows the network to learn to disentangle the inconsistent artifacts from the consistent tissue signal. Unlike supervised methods, our model requires no domain-specific fine-tuning or paired data, making it adaptable across anatomical regions and acquisition setups. The entire pipeline supports efficient training with low computational cost due to the use of a lightweight architecture, which comprises only two convolutional layers. Evaluations on simulation, phantom, and in vivo data demonstrate superior contrast enhancement and structure preservation compared to both classical and deep learning-based denoising methods.

</details>
