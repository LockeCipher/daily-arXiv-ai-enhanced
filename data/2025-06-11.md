<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 6]
- [cs.CV](#cs.CV) [Total: 72]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.HC](#cs.HC) [Total: 3]
- [eess.IV](#eess.IV) [Total: 9]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Solving partial differential equations in participating media](https://arxiv.org/abs/2506.08237)
*Bailey Miller,Rohan Sawhney,Keenan Crane,Ioannis Gkioulekas*

Main category: cs.GR

TL;DR: 论文提出了一种基于参与介质的随机建模方法，用于解决复杂微粒几何中的偏微分方程（PDE）问题，并开发了两种新算法，实验证明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决复杂微粒几何中PDE模拟的难题，传统方法难以或无法显式建模。

Method: 将域视为参与介质，通过统计特性建模微粒几何，开发了两种新算法：volumetric walk on spheres和volumetric walk on stars。

Result: 新算法在拉普拉斯边界值问题中比传统方法（如集合平均和均匀化）更准确高效。

Conclusion: 提出的随机建模方法和算法为复杂微粒几何中的PDE模拟提供了高效且无离散化的解决方案。

Abstract: We consider the problem of solving partial differential equations (PDEs) in domains with complex microparticle geometry that is impractical, or intractable, to model explicitly. Drawing inspiration from volume rendering, we propose tackling this problem by treating the domain as a participating medium that models microparticle geometry stochastically, through aggregate statistical properties (e.g., particle density). We first introduce the problem setting of PDE simulation in participating media. We then specialize to exponential media and describe the properties that make them an attractive model of microparticle geometry for PDE simulation problems. We use these properties to develop two new algorithms, volumetric walk on spheres and volumetric walk on stars, that generalize previous Monte Carlo algorithms to enable efficient and discretization-free simulation of linear elliptic PDEs (e.g., Laplace) in participating media. We demonstrate experimentally that our algorithms can solve Laplace boundary value problems with complex microparticle geometry more accurately and more efficiently than previous approaches, such as ensemble averaging and homogenization.

</details>


### [2] [Complex-Valued Holographic Radiance Fields](https://arxiv.org/abs/2506.08350)
*Yicheng Zhan,Dong-Ha Shin,Seung-Hwan Baek,Kaan Akşit*

Main category: cs.GR

TL;DR: 提出了一种基于复值高斯基元的3D全息场景表示方法，显著提升了渲染速度。


<details>
  <summary>Details</summary>
Motivation: 为了支持全息显示中的物理真实渲染，需要一种能够同时建模光的振幅和相位的3D表示方法。

Method: 通过RGBD多视图图像，直接优化复值高斯基元作为3D全息场景表示，避免了计算昂贵的光栅重新优化。

Result: 相比现有方法，速度提升了30倍至10,000倍，同时保持图像质量。

Conclusion: 该方法为几何对齐、物理真实的3D全息场景表示迈出了第一步。

Abstract: Modeling the full properties of light, including both amplitude and phase, in 3D representations is crucial for advancing physically plausible rendering, particularly in holographic displays. To support these features, we propose a novel representation that optimizes 3D scenes without relying on intensity-based intermediaries. We reformulate 3D Gaussian splatting with complex-valued Gaussian primitives, expanding support for rendering with light waves. By leveraging RGBD multi-view images, our method directly optimizes complex-valued Gaussians as a 3D holographic scene representation. This eliminates the need for computationally expensive hologram re-optimization. Compared with state-of-the-art methods, our method achieves 30x-10,000x speed improvements while maintaining on-par image quality, representing a first step towards geometrically aligned, physically plausible holographic scene representations.

</details>


### [3] [Solving partial differential equations in participating media](https://arxiv.org/abs/2506.08237)
*Bailey Miller,Rohan Sawhney,Keenan Crane,Ioannis Gkioulekas*

Main category: cs.GR

TL;DR: 提出两种新算法，用于在复杂微粒子几何中高效求解线性椭圆偏微分方程。


<details>
  <summary>Details</summary>
Motivation: 解决在复杂微粒子几何中显式建模不切实际或不可行的偏微分方程求解问题。

Method: 将域视为参与介质，通过统计特性建模微粒子几何，并开发了两种新算法：volumetric walk on spheres和volumetric walk on stars。

Result: 实验证明新算法在求解Laplace边界值问题时，比现有方法更准确高效。

Conclusion: 新算法为复杂微粒子几何中的偏微分方程求解提供了高效且无离散化的解决方案。

Abstract: We consider the problem of solving partial differential equations (PDEs) in domains with complex microparticle geometry that is impractical, or intractable, to model explicitly. Drawing inspiration from volume rendering, we propose tackling this problem by treating the domain as a participating medium that models microparticle geometry stochastically, through aggregate statistical properties (e.g., particle density). We first introduce the problem setting of PDE simulation in participating media. We then specialize to exponential media and describe the properties that make them an attractive model of microparticle geometry for PDE simulation problems. We use these properties to develop two new algorithms, volumetric walk on spheres and volumetric walk on stars, that generalize previous Monte Carlo algorithms to enable efficient and discretization-free simulation of linear elliptic PDEs (e.g., Laplace) in participating media. We demonstrate experimentally that our algorithms can solve Laplace boundary value problems with complex microparticle geometry more accurately and more efficiently than previous approaches, such as ensemble averaging and homogenization.

</details>


### [4] [Complex-Valued Holographic Radiance Fields](https://arxiv.org/abs/2506.08350)
*Yicheng Zhan,Dong-Ha Shin,Seung-Hwan Baek,Kaan Akşit*

Main category: cs.GR

TL;DR: 提出了一种基于复值高斯基元的3D全息场景表示方法，显著提升了渲染速度。


<details>
  <summary>Details</summary>
Motivation: 为了在3D表示中完整建模光的振幅和相位特性，以支持物理真实的渲染（尤其是全息显示）。

Method: 通过RGBD多视角图像直接优化复值高斯基元，避免使用基于强度的中间表示。

Result: 相比现有方法，速度提升30x-10,000倍，同时保持图像质量。

Conclusion: 该方法为几何对齐、物理真实的3D全息场景表示提供了初步解决方案。

Abstract: Modeling the full properties of light, including both amplitude and phase, in 3D representations is crucial for advancing physically plausible rendering, particularly in holographic displays. To support these features, we propose a novel representation that optimizes 3D scenes without relying on intensity-based intermediaries. We reformulate 3D Gaussian splatting with complex-valued Gaussian primitives, expanding support for rendering with light waves. By leveraging RGBD multi-view images, our method directly optimizes complex-valued Gaussians as a 3D holographic scene representation. This eliminates the need for computationally expensive hologram re-optimization. Compared with state-of-the-art methods, our method achieves 30x-10,000x speed improvements while maintaining on-par image quality, representing a first step towards geometrically aligned, physically plausible holographic scene representations.

</details>


### [5] [Solving partial differential equations in participating media](https://arxiv.org/abs/2506.08237)
*Bailey Miller,Rohan Sawhney,Keenan Crane,Ioannis Gkioulekas*

Main category: cs.GR

TL;DR: 提出两种新算法（volumetric walk on spheres和volumetric walk on stars），用于在复杂微粒子几何域中高效求解线性椭圆PDE，比现有方法更准确高效。


<details>
  <summary>Details</summary>
Motivation: 解决复杂微粒子几何域中PDE求解的难题，避免显式建模的不可行性。

Method: 将域视为参与介质，通过统计特性建模微粒子几何，并基于指数介质的特性开发新算法。

Result: 新算法在求解Laplace边界值问题时，比传统方法（如集合平均和均匀化）更准确高效。

Conclusion: 提出的算法为复杂几何域中的PDE求解提供了高效且无离散化的解决方案。

Abstract: We consider the problem of solving partial differential equations (PDEs) in domains with complex microparticle geometry that is impractical, or intractable, to model explicitly. Drawing inspiration from volume rendering, we propose tackling this problem by treating the domain as a participating medium that models microparticle geometry stochastically, through aggregate statistical properties (e.g., particle density). We first introduce the problem setting of PDE simulation in participating media. We then specialize to exponential media and describe the properties that make them an attractive model of microparticle geometry for PDE simulation problems. We use these properties to develop two new algorithms, volumetric walk on spheres and volumetric walk on stars, that generalize previous Monte Carlo algorithms to enable efficient and discretization-free simulation of linear elliptic PDEs (e.g., Laplace) in participating media. We demonstrate experimentally that our algorithms can solve Laplace boundary value problems with complex microparticle geometry more accurately and more efficiently than previous approaches, such as ensemble averaging and homogenization.

</details>


### [6] [Complex-Valued Holographic Radiance Fields](https://arxiv.org/abs/2506.08350)
*Yicheng Zhan,Dong-Ha Shin,Seung-Hwan Baek,Kaan Akşit*

Main category: cs.GR

TL;DR: 提出了一种基于复值高斯基元的3D全息场景表示方法，显著提升了渲染速度。


<details>
  <summary>Details</summary>
Motivation: 为了在3D表示中完整建模光的振幅和相位特性，以支持物理真实的渲染和全息显示。

Method: 通过复值高斯基元重新定义3D高斯泼溅，并利用RGBD多视角图像直接优化复值高斯基元作为3D全息场景表示。

Result: 相比现有方法，速度提升30倍至10,000倍，同时保持图像质量。

Conclusion: 该方法为几何对齐且物理真实的3D全息场景表示提供了初步解决方案。

Abstract: Modeling the full properties of light, including both amplitude and phase, in 3D representations is crucial for advancing physically plausible rendering, particularly in holographic displays. To support these features, we propose a novel representation that optimizes 3D scenes without relying on intensity-based intermediaries. We reformulate 3D Gaussian splatting with complex-valued Gaussian primitives, expanding support for rendering with light waves. By leveraging RGBD multi-view images, our method directly optimizes complex-valued Gaussians as a 3D holographic scene representation. This eliminates the need for computationally expensive hologram re-optimization. Compared with state-of-the-art methods, our method achieves 30x-10,000x speed improvements while maintaining on-par image quality, representing a first step towards geometrically aligned, physically plausible holographic scene representations.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [7] [CuRe: Cultural Gaps in the Long Tail of Text-to-Image Systems](https://arxiv.org/abs/2506.08071)
*Aniket Rege,Zinnia Nie,Mahesh Ramesh,Unmesh Raskar,Zhuoran Yu,Aditya Kusupati,Yong Jae Lee,Ramya Korlakai Vinayak*

Main category: cs.CV

TL;DR: CuRe是一个用于评估文本到图像（T2I）系统文化代表性的新基准和评分套件，通过分析系统对文本条件增加的响应来衡量文化多样性。


<details>
  <summary>Details</summary>
Motivation: 当前T2I系统训练数据偏向欧美文化，缺乏对全球南方文化的代表性，CuRe旨在量化并解决这一问题。

Method: 利用Wikimedia知识图谱构建包含300种文化物品的层次化数据集，通过分析T2I系统对文本条件增加的响应来评估文化代表性。

Result: CuRe评分与人类对感知相似性、图文对齐和文化多样性的判断高度相关，适用于多种图像编码器和T2I系统。

Conclusion: CuRe为T2I系统的文化多样性评估提供了可扩展且细粒度的工具，数据集和代码已开源。

Abstract: Popular text-to-image (T2I) systems are trained on web-scraped data, which is heavily Amero and Euro-centric, underrepresenting the cultures of the Global South. To analyze these biases, we introduce CuRe, a novel and scalable benchmarking and scoring suite for cultural representativeness that leverages the marginal utility of attribute specification to T2I systems as a proxy for human judgments. Our CuRe benchmark dataset has a novel categorical hierarchy built from the crowdsourced Wikimedia knowledge graph, with 300 cultural artifacts across 32 cultural subcategories grouped into six broad cultural axes (food, art, fashion, architecture, celebrations, and people). Our dataset's categorical hierarchy enables CuRe scorers to evaluate T2I systems by analyzing their response to increasing the informativeness of text conditioning, enabling fine-grained cultural comparisons. We empirically observe much stronger correlations of our class of scorers to human judgments of perceptual similarity, image-text alignment, and cultural diversity across image encoders (SigLIP 2, AIMV2 and DINOv2), vision-language models (OpenCLIP, SigLIP 2, Gemini 2.0 Flash) and state-of-the-art text-to-image systems, including three variants of Stable Diffusion (1.5, XL, 3.5 Large), FLUX.1 [dev], Ideogram 2.0, and DALL-E 3. The code and dataset is open-sourced and available at https://aniketrege.github.io/cure/.

</details>


### [8] [Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion Models in a Vision-Language-Action Framework](https://arxiv.org/abs/2506.08185)
*Huixin Zhan,Jason H. Moore*

Main category: cs.CV

TL;DR: 本文提出了一种基于离散扩散框架和视觉-语言-动作（VLA）流程的新方法，用于建模外科医生在机器人手术中的个性化风格指纹，并探讨了其隐私风险。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统常忽略外科医生的个性化操作风格，而本文旨在通过多模态输入（如内窥镜视频、手术意图语言和隐私感知的身份嵌入）来捕捉这些差异。

Method: 采用离散扩散框架，将手势预测建模为结构化序列去噪任务，结合视觉、语言和动作输入，并通过自然语言提示编码个性化风格。

Result: 在JIGSAWS数据集上验证了方法的有效性，能够准确重建手势序列并学习到每位外科医生的独特运动指纹，但也发现更个性化的嵌入会增加身份泄露风险。

Conclusion: 个性化嵌入虽能提升任务性能，但也增加了隐私风险，需在手术建模中平衡个性化与隐私保护。

Abstract: Surgeons exhibit distinct operating styles due to differences in training, experience, and motor behavior - yet current AI systems often ignore this personalization signal. We propose a novel approach to model fine-grained, surgeon-specific fingerprinting in robotic surgery using a discrete diffusion framework integrated with a vision-language-action (VLA) pipeline. Our method formulates gesture prediction as a structured sequence denoising task, conditioned on multimodal inputs including endoscopic video, surgical intent language, and a privacy-aware embedding of surgeon identity and skill. Personalized surgeon fingerprinting is encoded through natural language prompts using third-party language models, allowing the model to retain individual behavioral style without exposing explicit identity. We evaluate our method on the JIGSAWS dataset and demonstrate that it accurately reconstructs gesture sequences while learning meaningful motion fingerprints unique to each surgeon. To quantify the privacy implications of personalization, we perform membership inference attacks and find that more expressive embeddings improve task performance but simultaneously increase susceptibility to identity leakage. These findings demonstrate that while personalized embeddings improve performance, they also increase vulnerability to identity leakage, revealing the importance of balancing personalization with privacy risk in surgical modeling. Code is available at: https://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting.

</details>


### [9] [A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation](https://arxiv.org/abs/2506.08210)
*Andrew Z. Wang,Songwei Ge,Tero Karras,Ming-Yu Liu,Yogesh Balaji*

Main category: cs.CV

TL;DR: 研究探讨了使用现代仅解码器LLM作为文本编码器在文本到图像扩散模型中的效果，发现通过层归一化平均嵌入优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型仍使用过时的T5和CLIP作为文本编码器，研究旨在评估现代LLM的潜力。

Method: 构建标准化训练和评估流程，训练27个模型，比较12种文本编码器，分析嵌入提取方法、LLM变体和模型大小的影响。

Result: 使用最后一层嵌入效果较差，而层归一化平均嵌入显著提升复杂提示的对齐性，多数LLM表现优于T5基线。

Conclusion: 现代LLM作为文本编码器能提升文本到图像生成性能，尤其在复杂视觉语言推理任务中。

Abstract: Both text-to-image generation and large language models (LLMs) have made significant advancements. However, many text-to-image models still employ the somewhat outdated T5 and CLIP as their text encoders. In this work, we investigate the effectiveness of using modern decoder-only LLMs as text encoders for text-to-image diffusion models. We build a standardized training and evaluation pipeline that allows us to isolate and evaluate the effect of different text embeddings. We train a total of 27 text-to-image models with 12 different text encoders to analyze the critical aspects of LLMs that could impact text-to-image generation, including the approaches to extract embeddings, different LLMs variants, and model sizes. Our experiments reveal that the de facto way of using last-layer embeddings as conditioning leads to inferior performance. Instead, we explore embeddings from various layers and find that using layer-normalized averaging across all layers significantly improves alignment with complex prompts. Most LLMs with this conditioning outperform the baseline T5 model, showing enhanced performance in advanced visio-linguistic reasoning skills.

</details>


### [10] [Highly Compressed Tokenizer Can Generate Without Training](https://arxiv.org/abs/2506.08257)
*L. Lao Beyer,T. Li,X. Chen,S. Karaman,K. He*

Main category: cs.CV

TL;DR: 1D图像分词器通过高度压缩的图像表示实现启发式编辑和生成能力，无需训练生成模型。


<details>
  <summary>Details</summary>
Motivation: 探索1D图像分词器在高度压缩下的潜在空间表达能力，以实现图像编辑和生成。

Method: 利用基于梯度的测试时优化和即插即用的损失函数（如重建或CLIP相似性）构建图像生成流程。

Result: 实现了细粒度图像编辑（如外观和语义属性转移）和多样化、逼真的样本生成。

Conclusion: 1D图像分词器在图像编辑和生成任务中表现出高效性和灵活性，无需依赖生成模型训练。

Abstract: Commonly used image tokenizers produce a 2D grid of spatially arranged tokens. In contrast, so-called 1D image tokenizers represent images as highly compressed one-dimensional sequences of as few as 32 discrete tokens. We find that the high degree of compression achieved by a 1D tokenizer with vector quantization enables image editing and generative capabilities through heuristic manipulation of tokens, demonstrating that even very crude manipulations -- such as copying and replacing tokens between latent representations of images -- enable fine-grained image editing by transferring appearance and semantic attributes. Motivated by the expressivity of the 1D tokenizer's latent space, we construct an image generation pipeline leveraging gradient-based test-time optimization of tokens with plug-and-play loss functions such as reconstruction or CLIP similarity. Our approach is demonstrated for inpainting and text-guided image editing use cases, and can generate diverse and realistic samples without requiring training of any generative model.

</details>


### [11] [Seeing Voices: Generating A-Roll Video from Audio with Mirage](https://arxiv.org/abs/2506.08279)
*Aditi Sundararaman,Amogh Adishesha,Andrew Jaegle,Dan Bigioi,Hyoung-Kyu Song,Jon Kyl,Justin Mao,Kevin Lan,Mojtaba Komeili,ShahRukh Athar,Sheila Babayan,Stanislau Beliasau,William Buchwalter*

Main category: cs.CV

TL;DR: Mirage是一个音频到视频的基础模型，能够根据音频输入生成逼真、富有表现力的视频。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法要么忽略声音专注于无声图像序列生成，要么局限于特定应用领域（如重新配音），缺乏通用的音频到视频生成解决方案。

Method: 提出了一种统一的训练方法，基于自注意力机制，支持从头训练或基于现有权重训练，适用于音频到视频生成。

Result: Mirage生成的视频在主观质量上优于其他方法，能够根据音频输入生成逼真的视频内容。

Conclusion: Mirage为音频到视频生成提供了一种通用且高质量的解决方案，尤其在语音相关视频生成中表现突出。

Abstract: From professional filmmaking to user-generated content, creators and consumers have long recognized that the power of video depends on the harmonious integration of what we hear (the video's audio track) with what we see (the video's image sequence). Current approaches to video generation either ignore sound to focus on general-purpose but silent image sequence generation or address both visual and audio elements but focus on restricted application domains such as re-dubbing. We introduce Mirage, an audio-to-video foundation model that excels at generating realistic, expressive output imagery from scratch given an audio input. When integrated with existing methods for speech synthesis (text-to-speech, or TTS), Mirage results in compelling multimodal video. When trained on audio-video footage of people talking (A-roll) and conditioned on audio containing speech, Mirage generates video of people delivering a believable interpretation of the performance implicit in input audio. Our central technical contribution is a unified method for training self-attention-based audio-to-video generation models, either from scratch or given existing weights. This methodology allows Mirage to retain generality as an approach to audio-to-video generation while producing outputs of superior subjective quality to methods that incorporate audio-specific architectures or loss components specific to people, speech, or details of how images or audio are captured. We encourage readers to watch and listen to the results of Mirage for themselves (see paper and comments for links).

</details>


### [12] [How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models](https://arxiv.org/abs/2506.08351)
*Huixuan Zhang,Junzhe Zhang,Xiaojun Wan*

Main category: cs.CV

TL;DR: 本文提出了一种名为Step AG的自适应引导策略，通过限制分类器自由引导的使用步骤，显著提升了文本到图像生成模型的效率，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 分类器自由引导方法在文本到视觉生成扩散模型中普遍使用，但其需要两倍于无条件生成的步骤，导致成本显著增加。现有自适应引导方法缺乏分析和实证支持，无法通用。

Method: 提出Step AG策略，将分类器自由引导限制在前几个去噪步骤中，以减少计算成本。

Result: 实验表明，该方法在图像质量和文本对齐方面表现良好，平均提速20%至30%，且适用于不同设置和模型。

Conclusion: Step AG是一种简单通用的自适应引导策略，显著提升了生成效率，适用于多种扩散模型。

Abstract: With the rapid development of text-to-vision generation diffusion models, classifier-free guidance has emerged as the most prevalent method for conditioning. However, this approach inherently requires twice as many steps for model forwarding compared to unconditional generation, resulting in significantly higher costs. While previous study has introduced the concept of adaptive guidance, it lacks solid analysis and empirical results, making previous method unable to be applied to general diffusion models. In this work, we present another perspective of applying adaptive guidance and propose Step AG, which is a simple, universally applicable adaptive guidance strategy. Our evaluations focus on both image quality and image-text alignment. whose results indicate that restricting classifier-free guidance to the first several denoising steps is sufficient for generating high-quality, well-conditioned images, achieving an average speedup of 20% to 30%. Such improvement is consistent across different settings such as inference steps, and various models including video generation models, highlighting the superiority of our method.

</details>


### [13] [Enhancing Motion Dynamics of Image-to-Video Models via Adaptive Low-Pass Guidance](https://arxiv.org/abs/2506.08456)
*June Suk Choi,Kyungmin Lee,Sihyun Yu,Yisol Choi,Jinwoo Shin,Kimin Lee*

Main category: cs.CV

TL;DR: 论文提出自适应低通引导（ALG）方法，解决图像到视频（I2V）生成中动态性不足的问题，显著提升视频动态性。


<details>
  <summary>Details</summary>
Motivation: 现有I2V方法因过早暴露高频细节导致生成视频动态性不足，ALG旨在解决这一问题。

Method: ALG在去噪早期阶段自适应调节条件图像频率内容，应用低通滤波。

Result: ALG显著提升视频动态性（VBench-I2V测试中动态度平均提升36%），同时保持图像质量和文本对齐。

Conclusion: ALG是一种简单有效的改进方法，能显著提升I2V模型的动态性表现。

Abstract: Recent text-to-video (T2V) models have demonstrated strong capabilities in producing high-quality, dynamic videos. To improve the visual controllability, recent works have considered fine-tuning pre-trained T2V models to support image-to-video (I2V) generation. However, such adaptation frequently suppresses motion dynamics of generated outputs, resulting in more static videos compared to their T2V counterparts. In this work, we analyze this phenomenon and identify that it stems from the premature exposure to high-frequency details in the input image, which biases the sampling process toward a shortcut trajectory that overfits to the static appearance of the reference image. To address this, we propose adaptive low-pass guidance (ALG), a simple fix to the I2V model sampling procedure to generate more dynamic videos without compromising per-frame image quality. Specifically, ALG adaptively modulates the frequency content of the conditioning image by applying low-pass filtering at the early stage of denoising. Extensive experiments demonstrate that ALG significantly improves the temporal dynamics of generated videos, while preserving image fidelity and text alignment. Especially, under VBench-I2V test suite, ALG achieves an average improvement of 36% in dynamic degree without a significant drop in video quality or image fidelity.

</details>


### [14] [LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid Temporal Modeling with Only 4$\times$RTX 4090s](https://arxiv.org/abs/2506.08529)
*Xijun Wang,Xin Li,Bingchen Li,Zhibo Chen*

Main category: cs.CV

TL;DR: LiftVSR是一种高效的视频超分辨率框架，通过结合动态时间注意力和注意力记忆缓存，显著降低了计算成本，同时保持长期一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在时间一致性和计算成本方面存在问题，尤其是长视频处理需要大量GPU资源。

Method: 提出混合时间建模机制，包括动态时间注意力（DTA）和注意力记忆缓存（AMC），并结合非对称采样策略。

Result: 在多个VSR基准测试中表现出色，计算成本显著降低。

Conclusion: LiftVSR在效率和性能上取得了平衡，为视频超分辨率提供了实用解决方案。

Abstract: Diffusion models have significantly advanced video super-resolution (VSR) by enhancing perceptual quality, largely through elaborately designed temporal modeling to ensure inter-frame consistency. However, existing methods usually suffer from limited temporal coherence and prohibitively high computational costs (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for long videos. In this work, we propose LiftVSR, an efficient VSR framework that leverages and elevates the image-wise diffusion prior from PixArt-$\alpha$, achieving state-of-the-art results using only 4$\times$RTX 4090 GPUs. To balance long-term consistency and efficiency, we introduce a hybrid temporal modeling mechanism that decomposes temporal learning into two complementary components: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal modeling within short frame segment ($\textit{i.e.}$, low complexity), and (ii) Attention Memory Cache (AMC) for long-term temporal modeling across segments ($\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token flows across frames within multi-head query and key tokens to warp inter-frame contexts in the value tokens. AMC adaptively aggregates historical segment information via a cache unit, ensuring long-term coherence with minimal overhead. To further stabilize the cache interaction during inference, we introduce an asymmetric sampling strategy that mitigates feature mismatches arising from different diffusion sampling steps. Extensive experiments on several typical VSR benchmarks have demonstrated that LiftVSR achieves impressive performance with significantly lower computational costs.

</details>


### [15] [A Probability-guided Sampler for Neural Implicit Surface Rendering](https://arxiv.org/abs/2506.08619)
*Gonçalo Dias Pais,Valter Piedade,Moitreya Chatterjee,Marcus Greiff,Pedro Miraldo*

Main category: cs.CV

TL;DR: 该论文提出了一种基于隐式表面表示和3D图像投影空间概率密度函数的采样策略，结合新的表面重建损失函数，显著提升了3D重建和图像渲染的精度。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF方法因可扩展性问题无法对所有可能的输入数据进行训练，导致采样效率低下。论文旨在通过更针对性的采样策略和新的损失函数提升渲染效果。

Method: 利用隐式表面表示和3D图像投影空间的概率密度函数进行针对性采样，并提出新的表面重建损失函数，结合近表面和空白空间信息。

Result: 在现有神经隐式表面渲染器中集成新方法后，3D重建和图像渲染的精度显著提升，尤其是对场景中的感兴趣区域。

Conclusion: 论文提出的采样策略和损失函数有效提升了渲染和重建的精度，为NeRF方法的进一步优化提供了新思路。

Abstract: Several variants of Neural Radiance Fields (NeRFs) have significantly improved the accuracy of synthesized images and surface reconstruction of 3D scenes/objects. In all of these methods, a key characteristic is that none can train the neural network with every possible input data, specifically, every pixel and potential 3D point along the projection rays due to scalability issues. While vanilla NeRFs uniformly sample both the image pixels and 3D points along the projection rays, some variants focus only on guiding the sampling of the 3D points along the projection rays. In this paper, we leverage the implicit surface representation of the foreground scene and model a probability density function in a 3D image projection space to achieve a more targeted sampling of the rays toward regions of interest, resulting in improved rendering. Additionally, a new surface reconstruction loss is proposed for improved performance. This new loss fully explores the proposed 3D image projection space model and incorporates near-to-surface and empty space components. By integrating our novel sampling strategy and novel loss into current state-of-the-art neural implicit surface renderers, we achieve more accurate and detailed 3D reconstructions and improved image rendering, especially for the regions of interest in any given scene.

</details>


### [16] [RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping](https://arxiv.org/abs/2506.08632)
*Yang Bai,Liudi Yang,George Eskandar,Fengyi Shen,Dong Chen,Mohammad Altillawi,Ziyuan Liu,Gitta Kutyniok*

Main category: cs.CV

TL;DR: RoboSwap提出了一种基于GAN和扩散模型的视频编辑框架，用于在未配对数据中交换机械臂，解决了跨平台机器人学习中的数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 解决视频条件下机器人学习中数据稀缺和跨平台泛化能力不足的问题。

Method: 结合GAN和扩散模型，分段处理机械臂与背景，通过未配对GAN模型翻译机械臂，再用扩散模型增强一致性和运动真实性。

Result: 在三个基准测试中，RoboSwap在结构一致性和运动一致性上优于现有视频和图像编辑模型。

Conclusion: RoboSwap为机器人学习提供了可靠的跨平台数据生成解决方案。

Abstract: Recent advancements in generative models have revolutionized video synthesis and editing. However, the scarcity of diverse, high-quality datasets continues to hinder video-conditioned robotic learning, limiting cross-platform generalization. In this work, we address the challenge of swapping a robotic arm in one video with another: a key step for crossembodiment learning. Unlike previous methods that depend on paired video demonstrations in the same environmental settings, our proposed framework, RoboSwap, operates on unpaired data from diverse environments, alleviating the data collection needs. RoboSwap introduces a novel video editing pipeline integrating both GANs and diffusion models, combining their isolated advantages. Specifically, we segment robotic arms from their backgrounds and train an unpaired GAN model to translate one robotic arm to another. The translated arm is blended with the original video background and refined with a diffusion model to enhance coherence, motion realism and object interaction. The GAN and diffusion stages are trained independently. Our experiments demonstrate that RoboSwap outperforms state-of-the-art video and image editing models on three benchmarks in terms of both structural coherence and motion consistency, thereby offering a robust solution for generating reliable, cross-embodiment data in robotic learning.

</details>


### [17] [Orientation Matters: Making 3D Generative Models Orientation-Aligned](https://arxiv.org/abs/2506.08640)
*Yichong Lu,Yuzhuo Tian,Zijin Jiang,Yikun Zhao,Yuanbo Yang,Hao Ouyang,Haoji Hu,Huimin Yu,Yujun Shen,Yiyi Liao*

Main category: cs.CV

TL;DR: 论文提出了一种解决3D生成模型方向不一致问题的方法，通过构建对齐数据集和微调模型，实现了方向一致的3D对象生成。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成模型因训练数据不一致导致生成结果方向不对齐，限制了其在下游任务中的应用。

Method: 构建Objaverse-OA数据集（14,832个对齐3D模型），并基于多视图扩散和3D变分自编码器框架微调生成模型。

Result: 实验表明该方法优于后处理对齐方法，并展示了在下游任务（如零样本方向估计和高效旋转操作）中的应用。

Conclusion: 通过方向对齐的3D生成，解决了现有模型的局限性，并拓展了下游任务的可能性。

Abstract: Humans intuitively perceive object shape and orientation from a single image, guided by strong priors about canonical poses. However, existing 3D generative models often produce misaligned results due to inconsistent training data, limiting their usability in downstream tasks. To address this gap, we introduce the task of orientation-aligned 3D object generation: producing 3D objects from single images with consistent orientations across categories. To facilitate this, we construct Objaverse-OA, a dataset of 14,832 orientation-aligned 3D models spanning 1,008 categories. Leveraging Objaverse-OA, we fine-tune two representative 3D generative models based on multi-view diffusion and 3D variational autoencoder frameworks to produce aligned objects that generalize well to unseen objects across various categories. Experimental results demonstrate the superiority of our method over post-hoc alignment approaches. Furthermore, we showcase downstream applications enabled by our aligned object generation, including zero-shot object orientation estimation via analysis-by-synthesis and efficient arrow-based object rotation manipulation.

</details>


### [18] [TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary Large-Scale Scene Rendering](https://arxiv.org/abs/2506.08704)
*Xiaohan Zhang,Sitong Wang,Yushen Yan,Yi Yang,Mingda Xu,Qi Liu*

Main category: cs.CV

TL;DR: TraGraph-GS提出了一种基于轨迹图的空间分区方法，解决了大规模场景中视图合成的泛化问题，显著提升了渲染质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在分区和合并时存在泛化能力不足和纹理失真的问题，无法适应任意相机轨迹。

Method: 采用轨迹图进行空间分区，引入正则化约束和渐进渲染策略以减少高斯重叠和提升纹理渲染。

Result: 在四个空中和四个地面数据集上表现优异，PSNR平均提升1.86 dB（空中）和1.62 dB（地面）。

Conclusion: TraGraph-GS通过创新方法有效解决了大规模场景渲染的挑战，显著优于现有技术。

Abstract: High-quality novel view synthesis for large-scale scenes presents a challenging dilemma in 3D computer vision. Existing methods typically partition large scenes into multiple regions, reconstruct a 3D representation using Gaussian splatting for each region, and eventually merge them for novel view rendering. They can accurately render specific scenes, yet they do not generalize effectively for two reasons: (1) rigid spatial partition techniques struggle with arbitrary camera trajectories, and (2) the merging of regions results in Gaussian overlap to distort texture details. To address these challenges, we propose TraGraph-GS, leveraging a trajectory graph to enable high-precision rendering for arbitrarily large-scale scenes. We present a spatial partitioning method for large-scale scenes based on graphs, which incorporates a regularization constraint to enhance the rendering of textures and distant objects, as well as a progressive rendering strategy to mitigate artifacts caused by Gaussian overlap. Experimental results demonstrate its superior performance both on four aerial and four ground datasets and highlight its remarkable efficiency: our method achieves an average improvement of 1.86 dB in PSNR on aerial datasets and 1.62 dB on ground datasets compared to state-of-the-art approaches.

</details>


### [19] [SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting](https://arxiv.org/abs/2506.08710)
*Mengjiao Ma,Qi Ma,Yue Li,Jiahuan Cheng,Runyi Yang,Bin Ren,Nikola Popovic,Mingqiang Wei,Nicu Sebe,Luc Van Gool,Theo Gevers,Martin R. Oswald,Danda Pani Paudel*

Main category: cs.CV

TL;DR: 3D高斯泼溅（3DGS）是一种高效编码场景几何、外观和语义的方法。本文提出首个大规模基准测试，评估三种语言高斯泼溅方法在3D空间中的表现，并引入GaussianWorld-49K数据集，展示通用方法的优势。


<details>
  <summary>Details</summary>
Motivation: 现有方法多基于2D视图评估，缺乏对整体3D理解的深入洞察，因此需要直接评估3D空间中的方法性能。

Method: 提出大规模基准测试，评估三类方法（基于场景优化、无优化和通用方法）在1060个场景中的表现，并引入49K场景数据集。

Result: 通用方法在放松场景限制、快速推理和分割性能上表现优越。

Conclusion: 通用方法结合大数据集能显著提升3DGS场景理解，相关资源将公开以推动研究。

Abstract: 3D Gaussian Splatting (3DGS) serves as a highly performant and efficient encoding of scene geometry, appearance, and semantics. Moreover, grounding language in 3D scenes has proven to be an effective strategy for 3D scene understanding. Current Language Gaussian Splatting line of work fall into three main groups: (i) per-scene optimization-based, (ii) per-scene optimization-free, and (iii) generalizable approach. However, most of them are evaluated only on rendered 2D views of a handful of scenes and viewpoints close to the training views, limiting ability and insight into holistic 3D understanding. To address this gap, we propose the first large-scale benchmark that systematically assesses these three groups of methods directly in 3D space, evaluating on 1060 scenes across three indoor datasets and one outdoor dataset. Benchmark results demonstrate a clear advantage of the generalizable paradigm, particularly in relaxing the scene-specific limitation, enabling fast feed-forward inference on novel scenes, and achieving superior segmentation performance. We further introduce GaussianWorld-49K a carefully curated 3DGS dataset comprising around 49K diverse indoor and outdoor scenes obtained from multiple sources, with which we demonstrate the generalizable approach could harness strong data priors. Our codes, benchmark, and datasets will be made public to accelerate research in generalizable 3DGS scene understanding.

</details>


### [20] [Gaussian2Scene: 3D Scene Representation Learning via Self-supervised Learning with 3D Gaussian Splatting](https://arxiv.org/abs/2506.08777)
*Keyi Liu,Weidong Yang,Ben Fei,Ying He*

Main category: cs.CV

TL;DR: Gaussian2Scene是一种新型的自监督学习框架，利用3D高斯泼溅（3DGS）进行点云预训练，解决了现有方法在计算和几何结构捕捉上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法依赖隐式场景表示和高内存需求，且重建目标仅适用于2D空间，无法充分捕捉3D几何结构。

Method: 采用两阶段训练策略：第一阶段通过双分支掩码自编码器学习2D和3D场景表示；第二阶段利用重建点云和高斯基元的几何位置进行监督学习。

Result: 在多个下游3D物体检测任务中表现优于现有预训练方法。

Conclusion: Gaussian2Scene通过3DGS提升了计算效率和几何理解能力，为3D视觉任务提供了更有效的预训练框架。

Abstract: Self-supervised learning (SSL) for point cloud pre-training has become a cornerstone for many 3D vision tasks, enabling effective learning from large-scale unannotated data. At the scene level, existing SSL methods often incorporate volume rendering into the pre-training framework, using RGB-D images as reconstruction signals to facilitate cross-modal learning. This strategy promotes alignment between 2D and 3D modalities and enables the model to benefit from rich visual cues in the RGB-D inputs. However, these approaches are limited by their reliance on implicit scene representations and high memory demands. Furthermore, since their reconstruction objectives are applied only in 2D space, they often fail to capture underlying 3D geometric structures. To address these challenges, we propose Gaussian2Scene, a novel scene-level SSL framework that leverages the efficiency and explicit nature of 3D Gaussian Splatting (3DGS) for pre-training. The use of 3DGS not only alleviates the computational burden associated with volume rendering but also supports direct 3D scene reconstruction, thereby enhancing the geometric understanding of the backbone network. Our approach follows a progressive two-stage training strategy. In the first stage, a dual-branch masked autoencoder learns both 2D and 3D scene representations. In the second stage, we initialize training with reconstructed point clouds and further supervise learning using the geometric locations of Gaussian primitives and rendered RGB images. This process reinforces both geometric and cross-modal learning. We demonstrate the effectiveness of Gaussian2Scene across several downstream 3D object detection tasks, showing consistent improvements over existing pre-training methods.

</details>


### [21] [A PDE-Based Image Dehazing Method via Atmospheric Scattering Theory](https://arxiv.org/abs/2506.08793)
*Zhuoran Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种基于偏微分方程（PDE）的单幅图像去雾方法，结合大气散射模型、非局部正则化和暗通道先验，改进了PDE框架，并证明了弱解的存在性和唯一性。实验表明该方法是一种有效的去雾解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有的去雾方法在处理复杂场景时效果有限，因此需要一种更高效且理论完备的PDE框架来提升去雾性能。

Method: 提出了一种改进的PDE框架，结合边缘保持扩散系数、高斯卷积算子和自适应正则化参数，并通过固定点迭代方案实现高效计算。

Result: 实验证明该方法在去雾任务中表现优异，且可推广至深度学习模型。

Conclusion: 该方法为单幅图像去雾提供了一种理论完备且高效的解决方案，具有广泛的应用潜力。

Abstract: This paper presents a novel partial differential equation (PDE) framework for single-image dehazing. By integrating the atmospheric scattering model with nonlocal regularization and dark channel prior, we propose the improved PDE: \[ -\text{div}\left(D(\nabla u)\nabla u\right) + \lambda(t) G(u) = \Phi(I,t,A) \] where $D(\nabla u) = (|\nabla u| + \epsilon)^{-1}$ is the edge-preserving diffusion coefficient, $G(u)$ is the Gaussian convolution operator, and $\lambda(t)$ is the adaptive regularization parameter based on transmission map $t$. We prove the existence and uniqueness of weak solutions in $H_0^1(\Omega)$ using Lax-Milgram theorem, and implement an efficient fixed-point iteration scheme accelerated by PyTorch GPU computation. The experimental results demonstrate that this method is a promising deghazing solution that can be generalized to the deep model paradigm.

</details>


### [22] [HunyuanVideo-HOMA: Generic Human-Object Interaction in Multimodal Driven Human Animation](https://arxiv.org/abs/2506.08797)
*Ziyao Huang,Zixiang Zhou,Juan Cao,Yifeng Ma,Yi Chen,Zejing Rao,Zhiyong Xu,Hongmei Wang,Qin Lin,Yuan Zhou,Qinglin Lu,Fan Tang*

Main category: cs.CV

TL;DR: HunyuanVideo-HOMA是一个弱条件多模态驱动框架，用于解决HOI视频生成中的关键限制，如依赖精选运动数据、泛化能力不足和可访问性受限。


<details>
  <summary>Details</summary>
Motivation: 解决HOI视频生成中对精选运动数据的依赖、对新对象/场景的泛化能力有限以及可访问性受限的问题。

Method: 通过稀疏解耦的运动引导增强可控性，将外观和运动信号编码到多模态扩散变换器（MMDiT）的双输入空间中，并在共享上下文空间中融合以生成时间一致且物理合理的交互。

Result: 在弱监督下实现了交互自然性和泛化能力的先进性能，并在文本条件生成和交互式对象操作中表现出多功能性。

Conclusion: HunyuanVideo-HOMA通过创新的框架设计，显著提升了HOI视频生成的性能和可访问性。

Abstract: To address key limitations in human-object interaction (HOI) video generation -- specifically the reliance on curated motion data, limited generalization to novel objects/scenarios, and restricted accessibility -- we introduce HunyuanVideo-HOMA, a weakly conditioned multimodal-driven framework. HunyuanVideo-HOMA enhances controllability and reduces dependency on precise inputs through sparse, decoupled motion guidance. It encodes appearance and motion signals into the dual input space of a multimodal diffusion transformer (MMDiT), fusing them within a shared context space to synthesize temporally consistent and physically plausible interactions. To optimize training, we integrate a parameter-space HOI adapter initialized from pretrained MMDiT weights, preserving prior knowledge while enabling efficient adaptation, and a facial cross-attention adapter for anatomically accurate audio-driven lip synchronization. Extensive experiments confirm state-of-the-art performance in interaction naturalness and generalization under weak supervision. Finally, HunyuanVideo-HOMA demonstrates versatility in text-conditioned generation and interactive object manipulation, supported by a user-friendly demo interface. The project page is at https://anonymous.4open.science/w/homa-page-0FBE/.

</details>


### [23] [HiSin: Efficient High-Resolution Sinogram Inpainting via Resolution-Guided Progressive Inference](https://arxiv.org/abs/2506.08809)
*Jiaze E,Srutarshi Banerjee,Tekin Bicer,Guannan Wang,Yanfu Zhang,Bin Ren*

Main category: cs.CV

TL;DR: HiSin是一种基于扩散模型的高效正弦图修复框架，通过分辨率引导的渐进推理实现内存高效修复。


<details>
  <summary>Details</summary>
Motivation: 高分辨率正弦图修复对CT重建至关重要，但现有扩散模型因内存和计算需求过高而受限。

Method: HiSin采用分辨率引导的渐进推理，先在低分辨率提取全局结构，再在高分辨率处理小补丁，并结合频率感知补丁跳过和结构自适应步骤分配。

Result: 实验表明，HiSin峰值内存使用减少31.25%，推理时间减少18.15%，且修复精度不受影响。

Conclusion: HiSin为高分辨率正弦图修复提供了一种高效且准确的解决方案。

Abstract: High-resolution sinogram inpainting is essential for computed tomography reconstruction, as missing high-frequency projections can lead to visible artifacts and diagnostic errors. Diffusion models are well-suited for this task due to their robustness and detail-preserving capabilities, but their application to high-resolution inputs is limited by excessive memory and computational demands. To address this limitation, we propose HiSin, a novel diffusion based framework for efficient sinogram inpainting via resolution-guided progressive inference. It progressively extracts global structure at low resolution and defers high-resolution inference to small patches, enabling memory-efficient inpainting. It further incorporates frequency-aware patch skipping and structure-adaptive step allocation to reduce redundant computation. Experimental results show that HiSin reduces peak memory usage by up to 31.25% and inference time by up to 18.15%, and maintains inpainting accuracy across datasets, resolutions, and mask conditions.

</details>


### [24] [StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams](https://arxiv.org/abs/2506.08862)
*Zike Wu,Qi Yan,Xuanyu Yi,Lele Wang,Renjie Liao*

Main category: cs.CV

TL;DR: StreamSplat是一个实时处理未校准视频流并生成动态3D高斯泼溅表示的框架，解决了现有方法在实时性、动态建模和长期稳定性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时处理未校准输入、动态场景建模和长期稳定性，StreamSplat旨在解决这些问题。

Method: 提出静态编码器中的概率采样机制和动态解码器中的双向变形场，实现高效的动态建模。

Result: 在静态和动态基准测试中，StreamSplat在重建质量和动态建模上优于现有方法，并支持在线重建。

Conclusion: StreamSplat是首个支持在线重建未校准视频流的框架，具有高效和稳定的动态建模能力。

Abstract: Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams is crucial for numerous real-world applications. However, existing methods struggle to jointly address three key challenges: 1) processing uncalibrated inputs in real time, 2) accurately modeling dynamic scene evolution, and 3) maintaining long-term stability and computational efficiency. To this end, we introduce StreamSplat, the first fully feed-forward framework that transforms uncalibrated video streams of arbitrary length into dynamic 3D Gaussian Splatting (3DGS) representations in an online manner, capable of recovering scene dynamics from temporally local observations. We propose two key technical innovations: a probabilistic sampling mechanism in the static encoder for 3DGS position prediction, and a bidirectional deformation field in the dynamic decoder that enables robust and efficient dynamic modeling. Extensive experiments on static and dynamic benchmarks demonstrate that StreamSplat consistently outperforms prior works in both reconstruction quality and dynamic scene modeling, while uniquely supporting online reconstruction of arbitrarily long video streams. Code and models are available at https://github.com/nickwzk/StreamSplat.

</details>


### [25] [Product of Experts for Visual Generation](https://arxiv.org/abs/2506.08894)
*Yunzhi Zhang,Carson Murtuza-Lanier,Zizhang Li,Yilun Du,Jiajun Wu*

Main category: cs.CV

TL;DR: 提出了一种基于专家乘积（PoE）的框架，通过训练无关的方法（AIS）从异构模型中组合知识，提升图像和视频合成的可控性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现代神经模型在共享数据领域（如图像和视频）上具有丰富的先验和互补知识，但如何整合来自不同来源（如生成模型、视觉语言模型、图形引擎等）的多样化知识尚未充分探索。

Method: 采用专家乘积（PoE）框架，通过退火重要性采样（AIS）从异构模型的乘积分布中进行推理时知识组合。

Result: 在图像和视频合成任务中表现出实用性，比单一方法具有更好的可控性，并提供灵活的用户界面以指定视觉生成目标。

Conclusion: 提出的框架为异构模型的知识组合提供了一种有效且灵活的方法，适用于视觉生成任务。

Abstract: Modern neural models capture rich priors and have complementary knowledge over shared data domains, e.g., images and videos. Integrating diverse knowledge from multiple sources -- including visual generative models, visual language models, and sources with human-crafted knowledge such as graphics engines and physics simulators -- remains under-explored. We propose a Product of Experts (PoE) framework that performs inference-time knowledge composition from heterogeneous models. This training-free approach samples from the product distribution across experts via Annealed Importance Sampling (AIS). Our framework shows practical benefits in image and video synthesis tasks, yielding better controllability than monolithic methods and additionally providing flexible user interfaces for specifying visual generation goals.

</details>


### [26] [SkipVAR: Accelerating Visual Autoregressive Modeling via Adaptive Frequency-Aware Skipping](https://arxiv.org/abs/2506.08908)
*Jiajun Li,Yue Ma,Xinyu Zhang,Qingyan Wei,Songhua Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: 论文提出SkipVAR框架，通过自动跳过冗余步骤和替换无条件分支，动态选择加速策略，显著提升VAR模型效率。


<details>
  <summary>Details</summary>
Motivation: 研究发现VAR模型的高频组件或后期步骤导致推理延迟，但计算冗余未被充分研究。

Method: 分析VAR推理过程，提出自动跳过冗余步骤和无条件分支替换技术，并设计样本自适应框架SkipVAR。

Result: SkipVAR在GenEval基准上实现1.81倍加速和2.62倍速度提升，同时保持模型质量。

Conclusion: 频率感知的无训练自适应加速方法对可扩展自回归图像生成有效。

Abstract: Recent studies on Visual Autoregressive (VAR) models have highlighted that high-frequency components, or later steps, in the generation process contribute disproportionately to inference latency. However, the underlying computational redundancy involved in these steps has yet to be thoroughly investigated. In this paper, we conduct an in-depth analysis of the VAR inference process and identify two primary sources of inefficiency: step redundancy and unconditional branch redundancy. To address step redundancy, we propose an automatic step-skipping strategy that selectively omits unnecessary generation steps to improve efficiency. For unconditional branch redundancy, we observe that the information gap between the conditional and unconditional branches is minimal. Leveraging this insight, we introduce unconditional branch replacement, a technique that bypasses the unconditional branch to reduce computational cost. Notably, we observe that the effectiveness of acceleration strategies varies significantly across different samples. Motivated by this, we propose SkipVAR, a sample-adaptive framework that leverages frequency information to dynamically select the most suitable acceleration strategy for each instance. To evaluate the role of high-frequency information, we introduce high-variation benchmark datasets that test model sensitivity to fine details. Extensive experiments show SkipVAR achieves over 0.88 average SSIM with up to 1.81x overall acceleration and 2.62x speedup on the GenEval benchmark, maintaining model quality. These results confirm the effectiveness of frequency-aware, training-free adaptive acceleration for scalable autoregressive image generation. Our code is available at https://github.com/fakerone-li/SkipVAR and has been publicly released.

</details>


### [27] [Diffuse and Disperse: Image Generation with Representation Regularization](https://arxiv.org/abs/2506.09027)
*Runqian Wang,Kaiming He*

Main category: cs.CV

TL;DR: 提出了一种名为Dispersive Loss的简单正则化方法，用于改进扩散生成模型，无需额外数据或预训练。


<details>
  <summary>Details</summary>
Motivation: 扩散生成模型通常缺乏显式正则化，且与表示学习进展脱节。本文旨在通过一种简单的方法弥合这一差距。

Method: 提出Dispersive Loss，一种鼓励隐空间表示分散的正则化方法，无需正样本对，不影响回归采样过程。

Result: 在ImageNet数据集上评估，Dispersive Loss在多种模型中均优于基线方法。

Conclusion: Dispersive Loss为生成模型与表示学习的结合提供了有效途径。

Abstract: The development of diffusion-based generative models over the past decade has largely proceeded independently of progress in representation learning. These diffusion models typically rely on regression-based objectives and generally lack explicit regularization. In this work, we propose \textit{Dispersive Loss}, a simple plug-and-play regularizer that effectively improves diffusion-based generative models. Our loss function encourages internal representations to disperse in the hidden space, analogous to contrastive self-supervised learning, with the key distinction that it requires no positive sample pairs and therefore does not interfere with the sampling process used for regression. Compared to the recent method of representation alignment (REPA), our approach is self-contained and minimalist, requiring no pre-training, no additional parameters, and no external data. We evaluate Dispersive Loss on the ImageNet dataset across a range of models and report consistent improvements over widely used and strong baselines. We hope our work will help bridge the gap between generative modeling and representation learning.

</details>


### [28] [Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better](https://arxiv.org/abs/2506.09040)
*Dianyi Wang,Wei Song,Yikun Wang,Siyuan Wang,Kaicheng Yu,Zhongyu Wei,Jiaqi Wang*

Main category: cs.CV

TL;DR: ASVR提出了一种联合学习视觉和文本模态的自回归框架，通过重建图像的语义表示而非原始外观，显著提升了多模态理解性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型仅对文本序列进行自回归监督，未能充分利用视觉模态，导致无法处理无标注图像、遗漏视觉细节及难以表达视觉内容。

Method: ASVR在统一的自回归框架中联合学习视觉和文本模态，通过重建图像的语义表示而非原始外观。

Result: ASVR在多种数据规模和LLM骨干上表现优异，尤其在LLaVA-1.5上平均提升了5%的多模态基准分数。

Conclusion: ASVR通过语义重建显著提升了多模态理解能力，证明了自回归视觉监督的有效性。

Abstract: Typical large vision-language models (LVLMs) apply autoregressive supervision solely to textual sequences, without fully incorporating the visual modality into the learning process. This results in three key limitations: (1) an inability to utilize images without accompanying captions, (2) the risk that captions omit critical visual details, and (3) the challenge that certain vision-centric content cannot be adequately conveyed through text. As a result, current LVLMs often prioritize vision-to-language alignment while potentially overlooking fine-grained visual information. While some prior works have explored autoregressive image generation, effectively leveraging autoregressive visual supervision to enhance image understanding remains an open challenge. In this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR), which enables joint learning of visual and textual modalities within a unified autoregressive framework. We show that autoregressively reconstructing the raw visual appearance of images does not enhance and may even impair multimodal understanding. In contrast, autoregressively reconstructing the semantic representation of images consistently improves comprehension. Notably, we find that even when models are given continuous image features as input, they can effectively reconstruct discrete semantic tokens, resulting in stable and consistent improvements across a wide range of multimodal understanding benchmarks. Our approach delivers significant performance gains across varying data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is available at https://github.com/AlenjandroWang/ASVR.

</details>


### [29] [Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models](https://arxiv.org/abs/2506.09042)
*Xuanchi Ren,Yifan Lu,Tianshi Cao,Ruiyuan Gao,Shengyu Huang,Amirmojtaba Sabour,Tianchang Shen,Tobias Pfaff,Jay Zhangjie Wu,Runjian Chen,Seung Wook Kim,Jun Gao,Laura Leal-Taixe,Mike Chen,Sanja Fidler,Huan Ling*

Main category: cs.CV

TL;DR: Cosmos-Drive-Dreams是一个合成数据生成（SDG）管道，旨在生成具有挑战性的场景，以支持自动驾驶系统的感知和驾驶策略训练。


<details>
  <summary>Details</summary>
Motivation: 收集和标注真实世界数据用于自动驾驶系统（AV）等安全关键物理AI系统既耗时又昂贵，尤其是难以捕捉对训练和测试至关重要的罕见边缘案例。

Method: 利用Cosmos-Drive（基于NVIDIA Cosmos世界基础模型的专用套件）生成可控、高保真、多视角且时空一致的驾驶视频。

Result: 实验证明，生成的数据有助于缓解长尾分布问题，并提升下游任务（如3D车道检测、3D物体检测和驾驶策略学习）的泛化能力。

Conclusion: Cosmos-Drive-Dreams通过开源工具包、数据集和模型权重，为自动驾驶领域提供了高效的数据生成解决方案。

Abstract: Collecting and annotating real-world data for safety-critical physical AI systems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is especially challenging to capture rare edge cases, which play a critical role in training and testing of an AV system. To address this challenge, we introduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline that aims to generate challenging scenarios to facilitate downstream tasks such as perception and driving policy training. Powering this pipeline is Cosmos-Drive, a suite of models specialized from NVIDIA Cosmos world foundation model for the driving domain and are capable of controllable, high-fidelity, multi-view, and spatiotemporally consistent driving video generation. We showcase the utility of these models by applying Cosmos-Drive-Dreams to scale the quantity and diversity of driving datasets with high-fidelity and challenging scenarios. Experimentally, we demonstrate that our generated data helps in mitigating long-tail distribution problems and enhances generalization in downstream tasks such as 3D lane detection, 3D object detection and driving policy learning. We open source our pipeline toolkit, dataset and model weights through the NVIDIA's Cosmos platform.   Project page: https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams

</details>


### [30] [MagCache: Fast Video Generation with Magnitude-Aware Cache](https://arxiv.org/abs/2506.09045)
*Zehong Ma,Longhui Wei,Feng Wang,Shiliang Zhang,Qi Tian*

Main category: cs.CV

TL;DR: 本文提出了一种基于统一幅度规律的视频扩散模型加速方法MagCache，通过自适应跳过不重要时间步和缓存策略，显著提升了速度并保持了视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型加速技术通常依赖统一启发式方法或时间嵌入变体，容易因提示特定过拟合导致输出不一致，且需要大量校准样本。

Method: 发现不同模型和提示中统一的幅度规律，提出Magnitude-aware Cache（MagCache），通过误差建模机制和自适应缓存策略跳过不重要时间步。

Result: MagCache在Open-Sora和Wan 2.1上分别实现2.1倍和2.68倍加速，且在LPIPS、SSIM和PSNR指标上显著优于现有方法。

Conclusion: MagCache是一种高效且鲁棒的加速方法，仅需单一样本校准，显著提升了视频扩散模型的性能和效率。

Abstract: Existing acceleration techniques for video diffusion models often rely on uniform heuristics or time-embedding variants to skip timesteps and reuse cached features. These approaches typically require extensive calibration with curated prompts and risk inconsistent outputs due to prompt-specific overfitting. In this paper, we introduce a novel and robust discovery: a unified magnitude law observed across different models and prompts. Specifically, the magnitude ratio of successive residual outputs decreases monotonically and steadily in most timesteps while rapidly in the last several steps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache) that adaptively skips unimportant timesteps using an error modeling mechanism and adaptive caching strategy. Unlike existing methods requiring dozens of curated samples for calibration, MagCache only requires a single sample for calibration. Experimental results show that MagCache achieves 2.1x and 2.68x speedups on Open-Sora and Wan 2.1, respectively, while preserving superior visual fidelity. It significantly outperforms existing methods in LPIPS, SSIM, and PSNR, under comparable computational budgets.

</details>


### [31] [CuRe: Cultural Gaps in the Long Tail of Text-to-Image Systems](https://arxiv.org/abs/2506.08071)
*Aniket Rege,Zinnia Nie,Mahesh Ramesh,Unmesh Raskar,Zhuoran Yu,Aditya Kusupati,Yong Jae Lee,Ramya Korlakai Vinayak*

Main category: cs.CV

TL;DR: CuRe是一个用于评估文本到图像系统文化代表性的新基准，通过属性规范的边际效用作为人类判断的代理。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像系统的训练数据偏向欧美文化，忽视了全球南方的文化多样性，CuRe旨在量化并解决这一问题。

Method: 利用来自维基百科知识图谱的300种文化物品构建数据集，通过文本条件的逐步增加来评估系统的文化代表性。

Result: CuRe评分器在多种图像编码器和文本到图像系统中表现出与人类判断的高度相关性。

Conclusion: CuRe为文化多样性的评估提供了可扩展的工具，并开源了代码和数据集。

Abstract: Popular text-to-image (T2I) systems are trained on web-scraped data, which is heavily Amero and Euro-centric, underrepresenting the cultures of the Global South. To analyze these biases, we introduce CuRe, a novel and scalable benchmarking and scoring suite for cultural representativeness that leverages the marginal utility of attribute specification to T2I systems as a proxy for human judgments. Our CuRe benchmark dataset has a novel categorical hierarchy built from the crowdsourced Wikimedia knowledge graph, with 300 cultural artifacts across 32 cultural subcategories grouped into six broad cultural axes (food, art, fashion, architecture, celebrations, and people). Our dataset's categorical hierarchy enables CuRe scorers to evaluate T2I systems by analyzing their response to increasing the informativeness of text conditioning, enabling fine-grained cultural comparisons. We empirically observe much stronger correlations of our class of scorers to human judgments of perceptual similarity, image-text alignment, and cultural diversity across image encoders (SigLIP 2, AIMV2 and DINOv2), vision-language models (OpenCLIP, SigLIP 2, Gemini 2.0 Flash) and state-of-the-art text-to-image systems, including three variants of Stable Diffusion (1.5, XL, 3.5 Large), FLUX.1 [dev], Ideogram 2.0, and DALL-E 3. The code and dataset is open-sourced and available at https://aniketrege.github.io/cure/.

</details>


### [32] [Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion Models in a Vision-Language-Action Framework](https://arxiv.org/abs/2506.08185)
*Huixin Zhan,Jason H. Moore*

Main category: cs.CV

TL;DR: 论文提出了一种基于离散扩散框架和视觉-语言-动作（VLA）管道的方法，用于建模外科医生在机器人手术中的个性化行为指纹，同时探讨了个性化嵌入在任务性能与隐私风险之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统常忽略外科医生个性化行为信号，而外科医生的操作风格因训练、经验和运动行为差异而显著不同。

Method: 通过离散扩散框架结合VLA管道，将手势预测建模为结构化序列去噪任务，利用内窥镜视频、手术意图语言和隐私感知的外科医生身份与技能嵌入作为多模态输入。

Result: 在JIGSAWS数据集上验证，模型能准确重建手势序列并学习到每位外科医生独特的运动指纹。但更富表现力的嵌入虽提升任务性能，却增加了身份泄露风险。

Conclusion: 个性化嵌入虽能提升性能，但也增加了隐私风险，需在手术建模中平衡个性化与隐私保护。

Abstract: Surgeons exhibit distinct operating styles due to differences in training, experience, and motor behavior - yet current AI systems often ignore this personalization signal. We propose a novel approach to model fine-grained, surgeon-specific fingerprinting in robotic surgery using a discrete diffusion framework integrated with a vision-language-action (VLA) pipeline. Our method formulates gesture prediction as a structured sequence denoising task, conditioned on multimodal inputs including endoscopic video, surgical intent language, and a privacy-aware embedding of surgeon identity and skill. Personalized surgeon fingerprinting is encoded through natural language prompts using third-party language models, allowing the model to retain individual behavioral style without exposing explicit identity. We evaluate our method on the JIGSAWS dataset and demonstrate that it accurately reconstructs gesture sequences while learning meaningful motion fingerprints unique to each surgeon. To quantify the privacy implications of personalization, we perform membership inference attacks and find that more expressive embeddings improve task performance but simultaneously increase susceptibility to identity leakage. These findings demonstrate that while personalized embeddings improve performance, they also increase vulnerability to identity leakage, revealing the importance of balancing personalization with privacy risk in surgical modeling. Code is available at: https://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting.

</details>


### [33] [A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation](https://arxiv.org/abs/2506.08210)
*Andrew Z. Wang,Songwei Ge,Tero Karras,Ming-Yu Liu,Yogesh Balaji*

Main category: cs.CV

TL;DR: 研究探讨了使用现代仅解码器LLM作为文本编码器在文本到图像扩散模型中的效果，发现多层平均嵌入优于传统最后一层嵌入，性能超过基线T5模型。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型仍使用过时的T5和CLIP作为文本编码器，研究旨在探索现代LLM的潜力。

Method: 构建标准化训练和评估流程，训练27个模型，分析不同文本嵌入的影响，包括嵌入提取方法、LLM变体和模型大小。

Result: 多层平均嵌入显著提升复杂提示的对齐效果，多数LLM性能优于基线T5模型。

Conclusion: 现代LLM作为文本编码器在文本到图像生成中表现更优，尤其是多层平均嵌入策略。

Abstract: Both text-to-image generation and large language models (LLMs) have made significant advancements. However, many text-to-image models still employ the somewhat outdated T5 and CLIP as their text encoders. In this work, we investigate the effectiveness of using modern decoder-only LLMs as text encoders for text-to-image diffusion models. We build a standardized training and evaluation pipeline that allows us to isolate and evaluate the effect of different text embeddings. We train a total of 27 text-to-image models with 12 different text encoders to analyze the critical aspects of LLMs that could impact text-to-image generation, including the approaches to extract embeddings, different LLMs variants, and model sizes. Our experiments reveal that the de facto way of using last-layer embeddings as conditioning leads to inferior performance. Instead, we explore embeddings from various layers and find that using layer-normalized averaging across all layers significantly improves alignment with complex prompts. Most LLMs with this conditioning outperform the baseline T5 model, showing enhanced performance in advanced visio-linguistic reasoning skills.

</details>


### [34] [Highly Compressed Tokenizer Can Generate Without Training](https://arxiv.org/abs/2506.08257)
*L. Lao Beyer,T. Li,X. Chen,S. Karaman,K. He*

Main category: cs.CV

TL;DR: 1D图像分词器通过高度压缩的一维序列表示图像，支持启发式操作实现图像编辑和生成，无需训练生成模型。


<details>
  <summary>Details</summary>
Motivation: 探索1D图像分词器的表达能力，利用其高度压缩的潜在空间实现图像编辑和生成。

Method: 基于梯度优化的测试时分词器操作，结合重建或CLIP相似性等损失函数。

Result: 实现了细粒度图像编辑和多样化、逼真的样本生成，适用于修复和文本引导编辑。

Conclusion: 1D分词器的潜在空间具有强大表达能力，为图像编辑和生成提供了新途径。

Abstract: Commonly used image tokenizers produce a 2D grid of spatially arranged tokens. In contrast, so-called 1D image tokenizers represent images as highly compressed one-dimensional sequences of as few as 32 discrete tokens. We find that the high degree of compression achieved by a 1D tokenizer with vector quantization enables image editing and generative capabilities through heuristic manipulation of tokens, demonstrating that even very crude manipulations -- such as copying and replacing tokens between latent representations of images -- enable fine-grained image editing by transferring appearance and semantic attributes. Motivated by the expressivity of the 1D tokenizer's latent space, we construct an image generation pipeline leveraging gradient-based test-time optimization of tokens with plug-and-play loss functions such as reconstruction or CLIP similarity. Our approach is demonstrated for inpainting and text-guided image editing use cases, and can generate diverse and realistic samples without requiring training of any generative model.

</details>


### [35] [Seeing Voices: Generating A-Roll Video from Audio with Mirage](https://arxiv.org/abs/2506.08279)
*Aditi Sundararaman,Amogh Adishesha,Andrew Jaegle,Dan Bigioi,Hyoung-Kyu Song,Jon Kyl,Justin Mao,Kevin Lan,Mojtaba Komeili,ShahRukh Athar,Sheila Babayan,Stanislau Beliasau,William Buchwalter*

Main category: cs.CV

TL;DR: Mirage是一种音频到视频的基础模型，能够根据音频输入生成逼真、富有表现力的视频。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成方法要么忽略声音专注于无声图像序列生成，要么局限于特定应用领域（如重新配音）。Mirage旨在实现音频与视频的和谐集成。

Method: Mirage采用基于自注意力的统一训练方法，支持从零开始训练或基于现有权重训练，适用于音频到视频生成。

Result: Mirage生成的视频在主观质量上优于其他方法，尤其擅长生成与输入音频匹配的人物说话视频。

Conclusion: Mirage为音频到视频生成提供了通用且高质量的解决方案，支持多模态视频生成。

Abstract: From professional filmmaking to user-generated content, creators and consumers have long recognized that the power of video depends on the harmonious integration of what we hear (the video's audio track) with what we see (the video's image sequence). Current approaches to video generation either ignore sound to focus on general-purpose but silent image sequence generation or address both visual and audio elements but focus on restricted application domains such as re-dubbing. We introduce Mirage, an audio-to-video foundation model that excels at generating realistic, expressive output imagery from scratch given an audio input. When integrated with existing methods for speech synthesis (text-to-speech, or TTS), Mirage results in compelling multimodal video. When trained on audio-video footage of people talking (A-roll) and conditioned on audio containing speech, Mirage generates video of people delivering a believable interpretation of the performance implicit in input audio. Our central technical contribution is a unified method for training self-attention-based audio-to-video generation models, either from scratch or given existing weights. This methodology allows Mirage to retain generality as an approach to audio-to-video generation while producing outputs of superior subjective quality to methods that incorporate audio-specific architectures or loss components specific to people, speech, or details of how images or audio are captured. We encourage readers to watch and listen to the results of Mirage for themselves (see paper and comments for links).

</details>


### [36] [How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models](https://arxiv.org/abs/2506.08351)
*Huixuan Zhang,Junzhe Zhang,Xiaojun Wan*

Main category: cs.CV

TL;DR: 提出了一种名为Step AG的自适应引导策略，通过限制分类器自由引导在前几步去噪步骤中，实现了20%到30%的速度提升，同时保持图像质量和文本对齐。


<details>
  <summary>Details</summary>
Motivation: 分类器自由引导方法需要两倍于无条件生成的步骤，导致成本显著增加，而现有自适应引导方法缺乏分析和实证支持。

Method: 提出Step AG策略，限制分类器自由引导在前几步去噪步骤中，适用于通用扩散模型。

Result: 实验表明，该方法在图像质量和文本对齐方面表现良好，速度提升20%到30%，且适用于不同设置和模型。

Conclusion: Step AG是一种简单、通用的自适应引导策略，显著提高了效率，同时保持了生成质量。

Abstract: With the rapid development of text-to-vision generation diffusion models, classifier-free guidance has emerged as the most prevalent method for conditioning. However, this approach inherently requires twice as many steps for model forwarding compared to unconditional generation, resulting in significantly higher costs. While previous study has introduced the concept of adaptive guidance, it lacks solid analysis and empirical results, making previous method unable to be applied to general diffusion models. In this work, we present another perspective of applying adaptive guidance and propose Step AG, which is a simple, universally applicable adaptive guidance strategy. Our evaluations focus on both image quality and image-text alignment. whose results indicate that restricting classifier-free guidance to the first several denoising steps is sufficient for generating high-quality, well-conditioned images, achieving an average speedup of 20% to 30%. Such improvement is consistent across different settings such as inference steps, and various models including video generation models, highlighting the superiority of our method.

</details>


### [37] [Enhancing Motion Dynamics of Image-to-Video Models via Adaptive Low-Pass Guidance](https://arxiv.org/abs/2506.08456)
*June Suk Choi,Kyungmin Lee,Sihyun Yu,Yisol Choi,Jinwoo Shin,Kimin Lee*

Main category: cs.CV

TL;DR: 论文提出自适应低通引导（ALG）方法，解决图像到视频（I2V）生成中动态性不足的问题，显著提升视频动态性。


<details>
  <summary>Details</summary>
Motivation: 现有I2V生成方法因过早暴露高频细节导致视频动态性不足，ALG旨在解决这一问题。

Method: 通过自适应低通滤波在去噪早期阶段调制输入图像频率内容。

Result: ALG显著提升视频动态性（VBench-I2V测试中动态度平均提升36%），同时保持图像质量和文本对齐。

Conclusion: ALG是一种简单有效的改进方法，能显著提升I2V生成的动态性。

Abstract: Recent text-to-video (T2V) models have demonstrated strong capabilities in producing high-quality, dynamic videos. To improve the visual controllability, recent works have considered fine-tuning pre-trained T2V models to support image-to-video (I2V) generation. However, such adaptation frequently suppresses motion dynamics of generated outputs, resulting in more static videos compared to their T2V counterparts. In this work, we analyze this phenomenon and identify that it stems from the premature exposure to high-frequency details in the input image, which biases the sampling process toward a shortcut trajectory that overfits to the static appearance of the reference image. To address this, we propose adaptive low-pass guidance (ALG), a simple fix to the I2V model sampling procedure to generate more dynamic videos without compromising per-frame image quality. Specifically, ALG adaptively modulates the frequency content of the conditioning image by applying low-pass filtering at the early stage of denoising. Extensive experiments demonstrate that ALG significantly improves the temporal dynamics of generated videos, while preserving image fidelity and text alignment. Especially, under VBench-I2V test suite, ALG achieves an average improvement of 36% in dynamic degree without a significant drop in video quality or image fidelity.

</details>


### [38] [LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid Temporal Modeling with Only 4$\times$RTX 4090s](https://arxiv.org/abs/2506.08529)
*Xijun Wang,Xin Li,Bingchen Li,Zhibo Chen*

Main category: cs.CV

TL;DR: LiftVSR是一种高效的视频超分辨率框架，通过结合动态时间注意力和注意力记忆缓存，显著降低了计算成本，同时提升了长期一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在时间一致性和计算成本方面存在不足，尤其是长视频处理需要大量GPU资源。

Method: 提出混合时间建模机制，包括动态时间注意力（DTA）和注意力记忆缓存（AMC），并引入非对称采样策略。

Result: 在多个基准测试中表现出色，计算成本显著降低。

Conclusion: LiftVSR在效率和性能上实现了平衡，为视频超分辨率提供了实用解决方案。

Abstract: Diffusion models have significantly advanced video super-resolution (VSR) by enhancing perceptual quality, largely through elaborately designed temporal modeling to ensure inter-frame consistency. However, existing methods usually suffer from limited temporal coherence and prohibitively high computational costs (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for long videos. In this work, we propose LiftVSR, an efficient VSR framework that leverages and elevates the image-wise diffusion prior from PixArt-$\alpha$, achieving state-of-the-art results using only 4$\times$RTX 4090 GPUs. To balance long-term consistency and efficiency, we introduce a hybrid temporal modeling mechanism that decomposes temporal learning into two complementary components: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal modeling within short frame segment ($\textit{i.e.}$, low complexity), and (ii) Attention Memory Cache (AMC) for long-term temporal modeling across segments ($\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token flows across frames within multi-head query and key tokens to warp inter-frame contexts in the value tokens. AMC adaptively aggregates historical segment information via a cache unit, ensuring long-term coherence with minimal overhead. To further stabilize the cache interaction during inference, we introduce an asymmetric sampling strategy that mitigates feature mismatches arising from different diffusion sampling steps. Extensive experiments on several typical VSR benchmarks have demonstrated that LiftVSR achieves impressive performance with significantly lower computational costs.

</details>


### [39] [A Probability-guided Sampler for Neural Implicit Surface Rendering](https://arxiv.org/abs/2506.08619)
*Gonçalo Dias Pais,Valter Piedade,Moitreya Chatterjee,Marcus Greiff,Pedro Miraldo*

Main category: cs.CV

TL;DR: 本文提出了一种基于隐式表面表示和3D图像投影空间概率密度函数的采样策略，以及新的表面重建损失函数，显著提升了3D重建和图像渲染的精度。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF变体因可扩展性问题无法训练所有可能的输入数据，且采样策略不够高效。本文旨在通过更精准的采样和新的损失函数提升渲染效果。

Method: 利用隐式表面表示建模3D图像投影空间的概率密度函数，实现针对性采样；提出新的表面重建损失函数，结合近表面和空空间信息。

Result: 在现有神经隐式表面渲染器中集成新方法后，3D重建和图像渲染的精度显著提升，尤其是对感兴趣区域。

Conclusion: 本文提出的采样策略和损失函数有效提升了渲染和重建的精度，为相关领域提供了新思路。

Abstract: Several variants of Neural Radiance Fields (NeRFs) have significantly improved the accuracy of synthesized images and surface reconstruction of 3D scenes/objects. In all of these methods, a key characteristic is that none can train the neural network with every possible input data, specifically, every pixel and potential 3D point along the projection rays due to scalability issues. While vanilla NeRFs uniformly sample both the image pixels and 3D points along the projection rays, some variants focus only on guiding the sampling of the 3D points along the projection rays. In this paper, we leverage the implicit surface representation of the foreground scene and model a probability density function in a 3D image projection space to achieve a more targeted sampling of the rays toward regions of interest, resulting in improved rendering. Additionally, a new surface reconstruction loss is proposed for improved performance. This new loss fully explores the proposed 3D image projection space model and incorporates near-to-surface and empty space components. By integrating our novel sampling strategy and novel loss into current state-of-the-art neural implicit surface renderers, we achieve more accurate and detailed 3D reconstructions and improved image rendering, especially for the regions of interest in any given scene.

</details>


### [40] [RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping](https://arxiv.org/abs/2506.08632)
*Yang Bai,Liudi Yang,George Eskandar,Fengyi Shen,Dong Chen,Mohammad Altillawi,Ziyuan Liu,Gitta Kutyniok*

Main category: cs.CV

TL;DR: RoboSwap提出了一种基于GAN和扩散模型的视频编辑框架，用于在未配对数据中替换机械臂，解决了跨平台机器人学习中的数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频合成和编辑技术虽先进，但高质量多样化数据集的缺乏限制了视频条件下的机器人学习，尤其是跨平台泛化能力。

Method: RoboSwap结合GAN和扩散模型，先分割机械臂并训练未配对GAN进行翻译，再通过扩散模型增强一致性和运动真实性。

Result: 实验表明，RoboSwap在三个基准测试中优于现有视频和图像编辑模型，结构一致性和运动一致性表现更优。

Conclusion: RoboSwap为机器人学习提供了可靠的跨平台数据生成方案，解决了数据收集的难题。

Abstract: Recent advancements in generative models have revolutionized video synthesis and editing. However, the scarcity of diverse, high-quality datasets continues to hinder video-conditioned robotic learning, limiting cross-platform generalization. In this work, we address the challenge of swapping a robotic arm in one video with another: a key step for crossembodiment learning. Unlike previous methods that depend on paired video demonstrations in the same environmental settings, our proposed framework, RoboSwap, operates on unpaired data from diverse environments, alleviating the data collection needs. RoboSwap introduces a novel video editing pipeline integrating both GANs and diffusion models, combining their isolated advantages. Specifically, we segment robotic arms from their backgrounds and train an unpaired GAN model to translate one robotic arm to another. The translated arm is blended with the original video background and refined with a diffusion model to enhance coherence, motion realism and object interaction. The GAN and diffusion stages are trained independently. Our experiments demonstrate that RoboSwap outperforms state-of-the-art video and image editing models on three benchmarks in terms of both structural coherence and motion consistency, thereby offering a robust solution for generating reliable, cross-embodiment data in robotic learning.

</details>


### [41] [Orientation Matters: Making 3D Generative Models Orientation-Aligned](https://arxiv.org/abs/2506.08640)
*Yichong Lu,Yuzhuo Tian,Zijin Jiang,Yikun Zhao,Yuanbo Yang,Hao Ouyang,Haoji Hu,Huimin Yu,Yujun Shen,Yiyi Liao*

Main category: cs.CV

TL;DR: 论文提出了一种方向对齐的3D物体生成方法，通过构建数据集Objaverse-OA并微调现有模型，解决了现有3D生成模型因训练数据不一致导致的方向错位问题。


<details>
  <summary>Details</summary>
Motivation: 人类能从单张图像中直觉感知物体的形状和方向，但现有3D生成模型因训练数据不一致导致生成结果方向错位，限制了其在下游任务中的应用。

Method: 构建了方向对齐的3D模型数据集Objaverse-OA（14,832个模型，1,008个类别），并基于多视角扩散和3D变分自编码器框架微调了两类代表性3D生成模型。

Result: 实验结果表明，该方法优于后处理对齐方法，并能推广到未见过的物体类别。

Conclusion: 方向对齐的3D物体生成方法提升了生成结果的一致性，支持了下游任务如零样本方向估计和高效物体旋转操作。

Abstract: Humans intuitively perceive object shape and orientation from a single image, guided by strong priors about canonical poses. However, existing 3D generative models often produce misaligned results due to inconsistent training data, limiting their usability in downstream tasks. To address this gap, we introduce the task of orientation-aligned 3D object generation: producing 3D objects from single images with consistent orientations across categories. To facilitate this, we construct Objaverse-OA, a dataset of 14,832 orientation-aligned 3D models spanning 1,008 categories. Leveraging Objaverse-OA, we fine-tune two representative 3D generative models based on multi-view diffusion and 3D variational autoencoder frameworks to produce aligned objects that generalize well to unseen objects across various categories. Experimental results demonstrate the superiority of our method over post-hoc alignment approaches. Furthermore, we showcase downstream applications enabled by our aligned object generation, including zero-shot object orientation estimation via analysis-by-synthesis and efficient arrow-based object rotation manipulation.

</details>


### [42] [TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary Large-Scale Scene Rendering](https://arxiv.org/abs/2506.08704)
*Xiaohan Zhang,Sitong Wang,Yushen Yan,Yi Yang,Mingda Xu,Qi Liu*

Main category: cs.CV

TL;DR: 论文提出TraGraph-GS方法，通过轨迹图解决大规模场景中高质量新视角合成的挑战，显著提升了渲染精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在空间分区和合并时存在泛化性差和纹理失真的问题，无法适应任意相机轨迹。

Method: 采用基于图的场景分区方法，结合正则化约束和渐进渲染策略，减少高斯重叠导致的失真。

Result: 在四个空中和四个地面数据集上，PSNR平均提升1.86 dB和1.62 dB，优于现有方法。

Conclusion: TraGraph-GS有效解决了大规模场景渲染的挑战，显著提升了渲染质量和效率。

Abstract: High-quality novel view synthesis for large-scale scenes presents a challenging dilemma in 3D computer vision. Existing methods typically partition large scenes into multiple regions, reconstruct a 3D representation using Gaussian splatting for each region, and eventually merge them for novel view rendering. They can accurately render specific scenes, yet they do not generalize effectively for two reasons: (1) rigid spatial partition techniques struggle with arbitrary camera trajectories, and (2) the merging of regions results in Gaussian overlap to distort texture details. To address these challenges, we propose TraGraph-GS, leveraging a trajectory graph to enable high-precision rendering for arbitrarily large-scale scenes. We present a spatial partitioning method for large-scale scenes based on graphs, which incorporates a regularization constraint to enhance the rendering of textures and distant objects, as well as a progressive rendering strategy to mitigate artifacts caused by Gaussian overlap. Experimental results demonstrate its superior performance both on four aerial and four ground datasets and highlight its remarkable efficiency: our method achieves an average improvement of 1.86 dB in PSNR on aerial datasets and 1.62 dB on ground datasets compared to state-of-the-art approaches.

</details>


### [43] [SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting](https://arxiv.org/abs/2506.08710)
*Mengjiao Ma,Qi Ma,Yue Li,Jiahuan Cheng,Runyi Yang,Bin Ren,Nikola Popovic,Mingqiang Wei,Nicu Sebe,Luc Van Gool,Theo Gevers,Martin R. Oswald,Danda Pani Paudel*

Main category: cs.CV

TL;DR: 论文提出了首个大规模基准测试，系统性评估3D高斯溅射（3DGS）在3D空间中的表现，并引入GaussianWorld-49K数据集，展示通用方法的优势。


<details>
  <summary>Details</summary>
Motivation: 当前3DGS方法多局限于少量场景和视角的2D渲染评估，缺乏对整体3D理解的深入分析。

Method: 提出大规模基准测试，评估三类3DGS方法（优化、免优化、通用）在1060个场景中的表现，并引入49K场景数据集。

Result: 通用方法在放松场景限制、快速推理和分割性能上表现优越。

Conclusion: 通用方法结合大数据先验具有潜力，公开代码、基准和数据集以推动研究。

Abstract: 3D Gaussian Splatting (3DGS) serves as a highly performant and efficient encoding of scene geometry, appearance, and semantics. Moreover, grounding language in 3D scenes has proven to be an effective strategy for 3D scene understanding. Current Language Gaussian Splatting line of work fall into three main groups: (i) per-scene optimization-based, (ii) per-scene optimization-free, and (iii) generalizable approach. However, most of them are evaluated only on rendered 2D views of a handful of scenes and viewpoints close to the training views, limiting ability and insight into holistic 3D understanding. To address this gap, we propose the first large-scale benchmark that systematically assesses these three groups of methods directly in 3D space, evaluating on 1060 scenes across three indoor datasets and one outdoor dataset. Benchmark results demonstrate a clear advantage of the generalizable paradigm, particularly in relaxing the scene-specific limitation, enabling fast feed-forward inference on novel scenes, and achieving superior segmentation performance. We further introduce GaussianWorld-49K a carefully curated 3DGS dataset comprising around 49K diverse indoor and outdoor scenes obtained from multiple sources, with which we demonstrate the generalizable approach could harness strong data priors. Our codes, benchmark, and datasets will be made public to accelerate research in generalizable 3DGS scene understanding.

</details>


### [44] [Gaussian2Scene: 3D Scene Representation Learning via Self-supervised Learning with 3D Gaussian Splatting](https://arxiv.org/abs/2506.08777)
*Keyi Liu,Weidong Yang,Ben Fei,Ying He*

Main category: cs.CV

TL;DR: 论文提出Gaussian2Scene，一种基于3D高斯抛光的自监督学习框架，用于点云预训练，解决了现有方法在计算和几何理解上的限制。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法依赖隐式场景表示和高内存需求，且重建目标仅应用于2D空间，难以捕捉3D几何结构。

Method: 采用3D高斯抛光（3DGS）进行预训练，分两阶段：首先通过双分支掩码自编码器学习2D和3D表示，然后利用高斯基元的几何位置和渲染RGB图像监督学习。

Result: 在多个下游3D目标检测任务中表现优于现有预训练方法。

Conclusion: Gaussian2Scene通过直接3D场景重建和高效计算，提升了模型的几何理解能力。

Abstract: Self-supervised learning (SSL) for point cloud pre-training has become a cornerstone for many 3D vision tasks, enabling effective learning from large-scale unannotated data. At the scene level, existing SSL methods often incorporate volume rendering into the pre-training framework, using RGB-D images as reconstruction signals to facilitate cross-modal learning. This strategy promotes alignment between 2D and 3D modalities and enables the model to benefit from rich visual cues in the RGB-D inputs. However, these approaches are limited by their reliance on implicit scene representations and high memory demands. Furthermore, since their reconstruction objectives are applied only in 2D space, they often fail to capture underlying 3D geometric structures. To address these challenges, we propose Gaussian2Scene, a novel scene-level SSL framework that leverages the efficiency and explicit nature of 3D Gaussian Splatting (3DGS) for pre-training. The use of 3DGS not only alleviates the computational burden associated with volume rendering but also supports direct 3D scene reconstruction, thereby enhancing the geometric understanding of the backbone network. Our approach follows a progressive two-stage training strategy. In the first stage, a dual-branch masked autoencoder learns both 2D and 3D scene representations. In the second stage, we initialize training with reconstructed point clouds and further supervise learning using the geometric locations of Gaussian primitives and rendered RGB images. This process reinforces both geometric and cross-modal learning. We demonstrate the effectiveness of Gaussian2Scene across several downstream 3D object detection tasks, showing consistent improvements over existing pre-training methods.

</details>


### [45] [A PDE-Based Image Dehazing Method via Atmospheric Scattering Theory](https://arxiv.org/abs/2506.08793)
*Zhuoran Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种基于偏微分方程（PDE）的单图像去雾新框架，结合大气散射模型、非局部正则化和暗通道先验，改进了PDE模型，并证明了弱解的存在唯一性。实验表明该方法是一种有前景的去雾解决方案。


<details>
  <summary>Details</summary>
Motivation: 单图像去雾是一个重要的计算机视觉问题，现有方法在复杂场景中效果有限。本文旨在通过改进的PDE框架提升去雾效果。

Method: 提出了一种改进的PDE模型，结合了非局部正则化和暗通道先验，并采用自适应正则化参数。通过Lax-Milgram定理证明了弱解的存在唯一性，并实现了基于PyTorch的高效GPU加速迭代算法。

Result: 实验结果表明，该方法在去雾任务中表现优异，且具有推广到深度学习模型的潜力。

Conclusion: 本文提出的PDE框架为单图像去雾提供了一种有效且可扩展的解决方案，未来可进一步结合深度学习模型。

Abstract: This paper presents a novel partial differential equation (PDE) framework for single-image dehazing. By integrating the atmospheric scattering model with nonlocal regularization and dark channel prior, we propose the improved PDE: \[ -\text{div}\left(D(\nabla u)\nabla u\right) + \lambda(t) G(u) = \Phi(I,t,A) \] where $D(\nabla u) = (|\nabla u| + \epsilon)^{-1}$ is the edge-preserving diffusion coefficient, $G(u)$ is the Gaussian convolution operator, and $\lambda(t)$ is the adaptive regularization parameter based on transmission map $t$. We prove the existence and uniqueness of weak solutions in $H_0^1(\Omega)$ using Lax-Milgram theorem, and implement an efficient fixed-point iteration scheme accelerated by PyTorch GPU computation. The experimental results demonstrate that this method is a promising deghazing solution that can be generalized to the deep model paradigm.

</details>


### [46] [HunyuanVideo-HOMA: Generic Human-Object Interaction in Multimodal Driven Human Animation](https://arxiv.org/abs/2506.08797)
*Ziyao Huang,Zixiang Zhou,Juan Cao,Yifeng Ma,Yi Chen,Zejing Rao,Zhiyong Xu,Hongmei Wang,Qin Lin,Yuan Zhou,Qinglin Lu,Fan Tang*

Main category: cs.CV

TL;DR: HunyuanVideo-HOMA是一个弱条件多模态驱动框架，旨在解决HOI视频生成中的关键限制，如依赖精选运动数据、对新对象/场景的泛化能力有限以及可访问性受限。


<details>
  <summary>Details</summary>
Motivation: 解决HOI视频生成中对精确输入的依赖、泛化能力不足和可访问性受限的问题。

Method: 通过稀疏解耦的运动引导增强可控性，将外观和运动信号编码到多模态扩散变换器（MMDiT）的双输入空间中，并在共享上下文空间中融合以合成时间一致且物理合理的交互。

Result: 在弱监督下实现了交互自然性和泛化性的最先进性能，并在文本条件生成和交互式对象操作中表现出多功能性。

Conclusion: HunyuanVideo-HOMA通过创新的框架设计和适配器优化，显著提升了HOI视频生成的性能和可访问性。

Abstract: To address key limitations in human-object interaction (HOI) video generation -- specifically the reliance on curated motion data, limited generalization to novel objects/scenarios, and restricted accessibility -- we introduce HunyuanVideo-HOMA, a weakly conditioned multimodal-driven framework. HunyuanVideo-HOMA enhances controllability and reduces dependency on precise inputs through sparse, decoupled motion guidance. It encodes appearance and motion signals into the dual input space of a multimodal diffusion transformer (MMDiT), fusing them within a shared context space to synthesize temporally consistent and physically plausible interactions. To optimize training, we integrate a parameter-space HOI adapter initialized from pretrained MMDiT weights, preserving prior knowledge while enabling efficient adaptation, and a facial cross-attention adapter for anatomically accurate audio-driven lip synchronization. Extensive experiments confirm state-of-the-art performance in interaction naturalness and generalization under weak supervision. Finally, HunyuanVideo-HOMA demonstrates versatility in text-conditioned generation and interactive object manipulation, supported by a user-friendly demo interface. The project page is at https://anonymous.4open.science/w/homa-page-0FBE/.

</details>


### [47] [HiSin: Efficient High-Resolution Sinogram Inpainting via Resolution-Guided Progressive Inference](https://arxiv.org/abs/2506.08809)
*Jiaze E,Srutarshi Banerjee,Tekin Bicer,Guannan Wang,Yanfu Zhang,Bin Ren*

Main category: cs.CV

TL;DR: HiSin是一种基于扩散模型的高效正弦图修复框架，通过分辨率引导的渐进推理实现内存高效修复。


<details>
  <summary>Details</summary>
Motivation: 高分辨率正弦图修复对CT重建至关重要，但现有扩散模型因内存和计算需求过高而受限。

Method: HiSin采用分辨率引导的渐进推理，先在低分辨率提取全局结构，再在小块上进行高分辨率推理，并结合频率感知块跳过和结构自适应步长分配以减少冗余计算。

Result: 实验表明，HiSin峰值内存使用减少31.25%，推理时间减少18.15%，并在不同数据集、分辨率和掩码条件下保持修复精度。

Conclusion: HiSin通过渐进推理和优化策略，有效解决了高分辨率正弦图修复的内存和计算问题。

Abstract: High-resolution sinogram inpainting is essential for computed tomography reconstruction, as missing high-frequency projections can lead to visible artifacts and diagnostic errors. Diffusion models are well-suited for this task due to their robustness and detail-preserving capabilities, but their application to high-resolution inputs is limited by excessive memory and computational demands. To address this limitation, we propose HiSin, a novel diffusion based framework for efficient sinogram inpainting via resolution-guided progressive inference. It progressively extracts global structure at low resolution and defers high-resolution inference to small patches, enabling memory-efficient inpainting. It further incorporates frequency-aware patch skipping and structure-adaptive step allocation to reduce redundant computation. Experimental results show that HiSin reduces peak memory usage by up to 31.25% and inference time by up to 18.15%, and maintains inpainting accuracy across datasets, resolutions, and mask conditions.

</details>


### [48] [StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams](https://arxiv.org/abs/2506.08862)
*Zike Wu,Qi Yan,Xuanyu Yi,Lele Wang,Renjie Liao*

Main category: cs.CV

TL;DR: StreamSplat是一个实时动态3D场景重建框架，通过前馈方式处理未校准视频流，解决了实时性、动态建模和长期稳定性的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时处理未校准输入、动态场景建模和长期稳定性，因此需要一种新方法来解决这些问题。

Method: StreamSplat采用静态编码器中的概率采样机制和动态解码器中的双向变形场，实现高效的动态3D高斯分布建模。

Result: 实验表明，StreamSplat在重建质量和动态场景建模上优于现有方法，并支持任意长度视频流的在线重建。

Conclusion: StreamSplat为动态3D场景重建提供了一种高效且稳定的解决方案，适用于实际应用。

Abstract: Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams is crucial for numerous real-world applications. However, existing methods struggle to jointly address three key challenges: 1) processing uncalibrated inputs in real time, 2) accurately modeling dynamic scene evolution, and 3) maintaining long-term stability and computational efficiency. To this end, we introduce StreamSplat, the first fully feed-forward framework that transforms uncalibrated video streams of arbitrary length into dynamic 3D Gaussian Splatting (3DGS) representations in an online manner, capable of recovering scene dynamics from temporally local observations. We propose two key technical innovations: a probabilistic sampling mechanism in the static encoder for 3DGS position prediction, and a bidirectional deformation field in the dynamic decoder that enables robust and efficient dynamic modeling. Extensive experiments on static and dynamic benchmarks demonstrate that StreamSplat consistently outperforms prior works in both reconstruction quality and dynamic scene modeling, while uniquely supporting online reconstruction of arbitrarily long video streams. Code and models are available at https://github.com/nickwzk/StreamSplat.

</details>


### [49] [Product of Experts for Visual Generation](https://arxiv.org/abs/2506.08894)
*Yunzhi Zhang,Carson Murtuza-Lanier,Zizhang Li,Yilun Du,Jiajun Wu*

Main category: cs.CV

TL;DR: 提出了一种基于专家乘积（PoE）的框架，通过异构模型的推理时间知识组合，提升图像和视频合成的可控性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现代神经模型在共享数据域（如图像和视频）上具有丰富的先验和互补知识，但如何整合来自不同来源（如视觉生成模型、视觉语言模型和人类知识来源）的多样化知识尚未充分探索。

Method: 采用专家乘积（PoE）框架，通过退火重要性采样（AIS）从异构模型的乘积分布中采样，实现训练自由的知识组合。

Result: 在图像和视频合成任务中表现出实际优势，比单一方法具有更好的可控性，并提供灵活的用户界面以指定视觉生成目标。

Conclusion: 该框架为异构模型的知识整合提供了一种有效且灵活的方法，适用于视觉生成任务。

Abstract: Modern neural models capture rich priors and have complementary knowledge over shared data domains, e.g., images and videos. Integrating diverse knowledge from multiple sources -- including visual generative models, visual language models, and sources with human-crafted knowledge such as graphics engines and physics simulators -- remains under-explored. We propose a Product of Experts (PoE) framework that performs inference-time knowledge composition from heterogeneous models. This training-free approach samples from the product distribution across experts via Annealed Importance Sampling (AIS). Our framework shows practical benefits in image and video synthesis tasks, yielding better controllability than monolithic methods and additionally providing flexible user interfaces for specifying visual generation goals.

</details>


### [50] [SkipVAR: Accelerating Visual Autoregressive Modeling via Adaptive Frequency-Aware Skipping](https://arxiv.org/abs/2506.08908)
*Jiajun Li,Yue Ma,Xinyu Zhang,Qingyan Wei,Songhua Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: 论文提出SkipVAR框架，通过选择性跳过生成步骤和替换无条件分支来优化VAR模型的推理效率，实现1.81倍加速且保持模型质量。


<details>
  <summary>Details</summary>
Motivation: 研究发现VAR模型的高频组件或后期步骤导致推理延迟，但计算冗余未被充分研究。

Method: 提出自动跳过步骤策略和替换无条件分支技术，并设计样本自适应框架SkipVAR。

Result: 实验显示SkipVAR平均SSIM达0.88，最高加速1.81倍，GenEval基准上提速2.62倍。

Conclusion: 频率感知的自适应加速策略有效，为可扩展自回归图像生成提供训练免费方案。

Abstract: Recent studies on Visual Autoregressive (VAR) models have highlighted that high-frequency components, or later steps, in the generation process contribute disproportionately to inference latency. However, the underlying computational redundancy involved in these steps has yet to be thoroughly investigated. In this paper, we conduct an in-depth analysis of the VAR inference process and identify two primary sources of inefficiency: step redundancy and unconditional branch redundancy. To address step redundancy, we propose an automatic step-skipping strategy that selectively omits unnecessary generation steps to improve efficiency. For unconditional branch redundancy, we observe that the information gap between the conditional and unconditional branches is minimal. Leveraging this insight, we introduce unconditional branch replacement, a technique that bypasses the unconditional branch to reduce computational cost. Notably, we observe that the effectiveness of acceleration strategies varies significantly across different samples. Motivated by this, we propose SkipVAR, a sample-adaptive framework that leverages frequency information to dynamically select the most suitable acceleration strategy for each instance. To evaluate the role of high-frequency information, we introduce high-variation benchmark datasets that test model sensitivity to fine details. Extensive experiments show SkipVAR achieves over 0.88 average SSIM with up to 1.81x overall acceleration and 2.62x speedup on the GenEval benchmark, maintaining model quality. These results confirm the effectiveness of frequency-aware, training-free adaptive acceleration for scalable autoregressive image generation. Our code is available at https://github.com/fakerone-li/SkipVAR and has been publicly released.

</details>


### [51] [Diffuse and Disperse: Image Generation with Representation Regularization](https://arxiv.org/abs/2506.09027)
*Runqian Wang,Kaiming He*

Main category: cs.CV

TL;DR: 提出了一种名为Dispersive Loss的简单正则化方法，用于改进扩散生成模型，无需额外数据或预训练。


<details>
  <summary>Details</summary>
Motivation: 扩散生成模型通常缺乏显式正则化，且与表示学习的进展脱节。

Method: 提出Dispersive Loss，鼓励隐藏空间中的内部表示分散，类似于对比自监督学习，但无需正样本对。

Result: 在ImageNet数据集上评估，显示对多种模型均有稳定改进。

Conclusion: 该方法有望弥合生成建模与表示学习之间的差距。

Abstract: The development of diffusion-based generative models over the past decade has largely proceeded independently of progress in representation learning. These diffusion models typically rely on regression-based objectives and generally lack explicit regularization. In this work, we propose \textit{Dispersive Loss}, a simple plug-and-play regularizer that effectively improves diffusion-based generative models. Our loss function encourages internal representations to disperse in the hidden space, analogous to contrastive self-supervised learning, with the key distinction that it requires no positive sample pairs and therefore does not interfere with the sampling process used for regression. Compared to the recent method of representation alignment (REPA), our approach is self-contained and minimalist, requiring no pre-training, no additional parameters, and no external data. We evaluate Dispersive Loss on the ImageNet dataset across a range of models and report consistent improvements over widely used and strong baselines. We hope our work will help bridge the gap between generative modeling and representation learning.

</details>


### [52] [Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better](https://arxiv.org/abs/2506.09040)
*Dianyi Wang,Wei Song,Yikun Wang,Siyuan Wang,Kaicheng Yu,Zhongyu Wei,Jiaqi Wang*

Main category: cs.CV

TL;DR: 论文提出ASVR方法，通过自回归语义视觉重构联合学习视觉和文本模态，显著提升多模态理解性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型（LVLMs）仅对文本序列进行自回归监督，未充分利用视觉模态，导致无法处理无标注图像、忽略关键视觉细节及难以表达视觉中心内容。

Method: 引入自回归语义视觉重构（ASVR），在统一自回归框架中联合学习视觉和文本模态，重构图像的语义表示而非原始外观。

Result: ASVR在多种数据规模和LLM骨干上表现优异，如将LLaVA-1.5在14个多模态基准上的平均分提升5%。

Conclusion: 自回归重构语义表示能稳定提升多模态理解，而重构原始视觉外观可能损害性能。

Abstract: Typical large vision-language models (LVLMs) apply autoregressive supervision solely to textual sequences, without fully incorporating the visual modality into the learning process. This results in three key limitations: (1) an inability to utilize images without accompanying captions, (2) the risk that captions omit critical visual details, and (3) the challenge that certain vision-centric content cannot be adequately conveyed through text. As a result, current LVLMs often prioritize vision-to-language alignment while potentially overlooking fine-grained visual information. While some prior works have explored autoregressive image generation, effectively leveraging autoregressive visual supervision to enhance image understanding remains an open challenge. In this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR), which enables joint learning of visual and textual modalities within a unified autoregressive framework. We show that autoregressively reconstructing the raw visual appearance of images does not enhance and may even impair multimodal understanding. In contrast, autoregressively reconstructing the semantic representation of images consistently improves comprehension. Notably, we find that even when models are given continuous image features as input, they can effectively reconstruct discrete semantic tokens, resulting in stable and consistent improvements across a wide range of multimodal understanding benchmarks. Our approach delivers significant performance gains across varying data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is available at https://github.com/AlenjandroWang/ASVR.

</details>


### [53] [Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models](https://arxiv.org/abs/2506.09042)
*Xuanchi Ren,Yifan Lu,Tianshi Cao,Ruiyuan Gao,Shengyu Huang,Amirmojtaba Sabour,Tianchang Shen,Tobias Pfaff,Jay Zhangjie Wu,Runjian Chen,Seung Wook Kim,Jun Gao,Laura Leal-Taixe,Mike Chen,Sanja Fidler,Huan Ling*

Main category: cs.CV

TL;DR: 论文介绍了Cosmos-Drive-Dreams，一种用于生成高保真、多视角、时空一致的驾驶场景的合成数据生成（SDG）管道，旨在解决自动驾驶系统（AV）中数据收集和标注的挑战。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据的收集和标注成本高且耗时，尤其是难以捕捉对AV系统训练和测试至关重要的罕见边缘案例。

Method: 利用NVIDIA Cosmos世界基础模型，开发了专用于驾驶领域的Cosmos-Drive模型套件，生成可控、高保真、多视角且时空一致的驾驶视频。

Result: 实验表明，生成的数据有助于缓解长尾分布问题，并提升下游任务（如3D车道检测、3D目标检测和驾驶策略学习）的泛化能力。

Conclusion: Cosmos-Drive-Dreams为自动驾驶系统提供了高质量、多样化的合成数据解决方案，相关工具、数据集和模型权重已开源。

Abstract: Collecting and annotating real-world data for safety-critical physical AI systems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is especially challenging to capture rare edge cases, which play a critical role in training and testing of an AV system. To address this challenge, we introduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline that aims to generate challenging scenarios to facilitate downstream tasks such as perception and driving policy training. Powering this pipeline is Cosmos-Drive, a suite of models specialized from NVIDIA Cosmos world foundation model for the driving domain and are capable of controllable, high-fidelity, multi-view, and spatiotemporally consistent driving video generation. We showcase the utility of these models by applying Cosmos-Drive-Dreams to scale the quantity and diversity of driving datasets with high-fidelity and challenging scenarios. Experimentally, we demonstrate that our generated data helps in mitigating long-tail distribution problems and enhances generalization in downstream tasks such as 3D lane detection, 3D object detection and driving policy learning. We open source our pipeline toolkit, dataset and model weights through the NVIDIA's Cosmos platform.   Project page: https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams

</details>


### [54] [MagCache: Fast Video Generation with Magnitude-Aware Cache](https://arxiv.org/abs/2506.09045)
*Zehong Ma,Longhui Wei,Feng Wang,Shiliang Zhang,Qi Tian*

Main category: cs.CV

TL;DR: 论文提出了一种基于统一幅度规律的自适应缓存方法（MagCache），显著提升了视频扩散模型的加速效果，同时保持视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 现有加速技术依赖统一启发式方法或时间嵌入变体，容易导致输出不一致且需要大量校准。本文旨在解决这些问题。

Method: 通过观察残差输出幅度比的变化规律，提出MagCache，利用误差建模和自适应缓存策略跳过不重要时间步。

Result: MagCache在Open-Sora和Wan 2.1上分别实现2.1倍和2.68倍加速，且在LPIPS、SSIM和PSNR指标上优于现有方法。

Conclusion: MagCache是一种高效且鲁棒的加速方法，仅需单一样本校准，显著优于现有技术。

Abstract: Existing acceleration techniques for video diffusion models often rely on uniform heuristics or time-embedding variants to skip timesteps and reuse cached features. These approaches typically require extensive calibration with curated prompts and risk inconsistent outputs due to prompt-specific overfitting. In this paper, we introduce a novel and robust discovery: a unified magnitude law observed across different models and prompts. Specifically, the magnitude ratio of successive residual outputs decreases monotonically and steadily in most timesteps while rapidly in the last several steps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache) that adaptively skips unimportant timesteps using an error modeling mechanism and adaptive caching strategy. Unlike existing methods requiring dozens of curated samples for calibration, MagCache only requires a single sample for calibration. Experimental results show that MagCache achieves 2.1x and 2.68x speedups on Open-Sora and Wan 2.1, respectively, while preserving superior visual fidelity. It significantly outperforms existing methods in LPIPS, SSIM, and PSNR, under comparable computational budgets.

</details>


### [55] [CuRe: Cultural Gaps in the Long Tail of Text-to-Image Systems](https://arxiv.org/abs/2506.08071)
*Aniket Rege,Zinnia Nie,Mahesh Ramesh,Unmesh Raskar,Zhuoran Yu,Aditya Kusupati,Yong Jae Lee,Ramya Korlakai Vinayak*

Main category: cs.CV

TL;DR: CuRe是一个用于评估文本到图像系统文化代表性的新基准和评分套件，通过分析系统对文本条件信息增加的响应来量化文化偏差。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像系统数据偏向欧美文化，忽视了全球南方的文化多样性，CuRe旨在填补这一空白。

Method: 利用维基百科知识图谱构建了一个包含300个文化物品的层次化数据集，通过分析系统对文本条件变化的响应来评估文化代表性。

Result: CuRe评分与人类对感知相似性、图像-文本对齐和文化多样性的判断高度相关，适用于多种图像编码器和文本到图像系统。

Conclusion: CuRe提供了一种可扩展的方法来量化文化偏差，并开源了代码和数据集以促进未来研究。

Abstract: Popular text-to-image (T2I) systems are trained on web-scraped data, which is heavily Amero and Euro-centric, underrepresenting the cultures of the Global South. To analyze these biases, we introduce CuRe, a novel and scalable benchmarking and scoring suite for cultural representativeness that leverages the marginal utility of attribute specification to T2I systems as a proxy for human judgments. Our CuRe benchmark dataset has a novel categorical hierarchy built from the crowdsourced Wikimedia knowledge graph, with 300 cultural artifacts across 32 cultural subcategories grouped into six broad cultural axes (food, art, fashion, architecture, celebrations, and people). Our dataset's categorical hierarchy enables CuRe scorers to evaluate T2I systems by analyzing their response to increasing the informativeness of text conditioning, enabling fine-grained cultural comparisons. We empirically observe much stronger correlations of our class of scorers to human judgments of perceptual similarity, image-text alignment, and cultural diversity across image encoders (SigLIP 2, AIMV2 and DINOv2), vision-language models (OpenCLIP, SigLIP 2, Gemini 2.0 Flash) and state-of-the-art text-to-image systems, including three variants of Stable Diffusion (1.5, XL, 3.5 Large), FLUX.1 [dev], Ideogram 2.0, and DALL-E 3. The code and dataset is open-sourced and available at https://aniketrege.github.io/cure/.

</details>


### [56] [Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion Models in a Vision-Language-Action Framework](https://arxiv.org/abs/2506.08185)
*Huixin Zhan,Jason H. Moore*

Main category: cs.CV

TL;DR: 本文提出了一种基于离散扩散框架和视觉-语言-动作（VLA）管道的个性化手术风格建模方法，用于识别外科医生的独特操作风格，并探讨了隐私与性能之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统常忽略外科医生的个性化操作风格，而本文旨在通过多模态输入（如视频、语言和隐私嵌入）建模外科医生的独特行为特征。

Method: 采用离散扩散框架结合VLA管道，将手势预测建模为结构化序列去噪任务，并通过自然语言提示编码外科医生的个性化指纹。

Result: 在JIGSAWS数据集上验证了方法的有效性，能够准确重建手势序列并学习到独特的外科医生运动指纹，但发现更个性化的嵌入会增加身份泄露风险。

Conclusion: 个性化嵌入虽能提升性能，但也增加了隐私风险，需在手术建模中平衡个性化与隐私保护。

Abstract: Surgeons exhibit distinct operating styles due to differences in training, experience, and motor behavior - yet current AI systems often ignore this personalization signal. We propose a novel approach to model fine-grained, surgeon-specific fingerprinting in robotic surgery using a discrete diffusion framework integrated with a vision-language-action (VLA) pipeline. Our method formulates gesture prediction as a structured sequence denoising task, conditioned on multimodal inputs including endoscopic video, surgical intent language, and a privacy-aware embedding of surgeon identity and skill. Personalized surgeon fingerprinting is encoded through natural language prompts using third-party language models, allowing the model to retain individual behavioral style without exposing explicit identity. We evaluate our method on the JIGSAWS dataset and demonstrate that it accurately reconstructs gesture sequences while learning meaningful motion fingerprints unique to each surgeon. To quantify the privacy implications of personalization, we perform membership inference attacks and find that more expressive embeddings improve task performance but simultaneously increase susceptibility to identity leakage. These findings demonstrate that while personalized embeddings improve performance, they also increase vulnerability to identity leakage, revealing the importance of balancing personalization with privacy risk in surgical modeling. Code is available at: https://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting.

</details>


### [57] [A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation](https://arxiv.org/abs/2506.08210)
*Andrew Z. Wang,Songwei Ge,Tero Karras,Ming-Yu Liu,Yogesh Balaji*

Main category: cs.CV

TL;DR: 研究探讨了使用现代仅解码器LLMs作为文本编码器在文本到图像扩散模型中的效果，发现多层归一化平均嵌入优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型仍使用过时的T5和CLIP作为文本编码器，研究旨在探索现代LLMs的潜力。

Method: 构建标准化训练和评估流程，训练27个模型，比较12种文本编码器，分析嵌入提取方法、LLMs变体和模型大小的影响。

Result: 多层归一化平均嵌入显著提升复杂提示的对齐效果，多数LLMs表现优于基线T5模型。

Conclusion: 现代LLMs作为文本编码器在文本到图像生成中表现更优，尤其在复杂视觉语言推理任务中。

Abstract: Both text-to-image generation and large language models (LLMs) have made significant advancements. However, many text-to-image models still employ the somewhat outdated T5 and CLIP as their text encoders. In this work, we investigate the effectiveness of using modern decoder-only LLMs as text encoders for text-to-image diffusion models. We build a standardized training and evaluation pipeline that allows us to isolate and evaluate the effect of different text embeddings. We train a total of 27 text-to-image models with 12 different text encoders to analyze the critical aspects of LLMs that could impact text-to-image generation, including the approaches to extract embeddings, different LLMs variants, and model sizes. Our experiments reveal that the de facto way of using last-layer embeddings as conditioning leads to inferior performance. Instead, we explore embeddings from various layers and find that using layer-normalized averaging across all layers significantly improves alignment with complex prompts. Most LLMs with this conditioning outperform the baseline T5 model, showing enhanced performance in advanced visio-linguistic reasoning skills.

</details>


### [58] [Highly Compressed Tokenizer Can Generate Without Training](https://arxiv.org/abs/2506.08257)
*L. Lao Beyer,T. Li,X. Chen,S. Karaman,K. He*

Main category: cs.CV

TL;DR: 1D图像分词器通过高度压缩的图像表示（如32个离散标记）实现图像编辑和生成能力，无需训练生成模型。


<details>
  <summary>Details</summary>
Motivation: 探索1D图像分词器在高度压缩下仍能支持精细图像编辑和生成的潜力。

Method: 利用基于梯度的测试时优化和即插即用的损失函数（如重建或CLIP相似性）构建图像生成流程。

Result: 在修复和文本引导的图像编辑任务中生成多样且真实的样本。

Conclusion: 1D图像分词器的潜在空间具有高度表达能力，支持无需生成模型的图像编辑和生成。

Abstract: Commonly used image tokenizers produce a 2D grid of spatially arranged tokens. In contrast, so-called 1D image tokenizers represent images as highly compressed one-dimensional sequences of as few as 32 discrete tokens. We find that the high degree of compression achieved by a 1D tokenizer with vector quantization enables image editing and generative capabilities through heuristic manipulation of tokens, demonstrating that even very crude manipulations -- such as copying and replacing tokens between latent representations of images -- enable fine-grained image editing by transferring appearance and semantic attributes. Motivated by the expressivity of the 1D tokenizer's latent space, we construct an image generation pipeline leveraging gradient-based test-time optimization of tokens with plug-and-play loss functions such as reconstruction or CLIP similarity. Our approach is demonstrated for inpainting and text-guided image editing use cases, and can generate diverse and realistic samples without requiring training of any generative model.

</details>


### [59] [Seeing Voices: Generating A-Roll Video from Audio with Mirage](https://arxiv.org/abs/2506.08279)
*Aditi Sundararaman,Amogh Adishesha,Andrew Jaegle,Dan Bigioi,Hyoung-Kyu Song,Jon Kyl,Justin Mao,Kevin Lan,Mojtaba Komeili,ShahRukh Athar,Sheila Babayan,Stanislau Beliasau,William Buchwalter*

Main category: cs.CV

TL;DR: Mirage是一个音频到视频的基础模型，能够根据音频输入生成逼真、富有表现力的视频，尤其擅长生成人物说话的A-roll视频。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法要么忽略音频专注于无声图像序列生成，要么局限于特定应用领域（如重新配音），缺乏通用的音频到视频生成解决方案。

Method: Mirage采用基于自注意力的统一训练方法，支持从零开始训练或基于现有权重训练，无需特定音频架构或损失组件。

Result: Mirage生成的视频在主观质量上优于其他方法，能够根据音频输入生成逼真的人物表演视频。

Conclusion: Mirage为音频到视频生成提供了通用且高质量的解决方案，尤其在人物说话视频生成方面表现突出。

Abstract: From professional filmmaking to user-generated content, creators and consumers have long recognized that the power of video depends on the harmonious integration of what we hear (the video's audio track) with what we see (the video's image sequence). Current approaches to video generation either ignore sound to focus on general-purpose but silent image sequence generation or address both visual and audio elements but focus on restricted application domains such as re-dubbing. We introduce Mirage, an audio-to-video foundation model that excels at generating realistic, expressive output imagery from scratch given an audio input. When integrated with existing methods for speech synthesis (text-to-speech, or TTS), Mirage results in compelling multimodal video. When trained on audio-video footage of people talking (A-roll) and conditioned on audio containing speech, Mirage generates video of people delivering a believable interpretation of the performance implicit in input audio. Our central technical contribution is a unified method for training self-attention-based audio-to-video generation models, either from scratch or given existing weights. This methodology allows Mirage to retain generality as an approach to audio-to-video generation while producing outputs of superior subjective quality to methods that incorporate audio-specific architectures or loss components specific to people, speech, or details of how images or audio are captured. We encourage readers to watch and listen to the results of Mirage for themselves (see paper and comments for links).

</details>


### [60] [How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models](https://arxiv.org/abs/2506.08351)
*Huixuan Zhang,Junzhe Zhang,Xiaojun Wan*

Main category: cs.CV

TL;DR: 论文提出了一种名为Step AG的自适应引导策略，通过限制分类器自由引导在去噪的前几步，实现了20%至30%的速度提升，同时保持图像质量和文本对齐。


<details>
  <summary>Details</summary>
Motivation: 当前分类器自由引导方法需要两倍的计算步骤，成本高昂，且现有自适应引导方法缺乏分析和实证支持。

Method: 提出Step AG策略，将分类器自由引导限制在去噪的前几步，适用于通用扩散模型。

Result: 实验表明，该方法在图像质量、文本对齐和速度上均有显著提升，且适用于不同模型和设置。

Conclusion: Step AG是一种简单、通用的自适应引导策略，能显著提升效率而不牺牲生成质量。

Abstract: With the rapid development of text-to-vision generation diffusion models, classifier-free guidance has emerged as the most prevalent method for conditioning. However, this approach inherently requires twice as many steps for model forwarding compared to unconditional generation, resulting in significantly higher costs. While previous study has introduced the concept of adaptive guidance, it lacks solid analysis and empirical results, making previous method unable to be applied to general diffusion models. In this work, we present another perspective of applying adaptive guidance and propose Step AG, which is a simple, universally applicable adaptive guidance strategy. Our evaluations focus on both image quality and image-text alignment. whose results indicate that restricting classifier-free guidance to the first several denoising steps is sufficient for generating high-quality, well-conditioned images, achieving an average speedup of 20% to 30%. Such improvement is consistent across different settings such as inference steps, and various models including video generation models, highlighting the superiority of our method.

</details>


### [61] [Enhancing Motion Dynamics of Image-to-Video Models via Adaptive Low-Pass Guidance](https://arxiv.org/abs/2506.08456)
*June Suk Choi,Kyungmin Lee,Sihyun Yu,Yisol Choi,Jinwoo Shin,Kimin Lee*

Main category: cs.CV

TL;DR: 论文提出自适应低通引导（ALG）方法，解决图像到视频（I2V）生成中动态性不足的问题，显著提升视频动态性。


<details>
  <summary>Details</summary>
Motivation: 现有I2V方法因过早暴露高频细节导致视频动态性不足，ALG旨在解决这一问题。

Method: 通过自适应低通滤波在去噪早期阶段调制输入图像的频率内容。

Result: ALG显著提升视频动态性（VBench-I2V测试中动态度平均提升36%），同时保持图像质量和文本对齐。

Conclusion: ALG是一种简单有效的方法，能显著改善I2V生成的动态性。

Abstract: Recent text-to-video (T2V) models have demonstrated strong capabilities in producing high-quality, dynamic videos. To improve the visual controllability, recent works have considered fine-tuning pre-trained T2V models to support image-to-video (I2V) generation. However, such adaptation frequently suppresses motion dynamics of generated outputs, resulting in more static videos compared to their T2V counterparts. In this work, we analyze this phenomenon and identify that it stems from the premature exposure to high-frequency details in the input image, which biases the sampling process toward a shortcut trajectory that overfits to the static appearance of the reference image. To address this, we propose adaptive low-pass guidance (ALG), a simple fix to the I2V model sampling procedure to generate more dynamic videos without compromising per-frame image quality. Specifically, ALG adaptively modulates the frequency content of the conditioning image by applying low-pass filtering at the early stage of denoising. Extensive experiments demonstrate that ALG significantly improves the temporal dynamics of generated videos, while preserving image fidelity and text alignment. Especially, under VBench-I2V test suite, ALG achieves an average improvement of 36% in dynamic degree without a significant drop in video quality or image fidelity.

</details>


### [62] [LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid Temporal Modeling with Only 4$\times$RTX 4090s](https://arxiv.org/abs/2506.08529)
*Xijun Wang,Xin Li,Bingchen Li,Zhibo Chen*

Main category: cs.CV

TL;DR: LiftVSR是一种高效的视频超分辨率框架，通过结合动态时间注意力和注意力内存缓存，显著降低了计算成本，同时保持了长期一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在时间一致性和计算成本上存在不足，尤其是长视频处理需要大量资源。

Method: 提出混合时间建模机制，包括动态时间注意力（DTA）和注意力内存缓存（AMC），并引入非对称采样策略。

Result: 在多个基准测试中表现出色，计算成本显著降低。

Conclusion: LiftVSR在效率和性能上取得了平衡，是视频超分辨率领域的重要进展。

Abstract: Diffusion models have significantly advanced video super-resolution (VSR) by enhancing perceptual quality, largely through elaborately designed temporal modeling to ensure inter-frame consistency. However, existing methods usually suffer from limited temporal coherence and prohibitively high computational costs (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for long videos. In this work, we propose LiftVSR, an efficient VSR framework that leverages and elevates the image-wise diffusion prior from PixArt-$\alpha$, achieving state-of-the-art results using only 4$\times$RTX 4090 GPUs. To balance long-term consistency and efficiency, we introduce a hybrid temporal modeling mechanism that decomposes temporal learning into two complementary components: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal modeling within short frame segment ($\textit{i.e.}$, low complexity), and (ii) Attention Memory Cache (AMC) for long-term temporal modeling across segments ($\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token flows across frames within multi-head query and key tokens to warp inter-frame contexts in the value tokens. AMC adaptively aggregates historical segment information via a cache unit, ensuring long-term coherence with minimal overhead. To further stabilize the cache interaction during inference, we introduce an asymmetric sampling strategy that mitigates feature mismatches arising from different diffusion sampling steps. Extensive experiments on several typical VSR benchmarks have demonstrated that LiftVSR achieves impressive performance with significantly lower computational costs.

</details>


### [63] [A Probability-guided Sampler for Neural Implicit Surface Rendering](https://arxiv.org/abs/2506.08619)
*Gonçalo Dias Pais,Valter Piedade,Moitreya Chatterjee,Marcus Greiff,Pedro Miraldo*

Main category: cs.CV

TL;DR: 该论文提出了一种基于隐式表面表示和3D图像投影空间概率密度函数的采样策略，结合新的表面重建损失，显著提升了3D重建和图像渲染的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF变体因可扩展性问题无法对所有可能的输入数据进行训练，导致采样效率不高。本文旨在通过更针对性的采样和新型损失函数提升渲染效果。

Method: 利用隐式表面表示建模3D图像投影空间的概率密度函数，实现针对性采样；提出新的表面重建损失，结合近表面和空白空间信息。

Result: 通过集成新采样策略和损失函数，显著提升了3D重建和图像渲染的准确性，尤其在感兴趣区域表现更优。

Conclusion: 论文提出的方法在现有神经隐式表面渲染器中实现了更精确的3D重建和图像渲染，为场景中的关键区域提供了更高质量的视觉效果。

Abstract: Several variants of Neural Radiance Fields (NeRFs) have significantly improved the accuracy of synthesized images and surface reconstruction of 3D scenes/objects. In all of these methods, a key characteristic is that none can train the neural network with every possible input data, specifically, every pixel and potential 3D point along the projection rays due to scalability issues. While vanilla NeRFs uniformly sample both the image pixels and 3D points along the projection rays, some variants focus only on guiding the sampling of the 3D points along the projection rays. In this paper, we leverage the implicit surface representation of the foreground scene and model a probability density function in a 3D image projection space to achieve a more targeted sampling of the rays toward regions of interest, resulting in improved rendering. Additionally, a new surface reconstruction loss is proposed for improved performance. This new loss fully explores the proposed 3D image projection space model and incorporates near-to-surface and empty space components. By integrating our novel sampling strategy and novel loss into current state-of-the-art neural implicit surface renderers, we achieve more accurate and detailed 3D reconstructions and improved image rendering, especially for the regions of interest in any given scene.

</details>


### [64] [RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping](https://arxiv.org/abs/2506.08632)
*Yang Bai,Liudi Yang,George Eskandar,Fengyi Shen,Dong Chen,Mohammad Altillawi,Ziyuan Liu,Gitta Kutyniok*

Main category: cs.CV

TL;DR: RoboSwap是一种新型视频编辑框架，结合GAN和扩散模型，用于在未配对数据中替换机器人手臂，提升跨平台机器人学习的数据生成能力。


<details>
  <summary>Details</summary>
Motivation: 解决视频条件下机器人学习中数据稀缺和跨平台泛化能力不足的问题。

Method: 提出RoboSwap框架，结合GAN和扩散模型，分割机器人手臂并翻译为另一种手臂，再通过扩散模型增强视频连贯性和运动真实性。

Result: 在三个基准测试中，RoboSwap在结构连贯性和运动一致性上优于现有视频和图像编辑模型。

Conclusion: RoboSwap为机器人学习提供了可靠的跨平台数据生成解决方案。

Abstract: Recent advancements in generative models have revolutionized video synthesis and editing. However, the scarcity of diverse, high-quality datasets continues to hinder video-conditioned robotic learning, limiting cross-platform generalization. In this work, we address the challenge of swapping a robotic arm in one video with another: a key step for crossembodiment learning. Unlike previous methods that depend on paired video demonstrations in the same environmental settings, our proposed framework, RoboSwap, operates on unpaired data from diverse environments, alleviating the data collection needs. RoboSwap introduces a novel video editing pipeline integrating both GANs and diffusion models, combining their isolated advantages. Specifically, we segment robotic arms from their backgrounds and train an unpaired GAN model to translate one robotic arm to another. The translated arm is blended with the original video background and refined with a diffusion model to enhance coherence, motion realism and object interaction. The GAN and diffusion stages are trained independently. Our experiments demonstrate that RoboSwap outperforms state-of-the-art video and image editing models on three benchmarks in terms of both structural coherence and motion consistency, thereby offering a robust solution for generating reliable, cross-embodiment data in robotic learning.

</details>


### [65] [Orientation Matters: Making 3D Generative Models Orientation-Aligned](https://arxiv.org/abs/2506.08640)
*Yichong Lu,Yuzhuo Tian,Zijin Jiang,Yikun Zhao,Yuanbo Yang,Hao Ouyang,Haoji Hu,Huimin Yu,Yujun Shen,Yiyi Liao*

Main category: cs.CV

TL;DR: 提出了一种方向对齐的3D物体生成任务，并构建了Objaverse-OA数据集，通过微调现有模型实现了跨类别的一致性生成。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成模型因训练数据不一致导致结果未对齐，限制了其在下游任务中的应用。

Method: 构建Objaverse-OA数据集，基于多视角扩散和3D变分自编码器框架微调模型。

Result: 实验表明方法优于后处理对齐方法，并展示了零样本方向估计和高效旋转操作等下游应用。

Conclusion: 方向对齐的3D生成方法显著提升了模型的一致性和实用性。

Abstract: Humans intuitively perceive object shape and orientation from a single image, guided by strong priors about canonical poses. However, existing 3D generative models often produce misaligned results due to inconsistent training data, limiting their usability in downstream tasks. To address this gap, we introduce the task of orientation-aligned 3D object generation: producing 3D objects from single images with consistent orientations across categories. To facilitate this, we construct Objaverse-OA, a dataset of 14,832 orientation-aligned 3D models spanning 1,008 categories. Leveraging Objaverse-OA, we fine-tune two representative 3D generative models based on multi-view diffusion and 3D variational autoencoder frameworks to produce aligned objects that generalize well to unseen objects across various categories. Experimental results demonstrate the superiority of our method over post-hoc alignment approaches. Furthermore, we showcase downstream applications enabled by our aligned object generation, including zero-shot object orientation estimation via analysis-by-synthesis and efficient arrow-based object rotation manipulation.

</details>


### [66] [TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary Large-Scale Scene Rendering](https://arxiv.org/abs/2506.08704)
*Xiaohan Zhang,Sitong Wang,Yushen Yan,Yi Yang,Mingda Xu,Qi Liu*

Main category: cs.CV

TL;DR: TraGraph-GS提出了一种基于轨迹图的空间分区方法，解决了大规模场景中高斯重叠和纹理失真的问题，显著提升了新视角合成的质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理大规模场景时因刚性空间分区和高斯重叠导致泛化能力不足，无法适应任意相机轨迹。

Method: 利用轨迹图进行空间分区，引入正则化约束和渐进渲染策略以减少高斯重叠和纹理失真。

Result: 在四个空中和四个地面数据集上，PSNR平均提升了1.86 dB和1.62 dB，优于现有方法。

Conclusion: TraGraph-GS通过轨迹图和渐进渲染策略，显著提升了大规模场景的新视角合成效果。

Abstract: High-quality novel view synthesis for large-scale scenes presents a challenging dilemma in 3D computer vision. Existing methods typically partition large scenes into multiple regions, reconstruct a 3D representation using Gaussian splatting for each region, and eventually merge them for novel view rendering. They can accurately render specific scenes, yet they do not generalize effectively for two reasons: (1) rigid spatial partition techniques struggle with arbitrary camera trajectories, and (2) the merging of regions results in Gaussian overlap to distort texture details. To address these challenges, we propose TraGraph-GS, leveraging a trajectory graph to enable high-precision rendering for arbitrarily large-scale scenes. We present a spatial partitioning method for large-scale scenes based on graphs, which incorporates a regularization constraint to enhance the rendering of textures and distant objects, as well as a progressive rendering strategy to mitigate artifacts caused by Gaussian overlap. Experimental results demonstrate its superior performance both on four aerial and four ground datasets and highlight its remarkable efficiency: our method achieves an average improvement of 1.86 dB in PSNR on aerial datasets and 1.62 dB on ground datasets compared to state-of-the-art approaches.

</details>


### [67] [SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting](https://arxiv.org/abs/2506.08710)
*Mengjiao Ma,Qi Ma,Yue Li,Jiahuan Cheng,Runyi Yang,Bin Ren,Nikola Popovic,Mingqiang Wei,Nicu Sebe,Luc Van Gool,Theo Gevers,Martin R. Oswald,Danda Pani Paudel*

Main category: cs.CV

TL;DR: 论文提出了首个大规模基准测试，系统评估3D高斯泼溅（3DGS）在3D空间中的表现，并引入新数据集GaussianWorld-49K，展示了通用化方法的优势。


<details>
  <summary>Details</summary>
Motivation: 当前基于3DGS的语言方法多局限于少量场景和视角的2D渲染评估，缺乏对整体3D理解的深入分析。

Method: 提出大规模基准测试，评估三类3DGS方法（优化、非优化、通用化），并引入49K场景数据集。

Result: 通用化方法在放松场景限制、快速推理和分割性能上表现优越。

Conclusion: 通用化3DGS方法具有潜力，公开代码和数据集以推动研究。

Abstract: 3D Gaussian Splatting (3DGS) serves as a highly performant and efficient encoding of scene geometry, appearance, and semantics. Moreover, grounding language in 3D scenes has proven to be an effective strategy for 3D scene understanding. Current Language Gaussian Splatting line of work fall into three main groups: (i) per-scene optimization-based, (ii) per-scene optimization-free, and (iii) generalizable approach. However, most of them are evaluated only on rendered 2D views of a handful of scenes and viewpoints close to the training views, limiting ability and insight into holistic 3D understanding. To address this gap, we propose the first large-scale benchmark that systematically assesses these three groups of methods directly in 3D space, evaluating on 1060 scenes across three indoor datasets and one outdoor dataset. Benchmark results demonstrate a clear advantage of the generalizable paradigm, particularly in relaxing the scene-specific limitation, enabling fast feed-forward inference on novel scenes, and achieving superior segmentation performance. We further introduce GaussianWorld-49K a carefully curated 3DGS dataset comprising around 49K diverse indoor and outdoor scenes obtained from multiple sources, with which we demonstrate the generalizable approach could harness strong data priors. Our codes, benchmark, and datasets will be made public to accelerate research in generalizable 3DGS scene understanding.

</details>


### [68] [Gaussian2Scene: 3D Scene Representation Learning via Self-supervised Learning with 3D Gaussian Splatting](https://arxiv.org/abs/2506.08777)
*Keyi Liu,Weidong Yang,Ben Fei,Ying He*

Main category: cs.CV

TL;DR: 论文提出了一种名为Gaussian2Scene的自监督学习框架，利用3D高斯泼溅（3DGS）进行点云预训练，解决了现有方法依赖隐式场景表示和高内存需求的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法在场景级别依赖体积渲染和RGB-D图像，存在内存需求高且难以捕捉3D几何结构的局限性。

Method: 采用两阶段训练策略：第一阶段通过双分支掩码自编码器学习2D和3D场景表示；第二阶段利用重建点云和高斯基元的几何位置进行监督学习。

Result: 在多个下游3D物体检测任务中表现优于现有预训练方法。

Conclusion: Gaussian2Scene通过显式3DGS和两阶段训练策略，显著提升了3D几何理解和跨模态学习效果。

Abstract: Self-supervised learning (SSL) for point cloud pre-training has become a cornerstone for many 3D vision tasks, enabling effective learning from large-scale unannotated data. At the scene level, existing SSL methods often incorporate volume rendering into the pre-training framework, using RGB-D images as reconstruction signals to facilitate cross-modal learning. This strategy promotes alignment between 2D and 3D modalities and enables the model to benefit from rich visual cues in the RGB-D inputs. However, these approaches are limited by their reliance on implicit scene representations and high memory demands. Furthermore, since their reconstruction objectives are applied only in 2D space, they often fail to capture underlying 3D geometric structures. To address these challenges, we propose Gaussian2Scene, a novel scene-level SSL framework that leverages the efficiency and explicit nature of 3D Gaussian Splatting (3DGS) for pre-training. The use of 3DGS not only alleviates the computational burden associated with volume rendering but also supports direct 3D scene reconstruction, thereby enhancing the geometric understanding of the backbone network. Our approach follows a progressive two-stage training strategy. In the first stage, a dual-branch masked autoencoder learns both 2D and 3D scene representations. In the second stage, we initialize training with reconstructed point clouds and further supervise learning using the geometric locations of Gaussian primitives and rendered RGB images. This process reinforces both geometric and cross-modal learning. We demonstrate the effectiveness of Gaussian2Scene across several downstream 3D object detection tasks, showing consistent improvements over existing pre-training methods.

</details>


### [69] [A PDE-Based Image Dehazing Method via Atmospheric Scattering Theory](https://arxiv.org/abs/2506.08793)
*Zhuoran Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种基于偏微分方程（PDE）的单图像去雾方法，结合大气散射模型、非局部正则化和暗通道先验，改进了PDE框架，并通过理论证明和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决单图像去雾问题，通过改进PDE框架提高去雾效果，并探索其在深度学习模型中的泛化能力。

Method: 提出改进的PDE方程，结合边缘保持扩散系数、高斯卷积算子和自适应正则化参数，利用Lax-Milgram定理证明解的存在唯一性，并通过PyTorch GPU加速实现固定点迭代方案。

Result: 实验结果表明，该方法是一种有效的去雾解决方案，并具备在深度学习模型中泛化的潜力。

Conclusion: 该方法在理论和实验上均表现出色，为单图像去雾提供了新的思路，并展示了与深度学习结合的潜力。

Abstract: This paper presents a novel partial differential equation (PDE) framework for single-image dehazing. By integrating the atmospheric scattering model with nonlocal regularization and dark channel prior, we propose the improved PDE: \[ -\text{div}\left(D(\nabla u)\nabla u\right) + \lambda(t) G(u) = \Phi(I,t,A) \] where $D(\nabla u) = (|\nabla u| + \epsilon)^{-1}$ is the edge-preserving diffusion coefficient, $G(u)$ is the Gaussian convolution operator, and $\lambda(t)$ is the adaptive regularization parameter based on transmission map $t$. We prove the existence and uniqueness of weak solutions in $H_0^1(\Omega)$ using Lax-Milgram theorem, and implement an efficient fixed-point iteration scheme accelerated by PyTorch GPU computation. The experimental results demonstrate that this method is a promising deghazing solution that can be generalized to the deep model paradigm.

</details>


### [70] [HunyuanVideo-HOMA: Generic Human-Object Interaction in Multimodal Driven Human Animation](https://arxiv.org/abs/2506.08797)
*Ziyao Huang,Zixiang Zhou,Juan Cao,Yifeng Ma,Yi Chen,Zejing Rao,Zhiyong Xu,Hongmei Wang,Qin Lin,Yuan Zhou,Qinglin Lu,Fan Tang*

Main category: cs.CV

TL;DR: HunyuanVideo-HOMA是一个弱条件多模态驱动框架，通过稀疏解耦运动指导提升可控性，减少对精确输入的依赖，实现人-物交互视频生成。


<details>
  <summary>Details</summary>
Motivation: 解决人-物交互视频生成中对精选运动数据的依赖、对新物体/场景的泛化能力有限以及可访问性受限等关键问题。

Method: 通过多模态扩散变换器（MMDiT）将外观和运动信号编码到双输入空间，并在共享上下文空间中融合，结合参数空间HOI适配器和面部交叉注意力适配器优化训练。

Result: 在弱监督下实现交互自然性和泛化能力的先进性能，支持文本条件生成和交互式物体操作。

Conclusion: HunyuanVideo-HOMA在提升可控性和泛化能力方面表现出色，具有广泛的应用潜力。

Abstract: To address key limitations in human-object interaction (HOI) video generation -- specifically the reliance on curated motion data, limited generalization to novel objects/scenarios, and restricted accessibility -- we introduce HunyuanVideo-HOMA, a weakly conditioned multimodal-driven framework. HunyuanVideo-HOMA enhances controllability and reduces dependency on precise inputs through sparse, decoupled motion guidance. It encodes appearance and motion signals into the dual input space of a multimodal diffusion transformer (MMDiT), fusing them within a shared context space to synthesize temporally consistent and physically plausible interactions. To optimize training, we integrate a parameter-space HOI adapter initialized from pretrained MMDiT weights, preserving prior knowledge while enabling efficient adaptation, and a facial cross-attention adapter for anatomically accurate audio-driven lip synchronization. Extensive experiments confirm state-of-the-art performance in interaction naturalness and generalization under weak supervision. Finally, HunyuanVideo-HOMA demonstrates versatility in text-conditioned generation and interactive object manipulation, supported by a user-friendly demo interface. The project page is at https://anonymous.4open.science/w/homa-page-0FBE/.

</details>


### [71] [HiSin: Efficient High-Resolution Sinogram Inpainting via Resolution-Guided Progressive Inference](https://arxiv.org/abs/2506.08809)
*Jiaze E,Srutarshi Banerjee,Tekin Bicer,Guannan Wang,Yanfu Zhang,Bin Ren*

Main category: cs.CV

TL;DR: HiSin是一种基于扩散模型的高效正弦图修复框架，通过分辨率引导的渐进推理降低内存和计算需求。


<details>
  <summary>Details</summary>
Motivation: 高分辨率正弦图修复对CT重建至关重要，但现有扩散模型因内存和计算需求过高而受限。

Method: HiSin采用分辨率引导的渐进推理，先提取低分辨率全局结构，再处理高分辨率小补丁，并结合频率感知补丁跳过和结构自适应步长分配。

Result: 实验显示，HiSin峰值内存使用减少31.25%，推理时间减少18.15%，且修复精度不受影响。

Conclusion: HiSin为高分辨率正弦图修复提供了一种高效且准确的解决方案。

Abstract: High-resolution sinogram inpainting is essential for computed tomography reconstruction, as missing high-frequency projections can lead to visible artifacts and diagnostic errors. Diffusion models are well-suited for this task due to their robustness and detail-preserving capabilities, but their application to high-resolution inputs is limited by excessive memory and computational demands. To address this limitation, we propose HiSin, a novel diffusion based framework for efficient sinogram inpainting via resolution-guided progressive inference. It progressively extracts global structure at low resolution and defers high-resolution inference to small patches, enabling memory-efficient inpainting. It further incorporates frequency-aware patch skipping and structure-adaptive step allocation to reduce redundant computation. Experimental results show that HiSin reduces peak memory usage by up to 31.25% and inference time by up to 18.15%, and maintains inpainting accuracy across datasets, resolutions, and mask conditions.

</details>


### [72] [StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams](https://arxiv.org/abs/2506.08862)
*Zike Wu,Qi Yan,Xuanyu Yi,Lele Wang,Renjie Liao*

Main category: cs.CV

TL;DR: StreamSplat是一个实时从无标定视频流重建动态3D场景的框架，解决了实时处理、动态建模和长期稳定性的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时处理无标定输入、动态场景建模和长期稳定性，StreamSplat旨在解决这些问题。

Method: 提出静态编码器中的概率采样机制和动态解码器中的双向变形场，实现高效动态建模。

Result: 在静态和动态基准测试中表现优于现有方法，支持任意长度视频流的在线重建。

Conclusion: StreamSplat在重建质量和动态场景建模上具有优势，且支持实时处理。

Abstract: Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams is crucial for numerous real-world applications. However, existing methods struggle to jointly address three key challenges: 1) processing uncalibrated inputs in real time, 2) accurately modeling dynamic scene evolution, and 3) maintaining long-term stability and computational efficiency. To this end, we introduce StreamSplat, the first fully feed-forward framework that transforms uncalibrated video streams of arbitrary length into dynamic 3D Gaussian Splatting (3DGS) representations in an online manner, capable of recovering scene dynamics from temporally local observations. We propose two key technical innovations: a probabilistic sampling mechanism in the static encoder for 3DGS position prediction, and a bidirectional deformation field in the dynamic decoder that enables robust and efficient dynamic modeling. Extensive experiments on static and dynamic benchmarks demonstrate that StreamSplat consistently outperforms prior works in both reconstruction quality and dynamic scene modeling, while uniquely supporting online reconstruction of arbitrarily long video streams. Code and models are available at https://github.com/nickwzk/StreamSplat.

</details>


### [73] [Product of Experts for Visual Generation](https://arxiv.org/abs/2506.08894)
*Yunzhi Zhang,Carson Murtuza-Lanier,Zizhang Li,Yilun Du,Jiajun Wu*

Main category: cs.CV

TL;DR: 提出了一种基于Product of Experts (PoE)框架的训练无关方法，通过Annealed Importance Sampling (AIS)从异构模型中组合知识，提升了图像和视频合成的可控性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现代神经模型具有丰富的先验知识和互补性，但如何整合来自不同来源（如生成模型、视觉语言模型和人类知识）的多样化知识仍待探索。

Method: 采用Product of Experts (PoE)框架，通过Annealed Importance Sampling (AIS)在推理时从异构模型中组合知识。

Result: 在图像和视频合成任务中表现出色，比单一方法更具可控性，并提供灵活的用户界面以指定生成目标。

Conclusion: 该框架为异构知识组合提供了一种有效的训练无关方法，具有实际应用价值。

Abstract: Modern neural models capture rich priors and have complementary knowledge over shared data domains, e.g., images and videos. Integrating diverse knowledge from multiple sources -- including visual generative models, visual language models, and sources with human-crafted knowledge such as graphics engines and physics simulators -- remains under-explored. We propose a Product of Experts (PoE) framework that performs inference-time knowledge composition from heterogeneous models. This training-free approach samples from the product distribution across experts via Annealed Importance Sampling (AIS). Our framework shows practical benefits in image and video synthesis tasks, yielding better controllability than monolithic methods and additionally providing flexible user interfaces for specifying visual generation goals.

</details>


### [74] [SkipVAR: Accelerating Visual Autoregressive Modeling via Adaptive Frequency-Aware Skipping](https://arxiv.org/abs/2506.08908)
*Jiajun Li,Yue Ma,Xinyu Zhang,Qingyan Wei,Songhua Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: 论文提出SkipVAR框架，通过动态跳过冗余生成步骤和替换无条件分支，显著加速视觉自回归模型的推理过程，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有视觉自回归模型在高频生成步骤中存在计算冗余，导致推理延迟增加，但相关研究尚未深入。

Method: 提出自动跳过步骤策略和无条件分支替换技术，并基于频率信息动态选择加速策略。

Result: SkipVAR在保持模型质量的同时，实现了1.81倍整体加速和2.62倍速度提升。

Conclusion: 频率感知的自适应加速策略有效提升了自回归图像生成的可扩展性。

Abstract: Recent studies on Visual Autoregressive (VAR) models have highlighted that high-frequency components, or later steps, in the generation process contribute disproportionately to inference latency. However, the underlying computational redundancy involved in these steps has yet to be thoroughly investigated. In this paper, we conduct an in-depth analysis of the VAR inference process and identify two primary sources of inefficiency: step redundancy and unconditional branch redundancy. To address step redundancy, we propose an automatic step-skipping strategy that selectively omits unnecessary generation steps to improve efficiency. For unconditional branch redundancy, we observe that the information gap between the conditional and unconditional branches is minimal. Leveraging this insight, we introduce unconditional branch replacement, a technique that bypasses the unconditional branch to reduce computational cost. Notably, we observe that the effectiveness of acceleration strategies varies significantly across different samples. Motivated by this, we propose SkipVAR, a sample-adaptive framework that leverages frequency information to dynamically select the most suitable acceleration strategy for each instance. To evaluate the role of high-frequency information, we introduce high-variation benchmark datasets that test model sensitivity to fine details. Extensive experiments show SkipVAR achieves over 0.88 average SSIM with up to 1.81x overall acceleration and 2.62x speedup on the GenEval benchmark, maintaining model quality. These results confirm the effectiveness of frequency-aware, training-free adaptive acceleration for scalable autoregressive image generation. Our code is available at https://github.com/fakerone-li/SkipVAR and has been publicly released.

</details>


### [75] [Diffuse and Disperse: Image Generation with Representation Regularization](https://arxiv.org/abs/2506.09027)
*Runqian Wang,Kaiming He*

Main category: cs.CV

TL;DR: 提出了一种名为Dispersive Loss的简单正则化方法，用于改进扩散生成模型，无需额外数据或预训练。


<details>
  <summary>Details</summary>
Motivation: 扩散生成模型通常缺乏显式正则化，且与表示学习进展脱节。

Method: 提出Dispersive Loss，鼓励隐藏空间中的表示分散，类似于对比自监督学习，但无需正样本对。

Result: 在ImageNet数据集上评估，显示对多种模型的性能均有提升。

Conclusion: 该方法有望弥合生成建模与表示学习之间的差距。

Abstract: The development of diffusion-based generative models over the past decade has largely proceeded independently of progress in representation learning. These diffusion models typically rely on regression-based objectives and generally lack explicit regularization. In this work, we propose \textit{Dispersive Loss}, a simple plug-and-play regularizer that effectively improves diffusion-based generative models. Our loss function encourages internal representations to disperse in the hidden space, analogous to contrastive self-supervised learning, with the key distinction that it requires no positive sample pairs and therefore does not interfere with the sampling process used for regression. Compared to the recent method of representation alignment (REPA), our approach is self-contained and minimalist, requiring no pre-training, no additional parameters, and no external data. We evaluate Dispersive Loss on the ImageNet dataset across a range of models and report consistent improvements over widely used and strong baselines. We hope our work will help bridge the gap between generative modeling and representation learning.

</details>


### [76] [Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better](https://arxiv.org/abs/2506.09040)
*Dianyi Wang,Wei Song,Yikun Wang,Siyuan Wang,Kaicheng Yu,Zhongyu Wei,Jiaqi Wang*

Main category: cs.CV

TL;DR: 论文提出ASVR方法，通过自回归语义视觉重建联合学习视觉和文本模态，提升多模态理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型仅对文本序列进行自回归监督，未充分利用视觉模态，导致无法处理无标注图像、遗漏视觉细节及难以表达视觉中心内容。

Method: 引入ASVR，在统一自回归框架中联合学习视觉和文本模态，通过自回归重建图像的语义表示而非原始视觉外观。

Result: ASVR在多种数据规模和LLM骨干上表现优异，显著提升多模态理解能力，如LLaVA-1.5在14个基准测试中平均提升5%。

Conclusion: 自回归语义视觉重建能稳定提升多模态理解，且适用于不同规模和类型的模型。

Abstract: Typical large vision-language models (LVLMs) apply autoregressive supervision solely to textual sequences, without fully incorporating the visual modality into the learning process. This results in three key limitations: (1) an inability to utilize images without accompanying captions, (2) the risk that captions omit critical visual details, and (3) the challenge that certain vision-centric content cannot be adequately conveyed through text. As a result, current LVLMs often prioritize vision-to-language alignment while potentially overlooking fine-grained visual information. While some prior works have explored autoregressive image generation, effectively leveraging autoregressive visual supervision to enhance image understanding remains an open challenge. In this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR), which enables joint learning of visual and textual modalities within a unified autoregressive framework. We show that autoregressively reconstructing the raw visual appearance of images does not enhance and may even impair multimodal understanding. In contrast, autoregressively reconstructing the semantic representation of images consistently improves comprehension. Notably, we find that even when models are given continuous image features as input, they can effectively reconstruct discrete semantic tokens, resulting in stable and consistent improvements across a wide range of multimodal understanding benchmarks. Our approach delivers significant performance gains across varying data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is available at https://github.com/AlenjandroWang/ASVR.

</details>


### [77] [Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models](https://arxiv.org/abs/2506.09042)
*Xuanchi Ren,Yifan Lu,Tianshi Cao,Ruiyuan Gao,Shengyu Huang,Amirmojtaba Sabour,Tianchang Shen,Tobias Pfaff,Jay Zhangjie Wu,Runjian Chen,Seung Wook Kim,Jun Gao,Laura Leal-Taixe,Mike Chen,Sanja Fidler,Huan Ling*

Main category: cs.CV

TL;DR: Cosmos-Drive-Dreams是一个合成数据生成（SDG）管道，旨在生成具有挑战性的场景，以支持自动驾驶系统的感知和驾驶策略训练。


<details>
  <summary>Details</summary>
Motivation: 收集和标注真实世界数据用于安全关键的物理AI系统（如自动驾驶车辆）耗时且昂贵，尤其是难以捕捉罕见的边缘案例。

Method: 利用NVIDIA Cosmos世界基础模型开发的Cosmos-Drive模型，生成可控、高保真、多视角且时空一致的驾驶视频。

Result: 生成的数据有助于缓解长尾分布问题，并提升下游任务（如3D车道检测、3D物体检测和驾驶策略学习）的泛化能力。

Conclusion: Cosmos-Drive-Dreams通过开源工具包、数据集和模型权重，为自动驾驶领域提供了高效的数据生成解决方案。

Abstract: Collecting and annotating real-world data for safety-critical physical AI systems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is especially challenging to capture rare edge cases, which play a critical role in training and testing of an AV system. To address this challenge, we introduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline that aims to generate challenging scenarios to facilitate downstream tasks such as perception and driving policy training. Powering this pipeline is Cosmos-Drive, a suite of models specialized from NVIDIA Cosmos world foundation model for the driving domain and are capable of controllable, high-fidelity, multi-view, and spatiotemporally consistent driving video generation. We showcase the utility of these models by applying Cosmos-Drive-Dreams to scale the quantity and diversity of driving datasets with high-fidelity and challenging scenarios. Experimentally, we demonstrate that our generated data helps in mitigating long-tail distribution problems and enhances generalization in downstream tasks such as 3D lane detection, 3D object detection and driving policy learning. We open source our pipeline toolkit, dataset and model weights through the NVIDIA's Cosmos platform.   Project page: https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams

</details>


### [78] [MagCache: Fast Video Generation with Magnitude-Aware Cache](https://arxiv.org/abs/2506.09045)
*Zehong Ma,Longhui Wei,Feng Wang,Shiliang Zhang,Qi Tian*

Main category: cs.CV

TL;DR: 提出了一种基于残差输出幅度规律的自适应缓存策略（MagCache），显著加速视频扩散模型，同时保持视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 现有加速方法依赖统一启发式或时间嵌入变体，易导致输出不一致且需大量校准样本。

Method: 发现残差输出幅度比单调递减规律，设计MagCache自适应跳过不重要时间步，仅需单样本校准。

Result: 在Open-Sora和Wan 2.1上分别实现2.1倍和2.68倍加速，且LPIPS、SSIM、PSNR优于现有方法。

Conclusion: MagCache是一种高效、鲁棒的视频扩散模型加速方法，显著优于现有技术。

Abstract: Existing acceleration techniques for video diffusion models often rely on uniform heuristics or time-embedding variants to skip timesteps and reuse cached features. These approaches typically require extensive calibration with curated prompts and risk inconsistent outputs due to prompt-specific overfitting. In this paper, we introduce a novel and robust discovery: a unified magnitude law observed across different models and prompts. Specifically, the magnitude ratio of successive residual outputs decreases monotonically and steadily in most timesteps while rapidly in the last several steps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache) that adaptively skips unimportant timesteps using an error modeling mechanism and adaptive caching strategy. Unlike existing methods requiring dozens of curated samples for calibration, MagCache only requires a single sample for calibration. Experimental results show that MagCache achieves 2.1x and 2.68x speedups on Open-Sora and Wan 2.1, respectively, while preserving superior visual fidelity. It significantly outperforms existing methods in LPIPS, SSIM, and PSNR, under comparable computational budgets.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [79] [Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models](https://arxiv.org/abs/2506.08480)
*Huixuan Zhang,Xiaojun Wan*

Main category: cs.CL

TL;DR: 论文指出现有文本到图像生成评估框架的不足，提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注与人类判断的一致性，忽略了评估框架的其他关键特性。

Method: 识别可靠评估的两个关键方面，实证分析现有框架的不足。

Result: 主流评估框架未能满足这些特性。

Conclusion: 提出改进图像-文本对齐评估的建议。

Abstract: Text-to-image models often struggle to generate images that precisely match textual prompts. Prior research has extensively studied the evaluation of image-text alignment in text-to-image generation. However, existing evaluations primarily focus on agreement with human assessments, neglecting other critical properties of a trustworthy evaluation framework. In this work, we first identify two key aspects that a reliable evaluation should address. We then empirically demonstrate that current mainstream evaluation frameworks fail to fully satisfy these properties across a diverse range of metrics and models. Finally, we propose recommendations for improving image-text alignment evaluation.

</details>


### [80] [Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models](https://arxiv.org/abs/2506.08480)
*Huixuan Zhang,Xiaojun Wan*

Main category: cs.CL

TL;DR: 本文指出现有文本-图像对齐评估框架的不足，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要依赖人类判断，忽略了评估框架的其他关键特性。

Method: 识别可靠评估的两个关键方面，并通过实验验证当前框架的不足。

Result: 当前主流评估框架未能满足这些关键特性。

Conclusion: 提出了改进文本-图像对齐评估的建议。

Abstract: Text-to-image models often struggle to generate images that precisely match textual prompts. Prior research has extensively studied the evaluation of image-text alignment in text-to-image generation. However, existing evaluations primarily focus on agreement with human assessments, neglecting other critical properties of a trustworthy evaluation framework. In this work, we first identify two key aspects that a reliable evaluation should address. We then empirically demonstrate that current mainstream evaluation frameworks fail to fully satisfy these properties across a diverse range of metrics and models. Finally, we propose recommendations for improving image-text alignment evaluation.

</details>


### [81] [Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models](https://arxiv.org/abs/2506.08480)
*Huixuan Zhang,Xiaojun Wan*

Main category: cs.CL

TL;DR: 本文指出现有文本到图像生成评估框架的不足，提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要依赖人类判断，忽略了评估框架的其他关键特性。

Method: 识别可靠评估的两个关键方面，实证分析现有框架的不足。

Result: 发现主流评估框架未能满足这些特性。

Conclusion: 提出改进图像-文本对齐评估的建议。

Abstract: Text-to-image models often struggle to generate images that precisely match textual prompts. Prior research has extensively studied the evaluation of image-text alignment in text-to-image generation. However, existing evaluations primarily focus on agreement with human assessments, neglecting other critical properties of a trustworthy evaluation framework. In this work, we first identify two key aspects that a reliable evaluation should address. We then empirically demonstrate that current mainstream evaluation frameworks fail to fully satisfy these properties across a diverse range of metrics and models. Finally, we propose recommendations for improving image-text alignment evaluation.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [82] [SakugaFlow: A Stagewise Illustration Framework Emulating the Human Drawing Process and Providing Interactive Tutoring for Novice Drawing Skills](https://arxiv.org/abs/2506.08443)
*Kazuki Kawamura,Jun Rekimoto*

Main category: cs.HC

TL;DR: SakugaFlow是一个四阶段流程，结合扩散模型和大型语言模型，为初学者提供实时反馈，支持非线性修改和多版本探索，将黑盒生成器转化为学习工具。


<details>
  <summary>Details</summary>
Motivation: 当前AI绘图工具虽能生成高质量图像，但缺乏人类艺术家逐步创作的过程，无法提供学习支持。

Method: 采用四阶段流程，结合扩散模型和语言模型，提供实时反馈、非线性修改和多版本分支功能。

Result: SakugaFlow通过展示中间输出和嵌入教学对话，支持创意探索和技能学习。

Conclusion: SakugaFlow成功将黑盒生成器转化为支持学习和创作的工具。

Abstract: While current AI illustration tools can generate high-quality images from text prompts, they rarely reveal the step-by-step procedure that human artists follow. We present SakugaFlow, a four-stage pipeline that pairs diffusion-based image generation with a large-language-model tutor. At each stage, novices receive real-time feedback on anatomy, perspective, and composition, revise any step non-linearly, and branch alternative versions. By exposing intermediate outputs and embedding pedagogical dialogue, SakugaFlow turns a black-box generator into a scaffolded learning environment that supports both creative exploration and skills acquisition.

</details>


### [83] [SakugaFlow: A Stagewise Illustration Framework Emulating the Human Drawing Process and Providing Interactive Tutoring for Novice Drawing Skills](https://arxiv.org/abs/2506.08443)
*Kazuki Kawamura,Jun Rekimoto*

Main category: cs.HC

TL;DR: SakugaFlow是一个四阶段流程，结合扩散模型和大型语言模型，为初学者提供实时反馈，支持非线性修改和学习。


<details>
  <summary>Details</summary>
Motivation: 当前AI绘图工具虽能生成高质量图像，但缺乏人类艺术家逐步创作的过程，SakugaFlow旨在填补这一空白。

Method: 采用扩散模型生成图像，结合大型语言模型提供实时反馈，支持非线性修改和多版本分支。

Result: 通过展示中间输出和嵌入教学对话，SakugaFlow将黑盒生成器转变为支持创意探索和技能学习的环境。

Conclusion: SakugaFlow不仅提升了图像生成的质量，还为初学者提供了学习艺术创作的有效工具。

Abstract: While current AI illustration tools can generate high-quality images from text prompts, they rarely reveal the step-by-step procedure that human artists follow. We present SakugaFlow, a four-stage pipeline that pairs diffusion-based image generation with a large-language-model tutor. At each stage, novices receive real-time feedback on anatomy, perspective, and composition, revise any step non-linearly, and branch alternative versions. By exposing intermediate outputs and embedding pedagogical dialogue, SakugaFlow turns a black-box generator into a scaffolded learning environment that supports both creative exploration and skills acquisition.

</details>


### [84] [SakugaFlow: A Stagewise Illustration Framework Emulating the Human Drawing Process and Providing Interactive Tutoring for Novice Drawing Skills](https://arxiv.org/abs/2506.08443)
*Kazuki Kawamura,Jun Rekimoto*

Main category: cs.HC

TL;DR: SakugaFlow是一个四阶段流程，结合扩散模型和大型语言模型，为初学者提供实时反馈，支持非线性修改和学习。


<details>
  <summary>Details</summary>
Motivation: 当前AI绘图工具缺乏人类艺术家逐步创作过程的透明度，SakugaFlow旨在通过揭示中间步骤和嵌入教学对话，将其转化为学习工具。

Method: 采用四阶段流程，结合扩散模型生成图像和语言模型提供实时反馈，支持非线性修改和版本分支。

Result: 将黑盒生成器转变为支持创意探索和技能学习的教学环境。

Conclusion: SakugaFlow通过透明化生成过程和嵌入教学功能，提升了AI绘图工具的教育价值。

Abstract: While current AI illustration tools can generate high-quality images from text prompts, they rarely reveal the step-by-step procedure that human artists follow. We present SakugaFlow, a four-stage pipeline that pairs diffusion-based image generation with a large-language-model tutor. At each stage, novices receive real-time feedback on anatomy, perspective, and composition, revise any step non-linearly, and branch alternative versions. By exposing intermediate outputs and embedding pedagogical dialogue, SakugaFlow turns a black-box generator into a scaffolded learning environment that supports both creative exploration and skills acquisition.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [85] [Plug-and-Play Linear Attention for Pre-trained Image and Video Restoration Models](https://arxiv.org/abs/2506.08520)
*Srinivasan Kidambi,Pravin Nair*

Main category: eess.IV

TL;DR: PnP-Nystra是一种基于Nyström的线性自注意力近似方法，作为即插即用模块，可在不重新训练的情况下加速预训练的图像和视频修复模型。


<details>
  <summary>Details</summary>
Motivation: 多头自注意力（MHSA）的二次复杂度在实时和资源受限环境中成为计算瓶颈，需要高效替代方案。

Method: 提出PnP-Nystra，作为MHSA的线性近似替代模块，适用于SwinIR、Uformer等窗口式Transformer架构。

Result: 在图像和视频修复任务中，PnP-Nystra在GPU和CPU上分别实现2-4倍和2-5倍加速，PSNR最大仅下降1.5 dB。

Conclusion: PnP-Nystra首次展示了线性注意力可作为无需训练的MHSA替代方案，显著提升效率且性能损失极小。

Abstract: Multi-head self-attention (MHSA) has become a core component in modern computer vision models. However, its quadratic complexity with respect to input length poses a significant computational bottleneck in real-time and resource constrained environments. We propose PnP-Nystra, a Nystr\"om based linear approximation of self-attention, developed as a plug-and-play (PnP) module that can be integrated into the pre-trained image and video restoration models without retraining. As a drop-in replacement for MHSA, PnP-Nystra enables efficient acceleration in various window-based transformer architectures, including SwinIR, Uformer, and RVRT. Our experiments across diverse image and video restoration tasks, including denoising, deblurring, and super-resolution, demonstrate that PnP-Nystra achieves a 2-4x speed-up on an NVIDIA RTX 4090 GPU and a 2-5x speed-up on CPU inference. Despite these significant gains, the method incurs a maximum PSNR drop of only 1.5 dB across all evaluated tasks. To the best of our knowledge, we are the first to demonstrate a linear attention functioning as a training-free substitute for MHSA in restoration models.

</details>


### [86] [MAMBO: High-Resolution Generative Approach for Mammography Images](https://arxiv.org/abs/2506.08677)
*Milica Škipina,Nikola Jovišić,Nicola Dall'Asen,Vanja Švenda,Anil Osman Tur,Slobodan Ilić,Elisa Ricci,Dubravko Ćulibrk*

Main category: eess.IV

TL;DR: 论文提出了一种名为MAMBO的基于扩散模型的方法，用于生成高分辨率乳腺X光片，以解决训练AI系统时数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 由于隐私和伦理限制，获取大规模多样化的乳腺X光数据集困难，影响了AI系统的训练效果。

Method: MAMBO采用基于块的扩散模型，结合局部和全局上下文信息，生成高达3840x3840像素的高分辨率乳腺X光片。

Result: MAMBO能够生成高度逼真的乳腺X光片，并通过实验验证了其在图像生成、超分辨率和异常检测中的潜力。

Conclusion: MAMBO有望提升乳腺X光分析的准确性，帮助更早发现病变。

Abstract: Mammography is the gold standard for the detection and diagnosis of breast cancer. This procedure can be significantly enhanced with Artificial Intelligence (AI)-based software, which assists radiologists in identifying abnormalities. However, training AI systems requires large and diverse datasets, which are often difficult to obtain due to privacy and ethical constraints. To address this issue, the paper introduces MAMmography ensemBle mOdel (MAMBO), a novel patch-based diffusion approach designed to generate full-resolution mammograms. Diffusion models have shown breakthrough results in realistic image generation, yet few studies have focused on mammograms, and none have successfully generated high-resolution outputs required to capture fine-grained features of small lesions. To achieve this, MAMBO integrates separate diffusion models to capture both local and global (image-level) contexts. The contextual information is then fed into the final patch-based model, significantly aiding the noise removal process. This thoughtful design enables MAMBO to generate highly realistic mammograms of up to 3840x3840 pixels. Importantly, this approach can be used to enhance the training of classification models and extended to anomaly detection. Experiments, both numerical and radiologist validation, assess MAMBO's capabilities in image generation, super-resolution, and anomaly detection, highlighting its potential to enhance mammography analysis for more accurate diagnoses and earlier lesion detection.

</details>


### [87] [Enhancing Synthetic CT from CBCT via Multimodal Fusion: A Study on the Impact of CBCT Quality and Alignment](https://arxiv.org/abs/2506.08716)
*Maximilian Tschuchnig,Lukas Lamminger,Philipp Steininger,Michael Gadermayr*

Main category: eess.IV

TL;DR: 本文通过多模态学习提升合成CT（sCT）生成质量，结合术中CBCT和术前CT数据，验证其在真实数据集上的效果。


<details>
  <summary>Details</summary>
Motivation: CBCT虽广泛用于术中成像，但存在伪影和视觉质量低的问题，需通过sCT生成改善。

Method: 采用多模态学习方法，整合CBCT和CT数据，并通过合成数据集分析对齐和质量对sCT的影响。

Result: 多模态sCT表现优于单模态基线，尤其在低质量CBCT-CT对齐情况下提升显著。

Conclusion: 研究结果在真实临床数据中具有高度可重复性，验证了方法的有效性。

Abstract: Cone-Beam Computed Tomography (CBCT) is widely used for real-time intraoperative imaging due to its low radiation dose and high acquisition speed. However, despite its high resolution, CBCT suffers from significant artifacts and thereby lower visual quality, compared to conventional Computed Tomography (CT). A recent approach to mitigate these artifacts is synthetic CT (sCT) generation, translating CBCT volumes into the CT domain. In this work, we enhance sCT generation through multimodal learning, integrating intraoperative CBCT with preoperative CT. Beyond validation on two real-world datasets, we use a versatile synthetic dataset, to analyze how CBCT-CT alignment and CBCT quality affect sCT quality. The results demonstrate that multimodal sCT consistently outperform unimodal baselines, with the most significant gains observed in well-aligned, low-quality CBCT-CT cases. Finally, we demonstrate that these findings are highly reproducible in real-world clinical datasets.

</details>


### [88] [Plug-and-Play Linear Attention for Pre-trained Image and Video Restoration Models](https://arxiv.org/abs/2506.08520)
*Srinivasan Kidambi,Pravin Nair*

Main category: eess.IV

TL;DR: PnP-Nystra是一种基于Nyström的线性自注意力近似方法，作为即插即用模块，可在不重新训练的情况下加速预训练模型，显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 多头自注意力（MHSA）在计算机视觉模型中广泛应用，但其二次复杂度在实时和资源受限环境中成为计算瓶颈。

Method: 提出PnP-Nystra，一种基于Nyström的线性自注意力近似方法，作为即插即用模块，可直接替换MHSA。

Result: 实验表明，PnP-Nystra在图像和视频修复任务中实现2-4倍GPU加速和2-5倍CPU加速，PSNR最大仅下降1.5 dB。

Conclusion: PnP-Nystra是首个无需训练的线性注意力替代方案，显著提升计算效率且性能损失极小。

Abstract: Multi-head self-attention (MHSA) has become a core component in modern computer vision models. However, its quadratic complexity with respect to input length poses a significant computational bottleneck in real-time and resource constrained environments. We propose PnP-Nystra, a Nystr\"om based linear approximation of self-attention, developed as a plug-and-play (PnP) module that can be integrated into the pre-trained image and video restoration models without retraining. As a drop-in replacement for MHSA, PnP-Nystra enables efficient acceleration in various window-based transformer architectures, including SwinIR, Uformer, and RVRT. Our experiments across diverse image and video restoration tasks, including denoising, deblurring, and super-resolution, demonstrate that PnP-Nystra achieves a 2-4x speed-up on an NVIDIA RTX 4090 GPU and a 2-5x speed-up on CPU inference. Despite these significant gains, the method incurs a maximum PSNR drop of only 1.5 dB across all evaluated tasks. To the best of our knowledge, we are the first to demonstrate a linear attention functioning as a training-free substitute for MHSA in restoration models.

</details>


### [89] [MAMBO: High-Resolution Generative Approach for Mammography Images](https://arxiv.org/abs/2506.08677)
*Milica Škipina,Nikola Jovišić,Nicola Dall'Asen,Vanja Švenda,Anil Osman Tur,Slobodan Ilić,Elisa Ricci,Dubravko Ćulibrk*

Main category: eess.IV

TL;DR: 论文提出了一种名为MAMBO的基于扩散模型的乳腺X光片生成方法，能够生成高分辨率图像以辅助AI训练和诊断。


<details>
  <summary>Details</summary>
Motivation: 解决乳腺X光片数据稀缺问题，同时满足隐私和伦理要求，提升AI辅助诊断的准确性。

Method: 采用基于补丁的扩散模型，结合局部和全局上下文信息，生成3840x3840像素的高分辨率乳腺X光片。

Result: MAMBO能够生成高度真实的乳腺X光片，并在图像生成、超分辨率和异常检测任务中表现优异。

Conclusion: MAMBO为乳腺X光片分析提供了增强训练数据的潜力，有助于更准确的诊断和早期病变检测。

Abstract: Mammography is the gold standard for the detection and diagnosis of breast cancer. This procedure can be significantly enhanced with Artificial Intelligence (AI)-based software, which assists radiologists in identifying abnormalities. However, training AI systems requires large and diverse datasets, which are often difficult to obtain due to privacy and ethical constraints. To address this issue, the paper introduces MAMmography ensemBle mOdel (MAMBO), a novel patch-based diffusion approach designed to generate full-resolution mammograms. Diffusion models have shown breakthrough results in realistic image generation, yet few studies have focused on mammograms, and none have successfully generated high-resolution outputs required to capture fine-grained features of small lesions. To achieve this, MAMBO integrates separate diffusion models to capture both local and global (image-level) contexts. The contextual information is then fed into the final patch-based model, significantly aiding the noise removal process. This thoughtful design enables MAMBO to generate highly realistic mammograms of up to 3840x3840 pixels. Importantly, this approach can be used to enhance the training of classification models and extended to anomaly detection. Experiments, both numerical and radiologist validation, assess MAMBO's capabilities in image generation, super-resolution, and anomaly detection, highlighting its potential to enhance mammography analysis for more accurate diagnoses and earlier lesion detection.

</details>


### [90] [Enhancing Synthetic CT from CBCT via Multimodal Fusion: A Study on the Impact of CBCT Quality and Alignment](https://arxiv.org/abs/2506.08716)
*Maximilian Tschuchnig,Lukas Lamminger,Philipp Steininger,Michael Gadermayr*

Main category: eess.IV

TL;DR: 通过多模态学习提升CBCT到CT的合成质量，结果显示多模态方法优于单模态基线，尤其在低质量CBCT-CT对齐情况下效果显著。


<details>
  <summary>Details</summary>
Motivation: CBCT虽快速低辐射，但存在伪影和视觉质量低的问题，需通过合成CT（sCT）改善。

Method: 整合术中CBCT与术前CT进行多模态学习，并在真实和合成数据集上验证。

Result: 多模态sCT表现优于单模态基线，低质量CBCT-CT对齐情况下提升最明显。

Conclusion: 多模态方法在真实临床数据中具有高度可重复性。

Abstract: Cone-Beam Computed Tomography (CBCT) is widely used for real-time intraoperative imaging due to its low radiation dose and high acquisition speed. However, despite its high resolution, CBCT suffers from significant artifacts and thereby lower visual quality, compared to conventional Computed Tomography (CT). A recent approach to mitigate these artifacts is synthetic CT (sCT) generation, translating CBCT volumes into the CT domain. In this work, we enhance sCT generation through multimodal learning, integrating intraoperative CBCT with preoperative CT. Beyond validation on two real-world datasets, we use a versatile synthetic dataset, to analyze how CBCT-CT alignment and CBCT quality affect sCT quality. The results demonstrate that multimodal sCT consistently outperform unimodal baselines, with the most significant gains observed in well-aligned, low-quality CBCT-CT cases. Finally, we demonstrate that these findings are highly reproducible in real-world clinical datasets.

</details>


### [91] [Plug-and-Play Linear Attention for Pre-trained Image and Video Restoration Models](https://arxiv.org/abs/2506.08520)
*Srinivasan Kidambi,Pravin Nair*

Main category: eess.IV

TL;DR: PnP-Nystra是一种基于Nyström的线性自注意力近似方法，作为即插即用模块，无需重新训练即可加速预训练的图像和视频修复模型。


<details>
  <summary>Details</summary>
Motivation: 多头自注意力（MHSA）的二次复杂度在实时和资源受限环境中成为计算瓶颈，需要一种高效的替代方案。

Method: 提出PnP-Nystra，一种Nyström线性近似方法，作为MHSA的即插即用替代模块，适用于多种窗口式Transformer架构。

Result: 实验表明，PnP-Nystra在GPU和CPU上分别实现2-4倍和2-5倍的加速，PSNR下降最多仅1.5 dB。

Conclusion: PnP-Nystra是首个无需训练的线性注意力替代方案，显著提升了计算效率且性能损失极小。

Abstract: Multi-head self-attention (MHSA) has become a core component in modern computer vision models. However, its quadratic complexity with respect to input length poses a significant computational bottleneck in real-time and resource constrained environments. We propose PnP-Nystra, a Nystr\"om based linear approximation of self-attention, developed as a plug-and-play (PnP) module that can be integrated into the pre-trained image and video restoration models without retraining. As a drop-in replacement for MHSA, PnP-Nystra enables efficient acceleration in various window-based transformer architectures, including SwinIR, Uformer, and RVRT. Our experiments across diverse image and video restoration tasks, including denoising, deblurring, and super-resolution, demonstrate that PnP-Nystra achieves a 2-4x speed-up on an NVIDIA RTX 4090 GPU and a 2-5x speed-up on CPU inference. Despite these significant gains, the method incurs a maximum PSNR drop of only 1.5 dB across all evaluated tasks. To the best of our knowledge, we are the first to demonstrate a linear attention functioning as a training-free substitute for MHSA in restoration models.

</details>


### [92] [MAMBO: High-Resolution Generative Approach for Mammography Images](https://arxiv.org/abs/2506.08677)
*Milica Škipina,Nikola Jovišić,Nicola Dall'Asen,Vanja Švenda,Anil Osman Tur,Slobodan Ilić,Elisa Ricci,Dubravko Ćulibrk*

Main category: eess.IV

TL;DR: 论文提出了一种名为MAMBO的基于扩散模型的乳腺X光片生成方法，能够生成高分辨率图像以辅助乳腺癌诊断。


<details>
  <summary>Details</summary>
Motivation: 由于隐私和伦理限制，获取大规模多样化的乳腺X光片数据集困难，影响了AI系统的训练效果。

Method: MAMBO采用基于块的扩散模型，结合局部和全局上下文信息，生成高达3840x3840像素的高分辨率乳腺X光片。

Result: 实验表明，MAMBO能生成高度真实的乳腺X光片，并可用于分类模型训练和异常检测。

Conclusion: MAMBO有望提升乳腺X光片分析的准确性，助力早期病变检测。

Abstract: Mammography is the gold standard for the detection and diagnosis of breast cancer. This procedure can be significantly enhanced with Artificial Intelligence (AI)-based software, which assists radiologists in identifying abnormalities. However, training AI systems requires large and diverse datasets, which are often difficult to obtain due to privacy and ethical constraints. To address this issue, the paper introduces MAMmography ensemBle mOdel (MAMBO), a novel patch-based diffusion approach designed to generate full-resolution mammograms. Diffusion models have shown breakthrough results in realistic image generation, yet few studies have focused on mammograms, and none have successfully generated high-resolution outputs required to capture fine-grained features of small lesions. To achieve this, MAMBO integrates separate diffusion models to capture both local and global (image-level) contexts. The contextual information is then fed into the final patch-based model, significantly aiding the noise removal process. This thoughtful design enables MAMBO to generate highly realistic mammograms of up to 3840x3840 pixels. Importantly, this approach can be used to enhance the training of classification models and extended to anomaly detection. Experiments, both numerical and radiologist validation, assess MAMBO's capabilities in image generation, super-resolution, and anomaly detection, highlighting its potential to enhance mammography analysis for more accurate diagnoses and earlier lesion detection.

</details>


### [93] [Enhancing Synthetic CT from CBCT via Multimodal Fusion: A Study on the Impact of CBCT Quality and Alignment](https://arxiv.org/abs/2506.08716)
*Maximilian Tschuchnig,Lukas Lamminger,Philipp Steininger,Michael Gadermayr*

Main category: eess.IV

TL;DR: 通过多模态学习提升CBCT到CT的合成质量，显著改善对齐不良或低质量CBCT-CT的情况。


<details>
  <summary>Details</summary>
Motivation: CBCT虽然快速低辐射，但存在伪影和视觉质量低的问题，需通过合成CT（sCT）改善。

Method: 结合术中CBCT与术前CT进行多模态学习，生成高质量sCT。

Result: 多模态sCT表现优于单模态基线，尤其在对齐良好但质量低的CBCT-CT案例中效果显著。

Conclusion: 多模态学习方法在真实临床数据中具有高度可重复性，有效提升sCT质量。

Abstract: Cone-Beam Computed Tomography (CBCT) is widely used for real-time intraoperative imaging due to its low radiation dose and high acquisition speed. However, despite its high resolution, CBCT suffers from significant artifacts and thereby lower visual quality, compared to conventional Computed Tomography (CT). A recent approach to mitigate these artifacts is synthetic CT (sCT) generation, translating CBCT volumes into the CT domain. In this work, we enhance sCT generation through multimodal learning, integrating intraoperative CBCT with preoperative CT. Beyond validation on two real-world datasets, we use a versatile synthetic dataset, to analyze how CBCT-CT alignment and CBCT quality affect sCT quality. The results demonstrate that multimodal sCT consistently outperform unimodal baselines, with the most significant gains observed in well-aligned, low-quality CBCT-CT cases. Finally, we demonstrate that these findings are highly reproducible in real-world clinical datasets.

</details>
