{"id": "2506.08237", "pdf": "https://arxiv.org/pdf/2506.08237", "abs": "https://arxiv.org/abs/2506.08237", "authors": ["Bailey Miller", "Rohan Sawhney", "Keenan Crane", "Ioannis Gkioulekas"], "title": "Solving partial differential equations in participating media", "categories": ["cs.GR"], "comment": "SIGGRAPH 2025. Project page   https://imaging.cs.cmu.edu/volumetric_walk_on_spheres", "summary": "We consider the problem of solving partial differential equations (PDEs) in domains with complex microparticle geometry that is impractical, or intractable, to model explicitly. Drawing inspiration from volume rendering, we propose tackling this problem by treating the domain as a participating medium that models microparticle geometry stochastically, through aggregate statistical properties (e.g., particle density). We first introduce the problem setting of PDE simulation in participating media. We then specialize to exponential media and describe the properties that make them an attractive model of microparticle geometry for PDE simulation problems. We use these properties to develop two new algorithms, volumetric walk on spheres and volumetric walk on stars, that generalize previous Monte Carlo algorithms to enable efficient and discretization-free simulation of linear elliptic PDEs (e.g., Laplace) in participating media. We demonstrate experimentally that our algorithms can solve Laplace boundary value problems with complex microparticle geometry more accurately and more efficiently than previous approaches, such as ensemble averaging and homogenization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53c2\u4e0e\u4ecb\u8d28\u7684\u968f\u673a\u5efa\u6a21\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u590d\u6742\u5fae\u7c92\u51e0\u4f55\u4e2d\u7684\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u4e24\u79cd\u65b0\u7b97\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u5fae\u7c92\u51e0\u4f55\u4e2dPDE\u6a21\u62df\u7684\u96be\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6216\u65e0\u6cd5\u663e\u5f0f\u5efa\u6a21\u3002", "method": "\u5c06\u57df\u89c6\u4e3a\u53c2\u4e0e\u4ecb\u8d28\uff0c\u901a\u8fc7\u7edf\u8ba1\u7279\u6027\u5efa\u6a21\u5fae\u7c92\u51e0\u4f55\uff0c\u5f00\u53d1\u4e86\u4e24\u79cd\u65b0\u7b97\u6cd5\uff1avolumetric walk on spheres\u548cvolumetric walk on stars\u3002", "result": "\u65b0\u7b97\u6cd5\u5728\u62c9\u666e\u62c9\u65af\u8fb9\u754c\u503c\u95ee\u9898\u4e2d\u6bd4\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u96c6\u5408\u5e73\u5747\u548c\u5747\u5300\u5316\uff09\u66f4\u51c6\u786e\u9ad8\u6548\u3002", "conclusion": "\u63d0\u51fa\u7684\u968f\u673a\u5efa\u6a21\u65b9\u6cd5\u548c\u7b97\u6cd5\u4e3a\u590d\u6742\u5fae\u7c92\u51e0\u4f55\u4e2d\u7684PDE\u6a21\u62df\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u65e0\u79bb\u6563\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08350", "pdf": "https://arxiv.org/pdf/2506.08350", "abs": "https://arxiv.org/abs/2506.08350", "authors": ["Yicheng Zhan", "Dong-Ha Shin", "Seung-Hwan Baek", "Kaan Ak\u015fit"], "title": "Complex-Valued Holographic Radiance Fields", "categories": ["cs.GR", "cs.CV", "cs.ET"], "comment": "28 pages, 21 figures", "summary": "Modeling the full properties of light, including both amplitude and phase, in 3D representations is crucial for advancing physically plausible rendering, particularly in holographic displays. To support these features, we propose a novel representation that optimizes 3D scenes without relying on intensity-based intermediaries. We reformulate 3D Gaussian splatting with complex-valued Gaussian primitives, expanding support for rendering with light waves. By leveraging RGBD multi-view images, our method directly optimizes complex-valued Gaussians as a 3D holographic scene representation. This eliminates the need for computationally expensive hologram re-optimization. Compared with state-of-the-art methods, our method achieves 30x-10,000x speed improvements while maintaining on-par image quality, representing a first step towards geometrically aligned, physically plausible holographic scene representations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u590d\u503c\u9ad8\u65af\u57fa\u5143\u76843D\u5168\u606f\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6e32\u67d3\u901f\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u652f\u6301\u5168\u606f\u663e\u793a\u4e2d\u7684\u7269\u7406\u771f\u5b9e\u6e32\u67d3\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u5efa\u6a21\u5149\u7684\u632f\u5e45\u548c\u76f8\u4f4d\u76843D\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7RGBD\u591a\u89c6\u56fe\u56fe\u50cf\uff0c\u76f4\u63a5\u4f18\u5316\u590d\u503c\u9ad8\u65af\u57fa\u5143\u4f5c\u4e3a3D\u5168\u606f\u573a\u666f\u8868\u793a\uff0c\u907f\u514d\u4e86\u8ba1\u7b97\u6602\u8d35\u7684\u5149\u6805\u91cd\u65b0\u4f18\u5316\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u901f\u5ea6\u63d0\u5347\u4e8630\u500d\u81f310,000\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u51e0\u4f55\u5bf9\u9f50\u3001\u7269\u7406\u771f\u5b9e\u76843D\u5168\u606f\u573a\u666f\u8868\u793a\u8fc8\u51fa\u4e86\u7b2c\u4e00\u6b65\u3002"}}
{"id": "2506.08071", "pdf": "https://arxiv.org/pdf/2506.08071", "abs": "https://arxiv.org/abs/2506.08071", "authors": ["Aniket Rege", "Zinnia Nie", "Mahesh Ramesh", "Unmesh Raskar", "Zhuoran Yu", "Aditya Kusupati", "Yong Jae Lee", "Ramya Korlakai Vinayak"], "title": "CuRe: Cultural Gaps in the Long Tail of Text-to-Image Systems", "categories": ["cs.CV"], "comment": "41 pages, 22 figures, 17 tables", "summary": "Popular text-to-image (T2I) systems are trained on web-scraped data, which is heavily Amero and Euro-centric, underrepresenting the cultures of the Global South. To analyze these biases, we introduce CuRe, a novel and scalable benchmarking and scoring suite for cultural representativeness that leverages the marginal utility of attribute specification to T2I systems as a proxy for human judgments. Our CuRe benchmark dataset has a novel categorical hierarchy built from the crowdsourced Wikimedia knowledge graph, with 300 cultural artifacts across 32 cultural subcategories grouped into six broad cultural axes (food, art, fashion, architecture, celebrations, and people). Our dataset's categorical hierarchy enables CuRe scorers to evaluate T2I systems by analyzing their response to increasing the informativeness of text conditioning, enabling fine-grained cultural comparisons. We empirically observe much stronger correlations of our class of scorers to human judgments of perceptual similarity, image-text alignment, and cultural diversity across image encoders (SigLIP 2, AIMV2 and DINOv2), vision-language models (OpenCLIP, SigLIP 2, Gemini 2.0 Flash) and state-of-the-art text-to-image systems, including three variants of Stable Diffusion (1.5, XL, 3.5 Large), FLUX.1 [dev], Ideogram 2.0, and DALL-E 3. The code and dataset is open-sourced and available at https://aniketrege.github.io/cure/.", "AI": {"tldr": "CuRe\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u7cfb\u7edf\u6587\u5316\u4ee3\u8868\u6027\u7684\u65b0\u57fa\u51c6\u548c\u8bc4\u5206\u5957\u4ef6\uff0c\u901a\u8fc7\u5206\u6790\u7cfb\u7edf\u5bf9\u6587\u672c\u6761\u4ef6\u589e\u52a0\u7684\u54cd\u5e94\u6765\u8861\u91cf\u6587\u5316\u591a\u6837\u6027\u3002", "motivation": "\u5f53\u524dT2I\u7cfb\u7edf\u8bad\u7ec3\u6570\u636e\u504f\u5411\u6b27\u7f8e\u6587\u5316\uff0c\u7f3a\u4e4f\u5bf9\u5168\u7403\u5357\u65b9\u6587\u5316\u7684\u4ee3\u8868\u6027\uff0cCuRe\u65e8\u5728\u91cf\u5316\u5e76\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528Wikimedia\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u5305\u542b300\u79cd\u6587\u5316\u7269\u54c1\u7684\u5c42\u6b21\u5316\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5206\u6790T2I\u7cfb\u7edf\u5bf9\u6587\u672c\u6761\u4ef6\u589e\u52a0\u7684\u54cd\u5e94\u6765\u8bc4\u4f30\u6587\u5316\u4ee3\u8868\u6027\u3002", "result": "CuRe\u8bc4\u5206\u4e0e\u4eba\u7c7b\u5bf9\u611f\u77e5\u76f8\u4f3c\u6027\u3001\u56fe\u6587\u5bf9\u9f50\u548c\u6587\u5316\u591a\u6837\u6027\u7684\u5224\u65ad\u9ad8\u5ea6\u76f8\u5173\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u56fe\u50cf\u7f16\u7801\u5668\u548cT2I\u7cfb\u7edf\u3002", "conclusion": "CuRe\u4e3aT2I\u7cfb\u7edf\u7684\u6587\u5316\u591a\u6837\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u7ec6\u7c92\u5ea6\u7684\u5de5\u5177\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.08185", "pdf": "https://arxiv.org/pdf/2506.08185", "abs": "https://arxiv.org/abs/2506.08185", "authors": ["Huixin Zhan", "Jason H. Moore"], "title": "Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion Models in a Vision-Language-Action Framework", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Surgeons exhibit distinct operating styles due to differences in training, experience, and motor behavior - yet current AI systems often ignore this personalization signal. We propose a novel approach to model fine-grained, surgeon-specific fingerprinting in robotic surgery using a discrete diffusion framework integrated with a vision-language-action (VLA) pipeline. Our method formulates gesture prediction as a structured sequence denoising task, conditioned on multimodal inputs including endoscopic video, surgical intent language, and a privacy-aware embedding of surgeon identity and skill. Personalized surgeon fingerprinting is encoded through natural language prompts using third-party language models, allowing the model to retain individual behavioral style without exposing explicit identity. We evaluate our method on the JIGSAWS dataset and demonstrate that it accurately reconstructs gesture sequences while learning meaningful motion fingerprints unique to each surgeon. To quantify the privacy implications of personalization, we perform membership inference attacks and find that more expressive embeddings improve task performance but simultaneously increase susceptibility to identity leakage. These findings demonstrate that while personalized embeddings improve performance, they also increase vulnerability to identity leakage, revealing the importance of balancing personalization with privacy risk in surgical modeling. Code is available at: https://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u79bb\u6563\u6269\u6563\u6846\u67b6\u548c\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6d41\u7a0b\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u5916\u79d1\u533b\u751f\u5728\u673a\u5668\u4eba\u624b\u672f\u4e2d\u7684\u4e2a\u6027\u5316\u98ce\u683c\u6307\u7eb9\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u9690\u79c1\u98ce\u9669\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u5e38\u5ffd\u7565\u5916\u79d1\u533b\u751f\u7684\u4e2a\u6027\u5316\u64cd\u4f5c\u98ce\u683c\uff0c\u800c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u591a\u6a21\u6001\u8f93\u5165\uff08\u5982\u5185\u7aa5\u955c\u89c6\u9891\u3001\u624b\u672f\u610f\u56fe\u8bed\u8a00\u548c\u9690\u79c1\u611f\u77e5\u7684\u8eab\u4efd\u5d4c\u5165\uff09\u6765\u6355\u6349\u8fd9\u4e9b\u5dee\u5f02\u3002", "method": "\u91c7\u7528\u79bb\u6563\u6269\u6563\u6846\u67b6\uff0c\u5c06\u624b\u52bf\u9884\u6d4b\u5efa\u6a21\u4e3a\u7ed3\u6784\u5316\u5e8f\u5217\u53bb\u566a\u4efb\u52a1\uff0c\u7ed3\u5408\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u52a8\u4f5c\u8f93\u5165\uff0c\u5e76\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u7f16\u7801\u4e2a\u6027\u5316\u98ce\u683c\u3002", "result": "\u5728JIGSAWS\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u51c6\u786e\u91cd\u5efa\u624b\u52bf\u5e8f\u5217\u5e76\u5b66\u4e60\u5230\u6bcf\u4f4d\u5916\u79d1\u533b\u751f\u7684\u72ec\u7279\u8fd0\u52a8\u6307\u7eb9\uff0c\u4f46\u4e5f\u53d1\u73b0\u66f4\u4e2a\u6027\u5316\u7684\u5d4c\u5165\u4f1a\u589e\u52a0\u8eab\u4efd\u6cc4\u9732\u98ce\u9669\u3002", "conclusion": "\u4e2a\u6027\u5316\u5d4c\u5165\u867d\u80fd\u63d0\u5347\u4efb\u52a1\u6027\u80fd\uff0c\u4f46\u4e5f\u589e\u52a0\u4e86\u9690\u79c1\u98ce\u9669\uff0c\u9700\u5728\u624b\u672f\u5efa\u6a21\u4e2d\u5e73\u8861\u4e2a\u6027\u5316\u4e0e\u9690\u79c1\u4fdd\u62a4\u3002"}}
{"id": "2506.08210", "pdf": "https://arxiv.org/pdf/2506.08210", "abs": "https://arxiv.org/abs/2506.08210", "authors": ["Andrew Z. Wang", "Songwei Ge", "Tero Karras", "Ming-Yu Liu", "Yogesh Balaji"], "title": "A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "CVPR 2025", "summary": "Both text-to-image generation and large language models (LLMs) have made significant advancements. However, many text-to-image models still employ the somewhat outdated T5 and CLIP as their text encoders. In this work, we investigate the effectiveness of using modern decoder-only LLMs as text encoders for text-to-image diffusion models. We build a standardized training and evaluation pipeline that allows us to isolate and evaluate the effect of different text embeddings. We train a total of 27 text-to-image models with 12 different text encoders to analyze the critical aspects of LLMs that could impact text-to-image generation, including the approaches to extract embeddings, different LLMs variants, and model sizes. Our experiments reveal that the de facto way of using last-layer embeddings as conditioning leads to inferior performance. Instead, we explore embeddings from various layers and find that using layer-normalized averaging across all layers significantly improves alignment with complex prompts. Most LLMs with this conditioning outperform the baseline T5 model, showing enhanced performance in advanced visio-linguistic reasoning skills.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u73b0\u4ee3\u4ec5\u89e3\u7801\u5668LLM\u4f5c\u4e3a\u6587\u672c\u7f16\u7801\u5668\u5728\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u901a\u8fc7\u5c42\u5f52\u4e00\u5316\u5e73\u5747\u5d4c\u5165\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4ecd\u4f7f\u7528\u8fc7\u65f6\u7684T5\u548cCLIP\u4f5c\u4e3a\u6587\u672c\u7f16\u7801\u5668\uff0c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u73b0\u4ee3LLM\u7684\u6f5c\u529b\u3002", "method": "\u6784\u5efa\u6807\u51c6\u5316\u8bad\u7ec3\u548c\u8bc4\u4f30\u6d41\u7a0b\uff0c\u8bad\u7ec327\u4e2a\u6a21\u578b\uff0c\u6bd4\u8f8312\u79cd\u6587\u672c\u7f16\u7801\u5668\uff0c\u5206\u6790\u5d4c\u5165\u63d0\u53d6\u65b9\u6cd5\u3001LLM\u53d8\u4f53\u548c\u6a21\u578b\u5927\u5c0f\u7684\u5f71\u54cd\u3002", "result": "\u4f7f\u7528\u6700\u540e\u4e00\u5c42\u5d4c\u5165\u6548\u679c\u8f83\u5dee\uff0c\u800c\u5c42\u5f52\u4e00\u5316\u5e73\u5747\u5d4c\u5165\u663e\u8457\u63d0\u5347\u590d\u6742\u63d0\u793a\u7684\u5bf9\u9f50\u6027\uff0c\u591a\u6570LLM\u8868\u73b0\u4f18\u4e8eT5\u57fa\u7ebf\u3002", "conclusion": "\u73b0\u4ee3LLM\u4f5c\u4e3a\u6587\u672c\u7f16\u7801\u5668\u80fd\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2506.08257", "pdf": "https://arxiv.org/pdf/2506.08257", "abs": "https://arxiv.org/abs/2506.08257", "authors": ["L. Lao Beyer", "T. Li", "X. Chen", "S. Karaman", "K. He"], "title": "Highly Compressed Tokenizer Can Generate Without Training", "categories": ["cs.CV", "cs.AI"], "comment": "Main manuscript: 9 pages, 7 figures. Appendix: 8 pages, 9 figures. To   appear in the Proceedings of the 42nd International Conference on Machine   Learning", "summary": "Commonly used image tokenizers produce a 2D grid of spatially arranged tokens. In contrast, so-called 1D image tokenizers represent images as highly compressed one-dimensional sequences of as few as 32 discrete tokens. We find that the high degree of compression achieved by a 1D tokenizer with vector quantization enables image editing and generative capabilities through heuristic manipulation of tokens, demonstrating that even very crude manipulations -- such as copying and replacing tokens between latent representations of images -- enable fine-grained image editing by transferring appearance and semantic attributes. Motivated by the expressivity of the 1D tokenizer's latent space, we construct an image generation pipeline leveraging gradient-based test-time optimization of tokens with plug-and-play loss functions such as reconstruction or CLIP similarity. Our approach is demonstrated for inpainting and text-guided image editing use cases, and can generate diverse and realistic samples without requiring training of any generative model.", "AI": {"tldr": "1D\u56fe\u50cf\u5206\u8bcd\u5668\u901a\u8fc7\u9ad8\u5ea6\u538b\u7f29\u7684\u56fe\u50cf\u8868\u793a\u5b9e\u73b0\u542f\u53d1\u5f0f\u7f16\u8f91\u548c\u751f\u6210\u80fd\u529b\uff0c\u65e0\u9700\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u3002", "motivation": "\u63a2\u7d221D\u56fe\u50cf\u5206\u8bcd\u5668\u5728\u9ad8\u5ea6\u538b\u7f29\u4e0b\u7684\u6f5c\u5728\u7a7a\u95f4\u8868\u8fbe\u80fd\u529b\uff0c\u4ee5\u5b9e\u73b0\u56fe\u50cf\u7f16\u8f91\u548c\u751f\u6210\u3002", "method": "\u5229\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u6d4b\u8bd5\u65f6\u4f18\u5316\u548c\u5373\u63d2\u5373\u7528\u7684\u635f\u5931\u51fd\u6570\uff08\u5982\u91cd\u5efa\u6216CLIP\u76f8\u4f3c\u6027\uff09\u6784\u5efa\u56fe\u50cf\u751f\u6210\u6d41\u7a0b\u3002", "result": "\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u56fe\u50cf\u7f16\u8f91\uff08\u5982\u5916\u89c2\u548c\u8bed\u4e49\u5c5e\u6027\u8f6c\u79fb\uff09\u548c\u591a\u6837\u5316\u3001\u903c\u771f\u7684\u6837\u672c\u751f\u6210\u3002", "conclusion": "1D\u56fe\u50cf\u5206\u8bcd\u5668\u5728\u56fe\u50cf\u7f16\u8f91\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u7075\u6d3b\u6027\uff0c\u65e0\u9700\u4f9d\u8d56\u751f\u6210\u6a21\u578b\u8bad\u7ec3\u3002"}}
{"id": "2506.08279", "pdf": "https://arxiv.org/pdf/2506.08279", "abs": "https://arxiv.org/abs/2506.08279", "authors": ["Aditi Sundararaman", "Amogh Adishesha", "Andrew Jaegle", "Dan Bigioi", "Hyoung-Kyu Song", "Jon Kyl", "Justin Mao", "Kevin Lan", "Mojtaba Komeili", "ShahRukh Athar", "Sheila Babayan", "Stanislau Beliasau", "William Buchwalter"], "title": "Seeing Voices: Generating A-Roll Video from Audio with Mirage", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Technical report website: mirage.app/research/seeing-voices, product   website: mirage.app", "summary": "From professional filmmaking to user-generated content, creators and consumers have long recognized that the power of video depends on the harmonious integration of what we hear (the video's audio track) with what we see (the video's image sequence). Current approaches to video generation either ignore sound to focus on general-purpose but silent image sequence generation or address both visual and audio elements but focus on restricted application domains such as re-dubbing. We introduce Mirage, an audio-to-video foundation model that excels at generating realistic, expressive output imagery from scratch given an audio input. When integrated with existing methods for speech synthesis (text-to-speech, or TTS), Mirage results in compelling multimodal video. When trained on audio-video footage of people talking (A-roll) and conditioned on audio containing speech, Mirage generates video of people delivering a believable interpretation of the performance implicit in input audio. Our central technical contribution is a unified method for training self-attention-based audio-to-video generation models, either from scratch or given existing weights. This methodology allows Mirage to retain generality as an approach to audio-to-video generation while producing outputs of superior subjective quality to methods that incorporate audio-specific architectures or loss components specific to people, speech, or details of how images or audio are captured. We encourage readers to watch and listen to the results of Mirage for themselves (see paper and comments for links).", "AI": {"tldr": "Mirage\u662f\u4e00\u4e2a\u97f3\u9891\u5230\u89c6\u9891\u7684\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u6839\u636e\u97f3\u9891\u8f93\u5165\u751f\u6210\u903c\u771f\u3001\u5bcc\u6709\u8868\u73b0\u529b\u7684\u89c6\u9891\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u8981\u4e48\u5ffd\u7565\u58f0\u97f3\u4e13\u6ce8\u4e8e\u65e0\u58f0\u56fe\u50cf\u5e8f\u5217\u751f\u6210\uff0c\u8981\u4e48\u5c40\u9650\u4e8e\u7279\u5b9a\u5e94\u7528\u9886\u57df\uff08\u5982\u91cd\u65b0\u914d\u97f3\uff09\uff0c\u7f3a\u4e4f\u901a\u7528\u7684\u97f3\u9891\u5230\u89c6\u9891\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u652f\u6301\u4ece\u5934\u8bad\u7ec3\u6216\u57fa\u4e8e\u73b0\u6709\u6743\u91cd\u8bad\u7ec3\uff0c\u9002\u7528\u4e8e\u97f3\u9891\u5230\u89c6\u9891\u751f\u6210\u3002", "result": "Mirage\u751f\u6210\u7684\u89c6\u9891\u5728\u4e3b\u89c2\u8d28\u91cf\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u80fd\u591f\u6839\u636e\u97f3\u9891\u8f93\u5165\u751f\u6210\u903c\u771f\u7684\u89c6\u9891\u5185\u5bb9\u3002", "conclusion": "Mirage\u4e3a\u97f3\u9891\u5230\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u8bed\u97f3\u76f8\u5173\u89c6\u9891\u751f\u6210\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2506.08351", "pdf": "https://arxiv.org/pdf/2506.08351", "abs": "https://arxiv.org/abs/2506.08351", "authors": ["Huixuan Zhang", "Junzhe Zhang", "Xiaojun Wan"], "title": "How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "With the rapid development of text-to-vision generation diffusion models, classifier-free guidance has emerged as the most prevalent method for conditioning. However, this approach inherently requires twice as many steps for model forwarding compared to unconditional generation, resulting in significantly higher costs. While previous study has introduced the concept of adaptive guidance, it lacks solid analysis and empirical results, making previous method unable to be applied to general diffusion models. In this work, we present another perspective of applying adaptive guidance and propose Step AG, which is a simple, universally applicable adaptive guidance strategy. Our evaluations focus on both image quality and image-text alignment. whose results indicate that restricting classifier-free guidance to the first several denoising steps is sufficient for generating high-quality, well-conditioned images, achieving an average speedup of 20% to 30%. Such improvement is consistent across different settings such as inference steps, and various models including video generation models, highlighting the superiority of our method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStep AG\u7684\u81ea\u9002\u5e94\u5f15\u5bfc\u7b56\u7565\uff0c\u901a\u8fc7\u9650\u5236\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u7684\u4f7f\u7528\u6b65\u9aa4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u65b9\u6cd5\u5728\u6587\u672c\u5230\u89c6\u89c9\u751f\u6210\u6269\u6563\u6a21\u578b\u4e2d\u666e\u904d\u4f7f\u7528\uff0c\u4f46\u5176\u9700\u8981\u4e24\u500d\u4e8e\u65e0\u6761\u4ef6\u751f\u6210\u7684\u6b65\u9aa4\uff0c\u5bfc\u81f4\u6210\u672c\u663e\u8457\u589e\u52a0\u3002\u73b0\u6709\u81ea\u9002\u5e94\u5f15\u5bfc\u65b9\u6cd5\u7f3a\u4e4f\u5206\u6790\u548c\u5b9e\u8bc1\u652f\u6301\uff0c\u65e0\u6cd5\u901a\u7528\u3002", "method": "\u63d0\u51faStep AG\u7b56\u7565\uff0c\u5c06\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u9650\u5236\u5728\u524d\u51e0\u4e2a\u53bb\u566a\u6b65\u9aa4\u4e2d\uff0c\u4ee5\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u6587\u672c\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u5e73\u5747\u63d0\u901f20%\u81f330%\uff0c\u4e14\u9002\u7528\u4e8e\u4e0d\u540c\u8bbe\u7f6e\u548c\u6a21\u578b\u3002", "conclusion": "Step AG\u662f\u4e00\u79cd\u7b80\u5355\u901a\u7528\u7684\u81ea\u9002\u5e94\u5f15\u5bfc\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6548\u7387\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6269\u6563\u6a21\u578b\u3002"}}
{"id": "2506.08456", "pdf": "https://arxiv.org/pdf/2506.08456", "abs": "https://arxiv.org/abs/2506.08456", "authors": ["June Suk Choi", "Kyungmin Lee", "Sihyun Yu", "Yisol Choi", "Jinwoo Shin", "Kimin Lee"], "title": "Enhancing Motion Dynamics of Image-to-Video Models via Adaptive Low-Pass Guidance", "categories": ["cs.CV"], "comment": "Preprint. Under review. Project page available at   http://choi403.github.io/ALG", "summary": "Recent text-to-video (T2V) models have demonstrated strong capabilities in producing high-quality, dynamic videos. To improve the visual controllability, recent works have considered fine-tuning pre-trained T2V models to support image-to-video (I2V) generation. However, such adaptation frequently suppresses motion dynamics of generated outputs, resulting in more static videos compared to their T2V counterparts. In this work, we analyze this phenomenon and identify that it stems from the premature exposure to high-frequency details in the input image, which biases the sampling process toward a shortcut trajectory that overfits to the static appearance of the reference image. To address this, we propose adaptive low-pass guidance (ALG), a simple fix to the I2V model sampling procedure to generate more dynamic videos without compromising per-frame image quality. Specifically, ALG adaptively modulates the frequency content of the conditioning image by applying low-pass filtering at the early stage of denoising. Extensive experiments demonstrate that ALG significantly improves the temporal dynamics of generated videos, while preserving image fidelity and text alignment. Especially, under VBench-I2V test suite, ALG achieves an average improvement of 36% in dynamic degree without a significant drop in video quality or image fidelity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u81ea\u9002\u5e94\u4f4e\u901a\u5f15\u5bfc\uff08ALG\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3\u56fe\u50cf\u5230\u89c6\u9891\uff08I2V\uff09\u751f\u6210\u4e2d\u52a8\u6001\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u52a8\u6001\u6027\u3002", "motivation": "\u73b0\u6709I2V\u65b9\u6cd5\u56e0\u8fc7\u65e9\u66b4\u9732\u9ad8\u9891\u7ec6\u8282\u5bfc\u81f4\u751f\u6210\u89c6\u9891\u52a8\u6001\u6027\u4e0d\u8db3\uff0cALG\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "ALG\u5728\u53bb\u566a\u65e9\u671f\u9636\u6bb5\u81ea\u9002\u5e94\u8c03\u8282\u6761\u4ef6\u56fe\u50cf\u9891\u7387\u5185\u5bb9\uff0c\u5e94\u7528\u4f4e\u901a\u6ee4\u6ce2\u3002", "result": "ALG\u663e\u8457\u63d0\u5347\u89c6\u9891\u52a8\u6001\u6027\uff08VBench-I2V\u6d4b\u8bd5\u4e2d\u52a8\u6001\u5ea6\u5e73\u5747\u63d0\u534736%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u548c\u6587\u672c\u5bf9\u9f50\u3002", "conclusion": "ALG\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u63d0\u5347I2V\u6a21\u578b\u7684\u52a8\u6001\u6027\u8868\u73b0\u3002"}}
{"id": "2506.08529", "pdf": "https://arxiv.org/pdf/2506.08529", "abs": "https://arxiv.org/abs/2506.08529", "authors": ["Xijun Wang", "Xin Li", "Bingchen Li", "Zhibo Chen"], "title": "LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid Temporal Modeling with Only 4$\\times$RTX 4090s", "categories": ["cs.CV"], "comment": "Project page: https://kopperx.github.io/projects/liftvsr", "summary": "Diffusion models have significantly advanced video super-resolution (VSR) by enhancing perceptual quality, largely through elaborately designed temporal modeling to ensure inter-frame consistency. However, existing methods usually suffer from limited temporal coherence and prohibitively high computational costs (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for long videos. In this work, we propose LiftVSR, an efficient VSR framework that leverages and elevates the image-wise diffusion prior from PixArt-$\\alpha$, achieving state-of-the-art results using only 4$\\times$RTX 4090 GPUs. To balance long-term consistency and efficiency, we introduce a hybrid temporal modeling mechanism that decomposes temporal learning into two complementary components: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal modeling within short frame segment ($\\textit{i.e.}$, low complexity), and (ii) Attention Memory Cache (AMC) for long-term temporal modeling across segments ($\\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token flows across frames within multi-head query and key tokens to warp inter-frame contexts in the value tokens. AMC adaptively aggregates historical segment information via a cache unit, ensuring long-term coherence with minimal overhead. To further stabilize the cache interaction during inference, we introduce an asymmetric sampling strategy that mitigates feature mismatches arising from different diffusion sampling steps. Extensive experiments on several typical VSR benchmarks have demonstrated that LiftVSR achieves impressive performance with significantly lower computational costs.", "AI": {"tldr": "LiftVSR\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u52a8\u6001\u65f6\u95f4\u6ce8\u610f\u529b\u548c\u6ce8\u610f\u529b\u8bb0\u5fc6\u7f13\u5b58\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u957f\u671f\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u8ba1\u7b97\u6210\u672c\u65b9\u9762\u5b58\u5728\u95ee\u9898\uff0c\u5c24\u5176\u662f\u957f\u89c6\u9891\u5904\u7406\u9700\u8981\u5927\u91cfGPU\u8d44\u6e90\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u65f6\u95f4\u5efa\u6a21\u673a\u5236\uff0c\u5305\u62ec\u52a8\u6001\u65f6\u95f4\u6ce8\u610f\u529b\uff08DTA\uff09\u548c\u6ce8\u610f\u529b\u8bb0\u5fc6\u7f13\u5b58\uff08AMC\uff09\uff0c\u5e76\u7ed3\u5408\u975e\u5bf9\u79f0\u91c7\u6837\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2aVSR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "LiftVSR\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u4e3a\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08619", "pdf": "https://arxiv.org/pdf/2506.08619", "abs": "https://arxiv.org/abs/2506.08619", "authors": ["Gon\u00e7alo Dias Pais", "Valter Piedade", "Moitreya Chatterjee", "Marcus Greiff", "Pedro Miraldo"], "title": "A Probability-guided Sampler for Neural Implicit Surface Rendering", "categories": ["cs.CV"], "comment": "Accepted in ECCV 2024", "summary": "Several variants of Neural Radiance Fields (NeRFs) have significantly improved the accuracy of synthesized images and surface reconstruction of 3D scenes/objects. In all of these methods, a key characteristic is that none can train the neural network with every possible input data, specifically, every pixel and potential 3D point along the projection rays due to scalability issues. While vanilla NeRFs uniformly sample both the image pixels and 3D points along the projection rays, some variants focus only on guiding the sampling of the 3D points along the projection rays. In this paper, we leverage the implicit surface representation of the foreground scene and model a probability density function in a 3D image projection space to achieve a more targeted sampling of the rays toward regions of interest, resulting in improved rendering. Additionally, a new surface reconstruction loss is proposed for improved performance. This new loss fully explores the proposed 3D image projection space model and incorporates near-to-surface and empty space components. By integrating our novel sampling strategy and novel loss into current state-of-the-art neural implicit surface renderers, we achieve more accurate and detailed 3D reconstructions and improved image rendering, especially for the regions of interest in any given scene.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u8868\u9762\u8868\u793a\u548c3D\u56fe\u50cf\u6295\u5f71\u7a7a\u95f4\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u7684\u91c7\u6837\u7b56\u7565\uff0c\u7ed3\u5408\u65b0\u7684\u8868\u9762\u91cd\u5efa\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u91cd\u5efa\u548c\u56fe\u50cf\u6e32\u67d3\u7684\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709NeRF\u65b9\u6cd5\u56e0\u53ef\u6269\u5c55\u6027\u95ee\u9898\u65e0\u6cd5\u5bf9\u6240\u6709\u53ef\u80fd\u7684\u8f93\u5165\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5bfc\u81f4\u91c7\u6837\u6548\u7387\u4f4e\u4e0b\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u66f4\u9488\u5bf9\u6027\u7684\u91c7\u6837\u7b56\u7565\u548c\u65b0\u7684\u635f\u5931\u51fd\u6570\u63d0\u5347\u6e32\u67d3\u6548\u679c\u3002", "method": "\u5229\u7528\u9690\u5f0f\u8868\u9762\u8868\u793a\u548c3D\u56fe\u50cf\u6295\u5f71\u7a7a\u95f4\u7684\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u8fdb\u884c\u9488\u5bf9\u6027\u91c7\u6837\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u8868\u9762\u91cd\u5efa\u635f\u5931\u51fd\u6570\uff0c\u7ed3\u5408\u8fd1\u8868\u9762\u548c\u7a7a\u767d\u7a7a\u95f4\u4fe1\u606f\u3002", "result": "\u5728\u73b0\u6709\u795e\u7ecf\u9690\u5f0f\u8868\u9762\u6e32\u67d3\u5668\u4e2d\u96c6\u6210\u65b0\u65b9\u6cd5\u540e\uff0c3D\u91cd\u5efa\u548c\u56fe\u50cf\u6e32\u67d3\u7684\u7cbe\u5ea6\u663e\u8457\u63d0\u5347\uff0c\u5c24\u5176\u662f\u5bf9\u573a\u666f\u4e2d\u7684\u611f\u5174\u8da3\u533a\u57df\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u91c7\u6837\u7b56\u7565\u548c\u635f\u5931\u51fd\u6570\u6709\u6548\u63d0\u5347\u4e86\u6e32\u67d3\u548c\u91cd\u5efa\u7684\u7cbe\u5ea6\uff0c\u4e3aNeRF\u65b9\u6cd5\u7684\u8fdb\u4e00\u6b65\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.08632", "pdf": "https://arxiv.org/pdf/2506.08632", "abs": "https://arxiv.org/abs/2506.08632", "authors": ["Yang Bai", "Liudi Yang", "George Eskandar", "Fengyi Shen", "Dong Chen", "Mohammad Altillawi", "Ziyuan Liu", "Gitta Kutyniok"], "title": "RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in generative models have revolutionized video synthesis and editing. However, the scarcity of diverse, high-quality datasets continues to hinder video-conditioned robotic learning, limiting cross-platform generalization. In this work, we address the challenge of swapping a robotic arm in one video with another: a key step for crossembodiment learning. Unlike previous methods that depend on paired video demonstrations in the same environmental settings, our proposed framework, RoboSwap, operates on unpaired data from diverse environments, alleviating the data collection needs. RoboSwap introduces a novel video editing pipeline integrating both GANs and diffusion models, combining their isolated advantages. Specifically, we segment robotic arms from their backgrounds and train an unpaired GAN model to translate one robotic arm to another. The translated arm is blended with the original video background and refined with a diffusion model to enhance coherence, motion realism and object interaction. The GAN and diffusion stages are trained independently. Our experiments demonstrate that RoboSwap outperforms state-of-the-art video and image editing models on three benchmarks in terms of both structural coherence and motion consistency, thereby offering a robust solution for generating reliable, cross-embodiment data in robotic learning.", "AI": {"tldr": "RoboSwap\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGAN\u548c\u6269\u6563\u6a21\u578b\u7684\u89c6\u9891\u7f16\u8f91\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u672a\u914d\u5bf9\u6570\u636e\u4e2d\u4ea4\u6362\u673a\u68b0\u81c2\uff0c\u89e3\u51b3\u4e86\u8de8\u5e73\u53f0\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u89c6\u9891\u6761\u4ef6\u4e0b\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u8de8\u5e73\u53f0\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408GAN\u548c\u6269\u6563\u6a21\u578b\uff0c\u5206\u6bb5\u5904\u7406\u673a\u68b0\u81c2\u4e0e\u80cc\u666f\uff0c\u901a\u8fc7\u672a\u914d\u5bf9GAN\u6a21\u578b\u7ffb\u8bd1\u673a\u68b0\u81c2\uff0c\u518d\u7528\u6269\u6563\u6a21\u578b\u589e\u5f3a\u4e00\u81f4\u6027\u548c\u8fd0\u52a8\u771f\u5b9e\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRoboSwap\u5728\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u8fd0\u52a8\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u89c6\u9891\u548c\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u3002", "conclusion": "RoboSwap\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u8de8\u5e73\u53f0\u6570\u636e\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08640", "pdf": "https://arxiv.org/pdf/2506.08640", "abs": "https://arxiv.org/abs/2506.08640", "authors": ["Yichong Lu", "Yuzhuo Tian", "Zijin Jiang", "Yikun Zhao", "Yuanbo Yang", "Hao Ouyang", "Haoji Hu", "Huimin Yu", "Yujun Shen", "Yiyi Liao"], "title": "Orientation Matters: Making 3D Generative Models Orientation-Aligned", "categories": ["cs.CV"], "comment": "Project Page: https://xdimlab.github.io/Orientation_Matters", "summary": "Humans intuitively perceive object shape and orientation from a single image, guided by strong priors about canonical poses. However, existing 3D generative models often produce misaligned results due to inconsistent training data, limiting their usability in downstream tasks. To address this gap, we introduce the task of orientation-aligned 3D object generation: producing 3D objects from single images with consistent orientations across categories. To facilitate this, we construct Objaverse-OA, a dataset of 14,832 orientation-aligned 3D models spanning 1,008 categories. Leveraging Objaverse-OA, we fine-tune two representative 3D generative models based on multi-view diffusion and 3D variational autoencoder frameworks to produce aligned objects that generalize well to unseen objects across various categories. Experimental results demonstrate the superiority of our method over post-hoc alignment approaches. Furthermore, we showcase downstream applications enabled by our aligned object generation, including zero-shot object orientation estimation via analysis-by-synthesis and efficient arrow-based object rotation manipulation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b33D\u751f\u6210\u6a21\u578b\u65b9\u5411\u4e0d\u4e00\u81f4\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u5bf9\u9f50\u6570\u636e\u96c6\u548c\u5fae\u8c03\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u65b9\u5411\u4e00\u81f4\u76843D\u5bf9\u8c61\u751f\u6210\u3002", "motivation": "\u73b0\u67093D\u751f\u6210\u6a21\u578b\u56e0\u8bad\u7ec3\u6570\u636e\u4e0d\u4e00\u81f4\u5bfc\u81f4\u751f\u6210\u7ed3\u679c\u65b9\u5411\u4e0d\u5bf9\u9f50\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u6784\u5efaObjaverse-OA\u6570\u636e\u96c6\uff0814,832\u4e2a\u5bf9\u9f503D\u6a21\u578b\uff09\uff0c\u5e76\u57fa\u4e8e\u591a\u89c6\u56fe\u6269\u6563\u548c3D\u53d8\u5206\u81ea\u7f16\u7801\u5668\u6846\u67b6\u5fae\u8c03\u751f\u6210\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u540e\u5904\u7406\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u96f6\u6837\u672c\u65b9\u5411\u4f30\u8ba1\u548c\u9ad8\u6548\u65cb\u8f6c\u64cd\u4f5c\uff09\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u901a\u8fc7\u65b9\u5411\u5bf9\u9f50\u76843D\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u62d3\u5c55\u4e86\u4e0b\u6e38\u4efb\u52a1\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2506.08704", "pdf": "https://arxiv.org/pdf/2506.08704", "abs": "https://arxiv.org/abs/2506.08704", "authors": ["Xiaohan Zhang", "Sitong Wang", "Yushen Yan", "Yi Yang", "Mingda Xu", "Qi Liu"], "title": "TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary Large-Scale Scene Rendering", "categories": ["cs.CV"], "comment": null, "summary": "High-quality novel view synthesis for large-scale scenes presents a challenging dilemma in 3D computer vision. Existing methods typically partition large scenes into multiple regions, reconstruct a 3D representation using Gaussian splatting for each region, and eventually merge them for novel view rendering. They can accurately render specific scenes, yet they do not generalize effectively for two reasons: (1) rigid spatial partition techniques struggle with arbitrary camera trajectories, and (2) the merging of regions results in Gaussian overlap to distort texture details. To address these challenges, we propose TraGraph-GS, leveraging a trajectory graph to enable high-precision rendering for arbitrarily large-scale scenes. We present a spatial partitioning method for large-scale scenes based on graphs, which incorporates a regularization constraint to enhance the rendering of textures and distant objects, as well as a progressive rendering strategy to mitigate artifacts caused by Gaussian overlap. Experimental results demonstrate its superior performance both on four aerial and four ground datasets and highlight its remarkable efficiency: our method achieves an average improvement of 1.86 dB in PSNR on aerial datasets and 1.62 dB on ground datasets compared to state-of-the-art approaches.", "AI": {"tldr": "TraGraph-GS\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f68\u8ff9\u56fe\u7684\u7a7a\u95f4\u5206\u533a\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u89c6\u56fe\u5408\u6210\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6e32\u67d3\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5206\u533a\u548c\u5408\u5e76\u65f6\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u548c\u7eb9\u7406\u5931\u771f\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u9002\u5e94\u4efb\u610f\u76f8\u673a\u8f68\u8ff9\u3002", "method": "\u91c7\u7528\u8f68\u8ff9\u56fe\u8fdb\u884c\u7a7a\u95f4\u5206\u533a\uff0c\u5f15\u5165\u6b63\u5219\u5316\u7ea6\u675f\u548c\u6e10\u8fdb\u6e32\u67d3\u7b56\u7565\u4ee5\u51cf\u5c11\u9ad8\u65af\u91cd\u53e0\u548c\u63d0\u5347\u7eb9\u7406\u6e32\u67d3\u3002", "result": "\u5728\u56db\u4e2a\u7a7a\u4e2d\u548c\u56db\u4e2a\u5730\u9762\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cPSNR\u5e73\u5747\u63d0\u53471.86 dB\uff08\u7a7a\u4e2d\uff09\u548c1.62 dB\uff08\u5730\u9762\uff09\u3002", "conclusion": "TraGraph-GS\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u573a\u666f\u6e32\u67d3\u7684\u6311\u6218\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2506.08710", "pdf": "https://arxiv.org/pdf/2506.08710", "abs": "https://arxiv.org/abs/2506.08710", "authors": ["Mengjiao Ma", "Qi Ma", "Yue Li", "Jiahuan Cheng", "Runyi Yang", "Bin Ren", "Nikola Popovic", "Mingqiang Wei", "Nicu Sebe", "Luc Van Gool", "Theo Gevers", "Martin R. Oswald", "Danda Pani Paudel"], "title": "SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting", "categories": ["cs.CV"], "comment": "15 pages, codes, data and benchmark will be released", "summary": "3D Gaussian Splatting (3DGS) serves as a highly performant and efficient encoding of scene geometry, appearance, and semantics. Moreover, grounding language in 3D scenes has proven to be an effective strategy for 3D scene understanding. Current Language Gaussian Splatting line of work fall into three main groups: (i) per-scene optimization-based, (ii) per-scene optimization-free, and (iii) generalizable approach. However, most of them are evaluated only on rendered 2D views of a handful of scenes and viewpoints close to the training views, limiting ability and insight into holistic 3D understanding. To address this gap, we propose the first large-scale benchmark that systematically assesses these three groups of methods directly in 3D space, evaluating on 1060 scenes across three indoor datasets and one outdoor dataset. Benchmark results demonstrate a clear advantage of the generalizable paradigm, particularly in relaxing the scene-specific limitation, enabling fast feed-forward inference on novel scenes, and achieving superior segmentation performance. We further introduce GaussianWorld-49K a carefully curated 3DGS dataset comprising around 49K diverse indoor and outdoor scenes obtained from multiple sources, with which we demonstrate the generalizable approach could harness strong data priors. Our codes, benchmark, and datasets will be made public to accelerate research in generalizable 3DGS scene understanding.", "AI": {"tldr": "3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u662f\u4e00\u79cd\u9ad8\u6548\u7f16\u7801\u573a\u666f\u51e0\u4f55\u3001\u5916\u89c2\u548c\u8bed\u4e49\u7684\u65b9\u6cd5\u3002\u672c\u6587\u63d0\u51fa\u9996\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e09\u79cd\u8bed\u8a00\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u57283D\u7a7a\u95f4\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5f15\u5165GaussianWorld-49K\u6570\u636e\u96c6\uff0c\u5c55\u793a\u901a\u7528\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u57fa\u4e8e2D\u89c6\u56fe\u8bc4\u4f30\uff0c\u7f3a\u4e4f\u5bf9\u6574\u4f533D\u7406\u89e3\u7684\u6df1\u5165\u6d1e\u5bdf\uff0c\u56e0\u6b64\u9700\u8981\u76f4\u63a5\u8bc4\u4f303D\u7a7a\u95f4\u4e2d\u7684\u65b9\u6cd5\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e09\u7c7b\u65b9\u6cd5\uff08\u57fa\u4e8e\u573a\u666f\u4f18\u5316\u3001\u65e0\u4f18\u5316\u548c\u901a\u7528\u65b9\u6cd5\uff09\u57281060\u4e2a\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5f15\u516549K\u573a\u666f\u6570\u636e\u96c6\u3002", "result": "\u901a\u7528\u65b9\u6cd5\u5728\u653e\u677e\u573a\u666f\u9650\u5236\u3001\u5feb\u901f\u63a8\u7406\u548c\u5206\u5272\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u901a\u7528\u65b9\u6cd5\u7ed3\u5408\u5927\u6570\u636e\u96c6\u80fd\u663e\u8457\u63d0\u53473DGS\u573a\u666f\u7406\u89e3\uff0c\u76f8\u5173\u8d44\u6e90\u5c06\u516c\u5f00\u4ee5\u63a8\u52a8\u7814\u7a76\u3002"}}
{"id": "2506.08777", "pdf": "https://arxiv.org/pdf/2506.08777", "abs": "https://arxiv.org/abs/2506.08777", "authors": ["Keyi Liu", "Weidong Yang", "Ben Fei", "Ying He"], "title": "Gaussian2Scene: 3D Scene Representation Learning via Self-supervised Learning with 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Self-supervised learning (SSL) for point cloud pre-training has become a cornerstone for many 3D vision tasks, enabling effective learning from large-scale unannotated data. At the scene level, existing SSL methods often incorporate volume rendering into the pre-training framework, using RGB-D images as reconstruction signals to facilitate cross-modal learning. This strategy promotes alignment between 2D and 3D modalities and enables the model to benefit from rich visual cues in the RGB-D inputs. However, these approaches are limited by their reliance on implicit scene representations and high memory demands. Furthermore, since their reconstruction objectives are applied only in 2D space, they often fail to capture underlying 3D geometric structures. To address these challenges, we propose Gaussian2Scene, a novel scene-level SSL framework that leverages the efficiency and explicit nature of 3D Gaussian Splatting (3DGS) for pre-training. The use of 3DGS not only alleviates the computational burden associated with volume rendering but also supports direct 3D scene reconstruction, thereby enhancing the geometric understanding of the backbone network. Our approach follows a progressive two-stage training strategy. In the first stage, a dual-branch masked autoencoder learns both 2D and 3D scene representations. In the second stage, we initialize training with reconstructed point clouds and further supervise learning using the geometric locations of Gaussian primitives and rendered RGB images. This process reinforces both geometric and cross-modal learning. We demonstrate the effectiveness of Gaussian2Scene across several downstream 3D object detection tasks, showing consistent improvements over existing pre-training methods.", "AI": {"tldr": "Gaussian2Scene\u662f\u4e00\u79cd\u65b0\u578b\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u75283D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u8fdb\u884c\u70b9\u4e91\u9884\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u548c\u51e0\u4f55\u7ed3\u6784\u6355\u6349\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u9690\u5f0f\u573a\u666f\u8868\u793a\u548c\u9ad8\u5185\u5b58\u9700\u6c42\uff0c\u4e14\u91cd\u5efa\u76ee\u6807\u4ec5\u9002\u7528\u4e8e2D\u7a7a\u95f4\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u63493D\u51e0\u4f55\u7ed3\u6784\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u53cc\u5206\u652f\u63a9\u7801\u81ea\u7f16\u7801\u5668\u5b66\u4e602D\u548c3D\u573a\u666f\u8868\u793a\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5229\u7528\u91cd\u5efa\u70b9\u4e91\u548c\u9ad8\u65af\u57fa\u5143\u7684\u51e0\u4f55\u4f4d\u7f6e\u8fdb\u884c\u76d1\u7763\u5b66\u4e60\u3002", "result": "\u5728\u591a\u4e2a\u4e0b\u6e383D\u7269\u4f53\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "conclusion": "Gaussian2Scene\u901a\u8fc73DGS\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u51e0\u4f55\u7406\u89e3\u80fd\u529b\uff0c\u4e3a3D\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u9884\u8bad\u7ec3\u6846\u67b6\u3002"}}
{"id": "2506.08793", "pdf": "https://arxiv.org/pdf/2506.08793", "abs": "https://arxiv.org/abs/2506.08793", "authors": ["Zhuoran Zheng"], "title": "A PDE-Based Image Dehazing Method via Atmospheric Scattering Theory", "categories": ["cs.CV", "eess.IV"], "comment": "report", "summary": "This paper presents a novel partial differential equation (PDE) framework for single-image dehazing. By integrating the atmospheric scattering model with nonlocal regularization and dark channel prior, we propose the improved PDE: \\[ -\\text{div}\\left(D(\\nabla u)\\nabla u\\right) + \\lambda(t) G(u) = \\Phi(I,t,A) \\] where $D(\\nabla u) = (|\\nabla u| + \\epsilon)^{-1}$ is the edge-preserving diffusion coefficient, $G(u)$ is the Gaussian convolution operator, and $\\lambda(t)$ is the adaptive regularization parameter based on transmission map $t$. We prove the existence and uniqueness of weak solutions in $H_0^1(\\Omega)$ using Lax-Milgram theorem, and implement an efficient fixed-point iteration scheme accelerated by PyTorch GPU computation. The experimental results demonstrate that this method is a promising deghazing solution that can be generalized to the deep model paradigm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u7684\u5355\u5e45\u56fe\u50cf\u53bb\u96fe\u65b9\u6cd5\uff0c\u7ed3\u5408\u5927\u6c14\u6563\u5c04\u6a21\u578b\u3001\u975e\u5c40\u90e8\u6b63\u5219\u5316\u548c\u6697\u901a\u9053\u5148\u9a8c\uff0c\u6539\u8fdb\u4e86PDE\u6846\u67b6\uff0c\u5e76\u8bc1\u660e\u4e86\u5f31\u89e3\u7684\u5b58\u5728\u6027\u548c\u552f\u4e00\u6027\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u662f\u4e00\u79cd\u6709\u6548\u7684\u53bb\u96fe\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u7684\u53bb\u96fe\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u573a\u666f\u65f6\u6548\u679c\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u7406\u8bba\u5b8c\u5907\u7684PDE\u6846\u67b6\u6765\u63d0\u5347\u53bb\u96fe\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684PDE\u6846\u67b6\uff0c\u7ed3\u5408\u8fb9\u7f18\u4fdd\u6301\u6269\u6563\u7cfb\u6570\u3001\u9ad8\u65af\u5377\u79ef\u7b97\u5b50\u548c\u81ea\u9002\u5e94\u6b63\u5219\u5316\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u56fa\u5b9a\u70b9\u8fed\u4ee3\u65b9\u6848\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u53bb\u96fe\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u53ef\u63a8\u5e7f\u81f3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5355\u5e45\u56fe\u50cf\u53bb\u96fe\u63d0\u4f9b\u4e86\u4e00\u79cd\u7406\u8bba\u5b8c\u5907\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.08797", "pdf": "https://arxiv.org/pdf/2506.08797", "abs": "https://arxiv.org/abs/2506.08797", "authors": ["Ziyao Huang", "Zixiang Zhou", "Juan Cao", "Yifeng Ma", "Yi Chen", "Zejing Rao", "Zhiyong Xu", "Hongmei Wang", "Qin Lin", "Yuan Zhou", "Qinglin Lu", "Fan Tang"], "title": "HunyuanVideo-HOMA: Generic Human-Object Interaction in Multimodal Driven Human Animation", "categories": ["cs.CV"], "comment": null, "summary": "To address key limitations in human-object interaction (HOI) video generation -- specifically the reliance on curated motion data, limited generalization to novel objects/scenarios, and restricted accessibility -- we introduce HunyuanVideo-HOMA, a weakly conditioned multimodal-driven framework. HunyuanVideo-HOMA enhances controllability and reduces dependency on precise inputs through sparse, decoupled motion guidance. It encodes appearance and motion signals into the dual input space of a multimodal diffusion transformer (MMDiT), fusing them within a shared context space to synthesize temporally consistent and physically plausible interactions. To optimize training, we integrate a parameter-space HOI adapter initialized from pretrained MMDiT weights, preserving prior knowledge while enabling efficient adaptation, and a facial cross-attention adapter for anatomically accurate audio-driven lip synchronization. Extensive experiments confirm state-of-the-art performance in interaction naturalness and generalization under weak supervision. Finally, HunyuanVideo-HOMA demonstrates versatility in text-conditioned generation and interactive object manipulation, supported by a user-friendly demo interface. The project page is at https://anonymous.4open.science/w/homa-page-0FBE/.", "AI": {"tldr": "HunyuanVideo-HOMA\u662f\u4e00\u4e2a\u5f31\u6761\u4ef6\u591a\u6a21\u6001\u9a71\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3HOI\u89c6\u9891\u751f\u6210\u4e2d\u7684\u5173\u952e\u9650\u5236\uff0c\u5982\u4f9d\u8d56\u7cbe\u9009\u8fd0\u52a8\u6570\u636e\u3001\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u548c\u53ef\u8bbf\u95ee\u6027\u53d7\u9650\u3002", "motivation": "\u89e3\u51b3HOI\u89c6\u9891\u751f\u6210\u4e2d\u5bf9\u7cbe\u9009\u8fd0\u52a8\u6570\u636e\u7684\u4f9d\u8d56\u3001\u5bf9\u65b0\u5bf9\u8c61/\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\u4ee5\u53ca\u53ef\u8bbf\u95ee\u6027\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7a00\u758f\u89e3\u8026\u7684\u8fd0\u52a8\u5f15\u5bfc\u589e\u5f3a\u53ef\u63a7\u6027\uff0c\u5c06\u5916\u89c2\u548c\u8fd0\u52a8\u4fe1\u53f7\u7f16\u7801\u5230\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\uff08MMDiT\uff09\u7684\u53cc\u8f93\u5165\u7a7a\u95f4\u4e2d\uff0c\u5e76\u5728\u5171\u4eab\u4e0a\u4e0b\u6587\u7a7a\u95f4\u4e2d\u878d\u5408\u4ee5\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u4e14\u7269\u7406\u5408\u7406\u7684\u4ea4\u4e92\u3002", "result": "\u5728\u5f31\u76d1\u7763\u4e0b\u5b9e\u73b0\u4e86\u4ea4\u4e92\u81ea\u7136\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5728\u6587\u672c\u6761\u4ef6\u751f\u6210\u548c\u4ea4\u4e92\u5f0f\u5bf9\u8c61\u64cd\u4f5c\u4e2d\u8868\u73b0\u51fa\u591a\u529f\u80fd\u6027\u3002", "conclusion": "HunyuanVideo-HOMA\u901a\u8fc7\u521b\u65b0\u7684\u6846\u67b6\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86HOI\u89c6\u9891\u751f\u6210\u7684\u6027\u80fd\u548c\u53ef\u8bbf\u95ee\u6027\u3002"}}
{"id": "2506.08809", "pdf": "https://arxiv.org/pdf/2506.08809", "abs": "https://arxiv.org/abs/2506.08809", "authors": ["Jiaze E", "Srutarshi Banerjee", "Tekin Bicer", "Guannan Wang", "Yanfu Zhang", "Bin Ren"], "title": "HiSin: Efficient High-Resolution Sinogram Inpainting via Resolution-Guided Progressive Inference", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "High-resolution sinogram inpainting is essential for computed tomography reconstruction, as missing high-frequency projections can lead to visible artifacts and diagnostic errors. Diffusion models are well-suited for this task due to their robustness and detail-preserving capabilities, but their application to high-resolution inputs is limited by excessive memory and computational demands. To address this limitation, we propose HiSin, a novel diffusion based framework for efficient sinogram inpainting via resolution-guided progressive inference. It progressively extracts global structure at low resolution and defers high-resolution inference to small patches, enabling memory-efficient inpainting. It further incorporates frequency-aware patch skipping and structure-adaptive step allocation to reduce redundant computation. Experimental results show that HiSin reduces peak memory usage by up to 31.25% and inference time by up to 18.15%, and maintains inpainting accuracy across datasets, resolutions, and mask conditions.", "AI": {"tldr": "HiSin\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9ad8\u6548\u6b63\u5f26\u56fe\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u8fa8\u7387\u5f15\u5bfc\u7684\u6e10\u8fdb\u63a8\u7406\u5b9e\u73b0\u5185\u5b58\u9ad8\u6548\u4fee\u590d\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u6b63\u5f26\u56fe\u4fee\u590d\u5bf9CT\u91cd\u5efa\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6269\u6563\u6a21\u578b\u56e0\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\u8fc7\u9ad8\u800c\u53d7\u9650\u3002", "method": "HiSin\u91c7\u7528\u5206\u8fa8\u7387\u5f15\u5bfc\u7684\u6e10\u8fdb\u63a8\u7406\uff0c\u5148\u5728\u4f4e\u5206\u8fa8\u7387\u63d0\u53d6\u5168\u5c40\u7ed3\u6784\uff0c\u518d\u5728\u9ad8\u5206\u8fa8\u7387\u5904\u7406\u5c0f\u8865\u4e01\uff0c\u5e76\u7ed3\u5408\u9891\u7387\u611f\u77e5\u8865\u4e01\u8df3\u8fc7\u548c\u7ed3\u6784\u81ea\u9002\u5e94\u6b65\u9aa4\u5206\u914d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHiSin\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u51cf\u5c1131.25%\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c1118.15%\uff0c\u4e14\u4fee\u590d\u7cbe\u5ea6\u4e0d\u53d7\u5f71\u54cd\u3002", "conclusion": "HiSin\u4e3a\u9ad8\u5206\u8fa8\u7387\u6b63\u5f26\u56fe\u4fee\u590d\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08862", "pdf": "https://arxiv.org/pdf/2506.08862", "abs": "https://arxiv.org/abs/2506.08862", "authors": ["Zike Wu", "Qi Yan", "Xuanyu Yi", "Lele Wang", "Renjie Liao"], "title": "StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams is crucial for numerous real-world applications. However, existing methods struggle to jointly address three key challenges: 1) processing uncalibrated inputs in real time, 2) accurately modeling dynamic scene evolution, and 3) maintaining long-term stability and computational efficiency. To this end, we introduce StreamSplat, the first fully feed-forward framework that transforms uncalibrated video streams of arbitrary length into dynamic 3D Gaussian Splatting (3DGS) representations in an online manner, capable of recovering scene dynamics from temporally local observations. We propose two key technical innovations: a probabilistic sampling mechanism in the static encoder for 3DGS position prediction, and a bidirectional deformation field in the dynamic decoder that enables robust and efficient dynamic modeling. Extensive experiments on static and dynamic benchmarks demonstrate that StreamSplat consistently outperforms prior works in both reconstruction quality and dynamic scene modeling, while uniquely supporting online reconstruction of arbitrarily long video streams. Code and models are available at https://github.com/nickwzk/StreamSplat.", "AI": {"tldr": "StreamSplat\u662f\u4e00\u4e2a\u5b9e\u65f6\u5904\u7406\u672a\u6821\u51c6\u89c6\u9891\u6d41\u5e76\u751f\u6210\u52a8\u60013D\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u65f6\u6027\u3001\u52a8\u6001\u5efa\u6a21\u548c\u957f\u671f\u7a33\u5b9a\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5904\u7406\u672a\u6821\u51c6\u8f93\u5165\u3001\u52a8\u6001\u573a\u666f\u5efa\u6a21\u548c\u957f\u671f\u7a33\u5b9a\u6027\uff0cStreamSplat\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u9759\u6001\u7f16\u7801\u5668\u4e2d\u7684\u6982\u7387\u91c7\u6837\u673a\u5236\u548c\u52a8\u6001\u89e3\u7801\u5668\u4e2d\u7684\u53cc\u5411\u53d8\u5f62\u573a\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u52a8\u6001\u5efa\u6a21\u3002", "result": "\u5728\u9759\u6001\u548c\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cStreamSplat\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u52a8\u6001\u5efa\u6a21\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u652f\u6301\u5728\u7ebf\u91cd\u5efa\u3002", "conclusion": "StreamSplat\u662f\u9996\u4e2a\u652f\u6301\u5728\u7ebf\u91cd\u5efa\u672a\u6821\u51c6\u89c6\u9891\u6d41\u7684\u6846\u67b6\uff0c\u5177\u6709\u9ad8\u6548\u548c\u7a33\u5b9a\u7684\u52a8\u6001\u5efa\u6a21\u80fd\u529b\u3002"}}
{"id": "2506.08894", "pdf": "https://arxiv.org/pdf/2506.08894", "abs": "https://arxiv.org/abs/2506.08894", "authors": ["Yunzhi Zhang", "Carson Murtuza-Lanier", "Zizhang Li", "Yilun Du", "Jiajun Wu"], "title": "Product of Experts for Visual Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://product-of-experts.github.io/", "summary": "Modern neural models capture rich priors and have complementary knowledge over shared data domains, e.g., images and videos. Integrating diverse knowledge from multiple sources -- including visual generative models, visual language models, and sources with human-crafted knowledge such as graphics engines and physics simulators -- remains under-explored. We propose a Product of Experts (PoE) framework that performs inference-time knowledge composition from heterogeneous models. This training-free approach samples from the product distribution across experts via Annealed Importance Sampling (AIS). Our framework shows practical benefits in image and video synthesis tasks, yielding better controllability than monolithic methods and additionally providing flexible user interfaces for specifying visual generation goals.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e13\u5bb6\u4e58\u79ef\uff08PoE\uff09\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u65e0\u5173\u7684\u65b9\u6cd5\uff08AIS\uff09\u4ece\u5f02\u6784\u6a21\u578b\u4e2d\u7ec4\u5408\u77e5\u8bc6\uff0c\u63d0\u5347\u56fe\u50cf\u548c\u89c6\u9891\u5408\u6210\u7684\u53ef\u63a7\u6027\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u73b0\u4ee3\u795e\u7ecf\u6a21\u578b\u5728\u5171\u4eab\u6570\u636e\u9886\u57df\uff08\u5982\u56fe\u50cf\u548c\u89c6\u9891\uff09\u4e0a\u5177\u6709\u4e30\u5bcc\u7684\u5148\u9a8c\u548c\u4e92\u8865\u77e5\u8bc6\uff0c\u4f46\u5982\u4f55\u6574\u5408\u6765\u81ea\u4e0d\u540c\u6765\u6e90\uff08\u5982\u751f\u6210\u6a21\u578b\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u56fe\u5f62\u5f15\u64ce\u7b49\uff09\u7684\u591a\u6837\u5316\u77e5\u8bc6\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u91c7\u7528\u4e13\u5bb6\u4e58\u79ef\uff08PoE\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u9000\u706b\u91cd\u8981\u6027\u91c7\u6837\uff08AIS\uff09\u4ece\u5f02\u6784\u6a21\u578b\u7684\u4e58\u79ef\u5206\u5e03\u4e2d\u8fdb\u884c\u63a8\u7406\u65f6\u77e5\u8bc6\u7ec4\u5408\u3002", "result": "\u5728\u56fe\u50cf\u548c\u89c6\u9891\u5408\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5b9e\u7528\u6027\uff0c\u6bd4\u5355\u4e00\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u53ef\u63a7\u6027\uff0c\u5e76\u63d0\u4f9b\u7075\u6d3b\u7684\u7528\u6237\u754c\u9762\u4ee5\u6307\u5b9a\u89c6\u89c9\u751f\u6210\u76ee\u6807\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u5f02\u6784\u6a21\u578b\u7684\u77e5\u8bc6\u7ec4\u5408\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u7075\u6d3b\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u89c6\u89c9\u751f\u6210\u4efb\u52a1\u3002"}}
{"id": "2506.08908", "pdf": "https://arxiv.org/pdf/2506.08908", "abs": "https://arxiv.org/abs/2506.08908", "authors": ["Jiajun Li", "Yue Ma", "Xinyu Zhang", "Qingyan Wei", "Songhua Liu", "Linfeng Zhang"], "title": "SkipVAR: Accelerating Visual Autoregressive Modeling via Adaptive Frequency-Aware Skipping", "categories": ["cs.CV"], "comment": null, "summary": "Recent studies on Visual Autoregressive (VAR) models have highlighted that high-frequency components, or later steps, in the generation process contribute disproportionately to inference latency. However, the underlying computational redundancy involved in these steps has yet to be thoroughly investigated. In this paper, we conduct an in-depth analysis of the VAR inference process and identify two primary sources of inefficiency: step redundancy and unconditional branch redundancy. To address step redundancy, we propose an automatic step-skipping strategy that selectively omits unnecessary generation steps to improve efficiency. For unconditional branch redundancy, we observe that the information gap between the conditional and unconditional branches is minimal. Leveraging this insight, we introduce unconditional branch replacement, a technique that bypasses the unconditional branch to reduce computational cost. Notably, we observe that the effectiveness of acceleration strategies varies significantly across different samples. Motivated by this, we propose SkipVAR, a sample-adaptive framework that leverages frequency information to dynamically select the most suitable acceleration strategy for each instance. To evaluate the role of high-frequency information, we introduce high-variation benchmark datasets that test model sensitivity to fine details. Extensive experiments show SkipVAR achieves over 0.88 average SSIM with up to 1.81x overall acceleration and 2.62x speedup on the GenEval benchmark, maintaining model quality. These results confirm the effectiveness of frequency-aware, training-free adaptive acceleration for scalable autoregressive image generation. Our code is available at https://github.com/fakerone-li/SkipVAR and has been publicly released.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSkipVAR\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u8df3\u8fc7\u5197\u4f59\u6b65\u9aa4\u548c\u66ff\u6362\u65e0\u6761\u4ef6\u5206\u652f\uff0c\u52a8\u6001\u9009\u62e9\u52a0\u901f\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347VAR\u6a21\u578b\u6548\u7387\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0VAR\u6a21\u578b\u7684\u9ad8\u9891\u7ec4\u4ef6\u6216\u540e\u671f\u6b65\u9aa4\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\uff0c\u4f46\u8ba1\u7b97\u5197\u4f59\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002", "method": "\u5206\u6790VAR\u63a8\u7406\u8fc7\u7a0b\uff0c\u63d0\u51fa\u81ea\u52a8\u8df3\u8fc7\u5197\u4f59\u6b65\u9aa4\u548c\u65e0\u6761\u4ef6\u5206\u652f\u66ff\u6362\u6280\u672f\uff0c\u5e76\u8bbe\u8ba1\u6837\u672c\u81ea\u9002\u5e94\u6846\u67b6SkipVAR\u3002", "result": "SkipVAR\u5728GenEval\u57fa\u51c6\u4e0a\u5b9e\u73b01.81\u500d\u52a0\u901f\u548c2.62\u500d\u901f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u3002", "conclusion": "\u9891\u7387\u611f\u77e5\u7684\u65e0\u8bad\u7ec3\u81ea\u9002\u5e94\u52a0\u901f\u65b9\u6cd5\u5bf9\u53ef\u6269\u5c55\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6709\u6548\u3002"}}
{"id": "2506.09027", "pdf": "https://arxiv.org/pdf/2506.09027", "abs": "https://arxiv.org/abs/2506.09027", "authors": ["Runqian Wang", "Kaiming He"], "title": "Diffuse and Disperse: Image Generation with Representation Regularization", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The development of diffusion-based generative models over the past decade has largely proceeded independently of progress in representation learning. These diffusion models typically rely on regression-based objectives and generally lack explicit regularization. In this work, we propose \\textit{Dispersive Loss}, a simple plug-and-play regularizer that effectively improves diffusion-based generative models. Our loss function encourages internal representations to disperse in the hidden space, analogous to contrastive self-supervised learning, with the key distinction that it requires no positive sample pairs and therefore does not interfere with the sampling process used for regression. Compared to the recent method of representation alignment (REPA), our approach is self-contained and minimalist, requiring no pre-training, no additional parameters, and no external data. We evaluate Dispersive Loss on the ImageNet dataset across a range of models and report consistent improvements over widely used and strong baselines. We hope our work will help bridge the gap between generative modeling and representation learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDispersive Loss\u7684\u7b80\u5355\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u6269\u6563\u751f\u6210\u6a21\u578b\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u6216\u9884\u8bad\u7ec3\u3002", "motivation": "\u6269\u6563\u751f\u6210\u6a21\u578b\u901a\u5e38\u7f3a\u4e4f\u663e\u5f0f\u6b63\u5219\u5316\uff0c\u4e14\u4e0e\u8868\u793a\u5b66\u4e60\u8fdb\u5c55\u8131\u8282\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4e00\u79cd\u7b80\u5355\u7684\u65b9\u6cd5\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u63d0\u51faDispersive Loss\uff0c\u4e00\u79cd\u9f13\u52b1\u9690\u7a7a\u95f4\u8868\u793a\u5206\u6563\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u65e0\u9700\u6b63\u6837\u672c\u5bf9\uff0c\u4e0d\u5f71\u54cd\u56de\u5f52\u91c7\u6837\u8fc7\u7a0b\u3002", "result": "\u5728ImageNet\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cDispersive Loss\u5728\u591a\u79cd\u6a21\u578b\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Dispersive Loss\u4e3a\u751f\u6210\u6a21\u578b\u4e0e\u8868\u793a\u5b66\u4e60\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2506.09040", "pdf": "https://arxiv.org/pdf/2506.09040", "abs": "https://arxiv.org/abs/2506.09040", "authors": ["Dianyi Wang", "Wei Song", "Yikun Wang", "Siyuan Wang", "Kaicheng Yu", "Zhongyu Wei", "Jiaqi Wang"], "title": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Typical large vision-language models (LVLMs) apply autoregressive supervision solely to textual sequences, without fully incorporating the visual modality into the learning process. This results in three key limitations: (1) an inability to utilize images without accompanying captions, (2) the risk that captions omit critical visual details, and (3) the challenge that certain vision-centric content cannot be adequately conveyed through text. As a result, current LVLMs often prioritize vision-to-language alignment while potentially overlooking fine-grained visual information. While some prior works have explored autoregressive image generation, effectively leveraging autoregressive visual supervision to enhance image understanding remains an open challenge. In this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR), which enables joint learning of visual and textual modalities within a unified autoregressive framework. We show that autoregressively reconstructing the raw visual appearance of images does not enhance and may even impair multimodal understanding. In contrast, autoregressively reconstructing the semantic representation of images consistently improves comprehension. Notably, we find that even when models are given continuous image features as input, they can effectively reconstruct discrete semantic tokens, resulting in stable and consistent improvements across a wide range of multimodal understanding benchmarks. Our approach delivers significant performance gains across varying data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is available at https://github.com/AlenjandroWang/ASVR.", "AI": {"tldr": "ASVR\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u5b66\u4e60\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u7684\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u5efa\u56fe\u50cf\u7684\u8bed\u4e49\u8868\u793a\u800c\u975e\u539f\u59cb\u5916\u89c2\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u7406\u89e3\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ec5\u5bf9\u6587\u672c\u5e8f\u5217\u8fdb\u884c\u81ea\u56de\u5f52\u76d1\u7763\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u89c6\u89c9\u6a21\u6001\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5904\u7406\u65e0\u6807\u6ce8\u56fe\u50cf\u3001\u9057\u6f0f\u89c6\u89c9\u7ec6\u8282\u53ca\u96be\u4ee5\u8868\u8fbe\u89c6\u89c9\u5185\u5bb9\u3002", "method": "ASVR\u5728\u7edf\u4e00\u7684\u81ea\u56de\u5f52\u6846\u67b6\u4e2d\u8054\u5408\u5b66\u4e60\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\uff0c\u901a\u8fc7\u91cd\u5efa\u56fe\u50cf\u7684\u8bed\u4e49\u8868\u793a\u800c\u975e\u539f\u59cb\u5916\u89c2\u3002", "result": "ASVR\u5728\u591a\u79cd\u6570\u636e\u89c4\u6a21\u548cLLM\u9aa8\u5e72\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728LLaVA-1.5\u4e0a\u5e73\u5747\u63d0\u5347\u4e865%\u7684\u591a\u6a21\u6001\u57fa\u51c6\u5206\u6570\u3002", "conclusion": "ASVR\u901a\u8fc7\u8bed\u4e49\u91cd\u5efa\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u81ea\u56de\u5f52\u89c6\u89c9\u76d1\u7763\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.09042", "pdf": "https://arxiv.org/pdf/2506.09042", "abs": "https://arxiv.org/abs/2506.09042", "authors": ["Xuanchi Ren", "Yifan Lu", "Tianshi Cao", "Ruiyuan Gao", "Shengyu Huang", "Amirmojtaba Sabour", "Tianchang Shen", "Tobias Pfaff", "Jay Zhangjie Wu", "Runjian Chen", "Seung Wook Kim", "Jun Gao", "Laura Leal-Taixe", "Mike Chen", "Sanja Fidler", "Huan Ling"], "title": "Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models", "categories": ["cs.CV"], "comment": "Xuanchi Ren, Yifan Lu, Tianshi Cao, Ruiyuan Gao: Equal contribution.   Only the core contributors are listed. The full list of contributors can be   found in Appendix A of this paper", "summary": "Collecting and annotating real-world data for safety-critical physical AI systems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is especially challenging to capture rare edge cases, which play a critical role in training and testing of an AV system. To address this challenge, we introduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline that aims to generate challenging scenarios to facilitate downstream tasks such as perception and driving policy training. Powering this pipeline is Cosmos-Drive, a suite of models specialized from NVIDIA Cosmos world foundation model for the driving domain and are capable of controllable, high-fidelity, multi-view, and spatiotemporally consistent driving video generation. We showcase the utility of these models by applying Cosmos-Drive-Dreams to scale the quantity and diversity of driving datasets with high-fidelity and challenging scenarios. Experimentally, we demonstrate that our generated data helps in mitigating long-tail distribution problems and enhances generalization in downstream tasks such as 3D lane detection, 3D object detection and driving policy learning. We open source our pipeline toolkit, dataset and model weights through the NVIDIA's Cosmos platform.   Project page: https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams", "AI": {"tldr": "Cosmos-Drive-Dreams\u662f\u4e00\u4e2a\u5408\u6210\u6570\u636e\u751f\u6210\uff08SDG\uff09\u7ba1\u9053\uff0c\u65e8\u5728\u751f\u6210\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\uff0c\u4ee5\u652f\u6301\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u611f\u77e5\u548c\u9a7e\u9a76\u7b56\u7565\u8bad\u7ec3\u3002", "motivation": "\u6536\u96c6\u548c\u6807\u6ce8\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff08AV\uff09\u7b49\u5b89\u5168\u5173\u952e\u7269\u7406AI\u7cfb\u7edf\u65e2\u8017\u65f6\u53c8\u6602\u8d35\uff0c\u5c24\u5176\u662f\u96be\u4ee5\u6355\u6349\u5bf9\u8bad\u7ec3\u548c\u6d4b\u8bd5\u81f3\u5173\u91cd\u8981\u7684\u7f55\u89c1\u8fb9\u7f18\u6848\u4f8b\u3002", "method": "\u5229\u7528Cosmos-Drive\uff08\u57fa\u4e8eNVIDIA Cosmos\u4e16\u754c\u57fa\u7840\u6a21\u578b\u7684\u4e13\u7528\u5957\u4ef6\uff09\u751f\u6210\u53ef\u63a7\u3001\u9ad8\u4fdd\u771f\u3001\u591a\u89c6\u89d2\u4e14\u65f6\u7a7a\u4e00\u81f4\u7684\u9a7e\u9a76\u89c6\u9891\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u751f\u6210\u7684\u6570\u636e\u6709\u52a9\u4e8e\u7f13\u89e3\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\uff08\u59823D\u8f66\u9053\u68c0\u6d4b\u30013D\u7269\u4f53\u68c0\u6d4b\u548c\u9a7e\u9a76\u7b56\u7565\u5b66\u4e60\uff09\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Cosmos-Drive-Dreams\u901a\u8fc7\u5f00\u6e90\u5de5\u5177\u5305\u3001\u6570\u636e\u96c6\u548c\u6a21\u578b\u6743\u91cd\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6570\u636e\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09045", "pdf": "https://arxiv.org/pdf/2506.09045", "abs": "https://arxiv.org/abs/2506.09045", "authors": ["Zehong Ma", "Longhui Wei", "Feng Wang", "Shiliang Zhang", "Qi Tian"], "title": "MagCache: Fast Video Generation with Magnitude-Aware Cache", "categories": ["cs.CV"], "comment": "Project Page: https://zehong-ma.github.io/MagCache", "summary": "Existing acceleration techniques for video diffusion models often rely on uniform heuristics or time-embedding variants to skip timesteps and reuse cached features. These approaches typically require extensive calibration with curated prompts and risk inconsistent outputs due to prompt-specific overfitting. In this paper, we introduce a novel and robust discovery: a unified magnitude law observed across different models and prompts. Specifically, the magnitude ratio of successive residual outputs decreases monotonically and steadily in most timesteps while rapidly in the last several steps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache) that adaptively skips unimportant timesteps using an error modeling mechanism and adaptive caching strategy. Unlike existing methods requiring dozens of curated samples for calibration, MagCache only requires a single sample for calibration. Experimental results show that MagCache achieves 2.1x and 2.68x speedups on Open-Sora and Wan 2.1, respectively, while preserving superior visual fidelity. It significantly outperforms existing methods in LPIPS, SSIM, and PSNR, under comparable computational budgets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7edf\u4e00\u5e45\u5ea6\u89c4\u5f8b\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u52a0\u901f\u65b9\u6cd5MagCache\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8df3\u8fc7\u4e0d\u91cd\u8981\u65f6\u95f4\u6b65\u548c\u7f13\u5b58\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u901f\u5ea6\u5e76\u4fdd\u6301\u4e86\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6269\u6563\u6a21\u578b\u52a0\u901f\u6280\u672f\u901a\u5e38\u4f9d\u8d56\u7edf\u4e00\u542f\u53d1\u5f0f\u65b9\u6cd5\u6216\u65f6\u95f4\u5d4c\u5165\u53d8\u4f53\uff0c\u5bb9\u6613\u56e0\u63d0\u793a\u7279\u5b9a\u8fc7\u62df\u5408\u5bfc\u81f4\u8f93\u51fa\u4e0d\u4e00\u81f4\uff0c\u4e14\u9700\u8981\u5927\u91cf\u6821\u51c6\u6837\u672c\u3002", "method": "\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u548c\u63d0\u793a\u4e2d\u7edf\u4e00\u7684\u5e45\u5ea6\u89c4\u5f8b\uff0c\u63d0\u51faMagnitude-aware Cache\uff08MagCache\uff09\uff0c\u901a\u8fc7\u8bef\u5dee\u5efa\u6a21\u673a\u5236\u548c\u81ea\u9002\u5e94\u7f13\u5b58\u7b56\u7565\u8df3\u8fc7\u4e0d\u91cd\u8981\u65f6\u95f4\u6b65\u3002", "result": "MagCache\u5728Open-Sora\u548cWan 2.1\u4e0a\u5206\u522b\u5b9e\u73b02.1\u500d\u548c2.68\u500d\u52a0\u901f\uff0c\u4e14\u5728LPIPS\u3001SSIM\u548cPSNR\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MagCache\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u52a0\u901f\u65b9\u6cd5\uff0c\u4ec5\u9700\u5355\u4e00\u6837\u672c\u6821\u51c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2506.08443", "pdf": "https://arxiv.org/pdf/2506.08443", "abs": "https://arxiv.org/abs/2506.08443", "authors": ["Kazuki Kawamura", "Jun Rekimoto"], "title": "SakugaFlow: A Stagewise Illustration Framework Emulating the Human Drawing Process and Providing Interactive Tutoring for Novice Drawing Skills", "categories": ["cs.HC", "cs.CV", "68T05", "H.5.2; K.3; I.2.7"], "comment": "5 pages, 1 figure; accepted as a paper to the Generative AI and HCI   (GenAICHI) workshop at CHI 2025 (Yokohama, 27 Apr 2025)", "summary": "While current AI illustration tools can generate high-quality images from text prompts, they rarely reveal the step-by-step procedure that human artists follow. We present SakugaFlow, a four-stage pipeline that pairs diffusion-based image generation with a large-language-model tutor. At each stage, novices receive real-time feedback on anatomy, perspective, and composition, revise any step non-linearly, and branch alternative versions. By exposing intermediate outputs and embedding pedagogical dialogue, SakugaFlow turns a black-box generator into a scaffolded learning environment that supports both creative exploration and skills acquisition.", "AI": {"tldr": "SakugaFlow\u662f\u4e00\u4e2a\u56db\u9636\u6bb5\u6d41\u7a0b\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4e3a\u521d\u5b66\u8005\u63d0\u4f9b\u5b9e\u65f6\u53cd\u9988\uff0c\u652f\u6301\u975e\u7ebf\u6027\u4fee\u6539\u548c\u591a\u7248\u672c\u63a2\u7d22\uff0c\u5c06\u9ed1\u76d2\u751f\u6210\u5668\u8f6c\u5316\u4e3a\u5b66\u4e60\u5de5\u5177\u3002", "motivation": "\u5f53\u524dAI\u7ed8\u56fe\u5de5\u5177\u867d\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u4f46\u7f3a\u4e4f\u4eba\u7c7b\u827a\u672f\u5bb6\u9010\u6b65\u521b\u4f5c\u7684\u8fc7\u7a0b\uff0c\u65e0\u6cd5\u63d0\u4f9b\u5b66\u4e60\u652f\u6301\u3002", "method": "\u91c7\u7528\u56db\u9636\u6bb5\u6d41\u7a0b\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u4f9b\u5b9e\u65f6\u53cd\u9988\u3001\u975e\u7ebf\u6027\u4fee\u6539\u548c\u591a\u7248\u672c\u5206\u652f\u529f\u80fd\u3002", "result": "SakugaFlow\u901a\u8fc7\u5c55\u793a\u4e2d\u95f4\u8f93\u51fa\u548c\u5d4c\u5165\u6559\u5b66\u5bf9\u8bdd\uff0c\u652f\u6301\u521b\u610f\u63a2\u7d22\u548c\u6280\u80fd\u5b66\u4e60\u3002", "conclusion": "SakugaFlow\u6210\u529f\u5c06\u9ed1\u76d2\u751f\u6210\u5668\u8f6c\u5316\u4e3a\u652f\u6301\u5b66\u4e60\u548c\u521b\u4f5c\u7684\u5de5\u5177\u3002"}}
{"id": "2506.08480", "pdf": "https://arxiv.org/pdf/2506.08480", "abs": "https://arxiv.org/abs/2506.08480", "authors": ["Huixuan Zhang", "Xiaojun Wan"], "title": "Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Text-to-image models often struggle to generate images that precisely match textual prompts. Prior research has extensively studied the evaluation of image-text alignment in text-to-image generation. However, existing evaluations primarily focus on agreement with human assessments, neglecting other critical properties of a trustworthy evaluation framework. In this work, we first identify two key aspects that a reliable evaluation should address. We then empirically demonstrate that current mainstream evaluation frameworks fail to fully satisfy these properties across a diverse range of metrics and models. Finally, we propose recommendations for improving image-text alignment evaluation.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u8bc4\u4f30\u6846\u67b6\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027\uff0c\u5ffd\u7565\u4e86\u8bc4\u4f30\u6846\u67b6\u7684\u5176\u4ed6\u5173\u952e\u7279\u6027\u3002", "method": "\u8bc6\u522b\u53ef\u9760\u8bc4\u4f30\u7684\u4e24\u4e2a\u5173\u952e\u65b9\u9762\uff0c\u5b9e\u8bc1\u5206\u6790\u73b0\u6709\u6846\u67b6\u7684\u4e0d\u8db3\u3002", "result": "\u4e3b\u6d41\u8bc4\u4f30\u6846\u67b6\u672a\u80fd\u6ee1\u8db3\u8fd9\u4e9b\u7279\u6027\u3002", "conclusion": "\u63d0\u51fa\u6539\u8fdb\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u8bc4\u4f30\u7684\u5efa\u8bae\u3002"}}
{"id": "2506.08520", "pdf": "https://arxiv.org/pdf/2506.08520", "abs": "https://arxiv.org/abs/2506.08520", "authors": ["Srinivasan Kidambi", "Pravin Nair"], "title": "Plug-and-Play Linear Attention for Pre-trained Image and Video Restoration Models", "categories": ["eess.IV", "cs.CV"], "comment": "6 pages, 1 pseudo-code, 3 figure panels, 2 plot panels, 7 tables, 24   references", "summary": "Multi-head self-attention (MHSA) has become a core component in modern computer vision models. However, its quadratic complexity with respect to input length poses a significant computational bottleneck in real-time and resource constrained environments. We propose PnP-Nystra, a Nystr\\\"om based linear approximation of self-attention, developed as a plug-and-play (PnP) module that can be integrated into the pre-trained image and video restoration models without retraining. As a drop-in replacement for MHSA, PnP-Nystra enables efficient acceleration in various window-based transformer architectures, including SwinIR, Uformer, and RVRT. Our experiments across diverse image and video restoration tasks, including denoising, deblurring, and super-resolution, demonstrate that PnP-Nystra achieves a 2-4x speed-up on an NVIDIA RTX 4090 GPU and a 2-5x speed-up on CPU inference. Despite these significant gains, the method incurs a maximum PSNR drop of only 1.5 dB across all evaluated tasks. To the best of our knowledge, we are the first to demonstrate a linear attention functioning as a training-free substitute for MHSA in restoration models.", "AI": {"tldr": "PnP-Nystra\u662f\u4e00\u79cd\u57fa\u4e8eNystr\u00f6m\u7684\u7ebf\u6027\u81ea\u6ce8\u610f\u529b\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u53ef\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u52a0\u901f\u9884\u8bad\u7ec3\u7684\u56fe\u50cf\u548c\u89c6\u9891\u4fee\u590d\u6a21\u578b\u3002", "motivation": "\u591a\u5934\u81ea\u6ce8\u610f\u529b\uff08MHSA\uff09\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u5728\u5b9e\u65f6\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u6210\u4e3a\u8ba1\u7b97\u74f6\u9888\uff0c\u9700\u8981\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51faPnP-Nystra\uff0c\u4f5c\u4e3aMHSA\u7684\u7ebf\u6027\u8fd1\u4f3c\u66ff\u4ee3\u6a21\u5757\uff0c\u9002\u7528\u4e8eSwinIR\u3001Uformer\u7b49\u7a97\u53e3\u5f0fTransformer\u67b6\u6784\u3002", "result": "\u5728\u56fe\u50cf\u548c\u89c6\u9891\u4fee\u590d\u4efb\u52a1\u4e2d\uff0cPnP-Nystra\u5728GPU\u548cCPU\u4e0a\u5206\u522b\u5b9e\u73b02-4\u500d\u548c2-5\u500d\u52a0\u901f\uff0cPSNR\u6700\u5927\u4ec5\u4e0b\u964d1.5 dB\u3002", "conclusion": "PnP-Nystra\u9996\u6b21\u5c55\u793a\u4e86\u7ebf\u6027\u6ce8\u610f\u529b\u53ef\u4f5c\u4e3a\u65e0\u9700\u8bad\u7ec3\u7684MHSA\u66ff\u4ee3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u4e14\u6027\u80fd\u635f\u5931\u6781\u5c0f\u3002"}}
{"id": "2506.08677", "pdf": "https://arxiv.org/pdf/2506.08677", "abs": "https://arxiv.org/abs/2506.08677", "authors": ["Milica \u0160kipina", "Nikola Jovi\u0161i\u0107", "Nicola Dall'Asen", "Vanja \u0160venda", "Anil Osman Tur", "Slobodan Ili\u0107", "Elisa Ricci", "Dubravko \u0106ulibrk"], "title": "MAMBO: High-Resolution Generative Approach for Mammography Images", "categories": ["eess.IV", "cs.CV"], "comment": "21 pages, 14 figures, 7 tables", "summary": "Mammography is the gold standard for the detection and diagnosis of breast cancer. This procedure can be significantly enhanced with Artificial Intelligence (AI)-based software, which assists radiologists in identifying abnormalities. However, training AI systems requires large and diverse datasets, which are often difficult to obtain due to privacy and ethical constraints. To address this issue, the paper introduces MAMmography ensemBle mOdel (MAMBO), a novel patch-based diffusion approach designed to generate full-resolution mammograms. Diffusion models have shown breakthrough results in realistic image generation, yet few studies have focused on mammograms, and none have successfully generated high-resolution outputs required to capture fine-grained features of small lesions. To achieve this, MAMBO integrates separate diffusion models to capture both local and global (image-level) contexts. The contextual information is then fed into the final patch-based model, significantly aiding the noise removal process. This thoughtful design enables MAMBO to generate highly realistic mammograms of up to 3840x3840 pixels. Importantly, this approach can be used to enhance the training of classification models and extended to anomaly detection. Experiments, both numerical and radiologist validation, assess MAMBO's capabilities in image generation, super-resolution, and anomaly detection, highlighting its potential to enhance mammography analysis for more accurate diagnoses and earlier lesion detection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMAMBO\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u4e73\u817aX\u5149\u7247\uff0c\u4ee5\u89e3\u51b3\u8bad\u7ec3AI\u7cfb\u7edf\u65f6\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u9690\u79c1\u548c\u4f26\u7406\u9650\u5236\uff0c\u83b7\u53d6\u5927\u89c4\u6a21\u591a\u6837\u5316\u7684\u4e73\u817aX\u5149\u6570\u636e\u96c6\u56f0\u96be\uff0c\u5f71\u54cd\u4e86AI\u7cfb\u7edf\u7684\u8bad\u7ec3\u6548\u679c\u3002", "method": "MAMBO\u91c7\u7528\u57fa\u4e8e\u5757\u7684\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u751f\u6210\u9ad8\u8fbe3840x3840\u50cf\u7d20\u7684\u9ad8\u5206\u8fa8\u7387\u4e73\u817aX\u5149\u7247\u3002", "result": "MAMBO\u80fd\u591f\u751f\u6210\u9ad8\u5ea6\u903c\u771f\u7684\u4e73\u817aX\u5149\u7247\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u56fe\u50cf\u751f\u6210\u3001\u8d85\u5206\u8fa8\u7387\u548c\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "MAMBO\u6709\u671b\u63d0\u5347\u4e73\u817aX\u5149\u5206\u6790\u7684\u51c6\u786e\u6027\uff0c\u5e2e\u52a9\u66f4\u65e9\u53d1\u73b0\u75c5\u53d8\u3002"}}
{"id": "2506.08716", "pdf": "https://arxiv.org/pdf/2506.08716", "abs": "https://arxiv.org/abs/2506.08716", "authors": ["Maximilian Tschuchnig", "Lukas Lamminger", "Philipp Steininger", "Michael Gadermayr"], "title": "Enhancing Synthetic CT from CBCT via Multimodal Fusion: A Study on the Impact of CBCT Quality and Alignment", "categories": ["eess.IV", "cs.CV"], "comment": "Data is open source. Code will be provided on acceptance. Paper   currently under review", "summary": "Cone-Beam Computed Tomography (CBCT) is widely used for real-time intraoperative imaging due to its low radiation dose and high acquisition speed. However, despite its high resolution, CBCT suffers from significant artifacts and thereby lower visual quality, compared to conventional Computed Tomography (CT). A recent approach to mitigate these artifacts is synthetic CT (sCT) generation, translating CBCT volumes into the CT domain. In this work, we enhance sCT generation through multimodal learning, integrating intraoperative CBCT with preoperative CT. Beyond validation on two real-world datasets, we use a versatile synthetic dataset, to analyze how CBCT-CT alignment and CBCT quality affect sCT quality. The results demonstrate that multimodal sCT consistently outperform unimodal baselines, with the most significant gains observed in well-aligned, low-quality CBCT-CT cases. Finally, we demonstrate that these findings are highly reproducible in real-world clinical datasets.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u5347\u5408\u6210CT\uff08sCT\uff09\u751f\u6210\u8d28\u91cf\uff0c\u7ed3\u5408\u672f\u4e2dCBCT\u548c\u672f\u524dCT\u6570\u636e\uff0c\u9a8c\u8bc1\u5176\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u6548\u679c\u3002", "motivation": "CBCT\u867d\u5e7f\u6cdb\u7528\u4e8e\u672f\u4e2d\u6210\u50cf\uff0c\u4f46\u5b58\u5728\u4f2a\u5f71\u548c\u89c6\u89c9\u8d28\u91cf\u4f4e\u7684\u95ee\u9898\uff0c\u9700\u901a\u8fc7sCT\u751f\u6210\u6539\u5584\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\uff0c\u6574\u5408CBCT\u548cCT\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u5206\u6790\u5bf9\u9f50\u548c\u8d28\u91cf\u5bf9sCT\u7684\u5f71\u54cd\u3002", "result": "\u591a\u6a21\u6001sCT\u8868\u73b0\u4f18\u4e8e\u5355\u6a21\u6001\u57fa\u7ebf\uff0c\u5c24\u5176\u5728\u4f4e\u8d28\u91cfCBCT-CT\u5bf9\u9f50\u60c5\u51b5\u4e0b\u63d0\u5347\u663e\u8457\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5728\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u4e2d\u5177\u6709\u9ad8\u5ea6\u53ef\u91cd\u590d\u6027\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
