<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 26]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Gbake: Baking 3D Gaussian Splats into Reflection Probes](https://arxiv.org/abs/2507.02257)
*Stephen Pasch,Joel K. Salzman,Changxi Zheng*

Main category: cs.GR

TL;DR: GBake工具用于从3D高斯场景中烘焙反射探针，使传统3D网格在Unity中实现真实反射映射。


<details>
  <summary>Details</summary>
Motivation: 解决3D高斯场景中传统网格因光照和几何编码方式不同而显得不协调的问题。

Method: 开发GBake工具，从高斯场景中烘焙反射探针。

Result: 实现了传统3D网格在高斯场景中的真实反射映射。

Conclusion: GBake工具成功解决了传统网格与高斯场景的兼容性问题。

Abstract: The growing popularity of 3D Gaussian Splatting has created the need to integrate traditional computer graphics techniques and assets in splatted environments. Since 3D Gaussian primitives encode lighting and geometry jointly as appearance, meshes are relit improperly when inserted directly in a mixture of 3D Gaussians and thus appear noticeably out of place. We introduce GBake, a specialized tool for baking reflection probes from Gaussian-splatted scenes that enables realistic reflection mapping of traditional 3D meshes in the Unity game engine.

</details>


### [2] [Real-time Image-based Lighting of Glints](https://arxiv.org/abs/2507.02674)
*Tom Kneiphof,Reinhard Klein*

Main category: cs.GR

TL;DR: 提出了一种高效的图像照明近似方法，用于实时渲染闪烁或闪亮材料，支持动态材质和环境贴图。


<details>
  <summary>Details</summary>
Motivation: 解决在实时渲染中，离散微面材料（如闪亮表面）在图像照明下的挑战。

Method: 基于区域光照明下的实时闪烁渲染，采用环境贴图滤波技术，快速处理每帧数据。假设环境贴图分为少量均匀区域，通过正态分布函数滤波计算微面反射概率，并利用双门高斯近似进行分层采样。

Result: 验证了该方法在多种材质和光照条件下接近真实渲染效果，性能稳定且开销低。

Conclusion: 该方法在实时渲染中高效且接近真实，仅需两倍内存存储预滤波环境贴图。

Abstract: Image-based lighting is a widely used technique to reproduce shading under real-world lighting conditions, especially in real-time rendering applications. A particularly challenging scenario involves materials exhibiting a sparkling or glittering appearance, caused by discrete microfacets scattered across their surface. In this paper, we propose an efficient approximation for image-based lighting of glints, enabling fully dynamic material properties and environment maps. Our novel approach is grounded in real-time glint rendering under area light illumination and employs standard environment map filtering techniques. Crucially, our environment map filtering process is sufficiently fast to be executed on a per-frame basis. Our method assumes that the environment map is partitioned into few homogeneous regions of constant radiance. By filtering the corresponding indicator functions with the normal distribution function, we obtain the probabilities for individual microfacets to reflect light from each region. During shading, these probabilities are utilized to hierarchically sample a multinomial distribution, facilitated by our novel dual-gated Gaussian approximation of binomial distributions. We validate that our real-time approximation is close to ground-truth renderings for a range of material properties and lighting conditions, and demonstrate robust and stable performance, with little overhead over rendering glints from a single directional light. Compared to rendering smooth materials without glints, our approach requires twice as much memory to store the prefiltered environment map.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [3] [HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars](https://arxiv.org/abs/2507.02803)
*Gent Serifi,Marcel C. Bühler*

Main category: cs.CV

TL;DR: HyperGaussians是一种基于3D高斯泼溅的新方法，用于高质量可动画人脸化身，解决了非线性变形和复杂光照等问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从单目视频创建可动画人脸化身时仍存在细节不足和效果不自然的问题，特别是在非线性变形和复杂光照条件下。

Method: 通过将3D高斯扩展到高维多变量高斯（HyperGaussians），并结合可学习的局部嵌入和‘逆协方差技巧’提高计算效率。

Result: 在4个数据集上的19个受试者测试中，HyperGaussians在数值和视觉上均优于3DGS，尤其在眼镜框、牙齿等高频细节上表现更优。

Conclusion: HyperGaussians为高质量可动画人脸化身提供了一种更高效的表达方式，显著提升了细节和动态效果。

Abstract: We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for high-quality animatable face avatars. Creating such detailed face avatars from videos is a challenging problem and has numerous applications in augmented and virtual reality. While tremendous successes have been achieved for static faces, animatable avatars from monocular videos still fall in the uncanny valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face through a collection of 3D Gaussian primitives. 3DGS excels at rendering static faces, but the state-of-the-art still struggles with nonlinear deformations, complex lighting effects, and fine details. While most related works focus on predicting better Gaussian parameters from expression codes, we rethink the 3D Gaussian representation itself and how to make it more expressive. Our insights lead to a novel extension of 3D Gaussians to high-dimensional multivariate Gaussians, dubbed 'HyperGaussians'. The higher dimensionality increases expressivity through conditioning on a learnable local embedding. However, splatting HyperGaussians is computationally expensive because it requires inverting a high-dimensional covariance matrix. We solve this by reparameterizing the covariance matrix, dubbed the 'inverse covariance trick'. This trick boosts the efficiency so that HyperGaussians can be seamlessly integrated into existing models. To demonstrate this, we plug in HyperGaussians into the state-of-the-art in fast monocular face avatars: FlashAvatar. Our evaluation on 19 subjects from 4 face datasets shows that HyperGaussians outperform 3DGS numerically and visually, particularly for high-frequency details like eyeglass frames, teeth, complex facial movements, and specular reflections.

</details>


### [4] [LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans](https://arxiv.org/abs/2507.02861)
*Zhening Huang,Xiaoyang Wu,Fangcheng Zhong,Hengshuang Zhao,Matthias Nießner,Joan Lasenby*

Main category: cs.CV

TL;DR: LiteReality是一个将RGB-D扫描转换为紧凑、逼真且交互式3D虚拟副本的管道，支持高质量渲染和物理交互。


<details>
  <summary>Details</summary>
Motivation: 解决将真实室内环境快速转换为高质量、可交互3D虚拟副本的需求，适用于AR/VR、游戏、机器人等领域。

Method: 通过场景理解、3D模型检索、材质增强和物理模拟，构建紧凑且可编辑的3D场景。

Result: 在Scan2CAD基准测试中达到最先进的相似性性能，并能处理复杂光照和遮挡情况。

Conclusion: LiteReality生成的场景适用于多种应用，并展示了高效性和鲁棒性。

Abstract: We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor environments into compact, realistic, and interactive 3D virtual replicas. LiteReality not only reconstructs scenes that visually resemble reality but also supports key features essential for graphics pipelines -- such as object individuality, articulation, high-quality physically based rendering materials, and physically based interaction. At its core, LiteReality first performs scene understanding and parses the results into a coherent 3D layout and objects with the help of a structured scene graph. It then reconstructs the scene by retrieving the most visually similar 3D artist-crafted models from a curated asset database. Next, the Material Painting module enhances realism by recovering high-quality, spatially varying materials. Finally, the reconstructed scene is integrated into a simulation engine with basic physical properties to enable interactive behavior. The resulting scenes are compact, editable, and fully compatible with standard graphics pipelines, making them suitable for applications in AR/VR, gaming, robotics, and digital twins. In addition, LiteReality introduces a training-free object retrieval module that achieves state-of-the-art similarity performance on the Scan2CAD benchmark, along with a robust material painting module capable of transferring appearances from images of any style to 3D assets -- even under severe misalignment, occlusion, and poor lighting. We demonstrate the effectiveness of LiteReality on both real-life scans and public datasets. Project page: https://litereality.github.io; Video: https://www.youtube.com/watch?v=ecK9m3LXg2c

</details>


### [5] [SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement](https://arxiv.org/abs/2507.02252)
*Zeyu Lei,Hongyuan Yu,Jinlin Wu,Zhen Chen*

Main category: cs.CV

TL;DR: SurgVisAgent是一种基于多模态大语言模型（MLLMs）的智能手术视觉代理，能够动态识别内窥镜图像中的失真类别和严重程度，执行多种增强任务，并优于传统单任务模型。


<details>
  <summary>Details</summary>
Motivation: 现有手术增强算法通常针对单一任务设计，难以应对复杂现实场景的多样化需求。

Method: 提出SurgVisAgent，结合先验模型和上下文少样本学习及链式推理（CoT），实现多任务图像增强。

Result: 在模拟真实手术失真的基准测试中，SurgVisAgent表现优于传统单任务模型。

Conclusion: SurgVisAgent有望成为手术辅助的统一解决方案。

Abstract: Precise surgical interventions are vital to patient safety, and advanced enhancement algorithms have been developed to assist surgeons in decision-making. Despite significant progress, these algorithms are typically designed for single tasks in specific scenarios, limiting their effectiveness in complex real-world situations. To address this limitation, we propose SurgVisAgent, an end-to-end intelligent surgical vision agent built on multimodal large language models (MLLMs). SurgVisAgent dynamically identifies distortion categories and severity levels in endoscopic images, enabling it to perform a variety of enhancement tasks such as low-light enhancement, overexposure correction, motion blur elimination, and smoke removal. Specifically, to achieve superior surgical scenario understanding, we design a prior model that provides domain-specific knowledge. Additionally, through in-context few-shot learning and chain-of-thought (CoT) reasoning, SurgVisAgent delivers customized image enhancements tailored to a wide range of distortion types and severity levels, thereby addressing the diverse requirements of surgeons. Furthermore, we construct a comprehensive benchmark simulating real-world surgical distortions, on which extensive experiments demonstrate that SurgVisAgent surpasses traditional single-task models, highlighting its potential as a unified solution for surgical assistance.

</details>


### [6] [MAC-Lookup: Multi-Axis Conditional Lookup Model for Underwater Image Enhancement](https://arxiv.org/abs/2507.02270)
*Fanghai Yi,Zehong Zheng,Zexiao Liang,Yihang Dong,Xiyang Fang,Wangyu Wu,Xuhang Chen*

Main category: cs.CV

TL;DR: MAC-Lookup模型通过CLTCC和MAAE模块提升水下图像质量，解决了传统方法和深度学习的不足。


<details>
  <summary>Details</summary>
Motivation: 水下图像因光线、浑浊度和气泡等问题导致视觉质量差，现有方法效果不佳。

Method: 结合CLTCC进行初步颜色校正和MAAE进行细节优化，避免过增强和饱和。

Result: 实验表明MAC-Lookup在恢复细节和颜色方面优于现有方法。

Conclusion: MAC-Lookup有效提升水下图像质量，代码已开源。

Abstract: Enhancing underwater images is crucial for exploration. These images face visibility and color issues due to light changes, water turbidity, and bubbles. Traditional prior-based methods and pixel-based methods often fail, while deep learning lacks sufficient high-quality datasets. We introduce the Multi-Axis Conditional Lookup (MAC-Lookup) model, which enhances visual quality by improving color accuracy, sharpness, and contrast. It includes Conditional 3D Lookup Table Color Correction (CLTCC) for preliminary color and quality correction and Multi-Axis Adaptive Enhancement (MAAE) for detail refinement. This model prevents over-enhancement and saturation while handling underwater challenges. Extensive experiments show that MAC-Lookup excels in enhancing underwater images by restoring details and colors better than existing methods. The code is https://github.com/onlycatdoraemon/MAC-Lookup.

</details>


### [7] [MAGIC: Mask-Guided Diffusion Inpainting with Multi-Level Perturbations and Context-Aware Alignment for Few-Shot Anomaly Generation](https://arxiv.org/abs/2507.02314)
*JaeHyuck Choi,MinJun Kim,JeHyeong Hong*

Main category: cs.CV

TL;DR: MAGIC提出了一种基于扩散模型的少样本异常生成方法，解决了背景保持、异常区域精准填充和语义合理性的问题。


<details>
  <summary>Details</summary>
Motivation: 工业质量控制中异常数据稀缺，现有方法无法同时满足背景保持、精准填充和语义合理性。

Method: MAGIC基于Stable Diffusion框架，结合高斯提示级扰动和空间噪声注入，增强异常多样性，并通过上下文感知对齐模块优化语义合理性。

Result: 在MVTec-AD数据集上，MAGIC在异常任务中表现优于现有方法。

Conclusion: MAGIC是一种高效的少样本异常生成方法，解决了现有技术的局限性。

Abstract: Few-shot anomaly generation is emerging as a practical solution for augmenting the scarce anomaly data in industrial quality control settings. An ideal generator would meet three demands at once, namely (i) keep the normal background intact, (ii) inpaint anomalous regions to tightly overlap with the corresponding anomaly masks, and (iii) generate anomalous regions in a semantically valid location, while still producing realistic, diverse appearances from only a handful of real examples. Existing diffusion-based methods usually satisfy at most two of these requirements: global anomaly generators corrupt the background, whereas mask-guided ones often falter when the mask is imprecise or misplaced. We propose MAGIC--Mask-guided inpainting with multi-level perturbations and Context-aware alignment--to resolve all three issues. At its core, MAGIC fine-tunes a Stable Diffusion inpainting backbone that preserves normal regions and ensures strict adherence of the synthesized anomaly to the supplied mask, directly addressing background corruption and misalignment. To offset the diversity loss that fine-tuning can cause, MAGIC adds two complementary perturbation strategies: (i) Gaussian prompt-level perturbation applied during fine-tuning and inference that broadens the global appearance of anomalies while avoiding low-fidelity textual appearances, and (ii) mask-guided spatial noise injection that enriches local texture variations. Additionally, the context-aware mask alignment module forms semantic correspondences and relocates masks so that every anomaly remains plausibly contained within the host object, eliminating out-of-boundary artifacts. Under a consistent identical evaluation protocol on the MVTec-AD dataset, MAGIC outperforms previous state-of-the-arts in downstream anomaly tasks.

</details>


### [8] [Are Synthetic Videos Useful? A Benchmark for Retrieval-Centric Evaluation of Synthetic Videos](https://arxiv.org/abs/2507.02316)
*Zecheng Zhao,Selena Song,Tong Chen,Zhi Chen,Shazia Sadiq,Yadan Luo*

Main category: cs.CV

TL;DR: SynTVA是一个新的数据集和基准，用于评估合成视频在文本到视频检索（TVR）任务中的实用性，通过四个关键语义对齐维度标注视频-文本对，并探索其与下游任务性能的关系。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频合成的评估指标主要关注视觉质量和时间一致性，缺乏对合成视频在下游任务（如TVR）中表现的评价。

Method: 基于800个多样化的用户查询生成合成视频，标注四个语义对齐维度，并开发自动评估器以估计对齐质量。

Result: SynTVA不仅作为基准工具，还能通过选择高质量合成样本显著提升TVR性能。

Conclusion: SynTVA为合成视频的评估和数据集增强提供了有价值的资源。

Abstract: Text-to-video (T2V) synthesis has advanced rapidly, yet current evaluation metrics primarily capture visual quality and temporal consistency, offering limited insight into how synthetic videos perform in downstream tasks such as text-to-video retrieval (TVR). In this work, we introduce SynTVA, a new dataset and benchmark designed to evaluate the utility of synthetic videos for building retrieval models. Based on 800 diverse user queries derived from MSRVTT training split, we generate synthetic videos using state-of-the-art T2V models and annotate each video-text pair along four key semantic alignment dimensions: Object \& Scene, Action, Attribute, and Prompt Fidelity. Our evaluation framework correlates general video quality assessment (VQA) metrics with these alignment scores, and examines their predictive power for downstream TVR performance. To explore pathways of scaling up, we further develop an Auto-Evaluator to estimate alignment quality from existing metrics. Beyond benchmarking, our results show that SynTVA is a valuable asset for dataset augmentation, enabling the selection of high-utility synthetic samples that measurably improve TVR outcomes. Project page and dataset can be found at https://jasoncodemaker.github.io/SynTVA/.

</details>


### [9] [Heeding the Inner Voice: Aligning ControlNet Training via Intermediate Features Feedback](https://arxiv.org/abs/2507.02321)
*Nina Konovalova,Maxim Nikolaev,Andrey Kuznetsov,Aibek Alanov*

Main category: cs.CV

TL;DR: InnerControl通过在全扩散步骤中强制执行空间一致性，改进了文本到图像扩散模型的空间控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如ControlNet++）仅在最终去噪步骤中应用循环一致性损失，忽略了中间生成阶段，限制了其效果。

Method: 提出InnerControl，通过训练轻量级卷积探针从中间UNet特征重建输入控制信号，并在整个扩散过程中最小化预测与目标条件之间的差异。

Result: 结合ControlNet++等技术，InnerControl在多种条件方法（如边缘、深度）上实现了最先进的性能。

Conclusion: InnerControl通过全步骤空间一致性优化，显著提升了控制保真度和生成质量。

Abstract: Despite significant progress in text-to-image diffusion models, achieving precise spatial control over generated outputs remains challenging. ControlNet addresses this by introducing an auxiliary conditioning module, while ControlNet++ further refines alignment through a cycle consistency loss applied only to the final denoising steps. However, this approach neglects intermediate generation stages, limiting its effectiveness. We propose InnerControl, a training strategy that enforces spatial consistency across all diffusion steps. Our method trains lightweight convolutional probes to reconstruct input control signals (e.g., edges, depth) from intermediate UNet features at every denoising step. These probes efficiently extract signals even from highly noisy latents, enabling pseudo ground truth controls for training. By minimizing the discrepancy between predicted and target conditions throughout the entire diffusion process, our alignment loss improves both control fidelity and generation quality. Combined with established techniques like ControlNet++, InnerControl achieves state-of-the-art performance across diverse conditioning methods (e.g., edges, depth).

</details>


### [10] [Holistic Tokenizer for Autoregressive Image Generation](https://arxiv.org/abs/2507.02358)
*Anlin Zheng,Haochen Wang,Yucheng Zhao,Weipeng Deng,Tiancai Wang,Xiangyu Zhang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: Hita是一种新型图像分词器，通过全局到局部的分词方案和关键策略改进自回归图像生成，显著提升了训练速度和生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决传统自回归图像生成模型无法捕捉全局关系以及视觉分词器缺乏全局信息的问题。

Method: 提出Hita分词器，结合可学习的全局查询和局部补丁标记，采用序列结构和轻量融合模块优化信息流。

Result: 在ImageNet基准测试中达到2.59 FID和281.9 IS，训练速度更快，并能有效捕捉全局图像属性。

Conclusion: Hita在自回归图像生成中表现优异，同时支持零样本风格迁移和图像修复。

Abstract: The vanilla autoregressive image generation model generates visual tokens in a step-by-step fashion, which limits the ability to capture holistic relationships among token sequences. Moreover, most visual tokenizers map local image patches into latent tokens, leading to limited global information. To address this, we introduce \textit{Hita}, a novel image tokenizer for autoregressive (AR) image generation. It introduces a holistic-to-local tokenization scheme with learnable holistic queries and local patch tokens. Besides, Hita incorporates two key strategies for improved alignment with the AR generation process: 1) it arranges a sequential structure with holistic tokens at the beginning followed by patch-level tokens while using causal attention to maintain awareness of previous tokens; and 2) before feeding the de-quantized tokens into the decoder, Hita adopts a lightweight fusion module to control information flow to prioritize holistic tokens. Extensive experiments show that Hita accelerates the training speed of AR generators and outperforms those trained with vanilla tokenizers, achieving \textbf{2.59 FID} and \textbf{281.9 IS} on the ImageNet benchmark. A detailed analysis of the holistic representation highlights its ability to capture global image properties such as textures, materials, and shapes. Additionally, Hita also demonstrates effectiveness in zero-shot style transfer and image in-painting. The code is available at \href{https://github.com/CVMI-Lab/Hita}{https://github.com/CVMI-Lab/Hita}

</details>


### [11] [LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local Implicit Feature Decoupling](https://arxiv.org/abs/2507.02363)
*Jiahao Wu,Rui Peng,Jianbo Jiao,Jiayu Yang,Luyang Tang,Kaiqiang Xiong,Jie Liang,Jinbo Yan,Runling Liu,Ronggang Wang*

Main category: cs.CV

TL;DR: LocalDyGS提出了一种动态场景重建框架，通过分解复杂动态场景为局部空间并解耦静态与动态特征，实现了对大规模和精细尺度运动场景的建模。


<details>
  <summary>Details</summary>
Motivation: 现实世界中复杂且高度动态的运动使得从多视角输入合成任意视角的动态视频具有挑战性。现有方法（如神经辐射场或3D高斯泼溅）难以建模精细尺度运动，限制了应用范围。

Method: 1) 将复杂动态场景分解为由种子定义的局部空间，通过捕获每个局部空间内的运动实现全局建模。2) 解耦静态和动态特征：静态特征捕捉静态信息，动态残差场提供时间特定特征，结合后生成时间高斯以建模局部空间内的运动。

Result: LocalDyGS在精细尺度数据集上表现优异，且首次尝试建模更大更复杂的高度动态场景。

Conclusion: LocalDyGS为动态场景重建提供了一种新框架，能够更真实地建模高度动态的现实场景。

Abstract: Due to the complex and highly dynamic motions in the real world, synthesizing dynamic videos from multi-view inputs for arbitrary viewpoints is challenging. Previous works based on neural radiance field or 3D Gaussian splatting are limited to modeling fine-scale motion, greatly restricting their application. In this paper, we introduce LocalDyGS, which consists of two parts to adapt our method to both large-scale and fine-scale motion scenes: 1) We decompose a complex dynamic scene into streamlined local spaces defined by seeds, enabling global modeling by capturing motion within each local space. 2) We decouple static and dynamic features for local space motion modeling. A static feature shared across time steps captures static information, while a dynamic residual field provides time-specific features. These are combined and decoded to generate Temporal Gaussians, modeling motion within each local space. As a result, we propose a novel dynamic scene reconstruction framework to model highly dynamic real-world scenes more realistically. Our method not only demonstrates competitive performance on various fine-scale datasets compared to state-of-the-art (SOTA) methods, but also represents the first attempt to model larger and more complex highly dynamic scenes. Project page: https://wujh2001.github.io/LocalDyGS/.

</details>


### [12] [UVLM: Benchmarking Video Language Model for Underwater World Understanding](https://arxiv.org/abs/2507.02373)
*Xizhe Xue,Yang Zhou,Dawei Yan,Ying Li,Haokui Zhang,Rong Xiao*

Main category: cs.CV

TL;DR: 论文提出了UVLM，一个用于水下观察的基准数据集，填补了现有视频语言模型在陆地场景外的空白。


<details>
  <summary>Details</summary>
Motivation: 现有视频语言模型主要关注陆地场景，忽视了水下观察的高需求应用。

Method: 通过结合人类专业知识和AI模型构建数据集，考虑了水下环境的独特挑战、数据多样性、任务多样性，并设计了评估指标。

Result: 实验表明，在UVLM上微调的视频语言模型显著提升了对水下世界的理解，并对现有陆地基准略有改进。

Conclusion: UVLM为水下观察提供了有效的基准，并展示了跨场景应用的潜力。

Abstract: Recently, the remarkable success of large language models (LLMs) has achieved a profound impact on the field of artificial intelligence. Numerous advanced works based on LLMs have been proposed and applied in various scenarios. Among them, video language models (VidLMs) are particularly widely used. However, existing works primarily focus on terrestrial scenarios, overlooking the highly demanding application needs of underwater observation. To overcome this gap, we introduce UVLM, an under water observation benchmark which is build through a collaborative approach combining human expertise and AI models. To ensure data quality, we have conducted in-depth considerations from multiple perspectives. First, to address the unique challenges of underwater environments, we selected videos that represent typical underwater challenges including light variations, water turbidity, and diverse viewing angles to construct the dataset. Second, to ensure data diversity, the dataset covers a wide range of frame rates, resolutions, 419 classes of marine animals, and various static plants and terrains. Next, for task diversity, we adopted a structured design where observation targets are categorized into two major classes: biological and environmental. Each category includes content observation and change/action observation, totaling 20 distinct task types. Finally, we designed several challenging evaluation metrics to enable quantitative comparison and analysis of different methods. Experiments on two representative VidLMs demonstrate that fine-tuning VidLMs on UVLM significantly improves underwater world understanding while also showing potential for slight improvements on existing in-air VidLM benchmarks, such as VideoMME and Perception text. The dataset and prompt engineering will be released publicly.

</details>


### [13] [PosDiffAE: Position-aware Diffusion Auto-encoder For High-Resolution Brain Tissue Classification Incorporating Artifact Restoration](https://arxiv.org/abs/2507.02405)
*Ayantika Das,Moitreya Chaudhuri,Koushik Bhat,Keerthi Ram,Mihail Bota,Mohanasankar Sivaprakasam*

Main category: cs.CV

TL;DR: 该论文提出了一种结合扩散模型和自动编码器的方法，用于学习图像特定的语义表示，并应用于脑图像的区域特异性模式识别、撕裂伪影修复和JPEG伪影修复。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然能生成高质量图像，但缺乏提取图像特定语义表示的能力，而自动编码器可以提供这种能力。通过结合两者，论文旨在实现更结构化的潜在空间和图像修复。

Method: 论文提出了一种扩散自动编码模型，通过编码器学习图像特定表示，并利用潜在空间进行脑图像区域特异性模式识别。此外，还设计了基于邻域感知的撕裂伪影修复方法和基于表示引导的JPEG伪影修复方法。

Result: 该方法成功构建了结构化的潜在空间，能够识别脑图像中的组织类型，并实现了无监督的撕裂伪影和JPEG伪影修复。

Conclusion: 通过结合扩散模型和自动编码器，论文提供了一种有效的方法来学习图像特定表示并实现图像修复，扩展了扩散模型的应用范围。

Abstract: Denoising diffusion models produce high-fidelity image samples by capturing the image distribution in a progressive manner while initializing with a simple distribution and compounding the distribution complexity. Although these models have unlocked new applicabilities, the sampling mechanism of diffusion does not offer means to extract image-specific semantic representation, which is inherently provided by auto-encoders. The encoding component of auto-encoders enables mapping between a specific image and its latent space, thereby offering explicit means of enforcing structures in the latent space. By integrating an encoder with the diffusion model, we establish an auto-encoding formulation, which learns image-specific representations and offers means to organize the latent space. In this work, First, we devise a mechanism to structure the latent space of a diffusion auto-encoding model, towards recognizing region-specific cellular patterns in brain images. We enforce the representations to regress positional information of the patches from high-resolution images. This creates a conducive latent space for differentiating tissue types of the brain. Second, we devise an unsupervised tear artifact restoration technique based on neighborhood awareness, utilizing latent representations and the constrained generation capability of diffusion models during inference. Third, through representational guidance and leveraging the inference time steerable noising and denoising capability of diffusion, we devise an unsupervised JPEG artifact restoration technique.

</details>


### [14] [AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars](https://arxiv.org/abs/2507.02419)
*Yiming Zhong,Xiaolin Zhang,Ligang Liu,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: AvatarMakeup是一种基于扩散模型的3D虚拟化妆方法，解决了现有方法在动态表情和多视角下的一致性问题，并实现了精细控制。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯编辑方法无法满足动态表情和多视角下的化妆一致性、身份保持及精细控制需求。

Method: 采用从粗到细的策略，利用预训练扩散模型生成化妆图像作为监督，提出Coherent Duplication方法确保一致性，并通过Refinement Module提升细节。

Result: 实验表明，AvatarMakeup在化妆转移质量和动画一致性上达到最先进水平。

Conclusion: AvatarMakeup为3D虚拟化妆提供了一种高效且一致的方法，解决了现有技术的局限性。

Abstract: Similar to facial beautification in real life, 3D virtual avatars require personalized customization to enhance their visual appeal, yet this area remains insufficiently explored. Although current 3D Gaussian editing methods can be adapted for facial makeup purposes, these methods fail to meet the fundamental requirements for achieving realistic makeup effects: 1) ensuring a consistent appearance during drivable expressions, 2) preserving the identity throughout the makeup process, and 3) enabling precise control over fine details. To address these, we propose a specialized 3D makeup method named AvatarMakeup, leveraging a pretrained diffusion model to transfer makeup patterns from a single reference photo of any individual. We adopt a coarse-to-fine idea to first maintain the consistent appearance and identity, and then to refine the details. In particular, the diffusion model is employed to generate makeup images as supervision. Due to the uncertainties in diffusion process, the generated images are inconsistent across different viewpoints and expressions. Therefore, we propose a Coherent Duplication method to coarsely apply makeup to the target while ensuring consistency across dynamic and multiview effects. Coherent Duplication optimizes a global UV map by recoding the averaged facial attributes among the generated makeup images. By querying the global UV map, it easily synthesizes coherent makeup guidance from arbitrary views and expressions to optimize the target avatar. Given the coarse makeup avatar, we further enhance the makeup by incorporating a Refinement Module into the diffusion model to achieve high makeup quality. Experiments demonstrate that AvatarMakeup achieves state-of-the-art makeup transfer quality and consistency throughout animation.

</details>


### [15] [IGDNet: Zero-Shot Robust Underexposed Image Enhancement via Illumination-Guided and Denoising](https://arxiv.org/abs/2507.02445)
*Hailong Yan,Junjian Huang,Tingwen Huang*

Main category: cs.CV

TL;DR: IGDNet是一种无需训练数据的零样本图像增强方法，通过分解和去噪模块恢复低曝光图像，避免过增强问题，并在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖成对数据集且可能导致过增强，IGDNet旨在解决这些问题，无需先验或训练数据。

Method: IGDNet包含分解模块（分离光照与反射）和去噪模块（光照引导的像素自适应校正），通过下采样生成噪声对并迭代优化。

Result: 在四个公开数据集上，IGDNet显著提升视觉质量，PSNR（20.41dB）和SSIM（0.860dB）优于14种无监督方法。

Conclusion: IGDNet在复杂光照条件下表现优异，具有强泛化能力，代码即将发布。

Abstract: Current methods for restoring underexposed images typically rely on supervised learning with paired underexposed and well-illuminated images. However, collecting such datasets is often impractical in real-world scenarios. Moreover, these methods can lead to over-enhancement, distorting well-illuminated regions. To address these issues, we propose IGDNet, a Zero-Shot enhancement method that operates solely on a single test image, without requiring guiding priors or training data. IGDNet exhibits strong generalization ability and effectively suppresses noise while restoring illumination. The framework comprises a decomposition module and a denoising module. The former separates the image into illumination and reflection components via a dense connection network, while the latter enhances non-uniformly illuminated regions using an illumination-guided pixel adaptive correction method. A noise pair is generated through downsampling and refined iteratively to produce the final result. Extensive experiments on four public datasets demonstrate that IGDNet significantly improves visual quality under complex lighting conditions. Quantitative results on metrics like PSNR (20.41dB) and SSIM (0.860dB) show that it outperforms 14 state-of-the-art unsupervised methods. The code will be released soon.

</details>


### [16] [Reconstructing Close Human Interaction with Appearance and Proxemics Reasoning](https://arxiv.org/abs/2507.02565)
*Buzhen Huang,Chen Li,Chongyang Xu,Dongyue Lu,Jinnan Chen,Yangang Wang,Gim Hee Lee*

Main category: cs.CV

TL;DR: 提出了一种基于双分支优化框架的方法，通过结合人类外观、社交距离和物理约束，从复杂视频中恢复准确的交互动作。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在视觉模糊和遮挡情况下无法准确估计人体姿态的问题。

Method: 使用扩散模型学习社交距离和姿态先验，结合双分支优化框架和多种约束（如3D高斯、2D关键点等）重建动作和外观。

Result: 在多个基准测试中表现优于现有方法，并构建了带有伪真实交互标注的数据集。

Conclusion: 该方法能够从复杂环境中准确估计交互动作，推动了姿态估计和人类行为理解的研究。

Abstract: Due to visual ambiguities and inter-person occlusions, existing human pose estimation methods cannot recover plausible close interactions from in-the-wild videos. Even state-of-the-art large foundation models~(\eg, SAM) cannot accurately distinguish human semantics in such challenging scenarios. In this work, we find that human appearance can provide a straightforward cue to address these obstacles. Based on this observation, we propose a dual-branch optimization framework to reconstruct accurate interactive motions with plausible body contacts constrained by human appearances, social proxemics, and physical laws. Specifically, we first train a diffusion model to learn the human proxemic behavior and pose prior knowledge. The trained network and two optimizable tensors are then incorporated into a dual-branch optimization framework to reconstruct human motions and appearances. Several constraints based on 3D Gaussians, 2D keypoints, and mesh penetrations are also designed to assist the optimization. With the proxemics prior and diverse constraints, our method is capable of estimating accurate interactions from in-the-wild videos captured in complex environments. We further build a dataset with pseudo ground-truth interaction annotations, which may promote future research on pose estimation and human behavior understanding. Experimental results on several benchmarks demonstrate that our method outperforms existing approaches. The code and data are available at https://www.buzhenhuang.com/works/CloseApp.html.

</details>


### [17] [APT: Adaptive Personalized Training for Diffusion Models with Limited Data](https://arxiv.org/abs/2507.02687)
*JungWoo Chae,Jiyoon Kim,JaeWoong Choi,Kyungyul Kim,Sangheum Hwang*

Main category: cs.CV

TL;DR: APT框架通过自适应训练调整、表示稳定化和注意力对齐，解决了有限数据下扩散模型的过拟合问题，保持了先验知识并提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决有限数据下扩散模型个性化训练中的过拟合、先验知识丢失和文本对齐退化问题。

Method: 提出APT框架，包含自适应训练调整、表示稳定化和注意力对齐三个组件，分别通过过拟合指示器、特征图正则化和注意力图对齐来优化模型。

Result: 实验表明APT有效减少过拟合，保持先验知识，并在有限数据下生成高质量多样化图像。

Conclusion: APT为有限数据下的扩散模型个性化训练提供了有效解决方案。

Abstract: Personalizing diffusion models using limited data presents significant challenges, including overfitting, loss of prior knowledge, and degradation of text alignment. Overfitting leads to shifts in the noise prediction distribution, disrupting the denoising trajectory and causing the model to lose semantic coherence. In this paper, we propose Adaptive Personalized Training (APT), a novel framework that mitigates overfitting by employing adaptive training strategies and regularizing the model's internal representations during fine-tuning. APT consists of three key components: (1) Adaptive Training Adjustment, which introduces an overfitting indicator to detect the degree of overfitting at each time step bin and applies adaptive data augmentation and adaptive loss weighting based on this indicator; (2)Representation Stabilization, which regularizes the mean and variance of intermediate feature maps to prevent excessive shifts in noise prediction; and (3) Attention Alignment for Prior Knowledge Preservation, which aligns the cross-attention maps of the fine-tuned model with those of the pretrained model to maintain prior knowledge and semantic coherence. Through extensive experiments, we demonstrate that APT effectively mitigates overfitting, preserves prior knowledge, and outperforms existing methods in generating high-quality, diverse images with limited reference data.

</details>


### [18] [CanonSwap: High-Fidelity and Consistent Video Face Swapping via Canonical Space Modulation](https://arxiv.org/abs/2507.02691)
*Xiangyang Luo,Ye Zhu,Yunfei Liu,Lijian Lin,Cong Wan,Zijian Cai,Shao-Lun Huang,Yu Li*

Main category: cs.CV

TL;DR: CanonSwap提出了一种新的视频人脸交换框架，通过解耦面部外观和运动信息，解决了现有方法在保持目标面部动态属性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视频人脸交换方法在身份转移上表现良好，但在保持目标面部的动态属性（如表情、姿态等）上存在不足，导致结果不一致。

Method: CanonSwap通过解耦运动与外观信息，在统一规范空间内修改身份，再将其重新整合到原始视频空间。此外，设计了部分身份调制模块和细粒度同步指标。

Result: 实验表明，CanonSwap在视觉质量、时间一致性和身份保持上显著优于现有方法。

Conclusion: CanonSwap通过解耦和精确调制，有效解决了视频人脸交换中的动态属性保持问题。

Abstract: Video face swapping aims to address two primary challenges: effectively transferring the source identity to the target video and accurately preserving the dynamic attributes of the target face, such as head poses, facial expressions, lip-sync, \etc. Existing methods mainly focus on achieving high-quality identity transfer but often fall short in maintaining the dynamic attributes of the target face, leading to inconsistent results. We attribute this issue to the inherent coupling of facial appearance and motion in videos. To address this, we propose CanonSwap, a novel video face-swapping framework that decouples motion information from appearance information. Specifically, CanonSwap first eliminates motion-related information, enabling identity modification within a unified canonical space. Subsequently, the swapped feature is reintegrated into the original video space, ensuring the preservation of the target face's dynamic attributes. To further achieve precise identity transfer with minimal artifacts and enhanced realism, we design a Partial Identity Modulation module that adaptively integrates source identity features using a spatial mask to restrict modifications to facial regions. Additionally, we introduce several fine-grained synchronization metrics to comprehensively evaluate the performance of video face swapping methods. Extensive experiments demonstrate that our method significantly outperforms existing approaches in terms of visual quality, temporal consistency, and identity preservation. Our project page are publicly available at https://luoxyhappy.github.io/CanonSwap/.

</details>


### [19] [UniMC: Taming Diffusion Transformer for Unified Keypoint-Guided Multi-Class Image Generation](https://arxiv.org/abs/2507.02713)
*Qin Guo,Ailing Zeng,Dongxu Yue,Ceyuan Yang,Yang Cao,Hanzhong Guo,Fei Shen,Wei Liu,Xihui Liu,Dan Xu*

Main category: cs.CV

TL;DR: 论文提出了一种名为UniMC的DiT框架，用于统一控制多类图像生成，并发布了HAIG-2.9M数据集，解决了现有关键点引导模型在非刚性物体和多重叠对象生成上的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有关键点引导模型在生成非刚性物体（如动物）和多重叠对象时效果不佳，主要受限于现有可控方法的固有缺陷和缺乏合适的数据集。

Method: 设计了UniMC框架，将实例和关键点条件整合为紧凑令牌，并提出了HAIG-2.9M数据集，包含丰富的标注和严格的人工检查。

Result: 实验证明HAIG-2.9M数据集的高质量和UniMC的有效性，尤其在重度遮挡和多类场景中表现突出。

Conclusion: UniMC和HAIG-2.9M为关键点引导的图像生成提供了更通用的解决方案，显著提升了生成质量和多样性。

Abstract: Although significant advancements have been achieved in the progress of keypoint-guided Text-to-Image diffusion models, existing mainstream keypoint-guided models encounter challenges in controlling the generation of more general non-rigid objects beyond humans (e.g., animals). Moreover, it is difficult to generate multiple overlapping humans and animals based on keypoint controls solely. These challenges arise from two main aspects: the inherent limitations of existing controllable methods and the lack of suitable datasets. First, we design a DiT-based framework, named UniMC, to explore unifying controllable multi-class image generation. UniMC integrates instance- and keypoint-level conditions into compact tokens, incorporating attributes such as class, bounding box, and keypoint coordinates. This approach overcomes the limitations of previous methods that struggled to distinguish instances and classes due to their reliance on skeleton images as conditions. Second, we propose HAIG-2.9M, a large-scale, high-quality, and diverse dataset designed for keypoint-guided human and animal image generation. HAIG-2.9M includes 786K images with 2.9M instances. This dataset features extensive annotations such as keypoints, bounding boxes, and fine-grained captions for both humans and animals, along with rigorous manual inspection to ensure annotation accuracy. Extensive experiments demonstrate the high quality of HAIG-2.9M and the effectiveness of UniMC, particularly in heavy occlusions and multi-class scenarios.

</details>


### [20] [FairHuman: Boosting Hand and Face Quality in Human Image Generation with Minimum Potential Delay Fairness in Diffusion Models](https://arxiv.org/abs/2507.02714)
*Yuxuan Wang,Tianwei Cao,Huayu Zhang,Zhongjiang He,Kongming Liang,Zhanyu Ma*

Main category: cs.CV

TL;DR: FairHuman提出了一种多目标微调方法，通过全局和局部目标优化，显著提升了生成图像中面部和手部等细节的质量。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型在生成人类图像时，面部和手部等局部细节的生成质量不足，主要由于训练中对局部区域的监督不足。

Method: 构建了三个学习目标：一个全局目标和两个针对手部和面部的局部目标，基于预标注的位置先验，并采用最小潜在延迟准则优化参数更新策略。

Result: 实验表明，该方法在保持整体生成质量的同时，显著提升了局部细节的生成效果。

Conclusion: FairHuman通过多目标优化和公平性策略，有效解决了人类图像生成中的局部细节问题。

Abstract: Image generation has achieved remarkable progress with the development of large-scale text-to-image models, especially diffusion-based models. However, generating human images with plausible details, such as faces or hands, remains challenging due to insufficient supervision of local regions during training. To address this issue, we propose FairHuman, a multi-objective fine-tuning approach designed to enhance both global and local generation quality fairly. Specifically, we first construct three learning objectives: a global objective derived from the default diffusion objective function and two local objectives for hands and faces based on pre-annotated positional priors. Subsequently, we derive the optimal parameter updating strategy under the guidance of the Minimum Potential Delay (MPD) criterion, thereby attaining fairness-ware optimization for this multi-objective problem. Based on this, our proposed method can achieve significant improvements in generating challenging local details while maintaining overall quality. Extensive experiments showcase the effectiveness of our method in improving the performance of human image generation under different scenarios.

</details>


### [21] [From Long Videos to Engaging Clips: A Human-Inspired Video Editing Framework with Multimodal Narrative Understanding](https://arxiv.org/abs/2507.02790)
*Xiangfeng Wang,Xiao Li,Yadong Wei,Xueyu Song,Yang Song,Xiaoqiang Xia,Fangrui Zeng,Zaiyi Chen,Liu Liu,Gu Xu,Tong Xu*

Main category: cs.CV

TL;DR: 提出了一种基于多模态叙事理解的自动视频编辑框架（HIVE），通过角色提取、对话分析和叙事总结提升编辑质量，并在新数据集DramaAD上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 在线视频内容快速增长，现有自动编辑方法依赖文本线索且忽视视觉上下文，导致输出不连贯。

Method: 结合多模态大语言模型进行角色提取、对话分析和叙事总结，并通过场景级分割将编辑过程分解为三个子任务。

Result: 在DramaAD数据集上，HIVE框架显著优于现有基线，缩小了自动与人工编辑视频的质量差距。

Conclusion: HIVE框架通过多模态叙事理解提升了自动视频编辑的连贯性和质量，为未来研究提供了新方向。

Abstract: The rapid growth of online video content, especially on short video platforms, has created a growing demand for efficient video editing techniques that can condense long-form videos into concise and engaging clips. Existing automatic editing methods predominantly rely on textual cues from ASR transcripts and end-to-end segment selection, often neglecting the rich visual context and leading to incoherent outputs. In this paper, we propose a human-inspired automatic video editing framework (HIVE) that leverages multimodal narrative understanding to address these limitations. Our approach incorporates character extraction, dialogue analysis, and narrative summarization through multimodal large language models, enabling a holistic understanding of the video content. To further enhance coherence, we apply scene-level segmentation and decompose the editing process into three subtasks: highlight detection, opening/ending selection, and pruning of irrelevant content. To facilitate research in this area, we introduce DramaAD, a novel benchmark dataset comprising over 800 short drama episodes and 500 professionally edited advertisement clips. Experimental results demonstrate that our framework consistently outperforms existing baselines across both general and advertisement-oriented editing tasks, significantly narrowing the quality gap between automatic and human-edited videos.

</details>


### [22] [RichControl: Structure- and Appearance-Rich Training-Free Spatial Control for Text-to-Image Generation](https://arxiv.org/abs/2507.02792)
*Liheng Zhang,Lexi Pang,Hang Ye,Xiaoxuan Ma,Yizhou Wang*

Main category: cs.CV

TL;DR: 本文提出了一种灵活的文本到图像扩散模型特征注入框架，解决了现有方法在结构对齐和视觉质量上的问题。


<details>
  <summary>Details</summary>
Motivation: 现有特征注入方法在结构对齐、条件泄漏和视觉伪影方面存在问题，尤其是在条件图像与自然RGB分布差异较大时。

Method: 提出了一种解耦注入时间步与去噪过程的框架，包括结构丰富的注入模块、外观丰富的提示和重启细化策略。

Result: 实验表明，该方法在多样化的零样本条件场景中实现了最先进的性能。

Conclusion: 该框架实现了无需训练的结构丰富且外观丰富的生成效果。

Abstract: Text-to-image (T2I) diffusion models have shown remarkable success in generating high-quality images from text prompts. Recent efforts extend these models to incorporate conditional images (e.g., depth or pose maps) for fine-grained spatial control. Among them, feature injection methods have emerged as a training-free alternative to traditional fine-tuning approaches. However, they often suffer from structural misalignment, condition leakage, and visual artifacts, especially when the condition image diverges significantly from natural RGB distributions. By revisiting existing methods, we identify a core limitation: the synchronous injection of condition features fails to account for the trade-off between domain alignment and structural preservation during denoising. Inspired by this observation, we propose a flexible feature injection framework that decouples the injection timestep from the denoising process. At its core is a structure-rich injection module, which enables the model to better adapt to the evolving interplay between alignment and structure preservation throughout the diffusion steps, resulting in more faithful structural generation. In addition, we introduce appearance-rich prompting and a restart refinement strategy to further enhance appearance control and visual quality. Together, these designs enable training-free generation that is both structure-rich and appearance-rich. Extensive experiments show that our approach achieves state-of-the-art performance across diverse zero-shot conditioning scenarios.

</details>


### [23] [LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion](https://arxiv.org/abs/2507.02813)
*Fangfu Liu,Hao Li,Jiawei Chi,Hanyang Wang,Minghui Yang,Fudong Wang,Yueqi Duan*

Main category: cs.CV

TL;DR: LangScene-X是一个生成框架，通过稀疏视图生成3D一致的多模态信息，解决了传统方法在有限视图下的渲染和语义合成问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖密集视图重建，导致在稀疏视图下出现渲染伪影和语义合成不准确的问题。

Method: 结合TriMap视频扩散模型生成RGB、几何和语义信息，并使用Language Quantized Compressor (LQC)编码语言嵌入，实现跨场景泛化。

Result: 实验表明，LangScene-X在质量和泛化性上优于现有方法。

Conclusion: LangScene-X通过生成一致的多模态信息，显著提升了稀疏视图下的3D重建和场景理解能力。

Abstract: Recovering 3D structures with open-vocabulary scene understanding from 2D images is a fundamental but daunting task. Recent developments have achieved this by performing per-scene optimization with embedded language information. However, they heavily rely on the calibrated dense-view reconstruction paradigm, thereby suffering from severe rendering artifacts and implausible semantic synthesis when limited views are available. In this paper, we introduce a novel generative framework, coined LangScene-X, to unify and generate 3D consistent multi-modality information for reconstruction and understanding. Powered by the generative capability of creating more consistent novel observations, we can build generalizable 3D language-embedded scenes from only sparse views. Specifically, we first train a TriMap video diffusion model that can generate appearance (RGBs), geometry (normals), and semantics (segmentation maps) from sparse inputs through progressive knowledge integration. Furthermore, we propose a Language Quantized Compressor (LQC), trained on large-scale image datasets, to efficiently encode language embeddings, enabling cross-scene generalization without per-scene retraining. Finally, we reconstruct the language surface fields by aligning language information onto the surface of 3D scenes, enabling open-ended language queries. Extensive experiments on real-world data demonstrate the superiority of our LangScene-X over state-of-the-art methods in terms of quality and generalizability. Project Page: https://liuff19.github.io/LangScene-X.

</details>


### [24] [USAD: An Unsupervised Data Augmentation Spatio-Temporal Attention Diffusion Network](https://arxiv.org/abs/2507.02827)
*Ying Yu,Hang Xiao,Siyao Li,Jiarui Li,Haotian Tang,Hanyu Liu,Chao Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于多注意力交互机制的综合优化方法，用于解决人类活动识别中的标注数据稀缺、特征提取不足和设备性能问题。


<details>
  <summary>Details</summary>
Motivation: 人类活动识别（HAR）在健康监测、安全防护和运动分析中有广泛应用，但仍面临标注数据稀缺、高级特征提取不足和设备性能不佳等挑战。

Method: 采用无监督统计引导扩散模型进行数据增强；设计多分支时空交互网络捕捉多尺度特征；结合时空注意力机制和跨分支特征融合单元；集成自适应多损失函数融合策略。

Result: 在WISDM、PAMAP2和OPPORTUNITY数据集上分别达到98.84%、93.81%和80.92%的准确率，显著优于现有方法。

Conclusion: 该方法通过综合优化显著提升了HAR的性能，并在嵌入式设备上验证了其高效性和可行性。

Abstract: The primary objective of human activity recognition (HAR) is to infer ongoing human actions from sensor data, a task that finds broad applications in health monitoring, safety protection, and sports analysis. Despite proliferating research, HAR still faces key challenges, including the scarcity of labeled samples for rare activities, insufficient extraction of high-level features, and suboptimal model performance on lightweight devices. To address these issues, this paper proposes a comprehensive optimization approach centered on multi-attention interaction mechanisms. First, an unsupervised, statistics-guided diffusion model is employed to perform data augmentation, thereby alleviating the problems of labeled data scarcity and severe class imbalance. Second, a multi-branch spatio-temporal interaction network is designed, which captures multi-scale features of sequential data through parallel residual branches with 3*3, 5*5, and 7*7 convolutional kernels. Simultaneously, temporal attention mechanisms are incorporated to identify critical time points, while spatial attention enhances inter-sensor interactions. A cross-branch feature fusion unit is further introduced to improve the overall feature representation capability. Finally, an adaptive multi-loss function fusion strategy is integrated, allowing for dynamic adjustment of loss weights and overall model optimization. Experimental results on three public datasets, WISDM, PAMAP2, and OPPORTUNITY, demonstrate that the proposed unsupervised data augmentation spatio-temporal attention diffusion network (USAD) achieves accuracies of 98.84%, 93.81%, and 80.92% respectively, significantly outperforming existing approaches. Furthermore, practical deployment on embedded devices verifies the efficiency and feasibility of the proposed method.

</details>


### [25] [Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection](https://arxiv.org/abs/2507.02844)
*Ziqi Miao,Yi Ding,Lijun Li,Jing Shao*

Main category: cs.CV

TL;DR: 该论文提出了一种新型视觉中心越狱攻击（VisCo Attack），通过视觉信息构建完整的越狱场景，显著提升了攻击效果。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在视觉模态中存在安全漏洞，现有攻击方法语义模糊且缺乏现实场景基础。

Method: 提出VisCo攻击，采用四种视觉策略动态生成辅助图像，结合毒性混淆和语义优化生成攻击提示。

Result: VisCo在MM-SafetyBench上对GPT-4o的毒性得分为4.78，攻击成功率达85%，显著优于基线。

Conclusion: VisCo攻击有效揭示了MLLMs在视觉模态中的安全风险，为未来防御提供了参考。

Abstract: With the emergence of strong visual-language capabilities, multimodal large language models (MLLMs) have demonstrated tremendous potential for real-world applications. However, the security vulnerabilities exhibited by the visual modality pose significant challenges to deploying such models in open-world environments. Recent studies have successfully induced harmful responses from target MLLMs by encoding harmful textual semantics directly into visual inputs. However, in these approaches, the visual modality primarily serves as a trigger for unsafe behavior, often exhibiting semantic ambiguity and lacking grounding in realistic scenarios. In this work, we define a novel setting: visual-centric jailbreak, where visual information serves as a necessary component in constructing a complete and realistic jailbreak context. Building on this setting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates contextual dialogue using four distinct visual-focused strategies, dynamically generating auxiliary images when necessary to construct a visual-centric jailbreak scenario. To maximize attack effectiveness, it incorporates automatic toxicity obfuscation and semantic refinement to produce a final attack prompt that reliably triggers harmful responses from the target black-box MLLMs. Specifically, VisCo achieves a toxicity score of 4.78 and an Attack Success Rate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming the baseline, which performs a toxicity score of 2.48 and an ASR of 22.2%. The code is available at https://github.com/Dtc7w3PQ/Visco-Attack.

</details>


### [26] [AnyI2V: Animating Any Conditional Image with Motion Control](https://arxiv.org/abs/2507.02857)
*Ziye Li,Hao Luo,Xincheng Shuai,Henghui Ding*

Main category: cs.CV

TL;DR: AnyI2V是一个无需训练的框架，通过用户定义的运动轨迹为任意条件图像生成动画，支持多种模态输入，并提供灵活的视频生成和编辑功能。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频（T2V）和图像到视频（I2V）方法在动态运动信号和空间约束的整合上存在不足，缺乏精确控制和编辑灵活性。

Method: 提出AnyI2V框架，支持多种模态的条件图像输入（如网格和点云），通过用户定义的运动轨迹实现动画生成，并支持混合输入和风格编辑。

Result: 实验表明AnyI2V在空间和运动控制的视频生成中表现优异，提供了更高的灵活性和多样性。

Conclusion: AnyI2V为视频生成领域提供了新的视角，解决了现有方法的局限性，支持更广泛的应用场景。

Abstract: Recent advancements in video generation, particularly in diffusion models, have driven notable progress in text-to-video (T2V) and image-to-video (I2V) synthesis. However, challenges remain in effectively integrating dynamic motion signals and flexible spatial constraints. Existing T2V methods typically rely on text prompts, which inherently lack precise control over the spatial layout of generated content. In contrast, I2V methods are limited by their dependence on real images, which restricts the editability of the synthesized content. Although some methods incorporate ControlNet to introduce image-based conditioning, they often lack explicit motion control and require computationally expensive training. To address these limitations, we propose AnyI2V, a training-free framework that animates any conditional images with user-defined motion trajectories. AnyI2V supports a broader range of modalities as the conditional image, including data types such as meshes and point clouds that are not supported by ControlNet, enabling more flexible and versatile video generation. Additionally, it supports mixed conditional inputs and enables style transfer and editing via LoRA and text prompts. Extensive experiments demonstrate that the proposed AnyI2V achieves superior performance and provides a new perspective in spatial- and motion-controlled video generation. Code is available at https://henghuiding.com/AnyI2V/.

</details>


### [27] [Less is Enough: Training-Free Video Diffusion Acceleration via Runtime-Adaptive Caching](https://arxiv.org/abs/2507.02860)
*Xin Zhou,Dingkang Liang,Kaijin Chen,Tianrui Feng,Xiwu Chen,Hongkai Lin,Yikang Ding,Feiyang Tan,Hengshuang Zhao,Xiang Bai*

Main category: cs.CV

TL;DR: EasyCache是一个无需训练的视频扩散模型加速框架，通过动态重用计算向量减少冗余计算，显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 视频生成模型因迭代去噪过程导致推理速度慢、计算成本高，限制了其广泛应用。

Method: 提出轻量级、运行时自适应的缓存机制，动态重用先前计算的变换向量，避免冗余计算。

Result: 在多个大规模视频生成模型上实现2.1-3.3倍的加速，PSNR提升高达36%。

Conclusion: EasyCache是一种高效且易于使用的高质量视频生成解决方案。

Abstract: Video generation models have demonstrated remarkable performance, yet their broader adoption remains constrained by slow inference speeds and substantial computational costs, primarily due to the iterative nature of the denoising process. Addressing this bottleneck is essential for democratizing advanced video synthesis technologies and enabling their integration into real-world applications. This work proposes EasyCache, a training-free acceleration framework for video diffusion models. EasyCache introduces a lightweight, runtime-adaptive caching mechanism that dynamically reuses previously computed transformation vectors, avoiding redundant computations during inference. Unlike prior approaches, EasyCache requires no offline profiling, pre-computation, or extensive parameter tuning. We conduct comprehensive studies on various large-scale video generation models, including OpenSora, Wan2.1, and HunyuanVideo. Our method achieves leading acceleration performance, reducing inference time by up to 2.1-3.3$\times$ compared to the original baselines while maintaining high visual fidelity with a significant up to 36% PSNR improvement compared to the previous SOTA method. This improvement makes our EasyCache a efficient and highly accessible solution for high-quality video generation in both research and practical applications. The code is available at https://github.com/H-EmbodVis/EasyCache.

</details>


### [28] [RefTok: Reference-Based Tokenization for Video Generation](https://arxiv.org/abs/2507.02862)
*Xiang Fan,Xiaohang Sun,Kushan Thakkar,Zhu Liu,Vimal Bhat,Ranjay Krishna,Xiang Hao*

Main category: cs.CV

TL;DR: RefTok是一种基于参考的帧标记方法，能有效捕捉视频中的时间依赖性和冗余信息，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法独立处理帧集，未能有效捕捉视频中的时间依赖性和冗余信息。

Method: RefTok通过未量化的参考帧对帧集进行编码和解码，保留运动连续性和对象外观。

Result: 在四个视频数据集上，RefTok平均提升36.7%的性能指标，并在视频生成任务中优于更大规模的模型。

Conclusion: RefTok在视频建模中表现出色，显著提升了时间动态捕捉和上下文信息保留能力。

Abstract: Effectively handling temporal redundancy remains a key challenge in learning video models. Prevailing approaches often treat each set of frames independently, failing to effectively capture the temporal dependencies and redundancies inherent in videos. To address this limitation, we introduce RefTok, a novel reference-based tokenization method capable of capturing complex temporal dynamics and contextual information. Our method encodes and decodes sets of frames conditioned on an unquantized reference frame. When decoded, RefTok preserves the continuity of motion and the appearance of objects across frames. For example, RefTok retains facial details despite head motion, reconstructs text correctly, preserves small patterns, and maintains the legibility of handwriting from the context. Across 4 video datasets (K600, UCF-101, BAIR Robot Pushing, and DAVIS), RefTok significantly outperforms current state-of-the-art tokenizers (Cosmos and MAGVIT) and improves all evaluated metrics (PSNR, SSIM, LPIPS) by an average of 36.7% at the same or higher compression ratios. When a video generation model is trained using RefTok's latents on the BAIR Robot Pushing task, the generations not only outperform MAGVIT-B but the larger MAGVIT-L, which has 4x more parameters, across all generation metrics by an average of 27.9%.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [29] [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)
*Dohoon Kim,Donghun Kang,Taesup Moon*

Main category: cs.CL

TL;DR: DoMIX提出了一种基于LoRA模块的高效、并行且对领域顺序鲁棒的领域自适应预训练方法，解决了现有持续DAP方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有持续DAP方法存在高计算成本、对增量数据顺序敏感以及无法为特定任务提供定制化预训练模型的问题。

Method: 利用LoRA模块（一种参数高效微调方法）实现高效并行训练，并对领域顺序鲁棒。

Result: DoMIX能够高效利用累积知识，为特定任务提供定制化预训练模型，并可扩展至标准LLM微调场景。

Conclusion: DoMIX解决了持续DAP的关键挑战，为领域自适应预训练提供了更高效的解决方案。

Abstract: Domain-Adaptive Pre-training (DAP) has recently gained attention for its effectiveness in fine-tuning pre-trained models. Building on this, continual DAP has been explored to develop pre-trained models capable of incrementally incorporating different domain datasets. However, existing continual DAP methods face several limitations: (1) high computational cost and GPU memory usage during training; (2) sensitivity to incremental data order; and (3) providing a single, generalized model for all end tasks, which contradicts the essence of DAP. In this paper, we propose DoMIX, a novel approach that addresses these challenges by leveraging LoRA modules, a representative parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient and parallel domain-adaptive pre-training that is robust to domain order and effectively utilizes accumulated knowledge to provide tailored pre-trained models for specific tasks. We also demonstrate that our method can be extended beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available at https://github.com/dohoonkim-ai/DoMIX.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [30] [Energy-Based Transformers are Scalable Learners and Thinkers](https://arxiv.org/abs/2507.02092)
*Alexi Gladstone,Ganesh Nanduru,Md Mofijul Islam,Peixuan Han,Hyeonjeong Ha,Aman Chadha,Yilun Du,Heng Ji,Jundong Li,Tariq Iqbal*

Main category: cs.LG

TL;DR: 论文提出了一种新型的Energy-Based Transformers（EBTs），通过无监督学习实现类似人类System 2 Thinking的推理能力，并在多模态任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有推理方法存在模态或问题特异性，且需要额外监督训练。本文探索是否可以通过无监督学习实现通用的System 2 Thinking。

Method: 训练EBTs为输入和候选预测对分配能量值，通过梯度下降优化能量最小化进行预测。

Result: EBTs在训练和推理中均优于现有方法，数据效率提升35%，语言任务性能提升29%，图像去噪表现更优。

Conclusion: EBTs是一种有前景的新范式，可扩展模型的学习和推理能力。

Abstract: Inference-time computation techniques, analogous to human System 2 Thinking, have recently become popular for improving model performances. However, most existing approaches suffer from several limitations: they are modality-specific (e.g., working only in text), problem-specific (e.g., verifiable domains like math and coding), or require additional supervision/training on top of unsupervised pretraining (e.g., verifiers or verifiable rewards). In this paper, we ask the question "Is it possible to generalize these System 2 Thinking approaches, and develop models that learn to think solely from unsupervised learning?" Interestingly, we find the answer is yes, by learning to explicitly verify the compatibility between inputs and candidate-predictions, and then re-framing prediction problems as optimization with respect to this verifier. Specifically, we train Energy-Based Transformers (EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy value to every input and candidate-prediction pair, enabling predictions through gradient descent-based energy minimization until convergence. Across both discrete (text) and continuous (visual) modalities, we find EBTs scale faster than the dominant Transformer++ approach during training, achieving an up to 35% higher scaling rate with respect to data, batch size, parameters, FLOPs, and depth. During inference, EBTs improve performance with System 2 Thinking by 29% more than the Transformer++ on language tasks, and EBTs outperform Diffusion Transformers on image denoising while using fewer forward passes. Further, we find that EBTs achieve better results than existing models on most downstream tasks given the same or worse pretraining performance, suggesting that EBTs generalize better than existing approaches. Consequently, EBTs are a promising new paradigm for scaling both the learning and thinking capabilities of models.

</details>


### [31] [Generative Latent Diffusion for Efficient Spatiotemporal Data Reduction](https://arxiv.org/abs/2507.02129)
*Xiao Li,Liangji Zhu,Anand Rangarajan,Sanjay Ranka*

Main category: cs.LG

TL;DR: 提出了一种结合变分自编码器和条件扩散模型的高效潜在扩散框架，显著提升了数据压缩的性能。


<details>
  <summary>Details</summary>
Motivation: 生成模型在条件设置下表现优异，但其可控性和重建精度限制了其在数据压缩中的实际应用。

Method: 通过变分自编码器和条件扩散模型的结合，仅压缩少量关键帧到潜在空间，并利用生成插值重建其余帧。

Result: 实验显示，该方法压缩比优于规则压缩方法10倍，性能优于学习型方法63%。

Conclusion: 该方法在显著降低存储成本的同时，实现了精确的时空重建。

Abstract: Generative models have demonstrated strong performance in conditional settings and can be viewed as a form of data compression, where the condition serves as a compact representation. However, their limited controllability and reconstruction accuracy restrict their practical application to data compression. In this work, we propose an efficient latent diffusion framework that bridges this gap by combining a variational autoencoder with a conditional diffusion model. Our method compresses only a small number of keyframes into latent space and uses them as conditioning inputs to reconstruct the remaining frames via generative interpolation, eliminating the need to store latent representations for every frame. This approach enables accurate spatiotemporal reconstruction while significantly reducing storage costs. Experimental results across multiple datasets show that our method achieves up to 10 times higher compression ratios than rule-based state-of-the-art compressors such as SZ3, and up to 63 percent better performance than leading learning-based methods under the same reconstruction error.

</details>


### [32] [L-VAE: Variational Auto-Encoder with Learnable Beta for Disentangled Representation](https://arxiv.org/abs/2507.02619)
*Hazal Mogultay Ozcan,Sinan Kalkan,Fatos T. Yarman-Vural*

Main category: cs.LG

TL;DR: L-VAE是一种新型模型，通过学习损失函数的超参数和模型架构参数，动态平衡解耦和重建损失，优于多种现有VAE变体。


<details>
  <summary>Details</summary>
Motivation: 解决β-VAE中超参数η需经验调整的问题，动态平衡解耦和重建损失。

Method: 提出L-VAE模型，学习损失项权重和模型参数，并添加正则化项防止偏置。

Result: 在多个数据集上表现优于或接近现有方法，成功解耦CelebA中的面部属性。

Conclusion: L-VAE有效平衡解耦和重建，性能优于现有方法。

Abstract: In this paper, we propose a novel model called Learnable VAE (L-VAE), which learns a disentangled representation together with the hyperparameters of the cost function. L-VAE can be considered as an extension of \b{eta}-VAE, wherein the hyperparameter, \b{eta}, is empirically adjusted. L-VAE mitigates the limitations of \b{eta}-VAE by learning the relative weights of the terms in the loss function to control the dynamic trade-off between disentanglement and reconstruction losses. In the proposed model, the weight of the loss terms and the parameters of the model architecture are learned concurrently. An additional regularization term is added to the loss function to prevent bias towards either reconstruction or disentanglement losses. Experimental analyses show that the proposed L-VAE finds an effective balance between reconstruction fidelity and disentangling the latent dimensions. Comparisons of the proposed L-VAE against \b{eta}-VAE, VAE, ControlVAE, DynamicVAE, and {\sigma}-VAE on datasets, such as dSprites, MPI3D-complex, Falcor3D, and Isaac3D reveals that L-VAE consistently provides the best or the second best performances measured by a set of disentanglement metrics. Moreover, qualitative experiments on CelebA dataset, confirm the success of the L-VAE model for disentangling the facial attributes.

</details>


### [33] [Embedding-Based Federated Data Sharing via Differentially Private Conditional VAEs](https://arxiv.org/abs/2507.02671)
*Francesco Di Salvo,Hanh Huyen My Nguyen,Christian Ledig*

Main category: cs.LG

TL;DR: 提出了一种基于差分隐私生成模型的数据共享方法，通过基础模型提取紧凑嵌入，降低通信成本并支持多任务。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在医学影像中数据稀缺和隐私问题，同时克服联邦学习的通信成本高和任务单一限制。

Method: 采用差分隐私条件变分自编码器（DP-CVAE）建模全局隐私感知数据分布，提取紧凑嵌入。

Result: 方法在隐私、可扩展性和效率上优于传统联邦学习，且嵌入保真度高于DP-CGAN，参数需求减少5倍。

Conclusion: 该方法为医学影像提供了一种高效、隐私保护的解决方案，支持多样化下游任务。

Abstract: Deep Learning (DL) has revolutionized medical imaging, yet its adoption is constrained by data scarcity and privacy regulations, limiting access to diverse datasets. Federated Learning (FL) enables decentralized training but suffers from high communication costs and is often restricted to a single downstream task, reducing flexibility. We propose a data-sharing method via Differentially Private (DP) generative models. By adopting foundation models, we extract compact, informative embeddings, reducing redundancy and lowering computational overhead. Clients collaboratively train a Differentially Private Conditional Variational Autoencoder (DP-CVAE) to model a global, privacy-aware data distribution, supporting diverse downstream tasks. Our approach, validated across multiple feature extractors, enhances privacy, scalability, and efficiency, outperforming traditional FL classifiers while ensuring differential privacy. Additionally, DP-CVAE produces higher-fidelity embeddings than DP-CGAN while requiring $5{\times}$ fewer parameters.

</details>
