<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 33]
- [cs.LG](#cs.LG) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D Generation](https://arxiv.org/abs/2508.00428)
*Nan Xiang,Tianyi Liang,Haiwen Huang,Shiqi Jiang,Hao Huang,Yifei Huang,Liangyu Chen,Changbo Wang,Chenhui Li*

Main category: cs.GR

TL;DR: Sel3DCraft是一个用于文本到3D生成的视觉提示工程系统，通过多视图评分和视觉分析提升3D模型的生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决文本到3D生成中盲目试错提示导致结果不可预测的问题，提升生成过程的效率和可控性。

Method: 采用双分支结构（检索与生成结合）、多视图混合评分方法（结合MLLMs和高层次指标）以及视觉分析工具。

Result: Sel3DCraft在支持设计师创造力方面优于其他文本到3D生成系统。

Conclusion: Sel3DCraft通过视觉提示工程显著提升了文本到3D生成的效率和质量。

Abstract: Text-to-3D (T23D) generation has transformed digital content creation, yet remains bottlenecked by blind trial-and-error prompting processes that yield unpredictable results. While visual prompt engineering has advanced in text-to-image domains, its application to 3D generation presents unique challenges requiring multi-view consistency evaluation and spatial understanding. We present Sel3DCraft, a visual prompt engineering system for T23D that transforms unstructured exploration into a guided visual process. Our approach introduces three key innovations: a dual-branch structure combining retrieval and generation for diverse candidate exploration; a multi-view hybrid scoring approach that leverages MLLMs with innovative high-level metrics to assess 3D models with human-expert consistency; and a prompt-driven visual analytics suite that enables intuitive defect identification and refinement. Extensive testing and user studies demonstrate that Sel3DCraft surpasses other T23D systems in supporting creativity for designers.

</details>


### [2] [SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation](https://arxiv.org/abs/2508.00782)
*Kien T. Pham,Yingqing He,Yazhou Xing,Qifeng Chen,Long Chen*

Main category: cs.GR

TL;DR: SpA2V是一个利用音频中的空间和语义线索生成视频的框架，通过两阶段方法实现高语义和空间对应。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注语义信息，忽略了音频中的空间属性（如位置和运动方向），而人类可以自然识别这些信息。SpA2V旨在填补这一空白。

Method: 1）音频引导的视频规划：利用MLLM从音频中提取空间和语义线索，构建视频场景布局（VSL）。2）基于布局的视频生成：将VSL作为条件输入预训练的扩散模型，实现训练自由的视频生成。

Result: 实验表明，SpA2V能够生成与输入音频在语义和空间上高度对齐的真实视频。

Conclusion: SpA2V通过利用音频中的空间线索，显著提升了视频生成的准确性和真实性。

Abstract: Audio-driven video generation aims to synthesize realistic videos that align with input audio recordings, akin to the human ability to visualize scenes from auditory input. However, existing approaches predominantly focus on exploring semantic information, such as the classes of sounding sources present in the audio, limiting their ability to generate videos with accurate content and spatial composition. In contrast, we humans can not only naturally identify the semantic categories of sounding sources but also determine their deeply encoded spatial attributes, including locations and movement directions. This useful information can be elucidated by considering specific spatial indicators derived from the inherent physical properties of sound, such as loudness or frequency. As prior methods largely ignore this factor, we present SpA2V, the first framework explicitly exploits these spatial auditory cues from audios to generate videos with high semantic and spatial correspondence. SpA2V decomposes the generation process into two stages: 1) Audio-guided Video Planning: We meticulously adapt a state-of-the-art MLLM for a novel task of harnessing spatial and semantic cues from input audio to construct Video Scene Layouts (VSLs). This serves as an intermediate representation to bridge the gap between the audio and video modalities. 2) Layout-grounded Video Generation: We develop an efficient and effective approach to seamlessly integrate VSLs as conditional guidance into pre-trained diffusion models, enabling VSL-grounded video generation in a training-free manner. Extensive experiments demonstrate that SpA2V excels in generating realistic videos with semantic and spatial alignment to the input audios.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [3] [World Consistency Score: A Unified Metric for Video Generation Quality](https://arxiv.org/abs/2508.00144)
*Akshat Rakheja,Aarsh Ashdhir,Aryan Bhattacharjee,Vanshika Sharma*

Main category: cs.CV

TL;DR: World Consistency Score (WCS) 是一种新的生成视频模型评估指标，强调视频的内部世界一致性，包含四个可解释的子指标，并通过学习权重公式综合评分。


<details>
  <summary>Details</summary>
Motivation: 现有视频评估指标主要关注视觉保真度或提示对齐，忽略了时间与物理一致性，WCS填补了这一空白。

Method: WCS整合了四个子指标（物体持久性、关系稳定性、因果合规性和闪烁惩罚），使用开源工具计算，并通过人类偏好数据训练权重。

Result: WCS通过实验验证（如VBench-2.0等基准测试）与人类评价相关性高，优于现有指标（如FVD、CLIPScore等）。

Conclusion: WCS为评估视频生成模型的世界一致性提供了全面且可解释的框架。

Abstract: We introduce World Consistency Score (WCS), a novel unified evaluation metric for generative video models that emphasizes internal world consistency of the generated videos. WCS integrates four interpretable sub-components - object permanence, relation stability, causal compliance, and flicker penalty - each measuring a distinct aspect of temporal and physical coherence in a video. These submetrics are combined via a learned weighted formula to produce a single consistency score that aligns with human judgments. We detail the motivation for WCS in the context of existing video evaluation metrics, formalize each submetric and how it is computed with open-source tools (trackers, action recognizers, CLIP embeddings, optical flow), and describe how the weights of the WCS combination are trained using human preference data. We also outline an experimental validation blueprint: using benchmarks like VBench-2.0, EvalCrafter, and LOVE to test WCS's correlation with human evaluations, performing sensitivity analyses, and comparing WCS against established metrics (FVD, CLIPScore, VBench, FVMD). The proposed WCS offers a comprehensive and interpretable framework for evaluating video generation models on their ability to maintain a coherent "world" over time, addressing gaps left by prior metrics focused only on visual fidelity or prompt alignment.

</details>


### [4] [Guided Depth Map Super-Resolution via Multi-Scale Fusion U-shaped Mamba Network](https://arxiv.org/abs/2508.00248)
*Chenggang Guo,Hao Xu,XianMing Wan*

Main category: cs.CV

TL;DR: 提出了一种多尺度融合U形Mamba模型（MSF-UM），用于深度图超分辨率任务，结合了局部特征提取和长距离依赖建模的优势，显著减少了参数数量并提高了重建精度。


<details>
  <summary>Details</summary>
Motivation: 传统卷积神经网络在处理长距离依赖和全局上下文信息时存在局限性，而Transformer的计算复杂度和内存消耗较高，限制了其在高分辨率深度图处理中的应用。

Method: 设计了结合残差密集通道注意力块和Mamba状态空间模块的多尺度U形融合结构，利用彩色图像的高频纹理信息指导深度图超分辨率过程。

Result: 模型在多个公开数据集上验证了有效性，显著减少了参数数量并提高了重建精度，尤其在大规模深度图超分辨率任务中表现出优秀的泛化能力。

Conclusion: MSF-UM模型通过结合局部特征提取和长距离依赖建模，以及多尺度跨模态融合策略，为深度图超分辨率任务提供了一种高效且准确的解决方案。

Abstract: Depth map super-resolution technology aims to improve the spatial resolution of low-resolution depth maps and effectively restore high-frequency detail information. Traditional convolutional neural network has limitations in dealing with long-range dependencies and are unable to fully model the global contextual information in depth maps. Although transformer can model global dependencies, its computational complexity and memory consumption are quadratic, which significantly limits its ability to process high-resolution depth maps. In this paper, we propose a multi-scale fusion U-shaped Mamba (MSF-UM) model, a novel guided depth map super-resolution framework. The core innovation of this model is to integrate Mamba's efficient state-space modeling capabilities into a multi-scale U-shaped fusion structure guided by a color image. The structure combining the residual dense channel attention block and the Mamba state space module is designed, which combines the local feature extraction capability of the convolutional layer with the modeling advantage of the state space model for long-distance dependencies. At the same time, the model adopts a multi-scale cross-modal fusion strategy to make full use of the high-frequency texture information from the color image to guide the super-resolution process of the depth map. Compared with existing mainstream methods, the proposed MSF-UM significantly reduces the number of model parameters while achieving better reconstruction accuracy. Extensive experiments on multiple publicly available datasets validate the effectiveness of the model, especially showing excellent generalization ability in the task of large-scale depth map super-resolution.

</details>


### [5] [PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian Splatting](https://arxiv.org/abs/2508.00259)
*Wentao Sun,Hanqing Xu,Quanyun Wu,Dedong Zhang,Yiping Chen,Lingfei Ma,John S. Zelek,Jonathan Li*

Main category: cs.CV

TL;DR: PointGauss是一种基于点云的实时多目标分割框架，通过高斯溅射表示实现高效3D分割，显著提升多视图一致性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在初始化时间长和多视图一致性不足的问题，PointGauss旨在通过点云分割驱动流程直接解析高斯基元来解决这些问题。

Method: 提出点云高斯基元解码器和GPU加速的2D掩码渲染系统，分别实现快速3D实例掩码生成和多视图一致性保证。

Result: 实验显示在多视图mIoU上性能提升1.89%至31.78%，同时保持高效计算。

Conclusion: PointGauss在3D分割中表现出色，并引入新数据集DesktopObjects-360以解决现有基准的局限性。

Abstract: We introduce PointGauss, a novel point cloud-guided framework for real-time multi-object segmentation in Gaussian Splatting representations. Unlike existing methods that suffer from prolonged initialization and limited multi-view consistency, our approach achieves efficient 3D segmentation by directly parsing Gaussian primitives through a point cloud segmentation-driven pipeline. The key innovation lies in two aspects: (1) a point cloud-based Gaussian primitive decoder that generates 3D instance masks within 1 minute, and (2) a GPU-accelerated 2D mask rendering system that ensures multi-view consistency. Extensive experiments demonstrate significant improvements over previous state-of-the-art methods, achieving performance gains of 1.89 to 31.78% in multi-view mIoU, while maintaining superior computational efficiency. To address the limitations of current benchmarks (single-object focus, inconsistent 3D evaluation, small scale, and partial coverage), we present DesktopObjects-360, a novel comprehensive dataset for 3D segmentation in radiance fields, featuring: (1) complex multi-object scenes, (2) globally consistent 2D annotations, (3) large-scale training data (over 27 thousand 2D masks), (4) full 360{\deg} coverage, and (5) 3D evaluation masks.

</details>


### [6] [Towards Robust Semantic Correspondence: A Benchmark and Insights](https://arxiv.org/abs/2508.00272)
*Wenyue Chong*

Main category: cs.CV

TL;DR: 论文提出了一个评估语义对应在恶劣条件下鲁棒性的新基准，发现现有方法在挑战性场景下性能显著下降，大模型能提升鲁棒性但微调会降低相对鲁棒性，DINO模型优于Stable Diffusion，且通用数据增强无效。


<details>
  <summary>Details</summary>
Motivation: 语义对应是计算机视觉的基础任务，但在恶劣条件下的鲁棒性研究不足，因此需要建立新基准评估其性能。

Method: 构建包含14种挑战性场景的基准数据集，评估现有方法和大模型（如DINO和Stable Diffusion）的鲁棒性，并分析常见增强策略的效果。

Result: 现有方法在恶劣条件下性能下降，大模型提升鲁棒性但微调降低相对鲁棒性，DINO优于Stable Diffusion，通用数据增强无效。

Conclusion: 语义对应在恶劣条件下的鲁棒性需任务特定设计，大模型和模型融合能提升性能，但需避免微调。

Abstract: Semantic correspondence aims to identify semantically meaningful relationships between different images and is a fundamental challenge in computer vision. It forms the foundation for numerous tasks such as 3D reconstruction, object tracking, and image editing. With the progress of large-scale vision models, semantic correspondence has achieved remarkable performance in controlled and high-quality conditions. However, the robustness of semantic correspondence in challenging scenarios is much less investigated. In this work, we establish a novel benchmark for evaluating semantic correspondence in adverse conditions. The benchmark dataset comprises 14 distinct challenging scenarios that reflect commonly encountered imaging issues, including geometric distortion, image blurring, digital artifacts, and environmental occlusion. Through extensive evaluations, we provide several key insights into the robustness of semantic correspondence approaches: (1) All existing methods suffer from noticeable performance drops under adverse conditions; (2) Using large-scale vision models can enhance overall robustness, but fine-tuning on these models leads to a decline in relative robustness; (3) The DINO model outperforms the Stable Diffusion in relative robustness, and their fusion achieves better absolute robustness; Moreover, We evaluate common robustness enhancement strategies for semantic correspondence and find that general data augmentations are ineffective, highlighting the need for task-specific designs. These results are consistent across both our dataset and real-world benchmarks.

</details>


### [7] [TITAN-Guide: Taming Inference-Time AligNment for Guided Text-to-Video Diffusion Models](https://arxiv.org/abs/2508.00289)
*Christian Simon,Masato Ishii,Akio Hayakawa,Zhi Zhong,Shusuke Takahashi,Takashi Shibuya,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 提出了一种名为TITAN-Guide的无训练指导框架，用于优化文本到视频扩散模型的推理时间对齐，解决了内存需求和控制优化问题。


<details>
  <summary>Details</summary>
Motivation: 现有无训练指导框架存在内存需求高或控制效果不佳的问题，限制了在计算密集型任务（如文本到视频扩散模型）中的应用。

Method: 开发了一种无需反向传播的高效扩散潜在优化方法，研究了前向梯度下降及其方向性指导选项。

Result: 实验表明，该方法在内存管理和潜在优化方面优于现有方法，显著提升了文本到视频扩散模型的性能。

Conclusion: TITAN-Guide不仅减少了内存需求，还在多个扩散指导基准测试中表现出色。

Abstract: In the recent development of conditional diffusion models still require heavy supervised fine-tuning for performing control on a category of tasks. Training-free conditioning via guidance with off-the-shelf models is a favorable alternative to avoid further fine-tuning on the base model. However, the existing training-free guidance frameworks either have heavy memory requirements or offer sub-optimal control due to rough estimation. These shortcomings limit the applicability to control diffusion models that require intense computation, such as Text-to-Video (T2V) diffusion models. In this work, we propose Taming Inference Time Alignment for Guided Text-to-Video Diffusion Model, so-called TITAN-Guide, which overcomes memory space issues, and provides more optimal control in the guidance process compared to the counterparts. In particular, we develop an efficient method for optimizing diffusion latents without backpropagation from a discriminative guiding model. In particular, we study forward gradient descents for guided diffusion tasks with various options on directional directives. In our experiments, we demonstrate the effectiveness of our approach in efficiently managing memory during latent optimization, while previous methods fall short. Our proposed approach not only minimizes memory requirements but also significantly enhances T2V performance across a range of diffusion guidance benchmarks. Code, models, and demo are available at https://titanguide.github.io.

</details>


### [8] [AniMer+: Unified Pose and Shape Estimation Across Mammalia and Aves via Family-Aware Transformer](https://arxiv.org/abs/2508.00298)
*Jin Lyu,Liang An,Li Lin,Pujin Cheng,Yebin Liu,Xiaoying Tang*

Main category: cs.CV

TL;DR: AniMer+是一个扩展的统一框架，用于重建哺乳动物和鸟类的姿态与形状，通过高容量的ViT和MoE设计实现高效学习，并利用扩散模型生成合成数据以解决数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 在基础模型时代，通过单一网络统一理解不同动态对象的需求日益增长，同时生物研究中跨物种的姿态与形状准确估计也至关重要。

Method: AniMer+采用高容量、家族感知的ViT结合MoE设计，将网络层分为物种特定和共享部分，并通过扩散模型生成合成数据集CtrlAni3D和CtrlAVES3D。

Result: 在41.3k哺乳动物和12.4k鸟类图像（含合成数据）上训练后，AniMer+在多个基准测试中表现优于现有方法，包括挑战性的Animal Kingdom数据集。

Conclusion: AniMer+的网络架构和合成数据集显著提升了实际应用性能，解决了数据稀缺和网络容量不足的问题。

Abstract: In the era of foundation models, achieving a unified understanding of different dynamic objects through a single network has the potential to empower stronger spatial intelligence. Moreover, accurate estimation of animal pose and shape across diverse species is essential for quantitative analysis in biological research. However, this topic remains underexplored due to the limited network capacity of previous methods and the scarcity of comprehensive multi-species datasets. To address these limitations, we introduce AniMer+, an extended version of our scalable AniMer framework. In this paper, we focus on a unified approach for reconstructing mammals (mammalia) and birds (aves). A key innovation of AniMer+ is its high-capacity, family-aware Vision Transformer (ViT) incorporating a Mixture-of-Experts (MoE) design. Its architecture partitions network layers into taxa-specific components (for mammalia and aves) and taxa-shared components, enabling efficient learning of both distinct and common anatomical features within a single model. To overcome the critical shortage of 3D training data, especially for birds, we introduce a diffusion-based conditional image generation pipeline. This pipeline produces two large-scale synthetic datasets: CtrlAni3D for quadrupeds and CtrlAVES3D for birds. To note, CtrlAVES3D is the first large-scale, 3D-annotated dataset for birds, which is crucial for resolving single-view depth ambiguities. Trained on an aggregated collection of 41.3k mammalian and 12.4k avian images (combining real and synthetic data), our method demonstrates superior performance over existing approaches across a wide range of benchmarks, including the challenging out-of-domain Animal Kingdom dataset. Ablation studies confirm the effectiveness of both our novel network architecture and the generated synthetic datasets in enhancing real-world application performance.

</details>


### [9] [Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via Motion Sequence](https://arxiv.org/abs/2508.00299)
*Danzhen Fu,Jiagao Hu,Daiguo Zhou,Fei Wang,Zepeng Wang,Wenhua Liao*

Main category: cs.CV

TL;DR: 提出了一种多视角行人视频编辑框架，通过视频修复和动作控制技术增强自动驾驶数据集的多样性。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶系统中行人检测模型因训练数据缺乏危险场景而鲁棒性不足的问题。

Method: 识别多视角行人区域，扩展检测框并拼接为统一画布，通过姿态序列控制条件进行行人编辑。

Result: 实验表明该方法能高质量完成行人编辑，具有视觉真实感、时空一致性和跨视角一致性。

Conclusion: 该方法为多视角行人视频生成提供了鲁棒且通用的解决方案，适用于数据增强和场景模拟。

Abstract: Pedestrian detection models in autonomous driving systems often lack robustness due to insufficient representation of dangerous pedestrian scenarios in training datasets. To address this limitation, we present a novel framework for controllable pedestrian video editing in multi-view driving scenarios by integrating video inpainting and human motion control techniques. Our approach begins by identifying pedestrian regions of interest across multiple camera views, expanding detection bounding boxes with a fixed ratio, and resizing and stitching these regions into a unified canvas while preserving cross-view spatial relationships. A binary mask is then applied to designate the editable area, within which pedestrian editing is guided by pose sequence control conditions. This enables flexible editing functionalities, including pedestrian insertion, replacement, and removal. Extensive experiments demonstrate that our framework achieves high-quality pedestrian editing with strong visual realism, spatiotemporal coherence, and cross-view consistency. These results establish the proposed method as a robust and versatile solution for multi-view pedestrian video generation, with broad potential for applications in data augmentation and scenario simulation in autonomous driving.

</details>


### [10] [Exploring Fourier Prior and Event Collaboration for Low-Light Image Enhancement](https://arxiv.org/abs/2508.00308)
*Chunyan She,Fujun Han,Chengyu Fang,Shukai Duan,Lidan Wang*

Main category: cs.CV

TL;DR: 论文提出了一种两阶段低光图像增强方法，利用事件相机的高动态范围和低延迟特性，分别进行可见性恢复和结构细化。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分利用事件相机和帧相机的模态优势，限制了性能提升。

Method: 分两阶段：1) 基于傅里叶空间幅度-相位纠缠的可见性恢复网络；2) 动态对齐融合策略细化结构信息。

Result: 实验表明，该方法优于现有最优模型。

Conclusion: 通过分阶段处理和动态对齐，有效提升了低光图像增强性能。

Abstract: The event camera, benefiting from its high dynamic range and low latency, provides performance gain for low-light image enhancement. Unlike frame-based cameras, it records intensity changes with extremely high temporal resolution, capturing sufficient structure information. Currently, existing event-based methods feed a frame and events directly into a single model without fully exploiting modality-specific advantages, which limits their performance. Therefore, by analyzing the role of each sensing modality, the enhancement pipeline is decoupled into two stages: visibility restoration and structure refinement. In the first stage, we design a visibility restoration network with amplitude-phase entanglement by rethinking the relationship between amplitude and phase components in Fourier space. In the second stage, a fusion strategy with dynamic alignment is proposed to mitigate the spatial mismatch caused by the temporal resolution discrepancy between two sensing modalities, aiming to refine the structure information of the image enhanced by the visibility restoration network. In addition, we utilize spatial-frequency interpolation to simulate negative samples with diverse illumination, noise and artifact degradations, thereby developing a contrastive loss that encourages the model to learn discriminative representations. Experiments demonstrate that the proposed method outperforms state-of-the-art models.

</details>


### [11] [GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2508.00312)
*Suhang Cai,Xiaohao Peng,Chong Wang,Xiaojie Cai,Jiangbo Qian*

Main category: cs.CV

TL;DR: 提出GV-VAD框架，利用文本条件视频生成模型生成可控的合成视频，增强训练数据，提升视频异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决真实异常数据稀缺、标注成本高的问题，提升模型的性能和泛化能力。

Method: 结合文本条件视频生成模型生成合成视频，并采用合成样本损失缩放策略优化训练。

Result: 在UCF-Crime数据集上优于现有方法。

Conclusion: GV-VAD框架有效提升了视频异常检测的性能和泛化能力。

Abstract: Video anomaly detection (VAD) plays a critical role in public safety applications such as intelligent surveillance. However, the rarity, unpredictability, and high annotation cost of real-world anomalies make it difficult to scale VAD datasets, which limits the performance and generalization ability of existing models. To address this challenge, we propose a generative video-enhanced weakly-supervised video anomaly detection (GV-VAD) framework that leverages text-conditioned video generation models to produce semantically controllable and physically plausible synthetic videos. These virtual videos are used to augment training data at low cost. In addition, a synthetic sample loss scaling strategy is utilized to control the influence of generated synthetic samples for efficient training. The experiments show that the proposed framework outperforms state-of-the-art methods on UCF-Crime datasets. The code is available at https://github.com/Sumutan/GV-VAD.git.

</details>


### [12] [Steering Guidance for Personalized Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.00319)
*Sunghyun Park,Seokeon Choi,Hyoungwoo Park,Sungrack Yun*

Main category: cs.CV

TL;DR: 提出了一种新的个性化引导方法，通过弱模型和权重插值动态控制输出，平衡目标分布和文本对齐。


<details>
  <summary>Details</summary>
Motivation: 解决现有引导方法（如CFG和AG）在个性化文本到图像扩散模型中无法平衡目标分布和文本对齐的问题。

Method: 利用未学习的弱模型和空文本提示，动态控制权重插值以平衡输出。

Result: 实验表明，该方法能提升文本对齐和目标分布保真度，且无需额外计算开销。

Conclusion: 提出的个性化引导方法简单有效，适用于多种微调策略。

Abstract: Personalizing text-to-image diffusion models is crucial for adapting the pre-trained models to specific target concepts, enabling diverse image generation. However, fine-tuning with few images introduces an inherent trade-off between aligning with the target distribution (e.g., subject fidelity) and preserving the broad knowledge of the original model (e.g., text editability). Existing sampling guidance methods, such as classifier-free guidance (CFG) and autoguidance (AG), fail to effectively guide the output toward well-balanced space: CFG restricts the adaptation to the target distribution, while AG compromises text alignment. To address these limitations, we propose personalization guidance, a simple yet effective method leveraging an unlearned weak model conditioned on a null text prompt. Moreover, our method dynamically controls the extent of unlearning in a weak model through weight interpolation between pre-trained and fine-tuned models during inference. Unlike existing guidance methods, which depend solely on guidance scales, our method explicitly steers the outputs toward a balanced latent space without additional computational overhead. Experimental results demonstrate that our proposed guidance can improve text alignment and target distribution fidelity, integrating seamlessly with various fine-tuning strategies.

</details>


### [13] [Spectral Sensitivity Estimation with an Uncalibrated Diffraction Grating](https://arxiv.org/abs/2508.00330)
*Lilika Makabe,Hiroaki Santo,Fumio Okura,Michael S. Brown,Yasuyuki Matsushita*

Main category: cs.CV

TL;DR: 提出一种基于衍射光栅的相机光谱灵敏度校准方法，无需专用设备，仅需普通光栅片即可实现高精度校准。


<details>
  <summary>Details</summary>
Motivation: 相机光谱灵敏度校准对计算机视觉任务（如色彩校正、光照估计和材质分析）至关重要，现有方法依赖专用设备或已知光谱反射率的目标，限制了实用性。

Method: 通过捕获直接光照及其通过光栅片的衍射图案图像，以闭合形式同时估计相机光谱灵敏度和光栅参数。

Result: 在合成和真实数据上的实验表明，该方法优于传统的基于参考目标的方法。

Conclusion: 该方法高效实用，为相机光谱灵敏度校准提供了一种更便捷的解决方案。

Abstract: This paper introduces a practical and accurate calibration method for camera spectral sensitivity using a diffraction grating. Accurate calibration of camera spectral sensitivity is crucial for various computer vision tasks, including color correction, illumination estimation, and material analysis. Unlike existing approaches that require specialized narrow-band filters or reference targets with known spectral reflectances, our method only requires an uncalibrated diffraction grating sheet, readily available off-the-shelf. By capturing images of the direct illumination and its diffracted pattern through the grating sheet, our method estimates both the camera spectral sensitivity and the diffraction grating parameters in a closed-form manner. Experiments on synthetic and real-world data demonstrate that our method outperforms conventional reference target-based methods, underscoring its effectiveness and practicality.

</details>


### [14] [SparseRecon: Neural Implicit Surface Reconstruction from Sparse Views with Feature and Depth Consistencies](https://arxiv.org/abs/2508.00366)
*Liang Han,Xu Zhang,Haichuan Song,Kanle Shi,Yu-Shen Liu,Zhizhong Han*

Main category: cs.CV

TL;DR: SparseRecon提出了一种新的稀疏视图神经隐式重建方法，结合体积渲染特征一致性和不确定性引导深度约束，解决了现有方法在未见视图上泛化能力差和重建质量受限的问题。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图重建的现有方法（泛化型和过拟合型）分别存在泛化能力不足和重建质量受限的问题，需要一种新方法来解决这些局限性。

Method: SparseRecon通过体积渲染特征一致性损失和不确定性引导深度约束，优化神经隐式场，提升稀疏视图下的重建质量和几何细节。

Result: 实验表明，SparseRecon在稀疏视图输入下优于现有方法，尤其是在视图重叠较少的场景中，能生成高质量的几何重建。

Conclusion: SparseRecon通过特征一致性和深度约束，显著提升了稀疏视图重建的质量和泛化能力。

Abstract: Surface reconstruction from sparse views aims to reconstruct a 3D shape or scene from few RGB images. The latest methods are either generalization-based or overfitting-based. However, the generalization-based methods do not generalize well on views that were unseen during training, while the reconstruction quality of overfitting-based methods is still limited by the limited geometry clues. To address this issue, we propose SparseRecon, a novel neural implicit reconstruction method for sparse views with volume rendering-based feature consistency and uncertainty-guided depth constraint. Firstly, we introduce a feature consistency loss across views to constrain the neural implicit field. This design alleviates the ambiguity caused by insufficient consistency information of views and ensures completeness and smoothness in the reconstruction results. Secondly, we employ an uncertainty-guided depth constraint to back up the feature consistency loss in areas with occlusion and insignificant features, which recovers geometry details for better reconstruction quality. Experimental results demonstrate that our method outperforms the state-of-the-art methods, which can produce high-quality geometry with sparse-view input, especially in the scenarios with small overlapping views. Project page: https://hanl2010.github.io/SparseRecon/.

</details>


### [15] [Video Forgery Detection with Optical Flow Residuals and Spatial-Temporal Consistency](https://arxiv.org/abs/2508.00397)
*Xi Xue,Kunio Suzuki,Nabarun Goswami,Takuya Shintate*

Main category: cs.CV

TL;DR: 提出了一种基于RGB外观特征和光流残差的双分支检测框架，用于检测AI生成视频中的伪造内容。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型生成的视频越来越逼真，现有方法难以捕捉细微的时间不一致性，因此需要更有效的检测方法。

Method: 采用双分支架构，分别分析RGB帧和光流残差，以检测外观和运动异常。

Result: 在多种生成模型上的实验表明，该方法具有鲁棒性和强泛化能力。

Conclusion: 通过结合空间和时间一致性特征，该方法能有效检测多种伪造视频。

Abstract: The rapid advancement of diffusion-based video generation models has led to increasingly realistic synthetic content, presenting new challenges for video forgery detection. Existing methods often struggle to capture fine-grained temporal inconsistencies, particularly in AI-generated videos with high visual fidelity and coherent motion. In this work, we propose a detection framework that leverages spatial-temporal consistency by combining RGB appearance features with optical flow residuals. The model adopts a dual-branch architecture, where one branch analyzes RGB frames to detect appearance-level artifacts, while the other processes flow residuals to reveal subtle motion anomalies caused by imperfect temporal synthesis. By integrating these complementary features, the proposed method effectively detects a wide range of forged videos. Extensive experiments on text-to-video and image-to-video tasks across ten diverse generative models demonstrate the robustness and strong generalization ability of the proposed approach.

</details>


### [16] [PMR: Physical Model-Driven Multi-Stage Restoration of Turbulent Dynamic Videos](https://arxiv.org/abs/2508.00406)
*Tao Wu,Jingyuan Ye,Ying Fu*

Main category: cs.CV

TL;DR: 论文提出了一种动态效率指数（DEI）和多阶段视频恢复框架（PMR），用于解决大气湍流导致的视频失真和模糊问题。


<details>
  <summary>Details</summary>
Motivation: 大气湍流导致的长距离动态场景视频质量下降，现有方法难以恢复边缘细节和消除混合失真，尤其是在强湍流和复杂动态条件下。

Method: 提出动态效率指数（DEI）量化视频动态强度，并设计物理模型驱动的多阶段视频恢复框架（PMR），包括去倾斜、运动分割增强和去模糊三个阶段。

Result: 实验表明，该方法有效抑制运动拖尾伪影，恢复边缘细节，并在高湍流和复杂动态的真实场景中表现出强泛化能力。

Conclusion: 该方法为高动态湍流条件下的视频恢复提供了高效且高质量的解决方案，代码和数据集将公开。

Abstract: Geometric distortions and blurring caused by atmospheric turbulence degrade the quality of long-range dynamic scene videos. Existing methods struggle with restoring edge details and eliminating mixed distortions, especially under conditions of strong turbulence and complex dynamics. To address these challenges, we introduce a Dynamic Efficiency Index ($DEI$), which combines turbulence intensity, optical flow, and proportions of dynamic regions to accurately quantify video dynamic intensity under varying turbulence conditions and provide a high-dynamic turbulence training dataset. Additionally, we propose a Physical Model-Driven Multi-Stage Video Restoration ($PMR$) framework that consists of three stages: \textbf{de-tilting} for geometric stabilization, \textbf{motion segmentation enhancement} for dynamic region refinement, and \textbf{de-blurring} for quality restoration. $PMR$ employs lightweight backbones and stage-wise joint training to ensure both efficiency and high restoration quality. Experimental results demonstrate that the proposed method effectively suppresses motion trailing artifacts, restores edge details and exhibits strong generalization capability, especially in real-world scenarios characterized by high-turbulence and complex dynamics. We will make the code and datasets openly available.

</details>


### [17] [Sortblock: Similarity-Aware Feature Reuse for Diffusion Model](https://arxiv.org/abs/2508.00412)
*Hanqi Chen,Xu Zhang,Xiaoliu Guan,Lielin Jiang,Guanzhong Wang,Zeyu Chen,Yi Liu*

Main category: cs.CV

TL;DR: Sortblock是一种无需训练的推理加速框架，通过动态缓存块级特征和选择性跳过冗余计算，显著提升DiTs的推理速度，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: DiTs的序列去噪过程导致高推理延迟，限制了实时应用。现有方法忽略了去噪阶段和Transformer块的语义变化。

Method: 提出Sortblock框架，动态缓存块级特征，基于相邻时间步的相似性排序，自适应确定重计算比例，并结合线性预测减少误差。

Result: 实验表明，Sortblock在多种任务和DiT架构中实现超过2倍的推理加速，且输出质量几乎无损。

Conclusion: Sortblock为扩散生成模型提供了一种高效且通用的加速解决方案。

Abstract: Diffusion Transformers (DiTs) have demonstrated remarkable generative capabilities, particularly benefiting from Transformer architectures that enhance visual and artistic fidelity. However, their inherently sequential denoising process results in high inference latency, limiting their deployment in real-time scenarios. Existing training-free acceleration approaches typically reuse intermediate features at fixed timesteps or layers, overlooking the evolving semantic focus across denoising stages and Transformer blocks.To address this, we propose Sortblock, a training-free inference acceleration framework that dynamically caches block-wise features based on their similarity across adjacent timesteps. By ranking the evolution of residuals, Sortblock adaptively determines a recomputation ratio, selectively skipping redundant computations while preserving generation quality. Furthermore, we incorporate a lightweight linear prediction mechanism to reduce accumulated errors in skipped blocks.Extensive experiments across various tasks and DiT architectures demonstrate that Sortblock achieves over 2$\times$ inference speedup with minimal degradation in output quality, offering an effective and generalizable solution for accelerating diffusion-based generative models.

</details>


### [18] [DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured Latent Space](https://arxiv.org/abs/2508.00413)
*Junyu Chen,Dongyun Zou,Wenkun He,Junsong Chen,Enze Xie,Song Han,Han Cai*

Main category: cs.CV

TL;DR: DC-AE 1.5是一种新型深度压缩自编码器，通过结构化潜在空间和增强扩散训练解决了高分辨率扩散模型中的收敛问题，提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 增加自编码器的潜在通道数可提高重建质量，但会导致扩散模型收敛缓慢，影响生成质量。

Method: 采用结构化潜在空间和增强扩散训练策略，前者对潜在空间施加通道结构，后者通过额外扩散目标加速收敛。

Result: DC-AE 1.5在ImageNet 512x512上比DC-AE生成质量更好且快4倍。

Conclusion: DC-AE 1.5通过创新方法解决了潜在扩散模型的质量上限问题，适用于更高空间压缩比的自编码器。

Abstract: We present DC-AE 1.5, a new family of deep compression autoencoders for high-resolution diffusion models. Increasing the autoencoder's latent channel number is a highly effective approach for improving its reconstruction quality. However, it results in slow convergence for diffusion models, leading to poorer generation quality despite better reconstruction quality. This issue limits the quality upper bound of latent diffusion models and hinders the employment of autoencoders with higher spatial compression ratios. We introduce two key innovations to address this challenge: i) Structured Latent Space, a training-based approach to impose a desired channel-wise structure on the latent space with front latent channels capturing object structures and latter latent channels capturing image details; ii) Augmented Diffusion Training, an augmented diffusion training strategy with additional diffusion training objectives on object latent channels to accelerate convergence. With these techniques, DC-AE 1.5 delivers faster convergence and better diffusion scaling results than DC-AE. On ImageNet 512x512, DC-AE-1.5-f64c128 delivers better image generation quality than DC-AE-f32c32 while being 4x faster. Code: https://github.com/dc-ai-projects/DC-Gen.

</details>


### [19] [IN2OUT: Fine-Tuning Video Inpainting Model for Video Outpainting Using Hierarchical Discriminator](https://arxiv.org/abs/2508.00418)
*Sangwoo Youn,Minji Lee,Nokap Tony Park,Yeonggyoo Jeon,Taeyoung Na*

Main category: cs.CV

TL;DR: 论文提出了一种基于视频修复模型的视频外绘方法，通过改进判别器设计和损失函数，显著提升了外绘效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频外绘方法主要关注背景生成，而忽略了对象流动和重建的一致性，导致效果模糊。

Method: 采用分层判别器，区分全局和局部对抗训练目标，并设计专用外绘损失函数。

Result: 方法在定量和定性上均优于现有技术。

Conclusion: 通过改进判别器和损失函数，实现了更高质量的视频外绘。

Abstract: Video outpainting presents a unique challenge of extending the borders while maintaining consistency with the given content. In this paper, we suggest the use of video inpainting models that excel in object flow learning and reconstruction in outpainting rather than solely generating the background as in existing methods. However, directly applying or fine-tuning inpainting models to outpainting has shown to be ineffective, often leading to blurry results. Our extensive experiments on discriminator designs reveal that a critical component missing in the outpainting fine-tuning process is a discriminator capable of effectively assessing the perceptual quality of the extended areas. To tackle this limitation, we differentiate the objectives of adversarial training into global and local goals and introduce a hierarchical discriminator that meets both objectives. Additionally, we develop a specialized outpainting loss function that leverages both local and global features of the discriminator. Fine-tuning on this adversarial loss function enhances the generator's ability to produce both visually appealing and globally coherent outpainted scenes. Our proposed method outperforms state-of-the-art methods both quantitatively and qualitatively. Supplementary materials including the demo video and the code are available in SigPort.

</details>


### [20] [Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting](https://arxiv.org/abs/2508.00427)
*Seunggeun Chi,Enna Sachdeva,Pin-Hao Huang,Kwonjoon Lee*

Main category: cs.CV

TL;DR: 提出了一种结合物理先验知识和多区域修复技术的新方法，用于动态场景中的人-物交互（HOI）遮挡补全，显著提升了生成结果的准确性和真实性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如预训练扩散模型）在动态HOI场景中难以生成合理的遮挡补全结果，因其对HOI的理解有限。

Method: 利用人类拓扑和接触信息的物理约束，定义主次区域，并在扩散模型中采用定制化的去噪策略进行多区域修复。

Result: 实验表明，该方法在HOI场景中显著优于现有方法，且无需真实接触标注仍具鲁棒性。

Conclusion: 该方法提升了机器对动态环境的感知能力，适用于3D重建和新视角/姿态合成等任务。

Abstract: Amodal completion, which is the process of inferring the full appearance of objects despite partial occlusions, is crucial for understanding complex human-object interactions (HOI) in computer vision and robotics. Existing methods, such as those that use pre-trained diffusion models, often struggle to generate plausible completions in dynamic scenarios because they have a limited understanding of HOI. To solve this problem, we've developed a new approach that uses physical prior knowledge along with a specialized multi-regional inpainting technique designed for HOI. By incorporating physical constraints from human topology and contact information, we define two distinct regions: the primary region, where occluded object parts are most likely to be, and the secondary region, where occlusions are less probable. Our multi-regional inpainting method uses customized denoising strategies across these regions within a diffusion model. This improves the accuracy and realism of the generated completions in both their shape and visual detail. Our experimental results show that our approach significantly outperforms existing methods in HOI scenarios, moving machine perception closer to a more human-like understanding of dynamic environments. We also show that our pipeline is robust even without ground-truth contact annotations, which broadens its applicability to tasks like 3D reconstruction and novel view/pose synthesis.

</details>


### [21] [PIF-Net: Ill-Posed Prior Guided Multispectral and Hyperspectral Image Fusion via Invertible Mamba and Fusion-Aware LoRA](https://arxiv.org/abs/2508.00453)
*Baisong Li,Xingwang Wang,Haixiao Xu*

Main category: cs.CV

TL;DR: PIF-Net是一个新的多光谱和高光谱图像融合框架，通过引入不适定先验和可逆Mamba架构，解决了数据对齐问题，并在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多光谱和高光谱图像融合任务由于光谱与空间信息的固有权衡以及观测数据有限，具有不适定性。现有研究未能有效解决数据不对齐问题。

Method: 提出PIF-Net框架，结合不适定先验，采用可逆Mamba架构平衡全局光谱建模与计算效率，并引入动态校准的Fusion-Aware Low-Rank Adaptation模块。

Result: 在多个基准数据集上，PIF-Net的图像恢复性能显著优于当前最优方法，同时保持模型高效。

Conclusion: PIF-Net通过创新的架构和模块设计，有效解决了多光谱和高光谱图像融合中的不适定问题，提升了性能与效率。

Abstract: The goal of multispectral and hyperspectral image fusion (MHIF) is to generate high-quality images that simultaneously possess rich spectral information and fine spatial details. However, due to the inherent trade-off between spectral and spatial information and the limited availability of observations, this task is fundamentally ill-posed. Previous studies have not effectively addressed the ill-posed nature caused by data misalignment. To tackle this challenge, we propose a fusion framework named PIF-Net, which explicitly incorporates ill-posed priors to effectively fuse multispectral images and hyperspectral images. To balance global spectral modeling with computational efficiency, we design a method based on an invertible Mamba architecture that maintains information consistency during feature transformation and fusion, ensuring stable gradient flow and process reversibility. Furthermore, we introduce a novel fusion module called the Fusion-Aware Low-Rank Adaptation module, which dynamically calibrates spectral and spatial features while keeping the model lightweight. Extensive experiments on multiple benchmark datasets demonstrate that PIF-Net achieves significantly better image restoration performance than current state-of-the-art methods while maintaining model efficiency.

</details>


### [22] [Semantic and Temporal Integration in Latent Diffusion Space for High-Fidelity Video Super-Resolution](https://arxiv.org/abs/2508.00471)
*Yiwen Wang,Xinning Chai,Yuhong Zhang,Zhengxue Cheng,Jun Zhao,Rong Xie,Li Song*

Main category: cs.CV

TL;DR: 提出了一种结合语义和时空引导的视频超分辨率方法（SeTe-VSR），通过潜在扩散空间实现细节恢复和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有视频超分辨率模型在生成过程中难以同时实现高保真对齐和时间一致性。

Method: 引入语义和时空引导，结合潜在扩散空间，平衡细节恢复和时间一致性。

Result: SeTe-VSR在细节恢复和感知质量上优于现有方法。

Conclusion: SeTe-VSR有效解决了复杂视频超分辨率任务中的挑战。

Abstract: Recent advancements in video super-resolution (VSR) models have demonstrated impressive results in enhancing low-resolution videos. However, due to limitations in adequately controlling the generation process, achieving high fidelity alignment with the low-resolution input while maintaining temporal consistency across frames remains a significant challenge. In this work, we propose Semantic and Temporal Guided Video Super-Resolution (SeTe-VSR), a novel approach that incorporates both semantic and temporal-spatio guidance in the latent diffusion space to address these challenges. By incorporating high-level semantic information and integrating spatial and temporal information, our approach achieves a seamless balance between recovering intricate details and ensuring temporal coherence. Our method not only preserves high-reality visual content but also significantly enhances fidelity. Extensive experiments demonstrate that SeTe-VSR outperforms existing methods in terms of detail recovery and perceptual quality, highlighting its effectiveness for complex video super-resolution tasks.

</details>


### [23] [LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer](https://arxiv.org/abs/2508.00477)
*Yuzhuo Chen,Zehua Ma,Jianhua Wang,Kai Kang,Shunyu Yao,Weiming Zhang*

Main category: cs.CV

TL;DR: LAMIC是一个无需训练的布局感知多图像合成框架，通过两种注意力机制（GIA和RMA）实现多参考场景下的图像生成，并在多个指标上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决多参考图像合成中保持空间布局一致性和连贯性的挑战。

Method: 基于MMDiT模型，引入Group Isolation Attention (GIA)和Region-Modulated Attention (RMA)两种注意力机制，无需额外训练。

Result: 在ID-S、BG-S、IN-R和AVG等指标上优于现有基线，展示了零样本泛化能力。

Conclusion: LAMIC为可控多图像合成提供了一种新的无需训练范式，具有强大的零样本泛化能力。

Abstract: In controllable image synthesis, generating coherent and consistent images from multiple references with spatial layout awareness remains an open challenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework that, for the first time, extends single-reference diffusion models to multi-reference scenarios in a training-free manner. Built upon the MMDiT model, LAMIC introduces two plug-and-play attention mechanisms: 1) Group Isolation Attention (GIA) to enhance entity disentanglement; and 2) Region-Modulated Attention (RMA) to enable layout-aware generation. To comprehensively evaluate model capabilities, we further introduce three metrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout control; and 2) Background Similarity (BG-S) for measuring background consistency. Extensive experiments show that LAMIC achieves state-of-the-art performance across most major metrics: it consistently outperforms existing multi-reference baselines in ID-S, BG-S, IN-R and AVG scores across all settings, and achieves the best DPG in complex composition tasks. These results demonstrate LAMIC's superior abilities in identity keeping, background preservation, layout control, and prompt-following, all achieved without any training or fine-tuning, showcasing strong zero-shot generalization ability. By inheriting the strengths of advanced single-reference models and enabling seamless extension to multi-image scenarios, LAMIC establishes a new training-free paradigm for controllable multi-image composition. As foundation models continue to evolve, LAMIC's performance is expected to scale accordingly. Our implementation is available at: https://github.com/Suchenl/LAMIC.

</details>


### [24] [Video Color Grading via Look-Up Table Generation](https://arxiv.org/abs/2508.00548)
*Seunghyun Shin,Dongmin Shin,Jisu Shin,Hae-Gon Jeon,Joon-Young Lee*

Main category: cs.CV

TL;DR: 本文提出了一种基于参考的视频色彩分级框架，通过扩散模型生成查找表（LUT）实现色彩属性对齐，并结合用户偏好文本提示进行低级特征增强。


<details>
  <summary>Details</summary>
Motivation: 视频色彩分级通常需要专业技能，本文旨在简化这一过程，使其更易于非专业人士操作。

Method: 使用扩散模型生成LUT以对齐参考场景与输入视频的色彩属性，并结合文本提示调整对比度和亮度等低级特征。

Result: 实验和用户研究表明，该方法能有效实现视频色彩分级，且不损失结构细节。

Conclusion: 提出的框架在视频色彩分级中表现出色，代码已开源。

Abstract: Different from color correction and transfer, color grading involves adjusting colors for artistic or storytelling purposes in a video, which is used to establish a specific look or mood. However, due to the complexity of the process and the need for specialized editing skills, video color grading remains primarily the domain of professional colorists. In this paper, we present a reference-based video color grading framework. Our key idea is explicitly generating a look-up table (LUT) for color attribute alignment between reference scenes and input video via a diffusion model. As a training objective, we enforce that high-level features of the reference scenes like look, mood, and emotion should be similar to that of the input video. Our LUT-based approach allows for color grading without any loss of structural details in the whole video frames as well as achieving fast inference. We further build a pipeline to incorporate a user-preference via text prompts for low-level feature enhancement such as contrast and brightness, etc. Experimental results, including extensive user studies, demonstrate the effectiveness of our approach for video color grading. Codes are publicly available at https://github.com/seunghyuns98/VideoColorGrading.

</details>


### [25] [DBLP: Noise Bridge Consistency Distillation For Efficient And Reliable Adversarial Purification](https://arxiv.org/abs/2508.00552)
*Chihan Huang,Belal Alsinglawi,Islam Al-qudah*

Main category: cs.CV

TL;DR: 提出了一种名为DBLP的高效扩散对抗净化方法，通过噪声桥蒸馏和自适应语义增强实现快速且高质量的净化。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络易受对抗扰动攻击，现有扩散净化方法计算成本高，难以实际部署。

Method: 采用噪声桥蒸馏目标，构建对抗噪声与干净数据的分布对齐，并结合多尺度边缘图增强语义保真度。

Result: 在多个数据集上达到SOTA鲁棒精度和图像质量，推理时间约0.2秒。

Conclusion: DBLP为实时对抗净化提供了有效解决方案。

Abstract: Recent advances in deep neural networks (DNNs) have led to remarkable success across a wide range of tasks. However, their susceptibility to adversarial perturbations remains a critical vulnerability. Existing diffusion-based adversarial purification methods often require intensive iterative denoising, severely limiting their practical deployment. In this paper, we propose Diffusion Bridge Distillation for Purification (DBLP), a novel and efficient diffusion-based framework for adversarial purification. Central to our approach is a new objective, noise bridge distillation, which constructs a principled alignment between the adversarial noise distribution and the clean data distribution within a latent consistency model (LCM). To further enhance semantic fidelity, we introduce adaptive semantic enhancement, which fuses multi-scale pyramid edge maps as conditioning input to guide the purification process. Extensive experiments across multiple datasets demonstrate that DBLP achieves state-of-the-art (SOTA) robust accuracy, superior image quality, and around 0.2s inference time, marking a significant step toward real-time adversarial purification.

</details>


### [26] [Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints](https://arxiv.org/abs/2508.00558)
*Jens U. Kreber,Joerg Stueckler*

Main category: cs.CV

TL;DR: PhysNAP是一种基于扩散模型的方法，用于生成与部分点云对齐且物理合理的铰接物体。


<details>
  <summary>Details</summary>
Motivation: 铰接物体是日常环境中重要的可交互对象，但现有方法在物理合理性和点云对齐方面存在不足。

Method: 使用SDF表示部分形状，通过点云对齐损失和非穿透、移动性约束引导扩散过程，并引入类别信息优化对齐。

Result: 在PartNet-Mobility数据集上评估，PhysNAP在约束一致性和生成能力上优于无引导扩散模型。

Conclusion: PhysNAP能有效提升铰接物体生成的物理合理性和点云对齐效果，同时平衡生成能力。

Abstract: Articulated objects are an important type of interactable objects in everyday environments. In this paper, we propose PhysNAP, a novel diffusion model-based approach for generating articulated objects that aligns them with partial point clouds and improves their physical plausibility. The model represents part shapes by signed distance functions (SDFs). We guide the reverse diffusion process using a point cloud alignment loss computed using the predicted SDFs. Additionally, we impose non-penetration and mobility constraints based on the part SDFs for guiding the model to generate more physically plausible objects. We also make our diffusion approach category-aware to further improve point cloud alignment if category information is available. We evaluate the generative ability and constraint consistency of samples generated with PhysNAP using the PartNet-Mobility dataset. We also compare it with an unguided baseline diffusion model and demonstrate that PhysNAP can improve constraint consistency and provides a tradeoff with generative ability.

</details>


### [27] [A Novel Modeling Framework and Data Product for Extended VIIRS-like Artificial Nighttime Light Image Reconstruction (1986-2024)](https://arxiv.org/abs/2508.00590)
*Yihe Tian,Kwan Man Cheng,Zhengbo Zhang,Tao Zhang,Suju Li,Dongmei Yan,Bing Xu*

Main category: cs.CV

TL;DR: 提出了一种新的夜间灯光（NTL）重建框架EVAL，解决了现有方法低估光强和结构缺失的问题，将VIIRS数据扩展到1986年，性能显著优于现有产品。


<details>
  <summary>Details</summary>
Motivation: 现有VIIRS数据始于2012年，限制了长期时间序列研究，且当前扩展方法存在光强低估和结构缺失问题。

Method: 采用两阶段重建框架：构建阶段使用分层融合解码器（HFD）提高初始重建保真度；细化阶段利用双特征细化器（DFR）结合高分辨率不透水面掩膜增强细节。

Result: EVAL产品将VIIRS数据扩展到1986年，R²从0.68提升至0.80，RMSE从1.27降至0.99，具有优异的时间一致性和社会经济参数相关性。

Conclusion: EVAL为研究社区提供了可靠的长期分析资源，公开可用。

Abstract: Artificial Night-Time Light (NTL) remote sensing is a vital proxy for quantifying the intensity and spatial distribution of human activities. Although the NPP-VIIRS sensor provides high-quality NTL observations, its temporal coverage, which begins in 2012, restricts long-term time-series studies that extend to earlier periods. Despite the progress in extending VIIRS-like NTL time-series, current methods still suffer from two significant shortcomings: the underestimation of light intensity and the structural omission. To overcome these limitations, we propose a novel reconstruction framework consisting of a two-stage process: construction and refinement. The construction stage features a Hierarchical Fusion Decoder (HFD) designed to enhance the fidelity of the initial reconstruction. The refinement stage employs a Dual Feature Refiner (DFR), which leverages high-resolution impervious surface masks to guide and enhance fine-grained structural details. Based on this framework, we developed the Extended VIIRS-like Artificial Nighttime Light (EVAL) product for China, extending the standard data record backwards by 26 years to begin in 1986. Quantitative evaluation shows that EVAL significantly outperforms existing state-of-the-art products, boosting the $\text{R}^2$ from 0.68 to 0.80 while lowering the RMSE from 1.27 to 0.99. Furthermore, EVAL exhibits excellent temporal consistency and maintains a high correlation with socioeconomic parameters, confirming its reliability for long-term analysis. The resulting EVAL dataset provides a valuable new resource for the research community and is publicly available at https://doi.org/10.11888/HumanNat.tpdc.302930.

</details>


### [28] [Wukong Framework for Not Safe For Work Detection in Text-to-Image systems](https://arxiv.org/abs/2508.00591)
*Mingrui Liu,Sixiao Zhang,Cheng Long*

Main category: cs.CV

TL;DR: 论文提出Wukong，一种基于Transformer的NSFW检测框架，利用扩散模型早期去噪步骤的中间输出，实现高效且准确的NSFW内容检测。


<details>
  <summary>Details</summary>
Motivation: 现有外部安全措施（文本过滤器和图像过滤器）存在不足：文本过滤器易受对抗攻击，图像过滤器计算成本高且延迟大。因此，需要一种更高效且准确的NSFW检测方法。

Method: Wukong利用扩散模型早期去噪步骤的中间输出，并重用U-Net预训练的交叉注意力参数，实现在扩散过程中早期检测NSFW内容。

Result: Wukong在性能上显著优于基于文本的安全措施，并与图像过滤器准确率相当，同时效率更高。

Conclusion: Wukong为T2I生成提供了一种高效且准确的NSFW检测解决方案，填补了现有方法的不足。

Abstract: Text-to-Image (T2I) generation is a popular AI-generated content (AIGC) technology enabling diverse and creative image synthesis. However, some outputs may contain Not Safe For Work (NSFW) content (e.g., violence), violating community guidelines. Detecting NSFW content efficiently and accurately, known as external safeguarding, is essential. Existing external safeguards fall into two types: text filters, which analyze user prompts but overlook T2I model-specific variations and are prone to adversarial attacks; and image filters, which analyze final generated images but are computationally costly and introduce latency. Diffusion models, the foundation of modern T2I systems like Stable Diffusion, generate images through iterative denoising using a U-Net architecture with ResNet and Transformer blocks. We observe that: (1) early denoising steps define the semantic layout of the image, and (2) cross-attention layers in U-Net are crucial for aligning text and image regions. Based on these insights, we propose Wukong, a transformer-based NSFW detection framework that leverages intermediate outputs from early denoising steps and reuses U-Net's pre-trained cross-attention parameters. Wukong operates within the diffusion process, enabling early detection without waiting for full image generation. We also introduce a new dataset containing prompts, seeds, and image-specific NSFW labels, and evaluate Wukong on this and two public benchmarks. Results show that Wukong significantly outperforms text-based safeguards and achieves comparable accuracy of image filters, while offering much greater efficiency.

</details>


### [29] [DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior](https://arxiv.org/abs/2508.00599)
*Junzhe Lu,Jing Lin,Hongkun Dou,Ailing Zeng,Yue Deng,Xian Liu,Zhongang Cai,Lei Yang,Yulun Zhang,Haoqian Wang,Ziwei Liu*

Main category: cs.CV

TL;DR: DPoser-X是一种基于扩散的3D全身人体姿态先验模型，通过扩散模型和变分扩散采样解决姿态任务，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于人体姿态的复杂性和高质量全身姿态数据集的稀缺性，构建一个通用且鲁棒的全身姿态先验模型具有挑战性。

Method: 提出DPoser-X模型，采用扩散模型作为姿态先验，引入截断时间步调度和掩码训练机制，结合全身和局部数据集。

Result: 在多个基准测试中表现优异，优于现有方法，为全身姿态先验建模设立了新标准。

Conclusion: DPoser-X展示了在全身人体姿态建模中的鲁棒性和通用性，为相关任务提供了高效解决方案。

Abstract: We present DPoser-X, a diffusion-based prior model for 3D whole-body human poses. Building a versatile and robust full-body human pose prior remains challenging due to the inherent complexity of articulated human poses and the scarcity of high-quality whole-body pose datasets. To address these limitations, we introduce a Diffusion model as body Pose prior (DPoser) and extend it to DPoser-X for expressive whole-body human pose modeling. Our approach unifies various pose-centric tasks as inverse problems, solving them through variational diffusion sampling. To enhance performance on downstream applications, we introduce a novel truncated timestep scheduling method specifically designed for pose data characteristics. We also propose a masked training mechanism that effectively combines whole-body and part-specific datasets, enabling our model to capture interdependencies between body parts while avoiding overfitting to specific actions. Extensive experiments demonstrate DPoser-X's robustness and versatility across multiple benchmarks for body, hand, face, and full-body pose modeling. Our model consistently outperforms state-of-the-art alternatives, establishing a new benchmark for whole-body human pose prior modeling.

</details>


### [30] [Minimum Data, Maximum Impact: 20 annotated samples for explainable lung nodule classification](https://arxiv.org/abs/2508.00639)
*Luisa Gallée,Catharina Silvia Lisson,Christoph Gerhard Lisson,Daniela Drees,Felix Weig,Daniel Vogele,Meinrad Beer,Michael Götz*

Main category: cs.CV

TL;DR: 提出一种生成模型方法，通过合成属性标注数据解决医学图像数据稀缺问题，提升可解释模型的性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分类模型需提供可解释性以增强临床信任，但属性标注数据稀缺限制了此类模型的应用。

Method: 改进扩散模型以生成属性标注数据，仅需少量真实标注样本（20个肺结节样本），并将其用于训练可解释模型。

Result: 合成数据显著提升模型性能，属性预测准确率提高13.4%，目标预测准确率提高1.8%。

Conclusion: 合成数据可有效解决医学图像数据稀缺问题，推动可解释模型在医学影像分析中的应用。

Abstract: Classification models that provide human-interpretable explanations enhance clinicians' trust and usability in medical image diagnosis. One research focus is the integration and prediction of pathology-related visual attributes used by radiologists alongside the diagnosis, aligning AI decision-making with clinical reasoning. Radiologists use attributes like shape and texture as established diagnostic criteria and mirroring these in AI decision-making both enhances transparency and enables explicit validation of model outputs. However, the adoption of such models is limited by the scarcity of large-scale medical image datasets annotated with these attributes. To address this challenge, we propose synthesizing attribute-annotated data using a generative model. We enhance the Diffusion Model with attribute conditioning and train it using only 20 attribute-labeled lung nodule samples from the LIDC-IDRI dataset. Incorporating its generated images into the training of an explainable model boosts performance, increasing attribute prediction accuracy by 13.4% and target prediction accuracy by 1.8% compared to training with only the small real attribute-annotated dataset. This work highlights the potential of synthetic data to overcome dataset limitations, enhancing the applicability of explainable models in medical image analysis.

</details>


### [31] [Can Large Pretrained Depth Estimation Models Help With Image Dehazing?](https://arxiv.org/abs/2508.00698)
*Hongfei Zhang,Kun Zhou,Ruizheng Wu,Jiangbo Lu*

Main category: cs.CV

TL;DR: 论文提出了一种基于预训练深度特征的图像去雾方法，通过RGB-D融合模块提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有去雾方法因架构特定设计难以适应多样场景，预训练深度特征的一致性为解决这一问题提供了可能。

Method: 研究预训练深度特征的泛化能力，提出可插拔的RGB-D融合模块，适配多种去雾架构。

Result: 多基准测试验证了方法的有效性和广泛适用性。

Conclusion: 预训练深度特征结合RGB-D模块能显著提升去雾性能，适应不同场景需求。

Abstract: Image dehazing remains a challenging problem due to the spatially varying nature of haze in real-world scenes. While existing methods have demonstrated the promise of large-scale pretrained models for image dehazing, their architecture-specific designs hinder adaptability across diverse scenarios with different accuracy and efficiency requirements. In this work, we systematically investigate the generalization capability of pretrained depth representations-learned from millions of diverse images-for image dehazing. Our empirical analysis reveals that the learned deep depth features maintain remarkable consistency across varying haze levels. Building on this insight, we propose a plug-and-play RGB-D fusion module that seamlessly integrates with diverse dehazing architectures. Extensive experiments across multiple benchmarks validate both the effectiveness and broad applicability of our approach.

</details>


### [32] [D3: Training-Free AI-Generated Video Detection Using Second-Order Features](https://arxiv.org/abs/2508.00701)
*Chende Zheng,Ruiqi suo,Chenhao Lin,Zhengyu Zhao,Le Yang,Shuai Liu,Minghui Yang,Cong Wang,Chao Shen*

Main category: cs.CV

TL;DR: 论文提出了一种基于二阶动力学分析的视频检测方法D3，用于区分真实视频与AI生成视频，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法对合成视频的时间伪影探索不足，导致检测效果有限。

Method: 通过牛顿力学下的二阶动力学分析建立理论框架，提出Second-order Central Difference特征，并开发了无需训练的检测方法D3。

Result: 在4个开源数据集上验证了D3的优越性，例如在GenVideo上比之前最佳方法提升了10.39%的平均精度。

Conclusion: D3在计算效率和鲁棒性方面表现优异，为合成视频检测提供了新思路。

Abstract: The evolution of video generation techniques, such as Sora, has made it increasingly easy to produce high-fidelity AI-generated videos, raising public concern over the dissemination of synthetic content. However, existing detection methodologies remain limited by their insufficient exploration of temporal artifacts in synthetic videos. To bridge this gap, we establish a theoretical framework through second-order dynamical analysis under Newtonian mechanics, subsequently extending the Second-order Central Difference features tailored for temporal artifact detection. Building on this theoretical foundation, we reveal a fundamental divergence in second-order feature distributions between real and AI-generated videos. Concretely, we propose Detection by Difference of Differences (D3), a novel training-free detection method that leverages the above second-order temporal discrepancies. We validate the superiority of our D3 on 4 open-source datasets (Gen-Video, VideoPhy, EvalCrafter, VidProM), 40 subsets in total. For example, on GenVideo, D3 outperforms the previous best method by 10.39% (absolute) mean Average Precision. Additional experiments on time cost and post-processing operations demonstrate D3's exceptional computational efficiency and strong robust performance. Our code is available at https://github.com/Zig-HS/D3.

</details>


### [33] [YOLO-Count: Differentiable Object Counting for Text-to-Image Generation](https://arxiv.org/abs/2508.00728)
*Guanning Zeng,Xiang Zhang,Zirui Wang,Haiyang Xu,Zeyuan Chen,Bingnan Li,Zhuowen Tu*

Main category: cs.CV

TL;DR: YOLO-Count是一个可微分的开放词汇对象计数模型，解决了通用计数问题，并为文本到图像生成提供精确的数量控制。


<details>
  <summary>Details</summary>
Motivation: 解决开放词汇计数与文本到图像生成控制之间的差距，提供更精确的对象数量估计和生成指导。

Method: 提出'基数'图作为回归目标，结合表示对齐和混合强弱监督方案，采用完全可微分架构。

Result: 实验表明YOLO-Count在计数准确性和生成控制方面达到最先进水平。

Conclusion: YOLO-Count成功实现了开放词汇计数与生成模型的精确数量控制，具有广泛的应用潜力。

Abstract: We propose YOLO-Count, a differentiable open-vocabulary object counting model that tackles both general counting challenges and enables precise quantity control for text-to-image (T2I) generation. A core contribution is the 'cardinality' map, a novel regression target that accounts for variations in object size and spatial distribution. Leveraging representation alignment and a hybrid strong-weak supervision scheme, YOLO-Count bridges the gap between open-vocabulary counting and T2I generation control. Its fully differentiable architecture facilitates gradient-based optimization, enabling accurate object count estimation and fine-grained guidance for generative models. Extensive experiments demonstrate that YOLO-Count achieves state-of-the-art counting accuracy while providing robust and effective quantity control for T2I systems.

</details>


### [34] [SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation](https://arxiv.org/abs/2508.00750)
*Prerana Ramkumar*

Main category: cs.CV

TL;DR: SU-ESRGAN是一种针对卫星图像的超分辨率框架，结合了ESRGAN、DeepLabv3分割损失和蒙特卡洛丢弃，生成像素级不确定性地图，提升了语义一致性和可信度。


<details>
  <summary>Details</summary>
Motivation: GANs在超分辨率任务中缺乏语义一致性和像素级置信度，限制了其在遥感关键应用中的可信度。

Method: 提出SU-ESRGAN框架，整合ESRGAN、DeepLabv3分割损失和蒙特卡洛丢弃，生成不确定性地图。

Result: 在PSNR、SSIM和LPIPS指标上与基线ESRGAN相当，适用于无人机和卫星系统。

Conclusion: SU-ESRGAN在跨域应用中表现良好，强调了领域感知训练的重要性。

Abstract: Generative Adversarial Networks (GANs) have achieved realistic super-resolution (SR) of images however, they lack semantic consistency and per-pixel confidence, limiting their credibility in critical remote sensing applications such as disaster response, urban planning and agriculture. This paper introduces Semantic and Uncertainty-Aware ESRGAN (SU-ESRGAN), the first SR framework designed for satellite imagery to integrate the ESRGAN, segmentation loss via DeepLabv3 for class detail preservation and Monte Carlo dropout to produce pixel-wise uncertainty maps. The SU-ESRGAN produces results (PSNR, SSIM, LPIPS) comparable to the Baseline ESRGAN on aerial imagery. This novel model is valuable in satellite systems or UAVs that use wide field-of-view (FoV) cameras, trading off spatial resolution for coverage. The modular design allows integration in UAV data pipelines for on-board or post-processing SR to enhance imagery resulting due to motion blur, compression and sensor limitations. Further, the model is fine-tuned to evaluate its performance on cross domain applications. The tests are conducted on two drone based datasets which differ in altitude and imaging perspective. Performance evaluation of the fine-tuned models show a stronger adaptation to the Aerial Maritime Drone Dataset, whose imaging characteristics align with the training data, highlighting the importance of domain-aware training in SR-applications.

</details>


### [35] [Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation](https://arxiv.org/abs/2508.00766)
*Irene Iele,Francesco Di Feola,Valerio Guarrasi,Paolo Soda*

Main category: cs.CV

TL;DR: 提出了一种动态测试时间适应（TTA）框架，用于医学图像翻译任务，通过量化域偏移和选择性调整模型特征，提升对分布外样本的处理能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有图像翻译技术在处理分布外样本时性能下降的问题。

Method: 引入重建模块量化域偏移，并使用动态适应块选择性调整预训练模型的内部特征。

Result: 在低剂量CT去噪和T1到T2 MRI翻译任务中表现优于基线模型和现有TTA方法。

Conclusion: 动态样本特异性调整是提升模型鲁棒性的有效途径。

Abstract: Image-to-image translation has emerged as a powerful technique in medical imaging, enabling tasks such as image denoising and cross-modality conversion. However, it suffers from limitations in handling out-of-distribution samples without causing performance degradation. To address this limitation, we propose a novel Test-Time Adaptation (TTA) framework that dynamically adjusts the translation process based on the characteristics of each test sample. Our method introduces a Reconstruction Module to quantify the domain shift and a Dynamic Adaptation Block that selectively modifies the internal features of a pretrained translation model to mitigate the shift without compromising the performance on in-distribution samples that do not require adaptation. We evaluate our approach on two medical image-to-image translation tasks: low-dose CT denoising and T1 to T2 MRI translation, showing consistent improvements over both the baseline translation model without TTA and prior TTA methods. Our analysis highlights the limitations of the state-of-the-art that uniformly apply the adaptation to both out-of-distribution and in-distribution samples, demonstrating that dynamic, sample-specific adjustment offers a promising path to improve model resilience in real-world scenarios. The code is available at: https://github.com/cosbidev/Sample-Aware_TTA.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [36] [Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product](https://arxiv.org/abs/2508.00230)
*Paul Albert,Frederic Z. Zhang,Hemanth Saratchandran,Anton van den Hengel,Ehsan Abbasnejad*

Main category: cs.LG

TL;DR: KRAdapter是一种新的参数高效微调方法，通过Khatri-Rao积生成权重更新，解决了LoRA在近似高有效秩矩阵时的不足，并在大规模模型上表现出色。


<details>
  <summary>Details</summary>
Motivation: LoRA在多模态和大语言模型中的局限性促使研究更高效的PEFT方法。

Method: 提出KRAdapter，利用Khatri-Rao积生成高有效秩的权重更新，并与LoRA进行定量比较。

Result: KRAdapter在1B参数的视觉语言模型和8B参数的大语言模型上表现优于LoRA，尤其在常识推理任务中。

Conclusion: KRAdapter在保持LoRA高效性的同时，提供了更强大的性能，是微调大规模模型的实用选择。

Abstract: Parameter-efficient fine-tuning (PEFT) has become a standard approach for adapting large pre-trained models. Amongst PEFT methods, low-rank adaptation (LoRA) has achieved notable success. However, recent studies have highlighted its limitations compared against full-rank alternatives, particularly when applied to multimodal and large language models. In this work, we present a quantitative comparison amongst full-rank and low-rank PEFT methods using a synthetic matrix approximation benchmark with controlled spectral properties. Our results confirm that LoRA struggles to approximate matrices with relatively flat spectrums or high frequency components -- signs of high effective ranks. To this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the Khatri-Rao product to produce weight updates, which, by construction, tends to produce matrix product with a high effective rank. We demonstrate performance gains with KRAdapter on vision-language models up to 1B parameters and on large language models up to 8B parameters, particularly on unseen common-sense reasoning tasks. In addition, KRAdapter maintains the memory and compute efficiency of LoRA, making it a practical and robust alternative to fine-tune billion-scale parameter models.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [37] [Diffusion-Based User-Guided Data Augmentation for Coronary Stenosis Detection](https://arxiv.org/abs/2508.00438)
*Sumin Seo,In Kyu Lee,Hyun-Woo Kim,Jaesik Min,Chung-Hwan Jung*

Main category: eess.IV

TL;DR: 提出了一种基于扩散模型的数据增强方法，用于生成逼真的冠状动脉狭窄病变，以解决标注数据不足和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉狭窄是缺血性心脏事件的主要风险因素，传统医疗分析耗时且劳动密集，深度学习虽具潜力但受限于数据问题。

Method: 采用扩散模型的修复方法生成用户可控制严重程度的病变，增强数据多样性。

Result: 在大规模内部和公共数据集上，该方法在病变检测和严重程度分类中表现优异，尤其在数据有限时仍保持高性能。

Conclusion: 该方法显著提升了狭窄严重程度评估的可靠性，优化了数据利用，具有重要临床意义。

Abstract: Coronary stenosis is a major risk factor for ischemic heart events leading to increased mortality, and medical treatments for this condition require meticulous, labor-intensive analysis. Coronary angiography provides critical visual cues for assessing stenosis, supporting clinicians in making informed decisions for diagnosis and treatment. Recent advances in deep learning have shown great potential for automated localization and severity measurement of stenosis. In real-world scenarios, however, the success of these competent approaches is often hindered by challenges such as limited labeled data and class imbalance. In this study, we propose a novel data augmentation approach that uses an inpainting method based on a diffusion model to generate realistic lesions, allowing user-guided control of severity. Extensive evaluation on lesion detection and severity classification across various synthetic dataset sizes shows superior performance of our method on both a large-scale in-house dataset and a public coronary angiography dataset. Furthermore, our approach maintains high detection and classification performance even when trained with limited data, highlighting its clinical importance in improving the assessment of severity of stenosis and optimizing data utilization for more reliable decision support.

</details>


### [38] [FMPlug: Plug-In Foundation Flow-Matching Priors for Inverse Problems](https://arxiv.org/abs/2508.00721)
*Yuxiang Wan,Ryan Devera,Wenjie Zhang,Ju Sun*

Main category: eess.IV

TL;DR: FMPlug是一种新型插件框架，通过利用观察对象与目标对象的相似性以及生成流的高斯性，增强基础流匹配先验，解决不适定逆问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖领域特定或无训练先验，FMPlug旨在通过通用基础模型更高效地解决逆问题。

Method: 引入时间自适应预热策略和锐利高斯性正则化，优化基础流匹配先验。

Result: 在图像超分辨率和高斯去模糊任务上显著优于现有方法。

Conclusion: FMPlug通过简单而强大的洞察力，释放了领域无关基础模型的潜力。

Abstract: We present FMPlug, a novel plug-in framework that enhances foundation flow-matching (FM) priors for solving ill-posed inverse problems. Unlike traditional approaches that rely on domain-specific or untrained priors, FMPlug smartly leverages two simple but powerful insights: the similarity between observed and desired objects and the Gaussianity of generative flows. By introducing a time-adaptive warm-up strategy and sharp Gaussianity regularization, FMPlug unlocks the true potential of domain-agnostic foundation models. Our method beats state-of-the-art methods that use foundation FM priors by significant margins, on image super-resolution and Gaussian deblurring.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [39] [Jet Image Generation in High Energy Physics Using Diffusion Models](https://arxiv.org/abs/2508.00250)
*Victor D. Martinez,Vidya Manian,Sudhir Malik*

Main category: hep-ph

TL;DR: 本文首次将扩散模型应用于生成LHC质子-质子碰撞事件的喷注图像，比较了基于分数和一致性模型的性能，发现后者在生成质量和稳定性上更优。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型在生成高能物理喷注图像中的应用，以改进计算效率和生成准确性。

Method: 将喷注的动力学变量映射为二维图像，训练扩散模型学习喷注成分的空间分布，比较分数扩散模型和一致性模型的表现。

Result: 一致性模型在生成质量和稳定性上优于分数扩散模型，FID评分更高。

Conclusion: 扩散模型为高能物理研究提供了高效且准确的喷注图像生成工具，一致性模型表现更佳。

Abstract: This article presents, for the first time, the application of diffusion models for generating jet images corresponding to proton-proton collision events at the Large Hadron Collider (LHC). The kinematic variables of quark, gluon, W-boson, Z-boson, and top quark jets from the JetNet simulation dataset are mapped to two-dimensional image representations. Diffusion models are trained on these images to learn the spatial distribution of jet constituents. We compare the performance of score-based diffusion models and consistency models in accurately generating class-conditional jet images. Unlike approaches based on latent distributions, our method operates directly in image space. The fidelity of the generated images is evaluated using several metrics, including the Fr\'echet Inception Distance (FID), which demonstrates that consistency models achieve higher fidelity and generation stability compared to score-based diffusion models. These advancements offer significant improvements in computational efficiency and generation accuracy, providing valuable tools for High Energy Physics (HEP) research.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [40] [On-Device Diffusion Transformer Policy for Efficient Robot Manipulation](https://arxiv.org/abs/2508.00697)
*Yiming Wu,Huan Wang,Zhenghao Chen,Jianxin Pang,Dong Xu*

Main category: cs.RO

TL;DR: LightDP是一个专为移动设备设计的框架，通过压缩去噪模块和减少采样步骤，加速Diffusion Policies的实时部署。


<details>
  <summary>Details</summary>
Motivation: Diffusion Policies在资源受限的移动平台上应用时存在计算效率低和内存占用大的问题。

Method: 采用网络压缩和减少采样步骤的策略，结合统一的剪枝和再训练流程以及一致性蒸馏技术。

Result: 在标准数据集上，LightDP实现了实时动作预测，性能与最先进的Diffusion Policies相当。

Conclusion: LightDP为资源受限环境中基于扩散的策略的实际部署迈出了重要一步。

Abstract: Diffusion Policies have significantly advanced robotic manipulation tasks via imitation learning, but their application on resource-constrained mobile platforms remains challenging due to computational inefficiency and extensive memory footprint. In this paper, we propose LightDP, a novel framework specifically designed to accelerate Diffusion Policies for real-time deployment on mobile devices. LightDP addresses the computational bottleneck through two core strategies: network compression of the denoising modules and reduction of the required sampling steps. We first conduct an extensive computational analysis on existing Diffusion Policy architectures, identifying the denoising network as the primary contributor to latency. To overcome performance degradation typically associated with conventional pruning methods, we introduce a unified pruning and retraining pipeline, optimizing the model's post-pruning recoverability explicitly. Furthermore, we combine pruning techniques with consistency distillation to effectively reduce sampling steps while maintaining action prediction accuracy. Experimental evaluations on the standard datasets, \ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that LightDP achieves real-time action prediction on mobile devices with competitive performance, marking an important step toward practical deployment of diffusion-based policies in resource-limited environments. Extensive real-world experiments also show the proposed LightDP can achieve performance comparable to state-of-the-art Diffusion Policies.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [41] [Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications](https://arxiv.org/abs/2508.00669)
*Wenxuan Wang,Zizhan Ma,Meidan Ding,Shiyi Zheng,Shengyuan Liu,Jie Liu,Jiaming Ji,Wenting Chen,Xiang Li,Linlin Shen,Yixuan Yuan*

Main category: cs.CL

TL;DR: 本文系统综述了大型语言模型（LLMs）在医学推理领域的发展，提出了增强推理技术的分类法，分析了不同数据模态和临床应用中的技术应用，并总结了关键挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 医学实践中系统、透明和可验证的推理是核心，但现有LLMs在此方面存在不足，因此需要开发专门用于医学推理的LLMs。

Method: 提出训练时和测试时的推理增强技术分类法，分析60项关键研究（2022-2025年），涵盖不同数据模态和临床应用。

Result: 总结了从简单准确性到复杂推理质量评估的评测基准演变，并识别了关键挑战（如忠实性与合理性差距）。

Conclusion: 未来需构建高效、稳健且社会技术责任强的医学AI，重点关注原生多模态推理等方向。

Abstract: The proliferation of Large Language Models (LLMs) in medicine has enabled impressive capabilities, yet a critical gap remains in their ability to perform systematic, transparent, and verifiable reasoning, a cornerstone of clinical practice. This has catalyzed a shift from single-step answer generation to the development of LLMs explicitly designed for medical reasoning. This paper provides the first systematic review of this emerging field. We propose a taxonomy of reasoning enhancement techniques, categorized into training-time strategies (e.g., supervised fine-tuning, reinforcement learning) and test-time mechanisms (e.g., prompt engineering, multi-agent systems). We analyze how these techniques are applied across different data modalities (text, image, code) and in key clinical applications such as diagnosis, education, and treatment planning. Furthermore, we survey the evolution of evaluation benchmarks from simple accuracy metrics to sophisticated assessments of reasoning quality and visual interpretability. Based on an analysis of 60 seminal studies from 2022-2025, we conclude by identifying critical challenges, including the faithfulness-plausibility gap and the need for native multimodal reasoning, and outlining future directions toward building efficient, robust, and sociotechnically responsible medical AI.

</details>
