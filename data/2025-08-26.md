<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 4]
- [cs.CV](#cs.CV) [Total: 53]
- [cs.RO](#cs.RO) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.HC](#cs.HC) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [DecoMind: A Generative AI System for Personalized Interior Design Layouts](https://arxiv.org/abs/2508.16696)
*Reema Alshehri,Rawan Alotaibi,Leen Almasri,Rawan Altaweel*

Main category: cs.GR

TL;DR: 基于用户输入自动生成室内设计布局的系统，使用CLIP选择家具、Stable Diffusion生成设计，并通过分类器验证设计质量


<details>
  <summary>Details</summary>
Motivation: 为用户提供自动化、个性化的室内设计方案，简化传统室内设计流程，提高设计效率和用户满意度

Method: 使用CLIP从数据集中提取相关家具，通过Stable Diffusion结合ControlNet生成包含选定家具的设计布局，最后用分类器验证设计是否符合用户输入要求

Result: 开发了一个能够根据用户偏好自动生成逼真室内设计布局的系统，实现了从家具选择到最终设计生成的全流程自动化

Conclusion: 该系统为室内设计提供了有效的自动化解决方案，能够生成符合用户需求的逼真设计，具有实际应用价值

Abstract: This paper introduces a system for generating interior design layouts based on user inputs, such as room type, style, and furniture preferences. CLIP extracts relevant furniture from a dataset, and a layout that contains furniture and a prompt are fed to Stable Diffusion with ControlNet to generate a design that incorporates the selected furniture. The design is then evaluated by classifiers to ensure alignment with the user's inputs, offering an automated solution for realistic interior design.

</details>


### [2] [A Survey of Deep Learning-based Point Cloud Denoising](https://arxiv.org/abs/2508.17011)
*Jinxi Wang,Ben Fei,Dasith de Silva Edirimuni,Zheng Liu,Ying He,Xuequan Lu*

Main category: cs.GR

TL;DR: 本文是关于深度学习点云去噪方法的综述，涵盖了监督和无监督方法，提出了功能分类法，建立了统一基准测试，并讨论了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 真实环境中获取的点云数据常因传感器、光照、材质等因素而包含噪声，影响几何保真度和下游任务性能。传统优化方法难以处理复杂噪声模式，深度学习展现出强大潜力。

Method: 从两个维度组织文献：(1)监督级别（监督vs无监督）(2)建模视角，提出基于去噪原理的功能分类法。建立统一基准测试，评估去噪质量、表面保真度、点分布和计算效率。

Result: 提供了截至2025年8月的深度学习点云去噪方法全面综述，分析了架构趋势，建立了标准化评估框架。

Conclusion: 点云去噪是基础性问题，深度学习在该领域取得显著进展，但仍存在开放挑战，需要进一步研究。

Abstract: Accurate 3D geometry acquisition is essential for a wide range of applications, such as computer graphics, autonomous driving, robotics, and augmented reality. However, raw point clouds acquired in real-world environments are often corrupted with noise due to various factors such as sensor, lighting, material, environment etc, which reduces geometric fidelity and degrades downstream performance. Point cloud denoising is a fundamental problem, aiming to recover clean point sets while preserving underlying structures. Classical optimization-based methods, guided by hand-crafted filters or geometric priors, have been extensively studied but struggle to handle diverse and complex noise patterns. Recent deep learning approaches leverage neural network architectures to learn distinctive representations and demonstrate strong outcomes, particularly on complex and large-scale point clouds. Provided these significant advances, this survey provides a comprehensive and up-to-date review of deep learning-based point cloud denoising methods up to August 2025. We organize the literature from two perspectives: (1) supervision level (supervised vs. unsupervised), and (2) modeling perspective, proposing a functional taxonomy that unifies diverse approaches by their denoising principles. We further analyze architectural trends both structurally and chronologically, establish a unified benchmark with consistent training settings, and evaluate methods in terms of denoising quality, surface fidelity, point distribution, and computational efficiency. Finally, we discuss open challenges and outline directions for future research in this rapidly evolving field.

</details>


### [3] [Generating Human-AI Collaborative Design Sequence for 3D Assets via Differentiable Operation Graph](https://arxiv.org/abs/2508.17645)
*Xiaoyang Huang,Bingbing Ni,Wenjun Zhang*

Main category: cs.GR

TL;DR: 该论文提出了一种将AI生成的3D内容与参数化设计工具连接的方法，通过可微分建模操作和分层图优化来生成设计操作序列，实现高几何保真度和行业兼容性。


<details>
  <summary>Details</summary>
Motivation: 解决AI生成内容与人类设计范式之间的表示不兼容问题，传统AI框架使用网格或神经表示，而设计师使用参数化建模工具，这种脱节降低了AI在3D行业的实用价值。

Method: 将基本建模操作（如拉伸、布尔运算）重新表述为可微分单元，通过基于梯度的学习联合优化连续和离散参数；构建具有门控机制的分层图，通过最小化Chamfer距离进行端到端优化；使用多阶段序列长度约束和领域规则惩罚实现无监督学习。

Result: 生成的操作序列实现了高几何保真度、平滑网格布线、合理的步骤组成和灵活的编辑能力，完全兼容设计行业标准。

Conclusion: 该方法成功弥合了AI生成内容与人类设计工作流之间的鸿沟，为3D行业提供了实用且高效的AI协作解决方案。

Abstract: The emergence of 3D artificial intelligence-generated content (3D-AIGC) has enabled rapid synthesis of intricate geometries. However, a fundamental disconnect persists between AI-generated content and human-centric design paradigms, rooted in representational incompatibilities: conventional AI frameworks predominantly manipulate meshes or neural representations (\emph{e.g.}, NeRF, Gaussian Splatting), while designers operate within parametric modeling tools. This disconnection diminishes the practical value of AI for 3D industry, undermining the efficiency of human-AI collaboration. To resolve this disparity, we focus on generating design operation sequences, which are structured modeling histories that comprehensively capture the step-by-step construction process of 3D assets and align with designers' typical workflows in modern 3D software. We first reformulate fundamental modeling operations (\emph{e.g.}, \emph{Extrude}, \emph{Boolean}) into differentiable units, enabling joint optimization of continuous (\emph{e.g.}, \emph{Extrude} height) and discrete (\emph{e.g.}, \emph{Boolean} type) parameters via gradient-based learning. Based on these differentiable operations, a hierarchical graph with gating mechanism is constructed and optimized end-to-end by minimizing Chamfer Distance to target geometries. Multi-stage sequence length constraint and domain rule penalties enable unsupervised learning of compact design sequences without ground-truth sequence supervision. Extensive validation demonstrates that the generated operation sequences achieve high geometric fidelity, smooth mesh wiring, rational step composition and flexible editing capacity, with full compatibility within design industry.

</details>


### [4] [MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian Splatting](https://arxiv.org/abs/2508.17811)
*Hanzhi Chang,Ruijie Zhu,Wenjie Chang,Mulin Yu,Yanzhe Liang,Jiahao Lu,Zhuoyuan Li,Tianzhu Zhang*

Main category: cs.GR

TL;DR: MeshSplat是一个基于高斯泼溅的可泛化稀疏视图表面重建框架，通过2D高斯泼溅连接新视角合成与几何先验，无需3D真值监督即可实现高质量网格重建


<details>
  <summary>Details</summary>
Motivation: 现有表面重建方法在输入视图极其稀疏时难以恢复准确的场景几何，需要解决稀疏视图下的几何重建难题

Method: 使用前馈网络预测每视图像素对齐的2D高斯泼溅，提出加权Chamfer距离损失正则化深度图，并引入法向预测网络对齐高斯泼溅方向

Result: 在可泛化稀疏视图网格重建任务中达到最先进性能，实验验证了所提改进的有效性

Conclusion: MeshSplat通过2D高斯泼溅桥接新视角合成和几何先验学习，成功解决了稀疏视图下的表面重建挑战

Abstract: Surface reconstruction has been widely studied in computer vision and graphics. However, existing surface reconstruction works struggle to recover accurate scene geometry when the input views are extremely sparse. To address this issue, we propose MeshSplat, a generalizable sparse-view surface reconstruction framework via Gaussian Splatting. Our key idea is to leverage 2DGS as a bridge, which connects novel view synthesis to learned geometric priors and then transfers these priors to achieve surface reconstruction. Specifically, we incorporate a feed-forward network to predict per-view pixel-aligned 2DGS, which enables the network to synthesize novel view images and thus eliminates the need for direct 3D ground-truth supervision. To improve the accuracy of 2DGS position and orientation prediction, we propose a Weighted Chamfer Distance Loss to regularize the depth maps, especially in overlapping areas of input views, and also a normal prediction network to align the orientation of 2DGS with normal vectors predicted by a monocular normal estimator. Extensive experiments validate the effectiveness of our proposed improvement, demonstrating that our method achieves state-of-the-art performance in generalizable sparse-view mesh reconstruction tasks. Project Page: https://hanzhichang.github.io/meshsplat_web

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [5] [Towards High-Precision Depth Sensing via Monocular-Aided iToF and RGB Integration](https://arxiv.org/abs/2508.16579)
*Yansong Du,Yutong Deng,Yuting Zhou,Feiyu Jiao,Jian Song,Xun Guan*

Main category: cs.CV

TL;DR: 提出了一种新颖的iToF-RGB融合框架，通过几何校准和双编码器网络，解决了iToF深度传感的空间分辨率低、视场窄和结构失真等问题，实现了深度超分辨率和视场扩展。


<details>
  <summary>Details</summary>
Motivation: 间接飞行时间(iToF)深度传感存在空间分辨率低、视场有限以及在复杂场景中结构失真的固有限制，需要与RGB图像融合来提升深度感知质量。

Method: 首先通过精确的几何校准将窄视场iToF深度图重投影到宽视场RGB坐标系，然后使用双编码器融合网络联合提取iToF深度和RGB图像的互补特征，并利用单目深度先验恢复精细结构细节和进行深度超分辨率。

Result: 在合成和真实数据集上的大量实验表明，该方法在准确性、结构一致性和视觉质量方面显著优于最先进的方法。

Conclusion: 该框架通过跨模态结构线索和深度一致性约束，实现了增强的深度准确性、改进的边缘锐度和无缝的视场扩展，为iToF深度感知提供了有效的解决方案。

Abstract: This paper presents a novel iToF-RGB fusion framework designed to address the inherent limitations of indirect Time-of-Flight (iToF) depth sensing, such as low spatial resolution, limited field-of-view (FoV), and structural distortion in complex scenes. The proposed method first reprojects the narrow-FoV iToF depth map onto the wide-FoV RGB coordinate system through a precise geometric calibration and alignment module, ensuring pixel-level correspondence between modalities. A dual-encoder fusion network is then employed to jointly extract complementary features from the reprojected iToF depth and RGB image, guided by monocular depth priors to recover fine-grained structural details and perform depth super-resolution. By integrating cross-modal structural cues and depth consistency constraints, our approach achieves enhanced depth accuracy, improved edge sharpness, and seamless FoV expansion. Extensive experiments on both synthetic and real-world datasets demonstrate that the proposed framework significantly outperforms state-of-the-art methods in terms of accuracy, structural consistency, and visual quality.

</details>


### [6] [CountLoop: Training-Free High-Instance Image Generation via Iterative Agent Guidance](https://arxiv.org/abs/2508.16644)
*Anindya Mondal,Ayan Banerjee,Sauradip Nag,Josep Lladós,Xiatian Zhu,Anjan Dutta*

Main category: cs.CV

TL;DR: CountLoop是一个无需训练的训练框架，通过迭代结构化反馈为扩散模型提供精确的实例控制，在复杂高密度场景中实现高达98%的计数准确性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在逼真图像合成方面表现出色，但在生成具有精确对象实例数量的场景时仍不可靠，特别是在复杂和高密度设置中。

Method: 采用训练免费框架，通过图像生成和多模态代理评估的交替过程，使用语言引导的规划器和批评器评估对象计数、空间布局和属性一致性，并引入实例驱动的注意力掩码和组合生成技术来改善对象分离。

Result: 在COCO Count、T2I CompBench和两个新高实例基准测试中，CountLoop实现了高达98%的计数准确性，同时保持空间保真度和视觉质量，以0.97的分数优于基于布局和梯度引导的基线方法。

Conclusion: CountLoop通过迭代结构化反馈有效解决了扩散模型在精确实例控制方面的局限性，为复杂场景生成提供了可靠的解决方案。

Abstract: Diffusion models have shown remarkable progress in photorealistic image synthesis, yet they remain unreliable for generating scenes with a precise number of object instances, particularly in complex and high-density settings. We present CountLoop, a training-free framework that provides diffusion models with accurate instance control through iterative structured feedback. The approach alternates between image generation and multimodal agent evaluation, where a language-guided planner and critic assess object counts, spatial arrangements, and attribute consistency. This feedback is then used to refine layouts and guide subsequent generations. To further improve separation between objects, especially in occluded scenes, we introduce instance-driven attention masking and compositional generation techniques. Experiments on COCO Count, T2I CompBench, and two new high-instance benchmarks show that CountLoop achieves counting accuracy of up to 98% while maintaining spatial fidelity and visual quality, outperforming layout-based and gradient-guided baselines with a score of 0.97.

</details>


### [7] [MSNav: Zero-Shot Vision-and-Language Navigation with Dynamic Memory and LLM Spatial Reasoning](https://arxiv.org/abs/2508.16654)
*Chenghao Liu,Zhimu Zhou,Jiachen Zhang,Minghao Zhang,Songfang Huang,Huiling Duan*

Main category: cs.CV

TL;DR: MSNav是一个用于视觉语言导航的框架，通过集成记忆、空间和决策三个模块，解决了现有方法在空间推理、跨模态对齐和长时记忆方面的缺陷，在R2R和REVERIE数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的端到端视觉语言导航方法存在空间推理能力弱、跨模态对齐差、长时任务内存过载等关键缺陷，需要系统性地解决这些问题。

Method: 提出MSNav框架，包含三个协同模块：1)记忆模块-动态地图记忆，通过选择性节点剪枝处理内存过载；2)空间模块-空间推理和物体关系推断，基于新构建的I-O-S数据集微调Qwen3-4B模型；3)决策模块-基于LLM的路径规划执行动作。

Result: 在Room-to-Room和REVERIE数据集上的实验表明，MSNav在成功率和路径长度加权成功率等指标上显著提升，达到最先进性能。Qwen-Spatial模型在物体列表提取任务上优于主流商业LLM，在I-O-S测试集上获得更高的F1和NDCG分数。

Conclusion: MSNav通过模块化架构将脆弱的推理转变为鲁棒的集成智能，有效解决了视觉语言导航中的关键挑战，为多模态导航任务提供了新的解决方案。

Abstract: Vision-and-Language Navigation (VLN) requires an agent to interpret natural language instructions and navigate complex environments. Current approaches often adopt a "black-box" paradigm, where a single Large Language Model (LLM) makes end-to-end decisions. However, it is plagued by critical vulnerabilities, including poor spatial reasoning, weak cross-modal grounding, and memory overload in long-horizon tasks. To systematically address these issues, we propose Memory Spatial Navigation(MSNav), a framework that fuses three modules into a synergistic architecture, which transforms fragile inference into a robust, integrated intelligence. MSNav integrates three modules: Memory Module, a dynamic map memory module that tackles memory overload through selective node pruning, enhancing long-range exploration; Spatial Module, a module for spatial reasoning and object relationship inference that improves endpoint recognition; and Decision Module, a module using LLM-based path planning to execute robust actions. Powering Spatial Module, we also introduce an Instruction-Object-Space (I-O-S) dataset and fine-tune the Qwen3-4B model into Qwen-Spatial (Qwen-Sp), which outperforms leading commercial LLMs in object list extraction, achieving higher F1 and NDCG scores on the I-O-S test set. Extensive experiments on the Room-to-Room (R2R) and REVERIE datasets demonstrate MSNav's state-of-the-art performance with significant improvements in Success Rate (SR) and Success weighted by Path Length (SPL).

</details>


### [8] [A Framework for Benchmarking Fairness-Utility Trade-offs in Text-to-Image Models via Pareto Frontiers](https://arxiv.org/abs/2508.16752)
*Marco N. Bochernitsan,Rodrigo C. Barros,Lucas S. Kupssinskü*

Main category: cs.CV

TL;DR: 提出了一种使用帕累托最优前沿评估文本到图像生成模型公平性和效用的方法，通过超参数优化来平衡公平性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成模型的公平性评估主要依赖定性判断或狭窄比较，缺乏同时评估公平性和效用的可重复方法，限制了去偏方法的有效评估。

Method: 使用帕累托最优前沿分析去偏方法的超参数配置，以Normalized Shannon Entropy评估公平性，ClipScore评估效用，对多种主流模型进行系统评估。

Result: 研究发现大多数文本到图像模型的默认超参数配置在公平性-效用空间中处于被支配地位，可以轻松找到更好的超参数配置。

Conclusion: 该方法为文本到图像模型提供了一种可重复、系统化的公平性和效用评估框架，能够有效指导模型优化和去偏方法的改进。

Abstract: Achieving fairness in text-to-image generation demands mitigating social biases without compromising visual fidelity, a challenge critical to responsible AI. Current fairness evaluation procedures for text-to-image models rely on qualitative judgment or narrow comparisons, which limit the capacity to assess both fairness and utility in these models and prevent reproducible assessment of debiasing methods. Existing approaches typically employ ad-hoc, human-centered visual inspections that are both error-prone and difficult to replicate. We propose a method for evaluating fairness and utility in text-to-image models using Pareto-optimal frontiers across hyperparametrization of debiasing methods. Our method allows for comparison between distinct text-to-image models, outlining all configurations that optimize fairness for a given utility and vice-versa. To illustrate our evaluation method, we use Normalized Shannon Entropy and ClipScore for fairness and utility evaluation, respectively. We assess fairness and utility in Stable Diffusion, Fair Diffusion, SDXL, DeCoDi, and FLUX text-to-image models. Our method shows that most default hyperparameterizations of the text-to-image model are dominated solutions in the fairness-utility space, and it is straightforward to find better hyperparameters.

</details>


### [9] [Improving Performance, Robustness, and Fairness of Radiographic AI Models with Finely-Controllable Synthetic Data](https://arxiv.org/abs/2508.16783)
*Stefania L. Moroianu,Christian Bluethgen,Pierre Chambon,Mehdi Cherti,Jean-Benoit Delbrouck,Magdalini Paschali,Brandon Price,Judy Gichoya,Jenia Jitsev,Curtis P. Langlotz,Akshay S. Chaudhari*

Main category: cs.CV

TL;DR: RoentGen-v2是一个用于胸部X光片的文本到图像扩散模型，能够生成具有人口统计学条件控制的合成数据，通过合成预训练策略显著提升下游疾病分类模型的性能、泛化能力和公平性。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像深度学习模型在不同患者群体中实现稳健性能和公平性的挑战，通过合成数据生成来弥补数据集规模和多样性的不足。

Method: 开发RoentGen-v2文本到图像扩散模型，生成超过565,000张具有人口统计学属性控制的合成胸部X光片，提出合成预训练后微调真实数据的改进训练策略。

Result: 在五个机构的137,000多张胸部X光片上评估，合成预训练使下游分类模型准确率提高6.5%，公平性差距减少19.3%，显著优于简单混合真实和合成数据的方法。

Conclusion: 合成影像数据在真实世界数据约束下具有推进公平和可泛化医学深度学习的潜力，为临床部署提供了有效解决方案。

Abstract: Achieving robust performance and fairness across diverse patient populations remains a challenge in developing clinically deployable deep learning models for diagnostic imaging. Synthetic data generation has emerged as a promising strategy to address limitations in dataset scale and diversity. We introduce RoentGen-v2, a text-to-image diffusion model for chest radiographs that enables fine-grained control over both radiographic findings and patient demographic attributes, including sex, age, and race/ethnicity. RoentGen-v2 is the first model to generate clinically plausible images with demographic conditioning, facilitating the creation of a large, demographically balanced synthetic dataset comprising over 565,000 images. We use this large synthetic dataset to evaluate optimal training pipelines for downstream disease classification models. In contrast to prior work that combines real and synthetic data naively, we propose an improved training strategy that leverages synthetic data for supervised pretraining, followed by fine-tuning on real data. Through extensive evaluation on over 137,000 chest radiographs from five institutions, we demonstrate that synthetic pretraining consistently improves model performance, generalization to out-of-distribution settings, and fairness across demographic subgroups. Across datasets, synthetic pretraining led to a 6.5% accuracy increase in the performance of downstream classification models, compared to a modest 2.7% increase when naively combining real and synthetic data. We observe this performance improvement simultaneously with the reduction of the underdiagnosis fairness gap by 19.3%. These results highlight the potential of synthetic imaging to advance equitable and generalizable medical deep learning under real-world data constraints. We open source our code, trained models, and synthetic dataset at https://github.com/StanfordMIMI/RoentGen-v2 .

</details>


### [10] [AIM 2025 Low-light RAW Video Denoising Challenge: Dataset, Methods and Results](https://arxiv.org/abs/2508.16830)
*Alexander Yakovenko,George Chakvetadze,Ilya Khrapov,Maksim Zhelezov,Dmitry Vatolin,Radu Timofte,Youngjin Oh,Junhyeong Kwon,Junyoung Park,Nam Ik Cho,Senyan Xu,Ruixuan Jiang,Long Peng,Xueyang Fu,Zheng-Jun Zha,Xiaoping Peng,Hansen Feng,Zhanyi Tie,Ziming Xia,Lizhi Wang*

Main category: cs.CV

TL;DR: AIM 2025低光照RAW视频去噪挑战赛综述，旨在开发利用时间冗余性、在帧率限制曝光时间内处理传感器特定信号相关噪声的方法


<details>
  <summary>Details</summary>
Motivation: 解决低光照条件下RAW视频的噪声问题，利用时间冗余性进行去噪，同时适应不同传感器的信号相关噪声特性

Method: 建立包含756个十帧序列的新基准数据集，使用14种智能手机传感器在9种光照和曝光条件下采集，通过爆发平均获得高信噪比参考帧

Result: 参与者处理线性RAW序列并输出去噪后的第10帧，保持拜耳模式，使用PSNR和SSIM指标在私有测试集上进行评估

Conclusion: 该挑战赛为低光照RAW视频去噪提供了标准化评估框架和新基准数据集，推动了该领域方法的发展

Abstract: This paper reviews the AIM 2025 (Advances in Image Manipulation) Low-Light RAW Video Denoising Challenge. The task is to develop methods that denoise low-light RAW video by exploiting temporal redundancy while operating under exposure-time limits imposed by frame rate and adapting to sensor-specific, signal-dependent noise. We introduce a new benchmark of 756 ten-frame sequences captured with 14 smartphone camera sensors across nine conditions (illumination: 1/5/10 lx; exposure: 1/24, 1/60, 1/120 s), with high-SNR references obtained via burst averaging. Participants process linear RAW sequences and output the denoised 10th frame while preserving the Bayer pattern. Submissions are evaluated on a private test set using full-reference PSNR and SSIM, with final ranking given by the mean of per-metric ranks. This report describes the dataset, challenge protocol, and submitted approaches.

</details>


### [11] [NinA: Normalizing Flows in Action. Training VLA Models with Normalizing Flows](https://arxiv.org/abs/2508.16845)
*Denis Tarasov,Alexander Nikulin,Ilya Zisman,Albina Klepach,Nikita Lyubaykin,Andrei Polubarov,Alexander Derevyagin,Vladislav Kurenkov*

Main category: cs.CV

TL;DR: NinA使用归一化流替代扩散模型作为VLA的动作解码器，实现单次采样，大幅提升推理速度，同时保持性能相当


<details>
  <summary>Details</summary>
Motivation: 扩散模型需要多次迭代去噪步骤或下游加速技术，限制了在需要高频控制的实际应用中的实用性

Method: 用归一化流(Normalizing Flow)替换扩散动作解码器，通过可逆变换实现一次性采样，集成到FLOWER VLA架构并在LIBERO基准上微调

Result: 在相同训练机制下，NinA性能与基于扩散的解码器相当，但推理速度显著更快

Conclusion: NinA为高效、高频的VLA控制提供了一条有前景的路径，且不牺牲性能

Abstract: Recent advances in Vision-Language-Action (VLA) models have established a two-component architecture, where a pre-trained Vision-Language Model (VLM) encodes visual observations and task descriptions, and an action decoder maps these representations to continuous actions. Diffusion models have been widely adopted as action decoders due to their ability to model complex, multimodal action distributions. However, they require multiple iterative denoising steps at inference time or downstream techniques to speed up sampling, limiting their practicality in real-world settings where high-frequency control is crucial. In this work, we present NinA (Normalizing Flows in Action), a fast and expressive alter- native to diffusion-based decoders for VLAs. NinA replaces the diffusion action decoder with a Normalizing Flow (NF) that enables one-shot sampling through an invertible transformation, significantly reducing inference time. We integrate NinA into the FLOWER VLA architecture and fine-tune on the LIBERO benchmark. Our experiments show that NinA matches the performance of its diffusion-based counterpart under the same training regime, while achieving substantially faster inference. These results suggest that NinA offers a promising path toward efficient, high-frequency VLA control without compromising performance.

</details>


### [12] [RF-PGS: Fully-structured Spatial Wireless Channel Representation with Planar Gaussian Splatting](https://arxiv.org/abs/2508.16849)
*Lihao Zhang,Zongtan Li,Haijian Sun*

Main category: cs.CV

TL;DR: RF-PGS是一个新颖的框架，通过平面高斯和特定射频优化，仅从稀疏路径损耗谱重建高保真无线电传播路径，显著提高了重建精度并降低了训练成本


<details>
  <summary>Details</summary>
Motivation: 6G时代需要大规模天线阵列和精确的空间信道状态信息，传统信道建模方法在空间分辨率、效率和可扩展性方面面临挑战，现有辐射场方法存在几何不准确和昂贵监督的问题

Method: 采用两阶段训练：1）几何训练阶段使用平面高斯作为几何基元实现密集表面对齐的场景重建；2）射频训练阶段结合完全结构化的无线电辐射和定制多视图损失来精确建模无线电传播行为

Result: 相比先前的辐射场方法，RF-PGS显著提高了重建精度，降低了训练成本，并实现了无线信道的高效表示

Conclusion: RF-PGS为可扩展的6G空间CSI建模提供了实用解决方案，解决了传统方法和现有辐射场方法的局限性

Abstract: In the 6G era, the demand for higher system throughput and the implementation of emerging 6G technologies require large-scale antenna arrays and accurate spatial channel state information (Spatial-CSI). Traditional channel modeling approaches, such as empirical models, ray tracing, and measurement-based methods, face challenges in spatial resolution, efficiency, and scalability. Radiance field-based methods have emerged as promising alternatives but still suffer from geometric inaccuracy and costly supervision. This paper proposes RF-PGS, a novel framework that reconstructs high-fidelity radio propagation paths from only sparse path loss spectra. By introducing Planar Gaussians as geometry primitives with certain RF-specific optimizations, RF-PGS achieves dense, surface-aligned scene reconstruction in the first geometry training stage. In the subsequent Radio Frequency (RF) training stage, the proposed fully-structured radio radiance, combined with a tailored multi-view loss, accurately models radio propagation behavior. Compared to prior radiance field methods, RF-PGS significantly improves reconstruction accuracy, reduces training costs, and enables efficient representation of wireless channels, offering a practical solution for scalable 6G Spatial-CSI modeling.

</details>


### [13] [Delta-SVD: Efficient Compression for Personalized Text-to-Image Models](https://arxiv.org/abs/2508.16863)
*Tangyuan Zhang,Shangyu Chen,Qixiang Chen,Jianfei Cai*

Main category: cs.CV

TL;DR: Delta-SVD是一种无需训练的后处理压缩方法，通过奇异值分解压缩DreamBooth微调产生的权重增量，实现大幅压缩同时保持生成质量


<details>
  <summary>Details</summary>
Motivation: DreamBooth等个性化文本到图像模型需要微调大规模扩散模型，导致存储大量特定主题模型时产生显著存储开销

Method: 利用权重增量具有强低秩结构的特点，首先应用奇异值分解分解权重增量，然后采用基于能量的秩截断策略平衡压缩效率和重建保真度

Result: 在多主题数据集上的实验表明，Delta-SVD实现了显著压缩，在CLIP分数、SSIM和FID等指标上生成质量损失可忽略不计

Conclusion: 该方法实现了可扩展和高效的个性化扩散模型部署，为需要存储和部署大规模主题定制的实际应用提供了实用解决方案

Abstract: Personalized text-to-image models such as DreamBooth require fine-tuning large-scale diffusion backbones, resulting in significant storage overhead when maintaining many subject-specific models. We present Delta-SVD, a post-hoc, training-free compression method that targets the parameter weights update induced by DreamBooth fine-tuning. Our key observation is that these delta weights exhibit strong low-rank structure due to the sparse and localized nature of personalization. Delta-SVD first applies Singular Value Decomposition (SVD) to factorize the weight deltas, followed by an energy-based rank truncation strategy to balance compression efficiency and reconstruction fidelity. The resulting compressed models are fully plug-and-play and can be re-constructed on-the-fly during inference. Notably, the proposed approach is simple, efficient, and preserves the original model architecture. Experiments on a multiple subject dataset demonstrate that Delta-SVD achieves substantial compression with negligible loss in generation quality measured by CLIP score, SSIM and FID. Our method enables scalable and efficient deployment of personalized diffusion models, making it a practical solution for real-world applications that require storing and deploying large-scale subject customizations.

</details>


### [14] [MDIQA: Unified Image Quality Assessment for Multi-dimensional Evaluation and Restoration](https://arxiv.org/abs/2508.16887)
*Shunyu Yao,Ming Liu,Zhilu Zhang,Zhaolin Wan,Zhilong Ji,Jinfeng Bai,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 提出多维度图像质量评估框架MDIQA，从技术和美学等多个维度建模图像质量，并通过融合各维度特征生成最终评分，还能灵活应用于图像复原任务。


<details>
  <summary>Details</summary>
Motivation: 现有图像质量评估方法主要关注拟合整体评分，忽略了人类通常从不同维度评估图像质量的事实，需要更符合人类感知的多维度评估方法。

Method: 构建多维度图像质量评估框架，包括5个技术维度和4个美学维度，每个维度单独训练分支，然后融合特征生成最终IQA评分。

Result: 实验证明MDIQA实现了优越的性能，并能有效灵活地应用于图像复原任务，通过调整感知维度权重使复原结果更好地符合不同用户偏好。

Conclusion: MDIQA框架成功解决了现有方法忽视多维度评估的问题，提供了更符合人类视觉感知的图像质量评估方案，并展示了在图像复原任务中的实用价值。

Abstract: Recent advancements in image quality assessment (IQA), driven by sophisticated deep neural network designs, have significantly improved the ability to approach human perceptions. However, most existing methods are obsessed with fitting the overall score, neglecting the fact that humans typically evaluate image quality from different dimensions before arriving at an overall quality assessment. To overcome this problem, we propose a multi-dimensional image quality assessment (MDIQA) framework. Specifically, we model image quality across various perceptual dimensions, including five technical and four aesthetic dimensions, to capture the multifaceted nature of human visual perception within distinct branches. Each branch of our MDIQA is initially trained under the guidance of a separate dimension, and the respective features are then amalgamated to generate the final IQA score. Additionally, when the MDIQA model is ready, we can deploy it for a flexible training of image restoration (IR) models, enabling the restoration results to better align with varying user preferences through the adjustment of perceptual dimension weights. Extensive experiments demonstrate that our MDIQA achieves superior performance and can be effectively and flexibly applied to image restoration tasks. The code is available: https://github.com/YaoShunyu19/MDIQA.

</details>


### [15] [Structural Energy-Guided Sampling for View-Consistent Text-to-3D](https://arxiv.org/abs/2508.16917)
*Qing Zhang,Jinguang Tong,Jie Hong,Jing Zhang,Xuesong Li*

Main category: cs.CV

TL;DR: SEGS是一种无需训练、即插即用的框架，通过在采样时注入结构能量梯度来解决文本到3D生成中的Janus问题（正面正确但其他角度几何重复或扭曲的问题）。


<details>
  <summary>Details</summary>
Motivation: 文本到3D生成存在Janus问题，即对象从正面看正确但从其他角度会出现几何重复或扭曲。这源于2D扩散先验中的视角偏差，该偏差会传播到3D优化过程中。

Method: 提出结构能量引导采样（SEGS）框架：在U-Net中间特征的PCA子空间中定义结构能量，并将其梯度注入去噪轨迹，引导几何朝向预期视角，同时保持外观保真度。可无缝集成到SDS/VSD流程中。

Result: SEGS显著减少了Janus伪影，实现了更好的几何对齐和视角一致性，且无需重新训练或权重修改。

Conclusion: SEGS通过采样时的结构能量引导有效解决了文本到3D生成中的视角一致性问题，是一种高效且无需额外训练的解决方案。

Abstract: Text-to-3D generation often suffers from the Janus problem, where objects look correct from the front but collapse into duplicated or distorted geometry from other angles. We attribute this failure to viewpoint bias in 2D diffusion priors, which propagates into 3D optimization. To address this, we propose Structural Energy-Guided Sampling (SEGS), a training-free, plug-and-play framework that enforces multi-view consistency entirely at sampling time. SEGS defines a structural energy in a PCA subspace of intermediate U-Net features and injects its gradients into the denoising trajectory, steering geometry toward the intended viewpoint while preserving appearance fidelity. Integrated seamlessly into SDS/VSD pipelines, SEGS significantly reduces Janus artifacts, achieving improved geometric alignment and viewpoint consistency without retraining or weight modification.

</details>


### [16] [Align 3D Representation and Text Embedding for 3D Content Personalization](https://arxiv.org/abs/2508.16932)
*Qi Song,Ziyuan Luo,Ka Chun Cheung,Simon See,Renjie Wan*

Main category: cs.CV

TL;DR: Invert3D是一个新颖的3D内容个性化框架，通过建立3D表示与文本嵌入空间的对齐，实现无需重新训练的自然语言驱动的3D内容个性化。


<details>
  <summary>Details</summary>
Motivation: 当前3D个性化方法主要依赖知识蒸馏，需要计算昂贵的重新训练过程。虽然视觉语言模型如CLIP能够实现2D图像的直接个性化，但由于3D内容与2D图像的结构差异，这些技术无法直接应用于3D个性化。

Method: 开发了一种相机条件的3D到文本逆机制，将3D内容投影到与文本嵌入对齐的3D嵌入空间中，建立3D表示与文本嵌入空间的对齐。

Result: 广泛的实验表明，Invert3D能够有效实现3D内容的个性化。

Conclusion: 该方法通过自然语言提示实现高效的3D内容操作和个性化，消除了计算密集型重新训练的需求，为3D内容个性化提供了便捷的解决方案。

Abstract: Recent advances in NeRF and 3DGS have significantly enhanced the efficiency and quality of 3D content synthesis. However, efficient personalization of generated 3D content remains a critical challenge. Current 3D personalization approaches predominantly rely on knowledge distillation-based methods, which require computationally expensive retraining procedures. To address this challenge, we propose \textbf{Invert3D}, a novel framework for convenient 3D content personalization. Nowadays, vision-language models such as CLIP enable direct image personalization through aligned vision-text embedding spaces. However, the inherent structural differences between 3D content and 2D images preclude direct application of these techniques to 3D personalization. Our approach bridges this gap by establishing alignment between 3D representations and text embedding spaces. Specifically, we develop a camera-conditioned 3D-to-text inverse mechanism that projects 3D contents into a 3D embedding aligned with text embeddings. This alignment enables efficient manipulation and personalization of 3D content through natural language prompts, eliminating the need for computationally retraining procedures. Extensive experiments demonstrate that Invert3D achieves effective personalization of 3D content. Our work is available at: https://github.com/qsong2001/Invert3D.

</details>


### [17] [RPD-Diff: Region-Adaptive Physics-Guided Diffusion Model for Visibility Enhancement under Dense and Non-Uniform Haze](https://arxiv.org/abs/2508.16956)
*Ruicheng Zhang,Puxin Yan,Zeyu Zhang,Yicheng Chang,Hongyi Chen,Zhi Jin*

Main category: cs.CV

TL;DR: RPD-Diff是一种区域自适应物理引导去雾扩散模型，通过物理引导中间状态目标和雾霾感知去噪时间步预测器，有效处理密集非均匀雾霾场景，在多个真实数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于扩散的去雾方法在密集非均匀雾霾条件下存在生成条件不足和空间适应性差的问题，导致恢复效果不佳，需要新的解决方案。

Method: 提出RPD-Diff模型，包含物理引导中间状态目标策略(PIST)利用物理先验重构扩散马尔可夫链，以及雾霾感知去噪时间步预测器(HADTP)通过透射图交叉注意力机制动态调整补丁特定去噪时间步。

Result: 在四个真实世界数据集上的广泛实验表明，RPD-Diff在挑战性的密集非均匀雾霾场景中实现了最先进的性能，生成高质量无雾图像，具有优异的细节清晰度和色彩保真度。

Conclusion: RPD-Diff通过物理引导和区域自适应机制，有效解决了密集非均匀雾霾去雾的挑战，为复杂雾霾场景下的能见度增强提供了鲁棒解决方案。

Abstract: Single-image dehazing under dense and non-uniform haze conditions remains challenging due to severe information degradation and spatial heterogeneity. Traditional diffusion-based dehazing methods struggle with insufficient generation conditioning and lack of adaptability to spatially varying haze distributions, which leads to suboptimal restoration. To address these limitations, we propose RPD-Diff, a Region-adaptive Physics-guided Dehazing Diffusion Model for robust visibility enhancement in complex haze scenarios. RPD-Diff introduces a Physics-guided Intermediate State Targeting (PIST) strategy, which leverages physical priors to reformulate the diffusion Markov chain by generation target transitions, mitigating the issue of insufficient conditioning in dense haze scenarios. Additionally, the Haze-Aware Denoising Timestep Predictor (HADTP) dynamically adjusts patch-specific denoising timesteps employing a transmission map cross-attention mechanism, adeptly managing non-uniform haze distributions. Extensive experiments across four real-world datasets demonstrate that RPD-Diff achieves state-of-the-art performance in challenging dense and non-uniform haze scenarios, delivering high-quality, haze-free images with superior detail clarity and color fidelity.

</details>


### [18] [HiCache: Training-free Acceleration of Diffusion Models via Hermite Polynomial-based Feature Caching](https://arxiv.org/abs/2508.16984)
*Liang Feng,Shikang Zheng,Jiacheng Liu,Yuqi Lin,Qinming Zhou,Peiliang Cai,Xinyu Wang,Junjie Chen,Chang Zou,Yue Ma,Linfeng Zhang*

Main category: cs.CV

TL;DR: HiCache是一个无需训练的特征缓存加速框架，通过Hermite多项式对齐特征导数的高斯特性，在扩散模型中实现6.24倍加速同时保持生成质量


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成效果出色但计算成本高昂，现有特征缓存方法因无法准确建模特征演化复杂动态而导致质量损失

Method: 利用扩散变换器中特征导数近似具有多元高斯特性的洞察，采用Hermite多项式作为理论最优基函数，并引入双尺度机制确保数值稳定性和预测精度

Result: 在FLUX.1-dev上实现6.24倍加速且质量超过基线，在文本到图像、视频生成和超分辨率任务中均表现优异

Conclusion: HiCache通过理论对齐和经验特性的结合，为扩散模型提供了高效且高质量的推理加速解决方案

Abstract: Diffusion models have achieved remarkable success in content generation but suffer from prohibitive computational costs due to iterative sampling. While recent feature caching methods tend to accelerate inference through temporal extrapolation, these methods still suffer from server quality loss due to the failure in modeling the complex dynamics of feature evolution. To solve this problem, this paper presents HiCache, a training-free acceleration framework that fundamentally improves feature prediction by aligning mathematical tools with empirical properties. Our key insight is that feature derivative approximations in Diffusion Transformers exhibit multivariate Gaussian characteristics, motivating the use of Hermite polynomials-the potentially theoretically optimal basis for Gaussian-correlated processes. Besides, We further introduce a dual-scaling mechanism that ensures numerical stability while preserving predictive accuracy. Extensive experiments demonstrate HiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding baseline quality, maintaining strong performance across text-to-image, video generation, and super-resolution tasks. Core implementation is provided in the appendix, with complete code to be released upon acceptance.

</details>


### [19] [Fiducial Marker Splatting for High-Fidelity Robotics Simulations](https://arxiv.org/abs/2508.17012)
*Diram Tabaa,Gianni Di Caro*

Main category: cs.CV

TL;DR: 基于高斯折射的混合框架，在复杂环境中高效生成准确的标记点，提升模拟真实性和机器人定位精度


<details>
  <summary>Details</summary>
Motivation: 传统网格基于渲染在复杂环境中表现局限，而神经渲染方法虽然视觉真实却缺乏标记点集成能力，而标记点对于机器人定位和控制至关重要

Method: 提出一种混合框架，结合高斯折射(GS)的照片实感性和结构化标记表示，设计新算法高效生成GS基于标记点

Result: 在效率和姿态估计准确性方面超过传统图像拟合技术，在温室模拟环境中验证框架效果

Conclusion: 该框架为复杂环境中的高保真度模拟提供了有效解决方案，在农业等具有挑战性的应用场景中显示出强大潜力

Abstract: High-fidelity 3D simulation is critical for training mobile robots, but its traditional reliance on mesh-based representations often struggle in complex environments, such as densely packed greenhouses featuring occlusions and repetitive structures. Recent neural rendering methods, like Gaussian Splatting (GS), achieve remarkable visual realism but lack flexibility to incorporate fiducial markers, which are essential for robotic localization and control. We propose a hybrid framework that combines the photorealism of GS with structured marker representations. Our core contribution is a novel algorithm for efficiently generating GS-based fiducial markers (e.g., AprilTags) within cluttered scenes. Experiments show that our approach outperforms traditional image-fitting techniques in both efficiency and pose-estimation accuracy. We further demonstrate the framework's potential in a greenhouse simulation. This agricultural setting serves as a challenging testbed, as its combination of dense foliage, similar-looking elements, and occlusions pushes the limits of perception, thereby highlighting the framework's value for real-world applications.

</details>


### [20] [Dual Orthogonal Guidance for Robust Diffusion-based Handwritten Text Generation](https://arxiv.org/abs/2508.17017)
*Konstantina Nikolaidou,George Retsinas,Giorgos Sfikas,Silvia Cascianelli,Rita Cucchiara,Marcus Liwicki*

Main category: cs.CV

TL;DR: 提出Dual Orthogonal Guidance (DOG)方法，通过正交投影引导扩散模型生成更清晰、更多样化的手写文本，解决了传统方法在风格变化和生成清晰度方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的手写文本生成方法容易记忆训练样本，在风格变化和生成清晰度方面表现不佳，特别是在处理困难风格时容易产生伪影和失真，影响文本可读性。

Method: 提出DOG引导策略，利用负向扰动提示的正交投影到原始正向提示，在潜在空间中引入更稳定、解耦的方向。采用三角形调度控制去噪过程中引导强度，在开始和结束时较弱，中间步骤最强。

Result: 在DiffusionPen和One-DM上的实验结果表明，DOG提高了内容清晰度和风格多样性，即使在词汇表外单词和挑战性书写风格下也能取得良好效果。

Conclusion: DOG方法有效解决了扩散模型在手写文本生成中的伪影和失真问题，提供了更稳定和多样化的生成效果，优于传统的Classifier-Free Guidance方法。

Abstract: Diffusion-based Handwritten Text Generation (HTG) approaches achieve impressive results on frequent, in-vocabulary words observed at training time and on regular styles. However, they are prone to memorizing training samples and often struggle with style variability and generation clarity. In particular, standard diffusion models tend to produce artifacts or distortions that negatively affect the readability of the generated text, especially when the style is hard to produce. To tackle these issues, we propose a novel sampling guidance strategy, Dual Orthogonal Guidance (DOG), that leverages an orthogonal projection of a negatively perturbed prompt onto the original positive prompt. This approach helps steer the generation away from artifacts while maintaining the intended content, and encourages more diverse, yet plausible, outputs. Unlike standard Classifier-Free Guidance (CFG), which relies on unconditional predictions and produces noise at high guidance scales, DOG introduces a more stable, disentangled direction in the latent space. To control the strength of the guidance across the denoising process, we apply a triangular schedule: weak at the start and end of denoising, when the process is most sensitive, and strongest in the middle steps. Experimental results on the state-of-the-art DiffusionPen and One-DM demonstrate that DOG improves both content clarity and style variability, even for out-of-vocabulary words and challenging writing styles.

</details>


### [21] [A Novel Local Focusing Mechanism for Deepfake Detection Generalization](https://arxiv.org/abs/2508.17029)
*Mingliang Li,Lin Yuanbo Wu,Changhong Liu,Hanxi Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的局部聚焦机制(LFM)来解决现有深作偶检测方法在跨物体类别和跨生成域上的精度不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有的重建学习方法基于深度卷积网络，但存在两个核心问题：一是模型容易过拟合特定语义特征分布，二是全局平均池化会丢失关键的局部偶冗线索。这导致方法在跨物体类别和跨生成域时性能差。

Method: 提出局部聚焦机制(LFM)，通过显式关注区别性局部特征来区分真伪图像。LFM整合了显著性网络(SNet)和任务特定的Top-K池化(TKP)模块，选择K个最有信息的局部模式。为了减轻Top-K池化引入的过拟合风险，还提出了两种正则化技术：基于排名的线性抑制(RBLD)和随机K采样(RKS)。

Result: LFM在准确率上比最先进的Neighboring Pixel Relationships(NPR)方法提高3.7%，平均精度提高2.8%，同时保持了高效性，在单台NVIDIA A6000 GPU上达到1789 FPS。

Conclusion: 该方法为跨域深作偶检测设置了新的标杆，通过局部特征聚焦有效解决了现有方法的跨域性能不足问题。

Abstract: The rapid advancement of deepfake generation techniques has intensified the need for robust and generalizable detection methods. Existing approaches based on reconstruction learning typically leverage deep convolutional networks to extract differential features. However, these methods show poor generalization across object categories (e.g., from faces to cars) and generation domains (e.g., from GANs to Stable Diffusion), due to intrinsic limitations of deep CNNs. First, models trained on a specific category tend to overfit to semantic feature distributions, making them less transferable to other categories, especially as network depth increases. Second, Global Average Pooling (GAP) compresses critical local forgery cues into a single vector, thus discarding discriminative patterns vital for real-fake classification. To address these issues, we propose a novel Local Focus Mechanism (LFM) that explicitly attends to discriminative local features for differentiating fake from real images. LFM integrates a Salience Network (SNet) with a task-specific Top-K Pooling (TKP) module to select the K most informative local patterns. To mitigate potential overfitting introduced by Top-K pooling, we introduce two regularization techniques: Rank-Based Linear Dropout (RBLD) and Random-K Sampling (RKS), which enhance the model's robustness. LFM achieves a 3.7 improvement in accuracy and a 2.8 increase in average precision over the state-of-the-art Neighboring Pixel Relationships (NPR) method, while maintaining exceptional efficiency at 1789 FPS on a single NVIDIA A6000 GPU. Our approach sets a new benchmark for cross-domain deepfake detection. The source code are available in https://github.com/lmlpy/LFM.git

</details>


### [22] [Styleclone: Face Stylization with Diffusion Based Data Augmentation](https://arxiv.org/abs/2508.17045)
*Neeraj Matiyali,Siddharth Srivastava,Gaurav Sharma*

Main category: cs.CV

TL;DR: StyleClone是一种通过文本反演和扩散引导图像生成来增强小规模风格数据集的方法，用于训练快速图像到图像转换网络，在保持源图像内容的同时实现高质量风格化


<details>
  <summary>Details</summary>
Motivation: 解决有限风格图像条件下的人脸风格化问题，传统方法在数据稀缺时性能受限，需要一种能够有效扩充风格数据集并提升风格化质量的方法

Method: 结合文本反演和基于扩散的引导图像生成技术，系统性地生成多样化风格样本，使用增强后的数据集训练快速的图像到图像转换网络

Result: 在多种风格上的实验表明，该方法提升了风格化质量，更好地保持了源图像内容，并显著加速了推理速度

Conclusion: StyleClone通过有效的数据增强技术解决了小样本风格化问题，在速度和质量方面均优于基于扩散的方法，为图像风格化提供了实用的解决方案

Abstract: We present StyleClone, a method for training image-to-image translation networks to stylize faces in a specific style, even with limited style images. Our approach leverages textual inversion and diffusion-based guided image generation to augment small style datasets. By systematically generating diverse style samples guided by both the original style images and real face images, we significantly enhance the diversity of the style dataset. Using this augmented dataset, we train fast image-to-image translation networks that outperform diffusion-based methods in speed and quality. Experiments on multiple styles demonstrate that our method improves stylization quality, better preserves source image content, and significantly accelerates inference. Additionally, we provide a systematic evaluation of the augmentation techniques and their impact on stylization performance.

</details>


### [23] [PVNet: Point-Voxel Interaction LiDAR Scene Upsampling Via Diffusion Models](https://arxiv.org/abs/2508.17050)
*Xianjing Cheng,Lintai Wu,Zuowen Wang,Junhui Hou,Jie Wen,Yong Xu*

Main category: cs.CV

TL;DR: PVNet是一个基于扩散模型的点云上采样框架，通过点-体素交互机制处理室外稀疏LiDAR点云，无需密集监督即可实现任意倍率的上采样。


<details>
  <summary>Details</summary>
Motivation: 室外LiDAR点云通常极其稀疏，严重影响3D感知任务性能。现有上采样方法主要针对单个物体，在复杂室外场景中泛化能力有限。

Method: 采用无分类器引导的DDPM扩散模型，以稀疏点云为条件输入，结合附近帧合成点云。设计体素补全模块和点-体素交互模块，融合点和体素特征。

Result: 在多个基准测试中达到最先进性能，是首个支持任意上采样倍率的场景级点云上采样方法。

Conclusion: PVNet通过扩散模型和点-体素交互机制，有效解决了室外稀疏点云上采样问题，显著提升了3D场景理解能力。

Abstract: Accurate 3D scene understanding in outdoor environments heavily relies on high-quality point clouds. However, LiDAR-scanned data often suffer from extreme sparsity, severely hindering downstream 3D perception tasks. Existing point cloud upsampling methods primarily focus on individual objects, thus demonstrating limited generalization capability for complex outdoor scenes. To address this issue, we propose PVNet, a diffusion model-based point-voxel interaction framework to perform LiDAR point cloud upsampling without dense supervision. Specifically, we adopt the classifier-free guidance-based DDPMs to guide the generation, in which we employ a sparse point cloud as the guiding condition and the synthesized point clouds derived from its nearby frames as the input. Moreover, we design a voxel completion module to refine and complete the coarse voxel features for enriching the feature representation. In addition, we propose a point-voxel interaction module to integrate features from both points and voxels, which efficiently improves the environmental perception capability of each upsampled point. To the best of our knowledge, our approach is the first scene-level point cloud upsampling method supporting arbitrary upsampling rates. Extensive experiments on various benchmarks demonstrate that our method achieves state-of-the-art performance. The source code will be available at https://github.com/chengxianjing/PVNet.

</details>


### [24] [REGEN: Real-Time Photorealism Enhancement in Games via a Dual-Stage Generative Network Framework](https://arxiv.org/abs/2508.17061)
*Stefanos Pasios,Nikos Nikolaidis*

Main category: cs.CV

TL;DR: REGEN框架通过双阶段生成网络实现游戏画面的实时照片级真实感增强，在保持视觉质量的同时大幅提升推理速度


<details>
  <summary>Details</summary>
Motivation: 现代游戏虽然硬件和渲染技术不断进步，但在动态环境中实现实时帧率的真正照片级真实感仍然面临视觉质量与性能之间的权衡挑战

Method: 提出REGEN框架，采用鲁棒的无配对图像到图像转换模型生成语义一致的照片级真实帧，将问题转化为更简单的配对图像到图像转换任务，使用轻量级方法实现实时推理

Result: 在GTA V上验证，视觉效果与鲁棒无配对Im2Im方法相当，推理速度提升32.14倍，且优于直接训练轻量级无配对Im2Im方法的结果

Conclusion: REGEN框架成功解决了游戏画面实时照片级真实感增强的挑战，在保持高质量视觉效果的同时实现了显著的性能提升

Abstract: Photorealism is an important aspect of modern video games since it can shape the player experience and simultaneously impact the immersion, narrative engagement, and visual fidelity. Although recent hardware technological breakthroughs, along with state-of-the-art rendering technologies, have significantly improved the visual realism of video games, achieving true photorealism in dynamic environments at real-time frame rates still remains a major challenge due to the tradeoff between visual quality and performance. In this short paper, we present a novel approach for enhancing the photorealism of rendered game frames using generative adversarial networks. To this end, we propose Real-time photorealism Enhancement in Games via a dual-stage gEnerative Network framework (REGEN), which employs a robust unpaired image-to-image translation model to produce semantically consistent photorealistic frames that transform the problem into a simpler paired image-to-image translation task. This enables training with a lightweight method that can achieve real-time inference time without compromising visual quality. We demonstrate the effectiveness of our framework on Grand Theft Auto V, showing that the approach achieves visual results comparable to the ones produced by the robust unpaired Im2Im method while improving inference speed by 32.14 times. Our findings also indicate that the results outperform the photorealism-enhanced frames produced by directly training a lightweight unpaired Im2Im translation method to translate the video game frames towards the visual characteristics of real-world images. Code, pre-trained models, and demos for this work are available at: https://github.com/stefanos50/REGEN.

</details>


### [25] [SSG-Dit: A Spatial Signal Guided Framework for Controllable Video Generation](https://arxiv.org/abs/2508.17062)
*Peng Hu,Yu Gu,Liang Luo,Fuji Ren*

Main category: cs.CV

TL;DR: SSG-DiT是一个新颖的可控视频生成框架，通过空间信号引导和双阶段解耦设计，显著提升了生成视频与文本提示的语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有可控视频生成模型在保持语义一致性方面存在困难，经常生成与提示细节不符的视频内容，需要解决这一关键挑战。

Method: 提出两阶段解耦流程：1）空间信号提示阶段利用预训练多模态模型生成空间感知视觉提示；2）通过轻量级SSG-Adapter将视觉提示与文本结合，注入冻结的视频DiT主干，采用双分支注意力机制。

Result: 在VBench基准测试中达到最先进性能，在空间关系控制和整体一致性等关键指标上超越现有模型。

Conclusion: SSG-DiT框架通过空间信号引导和参数高效的设计，成功解决了可控视频生成中的语义一致性问题，为高质量视频合成提供了有效解决方案。

Abstract: Controllable video generation aims to synthesize video content that aligns precisely with user-provided conditions, such as text descriptions and initial images. However, a significant challenge persists in this domain: existing models often struggle to maintain strong semantic consistency, frequently generating videos that deviate from the nuanced details specified in the prompts. To address this issue, we propose SSG-DiT (Spatial Signal Guided Diffusion Transformer), a novel and efficient framework for high-fidelity controllable video generation. Our approach introduces a decoupled two-stage process. The first stage, Spatial Signal Prompting, generates a spatially aware visual prompt by leveraging the rich internal representations of a pre-trained multi-modal model. This prompt, combined with the original text, forms a joint condition that is then injected into a frozen video DiT backbone via our lightweight and parameter-efficient SSG-Adapter. This unique design, featuring a dual-branch attention mechanism, allows the model to simultaneously harness its powerful generative priors while being precisely steered by external spatial signals. Extensive experiments demonstrate that SSG-DiT achieves state-of-the-art performance, outperforming existing models on multiple key metrics in the VBench benchmark, particularly in spatial relationship control and overall consistency.

</details>


### [26] [Structural Damage Detection Using AI Super Resolution and Visual Language Model](https://arxiv.org/abs/2508.17130)
*Catherine Hoier,Khandaker Mamun Ahmed*

Main category: cs.CV

TL;DR: 提出基于无人机影像、AI视频超分辨率和视觉语言模型的灾害损害评估框架，在土耳其地震和摩尔龙卷风数据上达到84.5%分类准确率


<details>
  <summary>Details</summary>
Motivation: 传统灾害损害评估方法劳动密集、成本高且危险，难以在资源有限环境下快速响应，需要自动化、低成本解决方案

Method: 集成无人机影像、Video Restoration Transformer视频超分辨率模型和Gemma3:27b视觉语言模型，构建四类损害分类系统

Result: 在2023土耳其地震和2013摩尔龙卷风数据上验证，达到84.5%的分类准确率

Conclusion: 该框架能提供高精度损害评估，非技术用户也可进行初步分析，显著提升灾害管理响应效率和可及性

Abstract: Natural disasters pose significant challenges to timely and accurate damage assessment due to their sudden onset and the extensive areas they affect. Traditional assessment methods are often labor-intensive, costly, and hazardous to personnel, making them impractical for rapid response, especially in resource-limited settings. This study proposes a novel, cost-effective framework that leverages aerial drone footage, an advanced AI-based video super-resolution model, Video Restoration Transformer (VRT), and Gemma3:27b, a 27 billion parameter Visual Language Model (VLM). This integrated system is designed to improve low-resolution disaster footage, identify structural damage, and classify buildings into four damage categories, ranging from no/slight damage to total destruction, along with associated risk levels. The methodology was validated using pre- and post-event drone imagery from the 2023 Turkey earthquakes (courtesy of The Guardian) and satellite data from the 2013 Moore Tornado (xBD dataset). The framework achieved a classification accuracy of 84.5%, demonstrating its ability to provide highly accurate results. Furthermore, the system's accessibility allows non-technical users to perform preliminary analyses, thereby improving the responsiveness and efficiency of disaster management efforts.

</details>


### [27] [MMCIG: Multimodal Cover Image Generation for Text-only Documents and Its Dataset Construction via Pseudo-labeling](https://arxiv.org/abs/2508.17199)
*Hyeyeon Kim,Sungwoo Han,Jingun Kwon,Hidetaka Kamigaito,Manabu Okumura*

Main category: cs.CV

TL;DR: 提出了一种新颖的多模态伪标注方法，用于从纯文本文档生成封面图像和摘要，通过联合评估图像和标题排名来构建高质量数据集


<details>
  <summary>Details</summary>
Motivation: 现有数据集无法支持从纯文本文档生成对应封面图像和摘要的任务，需要低成本构建高质量训练数据

Method: 多模态伪标注方法：收集含多图像和标题的文档，排除事实不一致实例；基于黄金摘要分别对图像和标题排名；当图像和对应标题均排名第一时标注伪标签；移除文本中直接引用图像的文档

Result: 实验结果表明，所提多模态伪标注方法比单独考虑标题和图像的方法构建更精确的数据集，生成更高质量的图像

Conclusion: 多模态伪标注方法能有效构建封面图像生成任务的高质量数据集，在图像生成质量上优于单模态方法

Abstract: In this study, we introduce a novel cover image generation task that produces both a concise summary and a visually corresponding image from a given text-only document. Because no existing datasets are available for this task, we propose a multimodal pseudo-labeling method to construct high-quality datasets at low cost. We first collect documents that contain multiple images with their captions, and their summaries by excluding factually inconsistent instances. Our approach selects one image from the multiple images accompanying the documents. Using the gold summary, we independently rank both the images and their captions. Then, we annotate a pseudo-label for an image when both the image and its corresponding caption are ranked first in their respective rankings. Finally, we remove documents that contain direct image references within texts. Experimental results demonstrate that the proposed multimodal pseudo-labeling method constructs more precise datasets and generates higher quality images than text- and image-only pseudo-labeling methods, which consider captions and images separately. We release our code at: https://github.com/HyeyeeonKim/MMCIG

</details>


### [28] [4D Visual Pre-training for Robot Learning](https://arxiv.org/abs/2508.17230)
*Chengkai Hou,Yanjie Ze,Yankai Fu,Zeyu Gao,Songbo Hu,Yue Yu,Shanghang Zhang,Huazhe Xu*

Main category: cs.CV

TL;DR: FVP是一个新颖的4D视觉预训练框架，通过点云预测扩散模型提升3D表示性能，在12个真实世界操作任务中将3D Diffusion Policy的成功率平均提升28%


<details>
  <summary>Details</summary>
Motivation: 现有的视觉预训练表示主要基于2D图像，忽略了世界的3D本质，但由于大规模3D数据稀缺，难以从网络数据集中提取通用3D表示

Method: 将视觉预训练目标构建为下一个点云预测问题，使用扩散模型作为预测模型，直接在大型公共数据集上进行预训练

Result: 在12个真实世界操作任务中，FVP将3D Diffusion Policy的平均成功率提升28%，达到模仿学习方法的最先进性能，并能适配不同点云编码器和数据集

Conclusion: FVP框架有效提升了3D表示性能，可广泛应用于不同模型和数据集，包括增强RDT-1B等大型视觉-语言-动作机器人模型在各种机器人任务上的表现

Abstract: General visual representations learned from web-scale datasets for robotics have achieved great success in recent years, enabling data-efficient robot learning on manipulation tasks; yet these pre-trained representations are mostly on 2D images, neglecting the inherent 3D nature of the world. However, due to the scarcity of large-scale 3D data, it is still hard to extract a universal 3D representation from web datasets. Instead, we are seeking a general visual pre-training framework that could improve all 3D representations as an alternative. Our framework, called FVP, is a novel 4D Visual Pre-training framework for real-world robot learning. FVP frames the visual pre-training objective as a next-point-cloud-prediction problem, models the prediction model as a diffusion model, and pre-trains the model on the larger public datasets directly. Across twelve real-world manipulation tasks, FVP boosts the average success rate of 3D Diffusion Policy (DP3) for these tasks by 28%. The FVP pre-trained DP3 achieves state-of-the-art performance across imitation learning methods. Moreover, the efficacy of FVP adapts across various point cloud encoders and datasets. Finally, we apply FVP to the RDT-1B, a larger Vision-Language-Action robotic model, enhancing its performance on various robot tasks. Our project page is available at: https://4d- visual-pretraining.github.io/.

</details>


### [29] [FoundDiff: Foundational Diffusion Model for Generalizable Low-Dose CT Denoising](https://arxiv.org/abs/2508.17299)
*Zhihao Chen,Qi Gao,Zilong Li,Junping Zhang,Yi Zhang,Jun Zhao,Hongming Shan*

Main category: cs.CV

TL;DR: FoundDiff是一个基于扩散模型的基础性低剂量CT去噪方法，能够统一处理不同剂量水平和解剖区域，通过两阶段策略实现剂量-解剖感知和自适应去噪。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的低剂量CT去噪方法通常在特定剂量水平和解剖区域训练，难以处理不同扫描条件下的噪声特征和解剖异质性，限制了在临床场景中的泛化性和鲁棒性。

Method: 采用两阶段策略：1) 剂量-解剖感知的对比语言图像预训练模型(DA-CLIP)，通过对比学习学习连续表示来量化剂量变化和识别解剖区域；2) 剂量-解剖感知的扩散模型(DA-Diff)，通过基于Mamba的剂量解剖条件块整合学习到的嵌入信息。

Result: 在两个公共LDCT数据集上的广泛实验表明，FoundDiff在八个剂量水平和三个解剖区域上表现出优于现有最先进方法的去噪性能，并对未见剂量水平具有显著泛化能力。

Conclusion: FoundDiff提供了一个统一且可泛化的低剂量CT去噪解决方案，通过创新的剂量-解剖感知机制和扩散模型集成，显著提升了在不同临床条件下的性能表现。

Abstract: Low-dose computed tomography (CT) denoising is crucial for reduced radiation exposure while ensuring diagnostically acceptable image quality. Despite significant advancements driven by deep learning (DL) in recent years, existing DL-based methods, typically trained on a specific dose level and anatomical region, struggle to handle diverse noise characteristics and anatomical heterogeneity during varied scanning conditions, limiting their generalizability and robustness in clinical scenarios. In this paper, we propose FoundDiff, a foundational diffusion model for unified and generalizable LDCT denoising across various dose levels and anatomical regions. FoundDiff employs a two-stage strategy: (i) dose-anatomy perception and (ii) adaptive denoising. First, we develop a dose- and anatomy-aware contrastive language image pre-training model (DA-CLIP) to achieve robust dose and anatomy perception by leveraging specialized contrastive learning strategies to learn continuous representations that quantify ordinal dose variations and identify salient anatomical regions. Second, we design a dose- and anatomy-aware diffusion model (DA-Diff) to perform adaptive and generalizable denoising by synergistically integrating the learned dose and anatomy embeddings from DACLIP into diffusion process via a novel dose and anatomy conditional block (DACB) based on Mamba. Extensive experiments on two public LDCT datasets encompassing eight dose levels and three anatomical regions demonstrate superior denoising performance of FoundDiff over existing state-of-the-art methods and the remarkable generalization to unseen dose levels. The codes and models are available at https://github.com/hao1635/FoundDiff.

</details>


### [30] [PosBridge: Multi-View Positional Embedding Transplant for Identity-Aware Image Editing](https://arxiv.org/abs/2508.17302)
*Peilin Xiong,Junwen Chen,Honghui Yuan,Keiji Yanai*

Main category: cs.CV

TL;DR: PosBridge是一个无需训练的高效图像编辑框架，通过位置嵌入移植和角点中心布局技术，在目标场景中精确插入用户指定的对象


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的规模不断扩大，训练成本急剧增加，需要开发无需训练且可扩展的图像编辑框架来实现定制化对象的无缝插入

Method: 采用位置嵌入移植技术引导扩散模型复制参考对象的结构特征，结合角点中心布局将参考图像和背景图像拼接作为FLUX.1-Fill模型的输入，在渐进去噪过程中指导目标区域的噪声分布

Result: 大量实验表明PosBridge在结构一致性、外观保真度和计算效率方面均优于主流基线方法

Conclusion: PosBridge框架具有实际应用价值和广泛采用潜力，为无需训练的定制化图像编辑提供了高效解决方案

Abstract: Localized subject-driven image editing aims to seamlessly integrate user-specified objects into target scenes. As generative models continue to scale, training becomes increasingly costly in terms of memory and computation, highlighting the need for training-free and scalable editing frameworks.To this end, we propose PosBridge an efficient and flexible framework for inserting custom objects. A key component of our method is positional embedding transplant, which guides the diffusion model to faithfully replicate the structural characteristics of reference objects.Meanwhile, we introduce the Corner Centered Layout, which concatenates reference images and the background image as input to the FLUX.1-Fill model. During progressive denoising, positional embedding transplant is applied to guide the noise distribution in the target region toward that of the reference object. In this way, Corner Centered Layout effectively directs the FLUX.1-Fill model to synthesize identity-consistent content at the desired location. Extensive experiments demonstrate that PosBridge outperforms mainstream baselines in structural consistency, appearance fidelity, and computational efficiency, showcasing its practical value and potential for broad adoption.

</details>


### [31] [DiCache: Let Diffusion Model Determine Its Own Cache](https://arxiv.org/abs/2508.17356)
*Jiazi Bu,Pengyang Ling,Yujie Zhou,Yibin Wang,Yuhang Zang,Tong Wu,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: DiCache是一种无需训练的自适应缓存策略，通过浅层在线探针实时获取缓存误差先验，结合动态缓存轨迹对齐技术，在扩散模型中实现高效加速和更好的视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有的缓存加速方法依赖预定义经验法则或数据集级先验，在高度动态的扩散过程中泛化性有限，无法处理异常样本。研究发现浅层特征差异变化模式与最终输出变化模式存在强相关性。

Method: DiCache包含两个核心组件：1) 在线探针分析方案，利用浅层在线探针实时获取稳定缓存误差先验；2) 动态缓存轨迹对齐，基于浅层探针特征轨迹组合多步缓存来更好近似当前特征。

Result: 在WAN 2.1、HunyuanVideo视频生成和Flux图像生成等多种领先扩散模型上的实验验证，DiCache在效率和视觉保真度方面均优于最先进方法。

Conclusion: DiCache提供了一个统一的框架来解决何时缓存和如何使用缓存的问题，无需训练即可实现自适应缓存策略，在扩散模型加速方面表现出优异的性能和泛化能力。

Abstract: Recent years have witnessed the rapid development of acceleration techniques for diffusion models, especially caching-based acceleration methods. These studies seek to answer two fundamental questions: "When to cache" and "How to use cache", typically relying on predefined empirical laws or dataset-level priors to determine the timing of caching and utilizing handcrafted rules for leveraging multi-step caches. However, given the highly dynamic nature of the diffusion process, they often exhibit limited generalizability and fail on outlier samples. In this paper, a strong correlation is revealed between the variation patterns of the shallow-layer feature differences in the diffusion model and those of final model outputs. Moreover, we have observed that the features from different model layers form similar trajectories. Based on these observations, we present DiCache, a novel training-free adaptive caching strategy for accelerating diffusion models at runtime, answering both when and how to cache within a unified framework. Specifically, DiCache is composed of two principal components: (1) Online Probe Profiling Scheme leverages a shallow-layer online probe to obtain a stable prior for the caching error in real time, enabling the model to autonomously determine caching schedules. (2) Dynamic Cache Trajectory Alignment combines multi-step caches based on shallow-layer probe feature trajectory to better approximate the current feature, facilitating higher visual quality. Extensive experiments validate DiCache's capability in achieving higher efficiency and improved visual fidelity over state-of-the-art methods on various leading diffusion models including WAN 2.1, HunyuanVideo for video generation, and Flux for image generation.

</details>


### [32] [Condition Weaving Meets Expert Modulation: Towards Universal and Controllable Image Generation](https://arxiv.org/abs/2508.17364)
*Guoqing Zhang,Xingtong Ge,Lu Shi,Xin Zhang,Muqing Xue,Wanru Xu,Yigang Cen*

Main category: cs.CV

TL;DR: 提出了UniGen统一图像生成框架，通过CoMoE模块解决多条件生成中的参数冗余问题，使用WeaveNet机制增强主干网络与条件分支的交互，在多个数据集上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法为每种条件类型训练单独的控制分支，导致模型结构冗余和计算资源使用效率低下，需要统一的框架来支持多样化条件输入。

Method: 提出Condition Modulated Expert (CoMoE)模块聚合语义相似的补丁特征并分配给专用专家模块，以及WeaveNet动态连接机制实现主干网络与条件分支的有效交互。

Result: 在Subjects-200K和MultiGen-20M数据集上的广泛实验表明，该方法在各种条件图像生成任务中始终达到最先进的性能。

Conclusion: UniGen框架在多功能性和有效性方面都具有优势，能够有效缓解多条件场景中的特征纠缠和冗余计算问题。

Abstract: The image-to-image generation task aims to produce controllable images by leveraging conditional inputs and prompt instructions. However, existing methods often train separate control branches for each type of condition, leading to redundant model structures and inefficient use of computational resources. To address this, we propose a Unified image-to-image Generation (UniGen) framework that supports diverse conditional inputs while enhancing generation efficiency and expressiveness. Specifically, to tackle the widely existing parameter redundancy and computational inefficiency in controllable conditional generation architectures, we propose the Condition Modulated Expert (CoMoE) module. This module aggregates semantically similar patch features and assigns them to dedicated expert modules for visual representation and conditional modeling. By enabling independent modeling of foreground features under different conditions, CoMoE effectively mitigates feature entanglement and redundant computation in multi-condition scenarios. Furthermore, to bridge the information gap between the backbone and control branches, we propose WeaveNet, a dynamic, snake-like connection mechanism that enables effective interaction between global text-level control from the backbone and fine-grained control from conditional branches. Extensive experiments on the Subjects-200K and MultiGen-20M datasets across various conditional image generation tasks demonstrate that our method consistently achieves state-of-the-art performance, validating its advantages in both versatility and effectiveness. The code has been uploaded to https://github.com/gavin-gqzhang/UniGen.

</details>


### [33] [Enhancing Underwater Images via Deep Learning: A Comparative Study of VGG19 and ResNet50-Based Approaches](https://arxiv.org/abs/2508.17397)
*Aoqi Li,Yanghui Song,Jichao Dao,Chengfu Yang*

Main category: cs.CV

TL;DR: 本文提出基于VGG19和ResNet50双深度学习模型融合的水下图像增强方法，通过多尺度特征分析和统一模型整合实现更全面的图像增强效果。


<details>
  <summary>Details</summary>
Motivation: 解决复杂水下场景中图像增强的挑战性问题，利用深度学习技术提升水下视觉质量。

Method: 集成VGG19和ResNet50两个深度卷积神经网络模型，进行多尺度多层次深度特征分析，构建统一模型整合两者的互补优势。

Result: 使用PSNR、UCIQE、UIQM等图像质量评估指标进行定量比较，在不同场景下分析不同模型的性能表现。

Conclusion: 提供了模型优化、多模型融合和硬件选择等实用建议，为复杂水下环境中的视觉增强任务提供技术支持。

Abstract: This paper addresses the challenging problem of image enhancement in complex underwater scenes by proposing a solution based on deep learning. The proposed method skillfully integrates two deep convolutional neural network models, VGG19 and ResNet50, leveraging their powerful feature extraction capabilities to perform multi-scale and multi-level deep feature analysis of underwater images. By constructing a unified model, the complementary advantages of the two models are effectively integrated, achieving a more comprehensive and accurate image enhancement effect.To objectively evaluate the enhancement effect, this paper introduces image quality assessment metrics such as PSNR, UCIQE, and UIQM to quantitatively compare images before and after enhancement and deeply analyzes the performance of different models in different scenarios.Furthermore, to improve the practicality and stability of the underwater visual enhancement system, this paper also provides practical suggestions from aspects such as model optimization, multi-model fusion, and hardware selection, aiming to provide strong technical support for visual enhancement tasks in complex underwater environments.

</details>


### [34] [MoCo: Motion-Consistent Human Video Generation via Structure-Appearance Decoupling](https://arxiv.org/abs/2508.17404)
*Haoyu Wang,Hao Tang,Donglin Di,Zhilu Zhang,Wangmeng Zuo,Feng Gao,Siwei Ma,Shiliang Zhang*

Main category: cs.CV

TL;DR: MoCo是一个新的人类视频生成方法，通过将生成过程解耦为结构生成和外观生成两个组件，解决了现有方法在全身运动和长距离运动方面的一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型过于注重外观保真度，导致人体运动不真实、物理上不可行且结构连贯性差。现有数据集主要关注面部或上半身运动，限制了生成方法的适用范围。

Method: 提出MoCo方法：1）使用高效的3D结构生成器从文本提示生成人体运动序列；2）在生成的结构序列指导下合成视频外观；3）引入Human-Aware Dynamic Control模块和改进的密集跟踪约束；4）构建大规模全身人体视频数据集。

Result: 大量实验证明，MoCo在生成真实且结构连贯的人体视频方面优于现有方法。

Conclusion: MoCo通过解耦结构和外观生成过程，结合新的控制模块和数据集，显著提升了人体视频生成的质量和结构一致性，为复杂人体运动生成提供了有效解决方案。

Abstract: Generating human videos with consistent motion from text prompts remains a significant challenge, particularly for whole-body or long-range motion. Existing video generation models prioritize appearance fidelity, resulting in unrealistic or physically implausible human movements with poor structural coherence. Additionally, most existing human video datasets primarily focus on facial or upper-body motions, or consist of vertically oriented dance videos, limiting the scope of corresponding generation methods to simple movements. To overcome these challenges, we propose MoCo, which decouples the process of human video generation into two components: structure generation and appearance generation. Specifically, our method first employs an efficient 3D structure generator to produce a human motion sequence from a text prompt. The remaining video appearance is then synthesized under the guidance of the generated structural sequence. To improve fine-grained control over sparse human structures, we introduce Human-Aware Dynamic Control modules and integrate dense tracking constraints during training. Furthermore, recognizing the limitations of existing datasets, we construct a large-scale whole-body human video dataset featuring complex and diverse motions. Extensive experiments demonstrate that MoCo outperforms existing approaches in generating realistic and structurally coherent human videos.

</details>


### [35] [TinySR: Pruning Diffusion for Real-World Image Super-Resolution](https://arxiv.org/abs/2508.17434)
*Linwei Dong,Qingnan Fan,Yuhang Yu,Qi Zhang,Jinwei Chen,Yawei Luo,Changqing Zou*

Main category: cs.CV

TL;DR: TinySR是一个紧凑高效的扩散模型，专门用于实时图像超分辨率，通过深度剪枝、VAE压缩和预缓存技术，在保持感知质量的同时实现了5.68倍加速和83%参数减少


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在图像超分辨率中虽然效果好，但迭代去噪过程计算开销大，而一步蒸馏方法又受限于大模型架构，难以满足实时应用需求

Method: 提出动态块间激活和扩展-腐蚀策略进行深度剪枝，通过通道剪枝、注意力移除和轻量SepConv实现VAE压缩，消除时间和提示相关模块，并采用预缓存技术加速

Result: 相比教师模型TSD-SR，实现了5.68倍加速和83%参数减少，同时仍能提供高质量的超分辨率结果

Conclusion: TinySR证明了在保持扩散模型感知质量优势的同时，通过有效的模型压缩和优化技术可以实现实时性能，为实际应用提供了可行的解决方案

Abstract: Real-world image super-resolution (Real-ISR) focuses on recovering high-quality images from low-resolution inputs that suffer from complex degradations like noise, blur, and compression. Recently, diffusion models (DMs) have shown great potential in this area by leveraging strong generative priors to restore fine details. However, their iterative denoising process incurs high computational overhead, posing challenges for real-time applications. Although one-step distillation methods, such as OSEDiff and TSD-SR, offer faster inference, they remain fundamentally constrained by their large, over-parameterized model architectures. In this work, we present TinySR, a compact yet effective diffusion model specifically designed for Real-ISR that achieves real-time performance while maintaining perceptual quality. We introduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy to facilitate more effective decision-making in depth pruning. We achieve VAE compression through channel pruning, attention removal and lightweight SepConv. We eliminate time- and prompt-related modules and perform pre-caching techniques to further speed up the model. TinySR significantly reduces computational cost and model size, achieving up to 5.68x speedup and 83% parameter reduction compared to its teacher TSD-SR, while still providing high quality results.

</details>


### [36] [An LLM-LVLM Driven Agent for Iterative and Fine-Grained Image Editing](https://arxiv.org/abs/2508.17435)
*Zihan Liang,Jiahao Sun,Haoran Ma*

Main category: cs.CV

TL;DR: RefineEdit-Agent是一个无需训练的多模态智能代理框架，通过LLM规划能力和LVLM视觉理解实现复杂迭代式图像编辑，在LongBench-T2I-Edit基准上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型在细粒度迭代编辑方面存在局限，包括指令理解不精确、上下文保持困难以及缺乏智能反馈机制。

Method: 构建闭环系统，包含LVLM驱动的指令解析和场景理解模块、多级LLM驱动的编辑规划器、迭代图像编辑模块以及LVLM驱动的反馈评估循环。

Result: 在500张初始图像的新基准测试中平均得分3.67，显著优于Direct Re-Prompting(2.29)、InstructPix2Pix(2.91)、GLIGEN-based Edit(3.16)和ControlNet-XL(3.39)。

Conclusion: RefineEdit-Agent通过智能代理设计有效提升了编辑保真度和上下文保持能力，消融实验和人工评估验证了其有效性。

Abstract: Despite the remarkable capabilities of text-to-image (T2I) generation models, real-world applications often demand fine-grained, iterative image editing that existing methods struggle to provide. Key challenges include granular instruction understanding, robust context preservation during modifications, and the lack of intelligent feedback mechanisms for iterative refinement. This paper introduces RefineEdit-Agent, a novel, training-free intelligent agent framework designed to address these limitations by enabling complex, iterative, and context-aware image editing. RefineEdit-Agent leverages the powerful planning capabilities of Large Language Models (LLMs) and the advanced visual understanding and evaluation prowess of Vision-Language Large Models (LVLMs) within a closed-loop system. Our framework comprises an LVLM-driven instruction parser and scene understanding module, a multi-level LLM-driven editing planner for goal decomposition, tool selection, and sequence generation, an iterative image editing module, and a crucial LVLM-driven feedback and evaluation loop. To rigorously evaluate RefineEdit-Agent, we propose LongBench-T2I-Edit, a new benchmark featuring 500 initial images with complex, multi-turn editing instructions across nine visual dimensions. Extensive experiments demonstrate that RefineEdit-Agent significantly outperforms state-of-the-art baselines, achieving an average score of 3.67 on LongBench-T2I-Edit, compared to 2.29 for Direct Re-Prompting, 2.91 for InstructPix2Pix, 3.16 for GLIGEN-based Edit, and 3.39 for ControlNet-XL. Ablation studies, human evaluations, and analyses of iterative refinement, backbone choices, tool usage, and robustness to instruction complexity further validate the efficacy of our agentic design in delivering superior edit fidelity and context preservation.

</details>


### [37] [Pixie: Fast and Generalizable Supervised Learning of 3D Physics from Pixels](https://arxiv.org/abs/2508.17437)
*Long Le,Ryan Lucas,Chen Wang,Chuhao Chen,Dinesh Jayaraman,Eric Eaton,Lingjie Liu*

Main category: cs.CV

TL;DR: PIXIE是一种新颖的神经网络方法，能够从3D视觉特征快速推断物理材质属性，比现有方法快几个数量级，并且能够零样本泛化到真实场景。


<details>
  <summary>Details</summary>
Motivation: 从视觉信息推断3D场景物理属性是创建交互式虚拟世界的关键挑战，现有方法依赖缓慢的逐场景优化，限制了通用性和应用。

Method: 训练通用神经网络从3D视觉特征预测物理属性，使用监督损失，结合高斯泼溅等静态场景表示，支持快速前向推理。

Result: PIXIE比测试时优化方法好1.46-4.39倍，速度快几个数量级，通过CLIP等预训练视觉特征能够零样本泛化到真实场景。

Conclusion: PIXIE提供了一种快速、通用的物理属性推断方法，为创建逼真的交互式虚拟世界提供了有效解决方案，并发布了大型数据集PIXIEVERSE支持相关研究。

Abstract: Inferring the physical properties of 3D scenes from visual information is a critical yet challenging task for creating interactive and realistic virtual worlds. While humans intuitively grasp material characteristics such as elasticity or stiffness, existing methods often rely on slow, per-scene optimization, limiting their generalizability and application. To address this problem, we introduce PIXIE, a novel method that trains a generalizable neural network to predict physical properties across multiple scenes from 3D visual features purely using supervised losses. Once trained, our feed-forward network can perform fast inference of plausible material fields, which coupled with a learned static scene representation like Gaussian Splatting enables realistic physics simulation under external forces. To facilitate this research, we also collected PIXIEVERSE, one of the largest known datasets of paired 3D assets and physic material annotations. Extensive evaluations demonstrate that PIXIE is about 1.46-4.39x better and orders of magnitude faster than test-time optimization methods. By leveraging pretrained visual features like CLIP, our method can also zero-shot generalize to real-world scenes despite only ever been trained on synthetic data. https://pixie-3d.github.io/

</details>


### [38] [A Synthetic Dataset for Manometry Recognition in Robotic Applications](https://arxiv.org/abs/2508.17468)
*Pedro Antonio Rabelo Saraiva,Enzo Ferreira de Souza,Joao Manoel Herrera Pinheiro,Thiago H. Segreto,Ricardo V. Godoy,Marcelo Becker*

Main category: cs.CV

TL;DR: 提出混合数据合成管道，结合程序化渲染和AI视频生成，解决工业环境中数据稀缺问题，证明合成数据与真实数据混合训练能显著提升目标检测性能


<details>
  <summary>Details</summary>
Motivation: 解决复杂工业环境（如海上石油平台）中数据稀缺和高采集成本的问题，这些危险环境中的实际和经济障碍阻碍了自主检测系统的发展

Method: 使用BlenderProc创建具有精确标注和可控域随机化的照片级真实图像，整合NVIDIA的Cosmos-Predict2世界基础模型合成物理上合理的视频序列，采用YOLO检测网络在混合数据集上进行训练

Result: 在真实图像与合成数据的1:1混合数据集上训练的模型达到了最高准确率，超越了仅使用真实数据的基线模型

Conclusion: 合成优先方法是在安全关键和资源受限的工业应用中开发可靠感知系统的高效、经济且安全的可行替代方案

Abstract: This work addresses the challenges of data scarcity and high acquisition costs for training robust object detection models in complex industrial environments, such as offshore oil platforms. The practical and economic barriers to collecting real-world data in these hazardous settings often hamper the development of autonomous inspection systems. To overcome this, in this work we propose and validate a hybrid data synthesis pipeline that combines procedural rendering with AI-driven video generation. Our methodology leverages BlenderProc to create photorealistic images with precise annotations and controlled domain randomization, and integrates NVIDIA's Cosmos-Predict2 world-foundation model to synthesize physically plausible video sequences with temporal diversity, capturing rare viewpoints and adverse conditions. We demonstrate that a YOLO-based detection network trained on a composite dataset, blending real images with our synthetic data, achieves superior performance compared to models trained exclusively on real-world data. Notably, a 1:1 mixture of real and synthetic data yielded the highest accuracy, surpassing the real-only baseline. These findings highlight the viability of a synthetic-first approach as an efficient, cost-effective, and safe alternative for developing reliable perception systems in safety-critical and resource-constrained industrial applications.

</details>


### [39] [T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image Generation](https://arxiv.org/abs/2508.17472)
*Kaiyue Sun,Rongyao Fang,Chengqi Duan,Xian Liu,Xihui Liu*

Main category: cs.CV

TL;DR: T2I-ReasonBench是一个评估文本到图像模型推理能力的基准测试，包含四个维度：成语解释、文本图像设计、实体推理和科学推理，采用两阶段评估协议


<details>
  <summary>Details</summary>
Motivation: 评估文本到图像模型的推理能力，现有基准测试在这方面存在不足

Method: 提出包含四个维度的基准测试和两阶段评估协议（推理准确性和图像质量评估）

Result: 对各种T2I生成模型进行了基准测试，并提供了全面的性能分析

Conclusion: T2I-ReasonBench为评估文本到图像模型的推理能力提供了有效的基准测试工具

Abstract: We propose T2I-ReasonBench, a benchmark evaluating reasoning capabilities of text-to-image (T2I) models. It consists of four dimensions: Idiom Interpretation, Textual Image Design, Entity-Reasoning and Scientific-Reasoning. We propose a two-stage evaluation protocol to assess the reasoning accuracy and image quality. We benchmark various T2I generation models, and provide comprehensive analysis on their performances.

</details>


### [40] [IDU: Incremental Dynamic Update of Existing 3D Virtual Environments with New Imagery Data](https://arxiv.org/abs/2508.17579)
*Meida Chen,Luis Leal,Yue Hu,Rong Liu,Butian Xiong,Andrew Feng,Jiuyi Xu,Yangming Shi*

Main category: cs.CV

TL;DR: 提出了增量动态更新（IDU）管道，通过少量新图像高效更新3D高斯溅射模型，显著减少军事场景中3D模型更新的时间和成本


<details>
  <summary>Details</summary>
Motivation: 军事组织开发高分辨率3D虚拟环境成本高昂，战场环境动态变化需要频繁更新，传统全规模更新方法耗时耗力

Method: IDU管道包含相机姿态估计、变化检测、3D生成AI模型创建新资产、人工指导下的无缝集成，每次更新专注于单个新对象

Result: 实验证实IDU管道显著减少更新时间和人工成本

Conclusion: IDU提供了一种经济高效、有针对性的解决方案，适用于快速演变的军事场景中保持3D模型的最新状态

Abstract: For simulation and training purposes, military organizations have made substantial investments in developing high-resolution 3D virtual environments through extensive imaging and 3D scanning. However, the dynamic nature of battlefield conditions-where objects may appear or vanish over time-makes frequent full-scale updates both time-consuming and costly. In response, we introduce the Incremental Dynamic Update (IDU) pipeline, which efficiently updates existing 3D reconstructions, such as 3D Gaussian Splatting (3DGS), with only a small set of newly acquired images. Our approach starts with camera pose estimation to align new images with the existing 3D model, followed by change detection to pinpoint modifications in the scene. A 3D generative AI model is then used to create high-quality 3D assets of the new elements, which are seamlessly integrated into the existing 3D model. The IDU pipeline incorporates human guidance to ensure high accuracy in object identification and placement, with each update focusing on a single new object at a time. Experimental results confirm that our proposed IDU pipeline significantly reduces update time and labor, offering a cost-effective and targeted solution for maintaining up-to-date 3D models in rapidly evolving military scenarios.

</details>


### [41] [HERO: Hierarchical Extrapolation and Refresh for Efficient World Models](https://arxiv.org/abs/2508.17588)
*Quanjian Song,Xinyu Wang,Donghao Zhou,Jingyu Lin,Cunjian Chen,Yue Ma,Xiu Li*

Main category: cs.CV

TL;DR: HERO是一个针对世界模型的无训练分层加速框架，通过浅层补丁刷新机制和深层线性外推方案，实现1.73倍加速且质量损失最小


<details>
  <summary>Details</summary>
Motivation: 生成式世界模型由于扩散模型的迭代特性导致推理速度慢，直接应用现有加速技术会导致质量下降。研究发现世界模型中存在特征耦合现象：浅层特征时间变化大，深层特征更稳定

Method: 分层加速策略：浅层使用补丁刷新机制选择需要重新计算的token，避免额外度量计算；深层使用线性外推方案直接估计中间特征，完全绕过注意力模块和前馈网络的计算

Result: 实验显示HERO实现了1.73倍的加速，质量退化极小，显著优于现有的扩散加速方法

Conclusion: HERO框架有效解决了世界模型加速中的质量退化问题，通过分层处理特征耦合现象，实现了高效且高质量的推理加速

Abstract: Generation-driven world models create immersive virtual environments but suffer slow inference due to the iterative nature of diffusion models. While recent advances have improved diffusion model efficiency, directly applying these techniques to world models introduces limitations such as quality degradation. In this paper, we present HERO, a training-free hierarchical acceleration framework tailored for efficient world models. Owing to the multi-modal nature of world models, we identify a feature coupling phenomenon, wherein shallow layers exhibit high temporal variability, while deeper layers yield more stable feature representations. Motivated by this, HERO adopts hierarchical strategies to accelerate inference: (i) In shallow layers, a patch-wise refresh mechanism efficiently selects tokens for recomputation. With patch-wise sampling and frequency-aware tracking, it avoids extra metric computation and remain compatible with FlashAttention. (ii) In deeper layers, a linear extrapolation scheme directly estimates intermediate features. This completely bypasses the computations in attention modules and feed-forward networks. Our experiments show that HERO achieves a 1.73$\times$ speedup with minimal quality degradation, significantly outperforming existing diffusion acceleration methods.

</details>


### [42] [JCo-MVTON: Jointly Controllable Multi-Modal Diffusion Transformer for Mask-Free Virtual Try-on](https://arxiv.org/abs/2508.17614)
*Aowen Wang,Wei Li,Hao Luo,Mengxing Ao,Chenyu Zhu,Xinyang Li,Fan Wang*

Main category: cs.CV

TL;DR: JCo-MVTON是一个无需掩码的多模态扩散变换器虚拟试穿框架，通过多模态条件融合和双向生成策略，解决了传统方法对掩码的依赖、控制粒度不足和泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统虚拟试穿系统严重依赖人体掩码，对服装属性的细粒度控制有限，且在真实场景中泛化能力差。本文旨在克服这些限制，开发一个无需掩码、可控性强且泛化能力好的虚拟试穿系统。

Method: 基于多模态扩散变换器(MM-DiT)架构，通过专用条件路径将参考人物图像和目标服装图像直接整合到去噪过程中。采用双向生成策略构建数据集：一个管道使用基于掩码的模型生成真实参考图像，另一个对称的"Try-Off"模型以自监督方式恢复相应服装图像。

Result: 在DressCode等公共基准测试中达到最先进性能，在定量指标和人工评估方面显著优于现有方法。在真实世界应用中表现出强大的泛化能力，超越了商业系统。

Conclusion: JCo-MVTON通过多模态条件融合和创新的数据集构建方法，成功实现了无需掩码的高质量虚拟试穿，在控制精度和泛化能力方面取得了显著突破，为虚拟试穿技术的实际应用提供了有效解决方案。

Abstract: Virtual try-on systems have long been hindered by heavy reliance on human body masks, limited fine-grained control over garment attributes, and poor generalization to real-world, in-the-wild scenarios. In this paper, we propose JCo-MVTON (Jointly Controllable Multi-Modal Diffusion Transformer for Mask-Free Virtual Try-On), a novel framework that overcomes these limitations by integrating diffusion-based image generation with multi-modal conditional fusion. Built upon a Multi-Modal Diffusion Transformer (MM-DiT) backbone, our approach directly incorporates diverse control signals -- such as the reference person image and the target garment image -- into the denoising process through dedicated conditional pathways that fuse features within the self-attention layers. This fusion is further enhanced with refined positional encodings and attention masks, enabling precise spatial alignment and improved garment-person integration. To address data scarcity and quality, we introduce a bidirectional generation strategy for dataset construction: one pipeline uses a mask-based model to generate realistic reference images, while a symmetric ``Try-Off'' model, trained in a self-supervised manner, recovers the corresponding garment images. The synthesized dataset undergoes rigorous manual curation, allowing iterative improvement in visual fidelity and diversity. Experiments demonstrate that JCo-MVTON achieves state-of-the-art performance on public benchmarks including DressCode, significantly outperforming existing methods in both quantitative metrics and human evaluations. Moreover, it shows strong generalization in real-world applications, surpassing commercial systems.

</details>


### [43] [CATformer: Contrastive Adversarial Transformer for Image Super-Resolution](https://arxiv.org/abs/2508.17708)
*Qinyi Tian,Spence Cox,Laura E. Dalton*

Main category: cs.CV

TL;DR: CATformer是一种结合扩散模型、对抗学习和对比学习的超分辨率Transformer网络，通过双分支架构和残差密集块实现了优异的图像重建质量


<details>
  <summary>Details</summary>
Motivation: 现有的超分辨率方法在transformer、扩散模型和GAN之间存在性能差距，需要一种能够整合这些方法优势的统一框架来提升图像重建质量和效率

Method: 提出CATformer网络，采用双分支架构：主分支使用扩散启发的transformer逐步细化潜在表示，辅助分支通过学习的潜在对比增强噪声鲁棒性，最后通过残差密集块进行特征融合和重建

Result: 在基准数据集上的大量实验表明，CATformer在效率和视觉图像质量方面均优于最近的transformer和扩散方法

Conclusion: CATformer成功弥合了transformer、扩散和GAN方法之间的性能差距，为扩散启发transformer在超分辨率中的实际应用奠定了基础

Abstract: Super-resolution remains a promising technique to enhance the quality of low-resolution images. This study introduces CATformer (Contrastive Adversarial Transformer), a novel neural network integrating diffusion-inspired feature refinement with adversarial and contrastive learning. CATformer employs a dual-branch architecture combining a primary diffusion-inspired transformer, which progressively refines latent representations, with an auxiliary transformer branch designed to enhance robustness to noise through learned latent contrasts. These complementary representations are fused and decoded using deep Residual-in-Residual Dense Blocks for enhanced reconstruction quality. Extensive experiments on benchmark datasets demonstrate that CATformer outperforms recent transformer-based and diffusion-inspired methods both in efficiency and visual image quality. This work bridges the performance gap among transformer-, diffusion-, and GAN-based methods, laying a foundation for practical applications of diffusion-inspired transformers in super-resolution.

</details>


### [44] [NGD: Neural Gradient Based Deformation for Monocular Garment Reconstruction](https://arxiv.org/abs/2508.17712)
*Soham Dasgupta,Shanthika Naik,Preet Savalia,Sujay Kumar Ingle,Avinash Sharma*

Main category: cs.CV

TL;DR: NGD方法通过神经梯度变形和自适应网格重划分技术，从单目视频中重建动态服装，解决了现有方法在细节建模和变形方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有神经渲染方法在动态服装重建中存在两个主要问题：隐式表示方法产生过于平滑的几何形状，无法建模高频细节；模板重建方法使用顶点位移导致变形伪影。

Method: 提出神经梯度变形方法(NGD)，结合自适应网格重划分策略来建模动态演化的表面细节（如褶皱），并学习动态纹理图来捕捉每帧的光照和阴影效果。

Result: 通过广泛的定性和定量评估，证明该方法相比现有最先进方法有显著改进，能够提供高质量的服装重建结果。

Conclusion: NGD方法成功解决了动态服装重建中的关键挑战，在细节建模和变形质量方面取得了突破性进展。

Abstract: Dynamic garment reconstruction from monocular video is an important yet challenging task due to the complex dynamics and unconstrained nature of the garments. Recent advancements in neural rendering have enabled high-quality geometric reconstruction with image/video supervision. However, implicit representation methods that use volume rendering often provide smooth geometry and fail to model high-frequency details. While template reconstruction methods model explicit geometry, they use vertex displacement for deformation, which results in artifacts. Addressing these limitations, we propose NGD, a Neural Gradient-based Deformation method to reconstruct dynamically evolving textured garments from monocular videos. Additionally, we propose a novel adaptive remeshing strategy for modelling dynamically evolving surfaces like wrinkles and pleats of the skirt, leading to high-quality reconstruction. Finally, we learn dynamic texture maps to capture per-frame lighting and shadow effects. We provide extensive qualitative and quantitative evaluations to demonstrate significant improvements over existing SOTA methods and provide high-quality garment reconstructions.

</details>


### [45] [Instant Preference Alignment for Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.17718)
*Yang Li,Songlin Yang,Xiaoxuan Han,Wei Wang,Jing Dong,Yueming Lyu,Ziyu Xue*

Main category: cs.CV

TL;DR: 提出基于多模态大语言模型的训练免费框架，实现实时偏好对齐的文本到图像生成，通过偏好理解和偏好引导生成两个组件，支持多轮交互式优化。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成方法依赖静态预收集偏好或微调，难以适应动态变化的用户意图，需要实现实时、训练免费的偏好对齐生成。

Method: 利用MLLM从参考图像自动提取全局偏好信号并丰富提示词，结合全局关键词控制和局部区域感知交叉注意力调制来引导扩散模型，无需额外训练。

Result: 在Viper数据集和自建基准测试中，该方法在定量指标和人工评估上均优于现有方法，为基于对话的生成和MLLM-扩散模型集成开辟了新可能。

Conclusion: 该框架成功实现了实时、训练免费的偏好对齐图像生成，支持多轮交互优化，在全局属性和局部元素对齐方面表现出色。

Abstract: Text-to-image (T2I) generation has greatly enhanced creative expression, yet achieving preference-aligned generation in a real-time and training-free manner remains challenging. Previous methods often rely on static, pre-collected preferences or fine-tuning, limiting adaptability to evolving and nuanced user intents. In this paper, we highlight the need for instant preference-aligned T2I generation and propose a training-free framework grounded in multimodal large language model (MLLM) priors. Our framework decouples the task into two components: preference understanding and preference-guided generation. For preference understanding, we leverage MLLMs to automatically extract global preference signals from a reference image and enrich a given prompt using structured instruction design. Our approach supports broader and more fine-grained coverage of user preferences than existing methods. For preference-guided generation, we integrate global keyword-based control and local region-aware cross-attention modulation to steer the diffusion model without additional training, enabling precise alignment across both global attributes and local elements. The entire framework supports multi-round interactive refinement, facilitating real-time and context-aware image generation. Extensive experiments on the Viper dataset and our collected benchmark demonstrate that our method outperforms prior approaches in both quantitative metrics and human evaluations, and opens up new possibilities for dialog-based generation and MLLM-diffusion integration.

</details>


### [46] [CEIDM: A Controlled Entity and Interaction Diffusion Model for Enhanced Text-to-Image Generation](https://arxiv.org/abs/2508.17760)
*Mingyue Yang,Dianxi Shi,Jialu Zhou,Xinyu Wei,Leqian Li,Shaowu Yang,Chunping Qiu*

Main category: cs.CV

TL;DR: CEIDM是一种基于扩散模型的文本到图像生成方法，通过双重控制机制（实体控制和交互控制）来解决复杂实体及其交互关系的生成难题。


<details>
  <summary>Details</summary>
Motivation: 在文本到图像生成中，复杂实体及其精细交互关系的控制是一个重大挑战。现有基于扩散模型的方法难以有效控制实体和交互关系，导致生成图像质量不高、交互逻辑不合理。

Method: 1. 基于大语言模型的实体交互关系挖掘方法，通过思维链提取丰富隐含交互关系；2. 交互动作聚类和偏移方法，通过全局和局部双向偏移增强语义理解；3. 实体控制网络，生成语义引导掩码并使用多尺度卷积网络和动态网络融合特征。

Result: 实验表明，CEIDM方法在实体控制和交互控制方面均优于现有最具代表性的方法，能够生成更符合现实逻辑、交互关系更合理的高质量图像。

Conclusion: CEIDM通过双重控制机制有效解决了复杂实体及其交互关系的生成问题，在图像质量和交互准确性方面取得了显著提升，为文本到图像生成提供了新的解决方案。

Abstract: In Text-to-Image (T2I) generation, the complexity of entities and their intricate interactions pose a significant challenge for T2I method based on diffusion model: how to effectively control entity and their interactions to produce high-quality images. To address this, we propose CEIDM, a image generation method based on diffusion model with dual controls for entity and interaction. First, we propose an entity interactive relationships mining approach based on Large Language Models (LLMs), extracting reasonable and rich implicit interactive relationships through chain of thought to guide diffusion models to generate high-quality images that are closer to realistic logic and have more reasonable interactive relationships. Furthermore, We propose an interactive action clustering and offset method to cluster and offset the interactive action features contained in each text prompts. By constructing global and local bidirectional offsets, we enhance semantic understanding and detail supplementation of original actions, making the model's understanding of the concept of interactive "actions" more accurate and generating images with more accurate interactive actions. Finally, we design an entity control network which generates masks with entity semantic guidance, then leveraging multi-scale convolutional network to enhance entity feature and dynamic network to fuse feature. It effectively controls entities and significantly improves image quality. Experiments show that the proposed CEIDM method is better than the most representative existing methods in both entity control and their interaction control.

</details>


### [47] [Robust Anomaly Detection in Industrial Environments via Meta-Learning](https://arxiv.org/abs/2508.17789)
*Muhammad Aqeel,Shakiba Sharifi,Marco Cristani,Francesco Setti*

Main category: cs.CV

TL;DR: RAD是一个结合标准化流和模型无关元学习的鲁棒异常检测框架，专门解决工业环境中训练数据标签噪声问题，在50%错误标签情况下仍能保持优异性能


<details>
  <summary>Details</summary>
Motivation: 工业异常检测中训练数据经常包含错误标签样本，传统方法对此敏感，需要开发对标签噪声具有鲁棒性的检测框架

Method: 采用双层优化策略：元学习快速适应不同噪声条件，不确定性量化指导自适应L2正则化保持模型稳定性。结合预训练特征提取器进行多尺度特征处理，利用标准化流的精确似然估计进行鲁棒异常评分

Result: 在MVTec-AD和KSDD2数据集上分别达到95.4%和94.6%的I-AUROC分数（干净条件下），在50%错误标签情况下仍能保持86.8%和92.1%的检测性能

Conclusion: RAD框架对噪声训练条件具有卓越的鲁棒性，能够检测各种工业场景中的细微异常，是实际应用中数据标注不完美时的实用解决方案

Abstract: Anomaly detection is fundamental for ensuring quality control and operational efficiency in industrial environments, yet conventional approaches face significant challenges when training data contains mislabeled samples-a common occurrence in real-world scenarios. This paper presents RAD, a robust anomaly detection framework that integrates Normalizing Flows with Model-Agnostic Meta-Learning to address the critical challenge of label noise in industrial settings. Our approach employs a bi-level optimization strategy where meta-learning enables rapid adaptation to varying noise conditions, while uncertainty quantification guides adaptive L2 regularization to maintain model stability. The framework incorporates multiscale feature processing through pretrained feature extractors and leverages the precise likelihood estimation capabilities of Normalizing Flows for robust anomaly scoring. Comprehensive evaluation on MVTec-AD and KSDD2 datasets demonstrates superior performance, achieving I-AUROC scores of 95.4% and 94.6% respectively under clean conditions, while maintaining robust detection capabilities above 86.8% and 92.1% even when 50% of training samples are mislabeled. The results highlight RAD's exceptional resilience to noisy training conditions and its ability to detect subtle anomalies across diverse industrial scenarios, making it a practical solution for real-world anomaly detection applications where perfect data curation is challenging.

</details>


### [48] [TemCoCo: Temporally Consistent Multi-modal Video Fusion with Visual-Semantic Collaboration](https://arxiv.org/abs/2508.17817)
*Meiqi Gong,Hao Zhang,Xunpeng Yi,Linfeng Tang,Jiayi Ma*

Main category: cs.CV

TL;DR: 首个视频融合框架，通过视觉-语义协作和时间建模，同时保证视觉保真度、语义准确性和时间一致性


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合方法直接将静态帧级图像融合技术应用于视频融合任务，忽略了时间依赖性，导致帧间结果不一致

Method: 1) 视觉-语义交互模块（Dinov2和VGG19蒸馏）；2) 时间协作模块整合视频退化增强；3) 时间增强机制和时间损失函数；4) 两个新的视频融合评估指标

Result: 在公共视频数据集上的广泛实验证明了方法的优越性

Conclusion: 提出的TemCoCo框架通过显式的时间建模和视觉-语义协作，有效解决了视频融合中的时间一致性问题，为视频融合任务提供了新的解决方案

Abstract: Existing multi-modal fusion methods typically apply static frame-based image fusion techniques directly to video fusion tasks, neglecting inherent temporal dependencies and leading to inconsistent results across frames. To address this limitation, we propose the first video fusion framework that explicitly incorporates temporal modeling with visual-semantic collaboration to simultaneously ensure visual fidelity, semantic accuracy, and temporal consistency. First, we introduce a visual-semantic interaction module consisting of a semantic branch and a visual branch, with Dinov2 and VGG19 employed for targeted distillation, allowing simultaneous enhancement of both the visual and semantic representations. Second, we pioneer integrate the video degradation enhancement task into the video fusion pipeline by constructing a temporal cooperative module, which leverages temporal dependencies to facilitate weak information recovery. Third, to ensure temporal consistency, we embed a temporal-enhanced mechanism into the network and devise a temporal loss to guide the optimization process. Finally, we introduce two innovative evaluation metrics tailored for video fusion, aimed at assessing the temporal consistency of the generated fused videos. Extensive experimental results on public video datasets demonstrate the superiority of our method. Our code is released at https://github.com/Meiqi-Gong/TemCoCo.

</details>


### [49] [Diffusion-Based Data Augmentation for Medical Image Segmentation](https://arxiv.org/abs/2508.17844)
*Maham Nazir,Muhammad Aqeel,Francesco Setti*

Main category: cs.CV

TL;DR: DiffAug是一个结合文本引导扩散生成和自动分割验证的新框架，用于解决医学图像中罕见异常分割问题，通过条件生成和动态验证在多个基准测试中取得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 医学图像分割模型在处理罕见异常时面临标注病理数据稀缺的挑战，需要一种能够生成高质量异常样本并确保准确性的方法

Method: 使用基于潜在扩散模型的文本引导生成方法，通过医学文本描述和空间掩码条件在正常图像上进行修复生成异常样本，并采用潜在空间分割网络进行动态质量验证

Result: 在三个医学成像基准测试(CVC-ClinicDB、Kvasir-SEG、REFUGE2)上达到最先进性能，Dice系数比基线提高8-10%，假阴性率降低高达28%

Conclusion: DiffAug框架通过文本引导的扩散生成和自动验证机制，有效解决了医学图像中罕见异常分割的数据稀缺问题，在早期筛查应用中具有重要价值

Abstract: Medical image segmentation models struggle with rare abnormalities due to scarce annotated pathological data. We propose DiffAug a novel framework that combines textguided diffusion-based generation with automatic segmentation validation to address this challenge. Our proposed approach uses latent diffusion models conditioned on medical text descriptions and spatial masks to synthesize abnormalities via inpainting on normal images. Generated samples undergo dynamic quality validation through a latentspace segmentation network that ensures accurate localization while enabling single-step inference. The text prompts, derived from medical literature, guide the generation of diverse abnormality types without requiring manual annotation. Our validation mechanism filters synthetic samples based on spatial accuracy, maintaining quality while operating efficiently through direct latent estimation. Evaluated on three medical imaging benchmarks (CVC-ClinicDB, Kvasir-SEG, REFUGE2), our framework achieves state-of-the-art performance with 8-10% Dice improvements over baselines and reduces false negative rates by up to 28% for challenging cases like small polyps and flat lesions critical for early detection in screening applications.

</details>


### [50] [Camera Pose Refinement via 3D Gaussian Splatting](https://arxiv.org/abs/2508.17876)
*Lulu Hao,Lipu Zhou,Zhenzhong Wei,Xu Wang*

Main category: cs.CV

TL;DR: 提出基于3D高斯泼溅的相机位姿优化框架GS-SMC，无需重新训练即可直接应用于不同场景，通过多视角渲染和极几何约束迭代优化位姿精度


<details>
  <summary>Details</summary>
Motivation: 现有相机位姿优化方法需要重新构建场景或重新训练网络，缺乏几何约束的方法精度不足，需要一种轻量级且通用的解决方案

Method: 利用现有3DGS模型渲染新视角，通过查询图像与多个渲染图像间的极几何约束进行迭代优化，可灵活选择特征提取器和匹配器

Result: 在7-Scenes和Cambridge数据集上显著优于现有方法，平移误差中位数分别降低53.3%和40.7%，旋转误差降低56.9%和53.2%

Conclusion: GS-SMC框架提供了一种无需重新训练的高精度相机位姿优化方案，充分利用3DGS的广泛适用性和几何约束的优势

Abstract: Camera pose refinement aims at improving the accuracy of initial pose estimation for applications in 3D computer vision. Most refinement approaches rely on 2D-3D correspondences with specific descriptors or dedicated networks, requiring reconstructing the scene again for a different descriptor or fully retraining the network for each scene. Some recent methods instead infer pose from feature similarity, but their lack of geometry constraints results in less accuracy. To overcome these limitations, we propose a novel camera pose refinement framework leveraging 3D Gaussian Splatting (3DGS), referred to as GS-SMC. Given the widespread usage of 3DGS, our method can employ an existing 3DGS model to render novel views, providing a lightweight solution that can be directly applied to diverse scenes without additional training or fine-tuning. Specifically, we introduce an iterative optimization approach, which refines the camera pose using epipolar geometric constraints among the query and multiple rendered images. Our method allows flexibly choosing feature extractors and matchers to establish these constraints. Extensive empirical evaluations on the 7-Scenes and the Cambridge Landmarks datasets demonstrate that our method outperforms state-of-the-art camera pose refinement approaches, achieving 53.3% and 56.9% reductions in median translation and rotation errors on 7-Scenes, and 40.7% and 53.2% on Cambridge.

</details>


### [51] [ISALux: Illumination and Segmentation Aware Transformer Employing Mixture of Experts for Low Light Image Enhancement](https://arxiv.org/abs/2508.17885)
*Raul Balmez,Alexandru Brateanu,Ciprian Orhei,Codruta Ancuti,Cosmin Ancuti*

Main category: cs.CV

TL;DR: ISALux是一种基于Transformer的低光照图像增强方法，通过整合光照和语义先验，采用混合光照语义感知自注意力机制和MoE前馈网络，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有低光照图像增强方法因基准数据集光照模式差异导致的过拟合问题，同时更好地整合光照和语义信息来提升图像增强效果。

Method: 提出HISA-MSA自注意力模块分别处理光照和语义特征，采用MoE前馈网络进行条件专家激活，并引入LoRA低秩矩阵适应来防止过拟合。

Result: 在多个专业数据集上的定性和定量评估表明，ISALux与最先进方法相比具有竞争力，消融研究验证了各组件的重要性。

Conclusion: ISALux通过有效整合光照和语义先验，结合创新的注意力机制和MoE架构，为低光照图像增强提供了有效的解决方案，代码将在发表后开源。

Abstract: We introduce ISALux, a novel transformer-based approach for Low-Light Image Enhancement (LLIE) that seamlessly integrates illumination and semantic priors. Our architecture includes an original self-attention block, Hybrid Illumination and Semantics-Aware Multi-Headed Self- Attention (HISA-MSA), which integrates illumination and semantic segmentation maps for en- hanced feature extraction. ISALux employs two self-attention modules to independently process illumination and semantic features, selectively enriching each other to regulate luminance and high- light structural variations in real-world scenarios. A Mixture of Experts (MoE)-based Feed-Forward Network (FFN) enhances contextual learning, with a gating mechanism conditionally activating the top K experts for specialized processing. To address overfitting in LLIE methods caused by distinct light patterns in benchmarking datasets, we enhance the HISA-MSA module with low-rank matrix adaptations (LoRA). Extensive qualitative and quantitative evaluations across multiple specialized datasets demonstrate that ISALux is competitive with state-of-the-art (SOTA) methods. Addition- ally, an ablation study highlights the contribution of each component in the proposed model. Code will be released upon publication.

</details>


### [52] [Designing Practical Models for Isolated Word Visual Speech Recognition](https://arxiv.org/abs/2508.17894)
*Iason Ioannis Panagos,Giorgos Sfikas,Christophoros Nikou*

Main category: cs.CV

TL;DR: 开发轻量级视觉语音识别架构，降低硬件成本同时保持良好性能


<details>
  <summary>Details</summary>
Motivation: 现有VSR系统依赖深度神经网络，计算成本高，硬件需求大，限制了在实际资源受限场景中的应用和部署

Method: 采用标准双网络设计范式，首先对图像分类中的高效模型进行基准测试，然后在时序卷积网络骨干中采用轻量级块设计，创建多个资源需求低但识别性能强的统一模型

Result: 在最大的英语单词公共数据库上的实验证明了所开发模型的有效性和实用性

Conclusion: 提出的轻量级架构能够有效降低VSR系统的硬件成本，同时保持强大的识别性能，提高了在实际应用中的可行性

Abstract: Visual speech recognition (VSR) systems decode spoken words from an input sequence using only the video data. Practical applications of such systems include medical assistance as well as human-machine interactions. A VSR system is typically employed in a complementary role in cases where the audio is corrupt or not available. In order to accurately predict the spoken words, these architectures often rely on deep neural networks in order to extract meaningful representations from the input sequence. While deep architectures achieve impressive recognition performance, relying on such models incurs significant computation costs which translates into increased resource demands in terms of hardware requirements and results in limited applicability in real-world scenarios where resources might be constrained. This factor prevents wider adoption and deployment of speech recognition systems in more practical applications. In this work, we aim to alleviate this issue by developing architectures for VSR that have low hardware costs. Following the standard two-network design paradigm, where one network handles visual feature extraction and another one utilizes the extracted features to classify the entire sequence, we develop lightweight end-to-end architectures by first benchmarking efficient models from the image classification literature, and then adopting lightweight block designs in a temporal convolution network backbone. We create several unified models with low resource requirements but strong recognition performance. Experiments on the largest public database for English words demonstrate the effectiveness and practicality of our developed models. Code and trained models will be made publicly available.

</details>


### [53] [Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation](https://arxiv.org/abs/2508.18032)
*Yaqi Li,Peng Chen,Mingyang Han,Bu Pi,Haoxiang Shi,Runzhou Zhao,Yang Yao,Xuan Zhang,Jun Song*

Main category: cs.CV

TL;DR: 本文提出Visual-CoG方法，通过阶段性奖励指导改善自回归文本到图像生成模型的多属性和模糊提示处理能力，在多个测试集上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有自回归T2I生成模型在处理多属性和模糊提示时能力有限，而传统的最终奖励指导方式导致子步骤贡献难以识别和策略优化困难。

Method: 提出Visual-CoG范式，包含语义推理、过程精炼和结果评估三个阶段，通过阶段性奖励提供全程立即指导，并构建VisCog-Bench视觉认别测试集。

Result: 在GenEval、T2I-CompBench和VisCog-Bench三个测试集上分别获得15%、5%和19%的性能提升，显示了方法的优势性能。

Conclusion: Visual-CoG通过阶段性奖励指导有效提升了T2I生成模型的语义理解和生成质量，为处理复杂提示提供了有效解决方案。

Abstract: Despite the promising progress of recent autoregressive models in text-to-image (T2I) generation, their ability to handle multi-attribute and ambiguous prompts remains limited. To address these limitations, existing works have applied chain-of-thought (CoT) to enable stage-aware visual synthesis and employed reinforcement learning (RL) to improve reasoning capabilities. However, most models provide reward signals only at the end of the generation stage. This monolithic final-only guidance makes it difficult to identify which stages contribute positively to the final outcome and may lead to suboptimal policies. To tackle this issue, we propose a Visual-Chain of Guidance (Visual-CoG) paradigm consisting of three stages: semantic reasoning, process refining, and outcome evaluation, with stage-aware rewards providing immediate guidance throughout the image generation pipeline. We further construct a visual cognition benchmark, VisCog-Bench, which comprises four subtasks to evaluate the effectiveness of semantic reasoning. Comprehensive evaluations on GenEval, T2I-CompBench, and the proposed VisCog-Bench show improvements of 15%, 5%, and 19%, respectively, demonstrating the superior performance of the proposed Visual-CoG. We will release all the resources soon.

</details>


### [54] [EventTracer: Fast Path Tracing-based Event Stream Rendering](https://arxiv.org/abs/2508.18071)
*Zhenyang Li,Xiaoyang Bai,Jinfan Lu,Pengfei Shen,Edmund Y. Lam,Yifan Peng*

Main category: cs.CV

TL;DR: EventTracer是一个基于路径追踪的高效事件流模拟管道，通过低SPP路径追踪加速渲染，并使用轻量级脉冲网络去噪生成逼真的事件序列，显著提高了模拟速度和保真度。


<details>
  <summary>Details</summary>
Motivation: 现有事件流模拟方法需要昂贵的无噪声RGB帧渲染，时间分辨率仅100-300FPS，远低于真实事件数据，无法满足大规模高频率数据需求。

Method: 采用低样本数路径追踪加速渲染，训练带有双极性泄漏积分发放(BiLIF)单元的轻量脉冲网络，使用双向地球移动距离(EMD)损失进行去噪和物理特性建模。

Result: EventTracer以约4分钟/秒720p视频的速度运行，在细节捕捉和真实事件数据相似性方面优于其他模拟器，准确继承了路径追踪的时空建模优势。

Conclusion: 该工具为低成本创建大规模事件-RGB数据集提供了有前景的解决方案，缩小了事件视觉的模拟-现实差距，可促进机器人、自动驾驶和VR/AR等应用发展。

Abstract: Simulating event streams from 3D scenes has become a common practice in event-based vision research, as it meets the demand for large-scale, high temporal frequency data without setting up expensive hardware devices or undertaking extensive data collections. Yet existing methods in this direction typically work with noiseless RGB frames that are costly to render, and therefore they can only achieve a temporal resolution equivalent to 100-300 FPS, far lower than that of real-world event data. In this work, we propose EventTracer, a path tracing-based rendering pipeline that simulates high-fidelity event sequences from complex 3D scenes in an efficient and physics-aware manner. Specifically, we speed up the rendering process via low sample-per-pixel (SPP) path tracing, and train a lightweight event spiking network to denoise the resulting RGB videos into realistic event sequences. To capture the physical properties of event streams, the network is equipped with a bipolar leaky integrate-and-fired (BiLIF) spiking unit and trained with a bidirectional earth mover distance (EMD) loss. Our EventTracer pipeline runs at a speed of about 4 minutes per second of 720p video, and it inherits the merit of accurate spatiotemporal modeling from its path tracing backbone. We show in two downstream tasks that EventTracer captures better scene details and demonstrates a greater similarity to real-world event data than other event simulators, which establishes it as a promising tool for creating large-scale event-RGB datasets at a low cost, narrowing the sim-to-real gap in event-based vision, and boosting various application scenarios such as robotics, autonomous driving, and VRAR.

</details>


### [55] [Follow My Hold: Hand-Object Interaction Reconstruction through Geometric Guidance](https://arxiv.org/abs/2508.18213)
*Ayce Idil Aytekin,Helge Rhodin,Rishabh Dabral,Christian Theobalt*

Main category: cs.CV

TL;DR: 提出基于扩散模型的单目RGB图像手持物体3D重建框架，利用手-物交互作为几何指导，通过推理时指导和优化循环设计直接生成高质量几何


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大量后处理或产生低质量重建，需要利用手-物交互的几何约束来直接生成高质量且物理合理的3D物体重建

Method: 使用潜在扩散模型，以修复后的物体外观为条件，通过速度场监督和多模态几何线索（法向深度对齐、轮廓一致性、关键点重投影）进行推理时指导，同时优化手和物体的变换，并加入SDF监督和接触约束

Result: 方法能够产生准确、鲁棒且连贯的重建结果，在遮挡情况下表现良好，并能很好地泛化到真实场景

Conclusion: 该扩散框架通过手-物交互几何指导和优化循环设计，实现了高质量的单目3D物体重建，解决了现有方法的局限性

Abstract: We propose a novel diffusion-based framework for reconstructing 3D geometry of hand-held objects from monocular RGB images by leveraging hand-object interaction as geometric guidance. Our method conditions a latent diffusion model on an inpainted object appearance and uses inference-time guidance to optimize the object reconstruction, while simultaneously ensuring plausible hand-object interactions. Unlike prior methods that rely on extensive post-processing or produce low-quality reconstructions, our approach directly generates high-quality object geometry during the diffusion process by introducing guidance with an optimization-in-the-loop design. Specifically, we guide the diffusion model by applying supervision to the velocity field while simultaneously optimizing the transformations of both the hand and the object being reconstructed. This optimization is driven by multi-modal geometric cues, including normal and depth alignment, silhouette consistency, and 2D keypoint reprojection. We further incorporate signed distance field supervision and enforce contact and non-intersection constraints to ensure physical plausibility of hand-object interaction. Our method yields accurate, robust and coherent reconstructions under occlusion while generalizing well to in-the-wild scenarios.

</details>


### [56] [GSVisLoc: Generalizable Visual Localization for Gaussian Splatting Scene Representations](https://arxiv.org/abs/2508.18242)
*Fadi Khatib,Dror Moran,Guy Trostianetsky,Yoni Kasten,Meirav Galun,Ronen Basri*

Main category: cs.CV

TL;DR: GSVisLoc是一种基于3D高斯泼溅(3DGS)场景表示的视觉定位方法，通过粗匹配、精匹配和位姿细化三个步骤，实现无需修改、重训练或额外参考图像的相机位姿估计。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉定位方法通常需要复杂的场景表示或大量参考图像，而3DGS提供了一种显式的场景表示方式，但如何直接利用3DGS进行视觉定位仍是一个挑战。

Method: 方法分为三步：1) 通过下采样和编码3D高斯生成场景特征；2) 编码图像块获取图像特征；3) 进行粗匹配、精匹配，最后通过位姿细化获得精确的相机位姿估计。

Result: 在室内外场景的标准基准测试中表现出竞争力的定位性能，优于现有的3DGS基线方法，且能够有效泛化到新场景而无需额外训练。

Conclusion: GSVisLoc成功证明了直接利用3DGS显式场景表示进行视觉定位的可行性，为3DGS在视觉定位领域的应用提供了有效解决方案。

Abstract: We introduce GSVisLoc, a visual localization method designed for 3D Gaussian Splatting (3DGS) scene representations. Given a 3DGS model of a scene and a query image, our goal is to estimate the camera's position and orientation. We accomplish this by robustly matching scene features to image features. Scene features are produced by downsampling and encoding the 3D Gaussians while image features are obtained by encoding image patches. Our algorithm proceeds in three steps, starting with coarse matching, then fine matching, and finally by applying pose refinement for an accurate final estimate. Importantly, our method leverages the explicit 3DGS scene representation for visual localization without requiring modifications, retraining, or additional reference images. We evaluate GSVisLoc on both indoor and outdoor scenes, demonstrating competitive localization performance on standard benchmarks while outperforming existing 3DGS-based baselines. Moreover, our approach generalizes effectively to novel scenes without additional training.

</details>


### [57] [ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion Models](https://arxiv.org/abs/2508.18271)
*Haitang Feng,Jie Liu,Jie Tang,Gangshan Wu,Beiqi Chen,Jianhuang Lai,Guangcong Wang*

Main category: cs.CV

TL;DR: ObjFiller-3D是一种新颖的3D修复方法，通过利用视频编辑模型而非传统2D图像修复来解决多视角不一致性问题，在PSNR和LPIPS指标上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的3D修复方法依赖多视角2D图像修复，导致不同视角间存在不一致性，产生模糊纹理、空间不连续性和视觉伪影，影响3D对象完成的准确性和真实感。

Method: 提出ObjFiller-3D方法，使用精选的视频编辑模型来填充3D对象的掩码区域，分析3D与视频之间的表示差距，并引入基于参考的3D修复方法以进一步提升重建质量。

Result: 在多个数据集上的实验显示，ObjFiller-3D相比之前方法产生更忠实和精细的重建结果（PSNR 26.6 vs NeRFiller 15.9，LPIPS 0.19 vs Instant3dit 0.25）。

Conclusion: 该方法在真实世界3D编辑应用中展现出强大的实际部署潜力，为解决3D修复中的一致性问题提供了有效解决方案。

Abstract: 3D inpainting often relies on multi-view 2D image inpainting, where the inherent inconsistencies across different inpainted views can result in blurred textures, spatial discontinuities, and distracting visual artifacts. These inconsistencies pose significant challenges when striving for accurate and realistic 3D object completion, particularly in applications that demand high fidelity and structural coherence. To overcome these limitations, we propose ObjFiller-3D, a novel method designed for the completion and editing of high-quality and consistent 3D objects. Instead of employing a conventional 2D image inpainting model, our approach leverages a curated selection of state-of-the-art video editing model to fill in the masked regions of 3D objects. We analyze the representation gap between 3D and videos, and propose an adaptation of a video inpainting model for 3D scene inpainting. In addition, we introduce a reference-based 3D inpainting method to further enhance the quality of reconstruction. Experiments across diverse datasets show that compared to previous methods, ObjFiller-3D produces more faithful and fine-grained reconstructions (PSNR of 26.6 vs. NeRFiller (15.9) and LPIPS of 0.19 vs. Instant3dit (0.25)). Moreover, it demonstrates strong potential for practical deployment in real-world 3D editing applications. Project page: https://objfiller3d.github.io/ Code: https://github.com/objfiller3d/ObjFiller-3D .

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [58] [GWM: Towards Scalable Gaussian World Models for Robotic Manipulation](https://arxiv.org/abs/2508.17600)
*Guanxing Lu,Baoxiong Jia,Puhao Li,Yixin Chen,Ziwei Wang,Yansong Tang,Siyuan Huang*

Main category: cs.RO

TL;DR: 提出基于高斯原语传播的3D世界模型GWM，通过扩散变换器和3D变分自编码器实现精细场景重建，提升机器人操作任务的模仿学习和强化学习性能


<details>
  <summary>Details</summary>
Motivation: 现有基于图像的世界模型缺乏稳健的几何信息，无法提供一致的空间和物理理解，需要开发能更好理解三维世界的模型

Method: 提出高斯世界模型(GWM)，通过推断高斯原语在机器人动作影响下的传播来重建未来状态，核心是结合3D变分自编码器的潜在扩散变换器，使用高斯泼溅进行精细场景级重建

Result: 模拟和真实实验表明GWM能精确预测不同机器人动作条件下的未来场景，训练的策略在性能上显著超越现有最先进方法

Conclusion: GWM展示了3D世界模型在数据扩展方面的潜力，既能增强模仿学习的视觉表示，又能作为支持基于模型强化学习的神经模拟器

Abstract: Training robot policies within a learned world model is trending due to the inefficiency of real-world interactions. The established image-based world models and policies have shown prior success, but lack robust geometric information that requires consistent spatial and physical understanding of the three-dimensional world, even pre-trained on internet-scale video sources. To this end, we propose a novel branch of world model named Gaussian World Model (GWM) for robotic manipulation, which reconstructs the future state by inferring the propagation of Gaussian primitives under the effect of robot actions. At its core is a latent Diffusion Transformer (DiT) combined with a 3D variational autoencoder, enabling fine-grained scene-level future state reconstruction with Gaussian Splatting. GWM can not only enhance the visual representation for imitation learning agent by self-supervised future prediction training, but can serve as a neural simulator that supports model-based reinforcement learning. Both simulated and real-world experiments depict that GWM can precisely predict future scenes conditioned on diverse robot actions, and can be further utilized to train policies that outperform the state-of-the-art by impressive margins, showcasing the initial data scaling potential of 3D world model.

</details>


### [59] [A holistic perception system of internal and external monitoring for ground autonomous vehicles: AutoTRUST paradigm](https://arxiv.org/abs/2508.17969)
*Alexandros Gkillas,Christos Anagnostopoulos,Nikos Piperigkos,Dimitris Tsiktsiris,Theofilos Christodoulou,Theofanis Siamatras,Dimitrios Triantafyllou,Christos Basdekis,Theoktisti Marinopoulou,Panagiotis Lepentsiotis,Elefterios Blitsis,Aggeliki Zacharaki,Nearchos Stylianidis,Leonidas Katelaris,Lamberto Salvan,Aris S. Lalos,Christos Laoudias,Antonios Lalas,Konstantinos Votis*

Main category: cs.RO

TL;DR: 这篇论文提出了一种用于自主驾驶汽车内外监测的整体感知系统，通过AI技术优化车内体验和环境感知性能。


<details>
  <summary>Details</summary>
Motivation: 为自主驾驶汽车开发一种整体性的感知解决方案，同时监测车内驾驶员行为和车外环境，以提升安全性和乘坐体验。

Method: 系统包含两部分：内部监测系统使用多摄像头进行面部识别和行为预测，集成大语言模型作为虚拟助手，以及AI智能传感器监测空气质量和热舒适性。外部监测系统采用LiDAR技术进行成本效益高的语义分割，对低质量3D点云进行超分辨率处理。

Result: 该整体感知框架已在欧盟Horizon Europe项目AutoTRUST中开发，并部署在ALKE提供的真实电动汽车上。在意大利Ispra联合研究中心的实验验证显示，该感知架构的模块化模块实现了显著的性能和效率提升。

Conclusion: 该研究成功开发了一种创新的AI驱动的自适应感知框架，能够在真实汽车环境中有效地监测车内外环境，为自主驾驶汽车的安全性和用户体验提供了综合性解决方案。

Abstract: This paper introduces a holistic perception system for internal and external monitoring of autonomous vehicles, with the aim of demonstrating a novel AI-leveraged self-adaptive framework of advanced vehicle technologies and solutions that optimize perception and experience on-board. Internal monitoring system relies on a multi-camera setup designed for predicting and identifying driver and occupant behavior through facial recognition, exploiting in addition a large language model as virtual assistant. Moreover, the in-cabin monitoring system includes AI-empowered smart sensors that measure air-quality and perform thermal comfort analysis for efficient on and off-boarding. On the other hand, external monitoring system perceives the surrounding environment of vehicle, through a LiDAR-based cost-efficient semantic segmentation approach, that performs highly accurate and efficient super-resolution on low-quality raw 3D point clouds. The holistic perception framework is developed in the context of EU's Horizon Europe programm AutoTRUST, and has been integrated and deployed on a real electric vehicle provided by ALKE. Experimental validation and evaluation at the integration site of Joint Research Centre at Ispra, Italy, highlights increased performance and efficiency of the modular blocks of the proposed perception architecture.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [60] [HunyuanVideo-Foley: Multimodal Diffusion with Representation Alignment for High-Fidelity Foley Audio Generation](https://arxiv.org/abs/2508.16930)
*Sizhe Shan,Qiulin Li,Yutao Cui,Miles Yang,Yuehai Wang,Qun Yang,Jin Zhou,Zhao Zhong*

Main category: eess.AS

TL;DR: HunyuanVideo-Foley是一个端到端的文本-视频-音频生成框架，通过三个核心创新解决了视频到音频生成的关键挑战，包括多模态数据稀缺、模态不平衡和现有方法的音频质量限制，实现了与视觉动态和语义上下文精确对齐的高保真音频合成。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成技术虽然能产生视觉上真实的内容，但缺乏同步音频严重影响了沉浸感。需要解决多模态数据稀缺、模态不平衡以及现有方法音频质量有限等关键挑战。

Method: 采用三个核心创新：(1)可扩展的数据管道，通过自动标注策划100k小时的多模态数据集；(2)使用自监督音频特征的表征对齐策略指导潜在扩散训练；(3)新颖的多模态扩散变换器，通过联合注意力实现双流音频-视频融合，并通过交叉注意力注入文本语义。

Result: 综合评估表明，HunyuanVideo-Foley在音频保真度、视觉-语义对齐、时间对齐和分布匹配等方面实现了新的最先进性能。

Conclusion: 该框架成功解决了视频到音频生成的关键技术挑战，为多模态内容生成提供了有效的解决方案，显著提升了音频生成的质量和同步精度。

Abstract: Recent advances in video generation produce visually realistic content, yet the absence of synchronized audio severely compromises immersion. To address key challenges in video-to-audio generation, including multimodal data scarcity, modality imbalance and limited audio quality in existing methods, we propose HunyuanVideo-Foley, an end-to-end text-video-to-audio framework that synthesizes high-fidelity audio precisely aligned with visual dynamics and semantic context. Our approach incorporates three core innovations: (1) a scalable data pipeline curating 100k-hour multimodal datasets through automated annotation; (2) a representation alignment strategy using self-supervised audio features to guide latent diffusion training, efficiently improving audio quality and generation stability; (3) a novel multimodal diffusion transformer resolving modal competition, containing dual-stream audio-video fusion through joint attention, and textual semantic injection via cross-attention. Comprehensive evaluations demonstrate that HunyuanVideo-Foley achieves new state-of-the-art performance across audio fidelity, visual-semantic alignment, temporal alignment and distribution matching. The demo page is available at: https://szczesnys.github.io/hunyuanvideo-foley/.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [61] [WebSight: A Vision-First Architecture for Robust Web Agents](https://arxiv.org/abs/2508.16987)
*Tanvir Bhathal,Asanshay Gupta*

Main category: cs.AI

TL;DR: WebSight是一个纯视觉的自主网页代理，通过WebSight-7B视觉语言模型实现无需HTML/DOM输入的网页交互，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 消除对HTML或DOM输入的依赖，开发纯视觉感知的网页交互系统，提高网页导航的鲁棒性和可解释性。

Method: 采用模块化多智能体架构，包括规划、推理、视觉动作和验证智能体，通过情景记忆机制协调。使用LoRA在Wave-UI-25K数据集上微调WebSight-7B视觉语言模型。

Result: WebSight-7B在Showdown Clicks基准测试中达到58.84%的top-1准确率，优于多个大型通用模型。完整WebSight代理在WebVoyager基准测试中达到68.0%的成功率，超越OpenAI和HCompany的系统。

Conclusion: WebSight和WebSight-7B为可解释、鲁棒且高效的视觉网页导航设立了新标准。

Abstract: We introduce WebSight, a vision-based autonomous web agent, designed to interact with web environments purely through visual perception, eliminating dependence on HTML or DOM-based inputs. Central to our approach we introduce our new model, WebSight-7B, a fine-tuned vision-language model optimized for UI element interaction, trained using LoRA on a web-focused subset of the Wave-UI-25K dataset. WebSight integrates this model into a modular multi-agent architecture, comprising planning, reasoning, vision-action, and verification agents, coordinated through an episodic memory mechanism.   WebSight-7B achieves a top-1 accuracy of 58.84% on the Showdown Clicks benchmark, outperforming several larger generalist models while maintaining lower latency. The full WebSight agent achieves a 68.0% success rate on the WebVoyager benchmark, surpassing systems from labs such as OpenAI (61.0%) and HCompany (Runner H, 67.0%). Among tasks completed, WebSight answers correctly 97.14% of the time, indicating high precision. Together, WebSight and WebSight-7B establish a new standard for interpretable, robust, and efficient visual web navigation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [62] [Generative Feature Imputing - A Technique for Error-resilient Semantic Communication](https://arxiv.org/abs/2508.17957)
*Jianhao Huang,Qunsong Zeng,Hongyang Du,Kaibin Huang*

Main category: cs.LG

TL;DR: 提出生成式特征填补框架，通过空间错误集中打包、扩散模型特征重建和语义感知功率分配，提升语义通信在数字系统中的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 语义通信在6G网络中具有高效通信优势，但在数字系统中面临传输错误导致语义内容失真的挑战，需要确保鲁棒性

Method: 1. 空间错误集中打包策略：基于信道映射编码特征元素，集中特征失真；2. 生成式特征填补方法：使用扩散模型重建丢失特征；3. 语义感知功率分配：根据语义重要性进行不等差错保护

Result: 在块衰落条件下优于传统方法（DJSCC和JPEG2000），获得更高的语义准确性和更低的LPIPS分数

Conclusion: 所提框架能有效提升语义通信系统对传输错误的鲁棒性，为6G网络中的语义通信部署提供了可行解决方案

Abstract: Semantic communication (SemCom) has emerged as a promising paradigm for achieving unprecedented communication efficiency in sixth-generation (6G) networks by leveraging artificial intelligence (AI) to extract and transmit the underlying meanings of source data. However, deploying SemCom over digital systems presents new challenges, particularly in ensuring robustness against transmission errors that may distort semantically critical content. To address this issue, this paper proposes a novel framework, termed generative feature imputing, which comprises three key techniques. First, we introduce a spatial error concentration packetization strategy that spatially concentrates feature distortions by encoding feature elements based on their channel mappings, a property crucial for both the effectiveness and reduced complexity of the subsequent techniques. Second, building on this strategy, we propose a generative feature imputing method that utilizes a diffusion model to efficiently reconstruct missing features caused by packet losses. Finally, we develop a semantic-aware power allocation scheme that enables unequal error protection by allocating transmission power according to the semantic importance of each packet. Experimental results demonstrate that the proposed framework outperforms conventional approaches, such as Deep Joint Source-Channel Coding (DJSCC) and JPEG2000, under block fading conditions, achieving higher semantic accuracy and lower Learned Perceptual Image Patch Similarity (LPIPS) scores.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [63] [Neural Proteomics Fields for Super-resolved Spatial Proteomics Prediction](https://arxiv.org/abs/2508.17389)
*Bokai Zhao,Weiyang Shi,Hanqing Chao,Zijiang Yang,Yiyang Zhang,Ming Song,Tianzi Jiang*

Main category: q-bio.QM

TL;DR: 提出了首个用于空间蛋白质组学超分辨率任务的深度学习模型NPF，通过空间建模和形态建模模块，在连续空间中重建蛋白质分布，在基准数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前测序空间蛋白质组学技术空间分辨率低，且蛋白质表达存在显著的组织间变异性，现有分子数据预测方法性能受限。

Method: NPF将seq-SP建模为连续空间中的蛋白质重建问题，为每个组织训练专用网络，包含学习组织特异性蛋白质空间分布的空间建模模块和提取组织特异性形态特征的形态建模模块。

Result: NPF在Pseudo-Visium SP基准数据集上实现了最先进的性能，且参数量更少。

Conclusion: NPF模型具有推动空间蛋白质组学研究的潜力，代码和数据集已开源。

Abstract: Spatial proteomics maps protein distributions in tissues, providing transformative insights for life sciences. However, current sequencing-based technologies suffer from low spatial resolution, and substantial inter-tissue variability in protein expression further compromises the performance of existing molecular data prediction methods. In this work, we introduce the novel task of spatial super-resolution for sequencing-based spatial proteomics (seq-SP) and, to the best of our knowledge, propose the first deep learning model for this task--Neural Proteomics Fields (NPF). NPF formulates seq-SP as a protein reconstruction problem in continuous space by training a dedicated network for each tissue. The model comprises a Spatial Modeling Module, which learns tissue-specific protein spatial distributions, and a Morphology Modeling Module, which extracts tissue-specific morphological features. Furthermore, to facilitate rigorous evaluation, we establish an open-source benchmark dataset, Pseudo-Visium SP, for this task. Experimental results demonstrate that NPF achieves state-of-the-art performance with fewer learnable parameters, underscoring its potential for advancing spatial proteomics research. Our code and dataset are publicly available at https://github.com/Bokai-Zhao/NPF.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [64] [Generating Synthetic Contrast-Enhanced Chest CT Images from Non-Contrast Scans Using Slice-Consistent Brownian Bridge Diffusion Network](https://arxiv.org/abs/2508.16897)
*Pouya Shiri,Xin Yi,Neel P. Mistry,Samaneh Javadinia,Mohammad Chegini,Seok-Bum Ko,Amirali Baniasadi,Scott J. Adams*

Main category: eess.IV

TL;DR: 基于桥扩散模型从非对比CT生成合成对比增强CTA图像，避免对比剂风险，保持3D解剖完整性


<details>
  <summary>Details</summary>
Motivation: 对比剂CT成像对胸主动脉疾病诊断至关重要，但对比剂存在肾毒性和过敏反应风险，需要无对比剂的高保真合成CTA图像

Method: 使用Slice-Consistent Brownian Bridge Diffusion Model (SC-BBDM)，在保持切片一致性的同时建模复杂映射，通过预处理流程包括重采样、对称归一化配准和扩张分割掩模提取主动脉结构

Result: 在两个数据集（仅主动脉和主动脉+心脏）上验证，相比基线方法能更好地保持血管结构并提高对比度保真度

Conclusion: 提出的桥扩散方法首次实现了从非对比CT合成对比增强CTA，在低内存预算下保持3D解剖完整性，具有临床转化潜力

Abstract: Contrast-enhanced computed tomography (CT) imaging is essential for diagnosing and monitoring thoracic diseases, including aortic pathologies. However, contrast agents pose risks such as nephrotoxicity and allergic-like reactions. The ability to generate high-fidelity synthetic contrast-enhanced CT angiography (CTA) images without contrast administration would be transformative, enhancing patient safety and accessibility while reducing healthcare costs. In this study, we propose the first bridge diffusion-based solution for synthesizing contrast-enhanced CTA images from non-contrast CT scans. Our approach builds on the Slice-Consistent Brownian Bridge Diffusion Model (SC-BBDM), leveraging its ability to model complex mappings while maintaining consistency across slices. Unlike conventional slice-wise synthesis methods, our framework preserves full 3D anatomical integrity while operating in a high-resolution 2D fashion, allowing seamless volumetric interpretation under a low memory budget. To ensure robust spatial alignment, we implement a comprehensive preprocessing pipeline that includes resampling, registration using the Symmetric Normalization method, and a sophisticated dilated segmentation mask to extract the aorta and surrounding structures. We create two datasets from the Coltea-Lung dataset: one containing only the aorta and another including both the aorta and heart, enabling a detailed analysis of anatomical context. We compare our approach against baseline methods on both datasets, demonstrating its effectiveness in preserving vascular structures while enhancing contrast fidelity.

</details>


### [65] [Deep Learning Architectures for Medical Image Denoising: A Comparative Study of CNN-DAE, CADTra, and DCMIEDNet](https://arxiv.org/abs/2508.17223)
*Asadullah Bin Rahman,Masud Ibn Afjal,Md. Abdulla Al Mamun*

Main category: eess.IV

TL;DR: 本文对三种深度学习架构(CNN-DAE、CADTra、DCMIEDNet)在MRI脑图像去噪方面进行了系统比较，发现DCMIEDNet在低噪声水平表现最佳，CADTra在高噪声条件下更具鲁棒性，所有深度学习方法均显著优于传统小波方法。


<details>
  <summary>Details</summary>
Motivation: 医学成像模式容易受到噪声污染，这会降低诊断效用和临床评估准确性，因此需要开发有效的去噪方法来提高医学图像质量。

Method: 使用Figshare MRI脑数据集，在多个高斯噪声强度(σ=10,15,25)下系统评估三种深度学习架构：CNN-DAE、CADTra和DCMIEDNet，并与传统小波方法进行对比。

Result: DCMIEDNet在低噪声水平(σ=10和15)表现最优，PSNR分别为32.921±2.350 dB和30.943±2.339 dB；CADTra在高噪声条件(σ=25)下最鲁棒，PSNR达27.671±2.091 dB；深度学习方法比传统方法提升5-8 dB。

Conclusion: 本研究为医学图像去噪建立了量化基准，揭示了不同架构在不同噪声强度下的特定优势，为临床应用中选择合适的去噪方法提供了指导。

Abstract: Medical imaging modalities are inherently susceptible to noise contamination that degrades diagnostic utility and clinical assessment accuracy. This paper presents a comprehensive comparative evaluation of three state-of-the-art deep learning architectures for MRI brain image denoising: CNN-DAE, CADTra, and DCMIEDNet. We systematically evaluate these models across multiple Gaussian noise intensities ($\sigma = 10, 15, 25$) using the Figshare MRI Brain Dataset. Our experimental results demonstrate that DCMIEDNet achieves superior performance at lower noise levels, with PSNR values of $32.921 \pm 2.350$ dB and $30.943 \pm 2.339$ dB for $\sigma = 10$ and $15$ respectively. However, CADTra exhibits greater robustness under severe noise conditions ($\sigma = 25$), achieving the highest PSNR of $27.671 \pm 2.091$ dB. All deep learning approaches significantly outperform traditional wavelet-based methods, with improvements ranging from 5-8 dB across tested conditions. This study establishes quantitative benchmarks for medical image denoising and provides insights into architecture-specific strengths for varying noise intensities.

</details>


### [66] [Semantic Diffusion Posterior Sampling for Cardiac Ultrasound Dehazing](https://arxiv.org/abs/2508.17326)
*Tristan S. W. Stevens,Oisín Nolan,Ruud J. G. van Sloun*

Main category: eess.IV

TL;DR: 提出了一种基于语义引导扩散模型的超声心动图去雾算法，通过结合像素级噪声模型和生成先验，在MICCAI挑战赛中表现出色


<details>
  <summary>Details</summary>
Motivation: 超声心动图在心脏成像中至关重要，但多径混响导致的雾状伪影会严重影响图像质量，特别是在难以成像的患者中

Method: 将雾状输入的语义分割得到的像素级噪声模型集成到扩散后验采样框架中，并由在干净超声数据上训练的生成先验进行引导

Result: 在挑战数据集上的定量评估显示，该方法在对比度和保真度指标上均表现出强劲性能

Conclusion: 该语义引导的扩散去雾算法能有效提升超声心动图图像质量，为临床诊断提供更清晰的影像支持

Abstract: Echocardiography plays a central role in cardiac imaging, offering dynamic views of the heart that are essential for diagnosis and monitoring. However, image quality can be significantly degraded by haze arising from multipath reverberations, particularly in difficult-to-image patients. In this work, we propose a semantic-guided, diffusion-based dehazing algorithm developed for the MICCAI Dehazing Echocardiography Challenge (DehazingEcho2025). Our method integrates a pixel-wise noise model, derived from semantic segmentation of hazy inputs into a diffusion posterior sampling framework guided by a generative prior trained on clean ultrasound data. Quantitative evaluation on the challenge dataset demonstrates strong performance across contrast and fidelity metrics. Code for the submitted algorithm is available at https://github.com/tristan-deep/semantic-diffusion-echo-dehazing.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [67] [Negative Shanshui: Real-time Interactive Ink Painting Synthesis](https://arxiv.org/abs/2508.16612)
*Aven-Le Zhou*

Main category: cs.HC

TL;DR: 这篇论文提出了Negative Shanshui，一种基于稳定散布模型的实时交互式AI生成技术，通过重解中国山水墨笔画来应对生态危机。系统整合了视线驱动的图像修复和帧内插值技术，以VR体验形式展现动态变形动画。


<details>
  <summary>Details</summary>
Motivation: 重新解读传统中国山水墨笔画，以艺术形式回应人类世纪的生态危机问题，通过交互式技术使观众能够深度参与和反思。

Method: 使用细调的Stable Diffusion模型进行实时推理，结合视线驱动的图像修复(inpainting)和帧内插值技术，构建交互式VR系统。包括完整的技术流水线、优化策略和多模态部署。

Result: 实现了根据观众视线进行动态变形的动画效果，成功在艺术节中展出。观众反馈显示了通过共情、矛盾和批判性反思等不同方式与作品交互。

Conclusion: Negative Shanshui成功将传统中国画与先进AI技术相结合，创造了一种能够引发观众对生态问题深度思考的交互式艺术体验，为艺术与技术融合提供了新的可能性。

Abstract: This paper presents Negative Shanshui, a real-time interactive AI synthesis approach that reinterprets classical Chinese landscape ink painting, i.e., shanshui, to engage with ecological crises in the Anthropocene. Negative Shanshui optimizes a fine-tuned Stable Diffusion model for real-time inferences and integrates it with gaze-driven inpainting, frame interpolation; it enables dynamic morphing animations in response to the viewer's gaze and presents as an interactive virtual reality (VR) experience. The paper describes the complete technical pipeline, covering the system framework, optimization strategies, gaze-based interaction, and multimodal deployment in an art festival. Further analysis of audience feedback collected during its public exhibition highlights how participants variously engaged with the work through empathy, ambivalence, and critical reflection.

</details>
