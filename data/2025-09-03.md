<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 6]
- [cs.CV](#cs.CV) [Total: 53]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Curve-based slicer for multi-axis DLP 3D printing](https://arxiv.org/abs/2509.00040)
*Chengkai Dai,Tao Liu,Dezhao Guo,Binzhi Sun,Guoxin Fang,Yeung Yam,Charlie C. L. Wang*

Main category: cs.GR

TL;DR: 这篇论文提出了一种新的曲线基切片方法，通过动态变化层方向来解决DLP 3D打印中的悬空区域和阶梯效应问题，同时保持高分辨率和快速打印的优势。


<details>
  <summary>Details</summary>
Motivation: 解决DLP 3D打印中的关键挑战，包括大角度悬空区域和阶梯效应问题，同时保持高分辨率和快速打印的优势。

Method: 将切片问题形式化为优化任务，通过计算参数曲线来定义切片层和模型分割，这些曲线同时定义了构建平台的运动轨迹，并可以优化以满足无碰撞运动和无浮动沉积等制造目标。

Result: 通过在机器人多轴DLP打印设备上进行实际实验，验证了优化曲线可以稳固地指导复杂几何形状的平滑高质量制造。

Conclusion: 该曲线基切片方法为DLP 3D打印提供了一种有效的解决方案，能够在保持高分辨率和快速打印优势的同时，有效减少悬空问题和阶梯效应，实现高质量复杂形状的制造。

Abstract: This paper introduces a novel curve-based slicing method for generating planar layers with dynamically varying orientations in digital light processing (DLP) 3D printing. Our approach effectively addresses key challenges in DLP printing, such as regions with large overhangs and staircase artifacts, while preserving its intrinsic advantages of high resolution and fast printing speeds. We formulate the slicing problem as an optimization task, in which parametric curves are computed to define both the slicing layers and the model partitioning through their tangent planes. These curves inherently define motion trajectories for the build platform and can be optimized to meet critical manufacturing objectives, including collision-free motion and floating-free deposition. We validate our method through physical experiments on a robotic multi-axis DLP printing setup, demonstrating that the optimized curves can robustly guide smooth, high-quality fabrication of complex geometries.

</details>


### [2] [Lightning Fast Caching-based Parallel Denoising Prediction for Accelerating Talking Head Generation](https://arxiv.org/abs/2509.00052)
*Jianzhi Long,Wenhao Sun,Rongcheng Tu,Dacheng Tao*

Main category: cs.GR

TL;DR: 提出LightningCP和DFA两种创新方法，针对说话头生成任务优化扩散模型，显著提升推理速度同时保持视频质量


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型加速方法未能充分利用说话头生成任务特有的时空冗余性，导致推理速度慢，限制了实际应用

Method: 1. LightningCP：缓存静态特征，在推理时绕过大多数模型层；使用缓存特征和估计噪声潜在作为输入实现并行预测，绕过顺序采样。2. DFA：利用说话头视频的空间解耦特性，将注意力计算限制在动态前景区域；在某些层中移除参考特征以获得额外加速

Result: 大量实验证明该框架显著提高了推理速度，同时保持了视频质量

Conclusion: 提出的任务特定框架通过利用说话头生成的独特特性，成功解决了扩散模型推理速度慢的问题，为实际应用提供了可行的解决方案

Abstract: Diffusion-based talking head models generate high-quality, photorealistic videos but suffer from slow inference, limiting practical applications. Existing acceleration methods for general diffusion models fail to exploit the temporal and spatial redundancies unique to talking head generation. In this paper, we propose a task-specific framework addressing these inefficiencies through two key innovations. First, we introduce Lightning-fast Caching-based Parallel denoising prediction (LightningCP), caching static features to bypass most model layers in inference time. We also enable parallel prediction using cached features and estimated noisy latents as inputs, efficiently bypassing sequential sampling. Second, we propose Decoupled Foreground Attention (DFA) to further accelerate attention computations, exploiting the spatial decoupling in talking head videos to restrict attention to dynamic foreground regions. Additionally, we remove reference features in certain layers to bring extra speedup. Extensive experiments demonstrate that our framework significantly improves inference speed while preserving video quality.

</details>


### [3] [3D-LATTE: Latent Space 3D Editing from Textual Instructions](https://arxiv.org/abs/2509.00269)
*Maria Parelli,Michael Oechsle,Michael Niemeyer,Federico Tombari,Andreas Geiger*

Main category: cs.GR

TL;DR: 基于本地3D液化模型的训练免费编辑方法，通过3D注意力地图混合和几何意识正则化，实现高保真度的一致性3D编辑


<details>
  <summary>Details</summary>
Motivation: 当前基于2D先验的3D资产编辑方法存在视图不一致问题，编辑质量远落后于生成模型

Method: 在本地3D液化模型潜空间直接操作3D几何，通过3D注意力地图混合、几何意识正则化、傀里叶域调制策略和3D增强精细化步骤

Result: 方法在广泛的形状和语义操作中实现了高保真度、精确和稳健的编辑效果，超越了之前的3D编辑方法

Conclusion: 通过直接操作3D液化模型的本地潜空间，可以免除视图不一致问题，实现更高质量的3D资产编辑

Abstract: Despite the recent success of multi-view diffusion models for text/image-based 3D asset generation, instruction-based editing of 3D assets lacks surprisingly far behind the quality of generation models. The main reason is that recent approaches using 2D priors suffer from view-inconsistent editing signals. Going beyond 2D prior distillation methods and multi-view editing strategies, we propose a training-free editing method that operates within the latent space of a native 3D diffusion model, allowing us to directly manipulate 3D geometry. We guide the edit synthesis by blending 3D attention maps from the generation with the source object. Coupled with geometry-aware regularization guidance, a spectral modulation strategy in the Fourier domain and a refinement step for 3D enhancement, our method outperforms previous 3D editing methods enabling high-fidelity, precise, and robust edits across a wide range of shapes and semantic manipulations.

</details>


### [4] [RealMat: Realistic Materials with Diffusion and Reinforcement Learning](https://arxiv.org/abs/2509.01134)
*Xilong Zhou,Pedro Figueiredo,Miloš Hašan,Valentin Deschaintre,Paul Guerrero,Yiwei Hu,Nima Khademi Kalantari*

Main category: cs.GR

TL;DR: RealMat是一个基于扩散模型的材质生成器，通过结合合成材质数据和真实材质照片，利用强化学习提升生成材质的真实感，解决了合成数据与真实材质之间的视觉差距问题。


<details>
  <summary>Details</summary>
Motivation: 现有材质生成方法主要基于合成数据训练，虽然能提供精确的材质图监督，但与真实世界材质存在显著视觉差距。而基于真实闪光照片的方法虽然能保证真实感，但数据规模和多样性有限。

Method: 1. 在合成材质图2×2网格上微调预训练的Stable Diffusion XL模型；2. 通过强化学习进一步微调模型，使用基于大规模真实材质图像数据集构建的真实感奖励函数来鼓励生成更真实的材质。

Result: 该方法相比基础模型和相关工作，显著提高了生成材质的真实感，有效缩小了合成材质与真实材质之间的视觉差距。

Conclusion: RealMat通过结合合成数据学习和强化学习的真实性优化，成功实现了高质量、高真实感的材质生成，为3D内容创作提供了更实用的工具。

Abstract: Generative models for high-quality materials are particularly desirable to make 3D content authoring more accessible. However, the majority of material generation methods are trained on synthetic data. Synthetic data provides precise supervision for material maps, which is convenient but also tends to create a significant visual gap with real-world materials. Alternatively, recent work used a small dataset of real flash photographs to guarantee realism, however such data is limited in scale and diversity. To address these limitations, we propose RealMat, a diffusion-based material generator that leverages realistic priors, including a text-to-image model and a dataset of realistic material photos under natural lighting. In RealMat, we first finetune a pretrained Stable Diffusion XL (SDXL) with synthetic material maps arranged in $2 \times 2$ grids. This way, our model inherits some realism of SDXL while learning the data distribution of the synthetic material grids. Still, this creates a realism gap, with some generated materials appearing synthetic. We propose to further finetune our model through reinforcement learning (RL), encouraging the generation of realistic materials. We develop a realism reward function for any material image under natural lighting, by collecting a large-scale dataset of realistic material images. We show that this approach increases generated materials' realism compared to our base model and related work.

</details>


### [5] [GRMM: Real-Time High-Fidelity Gaussian Morphable Head Model with Learned Residuals](https://arxiv.org/abs/2509.02141)
*Mohit Mendiratta,Mayur Deshmukh,Kartik Teotia,Vladislav Golyanik,Adam Kortylewski,Christian Theobalt*

Main category: cs.GR

TL;DR: GRMM是首个基于高斯溅射的全头3D可变形模型，通过残差几何和外观组件增强传统3DMM，实现高保真细节捕捉和实时渲染


<details>
  <summary>Details</summary>
Motivation: 传统PCA网格模型分辨率有限，神经体积方法渲染速度慢，现有高斯溅射面部模型依赖网格先验，无法捕捉细粒度几何和完整头部细节

Method: 在基础3DMM上添加残差几何和外观组件，使用粗粒度解码器生成网格变形，细粒度解码器表示高斯外观，轻量CNN增强图像真实感

Result: 在单目3D人脸重建、新视角合成和表情迁移任务中超越现有方法，保持75FPS实时渲染性能

Conclusion: GRMM实现了高保真细节捕捉与实时渲染的平衡，EXPRESS-50数据集为高斯基3DMM的身份表情解耦提供了重要支撑

Abstract: 3D Morphable Models (3DMMs) enable controllable facial geometry and expression editing for reconstruction, animation, and AR/VR, but traditional PCA-based mesh models are limited in resolution, detail, and photorealism. Neural volumetric methods improve realism but remain too slow for interactive use. Recent Gaussian Splatting (3DGS) based facial models achieve fast, high-quality rendering but still depend solely on a mesh-based 3DMM prior for expression control, limiting their ability to capture fine-grained geometry, expressions, and full-head coverage. We introduce GRMM, the first full-head Gaussian 3D morphable model that augments a base 3DMM with residual geometry and appearance components, additive refinements that recover high-frequency details such as wrinkles, fine skin texture, and hairline variations. GRMM provides disentangled control through low-dimensional, interpretable parameters (e.g., identity shape, facial expressions) while separately modelling residuals that capture subject- and expression-specific detail beyond the base model's capacity. Coarse decoders produce vertex-level mesh deformations, fine decoders represent per-Gaussian appearance, and a lightweight CNN refines rasterised images for enhanced realism, all while maintaining 75 FPS real-time rendering. To learn consistent, high-fidelity residuals, we present EXPRESS-50, the first dataset with 60 aligned expressions across 50 identities, enabling robust disentanglement of identity and expression in Gaussian-based 3DMMs. Across monocular 3D face reconstruction, novel-view synthesis, and expression transfer, GRMM surpasses state-of-the-art methods in fidelity and expression accuracy while delivering interactive real-time performance.

</details>


### [6] [Unifi3D: A Study on 3D Representations for Generation and Reconstruction in a Common Framework](https://arxiv.org/abs/2509.02474)
*Nina Wiedemann,Sainan Liu,Quentin Leboutet,Katelyn Gao,Benjamin Ummenhofer,Michael Paulitsch,Kai Yuan*

Main category: cs.GR

TL;DR: 提出了一个统一的评估框架来比较不同3D表示方法（如体素网格、神经辐射场等）在重建和生成任务中的性能，重点关注质量、计算效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着文本和图像生成的快速发展，3D生成研究日益重要，但3D表示方法多样且分散，缺乏统一的评估标准来比较不同方法的优劣。

Method: 开发了一个统一的评估框架，对多种3D表示方法进行系统性比较，包括预处理、网格重建、自编码器压缩和生成等完整流程的评估。

Result: 研究发现重建误差对整体性能有显著影响，需要将生成和重建联合评估，为不同应用场景选择合适的3D模型提供了指导。

Conclusion: 该框架为3D生成领域提供了重要的评估基准，有助于开发更鲁棒和特定应用的3D生成解决方案，相关代码已开源。

Abstract: Following rapid advancements in text and image generation, research has increasingly shifted towards 3D generation. Unlike the well-established pixel-based representation in images, 3D representations remain diverse and fragmented, encompassing a wide variety of approaches such as voxel grids, neural radiance fields, signed distance functions, point clouds, or octrees, each offering distinct advantages and limitations. In this work, we present a unified evaluation framework designed to assess the performance of 3D representations in reconstruction and generation. We compare these representations based on multiple criteria: quality, computational efficiency, and generalization performance. Beyond standard model benchmarking, our experiments aim to derive best practices over all steps involved in the 3D generation pipeline, including preprocessing, mesh reconstruction, compression with autoencoders, and generation. Our findings highlight that reconstruction errors significantly impact overall performance, underscoring the need to evaluate generation and reconstruction jointly. We provide insights that can inform the selection of suitable 3D models for various applications, facilitating the development of more robust and application-specific solutions in 3D generation. The code for our framework is available at https://github.com/isl-org/unifi3d.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [7] [Deep Learning-Driven Multimodal Detection and Movement Analysis of Objects in Culinary](https://arxiv.org/abs/2509.00033)
*Tahoshin Alam Ishat*

Main category: cs.CV

TL;DR: 结合YOLOv8分割模型、LSTM手势动作序列模型和Whisper语音识别，为TinyLLaMA提供多模态数据以生成烹饪步骤指南


<details>
  <summary>Details</summary>
Motivation: 探索计算机视觉在日常厨房活动中的扩展应用，构建能够在复杂环境中执行特定任务的鲁棒系统

Method: 集成YOLOv8图像分割、LSTM手势动作序列分析和Whisper语音识别，为TinyLLaMA大型语言模型提供多模态输入数据

Result: 开发了一个能够预测食谱并生成逐步烹饪指南的任务专用系统

Conclusion: 这项工作扩展了计算机视觉在日常关键任务中的应用领域，证明了多模态AI系统在复杂环境中的无限应用潜力

Abstract: This is a research exploring existing models and fine tuning them to combine a YOLOv8 segmentation model, a LSTM model trained on hand point motion sequence and a ASR (whisper-base) to extract enough data for a LLM (TinyLLaMa) to predict the recipe and generate text creating a step by step guide for the cooking procedure. All the data were gathered by the author for a robust task specific system to perform best in complex and challenging environments proving the extension and endless application of computer vision in daily activities such as kitchen work. This work extends the field for many more crucial task of our day to day life.

</details>


### [8] [Dual-Stage Global and Local Feature Framework for Image Dehazing](https://arxiv.org/abs/2509.00108)
*Anas M. Ali,Anis Koubaa,Bilel Benjdira*

Main category: cs.CV

TL;DR: 提出SGLC框架解决高分辨率图像去雾问题，通过全局特征生成器和局部特征增强器的组合，有效融合全局上下文和局部细节，显著提升去雾性能。


<details>
  <summary>Details</summary>
Motivation: 现有去雾模型在高分辨率图像上表现不佳，通常需要降采样或分块处理，导致性能下降，主要原因是难以有效结合全局上下文信息和局部精细细节。

Method: 提出SGLC框架，包含全局特征生成器(GFG)和局部特征增强器(LFE)。GFG生成初步去雾结果，LFE细化局部细节，捕获全局外观与局部结构的交互作用。

Result: 在高分辨率数据集上实验显示，SGLC显著提升了PSNR指标，证明其在大规模图像去雾中的有效性。

Conclusion: SGLC是模型无关的框架，可与任何去雾网络结合，通过全局-局部特征融合机制，显著提升高分辨率环境下的视觉保真度。

Abstract: Addressing the challenge of removing atmospheric fog or haze from digital images, known as image dehazing, has recently gained significant traction in the computer vision community. Although contemporary dehazing models have demonstrated promising performance, few have thoroughly investigated high-resolution imagery. In such scenarios, practitioners often resort to downsampling the input image or processing it in smaller patches, which leads to a notable performance degradation. This drop is primarily linked to the difficulty of effectively combining global contextual information with localized, fine-grained details as the spatial resolution grows. In this chapter, we propose a novel framework, termed the Streamlined Global and Local Features Combinator (SGLC), to bridge this gap and enable robust dehazing for high-resolution inputs. Our approach is composed of two principal components: the Global Features Generator (GFG) and the Local Features Enhancer (LFE). The GFG produces an initial dehazed output by focusing on broad contextual understanding of the scene. Subsequently, the LFE refines this preliminary output by enhancing localized details and pixel-level features, thereby capturing the interplay between global appearance and local structure. To evaluate the effectiveness of SGLC, we integrated it with the Uformer architecture, a state-of-the-art dehazing model. Experimental results on high-resolution datasets reveal a considerable improvement in peak signal-to-noise ratio (PSNR) when employing SGLC, indicating its potency in addressing haze in large-scale imagery. Moreover, the SGLC design is model-agnostic, allowing any dehazing network to be augmented with the proposed global-and-local feature fusion mechanism. Through this strategy, practitioners can harness both scene-level cues and granular details, significantly improving visual fidelity in high-resolution environments.

</details>


### [9] [Category-level Text-to-Image Retrieval Improved: Bridging the Domain Gap with Diffusion Models and Vision Encoders](https://arxiv.org/abs/2509.00177)
*Faizan Farooq Khan,Vladan Stojnić,Zakaria Laskar,Mohamed Elhoseiny,Giorgos Tolias*

Main category: cs.CV

TL;DR: 提出了一种两步文本到图像检索方法：先用扩散模型将文本查询转换为视觉查询，再用视觉模型计算图像相似度，通过聚合网络融合多模态信息，显著提升检索性能


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型（如CLIP）将文本和图像映射到表示空间中较远的区域，存在模态鸿沟问题，限制了检索性能

Method: 1. 使用生成扩散模型将文本查询转换为视觉查询图像；2. 用视觉模型估计图像到图像相似度；3. 引入聚合网络将多个生成图像融合为单一向量表示，并跨两种查询模态融合相似度分数

Result: 经过广泛评估，该方法在依赖纯文本查询的检索方法上表现一致更优

Conclusion: 该方法有效利用了视觉编码器、视觉语言模型和文本到图像生成模型的进展，成功弥合了模态鸿沟，提升了语义类别指定查询的文本到图像检索性能

Abstract: This work explores text-to-image retrieval for queries that specify or describe a semantic category. While vision-and-language models (VLMs) like CLIP offer a straightforward open-vocabulary solution, they map text and images to distant regions in the representation space, limiting retrieval performance. To bridge this modality gap, we propose a two-step approach. First, we transform the text query into a visual query using a generative diffusion model. Then, we estimate image-to-image similarity with a vision model. Additionally, we introduce an aggregation network that combines multiple generated images into a single vector representation and fuses similarity scores across both query modalities. Our approach leverages advancements in vision encoders, VLMs, and text-to-image generation models. Extensive evaluations show that it consistently outperforms retrieval methods relying solely on text queries. Source code is available at: https://github.com/faixan-khan/cletir

</details>


### [10] [Generative AI for Industrial Contour Detection: A Language-Guided Vision System](https://arxiv.org/abs/2509.00284)
*Liang Gong,Tommy,Wang,Sara Chaker,Yanchen Dong,Fouad Bousetouane,Brenden Morton,Mark Mendez*

Main category: cs.CV

TL;DR: 这篇论文提出了一种语言导向的生成式视觉系统，用于制造业中的残余轮廓检测，通过GAN和视觉-语言模型实现了CAD级别精度的轮廓检测。


<details>
  <summary>Details</summary>
Motivation: 工业计算机视觉系统遇到噪声、材料变异性和非控制成像条件的挑战，传统边缘检测器和手工管道效果有限。

Method: 系统分为三个阶段：数据获取和预处理、使用条件GAN生成轮廓、通过视觉-语言模型进行多模态轮廓精细化，其中标准化提示通过人在循环过程制作并通过图像-文本导向合成应用。

Result: 在专有FabTrack数据集上，系统提高了轮廓保真度，增强了边缘连续性和几何对齐，减少了手工追踪。在精细化阶段，GPT-image-1在结构准确性和感知质量方面均超过Gemini 2.0 Flash。

Conclusion: 这些发现证明了VLM导向的生成式工作流在推进工业计算机视觉领域超越传统管道限制方面的潜力。

Abstract: Industrial computer vision systems often struggle with noise, material variability, and uncontrolled imaging conditions, limiting the effectiveness of classical edge detectors and handcrafted pipelines. In this work, we present a language-guided generative vision system for remnant contour detection in manufacturing, designed to achieve CAD-level precision. The system is organized into three stages: data acquisition and preprocessing, contour generation using a conditional GAN, and multimodal contour refinement through vision-language modeling, where standardized prompts are crafted in a human-in-the-loop process and applied through image-text guided synthesis. On proprietary FabTrack datasets, the proposed system improved contour fidelity, enhancing edge continuity and geometric alignment while reducing manual tracing. For the refinement stage, we benchmarked several vision-language models, including Google's Gemini 2.0 Flash, OpenAI's GPT-image-1 integrated within a VLM-guided workflow, and open-source baselines. Under standardized conditions, GPT-image-1 consistently outperformed Gemini 2.0 Flash in both structural accuracy and perceptual quality. These findings demonstrate the promise of VLM-guided generative workflows for advancing industrial computer vision beyond the limitations of classical pipelines.

</details>


### [11] [Iterative Low-rank Network for Hyperspectral Image Denoising](https://arxiv.org/abs/2509.00356)
*Jin Ye,Fengchao Xiong,Jun Zhou,Yuntao Qian*

Main category: cs.CV

TL;DR: ILRNet是一种结合模型驱动和数据驱动方法的迭代低秩网络，通过将秩最小化模块嵌入U-Net架构，在特征域利用HSI的光谱低秩特性进行有效去噪。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像去噪是后续任务的关键预处理步骤。干净的HSI通常存在于低维子空间中，可以通过低秩和稀疏表示来捕捉，但如何在有效去噪的同时保持图像细节是一个挑战。

Method: 提出ILRNet，在U-Net架构中嵌入秩最小化模块(RMM)，将特征图转换到小波域并对低频分量应用奇异值阈值处理(SVT)，自适应学习参数以灵活捕捉不同场景下的低秩特性，并采用迭代细化过程自适应结合中间去噪结果和噪声输入。

Result: 实验结果表明，ILRNet在合成和真实世界噪声去除任务中都达到了最先进的性能。

Conclusion: ILRNet成功整合了模型驱动和数据驱动方法的优势，通过特征域的低秩特性和迭代细化过程，实现了高效的高光谱图像去噪和细节保持。

Abstract: Hyperspectral image (HSI) denoising is a crucial preprocessing step for subsequent tasks. The clean HSI usually reside in a low-dimensional subspace, which can be captured by low-rank and sparse representation, known as the physical prior of HSI. It is generally challenging to adequately use such physical properties for effective denoising while preserving image details. This paper introduces a novel iterative low-rank network (ILRNet) to address these challenges. ILRNet integrates the strengths of model-driven and data-driven approaches by embedding a rank minimization module (RMM) within a U-Net architecture. This module transforms feature maps into the wavelet domain and applies singular value thresholding (SVT) to the low-frequency components during the forward pass, leveraging the spectral low-rankness of HSIs in the feature domain. The parameter, closely related to the hyperparameter of the singular vector thresholding algorithm, is adaptively learned from the data, allowing for flexible and effective capture of low-rankness across different scenarios. Additionally, ILRNet features an iterative refinement process that adaptively combines intermediate denoised HSIs with noisy inputs. This manner ensures progressive enhancement and superior preservation of image details. Experimental results demonstrate that ILRNet achieves state-of-the-art performance in both synthetic and real-world noise removal tasks.

</details>


### [12] [NoiseCutMix: A Novel Data Augmentation Approach by Mixing Estimated Noise in Diffusion Models](https://arxiv.org/abs/2509.00378)
*Shumpei Takezaki,Ryoma Bise,Shinnosuke Matsuo*

Main category: cs.CV

TL;DR: 提出NoiseCutMix方法，将CutMix概念引入扩散模型生成过程，通过融合两个类别的估计噪声来生成自然、高分辨率的跨类别融合图像


<details>
  <summary>Details</summary>
Motivation: 传统CutMix等方法在组合不同类别图像时会产生不自然的边界，需要一种能够生成自然融合图像的数据增强方法

Method: 在扩散模型中部分组合两个不同类别对应的估计噪声，利用扩散模型生成高质量图像的能力和CutMix的特征融合特性

Result: 在分类实验中验证了方法的有效性，相比传统多类别组合数据增强方法和随机图像生成方法表现更好

Conclusion: NoiseCutMix能够生成自然且高分辨率的跨类别融合图像，为数据增强提供了新的有效方法

Abstract: In this study, we propose a novel data augmentation method that introduces the concept of CutMix into the generation process of diffusion models, thereby exploiting both the ability of diffusion models to generate natural and high-resolution images and the characteristic of CutMix, which combines features from two classes to create diverse augmented data. Representative data augmentation methods for combining images from multiple classes include CutMix and MixUp. However, techniques like CutMix often result in unnatural boundaries between the two images due to contextual differences. Therefore, in this study, we propose a method, called NoiseCutMix, to achieve natural, high-resolution image generation featuring the fused characteristics of two classes by partially combining the estimated noise corresponding to two different classes in a diffusion model. In the classification experiments, we verified the effectiveness of the proposed method by comparing it with conventional data augmentation techniques that combine multiple classes, random image generation using Stable Diffusion, and combinations of these methods. Our codes are available at: https://github.com/shumpei-takezaki/NoiseCutMix

</details>


### [13] [Visually Grounded Narratives: Reducing Cognitive Burden in Researcher-Participant Interaction](https://arxiv.org/abs/2509.00381)
*Runtong Wu,Jiayao Song,Fei Teng,Xianhao Ren,Yuyan Gao,Kailun Yang*

Main category: cs.CV

TL;DR: 提出NAME新范式，将研究文档转化为连贯的故事图像，减轻叙事分析中文本解读的认知负担，仅需0.96%数据即可显著提升生成质量


<details>
  <summary>Details</summary>
Motivation: 传统叙事分析需要将各种数据转化为手写叙事故事，给研究者和参与者带来巨大数据分析负担，需要更高效、参与者友好的叙事制作和呈现方法

Method: 开发NAME范式，包含演员定位和形状模块以促进合理图像生成，设计包含三个关键维度的评估指标来客观衡量生成角色的感知质量和叙事一致性

Result: 在不同数据划分方案下均表现最优，仅用0.96%数据就将FID分数从195降至152；相同数据量下显著改进：70:30划分从175降至152，95:5划分从96降至49；新指标得分3.62超越基线2.66

Conclusion: NAME范式成功减轻了叙事分析中的认知负担，为叙事探究领域提供了更高效和参与者友好的方法，在数据效率和生成质量方面都取得了显著突破

Abstract: Narrative inquiry has been one of the prominent application domains for the analysis of human experience, aiming to know more about the complexity of human society. However, researchers are often required to transform various forms of data into coherent hand-drafted narratives in storied form throughout narrative analysis, which brings an immense burden of data analysis. Participants, too, are expected to engage in member checking and presentation of these narrative products, which involves reviewing and responding to large volumes of documents. Given the dual burden and the need for more efficient and participant-friendly approaches to narrative making and representation, we made a first attempt: (i) a new paradigm is proposed, NAME, as the initial attempt to push the field of narrative inquiry. Name is able to transfer research documents into coherent story images, alleviating the cognitive burden of interpreting extensive text-based materials during member checking for both researchers and participants. (ii) We develop an actor location and shape module to facilitate plausible image generation. (iii) We have designed a set of robust evaluation metrics comprising three key dimensions to objectively measure the perceptual quality and narrative consistency of generated characters. Our approach consistently demonstrates state-of-the-art performance across different data partitioning schemes. Remarkably, while the baseline relies on the full 100% of the available data, our method requires only 0.96% yet still reduces the FID score from 195 to 152. Under identical data volumes, our method delivers substantial improvements: for the 70:30 split, the FID score decreases from 175 to 152, and for the 95:5 split, it is nearly halved from 96 to 49. Furthermore, the proposed model achieves a score of 3.62 on the newly introduced metric, surpassing the baseline score of 2.66.

</details>


### [14] [Double-Constraint Diffusion Model with Nuclear Regularization for Ultra-low-dose PET Reconstruction](https://arxiv.org/abs/2509.00395)
*Mengxiao Geng,Ran Hong,Bingxuan Li,Qiegen Liu*

Main category: cs.CV

TL;DR: 提出双约束扩散模型(DCDM)用于超低剂量PET重建，通过冻结预训练扩散模型权重并注入可训练的双约束控制器，大幅减少可训练参数数量，在已知和未知剂量水平下均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 超低剂量PET重建可减少患者辐射暴露和检查时间，但会导致噪声增加和图像细节减少，需要开发既能保持图像质量又具有灵活性的重建方法。

Method: DCDM冻结预训练扩散模型权重，注入核变换器约束(NTC)和编码连接约束(ENC)两个可训练模块。NTC利用核范数近似矩阵秩最小化，将低秩特性整合到Transformer架构中；ENC利用压缩特征表示编码和控制扩散模型。

Result: 在UDPET公共数据集和临床数据集上的实验表明，DCDM在已知剂量减少因子(DRF)和未知DRF场景下均优于最先进方法，即使在1%全剂量的超低剂量水平下也表现优异。

Conclusion: DCDM通过双约束控制器有效减少了可训练参数数量，提高了重建灵活性，能够适应不同剂量水平而无需重新训练所有参数，在超低剂量PET重建中具有重要价值。

Abstract: Ultra-low-dose positron emission tomography (PET) reconstruction holds significant potential for reducing patient radiation exposure and shortening examination times. However, it may also lead to increased noise and reduced imaging detail, which could decrease the image quality. In this study, we present a Double-Constraint Diffusion Model (DCDM), which freezes the weights of a pre-trained diffusion model and injects a trainable double-constraint controller into the encoding architecture, greatly reducing the number of trainable parameters for ultra-low-dose PET reconstruction. Unlike full fine-tuning models, DCDM can adapt to different dose levels without retraining all model parameters, thereby improving reconstruction flexibility. Specifically, the two constraint modules, named the Nuclear Transformer Constraint (NTC) and the Encoding Nexus Constraint (ENC), serve to refine the pre-trained diffusion model. The NTC leverages the nuclear norm as an approximation for matrix rank minimization, integrates the low-rank property into the Transformer architecture, and enables efficient information extraction from low-dose images and conversion into compressed feature representations in the latent space. Subsequently, the ENC utilizes these compressed feature representations to encode and control the pre-trained diffusion model, ultimately obtaining reconstructed PET images in the pixel space. In clinical reconstruction, the compressed feature representations from NTC help select the most suitable ENC for efficient unknown low-dose PET reconstruction. Experiments conducted on the UDPET public dataset and the Clinical dataset demonstrated that DCDM outperforms state-of-the-art methods on known dose reduction factors (DRF) and generalizes well to unknown DRF scenarios, proving valuable even at ultra-low dose levels, such as 1% of the full dose.

</details>


### [15] [DAOVI: Distortion-Aware Omnidirectional Video Inpainting](https://arxiv.org/abs/2509.00396)
*Ryosuke Seshimo,Mariko Isogawa*

Main category: cs.CV

TL;DR: 提出了DAOVI模型，专门针对全景视频修复中的几何畸变问题，通过测地距离评估运动信息和深度感知特征传播模块，在定量和定性评估中均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 全景视频具有宽视场角，但经常包含不需要的物体。现有视频修复方法主要针对普通窄视场视频，无法处理全景视频在等距柱状投影中的畸变问题。

Method: 提出DAOVI深度学习模型，包含两个核心模块：1) 基于测地距离的图像空间时序运动信息评估模块；2) 特征空间中处理几何畸变的深度感知特征传播模块。

Result: 实验结果表明，所提出的方法在定量和定性评估中都优于现有的视频修复方法。

Conclusion: DAOVI模型有效解决了全景视频修复中的几何畸变问题，通过专门设计的模块实现了更好的空间和时间一致性保持。

Abstract: Omnidirectional videos that capture the entire surroundings are employed in a variety of fields such as VR applications and remote sensing. However, their wide field of view often causes unwanted objects to appear in the videos. This problem can be addressed by video inpainting, which enables the natural removal of such objects while preserving both spatial and temporal consistency. Nevertheless, most existing methods assume processing ordinary videos with a narrow field of view and do not tackle the distortion in equirectangular projection of omnidirectional videos. To address this issue, this paper proposes a novel deep learning model for omnidirectional video inpainting, called Distortion-Aware Omnidirectional Video Inpainting (DAOVI). DAOVI introduces a module that evaluates temporal motion information in the image space considering geodesic distance, as well as a depth-aware feature propagation module in the feature space that is designed to address the geometric distortion inherent to omnidirectional videos. The experimental results demonstrate that our proposed method outperforms existing methods both quantitatively and qualitatively.

</details>


### [16] [DevilSight: Augmenting Monocular Human Avatar Reconstruction through a Virtual Perspective](https://arxiv.org/abs/2509.00403)
*Yushuo Chen,Ruizhi Shao,Youxin Pang,Hongwen Zhang,Xinyi Wu,Rihui Wu,Yebin Liu*

Main category: cs.CV

TL;DR: 提出基于视频生成模型Human4DiT的新框架，通过生成多视角人体运动作为额外监督信号，解决单目视频重建人体avatar时细节缺失和视角不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从单目视频重建人体avatar时存在两个主要问题：无法捕捉输入中的细粒度动态细节，以及在新颖视角下生成合理细节的能力不足，这主要源于avatar模型表示能力有限和观测数据不足。

Method: 1) 利用先进视频生成模型Human4DiT生成替代视角的人体运动作为额外监督信号；2) 通过视频微调注入物理身份确保运动一致性；3) 采用基于patch的去噪算法获得更高分辨率输出。

Result: 实验结果表明，该方法优于当前最先进的方法，验证了所提出策略的有效性。

Conclusion: 通过利用视频生成模型提供多视角监督信号，结合身份保持和细节增强策略，成功解决了单目视频人体avatar重建中的细节缺失和视角一致性问题。

Abstract: We present a novel framework to reconstruct human avatars from monocular videos. Recent approaches have struggled either to capture the fine-grained dynamic details from the input or to generate plausible details at novel viewpoints, which mainly stem from the limited representational capacity of the avatar model and insufficient observational data. To overcome these challenges, we propose to leverage the advanced video generative model, Human4DiT, to generate the human motions from alternative perspective as an additional supervision signal. This approach not only enriches the details in previously unseen regions but also effectively regularizes the avatar representation to mitigate artifacts. Furthermore, we introduce two complementary strategies to enhance video generation: To ensure consistent reproduction of human motion, we inject the physical identity into the model through video fine-tuning. For higher-resolution outputs with finer details, a patch-based denoising algorithm is employed. Experimental results demonstrate that our method outperforms recent state-of-the-art approaches and validate the effectiveness of our proposed strategies.

</details>


### [17] [SemaMIL: Semantic Reordering with Retrieval-Guided State Space Modeling for Whole Slide Image Classification](https://arxiv.org/abs/2509.00442)
*Lubin Gan,Xiaoman Wu,Jing Zhang,Zhifeng Wang,Linhao Qu,Siying Wu,Xiaoyan Sun*

Main category: cs.CV

TL;DR: SemaMIL是一种新的多实例学习方法，通过语义重排序和语义引导检索状态空间模块，在计算病理学中实现了最先进的准确率，同时降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的注意力机制MIL方法忽略了上下文关系，Transformer模型计算成本高且容易过拟合，状态空间模型虽然计算复杂度低但打乱补丁顺序会破坏组织学意义。

Method: 提出SemaMIL框架，包含语义重排序（SR）方法对语义相似的补丁进行聚类和排序，以及语义引导检索状态空间模块（SRSM）选择代表性查询子集来调整状态空间参数。

Result: 在四个WSI亚型数据集上的评估显示，SemaMIL相比强基线方法实现了最先进的准确率，同时使用了更少的FLOPs和参数。

Conclusion: SemaMIL通过结合语义重排序和状态空间模型，在保持组织学意义和可解释性的同时，有效提升了全局建模能力并降低了计算成本。

Abstract: Multiple instance learning (MIL) has become the leading approach for extracting discriminative features from whole slide images (WSIs) in computational pathology. Attention-based MIL methods can identify key patches but tend to overlook contextual relationships. Transformer models are able to model interactions but require quadratic computational cost and are prone to overfitting. State space models (SSMs) offer linear complexity, yet shuffling patch order disrupts histological meaning and reduces interpretability. In this work, we introduce SemaMIL, which integrates Semantic Reordering (SR), an adaptive method that clusters and arranges semantically similar patches in sequence through a reversible permutation, with a Semantic-guided Retrieval State Space Module (SRSM) that chooses a representative subset of queries to adjust state space parameters for improved global modeling. Evaluation on four WSI subtype datasets shows that, compared to strong baselines, SemaMIL achieves state-of-the-art accuracy with fewer FLOPs and parameters.

</details>


### [18] [TRUST: Token-dRiven Ultrasound Style Transfer for Cross-Device Adaptation](https://arxiv.org/abs/2509.00508)
*Nhat-Tuong Do-Tran,Ngoc-Hoang-Lam Le,Ian Chiu,Po-Tsun Paul Kuo,Ching-Chun Huang*

Main category: cs.CV

TL;DR: TRUST是一个面向超声图像的双流框架，通过token驱动机制在保持源内容的同时转移目标域风格，提升下游任务性能


<details>
  <summary>Details</summary>
Motivation: 解决不同设备获取的超声图像风格差异导致下游任务性能下降的问题，现有方法未能有效筛选与下游任务相关的风格特征

Method: 提出token驱动的双流框架，包含数据视角选择合适目标token和模型视角识别最优目标token，并引入行为镜像损失和辅助提示

Result: 在超声数据集上的实验表明，TRUST在视觉质量和下游任务性能方面均优于现有UI2I方法

Conclusion: TRUST框架有效解决了超声图像风格迁移中的内容保持和风格对齐问题，为多设备超声图像分析提供了有效解决方案

Abstract: Ultrasound images acquired from different devices exhibit diverse styles, resulting in decreased performance of downstream tasks. To mitigate the style gap, unpaired image-to-image (UI2I) translation methods aim to transfer images from a source domain, corresponding to new device acquisitions, to a target domain where a frozen task model has been trained for downstream applications. However, existing UI2I methods have not explicitly considered filtering the most relevant style features, which may result in translated images misaligned with the needs of downstream tasks. In this work, we propose TRUST, a token-driven dual-stream framework that preserves source content while transferring the common style of the target domain, ensuring that content and style remain unblended. Given multiple styles in the target domain, we introduce a Token-dRiven (TR) module that operates from two perspectives: (1) a data view--selecting "suitable" target tokens corresponding to each source token, and (2) a model view--identifying ``optimal" target tokens for the downstream model, guided by a behavior mirror loss. Additionally, we inject auxiliary prompts into the source encoder to match content representation with downstream behavior. Experimental results on ultrasound datasets demonstrate that TRUST outperforms existing UI2I methods in both visual quality and downstream task performance.

</details>


### [19] [A Modality-agnostic Multi-task Foundation Model for Human Brain Imaging](https://arxiv.org/abs/2509.00549)
*Peirong Liu,Oula Puonti,Xiaoling Hu,Karthik Gopinath,Annabel Sorby-Adams,Daniel C. Alexander,W. Taylor Kimberly,Juan E. Iglesias*

Main category: cs.CV

TL;DR: BrainFM是一个模态无关的多任务视觉基础模型，通过创新的"轻度到重度"生成和"真实-合成"混合训练策略，能够处理多种脑成像任务，对图像外观变化具有强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有学习方法在校准医学成像（如CT）中表现优异，但在未校准模态（特别是MRI）中泛化能力差，对MR对比度、分辨率和方向的差异敏感，限制了在多样化临床协议中的广泛应用。

Method: 提出"轻度到重度"主体内生成和"真实-合成"混合训练策略，使模型对获取图像的外观（如模态、对比度、变形、分辨率、伪影）具有弹性。

Result: 在11个公共数据集上评估，证明BrainFM在所有任务和输入模态中都具有鲁棒性和有效性。

Conclusion: BrainFM作为一个模态无关的基础模型，能够直接应用于五种基本脑成像任务，为多样化临床协议提供了广泛的适用性。

Abstract: Recent learning-based approaches have made astonishing advances in calibrated medical imaging like computerized tomography (CT), yet they struggle to generalize in uncalibrated modalities -- notably magnetic resonance (MR) imaging, where performance is highly sensitive to the differences in MR contrast, resolution, and orientation. This prevents broad applicability to diverse real-world clinical protocols. Here we introduce BrainFM, a modality-agnostic, multi-task vision foundation model for human brain imaging. With the proposed "mild-to-severe" intra-subject generation and "real-synth" mix-up training strategy, BrainFM is resilient to the appearance of acquired images (e.g., modality, contrast, deformation, resolution, artifacts), and can be directly applied to five fundamental brain imaging tasks, including image synthesis for CT and T1w/T2w/FLAIR MRI, anatomy segmentation, scalp-to-cortical distance, bias field estimation, and registration. We evaluate the efficacy of BrainFM on eleven public datasets, and demonstrate its robustness and effectiveness across all tasks and input modalities. Code is available at https://github.com/jhuldr/BrainFM.

</details>


### [20] [C-DiffDet+: Fusing Global Scene Context with Generative Denoising for High-Fidelity Object Detection](https://arxiv.org/abs/2509.00578)
*Abdellah Zakaria Sellam,Ilyes Benaissa,Salah Eddine Bekhouche,Abdenour Hadid,Vito Renó,Cosimo Distante*

Main category: cs.CV

TL;DR: 通过上下文感知融合(CAF)技术，在汽车损坏检测领域实现了更准确的细粒度物体检测，超越了现有的DiffusionDet模型


<details>
  <summary>Details</summary>
Motivation: 解决DiffusionDet在上下文依赖场景中仅使用局部特征条件化的性能限制问题

Method: 提出上下文感知融合(CAF)机制，通过交叉注意力集成全局场景上下文和局部建议特征，使用独立编码器获取全面环境信息

Result: 在CarDD标准数据集上超越了现有最佳模型，为上下文感知的细粒度物体检测建立了新的性能标杆

Conclusion: CAF框架通过全局上下文融合显著提升了生成式检测的性能，为具有挑战性的细粒度视觉领域提供了更可靠的解决方案

Abstract: Fine-grained object detection in challenging visual domains, such as vehicle damage assessment, presents a formidable challenge even for human experts to resolve reliably. While DiffusionDet has advanced the state-of-the-art through conditional denoising diffusion, its performance remains limited by local feature conditioning in context-dependent scenarios. We address this fundamental limitation by introducing Context-Aware Fusion (CAF), which leverages cross-attention mechanisms to integrate global scene context with local proposal features directly. The global context is generated using a separate dedicated encoder that captures comprehensive environmental information, enabling each object proposal to attend to scene-level understanding. Our framework significantly enhances the generative detection paradigm by enabling each object proposal to attend to comprehensive environmental information. Experimental results demonstrate an improvement over state-of-the-art models on the CarDD benchmark, establishing new performance benchmarks for context-aware object detection in fine-grained domains

</details>


### [21] [ER-LoRA: Effective-Rank Guided Adaptation for Weather-Generalized Depth Estimation](https://arxiv.org/abs/2509.00665)
*Weilong Yan,Xin Zhang,Robby T. Tan*

Main category: cs.CV

TL;DR: 提出STM策略，通过参数高效微调视觉基础模型，仅使用少量正常天气数据实现恶劣天气下的单目深度估计，在多个真实基准测试中表现优异


<details>
  <summary>Details</summary>
Motivation: 恶劣天气条件下的单目深度估计面临缺乏可靠真值和难以从未标注真实数据学习的挑战，现有方法存在域差距或违反光度假设的问题

Method: 引入选择-调优-保持(STM)策略，基于熵秩和稳定秩结构分解预训练权重，自适应选择秩数和任务感知奇异方向进行初始化，并施加主方向正则化

Result: 在四个真实世界基准测试中，STM不仅优于现有PEFT方法和全微调，还超越了使用合成恶劣数据训练的方法，甚至超过了深度基础模型

Conclusion: STM策略能够灵活适应任务同时保持预训练模型的强泛化能力，为恶劣天气下的深度估计提供了有效解决方案

Abstract: Monocular depth estimation under adverse weather conditions (e.g.\ rain, fog, snow, and nighttime) remains highly challenging due to the lack of reliable ground truth and the difficulty of learning from unlabeled real-world data. Existing methods often rely on synthetic adverse data with pseudo-labels, which suffer from domain gaps, or employ self-supervised learning, which violates photometric assumptions in adverse scenarios. In this work, we propose to achieve weather--generalized depth estimation by Parameter--Efficient Fine--Tuning (PEFT) of Vision Foundation Models (VFMs), using only a small amount of high--visibility (normal) data. While PEFT has shown strong performance in semantic tasks such as segmentation, it remains underexplored for geometry--centric tasks like depth estimation -- especially in terms of balancing effective adaptation with the preservation of pretrained knowledge. To this end, we introduce the Selecting--Tuning--Maintaining (STM) strategy, which structurally decomposes the pretrained weights of VFMs based on two kinds of effective ranks (entropy--rank and stable--rank). In the tuning phase, we adaptively select the proper rank number as well as the task--aware singular directions for initialization, based on the entropy--rank and full--tuned weight; while in the maintaining stage, we enforce a principal direction regularization based on the stable--rank. This design guarantees flexible task adaptation while preserving the strong generalization capability of the pretrained VFM. Extensive experiments on four real--world benchmarks across diverse weather conditions demonstrate that STM not only outperforms existing PEFT methods and full fine--tuning but also surpasses methods trained with adverse synthetic data, and even the depth foundation model

</details>


### [22] [MarkSplatter: Generalizable Watermarking for 3D Gaussian Splatting Model via Splatter Image Structure](https://arxiv.org/abs/2509.00757)
*Xiufeng Huang,Ziyuan Luo,Qi Song,Ruofei Wang,Renjie Wan*

Main category: cs.CV

TL;DR: 首个通用化3D高斯泼溅水印框架，通过单次前向传播实现高效版权保护，无需对每个预定义消息进行昂贵微调


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅技术日益流行，需要有效的版权保护方法。现有水印方法需要对每个预定义消息进行计算昂贵的微调

Method: 提出GaussianBridge将非结构化3D高斯转换为Splatter Image格式，实现直接神经处理；设计高斯不确定性感知热图预测策略保持视觉质量；开发基于密集分割的提取机制确保鲁棒性

Result: 实现了高效的单次前向传播水印嵌入，在渲染视图中水印对象占据最小区域时仍能保持可靠提取

Conclusion: 该框架为Splatter Image-based 3DGS模型提供了首个通用化水印解决方案，实现了高效、不可感知且鲁棒的版权保护

Abstract: The growing popularity of 3D Gaussian Splatting (3DGS) has intensified the need for effective copyright protection. Current 3DGS watermarking methods rely on computationally expensive fine-tuning procedures for each predefined message. We propose the first generalizable watermarking framework that enables efficient protection of Splatter Image-based 3DGS models through a single forward pass. We introduce GaussianBridge that transforms unstructured 3D Gaussians into Splatter Image format, enabling direct neural processing for arbitrary message embedding. To ensure imperceptibility, we design a Gaussian-Uncertainty-Perceptual heatmap prediction strategy for preserving visual quality. For robust message recovery, we develop a dense segmentation-based extraction mechanism that maintains reliable extraction even when watermarked objects occupy minimal regions in rendered views. Project page: https://kevinhuangxf.github.io/marksplatter.

</details>


### [23] [Diffusion-Based Image-to-Brain Signal Generation with Cross-Attention Mechanisms for Visual Prostheses](https://arxiv.org/abs/2509.00787)
*Ganxi Xu,Jinyi Long,Jia Zhang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种基于去噪液体模型(DDPM)的图像-大脑框架，通过跨注意力机制提升视觉义胎修复装置中大脑信号的生物学可信性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉义胎修复装置在大脑编码阶段遇到困难，预测刺激的生物学可信性质量不高，缺乏来自真实大脑响应的监督信号。

Method: 构建了两个关键组件：预训练的CLIP视觉编码器提取语义表征，以及跨注意力增强的U-Net液体模型通过迭代去噪学习重建生物可信的大脑信号。跨注意力模块允许视觉特征与大脑信号表征之间的动态交互。

Result: 在THINGS-EEG2和THINGS-MEG两个多模态数据集上评估，证明了该框架在生成生物可信的大脑信号方面的有效性。可视化了训练和测试阶段的M/EEG地形图，展示了主体内和主体间的变化。

Conclusion: 该方法通过跨注意力机制实现了视觉特征与大脑信号的细粒度对齐，有效提升了视觉义胎修复装置中预测大脑信号的生物学可信性。

Abstract: Visual prostheses have shown great potential in restoring vision for blind individuals. On the one hand, researchers have been continuously improving the brain decoding framework of visual prostheses by leveraging the powerful image generation capabilities of diffusion models. On the other hand, the brain encoding stage of visual prostheses struggles to generate brain signals with sufficient biological similarity. Although existing works have recognized this problem, the quality of predicted stimuli still remains a critical issue, as existing approaches typically lack supervised signals from real brain responses to validate the biological plausibility of predicted stimuli. To address this issue, we propose a novel image-to-brain framework based on denoising diffusion probabilistic models (DDPMs) enhanced with cross-attention mechanisms. Our framework consists of two key architectural components: a pre-trained CLIP visual encoder that extracts rich semantic representations from input images, and a cross-attention enhanced U-Net diffusion model that learns to reconstruct biologically plausible brain signals through iterative denoising. Unlike conventional generative models that rely on simple concatenation for conditioning, our cross-attention modules enable dynamic interaction between visual features and brain signal representations, facilitating fine-grained alignment during the generation process. We evaluate our framework on two multimodal datasets (THINGS-EEG2 and THINGS-MEG) to demonstrate its effectiveness in generating biologically plausible brain signals. Moreover, we visualize the training and test M/EEG topographies for all subjects on both datasets to intuitively demonstrate the intra-subject variations and inter-subject variations in M/EEG signals.

</details>


### [24] [SWAGSplatting: Semantic-guided Water-scene Augmented Gaussian Splatting](https://arxiv.org/abs/2509.00800)
*Zhuodong Jiang,Haoran Wang,Guoxi Huang,Brett Seymour,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: 提出了一种基于语义引导的3D高斯泼溅方法，通过多模态知识融合和语义一致性损失，显著提升了水下3D重建的质量和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 水下3D重建面临光线失真、浑浊和能见度低等挑战，现有AI方法尚未充分利用语言模型与视觉处理的整合潜力。

Method: 在3D高斯泼溅框架中为每个高斯基元嵌入语义特征，使用CLIP提取的语义特征进行监督，并采用分阶段训练策略（从粗到细学习加后期参数优化）。

Result: 在SeaThru-NeRF和Submerged3D数据集上全面超越现有方法，PSNR平均提升高达3.09 dB。

Conclusion: 该方法为水下探索和海洋感知应用提供了强有力的解决方案，通过语义引导显著提升了重建质量和稳定性。

Abstract: Accurate 3D reconstruction in underwater environments remains a complex challenge due to issues such as light distortion, turbidity, and limited visibility. AI-based techniques have been applied to address these issues, however, existing methods have yet to fully exploit the potential of AI, particularly in integrating language models with visual processing. In this paper, we propose a novel framework that leverages multimodal cross-knowledge to create semantic-guided 3D Gaussian Splatting for robust and high-fidelity deep-sea scene reconstruction. By embedding an extra semantic feature into each Gaussian primitive and supervised by the CLIP extracted semantic feature, our method enforces semantic and structural awareness throughout the training. The dedicated semantic consistency loss ensures alignment with high-level scene understanding. Besides, we propose a novel stage-wise training strategy, combining coarse-to-fine learning with late-stage parameter refinement, to further enhance both stability and reconstruction quality. Extensive results show that our approach consistently outperforms state-of-the-art methods on SeaThru-NeRF and Submerged3D datasets across three metrics, with an improvement of up to 3.09 dB on average in terms of PSNR, making it a strong candidate for applications in underwater exploration and marine perception.

</details>


### [25] [UPGS: Unified Pose-aware Gaussian Splatting for Dynamic Scene Deblurring](https://arxiv.org/abs/2509.00831)
*Zhijing Wu,Longguang Wang*

Main category: cs.CV

TL;DR: 提出统一优化框架，将相机位姿作为可学习参数与3D高斯属性联合优化，解决动态场景重建中运动模糊导致的位姿估计误差问题


<details>
  <summary>Details</summary>
Motivation: 单目视频动态3D重建中，相机和物体运动导致的严重运动模糊会破坏位姿估计，现有两步流水线方法会累积位姿误差，导致重建质量下降

Method: 将相机和物体运动建模为3D高斯的SE(3)仿射变换，提出统一优化目标，采用三阶段训练策略：先固定位姿优化高斯，再固定高斯优化位姿，最后联合优化所有参数

Result: 在Stereo Blur数据集和真实场景序列上的实验表明，该方法在重建质量和位姿估计精度上显著优于现有动态去模糊方法

Conclusion: 通过端到端联合优化相机位姿和3D高斯属性，有效解决了运动模糊导致的位姿估计和重建质量问题，为动态场景重建提供了新思路

Abstract: Reconstructing dynamic 3D scenes from monocular video has broad applications in AR/VR, robotics, and autonomous navigation, but often fails due to severe motion blur caused by camera and object motion. Existing methods commonly follow a two-step pipeline, where camera poses are first estimated and then 3D Gaussians are optimized. Since blurring artifacts usually undermine pose estimation, pose errors could be accumulated to produce inferior reconstruction results. To address this issue, we introduce a unified optimization framework by incorporating camera poses as learnable parameters complementary to 3DGS attributes for end-to-end optimization. Specifically, we recast camera and object motion as per-primitive SE(3) affine transformations on 3D Gaussians and formulate a unified optimization objective. For stable optimization, we introduce a three-stage training schedule that optimizes camera poses and Gaussians alternatively. Particularly, 3D Gaussians are first trained with poses being fixed, and then poses are optimized with 3D Gaussians being untouched. Finally, all learnable parameters are optimized together. Extensive experiments on the Stereo Blur dataset and challenging real-world sequences demonstrate that our method achieves significant gains in reconstruction quality and pose estimation accuracy over prior dynamic deblurring methods.

</details>


### [26] [SegDINO: An Efficient Design for Medical and Natural Image Segmentation with DINO-V3](https://arxiv.org/abs/2509.00833)
*Sicheng Yang,Hongqiu Wang,Zhaohu Xing,Sixiang Chen,Lei Zhu*

Main category: cs.CV

TL;DR: SegDINO是一个高效的分割框架，将冻结的DINOv3主干网络与轻量级解码器结合，在多个分割基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的DINO模型自适应方法通常依赖重型解码器，带来大量参数开销和计算成本，需要更高效的解决方案。

Method: 使用冻结的DINOv3主干提取多级特征，对齐到统一分辨率和通道宽度，然后通过轻量级MLP头直接预测分割掩码。

Result: 在6个基准测试（包括3个医学数据集和3个自然图像数据集）上均达到最先进性能，同时显著减少可训练参数。

Conclusion: SegDINO证明了在保持基础特征表示能力的同时，通过轻量级设计可以实现高效且高性能的分割任务。

Abstract: The DINO family of self-supervised vision models has shown remarkable transferability, yet effectively adapting their representations for segmentation remains challenging. Existing approaches often rely on heavy decoders with multi-scale fusion or complex upsampling, which introduce substantial parameter overhead and computational cost. In this work, we propose SegDINO, an efficient segmentation framework that couples a frozen DINOv3 backbone with a lightweight decoder. SegDINO extracts multi-level features from the pretrained encoder, aligns them to a common resolution and channel width, and utilizes a lightweight MLP head to directly predict segmentation masks. This design minimizes trainable parameters while preserving the representational power of foundation features. Extensive experiments across six benchmarks, including three medical datasets (TN3K, Kvasir-SEG, ISIC) and three natural image datasets (MSD, VMD-D, ViSha), demonstrate that SegDINO consistently achieves state-of-the-art performance compared to existing methods. Code is available at https://github.com/script-Yang/SegDINO.

</details>


### [27] [Satellite Image Utilization for Dehazing with Swin Transformer-Hybrid U-Net and Watershed loss](https://arxiv.org/abs/2509.00835)
*Jongwook Si,Sungyoung Kim*

Main category: cs.CV

TL;DR: 提出了一种结合Swin Transformer和U-Net的混合去雾框架SUFERNOBWA，用于卫星图像去雾，在RICE和SateHaze1K数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 卫星图像在大气干扰和雾霾影响下清晰度下降，信息提取准确性降低，需要有效的去雾方法来提升图像质量。

Method: 使用Swin Transformer和U-Net结合的混合架构，采用SwinRRDB模块在编码器和解码器中提取特征，结合全局上下文学习和局部细节恢复，并设计了包含L2损失、引导损失和新型分水岭损失的复合损失函数。

Result: 在RICE数据集上达到PSNR 33.24 dB和SSIM 0.967，显著优于现有方法，在SateHaze1K数据集上也表现出色。

Conclusion: 该方法能有效减轻卫星图像中的大气干扰，在不同大气条件下保持结构一致性，在遥感应用中具有广泛的应用潜力。

Abstract: Satellite imagery plays a crucial role in various fields; however, atmospheric interference and haze significantly degrade image clarity and reduce the accuracy of information extraction. To address these challenges, this paper proposes a hybrid dehazing framework that integrates Swin Transformer and U-Net to balance global context learning and local detail restoration, called SUFERNOBWA. The proposed network employs SwinRRDB, a Swin Transformer-based Residual-in-Residual Dense Block, in both the encoder and decoder to effectively extract features. This module enables the joint learning of global contextual information and fine spatial structures, which is crucial for structural preservation in satellite image. Furthermore, we introduce a composite loss function that combines L2 loss, guided loss, and a novel watershed loss, which enhances structural boundary preservation and ensures pixel-level accuracy. This architecture enables robust dehazing under diverse atmospheric conditions while maintaining structural consistency across restored images. Experimental results demonstrate that the proposed method outperforms state-of-the-art models on both the RICE and SateHaze1K datasets. Specifically, on the RICE dataset, the proposed approach achieved a PSNR of 33.24 dB and an SSIM of 0.967, which is a significant improvement over existing method. This study provides an effective solution for mitigating atmospheric interference in satellite imagery and highlights its potential applicability across diverse remote sensing applications.

</details>


### [28] [Look Beyond: Two-Stage Scene View Generation via Panorama and Video Diffusion](https://arxiv.org/abs/2509.00843)
*Xueyang Kang,Zhengkang Xiang,Zezheng Zhang,Kourosh Khoshelham*

Main category: cs.CV

TL;DR: 提出了一种将单视图新视角合成分解为360度场景外推和视角插值的两阶段方法，通过全景扩散模型和视频扩散模型确保长期视角一致性


<details>
  <summary>Details</summary>
Motivation: 现有单图像新视角合成方法在处理大幅偏离输入视角或循环轨迹时，难以保持场景一致性和正确的视角对齐

Method: 第一阶段使用全景扩散模型从输入视角图像学习场景先验，第二阶段从生成的全景图中采样并扭曲关键帧，通过预训练视频扩散模型进行空间噪声扩散生成新视角

Result: 在多样化场景数据集上的实验表明，该方法在用户定义轨迹上生成连贯视图方面优于现有方法，即使在循环闭合场景中也能保持全局一致性

Conclusion: 该方法通过分解策略和关键帧条件化，成功解决了单视图新视角合成的长期一致性问题，同时支持灵活的相机控制

Abstract: Novel view synthesis (NVS) from a single image is highly ill-posed due to large unobserved regions, especially for views that deviate significantly from the input. While existing methods focus on consistency between the source and generated views, they often fail to maintain coherence and correct view alignment across long-range or looped trajectories. We propose a model that addresses this by decomposing single-view NVS into a 360-degree scene extrapolation followed by novel view interpolation. This design ensures long-term view and scene consistency by conditioning on keyframes extracted and warped from a generated panoramic representation. In the first stage, a panorama diffusion model learns the scene prior from the input perspective image. Perspective keyframes are then sampled and warped from the panorama and used as anchor frames in a pre-trained video diffusion model, which generates novel views through a proposed spatial noise diffusion process. Compared to prior work, our method produces globally consistent novel views -- even in loop closure scenarios -- while enabling flexible camera control. Experiments on diverse scene datasets demonstrate that our approach outperforms existing methods in generating coherent views along user-defined trajectories. Our implementation is available at https://github.com/YiGuYT/LookBeyond.

</details>


### [29] [DarkVRAI: Capture-Condition Conditioning and Burst-Order Selective Scan for Low-light RAW Video Denoising](https://arxiv.org/abs/2509.00917)
*Youngjin Oh,Junhyeong Kwon,Junyoung Park,Nam Ik Cho*

Main category: cs.CV

TL;DR: DarkVRAI是一个针对低光照RAW视频去噪的新框架，通过条件化方案和BOSS机制，在AIM 2025挑战赛中取得第一名，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决低光照RAW视频由于高传感器增益和短曝光时间导致的严重信号退化问题，这些限制源于视频帧率要求。

Method: 提出两个主要贡献：1) 将图像去噪的条件化方案成功应用于视频去噪，利用捕获元数据指导对齐和去噪过程；2) 提出Burst-Order Selective Scan (BOSS)机制，有效建模噪声视频序列中的长程时间依赖性。

Result: 在严格且真实的基准数据集上展示了最先进的性能，为低光照视频去噪设立了新标准。

Conclusion: 通过协同结合条件化方案和BOSS机制，DarkVRAI框架在低光照RAW视频去噪方面取得了突破性进展，赢得了AIM 2025挑战赛第一名。

Abstract: Low-light RAW video denoising is a fundamentally challenging task due to severe signal degradation caused by high sensor gain and short exposure times, which are inherently limited by video frame rate requirements. To address this, we propose DarkVRAI, a novel framework that achieved first place in the AIM 2025 Low-light RAW Video Denoising Challenge. Our method introduces two primary contributions: (1) a successful application of a conditioning scheme for image denoising, which explicitly leverages capture metadata, to video denoising to guide the alignment and denoising processes, and (2) a Burst-Order Selective Scan (BOSS) mechanism that effectively models long-range temporal dependencies within the noisy video sequence. By synergistically combining these components, DarkVRAI demonstrates state-of-the-art performance on a rigorous and realistic benchmark dataset, setting a new standard for low-light video denoising.

</details>


### [30] [Towards Integrating Multi-Spectral Imaging with Gaussian Splatting](https://arxiv.org/abs/2509.00989)
*Josef Grün,Lukas Meyer,Maximilian Weiherer,Bernhard Egger,Marc Stamminger,Linus Franke*

Main category: cs.CV

TL;DR: 该研究探讨了如何在3D高斯泼溅(3DGS)框架中有效整合RGB和多光谱图像，提出了三种优化策略并证明了联合优化方法的优越性。


<details>
  <summary>Details</summary>
Motivation: 虽然3DGS在RGB数据上表现出色，但直接将多光谱波段进行独立优化会导致重建质量不佳，因为不同光谱域中的几何结构会出现不一致的问题。

Method: 评估了三种策略：1) 独立波段重建；2) 分割优化（先优化RGB几何再复制到其他波段）；3) 联合优化（可选的RGB初始阶段）。提出了将多光谱数据直接整合到球谐颜色组件中的方法。

Result: 通过定量指标和定性新颖视角渲染验证了联合优化策略的有效性，不仅提高了整体光谱重建质量，还通过光谱串扰增强了RGB结果。

Conclusion: 研究揭示了在优化过程中引入光谱波段的关键权衡，为鲁棒的多模态3DGS重建提供了实用见解，建议将多光谱数据直接整合到球谐颜色组件中以紧凑建模每个高斯的反射特性。

Abstract: We present a study of how to integrate color (RGB) and multi-spectral imagery (red, green, red-edge, and near-infrared) into the 3D Gaussian Splatting (3DGS) framework, a state-of-the-art explicit radiance-field-based method for fast and high-fidelity 3D reconstruction from multi-view images. While 3DGS excels on RGB data, naive per-band optimization of additional spectra yields poor reconstructions due to inconsistently appearing geometry in the spectral domain. This problem is prominent, even though the actual geometry is the same, regardless of spectral modality. To investigate this, we evaluate three strategies: 1) Separate per-band reconstruction with no shared structure. 2) Splitting optimization, in which we first optimize RGB geometry, copy it, and then fit each new band to the model by optimizing both geometry and band representation. 3) Joint, in which the modalities are jointly optimized, optionally with an initial RGB-only phase. We showcase through quantitative metrics and qualitative novel-view renderings on multi-spectral datasets the effectiveness of our dedicated optimized Joint strategy, increasing overall spectral reconstruction as well as enhancing RGB results through spectral cross-talk. We therefore suggest integrating multi-spectral data directly into the spherical harmonics color components to compactly model each Gaussian's multi-spectral reflectance. Moreover, our analysis reveals several key trade-offs in when and how to introduce spectral bands during optimization, offering practical insights for robust multi-modal 3DGS reconstruction.

</details>


### [31] [CompSlider: Compositional Slider for Disentangled Multiple-Attribute Image Generation](https://arxiv.org/abs/2509.01028)
*Zixin Zhu,Kevin Duarte,Mamshad Nayeem Rizve,Chengyuan Xu,Ratheesh Kalarot,Junsong Yuan*

Main category: cs.CV

TL;DR: CompSlider是一种新的基于滑块的文本到图像生成方法，能够同时精确控制多个图像属性，通过解耦多个属性和保持结构一致性来解决现有方法中的属性干扰问题。


<details>
  <summary>Details</summary>
Motivation: 在文本到图像生成中，即使使用详细的文本提示，也难以实现对属性（如年龄或微笑）的细粒度控制。现有基于滑块的方法通常为每个属性单独训练适配器，忽略了多个属性之间的纠缠，导致不同属性之间产生干扰，无法同时精确控制多个属性。

Method: 提出CompSlider方法，为T2I基础模型生成条件先验以同时控制多个属性。引入新颖的解耦损失和结构损失来组合多个属性变化，同时保持图像内的结构一致性。该方法在条件先验的潜在空间中操作，无需重新训练基础模型。

Result: CompSlider减少了训练和推理的计算负担，能够可靠且独立地操纵多个属性。该方法在多种图像属性上进行了评估，并通过扩展到视频生成展示了其通用性。

Conclusion: CompSlider通过解耦多个属性和引入结构一致性约束，成功解决了基于滑块的文本到图像生成中多属性控制的干扰问题，实现了更可靠和独立的属性操作，同时具有计算效率和通用性优势。

Abstract: In text-to-image (T2I) generation, achieving fine-grained control over attributes - such as age or smile - remains challenging, even with detailed text prompts. Slider-based methods offer a solution for precise control of image attributes. Existing approaches typically train individual adapter for each attribute separately, overlooking the entanglement among multiple attributes. As a result, interference occurs among different attributes, preventing precise control of multiple attributes together. To address this challenge, we aim to disentangle multiple attributes in slider-based generation to enbale more reliable and independent attribute manipulation. Our approach, CompSlider, can generate a conditional prior for the T2I foundation model to control multiple attributes simultaneously. Furthermore, we introduce novel disentanglement and structure losses to compose multiple attribute changes while maintaining structural consistency within the image. Since CompSlider operates in the latent space of the conditional prior and does not require retraining the foundation model, it reduces the computational burden for both training and inference. We evaluate our approach on a variety of image attributes and highlight its generality by extending to video generation.

</details>


### [32] [A Unified Low-level Foundation Model for Enhancing Pathology Image Quality](https://arxiv.org/abs/2509.01071)
*Ziyi Liu,Zhe Xu,Jiabo Ma,Wenqaing Li,Junlin Hou,Fuxiang Huang,Xi Wang,Ronald Cheong Kin Chan,Terence Tsz Wai Wong,Hao Chen*

Main category: cs.CV

TL;DR: 提出了首个统一的低级别病理学基础模型LPFM，通过单一架构处理图像恢复（超分辨率、去模糊、去噪）和虚拟染色任务，在190万无标签病理图像上预训练，在大多数任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实病理图像存在噪声、模糊、低分辨率等退化问题，现有方法针对单一任务设计，缺乏处理多样化低级别视觉挑战的通用性，且物理染色成本高、耗时长、不一致。

Method: 使用对比预训练编码器学习可迁移的染色不变特征表示，通过统一的条件扩散过程根据文本提示动态适应特定任务，在87,810张全切片图像上训练。

Result: 在大多数任务（56/66）中显著优于最先进方法（p<0.01），图像恢复任务PSNR提升10-15%，虚拟染色任务SSIM提升12-18%。

Conclusion: LPFM作为首个统一的低级别病理学基础模型，能够有效处理多种图像增强任务，为病理学图像处理提供了通用且高效的解决方案。

Abstract: Foundation models have revolutionized computational pathology by achieving remarkable success in high-level diagnostic tasks, yet the critical challenge of low-level image enhancement remains largely unaddressed. Real-world pathology images frequently suffer from degradations such as noise, blur, and low resolution due to slide preparation artifacts, staining variability, and imaging constraints, while the reliance on physical staining introduces significant costs, delays, and inconsistency. Although existing methods target individual problems like denoising or super-resolution, their task-specific designs lack the versatility to handle the diverse low-level vision challenges encountered in practice. To bridge this gap, we propose the first unified Low-level Pathology Foundation Model (LPFM), capable of enhancing image quality in restoration tasks, including super-resolution, deblurring, and denoising, as well as facilitating image translation tasks like virtual staining (H&E and special stains), all through a single adaptable architecture. Our approach introduces a contrastive pre-trained encoder that learns transferable, stain-invariant feature representations from 190 million unlabeled pathology images, enabling robust identification of degradation patterns. A unified conditional diffusion process dynamically adapts to specific tasks via textual prompts, ensuring precise control over output quality. Trained on a curated dataset of 87,810 whole slied images (WSIs) across 34 tissue types and 5 staining protocols, LPFM demonstrates statistically significant improvements (p<0.01) over state-of-the-art methods in most tasks (56/66), achieving Peak Signal-to-Noise Ratio (PSNR) gains of 10-15% for image restoration and Structural Similarity Index Measure (SSIM) improvements of 12-18% for virtual staining.

</details>


### [33] [FICGen: Frequency-Inspired Contextual Disentanglement for Layout-driven Degraded Image Generation](https://arxiv.org/abs/2509.01107)
*Wenzhuang Wang,Yifan Zhao,Mingcan Ma,Ming Liu,Zhonglin Jiang,Yong Chen,Jia Li*

Main category: cs.CV

TL;DR: 本文提出FICGen方法，通过频率启发的上下文解耦生成范式，解决退化场景下布局到图像生成中的上下文幻觉困境，提升生成保真度和布局对齐性。


<details>
  <summary>Details</summary>
Motivation: 在退化场景（如低光照、水下）中，布局到图像生成存在生成保真度有限和与用户提供布局对齐性弱的问题，主要原因是前景实例被上下文主导的频率分布所淹没的"上下文幻觉困境"。

Method: 提出频率启发的上下文解耦生成范式（FICGen），包含两个主要步骤：1）可学习的双查询机制配合专用频率重采样器，从训练集退化样本中提取上下文频率原型；2）视觉频率增强注意力机制注入频率原型到退化生成过程，使用实例一致性图和自适应空间频率聚合模块来调节解耦和重建表示。

Result: 在5个基准测试上的广泛实验表明，FICGen在各种退化场景（从严重低光照到轻微模糊）中，在生成保真度、对齐性和下游辅助可训练性方面 consistently 超越现有L2I方法。

Conclusion: FICGen通过将退化图像的频率知识转移到潜在扩散空间，有效解决了退化场景下的上下文幻觉问题，显著提升了布局到图像生成的性能。

Abstract: Layout-to-image (L2I) generation has exhibited promising results in natural domains, but suffers from limited generative fidelity and weak alignment with user-provided layouts when applied to degraded scenes (i.e., low-light, underwater). We primarily attribute these limitations to the "contextual illusion dilemma" in degraded conditions, where foreground instances are overwhelmed by context-dominant frequency distributions. Motivated by this, our paper proposes a new Frequency-Inspired Contextual Disentanglement Generative (FICGen) paradigm, which seeks to transfer frequency knowledge of degraded images into the latent diffusion space, thereby facilitating the rendering of degraded instances and their surroundings via contextual frequency-aware guidance. To be specific, FICGen consists of two major steps. Firstly, we introduce a learnable dual-query mechanism, each paired with a dedicated frequency resampler, to extract contextual frequency prototypes from pre-collected degraded exemplars in the training set. Secondly, a visual-frequency enhanced attention is employed to inject frequency prototypes into the degraded generation process. To alleviate the contextual illusion and attribute leakage, an instance coherence map is developed to regulate latent-space disentanglement between individual instances and their surroundings, coupled with an adaptive spatial-frequency aggregation module to reconstruct spatial-frequency mixed degraded representations. Extensive experiments on 5 benchmarks involving a variety of degraded scenarios-from severe low-light to mild blur-demonstrate that FICGen consistently surpasses existing L2I methods in terms of generative fidelity, alignment and downstream auxiliary trainability.

</details>


### [34] [DynaMind: Reconstructing Dynamic Visual Scenes from EEG by Aligning Temporal Dynamics and Multimodal Semantics to Guided Diffusion](https://arxiv.org/abs/2509.01177)
*Junxiang Liu,Junming Lin,Jiangtong Li,Jie Li*

Main category: cs.CV

TL;DR: DynaMind是一个从EEG信号重建动态视觉场景的新框架，通过联合建模神经动力学和语义特征，在视频重建精度和视觉质量方面实现了显著提升


<details>
  <summary>Details</summary>
Motivation: 解决EEG信号低空间分辨率、神经记录与视频动态时间不匹配、以及脑活动中语义信息利用不足等问题，现有方法难以同时处理动态连贯性和复杂语义上下文

Method: 采用三个核心模块：区域感知语义映射器(RSM)提取多模态语义特征，时间感知动态对齐器(TDA)生成动态潜在序列确保时间一致性，双引导视频重建器(DGVR)将时序蓝图转换为高保真视频

Result: 在SEED-DV数据集上达到新的SOTA，视频和帧级准确率分别提升12.5和10.3个百分点，SSIM提升9.4%，FVMD降低19.7%，显示出卓越的视觉保真度和时间连贯性

Conclusion: 该框架在神经动力学和高保真视觉语义之间架起了关键桥梁，标志着脑解码领域的重大进展

Abstract: Reconstruction dynamic visual scenes from electroencephalography (EEG) signals remains a primary challenge in brain decoding, limited by the low spatial resolution of EEG, a temporal mismatch between neural recordings and video dynamics, and the insufficient use of semantic information within brain activity. Therefore, existing methods often inadequately resolve both the dynamic coherence and the complex semantic context of the perceived visual stimuli. To overcome these limitations, we introduce DynaMind, a novel framework that reconstructs video by jointly modeling neural dynamics and semantic features via three core modules: a Regional-aware Semantic Mapper (RSM), a Temporal-aware Dynamic Aligner (TDA), and a Dual-Guidance Video Reconstructor (DGVR). The RSM first utilizes a regional-aware encoder to extract multimodal semantic features from EEG signals across distinct brain regions, aggregating them into a unified diffusion prior. In the mean time, the TDA generates a dynamic latent sequence, or blueprint, to enforce temporal consistency between the feature representations and the original neural recordings. Together, guided by the semantic diffusion prior, the DGVR translates the temporal-aware blueprint into a high-fidelity video reconstruction. On the SEED-DV dataset, DynaMind sets a new state-of-the-art (SOTA), boosting reconstructed video accuracies (video- and frame-based) by 12.5 and 10.3 percentage points, respectively. It also achieves a leap in pixel-level quality, showing exceptional visual fidelity and temporal coherence with a 9.4% SSIM improvement and a 19.7% FVMD reduction. This marks a critical advancement, bridging the gap between neural dynamics and high-fidelity visual semantics.

</details>


### [35] [FocusDPO: Dynamic Preference Optimization for Multi-Subject Personalized Image Generation via Adaptive Focus](https://arxiv.org/abs/2509.01181)
*Qiaoqiao Jin,Siming Fu,Dong She,Weinan Jia,Hualiang Wang,Mu Liu,Jidong Jiang*

Main category: cs.CV

TL;DR: FocusDPO是一个多主体个性化图像生成框架，通过动态语义对应和复杂度感知的焦点区域识别，在DPO训练过程中实现自适应焦点分配，有效防止属性泄漏并保持主体保真度。


<details>
  <summary>Details</summary>
Motivation: 多主体个性化图像生成需要在不进行测试时优化的前提下合成包含多个指定主体的定制图像，但现有方法在保持主体保真度和防止跨主体属性泄漏方面存在困难。

Method: 基于动态语义对应和监督图像复杂度自适应识别焦点区域，在训练过程中跨噪声时间步渐进调整焦点区域，采用加权策略奖励信息丰富区域并惩罚低置信度区域，根据参考图像语义复杂度动态调整DPO过程中的焦点分配。

Result: 在现有预训练个性化生成模型上显著提升性能，在单主体和多主体个性化图像合成基准测试中达到最先进水平，有效缓解属性泄漏问题并在多样化生成场景中保持优异的主体保真度。

Conclusion: 该方法推进了可控多主体图像合成的前沿，通过动态焦点分配机制实现了对多个主体的细粒度独立控制。

Abstract: Multi-subject personalized image generation aims to synthesize customized images containing multiple specified subjects without requiring test-time optimization. However, achieving fine-grained independent control over multiple subjects remains challenging due to difficulties in preserving subject fidelity and preventing cross-subject attribute leakage. We present FocusDPO, a framework that adaptively identifies focus regions based on dynamic semantic correspondence and supervision image complexity. During training, our method progressively adjusts these focal areas across noise timesteps, implementing a weighted strategy that rewards information-rich patches while penalizing regions with low prediction confidence. The framework dynamically adjusts focus allocation during the DPO process according to the semantic complexity of reference images and establishes robust correspondence mappings between generated and reference subjects. Extensive experiments demonstrate that our method substantially enhances the performance of existing pre-trained personalized generation models, achieving state-of-the-art results on both single-subject and multi-subject personalized image synthesis benchmarks. Our method effectively mitigates attribute leakage while preserving superior subject fidelity across diverse generation scenarios, advancing the frontier of controllable multi-subject image synthesis.

</details>


### [36] [PRINTER:Deformation-Aware Adversarial Learning for Virtual IHC Staining with In Situ Fidelity](https://arxiv.org/abs/2509.01214)
*Yizhe Yuan,Bingsen Xue,Bangzheng Pu,Chengxiang Wang,Cheng Jin*

Main category: cs.CV

TL;DR: PRINTER是一个弱监督框架，通过原型驱动的内容-染色模式解耦和变形感知对抗学习，实现H&E到IHC的精确虚拟染色，解决组织切片空间错位问题


<details>
  <summary>Details</summary>
Motivation: 肿瘤空间异质性分析需要精确关联H&E形态学和IHC生物标志物表达，但现有方法在连续切片中存在空间错位问题，严重影响病理学原位解释

Method: 提出三个关键创新：1）原型驱动的染色模式转移与显式内容-风格解耦；2）循环配准-合成框架GapBridge，通过可变形结构对齐桥接H&E和IHC域；3）变形感知对抗学习，生成器和变形感知配准网络联合对抗优化风格聚焦判别器

Result: 大量实验证明PRINTER在保留H&E染色细节和虚拟染色保真度方面表现优异，优于最先进方法

Conclusion: 该工作为虚拟染色提供了稳健且可扩展的解决方案，推动了计算病理学领域的发展

Abstract: Tumor spatial heterogeneity analysis requires precise correlation between Hematoxylin and Eosin H&E morphology and immunohistochemical (IHC) biomarker expression, yet current methods suffer from spatial misalignment in consecutive sections, severely compromising in situ pathological interpretation. In order to obtain a more accurate virtual staining pattern, We propose PRINTER, a weakly-supervised framework that integrates PRototype-drIven content and staiNing patTERn decoupling and deformation-aware adversarial learning strategies designed to accurately learn IHC staining patterns while preserving H&E staining details. Our approach introduces three key innovations: (1) A prototype-driven staining pattern transfer with explicit content-style decoupling; and (2) A cyclic registration-synthesis framework GapBridge that bridges H&E and IHC domains through deformable structural alignment, where registered features guide cross-modal style transfer while synthesized outputs iteratively refine the registration;(3) Deformation-Aware Adversarial Learning: We propose a training framework where a generator and deformation-aware registration network jointly adversarially optimize a style-focused discriminator. Extensive experiments demonstrate that PRINTER effectively achieves superior performance in preserving H&E staining details and virtual staining fidelity, outperforming state-of-the-art methods. Our work provides a robust and scalable solution for virtual staining, advancing the field of computational pathology.

</details>


### [37] [FantasyHSI: Video-Generation-Centric 4D Human Synthesis In Any Scene through A Graph-based Multi-Agent Framework](https://arxiv.org/abs/2509.01232)
*Lingzhou Mu,Qiang Wang,Fan Jiang,Mengchao Wang,Yaqi Fan,Mu Xu,Kai Zhang*

Main category: cs.CV

TL;DR: FantasyHSI是一个基于视频生成和多智能体系统的人类-场景交互框架，通过动态有向图建模交互过程，包含场景导航器、规划器和批评器三个智能体，实现了无需配对数据的长时程、高层次任务生成，并在物理真实性和泛化能力方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决人类-场景交互中长时程、高层次任务处理的挑战，以及模型在未见场景中的泛化能力不足问题，同时避免对配对数据的依赖。

Method: 1) 将复杂交互过程建模为动态有向图；2) 构建多智能体系统：场景导航器（环境感知和路径规划）、规划器（目标分解为原子动作）、批评器（闭环反馈机制）；3) 使用直接偏好优化训练动作生成器提升物理真实性。

Result: 在自定义SceneBench基准测试中，FantasyHSI在泛化能力、长时程任务完成度和物理真实性方面显著优于现有方法，有效减少了肢体扭曲和脚部滑动等伪影。

Conclusion: FantasyHSI通过多智能体协作和闭环反馈机制，成功解决了HSI中的长时程一致性和物理真实性问题，为无需配对数据的人类行为生成提供了有效解决方案。

Abstract: Human-Scene Interaction (HSI) seeks to generate realistic human behaviors within complex environments, yet it faces significant challenges in handling long-horizon, high-level tasks and generalizing to unseen scenes. To address these limitations, we introduce FantasyHSI, a novel HSI framework centered on video generation and multi-agent systems that operates without paired data. We model the complex interaction process as a dynamic directed graph, upon which we build a collaborative multi-agent system. This system comprises a scene navigator agent for environmental perception and high-level path planning, and a planning agent that decomposes long-horizon goals into atomic actions. Critically, we introduce a critic agent that establishes a closed-loop feedback mechanism by evaluating the deviation between generated actions and the planned path. This allows for the dynamic correction of trajectory drifts caused by the stochasticity of the generative model, thereby ensuring long-term logical consistency. To enhance the physical realism of the generated motions, we leverage Direct Preference Optimization (DPO) to train the action generator, significantly reducing artifacts such as limb distortion and foot-sliding. Extensive experiments on our custom SceneBench benchmark demonstrate that FantasyHSI significantly outperforms existing methods in terms of generalization, long-horizon task completion, and physical realism. Ours project page: https://fantasy-amap.github.io/fantasy-hsi/

</details>


### [38] [Guided Model-based LiDAR Super-Resolution for Resource-Efficient Automotive scene Segmentation](https://arxiv.org/abs/2509.01317)
*Alexandros Gkillas,Nikos Piperigkos,Aris S. Lalos*

Main category: cs.CV

TL;DR: 首个端到端框架，联合处理LiDAR超分辨率和语义分割，通过联合优化和新的损失函数，在减少参数的同时达到与高分辨率64通道LiDAR相当的语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 高分辨率LiDAR成本高昂限制了大规模部署，而低成本16通道LiDAR产生的稀疏点云会降低分割精度，需要解决这一矛盾。

Method: 采用端到端框架进行联合优化训练，SR模块融入语义线索并保留细节；设计新的SR损失函数关注感兴趣区域；使用轻量级模型架构减少参数。

Result: 实验表明该方法在语义分割性能上达到了与昂贵64通道LiDAR数据相当的水平。

Conclusion: 提出的联合优化框架有效解决了低成本LiDAR传感器性能受限的问题，为自动驾驶大规模部署提供了可行方案。

Abstract: High-resolution LiDAR data plays a critical role in 3D semantic segmentation for autonomous driving, but the high cost of advanced sensors limits large-scale deployment. In contrast, low-cost sensors such as 16-channel LiDAR produce sparse point clouds that degrade segmentation accuracy. To overcome this, we introduce the first end-to-end framework that jointly addresses LiDAR super-resolution (SR) and semantic segmentation. The framework employs joint optimization during training, allowing the SR module to incorporate semantic cues and preserve fine details, particularly for smaller object classes. A new SR loss function further directs the network to focus on regions of interest. The proposed lightweight, model-based SR architecture uses significantly fewer parameters than existing LiDAR SR approaches, while remaining easily compatible with segmentation networks. Experiments show that our method achieves segmentation performance comparable to models operating on high-resolution and costly 64-channel LiDAR data.

</details>


### [39] [Identity-Preserving Text-to-Video Generation via Training-Free Prompt, Image, and Guidance Enhancement](https://arxiv.org/abs/2509.01362)
*Jiayi Gao,Changcheng Hua,Qingchao Chen,Yuxin Peng,Yang Liu*

Main category: cs.CV

TL;DR: 提出了一种无需训练的提示、图像和引导增强框架(TPIGE)，通过相互增强文本提示和参考图像，结合时空引导优化，实现了高质量的身份保持文本到视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有的身份保持文本到视频生成方法依赖对大型预训练视频扩散模型的微调，但面临数据稀缺和高调优成本的问题，需要一种更高效的解决方案。

Method: 1) 使用GPT-4o进行面部感知提示增强，从参考图像提取面部细节丰富文本提示；2) 使用身份保持图像生成器进行提示感知参考图像增强，修正与文本提示的冲突；3) 提出身份感知时空引导增强，在生成过程中联合优化身份保持和视频质量。

Result: 在1000个视频测试集上通过自动和人工评估验证，方法优于先前工作，在ACM Multimedia 2025身份保持视频生成挑战赛中获得第一名，展示了最先进的性能和强泛化能力。

Conclusion: TPIGE框架以最小成本实现了显著性能提升，解决了身份保持文本到视频生成中的数据稀缺和高成本问题，为相关研究提供了有效的训练免费解决方案。

Abstract: Identity-preserving text-to-video (IPT2V) generation creates videos faithful to both a reference subject image and a text prompt. While fine-tuning large pretrained video diffusion models on ID-matched data achieves state-of-the-art results on IPT2V, data scarcity and high tuning costs hinder broader improvement. We thus introduce a Training-Free Prompt, Image, and Guidance Enhancement (TPIGE) framework that bridges the semantic gap between the video description and the reference image and design sampling guidance that enhances identity preservation and video quality, achieving performance gains at minimal cost.Specifically, we first propose Face Aware Prompt Enhancement, using GPT-4o to enhance the text prompt with facial details derived from the reference image. We then propose Prompt Aware Reference Image Enhancement, leveraging an identity-preserving image generator to refine the reference image, rectifying conflicts with the text prompt. The above mutual refinement significantly improves input quality before video generation. Finally, we propose ID-Aware Spatiotemporal Guidance Enhancement, utilizing unified gradients to optimize identity preservation and video quality jointly during generation.Our method outperforms prior work and is validated by automatic and human evaluations on a 1000 video test set, winning first place in the ACM Multimedia 2025 Identity-Preserving Video Generation Challenge, demonstrating state-of-the-art performance and strong generality. The code is available at https://github.com/Andyplus1/IPT2V.git.

</details>


### [40] [Unsupervised Ultra-High-Resolution UAV Low-Light Image Enhancement: A Benchmark, Metric and Framework](https://arxiv.org/abs/2509.01373)
*Wei Lu,Lingyu Zhu,Si-Bao Chen*

Main category: cs.CV

TL;DR: 提出了U3D无人机低光增强数据集、EEI评估指标和U3LIE高效框架，解决无人机低光图像增强的独特挑战，实现实时4K处理。


<details>
  <summary>Details</summary>
Motivation: 无人机在低光条件下性能下降，现有低光增强方法难以处理无人机图像的超高分辨率、无配对数据、非均匀光照和部署限制等独特挑战。

Method: 开发了U3D无监督超高分辨率数据集；提出EEI指标平衡感知质量和部署效率；设计了U3LIE框架，包含自适应预增强增强(APA)和亮度区间损失(L_int)两个训练策略。

Result: U3LIE达到最先进性能，在单GPU上以23.8 FPS处理4K图像，适合实时机载部署。

Conclusion: 该研究提供了数据集、评估指标和方法三位一体的完整解决方案，推动无人机24/7全天候视觉能力发展。

Abstract: Low light conditions significantly degrade Unmanned Aerial Vehicles (UAVs) performance in critical applications. Existing Low-light Image Enhancement (LIE) methods struggle with the unique challenges of aerial imagery, including Ultra-High Resolution (UHR), lack of paired data, severe non-uniform illumination, and deployment constraints. To address these issues, we propose three key contributions. First, we present U3D, the first unsupervised UHR UAV dataset for LIE, with a unified evaluation toolkit. Second, we introduce the Edge Efficiency Index (EEI), a novel metric balancing perceptual quality with key deployment factors: speed, resolution, model complexity, and memory footprint. Third, we develop U3LIE, an efficient framework with two training-only designs-Adaptive Pre-enhancement Augmentation (APA) for input normalization and a Luminance Interval Loss (L_int) for exposure control. U3LIE achieves SOTA results, processing 4K images at 23.8 FPS on a single GPU, making it ideal for real-time on-board deployment. In summary, these contributions provide a holistic solution (dataset, metric, and method) for advancing robust 24/7 UAV vision. The code and datasets are available at https://github.com/lwCVer/U3D_Toolkit.

</details>


### [41] [Neural Scene Designer: Self-Styled Semantic Image Manipulation](https://arxiv.org/abs/2509.01405)
*Jianman Lin,Tianshui Chen,Chunmei Qing,Zhijing Yang,Shuangping Huang,Yuheng Ren,Liang Lin*

Main category: cs.CV

TL;DR: NSD是一个新颖的框架，通过并行交叉注意力机制和渐进式自风格表示学习模块，实现图像编辑中的语义控制和风格一致性保持。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑方法主要关注语义控制，但忽视了风格一致性的重要性，这影响了图像的整体美感和连贯性。

Method: 提出Neural Scene Designer框架，采用双并行交叉注意力机制分别处理文本和风格信息，并设计PSRL模块通过风格对比损失学习细粒度风格表示。

Result: 在建立的综合基准测试中，NSD框架表现出色，能够实现逼真的图像操作，同时保持语义对齐和风格一致性。

Conclusion: NSD框架有效解决了图像编辑中的风格一致性问题，提出的PSRL模块和评估基准为该领域的研究提供了重要贡献。

Abstract: Maintaining stylistic consistency is crucial for the cohesion and aesthetic appeal of images, a fundamental requirement in effective image editing and inpainting. However, existing methods primarily focus on the semantic control of generated content, often neglecting the critical task of preserving this consistency. In this work, we introduce the Neural Scene Designer (NSD), a novel framework that enables photo-realistic manipulation of user-specified scene regions while ensuring both semantic alignment with user intent and stylistic consistency with the surrounding environment. NSD leverages an advanced diffusion model, incorporating two parallel cross-attention mechanisms that separately process text and style information to achieve the dual objectives of semantic control and style consistency. To capture fine-grained style representations, we propose the Progressive Self-style Representational Learning (PSRL) module. This module is predicated on the intuitive premise that different regions within a single image share a consistent style, whereas regions from different images exhibit distinct styles. The PSRL module employs a style contrastive loss that encourages high similarity between representations from the same image while enforcing dissimilarity between those from different images. Furthermore, to address the lack of standardized evaluation protocols for this task, we establish a comprehensive benchmark. This benchmark includes competing algorithms, dedicated style-related metrics, and diverse datasets and settings to facilitate fair comparisons. Extensive experiments conducted on our benchmark demonstrate the effectiveness of the proposed framework.

</details>


### [42] [MILO: A Lightweight Perceptual Quality Metric for Image and Latent-Space Optimization](https://arxiv.org/abs/2509.01411)
*Uğur Çoğalan,Mojtaba Bemana,Karol Myszkowski,Hans-Peter Seidel,Colin Groth*

Main category: cs.CV

TL;DR: MILO是一个轻量级的多尺度感知度量标准，用于全参考图像质量评估，通过伪MOS监督训练，在图像和潜在空间中作为感知损失函数表现出色


<details>
  <summary>Details</summary>
Motivation: 为了解决传统图像质量评估方法需要大规模人工标注数据的问题，以及现有度量标准在感知对齐和计算效率方面的不足

Method: 使用可重现的失真图像和集成质量度量生成伪MOS监督数据，训练轻量级多尺度网络，结合空间掩蔽和课程学习策略进行感知优化

Result: 在标准FR-IQA基准测试中超越现有度量标准，在去噪、超分辨率和人脸修复等任务中显著提升性能，同时减少计算开销

Conclusion: MILO不仅是最先进的图像质量度量标准，还是生成管道中感知优化的实用工具，具有快速推理和实时应用能力

Abstract: We present MILO (Metric for Image- and Latent-space Optimization), a lightweight, multiscale, perceptual metric for full-reference image quality assessment (FR-IQA). MILO is trained using pseudo-MOS (Mean Opinion Score) supervision, in which reproducible distortions are applied to diverse images and scored via an ensemble of recent quality metrics that account for visual masking effects. This approach enables accurate learning without requiring large-scale human-labeled datasets. Despite its compact architecture, MILO outperforms existing metrics across standard FR-IQA benchmarks and offers fast inference suitable for real-time applications. Beyond quality prediction, we demonstrate the utility of MILO as a perceptual loss in both image and latent domains. In particular, we show that spatial masking modeled by MILO, when applied to latent representations from a VAE encoder within Stable Diffusion, enables efficient and perceptually aligned optimization. By combining spatial masking with a curriculum learning strategy, we first process perceptually less relevant regions before progressively shifting the optimization to more visually distorted areas. This strategy leads to significantly improved performance in tasks like denoising, super-resolution, and face restoration, while also reducing computational overhead. MILO thus functions as both a state-of-the-art image quality metric and as a practical tool for perceptual optimization in generative pipelines.

</details>


### [43] [InfoScale: Unleashing Training-free Variable-scaled Image Generation via Effective Utilization of Information](https://arxiv.org/abs/2509.01421)
*Guohui Zhang,Jiangtong Tan,Linjiang Huang,Zhonghang Yuan,Naishan Zheng,Jie Huang,Feng Zhao*

Main category: cs.CV

TL;DR: InfoScale是一个信息中心化的框架，通过渐进频率补偿、自适应信息聚合和噪声适应三个模块，解决扩散模型在不同分辨率图像生成中的信息损失、聚合不灵活和分布不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在训练尺度以外的分辨率上生成图像时性能下降，主要原因是不同分辨率的信息量差异需要不同的信息转换过程。

Method: 提出InfoScale框架，包含三个核心模块：1）渐进频率补偿模块解决扩张卷积导致的高频信息损失；2）自适应信息聚合模块实现信息灵活聚合；3）噪声适应模块重新分布初始噪声中的信息。

Result: 该方法即插即用，在变尺度图像生成方面表现出色，实验证明了其有效性。

Conclusion: InfoScale通过信息中心化的方法有效解决了扩散模型在变尺度图像生成中的关键问题，为多分辨率图像生成提供了实用解决方案。

Abstract: Diffusion models (DMs) have become dominant in visual generation but suffer performance drop when tested on resolutions that differ from the training scale, whether lower or higher. In fact, the key challenge in generating variable-scale images lies in the differing amounts of information across resolutions, which requires information conversion procedures to be varied for generating variable-scaled images. In this paper, we investigate the issues of three critical aspects in DMs for a unified analysis in variable-scaled generation: dilated convolution, attention mechanisms, and initial noise. Specifically, 1) dilated convolution in DMs for the higher-resolution generation loses high-frequency information. 2) Attention for variable-scaled image generation struggles to adjust the information aggregation adaptively. 3) The spatial distribution of information in the initial noise is misaligned with variable-scaled image. To solve the above problems, we propose \textbf{InfoScale}, an information-centric framework for variable-scaled image generation by effectively utilizing information from three aspects correspondingly. For information loss in 1), we introduce Progressive Frequency Compensation module to compensate for high-frequency information lost by dilated convolution in higher-resolution generation. For information aggregation inflexibility in 2), we introduce Adaptive Information Aggregation module to adaptively aggregate information in lower-resolution generation and achieve an effective balance between local and global information in higher-resolution generation. For information distribution misalignment in 3), we design Noise Adaptation module to re-distribute information in initial noise for variable-scaled generation. Our method is plug-and-play for DMs and extensive experiments demonstrate the effectiveness in variable-scaled image generation.

</details>


### [44] [A Continuous-Time Consistency Model for 3D Point Cloud Generation](https://arxiv.org/abs/2509.01492)
*Sebastian Eilermann,René Heesch,Oliver Niggemann*

Main category: cs.CV

TL;DR: ConTiCoM-3D是一个连续时间一致性模型，直接从点空间合成3D形状，无需离散化扩散步骤、预训练教师模型或潜在空间编码，实现了高效的一到两步推理。


<details>
  <summary>Details</summary>
Motivation: 快速准确的3D点云形状生成对于机器人、AR/VR和数字内容创建应用至关重要，需要避免传统方法中的迭代去噪和潜在解码器依赖。

Method: 整合TrigFlow启发的连续噪声调度和基于Chamfer距离的几何损失，使用时间条件神经网络在连续时间中操作，避免昂贵的雅可比向量乘积计算。

Result: 在ShapeNet基准测试中，ConTiCoM-3D在质量和效率上匹配或超越了最先进的扩散和潜在一致性模型。

Conclusion: 该方法为可扩展的3D形状生成建立了一个实用框架，实现了高几何保真度的快速生成。

Abstract: Fast and accurate 3D shape generation from point clouds is essential for applications in robotics, AR/VR, and digital content creation. We introduce ConTiCoM-3D, a continuous-time consistency model that synthesizes 3D shapes directly in point space, without discretized diffusion steps, pre-trained teacher models, or latent-space encodings. The method integrates a TrigFlow-inspired continuous noise schedule with a Chamfer Distance-based geometric loss, enabling stable training on high-dimensional point sets while avoiding expensive Jacobian-vector products. This design supports efficient one- to two-step inference with high geometric fidelity. In contrast to previous approaches that rely on iterative denoising or latent decoders, ConTiCoM-3D employs a time-conditioned neural network operating entirely in continuous time, thereby achieving fast generation. Experiments on the ShapeNet benchmark show that ConTiCoM-3D matches or outperforms state-of-the-art diffusion and latent consistency models in both quality and efficiency, establishing it as a practical framework for scalable 3D shape generation.

</details>


### [45] [Acoustic Interference Suppression in Ultrasound images for Real-Time HIFU Monitoring Using an Image-Based Latent Diffusion Model](https://arxiv.org/abs/2509.01557)
*Dejia Cai,Yao Ran,Kun Yang,Xinwang Shi,Yingying Zhou,Kexian Wu,Yang Xu,Yi Hu,Xiaowei Zhou*

Main category: cs.CV

TL;DR: HIFU-ILDiff是一种基于潜在扩散模型的深度学习新方法，能够实时抑制HIFU治疗中超声图像的干扰，显著优于传统陷波滤波器，实现15帧/秒的实时处理。


<details>
  <summary>Details</summary>
Motivation: HIFU治疗的成功和安全性依赖于实时监测，但使用超声引导HIFU治疗时存在干扰问题，影响治疗效果和安全性。

Method: 采用VQ-VAE将含噪超声图像编码到低维潜在空间，然后使用潜在扩散模型迭代去除干扰，最后解码重建高分辨率无干扰超声图像。

Result: 在体外实验中，HIFU-ILDiff达到SSIM 0.796和PSNR 23.780，显著优于陷波滤波器的SSIM 0.443和PSNR 14.420，处理速度达到15帧/秒。

Conclusion: HIFU-ILDiff能够实时去噪HIFU干扰，提高HIFU治疗精度，具有重要临床应用价值。

Abstract: High-Intensity Focused Ultrasound (HIFU) is a non-invasive therapeutic technique widely used for treating various diseases. However, the success and safety of HIFU treatments depend on real-time monitoring, which is often hindered by interference when using ultrasound to guide HIFU treatment. To address these challenges, we developed HIFU-ILDiff, a novel deep learning-based approach leveraging latent diffusion models to suppress HIFU-induced interference in ultrasound images. The HIFU-ILDiff model employs a Vector Quantized Variational Autoencoder (VQ-VAE) to encode noisy ultrasound images into a lower-dimensional latent space, followed by a latent diffusion model that iteratively removes interference. The denoised latent vectors are then decoded to reconstruct high-resolution, interference-free ultrasound images. We constructed a comprehensive dataset comprising 18,872 image pairs from in vitro phantoms, ex vivo tissues, and in vivo animal data across multiple imaging modalities and HIFU power levels to train and evaluate the model. Experimental results demonstrate that HIFU-ILDiff significantly outperforms the commonly used Notch Filter method, achieving a Structural Similarity Index (SSIM) of 0.796 and Peak Signal-to-Noise Ratio (PSNR) of 23.780 compared to SSIM of 0.443 and PSNR of 14.420 for the Notch Filter under in vitro scenarios. Additionally, HIFU-ILDiff achieves real-time processing at 15 frames per second, markedly faster than the Notch Filter's 5 seconds per frame. These findings indicate that HIFU-ILDiff is able to denoise HIFU interference in ultrasound guiding images for real-time monitoring during HIFU therapy, which will greatly improve the treatment precision in current clinical applications.

</details>


### [46] [Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling](https://arxiv.org/abs/2509.01624)
*Natalia Frumkin,Diana Marculescu*

Main category: cs.CV

TL;DR: Q-Sched是一种新的后训练量化方法，通过调整扩散模型调度器而非模型权重，在4倍模型压缩下实现全精度准确率，显著提升few-step扩散模型的效率。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型计算成本高昂，few-step方法虽然减少了去噪步骤但仍依赖大型未压缩模型，限制了在非数据中心GPU上的部署和后训练量化方法的应用。

Method: 提出Q-Sched范式，通过修改扩散调度器来调整few-step采样轨迹；引入JAQ损失函数，结合文本-图像兼容性和图像质量指标进行细粒度优化，无需全精度推理校准。

Result: 在FP16 4-step Latent Consistency Model上FID提升15.5%，在FP16 8-step Phased Consistency Model上提升16.6%；大规模用户研究（80,000+标注）证实了在FLUX.1和SDXL-Turbo上的有效性。

Conclusion: 量化和few-step蒸馏在高保真生成方面具有互补性，Q-Sched为高效扩散模型部署提供了有效解决方案。

Abstract: Text-to-image diffusion models are computationally intensive, often requiring dozens of forward passes through large transformer backbones. For instance, Stable Diffusion XL generates high-quality images with 50 evaluations of a 2.6B-parameter model, an expensive process even for a single batch. Few-step diffusion models reduce this cost to 2-8 denoising steps but still depend on large, uncompressed U-Net or diffusion transformer backbones, which are often too costly for full-precision inference without datacenter GPUs. These requirements also limit existing post-training quantization methods that rely on full-precision calibration. We introduce Q-Sched, a new paradigm for post-training quantization that modifies the diffusion model scheduler rather than model weights. By adjusting the few-step sampling trajectory, Q-Sched achieves full-precision accuracy with a 4x reduction in model size. To learn quantization-aware pre-conditioning coefficients, we propose the JAQ loss, which combines text-image compatibility with an image quality metric for fine-grained optimization. JAQ is reference-free and requires only a handful of calibration prompts, avoiding full-precision inference during calibration. Q-Sched delivers substantial gains: a 15.5% FID improvement over the FP16 4-step Latent Consistency Model and a 16.6% improvement over the FP16 8-step Phased Consistency Model, showing that quantization and few-step distillation are complementary for high-fidelity generation. A large-scale user study with more than 80,000 annotations further confirms Q-Sched's effectiveness on both FLUX.1[schnell] and SDXL-Turbo.

</details>


### [47] [GaussianGAN: Real-Time Photorealistic controllable Human Avatars](https://arxiv.org/abs/2509.01681)
*Mohamed Ilyes Lakhal,Richard Bowden*

Main category: cs.CV

TL;DR: GaussianGAN是一种实时可动画化的人体avatar渲染方法，通过高斯泼溅密度化策略和语义分割模块解决现有方法模糊问题，在79FPS下实现照片级真实感渲染。


<details>
  <summary>Details</summary>
Motivation: 当前神经渲染方法存在明显模糊问题，需要开发能够实时生成照片级真实感且可控的人体avatar的新方法。

Method: 提出高斯泼溅密度化策略从骨骼肢体周围的圆柱结构表面构建高斯点；引入新颖视角分割模块渲染准确语义分割；使用UNet生成器结合高斯泼溅特征和分割图生成avatar。

Result: 在ZJU Mocap数据集上达到32.94db像素保真度，在Thuman4数据集上达到33.39db，以79FPS实时运行，视觉感知和质量优于先前方法。

Conclusion: GaussianGAN在实时照片级人体avatar渲染方面达到最先进水平，有效解决了模糊问题并实现了高质量的视觉效果。

Abstract: Photorealistic and controllable human avatars have gained popularity in the research community thanks to rapid advances in neural rendering, providing fast and realistic synthesis tools. However, a limitation of current solutions is the presence of noticeable blurring. To solve this problem, we propose GaussianGAN, an animatable avatar approach developed for photorealistic rendering of people in real-time. We introduce a novel Gaussian splatting densification strategy to build Gaussian points from the surface of cylindrical structures around estimated skeletal limbs. Given the camera calibration, we render an accurate semantic segmentation with our novel view segmentation module. Finally, a UNet generator uses the rendered Gaussian splatting features and the segmentation maps to create photorealistic digital avatars. Our method runs in real-time with a rendering speed of 79 FPS. It outperforms previous methods regarding visual perception and quality, achieving a state-of-the-art results in terms of a pixel fidelity of 32.94db on the ZJU Mocap dataset and 33.39db on the Thuman4 dataset.

</details>


### [48] [Clinical Metadata Guided Limited-Angle CT Image Reconstruction](https://arxiv.org/abs/2509.01752)
*Yu Shi,Shuyi Fan,Changsheng Fang,Shuo Han,Haodong Li,Li Zhou,Bahareh Morovati,Dayang Wang,Hengyong Yu*

Main category: cs.CV

TL;DR: 提出基于结构化临床元数据的两阶段扩散框架，用于解决有限角度CT重建问题，通过元数据引导生成先验并提升重建质量


<details>
  <summary>Details</summary>
Motivation: 有限角度CT虽然能提高时间分辨率和降低辐射剂量，但由于投影截断会产生严重伪影，需要解决其病态重建问题

Method: 两阶段扩散框架：第一阶段基于transformer的扩散模型仅使用元数据生成粗略解剖先验；第二阶段整合粗略先验和元数据生成高保真结果，每个采样步骤都使用ADMM模块强制执行物理数据一致性

Result: 在合成和真实心脏CT数据集上的实验表明，加入元数据显著提高了重建保真度，特别是在严重角度截断情况下，在SSIM、PSNR、nMI和PCC指标上优于现有无元数据基线方法

Conclusion: 临床元数据在提高重建质量和效率方面具有双重作用，支持将其整合到未来的元数据引导医学成像框架中，消融研究证实不同类型元数据具有互补优势

Abstract: Limited-angle computed tomography (LACT) offers improved temporal resolution and reduced radiation dose for cardiac imaging, but suffers from severe artifacts due to truncated projections. To address the ill-posedness of LACT reconstruction, we propose a two-stage diffusion framework guided by structured clinical metadata. In the first stage, a transformer-based diffusion model conditioned exclusively on metadata, including acquisition parameters, patient demographics, and diagnostic impressions, generates coarse anatomical priors from noise. The second stage further refines the images by integrating both the coarse prior and metadata to produce high-fidelity results. Physics-based data consistency is enforced at each sampling step in both stages using an Alternating Direction Method of Multipliers module, ensuring alignment with the measured projections. Extensive experiments on both synthetic and real cardiac CT datasets demonstrate that incorporating metadata significantly improves reconstruction fidelity, particularly under severe angular truncation. Compared to existing metadata-free baselines, our method achieves superior performance in SSIM, PSNR, nMI, and PCC. Ablation studies confirm that different types of metadata contribute complementary benefits, particularly diagnostic and demographic priors under limited-angle conditions. These findings highlight the dual role of clinical metadata in improving both reconstruction quality and efficiency, supporting their integration into future metadata-guided medical imaging frameworks.

</details>


### [49] [PractiLight: Practical Light Control Using Foundational Diffusion Models](https://arxiv.org/abs/2509.01837)
*Yotam Erel,Rishabh Dabral,Vladislav Golyanik,Amit H. Bermano,Christian Theobalt*

Main category: cs.CV

TL;DR: PractiLight是一个实用的图像光照控制方法，通过轻量级LoRA回归器生成辐照度图，并利用分类器引导将所需光照融入生成过程，在少量训练数据下实现高质量的光照控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常在特定领域的大规模数据集上训练，限制了基础模型的泛化能力和适用性。本文旨在利用生成模型的基础知识，开发一个实用且通用的光照控制方法。

Method: 关键发现是图像中的光照关系与自注意力层的token交互相似。基于此，训练轻量级LoRA回归器生成直接辐照度图，然后使用分类器引导将所需光照融入另一图像的生成过程。

Result: 在多种场景类型上展示了最先进的性能，在质量和控制方面表现优异，相比领先工作具有更好的参数和数据效率。

Conclusion: 这项工作证实了通过挖掘基础知识可以可行地控制图像光照，实现了实用且通用的重光照功能。

Abstract: Light control in generated images is a difficult task, posing specific challenges, spanning over the entire image and frequency spectrum. Most approaches tackle this problem by training on extensive yet domain-specific datasets, limiting the inherent generalization and applicability of the foundational backbones used. Instead, PractiLight is a practical approach, effectively leveraging foundational understanding of recent generative models for the task. Our key insight is that lighting relationships in an image are similar in nature to token interaction in self-attention layers, and hence are best represented there. Based on this and other analyses regarding the importance of early diffusion iterations, PractiLight trains a lightweight LoRA regressor to produce the direct irradiance map for a given image, using a small set of training images. We then employ this regressor to incorporate the desired lighting into the generation process of another image using Classifier Guidance. This careful design generalizes well to diverse conditions and image domains. We demonstrate state-of-the-art performance in terms of quality and control with proven parameter and data efficiency compared to leading works over a wide variety of scenes types. We hope this work affirms that image lighting can feasibly be controlled by tapping into foundational knowledge, enabling practical and general relighting.

</details>


### [50] [Latent Gene Diffusion for Spatial Transcriptomics Completion](https://arxiv.org/abs/2509.01864)
*Paula Cárdenas,Leonardo Manrique,Daniela Vega,Daniela Ruiz,Pablo Arbeláez*

Main category: cs.CV

TL;DR: LGDiST是首个无参考的潜在基因扩散模型，用于处理空间转录组学数据缺失问题，在26个数据集上平均MSE降低18%，并提升6种最先进方法的基因表达预测性能达10%。


<details>
  <summary>Details</summary>
Motivation: 当前基于计算机视觉的空间转录组学分析方法受数据缺失限制，现有方法依赖单细胞RNA测序参考，存在对齐质量依赖、外部数据集依赖、批次效应和继承缺失等问题。

Method: 提出LGDiST模型，使用之前被认为无信息性的上下文基因构建丰富且具有生物学意义的遗传潜在空间，采用扩散模型架构，包含邻居条件等关键组件。

Result: 在26个数据集上平均MSE降低18%，在6种最先进方法上基因表达预测性能提升达10%。移除上下文基因、ST潜在空间或邻居条件等组件会导致性能显著下降。

Conclusion: LGDiST完整架构的性能显著优于任何孤立组件，证明了其在空间转录组学数据补全方面的有效性和优越性。

Abstract: Computer Vision has proven to be a powerful tool for analyzing Spatial Transcriptomics (ST) data. However, current models that predict spatially resolved gene expression from histopathology images suffer from significant limitations due to data dropout. Most existing approaches rely on single-cell RNA sequencing references, making them dependent on alignment quality and external datasets while also risking batch effects and inherited dropout. In this paper, we address these limitations by introducing LGDiST, the first reference-free latent gene diffusion model for ST data dropout. We show that LGDiST outperforms the previous state-of-the-art in gene expression completion, with an average Mean Squared Error that is 18% lower across 26 datasets. Furthermore, we demonstrate that completing ST data with LGDiST improves gene expression prediction performance on six state-of-the-art methods up to 10% in MSE. A key innovation of LGDiST is using context genes previously considered uninformative to build a rich and biologically meaningful genetic latent space. Our experiments show that removing key components of LGDiST, such as the context genes, the ST latent space, and the neighbor conditioning, leads to considerable drops in performance. These findings underscore that the full architecture of LGDiST achieves substantially better performance than any of its isolated components.

</details>


### [51] [DroneSR: Rethinking Few-shot Thermal Image Super-Resolution from Drone-based Perspective](https://arxiv.org/abs/2509.01898)
*Zhipeng Weng,Xiaopeng Liu,Ce Liu,Xingyuan Guo,Yukai Shi,Liang Lin*

Main category: cs.CV

TL;DR: 提出一种面向扩散模型的高斯量化表示学习方法，通过引入高斯量化表示学习和有效的监控机制，解决大规模架构在无人机红外图像超分辨率任务中的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 大规模模型在性能上取得显著改进，但过拟合问题经常削弱其泛化能力。在无人机拍摄的红外图像超分辨率任务中，扩散模型等生成模型通常采用大规模架构，但少样本训练数据经常导致严重的过拟合。

Method: 提出高斯量化表示学习方法，为扩散模型设计新的表示学习策略，同时建立有效的监控机制来跟踪训练过程中的过拟合迹象。构建了多源无人机红外图像基准数据集用于检测。

Result: 实验结果表明，该方法在构建的基准数据集上优于现有的超分辨率方法，显著减轻了大规模架构在复杂条件下的过拟合问题。

Conclusion: 高斯量化表示学习方法有效缓解了过拟合问题，同时保持了架构复杂性，为解决少样本无人机图像重建场景中的过拟合挑战提供了有效解决方案。

Abstract: Although large scale models achieve significant improvements in performance, the overfitting challenge still frequently undermines their generalization ability. In super resolution tasks on images, diffusion models as representatives of generative models typically adopt large scale architectures. However, few-shot drone-captured infrared training data frequently induces severe overfitting in large-scale architectures. To address this key challenge, our method proposes a new Gaussian quantization representation learning method oriented to diffusion models that alleviates overfitting and enhances robustness. At the same time, an effective monitoring mechanism tracks large scale architectures during training to detect signs of overfitting. By introducing Gaussian quantization representation learning, our method effectively reduces overfitting while maintaining architecture complexity. On this basis, we construct a multi source drone-based infrared image benchmark dataset for detection and use it to emphasize overfitting issues of large scale architectures in few sample, drone-based diverse drone-based image reconstruction scenarios. To verify the efficacy of the method in mitigating overfitting, experiments are conducted on the constructed benchmark. Experimental results demonstrate that our method outperforms existing super resolution approaches and significantly mitigates overfitting of large scale architectures under complex conditions. The code and DroneSR dataset will be available at: https://github.com/wengzp1/GARLSR.

</details>


### [52] [2D Gaussian Splatting with Semantic Alignment for Image Inpainting](https://arxiv.org/abs/2509.01964)
*Hongyu Li,Chaofeng Chen,Xiaoming Li,Guangming Lu*

Main category: cs.CV

TL;DR: 基于2D高斯泼溅的图像修复新框架，通过连续渲染范式实现像素级连贯性，结合DINO特征确保语义一致性，在标准基准测试中表现优异


<details>
  <summary>Details</summary>
Motivation: 探索高斯泼溅技术在图像修复领域的潜力，该任务需要局部像素合成的连贯性和全局语义恢复的一致性

Method: 提出首个基于2D高斯泼溅的图像修复框架，将不完整图像编码为2D高斯泼溅系数的连续场，通过可微分光栅化过程重建图像；引入分块光栅化策略提高效率；整合预训练DINO模型的全局特征确保语义一致性

Result: 在标准基准测试中实现了竞争性的定量指标和感知质量表现

Conclusion: 为高斯泼溅技术在2D图像处理中的应用开辟了新方向，证明了该方法在图像修复任务中的有效性

Abstract: Gaussian Splatting (GS), a recent technique for converting discrete points into continuous spatial representations, has shown promising results in 3D scene modeling and 2D image super-resolution. In this paper, we explore its untapped potential for image inpainting, which demands both locally coherent pixel synthesis and globally consistent semantic restoration. We propose the first image inpainting framework based on 2D Gaussian Splatting, which encodes incomplete images into a continuous field of 2D Gaussian splat coefficients and reconstructs the final image via a differentiable rasterization process. The continuous rendering paradigm of GS inherently promotes pixel-level coherence in the inpainted results. To improve efficiency and scalability, we introduce a patch-wise rasterization strategy that reduces memory overhead and accelerates inference. For global semantic consistency, we incorporate features from a pretrained DINO model. We observe that DINO's global features are naturally robust to small missing regions and can be effectively adapted to guide semantic alignment in large-mask scenarios, ensuring that the inpainted content remains contextually consistent with the surrounding scene. Extensive experiments on standard benchmarks demonstrate that our method achieves competitive performance in both quantitative metrics and perceptual quality, establishing a new direction for applying Gaussian Splatting to 2D image processing.

</details>


### [53] [Discrete Noise Inversion for Next-scale Autoregressive Text-based Image Editing](https://arxiv.org/abs/2509.01984)
*Quan Dao,Xiaoxiao He,Ligong Han,Ngan Hoai Nguyen,Amin Heyrani Nobar,Faez Ahmed,Han Zhang,Viet Anh Nguyen,Dimitris Metaxas*

Main category: cs.CV

TL;DR: VARIN是首个为视觉自回归模型设计的噪声反演编辑技术，通过位置感知Argmax反演生成逆Gumbel噪声，实现精确图像重建和文本引导编辑。


<details>
  <summary>Details</summary>
Motivation: 虽然条件生成已被广泛探索，但无需额外训练即可进行提示引导图像编辑的能力同样重要，因为它支持众多实际应用。

Method: 引入VARIN技术，利用新颖的位置感知Argmax反演(LAI)伪逆函数生成逆Gumbel噪声，实现源图像精确重建和文本对齐的针对性编辑。

Result: 大量实验表明VARIN能有效根据指定提示修改源图像，同时显著保留原始背景和结构细节。

Conclusion: VARIN被验证为一种实用的编辑方法，为视觉自回归模型提供了有效的文本到图像编辑能力。

Abstract: Visual autoregressive models (VAR) have recently emerged as a promising class of generative models, achieving performance comparable to diffusion models in text-to-image generation tasks. While conditional generation has been widely explored, the ability to perform prompt-guided image editing without additional training is equally critical, as it supports numerous practical real-world applications. This paper investigates the text-to-image editing capabilities of VAR by introducing Visual AutoRegressive Inverse Noise (VARIN), the first noise inversion-based editing technique designed explicitly for VAR models. VARIN leverages a novel pseudo-inverse function for argmax sampling, named Location-aware Argmax Inversion (LAI), to generate inverse Gumbel noises. These inverse noises enable precise reconstruction of the source image and facilitate targeted, controllable edits aligned with textual prompts. Extensive experiments demonstrate that VARIN effectively modifies source images according to specified prompts while significantly preserving the original background and structural details, thus validating its efficacy as a practical editing approach.

</details>


### [54] [Palette Aligned Image Diffusion](https://arxiv.org/abs/2509.02000)
*Elad Aharoni,Noy Porat,Dani Lischinski,Ariel Shamir*

Main category: cs.CV

TL;DR: Palette-Adapter是一种新颖的方法，通过将调色板解释为稀疏直方图并引入两个控制参数，在文本到图像扩散模型中实现稳定的调色板条件控制。


<details>
  <summary>Details</summary>
Motivation: 调色板是创意工作流程中广泛使用的直观工具，但在图像生成中作为条件使用时存在显著的模糊性和不稳定性，需要解决这一挑战。

Method: 将调色板解释为稀疏直方图，引入直方图熵和调色板到直方图距离两个标量控制参数，并采用负直方图机制来抑制不需要的色调，在精心策划的数据集上进行训练。

Result: 该方法能够在广泛的调色板和提示范围内实现稳定、语义连贯的生成，在定性和定量评估以及用户研究中始终优于现有方法。

Conclusion: Palette-Adapter通过创新的调色板解释和控制机制，成功解决了调色板条件图像生成中的模糊性和不稳定性问题，实现了更好的调色板遵循度和图像质量。

Abstract: We introduce the Palette-Adapter, a novel method for conditioning text-to-image diffusion models on a user-specified color palette. While palettes are a compact and intuitive tool widely used in creative workflows, they introduce significant ambiguity and instability when used for conditioning image generation. Our approach addresses this challenge by interpreting palettes as sparse histograms and introducing two scalar control parameters: histogram entropy and palette-to-histogram distance, which allow flexible control over the degree of palette adherence and color variation. We further introduce a negative histogram mechanism that allows users to suppress specific undesired hues, improving adherence to the intended palette under the standard classifier-free guidance mechanism. To ensure broad generalization across the color space, we train on a carefully curated dataset with balanced coverage of rare and common colors. Our method enables stable, semantically coherent generation across a wide range of palettes and prompts. We evaluate our method qualitatively, quantitatively, and through a user study, and show that it consistently outperforms existing approaches in achieving both strong palette adherence and high image quality.

</details>


### [55] [Enhancing Zero-Shot Pedestrian Attribute Recognition with Synthetic Data Generation: A Comparative Study with Image-To-Image Diffusion Models](https://arxiv.org/abs/2509.02161)
*Pablo Ayuso-Albizu,Juan C. SanMiguel,Pablo Carballeira*

Main category: cs.CV

TL;DR: 本文研究扩散模型在生成用于行人属性识别(PAR)任务的合成行人图像方面的有效性，通过优化文本提示和图像属性等关键参数，实现了4.5%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 行人属性识别面临大规模标注数据集稀缺的问题，特别是在复杂场景中。扩散模型能够生成多样且逼真的合成图像，但其在PAR任务中的潜力尚未充分探索。

Method: 识别img2img扩散数据扩展的关键参数（文本提示、图像属性等），研究它们对PAR图像生成质量的影响，并采用最佳扩展方法通过丰富零样本数据集来生成合成图像训练PAR模型。

Result: 实验结果表明提示对齐和图像属性是图像生成的关键因素，最优选择使PAR识别性能提高了4.5%。

Conclusion: 扩散模型能够有效生成适用于PAR任务的合成行人图像，通过精心设计的参数优化可以显著提升模型在真实场景中的鲁棒性和适应性。

Abstract: Pedestrian Attribute Recognition (PAR) involves identifying various human attributes from images with applications in intelligent monitoring systems. The scarcity of large-scale annotated datasets hinders the generalization of PAR models, specially in complex scenarios involving occlusions, varying poses, and diverse environments. Recent advances in diffusion models have shown promise for generating diverse and realistic synthetic images, allowing to expand the size and variability of training data. However, the potential of diffusion-based data expansion for generating PAR-like images remains underexplored. Such expansion may enhance the robustness and adaptability of PAR models in real-world scenarios. This paper investigates the effectiveness of diffusion models in generating synthetic pedestrian images tailored to PAR tasks. We identify key parameters of img2img diffusion-based data expansion; including text prompts, image properties, and the latest enhancements in diffusion-based data augmentation, and examine their impact on the quality of generated images for PAR. Furthermore, we employ the best-performing expansion approach to generate synthetic images for training PAR models, by enriching the zero-shot datasets. Experimental results show that prompt alignment and image properties are critical factors in image generation, with optimal selection leading to a 4.5% improvement in PAR recognition performance.

</details>


### [56] [Data-Driven Loss Functions for Inference-Time Optimization in Text-to-Image Generation](https://arxiv.org/abs/2509.02295)
*Sapir Esther Yiflach,Yuval Atzmon,Gal Chechik*

Main category: cs.CV

TL;DR: Learn-to-Steer框架通过训练轻量级分类器从扩散模型的交叉注意力图中解码空间关系，作为学习损失函数在推理时优化，显著提升了文本到图像模型的空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型在空间推理任务上表现不佳，如左右位置关系等简单任务经常失败。现有方法通过模型微调或手工设计的损失函数进行优化，但这些方法不够理想。

Method: 提出Learn-to-Steer框架：1）训练轻量级分类器从交叉注意力图中解码空间关系；2）使用双重反转策略避免分类器走捷径；3）将训练好的分类器作为学习损失函数在推理时进行优化。

Result: 方法显著提升了空间准确性：在FLUX.1-dev上从0.20提升到0.61，在SD2.1上从0.07提升到0.54。同时能够泛化到多种空间关系并显著提高准确性。

Conclusion: 通过直接从模型内部表示学习目标而非手工设计损失函数，Learn-to-Steer框架有效解决了扩散模型的空间推理问题，为改进文本到图像生成的空间准确性提供了新思路。

Abstract: Text-to-image diffusion models can generate stunning visuals, yet they often fail at tasks children find trivial--like placing a dog to the right of a teddy bear rather than to the left. When combinations get more unusual--a giraffe above an airplane--these failures become even more pronounced. Existing methods attempt to fix these spatial reasoning failures through model fine-tuning or test-time optimization with handcrafted losses that are suboptimal. Rather than imposing our assumptions about spatial encoding, we propose learning these objectives directly from the model's internal representations. We introduce Learn-to-Steer, a novel framework that learns data-driven objectives for test-time optimization rather than handcrafting them. Our key insight is to train a lightweight classifier that decodes spatial relationships from the diffusion model's cross-attention maps, then deploy this classifier as a learned loss function during inference. Training such classifiers poses a surprising challenge: they can take shortcuts by detecting linguistic traces rather than learning true spatial patterns. We solve this with a dual-inversion strategy that enforces geometric understanding. Our method dramatically improves spatial accuracy: from 0.20 to 0.61 on FLUX.1-dev and from 0.07 to 0.54 on SD2.1 across standard benchmarks. Moreover, our approach generalizes to multiple relations and significantly improves accuracy.

</details>


### [57] [From Noisy Labels to Intrinsic Structure: A Geometric-Structural Dual-Guided Framework for Noise-Robust Medical Image Segmentation](https://arxiv.org/abs/2509.02419)
*Tao Wang,Zhenxuan Zhang,Yuanbo Zhou,Xinlin Zhang,Yuanbin Chen,Tao Tan,Guang Yang,Tong Tong*

Main category: cs.CV

TL;DR: GSD-Net是一个几何结构双引导网络，通过整合几何和结构线索来提高对噪声标注的鲁棒性，在医学图像分割中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割需要大规模高质量标注，但专家标注也存在主观性和粗糙标注带来的噪声，这些噪声会破坏特征学习并影响模型性能。

Method: 提出GSD-Net网络，包含几何距离感知模块动态调整像素级权重，结构引导标签细化模块利用结构先验精炼标签，以及知识转移模块增强监督和对局部细节的敏感性。

Result: 在六个公开数据集上评估，在噪声标注下达到最先进性能，在Kvasir、Shenzhen、BU-SUC和BraTS2020数据集上分别提升2.52%、22.76%、8.87%和4.59%。

Conclusion: GSD-Net通过几何和结构双引导机制有效提升了医学图像分割模型对噪声标注的鲁棒性，在模拟和真实噪声环境下均表现优异。

Abstract: The effectiveness of convolutional neural networks in medical image segmentation relies on large-scale, high-quality annotations, which are costly and time-consuming to obtain. Even expert-labeled datasets inevitably contain noise arising from subjectivity and coarse delineations, which disrupt feature learning and adversely impact model performance. To address these challenges, this study propose a Geometric-Structural Dual-Guided Network (GSD-Net), which integrates geometric and structural cues to improve robustness against noisy annotations. It incorporates a Geometric Distance-Aware module that dynamically adjusts pixel-level weights using geometric features, thereby strengthening supervision in reliable regions while suppressing noise. A Structure-Guided Label Refinement module further refines labels with structural priors, and a Knowledge Transfer module enriches supervision and improves sensitivity to local details. To comprehensively assess its effectiveness, we evaluated GSD-Net on six publicly available datasets: four containing three types of simulated label noise, and two with multi-expert annotations that reflect real-world subjectivity and labeling inconsistencies. Experimental results demonstrate that GSD-Net achieves state-of-the-art performance under noisy annotations, achieving improvements of 2.52% on Kvasir, 22.76% on Shenzhen, 8.87% on BU-SUC, and 4.59% on BraTS2020 under SR simulated noise. The codes of this study are available at https://github.com/ortonwang/GSD-Net.

</details>


### [58] [Towards High-Fidelity, Identity-Preserving Real-Time Makeup Transfer: Decoupling Style Generation](https://arxiv.org/abs/2509.02445)
*Lydia Kin Ching Chau,Zhi Yu,Ruo Wei Jiang*

Main category: cs.CV

TL;DR: 高保真实时虚拟化妆试用框架，通过透明化妆提取和图形渲染两步骤实现身份保持和时间一致性


<details>
  <summary>Details</summary>
Motivation: 解决现有化妆转移方法在身份保持、质感重现和时间一致性方面的不足，特别是无法有效分离半透明化妆物与皮肤颜色的问题

Method: 两步法：先进行透明化妆提取，然后使用图形学方法进行实时渲染。训练数据通过图形渲染管线和无监督k-means聚类生成，采用专门的训练目标包括alpha权重重建和唇彩损失

Result: 方法在多样化姿态、表情和皮色情况下都能实现稳健的化妆转移，保持时间平滑性，在细节捐捉、时间稳定性和身份保持方面超过现有基线方法

Conclusion: 该框架通过创新的两步设计有效解决了实时虚拟化妆试用中的关键挑战，为高保真、身份保持的实时化妆效果提供了可行解决方案

Abstract: We present a novel framework for real-time virtual makeup try-on that achieves high-fidelity, identity-preserving cosmetic transfer with robust temporal consistency. In live makeup transfer applications, it is critical to synthesize temporally coherent results that accurately replicate fine-grained makeup and preserve user's identity. However, existing methods often struggle to disentangle semitransparent cosmetics from skin tones and other identify features, causing identity shifts and raising fairness concerns. Furthermore, current methods lack real-time capabilities and fail to maintain temporal consistency, limiting practical adoption. To address these challenges, we decouple makeup transfer into two steps: transparent makeup mask extraction and graphics-based mask rendering. After the makeup extraction step, the makeup rendering can be performed in real time, enabling live makeup try-on. Our makeup extraction model trained on pseudo-ground-truth data generated via two complementary methods: a graphics-based rendering pipeline and an unsupervised k-means clustering approach. To further enhance transparency estimation and color fidelity, we propose specialized training objectives, including alpha-weighted reconstruction and lip color losses. Our method achieves robust makeup transfer across diverse poses, expressions, and skin tones while preserving temporal smoothness. Extensive experiments demonstrate that our approach outperforms existing baselines in capturing fine details, maintaining temporal stability, and preserving identity integrity.

</details>


### [59] [TeRA: Rethinking Text-driven Realistic 3D Avatar Generation](https://arxiv.org/abs/2509.02466)
*Yanwen Wang,Yiyu Zhuang,Jiawei Zhang,Li Wang,Yifei Zeng,Xun Cao,Xinxin Zuo,Hao Zhu*

Main category: cs.CV

TL;DR: TeRA是一个两阶段训练的文本到3D头像生成框架，通过潜在扩散模型在结构化潜在空间中生成逼真3D人体头像，相比传统SDS方法更高效且支持文本局部定制


<details>
  <summary>Details</summary>
Motivation: 传统基于SDS的文本到头像生成模型存在迭代优化速度慢的问题，需要更高效且支持局部定制的3D头像生成方案

Method: 采用两阶段训练策略：1）从大型人体重建模型中提取解码器构建结构化潜在空间；2）训练文本控制的潜在扩散模型在该空间中生成3D头像

Result: 实验证明TeRA在主观和客观评估上均优于之前的文本到头像生成模型，消除了缓慢的迭代优化过程

Conclusion: TeRA框架通过结构化潜在空间和潜在扩散模型，实现了更高效、更有效的文本到3D头像生成，支持文本驱动的局部定制功能

Abstract: In this paper, we rethink text-to-avatar generative models by proposing TeRA, a more efficient and effective framework than the previous SDS-based models and general large 3D generative models. Our approach employs a two-stage training strategy for learning a native 3D avatar generative model. Initially, we distill a decoder to derive a structured latent space from a large human reconstruction model. Subsequently, a text-controlled latent diffusion model is trained to generate photorealistic 3D human avatars within this latent space. TeRA enhances the model performance by eliminating slow iterative optimization and enables text-based partial customization through a structured 3D human representation. Experiments have proven our approach's superiority over previous text-to-avatar generative models in subjective and objective evaluation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [60] [A-FloPS: Accelerating Diffusion Sampling with Adaptive Flow Path Sampler](https://arxiv.org/abs/2509.00036)
*Cheng Jin,Zhenyu Xiao,Yuantao Gu*

Main category: cs.LG

TL;DR: A-FloPS是一种无需训练的高效扩散模型采样框架，通过重新参数化采样轨迹和自适应速度分解，在极低函数评估次数下实现高质量图像生成


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成性能优异，但迭代采样过程计算成本高昂。现有的免训练加速方法受限于采样轨迹效率，需要更有效的解决方案

Method: 提出A-FloPS框架：1）将预训练扩散模型的采样轨迹重新参数化为流匹配形式；2）采用自适应速度分解机制，将速度场分解为线性漂移项和残差分量；3）抑制残差分量的时间变化，恢复高阶积分优势

Result: 在条件图像生成和文本到图像合成任务中，A-FloPS在样本质量和效率方面均优于最先进的免训练采样器。仅需5次函数评估即可获得显著更低的FID分数，生成更清晰、更连贯的图像

Conclusion: A-FloPS为高质量、低延迟生成建模提供了一个通用有效的解决方案，其自适应机制还能提升原生基于流的生成模型性能

Abstract: Diffusion models deliver state-of-the-art generative performance across diverse modalities but remain computationally expensive due to their inherently iterative sampling process. Existing training-free acceleration methods typically improve numerical solvers for the reverse-time ODE, yet their effectiveness is fundamentally constrained by the inefficiency of the underlying sampling trajectories. We propose A-FloPS (Adaptive Flow Path Sampler), a principled, training-free framework that reparameterizes the sampling trajectory of any pre-trained diffusion model into a flow-matching form and augments it with an adaptive velocity decomposition. The reparameterization analytically maps diffusion scores to flow-compatible velocities, yielding integration-friendly trajectories without retraining. The adaptive mechanism further factorizes the velocity field into a linear drift term and a residual component whose temporal variation is actively suppressed, restoring the accuracy benefits of high-order integration even in extremely low-NFE regimes. Extensive experiments on conditional image generation and text-to-image synthesis show that A-FloPS consistently outperforms state-of-the-art training-free samplers in both sample quality and efficiency. Notably, with as few as $5$ function evaluations, A-FloPS achieves substantially lower FID and generates sharper, more coherent images. The adaptive mechanism also improves native flow-based generative models, underscoring its generality. These results position A-FloPS as a versatile and effective solution for high-quality, low-latency generative modeling.

</details>


### [61] [AMCR: A Framework for Assessing and Mitigating Copyright Risks in Generative Models](https://arxiv.org/abs/2509.00641)
*Zhipeng Yin,Zichong Wang,Avash Palikhe,Zhen Liu,Jun Liu,Wenbin Zhang*

Main category: cs.LG

TL;DR: AMCR框架通过提示重构、注意力相似性分析和自适应风险缓解，有效检测和减轻生成模型中的潜在版权风险


<details>
  <summary>Details</summary>
Motivation: 生成模型在文本到图像任务中取得显著进展，但严重依赖大规模训练数据，可能无意中复制受版权保护的内容，带来法律和伦理挑战。现有的基于提示的方法在处理微妙侵权情况时效果有限

Method: 提出AMCR综合框架：1）系统重构风险提示为安全形式；2）通过基于注意力的相似性分析检测部分侵权；3）在生成过程中自适应缓解风险，保持图像质量

Result: 大量实验验证了AMCR在揭示和减轻潜在版权风险方面的有效性，为生成模型的安全部署提供了实用见解和基准

Conclusion: AMCR框架为解决生成模型中的版权问题提供了全面解决方案，能够在减少版权侵权的同时保持图像质量，促进生成技术的更安全应用

Abstract: Generative models have achieved impressive results in text to image tasks, significantly advancing visual content creation. However, this progress comes at a cost, as such models rely heavily on large-scale training data and may unintentionally replicate copyrighted elements, creating serious legal and ethical challenges for real-world deployment. To address these concerns, researchers have proposed various strategies to mitigate copyright risks, most of which are prompt based methods that filter or rewrite user inputs to prevent explicit infringement. While effective in handling obvious cases, these approaches often fall short in more subtle situations, where seemingly benign prompts can still lead to infringing outputs. To address these limitations, this paper introduces Assessing and Mitigating Copyright Risks (AMCR), a comprehensive framework which i) builds upon prompt-based strategies by systematically restructuring risky prompts into safe and non-sensitive forms, ii) detects partial infringements through attention-based similarity analysis, and iii) adaptively mitigates risks during generation to reduce copyright violations without compromising image quality. Extensive experiments validate the effectiveness of AMCR in revealing and mitigating latent copyright risks, offering practical insights and benchmarks for the safer deployment of generative models.

</details>


### [62] [Conditional-$t^3$VAE: Equitable Latent Space Allocation for Fair Generation](https://arxiv.org/abs/2509.02154)
*Aymene Mohammed Bouayed,Samuel Deslauriers-Gauthier,Adrian Iaccovelli,David Naccache*

Main category: cs.LG

TL;DR: Conditional-$t^3$VAE通过为每个类别定义Student's t联合先验，解决了传统VAE在类别不平衡数据集上潜在空间分配不均的问题，显著提高了生成公平性和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统VAE及其变体$t^3$VAE在类别不平衡数据集上，潜在空间分配与训练集类别频率成正比，导致尾部类别表示不足，生成公平性降低。

Method: 提出Conditional-$t^3$VAE，为每个类别定义Student's t联合先验，防止多数类别主导。使用γ-power散度推导的闭式目标进行优化，并推导等权重潜在混合Student's t分布用于类别平衡生成。

Result: 在SVHN-LT、CIFAR100-LT和CelebA数据集上，Conditional-$t^3$VAE始终获得比$t^3$VAE和基于高斯分布的VAE基线更低的FID分数，特别是在严重类别不平衡情况下。在每类F1评估中也优于条件高斯VAE。

Conclusion: 在轻微不平衡情况下（ρ≲3），基于高斯分布的模型仍有竞争力，但在更极端的不平衡情况下，Conditional-$t^3$VAE显著提高了生成公平性和多样性。

Abstract: Variational Autoencoders (VAEs) with global priors mirror the training set's class frequency in latent space, underrepresenting tail classes and reducing generative fairness on imbalanced datasets. While $t^3$VAE improves robustness via heavy-tailed Student's t-distribution priors, it still allocates latent volume proportionally to the class frequency.In this work, we address this issue by explicitly enforcing equitable latent space allocation across classes. To this end, we propose Conditional-$t^3$VAE, which defines a per-class \mbox{Student's t} joint prior over latent and output variables, preventing dominance by majority classes. Our model is optimized using a closed-form objective derived from the $\gamma$-power divergence. Moreover, for class-balanced generation, we derive an equal-weight latent mixture of Student's t-distributions. On SVHN-LT, CIFAR100-LT, and CelebA, Conditional-$t^3$VAE consistently achieves lower FID scores than both $t^3$VAE and Gaussian-based VAE baselines, particularly under severe class imbalance. In per-class F1 evaluations, Conditional-$t^3$VAE also outperforms the conditional Gaussian VAE across all highly imbalanced settings. While Gaussian-based models remain competitive under mild imbalance ratio ($\rho \lesssim 3$), our approach substantially improves generative fairness and diversity in more extreme regimes.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [63] [GS-TG: 3D Gaussian Splatting Accelerator with Tile Grouping for Reducing Redundant Sorting while Preserving Rasterization Efficiency](https://arxiv.org/abs/2509.00911)
*Joongho Jo,Jongsun Park*

Main category: cs.AR

TL;DR: GS-TG是一种基于瓦片分组的3D高斯溅射加速器，通过减少冗余排序操作和保持光栅化效率，实现1.54倍的平均加速效果。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射虽然比NeRF速度快，但仍无法满足实时应用的高帧率需求，存在瓦片大小与计算效率之间的权衡问题。

Method: 在排序阶段将小瓦片分组形成大瓦片来共享排序操作，在光栅化阶段使用位掩码标识相关小瓦片，实现排序结果的高效共享。

Result: 实验结果显示GS-TG相比最先进的3D-GS加速器平均加速1.54倍，且无需重新训练或微调。

Conclusion: GS-TG通过创新的瓦片分组和位掩码技术，有效解决了3D-GS渲染中的计算效率问题，是一种无损且可与其他优化技术无缝集成的加速方案。

Abstract: 3D Gaussian Splatting (3D-GS) has emerged as a promising alternative to neural radiance fields (NeRF) as it offers high speed as well as high image quality in novel view synthesis. Despite these advancements, 3D-GS still struggles to meet the frames per second (FPS) demands of real-time applications. In this paper, we introduce GS-TG, a tile-grouping-based accelerator that enhances 3D-GS rendering speed by reducing redundant sorting operations and preserving rasterization efficiency. GS-TG addresses a critical trade-off issue in 3D-GS rendering: increasing the tile size effectively reduces redundant sorting operations, but it concurrently increases unnecessary rasterization computations. So, during sorting of the proposed approach, GS-TG groups small tiles (for making large tiles) to share sorting operations across tiles within each group, significantly reducing redundant computations. During rasterization, a bitmask assigned to each Gaussian identifies relevant small tiles, to enable efficient sharing of sorting results. Consequently, GS-TG enables sorting to be performed as if a large tile size is used by grouping tiles during the sorting stage, while allowing rasterization to proceed with the original small tiles by using bitmasks in the rasterization stage. GS-TG is a lossless method requiring no retraining or fine-tuning and it can be seamlessly integrated with previous 3D-GS optimization techniques. Experimental results show that GS-TG achieves an average speed-up of 1.54 times over state-of-the-art 3D-GS accelerators.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [64] [Hybrid Perception and Equivariant Diffusion for Robust Multi-Node Rebar Tying](https://arxiv.org/abs/2509.00065)
*Zhitao Wang,Yirong Xiong,Roberto Horowitz,Yanke Wang,Yuxing Han*

Main category: cs.RO

TL;DR: 提出了一种结合几何感知和SE(3)等变去噪扩散的混合方法，用于自动化钢筋绑扎，仅需少量训练数据即可在复杂环境中实现鲁棒的多节点绑扎。


<details>
  <summary>Details</summary>
Motivation: 钢筋绑扎是混凝土施工中的重复性关键任务，传统手工操作存在人体工程学风险。现有机器人方法在拥挤钢筋节点中准确估计绑扎姿态面临挑战，需要大量训练数据或手动参数调整。

Method: 采用混合感知和运动规划方法：感知模块使用DBSCAN聚类、几何特征提取和PCA进行钢筋分割、节点识别和方向向量估计；运动规划基于Diffusion-EDFs，仅需5-10个演示样本即可生成优化的末端执行器姿态序列。

Result: 在单层、多层和杂乱配置等多种钢筋网格上验证，展示了高成功率的节点检测和准确的顺序绑扎性能，相比传统方法显著减少了数据需求。

Conclusion: 该方法证明了混合感知和扩散驱动规划在施工现场自动化任务中的潜力，能够提高安全性和劳动效率，为复杂环境下的机器人钢筋绑扎提供了有效解决方案。

Abstract: Rebar tying is a repetitive but critical task in reinforced concrete construction, typically performed manually at considerable ergonomic risk. Recent advances in robotic manipulation hold the potential to automate the tying process, yet face challenges in accurately estimating tying poses in congested rebar nodes. In this paper, we introduce a hybrid perception and motion planning approach that integrates geometry-based perception with Equivariant Denoising Diffusion on SE(3) (Diffusion-EDFs) to enable robust multi-node rebar tying with minimal training data. Our perception module utilizes density-based clustering (DBSCAN), geometry-based node feature extraction, and principal component analysis (PCA) to segment rebar bars, identify rebar nodes, and estimate orientation vectors for sequential ranking, even in complex, unstructured environments. The motion planner, based on Diffusion-EDFs, is trained on as few as 5-10 demonstrations to generate sequential end-effector poses that optimize collision avoidance and tying efficiency. The proposed system is validated on various rebar meshes, including single-layer, multi-layer, and cluttered configurations, demonstrating high success rates in node detection and accurate sequential tying. Compared with conventional approaches that rely on large datasets or extensive manual parameter tuning, our method achieves robust, efficient, and adaptable multi-node tying while significantly reducing data requirements. This result underscores the potential of hybrid perception and diffusion-driven planning to enhance automation in on-site construction tasks, improving both safety and labor efficiency.

</details>
