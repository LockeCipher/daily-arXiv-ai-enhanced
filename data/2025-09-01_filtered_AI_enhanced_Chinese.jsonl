{"id": "2508.21344", "pdf": "https://arxiv.org/pdf/2508.21344", "abs": "https://arxiv.org/abs/2508.21344", "authors": ["Jeong Uk Lee", "Sung Hee Choi"], "title": "ARGS: Advanced Regularization on Aligning Gaussians over the Surface", "categories": ["cs.GR", "cs.CV"], "comment": "9 pages, 4 figures", "summary": "Reconstructing high-quality 3D meshes and visuals from 3D Gaussian Splatting(3DGS) still remains a central challenge in computer graphics. Although existing models such as SuGaR offer effective solutions for rendering, there is is still room to improve improve both visual fidelity and scene consistency. This work builds upon SuGaR by introducing two complementary regularization strategies that address common limitations in both the shape of individual Gaussians and the coherence of the overall surface. The first strategy introduces an effective rank regularization, motivated by recent studies on Gaussian primitive structures. This regularization discourages extreme anisotropy-specifically, \"needle-like\" shapes-by favoring more balanced, \"disk-like\" forms that are better suited for stable surface reconstruction. The second strategy integrates a neural Signed Distance Function (SDF) into the optimization process. The SDF is regularized with an Eikonal loss to maintain proper distance properties and provides a continuous global surface prior, guiding Gaussians toward better alignment with the underlying geometry. These two regularizations aim to improve both the fidelity of individual Gaussian primitives and their collective surface behavior. The final model can make more accurate and coherent visuals from 3DGS data.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5728SuGaR\u57fa\u7840\u4e0a\u63d0\u51fa\u4e24\u79cd\u6b63\u5219\u5316\u7b56\u7565\uff1a\u6709\u6548\u79e9\u6b63\u5219\u5316\u9632\u6b62\u9ad8\u65af\u5f62\u72b6\u8fc7\u5ea6\u5404\u5411\u5f02\u6027\uff0c\u4ee5\u53ca\u795e\u7ecfSDF\u6b63\u5219\u5316\u63d0\u4f9b\u5168\u5c40\u8868\u9762\u5148\u9a8c\uff0c\u4ece\u800c\u63d0\u53473D\u9ad8\u65af\u6cfc\u6e85\u7684\u7f51\u683c\u91cd\u5efa\u8d28\u91cf\u548c\u89c6\u89c9\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u67093D\u9ad8\u65af\u6cfc\u6e85(3DGS)\u91cd\u5efa\u65b9\u6cd5\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u573a\u666f\u4e00\u81f4\u6027\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u7279\u522b\u662f\u5355\u4e2a\u9ad8\u65af\u5f62\u72b6\u7684\u4f18\u5316\u548c\u6574\u4f53\u8868\u9762\u8fde\u8d2f\u6027\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "1. \u6709\u6548\u79e9\u6b63\u5219\u5316\uff1a\u9f13\u52b1\u9ad8\u65af\u91c7\u7528\u66f4\u5e73\u8861\u7684\"\u5706\u76d8\u72b6\"\u5f62\u72b6\u800c\u975e\"\u9488\u72b6\"\u5f62\u72b6\uff1b2. \u795e\u7ecfSDF\u6b63\u5219\u5316\uff1a\u901a\u8fc7Eikonal\u635f\u5931\u4fdd\u6301\u8ddd\u79bb\u5c5e\u6027\uff0c\u63d0\u4f9b\u8fde\u7eed\u5168\u5c40\u8868\u9762\u5148\u9a8c\u3002", "result": "\u63d0\u51fa\u7684\u4e24\u79cd\u4e92\u8865\u6b63\u5219\u5316\u7b56\u7565\u80fd\u591f\u540c\u65f6\u6539\u5584\u5355\u4e2a\u9ad8\u65af\u57fa\u5143\u7684\u4fdd\u771f\u5ea6\u548c\u6574\u4f53\u8868\u9762\u884c\u4e3a\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u548c\u8fde\u8d2f\u7684\u89c6\u89c9\u91cd\u5efa\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u5f62\u72b6\u6b63\u5219\u5316\u548c\u5168\u5c40\u8868\u9762\u5148\u9a8c\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4ece3D\u9ad8\u65af\u6cfc\u6e85\u6570\u636e\u91cd\u5efa\u9ad8\u8d28\u91cf3D\u7f51\u683c\u548c\u89c6\u89c9\u6548\u679c\u7684\u80fd\u529b\u3002"}}
{"id": "2508.21090", "pdf": "https://arxiv.org/pdf/2508.21090", "abs": "https://arxiv.org/abs/2508.21090", "authors": ["Namu Kim", "Wonbin Kweon", "Minsoo Kim", "Hwanjo Yu"], "title": "Q-Align: Alleviating Attention Leakage in Zero-Shot Appearance Transfer via Query-Query Alignment", "categories": ["cs.CV"], "comment": null, "summary": "We observe that zero-shot appearance transfer with large-scale image generation models faces a significant challenge: Attention Leakage. This challenge arises when the semantic mapping between two images is captured by the Query-Key alignment. To tackle this issue, we introduce Q-Align, utilizing Query-Query alignment to mitigate attention leakage and improve the semantic alignment in zero-shot appearance transfer. Q-Align incorporates three core contributions: (1) Query-Query alignment, facilitating the sophisticated spatial semantic mapping between two images; (2) Key-Value rearrangement, enhancing feature correspondence through realignment; and (3) Attention refinement using rearranged keys and values to maintain semantic consistency. We validate the effectiveness of Q-Align through extensive experiments and analysis, and Q-Align outperforms state-of-the-art methods in appearance fidelity while maintaining competitive structure preservation.", "AI": {"tldr": "Q-Align\u65b9\u6cd5\u901a\u8fc7Query-Query\u5bf9\u9f50\u89e3\u51b3\u5927\u6a21\u578b\u96f6\u6837\u672c\u5916\u89c2\u8fc1\u79fb\u4e2d\u7684\u6ce8\u610f\u529b\u6cc4\u6f0f\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u7ed3\u6784\u7684\u540c\u65f6\u63d0\u5347\u5916\u89c2\u4fdd\u771f\u5ea6", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u96f6\u6837\u672c\u5916\u89c2\u8fc1\u79fb\u4e2d\u51fa\u73b0\u7684\u6ce8\u610f\u529b\u6cc4\u6f0f\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u6e90\u4e8eQuery-Key\u5bf9\u9f50\u5bfc\u81f4\u7684\u8bed\u4e49\u6620\u5c04\u9519\u8bef", "method": "\u63d0\u51faQ-Align\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u8d21\u732e\uff1a1) Query-Query\u5bf9\u9f50\u5b9e\u73b0\u7cbe\u7ec6\u7a7a\u95f4\u8bed\u4e49\u6620\u5c04\uff1b2) Key-Value\u91cd\u6392\u589e\u5f3a\u7279\u5f81\u5bf9\u5e94\uff1b3) \u4f7f\u7528\u91cd\u6392\u540e\u7684\u952e\u503c\u8fdb\u884c\u6ce8\u610f\u529b\u7cbe\u5316\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\uff0cQ-Align\u5728\u5916\u89c2\u4fdd\u771f\u5ea6\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u7ed3\u6784\u4fdd\u6301\u80fd\u529b", "conclusion": "Q-Align\u6709\u6548\u89e3\u51b3\u4e86\u6ce8\u610f\u529b\u6cc4\u6f0f\u95ee\u9898\uff0c\u4e3a\u96f6\u6837\u672c\u5916\u89c2\u8fc1\u79fb\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u8bed\u4e49\u5bf9\u9f50\u65b9\u6cd5"}}
{"id": "2508.21091", "pdf": "https://arxiv.org/pdf/2508.21091", "abs": "https://arxiv.org/abs/2508.21091", "authors": ["Xurui Peng", "Hong Liu", "Chenqian Yan", "Rui Ma", "Fangmin Chen", "Xing Wang", "Zhihua Wu", "Songwei Liu", "Mingbao Lin"], "title": "ERTACache: Error Rectification and Timesteps Adjustment for Efficient Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models suffer from substantial computational overhead due to their inherently iterative inference process. While feature caching offers a promising acceleration strategy by reusing intermediate outputs across timesteps, naive reuse often incurs noticeable quality degradation. In this work, we formally analyze the cumulative error introduced by caching and decompose it into two principal components: feature shift error, caused by inaccuracies in cached outputs, and step amplification error, which arises from error propagation under fixed timestep schedules. To address these issues, we propose ERTACache, a principled caching framework that jointly rectifies both error types. Our method employs an offline residual profiling stage to identify reusable steps, dynamically adjusts integration intervals via a trajectory-aware correction coefficient, and analytically approximates cache-induced errors through a closed-form residual linearization model. Together, these components enable accurate and efficient sampling under aggressive cache reuse. Extensive experiments across standard image and video generation benchmarks show that ERTACache achieves up to 2x inference speedup while consistently preserving or even improving visual quality. Notably, on the state-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x acceleration with minimal VBench degradation, effectively maintaining baseline fidelity while significantly improving efficiency. The code is available at https://github.com/bytedance/ERTACache.", "AI": {"tldr": "ERTACache\u662f\u4e00\u4e2a\u6269\u6563\u6a21\u578b\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u7f13\u5b58\u8bef\u5dee\u7684\u4e24\u4e2a\u4e3b\u8981\u6765\u6e90\uff08\u7279\u5f81\u504f\u79fb\u8bef\u5dee\u548c\u6b65\u957f\u653e\u5927\u8bef\u5dee\uff09\uff0c\u91c7\u7528\u79bb\u7ebf\u6b8b\u5dee\u5206\u6790\u548c\u8f68\u8ff9\u611f\u77e5\u6821\u6b63\u7cfb\u6570\uff0c\u5b9e\u73b02\u500d\u63a8\u7406\u52a0\u901f\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u7531\u4e8e\u8fed\u4ee3\u63a8\u7406\u8fc7\u7a0b\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5de8\u5927\u3002\u867d\u7136\u7279\u5f81\u7f13\u5b58\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u52a0\u901f\u7b56\u7565\uff0c\u4f46\u7b80\u5355\u7684\u91cd\u7528\u4f1a\u5bfc\u81f4\u660e\u663e\u7684\u8d28\u91cf\u4e0b\u964d\u3002\u9700\u8981\u89e3\u51b3\u7f13\u5b58\u5f15\u5165\u7684\u7d2f\u79ef\u8bef\u5dee\u95ee\u9898\u3002", "method": "\u63d0\u51faERTACache\u6846\u67b6\uff1a1\uff09\u79bb\u7ebf\u6b8b\u5dee\u5206\u6790\u8bc6\u522b\u53ef\u91cd\u7528\u6b65\u9aa4\uff1b2\uff09\u901a\u8fc7\u8f68\u8ff9\u611f\u77e5\u6821\u6b63\u7cfb\u6570\u52a8\u6001\u8c03\u6574\u79ef\u5206\u95f4\u9694\uff1b3\uff09\u4f7f\u7528\u95ed\u5f0f\u6b8b\u5dee\u7ebf\u6027\u5316\u6a21\u578b\u8fd1\u4f3c\u7f13\u5b58\u8bef\u5dee\u3002\u8054\u5408\u6821\u6b63\u7279\u5f81\u504f\u79fb\u8bef\u5dee\u548c\u6b65\u957f\u653e\u5927\u8bef\u5dee\u3002", "result": "\u5728\u6807\u51c6\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cERTACache\u5b9e\u73b0\u9ad8\u8fbe2\u500d\u7684\u63a8\u7406\u52a0\u901f\uff0c\u540c\u65f6\u4e00\u81f4\u4fdd\u6301\u751a\u81f3\u6539\u5584\u89c6\u89c9\u8d28\u91cf\u3002\u5728Wan2.1\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e0a\uff0c2\u500d\u52a0\u901f\u4e0bVBench\u6307\u6807\u4e0b\u964d\u6700\u5c0f\uff0c\u6709\u6548\u4fdd\u6301\u57fa\u7ebf\u4fdd\u771f\u5ea6\u3002", "conclusion": "ERTACache\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u7f13\u5b58\u8bef\u5dee\u673a\u5236\u5e76\u63d0\u51fa\u8054\u5408\u6821\u6b63\u65b9\u6848\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u7f13\u5b58\u52a0\u901f\u4e2d\u7684\u8d28\u91cf\u9000\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u91c7\u6837\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u52a0\u901f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.21099", "pdf": "https://arxiv.org/pdf/2508.21099", "abs": "https://arxiv.org/abs/2508.21099", "authors": ["Xiangtao Meng", "Yingkai Dong", "Ning Yu", "Li Wang", "Zheng Li", "Shanqing Guo"], "title": "Safe-Control: A Safety Patch for Mitigating Unsafe Content in Text-to-Image Generation Models", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "Despite the advancements in Text-to-Image (T2I) generation models, their potential for misuse or even abuse raises serious safety concerns. Model developers have made tremendous efforts to introduce safety mechanisms that can address these concerns in T2I models. However, the existing safety mechanisms, whether external or internal, either remain susceptible to evasion under distribution shifts or require extensive model-specific adjustments. To address these limitations, we introduce Safe-Control, an innovative plug-and-play safety patch designed to mitigate unsafe content generation in T2I models. Using data-driven strategies and safety-aware conditions, Safe-Control injects safety control signals into the locked T2I model, acting as an update in a patch-like manner. Model developers can also construct various safety patches to meet the evolving safety requirements, which can be flexibly merged into a single, unified patch. Its plug-and-play design further ensures adaptability, making it compatible with other T2I models of similar denoising architecture. We conduct extensive evaluations on six diverse and public T2I models. Empirical results highlight that Safe-Control is effective in reducing unsafe content generation across six diverse T2I models with similar generative architectures, yet it successfully maintains the quality and text alignment of benign images. Compared to seven state-of-the-art safety mechanisms, including both external and internal defenses, Safe-Control significantly outperforms all baselines in reducing unsafe content generation. For example, it reduces the probability of unsafe content generation to 7%, compared to approximately 20% for most baseline methods, under both unsafe prompts and the latest adversarial attacks.", "AI": {"tldr": "Safe-Control\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u5b89\u5168\u8865\u4e01\uff0c\u7528\u4e8e\u51cf\u5c11\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u7684\u4e0d\u5b89\u5168\u5185\u5bb9\u751f\u6210\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7b56\u7565\u548c\u5b89\u5168\u611f\u77e5\u6761\u4ef6\u5411\u9501\u5b9a\u6a21\u578b\u6ce8\u5165\u5b89\u5168\u63a7\u5236\u4fe1\u53f7\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5b89\u5168\u673a\u5236\u5bb9\u6613\u53d7\u5230\u5206\u5e03\u504f\u79fb\u7684\u89c4\u907f\uff0c\u6216\u9700\u8981\u5927\u91cf\u6a21\u578b\u7279\u5b9a\u8c03\u6574\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u548c\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u6570\u636e\u9a71\u52a8\u7b56\u7565\u548c\u5b89\u5168\u611f\u77e5\u6761\u4ef6\uff0c\u4ee5\u8865\u4e01\u5f62\u5f0f\u5411\u9501\u5b9a\u7684T2I\u6a21\u578b\u6ce8\u5165\u5b89\u5168\u63a7\u5236\u4fe1\u53f7\uff0c\u652f\u6301\u6784\u5efa\u591a\u79cd\u5b89\u5168\u8865\u4e01\u5e76\u7075\u6d3b\u5408\u5e76\u4e3a\u7edf\u4e00\u8865\u4e01\u3002", "result": "\u5728\u516d\u4e2a\u4e0d\u540c\u7684T2I\u6a21\u578b\u4e0a\u8bc4\u4f30\u663e\u793a\uff0cSafe-Control\u5c06\u4e0d\u5b89\u5168\u5185\u5bb9\u751f\u6210\u6982\u7387\u964d\u81f37%\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u7ea620%\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u6027\u56fe\u50cf\u8d28\u91cf\u548c\u6587\u672c\u5bf9\u9f50\u3002", "conclusion": "Safe-Control\u662f\u4e00\u79cd\u6709\u6548\u7684\u5373\u63d2\u5373\u7528\u5b89\u5168\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11T2I\u6a21\u578b\u7684\u4e0d\u5b89\u5168\u5185\u5bb9\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u5b89\u5168\u673a\u5236\u3002"}}
{"id": "2508.21254", "pdf": "https://arxiv.org/pdf/2508.21254", "abs": "https://arxiv.org/abs/2508.21254", "authors": ["Yidong Zhao", "Peter Kellman", "Hui Xue", "Tongyun Yang", "Yi Zhang", "Yuchi Han", "Orlando Simonetti", "Qian Tao"], "title": "Reverse Imaging for Wide-spectrum Generalization of Cardiac MRI Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Pretrained segmentation models for cardiac magnetic resonance imaging (MRI) struggle to generalize across different imaging sequences due to significant variations in image contrast. These variations arise from changes in imaging protocols, yet the same fundamental spin properties, including proton density, T1, and T2 values, govern all acquired images. With this core principle, we introduce Reverse Imaging, a novel physics-driven method for cardiac MRI data augmentation and domain adaptation to fundamentally solve the generalization problem. Our method reversely infers the underlying spin properties from observed cardiac MRI images, by solving ill-posed nonlinear inverse problems regularized by the prior distribution of spin properties. We acquire this \"spin prior\" by learning a generative diffusion model from the multiparametric SAturation-recovery single-SHot acquisition sequence (mSASHA) dataset, which offers joint cardiac T1 and T2 maps. Our method enables approximate but meaningful spin-property estimates from MR images, which provide an interpretable \"latent variable\" that lead to highly flexible image synthesis of arbitrary novel sequences. We show that Reverse Imaging enables highly accurate segmentation across vastly different image contrasts and imaging protocols, realizing wide-spectrum generalization of cardiac MRI segmentation.", "AI": {"tldr": "Reverse Imaging\u662f\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u539f\u7406\u7684\u5fc3\u810fMRI\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u9006\u5411\u63a8\u65ad\u5e95\u5c42\u81ea\u65cb\u5c5e\u6027\u6765\u89e3\u51b3\u4e0d\u540c\u6210\u50cf\u5e8f\u5217\u95f4\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5206\u5272\u6a21\u578b\u7684\u8de8\u57df\u6027\u80fd", "motivation": "\u9884\u8bad\u7ec3\u7684\u5fc3\u810fMRI\u5206\u5272\u6a21\u578b\u96be\u4ee5\u6cdb\u5316\u5230\u4e0d\u540c\u5bf9\u6bd4\u5ea6\u7684\u6210\u50cf\u5e8f\u5217\uff0c\u56e0\u4e3a\u6210\u50cf\u534f\u8bae\u53d8\u5316\u5bfc\u81f4\u56fe\u50cf\u5bf9\u6bd4\u5ea6\u5dee\u5f02\u5de8\u5927\uff0c\u4f46\u6240\u6709\u56fe\u50cf\u90fd\u7531\u76f8\u540c\u7684\u81ea\u65cb\u5c5e\u6027\uff08\u8d28\u5b50\u5bc6\u5ea6\u3001T1\u3001T2\u503c\uff09\u63a7\u5236", "method": "\u63d0\u51faReverse Imaging\u65b9\u6cd5\uff1a1\uff09\u4ece\u89c2\u6d4b\u7684MRI\u56fe\u50cf\u9006\u5411\u6c42\u89e3\u975e\u7ebf\u6027\u9006\u95ee\u9898\u63a8\u65ad\u5e95\u5c42\u81ea\u65cb\u5c5e\u6027\uff1b2\uff09\u4f7f\u7528\u6269\u6563\u6a21\u578b\u5b66\u4e60mSASHA\u6570\u636e\u96c6\u4e2d\u7684\u81ea\u65cb\u5c5e\u6027\u5148\u9a8c\u5206\u5e03\uff1b3\uff09\u57fa\u4e8e\u63a8\u65ad\u7684\u81ea\u65cb\u5c5e\u6027\u5408\u6210\u4efb\u610f\u65b0\u5e8f\u5217\u7684\u56fe\u50cf", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4eceMR\u56fe\u50cf\u83b7\u5f97\u6709\u610f\u4e49\u7684\u81ea\u65cb\u5c5e\u6027\u4f30\u8ba1\uff0c\u5b9e\u73b0\u9ad8\u5ea6\u7075\u6d3b\u7684\u56fe\u50cf\u5408\u6210\uff0c\u4f7f\u5206\u5272\u6a21\u578b\u5728\u5b8c\u5168\u4e0d\u540c\u56fe\u50cf\u5bf9\u6bd4\u5ea6\u548c\u6210\u50cf\u534f\u8bae\u4e0b\u90fd\u80fd\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5206\u5272", "conclusion": "Reverse Imaging\u901a\u8fc7\u7269\u7406\u9a71\u52a8\u7684\u6570\u636e\u589e\u5f3a\u548c\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u4ece\u6839\u672c\u4e0a\u89e3\u51b3\u4e86\u5fc3\u810fMRI\u5206\u5272\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5bbd\u8c31\u6cdb\u5316\u80fd\u529b"}}
{"id": "2508.21371", "pdf": "https://arxiv.org/pdf/2508.21371", "abs": "https://arxiv.org/abs/2508.21371", "authors": ["Qingran Miao", "Haixia Wang", "Haohao Sun", "Yilong Zhang"], "title": "Print2Volume: Generating Synthetic OCT-based 3D Fingerprint Volume from 2D Fingerprint Image", "categories": ["cs.CV"], "comment": null, "summary": "Optical Coherence Tomography (OCT) enables the acquisition of high-resolution, three-dimensional fingerprint data, capturing rich subsurface structures for robust biometric recognition. However, the high cost and time-consuming nature of OCT data acquisition have led to a scarcity of large-scale public datasets, significantly hindering the development of advanced algorithms, particularly data-hungry deep learning models. To address this critical bottleneck, this paper introduces Print2Volume, a novel framework for generating realistic, synthetic OCT-based 3D fingerprints from 2D fingerprint image. Our framework operates in three sequential stages: (1) a 2D style transfer module that converts a binary fingerprint into a grayscale images mimicking the style of a Z-direction mean-projected OCT scan; (2) a 3D Structure Expansion Network that extrapolates the 2D im-age into a plausible 3D anatomical volume; and (3) an OCT Realism Refiner, based on a 3D GAN, that renders the structural volume with authentic textures, speckle noise, and other imaging characteristics. Using Print2Volume, we generated a large-scale synthetic dataset of 420,000 samples. Quantitative experiments demonstrate the high quality of our synthetic data and its significant impact on recognition performance. By pre-training a recognition model on our synthetic data and fine-tuning it on a small real-world dataset, we achieved a remarkable reduction in the Equal Error Rate (EER) from 15.62% to 2.50% on the ZJUT-EIFD benchmark, proving the effectiveness of our approach in overcoming data scarcity.", "AI": {"tldr": "\u63d0\u51faPrint2Volume\u6846\u67b6\uff0c\u4ece2D\u6307\u7eb9\u56fe\u50cf\u751f\u6210\u903c\u771f\u7684\u5408\u6210OCT 3D\u6307\u7eb9\u6570\u636e\uff0c\u89e3\u51b3OCT\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u8bc6\u522b\u6027\u80fd", "motivation": "OCT\u6280\u672f\u80fd\u83b7\u53d6\u9ad8\u5206\u8fa8\u73873D\u6307\u7eb9\u6570\u636e\uff0c\u4f46\u6570\u636e\u91c7\u96c6\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\uff0c\u5bfc\u81f4\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u963b\u788d\u4e86\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u7684\u53d1\u5c55", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1\uff092D\u98ce\u683c\u8fc1\u79fb\u6a21\u5757\u5c06\u4e8c\u503c\u6307\u7eb9\u8f6c\u4e3a\u7070\u5ea6\u56fe\u50cf\uff1b2\uff093D\u7ed3\u6784\u6269\u5c55\u7f51\u7edc\u5c062D\u56fe\u50cf\u5916\u63a8\u4e3a3D\u89e3\u5256\u4f53\u79ef\uff1b3\uff09\u57fa\u4e8e3D GAN\u7684OCT\u771f\u5b9e\u6027\u7ec6\u5316\u5668\u6dfb\u52a0\u7eb9\u7406\u548c\u566a\u58f0", "result": "\u751f\u621042\u4e07\u4e2a\u5408\u6210\u6837\u672c\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u540e\u5728\u771f\u5b9e\u5c0f\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\uff0c\u5c06\u7b49\u9519\u8bef\u7387\u4ece15.62%\u964d\u81f32.50%", "conclusion": "Print2Volume\u80fd\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210OCT\u6307\u7eb9\u6570\u636e\uff0c\u663e\u8457\u7f13\u89e3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5927\u5e45\u63d0\u5347\u751f\u7269\u8bc6\u522b\u6027\u80fd"}}
{"id": "2508.21444", "pdf": "https://arxiv.org/pdf/2508.21444", "abs": "https://arxiv.org/abs/2508.21444", "authors": ["Jiayu Yang", "Weijian Su", "Songqian Zhang", "Yuqi Han", "Jinli Suo", "Qiang Zhang"], "title": "Scale-GS: Efficient Scalable Gaussian Splatting via Redundancy-filtering Training on Streaming Content", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) enables high-fidelity real-time rendering, a key requirement for immersive applications. However, the extension of 3DGS to dynamic scenes remains limitations on the substantial data volume of dense Gaussians and the prolonged training time required for each frame. This paper presents \\M, a scalable Gaussian Splatting framework designed for efficient training in streaming tasks. Specifically, Gaussian spheres are hierarchically organized by scale within an anchor-based structure. Coarser-level Gaussians represent the low-resolution structure of the scene, while finer-level Gaussians, responsible for detailed high-fidelity rendering, are selectively activated by the coarser-level Gaussians. To further reduce computational overhead, we introduce a hybrid deformation and spawning strategy that models motion of inter-frame through Gaussian deformation and triggers Gaussian spawning to characterize wide-range motion. Additionally, a bidirectional adaptive masking mechanism enhances training efficiency by removing static regions and prioritizing informative viewpoints. Extensive experiments demonstrate that \\M~ achieves superior visual quality while significantly reducing training time compared to state-of-the-art methods.", "AI": {"tldr": "M\u6846\u67b6\u901a\u8fc7\u5206\u5c42\u9ad8\u65af\u7403\u7ed3\u6784\u548c\u6df7\u5408\u53d8\u5f62\u751f\u6210\u7b56\u7565\uff0c\u5728\u52a8\u6001\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u8bad\u7ec3\u548c\u9ad8\u8d28\u91cf\u6e32\u67d3\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u548c\u8ba1\u7b97\u5f00\u9500", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u5728\u52a8\u6001\u573a\u666f\u4e2d\u5b58\u5728\u6570\u636e\u91cf\u5927\u548c\u8bad\u7ec3\u65f6\u95f4\u957f\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u9ad8\u6548\u8bad\u7ec3\u6846\u67b6", "method": "\u4f7f\u7528\u57fa\u4e8e\u951a\u70b9\u7684\u5206\u5c42\u9ad8\u65af\u7403\u7ed3\u6784\uff0c\u7c97\u7c92\u5ea6\u9ad8\u65af\u8868\u793a\u573a\u666f\u7ed3\u6784\uff0c\u7ec6\u7c92\u5ea6\u9ad8\u65af\u8d1f\u8d23\u7ec6\u8282\u6e32\u67d3\uff1b\u91c7\u7528\u6df7\u5408\u53d8\u5f62\u548c\u751f\u6210\u7b56\u7565\u5efa\u6a21\u8fd0\u52a8\uff1b\u53cc\u5411\u81ea\u9002\u5e94\u63a9\u7801\u673a\u5236\u63d0\u5347\u8bad\u7ec3\u6548\u7387", "result": "\u5728\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cM\u6846\u67b6\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u89c6\u89c9\u8d28\u91cf\uff0c\u540c\u65f6\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u52a8\u6001\u573a\u666f\u7684\u9ad8\u65af\u6cfc\u6e85\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u6e32\u67d3\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u8bad\u7ec3\u6548\u7387"}}
{"id": "2508.21529", "pdf": "https://arxiv.org/pdf/2508.21529", "abs": "https://arxiv.org/abs/2508.21529", "authors": ["Ronan Docherty", "Antonis Vamvakeros", "Samuel J. Cooper"], "title": "Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation", "categories": ["cs.CV", "cond-mat.mtrl-sci"], "comment": null, "summary": "Feature foundation models - usually vision transformers - offer rich semantic descriptors of images, useful for downstream tasks such as (interactive) segmentation and object detection. For computational efficiency these descriptors are often patch-based, and so struggle to represent the fine features often present in micrographs; they also struggle with the large image sizes present in materials and biological image analysis. In this work, we train a convolutional neural network to upsample low-resolution (i.e, large patch size) foundation model features with reference to the input image. We apply this upsampler network (without any further training) to efficiently featurise and then segment a variety of microscopy images, including plant cells, a lithium-ion battery cathode and organic crystals. The richness of these upsampled features admits separation of hard to segment phases, like hairline cracks. We demonstrate that interactive segmentation with these deep features produces high-quality segmentations far faster and with far fewer labels than training or finetuning a more traditional convolutional network.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e0a\u91c7\u6837\u5668\uff0c\u5c06\u4f4e\u5206\u8fa8\u7387\u57fa\u7840\u6a21\u578b\u7279\u5f81\u4e0e\u8f93\u5165\u56fe\u50cf\u7ed3\u5408\uff0c\u9ad8\u6548\u5904\u7406\u663e\u5fae\u955c\u56fe\u50cf\u5206\u5272\uff0c\u663e\u8457\u51cf\u5c11\u6807\u6ce8\u9700\u6c42\u5e76\u63d0\u9ad8\u5206\u5272\u8d28\u91cf", "motivation": "\u73b0\u6709\u57fa\u4e8epatch\u7684\u57fa\u7840\u6a21\u578b\u7279\u5f81\u5728\u5904\u7406\u663e\u5fae\u955c\u56fe\u50cf\u65f6\u9762\u4e34\u4e24\u4e2a\u95ee\u9898\uff1a1\uff09\u65e0\u6cd5\u6355\u6349\u5fae\u7ed3\u6784\u4e2d\u7684\u7cbe\u7ec6\u7279\u5f81\uff1b2\uff09\u96be\u4ee5\u5904\u7406\u6750\u6599\u79d1\u5b66\u548c\u751f\u7269\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u5927\u5c3a\u5bf8\u56fe\u50cf", "method": "\u8bad\u7ec3\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u53c2\u8003\u8f93\u5165\u56fe\u50cf\u5bf9\u4f4e\u5206\u8fa8\u7387\uff08\u5927patch\u5c3a\u5bf8\uff09\u57fa\u7840\u6a21\u578b\u7279\u5f81\u8fdb\u884c\u4e0a\u91c7\u6837\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5e94\u7528\u4e8e\u5404\u79cd\u663e\u5fae\u955c\u56fe\u50cf\u7684\u7279\u5f81\u63d0\u53d6\u548c\u5206\u5272", "result": "\u4e0a\u91c7\u6837\u540e\u7684\u4e30\u5bcc\u7279\u5f81\u80fd\u591f\u5206\u79bb\u96be\u4ee5\u5206\u5272\u7684\u76f8\uff08\u5982\u53d1\u4e1d\u88c2\u7eb9\uff09\uff0c\u4ea4\u4e92\u5f0f\u5206\u5272\u4f7f\u7528\u8fd9\u4e9b\u6df1\u5ea6\u7279\u5f81\u80fd\u591f\u4ee5\u66f4\u5c11\u7684\u6807\u7b7e\u548c\u66f4\u5feb\u7684\u901f\u5ea6\u4ea7\u751f\u9ad8\u8d28\u91cf\u5206\u5272\u7ed3\u679c", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u663e\u5fae\u955c\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u7279\u5f81\u63d0\u53d6\u548c\u5206\u5272\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684\u5377\u79ef\u7f51\u7edc\u8bad\u7ec3\u6216\u5fae\u8c03\u65b9\u6cd5"}}
{"id": "2508.21542", "pdf": "https://arxiv.org/pdf/2508.21542", "abs": "https://arxiv.org/abs/2508.21542", "authors": ["Ziwei Liao", "Mohamed Sayed", "Steven L. Waslander", "Sara Vicente", "Daniyar Turmukhambetov", "Michael Firman"], "title": "Complete Gaussian Splats from a Single Image with Denoising Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Main paper: 11 pages; Supplementary materials: 7 pages", "summary": "Gaussian splatting typically requires dense observations of the scene and can fail to reconstruct occluded and unobserved areas. We propose a latent diffusion model to reconstruct a complete 3D scene with Gaussian splats, including the occluded parts, from only a single image during inference. Completing the unobserved surfaces of a scene is challenging due to the ambiguity of the plausible surfaces. Conventional methods use a regression-based formulation to predict a single \"mode\" for occluded and out-of-frustum surfaces, leading to blurriness, implausibility, and failure to capture multiple possible explanations. Thus, they often address this problem partially, focusing either on objects isolated from the background, reconstructing only visible surfaces, or failing to extrapolate far from the input views. In contrast, we propose a generative formulation to learn a distribution of 3D representations of Gaussian splats conditioned on a single input image. To address the lack of ground-truth training data, we propose a Variational AutoReconstructor to learn a latent space only from 2D images in a self-supervised manner, over which a diffusion model is trained. Our method generates faithful reconstructions and diverse samples with the ability to complete the occluded surfaces for high-quality 360-degree renderings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u5355\u56fe\u50cf3D\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\uff0c\u4f7f\u7528\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u6765\u8865\u5168\u906e\u6321\u548c\u672a\u89c2\u6d4b\u533a\u57df\uff0c\u901a\u8fc7\u53d8\u5206\u81ea\u91cd\u6784\u5668\u5728\u65e0\u76d1\u7763\u6761\u4ef6\u4e0b\u5b66\u4e60\u6f5c\u5728\u7a7a\u95f4\uff0c\u652f\u6301\u9ad8\u8d28\u91cf360\u5ea6\u6e32\u67d3\u3002", "motivation": "\u4f20\u7edf\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u9700\u8981\u5bc6\u96c6\u89c2\u6d4b\uff0c\u65e0\u6cd5\u91cd\u5efa\u906e\u6321\u548c\u672a\u89c2\u6d4b\u533a\u57df\uff1b\u56de\u5f52\u65b9\u6cd5\u53ea\u80fd\u9884\u6d4b\u5355\u4e00\u6a21\u5f0f\uff0c\u5bfc\u81f4\u6a21\u7cca\u548c\u4e0d\u771f\u5b9e\u7684\u7ed3\u679c\uff0c\u65e0\u6cd5\u5904\u7406\u591a\u53ef\u80fd\u6027\u89e3\u91ca\u3002", "method": "\u4f7f\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5b66\u4e60\u57fa\u4e8e\u5355\u5f20\u8f93\u5165\u56fe\u50cf\u7684\u6761\u4ef6\u53163D\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u5206\u5e03\uff1b\u63d0\u51fa\u53d8\u5206\u81ea\u91cd\u6784\u5668\u4ece2D\u56fe\u50cf\u65e0\u76d1\u7763\u5b66\u4e60\u6f5c\u5728\u7a7a\u95f4\uff0c\u7136\u540e\u5728\u5176\u4e0a\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u3002", "result": "\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u5fe0\u5b9e\u91cd\u5efa\u7ed3\u679c\u548c\u591a\u6837\u5316\u6837\u672c\uff0c\u6210\u529f\u8865\u5168\u906e\u6321\u8868\u9762\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684360\u5ea6\u6e32\u67d3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u5f0f\u5efa\u6a21\u89e3\u51b3\u4e86\u5355\u56fe\u50cf3D\u91cd\u5efa\u4e2d\u7684\u906e\u6321\u8865\u5168\u95ee\u9898\uff0c\u76f8\u6bd4\u4f20\u7edf\u56de\u5f52\u65b9\u6cd5\u80fd\u591f\u4ea7\u751f\u66f4\u771f\u5b9e\u548c\u591a\u6837\u5316\u7684\u7ed3\u679c\u3002"}}
{"id": "2508.21712", "pdf": "https://arxiv.org/pdf/2508.21712", "abs": "https://arxiv.org/abs/2508.21712", "authors": ["Alvaro Patricio", "Atabak Dehban", "Rodrigo Ventura"], "title": "FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in diffusion-based generative models have demonstrated significant potential in augmenting scarce datasets for object detection tasks. Nevertheless, most recent models rely on resource-intensive full fine-tuning of large-scale diffusion models, requiring enterprise-grade GPUs (e.g., NVIDIA V100) and thousands of synthetic images. To address these limitations, we propose Flux LoRA Augmentation (FLORA), a lightweight synthetic data generation pipeline. Our approach uses the Flux 1.1 Dev diffusion model, fine-tuned exclusively through Low-Rank Adaptation (LoRA). This dramatically reduces computational requirements, enabling synthetic dataset generation with a consumer-grade GPU (e.g., NVIDIA RTX 4090). We empirically evaluate our approach on seven diverse object detection datasets. Our results demonstrate that training object detectors with just 500 synthetic images generated by our approach yields superior detection performance compared to models trained on 5000 synthetic images from the ODGEN baseline, achieving improvements of up to 21.3% in mAP@.50:.95. This work demonstrates that it is possible to surpass state-of-the-art performance with far greater efficiency, as FLORA achieves superior results using only 10% of the data and a fraction of the computational cost. This work demonstrates that a quality and efficiency-focused approach is more effective than brute-force generation, making advanced synthetic data creation more practical and accessible for real-world scenarios.", "AI": {"tldr": "FLORA\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u4f7f\u7528LoRA\u5fae\u8c03Flux 1.1\u6269\u6563\u6a21\u578b\uff0c\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\uff0c\u4ec5\u9700\u6d88\u8d39\u7ea7GPU\u5373\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u56fe\u50cf\u7528\u4e8e\u76ee\u6807\u68c0\u6d4b\u6570\u636e\u589e\u5f3a\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u9700\u8981\u4f01\u4e1a\u7ea7GPU\u548c\u5927\u91cf\u5408\u6210\u56fe\u50cf\uff0c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u8fc7\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u4f7f\u7528Flux 1.1 Dev\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94(LoRA)\u8fdb\u884c\u5fae\u8c03\uff0c\u6784\u5efa\u8f7b\u91cf\u7ea7\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\u3002", "result": "\u57287\u4e2a\u76ee\u6807\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u4ec5\u7528500\u5f20\u5408\u6210\u56fe\u50cf\u5c31\u80fd\u8d85\u8d8aODGEN\u57fa\u7ebf\u4f7f\u75285000\u5f20\u56fe\u50cf\u7684\u6027\u80fd\uff0cmAP@.50:.95\u63d0\u5347\u9ad8\u8fbe21.3%\u3002", "conclusion": "FLORA\u8bc1\u660e\u4e86\u8d28\u91cf\u4f18\u5148\u3001\u6548\u7387\u5bfc\u5411\u7684\u65b9\u6cd5\u6bd4\u66b4\u529b\u751f\u6210\u66f4\u6709\u6548\uff0c\u4ec5\u752810%\u7684\u6570\u636e\u548c\u5c11\u91cf\u8ba1\u7b97\u6210\u672c\u5c31\u80fd\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4f7f\u5408\u6210\u6570\u636e\u751f\u6210\u66f4\u52a0\u5b9e\u7528\u548c\u53ef\u53ca\u3002"}}
{"id": "2508.21732", "pdf": "https://arxiv.org/pdf/2508.21732", "abs": "https://arxiv.org/abs/2508.21732", "authors": ["Jo\u00e3o Valente", "Atabak Dehban", "Rodrigo Ventura"], "title": "CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities across various multimodal tasks. They continue, however, to struggle with trivial scenarios such as reading values from Digital Measurement Devices (DMDs), particularly in real-world conditions involving clutter, occlusions, extreme viewpoints, and motion blur; common in head-mounted cameras and Augmented Reality (AR) applications. Motivated by these limitations, this work introduces CAD2DMD-SET, a synthetic data generation tool designed to support visual question answering (VQA) tasks involving DMDs. By leveraging 3D CAD models, advanced rendering, and high-fidelity image composition, our tool produces diverse, VQA-labelled synthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we present DMDBench, a curated validation set of 1,000 annotated real-world images designed to evaluate model performance under practical constraints. Benchmarking three state-of-the-art LVLMs using Average Normalised Levenshtein Similarity (ANLS) and further fine-tuning LoRA's of these models with CAD2DMD-SET's generated dataset yielded substantial improvements, with InternVL showcasing a score increase of 200% without degrading on other tasks. This demonstrates that the CAD2DMD-SET training dataset substantially improves the robustness and performance of LVLMs when operating under the previously stated challenging conditions. The CAD2DMD-SET tool is expected to be released as open-source once the final version of this manuscript is prepared, allowing the community to add different measurement devices and generate their own datasets.", "AI": {"tldr": "CAD2DMD-SET\u662f\u4e00\u4e2a\u5408\u6210\u6570\u636e\u751f\u6210\u5de5\u5177\uff0c\u901a\u8fc73D CAD\u6a21\u578b\u548c\u9ad8\u7ea7\u6e32\u67d3\u6280\u672f\u751f\u6210\u6570\u5b57\u6d4b\u91cf\u8bbe\u5907(DMD)\u7684\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u8bfb\u6570\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b57\u6d4b\u91cf\u8bbe\u5907\u8bfb\u6570\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u771f\u5b9e\u4e16\u754c\u7684\u6742\u4e71\u3001\u906e\u6321\u3001\u6781\u7aef\u89c6\u89d2\u548c\u8fd0\u52a8\u6a21\u7cca\u7b49\u6311\u6218\u6027\u6761\u4ef6\u4e0b\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u5934\u6234\u5f0f\u6444\u50cf\u5934\u548c\u589e\u5f3a\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u5229\u75283D CAD\u6a21\u578b\u3001\u9ad8\u7ea7\u6e32\u67d3\u548c\u9ad8\u4fdd\u771f\u56fe\u50cf\u5408\u6210\u6280\u672f\u5f00\u53d1CAD2DMD-SET\u5de5\u5177\uff0c\u751f\u6210\u591a\u6837\u5316\u7684VQA\u6807\u6ce8\u5408\u6210DMD\u6570\u636e\u96c6\u3002\u540c\u65f6\u521b\u5efaDMDBench\u9a8c\u8bc1\u96c6\uff081000\u5f20\u6807\u6ce8\u771f\u5b9e\u56fe\u50cf\uff09\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u4f7f\u7528\u5e73\u5747\u5f52\u4e00\u5316Levenshtein\u76f8\u4f3c\u5ea6(ANLS)\u5bf9\u4e09\u4e2a\u6700\u5148\u8fdbLVLM\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7CAD2DMD-SET\u751f\u6210\u7684\u6570\u636e\u96c6\u5fae\u8c03LoRA\u540e\uff0cInternVL\u6a21\u578b\u5f97\u5206\u63d0\u5347200%\uff0c\u4e14\u5728\u5176\u4ed6\u4efb\u52a1\u4e0a\u6027\u80fd\u672a\u4e0b\u964d\u3002", "conclusion": "CAD2DMD-SET\u8bad\u7ec3\u6570\u636e\u96c6\u663e\u8457\u63d0\u9ad8\u4e86LVLM\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\uff0c\u8be5\u5de5\u5177\u5c06\u4f5c\u4e3a\u5f00\u6e90\u53d1\u5e03\uff0c\u5141\u8bb8\u793e\u533a\u6dfb\u52a0\u4e0d\u540c\u6d4b\u91cf\u8bbe\u5907\u5e76\u751f\u6210\u81ea\u5df1\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2508.21816", "pdf": "https://arxiv.org/pdf/2508.21816", "abs": "https://arxiv.org/abs/2508.21816", "authors": ["Yiming Lin", "Yuchen Niu", "Shang Wang", "Kaizhu Huang", "Qiufeng Wang", "Xiao-Bo Jin"], "title": "The Demon is in Ambiguity: Revisiting Situation Recognition with Single Positive Multi-Label Learning", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICDM 2025", "summary": "Context recognition (SR) is a fundamental task in computer vision that aims to extract structured semantic summaries from images by identifying key events and their associated entities. Specifically, given an input image, the model must first classify the main visual events (verb classification), then identify the participating entities and their semantic roles (semantic role labeling), and finally localize these entities in the image (semantic role localization). Existing methods treat verb classification as a single-label problem, but we show through a comprehensive analysis that this formulation fails to address the inherent ambiguity in visual event recognition, as multiple verb categories may reasonably describe the same image. This paper makes three key contributions: First, we reveal through empirical analysis that verb classification is inherently a multi-label problem due to the ubiquitous semantic overlap between verb categories. Second, given the impracticality of fully annotating large-scale datasets with multiple labels, we propose to reformulate verb classification as a single positive multi-label learning (SPMLL) problem - a novel perspective in SR research. Third, we design a comprehensive multi-label evaluation benchmark for SR that is carefully designed to fairly evaluate model performance in a multi-label setting. To address the challenges of SPMLL, we futher develop the Graph Enhanced Verb Multilayer Perceptron (GE-VerbMLP), which combines graph neural networks to capture label correlations and adversarial training to optimize decision boundaries. Extensive experiments on real-world datasets show that our approach achieves more than 3\\% MAP improvement while remaining competitive on traditional top-1 and top-5 accuracy metrics.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u573a\u666f\u8bc6\u522b\u4e2d\u52a8\u8bcd\u5206\u7c7b\u672c\u8d28\u4e0a\u662f\u591a\u6807\u7b7e\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5355\u6b63\u4f8b\u591a\u6807\u7b7e\u5b66\u4e60\u6846\u67b6\u548cGE-VerbMLP\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u4f20\u7edf\u6307\u6807\u7ade\u4e89\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u4e863%\u4ee5\u4e0a\u7684MAP\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u52a8\u8bcd\u5206\u7c7b\u89c6\u4e3a\u5355\u6807\u7b7e\u95ee\u9898\uff0c\u4f46\u89c6\u89c9\u4e8b\u4ef6\u8bc6\u522b\u5b58\u5728\u56fa\u6709\u6b67\u4e49\u6027\uff0c\u540c\u4e00\u56fe\u50cf\u53ef\u80fd\u88ab\u591a\u4e2a\u52a8\u8bcd\u7c7b\u522b\u5408\u7406\u63cf\u8ff0\uff0c\u8fd9\u79cd\u5355\u6807\u7b7e\u8bbe\u5b9a\u65e0\u6cd5\u5904\u7406\u52a8\u8bcd\u7c7b\u522b\u95f4\u7684\u8bed\u4e49\u91cd\u53e0\u95ee\u9898\u3002", "method": "\u5c06\u52a8\u8bcd\u5206\u7c7b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5355\u6b63\u4f8b\u591a\u6807\u7b7e\u5b66\u4e60(SPMLL)\u95ee\u9898\uff0c\u63d0\u51fa\u56fe\u589e\u5f3a\u52a8\u8bcd\u591a\u5c42\u611f\u77e5\u673a(GE-VerbMLP)\uff0c\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u6807\u7b7e\u76f8\u5173\u6027\uff0c\u4f7f\u7528\u5bf9\u6297\u8bad\u7ec3\u4f18\u5316\u51b3\u7b56\u8fb9\u754c\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u4f20\u7edftop-1\u548ctop-5\u51c6\u786e\u7387\u6307\u6807\u7ade\u4e89\u529b\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc73%\u7684\u5e73\u5747\u7cbe\u5ea6\u5747\u503c(MAP)\u63d0\u5347\u3002", "conclusion": "\u52a8\u8bcd\u5206\u7c7b\u672c\u8d28\u4e0a\u662f\u591a\u6807\u7b7e\u95ee\u9898\uff0c\u63d0\u51fa\u7684SPMLL\u6846\u67b6\u548cGE-VerbMLP\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u4e8b\u4ef6\u8bc6\u522b\u4e2d\u7684\u8bed\u4e49\u6a21\u7cca\u6027\uff0c\u4e3a\u573a\u666f\u8bc6\u522b\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\u548c\u89e3\u51b3\u65b9\u6848\u3002"}}
